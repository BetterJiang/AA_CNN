Abstract
Weintroducearecurrentarchitecturehavingamodularstructure
andweformulateatrainingprocedurebasedontheEMalgorithm.
TheresultingmodelhassimilaritiestohiddenMarkovmodels,but
supportsrecurrentnetworksprocessingstyleandallowstoexploit
thesupervisedlearningparadigmwhileusingmaximumlikelihood
estimation.
INTRODUCTION
Learningproblemsinvolvingsequentiallystructureddatacannotbee(cid:11)ectivelydealt
withstaticmodelssuchasfeedforwardnetworks.Recurrentnetworksallowtomodel
complexdynamicalsystemsandcanstoreandretrievecontextualinformationin
a(cid:13)exibleway.Upuntilthepresenttime,researche(cid:11)ortsofsupervisedlearning
forrecurrentnetworkshavealmostexclusivelyfocusedonerrorminimizationby
gradientdescentmethods.Althoughe(cid:11)ectiveforlearningshorttermmemories,
practicaldi(cid:14)cultieshavebeenreportedintrainingrecurrentneuralnetworksto
performtasksinwhichthetemporalcontingenciespresentintheinput/output
sequencesspanlongintervals(Bengioetal.,		;Mozer,		).
Previousworkonalternativetrainingalgorithms(Bengioetal.,		)couldsuggest
thattherootoftheproblemliesintheessentiallydiscretenatureoftheprocess
ofstoringinformationforaninde(cid:12)niteamountoftime.Thus,apotentialsolution
istopropagate,backwardintime,targetsinadiscretestatespaceratherthan
di(cid:11)erentialerrorinformation.Extendingpreviouswork(Bengio&Frasconi,		a),
inthispaperweproposeastatisticalapproachtotargetpropagation,basedonthe
EMalgorithm.Weconsideraparametricdynamicalsystemwithdiscretestatesand
weintroduceamodulararchitecture,withsubnetworksassociatedtodiscretestates.
Thearchitecturecanbeinterpretedasastatisticalmodelandcanbetrainedbythe
EMorgeneralizedEM(GEM)algorithms(Dempsteretal.,	),consideringthe
internalstatetrajectoriesasmissingdata.Inthiswaylearningisdecoupledinto
(cid:3)also,AT&TBellLabs,Holmdel,NJ

atemporalcreditassignmentsubproblemandastaticlearningsubproblemthat
consistsof(cid:12)ttingparameterstothenext-stateandoutputmappingsde(cid:12)nedbythe
estimatedtrajectories.InordertoiterativelytuneparameterswiththeEMorGEM
algorithms,thesystempropagatesforwardandbackwardadiscretedistributionover
thenstates,resultinginaproceduresimilartotheBaum-Welchalgorithmused
totrainstandardhiddenMarkovmodels(HMMs)(Levinsonetal.,	).HMMs
howeveradjusttheirparametersusingunsupervisedlearning,whereasweuseEM
inasupervisedfashion.Furthermore,themodelpresentedherecouldbecalled
Input/OutputHMM,orIOHMM,becauseitcanbeusedtolearntomapinput
sequencestooutputsequences(unlikestandardHMMs,whichlearntheoutput
sequencedistribution).Thismodelcanalsobeseenasarecurrentversionofthe
MixtureofExpertsarchitecture(Jacobsetal.,		),relatedtothemodelalready
proposedin(CacciatoreandNowlan,		).Experimentsonarti(cid:12)cialtasks(Bengio
&Frasconi,		a)haveshownthatEMrecurrentlearningcandealwithlong
termdependenciesmoree(cid:11)ectivelythanbackpropagationthroughtimeandother
alternativealgorithms.However,themodelusedin(Bengio&Frasconi,		a)has
verylimitedrepresentationalcapabilitiesandcanonlymapaninputsequencetoa
(cid:12)naldiscretestate.Inthepresentpaperwedescribeanextendedarchitecturethat
allowstofullyexploitbothinputandoutputportionsofthedata,asrequiredby
thesupervisedlearningparadigm.Inthisway,generalsequenceprocessingtasks,
suchasproduction,classi(cid:12)cation,orprediction,canbedealtwith.
THEPROPOSEDARCHITECTURE
Weconsideradiscretestatedynamicalsystembasedonthefollowingstatespace
description:
xt=f(xt(cid:0);ut)
()
yt=g(xt;ut)
whereutRmistheinputvectorattimet,ytRristheoutputvector,and
xtf;;:::;ngisadiscretestate.Theseequationsde(cid:12)neageneralizedMealy
(cid:12)nitestatemachine,inwhichinputsandoutputsmaytakeoncontinuousvalues.In
thispaper,weconsideraprobabilisticversionofthesedynamics,wherethecurrent
inputsandthecurrentstatedistributionareusedtoestimatethestatedistribution
andtheoutputdistributionforthenexttimestep.Admissiblestatetransitionswill
bespeci(cid:12)edbyadirectedgraphGwhoseverticescorrespondtothemodel'sstates
andthesetofsuccessorsforstatejisSj.
Thesystemde(cid:12)nedbyequations()canbemodeledbytherecurrentarchitecture
depictedinFigure(a).Thearchitectureiscomposedbyasetofstatenetworks
Nj;j=:::nandasetofoutputnetworksOj;j=:::n.Eachoneofthestate
andoutputnetworksisuniquelyassociatedtooneofthestates,andallnetworks
sharethesameinputut.EachstatenetworkNjhasthetaskofpredictingthenext
statedistribution,basedonthecurrentinputandgiventhatxt(cid:0)=j.Similarly,
eachoutputnetworkOjpredictstheoutputofthesystem,giventhecurrentstate
andinput.Allthesubnetworksareassumedtobestaticandtheyarede(cid:12)nedby
meansofsmoothmappingsNj(ut;(cid:18)j)andOj(ut;#j),where(cid:18)jand#jarevectors
ofadjustableparameters(e.g.,connectionweights).Therangesofthefunctions
Nj()maybeconstrainedinordertoaccountfortheunderlyingtransitiongraph
G.Eachoutput'ij;tofthestatesubnetworkNj(attimet)isassociatedtoone
ofthesuccessorsiofstatej.ThusthelastlayerofNjhasasmanyunitsasthe
cardinalityofSj.Forconvenienceofnotation,wesupposethat'ij;tarede(cid:12)nedfor
eachi;j=;:::;nandweimposethecondition'ij;t=foreachinotbelonging
toSj.Thesoftmaxfunctionisusedinthelastlayer:'ij;t=eaij;t=P`Sjea`j;t;j=
iSjwhereaij;tareintermediatevariablesthatcanbethoughtofasthe
;:::;n;

E[ y   |

t

z   =
t1

O

n

HMM

z =
t

delay

=

h =
t

t
1u
E[y   |     ]

t

current input

tu

t

t1

u
,     )
t

u
,     ]
t

O
1

softmax

N
1

softmax

N
n

...

P(x    |

t

u
t
)
1

...

convex
weighted
sum

convex
weighted
sum

P(x      | u

t1

t1
)1

current state distribution

h    =
1,t
x      =1
t1

t
1,t
x      =1
P( x   |

current expected output,
given past input sequence

xt(cid:0)
xt+
xt
yt(cid:0)xt(cid:0)
yt+
yt
xt+
xt
yt(cid:0)
yt
yt+
ut(cid:0)
ut+
ut
(b)
(a)
Figure:(a):TheproposedIOHMMarchitecture.(b):Bottom:Bayesiannetwork
expressingconditionaldependenciesforanIOHMM;top:Bayesiannetworkfora
standardHMM
.activationsoftheoutputunitsofsubnetworkNj.InthiswayPni='ij;t=j;t.
Thevector(cid:16)tRnrepresentstheinternalstateofthemodelanditiscomputedas
alinearcombinationoftheoutputsofthestatenetworks,gatedbythepreviously
computedinternalstate:
(cid:16)t=nXj=(cid:16)j;t(cid:0)'j;t
()
where'j;t=['j;t;:::;'nj;t].Outputnetworkscompetetopredicttheglobal
outputofthesystem(cid:17)tRr:
(cid:17)t=nXj=(cid:16)jt(cid:17)jt
()
where(cid:17)jtRristheoutputofsubnetworkOj.Atthislevel,wedonotneed
tofurtherspecifytheinternalarchitectureofthestateandoutputsubnetworks.
Dependingonthetask,thedesignermaydecidewhethertoincludehiddenlayers
andwhatactivationruletouseforthehiddenunits.
Thisconnectionistarchitecturecanbealsointerpretedasaprobabilitymodel.Let
usassumeamultinomialdistributionforthestatevariablextandletusconsider
(cid:16)t,themainvariableofthetemporalrecurrence().Ifweinitializethevector(cid:16)
topositivenumberssummingto,itcanbeinterpretedasavectorofinitialstate
probabilities.Ingeneral,weobtainrelation(cid:16)it=P(xt=ijut),havingdenoted
withutthesubsequenceofinputsfromtimetot,inclusively.Equation()then
hasthefollowingprobabilisticinterpretation:
P(xt=ijut)=nXj=P(xt=ijxt(cid:0)=j;ut)P(xt(cid:0)=jjut(cid:0)
)
()
i.e.,thesubnetworksNjcomputetransitionprobabilitiesconditionedontheinput
sequenceut:
'ij;t=P(xt=ijxt(cid:0)=j;ut)
()
Asinneuralnetworkstrainedtominimizetheoutputsquarederror,theoutput
(cid:17)tofthisarchitecturecanbeinterpretedasanexpected\positionparameter"
fortheprobabilitydistributionoftheoutputyt.However,inadditiontobeing
conditionalonaninputut,thisexpectationisalsoconditionalonthestatext,i.e.

IOHMM

j
(cid:17)t=E[ytjxt;ut].Theactualformoftheoutputdensity,denotedfY(yt;(cid:17)t),will
bechosenaccordingtothetask.Forexampleamultinomialdistributionissuitable
forsequenceclassi(cid:12)cation,orforsymbolicmutuallyexclusiveoutputs.Instead,a
Gaussiandistributionisadequateforproducingcontinuousoutputs.Inthe(cid:12)rst
caseweuseasoftmaxfunctionattheoutputofsubnetworksOj;inthesecondcase
weuselinearoutputunitsforthesubnetworksOj.
Inordertoreducetheamountofcomputation,weintroduceanindependencymodel
amongthevariablesinvolvedintheprobabilisticinterpretationofthearchitecture.
WeshalluseaBayesiannetworktocharacterizetheprobabilisticdependencies
amongthesevariables.Speci(cid:12)cally,wesupposethatthedirectedacyclicgraph
GdepictedatthebottomofFigurebisaBayesiannetworkforthedependency
modelassociatedtothevariablesuT;xT;yT.Oneofthemostevidentconsequences
ofthisindependencymodelisthatonlythepreviousstateandthecurrentinputare
relevanttodeterminethenext-state.Thisone-stepmemorypropertyisanalogue
totheMarkovassumptioninhiddenMarkovmodels(HMM).Infact,theBayesian
networkforHMMscanbeobtainedbysimplyremovingtheutnodesandarcsfrom
them(seetopofFigureb).
ASUPERVISEDLEARNINGALGORITHM
Thelearningalgorithmfortheproposedarchitectureisderivedfromthemaximum
likelihoodprinciple.ThetrainingdataareasetofPpairsofinput/outputsequences
(oflengthTp):D=f(uTp(p);yTp(p));p=:::Pg.Let(cid:2)denotethevectorof
parametersobtainedbycollectingalltheparameters(cid:18)jand#iofthearchitecture.
Thelikelihoodfunctionisthengivenby
L((cid:2);D)=PYp=P(yTp(p)juTp(p);(cid:2)):
()
Theoutputvalues(usedhereastargets)mayalsobespeci(cid:12)edintermittently.For
example,insequenceclassi(cid:12)cationtasks,onemayonlybeinterestedintheout-
putyTattheendofeachsequence.Themodi(cid:12)cationofthelikelihoodtoaccount
forintermittenttargetsisstraightforward.Accordingtothemaximumlikelihood
principle,theoptimalparametersareobtainedbymaximizing().
Inorderto
applyEMtoourcasewebeginbynotingthatthestatevariablesxtarenotob-
served.Knowledgeofthemodel'sstatetrajectorieswouldallowonetodecompose
thetemporallearningproblemintonstaticlearningsubproblems.Indeed,ifxt
wereknown,theprobabilities(cid:16)itwouldbeeitheroranditwouldbepossible
totraineachsubnetworkseparately,withouttakingintoaccountanytemporalde-
pendency.ThisobservationallowstolinkEMlearningtothetargetpropagation
approachdiscussedintheintroduction.NotethatifweusedaViterbi-likeapproxi-
mation(i.e.,consideringonlythemostlikelypath),wewouldindeedhavenstatic
learningproblemsateachepoch.Inordertowederivethelearningequations,let
usde(cid:12)nethecompletedataasDc=f(uTp(p);yTp(p);xTp(p));p=:::Pg.The
correspondingcompletedatalog-likelihoodis
lc((cid:2);Dc)=PXp=logP(yTp(p);zTp(p)juTp(p);(cid:2)):
()
Sincelc((cid:2);Dc)dependsonthehiddenstatevariablesitcannotbemaximizeddi-
rectly.TheMLEoptimizationisthensolvedbyintroducingtheauxiliaryfunction
Q((cid:2);^(cid:2))anditeratingthefollowingtwostepsfork=;;::::
ComputeQ((cid:2);^(cid:2))=E[lc((cid:2);Dc)(cid:12)(cid:12)D;^(cid:2)]
Estimation:
()
Maximization:Updatetheparametersas^(cid:2) argmax(cid:2)Q((cid:2);^(cid:2))

Theexpectationof()canbeexpressedas
Q((cid:2);^(cid:2))=PXp=TpXt=NXi=^(cid:16)itlogP(ytjxt=i;ut;(cid:2))+NXj=^hij;tlog'ij;t
(	)
wherehij;t=E[zitzj;t(cid:0)(cid:12)(cid:12)uT;yT;(cid:2)],denotingzitforanindicatorvariable=if
xt=iandotherwise.Thehatin^(cid:16)itand^hij;tmeansthatthesevariablesare
computedusingthe\old"parameters^(cid:2).Inordertocomputehij;tweintroduce
theforwardprobabilities(cid:11)it=P(yt;xt=i;ut)andthebackwardprobabilities
(cid:12)it=P(yTtjxt=i;uTt),thatareupdatedasfollows:
(cid:12)it=fY(yt;(cid:17)it)P`'`i(ut+)(cid:12)`;t+
()
(cid:11)it=fY(yt;(cid:17)it)P`'i`(ut)(cid:11)`;t(cid:0):
hij;t=(cid:12)it(cid:11)j;t(cid:0)'ij(ut)
Pi(cid:11)iT
()
EachiterationoftheEMalgorithmrequirestomaximizeQ((cid:2);^(cid:2)).We(cid:12)rst
considerasimpli(cid:12)edcase,
inwhichtheinputsarequantized(i.e.,belonging
toa(cid:12)nitealphabetf(cid:27);:::;(cid:27)Kg)andthesubnetworksbehavelikelookupta-
i.e.weinterpreteachparameteras
blesaddressedbytheinputsymbols(cid:27)t,
wijk=P(xt=ijxt(cid:0)=j;(cid:27)t=k).Forsimplicity,werestricttheanalysistoclas-
si(cid:12)cationtasksandwesupposethattargetsarespeci(cid:12)edasdesired(cid:12)nalstatesfor
eachsequence.Furthermore,nooutputsubnetworksareusedinthisparticular
applicationofthealgorithm.Inthiscaseweobtainthereestimationformulae:
wijk=PPp=Pt:(cid:27)t=k^(cid:12)it^(cid:16)j;t(cid:0)
^(cid:16)x?T;T
()
PiSjPPp=Pt:(cid:27)t=k^(cid:12)it^(cid:16)j;t(cid:0)
^(cid:16)x?T;T:
Ingeneral,however,ifthesubnetworkshavehiddensigmoidalunits,oruseasoft-
maxfunctiontoconstraintheiroutputstosumtoone,themaximumofQcannot
befoundanalytically.InthesecaseswecanresorttoaGEMalgorithm,thatsim-
plyproducesanincreaseinQ,forexamplebygradientascent.Inthiscase,the
derivativesofQwithrespecttotheparameterscanbeeasilycomputedasfollows.
Let(cid:18)jkbeagenericweightinthestatesubnetworkNj.Fromequation(	):
@Q((cid:2);^(cid:2))
=XpXtXi^hij;t'ij;t@'ij;t
()
@(cid:18)jk
@(cid:18)jk
wherethepartialderivatives@'ij;t
@(cid:18)jkcanbecomputedusingbackpropagation.Sim-
ilarly,denotingwith#ikagenericweightoftheoutputsubnetworkOi,wehave:
@Q((cid:2);^(cid:2))
=XpXtX`^(cid:16)i;t@@(cid:17)i`;tlogfY(yy;(cid:17)it)@(cid:17)i`;t
()
@#ik
@#ik
where@(cid:17)i`;t
@#ikarealsocomputedusingbackpropagation.Intuitively,theparameters
areupdatedasiftheestimationstepofEMhadprovidedtargetsfortheoutputsof
thensubnetworks,foreachtimet.AlthoughGEMalgorithmsarealsoguaranteed
to(cid:12)ndalocalmaximumofthelikelihood,theirconvergencemaybesigni(cid:12)cantly
slowercomparedtoEM.Inseveralexperimentswenoticedthatconvergencecanbe
acceleratedwithstochasticgradientascent.

COMPARISONS
Itappearsnaturalto(cid:12)ndsimilaritiesbetweentherecurrentarchitecturedescribed
sofarandstandardHMMs(Levinsonetal.,	).Thearchitectureproposedinthis
paperdi(cid:11)ersfromstandardHMMsintworespects:computingstyleandlearning.
WithIOHMMs,sequencesareprocessedsimilarlytorecurrentnetworks,e.g.,an
inputsequencecanbesynchronouslytransformedintoanoutputsequence.This
computingstyleisreal-timeandpredictionsoftheoutputsareavailableastheinput
sequenceisbeingprocessed.Thisarchitecturethusallowsonetoimplementallthree
fundamentalsequenceprocessingtasks:production,prediction,andclassi(cid:12)cation.
Finally,transitionprobabilitiesinstandardHMMsare(cid:12)xed,i.e.
statesforma
homogeneousMarkovchain.InIOHMMs,transitionprobabilitiesareconditional
ontheinputandthusdependontime,resultinginaninhomogeneousMarkovchain.
Consequently,thedynamicsofthesystem(speci(cid:12)edbythetransitionprobabilities)
arenot(cid:12)xedbutareadaptedintimedependingontheinputsequence.
Theotherfundamentaldi(cid:11)erenceisinthelearningprocedure.Whileinteresting
fortheircapabilitiesofmodelingsequentialphenomena,amajorweaknessofstan-
dardHMMsistheirpoordiscriminationpowerduetounsupervisedlearning.An
approachthathasbeenfoundusefultoimprovediscriminationinHMMsisbased
onmaximummutualinformation(MMI)training.Ithasbeenpointedoutthat
supervisedlearninganddiscriminantlearningcriterialikeMMIareactuallystrictly
related(Bridle,		).Althoughtheparameteradjustingprocedurewehavede(cid:12)ned
isbasedonMLE,yTisusedasdesiredoutputinresponsetotheinputuT,resulting
indiscriminantsupervisedlearning.Finally,itisworthmentioningthatanumber
ofhybridapproacheshavebeenproposedtointegrateconnectionistapproachesinto
theHMMframework.Forexamplein(Bengioetal.,		)theobservationsused
bytheHMMaregeneratedbyafeedforwardneuralnetwork.
In(Bourlardand
Wellekens,		)afeedforwardnetworkisusedtoestimatestateprobabilities,con-
ditionaltotheacousticsequence.Acommonfeatureofthesealgorithmsandthe
oneproposedinthispaperisthatneuralnetworksareusedtoextracttemporally
localinformationwhereasaMarkoviansystemintegrateslong-termconstraints.
WecanalsoestablishalinkbetweenIOHMMsandadaptivemixturesofexperts
(ME)(Jacobsetal.,		).Recently,Cacciatore&Nowlan(		)haveproposeda
recurrentextensiontotheMEarchitecture,calledmixtureofcontrollers(MC),in
whichthegatingnetworkhasfeedbackconnections,thusallowingtotaketemporal
contextintoaccount.OurIOHMMarchitecturecanbeinterpretedasaspecialcase
oftheMCarchitecture,inwhichthesetofstatesubnetworksplaytheroleofa
gatingnetworkhavingamodularstructureandsecondorderconnections.
REGULARGRAMMARINFERENCE
Inthissectionwedescribeanapplicationofourarchitecturetotheproblemof
grammaticalinference.Inthistaskthelearnerispresentedasetoflabeledstrings
andisrequestedtoinferasetofrulesthatde(cid:12)neaformallanguage.Itcanbe
consideredasaprototypeformorecomplexlanguageprocessingproblems.However,
eveninthe\simplest"case,i.e.
regulargrammars,thetaskcanbeprovedto
beNP-complete(AngluinandSmith,	).Wereportexperimentalresultson
asetofregulargrammarsintroducedbyTomita(	)andafterwardsusedby
otherresearcherstomeasuretheaccuracyofinferencemethodsbasedonrecurrent
networks(Gilesetal.,		;Pollack,		;WatrousandKuhn,		).
Weusedascalaroutputwithsupervisiononthe(cid:12)naloutputyTthatwasmodeled
asaBernoullivariablefY(yT;(cid:17)T)=(cid:17)yTT((cid:0)(cid:17)T)(cid:0)yT,withyT=ifthestring
isrejectedandyT=ifitisaccepted.
Inthisapplicationwedidnotapply

Table:SummaryofexperimentalresultsonthesevenTomita'sgrammars.
Convergence
Grammar
Sizes
Accuracies
n?
FSAmin
AverageWorst
BestW&KBest



.
.
.
.
.



.
.
.
.
.	
.



.
.
.
.
.



.
.
.
.
.



.
.
.
.
.



.
.
.
.
.



.
.
.
.
externalinputstotheoutputnetworks.ThiscorrespondstomodelingaMoore
(cid:12)nitestatemachine.Giventheabsenceofpriorknowledgeaboutplausiblestate
paths,weusedanergodictransitiongraph(i.e.,fullyconnected).Intheexperiments
wemeasuredconvergenceandgeneralizationperformanceusingdi(cid:11)erentsizesfor
therecurrentarchitecture.Foreachsettingwerantrialswithdi(cid:11)erentseeds
fortheinitialweights.Weconsideredatrialsuccessfulifthetrainednetworkwas
abletocorrectlylabelallthetrainingstrings.Themodelsizewaschosenusinga
cross-validationcriterionbasedonperformanceonrandomlygeneratedstrings
oflengthT(cid:20).Forcomparison,inTablewealsoreportforeachgrammar
thenumberofstatesoftheminimalrecognizingFSA(Tomita,	).Wetested
thetrainednetworksonacorpusof(cid:0)binarystringsoflengthT(cid:20).The
(cid:12)nalresultsaresummarizedinTable.Thecolumn\Convergence"reportsthe
fractionoftrialsthatsucceededtoseparatethetrainingset.Thenextthreecolumns
reportaveragesandorderstatistics(worstandbesttrial)ofthefractionofcorrectly
classi(cid:12)edstrings,measuredonthesuccessfultrials.Foreachgrammartheseresults
refertothemodelsizen?selectedbycross-validation.Generalizationwasalways
perfectongrammars,,and.Foreachgrammar,thebesttrialalsoattained
perfectgeneralization.Theseresultscompareveryfavorablytothoseobtainedwith
second-ordernetworkstrainedbygradientdescent,whenusingthelearningsets
proposedbyTomita.Forcomparison,inthelastcolumnofTablewereproduce
theresultsreportedbyWatrous&Kuhn(		)inthebestof(cid:12)vetrials.Inmost
ofthesuccessfultrialsthemodellearnedanactualFSAbehaviorwithtransition
probabilitiesasymptoticallyconvergingeithertoorto.Thisrenderstrivialthe
extractionofthecorrespondingFSA.Indeed,forgrammars,,,and,wefound
thatthetrainednetworksbehaveexactlyliketheminimalrecognizingFSA.
Apotentialtrainingproblemisthepresenceoflocalmaximainthelikelihoodfunc-
tion.Forexample,thenumberofconvergedtrialsforgrammars,,andisquite
smallandthedi(cid:14)cultyofdiscoveringtheoptimalsolutionmightbecomeaserious
restrictionfortasksinvolvingalargenumberofstates.Inotherexperiments(Ben-
gio&Frasconi,		a),wenoticedthatrestrictingtheconnectivityofthetransition
graphcansigni(cid:12)cantlyhelptoremoveproblemsofconvergence.Ofcourse,thisap-
proachcanbee(cid:11)ectivelyexploitedonlyifsomepriorknowledgeaboutthestate
spaceisavailable.Forexample,applicationsofHMMstospeechrecognitionalways
relyonstructuredtopologies.
CONCLUSIONS
Therearestillanumberofopenquestions.Inparticular,thee(cid:11)ectivenessofthe
modelontasksinvolvinglargeorverylargestatespacesneedstobecarefullyeval-
uated.In(Bengio&Frasconi		b)weshowthatlearninglongtermdependencies
inthesemodelsbecomesmoredi(cid:14)cultasweincreasetheconnectivityofthestate

transitiongraph.However,becausetransitionprobabilitiesofIOHMMschangeat
eacht,theydealbetterwiththisproblemoflong-termdependenciesthanstandard
HMMs.Anotherinterestingaspecttobeinvestigatedisthecapabilityofthemodel
tosuccessfullyperformtasksofsequenceproductionorprediction.Forexample,
interestingtasksthatcouldalsobeapproachedarethoserelatedtotimeseries
modelingandmotorcontrollearning.
