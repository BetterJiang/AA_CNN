Gradient Flow in Recurrent Nets: the

Diﬃculty of Learning Long-Term

Dependencies

Sepp Hochreiter

Fakult¨at f¨ur Informatik

Technische Universit¨at M¨unchen

80290 M¨unchen, Germany

hochreit@informatik.tu-muenchen.de

Yoshua Bengio

Dept. Informatique et Recherche Op´erationnelle

Universit´e de Montr´eal, CP 6128, Succ. Centre-Ville,

Montr´eal, Qu´ebec, Canada, H3C 3J7

bengioy@iro.umontreal.ca

Paolo Frasconi

Dept. of Systems and Computer Science

University of Florence

via di Santa Marta 3, 50139 Firenze (Italy)

paolo@dsi.unifi.it

J¨urgen Schmidhuber

IDSIA

Corso Elvezia 36

6900 Lugano, Switzerland

juergen@idsia.ch

1

1 Introduction

Recurrent networks (crossreference Chapter 12) can, in principle, use their
feedback connections to store representations of recent input events in the
form of activations. The most widely used algorithms for learning what to
put in short-term memory, however, take too much time to be feasible or
do not work well at all, especially when minimal time lags between inputs
and corresponding teacher signals are long. Although theoretically fascinat-
ing, they do not provide clear practical advantages over, say, backprop in
feedforward networks with limited time windows (see crossreference Chap-
ters 11 and 12). With conventional “algorithms based on the computation of
the complete gradient”, such as “Back-Propagation Through Time” (BPTT,
e.g., [23, 28, 27]) or “Real-Time Recurrent Learning” (RTRL, e.g., [22]) error
signals “ﬂowing backwards in time” tend to either (1) blow up or (2) vanish:
the temporal evolution of the backpropagated error exponentially depends
on the size of the weights [12, 6]. Case (1) may lead to oscillating weights,
while in case (2) learning to bridge long time lags takes a prohibitive amount
of time, or does not work at all.

In what follows, we give a theoretical analysis of this problem by study-
ing the asymptotic behavior of error gradients as a function of time lags. In
Section 2, we consider the case of standard RNNs and derive the main result
using the approach ﬁrst proposed in [12]. In Section 3, we consider the more
general case of adaptive dynamical systems, which include, besides standard
RNNs, other recurrent architectures based on diﬀerent connectivities and
choices of the activation function (e.g., RBF or second order connections).
Using the analysis reported in [6] we show that one of the following two un-
desirable situations necessarily arise: either the system is unable to robustly
store past information about its inputs, or gradients vanish exponentially.
Finally, in Section 4 we shortly review alternative optimization methods and
architectures that have been suggested to improve learning in the presence
of long-term dependencies.

2 Exponential error decay

Gradients of the error function

The results we are going to prove hold regardless of the particular kind of
cost function used (as long as its continuous in the output) and regardless of

2

the particular algorithm which is employed to compute the gradient. Here we
shortly explain how gradients are computed by the standard BPTT algorithm
(e.g., [28], see also crossreference Chapter 14 for more details) because its
analytical form is better suited to the forthcoming analyses.

The error at time t is denoted by E(t). Considering only the error at

time t, output unit k’s error signal is

δk(t) =

∂E(t)
∂netk(t)

and some unit j’s backpropagated error signal at time τ < t is

(cid:2)(cid:3)

(cid:4)

δj(τ ) = f(cid:2)

j(netj(τ ))

wij δi(τ + 1)

,

where

i

wij aj(τ − 1)

(cid:3)

j

neti(τ ) =

is unit i’s current net input,

ai(τ ) = fi(neti(τ ))

is the activation of a non-input unit i with diﬀerentiable transfer function fi,
and wij is the weight on the connection from unit j to i. The corresponding
contribution at time τ < t to wjl’s total weight update is η δj(τ ) al(τ − 1),
where η is the learning rate, and l stands for an arbitrary unit connected to
unit j.

Error path integral

Suppose we have a fully connected net whose non-input unit indices range
from 1 to n. Let us focus on local error ﬂow from output unit k to arbitrary
unit v (later we will see that the analysis immediately extends to global error
ﬂow). The error occurring at k at time step t is propagated “back in time”
for t − s time steps, to an arbitrary unit v at time s < t. This scales the
error by the following factor:
v(netv(t − 1)) wkv
f(cid:2)

(cid:5)

(cid:8)

t − s = 1
t − s > 1

.

(1)

∂δv(s)
∂δk(t)

=

f(cid:2)
v(netv(s))

∂δl(s+1)

∂δk(t)

l=1

wlv

(cid:6)(cid:7)n

3

In order to solve the above recurrence, we will expand it by unrolling over
time (as done for example in deriving BPTT). In particular, for s < τ < t let
lτ denote the index of a generic non input unit in the replica of the network
at time τ . Moreover, let ls = v and lt = k. We obtain:

⎞
⎠ f(cid:2)

⎞
⎠

f(cid:2)
lτ (netlτ (τ )) wlτ lτ−1

ls(netls(s))

n(cid:3)

. . .

n(cid:3)

lt−1=1

ls+1=1

⎛
⎝wltlt−1

⎛
⎝ s+1(cid:11)

τ =t−1

∂δv(s)
∂δk(t)

=

(proof by induction).

It can be immediately shown that if the local error vanishes, then the

(2)

global error vanishes too. To see this compute

(cid:3)

k∈O

∂δv(s)
∂δk(t)

where O denotes the set of output units.

(cid:14)(cid:14)(cid:14)f(cid:2)

(cid:14)(cid:14)(cid:14) > 1.0

Intuitive explanation of equation (2)

lτ (netlτ (τ )) wlτ lτ−1

If
for all τ then the largest product increases exponentially with t− s− 1. That
is, the error blows up, and conﬂicting error signals (with opposite signs)
arriving at unit v can lead to oscillating weights and unstable learning due
to “overshooting” (for error blow-ups or bifurcations see also [20, 2, 9]). On
the other hand, if
for all τ , then the largest product decreases exponentially with t − s − 1.
That is, the error vanishes, and nothing can be learned in acceptable time.
(cid:14)(cid:14)(cid:14)f(cid:2)
lτ is
0.25. If alτ−1 is constant and not equal to zero, then the size of the gradient

If flτ is the logistic sigmoid function, then the maximal value of f(cid:2)

(cid:14)(cid:14)(cid:14) takes on maximal values where

(cid:14)(cid:14)(cid:14) < 1.0

lτ (netlτ (τ )) wlτ lτ−1

lτ (netlτ )wlτ lτ−1

(cid:14)(cid:14)(cid:14)f(cid:2)

(cid:16)
(cid:14)(cid:14)(cid:14) → ∞, and it is less than 1.0
(cid:14)(cid:14)(cid:14) < 4.0 (e.g., if the absolute maximal weight value wmax is smaller

(cid:15)
(cid:14)(cid:14)(cid:14)wlτ lτ−1

wlτ lτ−1 =

netlτ

alτ−1

coth

1
2

1

the size of the derivative goes to zero for

(cid:14)(cid:14)(cid:14)wlτ lτ−1

for

,

4

than 4.0). Hence with conventional logistic sigmoid transfer functions, the
error ﬂow tends to vanish as long as the weights have absolute values below
4.0, especially in the beginning of the training phase. In general the use of
larger initial weights does not help though — as seen above, for
the relevant derivative goes to zero “faster” than the absolute weight can
grow (also, some weights may have to change their signs by crossing zero).
Likewise, increasing the learning rate does not help either — it does not
change the ratio of long-range error ﬂow and short-range error ﬂow. BPTT
is too sensitive to recent distractions. Note that since the summation terms
in equation (2) may have diﬀerent signs, increasing the number of units n
does not necessarily increase error ﬂow.

(cid:14)(cid:14)(cid:14)wlτ lτ−1

(cid:14)(cid:14)(cid:14) → ∞

Weak upper bound for scaling factor

The following, slightly extended vanishing error analysis also takes n, the
number of units, into account. For t− s > 1, formula (2) can be rewritten as

⎛
⎝ s+1(cid:11)

τ =t−2

⎞
⎠ Wv f(cid:2)

(cid:8)T

(cid:6)

W T
k

F (cid:2)

(t − 1)

W F (cid:2)

(τ )

v (netv(s)) ,

k ]i := [W ]ki = wki, and F (cid:2)

where the weight matrix W is deﬁned by [W ]ij := wij, v’s outgoing weight
vector Wv is deﬁned by [Wv]i := [W ]iv = wiv, k’s incoming weight vector W T
k
is deﬁned by [W T
(t) is the diagonal matrix of ﬁrst
order derivatives deﬁned as: [F (cid:2)
i(neti(t))
otherwise. Here T is the transposition operator, [A]ij is the element in the
i-th column and j-th row of matrix A, and [x]i is the i-th component of
vector x.
Using a matrix norm (cid:5).(cid:5)A compatible with vector norm (cid:5).(cid:5)x, we deﬁne

(t)]ij := 0 if i (cid:4)= j, and [F (cid:2)

(t)]ij := f(cid:2)

For maxi=1,...,n{|xi|} ≤ (cid:5)x(cid:5)x we get

{(cid:5)F (cid:2)

τ =t−1,...,s

f(cid:2)
max := max

(τ )(cid:5)A}.
(cid:14)(cid:14)(cid:14)xT y
(cid:14)(cid:14)(cid:14) ≤ n (cid:5)x(cid:5)x (cid:5)y(cid:5)x. Since
|f(cid:2)
(s)(cid:5)A ≤ f(cid:2)
v(netv(s))| ≤ (cid:5)F (cid:2)

max,

we obtain the following inequality:

(cid:14)(cid:14)(cid:14)(cid:14)(cid:14) ∂δv(s)

∂δk(t)

(cid:14)(cid:14)(cid:14)(cid:14)(cid:14) ≤ n (f(cid:2)

max)t−s (cid:5)Wv(cid:5)x (cid:5)W T

k (cid:5)x (cid:5)W(cid:5)t−s−2

A

≤ n (f(cid:2)

max (cid:5)W(cid:5)A)

t−s .

5

This inequality results from

(cid:5)Wv(cid:5)x = (cid:5)W ev(cid:5)x ≤ (cid:5)W(cid:5)A (cid:5)ev(cid:5)x ≤ (cid:5)W(cid:5)A

and

(cid:5)W T

k (cid:5)x = (cid:5)ekW(cid:5)x ≤ (cid:5)W(cid:5)A (cid:5)ek(cid:5)x ≤ (cid:5)W(cid:5)A,

where ek is the unit vector whose components are 0 except for the k-th
component, which is 1. Note that this is a weak, extreme case upper bound
(τ )(cid:5)A take on maximal values, and if the
— it will be reached only if all (cid:5)F (cid:2)
contributions of all paths across which error ﬂows back from unit k to unit
v have the same sign. Large (cid:5)W(cid:5)A, however, typically result in small values
of (cid:5)F (cid:2)

(τ )(cid:5)A, as conﬁrmed by experiments (see, e.g., [12]).

For example, with norms

(cid:5)W(cid:5)A := maxr

(cid:3)

s

|wrs|

and

we have f(cid:2)

max = 0.25 for the logistic sigmoid. We observe that if

(cid:5)x(cid:5)x := maxr

|xr| ,

|wij| ≤ wmax < 4.0
n

∀i, j,

(cid:6)

(cid:8)

then (cid:5)W(cid:5)A ≤ n wmax < 4.0 will result in exponential decay; by setting
λ :=

nwmax

4.0

< 1.0, we obtain(cid:14)(cid:14)(cid:14)(cid:14)(cid:14)∂δv(s)

∂δk(t)

(cid:14)(cid:14)(cid:14)(cid:14)(cid:14) ≤ n λt−s.

We refer to Hochreiter’s thesis [12] for more details.

3 Dilemma: Avoiding gradient decay prevents

long-term latching

In Bengio et al.’s paper [6], the analysis of the problem of gradient decays
is generalized to parameterized dynamical systems (hence including second
order and other recurrent architectures). The main theorem shows that a

6

suﬃcient condition to obtain gradient decay is also a necessary condition for
the system to robustly store discrete state information for the long-term. In
other words, when the weights and the state trajectory are such that the
network can “latch” on information in its hidden units (i.e., represent long-
term dependencies), the problem of gradient decay is obtained. When the
long-term gradients decay exponentially, it is very diﬃcult to learn such long-
term dependencies because the total gradient is the sum of long-term and
short-term inﬂuences and the short-term inﬂuences then completely dominate
the gradient.

This result is based on a decomposition of the state-space of hidden units
in two types of regions: one where gradients decay and one where it is not
possible to robustly latch information. Let y(t) denote the n-dimensional
state vector at time t (for example, the vector [net1(t), . . . , netn(t)] when
considering a standard ﬁrst-order recurrent network) and let y(t) = M(y(t−
1)) be the map from the state at time t− 1 to t for the autonomous (without
inputs) dynamical system. The above decomposition is expressed in terms
of the condition |M(cid:2)| > 1 (no robust latching possible) or |M(cid:2)| < 1 (gradient
decay), where |M(cid:2)| is the norm of the Jacobian (matrix of partial derivatives)
of the map M. The analysis focuses on the basins of attraction of attractors of
M in the domain of y(t) (or manifolds within that domain). In particular, the
analysis is concerned with so-called hyperbolic attractors, which are locally
stable (but need not be ﬁxed points) and where the eigenvalues of M(cid:2)
are
less than 1 in absolute value. If the state (or a function of it) remains within
a certain region of space (versus another region) even in the presence of
perturbations (such as noise in the inputs) then it is possible to store at least
one bit of information for arbitrary durations.
In regions where |M(cid:2)| > 1 it can be shown that arbitrarily small pertur-
bations (for example due to the inputs) can eventually kick the state out of
a basin of attraction [19] (see the sample trajectory on the right of Figure
1). In regions where |M(cid:2)| < λ < 1 there is a level of perturbation (depend-
ing on λ) below which the state will remain in the basin of attraction (and
will gradually get closer to a certain volume around the attractor — see left
of Figure 1). For this reason we call this condition “information latching,”
since it allows to store discrete information for arbitrary duration in the state
variable y(t).
Unfortunately, in the regions where |M(cid:2)| < 1 (where one can latch in-
formation) one can also show that gradients decay. The argument is similar
to the one developed in the previous section. The partial derivative of y(t)

7

|M(cid:2)| > 1

|M(cid:2)| > 1

y

y(t)
|M(cid:2)| < 1

|M(cid:2)| < 1

y

y(t)

Figure 1: Robust latching. For simplicity a ﬁxed-point attractor y is shown.
The shadow region is the basin of attraction. The dark shadow region is the
subset where |M(cid:2)| < 1 and robust latch occurs. See text for details.

with respect to y(s) with s < t is simply the product of the map derivatives
between s and t:

∂y(t)
∂y(s)

=

∂y(t − 1)
∂y(t − 2)

∂y(t)
∂y(t − 1)
∂y(τ−1) is equal to the W F (cid:2)

. . .

∂y(τ )

∂y(s + 1)

.

∂y(s)

(τ ) in previous formulas
(Using neural networks
— compare ﬁrst equation of subsection on weak upper bounds.) When the
norm of each of the factors on the right hand side is less than 1, the left hand
side converges exponentially fast to zero as t− s increases. The eﬀect of this
decay of gradients can be made explicit as follows:
∂E(t)
∂y(t)
Hence for a term of the sum with τ (cid:8) t, we have

∂E(t)
∂W =

∂y(τ )
∂W =

∂E(t)
∂y(τ )

∂y(t)
∂y(τ )

∂y(τ )
∂W

(cid:3)

τ≤t

.

τ≤t

(cid:3)
(cid:14)(cid:14)(cid:14)(cid:14)(cid:14) → 0.

(cid:14)(cid:14)(cid:14)(cid:14)(cid:14) ∂E(t)

∂y(τ )

∂y(τ )
∂W

This term tends to become very small in comparison to terms for which τ is
close to t. This means that even though there might exist a change in W that
would allow y(τ ) to jump to another (better) basin of attraction, the gradient
of the cost with respect to W does not clearly reﬂect that possibility. The
explanation is that the eﬀect of a small change in W would be felt mostly
on the near past (τ close to t).

8

4 Remedies

The above theoretical investigations indicate a basic limitation of gradient
descent as a search procedure for ﬁnding optimal weights in a RNN. Several
proposals have been made to cope with the problem of long-term dependen-
cies, some attempting to solve the optimization problem using alternative
search algorithms, other trying to devise alternative architectures.
In the
following we give a brief accounts of these proposals.

Time constants

To deal with long time lags, Mozer [18] uses time constants inﬂuencing
changes of unit activations (deVries and Principe’s related approach [8] may
be viewed as a mixture of time-delay neural networks (TDNN) [15] and time
constants). For long time lags, however, the time constants need external
ﬁne tuning [18]. Sun et al.’s alternative approach [26] updates the activa-
tion of a recurrent unit by adding the old activation and the (scaled) current
net input. The net input, however, tends to perturb the stored information,
which makes long-term storage impractical. Lin et al. [17] also propose vari-
ants of time-delay networks, called NARX networks (crossreference see also
Chapter 11). Gradient ﬂow in this architecture can be improved because em-
bedded memories eﬀectively introduce “shortcuts” in the error propagation
path through time. The same idea can be applied to other architectures, by
inserting multiple delays in the connections among hidden state units rather
than output units [16]. However, these architectures cannot solve the general
problem since they can only increase by a constant multiplicative factor the
duration of the temporal dependencies that can be learned. Finally, El Hihi
& Bengio [10] looked at hierarchically organized recurrent neural networks
with diﬀerent levels of time-constants or time-delays.

Ring’s approach

Ring [21] also proposes a method for bridging long time lags. Whenever a
unit in his network receives conﬂicting error signals, that is, certain error
signals suggest to increase the unit’s activity while others suggest otherwise,
he adds a higher order unit inﬂuencing appropriate connections. Although
his approach can sometimes be extremely fast, to bridge a time lag involving
100 steps may require the addition of 100 units. Also, Ring’s net does not

9

generalize to unseen lag durations.

Searching without gradients

The diﬃculty of learning long-term dependencies is strictly related to the
continuous optimization approach that guides the search for a weight solu-
tion. One possibility for avoiding the problem is to resort to other kinds of
search in weight space, in which the operators for generating another candi-
date weight solution are not based on continuous gradients. Bengio et al. [6]
investigate methods such as simulated annealing, multi-grid random search,
and discrete error propagation. Angeline et al.
[1] (see also crossreference
Chapter 15) propose a genetic approach that also avoids gradient computa-
tion.

The simplest kind of search without gradient, however, simply randomly
initializes all network weights until the resulting net happens to classify all
training sequences correctly. In fact, as discussed in crossreference Chapter
9 of this book, simple weight guessing solves several popular benchmarks de-
scribed in previous work faster than the recurrent net algorithms proposed
therein (compare [14]). This does not mean that weight guessing is a good
algorithm. It just means that the problems are very simple. More realis-
tic tasks require either many free parameters (e.g., input weights) or high
weight precision (e.g., for continuous-valued parameters), such that guessing
becomes completely infeasible. Currently it is unclear to which extent more
complex gradient-less methods can improve upon guessing in case of more
realistic tasks.

Probabilistic target propagation

Bengio and Frasconi [4] propose a probabilistic approach for propagating
targets. With n so-called “state networks”, at a given time, their system can
be in one of only n diﬀerent discrete states. Parameters are adjusted using
the expectation-maximization algorithm. But to solve problems that require
a signiﬁcant amount of memory to store contextual information, such systems
would require an unacceptable number of states (i.e., state networks).

Adaptive sequence chunkers

Schmidhuber’s hierarchical chunker systems [24, 25] can in principle bridge
arbitrary time lags, but only if there is local predictability across the sub-

10

sequences causing the time lags (see also [18]). For instance, in his post-
doctoral thesis [25], Schmidhuber uses hierarchical recurrent networks with
self-organizing time scales to rapidly solve certain grammar learning tasks
involving minimal time lags in excess of 1000 steps. The performance of
chunker systems, however, deteriorates as the noise level increases and the
input sequences become less compressible.

Long Short-Term Memory

There is a novel, eﬃcient, gradient-based method called “Long Short-Term
Memory” (LSTM) [13]. LSTM is designed to get rid of the vanishing error
problem. Truncating the gradient where this does not do harm, LSTM can
learn to bridge minimal time lags in excess of 1000 discrete time steps by en-
forcing constant error ﬂow through “constant error carrousels” within special
units. Multiplicative gate units learn to open and close access to the constant
error ﬂow. LSTM is local in space and time; its computational complexity
per time step and weight is O(1). So far, experiments with artiﬁcial data
involved local, distributed, real-valued, and noisy pattern representations.
In comparisons with RTRL, BPTT, Recurrent Cascade-Correlation, Elman
networks, and Neural Sequence Chunking, LSTM led to many more success-
ful runs, and learned much faster. LSTM also solved complex, artiﬁcial long
time lag tasks that have never been solved by previous recurrent network
algorithms. It will be interesting to examine to which extent LSTM is appli-
cable to real world problems such as language separation from prosody (ﬁrst
results in [7]) and speech recognition.

5 Conclusions

In principle, RNNs represent the most general and powerful sequence process-
ing method. For instance, unlike Hidden Markov Models (HMMs, the most
successful technique in several applications - see [3] for a review) they are not
limited to discrete internal states but allow for continuous, distributed se-
quence representations. Hence they can solve tasks no other current method
can solve (e.g., [11]). The problem of vanishing gradients, however, makes
conventional RNNs hard to train. We suspect this is why feedforward neu-
ral networks outnumber RNNs in terms of successful real-world applications.
Some of the remedies outlined in this chapter may lead to more eﬀective

11

learning systems. However, long lime lag research still seems to be in an
early stage — no commercial applications of any of these methods have been
reported so far.

Long time lags pose problems to any soft computing method, not just
RNNs. For instance, when dealing with long sequences (e.g., speech or bio-
logical data), HMMs mostly rely on localized representation of time by means
of highly constrained non ergodic transition diagrams (diﬀerent states are de-
signed for diﬀerent portions of a sequence). Belief propagation over long time
lags does not eﬀectively occur, a phenomenon called diﬀusion of credit [5],
which closely resembles the vanishing gradients problem in RNNs.

References

[1] P. J. Angeline, G. M. Saunders, and J. P. Pollack. An evolutionary
algorithm that constructs recurrent neural networks. IEEE Transactions
on Neural Networks, 5(1):54–65, 1994.

[2] P. Baldi and F. Pineda. Contrastive learning and neural oscillator. Neu-

ral Computation, 3:526–545, 1991.

[3] Y. Bengio. Markovian models for sequential data. Neural Computing

Surveys, 2:129–162, 1999.

[4] Y. Bengio and P. Frasconi. Credit assignment through time: Alterna-
tives to backpropagation. In J. D. Cowan, G. Tesauro, and J. Alspector,
editors, Advances in Neural Information Processing Systems 6, pages
75–82. San Mateo, CA: Morgan Kaufmann, 1994.

[5] Y. Bengio and P. Frasconi. Diﬀusion of context and credit information
in Markovian models. Journal of Artiﬁcial Intelligence Research, 3:249–
270, 1995.

[6] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependen-
IEEE Transactions on Neural

cies with gradient descent is diﬃcult.
Networks, 5(2):157–166, 1994.

[7] F. Cummins, F. A. Gers, and J. Schmidhuber. Language identiﬁca-
In Proceedings of EU-

tion from prosody without explicit features.
ROSPEECH’99, volume 1, pages 371–374, 1999.

12

[8] B. de Vries and J. C. Principe. A theory for neural networks with time
delays. In R. P. Lippmann, J. E. Moody, and D. S. Touretzky, editors,
Advances in Neural Information Processing Systems 3, pages 162–168.
San Mateo, CA: Morgan Kaufmann, 1991.

[9] K. Doya. Bifurcations in the learning of recurrent neural networks.
In Proceedings of 1992 IEEE International Symposium on Circuits and
Systems, pages 2777–2780, 1992.

[10] S. El Hihi and Y. Bengio. Hierarchical recurrent neural networks for
long-term dependencies. In D. S. Touretzky, M. C. Mozer, and M. E.
Hasselmo, editors, Advances in Neural Information Processing Systems
8, pages 493–499. MIT Press, Cambridge MA, 1996.

[11] F. A. Gers, J. Schmidhuber, and F. Cummins. Learning to forget: Con-
tinual prediction with lstm. In Proc. ICANN’99, Int. Conf. on Artiﬁcial
Neural Networks, pages 850–855, Edinburgh, Scotland, 1999. IEE, Lon-
don. Long version to appear in Neural Computation.

[12] S. Hochreiter.

Untersuchungen zu dynamischen neuronalen Net-
zen. Diploma thesis, Institut f¨ur Informatik, Lehrstuhl Prof. Brauer,
Technische Universit¨at M¨unchen, 1991.
See www7.informatik.tu-
muenchen.de/˜hochreit.

[13] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural

Computation, 9:1681–1726, 1997.

[14] S. Hochreiter and J. Schmidhuber. LSTM can solve hard long time
lag problems. In M. C. Mozer, M. I. Jordan, and T. Petsche, editors,
Advances in Neural Information Processing Systems 9, pages 473–479.
MIT Press, Cambridge MA, 1997.

[15] K. Lang, A. Waibel, and G. E. Hinton. A time-delay neural network
architecture for isolated word recognition. Neural Networks, 3:23–43,
1990.

[16] T. Lin, B. G. Horne, and C. L. Giles. How embedded memory in re-
current neural network architectures helps learning long-term temporal
dependencies. Neural Networks, 11(5):861–868, 1998.

13

[17] T. Lin, B. G. Horne, P. Ti˜no, and C. L. Giles. Learning long-term
dependencies in NARX recurrent neural networks. IEEE Transactions
on Neural Networks, 7(6):1329–1338, November 1996.

[18] M. C. Mozer.

Induction of multiscale temporal structure.

In D. S.
Lippman, J. E. Moody, and D. S. Touretzky, editors, Advances in Neu-
ral Information Processing Systems 4, pages 275–282. San Mateo, CA:
Morgan Kaufmann, 1992.

[19] J.M. Ortega and W.C. Rheinboldt.

Iterative Solution of Non-linear
Equations in Several Variables and Systems. Academic Press, New York,
1970.

[20] F. J. Pineda. Dynamics and architecture for neural computation. Jour-

nal of Complexity, 4:216–245, 1988.

[21] M. B. Ring. Learning sequential tasks by incrementally adding higher
orders. In J. D. Cowan S. J. Hanson and C. L. Giles, editors, Advances
in Neural Information Processing Systems 5, pages 115–122. Morgan
Kaufmann, 1993.

[22] A. J. Robinson and F. Fallside. The utility driven dynamic error propa-
gation network. Technical Report CUED/F-INFENG/TR.1, Cambridge
University Engineering Department, 1987.

[23] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning internal
representations by error propagation. In Parallel Distributed Processing,
volume 1, pages 318–362. MIT Press, 1986.

[24] J. Schmidhuber. Learning complex, extended sequences using the prin-
ciple of history compression. Neural Computation, 4(2):234–242, 1992.

[25] J. Schmidhuber. Netzwerkarchitekturen, Zielfunktionen und Ketten-
regel. Habilitationsschrift, Institut f¨ur Informatik, Technische Univer-
sit¨at M¨unchen, 1993.

[26] G. Sun, H. Chen, and Y. Lee. Time warping invariant neural networks.
In J. D. Cowan S. J. Hanson and C. L. Giles, editors, Advances in
Neural Information Processing Systems 5, pages 180–187. San Mateo,
CA: Morgan Kaufmann, 1993.

14

[27] P. J. Werbos. Generalization of backpropagation with application to a

recurrent gas market model. Neural Networks, 1, 1988.

[28] R. J. Williams and D. Zipser. Gradient-based learning algorithms
In Back-
for recurrent networks and their computational complexity.
propagation: Theory, Architectures and Applications. Hillsdale, NJ: Erl-
baum, 1992.

15

