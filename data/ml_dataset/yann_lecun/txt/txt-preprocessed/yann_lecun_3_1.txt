Abstract: La conception de tr`es grand syst`emes dapprentissage pose un
grand nombre de probl`emes non resolus. Savons nous, par exemple, construire
un algorithme qui regarde la television pendant quelques semaines et apprend
`a enumerer les objets presents dans ces images. Les lois dechelles de nos algo-
rithmes ne nous permettent pas de traiter les quantites massives de donnees que
cela implique. Lexperience sugg`ere que les algorithmes les mieux adaptes sont
les algorithmes stochastiques. Leur convergence est pourtant reputee beaucoup
plus lente que celle des meilleurs algorithmes doptimisation. Mais il sagit de
la convergence vers loptimum empirique. Notre papier reformule la question
en termes de convergence vers le point de meilleure generalisation et montre la
superiorite dun algorithme stochastique bien concu.

Keywords: Learning, Convergence speed, Online learning, Stochastic opti-

misation.

1

Introduction

During the last decade, we have seen a considerable improvement in our the-
oretical understanding of learning systems as statistical machines. The Vapnik
Chervonenkis theory (Vapnik, 1974) has spelled out the role of capacity and gen-
eralization in the design of learning algorithms. This understanding has inuenced
several recent learning algorithms such as support vector machines (Boser, Guyon
and Vapnik, 1992) and boosting (Drucker, Schapire and Simard, 1993), (Freund
and Schapire, 1996). These algorithms challenge the popular curse of dimension-


ality which says that statistical systems with large number of parameters require
impracticably large data sets.

Bridges have been established between learning algorithms and both classical
and Bayesian statistics. We have seen learning algorithms applied to problems
usually associated with statistics. We also have seen a massive application of
statistics to solve learning problems or improve learning algorithms.

Despite these advances, we have yet to see a spectacular increase in the size of
both the data sets and the learning machines. The MNIST data set (Bottou et al.,
1994), for instance, is still described as a relevant benchmark for more recent
algorithms. Systems dealing with more than a few millions examples seldom
compute more than simple counts or histograms. Do we know how to build a
machine which can learn how to enumerate objects in arbitrary scenes using TV
broadcasts as a data source?

The MNIST experiments were carried out on workstations featuring a 40MHz
processor and 32MB of memory. The computer hardware was a clear bottleneck.
Personal computers now feature fty times that speed and memory. Hard disk
technology has progressed even faster. Large data sources are now available, be-
cause audio and video capture hardware is now commonplace, and also because
the development of on-line technologies has provided abundant transaction logs.
This discrepancy indicates that we have reached another bottleneck. Our learn-
ing algorithms do not scale well enough to take advantage of such large datasets
and such large computing resources.

As datasets grow to practically innite sizes, we argue that on-line algorithms
(Bottou and Murata, 2002) outperform traditional learning algorithms that oper-
ate by repetitively sweeping over the entire training set. This paper shows that
performing a single epoch of a suitable on-line algorithm converges to the true
solution of the learning problem asymptotically as fast as any other algorithm.

The rst part of the paper presents the problem and discusses the main re-
sults and their consequences. The second part of the paper provides proofs and
mathematical details.

2 On-line Learning and Batch Learning

Many learning algorithms optimize an empirical cost function
expressed as a very large sum of terms
associated with running a model with parameter vector

that can be
. Each term measures the cost
on independently drawn








examples1

 .





	







(1)

(2)

Two kinds of optimization procedures are often mentioned in connection with this
problem:


Batch gradient: Parameter updates are performed on the basis of the gradi-

ent and Hessian information accumulated over the entire training set:





















where

On-line or stochastic gradient: Parameter updates are performed on the ba-

is an appropriately chosen positive denite symmetric matrix.

sis of a single sample

 picked randomly at each iteration:





where

is again an appropriately chosen positive denite symmetric ma-










 are chosen by cycling over a randomly

(3)

trix. Very often the examples
permuted training set. Each cycle is called an epoch. This paper how-
ever considers situations where the supply of training samples is practically
unlimited. Each iteration of the online algorithm utilizes a fresh sample,
unlikely to have been presented to the system before.

cost, that is to say

Simple batch algorithms converge linearly to the optimum

!
 of the empirical
 ). Careful choices of) make the
convergence super-linear (e.g. like"*#+",- in favorable cases (Dennis and Schnabel,

%$ converges like&('




1983).

"

#

By comparison, on-line algorithms appear to converge very slowly. This con-
vergence has been studied extensively (Benveniste, Metivier and Priouret, 1990;
Bottou, 1998). Under mild assumptions, they are shown to converge almost surely
to a local minimum2 of the cost. However, whereas on-line algorithms may con-
verge to the general area of the optimum at least as fast as batch algorithms (Le
Cun et al., 1998b), stochastic uctuations due to the noisy gradient estimate make

1Each example.0/
2We assume in this paper that the parameters= are conned in the neighborhood of a single

typically is an input/output pair132547698:4<; .

minimum.













































the parameter vector randomly wobble around the optimum in a region whose size
decreases slowly. The analysis shows that

At rst glance, on-line algorithms seem hopelessly slow. However, the above
discussion addresses the convergence toward the minimum of the empirical cost
, whereas one should be much more interested in the convergence toward

$ converges like"*

 at best.



#




the minimum of the expected cost:

















is the unknown probability distribution from which the samples are

(4)

where
drawn (Vapnik, 1974).




The main point of this paper is to show that, in situations where the supply
of training samples is essentially unlimited, a well designed on-line algorithm
converges toward the minimum of the expected cost just as fast as any batch algo-
rithm. In those situations, the convergence speed is mainly limited by the fact that
some informative examples have not yet been seen rather than by the fact that the
examples already seen have not been fully exploited by the minimization process.
This point is very signicant because on-line algorithms are considerably eas-
ier to implement. Each iteration of the batch algorithm (2) involves a large sum-
mation over all the available examples. Memory must be allocated to hold these
examples, and computations must be performed on each of them. On the other
hand, each iteration of the on-line algorithm (3) only involves one random exam-
ple which can be discarded afterward.

3 Learning Speed

. We must decide how to use our computer cycles:

Assume we have immediate access to an innite number of examples
independently drawn from



We can run an on-line learning algorithm (3) and visit as many examples

We can run a batch super-linear algorithm (2) on a subset of examples ex-

as possible during the allowed computer time. This procedure produces a
sequence of parameter vectors
represent the
where
total number of iterations achieved within the imparted time.

	!	


with









is the size of the largest subset of examples
amples 
that can be processed within the imparted time. This procedure accurately
.
produces the parameter vector

that minimizes the empirical cost



where




The number of examples
as the number of examples



processed by the batch algorithm cannot be as large
processed by the on-line algorithm. Comparing the





























complexity of equations (2) and (3) clearly shows that this would only allow for
a couple iterations of the batch algorithm. Even if we assume that
, we
show in this contribution that no batch algorithm can perform better than a well
designed on-line algorithm.

3.1 On-line algorithm

The mathematics of on-line learning algorithm easily extend to the minimization
of the expected cost. Examples

 are drawn at each iteration of an on-line al-
gorithm. When these examples are drawn from a set of examples, the on-line
 . When these examples are drawn

algorithm minimizes the empirical error
from the asymptotic distribution

Because the supply of training samples is practically unlimited, each iteration
of the on-line update rule (3) utilizes a fresh example. These fresh examples then
follow the asymptotic distribution. The parameter vectors
thus converge to
the optimum

, it minimizes the expected cost

. Furthermore,




.





5

$ converges

5  of the expected cost

like"*

 at best.
#

"*

3.2 Batch algorithm

How fast does

converge to the optimum

mum Likelihood case3, the bound suggests that

We consider now the sequence of solutions

.

than
algorithm running on a set of

A.3) provides the following recursive relation between

examples

A rst hint is provided by the well known Cramer-Rao bound. In the Maxi-

5  of the expected cost

?









. Our rst result (section

$ converges no faster
 computed by a batch learning


#
!
 and

$



.

(5)

(6)

	5
	





1





" and updating the pa-
; .
;%'&1
.(67=$#
;(. "!

and

69=

This relation (5) describes the

sequence as a recursive stochastic process
that is essentially similar to the on-line learning algorithm (3). Each iteration
of this algorithm consists in picking a fresh example
rameters according to (5). This is not a practical algorithm because we have no

with










5



; with both conditions

3i.e.

69=

69=;
1































'







'












'













$







'

'




$







'



1
.
.
.

1
.
analytical expression for the second order term. We can however apply the math-
ematics of on-line learning algorithms to this stochastic process. The similarity
between (5) and (3) can be enhanced by an appropriate choice of the positive

denite symmetric matrix in the on-line algorithm (3).

3.3 Convergence speed result

Therefore, the convergence of the following stochastic process describes both the
convergence of an online learning algorithm and the behavior of the solutions of
a batch learning algorithm.







:

	






Because a same stochastic process describes both convergences, we can hope
that they occur at identical speeds. It is therefore important to determine how the
convergence speed of (7) depends on the unspecied second order terms and on

(7)

(8)

the choice of .

Our main result (section A.4) characterizes the convergence speed of this

stochastic process under the following assumptions:





of the expected

risk at the optimum.




i) We only consider the nal convergence phase (Bottou and Murata, 2002).

More precisely we consider that the
where
has a single minimum

This convergence speed neither depends on the second order terms in our

 are conned in a bounded domain
5  .
ii) We assume that converges to the inverse of the hessian
with

iii) We rst assume that only depends on


#
still holds when depends mildly on
 as in equation (6).
stochastic process nor depends on how fast) converges to


	





represents the trace of a matrix, 
#  , and





. The result however
. More precisely

is the hessian of the expected
is a Gauss-Newton approximation of the hessian (Le Cun et al.,

where
risk in
1998b):


















$


we have:


'





'





$



'



$






'
'





'

'
























In the Maximum Likelihood case, it is well known that both

to the Fisher information matrix on the optimum. Equation (8) then indicates that
the convergence speed reaches the Cramer-Rao bound. Such a result was already
reported in the case of the Natural Gradient algorithm (Murata and Amari, 1999).
Our result extends Muratas result to vast classes of on-line learning algorithms
beyond Natural Gradient.

and
are equal

3.4 Discussion

This result has implications for our initial dilemma. Should we visit as many
examples as possible with a well designed on-line algorithm, or run a batch algo-
rithm on the largest subset of examples we can afford ?

The surprisingly simple answer is to use the algorithm that uses the most ex-
amples. Learning is mainly limited by the fact that some informative examples
have not yet been seen rather than by the fact that the examples already seen have
not been fully exploited by the minimization process.

As discussed above, the higher complexity of the batch algorithm update (2)

implies that the on-line algorithm can process more examples4.

of the Hessian of the cost function. The speed of this convergence is not critical.

This result holds for any on-line algorithm where
 converges to the inverse
It is however essential that  be a full rank approximation of the inverse hes-

sian. Maintaining such a full rank approximation in a large system is very costly.
This is probably the main justication for avoiding the otherwise elegant Natural
Gradient algorithm.

It is therefore important to design new approximations of the inverse hessian
that simultaneously are cost effective and still deliver near Cramer-Rao efciency.
We hope to achieve such a result using the new insights provided by the mathe-
matical tools underlying the results presented in this paper.

4 Conclusion

We have shown that learning very large data sets is best achieved using a well
designed on-line learning procedure. A well designed on-line learning algorithm
learns just as fast as any batch algorithm in terms of the number of examples.
Furthermore, on-line learning algorithms require less computing resources per
example, and therefore are able to process more examples for a given amount of
computing resources.

4Section A.5 shows that

the on-line algorithm and
same computing resources.

is the number of examples visited by
is the maximum set processed by a super-linear algorithm with the

; where

1"









A Mathematical discussion

A.1 Orders of magnitude

without much
The main discussion uses the well known notations
analysis.
In the case of stochastic sequences, these notations can have several
distinct denitions. Let us rst recall the denitions for non stochastic sequences

and



(
(































 and

( :

#

that






Let

and

be two stochastic sequences. The parameter represents
the elementary random variables. In the case of stochastic learning algorithms, for
instance,
represents the initial conditions and the sequence of observed exam-
ples
useful to make the


 . Although it is customary to simply write

 , it is sometimes

parameter explicit.

 or

We use the unmodied

of magnitude. It means that the above denitions apply for each particular

notations to represent pointwise orders
and

depends on

.

Denition 1. Pointwise orders of magnitude.

and



















$


	

"#!

"#!

The above denitions is poorly adequate for deriving probabilistic results. We
do not need the inequality to be true for absolutely all
. Nothing bad happens
if the inequality is violated on a set with zero probability. On the other hand it
. This motivates the following

independent from

!
!







"
"




is often desirable to make%


denition.

Denition 2. Almost uniform order of magnitudes.

















"*+
"*+
tion, in the spirit of the concept of convergence in probability.




'&




)
)




(
(




It is more practical however to make a slight modication of the above deni-




"
"

Denition 3. Stochastic order of magnitudes (Mann and Wald, 1943).




$
$

-,

	.
	.























/)
/)




"01!
"01!




2
2

3
3

4.
4.



































































&






















,






Most of the properties of the usual orders of magnitude also apply to stochastic

orders of magnitude. In particular it is known that

implies

. On the other hand the relation

This is why we introduce yet another concept:



$







is not true in general.

Denition 4. Almost uniformly bounded stochastic order of magnitude.




(

-,

Theorem 1. With the above denitions

and






(

",

(

-,
(

. Since

(








(


such that


0









Proof. Let us write

and a subscript

"0
Let us choose an arbitrary positive number

/)



!

. We dene the event

, there is a constant






 as follows:






Since





Therefore, for all





This proves the theorem.

A.2 Problem setup

!





there is a subscript
/)













"
$ such that






, we can write



(







.





Let the loss function



fails to handle example

measure how much a learning system with parameter
represents
. The unknown example distribution

the ground truth. Our goal is to minimize the expected risk

To achieve this goal, it is common to collect a nite training set
minimize the objective function

.











 and to



























3





















&














&



























,






$







$









)


)





































Online learning algorithms provide another way to achieve this goal. Each iter-
ation of a typical online algorithm consists of drawing a random example
and

applying the following parameter update rule, where the

tive denite symmetric matrices.

 are well chosen posi-



We assume that functions










:
)



	





,

, and

are three times differ-
,




, and the matrices) are uniformly bounded. These assumptions

entiable with continuous derivatives. We also assume that both the examples
the parameters
imply that many dependent variables are uniformly bounded because they are con-
tinuous functions of uniformly bounded variables. This rather strong assumption
is supported by experience. Unbounded online algorithms tend to diverge and be
useless.



5 

Our discussion addresses the nal convergence phase of online learning algo-
remain conned in a
is convex and has a single non

rithms. Therefore we further assume that the parameters
bounded domain
degenerate minimum

where the cost function

.

denotes the denite positive hessian of the expected cost in

Notation
Notation
denotes the expectation of the squared Jacobian of the loss func-

tion at the optimum. This matrix measures the noise introduced by the stochastic
selection of the examples. It is also related to the Gauss-Newton approximation
(Le Cun et al., 1998b). In the Maximum Likelihood case, it is equal to the well
known Fisher Information matrix (see (Murata and Amari, 1999) for a denition).








.

















A.3 Recursive formulation of the batch algoritms

The result discussed in section 3.2 addresses the minima
tive functions
and have a minimum on domain

. We must assume that the empirical costs

.
We dene the empirical hessians

)


 of the empirical objec-

are convex



	




3

(9)



'





'


















$






































$







'


and further assume that the eigenvalues of the empirical hessians are bounded by
with probability one5. This implies that the
some constants





4


hessians and their inverses are all

Theorem 2. With the above assumptions and notations,

Proof. Let us dene

and write a rst order expansion in point

.




3
5

$



























#


	




"*

	


where we use a uniform order of magnitude because the boundedness assump-
tions described in section A.2 mean that the second derivative of 
is uniformly
bounded. Since 
, we can then rewrite the left hand side
of this equality.

We can then transform the right hand side as

and write

(10)



$

Thanks to our pervasive boundedness assumptions, the above equality implies that

. We can then rewrite equation (10) as

and derive the theorem.

5This assumption is not very satisfying. We could consider that it is true only with some

. The results would then hold with probability

probability



!

	


$

as well.

&










'






'










'


&















'









'














'




&






'


$









'



'













'














'




&







'












'









&








'











'









'








'












'









'









'



&












'














'




&




!
A.4 Convergence speed

Section 3 denes a stochastic process that simultaneously describes (a) the dy-
namics of an online learning algorithm, and (b) the convergence of the solutions
of a batch learning algorithm running on training sets of increasing sizes. The fol-
lowing theorem addresses the convergence speed of this stochastic process when

in probability.

	






(11)

$

Theorem 3. We consider the following stochastic process

the scaling matrices converge to







with the assumptions described in section A.2 and where

i.) 

-,
ii.)  is a function of


We have then

.





5

only.

The proof relies on the following technical lemma.

Lemma 1. : Let the positive sequence

the value of the constant factor. Amazingly enough, this constant does not depend
on the unspecied low order terms of the recurrence.

 and also provides



 verify





 is asymptotically equivalent to"*
 and observe that


"

If 

 and 

, then

This result proves that

Proof. Let us dene



Multiplying the recurrence by gives:

"



(12)

'


'






'


&





'






'








$




'




'

























'




$





$
























































'










To prove the lemma, we must prove that (14) converges to zero. There is an integer
we can then

is positive for all 

This result implies that the rst term of (14) converges to zero. We now focus
. For all

such that

, there is a 








"


3

(13)

(14)

. For all

3



















Let us dene

(

and rewrite (12) as:



.

(

with 

and 

By repeated substitutions of (13), we obtain

"






with








.





write:

such that





Therefore 
on the second term. Since 
we can also write:



It is well known that





and that

Therefore we can write
















'


















'




















'






















































'





















































'

$









































































































and





with



for all 

$ .

(15)



$ because



there is a 



.	



3

Furthermore, for all

We can now bound the second term of (14) as follows:

such that

The rst term of this sum converges to zero because 
3
3	



When
is large enough, the second term is smaller than




3	
We can now gather all the terms in (14) and (15). We can choose
. We can also choose
than
, we can choose
(

We can now proceed with the proof of theorem 3.

Therefore we have just proven that for all

to ensure that

.3
. Hence

(

3	


$ .

.












.


.

.



*.

large enough
to ensure that the rst term of (14) and the rst term of (15) are each smaller than
large enough to ensure that the last term of (15) is smaller

large enough

#

=

-,

"*




Proof. To simplify the notation in the following proof, we assume that the opti-
mum
is located on the origin. This assumption entails no loss of generality
since it only involves a translation of the coordinate system. We also use notation

.

#

. Since the





The assumptions described in section A.2 are sufcient conditions for the gen-



eral results discussed in section 4 of (Bottou, 1998). We know therefore that
converges to

almost surely. This almost sure convergence implies that





We rst derive an expression for

 are uniformly bounded, we have

This operation generates a number of high order terms that can be summarized as



.





can be summarized as

(

 by squaring the recursive update formula.
"*
"*



5
5



5







.





$

.

In particular the term
because we have established that










5






















+

.


$





.























































































'







'





$
.










$


















$



'




$



$

'








'




'


'



'






'





'





'




'



$




(

)


We shall now take compute the conditional expectation

where the

and the selected examples

 represents all variables known by time , including the initial condi-
are xed and can be moved


5

and because function 








(16)







is




Using similar arguments we can also write the following equality.

Applying the trace operator gives the following expression.

We can then take the unconditional expectation, invoke theorem 1, and obtain

notation
tions



"

7

-

When the past

outside the expectation operator.

)



The following relation holds because
continuous and uniformly bounded.

.
5
 is known, variables) and



5







We also remark that
because
are uniformly bounded. Then
both and
5
)

5

9




":



	


'&)(
"
'

:
*
,+-./
%$0
#
%$
)


.
56


:
87:9
34
&<(
5=
;A>

&1(
.

We can now derive




&<(







.


2





&CB-D




"

Lemma 1 allows us to conclude.












-,


 "!



&)(
3?

and because

(17)





&<(
&E(

@=

$









'



'


'




'





'





'









'




'












'







'





'



'





'




'




'




'


'




'


'






'


'

'

'

'


'


'


'






'


$







'


'
;
'


'




'


$
;
>
&
$
'
$
'

;
$
+
;
$
>






$



=
+
7
9



'


$

=



'


$
;
>

$
'

?
$
'


;
$

=
+
;
$
>



$















'
$




'




'



$






$




'




'









Theorem 3 makes the assumption that) only depends on

therefore independent from
algorithms. However it is not veried by the empirical hessian (9) introduced in
theorem 2. The following result relies on a weaker assumption which is veried
by the empirical hessian (9).

and is
 . This assumption is reasonable for online learning




5





9

	





 can be written as

Theorem 4. The result from theorem 3 still holds when




past
instead use the following derivation where
variable.

The proof is essentially identical to the proof of theorem 3. The difference
is no longer a constant when the

with 
shows in equation (16). The scaling matrix)
) is known. We cannot move) outside the expectation operator. We must
)

9
)

"*
)



We can then invoke the following relation

)




is a consequence of Schwartzs inequal-






The last equality
ity:

is a uniformly bounded random

)






)


)


9



)





and obtain equation (17) without changes.

7

)












5



The same argument yields

The proof then proceeds without changes.





"

7

*#
)




)





*




)













'
















































$

$





'










'







'








'




'





'




'







'





'




'




'



'
$






A.5 Complexity of Batch vs. Online Learning

training examples re-
Each iteration of a batch learning algorithm running on
quires a time
respectively represent the time
to compute the gradient for each example, and the time to apply the update to the
parameters. Theorems 2 and 4 indicate that

$ . Constants

and






"*

We must perform enough iterations of the batch algorithm to approximate

. A superlinear algorithm with quadratic

with at least the same accuracy
convergence will achieve this in

Each iteration of an online learning algorithm requires a constant time. Pro-
cessing
. The number of examples pro-
cessed by both algorithms with the same computing resources are therefore related
by the following relation.

examples therefore requires time

iterations.





	


We assume now that the online learning algorithm fulls the conditions of theorem
3. Comparing the acuracies of both algorithms shows that the online algorithm
asymptotically performs better.













of the scaling matrices 

The essential condition for such a fast online algorithm is the convergence

. This is not very difculy in theory. The

well known Natural Gradient algorithm, for instance, meets this condition and
is known to perform optimally (Murata and Amari, 1999).

This means however that a full rank scaling matrix must be maintained. This
is unfortunately not practical for large learning systems with many parameters. It
is therefore important to nd out whether reduced rank scaling matrices offer the
same asymptotic properties.

to 

