Abstract. In pattern recognition, statistical modeling, or regression,
the amount of data is the most critical factor a(cid:11)ecting the performance.
If the amount of data and computational resources are near in(cid:12)nite,
many algorithms will provably converge to the optimal solution. When
this is not the case, one has to introduce regularizers and a-priori knowl-
edge to supplement the available data in order to boost the performance.
Invariance (or known dependence) with respect to transformation of the
input is a frequent occurrence of such a-priori knowledge. In this chapter,
we introduce the concept of tangent vectors, which compactly represent
the essence of these transformation invariances, and two classes of algo-
rithms, \Tangent distance" and \Tangent propagation", which make use
of these invariances to improve performance.



Introduction

Pattern Recognition is one of the main tasks of biological information process-
ing systems, and a major challenge of computer science. The problem of pattern
recognition is to classify objects into categories, given that objects in a particu-
lar category may vary widely, while objects in di(cid:11)erent categories may be very
similar. A typical example is handwritten digit recognition. Characters, typically
represented as (cid:12)xed-size images (say  by  pixels), must be classi(cid:12)ed into one
of  categories using a classi(cid:12)cation function. Building such a classi(cid:12)cation func-
tion is a major technological challenge, as variabilities among objects of the same
class must be eliminated, while di(cid:11)erences between objects of di(cid:11)erent classes
must be identi(cid:12)ed. Since classi(cid:12)cation functions for most real pattern recognition
tasks are too complicated to be synthesized \by hand", automatic techniques
must be devised to construct the classi(cid:12)cation function from a set of labeled
examples (the training set). These techniques can be divided into two camps,
according to the number of parameters they are requiring: the \memory based"
algorithms, which use a (compact) representation of the training set, and the
\learning" techniques, which require adjustments of a comparatively small num-
ber of parameters (during training) to compute the classi(cid:12)cation function. This
distinction can be somewhat arbitrary because some classi(cid:12)cation algorithms,

for instance Learning vector quantization or LVQ, are hybrids. The distinction
serves our purpose, nonetheless, because memory based algorithms often rely
on a metric which can be modi(cid:12)ed to incorporate transformation invariances,
while learning based algorithms consist of selecting a classi(cid:12)cation function, the
derivatives of which can be constrained to re(cid:13)ect the same transformation in-
variances. The two methods for incorporating invariances are di(cid:11)erent enough
to justify two independent sections.

. Memory based algorithms

To compute the classi(cid:12)cation function, many practical pattern recognition sys-
tems, and several biological models, simply store all the examples, together with
their labels, in a memory. Each incoming pattern can then be compared to all
the stored prototypes, and the label associated with the prototype that best
matches the input can be output-ed. The above method is the simplest example
of the memory-based models. Memory-based models require three things: a dis-
tance measure to compare inputs to prototypes, an output function to produce
an output by combining the labels of the prototypes, and a storage scheme to
build the set of prototypes.

All three aspects have been abundantly treated in the literature. Output
functions range from simply voting the labels associated with the k closest pro-
totypes (K-Nearest Neighbors), to computing a score for each class as a linear
combination of the distances to all the prototypes, using (cid:12)xed [] or learned []
coe(cid:14)cients. Storage schemes vary from storing the entire training set, to picking
appropriate subsets of it (see [], chapter , for a survey) to learning algorithms
such as Learning Vector Quantization (LVQ) [] and gradient descent. Distance
measures can be as simple as the Euclidean distance, assuming the patterns and
prototypes are represented as vectors, or more complex as in the generalized
quadratic metric [] or in elastic matching methods [].

Pattern to
be classified

Prototype A

Prototype B

Fig. . According to the Euclidean distance the pattern to be classi(cid:12)ed is more similar
to prototype B. A better distance measure would (cid:12)nd that prototype A is closer because
it di(cid:11)ers mainly by a rotation and a thickness transformation, two transformations
which should leave the classi(cid:12)cation invariant.

A simple but ine(cid:14)cient pattern recognition method is to combine a simple
distance measure, such as Euclidean distance between vectors, with a very large

set of prototypes. This method is ine(cid:14)cient because almost all possible instances
of a category must be present in the prototype set. In the case of handwritten
digit recognition, this means that digits of each class in all possible positions,
sizes, angles, writing styles, line thicknesses, skews, etc... must be stored. In
real situations, this approach leads to impractically large prototype sets or to
mediocre recognition accuracy as illustrated in Figure . An unlabeled image of a
thick, slanted \	" must be classi(cid:12)ed by (cid:12)nding the closest prototype image out of
two images representing respectively a thin, upright \	" and a thick, slanted\".
According to the Euclidean distance (sum of the squares of the pixel to pixel
di(cid:11)erences), the \" is closer. The result is an incorrect classi(cid:12)cation.

The classical way of dealing with this problem is to use a so-called feature
extractor whose purpose is to compute a representation of the patterns that is
minimally a(cid:11)ected by transformations of the patterns that do not modify their
category. For character recognition, the representation should be invariant with
respect to position, size changes, slight rotations, distortions, or changes in line
thickness. The design and implementation of feature extractors is the major
bottleneck of building a pattern recognition system. For example, the problem
illustrated in Figure  can be solved by deslanting and thinning the images.

An alternative to this is to use an invariant distance measure constructed
in such a way that the distance between a prototype and a pattern will not be
a(cid:11)ected by irrelevant transformations of the pattern or of the prototype. With an
invariant distance measure, each prototype can match many possible instances
of pattern, thereby greatly reducing the number of prototypes required.

The natural way of doing this is to use \deformable" prototypes. During the
matching process, each prototype is deformed so as to best (cid:12)t the incoming pat-
tern. The quality of the (cid:12)t, possibly combined with a measure of the amount
of deformation, is then used as the distance measure []. With the example
of Figure , the \	" prototype would be rotated and thickened so as to best
match the incoming \	". This approach has two shortcomings. First, a set of
allowed deformations must be known a priori. Fortunately, this is the case for
many tasks, including character recognition. Second, the search for the best-
matching deformation is often enormously expensive, and/or unreliable. Con-
sider the case of patterns that can be represented by vectors. For example, the
pixel values of a  by  pixel character image can be viewed as the components
of a -dimensional vector. One pattern, or one prototype, is a point in this
-dimensional space. Assuming that the set of allowable transformations is
continuous, the set of all the patterns that can be obtained by transforming one
prototype using one or a combination of allowable transformations is a surface
in the -D pixel space. More precisely, when a pattern P is transformed (e.g.
rotated) according to a transformation s(P; (cid:11)) which depends on one parameter
(cid:11) (e.g. the angle of the rotation), the set of all the transformed patterns

SP = fx j 	(cid:11) for which x = s(P; (cid:11))g

()

is a one-dimensional curve in the vector space of the inputs. In the remainder of
this chapter, we will always assume that we have chosen s be di(cid:11)erentiable with
respect to both P and (cid:11) and such that s(P; ) = P .

When the set of transformations is parameterized by n parameters (cid:11)i (ro-
tation, translation, scaling, etc.), the intrinsic dimension of the manifold SP is
at most n. For example, if the allowable transformations of character images
are horizontal and vertical shifts, rotations, and scaling, the surface will be a
-dimensional manifold.

In general, the manifold will not be linear. Even a simple image translation
corresponds to a highly non-linear transformation in the high-dimensional pixel
space. For example, if the image of a \" is translated upward, some pixels
oscillate from white to black and back several times. Matching a deformable
prototype to an incoming pattern now amounts to (cid:12)nding the point on the sur-
face that is at a minimum distance from the point representing the incoming
pattern. Because the manifold is non-linear, the matching can be very expen-
sive and unreliable. Simple minimization methods such as gradient descent (or
conjugate gradient) can be used to (cid:12)nd the minimum-distance point, however,
these methods only converge to a local minimum. In addition, running such an
iterative procedure for each prototype is prohibitively expensive.

If the set of transformations happens to be linear in pixel space, then the
manifold is a linear subspace (a plane). The matching procedure is then reduced
to (cid:12)nding the shortest distance between a point (vector) and a plane: an easy-to-
solve quadratic minimization problem. This special case has been studied in the
statistical literature and is sometimes referred to as Procrustes analysis []. It
has been applied to signature veri(cid:12)cation [] and on-line character recognition
[].

This chapter considers the more general case of non-linear transformations
such as geometric transformations of gray-level images. Remember that even a
simple image translation corresponds to a highly non-linear transformation in
the high-dimensional pixel space. The main idea of the chapter is to approx-
imate the surface of possible transforms of a pattern by its tangent plane at
the pattern, thereby reducing the matching to (cid:12)nding the shortest distance be-
tween two planes. This distance is called the Tangent Distance. The result of
the approximation is shown in Figure , in the case of rotation for handwritten
digits. At the top of the (cid:12)gure, is the theoretical curve in pixel space which
represents equation (), together with its linear approximation. Points of the
transformation curve are depicted below for various amounts of rotation (each
angle corresponds to a value of (cid:11)). The bottom of Figure  depicts the linear
approximation of the curve s(P; (cid:11)) given by the Taylor expansion of s around
(cid:11) = :

s(P; (cid:11)) = s(P; ) + (cid:11)

+ O((cid:11)) (cid:25) P + (cid:11)T

()

@s(P; (cid:11))

@(cid:11)

This linear approximation is completely characterized by the point P and the
tangent vector T = @s(P;(cid:11))
@(cid:11) . Tangent vectors, also called the Lie derivatives of the
transformation s, will be the subject of section . As can be seen from Figure ,
for small angles (k(cid:11)k < ), the approximation is very good.

Figure  illustrates the di(cid:11)erence between the Euclidean distance, the full
invariant distance (minimum distance between manifolds) and the tangent dis-
tance. In the (cid:12)gure, both the prototype and the pattern are deformable (two-

Pixel space

=2

P+    .T

=1

P

=0

T


s(P,   )

=2

=1



s
 =T

=

)

,


+ .

=

T

s(

P

=2

=1

=0

=1

=2

Fig. . Top: Representation of the e(cid:11)ect of the rotation in pixel space. Middle: Small
rotations of an original digitized image of the digit \", for di(cid:11)erent angle values of
(cid:11). Bottom: Images obtained by moving along the tangent to the transformation curve
for the same original digitized image P by adding various amounts ((cid:11)) of the tangent
vector T .

sided distance), but for simplicity or e(cid:14)ciency reasons, it is also possible to
deform only the prototype or only the unknown pattern (one-sided distance).

Although in the following we will concentrate on using tangent distance to
recognize images, the method can be applied to many di(cid:11)erent types of signals:
temporal signals, speech, sensor data...

. Learning based algorithms

Rather than trying to keep a representation of the training set, it is also possible
to compute the classi(cid:12)cation function by learning a set of parameters from the
(cid:12)nite training set. This is the approach taken in neural networks. Let F (x) be
the (unknown) function to be learned, B = f(x; F (x)); : : : ; (xp; F (xp))g be a
(cid:12)nite set of training data taken from a constant statistical distribution P, and
let Gw(x) be a set of functions, indexed by the vector of parameters w. The task

Tangent Distance

Euclidean distance
between P and E

P

E

Distance between
S    and S
E

P

S

P

S

E

Fig. . Illustration of the Euclidean distance and the tangent distance between P and
E. The curves Sp and Se represent the sets of points obtained by applying the chosen
transformations (for example translations and rotations) to P and E. The lines going
through P and E represent the tangent to these curves. Assuming that working space
has more dimensions than the number of chosen transformations (on the diagram,
assume D) the tangent spaces do not intersect and the tangent distance is uniquely
de(cid:12)ned.

is to (cid:12)nd a value for w from the (cid:12)nite training data set B such that Gw best
approximates F on its input x. For example, Gw may be the function computed
by a neural net and the vector w may be the neural nets weights, or Gw may
be a polynomial and w its coe(cid:14)cients. Without additional information, (cid:12)nding
a value for w is an ill-posed problem because the training set does not provide
enough information to distinguish the best solution among all the candidates
ws. This problem is illustrated in Figure  (left). The desired function F (solid
line) is to be approximated by a functions Gw (dotted line) from four examples
f(xi; F (xi))gi=;;;. As exempli(cid:12)ed in the picture, the (cid:12)tted function Gw largely
disagrees with the desired function F between the examples, but it is not possible
to infer this from the training set alone. Many values of w can generate many
di(cid:11)erent function Gw, some of which may be terrible approximations of F , even
though they are in complete agreement with the training set. Because of this,
it is customary to add \regularizers", or additional constraints, to restrict the
search of an acceptable w. For example, we may require the function Gw to
be \smooth", by adding the constraint that kwk should be minimized. It is
important that the regularizer re(cid:13)ects a property of F , hence regularizers require
additional a-priori knowledge about the function to be modeled.

wG  (x)

wG  (x)

F(x)

F(x)

x1

x2

x3

x4

x

x1

x2

x3

x4

x

Fig. . Learning a given function (solid line) from a limited set of examples (x to x).
The (cid:12)tted curves are shown by dotted line. Left: The only constraint is that the (cid:12)tted
curve goes through the examples. Right: The (cid:12)tted curves not only go through each
example but also its derivatives evaluated at the examples agree with the derivatives
of the given function.

Selecting a good family G = fGw; w  <qg of functions is a di(cid:14)cult task,
sometimes known as \model selection" [, ]. If G contains a large family of
functions, it is more likely that it will contain a good approximation of F (the
function we are trying to approximate), but it is also more likely that the selected
candidate (using the training set) will generalize poorly because many functions
in G will agree with the training data and take outrageous values between the
training samples. If, on the other hand, G contains a small family of functions, it
is more likely that a function Gw which (cid:12)ts the data will be a good approximation
of F . The capacity of the family of functions G is often referred to as the VC
dimension [, ]. If a large amount of data is available, G should contain a
large family of functions (high VC dimension), so that more functions can be
approximated, and in particular, F . If, on the other hand, the data is scarce, G
should be restricted to a small family of functions (low VC dimension), to control
the values between the (more distant) samples. The VC dimension can also be
controlled by putting a knob on how much e(cid:11)ect is given to some regularizers.
For instance it is possible to control the capacity of a neural network by adding
\weight decay" as a regularizer. Weight decay is a loose smoothness constraint
on the classi(cid:12)cation function which works by decreasing kwk as well as the error
on the training set. Since the classi(cid:12)cation function is not necessarily smooth, for
instance at a decision boundary, the weight decay regularizer can have adverse
e(cid:11)ects.

As mentioned earlier, the regularizer should re(cid:13)ect interesting properties (a
priori knowledge) of the function to be learned. If the functions F and Gw are
assumed to be di(cid:11)erentiable, which is generally the case, the search for Gw can be

 Note that this formalism could also be used for memory based system. In the case
where all the training data can be kept in memory, however, the VC dimension is
in(cid:12)nite, and the formalism is meaningless. The VC dimension is a learning paradigm
and is not useful unless learning is involved.

greatly improved by requiring that Gws derivatives evaluated at the points fxig
are more or less equal (this is the regularizer knob) to the derivatives of F at the
same points (Figure  right). This result can be extended to multidimensional
inputs. In this case, we can impose the equality of the derivatives of F and Gw
in certain directions, not necessarily in all directions of the input space.

Such constraints (cid:12)nd immediate use in traditional learning pattern recogni-
tion problems. It is often the case that a priori knowledge is available on how the
desired function varies with respect to some transformations of the input. It is
straightforward to derive the corresponding constraint on the directional deriva-
tives of the (cid:12)tted function Gw in the directions of the transformations (previously
named tangent vectors). Typical examples can be found in pattern recognition
where the desired classi(cid:12)cation function is known to be invariant with respect
to some transformation of the input such as translation, rotation, scaling, etc.,
in other words, the directional derivatives of the classi(cid:12)cation function in the
directions of these transformations is zero.

This is illustrated in Figure . The right part of the (cid:12)gure shows how the
additional constraints on Gw help generalization by constraining the values of Gw
outside the training set. For every transformation which has a known e(cid:11)ect on
the classi(cid:12)cation function, a regularizer can be added in the form of a constraint
on the directional derivative of Gw in the direction of the tangent vector (such
as the one depicted in Figure ), computed from the curve of transformation.

The next section will analyze in detail how to use a distance based on tangent
vector in memory based algorithms. The subsequent section will discuss the use
of tangent vectors in neural network, with the tangent propagation algorithm.
The last section will compare di(cid:11)erent algorithms to compute tangent vectors.

 Tangent Distance

The Euclidean distance between two patterns P and E is in general not ap-
propriate because it is sensitive to irrelevant transformations of P and of E. In
contrast, the distance D(E; P ) de(cid:12)ned to be the minimal distance between the
two manifolds SP and SE is truly invariant with respect to the transformation
used to generate SP and SE (see Figure ). Unfortunately, these manifolds have
no analytic expression in general, and (cid:12)nding the distance between them is a
di(cid:14)cult optimization problem with multiple local minima. Besides, true invari-
ance is not necessarily desirable since a rotation of a \" into a \	" does not
preserve the correct classi(cid:12)cation.

Our approach consists of computing the minimum distance between the linear
surfaces that best approximate the non-linear manifolds SP and SE. This solves
three problems at once: ) linear manifolds have simple analytical expressions
which can be easily computed and stored, ) (cid:12)nding the minimum distance
between linear manifolds is a simple least squares problem which can be solved
e(cid:14)ciently and, ) this distance is locally invariant but not globally invariant.
Thus the distance between a \" and a slightly rotated \" is small but the

distance between a \" and a \	" is large. The di(cid:11)erent distances between P
and E are represented schematically in Figure .

The (cid:12)gure represents two patterns P and E in -dimensional space. The man-
ifolds generated by s are represented by one-dimensional curves going through E
and P respectively. The linear approximations to the manifolds are represented
by lines tangent to the curves at E and P . These lines do not intersect in  di-
mensions and the shortest distance between them (uniquely de(cid:12)ned) is D(E; P ).
The distance between the two non-linear transformation curves D(E; P ) is also
shown on the (cid:12)gure.

An e(cid:14)cient implementation of the tangent distance D(E; P ) will be given
in the next section. Although the tangent distance can be applied to any kind
of patterns represented as vectors, we have concentrated our e(cid:11)orts on applica-
tions to image recognition. Comparison of tangent distance with the best known
competing method will be described. Finally we will discuss possible variations
on the tangent distance and how it can be generalized to problems other than
pattern recognition.

. Implementation

In this section we describe formally the computation of the tangent distance. Let
the function s transform an image P to s(P; (cid:11)) according to the parameter (cid:11). We
require s to be di(cid:11)erentiable with respect to (cid:11) and P , and require s(P; ) = P .
If P is a  dimensional image for instance, s(P; (cid:11)) could be a rotation of P by
the angle (cid:11). If we are interested in all transformations of images which conserve
distances (isometry), s(P; (cid:11)) would be a rotation by (cid:11)(cid:18) followed by a translation
by (cid:11)x; (cid:11)y of the image P . In this case (cid:11) = ((cid:11)(cid:18); (cid:11)x; (cid:11)y) is a vector of parameters
of dimension . In general, (cid:11) = ((cid:11); : : : ; (cid:11)m) is of dimension m.

Since s is di(cid:11)erentiable, the set SP = fx j 	(cid:11) for which x = s(P; (cid:11))g is
a di(cid:11)erentiable manifold which can be approximated to the (cid:12)rst order by a
hyperplane TP . This hyperplane is tangent to SP at P and is generated by the
columns of matrix

LP =

@s(P; (cid:11))

@(cid:11)

= (cid:20) @s(P; (cid:11))

@(cid:11)

; : : : ;

@s(P; (cid:11))

@(cid:11)m (cid:21)(cid:11)=

()

(cid:12)(cid:12)(cid:12)(cid:12)(cid:11)=

which are vectors tangent to the manifold. If E and P are two patterns to be
compared, the respective tangent planes TE and TP can be used to de(cid:12)ne a new
distance D between these two patterns. The tangent distance D(E; P ) between
E and P is de(cid:12)ned by

D(E; P ) = min

xTE ;yTP

kx (cid:0) yk

The equation of the tangent planes TE and TP is given by:

E((cid:11)E) = E + LE(cid:11)E
P ((cid:11)P ) = P + LP (cid:11)P

()

()

()

where LE and LP are the matrices containing the tangent vectors (see equa-
tion ()) and the vectors (cid:11)E and (cid:11)P are the coordinates of E and P  (using
bases LE and LP ) in the corresponding tangent planes. Note that E, E, LE
and (cid:11)E denote vectors and matrices in linear equations (). For example, if the
pixel space was of dimension , and there were two tangent vectors, we could
rewrite equation () as

E

E

E

E

E










E
E
E
E
E









L L
L L
L L
L L
L L









=

+

(cid:20) (cid:11)
(cid:11)(cid:21)

()

The quantities LE and LP are attributes of the patterns so in many cases they
can be precomputed and stored.

Computing the tangent distance

D(E; P ) = min
(cid:11)E ;(cid:11)P

kE((cid:11)E) (cid:0) P ((cid:11)P )k

()

amounts to solving a linear least squares problem. The optimality condition is
that the partial derivatives of D(E; P ) with respect to (cid:11)P and (cid:11)E should be
zero:

@D(E; P )

@(cid:11)E

@D(E; P )

@(cid:11)P

= (E((cid:11)E) (cid:0) P ((cid:11)P ))>LE = 

= (P ((cid:11)P ) (cid:0) E((cid:11)E))>LP = 

(	)

()

Substituting E and P  by their expressions yields to the following linear system
of equations, which we must solve for (cid:11)P and (cid:11)E:

L>
L>

P (E (cid:0) P (cid:0) LP (cid:11)P + LE(cid:11)E) = 
E(E (cid:0) P (cid:0) LP (cid:11)P + LE(cid:11)E) = 

()
()

()
()

The solution of this system is
E (cid:0) L>
P (cid:0) L>

(LP EL(cid:0)
(LEP L(cid:0)

EEL>
P P L>
ELE, LP E = L>

P )(E (cid:0) P ) = (LP EL(cid:0)
E)(E (cid:0) P ) = (LEE (cid:0) LEP L(cid:0)

EELEP (cid:0) LP P )(cid:11)P
P P LP E)(cid:11)E

P LE, LEP = L>

ELP and LP P = L>

where LEE = L>
P LP . LU
decompositions of LEE and LP P can be precomputed. The most expensive part
in solving this system is evaluating LEP (LP E can be obtained by transposing
LEP ). It requires mE (cid:2) mP dot products, where mE is the number of tangent
vectors for E and mP is the number of tangent vectors for P . Once LEP has
been computed, (cid:11)P and (cid:11)E can be computed by solving two (small) linear
systems of respectively mE and mP equations. The tangent distance is obtained
by computing kE((cid:11)E) (cid:0) P ((cid:11)P )k using the value of (cid:11)P and (cid:11)E in equations ()
and (). If n is the dimension of the input space (i.e. the length of vector E
and P ), the algorithm described above requires roughly n(mE + )(mP + ) +
(m
P ) multiply-adds. Approximations to the tangent distance can however
be computed more e(cid:14)ciently.

E +m

. Some illustrative results

Local Invariance: The \local invariance" of tangent distance can be illus-
trated by transforming a reference image by various amounts and measuring its
distance to a set of prototypes.

12

10

2

8

6

4

2

0

6

7

Euclidean Distance

0

1

9

4

5

3

8

6 5 4 3 2 1 0

1

2

3

4

5

6

12

10

8

6

4

2

0

Tangent Distance

0

6

1

2

7

4

9

5

8

3

6 5 4 3 2 1 0

1

2

3

4

5

6

Fig. . Euclidean and tangent distances between  typical handwritten images of the
 digits ( curves) and a digit image translated horizontally by various amounts
(indicated in absyssa, in pixel). On the (cid:12)gure the translated image is the digit \"
from the  digits above.

On Figure ,  typical handwritten digit images are selected (bottom), and
one of them { the digit \" { is chosen to be the reference. The reference is
translated from  pixels to the left, to  pixels to the right, and the translation
amount is indicated in absyssa. Each curve represents the Euclidean Distance
(or the Tangent Distance) between the (translated) reference and one of the 
digits.

Since the reference was chosen from the  digits, it is not surprising that
the curve corresponding to the digit \" goes to  when the reference is not
translated ( pixel translation). It is clear from the (cid:12)gure that if the reference

 Local invariance refers to invariance with respect to small transformations (i.e. a
rotation of a very small angle). In contrast, global invariance refers to invariance
with respect to arbitrarily large transformations (i.e. a rotation of  degrees).
Global invariance is not desirable in digit recognition, since we need to distinguish
\"s and \	"s.

(the image \") is translated by more than  pixels, the Euclidean Distance will
confuse it with other digits, namely \" or \". In contrast, there is no possible
confusion when Tangent Distance is used. As a matter of fact, in this example,
the Tangent Distance correctly identi(cid:12)es the reference up to a translation of 
pixels! Similar curves were obtained with all the other transformations (rotation,
scaling, etc...).

The \local" invariance of Tangent distance with respect to small transforma-
tions generally implies more accurate classi(cid:12)cation for much larger transforma-
tions. This is the single most important feature of Tangent Distance.

Tangent vectors

Points in the tangent plane

Original

Fig. . Left: Original image. Middle:  tangent vectors corresponding respectively to
the  transformations: scaling, rotation, expansion of the X axis while compressing
the Y axis, expansion of the (cid:12)rst diagonal while compressing the second diagonal and
thickening. Right:  points in the tangent space generated by adding or subtracting
each of the  tangent vectors.

The locality of the invariance has another important bene(cid:12)t: Local invariance
can be enforced with very few tangent vectors. The reason is that for in(cid:12)nitesimal
(local) transformations, there is a direct correspondence between the tangent
vectors of the tangent plane and the various compositions of transformations. For
example, the three tangent vectors for X-translation, Y-translation and rotations
of center O, generate a tangent plane corresponding to all the possible composi-
tions of horizontal translations, vertical translations and rotations of center O.
The resulting tangent distance is then locally invariant to all the translations and
all the rotations (of any center). Figure  further illustrates this phenomenon by
displaying points in the tangent plane generated from only  tangent vectors.

 an isomorphism actually, see \Lie algebra" in []

Each of these images looks like it has been obtained by applying various com-
binations of scaling, rotation, horizontal and vertical skewing, and thickening.
Yet, the tangent distance between any of these points and the original image is
.

Handwritten Digit Recognition: Experiments were conducted to evaluate
the performance of tangent distance for handwritten digit recognition. An inter-
esting characteristic of digit images is that we can readily identify a set of local
transformations which do not a(cid:11)ect the identity of the character, while covering
a large portion of the set of possible instances of the character. Seven such im-
age transformations were identi(cid:12)ed: X and Y translations, rotation, scaling, two
hyperbolic transformations (which can generate shearing and squeezing), and
line thickening or thinning. The (cid:12)rst six transformations were chosen to span
the set of all possible linear coordinate transforms in the image plane (neverthe-
less, they correspond to highly non-linear transforms in pixel space). Additional
transformations have been tried with less success. Three databases were used to
test our algorithm:

US Postal Service database: The database consisted of  (cid:2)  pixel size-
normalized images of handwritten digits, coming from US mail envelopes. The
training and testing set had respectively 		 and  examples.

NIST database: The second experiment was a competition organized by
the National Institute of Standards and Technology (NIST) in Spring 		. The
object of the competition was to classify a test set of 	, handwritten digits,
given a training set of , patterns.

NIST database: The third experiment was performed on a database made
out of the training and testing database provided by NIST (see above). NIST
had divided the data into two sets which unfortunately had di(cid:11)erent distribu-
tions. The training set (, patterns) was easier than the testing set (	,
patterns). In our experiments we combined these two sets / to make a train-
ing set of , patterns and testing/validation sets of , patterns each, all
having the same characteristics.

For each of these three databases we tried to evaluate human performance to
benchmark the di(cid:14)culty of the database. For USPS, two members of our group
went through the test set and both obtained a .% raw error performance.
The human performance on NIST was provided by the National Institute of
Standard and Technology. The human performance on NIST was measured on
a small subsample of the database and must therefore be taken with caution.
Several of the leading algorithms where tested on each of these databases.

The (cid:12)rst experiment used the K-Nearest Neighbor algorithm, using the or-
dinary Euclidean distance. The prototype set consisted of all available training
examples. A -Nearest Neighbor rule gave optimal performance in USPS while
a -Nearest Neighbors rule performed better in NIST.

The second experiment was similar to the (cid:12)rst, but the distance function
was changed to tangent distance with  transformations. For the USPS and
NIST databases, the prototype set was constructed as before, but for NIST it

was constructed by cycling through the training set. Any patterns which were
misclassi(cid:12)ed were added to the prototype set. After a few cycles, no more pro-
totypes are added (the training error was ). This resulted in , prototypes.
A -Nearest Neighbors rule gave optimal performance on this set.

Other algorithms such as Neural nets [, ], Optimal Margin Classi(cid:12)er [],
Local Learning [] and Boosting [	] were also used on these databases. A case
study can be found in [].

The results are summarized in Table .. As illustrated in the table, the Tan-

Human K-NN T.D. Lenet Lenet OMC LL Boost

USPS
NIST
NIST

.
.
.

.

.

.
.
.

.

.

. . .
.
. . .

.
.

Table . Results: Performances in % of errors for (in order) Human, K-nearest neigh-
bor, Tangent distance, Lenet (simple neural network), Lenet (large neural network),
Optimal Margin Classi(cid:12)er (OMC), Local learning (LL) and Boosting (Boost).

gent Distance algorithm equals or outperforms all other algorithms we tested, in
all cases except one: Boosted LeNet  was the winner on the NIST database.
This is not surprising. The K-nearest neighbor algorithm (with no preprocessing)
is very un-sophisticated in comparison to local learning, optimal margin classi-
(cid:12)er, and boosting. The advantange of tangent distance is the a priori knowledge
of transformation invariance embedded into the distance. When the training
data is su(cid:14)ciently large, as is the case in NIST, some of this knowledge can be
picked up from the data by the more sophisticated algorithms. In other words,
the a-priori knowledge advantage decreases with the size of the training set.

. How to make tangent distance work

This section is dedicated to the technological \know how" which is necessary to
make tangent distance work with various applications. These \tricks" are usually
not published for various reasons (they are not always theoretically sound, page
estate is too valuable, the tricks are speci(cid:12)c to one particular application, intel-
lectual property forbids telling anyone how to reproduce the result, etc.), but
they are often a determining factor in making the technology a success. Several
of these techniques will be discussed here.

Smoothing the input space: This is the single most important factor in
obtaining good performance with tangent distance. By de(cid:12)nition, the tangent
vectors are the Lie derivatives of the transformation function s(P; (cid:11)) with respect
to (cid:11). They can be written as:

LP =

@s(P; (cid:11))

@(cid:11)

= lim
(cid:15)!

(cid:12)(cid:12)(cid:12)(cid:12)

s(P; (cid:15)) (cid:0) s(P; )

(cid:15)

()

It is therefore very important that s be di(cid:11)erentiable (and well behaved) with
respect to (cid:11). In particular, it is clear from equation () that s(P; (cid:15)) must be
computed for (cid:15) arbitrarily small. Fortunately, even when P can only take dis-
crete values, it is easy to make s di(cid:11)erentiable. The trick is to use a smoothing
interpolating function C(cid:27) as a preprocessing for P , such that s(C(cid:27)(P ); (cid:11)) is dif-
ferentiable (with respect to C(cid:27)(P ) and (cid:11), not with respect to P ). For instance,
if the input space for P is binary images, C(cid:27)(P ) can be a convolution of P with
a Gaussian function of standard deviation (cid:27). If s(C(cid:27)(P ); (cid:11)) is a translation of
(cid:11) pixel, the derivative of s(C(cid:27)(P ); (cid:11)) can easily be computed since s(C(cid:27)(P ); (cid:15))
can be obtained by translating Gaussian functions. This preprocessing will be
discussed in more details in section .

The smoothing factor (cid:27) controls the locality of the invariance. The smoother
the transformation curve de(cid:12)ned by s is, the longer the linear approximation
will be valid. In general the best smoothing is the maximum smoothing which
does not blur the features. For example, in handwritten character recognition
with x pixel images, a Gaussian function with a standard deviation of 
pixel yielded the best results. Increased smoothing led to confusion (such as a
\" mistaken for \" because the lower loop had been closed by the smoothing)
and decreased smoothing didnt make full use of the invariance properties.

If computation allows it, the best strategy is to extract features (cid:12)rst, smooth
shamelessly, and then compute the tangent distance on the smoothed features.
Controlled deformation: The linear system given in equation () is singu-
lar if some of the tangent vectors for E or P are parallel. Although the probability
of this happening is zero when the data is taken from a real valued continuous
distribution (as is the case in handwritten character recognition), it is possible
that a pattern may be duplicated in both the training and the test set, resulting
in a division by zero error. The (cid:12)x is quite simple and elegant. Equation () can
be replaced by equation:

D(E; P ) = min
(cid:11)E ;(cid:11)P

kE + LE(cid:11)E (cid:0) P (cid:0) LP (cid:11)P k + kkLE(cid:11)Ek + kkLP (cid:11)P k

()

The physical interpretation of this equation, depicted in Figure , is that the
point E((cid:11)E) on the tangent plane TE is attached to E with a spring of elasticity
k and to P ((cid:11)p) (on the tangent plane TP ) with a sprint of elasticity , and P ((cid:11)p)
is also attached to P with a spring of elasticity k. The new tangent distance is the
total potential elastic energy of stored all three springs at equilibrium. As for the
standard tangent distance, the solution can easily be derived by di(cid:11)erentiating
equation () with respect to (cid:11)E and (cid:11)P . The di(cid:11)erentiation yields:

L>
L>

P (E (cid:0) P (cid:0) LP ( + k)(cid:11)P + LE(cid:11)E) = 
E(E (cid:0) P (cid:0) LP (cid:11)P + LE( + k)(cid:11)E) = 

()

()

The solution of this system is

(LP EL(cid:0)
(LEP L(cid:0)

EEL>
P P L>

E (cid:0) ( + k)L>
P (cid:0) ( + k)L>

P )(E (cid:0) P ) = (LP EL(cid:0)
E)(E (cid:0) P ) = (( + k)LEE (cid:0) LEP L(cid:0)

EELEP (cid:0) ( + k)LP P )(cid:11)P (	)
P P LP E)(cid:11)E ()

E

1

k

E

P

k

P

Fig. . The tangent distance between E and P is the elastic energy stored in each of
the three springs connecting P , P , E  and E. P  and E  can move without friction
along the tangent planes. The spring constants are indicated on the (cid:12)gure.

ELE, LP E = L>

P LE, LEP = L>

ELP and LP P = L>

where LEE = L>
P LP . The
system has the same complexity as the vanilla tangent distance except that,
it always has a solution for k > , and is more numerically stable. Note that
for k = , it is equivalent to the standard tangent distance, while for k = ,
we have the Euclidean distance. This approach is also very useful when the
number of tangent vectors is greater or equal than the number of dimensions of
the space. The standard tangent distance would most likely be zero (when the
tangent spaces intersect), but the \spring" tangent distance still yields valuable
information.

If the number of dimension of the input space is large compared to the
number of tangent vector, keeping k as small as possible is better because it
doesnt interfere with the \sliding" along the tangent plane (E and P  are less
constrained) .

Contrary to intuition, there is no danger of sliding too far in high dimensional
space because tangent vectors are always roughly orthogonal and they could only
slide far if they were parallel.

Hierarchy of distances: If several invariances are used, classi(cid:12)cation using
tangent distance alone would be quite expensive. Fortunately, if a typical memory
based algorithm is used, for example, K-nearest neighbors, it is quite unnecessary
to compute the full tangent distance between the unclassi(cid:12)ed pattern and all
the labeled samples. In particular, if a crude estimate of the tangent distance
indicates with a su(cid:14)cient con(cid:12)dence that a sample is very far from the pattern to
be classi(cid:12)ed, no more computation is needed to know that this sample is not one
of the K-nearest neighbors. Based on this observation one can build a hierarchy of
distances which can greatly reduce the computation of each classi(cid:12)cation. Lets
assume, for instance, that we have m approximations Di of the tangent distance,
ordered such that D is the crudest approximation of the tangent distance and

Dm is exactly tangent distance (for instance D to D could be the Euclidean
distance with increasing resolution, and D to D each add a tangent vector at
full resolution).

The basic idea is to keep a pool of all the prototypes which could poten-
tially be the K-nearest neighbors of the unclassi(cid:12)ed pattern. Initially the pool
contains all the samples. Each of the distances Di corresponds to a stage of the
classi(cid:12)cation process. The classi(cid:12)cation algorithm has  steps at each stage, and
proceeds from stage  to stage m or until the classi(cid:12)cation is complete: Step :
the distance Di between all the samples in the pool and the unclassi(cid:12)ed pattern
is computed. Step : A classi(cid:12)cation and a con(cid:12)dence score is computed with
these distances. If the con(cid:12)dence is good enough, lets say better than Ci (for
instance, if all the samples left in the pool are in the same class) the classi(cid:12)ca-
tion is complete, otherwise proceed to step . Step : The Ki closest samples
in the pool, according to distance Di are kept, while the remaining samples are
discarded.

Finding the Ki closest samples can be done in O(p) (where p is the number
of samples in the pool) since these elements need not to be sorted [, ]. The
reduced pool is then passed to stage i + .

The two constants Ci and Ki must be determined in advance using a valida-
tion set. This can easily be done graphically by plotting the error as a function
of Ki and Ci at each stage (starting with all Ki equal to the number of labeled
samples and Ci =  for all stages). At each stage there is a minimum Ki and
minimum Ci which give optimal performance on the validation set. By taking
larger values, we can decrease the probability of making errors on the test sets.
The slightly worse performance of using a hierarchy of distances are well worth
the speed up. The computational cost of a pattern classi(cid:12)cation is then equal
to:

computational cost (cid:25) Xi

number of
prototypes
at stage i

(cid:2)

distance
complexity
at stage i

(cid:2)

probability
to reach
stage i

()

All this is better illustrated with an example as in Figure . This system was
used for the USPS experiment described in a previous section. In classi(cid:12)cation
of handwritten digits (x pixel images), D, D, and D, were the Euclidean
distances at resolution  (cid:2) ,  (cid:2)  and  (cid:2)  respectively. D was the one sided
tangent distance with X-translation, on the sample side only, at resolution  (cid:2) .
D was the double sided tangent distance with X-translation at resolution (cid:2).
Each of the subsequent distance added one tangent vector on each side (Y-
translation, scaling, rotation, hyperbolic deformation, hyperbolic deformation
and thickness) until the full tangent distance was computed (D).

Table  shows the expected number of multiply-adds at each of the stages. It
should be noted that the full tangent distance need only be computed for  in 
unknown patterns (probability :), and only with  samples out of the original
; . The net speed up was in the order of , compared with computing the
full tangent distance between every unknown pattern and every sample (this is
 times faster than computing the the Euclidean distance at full resolution).

Unknown Pattern

Prototypes

10,000

Euc. Dist

2x2

Cost: 4

3,500

Euc. Dist

4x4

Cost: 16

500

...

...

Category

Tang.Dist
14 vectors

16x16

Cost:20000

5

Confidence

Confidence

Confidence

Fig. . Pattern recognition using a hierarchy of distances. The (cid:12)lter proceeds from
left (starting with the whole database) to right (where only a few prototypes remain).
At each stage distances between prototypes and the unknown pattern are computed,
sorted, and the best candidate prototypes are selected for the next stage. As the com-
plexity of the distance increases, the number of prototypes decreases, making compu-
tation feasible. At each stage a classi(cid:12)cation is attempted and a con(cid:12)dence score is
computed. If the con(cid:12)dence score is high enough, the remaining stages are skipped.

Multiple iterations: Tangent distance can be viewed as one iteration of a
Newton-type algorithm which (cid:12)nds the points of minimum distance on the true
transformation manifolds. The vectors (cid:11)E and (cid:11)P are the coordinates of the two
closest points in the respective tangent spaces, but they can also be interpreted
as the value for the real (non-linear) transformations. In other words, we can
use (cid:11)E and (cid:11)P to compute the points s(E; (cid:11)E) and s(P; (cid:11)P ), the real non-
linear transformation of E and P . From these new points, we can recompute
the tangent vectors, and the tangent distance and reiterate the process. If the
appropriate conditions are met, this process can converge to a local minimum in
the distance between the two transformation manifold of P and E.

This process did not improved handwritten character recognition, but it
yielded impressive results in face recognition [	]. In that case, each succes-
sive iteration was done at increasing resolution (hence combining hierarchical
distances and multiple iterations), making the whole process computationally
e(cid:14)cient.

 Tangent Propagation

Instead of de(cid:12)ning a metric for comparing labeled samples and unknown pat-
terns, it is also possible to incorporate the invariance directly into a classi(cid:12)cation
function. This can be done by learning explicitly the transformation invariance
into the classi(cid:12)cation function. In this section, we present a modi(cid:12)cation of the
backpropagation algorithm, called \tangent propagation", in which the invari-
ance are learned by gradient descent.

We again assume F (x) to be the (unknown) function to be learned, B =
f(x; F (x); : : : ; (xp; F (xp)g to be a (cid:12)nite set of training data taken from a con-
stant statistical distribution P, and Gw(x) to be a set of functions, indexed

i # of T.V. Reso # of proto (Ki) # of prod Probab # of mul/add
,

,

,


,
,

,

,

,


,
,


,









 
 
 
 
 
 
 













.
.
.
.	
.
.
.
.
.
.
.













Table . Summary computation for the classi(cid:12)cation of  pattern: The (cid:12)rst column is
the distance index, the second column indicates the number of tangent vector ( for the
Euclidean distance), and the third column indicates the resolution in pixels, the fourth
is Ki or the number of prototypes on which the distance Di must be computed, the
(cid:12)fth column indicates the number of additional dot products which must be computed
to evaluate distance Di, the sixth column indicates the probability to not skip that
stage after the con(cid:12)dence score has been used, and the last column indicates the total
average number of multiply-adds which must be performed (product of column  to )
at each stage.

by the vector of parameters w. We wish to (cid:12)nd w which minimizes the energy
function

E = Z kGw(x) (cid:0) F (x)kdP(x)

()

given a (cid:12)nite sample B taken from the distribution P. This can be achieved by
minimizing

p

Ep =

Xi=

kGw(xi) (cid:0) F (xi)k

()

over the training set B. An estimate of w can be computed by following a gradient
descent using the weight-update rule:

(cid:1)w = (cid:0)(cid:17)

@Ep
@w

()

Lets assume that in addition to the training set B, we also know an input
transformation s(x; (cid:11)) with respect to parameter (cid:11), such that s(x; ) = x. We
also assume that @F (s(xi;(cid:11)))
is known at (cid:11) = . To incorporate the invariance
property into Gw(x), we add that the following constraint on the derivative:

@(cid:11)

Er =

p

Xi=

(cid:12)(cid:12)(cid:12)(cid:12)

should be small at (cid:11) = . In many pattern classi(cid:12)cation problems, we are inter-
ested in the local classi(cid:12)cation invariance property for F (x) with respect to the

@Gw(s(xi; (cid:11)))

@(cid:11)

(cid:0)

@F (s(xi; (cid:11)))

@(cid:11)



(cid:11)=

()

(cid:12)(cid:12)(cid:12)(cid:12)

transformation s (the classi(cid:12)cation does not change when the input is slightly
transformed), so we can simplify equation () to:

Er =

p

Xi=

(cid:12)(cid:12)(cid:12)(cid:12)

@Gw(s(xi; (cid:11)))

@(cid:11)



(cid:12)(cid:12)(cid:12)(cid:12)

(cid:11)=

()

since @F (s(xi;(cid:11)))
rule to use the energy function

@(cid:11)

= . To minimize this term we can modify the gradient descent

with the weight update rule:

E = (cid:17)Ep + (cid:22)Er

(cid:1)w = (cid:0)

@E
@w

()

()

The learning rates (or regularization parameters) (cid:17) and (cid:22) are tremendously
important, because they determine the tradeo(cid:11) between minimizing the usual
objective function and minimizing the directional derivative error.

The local variation of the classi(cid:12)cation function, which appear in equa-

tion () can be written as:

@Gw(s(x; (cid:11)))

@(cid:11)

=

@Gw(s(x; (cid:11)))

@s(x; (cid:11))

(cid:12)(cid:12)(cid:12)(cid:12)(cid:11)=

@s(x; (cid:11))

@(cid:11) (cid:12)(cid:12)(cid:12)(cid:12)(cid:11)=

= rxGw(x):

@s(x; (cid:11))

@(cid:11) (cid:12)(cid:12)(cid:12)(cid:12)(cid:11)=

(	)
since s(x; (cid:11)) = x if (cid:11) = . where rxGw(x) is the Jacobian of Gw(x) for pattern
x, and @s((cid:11); x)=@(cid:11) is the tangent vector associated with transformation s as
described in the previous section. Multiplying the tangent vector by the Jacobian
involves one forward propagation through a \linearized" version of the network.
If (cid:11) is multi-dimensional, the forward propagation must be repeated for each
tangent vector.

The theory of Lie algebras [] ensures that compositions of local (small)
transformations correspond to linear combinations of the corresponding tan-
gent vectors (this result will be discussed further in section ). Consequently, if
Er(x) =  is veri(cid:12)ed, the network derivative in the direction of a linear combina-
tion of the tangent vectors is equal to the same linear combination of the desired
derivatives. In other words, if the network is successfully trained to be locally
invariant with respect to, say, horizontal translations and vertical translations,
it will be invariant with respect to compositions thereof.

It is possible to devise an e(cid:14)cient algorithm, \tangent prop", for performing
the weight update (equation ()). It is analogous to ordinary backpropagation,
but in addition to propagating neuron activations, it also propagates the tangent
vectors. The equations can be easily derived from Figure 	.

. Local rule

The forward propagation equation is:

al

i = Xj

wl

ij xl(cid:0)

j

i = (cid:27)(al
xl
i)

()

wl+
ki

wl+
ki

bl
i

xl
i

(cid:12)l
i

(cid:24)l
i

(cid:27)

(cid:27) 

yl
i

al
i

l
i

(cid:13)l
i

wl
ij

wl
ij

bl(cid:0)
j

xl(cid:0)

j

(cid:12)l(cid:0)
j

(cid:24)l(cid:0)
j

Network

Jacobian network

Fig. 	. Forward propagated variables (a; x; (cid:13); (cid:24)), and backward propagated variables
(b; y; (cid:12);  ) in the regular network (roman symbols) and the Jacobian (linearized) net-
work (greek symbols). Converging forks (in the direction in which the signal is traveling)
are sums, diverging forks just duplicate the values.

Where (cid:27) is a non linear di(cid:11)erentiable function (typically a sigmoid). The forward
propagation starts at the (cid:12)rst layer (l = ), with x being the input layer, and
ends at the output layer (l = L). Similarly, The tangent forward propagation
(tangent prop) is de(cid:12)ned by:

(cid:13)l

i = Xj

wl

ij (cid:24)l(cid:0)

j

i = (cid:27)(al
(cid:24)l

i)(cid:13)l

i

()

The tangent forward propagation starts at the (cid:12)rst layer (l = ), with (cid:24) being
the tangent vector @s(x;(cid:11))
@(cid:11) , and ends at the output layer (l = L). The tangent
gradient backpropagation can be computed using the chain rule:

@E
@(cid:24)l
i

= Xk

@E
@(cid:13)l+

k

@(cid:13)l+
k
@(cid:24)l
i

@E
@(cid:13)l
i

=

@E
@(cid:24)l
i

@(cid:24)l
i
@(cid:13)l
i

(cid:12)l

i = Xk

l+
k wl+

ki

i = (cid:12)l
l

i(cid:27)(al
i)

()

()

The tangent backward propagation starts at the output layer (l = L), with (cid:24)L
being the network variation @Gw(s(x;(cid:11)))
, and ends at the input layer. Similarly,

@(cid:11)

the gradient backpropagation is:

@E
@xl
i

= Xk
i = Xk

bl

yl+
k wl+

ki

@E
@al+

k

@al+
k
@xl
i

@E
@al
i

=

@E
@xl
i

@xl
i
@al
i

+

@E
@(cid:24)l
i

@(cid:24)l
i
@al
i

yl
i = bl

i(cid:27)(al

i) + (cid:12)i(cid:27)(al

i)(cid:13)l

i

()

()

The standard backward propagation starts at the output layer (l = L), with
xL = Gw(x) being the network output, and ends at the input layer. Finally,
the weight update is:

(cid:1)wl

ij = (cid:0)

@E
@al
i

@al
i
@wl
ij

(cid:0)

@E
@(cid:13)l
i

@(cid:13)l
i
@wl
ij

(cid:1)wl

ij = (cid:0)yl

ixl(cid:0)

j (cid:0)  l

i(cid:24)l(cid:0)

j

()

()

The computation requires one forward propagation and one backward propaga-
tion per pattern and per tangent vector during training. After the network is
trained, it is simply locally invariant with respect to the chosen transformation.
The classi(cid:12)cation computation is in all ways identical to a network which is not
trained for invariance (except for the weights which have di(cid:11)erent values).

. Results

Two experiments illustrate the advantages of tangent prop. The (cid:12)rst experiment
is a classi(cid:12)cation task, using a small (linearly separable) set of  binarized
handwritten digits. The training sets consist of , , , ,  or  pat-
terns, and the test set contains the remaining  patterns. The patterns are
smoothed using a Gaussian kernel with standard deviation of one half pixel. For
each of the training set patterns, the tangent vectors for horizontal and vertical
translation are computed. The network has two hidden layers with locally con-
nected shared weights, and one output layer with  units (	 connections,
 free parameters) [	]. The generalization performance as a function of the
training set size for traditional backprop and tangent prop are compared in Fig-
ure . We have conducted additional experiments in which we implemented not
only translations but also rotations, expansions and hyperbolic deformations.
This set of  generators is a basis for all linear transformations of coordinates
for two dimensional images. It is straightforward to implement other generators
including gray-level-shifting, \smooth" segmentation, local continuous coordi-
nate transformations and independent image segment transformations.

The next experiment is designed to show that in applications where data is
highly correlated, tangent prop yields a large speed advantage. Since the dis-
tortion model implies adding lots of highly correlated data, the advantage of
tangent prop over the distortion model becomes clear.

The task is to approximate a function that has plateaus at three locations.
We want to enforce local invariance near each of the training points (Figure ,

60

50

40

30

20

10

%Error on
the test set

Backprop

Tangent Prop

0

10

20

40

80

160

320

Training set size

Fig. . Generalization performance curve as a function of the training set size for the
tangent prop and the backprop algorithms

bottom). The network has one input unit,  hidden units and one output unit.
Two strategies are possible: either generate a small set of training points covering
each of the plateaus (open squares on Figure  bottom), or generate one training
point for each plateau (closed squares), and enforce local invariance around them
(by setting the desired derivative to ). The training set of the former method
is used as a measure of performance for both methods. All parameters were
adjusted for approximately optimal performance in all cases. The learning curves
for both models are shown in Figure  (top). Each sweep through the training
set for tangent prop is a little faster since it requires only  forward propagations,
while it requires 	 in the distortion model. As can be seen, stable performance is
achieved after  sweeps for the tangent prop, versus  for the distortion
model. The overall speedup is therefore about .

Tangent prop in this example can take advantage of a very large regulariza-
tion term. The distortion model is at a disadvantage because the only parameter
that e(cid:11)ectively controls the amount of regularization is the magnitude of the dis-
tortions, and this cannot be increased to large values because the right answer
is only invariant under small distortions.

. How to make tangent prop work

Large network capacity: Not many experiments have been done with tangent
propagation. It is clear, however, that the invariance constraint is extremely
strong. If the network does not have enough capacity, it will not bene(cid:12)t from
the extra knowledge introduced by the invariance.

Average NMSE vs age

Average NMSE vs age

.15

.1

0

1000 2000 3000 4000 5000 6000 7000 8000 9000 10000

0

0

1000 2000 3000 4000 5000 6000 7000 8000 9000 10000

output

output

1.5

1

.5

0

.5

1

0.15

0.1

0

1.5

1

0.5

0

0.5

1

1.5

1.5

1

0.5

0

0.5

1

1.5

1.5

1.5

1

.5

0

.5

1

1.5

Distortion model

Tangent prop

Fig. . Comparison of the distortion model (left column) and tangent prop (right
column). The top row gives the learning curves (error versus number of sweeps through
the training set). The bottom row gives the (cid:12)nal input-output function of the network;
the dashed line is the result for unadorned back prop.

Interleaving of the tangent vectors: Since the tangent vectors introduce
even more correlation inside the training set, a substantial speed up can be
obtained by alternating a regular forward and backward propagation with a
tangent forward and backward propagation (even if there are several tangent
vectors, only one is used at each pattern). For instance, if there were  tangent
vectors, the training sequence could be:

x; t(x); x; t(x); x; t(x); x; t(x); x; t(x); : : :

()

where xi means a forward and backward propagation for pattern i and tj(xi)
means a tangent forward and backward propagation of tangent vector j of pat-
tern i. With such interleaving, the learning converges faster than grouping all the
tangent vectors together. Of course, this only makes sense with on-line learning.

 Tangent Vectors

In this section, we consider the general paradigm for transformation invariance
and for the tangent vectors which have been used in the two previous sections.
Before we introduce each transformation and their corresponding tangent vec-
tors, a brief explanation is given of the theory behind the practice. There are
two aspects to the problem. First it is possible to establish a formal connection
between groups of transformations of the input space (such as translation, rota-
tion, etc. of <) and their e(cid:11)ect on a functional of that space (such as a mapping
of < to <, which may represent an image, in continuous form). The theory of
Lie groups and Lie algebra [] allows us to do this. The second problem has to do
with coding. Computer images are (cid:12)nite vectors of discrete variables. How can
a theory which was developed for di(cid:11)erentiable functional of < to < be applied
to these vectors?

We (cid:12)rst give a brief explanation of the theorems of Lie groups and Lie al-
gebras which are applicable to pattern recognition. Next, we explore solutions
to the coding problem. Finally some examples of transformation and coding are
given for particular applications.

. Lie groups and Lie algebras

Consider an input space I (the plane < for example) and a di(cid:11)erentiable func-
tion f which maps points of I to <.

f : X  B (cid:0)! f (X)  <

(	)

The function f (X) = f (x; y) can be interpreted as the continuous (de(cid:12)ned for
all points of <) equivalent of the discrete computer image P [i; j].

Next, consider a family of transformations t(cid:11), parameterized by (cid:11), which

maps bijectively a point of I to a point of I.

t(cid:11) : X  I (cid:0)! t(cid:11)(X)  I

()

We assume that t(cid:11) is di(cid:11)erentiable with respect to (cid:11) and X, and that t is the
identity. For example t(cid:11) could be the group of a(cid:14)ne transformations of <:

t(cid:11) : (cid:18) x

y(cid:19) (cid:0)! (cid:18) x + (cid:11)x + (cid:11)y + (cid:11)

(cid:11)x + y + (cid:11)y + (cid:11)(cid:19) with

(cid:11)

(cid:12)(cid:12)(cid:12)(cid:12)
y(cid:19) (cid:0)! (cid:18) x cos (cid:18) (cid:0) y sin (cid:18) + a
x sin (cid:18) + y cos (cid:18) + b(cid:19)

t(cid:11) : (cid:18) x

Its a Lie group of  parameters. Another example is the group of direct isom-
etry:

 + (cid:11) (cid:11)

 + (cid:11)(cid:12)(cid:12)(cid:12)(cid:12)

= 

()

()

Its a Lie group of  parameters.

 A Lie group is a group that is also a di(cid:11)erentiable manifold such that the di(cid:11)eren-

tiable structure is compatible with the group structure.

We now consider the functional s(f; (cid:11)), de(cid:12)ned by

s(f; (cid:11)) = f (cid:14) t(cid:0)
(cid:11)

()

This functional s, which takes another functional f as an argument, should
remind the reader of Figure  where P , the discrete equivalent of f , is the
argument of s.

The Lie algebra associated with the action of t(cid:11) on f is the space generated

by the m local transformations L(cid:11)i of f de(cid:12)ned by:

We can now write the local approximation of s as:

Lai(f ) =

@s(f; (cid:11))

@(cid:11)i

(cid:12)(cid:12)(cid:12)(cid:12)(cid:11)=

()

s(f; (cid:11)) = f + (cid:11)L(cid:11)(f ) + (cid:11)L(cid:11)(f ) + : : : + (cid:11)mL(cid:11)m(f ) + o(k(cid:11)k)(f )

()

This equation is the continuous equivalent of equation () used in the introduc-
tion.

The following example illustrates how L(cid:11)i can be computed from t(cid:11). Lets
consider the group of direct isometry de(cid:12)ned in equation () (with parameter
(cid:11) = ((cid:18); a; b) as before, and X = (x; y)).

s(f; (cid:11))(X) = f ((x (cid:0) a) cos (cid:18) + (y (cid:0) b) sin (cid:18); (cid:0)(x (cid:0) a) sin (cid:18) + (y (cid:0) b) cos (cid:18)) ()

If we di(cid:11)erentiate around (cid:11) = (; ; ) with respect to (cid:18), we obtain

@s(f; (cid:11))

@(cid:18)

(X) = y

@f
@x

(x; y) + ((cid:0)x)

@f
@y

(x; y)

()

that is

L(cid:18) = y

@
@x

+ ((cid:0)x)

The transformation La = (cid:0) @
fashion. All local transformation of the group can be written as

@x and Lb = (cid:0) @

@
@y
@y can be obtained in a similar

()

s(f; (cid:11)) = f + (cid:18)(y

@f
@x

+ ((cid:0)x)

@f
@y

) (cid:0) a

@f
@x

(cid:0) b

@f
@y

+ o(k(cid:11)k)(f )

(	)

which corresponds to a linear combination of the  basic operators L(cid:18), La and
. The property which is most important to us is that the  operators generate
Lb
the whole space of local transformations. The result of applying the operators to
a function f , such as a D image for example, is the set of vectors which we have
been calling \tangent vector" in the previous sections. Each point in the tangent
space correspond to a unique transformation and conversely any transformation
of the Lie group (in the example all rotations of any angle and center together
with all translations) corresponds to a point in the tangent plane.

 These operators are said to generate a Lie algebra, because on top of the addition
and multiplication by a scalar, we can de(cid:12)ned a special multiplication called \Lie
bracket" and de(cid:12)ned by [L; L] = L (cid:14) L (cid:0) L (cid:14) L. In the above example we have
[L(cid:18); La] = Lb, [La; Lb] = , and [Lb; L(cid:18)] = La.

. Tangent vectors

The last problem which remains to be solved is the problem of coding. Computer
images, for instance, are coded as a (cid:12)nite set of discrete (even binary) values.
These are hardly the di(cid:11)erentiable mappings of I to < which we have been
assuming in the previous subsection.

To solve this problem we introduce a smooth interpolating function C which
maps the discrete vectors to continuous mapping of I to <. For example, if P is
a image of n pixels, it can be mapped to a continuously valued function f over
< by convolving it with a two dimensional Gaussian function g(cid:27) of standard
deviation (cid:27). This is because g(cid:27) is a di(cid:11)erentiable mapping of < to <, and P
can be interpreted as a sum of impulse functions. In the two dimensional case
we can write the new interpretation of P as:

P (x; y) = Xi;j

P [i][j](cid:14)(x (cid:0) i)(cid:14)(y (cid:0) j)

()

where P [i][j] denotes the (cid:12)nite vector of discrete values, as stored in a computer.
The result of the convolution is of course di(cid:11)erentiable because it is a sum of
Gaussian functions. The Gaussian mapping is given by:

C(cid:27) : P (cid:0)! f = P  (cid:3) g(cid:27)

In the two dimensional case, the function f can be written as:

f (x; y) = Xi;j

P [i][j]g(cid:27)(x (cid:0) i; y (cid:0) j)

()

()

Other coding function C can be used, such as cubic spline or even bilinear
interpolation. Bilinear interpolation between the pixels yields a function f which
is di(cid:11)erentiable almost everywhere. The fact that the derivatives have two values
at the integer locations (because the bilinear interpolation is di(cid:11)erent on both
side of each pixels) is not a problem in practice (just choose one of the two
values).

The Gaussian mapping is preferred for two reasons: First, the smoothing
parameter (cid:27) can be used to control the locality of the invariance. This is because
when f is smoother, the local approximation of equation () is better (it is valid
for large transformation). And second, when combined with the transformation
operator L, the derivative can be applied on the closed form of the Gaussian
function. For instance, if the X-translation operator L = @
@x is applied to f =
P  (cid:3) g(cid:27), the actual computation becomes:

LX (f ) =

@
@x

(P  (cid:3) g(cid:27)) = P  (cid:3)

@g(cid:27)
@x

()

because of the di(cid:11)erentiation properties of convolution when the support is com-
pact. This is easily done by convolving the original image with the X-derivative

=

=

=

*

*

*

Fig. . Graphic illustration of the computation of f and two tangent vectors corre-
sponding to Lx = @=@x (X-translation) and Lx = @=@y (Y-translation), from a binary
x
+y
image I. The Gaussian function g(x; y) = exp((cid:0)
(cid:27) ) has a standard deviation of
(cid:27) = :	 in this example although its graphic representation (small images on the right)
have been rescaled for clarity.

of the Gaussian function g(cid:27). This operation is illustrated in Figure . Similarly,
the tangent vector for scaling can be computed with

LS(f ) = (cid:18)x

@
@x

+ y

@

@y(cid:19) (I (cid:3) g(cid:27)) = x(I (cid:3)

@g(cid:27)
@x

) + y(I (cid:3)

@g(cid:27)
@y

)

()

This operation is illustrated in Figure .

. Important transformations in image processing

This section summarizes how to compute the tangent vectors for image process-
ing (in D). Each discrete image Ii is convolved with a Gaussian of standard
deviation g(cid:27) to obtain a representation of the continuous image fi, according to
equation:

fi = Ii (cid:3) g(cid:27):

()

The resulting image fi will be used in all the computations requiring Ii (except
for computing the tangent vector). For each image Ii, the tangent vectors are
computed by applying the operators corresponding to the transformations of
interest to the expression Ii (cid:3) g(cid:27). The result, which can be precomputed, is an
image which is the tangent vector. The following list contains some of the most
useful tangent vectors:

x

x

=

=

Fig. . Graphic illustration of the computation of the tangent vector Tu = DxSx +
DySy (bottom image). In this example the displacement for each pixel is proportional
to the distance of the pixel to the center of the image (Dx(x; y) = x(cid:0)x and Dy(x; y) =
y (cid:0) y). The two multiplications (horizontal lines) as well as the addition (vertical right
column) are done pixel to pixel.

X-translation: This transformation is useful when the classi(cid:12)cation function

is know to be invariant with respect to the input transformation:

t(cid:11) : (cid:18) x

y (cid:19)
y(cid:19) (cid:0)! (cid:18) x + (cid:11)

The Lie operator is de(cid:12)ned by:

LX =

@
@x

()

()

Y-translation: This transformation is useful when the classi(cid:12)cation function

is know to be invariant with respect to the input transformation:

t(cid:11) : (cid:18) x

y(cid:19) (cid:0)! (cid:18) x

y + (cid:11)(cid:19)

The Lie operator is de(cid:12)ned by:

LY =

@
@y

()

(	)

Rotation: This transformation is useful when the classi(cid:12)cation function is know

to be invariant with respect to the input transformation:

t(cid:11) : (cid:18) x

y(cid:19) (cid:0)! (cid:18) x cos (cid:11) (cid:0) y sin (cid:11)
x sin (cid:11) + y cos (cid:11)(cid:19)

()

The Lie operator is de(cid:12)ned by:

LR = y

@
@x

+ ((cid:0)x)

@
@y

()

Scaling: This transformation is useful when the classi(cid:12)cation function is know

to be invariant with respect to the input transformation:

t(cid:11) : (cid:18) x

y(cid:19) (cid:0)! (cid:18) x + (cid:11)x
y + (cid:11)y(cid:19)

The Lie operator is de(cid:12)ned by:

LS = x

@
@x

+ y

@
@y

()

()

Parallel hyperbolic transformation: This transformation is useful when the
classi(cid:12)cation function is know to be invariant with respect to the input trans-
formation:

()

()

t(cid:11) : (cid:18) x

y(cid:19) (cid:0)! (cid:18) x + (cid:11)x
y (cid:0) (cid:11)y(cid:19)

The Lie operator is de(cid:12)ned by:

LS = x

@
@x

(cid:0) y

@
@y

Diagonal hyperbolic transformation: This transformation is useful when
the classi(cid:12)cation function is know to be invariant with respect to the in-
put transformation:

t(cid:11) : (cid:18) x

y(cid:19) (cid:0)! (cid:18) x + (cid:11)y
y + (cid:11)x(cid:19)

The Lie operator is de(cid:12)ned by:

LS = y

@
@x

+ x

@
@y

()

()

The resulting tangent vector is is the norm of the gradient of the image,
which is very easy to compute.

Thickening: This transformation is useful when the classi(cid:12)cation function is
know to be invariant with respect to variation of thickness. This is know
in morphology as dilation, or erosion. It is very useful in certain domain,
such as handwritten character recognition because thickening and thinning
are natural variations which correspond to the pressure applied on a pen, or
to di(cid:11)erent absorbtion property of the ink on the paper. A dilation (resp.
erosion) can be de(cid:12)ned as the operation of replacing each value f(x,y) by
the largest (resp. smallest) value of f (x; y) for (x; y) within a region of a
certain shape, centered at (x; y). The region is called the structural element.
We will assume that the structural element is a sphere of radius (cid:11). We de(cid:12)ne

the thickening transformation as the function which takes the function f and
generates the function f 

(cid:11) de(cid:12)ned by:

f 
(cid:11)(X) = max
krk<(cid:11)

f (X + r)

for (cid:11) (cid:21) 

f 
(cid:11)(X) = min

krk<(cid:0)(cid:11)

f (X + r)

for (cid:11) < 

The derivative of the thickening for (cid:11) >=  can be written as:

lim
(cid:11)(cid:0)!

f (X) (cid:0) f (X)

(cid:11)

= lim
(cid:11)(cid:0)!

maxkrk<(cid:11) f (X + r) (cid:0) f (X)

(cid:11)

()

(	)

()

f (X) can be put within the max expression because it does not depend on
krk. Since k(cid:11)k tends toward , we can write:

f (X + r) (cid:0) f (X) = r:rf (X) + O(krk) (cid:25) r:rf (X)

()

The maximum of

max
krk<(cid:11)

f (X + r) (cid:0) f (X) = max
krk<(cid:11)

r:rf (X)

is attained when r and rf (X) are co-linear, that is when

r = (cid:11)

rf (X)

krf (X)k

()

()

assuming (cid:11) > . It can easily be shown that this equation holds when (cid:11) is
negative, because we then try to minimize equation (	). We therefore have:

lim
(cid:11)(cid:0)!

f 
(cid:11)(X) (cid:0) f (X)

(cid:11)

= krf (X)k

()

Which is the tangent vector of interest. Note that this is true for (cid:11) positive
or negative. The tangent vectors for thickening and thinning are identical.
Alternatively, we can use our computation of the displacement r and de(cid:12)ne
the following transformation of the input:

where

t(cid:11)(f ) : (cid:18) x

y(cid:19) (cid:0)! (cid:18) x + (cid:11)rx
y + (cid:11)ry (cid:19)

(rx; ry) = r = (cid:11)

rf (X)

krf (X)k

()

()

This transformation of the input space is di(cid:11)erent for each pattern f (we
do not have a Lie group of transformation, but the (cid:12)eld structure generated
by the (pseudo Lie) operator is still useful. The operator used to (cid:12)nd the
tangent vector is de(cid:12)ned by:

LT = krk

()

which means that the tangent vector image is obtained by computing the
normalized gray level gradient of the image at each point (the gradient at
each point is normalized).

The last  transformation are depicted in Figure  with the tangent vector.
The last operator corresponds to a thickening or thinning of the image. This un-
usual transformation is extremely useful for handwritten character recognition.

Scaling

Rotation

Axis deformation

Diagonal
deformation

Thickness
deformation

Fig. . Illustration of  tangent vectors (top), with corresponding displacements (mid-
dle) and transformation e(cid:11)ects (bottom). The displacement Dx and Dy are represented
in the form of vector (cid:12)eld. It can be noted that the tangent vector for the thickness
deformation (right column) correspond to the norm of the gradient of the gray level
image.

 Conclusion

The basic Tangent Distance algorithm is quite easy to understand and imple-
ment. Even though hardly any preprocessing or learning is required, the perfor-
mance is surprisingly good and compares well to the best competing algorithms.
We believe that the main reason for this success is its ability to incorporate a
priori knowledge into the distance measure. The only algorithm which performed
better than Tangent Distance on one of the three databases was boosting, which
has the similar a priori knowledge about transformations built into it.

Many improvements are of course possible. First, a smart preprocessing can
allow us to measure the Tangent Distance in a more appropriate \feature" space,
instead of the original pixel space. In image classi(cid:12)cation, for example, the fea-
tures could be horizontal and vertical edges. This would most likely further
improve the performance The only requirement is that the preprocessing must

 There may be an additional cost for computing the tangent vectors in the feature

space if the feature space is very complex.

be di(cid:11)erentiable, so that the tangent vectors can be computed (propagated) into
the feature space.

Second, the Tangent Vectors and the prototypes can be learned from the
training data, rather then chosen a priori. In applications such as speech, where
it is not clear to what transformation the classi(cid:12)cation is invariant, the ability to
learn the Tangent Vector is a requirement. It is straightforward to modify algo-
rithms such as LVQ (Learning Vector quantization) to use a tangent distance. It
is also straightforward to derive an batch [] or on-line [] algorithm to train
the tangent vectors

Finally, many optimizations which are commonly used in distance based al-
gorithms can be used as successfully with Tangent Distance to speed up com-
putation. The multi-resolution approach have already been tried successfully
[]. Other methods like \multi-edit-condensing" [, ] and K-d tree [] are also
possible.

The main advantage of Tangent Distance is that it is a modi(cid:12)cation of a
standard distance measure to allow it to incorporate a priori knowledge that is
speci(cid:12)c to your problem. Any algorithms based on a common distance measure
(as it is often the case in classi(cid:12)cation, vector quantization, predictions, etc...)
can potentially bene(cid:12)t from a more problem-speci(cid:12)c distance. Many of this \dis-
tance based" algorithms do not require any learning, which means that they can
be adapted instantly by just adding new patterns in the database. These addi-
tions are leveraged by the a-priori knowledge put in the tangent distance.

The two drawbacks of tangent distance are its memory and computational
requirements. The most computationally and memory e(cid:14)cient algorithms gen-
erally involve learning []. Fortunately, the concept of tangent vectors can also
be used in learning. This is the basis for the tangent propagation algorithm.
The concept is quite simple: instead of learning a classi(cid:12)cation function from
examples of its values, one can also use information about its derivatives. This
information is provided by the tangent vectors. Unfortunately, not many exper-
iments have been done in this direction. The two main problems with tangent
propagation are that the capacity of the learning machine has to be adjusted
to incorporate the additional information pertinent to the tangent vectors, and
that training time must be increased. After training, the classi(cid:12)cation time and
complexity are unchanged, but the classi(cid:12)ers performance is improved.

To a (cid:12)rst approximation, using Tangent Distance or Tangent propagation is
like having a much larger database. If the database was plenty large to begin
with, tangent distance or tangent propagation would not improve the perfor-
mance. To a better approximation, tangent vector are like using a distortion
model to magnify the size of the training set. In many cases, using tangent vec-
tor will be preferable to collecting (and labeling!) vastly more training data,
and preferable (especially for memory-based classi(cid:12)ers) to dealing with all the
data generated by the distortion model. Tangent vectors provide a compact and
powerful representation of a-priori knowledge which can easily be integrated in
the most popular algorithms.

