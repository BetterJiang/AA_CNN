Information Genealogy: Uncovering the Flow of Ideas in

Non-Hyperlinked Document Databases

Benyah Shaparenko

Department of Computer Science

Cornell University
Ithaca, NY 14853

benyah@cs.cornell.edu

Thorsten Joachims

Department of Computer Science

Cornell University
Ithaca, NY 14853
tj@cs.cornell.edu

ABSTRACT
We now have incrementally-grown databases of text docu-
ments ranging back for over a decade in areas ranging from
personal email, to news-articles and conference proceedings.
While accessing individual documents is easy, methods for
overviewing and understanding these collections as a whole
are lacking in number and in scope. In this paper, we ad-
dress one such global analysis task, namely the problem of
automatically uncovering how ideas spread through the col-
lection over time. We refer to this problem as Information
Genealogy.
In contrast to bibliometric methods that are
limited to collections with explicit citation structure, we in-
vestigate content-based methods requiring only the text and
timestamps of the documents. In particular, we propose a
language-modeling approach and a likelihood ratio test to
detect inﬂuence between documents in a statistically well-
founded way. Furthermore, we show how this method can
be used to infer citation graphs and to identify the most
inﬂuential documents in the collection. Experiments on the
NIPS conference proceedings and the Physics ArXiv show
that our method is more eﬀective than methods based on
document similarity.
Categories and Subject Descriptors
H.4 [Information Systems Applications]: Miscellaneous
General Terms
Algorithms, Measurement, Performance
Keywords
Information Genealogy, Flow of Ideas, Language Models,
Citation Inference, Text Mining, Temporal Data

1.

INTRODUCTION

In many domains, complete electronic records of docu-
ments now reach back for over a decade, including computer
science research papers, US news articles, and most peo-
ple’s personal email. These databases incrementally grow

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
KDD’07, August 12–15, 2007, San Jose, California, USA.
Copyright 2007 ACM 978-1-59593-609-7/07/0008 ...$5.00.

through an “evolutionary” process, where new documents
are inﬂuenced by the content of already existing documents.
For example, scientiﬁc documents extend existing ideas, newssto-
ries reﬁne and comment on other articles, and emails aggre-
gate or respond to other emails.

While keyword-based retrieval systems allow eﬃcient ac-
cess to individual documents in such corpora, we yet lack
methods to understand the corpus as a whole. To rem-
edy this shortcoming, this paper investigates whether it is
possible to uncover the temporal dependency structure of a
corpus. Which documents inﬂuenced each other? How did
ideas spread through the corpus over time? Which docu-
ments (or authors) were most inﬂuential? While many of
these question have been addressed for hyperlinked data
with explicit citation structure, explicit citations are not
available in most domains. We therefore aim to address
these questions based on the (textual) content of the docu-
ments alone.

The premise for this research is that ideas manifest them-
selves in statistical properties of a document (e.g. the dis-
tribution of words), and that these properties can act as
a signature for an idea which can be traced through the
database. Following this premise, we present a probabilistic
model of inﬂuence between documents and design a content-
based signiﬁcance test to detect whether one document was
inﬂuenced by an idea ﬁrst presented in another document.
The test takes the form of a Likelihood Ratio Test (LRT)
and leads to a convex programming problem that can be
solved eﬃciently. Our goal is to use this test for inferring
an inﬂuence graph derived from the text of the documents
alone. Analogous to detecting inheritance from genes, we
refer to this text-mining problem as Information Genealogy.
Using corpora of scientiﬁc literature, we show that it is in-
deed possible to infer meaningful inﬂuence graphs from the
text of the documents. Evaluating against the explicit cita-
tion graphs for these corpora, we ﬁnd that the automatically-
computed inﬂuence graphs are similar to the citation graphs.
The ablility to automatically generate an inﬂuence graph for
a collection enables a range of applications, from browsing,
to visualizing and mining the structure of the network. As
a simple example, we demonstrate that the in-degree of the
inﬂuence graph provides an interesting measure of document
impact, similar to the in-degree of the citation graph.

2. MEASURING INFLUENCE

In this paper, we investigate and operationalize the no-
tion of inﬂuence between documents. Inﬂuence is an inter-
esting relationship between documents in historically grown

databases, since such corpora have grown through a self-
referential process: documents are inﬂuenced by the con-
tent of prior documents, but also contribute new ideas which
in turn inﬂuence later documents. Our goal is to uncover
and mine how ideas introduced in some document spread
through the corpus over time.

At ﬁrst glance, one might think that similarity, as cap-
tured by information retrieval metrics like TFIDF cosine
similarity (see e.g.
[32]), provides the full picture of inﬂu-
ence. However, this is not the case.

On the one hand, similarity can occur without inﬂuence.
First, if a document d(1) introduces an idea that is picked up
in documents d(2) and d(3), then d(2) and d(3) will likely be
similar but do not necessarily inﬂuence each other. Second,
two documents might concurrently propose the same idea.
Again, neither document inﬂuences the other although the
documents likely are similar.

On the other hand, inﬂuence can occur with very little
similarity. In the scientiﬁc literature, for example, a large
textbook might devote a section to an idea introduced in an
earlier research paper. Clearly, the paper had inﬂuence on
the textbook. However, the overall similarity between the
book and the paper is small, since the book covers many
other ideas as well.

As we will brieﬂy review in the following, most prior work
on analyzing temporal corpora has focused on identifying
relatedness between documents, not inﬂuence. We will then
develop a probabilistic model and a statistical test for de-
tecting inﬂuence, and show that it captures inﬂuence better
than similarity and provides a more complete understanding
and model of inﬂuence.
2.1 Topic Detection and Tracking

Topic Detection and Tracking (TDT) [5, 6] has the goal
of grouping documents by topic. Unlike inﬂuence, which
is a directed relationship, TDT aims to group documents
into equivalence classes. While TDT approaches have relied
heavily on ﬁnding similarity measures that capture closeness
in topic, this approach is not necessarily detecting inﬂuence,
as we have argued above. Methods that model inﬂuence
not only can detect and track topics and ideas, but also
can provide reference points for why a document collection
developed as it did. Another minor diﬀerence is that the
TDT studies were performed in an online setting, while we
assume access to the full corpus at any time.

Similar work on detecting and visualizing topic develop-
ment includes visualization methods such as Temporal Clus-
ter Histograms [34] and ThemeRiver [15], EM-based cor-
pus evolution detection [29], temporal clustering methods [7,
37], continuous time clustering models [37], Thread Decom-
position [14], Independent Component Analysis [22], topic-
intensity tracking [23], and Topical Precedence [27].
2.2 Real-World Inﬂuence on Documents

Research on Burst Detection [21] and TimeMines [36] aims
to identify hidden causes based on changes in the word dis-
tribution over time. However, their notion of inﬂuence is
diﬀerent from ours. These approaches determine inﬂuence
from real-world events on topics (e.g., events inﬂuencing US
State of the Union Addresses). Instead, we model the inﬂu-
ence of documents on each other.

2.3 Citation and Hyperlink Analysis

In bibliometrics, a document’s inﬂuence is measured through

properties of the citation graph [30, 31, 20, 12]. Our work
diﬀers from citation analysis because our method is based
on document content, not on citations. We assume that in-
ﬂuence is inherently reﬂected in the statistical properties of
documents. In particular, we conjecture that when one doc-
ument inﬂuences another, the inﬂuenced document shows
traces of the word distribution of the original document1.
Besides bibliometrics’ consideration of citation analysis on
research papers, other methods work on general hyperlink
structure. One of the most well-known such methods is
PageRank [31], which uses hyperlink structure to ﬁnd in-
ﬂuential Web pages.
2.4 Automatic Hypertext

There is related work on automatically adding hyperlinks
in information retrieval and related ﬁelds. Most promi-
nently, Link Detection was a key task in the TDT evalu-
ations [5]. Several proposals and methods exist for intro-
ducing hyperlinks between similar documents or passages of
documents [11, 10, 33, 26, 2, 4, 3, 24, 25]. Good surveys
are given in [38] and the 1997 special issue of Information
Processing and Management [1]. The work we propose is
diﬀerent in several respects. First, our goal is to detect
inﬂuence between documents, not just their “relatedness.”
This will allow a causal interpretation of the resulting cita-
tion graph. Second, we take a statistical testing approach
to the problem of identifying inﬂuence links, which can be
seen as synonymous to citations. This will give a formal
semantic to the predictions of the methods, give theoretical
guidance on how to apply the methods, and expose under-
lying assumptions.
2.5 Language and Topic Models

We take a probabilistic language modeling approach in
the development of our methods. While we rely on a rather
basic language model for the sake of simplicity, more de-
tailed language models exist and can possibly be employed
as well. Previous work by Steyvers et al. [35] looks at how
document text can be generated by a two-step model of gen-
erating topics probabilistically from authors, and then words
probabilistically from topics. There has also been language
modeling work done in the natural language processing and
machine learning [28, 16, 8], speech recognition [19], and
information retrieval communities [39, 24, 25].

3. METHODS

In constructing an inﬂuence graph for a database of doc-
uments, the core problem is to determine when and where
ideas ﬂow from one document to another document. In the
following, we propose a probabilistic model of inﬂuence in
a language-modeling framework, and develop a Likelihood
Ratio Test (LRT) [9] for detecting whether one document
has signiﬁcantly inﬂuenced another document.
3.1 Probabilistic Model and Motivation

To make the method widely applicable, we have only two
basic requirements for our corpus of documents — ﬁrst, the
documents contain text and, second, the documents have

1Note that our goal is not plagiarism detection, where au-
thors would try to disguise their choice of words.

timestamps. Formally, the corpus D is a collection of n doc-
uments {D(1) ··· D(n)}, where each document D(i) ∈ D has
an associated timestamp time(D(i)). There are m diﬀer-
ent terms (i.e. words) across the entire corpus, which are
denoted by {t1 ··· tm}.
We assume that the document is a vector-valued random
variable D = (W1 ··· W|D|), which describes a document as
a sequence of random variables Wi, one for each word in
the document. A particular observed document is denoted
as d = (w1 ··· w|d|). In the following, we assume that each
document D(i) ∈ D was generated by a unigram language
model P (D(i) = d(i) | θ(i)) with parameters θ(i) speciﬁc to
that document.

Model 1. (Document Language Model)
A document D(i) ∈ D is assumed to be generated by
independently drawing |D(i)| words from a document spe-
ciﬁc distribution with individual word probabilities θ(i) =
(θ(i)
t1 , ..., θ(i)
P (D(i) = d(i) | θ(i)) = P (D(i) = (w(i)

tm ), i.e.

1 ··· w(i)

|D(i)|) | θ(i))

|D(i)|Y
|D(i)|Y

j=1

=

=

P (W (i) = w(i)

j

| θ(i))

θ(i)
wj

j=1

Note that we do not explicitly model document length.
We chose this basic language model for mathematical and
computational convenience. However, our approach can be
extended to more complex language models as well (e.g. n-
gram models).

Since we wish to detect the ﬂow of ideas and inﬂuence
between documents, we also need a model of inter-document
relationship. We formalize this as a question of how the
language model θ(new) of a new document D(new) depends
on the language models {θ(1) ···} of the documents that
precede θ(new) in time. In particular, we assume that the
language model of a new document can be (approximately)
expressed as a mixture distribution over the language models
of previous documents.

Model 2. (Inter-Document Influence Model)

A new document D(new) is generated by a mixture distri-
bution of the already existing documents D(i) with i ∈ P for
P = {i : time(D(i)) < t0}, in particular

P(D(new)= d(new)|π) =

with mixing weights π satisfying 0 ≤ πi andP

πpP(W (p)= wj|θ(p))

p∈P

j=1

πi = 1.

(1)

i

In this dependency model, a new document is composed
of parts generated by the word distributions of old docu-
ments, where the mixing coeﬃcient πp indicates the frac-
tion of D(new) that is generated from D(p). Clearly, there is
direct inﬂuence of a document D(p) on D(new), if the respec-
tive mixing coeﬃcient is non-zero. Note that the resulting
language model for D(new) is again a unigram model, so that
P (D(new)= d(new) | π) = P (D(new)= d(new) | θ(new)) with

|D(new)|Y

X

X

p∈P

In actual document collections, documents typically con-
tain some original part that does not come from previous
documents. To account for the original portion of a docu-
ment in our model, we include a distribution θ(o) with weight
πo in the mixture. It models the distribution of words that
is original to the document and that cannot be explained by
previous documents. (In practice, we will assume that πo is
ﬁxed, but that we have no knowledge of θ(o).

Model 3. (Inter-Document Influence Model with

Original Content)
A new document D(new) is generated by a mixture distri-
bution of the already existing documents D(i) with i ∈ P for
P = {i : time(D(i)) < t0}, and a document speciﬁc mixture
component θ(o) with weight πo, in particular

|D(new)|Y

X

P(D(new)= d(new)|π) =

with mixing weights π s.t. 0 ≤ πi, πo and πo +P

p∈P∪{o}

πpP(W (p)= wj|θ(p))

j=1

πi = 1.

(3)

i

In the case when the documents have no original content,
setting πo = 0 in the Inter-Document Inﬂuence Model with
Original Content results in Model 2. Vice versa, Model 2
also subsumes Model 3 by simply introducing an artiﬁcial
single-word document for each term in the corpus and con-
straining their mixture weights to sum to πo. We will there-
fore focus our further derivations on Model 2 for the sake of
simplicity.

We will now show how this probabilistic setup can be
used in a signiﬁcance test for detecting whether a particular
mixing weight πp is non-zero in a given document collection.
3.2 A Statistical Test for Detecting Inﬂuence
How can one decide whether a candidate inﬂuential doc-
ument d(can) had a signiﬁcant inﬂuence on d(new) given
the other documents in the collection? First, d(can) can
only have had an inﬂuence on d(new) if it had been pub-
lished before d(new) (i.e. time(d(can)) < time(d(new))). Note
that this is already encoded in the Inter-Document Inﬂu-
ence Models deﬁned above. Second, inﬂuence should be
attributed to the ﬁrst publication that introduced an idea
through an original section or portion, not to other docu-
ments that later copied an idea. To illustrate this in the
context of research papers, this means that inﬂuence should
be credited to the original article, not a tutorial that repro-
duced the original idea.

Under these conditions, the decision of whether document
d(new) shows signiﬁcant inﬂuence from d(can) can be phrased
as a Likelihood Ratio Test [9]. In general, a Likelihood Ratio
Test decides between two families of densities described by
sets of parameters Π and Π0 that are nested, i.e. Π0 ⊂
Π. Applied to our case, Π will be all mixture models of
D(new) as in Eq. (1) with parameters πi for all documents
P published prior to t0 = time(d(can)) (and therefore prior
to d(new)), as well as a parameter πcan for d(can).

8<:π :

Π =

X

i∈P∪{can}

πi = 1 ∧ πi ≥ 0

9=;

θ(new) =

πpθ(p).

(2)

The subset Π0 of the mixture models in Π will be the models
where d(can) has zero mixture weight (i.e. πcan = 0).

8<:π :

Π0 =

X

i∈P∪{can}

πi = 1 ∧ πi ≥ 0 ∧ πcan = 0

9=;

Note that the set of prior documents P = {i : time(d(i)) <
time(d(can))} serves as a “background model” of what was
already known when d(can) was published. Against this
background, we can then measure how much the new ideas
in document d(can) inﬂuenced d(new).

The null hypothesis of the Likelihood Ratio test is that
the data comes from a model in Π0 (i.e. document d(new)
was not inﬂuenced by d(can) given the documents published
before d(can)). To reject this null hypothesis, a likelihood
ratio test considers the following test statistic

supπ∈Π0{P (D(new) = d(new)|π)}
supπ0∈Π{P (D(new) = d(new)|π0)}

Λd(can) (d(new)) =
Note that P (D(new) = d(new)|π) is convex over Π and Π0,
so that the suprema can be computed eﬃciently. We will
elaborate on the computational aspects below. Intuitively,
the value of Λd(can) (d(new)) measures whether using d(can) in
the mixture model better explains the content of d(new) than
just using previously published documents. More formally,
Λd(can) (d(new)) compares the likelihood supπ0∈Π{P (D(new) =
d(new)|π0)} of the best mixture model containing d(can) with
the likelihood supπ∈Π0{P (D(new) = d(new)|π)} of the best
mixture model that does not use d(can) (i.e. πcan = 0). The
test then decides whether there is signiﬁcant evidence that
a non-empty part of d(new) was generated from d(can), in
comparison to using a mixture only over the other language
models.
If the null hypothesis is true, then the distribution of the
LRT statistic −2 log(Λd(can) (d(new))) is asymptotically (in
the document length under the unigram model) χ2 with one
degree of freedom.

−2 log(Λd(can) (d(new))) ∼ χ2
The null hypothesis H0 should be rejected, if
−2 log(Λd(can) (d(new))) > c

1

for some c selected dependent on the desired signiﬁcance
level. For a signiﬁcance level of 95%, c should be 3.84. This
captures the intuition that we can reject the null hypothe-
sis and conclude that d(can) had a signiﬁcant inﬂuence on
d(new), if the best model that does not use d(can) has a
much worse likelihood than the best model that considers
d(can). Speciﬁcally, if −2 log(Λd(can) (d(new))) is large, then
d(can) signiﬁcantly inﬂuenced d(new) given all other docu-
ments published at that time.

To estimate the language models θ(i) of the documents en-
tering into the mixture model of d(new), we use the maximum-
likelihood estimate. We denote with tf (i) the term frequency
(TF) vector of document d(i), where each entry tf (i)
is the
number of times that term tj appears in the document d(i).
The estimator is

j

θ(i)
wj =

tf (i)
wj
|d(i)| ,

which is simply the fraction of times the particular word oc-
curs in the observed document d(i). Using a more advanced

estimator instead is straightforward, but we will not discuss
this for the sake of simplicity.
3.3 Relating the LRT to Detecting Inﬂuence

What does it mean for the LRT to signiﬁcantly reject
the null hypothesis? A good intuition is to think of this
method in the context of trying to explain the ideas and
content found in d(new). There are two choices. First, ex-
plain d(new) using only other documents preceding d(can) as
well as some original component. Second, explain d(new)
with these plus an additional d(can). If the ﬁrst case already
provides a wonderful model for d(new), then adding d(can)
will not explain d(new) any more accurately. Thus, d(can)
really does not contribute to d(new). On the other hand,
if d(can) introduced some new ideas and terminology that
then ﬂowed to d(new), using d(can) will provide a better ex-
planation than only using P. Consequently, the likelihood
of d(new) using d(can) will be signiﬁcantly higher than with-
out it, and we can reject the null hypothesis. To summarize,
rejecting the null hypothesis means that d(can) signiﬁcantly
exerted inﬂuence on d(new).
3.4 Computing the LRT

Computing the value of Λd(can) (d(new)) requires solving

two optimization problems.

L0 = sup
π∈Π0
L = sup
π∈Π

{P (D(new) = d(new)|π)} and
{P (D(new) = d(new)|π)}.

(4)

(5)

Given our model, these problems can be solved eﬃciently.
Note that we can write the log-likelihood L(π | d(new),S) of
the document d(new) w.r.t. a ﬁxed π as

log L(π | d(new)) = log P (d(new) | π)

mX

j=1

X

i∈S

=

tf (new)

j

log(

πiθ(i)

j ).

With S we denote the set of documents considered in the
model. This gives S = P∪{can} for Π and S = P for Π0. In
this notation, each of the optimization problems in Eq. (4)
and (5) takes the form

max
π∈<|S|

subject to

log L(π | d(new))

X

πi = 1

i∈S
∀i ∈ S : πi ≥ 0.

For Model 3 an additional linear constraint is introduced to
limit the amount of original content πo to not be more than
a user-speciﬁed parameter σ. This constraint is necessary,
since otherwise the θ(o) mixture component could always
perfectly explain d(new).

It is easy to see that these optimization problems are con-
vex, which means that they have no local optima and that
there are eﬃcient methods for computing the solution. We
currently use the separable convex implementation for the
general-purpose solver Mosek [18] to solve the optimization
problems. However, more specialized code is likely to be
substantially more eﬃcient.

While solving each optimization problem is eﬃcient, an-
alyzing a collection requires a quadratic number of LRTs,
each with on the order of n documents in the background

model. In particular, for each document d(new), we need to
test all prior documents

d(i) : time(d(i)) < time(d(new))

(6)

in the collection, since all of these are candidates for having
inﬂuenced d(new). For each document d(can) in the candidate
candidate set C of d(new), we then have a background model

n

C =

n

Pd(can) =

o

o

d(i) : time(d(i)) < time(d(can))

.

(7)

Computing all tests exhaustively for a large corpus can be
expensive. We therefore use the following approximations.
Both approximations are based on the insight that some
similarity is necessary for inﬂuence. The potentially inﬂuen-
tial document d(can) must have some similarity with d(new).
Therefore, we ﬁrst approximate the candidate set to con-
tain the kC nearest neighbors of d(new) from C. We use
cosine distance between TF and TFIDF vectors for docu-
ment similarity. Second, an analogous argument applies to
the background models Pd(can) . We therefore approximate
the background model, using only the kP most similar docu-
ments from P. Since selecting P combines document vectors
by addition, we use cosine distance between document TF
vectors to select P. In the experiments we set kC = kP and
refer to this parameter as k. We will empirically evaluate
the eﬀect of these approximations depending on k.

4. EXPERIMENTS

We wish to measure how well these models’ assumptions
match real data. First, how does an inﬂuence graph inferred
by the LRT method compare against a citation graph? Sec-
ond, can the inﬂuence graph identify top inﬂuential papers?
4.1 Experiment Setup and Corpora

The concept of inﬂuence and idea ﬂow between documents
corresponds well with the notion of a citation. Consequently,
we focus on research papers to provide a quantitative eval-
uation of the LRT method by comparing with citations.

The ﬁrst corpus is the full-text proceedings of the Neu-
ral Information Processing Systems (NIPS) conference [17]
from 1987-2000, with a timestamp of the publication year.
NIPS has 1955 documents, with 74731 terms (features). We
manually constructed the graph of 1512 intra-corpus cita-
tions, but only compare to citations of previous documents
in time. We ignore citations of ﬁrst-year documents since
the LRT requires a background model.

The second corpus is the theoretical high-energy physics
(HEPTH) section of the Physics ArXiv [13] from Aug. 1991
to Apr. 2006. We aggregate the full-text papers by year.
HEPTH has 39008 documents, 229194 terms, and 557582
citations. SLAC-SPIRES compiled these citations.
4.2

Inferring Inﬂuence Graphs

This set of experiments analyzes how well the LRT re-
covers the inﬂuence graph. After an illustrative example,
we explore the LRT’s sensitivity on synthetic data under
controlled experiment conditions, and then evaluate on two
real-world datasets.
4.2.1 Qualitative Evaluation
We ﬁrst discuss a simple example to illustrate the LRT
method’s behavior and how it compares to citations. Fig-
ure 2 shows those documents that NIPS document 1541

Figure 1: ROC-Area comparing the LRT method
against a cosine similarity baseline. The x-axis is
πcan. At a πcan level, the ROC-Area measures the
quality of inﬂuence prediction in documents with the
speciﬁed πcan as compared against documents with
πcan = 0.

(Schoelkopf et al. on “Shrinking the Tube: a New Sup-
port Vector Regression Algorithm”) most signiﬁcantly in-
ﬂuenced according to the LRT statistic. Three of the top
ﬁve papers actually cite document 1541 (or a document with
equivalent content from another venue). Furthermore, the
top document could arguably have cited 1541 as well, since
it relies on the ν-parameterization of SVMs that document
1541 introduced to NIPS. In fact, all papers (except “Fast
Training of Support Vector Classiﬁers”) consider this new
parameterization. Note that the paper “ν-arc: Ensemble
Learning in the Presence of Outliers” is not about SVMs,
but uses the ν-parametrization in the context of boosting.

The LRT appears to accurately focus on the paper’s origi-
nal contribution, the ν-parameterization. General SVM pa-
pers do not score highly, since they are already modeled by
earlier papers, e.g. paper 1217 “Support Vector Method for
Function Approximation, Regression Estimation, and Sig-
nal Processing” of V. Vapnik et al., which was one of the
ﬁrst SVM papers in NIPS. When considering inﬂuencers
of “A Support Vector Method for Clustering” by A. Ben-
Hur et al. (using the conventional parameterization), the
method correctly recognizes that paper 1541’s inﬂuence is
low (−2 log(Λd(1541) (d(new))) = 67.0) even though the docu-
ments are similar. Paper 1217 already “explains” the SVM
content (−2 log(Λd(1217) (d(new))) = 535.0).
4.2.2 Quantitative Evaluation on Synthetic Data
Beyond this qualitative example, how accurately can the
LRT discover inﬂuence? How much must d(new) copy from
d(can) before the LRT can detect it?

To explore these questions, we constructed artiﬁcial docu-
ments d(new) from the NIPS corpus as follows. A candidate
document d(can) and a set P of k = 100 previous documents
are chosen at random form the NIPS corpus so that the
documents in P preceed d(can) in time. Then, 101 artiﬁcial
new documents are generated according to Eq. 1, where each
new document has been inﬂuenced by d(can) at the fractional
levels of πcan ∈ {0.00, 0.01, 0.02,··· , 1.00}. The remaining
mixing weights πi are selected by generating random num-

 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1 0 0.2 0.4 0.6 0.8 1ROC-AreaCandidate document mixing weightLRT ROC-AreaSIM ROC-Area−2 log(Λd(1541)(d(new))) Cite? Title and Author(s) of d0

321.2455

no

221.8297

219.8769

184.5493

168.8972

yes

yes

no

yes

“Support Vector Method for Novelty Detection”, B. Schoelkopf, Robert C. Williamson,
Alex Smola, John Shawe-Taylor, John C. Platt.
“An Improved Decomposition Algorithm for Regression Support Vector Machines”, Pavel
Laskov.
“ν-arc: Ensemble Learning in the Presence of Outliers, Gunnar Raetsch”, B. Scholkopf,
Alex Smola, Kenneth D. Miller, Takashi Onoda, Steve Mims.
“Fast Training of Support Vector Classiﬁers”, Fernando Perez-Cruz, Pedro Alarcon-
Diana, Angel Navia-Vazquez, Antonio Artes-Rodriguez.
“Uniqueness of the SVM Solution”, Christopher J. C. Burges, David J. Crisp.

Figure 2: Papers that are inﬂuenced by NIPS paper 1541, “Shrinking the Tube: a New Support Vector
Regression Algorithm” written by B. Schoelkopf, P. Bartlett, A. Smola, and R. Williamson. The leftmost
column shows the LRT statistic value. (Larger LRT statistic values represent greater inﬂuence.)

bers uniformly on the interval [0, 1], and then normalizing
them so that they sum to 1 − πcan. The LRTs are run on
each new document. Additionally, TF document vector co-
sine similarity is measured between d(can) and each d(new).
The entire process is repeated for 1000 random selections of
P and d(can).
We computed ROC-Area in the following manner. First,
we select a particular πcan ∈ {0.01··· 1.00}. The generated
documents at the πcan level are marked as positive exam-
ples. The negative examples are documents with πcan = 0.
Finally, a ranking, either LRT statistic scores or cosine dis-
tance similarity, is used to compute ROC-Area.

Figure 1 shows that even if only a small portion (i.e. a
few percent) of d(new) is drawn from d(can), the LRT accu-
rately detects the inﬂuence. The similarity baseline needs a
much larger signal. This example illustrates that similarity
and inﬂuence are in fact diﬀerent, and that the well-founded
statistical approach can be more accurate and sensitive than
an ad-hoc heuristic.

4.2.3 Quantitative Evaluation on Real Data
Moving to real data, we use the LRTs to discover the in-
ﬂuence graph for NIPS and HEPTH. For each document
d(new), we ﬁrst compute a set of candidate documents C
based on similarity. The elements of C are then ranked ac-
cording to the LRT statistic (i.e. whether d(can) was signif-
icant in explaining d(new)). The higher d(can) is ranked, the
more likely that it inﬂuenced d(new), and we can derive the
inﬂuence graph by thresholding (discussed below).

We evaluate the inﬂuence graph by a graph-based mean-
average-precision (G-MAP) metric. For a document d, aver-
age the precision of the ranked predicted list of inﬂuencers
at the positions corresponding to documents that d actu-
ally cites. Citations not in the list are averaged as 0, i.e.
ranked at inﬁnity. (As an information retrieval analogy, the
inﬂuence list is the search result page, with citations being
relevant results.) G-MAP is the mean of the per-document
average precision scores. We exclude documents from the
ﬁrst two years due to edge eﬀects (the LRT cannot predict
citations for the ﬁrst years since C or P are empty).

We compare G-MAP for the LRT method against G-MAP
of a similarity-based heuristic, which serves as a baseline.
This baseline method ranks the elements of C not by LRT
score, but by similarity. We explored several similarity mea-
sures. The best similarity measures in our experiments are
TF cosine and TFIDF cosine. We report their performance.
Note that citations are not necessarily a perfect gold stan-

TF

TFIDF

G-MAP
NIPS

LRT
0.4489
HEPTH 0.2432

SIM

0.3948
0.2216

LRT
0.4531
0.2543

SIM

0.4412
0.2167

Table 1: G-MAP scores comparing the LRT against
the similarity baseline. The similarity measure to
select P is the TF cosine and to select/rank C is
either the TF cosine or the TFIDF cosine. Results
are reported for k = 100 and σ = 0.05.

TFIDF C for LRT with k = 100 and S = .05

Figure 3: Precision vs. Recall on NIPS. The three
lines are (from top to bottom) the LRT method’s
precision at a recall level with TFIDF cosine used
to select C, the TFIDF distance C similarity baseline,
and the TF distance C similarity baseline.

dard for inﬂuence, since they reﬂect idiosycracies of how sci-
entiﬁc communities cite prior work. For example, in Figure 2
authors sometimes cited a journal paper or book instead of
the NIPS paper. Therefore, a G-MAP of 1 is not achievable.

LRTs are more accurate than similarities.

Table 1 shows that the LRT achieves higher G-MAP scores
than the similarity baselines on both NIPS and HEPTH.
Among the two heuristic baselines, TFIDF cosine performs
better then TF cosine. TFIDF cosine also appears to select
better sets C for the LRT. The HEPTH results are reported
for a random sample of 1600 documents.

 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1PrecisionRecallNIPS k=100 Prec vs RecTF Similarity BaselineTFIDF Similarity BaselineTF

TFIDF

G-MAP
σ = .001
σ = .01
σ = .05
σ = .1
σ = .2

LRT
0.4575
0.4620
0.4489
0.4475
0.4373

SIM

0.3948

SIM

0.4412

LRT
0.4597
0.4649
0.4531
0.4535
0.4447

Table 2: G-MAP scores comparing the LRT for a
range of d(can) inﬂuence mixing weights σ against
the similarity baseline. The similarity measure to
select C is either TF or TFIDF cosine. Results are
reported on NIPS for k = 100.

G-MAP
NIPS (k = 100)
NIPS (k = 10)
HEPTH (k = 100)
HEPTH (k = 20)

TF

TFIDF

LRT
0.4489
0.4067
0.2432
0.2227

SIM

0.3948
0.3754
0.2216
0.2037

LRT
0.4531
0.4580
0.2543
0.2264

SIM

0.4412
0.4226
0.2167
0.1943

Table 3: G-MAP scores comparing the LRT against
the similarity baseline for two k-NN approximation
levels. The similarity measure for selecting C is ei-
ther TF or TFIDF cosine. Results are reported on
NIPS and HEPTH for σ = .05.

LRT scores are more comparable than similarities.

Table 1 showed that the LRT can ﬁnd the most inﬂuential
papers for one particular document. Figure 3 measures how
well it can ﬁnd the strongest edges in the whole inﬂuence
graph. This precision-recall graph uses the ranking of all
LRT statistic scores of all documents, with actual citations
marked as positive examples. Figure 3 also shows the scores
for using lists of TF and TFIDF cosine similarities. The
LRT graph dominates the similarity baselines over the whole
range and the diﬀerence in performance is larger than in
the per-document evaluation. We conclude from this that
LRT scores are more comparable between documents than
similarity scores. This is to be expected because the LRT
values have a clear probabilistic semantic. However, the
similarity scores have no such guarantees.

Effects of the σ parameter.

Table 2 shows that the LRT is robust over a large range σ
values. The LRT’s G-MAP dominates the similarity base-
lines. However, σ = 0.01 seems to perform better than our
initial guess of 0.05 used above.

Effect of k parameter in LRT approximations.

Table 3 shows G-MAP scores at diﬀering levels of the k-
NN approximation. Recall from Table 1 that G-MAP scores
for HEPTH are substantially lower than for NIPS. We con-
jecture that this is due to the size of the corpus in relation
to k. With a large corpus, k = 100 is likely to exclude too
many relevant documents from consideration. We further
analyze the role of k, in its two roles in controlling the sizes
of C and P.

First, k controls the size of C.

If k is too small, truly
inﬂuential documents will not be tested by the LRT. E.g., in
HEPTH, each document has 14 citations on average. With
k = 10, it would be simply impossible to recover the entire
citation graph. Therefore we conclude that k must be large
enough to include all documents that make contributions to
d(new). On HEPTH, k = 100 is better than k = 20 for TF

Dataset (C)
NIPS (TFIDF)
NIPS (TF)
HEPTH (TFIDF)
HEPTH (TF)

GMAP GMAP (perfect C)
0.4531
0.4489
0.2543
0.2432

0.4556
0.4590
0.3803
0.3906

Table 4: How close is the approximation to the op-
timal? G-MAP scores are reported for S = .05.

and TFIDF cosine, and for LRT and similarity baseline. We
believe this is because k = 20 is too restrictive. NIPS with
TF cosine shows the same behavior.
Optimal C.
To better understand how much loss in performance is due
to the k-NN approximation of C, the following experiment
explores the G-MAP scores of the LRT for a “perfect” C. In
particular, we construct C so that it includes all documents
that d(new) actually cites, and then ﬁll the remaining places
in C with the most similar documents. Table 4 shows that
for k = 100 the loss in performance due to an approximate
C is fairly small on NIPS. For HEPTH, on the other hand,
k = 100 shows a much greater loss, with G-MAP scores
only about 60-65% of the optimal. We believe this loss oc-
curs because C is too small to accomodate all the inﬂuential
documents.
4.3

Identifying Inﬂuential Documents

What are the inﬂuential documents that have the most
eﬀect on the document collection’s development? Which
documents should one read to best grasp this development?
We have already shown that LRTs can be used to infer an
inﬂuence graph that is similar to a citation graph. We now
investigate whether this inﬂuence graph can be used to iden-
tify the documents with the overall largest inﬂuence on the
collection. In analogy to citation counts (i.e. the in-degree
in the citation graph), we propose the in-degree in the inﬂu-
ence graph as a measure of impact. If not noted otherwise,
we form the inﬂuence graph by connecting each document
d(new) with the l other nodes that receive the highest LRT
value. We typically use l = 10, although we also explore this
parameter’s eﬀect on performance.

4.3.1 Qualitative evaluation
For each year in NIPS, Table 5 lists the paper with the
highest in-degree in the inﬂuence graph computed by the
LRT method with k = 100 and l = 10. We expect these
to have high citation counts, which we test by showing the
paper’s citation counts both from within the NIPS corpus
(as of 2000) and from Google Scholar (as of 2007). For
most documents, the citation count is indeed high when
compared to the average NIPS document citation count of
0.7734 other NIPS papers. An interesting example is “Sup-
port Vector Method for Function Approximation, Regres-
sion Estimation, and Signal Processing” from 1996. While
this is one of the papers that introduced SVMs to NIPS,
it has only 3 citations within NIPS and only 44 citations
in Google Scholar. Nevertheless, SVMs had a huge impact
on NIPS. In this sense our LRT method is correct and is
not inﬂuenced by citation habits. In this example, most au-
thors cite Vapnik’s later book (with 5144 citations) instead
of this paper. The LRT method is unaﬀected and correctly
identiﬁes the SVM idea as highly inﬂuential on NIPS.

Year Document Title and Author(s)
1988
1989

Document

Citation Counts

NIPS Google Scholar

“An Optimality Principle for Unsupervised Learning” by Terence D. Sanger
“Training Stochastic Model Recognition Algorithms as Networks Can Lead to Maximum Mutual
Information Estimation of Parameters” by John S. Bridle
“Learning Theory and Experiments with Competitive Networks” by Gniﬀ L. Bilbro, David E.
van den Bout
“The Eﬀective Number of Parameters: An Analysis of Generalization and Regularization in
Nonlinear Learning Systems” by John Moody
“Reinforcement Learning Applied to Linear Quadratic Regulation” by Steven J. Bradtke
“Supervised Learning from Incomplete Data via an EM approach” by Zoubin Ghahramani,
Michael I. Jordan
“Reinforcement Learning Algorithm for Partially Observable Markov Decision Problems” by
Tommi Jakkola, Sizarad Singhal, Michael I. Jordan
“EM Optimization of Latent-Variable Density Models” by Chris M. Bishop, M. Svensen, Chisto-
pher K.I. Williams
“Support Vector Method for Function Approximation, Regression Estimation, and Signal Pro-
cessing” by V. Vapnik, Steven E. Golowich, Alex Smola
“EM Algorithms for PCA and SPCA” by Sam Roweis

1990

1991

1992
1993

1994

1995

1996

1997

4
11

0

12

6
12

10

1

3

1

61
113

0

234

56
163

142

27

44 (5144)

177

Table 5: The most inﬂuential paper per year in NIPS, as measured by inﬂuence graph in-degree, with k = 100,
σ = .05, and TFIDF cosine for C. We exclude years with edge eﬀects and the last 3 years, since they do not have
statistically signiﬁcant counts. Comparison is against the within-NIPS citation counts, and Google-scholar
citation counts (on Feb. 28, 2007).

Corpus

τ

NIPS
0.4216
HEPTH 0.3887

LRT

RMap@3

0.2771
0.2558

TF

SIM

@12

0.3126
0.2376

τ

RMap@3

0.3379
0.3497

0.1475
0.1421

@12

0.2561
0.1594

τ

0.4163
0.3549

LRT

RMap@3

0.2751
0.1456

TFIDF

SIM

@12

0.3022
0.1582

τ

RMap@3

0.3686
0.3190

0.1959
0.1139

@12

0.2585
0.1138

Table 6: Rank metrics comparing the LRT against similarity on NIPS (k = 100) and HEPTH (k = 20), using
σ = .05 and TF or TFIDF cosine for C. We ignore the ﬁrst two and last two years because of edge eﬀects.

4.3.2 Quantitative Evaluation
We compare the ranking of documents by in-degree in the
inﬂuence graph to the ranking by citation count. As simi-
larity measures, we use Kendall’s τ and a ranking version of
MAP, which we term R-MAP.

Kendall’s τ.

Kendall’s τ measures how many pairs two rankings rank
in the same order. It ranges between -1 and 1, with higher
numbers indicating greater similarity. Formally,
2 · number of concordant pairs

total number of pairs − number of tied pairs

τ =

R-MAP@k.

R-MAP@k measures the average precision of a ranking.
With the k top-ranked documents as positive examples, av-
erage the ranking’s precision at the positions of these docu-
ments. We calculate R-MAP@3 and R-MAP@12.

There is one caveat with rank-based metrics. Edge eﬀects
(e.g., older papers have more citations, papers from the last
year have no citations) make it diﬃcult to present one uni-
ﬁed ranking of all documents. Therefore, we calculate each
metric per-year and average the year-by-year values to get
a single score for the entire corpus. Additionally, because
of edge eﬀects, the ﬁrst two and the last two years are not
used, since they do not contain meaningful results.

The TF and TFIDF baselines use the most similar docu-

ments instead of the LRT predictions.

LRTs are better than similarity.

Table 6 shows that the LRT gives substantially better
rankings than the similarity baseline for all metrics on both
NIPS and HEPTH with both TF and TFIDF cosine C.

Effect of the parameter l.

The left plot of Figure 4 explores whether selecting in-
ﬂuencers is sensitive to the parameter l. For the inﬂuence
graph, we considered each document’s l predicted inﬂuencers
with highest LRT scores. Figure 4 shows how varying l af-
fects τ for both LRT and the similarity baseline. Since NIPS
documents do not have many citations, we explore l = 1 to
15. The upper line is LRT performance with 95% conﬁ-
dence interval error bars. (The conﬁdence interval is com-
puted using the multiple τ values per data point, because
each graphed τ is the average of multiple (here, 10) years
of τ metric scores.) The lower line depicts τ on the similar-
ity baseline. For the TFIDF cosine C, when l is small, the
method computes a count over only the few top inﬂuential
documents selected by the LRTs for d(new).
It turns out
that small l seem to perform better than our initial guess
of l = 10. As l increases, more non-inﬂuential documents
are counted and τ correspondingly falls. When l approaches
100 (not shown), the LRT and the baseline are identical as
expected by construction.

Thresholding on the LRT score.

The right plot of Figure 4 depicts how τ varies if we do
not select a ﬁxed number of l neighbors per document, but
instead use a threshold on the LRT statistic. The LRT is
set up to reject the null hypothesis and declare that d(can)
inﬂuences d(new) if the LRT statistic is suﬃciently large.
Varying this threshold controls the level of conﬁdence in the
LRT, so we use the threshold level as the x-axis and examine
how it aﬀects τ . Thresholding the LRT values actually gives
better performance than using the l parameter, since we are
not forcing a certain number of inﬂuence links for each doc-
ument. There are four diﬀerent regions in this graph. First,

Figure 4: Using τ to compare the LRT against the similarity baseline, both with the l parameter (left) and
by thresholding the LRT statistic values (right). Results are for NIPS with TFIDF cosine C and k = 100. The
TF plot looks similar, except that the baseline is smoother.

if the threshold is too low, performance suﬀers because the
null hypothesis is being accepted erroneously. Second, per-
formance increases as the threshold approaches reasonable
conﬁdence levels. Third, a large range of threshold values
(approximately 100-2000) give good and similar τ scores,
showing that the LRT method is robust. Fourth, when the
threshold is too high, many inﬂuential documents are no
longer detected, and performance subsequently falls.

Note that a conﬁdence level of 95% per test (i.e. a thresh-
old of 3.84) performs quite poorly. This level means that 5%
of the inﬂuence links are erroneous. NIPS, with 2000 papers,
would have an expected 100,050 false links (and only 1512
real citations). Therefore, we need a much higher conﬁdence
level to account for the multiple-testing bias. Using Bonfer-
roni adjustment, each test’s level is the overall level divided
by the number of tests.

5. DISCUSSION AND FUTURE WORK

One obvious limitation of the current model is the sim-
plicity of the language model. The assumption that each
document is a sequence of independent words is, in reality,
likely violated. This observation motivates more expressive
language models such as n-gram language models.

There is also the question of whether these methods can
generalize to other domains. LRTs do not use citation data,
so many domains should be applicable. However, we have
only conducted experiments on research publications.

Finally, there is scalability and eﬃciency. Much of the
computing time is spent solving convex optimization prob-
lems. While C and P prune this space, there may be other
criteria to provably eliminate certain LRTs without aﬀecting
the results. Furthermore, the optimization problems have a
special structure, which can probably be exploited by spe-
cialized methods to solve the optimization problems.

6. CONCLUSIONS

We presented a probabilistic model of inﬂuence between
documents for corpora that have grown over time. In this
model, we derived a Likelihood Ratio Test to detect inﬂu-
ence based on the content of documents and showed how the
test can be computed eﬃciently. We found that the inﬂuence

graphs derived from the content resemble the structure of ex-
plicit citation graphs for corpora of scientiﬁc literature. Fur-
thermore, we showed that in-degree in the inﬂuence graph
is an eﬀective indicator of a document’s impact. The ability
to create inﬂuence graphs based on document content alone
has the potential to open databases without explicit citation
structure to the large repertoire of graph mining algorithms.

7. ACKNOWLEDGMENTS

We thank Johannes Gehrke and Rich Caruana for the dis-
cussions that lead to this work. This work was funded in
part by NSF Career Award IIS-0237381, NSF Award OISE-
0611783, and the KD-D grant.

8. REFERENCES
[1] M. Agosti and J. Allan. Introduction to the special

issue on methods and tools for the automatic
construction of hypertext. Inf. Process. Manage,
33(2):129–131, 1997.

[2] M. Agosti and F. Crestani. A methodology for the

automatic construction of a hypertext for information
retrieval. In Proceedings of the 1993 ACM/SIGAPP
Symposium on Applied Computing, pages 745–753,
Indianapolis IN, Feb. 1993.

[3] M. Agosti, F. Crestani, and M. Melucci. On the use of

information retrieval techniques for the automatic
construction of hypertext. Information Processing and
Management, 33:133–144, 1997.

[4] J. Allan. Automatic Hypertext Construction. PhD

thesis, Cornell University, 1995.

[5] J. Allan, J. Carbonell, G. Doddington, J. Yamron,
and Y. Yang. Topic Detection and Tracking Pilot
Study: Final Report. In Proceedings of the DARPA
Broadcast News Transcription and Understanding
Workshop-1998, 1998.

[6] J. Allan, R. Papka, and V. Lavrenko. On-Line New

Event Detection and Tracking. In Research and
Development in Information Retrieval, pages 37–45,
1998.

[7] D. Blei and J. Laﬀerty. Correlated topic models. In
Advances in Neural Info. Processing Systems, 2005.

 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0 2 4 6 8 10 12 14 16Kendall’s Tauparameter lTau on LRT@lTau on KNN@l 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.1 1 10 100 1000 10000 0 1Kendall’s TauLRT statistic thresholdSimilarity thresholdTau on LRTTau on KNN[8] D. Blei, A. Ng, and M. Jordan. Latent dirichlet

allocation. Journal of Machine Learning Research,
3:993–1022, 2003.

[9] G. Casella and R. L. Berger. Statistical Inference,
chapter 10.3.1 Asymptotic Distribution of LRTs,
pages 488–492. Duxbury, 2002.

[10] J. H. Coombs. Hypertext, full text, and automatic

linking. In Proc. Thirteenth Int’l. Conf. on Res. and
Development in Information Retrieval, Hypertext and
Image Retrieval, page 83, 1990.

[11] R. Furuta, C. Plaisant, and B. Shneiderman. A
spectrum of automatic hypertext constructions.
Hypermedia, 1(2):179–195, 1989.

[12] E. Garﬁeld. The Meaning of the Impact Factor.

International Journal of Clinical and Health
Psychology, 3(2):363–369, 2003.

[13] P. Ginsparg. The physics e-print arxiv.

http://www.arxiv.org.

[14] R. Guha, D. Sivakumar, R. Kumar, and R. Sundaram.

Unweaving a Web of Documents. In Proceedings of
KDD-2005, Chicago, Illinois, 2005.

[15] S. Havre, B. Hetzler, and L. Nowell. ThemeRiver: In
Search of Trends, Patterns, and Relationships. IEEE
Transactions on Visualization and Computer
Graphics, 2002.

[16] T. Hofmann. Probabilistic latent semantic analysis. In

Fifteenth Conference on Uncertainty in Artiﬁcial
Inteligence (UAI), 1999.

[17] http://nips.djvuzone.org/txt.html. NIPS Online: The

Text Repository.

[18] http://www.mosek.com/index.html. Mosek.
[19] F. Jelinek. Statistical Methods for Speech Recognition,
chapter Basic Language Modeling, pages 57–78. MIT
Press, 1998.

[20] J. Kleinberg. Authoritative Sources in a Hyperlinked

Environment. Journal of the ACM, 46(5):604–632,
1999.

[21] J. Kleinberg. Bursty and Hierarchical Structure in
Streams. In Proceedings of KDD-2002, Edmonton,
Alberta, Canada, 2002.

[22] T. Kolenda, L. K. Hansen, and J. Larsen. Signal
Detection using ICA: Application to Chat Room
Topic Spotting. In Lee, Jung, Makeig, and Sejnowski,
editors, Proc. of the Third International Conference
on Independent Component Analysis and Signal
Separation (ICA2001), pages 540–545, San Diego, CA,
USA, 2001.

[23] A. Krause, J. Leskovec, and C. Guestrin. Data

association for topic intensity tracking. In
International Conference on Machine Learning
(ICML), 2006.

[24] O. Kurland and L. Lee. Corpus structure, language
models, and ad hoc information retrieval. In Special
Interest Group on Information Retrieval (SIGIR),
pages 194–201, 2004.

[25] O. Kurland and L. Lee. Respect my authority! hits
without hyperlinks, utilizing cluster-based language
models. In Special Interest Group on Information
Retrieval (SIGIR), pages 83–90, 2006.

[26] A. Lelu. Automatic generation of hypertext links in

information retrieval systems: A stochastic and an

incremental algorithm. In V. V. Bookstein, A.;
Chiaramella, Y.; Salton, G.; Raghavan, editor, ACM
SIGIR Conference on Research and Development in
Information Retrieval (SIGIR), pages 326–336,
Chicago, Ill., USA, Oct. 1991. ACM Press.

[27] G. Mann, D. Mimno, and A. McCallum. Bibliometric

impact measures leveraging topic analysis. In Joint
Conference on Digital Libraries (JCDL), 2006.

[28] C. D. Manning and H. Schuetze. Foundations of

Statistical Natural Language Processing. MIT Press,
1999.

[29] Q. Mei and C. Zhai. Discovering Evolutionary Theme

Patterns from Text - An Exploration of Temporal
Text Mining. In Proceedings of KDD-2005, Chicago,
Illinois, 2005.

[30] F. Osareh. Bibliometrics, Citation Analysis and

Co-citation Analysis: A Review of Literature I. Libri,
46:149–158, 1996.

[31] L. Page, S. Brin, R. Motwani, and T. Winograd. The
pagerank citation ranking: Bringing order to the web.
Technical report, Computer Science Department,
Stanford University, 1998.

[32] G. Salton and C. Buckley. Term weighting approaches

in automatic text retrieval. Information Processing
and Management, 24(5):513–523, 1988.

[33] G. Salton and C. Buckley. Automatic text structuring
and retrieval – experiments in automatic encyclopedia
searching. In Proc. Fourteenth Int’l. Conf. on Res. and
Development in Information Retrieval, Document
Structure, page 21, 1991.

[34] B. Shaparenko, R. Caruana, J. Gehrke, and

T. Joachims. Identifying temporal patterns and key
players in document collections. In IEEE ICDM
Workshop on Temporal Data Mining: Algorithms,
Theory and Applications (TDM-05), pages 165–174,
2005.

[35] M. Steyvers, P. Smyth, M. Rosen-Zvi, and

T. Griﬃths. Probabilistic author-topic models for
information discovery. In Knowledge Discovery and
Data-Mining (KDD), 2004.

[36] R. Swan and D. Jensen. TimeMines: Constructing

Timelines with Statistical Models of Word Usage. In
Proceedings of KDD-2000, pages 73–80, Boston, MA,
2000.

[37] X. Wang and A. McCallum. Topics over time: A

nonmarkov continuoustime model of topical trends. In
Knowledge Discovery and Data Mining (KDD), pages
424–433, 2006.

[38] R. Wilkinson and A. F. Smeaton. Automatic link

generation. ACM Computing Surveys (CSUR),
31(4es), 1999.

[39] C. Zhai. Risk Minimization and Language Modeling in

Information Retrieval. PhD thesis, Carnegie Mellon
University, 2002.

