Abstract
Thesameproblemoccursininformation(cid:12)ltering,i.e.
theadaptiveclassi(cid:12)cationofdocumentswithrespect
Formanylearningtaskswheredataiscol-
toaparticularuserinterest.Information(cid:12)lteringtech-
lectedoveranextendedperiodoftime,its
niquesareused,forexample,tobuildpersonalized
underlyingdistributionislikelytochange.A
news(cid:12)lters,whichlearnaboutthenews-readingpref-
typicalexampleisinformation(cid:12)ltering,i.e.
erencesofauserorto(cid:12)ltere-mail.Boththeinterestof
theadaptiveclassi(cid:12)cationofdocumentswith
theuserandthedocumentcontentchangeovertime.
respecttoaparticularuserinterest.Both
A(cid:12)lteringsystemshouldbeabletoadapttosuchcon-
theinterestoftheuserandthedocument
ceptchanges.
contentchangeovertime.A(cid:12)lteringsys-
temshouldbeabletoadapttosuchconcept
Thispaperproposesanewmethodfordetectingand
changes.Thispaperproposesanewmethod
handlingconceptchangeswithsupportvectorma-
torecognizeandhandleconceptchangeswith
chines.Theapproachhasacleartheoreticalmoti-
supportvectormachines.Themethodmain-
vationanddoesnotrequirecomplicatedparameter
tainsawindowonthetrainingdata.The
tuning.Afterreviewingotherworkonadaptationto
keyideaistoautomaticallyadjustthewin-
changingconceptsandshortlydescribingsupportvec-
dowsizesothattheestimatedgeneralization
tormachines,thispaperexplainsthenewwindowad-
errorisminimized.Thenewapproachisboth
justmentapproachandevaluatesitinthreesimulated
theoreticallywell-foundedaswellase(cid:11)ective
conceptdriftscenariosonreal-worldtextdata.The
ande(cid:14)cientinpractice.Sinceitdoesnotre-
experimentsshowthattheapproache(cid:11)ectivelyselects
quirecomplicatedparameterization,itissim-
anappropriatewindowsizeandresultsinalowpre-
plertouseandmorerobustthancomparable
dictiveerrorrate.
heuristics.Experimentswithsimulatedcon-
ceptdriftscenariosbasedonreal-worldtext
.ConceptDrift
datacomparethenewmethodwithother
windowmanagementapproaches.Weshow
Throughoutthispaper,westudytheproblemofcon-
thatitcane(cid:11)ectivelyselectanappropriate
ceptdriftforthepatternrecognitionprobleminthe
windowsizeinarobustway.
followingframework.Eachexample~z=(~x;y)consists
ofafeaturevector~xRNandalabelyf(cid:0);+g
indicatingitsclassi(cid:12)cation.Dataarrivesovertimein
.Introduction
batches.Withoutlossofgeneralitythesebatchesare
assumedtobeofequalsize,eachcontainingmexam-
Machinelearningmethodsareoftenappliedtoprob-
ples.
lems,wheredataiscollectedoveranextendedperiod
oftime.
Inmanyreal-worldapplicationsthisintro-
~z(;);:::;~z(;m);~z(;);:::;~z(;m);(cid:1)(cid:1)(cid:1);~z(t;);:::;~z(t;m);~z(t+;);:::;~z(t+;m)
ducestheproblemthatthedistributionunderlying
~z(i;j)denotesthej-thexampleofbatchi.Foreachbatch
thedataislikelytochangeovertime.Forexam-
ithedataisindependentlyidenticallydistributedwith
ple,companiescollectanincreasingamountofdata
respecttoadistributionPri(~x;y).Dependingonthe
likesales(cid:12)guresandcustomerdatato(cid:12)ndpatterns
amountandtypeofconceptdrift,theexampledistri-
inthecustomerbehaviourandtopredictfuturesales.
butionPri(~x;y)andPri+(~x;y)betweenbatcheswill
Asthecustomerbehaviourtendstochangeovertime,
di(cid:11)er.ThegoalofthelearnerListosequentiallypre-
themodelunderlyingsuccessfulpredictionsshouldbe
dictthelabelsofthenextbatch.Forexample,after
adaptedaccordingly.

batchtthelearnercanuseanysubsetofthetraining
examplesfrombatchestottopredictthelabelsof
batcht+.Thelearneraimstominimizethecumu-
latednumberofpredictionerrors.
Inmachinelearning,changingconceptsareoftenhan-
dledbytimewindowsof(cid:12)xedoradaptivesizeonthe
trainingdata(Mitchelletal.,		;Widmer&Kubat,
		;Lanquillon,		;Klinkenberg&Renz,		)or
byweightingdataorpartsofthehypothesisaccording
totheirageand/orutilityfortheclassi(cid:12)cationtask
(Kunisch,		;Tayloretal.,		).Thelatterap-
proachofweightingexampleshasalreadybeenused
forinformation(cid:12)lteringintheincrementalrelevance
feedbackapproachesofAllan(		)andBalabanovic
(		).Inthispaper,theearlierapproachmaintaining
awindowofadaptivesizeisexplored.Moredetailed
descriptionsofthemethodsdescribedaboveandfur-
therapproachescanbefoundinKlinkenberg(		).
Forwindowsof(cid:12)xedsize,thechoiceofa\good"
windowsizeisacompromisebetweenfastadaptiv-
ity(smallwindow)andgoodgeneralizationinphases
withoutconceptchange(largewindow).Thebasic
ideaofadaptivewindowmanagementistoadjustthe
windowsizetothecurrentextentofconceptdrift.
Thetaskoflearningdriftingortime-varyingconcepts
hasalsobeenstudiedincomputationallearningthe-
ory.Learningachangingconceptisinfeasible,ifno
restrictionsareimposedonthetypeofadmissiblecon-
ceptchanges,butdriftingconceptsareprovablye(cid:14)-
cientlylearnable(atleastforcertainconceptclasses),
iftherateortheextentofdriftislimitedinparticular
ways.Helmbold,&Long(		)assumeapossiblyperma-
nentbutslowconceptdriftandde(cid:12)netheextentof
driftastheprobabilitythattwosubsequentconcepts
disagreeonarandomlydrawnexample.Theirresults
includeanupperboundfortheextendofdriftmaxi-
mallytolerablebyanylearnerandalgorithmsthatcan
learnconceptsthatdonotdriftmorethanacertain
constantextentofdrift.Furthermoretheyshowthatit
issu(cid:14)cientforalearnertoseea(cid:12)xednumberofthe
mostrecentexamples.Henceawindowofacertain
minimal(cid:12)xedsizeallowstolearnconceptsforwhich
theextentofdriftisappropriatelylimited.
E.g.afunctionrandomlyjumpingbetweenthevalues
oneandzerocannotbepredictedbyanylearnerwithmore
than%accuracy.

WhileHelmboldandLongrestricttheextendofdrift,
Kuhetal.(		)determineamaximalrateofdrift
thatisacceptablebyanylearner,i.e.amaximally
acceptablefrequencyofconceptchanges,whichimplies
alowerboundforthesizeofa(cid:12)xedwindowforatime-
varyingconcepttobelearnable,whichissimilartothe
lowerboundofHelmboldandLong.
Inpractice,however,
itusuallycannotbeguaran-
teedthattheapplicationathandobeystheserestric-
tions,e.g.areaderofelectronicnewsmaychange
hisinterests(almost)arbitrarilyoftenandradically.
Furthermorethelargetimewindowsizes,forwhich
thetheoreticalresultshold,wouldbeimpractical.
Hencemoreapplicationorientedapproachesrelyon
farsmallerwindowsof(cid:12)xedsizeoronwindowadjust-
mentheuristicsthatallowfarsmallerwindowsizes
andusuallyperformbetterthan(cid:12)xedand/orlarger
windows(Widmer&Kubat,		;Lanquillon,		;
Klinkenberg&Renz,		).Whiletheseheuristicsare
intuitiveandworkwellintheirparticularapplication
domain,theyusuallyrequiretuningtheirparameters,
areoftennottransferabletootherdomains,andlack
apropertheoreticalfoundation.
Syedetal.(			)describeanapproachtoincremen-
tallylearningsupportvectormachinesthathandles
virtualconceptdriftimpliedbyincrementallylearning
fromseveralsubsamplesofalargetrainingset,but
theydonotaddresstheproblemof(real)conceptdrift
addressedhere.
.SupportVectorMachines
Thewindowadjustmentapproachdescribedinthis
paperusessupportvectormachines(Vapnik,		)
astheircorelearningalgorithm.Supportvectorma-
chinesarebasedonthestructuralriskminimization
principle(Vapnik,		)fromstatisticallearningthe-
ory.Intheirbasicform,SVMslearnlineardecision
rulesh(~x)=signf~w(cid:1)~x+bg=(cid:26)+;
if~w(cid:1)~x+b>
()
(cid:0);else
describedbyaweightvector~wandathresholdb.The
ideaofstructuralriskminimizationisto(cid:12)ndahypoth-
esishforwhichonecanguaranteethelowestproba-
bilityoferror.ForSVMs,Vapnik(		)showsthat
thisgoalcanbetranslatedinto(cid:12)ndingthehyperplane
withmaximumsoft-margin.Computingthishyper-
planeisequivalenttosolvingthefollowingoptimiza-
tionproblem.
SeeBurges(		)foranintroductiontoSVMs.

OptimizationProblem(SVM(primal))
V(~w;b;~(cid:24))=~w(cid:1)~w+CnXi=(cid:24)i()
minimize:
ni=:yi[~w(cid:1)~xi+b](cid:21)(cid:0)(cid:24)i
subjectto:
()
ni=:(cid:24)i>
()
Inthisoptimizationproblem,theEuclideanlength
jj~wjjoftheweightvectorisinverselyproportionalto
thesoft-marginofthedecisionrule.Theconstraints
()requirethatalltrainingexamplesareclassi(cid:12)edcor-
rectlyuptosomeslack(cid:24)i.Ifatrainingexamplelieson
the\wrong"sideofthehyperplane,thecorresponding
(cid:24)iisgreaterorequalto.ThereforePni=(cid:24)iisan
upperboundonthenumberoftrainingerrors.The
factorCin()isaparameterthatallowstrading-o(cid:11)
trainingerrorvs.modelcomplexity.
Forcomputationalreasonsitisusefultosolvethe
Wolfedual(Fletcher,	)ofoptimizationproblem
insteadofsolvingoptimizationproblemdirectly
(Vapnik,		).
OptimizationProblem(SVM(dual))
minimize:W(~(cid:11))=(cid:0)nXi=(cid:11)i+nXi=nXj=yiyj(cid:11)i(cid:11)j(~xi(cid:1)~xj)()
subjectto:nXi=yi(cid:11)i=
()
ni=:(cid:20)(cid:11)i(cid:20)C
()
Inthispaper,SVMlight(Joachims,			)isusedfor
computingthesolutionofthisoptimizationproblem.
Supportvectorsarethosetrainingexamples~xiwith
(cid:11)i>atthesolution.Fromthesolutionofoptimiza-
tionproblemthedecisionrulecanbecomputedas
~w(cid:1)~x=nXi=(cid:11)iyi(~xi(cid:1)~x)andb=yusv(cid:0)~w(cid:1)~xusv()
Thetrainingexample(~xusv;yusv)forcalculatingb
mustbeasupportvectorwith(cid:11)usv<C.Finally,the
traininglosses(cid:24)icanbecomputedas(cid:24)i=max((cid:0)
yi[~w(cid:1)~xi+b];).
Forbothsolvingoptimizationproblemaswellasap-
plyingthelearneddecisionrule,itissu(cid:14)cienttobe
abletocalculateinnerproductsbetweenfeaturevec-
tors.Exploitingthisproperty,Boseretal.introduced
theuseofkernelsK(~x;~x)forlearningnon-linearde-
cisionrules.Dependingonthetypeofkernelfunc-
tion,SVMslearnpolynomialclassi(cid:12)ers,radialbasis
SVMLightisavailableathttp://www-ai.informatik.
uni-dortmund.de/svmlight

function(RBF)classi(cid:12)ers,ortwolayersigmoidneu-
ralnets.Suchkernelscalculateaninner-productin
somefeaturespaceandreplacetheinner-productin
theformulasabove.
.WindowAdjustmentbyOptimizing
Performance
Ourapproachtohandlingdriftinthedistributionof
examplesusesawindowonthetrainingdata.This
windowshouldincludeonlythoseexamplewhichare
su(cid:14)ciently\close"tothecurrenttargetconcept.As-
sumingtheamountofdriftincreaseswithtime,the
windowincludesthelastntrainingexamples.Pre-
viousapproachesusedsimilarwindowingstrategies.
Theirshortcomingsarethattheyeither(cid:12)xthewin-
dowsize(Mitchelletal.,		)orinvolvecompli-
catedheuristics(Widmer&Kubat,		;Lanquillon,
		;Klinkenberg&Renz,		).A(cid:12)xedwindowsize
makesstrongassumptionsabouthowquicklythecon-
ceptchanges.Whileheuristicscanadapttodi(cid:11)erent
speedandamountofdrift,theyinvolvemanyparam-
etersthataredi(cid:14)culttotune.Here,wepresentan
approachtoselectinganappropriatewindowsizethat
doesnotinvolvecomplicatedparameterization.They
keyideaistoselectthewindowsizesothatthees-
timatedgeneralizationerroronnewexamplesismin-
imized.Togetanestimateofthegeneralizationer-
rorweuseaspecialformof(cid:24)(cid:11)-estimates(Joachims,
).(cid:24)(cid:11)-estimatesareaparticularlye(cid:14)cientmethod
forestimatingtheperformanceofaSVM.
.(cid:24)(cid:11)-Estimators
(cid:24)(cid:11)-estimatorsarebasedontheideaofleave-one-out
estimation(Lunts&Brailovskiy,	).Theleave-
one-outestimatoroftheerrorrateproceedsasfollows.
FromthetrainingsampleS=((~x;y);(cid:1)(cid:1)(cid:1);(~xn;yn))
the(cid:12)rstexample(~x;y)isremoved.Theresulting
sampleSn=((~x;y);(cid:1)(cid:1)(cid:1);(~xn;yn))isusedfortrain-
ing,leadingtoaclassi(cid:12)cationrulehnL.Thisclassi(cid:12)-
cationruleistestedontheheldoutexample(~x;y).
Iftheexampleisclassi(cid:12)edincorrectlyitissaidtopro-
ducealeave-one-outerror.Thisprocessisrepeated
foralltrainingexamples.Thenumberofleave-one-
outerrorsdividedbynistheleave-one-outestimate
ofthegeneralizationerror.
Whiletheleave-one-outestimateisusuallyveryaccu-
rate,itisveryexpensivetocompute.Withatraining
sampleofsizen,onemustrunthelearnerntimes.
(cid:24)(cid:11)-estimatorsovercomethisproblemusinganupper
boundonthenumberofleave-one-outerrorsinstead
ofcalculatingthembruteforce.Theyowetheirname

tothetwoargumentstheyarecomputedfrom.~(cid:24)is
thevectoroftraininglossesatthesolutionofthepri-
malSVMtrainingproblem.~(cid:11)isthesolutionofthe
dualSVMtrainingproblem.Basedonthesetwovec-
tors|bothareavailableaftertrainingtheSVMat
noextracost|the(cid:24)(cid:11)-estimatorsarede(cid:12)nedusing
thefollowingtwocounts.WithR(cid:1)beingthemax-
imumdi(cid:11)erenceofanytwoelementsoftheHessian
(i.e.R(cid:1)(cid:21)max~x;~x(K(~x;~x)(cid:0)K(~x;~x))),
d=jfi:((cid:11)iR(cid:1)+(cid:24)i)(cid:21)gj
(	)
countsthenumberoftrainingexamples,forwhichthe
quantity(cid:11)iR(cid:1)+(cid:24)iexceedsone.Sincethedocument
vectorsarenormalizedtounitlengthintheexperi-
mentsdescribedinthispaper,hereR(cid:1)=.
Itis
proveninJoachims()thatdisanapproximate
upperboundonthenumberofleave-one-outerrorsin
thetrainingset.Withnasthetotalnumberoftrain-
ingexamples,the(cid:24)(cid:11)-estimatorsoftheerrorrateis
Errn(cid:24)(cid:11)(hL)=jfi:((cid:11)iR(cid:1)+(cid:24)i)(cid:21)gj
()
n
Thetheoreticalpropertiesofthis(cid:24)(cid:11)-estimatoraredis-
cussedinJoachims().Itcanbeshownthatthe
estimatorispessimisticallybiased,overestimatingthe
trueerrorrateonaverage.Experimentsshowthatthe
biasisacceptablysmallfortextclassi(cid:12)cationproblems
andthatthevarianceofthe(cid:24)(cid:11)-estimatorisessentially
aslowasthatofaholdoutestimateusingtwiceas
muchdata.Itisalsopossibletodesignsimilaresti-
matorsforprecisionandrecall,aswellascombined
measureslikeF(Joachims,).
.WindowAdjustmentAlgorithm
Awindowadjustmentalgorithmhastosolvethefol-
lowingtrade-o(cid:11).Alargewindowprovidesthelearner
withmuchtrainingdata,allowingittogeneralizewell
giventhattheconceptdidnotchange.Ontheother
hand,alargewindowcancontainolddatathatisno
longerrelevant(orevenconfusing)forthecurrenttar-
getconcept.Findingtherightsizemeanstrading-o(cid:11)
thequalityagainstthenumberoftrainingexamples.
Toanswerthisquestionthewindowadjustmentalgo-
rithmproposedinthefollowinguses(cid:24)(cid:11)-estimatesin
aparticularway.Atbatcht,itessentiallytriesvar-
iouswindowsizes,trainingaSVMforeachresulting
trainingset.
~z(t;);:::;~z(t;m)
()
~z(t(cid:0););:::;~z(t(cid:0);m);~z(t;);:::;~z(t;m)
()
~z(t(cid:0););:::;~z(t(cid:0);m);~z(t(cid:0););:::;~z(t(cid:0);m);~z(t;);:::;~z(t;m)
()
...

Foreachwindowsizeitcomputesa(cid:24)(cid:11)-estimatebased
ontheresultoftraining.Incontrasttotheprevious
section,the(cid:24)(cid:11)-estimatorusedhereconsidersonlythe
lastbatch,thatisthemmostrecenttrainingexamples
~z(t;);:::;~z(t;m).
Errm(cid:24)(cid:11)(hL)=jfi:(cid:20)i(cid:20)m^((cid:11)(t;i)R(cid:1)+(cid:24)(t;i))(cid:21)gj
()
m
Thisre(cid:13)ectstheassumptionthatthemostrecentex-
amplesaremostsimilartothenewexamplesinbatch
t+.Thewindowsizeminimizingthe(cid:24)(cid:11)-estimateof
theerrorrateisselectedbythealgorithm.
Thealgorithmcanbesummarizedasfollows:
(cid:15)input:Strainingsampleconsistingof
tbatchescontainingmexampleseach
(cid:15)forhf;:::;t(cid:0)g
{trainSVMonexamples~z(t(cid:0)h;);:::;~z(t;m)
{compute(cid:24)(cid:11)-estimateonexamples
~z(t;);:::;~z(t;m)
(cid:15)output:windowsizewhichminimizes
(cid:24)(cid:11)-estimate
.Experiments
.ExperimentalSetup
Eachofthefollowingdatamanagementapproachesis
evaluatedincombinationwiththeSVM:
(cid:15)\FullMemory":Thelearnergeneratesitsclassi-
(cid:12)cationmodelfromallpreviouslyseenexamples,
i.e.itcannot\forget"oldexamples.
(cid:15)\NoMemory":Thelearneralwaysinducesitshy-
pothesisonlyfromthemostrecentbatch.This
correspondstousingawindowofthe(cid:12)xedsizeof
onebatch.
(cid:15)Windowof\FixedSize":Awindowofthe(cid:12)xed
sizeofthreebatchesisused.
(cid:15)Windowof\AdaptiveSize":Thewindowadjust-
mentalgorithmproposedintheprevioussection
adaptsthewindowsizetothecurrentconcept
driftsituation.
Theexperimentsareperformedinaninformation(cid:12)l-
teringdomain,atypicalapplicationareaforlearning
driftingconcept.Textdocumentsarerepresentedas
attribute-valuevectors(bagofwordsmodel),where
eachdistinctwordcorrespondstoafeaturewhose


.
.
.
.
.
.

Table.RelevanceofthecategoriesintheconceptchangescenariosA,B,andC.
Sce-Cate-Probabilityofbeingrelevantforadocumentofthespeci(cid:12)edcategoryatthespeci(cid:12)edtimestep(batch)













nario
gory






.
.
.
.
.
.
.
.
.
.
.
.
.
.
A

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
B

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
C

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
(cid:12)rstdocumentsofcategoryareconsideredrelevant
valueisthe\ltc"-TF/IDF-weight(Salton&Buckley,
fortheuserinterestandallotherdocumentsirrele-
	)ofthatwordinthatdocument.Wordsoccurring
vant.Thischangesslowly(conceptdrift)frombatch
lessthanthreetimesinthetrainingdataoroccurring
tobatch,wheredocumentsofcategoryarerele-
inagivenlistofstopwordsarenotconsidered.Each
vantandallothersirrelevant.Thethirdscenario(sce-
documentfeaturevectorisnormalizedtounitlength
narioC)simulatesanabruptconceptshiftintheuser
toabstractfromdi(cid:11)erentdocumentlengths.
interestfromcategorytocategoryinbatch	and
Theperformanceofaclassi(cid:12)erismeasuredbythe
backtocategoryinbatch.
threemetricspredictionerror,recall,andprecision.
Recallistheprobability,thattheclassi(cid:12)errecognizes
.Results
arelevantdocumentasrelevant.Precisionistheprob-
ability,thatadocumentclassi(cid:12)edasrelevantactually
Figurecomparesthepredictionerrorratesofthe
isrelevant.Allreportedresultsareestimatesaveraged
adaptivewindowsizealgorithmwiththenon-adaptive
overtenruns.
methods.Thegraphsshowthepredictionerroronthe
followingbatch.Inallthreescenarios,thefullmem-
Theexperimentsuseasubsetofdocumentsof
orystrategyandtheadaptivewindowsizealgorithm
thedatasetoftheTextREtrievalConference(TREC)
essentiallycoincideaslongasthereisnoconceptdrift.
consistingofEnglishbusinessnewstexts.Eachtextis
Duringthisstablephase,bothshowlowerprediction
assignedtooneorseveralcategories.Thecategories
errorthanthe(cid:12)xedsizeandthenomemoryapproach.
consideredhereare(AntitrustCasesPending),
Atthepointofconceptdrift,theperformanceofall
(JointVentures),(DebtRescheduling),(Dumping
methodsdeteriorates.Whiletheperformanceofno
Charges),and(ThirdWorldDebtRelief).Forthe
memoryandadaptivesizerecoversquicklyafterthe
experiments,threeconceptchangescenariosaresimu-
conceptdrift,theerrorratefullmemoryapproachre-
lated.Thetextsarerandomlysplitintobatchesof
mainshighespeciallyinscenariosAandB.Likebefore
equalsizecontainingdocumentseach.Thetexts
theconceptdrift,thenomemoryandthe(cid:12)xedsize
ofeachcategoryaredistributedasequallyaspossible
strategiesexhibithighererrorratesthantheadaptive
overthebatches.
windowalgorithminthestablephaseaftertheconcept
Tabledescribestherelevanceofthecategoriesin
drift.Thisshowsthatthenomemory,the(cid:12)xedsize,
thethreeconceptchangescenariosA,B,andC.For
andthefullmemoryapproachesallperformsubopti-
eachtimestep(batch),theprobabilityofbeingrele-
mallyinsomesituation.Onlytheadaptivewindow
vant(interestingtotheuser)isspeci(cid:12)edfordocuments
sizealgorithmcanachievearelativelylowerrorrate
ofcategoriesand,respectively.Documentsofthe
overallphasesinallscenarios.Thisisalsore(cid:13)ected
classes,,andareneverrelevantinanyofthese
intheaverageerrorratesoverallbatchesgiveninTa-
scenarios.Inthe(cid:12)rstscenario(scenarioA),(cid:12)rstdoc-
ble.Theadaptivewindowsizealgorithmachievesa
umentsofcategoryareconsideredrelevantforthe
lowaverageerrorrateonallthreescenarios.Similarly,
userinterestandallotherdocumentsirrelevant.This
precisionandrecallareconsistentlyhigh.
changesabruptly(conceptshift)inbatch,where
Thebehavioroftheadaptivewindowalgorithmisbest
documentsofcategoryarerelevantandallothersir-
explainedbylookingatthewindowsizesitselects.
relevant.Inthesecondscenario(scenarioB),again
Figureshowstheaveragetrainingwindowranges.
Hence,ineachtrial,outofthedocuments,eight
Thebottomofeachgraphdepictsthetimeandextent
randomlyselectedtextsarenotconsidered.
ofconceptdriftinthecorrespondingscenario.For

)



%
n
i
(


t

e
a
R


r
o
r
r

E

)



%
n
i
(

e

t

a
R


r
o
r
r

E

)



%
n
i
(

e
t
a
R


r
o
r
r

E

Adaptive Size
No Memory
Fixed Size
Full Memory

2

4

6

8

10

Batch

12

14

16

18

Adaptive Size
No Memory
Fixed Size
Full Memory

50

45

40

35

30

25

20

15

10

5

0

50

45

40

35

30

25

20

15

10

50

45

40

35

30

25

6

4

2

5

0

8

18

16

14

12

10

Batch

Adaptive Size
No Memory
Fixed Size
Full Memory

Figure.Comparisonofthepredictionerrorratesforsce-
narioA(top),B(middle),andC(bottom).Thex-axis
denotesthebatchnumberandthey-axistheaveragepre-
dictionerror.

Batch

10

12

14

16

18

8

0

5

2

4

6

15

10

20

19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1

h
c
t

a
B

0

260

520

780

19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1

h
c
t

a
B

1040

Window Range

1300

1560

1820

2080

2340

h
c
t
a
B

0

780

520

260

2080

1820

1560

1300

1040

2340

Window Range

19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1

Figure.WindowsizeandrangeforscenarioA(top),B
(middle),andC(bottom).They-axisdenotesthebatch
number.Eachhorizontallinemarkstheaveragetraining
windowrangeselectedatthatbatchnumber.Thebottom
partofeachgraphdepictsthelocationandtypeofthe
conceptshift.

Window Range

2340

1040

1300

1560

1820

2080

260

520

780

0

Table.Error,accuracy,recall,andprecisionofallwindow
managementapproachesforallscenariosaveragedover
trialswithbatcheseach(standardsampleerrorinparen-
theses).
Full
No
Fixed
Adaptive
MemoryMemory
Size
Size
ScenarioA:
Error
.%
.%
.	%
.%
(.%)
(.	%)
(.%)
(.	%)
Recall
.	%.%.%
.%
(.%)
(.%)
(.%)
(.	%)
Precision
.%	.	%.%
.%
(.%)
(.%)
(.	%)
(.%)
ScenarioB:
Error
.%
.%
.%
.%
(.	%)
(.%)
(.%)
(.%)
Recall
	.%.%.%
.%
(.%)
(.%)
(.%)
(.%)
Precision
.	%.%.	%
.%
(.	%)
(.%)
(.%)
(.%)
ScenarioC:
Error
.%
.	%.%
.%
(.%)
(.%)
(.%)
(.%)
Recall
.%.%.%
.%
(.%)
(.%)
(.%)
(.%)
Precision
.%.%	.%
.%
(.	%)
(.%)
(.	%)
(.		%)
scenarioAthetrainingwindowincreasesuptothe
abruptconceptchangeafterbatch,coveringalmost
allexamplesavailableforthecurrentconcept.Onlyin
batchestotheaveragetrainingsetsizeisslightly
smallerthanmaximallypossible.Ourexplanationis
thatforlargetrainingsetsarelativelysmallnumberof
additionalexamplesdoesnotalwaysmakea\notice-
able"di(cid:11)erence.Aftertheconceptchangeinbatch
theadaptivewindowsizealgorithmnowpickstrain-
ingwindowscoveringonlythoseexamplesfromafter
thedriftasdesired.Asimilarbehaviorisfoundfor
scenarioB(Figure,middle).Sincethedriftisless
abrupt,theadaptivewindowsizealgorithminterme-
diatelyselectstrainingexamplesfrombothconcepts
inbatch.Aftersu(cid:14)cientlymanytrainingexamples
fromthenewdistributionareavailable,thoseearlier
examplesarediscarded.Thebehavioroftheadaptive
windowsizealgorithminscenarioCisreasonableas
well(Figure,bottom).Aparticularsituationoccurs
inbatch.Herethewindowsizeexhibitsalarge
variance.Foroftherunsthealgorithmselects
asmalltrainingsetsizeofonebatch,whileforthe
remainingrunsitselectsallavailabletrainingexam-
plesstartingwithbatch.Herethereappearstobe
aborderlinedecisionbetweenaccepting(outof)
batchesof\bad"examplesorjusttrainingonasingle
batch.

5

1

1

50

40

30

20

10

0

15

15

10

10

Batch

Error Estimate (in %)

5
Window Range

Error Estimate
Minimum Error Estimate

Figure.Average(cid:24)(cid:11)-estimatesatdi(cid:11)erentbatchesandfor
varyingtrainingwindowsizesforscenarioA.Thedashed
curvemarksthebeginningofthewindowwiththelowest
errorestimate.
Furtherinsightonhowthealgorithmselectsthewin-
dowsizeisgainedfrom(cid:12)gure.Itplotstheaverage
(cid:24)(cid:11)-estimateinscenarioAoverallbatchesandforvary-
ingwindowsize.Thex-axisdenotesthenumberof
thecurrentbatch(increasingfromrighttoleft)and
thex-axisthebatchofthewindowstart.Thedashed
lineindicatesthebeginningofthewindowwiththe
lowestestimateinthebatch.Thegraphshowsthat
theerrorestimatedecreaseswithgrowingwindowsize
inbatchesto.Afterbatch,theestimateaccu-
ratelyre(cid:13)ectstheconceptchange.Theerrorestimate
decreaseswithtrainingwindowsgrowingtowardsthe
abruptconceptchange.Ifthewindowisenlargedbe-
yondthischange,theestimatederrorincreasessteeply
asexpected.
.SummaryandConclusions
Inthispaper,weproposedanewmethodforhan-
dlingconceptdriftwithsupportvectormachines.The
methoddirectlyimplementsthegoalofdiscardingir-
relevantdatawiththeaimofminimizinggeneraliza-
tionerror.ExploitingthespecialpropertiesofSVMs,
weadapted(cid:24)(cid:11)-estimatestothewindowsizeselec-
tionproblem.Unlikefortheconventionalheuristic
approaches,thisgivesthenewmethodaclearand
simpletheoreticalmotivation.Furthermore,thenew
methodiseasiertouseinpracticalapplications,since
itinvolveslessparametersthancomplicatedheuris-
tics.Experimentsinaninformation(cid:12)lteringdomain
showthatthenewalgorithmachievesalowerrorrate
andselectsappropriatewindowsizesoververydi(cid:11)er-
entconceptdriftscenarios.
Anopenquestionsishowsensitivethealgorithmis
tothesizeofindividualbatches.Sinceinthecurrent
versionofthealgorithmthebatchsizedeterminesthe
estimationwindow,thevarianceofthewindowsizeis

likelytoincreasewithsmallerbatches.
Itmightbe
necessarytoselecttheestimationwindowsizeinde-
pendentofthebatchsize.Ashortcomingofmostex-
istingalgorithmshandlingconceptdrift(anexception
isLanquillon(			))isthattheycandetectconcept
driftonlyafterlabeleddataisavailable.Thatis,after
thelearningalgorithmstartsmakingmistakes.While
thisappearsunavoidableforconceptdriftwithrespect
toPr(yj~x),itmightbepossibletodetectconceptdrift
inPr(~x)earlierbyusingtransductivesupportvector
machines.
Acknowledgments
ThisworkwassupportedbytheDFGCollaborative
ResearchCenteronComplexityReductioninMulti-
variateData(SFB)andbytheDFGCollabora-
tiveResearchCenteronComputationalIntelligence
(SFB).
