Summary

of inexact inference.

 Reviewed structural SVMs.
 Explained the consequences
 Theoretically and empirically
analyzed two approximation
families.
 Undergenerating (i.e.,
 Overgenerating (i.e.,

local)

relaxations)

 Completely connected binary
pairwise MRFs applied to
multilabel classication serves
as example application.

 Overgenerating methods:
 Preserve key theoretical
 Learn robust stable

SSVM properties.

predictive models.

Software

 SVMpython: SVMstruct, but API functions in Python, not

C.  Obviates annoying details (IO of model
structures, memory management).
http://www.cs.cornell.edu/~tomf/svmpython2/

 PyGLPK: GNU Linear Programming Kit (Andrew

Makhorin) as a Pythonic extension module.
http://www.cs.cornell.edu/~tomf/pyglpk/

 PyGraphcut: Graphcut based energy optimization
framework (Boykov and Kolmogorov) as a Pythonic
extension module.
http://www.cs.cornell.edu/~tomf/pygraphcut/

Thank you

Questions?

More Slides

 The detailed tables.

The Sorry State of LBP
 Lower is better

Losses per Dataset.  Inference method used during training and prediction.

25

20

15

10

5

0

Scene
Greedy

Yeast

Reuters Mediamill

Synth1

LBP

Combine

Exact

Synth2
LProg

The Sorry State of LBP

Bad as a training method (all predicted with Exact)...

25
20
15
10
5
0

18
15
12
9
6
3
0

12
10
8
6
4
2
0

12
10
8
6
4
2
0

25
20
15
10
5
0

36.830
30.692
24.553
18.415
12.277
6.138
0

15
12
9
6
3
0

15
12
9
6
3
0

10
8
6
4
2
0

25.710
20.568
15.426
10.284
5.142
0

Synth1

LProg

Synth2

Scene

Synth2
Bad as a prediction method (all trained with Exact)...

Mediamill

Reuters

Synth1

Yeast

45.90
36.72
27.54
18.36
9.18
0

Scene

Yeast
Greedy

18
15
12
9
6
3
0

LBP

Reuters

Mediamill

Combine

Exact

Table 1: Multi-labeling loss on six datasets. Results are grouped by dataset. Rows indicate sepa-
ration oracle method. Columns indicate classication inference method. The two quantities in the
dataset name row are edgeless (baseline) and default performance.

Great Big Table

LBP

Combine

Yeast Dataset

20.91.55

25.09 Synth1 Dataset

Combine

Exact

18.60.14

Exact

11.43.29

Greedy
Scene Dataset

Relaxed

LBP
18.10 Mediamill Dataset

Greedy

Relaxed
25.37
Greedy 10.67.28 10.74.28 10.67.28 10.67.28 10.67.28 23.39.16 25.66.17 24.32.17 24.92.17 27.05.18
LBP 10.45.27 10.54.27 10.45.27 10.42.27 10.49.27 22.83.16 22.83.16 22.83.16 22.83.16 22.83.16
Combine 10.72.28 11.78.30 10.72.28 10.77.28 11.20.29 19.56.14 20.12.15 19.72.14 19.82.14 20.23.15
Exact 10.08.26 10.33.27 10.08.26 10.06.26 10.20.26 19.07.14 27.23.18 19.08.14 18.75.14 36.83.21
Relaxed 10.55.27 10.49.27 10.49.27 10.49.27 10.49.27 18.50.14 18.26.14 18.26.14 18.21.14 18.29.14
8.99.08
16.34
8.86.08
8.86.08
Greedy 21.62.56 21.77.56 21.58.56 21.62.56 24.42.61
LBP 24.32.61 24.32.61 24.32.61 24.32.61 24.32.61 13.94.12 13.94.12 13.94.12 13.94.12 13.94.12
8.86.08
Combine 22.33.57 37.24.77 22.32.57 21.82.56 42.72.81
8.86.08
6.86.06
Exact 23.38.59 21.99.57 21.06.55 20.23.53 45.90.82
6.86.06
8.94.08
Relaxed 20.47.54 20.45.54 20.47.54 20.48.54 20.49.54
8.94.08
9.80.09
4.96.09
10.00
Greedy
5.42.09 16.98.26
7.28.07 19.03.15
LBP 15.80.25 15.80.25 15.80.25 15.80.25 15.80.25 10.00.09 10.00.09 10.00.09 10.00.09 10.00.09
4.55.08
4.49.08
Combine
7.90.07 18.11.15
Exact
7.04.07 17.80.15
5.59.10
5.62.10
Relaxed
6.29.06
5.83.05
6.38.11
6.38.11

8.86.08
8.86.08
8.86.08
8.86.08
6.89.06
6.86.06
8.94.08
8.94.08
7.27.07 27.92.20
7.90.07 26.39.19
7.04.07 25.71.19
6.63.06
5.83.05

Reuters Dataset
5.32.09 13.38.21
4.57.08
4.90.09
6.36.11
5.54.10
6.41.11
6.73.12

8.86.08
8.86.08
6.86.06
8.94.08
7.27.07
7.90.07
7.04.07
5.83.05

5.06.09
4.53.08
5.67.10
6.38.11

15.80 Synth2 Dataset

Table 1: Multi-labeling loss on six datasets. Results are grouped by dataset. Rows indicate sepa-
ration oracle method. Columns indicate classication inference method. The two quantities in the
dataset name row are edgeless (baseline) and default performance.

Great Big Table

LBP

Combine

Yeast Dataset

20.91.55

25.09 Synth1 Dataset

Combine

Exact

18.60.14

Exact

11.43.29

Greedy
Scene Dataset

Relaxed

LBP
18.10 Mediamill Dataset

Greedy

Relaxed
25.37
Greedy 10.67.28 10.74.28 10.67.28 10.67.28 10.67.28 23.39.16 25.66.17 24.32.17 24.92.17 27.05.18
LBP 10.45.27 10.54.27 10.45.27 10.42.27 10.49.27 22.83.16 22.83.16 22.83.16 22.83.16 22.83.16
Combine 10.72.28 11.78.30 10.72.28 10.77.28 11.20.29 19.56.14 20.12.15 19.72.14 19.82.14 20.23.15
Exact 10.08.26 10.33.27 10.08.26 10.06.26 10.20.26 19.07.14 27.23.18 19.08.14 18.75.14 36.83.21
Relaxed 10.55.27 10.49.27 10.49.27 10.49.27 10.49.27 18.50.14 18.26.14 18.26.14 18.21.14 18.29.14
8.99.08
16.34
8.86.08
8.86.08
Greedy 21.62.56 21.77.56 21.58.56 21.62.56 24.42.61
LBP 24.32.61 24.32.61 24.32.61 24.32.61 24.32.61 13.94.12 13.94.12 13.94.12 13.94.12 13.94.12
8.86.08
Combine 22.33.57 37.24.77 22.32.57 21.82.56 42.72.81
8.86.08
6.86.06
Exact 23.38.59 21.99.57 21.06.55 20.23.53 45.90.82
6.86.06
8.94.08
Relaxed 20.47.54 20.45.54 20.47.54 20.48.54 20.49.54
8.94.08
9.80.09
4.96.09
10.00
Greedy
5.42.09 16.98.26
7.28.07 19.03.15
LBP 15.80.25 15.80.25 15.80.25 15.80.25 15.80.25 10.00.09 10.00.09 10.00.09 10.00.09 10.00.09
4.55.08
4.49.08
Combine
7.90.07 18.11.15
Exact
7.04.07 17.80.15
5.59.10
5.62.10
Relaxed
6.38.11
6.29.06
5.83.05
6.38.11
 Results per dataset in blocks.

8.86.08
8.86.08
8.86.08
8.86.08
6.89.06
6.86.06
8.94.08
8.94.08
7.27.07 27.92.20
7.90.07 26.39.19
7.04.07 25.71.19
6.63.06
5.83.05

Reuters Dataset
5.32.09 13.38.21
4.57.08
4.90.09
6.36.11
5.54.10
6.41.11
6.73.12

8.86.08
8.86.08
6.86.06
8.94.08
7.27.07
7.90.07
7.04.07
5.83.05

5.06.09
4.53.08
5.67.10
6.38.11

15.80 Synth2 Dataset

Table 1: Multi-labeling loss on six datasets. Results are grouped by dataset. Rows indicate sepa-
ration oracle method. Columns indicate classication inference method. The two quantities in the
dataset name row are edgeless (baseline) and default performance.

Great Big Table

LBP

Combine

method (separation oracle).

Yeast Dataset

20.91.55

25.09 Synth1 Dataset

Combine

Exact

18.60.14

Exact

11.43.29

Greedy
Scene Dataset

Relaxed

LBP
18.10 Mediamill Dataset

Greedy

Relaxed
25.37
Greedy 10.67.28 10.74.28 10.67.28 10.67.28 10.67.28 23.39.16 25.66.17 24.32.17 24.92.17 27.05.18
LBP 10.45.27 10.54.27 10.45.27 10.42.27 10.49.27 22.83.16 22.83.16 22.83.16 22.83.16 22.83.16
Combine 10.72.28 11.78.30 10.72.28 10.77.28 11.20.29 19.56.14 20.12.15 19.72.14 19.82.14 20.23.15
Exact 10.08.26 10.33.27 10.08.26 10.06.26 10.20.26 19.07.14 27.23.18 19.08.14 18.75.14 36.83.21
Relaxed 10.55.27 10.49.27 10.49.27 10.49.27 10.49.27 18.50.14 18.26.14 18.26.14 18.21.14 18.29.14
8.99.08
16.34
8.86.08
8.86.08
Greedy 21.62.56 21.77.56 21.58.56 21.62.56 24.42.61
LBP 24.32.61 24.32.61 24.32.61 24.32.61 24.32.61 13.94.12 13.94.12 13.94.12 13.94.12 13.94.12
8.86.08
Combine 22.33.57 37.24.77 22.32.57 21.82.56 42.72.81
8.86.08
6.86.06
Exact 23.38.59 21.99.57 21.06.55 20.23.53 45.90.82
6.86.06
8.94.08
Relaxed 20.47.54 20.45.54 20.47.54 20.48.54 20.49.54
8.94.08
9.80.09
4.96.09
10.00
Greedy
5.42.09 16.98.26
7.28.07 19.03.15
LBP 15.80.25 15.80.25 15.80.25 15.80.25 15.80.25 10.00.09 10.00.09 10.00.09 10.00.09 10.00.09
4.55.08
4.49.08
Combine
7.90.07 18.11.15
Exact
7.04.07 17.80.15
5.59.10
5.62.10
Relaxed
6.38.11
6.29.06
5.83.05
6.38.11
 Results per dataset in blocks.
 Rows indicate training inference

8.86.08
8.86.08
8.86.08
8.86.08
6.89.06
6.86.06
8.94.08
8.94.08
7.27.07 27.92.20
7.90.07 26.39.19
7.04.07 25.71.19
6.63.06
5.83.05

Reuters Dataset
5.32.09 13.38.21
4.57.08
4.90.09
6.36.11
5.54.10
6.41.11
6.73.12

8.86.08
8.86.08
6.86.06
8.94.08
7.27.07
7.90.07
7.04.07
5.83.05

5.06.09
4.53.08
5.67.10
6.38.11

15.80 Synth2 Dataset

Table 1: Multi-labeling loss on six datasets. Results are grouped by dataset. Rows indicate sepa-
ration oracle method. Columns indicate classication inference method. The two quantities in the
dataset name row are edgeless (baseline) and default performance.

Great Big Table

LBP

Combine

Exact

11.43.29

Yeast Dataset

20.91.55

25.09 Synth1 Dataset

Combine

Exact

18.60.14

Greedy
Scene Dataset

Relaxed

LBP
18.10 Mediamill Dataset

Greedy

Relaxed
25.37
Greedy 10.67.28 10.74.28 10.67.28 10.67.28 10.67.28 23.39.16 25.66.17 24.32.17 24.92.17 27.05.18
LBP 10.45.27 10.54.27 10.45.27 10.42.27 10.49.27 22.83.16 22.83.16 22.83.16 22.83.16 22.83.16
Combine 10.72.28 11.78.30 10.72.28 10.77.28 11.20.29 19.56.14 20.12.15 19.72.14 19.82.14 20.23.15
Exact 10.08.26 10.33.27 10.08.26 10.06.26 10.20.26 19.07.14 27.23.18 19.08.14 18.75.14 36.83.21
Relaxed 10.55.27 10.49.27 10.49.27 10.49.27 10.49.27 18.50.14 18.26.14 18.26.14 18.21.14 18.29.14
8.99.08
16.34
8.86.08
8.86.08
Greedy 21.62.56 21.77.56 21.58.56 21.62.56 24.42.61
LBP 24.32.61 24.32.61 24.32.61 24.32.61 24.32.61 13.94.12 13.94.12 13.94.12 13.94.12 13.94.12
8.86.08
Combine 22.33.57 37.24.77 22.32.57 21.82.56 42.72.81
8.86.08
6.86.06
Exact 23.38.59 21.99.57 21.06.55 20.23.53 45.90.82
6.86.06
8.94.08
Relaxed 20.47.54 20.45.54 20.47.54 20.48.54 20.49.54
8.94.08
9.80.09
4.96.09
10.00
Greedy
5.42.09 16.98.26
7.28.07 19.03.15
LBP 15.80.25 15.80.25 15.80.25 15.80.25 15.80.25 10.00.09 10.00.09 10.00.09 10.00.09 10.00.09
4.55.08
4.49.08
Combine
7.90.07 18.11.15
Exact
7.04.07 17.80.15
5.59.10
5.62.10
Relaxed
6.38.11
6.29.06
5.83.05
6.38.11
 Results per dataset in blocks.
 Rows indicate training inference
 Columns indicate prediction

8.86.08
8.86.08
8.86.08
8.86.08
6.89.06
6.86.06
8.94.08
8.94.08
7.27.07 27.92.20
7.90.07 26.39.19
7.04.07 25.71.19
6.63.06
5.83.05

Reuters Dataset
5.32.09 13.38.21
4.57.08
4.90.09
6.36.11
5.54.10
6.41.11
6.73.12

8.86.08
8.86.08
6.86.06
8.94.08
7.27.07
7.90.07
7.04.07
5.83.05

method (separation oracle).

5.06.09
4.53.08
5.67.10
6.38.11

15.80 Synth2 Dataset

inference method.

Table 1: Multi-labeling loss on six datasets. Results are grouped by dataset. Rows indicate sepa-
ration oracle method. Columns indicate classication inference method. The two quantities in the
dataset name row are edgeless (baseline) and default performance.

Great Big Table

Greedy
Scene Dataset

LBP

Combine

Exact

11.43.29

Yeast Dataset

20.91.55

25.09 Synth1 Dataset

Combine

Exact

18.60.14

Relaxed

LBP
18.10 Mediamill Dataset

Greedy

Relaxed
25.37
Greedy 10.67.28 10.74.28 10.67.28 10.67.28 10.67.28 23.39.16 25.66.17 24.32.17 24.92.17 27.05.18
LBP 10.45.27 10.54.27 10.45.27 10.42.27 10.49.27 22.83.16 22.83.16 22.83.16 22.83.16 22.83.16
Combine 10.72.28 11.78.30 10.72.28 10.77.28 11.20.29 19.56.14 20.12.15 19.72.14 19.82.14 20.23.15
Exact 10.08.26 10.33.27 10.08.26 10.06.26 10.20.26 19.07.14 27.23.18 19.08.14 18.75.14 36.83.21
Relaxed 10.55.27 10.49.27 10.49.27 10.49.27 10.49.27 18.50.14 18.26.14 18.26.14 18.21.14 18.29.14
8.99.08
16.34
8.86.08
8.86.08
Greedy 21.62.56 21.77.56 21.58.56 21.62.56 24.42.61
LBP 24.32.61 24.32.61 24.32.61 24.32.61 24.32.61 13.94.12 13.94.12 13.94.12 13.94.12 13.94.12
8.86.08
Combine 22.33.57 37.24.77 22.32.57 21.82.56 42.72.81
8.86.08
6.86.06
Exact 23.38.59 21.99.57 21.06.55 20.23.53 45.90.82
6.86.06
8.94.08
Relaxed 20.47.54 20.45.54 20.47.54 20.48.54 20.49.54
8.94.08
9.80.09
4.96.09
10.00
Greedy
5.42.09 16.98.26
7.28.07 19.03.15
LBP 15.80.25 15.80.25 15.80.25 15.80.25 15.80.25 10.00.09 10.00.09 10.00.09 10.00.09 10.00.09
4.55.08
4.49.08
Combine
7.90.07 18.11.15
Exact
7.04.07 17.80.15
5.59.10
5.62.10
Relaxed
6.38.11
6.29.06
5.83.05
6.38.11
 Results per dataset in blocks.
 Rows indicate training inference
 Columns indicate prediction

 Numbers are Hamming loss
percentage,  standard error
(with a twist).

8.86.08
8.86.08
8.86.08
8.86.08
6.89.06
6.86.06
8.94.08
8.94.08
7.27.07 27.92.20
7.90.07 26.39.19
7.04.07 25.71.19
6.63.06
5.83.05

Reuters Dataset
5.32.09 13.38.21
4.57.08
4.90.09
6.36.11
5.54.10
6.41.11
6.73.12

8.86.08
8.86.08
6.86.06
8.94.08
7.27.07
7.90.07
7.04.07
5.83.05

method (separation oracle).

5.06.09
4.53.08
5.67.10
6.38.11

15.80 Synth2 Dataset

inference method.

Table 1: Multi-labeling loss on six datasets. Results are grouped by dataset. Rows indicate sepa-
ration oracle method. Columns indicate classication inference method. The two quantities in the
dataset name row are edgeless (baseline) and default performance.

Great Big Table

Greedy
Scene Dataset

LBP

Combine

Exact

11.43.29

Yeast Dataset

20.91.55

25.09 Synth1 Dataset

Combine

Exact

18.60.14

Relaxed

LBP
18.10 Mediamill Dataset

Greedy

Relaxed
25.37
Greedy 10.67.28 10.74.28 10.67.28 10.67.28 10.67.28 23.39.16 25.66.17 24.32.17 24.92.17 27.05.18
LBP 10.45.27 10.54.27 10.45.27 10.42.27 10.49.27 22.83.16 22.83.16 22.83.16 22.83.16 22.83.16
Combine 10.72.28 11.78.30 10.72.28 10.77.28 11.20.29 19.56.14 20.12.15 19.72.14 19.82.14 20.23.15
Exact 10.08.26 10.33.27 10.08.26 10.06.26 10.20.26 19.07.14 27.23.18 19.08.14 18.75.14 36.83.21
Relaxed 10.55.27 10.49.27 10.49.27 10.49.27 10.49.27 18.50.14 18.26.14 18.26.14 18.21.14 18.29.14
8.99.08
16.34
8.86.08
8.86.08
Greedy 21.62.56 21.77.56 21.58.56 21.62.56 24.42.61
LBP 24.32.61 24.32.61 24.32.61 24.32.61 24.32.61 13.94.12 13.94.12 13.94.12 13.94.12 13.94.12
8.86.08
Combine 22.33.57 37.24.77 22.32.57 21.82.56 42.72.81
8.86.08
6.86.06
Exact 23.38.59 21.99.57 21.06.55 20.23.53 45.90.82
6.86.06
8.94.08
Relaxed 20.47.54 20.45.54 20.47.54 20.48.54 20.49.54
8.94.08
9.80.09
4.96.09
10.00
Greedy
5.42.09 16.98.26
7.28.07 19.03.15
LBP 15.80.25 15.80.25 15.80.25 15.80.25 15.80.25 10.00.09 10.00.09 10.00.09 10.00.09 10.00.09
4.55.08
4.49.08
Combine
7.90.07 18.11.15
Exact
7.04.07 17.80.15
5.59.10
5.62.10
Relaxed
6.38.11
6.29.06
5.83.05
6.38.11
 Results per dataset in blocks.
 Rows indicate training inference
 Columns indicate prediction

 Numbers are Hamming loss
percentage,  standard error
(with a twist).

8.86.08
8.86.08
8.86.08
8.86.08
6.89.06
6.86.06
8.94.08
8.94.08
7.27.07 27.92.20
7.90.07 26.39.19
7.04.07 25.71.19
6.63.06
5.83.05

 Edgeless loss next to name.

Reuters Dataset
5.32.09 13.38.21
4.57.08
4.90.09
6.36.11
5.54.10
6.41.11
6.73.12

8.86.08
8.86.08
6.86.06
8.94.08
7.27.07
7.90.07
7.04.07
5.83.05

method (separation oracle).

5.06.09
4.53.08
5.67.10
6.38.11

15.80 Synth2 Dataset

inference method.

Table 1: Multi-labeling loss on six datasets. Results are grouped by dataset. Rows indicate sepa-
ration oracle method. Columns indicate classication inference method. The two quantities in the
dataset name row are edgeless (baseline) and default performance.

Great Big Table

Greedy
Scene Dataset

LBP

Combine

Exact

11.43.29

Yeast Dataset

15.80 Synth2 Dataset

20.91.55

25.09 Synth1 Dataset

Combine

Exact

18.60.14

Relaxed

LBP
18.10 Mediamill Dataset

Greedy

Relaxed
25.37
Greedy 10.67.28 10.74.28 10.67.28 10.67.28 10.67.28 23.39.16 25.66.17 24.32.17 24.92.17 27.05.18
LBP 10.45.27 10.54.27 10.45.27 10.42.27 10.49.27 22.83.16 22.83.16 22.83.16 22.83.16 22.83.16
Combine 10.72.28 11.78.30 10.72.28 10.77.28 11.20.29 19.56.14 20.12.15 19.72.14 19.82.14 20.23.15
Exact 10.08.26 10.33.27 10.08.26 10.06.26 10.20.26 19.07.14 27.23.18 19.08.14 18.75.14 36.83.21
Relaxed 10.55.27 10.49.27 10.49.27 10.49.27 10.49.27 18.50.14 18.26.14 18.26.14 18.21.14 18.29.14
8.99.08
16.34
8.86.08
8.86.08
Greedy 21.62.56 21.77.56 21.58.56 21.62.56 24.42.61
LBP 24.32.61 24.32.61 24.32.61 24.32.61 24.32.61 13.94.12 13.94.12 13.94.12 13.94.12 13.94.12
8.86.08
Combine 22.33.57 37.24.77 22.32.57 21.82.56 42.72.81
8.86.08
6.86.06
Exact 23.38.59 21.99.57 21.06.55 20.23.53 45.90.82
6.86.06
8.94.08
Relaxed 20.47.54 20.45.54 20.47.54 20.48.54 20.49.54
8.94.08
9.80.09
4.96.09
10.00
Greedy
5.42.09 16.98.26
7.28.07 19.03.15
LBP 15.80.25 15.80.25 15.80.25 15.80.25 15.80.25 10.00.09 10.00.09 10.00.09 10.00.09 10.00.09
4.55.08
4.49.08
Combine
7.90.07 18.11.15
Exact
7.04.07 17.80.15
5.59.10
5.62.10
Relaxed
6.38.11
6.29.06
5.83.05
6.38.11
 Results per dataset in blocks.
 Rows indicate training inference
 Columns indicate prediction

 Numbers are Hamming loss
percentage,  standard error
(with a twist).

8.86.08
8.86.08
8.86.08
8.86.08
6.89.06
6.86.06
8.94.08
8.94.08
7.27.07 27.92.20
7.90.07 26.39.19
7.04.07 25.71.19
6.63.06
5.83.05

Reuters Dataset
5.32.09 13.38.21
4.57.08
4.90.09
6.36.11
5.54.10
6.41.11
6.73.12

8.86.08
8.86.08
6.86.06
8.94.08
7.27.07
7.90.07
7.04.07
5.83.05

method (separation oracle).

 Edgeless loss next to name.
 Default loss next to that.

inference method.

5.06.09
4.53.08
5.67.10
6.38.11

Table 1: Multi-labeling loss on six datasets. Results are grouped by dataset. Rows indicate sepa-
ration oracle method. Columns indicate classication inference method. The two quantities in the
dataset name row are edgeless (baseline) and default performance.

The Sorry State of LBP

LBP

Combine

Yeast Dataset

20.91.55

25.09 Synth1 Dataset

Combine

Exact

18.60.14

Exact

11.43.29

Greedy
Scene Dataset

Relaxed

LBP
18.10 Mediamill Dataset

Greedy

Relaxed
25.37
Greedy 10.67.28 10.74.28 10.67.28 10.67.28 10.67.28 23.39.16 25.66.17 24.32.17 24.92.17 27.05.18
LBP 10.45.27 10.54.27 10.45.27 10.42.27 10.49.27 22.83.16 22.83.16 22.83.16 22.83.16 22.83.16
Combine 10.72.28 11.78.30 10.72.28 10.77.28 11.20.29 19.56.14 20.12.15 19.72.14 19.82.14 20.23.15
Exact 10.08.26 10.33.27 10.08.26 10.06.26 10.20.26 19.07.14 27.23.18 19.08.14 18.75.14 36.83.21
Relaxed 10.55.27 10.49.27 10.49.27 10.49.27 10.49.27 18.50.14 18.26.14 18.26.14 18.21.14 18.29.14
8.99.08
16.34
8.86.08
8.86.08
Greedy 21.62.56 21.77.56 21.58.56 21.62.56 24.42.61
LBP 24.32.61 24.32.61 24.32.61 24.32.61 24.32.61 13.94.12 13.94.12 13.94.12 13.94.12 13.94.12
8.86.08
Combine 22.33.57 37.24.77 22.32.57 21.82.56 42.72.81
8.86.08
6.86.06
Exact 23.38.59 21.99.57 21.06.55 20.23.53 45.90.82
6.86.06
8.94.08
Relaxed 20.47.54 20.45.54 20.47.54 20.48.54 20.49.54
8.94.08
9.80.09
4.96.09
10.00
Greedy
5.42.09 16.98.26
7.28.07 19.03.15
LBP 15.80.25 15.80.25 15.80.25 15.80.25 15.80.25 10.00.09 10.00.09 10.00.09 10.00.09 10.00.09
4.55.08
4.49.08
Combine
7.90.07 18.11.15
Exact
7.04.07 17.80.15
5.59.10
5.62.10
Relaxed
6.29.06
5.83.05
6.38.11
6.38.11

8.86.08
8.86.08
8.86.08
8.86.08
6.89.06
6.86.06
8.94.08
8.94.08
7.27.07 27.92.20
7.90.07 26.39.19
7.04.07 25.71.19
6.63.06
5.83.05

Reuters Dataset
5.32.09 13.38.21
4.57.08
4.90.09
6.36.11
5.54.10
6.41.11
6.73.12

8.86.08
8.86.08
6.86.06
8.94.08
7.27.07
7.90.07
7.04.07
5.83.05

5.06.09
4.53.08
5.67.10
6.38.11

15.80 Synth2 Dataset

Table 1: Multi-labeling loss on six datasets. Results are grouped by dataset. Rows indicate sepa-
ration oracle method. Columns indicate classication inference method. The two quantities in the
dataset name row are edgeless (baseline) and default performance.

The Sorry State of LBP

LBP

Combine

Yeast Dataset

20.91.55

25.09 Synth1 Dataset

Combine

Exact

18.60.14

Exact

11.43.29

Greedy
Scene Dataset

Relaxed

LBP
18.10 Mediamill Dataset

Greedy

Relaxed
25.37
Greedy 10.67.28 10.74.28 10.67.28 10.67.28 10.67.28 23.39.16 25.66.17 24.32.17 24.92.17 27.05.18
LBP 10.45.27 10.54.27 10.45.27 10.42.27 10.49.27 22.83.16 22.83.16 22.83.16 22.83.16 22.83.16
Combine 10.72.28 11.78.30 10.72.28 10.77.28 11.20.29 19.56.14 20.12.15 19.72.14 19.82.14 20.23.15
Exact 10.08.26 10.33.27 10.08.26 10.06.26 10.20.26 19.07.14 27.23.18 19.08.14 18.75.14 36.83.21
Relaxed 10.55.27 10.49.27 10.49.27 10.49.27 10.49.27 18.50.14 18.26.14 18.26.14 18.21.14 18.29.14
8.99.08
16.34
8.86.08
8.86.08
Greedy 21.62.56 21.77.56 21.58.56 21.62.56 24.42.61
LBP 24.32.61 24.32.61 24.32.61 24.32.61 24.32.61 13.94.12 13.94.12 13.94.12 13.94.12 13.94.12
8.86.08
Combine 22.33.57 37.24.77 22.32.57 21.82.56 42.72.81
8.86.08
6.86.06
Exact 23.38.59 21.99.57 21.06.55 20.23.53 45.90.82
6.86.06
8.94.08
Relaxed 20.47.54 20.45.54 20.47.54 20.48.54 20.49.54
8.94.08
9.80.09
4.96.09
10.00
Greedy
5.42.09 16.98.26
7.28.07 19.03.15
LBP 15.80.25 15.80.25 15.80.25 15.80.25 15.80.25 10.00.09 10.00.09 10.00.09 10.00.09 10.00.09
4.55.08
4.49.08
Combine
7.90.07 18.11.15
Exact
7.04.07 17.80.15
5.59.10
5.62.10
Relaxed
6.38.11
6.29.06
5.83.05
6.38.11
 Models trained with LBP often
have terrible performance.

8.86.08
8.86.08
8.86.08
8.86.08
6.89.06
6.86.06
8.94.08
8.94.08
7.27.07 27.92.20
7.90.07 26.39.19
7.04.07 25.71.19
6.63.06
5.83.05

Reuters Dataset
5.32.09 13.38.21
4.57.08
4.90.09
6.36.11
5.54.10
6.41.11
6.73.12

8.86.08
8.86.08
6.86.06
8.94.08
7.27.07
7.90.07
7.04.07
5.83.05

5.06.09
4.53.08
5.67.10
6.38.11

15.80 Synth2 Dataset

Table 1: Multi-labeling loss on six datasets. Results are grouped by dataset. Rows indicate sepa-
ration oracle method. Columns indicate classication inference method. The two quantities in the
dataset name row are edgeless (baseline) and default performance.

The Sorry State of LBP

LBP

Combine

Exact

11.43.29

Yeast Dataset

20.91.55

25.09 Synth1 Dataset

Combine

Exact

18.60.14

Greedy
Scene Dataset

Relaxed

LBP
18.10 Mediamill Dataset

Greedy

Relaxed
25.37
Greedy 10.67.28 10.74.28 10.67.28 10.67.28 10.67.28 23.39.16 25.66.17 24.32.17 24.92.17 27.05.18
LBP 10.45.27 10.54.27 10.45.27 10.42.27 10.49.27 22.83.16 22.83.16 22.83.16 22.83.16 22.83.16
Combine 10.72.28 11.78.30 10.72.28 10.77.28 11.20.29 19.56.14 20.12.15 19.72.14 19.82.14 20.23.15
Exact 10.08.26 10.33.27 10.08.26 10.06.26 10.20.26 19.07.14 27.23.18 19.08.14 18.75.14 36.83.21
Relaxed 10.55.27 10.49.27 10.49.27 10.49.27 10.49.27 18.50.14 18.26.14 18.26.14 18.21.14 18.29.14
8.99.08
16.34
8.86.08
8.86.08
Greedy 21.62.56 21.77.56 21.58.56 21.62.56 24.42.61
LBP 24.32.61 24.32.61 24.32.61 24.32.61 24.32.61 13.94.12 13.94.12 13.94.12 13.94.12 13.94.12
8.86.08
Combine 22.33.57 37.24.77 22.32.57 21.82.56 42.72.81
8.86.08
6.86.06
Exact 23.38.59 21.99.57 21.06.55 20.23.53 45.90.82
6.86.06
8.94.08
Relaxed 20.47.54 20.45.54 20.47.54 20.48.54 20.49.54
8.94.08
9.80.09
4.96.09
10.00
Greedy
5.42.09 16.98.26
7.28.07 19.03.15
LBP 15.80.25 15.80.25 15.80.25 15.80.25 15.80.25 10.00.09 10.00.09 10.00.09 10.00.09 10.00.09
4.55.08
4.49.08
Combine
7.90.07 18.11.15
Exact
7.04.07 17.80.15
5.59.10
5.62.10
Relaxed
6.29.06
6.38.11
5.83.05
6.38.11
 Predictions made with LBP also
 Models trained with LBP often
have terrible performance.

8.86.08
8.86.08
8.86.08
8.86.08
6.89.06
6.86.06
8.94.08
8.94.08
7.27.07 27.92.20
7.90.07 26.39.19
7.04.07 25.71.19
6.63.06
5.83.05

Reuters Dataset
5.32.09 13.38.21
4.57.08
4.90.09
6.36.11
5.54.10
6.41.11
6.73.12

8.86.08
8.86.08
6.86.06
8.94.08
7.27.07
7.90.07
7.04.07
5.83.05

are often quite poor.

5.06.09
4.53.08
5.67.10
6.38.11

15.80 Synth2 Dataset

Table 1: Multi-labeling loss on six datasets. Results are grouped by dataset. Rows indicate sepa-
ration oracle method. Columns indicate classication inference method. The two quantities in the
dataset name row are edgeless (baseline) and default performance.

The Sorry State of LBP

LBP

Combine

Exact

11.43.29

Yeast Dataset

20.91.55

25.09 Synth1 Dataset

Combine

Exact

18.60.14

Greedy
Scene Dataset

Relaxed

LBP
18.10 Mediamill Dataset

Greedy

Relaxed
25.37
Greedy 10.67.28 10.74.28 10.67.28 10.67.28 10.67.28 23.39.16 25.66.17 24.32.17 24.92.17 27.05.18
LBP 10.45.27 10.54.27 10.45.27 10.42.27 10.49.27 22.83.16 22.83.16 22.83.16 22.83.16 22.83.16
Combine 10.72.28 11.78.30 10.72.28 10.77.28 11.20.29 19.56.14 20.12.15 19.72.14 19.82.14 20.23.15
Exact 10.08.26 10.33.27 10.08.26 10.06.26 10.20.26 19.07.14 27.23.18 19.08.14 18.75.14 36.83.21
Relaxed 10.55.27 10.49.27 10.49.27 10.49.27 10.49.27 18.50.14 18.26.14 18.26.14 18.21.14 18.29.14
8.99.08
16.34
8.86.08
8.86.08
Greedy 21.62.56 21.77.56 21.58.56 21.62.56 24.42.61
LBP 24.32.61 24.32.61 24.32.61 24.32.61 24.32.61 13.94.12 13.94.12 13.94.12 13.94.12 13.94.12
8.86.08
Combine 22.33.57 37.24.77 22.32.57 21.82.56 42.72.81
8.86.08
6.86.06
Exact 23.38.59 21.99.57 21.06.55 20.23.53 45.90.82
6.86.06
8.94.08
Relaxed 20.47.54 20.45.54 20.47.54 20.48.54 20.49.54
8.94.08
9.80.09
4.96.09
10.00
Greedy
5.42.09 16.98.26
7.28.07 19.03.15
LBP 15.80.25 15.80.25 15.80.25 15.80.25 15.80.25 10.00.09 10.00.09 10.00.09 10.00.09 10.00.09
4.55.08
4.49.08
Combine
7.90.07 18.11.15
Exact
7.04.07 17.80.15
5.59.10
5.62.10
Relaxed
6.29.06
6.38.11
5.83.05
6.38.11
 Predictions made with LBP also
 Models trained with LBP often
have terrible performance.
 Likely explanation?

8.86.08
8.86.08
8.86.08
8.86.08
6.89.06
6.86.06
8.94.08
8.94.08
7.27.07 27.92.20
7.90.07 26.39.19
7.04.07 25.71.19
6.63.06
5.83.05

Reuters Dataset
5.32.09 13.38.21
4.57.08
4.90.09
6.36.11
5.54.10
6.41.11
6.73.12

8.86.08
8.86.08
6.86.06
8.94.08
7.27.07
7.90.07
7.04.07
5.83.05

are often quite poor.

5.06.09
4.53.08
5.67.10
6.38.11

15.80 Synth2 Dataset

Table 1: Multi-labeling loss on six datasets. Results are grouped by dataset. Rows indicate sepa-
ration oracle method. Columns indicate classication inference method. The two quantities in the
dataset name row are edgeless (baseline) and default performance.

Relaxation

LBP

Combine

Yeast Dataset

20.91.55

25.09 Synth1 Dataset

Combine

Exact

18.60.14

Exact

11.43.29

Greedy
Scene Dataset

Relaxed

LBP
18.10 Mediamill Dataset

Greedy

Relaxed
25.37
Greedy 10.67.28 10.74.28 10.67.28 10.67.28 10.67.28 23.39.16 25.66.17 24.32.17 24.92.17 27.05.18
LBP 10.45.27 10.54.27 10.45.27 10.42.27 10.49.27 22.83.16 22.83.16 22.83.16 22.83.16 22.83.16
Combine 10.72.28 11.78.30 10.72.28 10.77.28 11.20.29 19.56.14 20.12.15 19.72.14 19.82.14 20.23.15
Exact 10.08.26 10.33.27 10.08.26 10.06.26 10.20.26 19.07.14 27.23.18 19.08.14 18.75.14 36.83.21
Relaxed 10.55.27 10.49.27 10.49.27 10.49.27 10.49.27 18.50.14 18.26.14 18.26.14 18.21.14 18.29.14
8.99.08
16.34
8.86.08
8.86.08
Greedy 21.62.56 21.77.56 21.58.56 21.62.56 24.42.61
LBP 24.32.61 24.32.61 24.32.61 24.32.61 24.32.61 13.94.12 13.94.12 13.94.12 13.94.12 13.94.12
8.86.08
Combine 22.33.57 37.24.77 22.32.57 21.82.56 42.72.81
8.86.08
6.86.06
Exact 23.38.59 21.99.57 21.06.55 20.23.53 45.90.82
6.86.06
8.94.08
Relaxed 20.47.54 20.45.54 20.47.54 20.48.54 20.49.54
8.94.08
9.80.09
4.96.09
10.00
Greedy
5.42.09 16.98.26
7.28.07 19.03.15
LBP 15.80.25 15.80.25 15.80.25 15.80.25 15.80.25 10.00.09 10.00.09 10.00.09 10.00.09 10.00.09
4.55.08
4.49.08
Combine
7.90.07 18.11.15
Exact
7.04.07 17.80.15
5.59.10
5.62.10
Relaxed
6.29.06
5.83.05
6.38.11
6.38.11

8.86.08
8.86.08
8.86.08
8.86.08
6.89.06
6.86.06
8.94.08
8.94.08
7.27.07 27.92.20
7.90.07 26.39.19
7.04.07 25.71.19
6.63.06
5.83.05

Reuters Dataset
5.32.09 13.38.21
4.57.08
4.90.09
6.36.11
5.54.10
6.41.11
6.73.12

8.86.08
8.86.08
6.86.06
8.94.08
7.27.07
7.90.07
7.04.07
5.83.05

5.06.09
4.53.08
5.67.10
6.38.11

15.80 Synth2 Dataset

Table 1: Multi-labeling loss on six datasets. Results are grouped by dataset. Rows indicate sepa-
ration oracle method. Columns indicate classication inference method. The two quantities in the
dataset name row are edgeless (baseline) and default performance.

Relaxation

LBP

Combine

Yeast Dataset

20.91.55

Combine

Exact

18.60.14

Exact

11.43.29

Greedy
Scene Dataset

Relaxed

LBP
18.10 Mediamill Dataset

Greedy

Relaxed
25.37
Greedy 10.67.28 10.74.28 10.67.28 10.67.28 10.67.28 23.39.16 25.66.17 24.32.17 24.92.17 27.05.18
LBP 10.45.27 10.54.27 10.45.27 10.42.27 10.49.27 22.83.16 22.83.16 22.83.16 22.83.16 22.83.16
Combine 10.72.28 11.78.30 10.72.28 10.77.28 11.20.29 19.56.14 20.12.15 19.72.14 19.82.14 20.23.15
Exact 10.08.26 10.33.27 10.08.26 10.06.26 10.20.26 19.07.14 27.23.18 19.08.14 18.75.14 36.83.21
Relaxed 10.55.27 10.49.27 10.49.27 10.49.27 10.49.27 18.50.14 18.26.14 18.26.14 18.21.14 18.29.14
8.99.08
16.34
8.86.08
8.86.08
Greedy 21.62.56 21.77.56 21.58.56 21.62.56 24.42.61
LBP 24.32.61 24.32.61 24.32.61 24.32.61 24.32.61 13.94.12 13.94.12 13.94.12 13.94.12 13.94.12
8.86.08
Combine 22.33.57 37.24.77 22.32.57 21.82.56 42.72.81
8.86.08
6.86.06
Exact 23.38.59 21.99.57 21.06.55 20.23.53 45.90.82
6.86.06
8.94.08
Relaxed 20.47.54 20.45.54 20.47.54 20.48.54 20.49.54
8.94.08
9.80.09
4.96.09
10.00
Greedy
5.42.09 16.98.26
7.28.07 19.03.15
LBP 15.80.25 15.80.25 15.80.25 15.80.25 15.80.25 10.00.09 10.00.09 10.00.09 10.00.09 10.00.09
4.55.08
4.49.08
Combine
7.90.07 18.11.15
Exact
7.04.07 17.80.15
5.59.10
5.62.10
Relaxed
6.29.06
5.83.05
6.38.11
6.38.11

8.86.08
8.86.08
8.86.08
8.86.08
6.89.06
6.86.06
8.94.08
8.94.08
7.27.07 27.92.20
7.90.07 26.39.19
7.04.07 25.71.19
6.63.06
5.83.05

Reuters Dataset
5.32.09 13.38.21
4.57.08
4.90.09
6.36.11
5.54.10
6.41.11
6.73.12

8.86.08
8.86.08
6.86.06
8.94.08
7.27.07
7.90.07
7.04.07
5.83.05

5.06.09
4.53.08
5.67.10
6.38.11

25.09 Synth1 Dataset

15.80 Synth2 Dataset

 Notice predictor consistency

with relaxed trained models.

Table 1: Multi-labeling loss on six datasets. Results are grouped by dataset. Rows indicate sepa-
ration oracle method. Columns indicate classication inference method. The two quantities in the
dataset name row are edgeless (baseline) and default performance.

Relaxation

LBP

Combine

Yeast Dataset

20.91.55

Combine

Exact

18.60.14

Exact

11.43.29

Greedy
Scene Dataset

Relaxed

LBP
18.10 Mediamill Dataset

Greedy

Relaxed
25.37
Greedy 10.67.28 10.74.28 10.67.28 10.67.28 10.67.28 23.39.16 25.66.17 24.32.17 24.92.17 27.05.18
LBP 10.45.27 10.54.27 10.45.27 10.42.27 10.49.27 22.83.16 22.83.16 22.83.16 22.83.16 22.83.16
Combine 10.72.28 11.78.30 10.72.28 10.77.28 11.20.29 19.56.14 20.12.15 19.72.14 19.82.14 20.23.15
Exact 10.08.26 10.33.27 10.08.26 10.06.26 10.20.26 19.07.14 27.23.18 19.08.14 18.75.14 36.83.21
Relaxed 10.55.27 10.49.27 10.49.27 10.49.27 10.49.27 18.50.14 18.26.14 18.26.14 18.21.14 18.29.14
8.99.08
16.34
8.86.08
8.86.08
Greedy 21.62.56 21.77.56 21.58.56 21.62.56 24.42.61
LBP 24.32.61 24.32.61 24.32.61 24.32.61 24.32.61 13.94.12 13.94.12 13.94.12 13.94.12 13.94.12
8.86.08
Combine 22.33.57 37.24.77 22.32.57 21.82.56 42.72.81
8.86.08
6.86.06
Exact 23.38.59 21.99.57 21.06.55 20.23.53 45.90.82
6.86.06
8.94.08
Relaxed 20.47.54 20.45.54 20.47.54 20.48.54 20.49.54
8.94.08
9.80.09
4.96.09
10.00
Greedy
5.42.09 16.98.26
7.28.07 19.03.15
LBP 15.80.25 15.80.25 15.80.25 15.80.25 15.80.25 10.00.09 10.00.09 10.00.09 10.00.09 10.00.09
4.55.08
4.49.08
Combine
7.90.07 18.11.15
Exact
7.04.07 17.80.15
5.59.10
5.62.10
Relaxed
6.29.06
5.83.05
6.38.11
6.38.11

8.86.08
8.86.08
8.86.08
8.86.08
6.89.06
6.86.06
8.94.08
8.94.08
7.27.07 27.92.20
7.90.07 26.39.19
7.04.07 25.71.19
6.63.06
5.83.05

Reuters Dataset
5.32.09 13.38.21
4.57.08
4.90.09
6.36.11
5.54.10
6.41.11
6.73.12

8.86.08
8.86.08
6.86.06
8.94.08
7.27.07
7.90.07
7.04.07
5.83.05

5.06.09
4.53.08
5.67.10
6.38.11

25.09 Synth1 Dataset

15.80 Synth2 Dataset

with relaxed trained models.

 Notice predictor consistency
 Notice occasional ludicrously
poor performance of relaxation
as a classier.

Yeast Dataset

20.91.55

Combine

Exact

18.60.14

Exact

11.43.29

Greedy
Scene Dataset

Relaxed

LBP
18.10 Mediamill Dataset

Greedy

Relaxed
25.37
Greedy 10.67.28 10.74.28 10.67.28 10.67.28 10.67.28 23.39.16 25.66.17 24.32.17 24.92.17 27.05.18
LBP 10.45.27 10.54.27 10.45.27 10.42.27 10.49.27 22.83.16 22.83.16 22.83.16 22.83.16 22.83.16
Combine 10.72.28 11.78.30 10.72.28 10.77.28 11.20.29 19.56.14 20.12.15 19.72.14 19.82.14 20.23.15
Exact 10.08.26 10.33.27 10.08.26 10.06.26 10.20.26 19.07.14 27.23.18 19.08.14 18.75.14 36.83.21
Relaxed 10.55.27 10.49.27 10.49.27 10.49.27 10.49.27 18.50.14 18.26.14 18.26.14 18.21.14 18.29.14
8.99.08
16.34
8.86.08
8.86.08
Greedy 21.62.56 21.77.56 21.58.56 21.62.56 24.42.61
LBP 24.32.61 24.32.61 24.32.61 24.32.61 24.32.61 13.94.12 13.94.12 13.94.12 13.94.12 13.94.12
8.86.08
Combine 22.33.57 37.24.77 22.32.57 21.82.56 42.72.81
8.86.08
6.86.06
Exact 23.38.59 21.99.57 21.06.55 20.23.53 45.90.82
6.86.06
8.94.08
Relaxed 20.47.54 20.45.54 20.47.54 20.48.54 20.49.54
8.94.08
9.80.09
4.96.09
10.00
Greedy
5.42.09 16.98.26
7.28.07 19.03.15
LBP 15.80.25 15.80.25 15.80.25 15.80.25 15.80.25 10.00.09 10.00.09 10.00.09 10.00.09 10.00.09
4.55.08
4.49.08
Combine
7.90.07 18.11.15
Exact
7.04.07 17.80.15
5.59.10
5.62.10
Relaxed
6.29.06
5.83.05
6.38.11
6.38.11
 Presence of fractional constraints

8.86.08
8.86.08
8.86.08
8.86.08
6.89.06
6.86.06
8.94.08
8.94.08
7.27.07 27.92.20
7.90.07 26.39.19
7.04.07 25.71.19
6.63.06
5.83.05

Reuters Dataset
5.32.09 13.38.21
4.57.08
4.90.09
6.36.11
5.54.10
6.41.11
6.73.12

8.86.08
8.86.08
6.86.06
8.94.08
7.27.07
7.90.07
7.04.07
5.83.05

5.06.09
4.53.08
5.67.10
6.38.11

25.09 Synth1 Dataset

15.80 Synth2 Dataset

Table 1: Multi-labeling loss on six datasets. Results are grouped by dataset. Rows indicate sepa-
ration oracle method. Columns indicate classication inference method. The two quantities in the
dataset name row are edgeless (baseline) and default performance.

Relaxation

LBP

Combine

with relaxed trained models.

 Notice predictor consistency
 Notice occasional ludicrously
poor performance of relaxation
as a classier.

leads to smoothed easier
space.

Yeast Dataset

20.91.55

25.09 Synth1 Dataset

LBP

Combine

Relaxed

Exact

18.60.14

Exact

11.43.29

Greedy
Scene Dataset

LBP
18.10 Mediamill Dataset

Greedy

Relaxed
25.37
Greedy 10.67.28 10.74.28 10.67.28 10.67.28 10.67.28 23.39.16 25.66.17 24.32.17 24.92.17 27.05.18
LBP 10.45.27 10.54.27 10.45.27 10.42.27 10.49.27 22.83.16 22.83.16 22.83.16 22.83.16 22.83.16
Combine 10.72.28 11.78.30 10.72.28 10.77.28 11.20.29 19.56.14 20.12.15 19.72.14 19.82.14 20.23.15
Exact 10.08.26 10.33.27 10.08.26 10.06.26 10.20.26 19.07.14 27.23.18 19.08.14 18.75.14 36.83.21
Relaxed 10.55.27 10.49.27 10.49.27 10.49.27 10.49.27 18.50.14 18.26.14 18.26.14 18.21.14 18.29.14
8.99.08
16.34
8.86.08
8.86.08
Greedy 21.62.56 21.77.56 21.58.56 21.62.56 24.42.61
LBP 24.32.61 24.32.61 24.32.61 24.32.61 24.32.61 13.94.12 13.94.12 13.94.12 13.94.12 13.94.12
8.86.08
Combine 22.33.57 37.24.77 22.32.57 21.82.56 42.72.81
8.86.08
6.86.06
Exact 23.38.59 21.99.57 21.06.55 20.23.53 45.90.82
6.86.06
8.94.08
Relaxed 20.47.54 20.45.54 20.47.54 20.48.54 20.49.54
8.94.08
9.80.09
4.96.09
10.00
Greedy
5.42.09 16.98.26
7.28.07 19.03.15
LBP 15.80.25 15.80.25 15.80.25 15.80.25 15.80.25 10.00.09 10.00.09 10.00.09 10.00.09 10.00.09
4.55.08
4.49.08
Combine
7.90.07 18.11.15
Exact
7.04.07 17.80.15
5.59.10
5.62.10
Relaxed
6.29.06
5.83.05
6.38.11
6.38.11
 Presence of fractional constraints

8.86.08
8.86.08
8.86.08
8.86.08
6.89.06
6.86.06
8.94.08
8.94.08
7.27.07 27.92.20
7.90.07 26.39.19
7.04.07 25.71.19
6.63.06
5.83.05

Reuters Dataset
5.32.09 13.38.21
4.57.08
4.90.09
6.36.11
5.54.10
6.41.11
6.73.12

8.86.08
8.86.08
6.86.06
8.94.08
7.27.07
7.90.07
7.04.07
5.83.05

5.06.09
4.53.08
5.67.10
6.38.11

15.80 Synth2 Dataset

Table 1: Multi-labeling loss on six datasets. Results are grouped by dataset. Rows indicate sepa-
ration oracle method. Columns indicate classication inference method. The two quantities in the
dataset name row are edgeless (baseline) and default performance.

Relaxation

Combine

with relaxed trained models.

 Notice predictor consistency
 Notice occasional ludicrously
poor performance of relaxation
as a classier.

leads to smoothed easier
space.

 Lack of fractional constraints in

other models hurts relaxed
predictor.


