Abstract

We propose Coactive Learning as a model of
interaction between a learning system and a
human user, where both have the common
goal of providing results of maximum util-
ity to the user. At each step, the system
(e.g. search engine) receives a context (e.g.
query) and predicts an object (e.g. ranking).
The user responds by correcting the system
if necessary, providing a slightly improved 
but not necessarily optimal  object as feed-
back. We argue that such feedback can often
be inferred from observable user behavior, for
example, from clicks in web-search. Evalu-
ating predictions by their cardinal utility to
the user, we propose ecient learning algo-
rithms that have O( 1T
) average regret, even
though the learning algorithm never observes
cardinal utility values as in conventional on-
line learning. We demonstrate the applica-
bility of our model and learning algorithms
on a movie recommendation task, as well as
ranking for web-search.

Movie Recommendation: An online service recom-
mends movie A to a user. However, the user rents
movie B after browsing the collection.

Machine Translation: An online machine transla-
tor is used to translate a wiki page from language
A to B. The system observes some corrections the
user makes to the translated text.

In all the above examples, the user provides some
feedback about the results of the system. However,
the feedback is only an incremental improvement, not
necessarily the optimal result. For example, from the
clicks on the web-search results we can infer that the
user would have preferred the ranking [B, D, A, C, ...]
over the one we presented. However, this is unlikely to
be the best possible ranking. Similarly in the recom-
mendation example, movie B was preferred over movie
A, but there may have been even better movies that
the user did not nd while browsing. In summary, the
algorithm typically receives a slightly improved result
from the user as feedback, but not necessarily the op-
timal prediction nor any cardinal utilities. We conjec-
ture that many other applications fall into this schema,
ranging from news ltering to personal robotics.

1. Introduction

In a wide range of systems in use today, the interac-
tion between human and system takes the following
form. The user issues a command (e.g. query) and re-
ceives a  possibly structured  result in response (e.g.
ranking). The user then interacts with the results (e.g.
clicks), thereby providing implicit feedback about the
users utility function. Here are three examples of such
systems and their typical interaction patterns:

Web-search: In response to a query, a search engine
presents the ranking [A, B, C, D, ...] and observes
that the user clicks on documents B and D.

Appearing in Proceedings of the 29 th International Confer-
ence on Machine Learning, Edinburgh, Scotland, UK, 2012.
Copyright 2012 by the author(s)/owner(s).

Our key contributions in this paper are threefold.
First, we formalize Coactive Learning as a model of
interaction between a learning system and its user,
dene a suitable notion of regret, and validate the
key modeling assumption  namely whether observ-
able user behavior can provide valid feedback in our
model  in a web-search user study. Second, we derive
learning algorithms for the Coactive Learning Model,
including the cases of linear utility models and convex

cost functions, and show O(1/T ) regret bounds in

either case with a matching lower bound. The learn-
ing algorithms perform structured output prediction
(see (Bakir et al., 2007)) and thus can be applied in
a wide variety of problems. Several extensions of the
model and the algorithm are discussed as well. Third,
we provide extensive empirical evaluations of our algo-
rithms on a movie recommendation and a web-search
task, showing that the algorithms are highly ecient
and eective in practical settings.

Coactive Learning

2. Related Work

The Coactive Learning Model bridges the gap between
two forms of feedback that have been well studied
in online learning. On one side there is the multi-
armed bandit model (Auer et al., 2002b;a), where an
algorithm chooses an action and observes the util-
ity of (only) that action. On the other side, utili-
ties of all possible actions are revealed in the case of
learning with expert advice (Cesa-Bianchi & Lugosi,
2006). Online convex optimization (Zinkevich, 2003)
and online convex optimization in the bandit setting
(Flaxman et al., 2005) are continuous relaxations of
the expert and the bandit problems respectively. Our
model, where information about two arms is revealed
at each iteration sits between the expert and the ban-
dit setting. Most closely related to Coactive Learn-
ing is the dueling bandits setting (Yue et al., 2009;
Yue & Joachims, 2009). The key dierence is that
both arms are chosen by the algorithm in the duel-
ing bandits setting, whereas one of the arms is chosen
by the user in the Coactive Learning setting.

While feedback in Coactive Learning takes the form
of a preference, it is dierent from ordinal regression
and ranking. Ordinal regression (Crammer & Singer,
2001) assumes training examples (x, y), where y is a
rank. In the Coactive Learning model, absolute ranks
are never revealed. Closely related is learning with
pairs of examples (Herbrich et al., 2000; Freund et al.,
2003; Chu & Ghahramani, 2005) where absolute ranks
are not needed; however, existing approaches require
an iid assumption and typically perform batch learn-
ing. There is also a large body of work on ranking
(see (Liu, 2009)). These approaches are dierent from
Coactive Learning; they require training data (x, y)
where y is the optimal ranking for query x.

3. Coactive Learning Model

We now introduce coactive learning as a model of in-
teraction (in rounds) between a learning system (e.g.
search engine) and a human (e.g. search user) where
both the human and learning algorithm have the same
goal (of obtaining good results). At each round t, the
learning algorithm observes a context xt  X (e.g. a
search query) and presents a structured object yt  Y
(e.g. a ranked list of URLs). The utility of yt  Y to
the user for context xt  X is described by a utility
function U (xt, yt), which is unknown to the learning
algorithm. As feedback the user returns an improved
object yt  Y (e.g. reordered list of URLs), i.e.,

U (xt, yt) > U (xt, yt),

(1)

when such an object yt exists.
In fact, we will also
allow violations of (1) when we formally model user
feedback in Section 3.1. The process by which the
user generates the feedback yt can be understood as
an approximate utility-maximizing search, but over a
user-dened subset Yt of all possible Y. This mod-
els an approximately and boundedly rational user that
may employ various tools (e.g., query reformulations,
browsing) to perform this search. Importantly, how-
ever, the feedback yt is typically not the optimal label

yt := argmaxyYU (xt, y).

(2)

In this way, Coactive Learning covers settings where
the user cannot manually optimize the argmax over the
full Y (e.g. produce the best possible ranking in web-
search), or has diculty expressing a bandit-style car-
dinal rating for yt in a consistent manner. This puts
our preference feedback yt in stark contrast to super-
vised learning approaches which require (xt, yt ). But
even more importantly, our model implies that reliable
preference feedback (1) can be derived from observable
user behavior (i.e., clicks), as we will demonstrate in
Section 3.2 for web-search. We conjecture that simi-
lar feedback strategies also exist for other applications,
where users can be assumed to act approximately and
boundedly rational according to U .

Despite the weak preference feedback, the aim of a
coactive learning algorithm is to still present objects
with utility close to that of the optimal yt . Whenever,
the algorithm presents an object yt under context xt,
we say that it suers a regret U (xt, yt )  U (xt, yt) at
time step t. Formally, we consider the average regret
suered by an algorithm over T steps as follows:

REGT =

1
T

T

Xt=1

(U (xt, yt )  U (xt, yt)) .

(3)

The goal of the learning algorithm is to minimize
REGT , thereby providing the human with predictions
yt of high utility. Note, however, that a cardinal value
of U is never observed by the learning algorithm, but
