Training Structural SVMs with Kernels Using Sampled Cuts

Chun-Nam John Yu

Department of Computer Science

Cornell University
Ithaca, NY 14853

cnyu@cs.cornell.edu

Thorsten Joachims

Department of Computer Science

Cornell University
Ithaca, NY 14853

tj@cs.cornell.edu

ABSTRACT
Discriminative training for structured outputs has found in-
creasing applications in areas such as natural language pro-
cessing, bioinformatics, information retrieval, and computer
vision. Focusing on large-margin methods, the most gen-
eral (in terms of loss function and model structure) training
algorithms known to date are based on cutting-plane ap-
proaches. While these algorithms are very eﬃcient for lin-
ear models, their training complexity becomes quadratic in
the number of examples when kernels are used. To overcome
this bottleneck, we propose new training algorithms that use
approximate cutting planes and random sampling to enable
eﬃcient training with kernels. We prove that these algo-
rithms have improved time complexity while providing ap-
proximation guarantees. In empirical evaluations, our algo-
rithms produced solutions with training and test error rates
close to those of exact solvers. Even on binary classiﬁca-
tion problems where highly optimized conventional training
methods exist (e.g. SVM-light), our methods are about an
order of magnitude faster than conventional training meth-
ods on large datasets, while remaining competitive in speed
on datasets of medium size.
Categories and Subject Descriptors
I.2.6 [Artiﬁcial Intelligence]: Learning
General Terms
Algorithms, Experimentation, Performance
Keywords
Support Vector Machines, Kernels, Large-Scale Problems
1.

INTRODUCTION

Large-margin methods for structured output prediction
like Maximum-Margin Markov Networks [13] and Structural
SVMs [15] have recently received substantial interest for
challenging problems in natural language processing [14],
bioinformatics [19], and information retrieval [20]. As train-
ing algorithms for these problems, cutting-plane approaches

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
KDD’08, August 24–27, 2008, Las Vegas, Nevada, USA.
Copyright 2008 ACM 978-1-60558-193-4/08/08 ...$5.00.

[15, 6] are among the most generally applicable methods that
provide well-understood performance guarantees. First, cutting-
plane methods can be use to train any type of structured
linear prediction model for which inference and subgradi-
ent can be computed (or at least approximated) eﬃciently.
This makes them applicable to problems ranging from HMM
training and natural language parsing, to supervised clus-
tering and learning ranking functions. Second, they allow
optimizing directly to non-standard loss functions that do
not necessarily have to decompose linearly (e.g. Average
Precision, ROC-Area, F1-score) [4, 20]. And third, their
runtime provably scales linearly with the number of train-
ing examples for linear models. This makes cutting-plane
methods not only attractive for training structured predic-
tion models, but they are also orders of magnitude faster
than conventional methods for training binary classiﬁers [5].

Unfortunately, the computational eﬃciency of cutting-plane

methods becomes substantially worse for non-linear models
that involve kernels. While it is possible to train kernel mod-
els, the computational complexity scales quadratically with
the number of examples, not linearly as in the non-kernel
case. In particular, each iteration of the algorithm requires a
quadratic number of kernel evaluations. This makes it infea-
sible to train large-scale structural models that involve ker-
nels, and it makes cutting-plane methods non-competitive
for training kernelized binary classiﬁers compared to con-
ventional decomposition methods like SVM-light.

In this paper we present new cutting-plane training meth-
ods for structural SVMs that can be used to train kernel-
ized models eﬃciently. These methods are equally broadly
applicable, requiring only the ability to compute subgradi-
ents eﬃciently, but exploit sparse approximations to each
cut in order to limit the number of kernel computations. In
particular, we present two new cutting-plane methods that
exploit random sampling in computing a cut, so that the
number of kernel evaluations depends only linearly on the
number of examples in one algorithm, or is independent of
the number of examples in the other algorithm. Instead, the
number of kernel evaluations depends only on the quality of
the solution that the user desires and that is sensible for
the learning task. In addition to providing theoretical guar-
antees regarding runtime and quality of the solutions, we
also provide empirical results in comparison to conventional
decomposition methods and a subspace method that uses a
Cholesky decomposition.
2. STRUCTURAL SVMS
Structual SVMs are a method for learning rules h : X →
Y from some space X of complex and structured objects

x ∈ X to some space Y of complex and structured objects
y ∈ Y (e.g. sentences X to parse trees Y in natural language
parsing). Given a labeled training sample

S = ((x1, y1), . . . , (xN , yN )),

structural SVMs learn a linear discriminant rule

h(x) = argmaxy∈Y (cid:1)w · Ψ(x, y)
(cid:1)

by minimizing a regularized version of the empirical risk
N
RS(h) =
i=1 ∆(yi, h(xi)) for a given non-negative loss
function ∆. In the case of margin-rescaling [15, 13] we con-
sider in this paper, training a structural SVM amounts to
solving the following quadratic program.

Optimization Problem 1. (Struct SVM Primal)

N(cid:2)

min
(cid:1)w,ξ≥0

s.t.

ξi

+

C
N

(cid:3) (cid:1)w(cid:3)2
1
2
∀ˆy1 ∈ Y : ∆(y1, ˆy1) − (cid:1)w · δΨ1(ˆy1) ≤ ξ1
...∀ˆyN ∈ Y : ∆(yN , ˆyN ) − (cid:1)w · δΨN (ˆyN ) ≤ ξN

i=1

We use the short-hand notation δΨi(ˆy) := Ψ(xi, yi)−Ψ(xi, ˆy).
While this program is convex, it has an exponential or in-
ﬁnite number of constraints (i.e. proportional to |Y |) on
most interesting problems, making naive approaches to its
solution intractable. Fortunately, it can be shown that the
cutting-plane Algorithm 1 can nevertheless solve OP1 to ar-
bitrary precision .

J = J ∪ {(c(t), (cid:1)g(t))}
t = t + 1
( (cid:1)w, ξ) = Solve QP(J)
(c(t), (cid:1)g(t)) = Find Cutting Plane( (cid:1)w)

Algorithm 1 1-Slack Cutting Plane Algorithm
1: Input: S = ((x1, y1), . . . , (xN , yN )), C, 
2: J = {}, t = 0, (cid:1)w = (cid:1)0, ξ = 0
3: (c(t), (cid:1)g(t)) = Find Cutting Plane( (cid:1)w)
4: while c(t) + (cid:1)w · (cid:1)g(t) > ξ +  do
5:
6:
7:
8:
9: end while
10: return ( (cid:1)w, ξ)
11: procedure Find Cutting Plane( (cid:1)w)
12:
13:
14:
15:
16: end procedure
17: procedure Solve QP(J)
argmin (cid:1)w,ξ≥0
s.t. ∀(c, (cid:1)g) ∈ J, c + (cid:1)w · (cid:1)g ≤ ξ

i=1 ∆(yi, ˆyi),− 1

end for
return ( 1
N

(cid:3) (cid:1)w(cid:3)2 + Cξ

for i = 1 to N do

ˆyi = argmax ˆy∈Y (∆(yi, ˆy) + (cid:1)w · Ψ(xi, ˆy))

( (cid:1)w, ξ) =

(cid:1)

(cid:1)

(cid:3)

18:

1
2

N

N

N
i=1 δΨi(ˆyi))

return ( (cid:1)w, ξ)
19:
20: end procedure

This cutting plane algorithm is currently one of the fastest
solution method for large margin structural learning prob-
lems. Its time complexity scales linearly with the number
of examples N [5, 6] when the learned discriminant func-
tion (cid:1)w · Ψ(x, y) is linear. However, with the use of kernels,
it becomes neccessary to work in the dual and Algorithm
1 now scales quadratic in the number of examples. To see
this, let’s look at this dual optimization problem.

Optimization Problem 2. (Cutting-Plane Dual)

max
(cid:1)α∈Rt
s.t.

(cid:1)αT G(cid:1)α + (cid:1)hT (cid:1)α

− 1
2
(cid:1)α ≥ (cid:1)0 and (cid:1)αT(cid:1)1 ≤ C

where Gij = (cid:1)g(i) · (cid:1)g(j) and hi = c(i) for i, j = 1 to t.

The primal and dual solution are related via (cid:1)w =−(cid:1)

One of the major issue with the dual algorithm is the compu-
tation of the inner product (cid:1)g(i) · (cid:1)g(j) in the nonlinear case,

t

i=1 αi(cid:1)g(i).

(i)

k )]·[Ψ(xl, yl) − Ψ(xl, ˆy

(j)
l

)]

(j)

N(cid:2)
[Ψ(xk, yk) − Ψ(xk, ˆy
N(cid:2)

l=1

(cid:1)g

(i)·(cid:1)g
N(cid:2)
1
N(cid:2)
N 2

k=1

1
N 2

=

=

k=1

l=1

[K(xk, yk, xl, yl) − K(xk, yk, xl, ˆy
− K(xk, ˆy

(i)
k , xl, yl) + K(xk, ˆy

(j)
(i)
k , xl, ˆy
l

)]

(j)
l

)

(1)
which involves O(N 2) kernel computations. This makes Al-
gorithm 1 impractical even if N is only moderately large.
Removing this bottleneck is central to our approach.

3. RELATED WORKS

There has been many training methods proposed in the
structural learning literature. The Maximum-Margin Markov
Networks [13] use SMO[8] for training with linearly decom-
posable loss functions, while the more general framework of
structural SVM [15] introduces the cutting plane method
as a training procedure. Subgradient methods [9] have also
been proposed as an eﬃcient training method for structural
learning. Recently a faster 1-slack version of the cutting
plane algorithm [6] has been introduced to solve large margin
structural learning problems. A generalization of the cutting
plane method called the bundle method [12] has also been
recently proposed for the minimization of diﬀerent convex
loss functions in structural learning. Most of these works
consider only linear discriminant functions. Our work con-
tinues this line of research by extending the cutting plane
method to structural learning with kernels.

Our work is also related to the use of stochastic optimiza-
tion in structural learning. The work in [16] investigated
the use of stochastic gradient in the training of Conditional
Random Fields, while the work in [11] employed stochastic
subgradient to train linear SVMs. In stochastic optimization
methods, decreasing step sizes or more accurate estimates of
the gradient is required as the optimization progresses. We
aim to provide methods that automatically terminate when
a solution with guaranteed precision is reached. We take a
somewhat diﬀerent approach by directly modifying the op-
timization method.

Besides structural learning there have also been extensive
work on speeding up kernel methods based on kernel matrix
approximation. The Nystr¨om method has been proposed
in [18] to approximate the kernel matrix used for Gaussian
Process classiﬁcation. Low-rank approximation has been ex-
ploited to speed up the training of kernel SVMs[2]. A greedy
basis-pursuit-style algorithm is also proposed in [7] to build
sparse kernel SVMs to speed up both training and classiﬁ-
cation.

4. THE CUT SUBSAMPLING ALGORITHMS

Our main idea is to speed up the expensive double sum
kernel computations in Equation 1 with approximate cuts
that involve fewer basis functions. Such approximate cuts
could be constructed by various methods such as greedy ap-
proaches, but we take the simpler approach of sampling since

it allows us to prove performance guarantees later. In the
following we will present two diﬀerent sampling strategies
and analyze their complexity.
4.1 A Constant Time Algorithm

Our ﬁrst algorithm has constant time scaling with re-
spect to the training set size. Let us look at the new cut-
ting plane oracle in Algorithm 2, modiﬁed from Algorithm
1. There are no other changes apart from the function
Find Cutting Plane(). The vector (cid:1)S contains r indices
sampled uniformly from 1 to N . Both the oﬀset c(t) and
the subgradient (cid:1)g(t) are constructed from these r examples
instead of the full training set. In general, the approximate
subgradient points in a diﬀerent direction than the exact
subgradient. If we regard the exact constraint as a state-
ment of how we want the classiﬁer to behave on the whole
training set, we can regard the sampled cut as a statement
on a bootstrap sample. Notice that the exit condition of the
while loop on Line 4 of Algorithm 2 is now based on an es-
timate of the loss from a small sample instead of the whole
training set.

J = J ∪ {(c(t), (cid:1)g(t))}
t = t + 1
( (cid:1)w, ξ) = Solve QP(J)
(c(t), (cid:1)g(t)) = Find Cutting Plane( (cid:1)w)

Algorithm 2 Constant Time Cut Subsampling Algorithm
for Structural SVM
1: Input: S = ((x1, y1), . . . , (xN , yN )), C, 
2: J = {}, t = 0, (cid:1)w = (cid:1)0, ξ = 0
3: (c(t), (cid:1)g(t)) = Find Cutting Plane( (cid:1)w)
4: while c(t) + (cid:1)w · (cid:1)g(t) > ξ +  do
5:
6:
7:
8:
9: end while
10: return ( (cid:1)w, ξ)
11: procedure Find Cutting Plane( (cid:1)w)
Sample r examples uniformly for (cid:1)S
12:
for j = 1 to r do
13:
14:
end for
15:
(c, (cid:1)g) = ( 1
16:
r
return (c, (cid:1)g)
17:
18: end procedure

ˆySj = argmax ˆy∈Y (∆(ySj , ˆy) + (cid:1)w · Ψ(xSj , ˆy))

j=1 ∆(ySj , ˆySj ),− 1

r
j=1 δΨSj (ˆySj ))

(cid:1)

(cid:1)

r

r

4.1.1 Complexity per Iteration

Since the optimization problem is solved in the dual, we
focus on complexity analysis of the dual of Algorithm 2. We
defer the analysis on the number of cutting planes required
before convergence to the next section, and analyze the time
and especially the number of kernel computations required
in each iteration. The dual form of the argmax operation of
line 14 in Algorithm 2 is:

(cid:5)

ˆyj = argmax ˆy∈Y

∆(ySj , ˆy) +

(k) · Ψ(xSj , ˆy)

αk(cid:1)g

(2)

Expanding the inner product in Equation 2,
(k)·Ψ(xSj , ˆy))=

, xSj , ˆy)−K(x

[K(x

, y

(cid:1)g

(k)
l

S

(k)
l

S

(k)
, ˆy
l

(k)
l

S

, xSj , ˆy)]

(3)
where S
denotes Sl and ˆyl at the kth iteration.
This involves O(tr) kernel computations at iteration t when

(k)
and ˆy
l

(k)
l

t−1(cid:2)

k=1

(cid:4)

r(cid:2)

l=1

1
r

we sum up from k = 1 to t− 1, provided the argmax compu-
tation over ˆy involves only a small constant number of kernel
computation overall for diﬀerent ˆy. This is true for binary
or multi-class classiﬁcation, and also true for the case when
the kernel function factorizes into components (e.g. MRF
cliques). Since we need to compute this inner product for all
the sampled examples ˆyj for j = 1 to r, the overall complex-
ity of sampling a cut involves O(tr2) kernel computations.
For computing the Gram matrix G, we can update it in-
crementally from one iteration to the next. At iteration t,
it involves expanding G by computing Git for 1 ≤ i ≤ t.
Following from Equation 1 in the case of the exact algo-
rithm, we can infer that the inner product of two sampled
cuts (cid:1)g(i) · (cid:1)g(j) involves O(r2) kernel computations. It takes
O(tr2) kernel computations overall since we need to do this
for 1 ≤ i ≤ t. We can see that the subsequent iterations
are more expensive since the cost scales linearly with t. If it
takes T iterations for the algorithm to terminate, then the
overall complexity would be O(T 2r2) kernel computations.
We omit the time spent on the quadratic program in this
analysis since in practice kernel computations account for
over 95% of training time.

4.2 A Linear Time Algorithm

The previous sampling approach never looks at the whole
training set, making the complexity independent of the train-
ing set size N in each iteration. Our second sampling algo-
rithm trades oﬀ additional work in each iteration for the
ability to sample in a more targeted way. Let us consider
Algorithm 3, especially the changes to the cutting plane or-
acle. Like the exact algorithm, it computes the argmax and
the loss over all examples. However, it only samples r of the
examples with non-zero loss to construct the cutting plane.
This has the eﬀect of focusing on those examples that are
more important to determining the decision surface. Two
(cid:3)
cutting planes (c(t), (cid:1)g(t)) and (c
) are returned, one for in-
clusion in the optimization problem while the other is used
for the stopping criterion.

, (cid:1)g

(cid:3)

In the case of a linear feature space this sampling is not
needed because the cutting plane can be represented com-
pactly by just adding up the N feature vectors returned by
the argmax computation. But in the nonlinear kernel case,
sampling helps because it reduces the number of basis func-
tions used in the kernel expansion from O(N ) to O(r). Since
the argmax computation is performed on all N examples,
the algorithm has more information on the whole training
set compared to the constant time algorithm, such as the
average loss and the primal objective value. In particular
(cid:3)
we can use the exact cutting plane (c
) as the stopping
criterion of the algorithm.

, (cid:1)g

(cid:3)

4.2.1 Complexity per Iteration

Since we are computing the argmax over all N examples,
it is possible to save computation in return for increased
memory usage. Suppose we have a structure Aki to store
all the information required to compute (cid:1)g(k) · Ψ(xi, ˆy) for
1 ≤ k ≤ t, 1 ≤ i ≤ N , and for all ˆy ∈ Y. This is a single
inner product (cid:1)g(k) · φ(xi) for binary classiﬁcation, and m
numbers for multi-class classiﬁcation if there are m classes,
one for each class. For HMM with kernelized emissions,
this involves storing the kernelized emission score at each
position for each possible hidden state. In all of these cases
it amounts to O(N ) storage requirement for each cut.

(cid:3)

, (cid:1)g

)) = Find Cutting Plane( (cid:1)w)

)) = Find Cutting Plane( (cid:1)w)
> ξ +  do

J = J ∪ {(c(t), (cid:1)g(t))}
t = t + 1
( (cid:1)w, ξ) = Solve QP(J)
(cid:3)
((c(t), (cid:1)g(t)), (c

Algorithm 3 Linear Time Cut Subsampling Algorithm for
Structural SVM
1: Input: S = ((x1, y1), . . . , (xN , yN )), C, 
2: J = {}, t = 0, (cid:1)w = (cid:1)0, ξ = 0
(cid:3)
(cid:3)
3: ((c(t), (cid:1)g(t)), (c
, (cid:1)g
+ (cid:1)w · (cid:1)g
(cid:3)
(cid:3)
4: while c
5:
6:
7:
8:
9: end while
10: return ( (cid:1)w, ξ)
11: procedure Find Cutting Plane( (cid:1)w)
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22: end procedure

(cid:1)
end for
I = {1 ≤ i ≤ N | ∆(yi, ˆyi) > 0}
i=1 ∆(yi, ˆyi),− 1
(cid:3)
(c
(cid:1)
repeat
Sample r examples uniformly from I for (cid:1)S
i=1 ∆(yi, ˆyi),− |I|
(c, (cid:1)g) = ( 1
(cid:3) ≤ ξ +  or c + (cid:1)w · (cid:1)g > ξ + 
+ (cid:1)w · (cid:1)g
(cid:3)
N
until c
(cid:3)
(cid:3)
return ((c, (cid:1)g), (c

ˆyi = argmax ˆy∈Y (∆(yi, ˆy) + (cid:1)w · Ψ(xi, ˆy))

for i = 1 to N do

N
i=1 δΨi(ˆyi))

) = ( 1
N

(cid:1)

(cid:1)

, (cid:1)g

, (cid:1)g

))

Nr

N

N

N

(cid:3)

r
j=1 δΨSj (ˆySj ))

5. ANALYSIS OF THE ALGORITHMS

In this section we analyze theoretically the termination
and solution accuracies of the two algorithms. We ﬁrst prove
bounds on the number of iterations for the algorithms to
terminate, and then use the results to prove error bounds on
the solutions. We prove the results for the two algorithms
under a general framework to show that these results could
also apply to the design of other sampling schemes.
5.1 Termination

To prove termination for the above algorithms, we con-
sider the following template of the cutting plane algorithm:

)) = Find Cutting Plane( (cid:1)w(t))
(cid:3)

(cid:3)
, (cid:1)g
+ (cid:1)w(t) · (cid:1)g

Algorithm 4 Generic Cutting Plane Algorithm
1: J = {}, t = 0, (cid:1)w(0) = (cid:1)0, ξ = 0
(cid:3)
2: ((c(t), (cid:1)g(t)), (c
(cid:3)
3: while c
4:
5:
6:
7:
8: end while
9: return ( (cid:1)w(t), ξ)

J = J ∪ {(c(t), (cid:1)g(t))}
t = t + 1
( (cid:1)w(t), ξ) = Solve QP(J)
(cid:3)
((c(t), (cid:1)g(t)), (c

> ξ +  do

, (cid:1)g

(cid:3)

)) = Find Cutting Plane( (cid:1)w(t))

(cid:4)

t−1(cid:2)

(cid:5)

The dual form of the argmax operation in line 16 is:

(k) · Ψ(xi, ˆy)

k=1

αk(cid:1)g

∆(yi, ˆy) +

ˆyi = argmax ˆy∈Y

(4)
With the saved kernel computations in Aki for 1 ≤ k < t, the
argmax computation requires no extra kernel computations
since the term (cid:1)g(k) · Ψ(xi, ˆy) can be retrieved from Aki.
Updating Ati for a new iteration t involves computing
(t)·Ψ(xi, ˆy)=

, xi, ˆy)−K(x

r(cid:2)

[K(x

, y

(t)

(cid:1)g

(t)
l

S

(t)
l

S

(t)
l

S

, ˆy
S

(t)
l

, xi, ˆy)].

1
r

l=1

This requires O(r) kernel computations, assuming that com-
puting and storing the information required for recontruct-
ing the above inner product for each ˆy takes a constant num-
ber of kernel computations and storage. As this has to be
done for all N examples, the overall complexity is O(N r)
kernel compuations for each update in each iteration.

The Gram matrix G can be updated conveniently with

=

(j)

1
r

(i)·Ψ(x

the information stored in Aki, since
(i)·(cid:1)g
)− 1
).
(cid:1)g
r
This involves no new kernel computations since both (cid:1)g(i) ·
Ψ(x
) can be reconstructed

) and (cid:1)g(i) · Ψ(x

(i)·Ψ(x

, ˆy
S

(j)
l

(j)
l

(j)
l

(j)
l

l=1

l=1

, y

, y

(j)

(j)

(cid:1)g

(cid:1)g

S

S

S

r(cid:2)

r(cid:2)

(j)
l

S

, ˆy
S

(j)
l

(j)
l

S

S

(j)
l
.

from A

iS

(j)
l

Therefore if the algorithm terminates in T iterations, the

overall complexity is O(T N r) kernel compuations, with O(T N )
storage required. Although storing each cut requires O(N )
storage, it is still feasible even for large datasets if the num-
ber of active cuts is small(e.g., less than 100). This is the
basic assumption in this space-time tradeoﬀ and is conﬁrmed
by our experiments in section 6.

Notice that what the above algorithm returns as solution de-
pends crucially on the implementation of Find Cutting Plane().
However the speciﬁc detail of the implementation does not
aﬀect the termination property of the above cutting plane
algorithm, and we have the following theorem:

(cid:3)

) returned by the cutting plane oracle Find Cutting Plane():

Theorem 1. Assume the following holds for the cuts (c(t), (cid:1)g(t)),
(cid:3)
, (cid:1)g
(c
(i) 0 ≤ c
, c(t) ≤ ¯∆
(cid:3)
(cid:3)(cid:3),(cid:3)(cid:1)g(t)(cid:3) ≤ R
(ii) (cid:3)(cid:1)g
+ (cid:1)w(t) · (cid:1)g
(cid:3)
(cid:3)
(iii) if c

> ξ + , then c(t) + (cid:1)w(t) · (cid:1)g(t) > ξ + 

Then Algorithm 4 terminates after at most 8C ¯∆R2/2 calls
to the cutting plane oracle Find Cutting Plane().

Proof. Consider the optimization problem solved by Solve QP()

on line 6 of the generic cutting plane algorithm:

Optimization Problem 3.

(cid:3) (cid:1)w(cid:3)2

1
2

min
(cid:1)w,ξ≥0

+ Cξ
s.t. ∀(c, (cid:1)g) ∈ J, c + (cid:1)w · (cid:1)g ≤ ξ

(5)

Consider also the following optimization problem:
Optimization Problem 4.

(cid:3) (cid:1)w(cid:3)2

1
2

(6)

+ Cξ

min
(cid:1)w,ξ≥0
s.t. ∀(c, (cid:1)g) ∈ C, c + (cid:1)w · (cid:1)g ≤ ξ
where C = {(c, (cid:1)g) | c ∈ R, 0 ≤ c ≤ ¯∆, (cid:1)g ∈ H, (cid:3)(cid:1)g(cid:3) ≤ R}
C contains all possible bounded cutting planes where c is
bounded above by ¯∆ and (cid:1)g is bounded above in norm by R.
Since conditions (i) and (ii) hold for the cutting plane ora-
cle, OP3 is always a relaxation of OP4. Therefore the value
of the primal solution of OP3 is always smaller than the
value of the primal solution of OP4, and hence the value of
any feasible solution of OP4 upper bounds the value of any

dual solution of OP3. As (cid:1)w = (cid:1)0, ξ = ¯∆ is a feasible solu-
tion to OP4, the value of the dual solution of OP3 is upper
bounded by C ¯∆. By Proposition 17 of [15], the inclusion
of each -violated constraint increases the dual objective of
OP3 by at least 2/8R2, where R is the upper bound on the
norm of any (cid:1)g. As the dual objective is bounded from above
by C ¯∆, at most 8C ¯∆R2/2 constraints could be added be-
fore the cutting plane algorithm terminates.

Condition (iii) ensures that whenever we are not termi-
nating the while loop, an -violated constraint (c(t), (cid:1)g(t)) will
always be added to the working set.

Corollary 1. Let ¯∆ = maxi,y ∆(yi, y) and

R = maxi,y (cid:3)δΨi(y)(cid:3). Algorithm 2 terminates after at most
8C ¯∆R2/2 calls to Find Cutting Plane().

Proof. First of all notice that Algorithm 2 ﬁts into the
generic template of Algorithm 4. The cut (c(t), (cid:1)g(t)) returned
by Find Cutting Plane() in Algorithm 2 serves both as
the cut to be included into the working cut set J and also
(cid:3)
as the cut for the termination criterion (c
) as in line 3
of Algorithm 4 above. Therefore condition (iii) of Theorem
j=1 ∆(ySj , ˆySj ) ≤ ¯∆
1 holds trivially. Since 0 ≤ c(t) = 1
and (cid:3)(cid:1)g(t)(cid:3) = (cid:3)− 1
j=1 δΨSj (ˆySj )(cid:3) ≤ R, both conditions (i)
and (ii) hold. Invoking Theorem 1, we can conclude that at
most 8C ¯∆R2/2 calls are made to Find Cutting Plane()
before Algorithm 2 terminates.

(cid:1)

(cid:1)

, (cid:1)g

(cid:3)

r

r

r

r

Corollary 2. Let ¯∆ = maxi,y ∆(yi, y) and

R = maxi,y (cid:3)δΨi(y)(cid:3). Algorithm 3 terminates after at most
8C ¯∆R2/2 calls to Find Cutting Plane().

(cid:1)

(cid:3)

N

= 1
N

= 1
N

(cid:1)

(cid:1)

r
j=1 δΨSj (ˆySj ) and (cid:1)g

Proof. The proof is very similar to the previous corol-
lary. Algorithm 3 ﬁts the generic template of Algorithm 4.
i=1 ∆(yi, ˆyi) ≤ ¯∆, so condition
(cid:3)
First of all c(t) = c
(i) of Theorem 1 is satisﬁed. It is also easy to see that (cid:1)g(t) =
|I|
N
i=1 δΨi(ˆyi) are bounded
Nr
in norm by R, so condition (ii) holds as well.
It is also
easy to see that the exit condition of the repeat loop on line
20 of Algorithm 3 makes condition (iii) hold. Therefore we
can invoke Theorem 1 and conclude that at most 8C ¯∆R2/2
calls are made to Find Cutting Plane() before termina-
tion. Moreover, the repeat loop in Find Cutting Plane()
always terminate in ﬁnte expected time. When the ex-
act cutting plane is -violated, we can always sample an
-violated approximate cut with probability bounded away
from 0 (for example, by sampling the worst violating exam-
ple r times).

5.2 Accuracy of Solution

After proving termination and bounding the number of
cutting planes required, we turn our attention to the accu-
racy of the solutions. Speciﬁcally we will characterize the
diﬀerence between the regularized risk of the exact solution
and our approximate solutions. The main idea used in the
proof is:
if the error introduced by each approximate cut
is small with high probability, then the diﬀerence between
the exact and approximate solutions will also be small with
high probability. Bounding the diﬀerence between the ex-
act cut and the sampled cut can be done with Hoeﬀding’s
inequality.

Let us start the proofs by deﬁning some notation. Let
be an exact cutting plane model

c(t) + (cid:1)w · (cid:1)g(t)

(cid:6)

(cid:7)

f ( (cid:1)w)= max
1≤t≤T

(cid:6)

(cid:7)

of the empirical risk, and let ˜f ( (cid:1)w) = max
1≤t≤T

˜c(t) + (cid:1)w · ˜(cid:1)g(t)
be an approximate cutting plane model, with (˜c(t), ˜(cid:1)g(t)) be-
ing the approximate cutting planes. We have the following
lemma:

Lemma 1. Let a ﬁxed (cid:1)v in the RKHS H be given. Suppose
for some γ > 0 each of the cutting plane and its approximate
counterpart satisfy

Pr

(t)

(˜c

+ (cid:1)v · ˜(cid:1)g

(t)

) − (c

(t)

+ (cid:1)v · (cid:1)g

(t)

) ≥ γ

< pγ,

(cid:6)

(cid:7)

for t = 1 . . . T . Then ˜f ((cid:1)v) < f ((cid:1)v) + γ with probability at
least 1 − T pγ.

Proof. By union bound we know that (˜c(t) + (cid:1)v · ˜(cid:1)g(t)) −
(c(t) + (cid:1)v · (cid:1)g(t)) < γ for 1 ≤ t ≤ T occurs with probability at
least 1 − T pγ. The following chain of implications holds:

+ (cid:1)v · ˜(cid:1)g

(t)

) − (c

(t)

+ (cid:1)v · (cid:1)g

(t)

) < γ

(cid:7)

(cid:7)

T(cid:8)

(cid:6)

(t)

(cid:6)

t=1

(˜c
⇒ max
1≤t≤T
⇒ max
1≤t≤T

(t)

(˜c

(t)

+ (cid:1)v · ˜(cid:1)g
+ (cid:1)v · ˜(cid:1)g

(t)

(t)

(t)

) − (c
) − max
1≤t≤T

+ (cid:1)v · (cid:1)g

(t)

)
+ (cid:1)v · (cid:1)g

(t)

(t)

< γ

(˜c

) < γ
Hence ˜f ((cid:1)v) < f ((cid:1)v) + γ with probability at least 1− T pγ.

(c

The lemma shows that the approximate cutting plane model
does not overestimate the loss by more than a certain amount
with high probability. Notice that T is a ﬁxed number above.
If T is a bounded random variable such as the termination
iteration, then we can replace T by its upper bound ¯T and
the lemma still holds. From the termination bound in sec-
tion 5.1 we have ¯T = 8C ¯∆R2/2.

N

(cid:1)

Now we are going to use this lemma to analyze the lin-
(cid:1)
In the linear time algo-
ear time algorithm Algorithm 3.
rithm we denote the exact cutting plane (c(t), (cid:1)g(t)) with
(cid:1)
i=1 ∆(yi, ˆyi), − 1
( 1
N
i=1 δΨi(ˆyi)), and the approximate
i=1 ∆(yi, ˆyi), − 1
N
N
cut (˜c(t), ˜(cid:1)g(t)) with ( 1
N
r
j=1 δΨSj (ˆySj )).
N
We can bound the diﬀerence between the exact cutting planes
and the approximate cutting planes using Hoeﬀding’s in-
equality in the following lemma:

(cid:1)

r

√

(cid:7)

(cid:6)

Lemma 2. Let a ﬁxed (cid:1)v ∈ H, (cid:3)(cid:1)v(cid:3) ≤

2C ¯∆ be given, and
let the exact cutting planes (c(t), (cid:1)g(t)) and approximate cut-
ting planes (˜c(t), ˜(cid:1)g(t)) be deﬁned as above. We have for each
t = 1 . . . T ,

(t)

+(cid:1)v · ˜(cid:1)g

(t)

)−(c

(t)

+(cid:1)v · (cid:1)g

(t)

)≥ γ

(˜c

< exp

4C ¯∆R2η2

Pr
where η = |I|/N , I being the index set at the t-th iteration.
Proof. Deﬁne Zj = −(cid:1)v·δΨSj (ˆySj ). Since Sj are sampled
uniformly from the index set I, Zj ’s are independent with
E(Zj ) = − 1|I|
i∈I (cid:1)v · δΨi(ˆyi). Each Zj is also bounded
√
between [−
2C ¯∆R]. Apply Hoeﬀding’s inequality
and after some arithmetic we obtain the result.

2C ¯∆R,

(cid:1)

√

(cid:9) −rγ2

(cid:10)

Now we are ready to prove our main theorem relating the
regularized risk of the optimal solution to our approximate
solution. Let (cid:1)v∗ be the optimal solution to OP1. We have
the following theorem:

∗

Theorem 2. Suppose Algorithm 3 terminates in T iter-
as solution. Then with probability at
(cid:3)(cid:1)v
∗(cid:3)2

ations and return (cid:1)w
least 1 − δ,
(cid:3) (cid:1)w
)≤ 1
1
2
2

4C ¯∆R2

+CL( (cid:1)w

+CL((cid:1)v

(cid:12)

∗(cid:3)2

(cid:11)

¯T
δ

)+C

log

+

∗

∗

r

(cid:13)

where ¯T = 8C ¯∆R2/2, and L( (cid:1)w) is the margin loss 1
N
as in OP1.

N
i=1 ξi

Proof. With the exact cutting planes (c(t), (cid:1)g(t)) and ap-
proximate cutting planes (˜c(t), ˜(cid:1)g(t)) as deﬁned in Lemma 2,
, and pγ = exp(−rγ2/4C ¯∆R2)
we apply Lemma 1. Put (cid:1)v = (cid:1)v
(we omit η since it is bounded above by 1), we obtain ˜f ((cid:1)v
) <
) + γ with probability at least 1− ¯T exp(−rγ2/4C ¯∆R2).
f ((cid:1)v
Inverting the statement and we have with probability at
least 1 − δ:

∗

∗

∗

(cid:12)

(cid:1)

∗

˜f ((cid:1)v

) < f ((cid:1)v

∗

) +

4C ¯∆R2

r

¯T
log
δ
(cid:3) (cid:1)w(cid:3)2 + C ˜f ( (cid:1)w) at

1
2

∗

Since (cid:1)w
is the optimal solution of min (cid:1)w
the T th iteration, we have the following:
∗

∗

∗(cid:3)2

+C ˜f ( (cid:1)w

+C ˜f ((cid:1)v

)

1
2

(cid:3) (cid:1)w
(cid:3)(cid:1)v
(cid:3)(cid:1)v

<

1
2
≤ 1
2

(cid:12)
) ≤ 1
2
(cid:12)

)+C

∗(cid:3)2

(cid:3)(cid:1)v
4C ¯∆R2

r

4C ¯∆R2

r

∗(cid:3)2

∗

+Cf ((cid:1)v

∗(cid:3)2

+CL((cid:1)v

∗

)+C

log

log

¯T
δ
¯T
δ

(with prob. 1 − δ)

(subgrad. property)

(7)
The last line makes use of the subgradient property that
f ( (cid:1)w) ≤ L( (cid:1)w) for any exact cutting plane model f of a convex
loss function L. Since we are using the exact cutting plane
as the condition for exiting the while loop, so we must have
at termination:

N(cid:2)

i=1

1
N

∆(yi, ˆyi) − 1
N(cid:2)
N

[∆(yi, ˆyi) − (cid:1)w

i=1

max

( ˆy1,..., ˆyN )∈YN

1
N

i=1

(cid:3)
c

∗ · N(cid:2)

∗ · (cid:1)g

(cid:3) ≤ ξ + 
+ (cid:1)w
δΨi(ˆyi) ≤ ˜f ( (cid:1)w
∗

(cid:1)w

) + 

) + 

) + 

∗

∗

L( (cid:1)w

∗ · δΨi(ˆyi)] ≤ ˜f ( (cid:1)w
) ≤ ˜f ( (cid:1)w
(cid:13)

+ C( ˜f ( (cid:1)w

(cid:12)
∗(cid:3)2

) + )

∗

∗

Therefore we have:

∗(cid:3)2

∗

+ CL( (cid:1)w

(cid:11)
(cid:3) (cid:1)w
) ≤ 1
2

(cid:3) (cid:1)w
(cid:3)(cid:1)v

1
2
≤ 1
2

∗(cid:3)2

+ CL((cid:1)v

) + C
with probability at least 1 − δ.

∗

 +

4C ¯∆R2

r

log

¯T
δ

The theorem shows that as far as obtaining a ﬁnite precision
solution to the regularized risk minimization problem is con-
cerned, it is suﬃcient to use sampled cuts with suﬃciently
large sample size r to match the desired accuracy  of the
solution. We will see in the experiment section that fairly
small values of r work well in practice.

We state a similar result for Algorithm 2. The proof is
fairly similar with a few technical diﬀerences. We assign its
proof to the appendix.

Theorem 3. Suppose Algorithm 2 terminates in T iter-
returned as solution. Then with probability

ations with (cid:1)w

∗

at least 1 − 2δ,
(cid:3) (cid:1)w

+CL( (cid:1)w

∗(cid:3)2

∗

)≤1
2

(cid:16)
⎛
⎝+2

√
( ¯∆+2

2C ¯∆R)2
2r

⎞
⎠

¯T
δ

log

(cid:3)(cid:1)v

∗(cid:3)2

+CL((cid:1)v

∗

)+C

1
2

,

where ¯T = 8C ¯∆R2/2.

6. EXPERIMENTS

While theory gives us the worst case bounds that are re-
assuring, we now study the empirical behaviour of the algo-
rithms.
6.1 Experiment Setup

We implemented Algorithm 2 and Algorithm 3 and eval-
uated them on the task of binary classiﬁcation with kernels.
We choose this task for evaluation because binary classi-
ﬁcation with kernels is a well-studied problem, and there
are stable SVM solvers that are suitable for comparisons.
Moreover, scaling up SVM with kernels to large datasets is
an interesting research problem on its own [1].

In binary classiﬁcation the loss function ∆ is just the zero-
one loss. The feature map Ψ is deﬁned by Ψ(x, y) = yφ(x),
where y ∈ {1, −1} and φ is the nonlinear feature map in-
duced from a Mercer kernel (such as the commonly used
polynomial kernels and Gaussian kernels).

We implemented the algorithms in C, using Mosek as the
quadratic program solver and the SFMT implementation
[10] of Mersenne Twister as the random number generator.
The experiments were run on machines with Opteron 2.0Ghz
CPUs with 2Gb of memory (with the exception of the control
experiments with incomplete Cholesky factorization, which
we ran on machines with 4Gb of memory).

For all the experiments below we ﬁx the precision parame-
ter  at 0.001. We remove cuts that are inactive for 20 itera-
tions. We found that the constant time algorithm has better
performance if we use a more stringent stopping criterion.
We terminate the algorithm only when for p consecutive it-
erations, the sampled cut is not violated by more than . In
the experiments below we use p = 4. For each combination
of parameters we ran the experiment for 3 runs using diﬀer-
ent random seeds, and report the average result in the plots
and tables below. In section 6.4 we also investigate the sta-
bility of the algorithms by reporting the standard deviation
of the results.

In the experiments below we test our algorithms on three
diﬀerent datasets: Checkers, Adult, and Covertype. Check-
ers is a synthetic dataset with 1 million training points, with
classes alternating on a 4x4 checkerboard. We generated the
data using the SimpleSVM toolbox [17], with noise level pa-
rameter sigma set to 0.02. The kernel width for the Gaussian
kernel used for the Checkers dataset was determined by cross
validation on a small subsample of 10000 examples. Adult
is a medium-sized dataset with 32562 examples, with a sam-
ple of 22697 examples taken as training set. The Gaussian
kernel width is taken from [8]. Covertype is a dataset with
522910 training points, the kernel width of the Gaussian
kernel we use below is obtained from the study [1].
6.2 Scaling with Training Set Size

Our ﬁrst set of experiments is about how the two algo-
rithms scale with training set size. We perform the exper-
iments on the two large datasets Checkers and Covertype.
We pick C to be 1 multiplied by the training set size, since

 100000

 10000

 1000

 100

 10

 1

 0.1

i

e
m
T
U
P
C

 

r
o
r
r

E

 
t

 

e
S
g
n
n
a
r
T

i

i

Linear
Const
svmlight

 1000  10000  100000  1e+06

 1000  10000  100000  1e+06

Training Set Size

Training Set Size

Figure 1: CPU Time Against Training Set Size

Checkers

Covertype

 0.03

 0.025

 0.02

 0.015

 0.01

 0.005

 0
 1000

 10000

Linear
Const
svmlight

 100000

 0.2

 0.18

 0.16

 0.14

 0.12

 0.1

 0.08

 0.06

r
o
r
r

E

 
t

 

e
S
g
n
n
a
r
T

i

i

 1e+06

 0.04

 1000

 10000

Linear
Const
svmlight

 100000

 1e+06

Training Set Size

Training Set Size

Figure 2: Training Set Error Against Training Set
Size

that is the largest value of C we could get SVMlightto train
within 5 days. For the linear time algorithm we ﬁx the sam-
ple size r at 400, and for the constant time algorithm we
use a sample size r of 1000 to compensate for the less eﬃ-
cient sampling. We train SVM models on subsets of the full
training sets of various sizes to evaluate scaling.

Figure 1 shows the CPU time required to train SVMs on
training sets of diﬀerent sizes on the Checkers and Cover-
type dataset. We can observe that the linear time algorithm
scales roughly linearly in the log-log plot, while the constant
time algorithm has a roughly ﬂat curve in both plots. This
conﬁrms the scaling behaviour we expect from the complex-
ity of each iteration. SVMlight shows superlinear scaling on
both of these datasets.

Figures 2 and 3 show the training and test set errors of

Checkers

Covertype

 0.075

 0.07

 0.065

 0.06

 0.055

 0.05

 0.045

 0.04

 0.035

 0.03

r
o
r
r

E

 
t

e
S

 
t
s
e
T

Linear
Const
svmlight

 0.28

 0.27

 0.26

 0.25

 0.24

 0.23

 0.22

 0.21

 0.2

 0.19

r
o
r
r

E

 
t

e
S

 
t
s
e
T

Linear
Const
svmlight

 0.025

 1000

 10000

 100000

 1e+06

Training Set Size

 0.18

 1000

 10000

 100000

 1e+06

Training Set Size

Figure 3: Test Set Error Against Training Set Size

Checkers

Constant Time Alg. on Adult

 1e+06

 100000

 10000

 1000

 100

i

e
m
T
U
P
C

 

 10

Covertype

Linear
Const
svmlight

i

e
m
T
U
P
C

 

 1e+06

 100000

 10000

 1000

 100

 10

 1

 10

C=226
C=2269
C=22697
C=226970
C=2269700

Linear Time Alg. on Adult

C=226
C=2269
C=22697
C=226970
C=2269700

 1e+06

 100000

 10000

 1000

 100

i

e
m
T
U
P
C

 

 1000  10000

 100
Sample Size

 10

 10

 100  1000  10000
Sample Size

Figure 4: CPU Time Against Sample Size

Constant Time Alg. on Adult

Linear Time Alg. on Adult

C=226
C=2269
C=22697
C=226970
C=2269700

 3000

 2500

 2000

 1500

 1000

 500

s
n
o
i
t
a
r
e

t
I
 
f

o

 
r
e
b
m
u
N

C=226
C=2269
C=22697
C=226970
C=2269700

 6000

 5000

 4000

 3000

 2000

 1000

s
n
o

i
t

a
r
e

t
I
 
f

o

 
r
e
b
m
u
N

 0
 100

 1000

 10000

Sample Size

 0
 100

 1000

 10000

Sample Size

Figure 5: Number of Iteration Against Sample Size

the algorithms. In general SVMlighthas the lowest training
and test set errors, followed by the linear time algorithm
and then the constant time algorithm. Both the training
and test set errors lie within a very narrow band, and they
are never more than 0.5 percentage point apart even in the
worst case.
6.3 Effect of Different Sample Sizes

The next set of experiments is about the eﬀect of the
sample size r on training time and solution quality. We
investigate the eﬀect of sample size using the Adult dataset,
since on this dataset it is easier to collect more data points
for diﬀerent sample sizes. We use sample sizes r from {100,
400, 1000, 4000, 10000} and C from {0.01, 0.1, 1, 10, 100}
multiplied by the training set size 22697. The constant time
algorithm did not ﬁnish the training within 5 days for the
largest sample size 10000 and C ∈ {10, 100}, hence there are
two missing data points in the ﬁgures.

In Figure 5 shows that the number of iterations required
generally decreases with increasing sample size. However
the decrease in the number of iterations to convergence does
not result in overall savings in time due to the extra cost in-
volved in each iteration with larger sample sizes. This can be
observed from the CPU Time plots in Figure 4. In general,
the linear time algorithm has better scaling behaviour with
respect to sample size compared to the constant time algo-
rithm. This is predicted by our complexity analysis. What
is most interesting is the stability of training and test set
errors with respect to changes to sample size, as shown in
Figures 6 and 7. Except for very small sample sizes like 100
or small values of C like 0.01 the sets of curves are essentially
ﬂat.

Constant Time Alg. on Adult

Linear Time Alg. on Adult

 0.2

 0.18

 0.16

 0.14

 0.12

 0.1

 0.08

C=226
C=2269
C=22697
C=226970
C=2269700

r
o
r
r

E

 
t

 

e
S
g
n
n
a
r
T

i

i

C=226
C=2269
C=22697
C=226970
C=2269700

 0.2

 0.18

 0.16

 0.14

 0.12

 0.1

 0.08

r
o
r
r

E

 
t

 

e
S
g
n
n
a
r
T

i

i

 0.06

 100

 1000

 10000

Sample Size

 0.06

 100

 1000

 10000

Sample Size

Figure 6: Training Set Error Against Sample Size

Constant Time Alg. on Adult

Linear Time Alg. on Adult

C=226
C=2269
C=22697
C=226970
C=2269700

C=226
C=2269
C=22697
C=226970
C=2269700

 0.19

 0.18

 0.17

 0.16

 0.15

 0.14

 0.13

 0.12

 0.11

r
o
r
r

E

 
t

e
S

 
t
s
e
T

 0.1

 100

 1000

 10000

Sample Size

r
o
r
r

E

 
t

e
S

 
t
s
e
T

 0.19
 0.18
 0.17
 0.16
 0.15
 0.14
 0.13
 0.12
 0.11
 0.1

 100

 1000

 10000

Sample Size

Figure 7: Test Set Error Against Sample Size

6.4 Quality of Solutions

Table 1 shows a comparison of the two algorithms against
two conventional training methods, namely SVMlight and
a sampling-based method that uses Cholesky decomposi-
tion as described below. For each dataset we train diﬀer-
ent models using values of C ∈ {0.01, 0.1, 1, 10, 100}, mul-
tipled by the size of the training set. We used the results
of SVMlightas a yardstick to compare against, and report
the value of C for which the test performance is optimal for
SVMlight. For the larger datasets Checkers and Covertype,
SVMlightterminated early due to slow progress for C ≥ 10,
so for those two datasets we use C = 1.

First of all, we notice from Table 1 that all the solutions
have training and test set error rates very close to the so-
lutions produced by SVMlight. For the constant time algo-
rithm the error rates are usually within 0.3 to 0.5 above the
SVMlightsolutions, while the linear time algorithm has error
rates usually within 0.2 above the SVMlightsolutions. The
error rates also have very small standard deviation, on the
order of 0.1, which is the same as our tolerance parameter
. We also notice when using the same sample size r, the
linear time algorithm provides more accurate solutions than
the constant time algorithm due to its use of more focused
sampling.

We also provide control experiments with Cholesky de-
composition method, where we subsample a set of points
from the training set, and then compute the projection of all
the points in the training set onto the subspace spanned by
these examples. Then we train a linear SVM using SVMperf [6]
(with options -t 2 -w 3 -b 0) on the whole training set.
Our implementation involves storing all the projected train-
ing vectors, and this consumes a lot of memory, especially
for large datasets like Checkers and Covertype. We can only
do 250 and 500 basis functions on those datasets respectively

without running out of memory on a 4Gb machine, and on
the Adult dataset we can only do up to 10000 basis func-
tions. An alternative implementation with smaller storage
requirement would involve recomputing the projected train-
ing vector when needed, but this would become prohibitively
expensive.

We observe that the Cholesky decomposition is generally
faster than all the other methods, but its accuracy is usu-
ally substantially below that of SVMlight and our sampling
algorithms. Moreover, unlike our algorithms, the accuracy
of the Cholesky method depends crucially on the number of
basis functions, which is diﬃcult to pick in advance. The
accuracies of our sampling algorithms are more stable with
respect to the choice of sample size, where decreasing the
sample size ususally results in more iterations to converge
without much loss in accuracy of the solutions.

7. CONCLUSIONS

We presented two methods that make cutting-plane train-
ing of structural SVMs with kernels tractable through the
use of random sampling in constructing a cut. The meth-
ods maintain the full generality of the cutting-plane ap-
proach, making it possible to kernelize any structural predic-
tion problem where linear models are currently used. The
theoretical analysis shows that these algorithms have lin-
ear or constant-time termination guarantees while providing
bounds on the solution quality. Empirically, the algorithms
can handle datasets with hundred-thousands of examples,
and they are competitive or faster than conventional de-
composition methods even on binary classiﬁcation problems,
where highly optimized special-purpose algorithms exist.

The current algorithms can be improved along several di-
rections. The two sampling methods presented here are cho-
sen for their simplicity and ease of analysis. Sampling eﬃ-
ciency can be improved by designing alternative sampling
schemes, for example, by having diﬀerent sampling rates
for bound support vectors and non-bound support vectors
following the popular shrinking heuristic used in training
SVMs. On the other hand, one major bottleneck in the
speed of the current algorithm is the large number of cuts
required before convergence. Recently [3] proposes a sta-
bilized cutting plane algorithm for linear SVMs with much
improved convergence, and it will be interesting to extend
their techniques to improve the speed of our sampled-cut
algorithm for kernels.

8. ACKNOWLEDGMENTS

We would like to thank the reviewers for their careful read-
ing and helpful comments for improving this paper. This
work was supported in part by NSF Award IIS-0713483 and
by a gift from Yahoo!.

9. REFERENCES
[1] R. Collobert, S. Bengio, and Y. Bengio. A parallel mixture

of SVMs for very large scale problems. In NIPS, 2002.

[2] S. Fine and K. Scheinberg. Eﬃcient SVM training using
low-rank kernel representations. JMLR, 2:243–264, 2001.

[3] V. Franc and S. Sonnenburg. Optimized cutting plane

algorithm for support vector machines. In ICML, 2008.
[4] T. Joachims. A support vector method for multivariate

performance measures. In ICML, 2005.

[5] T. Joachims. Training linear SVMs in linear time. In KDD,

2006.

Checkers

Adult

Covertype

(N=1000000, C=1, σ2=0.05)

(N=22697, C=100, σ2=20)

(N=522910, C=1, σ2=1.7)

CPU sec
1411(40)
1854(175)

Err(Train) Err(Test)
3.12(0.01) 3.23(0.06)
2.96(0.01) 3.14(0.13)
2.90(0.01) 2.99(0.01) 60101(1042)

Err(Train) Err(Test)
Err(Train) Err(Test)
Algorithm
Const (r = 400)
7.90(0.05) 10.45(0.08) 17487(1175) 18.49(0.11) 18.61(0.13)
Const (r = 1000)
7.65(0.01) 10.29(0.07) 38617(604)
Linear (r = 400)
7.43(0.09) 10.19(0.15) 11136(640)
Linear (r = 1000) 2.89(0.00) 2.95(0.05) 151759(3334) 7.44(0.01) 10.19(0.15) 15191(612)
Cholesky(250)
Cholesky(500)
Cholesky(5000)
Cholesky(10000)
SVMlight

604
537
3386
8836
11630

2447
N/A
N/A
N/A
36533

15.09
14.50
12.43
11.41
10.37

21.73
20.29
N/A
N/A
17.87

21.85
20.35
N/A
N/A
18.09

15.10
14.60
10.89
9.12
7.52

3.32
N/A
N/A
N/A
2.94

3.11
N/A
N/A
N/A
2.87

CPU sec

18.34(0.09) 18.55(0.02) 12946(1400)
18.16(0.03) 18.28(0.02)
62184(485)
17.96(0.03) 18.22(0.02) 155196(6649)

1937
3093
N/A
N/A

273021

CPU sec
3496(408)

Table 1: Runtime and training/test error of sampling algorithms compared to SVMlight and Cholesky.

planes (˜c(t), ˜(cid:2)g(t)) be deﬁned as above. We have for 1 ≤ t ≤ T ,
(cid:13)

(cid:11)

(cid:6)

(t)

+(cid:2)v · ˜(cid:2)g

(t)

)−(c

(t)

+(cid:2)v · (cid:2)g

(t)

Pr

(˜c

<exp

(cid:7)
)≥γ

−2rγ2
√

( ¯∆ + 2

2C ¯∆R)2

Proof. Deﬁne Zj = ∆(ySj , ˆySj ) − (cid:2)v · δΨSj (ˆySj ). Since Sj
is sampled uniformly from 1 to N , Zj’s are independent with
N (∆(yi, ˆyi) − (cid:2)v · δΨi(ˆyi)). Each Zj is bounded in the
E(Zj) = 1
√
interval [−
2C ¯∆R]. Apply Hoeﬀding’s inequality
2C ¯∆R, ¯∆ +
and we obtain the result.

√

Using Lemma 1 and Lemma 3, we can adopt a similar proof as

in Theorem 2 up to Equation 7 and obtain

(cid:16)
⎛
⎝+

√

( ¯∆+2

2C ¯∆R)2
2r

log

⎞
⎠

¯T
δ

(cid:2) (cid:2)w

1
2

∗(cid:2)2

+C ˜f ( (cid:2)w

∗

)≤1
2

(cid:2)(cid:2)v

∗(cid:2)2

+CL((cid:2)v

∗

)+C

(8)
Unlike Algorithm 3, we terminate using an approximated objec-
) ≤
tive instead of the true objective, so we do not have L( (cid:2)w
∗
˜f ( (cid:2)w
) + . However we can show that with high probability
) ≤ ˜f ( (cid:2)w
∗
(cid:13)
> 0. Analogous to Lemma 3,
L( (cid:2)w
we can prove using Hoeﬀding’s inequality the following bound:

for some γ

) +  + γ

(cid:11)

∗

∗

(cid:3)

(cid:3)

(cid:6)

Pr

(c

(t)

+(cid:2)w

(t) · (cid:2)g

(t)

)−(˜c

(t)

+(cid:2)w

(t) · ˜(cid:2)g

(t)

<exp

(cid:7)
)≥γ

−2rγ2
√

( ¯∆+2

2C ¯∆R)2

where (cid:2)w(t) is the weight vector at the beginning of iteration t.
Notice (cid:2)w(t) is ﬁxed in the sense that it is chosen before sampling
for (˜c(t), ˜(cid:2)g(t)) occurs. Moreover, the value c(t) + (cid:2)w(t) · (cid:2)g(t) equals
L( (cid:2)w(t)) at successive iterates (cid:2)w(t). Therefore by union bound we
have L( (cid:2)w(t)) − (˜c(t) + (cid:2)w(t) · ˜(cid:2)g(t)) < γ
(cid:3)
√
for t = 1 . . . T with proba-
bility at least 1 − ¯T exp(−2rγ
(cid:3)2/( ¯∆ + 2
2C ¯∆R)2). In particular
at termination we have:
∗ · ˜(cid:2)g(T )
ξ +  ≥ ˜c(T ) + (cid:2)w
) +  ≥ L( (cid:2)w
) − γ
(cid:3)
∗
with probability at least 1 − ¯T exp(−2rγ
2C ¯∆R)2).
(cid:16)
Inverting the statement we have with probability at least 1 − δ
(cid:3)
,
√

√
(cid:3)2/( ¯∆ + 2

˜f ( (cid:2)w

∗

∗

) +  ≤ L( (cid:2)w

∗

˜f ( (cid:2)w

) +

( ¯∆ + 2

2C ¯∆R)2
2r

log

¯T
δ(cid:3)

= δ, and combining with Equation 8, we obtain the

(cid:3)

By putting δ
theorem.

[6] T. Joachims, T. Finley, and C.-N. Yu. Cutting plane

training of structural SVMs. MLJ, To Appear.

[7] S. S. Keerthi, O. Chapelle, and D. DeCoste. Building

support vector machines with reduced classiﬁer complexity.
JMLR, 7:1493–1515, 2006.

[8] J. Platt. Fast training of support vector machines using

sequential minimal optimization. In B. Sch¨olkopf,
C. Burges, and A. Smola, editors, Advances in Kernel
Methods - Support Vector Learning, chapter 12. MIT-Press,
1999.

[9] N. D. Ratliﬀ, J. A. Bagnell, and M. A. Zinkevich. (Online)

subgradient methods for structured prediction. In
AISTATS, 2007.

[10] M. Saito and M. Matsumoto. SIMD-oriented fast mersenne

twister: a 128-bit pseudorandom number generator. In
Monte Carlo and Quasi-Monte Carlo Methods 2006, 2006.

[11] S. Shalev-Shwartz, Y. Singer, and N. Srebro. Pegasos:

Primal Estimated sub-GrAdient SOlver for SVM. In ICML,
2007.

[12] A. J. Smola, S. V. N. Vishwanathan, and Q. Le. Bundle

methods for machine learning. In NIPS, 2007.

[13] B. Taskar, C. Guestrin, and D. Koller. Maximum-Margin

Markov networks. In NIPS, 2003.

[14] B. Taskar, D. Klein, M. Collins, D. Koller, and

C. Manning. Max-margin Parsing. In EMNLP, 2004.

[15] I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun.

Large margin methods for structured and interdependent
output variables. JMLR, 6:1453 – 1484, September 2005.

[16] S. V. N. Vishwanathan, N. N. Schraudolph, M. W.
Schmidt, and K. P. Murphy. Accelerated training of
conditional random ﬁelds with stochastic gradient methods.
In ICML, 2006.

[17] S. V. N. Vishwanathan, A. J. Smola, and M. N. Murty.

SimpleSVM. In ICML, 2003.

[18] C. K. I. Williams and M. Seeger. Using the Nystr¨om
method to speed up kernel machines. In NIPS, 2001.

[19] C.-N. Yu, T. Joachims, R. Elber, and J. Pillardy. Support
vector training of protein alignment models. In RECOMB,
2007.

[20] Y. Yue, T. Finley, F. Radlinski, and T. Joachims. A

support vector method for optimizing average precision. In
SIGIR, 2007.

(cid:1)

APPENDIX
Proof of Theorem 3. Deﬁne the exact cutting planes (c(t), (cid:2)g(t))
to be ( 1
N
i=1 δΨi(ˆyi)), and the approxi-
N
mate cutting planes (˜c(t), ˜(cid:2)g(t)) to be 1
r
j=1 ∆(ySj , ˆySj ) and
− 1
r
r
j=1 δΨSj (ˆySj ). We can bound the diﬀerence between the
approximate and exact cutting planes with Hoeﬀding’s inequality:

i=1 ∆(yi, ˆyi), − 1

(cid:1)

(cid:1)

(cid:1)

N

N

r

Lemma 3. Let a ﬁxed (cid:2)v ∈ H,(cid:2)(cid:2)v(cid:2) ≤

2C ¯∆ be given, and
let the exact cutting planes (c(t), (cid:2)g(t)) and approximate cutting

√

