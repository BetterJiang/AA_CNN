Online Inference of Topics with Latent Dirichlet Allocation

Lei Shi

Helen Wills Neuroscience Institute

University of California

Berkeley, CA 94720
lshi@berkeley.edu

Thomas L. Griﬃths

Department of Psychology

University of California

Berkeley, CA 94720

tom griffiths@berkeley.edu

Kevin R. Canini

Computer Science Division

University of California

Berkeley, CA 94720

kevin@cs.berkeley.edu

Abstract

Inference algorithms for topic models are typ-
ically designed to be run over an entire col-
lection of documents after they have been
observed. However, in many applications of
these models, the collection grows over time,
making it infeasible to run batch algorithms
repeatedly. This problem can be addressed
by using online algorithms, which update es-
timates of the topics as each document is
observed. We introduce two related Rao-
Blackwellized online inference algorithms for
the latent Dirichlet allocation (LDA) model –
incremental Gibbs samplers and particle ﬁl-
ters – and compare their runtime and perfor-
mance to that of existing algorithms.

1 INTRODUCTION

Probabilistic topic models are often used to analyze
collections of documents, each of which is represented
as a mixture of topics, where each topic is a proba-
bility distribution over words. Applying these mod-
els to a document collection involves estimating the
topic distributions and the weight each topic receives
in each document. A number of algorithms exist for
solving this problem (e.g., Hofmann, 1999; Blei et al.,
2003; Minka and Laﬀerty, 2002; Griﬃths and Steyvers,
2004), most of which are intended to be run in “batch”
mode, being applied to all the documents once they are
collected. However, many applications of topic mod-
els are in contexts where the collection of documents
is growing. For example, when inferring the topics
of news articles or communications logs, documents

Appearing in Proceedings of the 12th International Confe-
rence on Artiﬁcial Intelligence and Statistics (AISTATS)
2009, Clearwater Beach, Florida, USA. Volume 5 of JMLR:
W&CP 5. Copyright 2009 by the authors.

arrive in a continuous stream, and decisions must be
made on a regular basis, without waiting for future
documents to arrive. In these settings, repeatedly run-
ning a batch algorithm can be infeasible or wasteful.
In this paper, we explore the possibility of using online
inference algorithms for topic models, whereby the rep-
resentation of the topics in a collection of documents
is incrementally updated as each document is added.
In addition to providing a solution to the problem of
growing document collections, online algorithms also
open up diﬀerent routes for parallelization of infer-
ence from batch algorithms, providing ways to draw
on the enhanced computing power of multiprocessor
systems, and diﬀerent tradeoﬀs in runtime and perfor-
mance from other algorithms.
We discuss algorithms for a particular topic model: la-
tent Dirichlet allocation (LDA) (Blei et al., 2003). The
state space from which these algorithms draw samples
is deﬁned at time i to be all possible topic assignments
to each of the words in the documents observed up to
time i. The result is a Rao-Blackwellized sampling
scheme (Doucet et al., 2000), analytically integrating
out the distributions over words associated with topics
and the per-document weights of those topics.
The plan of the paper is as follows. Section 2 intro-
duces the LDA model in more detail. Section 3 dis-
cusses one batch and three online algorithms for sam-
pling from LDA. Section 4 discusses the eﬃcient im-
plementation of one of the online algorithms – particle
ﬁlters. Section 5 describes a comparative evaluation
of the algorithms, and Section 6 concludes the paper.

2 INFERRING TOPICS

Latent Dirichlet allocation (Blei et al., 2003) is widely
used for identifying the topics in a set of documents,
building on previous work by Hofmann (1999). In this
model, each document is represented as a mixture of
a ﬁxed number of topics, with topic z receiving weight

         65Online Inference of Topics with Latent Dirichlet Allocation

θ(d)
in document d, and each topic is a probability dis-
z
tribution over a ﬁnite vocabulary of words, with word
w having probability φ(z)
w in topic z. The generative
model assumes that documents are produced by inde-
pendently sampling a topic z for each word from θ(d)
and then independently sampling the word from φ(z).
The independence assumptions mean that the docu-
ment is treated as a bag of words, so word ordering
is irrelevant to the model. Symmetric Dirichlet priors
are placed on θ(d) and φ(z), with θ(d) ∼ Dirichlet(α)
and φ(z) ∼ Dirichlet(β), where α and β are hyper-
parameters that aﬀect the relative sparsity of these
distributions. The complete probability model is thus

wi|zi, φ(zi) ∼ Discrete(φ(zi)),
∼ Dirichlet(β),
φ(z)
∼ Discrete(θ(di)),
zi|θ(di)
∼ Dirichlet(α),
θ(d)

i = 1, . . . , N,
z = 1, . . . , T,
i = 1, . . . , N,
d = 1, . . . , D,

where N is the total number of words in the collection,
T is the number of topics, D is the number of docu-
ments, and di and zi are, respectively, the document
and topic of the ith word, wi. The goal of inference in
this model is to identify the values of φ and θ, given a
document collection represented by the sequence of N
words wN = (w1, . . . , wN ). Estimation is complicated
by the latent variables zN = (z1, . . . , zN ), the topic
assignments of the words. Various algorithms have
been proposed for solving this problem, including a
variational Expectation-Maximization algorithm (Blei
et al., 2003) and Expectation-Propagation (Minka and
Laﬀerty, 2002). In the collapsed Gibbs sampling algo-
rithm of Griﬃths and Steyvers (2004), φ and θ are ana-
lytically integrated out of the model to collect samples
from P (zN|wN ). The use of conjugate Dirichlet priors
on φ and θ makes this analytic integration straightfor-
ward, and also makes it easy to recover the posterior
distribution on φ and θ given zN and wN , meaning
that a set of samples from P (zN|wN ) is suﬃcient to
estimate φ and θ.
Existing inference algorithms provide users with sev-
eral options in trading oﬀ bias and runtime. However,
most of these algorithms are designed to be run over an
entire document collection, requiring multiple sweeps
to produce good estimates of φ and θ. While some ap-
plications of these models involve the analysis of static
databases, more typically, users work with document
collections that grow over time. In the remainder of
the paper, we outline three related algorithms that can
be used for inference in such a setting.

3 ALGORITHMS

In this section, we describe a batch sampling algorithm
for LDA. We then discuss ways in which this algorithm
can be extended to yield three online algorithms.

Algorithm 1 batch Gibbs sampler for LDA
1: initialize zN randomly from {1, . . . , T}N
2: loop
3:
4:

choose j from {1, . . . , N}
sample zj from P (zj|zN\j, wN )

3.1 BATCH GIBBS SAMPLER

Griﬃths and Steyvers (2004) presented a collapsed
Gibbs sampler for LDA, where the state space is the set
of all possible topic assignments to the words in every
document. The Gibbs sampler is “collapsed” because
the variables θ and φ are analytically integrated out,
and only the latent topic variables zN are sampled.
The topic assignment of word j is sampled according
to its conditional distribution
P (zj|zN\j, wN ) ∝ n(wj )

(1)

zj ,N\j + β
n(·)
zj ,N\j + W β

n(dj )
zj ,N\j + α
n(dj )
·,N\j + T α

,

where zN\j indicates (z1, . . . , zj−1, zj+1, . . . , zN ), W is
the size of the vocabulary, n(wj )
zj ,N\j is the number of
times word wj is assigned to topic zj, n(·)
zj ,N\j is the
total number of words assigned to topic zj, n(dj )
zj ,N\j is
the number of times a word in document dj is assigned
to topic zj, and n(dj )
·,N\j is the total number of words in
document dj, and all the counts are taken over words
1 through N, excluding the word at position j itself
(hence the N\j subscripts).
The Gibbs sampling procedure, outlined in Algo-
rithm 1, converges to the desired posterior distribu-
tion P (zN|wN ). This batch Gibbs sampler can be
extended in several ways, leading to eﬃcient online
sampling algorithms for LDA.

3.2 O-LDA

A simple modiﬁcation of the batch Gibbs sampler
yields an online algorithm presented by Song et al.
(2005) and called “o-LDA” by Banerjee and Basu
(2007). This procedure, outlined in Algorithm 2, ﬁrst
applies the batch Gibbs sampler to a preﬁx of the full
dataset, then samples the topic of each new word i by
conditioning on the words observed so far1:

P (zi|zi−1, wi) ∝ n(wi)

zi,i\i + β
n(·)
zi,i\i + W β

n(di)
zi,i\i + α
n(di)
·,i\i + T α

.

(2)

1The o-LDA algorithm as presented by Banerjee and
Basu (2007) samples the next topic by conditioning only
on the topics of the words up to the end of the previous
document, rather than all previous words. The algorithm
presented here is slightly slower, but more accurate.

         66Canini, Shi, Griﬃths

0 = P −1 for p = 1, . . . , P

Algorithm 4 particle ﬁlter for LDA
1: initialize weights ω(p)
2: for i = 1, . . . , N do
3:
4:

for p = 1, . . . , P do

i = ω(p)

set ω(p)
sample z(p)

i

i−1P (wi|z(p)
from P (z(p)

i−1, wi−1)
|z(p)
i−1, wi)

i

5:
6:
7:
8:
9:
10:
11:

12:

normalize weights ωi to sum to 1
if (cid:107)ωi(cid:107)−2 ≤ ESS threshold then

resample particles
for j in R(i) do

for p = 1, . . . , P do

|z(p)
sample z(p)
i\j, wi)
i = P −1 for p = 1, . . . , P

from P (z(p)

j

j

set ω(p)

Algorithm 2 o-LDA (initialized with ﬁrst σ words)
1: sample zσ using batch Gibbs sampler
2: for i = σ + 1, . . . , N do
3:

sample zi from P (zi|zi−1, wi)

Algorithm 3 incremental Gibbs sampler for LDA
1: for i = 1, . . . , N do
sample zi from P (zi|zi−1, wi)
2:
for j in R(i) do
3:
4:

sample zj from P (zj|zi\j, wi)

After its batch initialization phase, o-LDA applies
Equation (2) incrementally for each new word wi,
never resampling old topic variables. For this reason,
its performance depends critically on the accuracy of
the topics inferred during the batch phase. If the doc-
uments used to initialize o-LDA are not representative
of the full dataset, it could be led to make poor infer-
ences. Also, because each topic variable is sampled by
conditioning only on previous words and topics, sam-
ples drawn with o-LDA are not distributed according
to the true posterior distribution P (zN|wN ). To rem-
edy these issues, we consider online algorithms that re-
vise their decisions about previous topic assignments.

3.3

INCREMENTAL GIBBS SAMPLER

Extending o-LDA to occasionally resample topic vari-
ables, we introduce the incremental Gibbs sampler, an
algorithm that rejuvenates old topic assignments in
light of new data. The incremental Gibbs sampler,
outlined in Algorithm 3, does not have a batch initial-
ization phase like o-LDA, but it does use Equation (2)
to sample topic variables of new words. After each step
i, the incremental Gibbs sampler resamples the topics
of some of the previous words. The topic assignment
zj of each index j in the “rejuvenation sequence” R(i)
is drawn from its conditional distribution

P (zj|zi\j, wi) ∝ n(wj )

zj ,i\j + β
n(·)
zj ,i\j + W β

n(dj )
zj ,i\j + α
n(dj )
·,i\j + T α

.

(3)

If these rejuvenation steps are performed often enough
(depending on the mixing time of the induced Markov
chain), the incremental Gibbs sampler closely approxi-
mates the posterior distribution P (zi|wi) at every step
i. Indeed, convergence is guaranteed as the number of
times each zj is resampled goes to inﬁnity, since the al-
gorithm becomes a batch Gibbs sampler for P (zi|wi)
in the limit. More generally, the incremental Gibbs
sampler is an instance of the decayed MCMC frame-
work introduced by Marthi et al. (2002). The choice
of the number of rejuvenation steps to perform deter-
mines the runtime of the incremental Gibbs sampler.

If |R(i)| is bounded as a function of i, then the overall
runtime is linear. However, if |R(i)| grows logarith-
mically or linearly with i, then the overall runtime is
log-linear or quadratic, respectively. R(i) can also be
chosen to be nonempty only at certain intervals, lead-
ing to an incremental Gibbs sampler that only rejuve-
nates itself periodically (for example, whenever there
is time to spare between observing documents).
An alternative approach to frequently resampling pre-
vious topic assignments is to concurrently maintain
multiple samples of zi, rejuvenating them less fre-
quently. This option is desirable because it allows the
algorithm to simultaneously explore several regions of
the state space. It is also useful in a multi-processor
environment, since it is simpler to parallelize multiple
samples – dedicating each sample to a single machine
– than it is to parallelize operations on one sample. An
ensemble of independent samples from the incremen-
tal Gibbs sampler could be used to approximate the
posterior distribution P (zN|wN ); however, if the sam-
ples are not rejuvenated often enough, they will not
have the desired distribution. With this motivation,
we turn to particle ﬁlters, which perform importance
weighting on a set of sequentially-generated samples.

3.4 PARTICLE FILTER

Particle ﬁlters are a sequential Monte Carlo method
commonly used for approximating a probability dis-
tribution over a latent variable as observations are ac-
quired (Doucet et al., 2001). We can extend the incre-
mental Gibbs sampler to obtain a Rao-Blackwellized
particle ﬁlter (Doucet et al., 2000), again analytically
integrating out φ and θ to sample from P (zi|wi). This
use of particle ﬁlters is slightly nonstandard, since the
state space grows with each observation.
The particle ﬁlter for LDA, outlined in Algorithm 4,
updates samples from P (zi−1|wi−1) to generate sam-

         67Online Inference of Topics with Latent Dirichlet Allocation

i

i

ples from the target distribution P (zi|wi) after each
It does this by ﬁrst generat-
word wi is observed.
ing a value of z(p)
for each particle p from a proposal
|z(p)
distribution Q(z(p)
i−1, wi). The prior distribution,
|z(p)
P (z(p)
i−1, wi−1), is typically used for the proposal
because it is often infeasible to sample from the pos-
|z(p)
terior, P (z(p)
i−1, wi). However, since zi is drawn
from a constant, ﬁnite set of values, we can use the
posterior, which is given by Equation (2) and min-
imizes the variance of the resulting particle weights
(Doucet et al., 2000). Next, the unnormalized impor-
tance weights of the particles are calculated using the
standard iterative equation

i

i

ω(p)
i
ω(p)
i−1

∝ P (wi|z(p)
= P (wi|z(p)

, wi−1)P (z(p)
i
|z(p)
Q(z(p)
i−1, wi)
i−1, wi−1).

i

i

|z(p)
i−1)

(4)

(5)

The weights are then normalized to sum to 1. Equa-
tion (5) is designed so that after the weight normaliza-
tion step, the particle ﬁlter approximates the posterior
distribution over topic assignments as follows:

P (zi|wi) ≈ P(cid:88)

ω(p)
i 1zi(z(p)

i

),

(6)

p=1

where 1zi(·) is the indicator function for zi. As P →
∞, the right side converges to the left side, since zi
can assume only a ﬁnite number of values.
Over time, the weights assigned to particles diverge
signiﬁcantly, as a few particles come to provide a sig-
niﬁcantly better account of the observed data than the
others. Resampling addresses this issue by producing a
new set of particles that are more highly concentrated
on states with high weight whenever the variance of the
weights becomes large. A standard measure of weight
variance is an approximation to the eﬀective sample
size, ESS ≈ (cid:107)ω(cid:107)−2, and a threshold can be expressed
as some proportion of the number of particles, P .
The simplest form of resampling is to draw from
the multinomial distribution deﬁned by the normal-
ized weights. However, more sophisticated resampling
methods also exist, such as stratiﬁed sampling (Kita-
gawa, 1996), quasi-deterministic methods (Fearnhead,
2004), and residual resampling (Liu and Chen, 1998),
which produce more diverse sets of particles. Residual
resampling was used in our evaluations. Whenever the
particles are resampled, their weights are all reset to
P −1, since each is now a draw from the same distri-
bution and the previous weights are reﬂected in their
relative resampling frequencies.
As in the resample-move algorithm of Gilks and
Berzuini (2001), Markov chain Monte Carlo (MCMC)

is used after particle resampling to restore diversity
to the particle set in the same way that the incremen-
tal Gibbs sampler rejuvenates its samples, by choosing
a rejuvenation sequence R(i) of topic variables to re-
sample. The length of R(i) can be chosen to trade oﬀ
runtime against performance, and the variables to be
resampled can be randomly selected either uniformly
or using a decayed distribution that favors more recent
history, as in Marthi et al. (2002). While a uniform
schedule visits earlier sites more overall, using a distri-
bution that approaches zero quickly enough for sites
in the past ensures that in expectation, each site is
sampled the same number of times.

4 EFFICIENT IMPLEMENTATION

In order to be feasible as an online algorithm, the par-
ticle ﬁlter must be implemented with an eﬃcient data
representation.
In particular, the amount of time it
takes to incrementally process a document must not
grow with the amount of data previously seen. In ini-
tial implementations of the algorithm, individual par-
ticles were represented as linear arrays of topic assign-
ment values, consistent with the interpretation of the
state space zi = (z1, . . . , zi) as a sequence of variables
stored in an array.
It was found that nearly all of
the computing time was spent resampling the parti-
cles (line 8 of Algorithm 4). This is due to the fact
that when a particle is resampled more than once,
the na¨ıve implementation makes copies of the zi ar-
ray for each child particle. Since these structures grow
linearly with the observed data and resampling is per-
formed at a roughly constant rate, the total time spent
resampling particles grows quadratically.
This problem can be alleviated by using a shared rep-
resentation of the particles, exploiting the high degree
of redundancy among particles with common lineages.
When a particle is resampled multiple times, the re-
sulting copies all share the same parent particle and
implicitly inherit its zi vector as their history of topic
assignments. Each particle maintains a hash table that
is used to store the diﬀerences between its topic as-
signments and its parent’s; consequently, the compu-
tational complexity of the resampling step is reduced
from quadratic to linear. As illustrated in Figure 1,
the particles are thus stored as a directed tree2, with
parent-child relationships indicating the hierarchy of
inheritance for topic variables.

To look up the value z(p)
of topic assignment i in par-
ticle p, the particle’s hash table is consulted ﬁrst. If
the value is missing, the particle’s parent’s hash table
is checked, recursing up the tree towards the root and

i

2More precisely, a forest of directed trees, since it is

possible that not all particles share a common ancestor.

         68Canini, Shi, Griﬃths

idence conﬁrms that the time overhead of maintaining
a directed tree of hash tables is negligible compared to
the increase in speed and decrease in memory usage it
aﬀords. Speciﬁcally, by reducing the resampling step
to have linear runtime, this implementation detail is
the key to making the particle ﬁlter feasible to run.

5 EVALUATION

In online topic modeling settings, such as news article
clustering, we care about two aspects of performance:
the quality of the solutions recovered and runtime. As
documents arrive and are incrementally processed, we
would like online algorithms to maintain high-quality
inferences and to produce topic labels quickly for new
documents. Since there is a tradeoﬀ between runtime
and inference quality, the algorithms were evaluated
by comparing the quality of their inferences while con-
straining the amount of time spent per document. We
compared the performance of the three online algo-
rithms presented in Section 3: o-LDA, the incremental
Gibbs sampler, and the particle ﬁlter. Our evaluation
is a variation of the comparison of o-LDA to other on-
line algorithms by Banerjee and Basu (2007), using the
same datasets and performance metric.

5.1 DATASETS

The datasets used to test the algorithms are each a
collection of categorized documents. They consist of
four subsets derived from the 20 Newsgroups corpus3:
(1) diff-3 (2995 documents, 7670 word types, 3 cate-
gories), (2) rel-3 (2996 documents, 10091 word types,
3 categories), (3) sim-3 (2980 documents, 5950 word
types, 3 categories), and (4) subset-20 (1997 docu-
ments, 13341 word types, 20 categories), represent-
ing diﬀerent levels of size and diﬃculty, as well as
news articles harvested from the Slashdot website: (5)
slash-7 (6714 documents, 5769 word types, 7 cate-
gories) and (6) slash-6 (5182 documents, 4498 word
types, 6 categories).

5.2 METHODOLOGY

Our testing methodology is designed to approximate a
real-world online inference task. For each dataset, the
algorithms were given the ﬁrst 10% of the documents
to use for initialization. A single sample drawn using
the batch Gibbs sampler on this initial set was used to
initialize all of the online algorithms. This constituted
the explicit batch initialization phase of o-LDA, and
the other two online algorithms used the same starting
conﬁguration.

3Available online at http://people.csail.mit.edu/

jrennie/20Newsgroups/

Figure 1: An example of the “directed tree of hashta-
bles” implementation of the particle ﬁlter. Particle 2
is the root, so all other particles descended from it.
Particle 0 directly depends on particle 2, altering the
topics of the words “choosing” and “where”. The other
child of particle 2 is not itself an active sample, but an
inactive remnant of an old particle that was not resam-
pled. It is retained because multiple active particles
depend on its hashed topic values. If either particle 1
or particle 3 is not resampled in the future, the remain-
ing one will be merged with its parent, maintaining the
bound on the tree depth.

i

eventually terminating when the value is found in an
ancestor’s hash table. To change the value z(p)
, the
new value is inserted into particle p’s hash table, and
the old value is inserted into each of particle p’s chil-
dren’s hash tables (if they don’t already have an entry)
to ensure consistency.
In order to ensure that variable lookup is a constant-
time operation, it is necessary that the depth of the
tree does not grow with the amount of data. This can
be ensured by selectively pruning the tree just after
the resampling step. After resampling is performed,
a node is called active if it has been sampled one or
more times, and inactive otherwise. If an entire sub-
tree of nodes is inactive, it is deleted. If an inactive
node has only one active descendant depending on its
history, that descendant’s hash table is merged with
its own. When this operation is performed after each
resampling step, it can be shown that the depth of the
tree is never greater than P . Furthermore, merging the
hash tables takes linear amortized time. Empirical ev-

         69!"#$%&’(#)’*!+,-.()-/0-1!2$-//-3’(.1!)4-55-!"6’16$7-08-+9’’7!":-0;-&9$($-,<-)’-,=-#(.&-/>-)9$-,?-1!"$-/!"#$%&’()*!"#$%&’(#)’*!+8-+9’’7!":-5;-&9$($-0!"#$%&’()+!"#$%&’(#)’*!+,-.()-08-+9’’7!":-5=-#(.&-0,-.)!"#$%&’(/!"#$%&’(#)’*!+,-.()-/!"#$%&’()0!"#$%&’(#)’*!+;-&9$($-/?-1!"$-5!"#$%&’()1Online Inference of Topics with Latent Dirichlet Allocation

After the batch initialization set is chosen, the o-LDA
algorithm has no remaining parameters and is the
fastest of the three online algorithms, since it does
not rejuvenate its topic assignments. The incremental
Gibbs sampler has one parameter: the choice of re-
juvenation sequence R(i). The particle ﬁlter has two
parameters: the eﬀective sample size (ESS) threshold,
which controls how often the particles are resampled,
and the choice of rejuvenation sequence. Since the run-
time and performance of the incremental Gibbs sam-
pler and the particle ﬁlter depend on these parameters,
there is a compromise to be made. We set the param-
eters so that these algorithms ran within roughly 6
times the amount of time taken by o-LDA on each
dataset. For the incremental Gibbs sampler, R(i) was
chosen to be a set of 4 indices from 1 to i chosen uni-
formly at random. For the particle ﬁlter, the ESS
threshold was set at 20 for the diff-3, rel-3, and
sim-3 datasets and at 10 for the subset-20, slash-6,
and slash-7 datasets, |R(i)| was set at 30 for the
diff-3, rel-3, and sim-3 datasets and at 10 for the
subset-20, slash-6, and slash-7 datasets, and the
particular values of R(i) were chosen uniformly at ran-
dom from 1 to i. The runtime of these algorithms can
be chosen to ﬁt any constraints, but we selected one
point that we felt was reasonable. Indeed, an impor-
tant strength of these two online algorithms is that
they can take full advantage of any amount of comput-
ing power by appropriate choice of their parameters.
The LDA hyperparameters α and β were both set to
be 0.1. The particle ﬁlter was run with 100 particles;
to allow the other algorithms the same advantage of
multiple samples, they were each run 100 times inde-
pendently, with the sample having highest posterior
probability at each step being used for evaluation.
Since the datasets are collections of documents with
known category memberships, we evaluated how well
the clustering implied by the inferred topics matched
the true categories. That is, for each dataset, the num-
ber of topics T was set equal to the number of cate-
gories, and the documents were clustered according to
their most frequent topic. Normalized mutual infor-
mation (nMI) was used to measure the similarity of
this implied partition to the true document categories
(Banerjee and Basu, 2007). Scores are between 0 and
1, with a perfect match receiving a score of 1.
Two diﬀerent evaluations were made for each algo-
rithm on each dataset. First, we evaluated how well
the algorithms clustered the documents on which they
were trained. That is, at regular intervals through-
out each dataset, the sample with maximum posterior
probability was drawn, and the quality of the induced
clustering of the documents observed so far was mea-
sured. Second, we evaluated how well the algorithms

clustered a randomly-chosen held-out set consisting of
10% of the documents, as a function of the amount of
the training set that had been observed so far. That is,
at regular intervals throughout the training set, each
algorithm was run on the held-out documents as if they
were the next ones to be observed, the nMI score was
calculated for the held-out documents, and the algo-
rithm was returned to its original state and position
in the training set.

5.3 RESULTS

The results of the training set evaluation are shown
in Figure 2. The particle ﬁlter and incremental Gibbs
sampler perform about equally well, with the parti-
cle ﬁlter performing better for some datasets. As ex-
pected, o-LDA consistently has the lowest score of
the three algorithms. The dashed horizontal line in
each ﬁgure represents the performance of the batch
Gibbs sampler on the entire dataset, which is approx-
imately the best possible performance an online algo-
rithm could achieve using the LDA model.
The results of the evaluation on the held-out set are
shown in Figure 3. For each held-out set, the mean
performance of the particle ﬁlter is consistently better
than that of the incremental Gibbs sampler, which is
consistently better than that of o-LDA. With the ex-
ception of the sim-3 and subset-20 datasets, the al-
gorithms’ performances are separated by at least two
standard deviations. Interestingly, the performance of
all the algorithms on all the held-out document sets
does not change signiﬁcantly as more training data is
observed. This seems to indicate that a majority of
the information about the topics comes from the ﬁrst
10% of the documents. In rel-3, performance on the
held-out set seems to decrease as more of the training
set is observed. This could be because the held-out
documents are more closely related to those at the be-
ginning of the training set than those at the end.
As mentioned earlier, the algorithms’ performance
strongly depends on their parameters; allowing more
time for rejuvenation of old topic assignments would
improve the performance of the particle ﬁlter and the
incremental Gibbs sampler. Table 1 summarizes the
total runtimes of each algorithm on each dataset. Al-
though there is some variation, the incremental Gibbs
sampler and particle ﬁlter each take about 6 times
longer than o-LDA.
The top ten words from 5 of the 20 topics found by the
particle ﬁltering algorithm on the subset-20 dataset
are listed in Table 2. Although the normalized mutual
information is not as high as that of the batch Gibbs
sampler, the recovered topics seem intelligible.
We also noticed that the particle ﬁlter used signiﬁ-

         70Canini, Shi, Griﬃths

Figure 2: nMI traces for each algorithm on each dataset. The algorithms were initialized with the same conﬁg-
uration on the ﬁrst 10% of the documents. Each dashed horizontal line represents the nMI score for the batch
Gibbs sampler on an entire dataset. Solid lines show mean performance over 30 runs, and shading indicates plus
and minus one sample standard deviation.

Figure 3: nMI traces for each algorithm on held-out test sets, as a function of the amount of the training set
observed. The algorithms were initialized with the same conﬁguration on the ﬁrst 10% of the documents. Each
dashed horizontal line represents the nMI score on the held-out set for the batch Gibbs sampler given the entire
training set. Solid lines show mean performance over 30 runs, and shading indicates plus and minus one sample
standard deviation.

         71Online Inference of Topics with Latent Dirichlet Allocation

Table 1: Runtimes of algorithms in seconds. Numbers
in parentheses give multiples of o-LDA runtime.

diff-3
rel-3
sim-3

subset-20

slash-6
slash-7

o-LDA inc. Gibbs
185 (5.4)
338 (5.9)
176 (5.5)
1221 (8.1)
255 (5.8)
420 (6.1)

34
57
32
150
44
69

particle ﬁlter

183 (5.4)
251 (4.4)
148 (4.6)
1029 (6.9)
256 (5.8)
521(7.6)

Acknowledgements

This work was supported by the DARPA CALO project
and NSF grant BCS-0631518. The authors thank Jason
Wolfe for helpful discussions and Sugato Basu for providing
the datasets and nMI code used for evaluation.

References

Banerjee, A. and Basu, S. 2007. Topic models over text
streams: a study of batch and online unsupervised learn-
ing. In Proc. 7th SIAM Int’l. Conf. on Data Mining.

Blei, D. M., Griﬃths, T. L., Jordan, M. I., and Tenen-
baum, J. B. 2004. Hierarchical topic models and the
nested Chinese restaurant process. In Advances in Neu-
ral Information Processing Systems 16.

Blei, D. M. and Laﬀerty, J. D. 2005. Correlated topic mod-
els. In Advances in Neural Information Processing Sys-
tems 18.

Blei, D. M., Ng, A. Y., and Jordan, M. I. 2003. Latent
Dirichlet allocation. Journal of Machine Learning Re-
search, 3:993–1022.

Doucet, A., de Freitas, N., and Gordon, N., eds. 2001.
Sequential Monte Carlo Methods in Practice. Springer.
Doucet, A., de Freitas, N., Murphy, K. P., and Russell, S. J.
2000. Rao-Blackwellised particle ﬁltering for dynamic
Bayesian networks. In Proc. 16th Conf. in Uncertainty
in Artiﬁcial Intelligence.

Fearnhead, P. 2004. Particle ﬁlters for mixture models
with an unknown number of components. Statistics and
Computing, 14(1):11–21.

Gilks, W. R. and Berzuini, C. 2001. Following a mov-
ing target–Monte Carlo inference for dynamic Bayesian
models. Journal of the Royal Statistical Society. Series
B (Statistical Methodology), 63(1):127–146.

Griﬃths, T. L. and Steyvers, M. 2004. Finding scientiﬁc
topics. Proc. National Academy of Sciences of the USA,
101(Suppl. 1):5228–5235.

Griﬃths, T. L., Steyvers, M., Blei, D. M., and Tenenbaum,
J. B. 2004. Integrating topics and syntax. In Advances
in Neural Information Processing Systems 17.

Hofmann, T. 1999. Probabilistic latent semantic indexing.
In Proc. 22nd Annual Int’l. ACM SIGIR Conf. on Re-
search and Development in Information Retrieval.

Kitagawa, G. 1996. Monte Carlo ﬁlter and smoother for
non-Gaussian nonlinear state space models. Journal of
Computational and Graphical Statistics, 5(1):1–25.

Liu, J. S. and Chen, R. 1998. Sequential Monte Carlo
methods for dynamic systems. Journal of the American
Statistical Association, 93(443):1032–1044.

Marthi, B., Pasula, H., Russell, S. J., and Peres, Y. 2002.
Decayed MCMC ﬁltering. In Proc. 18th Conf. in Uncer-
tainty in Artiﬁcial Intelligence.

Minka, T. P. and Laﬀerty, J. D. 2002. Expectation-
propagation for the generative aspect model. In Proc.
18th Conf. in Uncertainty in Artiﬁcial Intelligence.

Rosen-Zvi, M., Griﬃths, T. L., Steyvers, M., and Smyth,
P. 2004. The author-topic model for authors and docu-
ments. In Proc. 20th Conf. in Uncertainty in Artiﬁcial
Intelligence.

Song, X., Lin, C.-Y., Tseng, B. L., and Sun, M.-T. 2005.
Modeling and predicting personal information dissem-
ination behavior.
In Proc. 11th ACM SIGKDD Int’l.
Conf. on Knowledge Discovery and Data Mining.

Teh, Y. W., Jordan, M. I., Beal, M. J., and Blei, D. M.
2006. Hierarchical Dirichlet processes. Journal of the
American Statistical Association, 101(476):1566–1581.

Table 2: The top 10 words from 5 topics found by the
particle ﬁlter for the subset-20 dataset.

gun
police
guns
semi
ﬁre
cops

revolver
carry
auto
koresh

list
mail
users
email
internet
access
unix

address
security
system

space
cost
nasa
mass
rockets
station
orbit
launch

dod
drink

god
bible
moral
choose
christian

jesus
church

christianity

absolute

life

wire

ground
wiring
cable
power
neutral

nec

circuit
box

current

cantly less memory than the other online algorithms.
This is due to the shared representation of the par-
ticles, as discussed in Section 4, which allows more
accurate inferences to be computed with less memory
than algorithms that maintain independent samples.

6 DISCUSSION

We have discussed a number of algorithms for the
problem of inferring topics using LDA. We have shown
how to extend a batch algorithm into a series of
online algorithms, each more ﬂexible than the last.
Our results demonstrate that these algorithms per-
form eﬀectively in recovering the topics used in multi-
ple datasets. Latent Dirichlet allocation has been ex-
tended in a number of directions, including incorpora-
tion of hierarchical representations (Blei et al., 2004),
minimal syntax (Griﬃths et al., 2004), the interests of
authors (Rosen-Zvi et al., 2004), and correlations be-
tween topics (Blei and Laﬀerty, 2005). We anticipate
that the algorithms we have outlined here will natu-
rally generalize to many of these models. In particu-
lar, applications of the hierarchical Dirichlet process to
text (Teh et al., 2006) can be viewed as an analogue to
LDA in which the number of topics is allowed to vary.
Our particle ﬁltering framework requires little modiﬁ-
cation to handle this case, providing an online alter-
native to existing inference algorithms for this model.

         72