TopicsinSemanticRepresentationThomasL.GriffithsUniversityofCalifornia,BerkeleyMarkSteyversUniversityofCalifornia,IrvineJoshuaB.TenenbaumMassachusettsInstituteofTechnologyProcessinglanguagerequirestheretrievalofconceptsfrommemoryinresponsetoanongoingstreamofinformation.Thisretrievalisfacilitatedifonecaninferthegistofasentence,conversation,ordocumentandusethatgisttopredictrelatedconceptsanddisambiguatewords.Thisarticleanalyzestheabstractcomputationalproblemunderlyingtheextractionanduseofgist,formulatingthisproblemasarationalstatisticalinference.Thisleadstoanovelapproachtosemanticrepresentationinwhichwordmeaningsarerepresentedintermsofasetofprobabilistictopics.Thetopicmodelperformswellinpredictingwordassociationandtheeffectsofsemanticassociationandambiguityonavarietyoflanguage-processingandmemorytasks.Italsoprovidesafoundationfordevelopingmorerichlystructuredstatisticalmodelsoflanguage,asthegenerativeprocessassumedinthetopicmodelcaneasilybeextendedtoincorporateotherkindsofsemanticandsyntacticstructure.Keywords:probabilisticmodels,Bayesianmodels,semanticmemory,semanticrepresentation,compu-tationalmodelsManyaspectsofperceptionandcognitioncanbeunderstoodbyconsideringthecomputationalproblemthatisaddressedbyaparticularhumancapacity(Anderson,1990;Marr,1982).Percep-tualcapacitiessuchasidentifyingshapefromshading(Freeman,1994),motionperception(Weiss,Simoncelli,&Adelson,2002),andsensorimotorintegration(Ko¨rding&Wolpert,2004;Wolpert,Ghahramani,&Jordan,1995)appeartocloselyapproximateop-timalstatisticalinferences.Cognitivecapacitiessuchasmemoryandcategorizationcanbeseenassystemsforefficientlymakingpredictionsaboutthepropertiesofanorganism’senvironment(e.g.,Anderson,1990).Solvingproblemsofinferenceandpredic-tionrequiressensitivitytothestatisticsoftheenvironment.Sur-prisinglysubtleaspectsofhumanvisioncanbeexplainedintermsofthestatisticsofnaturalscenes(Geisler,Perry,Super,&Gallo-gly,2001;Simoncelli&Olshausen,2001),andhumanmemoryseemstobetunedtotheprobabilitieswithwhichparticulareventsoccurintheworld(Anderson&Schooler,1991).Sensitivitytorelevantworldstatisticsalsoseemstoguideimportantclassesofcognitivejudgments,suchasinductiveinferencesabouttheprop-ertiesofcategories(Kemp,Perfors,&Tenenbaum,2004),predic-tionsaboutthedurationsormagnitudesofevents(Griffiths&Tenenbaum,2006),andinferencesabouthiddencommoncausesfrompatternsofcoincidence(Griffiths&Tenenbaum,inpress).Inthisarticle,weexaminehowthestatisticsofoneveryimportantaspectoftheenvironment—naturallanguage—influencehumanmemory.Ourapproachismotivatedbyananal-ysisofsomeofthecomputationalproblemsaddressedbysemanticmemory,inthespiritofMarr(1982)andAnderson(1990).Undermanyaccountsoflanguageprocessing,understandingsentencesrequiresretrievingavarietyofconceptsfrommemoryinresponsetoanongoingstreamofinformation.Onewaytodothisistousethesemanticcontext—thegistofasentence,conversation,ordocument—topredictrelatedconceptsanddisambiguatewords(Ericsson&Kintsch,1995;Kintsch,1988;Potter,1993).Theretrievalofrelevantinformationcanbefacilitatedbypredictingwhichconceptsarelikelytoberelevantbeforetheyareneeded.Forexample,ifthewordbankappearsinasentence,itmightbecomemorelikelythatwordslikefederalandreservewillalsoappearinthatsentence,andthisinformationcouldbeusedtoinitiateretrievaloftheinformationrelatedtothesewords.Thispredictiontaskiscomplicatedbythefactthatwordshavemultiplesensesormeanings:Bankshouldinfluencetheprobabilitiesoffederalandreserveonlyifthegistofthesentenceindicatesthatitreferstoafinancialinstitution.IfwordslikestreamormeadowThomasL.Griffiths,DepartmentofPsychology,UniversityofCalifor-nia,Berkeley;MarkSteyvers,DepartmentofCognitiveSciences,Univer-sityofCalifornia,Irvine;JoshuaB.Tenenbaum,DepartmentofBrainandCognitiveSciences,MassachusettsInstituteofTechnology.ThisworkwassupportedbyagrantfromtheNTTCommunicationSciencesLaboratoryandtheDefenseAdvancedResearchProjectsAgency“CognitiveAgentThatLearnsandOrganizes”project.Whilecompletingthiswork,ThomasL.GriffithswassupportedbyaStanfordGraduateFellowshipandagrantfromtheNationalScienceFoundation(BCS0631518),andJoshuaB.TenenbaumwassupportedbythePaulE.Newtonchair.WethankTouchstoneAppliedSciences,TomLandauer,andDarrellLahamformakingtheTouchstoneAppliedScienceAssociatescorpusavailableandStevenSlomanforcommentsonthemanuscript.AMAT-LABtoolboxcontainingcodeforsimulatingthevarioustopicmodelsdescribedinthisarticleisavailableathttp://psiexp.ss.uci.edu/research/programs_data/toolbox.htm.CorrespondenceconcerningthisarticleshouldbeaddressedtoThomasL.Griffiths,DepartmentofPsychology,UniversityofCalifornia,Berkeley,3210TolmanHall,MC1650,Berkeley,CA94720-1650.E-mail:tom_griffiths@berkeley.eduPsychologicalReviewCopyright2007bytheAmericanPsychologicalAssociation2007,Vol.114,No.2,211–2440033-295X/07/$12.00DOI:10.1037/0033-295X.114.2.211211alsoappearinthesentence,thenitislikelythatbankreferstothesideofariver,andwordslikewoodsandfieldshouldincreaseinprobability.Theabilitytoextractgisthasinfluencesthatreachbeyondlanguageprocessing,pervadingevensimpletaskssuchasmemo-rizinglistsofwords.Anumberofstudieshaveshownthatwhenpeopletrytorememberalistofwordsthataresemanticallyassociatedwithawordthatdoesnotappearonthelist,theassociatedwordintrudesontheirmemory(Deese,1959;McEvoy,Nelson,&Komatsu,1999;Roediger,Watson,McDermott,&Gallo,2001).Resultsofthiskindhaveledtothedevelopmentofdual-routememorymodels,whichsuggestthatpeopleencodenotjusttheverbatimcontentofalistofwordsbutalsotheirgist(Brainerd,Reyna,&Mojardin,1999;Brainerd,Wright,&Reyna,2002;Mandler,1980).Thesemodelsleaveopenthequestionofhowthememorysystemidentifiesthisgist.Inthisarticle,weanalyzetheabstractcomputationalproblemofextractingandusingthegistofasetofwordsandexaminehowwelldifferentsolutionstothisproblemcorrespondtohumanbehavior.Thekeydifferencebetweenthesesolutionsisthewaythattheyrepresentgist.Inpreviouswork,theextractionanduseofgisthasbeenmodeledusingassociativesemanticnetworks(e.g.,Collins&Loftus,1975)andsemanticspaces(e.g.,Landauer&Dumais,1997;Lund&Burgess,1996).ExamplesofthesetworepresentationsareshowninFigures1aand1b,respectively.Wetakeastepbackfromthesespecificproposalsandprovideamoregeneralformulationofthecomputationalproblemthattheserep-resentationsareusedtosolve.Weexpresstheproblemasoneofstatisticalinference:givensomedata—thesetofwords—inferringthelatentstructurefromwhichitwasgenerated.Statingtheprobleminthesetermsmakesitpossibletoexploreformsofsemanticrepresentationthatgobeyondnetworksandspaces.Identifyingthestatisticalproblemunderlyingtheextractionanduseofgistmakesitpossibletouseanyformofsemanticrepre-sentation;allthatneedstobespecifiedisaprobabilisticprocessbywhichasetofwordsisgeneratedusingthatrepresentationoftheirgist.Inmachinelearningandstatistics,suchaprobabilisticprocessiscalledagenerativemodel.Mostcomputationalapproachestonaturallanguagehavetendedtofocusexclusivelyoneitherstruc-turedrepresentations(e.g.,Chomsky,1965;Pinker,1999)orsta-tisticallearning(e.g.,Elman,1990;Plunkett&Marchman,1993;Rumelhart&McClelland,1986).Generativemodelsprovideawaytocombinethestrengthsofthesetwotraditions,makingitpossibletousestatisticalmethodstolearnstructuredrepresenta-tions.Asaconsequence,generativemodelshaverecentlybecomepopularinbothcomputationallinguistics(e.g.,Charniak,1993;Jurafsky&Martin,2000;Manning&Schu¨tze,1999)andpsycho-linguistics(e.g.,Baldewein&Keller,2004;Jurafsky,1996),al-thoughthisworkhastendedtoemphasizesyntacticstructureoversemantics.Thecombinationofstructuredrepresentationswithstatisticalinferencemakesgenerativemodelstheperfecttoolforevaluatingnovelapproachestosemanticrepresentation.Weuseourformalframeworktoexploretheideathatthegistofasetofwordscanberepresentedasaprobabilitydistributionoverasetoftopics.Eachtopicisaprobabilitydistributionoverwords,andthecontentofthetopicisreflectedinthewordstowhichitassignshighprobability.Forexample,highprobabilitiesforwoodsandstreamwouldsuggestthatatopicreferstothecountryside,whereashighprob-abilitiesforfederalandreservewouldsuggestthatatopicreferstofinance.AschematicillustrationofthisformofrepresentationappearsinFigure1c.Followingworkintheinformationretrievalliterature(Blei,Ng,&Jordan,2003),weuseasimplegenerativemodelthatdefinesaprobabilitydistributionoverasetofwords,suchasalistoradocument,givenaprobabilitydistributionovertopics.WithmethodsdrawnfromBayesianstatistics,asetoftopicscanbelearnedautomaticallyfromacollectionofdocu-ments,asacomputationalanalogueofhowhumanlearnersmightformsemanticrepresentationsthroughtheirlinguisticexperience(Griffiths&Steyvers,2002,2003,2004).Thetopicmodelprovidesastartingpointforaninvestigationofnewformsofsemanticrepresentation.Representingwordsusingtopicshasanintuitivecorrespondencetofeature-basedmodelsofsimilarity.Wordsthatreceivehighprobabilityunderthesametopicswilltendtobehighlypredictiveofoneanother,justasstimulithatsharemanyfeatureswillbehighlysimilar.Weshowthatthisintuitivecorrespondenceissupportedbyaformalcorre-spondencebetweenthetopicmodelandTversky’s(1977)feature-basedapproachtomodelingsimilarity.Becausethetopicmodelusesexactlythesameinputaslatentsemanticanalysis(LSA;(a)STREAMBANKMEADOWFEDERALRESERVE(b)BANKCOMMERCIALCRUDEDEEPDEPOSITSDRILLFEDERALFIELDGASOLINELOANSMEADOWMONEYOILPETROLEUMRESERVERIVERSTREAMWOODS(c)TopicMONEYBANKFEDERALRESERVELOANSDEPOSITSCOMMERCIALDEEPMEADOWOILRIVERCRUDEDRILLFIELDGASOLINEPETROLEUMSTREAMWOODSSTREAMRIVERBANKDEEPWOODSFIELDMONEYMEADOWOILFEDERALDEPOSITSLOANSCOMMERCIALCRUDEDRILLGASOLINEPETROLEUMRESERVEOILPETROLEUMGASOLINECRUDECOMMERCIALDEPOSITSRIVERDRILLMONEYDEEPFIELDSTREAMRESERVEBANKLOANSFEDERALMEADOWWOODS123BANKCOMMERCIALCRUDEDEEPDEPOSITSDRILLFEDERALFIELDGASOLINELOANSMEADOWMONEYOILPETROLEUMRESERVERIVERSTREAMWOODSFigure1.Approachestosemanticrepresentation.(a)Inasemanticnetwork,wordsarerepresentedasnodes,andedgesindicatesemanticrelationships.(b)Inasemanticspace,wordsarerepresentedaspoints,andproximityindicatessemanticassociation.Thesearethefirsttwodimensionsofasolutionproducedbylatentsemanticanalysis(Landauer&Dumais,1997).Theblackdotistheorigin.(c)Inthetopicmodel,wordsarerepresentedasbelongingtoasetofprobabilistictopics.Thematrixshownontheleftindicatestheprobabilityofeachwordundereachofthreetopics.Thethreecolumnsontherightshowthewordsthatappearinthosetopics,orderedfromhighesttolowestprobability.212GRIFFITHS,STEYVERS,ANDTENENBAUMLandauer&Dumais,1997),aleadingmodeloftheacquisitionofsemanticknowledgeinwhichtheassociationbetweenwordsde-pendsonthedistancebetweentheminasemanticspace,wecancomparethesetwomodelsasameansofexaminingtheimplica-tionsofdifferentkindsofsemanticrepresentation,justasfeaturalandspatialrepresentationshavebeencomparedasmodelsofhumansimilarityjudgments(Tversky,1977;Tversky&Gati,1982;Tversky&Hutchinson,1986).Furthermore,thetopicmodelcaneasilybeextendedtocaptureotherkindsoflatentlinguisticstructure.Introducingnewelementsintoagenerativemodelisstraightforward,andbyaddingcomponentstothemodelthatcancapturerichersemanticstructureorrudimentarysyntax,wecanbegintodevelopmorepowerfulstatisticalmodelsoflanguage.Theplanofthearticleisasfollows.First,weprovideamoredetailedspecificationofthekindofsemanticinformationweaimtocaptureinourmodelsandsummarizethewaysinwhichthishasbeendoneinpreviouswork.Wethenanalyzetheabstractcom-putationalproblemofextractingandusinggist,formulatingthisproblemasoneofstatisticalinferenceandintroducingthetopicmodelasonemeansofsolvingthiscomputationalproblem.Thebodyofthearticleisconcernedwithassessinghowwelltherepresentationrecoveredbythetopicmodelcorrespondswithhumansemanticmemory.InananalysisinspiredbyTversky’s(1977)critiqueofspatialmeasuresofsimilarity,weshowthatseveralaspectsofwordassociationthatcanbeexplainedbythetopicmodelareproblematicforLSA.Wethencomparetheper-formanceofthetwomodelsinavarietyofothertaskstappingsemanticrepresentationandoutlinesomeofthewaysinwhichthetopicmodelcanbeextended.ApproachestoSemanticRepresentationSemanticrepresentationisoneofthemostformidabletopicsincognitivepsychology.Thefieldisfraughtwithmurkyandpoten-tiallynever-endingdebates;itishardtoimaginethatonecouldgiveacompletetheoryofsemanticrepresentationoutsideofacompletetheoryofcognitioningeneral.Consequently,formalapproachestomodelingsemanticrepresentationhavefocusedonvarioustractableaspectsofsemanticknowledge.Beforepresent-ingourapproach,wemustclarifywhereitsfocuslies.Semanticknowledgecanbethoughtofasknowledgeaboutrelationsamongseveraltypesofelements,includingwords,con-cepts,andpercepts.Somerelationsthathavebeenstudiedincludethefollowing:Word–conceptrelations:Knowledgethattheworddogreferstotheconcept“dog,”thewordanimalreferstotheconcept“animal,”orthewordtoasterreferstotheconcept“toaster.”Concept–conceptrelations:Knowledgethatdogsareakindofanimal,thatdogshavetailsandcanbark,orthatanimalshavebodiesandcanmove.Concept–perceptorconcept–actionrelations:Knowledgeaboutwhatdogslooklike,howadogcanbedistinguishedfromacat,orhowtopetadogoroperateatoaster.Word–wordrelations:Knowledgethattheworddogtendstobeassociatedwithorco-occurwithwordssuchastail,bone,andcatorthatthewordtoastertendstobeassociatedwithkitchen,oven,orbread.Thesedifferentaspectsofsemanticknowledgearenotneces-sarilyindependent.Forinstance,thewordcatmaybeassociatedwiththeworddogbecausecatreferstocats,dogreferstodogs,andcatsanddogsarebothcommonkindsofanimals.Yetdifferentaspectsofsemanticknowledgecaninfluencebehaviorindifferentwaysandseemtobebestcapturedbydifferentkindsofformalrepresentations.Asaresult,differentapproachestomodelingsemanticknowledgetendtofocusondifferentaspectsofthisknowledge,dependingonwhatfitsmostnaturallywiththerepre-sentationalsystemtheyadopt,andtherearecorrespondingdiffer-encesinthebehavioralphenomenatheyemphasize.Computa-tionalmodelsalsodifferintheextenttowhichtheirsemanticrepresentationscanbelearnedautomaticallyfromsomenaturallyoccurringdataormustbehand-codedbythemodeler.Althoughmanydifferentmodelingapproachescanbeimaginedwithinthisbroadlandscape,therearetwoprominenttraditions.Onetraditionemphasizesabstractconceptualstructure,focusingonrelationsamongconceptsandrelationsbetweenconceptsandperceptsoractions.Thisknowledgeistraditionallyrepresentedintermsofsystemsofabstractpropositions,suchasis-acanarybird,hasbirdwings,andsoon(Collins&Quillian,1969).Modelsinthistraditionhavefocusedonexplainingphenomenasuchasthedevelopmentofconceptualhierarchiesthatsupportpropositionalknowledge(e.g.,Keil,1979),reactiontimetoverifyconceptualpropositionsinnormaladults(e.g.,Collins&Quillian,1969),andthedecayofpropositionalknowledgewithagingorbraindamage(e.g.,Warrington,1975).Thisapproachdoesnotworrymuchaboutthemappingsbetweenwordsandconceptsorassociativerelationsbetweenwords;inpractice,thedistinctionbetweenwordsandconceptsistypicallycollapsed.Actuallanguageuseisad-dressedonlyindirectly:Therelevantexperimentsareoftencon-ductedwithlinguisticstimuliandresponses,buttheprimaryinterestisnotintherelationbetweenlanguageuseandconceptualstructure.Representationsofabstractsemanticknowledgeofthiskindhavetraditionallybeenhand-codedbymodelers(Collins&Quillian,1969),inpartbecauseitisnotclearhowtheycouldbelearnedautomatically.Recentlytherehasbeensomeprogressinlearningdistributedrepresentationsofconceptualrelations(Rog-ers&McClelland,2004),althoughtheinputtotheselearningmodelsisstillquiteidealized,intheformofhand-codeddatabasesofsimplepropositions.Learninglarge-scalerepresentationsofabstractconceptualrelationsfromnaturallyoccurringdataremainsanunsolvedproblem.Asecondtraditionofstudyingsemanticrepresentationhasfocusedmoreonthestructureofassociativerelationsbetweenwordsinnaturallanguageuseandrelationsbetweenwordsandconcepts,alongwiththecontextualdependenceoftheserelations.Forinstance,whenonehearsthewordbird,itbecomesmorelikelythatonewillalsohearwordslikesing,fly,andnestinthesamecontext—butperhapslesssoifthecontextalsocontainsthewordsthanksgiving,turkey,anddinner.Theseexpectationsreflectthefactthatbirdhasmultiplesenses,ormultipleconceptsitcanreferto,includingbothataxonomiccategoryandafoodcategory.Thesemanticphenomenastudiedinthistraditionmayappeartobesomewhatsuperficial,inthattheytypicallydonottapdeepcon-ceptualunderstanding.Thedatatendtobetiedmoredirectlyto213TOPICSINSEMANTICREPRESENTATIONlanguageuseandthememorysystemsthatsupportonlinelinguis-ticprocessing,suchasword-associationnorms(e.g.,Nelson,Mc-Evoy,&Schreiber,1998),wordreadingtimesinsentencepro-cessing(e.g.,Sereno,Pacht,&Rayner,1992),semanticpriming(e.g.,Till,Mross,&Kintsch,1988),andeffectsofsemanticcontextinfreerecall(e.g.,Roediger&McDermott,1995).Com-paredwithapproachesthatfocusondeeperconceptualrelations,classicmodelsofsemanticassociationtendtoinvokemuchsim-plersemanticrepresentations,suchassemanticspacesorholisticspreadingactivationnetworks(e.g.,Collins&Loftus,1975;Deese,1959).Thissimplicityhasitsadvantages:Therehasre-centlybeenconsiderablesuccessinlearningthestructureofsuchmodelsfromlarge-scalelinguisticcorpora(e.g.,Landauer&Du-mais,1997;Lund&Burgess,1996).Werecognizetheimportanceofbothofthesetraditionsinstudyingsemanticknowledge.Theyhavecomplementarystrengthsandweaknesses,andultimatelyideasfrombotharelikelytobeimportant.Ourworkhereismoreclearlyinthesecondtradition,withitsemphasisonrelativelylightrepresentationsthatcanbelearnedfromlargetextcorpora,andonexplainingthestructureofword–wordandword–conceptassociations,rootedinthecontextsofactuallanguageuse.Althoughtheinterpretationofsentencesrequiressemanticknowledgethatgoesbeyondthesecontextualassociativerelationships,manytheoriesstillidentifythislevelofknowledgeasplayinganimportantroleintheearlystagesoflanguageprocessing(Ericsson&Kintsch,1995;Kintsch,1988;Potter,1993).Specifically,itsupportssolutionstothreecorecomputationalproblems:Prediction:Predictthenextwordorconcept,facilitatingretrieval.Disambiguation:Identifythesensesormeaningsofwords.Gistextraction:Pickoutthegistofasetofwords.Ourgoalistounderstandhowcontextualsemanticassociationisrepresented,used,andacquired.Wearguethatconsideringrela-tionsbetweenlatentsemantictopicsandobservablewordformsprovidesawaytocapturemanyaspectsofthislevelofknowledge:Itprovidesprincipledandpowerfulsolutionstothesethreecoretasks,anditisalsoeasilylearnablefromnaturallinguisticexpe-rience.Beforeintroducingthismodelingframework,wesumma-rizethetwodominantapproachestotherepresentationofsemanticassociation,semanticnetworksandsemanticspaces,establishingthebackgroundtotheproblemsweconsider.SemanticNetworksInanassociativesemanticnetwork,suchasthatshowninFigure1a,asetofwordsorconceptsisrepresentedasnodesconnectedbyedgesthatindicatepairwiseassociations(e.g.,Col-lins&Loftus,1975).Seeingawordactivatesitsnode,andacti-vationspreadsthroughthenetwork,activatingnodesthatarenearby.Semanticnetworksprovideanintuitiveframeworkforexpressingthesemanticrelationshipsbetweenwords.Theyalsoprovidesimplesolutionstothethreeproblemsforwhichcontex-tualknowledgemightbeused.Treatingthoseproblemsinthereverseoftheorderidentifiedabove,gistextractionsimplycon-sistsofactivatingeachwordthatoccursinagivencontextandallowingthatactivationtospreadthroughthenetwork.Thegistisrepresentedbythepatternofnodeactivities.Ifdifferentmeaningsofwordsarerepresentedasdifferentnodes,thenthenetworkdisambiguatesbycomparingtheactivationofthosenodes.Finally,thewordsthatonemightexpecttoseenextinthatcontextwillbethewordsthathavehighactivationsasaresultofthisprocess.Mostsemanticnetworksthatareusedascomponentsofcogni-tivemodelsareconsiderablymorecomplexthantheexampleshowninFigure1a,allowingmultipledifferentkindsofnodesandconnections(e.g.,Anderson,1983;Norman,Rumelhart,&TheLNRResearchGroup,1975).Inadditiontoexcitatoryconnec-tions,inwhichactivationofonenodeincreasesactivationofanother,somesemanticnetworksfeatureinhibitoryconnections,allowingactivationofonenodetodecreaseactivationofanother.Theneedforinhibitoryconnectionsisindicatedbyempiricalresultsintheliteratureonpriming.Asimplenetworkwithoutinhibitoryconnectionscanexplainwhyprimingmightfacilitatelexicaldecision,makingiteasiertorecognizethatatargetisanEnglishword.Forexample,awordlikenurseprimestheworddoctorbecauseitactivatesconceptsthatarecloselyrelatedtodoctor,andthespreadofactivationultimatelyactivatesdoctor.However,notallprimingeffectsareofthisform.Forexample,Neely(1976)showedthatprimingwithirrelevantcuescouldhaveaninhibitoryeffectonlexicaldecision.TouseanexamplefromMarkman(1998),primingwithhockeycouldproduceaslowerreactiontimefordoctorthanpresentingacompletelyneutralprime.Effectslikethesesuggestthatweneedtoincorporateinhibitorylinksbetweenwords.Ofinterest,itwouldseemthatagreatmanysuchlinkswouldberequired,becausethereisnoobviousspecialrelationshipbetweenhockeyanddoctor;thetwowordsjustseemunrelated.Thus,inhibitorylinkswouldseemtobeneededbetweenallpairsofunrelatedwordsinordertoexplaininhibitorypriming.SemanticSpacesAnalternativetosemanticnetworksistheideathatthemeaningofwordscanbecapturedusingaspatialrepresentation.Inasemanticspace,suchasthatshowninFigure1b,wordsarenearbyiftheyaresimilarinmeaning.Thisideaappearsinearlyworkexploringtheuseofstatisticalmethodstoextractrepresentationsofthemeaningofwordsfromhumanjudgments(Deese,1959;Fillenbaum&Rapoport,1971).Recentresearchhaspushedthisideaintwodirections.First,connectionistmodelsusingdistrib-utedrepresentationsforwords—whicharecommonlyinterpretedasaformofspatialrepresentation—havebeenusedtopredictbehavioronavarietyoflinguistictasks(e.g.,Kawamoto,1993;Plaut,1997;Rodd,Gaskell,&Marslen-Wilson,2004).Thesemodelsperformrelativelycomplexcomputationsontheunderly-ingrepresentationsandallowwordstoberepresentedasmultiplepointsinspace,buttheyaretypicallytrainedonartificiallygen-erateddata.Asecondthrustofrecentresearchhasbeenexploringmethodsforextractingsemanticspacesdirectlyfromreallinguis-ticcorpora(Landauer&Dumais,1997;Lund&Burgess,1996).Thesemethodsarebasedoncomparativelysimplemodels—forexample,theyassumeeachwordisrepresentedasonlyasinglepoint—butprovideadirectmeansofinvestigatingtheinfluenceofthestatisticsoflanguageonsemanticrepresentation.214GRIFFITHS,STEYVERS,ANDTENENBAUMLSAisoneofthemostprominentmethodsforextractingaspatialrepresentationforwordsfromamultidocumentcorpusoftext.TheinputtoLSAisaword–documentco-occurrencematrix,suchasthatshowninFigure2.Inaword–documentco-occurrencematrix,eachrowrepresentsaword,eachcolumnrepresentsadocument,andtheentriesindicatethefrequencywithwhichthatwordoccurredinthatdocument.ThematrixshowninFigure2isaportionofthefullco-occurrencematrixfortheTouchstoneAppliedScienceAssociates(TASA)corpus(Landauer&Dumais,1997),acollectionofpassagesexcerptedfromeducationaltextsusedincurriculafromthefirstyearofschooltothefirstyearofcollege.TheoutputfromLSAisaspatialrepresentationforwordsanddocuments.Afteroneappliesvarioustransformationstotheentriesinaword–documentco-occurrencematrix(onestandardsetoftransformationsisdescribedinGriffiths&Steyvers,2003),sin-gularvaluedecompositionisusedtofactorizethismatrixintothreesmallermatrices,U,D,andV,asshowninFigure3a.Eachofthesematriceshasadifferentinterpretation.TheUmatrixprovidesanorthonormalbasisforaspaceinwhicheachwordisapoint.TheDmatrix,whichisdiagonal,isasetofweightsforthedimensionsofthisspace.TheVmatrixprovidesanorthonormalbasisforaspaceinwhicheachdocumentisapoint.Anapprox-imationtotheoriginalmatrixoftransformedcountscanbeob-tainedbyremultiplyingthesematricesbutchoosingtouseonlytheinitialportionsofeachmatrix,correspondingtotheuseofalowerdimensionalspatialrepresentation.InpsychologicalapplicationsofLSA,thecriticalresultofthisprocedureisthefirstmatrix,U,whichprovidesaspatialrepresen-tationforwords.Figure1bshowsthefirsttwodimensionsofUfortheword–documentco-occurrencematrixshowninFigure2.TheresultsshowninthefiguredemonstratethatLSAidentifiessomeappropriateclustersofwords.Forexample,oil,petroleum,andcrudeareclosetogether,asarefederal,money,andreserve.Theworddepositsliesbetweenthetwoclusters,reflectingthefactthatitcanappearineithercontext.ThecosineoftheanglebetweenthevectorscorrespondingtowordsinthesemanticspacedefinedbyUhasproventobeaneffectivemeasureofthesemanticassociationbetweenthosewords(Landauer&Dumais,1997).Thecosineoftheanglebetweentwovectorsw1andw2(bothrowsofU,convertedtocolumnvectors)iscos(w1,w2)⫽w1Tw2储w1储储w2储,(1)wherew1Tw2istheinnerproductofthevectorsw1andw2,and||w||denotesthenorm,冑wTw.Performanceinpredictinghumanjudg-mentsistypicallybetterwhenoneusesonlythefirstfewhundredderiveddimensions,becausereducingthedimensionalityoftherepresentationcandecreasetheeffectsofstatisticalnoiseandemphasizethelatentcorrelationsamongwords(Landauer&Du-mais,1997).LSAprovidesasimpleprocedureforextractingaspatialrepre-sentationoftheassociationsbetweenwordsfromaword–documentco-occurrencematrix.Thegistofasetofwordsisrepresentedbytheaverageofthevectorsassociatedwiththosewords.ApplicationsofLSAoftenevaluatethesimilaritybetweentwodocumentsbycomputingthecosinebetweentheaveragewordvectorsforthosedocuments(Landauer&Dumais,1997;Rehderetal.,1998;Wolfeetal.,1998).Thisrepresentationofthegistofasetofwordscanbeusedtoaddressthepredictionproblem:Weshouldpredictthatwordswithvectorsclosetothegistvectorarelikelytooccurinthesamecontext.However,therepresentationofwordsaspointsinanundifferentiatedeuclideanspacemakesitdifficultforLSAtosolvethedisambiguationproblem.Thekeyissueisthatthisrelativelyunstructuredrepresentationdoesnotexplicitlyidentifythedifferentsensesofwords.Althoughdepositsliesbetweenwordshavingtodowithfinanceandwordshavingtodowithoil,thefactthatthiswordhasmultiplesensesisnotencodedintherepresentation.ExtractingandUsingGistasStatisticalProblemsSemanticnetworksandsemanticspacesarebothproposalsforaformofsemanticrepresentationthatcanguidelinguisticprocess-ing.Wenowtakeastepbackfromthesespecificproposalsandconsidertheabstractcomputationalproblemthattheyareintendedtosolve,inthespiritofMarr’s(1982)notionofthecomputationallevelandAnderson’s(1990)rationalanalysis.Ouraimistoclarifythegoalsofthecomputationandtoidentifythelogicbywhichthesegoalscanbeachieved,sothatthislogiccanbeusedasthebasisforexploringotherapproachestosemanticrepresentation.Assumewehaveseenasequenceofwordsw⫽共w1,w2,...,wn).Thesenwordsmanifestsomelatentsemanticstructurel.WewillDocument102030405060708090BANKCOMMERCIALCRUDEDEEPDEPOSITSDRILLFEDERALFIELDGASOLINELOANSMEADOWMONEYOILPETROLEUMRESERVERIVERSTREAMWOODSFigure2.Aword–documentco-occurrencematrix,indicatingthefrequenciesof18wordsacross90docu-mentsextractedfromtheTouchstoneAppliedScienceAssociatescorpus.Atotalof30documentsusethewordmoney,30usethewordoil,and30usethewordriver.Eachrowcorrespondstoawordinthevocabulary,andeachcolumncorrespondstoadocumentinthecorpus.Grayscaleindicatesthefrequencywithwhichthe731tokensofthosewordsappearedinthe90documents,withblackbeingthehighestfrequencyandwhitebeingzero.215TOPICSINSEMANTICREPRESENTATIONassumethatlconsistsofthegistofthatsequenceofwordsgandthesenseormeaningofeachword,z⫽共z1,z2,...,zn),sol⫽(g,z).Wecannowformalizethethreeproblemsidentifiedintheprevioussection:Prediction:Predictwn⫹1fromw.Disambiguation:Inferzfromw.Gistextraction:Infergfromw.Eachoftheseproblemscanbeformulatedasastatisticalproblem.Thepredictionproblemrequirescomputingtheconditionalprob-abilityofwn⫹1givenw,P(wn⫹1|w).Thedisambiguationproblemrequirescomputingtheconditionalprobabilityofzgivenw,P(z|w).Thegistextractionproblemrequirescomputingtheprob-abilityofggivenw,P(g|w).Alloftheprobabilitiesneededtosolvetheproblemsofpredic-tion,disambiguation,andgistextractioncanbecomputedfromasinglejointdistributionoverwordsandlatentstructures,P(w,l).Theproblemsofprediction,disambiguation,andgistextractioncanthusbesolvedbylearningthejointprobabilitiesofwordsandlatentstructures.Thiscanbedoneusingagenerativemodelforlanguage.Generativemodelsarewidelyusedinmachinelearningandstatisticsasameansoflearningstructuredprobabilitydistri-butions.Agenerativemodelspecifiesahypotheticalcausalpro-cessbywhichdataaregenerated,breakingthisprocessdownintoprobabilisticsteps.Critically,thisprocedurecaninvolveunob-servedvariables,correspondingtolatentstructurethatplaysaroleingeneratingtheobserveddata.Statisticalinferencecanbeusedtoidentifythelatentstructuremostlikelytohavebeenresponsibleforasetofobservations.AschematicgenerativemodelforlanguageisshowninFig-ure4a.Inthismodel,latentstructurelgeneratesanobservedsequenceofwordsw⫽共w1,...,wn).Thisrelationshipisillus-tratedusinggraphicalmodelnotation(e.g.,Jordan,1998;Pearl,Dweightsdimensionstopicdistributions=documentsUwordspace=documentsdimensionsdimensionstopicstopics(b)(a)LSATopic modeldimensionsP(z g)sdrowsdrowover words P(w z)wordswordsXtransformedword-documentco-occurrencematrixP(w g)probabilitydistributionsover words TVdocumentsdocument spacedocumentsdocument distributions over topicsFigure3.(a)Latentsemanticanalysis(LSA)performsdimensionalityreductionusingthesingularvaluedecomposition.Thetransformedword–documentco-occurrencematrix,X,isfactorizedintothreesmallermatrices,U,D,andV.Uprovidesanorthonormalbasisforaspatialrepresentationofwords,Dweightsthosedimensions,andVprovidesanorthonormalbasisforaspatialrepresentationofdocuments.(b)Thetopicmodelperformsdimensionalityreductionusingstatisticalinference.Theprobabilitydistributionoverwordsforeachdocumentinthecorpusconditionedonitsgist,P(w|g),isapproximatedbyaweightedsumoverasetofprobabilistictopics,representedwithprobabilitydistributionsoverwords,P(w|z),wheretheweightsforeachdocumentareprobabilitydistributionsovertopics,P(z|g),determinedbythegistofthedocument,g.Figure4.Generativemodelsforlanguage.(a)Aschematicrepresenta-tionofgenerativemodelsforlanguage.Latentstructurelgenerateswordsw.Thisgenerativeprocessdefinesaprobabilitydistributionoverl,P(l),andwgivenl,P(w|l).ApplyingBayes’srulewiththesedistributionsmakesitpossibletoinvertthegenerativeprocess,inferringlfromw.(b)LatentDirichletallocation(Bleietal.,2003),atopicmodel.Adocumentisgeneratedbychoosingadistributionovertopicsthatreflectsthegistofthedocument,g,choosingatopicziforeachpotentialwordfromadistributiondeterminedbyg,andthenchoosingtheactualwordwifromadistributiondeterminedbyzi.216GRIFFITHS,STEYVERS,ANDTENENBAUM1988).Graphicalmodelsprovideanefficientandintuitivemethodofillustratingstructuredprobabilitydistributions.Inagraphicalmodel,adistributionisassociatedwithagraphinwhichnodesarerandomvariablesandedgesindicatedependence.Unlikeartificialneuralnetworks,inwhichanodetypicallyindicatesasingleunidimensionalvariable,thevariablesassociatedwithnodescanbearbitrarilycomplex.lcanbeanykindoflatentstructure,andwrepresentsasetofnwords.ThegraphicalmodelshowninFigure4aisadirectedgraphicalmodel,witharrowsindicatingthedirectionoftherelationshipamongthevariables.Theresultisadirectedgraph,inwhich“parent”nodeshavearrowstotheir“children.”Inagenerativemodel,thedirectionofthesearrowsspecifiesthedirectionofthecausalprocessbywhichdataaregenerated:Avalueischosenforeachvariablebysamplingfromadistributionthatconditionsontheparentsofthatvariableinthegraph.Thegraphicalmodelshowninthefigureindicatesthatwordsaregeneratedbyfirstsamplingalatentstructure,l,fromadistributionoverlatentstructures,P(l),andthensamplingasequenceofwords,w,con-ditionedonthatstructurefromadistributionP(w|l).Theprocessofchoosingeachvariablefromadistributionconditionedonitsparentsdefinesajointdistributionoverobserveddataandlatentstructures.InthegenerativemodelshowninFigure4a,thisjointdistributionisP(w,l)⫽P(w|l)P(l).Withanappropriatechoiceofl,thisjointdistributioncanbeusedtosolvetheproblemsofprediction,disambiguation,andgistextractionidentifiedabove.Inparticular,theprobabilityofthelatentstructurelgiventhesequenceofwordswcanbecomputedbyapplyingBayes’srule:P共l兩w兲⫽P(w兩l)P(l)P(w),(2)whereP(w)⫽冘lP(w兩l)P(l).ThisBayesianinferenceinvolvescomputingaprobabilitythatgoesagainstthedirectionofthearrowsinthegraphicalmodel,invertingthegenerativeprocess.Equation2providesthefoundationforsolvingtheproblemsofprediction,disambiguation,andgistextraction.Theprobabilityneededforprediction,P(wn⫹1兩w),canbewrittenasP(wn⫹1兩w)⫽冘lP(wn⫹1兩l,w)P(l兩w),(3)whereP(wn⫹1兩l)isspecifiedbythegenerativeprocess.Distribu-tionsoverthesensesofwords,z,andtheirgist,g,canbecomputedbysummingouttheirrelevantaspectofl,P(z兩w)⫽冘gP(l兩w)(4)P(g兩w)⫽冘zP(l兩w),(5)whereweassumethatthegistofasetofwordstakesonadiscretesetofvalues—ifitiscontinuous,thenEquation5requiresanintegralratherthanasum.Thisabstractschemagivesageneralformcommontoallgenerativemodelsforlanguage.Specificmodelsdifferinthelatentstructurelthattheyassume,theprocessbywhichthislatentstructureisgenerated(whichdefinesP(l)),andtheprocessbywhichwordsaregeneratedfromthislatentstructure(whichde-finesP(w|l)).Mostgenerativemodelsthathavebeenappliedtolanguagefocusonlatentsyntacticstructure(e.g.,Charniak,1993;Jurafsky&Martin,2000;Manning&Schu¨tze,1999).Inthenextsection,wedescribeagenerativemodelthatrepresentsthelatentsemanticstructurethatunderliesasetofwords.RepresentingGistWithTopicsAtopicmodelisagenerativemodelthatassumesalatentstructurel⫽(g,z),representingthegistofasetofwords,g,asadistributionoverTtopicsandthesenseormeaningusedfortheithword,zi,asanassignmentofthatwordtooneofthesetopics.1Eachtopicisaprobabilitydistributionoverwords.Adocument—asetofwords—isgeneratedbychoosingthedistributionovertopicsreflectingitsgist,usingthisdistributiontochooseatopicziforeachwordwiandthengeneratingtheworditselffromthedistri-butionoverwordsassociatedwiththattopic.Giventhegistofthedocumentinwhichitiscontained,thisgenerativeprocessdefinestheprobabilityoftheithwordtobeP(wi兩g)⫽冘zi⫽1TP(wi兩zi)P(zi兩g),(6)inwhichthetopics,specifiedbyP(w|z),aremixedtogetherwithweightsgivenbyP(z|g),whichvaryacrossdocuments.2ThedependencystructureamongvariablesinthisgenerativemodelisshowninFigure4b.Intuitively,P(w|z)indicateswhichwordsareimportanttoatopic,whereasP(z|g)istheprevalenceofthosetopicsinadocu-ment.Forexample,ifwelivedinaworldwherepeopleonlywroteaboutfinance,theEnglishcountryside,andoilmining,thenwecouldmodelalldocumentswiththethreetopicsshowninFigure1c.ThecontentofthethreetopicsisreflectedinP(w|z):Thefinancetopicgiveshighprobabilitytowordslikereserveandfederal,thecountrysidetopicgiveshighprobabilitytowordslikestreamandmeadow,andtheoiltopicgiveshighprobabilitytowordslikepetroleumandgasoline.Thegistofadocument,g,indicateswhetheraparticulardocumentconcernsfinance,the1Thisformulationofthemodelmakestheassumptionthateachtopiccapturesadifferentsenseormeaningofaword.Thisneednotbethecase—theremaybeamany-to-onerelationshipbetweentopicsandthesensesormeaningsinwhichwordsareused.However,thetopicassign-mentstillcommunicatesinformationthatcanbeusedindisambiguationandpredictioninthewaythatthesenseormeaningmustbeused.Henceforth,wefocusontheuseofzitoindicateatopicassignment,ratherthanasenseormeaningforaparticularword.2WehavesuppressedthedependenceoftheprobabilitiesdiscussedinthissectionontheparametersspecifyingP(w|z)andP(z|g),assumingthattheseparametersareknown.AmorerigoroustreatmentofthecomputationoftheseprobabilitiesisgiveninAppendixA.217TOPICSINSEMANTICREPRESENTATIONcountryside,oilmining,orfinancinganoilrefineryinLeicester-shire,bydeterminingthedistributionovertopics,P(z|g).Equation6givestheprobabilityofawordconditionedonthegistofadocument.Wecandefineagenerativemodelforacollectionofdocumentsbyspecifyinghowthegistofeachdoc-umentischosen.Becausethegistisadistributionovertopics,thisrequiresusingadistributionovermultinomialdistributions.Theideaofrepresentingdocumentsasmixturesofprobabilistictopicshasbeenusedinanumberofapplicationsininformationretrievalandstatisticalnaturallanguageprocessing,withdifferentmodelsmakingdifferentassumptionsabouttheoriginsofthedistributionovertopics(e.g.,Bigi,DeMori,El-Beze,&Spriet,1997;Bleietal.,2003;Hofmann,1999;Iyer&Ostendorf,1999;Ueda&Saito,2003).WewilluseagenerativemodelintroducedbyBleietal.(2003)calledlatentDirichletallocation.Inthismodel,themulti-nomialdistributionrepresentingthegistisdrawnfromaDirichletdistribution,astandardprobabilitydistributionovermultinomials(e.g.,Gelman,Carlin,Stern,&Rubin,1995).Havingdefinedagenerativemodelforacorpusbasedonsomeparameters,onecanthenusestatisticalmethodstoinferthepa-rametersfromthecorpus.Inourcase,thismeansfindingasetoftopicssuchthateachdocumentcanbeexpressedasamixtureofthosetopics.Analgorithmforextractingasetoftopicsisde-scribedinAppendixA,andamoredetaileddescriptionandap-plicationofthisalgorithmcanbefoundinGriffithsandSteyvers(2004).Thisalgorithmtakesasinputaword–documentco-occurrencematrix.Theoutputisasetoftopics,eachbeingaprobabilitydistributionoverwords.ThetopicsshowninFigure1careactuallytheoutputofthisalgorithmwhenappliedtotheword–documentco-occurrencematrixshowninFigure2.Theseresultsillustratehowwellthetopicmodelhandleswordswithmultiplemeaningsorsenses:fieldappearsinboththeoilandcountrysidetopics,bankappearsinbothfinanceandcountryside,anddepositsappearsinbothoilandfinance.Thisisakeyadvan-tageofthetopicmodel:Byassumingamorestructuredrepresen-tation,inwhichwordsareassumedtobelongtotopics,themodelallowsthedifferentmeaningsorsensesofambiguouswordstobedifferentiated.Prediction,Disambiguation,andGistExtractionThetopicmodelprovidesadirectsolutiontotheproblemsofprediction,disambiguation,andgistextractionidentifiedintheprevioussection.ThedetailsofthesecomputationsarepresentedinAppendixA.Toillustratehowtheseproblemsaresolvedbythemodel,weconsiderasimplifiedcaseinwhichallwordsinasentenceareassumedtohavethesametopic.Inthiscase,gisadistributionthatputsallofitsprobabilityonasingletopic,z,andzi⫽zforalli.This“singletopic”assumptionmakesthemathe-maticsstraightforwardandisareasonableworkingassumptioninmanyofthesettingsweexplore.3Underthesingle-topicassumption,disambiguationandgistextractionbecomeequivalent:Thesensesandthegistofasetofwordsarebothexpressedinthesingletopic,z,thatwasrespon-sibleforgeneratingwordsw⫽兵w1,w2,…,wn}.ApplyingBayes’srule,wehaveP(z兩w)⫽P(w兩z)P(z)P(w)⫽写i⫽1nP(wi兩z)P(z)冘z写i⫽1nP(wi兩z)P(z),(7)wherewehaveusedthefactthatthewiareindependentgivenz.Ifweassumeauniformpriorovertopics,P(z)⫽1T,thedistributionovertopicsdependsonlyontheproductoftheprobabilitiesofeachofthewiundereachtopicz.Theproductactslikealogical“and”:Atopicwillbelikelyonlyifitgivesreasonablyhighprobabilitytoallofthewords.Figure5showshowthisfunctionstodisam-biguatewords,usingthetopicsfromFigure1.Whenthewordbankisseen,boththefinanceandthecountrysidetopicshavehighprobability.Seeingstreamquicklyswingstheprobabilityinfavorofthebucolicinterpretation.Solvingthedisambiguationproblemisthefirststepinsolvingthepredictionproblem.IncorporatingtheassumptionthatwordsareindependentgiventheirtopicsintoEquation3,wehaveP(wn⫹1兩w)⫽冘zP(wn⫹1兩z)P(z兩w).(8)Thepredicteddistributionoverwordsisthusamixtureoftopics,witheachtopicbeingweightedbythedistributioncomputedinEquation7.ThisisillustratedinFigure5:Whenbankisread,thepredicteddistributionoverwordsisamixtureofthefinanceandcountrysidetopics,butstreammovesthisdistributiontowardthecountrysidetopic.TopicsandSemanticNetworksThetopicmodelprovidesaclearwayofthinkingabouthowandwhy“activation”mightspreadthroughasemanticnetwork,anditcanalsoexplaininhibitoryprimingeffects.Thestandardconcep-tionofasemanticnetworkisagraphwithedgesbetweenwordnodes,asshowninFigure6a.Suchagraphisunipartite:Thereisonlyonetypeofnode,andthosenodescanbeinterconnectedfreely.Incontrast,bipartitegraphsconsistofnodesoftwotypes,andonlynodesofdifferenttypescanbeconnected.Wecanformabipartitesemanticnetworkbyintroducingasecondclassofnodesthatmediatetheconnectionsbetweenwords.Onewaytothinkabouttherepresentationofthemeaningsofwordsprovidedbythetopicmodelisintermsofthebipartitesemanticnetwork3Itisalsopossibletodefineagenerativemodelthatmakesthisassump-tiondirectly,havingjustonetopicpersentence,andtousetechniqueslikethosedescribedinAppendixAtoidentifytopicsusingthismodel.Wedidnotusethismodelbecauseitusesadditionalinformationaboutthestruc-tureofthedocuments,makingithardertocompareagainstalternativeapproachessuchasLSA(Landauer&Dumais,1997).Thesingle-topicassumptioncanalsobederivedastheconsequenceofhavingahyperpa-rameter␣favoringchoicesofzthatemployfewtopics:Thesingle-topicassumptionisproducedbyallowing␣toapproach0.218GRIFFITHS,STEYVERS,ANDTENENBAUMshowninFigure6b,wherethenodesinthesecondclassarethetopics.Inanycontext,thereisuncertaintyaboutwhichtopicsarerelevanttothatcontext.Whenawordisseen,theprobabilitydistributionovertopicsmovestofavorthetopicsassociatedwiththatword:P(z|w)movesawayfromuniformity.Thisincreaseintheprobabilityofthosetopicsisintuitivelysimilartotheideathatactivationspreadsfromthewordstothetopicsthatareconnectedwiththem.FollowingEquation8,thewordsassociatedwiththosetopicsalsoreceivehigherprobability.Thisdispersionofprobabil-itythroughoutthenetworkisagainreminiscentofspreadingactivation.However,thereisanimportantdifferencebetweenspreadingactivationandprobabilisticinference:Theprobabilitydistributionovertopics,P(z|w),isconstrainedtosumto1.Thismeansthatastheprobabilityofonetopicincreases,theprobabilityofanothertopicdecreases.Theconstraintthattheprobabilitydistributionovertopicssumsto1issufficienttoproducethephenomenonofinhibitoryprimingdiscussedabove.Inhibitoryprimingoccursasanecessaryconse-quenceofexcitatorypriming:Whentheprobabilityofonetopicincreases,theprobabilityofanothertopicdecreases.Conse-quently,itispossibleforonewordtodecreasethepredictedprobabilitywithwhichanotherwordwilloccurinaparticularcontext.Forexample,accordingtothetopicmodel,theprobabilityoftheworddoctoris.000334.Underthesingle-topicassumption,theprobabilityoftheworddoctorconditionedonthewordnurseis.0071,aninstanceofexcitatorypriming.However,theproba-bilityofdoctordropsto.000081whenconditionedonhockey.Thewordhockeysuggeststhatthetopicconcernssports,andconse-quentlytopicsthatgivedoctorhighprobabilityhavelowerweightinmakingpredictions.Byincorporatingtheconstraintthatprob-abilitiessumto1,generativemodelsareabletocaptureboththeexcitatoryandtheinhibitoryinfluenceofinformationwithoutrequiringtheintroductionoflargenumbersofinhibitorylinksbetweenunrelatedwords.TopicsandSemanticSpacesOurclaimthatmodelsthatcanaccuratelypredictwhichwordsarelikelytoariseinagivencontextcanprovidecluesabouthumanlanguageprocessingissharedwiththespiritofmanyconnectionistmodels(e.g.,Elman,1990).However,thestrongestparallelsbe-tweenourapproachandworkbeingdoneonspatialrepresenta-tionsofsemanticsareperhapsthosethatexistbetweenthetopicmodelandLSA.Indeed,theprobabilistictopicmodeldevelopedbyHofmann(1999)wasmotivatedbythesuccessofLSAandprovidedtheinspirationforthemodelintroducedbyBleietal.(2003)thatweusehere.BothLSAandthetopicmodeltakeaword–documentco-occurrencematrixasinput.BothLSAandthetopicmodelprovidearepresentationofthegistofadocument,eitherasapointinspaceorasadistributionovertopics.AndbothLSAandthetopicmodelcanbeviewedasaformof“dimension-alityreduction,”attemptingtofindalowerdimensionalrepresen-tationofthestructureexpressedinacollectionofdocuments.Inthetopicmodel,thisdimensionalityreductionconsistsoftryingtoexpressthelargenumberofprobabilitydistributionsoverwordsprovidedbythedifferentdocumentsintermsofasmallnumberoftopics,asillustratedinFigure3b.However,therearetwoimportantdifferencesbetweenLSAandthetopicmodel.ThemajordifferenceisthatLSAisnotagener-ativemodel.Itdoesnotidentifyahypotheticalcausalprocessresponsibleforgeneratingdocumentsandtheroleofthemeaningsofwordsinthisprocess.Asaconsequence,itisdifficulttoextendLSAtoincorporatedifferentkindsofsemanticstructureortorecognizethesyntacticrolesthatwordsplayinadocument.ThisleadstotheseconddifferencebetweenLSAandthetopicmodel:thenatureoftherepresentation.LSAisbasedonthesingularvaluedecomposition,amethodfromlinearalgebrathatcanyieldarepresentationofthemeaningsofwordsonlyaspointsinanundifferentiatedeuclideanspace.Incontrast,thestatisticalinfer-encetechniquesusedwithgenerativemodelsareflexibleandmakeitpossibletousestructuredrepresentations.Thetopicmodelprovidesasimplestructuredrepresentation:asetofindividuallymeaningfultopicsandinformationaboutwhichwordsbelongtoBANK12300.51TopicProbability00.20.4DRILLCRUDEGASOLINEPETROLEUMCOMMERCIALMEADOWOILFIELDWOODSDEPOSITSLOANSDEEPRESERVERIVERSTREAMFEDERALBANKMONEYProbabilityBANKSTREAM12300.51Topic00.20.4RESERVEDRILLCOMMERCIALCRUDEGASOLINEPETROLEUMLOANSDEPOSITSFEDERALOILMEADOWMONEYFIELDWOODSBANKDEEPRIVERSTREAMProbability(a)(b)(c)Figure5.Predictionanddisambiguation.(a)Wordsobservedinasen-tence,w.(b)Thedistributionovertopicsconditionedonthosewords,P(z|w).(c)Thepredicteddistributionoverwordsresultingfromsummingoverthisdistributionovertopics,P(wn⫹1兩w)⫽冘zP(wn⫹1兩z)P(z兩w).Onseeingbank,themodelisunsurewhetherthesentenceconcernsfinanceorthecountryside.Subsequentlyseeingstreamresultsinastrongconvictionthatbankdoesnotrefertoafinancialinstitution.(b)(a)wordwordwordwordwordwordwordwordtopictopicFigure6.Semanticnetworks.(a)Inaunipartitenetwork,thereisonlyoneclassofnodes.Inthiscase,allnodesrepresentwords.(b)Inabipartitenetwork,therearetwoclasses,andconnectionsexistonlybetweennodesofdifferentclasses.Inthiscase,oneclassofnodesrepresentswordsandtheotherclassrepresentstopics.219TOPICSINSEMANTICREPRESENTATIONthosetopics.Wewillshowthateventhissimplestructureissufficienttoallowthetopicmodeltocapturesomeofthequali-tativefeaturesofwordassociationthatproveproblematicforLSAandtopredictquantitiesthatcannotbepredictedbyLSA,suchasthenumberofmeaningsorsensesofaword.ComparingTopicsandSpacesThetopicmodelprovidesasolutiontotheproblemofextractingandusingthegistofasetofwords.Inthissection,weevaluatethetopicmodelasapsychologicalaccountofthecontentofhumansemanticmemory,comparingitsperformancewiththatofLSA.ThetopicmodelandLSAusethesameinput—aword–documentco-occurrencematrix—buttheydifferinhowthisinputisana-lyzedandinthewaythattheyrepresentthegistofdocumentsandthemeaningofwords.Bycomparingthesemodels,wehopetodemonstratetheutilityofgenerativemodelsforexploringques-tionsofsemanticrepresentationandtogainsomeinsightintothestrengthsandlimitationsofdifferentkindsofrepresentation.OurcomparisonofthetopicmodelandLSAhastwoparts.Inthissection,weanalyzethepredictionsofthetwomodelsindepthusingaword-associationtask,consideringboththequantitativeandthequalitativepropertiesofthesepredictions.Inparticular,weshowthatthetopicmodelcanexplainseveralphenomenaofwordassociationthatareproblematicforLSA.Thesephenomenaareanaloguesofthephenomenaofsimilarityjudgmentsthatareproblematicforspatialmodelsofsimilarity(Tversky,1977;Tver-sky&Gati,1982;Tversky&Hutchinson,1986).Inthenextsectionwecomparethetwomodelsacrossabroadrangeoftasks,showingthatthetopicmodelproducesthephenomenathatwereoriginallyusedtosupportLSAanddescribinghowthemodelcanbeusedtopredictdifferentaspectsofhumanlanguageprocessingandmemory.QuantitativePredictionsforWordAssociationArethereanymorefascinatingdatainpsychologythantablesofassociation?(Deese,1965,p.viii)AssociationhasbeenpartofthetheoreticalarmoryofcognitivepsychologistssinceThomasHobbesusedthenotiontoaccountforthestructureofour“trayneofthoughts”(Hobbes,1651/1998;detailedhistoriesofassociationareprovidedbyDeese,1965,andAnderson&Bower,1974).OneofthefirstexperimentalstudiesofassociationwasconductedbyGalton(1880),whousedaword-associationtasktostudydifferentkindsofassociation.SinceGalton,severalpsychologistshavetriedtoclassifykindsofasso-ciationortootherwisedivineitsstructure(e.g.,Deese,1962,1965).Thistheoreticalworkhasbeensupplementedbythedevel-opmentofextensiveword-associationnorms,listingcommonlynamedassociatesforavarietyofwords(e.g.,Cramer,1968;Kiss,Armstrong,Milroy,&Piper,1973;Nelson,McEvoy,&Schreiber,1998).Thesenormsprovidearichbodyofdata,whichhasonlyrecentlybeguntobeaddressedusingcomputationalmodels(Den-nis,2003;Nelson,McEvoy,&Dennis,2000;Steyvers,Shiffrin,&Nelson,2004).ThoughunlikeDeese(1965),wesuspectthattheremaybemorefascinatingpsychologicaldatathantablesofassociation,wordassociationprovidesausefulbenchmarkforevaluatingmodelsofhumansemanticrepresentation.Therelationshipbetweenwordassociationandsemanticrepresentationisanalogoustothatbe-tweensimilarityjudgmentsandconceptualrepresentation,beinganaccessiblebehaviorthatprovidescluesandconstraintsthatguidetheconstructionofpsychologicalmodels.Also,likesimi-larityjudgments,associationscoresarehighlypredictiveofotheraspectsofhumanbehavior.Word-associationnormsarecom-monlyusedinconstructingmemoryexperiments,andstatisticsderivedfromthesenormshavebeenshowntobeimportantinpredictingcuedrecall(Nelson,McKinney,Gee,&Janczura,1998),recognition(Nelson,McKinney,etal.,1998;Nelson,Zhang,&McKinney,2001),andfalsememories(Deese,1959;McEvoyetal.,1999;Roedigeretal.,2001).Itisnotourgoaltodevelopamodelofwordassociation,asmanyfactorsotherthansemanticassociationareinvolvedinthistask(e.g.,Ervin,1961;McNeill,1966),butwebelievethatissuesraisedbyword-associationdatacanprovideinsightintomodelsofsemanticrep-resentation.WeusedthenormsofNelson,McEvoy,andSchreiber(1998)toevaluatetheperformanceofLSAandthetopicmodelinpredictinghumanwordassociation.Thesenormswerecollectedusingafree-associationtask,inwhichparticipantswereaskedtoproducethefirstwordthatcameintotheirheadinresponsetoacueword.Theresultsareunusuallycomplete,withassociatesbeingderivedforeverywordthatwasproducedmorethanonceasanassociateforanyotherword.Foreachword,thenormsprovideasetofassociatesandthefrequencieswithwhichtheywerenamed,mak-ingitpossibletocomputetheprobabilitydistributionoverasso-ciatesforeachcue.WewilldenotethisdistributionP(w2|w1)foracuew1andassociatew2andorderassociatesbythisprobability:Thefirstassociatehashighestprobability,thesecondhasthenexthighest,andsoforth.WeobtainedpredictionsfromthetwomodelsbyderivingsemanticrepresentationsfromtheTASAcorpus(Landauer&Dumais,1997),whichisacollectionofexcerptsfromreadingmaterialscommonlyencounteredbetweenthefirstyearofschoolandthefirstyearofcollege.WeusedasmallervocabularythanpreviousapplicationsofLSAtoTASA,consideringonlywordsthatoccurredatleast10timesinthecorpusandwerenotincludedinastandard“stop”listcontain-ingfunctionwordsandotherhigh-frequencywordswithlowseman-ticcontent.Thisleftuswithavocabularyof26,243words,ofwhich4,235,314tokensappearedinthe37,651documentscontainedinthecorpus.Weusedthesingularvaluedecompositiontoextracta700-dimensionalrepresentationoftheword–documentco-occurrencesta-tistics,andweexaminedtheperformanceofthecosineasapredictorofwordassociationusingthisandavarietyofsubspacesoflowerdimensionality.Wealsocomputedtheinnerproductbetweenwordvectorsasanalternativemeasureofsemanticassociation,whichwediscussindetaillaterinthearticle.Ourchoicetouse700dimensionsasanupperlimitwasguidedbytwofactors,onetheoreticalandtheotherpractical:Previousanalyseshavesuggestedthattheperfor-manceofLSAwasbestwithonlyafewhundreddimensions(Land-auer&Dumais,1997),anobservationthatwasconsistentwithperformanceonourtask,and700dimensionsisthelimitofstandardalgorithmsforsingularvaluedecompositionwithamatrixofthissizeonaworkstationwith2GBofRAM.WeappliedthealgorithmforfindingtopicsdescribedinAp-pendixAtothesameword–documentco-occurrencematrix,ex-220GRIFFITHS,STEYVERS,ANDTENENBAUMtractingrepresentationswithupto1,700topics.Ouralgorithmisfarmorememoryefficientthanthesingularvaluedecomposition,asalloftheinformationrequiredthroughoutthecomputationcanbestoredinsparsematrices.Consequently,weranthealgorithmatincreasinglyhighdimensionalities,untilpredictionperformancebegantolevelout.Ineachcase,thesetoftopicsfoundbythealgorithmwashighlyinterpretable,expressingdifferentaspectsofthecontentofthecorpus.Aselectionoftopicsfromthe1,700-topicsolutionisshowninFigure7.Thetopicsfoundbythealgorithmpickoutsomeofthekeynotionsaddressedbydocumentsinthecorpus,includingveryspecificsubjectslikeprintingandcombustionengines.Thetopicsareextractedpurelyonthebasisofthestatisticalpropertiesofthewordsinvolved—roughly,thatthesewordstendtoappearinthesamedocuments—andthealgorithmdoesnotrequireanyspecialinitializationorotherhumanguidance.ThetopicsshowninFig-ure7werechosentoberepresentativeoftheoutputofthealgorithmandtoillustratehowpolysemousandhomonymouswordsarerepresentedinthemodel:Differenttopicscapturedif-ferentcontextsinwhichwordsareusedand,thus,differentmean-ingsorsenses.Forexample,thefirsttwotopicsshowninthefigurecapturetwodifferentmeaningsofcharacters:thesymbolsusedinprintingandthepersonasinaplay.Tomodelwordassociationwiththetopicmodel,weneedtospecifyaprobabilisticquantitythatcorrespondstothestrengthofassociation.Thediscussionoftheproblemofpredictionabovesuggestsanaturalmeasureofsemanticassociation:P(w2|w1),theprobabilityofwordw2givenwordw1.Usingthesingle-topicassumption,wehaveP(w2兩w1)⫽冘zP(w2兩z)P(z兩w1),(9)whichissimplyEquation8withn⫽1.ThedetailsofevaluatingthisprobabilityaregiveninAppendixA.Thisconditionalproba-bilityautomaticallycompromisesbetweenwordfrequencyandsemanticrelatedness:Higherfrequencywordswilltendtohavehigherprobabilitiesacrossalltopics,andthiswillbereflectedinP(w2|z),butthedistributionovertopicsobtainedbyconditioningonw1,P(z|w1),willensurethatsemanticallyrelatedtopicsdomi-natethesum.Ifw1ishighlydiagnosticofaparticulartopic,thenthattopicwilldeterminetheprobabilitydistributionoverw2.Ifw1providesnoinformationaboutthetopic,thenP(w2|w1)willbedrivenbywordfrequency.TheoverlapbetweenthewordsusedinthenormsandthevocabularyderivedfromTASAwas4,471words,andallanalysespresentedinthisarticlearebasedonthesubsetofthenormsthatusesthesewords.Ourevaluationofthetwomodelsinpredictingwordassociationwasbasedontwoperformancemeasures:(a)themedianrankofthefirstfiveassociatesundertheorderingimposedbythecosineortheconditionalprobabilityand(b)theprobabilityofthefirstassociatebeingincludedinsetsofwordsderivedfromthisordering.ForLSA,thefirstofthesemeasureswasassessedbycomputingthecosineforeachwordw2witheachcuew1,rankingthechoicesofw2bycos(w1,w2)suchthatthehighestrankedwordhadhighestcosine,andthenfindingtheranksofthefirstfiveassociatesforthatcue.Afterapplyingthisproceduretoall4,471cues,wecomputedthemedianranksforeachofthefirstfiveassociates.Ananalogousprocedurewasperformedwiththetopicmodel,usingP(w2|w1)intheplaceofcos(w1,w2).Thesecondofourmeasureswastheprobabilitythatthefirstassociateisincludedinthesetofthemwordswiththehighestranksundereachmodel,varyingm.Thesetwomeasuresarecomplementary:Thefirstindicatescentraltendency,whereasthesecondgivesthedistribu-tionoftherankofthefirstassociate.ThetopicmodeloutperformsLSAinpredictingassociationsbetweenwords.TheresultsofouranalysesareshowninFigure8.WetestedLSAsolutionswith100,200,300,400,500,600,and700dimensions.Inpredictionsofthefirstassociate,performancelevelsoutataround500dimensions,beingapproximatelythesameat600and700dimensions.Weusethe700-dimensionalsolutionfortheremainderofouranalyses,althoughourpointsaboutthequalitativepropertiesofLSAholdregardlessofdimensionality.Themedianrankofthefirstassociateinthe700-dimensionalsolutionwas31outof4,470,andthewordwiththehighestcosinewasthefirstassociatein11.54%ofcases.Wetestedthetopicmodelwith500,700,900,1,100,1,300,1,500,and1,700topics,findingthatperformancelevelsoutataround1,500topics.Weusethe1,700-dimensionalsolutionfortheremainderofouranalyses.ThemedianrankofthefirstassociateinP(w2|w1)was18,andthewordwiththehighestprobabilityunderthemodelwasthefirstFigure7.Asampleofthe1,700topicsderivedfromtheTouchstoneAppliedScienceAssociatescorpus.Eachcolumncontainsthe20highestprobabilitywordsinasingletopic,asindicatedbyP(w|z).Wordsinboldfaceoccurindifferentsensesinneighboringtopics,illustratinghowthemodeldealswithpolysemyandhomonymy.Thesetopicswerediscoveredinacompletelyunsupervisedfashion,usingjustword–documentco-occurrencefrequencies.221TOPICSINSEMANTICREPRESENTATIONassociatein16.15%ofcases,inbothinstancesanimprovementofaround40%onLSA.Theperformanceofbothmodelsonthetwomeasureswasfarbetterthanchance,whichwouldbe2,235.5and0.02%forthemedianrankandtheproportioncorrect,respectively.Thedimen-sionalityreductionperformedbythemodelsseemstoimprovepredictions.TheconditionalprobabilityP(w2|w1)computeddi-rectlyfromthefrequencieswithwhichwordsappearedindifferentdocumentsgaveamedianrankof50.5andpredictedthefirstassociatecorrectlyin10.24%ofcases.LSAthusimprovedontherawco-occurrenceprobabilitybybetween20%and40%,whereasthetopicmodelgaveanimprovementofover60%.Inbothcases,thisimprovementresultedpurelyfromhavingderivedalowerdimensionalrepresentationfromtherawfrequencies.Figure9showssomeexamplesoftheassociatesproducedbypeopleandbythetwodifferentmodels.Thefigureshowstwoexamplesrandomlychosenfromeachoffoursetsofcues:thoseforwhichbothmodelscorrectlypredictthefirstassociate,thoseforwhichonlythetopicmodelpredictsthefirstassociate,thoseforwhichonlyLSApredictsthefirstassociate,andthoseforwhichneithermodelpredictsthefirstassociate.Theseexam-pleshelptoillustratehowthetwomodelssometimesfail.Forexample,LSAsometimeslatchesontothewrongsenseofaword,aswithpen,andtendstogivehighscorestoinappropriatelow-frequencywords,suchaswhale,comma,andmildew.Bothmodelssometimespickoutcorrelationsbetweenwordsthatdonotoccurforreasonshavingtodowiththemeaningofthosewords:buckandbumblebothoccurwithdestructioninasingledocument,whichissufficientfortheselow-frequencywordstobecomeassociated.Insomecases,aswithrice,themostsalientpropertiesofanobjectarenotthosethatarereflectedinitsuse,andthemodelsfaildespiteproducingmeaningful,semanticallyrelatedpredictions.QualitativePropertiesofWordAssociationQuantitativemeasuressuchasthoseshowninFigure8provideasimplemeansofsummarizingtheperformanceofthetwomod-els.However,theymasksomeofthedeeperqualitativedifferencesthatresultfromusingdifferentkindsofrepresentations.Tversky(1977;Tversky&Gati,1982;Tversky&Hutchinson,1986)arguedagainstdefiningthesimilaritybetweentwostimuliintermsofthedistancebetweenthosestimuliinaninternalizedspatialrepresentation.Tversky’sargumentwasfoundedonviolationsofthemetricaxioms—formalprinciplesthatholdforalldistancemeasures,whicharealsoknownasmetrics—insimilarityjudg-Figure8.Performanceoflatentsemanticanalysisandthetopicmodelinpredictingwordassociation.(a)Themedianranksofthefirstfiveempiricalassociatesintheorderingpredictedbydifferentmeasuresofsemanticassociationatdifferentdimensionalities.Smallerranksindicatebetterperformance.Thedottedlineshowsbaselineperformance,correspondingtotheuseoftherawfrequencieswithwhichwordsoccurinthesamedocuments.(b)Theprobabilitythatasetcontainingthemhighestrankedwordsunderthedifferentmeasureswouldcontainthefirstempiricalassociate,withplotmarkerscorrespondingtom⫽1,5,10,25,50,100.Theresultsforthecosineandinnerproductarethebestresultsobtainedoverallchoicesofbetween100and700dimensions,whereastheresultsforthetopicmodelusejustthe1,700-topicsolution.Thedottedlineisbaselineperformancederivedfromco-occurrencefrequency.222GRIFFITHS,STEYVERS,ANDTENENBAUMments.Specifically,similarity(a)canbeasymmetric,becausethesimilarityofxtoycandifferfromthesimilarityofytox;(b)violatesthetriangleinequality,becausexcanbesimilartoyandytozwithoutxbeingsimilartoz;and(c)showsaneighborhoodstructureinconsistentwiththeconstraintsimposedbyspatialrep-resentations.Tverskyconcludedthatconceptualstimuliarebetterrepresentedintermsofsetsoffeatures.Tversky’sargumentsabouttheadequacyofspacesandfeaturesforcapturingthesimilaritybetweenconceptualstimulihavedirectrelevancetotheinvestigationofsemanticrepresentation.Wordsareconceptualstimuli,andLSAassumesthatwordscanberep-resentedaspointsinaspace.Thecosine,thestandardmeasureofassociationusedinLSA,isamonotonicfunctionoftheanglebetweentwovectorsinahigh-dimensionalspace.Theanglebe-tweentwovectorsisametric,satisfyingthemetricaxiomsofbeingzeroforidenticalvectors,beingsymmetric,andobeyingthetriangleinequality.Consequently,thecosineexhibitsmanyoftheconstraintsofametric.Thetopicmodeldoesnotsufferfromthesameconstraints.Infact,thetopicmodelcanbethoughtofasprovidingafeature-basedrepresentationforthemeaningofwords,withthetopicsunderwhichawordhashighprobabilitybeingitsfeatures.InAppendixB,weshowthatthereisactuallyaformalcorrespondencebetweenevaluatingP(w2|w1)usingEquation9andcomputingsimilarityinoneofTversky’s(1977)feature-basedmodels.Theassociationbetweentwowordsisincreasedbyeachtopicthatassignshighprobabilitytobothandisdecreasedbytopicsthatassignhighprobabilitytoonebutnottheother,inthesamewaythatTverksyclaimedcommonanddistinctivefeaturesshouldaffectsimilarity.ThetwomodelswehavebeenconsideringthuscorrespondtothetwokindsofrepresentationconsideredbyTversky.WordassociationalsoexhibitsphenomenathatparallelTversky’sanal-ysesofsimilarity,beinginconsistentwiththemetricaxioms.Wewilldiscussthreequalitativephenomenaofwordassociation—effectsofwordfrequency,violationofthetriangleinequality,andthelarge-scalestructureofsemanticnetworks—connectingthesephenomenatothenotionsusedinTversky’s(1977;Tversky&Gati,1982;Tversky&Hutchinson,1986)critiqueofspatialrep-resentations.WewillshowthatLSAcannotexplainthesephe-nomena(atleastwhenthecosineisusedasthemeasureofsemanticassociation),owingtotheconstraintsthatarisefromtheuseofdistances,butthatthesephenomenaemergenaturallywhenwordsarerepresentedusingtopics,justastheycanbeproducedusingfeature-basedrepresentationsforsimilarity.Asymmetriesandwordfrequency.Theasymmetryofsimilar-ityjudgmentswasoneofTversky’s(1977)objectionstotheuseofspatialrepresentationsforsimilarity.Bydefinition,anymetricdmustbesymmetric:d(x,y)⫽d(y,x).Ifsimilarityisafunctionofdistance,similarityshouldalsobesymmetric.However,itispossibletofindstimuliforwhichpeopleproduceasymmetricsimilarityjudgments.OneclassicexampleinvolvesChinaandNorthKorea:PeopletypicallyhavetheintuitionthatNorthKoreaismoresimilartoChinathanChinaistoNorthKorea.Tversky’sexplanationforthisphenomenonappealedtothedistributionoffeaturesacrosstheseobjects:People’srepresentationofChinainvolvesalargenumberoffeatures,onlysomeofwhicharesharedwithNorthKorea,whereastheirrepresentationofNorthKoreainvolvesasmallnumberoffeatures,manyofwhicharesharedwithChina.Wordfrequencyisanimportantdeterminantofwhetherawordwillbenamedasanassociate.Onecanseethisbylookingforasymmetricassociations:pairsofwordsw1,w2inwhichonewordisnamedasanassociateoftheothermuchmoreoftenthanviceversa(i.e.,eitherP(w2|w1)P(w1|w2)orP(w1|w2)P(w2|w1)).OnecanthenevaluatetheeffectofwordfrequencybyexaminingtheextenttowhichtheobservedasymmetriescanbeaccountedforbyFigure9.Actualandpredictedassociatesforasubsetofcues.Twocueswererandomlyselectedfromthesetsofcuesforwhich(fromlefttoright)bothmodelscorrectlypredictedthefirstassociate,onlythetopicmodelmadethecorrectprediction,onlylatentsemanticanalysis(LSA)madethecorrectprediction,andneithermodelmadethecorrectprediction.Eachcolumnliststhecue,humanassociates,predictionsofthetopicmodel,andpredictionsofLSA,presentingthefirstfivewordsinorder.TherankofthefirstassociateisgiveninparenthesesbelowthepredictionsofthetopicmodelandLSA.223TOPICSINSEMANTICREPRESENTATIONthefrequenciesofthewordsinvolved.Wedefinedtwowordsw1,w2tobeassociatedifonewordwasnamedasanassociateoftheotheratleastonce(i.e.,eitherP(w2|w1)orP(w1|w2)⬎0)andassessedasymmetriesinassociationbycomputingtheratioofcue–associateprobabilitiesforallassociatedwords,P(w2兩w1)P(w1兩w2).Ofthe45,063pairsofassociatedwordsinoursubsetofthenorms,38,744(85.98%)hadratiosindicatingadifferenceinprobabilityofatleastanorderofmagnitudeasafunctionofdirectionofassociation.Goodexamplesofasymmetricpairsincludekeg–beer,text–book,trousers–pants,meow–cat,andcobra–snake.Ineachofthesecases,thefirstwordelicitsthesecondasanassociatewithhighprobability,whereasthesecondisunlikelytoelicitthefirst.Ofthe38,744asymmetricassociations,30,743(79.35%)couldbeaccountedforbythefrequenciesofthewordsinvolved,withthehigherfrequencywordbeingnamedasanassociatemoreoften.LSAdoesnotpredictwordfrequencyeffects,includingasym-metriesinassociation.Thecosineisusedasameasureofthesemanticassociationbetweentwowordspartlybecauseitcoun-teractstheeffectofwordfrequency.Thecosineisalsoinherentlysymmetric,ascanbeseenfromEquation1:cos(w1,w2)⫽cos(w2,w1)forallwordsw1,w2.Thissymmetrymeansthatthemodelcannotpredictasymmetriesinwordassociationwithoutadoptingamorecomplexmeasureoftheassociationbetweenwords(cf.Krumhansl,1978;Nosofsky,1991).Incontrast,thetopicmodelcanpredicttheeffectoffrequencyonwordassociation.WordfrequencyisoneofthefactorsthatcontributetoP(w2|w1).Themodelcanaccountfortheasymmetriesintheword-associationnorms.Asaconditionalprobability,P(w2|w1)isinherentlyasym-metric,andthemodelcorrectlypredictedthedirectionof30,905(79.77%)ofthe38,744asymmetricassociations,includingalloftheexamplesgivenabove.Thetopicmodelthusaccountedforalmostexactlythesameproportionofasymmetriesaswordfre-quency—thedifferencewasnotstatisticallysignificant,␹2(1,77488)⫽2.08,p⫽.149.TheexplanationforasymmetriesinwordassociationprovidedbythetopicmodelisextremelysimilartoTversky’s(1977)explanationforasymmetriesinsimilarityjudgments.FollowingEquation9,P(w2|w1)reflectstheextenttowhichthetopicsinwhichw1appearsgivehighprobabilitytotopicw2.High-frequencywordstendtoappearinmoretopicsthanlow-frequencywords.Ifwhisahigh-frequencywordandwlisalow-frequencyword,whislikelytoappearinmanyofthetopicsinwhichwlappears,butwlwillappearinonlyafewofthetopicsinwhichwhappears.Consequently,P(wh|wl)willbelarge,butP(wl|wh)willbesmall.Violationofthetriangleinequality.Thetriangleinequalityisanotherofthemetricaxioms:Forametricd,d(x,z)ⱕd(x,y)⫹d(y,z).Thisisreferredtoasthetriangleinequalitybecauseifx,y,andzareinterpretedaspointscomposingatriangle,theequationindicatesthatnosideofthattrianglecanbelongerthanthesumoftheothertwosides.Thisinequalityplacesstrongconstraintsondistancemeasuresandstrongconstraintsonthelocationsofpointsinaspacegivenasetofdistances.Ifsimilarityisassumedtobeamonotonicallydecreasingfunctionofdistance,thenthisinequalitytranslatesintoaconstraintonsimilarityrelations:Ifxissimilartoyandyissimilartoz,thenxmustbesimilartoz.TverskyandGati(1982)providedseveralexamplesinwhichthisrelationshipdoesnothold.Theseexamplestypicallyinvolveshiftingthefeaturesonwhichsimilarityisassessed.Forinstance,takinganexamplefromJames(1890),agasjetissimilartothemoon,asbothcastlight,andthemoonissimilartoaball,becauseofitsshape,butagasjetisnotatallsimilartoaball.Wordassociationviolatesthetriangleinequality.AtriangleinequalityinassociationwouldmeanthatifP(w2|w1)ishighandP(w3|w2)ishigh,thenP(w3|w1)mustbehigh.Itiseasytofindsetsofwordsthatareinconsistentwiththisconstraint.Forexample,asteroidishighlyassociatedwithbelt,andbeltishighlyassociatedwithbuckle,butasteroidandbucklehavelittleassociation.Suchcasesaretheruleratherthantheexcep-tion,asshowninFigure10a.Eachofthehistogramsshowninthefigurewasproducedbyselectingallsetsofthreewordsw1,w2,w3suchthatP(w2|w1)andP(w3|w2)weregreaterthansomethreshold␶andthencomputingthedistributionofP(w3|w1).Regardlessofthevalueof␶,thereexistagreatmanytriplesinwhichw1andw3aresoweaklyassociatedasnottobenamedinthenorms.LSAcannotexplainviolationsofthetriangleinequality.Asamonotonicfunctionoftheanglebetweentwovectors,thecosineobeysananalogueofthetriangleinequality.Giventhreevectorsw1,w2,andw3,theanglebetweenw1andw3mustbelessthanorequaltothesumoftheanglebetweenw1andw2andtheanglebetweenw2andw3.Consequently,cos(w1,w3)mustbegreaterthanthecosineofthesumofthew1–w2andw2–w3angles.Usingthetrigonometricexpressionforthecosineofthesumoftwoangles,weobtaintheinequalitycos共w1,w3)ⱖcos共w1,w2)cos共w2,w3)⫺sin共w1,w2)sin共w2,w3),wheresin(w1,w2)canbedefinedanalogouslytoEquation1.Thisinequalityrestrictsthepossiblerelationshipsbetweenthreewords:Ifw1andw2arehighlyassociatedandw2andw3arehighlyassociated,thenw1andw3mustbehighlyassociated.Figure10bshowshowthetriangleinequalitymanifestsinLSA.Highvaluesofcos(w1,w2)andcos(w2,w3)inducehighvaluesofcos(w1,w3).TheimplicationsofthetriangleinequalityaremadeexplicitinFigure10d:Evenforthelowestchoiceofthreshold,theminimumvalueofcos(w1,w3)wasabovethe97thpercentileofcosinesbetweenallwordsinthecorpus.TheexpressionofthetriangleinequalityinLSAissubtle.Itishardtofindtriplesforwhichahighvalueofcos(w1,w2)andcos(w2,w3)induceahighvalueofcos(w1,w3),althoughasteroid–belt–buckleisonesuchexample:Ofthe4,470wordsinthenorms(excludingself-associations),belthasthe13thhighestcosinewithasteroid,bucklehasthe2ndhighestcosinewithbelt,andconse-quentlybucklehasthe41sthighestcosinewithasteroid,higherthantail,impact,orshower.Theconstraintistypicallyexpressednotbyinducingspuriousassociationsbetweenwordsbutbylo-catingwordsthatmightviolatethetriangleinequalitythataresufficientlyfarapartastobeunaffectedbythelimitationsitimposes.AsshowninFigure10b,thetheoreticallowerboundoncos(w1,w3)becomesanissueonlywhenbothcos(w1,w2)andcos(w2,w3)aregreaterthan.70.AsillustratedinFigure7,thetopicmodelnaturallyrecoversthemultiplesensesofpolysemousandhomonymouswords,placingthemindifferenttopics.Thismakesitpossibleforviolationsofthetriangleinequalitytooccur:Ifw1hashighprobabilityinTopic1butnotTopic2,w2hashighprobabilityinbothTopics1and2,224GRIFFITHS,STEYVERS,ANDTENENBAUMandw3hashighprobabilityinTopic2butnotTopic1,thenP(w2|w1)andP(w3|w2)canbequitehighwhereasP(w3|w1)stayslow.AnempiricaldemonstrationthatthisisthecaseforourderivedrepresentationisshowninFigure10c:LowvaluesofP(w3|w1)areobservedevenwhenP(w2|w1)andP(w3|w2)arebothhigh.AsshowninFigure10e,thepercentilerankoftheminimumvalueofP(w3|w1)startsverylowandincreasesfarmoreslowlythanthecosine.Predictingthestructureofsemanticnetworks.Word-associationdatacanbeusedtoconstructsemanticnetworks,withnodesrepresentingwordsandedgesrepresentinganonzeroprob-abilityofawordbeingnamedasanassociate.Thesemanticnetworksformedinthiswaycanbedirected,markingwhetheraparticularwordactedasacueoranassociateusingthedirectionofeachedge,orundirected,withanedgebetweenwordsregardlessofwhichactedasthecue.SteyversandTenenbaum(2005)ana-lyzedthelarge-scalepropertiesofbothdirectedandundirectedsemanticnetworksformedfromtheword-associationnormsofNelson,McEvoy,andSchreiber(1998),findingthattheyhavesomestatisticalpropertiesthatdistinguishthemfromclassicalrandomgraphs.Thepropertiesthatwefocusonherearescale-freedegreedistributionsandclustering.Ingraphtheory,the“degree”ofanodeisthenumberofedgesassociatedwiththatnode,equivalenttothenumberofneighbors.Foradirectedgraph,thedegreecandifferaccordingtothedirec-tionoftheedgesinvolved:Thein-degreeisthenumberofincom-ingedges,andtheout-degree,thenumberoutgoing.Byaggregat-ingacrossmanynodes,itispossibletofindthedegreedistributionforaparticulargraph.Researchonnetworksarisinginnaturehasfoundthatformanysuchnetworksthedegreekfollowsapower-lawdistribution,withP(k)⬃k–␥forsomeconstant␥.Suchadistributionisoftencalled“scalefree,”becausepower-lawdistri-butionsareinvariantwithrespecttomultiplicativechangesofthescale.Apower-lawdistributioncanberecognizedbyplottinglogτ = −1Cosineτ = 0.6τ = 0.7τ = 0.75τ = 0.8τ = 0.8500.51τ = 0.9cos( w1 , w3 )τ = 0Associationτ = 0.15τ = 0.2τ = 0.25τ = 0.35τ = 0.4500.5τ = 0.55P( w3 | w1 )τ = 0Topicsτ = 0.03τ = 0.04τ = 0.05τ = 0.06τ = 0.0800.5τ = 0.1P( w3 | w1 )τ = −∞Inner productτ = 1τ = 1.2τ = 1.4τ = 1.6τ = 1.9506τ = 2.15 w1T w31000010001001000.10.20.30.40.50.60.70.80.91Percentile rank of weakest w1 − w3 associationNumber of w1 − w2 − w3 triples above thresholdCosineLower bound fromtriangle inequalityInner productTopics(a)(b)(c)(d)(e)Figure10.Expressionofthetriangleinequalityinassociation,latentsemanticanalysis(LSA),andthetopicmodel.(a)Eachrowgivesthedistributionoftheassociationprobability,P(w3|w1),foratriplew1,w2,w3suchthatP(w2|w1)andP(w3|w2)arebothgreaterthan␶,withthevalueof␶increasingdownthecolumn.Irrespectiveofthechoiceof␶,thereremaincasesinwhichP(w3|w1)⫽0,suggestingviolationofthetriangleinequality.(b)QuitedifferentbehaviorisobtainedfromLSA,inwhichthetriangleinequalityenforcesalowerbound(shownwiththedottedline)onthevalueofcos(w1,w3)asaresultofthevaluesofcos(w2,w3)andcos(w1,w2).(c)Thetopicmodelshowsonlyaweakeffectofincreasing␶,(d)asdoestheinnerproductinLSA.In(a)–(d),thevalueof␶foreachplotwaschosentomakethenumberoftriplesabovethresholdapproximatelyequalacrosseachrow.(e)Thesignificanceofthechangeindistributioncanbeseenbyplottingthepercentilerankamongallwordpairsofthelowestvalueofcos(w1,w3)andP(w3|w1)asafunctionofthenumberoftriplesselectedbysomevalueof␶.Theplotmarkersshowthepercentilerankoftheleftmostvaluesappearinginthehistogramsin(b)–(d),fordifferentvaluesof␶.Theminimumvalueofcos(w1,w3)hasahighpercentilerankevenforthelowestvalueof␶,whileP(w3|w1)increasesmoregraduallyasafunctionof␶.Theminimuminnerproductremainslowforallvaluesof␶.225TOPICSINSEMANTICREPRESENTATIONP(k)againstlogk:IfP(k)⬃k–␥,thentheresultshouldbeastraightlinewithslope–␥.SteyversandTenenbaum(2005)foundthatsemanticnetworksconstructedfromword-associationdatahavepower-lawdegreedistributions.WereproducedtheiranalysesforoursubsetofNelson,McEvoy,andSchreiber’s(1998)norms,computingthedegreeofeachwordforbothdirectedandundirectedgraphsconstructedfromthenorms.ThedegreedistributionsareshowninFigure11.Inthedirectedgraph,theout-degree(thenumberofassociatesforeachcue)followsadistributionthatisunimodalandexponentialtailed,butthein-degree(thenumberofcuesforwhichawordisanassociate)followsapower-lawdistribution,indicatedbythelinearityoflogP(k)asafunctionoflogk.Thisrelationshipinducesapower-lawdegreedistributionintheundirectedgraph.Wecomputedthreesummarystatisticsforthesetwopower-lawdistributions:themeandegree,k៮,thestandarddeviationofk,sk,andthebestfittingpower-lawexponent,␥.Themeandegreeservestodescribetheoveralldensityofthegraph,andskand␥aremeasuresoftherateatwhichP(k)fallsoffaskbecomeslarge.IfP(k)isstronglypositivelyskewed,asitshouldbeforapower-lawdistribution,thenskwillbelarge.Therelationshipbetween␥andP(k)ispreciselytheopposite,withlargevaluesof␥indicatingarapiddeclineinP(k)asafunctionofk.ThevaluesofthesesummarystatisticsaregiveninTable1.Thedegreedistributioncharacterizesthenumberofneighborsforanygivennode.Asecondpropertyofsemanticnetworks,clustering,describestherelationshipsthatholdamongthoseneigh-bors.Semanticnetworkstendtocontainfarmoreclustersofdenselyinterconnectednodesthanwouldbeexpectedtoariseifedgesweresimplyaddedbetweennodesatrandom.Astandardmeasureofclustering(Watts&Strogatz,1998)istheclusteringcoefficient,C៮,themeanproportionoftheneighborsofanodethatarealsoneighborsofoneanother.Foranynodew,thisproportionisCw⫽Tw(2kw)⫽2Twkw(kw⫺1),whereTwisthenumberofneighborsofwthatareneighborsofoneanotherandkwisthenumberofneighborsofw.Ifanodehasnoneighbors,Cwisdefinedtobe1.Theclusteringcoefficient,C៮,iscomputedbyaveragingCwoverallwordsw.Inagraphformedfromword-associationdata,theclusteringcoefficientindicatestheproportionoftheassociatesofawordthatarethemselvesassoci-ated.SteyversandTenenbaum(2005)foundthattheclusteringcoefficientofsemanticnetworksisfargreaterthanthatofarandomgraph.TheclusteringproportionsCwhavebeenfoundtobeusefulinpredictingvariousphenomenainhumanmemory,includingcuedrecall(Nelson,McKinney,etal.,1998),recognition(Nelsonetal.,2001),andprimingeffects(Nelson&Goodmon,2002),althoughthisquantityistypicallyreferredtoasthe“con-nectivity”ofaword.Power-lawdegreedistributionsinsemanticnetworksaresignif-icantbecausetheyindicatethatsomewordshaveextremelylargenumbersofneighbors.Inparticular,thepowerlawinin-degreeindicatesthatthereareasmallnumberofwordsthatappearasassociatesforagreatvarietyofcues.AsSteyversandTenenbaum(2005)pointedout,thiskindofphenomenonisdifficulttorepro-duceinaspatialrepresentation.ThiscanbedemonstratedbyattemptingtoconstructtheequivalentgraphusingLSA.Because0204000.050.10.150.20.250.3Number of associatesProbabilityAssociation10010110210310−5100ProbabilityNumber of cuesAssociationγ=2.0280204000.050.10.150.20.250.3Number of associatesTopics10010110210310−5100Number of cuesTopicsγ=1.94810010110210310−5100 k P( k )Associationγ=2.99910010110210310−5100 kCosineγ=1.97110010110210310−5100 k Inner productγ=1.17610010110210310−5100 k Topicsγ=2.746 )d( )c( )b( )a( )h( )g( )f( )e(Figure11.Degreedistributionsforsemanticnetworks.(a)Thepower-lawdegreedistributionfortheundi-rectedgraph,shownasalinearfunctiononlog–logcoordinates.(b)–(c)NeitherthecosinenortheinnerproductinLSAproducestheappropriatedegreedistribution.(d)Thetopicmodelproducesapowerlawwiththeappropriateexponent.(e)Inthedirectedgraph,theout-degreeisunimodalandexponentialtailed.(f)Thetopicmodelproducesasimilardistribution.(g)Thein-degreedistributionforthedirectedgraphispowerlaw.(h)Thetopicmodelalsoprovidesaclosematchtothisdistribution.226GRIFFITHS,STEYVERS,ANDTENENBAUMthecosineissymmetric,thesimpleapproachofconnectingeachwordw1toallwordsw2suchthatcos(w1,w2)⬎␶forsomethreshold␶resultsinanundirectedgraph.Weusedthisproceduretoconstructagraphwiththesamedensityastheundirectedword-associationgraphandsubjectedittothesameanalyses.TheresultsoftheseanalysesarepresentedinTable1.ThedegreeofindividualnodesintheLSAgraphisweaklycorrelatedwiththedegreeofnodesintheassociationgraph(␳⫽.104).However,wordfrequencyisafarbetterpredictorofdegree(␳⫽.530).Furthermore,theformofthedegreedistributionisincorrect,asisshowninFigure11.Thedegreedistributionresultingfromusingthecosineinitiallyfallsoffmuchmoreslowlythanapower-lawdistribution,resultingintheestimate␥⫽1.972,lowerthantheobservedvalueof2.999,andthenfallsoffmorerapidly,resultinginavalueofskof14.51,lowerthantheobservedvalueof18.08.Similarresultsareobtainedwithotherchoicesofdimensionality,andSteyversandTenenbaumfoundthatseveralmoreelaboratemethodsofconstructinggraphs(bothdirectedandundirected)fromLSAwerealsounabletoproducetheappropriatedegreedistribution.Althoughtheyexhibitadifferentdegreedistributionfromse-manticnetworksconstructedfromassociationdata,graphscon-structedbythresholdingthecosineseemtoexhibittheappropriateamountofclustering.WefoundCwforeachofthewordsinoursubsetoftheword-associationnormsandusedthesetocomputetheclusteringcoefficientC៮.WeperformedthesameanalysisonthegraphconstructedusingLSAandfoundasimilarbutslightlyhigherclusteringcoefficient.However,LSAdiffersfromtheas-sociationnormsinpredictingwhichwordsshouldbelongtoclus-ters:TheclusteringproportionsforeachwordintheLSAgraphareonlyweaklycorrelatedwiththecorrespondingquantitiesintheword-associationgraph,␳⫽.146.Again,wordfrequencyisabetterpredictorofclusteringproportion,with␳⫽–.462.TheneighborhoodstructureofLSAseemstobeinconsistentwiththepropertiesofwordassociation.ThisresultisreminiscentofTverskyandHutchinson’s(1986)analysisoftheconstraintsthatspatialrepresentationsplaceontheconfigurationsofpointsinlow-dimensionalspaces.ThemajorconcernofTverskyandHutchinsonwastheneighborhoodrelationsthatcouldholdamongasetofpoints,andspecificallythenumberofpointstowhichapointcouldbethenearestneighbor.Inlow-dimensionalspaces,thisquantityisheavilyrestricted:Inonedimension,apointcanbethenearestneighborofonlytwoothers;intwodimensions,itcanbethenearestneighboroffive.Thisconstraintseemedtobeatoddswiththekindsofstructurethatcanbeexpressedbyconcep-tualstimuli.OneoftheexamplesconsideredbyTverskyandHutchinsonwashierarchicalstructure:Itseemsthatapple,orange,andbananashouldallbeextremelysimilartotheabstractnotionoffruit,yetinalow-dimensionalspatialrepresentation,fruitcanbethenearestneighborofonlyasmallsetofpoints.Inwordasso-ciation,power-lawdegreedistributionsmeanthatafewwordsneedtobeneighborsofalargenumberofotherwords,somethingthatisdifficulttoproduceeveninhigh-dimensionalspatialrepre-sentations.Semanticnetworksconstructedfromthepredictionsofthetopicmodelprovideabettermatchtothosederivedfromword-associationdata.TheasymmetryofP(w2|w1)makesitpossibletoconstructbothdirectedandundirectedsemanticnetworksbythresholdingtheconditionalprobabilityofassociatesgivencues.Weconstructeddirectedandundirectedgraphsbychoosingthethresholdtomatchthedensity,k៮,ofthesemanticnetworkformedfromassociationdata.Thesemanticnetworksproducedbythetopicmodelwereextremelyconsistentwiththesemanticnetworksderivedfromwordassociation,withthestatisticsgiveninTable1.AsshowninFigure11a,thedegreedistributionfortheundi-rectedgraphwaspowerlawwithanexponentof␥⫽2.746andastandarddeviationofsk⫽21.36,providingaclosermatchtothetruedistributionthanLSA.Furthermore,thedegreeofindividualnodesinthesemanticnetworkformedbythresholdingP(w2|w1)correlatedwellwiththedegreeofnodesinthesemanticnetworkformedfromtheword-associationdata,␳⫽.487.Theclusteringcoefficientwasclosetothatofthetruegraph,C៮⫽.303,andtheclusteringproportionsofindividualnodeswerealsowellcorre-latedacrossthetwographs,␳⫽.396.Forthedirectedgraph,thetopicmodelproducedappropriatedistributionsforboththeout-degree(thenumberofassociatespercue)andthein-degree(thenumberofcuesforwhichawordisanassociate),asshowninFigure11b.Thein-degreedistributionwaspowerlaw,withanexponentof␥⫽1.948andastandarddevia-tionofsk⫽21.65,bothbeingclosetothetruevalues.TheTable1StructuralStatisticsandCorrelationsforSemanticNetworksMeasureUndirected(k៮⫽20.16)Directed(k៮⫽11.67)AssociationCosineInnerproductTopicsAssociationTopicsStatisticssk18.0814.5133.7721.3618.7221.65␥2.9991.9721.1762.7462.0281.948C៮.187.267.625.303.187.308L៮3.0923.6532.9393.1574.2984.277Correlationsk(.530).104.465.487(.582).606C(⫺.462).146.417.396(⫺.462).391Note.k៮andskarethemeanandstandarddeviationofthedegreedistribution,␥isthepower-lawexponent,C៮isthemeanclusteringcoefficient,andL៮isthemeanlengthoftheshortestpathbetweenpairsofwords.Correlationsinparenthesesshowtheresultsofusingwordfrequencyasapredictor.227TOPICSINSEMANTICREPRESENTATIONclusteringcoefficientwassimilarbutslightlyhigherthanthedata,C៮⫽.308,andthepredictedin-degreeandclusteringproportionsofindividualnodescorrelatedwellwiththosefortheassociationgraph,␳⫽.606and␳⫽.391,respectively.InnerProductsasanAlternativeMeasureofAssociationInouranalysessofar,wehavefocusedonthecosineasameasureofsemanticassociationinLSA,consistentwiththevastmajorityofusesofthemodel.However,inafewapplications,ithasbeenfoundthattheunnormalizedinnerproductgivesbetterpredictions(e.g.,Rehderetal.,1998).Althoughitissymmetric,theinnerproductdoesnotobeyatriangleinequalityorhaveeasilydefinedconstraintsonneighborhoodrelations.WecomputedtheinnerproductsbetweenallpairsofwordsfromourderivedLSArepresentationsandappliedtheprocedureusedtotestthecosineandthetopicmodel.Wefoundthattheinnerproductgavebetterquantitativeperformancethanthecosine,butworsethanthetopicmodel,withamedianrankforthefirstassociateof28.Atotalof14.23%oftheempiricalfirstassociatesmatchedthewordwiththehighestinnerproduct.TheseresultsareshowninFigure8.Asistobeexpectedforameasurethatdoesnotobeythetriangleinequal-ity,therewaslittleeffectofthestrengthofassociationfor(w1,w2)pairsand(w2,w3)pairsonthestrengthofassociationfor(w1,w3)pairs,asshowninFigure10dand10e.Aswiththeothermodels,weconstructedasemanticnetworkbythresholdingtheinnerproduct,choosingthethresholdtomatchthedensityoftheassociationgraph.Theinnerproductdoespoorlyinreproducingtheneighborhoodstructureofwordassociation,pro-ducingadegreedistributionthatfallsofftooslowly(␥⫽1.176,sk⫽33.77)andanextremelyhighclusteringcoefficient(C៮⫽.625).However,itdoesreasonablywellinpredictingthedegree(␳⫽.465)andclusteringproportions(␳⫽.417)ofindividualnodes.Theexplanationforthispatternofresultsisthattheinnerproductisstronglyaffectedbywordfrequency,andthefrequencyofwordsisanimportantcomponentinpredictingassociations.However,theinnerproductgivestoomuchweighttowordfre-quencyinformingthesepredictions,andhigh-frequencywordsappearasassociatesforagreatmanycues.Thisresultsinthelowexponentandhighstandarddeviationofthedegreedistribution.ThetwomeasuresofsemanticassociationusedinLSArepresenttwoextremesintheiruseofwordfrequency:Thecosineisonlyweaklyaffectedbywordfrequency,whereastheinnerproductisstronglyaffected.Humansemanticmemoryissensitivetowordfrequency,butitssensitivityliesbetweentheseextremes.SummaryTheresultspresentedinthissectionprovideanaloguesinse-manticassociationtotheproblemsthatTversky(1977;Tversky&Gati,1982;Tversky&Hutchinson,1986)identifiedforspatialaccountsofsimilarity.Tversky’sargumentwasnotagainstspatialrepresentationspersebutagainsttheideathatsimilarityisamonotonicfunctionofametric,suchasdistanceinpsychologicalspace(cf.Shepard,1987).Eachofthephenomenahenoted—asymmetry,violationofthetriangleinequality,andneighborhoodstructure—couldbeproducedfromaspatialrepresentationunderasufficientlycreativeschemeforassessingsimilarity.Asymmetryprovidesanexcellentexample,asseveralmethodsforproducingasymmetriesfromspatialrepresentationshavealreadybeensug-gested(Krumhansl,1978;Nosofsky,1991).However,hisargu-mentshowsthatthedistancebetweentwopointsinpsychologicalspaceshouldnotbetakenasanabsolutemeasureofthesimilaritybetweentheobjectsthatcorrespondtothosepoints.Analogously,ourresultssuggestthatthecosine(whichiscloselyrelatedtoametric)shouldnotbetakenasanabsolutemeasureoftheassoci-ationbetweentwowords.Onewaytoaddresssomeoftheproblemsthatwehavehigh-lightedinthissectionmaybetousespatialrepresentationsinwhicheachwordisrepresentedasmultiplepointsratherthanasinglepoint.Thisisthestrategytakeninmanyconnectionistmodelsofsemanticrepresentation(e.g.,Kawamoto,1993;Plaut,1997;Roddetal.,2004),inwhichdifferentpointsinspaceareusedtorepresentdifferentmeaningsorsensesofwords.However,typicallytheserepresentationsarelearnednotfromtextbutfromdataconsistingoflabeledpairsofwordsandtheirmeanings.Automaticallyextractingsucharepresentationfromtextwouldinvolvesomesignificantcomputationalchallenges,suchasdecid-inghowmanysenseseachwordshouldhaveandwhenthosesensesarebeingused.ThefactthattheinnerproductdoesnotexhibitsomeoftheproblemsweidentifiedwiththecosinereinforcesthefactthattheissueisnotwiththeinformationextractedbyLSAbutwithusingameasureofsemanticassociationthatisrelatedtoametric.TheinnerproductinLSAhasaninterestingprobabilisticinterpretationthatexplainswhyitshouldbesostronglyaffectedbywordfrequency.Underweakassumptionsaboutthepropertiesofacorpus,itcanbeshownthattheinnerproductbetweentwowordvectorsisapproximatelyproportionaltoasmoothedversionofthejointprobabilityofthosetwowords(Griffiths&Steyvers,2003).Wordfrequencywillbeamajordeterminantofthisjointproba-bility,andhenceithasastronginfluenceontheinnerproduct.Thisanalysissuggeststhatalthoughtheinnerproductprovidesameansofmeasuringsemanticassociationthatisnominallydefinedintermsofanunderlyingsemanticspace,muchofitssuccessmayactuallybeaconsequenceofapproximatingaprobability.ThetopicmodelprovidesanalternativetoLSAthatautomati-callysolvestheproblemofunderstandingthedifferentsensesinwhichawordmightbeused,andgivesanaturalprobabilisticmeasureofassociationthatisnotsubjecttotheconstraintsofametric.Itgivesmoreaccuratequantitativepredictionsofword-associationdatathanusingeitherthecosineortheinnerproductintherepresentationextractedbyLSA.Italsoproducespredictionsthatareconsistentwiththequalitativepropertiesofsemanticassociationthatareproblematicforspatialrepresentations.Intheremainderofthearticle,weconsidersomefurtherapplicationsofthismodel,includingothercomparisonswithLSA,andhowitcanbeextendedtoaccommodatemorecomplexsemanticandsyntac-ticstructures.FurtherApplicationsOuranalysisofwordassociationprovidedanin-depthexplora-tionofthedifferencesbetweenLSAandthetopicmodel.How-ever,thesemodelsareintendedtoprovideanaccountofabroadrangeofempiricaldata,collectedthroughavarietyoftasksthattaptherepresentationsusedinprocessinglanguage.Inthissection,wepresentaseriesofexamplesofapplicationsofthesemodelsto228GRIFFITHS,STEYVERS,ANDTENENBAUMothertasks.TheseexamplesshowthatthetopicmodelreproducesmanyofthephenomenathatwereoriginallyusedtosupportLSA,provideabroaderbasisforcomparisonbetweenthetwomodels,andillustratehowtherepresentationextractedbythetopicmodelcanbeusedinothersettings.SynonymTestsOneoftheoriginalapplicationsofLSAwastothesynonymstaskoftheTestofEnglishasaForeignLanguage(TOEFL),usedtoassessfluencyinEnglishfornonnativespeakers(Landauer&Dumais,1997).Toallowdirectcomparisonbetweenthepredic-tionsofLSAandthetopicmodel,wereplicatedtheseresultsandevaluatedtheperformanceofthetopicmodelonthesametask.Thetestcontained90questions,consistingofaprobewordandfouranswers.Ouranalysesincludedonlyquestionsforwhichallfivewords(probeandanswers)wereinour26,243-wordvocab-ulary,resultinginasetof44questions.WeusedthesolutionsobtainedfromtheTASAcorpus,asdescribedintheprevioussection.ForLSA,wecomputedthecosineandinnerproductbetweenprobeandanswersforLSAsolutionswithbetween100and700dimensions.Forthetopicmodel,wecomputedP(wprobe|wanswer)andP(wanswer|wprobe)forbetween500and1,700topics,wherewprobeandwansweraretheprobeandanswerwords,respectively,andEquation8wasusedtocalculatetheconditionalprobabilities.Ourfirststepinevaluatingthemodelswastoexaminehowoftentheanswerthateachmodelidentifiedasbeingmostsimilartotheprobewasthecorrectanswer.LandauerandDumais(1997)reportedthatLSA(trainedontheTASAcorpusbutwithalargervocabularythanweusedhere)produced64.4%correctanswers,closetotheaverageof64.5%producedbycollegeapplicantsfromnon-English-speakingcountries.Ourresultsweresimilar:Thebestperformanceusingthecosinewaswithasolutionusing500dimensions,resultingin63.6%correctresponses.Therewerenosystematiceffectsofnumberofdimensionsandonlyasmallamountofvariation.Theinnerproductlikewiseproducedthebestperformancewith500dimensions,getting61.5%correct.ThetopicmodelperformedsimilarlytoLSAontheTOEFLtest:UsingP(wprobe|wanswer)toselectanswers,themodelproducedthebestperformancewith500topics,with70.5%correct.Again,therewasnosystematiceffectofnumberoftopics.SelectinganswersusingP(wanswer|wprobe)producedresultssimilartothecosineforLSA,withthebestperformancebeing63.6%correct,obtainedwith500topics.Thedifferencebetweenthesetwowaysofeval-uatingtheconditionalprobabilityliesinwhetherthefrequenciesofthepossibleanswersaretakenintoaccount.ComputingP(wprobe|wanswer)controlsforthefrequencywithwhichthewordswanswergenerallyoccurandisperhapsmoredesirableinthecontextofavocabularytest.Asafinaltestofthetwomodels,wecomputedthecorrelationbetweentheirpredictionsandtheactualfrequencieswithwhichpeopleselectedthedifferentresponses.FortheLSAsolutionwith500dimensions,themeancorrelationbetweenthecosineandresponsefrequencies(obtainedbyaveragingacrossitems)wasr⫽.30,withr⫽.25fortheinnerproduct.Forthetopicmodelwith500topics,thecorrespondingcorrelationswerer⫽.46and.34forlogP(wprobe|wanswer)andlogP(wanswer|wprobe),respectively.Thus,thesemodelsproducedpredictionsthatwerenotjustcorrectbutcapturedsomeofthevariationinhumanjudgmentsonthistask.SemanticPrimingofDifferentWordMeaningsTilletal.(1988)examinedthetimecourseoftheprocessingofwordmeaningsusingaprimingstudyinwhichparticipantsreadsentencescontainingambiguouswordsandthenperformedalex-icaldecisiontask.Thesentenceswereconstructedtoprovidecontextualinformationaboutthemeaningoftheambiguousword.Forexample,twoofthesentencesusedinthestudywere1A.Thetownspeoplewereamazedtofindthatallofthebuildingshadcollapsedexceptthemint.Obviously,ithadbeenbuilttowithstandnaturaldisasters.1B.Thinkingoftheamountofgarlicinhisdinner,theguestaskedforamint.Hesoonfeltmorecomfortablesocializingwiththeothers.whichareintendedtopickoutthedifferentmeaningsofmint.Thetargetwordsusedinthelexicaldecisiontaskeithercorrespondedtothedifferentmeaningsoftheambiguousword(inthiscase,moneyandcandy)orwereinferentiallyrelatedtothecontentofthesentence(inthiscase,earthquakeandbreath).Thedelaybetweenthepresentationofthesentenceandthedecisiontaskwasvaried,makingitpossibletoexaminehowthetimecourseofprocessingaffectedthefacilitationoflexicaldecisions(i.e.,priming)fordifferentkindsoftargets.ThebasicresultreportedbyTilletal.(1988)wasthatbothofthemeaningsoftheambiguouswordandneitheroftheinferencetargetswereprimedwhentherewasashortdelaybetweensen-tencepresentationandlexicaldecision,andthattherewasasubsequentshifttofavortheappropriatemeaningandinferentiallyrelatedtargetwhenthedelaywasincreased.LandauerandDumais(1997)suggestedthatthiseffectcouldbeexplainedbyLSA,usingthecosinebetweentheambiguouswordandthetargetstomodelprimingatshortdelaysandthecosinebetweentheentiresentenceandthetargetstomodelprimingatlongdelays.TheyshowedthateffectssimilartothosereportedbyTilletal.emergedfromthisanalysis.WereproducedtheanalysisofLandauerandDumais(1997)usingtherepresentationsweextractedfromtheTASAcorpus.Ofthe28pairsofsentencesusedbyTilletal.(1988),therewere20forwhichtheambiguousprimesandallfourtargetwordsappearedinourvocabulary.Tosimulateprimingearlyinprocessing,wecomputedthecosineandinnerproductbetweentheprimesandthetargetwordsusingtherepresentationextractedbyLSA.Tosim-ulatepriminginthelaterstagesofprocessing,wecomputedthecosineandinnerproductbetweentheaveragevectorsforeachofthefullsentences(includingonlythosewordsthatappearedinourvocabulary)andthetargetwords.Thevaluesproducedbytheseanalyseswerethenaveragedoverall20pairs.Theresultsforthe700-dimensionalsolutionareshowninTable2(similarresultswereobtainedwithdifferentnumbersofdimensions).TheresultsofthisanalysisillustratethetrendsidentifiedbyLandauerandDumais(1997).Boththecosineandtheinnerproductgivereasonablyhighscorestothetwomeaningswhenjusttheprimeisused(relativetothedistributionsshowninFigure10)andshifttogivehigherscorestothemeaningandinferentiallyrelatedtargetappropriatetothesentencewhentheentiresentence229TOPICSINSEMANTICREPRESENTATIONisused.Toconfirmthatthetopicmodelmakessimilarpredictionsinthecontextofsemanticpriming,weusedthesameprocedurewiththetopic-basedrepresentation,computingtheconditionalprobabilitiesofthedifferenttargets(a)basedjustontheprimeand(b)basedontheentiresentences,thenaveragingthelogprobabil-itiesoverallpairsofsentences.Theresultsforthe1,700-topicsolutionareshowninTable2(similarresultswereobtainedwithdifferentnumbersoftopics).Thetopicmodelproducesthesametrends:Itinitiallygiveshighprobabilitytobothmeaningsandthenswitchestogivehighprobabilitiestothesentence-appropriatetargets.SensitivityofReadingTimetoFrequencyofMeaningsExaminingthetimethatpeopletaketoreadwordsandsentenceshasbeenoneofthemostwidelyusedmethodsforevaluatingthecontributionsofsemanticrepresentationtolinguisticprocessing.Inparticular,severalstudieshaveusedreadingtimetoexploretherepresentationofambiguouswords(e.g.,Duffy,Morris,&Rayner,1988;Rayner&Duffy,1986;Rayner&Frazier,1989).Develop-ingacompleteaccountofhowthekindofcontextualinformationwehavebeendiscussinginfluencesreadingtimeisbeyondthescopeofthisarticle.However,weusedthetopicmodeltopredicttheresultsofonesuchstudy,toprovideanillustrationofhowitcanbeappliedtoataskofthiskind.Serenoetal.(1992)conductedastudyinwhichtheeyemove-mentsofparticipantsweremonitoredwhiletheyreadsentencescontainingambiguouswords.Theseambiguouswordswerese-lectedtohaveonehighlydominantmeaning,butthesentencesestablishedacontextthatsupportedthesubordinatemeaning.Forexample,onesentencereadThedinnerpartywasproceedingsmoothlywhen,justasMarywasservingtheport,oneoftheguestshadaheartattack.wherethecontextsupportsthesubordinatemeaningofport.Theaimofthestudywastoestablishwhetherreadingtimeforambig-uouswordswasbetterexplainedbytheoverallfrequencywithwhichawordoccursinallofitsmeaningsorsensesorbythefrequencyofaparticularmeaning.Totestthis,theexperimentershadparticipantsreadsentencescontainingeithertheambiguousword,awordwithfrequencymatchedtothesubordinatesense(thelow-frequencycontrol),orawordwithfrequencymatchedtothedominantsense(thehigh-frequencycontrol).Forexample,thecontrolwordsforportwerevealandsoup,respectively.TheresultsaresummarizedinTable3:Ambiguouswordsusingtheirsubordinatemeaningwerereadmoreslowlythanwordswithafrequencycorrespondingtothedominantmeaningbutnotquiteasslowlyaswordsthatmatchedthefrequencyofthesubordinatemeaning.AsubsequentstudybySereno,O’Donnell,andRayner(2006,Experiment3)producedthesamepatternofresults.Readingtimestudiespresentanumberofchallengesforcom-putationalmodels.ThestudyofSerenoetal.(1992)isparticularlyconducivetomodeling,asallthreetargetwordsaresubstitutedintothesamesentenceframe,meaningthattheresultsarenotaffectedbysentencesdifferinginnumberofwords,bythevocab-ularyofthemodels,orbyotherfactorsthatintroduceadditionalvariance.However,tomodelthesedatawestillneedtomakeanassumptionaboutthefactorsinfluencingreadingtime.Theab-stractcomputational-levelanalysesprovidedbygenerativemodelsdonotmakeassertionsaboutthealgorithmicprocessesunderlyinghumancognitionandcanconsequentlybedifficulttotranslateintopredictionsabouttheamountoftimeitshouldtaketoperformaTable2PredictionsofModelsforSemanticPrimingTaskofTilletal.(1988)Model/ConditionMeaningA(e.g.,money)InferenceA(e.g.,earthquake)MeaningB(e.g.,candy)InferenceB(e.g.,breath)CosineEarly0.0990.0380.1350.028LateA0.0600.1030.0460.017LateB0.0500.0240.0670.046InnerproductEarly0.2080.0240.3420.017LateA0.0810.0390.0600.012LateB0.0600.0090.0660.024Topics(log10probability)Early⫺3.22⫺4.31⫺3.16⫺4.42LateA⫺4.03⫺4.13⫺4.58⫺4.77LateB⫺4.52⫺4.73⫺4.21⫺4.24Note.Theexamplesareforsentencescontainingtheambiguoustargetwordmint.Table3PredictionsofModelsforReadingTimeTaskofSerenoetal.(1992)MeasureAmbiguouswordLow-frequencycontrolHigh-frequencycontrolHumangazeduration(ms)281287257Cosine.021.048.043Innerproduct.011.010.025Topics(log10probability)⫺4.96⫺5.26⫺4.68230GRIFFITHS,STEYVERS,ANDTENENBAUMtask.Inthetopicmodel,thereareavarietyoffactorsthatcouldproduceanincreaseinthetimetakentoreadaparticularword.Somepossiblecandidatesincludeuncertaintyaboutthetopicofthesentence,asreflectedintheentropyofthedistributionovertopics;asuddenchangeinperceivedmeaning,producingadiffer-enceinthedistributionovertopicsbeforeandafterseeingtheword;orsimplyencounteringanunexpectedword,resultingingreatereffortforretrievingtherelevantinformationfrommemory.Wechosetouseonlythelastofthesemeasures,asitwasthesimplestandthemostdirectlyrelatedtoourconstrualofthecomputationalproblemunderlyinglinguisticprocessing,butwesuspectthatagoodmodelofreadingtimewouldneedtoincor-poratesomecombinationofallofthesefactors.Lettingwtargetbethetargetwordandwsentencebethesequenceofwordsinthesentencebeforetheoccurrenceofthetarget,wewanttocomputeP(wtarget|wsentence).ApplyingEquation8,wehaveP(wtarget兩wsentence)⫽冘zP(wtarget兩z)P(z兩wsentence),(10)whereP(z|wsentence)isthedistributionovertopicsencodingthegistofwsentence.Weusedthe1,700-topicsolutiontocomputethisquantityforthe21ofthe24sentencesusedbySerenoetal.(1992)forwhichallthreetargetwordsappearedinourvocabulary,andaveragedtheresultinglogprobabilitiesoverallsentences.TheresultsareshowninTable3.ThetopicmodelpredictstheresultsfoundbySerenoetal.(1992):Theambiguouswordsareassignedlowerprobabilitiesthanthehigh-frequencycontrols,althoughnotquiteaslowasthelow-frequencycontrols.ThemodelpredictsthiseffectbecausethedistributionovertopicsP(z|wsentence)favorsthosetopicsthatincorporatethesubordinatesense.Asaconse-quence,theprobabilityofthetargetwordisreduced,becauseP(wtarget|z)islowerforthosetopics.However,ifthereisanyuncertainty,providingsomeresidualprobabilitytotopicsinwhichthetargetwordoccursinitsdominantsense,theprobabilityoftheambiguouswordwillbeslightlyhigherthantherawfrequencyofthesubordinatesensesuggests.Forcomparison,wecomputedthecosineandinnerproductforthethreevaluesofwtargetandtheaveragevectorsforwsentenceinthe700-dimensionalLSAsolution.TheresultsareshowninTable3.Thecosinedoesnotpredictthiseffect,withthehighestmeancosinesbeingobtainedbythecontrolwords,withlittleeffectoffrequency.Thisisduetothefactthatthecosineisrelativelyinsensitivetowordfrequency,asdiscussedabove.Theinnerproduct,whichissensitivetowordfrequency,producespredictionsthatareconsistentwiththeresultsofSerenoetal.(1992).SemanticIntrusionsinFreeRecallWordassociationinvolvesmakinginferencesabouttheseman-ticrelationshipsamongapairofwords.Thetopicmodelcanalsobeusedtomakepredictionsabouttherelationshipsbetweenmul-tiplewords,asmightbeneededinepisodicmemorytasks.SinceBartlett(1932),manymemoryresearchershaveproposedthatepisodicmemorymightbebasednotonlyonspecificmemoryoftheexperiencedepisodesbutalsoonreconstructiveprocessesthatextracttheoverallthemeorgistofacollectionofexperiences.Oneprocedureforstudyinggist-basedmemoryistheDeese–Roediger–McDermott(DRM)paradigm(Deese,1959;Roediger&McDermott,1995).Inthisparadigm,participantsareinstructedtoremembershortlistsofwordsthatareallassociativelyrelatedtoasingleword(thecriticallure)thatisnotpresentedonthelist.Forexample,oneDRMlistconsistsofthewordsbed,rest,awake,tired,dream,wake,snooze,blanket,doze,slumber,snore,nap,peace,yawn,anddrowsy.Attest,61%ofsubjectsfalselyrecallthecriticalluresleep,whichisassociativelyrelatedtoallofthepresentedwords.Thetopicmodelmaybeabletoplayapartinatheoreticalaccountforthesesemanticintrusionsinepisodicmemory.Previ-oustheoreticalaccountsofsemanticintrusionshavebeenbasedondual-routemodelsofmemory.Thesemodelsdistinguishbetweendifferentroutestoretrieveinformationfrommemory:averbatimmemoryroute,basedonthephysicaloccurrenceofaninput,andagistmemoryroute,basedonsemanticcontent(e.g.,Brainerdetal.,1999,2002;Mandler,1980).Therepresentationofthegistortheprocessesinvolvedincomputingthegistitselfhavenotbeenspecifiedwithinthedual-routeframework.Computationalmodel-inginthisdomainhasbeenmostlyconcernedwiththeestimationoftherelativestrengthofdifferentmemoryrouteswithintheframeworkofmultinomialprocessingtreemodels(Batchelder&Riefer,1999).Thetopicmodelcanprovideamoreprecisetheoreticalaccountofgist-basedmemorybydetailingboththerepresentationofthegistandtheinferenceprocessesbasedonthegist.WecanmodeltheretrievalprobabilityofasinglewordattestonthebasisofasetofstudiedwordsbycomputingP(wrecall|wstudy).Withthetopicmodel,wecanuseEquation8toobtainP(wrecall兩wstudy)⫽冘zP(wrecall兩z)P(z兩wstudy).(11)ThegistofthestudylistisrepresentedbyP(z|wstudy),whichdescribesthedistributionovertopicsforagivenstudylist.IntheDRMparadigm,eachlistofwordswillleadtoadifferentdistri-butionovertopics.Listsofrelativelyunrelatedwordswillleadtoflatdistributionsovertopicswherenotopicisparticularlylikely,whereasmoresemanticallyfocusedlistswillleadtodistributionswhereonlyafewtopicsdominate.ThetermP(wrecall|z)capturestheretrievalprobabilityofwordsgiveneachoftheinferredtopics.Weobtainedpredictionsfromthismodelforthe55DRMlistsreportedbyRoedigeretal.(2001),usingthe1,700-topicsolutionderivedfromtheTASAcorpus.ThreeDRMlistswereexcludedbecausethecriticalitemswereabsentfromthevocabularyofthemodel.Oftheremaining52DRMlists,amedianof14outof15originalstudywordswereinourvocabulary.ForeachDRMlist,wecomputedtheretrievalprobabilityoverthewhole26,243-wordvocabulary,whichincludedthestudiedwordsaswellasextra-listwords.Forexample,Figure12showsthepredictedgist-basedretrievalprobabilitiesforthesleeplist.Theretrievalprobabilitiesareseparatedintotwolists:thewordsonthestudylistandthe8mostlikelyextra-listwords.Theresultsshowthatsleepisthemostlikelywordtoberetrieved,whichqualitativelyfitswiththeob-servedhighfalse-recallrateofthisword.Toassesstheperformanceofthetopicmodel,wecorrelatedtheretrievalprobabilityofthecriticalDRMwordsaspredictedbythetopicmodelwiththeobservedintrusionratesreportedbyRoedigeretal.(2001).Therank-ordercorrelationwas.437,witha95%231TOPICSINSEMANTICREPRESENTATIONconfidenceinterval(estimatedby1,000-samplebootstrap)of.217–.621.Wecomparedthisperformancewiththepredictionsofthe700-dimensionalLSAsolution.UsingLSA,thegistofthestudylistwasrepresentedbytheaverageofallwordvectorsfromthestudylist.WethencomputedthecosineofthecriticalDRMwordwiththeaveragewordvectorfortheDRMlistandcorrelatedthiscosinewiththeobservedintrusionrate.Thecorrelationwas.295,witha95%confidenceinterval(estimatedby1,000-samplebootstrap)of.041–.497.TheimprovementinpredictingsemanticintrusionsproducedbythetopicmodeloverLSAwasthusnotstatisticallysignificantbutsuggeststhatthetwomodelsmightbediscriminatedthroughfurtherexperiments.OneinterestingobservationfromFigure12isthatwordsthatdonotappearonthestudylist,suchassleep,canbegivenhigherprobabilitiesthanthewordsthatactuallydoappearonthelist.Becauseparticipantsinfree-recallstudiesgenerallydowellinretrievingtheitemsthatappearonthestudylist,thisillustratesthatthekindofgist-basedmemorythatthetopicmodelembodiesisnotsufficienttoaccountforbehavioronthistask.Thegist-basedretrievalprocesswouldhavetobecomplementedwithaverbatimretrievalprocessinordertoaccountfortherelativelyhighretrievalprobabilityforwordsonthestudylist,asassumedinthedual-routemodelsmentionedabove(Brainerdetal.,1999,2002;Mandler,1980).Theseissuescouldbeaddressedbyextendingthetopicmodeltotakeintoaccountthepossibleinteractionbetweenthegistandverbatimroutes.Meanings,Senses,andTopicsThetopicmodelassumesasimplestructuredrepresentationforwordsanddocuments,inwhichwordsareallocatedtoindividuallyinterpretabletopics.ThisrepresentationdiffersfromthatassumedbyLSA,inwhichthedimensionsarenotindividuallyinterpretableandthesimilaritybetweenwordsisinvariantwithrespecttorotationoftheaxes.Thetopic-basedrepresentationalsoprovidestheopportunitytoexplorequestionsaboutlanguagethatcannotbeposedusinglessstructuredrepresentations.Aswehaveseenalready,differenttopicscancapturedifferentmeaningsorsensesofaword.Asafinaltestofthetopicmodel,weexaminedhowwellthenumberoftopicsinwhichawordparticipatespredictsthenumberofmeaningsorsensesofthatwordandhowthisquantitycanbeusedinmodelingrecognitionmemory.Thenumberofmeaningsorsensesthatawordpossesseshasacharacteristicdistribution,aswasfirstnotedbyZipf(1965).Zipfexaminedthenumberofentriesthatappearedindictionarydefi-nitionsforwordsandfoundthatthisquantityfollowedapower-lawdistribution.SteyversandTenenbaum(2005)conductedsim-ilaranalysesusingRoget’sThesaurus(Roget,1911)andtheWordNetlexicaldatabase(Miller&Fellbaum,1998).Theyalsofoundthatthenumberofentriesfollowedapower-lawdistribu-tion,withanexponentof␥⬇3.Plotsofthesedistributionsinlog–logcoordinatesareshowninFigure13.ThenumberoftopicsinwhichawordappearsinthetopicmodelcorrespondswellwiththenumberofmeaningsorsensesofFigure13.Thedistributionofthenumberofcontextsinwhichawordcanappearhasacharacteristicform,whethercomputedfromthenumberofsensesinWordNet,thenumberofentriesinRoget’sThesaurus,orthenumberoftopicsinwhichawordappears.00.050.10.150.2Retrieval Probability Study ListBEDTIREDRESTWAKEAWAKENAPDREAMYAWNDROWSYBLANKETSNORESLUMBERDOZEPEACE00.050.10.150.2Retrieval Probability Extra ListSLEEPNIGHTASLEEPMORNINGHOURSSLEEPYEYESAWAKENEDFigure12.Retrievalprobabilities,P(wrecall|wstudy),forastudylistcon-tainingwordssemanticallyassociatedwithsleep.Theupperpanelshowstheprobabilitiesofeachofthewordsonthestudylist.Thelowerpanelshowstheprobabilitiesofthemostlikelyextra-listwords.sleephasahighretrievalprobabilityandwouldthusbelikelytobefalselyrecalled.232GRIFFITHS,STEYVERS,ANDTENENBAUMwordsasassessedusingRoget’sThesaurusandWordNet,bothindistributionandinthevaluesforindividualwords.Thedistributionofthemeannumberoftopicstowhichawordwasassignedinthe1,700-topicsolutionisshowninFigure13.4ThetailofthisdistributionmatchesthetailofthedistributionsobtainedfromRoget’sThesaurusandWordNet,withallthreedistributionsbeingpowerlawwithasimilarparameter.Furthermore,thenumberoftopicsinwhichawordappearsiscloselycorrelatedwiththeseothermeasures:Therank-ordercorrelationbetweennumberoftopicsandnumberofentriesinRoget’sThesaurusis␳⫽.328,witha95%confidenceinterval(estimatedby1,000-sampleboot-strap)of.300–.358,andthecorrelationbetweennumberoftopicsandWordNetsensesis␳⫽.508,witha95%confidenceintervalof.486–.531.Forcomparison,themostobviouspredictorofthenumberofmeaningsorsensesofaword—wordfrequency—givescorrelationsthatfallbelowtheseconfidenceintervals:Wordfre-quencypredictsRogetentrieswitharank-ordercorrelationof␳⫽.243andpredictsWordNetsenseswith␳⫽.431.MoredetailsofthefactorsaffectingthedistributionofthenumberoftopicsperwordaregiveninGriffithsandSteyvers(2002).CapturingContextVariabilityThenumberoftopicsinwhichawordappearsalsoprovidesanovelmeansofmeasuringanimportantpropertyofwords:contextvariability.Recentresearchinrecognitionmemoryhassuggestedthatthenumberofcontextsinwhichwordsappearmightexplainwhysomewordsaremorelikelythanotherstobeconfusedforitemsappearingonthestudylistinrecognitionmemoryexperiments(Den-nis&Humphreys,2001;McDonald&Shillcock,2001;Steyvers&Malmberg,2003).Theexplanationforthiseffectisthatwhenawordisencounteredinalargernumberofcontexts,thestudylistcontextbecomeslessdiscriminablefromthesepreviousexposures(Dennis&Humphreys,2001).SteyversandMalmberg(2003)operationallyde-finedcontextvariabilityasthenumberofdocumentsinwhichawordappearsinalargedatabaseoftext,ameasurewerefertoasdocumentfrequency.SteyversandMalmbergfoundthatthismeasurehasaneffectonrecognitionmemoryindependentofwordfrequency.Thedocumentfrequencymeasureisaroughproxyforcontextvariabilitybecauseitdoesnottaketheotherwordsoccurringindocumentsintoaccount.Theunderlyingassumptionisthatdocumentsareequallydifferentfromeachother.Consequently,iftherearemanydocumentsthatcoververysimilarsetsoftopics,thencontextvariabilitywillbeoverestimated.Thetopicmodelprovidesanalternativewaytoassesscontextvariability.Wordsthatareusedindifferentcontextstendtobeassociatedwithdifferenttopics.Therefore,wecanassesscontextvariabilitybythenumberofdifferenttopicsawordisassociatedwith,ameasurewerefertoastopicvariability.Unlikedocumentfrequency,thismeasuredoestakeintoaccountthesimilaritybetweendifferentdocumentsinevaluatingcontextvariability.Tounderstandhowtopicvariabilitycompareswithwordfre-quencyandcontextualvariability,weperformedanalysesonthedatafromtheexperimentbySteyversandMalmberg(2003).Therewere287distinctwordsintheexperiment,eachusedaseitheratargetoradistractor.Foreachwordwecomputedthesensitivity(d⬘),measuringthedegreetowhichsubjectscoulddistinguishthatwordasatargetordistractorintherecognitionmemoryexperi-ment.Table4showsthecorrelationsbetweend⬘andthethreemeasures:topicvariability,wordfrequency,anddocumentfre-quency.Allthreewordmeasureswerelogarithmicallyscaled.Theresultsshowthatwordfrequency,contextvariability,andtopicvariabilityallcorrelatewithrecognitionmemoryperfor-manceasexpected:Highwordfrequency,highdocumentfre-quency,andhightopicvariabilityareallassociatedwithpoorrecognitionmemoryperformance.Topicvariabilitycorrelatesmorestronglywithperformancethandotheothermeasures(p⬍.05)andisalsolesscorrelatedwiththeothermeasures.Thissuggeststhattopicvariabilityisagoodpredictivemeasureforrecognitionmemoryconfusabilityandisatleastasgoodapredic-toraswordfrequencyordocumentfrequencyandpotentiallyamoredirectmeasureofcontextvariability.SummaryTheresultspresentedinthissectionillustratethatthetopicmodelcanbeusedtopredictbehavioronavarietyoftasksrelatingtolinguisticprocessingandsemanticmemory.Themodelrepro-ducesmanyofthephenomenathathavebeenusedtosupportLSA,anditconsistentlyprovidesbetterperformancethanusingthecosineortheinnerproductbetweenwordvectorstomeasuresemanticassociation.Theformoftherepresentationextractedbythetopicmodelalsomakesitpossibletodefinenovelmeasuresofpropertiesofwordssuchasthenumberoftopicsinwhichtheyappear,whichseemstobeagoodguidetothenumberofsensesormeaningsofaword,aswellasaneffectivepredictorofrecognitionmemoryperformance.ExtendingtheGenerativeModelFormulatingtheproblemofextractingandusinggistintermsofgenerativemodelsallowedustoexploreanovelformofsemanticrepresentationthroughthetopicmodel.Thisformulationoftheproblemalsohasotheradvantages.Generativemodelsprovideaveryflexibleframeworkforspecifyingstructuredprobabilitydis-tributions,anditiseasytoextendthetopicmodeltoincorporatericherlatentstructurebyaddingfurtherstepstothegenerative4Asthenumberoftopicstowhichawordisassignedwillbeaffectedbythenumberoftopicsinthesolution,thesevaluescannotbetakenasrepresentingthenumberofmeaningsorsensesofaworddirectly.Asmentionedpreviously,thecorrespondencewillbemanytoone.Table4CorrelationsofRecognitionMemorySensitivities(d⬘)WithWordFrequency(WF),DocumentFrequency(DF),andTopicVariability(TV)Variabled⬘logWFlogDFlogWF⫺.50*—logDF⫺.58*.97*—logTV⫺.67*.69*.82*logTVlogWFa⫺.53*logTVlogDFa⫺.43*aPartialcorrelationswheretheeffectofthesecondvariableispartialedoutoftheeffectofthefirstvariable.*p⬍.0001.233TOPICSINSEMANTICREPRESENTATIONprocess.Wewilldiscussfiveextensionstothemodel:determiningthenumberoftopics,learningtopicsfromotherkindsofdata,incorporatingcollocations,inferringtopichierarchies,andinclud-ingrudimentarysyntax.LearningtheNumberofTopicsIntheprecedingdiscussion,weassumedthatthenumberoftopics,T,inthemodelwasfixed.Thisassumptionseemsinconsistentwiththedemandsofhumanlanguageprocessing,inwhichmoretopicsareintroducedwitheveryconversation.Fortunately,thisassumptionisnotnecessary.UsingmethodsfromnonparametricBayesianstatistics(Muller&Quintana,2004;Neal,2000),wecanassumethatourdataaregeneratedbyamodelwithanunboundednumberofdimensions,ofwhichonlyafinitesubsethavebeenobserved.Thebasicideabehindthesenonparametricapproachesistodefineapriorprobabilitydistributionontheassignmentsofwordstotopics,z,thatdoesnotassumeanupperboundonthenumberoftopics.Inferringthetopicassignmentsforthewordsthatappearinacorpussimultaneouslydeterminesthenumberoftopicsaswellastheircontent.Blei,Griffiths,Jordan,andTenenbaum(2004)andTeh,Jordan,Beal,andBlei(2004)haveappliedthisstrategytolearnthedimensionalityoftopicmodels.ThesemethodsarecloselyrelatedtotherationalmodelofcategorizationproposedbyAnderson(1990),whichrepresentscategoriesintermsofasetofclusters,withnewclustersaddedautomaticallyasmoredatabecomeavailable(seeNeal,2000).LearningTopicsFromOtherDataOurformulationofthebasictopicmodelalsoassumesthatwordsaredividedintodocumentsorotherwisebrokenupintounitsthatsharethesamegist.AsimilarassumptionismadebyLSA,butthisisnottrueofallmethodsforautomaticallyextractingsemanticrepre-sentationsfromtext(e.g.,Dennis,2004;Jones&Mewhort,2007;Lund&Burgess,1996).Thisassumptionisnotappropriateforallsettingsinwhichwemakelinguisticinferences:Althoughwemightdifferentiatethedocumentsweread,manyformsoflinguisticinter-action,suchasmeetingsorconversations,lackclearmarkersthatbreakthemupintosetsofwordswithacommongist.Oneapproachtothisproblemistodefineagenerativemodelinwhichthedocumentboundariesarealsolatentvariables,astrategypursuedbyPurver,Ko¨rding,Griffiths,andTenenbaum(2006).Alternatively,meetingsorconversationsmightbebettermodeledbyassociatingthegistofasetofwordswiththepersonwhouttersthosewordsratherthansimplygroupingwordsintemporalproximity.Rosen-Zvi,Griffiths,Steyvers,andSmyth(2004)andSteyvers,Smyth,Rosen-Zvi,andGriffiths(2004)haveextensivelyinvestigatedmodelsofthisform.InferringTopicHierarchiesWecanalsousethegenerativemodelframeworkasthebasisfordefiningmodelsthatuserichersemanticrepresentations.Thetopicmodelassumesthattopicsarechosenindependentlywhenadocumentisgenerated.However,peopleknowthattopicsbearcertainrelationstooneanotherandthatwordshaverelationshipsthatgobeyondtopicmembership.Forexample,sometopicsaremoregeneralthanothers,subsumingsomeofthecontentofthoseothertopics.Thetopicofsportismoregeneralthanthetopicoftennis,andthewordsporthasawidersetofassociatesthanthewordtennis.Onecanaddresstheseissuesbydevelopingmodelsinwhichthelatentstructurecon-cernsnotjustthesetoftopicsthatparticipateinadocumentbuttherelationshipsamongthosetopics.Generativemodelsthatusetopichierarchiesprovideoneexampleofthis,makingitpossibletocapturethefactthatcertaintopicsaremoregeneralthanothers.Bleietal.(2004)providedanalgorithmthatsimultaneouslylearnsthestructureofatopichierarchyandthetopicsthatarecontainedwithinthathierarchy.Thisalgorithmcanbeusedtoextracttopichierarchiesfromlargedocumentcollections.Figure14showstheresultsofapplyingthisalgo-rithmtotheabstractsofallarticlespublishedinPsychologicalReviewsince1967.Thealgorithmrecognizesthatthejournalpublishesworkincognitivepsychology,5socialpsychology,visionresearch,andbiopsychology,splittingthesesubjectsintoseparatetopicsatthesecondlevelofthehierarchyandfindingmeaningfulsubdivisionsofthosesubjectsatthethirdlevel.Similaralgorithmscanbeusedtoexploreotherrepresentationsthatassumedependenciesamongtopics(Blei&Lafferty,2006).CollocationsandAssociationsBasedonWordOrderInthebasictopicmodel,theprobabilityofasequenceofwordsisnotaffectedbytheorderinwhichtheyappear.Asaconse-quence,therepresentationextractedbythemodelcancaptureonlycoarse-grainedcontextualinformation,suchasthefactthatwordstendtoappearinthesamesortofconversationsordocuments.Thisisreflectedinthefactthattheinputtothetopicmodel,aswithLSA,isaword–documentco-occurrencematrix:Theorderinwhichthewordsappearinthedocumentsdoesnotmatter.How-ever,itisclearthatwordorderisimportanttomanyaspectsoflinguisticprocessing,includingthesimpleword-associationtaskthatwediscussedextensivelyearlierinthearticle(Ervin,1961;Hutchinson,2003;McNeill,1966).Afirststeptowardrelaxingtheinsensitivitytowordorderdisplayedbythetopicmodelistoextendthemodeltoincorporatecollocations—wordsthattendtofollowoneanotherwithhighfrequency.Forexample,thebasictopicmodelwouldtreatthephraseunitedkingdomoccurringinadocumentasoneinstanceofunitedandoneinstanceofkingdom.However,thesetwowordscarrymoresemanticinformationwhentreatedasasinglechunkthantheydoalone.Byextendingthemodeltoincorporateasensitivitytocollocations,wegaintheopportunitytoexaminehowincorporatingthisadditionalsourceofpredictiveinformationaf-fectspredictionsabouttheassociationsthatexistbetweenwords.Toextendthetopicmodeltoincorporatecollocations,wein-troducedanadditionalsetofvariablesthatindicatewhetherawordispartofacollocation.Eachwordwithushasatopicassignmentziandacollocationassignmentxi.Thexivariablescantakeontwovalues.Ifxi⫽1,thenwiispartofacollocationandisgeneratedfromadistributionthatdependsjustonthepreviousword,P(wi|wi-1,xi⫽1).Ifxi⫽0,thenwiisgeneratedfromthe5Amoreprecisedescriptionwouldbepsychologybasedonaninformation-processingapproachtostudyingthemind.234GRIFFITHS,STEYVERS,ANDTENENBAUMdistributionassociatedwithitstopic,P(wi|zi,xi⫽0).Itisimpor-tanttonotethatthevalueofxiischosenonthebasisofthepreviousword,wi-1,beingdrawnfromthedistributionP(xi|wi-1).Thismeansthatthemodelcancapturedependenciesbetweenwords:Ifwi-1isunited,itislikelythatxi⫽1,meaningthatwiisgeneratedsolelyonthebasisthatitfollowsunitedandnotonthetopic.Thegraphicalmodelcorrespondingtothisextendedgener-ativeprocessisshowninFigure15.AmoredetaileddescriptionofthemodelappearsinAppendixC,togetherwithanalgorithmthatcanbeusedtosimultaneouslylearnP(wi|wi-1,xi⫽1),P(wi|zi,xi⫽0),andP(xi⫽1|wi-1)fromacorpus.Usingthisextendedtopicmodel,theconditionalprobabilityofonewordgivenanotherissimplyP(w2兩w1)⫽P(w2兩w1,x2⫽1)P(x2⫽1兩w1)⫹P(w2兩w1,x2⫽0)P(x2⫽0兩w1),(12)whereP(w2|w1,x2⫽0)iscomputedasinthebasictopicmodel,usingEquation9.Thus,w2willbehighlyassociatedwithw1eitherifw2tendstofolloww1orifthetwowordstendtooccurinthesamesemanticcontexts.WeusedthealgorithmdescribedinAp-pendixCtoestimatetheprobabilitiesrequiredtocomputeP(w2|w1)fromtheTASAcorpus,usingthesameproceduretoremovestopwordsasinourpreviousanalysesbutsupplyingthewordstothealgorithmintheorderinwhichtheyactuallyoccurredwithineachdocument.Wethenexaminedhowwellsolutionswith500,900,1,300,and1,700topicspredictedtheword-associationnormscollectedbyNelson,McEvoy,andSchreiber(1998).Introducingthecapacitytoproducecollocationschangestheassociatesthatthemodelidentifies.Onewaytoseethisistoexaminecue–associatepairsproducedbypeoplethatareinthesetFigure14.Atopichierarchy,learnedfromtheabstractsofarticlesappearinginPsychologicalReviewsince1967.Eachdocumentisgeneratedbychoosingapathfromtheroot(thetopnode)toaleaf(thebottomnodes).Consequently,wordsintheroottopicappearinalldocuments,thesecond-leveltopicspickoutbroadtrendsacrossdocuments,andthetopicsattheleavespickoutspecifictopicswithinthosetrends.Themodeldifferentiatescognitive,social,vision,andbiopsychologicalresearchatthesecondlevelandidentifiesfinergraineddistinctionswithinthesesubjectsattheleaves.Figure15.Graphicalmodelindicatingdependenciesamongvariablesinthecollocationmodel.Thevariablexidetermineswhetherthewordwiisgeneratedfromadistributionthatdependsonlyonthepreviousword,beingacollocation,orfromadistributionthatdependsonlyonthetopiczi.235TOPICSINSEMANTICREPRESENTATIONof10wordsforwhichP(w2|w1)ishighestunderthecollocationmodelbutnotinthissetforthebasictopicmodel.Consideringjustthefirstassociatespeopleproduceandusingthe1,700-topicmodel,wefindpairssuchasunited–kingdom,bumble–bee,storage–space,metric–system,main–street,evil–devil,foreign–language,fried–chicken,stock–market,interstate–highway,bowling–ball,andserial–number.Theseexamplesthusshowhowthecollocationmodelisabletopredictsomeassociationsthatarebasedonwordorderratherthansemanticcontext.InTable5wecomparethemedianranksoftheassociatesundertheorderingimposedbyP(w2|w1)forthecollocationmodelandthebasictopicmodel.Theresultsshowthatthemodelsperformverysimilarly:Addingthecapacitytocaptureassociationsbasedonwordorderdoesnotresultinamajorimprovementintheperformanceofthemodel.Hutchinson(2003)suggestedthat11.6%ofassociationsresultfromwordorder,whichwouldleadustoexpectsomeimprovementinperformance.Thelackofimprovementmaybeaconsequenceofthefactthatincorporatingtheextraprocessformodelingcollocationsreducestheamountofdataavailableforestimatingtopics,meaningthatthemodelfailstocapturesomesemanticassociations.IntegratingTopicsandSyntaxThemodeldescribedintheprevioussectionprovidesanex-tremelysimplesolutiontothequestionofhowtopicmodelscanbeextendedtocapturewordorder,butourapproachalsosupportsmoresophisticatedsolutions.Generativemodelscanbeusedtoovercomeamajorweaknessofmoststatisticalmodelsoflan-guage:thattheytendtomodeleithersyntaxorsemantics(althoughrecentworkprovidessomeexceptions,includingDennis,2004,andJones&Mewhort,2007).Manyofthemodelsusedincom-putationallinguistics,suchashiddenMarkovmodelsandproba-bilisticcontext-freegrammars(Charniak,1993;Jurafsky&Mar-tin,2000;Manning&Schu¨tze,1999),generatewordspurelyonthebasisofsequentialdependenciesamongunobservedsyntacticclasses,notmodelingthevariationincontentthatoccursacrossdocuments.Incontrast,topicmodelsgeneratewordsinawaythatisintendedtocapturethevariationacrossdocuments,buttheyignoresequentialdependencies.Incognitivescience,methodssuchasdistributionalclustering(Redington,Chater,&Finch,1998)areusedtoinferthesyntacticclassesofwords,whilemethodssuchasLSAareusedtoanalyzetheirmeaning,anditisnotclearhowthesedifferentformsofstatisticalanalysisshouldbecombined.Generativemodelscanbeusedtodefineamodelthatcapturesbothsequentialdependenciesandvariationincontentacrosscon-texts.Thishybridmodelillustratestheappealingmodularityofgenerativemodels.Becauseaprobabilisticlanguagemodelspec-ifiesaprobabilitydistributionoverwordsinadocumentintermsofcomponentsthatarethemselvesprobabilitydistributionsoverwords,differentmodelsareeasilycombinedbymixingtheirpredictionsorembeddingoneinsidetheother.Griffiths,Steyvers,Blei,andTenenbaum(2005;seealsoGriffiths&Steyvers,2003)exploredacompositegenerativemodelforlanguage,inwhichoneoftheprobabilitydistributionsoverwordsusedindefiningasyntacticmodelwasreplacedwithasemanticmodel.Thisallowsthesyntacticmodeltochoosewhentoemitasemanticallyappro-priatewordandallowsthesemanticmodeltochoosewhichwordtoemit.Thesyntacticmodelusedinthiscasewasextremelysimple,butthisexampleservestoillustratetwopoints:thatasimplemodelcandiscovercategoriesofwordsthataredefinedintermsofboththeirsyntacticrolesandtheirsemanticroles,andthatdefiningagenerativemodelthatincorporatesbothofthesefactorsisstraightforward.Asimilarstrategycouldbepursuedwithamorecomplexprobabilisticmodelofsyntax,suchasaprobabilisticcontext-freegrammar.ThestructureofthecompositegenerativemodelisshowninFigure16.Inthismodel,awordcanappearinadocumentfortworeasons:becauseitfulfillsafunctionalsyntacticroleorbecauseitcontributestothesemanticcontent.Accordingly,themodelhastwoparts,oneresponsibleforcapturingsequentialdependenciesproducedbysyntaxandtheotherexpressingsemanticdependen-cies.ThesyntacticdependenciesareintroducedviaahiddenMarkovmodel,apopularprobabilisticmodelforlanguagethatisessentiallyaprobabilisticregulargrammar(Charniak,1993;Ju-rafsky&Martin,2000;Manning&Schu¨tze,1999).InahiddenMarkovmodel,eachwordwiisgeneratedbyfirstchoosingaclasscifromadistributionthatdependsontheclassofthepreviousword,ci-1,andthengeneratingwifromadistributionthatdependsonci.Thecompositemodelsimplyreplacesthedistributionasso-ciatedwithoneoftheclasseswithatopicmodel,whichcapturesthelong-rangesemanticdependenciesamongwords.AnalgorithmsimilartothatdescribedinAppendixAcanbeusedtoinferthedistributionsoverwordsassociatedwiththetopicsandclassesfromacorpus(Griffithsetal.,2005).TheresultsofapplyingthecompositemodeltoacombinationoftheTASAandBrown(Kucˇera&Francis,1967)corporaareshowninFigure17.Thefactorizationofwordsintothosethatappearasaresultofsyntacticdependencies(asrepresentedbytheclassdistributions)andthosethatappearasaresultofsemanticdepen-dencies(representedbythetopicdistributions)pullsapartfunctionandcontentwords.Inadditiontolearningasetofsemantictopics,themodelfindsasetofsyntacticclassesofwordsthatdiscriminatedeterminers,prepositions,pronouns,adjectives,andpresent-andpast-tenseverbs.ThemodelperformsaboutaswellasastandardhiddenMarkovmodel—whichisastate-of-the-artmethod—atidentifyingsyntacticclasses,anditoutperformsdistributionalclus-tering(Redingtonetal.,1998)inthistask(Griffithsetal.,2005).Theabilitytoidentifycategoriesofwordsthatcapturetheirsyntacticandsemanticroles,purelyonthebasisoftheirdistribu-tionalproperties,couldbeavaluablebuildingblockfortheinitialTable5MedianRanksoftheCollocationModelandBasicTopicModelinPredictingWordAssociationNumberoftopicsAssociate1st2nd3rd4th5th50027(29)66(70)104(106)139(141)171(175)90022(22)59(57)105(101)134(131)159(159)130020(20)58(56)105(99)131(128)160(163)170019(18)57(54)102(100)131(130)166(164)Note.Numbersinparenthesesshowtheperformanceofthebasictopicmodelwithoutcollocations.236GRIFFITHS,STEYVERS,ANDTENENBAUMstagesoflanguagelearningorforfacilitatingtheextractionofgist.Forexample,learningthesyntaxofnaturallanguagerequiresachildtodiscovertherulesofthegrammaraswellastheabstractsyntacticcategoriesoverwhichthoserulesaredefined.Thesesyntacticcategoriesandrulesaredefinedonlywithrespecttoeachother,makingithardtoseehowonecouldlearnbothstartingwithneither.Thesyntacticallyorganizedwordclassesdiscoveredbyoursimplestatisticalmodelcouldprovideavaluablestartingpointforlearningsyntax,eventhoughthenotionofsyntacticstructureusedinthemodelisfartoosimplistictocapturethesyntaxofEnglishoranyothernaturallanguage.Thecapacitytoseparateoutthecriticalsemanticcontentwordsinadocumentfromthosewordsplayingprimarilysyntacticfunctionscouldalsobevaluableformodelingadultlanguageprocessingorinmachineinformation-retrievalapplications.Onlythesemanticcontentwordswouldberelevant,forexample,inidentifyingthegistofadocumentorsentence.Thesyntacticfunctionwordscanbe—andusuallyare,byexpertlanguageprocessors—safelyignored.SummaryUsinggenerativemodelsasafoundationforspecifyingpsycho-logicalaccountsoflinguisticprocessingandsemanticmemoryprovidesawaytodefinemodelsthatcanbeextendedtoincorpo-ratemorecomplexaspectsofthestructureoflanguage.Theextensionstothetopicmodeldescribedinthissectionbegintoillustratethispotential.Wehopetousethisframeworktodevelopstatisticalmodelsthatwillallowustoinferrichsemanticstruc-turesthatprovideaclosermatchtothehumansemanticrepresen-tation.Inparticular,themodularityofgenerativemodelsprovidesthebasisforexploringtheinteractionbetweensyntaxandseman-ticsinhumanlanguageprocessingandsuggestshowdifferentkindsofrepresentationcanbecombinedinsolvingcomputationalproblemsthatariseinothercontexts.ConclusionPartoflearningandusinglanguageisidentifyingthelatentsemanticstructureresponsibleforgeneratingasetofwords.Prob-abilisticgenerativemodelsprovidesolutionstothisproblem,mak-ingitpossibletousepowerfulstatisticallearningtoinferstruc-turedrepresentations.Thetopicmodelisoneinstanceofthisapproachandservesasastartingpointforexploringhowgener-ativemodelscanbeusedtoaddressquestionsabouthumanse-Figure16.Graphicalmodelindicatingdependenciesamongvariablesinthecompositemodel,inwhichsyntacticdependenciesarecapturedbyahiddenMarkovmodel(withthecivariablesbeingtheclassesfromwhichwordsaregenerated)andsemanticdependenciesarecapturedbyatopicmodel.Figure17.ResultsofapplyingacompositemodelthathasbothsyntacticandsemanticlatentstructuretoaconcatenationoftheTouchstoneAppliedScienceAssociatesandBrowncorpora.ThemodelsimultaneouslyfindsthekindofsemantictopicsidentifiedbythetopicmodelandsyntacticclassesofthekindproducedbyahiddenMarkovmodel.Asterisksindicatelow-frequencywords,occurringfewerthan10timesinthecorpus.237TOPICSINSEMANTICREPRESENTATIONmanticrepresentation.ItoutperformsLSA,aleadingmodeloftheacquisitionofsemanticknowledge,inpredictingwordassociationandavarietyofotherlinguisticprocessingandmemorytasks.Italsoexplainsseveralaspectsofwordassociationthatareproblem-aticforLSA:wordfrequencyandasymmetry,violationofthetriangleinequality,andthepropertiesofsemanticnetworks.Thesuccessofthemodelonthesetaskscomesfromthestructuredrepresentationthatitassumes:Byexpressingthemeaningofwordsintermsofdifferenttopics,themodelisabletocapturetheirdifferentmeaningsandsenses.Beyondthetopicmodel,generativemodelsprovideapathtowardamorecomprehensiveexplorationoftheroleofstructuredrepresen-tationsandstatisticallearningintheacquisitionandapplicationofsemanticknowledge.Wehavesketchedsomeofthewaysinwhichthetopicmodelcanbeextendedtobringitclosertocapturingtherichnessofhumanlanguage.Althoughwearestillfarfromunder-standinghowpeoplecomprehendandacquirelanguage,theseexam-plesillustratehowincreasinglycomplexstructurescanbelearnedusingstatisticalmethods,andtheyshowsomeofthepotentialforgenerativemodelstoprovideinsightintothepsychologicalquestionsraisedbyhumanlinguisticabilities.Acrossmanyareasofcognition,perception,andaction,probabilisticgenerativemodelshaverecentlycometoofferaunifyingframeworkforunderstandingaspectsofhumanintelligenceasrationaladaptationstothestatisticalstructureoftheenvironment(Anderson,1990;Anderson&Schooler,1991;Gei-sleretal.,2001;Griffiths&Tenenbaum,2006,inpress;Kempetal.,2004;Ko¨rding&Wolpert,2004;Simoncelli&Olshausen,2001;Wolpertetal.,1995).Itremainstobeseenhowfarthisapproachcanbecarriedinthestudyofsemanticrepresentationandlanguageuse,buttheexistenceoflargecorporaoflinguisticdataandpowerfulstatisticalmodelsforlanguageclearlymakethisadirectionworthpursuing.ReferencesAnderson,J.R.(1983).Aspreadingactivationtheoryofmemory.JournalofVerbalLearningandVerbalBehavior,22,261–295.Anderson,J.R.(1990).Theadaptivecharacterofthought.Hillsdale,NJ:Erlbaum.Anderson,J.R.,&Bower,G.H.(1974).Humanassociativememory.Washington,DC:Hemisphere.Anderson,J.R.,&Schooler,L.J.(1991).Reflectionsoftheenvironmentinmemory.PsychologicalScience,2,396–408.Baldewein,U.,&Keller,F.(2004).Modelingattachmentdecisionswithaprobabilisticparser:Thecaseofheadfinalstructures.InK.Forbus,D.Gentner,&T.Regier(Eds.),Proceedingsofthe26thAnnualConferenceoftheCognitiveScienceSociety(pp.73–78).Hillsdale,NJ:Erlbaum.Bartlett,F.C.(1932).Remembering:Astudyinexperimentalandsocialpsychology.Cambridge,England:CambridgeUniversityPress.Batchelder,W.H.,&Riefer,D.M.(1999).Theoreticalandempiricalreviewofmultinomialprocessingtreemodeling.PsychonomicBulletin&Review,6,57–86.Bigi,B.,DeMori,R.,El-Beze,M.,&Spriet,T.(1997).Combinedmodelsfortopicspottingandtopic-dependentlanguagemodeling.In1997IEEEWork-shoponAutomaticSpeechRecognitionandUnderstandingProceedings(pp.535–542).Piscataway,NJ:IEEESignalProcessingSociety.Blei,D.M.,Griffiths,T.L.,Jordan,M.I.,&Tenenbaum,J.B.(2004).HierarchicaltopicmodelsandthenestedChineserestaurantprocess.InS.Thrun,L.Saul,&B.Scho¨lkopf(Eds.),Advancesinneuralinformationprocessingsystems(Vol.16,pp.17–24).Cambridge,MA:MITPress.Blei,D.,&Lafferty,J.(2006).Correlatedtopicmodels.InY.Weiss,B.Scho¨lkopf,&Y.Platt(Eds.),Advancesinneuralinformationprocessingsystems(Vol.18).Cambridge,MA:MITPress.Blei,D.M.,Ng,A.Y.,&Jordan,M.I.(2003).LatentDirichletallocation.JournalofMachineLearningResearch,3,993–1022.Brainerd,C.J.,Reyna,V.F.,&Mojardin,A.H.(1999).Conjointrecog-nition.PsychologicalReview,106,160–179.Brainerd,C.J.,Wright,R.,&Reyna,V.F.(2002).Dual-retrievalprocessesinfreeandassociativerecall.JournalofMemoryandLanguage,46,120–152.Buntine,W.(2002).VariationalextensionstoEMandmultinomialPCA.InProceedingsofthe13thEuropeanConferenceonMachineLearning(pp.23–24).Berlin,Germany:Springer-Verlag.Buntine,W.,&Jakulin,A.(2004).ApplyingdiscretePCAindataanalysis.InM.Chickering&J.Halpern(Eds.),Proceedingsofthe20thConfer-enceonUncertaintyinArtificialIntelligence(UAI)(pp.59–66).SanFrancisco:MorganKaufmann.Charniak,E.(1993).Statisticallanguagelearning.Cambridge,MA:MITPress.Chomsky,N.(1965).Aspectsofthetheoryofsyntax.Cambridge,MA:MITPress.Collins,A.M.,&Loftus,E.F.(1975).Aspreadingactivationtheoryofsemanticprocessing.PsychologicalReview,82,407–428.Collins,A.M.,&Quillian,M.R.(1969).Retrievaltimefromsemanticmemory.JournalofVerbalLearningandVerbalBehavior,8,240–247.Cramer,P.(1968).Wordassociation.NewYork:AcademicPress.Deese,J.(1959).Onthepredictionofoccurrenceofparticularverbalintrusionsinimmediaterecall.JournalofExperimentalPsychology,58,17–22.Deese,J.(1962).Onthestructureofassociativemeaning.PsychologicalReview,69,161–175.Deese,J.(1965).Thestructureofassociationsinlanguageandthought.Baltimore:JohnsHopkinsUniversityPress.Dennis,S.(2003).Acomparisonofstatisticalmodelsfortheextractionoflexicalinformationfromtextcorpora.InProceedingsoftheTwenty-FifthConferenceoftheCognitiveScienceSociety(pp.330–335).Hill-sdale,NJ:Erlbaum.Dennis,S.(2004).Anunsupervisedmethodfortheextractionofproposi-tionalinformationfromtext.ProceedingsoftheNationalAcademyofSciences,USA,101,5206–5213.Dennis,S.,&Humphreys,M.S.(2001).Acontextnoisemodelofepisodicwordrecognition.PsychologicalReview,108,452–478.Duffy,S.A.,Morris,R.K.,&Rayner,K.(1988).Lexicalambiguityandfixationtimesinreading.JournalofMemoryandLanguage,27,429–446.Elman,J.L.(1990).Findingstructureintime.CognitiveScience,14,179–211.Ericsson,K.A.,&Kintsch,W.(1995).Long-termworkingmemory.PsychologicalReview,102,211–245.Erosheva,E.A.(2002).Gradeofmembershipandlatentstructuremodelswithapplicationstodisabilitysurveydata.Unpublisheddoctoraldissertation,DepartmentofStatistics,CarnegieMellonUniversity,Pittsburgh,PA.Ervin,S.M.(1961).Changeswithageintheverbaldeterminantsofwordassociation.AmericanJournalofPsychology,74,361–372.Fillenbaum,S.,&Rapoport,A.(1971).Structuresinthesubjectivelexicon.NewYork:AcademicPress.Freeman,W.T.(1994,April7).Thegenericviewpointassumptioninaframeworkforvisualperception.Nature,368,542–545.Galton,F.(1880).Psychometricexperiments.Brain,2,149–162.Geisler,W.S.,Perry,J.S.,Super,B.J.,&Gallogly,D.P.(2001).Edgeco-occurrenceinnaturalimagespredictscontourgroupingperformance.VisionResearch,41,711–724.Gelman,A.,Carlin,J.B.,Stern,H.S.,&Rubin,D.B.(1995).Bayesiandataanalysis.NewYork:Chapman&Hall.Gilks,W.,Richardson,S.,&Spiegelhalter,D.J.(Eds.).(1996).MarkovchainMonteCarloinpractice.Suffolk,England:Chapman&Hall.238GRIFFITHS,STEYVERS,ANDTENENBAUMGriffiths,T.L.,&Steyvers,M.(2002).Aprobabilisticapproachtosemanticrepresentation.InProceedingsoftheTwenty-FourthAnnualConferenceoftheCognitiveScienceSociety.Hillsdale,NJ:Erlbaum.Griffiths,T.L.,&Steyvers,M.(2003).Predictionandsemanticassocia-tion.InAdvancesinneuralinformationprocessingsystems(Vol.15,pp.11–18).Cambridge,MA:MITPress.Griffiths,T.L.,&Steyvers,M.(2004).Findingscientifictopics.Proceed-ingsoftheNationalAcademyofSciences,USA,101,5228–5235.Griffiths,T.L.,Steyvers,M.,Blei,D.M.,&Tenenbaum,J.B.(2005).Integratingtopicsandsyntax.InAdvancesinneuralinformationpro-cessingsystems(Vol.17,pp.537–544).Cambridge,MA:MITPress.Griffiths,T.L.,&Tenenbaum,J.B.(2006).Optimalpredictionsineverydaycognition.PsychologicalScience,17,767–773.Griffiths,T.L.,&Tenenbaum,J.B.(inpress).Frommerecoincidencestomeaningfuldiscoveries.Cognition.Hobbes,T.(1998).Leviathan.Oxford,England:OxfordUniversityPress.(Originalworkpublished1651)Hofmann,T.(1999).Probabilisticlatentsemanticindexing.InProceedingsoftheTwenty-SecondAnnualInternationalSIGIRConference(pp.50–57).NewYork:ACMPress.Hutchinson,K.A.(2003).Issemanticprimingduetoassociationstrengthorfeatureoverlap?PsychonomicBulletin&Review,10,785–813.Iyer,R.M.,&Ostendorf,M.(1999).Modelinglongdistancedependenceinlanguage:Topicmixturesversusdynamiccachemodels.IEEETrans-actionsonSpeechandAudioProcessing,7,30–39.James,W.(1890).Principlesofpsychology.NewYork:Holt.Jones,M.N.,&Mewhort,D.J.K.(2007).Representingwordmeaningandorderinformationinacompositeholographiclexicon.PsychologicalReview,114,1–37.Jordan,M.I.(1998).Learningingraphicalmodels.Cambridge,MA:MITPress.Jurafsky,D.(1996).Aprobabilisticmodeloflexicalandsyntacticaccessanddisambiguation.CognitiveScience,20,137–194.Jurafsky,D.,&Martin,J.H.(2000).Speechandlanguageprocessing.UpperSaddleRiver,NJ:PrenticeHall.Kawamoto,A.H.(1993).Nonlineardynamicsintheresolutionoflexicalambiguity:Aparalleldistributedprocessingaccount.JournalofMemoryandLanguage,32,474–516.Keil,F.C.(1979).Semanticandconceptualdevelopment:Anontologicalperspective.Cambridge,MA:HarvardUniversityPress.Kemp,C.,Perfors,A.,&Tenenbaum,J.B.(2004).Learningdomainstructures.InK.Forbus,D.Gentner,&T.Regier(Eds.),Proceedingsofthe26thAnnualConferenceoftheCognitiveScienceSociety(pp.672–677)Hillsdale,NJ:Erlbaum.Kintsch,W.(1988).Theroleofknowledgeindiscoursecomprehension:Aconstruction-integrationmodel.PsychologicalReview,95,163–182.Kiss,G.R.,Armstrong,C.,Milroy,R.,&Piper,J.(1973).AnassociativethesaurusofEnglishanditscomputeranalysis.InA.J.Aitkin,R.W.Bailey,&N.Hamilton-Smith(Eds.),Thecomputerandliterarystudies(pp.153–165).Edinburgh,Scotland:EdinburghUniversityPress.Ko¨rding,K.P.,&Wolpert,D.M.(2004,January15).Bayesianintegrationinsensorimotorlearning.Nature,427,244–248.Krumhansl,C.(1978).Concerningtheapplicabilityofgeometricmodelstosimilaritydata:Theinterrelationshipbetweensimilarityandspatialdensity.PsychologicalReview,85,450–463.Kucˇera,H.,&Francis,W.N.(1967).Computationalanalysisofpresent-dayAmericanEnglish.Providence,RI:BrownUniversityPress.Landauer,T.K.,&Dumais,S.T.(1997).AsolutiontoPlato’sproblem:Thelatentsemanticanalysistheoryofacquisition,induction,andrep-resentationofknowledge.PsychologicalReview,104,211–240.Lund,K.,&Burgess,C.(1996).Producinghigh-dimensionalsemanticspacesfromlexicalco-occurrence.BehaviorResearchMethods,Instru-ments,&Computers,28,203–208.Mandler,G.(1980).Recognizing:Thejudgmentofpreviousoccurrence.PsychologicalReview,87,252–271.Manning,C.,&Schu¨tze,H.(1999).Foundationsofstatisticalnaturallanguageprocessing.Cambridge,MA:MITPress.Markman,A.B.(1998).Knowledgerepresentation.Hillsdale,NJ:Erl-baum.Marr,D.(1982).Vision.SanFrancisco:Freeman.McDonald,S.A.,&Shillcock,R.C.(2001).Rethinkingthewordfre-quencyeffect:Theneglectedroleofdistributionalinformationinlexicalprocessing.LanguageandSpeech,44,295–323.McEvoy,C.L.,Nelson,D.L.,&Komatsu,T.(1999).What’stheconnec-tionbetweentrueandfalsememories:Thedifferentrolesofinter-itemassociationsinrecallandrecognition.JournalofExperimentalPsychol-ogy:Learning,Memory,andCognition,25,1177–1194.McNeill,D.(1966).Astudyofwordassociation.JournalofVerbalLearningandVerbalBehavior,2,250–262.Miller,G.A.,&Fellbaum,C.(1998).WordNet:Anelectroniclexicaldatabase.Cambridge,MA:MITPress.Minka,T.,&Lafferty,J.(2002).Expectation–propagationforthegener-ativeaspectmodel.InProceedingsofthe18thConferenceonUncer-taintyinArtificialIntelligence(UAI)(pp.352–359).SanFrancisco:MorganKaufmann.Muller,P.,&Quintana,F.A.(2004).NonparametricBayesiandataanal-ysis.StatisticalScience,19,95–110.Neal,R.M.(2000).MarkovchainsamplingmethodsforDirichletprocessmixturemodels.JournalofComputationalandGraphicalStatistics,9,249–265.Neely,J.H.(1976).Semanticprimingandretrievalfromlexicalmemory:Evidenceforfacilitatoryandinhibitoryprocesses.Memory&Cognition,4,648–654.Nelson,D.L.,&Goodmon,L.B.(2002).Experiencingawordcanprimeitsaccessibilityanditsassociativeconnectionstorelatedwords.Memory&Cognition,30,380–398.Nelson,D.L.,McEvoy,C.L.,&Dennis,S.(2000).Whatisfreeassoci-ationandwhatdoesitmeasure?Memory&Cognition,28,887–899.Nelson,D.L.,McEvoy,C.L.,&Schreiber,T.A.(1998).TheUniversityofSouthFloridawordassociation,rhyme,andwordfragmentnorms.Retrieved2001fromhttp://www.usf.edu/FreeAssociation/Nelson,D.L.,McKinney,V.M.,Gee,N.R.,&Janczura,G.A.(1998).Interpretingtheinfluenceofimplicitlyactivatedmemoriesonrecallandrecognition.PsychologicalReview,105,299–324.Nelson,D.L.,Zhang,N.,&McKinney,V.M.(2001).Thetiesthatbindwhatisknowntotherecognitionofwhatisnew.JournalofExperimen-talPsychology:Learning,Memory,andCognition,27,1147–1159.Norman,D.A.,Rumelhart,D.E.,&TheLNRResearchGroup.(1975).Explorationsincognition.SanFrancisco:Freeman.Nosofsky,R.M.(1991).Stimulusbias,asymmetricsimilarity,andclassi-fication.CognitivePsychology,23,94–140.Pearl,J.(1988).Probabilisticreasoninginintelligentsystems.SanFran-cisco:MorganKaufmann.Pinker,S.(1999).Wordsandrules:Theingredientsoflanguage.NewYork:BasicBooks.Plaut,D.C.(1997).Structureandfunctioninthelexicalsystem:Insightsfromdistributedmodelsofwordreadingandlexicaldecision.LanguageandCognitiveProcesses,12,765–805.Plunkett,K.,&Marchman,V.(1993).Fromrotelearningtosystembuilding:Acquiringverbmorphologyinchildrenandconnectionistnets.Cognition,48,21–69.Potter,M.C.(1993).Veryshorttermconceptualmemory.Memory&Cognition,21,156–161.Pritchard,J.K.,Stephens,M.,&Donnelly,P.(2000).Inferenceofpopu-lationstructureusingmultilocusgenotypedata.Genetics,155,945–955.Purver,M.,Ko¨rding,K.P.,Griffiths,T.L.,&Tenenbaum,J.B.(2006).Unsupervisedtopicmodellingformulti-partyspokendiscourse.InPro-239TOPICSINSEMANTICREPRESENTATIONceedingsofCOLING/ACL(pp.17–24).Sydney,Australia:AssociationforComputationalLinguistics.Rayner,K.,&Duffy,S.A.(1986).Lexicalcomplexityandfixationtimesinreading:Effectsofwordfrequency,verbcomplexity,andlexicalambiguity.Memory&Cognition,14,191–201.Rayner,K.,&Frazier,L.(1989).Selectionmechanismsinreadinglexi-callyambiguouswords.JournalofExperimentalPsychology:Learning,Memory,andCognition,15,779–790.Redington,M.,Chater,N.,&Finch,S.(1998).Distributionalinformation:Apowerfulcueforacquiringsyntacticcategories.CognitiveScience,22,425–469.Rehder,B.,Schreiner,M.E.,Wolfe,M.B.,Laham,D.,Landauer,T.K.,&Kintsch,W.(1998).Usinglatentsemanticanalysistoassessknowl-edge:Sometechnicalconsiderations.DiscourseProcesses,25,337–354.Rodd,J.M.,Gaskell,M.G.,&Marslen-Wilson,W.D.(2004).Modellingtheeffectsofsemanticambiguityinwordrecognition.CognitiveSci-ence,28,89–104.Roediger,H.L.,&McDermott,K.B.(1995).Creatingfalsememories:Rememberingwordsnotpresentedinlists.JournalofExperimentalPsychology:Learning,Memory,andCognition,21,803–814.Roediger,H.L.,Watson,J.M.,McDermott,K.B.,&Gallo,D.A.(2001).Factorsthatdeterminefalserecall:Amultipleregressionanalysis.Psy-chonomicBulletin&Review,8,385–407.Rogers,T.,&McClelland,J.(2004).Semanticcognition:Aparalleldistributedprocessingapproach.Cambridge,MA:MITPress.Roget,P.(1911).Roget’sthesaurusofEnglishwordsandphrases.Re-trieved2001fromhttp://www.gutenberg.org/etext/10681Rosen-Zvi,M.,Griffiths,T.,Steyvers,M.,&Smyth,P.(2004).Theauthor–topicmodelforauthorsanddocuments.InProceedingsoftheTwentiethConferenceonUncertaintyinArtificialIntelligence(UAI).SanFrancisco:MorganKaufmann.Rumelhart,D.,&McClelland,J.(1986).OnlearningthepasttensesofEnglishverbs.InJ.McClelland,D.Rumelhart,&ThePDPResearchGroup(Eds.),Paralleldistributedprocessing:Explorationsinthemicrostructureofcog-nition(Vol.2,pp.216–271).Cambridge,MA:MITPress.Sereno,S.C.,O’Donnell,P.J.,&Rayner,K.(2006).Eyemovementsandlexicalambiguityresolution:Investigatingthesubordinate-biaseffect.JournalofExperimentalPsychology:HumanPerceptionandPerfor-mance,32,335–350.Sereno,S.C.,Pacht,J.M.,&Rayner,K.(1992).Theeffectofmeaningfrequencyonprocessinglexicallyambiguouswords:Evidencefromeyefixations.PsychologicalScience,3,296–300.Shepard,R.N.(1987,September11).Towardsauniversallawofgener-alizationforpsychologicalscience.Science,237,1317–1323.Shepard,R.N.,&Arabie,P.(1979).Additiveclustering:Representationofsimilaritiesascombinationsofdiscreteoverlappingproperties.Psycho-logicalReview,86,87–123.Simoncelli,E.P.,&Olshausen,B.(2001).Naturalimagestatisticsandneuralrepresentation.AnnualReviewofNeuroscience,24,1193–1216.Steyvers,M.,&Malmberg,K.(2003).Theeffectofnormativecontextvariabilityonrecognitionmemory.JournalofExperimentalPsychol-ogy:Learning,Memory,andCognition,29,760–766.Steyvers,M.,Shiffrin,R.M.,&Nelson,D.L.(2004).Wordassociationspacesforpredictingsemanticsimilarityeffectsinepisodicmemory.InA.F.Healy(Ed.),Experimentalcognitivepsychologyanditsapplications:FestschriftinhonorofLyleBourne,WalterKintsch,andThomasLandauer(pp.237–249).Washington,DC:AmericanPsychologicalAssociation.Steyvers,M.,Smyth,P.,Rosen-Zvi,M.,&Griffiths,T.(2004).Probabi-listicauthor–topicmodelsforinformationdiscovery.InTheTenthACMSIGKDDInternationalConferenceonKnowledgeDiscoveryandDataMining(pp.306–315).NewYork:ACMPress.Steyvers,M.,&Tenenbaum,J.B.(2005).Thelargescalestructureofsemanticnetworks:Statisticalanalysesandamodelofsemanticgrowth.CognitiveScience,29,41–78.Teh,Y.,Jordan,M.,Beal,M.,&Blei,D.(2004).HierarchicalDirichletprocesses.InAdvancesinneuralinformationprocessingsystems(Vol.17,pp.1385–1392).Cambridge,MA:MITPress.Tenenbaum,J.B.,&Griffiths,T.L.(2001).Generalization,similarity,andBayesianinference.BehavioralandBrainSciences,24,629–641.Till,R.E.,Mross,E.F.,&Kintsch,W.(1988).Timecourseofprimingforassociateandinferencewordsinadiscoursecontext.Memory&Cog-nition,16,283–298.Tversky,A.(1977).Featuresofsimilarity.PsychologicalReview,84,327–352.Tversky,A.,&Gati,I.(1982).Similarity,separabilityandthetriangleinequality.PsychologicalReview,89,123–154.Tversky,A.,&Hutchinson,J.W.(1986).Nearestneighboranalysisofpsychologicalspaces.PsychologicalReview,93,3–22.Ueda,N.,&Saito,K.(2003).Parametricmixturemodelsformulti-labeledtext.InAdvancesinneuralinformationprocessingsystems(Vol.15,pp.721–728).Cambridge,MA:MITPress.Warrington,E.K.(1975).Theselectiveimpairmentofsemanticmemory.QuarterlyJournalofExperimentalPsychology,27,635–657.Watts,D.J.,&Strogatz,S.H.(1998,June4).Collectivedynamicsof“small-world”networks.Nature,393,440–442.Weiss,Y.,Simoncelli,E.P.,&Adelson,E.H.(2002).Motionillusionsasoptimalpercepts.NatureNeuroscience,5,598–604.Wolfe,M.B.,Schreiner,M.E.,Rehder,B.,Laham,D.,Foltz,P.W.,Kintsch,W.,&Landauer,T.(1998).Learningfromtext:Matchingreadersandtextbylatentsemanticanalysis.DiscourseProcesses,25,309–336.Wolpert,D.M.,Ghahramani,Z.,&Jordan,M.I.(1995,September29).Aninternalmodelforsensorimotorintegration.Science,269,1880–1882.Yantis,S.,Meyer,D.E.,&Smith,J.E.K.(1991).Analysesofmultinomialmixturedistributions:Newtestsforstochasticmodelsofcognitionandaction.PsychologicalBulletin,110,350–374.Zipf,G.K.(1965).Humanbehaviorandtheprincipleofleasteffort.NewYork:Hafner.240GRIFFITHS,STEYVERS,ANDTENENBAUMAppendixAStatisticalFormulationoftheTopicModelAnumberofapproachestostatisticalmodelingoflanguagehavebeenbasedonprobabilistictopics.Thenotionthatatopiccanberepresentedasaprobabilitydistributionoverwordsappearsinseveralplacesinthenaturallanguageprocessingliterature(e.g.,Iyer&Ostendorf,1999).CompletelyunsupervisedmethodsforextractingsetsoftopicsfromlargecorporawerepioneeredbyHofmann(1999),inhisprobabilisticlatentsemanticindexingmethod(alsoknownastheaspectmodel).Bleietal.(2003)extendedthisapproachbyintroducingaprioronthedistributionovertopics,turningthemodelintoagenuinegenerativemodelforcollectionsofdocuments.UedaandSaito(2003)exploredasim-ilarmodel,inwhichdocumentsarebalancedmixturesofasmallsetoftopics.Alloftheseapproachesuseacommonrepresentation,characterizingthecontentofwordsanddocumentsintermsofprobabilistictopics.Thestatisticalmodelunderlyingmanyoftheseapproacheshasalsobeenappliedtodataotherthantext.Erosheva(2002)de-scribedamodel,equivalenttoatopicmodel,appliedtodisabilitydata.Thesamemodelhasbeenappliedtodataanalysisingenetics(Pritchard,Stephens,&Donnelly,2000).Topicmodelsalsomakeanappearanceinthepsychologicalliteratureondataanalysis(Yantis,Meyer,&Smith,1991).Buntine(2002)pointedoutaformalcorrespondencebetweentopicmodelsandprincipal-componentsanalysis,providingafurtherconnectiontoLSA.Amultidocumentcorpuscanbeexpressedasavectorofwordsw⫽兵w1,...,wn},whereeachwibelongstosomedocumentdi,asinaword–documentco-occurrencematrix.UnderthegenerativemodelintroducedbyBleietal.(2003),thegistofeachdocument,g,isencodedusingamultinomialdistributionovertheTtopics,withparameters␪(d),andsoforawordindocumentd,P(z兩g)⫽␪z(d).ThezthtopicisrepresentedbyamultinomialdistributionovertheWwordsinthevocabulary,withparameters␾(z),andsoP(w兩z)⫽␾w(z).WethentakeasymmetricDirichlet(␣)prioron␪(d)foralldocumentsandasymmetricDirichlet(␤)prioron␾(z)foralltopics.Thecompletestatisticalmodelcanthusbewrittenaswi兩zi,␾(zi)⬃Discrete共␾共zi))␾共z)⬃Dirichlet共␤兲zi兩␪(di)⬃Discrete共␪共di))␪共d兲⬃Dirichlet共␣兲.Theuserofthealgorithmcanspecify␣and␤,whicharehyper-parametersthataffectthegranularityofthetopicsdiscoveredbythemodel(seeGriffiths&Steyvers,2004).AnAlgorithmforFindingTopicsSeveralalgorithmshavebeenproposedforlearningtopics,includingexpectationmaximization(Hofmann,1999),variationalexpectationmaximization(Bleietal.,2003;Buntine,2002),ex-pectationpropagation(Minka&Lafferty,2002),andseveralformsofMarkovchainMonteCarlo(MCMC;Buntine&Jakulin,2004;Erosheva,2002;Griffiths&Steyvers,2002,2003,2004;Pritchardetal.,2000).WeuseGibbssampling,aformofMCMC.Weextractasetoftopicsfromacollectionofdocumentsinacompletelyunsupervisedfashion,usingBayesianinference.TheDirichletpriorsareconjugatetothemultinomialdistributions␾,␪,allowingustocomputethejointdistributionP(w,z)byintegratingout␾and␪.BecauseP(w,z)⫽P(w|z)P(z)and␾and␪appearonlyinthefirstandsecondterms,respectively,wecanperformtheseintegralsseparately.Integratingout␾givesthefirstterm,P(w兩z)⫽冉⌫共W␤)⌫(␤)W冊T写j⫽1T写w⌫(nj(w)⫹␤)⌫(nj(䡠)⫹W␤),(A1)inwhichnj(w)isthenumberoftimeswordwhasbeenassignedtotopicjinthevectorofassignmentszand⌫(䡠)isthestandardgammafunction.Thesecondtermresultsfromintegratingout␪,togiveP(z)⫽冉⌫共T␣)⌫(␣)T冊D写d⫽1D写j⌫共nj(d)⫹␣兲⌫共n䡠(d)⫹T␣),wherenj(d)isthenumberoftimesawordfromdocumentdhasbeenassignedtotopicj.Wecanthenaskquestionsabouttheposteriordistributionoverzgivenw,givenbyBayes’srule:P(z兩w)⫽P(w,z)冘zP(w,z).Unfortunately,thesuminthedenominatorisintractable,havingTnterms,andweareforcedtoevaluatethisposteriorusingMCMC.MCMCisaprocedureforobtainingsamplesfromcomplicatedprobabilitydistributions,allowingaMarkovchaintoconvergetothetargetdistributionandthendrawingsamplesfromtheMarkovchain(seeGilks,Richardson,&Spiegelhalter,1996).Eachstateofthechainisanassignmentofvaluestothevariablesbeingsam-pled,andtransitionsbetweenstatesfollowasimplerule.WeuseGibbssampling,inwhichthenextstateisreachedbysequentiallysamplingallvariablesfromtheirdistributionwhenconditionedonthecurrentvaluesofallothervariablesandthedata.Wesampleonlytheassignmentsofwordstotopics,zi.TheconditionalposteriordistributionforziisgivenbyP(zi⫽j兩z⫺i,w)⬀n⫺i,j(wi)⫹␤n⫺i,j(䡠)⫹W␤n⫺i,j(di)⫹␣n⫺i,䡠(di)⫹T␣,(A2)(Appendixescontinue)241TOPICSINSEMANTICREPRESENTATIONwherez-iistheassignmentofallzksuchthatk⫽i,andn⫺i,j(wi)isthenumberofwordsassignedtotopicjthatarethesameaswi,n⫺i,j(䡠)isthetotalnumberofwordsassignedtotopicj,n⫺i,j(di)isthenumberofwordsfromdocumentdiassignedtotopicj,andn⫺i,䡠(di)isthetotalnumberofwordsindocumentdi,allnotcountingtheassignmentofthecurrentwordwi.TheMCMCalgorithmisthenstraightforward.Theziareinitializedtovaluesbetween1andT,determiningtheinitialstateoftheMarkovchain.Thechainisthenrunforanumberofiterations,eachtimefindinganewstatebysamplingeachzifromthedistributionspecifiedbyEquationA2.Afterenoughiterationsforthechaintoapproachthetargetdistribution,thecurrentvaluesoftheziarerecorded.Subsequentsamplesaretakenafteranappropriatelagtoensurethattheirautocorrela-tionislow.FurtherdetailsofthealgorithmareprovidedinGriffithsandSteyvers(2004),whereweshowhowitcanbeusedtoanalyzethecontentofdocumentcollections.ThevariablesinvolvedintheMCMCalgorithm,andtheirmodificationacrosssamples,areillustratedinFigureA1,whichusesthedatafromFigure2.Eachwordtokeninthecorpus,wi,hasatopicassignment,zi,ateachiterationofthesamplingprocedure.Inthiscase,wehave90documentsandatotalof731wordswi,eachwithitsownzi.Inthefigure,wefocusonthetokensofthreewords:money,bank,andstream.Eachwordtokenisinitiallyrandomlyassignedtoatopic,andeachitera-tionofMCMCresultsinanewsetofassignmentsoftokenstotopics.Afterafewiterations,thetopicassignmentsbegintoreflectthedifferentusagepatternsofmoneyandstream,withtokensofthesewordsendingupindifferenttopics,andthemultiplesensesofbank.TheresultoftheMCMCalgorithmisasetofsamplesfromP(z|w),reflectingtheposteriordistributionovertopicassignmentsgivenacollectionofdocuments.Fromanysinglesamplewecanobtainanestimateoftheparameters␾and␪fromzvia␾ˆw(j)⫽nj(w)⫹␤nj(䡠)⫹W␤(A3)␪ˆj(d)⫽nj(d)⫹␣n䡠(d)⫹T␣.(A4)MONEYBANKSTREAM9080706050403020100DocumentRandom initialization of words to topicsTopic 1Topic 2Topic 3MONEYBANKSTREAM9080706050403020100After 1600 iterations of Gibbs samplingDocumentTopic 1Topic 2Topic 3FigureA1.IllustrationoftheGibbssamplingalgorithmforlearningtopics,usingthedatafromFigure2.Eachwordtokenwiappearinginthecorpushasatopicassignment,zi.Thefigureshowstheassignmentsofalltokensofthreetypes—money,bank,andstream—beforeandafterrunningthealgorithm.Eachmarkercorrespondstoasingletokenappearinginaparticulardocument,andshapeandcolorindicateassignment:Topic1isablackcircle,Topic2isagraysquare,andTopic3isawhitetriangle.Beforethealgorithmisrun,assignmentsarerelativelyrandom,asshownintheleftpanel.Afterthealgorithmisrun,tokensofmoneyarealmostexclusivelyassignedtoTopic3,tokensofstreamarealmostexclusivelyassignedtoTopic1,andtokensofbankareassignedtowhicheverofTopic1andTopic3seemstodominateagivendocument.Thealgorithmconsistsofiterativelychoosinganassignmentforeachtokenusingaprobabilitydistributionovertokensthatguaranteesconvergencetotheposteriordistributionoverassignments.242GRIFFITHS,STEYVERS,ANDTENENBAUMThesevaluescorrespondtothepredictivedistributionsovernewwordswandnewtopicszconditionedonwandz,andtheposteriormeansof␪and␾givenwandz.Prediction,Disambiguation,andGistExtractionThegenerativemodelallowsdocumentstocontainmultipletopics,whichisimportantinmodelinglongandcomplexdocu-ments.Assumewehaveanestimateofthetopicparameters,␾.Thentheproblemsofprediction,disambiguation,andgistextrac-tioncanbereducedtocomputingP(wn⫹1兩w;␾)⫽冘z,zn⫹1P(wn⫹1兩zn⫹1;␾)P(zn⫹1兩z)P(z兩w;␾),(A5)P(z兩w;␾)⫽P(w,z兩␾)冘zP(w,z兩␾),(A6)andP(g兩w;␾)⫽冘zP(g兩z)P(z兩w;␾),(A7)respectively.Thesumsoverzthatappearineachoftheseexpres-sionsquicklybecomeintractable,beingoverTnterms,buttheycanbeapproximatedusingMCMC.Inmanysituations,suchasprocessingasinglesentence,itisreasonabletoassumethatwearedealingwithwordsthataredrawnfromasingletopic.Underthisassumption,gisrepre-sentedbyamultinomialdistribution␪thatputsallofitsmassonasingletopic,z,andzi⫽zforalli.Theproblemsofdisambiguationandgistextractionthusreducetoinferringz.ApplyingBayes’srule,P(z兩w;␾)⫽P(w兩z;␾)P(z)冘zP(w兩z;␾)P(z)⫽写i⫽1nP(wi兩z;␾)P(z)冘z写i⫽1nP(wi兩z;␾)P(z)⫽写i⫽1n␾wi(z)冘z写i⫽1n␾wi(z),wherethelastequalityassumesauniformprior,P(z)⫽1T,consistentwiththesymmetricDirichletpriorsassumedabove.WecanthenformpredictionsviaP(wn⫹1兩w;␾)⫽冘zP(wn⫹1,z兩w;␾)⫽冘zP(wn⫹1兩z;␾)P(z兩w;␾)⫽冘z写i⫽1n⫹1␾wi(z)冘z写i⫽1n␾wi(z).Thispredictivedistributioncanbeaveragedovertheestimatesof␾yieldedbyasetofsamplesfromtheMCMCalgorithm.Fortheresultsdescribedinthearticle,weranthreeMarkovchainsfor1,600iterationsateachvalueofT,using␣⫽50/Tand␤⫽.01.Westartedsamplingafter800iterations,takingonesampleevery100iterationsthereafter.Thisgaveatotalof24samplesforeachchoiceofdimensionality.ThetopicsshowninFigure7aretakenfromasinglesamplefromtheMarkovchainforthe1,700-dimensionalmodel.Wecomputedanesti-mateof␾usingEquationA3andusedthesevaluestocomputeP(w2|w1)foreachsample;thenweaveragedtheresultsacrossallofthesamplestogetanestimateofthefullposteriorpredictivedistribution.Thisaverageddistributionwasusedinevaluatingthemodelontheword-associationdata.AppendixBTopicsandFeaturesTversky(1977)consideredanumberofdifferentmodelsforthesimilaritybetweentwostimuli,basedontheideaofcombiningcommonanddistinctivefeatures.Mostfamousisthecontrastmodel,inwhichthesimilaritybetweenxandy,S(x,y),isgivenbyS(x,y)⫽␪f(X艚Y)⫺␣f(Y⫺X)⫺␤f(X⫺Y),whereXisthesetoffeaturestowhichxbelongs,Yisthesetoffeaturestowhichybelongs,X艚Yisthesetofcommonfeatures,Y⫺Xisthesetofdistinctivefeaturesofy,f(䡠)isameasureoverthosesets,and␪,␣,and␤areparametersofthemodel.AnothermodelconsideredbyTversky,whichisalsoconsistentwiththeaxiomsusedtoderivethecontrastmodel,istheratiomodel,inwhichS(x,y)⫽1/冋1⫹␣f(Y⫺X)⫹␤f(X⫺Y)␪f(X艚Y)册.Asinthecontrastmodel,commonfeaturesincreasesimilarityanddistinctivefeaturesdecreasesimilarity.Theonlydifferencebe-tweenthetwomodelsistheformofthefunctionbywhichtheyarecombined.Tversky’s(1977)analysisassumesthatthefeaturesofxandyareknown.However,insomecircumstances,possessionofaparticularfeaturemaybeuncertain.Forsomehypotheticalfeatureh,wemightjusthaveaprobabilitythatxpossessesh,P(x僆h).Onemeansofdealingwiththisuncertaintyisreplacingf(䡠)withits(Appendixescontinue)243TOPICSINSEMANTICREPRESENTATIONexpectationwithrespecttotheprobabilitiesoffeaturepossession.Ifweassumethatf(䡠)islinear(asinadditiveclusteringmodels,e.g.,Shepard&Arabie,1979)andgivesuniformweighttoallfeatures,theratiomodelbecomesS(x,y)⫽1/冤1⫹␣冘h[1⫺P(x僆h)]P(y僆h)⫹␤冘h[1⫺P(y僆h)]P(x僆h)␪冘hP(x僆h)P(y僆h)冥,(B1)wherewetakeP(x僆h)tobeindependentforallxandh.Thesumsinthisequationreducetocountsofthecommonanddistinctivefeaturesiftheprobabilitiesalltakeonvaluesof0or1.Inthetopicmodel,semanticassociationisassessedintermsoftheconditionalprobabilityP(w2|w1).Withauniformprioronz,thisquantityreducestoP(w2兩w1)⫽冘zP(w2兩z)P(w1兩z)冘zP(w1兩z)⫽冘zP(w2兩z)P(w1兩z)冘zP(w2兩z)P(w1兩z)⫹冘z[1⫺P(w2兩z)]P(w1兩z)⫽1/冤1⫹冘z[1⫺P(w2兩z)]P(w1兩z)冘zP(w2兩z)P(w1兩z)冥,whichcanbeseentobeofthesameformastheprobabilisticratiomodelspecifiedinEquationB1,with␣⫽1,␤⫽0,␪⫽1,topicszintheplaceoffeaturesh,andP(w|z)replacingP(X僆h).ThisresultissimilartothatofTenenbaumandGriffiths(2001),whoshowedthattheirBayesianmodelofgeneralizationwasequivalenttotheratiomodel.AppendixCTheCollocationModelUsingthenotationintroducedabove,thecollocationmodelcanbewrittenaswi兩zi,xi⫽0,␾共zi)⬃Discrete(␾(zi))wi兩wi⫺1,xi⫽1,␾共wi⫺1)⬃Discrete共␾共wi⫺1))␾共z)⬃Dirichlet共␤兲␾共w)⬃Dirichlet共␦兲zi兩␪(di)⬃Discrete共␪共di))␪共d)⬃Dirichlet共␣兲xi兩wi⫺1⬃Discrete共␲共wi⫺1))␲共w)⬃Beta共␥0,␥1兲,where␾共wi)isthedistributionoverwi,givenwi-1,and␲共wi⫺1)isthedistributionoverxi,givenwi-1.TheGibbssamplerforthismodelisasfollows.Ifxi⫽0,thenziisdrawnfromthedistributionP(zi兩z⫺i,w,x)⬀nwi(zi)⫹␤n䡠(zi)⫹W␤nzidi⫹␣n⫹T␣,whereallcountsexcludethecurrentcaseandonlyrefertothewordsforwhichxi⫽0,whicharethewordsassignedtothetopicmodel(e.g.,nisthetotalnumberofwordsforwhichxi⫽0,notthetotalnumberofwordsinthecorpus).Ifxi⫽1,thenziissampledfromP(zi兩z⫺i,w,x)⬀nzidi⫹␣n⫹T␣,whereagainthecountsareonlyforthewordsforwhichxi⫽0.Finally,xiisdrawnfromthedistributionP(xi兩xⴚi,w,z)⬀冦nwi(zi)⫹␤n䡠(zi)⫹W␤n0(wi⫺1)⫹␥0n䡠(wi⫺1)⫹␥0⫹␥1xi⫽0nwi(wi⫺1)⫹␦n䡠(wi⫺1)⫹W␦n1(wi⫺1)⫹␥1n䡠(wi⫺1)⫹␥0⫹␥1xi⫽1,wheren0(wi⫺1)andn1(wi⫺1)arethenumberoftimesthewordwi-1hasbeendrawnfromatopicorformedpartofacollocation,respec-tively,andallcountsexcludethecurrentcase.Toestimatetheparametersofthemodelforeachsample,wecanagainusetheposteriormean.Theestimatorfor␾共z)isjust␾ˆ共z)fromEquationA3.Asimilarestimatorexistsforthedistributionasso-ciatedwithsuccessivewords:␾ˆw2(w1)⫽nw2(w1)⫹␦n䡠(w1)⫹W␦.For␲共w1),whichistheprobabilitythatx2⫽1givenw1,wehave␲ˆ共w1)⫽n1(w1)⫹␥1n䡠(w1)⫹␥0⫹␥1.Usingtheseestimates,Equation12becomesP(w2兩w1)⫽␲共w1)␾w2(w1)⫹(1⫺␲共w1))冘z␾w2(z)␾w1(z)冘z␾w1.Theresultsdescribedinthearticlewereaveragedover24samplesproducedbytheMCMCalgorithm,with␤⫽.01,␣⫽50/T,␥0⫽0.1,␥1⫽0.1,and␦⫽0.1.Thesampleswerecollectedfromthreechainsinthesamewayasforthebasictopicmodel.ReceivedApril29,2005RevisionreceivedOctober25,2006AcceptedOctober26,2006䡲244GRIFFITHS,STEYVERS,ANDTENENBAUM