 ACL  2007PRAGUEThe Association for Computational LinguisticsProceedings of the 45th Annual Meeting of the Association for Computational LinguisticsJune 23–30, 2007Prague, Czech RepublicACL  2007iii

Itiswithgreatsadnessthatwerecordthedeathon4April2007ofKarenSp¨arckJones,whoamongmanyotherhonorsandawards,wasthePresidentofACLin1994andrecipientoftheACLLifetimeAchievementAwardin2004.Theseproceedingsarededicatedtohermemory.iv

ProductionandManufacturingbyOmnipress2600AndersonStreetMadison,WI53704USAc(cid:13)2007AssociationforComputationalLinguisticsOrdercopiesofthisandotherACLproceedingsfrom:AssociationforComputationalLinguistics(ACL)209N.EighthStreetStroudsburg,PA18360USATel:+1-570-476-8006Fax:+1-570-476-0860acl@aclweb.orgTable of Contents

Preface: General Chair . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xv

Preface: Program Chairs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xvii

Organizers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xix

Program Committee . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .xxi

Conference Program . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xxv

Guiding Statistical Word Alignment Models With Prior Knowledge

Yonggang Deng and Yuqing Gao . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1

A Discriminative Syntactic Word Order Model for Machine Translation

Pi-Chuan Chang and Kristina Toutanova . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9

Tailoring Word Alignments to Syntactic Machine Translation

John DeNero and Dan Klein . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17

Transductive Learning for Statistical Machine Translation

Nicola Uefﬁng, Gholamreza Haffari and Anoop Sarkar . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25

Word Sense Disambiguation Improves Statistical Machine Translation

Yee Seng Chan, Hwee Tou Ng and David Chiang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33

Learning Expressive Models for Word Sense Disambiguation

Lucia Specia, Mark Stevenson and Maria das Grac¸as Volpe Nunes . . . . . . . . . . . . . . . . . . . . . . . . . . . 41

Domain Adaptation with Active Learning for Word Sense Disambiguation

Yee Seng Chan and Hwee Tou Ng. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .49

Making Lexical Ontologies Functional and Context-Sensitive

Tony Veale and Yanfen Hao . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57

A Bayesian Model for Discovering Typological Implications

Hal Daum´e III and Lyle Campbell . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65

A Discriminative Language Model with Pseudo-negative Samples

Daisuke Okanohara and Jun’ichi Tsujii . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73

v

Detecting Erroneous Sentences using Automatically Mined Sequential Patterns

Guihua Sun, Xiaohua Liu, Gao Cong, Ming Zhou, Zhongyang Xiong, John Lee and
Chin-Yew Lin . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81

Vocabulary Decomposition for Estonian Open Vocabulary Speech Recognition

Antti Puurula and Mikko Kurimo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89

Phonological Constraints and Morphological Preprocessing for Grapheme-to-Phoneme Conversion

Vera Demberg, Helmut Schmid and Gregor M¨ohler . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96

Redundancy Ratio: An Invariant Property of the Consonant Inventories of the World’s Languages

Animesh Mukherjee, Monojit Choudhury, Anupam Basu and Niloy Ganguly . . . . . . . . . . . . . . . . . 104

Multilingual Transliteration Using Feature based Phonetic Method

Su-Youn Yoon, Kyoung-Young Kim and Richard Sproat . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112

Semantic Transliteration of Personal Names

Haizhou Li, Khe Chai Sim, Jin-Shea Kuo and Minghui Dong . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120

Generating Complex Morphology for Machine Translation

Einat Minkov, Kristina Toutanova and Hisami Suzuki . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128

Assisting Translators in Indirect Lexical Transfer

Bogdan Babych, Anthony Hartley, Serge Sharoff and Olga Mudraya . . . . . . . . . . . . . . . . . . . . . . . . 136

Forest Rescoring: Faster Decoding with Integrated Language Models

Liang Huang and David Chiang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144

Statistical Machine Translation through Global Lexical Selection and Sentence Reconstruction

Srinivas Bangalore, Patrick Haffner and Stephan Kanthak . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152

Mildly Context-Sensitive Dependency Languages

Marco Kuhlmann and Mathias M¨ohl . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160

Transforming Projective Bilexical Dependency Grammars into efﬁciently-parsable CFGs with Unfold-
Fold

Mark Johnson. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .168

Parsing and Generation as Datalog Queries

Makoto Kanazawa . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176

Optimizing Grammars for Minimum Dependency Length

Daniel Gildea and David Temperley . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184

Generalizing Semantic Role Annotations Across Syntactically Similar Verbs

Andrew Gordon and Reid Swanson . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192

A Grammar-driven Convolution Tree Kernel for Semantic Role Classiﬁcation

Min Zhang, Wanxiang Che, Aiti Aw, Chew Lim Tan, Guodong Zhou, Ting Liu and Sheng Li . . 200

vi

Learning Predictive Structures for Semantic Role Labeling of NomBank

Chang Liu and Hwee Tou Ng . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208

A Simple, Similarity-based Model for Selectional Preferences

Katrin Erk . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216

SVM Model Tampering and Anchored Learning: A Case Study in Hebrew NP Chunking

Yoav Goldberg and Michael Elhadad . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224

Fully Unsupervised Discovery of Concept-Speciﬁc Relationships by Web Mining

Dmitry Davidov, Ari Rappoport and Moshe Koppel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 232

Adding Noun Phrase Structure to the Penn Treebank

David Vadas and James Curran . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 240

Formalism-Independent Parser Evaluation with CCG and DepBank

Stephen Clark and James Curran . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248

Frustratingly Easy Domain Adaptation

Hal Daum´e III . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256

Instance Weighting for Domain Adaptation in NLP

Jing Jiang and ChengXiang Zhai . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 264

The Inﬁnite Tree

Jenny Rose Finkel, Trond Grenager and Christopher D. Manning . . . . . . . . . . . . . . . . . . . . . . . . . . . 272

Guiding Semi-Supervision with Constraint-Driven Learning

Ming-Wei Chang, Lev Ratinov and Dan Roth . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 280

Supertagged Phrase-Based Statistical Machine Translation

Hany Hassan, Khalil Sima’an and Andy Way . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 288

Regression for Sentence-Level MT Evaluation with Pseudo References

Joshua Albrecht and Rebecca Hwa . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 296

Bootstrapping Word Alignment via Word Packing

Yanjun Ma, Nicolas Stroppa and Andy Way. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .304

Improved Word-Level System Combination for Machine Translation

Antti-Veikko Rosti, Spyros Matsoukas and Richard Schwartz . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 312

Generating Constituent Order in German Clauses

Katja Filippova and Michael Strube . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 320

A Symbolic Approach to Near-Deterministic Surface Realisation using Tree Adjoining Grammar

Claire Gardent and Eric Kow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 328

Sentence Generation as a Planning Problem

Alexander Koller and Matthew Stone. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .336

vii

GLEU: Automatic Evaluation of Sentence-Level Fluency

Andrew Mutton, Mark Dras, Stephen Wan and Robert Dale . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 344

Conditional Modality Fusion for Coreference Resolution

Jacob Eisenstein and Randall Davis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 352

The Utility of a Graphical Representation of Discourse Structure in Spoken Dialogue Systems

Mihai Rotaru and Diane Litman . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 360

Automated Vocabulary Acquisition and Interpretation in Multimodal Conversational Systems

Yi Liu, Joyce Chai and Rong Jin . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 368

A Multimodal Interface for Access to Content in the Home

Michael Johnston, Luis Fernando D’Haro, Michelle Levine and Bernard Renger . . . . . . . . . . . . . . 376

Fast Unsupervised Incremental Parsing

Yoav Seginer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 384

K-best Spanning Tree Parsing

Keith Hall . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 392

Is the End of Supervised Parsing in Sight?

Rens Bod. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .400

An Ensemble Method for Selection of High Quality Parses

Roi Reichart and Ari Rappoport . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 408

Opinion Mining using Econometrics: A Case Study on Reputation Systems

Anindya Ghose, Panagiotis Ipeirotis and Arun Sundararajan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 416

PageRanking WordNet Synsets: An Application to Opinion Mining

Andrea Esuli and Fabrizio Sebastiani . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 424

Structured Models for Fine-to-Coarse Sentiment Analysis

Ryan McDonald, Kerry Hannan, Tyler Neylon, Mike Wells and Jeff Reynar . . . . . . . . . . . . . . . . . . 432

Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classiﬁcation

John Blitzer, Mark Dredze and Fernando Pereira . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 440

Clustering Clauses for High-Level Relation Detection: An Information-theoretic Approach

Samuel Brody . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 448

Instance-based Evaluation of Entailment Rule Acquisition

Idan Szpektor, Eyal Shnarch and Ido Dagan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 456

Statistical Machine Translation for Query Expansion in Answer Retrieval

Stefan Riezler, Alexander Vasserman, Ioannis Tsochantaridis, Vibhu Mittal and Yi Liu . . . . . . . . 464

A Computational Model of Text Reuse in Ancient Literary Texts

John Lee . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 472

viii

Finding Document Topics for Improving Topic Segmentation

Olivier Ferret . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 480

The Utility of Parse-derived Features for Automatic Discourse Segmentation

Seeger Fisher and Brian Roark . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 488

PERSONAGE: Personality Generation for Dialogue

Franc¸ois Mairesse and Marilyn Walker . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 496

Making Sense of Sound: Unsupervised Topic Segmentation over Acoustic Input

Igor Malioutov, Alex Park, Regina Barzilay and James Glass . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 504

Randomised Language Modelling for Statistical Machine Translation

David Talbot and Miles Osborne . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 512

Bilingual-LSA Based LM Adaptation for Spoken Language Translation

Yik-Cheung Tam, Ian Lane and Tanja Schultz . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 520

Coreference Resolution Using Semantic Relatedness Information from Automatically Discovered Patterns
Xiaofeng Yang and Jian Su . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 528

Semantic Class Induction and Coreference Resolution

Vincent Ng . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 536

Generating a Table-of-Contents

S. R. K. Branavan, Pawan Deshpande and Regina Barzilay . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 544

Towards an Iterative Reinforcement Approach for Simultaneous Document Summarization and Keyword
Extraction

Xiaojun Wan, Jianwu Yang and Jianguo Xiao . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 552

Fast Semantic Extraction Using a Novel Neural Network Architecture

Ronan Collobert and Jason Weston. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .560

Improving the Interpretation of Noun Phrases with Cross-linguistic Information

Roxana Girju . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 568

Learning to Extract Relations from the Web using Minimal Supervision

Razvan Bunescu and Raymond Mooney . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 576

A Seed-driven Bottom-up Machine Learning Framework for Extracting Relations of Various Complexity
Feiyu Xu, Hans Uszkoreit and Hong Li . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 584

A Multi-resolution Framework for Information Extraction from Free Text

Mstislav Maslennikov and Tat-Seng Chua . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 592

Using Corpus Statistics on Entities to Improve Semi-supervised Relation Extraction from the Web

Benjamin Rosenfeld and Ronen Feldman . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 600

ix

Beyond Projectivity: Multilingual Evaluation of Constraints and Measures on Non-Projective Structures
Jiˇr´ı Havelka. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .608

Self-Training for Enhancement and Domain Adaptation of Statistical Parsers Trained on Small Datasets
Roi Reichart and Ari Rappoport . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 616

HPSG Parsing with Shallow Dependency Constraints

Kenji Sagae, Yusuke Miyao and Jun’ichi Tsujii. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .624

Constituent Parsing with Incremental Sigmoid Belief Networks

Ivan Titov and James Henderson . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 632

Corpus Effects on the Evaluation of Automated Transliteration Systems

Sarvnaz Karimi, Andrew Turpin and Falk Scholer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 640

Collapsed Consonant and Vowel Models: New Approaches for English-Persian Transliteration and Back-
Transliteration

Sarvnaz Karimi, Falk Scholer and Andrew Turpin . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 648

Alignment-Based Discriminative String Similarity

Shane Bergsma and Grzegorz Kondrak . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 656

Bilingual Terminology Mining - Using Brain, not Brawn Comparable Corpora

Emmanuel Morin, B´eatrice Daille, Koichi Takeuchi and Kyo Kageura . . . . . . . . . . . . . . . . . . . . . . . 664

Unsupervised Language Model Adaptation Incorporating Named Entity Information

Feifan Liu and Yang Liu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 672

Coordinate Noun Phrase Disambiguation in a Generative Parsing Model

Deirdre Hogan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 680

A Uniﬁed Tagging Approach to Text Normalization

Conghui Zhu, Jie Tang, Hang Li, Hwee Tou Ng and Tiejun Zhao . . . . . . . . . . . . . . . . . . . . . . . . . . . . 688

Sparse Information Extraction: Unsupervised Language Models to the Rescue

Doug Downey, Stefan Schoenmackers and Oren Etzioni. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .696

Forest-to-String Statistical Translation Rules

Yang Liu, Yun Huang, Qun Liu and Shouxun Lin . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 704

Ordering Phrases with Function Words

Hendra Setiawan, Min-Yen Kan and Haizhou Li . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 712

A Probabilistic Approach to Syntax-based Reordering for Statistical Machine Translation

Chi-Ho Li, Minghui Li, Dongdong Zhang, Mu Li, Ming Zhou and Yi Guan . . . . . . . . . . . . . . . . . . 720

Machine Translation by Triangulation: Making Effective Use of Multi-Parallel Corpora

Trevor Cohn and Mirella Lapata . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 728

x

A Maximum Expected Utility Framework for Binary Sequence Labeling

Martin Jansche . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 736

A Fully Bayesian Approach to Unsupervised Part-of-Speech Tagging

Sharon Goldwater and Tom Grifﬁths . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 744

Computationally Efﬁcient M-Estimation of Log-Linear Structure Models

Noah A. Smith, Douglas L. Vail and John D. Lafferty . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 752

Guided Learning for Bidirectional Sequence Classiﬁcation

Libin Shen, Giorgio Satta and Aravind Joshi . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 760

Different Structures for Evaluating Answers to Complex Questions: Pyramids Won’t Topple, and Neither
Will Human Assessors

Hoa Trang Dang and Jimmy Lin . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 768

Exploiting Syntactic and Shallow Semantic Kernels for Question Answer Classiﬁcation

Alessandro Moschitti, Silvia Quarteroni, Roberto Basili and Suresh Manandhar . . . . . . . . . . . . . . 776

Language-independent Probabilistic Answer Ranking for Question Answering

Jeongwoo Ko, Teruko Mitamura and Eric Nyberg . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 784

Learning to Compose Effective Strategies from a Library of Dialogue Components

Martijn Spitters, Marco De Boni, Jakub Zavrel and Remko Bonnema . . . . . . . . . . . . . . . . . . . . . . . . 792

On the Role of Context and Prosody in the Interpretation of ’Okay’

Agust´ın Gravano, Stefan Benus, H´ector Ch´avez, Julia Hirschberg and Lauren Wilcox . . . . . . . . . 800

Predicting Success in Dialogue

David Reitter and Johanna D. Moore . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 808

Resolving It, This, and That in Unrestricted Multi-Party Dialog

Christoph M¨uller . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 816

A Comparative Study of Parameter Estimation Methods for Statistical Natural Language Processing

Jianfeng Gao, Galen Andrew, Mark Johnson and Kristina Toutanova . . . . . . . . . . . . . . . . . . . . . . . . 824

Grammar Approximation by Representative Sublanguage: A New Model for Language Learning

Smaranda Muresan and Owen Rambow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 832

Chinese Segmentation with a Word-Based Perceptron Algorithm

Yue Zhang and Stephen Clark . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 840

Unsupervised Coreference Resolution in a Nonparametric Bayesian Model

Aria Haghighi and Dan Klein. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .848

Pivot Language Approach for Phrase-Based Statistical Machine Translation

Hua Wu and Haifeng Wang . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 856

xi

Bootstrapping a Stochastic Transducer for Arabic-English Transliteration Extraction

Tarek Sherif and Grzegorz Kondrak . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 864

Beneﬁts of the Passively Parallel Rosetta Stone? Cross-Language Information Retrieval with over 30
Languages

Peter Chew and Ahmed Abdelali . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 872

A Re-examination of Machine Learning Approaches for Sentence-Level MT Evaluation

Joshua Albrecht and Rebecca Hwa . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 880

Automatic Acquisition of Ranked Qualia Structures from the Web

Philipp Cimiano and Johanna Wenderoth . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 888

A Sequencing Model for Situation Entity Classiﬁcation

Alexis Palmer, Elias Ponvert, Jason Baldridge and Carlota Smith . . . . . . . . . . . . . . . . . . . . . . . . . . . . 896

Words and Echoes: Assessing and Mitigating the Non-Randomness Problem in Word Frequency Distri-
bution Modeling

Baroni Marco and Evert Stefan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 904

A System for Large-Scale Acquisition of Verbal, Nominal and Adjectival Subcategorization Frames from
Corpora

Judita Preiss, Ted Briscoe and Anna Korhonen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 912

A Language-Independent Unsupervised Model for Morphological Segmentation

Vera Demberg . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 920

Using Mazurkiewicz Trace Languages for Partition-Based Morphology

Franc¸ois Barth´elemy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 928

Much ado about nothing: A Social Network Model of Russian Paradigmatic Gaps

Robert Daland, Andrea D. Sims and Janet Pierrehumbert . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 936

Substring-Based Transliteration

Tarek Sherif and Grzegorz Kondrak . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 944

Pipeline Iteration

Kristy Hollingshead and Brian Roark. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .952

Learning Synchronous Grammars for Semantic Parsing with Lambda Calculus

Yuk Wah Wong and Raymond Mooney . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 960

Generalizing Tree Transformations for Inductive Dependency Parsing

Jens Nilsson, Joakim Nivre and Johan Hall . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 968

Learning Multilingual Subjective Language via Cross-Lingual Projections

Rada Mihalcea, Carmen Banea and Janyce Wiebe . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 976

Sentiment Polarity Identiﬁcation in Financial News: A Cohesion-based Approach

Ann Devitt and Khurshid Ahmad . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 984

xii

Weakly Supervised Learning for Hedge Classiﬁcation in Scientiﬁc Literature

Ben Medlock and Ted Briscoe . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 992

Text Analysis for Automatic Image Annotation

Koen Deschacht and Marie-Francine Moens . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1000

User Requirements Analysis for Meeting Information Retrieval Based on Query Elicitation

Vincenzo Pallotta, Violeta Seretan and Marita Ailomaa . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1008

Combining Multiple Knowledge Sources for Dialogue Segmentation in Multimedia Archives

Pei-Yun Hsueh and Johanna D. Moore . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1016

Topic Analysis for Psychiatric Document Retrieval

Liang-Chih Yu, Chung-Hsien Wu, Chin-Yew Lin, Eduard Hovy and Chia-Ling Lin . . . . . . . . . . 1024

What to be? - Electronic Career Guidance Based on Semantic Relatedness

Iryna Gurevych, Christof M¨uler and Torsten Zesch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1032

Extracting Social Networks and Biographical Facts From Conversational Speech Transcripts

Hongyan Jing, Nanda Kambhatla and Salim Roukos . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1040

Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1049

xiii

xv

Preface:GeneralChairOnbehalfoftheorganizingcommitteeIamdelightedtowelcomeyoutothe45thAnnualMeetingoftheAssociationforComputationalLinguistics,inPrague.SettingupandrunningtheACLconferenceinvolvesalotofworkbymanypeople.Someofthemareofﬁciallyidentiﬁedasbeingresponsibleforvariousaspectsoftheconference,whilethecontributionsofothersarelessvisible.Iwouldliketosayawarmthankyoutothepeoplenamedbelow,withapologiestoanyoneIhaveoverlooked.TheProgramChairs,AntalvandenBoschandAnnieZaenenhavedoneagreatjobinmanagingthealmost600submissionsforthemainconferenceandputtingtogetherahighqualityprogram.ThroughthisprocessAntalhasbecomea‘grandmaster’oftheSTARTpapermanagementsystemandhasgivenalotofhelptootherchairswhohavehadtodealwiththeirownsetsofsubmissions.ManythanksalsototheirAreaChairsandtheprogramcommitteeofreviewers,andtoFlorenceReederforcoordinatingthepre-submissionmentoringservice.(AntalandAnniereﬂectontheirPCexperienceoverleaf).SophiaAnaniadouisChairoftheDemo/Posterpartoftheconference,andhasoverseenaseparatereviewprocesstoselectahighqualitysetofpresentations.TheStudentResearchWorkshopChairs,ChrisBiemann,VioletaSeretanandEllenRiloffhaveassem-bledanexcellentprogramofpapersandposters.Iencourageeveryonetoattendthestudentworkshoptohearabouttheexcitingworkbeingcarriedoutbyresearchersjuststartingoutontheircareersincompu-tationallinguistics.WorkshopsChairSimoneTeufelisoverseeing15workshops–themosteveratanACLconference–chosen(withthehelpofBethAnnHockey,KatjaMarkertandDekaiWu)fromatotalof27proposals.Thescaleoftheworkshopprogramcanbegaugedbythefactthattheyreceivedanaggregatetotalof470submissions(withoutevencountingIWPTandEMNLP-CoNLL).JoakimNivre,asTutorialsChair,hasassembledaprogramof5attractiveandcomplementarytutorials,selectedfrom20proposalswithadvicefromWalterDaelemans,RobertDale,NancyIde,DianeLitmanandChrisManning.OneofthemostdemandingyetleastnoticedorganizationalrolesisthatofPublicationsChair.SuJianhasdoneafantasticjobinproducingthehardcopyandelectronicrecordoftheconference,supportedbyhisteam—withnotablecontributionsfromUpaliKohombanwhohascheerfullyhelpedatallstagesandwheneverneeded,dayandnight,weekdaysandweekends.Sponsorshipisanothersuccessstory,thankstotheSponsorshipChairsMarthaPalmer,GaborProszeky,JanHajicandJun’ichiTsujii,whohaverecruited12corporatesponsors.Weareverygratefulfortheirﬁnancialsupport.EvaHajicova,theLocalArrangementsChair,assistedbyJanHajicandAnnaKotesovcova,haveputinanenormousamountofdetailedworktomakethisconferenceasuccess,ablysupportedbythelo-calteamofMilanFucik,JaroslavaHlavacova,MarketaLopatkova,JiriMirovsky,PavelPecina,xvi

PavelSchlesinger,JurajSimlovic,MiroslavSpousta,PavelStranak,ZlatkaSubrova,JanVotrubec,ZdenekZabokrtskyandDanielZeman.FromtheACLitself,KathyMcCoy,DragomirRadev,PriscillaRasmussen,MarkSteedmanandJun’ichiTsujiihaveplayedstrongrolesinmakingdeci-sionsandgivingadvice,andkepteverythingontrackwhileIwasoutofactionduetoillhealthlastyear.Andﬁnally,manythankstotheauthorsandpresentersinthemainconference,workshopsandco-locatedevents...andtoalltheparticipants.Ihopeyouenjoytheformallyorganizedaspectsoftheconference,takeadvantageoftheopportunitytonetworkwithcolleaguesoldandnew,andthatyoualsohaveachancetoappreciatethehistoryandsightsofPraguewhileyouarehere.JohnCarrollACL2007GeneralChairxvii

Preface:ProgramChairsThenumberofsubmissionsforthisACLbrokeanewrecord:theprogramcommittee’sselectionof131paperswasbasedon588submissions(afterwithdrawals).Anupdatedprogramdesignwithfourparallelsessionsand25-minutepapersallowedanacceptancerateof22.3%,andanacceptanceofallsubmissionsthatwererecommendedwithprioritybytheareachairs.Firstandforemost,wethankalltheauthorsforsubmittingpapersdescribingtheirrecentwork;thesheeramountofsubmissionsreﬂectshowactiveourﬁeldis.WethankFlorenceReederforprovidedmentoringto17authorteamswhofelttheyneededsomewritingsupport.Fortheselection,weareindebtedtothe332programcommitteemembers,whoproducedonetoelevenreviewsperreviewer,foratotalofcloseto1,800reviews,andtothetenareachairsonwhoseshouldersrestedmostoftheworkoforganizingthereviewprocess.Wedecidedtoworkwithoutanareachairsmeeting:thetwoprogramco-chairsmetfortwodaysatTilburgUniversity,andinteractedduringthattimevigorouslywiththeareachairsbyemailandsometimesbyphone.Asusualthemainprogramwillrunforthreedays:therewillbefourparallelsessionsofmainsessionpre-sentations,ademo/postersessionorganizedbySophiaAnaniadou,nameMiroslavSpoustaandZdenekZabokrtsky,andaStudentResearchWorkshop–thankstoEllenRiloff,VioletaSeretanandChrisBiemannfororganizingit.Alsoasusualtheconferenceisﬂankedbytutorialsessionsandworkshops;ourthanksgotoJoakimNivreandSimoneTeufelfororganizingandcompilinganexcellentpackage.TheannouncementsoftheACLLifetimeAwardandoftheBestPaperAwardwillprovidethecustomarysuspense.Theywilltakeplaceinplenarysessions.Otherplenarysessionswillbedevotedtothebusinessmeetingandthetwoinvitedtalks,whichthisyearwillbedeliveredbyTomMitchellandBarneyPell.Wearegratefulfortheirkindacceptationofourinvitation.WethankJohnCarroll,GeneralConferenceChair,theLocalArrangementsCommitteeheadedbyEvaHajicova,andtheACLexecutive,especiallyDragomirRadev,fortheirhelpandadvice,andlastyear’sco-chairs,ClaireCardieandPierreIsabelle,forsharingtheirexperience.OursincerethanksgotoSuJianforputtingtogethertheproceedings.Enjoytheconference,AntalvandenBoschandAnnieZaenenACL-2007ProgramChairsxix

OrganizersGeneralChair:JohnCarroll,UniversityofSussex,UKLocalArrangementsChair:EvaHajicova,CharlesUniversity,CzechRepublicProgramChairs:AntalvandenBosch,TilburgUniversity,TheNetherlandsAnnieZaenen,PaloAltoResearchCenter(PARC),USAStudentResearchWorkshop:VioletaSeretan,UniversityofGeneva,SwitzerlandChrisBiemann,UniversityofLeipzig,GermanyEllenRiloff(FacultyAdvisor),UniversityofUtah,USAWorkshopChair:SimoneTeufel,UniversityofCambridge,UKTutorialChair:JoakimNivre,V¨axj¨oUniversity,SwedenDemo/PosterChair:SophiaAnaniadou,TheUniversityofManchester,UKExhibitsChairs:JaroslavaHlavacova,CharlesUniversity,CzechRepublicPavelPecina,CharlesUniversity,CzechRepublicSponsorshipChairs:MarthaPalmer,UniversityofColorado,USAGaborProszeky,Morphologic,HungaryJanHajic,CharlesUniversity,CzechRepublicJun’ichiTsujii,UniversityofTokyo,Japanxx

PublicityChairs:PavelStranak,CharlesUniversity,CzechRepublicJiriMirovsky,CharlesUniversity,CzechRepublicPublicationChair:SuJian,InstituteforInfocommResearch(I2R),SingaporeMentoringService:FlorenceReeder,TheMITRECorporation,USAStudentVolunteers:MarketaLopatkova,CharlesUniversity,CzechRepublicWebmasters:ZlatkaSubrova,CzechRepublicJurajSimlovic,CzechRepublicSecretariat:AnnaKotesovcova,CharlesUniversity,CzechRepublicRegistration:PriscillaRasmussen,AssociationforComputationalLinguistics(ACL)ACLExecutiveCommittee:MarkSteedman,UniversityofEdinburgh,UKBonnieDorr,UniversityofMaryland,USAStevenBird,UniversityofMelbourne,AustraliaJun’ichiTsujii,TheUniversityofTokyo,JapanKathleenF.McCoy,UniversityofDelaware,USADragomirR.Radev,UniversityofMichigan,USAOwenRambow,ColumbiaUniversity,USAAlexLascarides,UniversityofEdinburgh,UKKeh-YihSu,BehaviorDesignCorporation,TaiwanClaireCardie,CornellUniversity,USANicolettaCalzolari,IstitutodiLinguisticaComputazionaledelCNR,Italyxxi

ProgramCommitteeProgramChairs:AntalvandenBosch,TilburgUniversity,TheNetherlandsAnnieZaenen,PaloAltoResearchCenter,USAAreaChairs:TimBaldwin,UniversityofMelbourne,AustraliaKeesvanDeemter,UniversityofAberdeen,UKBarbaraDiEugenio,UniversityofIllinoisatChicago,USAJosefvanGenabith,DublinCityUniversity,IrelandClaireGrover,UniversityofEdinburgh,UKDianaMcCarthy,UniversityofSussex,UKDanRoth,UniversityofIllinoisatUrbana-Champaign,USARichardSproat,UniversityofIllinoisatUrbana-Champaign,USAMarcSwerts,TilburgUniversity,TheNetherlandsAndyWay,DublinCityUniversity,IrelandProgramCommitteeMembers:TakeshiAbekawa,EnekoAgirre,GregoryAist,CyrilAllauzen,ElisabethAndr´e,ShlomoArga-mon,RonArtstein,CollinBaker,SrinivasBangalore,ColinBannard,ReginaBarzilay,RobertoBasili,JohnBate-man,KennethBeesley,AnjaBelz,SlavenBilac,StevenBird,GuidoBoella,FrancisBond,KalinaBontcheva,JohanBos,GosseBouma,SusanBrennan,ChrisBrew,TedBriscoe,RalfBrown,PaulBuitelaar,RazvanBunescu,HarryBunt,AoifeCahill,JanetCahn,MaryElaineCaliff,CharlesCallaway,ChrisCallison-Burch,NicolettaCalzolari,GiuseppeCarenini,MichaelCarl,JeanCarletta,XavierCarreras,JustineCassell,JoyceChai,Jing-ShinChang,Ming-WeiChang,JinyingChen,Keh-JiannChen,ColinCherry,DavidChiang,GrzegorzChrupala,JenniferChu-Carroll,IlyasCicekli,PhilippCimiano,StephenClark,MichaelCollins,MichaelConnor,MarkCore,MathiasCreutz,JamesCurran,WalterDaelemans,IdoDagan,HerculesDalianis,HalDaume,EricdelaClergerie,MaartendeRijke,BarbaraDiEugenio,MonaDiab,GaelDiaz,BonnieDorr,LailaDybkjær,JasonEisner,NoemieElhadad,KatrinErk,DominiqueEstival,StefanEvert,xxii

DavidFarwell,AfsanehFazly,MarcelloFederico,KatjaFilippova,KateForbesRiley,MikelFor-cada,GeorgeFoster,MaryEllenFoster,AnetteFrank,RevaFreedman,GuohongFu,PascaleFung,RobGaizauskas,JianfengGao,KallirroiGeorgila,DanielGildea,RoxanaGirju,AlﬁoGliozzo,SharonGoldwater,FernandoGomez,NancyGreen,MarkGreenwood,RalphGrishman,DeclanGroves,NizarHabash,KeithHall,SusanHaller,SandaHarabagiu,HenkHarkema,TonyHartley,SvenHartrumpf,MaryHearne,MartiHearst,PeterHeeman,JamesHenderson,MarkHepple,RyuichiroHigashinaka,JuliaHirschberg,GraemeHirst,BarboraHladka,JuliaHockenmaier,DeirdreHogan,BadenHughes,DianaInkpen,KentaroInui,MartinJansche,ValentinJijkoun,MarkJohnson,KristiinaJokinen,PamelaJordan,LauraKallmeyer,StephanKanthak,VangelisKarkaletsis,DaisukeKawahara,JohnKelleher,FrankKeller,AndreKempe,RodgerKibble,AdamKilgarriff,TracyKing,EwanKlein,AlexandreKle-mentiev,KevinKnight,AlistairKnott,PhilippKoehn,RobKoeling,AlexanderKoller,GrzegorzKondrak,MosheKoppel,ValiaKordoni,AnnaKorhonen,EmielKrahmer,SandraKuebler,JonasKuhn,SadaoKurohashi,PhilippeLanglais,GuyLapalme,MirellaLapata,StaffanLarsson,AlbertoLavelli,AlonLavie,Lil-lianLee,JochenLeidner,OliverLemon,YvesLepage,LeonardoLesmo,GinaLevow,IanLewin,WilliamLewis,MuLi,DekangLin,JimmyLin,DianeLitman,TingLiu,SaturninoLuz,BernardoMagnini,GideonMann,ChrisManning,DanielMarcu,KatjaMarkert,LluisMarquez,ErwinMarsi,DavidMartinez,CarlosMartin-Vide,EvgenyMatusov,JohnMaxwell,MikeMaxwell,DianaMaynard,AndrewMcCallum,RyanMcDonald,KathyMcKeown,SusanMcRoy,MikeMcTear,DanMelamed,ChrisMellish,ArulMenezes,HelenMeng,PaolaMerlo,DetmarMeur-ers,RadaMihalcea,MariaMilosavljevic,EleniMiltsakaki,DavidMilward,YusukeMiyao,DanMoldovan,DiegoMolla,RaymondMooney,RogerMoore,AlessandroMoschitti,KarinMueller,ReinhardMuskens,HiroshiNakagawa,HiromiNakaiwa,RobertoNavigli,Mark-JanNederhof,GoranNenadic,JohnNerbonne,HweeTouNg,VincentNg,Jian-YunNie,MalvinaNissim,JoakimNivre,DavidNovick,JonOberlander,FranzOch,StephanOepen,KemalOﬂazer,IanO’Neill,MilesOsborne,MariOstendorf,TimPaek,MarthaPalmer,PatrickPantel,IvandreParaboni,BeckyPassonneau,MichaelPaul,FernandoPereira,ScottPiao,ManfredPinkal,PaulPiwek,MassimoPoesio,JoePolifroni,RichardPower,JohnPrager,StephenPulman,VasinPunyakanok,ChrisQuirk,xxiii

ReinhardRapp,LevRatinov,EhudReiter,MartinReynaert,StefanRiezler,GermanRigau,EllenRiloff,BrianRoark,LaurentRomary,BogdanSacaleanu,HoracioSaggion,AnnaSagvallHein,MarkSanderson,AnoopSarkar,AvikSarkar,HelmutSchmid,GeroldSchneider,PatrickSchone,HinrichSchuetze,SabineSchulteimWalde,SatoshiSekine,StephanieSeneff,IzhakShafran,AdvaithSiddharthan,KhalilSimaan,KevinSmall,NoahA.Smith,HaroldSomers,VirachSornlertlamvanich,ManfredStede,MarkSteedman,AmandaStent,MarkStevenson,CarloStrapparava,OliverStreiter,NicolasStroppa,MichaelStrube,JianSu,RajenSubba,StanSzpakowicz,MaiteTaboada,HiroyaTakamura,KumikoTanaka-Ishii,LouistenBosch,JoelTetreault,SimoneTeufel,MarietTheune,JoergTiedemann,TakenobuTokunaga,KristinaToutanova,IsabelTran-coso,RichardTsai,GokhanTur,NicolaUefﬁng,NathanVaillette,GertjanvanNoord,MennovanZaanen,SebastianVarges,PaolaVelardi,RenataVieira,BegonaVilladaMoiron,CarlVogel,StephanVogel,PiekVossen,MarilynWalker,HaifengWang,XinglongWang,BonnieWebber,DavidWeir,BenWellner,RichardWicentowski,JanyceWiebe,RossWilkinson,YorickWilks,TheresaWilson,ShulyWintner,DekaiWu,FeiXia,NianwenXue,EndongXun,MuyunYang,ScottWen-tauYih,AnssiYli-Jyr¨a,DmitryZelenko,RichardZens,ChengXiangZhai,MinZhang,TongZhang,GuodongZhouConference Program

Monday, June 25, 2007

8:45–9:00

Opening Session

9:00–10:00

Invited Talk by Tom Mitchell

10:00–10:30 Break

Session 1A: Machine Translation 1

10:30–10:55 Guiding Statistical Word Alignment Models With Prior Knowledge

Yonggang Deng and Yuqing Gao

10:55–11:20

A Discriminative Syntactic Word Order Model for Machine Translation
Pi-Chuan Chang and Kristina Toutanova

11:20–11:45

Tailoring Word Alignments to Syntactic Machine Translation
John DeNero and Dan Klein

11:45–12:10

Transductive Learning for Statistical Machine Translation
Nicola Uefﬁng, Gholamreza Haffari and Anoop Sarkar

Session 1B: Word Sense Disambiguation

10:30–10:55 Word Sense Disambiguation Improves Statistical Machine Translation

Yee Seng Chan, Hwee Tou Ng and David Chiang

10:55–11:20

Learning Expressive Models for Word Sense Disambiguation
Lucia Specia, Mark Stevenson and Maria das Grac¸as Volpe Nunes

11:20–11:45 Domain Adaptation with Active Learning for Word Sense Disambiguation

Yee Seng Chan and Hwee Tou Ng

11:45–12:10 Making Lexical Ontologies Functional and Context-Sensitive

Tony Veale and Yanfen Hao

xxv

Monday, June 25, 2007 (continued)

Session 1C: Language Modeling 1

10:30–10:55

A Bayesian Model for Discovering Typological Implications
Hal Daum´e III and Lyle Campbell

10:55–11:20

A Discriminative Language Model with Pseudo-negative Samples
Daisuke Okanohara and Jun’ichi Tsujii

11:20–11:45 Detecting Erroneous Sentences using Automatically Mined Sequential Patterns

Guihua Sun, Xiaohua Liu, Gao Cong, Ming Zhou, Zhongyang Xiong, John Lee and Chin-
Yew Lin

11:45–12:10

Vocabulary Decomposition for Estonian Open Vocabulary Speech Recognition
Antti Puurula and Mikko Kurimo

Session 1D: Phonology and Morphology 1

10:30–10:55

Phonological Constraints and Morphological Preprocessing for Grapheme-to-Phoneme
Conversion
Vera Demberg, Helmut Schmid and Gregor M¨ohler

10:55–11:20

Redundancy Ratio: An Invariant Property of the Consonant Inventories of the World’s
Languages
Animesh Mukherjee, Monojit Choudhury, Anupam Basu and Niloy Ganguly

11:20–11:45 Multilingual Transliteration Using Feature based Phonetic Method

Su-Youn Yoon, Kyoung-Young Kim and Richard Sproat

11:45–12:10

Semantic Transliteration of Personal Names
Haizhou Li, Khe Chai Sim, Jin-Shea Kuo and Minghui Dong

12:10–13:30

Lunch

Session 2A: Machine Translation 2

13:30–13:55 Generating Complex Morphology for Machine Translation

Einat Minkov, Kristina Toutanova and Hisami Suzuki

13:55–14:20

Assisting Translators in Indirect Lexical Transfer
Bogdan Babych, Anthony Hartley, Serge Sharoff and Olga Mudraya

xxvi

Monday, June 25, 2007 (continued)

14:20–14:45

Forest Rescoring: Faster Decoding with Integrated Language Models
Liang Huang and David Chiang

14:45–15:10

Statistical Machine Translation through Global Lexical Selection and Sentence Recon-
struction
Srinivas Bangalore, Patrick Haffner and Stephan Kanthak

Session 2B: Grammars

13:30–13:55 Mildly Context-Sensitive Dependency Languages

Marco Kuhlmann and Mathias M¨ohl

13:55–14:20

Transforming Projective Bilexical Dependency Grammars into Efﬁciently-parsable CFGs
with Unfold-Fold
Mark Johnson

14:20–14:45

Parsing and Generation as Datalog Queries
Makoto Kanazawa

14:45–15:10 Optimizing Grammars for Minimum Dependency Length

Daniel Gildea and David Temperley

Session 2C: Semantic Role Labeling

13:30–13:55 Generalizing Semantic Role Annotations Across Syntactically Similar Verbs

Andrew Gordon and Reid Swanson

13:55–14:20

A Grammar-driven Convolution Tree Kernel for Semantic Role Classiﬁcation
Min Zhang, Wanxiang Che, Aiti Aw, Chew Lim Tan, Guodong Zhou, Ting Liu and Sheng
Li

14:20–14:45

Learning Predictive Structures for Semantic Role Labeling of NomBank
Chang Liu and Hwee Tou Ng

14:45–15:10

A Simple, Similarity-based Model for Selectional Preferences
Katrin Erk

xxvii

Monday, June 25, 2007 (continued)

Session 2D: Language Resources

13:30–13:55

SVM Model Tampering and Anchored Learning: A Case Study in Hebrew NP Chunking
Yoav Goldberg and Michael Elhadad

13:55–14:20

Fully Unsupervised Discovery of Concept-Speciﬁc Relationships by Web Mining
Dmitry Davidov, Ari Rappoport and Moshe Koppel

14:20–14:45

Adding Noun Phrase Structure to the Penn Treebank
David Vadas and James Curran

14:45–15:10

Formalism-Independent Parser Evaluation with CCG and DepBank
Stephen Clark and James Curran

15:10–15:45 Break

Session 3A, Machine Learning Methods 1

15:45–16:10

Frustratingly Easy Domain Adaptation
Hal Daum´e III

16:10–16:35

Instance Weighting for Domain Adaptation in NLP
Jing Jiang and ChengXiang Zhai

16:35–17:00

The Inﬁnite Tree
Jenny Rose Finkel, Trond Grenager and Christopher D. Manning

17:00–17:25 Guiding Semi-Supervision with Constraint-Driven Learning

Ming-Wei Chang, Lev Ratinov and Dan Roth

Session 3B: Machine Translation 3

15:45–16:10

Supertagged Phrase-Based Statistical Machine Translation
Hany Hassan, Khalil Sima’an and Andy Way

16:10–16:35

Regression for Sentence-Level MT Evaluation with Pseudo References
Joshua S. Albrecht and Rebecca Hwa

16:35–17:00

Bootstrapping Word Alignment via Word Packing
Yanjun Ma, Nicolas Stroppa and Andy Way

17:00–17:25

Improved Word-Level System Combination for Machine Translation
Antti-Veikko Rosti, Spyros Matsoukas and Richard Schwartz

xxviii

Monday, June 25, 2007 (continued)

Session 3C: Generation

15:45–16:10 Generating Constituent Order in German Clauses

Katja Filippova and Michael Strube

16:10–16:35

A Symbolic Approach to Near-Deterministic Surface Realisation using Tree Adjoining
Grammar
Claire Gardent and Eric Kow

16:35–17:00

Sentence Generation as a Planning Problem
Alexander Koller and Matthew Stone

17:00–17:25 GLEU: Automatic Evaluation of Sentence-Level Fluency

Andrew Mutton, Mark Dras, Stephen Wan and Robert Dale

Session 3D: Multimodality 1

15:45–16:10 Conditional Modality Fusion for Coreference Resolution

Jacob Eisenstein and Randall Davis

16:10–16:35

The Utility of a Graphical Representation of Discourse Structure in Spoken Dialogue Sys-
tems
Mihai Rotaru and Diane Litman

16:35–17:00

Automated Vocabulary Acquisition and Interpretation in Multimodal Conversational Sys-
tems
Yi Liu, Joyce Chai and Rong Jin

17:00–17:25

A Multimodal Interface for Access to Content in the Home
Michael Johnston, Luis Fernando D’Haro, Michelle Levine and Bernard Renger

xxix

Tuesday, June 26, 2007

Session 4A, Parsing 1

09:00–09:25

Fast Unsupervised Incremental Parsing
Yoav Seginer

09:25–09:50 K-best Spanning Tree Parsing

Keith Hall

09:50–10:15

Is the End of Supervised Parsing in Sight?
Rens Bod

10:15–10:40

An Ensemble Method for Selection of High Quality Parses
Roi Reichart and Ari Rappoport

Session 4B: Sentiment 1

09:00–09:25 Opinion Mining using Econometrics: A Case Study on Reputation Systems

Anindya Ghose, Panagiotis Ipeirotis and Arun Sundararajan

09:25–09:50

PageRanking WordNet Synsets: An Application to Opinion Mining
Andrea Esuli and Fabrizio Sebastiani

09:50–10:15

Structured Models for Fine-to-Coarse Sentiment Analysis
Ryan McDonald, Kerry Hannan, Tyler Neylon, Mike Wells and Jeff Reynar

10:15–10:40

Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment
Classiﬁcation
John Blitzer, Mark Dredze and Fernando Pereira

Session 4C: Paraphrasing, Textual Entailment

09:00–09:25 Clustering Clauses for High-Level Relation Detection: An Information-theoretic Ap-

proach
Samuel Brody

09:25–09:50

Instance-based Evaluation of Entailment Rule Acquisition
Idan Szpektor, Eyal Shnarch and Ido Dagan

09:50–10:15

Statistical Machine Translation for Query Expansion in Answer Retrieval
Stefan Riezler, Alexander Vasserman, Ioannis Tsochantaridis, Vibhu Mittal and Yi Liu

10:15–10:40

A Computational Model of Text Reuse in Ancient Literary Texts
John Lee

xxx

Tuesday, June 26, 2007 (continued)

Session 4D: Discourse and Dialog 1

09:00–09:25

Finding Document Topics for Improving Topic Segmentation
Olivier Ferret

09:25–09:50

The Utility of Parse-derived Features for Automatic Discourse Segmentation
Seeger Fisher and Brian Roark

09:50–10:15

PERSONAGE: Personality Generation for Dialogue
Franc¸ois Mairesse and Marilyn Walker

10:15–10:40 Making Sense of Sound: Unsupervised Topic Segmentation over Acoustic Input

Igor Malioutov, Alex Park, Regina Barzilay and James Glass

10:40–11:10 Break

11:10-12:10

LifeTime Achievement Award

12:10–13:30

Lunch

13:00-14:30 ACL Business Meeting

Session 5A: Language Modeling 2

14:30–14:55

Randomised Language Modelling for Statistical Machine Translation
David Talbot and Miles Osborne

14:55–15:20

Bilingual-LSA Based LM Adaptation for Spoken Language Translation
Yik-Cheung Tam, Ian Lane and Tanja Schultz

Session 5B: Coreference

14:30–14:55 Coreference Resolution Using Semantic Relatedness Information from Automatically Dis-

covered Patterns
Xiaofeng Yang and Jian Su

14:55–15:20

Semantic Class Induction and Coreference Resolution
Vincent Ng

xxxi

Tuesday, June 26, 2007 (continued)

Session 5C: Summarization

14:30–14:55 Generating a Table-of-Contents

S. R. K. Branavan, Pawan Deshpande and Regina Barzilay

14:55–15:20

Towards an Iterative Reinforcement Approach for Simultaneous Document Summarization
and Keyword Extraction
Xiaojun Wan, Jianwu Yang and Jianguo Xiao

Session 5D: Semantic Relations

14:30–14:55

Fast Semantic Extraction Using a Novel Neural Network Architecture
Ronan Collobert and Jason Weston

14:55–15:20

Improving the Interpretation of Noun Phrases with Cross-linguistic Information
Roxana Girju

15:20–15:45 Break

Session 6A: Information Extraction

15:45–16:10

Learning to Extract Relations from the Web using Minimal Supervision
Razvan Bunescu and Raymond Mooney

16:10–16:35

A Seed-driven Bottom-up Machine Learning Framework for Extracting Relations of Vari-
ous Complexity
Feiyu Xu, Hans Uszkoreit and Hong Li

16:35–17:00

A Multi-resolution Framework for Information Extraction from Free Text
Mstislav Maslennikov and Tat-Seng Chua

17:00–17:25 Using Corpus Statistics on Entities to Improve Semi-supervised Relation Extraction from

the Web
Benjamin Rosenfeld and Ronen Feldman

xxxii

Tuesday, June 26, 2007 (continued)

Session 6B: Parsing 2

15:45–16:10

Beyond Projectivity: Multilingual Evaluation of Constraints and Measures on Non-
Projective Structures
Jiˇr´ı Havelka

16:10–16:35

Self-Training for Enhancement and Domain Adaptation of Statistical Parsers Trained on
Small Datasets
Roi Reichart and Ari Rappoport

16:35–17:00 HPSG Parsing with Shallow Dependency Constraints

Kenji Sagae, Yusuke Miyao and Jun’ichi Tsujii

17:00–17:25 Constituent Parsing with Incremental Sigmoid Belief Networks

Ivan Titov and James Henderson

Session 6C: Multilinguality 1

15:45–16:10 Corpus Effects on the Evaluation of Automated Transliteration Systems

Sarvnaz Karimi, Andrew Turpin and Falk Scholer

16:10–16:35 Collapsed Consonant and Vowel Models: New Approaches for English-Persian Translit-

eration and Back-Transliteration
Sarvnaz Karimi, Falk Scholer and Andrew Turpin

16:35–17:00

Alignment-Based Discriminative String Similarity
Shane Bergsma and Grzegorz Kondrak

17:00–17:25

Bilingual Terminology Mining - Using Brain, not brawn comparable corpora
Emmanuel Morin, B´eatrice Daille, Koichi Takeuchi and Kyo Kageura

Session 6D: Language Modeling 3

15:45–16:10 Unsupervised Language Model Adaptation Incorporating Named Entity Information

Feifan Liu and Yang Liu

16:10–16:35 Coordinate Noun Phrase Disambiguation in a Generative Parsing Model

Deirdre Hogan

16:35–17:00

A Uniﬁed Tagging Approach to Text Normalization
Conghui Zhu, Jie Tang, Hang Li, Hwee Tou Ng and Tiejun Zhao

17:00–17:25

Sparse Information Extraction: Unsupervised Language Models to the Rescue
Doug Downey, Stefan Schoenmackers and Oren Etzioni

xxxiii

Wednesday, June 27, 2007

Session 7A: Machine Translation 4

09:00–09:25

Forest-to-String Statistical Translation Rules
Yang Liu, Yun Huang, Qun Liu and Shouxun Lin

09:25–09:50 Ordering Phrases with Function Words

Hendra Setiawan, Min-Yen Kan and Haizhou Li

09:50–10:15

A Probabilistic Approach to Syntax-based Reordering for Statistical Machine Translation
Chi-Ho Li, Minghui Li, Dongdong Zhang, Mu Li, Ming Zhou and Yi Guan

10:15–10:40 Machine Translation by Triangulation: Making Effective Use of Multi-Parallel Corpora

Trevor Cohn and Mirella Lapata

Session 7B: Sequence Processing

09:00–09:25

A Maximum Expected Utility Framework for Binary Sequence Labeling
Martin Jansche

09:25–09:50

A Fully Bayesian Approach to Unsupervised Part-of-speech Tagging
Sharon Goldwater and Tom Grifﬁths

09:50–10:15 Computationally Efﬁcient M-Estimation of Log-Linear Structure Models

Noah A. Smith, Douglas L. Vail and John D. Lafferty

10:15–10:40 Guided Learning for Bidirectional Sequence Classiﬁcation

Libin Shen, Giorgio Satta and Aravind Joshi

Session 7C: Question Answering

09:00–09:25 Different Structures for Evaluating Answers to Complex Questions: Pyramids Won’t Top-

ple, and Neither Will Human Assessors
Hoa Trang Dang and Jimmy Lin

09:25–09:50

Exploiting Syntactic and Shallow Semantic Kernels for Question Answer Classiﬁcation
Alessandro Moschitti, Silvia Quarteroni, Roberto Basili and Suresh Manandhar

09:50–10:15

Language-independent Probabilistic Answer Ranking for Question Answering
Jeongwoo Ko, Teruko Mitamura and Eric Nyberg

xxxiv

Wednesday, June 27, 2007 (continued)

Session 7D: Discourse and Dialog 2

09:00–09:25

Learning to Compose Effective Strategies from a Library of Dialogue Components
Martijn Spitters, Marco De Boni, Jakub Zavrel and Remko Bonnema

09:25–09:50 On the Role of Context and Prosody in the Interpretation of ’okay’

Agust´ın Gravano, Stefan Benus, H´ector Ch´avez, Julia Hirschberg and Lauren Wilcox

09:50–10:15

Predicting Success in Dialogue
David Reitter and Johanna D. Moore

10:15–10:40

Resolving It, This, and That in Unrestricted Multi-Party Dialog
Christoph M¨uller

10:40–11:10 Break

11:10–12:10

Invited Talk by Barney Pell

12:10–13:30

Lunch

Session 8A: Machine Learning Methods 2

13:30–13:55

A Comparative Study of Parameter Estimation Methods for Statistical Natural Language
Processing
Jianfeng Gao, Galen Andrew, Mark Johnson and Kristina Toutanova

13:55–14:20 Grammar Approximation by Representative Sublanguage: A New Model for Language

Learning
Smaranda Muresan and Owen Rambow

14:20–14:45 Chinese Segmentation with a Word-Based Perceptron Algorithm

Yue Zhang and Stephen Clark

14:45–15:10 Unsupervised Coreference Resolution in a Nonparametric Bayesian Model

Aria Haghighi and Dan Klein

xxxv

Wednesday, June 27, 2007 (continued)

Session 8B: Machine Translation and Multilinguality

13:30–13:55

Pivot Language Approach for Phrase-Based Statistical Machine Translation
Hua Wu and Haifeng Wang

13:55–14:20

Bootstrapping a Stochastic Transducer for Arabic-English Transliteration Extraction
Tarek Sherif and Grzegorz Kondrak

14:20–14:45

Beneﬁts of the Passively Parallel Rosetta Stone? Cross-Language Information Retrieval
with over 30 Languages
Peter A. Chew and Ahmed Abdelali

14:45–15:10

A Re-examination of Machine Learning Approaches for Sentence-Level MT Evaluation
Joshua S. Albrecht and Rebecca Hwa

Session 8C: Lexicon and Lexical Semantics

13:30–13:55

Automatic Acquisition of Ranked Qualia Structures from the Web
Philipp Cimiano and Johanna Wenderoth

13:55–14:20

A Sequencing Model for Situation Entity Classiﬁcation
Alexis Palmer, Elias Ponvert, Jason Baldridge and Carlota Smith

14:20–14:45 Words and Echoes: Assessing and Mitigating the Non-Randomness Problem in Word Fre-

quency Distribution Modeling
Marco Baroni and Stefan Evert

14:45–15:10

A System for Large-Scale Acquisition of Verbal, Nominal and Adjectival Subcategorization
Frames from Corpora
Judita Preiss, Ted Briscoe and Anna Korhonen

xxxvi

Wednesday, June 27, 2007 (continued)

Session 8D: Phonology and Morphology 2

13:30–13:55

A Language-Independent Unsupervised Model for Morphological Segmentation
Vera Demberg

13:55–14:20 Using Mazurkiewicz Trace Languages for Partition-Based Morphology

Franc¸ois Barth´elemy

14:20–14:45 Much ado about nothing: A Social Network Model of Russian Paradigmatic Gaps

Robert Daland, Andrea D. Sims and Janet Pierrehumbert

14:45–15:10

Substring-Based Transliteration
Tarek Sherif and Grzegorz Kondrak

15:10–15:45 Break

Session 9A: Parsing 3

15:45–16:10

Pipeline Iteration
Kristy Hollingshead and Brian Roark

16:10–16:35

Learning Synchronous Grammars for Semantic Parsing with Lambda Calculus
Yuk Wah Wong and Raymond Mooney

16:35–17:00 Generalizing Tree Transformations for Inductive Dependency Parsing

Jens Nilsson, Joakim Nivre and Johan Hall

Session 9B: Sentiment 2

15:45–16:10

Learning Multilingual Subjective Language via Cross-Lingual Projections
Rada Mihalcea, Carmen Banea and Janyce Wiebe

16:10–16:35

Sentiment Polarity Identiﬁcation in Financial News: A Cohesion-based Approach
Ann Devitt and Khurshid Ahmad

16:35–17:00 Weakly Supervised Learning for Hedge Classiﬁcation in Scientiﬁc Literature

Ben Medlock and Ted Briscoe

xxxvii

Wednesday, June 27, 2007 (continued)

Session 9C: Multimodality 2

15:45–16:10

Text Analysis for Automatic Image Annotation
Koen Deschacht and Marie-Francine Moens

16:10–16:35 User Requirements Analysis for Meeting Information Retrieval Based on Query Elicitation

Vincenzo Pallotta, Violeta Seretan and Marita Ailomaa

16:35–17:00 Combining Multiple Knowledge Sources for Dialogue Segmentation in Multimedia

Archives
Pei-Yun Hsueh and Johanna D. Moore

Session 9D: Text mining and Retrieval

15:45–16:10

Topic Analysis for Psychiatric Document Retrieval
Liang-Chih Yu, Chung-Hsien Wu, Chin-Yew Lin, Eduard Hovy and Chia-Ling Lin

16:10–16:35 What to be? - Electronic Career Guidance Based on Semantic Relatedness

Iryna Gurevych, Christof M¨uller and Torsten Zesch

16:35–17:00

Extracting Social Networks and Biographical Facts From Conversational Speech Tran-
scripts
Hongyan Jing, Nanda Kambhatla and Salim Roukos

17:10

Best Paper Award (Plenary Session)

xxxviii

Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 1–8,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

1

GuidingStatisticalWordAlignmentModelsWithPriorKnowledgeYonggangDengandYuqingGaoIBMT.J.WatsonResearchCenterYorktownHeights,NY10598{ydeng,yuqing}@us.ibm.comAbstractWepresentageneralframeworktoincor-poratepriorknowledgesuchasheuristicsorlinguisticfeaturesinstatisticalgenerativewordalignmentmodels.Priorknowledgeplaysaroleofprobabilisticsoftconstraintsbetweenbilingualwordpairsthatshallbeusedtoguidewordalignmentmodeltrain-ing.Weinvestigateknowledgethatcanbederivedautomaticallyfromentropyprinci-pleandbilinguallatentsemanticanalysisandshowhowtheycanbeappliedtoim-provetranslationperformance.1IntroductionStatisticalwordalignmentmodelslearnwordas-sociationsbetweenparallelsentencesfromstatis-tics.Mostmodelsaretrainedfromcorporainanunsupervisedmannerwhosesuccessisheavilyde-pendentonthequalityandquantityofthetrainingdata.Ithasbeenshownthathumanknowledge,intheformofasmallamountofmanuallyanno-tatedparalleldatatobeusedtoseedorguidemodeltraining,cansigniﬁcantlyimprovewordalignmentF-measureandtranslationperformance(IttycheriahandRoukos,2005;FraserandMarcu,2006).Asformulatedinthecompetitivelinkingalgo-rithm(Melamed,2000),theproblemofwordalign-mentcanberegardedasaprocessofwordlink-agedisambiguation,thatis,choosingcorrectasso-ciationsamongallcompetinghypothesis.Themorereasonableconstraintsareimposedonthisprocess,theeasierthetaskwouldbecome.Forinstance,themostrelaxedIBMModel-1,whichassumesthatanysourcewordcanbegeneratedbyanytargetwordequallyregardlessofdistance,canbeimprovedbydemandingaMarkovprocessofalignmentsasinHMM-basedmodels(Vogeletal.,1996),orimple-mentingadistributionofnumberoftargetwordslinkedtoasourcewordasinIBMfertility-basedmodels(Brownetal.,1993).Followingthepath,weshallputmoreconstraintsonwordalignmentmodelsandinvestigatewaysofimplementingtheminastatisticalframework.Wehaveseenexamplesshowingthatnamestendtoaligntonamesandfunctionwordsarelikelytobelinkedtofunctionwords.Theseobservationsareindependentoflanguageandcanbeunderstoodbycommonsense.Moreover,thereareotherlinguis-ticallymotivatedconstraints.Forinstance,wordsalignedtoeachotherpresumablyaresemanticallyconsistent;andlikelytobe,theyaresyntacticallyagreeable.Inthesepaper,weshallexploitsomeoftheseconstraintsinbuildingbetterwordalignmentsintheapplicationofstatisticalmachinetranslation.Weproposeasimpleframeworkthatcaninte-gratepriorknowledgeintostatisticalwordalign-mentmodeltraining.Intheframework,priorknowl-edgeservesasprobabilisticsoftconstraintsthatwillguidewordalignmentmodeltraining.Wepresenttwotypesofconstraintsthatarederivedinanun-supervisedway:oneisbasedontheentropyprin-ciple,theothercomesfrombilinguallatentseman-ticanalysis.Weinvestigatetheirimpactonwordalignmentsandshowtheireffectivenessinimprov-ingtranslationperformance.2

2ConstrainedWordAlignmentModelsTheframeworkthatweproposetoincorporatesta-tisticalconstraintsintowordalignmentmodelsisgeneric.ItcanbeappliedtocomplicatedmodelssuchIBMModel-4(Brownetal.,1993).WeshalltakeHMM-basedwordalignmentmodel(Vogeletal.,1996)asanexampleandfollowthenotationof(Brownetal.,1993).Lete=el1representasourcestringandf=fm1atargetstring.Therandomvari-ablea=am1speciﬁestheindicesofsourcewordsthattargetwordsarealignedto.InanHMM-basedwordalignmentmodel,sourcewordsaretreatedasMarkovstateswhiletargetwordsareobservationsthataregeneratedwhenjumpingtostates:P(a,f|e)=mYj=1P(aj|aj−1,e)t(fj|eaj)Noticethatatargetwordfisgeneratedfromasourcestateebyasimplelookupofthetranslationtable,a.k.a.,t-tablet(f|e),asdepictedin(A)ofFig-ure1.Toincorporatepriorknowledgeorimposeconstraints,weintroducetwonodesEandFrepre-sentingthehiddentagsofthesourcewordeandthetargetwordfrespectively,andorganizethedepen-dencystructureasin(B)ofFigure1.Giventhisgen-erativeprocedure,fwillalsodependonitstagF,whichisdeterminedprobabilisticallybythesourcetagE.ThedependencyfromEtoFfunctionsasasoftconstraintshowinghowthetwohiddentagsareagreeabletoeachother.Mathematically,thecondi-tionaldistributionfollows:P(f|e)=XE,FP(f,E,F|e)=XE,FP(E|e)P(F|E)P(f|e,F)=t(f|e)·Con(f,e),(1)whereCon(f,e)=XE,FP(E|e)P(F|E)P(F|f)/P(F)(2)isthesoftweightattachedtothet-tableentry.Itcon-sidersallpossiblehiddentagsofeandfandservesasconstraintbetweenthelink.   f e f e EFA B Figure1:Asimpletablelookup(A)vs.acon-strainedprocedure(B)ofgeneratingatargetwordffromasourceworde.WedonotchangethevalueofCon(f,e)duringiterativemodeltrainingbutratherkeepitconstantasanindicatorofhowstrongthewordpairshouldbeconsideredasacandidate.Thisinformationisde-rivedbeforewordalignmentmodeltrainingandwillactassoftconstraintsthatneedtoberespecteddur-ingtrainingandalignments.Foragivenwordpair,thesoftconstraintcanhavedifferentassignmentindifferentsentencepairssincethewordtagscanbecontextdependent.Tounderstandwhywetakethe“detour”ofgen-eratingatargetwordratherthandirectlyfromat-table,considerthehiddentagasbinaryvaluein-dicatingbeinganameornot.Withoutthesecon-straints,t-tableentriesfornameswithlowfrequencytendtobeﬂatandwordalignmentscanbechosenrandomlywithoutsufﬁcientstatisticsorstronglexi-calpreferenceundermaximumlikelihoodcriterion.Ifweassumethatanameisproducedbyanamewithahighprobabilitybutbyanon-namewithalowprobability,i.e.P(F=E)>>P(F6=E),propernameswithlowcountsthenareencouragedtolinktopropernamesduringtraining;andconse-quently,conditionalprobabilitymasswouldbemorefocusedoncorrectnametranslations.Ontheotherhand,namesarediscouragedtoproducenon-names.Thiswillpotentiallyavoidincorrectwordassocia-tions.Weareabletoapplythistypeofconstraintsinceusuallytherearemanymonolingualresourcesavailabletobuildahighperformanceprobabilisticnametagger.Theexamplesuggeststhatputtingrea-sonableconstraintslearnedfrommonolingualanaly-siscanalleviatedatasparenessprobleminbilingualapplications.TheweightsCon(f,e)arethepriorknowledgethatshallbeassignedwithcarebutrespecteddur-ingtraining.Thebaselineistosetalltheseweights3

to1,whichisequivalenttoplacingnopriorknowl-edgeonmodeltraining.Theintroductionoftheseweightsdoesnotcomplicateparameterestimationprocedure.Wheneverasourcewordeishypoth-esizedtogenerateatargetwordf,thetranslationprobabilityt(f|e)shouldbeweightedbyCon(f,e).Wepointoutthattheconstraintsbetweenfandethroughtheirhiddentagsareinprobabilities.Therearenoharddecisionsmadebeforetraining.Astrongpreferencebetweentwowordscanbeexpressedbyassigningcorrespondingweightscloseto1.Thiswillaffecttheﬁnalalignmentmodel.Dependingonthehiddentags,therearemanyre-alizationsofreasonableconstraintsthatcanbeputbeforehand.Theycanbesemanticclasses,syntacticannotations,orassimpleaswhetherbeingafunctionwordorcontentword.Moreover,thesourcesideandthetargetsidedonothavetosharethesamesetoftags.Theframeworkisalsoﬂexibletosupportmul-tipletypesofconstraintsthatcanbeimplementedinparallelorcascadedsequence.Moreover,thecon-straintsbetweenwordscanbedependentoncontextwithinparallelsentences.Next,wewilldescribetwotypesofconstraintsthatweproposed.Bothofthemarederivedfromdatainanunsupervisedway.2.1EntropyPrincipleItisassumedthatgenerallyspeaking,asourcefunc-tionwordgeneratesatargetfunctionwordwithahigherprobabilitythangeneratingatargetcontentword;similarassumptionappliestoasourcecon-tentwordaswell.WecapturethistypeofconstraintbydeﬁningthehiddentagEandFasbinarylabelsindicatingbeingacontentwordornot.Basedontheassumption,wedesignprobabilisticrelationshipbetweenthetwohiddentagsas:P(E=F)=1−P(E6=F)=α,whereαisascalarwhosevalueiscloseto1,say0.9.Thebiggerαis,thetighterconstraintweputonwordpairstobeconnectedrequiringthesametypeoflabel.Todeterminetheprobabilityofawordbeingafunctionword,weapplytheentropyprinciple.Afunctionword,say“of”,“in”or“have”,appearsmorefrequentlythanacontentword,say“journal”or“chemistry”,inadocumentorsentence.Wewillapproximatetheprobabilityofawordasafunctionwordwiththerelativeuncertaintyofitsbeingob-servedinasentence.Morespeciﬁcally,supposewehaveNparallelsentencesinthetrainingcorpus.Foreachwordwi1,letcijbethenumberofwordwiobservedinthej-thsentencepair,andletcibethetotalnumberofoc-currencesofwiinthecorpus.Wedeﬁnetherelativeentropyofwordwiaswi=−1logNNXj=1cijcilogcijci.Withtheentropyofaword,thelikelihoodofwordwbeingtaggedasafunctionwordisapproximatedwithw(1)=wandbeingtaggedasacontentwordwithw(0)=1−w.WeignorethedenominatorinEqu.(2)andﬁndtheconstraintundertheentropyprinciple:Con(f,e)=α(e(0)f(0)+e(1)f(1))+(1−α)(e(1)f(0)+e(0)f(1)).Ascanbeseen,theconnectionbetweentwowordsissimulatedwithabinarysymmetricchan-nel.Anexampledistributionoftheconstraintfunc-tionisillustratedinFigure2.Ahighvalueofαencouragesconnectingwordpairswithcompara-bleentropy;Whenα=0.5,Con(f,e)isconstantwhichcorrespondstoapplyingnopriorconstraint;Whenαiscloseto0,thefunctionplaysoppositeroleonwordalignmenttrainingwhereahighfre-quencywordispushedtoassociatewithalowfre-quencyword.2.2BilingualLatentSemanticAnalysisLatentSemanticAnalysis(LSA)isatheoryandmethodforextractingandrepresentingthemeaningofwordsbystatisticallyanalyzingwordcontextualusagesinacollectionoftext.Itprovidesamethodbywhichtocalculatethesimilarityofmeaningofgivenwordsanddocuments.LSAhasbeensuccess-fullyappliedtoinformationretrieval(Deerwesteretal.,1990),statisticallangaugemodeling(Belle-garda,2000)andetc.1Wepreﬁx‘E’tosourcewordsand‘F’totargetwordstodistinguishwordsthathavethesamespellingbutarefromdifferentlanguages.4

 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Con(f,e)alpha=0.9e(0)f(0) 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Con(f,e)alpha=0.1e(0)f(0)Figure2:Distributionoftheconstraintfunctionbasedonentropyprinciplewhenα=0.9ontheleftandα=0.1ontheright.WeexploreLSAtechniquesinbilingualenviron-menttoderivesemanticconstraintsaspriorknowl-edgeforguidingawordalignmentmodeltrain-ing.Theideaistoﬁndsemanticrepresentationofsourcewordsandtargetwordsintheso-calledlow-dimensionalLSA-space,andthentousetheirsim-ilaritiestoquantitativelyestablishsemanticconsis-tencies.Weproposetwodifferentapproaches.2.2.1ASimpleBag-of-wordModelOnemethodweinvestigateisasimplebag-of-wordmodelasinmonolingualLSA.Wetreateachsentencepairasadocumentanddonotdistin-guishsourcewordsandtargetwordsasiftheyaretermsgeneratedfromthesamevocabulary.AsparsematrixWcharacterizingword-documentco-occurrenceisconstructed.Followingthenotationinsection2.1,theij-thentryofthematrixWisde-ﬁnedasin(Bellegarda,2000)Wij=(1−wi)cijcj,wherecjisthetotalnumberofwordsinthej-thsentencepair.Thisconstructionconsiderstheim-portanceofwordsglobally(corpuswide)andlocally(withinsentencepairs).AlternativeconstructionsofthematrixarepossibleusingrawcountsorTF-IDF(Deerwesteretal.,1990).WisaM×Nsparsematrix,whereMisthesizeofvocabularyincludingbothsourceandtargetwords.Toobtainacompactrepresentation,singularvaluedecomposition(SVD)isemployed(cf.Berryetal(1993))toyieldW≈ˆW=U×S×VTasFigure3shows,where,forsomeorderR(cid:28)min(M,N)ofthedecomposition,UisaM×Rleftsingularmatrixwithrowsui,i=1,···,M,SisaR×Rdiagonalmatrixofsingularvaluess1≥s2≥...≥sR(cid:29)0,andVisN×Rarightsingularma-trixwithrowsvj,j=1,···,N.Foreachi,thescaledR-vectoruiSmaybeviewedasrepresentingwi,thei-thwordinthevocabulary,andsimilarlythescaledR-vectorvjSasrepresentingdj,j-thdocu-mentinthecorpus.NotethattheuiS’sandvjS’sbothbelongtoIRR,theso-calledLSA-space.AlltargetandsourcewordsareprojectedintothesameLSA-spacetoo.NM×RM×RR×NR×R orthonormalvectorsDocuments1wMwWordsWUSTV1dNdR orthonormalvectorsFigure3:SVDoftheSparseMatrixW.AsEqu.(2)suggested,toinducesemanticcon-straintsinastraightforwardway,onewouldproceedasfollows:ﬁrstly,performwordsemanticcluster-ingwith,say,theircompactrepresentationsintheLSA-space;secondly,constructclustergeneratingdependenciesbyspecifyingtheconditionaldistribu-tionofP(F|E);andﬁnally,foreachwordpair,in-ducethesemanticconstraintbyconsideringallpos-siblesemanticlabelingschemes.Weapproximatethislongprocesswithsimplyﬁndingwordsimilar-itiesdeﬁnedbytheircosinedistanceinthelowdi-mensionspace:Con(f,e)=12(cos(ufS,ueS)+1)(3)Thelinearmappingaboveisintroducedtoavoidnegativeconstraintsandtosetthemaximumcon-straintvalueas1.Inbuildingwordalignmentmodels,aspecial“NULL”wordisusuallyintroducedtoaddresstar-getwordsthataligntonosourcewords.Sincethisphysicallynon-existingwordisnotinthevocabu-laryofthebilingualLSA,weusethecentroidofallsourcewordsasitsvectorrepresentationintheLSA-space.Thesemanticconstraintsbetween“NULL”andanytargetwordscanbederivedinthesameway.However,thisischosenformostlycomputational5

convenience,andisnottheonlywaytoaddresstheemptywordissue.2.2.2UtilizingWordAlignmentStatisticsWhilethesimplebag-of-wordmodelputsallsourcewordsandtargetwordsasrowsinthema-trix,anothermethodofderivingsemanticconstraintconstructsthesparsematrixbytakingsourcewordsasrowsandtargetwordsascolumnsandusesstatis-ticsfromwordalignmenttrainingtoformwordpairco-occurrenceassociation.Morespeciﬁcally,weregardeachtargetwordfasa“document”andeachsourcewordeasa“term”.Thenumberofoccurrencesofthesourcewordeinthedocumentfisdeﬁnedastheexpectednumberoftimesthatfgenerateseintheparallelcorpusunderthewordalignmentmodel.Thismethodre-quirestrainingthebaselinewordalignmentmodelinanotherdirectionbytakingfsassourcewordsandesastargetwords,whichisoftendoneforsymmetricalignments,andthendumpingoutthesoftcountswhenmodelconverges.Wethresholdtheminimumword-to-wordtranslationprobabilitytoremovewordpairsthathavelowco-occurrencecounts.Followingthesimilarityinducedsemanticcon-straintsinsection2.2.1,weneedtoﬁndthedistancebetweenatermandadocument.Letvfbethepro-jectionofthedocumentrepresentingthetargetwordfanduetheprojectionofthetermrepresentingthesourcewordeafterperformingSVDonthesparsematrix,wecalculatethesimilaritybetween(f,e)andthenﬁndtheirsemanticconstrainttobeCon(f,e)=12(cos(vfS1/2,ueS1/2)+1)(4)Unlikethemethodinsection2.2.1,thereisnoemptywordissueheresincewedohavestatisticsofthe“NULL”wordasasourcewordgeneratingewordsandthereforethereisa“document”assignedtoit.3ExperimentalResultsWetestourframeworkonthetaskoflargevocab-ularytranslationfromdialectical(Iraqi)Arabicut-terancesintoEnglish.Thetaskcoversmultipledo-mainsincludingtravel,emergencymedicaldiagno-sis,defense-orientedforceprotection,securityandetc.Toavoidimpactsofspeechrecognitionerrors,weonlyreportexperimentsfromtexttotexttransla-tion.Thetrainingcorpusconsistsof390Ksentencepairs,withtotal2.43MArabicwordsand3.38MEn-glishwords.Thesesentencesareintypicalspokentranscriptionform,i.e.,spellingerrors,disﬂuencies,suchaswordorphraserepetition,andungrammat-icalutterancesarecommonlyobserved.Arabicut-terancelengthrangesfrom3to70wordswiththeaverageof6words.Thereare25KentriesintheEnglishvocabularyand90KinArabicside.Datasparsenessseverelychallengeswordalignmentmodelandconsequentlyautomaticphrasetranslationinduction.Thereare42KsingletonsinArabicvocabulary,and14KAra-bicwordswithoccurrenceoftwiceeachinthecor-pus.SinceArabicisamorphologicallyrichlan-guagewhereafﬁxesareattachedtostemwordstoindicategender,tense,caseandetc,inordertore-ducevocabularysizeandaddressout-of-vocabularywords,wesplitArabicwordsintoafﬁxandrootac-cordingtoarule-basedsegmentationscheme(Xiangetal.,2006)withthehelpfromtheBuckwalterana-lyzer(LDC,2002)output.ThisreducesthesizeofArabicvocabularyto52K.Ourtestdataconsistsof1294sentencepairs.Theyaresplitintotwoparts:halfofthemisusedasthedevelopmentset,onwhichtrainingparametersanddecodingfeatureweightsaretuned,theotherhalfisfortest.3.1TrainingandTranslationSetupStartingfromthecollectionofparalleltrainingsen-tences,wetrainwordalignmentmodelsintwotrans-lationdirections,fromEnglishtoIraqiArabicandfromIraqiArabictoEnglish,andderivetwosetsofViterbialignments.Bycombiningwordalign-mentsintwodirectionsusingheuristics(OchandNey,2003),asinglesetofstaticwordalignmentsisthenformed.Allphrasepairswhichrespecttothewordalignmentboundaryconstraintareiden-tiﬁedandpooledtobuildphrasetranslationtableswiththeMaximumLikelihoodcriterion.Weprunephrasetranslationentriesbytheirprobabilities.ThemaximumnumberoftokensinArabicphrasesissetto5forallconditions.Ourdecoderisaphrase-basedmulti-stackimple-6

mentationofthelog-linearmodelsimilartoPharaoh(Koehnetal.,2003).Likeotherlog-linearmodelbaseddecoders,activefeaturesinourtranslationen-gineincludetranslationmodelsintwodirections,lexiconweightsintwodirections,languagemodel,distortionmodel,andsentencelengthpenalty.Thesefeatureweightsaretunedonthedevsettoachieveoptimaltranslationperformanceusingdownhillsim-plexmethod(OchandNey,2002).ThelanguagemodelisastatisticaltrigrammodelestimatedwithModiﬁedKneser-Neysmoothing(ChenandGood-man,1996)usingallEnglishsentencesintheparal-leltrainingdata.WemeasuretranslationperformancebytheBLEUscore(Papinenietal.,2002)andTranslationErrorRate(TER)(Snoveretal.,2006)withoneref-erenceforeachhypothesis.Wordalignmentmod-elstrainedwithdifferentconstraintsarecomparedtoshowtheireffectsontheresultingphrasetransla-tiontablesandtheﬁnaltranslationperformance.3.2TranslationResultsOurbaselinewordalignmentmodelistheword-to-wordHiddenMarkovModel(Vogeletal.,1996).Basicmodelsintwotranslationdirectionsaretrainedsimultaneouslywherestatisticsoftwodirec-tionsaresharedtolearnsymmetrictranslationlexi-conandwordalignmentswithhighprecisionmoti-vatedby(Zensetal.,2004)and(Liangetal.,2006).Thebaselinetranslationresults(BLEUandTER)onthedevandtestsetarepresentedintheline“HMM”ofTable1.WealsocomparewithresultsofIBMModel-4wordalignmentsimplementedinGIZA++toolkit(OchandNey,2003).Westudyandcomparetwotypesofconstraintandseehowtheyaffectwordalignmentsandtranslationoutput.Oneisbasedontheentropyprincipleasde-scribedinSection2.1,whereαissetto0.9;Theotherisbasedonbilinguallatentsemanticanalysis.Forthesimplebag-of-wordbilingualLSAasde-scribedinSection2.2.1,afterSVDonthesparsema-trixusingthetoolkitSVDPACK(Berryetal.,1993),allsourceandtargetwordsareprojectedintoalow-dimensional(R=88)LSA-space.Wordpairse-manticconstrainsarecalculatedbasedontheirsim-ilarityasinEqu.3beforewordalignmenttraining.Likethebaseline,weperform6iterationsofIBMModel-1trainingandthen4iterationofHMMtrain-ing.Thesemanticconstraintsareusedtoguidewordalignmentmodeltrainingforeachiteration.TheBLEUscoreandTERwiththisconstraintareshownintheline“BiLSA-1”ofTable1.ToexploitwordalignmentstatisticsinbilingualLSAasdescribedinSection2.2.2,wedumpoutthestatisticsofthebaselinewordalignmentmodelandusethemtoconstructthesparsematrix.Weﬁndlow-dimensionalrepresentation(R=67)ofEnglishwordsandArabicwordsandusetheirsimilaritytoestablishsemanticconstraintsasinEqu.4.Thetrainingprocedureisthesameasthebaselineand“BiLSA-1”.Thetranslationresultswiththesewordalignmentsareshownas“BiLSA-2”inTable1.AsTable1shows,whentheentropybasedcon-straintsareapplied,BLEUscoreimproves0.5pointonthetestset.Clearly,whenbilingualLSAcon-straintsareapplied,translationperformancecanbeimprovedupto1.6BLEUpoints.WealsoobservethatTERcandrop2.1pointswiththe“BiLSA-1”constraint.While“BiLSA-1”constraintperformsbetteronthetestset,“BiLSA-2”constraintachievesslightlyhigherBLEUscoreonthedevset.Wethentryasimplecombinationofthesetwotypesofconstraints,thatisthegeometricmeanofConBiLSA−1(f,e)andConBiLSA−2(f,e),andﬁndoutthatBLEUscorecanbeimprovedalittlebitfur-theronbothsetsastheline“Mix”shows.WenoticethattherelativelysimplerHMMmodelcanperformcomparableorbetterthanthesophis-ticatedModel-4whenproperconstraintsareactiveinguidingwordalignmentmodeltraining.WealsotrytoputconstraintsinModel-4.AstheEquation1implies,whenaword-to-wordgenerativeproba-bilityisneeded,oneshouldmultiplycorrespondinglexiconentryinthet-tablewiththewordpaircon-straint.WesimplymodifytheGIZA++toolkit(OchandNey,2003)byalwaysweightinglexiconproba-bilitieswithsoftconstraintsduringiterativemodeltraining,andobtain0.7%TERreductiononbothsetsand0.4%BLEUimprovementonthetestset.3.3AnalysisTounderstandhowpriorknowledgeencodedassoftconstraintsplaysaroleinguidingwordalignmenttraining,wecomparestatisticsofdifferentwordalignmentmodels.WeﬁndthatourbaselineHMM7

Table1:TranslationResultswithdifferentwordalignments.BLEUTERAlignmentsdevtestdevtestModel-40.3100.2960.5280.530+Mix0.3060.3000.5210.523HMM0.2890.2880.5430.542+Entropy0.2890.2930.5340.536+BiLSA-10.2940.3000.5310.521+BiLSA-20.2980.2920.5300.528+Mix0.3020.3040.5320.524generates2.6%lessnumberoftotalwordlinksthanthatofModel-4.Partofthereasonisthatmod-elsoftwodirectionsinthebaselinearetrainedsi-multaneously.Therequirementofbi-directionalev-idenceplacesacertainconstraintonwordalign-ments.When“BiLSA-1”constraintsareappliedinthebaselinemodel,2.7%lessnumberoftotalwordlinksarehypothesized,andconsequently,lessnum-berofArabicn-gramtranslationsintheﬁnalphrasetranslationtableareinduced.Theobservationsug-geststhattheconstraintsimprovewordalignmentprecisionandaccuracyofphrasetranslationtablesaswell. bAl_ mrM mAl _tk in your esophagus HMM bAl_ mrM mAl _tk in your esophagus +BiLSA-1 bAl_ mrM mAl _tk in your esophagus Model-4 (in) (esophagus) gloss (ownership) (yours) Figure4:Anexampleofwordalignmentsunderdif-ferentmodelsFigure4showsexamplewordalignmentsofapar-tialsentencepair.ThecompleteEnglishsentenceis“haveyoueverhadlikeanyreﬂuxdiseasesinyouresophagus”.WenoticethattheArabicword“mrM”(meansesophagus)appearsonlyonceinthecorpus.SomeofthewordpairconstraintsarelistedinTa-ble2.Theexampledemosthatduetoreasonableconstraintsplacedinwordalignmenttraining,thelinkto“tK”iscorrectedandconsequentlywehaveaccuratewordtranslationfortheArabicsingletonTable2:WordpairconstraintvaluesEnglisheArabicfConBiLSA−1(f,e)esophagusmrM0.6424mAl0.1819tk0.2897yourmrM0.6319mAl0.4930tk0.9672“mrM”.4RelatedWorkHeuristicsbasedonco-occurrenceanalysis,suchaspoint-wisemutualinformationorDicecoefﬁcients,havebeenshowntobeindicativeforwordalign-ments(ZhangandVogel,2005;Melamed,2000).Theframeworkpresentedinthispaperdemonstratesthepossibilityoftakingheuristicsasconstraintsguidingstatisticalgenerativewordalignmentmodeltraining.Theireffectivenesscanbeexpectedespe-ciallywhendatasparsenessissevere.Discriminativewordalignmentmodels,suchasIttycheriahandRoukos(2005);Moore(2005);BlunsomandCohn(2006),havereceivedgreatamountofstudyrecently.Theyhaveproventhatlin-guisticknowledgeisusefulinmodelingwordalign-mentsunderlog-lineardistributionsasmorphologi-cal,semanticorsyntacticfeatures.Ourframeworkproposestoexploitthesefeaturesdifferentlybytak-ingthemassoftconstraintsoftranslationlexiconun-deragenerativemodel.Whilewordalignmentscanhelpidentifyingse-manticrelations(vanderPlasandTiedemann,2006),weproceedinthereversedirection.Wein-vestigatetheimpactofsemanticconstraintsonsta-tisticalwordalignmentmodelsaspriorknowledge.In(Maetal.,2004),bilingualsemanticmapsareconstructedtoguidewordalignment.Theframe-workweproposedseamlesslyintegratesderivedse-manticsimilaritiesintoastatisticalwordalignmentmodel.Andweextendedmonolinguallatentseman-ticanalysisinbilingualapplications.Toutanovaetal.(2002)augmentedbilingualsen-tencepairswithpart-of-speechtagsaslinguisticconstraintsforHMM-basedwordalignments.Theconstraintsbetweentagsareautomaticallylearnedinaparallelgenerativeprocedurealongwithlex-8

icon.Wehaveintroducedhiddentagsbetweenawordpairtospecializetheirsoftconstraints,whichserveaspriorknowledgethatwillbeusedinguidingwordalignmentmodeltraining.Constraintbetweentagsareembeddedintothewordtowordgenerativeprocess.5ConclusionsandFutureWorkWehavepresentedasimpleandeffectiveframeworktoincorporatepriorknowledgesuchasheuristicsorlinguisticfeaturesintostatisticalgenerativewordalignmentmodels.Priorknowledgeservesassoftconstraintsthatshallbeplacedontranslationlexi-contoguidewordalignmentmodeltraininganddis-ambiguationduringViterbialignmentprocess.Westudiedtwotypesofconstraintsthatcanbeobtainedautomaticallyfromdataandshowedimprovedper-formance(upto1.6%absoluteBLEUincreaseor2.1%absoluteTERreduction)intranslatingdialec-ticalArabicintoEnglish.Futureworkincludesim-plementingtheideainalternativealignmentmod-elsandalsoexploitingpriorknowledgederivedfromsuchasmanually-aligneddataandpre-existinglin-guisticresources.AcknowledgementWethankMohamedAﬁfyfordiscussionsandtheanonymousreviewersforsug-gestions.ReferencesJ.R.Bellegarda.2000.Exploitinglatentsemanticinforma-tioninstatisticallanguagemodeling.Proc.oftheIEEE,88(8):1279–1296,August.M.Berry,T.Do,andS.Varadhan.1993.Svdpackc(version1.0)user’sguide.Tech.reportcs-93-194,UniversityofTen-nessee,Knoxville,TN.P.BlunsomandT.Cohn.2006.Discriminativewordalignmentwithconditionalrandomﬁelds.InProc.ofCOLING/ACL,pages65–72.P.Brown,S.DellaPietra,V.DellaPietra,andR.Mercer.1993.Themathematicsofmachinetranslation:Parameterestima-tion.ComputationalLinguistics,19:263–312.S.F.ChenandJ.Goodman.1996.Anempiricalstudyofsmoothingtechniquesforlanguagemodeling.InProc.ofACL,pages310–318.S.C.Deerwester,S.T.Dumais,T.K.Landauer,G.W.Furnas,andR.A.Harshman.1990.Indexingbylatentsemanticanalysis.JournaloftheAmericanSocietyofInformationScience,41(6):391–407.A.FraserandD.Marcu.2006.Semi-supervisedtrainingforstatisticalwordalignment.InProc.ofCOLING/ACL,pages769–776.A.IttycheriahandS.Roukos.2005.Amaximumentropywordalignerforarabic-englishmachinetranslation.InProc.ofHLT/EMNLP,pages89–96.P.Koehn,F.Och,andD.Marcu.2003.Statisticalphrase-basedtranslation.InProc.ofHLT-NAACL.LDC,2002.BuckwalterArabicMorphologicalAnalyzerVer-sion1.0.LDCCatalogNumberLDC2002L49.P.Liang,B.Taskar,andD.Klein.2006.Alignmentbyagree-ment.InProc.ofHLT/NAACL,pages104–111.Q.Ma,K.Kanzaki,Y.Zhang,M.Murata,andH.Isahara.2004.Self-organizingsemanticmapsanditsapplicationtowordalignmentinjapanese-chineseparallelcorpora.NeuralNetw.,17(8-9):1241–1253.I.Dan.Melamed.2000.Modelsoftranslationalequivalenceamongwords.ComputationalLinguistics,26(2):221–249.R.C.Moore.2005.Adiscriminativeframeworkforbilingualwordalignment.InProc.ofHLT/EMNLP,pages81–88.F.J.OchandH.Ney.2002.Discriminativetrainingandmax-imumentropymodelsforstatisticalmachinetranslation.InProc.ofACL,pages295–302.F.J.OchandH.Ney.2003.Asystematiccomparisonofvari-ousstatisticalalignmentmodels.ComputationalLinguistics,29(1):19–51.K.Papineni,S.Roukos,T.Ward,andW.Zhu.2002.Bleu:amethodforautomaticevaluationofmachinetranslation.InProc.ofACL,pages311–318.M.Snover,B.Dorr,R.Schwartz,L.Micciulla,andJ.Makhoul.2006.Astudyoftranslationeditratewithtargetedhumanannotation.InProc.ofAMTA.K.Toutanova,H.T.Ilhan,andC.Manning.2002.ExtentionstoHMM-basedstatisticalwordalignmentmodels.InProc.ofEMNLP.LonnekevanderPlasandJ¨orgTiedemann.2006.Findingsyn-onymsusingautomaticwordalignmentandmeasuresofdis-tributionalsimilarity.InProc.oftheCOLING/ACL2006MainConferencePosterSessions,pages866–873.S.Vogel,H.Ney,andC.Tillmann.1996.HMMbasedwordalignmentinstatisticaltranslation.InProc.ofCOLING.B.Xiang,K.Nguyen,L.Nguyen,R.Schwartz,andJ.Makhoul.2006.Morphologicaldecompositionforarabicbroadcastnewstranscription.InProc.ofICASSP,pages1089–1092.R.Zens,E.Matusov,andH.Ney.2004.Improvedwordalign-mentusingasymmetriclexiconmodel.InProc.ofCOL-ING,pages36–42.Y.ZhangandS.Vogel.2005.Competitivegroupingininte-gratedphrasesegmentationandalignmentmodel.InProc.oftheACLWorkshoponBuildingandUsingParallelTexts,pages159–162.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 9–16,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

9

ADiscriminativeSyntacticWordOrderModelforMachineTranslationPi-ChuanChang∗ComputerScienceDepartmentStanfordUniversityStanford,CA94305pichuan@stanford.eduKristinaToutanovaMicrosoftResearchRedmond,WAkristout@microsoft.comAbstractWepresentaglobaldiscriminativestatisticalwordordermodelformachinetranslation.Ourmodelcombinessyntacticmovementandsurfacemovementinformation,andisdiscriminativelytrainedtochooseamongpossiblewordorders.Weshowthatcom-biningdiscriminativetrainingwithfeaturestodetectthesetwodifferentkindsofmove-mentphenomenaleadstosubstantialim-provementsinwordorderingperformanceoverstrongbaselines.IntegratingthiswordordermodelinabaselineMTsystemresultsina2.4pointsimprovementinBLEUforEnglishtoJapanesetranslation.1IntroductionThemachinetranslationtaskcanbeviewedascon-sistingoftwosubtasks:predictingthecollectionofwordsinatranslation,anddecidingtheorderofthepredictedwords.Forsomelanguagepairs,suchasEnglishandJapanese,theorderingproblemises-peciallyhard,becausethetargetwordorderdifferssigniﬁcantlyfromthesourcewordorder.Previousworkhasshownthatitisusefultomodeltargetlanguageorderintermsofmovementofsyn-tacticconstituentsinconstituencytrees(YamadaandKnight,2001;Galleyetal.,2006)ordepen-dencytrees(Quirketal.,2005),whichareobtainedusingaparsertrainedtodeterminelinguisticcon-stituency.Alternatively,orderismodelledintermsofmovementofautomaticallyinducedhierarchicalstructureofsentences(Chiang,2005;Wu,1997).∗Thisresearchwasconductedduringtheauthor’sintern-shipatMicrosoftResearch.Theadvantagesofmodelinghowatargetlan-guagesyntaxtreemoveswithrespecttoasourcelan-guagesyntaxtreearethat(i)wecancapturethefactthatconstituentsmoveasawholeandgenerallyre-spectthephrasalcohesionconstraints(Fox,2002),and(ii)wecanmodelbroadsyntacticreorderingphenomena,suchassubject-verb-objectconstruc-tionstranslatingintosubject-object-verbones,asisgenerallythecaseforEnglishandJapanese.Ontheotherhand,thereisalsosigniﬁcantamountofinformationinthesurfacestringsofthesourceandtargetandtheiralignment.Manystate-of-the-artSMTsystemsdonotusetreesandbasetheorderingdecisionsonsurfacephrases(OchandNey,2004;Al-OnaizanandPapineni,2006;Kuhnetal.,2006).Inthispaperwedevelopanordermodelformachinetranslationwhichmakesuseofbothsyntacticandsurfaceinformation.Theframeworkforourstatisticalmodelisasfol-lows.Weassumetheexistenceofadependencytreeforthesourcesentence,anunordereddependencytreeforthetargetsentence,andawordalignmentbetweenthetargetandsourcesentences.Figure1(a)showsanexampleofalignedsourceandtargetdependencytrees.Ourtaskistoorderthetargetde-pendencytree.Wetrainastatisticalmodeltoselectthebestor-deroftheunorderedtargetdependencytree.Anim-portantadvantageofourmodelisthatitisglobal,anddoesnotdecomposethetaskoforderingatar-getsentenceintoaseriesoflocaldecisions,asintherecentlyproposedordermodelsforMachineTransi-tion(Al-OnaizanandPapineni,2006;Xiongetal.,2006;Kuhnetal.,2006).Thusweareabletodeﬁnefeaturesovercompletetargetsentenceorders,andavoidtheindependenceassumptionsmadebythese10

allconstraintsaresatisfied[(cid:2091)(cid:2730)][(cid:14741)(cid:1625)][(cid:22329)][(cid:22307)(cid:22339)(cid:22320)][(cid:16107)(cid:22313)(cid:22303)][(cid:22358)(cid:22344)(cid:22305)(cid:22313)]“restriction”“condition”TOPIC“all”“satisfy”PASSIVE-PRES(cid:99)(cid:100)(cid:101)(cid:102)(cid:103)(cid:104)(a)(cid:102)(cid:101)(cid:99)(cid:100)(cid:103)(cid:104)(cid:102)(cid:101)(cid:99)(cid:100)(cid:103)(cid:104)(cid:102)(cid:101)(cid:99)(cid:100)(cid:103)(cid:104)(b)Figure1:(a)Asentencepairwithsourcedepen-dencytree,projectedtargetdependencytree,andwordalignments.(b)Exampleordersviolatingthetargettreeprojectivityconstraints.models.Ourmodelisdiscriminativelytrainedtose-lectthebestorder(accordingtotheBLEUmeasure)(Papinenietal.,2001)ofanunorderedtargetdepen-dencytreefromthespaceofpossibleorders.Sincethespaceofallpossibleordersofanun-ordereddependencytreeisfactoriallylarge,wetrainourmodelonN-bestlistsofpossibleorders.TheseN-bestlistsaregeneratedusingapproximatesearchandsimplermodels,asinthere-rankingapproachof(Collins,2000).Weﬁrstevaluateourmodelonthetaskoforderingtargetsentences,givencorrect(reference)unorderedtargetdependencytrees.Ourresultsshowthatcom-biningfeaturesderivedfromthesourceandtar-getdependencytrees,distortionsurfaceorder-basedfeatures(likethedistortionusedinPharaoh(Koehn,2004))andlanguagemodel-likefeaturesresultsinamodelwhichsigniﬁcantlyoutperformsmodelsusingonlysomeoftheinformationsources.WealsoevaluatethecontributionofourmodeltotheperformanceofanMTsystem.Weinte-grateourordermodelintheMTsystem,bysimplyre-orderingthetargettranslationsentencesoutputbythesystem.Themodelresultedinanimprove-mentfrom33.6to35.4BLEUpointsinEnglish-to-Japanesetranslationonacomputerdomain.2TaskSetupTheorderingprobleminMTcanbeformulatedasthetaskoforderingatargetbagofwords,givenasourcesentenceandwordalignmentsbetweentar-getandsourcewords.Inthisworkwealsoassumeasourcedependencytreeandanunorderedtargetdependencytreearegiven.Figure1(a)showsanex-ample.Webuildamodelthatpredictsanorderofthetargetdependencytree,whichinducesanorderonthetargetsentencewords.Thedependencytreeconstrainsthepossibleordersofthetargetsentenceonlytotheonesthatareprojectivewithrespecttothetree.Anorderofthesentenceisprojectivewithrespecttothetreeifeachwordanditsdescendantsformacontiguoussubsequenceintheorderedsen-tence.Figure1(b)showsseveralordersofthesen-tencewhichviolatethisconstraint.1Previousstudieshaveshownthatifboththesourceandtargetdependencytreesrepresentlin-guisticconstituency,thealignmentbetweensubtreesinthetwolanguagesisverycomplex(Wellingtonetal.,2006).ThussuchparalleltreeswouldbedifﬁcultforMTsystemstoconstructintranslation.Inthisworkonlythesourcedependencytreesarelinguisti-callymotivatedandconstructedbyaparsertrainedtodeterminelinguisticstructure.Thetargetdepen-dencytreesareobtainedthroughprojectionofthesourcedependencytrees,usingthewordalignment(weuseGIZA++(OchandNey,2004)),ensuringbetterparallelismofthesourceandtargetstructures.2.1ObtainingTargetDependencyTreesThroughProjectionOuralgorithmforobtainingtargetdependencytreesbyprojectionofthesourcetreesviathewordalign-mentistheoneusedintheMTsystemof(Quirketal.,2005).Wedescribethealgorithmschemat-icallyusingtheexampleinFigure1.Projectionofthedependencytreethroughalignmentsisnotatallstraightforward.Oneofthereasonsofdifﬁcultyisthatthealignmentdoesnotrepresentanisomor-phismbetweenthesentences,i.e.itisveryoftennotaone-to-oneandontomapping.2Ifthealign-mentwereone-to-onewecoulddeﬁnetheparentofawordwtinthetargettobethetargetwordalignedtotheparentofthesourcewordsialignedtowt.Anadditionaldifﬁcultyisthatsuchadeﬁnitioncouldre-sultinanon-projectivetargetdependencytree.Theprojectionalgorithmof(Quirketal.,2005)deﬁnesheuristicsforeachoftheseproblems.Incaseofone-to-manyalignments,forexample,thecaseof“constraints”aligningtotheJapanesewordsfor“re-striction”and“condition”,thealgorithmcreatesa1Forexample,intheﬁrstordershown,thedescendantsofword6arenotcontiguousandthusthisorderviolatesthecon-straint.2Inanontomapping,everywordonthetargetsideisasso-ciatedwithsomewordonthesourceside.11

subtreeinthetargetrootedattherightmostofthesewordsandattachestheotherword(s)toit.Incaseofnon-projectivity,thedependencytreeismodiﬁedbyre-attachingnodeshigherupinthetree.Suchastepisnecessaryforourexamplesentence,becausethetranslationsofthewords“all”and“constraints”arenotcontiguousinthetargeteventhoughtheyformaconstituentinthesource.Animportantcharacteristicoftheprojectionalgo-rithmisthatallofitsheuristicsusethecorrecttargetwordorder.3Thusthetargetdependencytreesen-codemoreinformationthanispresentinthesourcedependencytreesandalignment.2.2TaskSetupforReferenceSentencesvsMTOutputOurmodelusesinputofthesameformwhentrained/testedonreferencesentencesandwhenusedinmachinetranslation:asourcesentencewithade-pendencytree,anunorderedtargetsentencewithandunorderedtargetdependencytree,andwordalignments.Wetrainourmodelonreferencesentences.Inthissetting,thegiventargetdependencytreecontainsthecorrectbagoftargetwordsaccordingtoareferencetranslation,andisprojectivewithrespecttothecor-rectwordorderofthereferencebyconstruction.Wealsoevaluateourmodelinthissetting;suchaneval-uationisusefulbecausewecanisolatethecontribu-tionofanordermodel,anddevelopitindependentlyofanMTsystem.Whentranslatingnewsentencesitisnotpossibletoderivetargetdependencytreesbytheprojectionalgorithmdescribedabove.Inthissetting,weusetargetdependencytreesconstructedbyourbaselineMTsystem(describedindetailin6.1).ThesystemconstructsdependencytreesoftheformshowninFigure1foreachtranslationhypothesis.Inthiscasethetargetdependencytreesveryoftendonotcon-tainthecorrecttargetwordsand/orarenotprojectivewithrespecttothebestpossibleorder.3Forexample,checkingwhichwordistherightmostfortheheuristicforone-to-manymappingsandcheckingwhethertheconstructedtreeisprojectiverequiresknowledgeofthecorrectwordorderofthetarget.3LanguageModelwithSyntacticConstraints:APilotStudyInthissectionwereporttheresultsofapilotstudytoevaluatethedifﬁcultyoforderingatargetsentenceifwearegivenatargetdependencytreeastheoneinFigure1,versusifwearejustgivenanunorderedbagoftargetlanguagewords.Thedifferencebetweenthosetwosettingsisthatwhenorderingatargetdependencytree,manyoftheordersofthesentencearenotallowed,becausetheywouldbenon-projectivewithrespecttothetree.Figure1(b)showssomeorderswhichviolatetheprojectivityconstraint.Ifthegiventargetdepen-dencytreeisprojectivewithrespecttothecorrectwordorder,constrainingthepossibleorderstotheonesconsistentwiththetreecanonlyhelpperfor-mance.Inourexperimentsonreferencesentences,thetargetdependencytreesareprojectivebycon-struction.If,however,thetargetdependencytreeprovidedisnotnecessarilyprojectivewithrespecttothebestwordorder,theconstraintmayormaynotbeuseful.ThiscouldhappeninourexperimentsonorderingMToutputsentences.Thusinthissectionweaimtoevaluatetheuse-fulnessoftheconstraintinbothsettings:referencesentenceswithprojectivedependencytrees,andMToutputsentenceswithpossiblynon-projectivede-pendencytrees.Wealsoseektoestablishabaselineforourtask.Ourmethodologyistotestasimpleandeffectiveordermodel,whichisusedbyallstateoftheartSMTsystems–atrigramlanguagemodel–inthetwosettings:orderinganunorderedbagofwords,andorderingatargetdependencytree.Ourexperimentaldesignisasfollows.Givenanunorderedsentencetandanunorderedtargetde-pendencytreetree(t),wedeﬁnetwospacesoftar-getsentenceorders.Thesearetheunconstrainedspaceofallpermutations,denotedbyPermutations(t)andthespaceofallordersoftwhichareprojec-tivewithrespecttothetargetdependencytree,de-notedbyTargetProjective(t,tree(t)).ForbothspacesS,weapplyastandardtrigramtargetlanguagemodeltoselectamostlikelyorderfromthespace;i.e.,weﬁndatargetorderorder∗S(t)suchthat:order∗S(t)=argmaxorder(t)∈SPrLM(order(t)).Theoperatorwhichﬁndsorder∗S(t)isdifﬁculttoimplementsincethetaskisNP-hardinbothset-12

ReferenceSentencesSpaceBLEUAvg.SizePermutations58.8261TargetProjective83.9229MTOutputSentencesSpaceBLEUAvg.SizePermutations26.3256TargetProjective31.7225Table1:Performanceofatri-gramlanguagemodelonorderingreferenceandMToutputsentences:un-constrainedorsubjecttotargettreeprojectivitycon-straints.tings,evenforabi-gramlanguagemodel(EisnerandTromble,2006).4Weimplementedleft-to-rightbeamA*searchforthePermutationsspace,andatree-basedbottomupbeamA*searchfortheTar-getProjectivespace.Togiveanestimateofthesearcherrorineachcase,wecomputedthenumberoftimesthecorrectorderhadabetterlanguagemodelscorethantheorderreturnedbythesearchalgorithm.5Thelowerboundsonsearcherrorwere4%forPer-mutationsand2%forTargetProjective,computedonreferencesentences.WecomparetheperformanceinBLEUofordersselectedfrombothspaces.Weevaluatetheperfor-manceonreferencesentencesandonMToutputsentences.Table1showstheresults.InadditiontoBLEUscores,thetableshowsthemediannumberofpossibleorderspersentenceforthetwospaces.ThehighestachievableBLEUonreferencesen-tencesis100,becausewearegiventhecorrectbagofwords.ThehighestachievableBLEUonMTout-putsentencesiswellbelow100(theBLEUscoreoftheMToutputsentencesis33).Table3describesthecharacteristicsofthemaindata-setsusedintheexperimentsinthispaper;thetestsetsweuseinthepresentpilotstudyarethereferencetestset(Ref-test)of1KsentencesandtheMTtestset(MT-test)of1,000sentences.Theresultsfromourexperimentshowthatthetar-gettreeprojectivityconstraintisextremelypowerfulonreferencesentences,wherethetreegivenisin-deedprojective.(Recallthatinordertoobtainthetargetdependencytreeinthissettingwehaveusedinformationfromthetrueorder,whichexplainsinpartthelargeperformancegain.)4Eventhoughthedependencytreeconstrainsthespace,thenumberofchildrenofanodeisnotboundedbyaconstant.5Thisisanunderestimateofsearcherror,becausewedon’tknowiftherewasanother(non-reference)orderwhichhadabetterscore,butwasnotfound.ThegaininBLEUduetotheconstraintwasnotaslargeonMToutputsentences,butwasstillcon-siderable.Thereductioninsearchspacesizeduetotheconstraintisenormous.Thereareabout230timesfewerorderstoconsiderinthespaceoftar-getprojectiveorders,comparedtothespaceofallpermutations.Fromtheseexperimentsweconcludethattheconstraintsimposedbyaprojectivetargetdependencytreeareextremelyinformative.WealsoconcludethattheconstraintsimposedbythetargetdependencytreesconstructedbyourbaselineMTsystemareveryinformativeaswell,eventhoughthetreesarenotnecessarilyprojectivewithrespecttothebestorder.ThustheprojectivityconstraintwithrespecttoareasonablygoodtargetdependencytreeisusefulforaddressingthesearchandmodelingproblemsforMTordering.4AGlobalOrderModelforTargetDependencyTreesIntherestofthepaperwepresentournewwordor-dermodelandevaluateitonreferencesentencesandinmachinetranslation.InlinewithpreviousworkonNLPtaskssuchasparsingandrecentworkonmachinetranslation,wedevelopadiscriminativeor-dermodel.Anadvantageofsuchamodelisthatwecaneasilycombinedifferentkindsoffeatures(suchassyntax-basedandsurface-based),andthatwecanoptimizetheparametersofourmodeldirectlyfortheevaluationmeasuresofinterest.Additionally,wedevelopagloballynormalizedmodel,whichavoidstheindependenceassumptionsinlocallynormalizedconditionalmodels.6Wetrainagloballog-linearmodelwitharichsetofsyntacticandsurfacefeatures.Becausethespaceofpossibleordersofanunordereddependencytreeisfactori-allylarge,weusesimplermodelstogenerateN-bestorders,whichwethenre-rankwithaglobalmodel.4.1GeneratingN-bestOrdersThesimplermodelswhichweusetogenerateN-bestordersoftheunorderedtargetdependencytreesarethestandardtrigramlanguagemodelusedinSection3,andanotherstatisticalmodel,whichwecallaLo-calTreeOrderModel(LTOM).TheLTOMmodel6Thosemodelsoftenassumethatcurrentdecisionsareinde-pendentoffutureobservations.13

[(cid:11779)(cid:7574)]this-1eliminatesthesixminutedelay+1[(cid:19234)(cid:19291)-2][(cid:19258)(cid:19287)(cid:19289)][6][(cid:3261)][(cid:13382)][(cid:19261)][(cid:12675)(cid:19291)-1][(cid:19227)][(cid:19236)(cid:19291)(cid:19277)(cid:19240)]PronVerbDetFuncwFuncwNoun[kore][niyori][roku][fun][kan][no][okure][ga][kaishou][saremasu]PronPospNounNounNounPospNounPospVnAuxv“this”“by”6“minute”“period”“of”“delay”“eliminate”PASSIVEFigure2:Dependencyparseonthesource(English)sentence,alignmentandprojectedtreeonthetarget(Japanese)sentence.Noticethattheprojectedtreeisonlypartialandisusedtoshowthehead-relativemovement.usessyntacticinformationfromthesourceandtar-getdependencytrees,andorderseachlocaltreeofthetargetdependencytreeindependently.Itfollowstheordermodeldeﬁnedin(Quirketal.,2005).Themodelassignsaprobabilitytothepositionofeachtargetnode(modiﬁer)relativetoitspar-ent(head),basedoninformationinboththesourceandtargettrees.Theprobabilityofanorderofthecompletetargetdependencytreedecomposesintoaproductoverprobabilitiesofpositionsforeachnodeinthetreeasfollows:P(order(t)|s,t)=Yn∈tP(pos(n,parent(n))|s,t)Here,positionismodelledintermsofclosenesstotheheadinthedependencytree.Theclosestpre-modiﬁerofagivenheadhasposition−1;theclosestpost-modiﬁerhasaposition1.Figure2showsanexampledependencytreepairannotatedwithhead-relativepositions.AsmallsetoffeaturesisusedtoreﬂectlocalinformationinthedependencytreetomodelP(pos(n,parent(n))|s,t):(i)lexicalitemsofnandparent(n),(ii)lexicalitemsofthesourcenodesalignedtonandparent(n),(iii)part-of-speechofthesourcenodesalignedtothenodeanditsparent,and(iv)head-relativepositionofthesourcenodealignedtothetargetnode.Wetrainalog-linearmodelwhichusesthesefea-turesonatrainingsetofalignedsentenceswithsourceandtargetdependencytreesintheformofFigure2.Themodelisalocal(non-sequence)clas-siﬁer,becausethedecisiononwheretoplaceeachnodedoesnotdependontheplacementofanyothernodes.Sincethelocaltreeordermodellearnstoorderwholesubtreesofthetargetdependencytree,andsinceitusessyntacticinformationfromthesource,itprovidesanalternativeviewcomparedtothetrigramlanguagemodel.TheexampleinFigure2showsthattheheadword“eliminates”takesadependent“this”totheleft(position−1),andontheJapaneseside,theheadword“kaishou”(correspondingto“eliminates”)takesadependent“kore”(correspond-ingto“this”)totheleft(position−2).Thetrigramlanguagemodelwouldnotcapturethepositionof“kore”withrespectto“kaishou”,becausethewordsarefartherthanthreepositionsaway.Weusethelanguagemodelandthelocaltreeor-dermodeltocreateN-besttargetdependencytreeorders.Inparticular,wegeneratetheN-bestlistsfromasimplelog-linearcombinationofthetwomodels:P(o(t)|s,t)∝PLM(o(t)|t)PLTOM(o(t)|s,t)λwhereo(t)denotesanorderofthetarget.7Weusedabottom-upbeamA*searchtogenerateN-bestor-ders.Theperformanceofeachofthesetwomodelsandtheircombination,togetherwiththe30-bestor-acleperformanceonreferencesentencesisshowninTable2.Aswecansee,the30-bestoracleperfor-manceofthecombinedmodel(98.0)ismuchhigherthanthe1-bestperformance(92.6)andthusthereisalotofroomforimprovement.4.2ModelThelog-linearrerankingmodelisdeﬁnedasfol-lows.Foreachsentencepairspl(l=1,2,...,L)inthetrainingdata,wehaveNcandidatetargetwordordersol,1,ol,2,...,ol,N,whicharetheordersgener-atedfromthesimplermodels.Withoutlossofgen-erality,wedeﬁneol,1tobetheorderwiththehighestBLEUscorewithrespecttothecorrectorder.8Wedeﬁneasetoffeaturefunctionsfm(ol,n,spl)todescribeatargetwordorderol,nofagivensen-tencepairspl.Inthelog-linearmodel,acorrespond-ingweightsvectorλisusedtodeﬁnethedistributionoverallpossiblecandidateorders:p(ol,n|spl,λ)=eλF(ol,n,spl)Pn′eλF(ol,n′,spl)7Weusedthevalueλ=.5,whichweselectedonadevel-opmentsettomaximizeBLEU.8ToavoidtheproblemthatallorderscouldhaveaBLEUscoreof0ifnoneofthemcontainsacorrectwordfour-gram,wedeﬁnesentence-levelk-gramBLEU,wherekisthehighestorder,k≤4,forwhichthereexistsacorrectk-graminatleastoneoftheN-Bestorders.14

Wetraintheparametersλbyminimizingtheneg-ativelog-likelihoodofthetrainingdataplusaquadraticregularizationterm:L(λ)=−Pllogp(ol,1|spi,λ)+12σ2Pmλm2WealsoexploredmaximizingexpectedBLEUasourobjectivefunction,butsinceitisnotconvex,theperformancewaslessstableandultimatelyslightlyworse,ascomparedtothelog-likelihoodobjective.4.3FeaturesWedesignfeaturestocaptureboththehead-relativemovementandthesurfacesequencemovementofwordsinasentence.Weexperimentwithdifferentcombinationsoffeaturesandshowtheircontribu-tioninTable2forreferencesentencesandTable4inmachinetranslation.Thenotationsusedintheta-blesaredeﬁnedasfollows:Baseline:LTOM+LMasdescribedinSection4.1WordBigram:Wordbigramsofthetargetsen-tence.ExamplesfromFigure2:“kore”+“niyori”,“niyori”+“roku”.DISP:Displacementfeature.Foreachwordposi-tioninthetargetsentence,weexaminethealign-mentofthecurrentwordandthepreviousword,andcategorizethepossiblepatternsinto3kinds:(a)par-allel,(b)crossing,and(c)widening.Figure3showshowthesethreecategoriesaredeﬁned.PharaohDISP:DisplacementasusedinPharaoh(Koehn,2004).Foreachpositioninthesentence,thevalueofthefeatureisonelessthanthedifference(absolutevalue)ofthepositionsofthesourcewordsalignedtothecurrentandtheprevioustargetword.POSsandPOSt:POStagsonthesourceandtargetsides.ForJapanese,wehaveasetof19POStags.’+’meansmakingconjunctionoffeaturesandprev()meansusingtheinformationassociatedwiththewordfromposition−1.Inallexploredmodels,weincludethelog-probabilityofanorderaccordingtothelanguagemodelandthelog-probabilityaccordingtothelo-caltreeordermodel,thetwofeaturesusedbythebaselinemodel.5EvaluationonReferenceSentencesOurexperimentsonorderingreferencesentencesuseasetof445KEnglishsentenceswiththeirref-erenceJapanesetranslations.Thisisasubsetofthe(cid:40)(cid:78)(cid:40)(cid:78)(cid:14)(cid:20)(cid:45)(cid:76)(cid:45)(cid:76)(cid:14)(cid:20)(a)parallel(cid:40)(cid:78)(cid:40)(cid:78)(cid:14)(cid:81)(cid:45)(cid:76)(cid:14)(cid:20)(cid:45)(cid:76)(cid:14)(cid:21)(b)crossing(cid:40)(cid:78)(cid:40)(cid:78)(cid:14)(cid:81)(cid:45)(cid:76)(cid:14)(cid:20)(cid:45)(cid:76)(cid:14)(cid:21)(c)wideningFigure3:Displacementfeature:differentalignmentpatternsoftwocontiguouswordsinthetargetsen-tence.setMT-traininTable3.Thesentenceswereanno-tatedwithalignment(usingGIZA++(OchandNey,2004))andsyntacticdependencystructuresofthesourceandtarget,obtainedasdescribedinSection2.JapanesePOStagswereassignedbyanautomaticPOStagger,whichisalocalclassiﬁernotusingtagsequenceinformation.Weused400Ksentencepairsfromthecompletesettotraintheﬁrstpassmodels:thelanguagemodelwastrainedon400Ksentences,andthelocaltreeordermodelwastrainedon100Kofthem.Wegen-eratedN-besttargettreeordersfortherestofthedata(45Ksentencepairs),anduseditfortrainingandevaluatingthere-rankingmodel.There-rankingmodelwastrainedon44Ksentencepairs.Allmod-elswereevaluatedontheremaining1,000sentencepairsset,whichisthesetRef-testinTable3.ThetoppartofTable2presentsthe1-bestBLEUscores(actualperformance)and30-bestor-acleBLEUscoresoftheﬁrst-passmodelsandtheirlog-linearcombination,describedinSection4.Wecanseethatthecombinationofthelanguagemodelandthelocaltreeordermodeloutperformedeithermodelbyalargemargin.Thisindicatesthatcombin-ingsyntactic(fromtheLTOMmodel)andsurface-based(fromthelanguagemodel)informationisveryeffectiveevenatthisstageofselectingN-bestordersforre-ranking.Accordingtothe30-bestoracleper-formanceofthecombinedmodelLTOM+LM,98.0BLEUistheupperboundonperformanceofourre-rankingapproach.Thebottompartofthetableshowstheperfor-manceofthegloballog-linearmodel,whenfeaturesinadditiontothescoresfromthetwoﬁrst-passmod-elsareaddedtothemodel.Addingword-bigramfeaturesincreasedperformancebyabout0.6BLEUpoints,indicatingthattraininglanguage-modellikefeaturesdiscriminativelytooptimizeorderingper-formance,isindeedworthwhile.Nextwecompare15

First-passmodelsModelBLEU1best30bestLangModel(Permutations)58.871.2LangModel(TargetProjective)83.995.0LocalTreeOrderModel75.887.3LocalTreeOrderModel+LangModel92.698.0Re-rankingModelsFeaturesBLEUBaseline92.60WordBigram93.19PharaohDISP92.94DISP93.57DISP+POSs94.04DISP+POSs+POSt94.14DISP+POSs+POSt,prev(DISP)+POSs+POSt94.34DISP+POSs+POSt,prev(DISP)+POSs+POSt,WB94.50Table2:Performanceoftheﬁrst-passordermodelsand30-bestoracleperformance,followedbyperfor-manceofre-rankingmodelfordifferentfeaturesets.Resultsareonreferencesentences.thePharaohdisplacementfeaturetothedisplace-mentfeatureweillustratedinFigure3.WecanseethatthePharaohdisplacementfeatureimprovesperformanceofthebaselineby.34points,whereasourdisplacementfeatureimprovesperformancebynearly1BLEUpoint.ConcatenatingtheDISPfea-turewiththePOStagofthesourcewordalignedtothecurrentwordimprovedperformanceslightly.Theresultsshowthatsurfacemovementfeatures(i.e.theDISPfeature)improvetheperformanceofamodelusingsyntactic-movementfeatures(i.e.theLTOMmodel).Additionally,addingpart-of-speechinformationfrombothlanguagesincombi-nationwithdisplacement,andusingahigherorderonthedisplacementfeatureswasuseful.Theper-formanceofourbestmodel,whichincludedallin-formationsources,is94.5BLEUpoints,whichisa35%improvementovertheﬁst-passmodels,relativetotheupperbound.6EvaluationinMachineTranslationWeapplyourmodeltomachinetranslationbyre-orderingthetranslationproducedbyabaselineMTsystem.OurbaselineMTsystemconstructs,foreachtargettranslationhypothesis,atargetdepen-dencytree.ThuswecanapplyourmodeltoMToutputinexactlythesamewayasforreferencesen-tences,butusingmuchnoisierinput:asourcesen-tencewithadependencytree,wordalignmentandanunorderedtargetdependencytreeastheexampleshowninFigure2.Thedifferenceisthatthetargetdependencytreewilllikelynotcontainthecorrectdatasetnumsent.EnglishJapaneseavg.lenvocabavg.lenvocabMT-train500K15.877K18.779KMT-test1K17.5–20.9–Ref-test1K17.5–21.2–Table3:Maindatasetsusedinexperiments.targetwordsand/orwillnotbeprojectivewithre-specttothebestpossibleorder.6.1BaselineMTSystemOurbaselineSMTsystemisthesystemofQuirketal.(2005).Ittranslatesbyﬁrstderivingadepen-dencytreeforthesourcesentenceandthentrans-latingthesourcedependencytreetoatargetdepen-dencytree,usingasetofprobabilisticmodels.Thetranslationisbasedontreeletpairs.Atreeletisaconnectedsubgraphofthesourceortargetdepen-dencytree.Atreelettranslationpairisapairofword-alignedsourceandtargettreelets.ThebaselineSMTmodelcombinesthistreelettranslationmodelwithotherfeaturefunctions—atargetlanguagemodel,atreeordermodel,lexicalweightingfeaturestosmooththetranslationprob-abilities,wordcountfeature,andtreelet-pairscountfeature.Thesemodelsarecombinedasfeaturefunc-tionsina(log)linearmodelforpredictingatargetsentencegivenasourcesentence,intheframeworkproposedby(OchandNey,2002).TheweightsofthismodelaretrainedtomaximizeBLEU(OchandNey,2004).TheSMTsystemistrainedusingthesameformofdataasourordermodel:parallelsourceandtargetdependencytreesasinFigure2.OfparticularinterestarethecomponentsinthebaselineSMTsystemcontributingmosttowordor-derdecisions.TheSMTsystemusesthesametargetlanguagetrigrammodelandlocaltreeordermodel,asweareusingforgeneratingN-bestordersforre-ranking.Thusthebaselinesystemalreadyusesourﬁrst-passordermodelsandonlylackstheadditionalinformationprovidedbyourre-rankingordermodel.6.2DataandExperimentalResultsThebaselineMTsystemwastrainedontheMT-traindatasetdescribedinTable3.ThetestsetfortheMTexperimentisa1Ksentencessetfromthesamedo-main(shownasMT-testinthetable).TheweightsinthelinearmodelusedbythebaselineSMTsystemweretunedonaseparatedevelopmentset.Table4showstheperformanceoftheﬁrst-passmodelsinthetoppart,andtheperformanceofour16

First-passmodelsModelBLEU1best30bestBaselineMTSystem33.0–LangModel(Permutations)26.328.7LangModel(TargetCohesive)31.735.0LocalTreeOrderModel27.231.5LocalTreeOrderModel+LangModel33.636.0Re-rankingModelsFeaturesBLEUBaseline33.56WordBigram34.11PharaohDISP34.67DISP34.90DISP+POSs35.28DISP+POSs+POSt35.22DISP+POSs+POSt,prev(DISP)+POSs+POSt35.33DISP+POSs+POSt,prev(DISP)+POSs+POSt,WB35.37Table4:Performanceoftheﬁrstpassordermodelsand30-bestoracleperformance,followedbyperfor-manceofre-rankingmodelfordifferentfeaturesets.ResultsareinMT.re-rankingmodelinthebottompart.TheﬁrstrowofthetableshowstheperformanceofthebaselineMTsystem,whichisaBLEUscoreof33.Ourﬁrst-passandre-rankingmodelsre-orderthewordsofthis1-bestoutputfromtheMTsystem.Asforref-erencesentences,thecombinationofthetwoﬁrst-passmodelsoutperformstheindividualmodels.The1-bestperformanceofthecombinationis33.6andthe30-bestoracleis36.0.Thusthebestwecoulddowithourre-rankingmodelinthissettingis36BLEUpoints.9Ourbestre-rankingmodelachieves2.4BLEUpointsimprovementoverthebaselineMTsystemand1.8pointsimprovementovertheﬁrst-passmodels,asshowninthetable.Thetrendsherearesimilartotheonesobservedinourreferenceex-periments,withthedifferencethattargetPOStagswerelessuseful(perhapsduetoungrammaticalcan-didates)andthedisplacementfeaturesweremoreuseful.Wecanseethatourre-rankingmodelal-mostreachedtheupperboundoracleperformance,reducingthegapbetweentheﬁrst-passmodelsper-formance(33.6)andtheoracle(36.0)by75%.7ConclusionsandFutureWorkWehavepresentedadiscriminativesyntax-basedor-dermodelformachinetranslation,trainedtotose-9Noticethatthecombinationofourtwoﬁrst-passmodelsoutperformsthebaselineMTsystembyhalfapoint(33.6ver-sus33.0).ThisisperhapsduetothefactthattheMTsystemsearchesthroughamuchlargerspace(possiblewordtransla-tionsinadditiontowordorders),andthuscouldhaveahighersearcherror.lectfromthespaceofordersprojectivewithrespecttoatargetdependencytree.Weinvestigatedacom-binationoffeaturesmodelingsurfacemovementandsyntacticmovementphenomenaandshowedthatthesetwoinformationsourcesarecomplementaryandtheircombinationispowerful.Ourresultsonor-deringMToutputandreferencesentenceswereveryencouraging.Weobtainedsubstantialimprovementbythesimplemethodofpost-processingthe1-bestMToutputtore-ordertheproposedtranslation.Inthefuture,wewouldliketoexploretighterintegra-tionofourordermodelwiththeSMTsystemandtodevelopmoreaccuratealgorithmsforconstructingprojectivetargetdependencytreesintranslation.ReferencesY.Al-OnaizanandK.Papineni.2006.Distortionmodelsforstatisticalmachinetranslation.InACL.D.Chiang.2005.Ahierarchicalphrase-basedmodelforstatis-ticalmachinetranslation.InACL.M.Collins.2000.Discriminativererankingfornaturallanguageparsing.InICML,pages175–182.JEisnerandR.W.Tromble.2006.Localsearchwithverylarge-scaleneighborhoodsforoptimalpermutationsinma-chinetranslation.InHLT-NAACLWorkshop.H.Fox.2002.Phrasalcohesionandstatisticalmachinetransla-tion.InEMNLP.M.Galley,J.Graehl,K.Knight,D.Marcu,S.DeNeefe,W.Wang,andI.Thayer.2006.Scalableinferenceandtrain-ingofcontext-richsyntactictranslationmodels.InACL.P.Koehn.2004.Pharaoh:Abeamsearchdecoderforphrase-basedstatisticalmachinetranslationmodels.InAMTA.R.Kuhn,D.Yuen,M.Simard,P.Paul,G.Foster,E.Joanis,andH.Johnson.2006.Segmentchoicemodels:Feature-richmodelsforglobaldistortioninstatisticalmachinetransla-tion.InHLT-NAACL.F.J.OchandH.Ney.2002.Discriminativetrainingandmax-imumentropymodelsforstatisticalmachinetranslation.InACL.F.J.OchandH.Ney.2004.Thealignmenttemplateapproachtostatisticalmachinetranslation.ComputationalLinguistics,30(4).K.Papineni,S.Roukos,T.Ward,andW.Zhu.2001.BLEU:amethodforautomaticevaluationofmachinetranslation.InACL.C.Quirk,A.Menezes,andC.Cherry.2005.Dependencytreelettranslation:SyntacticallyinformedphrasalSMT.InACL.B.Wellington,S.Waxmonsky,andI.DanMelamed.2006.Empiricallowerboundsonthecomplexityoftranslationalequivalence.InACL-COLING.D.Wu.1997.Stochasticinversiontransductiongrammarsandbilingualparsingofparallelcorpora.ComputationalLin-guistics,23(3):377–403.D.Xiong,Q.Liu,andS.Lin.2006.Maximumentropybasedphrasereorderingmodelforstatisticalmachinetranslation.InACL.K.YamadaandKevinKnight.2001.Asyntax-basedstatisticaltranslationmodel.InACL.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 17–24,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

17

TailoringWordAlignmentstoSyntacticMachineTranslationJohnDeNeroComputerScienceDivisionUniversityofCalifornia,Berkeleydenero@berkeley.eduDanKleinComputerScienceDivisionUniversityofCalifornia,Berkeleyklein@cs.berkeley.eduAbstractExtractingtreetransducerrulesforsyntac-ticMTsystemscanbehinderedbywordalignmenterrorsthatviolatesyntacticcorre-spondences.Weproposeanovelmodelforunsupervisedwordalignmentwhichexplic-itlytakesintoaccounttargetlanguagecon-stituentstructure,whileretainingtherobust-nessandefﬁciencyoftheHMMalignmentmodel.Ourmodel’spredictionsimprovetheyieldofatreetransducerextractionsystem,withoutsacriﬁcingalignmentquality.Wealsodiscusstheimpactofvariousposterior-basedmethodsofreconcilingbidirectionalalignments.1IntroductionSyntacticmethodsareanincreasinglypromisingap-proachtostatisticalmachinetranslation,beingbothalgorithmicallyappealing(Melamed,2004;Wu,1997)andempiricallysuccessful(Chiang,2005;Galleyetal.,2006).However,despiterecentprogress,almostallsyntacticMTsystems,indeedstatisticalMTsystemsingeneral,builduponcrudelegacymodelsofwordalignment.Thisdependencerunsdeep;forexample,Galleyetal.(2006)requireswordalignmentstoprojecttreesfromthetargetlan-guagetothesource,whileChiang(2005)requiresalignmentstoinducegrammarrules.Wordalignmentmodelshavenotstoodstillinre-centyears.Unsupervisedmethodshaveseensub-stantialreductionsinalignmenterror(Liangetal.,2006)asmeasuredbythenowmuch-malignedAERmetric.Ahostofdiscriminativemethodshavebeenintroduced(Taskaretal.,2005;Moore,2005;AyanandDorr,2006).However,fewofthesemethodshaveexplicitlyaddressedthetensionbetweenwordalignmentsandthesyntacticprocessesthatemploythem(CherryandLin,2006;Daum´eIIIandMarcu,2005;LopezandResnik,2005).WeareparticularlymotivatedbysystemsliketheonedescribedinGalleyetal.(2006),whichcon-structstranslationsusingtree-to-stringtransducerrules.Theserulesareextractedfromabitextanno-tatedwithbothEnglish(targetside)parsesandwordalignments.Rulesareextractedfromtargetsideconstituentsthatcanbeprojectedontocontiguousspansofthesourcesentenceviathewordalignment.Constituentsthatprojectontonon-contiguousspansofthesourcesentencedonotyieldtransducerrulesthemselves,andcanonlybeincorporatedintolargertransducerrules.Thus,ifthewordalignmentofasentencepairdoesnotrespecttheconstituentstruc-tureofthetargetsentence,thentheminimaltransla-tionunitsmustspanlargetreefragments,whichdonotgeneralizewell.Wepresentandevaluateanunsupervisedwordalignmentmodelsimilarincharacterandcompu-tationtotheHMMmodel(NeyandVogel,1996),butwhichincorporatesanovel,syntax-awaredistor-tioncomponentwhichconditionsontargetlanguageparsetrees.Thesetrees,whileautomaticallygener-atedandthereforeimperfect,arenonetheless(1)ausefulsourceofstructuralbiasand(2)thesametreeswhichconstrainfuturestagesofprocessinganyway.Inourmodel,thetreesdonotruleoutanyalign-ments,butrathersoftlyinﬂuencetheprobabilityoftransitioningbetweenalignmentpositions.Inpar-ticular,transitionprobabilitiesconditionuponpathsthroughthetargetparsetree,allowingthemodeltopreferdistortionswhichrespectthetreestructure.18

Ourmodelgenerateswordalignmentsthatbetterrespecttheparsetreesuponwhichtheyarecondi-tioned,withoutsacriﬁcingalignmentquality.UsingthejointtrainingtechniqueofLiangetal.(2006)toinitializethemodelparameters,weachieveanAERsuperiortotheGIZA++implementationofIBMmodel4(OchandNey,2003)andareduc-tionof56.3%inalignedinteriornodes,ameasureofagreementbetweenalignmentsandparses.Asaresult,ouralignmentsyieldmorerules,whichbettermatchthosewewouldextracthadweusedmanualalignments.2TranslationwithTreeTransducersInatreetransducersystem,asinphrase-basedsys-tems,thecoverageandgeneralityofthetransducerinventoryisstronglyrelatedtotheeffectivenessofthetranslationmodel(Galleyetal.,2006).Wewilldemonstratethatthiscoverage,inturn,isrelatedtothedegreetowhichinitialwordalignmentsrespectsyntacticcorrespondences.2.1RuleExtractionGalleyetal.(2004)proposesamethodforextractingtreetransducerrulesfromaparallelcorpus.Givenasourcelanguagesentences,atargetlanguageparsetreetofitstranslation,andaword-levelalignment,theiralgorithmidentiﬁestheconstituentsintwhichmapontocontiguoussubstringsofsviathealign-ment.Therootnodesofsuchconstituents–denotedfrontiernodes–serveastherootsandleavesoftreefragmentsthatformminimaltransducerrules.Frontiernodesaredistinguishedbytheircompat-ibilitywiththewordalignment.Foraconstituentcoft,weconsiderthesetofsourcewordsscthatarealignedtoc.Ifnoneofthesourcewordsinthelin-earclosures∗c(thewordsbetweentheleftmostandrightmostmembersofsc)alignstoatargetwordout-sideofc,thentherootofcisafrontiernode.Theremaininginteriornodesdonotgeneraterules,butcanplayasecondaryroleinatranslationsystem.1Therootsofnull-alignedconstituentsarenotfron-tiernodes,butcanattachproductivelytomultipleminimalrules.1Interiornodescanbeused,forinstance,inevaluatingsyntax-basedlanguagemodels.Theyalsoservetodifferentiatetransducerrulesthathavethesamefrontiernodesbutdifferentinternalstructure.Twotransducerrules,t1→s1andt2→s2,canbecombinedtoformlargertranslationunitsbycomposingt1andt2atasharedfrontiernodeandappropriatelyconcatenatings1ands2.How-ever,notechniquehasyetbeenshowntorobustlyextractsmallercomponentrulesfromalargetrans-ducerrule.Thus,forthepurposeofmaximizingthecoverageoftheextractedtranslationmodel,wepre-fertoextractmanysmall,minimalrulesandgen-eratelargerrulesviacomposition.Maximizingthenumberoffrontiernodessupportsthisgoal,whileinducingmanyalignedinteriornodeshindersit.2.2WordAlignmentInteractionsWenowturntotheinteractionbetweenwordalign-mentsandthetransducerextractionalgorithm.Con-sidertheexamplesentenceinﬁgure1A,whichdemonstrateshowaparticulartypeofalignmenter-rorpreventstheextractionofmanyusefultransducerrules.Themistakenlink[la⇒the]intervenesbe-tweenax´esandcarri`er,whichbothalignwithinanEnglishadjectivephrase,whilelaalignstoadistantsubspanoftheEnglishparsetree.Inthisway,thealignmentviolatestheconstituentstructureoftheEnglishparse.Whilealignmenterrorsareundesirableingen-eral,thiserrorisparticularlyproblematicforasyntax-basedtranslationsystem.Inaphrase-basedsystem,thislinkwouldblockextractionofthephrases[ax´essurlacarri`er⇒careeroriented]and[lesemplois⇒thejobs]becausetheerroroverlapswithboth.However,theinterveningphrase[em-ploissont⇒jobsare]wouldstillbeextracted,atleastcapturingthetransferofsubject-verbagree-ment.Bycontrast,thetreetransducerextractionmethodfailstoextractanyofthesefragments:thealignmenterrorcausesallnon-terminalnodesintheparsetreetobeinteriornodes,excludingpre-terminalsandtheroot.Figure1Bexposestheconse-quences:awidearrayofdesiredrulesarelostduringextraction.Thedegreetowhichawordalignmentrespectstheconstituentstructureofaparsetreecanbequan-tiﬁedbythefrequencyofinteriornodes,whichindi-catealignmentpatternsthatcrossconstituentbound-aries.Toachievemaximumcoverageofthetrans-lationmodel,wehopetoinfertree-violatingalign-mentsonlywhensyntacticstructurestrulydiverge.19

.(A)(B)(i)(ii)SNPVPADJPNNVBNNNSDTAUXThejobsarecareeroriented.lesemploissontaxéssurlacarrière..LegendCorrect proposed word alignment consistent with human annotation.Proposed word alignment error inconsistent with human annotation.Word alignment constellation that renders the root of the relevant constituent to be an interior node.Word alignment constellation that would allow a phrase extraction in a phrase-based translation system, but which does not correspond to an English constituent.BoldItalicFrontier node (agrees with alignment)Interior node (inconsistent with alignment)(S(NP(DT[0]NNS[1])(VPAUX[2](ADJVNN[3]VBN[4]).[5])→[0][1][2][3][4][5](S(NP(DT[0](NNSjobs))(VPAUX[1](ADJVNN[2]VBN[3]).[4])→[0]sont[1][2][3][4](S(NP(DT[0](NNSjobs))(VP(AUXare)(ADJVNN[1]VBN[2]).[3])→[0]emploissont[1][2][3](SNP[0]VP[1].[2])→[0][1][2](S(NP(DT[0]NNS[1])VP[2].[3])→[0][1][2][3](S(NP(DT[0](NNSjobs))VP[2].[3])→[0]emplois[2][3](S(NP(DT[0](NNSjobs))(VPAUX[1]ADJV[2]).[3])→[0]emplois[1][2][3](S(NP(DT[0](NNSjobs))(VP(AUXare)ADJV[1]).[2])→[0]emploissont[1][2]Figure1:Inthistransducerextractionexample,(A)showsaproposedalignmentfromourtestsetwithanalignmenterrorthatviolatestheconstituentstructureoftheEnglishsentence.Theresultingfrontiernodesareprintedinbold;allnodeswouldbefrontiernodesunderacorrectalignment.(B)showsasmallsampleoftherulesextractedundertheproposedalignment,(ii),andthecorrectalignment,(i)and(ii).Thesinglealignmenterrorpreventstheextractionofallrulesin(i)andmanymore.Thisalignmentpatternwasobservedinourtestsetandcorrectedbyourmodel.3UnsupervisedWordAlignmentToallowforthispreference,wepresentanovelcon-ditionalalignmentmodelofaforeign(source)sen-tencef={f1,...,fJ}givenanEnglish(target)sen-tencee={e1,...,eI}andatargettreestructuret.LiketheclassicIBMmodels(Brownetal.,1994),ourmodelwillintroducealatentalignmentvectora={a1,...,aJ}thatspeciﬁesthepositionofanalignedtargetwordforeachsourceword.Formally,ourmodeldescribesp(a,f|e,t),butotherwisebor-rowsheavilyfromtheHMMalignmentmodelofNeyandVogel(1996).TheHMMmodelcapturestheintuitionthatthealignmentvectorawillingeneralprogressacrossthesentenceeinapatternwhichismostlylocal,per-hapswithafewlargejumps.Thatis,alignmentsarelocallymonotonicmoreoftenthannot.Formally,theHMMmodelfactorsas:p(a,f|e)=JYj=1pd(aj|aj−,j)p‘(fj|eaj)wherej−isthepositionofthelastnon-null-alignedsourcewordbeforepositionj,p‘isalexicaltransfermodel,andpdisalocaldistortionmodel.Asinallsuchmodels,thelexicalcomponentp‘isacollec-tionofunsmoothedmultinomialdistributionsover20

foreignwords.Thedistortionmodelpd(aj|aj−,j)isadistribu-tionoverthesigneddistanceaj−aj−,typicallyparameterizedasamultinomial,Gaussianorexpo-nentialdistribution.Theimplementationthatservesasourbaselineusesamultinomialdistributionwithseparateparametersforj=1,j=Jandsharedparametersforall1<j<J.Nullalignmentshaveﬁxedprobabilityatanyposition.Inferenceoverarequiresonlythestandardforward-backwardalgo-rithm.3.1Syntax-SensitiveDistortionThebroadandrobustsuccessoftheHMMalign-mentmodelunderscorestheutilityofitsassump-tions:thatword-leveltranslationscanbeusefullymodeledviaﬁrst-degreeMarkovtransitionsandin-dependentlexicalproductions.However,itsdistor-tionmodelconsidersonlystringdistance,disregard-ingtheconstituentstructureoftheEnglishsentence.Toallowsyntax-sensitivedistortion,weconsideranewdistortionmodeloftheformpd(aj|aj−,j,t).Weconditionontviaagenerativeprocessthattran-sitionsbetweentwoEnglishpositionsbytraversingtheuniqueshortestpathρ(aj−,aj,t)throughtfromaj−toaj.Weconstrainourselvestothisshortestpathusingastagedgenerativeprocess.Stage1(POP(ˆn),STOP(ˆn)):Startingintheleafnodeataj−,wechoosewhethertoSTOPorPOPfromchildtoparent,conditioningonthetypeoftheparentnodeˆn.UponchoosingSTOP,wetransitiontostage2.Stage2(MOVE(ˆn,d)):Again,conditioningonthetypeoftheparentˆnofthecurrentnoden,wechooseasibling¯nbasedonthesigneddistanced=φˆn(n)−φˆn(¯n),whereφˆn(n)istheindexofninthechildlistofˆn.Zerodistancemovesaredisallowed.AfterexactlyoneMOVE,wetransitiontostage3.Stage3(PUSH(n,φn(˘n))):Giventhecurrentnoden,weselectoneofitschildren˘n,conditioningonthetypeofnandthepositionofthechildφn(˘n).WecontinuetoPUSHuntilreachingaleaf.Thisprocessisaﬁrst-degreeMarkovwalkthroughthetree,conditioningonthecurrentnodeStage 1: { Pop(VBN), Pop(ADJP), Pop(VP), Stop(S) }Stage 2: { Move(S, -1) }Stage 3: { Push(NP, 1), Push(DT, 1) }SNPVPADJPNNVBNNNSDTAUXThejobsarecareeroriented..Figure2:Anexamplesequenceofstagedtreetran-sitionsimpliedbytheuniqueshortestpathfromthewordoriented(aj−=5)tothewordthe(aj=1).anditsimmediatesurroundingsateachstep.Ween-forcethepropertythatρ(aj−,aj,t)beuniquebystag-ingtheprocessanddisallowingzerodistancemovesinstage2.Figure2givesanexamplesequenceoftreetransitionsforasmallparsetree.Theparameterizationofthisdistortionmodelfol-lowsdirectlyfromitsgenerativeprocess.Givenapathρ(aj−,aj,t)withr=k+m+3nodesincludingthetwoleaves,thenearestcommonancestor,kin-terveningnodesontheascentandmonthedescent,weexpressitasatripleofstagedtreetransitionsthatincludekPOPs,aSTOP,aMOVE,andmPUSHes:{POP(n2),...,POP(nk+1),STOP(nk+2)}{MOVE(nk+2,φ(nk+3)−φ(nk+1))}{PUSH(nk+3,φ(nk+4)),...,PUSH(nr−1,φ(nr))}Next,weassignprobabilitiestoeachtreetransi-tionineachstage.Inselectingthesedistributions,weaimtomaintaintheoriginalHMM’ssensitivitytotargetwordorder:•SelectingPOPorSTOPisasimpleBernoullidistributionconditioneduponanodetype.•WemodelbothMOVEandPUSHasmultino-mialdistributionsoverthesigneddistanceinpositions(assumingastartingpositionof0forPUSH),echoingtheparameterizationpopularinimplementationsoftheHMMmodel.ThismodelreducestotheclassicHMMdistor-tionmodelgivenminimalEnglishtreesofonlyuni-formlylabeledpre-terminalsandarootnode.Theclassic0-distancedistortionwouldcorrespondtothe21

00.20.40.6-2-1012345LikelihoodHMMSyntacticThiswouldrelievethepressureonoil.SVBDT.MDVPVPNPPPDTNNINNNFigure3:Forthisexamplesentence,thelearneddis-tortiondistributionofpd(aj|aj−,j,t)resemblesitscounterpartpd(aj|aj−,j)oftheHMMmodelbutre-ﬂectstheconstituentstructureoftheEnglishtreet.Forinstance,theshortpathfromrelievetoongivesahightransitionlikelihood.STOPprobabilityofthepre-terminallabel;allotherdistanceswouldcorrespondtoMOVEprobabilitiesconditionedontherootlabel,andtheprobabilityoftransitioningtotheterminalstatewouldcorrespondtothePOPprobabilityoftherootlabel.Asinamultinomial-distortionimplementationoftheclassicHMMmodel,wemustsometimesartiﬁ-ciallynormalizethesedistributionsinthedeﬁcientcasethatcertainjumpsextendbeyondtheendsofthelocalrules.Forthisreason,MOVEandPUSHareactuallyparameterizedbythreevalues:anodetype,asigneddistance,andarangeofoptionsthatdictatesanormalizationadjustment.Onceeachtreetransitiongeneratesascore,theirproductgivestheprobabilityoftheentirepath,andtherebythecostofthetransitionbetweenstringpo-sitions.Figure3showsanexamplelearneddistribu-tionthatreﬂectsthestructureofthegivenparse.Withthesederivationstepsinplace,wemustad-dressahandfulofspecialcasestocompletethegen-erativemodel.WerequirethattheMarkovwalkfromleaftoleafoftheEnglishtreemuststartandendattheroot,usingthefollowingassumptions.1.Givennopreviousalignment,weforegostages1and2andbeginwithaseriesofPUSHesfromtherootofthetreetothedesiredleaf.2.Givennosubsequentalignments,weskipstages2and3afteraseriesofPOPsincludingapopconditionedontherootnode.3.Iftheﬁrstchoiceinstage1istoSTOPatthecurrentleaf,thenstage2and3areunneces-sary.Hence,achoicetoSTOPimmediatelyisachoicetoemitanotherforeignwordfromthecurrentEnglishword.4.Weﬂattenunarytransitionsfromthetreewhencomputingdistortionprobabilities.5.NullalignmentsaretreatedjustasintheHMMmodel,incurringaﬁxedcostfromanyposition.Thismodelcanbesimpliﬁedbyremovingallcon-ditioningonnodetypes.However,wefoundthisvarianttoslightlyunderperformthefullmodelde-scribedabove.Intuitively,typescarryinformationaboutcross-linguisticorderingpreferences.3.2TrainingApproachBecauseourmodellargelymirrorsthegenera-tiveprocessandstructureoftheoriginalHMMmodel,weapplyanearlyidenticaltrainingproce-duretoﬁttheparameterstothetrainingdataviatheExpectation-Maximizationalgorithm.OchandNey(2003)givesadetailedexpositionofthetechnique.IntheE-step,weemploytheforward-backwardalgorithmandcurrentparameterstoﬁndexpectedcountsforeachpotentialpairoflinksineachtrain-ingpair.Inthisfamiliardynamicprogrammingap-proach,wemustcomputethedistortionprobabilitiesforeachpairofEnglishpositions.Theminimalpathbetweentwoleavesinatreecanbecomputedefﬁcientlybyﬁrstﬁndingthepathfromtheroottoeachleaf,thencomparingthosepathstoﬁndthenearestcommonancestorandapaththroughit–requiringtimelinearintheheightofthetree.Computingdistortioncostsindependentlyforeachpairofwordsinthesentenceimposedacomputa-tionaloverheadofroughly50%overtheoriginalHMMmodel.Thebulkofthisincreasearisesfromthefactthatdistortionprobabilitiesinthismodelmustbecomputedforeachuniquetree,incontrast22

totheoriginalHMMwhichhasthesamedistortionprobabilitiesforallsentencesofagivenlength.IntheM-step,were-estimatetheparametersofthemodelusingtheexpectedcountscollecteddur-ingtheE-step.Allofthecomponentdistributionsofourlexicalanddistortionmodelsaremultinomi-als.Thus,uponassumingtheseexpectationsasval-uesforthehiddenalignmentvectors,wemaximizelikelihoodofthetrainingdatasimplybycomput-ingrelativefrequenciesforeachcomponentmulti-nomial.Forthedistortionmodel,anexpectedcountc(aj,aj−)isallocatedtoalltreetransitionsalongthepathρ(aj−,aj,t).Theseallocationsaresummedandnormalizedforeachtreetransitiontypetocompletere-estimation.Themethodofre-estimatingthelexi-calmodelremainsunchanged.Initializationofthelexicalmodelaffectsperfor-mancedramatically.UsingthesimplebuteffectivejointtrainingtechniqueofLiangetal.(2006),weinitializedthemodelwithlexicalparametersfromajointlytrainedimplementationofIBMModel1.3.3ImprovedPosteriorInferenceLiangetal.(2006)showsthatthresholdingthepos-teriorprobabilitiesofalignmentsimprovesAERrel-ativetocomputingViterbialignments.Thatis,wechooseathresholdτ(typicallyτ=0.5),andtakea={(i,j):p(aj=i|f,e)>τ}.Posteriorthresholdingprovidescomputationallyconvenientwaystocombinemultiplealignments,andbidirectionalcombinationoftencorrectsforerrorsinindividualdirectionalalignmentmodels.Liangetal.(2006)suggestsasoftintersectionofamodelmwithareversemodelr(foreigntoEnglish)thatthresholdstheproductoftheirposteriorsateachposition:a={(i,j):pm(aj=i|f,e)·pr(ai=j|f,e)>τ}.Theseintersectedalignmentscanbequitesparse,boostingprecisionattheexpenseofrecall.Weexploreageneralizedversiontothisapproachbyvaryingthefunctioncthatcombinespmandpr:a={(i,j):c(pm,pr)>τ}.Ifcisthemaxfunc-tion,werecoverthe(hard)unionoftheforwardandreverseposterioralignments.Ifcistheminfunc-tion,werecoverthe(hard)intersection.Anovel,highperformingalternativeisthesoftunion,whichweevaluateinthenextsection:c(pm,pr)=pm(aj=i|f,e)+pr(ai=j|f,e)2.Syntax-alignmentcompatibilitycanbefurtherpromotedwithasimpleposteriordecodingheuristicwecallcompetitivethresholding.Givenathresholdandamatrixcofcombinedweightsforeachpos-siblelinkinanalignment,weincludealink(i,j)onlyifitsweightcijisabove-thresholdanditiscon-nectedtothemaximumweightedlinkinbothrowiandcolumnj.Thatis,onlythemaximumineachcolumnandrowandacontiguousenclosingspanofabove-thresholdlinksareincludedinthealignment.3.4RelatedWorkThisproposedmodelisnottheﬁrstvariantoftheHMMmodelthatincorporatessyntax-baseddistor-tion.LopezandResnik(2005)considersasim-plertreedistancedistortionmodel.Daum´eIIIandMarcu(2005)employsasyntax-awaredistortionmodelforaligningsummariestodocuments,butconditionupontherootsoftheconstituentsthatarejumpedoverduringatransition,insteadofthosethatarevisitedduringawalkthroughthetree.Inthecaseofsyntacticmachinetranslation,wewanttocondi-tiononcrossingconstituentboundaries,evenifnoconstituentsareskippedintheprocess.4ExperimentalResultsTounderstandthebehaviorofthismodel,wecom-putedthestandardalignmenterrorrate(AER)per-formancemetric.2Wealsoinvestigatedextraction-speciﬁcmetrics:thefrequencyofinteriornodes–ameasureofhowoftenthealignmentsviolatethecon-stituentstructureofEnglishparses–andavariantoftheCPERmetricofAyanandDorr(2006).WeevaluatedtheperformanceofourmodelonbothFrench-EnglishandChinese-Englishmanuallyaligneddatasets.ForChinese,wetrainedontheFBIScorpusandtheLDCbilingualdictionary,thentestedon491hand-alignedsentencesfromthe20022Thehand-alignedtestdatahasbeenannotatedwithbothsurealignmentsSandpossiblealignmentsP,withS⊆P,ac-cordingtothespeciﬁcationsdescribedinOchandNey(2003).Withthesealignments,wecomputeAERforaproposedalign-mentAas:“1−|A∩S|+|A∩P||A|+|S|”×100%.23

FrenchPrecisionRecallAERClassicHMM93.993.06.5SyntacticHMM95.291.56.4GIZA++96.086.18.6ChinesePrecisionRecallAERClassicHMM81.678.819.8SyntacticHMM82.276.820.5GIZA++∗61.982.629.7Table1:Alignmenterrorrates(AER)for100ktrain-ingsentences.TheevaluatedalignmentsareasoftunionforFrenchandahardunionforChinese,bothusingcompetitivethresholdingdecoding.∗FromAyanandDorr(2006),grow-diag-ﬁnalheuristic.NISTMTevaluationset.ForFrench,weusedtheHansardsdatafromtheNAACL2003SharedTask.3Wetrainedon100ksentencesforeachlanguage.4.1AlignmentErrorRateWecomparedourmodeltotheoriginalHMMmodel,identicalinimplementationtooursyntac-ticHMMmodelsavethedistortioncomponent.BothmodelswereinitializedusingthesamejointlytrainedModel1parameters(5iterations),thentrainedindependentlyfor5iterations.BothmodelswerethencombinedwithanindependentlytrainedHMMmodelintheoppositedirection:f→e.4Ta-ble1summarizestheresults;thetwomodelsper-formsimilarly.Themainbeneﬁtofourmodelistheeffectonruleextraction,discussedbelow.WealsocomparedourFrenchresultstothepub-licbaselineGIZA++usingthescriptpublishedfortheNAACL2006MachineTranslationWorkshopSharedTask.5Similarly,wecomparedourChi-neseresultstotheGIZA++resultsinAyanandDorr(2006).OurmodelssubstantiallyoutperformGIZA++,conﬁrmingresultsinLiangetal.(2006).Table2showstheeffectonAERofcompetitivethresholdinganddifferentcombinationfunctions.3Followingpreviouswork,wedevelopedoursystemonthe37providedvalidationsentencesandtheﬁrst100sentencesofthecorpustestset.Weusedtheremainderasatestset.4Nullemissionprobabilitieswereﬁxedto1|e|,inverselypro-portionaltothelengthoftheEnglishsentence.Thedecodingthresholdwasheldﬁxedatτ=0.5.5Trainingincludes16iterationsofvariousIBMmodelsandaﬁxednullemissionprobabilityof.01.TheoutputofrunningGIZA++inbothdirectionswascombinedviaintersection.Frenchw/oCTwithCTHardIntersection(Min)8.48.4HardUnion(Max)12.37.7SoftIntersection(Product)6.97.1SoftUnion(Average)6.76.4Chinesew/oCTwithCTHardIntersection(Min)27.427.4HardUnion(Max)25.020.5SoftIntersection(Product)25.025.2SoftUnion(Average)21.121.6Table2:Alignmenterrorrates(AER)bydecodingmethodforthesyntacticHMMmodel.Thecompet-itivethresholdingheuristic(CT)isparticularlyhelp-fulforthehardunioncombinationmethod.Themostdramaticeffectofcompetitivethreshold-ingistoimprovealignmentqualityforhardunions.Italsoimpactsruleextractionsubstantially.4.2RuleExtractionResultsWhileitscompetitiveAERcertainlyspeakstothepotentialutilityofoursyntacticdistortionmodel,weproposedthemodelforadifferentpurpose:tomini-mizetheparticularlytroublingalignmenterrorsthatcrossconstituentboundariesandviolatethestruc-tureofEnglishparsetrees.WefoundthatwhiletheHMMandSyntacticmodelshaveverysimilarAER,theymakesubstantiallydifferenterrors.Toinvestigatethedifferences,wemeasuredthedegreetowhicheachsetofalignmentsviolatedthesuppliedparsetrees,bycountingthefrequencyofinteriornodesthatarenotnullaligned.Figure4summarizestheresultsoftheexperimentforFrench:theSyntacticdistortionwithcompetitivethreshold-ingreducestreeviolationssubstantially.Interiornodefrequencyisreducedby56%overall,withthemostdramaticimprovementobservedforclausalconstituents.Weobservedasimilar50%reductionfortheChinesedata.Additionally,weevaluatedourmodelwiththetransduceranalogtotheconsistentphraseerrorrate(CPER)metricofAyanandDorr(2006).Thisevalu-ationcomputesprecision,recall,andF1oftherulesextractedunderaproposedalignment,relativetotherulesextractedunderthegold-standardsurealign-ments.Table3showsimprovementsinF1byusing24

Reduction (percent)NP54.114.6VP46.310.3PP52.46.3S77.54.8SBAR58.01.9Non-Terminals53.141.1All56.3100.0CorpusFrequency0.05.010.015.020.025.030.0Interior Node Frequency(percent)HMM ModelSyntactic Model + CTCorpus frequency:Reduction (percent):38.947.245.354.859.743.745.114.610.36.34.81.941.1100Figure4:Thesyntacticdistortionmodelwithcom-petitivethresholdingdecreasesthefrequencyofin-teriornodesforeachtypeandthewholecorpus.thesyntacticHMMmodelandcompetitivethresh-oldingtogether.Individually,eachofthesechangescontributessubstantiallytothisincrease.Together,theirbeneﬁtsarepartially,butnotfully,additive.5ConclusionInlightoftheneedtoreconcilewordalignmentswithphrasestructuretreesforsyntacticMT,wehaveproposedanHMM-likemodelwhosedistortionissensitivetosuchtrees.Ourmodelsubstantiallyre-ducesthenumberofinteriornodesinthealignedcorpusandimprovesruleextractionwhilenearlyretainingthespeedandalignmentaccuracyoftheHMMmodel.Whileitremainstobeseenwhethertheseimprovementsimpactﬁnaltranslationaccu-racy,itisreasonabletohopethat,allelseequal,alignmentswhichbetterrespectsyntacticcorrespon-denceswillbesuperiorforsyntacticMT.ReferencesNecipFazilAyanandBonnieJ.Dorr.2006.Goingbeyondaer:Anextensiveanalysisofwordalignmentsandtheirimpactonmt.InACL.PeterF.Brown,StephenA.DellaPietra,VincentJ.DellaPietra,andRobertL.Mercer.1994.Themathematicsofstatisticalmachinetranslation:Parameterestimation.ComputationalLinguistics,19:263–311.ColinCherryandDekangLin.2006.Softsyntacticconstraintsforwordalignmentthroughdiscriminativetraining.InACL.DavidChiang.2005.Ahierarchicalphrase-basedmodelforstatisticalmachinetranslation.InACL.HalDaum´eIIIandDanielMarcu.2005.Inductionofwordandphrasealignmentsforautomaticdocumentsummarization.ComputationalLinguistics,31(4):505–530,December.FrenchPrec.RecallF1ClassicHMMBaseline40.917.624.6SyntacticHMM+CT33.922.427.0Relativechange-17%27%10%ChinesePrec.RecallF1HMMBaseline(hard)66.114.523.7HMMBaseline(soft)36.739.137.8Syntactic+CT(hard)48.041.644.6Syntactic+CT(soft)32.948.739.2Relativechange∗31%6%18%Table3:RelativetotheclassicHMMbaseline,oursyntacticdistortionmodelwithcompetitivethresh-oldingimprovesthetradeoffbetweenprecisionandrecallofextractedtransducerrules.BothFrenchalignersweredecodedusingthebest-performingsoftunioncombiner.ForChinese,weshowalignersunderbothsoftandhardunioncombiners.∗Denotesrelativechangefromthesecondlinetothethirdline.MichelGalley,MarkHopkins,KevinKnight,andDanielMarcu.2004.What’sinatranslationrule?InHLT-NAACL.MichelGalley,JonathanGraehl,KevinKnight,DanielMarcu,SteveDeNeefe,WeiWang,andIgnacioThayer.2006.Scal-ableinferenceandtrainingofcontext-richsyntactictransla-tionmodels.InACL.PercyLiang,BenTaskar,andDanKlein.2006.Alignmentbyagreement.InHLT-NAACL.A.LopezandP.Resnik.2005.Improvedhmmalignmentmod-elsforlanguageswithscarceresources.InACLWPT-05.I.DanMelamed.2004.Algorithmsforsyntax-awarestatisticalmachinetranslation.InProceedingsoftheConferenceonTheoreticalandMethodologicalIssuesinMachineTransla-tion.RobertC.Moore.2005.Adiscriminativeframeworkforbilin-gualwordalignment.InEMNLP.HermannNeyandStephanVogel.1996.Hmm-basedwordalignmentinstatisticaltranslation.InCOLING.FranzJosefOchandHermannNey.2003.Asystematiccom-parisonofvariousstatisticalalignmentmodels.Computa-tionalLinguistics,29:19–51.BenTaskar,SimonLacoste-Julien,andDanKlein.2005.Adiscriminativematchingapproachtowordalignment.InEMNLP.DekaiWu.1997.Stochasticinversiontransductiongrammarsandbilingualparsingofparallelcorpora.ComputationalLinguistics,23:377–404.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 25–32,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

25

TransductivelearningforstatisticalmachinetranslationNicolaUefﬁngNationalResearchCouncilCanadaGatineau,QC,Canadanicola.ueffing@nrc.gc.caGholamrezaHaffariandAnoopSarkarSimonFraserUniversityBurnaby,BC,Canada{ghaffar1,anoop}@cs.sfu.caAbstractStatisticalmachinetranslationsystemsareusuallytrainedonlargeamountsofbilin-gualtextandmonolingualtextinthetar-getlanguage.Inthispaperweexploretheuseoftransductivesemi-supervisedmeth-odsfortheeffectiveuseofmonolingualdatafromthesourcelanguageinordertoim-provetranslationquality.Weproposesev-eralalgorithmswiththisaim,andpresentthestrengthsandweaknessesofeachone.WepresentdetailedexperimentalevaluationsontheFrench–EnglishEuroParldatasetandondatafromtheNISTChinese–Englishlarge-datatrack.Weshowasigniﬁcantimprove-mentintranslationqualityonbothtasks.1IntroductionInstatisticalmachinetranslation(SMT),translationismodeledasadecisionprocess.Thegoalistoﬁndthetranslationtofsourcesentenceswhichmaxi-mizestheposteriorprobability:argmaxtp(t|s)=argmaxtp(s|t)·p(t)(1)Thisdecompositionoftheprobabilityyieldstwodif-ferentstatisticalmodelswhichcanbetrainedin-dependentlyofeachother:thetranslationmodelp(s|t)andthetargetlanguagemodelp(t).State-of-the-artSMTsystemsaretrainedonlargecollectionsoftextwhichconsistofbilingualcorpora(tolearntheparametersofp(s|t)),andofmonolin-gualtargetlanguagecorpora(forp(t)).Ithasbeenshownthataddinglargeamountsoftargetlanguagetextimprovestranslationqualityconsiderably.How-ever,theavailabilityofmonolingualcorporainthesourcelanguagedoesnothelpimprovethesystem’sperformance.Wewillshowhowsuchcorporacanbeusedtoachievehighertranslationquality.Eveniflargeamountsofbilingualtextaregiven,thetrainingofthestatisticalmodelsusuallysuffersfromsparsedata.Thenumberofpossibleevents,i.e.phrasepairsorpairsofsubtreesinthetwolan-guages,istoobigtoreliablyestimateaprobabil-itydistributionoversuchpairs.Anotherproblemisthatformanylanguagepairstheamountofavailablebilingualtextisverylimited.Inthiswork,wewilladdressthisproblemandproposeageneralframe-worktosolveit.Ourhypothesisisthataddinginfor-mationfromsourcelanguagetextcanalsoprovideimprovements.Unlikeaddingtargetlanguagetext,thishypothesisisanaturalsemi-supervisedlearn-ingproblem.Totacklethisproblem,weproposealgorithmsfortransductivesemi-supervisedlearn-ing.Bytransductive,wemeanthatwerepeatedlytranslatesentencesfromthedevelopmentsetortestsetandusethegeneratedtranslationstoimprovetheperformanceoftheSMTsystem.Notethattheeval-uationstepisstilldonejustonceattheendofourlearningprocess.Inthispaper,weshowthatsuchanapproachcanleadtobettertranslationsdespitethefactthatthedevelopmentandtestdataaretypi-callymuchsmallerinsizethantypicaltrainingdataforSMTsystems.TransductivelearningcanbeseenasameanstoadapttheSMTsystemtoanewtypeoftext.Sayasystemtrainedonnewswireisusedtotranslatewe-blogtexts.Theproposedmethodadaptsthetrainedmodelstothestyleanddomainofthenewinput.2BaselineMTSystemTheSMTsystemweappliedinourexperimentsisPORTAGE.Thisisastate-of-the-artphrase-basedtranslationsystemwhichhasbeenmadeavailable26

toCanadianuniversitiesforresearchandeducationpurposes.Weprovideabasicdescriptionhere;foradetaileddescriptionsee(Uefﬁngetal.,2007).Themodels(orfeatures)whichareemployedbythedecoderare:(a)oneorseveralphrasetable(s),whichmodelthetranslationdirectionp(s|t),(b)oneorseveraln-gramlanguagemodel(s)trainedwiththeSRILMtoolkit(Stolcke,2002);intheexperi-mentsreportedhere,weused4-grammodelsontheNISTdata,andatrigrammodelonEuroParl,(c)adistortionmodelwhichassignsapenaltybasedonthenumberofsourcewordswhichareskippedwhengeneratinganewtargetphrase,and(d)awordpenalty.Thesedifferentmodelsarecombinedlog-linearly.Theirweightsareoptimizedw.r.t.BLEUscoreusingthealgorithmdescribedin(Och,2003).Thisisdoneonadevelopmentcorpuswhichwewillcalldev1inthispaper.Thesearchalgorithmimple-mentedinthedecoderisadynamic-programmingbeam-searchalgorithm.Afterthemaindecodingstep,rescoringwithad-ditionalmodelsisperformed.Thesystemgeneratesa5,000-bestlistofalternativetranslationsforeachsourcesentence.Theselistsarerescoredwiththefollowingmodels:(a)thedifferentmodelsusedinthedecoderwhicharedescribedabove,(b)twodif-ferentfeaturesbasedonIBMModel1(Brownetal.,1993),(c)posteriorprobabilitiesforwords,phrases,n-grams,andsentencelength(ZensandNey,2006;UefﬁngandNey,2007),allcalculatedovertheN-bestlistandusingthesentenceprobabilitieswhichthebaselinesystemassignstothetranslationhy-potheses.TheweightsoftheseadditionalmodelsandofthedecodermodelsareagainoptimizedtomaximizeBLEUscore.Thisisperformedonasec-onddevelopmentcorpus,dev2.3TheFramework3.1TheAlgorithmOurtransductivelearningalgorithm,Algorithm1,isinspiredbytheYarowskyalgorithm(Yarowsky,1995;Abney,2004).Thealgorithmworksasfol-lows:First,thetranslationmodelisestimatedbasedonthesentencepairsinthebilingualtrainingdataL.Then,asetofsourcelanguagesentences,U,istrans-latedbasedonthecurrentmodel.Asubsetofgoodtranslationsandtheirsources,Ti,isselectedineachiterationandaddedtothetrainingdata.Thesese-lectedsentencepairsarereplacedineachiteration,andonlytheoriginalbilingualtrainingdata,L,iskeptﬁxedthroughoutthealgorithm.Theprocessofgeneratingsentencepairs,selectingasubsetofgoodsentencepairs,andupdatingthemodeliscon-tinueduntilastoppingconditionismet.NotethatwerunthisalgorithminatransductivesettingwhichmeansthatthesetofsentencesUisdrawneitherfromadevelopmentsetorthetestsetthatwillbeusedeventuallytoevaluatetheSMTsystemorfromadditionaldatawhichisrelevanttothedevelopmentortestset.InAlgorithm1,changingthedeﬁnitionofEstimate,ScoreandSelectwillgiveusthedif-ferentsemi-supervisedlearningalgorithmswewilldiscussinthispaper.Giventheprobabilitymodelp(t|s),considerthedistributionoverallpossiblevalidtranslationstforaparticularinputsentences.Wecaninitializethisprobabilitydistributiontotheuniformdistribu-tionforeachsentencesintheunlabeleddataU.Thus,thisdistributionovertranslationsofsentencesfromUwillhavethemaximumentropy.Undercertainpreciseconditions,asdescribedin(Abney,2004),wecananalyzeAlgorithm1asminimizingtheentropyofthedistributionovertranslationsofU.However,thisistrueonlywhenthefunctionsEsti-mate,ScoreandSelecthaveveryprescribeddeﬁni-tions.Inthispaper,ratherthananalyzetheconver-genceofAlgorithm1werunitforaﬁxednumberofiterationsandinsteadfocusonﬁndingusefuldef-initionsforEstimate,ScoreandSelectthatcanbeexperimentallyshowntoimproveMTperformance.3.2TheEstimateFunctionWeconsiderthefollowingdifferentdeﬁnitionsforEstimateinAlgorithm1:FullRe-training(ofalltranslationmodels):IfEstimate(L,T)estimatesthemodelparametersbasedonL∪T,thenwehaveasemi-supervisedal-gorithmthatre-trainsamodelontheoriginaltrain-ingdataLplusthesentencesdecodedinthelastit-eration.ThesizeofLcanbecontrolledbyﬁlteringthetrainingdata(seeSection3.5).AdditionalPhraseTable:If,ontheotherhand,anewphrasetranslationtableislearnedonTonlyandthenaddedasanewcomponentinthelog-linearmodel,wehaveanalternativetothefullre-training27

Algorithm1Transductivelearningalgorithmforstatisticalmachinetranslation1:Input:trainingsetLofparallelsentencepairs.//Bilingualtrainingdata.2:Input:unlabeledsetUofsourcetext.//Monolingualsourcelanguagedata.3:Input:numberofiterationsR,andsizeofn-bestlistN.4:T−1:={}.//Additionalbilingualtrainingdata.5:i:=0.//Iterationcounter.6:repeat7:Trainingstep:π(i):=Estimate(L,Ti−1).8:Xi:={}.//Thesetofgeneratedtranslationsforthisiteration.9:forsentences∈Udo10:Labelingstep:Decodesusingπ(i)toobtainNbestsentencepairswiththeirscores11:Xi:=Xi∪{(tn,s,π(i)(tn|s))Nn=1}12:endfor13:Scoringstep:Si:=Score(Xi)//Assignascoretosentencepairs(t,s)fromX.14:Selectionstep:Ti:=Select(Xi,Si)//Chooseasubsetofgoodsentencepairs(t,s)fromX.15:i:=i+1.16:untili>RofthemodelonlabeledandunlabeleddatawhichcanbeveryexpensiveifLisverylarge(asontheChinese–Englishdataset).Thisadditionalphrasetableissmallandspeciﬁctothedevelopmentortestsetitistrainedon.Itoverlapswiththeorigi-nalphrasetables,butalsocontainsmanynewphrasepairs(Uefﬁng,2006).MixtureModel:AnotheralternativeforEstimateistocreateamixturemodelofthephrasetableprob-abilitieswithnewphrasetableprobabilitiesp(s|t)=λ·Lp(s|t)+(1−λ)·Tp(s|t)(2)whereLpandTparephrasetableprobabilitiesesti-matedonLandT,respectively.IncaseswherenewphrasepairsarelearnedfromT,theygetaddedintothemergedphrasetable.3.3TheScoringFunctionInAlgorithm1,theScorefunctionassignsascoretoeachtranslationhypothesist.Weusedthefollowingscoringfunctionsinourexperiments:Length-normalizedScore:Eachtranslatedsen-tencepair(t,s)isscoredaccordingtothemodelprobabilityp(t|s)normalizedbythelength|t|ofthetargetsentence:Score(t,s)=p(t|s)1|t|(3)ConﬁdenceEstimation:Theconﬁdenceestimationwhichweimplementedfollowstheapproachessug-gestedin(Blatzetal.,2003;UefﬁngandNey,2007):Theconﬁdencescoreofatargetsentencetiscal-culatedasalog-linearcombinationofphrasepos-teriorprobabilities,Levenshtein-basedwordposte-riorprobabilities,andatargetlanguagemodelscore.Theweightsofthedifferentscoresareoptimizedw.r.t.classiﬁcationerrorrate(CER).Thephraseposteriorprobabilitiesaredeterminedbysummingthesentenceprobabilitiesofalltrans-lationhypothesesintheN-bestlistwhichcontainthisphrasepair.Thesegmentationofthesentenceintophrasesisprovidedbythedecoder.ThissumisthennormalizedbythetotalprobabilitymassoftheN-bestlist.Toobtainascoreforthewholetar-getsentence,theposteriorprobabilitiesofalltargetphrasesaremultiplied.Thewordposteriorproba-bilitiesarecalculatedonbasisoftheLevenshteinalignmentbetweenthehypothesisunderconsider-ationandallothertranslationscontainedintheN-bestlist.Fordetails,see(UefﬁngandNey,2007).Again,thesinglevaluesaremultipliedtoobtainascoreforthewholesentence.ForNIST,thelan-guagemodelscoreisdeterminedusinga5-grammodeltrainedontheEnglishGigawordcorpus,andonFrench–English,weusethetrigrammodelwhichwasprovidedfortheNAACL2006sharedtask.3.4TheSelectionFunctionTheSelectfunctioninAlgorithm1isusedtocreatetheadditionaltrainingdataTiwhichwillbeusedin28

thenextiterationi+1byEstimatetoaugmenttheoriginalbilingualtrainingdata.Weusethefollow-ingselectionfunctions:ImportanceSampling:ForeachsentencesinthesetofunlabeledsentencesU,theLabelingstepinAlgorithm1generatesanN-bestlistoftranslations,andthesubsequentScoringstepassignsascoreforeachtranslationtinthislist.ThesetofgeneratedtranslationsforallsentencesinUistheeventspaceandthescoresareusedtoputaprobabilitydistri-butionoverthisspace,simplybyrenormalizingthescoresdescribedinSection3.3.WeuseimportancesamplingtoselectKtranslationsfromthisdistri-bution.Samplingisdonewithreplacementwhichmeansthatthesametranslationmaybechosensev-eraltimes.TheseKsampledtranslationsandtheirassociatedsourcesentencesmakeuptheadditionaltrainingdataTi.SelectionusingaThreshold:Thismethodcom-paresthescoreofeachsingle-besttranslationtoathreshold.ThetranslationisconsideredreliableandaddedtothesetTiifitsscoreexceedsthethresh-old.Elseitisdiscardedandnotusedintheaddi-tionaltrainingdata.Thethresholdisoptimizedonthedevelopmentbeforehand.Sincethescoresofthetranslationschangeineachiteration,thesizeofTialsochanges.KeepAll:Thismethoddoesnotperformanyﬁl-teringatall.Itissimplyassumedthatalltransla-tionsinthesetXiarereliable,andnoneofthemarediscarded.Thus,ineachiteration,theresultoftheselectionstepwillbeTi=Xi.Thismethodwasimplementedmainlyforcomparisonwithotherse-lectionmethods.3.5FilteringtheTrainingDataIngeneral,havingmoretrainingdataimprovesthequalityofthetrainedmodels.However,whenitcomestothetranslationofaparticulartestset,thequestioniswhetheralloftheavailabletrainingdataarerelevanttothetranslationtaskornot.Moreover,workingwithlargeamountsoftrainingdatarequiresmorecomputationalpower.Soifwecanidentifyasubsetoftrainingdatawhicharerelevanttothecur-renttaskanduseonlythistore-trainthemodels,wecanreducecomputationalcomplexitysigniﬁcantly.WeproposetoFilterthetrainingdata,eitherbilingualormonolingualtext,toidentifythepartscorpususesentencesEuroParlphrasetable+LM688Ktrain100kphrasetable100Ktrain150kphrasetable150Kdev06dev12,000test06test3,064Table1:French–Englishcorporacorpususesentencesnon-UNphrasetable+LM3.2MUNphrasetable+LM5.0MEnglishGigawordLM11.7Mmulti-p3dev1935multi-p4dev2919eval-04test1,788eval-06test3,940Table2:NISTChinese–Englishcorporawhicharerelevantw.r.t.thetestset.Thisﬁlteringisbasedonn-gramcoverage.Forasourcesentencesinthetrainingdata,itsn-gramcoverageoverthesentencesinthetestsetiscomputed.Theaverageoverseveraln-gramlengthsisusedasameasureofrelevanceofthistrainingsentencew.r.t.thetestcorpus.Basedonthis,weselectthetopKsourcesentencesorsentencepairs.4ExperimentalResults4.1SettingWeranexperimentsontwodifferentcorpora:oneistheFrench–EnglishtranslationtaskfromtheEu-roParlcorpus,andtheotheroneisChinese–EnglishtranslationasperformedintheNISTMTevaluation(www.nist.gov/speech/tests/mt).FortheFrench–Englishtranslationtask,weusedtheEuroParlcorpusasdistributedforthesharedtaskintheNAACL2006workshoponstatisticalma-chinetranslation.ThecorpusstatisticsareshowninTable1.FurthermoreweﬁlteredtheEuroParlcorpus,asexplainedinSection3.5,tocreatetwosmallerbilingualcorpora(train100kandtrain150kinTable1).Thedevelopmentsetisusedtooptimizethemodelweightsinthedecoder,andtheevaluationisdoneonthetestsetprovidedfortheNAACL2006sharedtask.FortheChinese–Englishtranslationtask,weusedthecorporadistributedforthelarge-datatrackinthe29

settingEuroParlNISTfullre-trainingw/ﬁltering∗∗∗fullre-training∗∗†mixturemodel∗†newphrasetableff:keepall∗∗∗imp.samplingnorm.∗∗∗conf.∗∗∗thresholdnorm.∗∗∗conf.∗∗∗Table3:FeasibilityofsettingsforAlgorithm12006NISTevaluation(seeTable2).WeusedtheLDCsegmenterforChinese.Themultipletransla-tioncorporamulti-p3andmulti-p4wereusedasde-velopmentcorpora.Evaluationwasperformedonthe2004and2006testsets.Notethatthetrain-ingdataconsistsmainlyofwrittentext,whereasthetestsetscomprisethreeandfourdifferentgenres:editorials,newswireandpoliticalspeechesinthe2004testset,andbroadcastconversations,broad-castnews,newsgroupsandnewswireinthe2006testset.Mostofthesedomainshavecharacteristicswhicharedifferentfromthoseofthetrainingdata,e.g.,broadcastconversationshavecharacteristicsofspontaneousspeech,andthenewsgroupdataiscom-parativelyunstructured.Giventheparticulardatasetsdescribedabove,Ta-ble3showsthevariousoptionsfortheEstimate,ScoreandSelectfunctions(seeSection3).Theta-bleprovidesaquickguidetotheexperimentswepresentinthispapervs.thosewedidnotattemptduetocomputationalinfeasibility.Weranexperimentscorrespondingtoallentriesmarkedwith∗(seeSec-tion4.2).Forthosemarked∗∗theexperimentspro-ducedonlyminimalimprovementoverthebaselineandsowedonotdiscusstheminthispaper.Theen-triesmarkedas†werenotattemptedbecausetheyarenotfeasible(e.g.fullre-trainingontheNISTdata).However,thesewererunonthesmallerEu-roParlcorpus.EvaluationMetricsWeevaluatedthegeneratedtranslationsusingthreedifferentevaluationmetrics:BLEUscore(Pa-pinenietal.,2002),mWER(multi-referenceworderrorrate),andmPER(multi-referenceposition-independentworderrorrate)(Nießenetal.,2000).NotethatBLEUscoremeasuresquality,whereasmWERandmPERmeasuretranslationerrors.Wewillpresent95%-conﬁdenceintervalsforthebase-linesystemwhicharecalculatedusingbootstrapre-sampling.Themetricsarecalculatedw.r.t.oneandfourEnglishreferences:theEuroParldatacomeswithonereference,theNIST2004evaluationsetandtheNISTsectionofthe2006evaluationsetareprovidedwithfourreferenceseach,whereastheGALEsectionofthe2006evaluationsetcomeswithonereferenceonly.ThisresultsinmuchlowerBLEUscoresandhighererrorratesforthetransla-tionsoftheGALEset(seeSection4.2).Notethatthesevaluesdonotindicatelowertranslationqual-ity,butaresimplyaresultofusingonlyonerefer-ence.4.2ResultsEuroParlWeranourinitialexperimentsonEuroParltoex-plorethebehaviorofthetransductivelearningalgo-rithm.Inallexperimentsreportedinthissubsec-tion,thetestsetwasusedasunlabeleddata.Theselectionandscoringwascarriedoutusingimpor-tancesamplingwithnormalizedscores.Inonesetofexperiments,weusedthe100Kand150Ktrain-ingsentencesﬁlteredaccordington-gramcoverageoverthetestset.Wefullyre-trainedthephraseta-blesonthesedataand8,000testsentencepairssam-pledfrom20-bestlistsineachiteration.TheresultsonthetestsetcanbeseeninFigure1.TheBLEUscoreincreases,althoughwithslightvariation,overtheiterations.Intotal,itincreasesfrom24.1to24.4forthe100Kﬁlteredcorpus,andfrom24.5to24.8for150K,respectively.Moreover,weseethattheBLEUscoreofthesystemusing100Ktrainingsen-tencepairsandtransductivelearningisthesameasthatoftheonetrainedon150Ksentencepairs.Sotheinformationextractedfromuntranslatedtestsen-tencesisequivalenttohavinganadditional50Ksen-tencepairs.Inasecondsetofexperiments,weusedthewholeEuroParlcorpusandthesampledsentencesforfullyre-trainingthephrasetablesineachiteration.WeranthealgorithmforthreeiterationsandtheBLEUscoreincreasedfrom25.3to25.6.Eventhoughthis30

02468101214161824.0524.124.1524.224.2524.324.3524.424.45IterationBleu score024681012141624.4524.524.5524.624.6524.724.7524.824.85IterationBleu scoreFigure1:Translationqualityforimportancesamplingwithfullre-trainingontrain100k(left)andtrain150k(right).EuroParlFrench–Englishtask.isasmallincrease,itshowsthattheunlabeleddatacontainssomeinformationwhichcanbeexploredintransductivelearning.Inathirdexperiment,weappliedthemixturemodelideaasexplainedinSection3.2.Theinitiallylearnedphrasetablewasmergedwiththelearnedphrasetableineachiterationwithaweightofλ=0.1.Thisvalueforλwasfoundbasedoncrossval-idationonadevelopmentset.Weranthealgorithmfor20iterationsandBLEUscoreincreasedfrom25.3to25.7.Sincethisisverysimilartothere-sultobtainedwiththepreviousmethod,butwithanadditionalparameterλtooptimize,wedidnotusemixturemodelsonNIST.Notethatthesingleimprovementsachievedhereareslightlybelowthe95%-signiﬁcancelevel.How-ever,weobservethemconsistentlyinallsettings.NISTTable4presentstranslationresultsonNISTwithdifferentversionsofthescoringandselectionmeth-odsintroducedinSection3.Intheseexperiments,theunlabeleddataUforAlgorithm1isthedevelop-mentortestcorpus.ForthiscorpusU,5,000-bestlistsweregeneratedusingthebaselineSMTsystem.Sincere-trainingthefullphrasetablesisnotfeasi-blehere,a(small)additionalphrasetable,speciﬁctoU,wastrainedandpluggedintotheSMTsystemasanadditionalmodel.Thedecoderweightsthushadtobeoptimizedagaintodeterminetheappropriateweightforthisnewphrasetable.Thiswasdoneonthedev1corpus,usingthephrasetablespeciﬁctodev1.Everytimeanewcorpusistobetranslated,anadaptedphrasetableiscreatedusingtransductivelearningandusedwiththeweightwhichhasbeenlearnedondev1.IntheﬁrstexperimentpresentedinTable4,allofthegenerated1-besttranslationswerekeptandusedfortrainingtheadaptedphrasetables.Thismethodyieldsslightlyhighertransla-tionqualitythanthebaselinesystem.Thesecondapproachwestudiedistheuseofimportancesam-pling(IS)over20-bestlists,basedeitheronlength-normalizedsentencescores(norm.)orconﬁdencescores(conf.).AstheresultsinTable4show,bothvariantsoutperformtheﬁrstmethod,withaconsis-tentimprovementoverthebaselineacrossalltestcorporaandevaluationmetrics.Thethirdmethodusesathreshold-basedselectionmethod.Combinedwithconﬁdenceestimationasscoringmethod,thisyieldsthebestresults.Allimprovementsoverthebaselinearesigniﬁcantatthe95%-level.Table5showsthetranslationqualityachievedontheNISTtestsetswhenadditionalsourcelanguagedatafromtheChineseGigawordcorpuscompris-ingnewswiretextisusedfortransductivelearning.TheseChinesesentencesweresortedaccordingtotheirn-gramoverlap(seeSection3.5)withthede-velopmentcorpus,andthetop5,000Chinesesen-tenceswereused.TheselectionandscoringinAl-gorithm1wereperformedusingconﬁdenceestima-tionwithathreshold.Again,anewphrasetablewastrainedonthesedata.AscanbeseeninTable5,this31

selectscoreBLEU[%]mWER[%]mPER[%]eval-04(4refs.)baseline31.8±0.766.8±0.741.5±0.5keepall33.166.041.3ISnorm.33.565.840.9conf.33.265.640.4thrnorm.33.565.940.8conf.33.565.340.8eval-06GALE(1ref.)baseline12.7±0.575.8±0.654.6±0.6keepall12.975.755.0ISnorm.13.274.754.1conf.12.974.453.5thrnorm.12.775.254.2conf.13.673.453.2eval-06NIST(4refs.)baseline27.9±0.767.2±0.644.0±0.5keepall28.166.544.2ISnorm.28.766.143.6conf.28.465.843.2thrnorm.28.366.143.5conf.29.365.643.2Table4:Translationqualityusinganadditionaladaptedphrasetabletrainedonthedev/testsets.Differentselectionandscoringmethods.NISTChinese–English,bestresultsprintedinboldface.systemoutperformsthebaselinesystemonalltestcorpora.Theerrorratesaresigniﬁcantlyreducedinallthreesettings,andBLEUscoreincreasesinallcases.AcomparisonwithTable4showsthattrans-ductivelearningonthedevelopmentsetandtestcor-pora,adaptingthesystemtotheirdomainandstyle,ismoreeffectiveinimprovingtheSMTsystemthantheuseofadditionalsourcelanguagedata.InallexperimentsonNIST,Algorithm1wasrunforoneiteration.Wealsoinvestigatedtheuseofaniterativeprocedurehere,butthisdidnotyieldanyimprovementintranslationquality.5PreviousWorkSemi-supervisedlearninghasbeenpreviouslyap-pliedtoimprovewordalignments.In(Callison-Burchetal.,2004),agenerativemodelforwordalignmentistrainedusingunsupervisedlearningonparalleltext.Inaddition,anothermodelistrainedonasmallamountofhand-annotatedwordalignmentdata.AmixturemodelprovidesaprobabilityforsystemBLEU[%]mWER[%]mPER[%]eval-04(4refs.)baseline31.8±0.766.8±0.741.5±0.5addChin.data32.865.740.9eval-06GALE(1ref.)baseline12.7±0.575.8±0.654.6±0.6addChin.data13.173.953.5eval-06NIST(4refs.)baseline27.9±0.767.2±0.644.0±0.5addChin.data28.165.843.2Table5:TranslationqualityusinganadditionalphrasetabletrainedonmonolingualChinesenewsdata.Selectionstepusingthresholdonconﬁdencescores.NISTChinese–English.wordalignment.Experimentsshowedthatputtingalargeweightonthemodeltrainedonlabeleddataperformsbest.Alongsimilarlines,(FraserandMarcu,2006)combineagenerativemodelofwordalignmentwithalog-lineardiscriminativemodeltrainedonasmallsetofhandalignedsentences.Thewordalignmentsareusedtotrainastandardphrase-basedSMTsystem,resultinginincreasedtranslationquality.In(Callison-Burch,2002)co-trainingisappliedtoMT.Thisapproachrequiresseveralsourcelan-guageswhicharesentence-alignedwitheachotherandalltranslateintothesametargetlanguage.Onelanguagepaircreatesdataforanotherlanguagepairandcanbenaturallyusedina(BlumandMitchell,1998)-styleco-trainingalgorithm.ExperimentsontheEuroParlcorpusshowadecreaseinWER.How-ever,theselectionalgorithmappliedthereisactuallysupervisedbecauseittakesthereferencetranslationintoaccount.Moreover,whenthealgorithmisrunlongenough,largeamountsofco-traineddatain-jectedtoomuchnoiseandperformancedegraded.Self-trainingforSMTwasproposedin(Uefﬁng,2006).AnexistingSMTsystemisusedtotranslatethedevelopmentortestcorpus.Amongthegener-atedmachinetranslations,thereliableonesareau-tomaticallyidentiﬁedusingthresholdingonconﬁ-dencescores.Theworkwhichwepresentedherediffersfrom(Uefﬁng,2006)asfollows:•Weinvestigateddifferentwaysofscoringandselectingthereliabletranslationsandcomparedourmethodtothiswork.Inadditiontothecon-32

ﬁdenceestimationusedthere,weappliedim-portancesamplingandcombineditwithconﬁ-denceestimationfortransductivelearning.•Westudiedadditionalwaysofexploringthenewlycreatedbilingualdata,namelyre-trainingthefullphrasetranslationmodelorcre-atingamixturemodel.•Weproposedaniterativeprocedurewhichtranslatesthemonolingualsourcelanguagedataanewineachiterationandthenre-trainsthephrasetranslationmodel.•Weshowedhowadditionalmonolingualsource-languagedatacanbeusedintransduc-tivelearningtoimprovetheSMTsystem.6DiscussionItisnotintuitivelyclearwhytheSMTsystemcanlearnsomethingfromitsownoutputandisimprovedthroughsemi-supervisedlearning.Therearetwomainreasonsforthisimprovement:Firstly,these-lectionstepprovidesimportantfeedbackforthesys-tem.Theconﬁdenceestimation,forexample,dis-cardstranslationswithlowlanguagemodelscoresorposteriorprobabilities.Theselectionstepdiscardsbadmachinetranslationsandreinforcesphrasesofhighquality.Asaresult,theprobabilitiesoflow-qualityphrasepairs,suchasnoiseinthetableoroverlyconﬁdentsingletons,degrade.Ourexperi-mentscomparingthevarioussettingsfortransduc-tivelearningshowsthatselectionclearlyoutper-formsthemethodwhichkeepsallgeneratedtransla-tionsasadditionaltrainingdata.Theselectionmeth-odsinvestigatedherehavebeenshowntobewell-suitedtoboosttheperformanceofsemi-supervisedlearningforSMT.Secondly,ouralgorithmconstitutesawayofadaptingtheSMTsystemtoanewdomainorstylewithoutrequiringbilingualtrainingordevelopmentdata.Thosephrasesintheexistingphrasetableswhicharerelevantfortranslatingthenewdataarereinforced.Theprobabilitydistributionoverthephrasepairsthusgetsmorefocusedonthe(reliable)partswhicharerelevantforthetestdata.Forananal-ysisoftheself-trainedphrasetables,examplesoftranslatedsentences,andthephrasesusedintrans-lation,see(Uefﬁng,2006).ReferencesS.Abney.2004.UnderstandingtheYarowskyAlgo-rithm.Comput.Ling.,30(3).J.Blatz,E.Fitzgerald,G.Foster,S.Gandrabur,C.Goutte,A.Kulesza,A.Sanchis,andN.Uefﬁng.2003.Conﬁdenceestimationformachinetransla-tion.Finalreport,JHU/CLSPSummerWorkshop.www.clsp.jhu.edu/ws2003/groups/estimate/.A.BlumandT.Mitchell.1998.CombiningLabeledandUnlabeledDatawithCo-Training.InProc.Computa-tionalLearningTheory.P.F.Brown,S.A.DellaPietra,V.J.DellaPietra,andR.L.Mercer.1993.TheMathematicsofStatisticalMachineTranslation:ParameterEstimation.Compu-tationalLinguistics,19(2).C.Callison-Burch,D.Talbot,andM.Osborne.2004.Statisticalmachinetranslationwithword-andsentence-alignedparallelcorpora.InProc.ACL.C.Callison-Burch.2002.Co-trainingforstatisticalma-chinetranslation.Master’sthesis,SchoolofInformat-ics,UniversityofEdinburgh.A.FraserandD.Marcu.2006.Semi-supervisedtrainingforstatisticalwordalignment.InProc.ACL.S.Nießen,F.J.Och,G.Leusch,andH.Ney.2000.Anevaluationtoolformachinetranslation:Fastevalua-tionforMTresearch.InProc.LREC.F.J.Och.2003.Minimumerrorratetraininginstatisticalmachinetranslation.InProc.ACL.K.Papineni,S.Roukos,T.Ward,andW.-J.Zhu.2002.BLEU:amethodforautomaticevaluationofmachinetranslation.InProc.ACL.A.Stolcke.2002.SRILM-anextensiblelanguagemod-elingtoolkit.InProc.ICSLP.N.UefﬁngandH.Ney.2007.Word-levelconﬁdencees-timationformachinetranslation.ComputationalLin-guistics,33(1):9–40.N.Uefﬁng,M.Simard,S.Larkin,andJ.H.Johnson.2007.NRC’sPortagesystemforWMT2007.InProc.ACLWorkshoponSMT.N.Uefﬁng.2006.Usingmonolingualsource-languagedatatoimproveMTperformance.InProc.IWSLT.D.Yarowsky.1995.UnsupervisedWordSenseDisambiguationRivalingSupervisedMethods.InProc.ACL.R.ZensandH.Ney.2006.N-gramposteriorprobabilitiesforstatisticalmachinetranslation.InProc.HLT/NAACLWorkshoponSMT.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 33–40,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

33

WordSenseDisambiguationImprovesStatisticalMachineTranslationYeeSengChanandHweeTouNgDepartmentofComputerScienceNationalUniversityofSingapore3ScienceDrive2Singapore117543{chanys,nght}@comp.nus.edu.sgDavidChiangInformationSciencesInstituteUniversityofSouthernCalifornia4676AdmiraltyWay,Suite1001MarinadelRey,CA90292,USAchiang@isi.eduAbstractRecentresearchpresentsconﬂictingevi-denceonwhetherwordsensedisambigua-tion(WSD)systemscanhelptoimprovetheperformanceofstatisticalmachinetransla-tion(MT)systems.Inthispaper,wesuc-cessfullyintegrateastate-of-the-artWSDsystemintoastate-of-the-arthierarchicalphrase-basedMTsystem,Hiero.WeshowfortheﬁrsttimethatintegratingaWSDsys-temimprovestheperformanceofastate-of-the-artstatisticalMTsystemonanactualtranslationtask.Furthermore,theimprove-mentisstatisticallysigniﬁcant.1IntroductionManywordshavemultiplemeanings,dependingonthecontextinwhichtheyareused.Wordsensedis-ambiguation(WSD)isthetaskofdeterminingthecorrectmeaningorsenseofawordincontext.WSDisregardedasanimportantresearchproblemandisassumedtobehelpfulforapplicationssuchasma-chinetranslation(MT)andinformationretrieval.Intranslation,differentsensesofawordwinasourcelanguagemayhavedifferenttranslationsinatargetlanguage,dependingontheparticularmean-ingofwincontext.Hence,theassumptionisthatinresolvingsenseambiguity,aWSDsystemwillbeabletohelpanMTsystemtodeterminethecorrecttranslationforanambiguousword.Todeterminethecorrectsenseofaword,WSDsystemstypicallyuseawidearrayoffeaturesthatarenotlimitedtothelo-calcontextofw,andsomeofthesefeaturesmaynotbeusedbystate-of-the-artstatisticalMTsystems.Toperformtranslation,state-of-the-artMTsys-temsuseastatisticalphrase-basedapproach(MarcuandWong,2002;Koehnetal.,2003;OchandNey,2004)bytreatingphrasesasthebasicunitsoftranslation.Inthisapproach,aphrasecanbeanysequenceofconsecutivewordsandisnotnec-essarilylinguisticallymeaningful.Capitalizingonthestrengthofthephrase-basedapproach,Chiang(2005)introducedahierarchicalphrase-basedsta-tisticalMTsystem,Hiero,whichachievessigniﬁ-cantlybettertranslationperformancethanPharaoh(Koehn,2004a),whichisastate-of-the-artphrase-basedstatisticalMTsystem.Recently,someresearchersinvestigatedwhetherperformingWSDwillhelptoimprovetheperfor-manceofanMTsystem.CarpuatandWu(2005)integratedthetranslationpredictionsfromaChineseWSDsystem(Carpuatetal.,2004)intoaChinese-Englishword-basedstatisticalMTsystemusingtheISIReWritedecoder(Germann,2003).ThoughtheyacknowledgedthatdirectlyusingEnglishtransla-tionsaswordsenseswouldbeideal,theyinsteadpredictedtheHowNetsenseofawordandthenusedtheEnglishglossoftheHowNetsenseastheWSDmodel’spredictedtranslation.Theydidnotincor-poratetheirWSDmodeloritspredictionsintotheirtranslationmodel;rather,theyusedtheWSDpre-dictionseithertoconstraintheoptionsavailabletotheirdecoder,ortopostedittheoutputoftheirde-coder.TheyreportedthenegativeresultthatWSDdecreasedtheperformanceofMTbasedontheirex-periments.Inanotherwork(Vickreyetal.,2005),theWSDproblemwasrecastasawordtranslationtask.The34

translationchoicesforawordwweredeﬁnedasthesetofwordsorphrasesalignedtow,asgatheredfromaword-alignedparallelcorpus.Theauthorsshowedthattheywereabletoimprovetheirmodel’saccuracyontwosimpliﬁedtranslationtasks:wordtranslationandblank-ﬁlling.Recently,CabezasandResnik(2005)experi-mentedwithincorporatingWSDtranslationsintoPharaoh,astate-of-the-artphrase-basedMTsys-tem(Koehnetal.,2003).TheirWSDsystempro-videdadditionaltranslationstothephrasetableofPharaoh,whichﬁredanewmodelfeature,sothatthedecodercouldweightheadditionalalternativetranslationsagainstitsown.However,theycouldnotautomaticallytunetheweightofthisfeatureinthesamewayastheothers.Theyobtainedarela-tivelysmallimprovement,andnostatisticalsigniﬁ-cancetestwasreportedtodetermineiftheimprove-mentwasstatisticallysigniﬁcant.Notethattheexperimentsin(CarpuatandWu,2005)didnotuseastate-of-the-artMTsystem,whiletheexperimentsin(Vickreyetal.,2005)werenotdoneusingafull-ﬂedgedMTsystemandtheevaluationwasnotonhowwelleachsourcesentencewastranslatedasawhole.Therelativelysmallim-provementreportedbyCabezasandResnik(2005)withoutastatisticalsigniﬁcancetestappearstobeinconclusive.Consideringtheconﬂictingresultsre-portedbypriorwork,itisnotclearwhetheraWSDsystemcanhelptoimprovetheperformanceofastate-of-the-artstatisticalMTsystem.Inthispaper,wesuccessfullyintegrateastate-of-the-artWSDsystemintothestate-of-the-arthi-erarchicalphrase-basedMTsystem,Hiero(Chiang,2005).Theintegrationisaccomplishedbyintroduc-ingtwoadditionalfeaturesintotheMTmodelwhichoperateontheexistingrulesofthegrammar,with-outintroducingcompetingrules.Thesefeaturesaretreated,bothinfeature-weighttuningandindecod-ing,onthesamefootingastherestofthemodel,allowingittoweightheWSDmodelpredictionsagainstotherpiecesofevidencesoastooptimizetranslationaccuracy(asmeasuredbyBLEU).ThecontributionofourworkliesinshowingfortheﬁrsttimethatintegratingaWSDsystemsigniﬁcantlyim-provestheperformanceofastate-of-the-artstatisti-calMTsystemonanactualtranslationtask.Inthenextsection,wedescribeourWSDsystem.Then,inSection3,wedescribetheHieroMTsys-temandintroducethetwonewfeaturesusedtointe-gratetheWSDsystemintoHiero.InSection4,wedescribethetrainingdatausedbytheWSDsystem.InSection5,wedescribehowtheWSDtranslationsprovidedareusedbythedecoderoftheMTsystem.InSection6and7,wepresentandanalyzeourex-perimentalresults,beforeconcludinginSection8.2WordSenseDisambiguationPriorresearchhasshownthatusingSupportVectorMachines(SVM)asthelearningalgorithmforWSDachievesgoodresults(LeeandNg,2002).Forourexperiments,weusetheSVMimplementationof(ChangandLin,2001)asitisabletoworkonmulti-classproblemstooutputtheclassiﬁcationprobabil-ityforeachclass.OurimplementedWSDclassiﬁerusestheknowl-edgesourcesoflocalcollocations,parts-of-speech(POS),andsurroundingwords,followingthesuc-cessfulapproachof(LeeandNg,2002).Forlocalcollocations,weuse3features,w−1w+1,w−1,andw+1,wherew−1(w+1)isthetokenimmediatelytotheleft(right)ofthecurrentambiguouswordoc-currencew.Forparts-of-speech,weuse3features,P−1,P0,andP+1,whereP0isthePOSofw,andP−1(P+1)isthePOSofw−1(w+1).Forsurround-ingwords,weconsiderallunigrams(singlewords)inthesurroundingcontextofw.Theseunigramscanbeinadifferentsentencefromw.Weperformfea-tureselectiononsurroundingwordsbyincludingaunigramonlyifitoccurs3ormoretimesinsomesenseofwinthetrainingdata.TomeasuretheaccuracyofourWSDclassiﬁer,weevaluateitonthetestdataofSENSEVAL-3Chi-neselexical-sampletask.Weobtainaccuracythatcomparesfavorablytothebestparticipatingsysteminthetask(Carpuatetal.,2004).3HieroHiero(Chiang,2005)isahierarchicalphrase-basedmodelforstatisticalmachinetranslation,basedonweightedsynchronouscontext-freegrammar(CFG)(LewisandStearns,1968).AsynchronousCFGconsistsofrewriterulessuchasthefollowing:X→hγ,αi(1)35

whereXisanon-terminalsymbol,γ(α)isastringofterminalandnon-terminalsymbolsinthesource(target)language,andthereisaone-to-onecorre-spondencebetweenthenon-terminalsinγandαin-dicatedbyco-indexation.Hence,γandαalwayshavethesamenumberofnon-terminalsymbols.Forinstance,wecouldhavethefollowinggrammarrule:X→h຅ഢտX1,gotoX1everymonthtoi(2)whereboxedindicesrepresentthecorrespondencesbetweennon-terminalsymbols.HieroextractsthesynchronousCFGrulesauto-maticallyfromaword-alignedparallelcorpus.Totranslateasourcesentence,thegoalistoﬁnditsmostprobablederivationusingtheextractedgram-marrules.Hierousesagenerallog-linearmodel(OchandNey,2002)wheretheweightofaderiva-tionDforaparticularsourcesentenceanditstrans-lationisw(D)=Yiφi(D)λi(3)whereφiisafeaturefunctionandλiistheweightforfeatureφi.Toensureefﬁcientdecoding,theφiaresubjecttocertainlocalityrestrictions.Essentially,theyshouldbedeﬁnedasproductsoffunctionsde-ﬁnedonisolatedsynchronousCGFrules;however,itispossibletoextendthedomainoflocalityofthefeaturessomewhat.An-gramlanguagemodeladdsadependenceon(n−1)neighboringtarget-sidewords(Wu,1996;Chiang,2007),makingdecodingmuchmoredifﬁcultbutstillpolynomial;inthispa-per,weaddfeaturesthatdependontheneighboringsource-sidewords,whichdoesnotaffectdecodingcomplexityatallbecausethesourcestringisﬁxed.Inprinciplewecouldaddfeaturesthatdependonarbitrarysource-sidecontext.3.1NewFeaturesinHieroforWSDToincorporateWSDintoHiero,weusethetrans-lationsproposedbytheWSDsystemtohelpHieroobtainabetterormoreprobablederivationduringthetranslationofeachsourcesentence.Toachievethis,whenagrammarruleRisconsideredduringdecoding,andwerecognizethatsomeoftheter-minalsymbols(words)inαarealsochosenbytheWSDsystemastranslationsforsometerminalsym-bols(words)inγ,wecomputethefollowingfea-tures:•Pwsd(t|s)givesthecontextualprobabilityoftheWSDclassiﬁerchoosingtasatranslationfors,wheret(s)issomesubstringofterminalsymbolsinα(γ).Becausethisprobabilityonlyappliestosomerules,andwedon’twanttope-nalizethoserules,wemustaddanotherfeature,•Ptywsd=exp(−|t|),wheretisthetranslationchosenbytheWSDsystem.Thisfeature,withanegativeweight,rewardsrulesthatusetrans-lationssuggestedbytheWSDmodule.Notethatwecantakethenegativelogarithmoftherule/derivationweightsandthinkofthemascostsratherthanprobabilities.4GatheringTrainingExamplesforWSDOurexperimentswereforChinesetoEnglishtrans-lation.Hence,inthecontextofourwork,asyn-chronousCFGgrammarruleX→hγ,αigatheredbyHieroconsistsofaChineseportionγandacor-respondingEnglishportionα,whereeachportionisasequenceofwordsandnon-terminalsymbols.OurWSDclassiﬁersuggestsalistofEnglishphrases(whereeachphraseconsistsofoneormoreEnglishwords)withassociatedcontextualprobabil-itiesaspossibletranslationsforeachparticularChi-nesephrase.Ingeneral,theChinesephrasemayconsistofkChinesewords,wherek=1,2,3,....However,welimitkto1or2forexperimentsre-portedinthispaper.Futureworkcanexploreen-largingk.WheneverHieroisabouttoextractagrammarrulewhereitsChineseportionisaphraseofoneortwoChinesewordswithnonon-terminalsymbols,wenotethelocation(sentenceandtokenoffset)intheChinesehalfoftheparallelcorpusfromwhichtheChineseportionoftheruleisextracted.Theac-tualsentenceinthecorpuscontainingtheChinesephrase,andtheonesentencebeforeandtheonesen-tenceafterthatactualsentence,willserveasthecon-textforonetrainingexamplefortheChinesephrase,withthecorrespondingEnglishphraseofthegram-marruleasitstranslation.Hence,unliketraditionalWSDwherethesenseclassesaretiedtoaspeciﬁcsenseinventory,our“senses”hereconsistoftheEn-glishphrasesextractedastranslationsforeachChi-nesephrase.Sincetheextractedtrainingdatamay36

benoisy,foreachChinesephrase,weremoveEn-glishtranslationsthatoccuronlyonce.Furthermore,weonlyattemptWSDclassiﬁcationforthoseChi-nesephraseswithatleast10trainingexamples.UsingtheWSDclassiﬁerdescribedinSection2,weclassiﬁedthewordsineachChinesesourcesen-tencetobetranslated.WeﬁrstperformedWSDonallsingleChinesewordswhichareeithernoun,verb,oradjective.Next,weclassiﬁedtheChinesephrasesconsistingof2consecutiveChinesewordsbysimplytreatingthephraseasasingleunit.Whenperform-ingclassiﬁcation,wegiveasoutputthesetofEn-glishtranslationswithassociatedcontext-dependentprobabilities,whicharetheprobabilitiesofaChi-neseword(phrase)translatingintoeachEnglishphrase,dependingonthecontextoftheChineseword(phrase).AfterWSD,theithwordciineveryChinesesentencemayhaveupto3setsofassoci-atedtranslationsprovidedbytheWSDsystem:asetoftranslationsforciasasingleword,asecondsetoftranslationsforci−1ciconsideredasasingleunit,andathirdsetoftranslationsforcici+1consideredasasingleunit.5IncorporatingWSDduringDecodingThefollowingtasksaredoneforeachrulethatisconsideredduringdecoding:•identifyChinesewordstosuggesttranslationsfor•matchsuggestedtranslationsagainsttheEn-glishsideoftherule•computefeaturesfortheruleTheWSDsystemisabletopredicttranslationsonlyforasubsetofChinesewordsorphrases.Hence,wemustﬁrstidentifywhichpartsoftheChinesesideoftherulehavesuggestedtranslationsavailable.Here,weconsidersubstringsoflengthuptotwo,andwegiveprioritytolongersubstrings.Next,wewanttoknow,foreachChinesesub-stringconsidered,whethertheWSDsystemsup-portstheChinese-Englishtranslationrepresentedbytherule.IftheruleisﬁnallychosenaspartofthebestderivationfortranslatingtheChinesesentence,thenallthewordsintheEnglishsideoftherulewillappearinthetranslatedEnglishsentence.Hence,weneedtomatchthetranslationssuggestedbytheWSDsystemagainsttheEnglishsideoftherule.ItisforthesematchingrulesthattheWSDfeatureswillapply.ThetranslationsproposedbytheWSDsystemmaybemorethanonewordlong.Inorderforaproposedtranslationtomatchtherule,werequiretwoconditions.First,theproposedtranslationmustbeasubstringoftheEnglishsideoftherule.Forexample,theproposedtranslation“everyto”wouldnotmatchthechunk“everymonthto”.Second,thematchmustcontainatleastonealignedChinese-Englishwordpair,butwedonotmakeanyotherrequirementsaboutthealignmentoftheotherChi-neseorEnglishwords.1Iftherearemultiplepossi-blematches,wechoosethelongestproposedtrans-lation;inthecaseofatie,wechoosetheproposedtranslationwiththehighestscoreaccordingtotheWSDmodel.Deﬁneachunkofaruletobeamaximalsub-stringofterminalsymbolsontheEnglishsideoftherule.Forexample,inRule(2),thechunkswouldbe“goto”and“everymonthto”.WheneverweﬁndamatchingWSDtranslation,wemarkthewholechunkontheEnglishsideasconsumed.Finally,wecomputethefeaturevaluesfortherule.ThefeaturePwsd(t|s)isthesumofthecosts(accordingtotheWSDmodel)ofallthematchedtranslations,andthefeaturePtywsdisthesumofthelengthsofallthematchedtranslations.Figure1showsthepseudocodefortherulescor-ingalgorithminmoredetail,particularlywithre-gardstoresolvingconﬂictsbetweenoverlappingmatches.ToillustratethealgorithmgiveninFigure1,considerRule(2).Hereafter,wewillusesymbolstorepresenttheChineseandEnglishwordsintherule:c1,c2,andc3willrepresenttheChinesewords“຅”,“ഢ”,and“տ”respectively.Similarly,e1,e2,e3,e4,ande5willrepresenttheEnglishwordsgo,to,every,month,andtorespectively.Hence,Rule(2)hastwochunks:e1e2ande3e4e5.Whentheruleisextractedfromtheparallelcorpus,ithasthesealignmentsbetweenthewordsofitsChineseandEnglishportion:{c1–e3,c2–e4,c3–e1,c3–e2,c3–e5},whichmeansthatc1isalignedtoe3,c2isalignedto1Inordertocheckthisrequirement,weextendedHierotomakewordalignmentinformationavailabletothedecoder.37

Input:ruleRconsideredduringdecodingwithitsownassociatedcostRLc=listofsymbolsinChineseportionofRWSDcost=0i=1whilei≤len(Lc):ci=ithsymbolinLcifciisaChineseword(i.e.,notanon-terminalsymbol):seenChunk=∅//seenChunkisaglobalvariableandispassedbyreferencetomatchWSDif(ciisnotthelastsymbolinLc)and(ci+1isaterminalsymbol):thenci+1=(i+1)thsymbolinLc,elseci+1=NULLif(ci+1!=NULL)and(ci,ci+1)asasingleunithasWSDtranslations:WSDc=setofWSDtranslationsfor(ci,ci+1)asasingleunitwithcontext-dependentprobabilitiesWSDcost=WSDcost+matchWSD(ci,WSDc,seenChunk)WSDcost=WSDcost+matchWSD(ci+1,WSDc,seenChunk)i=i+1else:WSDc=setofWSDtranslationsforciwithcontext-dependentprobabilitiesWSDcost=WSDcost+matchWSD(ci,WSDc,seenChunk)i=i+1costR=costR+WSDcostmatchWSD(c,WSDc,seenChunk)://seenChunkisthesetofchunksofRalreadyexaminedforpossiblematchingWSDtranslationscost=0ChunkSet=setofchunksinRalignedtocforchunkjinChunkSet:ifchunkjnotinseenChunk:seenChunk=seenChunk∪{chunkj}Echunkj=setofEnglishwordsinchunkjalignedtocCandidatewsd=∅forwsdkinWSDc:if(wsdkissub-sequenceofchunkj)and(wsdkcontainsatleastonewordinEchunkj)Candidatewsd=Candidatewsd∪{wsdk}wsdbest=bestmatchingtranslationinCandidatewsdagainstchunkjcost=cost+costByWSDfeatures(wsdbest)//costByWSDfeaturessumsupthecostofthetwoWSDfeaturesreturncostFigure1:WSDtranslationsaffectingthecostofaruleRconsideredduringdecoding.e4,andc3isalignedtoe1,e2,ande5.Althoughallwordsarealignedhere,ingeneralforarule,someofitsChineseorEnglishwordsmaynotbeassociatedwithanyalignments.Inourexperiment,c1c2asaphrasehasalistoftranslationsproposedbytheWSDsystem,includ-ingtheEnglishphrase“everymonth”.matchWSDwillﬁrstbeinvokedforc1,whichisalignedtoonlyonechunke3e4e5viaitsalignmentwithe3.Since“everymonth”isasub-sequenceofthechunkandalsocontainstheworde3(“every”),itisnotedasacandidatetranslation.Later,itisdeterminedthatthemostnumberofwordsanycandidatetranslationhasistwowords.Sinceamongallthe2-wordcandi-datetranslations,thetranslation“everymonth”hasthehighesttranslationprobabilityasassignedbytheWSDclassiﬁer,itischosenasthebestmatchingtranslationforthechunk.matchWSDistheninvokedforc2,whichisalignedtoonlyonechunke3e4e5.However,sincethischunkhasalreadybeenexam-inedbyc1withwhichitisconsideredasaphrase,nofurthermatchingisdoneforc2.Next,matchWSDisinvokedforc3,whichisalignedtobothchunksofR.TheEnglishphrases“goto”and“to”areamongthelistoftranslationsproposedbytheWSDsystemforc3,andtheyareeventuallychosenasthebestmatch-ingtranslationsforthechunkse1e2ande3e4e5,re-spectively.6ExperimentsAsmentioned,ourexperimentswereonChinesetoEnglishtranslation.Similarto(Chiang,2005),wetrainedtheHierosystemontheFBIScorpus,usedtheNISTMT2002evaluationtestsetasourdevel-opmentsettotunethefeatureweights,andtheNISTMT2003evaluationtestsetasourtestdata.Using38

SystemBLEU-4Individualn-gramprecisions1234Hiero29.7374.7340.1421.8311.93Hiero+WSD30.3074.8240.4022.4512.42Table1:BLEUscoresFeaturesSystemPlm(e)P(γ|α)P(α|γ)Pw(γ|α)Pw(α|γ)PtyphrGluePtywordPwsd(t|s)PtywsdHiero0.23370.08820.16660.03930.13570.0665−0.0582−0.4806--Hiero+WSD0.19370.07700.11240.04870.03800.0988−0.0305−0.17470.1051−0.1611Table2:WeightsforeachfeatureobtainedbyMERTtraining.TheﬁrsteightfeaturesarethoseusedbyHieroin(Chiang,2005).theEnglishportionoftheFBIScorpusandtheXin-huaportionoftheGigawordcorpus,wetrainedatri-gramlanguagemodelusingtheSRILanguageMod-ellingToolkit(Stolcke,2002).Following(Chiang,2005),weusedtheversion11aNISTBLEUscriptwithitsdefaultsettingstocalculatetheBLEUscores(Papinenietal.,2002)basedoncase-insensitiven-grammatching,wherenisupto4.First,weperformedwordalignmentontheFBISparallelcorpususingGIZA++(OchandNey,2000)inbothdirections.Thewordalignmentsofbothdirectionsarethencombinedintoasinglesetofalignmentsusingthe“diag-and”methodofKoehnetal.(2003).Basedonthesealignments,syn-chronousCFGrulesarethenextractedfromthecor-pus.WhileHieroisextractinggrammarrules,wegatheredWSDtrainingdatabyfollowingtheproce-duredescribedinsection4.6.1HieroResultsUsingtheMT2002testset,werantheminimum-errorratetraining(MERT)(Och,2003)withthedecodertotunetheweightsforeachfeature.TheweightsobtainedareshownintherowHieroofTable2.Usingtheseweights,werunHiero’sde-codertoperformtheactualtranslationoftheMT2003testsentencesandobtainedaBLEUscoreof29.73,asshownintherowHieroofTable1.Thisishigherthanthescoreof28.77reportedin(Chiang,2005),perhapsduetodifferencesinwordsegmenta-tion,etc.NotethatcomparingwiththeMTsystemsusedin(CarpuatandWu,2005)and(CabezasandResnik,2005),theHierosystemweareusingrep-resentsamuchstrongerbaselineMTsystemuponwhichtheWSDsystemmustimprove.6.2Hiero+WSDResultsWethenaddedtheWSDfeaturesofSection3.1intoHieroandrerantheexperiment.Theweightsob-tainedbyMERTareshownintherowHiero+WSDofTable2.WenotethatanegativeweightislearntforPtywsd.Thismeansthatingeneral,themodelprefersgrammarruleshavingchunksthatmatchesWSDtranslations.Thismatchesourintuition.Us-ingtheweightsobtained,wetranslatedthetestsen-tencesandobtainedaBLEUscoreof30.30,asshownintherowHiero+WSDofTable1.Theim-provementof0.57isstatisticallysigniﬁcantatp<0.05usingthesign-testasdescribedbyCollinsetal.(2005),with374(+1),318(−1)and227(0).Us-ingthebootstrap-samplingtestdescribedin(Koehn,2004b),theimprovementisstatisticallysigniﬁcantatp<0.05.Thoughtheimprovementismodest,itisstatisticallysigniﬁcantandthispositiveresultisim-portantinviewofthenegativeﬁndingsin(CarpuatandWu,2005)thatWSDdoesnothelpMT.Fur-thermore,notethatHiero+WSDhashighern-gramprecisionsthanHiero.7AnalysisIdeally,theWSDsystemshouldbesuggestinghigh-qualitytranslationswhicharefrequentlypartofthereferencesentences.Todeterminethis,wenotethesetofgrammarrulesusedinthebestderivationfortranslatingeachtestsentence.Fromtherulesofeachtestsentence,wetabulatedthesetoftranslationsproposedbytheWSDsystemandcheckwhethertheyarefoundintheassociatedreferencesentences.OntheentiresetofNISTMT2003evaluationtestsentences,anaverageof10.36translationsproposed39

No.ofAlltestsentences+1fromCollinssign-testwordsinNo.of%matchNo.of%matchWSDtranslationsWSDtranslationsusedreferenceWSDtranslationsusedreference1708777.31307877.682193066.1186164.92337143.1317148.54412426.615228.85Table3:NumberofWSDtranslationsusedandproportionthatmatchesagainstrespectivereferencesen-tences.WSDtranslationslongerthan4wordsareverysparse(lessthan10occurrences)andthustheyarenotshown.bytheWSDsystemwereusedforeachsentence.Whenlimitedtothesetof374sentenceswhichwerejudgedbytheCollinssign-testtohavebettertranslationsfromHiero+WSDthanfromHiero,ahighernumber(11.14)ofproposedtranslationswereusedonaverage.Further,fortheentiresetoftestsentences,73.01%oftheproposedtranslationsarefoundinthereferencesentences.Thisincreasedtoaproportionof73.22%whenlimitedtothesetof374sentences.Theseﬁguresshowthathavingmore,andhigher-qualityproposedtranslationscontributedtothesetof374sentencesbeingbettertranslationsthantheirrespectiveoriginaltranslationsfromHi-ero.Table3givesadetailedbreakdownoftheseﬁguresaccordingtothenumberofwordsineachproposedtranslation.Forinstance,overallthetestsentences,theWSDmodulegave7087translationsofsingle-wordlength,and77.31%ofthesetrans-lationsmatchtheirrespectivereferencesentences.Wenotethatalthoughtheproportionofmatching2-wordtranslationsisslightlylowerforthesetof374sentences,theproportionincreasesfortranslationshavingmorewords.AftertheexperimentsinSection6werecom-pleted,wevisuallyinspectedthetranslationoutputofHieroandHiero+WSDtocategorizethewaysinwhichintegratingWSDcontributestobettertrans-lations.TheﬁrstwayinwhichWSDhelpsiswhenitenablestheintegratedHiero+WSDsystemtoout-putextraappropriateEnglishwords.Forexample,thetranslationsfortheChinesesentence“...ୈԢвˑૌ֫៞ϛ˒ƥयು໿ئ੧ഛ࠰ప֭ୈԢвᡥ๣ˊ”areasfollows.•Hiero:...orotherbadbehavior”,willbemoreaidandotherconcessions.•Hiero+WSD:...orotherbadbehavior”,willbeunabletoobtainmoreaidandotherconces-sions.Here,theChinesewords“ು໿ئ੧”arenottrans-latedbyHieroatall.Byprovidingthecorrecttrans-lationof“unabletoobtain”for“ು໿ئ੧”,thetranslationoutputofHiero+WSDismorecomplete.AsecondwayinwhichWSDhelpsisbycorrect-ingapreviouslyincorrecttranslation.Forexample,fortheChinesesentence“...ƥޗԘދهಽОຜƥ...”,theWSDsystemhelpstocorrectHiero’soriginaltranslationbyprovidingthecorrecttransla-tionof“allethnicgroups”fortheChinesephrase“هಽ”:•Hiero:...,andpeopleofallnationalitiesacrossthecountry,...•Hiero+WSD:...,andpeopleofallethnicgroupsacrossthecountry,...Wealsolookedatthesetof318sentencesthatwerejudgedbytheCollinssign-testtobeworsetranslations.Wefoundthatinsomesituations,Hiero+WSDhasprovidedextraappropriateEnglishwords,butthoseparticularwordsarenotusedinthereferencesentences.AninterestingexampleisthetranslationoftheChinesesentence“ဉ༬࠮ᮿெ׏ᱻ៞ϛૌ֫यು໿ئ੧ഛ࠰ప֭”.•Hiero:AustralianforeignministersaidthatNorthKoreabadbehaviorwillbemoreaid•Hiero+WSD:AustralianforeignministersaidthatNorthKoreabadbehaviorwillbeunabletoobtainmoreaidThisissimilartotheexamplementionedearlier.Inthiscasehowever,thoseextraEnglishwordspro-videdbyHiero+WSD,thoughappropriate,donot40

resultinmoren-grammatchesasthereferencesen-tencesusedphrasessuchas“willnotgain”,“willnotget”,etc.SincetheBLEUmetricisprecisionbased,thelongersentencetranslationbyHiero+WSDgetsalowerBLEUscoreinstead.8ConclusionWehaveshownthatWSDimprovesthetransla-tionperformanceofastate-of-the-arthierarchicalphrase-basedstatisticalMTsystemandthisim-provementisstatisticallysigniﬁcant.WehavealsodemonstratedonewaytointegrateaWSDsystemintoanMTsystemwithoutintroducinganyrulesthatcompeteagainstexistingrules,andwherethefeature-weighttuninganddecodingplacetheWSDsystemonanequalfootingwiththeothermodelcomponents.Forfuturework,animmediatestepwouldbefortheWSDclassiﬁertoprovidetrans-lationsforlongerChinesephrases.Also,differentalternativescouldbetriedtomatchthetranslationsprovidedbytheWSDclassiﬁeragainstthechunksofrules.Finally,besidesourproposedapproachofintegratingWSDintostatisticalMTviatheintro-ductionoftwonewfeatures,wecouldexploreotheralternativewaysofintegration.AcknowledgementsYeeSengChanissupportedbyaSingaporeMillen-niumFoundationScholarship(refno.SMF-2004-1076).DavidChiangwaspartiallysupportedun-dertheGALEprogramoftheDefenseAdvancedResearchProjectsAgency,contractHR0011-06-C-0022.ReferencesC.CabezasandP.Resnik.2005.UsingWSDtechniquesforlexicalselectioninstatisticalmachinetranslation.Technicalreport,UniversityofMaryland.M.CarpuatandD.Wu.2005.Wordsensedisambiguationvs.statisticalmachinetranslation.InProc.ofACL05,pages387–394.M.Carpuat,W.Su,andD.Wu.2004.AugmentingensembleclassiﬁcationforwordsensedisambiguationwithakernelPCAmodel.InProc.ofSENSEVAL-3,pages88–92.C.C.ChangandC.J.Lin,2001.LIBSVM:alibraryforsup-portvectormachines.Softwareavailableathttp://www.csie.ntu.edu.tw/˜cjlin/libsvm.D.Chiang.2005.Ahierarchicalphrase-basedmodelforsta-tisticalmachinetranslation.InProc.ofACL05,pages263–270.D.Chiang.2007.Hierarchicalphrase-basedtranslation.ToappearinComputationalLinguistics,33(2).M.Collins,P.Koehn,andI.Kucerova.2005.Clauserestruc-turingforstatisticalmachinetranslation.InProc.ofACL05,pages531–540.U.Germann.2003.Greedydecodingforstatisticalmachinetranslationinalmostlineartime.InProc.ofHLT-NAACL03,pages72–79.P.Koehn,F.J.Och,andD.Marcu.2003.Statisticalphrase-basedtranslation.InProc.ofHLT-NAACL03,pages48–54.P.Koehn.2003.NounPhraseTranslation.Ph.D.thesis,Uni-versityofSouthernCalifornia.P.Koehn.2004a.Pharaoh:Abeamsearchdecoderforphrase-basedstatisticalmachinetranslationmodels.InProc.ofAMTA04,pages115–124.P.Koehn.2004b.Statisticalsigniﬁcancetestsformachinetranslationevaluation.InProc.ofEMNLP04,pages388–395.Y.K.LeeandH.T.Ng.2002.Anempiricalevaluationofknowledgesourcesandlearningalgorithmsforwordsensedisambiguation.InProc.ofEMNLP02,pages41–48.P.M.IILewisandR.E.Stearns.1968.Syntax-directedtrans-duction.JournaloftheACM,15(3):465–488.D.MarcuandW.Wong.2002.Aphrase-based,jointproba-bilitymodelforstatisticalmachinetranslation.InProc.ofEMNLP02,pages133–139.F.J.OchandH.Ney.2000.Improvedstatisticalalignmentmodels.InProc.ofACL00,pages440–447.F.J.OchandH.Ney.2002.Discriminativetrainingandmax-imumentropymodelsforstatisticalmachinetranslation.InProc.ofACL02,pages295–302.F.J.OchandH.Ney.2004.Thealignmenttemplateapproachtostatisticalmachinetranslation.ComputationalLinguis-tics,30(4):417–449.F.J.Och.2003.Minimumerrorratetraininginstatisticalma-chinetranslation.InProc.ofACL03,pages160–167.K.Papineni,S.Roukos,T.Ward,andW.J.Zhu.2002.BLEU:Amethodforautomaticevaluationofmachinetranslation.InProc.ofACL02,pages311–318.A.Stolcke.2002.SRILM-anextensiblelanguagemodelingtoolkit.InProc.ofICSLP02,pages901–904.D.Vickrey,L.Biewald,M.Teyssier,andD.Koller.2005.Word-sensedisambiguationformachinetranslation.InProc.ofEMNLP05,pages771–778.D.Wu.1996.Apolynomial-timealgorithmforstatisticalma-chinetranslation.InProc.ofACL96,pages152–158.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 41–48,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

41

Learning Expressive Models for Word Sense Disambiguation Lucia Specia NILC/ICMC University of São Paulo Caixa Postal 668, 13560-970 São Carlos, SP, Brazil lspecia@icmc.usp.br Mark Stevenson Department of Computer Science University of Sheffield Regent Court, 211 Portobello St. Sheffield, S1 4DP, UK  marks@dcs.shef.ac.uk Maria das Graças V. Nunes NILC/ICMC University of São Paulo Caixa Postal 668, 13560-970  São Carlos, SP, Brazil gracan@icmc.usp.br   Abstract We present a novel approach to the word sense disambiguation problem which makes use of corpus-based evidence com-bined with background knowledge. Em-ploying an inductive logic programming algorithm, the approach generates expres-sive disambiguation rules which exploit several knowledge sources and can also model relations between them. The ap-proach is evaluated in two tasks: identifica-tion of the correct translation for a set of highly ambiguous verbs in English-Portuguese translation and disambiguation of verbs from the Senseval-3 lexical sam-ple task. The average accuracy obtained for the multilingual task outperforms the other machine learning techniques investigated. In the monolingual task, the approach per-forms as well as the state-of-the-art sys-tems which reported results for the same set of verbs. 1 Introduction Word Sense Disambiguation (WSD) is concerned with the identification of the meaning of ambi-guous words in context. For example, among the possible senses of the verb “run” are “to move fast by using one's feet” and “to direct or control”. WSD can be useful for many applications, includ-ing information retrieval, information extraction and machine translation. Sense ambiguity has been recognized as one of the most important obstacles to successful language understanding since the ear-ly 1960’s and many techniques have been pro-posed to solve the problem. Recent approaches focus on the use of various lexical resources and corpus-based techniques in order to avoid the sub-stantial effort required to codify linguistic know-ledge. These approaches have shown good results; particularly those using supervised learning (see Mihalcea et al., 2004 for an overview of state-of-the-art systems). However, current approaches rely on limited knowledge representation and modeling techniques: traditional machine learning algorithms and attribute-value vectors to represent disambigu-ation instances. This has made it difficult to exploit deep knowledge sources in the generation of the disambiguation models, that is, knowledge that goes beyond simple features extracted directly from the corpus, like bags-of-words and colloca-tions, or provided by shallow natural language tools like part-of-speech taggers.  In this paper we present a novel approach for WSD that follows a hybrid strategy, i.e. combines knowledge and corpus-based evidence, and em-ploys a first-order formalism to allow the represen-tation of deep knowledge about disambiguation examples together with a powerful modeling tech-nique to induce theories based on the examples and background knowledge. This is achieved using Inductive Logic Programming (ILP) (Muggleton, 1991), which has not yet been applied to WSD.  Our hypothesis is that by using a very expres-sive representation formalism, a range of (shallow and deep) knowledge sources and ILP as learning technique, it is possible to generate models that, when compared to models produced by machine learning algorithms conventionally applied to 42

WSD, are both more accurate for fine-grained dis-tinctions, and “interesting”, from a knowledge ac-quisition point of view (i.e., convey potentially new knowledge that can be easily interpreted by humans).  WSD systems have generally been more suc-cessful in the disambiguation of nouns than other grammatical categories (Mihalcea et al., 2004). A common approach to the disambiguation of nouns has been to consider a wide context around the ambiguous word and treat it as a bag of words or limited set of collocates. However, disambiguation of verbs generally benefits from more specific knowledge sources, such as the verb’s relation to other items in the sentence (for example, by ana-lysing the semantic type of its subject and object). Consequently, we believe that the disambiguation of verbs is task to which ILP is particularly well-suited. Therefore, this paper focuses on the disam-biguation of verbs, which is an interesting task since much of the previous work on WSD has con-centrated on the disambiguation of nouns.  WSD is usually approached as an independent task, however, it has been argued that different applications may have specific requirements (Res-nik and Yarowsky, 1997). For example, in machine translation, WSD, or translation disambiguation, is responsible for identifying the correct translation for an ambiguous source word. There is not always a direct relation between the possible senses for a word in a (monolingual) lexicon and its transla-tions to a particular language, so this represents a different task to WSD against a (monolingual) lexicon (Hutchins and Somers, 1992). Although it has been argued that WSD does not yield better translation quality than a machine translation system alone, it has been recently shown that a WSD module that is developed following specific multilingual requirements can significantly im-prove the performance of a machine translation system (Carpuat et al., 2006). This paper focuses on the application of our ap-proach to the translation of verbs in English to Por-tuguese translation, specifically for a set of 10 mainly light and highly ambiguous verbs. We also experiment with a monolingual task by using the verbs from Senseval-3 lexical sample task. We explore knowledge from 12 syntactic, semantic and pragmatic sources. In principle, the proposed approach could also be applied to any lexical dis-ambiguation task by customizing the sense reposi-tory and knowledge sources. In the remainder of this paper we first present related approaches to WSD and discuss their limi-tations (Section 2). We then describe some basic concepts on ILP and our application of this tech-nique to WSD (Section 3). Finally, we described our experiments and their results (Section 4).  2 Related Work WSD approaches can be classified as (a) know-ledge-based approaches, which make use of lin-guistic knowledge, manually coded or extracted from lexical resources (Agirre and Rigau, 1996; Lesk 1986); (b) corpus-based approaches, which make use of shallow knowledge automatically ac-quired from corpus and statistical or machine learning algorithms to induce disambiguation models (Yarowsky, 1995; Schütze 1998); and (c) hybrid approaches, which mix characteristics from the two other approaches to automatically acquire disambiguation models from corpus supported by linguistic knowledge (Ng and Lee 1996; Stevenson and Wilks, 2001). Hybrid approaches can combine advantages from both strategies, potentially yielding accurate and comprehensive systems, particularly when deep knowledge is explored. Linguistic knowledge is available in electronic resources suitable for practical use, such as WordNet (Fellbaum, 1998), dictionaries and parsers. However, the use of this information has been hampered by the limitations of the modeling techniques that have been ex-plored so far: using deep sources of domain know-ledge is beyond the capabilities of such techniques, which are in general based on attribute-value vec-tor representations. Attribute-value vectors consist of a set of attributes intended to represent properties of the examples. Each attribute has a type (its name) and a single value for a given example. Therefore, attribute-value vectors have the same expressive-ness as propositional formalisms, that is, they only allow the representation of atomic propositions and constants. These are the representations used by most of the machine learning algorithms conven-tionally employed to WSD, for example Naïve Bayes and decision-trees. First-order logic, a more expressive formalism which is employed by ILP, allows the representation of variables and n-ary predicates, i.e., relational knowledge.  43

In the hybrid approaches that have been ex-plored so far, deep knowledge, like selectional pre-ferences, is either pre-processed into a vector representation to accommodate machine learning algorithms, or used in previous steps to filter out possible senses e.g. (Stevenson and Wilks, 2001). This may cause information to be lost and, in addi-tion, deep knowledge sources cannot interact in the learning process. As a consequence, the models produced reflect only the shallow knowledge that is provided to the learning algorithm.  Another limitation of attribute-value vectors is the need for a unique representation for all the ex-amples: one attribute is created for every knowl-edge feature and the same structure is used to characterize all the examples. This usually results in a very sparse representation of the data, given that values for certain features will not be available for many examples. The problem of data sparse-ness increases as more knowledge is exploited and this can cause problems for the machine learning algorithms. A final disadvantage of attribute-value vectors is that equivalent features may have to be bounded to distinct identifiers. An example of this occurs when the syntactic relations between words in a sentence are represented by attributes for each pos-sible relation, sentences in which there is more than one instantiation for a particular grammatical role cannot be easily represented.  For example, the sentence “John and Anna gave Mary a present.” contains a coordinate subject and, since each fea-ture requires a unique identifier, two are required (subj1-verb1, subj2-verb1). These will be treated as two independent pieces of knowledge by the learn-ing algorithm.  First-order formalisms allow a generic predicate to be created for every possible syntactic role, re-lating two or more elements. For example has_subject(verb, subject), which could then have two instantiations: has_subject(give, john) and has_subject(give, anna). Since each example is represented independently from the others, the data sparseness problem is minimized. Therefore, ILP seems to provide the most general-purpose frame-work for dealing with such data: it does not suffer from the limitations mentioned above since there are explicit provisions made for the inclusion of background knowledge of any form, and the repre-sentation language is powerful enough to capture contextual relationships. 3 A hybrid relational approach to WSD In what follows we provide an introduction to ILP and then outline how it is applied to WSD by pre-senting the sample corpus and knowledge sources used in our experiments. 3.1 Inductive Logic Programming Inductive Logic Programming (Muggleton, 1991) employs techniques from Machine Learning and Logic Programming to build first-order theories from examples and background knowledge, which are also represented by first-order clauses. It allows the efficient representation of substantial know-ledge about the problem, which is used during the learning process, and produces disambiguation models that can make use of this knowledge. The general approach underlying ILP can be outlined as follows:  Given: -  a set of positive and negative examples E = E+ ¨¨¨¨ E- - a predicate p specifying the target relation to be learned - knowledge KKKK of the domain, described ac-cording to a language Lk, which specifies which predicates qi can be part of the definition of p. The goal is: to induce a hypothesis (or theory) h for p, with relation to E and KKKK, which covers most of the E+, without covering the E-, i.e., K (cid:217)(cid:217)(cid:217)(cid:217) h  E+ and K (cid:217)(cid:217)(cid:217)(cid:217) h  E-.   We use the Aleph ILP system (Srinivasan, 2000), which provides a complete inference engine and can be customized in various ways. The default inference engine induces a theory iteratively using the following steps: 1. One instance is randomly selected to be gen-eralized.  2. A more specific clause (the bottom clause) is built using inverse entailment (Muggleton, 1995), generally consisting of the representation of all the knowledge about that example. 3. A clause that is more generic than the bottom clause is searched for using a given search (e.g., best-first) and evaluation strategy (e.g., number of positive examples covered). 4. The best clause is added to the theory and the examples covered by that clause are removed from the sample set. Stop if there are more no examples in the training set, otherwise return to step 1. 44

3.2 Sample data This approach was evaluated using two scenarios: (1) an English-Portuguese multilingual setting ad-dressing 10 very frequent and problematic verbs selected in a previous study (Specia et. al., 2005); and (2) an English setting consisting of 32 verbs from Senseval-3 lexical sample task (Mihalcea et. al. 2004). For the first scenario a corpus containing 500 sentences for each of the 10 verbs was constructed. The text was randomly selected from corpora of different domains and genres, including literary fiction, Bible, computer science dissertation ab-stracts, operational system user manuals, newspa-pers and European Parliament proceedings. This corpus was automatically annotated with the trans-lation of the verb using a tagging system based on parallel corpus, statistical information and transla-tion dictionaries (Specia et al., 2005), followed by a manual revision. For each verb, the sense reposi-tory was defined as the set of all the possible trans-lations of that verb in the corpus. 80% of the corpus was randomly selected and used for train-ing, with the remainder retained for testing. The 10 verbs, number of possible translations and the per-centage of sentences for each verb which use the most frequent translation are shown in Table 1. For the monolingual scenario, we use the sense tagged corpus and sense repositories provided for verbs in Senseval-3. There are 32 verbs with be-tween 40 and 398 examples each. The number of senses varies between 3 and 10 and the average percentage of examples with the majority (most frequent) sense is 55%.    Verb # Translations Most frequent translation - % ask 7 53 come 29 36 get 41 13 give 22 72 go 30 53 live 8 66 look 12 41 make 21 70 take 32 25 tell 8 66 Table 1. Verbs and possible senses in our corpus  Both corpora were lemmatized and part-of-speech (POS) tagged using Minipar (Lin, 1993) and Mxpost (Ratnaparkhi, 1996), respectivelly. Addi-tionally, proper nouns identified by the tagger were replaced by a single identifier (proper_noun) and pronouns replaced by identifiers representing classes of pronouns (relative_pronoun, etc.).  3.3 Knowledge sources We now describe the background knowledge sources used by the learning algorithm, having as an example sentence (1), in which the word “com-ing” is the target verb being disambiguated.  (1) "If there is such a thing as reincarnation, I would not mind coming back as a squirrel".  KS1. Bag-of-words consisting of 5 words to the right and left of the verb (excluding stop words), represented using definitions of the form has_bag(snt, word): has_bag(snt1, mind). has_bag(snt1, not). …  KS2. Frequent bigrams consisting of pairs of adja-cent words in a sentence (other than the target verb) which occur more than 10 times in the cor-pus, represented by has_bigram(snt, word1, word2): has_bigram(snt1, back, as). has_bigram(snt1, such, a). …  KS3. Narrow context containing 5 content words to the right and left of the verb, identified using POS tags, represented by has_narrow(snt, word_position, word): has_narrow(snt1, 1st_word_left, mind). has_narrow(snt1, 1st_word_right, back). …  KS4. POS tags of 5 words to the right and left of the verb, represented by has_pos(snt, word_position, pos): has pos(snt1, 1st_word_left, nn). has pos(snt1, 1st_word_right, rb). …  KS5. 11 collocations of the verb: 1st preposition to the right, 1st and 2nd words to the left and right, 1st noun, 1st adjective, and 1st verb to the left and right. These are represented using definitions of the form has_collocation(snt, type, collocation): has_collocation(snt1, 1st_prep_right, back). has_collocation(snt1, 1st_noun_left, mind).… 45

KS6. Subject and object of the verb obtained using Minipar and represented by has_rel(snt, type, word): has_rel(snt1, subject, i). has_rel(snt1, object, nil). …  KS7. Grammatical relations not including the tar-get verb also identified using Minipar. The rela-tions (verb-subject, verb-object, verb-modifier, subject-modifier, and object-modifier) occurring more than 10 times in the corpus are represented by has_related_pair(snt, word1, word2): has_related_pair(snt1, there, be). …  KS8. The sense with the highest count of overlap-ping words in its dictionary definition and in the sentence containing the target verb (excluding stop words) (Lesk, 1986), represented by has_overlapping(sentence, translation): has_overlapping(snt1, voltar).  KS9. Selectional restrictions of the verbs defined using LDOCE (Procter, 1978). WordNet is used when the restrictions imposed by the verb are not part of the description of its arguments, but can be satisfied by synonyms or hyperonyms of those ar-guments. A hierarchy of feature types is used to account for restrictions established by the verb that are more generic than the features describing its arguments in the sentence. This information is represented by definitions of the form satis-fy_restriction(snt, rest_subject, rest_object): satisfy_restriction(snt1, [human], nil). satisfy_restriction(snt1, [animal, human], nil).  KS1-KS9 can be applied to both multilingual and monolingual disambiguation tasks. The following knowledge sources were specifically designed for multilingual applications:  KS10. Phrasal verbs in the sentence identified using a list extracted from various dictionaries. (This information was not used in the monolingual task because phrasal constructions are not considered verb senses in Senseval data.) These are represented by definitions of the form has_expression(snt, verbal_expression):  has_expression(snt1, “come back”).  KS11. Five words to the right and left of the target verb in the Portuguese translation. This could be obtained using a machine translation system that would first translate the non-ambiguous words in the sentence. In our experiments it was extracted using a parallel corpus and represented using defi-nitions of the form has_bag_trns(snt, portu-guese_word): has_bag_trns(snt1, coelho). has_bag_trns(snt1, reincarnação). …  KS12. Narrow context consisting of 5 collocations of the verb in the Portuguese translation, which take into account the positions of the words, represented by has_narrow_trns(snt, word_position, portuguese_word): has_narrow_trns(snt1, 1st_word_right, como). has_narrow_trns(snt1, 2nd_word_right, um). …  In addition to background knowledge, the system learns from a set of examples. Since all knowledge about them is expressed as background knowledge, their representation is very simple, containing only the sentence identifier and the sense of the verb in that sentence, i.e. sense(snt, sense): sense(snt1,voltar).  sense(snt2,ir). …  Based on the examples, background knowledge and a series of settings specifying the predicate to be learned (i.e., the heads of the rules), the predi-cates that can be in the conditional part of the rules, how the arguments can be shared among dif-ferent  predicates and several other parameters, the inference engine produces a set of symbolic rules. Figure 1 shows examples of the rules induced for the verb “to come” in the multilingual task.                 Figure 1. Examples of rules produced for the verb “come” in the multilingual task  Rule_1. sense(A, voltar) :-     has_collocation(A, 1st_prep_right, back). Rule_2. sense(A, chegar) :-    has_rel(A, subj, B), has_bigram(A, today, B),    has_bag_trans(A, hoje). Rule_3. sense(A, chegar) :-     satisfy_restriction(A, [animal, human], [concrete]);     has_expression(A, 'come at'). Rule_4. sense(A, vir) :-     satisfy_restriction(A, [animate], nil);      (has_rel(A, subj, B),     (has_pos(A, B, nnp); has_pos(A, B, prp))).  46

Models learned with ILP are symbolic and can be easily interpreted. Additionally, innovative knowl-edge about the problem can emerge from the rules learned by the system. Although some rules simply test shallow features such as collocates, others pose conditions on sets of knowledge sources, including relational sources, and allow non-instantiated ar-guments to be shared amongst them by means of variables. For example, in Figure 1, Rule_1 states that the translation of the verb in a sentence A will be “voltar” (return) if the first preposition to the right of the verb in that sentence is “back”. Rule_2 states that the translation of the verb will be “chegar” (arrive) if it has a certain subject B, which occurs frequently with the word “today” as a bigram, and if the partially translated sentence con-tains the word “hoje” (the translation of “today”). Rule_3 says that the translation of the verb will be “chegar” (reach) if the subject of the verb has the features “animal” or “human” and the object has the feature “concrete”, or if the verb occurs in the expression “come at”. Rule_4 states that the trans-lation of the verb will be “vir” (move toward) if the subject of the verb has the feature “animate” and there is no object, or if the verb has a subject B that is a proper noun (nnp) or a personal pronoun (prp). 4 Experiments and results To assess the performance of the approach the model produced for each verb was tested on the corresponding set of test cases by applying the rules in a decision-list like approach, i.e., retaining the order in which they were produced and backing off to the most frequent sense in the training set to classify cases that were not covered by any of the rules. All the knowledge sources were made avail-able to be used by the inference engine, since pre-vious experiments showed that they are all relevant (Specia, 2006). In what follows we present the re-sults and discuss each task.  4.1 Multilingual task Table 2 shows the accuracies (in terms of percen-tage of corpus instances which were correctly dis-ambiguated) obtained by the Aleph models. Results are compared against the accuracy that would be obtained by using the most frequent translation in the training set to classify all the ex-amples of the test set (in the column labeled “Ma-jority sense”). For comparison, we ran experiments with three learning algorithms frequently used for WSD, which rely on knowledge represented as attribute-value vectors: C4.5 (decision-trees), Naive Bayes and Support Vector Machine (SVM)1. In order to represent all knowledge sources in attribute-value vectors, KS2, KS7, KS9 and KS10 had to be pre-processed to be transformed into bi-nary attributes. For example, in the case of selec-tional restrictions (KS9), one attribute was created for each possible sense of the verb and a true/false value was assigned to it depending on whether the arguments of the verb satisfied any restrictions re-ferring to that sense. Results for each of these algo-rithms are also shown in Table 2. As we can see in Table 2, the accuracy of the ILP approach is considerably better than the most frequent sense baseline and also outperforms the other learning algorithms. This improvement is statistically significant (paired t-test; p < 0.05). As expected, accuracy is generally higher for verbs with fewer possible translations.  The models produced by Aleph for all the verbs are reasonably compact, containing 50 to 96 rules. In those models the various knowledge sources appear in different rules and all are used. This demonstrates that they are all useful for the disam-biguation of verbs.  Verb Majori- ty sense C4.5 Naïve  Bayes SVM Aleph ask 0.68 0.68 0.82 0.88 0.92 come 0.46 0.57 0.61 0.68 0.73 get 0.03 0.25 0.46 0.47 0.49 give 0.72 0.71 0.74 0.74 0.74 go 0.49 0.61 0.66 0.66 0.66 live 0.71 0.72 0.64 0.73 0.87 look 0.48 0.69 0.81 0.83 0.93 make 0.64 0.62 0.60 0.64 0.68 take 0.14 0.41 0.50 0.51 0.59 tell 0.65 0.67 0.66 0.68 0.82 Average 0.50 0.59 0.65 0.68 0.74 Table 2. Accuracies obtained by Aleph and other learning algorithms in the multilingual task  These results are very positive, particularly if we consider the characteristics of the multilingual sce-nario: (1) the verbs addressed are highly ambi-guous; (2) the corpus was automatically tagged and thus distinct synonym translations were sometimes                                                            1 The implementations provided by Weka were used. Weka is available from http://www.cs.waikato.ac.nz/ml/weka/ 47

used to annotate different examples (these count as different senses for the inference engine); and (3) certain translations occur very infrequently (just 1 or 2 examples in the whole corpus). It is likely that a less strict evaluation regime, such as one which takes account of synonym translations, would re-sult in higher accuracies. It is worth noticing that we experimented with a few relevant parameters for both Aleph and the other learning algorithms. Values that yielded the best average predictive accuracy in the training sets were assumed to be optimal and used to eva-luate the test sets.  4.2 Monolingual task Table 3 shows the average accuracy obtained by Aleph in the monolingual task (Senseval-3 verbs with fine-grained sense distinctions and using the evaluation system provided by Senseval). It also shows the average accuracy of the most frequent sense and accuracies reported on the same set of verbs by the best systems submitted by the sites which participated in this task. Syntalex-3 (Mo-hammad and Pedersen, 2004) is based on an en-semble of bagged decision trees with narrow context part-of-speech features and bigrams. CLaC1 (Lamjiri et al., 2004) uses a Naive Bayes algorithm with a dynamically adjusted context window around the target word. Finally, MC-WSD (Ciaramita and Johnson, 2004) is a multi-class av-eraged perceptron classifier using syntactic and narrow context features, with one component trained on the data provided by Senseval and other trained on WordNet glosses.   System % Average accuracy Majority sense 0.56 Syntalex-3 0.67 CLaC1 0.67 MC-WSD 0.72 Aleph 0.72 Table 3. Accuracies obtained by Aleph and other approaches in the monolingual task  As we can see in Table 3, results are very encour-aging: even without being particularly customized for this monolingual task, the ILP approach signif-icantly outperforms the majority sense baseline and performs as well as the state-of-the-art system re-porting results for the same set of verbs. As with the multilingual task, the models produced contain a small number of rules (from 6, for verbs with a few examples, to 88) and all knowledge sources are used across different rules and verbs. In general, results from both multilingual and monolingual tasks demonstrate that the hypothesis put forward in Section 1, that ILP’s ability to gen-erate expressive rules which combine and integrate a wide range of knowledge sources is beneficial for WSD systems, is correct.  5 Conclusion We have introduced a new hybrid approach to WSD which uses ILP to combine deep and shallow knowledge sources. ILP induces expressive disam-biguation models which include relations between knowledge sources. It is an interesting approach to learning which has been considered promising for several applications in natural language processing and has been explored for a few of them, namely POS-tagging, grammar acquisition and semantic parsing (Cussens et al., 1997; Mooney, 1997). This paper has demonstrated that ILP also yields good results for WSD, in particular for the disambigua-tion of verbs.  We plan to further evaluate our approach for other sets of words, including other parts-of-speech to allow further comparisons with other approach-es. For example, Dang and Palmer (2005) also use a rich set of features with a traditional learning al-gorithm (maximum entropy). Currently, we are evaluating the role of the WSD models for the 10 verbs of the multilingual task in an English-Portuguese statistical machine translation system. References Eneko Agirre and German Rigau. 1996. Word Sense Disambiguation using Conceptual Density. Proceed-ings of the 15th Conference on Computational Lin-guistics (COLING-96). Copenhagen, pages 16-22. Marine Carpuat, Yihai Shen, Xiaofeng Yu, and Dekai WU. 2006. Toward Integrating Word Sense and Enti-ty Disambiguation into Statistical Machine Transla-tion. Proceedings of the Third International Workshop on Spoken Language Translation,. Kyoto, pages 37-44. Massimiliano Ciaramita and Mark Johnson. 2004. Mul-ti-component Word Sense Disambiguation. Proceed-ings of Senseval-3: 3rd International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, Barcelona, pages 97-100. 48

James Cussens, David Page, Stephen Muggleton, and Ashwin Srinivasan. 1997. Using Inductive Logic Programming for Natural Language Processing. Workshop Notes on Empirical Learning of Natural Language Tasks, Prague, pages 25-34. Hoa T. Dang and Martha Palmer. 2005. The Role of Semantic Roles in Disambiguating Verb Senses. Proceedings of the 43rd Meeting of the Association for Computational Linguistics (ACL-05), Ann Arbor, pages 42–49. Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press, Massachusetts.  W. John Hutchins and Harold L. Somers. 1992. An In-troduction to Machine Translation. Academic Press, Great Britain. Abolfazl K. Lamjiri, Osama El Demerdash, Leila Kos-seim. 2004. Simple features for statistical Word Sense Disambiguation. Proceedings of Senseval-3: 3rd International Workshop on the Evaluation of Sys-tems for the Semantic Analysis of Text, Barcelona, pages 133-136. Michael Lesk. 1986. Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone. ACM SIGDOC Conference, Toronto, pages 24-26. Dekang Lin. 1993. Principle based parsing without overgeneration. Proceedings of the 31st Meeting of the Association for Computational Linguistics (ACL-93), Columbus, pages 112-120. Rada Mihalcea, Timothy Chklovski and Adam Kilga-riff. 2004. The Senseval-3 English Lexical Sample Task. Proceedings of Senseval-3: 3rd International Workshop on the Evaluation of Systems for Semantic Analysis of Text, Barcelona, pages 25-28. Saif Mohammad and Ted Pedersen. 2004. Complemen-tarity of Lexical and Simple Syntactic Features: The SyntaLex Approach to Senseval-3. Proceedings of Senseval-3: 3rd International Workshop on the Eval-uation of Systems for the Semantic Analysis of Text, Barcelona, pages 159-162. Raymond J. Mooney. 1997. Inductive Logic Program-ming for Natural Language Processing. Proceedings of the 6th International Workshop on ILP, LNAI 1314, Stockolm, pages 3-24. Stephen Muggleton. 1991. Inductive Logic Program-ming. New Generation Computing, 8(4):295-318. Stephen Muggleton. 1995. Inverse Entailment and Pro-gol. New Generation Computing, 13:245-286.  Hwee T. Ng and Hian B. Lee. 1996. Integrating mul-tiple knowledge sources to disambiguate word sense: an exemplar-based approach. Proceedings of the 34th Meeting of the Association for Computational Linguistics (ACL-96), Santa Cruz, CA, pages 40-47. Paul Procter (editor). 1978. Longman Dictionary of Contemporary English. Longman Group, Essex. Adwait Ratnaparkhi. 1996. A Maximum Entropy Part-Of-Speech Tagger. Proceedings of the Conference on Empirical Methods in Natural Language Processing, New Jersey, pages 133-142. Phillip Resnik and David Yarowsky. 1997. A Perspec-tive on Word Sense Disambiguation Methods and their Evaluating. Proceedings of the ACL-SIGLEX Workshop Tagging Texts with Lexical Semantics: Why, What and How?, Washington. Hinrich Schütze. 1998. Automatic Word Sense Discrim-ination. Computational Linguistics, 24(1): 97-123. Lucia Specia, Maria G.V. Nunes, and Mark Stevenson. 2005. Exploiting Parallel Texts to Produce a Multilingual Sense Tagged Corpus for Word Sense Disambiguation. Proceedings of the Conference on Recent Advances on Natural Language Processing (RANLP-2005), Borovets, pages 525-531. Lucia Specia. 2006. A Hybrid Relational Approach for WSD - First Results. Proceedings of the COLING/ACL 06 Student Research Workshop, Syd-ney, pages 55-60.  Ashwin Srinivasan. 2000. The Aleph Manual. Technical Report. Computing Laboratory, Oxford University. Mark Stevenson and Yorick Wilks. 2001. The Interaction of Knowledge Sources for Word Sense Disambiguation. Computational Linguistics, 27(3):321-349. Yorick Wilks and Mark Stevenson. 1998. The Grammar of Sense: Using Part-of-speech Tags as a First Step in Semantic Disambiguation. Journal of Natural Lan-guage Engineering, 4(1):1-9 David Yarowsky. 1995. Unsupervised Word-Sense Dis-ambiguation Rivaling Supervised Methods. Proceedings of the 33rd Meeting of the Association for Computational Linguistics (ACL-05), Cambridge, MA, pages 189-196.   Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 49–56,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

49

DomainAdaptationwithActiveLearningforWordSenseDisambiguationYeeSengChanandHweeTouNgDepartmentofComputerScienceNationalUniversityofSingapore3ScienceDrive2,Singapore117543{chanys,nght}@comp.nus.edu.sgAbstractWhenawordsensedisambiguation(WSD)systemistrainedononedomainbutap-pliedtoadifferentdomain,adropinac-curacyisfrequentlyobserved.Thishigh-lightstheimportanceofdomainadaptationforwordsensedisambiguation.Inthispa-per,weﬁrstshowthatanactivelearningap-proachcanbesuccessfullyusedtoperformdomainadaptationofWSDsystems.Then,byusingthepredominantsensepredictedbyexpectation-maximization(EM)andadopt-ingacount-mergingtechnique,weimprovetheeffectivenessoftheoriginaladaptationprocessachievedbythebasicactivelearn-ingapproach.1IntroductionInnaturallanguage,awordoftenassumesdifferentmeanings,andthetaskofdeterminingthecorrectmeaning,orsense,ofawordindifferentcontextsisknownaswordsensedisambiguation(WSD).Todate,thebestperformingsystemsinWSDuseacorpus-based,supervisedlearningapproach.Withthisapproach,onewouldneedtocollectatextcor-pus,inwhicheachambiguouswordoccurrenceisﬁrsttaggedwithitscorrectsensetoserveastrainingdata.TherelianceofsupervisedWSDsystemsonan-notatedcorpusraisestheimportantissueofdo-maindependence.Toinvestigatethis,Escuderoetal.(2000)andMartinezandAgirre(2000)con-ductedexperimentsusingtheDSOcorpus,whichcontainssentencesfromtwodifferentcorpora,namelyBrownCorpus(BC)andWallStreetJour-nal(WSJ).TheyfoundthattrainingaWSDsystemononepart(BCorWSJ)oftheDSOcorpus,andapplyingittotheother,canresultinanaccuracydropofmorethan10%,highlightingtheneedtoper-formdomainadaptationofWSDsystemstonewdo-mains.Escuderoetal.(2000)pointedoutthatoneofthereasonsforthedropinaccuracyisthedif-ferenceinsensepriors(i.e.,theproportionsofthedifferentsensesofaword)betweenBCandWSJ.Whentheauthorsassumedtheyknewthesensepri-orsofeachwordinBCandWSJ,andadjustedthesetwodatasetssuchthattheproportionsofthediffer-entsensesofeachwordwerethesamebetweenBCandWSJ,accuracyimprovedby9%.Inthispaper,weexploredomainadaptationofWSDsystems,byaddingtrainingexamplesfromthenewdomainasadditionaltrainingdatatoaWSDsystem.ToreducetheeffortrequiredtoadaptaWSDsystemtoanewdomain,weemployanac-tivelearningstrategy(LewisandGale,1994)tose-lectexamplestoannotatefromthenewdomainofinterest.Toourknowledge,ourworkistheﬁrsttouseactivelearningfordomainadaptationforWSD.AsimilarworkistherecentresearchbyChenetal.(2006),whereactivelearningwasusedsuccessfullytoreducetheannotationeffortforWSDof5Englishverbsusingcoarse-grainedevaluation.Inthatwork,theauthorsonlyusedactivelearningtoreducetheannotationeffortanddidnotdealwiththeportingofaWSDsystemtoanewdomain.Domainadaptationisnecessarywhenthetrain-ingandtargetdomainsaredifferent.Inthispaper,50

weperformdomainadaptationforWSDofasetofnounsusingﬁne-grainedevaluation.Thecontribu-tionofourworkisnotonlyinshowingthatactivelearningcanbesuccessfullyemployedtoreducetheannotationeffortrequiredfordomainadaptationinaﬁne-grainedWSDsetting.Moreimportantly,ourmainfocusandcontributionisinshowinghowwecanimprovetheeffectivenessofabasicactivelearn-ingapproachwhenitisusedfordomainadaptation.Inparticular,weexploretheissueofdifferentsensepriorsacrossdifferentdomains.Usingthesensepriorsestimatedbyexpectation-maximization(EM),thepredominantsenseinthenewdomainispre-dicted.Usingthispredictedpredominantsenseandadoptingacount-mergingtechnique,weimprovetheeffectivenessoftheadaptationprocess.Inthenextsection,wediscussthechoiceofcor-pusandnounsusedinourexperiments.Wethenintroduceactivelearningfordomainadaptation,fol-lowedbycount-merging.Next,wedescribeanEM-basedalgorithmtoestimatethesensepriorsinthenewdomain.Performanceofdomainadaptationus-ingactivelearningandcount-mergingisthenpre-sented.Next,weshowthatbyusingthepredom-inantsenseofthetargetdomainaspredictedbytheEM-basedalgorithm,weimprovetheeffective-nessoftheadaptationprocess.Ourempiricalresultsshowthatforthesetofnounswhichhavedifferentpredominantsensesbetweenthetrainingandtargetdomains,weareabletoreducetheannotationeffortby71%.2ExperimentalSettingInthissection,wediscussthemotivationsforchoos-ingtheparticularcorpusandthesetofnounstocon-ductourdomainadaptationexperiments.2.1ChoiceofCorpusTheDSOcorpus(NgandLee,1996)contains192,800annotatedexamplesfor121nounsand70verbs,drawnfromBCandWSJ.WhiletheBCisbuiltasabalancedcorpus,containingtextsinvar-iouscategoriessuchasreligion,politics,humani-ties,ﬁction,etc,theWSJcorpusconsistsprimarilyofbusinessandﬁnancialnews.Exploitingthedif-ferenceincoveragebetweenthesetwocorpora,Es-cuderoetal.(2000)separatedtheDSOcorpusintoitsBCandWSJpartstoinvestigatethedomainde-pendenceofseveralWSDalgorithms.Followingthesetupof(Escuderoetal.,2000),wesimilarlymadeuseoftheDSOcorpustoperformourexperimentsondomainadaptation.Amongthefewcurrentlyavailablemanuallysense-annotatedcorporaforWSD,theSEMCOR(SC)corpus(Milleretal.,1994)isthemostwidelyused.SEMCORisasubsetofBCwhichissense-annotated.SinceBCisabalancedcorpus,andsinceperformingadaptationfromageneralcorpustoamorespeciﬁccorpusisanaturalscenario,wefocusonadaptingaWSDsystemtrainedonBCtoWSJinthispaper.Henceforth,out-of-domaindatawillre-fertoBCexamples,andin-domaindatawillrefertoWSJexamples.2.2ChoiceofNounsTheWordNetDomainsresource(MagniniandCavaglia,2000)assignsdomainlabelstosynsetsinWordNet.SincethefocusoftheWSJcorpusisonbusinessandﬁnancialnews,wecanmakeuseofWordNetDomainstoselectthesetofnounshavingatleastonesynsetlabeledwithabusinessorﬁnancerelateddomainlabel.Thisissimilartotheapproachtakenin(Koelingetal.,2005)wheretheyfocusondeterminingthepredominantsenseofwordsincor-poradrawnfromﬁnanceversussportsdomains.1Hence,weselectthesubsetofDSOnounsthathaveatleastonesynsetlabeledwithanyofthesedomainlabels:commerce,enterprise,money,ﬁnance,bank-ing,andeconomy.Thisgivesasetof21nouns:book,business,center,community,condition,ﬁeld,ﬁgure,house,interest,land,line,money,need,num-ber,order,part,power,society,term,use,value.2Foreachnoun,alltheBCexamplesareusedasout-of-domaintrainingdata.One-thirdoftheWSJexamplesforeachnounaresetasideasevaluation1NotehoweverthatthecoverageoftheWordNetDomainsresourceisnotcomprehensive,asabout31%ofthesynsetsaresimplylabeledwith“factotum”,indicatingthatthesynsetdoesnotbelongtoaspeciﬁcdomain.225nounshaveatleastonesynsetlabeledwiththelisteddomainlabels.Inourexperiments,4outofthese25nounshaveanaccuracyofmorethan90%beforeadaptation(i.e.,trainingonjusttheBCexamples)andaccuracyimprovementislessthan1%afteralltheavailableWSJadaptationexamplesareaddedasadditionaltrainingdata.Toobtainaclearerpictureoftheadaptationprocess,wediscardthese4nouns,leavingasetof21nouns.51

DatasetNo.ofMFSNo.ofNo.ofsensesacc.trainingadaptationBCWSJ(%)examplesexamples21nouns6.76.861.13104069nouns7.98.665.8276416Table1:TheaveragenumberofsensesinBCandWSJ,averageMFSaccuracy,averagenumberofBCtraining,andWSJadaptationexamplespernoun.data,andtherestoftheWSJexamplesaredesig-natedasin-domainadaptationdata.Therow21nounsinTable1showssomeinformationaboutthese21nouns.Forinstance,thesenounshaveanaverageof6.7sensesinBCand6.8sensesinWSJ.Thisisslightlyhigherthanthe5.8sensesperverbin(Chenetal.,2006),wheretheexperimentswerecon-ductedusingcoarse-grainedevaluation.Assumingwehaveaccesstoan“oracle”whichdeterminesthepredominantsense,ormostfrequentsense(MFS),ofeachnouninourWSJtestdataperfectly,andweassignthismostfrequentsensetoeachnouninthetestdata,wewillhaveachievedanaccuracyof61.1%asshowninthecolumnMFSaccuracyofTa-ble1.Finally,wenotethatwehaveanaverageof310BCtrainingexamplesand406WSJadaptationexamplespernoun.3ActiveLearningForourexperiments,weusenaiveBayesasthelearningalgorithm.Theknowledgesourcesweuseincludeparts-of-speech,localcollocations,andsur-roundingwords.Theseknowledgesourceswereef-fectivelyusedtobuildastate-of-the-artWSDpro-graminoneofourpriorwork(LeeandNg,2002).InperformingWSDwithanaiveBayesclassiﬁer,thesensesassignedtoanexamplewithfeaturesf1,...,fnischosensoastomaximize:p(s)nYj=1p(fj|s)Inourdomainadaptationstudy,westartwithaWSDsystembuiltusingtrainingexamplesdrawnfromBC.Wetheninvestigatetheutilityofaddingadditionalin-domaintrainingdatafromWSJ.Inthebaselineapproach,theadditionalWSJexamplesarerandomlyselected.Withactivelearning(LewisandGale,1994),weuseuncertaintysamplingasshownDT←thesetofBCtrainingexamplesDA←thesetofuntaggedWSJadaptationexamplesΓ←WSDsystemtrainedonDTrepeatpmin←∞foreachd∈DAdobs←wordsensepredictionfordusingΓp←conﬁdenceofpredictionbsifp<pminthenpmin←p,dmin←dendendDA←DA−dminprovidecorrectsensesfordminandadddmintoDTΓ←WSDsystemtrainedonnewDTendFigure1:ActivelearninginFigure1.Ineachiteration,wetrainaWSDsys-temontheavailabletrainingdataandapplyitontheWSJadaptationexamples.AmongtheseWSJex-amples,theexamplepredictedwiththelowestcon-ﬁdenceisselectedandremovedfromtheadaptationdata.Thecorrectlabelisthensuppliedforthisex-ampleanditisaddedtothetrainingdata.Notethatintheexperimentsreportedinthispa-per,alltheadaptationexamplesarealreadypre-annotatedbeforetheexperimentsstart,sincealltheWSJadaptationexamplescomefromtheDSOcorpuswhichhavealreadybeensense-annotated.Hence,theannotationofanexampleneededduringeachadaptationiterationissimulatedbyperformingalookupwithoutanymanualannotation.4Count-mergingWealsoemployatechniqueknownascount-merginginourdomainadaptationstudy.Count-mergingassignsdifferentweightstodifferentex-amplestobetterreﬂecttheirrelativeimportance.RoarkandBacchiani(2003)showedthatweightedcount-mergingisaspecialcaseofmaximumapos-teriori(MAP)estimation,andsuccessfullyuseditforprobabilisticcontext-freegrammardomainadap-tation(RoarkandBacchiani,2003)andlanguagemodeladaptation(BacchianiandRoark,2003).Count-mergingcanberegardedasscalingofcountsobtainedfromdifferentdatasets.Weletecdenotethecountsfromout-of-domaintrainingdata,¯cdenotethecountsfromin-domainadapta-tiondata,andbpdenotetheprobabilityestimateby52

count-merging.Wecanscaletheout-of-domainandin-domaincountswithdifferentfactors,orjustuseasingleweightparameterβ:bp(fj|si)=ec(fj,si)+β¯c(fj,si)ec(si)+β¯c(si)(1)Similarly,bp(si)=ec(si)+β¯c(si)ec+β¯c(2)Obtaininganoptimumvalueforβisnotthefocusofthiswork.Instead,weareinterestedtoseeifas-signingahigherweighttothein-domainWSJadap-tationexamples,ascomparedtotheout-of-domainBCexamples,willimprovetheadaptationprocess.Hence,wejustuseaβvalueof3inourexperimentsinvolvingcount-merging.5EstimatingSensePriorsInthissection,wedescribeanEM-basedalgorithmthatwasintroducedbySaerensetal.(2002),whichcanbeusedtoestimatethesensepriors,oraprioriprobabilitiesofthedifferentsensesinanewdataset.Wehaverecentlyshownthatthisalgorithmiseffec-tiveinestimatingthesensepriorsofasetofnouns(ChanandNg,2005).Mostofthissectionisbasedon(Saerensetal.,2002).AssumewehaveasetoflabeleddataDLwithnclassesandasetofNindependentinstances(x1,...,xN)fromanewdataset.ThelikelihoodoftheseNinstancescanbedeﬁnedas:L(x1,...,xN)=NYk=1p(xk)=NYk=1"nXi=1p(xk,ωi)#=NYk=1"nXi=1p(xk|ωi)p(ωi)#(3)Assumingthewithin-classdensitiesp(xk|ωi),i.e.,theprobabilitiesofobservingxkgiventheclassωi,donotchangefromthetrainingsetDLtothenewdataset,wecandeﬁne:p(xk|ωi)=pL(xk|ωi).Todeterminetheaprioriprobabilityestimatesbp(ωi)ofthenewdatasetthatwillmaximizethelikelihoodof(3)withrespecttop(ωi),wecanapplytheiterativeprocedureoftheEMalgorithm.Ineffect,throughmaximizingthelikelihoodof(3),weobtaintheaprioriprobabilityestimatesasaby-product.Letusnowdeﬁnesomenotations.Whenweap-plyaclassiﬁertrainedonDLonaninstancexkdrawnfromthenewdatasetDU,wegetbpL(ωi|xk),whichwedeﬁneastheprobabilityofinstancexkbeingclassiﬁedasclassωibytheclassiﬁertrainedonDL.Further,letusdeﬁnebpL(ωi)astheapri-oriprobabilityofclassωiinDL.Thiscanbeesti-matedbytheclassfrequencyofωiinDL.Wealsodeﬁnebp(s)(ωi)andbp(s)(ωi|xk)asestimatesofthenewaprioriandaposterioriprobabilitiesatstepsoftheiterativeEMprocedure.Assumingweinitial-izebp(0)(ωi)=bpL(ωi),thenforeachinstancexkinDUandeachclassωi,theEMalgorithmprovidesthefollowingiterativesteps:bp(s)(ωi|xk)=bpL(ωi|xk)bp(s)(ωi)bpL(ωi)Pnj=1bpL(ωj|xk)bp(s)(ωj)bpL(ωj)(4)bp(s+1)(ωi)=1NNXk=1bp(s)(ωi|xk)(5)whereEquation(4)representstheexpectationE-step,Equation(5)representsthemaximizationM-step,andNrepresentsthenumberofinstancesinDU.NotethattheprobabilitiesbpL(ωi|xk)andbpL(ωi)inEquation(4)willstaythesamethrough-outtheiterationsforeachparticularinstancexkandclassωi.Thenewaposterioriprobabilitiesbp(s)(ωi|xk)atstepsinEquation(4)aresimplytheaposterioriprobabilitiesintheconditionsofthela-beleddata,bpL(ωi|xk),weightedbytheratioofthenewpriorsbp(s)(ωi)totheoldpriorsbpL(ωi).Thede-nominatorinEquation(4)issimplyanormalizingfactor.Theaposterioribp(s)(ωi|xk)andaprioriproba-bilitiesbp(s)(ωi)arere-estimatedsequentiallydur-ingeachiterationsforeachnewinstancexkandeachclassωi,untiltheconvergenceoftheestimatedprobabilitiesbp(s)(ωi),whichwillbeourestimatedsensepriors.Thisiterativeprocedurewillincreasethelikelihoodof(3)ateachstep.6ExperimentalResultsForeachadaptationexperiment,westartoffwithaclassiﬁerbuiltfromaninitialtrainingsetconsisting53

 52 54 56 58 60 62 64 66 68 70 72 74 76 0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100WSD Accuracy (%)Percentage of adaptation examples added (%)a-cara-truePriorFigure2:Adaptationprocessforall21nouns.oftheBCtrainingexamples.Ateachadaptationiter-ation,WSJadaptationexamplesareselectedoneatatimeandaddedtothetrainingset.Theadaptationprocesscontinuesuntilalltheadaptationexamplesareadded.Classiﬁcationaccuraciesaveragedover3randomtrialsontheWSJtestexamplesateachiterationarecalculated.SincethenumberofWSJadaptationexamplesdiffersforeachofthe21nouns,thelearningcurveswewillshowinthevariousﬁg-uresareplottedintermsofdifferentpercentageofadaptationexamplesadded,varyingfrom0to100percentinstepsof1percent.Toobtainthesecurves,weﬁrstcalculateforeachnoun,theWSDaccuracywhendifferentpercentagesofadaptationexamplesareadded.Then,foreachpercentage,wecalculatethemacro-averageWSDaccuracyoverallthenounstoobtainasinglelearningcurverepresentingallthenouns.6.1UtilityofActiveLearningandCount-mergingInFigure2,thecurverrepresentstheadaptationprocessofthebaselineapproach,whereadditionalWSJexamplesarerandomlyselectedduringeachadaptationiteration.Theadaptationprocessusingactivelearningisrepresentedbythecurvea,whileapplyingcount-mergingwithactivelearningisrep-resentedbythecurvea-c.Notethatrandomselec-tionrachievesitshighestWSDaccuracyafteralltheadaptationexamplesareadded.Toreachthesameaccuracy,theaapproachrequirestheadditionofonly57%ofadaptationexamples.Thea-cap-proachisevenmoreeffectiveandrequiresonly42%ofadaptationexamples.Thisdemonstratestheef-fectivenessofcount-merginginfurtherreducingtheannotationeffort,whencomparedtousingonlyac-tivelearning.ToreachtheMFSaccuracyof61.1%asshownearlierinTable1,a-crequiresjust4%oftheadaptationexamples.Todeterminetheutilityoftheout-of-domainBCexamples,wehavealsoconductedthreeactivelearn-ingrunsusingonlyWSJadaptationexamples.Us-ing10%,20%,and30%ofWSJadaptationexam-plestobuildaclassiﬁer,theaccuracyoftheserunsislowerthantheactivelearningacurveandpairedt-testsshowthatthedifferenceisstatisticallysignif-icantatthelevelofsigniﬁcance0.01.6.2UsingSensePriorsInformationAsmentionedinsection1,researchin(Escuderoetal.,2000)notedanimprovementinaccuracywhentheyadjustedtheBCandWSJdatasetssuchthattheproportionsofthedifferentsensesofeachwordwerethesamebetweenBCandWSJ.Wecansimi-larlychooseBCexamplessuchthatthesensepriorsintheBCtrainingdataadheretothesensepriorsintheWSJevaluationdata.Togaugetheeffectivenessofthisapproach,weﬁrstassumethatweknowthetruesensepriorsofeachnounintheWSJevalua-tiondata.WethengatherBCtrainingexamplesforanountoadhereasmuchaspossibletothesensepriorsinWSJ.AssumesensesiisthepredominantsenseintheWSJevaluationdata,sihasasensepriorofpiintheWSJdataandhasniBCtrainingexam-ples.Takingniexamplestorepresentasensepriorofpi,weproportionallydeterminethenumberofBCexamplestogatherforothersensessaccordingtotheirrespectivesensepriorsinWSJ.Iftherearein-sufﬁcienttrainingexamplesinBCforsomesenses,whateveravailableexamplesofsareused.Thisapproachgivesanaverageof195BCtrain-ingexamplesforthe21nouns.Withthisnewsetoftrainingexamples,weperformadaptationusingactivelearningandobtainthea-truePriorcurveinFigure2.Thea-truePriorcurveshowsthatbyen-suringthatthesensepriorsintheBCtrainingdataadhereasmuchaspossibletothesensepriorsintheWSJdata,westartoffwithahigherWSDaccuracy.However,theperformanceisnodifferentfromthea54

curveafter35%ofadaptationexamplesareadded.ApossiblereasonmightbethatbystrictlyadheringtothesensepriorsintheWSJdata,wehaveremovedtoomanyBCtrainingexamples,fromanaverageof310examplespernounasshowninTable1,toanaverageof195examples.6.3UsingPredominantSenseInformationResearchbyMcCarthyetal.(2004)andKoelingetal.(2005)pointedoutthatachangeofpredominantsenseisoftenindicativeofachangeindomain.Forexample,thepredominantsenseofthenouninterestintheBCpartoftheDSOcorpushasthemeaning“asenseofconcernwithandcuriosityaboutsome-oneorsomething”.IntheWSJpartoftheDSOcor-pus,thenouninteresthasadifferentpredominantsensewiththemeaning“aﬁxedchargeforborrow-ingmoney”,whichisreﬂectiveofthebusinessandﬁnancefocusoftheWSJcorpus.InsteadofrestrictingtheBCtrainingdatatoad-herestrictlytothesensepriorsinWSJ,anotheralter-nativeisjusttoensurethatthepredominantsenseinBCisthesameasthatofWSJ.Outofthe21nouns,12nounshavethesamepredominantsenseinbothBCandWSJ.Theremaining9nounsthathavedif-ferentpredominantsensesintheBCandWSJdataare:center,ﬁeld,ﬁgure,interest,line,need,order,term,value.Therow9nounsinTable1givessomeinformationforthissetof9nouns.Togaugetheutilityofthisapproach,weconductexperimentsonthesenounsbyﬁrstassumingthatweknowthetruepredominantsenseintheWSJdata.AssumethattheWSJpredominantsenseofanounissiandsihasniexamplesintheBCdata.WethengatherBCexam-plesforanountoadheretothisWSJpredominantsense,bygatheringonlyuptoniBCexamplesforeachsenseofthisnoun.Thisapproachgivesanav-erageof190BCexamplesforthe9nouns.Thisishigherthananaverageof83BCexamplesforthese9nounsifBCexamplesareselectedtofollowthesensepriorsofWSJevaluationdataasdescribedinthelastsubsection6.2.Forthese9nouns,theaverageKL-divergencebe-tweenthesensepriorsoftheoriginalBCdataandWSJevaluationdatais0.81.Thisdropsto0.51af-terensuringthatthepredominantsenseinBCisthesameasthatofWSJ,conﬁrmingthatthesensepriorsinthenewlygatheredBCdatamorecloselyfollow 44 46 48 50 52 54 56 58 60 62 64 66 68 70 72 74 76 78 80 82 0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100WSD Accuracy (%)Percentage of adaptation examples added (%)a-truePriora-truePredaFigure3:Usingtruepredominantsenseforthe9nouns.thesensepriorsinWSJ.Usingthisnewsetoftrain-ingexamples,weperformdomainadaptationusingactivelearningtoobtainthecurvea-truePredinFig-ure3.Forcomparison,wealsoplotthecurvesaanda-truePriorforthissetof9nounsinFigure3.ResultsinFigure3showthata-truePredstartsoffatahigheraccuracyandperformsconsistentlybet-terthantheacurve.Incontrast,thougha-truePriorstartsatahighaccuracy,itsperformanceislowerthana-truePredandaafter50%ofadaptationex-amplesareadded.Theapproachrepresentedbya-truePredisacompromisebetweenensuringthatthesensepriorsinthetrainingdatafollowascloselyaspossiblethesensepriorsintheevaluationdata,whileretainingenoughtrainingexamples.Thesere-sultshighlighttheimportanceofstrikingabalancebetweenthesetwogoals.In(McCarthyetal.,2004),amethodwaspre-sentedtodeterminethepredominantsenseofawordinacorpus.However,in(ChanandNg,2005),weshowedthatinasupervisedsettingwhereonehasaccesstosomeannotatedtrainingdata,theEM-basedmethodinsection5estimatesthesensepriorsmoreeffectivelythanthemethoddescribedin(Mc-Carthyetal.,2004).Hence,weusetheEM-basedalgorithmtoestimatethesensepriorsintheWSJevaluationdataforeachofthe21nouns.Thesensewiththehighestestimatedsenseprioristakenasthepredominantsenseofthenoun.Forthesetof12nounswherethepredominant55

 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100WSD Accuracy (%)Percentage of adaptation examples added (%)a-c-estPreda-truePreda-estPredarFigure4:Usingestimatedpredominantsenseforthe9nouns.Accuracy%adaptationexamplesneededraa-estPreda-c-estPred50%:61.187(0.88)5(0.63)4(0.50)60%:64.5109(0.90)7(0.70)5(0.50)70%:68.01512(0.80)9(0.60)6(0.40)80%:71.52316(0.70)12(0.52)9(0.39)90%:74.94624(0.52)21(0.46)15(0.33)100%:78.410051(0.51)38(0.38)29(0.29)Table2:Annotationsavingsandpercentageofadap-tationexamplesneededtoreachvariousaccuracies.senseremainsunchangedbetweenBCandWSJ,theEM-basedalgorithmisabletopredictthatthepre-dominantsenseremainsunchangedforall12nouns.Hence,wewillfocusonthe9nounswhichhavedifferentpredominantsensesbetweenBCandWSJforourremainingadaptationexperiments.Forthese9nouns,theEM-basedalgorithmcorrectlypredictstheWSJpredominantsensefor6nouns.Hence,thealgorithmisabletopredictthecorrectpredominantsensefor18outof21nounsoverall,representinganaccuracyof86%.Figure4plotsthecurvea-estPred,whichissimi-lartoa-truePred,exceptthatthepredominantsenseisnowestimatedbytheEM-basedalgorithm.Em-ployingcount-mergingwitha-estPredproducesthecurvea-c-estPred.Forcomparison,thecurvesr,a,anda-truePredarealsoplotted.Theresultsshowthata-estPredperformsconsistentlybetterthana,anda-c-estPredinturnperformsbetterthana-estPred.Hence,employingthepredictedpredom-inantsenseandcount-merging,wefurtherimprovetheeffectivenessoftheactivelearning-basedadap-tationprocess.WithreferencetoFigure4,theWSDaccuraciesoftherandacurvesbeforeandafteradaptationare43.7%and78.4%respectively.Startingfromthemid-point61.1%accuracy,whichrepresentsa50%accuracyincreasefrom43.7%,weshowinTable2thepercentageofadaptationexamplesre-quiredbythevariousapproachestoreachcertainlevelsofWSDaccuracies.Forinstance,toreachtheﬁnalaccuracyof78.4%,r,a,a-estPred,anda-c-estPredrequiretheadditionof100%,51%,38%,and29%adaptationexamplesrespectively.Thenumbersinbracketsgivetheratioofadaptationex-amplesneededbya,a-estPred,anda-c-estPredver-susrandomselectionr.Forinstance,toreachaWSDaccuracyof78.4%,a-c-estPredneedsonly29%adaptationexamples,representingaratioof0.29andanannotationsavingof71%.Notethatthisrepresentsamoreeffectiveadaptationprocessthanthebasicactivelearningaapproach,whichrequires51%adaptationexamples.Hence,besidesshowingthatactivelearningcanbeusedtoreducetheannota-tioneffortrequiredfordomainadaptation,wehavefurtherimprovedtheeffectivenessoftheadaptationprocessbyusingthepredictedpredominantsenseofthenewdomainandadoptingthecount-mergingtechnique.7RelatedWorkInapplyingactivelearningfordomainadapta-tion,Zhangetal.(2003)presentedworkonsen-tenceboundarydetectionusinggeneralizedWin-now,whileTuretal.(2004)performedlanguagemodeladaptationofautomaticspeechrecognitionsystems.Inbothpapers,out-of-domainandin-domaindataweresimplymixedtogetherwithoutMAPestimationsuchascount-merging.ForWSD,Fujiietal.(1998)usedselectivesamplingforaJapaneselanguageWSDsystem,Chenetal.(2006)usedactivelearningfor5verbsusingcoarse-grainedevaluation,andH.T.Dang(2004)employedactivelearningforanothersetof5verbs.However,theirworkonlyinvestigatedtheuseofactivelearningtoreducetheannotationeffortnecessaryforWSD,but56

didnotdealwiththeportingofaWSDsystemtoadifferentdomain.Escuderoetal.(2000)usedtheDSOcorpustohighlighttheimportanceoftheissueofdomaindependenceofWSDsystems,butdidnotproposemethodssuchasactivelearningorcount-mergingtoaddressthespeciﬁcproblemofhowtoperformdomainadaptationforWSD.8ConclusionDomainadaptationisimportanttoensurethegen-eralapplicabilityofWSDsystemsacrossdifferentdomains.Inthispaper,wehaveshownthatactivelearningiseffectiveinreducingtheannotationef-fortrequiredinportingaWSDsystemtoanewdo-main.Also,wehavesuccessfullyusedanEM-basedalgorithmtodetectachangeinpredominantsensebetweenthetrainingandnewdomain.Withthisinformationonthepredominantsenseofthenewdomainandincorporatingcount-merging,wehaveshownthatweareabletoimprovetheeffectivenessoftheoriginaladaptationprocessachievedbythebasicactivelearningapproach.AcknowledgementYeeSengChanissupportedbyaSingaporeMillen-niumFoundationScholarship(refno.SMF-2004-1076).ReferencesM.BacchianiandB.Roark.2003.Unsupervisedlan-guagemodeladaptation.InProc.ofIEEEICASSP03.Y.S.ChanandH.T.Ng.2005.Wordsensedisambigua-tionwithdistributionestimation.InProc.ofIJCAI05.J.Chen,A.Schein,L.Ungar,andM.Palmer.2006.Anempiricalstudyofthebehaviorofactivelearn-ingforwordsensedisambiguation.InProc.ofHLT/NAACL06.H.T.Dang.2004.InvestigationsintotheRoleofLex-icalSemanticsinWordSenseDisambiguation.PhDdissertation,UniversityofPennsylvania.G.Escudero,L.Marquez,andG.Rigau.2000.Anempiricalstudyofthedomaindependenceofsuper-visedwordsensedisambiguationsystems.InProc.ofEMNLP/VLC00.A.Fujii,K.Inui,T.Tokunaga,andH.Tanaka.1998.Selectivesamplingforexample-basedwordsensedis-ambiguation.ComputationalLinguistics,24(4).R.Koeling,D.McCarthy,andJ.Carroll.2005.Domain-speciﬁcsensedistributionsandpredominantsenseac-quisition.InProc.ofJointHLT-EMNLP05.Y.K.LeeandH.T.Ng.2002.Anempiricalevaluationofknowledgesourcesandlearningalgorithmsforwordsensedisambiguation.InProc.ofEMNLP02.D.D.LewisandW.A.Gale.1994.Asequentialalgo-rithmfortrainingtextclassiﬁers.InProc.ofSIGIR94.B.MagniniandG.Cavaglia.2000.IntegratingsubjectﬁeldcodesintoWordNet.InProc.ofLREC-2000.D.MartinezandE.Agirre.2000.Onesensepercollocationandgenre/topicvariations.InProc.ofEMNLP/VLC00.D.McCarthy,R.Koeling,J.Weeds,andJ.Carroll.2004.Findingpredominantwordsensesinuntaggedtext.InProc.ofACL04.G.A.Miller,M.Chodorow,S.Landes,C.Leacock,andR.G.Thomas.1994.Usingasemanticconcordanceforsenseidentiﬁcation.InProc.ofHLT94WorkshoponHumanLanguageTechnology.H.T.NgandH.B.Lee.1996.Integratingmultipleknowledgesourcestodisambiguatewordsense:Anexemplar-basedapproach.InProc.ofACL96.B.RoarkandM.Bacchiani.2003.Supervisedandunsu-pervisedPCFGadaptationtonoveldomains.InProc.ofHLT-NAACL03.M.Saerens,P.Latinne,andC.Decaestecker.2002.Ad-justingtheoutputsofaclassiﬁertonewaprioriprob-abilities:Asimpleprocedure.NeuralComputation,14(1).D.H.Tur,G.Tur,M.Rahim,andG.Riccardi.2004.Unsupervisedandactivelearninginautomaticspeechrecognitionforcallclassiﬁcation.InProc.ofIEEEICASSP04.T.Zhang,F.Damerau,andD.Johnson.2003.Updat-inganNLPsystemtoﬁtnewdomains:anempiricalstudyonthesentencesegmentationproblem.InProc.ofCONLL03.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 57–64,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

57

MakingLexicalOntologiesFunctionalandContext-SensitiveTonyVealeComputerScienceandInformaticsUniversityCollegeDublinIrelandtony.veale@ucd.ieYanfenHaoComputerScienceandInformaticsUniversityCollegeDublinIrelandyanfen.hao@ucd.ieAbstractHumancategorizationisneitherabinarynoracontext-freeprocess.Rather,somecon-ceptsarebetterexamplesofacategorythanothers,whilethecriteriaforcategorymem-bershipmaybesatisﬁedtodifferentdegreesbydifferentconceptsindifferentcontexts.Inlightoftheseempiricalfacts,WordNet’sstaticcategorystructureappearsbothexces-sivelyrigidandundulyfragileforprocess-ingrealtexts.Inthispaperwedescribeasyntagmatic,corpus-basedapproachtore-deﬁningWordNet’scategoriesinafunc-tional,gradableandcontext-sensitivefash-ion.Wedescribehowthediagnosticprop-ertiesforthesedeﬁnitionsareautomati-callyacquiredfromtheweb,andhowtheincreasedﬂexibilityincategorizationthatarisesfromtheseredeﬁnitionsoffersaro-bustaccountofmetaphorcomprehensioninthemoldofGlucksberg’s(2001)the-oryofcategory-inclusion.Furthermore,wedemonstratehowthiscompetencewithﬁgu-rativecategorizationcaneffectivelybegov-ernedbyautomatically-generatedontologi-calconstraints,alsoacquiredfromtheweb.1IntroductionLinguisticvariationacrosscontextsisoftensymp-tomaticofontologicaldifferencesbetweencontexts.Theseobservablevariationscanserveasvaluablecluesnotjusttothespeciﬁcsensesofwordsincon-text(e.g.,seePustejovsky,HanksandRumshisky,2004)buttotheunderlyingontologicalstructureit-self(seeCimiano,HothoandStaab,2005).Themostrevealingvariationsaresyntagmaticinnature,whichistosay,theylookbeyondindividualwordformstolargerpatternsofcontiguoususage(Hanks,2004).Inmostcontexts,thesimilaritybetweenchocolate,say,andanarcoticlikeheroinwillmea-gerlyreﬂectthesimpleontologicalfactthatbotharekindsofsubstances;certainly,taxonomicmeasuresofsimilarityasdiscussedinBudanitskyandHirst(2006)willcapturelittlemorethanthiscommon-ality.However,inacontextinwhichtheaddictivepropertiesofchocolateareverysalient(e.g.,anon-linedietingforum),chocolateismorelikelytobecategorizedasadrugandthusbeconsideredmoresimilartoheroin.Look,forinstance,atthesimi-larwaysinwhichthesewordscanbeused:onecanbe”chocolate-crazed”or”chocolate-addicted”andsuffer”chocolate-induced”symptoms(e.g.,eachoftheseusescanbefoundinthepagesofWikipedia).Inacontextthatgivesrisetotheseexpressions,itisunsurprisingthatchocolateshouldappearaltogethermoresimilartoaharmfulnarcotic.Inthispaperwecomputationallymodelthisideathatlanguageusereﬂectscategorystructure.AsnotedbyDeLeenheeranddeMoor(2005),ontolo-giesarelexicalrepresentationsofconcepts,sowecanexpecttheeffectsofcontextonlanguageusetocloselyreﬂecttheeffectsofcontextonontolog-icalstructure.Anunderstandingofthelinguisticef-fectsofcontext,asexpressedthroughsyntagmaticpatternsofwordusage,shouldleadthereforetothedesignofmoreﬂexiblelexicalontologiesthatnatu-rallyadapttotheircontextsofuse.WordNet(Fell-58

baum,1998)isjustonesuchlexicalontologythatcanbeneﬁtgreatlyfromtheaddedﬂexibilitythatcontext-sensitivitycanbring.Thoughcomprehen-siveinscaleandwidelyused,WordNetsuffersfromanobviousstructuralrigidityinwhichconceptsareeitherentirelywithinacategoryorentirelyoutsideacategory:nogradationofcategorymembershipisallowed,andnocontextualfactorsarebroughttobearoncriteriaformembership.Thus,agunisal-waysaweaponinWordNetwhileanaxeisneverso,despitetheuses(sportingormurderous)towhicheachcanbeput.InsectiontwowedescribeacomputationalframeworkforgivingWordNetsensesafunctional,context-sensitiveform.Thesefunctionalformssi-multaneouslyrepresenti)anintensionaldeﬁnitionforeachwordsense;ii)astructuredquerycapableofretrievinginstancesofthecorrespondingcategoryfromacontext-speciﬁccorpus;andiii)amember-shipfunctionthatassignsgradatedscorestotheseinstancesbasedonavailablesyntagmaticevidence.Insectionthreewedescribehowtheknowledgere-quiredtoautomatethisfunctionalre-deﬁnitionisac-quiredfromthewebandlinkedtoWordNet.Insec-tionfourwedescribehowthesere-deﬁnitionscanproducearobustmodelofmetaphor,beforeweeval-uatethedescriptivesufﬁciencyofthisapproachinsectionﬁve,comparingittotheknowledgealreadyavailablewithinWordNet.Weconcludewithsomeﬁnalremarksinsectionsix.2FunctionalContext-SensitiveCategoriesWetakeawhollytextualviewofcontextandas-sumethatagivencontextcanbeimplicitlycharac-terizedbyarepresentativetextcorpus.Thiscorpuscanbeaslargeasatextarchiveoranencyclopedia(e.g.,thecompletetextofWikipedia),orassmallasasingledocument,asentenceorevenasinglenoun-phrase.Forinstance,themicro-context”alco-holicapple-juice”isenoughtoimplicatethecate-goryLiquor,ratherthanJuice,asasemantichead,while”lovablesnake”canbeenoughofacontexttolocallycategorizeSnakeasakindofPet.Thereisarangeofsyntagmaticpatternsthatonecanexploittogleancategoryinsightsfromatext.Forinstance,the”Xkills”patternisenoughtocategorizeXasakindofKiller,”huntsX”isenoughtocategorizeXasakindofPrey,while”X-covered”,”X-dipped”and”X-frosted”allindicatethatXisakindofCovering.Likewise,”armyofX”suggeststhatacontextviewsXasakindofSoldier,while”barrageofX”suggeststhatXshouldbeseenasakindofProjectile.Weoperationalizethecollocation-typeofadjec-tiveandnounviathefunction(attrADJNOUN),whichreturnsanumberintherange0...1;thisrepresentstheextenttowhichADJisusedtomodifyNOUNinthecontext-deﬁningcorpus.Dice’scoefﬁcient(e.g.,seeCimianoetal.,2005)isusedtoimplementthismeasure.Acontext-sensitivecategorymembershipfunctioncanbedeﬁned,asinthatforFundamentalistinFigure1:(deﬁneFundamentalist.0(arg0)(*(max(%isaarg0Person.0)(%isaarg0Group.0))(min(max(attrpoliticalarg0)(attrreligiousarg0))(max(attrextremearg0)(attrviolentarg0)(attrradicalarg0)))))Figure1.Afunctionalre-deﬁnitionofthecat-egoryFundamentalist.ThefunctionofFigure1takes,asasinglear-gumentarg0,aputativememberofthecategoryFundamentalist.0(notehowthesensetag,0,isusedtoidentifyaspeciﬁcWordNetsenseof”fun-damentalist”),andreturnsamembershipscoreintherange0...1forthisterm.Thisscorereﬂectsthesyntagmaticevidenceforconsideringarg0tobepoliticalorreligious,aswellasextremeorviolentorradical.Thefunction(%isaarg0CAT)returnsavalueof1.0ifsomesenseofarg0isadescendentofCAT(herePerson.0orGroup.0),otherwise0.Thissafeguardsontologicalcoherenceandensuresthatonlykindsofpeopleorgroupscaneverbeconsideredasfundamentalists.TheexampleofFigure1ishand-crafted,butafunctionalformcanbeassignedautomaticallytomanyofthesynsetsinWordNetbyheuristicmeans.59

Forinstance,thoseofFigure2areautomaticallyderivedfromWordNet’smorpho-semanticlinks:(deﬁneFraternity.0(arg0)(*(%simarg0Fraternity.0)(max(attrfraternalarg0)(attrbrotherlyarg0))))(deﬁneOrgasm.0(arg0)(*(%simarg0Orgasm.0)(max(attrclimacticarg0)(attrorgasmicarg0))))Figure2.ExploitingtheWordNetlinksbe-tweennounsandtheiradjectivalforms.Thefunction(%simarg0CAT)reﬂectstheperceivedsimilaritybetweentheputativememberarg0andasynsetCATinWordNet,usingoneofthestandardformulationsdescribedinBudanitskyandHirst(2006).Thus,anykindofgroup(e.g.,agleeclub,aMasoniclodge,orabarbershopquartet)describedinatextas”fraternal”or”brotherly”(bothoccupythesameWordNetsynset)canbeconsideredaFraternitytothecorrespondingdegree,temperedbyitsapriorisimilaritytoaFraternity;likewise,anyclimacticeventcanbecategorizedasanOrgasmtoamoreorlessdegree.Alternately,thefunctionofFigure3isautomat-icallyobtainedforthelexicalconceptEspressobyshallowparsingitsWordNetgloss:”strongblackcoffeebrewedbyforcingsteamunderpressurethroughpowderedcoffeebeans”.(deﬁneEspresso.0(arg0)(*(%simarg0Espresso.0)(min(attrstrongarg0)(attrblackarg0))))Figure3.Afunctionalre-deﬁnitionofthecat-egoryEspressobasedonitsWordNetgloss.Itfollowsthatanysubstance(e.g.,oilortea)describedlocallyas”black”and”strong”withanon-zerotaxonomicsimilaritytocoffeecanbeconsideredakindofEspresso.CombiningthecontentsofWordNet1.6andWordNet2.1,27,732differentglosses(sharedby51,035uniquewordsenses)canbeshallowparsedtoyieldadeﬁnitionofthekindshowninFigure3.Ofthese,4525glossesyieldtwoormorepropertiesthatcanbegivenfunctionalformviaattr.However,onecanquestionwhetherthesefeaturesaresufﬁcient,andmoreimportantly,whethertheyaretrulydiag-nosticofthecategoriestheyareusedtodeﬁne.Inthenextsectionweconsideranothersourceofdiag-nosticproperties,explicitsimilesontheweb,before,insection5,comparingthequalityoftheseproper-tiestothoseavailablefromWordNet.3DiagnosticPropertiesontheWebWeemploytheGooglesearchengineasaretrievalmechanismforacquiringthediagnosticpropertiesofcategoriesfromtheweb,sincetheGoogleAPIanditssupportforthewildcardterm*allowsthisprocesstobefullyautomated.Theguidingintu-itionhereisthatlookingforexplicitsimilesoftheform”XisasPasY”isthesurestwayofﬁndingthemostsalientpropertiesofatermY;withothersyntagmaticpatterns,suchasadjective:nouncollo-cations,onecannotbesurethattheadjectiveiscen-traltothenoun.Sinceweexpectthatexplicitsimileswilltendtoexploitpropertiesthatoccupyanexemplarypointonascale,weﬁrstextractalistofantonymousadjec-tives,suchas”hot”or”cold”,fromWordNet.ForeveryadjectiveADJonthislist,wesendthequery”asADJas*”toGoogleandscantheﬁrst200snip-petsreturnedtoextractdifferentnounvaluesforthewildcard*.FromeachsetofsnippetswecanalsoascertaintherelativefrequenciesofdifferentnounvaluesforADJ.Thecompletesetofnounsextractedinthiswayisthenusedtodriveasecondphaseofthesearch,inwhichthequerytemplate”as*asaNOUN”isusedtoacquiresimilesthatmayhavelainbeyondthe200-snippethorizonoftheoriginalsearch,orthatmayhingeonadjectivesnotincludedontheoriginallist.Together,bothphasescollectawide-rangingseriesofcoresamples(of200hitseach)fromacrosstheweb,yieldingasetof74,704simileinstances(of42,618uniquetypes)relating60

3769differentadjectivesto9286differentnouns3.1PropertyFilteringUnfortunately,manyofthesesimilesarenotsufﬁ-cientlywell-formedtoidentifysalientproperties.Inmanycases,thenounvalueformspartofalargernounphrase:itmaybethemodiﬁerofacompoundnoun(asin”breadlover”),ortheheadofcomplexnounphrase(suchas”gangofthieves”or”woundthatrefusestoheal”).Intheformercase,thecom-poundisusedifitcorrespondstoacompoundterminWordNetandthusconstitutesasinglelexicalunit;ifnot,orifthelattercase,thesimileisrejected.Othersimilesaresimplytoocontextualorunder-speciﬁedtofunctionwellinanullcontext,soifonemustreadtheoriginaldocumenttomakesenseofthesimile,itisrejected.Moresurprisingly,per-haps,asubstantialnumberoftheretrievedsimi-lesareironic,inwhichtheliteralmeaningofthesimileiscontrarytothemeaningdictatedbycom-monsense.Forinstance,”ashairyasabowlingball”(foundonce)isanironicwayofsaying”ashairlessasabowlingball”(alsofoundjustonce).Manyironiescanonlyberecognizedusingworldknowledge,suchas”assoberasaKennedy”and”astannedasanIrishman”.Giventhecreativityinvolvedintheseconstruc-tions,onecannotimagineareliableautomaticﬁl-tertosafelyidentifybona-ﬁdesimiles.Forthisreason,theﬁlteringtaskisperformedbyahumanjudge,whoannotated30,991ofthesesimilein-stances(for12,259uniqueadjective/nounpairings)asnon-ironicandmeaningfulinanullcontext;thesesimilesrelateasetof2635adjectivestoasetof4061differentnouns.Inaddition,thejudgealsoannotated4685simileinstances(of2798types)asironic;thesesimilesrelateasetof936adjectivestoasetof1417nouns.Perhapssurprisingly,ironicpairingsaccountforover13%ofallannotatedsim-ileinstancesandover20%ofallannotatedtypes.3.2LinkingtoWordNetSensesTocreatefunctionalWordNetdeﬁnitionsfromtheseadjective:nounpairings,weﬁrstneedtoidentifytheWordNetsenseofeachnoun.Forinstance,”asstiffasazombie”mightrefereithertoare-animatedcorpseortoanalcoholiccocktail(botharesensesof”zombie”inWordNet,anddrinkscanbe”stiff”too).DisambiguationistrivialfornounswithjustasinglesenseinWordNet.Fornounswithtwoormoreﬁne-grainedsensesthatarealltaxonomicallyclose,suchas”gladiator”(twosenses:aboxerandacombatant),weconsidereachsensetobeasuitabletarget.Insomecases,theWordNetglossforaspar-ticularsensewillliterallymentiontheadjectiveofthesimile,andsothissenseischosen.Inallothercases,weemployastrategyofmutualdisambigua-tiontorelatethenounvehicleineachsimiletoaspe-ciﬁcsenseinWordNet.Twosimiles”asAasN1”and”asAasN2”aremutuallydisambiguatingifN1andN2aresynonymsinWordNet,orifsomesenseofN1isahypernymorhyponymofsomesenseofN2inWordNet.Forinstance,theadjective”scary”isusedtodescribeboththenoun”rattler”andthenoun”rattlesnake”inbona-ﬁde(non-ironic)similes;sincethesenounsshareasense,wecanassumethattheintendedsenseof”rattler”isthatofadanger-oussnakeratherthanachild’stoy.Similarly,theadjective”brittle”isusedtodescribebothsaltinesandcrackers,suggestingthatitisthebreadsenseof”cracker”ratherthanthehacker,ﬁreworkorhillbillysenses(allinWordNet)thatisintended.Theseheuristicsallowustoautomaticallydisam-biguate10,378bona-ﬁdesimiletypes(85%),yield-ingamappingof2124adjectivesto3778differentWordNetsenses.Likewise,77%(or2164)ofthesimiletypesannotatedasironicaredisambiguatedautomatically.AremarkablestabilityisobservedinthealignmentofnounvehiclestoWordNetsenses:100%oftheironicvehiclesalwaysdenotethesamesense,nomattertheadjectiveinvolved,while96%ofbona-ﬁdevehiclesalwaysdenotethesamesense.Thisstabilitysuggeststwoconclusions:thedis-ambiguationprocessisconsistentandaccurate;butmoreintriguingly,onlyonecoarse-grainedsenseofanywordislikelytobesufﬁcientlyexemplaryofsomepropertytobeusefulinasimile.4FromSimilestoCategoryFunctionsAsnotedinsection3,theﬁlteredwebdatayields12,259bona-ﬁdesimilesdescribing4061targetnounsintermsof2635differentadjectivalprop-erties.Word-sensedisambiguationallows3778synsetsinWordNettobegivenafunctionalre-deﬁnitionintermsof2124diagnosticproperties,as61

inthedeﬁnitionofGladiatorinFigure4:(deﬁneGladiator.0(arg0)(*(%isaarg0Person.0)(*(%simarg0Gladiator.0)(combine(attrstrongarg0)(attrviolentarg0)(attrmanlyarg0)))))Figure4.Aweb-baseddeﬁnitionofGladiator.Sincewecannotascertainfromthewebdatawhichpropertiesarenecessaryandwhicharecollectivelysufﬁcient,weusethefunctioncombinetoaggregatetheavailableevidence.Thisfunctionimplementsana¨ıveprobabilisticor,inwhicheachpieceofsyntagmaticevidenceisnaivelyassumedtobeindependent,asfollows:(combinee0e1)=e0+e1(1−e0)(combinee0e1...en)=(combinee0(combinee1...en))Thus,anycombatantorcompetitor(suchasasportsman)thatisdescribedasstrong,violentormanlyinacorpuscanbecategorizedasaGladiatorinthatcontext;themorepropertiesthathold,andthegreaterthedegreetowhichtheyhold,thegreaterthemembershipscorethatisassigned.Thesourceofthehardtaxonomicconstraint(%isaarg0Person.0)isexplainedinthenextsec-tion.Fornow,notehowtheuseof%siminthefunctionsofFigures2,3and4meansthatthesemembershipfunctionsreadilyadmitbothliteralandmetaphoricmembers.Sincethelinebetweenlit-eralandmetaphoricusesofacategoryisoftenim-possibletodraw,thebestonecandoistoacceptmetaphorasagradablephenomenon(seeHanks,2006).Theincorporationoftaxonomicsimilarityvia%simensuresthatliteralmemberswilltendtoreceivehighermembershipscores,andthatthemosttenuousmetaphorswillreceivethelowestmember-shipscores(closeto0.0).4.1ConstrainedCategoryInclusionSimileandmetaphorinvolvequitedifferentcon-ceptualmechanisms.Forinstance,anythingthatisparticularlystrongorblackmightmeaningfullybecalled”asblackasespresso”or”asstrongasespresso”,yetfewsuchthingscanmeaning-fullybecalledjust”espresso”.Whilesimileisamechanismforhighlightinginter-conceptsimilarity,metaphorisatheartamechanismofcategoryinclu-sion(seeGlucksberg,2001).Astheespressoexam-pledemonstrates,categoryinclusionismorethanamatterofsharedproperties:humanshavestrongin-tuitionsaboutthestructureofcategoriesandtheex-tenttowhichtheycanbestretchedtoincludenewmembers.Sowhileitissensibletoapplythecat-egoryEspressotoothersubstances,preferablyliq-uids,itseemsnonsensicaltoapplythecategorytoanimals,artifacts,placesandsoon.Muchasthesalientpropertiesofcategoriescanbeacquiredformtheweb(seesection3),sotoocantheintuitionsgoverninginclusionamongstcat-egories.Forinstance,anattestedweb-usageofthephrase”Espresso-likeCAT”tellsusthatsub-typesofCATareallowabletargetsofcategorizationbythecategoryEspresso.Thus,sincethequery”espresso-likesubstance”returns3hitsviaGoogle,typesofsubstance(oil,etc.)canbedescribedasEspressoiftheyarecontextuallystrongandblack.Incontrast,thequery”espresso-likeperson”returns0hits,sonoinstanceofpersoncanbedescribedasEspresso,nomatterhowblackorhowstrong.Whilethisisclearlyaheuristicapproachtoacomplexcognitiveproblem,itdoesallowustotapintothetacitknowl-edgethathumansemployincategorization.Moregenerally,aconceptXcanbeincludedinacategoryCifXexhibitssalientpropertiesofCand,forsomehypernymHofXinWordNet,wecanﬁndanat-testeduseof”C-likeH”ontheweb.Ifwecanpre-fetchallpossible”C-likeH”fromtheweb,thiswillallowcomprehensiontoproceedwithouthavingtoresorttowebanalysisinmid-categorization.WhiletherearetoomanypossiblevaluesofHtomakefullpre-fetchingapracticalreality,wecangeneralizetheproblemsomewhat,byselectingarangeofvaluesforHfromthemiddle-layerofWordNet,suchasPerson,Substance,Animal,Tool,Plant,Structure,Event,Vehicle,IdeaandPlace,andbypre-fetchingthequery”C-likeH”forall4061nounscollectedinsection3,combinedwiththislimitedsetofHvalues.Foreverynouninourdatabasethen,wepre-compileavectorofpossiblecategoryinclusions.62

Forinstance,”lattice”yieldsthefollowingvector:{structure(1620),substance(8),container(1),vehicle(1)}wherenumbersinparenthesesindicatetheweb-frequencyofthecorresponding”Lattice-likeH”query.Thus,thecategoryLatticecanbeusedtodescribe(andmetaphoricallyinclude)otherkindsofstructure(likecrystals),typesofsubstance(e.g.,crystallinesubstances),containers(likehoney-combs)andevenvehicles(e.g.,thosewithmanycompartments).Likewise,thenoun”snake”yieldsthefollowingvectorofpossibilities:{structure(125),animal(122),person(56),ve-hicle(17),tool(9)}(note,thefrequencyfor”person”includesthefrequencyfor”man”and”woman”).ThecategorySnakecanalsobeusedtodescribeandincludestructures(liketunnels),otheranimals(likeeels),people(e.g.,thedishonestvariety),vehicles(e.g.,articulatedtrucks,trains)andtools(e.g.,hoses).Thenoun”gladiator”yieldsavectorofjustoneelement,{person(1)},fromwhichthesimpleconstraint(%isaarg0Person.0)inFigure4isderived.Incon-trast,”snake”isnowgiventhedeﬁnitionofFigure5:(deﬁneSnake.0(arg0)(*(max(%isaarg0Structure.0)(%isaarg0Animal.0)(%isaarg0Person.0)(%isaarg0Vehicle.0))(*(%simarg0Snake.0)(combine(attrcunningarg0)(attrslipperyarg0)(attrﬂexiblearg0)(attrslimarg0)(attrsinuousarg0)(attrcrookedarg0)(attrdeadlyarg0)(attrpoisedarg0)))))Figure5.AmembershipfunctionforSnakeusingweb-derivedcategory-inclusionconstraints.Glucksberg(2001)notesthatthesamecategory,usedﬁguratively,canexhibitdifferentqualitiesindifferentmetaphors.Forinstance,Snakemightdescribeakindofcrookedpersoninonemetaphor,apoisedkillerinanothermetaphor,andakindofﬂexibletoolinyetanother.TheuseofcombineinFigure5meansthatasinglecategorydeﬁnitioncangiverisetoeachoftheseperspectivesintheappropriatecontexts.WethereforedonotneedadifferentcategorydeﬁnitionforeachmetaphoricuseofSnake.Toillustratethehigh-levelworkingsofcategory-inclusion,Table1generalizesoverthesetof3778disambiguatednounsfromsection3toestimatethepropensityforonesemanticcategory,likePerson,toincludemembersofanothercategory,likeAnimal,inX-likeYconstructs.X-likeYPASubTStr(P)erson.66.05.03.04.09(A)nimal.36.27.04.05.15(Sub)stance.14.03.37.05.32(T)ool.08.03.07.22.34(Str)ucture.04.03.03.03.43Table1.TheLikelihoodofacategoryXaccommo-datingacategoryY.Table1revealsthat36%of”ANIMAL-like”patternsonthewebdescribeakindofPerson,whileonly5%of”PERSON-like”patternsonthewebdescribeakindofAnimal.Categoryinclusionappearsheretobeaconservativemechanism,withlikedescribinglikeinmostcases;thus,typesofPersonaremostoftenusedtodescribeotherkindsofPerson(comprising66%of”PERSON-like”patterns),typesofsubstancetodescribeothersub-stances,andsoon.TheclearexceptionisAnimal,with”ANIMAL-like”phrasesmoreoftenusedtodescribepeople(36%)thanotherkindsofanimal(27%).Theanthropomorphicusesofthiscategorydemonstratetheimportanceoffolk-knowledgeinﬁgurativecategorization,ofthekindoneismorelikelytoﬁndinrealtext,andontheweb(asinsection3),ratherthaninresourceslikeWordNet.63

5EmpiricalEvaluationThesimilegatheringprocessofsection3,abettedbyGoogle’spracticeofrankingpagesaccordingtopopularity,shouldrevealthemostfrequently-usedcomparativenouns,andthus,themostusefulcat-egoriestocaptureinageneral-purposeontologylikeWordNet.Butthedescriptivesufﬁciencyofthesecategoriesisnotguaranteedunlessthedeﬁn-ingpropertiesascribedtoeachcanbeshowntobecollectivelyrichenough,andindividuallysalientenough,topredicthoweachcategoryisperceivedandappliedbyalanguageuser.Ifsimilesareindeedagoodbasisforminingthemostsalientanddiagnosticpropertiesofcate-gories,weshouldexpectthesetofpropertiesforeachcategorytoaccuratelypredicthowthecate-goryisperceivedasawhole.Forinstance,humans–unlikecomputers–donotgenerallyadoptadis-passionateviewofideas,butrathertendtoasso-ciatecertainpositiveornegativefeelings,oraffec-tivevalues,withparticularideas.Unsavouryactivi-ties,peopleandsubstancesgenerallypossessanega-tiveaffect,whilepleasantactivitiesandpeoplepos-sessapositiveaffect.Whissell(1989)reducesthenotionofaffecttoasinglenumericdimension,toproduceadictionaryofaffectthatassociatesanu-mericvalueintherange1.0(mostunpleasant)to3.0(mostpleasant)withover8000wordsinarangeofsyntacticcategories(includingadjectives,verbsandnouns).Sototheextentthattheadjectivalproper-tiesyieldedbyprocessingsimilespaintanaccuratepictureofeachcategory/noun-sense,weshouldbeabletopredicttheaffectiveratingofeachvehicleviaaweightedaverageoftheaffectiveratingsoftheadjectivalpropertiesascribedtothesenouns(i.e.,wheretheaffectratingofeachadjectivecontributestotheestimatedratingofanouninproportiontoitsfrequencyofco-occurrencewiththatnouninoursimiledata).Morespeciﬁcally,weshouldexpectthatratingsestimatedviathesesimile-derivedprop-ertiesshouldcorrelatewellwiththeindependentrat-ingscontainedinWhissell’sdictionary.Todeterminewhethersimilesdooffertheclearestperspectiveonacategory’smostsalientproperties,wecalculateandcomparethiscorrelationusingthefollowingdatasets:A.Adjectivesderivedfromannotatedbona-ﬁde(non-ironic)similesonly.B.Adjectivesderivedfromallannotatedsimiles(bothironicandnon-ironic).C.Adjectivesderivedfromironicsimilesonly.D.Alladjectivesusedtomodifyagivennouninalargecorpus.Weuseover2-gigabytesoftextfromtheonlineencyclopaediaWikipediaasourcorpus.E.Thesetof63,935uniqueproperty-of-nounpairingsextractedviashallow-parsingfromWordNetglossesinsection2,e.g.,strongandblackforEspresso.PredictionsofaffectiveratingweremadefromeachofthesedatasourcesandthencorrelatedwiththeratingsreportedinWhissell’sdictionaryofaffectusingatwo-tailedPearsontest(p<0.01).Asex-pected,propertysetsderivedfrombona-ﬁdesimi-lesonly(A)yieldedthebestcorrelation(+0.514)whilepropertiesderivedfromironicsimilesonly(C)yieldedtheworst(-0.243);amiddlingcorre-lationcoefﬁcientof0.347wasfoundforallsimi-lestogether,demonstratingthefactthatbona-ﬁdesimilesoutnumberironicsimilesbyaratioof4to1.Aweakercorrelationof0.15wasfoundus-ingthecorpus-derivedadjectivalmodiﬁersforeachnoun(D);whilethisdataprovidesquitelargeprop-ertysetsforeachnoun,thesepropertiesmerelyre-ﬂectpotentialratherthanintrinsicpropertiesofeachnounandsodonotrevealwhatismostdiagnosticaboutacategory.Moresurprisingly,propertysetsderivedfromWordNetglosses(E)arealsopoorlypredictive,yieldingacorrelationwithWhissell’saf-fectratingsofjust0.278.Thissuggeststhatthepropertiesusedtodeﬁnecategoriesinhand-craftedresourceslikeWordNetarenotalwaysthosethatac-tuallyreﬂecthowhumansthinkofthesecategories.6ConcludingRemarksMuchofwhatweunderstandaboutdifferentcate-goriesisbasedontacitanddefeasibleknowledgeoftheoutsideworld,knowledgethatcannoteasilybeshoe-hornedintotherigidis-astructureofanon-tologylikeWordNet.Thisalready-complexpicture64

iscomplicatedevenfurtherbytheoftenmetaphoricrelationshipbetweenwordsandthecategoriestheydenote,andbythefactthatthemetaphor/literaldis-tinctionisnotbinarybutgradable.Furthermore,thegradabilityofcategorymembershipisclearlyinﬂu-encedbycontext:inacorpusdescribingtheexploitsofVikings,anaxewillmostlikelybeseenasakindofweapon,butinacorpusdedicatedtoforestry,itwilllikelydescribeatool.AresourcelikeWordNet,inwhichis-alinksarereservedforcategoryrelation-shipsthatarealwaystrue,inanycontext,isgoingtobeinherentlylimitedwhendealingwithrealtext.WehavedescribedanapproachthatcanbeseenasafunctionalequivalenttotheCPA(CorpusPatternAnalysis)approachofPustejovskyetal.(2004),inwhichourgoalisnotthatofautomatedinductionofwordsensesincontext(asitisinCPA)buttheau-tomatedinductionofﬂexible,context-sensitivecat-egorystructures.Assuch,ourgoalisprimarilyon-tologicalratherthanlexicographic,thoughbothap-proachesarecomplementarysinceeachviewssyn-tagmaticevidenceasthekeytounderstandingtheuseoflexicalconceptsincontext.Bydeﬁningcat-egorymembershipintermsofsyntagmaticexpec-tations,weestablishafunctionalandgradableba-sisfordeterminingwhetheronelexicalconcept(orsynset)inWordNetdeservestobeseenasade-scendantofanotherinaparticularcorpusandcon-text.Augmentedwithontologicalconstraintsde-rivedfromtheusageof”X-likeY”patternsontheweb,wealsoshowhowthesemembershipfunctionscanimplementGlucksberg’s(2001)theoryofcate-goryinclusion.Wehavefocusedonjustonesyntagmaticpatternhere–adjectivalmodiﬁcationofnouns–butcate-gorizationcanbeinferredfromawiderangeofpro-ductivepatternsintext,particularlythoseconcern-ingverbsandtheircase-ﬁllers.Forinstance,verb-centredsimilesoftheform”toV+inflikea|anN”and”tobeV+pastlikea|anN”revealinsightsintothediagnosticbehaviourofentities(e.g.,thatpreda-torshunt,thatpreyishunted,thateaglessoarandbombsexplode).Takentogether,adjective-basedpropertiesandverb-basedbehaviourscanpaintanevenmorecomprehensivepictureofeachlexicalconcept,sothate.g.,politicalagentsthatkillcanbecategorizedasassassins,loyalentitiesthatﬁghtcanbecategorizedassoldiers,andsoon.Anim-portantnextstep,then,istominethesebehavioursfromthewebandincorporatethecorrespondingsyntagmaticexpectationsintoourcategorydeﬁni-tions.Thesymbolicnatureoftheresultingdeﬁni-tionsmeansthesecanservenotjustasmathematicalmembershipfunctions,butas”activeglosses”,capa-bleofrecruitingtheirownmembersinaparticularcontextwhiledemonstratingaﬂexibilitywithcate-gorizationandagenuinecompetencewithmetaphor.ReferencesAlexanderBudanitskyandGraemeHirst.2006.Eval-uatingWordNet-basedMeasuresofLexicalSemanticRelatedness.ComputationalLinguistics,32(1),pp13-47.ChristianeFellbaum(ed.).1998.WordNet:AnElec-tronicLexicalDatabase.TheMITPress,Cambridge,MA.CynthiaWhissell.1989.Thedictionaryofaffectinlan-guage.InR.Plutchnik&H.Kellerman(Eds.).Emo-tion:Theoryandresearch.NewYork,HarcourtBrace,113-131.JamesPustejovsky,PatrickHanksandAnnaRumshisky.2004.AutomatedInductionofSenseinContext.InProceedingsofCOLING2004,Geneva,pp924-931.PatrickHanks.2006.MetaphoricityisaGradable.InA.StefanowitschandS.Gries(eds.).CorporainCog-nitiveLinguistics.Vol.1:MetaphorandMetonymy.Berlin:Mouton.PatrickHanks.2004.Thesyntagmaticsofmetaphorandidiom.InternationalJournalofLexicography,17(3).PhilippCimiano,AndreasHotho,andSteffenStaab.2005.LearningConceptHierarchiesfromTextCor-porausingFormalConceptAnalysis.JournalofAIResearch,24:305-339.PieterDeLeenheerandAldodeMoor.2005.Context-drivenDisambiguationinOntologyElicitation.InShvaikoP.&EuzenatJ.(eds.),ContextandOntolo-gies:Theory,PracticeandApplications,AAAITechReportWS-05-01.AAAIPress,pp17-24.SamGlucksberg.2001.Understandingﬁgurativelan-guage:Frommetaphorstoidioms.Oxford:OxfordUniversityPress.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 65–72,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

65

ABayesianModelforDiscoveringTypologicalImplicationsHalDaum´eIIISchoolofComputingUniversityofUtahme@hal3.nameLyleCampbellDepartmentofLinguisticsUniversityofUtahlcampbel@hum.utah.eduAbstractAstandardformofanalysisforlinguis-tictypologyistheuniversalimplication.Theseimplicationsstatefactsabouttherangeofextantlanguages,suchas“ifob-jectscomeafterverbs,thenadjectivescomeafternouns.”Suchimplicationsaretypi-callydiscoveredbypainstakinghandanal-ysisoverasmallsampleoflanguages.Weproposeacomputationalmodelforassist-ingatthisprocess.Ourmodelisabletodiscoverbothwell-knownimplicationsaswellassomenovelimplicationsthatdeservefurtherstudy.Moreover,throughacarefulapplicationofhierarchicalanalysis,weareabletocopewiththewell-knownsamplingproblem:languagesarenotindependent.1IntroductionLinguistictypologyaimstodistinguishbetweenlog-icallypossiblelanguagesandactuallyobservedlan-guages.Afundamentalbuildingblockforsuchanunderstandingistheuniversalimplication(Green-berg,1963).Theseareshortstatementsthatrestrictthespaceoflanguagesinaconcreteway(forin-stance“object-verborderingimpliesadjective-nounordering”);Croft(2003),Hawkins(1983)andSong(2001)provideexcellentintroductionstolinguistictypology.Wepresentastatisticalmodelforauto-maticallydiscoveringsuchimplicationsfromalargetypologicaldatabase(Haspelmathetal.,2005).Analysesofuniversalimplicationsaretypicallyperformedbylinguists,inspectinganarrayof30-100languagesandafewpairsoffeatures.Lookingatallpairsoffeatures(typicallyseveralhundred)isvirtuallyimpossiblebyhand.Moreover,itisinsuf-ﬁcienttosimplylookatcounts.Forinstance,resultspresentedintheform“verbprecedesobjectimpliesprepositionsin16/19languages”arenonconclusive.Whilecompelling,thisisnotenoughevidencetode-cideifthisisastatisticallywell-foundedimplica-tion.Forone,maybe99%oflanguageshaveprepo-sitions:thenthefactthatwe’veachievedarateof84%actuallyseemsreallybad.Moreover,ifthe16languagesarehighlyrelatedhistoricallyorareally(geographically),andtheother3arenot,thenwemayhaveonlylearnedsomethingaboutgeography.Inthiswork,weproposeastatisticalmodelthatdealscleanlywiththesedifﬁculties.Bybuildingacomputationalmodel,itispossibletoapplyittoaverylargetypologicaldatabaseandsearchovermanythousandsofpairsoffeatures.Ourmodelhingesontwonovelcomponents:astatisticalnoisemodelahierarchicalinferenceoverlanguagefam-ilies.Toourknowledge,thereisnopriorworkdirectlyinthisarea.Theclosestworkisrepre-sentedbythebooksPossibleandProbableLan-guages(Newmeyer,2005)andLanguageClassiﬁca-tionbyNumbers(McMahonandMcMahon,2005),butthefocusofthesebooksisonautomaticallydis-coveringphylogenetictreesforlanguagesbasedonIndo-Europeancognatesets(Dyenetal.,1992).2DataThedatabaseonwhichweperformouranalysisistheWorldAtlasofLanguageStructures(Haspel-mathetal.,2005).Thisdatabasecontainsinfor-mationabout2150languages(sampledfromacrosstheworld;Figure1depictsthelocationsoflan-66

NumeralGlottalizedNumberofLanguageClassiﬁersRel/NOrderO/VOrderConsonantsToneGendersEnglishAbsentNRelVONoneNoneThreeHindiAbsentRelNOVNoneNoneTwoMandarinObligatoryRelNVONoneComplexNoneRussianAbsentNRelVONoneNoneThreeTukangBesiAbsent?EitherImplosivesNoneThreeZuluAbsentNRelVOEjectivesSimpleFive+Table1:Exampledatabaseentriesforaselectionofdiverselanguagesandfeatures.−150−100−50050100150−40−200204060Figure1:Mapofthe2150languagesinthedatabase.guages).Thereare139featuresinthisdatabase,brokendownintocategoriessuchas“NominalCate-gories,”“SimpleClauses,”“Phonology,”“WordOr-der,”etc.Thedatabaseissparse:formanylan-guage/featurepairs,thefeaturevalueisunknown.Infact,onlyabout16%ofallpossiblelanguage/featurepairsareknown.AsampleofﬁvelanguagesandsixfeaturesfromthedatabaseareshowninTable1.Importantly,thedensityofsamplesisnotrandom.Forcertainlanguages(eg.,English,Chinese,Rus-sian),nearlyallfeaturesareknown,whereasotherlanguages(eg.,Asturian,Omagua,Frisian)thathavefewerthanﬁvefeaturevaluesknown.Furthermore,somefeaturesareknownformanylanguages.Thisisduetothefactthatcertainfeaturestakelessefforttoidentifythanothers.Identifying,forinstance,ifalanguagehasaparticularsetofphonologicalfea-tures(suchasglottalizedconsonants)requiresonlylisteningtospeakers.Otherfeatures,suchasdeter-miningtheorderofrelativeclausesandnounsre-quireunderstandingmuchmoreofthelanguage.3ModelsInthissection,weproposetwomodelsforautomat-icallyuncoveringuniversalimplicationsfromnoisy,sparsedata.First,notethatevenwellattestedimpli-cationsarenotalwaysexceptionless.Acommonex-ampleisthatverbsprecedingobjects(“VO”)impliesadjectivesfollowingnouns(“NA”).Thisimplication(VO⊃NA)hasoneglaringexception:English.Thisisoneparticularformofnoise.Anothersourceofnoisestemsfromtranscription.WALScontainsdataaboutlanguagesdocumentedbyﬁeldlinguistsasearlyasthe1900s.Muchofthisolderdatawascollectedbeforetherewassigniﬁcantagreementindocumentationstyle.Differentﬁeldlinguistsof-tenhaddifferentdimensionsalongwhichtheyseg-mentedlanguagefeaturesintoclasses.Thisleadstonoiseinthepropertiesofindividuallanguages.Anotherdifﬁcultystemsfromthesamplingprob-lem.Thisisawell-documentedissue(see,eg.,(Croft,2003))stemmingfromthefactthatanysetoflanguagesisnotsampleduniformlyfromthespaceofallprobablelanguages.Politicallyinterestinglanguages(eg.,Indo-European)andtypologicallyunusuallanguages(eg.,Dyirbal)arebetterdocu-mentedthanothers.Moreover,languagesarenotin-dependent:GermanandDutcharemoresimilarthanGermanandHindiduetohistoryandgeography.Theﬁrstmodel,FLAT,treatseachlanguageasin-dependent.Itisthussusceptibletosamplingprob-lems.Forinstance,theWALSdatabasecontainsahalfdozenversionsofGerman.TheFLATmodelconsiderstheseversionsofGermanjustasstatisti-callyindependentas,say,GermanandHindi.Tocopewiththisproblem,wethenaugmenttheFLATmodelintoaHIERarchicalmodelthattakesadvan-tageofknownhierarchiesinlinguisticphylogenet-ics.TheHIERmodelexplicitlymodelsthefactthatindividuallanguagesarenotindependentandexhibitstrongfamilialdependencies.Inbothmodels,weinitiallyrestrictourattentiontopairsoffeatures.Wewilldescribeourmodelsasifallfeaturesarebinary.Weexpandanymulti-valuedfeaturewithKvaluesintoKbinaryfeaturesina“oneversusrest”manner.3.1TheFLATModelIntheFLATmodel,weconsidera2×Nmatrixoffeaturevalues.TheNcorrespondstothenumberoflanguages,whilethe2correspondstothetwofea-turescurrentlyunderconsideration(eg.,object/verborderandnoun/adjectiveorder).Theorderofthe67

twofeaturesisimportant:f1impliesf2islogicallydifferentfromf2impliesf1.Someoftheentriesinthematrixwillbeunknown.Wemaysafelyremovealllanguagesfromconsiderationforwhichbothareunknown,butwedonotremovelanguagesforwhichonlyoneisunknown.Wedosobecauseourmodelneedstocapturethefactthatiff2isalwaystrue,thenf1⊃f2isuninteresting.Thestatisticalmodelissetupasfollows.Thereisasinglevariable(wewilldenotethisvariable“m”)correspondingtowhethertheimplicationholds.Thus,m=1meansthatf1impliesf2andm=0meansthatitdoesnot.Independentofm,wespecifytwofeaturepriors,π1andπ2forf1andf2respec-tively.π1speciﬁesthepriorprobabilitythatf1willbetrue,andπ2speciﬁesthepriorprobabilitythatf2willbetrue.Onecanthenputthemodeltogetherna¨ıvelyasfollows.Ifm=0(i.e.,theimplicationdoesnothold),thentheentiredatamatrixisgener-atedbychoosingvaluesforf1(resp.,f2)indepen-dentlyaccordingtothepriorprobabilityπ1(resp.,π2).Ontheotherhand,ifm=1(i.e.,theimpli-cationdoeshold),thentheﬁrstcolumnofthedatamatrixisgeneratedbychoosingvaluesforf1inde-pendentlybyπ1,butthesecondcolumnisgenerateddifferently.Inparticular,ifforaparticularlanguage,wehavethatf1istrue,thenthefactthattheimplica-tionholdsmeansthatf2mustbetrue.Ontheotherhand,iff1isfalseforaparticularlanguage,thenwemaygeneratef2accordingtothepriorprobabilityπ2.Thus,havingm=1meansthatthemodelissigniﬁcantlymoreconstrained.Inequations:p(f1|π1)=πf11(1−π1)1−f1p(f2|f1,π2,m)=f2m=f1=1πf22(1−π2)1−f2otherwiseTheproblemwiththisna¨ıvemodelisthatitdoesnottakeintoaccountthefactthatthereis“noise”inthedata.(Bynoise,werefereithertomis-annotations,orto“strange”languageslikeEnglish.)Toaccountforthis,weintroduceasimplenoisemodel.Thereareseveraloptionsforparameteriz-ingthenoise,dependingonwhatindependenceas-sumptionswewishtomake.Onecouldsimplyspec-ifyanoiseratefortheentiredataset.Onecouldalternativelyspecifyalanguage-speciﬁcnoiserate.Oronecouldspecifyafeature-speciﬁcnoiserate.Weoptforablendbetweentheﬁrstandsecondop-Figure2:GraphicalmodelfortheFLATmodel.tion.Weassumeanunderlyingnoiseratefortheen-tiredataset,butthat,conditionedonthisunderlyingrate,thereisalanguage-speciﬁcnoiselevel.Webe-lievethistobeanappropriatenoisemodelbecauseitmodelsthefactthatthemajorityofinformationforasinglelanguageisfromasinglesource.Thus,ifthereisanerrorinthedatabase,itismorelikelythatothererrorswillbeforthesamelanguages.Inordertomodelthisstatistically,weassumethattherearelatentvariablese1,nande2,nforeachlan-guagen.Ife1,n=1,thentheﬁrstfeatureforlan-guageniswrong.Similarly,ife2,n=1,thenthesecondfeatureforlanguageniswrong.Giventhismodel,theprobabilitiesareexactlyasinthena¨ıvemodel,withtheexceptionthatinsteadofusingf1(resp.,f2),weusetheexclusive-or1f1⊗e1(resp.,f2⊗e2)sothatthefeaturevaluesareﬂippedwhen-everthenoisemodelsuggestsanerror.ThegraphicalmodelfortheFLATmodelisshowninFigure2.Circularnodesdenoterandomvariablesandarrowsdenoteconditionaldependencies.TherectangularplatedenotesthefactthattheelementscontainedwithinitarereplicatedNtimes(Nisthenumberoflanguages).Inthismodel,therearefour“root”nodes:theimplicationvaluem;thetwofea-turepriorprobabilitiesπ1andπ2;andthelanguage-speciﬁcerrorrateǫ.OnallofthesenodesweplaceBayesianpriors.Sincemisabinaryrandomvari-able,weplaceaBernoulliprioronit.TheπsareBernoullirandomvariables,sotheyaregiveninde-pendentBetapriors.Finally,thenoiserateǫisalsogivenaBetaprior.ForthetwoBetaparametersgov-erningtheerrorrate(i.e.,aǫandbǫ)wesetthesebyhandsothatthemeanexpectederrorrateis5%andtheprobabilityoftheerrorratebeingbetween0%and10%is50%(thisnumberisbasedonanexpertopinionofthenoise-rateinthedata).Fortherestof1Theexclusive-orofaandb,writtena⊗b,istrueexactlywheneitheraorbistruebutnotboth.68

theparametersweuseuniformpriors.3.2TheHIERModelAsigniﬁcantdifﬁcultyinworkingwithanylargety-pologicaldatabaseisthatthelanguageswillbesam-plednonuniformly.Inourcase,thismeansthatim-plicationsthatseemtrueintheFLATmodelmayonlybetruefor,say,Indo-European,andtheremain-inglanguagesareconsiderednoise.Whilethismaybeinterestinginitsownright,wearemoreinterestedindiscoveringimplicationsthataretrulyuniversal.WemodelthisusingahierarchicalBayesianmodel.Inessence,wetaketheFLATmodelandbuildanotionoflanguagerelatednessintoit.Inparticular,weenforceahierarchyonthemimpli-cationvariables.Forsimplicity,supposethatour“hierarchy”oflanguagesisnearlyﬂat.OftheNlanguages,halfofthemareIndo-EuropeanandtheotherhalfareAustronesian.WewilluseanearlyidenticalmodeltotheFLATmodel,butinsteadofhavingasinglemvariable,wehavethree:oneforIE,oneforAustronesianandonefor“alllanguages.”Forageneraltree,weassignoneimplicationvari-ableforeachnode(includingtherootandleaves).Thegoaloftheinferenceistoinferthevalueofthemvariablecorrespondingtotherootofthetree.AllthatislefttospecifythefullHIERmodelistospecifytheprobabilitydistributionofthemrandomvariables.Wedothisasfollows.WeplaceazeromeanGaussianpriorwith(unknown)varianceσ2ontherootm.Then,foranon-rootnode,weuseaGaussianwithmeanequaltothe“m”valueoftheparentandtiedvarianceσ2.Inourthree-nodeexample,thismeansthattherootisdistributedNor(0,σ2)andeachchildisdistributedNor(mroot,σ2),wheremrootistherandomvariablecorrespondingtotheroot.Finally,theleaves(cor-respondingtothelanguagesthemselves)aredis-tributedlogistic-binomial.Thus,themrandomvari-ablecorrespondingtoaleaf(language)isdistributedBin(s(mpar)),wheremparisthemvalueforthepar-ent(internal)nodeandsisthesigmoidfunctions(x)=[1+exp(−x)]−1.Theintuitionbehindthismodelisthatthemvalueateachnodeinthetree(whereanodeiseither“alllanguages”oraspeciﬁclanguagefamilyoranin-dividuallanguage)speciﬁestheextenttowhichtheimplicationunderconsiderationholdsforthatnode.Alargepositivemmeansthattheimplicationisverylikelytohold.Alargenegativevaluemeansitisverylikelytonothold.Thenormaldistributionsacrossedgesinthetreeindicatethatweexpectthemvaluesnottochangetoomuchacrossthetree.Attheleaves(i.e.,individuallanguages),thelogistic-binomialsimplytransformsthereal-valuedmsintotherange[0,1]soastomakeanappropriateinputtothebinomialdistribution.4StatisticalInferenceInthissection,wedescribehowweuseMarkovchainMonteCarlomethodstoperforminferenceinthestatisticalmodelsdescribedintheprevioussection;Andrieuetal.(2003)provideanexcel-lentintroductiontoMCMCtechniques.ThekeyideabehindMCMCtechniquesistoapproximatein-tractableexpectationsbydrawingrandomsamplesfromtheprobabilitydistributionofinterest.Theex-pectationcanthenbeapproximatedbyanempiricalexpectationoverthesesample.FortheFLATmodel,weuseacombinationofGibbssamplingwithrejectionsamplingasasub-routine.Essentially,allsamplingstepsarestandardGibbssteps,exceptforsamplingtheerrorratese.TheGibbsstepisnotavailableanalyticallyforthese.Hence,weuserejectionsampling(drawingfromtheBetapriorandacceptingaccordingtotheposterior).ThesamplingprocedurefortheHIERmodelisonlyslightlymorecomplicated.Insteadofperform-ingasimpleGibbssampleforminStep(4),weﬁrstsamplethemvaluesfortheinternalnodesus-ingsimpleGibbsupdates.Fortheleafnodes,weuserejectionsampling.Forthisrejection,wedrawproposalvaluesfromtheGaussianspeciﬁedbytheparentm,andcomputeacceptanceprobabilities.Inallcases,weruntheouterGibbssamplerfor1000iterationsandeachrejectionsamplerfor20it-erations.Wecomputethemarginalvaluesforthemimplicationvariablesbyaveragingthesampledval-uesafterdropping200“burn-in”iterations.5DataPreprocessingandSearchAfterextractingtherawdatafromtheWALSelec-tronicdatabase(Haspelmathetal.,2005)2,weper-formaminoramountofpreprocessing.Essen-tially,wehavemanuallyremovedcertainfeature2Thisisnontrivial—wearecurrentlyexploringthepossibil-ityoffreelysharingthesedata.69

valuesfromthedatabasebecausetheyareunderrep-resented.Forinstance,the“GlottalizedConsonants”featurehaseightpossiblevalues(onefor“none”andsevenfordifferentvarietiesofglottalizedconso-nants).Wereducethistosimplytwovalues“has”or“hasnot.”313languageshavenoglottalizedconso-nantsand139havesomevarietyofglottalizedcon-sonant.Wehavedonesomethingsimilarwithap-proximatelytwentyofthefeatures.FortheHIERmodel,weobtainthehierarchyinoneoftwoways.Theﬁrsthierarchyweuseisthe“linguistichierarchy”speciﬁedaspartoftheWALSdata.Thishierarchydivideslanguagesintofamiliesandsubfamilies.Thisleadstoatreewiththeleavesatdepthfour.Theroothas38immediatechildren(correspondingtothemajorfamilies),andthereareatotalof314internalnodes.Thesecondhierar-chyweuseisanarealhierarchyobtainedbyclus-teringlanguagesaccordingtotheirlatitudeandlon-gitude.Fortheclusteringweﬁrstclusterallthelan-guagesinto6“macro-clusters.”Wethenclustereachmacro-clusterindividuallyinto25“micro-clusters.”Thesemicro-clustersthenhavethelanguagesattheirleaves.Thisyieldsatreewith31internalnodes.Giventhedatabase(whichcontainsapproxi-mately140features),performingarawsearchevenoverallpossiblepairsoffeatureswouldleadtoover19,000computations.Inordertoreducethisspacetoamoremanageablenumber,weﬁlter:•Theremustbeatleast250languagesforwhichbothfea-turesareknown.•Theremustbeatleast15languagesforwhichbothfea-turevaluesholdsimultaneously.•Wheneverf1istrue,atleasthalfofthelanguagesalsohavef2true.Performingalltheseﬁltrationstepsreducesthenumberofpairsunderconsiderationto3442.Whilethisremainsacomputationallyexpensiveprocedure,wewereabletoperformalltheimplicationcompu-tationsforthese3442possiblepairsinaboutaweekonasinglemodernmachine(inMatlab).6ResultsThetaskofdiscoveringuniversalimplicationsis,atitsheart,adata-miningtask.Assuch,itisdifﬁculttoevaluate,sinceweoftendonotknowthecorrectanswers!Ifourmodelonlyfoundwell-documentedimplications,thiswouldbeinterestingbutuselessfromtheperspectiveofaidinglinguistsfocustheirenergiesonnew,plausibleimplications.Inthissec-tion,wepresenttheresultsofourmethod,togetherwithbothaquantitativeandqualitativeevaluation.6.1QuantitativeEvaluationInthissection,weperformaquantitativeevaluationoftheresultsbasedonpredictivepower.Thatis,onegenerallywouldpreferasystemthatﬁndsim-plicationsthatholdwithhighprobabilityacrossthedata.Theword“generally”isimportant:thisqual-ityisneithernecessarynorsufﬁcientforthemodeltobegood.Forinstance,ﬁnding1000implicationsoftheformA1⊃X,A2⊃X,...,A1000⊃XiscompletelyuninterestingifXistruein99%ofthecases.Similarly,supposethatamodelcanﬁnd1000implicationsoftheformX⊃A1,...,X⊃A1000,butXisonlytrueinﬁvelanguages.Inbothofthesecases,accordingtoa“predictivepower”measure,thesewouldbeidealsystems.Buttheyarebothsomewhatuninteresting.Despitethesedifﬁcultieswithapredictivepower-basedevaluation,wefeelthatitisagoodwaytoun-derstandtherelativemeritsofourdifferentmodels.Thus,wecomparethefollowingsystems:FLAT(ourproposedﬂatmodel),LINGHIER(ourmodelusingthephylogenetichierarchy),DISTHIER(ourmodelusingthearealhierarchy)andRANDOM(amodelthatranksimplications—thatmeetthethreequaliﬁ-cationsfromtheprevioussection—randomly).Themodelsarescoredasfollows.WetaketheentireWALSdatasetand“hide”arandom10%oftheentries.Wethenperformfullinferenceandasktheinferredmodeltopredictthemissingval-ues.Theaccuracyofthemodelistheaccuracyofitspredictions.Toobtainasenseofthequalityoftheranking,weperformthiscomputationonthetopkrankedimplicationsprovidedbyeachmodel;k∈{2,4,8,...,512,1024}.TheresultsofthisquantitativeevaluationareshowninFigure3(onalog-scaleforthex-axis).Thetwobest-performingmodelsarethetwohier-archicalmodels.Theﬂatmodeldoessigniﬁcantlyworseandtherandommodeldoesterribly.Thever-ticallinesareastandarddeviationover100foldsoftheexperiment(hidingadifferent10%eachtime).Thedifferencebetweenthetwohierarchicalmod-elsistypicallynotstatisticallysigniﬁcant.Atthetopoftheranking,themodelbasedonphylogenetic70

0123456789100.650.70.750.80.850.90.951Number of Implications (log2)Prediction Accuracy  LingHierDistHierFlatRandomFigure3:Resultsofquantitative(predictive)evalua-tion.Topcurvesarethehierarchicalmodels;middleistheﬂatmodel;bottomistherandombaseline.informationperformsmarginallybetter;atthebot-tomoftheranking,theorderﬂips.Comparingthehierarchicalmodelstotheﬂatmodel,weseethatadequatelymodelingtheapriorisimilaritybetweenlanguagesisquiteimportant.6.2Cross-modelComparisonTheresultsintheprevioussectionsupportthecon-clusionthatthetwohierarchicalmodelsaredoingsomethingsigniﬁcantlydifferent(andbetter)thantheﬂatmodel.Thisclearlymustbethecase.Theresults,however,donotsaywhetherthetwohierar-chiesaresubstantiallydifferent.Moreover,aretheresultsthattheyproducesubstantiallydifferent.Theanswertothesetwoquestionsis“yes.”Weﬁrstaddresstheissueoftreesimilarity.Weconsiderallpairsoflanguageswhichareatdistance0inthearealtree(i.e.,havethesameparent).Wethenlookatthemeantree-distancebetweenthoselanguagesinthephylogenetictree.Wedothisforalldistancesinthearealtree(becauseofitsconstruc-tion,thereareonlythree:0,2and4).Themeandistancesinthephylogenetictreecorrespondingtothesethreedistancesinthearealtreeare:2.9,3.5and4.0,respectively.Thismeansthatlanguagesthatare“nearby”inthearealtreearequiteoftenveryfarapartinthephylogenetictree.Toanswertheissueofwhethertheresultsob-tainedbythetwotreesaresimilar,weemployKendall’sτstatistic.Giventwoorderedlists,theτstatisticcomputeshowcorrelatedtheyare.τisalwaysbetween0and1,with1indicatingidenticalorderingand0indicatedcompletelyreversedorder-ing.Theresultsareasfollows.ComparingFLATtoLINGHIERyieldτ=0.4144,averylowcorrela-tion.BetweenFLATandDISTHIER,τ=0.5213,alsoverylow.Thesetwoareasexpected.Fi-nally,betweenLINGHIERandDISTHIER,weob-tainτ=0.5369,averylowcorrelation,consideringthatbothperformwellpredictively.6.3QualitativeAnalysisForthepurposeofaqualitativeanalysis,were-producethetop30implicationsdiscoveredbytheLINGHIERmodelinTable2(seetheﬁnalpage).3Eachimplicationisnumbered,thentheactualim-plicationispresented.Forinstance,#7saysthatanylanguagethathasadjectivesprecedingtheirgoverningnounsalsohasnumeralsprecedingtheirnouns.Weadditionallyprovidean“analysis”ofmanyofthesediscoveredimplications.Manyofthem(eg.,#7)arewellknowninthetypologicallit-erature.Thesearesimplynumberedaccordingtowell-knownreferences.Forinstanceour#7isim-plication#18fromGreenberg,reproducedbySong(2001).ThosethatreferenceHawkins(eg.,#11)arebasedonimplicationsdescribedbyHawkins(1983);thosethatreferenceLehmannarereferencestotheprinciplesdecidedbyLehmann(1981)inCh4&8.Someoftheimplicationsourmodeldiscoversareobtainedbycompositionofwell-knownimplica-tions.Forinstance,our#3(namely,OV⊃Genitive-Noun)canbeobtainedbycombiningGreenberg#4(OV⊃Postpositions)andGreenberg#2a(Postpo-sitions⊃Genitive-Noun).Itisquiteencouragingthat14ofourtop21discoveredimplicationsarewell-knownintheliterature(andthis,notevencon-sideringthetautalogicallytrueimplications)!Thisstronglysuggeststhatourmodelisdoingsomethingreasonableandthatthereistruestructureinthedata.Inadditiontomanyoftheknownimplicationsfoundbyourmodel,therearemanythatare“un-known.”Spaceprecludesattemptingexplanationsofthemall,sowefocusonafew.Someareeasy.Consider#8(Stronglysufﬁxing⊃Tense-aspectsuf-ﬁxes):thisisquiteplausible—ifyouhavealan-3Intruth,ourmodeldiscoversseveraltautalogicalimplica-tionsthatwehaveremovedbyhandbeforepresentation.Theseareexampleslike“SVO⊃VO”or“Nounusualconsonants⊃noglottalizedconsonants.”Itis,ofcourse,goodthatourmodeldiscoversthese,sincetheyareobviouslytrue.However,tosavespace,wehavewithheldthemfrompresentationhere.The30thimplicationpresentedhereisactuallythe83rdinourfulllist.71

guagethattendstohavesufﬁxes,itwillprobablyhavesufﬁxesfortense/aspect.Similarly,#10statesthatlanguageswithverbmorphologyforquestionslackquestionparticles;again,thiscanbeeasilyex-plainedbyanappealtoeconomy.Someofthediscoveredimplicationsrequireamoreinvolvedexplanation.Onesuchexampleis#20:labial-velarsimpliesnouvulars.4Itturnsoutthatlabial-velarsaremostcommoninAfricajustnorthoftheequator,whichisalsoaplacethathasveryfewuvulars(thereareahandfulofotherex-amples,mostlyinPapuaNewGuinea).Whilethisimplicationhasnotbeenpreviouslyinvestigated,itmakessomesense:ifalanguagehasoneformofrareconsonant,itisunlikelytohaveanother.Asanotherexample,consider#28:Obligatorysufﬁxpronounsimpliesnopossessiveafﬁxes.Thismeansisthatinlanguages(likeEnglish)forwhichpro-dropisimpossible,possessionisnotmarkedmorphologicallyontheheadnoun(likeEnglish,“book”appearsthesameregarlessofifitis“hisbook”or“thebook”).Thisalsomakessense:ifyoucannotdroppronouns,thenoneusuallywillmarkpossessiononthepronoun,nottheheadnoun.Thus,youdonotneedmarkingontheheadnoun.Finally,consider#25:Highandmidfrontvowels(i.e.,/u/,etc.)implieslargevowelinventory(≥7vowels).Thisissupportedbytypologicalevidencethathighandmidfrontvowelsarethe“last”vowelstobeaddedtoalanguage’srepertoire.Thus,inordertogetthem,youmustalsohavemanyothertypesofvowelsalready,leadingtoalargevowelinventory.Notallexamplesadmitasimpleexplanationandareworthyoffurtherthought.Someofwhich(liketheonespredicatedon“SV”)mayjustbepeculiar-itiesoftheannotationstyle:thesubjectverborderchangesfrequentlybetweentransitiveandintransi-tiveusagesinmanylanguages,andtheannotationreﬂectsjustone.Someothersarebizzarre:whynothavingfricativesshouldmeanthatyoudon’thavetones(#27)isnotaprioriclear.6.4Multi-conditionalImplicationsManyimplicationsintheliteraturehavemultipleimplicants.Forinstance,muchresearchhasgone4Labial-velarsanduvularsarerareconsonants(order100languages).Labial-velarsarejoinedsoundslike/kp/and/gb/(toEnglishspeakers,soundinglikechickennoises);uvularssoundsaremadeinthebackofthethroat,likesnoring.ImplicantsImplicandPostpositions⊃Demonstrative-NounAdjective-NounPosessivepreﬁxes⊃Genitive-NounTense-aspectsufﬁxesCasesufﬁxes⊃Genitive-NounPluralsufﬁxAdjective-Noun⊃OVGenitive-NounHighcons/vowelratio⊃NotonesNofront-roundedvowelsNegativeafﬁx⊃OVGenitive-NounNofront-roundedvowels⊃LargevowelqualityinventoryLabialvelarsSubordinatingsufﬁx⊃PostpositionsTense-aspectsufﬁxesNocaseafﬁxes⊃InitialsubordinatorwordPrepositionsStronglysufﬁxing⊃Genitive-NounPluralsufﬁxTable3:TopimplicationsdiscoveredbytheLINGHIERmulti-conditionalmodel.intolookingatwhichimplicationshold,consideringonly“VO”languages,orconsideringonlylanguageswithprepositions.Itisstraightforwardtomodifyourmodelsothatitsearchesovertriplesoffeatures,conditioningontwoandpredictingthethird.Spaceprecludesanin-depthdiscussionoftheseresults,butwepresentthetopexamplesinTable3(afterremov-ingthetautalogicallytrueexamples,whicharemorenumerousinthiscase,aswellasexamplesthataredirectlyobtainablefromTable2).Itisencouragingthatinthetop1000multi-conditionalimplicationsfound,themostfrequentlyusedwere“OV”(176times)“Postpositions”(157times)and“Adjective-Noun”(89times).Thisresultagreeswithintuition.7DiscussionWehavepresentedaBayesianmodelfordiscoveringuniversallinguisticimplicationsfromatypologicaldatabase.Ourmodelisabletoaccountfornoiseinalinguisticallyplausiblemanner.Ourhierarchicalmodelsdealwiththesamplingissueinauniqueway,byusingpriorknowledgeaboutlanguagefamiliesto“group”relatedlanguages.Quantitatively,thehier-archicalinformationturnsouttobequiteuseful,re-gardlessofwhetheritisphylogenetically-orareally-based.Qualitatively,ourmodelcanrecovermanywell-knownimplicationsaswellasmanymorepo-tentialimplicationsthatcanbetheobjectoffuturelinguisticstudy.Webelievethatourmodelissuf-72

#Implicant⊃ImplicandAnalysis1Postpositions⊃Genitive-NounGreenberg#2a2OV⊃PostpositionsGreenberg#43OV⊃Genitive-NounGreenberg#4+Greenberg#2a4Genitive-Noun⊃PostpositionsGreenberg#2a(converse)5Postpositions⊃OVGreenberg#2b(converse)6SV⊃Genitive-Noun???7Adjective-Noun⊃Numeral-NounGreenberg#188Stronglysufﬁxing⊃Tense-aspectsufﬁxesClearexplanation9VO⊃Noun-RelativeClauseLehmann10Interrogativeverbmorph⊃NoquestionparticleAppealtoeconomy11Numeral-Noun⊃Demonstrative-NounHawkinsXVI(forpostpositionallanguages)12Prepositions⊃VOGreenberg#3(converse)13Adjective-Noun⊃Demonstrative-NounGreenberg#1814Noun-Adjective⊃PostpositionsLehmann15SV⊃Postpositions???16VO⊃PrepositionsGreenberg#317Initialsubordinatorword⊃PrepositionsOperator-operandprinciple(Lehmann)18Strongpreﬁxing⊃PrepositionsGreenberg#27b19Littleafﬁxation⊃Noun-Adjective???20Labial-velars⊃NouvularconsonantsSeetext21Negativeword⊃NopronominalpossessiveafﬁxesSeetext22Strongpreﬁxing⊃VOLehmann23Subordinatingsufﬁx⊃Stronglysufﬁxing???24Finalsubordinatorword⊃PostpositionsOperator-operandprinciple(Lehmann)25Highandmidfrontvowels⊃LargevowelinventoriesSeetext26Pluralpreﬁx⊃Noun-Genitive???27Nofricatives⊃Notones???28Obligatorysubjectpronouns⊃NopronominalpossessiveafﬁxesSeetext29Demonstrative-Noun⊃Tense-aspectsufﬁxesOperator-operandprinciple(Lehmann)30Prepositions⊃Noun-RelativeclauseLehmann,HawkinsTable2:Top30implicationsdiscoveredbytheLINGHIERmodel.ﬁcientlygeneralthatitcouldbeappliedtomanydifferenttypologicaldatabases—weattemptednotto“overﬁt”ittoWALS.Ourhopeisthattheau-tomaticdiscoveryofsuchimplicationsnotonlyaidtypologically-inclinedlinguists,butalsoothergroups.Forinstance,well-attesteduniversalimpli-cationshavethepotentialtoreducetheamountofdataﬁeldlinguistsneedtocollect.Theyhavealsobeenusedcomputationallytoaidinthelearningofunsupervisedpartofspeechtaggers(SchoneandJu-rafsky,2001).Manyextensionsarepossibletothismodel;forinstanceattemptingtouncovertypolog-icallyhierarchiesandotherhigher-orderstructures.Wehavemadethefulloutputofallmodelsavailableathttp://hal3.name/WALS.Acknowledgments.WearegratefultoYeeWhyeTeh,EricXingandthreeanonymousreviewersfortheirfeedbackonthiswork.ReferencesChristopheAndrieu,NandodeFreitas,ArnaudDoucet,andMichaelI.Jordan.2003.AnintroductiontoMCMCformachinelearning.MachineLearning(ML),50:5–43.WilliamCroft.2003.TypologyandUniverals.CambridgeUniversityPress.IsidoreDyen,JosephKurskal,andPaulBlack.1992.AnIndoeuropeanclassiﬁcation:Alexicostatisticalexperiment.TransactionsoftheAmericanPhilosophicalSociety,82(5).AmericanPhilosophicalSociety.JosephGreenberg,editor.1963.UniversalsofLanguages.MITPress.MartinHaspelmath,MatthewDryer,DavidGil,andBernardComrie,editors.2005.TheWorldAtlasofLanguageStruc-tures.OxfordUniversityPress.JohnA.Hawkins.1983.WordOrderUniversals:Quantitativeanalysesoflinguisticstructure.AcademicPress.WinfredLehmann,editor.1981.SyntacticTypology,volumexiv.UniversityofTexasPress.AprilMcMahonandRobertMcMahon.2005.LanguageClas-siﬁcationbyNumbers.OxfordUniversityPress.FrederickJ.Newmeyer.2005.PossibleandProbableLan-guages:AGenerativePerspectiveonLinguisticTypology.OxfordUniversityPress.PatrickSchoneandDanJurafsky.2001LanguageIndependentInductionofPartofSpeechClassLabelsUsingonlyLan-guageUniversals.MachineLearning:BeyondSupervision.JaeJungSong.2001.LinguisticTypology:MorphologyandSyntax.LongmanLinguisticsLibrary.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 73–80,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

73

ADiscriminativeLanguageModelwithPseudo-NegativeSamplesDaisukeOkanohara(cid:0)Jun’ichiTsujii(cid:0)(cid:1)(cid:2)(cid:0)DepartmentofComputerScience,UniversityofTokyoHongo7-3-1,Bunkyo-ku,Tokyo,Japan(cid:1)SchoolofInformatics,UniversityofManchester(cid:2)NaCTeM(NationalCenterforTextMining)(cid:3)hillbig,tsujii(cid:4)@is.s.u-tokyo.ac.jpAbstractInthispaper,weproposeanoveldiscrim-inativelanguagemodel,whichcanbeap-pliedquitegenerally.ComparedtothewellknownN-gramlanguagemodels,dis-criminativelanguagemodelscanachievemoreaccuratediscriminationbecausetheycanemployoverlappingfeaturesandnon-localinformation.However,discriminativelanguagemodelshavebeenusedonlyforre-rankinginspeciﬁcapplicationsbecausenegativeexamplesarenotavailable.Weproposesamplingpseudo-negativeexamplestakenfromprobabilisticlanguagemodels.However,thisapproachrequiresprohibitivecomputationalcostifwearedealingwithquiteafewfeaturesandtrainingsamples.Wetackletheproblembyestimatingthela-tentinformationinsentencesusingasemi-Markovclassmodel,andthenextractingfeaturesfromthem.Wealsouseanon-linemargin-basedalgorithmwithefﬁcientkernelcomputation.Experimentalresultsshowthatpseudo-negativeexamplescanbetreatedasrealnegativeexamplesandourmodelcanclassifythesesentencescorrectly.1IntroductionLanguagemodels(LMs)arefundamentaltoolsformanyapplications,suchasspeechrecognition,ma-chinetranslationandspellingcorrection.ThegoalofLMsistodeterminewhetherasentenceiscorrectorincorrectintermsofgrammarsandpragmatics.ThemostwidelyusedLMisaprobabilisticlan-guagemodel(PLM),whichassignsaprobabilitytoasentenceorawordsequence.Inparticular,N-gramswithmaximumlikelihoodestimation(NLMs)areoftenused.AlthoughNLMsaresimple,theyareeffectiveformanyapplications.However,NLMscannotdeterminecorrectnessofasentenceindependentlybecausetheprobabil-itydependsonthelengthofthesentenceandtheglobalfrequenciesofeachwordinit.Forexam-ple,(cid:1)(cid:0)(cid:2)(cid:1)(cid:1),where(cid:1)istheprobabilityofasentence(cid:1)givenbyanNLM,doesnotalwaysmeanthat(cid:1)(cid:1)ismorecorrect,butinsteadcouldoccurwhen(cid:1)(cid:1)isshorterthan(cid:1)(cid:0),orif(cid:1)(cid:1)hasmorecom-monwordsthan(cid:1)(cid:0).AnotherproblemisthatNLMscannothandleoverlappinginformationornon-localinformationeasily,whichisimportantformoreac-curatesentenceclassiﬁcation.Forexample,aNLMcouldassignahighprobabilitytoasentenceevenifitdoesnothaveaverb.Discriminativelanguagemodels(DLMs)havebeenproposedtoclassifysentencesdirectlyascor-rectorincorrect(Gaoetal.,2005;Roarketal.,2007),andthesemodelscanhandlebothnon-localandoverlappinginformation.HoweverDLMsinpreviousstudieshavebeenrestrictedtospeciﬁcap-plications.Thereforethemodelcannotbeusedforotherapplications.Ifwehadnegativeexamplesavailable,themodelscouldbetraineddirectlybydiscriminatingbetweencorrectandincorrectsen-tences.Inthispaper,weproposeagenericDLM,whichcanbeusednotonlyforspeciﬁcapplications,butalsomoregenerally,similartoPLMs.Toachieve74

thisgoal,weneedtosolvetwoproblems.Theﬁrstisthatsincewecannotobtainnegativeexamples(in-correctsentences),weneedtogeneratethem.Thesecondistheprohibitivecomputationalcostbecausethenumberoffeaturesandexamplesisverylarge.Inpreviousstudiesthisproblemdidnotarisebecausetheamountoftrainingdatawaslimitedandtheydidnotuseacombinationoffeatures,andthusthecom-putationalcostwasnegligible.Tosolvetheﬁrstproblem,weproposesamplingincorrectsentencestakenfromaPLMandthentrainingamodeltodiscriminatebetweencorrectandincorrectsentences.WecalltheseexamplesPseudo-Negativebecausetheyarenotactuallynegativesen-tences.WecallthismethodDLM-PN(DLMwithPseudo-Negativesamples).Todealwiththesecondproblem,weemployanonlinemargin-basedlearningalgorithmwithfastkernelcomputation.Thisenablesustoemploycom-binationsoffeatures,whichareimportantfordis-criminationbetweencorrectandincorrectsentences.Wealsoestimatethelatentinformationinsentencesbyusingasemi-Markovclassmodeltoextractfea-tures.Althoughtherearesubstantiallyfewerla-tentfeaturesthanexplicitfeaturessuchaswordsorphrases,latentfeaturescontainessentialinformationforsentenceclassiﬁcation.Experimentalresultsshowthatthesepseudo-negativesamplescanbetreatedasincorrectexam-ples,andthatDLM-PNcanlearntocorrectlydis-criminatebetweencorrectandincorrectsentencesandcanthereforeclassifythesesentencescorrectly.2PreviousworkProbabilisticlanguagemodels(PLMs)estimatetheprobabilityofwordstringsorsentences.Amongthesemodels,N-gramlanguagemodels(NLMs)arewidelyused.NLMsapproximatetheprobabilitybyconditioningonlyonthepreceding (cid:2)words.Forexample,let(cid:1)denoteasentenceofwords,(cid:1)(cid:3)(cid:4)(cid:5)(cid:0)(cid:6)(cid:5)(cid:1)(cid:6)(cid:7)(cid:7)(cid:7)(cid:6)(cid:5).Then,bythechainruleofprobabilityandtheapproximation,wehave(cid:1)(cid:4)(cid:5)(cid:0)(cid:6)(cid:5)(cid:1)(cid:6)(cid:7)(cid:7)(cid:7)(cid:6)(cid:5)(cid:4)(cid:0)(cid:1)(cid:2)(cid:0)(cid:2)(cid:2)(cid:2)(cid:5)(cid:1)(cid:1)(cid:5)(cid:1) (cid:0)(cid:6)(cid:7)(cid:7)(cid:7)(cid:6)(cid:5)(cid:1) (cid:0)(cid:7)(1)Theparameterscanbeestimatedusingthemaxi-mumlikelihoodmethod.SincethenumberofparametersinNLMisstilllarge,severalsmoothingmethodsareused(ChenandGoodman,1998)toproducemoreaccurateprobabilities,andtoassignnonzeroprobabilitiestoanywordstring.However,sincetheprobabilitiesinNLMsdependonthelengthofthesentence,twosentencesofdif-ferentlengthcannotbecompareddirectly.Recently,WholeSentenceMaximumEntropyModels(Rosenfeldetal.,2001)(WSMEs)havebeenintroduced.Theyassignaprobabilitytoeachsentenceusingamaximumentropymodel.AlthoughWSMEscanencodeallfeaturesofasentenceincludingnon-localones,theyareonlyslightlysuperiortoNLMs,inthattheyhavethedis-advantageofbeingcomputationallyexpensive,andnotallrelevantfeaturescanbeincluded.Adiscriminativelanguagemodel(DLM)assignsascore(cid:9)(cid:1)toasentence(cid:1),measuringthecorrect-nessofasentenceintermsofgrammarandprag-matics,sothat(cid:9)(cid:1)(cid:10)(cid:5)implies(cid:1)iscorrectand(cid:9)(cid:1)(cid:2)(cid:5)implies(cid:1)isincorrect.APLMcanbeconsideredasaspecialcaseofaDLMbydeﬁning(cid:9)using(cid:1).Forexample,wecantake(cid:9)(cid:1)(cid:4)(cid:1)(cid:11)(cid:1)(cid:1)(cid:1) (cid:12),where(cid:12)issomethreshold,and(cid:1)(cid:1)(cid:1)isthelengthof(cid:1).Givenasentence(cid:1),weextractafeaturevector((cid:13)(cid:1))fromitusingapre-deﬁnedsetoffeaturefunctions(cid:2)(cid:13)(cid:4)(cid:3)(cid:4)(cid:2)(cid:0).Theformofthefunction(cid:9)weuseis(cid:9)(cid:1)(cid:4)(cid:0)(cid:13)(cid:1)(cid:6)(2)where(cid:0)isafeatureweightingvector.Sincethereisnorestrictionindesigning(cid:13)(cid:1),DLMscanmakeuseofbothover-lappingandnon-localinformationin(cid:1).Weestimate(cid:0)usingtrainingsamples(cid:2)(cid:1)(cid:1)(cid:6)(cid:14)(cid:1)(cid:3)for(cid:15)(cid:4)(cid:2)(cid:7)(cid:7)(cid:7),where(cid:14)(cid:1)(cid:4)(cid:2)if(cid:1)(cid:1)iscorrectand(cid:14)(cid:1)(cid:4) (cid:2)if(cid:1)(cid:1)isincorrect.However,itishardtoobtainincorrectsentencesbecauseonlycorrectsentencesareavailablefromthecorpus.Thisproblemwasnotanissueforprevi-ousstudiesbecausetheywereconcernedwithspe-ciﬁcapplicationsandthereforewereabletoobtainrealnegativeexampleseasily.Forexample,Roark(2007)proposedadiscriminativelanguagemodel,inwhichamodelistrainedsothatacorrectsentenceshouldhavehigherscorethanothers.Thediffer-encebetweentheirapproachandoursisthatwedonotassumejustoneapplication.Moreover,theyhad75

Fori=1,2,...Chooseaword(cid:5)(cid:1)atrandomaccordingtothedistribution(cid:5)(cid:1)(cid:1)(cid:5)(cid:1) (cid:0)(cid:6)(cid:7)(cid:7)(cid:7)(cid:6)(cid:5)(cid:1) (cid:0)If(cid:5)(cid:1)(cid:4)"endofasentence"BreakEndEndFigure1:Sampleprocedureforpseudo-negativeex-amplestakenfromN-gramlanguagemodels.trainingsetsconsistingofonecorrectsentenceandmanyincorrectsentences,whichwereverysimilarbecausetheyweregeneratedbythesameinput.Ourframeworkdoesnotassumeanysuchtrainingsets,andwetreatcorrectorincorrectexamplesindepen-dentlyintraining.3DiscriminativeLanguageModelwithPseudo-NegativesamplesWeproposeanoveldiscriminativelanguagemodel;aDiscriminativeLanguageModelwithPseudo-Negativesamples(DLM-PN).Inthismodel,pseudo-negativeexamples,whichareallassumedtobeincorrect,aresampledfromPLMs.FirstaPLMisbuiltusingtrainingdataandthenexamples,whicharealmostallnegative,aresam-pledindependentlyfromPLMs.DLMsaretrainedusingcorrectsentencesfromacorpusandnegativeexamplesfromaPseudo-Negativegenerator.Anadvantageofsamplingisthatasmanynega-tiveexamplescanbecollectedascorrectones,andadistinctioncanbeclearlymadebetweentrulycor-rectsentencesandincorrectsentences,eventhoughthelattermightbecorrectinalocalsense.Forsampling,anyPLMscanbeusedaslongasthemodelsupportsasentencesamplingproce-dure.InthisresearchweusedNLMswithinterpo-latedsmoothingbecausesuchmodelssupportefﬁ-cientsentencesampling.Figure1describesthesam-plingprocedureandﬁgure2showsanexampleofapseudo-negativesentence.Sincethefocusisondiscriminatingbetweencor-rectsentencesfromacorpusandincorrectsentencessampledfromtheNLM,DLM-PNmaynotabletoclassifyincorrectsentencesthatarenotgeneratedfromtheNLM.However,thisdoesnotresultinase-Weknowofnoprogram,andanimateddiscussionsaboutprospectsfortradebarriersorregulationsontherulesofthegameasawhole,andelementsofdecorationofthispeanut-shapedtoprioritiestasksacrossbothtargetcountriesFigure2:ExampleofasentencesampledbyPLMs(Trigram).CorpusBuild a probabilistic language modelSample sentencesPositive(Pseudo-) NegativeBinary Classifiertest sentencesReturn positive/negative label or score (margin)Input training examplesProbabilistic LM(e.g. N-gram LM)Figure3:Frameworkofourclassiﬁcationprocess.riousproblem,becausethesesentences,iftheyexist,canbeﬁlteredoutbyNLMs.4Onlinemargin-basedlearningwithfastkernelcomputationTheDLM-PNcanbetrainedbyusinganybinaryclassiﬁcationlearningmethods.However,sincethenumberoftrainingexamplesisverylarge,batchtraininghassufferedfromprohibitivelylargecom-putationalcostintermsoftimeandmemory.There-forewemakeuseofanonlinelearningalgorithmproposedby(Crammeretal.,2006),whichhasamuchsmallercomputationalcost.Wefollowthedeﬁnitionin(Crammeretal.,2006).Theinitiationvector(cid:0)(cid:0)isinitializedto(cid:1)andforeachroundthealgorithmobservesatrainingexam-ple(cid:2)(cid:1)(cid:3)(cid:4)(cid:13)(cid:1)(cid:1)andpredictsitslabel(cid:14)(cid:1)(cid:1)tobeeither(cid:2)or (cid:2).Afterthepredictionismade,thetruela-bel(cid:14)(cid:1)isrevealedandthealgorithmsuffersaninstan-taneoushinge-loss (cid:0)(cid:7)(cid:2)(cid:1)(cid:6)(cid:14)(cid:1)(cid:4)(cid:2) (cid:14)(cid:1)(cid:0)(cid:1)(cid:2)(cid:1)whichreﬂectsthedegreetowhichitspredictionwaswrong.Ifthepredictionwaswrong,theparameter76

(cid:0)isupdatedas(cid:0)(cid:1)(cid:0)(cid:4)(cid:17)(cid:19)(cid:15)(cid:0)(cid:2)(cid:8)(cid:1)(cid:1)(cid:0) (cid:0)(cid:1)(cid:1)(cid:1)(cid:1)(cid:22)(cid:23)(3)subjectto (cid:0)(cid:7)(cid:2)(cid:1)(cid:6)(cid:14)(cid:1)(cid:5)(cid:23)and(cid:23)(cid:6)(cid:5)(cid:6)(4)where(cid:23)isaslacktermand(cid:22)isapositiveparameterwhichcontrolstheinﬂuenceoftheslacktermontheobjectivefunction.Alargevalueof(cid:22)willresultinamoreaggressiveupdatestep.Thishasaclosedformsolutionas(cid:0)(cid:1)(cid:0)(cid:4)(cid:0)(cid:1)(cid:24)(cid:1)(cid:14)(cid:1)(cid:2)(cid:1)(5)where(cid:24)(cid:1)(cid:4)(cid:10)(cid:2)(cid:22)(cid:6) (cid:0)(cid:2)(cid:1)(cid:0)(cid:2)(cid:0)(cid:3).AsinSVMs,aﬁ-nalweightvectorcanberepresentedasakernel-dependentcombinationofthestoredtrainingexam-ples.(cid:0)(cid:2)(cid:4)(cid:1)(cid:1)(cid:24)(cid:1)(cid:14)(cid:1)(cid:7)(cid:2)(cid:1)(cid:2)(cid:8)(6)Usingthisformulationtheinnerproductcanbere-placedwithageneralMercerkernel(cid:2)(cid:1)(cid:6)(cid:2)suchasapolynomialkerneloraGaussiankernel.Thecombinationoffeatures,whichcancapturecorrelationinformation,isimportantinDLMs.Ifthekernel-trick(TaylorandCristianini,2004)isap-pliedtoonlinemargin-basedlearning,asubsetoftheobservedexamples,calledtheactiveset,needstobestored.HoweverincontrasttothesupportsetinSVMs,anexampleisaddedtotheactiveseteverytimetheonlinealgorithmmakesapredictionmis-takeorwhenitsconﬁdenceinapredictionisinad-equatelylow.Thereforetheactivesetcanincreaseinsizesigniﬁcantlyandthusthetotalcomputationalcostbecomesproportionaltothesquareofthenum-beroftrainingexamples.Sincethenumberoftrain-ingexamplesisverylarge,thecomputationalcostisprohibitiveevenifweapplythekerneltrick.Thecalculationoftheinnerproductbetweentwoexamplescanbedonebyintersectionoftheacti-vatedfeaturesineachexample.Thisissimilartoamergesortandcanbeexecutedintimewhereistheaveragenumberofactivatedfea-turesinanexample.Whenthenumberofexamplesintheactivesetis(cid:28),thetotalcomputationalcostis(cid:28).Forfastkernelcomputation,thePoly-nomialKernelInvertedmethod(PKI))isproposed(KudoandMatsumoto,2003),whichisanexten-sionofInvertedIndexinInformationRetrieval.Thisalgorithmusesatable(cid:29)(cid:9)(cid:1)foreachfeatureitem,whichstoresexampleswhereafeature(cid:9)(cid:1)isﬁred.Let(cid:30)betheaverageof(cid:1)(cid:29)(cid:9)(cid:1)(cid:1)overallfeatureitem.Thenthekernelcomputationcanbeperformedin(cid:30)timewhichismuchlessthanthenormalkernelcomputationtimewhen(cid:30)(cid:9)(cid:28).Wecaneas-ilyextendthisalgorithmintotheonlinesettingbyupdating(cid:29)(cid:9)(cid:1)whenanobservedexampleisaddedtoanactiveset.5Latentfeaturesbysemi-MarkovclassmodelAnotherproblemforDLMsisthatthenumberoffeaturesbecomesverylarge,becauseallpossibleN-gramsareusedasfeatures.Inparticular,themem-oryrequirementbecomesaseriousproblembecausequiteafewactivesetswithmanyfeatureshavetobestored,notonlyattrainingtime,butalsoatclassi-ﬁcationtime.Onewaytodealwiththisistoﬁlteroutlow-conﬁdencefeatures,butitisdifﬁculttode-cidewhichfeaturesareimportantinonlinelearning.ForthisreasonweclustersimilarN-gramsusingasemi-Markovclassmodel.Theclassmodelwasoriginallyproposedby(Mar-tinetal.,1998).Intheclassmodel,determinis-ticword-to-classmappingsareestimated,keepingthenumberofclassesmuchsmallerthanthenum-berofdistinctwords.Asemi-Markovclassmodel(SMCM)isanextendedversionoftheclassmodel,apartofwhichwasproposedby(DeligneandBIM-BOT,1995).InSMCM,awordsequenceispar-titionedintoavariable-lengthsequenceofchunksandthenchunksareclusteredintoclasses(Figure4).Howachunkisclustereddependsonwhichchunksareadjacenttoit.Theprobabilityofasentence(cid:5)(cid:0)(cid:6)(cid:7)(cid:7)(cid:7)(cid:6)(cid:5),inabi-gramclassmodeliscalculatedby(cid:0)(cid:1)(cid:5)(cid:1)(cid:0)(cid:1)(cid:31)(cid:1)(cid:0)(cid:31)(cid:1)(cid:0)(cid:1)(cid:31)(cid:1)(cid:7)(7)Ontheotherhand,theprobabilitiesinabi-gramsemi-Markovclassmodelarecalculatedby(cid:1)(cid:0)(cid:1)(cid:31)(cid:1)(cid:1)(cid:31)(cid:1) (cid:0)(cid:5)(cid:1)(cid:8)(cid:1)(cid:0)(cid:8)(cid:2)(cid:2)(cid:2)(cid:8)	(cid:1)(cid:1)(cid:31)(cid:1)(cid:7)(8)wherevariesoverallpossiblepartitionsof(cid:1),(cid:15)and	(cid:15)denotethestartandendpositionsrespec-tivelyofthe(cid:15)-thchunkinpartition,and(cid:15)(cid:2)(cid:4)77

	(cid:15)(cid:2)forall(cid:15).Notethateachwordorvariable-lengthchunkbelongstoonlyoneclass,incontrasttoahiddenMarkovmodelwhereeachwordcanbe-longtoseveralclasses.Usingatrainingcorpus,themappingisestimatedbymaximumlikelihoodestimation.Theloglike-lihoodofthetrainingcorpus((cid:5)(cid:0)(cid:6)(cid:7)(cid:7)(cid:7)(cid:6)(cid:5))inabi-gramclassmodelcanbecalculatedas (cid:14)(cid:0)(cid:1)(cid:5)(cid:1)(cid:0)(cid:1)(cid:5)(cid:1)(9)(cid:4)(cid:1)(cid:1) (cid:14)(cid:5)(cid:1)(cid:0)(cid:1)(cid:31)(cid:1)(cid:0)(cid:31)(cid:1)(cid:0)(cid:1)(cid:31)(cid:1)(10)(cid:4)(cid:1)(cid:11)(cid:1)(cid:8)(cid:11)(cid:0)"(cid:31)(cid:0)(cid:6)(cid:31)(cid:1) (cid:14)"(cid:31)(cid:0)(cid:6)(cid:31)(cid:1)"(cid:31)(cid:0)"(cid:31)(cid:1)(11)(cid:1)(cid:12)"(cid:5) (cid:14)"(cid:5)(cid:7)where"(cid:5),"(cid:31)and"(cid:31)(cid:0)(cid:6)(cid:31)(cid:1)arefrequenciesofaword(cid:5),aclass(cid:31)andaclassbi-gram(cid:31)(cid:0)(cid:6)(cid:31)(cid:1)inthetrainingcorpus.In(11)onlytheﬁrsttermisused,sincethesecondtermdoesnotdependontheclassallocation.Theclassallocationproblemissolvedbyanexchangealgorithmasfollows.First,allwordsareassignedtoarandomlydeterminedclass.Next,foreachword(cid:5),wemoveittotheclass(cid:31)forwhichthelog-likelihoodismaximized.Thisprocedureiscontinueduntilthelog-likelihoodconvergestoalo-calmaximum.Anaiveimplementationoftheclus-teringalgorithmscalesquadraticallytothenumberofclasses,sinceeachtimeawordismovedbetweenclasses,allclassbi-gramcountsarepotentiallyaf-fected.However,byconsideringonlythosecountsthatactuallychange,thealgorithmcanbemadetoscalesomewherebetweenlinearlyandquadraticallytothenumberofclasses(Martinetal.,1998).InSMCM,partitionsofeachsentencearealsode-termined.WeusedaViterbidecoding(DeligneandBIMBOT,1995)forthepartition.WeappliedtheexchangealgorithmandtheViterbidecodingalter-natelyuntilthelog-likelihoodconvergedtothelocalmaximum.Sincethenumberofchunksisverylarge,forex-ample,inourexperimentsweusedabout(cid:15)millionchunks,thecomputationalcostisstilllarge.Wethereforeemployedthefollowingtwotechniques.Theﬁrstwastoapproximatethecomputationintheexchangealgorithm;thesecondwastomakeuseofw1  w2  w3  w4  w5  w6  w7  w8c1c2c3c4Figure4:Exampleofassignmentinsemi-Markovclassmodel.Asentenceispartitionedintovariable-lengthchunksandeachchunkisassignedauniqueclassnumber.bottom-upclusteringtostrengthentheconvergence.Ineachstepintheexchangealgorithm,theap-proximatevalueofthechangeofthelog-likelihoodwasexamined,andtheexchangealgorithmappliedonlyiftheapproximatevaluewaslargerthanapre-deﬁnedthreshold.Thesecondtechniquewastoreducememoryre-quirements.Sincethematricesusedintheexchangealgorithmcouldbecomeverylarge,weclusteredchunksinto(cid:8)classesandthenagainweclusteredthesetwointo(cid:8)each,thusobtaining(cid:16)classes.Thisprocedurewasappliedrecursivelyuntilthenumberofclassesreachedapre-deﬁnednumber.6Experiments6.1ExperimentalSetupWepartitionedaBNC-corpusintomodel-train,DLM-train-positive,andDLM-test-positivesets.Thenumbersofsentencesinmodel-train,DLM-train-positiveandDLM-test-positivewere(cid:16)(cid:17)(cid:5)(cid:5)k,(cid:8)(cid:17)(cid:5)k,and(cid:2)(cid:5)krespectively.AnNLMwasbuiltusingmodel-trainandPseudo-Negativeexamples((cid:8)(cid:17)(cid:5)ksentences)weresampledfromit.WemixedsentencesfromDLM-train-positiveandthePseudo-NegativeexamplesandthenshufﬂedtheorderofthesesentencestomakeDLM-train.Wealsocon-structedDLM-testbymixingDLM-test-positiveand(cid:2)(cid:5)knew(notalreadyused)sentencesfromthePseudo-Negativeexamples.WecallthesentencesfromDLM-train-positive“positive”examplesandthesentencesfromthePseudo-Negativeexamples“negative”examplesinthefollowing.Fromthesesentencestheoneswithlessthan(cid:17)wordswereex-cludedbeforehandbecauseitwasdifﬁculttodecidewhetherthesesentenceswerecorrectornot(e.g.78

Accuracy(%)Trainingtime(s)Linearclassiﬁerwordtri-gram51.28137.1POStri-gram52.6485.0SMCMbi-gram(#(cid:4)(cid:2)(cid:5)(cid:5))51.79304.9SMCMbi-gram(#(cid:4)(cid:17)(cid:5)(cid:5))54.45422.1(cid:15)rdorderPolynomialKernelwordtri-gram73.6520143.7POStri-gram66.5829622.9SMCMbi-gram(#(cid:4)(cid:2)(cid:5)(cid:5))67.1137181.6SMCMbi-gram(#(cid:4)(cid:17)(cid:5)(cid:5))74.1134474.7Table1:Performanceontheevaluationdata.compoundwords).Let#bethenumberofclassesinSMCMs.TwoSMCMs,onewith#(cid:4)(cid:2)(cid:5)(cid:5)andtheotherwith#(cid:4)(cid:17)(cid:5)(cid:5),wereconstructedfrommodel-train.EachSMCMcontained(cid:8)(cid:7)(cid:18)millionextractedchunks.6.2ExperimentsonPseudo-ExamplesWeexaminedthepropertyofasentencebeingPseudo-Negative,inordertojustifyourframework.AnativeEnglishspeakerandtwonon-nativeEn-glishspeakerwereaskedtoassigncorrect/incorrectlabelsto(cid:2)(cid:5)(cid:5)sentencesinDLM-train1.TheresultforannativeEnglishspeakerwasthatallpositivesentenceswerelabeledascorrectandallnegativesentencesexceptforonewerelabeledasincorrect.Ontheotherhand,theresultsfornon-nativeEnglishspeakersare67and70.Fromthisresult,wecansaythatthesamplingmethodwasabletogen-erateincorrectsentencesandifaclassiﬁercandis-criminatethem,theclassiﬁercanalsodiscriminatebetweencorrectandincorrectsentences.Notethatittakesanaverageof25secondsforthenativeEn-glishspeakertoassignthelabel,whichsuggeststhatitisdifﬁcultevenforahumantodeterminethecor-rectnessofasentence.Wethenexaminedwhetheritwaspossibletodis-criminatebetweencorrectandincorrectsentencesusingparsingmethods,sinceifso,wecouldhaveusedparsingasaclassiﬁcationtool.Weexam-ined(cid:2)(cid:5)(cid:5)sentencesusingaphrasestructureparser(CharniakandJohnson,2005)andanHPSGparser1SincethePLMalsomadeuseoftheBNC-corpusforposi-tiveexamples,wewerenotabletoclassifysentencesbasedonwordoccurrences(MiyaoandTsujii,2005).Allsentenceswereparsedcorrectlyexceptforonepositiveexample.Thisresultindicatesthatcorrectsentencesandpseudo-negativeexamplescannotbedifferentiatedsyntacti-cally.6.3ExperimentsonDLM-PNWeinvestigatedtheperformanceofclassiﬁersandtheeffectofdifferentsetsoffeatures.ForN-gramsandPartofSpeech(POS),weusedtri-gramfeatures.ForSMCM,weusedbi-gramfea-tures.WeusedDLM-trainasatrainingset.Inallexperiments,weset(cid:22)(cid:4)(cid:17)(cid:5)(cid:7)(cid:5)where(cid:22)isaparame-terintheclassiﬁcation(Section4).Inallkernelex-periments,a(cid:15)rdorderpolynomialkernelwasusedandvalueswerecomputedusingPKI(theinvertedindexingmethod).Table1showstheaccuracyre-sultswithdifferentfeatures,orinthecaseoftheSMCMs,differentnumbersofclasses.Thisresultshowsthatthekernelmethodisimportantinachiev-inghighperformance.NotethattheclassiﬁerwithSMCMfeaturesperformsaswellastheonewithword.Table2showsthenumberoffeaturesineachmethod.Notethatanewfeatureisaddedonlyiftheclassiﬁerneedstoupdateitsparameters.Thesenum-bersarethereforesmallerthanthepossiblenumberofallcandidatefeatures.Thisresultandtheprevi-ousresultindicatethatSMCMachieveshighperfor-mancewithveryfewfeatures.WethenexaminedtheeffectofPKI.Table3showstheresultsoftheclassiﬁerwith(cid:15)rdorderpolynomialkernelbothwithandwithoutPKI.Inthisexperiment,only(cid:8)(cid:5)(cid:5)sentencesinDLM-train79

#ofdistinctfeatureswordtri-gram15773230POStri-gram35376SMCM(#(cid:4)(cid:2)(cid:5)(cid:5))9335SMCM(#(cid:4)(cid:17)(cid:5)(cid:5))199745Table2:Thenumberoffeatures.trainingtime(s)predictiontime(ms)Baseline37665.5370.6+Index4664.947.8Table3:Comparisonbetweenclassiﬁcationperfor-mancewith/withoutindex0100200-3-2-10123MarginNumber of sentencesnegativepositiveFigure5:MargindistributionusingSMCMbi-gramfeatures.wereusedforbothexperimentsbecausetrainingus-ingallthetrainingdatawouldhaverequiredamuchlongertimethanwaspossiblewithourexperimentalsetup.Figure5showsthemargindistributionforpos-itiveandnegativeexamplesusingSMCMbi-gramfeatures.Althoughmanyexamplesareclosetotheborderline(margin(cid:4)(cid:5)),positiveandnegativeex-amplesaredistributedoneithersideof(cid:5).Thereforehigherrecallorprecisioncouldbeachievedbyusingapre-deﬁnedmarginthresholdotherthan(cid:5).Finally,wegeneratedlearningcurvestoexaminetheeffectofthesizeoftrainingdataonperformance.Figure6showstheresultoftheclassiﬁcationtaskusingSMCM-bi-gramfeatures.Theresultsuggeststhattheperformancecouldbefurtherimprovedbyenlargingthetrainingdataset.5055606570758050003500065000950001E+052E+052E+052E+052E+053E+053E+053E+054E+054E+054E+055E+055E+05Number of training examplesAccuracy (%)Figure6:AlearningcurveforSMCM(#(cid:4)(cid:17)(cid:5)(cid:5)).Theaccuracyisthepercentageofsentencesintheevaluationsetclassiﬁedcorrectly.7DiscussionExperimentalresultsonpseudo-negativeexamplesindicatethatcombinationoffeaturesiseffectiveinasentencediscriminationmethod.Thiscouldbebecausenegativeexamplesincludemanyunsuitablecombinationsofwordssuchasasentencecontain-ingmanynouns.AlthoughinpreviousPLMs,com-binationoffeatureshasnotbeendiscussedexceptforthetopic-basedlanguagemodel(DavidM.Blei,2003;Wangetal.,2005),ourresultmayencouragethestudyofthecombinationoffeaturesforlanguagemodeling.Acontrastiveestimationmethod(SmithandEis-ner,2005)issimilartoourswithregardtoconstruct-ingpseudo-negativeexamples.Theybuildaneigh-borhoodofinputexamplestoallowunsupervisedes-timationwhen,forexample,awordischangedordeleted.Alatticeisconstructed,andthenparame-tersareestimatedefﬁciently.Ontheotherhand,weconstructindependentpseudo-negativeexamplestoenabletraining.Althoughthemotivationsofthesestudiesaredifferent,wecouldcombinethesetwomethodstodiscriminatesentencesﬁnely.Inourexperiments,wedidnotexaminetheresultofusingothersamplingmethods,Forexample,itwouldbepossibletosamplesentencesfromawholesentencemaximumentropymodel(Rosenfeldetal.,2001)andthisisatopicforfutureresearch.80

8ConclusionInthispaperwehavepresentedanoveldiscrimi-nativelanguagemodelusingpseudo-negativeexam-ples.Wealsoshowedthatanonlinemargin-basedlearningmethodenabledustousehalfamillionsen-tencesastrainingdataandachieve(cid:20)(cid:16)accuracyinthetaskofdiscriminationbetweencorrectandin-correctsentences.Experimentalresultsindicatethatwhilepseudo-negativeexamplescanbeseenasin-correctsentences,theyarealsoclosetocorrectsen-tencesinthatparserscannotdiscriminatebetweenthem.Ourexperimentalresultsalsoshowedthatcom-binationoffeaturesisimportantfordiscriminationbetweencorrectandincorrectsentences.Thiscon-cepthasnotbeendiscussedinpreviousprobabilisticlanguagemodels.Ournextstepistoemployourmodelinmachinetranslationandspeechrecognition.Onemaindifﬁ-cultyconcernshowtoencodeglobalscoresfortheclassiﬁerinthelocalsearchspace,andanotherishowtoscaleuptheproblemsizeintermsofthenumberofexamplesandfeatures.Wewouldliketoseemorereﬁnedonlinelearningmethodswithker-nels(Chengetal.,2006;Dekeletal.,2005)thatwecouldapplyintheseareas.Wearealsointerestedinapplicationssuchascon-structinganextendedversionofaspellingcorrec-tiontoolbyidentifyingincorrectsentences.Anotherinterestingideaistoworkwithproba-bilisticlanguagemodelsdirectlywithoutsamplingandﬁndwaystoconstructamoreaccuratediscrim-inativemodel.ReferencesEugeneCharniakandMarkJohnson.2005.Coarse-to-ﬁnen-bestparsingandmaxentdiscriminativererank-ing.InProc.ofACL05,pages173–180,June.StanleyF.ChenandJoshuaGoodman.1998.Anempir-icalstudyofsmoothingtechniquesforlanguagemod-eling.Technicalreport,HarvardComputerScienceTechnicalreportTR-10-98.LiCheng,SVNVishwanathan,DaleSchuurmans,Shao-junWang,andTerryCaelli.2006.Implicitonlinelearningwithkernels.InNIPS2006.KobyCrammer,OferDekel,JosephKeshet,ShaiShalev-Shwartz,andYoramSinger.2006.Onlinepassive-aggressivealgorithms.JournalofMachineLearningResearch.MichaelI.JordanDavidM.Blei,AndrewY.Ng.2003.Latentdirichletallocation.JournalofMachineLearn-ingResearch.,3:993–1022.OferDekel,ShaiShalev-Shwartz,andYoramSinger.2005.Theforgetron:Akernel-basedperceptrononaﬁxedbudget.InProc.ofNIPS.SabineDeligneandFr´ed´ericBIMBOT.1995.Languagemodelingbyvariablelengthsequences:Theoreticalformulationandevaluationofmultigrams.InProc.ICASSP’95,pages169–172.JianfengGao,HaoYu,WeiYuan,andPengXu.2005.Minimumsampleriskmethodsforlanguagemodeling.InProc.ofHLT/EMNLP.TakuKudoandYujiMatsumoto.2003.Fastmethodsforkernel-basedtextanalysis.InACL.SvenMartin,J¨orgLiermann,andHermannNey.1998.Algorithmsforbigramandtrigramwordclustering.SpeechCommunicatoin,24(1):19–37.YusukeMiyaoandJun’ichiTsujii.2005.Probabilisticdisambiguationmodelsforwide-coveragehpsgpars-ing.InProc.ofACL2005.,pages83–90,AnnArbor,Michigan,June.BrianRoark,MuratSaraclar,andMichaelCollins.2007.Discriminativen-gramlanguagemodeling.computerspeechandlanguage.ComputerSpeechandLan-guage,21(2):373–392.RoniRosenfeld,StanleyF.Chen,andXiaojinZhu.2001.Whole-sentenceexponentiallanguagemodels:ave-hicleforlinguistic-statisticalintegration.ComputersSpeechandLanguage,15(1).NoahA.SmithandJasonEisner.2005.Contrastiveesti-mation:Traininglog-linearmodelsonunlabeleddata.InProc.ofACL.JohnS.TaylorandNello.Cristianini.2004.KernelMethodsforPatternAnalysis.CambiridgeUnivsityPress.ShaojunWang,ShaominWang,RussellGreiner,DaleSchuurmans,andLiCheng.2005.Exploitingsyntac-tic,semanticandlexicalregularitiesinlanguagemod-elingviadirectedmarkovrandomﬁelds.InProc.ofICML.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 81–88,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

81

DetectingErroneousSentencesusingAutomaticallyMinedSequentialPatternsGuihuaSun∗XiaohuaLiuGaoCongMingZhouChongqingUniversityMicrosoftResearchAsiasunguihua5018@163.com{xiaoliu,gaocong,mingzhou}@microsoft.comZhongyangXiongJohnLee†Chin-YewLinChongqingUniversityMITMicrosoftResearchAsiazyxiong@cqu.edu.cnjsylee@mit.educyl@microsoft.comAbstractThispaperstudiestheproblemofidentify-ingerroneous/correctsentences.Theprob-lemhasimportantapplications,e.g.,pro-vidingfeedbackforwritersofEnglishasaSecondLanguage,controllingthequalityofparallelbilingualsentencesminedfromtheWeb,andevaluatingmachinetranslationresults.Inthispaper,weproposeanewapproachtodetectingerroneoussentencesbyintegratingpatterndiscoverywithsuper-visedlearningmodels.Experimentalresultsshowthatourtechniquesarepromising.1IntroductionDetectingerroneous/correctsentenceshasthefol-lowingapplications.First,itcanprovidefeedbackforwritersofEnglishasaSecondLanguage(ESL)astowhetherasentencecontainserrors.Second,itcanbeappliedtocontrolthequalityofparallelbilin-gualsentencesminedfromtheWeb,whicharecriti-calsourcesforawiderangeofapplications,suchasstatisticalmachinetranslation(Brownetal.,1993)andcross-lingualinformationretrieval(Nieetal.,1999).Third,itcanbeusedtoevaluatemachinetranslationresults.Asdemonstratedin(Corston-Oliveretal.,2001;Gamonetal.,2005),thebetterhumanreferencetranslationscanbedistinguishedfrommachinetranslationsbyaclassiﬁcationmodel,theworsethemachinetranslationsystemis.∗WorkdonewhiletheauthorwasavisitingstudentatMSRA†WorkdonewhiletheauthorwasavisitingstudentatMSRAThepreviousworkonidentifyingerroneoussen-tencesmainlyaimstoﬁnderrorsfromthewritingofESLlearners.Thecommonmistakes(Yukioetal.,2001;GuiandYang,2003)madebyESLlearnersincludespelling,lexicalcollocation,sentencestruc-ture,tense,agreement,verbformation,wrongPart-Of-Speech(POS),articleusage,etc.Thepreviousworkfocusesongrammarerrors,includingtense,agreement,verbformation,articleusage,etc.How-ever,littleworkhasbeendonetodetectsentencestructureandlexicalcollocationerrors.Somemethodsofdetectingerroneoussentencesarebasedonmanualrules.Thesemethods(Hei-dorn,2000;Michaudetal.,2000;Benderetal.,2004)havebeenshowntobeeffectiveindetect-ingcertainkindsofgrammaticalerrorsinthewrit-ingofEnglishlearners.However,itcouldbeex-pensivetowriterulesmanually.Linguisticexpertsareneededtowriterulesofhighquality;Also,itisdifﬁculttoproduceandmaintainalargenum-berofnon-conﬂictingrulestocoverawiderangeofgrammaticalerrors.Moreover,ESLwritersofdiffer-entﬁrst-languagebackgroundsandskilllevelsmaymakedifferenterrors,andthusdifferentsetsofrulesmayberequired.Worsestill,itishardtowriterulesforsomegrammaticalerrors,forexample,detectingerrorsconcerningthearticlesandsingularpluralus-age(Nagataetal.,2006).Insteadofaskingexpertstowritehand-craftedrules,statisticalapproaches(ChodorowandLea-cock,2000;Izumietal.,2003;Brockettetal.,2006;Nagataetal.,2006)buildstatisticalmodelstoiden-tifysentencescontainingerrors.However,existing82

statisticalapproachesfocusonsomepre-deﬁneder-rorsandthereportedresultsarenotattractive.More-over,theseapproaches,e.g.,(Izumietal.,2003;Brockettetal.,2006)usuallyneederrorstobespec-iﬁedandtaggedinthetrainingsentences,whichre-quiresexperthelptoberecruitedandistimecon-sumingandlaborintensive.Consideringthelimitationsofthepreviouswork,inthispaperweproposeanovelapproachthatisbasedonpatterndiscoveryandsupervisedlearn-ingtosuccessfullyidentifyerroneous/correctsen-tences.Thebasicideaofourapproachistobuildamachinelearningmodeltoautomaticallyclassifyeachsentenceintooneofthetwoclasses,“erro-neous”and“correct.”Tobuildthelearningmodel,weautomaticallyextractlabeledsequentialpatterns(LSPs)frombotherroneoussentencesandcorrectsentences,andusethemasinputfeaturesforclassi-ﬁcationmodels.Ourmaincontributionsare:•Weminelabeledsequentialpatterns(LSPs)fromthepreprocessedtrainingdatatobuildleaningmodels.NotethatLSPsarealsoverydifferentfromN-gramlanguagemodelsthatonlyconsidercontinuoussequences.•WealsoenrichtheLSPfeatureswithotherauto-maticallycomputedlinguisticfeatures,includ-inglexicalcollocation,languagemodel,syn-tacticscore,andfunctionworddensity.Incon-trastwithpreviousworkfocusingon(aspe-ciﬁctypeof)grammaticalerrors,ourmodelcanhandleawiderangeoferrors,includinggram-mar,sentencestructure,andlexicalchoice.•WeempiricallyevaluateourmethodsontwodatasetsconsistingofsentenceswrittenbyJapaneseandChinese,respectively.Experi-mentalresultsshowthatlabeledsequentialpat-ternsarehighlyusefulfortheclassiﬁcationresults,andgreatlyoutperformotherfeatures.OurmethodoutperformsMicrosoftWord03andALEK(ChodorowandLeacock,2000)fromEducationalTestingService(ETS)insomecases.Wealsoapplyourlearningmodeltomachinetranslation(MT)dataasacomple-mentarymeasuretoevaluateMTresults.Therestofthispaperisorganizedasfollows.Thenextsectiondiscussesrelatedwork.Section3presentstheproposedtechnique.WeevaluateourproposedtechniqueinSection4.Section5con-cludesthispaperanddiscussesfuturework.2RelatedWorkResearchondetectingerroneoussentencescanbeclassiﬁedintotwocategories.Theﬁrstcategorymakesuseofhand-craftedrules,e.g.,templaterules(Heidorn,2000)andmal-rulesincontext-freegrammars(Michaudetal.,2000;Benderetal.,2004).AsdiscussedinSection1,manualrulebasedmethodshavesomeshortcomings.Thesecondcategoryusesstatisticaltechniquestodetecterroneoussentences.Anunsupervisedmethod(ChodorowandLeacock,2000)isem-ployedtodetectgrammaticalerrorsbyinferringnegativeevidencefromTOEFLadministratedbyETS.Themethod(Izumietal.,2003)aimstode-tectomission-typeandreplacement-typeerrorsandtransformation-basedleaningisemployedin(ShiandZhou,2005)tolearnrulestodetecterrorsforspeechrecognitionoutputs.Theyalsorequirespec-ifyingerrortagsthatcantellthespeciﬁcerrorsandtheircorrectionsinthetrainingcorpus.ThephrasalStatisticalMachineTranslation(SMT)tech-niqueisemployedtoidentifyandcorrectwritinger-rors(Brockettetal.,2006).Thismethodmustcol-lectalargenumberofparallelcorpora(pairsofer-roneoussentencesandtheircorrections)andperfor-mancedependsonSMTtechniquesthatarenotyetmature.Theworkin(Nagataetal.,2006)focusesonatypeoferror,namelymassvs.countnouns.Incontrasttoexistingstatisticalmethods,ourtech-niqueneedsneithererrorstaggednorparallelcor-pora,andisnotlimitedtoaspeciﬁctypeofgram-maticalerror.Therearealsostudiesonautomaticessayscoringatdocument-level.Forexample,E-rater(Bursteinetal.,1998),developedbytheETS,andIntelligentEssayAssessor(Foltzetal.,1999).Theevaluationcriteriafordocumentsaredifferentfromthoseforsentences.Adocumentisevaluatedmainlybyitsor-ganization,topic,diversityofvocabulary,andgram-marwhileasentenceisdonebygrammar,sentencestructure,andlexicalchoice.AnotherrelatedworkisMachineTranslation(MT)evaluation.Classiﬁcationmodelsareemployedin(Corston-Oliveretal.,2001;Gamonetal.,2005)83

toevaluatethewell-formednessofmachinetransla-tionoutputs.ThewritersofESLandMTnormallymakedifferentmistakes:ingeneral,ESLwriterscanwriteoverallgrammaticallycorrectsentenceswithsomelocalmistakeswhileMToutputsnormallypro-ducelocallywell-formedphraseswithoverallgram-maticallywrongsentences.Hence,themanualfea-turesdesignedforMTevaluationarenotapplicabletodetecterroneoussentencesfromESLlearners.LSPsdifferfromthetraditionalsequentialpat-terns,e.g.,(AgrawalandSrikant,1995;Peietal.,2001)inthatLSPsareattachedwithclasslabelsandwepreferthosewithdiscriminatingabilitytobuildclassiﬁcationmodel.Inourotherwork(Sunetal.,2007),labeledsequentialpatterns,togetherwithla-beledtreepatterns,areusedtobuildpattern-basedclassiﬁertodetecterroneoussentences.Theclas-siﬁcationmethodin(Sunetal.,2007)isdifferentfromthoseusedinthispaper.Moreover,insteadoflabeledsequentialpatterns,in(Sunetal.,2007)themostsigniﬁcantklabeledsequentialpatternswithconstraintsforeachtrainingsentenceareminedtobuildclassiﬁers.Anotherrelatedworkis(JindalandLiu,2006),wheresequentialpatternswithlabelsareusedtoidentifycomparativesentences.3ProposedTechniqueThissectionﬁrstgivesourproblemstatementandthenpresentsourproposedtechniquetobuildlearn-ingmodels.3.1ProblemStatementInthispaperwestudytheproblemofidentifyingerroneous/correctsentences.Asetoftrainingdatacontainingcorrectanderroneoussentencesisgiven.Unlikesomepreviouswork,ourtechniquerequiresneitherthattheerroneoussentencesaretaggedwithdetailederrors,northatthetrainingdataconsistofparallelpairsofsentences(anerrorsentenceanditscorrection).Theerroneoussentencecontainsawiderangeoferrorsongrammar,sentencestructure,andlexicalchoice.Wedonotconsiderspellingerrorsinthispaper.Weaddresstheproblembybuildingclassiﬁca-tionmodels.Themainchallengeistoautomaticallyextractrepresentativefeaturesforbothcorrectanderroneoussentencestobuildeffectiveclassiﬁcationmodels.Weillustratethechallengewithanexam-ple.Consideranerroneoussentence,“IfMaggiewillgotosupermarket,shewillbuyabagforyou.”Itisdifﬁcultforpreviousmethodsusingstatisticaltech-niquestocapturesuchanerror.Forexample,N-gramlanguagemodelisconsideredtobeeffectiveinwritingevaluation(Bursteinetal.,1998;Corston-Oliveretal.,2001).However,itbecomesveryex-pensiveifN>3andN-gramsonlyconsidercontin-uoussequenceofwords,whichisunabletodetecttheaboveerror“if...will...will”.Weproposelabeledsequentialpatternstoeffec-tivelycharacterizethefeaturesofcorrectander-roneoussentences(Section3.2),anddesignsomecomplementaryfeatures(Section3.3).3.2MiningLabeledSequentialPatterns(LSP)LabeledSequentialPatterns(LSP).Alabeledse-quentialpattern,p,isintheformofLHS→c,whereLHSisasequenceandcisaclasslabel.LetIbeasetofitemsandLbeasetofclasslabels.LetDbeasequencedatabaseinwhicheachtupleiscomposedofalistofitemsinIandaclasslabelinL.Wesaythatasequences1=<a1,...,am>iscontainedinasequences2=<b1,...,bn>ifthereexistintegersi1,...imsuchthat1≤i1<i2<...<im≤nandaj=bijforallj∈1,...,m.Similarly,wesaythataLSPp1iscontainedbyp2ifthesequencep1.LHSiscontainedbyp2.LHSandp1.c=p2.c.Notethatitisnotrequiredthats1appearscontinuouslyins2.Wewillfurtherreﬁnethedeﬁnitionof“contain”byimposingsomeconstraints(tobeexplainedsoon).ALSPpisattachedwithtwomeasures,supportandconﬁdence.Thesupportofp,denotedbysup(p),isthepercentageoftuplesindatabaseDthatcon-taintheLSPp.TheprobabilityoftheLSPpbeingtrueisreferredtoas“theconﬁdenceofp”,denotedbyconf(p),andiscomputedassup(p)sup(p.LHS).Thesupportistomeasurethegeneralityofthepatternpandminimumconﬁdenceisastatementofpredictiveabilityofp.Example1:Considerasequencedatabasecontain-ingthreetuplest1=(<a,d,e,f>,E),t2=(<a,f,e,f>,E)andt3=(<d,a,f>,C).OneexampleLSPp1=<a,e,f>→E,whichiscon-tainedintuplest1andt2.Itssupportis66.7%anditsconﬁdenceis100%.Asanotherexample,LSPp284

=<a,f>→Ewithsupport66.7%andconﬁdence66.7%.p1isabetterindicationofclassEthanp2.2GeneratingSequenceDatabase.WegeneratethedatabasebyapplyingPart-Of-Speech(POS)taggertotageachtrainingsentencewhilekeepingfunc-tionwords1andtimewords2.Aftertheprocess-ing,eachsentencetogetherwithitslabelbecomesadatabasetuple.ThefunctionwordsandPOStagsplayimportantrolesinbothgrammarsandsentencestructures.Inaddition,thetimewordsarekeycluesindetectingerrorsoftenseusage.Thecom-binationofthemallowsustocapturerepresentativefeaturesforcorrect/erroneoussentencesbyminingLSPs.SomeexampleLSPsinclude“<a,NNS>→Error”(singulardeterminerprecedingpluralnoun),and“<yesterday,is>→Error”.Notethatthecon-ﬁdencesoftheseLSPsarenotnecessary100%.First,weuseMXPOST-MaximumEntropyPartofSpeechTaggerToolkit3forPOStags.TheMXPOSTtaggercanprovideﬁne-grainedtaginformation.Forexample,nouncanbetaggedwith“NN”(singularnoun)and“NNS”(pluralnoun);verbcanbetaggedwith“VB”,”VBG”,”VBN”,”VBP”,”VBD”and”VBZ”.Second,thefunctionwordsandtimewordsthatweuseformakeywordlist.Ifawordinatrainingsentenceisnotcontainedinthekeywordlist,thenthewordwillbereplacedbyitsPOS.TheprocessedsentenceconsistsofPOSandthewordsofkeywordlist.Forexample,aftertheprocessing,thesentence“Inthepast,Johnwaskindtohissister”isconvertedinto“Inthepast,NNPwasJJtohisNN”,wherethewords“in”,“the”,“was”,“to”and“his”arefunctionwords,theword“past”istimeword,and“NNP”,“JJ”,and“NN”arePOStags.MiningLSPs.ThelengthofthediscoveredLSPsisﬂexibleandtheycanbecomposedofcontiguousordistantwords/tags.Existingfrequentsequentialpatternminingalgorithms(e.g.(Peietal.,2001))useminimumsupportthresholdtominefrequentse-quentialpatternswhosesupportislargerthanthethreshold.ThesealgorithmsarenotsufﬁcientforourproblemofminingLSPs.InordertoensurethatallourdiscoveredLSPsarediscriminatingandarecapa-1http://www.marlodge.supanet.com/museum/funcword.html2http://www.wjh.harvard.edu/%7Einquirer/Time%40.html3http://www.cogsci.ed.ac.uk/∼jamesc/taggers/MXPOST.htmlbleofpredictingcorrectorerroneoussentences,weimposeanotherconstraintminimumconﬁdence.Re-callthatthehighertheconﬁdenceofapatternis,thebetteritcandistinguishbetweencorrectsentencesanderroneoussentences.Inourexperiments,weempiricallysetminimumsupportat0.1%andmini-mumconﬁdenceat75%.MiningLSPsisnontrivialsinceitssearchspaceisexponential,althoughttherehavebeenahostofalgorithmsforminingfrequentsequentialpatterns.Weadaptthefrequentsequenceminingalgorithmin(Peietal.,2001)forminingLSPswithconstraints.ConvertingLSPstoFeatures.EachdiscoveredLSPformsabinaryfeatureastheinputforclassiﬁcationmodel.IfasentenceincludesaLSP,thecorrespond-ingfeatureissetat1.TheLSPscancharacterizethecorrect/erroneoussentencestructureandgrammar.Wegivesomeex-amplesofthediscoveredLSPs.(1)LSPsforerro-neoussentences.Forexample,“<this,NNS>”(e.g.containedin“thisbooksisstolen.”),“<past,is>”(e.g.containedin“inthepast,Johniskindtohissister.”),“<one,of,NN>”(e.g.containedin“itisoneofimportantworkinglanguage”,“<although,but>”(e.g.containedin“althoughhelikesit,buthecan’tbuyit.”),and“<only,if,I,am>”(e.g.con-tainedin“onlyifmyteacherhasgivenpermission,Iamallowedtoenterthisroom”).(2)LSPsforcor-rectsentences.Forinstance,“<would,VB>”(e.g.containedin“hewouldbuyit.”),and“<VBD,yeserday>”(e.g.containedin“Iboughtthisbookyesterday.”).3.3OtherLinguisticFeaturesWeusesomelinguisticfeaturesthatcanbecom-putedautomaticallyascomplementaryfeatures.LexicalCollocation(LC)Lexicalcollocationer-ror(Yukioetal.,2001;GuiandYang,2003)iscom-moninthewritingofESLlearners,suchas“strongtea”butnot“powerfultea.”OurLSPfeaturescan-notcaptureallLCssincewereplacesomewordswithPOStagsinminingLSPs.Wecollectﬁvetypesofcollocations:verb-object,adjective-noun,verb-adverb,subject-verb,andpreposition-objectfromageneralEnglishcorpus4.CorrectLCsarecollected4ThegeneralEnglishcorpusconsistsofabout4.4millionnativesentences.85

byextractingcollocationsofhighfrequencyfromthegeneralEnglishcorpus.ErroneousLCcandi-datesaregeneratedbyreplacingthewordincorrectcollocationswithitsconfusionwords,obtainedfromWordNet,includingsynonymsandwordswithsim-ilarspellingorpronunciation.Expertsareconsultedtoseeifacandidateisatrueerroneouscollocation.Wecomputethreestatisticalfeaturesforeachsen-tencebelow.(1)TheﬁrstfeatureiscomputedbymPi=1p(coi)/n,wheremisthenumberofCLs,nisthenumberofcollocationsineachsentence,andprobabilityp(coi)ofeachCLcoiiscalculatedus-ingthemethod(L¨uandZhou,2004).(2)Thesec-ondfeatureiscomputedbytheratioofthenumberofunknowncollocations(neithercorrectLCsnorer-roneousLCs)tothenumberofcollocationsineachsentence.(3)Thelastfeatureiscomputedbythera-tioofthenumberoferroneousLCstothenumberofcollocationsineachsentence.PerplexityfromLanguageModel(PLM)Perplex-itymeasuresareextractedfromatrigramlanguagemodeltrainedonageneralEnglishcorpususingtheSRILM-SRILanguageModelingToolkit(Stolcke,2002).Wecalculatetwovaluesforeachsentence:lexicalizedtrigramperplexityandpartofspeech(POS)trigramperplexity.Theerroneoussentenceswouldhavehigherperplexity.SyntacticScore(SC)Someerroneoussentencesof-tencontainwordsandconceptsthatarelocallycor-rectbutcannotformcoherentsentences(LiuandGildea,2005).Tomeasurethecoherenceofsen-tences,weuseastatisticalparserToolkit(Collins,1997)toassigneachsentenceaparser’sscorethatistherelatedlogprobabilityofparsing.Weassumethaterroneoussentenceswithundesirablesentencestructuresaremorelikelytoreceivelowerscores.FunctionWordDensity(FWD)Weconsiderthedensityoffunctionwords(Corston-Oliveretal.,2001),i.e.theratiooffunctionwordstocontentwords.Thisisinspiredbythework(Corston-Oliveretal.,2001)showingthatfunctionworddensitycanbeeffectiveindistinguishingbetweenhumanrefer-encesandmachineoutputs.Inthispaper,wecalcu-latethedensitiesofsevenkindsoffunctionwords55includingdeterminers/quantiﬁers,allpronouns,differentpronountypes:Wh,1st,2nd,and3rdpersonpronouns,prepo-DatasetTypeSourceNumberJC(+)theJapanTimesnewspaperandModelEnglishEssay16,857(-)HEL(HiroshimaEnglishLearners’Corpus)andJLE(JapaneseLearnersofEn-glishCorpus)17,301CC(+)the21stCenturynewspaper3,200(-)CLEC(ChineseLearnerEr-rorCorpus)3,199Table1:Corpora((+):correct;(-):erroneous)respectivelyas7features.4ExperimentalEvaluationWeevaluatedtheperformanceofourtechniqueswithsupportvectormachine(SVM)andNaiveBayesian(NB)classiﬁcationmodels.Wealsocom-paredtheeffectivenessofvariousfeatures.Inad-dition,wecomparedourtechniquewithtwoothermethodsofcheckingerrors,MicrosoftWord03andALEKmethod(ChodorowandLeacock,2000).Fi-nally,wealsoappliedourtechniquetoevaluatetheMachineTranslationoutputs.4.1ExperimentalSetupClassiﬁcationModels.Weusedtwoclassiﬁcationmodels,SVM6andNBclassiﬁcationmodel.Data.Wecollectedtwodatasetsfromdifferentdo-mains,JapaneseCorpus(JC)andChineseCorpus(CC).Table1givesthedetailsofourcorpora.Inthelearner’scorpora,allofthesentencesareerro-neous.Notethatourdatadoesnotconsistofparallelpairsofsentences(oneerrorsentenceanditscorrec-tion).Theerroneoussentencesincludesgrammar,sentencestructureandlexicalchoiceerrors,butnotspellingerrors.Foreachsentence,wegeneratedﬁvekindsoffea-turesaspresentedinSection3.Foranon-binaryfeatureX,itsvaluexisnormalizedbyz-score,norm(x)=x−mean(X)√var(X),wheremean(x)istheem-piricalmeanofXandvar(X)isthevarianceofX.Thuseachsentenceisrepresentedbyavector.MetricsWecalculatedtheprecision,recall,andF-scoreforcorrectanderroneoussentences,respectively,andalsoreporttheoverallaccuracy.sitionsandadverbs,auxiliaryverbs,andconjunctions.6http://svmlight.joachims.org/86

Alltheexperimentalresultsareobtainedthorough10-foldcross-validation.4.2ExperimentalResultsTheEffectivenessofVariousFeatures.Theexper-imentistoevaluatethecontributionofeachfeaturetotheclassiﬁcation.TheresultsofSVMaregiveninTable2.Wecanseethattheperformanceoflabeledsequentialpatterns(LSP)featureconsistentlyout-performsthoseofalltheotherindividualfeatures.Italsoperformsbetterevenifweusealltheotherfea-turestogether.Thisisbecauseotherfeaturesonlyprovidesomerelativelyabstractandsimplelinguis-ticinformation,whereasthediscoveredLSPschar-acterizesigniﬁcantlinguisticfeaturesasdiscussedbefore.WealsofoundthattheresultsofNBarealittleworsethanthoseofSVM.However,allthefea-turesperformconsistentlyonthetwoclassiﬁcationmodelsandwecanobservethesametrend.Duetospacelimitation,wedonotgiveresultsofNB.Inaddition,thediscoveredLSPsthemselvesareintuitiveandmeaningfulsincetheyareintuitivefea-turesthatcandistinguishcorrectsentencesfromer-roneoussentences.Wediscovered6309LSPsinJCdataand3742LSPsinCCdata.Someexam-pleLSPsdiscoveredfromerroneoussentencesare<a,NNS>(support:0.39%,conﬁdence:85.71%),<to,VBD>(support:0.11%,conﬁdence:84.21%),and<the,more,the,JJ>(support:0.19%,conﬁ-dence:0.93%)7;Similarly,wealsogivesomeexam-pleLSPsminedfromcorrectsentences:<NN,VBZ>(support:2.29%,conﬁdence:75.23%),and<have,VBN,since>(support:0.11%,conﬁdence:85.71%)8.However,otherfeaturesareabstractanditishardtoderivesomeintuitiveknowledgefromtheopaquestatisticalvaluesofthesefeatures.AsshowninTable2,ourtechniqueachievesthehighestaccuracy,e.g.81.75%ontheJapanesedataset,whenweuseallthefeatures.However,wealsonoticethattheimprovementisnotverysignif-icantcomparedwithusingLSPfeatureindividually(e.g.79.63%ontheJapanesedataset).ThesimilarresultsareobservedwhenwecombinedthefeaturesPLM,SC,FWD,andLC.Thiscouldbeexplained7a+pluralnoun;to+pasttenseformat;themore+the+baseformofadjective8singularormassnoun+the3rdpersonsingularpresentformat;have+pastparticipleformat+sincebytworeasons:(1)Asentencemaycontainsev-eralkindsoferrors.Asentencedetectedtobeer-roneousbyonefeaturemayalsobedetectedbyan-otherfeature;and(2)Variousfeaturesgiveconﬂict-ingresults.Thetwoaspectssuggestthedirectionsofourfutureeffortstoimprovetheperformanceofourmodels.ComparingwithOtherMethods.Itisdifﬁculttoﬁndbenchmarkmethodstocomparewithourtechniquebecause,asdiscussedinSection2,exist-ingmethodsoftenrequireerrortaggedcorporaorparallelcorpora,orfocusonaspeciﬁctypeofer-rors.Inthispaper,wecompareourtechniquewiththegrammarcheckerofMicrosoftWord03andtheALEK(ChodorowandLeacock,2000)methodusedbyETS.ALEKisusedtodetectinappropriateusageofspeciﬁcvocabularywords.Notethatwedonotconsiderspellingerrors.Duetospacelimitation,weonlyreporttheprecision,recall,F-scoreforerroneoussentences,andtheoverallaccuracy.AscanbeseenfromTable3,ourmethodout-performstheothertwomethodsintermsofover-allaccuracy,F-score,andrecall,whilethethreemethodsachievecomparableprecision.WerealizethatthegrammarcheckerofWordisageneraltoolandtheperformanceofALEK(ChodorowandLea-cock,2000)canbeimprovediflargertrainingdataisused.WefoundthatWordandALEKusuallycannotﬁndsentencestructureandlexicalcollocationerrors,e.g.,“ThemoreyoulistentoEnglish,theeasyitbe-comes.”containsthediscoveredLSP<the,more,the,JJ>→Error.Cross-domainResults.Tostudytheperformanceofourmethodoncross-domaindatafromwritersofthesameﬁrst-languagebackground,wecollectedtwodatasetsfromJapanesewriters,oneiscomposedof694parallelsentences(+:347,-:347),andtheother1,671non-parallelsentences(+:795,-:876).ThetwodatasetsareusedastestdatawhileweuseJCdatasetfortraining.NotethatthetestsentencescomefromdifferentdomainsfromtheJCdata.TheresultsaregivenintheﬁrsttworowsofTable4.Thisexperimentshowsthatourleaningmodeltrainedforonedomaincanbeeffectivelyappliedtoindepen-dentdataintheotherdomainsfromthewritesofthesameﬁrst-languagebackground,nomatterwhetherthetestdataisparallelornot.Wealsonoticedthat87

DatasetFeatureA(-)F(-)R(-)P(+)F(+)R(+)PJCLSP79.6380.6585.5676.2978.4973.7983.85LC69.5571.7277.8766.4767.0261.3673.82PLM61.6055.4650.8164.916270.2858.43SC53.6657.2968.4056.1234.1839.0432.22FWD68.0172.8286.3762.9561.1449.9478.82LC+PLM+SC+FWD71.6473.5279.3868.4669.4864.0375.94LSP+LC+PLM+SC+FWD81.7581.6081.4681.7481.9082.0481.76CCLSP78.1976.4070.6483.2079.7185.7274.50LC63.8262.3660.1264.7765.1767.4963.01PLM55.4664.4180.7253.6140.4130.2261.30SC50.5262.5887.3150.6413.7514.3313.22FWD61.3660.8060.7060.9061.9061.9961.80LC+PLM+SC+FWD67.6967.6267.5167.7767.7467.8767.64LSP+LC+PLM+SC+FWD79.8178.3372.7684.8481.1086.9276.02Table2:TheExperimentalResults(A:overallaccuracy;(-):erroneoussentences;(+):correctsentences;F:F-score;R:recall;P:precision)DatasetModelA(-)F(-)R(-)PJCOurs81.3981.2581.2481.28Word58.8733.6721.0384.73ALEK54.6920.3311.6778.95CCOurs79.1477.8173.1783.09Word58.4732.0219.8184.22ALEK55.2122.8313.4276.36Table3:TheComparisonResultsLSPsplaydominatingroleinachievingtheresults.Duetospacelimitation,nodetailsarereported.Tofurtherseetheperformanceofourmethodondatawrittenbywriterswithdifferentﬁrst-languagebackgrounds,weconductedtwoexperi-ments.(1)WemergetheJCdatasetandCCdataset.The10-foldcross-validationresultsonthemergeddatasetaregiveninthethirdrowofTable4.Theresultsdemonstratethatourmodelsworkwellwhenthetrainingdataandtestdatacontainsentencesfromdifferentﬁrst-languagebackgrounds.(2)WeusetheJCdataset(resp.CCdataset)fortrainingwhiletheCCdataset(resp.JCdataset)isusedastestdata.Asshowninthefourth(resp.ﬁfth)rowofTable4,theresultsareworsethantheircorrespondingresultsofWordgiveninTable3.Thereasonisthatthemis-takesmadebyJapaneseandChinesearedifferent,thusthelearningmodeltrainedononedatadoesnotworkwellontheotherdata.Notethatourmethodisnotdesignedtoworkinthisscenario.ApplicationtoMachineTranslationEvaluation.OurlearningmodelscouldbeusedtoevaluatetheMTresultsasancomplementarymeasure.ThisisbasedontheassumptionthatiftheMTresultscanbeaccuratelydistinguishedfromhumanreferencesDatasetA(-)F(-)R(-)PJC(Train)+nonparallel(Test)72.4968.5557.5184.84JC(Train)+parallel(Test)71.3369.5365.4274.18JC+CC79.9879.7279.2480.23JC(Train)+CC(Test)55.6241.7131.3262.40CC(Train)+JC(Test)57.5723.6416.9439.11Table4:TheCross-domainResultsofourMethodbyourtechnique,theMTresultsarenotnaturalandmaycontainerrorsaswell.Theexperimentwasconductedusing10-foldcrossvalidationontwoLDCdata,low-rankedandhigh-rankeddata9.TheresultsusingSVMasclassi-ﬁcationmodelaregiveninTable5.Asexpected,theclassiﬁcationaccuracyonlow-rankeddataishigherthanthatonhigh-rankeddatasincelow-rankedMTresultsaremoredifferentfromhumanreferencesthanhigh-rankedMTresults.WealsofoundthatLSPsarethemosteffectivefeatures.Inaddition,ourdiscoveredLSPscouldindicatethecommonerrorsmadebytheMTsystemsandprovidesomesugges-tionsforimprovingmachinetranslationresults.Asasummary,theminedLSPsareindeedeffec-tivefortheclassiﬁcationmodelsandourproposedtechniqueiseffective.5ConclusionsandFutureWorkThispaperproposedanewapproachtoidentifyingerroneous/correctsentences.Empiricalevaluatingusingdiversedatademonstratedtheeffectivenessof9OneLDCdatacontains14,604lowranked(score1-3)ma-chinetranslationsandthecorrespondinghumanreferences;theotherLDCdatacontains808highranked(score3-5)machinetranslationsandthecorrespondinghumanreferences88

DataFeatureA(-)F(-)R(-)P(+)F(+)R(+)PLow-rankeddata(1-3score)LSP84.2083.9582.1985.8284.4486.2582.73LSP+LC+PLM+SC+FWD86.6086.8488.9684.8386.3584.2788.56High-rankeddata(3-5score)LSP71.7473.0179.5667.5970.2364.4777.40LSP+LC+PLM+SC+FWD72.8773.6868.9569.2071.9267.2277.60Table5:TheResultsonMachineTranslationDataourtechniques.Moreover,weproposedtomineLSPsastheinputofclassiﬁcationmodelsfromasetofdatacontainingcorrectanderroneoussentences.TheLSPswereshowntobemuchmoreeffectivethantheotherlinguisticfeaturesalthoughtheotherfea-tureswerealsobeneﬁcial.Wewillinvestigatethefollowingproblemsinthefuture:(1)tomakeuseofthediscoveredLSPstopro-videdetailedfeedbackforESLlearners,e.g.theer-rorsinasentenceandsuggestedcorrections;(2)tointegratethefeatureseffectivelytoachievebetterre-sults;(3)tofurtherinvestigatetheapplicationofourtechniquesforMTevaluation.ReferencesRakeshAgrawalandRamakrishnanSrikant.1995.Miningse-quentialpatterns.InICDE.EmilyM.Bender,DanFlickinger,StephanOepen,AnnemarieWalsh,andTimothyBaldwin.2004.Arboretum:Usingaprecisiongrammarforgrammmarcheckingincall.InProc.InSTIL/ICALLSymposiumonComputerAssistedLearning.ChrisBrockett,WilliamDolan,andMichaelGamon.2006.Correctingeslerrorsusingphrasalsmttechniques.InACL.PeterEBrown,VincentJ.DellaPietra,StephenA.DellaPietra,andRobertL.Mercer.1993.Themathematicsofstatisticalmachinetranslation:Parameterestimation.ComputationalLinguistics,19:263–311.JillBurstein,KarenKukich,SusanneWolff,ChiLu,MartinChodorow,LisaBraden-Harder,andMaryDeeHarris.1998.Automatedscoringusingahybridfeatureidentiﬁcationtech-nique.InProc.ACL.MartinChodorowandClaudiaLeacock.2000.Anunsuper-visedmethodfordetectinggrammaticalerrors.InNAACL.MichaelCollins.1997.Threegenerative,lexicalisedmodelsforstatisticalparsing.InProc.ACL.SimonCorston-Oliver,MichaelGamon,andChrisBrockett.2001.Amachinelearningapproachtotheautomaticeval-uationofmachinetranslation.InProc.ACL.P.W.Foltz,D.Laham,andT.K.Landauer.1999.Automatedessayscoring:Applicationtoeducationaltechnology.InEd-Media’99.MichaelGamon,AnthonyAue,andMartineSmets.2005.Sentence-levelmtevaluationwithoutreferencetranslations:Beyondlanguagemodeling.InProc.EAMT.ShicunGuiandHuizhongYang.2003.ZhongguoXuexizheYingyuYuliaohu.(ChineseLearnerEnglishCorpus).Shang-hai:ShanghaiWaiyuJiaoyuChubanshe.(InChinese).GeorgeE.Heidorn.2000.IntelligentWritingAssistance.HandbookofNaturalLanguageProcessing.RobertDale,HermannMoisiandHaroldSomers(ed.).MarcelDekker.EmiIzumi,KiyotakaUchimoto,ToyomiSaiga,ThepchaiSup-nithi,andHitoshiIsahara.2003.Automaticerrordetectioninthejapaneselearners’englishspokendata.InProc.ACL.NitinJindalandBingLiu.2006.Identifyingcomparativesen-tencesintextdocuments.InSIGIR.DingLiuandDanielGildea.2005.Syntacticfeaturesforevaluationofmachinetranslation.InProc.ACLWorkshoponIntrinsicandExtrinsicEvaluationMeasuresforMachineTranslationand/orSummarization.YajuanL¨uandMingZhou.2004.Collocationtranslationac-quisitionusingmonolingualcorpora.InProc.ACL.LisaN.Michaud,KathleenF.McCoy,andChristopherA.Pen-nington.2000.Anintelligenttutoringsystemfordeaflearn-ersofwrittenenglish.InProc.4thInternationalACMCon-ferenceonAssistiveTechnologies.RyoNagata,AtsuoKawai,KoichiroMorihiro,andNaokiIsu.2006.Afeedback-augmentedmethodfordetectingerrorsinthewritingoflearnersofenglish.InProc.ACL.Jian-YunNie,MichelSimard,PierreIsabelle,andRichardDu-rand.1999.Cross-languageinformationretrievalbasedonparalleltextsandautomaticminingofparalleltextsfromtheweb.InSIGIR,pages74–81.JianPei,JiaweiHan,BehzadMortazavi-Asl,andHelenPinto.2001.Preﬁxspan:Miningsequentialpatternsefﬁcientlybypreﬁx-projectedpatterngrowth.InProc.ICDE.YongmeiShiandLinaZhou.2005.Errordetectionusinglin-guisticfeatures.InHLT/EMNLP.AndreasStolcke.2002.Srilm-anextensiblelanguagemodelingtoolkit.InProc.ICSLP.GuihuaSun,GaoCong,XiaohuaLiu,Chin-YewLin,andMingZhou.2007.Miningsequentialpatternsandtreepatternstodetecterroneoussentences.InAAAI.TonoYukio,T.Kaneko,H.Isahara,T.Saiga,andE.Izumi.2001.Thestandardspeakingtestcorpus:A1million-wordspokencorpusofjapaneselearnersofenglishanditsimpli-cationsforl2lexicography.InASIALEX:AsianBilingualismandtheDictionary.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 89–95,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

89

VocabularyDecompositionforEstonianOpenVocabularySpeechRecognitionAnttiPuurulaandMikkoKurimoAdaptiveInformaticsResearchCentreHelsinkiUniversityofTechnologyP.O.Box5400,FIN-02015HUT,Finland{puurula,mikkok}@cis.hut.fiAbstractSpeechrecognitioninmanymorphologi-callyrichlanguagessuffersfromaveryhighout-of-vocabulary(OOV)ratio.Earlierworkhasshownthatvocabularydecompositionmethodscanpracticallysolvethisproblemforasubsetoftheselanguages.Thispa-percomparesvariousvocabularydecompo-sitionapproachestoopenvocabularyspeechrecognition,usingEstonianspeechrecogni-tionasabenchmark.Comparisonsareper-formedutilizinglargemodelsof60000lex-icalitemsandsmallervocabulariesof5000items.Alargevocabularymodelbasedonamanuallyconstructedmorphologicaltag-gerisshowntogivethelowestworder-rorrate,whiletheunsupervisedmorphol-ogydiscoverymethodMorfessorBaselinegivesmarginallyweakerresults.OnlytheMorfessor-basedapproachisshowntoade-quatelyscaletosmallervocabularysizes.1Introduction1.1OOVproblemOpenvocabularyspeechrecognitionreferstoau-tomaticspeechrecognition(ASR)ofcontinuousspeech,or“speech-to-text”ofspokenlanguage,wheretherecognizerisexpectedtorecognizeanywordspokeninthatlanguage.Thiscapabilityisare-centdevelopmentinASR,andisrequiredorbeneﬁ-cialinmanyofthecurrentapplicationsofASRtech-nology.Moreover,largevocabularyspeechrecogni-tionisnotpossibleinmostlanguagesoftheworldwithoutﬁrstdevelopingthetoolsneededforopenvocabularyspeechrecognition.Thisisduetoafun-damentalobstacleincurrentASRcalledtheout-of-vocabulary(OOV)problem.TheOOVproblemreferstotheexistenceofwordsencounteredthataspeechrecognizerisunabletorecognize,astheyarenotcoveredinthevocabu-lary.TheOOVproblemiscausedbythreeinter-twinedissues.Firstly,thelanguagemodeltrainingdataandthetestdataalwayscomefromdifferentsamplingsofthelanguage,andthemismatchbe-tweentestandtrainingdataintroducessomeOOVwords,theamountdependingonthedifferencebe-tweenthedatasets.Secondly,ASRsystemsalwaysuseﬁniteandpreferablysmallsizedvocabularies,sincethespeedofdecodingrapidlyslowsdownasthevocabularysizeisincreased.Vocabularysizesdependontheapplicationdomain,sizeslargerthan60000beingveryrare.Assomeofthewordsen-counteredinthetrainingdataareleftoutofthevo-cabulary,therewillbeOOVwordsduringrecogni-tion.Thethirdandﬁnalissueisthefundamentalone;languagesformnovelsentencesnotonlybycombiningwords,butalsobycombiningsub-worditemscalledmorphstomakeupthewordsthem-selves.Thesemorphsinturncorrespondtoabstractgrammaticalitemscalledmorphemes,andmorphsofthesamemorphemearecalledallomorphsofthatmorpheme.Thestudyofthesefacetsoflanguageisaptlycalledmorphology,andhasbeenlargelyne-glectedinmodernASRtechnology.Thisisdueto90

ASRhavingbeendevelopedprimarilyforEnglish,wheretheOOVproblemisnotassevereasinotherlanguagesoftheworld.1.2RelevanceofmorphologyforASRMorphologiesinnaturallanguagesarecharacter-izedtypologicallyusingtwoparameters,calledin-dexesofsynthesisandfusion.Indexofsynthesishasbeenlooselydeﬁnedastheratioofmorphsperwordformsinthelanguage(Comrie,1989),whileindexoffusionreferstotheratioofmorphspermor-pheme.Highfrequencyofverbparadigmssuchas“hear,hear+d,hear+d”wouldresultinahighsyn-thesis,lowfusionlanguage,whereashighfrequencyofparadigmssuchas“sing,sang,sung”wouldre-sultinalmosttheopposite.Countingdistinctitemtypesandnotinstancesofthetypes,theﬁrstex-amplewouldhave2wordforms,2morphsand2morphemes,thesecond3wordforms,3morphsand1morpheme.Althoughintheﬁrstexample,thereare3wordinstancesofthe2wordforms,thelat-terwordformbeinganambiguousonereferringtotwodistinctgrammaticalconstructions.Itshouldalsobenotedthattheﬁrstmorphoftheﬁrstex-amplehas2pronunciations.Pronunciationalbound-ariesdonotalwaysfollowmorphologicalones,andamorphmayandwillhaveseveralpronunciationsthatdependoncontext,ifthelanguageinquestionhassigniﬁcantorthographicirregularity.Ascanbeseen,bothtypesofmorphologicalcom-plexityincreasetheamountofdistinctwordforms,resultinginanincreaseintheOOVrateofanyﬁ-nitesizedvocabularyforthatlanguage.Inprac-tice,theOOVincreasecausedbysynthesisismuchlarger,aslanguagescanhavethousandsofdiffer-entwordformsperwordthatarecausedbyaddi-tionofprocessesofwordformationfollowedbyin-ﬂections.ThustheOOVprobleminASRhasbeenmostpronouncedinlanguageswithmuchsynthesis,regardlessoftheamountoffusion.Themorpheme-basedmodelingapproachesevaluatedinthisworkareprimarilyintendedforﬁxingtheproblemcausedbysynthesis,andshouldworklesswellorevenad-verselywhenattemptedwithlowsynthesis,highfu-sionlanguages.Itshouldbenotedthatmodelsbasedonﬁnitestatetransducershavebeenshowntobead-equatefordescribingfusionaswell(Koskenniemi,1983),andfurtherworkshouldevaluatethesetypesofmodelsinASRoflanguageswithhigherindexesoffusion.1.3ApproachesforsolvingtheOOVproblemThetraditionalmethodforreducingOOVwouldbetosimplyincreasethevocabularysizesothattherateofOOVwordsbecomessufﬁcientlylow.Naturallythismethodfailswhenthewordsarederived,com-poundedorinﬂectedformsofrarerwords.WhilethisapproachmightstillbepracticalinlanguageswithalowindexofsynthesissuchasEnglish,itfailswithmostlanguagesintheworld.Forexam-ple,inEnglishwithlanguagemodels(LM)of60kwordstrainedfromtheGigawordCorpusV.2(Graffetal.,2005),andtestingonaverysimilarVoiceofAmerica-portionofTDT4speechcorpora(KongandGraff,2005),thisgivesaOOVrateof1.5%.ItshouldbenotedthateveryOOVcausesroughlytwoerrorsinrecognition,andvocabularydecompo-sitionapproachessuchastheonesevaluatedheregivesomebeneﬁtstoworderrorrate(WER)eveninrecognizinglanguagessuchasEnglish(BisaniandNey,2005).Fourdifferentapproachestolexicalunitselec-tionareevaluatedinthiswork,allofwhichhavebeenpresentedpreviously.Thesearehencecalled“word”,“hybrid”,“morph”and“grammar”.Thewordapproachisthedefaultapproachtolexicalitemselection,andisprovidedhereasabaselineforthealternativeapproaches.Thealternativestestedhereareallbasedondecomposingthein-vocabularywords,OOVwords,orboth,inLMtrainingdataintosequencesofsub-wordfragments.Duringrecogni-tionthedecodercanthenconstructtheOOVwordsencounteredascombinationsofthesefragments.WordboundariesaremarkedinLMswithtokenssothatthewordscanbereconstructedfromthesub-wordfragmentsafterdecodingsimplybyremovingspacesbetweenfragments,andchangingthewordboundariestokenstospaces.Assplittingtosub-worditemsmakesthespanofLMhistoriesshorter,higherordern-gramsmustbeusedtocorrectthis.Varigrams(SiivolaandPellom,2005)areusedinthiswork,andtomakeLMstrainedwitheachap-proachcomparable,thevarigramshavebeengrowntoroughlysizesof5millioncounts.Itshouldbenotedthatthenamesfortheapproachesherearesomewhatarbitrary,asfromatheoreticalperspec-91

tivebothmorph-andgrammar-basedapproachestrytomodelthegrammaticalmorphsetofalanguage,differencebeingthat“morph”doesthiswithanun-superviseddata-drivenmachinelearningalgorithm,whereas“grammar”doesthisusingsegmentationsfromamanuallyconstructedrule-basedmorpholog-icaltagger.2Modelingapproaches2.1WordapproachTheﬁrstapproachevaluatedinthisworkisthetra-ditionalwordbasedLM,whereitemsaresimplythemostfrequentwordsinthelanguagemodeltrainingdata.OOVwordsaresimplytreatedasunknownwordsinlanguagemodeltraining.Thishasbeenthedefaultapproachtoselectionoflexicalitemsinspeechrecognitionforseveraldecades,andasithasbeensufﬁcientinEnglishASR,therehasbeenlim-itedinterestinanyalternatives.2.2HybridapproachThesecondapproachisarecentreﬁnementofthetraditionalword-basedapproach.Thisissimilartowhatwasintroducedas“ﬂathybridmodel”(BisaniandNey,2005),andittriestomodelOOV-wordsassequencesofwordsandfragments.“Hybrid”referstotheLMhistoriesbeingcomposedofhy-bridsofwordsandfragments,while“ﬂat”referstothemodelbeingcomposedofonen-grammodelin-steadofseveralmodelsforthedifferentitemtypes.ThemodelstestedinthisworkdifferinthatsinceEstonianhasaveryregularphonemicorthography,graphemesequencescanbedirectlyusedinsteadofmorecomplexpronunciationmodeling.Subse-quentlythefragmentsusedarejustonegraphemeinlength.2.3MorphapproachThemorph-basedapproachhasshownsuperiorre-sultstoword-basedmodelsinlanguagesofhighsynthesisandlowfusion,includingEstonian.Thisapproach,called“MorfessorBaseline”isdescribedindetailin(Creutzetal.,2007).Anunsupervisedmachinelearningalgorithmisusedtodiscoverthemorphsetofthelanguageinquestion,usingmini-mumdescriptionlength(MDL)asanoptimizationcriterion.Thealgorithmisgivenawordlistofthelanguage,usuallyprunedtoabout100000words,thatitproceedstorecursivelysplittosmalleritems,usinggainsinMDLtooptimizetheitemset.Theresultingsetofmorphsmodelsthemorphsetwellinlanguagesofhighsynthesis,butasitdoesnottakefusionintoaccountanymanner,itshouldnotworkinlanguagesofhighfusion.Itneitherpreservesin-formationaboutpronunciations,andasthesedonotfollowmorphboundaries,theapproachisunsuitableinitsbasicformtolanguagesofhighorthographicirregularity.2.4GrammarapproachTheﬁnalapproachappliesamanuallyconstructedrule-basedmorphologicaltagger(Alumäe,2006).Thisapproachisexpectedtogivethebestresults,asthetaggershouldgivetheidealsegmentationalongthegrammaticalmorphsthattheunsupervisedandlanguage-independentmorphapproachtriestoﬁnd.Tomakethisapproachmorecomparabletothemorphmodels,OOVmorphsaremodeledassequencesofgraphemessimilartothehybridap-proach.Smallchangestotheoriginalapproachwerealsomadetomakethemodelcomparabletotheothermodelspresentedhere,suchasusingthetaggersegmentationsassuchandnotusingpseudo-morphemes,aswellasnottaggingtheitemsinanymanner.Thisapproachsuffersfromthesamehandi-capsasthemorphapproach,aswellasfromsomeadditionalones:morphologicalanalyzersarenotreadilyavailableformostlanguages,theymustbetailoredbylinguistsfornewdatasets,anditisanopenproblemastohowpronunciationdictionariesshouldbewrittenforgrammaticalmorphsinlan-guageswithsigniﬁcantorthographicirregularity.2.5TextsegmentationandlanguagemodelingFortrainingtheLMs,asubsetof43mil-lionwordsfromtheEstonianSegakorpuswasused(Segakorpus,2005),preprocessedwithamor-phologicalanalyzer(Alumäe,2006).Afterselectingtheitemtypes,segmentingthetrainingcorporaandgenerationofapronunciationdictionary,LMsweretrainedforeachlexicalitemtype.Table1showsthetextformatforLMtrainingdataaftersegmen-tationwitheachmodel.Ascanbeseen,theword-basedapproachdoesn’tusewordboundarytokens.TokeeptheLMscomparablebetweeneachmodel-92

modeltextsegmentationword5kvoodisreeglinaloemeword60kvoodisreeglinaloemehybrid5kvoodis<w>reeglina<w>loemehybrid60kvoodis<w>reeglina<w>loememorph5kvoodis<w>reeglina<w>loememorph60kvoodis<w>reeglina<w>loemegrammar5kvoodis<w>reeglina<w>loemegrammar60kvoodis<w>reeglina<w>loemeTable1.Samplesegmentedtextsforeachmodel.ingapproach,growingvarigrammodels(SiivolaandPellom,2005)wereusedwithnolimitsastotheor-derofn-grams,butlimitingthenumberofcountsto4.8and5millioncounts.Insomemodelsthisgrow-ingmethodresultedintheinclusionofveryfrequentlongitemsequencestothevarigram,uptoa28-gram.Modelsofboth5000and60000lexicalitemsweretrainedinordertotestifandhowthemodel-ingapproacheswouldscaletosmallerandthereforemuchfastervocabularies.Distributionofcountsinn-gramorderscanbeseeninﬁgure1.Figure1.Numberofcountsincludedforeachn-gramorderinthe60kvarigrammodels.Theperformanceofthestatisticallanguagemod-elsisoftenevaluatedbyperplexityorcross-entropy.However,wedecidedtoonlyreporttherealASRperformance,becauseperplexitydoesnotsuitwelltothecomparisonofmodelsthatusedifferentlex-ica,havedifferentOOVratesandhavelexicalunitsofdifferentlengths.3Experimentalsetup3.1EvaluationsetAcousticmodelsforEstonianASRweretrainedontheEstonianSpeechdat-likecorpus(Meisteretal.,2002).Thisconsistsofspokennewspapersentencesandshorterutterances,readoveratelephoneby1332differentspeakers.Thedatathereforewasquiteclearlyarticulated,butsufferedfrom8kHzsamplerate,differentmicrophones,channelnoisesandoccasionalbackgroundnoises.Ontopofthisthespeakerswereselectedtogiveaverybroadcov-erageofdifferentdialectalvarietiesofEstonianandwereofdifferentagegroups.Forthesereasons,inspiteofconsistingofrelativelycommonwordformsfromnewspapersentences,thedatabasecanbecon-sideredchallengingforASR.Held-outsentenceswerefromthesamecorpususedasdevelopmentandevaluationset.8differentsentencesfrom50speakerseachwereusedforeval-uation,whilesentencesfrom15speakerswereusedfordevelopment.LMscalingfactorwasoptimizedforeachmodelseparatelyonthedevelopmentset.Ontotalover200hoursofdatafromthedatabasewasusedforacousticmodeltraining,ofwhichlessthanhalfwasspeech.3.2DecodingTheacousticmodelswereHiddenMarkovModels(HMM)withGaussianMixtureModels(GMM)forstatemodelingbasedon39-dimensionalMFCC+P+D+DDfeatures,withwindowedcepstralmeansubtraction(CMS)of1.25secondwindow.Maximumlikelihoodlineartransformation(MLLT)wasusedduringtraining.State-tiedcross-wordtriphonesand3left-to-rightstateswereused,statedurationsweremodeledusinggammadistributions.Ontotal3103tiedstatesand16Gaussiansperstatewereused.DecodingwasdonewiththedecoderdevelopedatTKK(Pylkkönen,2005),whichisbasedonaone-passViterbibeamsearchwithtokenpassingonalexicalpreﬁxtree.Thelexicalpreﬁxtreeincludedacross-wordnetworkformodelingtriphonecontexts,andthenodesinthepreﬁxtreeweretiedatthetri-phonestatelevel.Bigramlook-aheadmodelswere93

usedinspeedingupdecoding,inadditiontoprun-ingwithglobalbeam,history,histogramandwordendpruning.Duetothepropertiesofthedecoderandvarigrammodels,veryhighordern-gramscouldbeusedwithoutsigniﬁcantdegradationindecodingspeed.Asthedecoderwasrunwithonlyonepass,adap-tationwasnotusedinthiswork.Inpreliminaryexperimentssimpleadaptationwithjustconstrainedmaximumlikelihoodlinearregression(CMLLR)wasshowntogiveasmuchas20%relativeworderrorratereductions(RWERR)withthisdataset.Adaptationwasnotused,sinceitinteractswiththemodeltypes,aswellaswiththeWERfromtheﬁrstroundofdecoding,providinglargerRWERRforthebettermodels.WithhighWERmodels,adaptationmatricesarelessaccurate,anditisalsoprobablethatthedecompositionmethodsyieldmoreaccuratema-trices,astheyproduceresultswherefewerHMM-statesaremisrecognized.Theseissuesshouldbein-vestigatedinfutureresearch.Afterdecoding,theresultswerepost-processedbyremovingwordsthatseemedtobesequencesofjunkfragments:consonant-onlysequencesand1-phonemewords.Thistreatmentshouldgiveverysigniﬁcantimprovementswithnoisydata,butinpre-liminaryexperimentsitwasnotedthattheuseofsentenceboundariesresultedinalmost10%RW-ERRweakerresultsfortheapproachesusingfrag-ments,asthatalmostnegatesthegainsachievedfromthispost-processing.Sincesentencebound-aryforcingisdonepriortojunkremoval,itseemstoworkerroneouslywhenitisforcedtooperateonnoisydata.Sentenceboundarieswereneverthelessused,asinthesameexperimentstheword-basedmodelsgainedsigniﬁcantlyfromtheiruse,mostlikelybecausetheycannotusethefragmentitemsfordetectionofacousticjunk,asthemodelswithfragmentscan.4ResultsResultsoftheexperimentswereconsistentwithear-lierﬁndings(Hirsimäkietal.,2006;Kurimoetal.,2006).TraditionalwordbasedLMsshowedtheworstperformance,withalloftherecentlyproposedalternativesgivingbetterresults.HybridLMscon-sistentlyoutperformedtraditionalword-basedLMsinbothlargeandsmallvocabularyconditions.Thetwomorphology-drivenapproachesgavesimilarandclearlysuperiorresults.Onlythemorphapproachseemstoscaledownwelltosmallervocabularysizes,astheWERforthegrammarapproachin-creasedrapidlyassizeofthevocabularywasde-creased.sizewordhybridmorphgrammar6000053.147.139.438.7500082.063.043.547.6Table2.Worderrorratesforthemodels(WER%).Table2showstheWERforthelarge(60000)andsmall(5000)vocabularysizesanddifferentmod-elingapproaches.Table3showsthecorrespond-inglettererrorrates(LER).LERsaremorecompa-rableacrosssomelanguagesthanWERs,asWERdependsmoreonfactorssuchaslength,morpho-logicalcomplexity,andOOVofthewords.How-ever,forwithin-languageandbetween-modelcom-parisons,theRWERRshouldstillbeavalidmet-ric,andisalsousableinlanguagesthatdonotuseaphonemicwritingsystem.TheRWERRsofdiffer-entnovelmethodsseemstobecomparablebetweendifferentlanguagesaswell.BothWERandLERarehighconsideringthetask.However,standardmeth-odssuchasadaptationwerenotused,astheinten-tionwasonlytostudytheRWERRofthedifferentapproaches.sizewordhybridmorphgrammar6000017.815.812.412.3500035.520.814.415.4Table3.Lettererrorratesforthemodels(LER%).5DiscussionFourdifferentapproachestolexicalitemselectionforlargeandopenvocabularyASRinEstonianwereevaluated.Itwasshownthatthethreeap-proachesutilizingvocabularydecompositiongivesubstantialimprovementsoverthetraditionalwordbasedapproach,andmakelargevocabularyASRtechnologypossibleforlanguagessimilartoEsto-nian,wherethetraditionalapproachfailsduetovery94

highOOVrates.TheseincludememeticrelativesFinnishandTurkish,amongotherlanguagesthathavemorphologiesofhighfusion,lowsynthesisandloworthographicirregularity.5.1PerformanceoftheapproachesThemorpheme-basedapproachesoutperformedtheword-andhybrid-basedapproachesclearly.There-sultsfor“hybrid”areinintherangesuggestedbyearlierwork(BisaniandNey,2005).Onepossi-bleexplanationforthediscrepancybetweenthehy-bridandmorpheme-basedapproacheswouldbethatthemorpheme-basedapproachescaptureitemsthatmakesenseinn-grammodeling,asmorphsareitemsthatthesystemoflanguagenaturallyoperateson.Theseitemswouldthenbeofmoreusewhentry-ingtopredictunseendata(Creutzetal.,2007).Asmodelingpronunciationsismuchmorestraightfor-wardinEstonian,themorpheme-basedapproachesdonotsufferfromerroneouspronunciations,result-inginclearlysuperiorperformance.Asforthesuperiorityofthe“grammar”overtheunsupervised“morph”,thedifferenceismarginalintermsofRWERR.Thegrammaticaltaggerwastai-loredbyhandforthatparticularlanguage,whereasMorfessormethodismeanttobeunsupervisedandlanguageindependent.Therearefurtherargumentsthatwouldsuggestthattheunsupervisedapproachisonethatshouldbefollowed;only“morph”scaledwelltosmallervocabularysizes,theusualpracticeofpruningthewordlisttoproducesmallermorphsetsgivesbetterresultsthanhereandmostimpor-tantly,itisquestionableif“grammar”canbetakentolanguageswithhighindexesoffusionandortho-graphicirregularity,asthemodelshavetotaketheseintoaccountaswell.5.2ComparisontopreviousresultsTherearefewpreviousresultspublishedonEstonianopenvocabularyASR.In(Alumäe,2006)aWERof44.5%wasobtainedwithword-basedtrigramsandaWERof37.2%withitemssimilartoonesfrom“grammar”usingthesamespeechcorpusasinthiswork.Comparedtothepresentwork,theWERforthemorpheme-basedmodelswasmeasuredwithcompoundwordssplitinbothhypothesisandref-erencetexts,makingthetaskslightlyeasierthanhere.In(Kurimoetal.,2006)aWERof57.6%wasachievedwithword-basedvarigramsandaWERof49.0%withmorphs-basedones.Thisusedthesameevaluationsetasthiswork,buthadslightlydifferentLMsanddifferentacousticmodellingwhichisthemainreasonforthehigherWERlevels.Insummary,morpheme-basedapproachesseemtoconsistentlyoutperformthetraditionalwordbasedoneinEsto-nianASR,regardlessofthespeciﬁcsoftherecogni-tionsystem,testsetandmodels.In(Hirsimäkietal.,2006)acorrespondingcom-parisonofunsupervisedandgrammar-basedmorphswaspresentedinFinnish,andthegrammar-basedmodelgaveasigniﬁcantlyhigherWERinoneofthetasks.Thisresultisinteresting,andmaystemfromanumberoffactors,amongthemthedifferentdecoderandacousticmodels,4-gramsversusvarigrams,aswellasdifferencesinpost-processing.Mostlikelythedifferenceisduetolackofcoveragefordomain-speciﬁcwordsintheFinnishtagger,asithasa4.2%OOVrateonthetrainingdata.OntopofthistheOOVwordsaremodeledsimplyasgraphemese-quences,insteadofmodelingonlyOOVmorphsinthatmanner,asisdoneinthiswork.5.3OpenproblemsinvocabularydecompositionAsstatedintheintroduction,modelinglanguageswithhighindexesoffusionsuchasArabicwillre-quiremorecomplexvocabularydecompositionap-proaches.Thisisveriﬁedbyrecentempiricalre-sults,wheregainsobtainedfromsimplemorpholog-icaldecompositionseemtobemarginal(Kirchhoffetal.,2006;Creutzetal.,2007).TheselanguageswouldpossiblyneednovelLMinferencealgorithmsanddecoderarchitectures.Currentresearchseemstobeheadinginthisdirection,withweightedﬁnitestatetransducersbecomingstandardrepresentationsforthevocabularyinsteadofthelexicalpreﬁxtree.Anotherissueinvocabularydecompositionisor-thographicirregularity,astheitemsresultingfromdecompositiondonotnecessarilyhaveunambigu-ouspronunciations.AsmostmodernrecognizersusetheViterbiapproximationwithvocabulariesofonepronunciationperitem,thisisproblematic.Onesolutiontothisisexpandingthedifferentitemswithtagsaccordingtopronunciation,shiftingtheprob-lemtolanguagemodeling(Creutzetal.,2007).Forexample,Englishplural“s”wouldexpandto“s#1”95

withpronunciation“/s/”,and“s#2”withpronunci-ation“/z/”,andsoon.Inthiscasethevocabularysizeincreasesbytheamountofdifferentpronunci-ationsadded.Thenewitemswillhavepronuncia-tionsthatdependontheirlanguagemodelcontext,enablingthepredictionofpronunciationswithlan-guagemodelprobabilities.Theonlydownsidetothisiscomplicatingthesearchforoptimalvocabu-larydecomposition,astheitemsshouldmakesenseinbothpronunciationalandmorphologicalterms.Onecanconsidertheoriginallypresentedhybridapproachasanapproachtovocabularydecompo-sitionthattriestokeepthepronunciationsoftheitemsasgoodaspossible,whereasthemorphap-proachtriestoﬁnditemsthatmakesenseintermsofmorphology.Thisisobviouslyduetothemeth-odshavingbeendevelopedonverydifferenttypesoflanguages.ThemorphapproachwasdevelopedfortheneedsofFinnishspeechrecognition,whichisahighsynthesis,moderatefusionandverylowor-thographicirregularitylanguage,whereasthehybridapproachin(BisaniandNey,2005)wasdevelopedforEnglish,whichhaslowsynthesis,moderatefu-sion,andveryhighorthographicirregularity.Auni-versalapproachtovocabularydecompositionwouldhavetotakeallofthesefactorsintoaccount.AcknowledgementsTheauthorswouldliketothankDr.TanelAlumäefromTallinnUniversityofTechnologyforhelpinperformingexperimentswithEstonianspeechandtextdatabases.ThisworkwassupportedbytheAcademyofFinlandintheproject:Newadaptiveandlearningmethodsinspeechrecognition.ReferencesBernardComrie.1972.LanguageUniversalsandLin-guisticTypology,SecondEdition.AthenæumPressLtd,Gateshead,UK.KimmoKoskenniemi.1983.Two-levelMorphol-ogy:aGeneralComputationalModelforWord-FormRecognitionandProduction.UniversityofHelsinki,Helsinki,Finland.TanelAlumäe.2006.MethodsforEstonianLargeVo-cabularySpeechRecognition.PhDThesis.TallinnUniversityofTechnology.Tallinn,Estonia.MaximilianBisani,HermannNey.2005.OpenVocab-ularySpeechRecognitionwithFlatHybridModels.INTERSPEECH-2005,725–728.JannePylkkönen.2005.AnEfﬁcientOne-passDecoderforFinnishLargeVocabularyContinuousSpeechRecognition.ProceedingsofThe2ndBalticCon-ferenceonHumanLanguageTechnologies,167–172.HLT’2005.Tallinn,Estonia.VesaSiivola,BryanL.Pellom.2005.Growingann-GramLanguageModel.INTERSPEECH-2005,1309–1312.DavidGraff,JunboKong,KeChenandKazuakiMaeda.2005.LDCGigawordCorpora:En-glishGigawordSecondEdition.InLDClink:http://www.ldc.upenn.edu/Catalog/index.jsp.JunboKongandDavidGraff.2005.TDT4Multilin-gualBroadcastNewsSpeechCorpus.InLDClink:http://www.ldc.upenn.edu/Catalog/index.jsp.Segakorpus.2005.Segakorpus-MixedCorpusofEsto-nian.TartuUniversity.http://test.cl.ut.ee/korpused/.EinarMeister,JürgenLasnandLyaMeister2002.Esto-nianSpeechDat:aprojectinprogress.InProceedingsoftheFonetiikanPäivät-PhoneticsSymposium2002inFinland,21–26.KatrinKirchhoff,DimitraVergyri,JeffBilmes,KevinDuhandAndreasStolcke2006.Morphology-basedlanguagemodelingforconversationalArabicspeechrecognition.ComputerSpeech&Language20(4):589–608.MathiasCreutz,TeemuHirsimäki,MikkoKurimo,AnttiPuurula,JannePylkkönen,VesaSiivola,MattiVar-jokallio,EbruArisoy,MuratSaraclarandAndreasStolcke2007.AnalysisofMorph-BasedSpeechRecognitionandtheModelingofOut-of-VocabularyWordsAcrossLanguagesToappearinProceedingsofHumanLanguageTechnologies:TheAnnualCon-ferenceoftheNorthAmericanChapteroftheAsso-ciationforComputationalLinguistics.NAACL-HLT2007,Rochester,NY,USAMikkoKurimo,AnttiPuurula,EbruArisoy,VesaSiivola,TeemuHirsimäki,JannePylkkönen,TanelAlumaeandMuratSaraclar2006.Unlimitedvocabularyspeechrecognitionforagglutinativelanguages.InHu-manLanguageTechnology,ConferenceoftheNorthAmericanChapteroftheAssociationforComputa-tionalLinguistics.HLT-NAACL2006.NewYork,USATeemuHirsimäki,MathiasCreutz,VesaSiivola,MikkoKurimo,SamiVirpiojaandJannePylkkönen2006.UnlimitedvocabularyspeechrecognitionwithmorphlanguagemodelsappliedtoFinnish.ComputerSpeech&Language20(4):515–541.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 96–103,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

96

PhonologicalConstraintsandMorphologicalPreprocessingforGrapheme-to-PhonemeConversionVeraDembergSchoolofInformaticsUniversityofEdinburghEdinburgh,EH89LW,GBv.demberg@sms.ed.ac.ukHelmutSchmidIMSUniversityofStuttgartD-70174Stuttgartschmid@ims.uni-stuttgart.deGregorM¨ohlerSpeechTechnologiesIBMDeutschlandEntwicklungD-71072B¨oblingenmoehler@de.ibm.comAbstractGrapheme-to-phonemeconversion(g2p)isacorecomponentofanytext-to-speechsys-tem.Weshowthataddingsimplesyllab-iﬁcationandstressassignmentconstraints,namely‘onenucleuspersyllable’and‘onemainstressperword’,toajointn-grammodelforg2pconversionleadstoadramaticimprovementinconversionaccuracy.Secondly,weassessedmorphologicalpre-processingforg2pconversion.Whilemor-phologicalinformationhasbeenincorpo-ratedinsomepastsystems,itscontributionhasneverbeenquantitativelyassessedforGerman.Wecomparetherelevanceofmor-phologicalpreprocessingwithrespecttothemorphologicalsegmentationmethod,train-ingsetsize,theg2pconversionalgorithm,andtwolanguages,EnglishandGerman.1IntroductionGrapheme-to-Phonemeconversion(g2p)isthetaskofconvertingawordfromitsspelling(e.g.“Stern-anis¨ol”,Engl:star-aniseoil)toitspronunciation(/"StERnPani:sPø:l/).Speechsynthesismoduleswithag2pcomponentareusedintext-to-speech(TTS)systemsandcanbebeappliedinspokendialoguesystemsorspeech-to-speechtranslationsystems.1.1SyllabiﬁcationandStressing2pconversionInordertocorrectlysynthesizeaword,itisnotonlynecessarytoconvertthelettersintophonemes,butalsotosyllabifythewordandtoassignwordstress.Theproblemsofwordphonemization,syllabiﬁca-tionandwordstressassignmentareinter-dependent.Informationaboutthepositionofasyllablebound-aryhelpsgrapheme-to-phonemeconversion.(Marc-handandDamper,2005)reportaworderrorrate(WER)reductionofapprox.5percentagepointsforEnglishwhentheletterstringisaugmentedwithsyl-labiﬁcationinformation.Thesameholdsvice-versa:wefoundthatWERwasreducedby50%whenrun-ningoursyllabiﬁeronphonemesinsteadofletters(seeTable4).Finally,wordstressisusuallydeﬁnedonsyllables;inlanguageswherewordstressisas-sumed1topartlydependonsyllableweight(suchasGermanorDutch),itisimportanttoknowwhereex-actlythesyllableboundariesareinordertocorrectlycalculatesyllableweight.ForGerman,(M¨uller,2001)showthatinformationaboutstressassignmentandthepositionofasyllablewithinawordimproveg2pconversion.1.2MorphologicalPreprocessingIthasbeenarguedthatusingmorphologicalin-formationisimportantforlanguageswheremor-phologyhasanimportantinﬂuenceonpronuncia-tion,syllabiﬁcationandwordstresssuchasGer-man,Dutch,Swedishor,toasmallerextent,alsoEnglish(Sproat,1996;M¨obius,2001;PounderandKommenda,1986;Blacketal.,1998;Taylor,2005).Unfortunately,thesepapersdonotquantifythecon-tributionofmorphologicalpreprocessinginthetask.Importantquestionswhenconsideringtheinte-grationofamorphologicalcomponentintoaspeech1Thisissueiscontroversialamonglinguists;foranoverviewsee(Jessen,1998).97

synthesissystemare1)Howlargearetheim-provementstobegainedfrommorphologicalpre-processing?2)Mustthemorphologicalsystembeperfectorcanperformanceimprovementsalsobereachedwithrelativelysimplemorphologicalcom-ponents?and3)Howmuchdoesthebeneﬁttobeexpectedfromexplicitmorphologicalinforma-tiondependontheg2palgorithm?Todeterminethesefactors,wecomparedmorphologicalsegmen-tationsbasedonmanualmorphologicalannotationfromCELEXtotworule-basedsystemsandseveralunsuperviseddata-basedapproaches.Wealsoanal-ysedtheroleofexplicitmorphologicalpreprocess-ingondatasetsofdifferentsizesandcompareditsrelevancewithrespecttoadecisiontreeandajointn-grammodelforg2pconversion.Thepaperisstructuredasfollows:Weintroducetheg2pconversionmodelweusedinsection2andexplainhowweimplementedthephonologicalcon-straintsinsection3.Section4isconcernedwiththerelationbetweenmorphology,wordpronuncia-tion,syllabiﬁcationandwordstressinGerman,andpresentsdifferentsourcesformorphologicalseg-mentation.Insection5,weevaluatethecontributionofeachofthecomponentsandcompareourmeth-odstostate-of-the-artsystems.Section6summa-rizesourresults.2MethodsWeusedajointn-grammodelforthegrapheme-to-phonemeconversiontask.Modelsofthistypehavepreviouslybeenshowntoyieldverygoodg2pconversionresults(BisaniandNey,2002;GalescuandAllen,2001;Chen,2003).Modelsthatdonotusejointletter-phonemestates,andthereforearenotconditionalontheprecedingletters,butonlyontheactualletterandtheprecedingphonemes,achievedinferiorresults.ExamplesofsuchapproachesusingHiddenMarkovModelsare(RentzepopoulosandKokkinakis,1991)(whoappliedtheHMMtotherelatedtaskofphoneme-to-graphemeconversion),(Taylor,2005)and(Minker,1996).Theg2ptaskisformulatedassearchingforthemostprobablesequenceofphonemesgiventheor-thographicformofaword.Onecanthinkofitasataggingproblemwhereeachletteristaggedwitha(possiblyempty)phoneme-sequencep.Inourpar-ticularimplementation,themodelisdeﬁnedasahigher-orderHiddenMarkovModel,wherethehid-denstatesarealetter–phoneme-sequencepairhl;pi,andtheobservedsymbolsarethelettersl.Theout-putprobabilityofahiddenstateisthenequaltoone,sinceallhiddenstatesthatdonotcontaintheob-servedletterarepruneddirectly.Themodelforgrapheme-to-phonemeconver-sionusestheViterbialgorithmtoefﬁcientlycom-putethemostprobablesequenceˆpn1ofphonemesˆp1,ˆp2,...,ˆpnforagivenlettersequenceln1.Theprobabilityofaletter–phon-seqpairdependsonthekprecedingletter–phon-seqpairs.Dummystates‘#’areappendedatbothendsofeachwordtoindicatethewordboundaryandtoensurethatallconditionalprobabilitiesarewell-deﬁned.ˆpn1=argmaxpn1n+1Yi=1P(hl;pii|hl;pii−1i−k)Inanintegratedmodelwhereg2pconversion,syl-labiﬁcationandwordstressassignmentareallper-formedatthesametime,astateadditionallycon-tainsasyllableboundaryﬂagbandastressﬂaga,yieldinghl;p;b;aii.Asanalternativearchitecture,wealsodesignedamodularsystemthatcomprisesonecomponentforsyllabiﬁcationandoneforwordstressassignment.Themodelforsyllabiﬁcationcomputesthemostprobablesequenceˆbn1ofsyllableboundary-tagsˆb1,ˆb2,...,ˆbnforagivenlettersequenceln1.ˆbn1=argmaxbn1n+1Yi=1P(hl;bii|hl;bii−1i−k)Thestressassignmentmodelworksonsyllables.Itcomputesthemostprobablesequenceˆan1ofwordaccent-tagsˆa1,ˆa2,...,ˆanforagivensyllablese-quencesyln1.ˆan1=argmaxan1n+1Yi=1P(hsyl;aii|hsyl;aii−1i−k)2.1SmoothingBecauseofmajordatasparsenessproblems,smooth-ingisanimportantissue,inparticularforthestressmodelwhichisbasedonsyllable–stress-tagpairs.Performancevariedbyupto20%infunctionofthesmoothingalgorithmchosen.Bestresultswereob-tainedwhenusingavariantofModiﬁedKneser-NeySmoothing2(ChenandGoodman,1996).2Foraformaldeﬁnition,see(Demberg,2006).98

2.2PruningIntheg2p-model,eachlettercanonaveragemapontooneof12alternativephoneme-sequences.Whenworkingwith5-grams3,thereareabout125=250,000statesequences.Toimprovetimeandspaceefﬁciency,weimplementedasimplepruningstrat-egythatonlyconsidersthetbeststatesatanymo-mentintime.Withathresholdoft=15,about120wordsareprocessedperminuteona1.5GHzma-chine.Conversionqualityisonlymarginallyworsethanwhenthewholesearchspaceiscalculated.RunningtimeforEnglishisfaster,becausetheav-eragenumberofcandidatephonemesforeachlet-terislower.Wemeasuredrunningtime(includingtrainingandtheactualg2pconversionin10-foldcrossvalidation)foraPerlimplementationofouralgorithmontheEnglishNetTalkcorpus(20,008words)onanIntelPentium4,3.0GHzmachine.Runningtimewaslessthan1hforeachofthefol-lowingthreetestconditions:c1)g2pconversiononly,c2)syllabiﬁcationﬁrst,theng2pconversion,c3)simultaneousg2pconversionandsyllabiﬁcation,givenperfectsyllableboundaryinput,c4)simulta-neousg2pconversionandsyllabiﬁcationwhencor-rectsyllabiﬁcationisnotavailablebeforehand.ThisismuchfasterthanthetimesforPronunciationbyAnalogy(PbA)(MarchandandDamper,2005)onthesamecorpus.MarchandandDamperreportedaprocessingtimeofseveralhoursforc4),twodaysforc2)andseveraldaysforc3).2.3AlignmentOurcurrentimplementationofthejointn-grammodelisnotintegratedwithanautomaticalignmentprocedure.Wethereforeﬁrstalignedlettersandphonemesinaseparate,semi-automaticstep.Eachletterwasalignedwithzerototwophonemesand,intheintegratedmodel,zerooronesyllablebound-ariesandstressmarkers.3IntegrationofPhonologicalConstraintsWhenanalysingtheresultsfromthemodelthatdoesg2pconversion,syllabiﬁcationandstressassign-3Thereisatrade-offbetweenlongcontextwindowswhichcapturethecontextaccuratelyanddatasparsenessissues.Theoptimalvaluekforthecontextwindowsizedependsonthesourcelanguage(existenceofmultilettergraphemes,complex-ityofsyllablesetc.).mentinasinglestep,wefoundthatalargepropor-tionoftheerrorswasduetotheviolationofbasicphonologicalconstraints.Somesyllableshadnosyllablenucleus,whileotherscontainedseveralvowels.ThereasonfortheerrorsisthatGermansyllablescanbeverylongandthereforesparse,oftencausingthemodeltoback-offtosmallercontexts.Ifthecontextistoosmalltocoverthesyllable,themodelcannotdecidewhetherthecurrentsyllablecontainsanucleus.Instressassignment,thisproblemisevenworse:thecontextwindowrarelycoversthewholeword.Thealgorithmdoesnotknowwhetheritalreadyas-signedawordstressoutsidethecontextwindow.Thisleadstoahigherrorratewith15-20%ofin-correctlystressedwords.Thereof,37%havemorethanonemainstress,about27%arenotassignedanystressand36%arestressedinthewrongposition.Thismeansthatwecanhopetoreducetheerrorsbyalmost2/3byusingphonologicalconstraints.WordstressassignmentisadifﬁcultprobleminGermanbecausetheunderlyingprocessesinvolvesomedeepermorphologicalknowledgewhichisnotavailabletothesimplemodel.Incomplexwords,stressmainlydependsonmorphologicalstructure(i.e.onthecompositionalityofcompoundsandonthestressingstatusofafﬁxes).Wordstressinsimplexwordsisassumedtodependonthesylla-blepositionwithinthewordstemandonsyllableweight.Thecurrentlanguage-independentapproachdoesnotmodeltheseprocesses,butonlycapturessomeofitsstatistics.Simpleconstraintscanhelptoovercometheprob-lemoflackingcontextbyexplicitlyrequiringthateverysyllablemusthaveexactlyonesyllablenu-cleusandthateverywordmusthaveexactlyonesyl-lablereceivingprimarystress.3.1ImplementationOurgoalistoﬁndthemostprobablesyllabiﬁedandstressedphonemizationofawordthatdoesnotviolatetheconstraints.Wetriedtwodifferentap-proachestoenforcetheconstraints.Intheﬁrstvariant(v1),wemodiﬁedtheproba-bilitymodeltoenforcetheconstraints.Eachstatenowcorrespondstoasequenceof4-tuplesconsist-ingofaletterl,aphonemesequencep,asyllableboundarytagb,anaccenttaga(asbefore)plustwo99

newﬂagsAandNwhichindicatewhetheranac-cent/nucleusprecedesornot.TheAandNﬂagsofthenewstateareafunctionofitsaccentandsyllableboundarytagandtheAandNﬂagoftheprecedingstate.Theyspliteachstateintofournewstates.Thenewtransitionprobabilitiesaredeﬁnedas:P(hl;p;b;aii|hl;p;b;aii−1i−k,A,N)Theprobabilityis0ifthetransitionviolatesacon-straint,e.g.,whentheAﬂagissetandaiindicatesanotheraccent.Apositivesideeffectofthesyllableﬂagisthatitstoresseparatephonemizationprobabilitiesforcon-sonantsinthesyllableonsetvs.consonantsinthecoda.Theﬂagintheonsetis0sincethenucleushasnotyetbeenencountered,whereasitissetto1inthecoda.InGerman,thiscane.g.helpinforsyllable-ﬁnaldevoicingofvoicedstopsandfricatives.Theincreaseinthenumberofstatesaggravatessparse-dataproblems.Therefore,weimplementedanothervariant(v2)whichusesthesamesetofstates(withAandNﬂags),butwiththetransitionproba-bilitiesoftheoriginalmodel,whichdidnotenforcetheconstraints.Instead,wemodiﬁedtheViterbial-gorithmtoeliminatetheinvalidtransitions:Forex-ample,atransitionfromastatewiththeAﬂagsettoastatewhereaiintroducesasecondstress,isal-waysignored.Onsmalldatasets,betterresultswereachievedwithv2(seeTable5).4MorphologicalPreprocessingInGerman,informationaboutmorphologicalboundariesisneededtocorrectlyinsertglottalstops[P]incomplexwords,todetermineirregularpro-nunciationofafﬁxes(vispronounced[v]inver-tikalbut[f]inver+ticker+n,andthesufﬁxsyllableheitisnotstressedalthoughsuperheavyandwordﬁnal)andtodisambiguateletters(e.g.eisalwayspronounced/@/whenoccurringininﬂectionalsuf-ﬁxes).Vowellengthandqualityhasbeenarguedtoalsodependonmorphologicalstructure(PounderandKommenda,1986).Furthermore,morphologi-calboundariesoverrundefaultsyllabiﬁcationrules,suchasthemaximumonsetprinciple.Applyingdefaultsyllabiﬁcationtotheword“Sternanis¨ol”wouldresultinasyllabiﬁcationintoSter-na-ni-s¨ol(andsubsequentphonemiza-tiontosomethinglike/StEö"na:nizø:l/)insteadofStern-a-nis-¨ol(/"StEönPani:sPø:l/).Syllabiﬁ-cationinturnaffectsphonemizationsincevoicedfricativesandstopsaredevoicedinsyllable-ﬁnalpo-sition.Morphologicalinformationalsohelpsforgraphemicparsingofwordssuchas“R¨oschen”(Engl:littlerose)wherethemorphologicalbound-arybetweenR¨osandchencausesthestringschtobetranscribedto/sç/insteadof/S/.Similarambigui-tiescanariseforallothersoundsthatarerepresentedbyseverallettersinorthography(e.g.doubledcon-sonants,diphtongs,ie,ph,th),andisalsovalidforEnglish.Finally,morphologicalinformationisalsocrucialtodeterminewordstressinmorphologicallycomplexwords.4.1MethodsforMorphologicalSegmentationGoodsegmentationperformanceonarbitrarywordsishardtoachieve.Wecomparedseveralapproacheswithdifferentamountsofbuilt-inknowledge.Themorphologicalinformationisencodedinthelet-terstring,wheredifferentdigitsrepresentdifferentkindsofmorphologicalboundaries(preﬁxes,stems,derivationalandinﬂectionalsufﬁxes).ManualAnnotationfromCELEXTodeterminetheupperboundofwhatcanbeachievedwhenexploitingperfectmorphologicalin-formation,weextractedmorphologicalboundariesandboundarytypesfromtheCELEXdatabase.Themanualannotationisnotperfectasitcon-tainssomeerrorsandmanycaseswherewordsarenotdecomposedentirely.Thewordstagged[F]for“lexicalizedinﬂection”,e.g.gedr¨angt(pastpartici-pleofdr¨angen,Engl:push)weredecomposedsemi-automaticallyforthepurposeofthisevaluation.Asexpected,annotatingwordswithCELEXmorpho-logicalsegmentationyieldedthebestg2pconver-sionresults.Manualannotationisonlyavailableforasmallnumberofwords.Therefore,onlyautomati-callyannotatedmorphologicalinformationcanscaleuptorealapplications.Rule-basedSystemsThetraditionalapproachistouselargemorphemelexicaandasetofrulesthatsegmentwordsintoaf-ﬁxesandstems.Drawbacksofusingsuchasystemarethehighdevelopmentcosts,limitedcoverage100

andproblemswithambiguityresolutionbetweenal-ternativeanalysesofaword.Thetworule-basedsystemsweevaluated,theETI4morphologicalsystemandSMOR5(Schmidetal.,2004),arebothhigh-qualitysystemswithlargelexicathathavebeendevelopedoverseveralyears.Theirperformanceresultscanhelptoestimatewhatcanrealisticallybeexpectedfromanautomaticseg-mentationsystem.Bothoftherule-basedsystemsachievedanF-scoreofapprox.80%morphologicalboundariescorrectwithrespecttoCELEXmanualannotation.UnsupervisedMorphologicalSystemsMostattractiveamongautomaticsystemsaremethodsthatuseunsupervisedlearning,becausetheserequireneitheranexpertlinguisttobuildlargerule-setsandlexicanorlargemanuallyannotatedwordlists,butonlylargeamountsoftokenizedtext,whichcanbeacquirede.g.fromtheinternet.Unsupervisedmethodsareinprinciple6language-independent,andcanthereforeeasilybeappliedtootherlanguages.Wecomparedfourdifferentstate-of-the-artunsu-pervisedsystemsformorphologicaldecomposition(cf.(Demberg,2006;Demberg,2007)).Thealgo-rithmsweretrainedonaGermannewspapercor-pus(taz),containingabout240millionwords.Thesamealgorithmshavepreviouslybeenshowntohelpaspeechrecognitiontask(Kurimoetal.,2006).5ExperimentalEvaluations5.1TrainingSetandTestSetDesignTheGermancorpususedintheseexperimentsisCELEX(GermanLinguisticUserGuide,1995).CELEXcontainsaphonemicrepresentationofeach4EloquentTechnology,Inc.(ETI)TTSsystem.http://www.mindspring.com/˜ssshp/ssshp_cd/ss_eloq.htm5ThelexiconusedbySMOR,IMSLEX,containsmorpho-logicallycomplexentries,whichleadstohighprecisionandlowrecall.TheresultsreportedhererefertoaversionofSMOR,wherethelexiconentriesweredecomposedusingaratherna¨ıvehigh-recallsegmentationmethod.SMORitselfdoesnotdisam-biguatemorphologicalanalysesofaword.OurversionusedtransitionweightslearntfromCELEXmorphologicalannota-tion.Formoredetailsreferto(Demberg,2006).6Mostsystemsmakesomeassumptionsabouttheunderly-ingmorphologicalsystem,forinstancethatmorphologyisaconcatenativeprocess,thatstemshaveacertainminimallengthorthatpreﬁxingandsufﬁxingarethemostrelevantphenomena.word,syllableboundariesandwordstressinfor-mation.Furthermore,itcontainsmanuallyveriﬁedmorphologicalboundaries.Ourtrainingsetcontainsapprox.240,000wordsandthetestsetconsistsof12,326words.Thetestsetisdesignedsuchthatwordstemsintrainingandtestsetsaredisjoint,i.e.theinﬂectionsofacertainstemareeitherallinthetrainingsetorallinthetestset.Stemoverlapbetweentrainingandtestsetonlyoccursincompoundsandderivations.Ifasimplerandomsplitting(90%fortrainingset,10%fortestset)isusedoninﬂectedcorpora,resultsaremuchbetter:Worderrorrates(WER)areabout60%lowerwhenthesetofstemsintrainingandtestsetarenotdisjoint.Thesameeffectcanalsobeobservedforthesyllabiﬁcationtask(seeTable4).5.2ResultsfortheJointn-gramModelThejointn-grammodelislanguage-independent.Analignedcorpuswithwordsandtheirpronuncia-tionsisneeded,butnofurtheradaptationisrequired.Table1showstheperformanceofourmodelincomparisontoalternativeapproachesontheGermanandEnglishversionsoftheCELEXcorpus,theEn-glishNetTalkcorpus,theEnglishTeacher’sWordBook(TWB)corpus,theEnglishbeepcorpusandtheFrenchBrulexcorpus.Thejointn-grammodelperformssigniﬁcantlybetterthanthedecisiontree(essentiallybasedon(LucassenandMercer,1984)),andachievesscorescomparabletothePronuncia-tionbyAnalogy(PbA)algorithm(MarchandandDamper,2005).FortheNettalkdata,wealsocom-paredtheinﬂuenceofsyllableboundaryannotationfroma)automaticallylearntandb)manuallyanno-tatedsyllabiﬁcationinformationonphonemeaccu-racy.Automaticsyllabiﬁcationforourmodelin-tegratedphonologicalconstraints(asdescribedinsection3.1),andthereforeledtoanimprovementinphonemeaccuracy,whiletheworderrorratein-creasedforthePbAapproach,whichdoesnotincor-poratesuchconstraints.(Chen,2003)alsousedajointn-grammodel.ThetwoapproachesdifferinthatChenusessmallchunks(h(l:|0..1|):(p:|0..1|)ipairsonly)andit-erativelyoptimizesletter-phonemealignmentduringtraining.Chensmootheshigher-orderMarkovMod-elswithGaussianPriorsandimplementsadditionallanguagemodellingsuchasconsonantdoubling.101

corpussizejntn-grPbAChendec.treeG-CELEX230k7.5%15.0%E-Nettalk20k35.4%34.65%34.6%a)auto.syll35.3%35.2%b)man.syll29.4%28.3%E-TWB18k28.5%28.2%E-beep200k14.3%13.3%E-CELEX100k23.7%31.7%F-Brulex27k10.9%Table1:Worderrorratesfordifferentg2pconver-sionalgorithms.ConstraintswereonlyusedintheE-Nettalkauto.syllcondition.5.3BeneﬁtofIntegratingConstraintsTheaccuracyimprovementsachievedbyintegrat-ingtheconstraints(seeTable2)arehighlystatis-ticallysigniﬁcant.Thenumbersforconditions“G-syllab.+stress+g2p”and“E-syllab.+g2p”inTable2differfromthenumbersfor“G-CELEX”and“E-Nettalk”inTable1becausephonemeconversionerrors,syllabiﬁcationerrorsandstressassignmenterrorsareallcountedtowardsworderrorratesre-portedinTable2.Worderrorrateinthecombinedg2p-syllable-stressmodelwasreducedfrom21.5%to13.7%.Fortheseparatetasks,weobservedsimilareffects:Theworderrorrateforinsertingsyllableboundarieswasreducedfrom3.48%to3.1%onlettersandfrom1.84%to1.53%onphonemes.Mostsigniﬁcantly,worderrorratewasdecreasedfrom30.9%to9.9%forwordstressassignmentongraphemes.WealsofoundsimilarlyimportantimprovementswhenapplyingthesyllabiﬁcationconstrainttoEn-glishgrapheme-to-phonemeconversionandsyllabi-ﬁcation.Thissuggeststhatourﬁndingsarenotspe-ciﬁctoGermanbutthatthiskindofgeneralcon-straintscanbebeneﬁcialforarangeoflanguages.noconstr.constraint(s)G-syllab.+stress+g2p21.5%13.7%G-syllab.onletters3.5%3.1%G-syllab.onphonemes1.84%1.53%G-stressassignm.onletters30.9%9.9%E-syllab.+g2p40.5%37.5%E-syllab.onphonemes12.7%8.8%Table2:Improvingperformanceong2pconver-sion,syllabiﬁcationandstressassignmentthroughtheintroductionofconstraints.ThetableshowsworderrorratesforGermanCELEX(G)andEn-glishNetTalk(E).5.4ModularityModularityisanadvantageiftheindividualcompo-nentsaremorespecializedtotheirtask(e.g.byap-plyingaparticularlevelofdescriptionoftheprob-lem,orbyincorporatingsomeadditionalsourceofknowledge).Inamodularsystem,onecomponentcaneasilybesubstitutedbyanother–forexample,ifabetterwayofdoingstressassignmentinGermanwasfound.Ontheotherhand,keepingeverythinginonemoduleforstronglyinter-dependenttasks(suchasdeterminingwordstressandphonemization)al-lowsustosimultaneouslyoptimizeforthebestcom-binationofphonemesandstress.Bestresultswereobtainedfromthejointn-grammodelthatdoessyllabiﬁcation,stressassignmentandg2pconversionallinasinglestepandinte-gratesphonologicalconstraintsforsyllabiﬁcationandwordstress(WER=14.4%usingmethodv1,WER=13.7%usingmethodv2).Ifthemodularar-chitectureischosen,bestresultsareobtainedwheng2pconversionisdonebeforesyllabiﬁcationandstressassignment(15.2%WER),whereasdoingsyl-labiﬁcationandstressassignmentﬁrstandtheng2pconversionleadstoaWERof16.6%.Wecancon-cludefromthisﬁndingthatanintegratedapproachissuperiortoapipelinearchitectureforstronglyinter-dependenttaskssuchasthese.5.5TheContributionofMorphologicalPreprocessingAstatisticallysigniﬁcant(accordingtoatwo-tailedt-test)improvementing2pconversionaccuracy(from13.7%WERto13.2%WER)wasobtainedwiththemanuallyannotatedmorphologicalbound-ariesfromCELEX.Thesegmentationfrombothoftherule-basedsystems(ETIandSMOR)alsore-sultedinanaccuracyincreasewithrespecttothebaseline(13.6%WER),whichisnotannotatedwithmorphologicalboundaries.Amongtheunsupervisedsystems,bestresults7ontheg2ptaskwithmorphologicalannotationwereob-tainedwiththeRePortSsystem(KeshavaandPitler,2006).Butnoneofthesegmentationsledtoaner-rorreductionwhencomparedtoabaselinethatusednomorphologicalinformation(seeTable3).Worderrorrateevenincreasedwhenthequalityofthe7Forallresultsreferto(Demberg,2006).102

Precis.RecallF-Meas.WERRePortS(unsuperv.)71.1%50.7%59.2%15.1%nomorphology13.7%SMOR(rule-based)87.1%80.4%83.6%ETI(rule-based)75.4%84.1%79.5%13.6%CELEX(manual)100%100%100%13.2%Table3:SystemsevaluationonGermanCELEXmanualannotationandontheg2ptaskusingajointn-grammodel.WERsrefertoimplementationv2.morphologicalsegmentationwastoolow(theunsu-pervisedalgorithmsachieved52%-62%F-measurewithrespecttoCELEXmanualannotation).Table4showsthathigh-qualitymorphologicalinformationcanalsosigniﬁcantlyimproveperfor-manceonasyllabiﬁcationtaskforGerman.Weusedthesyllabiﬁerdescribedin(Schmidetal.,2005),whichworkssimilartothejointn-grammodelusedforg2pconversion.Justasforg2pconversion,wefoundasigniﬁcantaccuracyimprovementwhenus-ingthemanuallyannotateddata,asmallerimprove-mentforusingdatafromtherule-basedmorpholog-icalsystem,andnoimprovementwhenusingseg-mentationsfromanunsupervisedalgorithm.Syllab-iﬁcationworksbestwhenperformedonphonemes,becausesyllablesarephonologicalunitsandthere-forecanbedeterminedmosteasilyintermsofphonologicalentitiessuchasphonemes.Whethermorphologicalsegmentationisworththeeffortdependsonmanyfactorssuchastrainingsetsize,theg2palgorithmandthelanguageconsidered.disj.stemsrandomRePortS(unsupervisedmorph.)4.95%nomorphology3.10%0.72%ETI(rule-basedmorph.)2.63%CELEX(manualannot.)1.91%0.53%onphonemes1.53%0.18%Table4:Worderrorrates(WER)forsyllabiﬁcationwithajointn-grammodelfortwodifferenttrainingandtestsetdesigns(seeSection5.1).MorphologyforDataSparsenessReductionProbablythemostimportantaspectofmorpho-logicalsegmentationinformationisthatitcanhelptoresolvedatasparsenessissues.Becauseofthead-ditionalknowledgegiventothesystemthroughthemorphologicalinformation,similarly-behavinglet-tersequencescanbegroupedmoreeffectively.Therefore,wehypothesizedthatmorphologicalinformationismostbeneﬁcialinsituationswherethetrainingcorpusisrathersmall.Ourﬁndingscon-ﬁrmthisexpectation,astherelativeerrorreductionthroughmorphologicalannotationforatrainingcor-pusof9,600wordsis6.67%,whileitisonly3.65%fora240,000-wordtrainingcorpus.Inourimplementation,thestressﬂagsandsylla-bleﬂagsweusetoenforcethephonologicalcon-straintsincreasedatasparseness.Wefoundv2(theimplementationthatusesthestateswithoutstressandsyllableﬂagsandenforcestheconstraintsbyeliminatinginvalidtransitions,cf.section3.1)tooutperformtheintegratedversion,v1,andmoresig-niﬁcantlyinthecaseofmoreseveredatasparseness.Theonlyconditionwhenwefoundv1toperformbetterthanv2waswithalargedatasetandaddi-tionaldatasparsenessreductionthroughmorpholog-icalannotation,asinsection4(seeTable5).WER:designsv1v2datasetsize240k9.6k240k9.6knomorph.14.4%32.3%13.7%25.5%CELEX12.5%29%13.2%23.8%Table5:Theinteractionsofconstraintsintraininganddifferentlevelsofdatasparseness.g2pConversionAlgorithmsThebeneﬁtofusingmorphologicalpreprocessingisalsoaffectedbythealgorithmthatisusedforg2pconversion.Therefore,wealsoevaluatedtherelativeimprovementofmorphologicalannotationwhenus-ingadecisiontreeforg2pconversion.Decisiontreeswereoneoftheﬁrstdata-basedap-proachestog2pandarestillwidelyused(KienappelandKneser,2001;Blacketal.,1998).Thetree’sefﬁciencyandabilityforgeneralizationlargelyde-pendsonpruningandthechoiceofpossibleques-tions.Inourimplementation,thedecisiontreecanaskaboutletterswithinacontextwindowofﬁvebackandﬁveahead,aboutﬁvephonemesbackandgroupsofletters(e.g.consonantsvs.vowels).Boththedecisiontreeandthejointn-grammodelconvertgraphemestophonemes,insertsyllableboundariesandassignwordstressinasinglestep(markedas“WER-ss”inTable6.Theimple-mentationofthejointn-grammodelincorporatesthephonologicalconstraintsdescribedinsection3(“WER-ss+”).Ourmainﬁndingisthatthejointn-grammodelproﬁtslessfrommorphologicalan-notation.Withouttheconstraints,theperformance103

differenceissmaller:thejointn-grammodelthenachievesaworderrorrateof21.5%ontheno-morphology-condition.Inveryrecentwork,(Demberg,2007)developedanunsupervisedalgorithm(f-meas:68%;anexten-sionofRePortS)whosesegmentationsimproveg2pwhenusingathedecisiontree(PER:3.45%).decisiontreejointn-gramPERWER-ssPERWER-ss+RePortS3.83%28.3%15.1%nomorph.3.63%26.59%2.52%13.7%ETI2.8%21.13%2.53%13.6%CELEX2.64%21.64%2.36%13.2%Table6:Theeffectofmorphologicalpreprocessingonphonemeerrorrates(PER)andworderrorrates(WER)ingrapheme-to-phonemeconversion.MorphologyforotherLanguagesWealsoinvestigatedtheeffectofmorphologicalinformationong2pconversionandsyllabiﬁcationinEnglish,usingmanuallyannotatedmorphologicalboundariesfromCELEXandtheautomaticunsuper-visedRePortSsystemwhichachievesanF-scoreofabout77%forEnglish.Thecaseswheremorpho-logicalinformationaffectswordpronunciationarerelativelyfewincomparisontoGerman,thereforetheoveralleffectisratherweakandwedidnotevenﬁndimprovementswithperfectboundaries.6ConclusionsOurresultsconﬁrmthattheintegrationofphonolog-icalconstraints‘onenucleuspersyllable’and‘onemainstressperword’cansigniﬁcantlyboostac-curacyforg2pconversioninGermanandEnglish.Weimplementedtheconstraintsusingajointn-grammodelforg2pconversion,whichislanguage-independentandwell-suitedtotheg2ptask.Wesystematicallyevaluatedthebeneﬁttobegainedfrommorphologicalpreprocessingong2pconversionandsyllabiﬁcation.Wefoundthatmor-phologicalsegmentationsfromrule-basedsystemsledtosomeimprovement.Butthemagnitudeoftheaccuracyimprovementstronglydependsontheg2palgorithmandontrainingsetsize.State-of-the-artunsupervisedmorphologicalsystemsdonotyetyieldsufﬁcientlygoodsegmentationstohelpthetask,ifagoodconversionalgorithmisused:Lowqualitysegmentationevenledtohighererrorrates.AcknowledgmentsWewouldliketothankHinrichSch¨utze,FrankKellerandtheACLreviewersforvaluablecommentsanddiscussion.TheﬁrstauthorwassupportedbyEvangelischesStudienwerke.V.Villigst.ReferencesM.BisaniandH.Ney.2002.Investigationsonjointmultigrammodelsforgrapheme-to-phonemeconversion.InICSLP.A.Black,K.Lenzo,andV.Pagel.1998.Issuesinbuildinggen-erallettertosoundrules.In3.ESCAonSpeechSynthesis.SFChenandJGoodman.1996.Anempiricalstudyofsmooth-ingtechniquesforlanguagemodeling.InProc.ofACL.S.F.Chen.2003.Conditionalandjointmodelsforgrapheme-to-phonemeconversion.InEurospeech.V.Demberg.2006.Letter-to-phonemeconversionforaGer-manTTS-System.Master’sthesis.IMS,Univ.ofStuttgart.V.Demberg.2007.Alanguage-independentunsupervisedmodelformorphologicalsegmentation.InProc.ofACL-07.L.GalescuandJ.Allen.2001.Bi-directionalconversionbe-tweengraphemesandphonemesusingajointn-grammodel.InProc.ofthe4thISCAWorkshoponSpeechSynthesis.CELEXGermanLinguisticUserGuide,1995.CenterforLex-icalInformation.Max-Planck-InstitutforPsycholinguistics,Nijmegen.M.Jessen,1998.WordProsodicSystemsintheLanguagesofEurope.MoutondeGruyter:Berlin.S.KeshavaandE.Pitler.2006.Asimpler,intuitiveapproachtomorphemeinduction.InProceedingsof2ndPascalChal-lengesWorkshop,pages31–35,Venice,Italy.A.K.KienappelandR.Kneser.2001.Designingverycom-pactdecisiontreesforgrapheme-to-phonemetranscription.InEurospeech,Scandinavia.M.Kurimo,M.Creutz,M.Varjokallio,E.Arisoy,andM.Sar-aclar.2006.Unsupervsiedsegmentationofwordsintomor-phemes–Challenge2005:Anintroductionandevaluationreport.InProc.of2ndPascalChallengesWorkshop,Italy.J.LucassenandR.Mercer.1984.Aninformationtheoreticapproachtotheautomaticdeterminationofphonemicbase-forms.InICASSP9.Y.MarchandandR.I.Damper.2005.Cansyllabiﬁcationim-provepronunciationbyanalogyofEnglish?NaturalLan-guageEngineering.W.Minker.1996.Grapheme-to-phonemeconversion-anap-proachbasedonhiddenmarkovmodels.B.M¨obius.2001.GermanandMultilingualSpeechSynthesis.phoneticAIMS,ArbeitspapieredesInstitutsf¨urMaschinelleSpachverarbeitung.K.M¨uller.2001.Automaticdetectionofsyllableboundariescombiningtheadvantagesoftreebankandbracketedcorporatraining.InProceedingsofACL,pages402–409.A.PounderandM.Kommenda.1986.MorphologicalanalysisforaGermantext-to-speechsystem.InCOLING1986.P.A.RentzepopoulosandG.K.Kokkinakis.1991.PhonemetographemeconversionusingHMM.InEurospeech.H.Schmid,A.Fitschen,andU.Heid.2004.SMOR:AGermancomputationalmorphologycoveringderivation,compositionandinﬂection.InProc.ofLREC.H.Schmid,B.M¨obius,andJ.Weidenkaff.2005.Taggingsyl-lableboundarieswithhiddenMarkovmodels.IMS,unpub.R.Sproat.1996.Multilingualtextanalysisfortext-to-speechsynthesis.InProc.ICSLP’96,Philadelphia,PA.P.Taylor.2005.HiddenMarkovmodelsforgraphemetophonemeconversion.InINTERSPEECH.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 104–111,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

104

RedundancyRatio:AnInvariantPropertyoftheConsonantInventoriesoftheWorld’sLanguagesAnimeshMukherjee,MonojitChoudhury,AnupamBasu,NiloyGangulyDepartmentofComputerScienceandEngineering,IndianInstituteofTechnology,Kharagpur{animeshm,monojit,anupam,niloy}@cse.iitkgp.ernet.inAbstractInthispaper,weputforwardaninformationtheoreticdeﬁnitionoftheredundancythatisobservedacrossthesoundinventoriesoftheworld’slanguages.Throughrigorousstatis-ticalanalysis,weﬁndthatthisredundancyisaninvariantpropertyoftheconsonantin-ventories.Thestatisticalanalysisfurtherun-foldsthatthevowelinventoriesdonotex-hibitanysuchproperty,whichinturnpointstothefactthattheorganizingprinciplesofthevowelandtheconsonantinventoriesarequitedifferentinnature.1IntroductionRedundancyisastrikinglycommonphenomenonthatisobservedacrossmanynaturalsystems.Thisredundancyispresentmainlytoreducetheriskofthecompletelossofinformationthatmightoc-curduetoaccidentalerrors(KrakauerandPlotkin,2002).Moreover,redundancyisfoundineverylevelofgranularityofasystem.Forinstance,inbiologi-calsystemsweﬁndredundancyinthecodons(Lesk,2002),inthegenes(Woollard,2005)andaswellintheproteins(Gatlin,1974).Alinguisticsystemisalsonotanexception.Thereisforexample,anum-berofwordswiththesamemeaning(synonyms)inalmosteverylanguageoftheworld.Similarly,thebasicunitoflanguage,thehumanspeechsoundsorthephonemes,isalsoexpectedtoexhibitsomesortofaredundancyintheinformationthatitencodes.Inthiswork,weattempttomathematicallycap-turetheredundancyobservedacrossthesound(morespeciﬁcallytheconsonant)inventoriesoftheworld’slanguages.Forthispurpose,wepresentaninformationtheoreticdeﬁnitionofredun-dancy,whichiscalculatedbasedonthesetoffea-tures1(Trubetzkoy,1931)thatareusedtoexpresstheconsonants.Aninterestingobservationisthatthisquantitativefeature-basedmeasureofredun-dancyisalmostaninvarianceovertheconsonantinventoriesoftheworld’slanguages.Theobserva-tionisimportantsinceitcanshedenoughlightontheorganizationoftheconsonantinventories,whichunlikethevowelinventories,lackacompleteandholisticexplanation.Theinvarianceofourmeasureimpliesthateveryinventorytriestobesimilarintermsofthemeasure,whichleadsustoarguethatredundancyplaysaveryimportantroleinshapingthestructureoftheconsonantinventories.Inordertovalidatethisargumentwedeterminethepossibil-ityofobservingsuchaninvarianceiftheconsonantinventorieshadevolvedbyrandomchance.Weﬁndthattheredundancyobservedacrosstherandomlygeneratedinventoriesissubstantiallydifferentfromtheirrealcounterparts,whichleadsustoconcludethattheinvarianceisnotjust“by-chance”andthemeasurethatwedeﬁne,indeed,largelygovernstheorganizingprinciplesoftheconsonantinventories.1Inphonology,featuresaretheelements,whichdistin-guishonephonemefromanother.Thefeaturesthatdistinguishtheconsonantscanbebroadlycategorizedintothreedifferentclassesnamelythemannerofarticulation,theplaceofarticu-lationandphonation.Mannerofarticulationspeciﬁeshowtheﬂowofairtakesplaceinthevocaltractduringarticulationofaconsonant,whereasplaceofarticulationspeciﬁestheactivespeechorganandalsotheplacewhereitacts.Phonationde-scribestheactivityregardingthevibrationofthevocalcordsduringthearticulationofaconsonant.105

Interestingly,thisredundancy,whenmeasuredforthevowelinventories,doesnotexhibitanysimilarinvariance.Thisimmediatelyrevealsthattheprin-ciplesthatgoverntheformationofthesetwotypesofinventoriesarequitedifferentinnature.Suchanobservationissigniﬁcantsincewhetherornottheseprinciplesaresimilar/differentforthetwoin-ventorieshadbeenaquestiongivingrisetoperen-nialdebateamongthepastresearchers(Trubet-zkoy,1969/1939;LindblomandMaddieson,1988;Boersma,1998;Clements,2004).Apossiblerea-sonfortheobserveddichotomyinthebehaviorofthevowelandconsonantinventorieswithrespecttoredundancycanbeasfollows:whiletheorganiza-tionofthevowelinventoriesisknowntobegov-ernedbyasingleforce-themaximalperceptualcontrast(Jakobson,1941;LiljencrantsandLind-blom,1972;deBoer,2000)),consonantinvento-riesareshapedbyacomplexinterplayofseveralforces(Mukherjeeetal.,2006).Theinvarianceofredundancy,perhaps,reﬂectssomesortofanequi-libriumthatarisesfromtheinteractionofthesedi-vergentforces.Therestofthepaperisstructuredasfollows.Insection2webrieﬂydiscusstheearlierworksincon-nectiontothesoundinventoriesandthensystemat-icallybuildupthequantitativedeﬁnitionofredun-dancyfromthelinguistictheoriesthatarealreadyavailableintheliterature.Section3detailsoutthedatasourcenecessaryfortheexperiments,describesthebaselinefortheexperiments,reportstheexper-imentsperformed,andpresentstheresultsobtainedeachtimecomparingthesamewiththebaselinere-sults.Finallyweconcludeinsection4bysumma-rizingourcontributions,pointingoutsomeoftheimplicationsofthecurrentworkandindicatingthepossiblefuturedirections.2FormulationofRedundancyLinguisticresearchhasdocumentedawiderangeofregularitiesacrossthesoundsystemsoftheworld’slanguages.Ithasbeenpostulatedearlierbyfunc-tionalphonologiststhatsuchregularitiesarethecon-sequencesofcertaingeneralprincipleslikemaxi-malperceptualcontrast(LiljencrantsandLindblom,1972),whichisdesirablebetweenthephonemesofalanguageforproperperceptionofeachindivid-ualphonemeinanoisyenvironment,easeofartic-ulation(LindblomandMaddieson,1988;deBoer,2000),whichrequiresthatthesoundsystemsofalllanguagesareformedofcertainuniversal(andhighlyfrequent)sounds,andeaseoflearnability(deBoer,2000),whichisnecessaryforaspeakertolearnthesoundsofalanguagewithminimumef-fort.Infact,theorganizationofthevowelinven-tories(especiallythosewithasmallersize)acrosslanguageshasbeensatisfactorilyexplainedintermsofthesingleprincipleofmaximalperceptualcon-trast(Jakobson,1941;LiljencrantsandLindblom,1972;deBoer,2000).Ontheotherhand,inspiteofseveralat-tempts(LindblomandMaddieson,1988;Boersma,1998;Clements,2004)theorganizationofthecon-sonantinventorieslacksasatisfactoryexplanation.However,oneoftheearliestobservationsabouttheconsonantinventorieshasbeenthatconsonantstendtooccurinpairsthatexhibitstrongcorrelationintermsoftheirfeatures(Trubetzkoy,1931).Inor-dertoexplainthesetrends,featureeconomywasproposedastheorganizingprincipleofthecon-sonantinventories(Martinet,1955).Accordingtothisprinciple,languagestendtomaximizethecom-binatorialpossibilitiesofafewdistinctivefeaturestogeneratealargenumberofconsonants.Stateddifferently,agivenconsonantwillhaveahigherthanexpectedchanceofoccurrenceininventoriesinwhichallofitsfeatureshavedistinctivelyoccurredinotherconsonants.Theideaisillustrated,withanexample,throughTable1.Variousattemptshavebeenmadeinthepasttoexplaintheaforementionedtrendsthroughlinguisticinsights(Boersma,1998;Clements,2004)mainlyestablishingtheirstatisticalsigniﬁcance.Onthecontrary,therehasbeenverylittleworkpertainingtothequantiﬁcationoffeatureeconomyexceptin(Clements,2004),wheretheau-thordeﬁneseconomyindex,whichistheratioofthesizeofaninventorytothenumberoffeaturesthatcharacterizestheinventory.However,thisdeﬁnitiondoesnottakeintoaccountthecomplexitythatisin-volvedincommunicatingtheinformationabouttheinventoryintermsofitsconstituentfeatures.Inspiredbytheaforementionedstudiesandtheconceptsofinformationtheory(ShannonandWeaver,1949)wetrytoquantitativelycapturetheamountofredundancyfoundacrosstheconsonant106

plosivevoicedvoicelessdental/d//t/bilabial/b//p/Table1:Thetableshowsfourplosives.Ifalanguagehasinitsconsonantinventoryanythreeofthefourphonemeslistedinthistable,thenthereisahigherthanaveragechancethatitwillalsohavethefourthphonemeofthetableinitsinventory.inventoriesintermsoftheirconstituentfeatures.Letusassumethatwewanttocommunicatetheinfor-mationaboutaninventoryofsizeNoveratransmis-sionchannel.Ideally,oneshouldrequirelogNbitstodothesame(wherethelogarithmiswithrespecttobase2).However,sinceeverynaturalsystemistosomeextentredundantandlanguagesarenoex-ceptions,thenumberofbitsactuallyusedtoencodetheinformationismorethanlogN.Ifweassumethatthefeaturesarebooleaninnature,thenwecancomputethenumberofbitsusedbyalanguagetoencodetheinformationaboutitsinventorybymea-suringtheentropyasfollows.ForaninventoryofsizeNlettherebepfconsonantsforwhichapartic-ularfeaturef(wherefisassumedtobebooleaninnature)ispresentandqfotherconsonantsforwhichthesameisabsent.Thustheprobabilitythatapar-ticularconsonantchosenuniformlyatrandomfromthisinventoryhasthefeaturefispfNandtheprob-abilitythattheconsonantlacksthefeaturefisqfN(=1–pfN).IfFisthesetofallfeaturespresentintheconsonantsformingtheinventory,thenfeatureentropyFEcanbeexpressedasFE=Xf∈F(−pfNlogpfN−qfNlogqfN)(1)FEisthereforethemeasureoftheminimumnumberofbitsthatisrequiredtocommunicatetheinforma-tionabouttheentireinventorythroughthetransmis-sionchannel.ThelowerthevalueofFEthebetteritisintermsoftheinformationtransmissionover-head.Inordertocapturetheredundancyinvolvedintheencodingwedeﬁnethetermredundancyratioasfollows,RR=FElogN(2)whichexpressestheexcessnumberofbitsthatisusedbytheconstituentconsonantsoftheinventoryFigure1:TheprocessofcomputingRRforahypo-theticalinventory.intermsofaratio.TheprocessofcomputingthevalueofRRforahypotheticalconsonantinventoryisillustratedinFigure1.Inthefollowingsection,wepresenttheexperi-mentalsetupandalsoreporttheexperimentswhichweperformbasedontheabovedeﬁnitionofredun-dancy.Wesubsequentlyshowthatredundancyratioisinvariantacrosstheconsonantinventorieswhereasthesameisnottrueinthecaseofthevowelinvento-ries.3ExperimentsandResultsInthissectionwediscussthedatasourcenecessaryfortheexperiments,describethebaselinefortheexperiments,reporttheexperimentsperformed,andpresenttheresultsobtainedeachtimecomparingthesamewiththebaselineresults.3.1DataSourceManytypologicalstudies(LadefogedandMad-dieson,1996;LindblomandMaddieson,1988)ofsegmentalinventorieshavebeencarriedoutinpastontheUCLAPhonologicalSegmentInven-toryDatabase(UPSID)(Maddieson,1984).UPSIDgathersphonologicalsystemsoflanguagesfromallovertheworld,samplingmoreorlessuniformlyallthelinguisticfamilies.InthisworkwehaveusedUPSIDcomprisingof317languagesand541con-sonantsfoundacrossthem,forourexperiments.107

3.2RedundancyRatioacrosstheConsonantInventoriesInthissectionwemeasuretheredundancyratio(de-scribedearlier)oftheconsonantinventoriesofthelanguagesrecordedinUPSID.Figure2showsthescatter-plotoftheredundancyratioRRofeachoftheconsonantinventories(y-axis)versustheinven-torysize(x-axis).Theplotimmediatelyrevealsthatthemeasure(i.e.,RR)isalmostinvariantacrosstheconsonantinventorieswithrespecttotheinventorysize.Infact,wecanﬁtthescatter-plotwithastraightline(bymeansofleastsquareregression),whichasdepictedinFigure2,hasanegligibleslope(m=–0.018)andthisinturnfurtherconﬁrmstheabovefactthatRRisaninvariantpropertyoftheconso-nantinventorieswithregardtotheirsize.Itisim-portanttomentionherethatinthisexperimentwereporttheredundancyratioofalltheinventoriesofsizelessthanorequalto40.Weneglecttheinven-toriesofthesizegreaterthan40sincetheyareex-tremelyrare(lessthan0.5%ofthelanguagesofUP-SID),andtherefore,cannotprovideuswithstatis-ticallymeaningfulestimates.Thesameconventionhasbeenfollowedinallthesubsequentexperiments.Nevertheless,wehavealsocomputedthevaluesofRRforlargerinventories,wherebywehavefoundthatforaninventorysize≤60theresultsaresim-ilartothosereportedhere.ItisinterestingtonotethatthelargestoftheconsonantinventoriesGa(size=173)hasanRR=1.9,whichislowerthanalltheotherinventories.TheaforementionedclaimthatRRisaninvari-antacrossconsonantinventoriescanbevalidatedbyperformingastandardtestofhypothesis.Forthispurpose,werandomlyconstructlanguageinvento-ries,asdiscussedlater,andformulateanullhypoth-esisbasedonthem.NullHypothesis:TheinvarianceinthedistributionofRRsobservedacrosstherealconsonantinvento-riesisalsoprevalentacrosstherandomlygeneratedinventories.Havingformulatedthenullhypothesiswenowsystematicallyattempttorejectthesamewithaveryhighprobability.Forthispurposeweﬁrstconstructrandominventoriesandthenperformatwosamplet-test(Cohen,1995)comparingtheRRsoftherealandtherandominventories.TheresultsshowthatFigure2:Thescatter-plotoftheredundancyratioRRofeachoftheconsonantinventories(y-axis)versustheinventorysize(x-axis).Thestraightline-ﬁtisalsodepictedbytheboldlineintheﬁgure.indeedthenullhypothesiscanberejectedwithaveryhighprobability.Weproceedasfollows.3.2.1ConstructionofRandomInventoriesWeemploytwodifferentmodelstogeneratetherandominventories.Intheﬁrstmodeltheinvento-riesareﬁlleduniformlyatrandomfromthepoolof541consonants.Inthesecondmodelweassumethatthedistributionoftheoccurrenceoftheconso-nantsoverlanguagesisknownapriori.Notethatinbothofthesecases,thesizeoftherandomin-ventoriesissameasitsrealcounterpart.TheresultsshowthatthedistributionofRRsobtainedfromthesecondmodelhasaclosermatchwiththerealin-ventoriesthanthatoftheﬁrstmodel.Thisindicatesthattheoccurrencefrequencytosomeextentgov-ernsthelawoforganizationoftheconsonantinven-tories.Thedetailofeachofthemodelsfollow.ModelI–PurelyRandomModel:Inthismodelweassumethatthedistributionoftheconsonantin-ventorysizeisknownapriori.ForeachlanguageinventoryLletthesizerecordedinUPSIDbede-notedbysL.Lettherebe317binscorrespondingtoeachconsonantinventoryL.AbincorrespondingtoaninventoryLispackedwithsLconsonantschosenuniformlyatrandom(withoutrepetition)fromthepoolof541availableconsonants.Thustheconso-nantinventoriesofthe317languagescorrespondingtothebinsaregenerated.Themethodissummarized108

inAlgorithm1.forI=1to317doforsize=1tosLdoChooseaconsonantcuniformlyatrandom(withoutrepetition)fromthepoolof541availableconsonants;PacktheconsonantcinthebincorrespondingtotheinventoryL;endendAlgorithm1:Algorithmtoconstructrandomin-ventoriesusingModelIModelII–OccurrenceFrequencybasedRandomModel:ForeachconsonantcletthefrequencyofoccurrenceinUPSIDbedenotedbyfc.Lettherebe317binseachcorrespondingtoalanguageinUP-SID.fcbinsarethenchosenuniformlyatrandomandtheconsonantcispackedintothesebins.Thustheconsonantinventoriesofthe317languagescor-respondingtothebinsaregenerated.TheentireideaissummarizedinAlgorithm2.foreachconsonantcdofori=1tofcdoChooseoneofthe317bins,correspondingtothelanguagesinUPSID,uniformlyatrandom;Packtheconsonantcintothebinsochosenifithasnotbeenalreadypackedintothisbinearlier;endendAlgorithm2:Algorithmtoconstructrandomin-ventoriesusingModelII3.2.2ResultsObtainedfromtheRandomModelsInthissectionweenumeratetheresultsobtainedbycomputingtheRRsoftherandomlygeneratedinventoriesusingModelIandModelIIrespectively.Wecomparetheresultswiththoseoftherealinven-ParametersRealInv.RandomInv.Mean2.511773.59331SDV0.2095310.475072ParametersValuest12.15DF66p≤9.289e-17Table2:Theresultsofthet-testcomparingthedis-tributionofRRsfortherealandtherandominvento-ries(obtainedthroughModelI).SDV:standarddevi-ation,t:t-valueofthetest,DF:degreesoffreedom,p:residualuncertainty.toriesandineachcaseshowthatthenullhypothesiscanberejectedwithasigniﬁcantlyhighprobability.ResultsfromModelI:Figure3illustrates,foralltheinventoriesobtainedfrom100differentsimula-tionrunsofAlgorithm1,theaverageredundancyratioexhibitedbytheinventoriesofaparticularsize(y-axis),versustheinventorysize(x-axis).Theterm“redundancyratioexhibitedbytheinventoriesofaparticularsize”actuallymeansthefollowing.Lettherebenconsonantinventoriesofaparticu-larinventory-sizek.Theaverageredundancyra-tiooftheinventoriesofsizekisthereforegivenby1nPni=1RRiwhereRRisigniﬁestheredundancyra-tiooftheithinventoryofsizek.InFigure3wealsopresentthesamecurvefortherealconsonantinven-toriesappearinginUPSID.Inthesecurveswefur-therdepicttheerrorbarsspanningtheentirerangeofvaluesstartingfromtheminimumRRtothemax-imumRRforagiveninventorysize.Thecurvesshowthatincaseofrealinventoriestheerrorbarsspanaverysmallrangeascomparedtothatoftherandomlyconstructedones.Moreover,theslopesofthecurvesarealsosigniﬁcantlydifferent.Inordertotestwhetherthisdifferenceissigniﬁcant,weper-format-testcomparingthedistributionoftheval-uesofRRthatgivesrisetosuchcurvesfortherealandtherandominventories.TheresultsofthetestarenotedinTable2.ThesestatisticsclearlyshowsthatthedistributionofRRsfortherealandtheran-dominventoriesaresigniﬁcantlydifferentinnature.Stateddifferently,wecanrejectthenullhypothesiswith(100-9.29e-15)%conﬁdence.ResultsfromModelII:Figure4illustrates,foralltheinventoriesobtainedfrom100differentsimu-109

Figure3:Curvesshowingtheaverageredundancyratioexhibitedbytherealaswellastherandomin-ventories(obtainedthroughModelI)ofaparticularsize(y-axis),versustheinventorysize(x-axis).lationrunsofAlgorithm2,theaverageredundancyratioexhibitedbytheinventoriesofaparticularsize(y-axis),versustheinventorysize(x-axis).Theﬁg-ureshowsthesamecurvefortherealconsonantin-ventoriesalso.Foreachofthecurve,theerrorbarsspantheentirerangeofvaluesstartingfromthemin-imumRRtothemaximumRRforagiveninventorysize.Itisquiteevidentfromtheﬁgurethattheerrorbarsforthecurverepresentingtherealinventoriesaresmallerthanthoseoftherandomones.Thena-tureofthetwocurvesarealsodifferentthoughthedifferenceisnotaspronouncedasincaseofModelI.Thisisindicativeofthefactthatitisnotonlytheoc-currencefrequencythatgovernstheorganizationoftheconsonantinventoriesandthereisamorecom-plexphenomenonthatresultsinsuchaninvariantproperty.Infact,inthiscasealso,thet-teststatisticscomparingthedistributionofRRsfortherealandtherandominventories,reportedinTable3,allowsustorejectthenullhypothesiswith(100–2.55e–3)%conﬁdence.3.3ComparisonwithVowelInventoriesUntilnowwehavebeenlookingintotheorganiza-tionalaspectsoftheconsonantinventories.Inthissectionweshowthatthisorganizationislargelydif-ferentfromthatofthevowelinventoriesinthesensethatthereisnosuchinvarianceobservedacrossthevowelinventoriesunlikethatofconsonants.ForthisreasonwestartbycomputingtheRRsofallFigure4:Curvesshowingtheaverageredundancyratioexhibitedbytherealaswellastherandomin-ventories(obtainedthroughModelII)ofaparticularsize(y-axis),versustheinventorysize(x-axis).ParametersRealInv.RandomInv.Mean2.511772.76679SDV0.2095310.228017ParametersValuest4.583DF60p≤2.552e-05Table3:Theresultsofthet-testcomparingthedis-tributionofRRsfortherealandtherandominven-tories(obtainedthroughModelII).thevowelinventoriesappearinginUPSID.Figure5showsthescatterplotoftheredundancyratioofeachofthevowelinventories(y-axis)versustheinven-torysize(x-axis).Theplotclearlyindicatesthatthemeasure(i.e.,RR)isnotinvariantacrossthevowelinventoriesandinfact,thestraightlinethatﬁtsthedistributionhasaslopeof–0.14,whichisaround10timeshigherthanthatoftheconsonantinventories.Figure6illustratestheaverageredundancyratioexhibitedbythevowelandtheconsonantinventoriesofaparticularsize(y-axis),versustheinventorysize(x-axis).TheerrorbarsindicatingthevariabilityofRRamongtheinventoriesofaﬁxedsizealsospanamuchlargerrangeforthevowelinventoriesthanfortheconsonantinventories.ThesigniﬁcanceofthedifferenceinthenatureofthedistributionofRRsforthevowelandtheconso-nantinventoriescanbeagainestimatedbyperform-ingat-test.Thenullhypothesisinthiscaseisasfollows.110

Figure5:Thescatter-plotoftheredundancyratioRRofeachofthevowelinventories(y-axis)versustheinventorysize(x-axis).Thestraightline-ﬁtisdepictedbytheboldlineintheﬁgure.Figure6:Curvesshowingtheaverageredundancyratioexhibitedbythevowelaswellastheconsonantinventoriesofaparticularsize(y-axis),versustheinventorysize(x-axis).NullHypothesis:ThenatureofthedistributionofRRsforthevowelandtheconsonantinventoriesissame.Wecannowperformthet-testtoverifywhetherwecanrejecttheabovehypothesis.Table4presentstheresultsofthetest.Thestatisticsimmediatelyconﬁrmsthatthenullhypothesiscanberejectedwith99.932%conﬁdence.ParametersConsonantInv.VowelInv.Mean2.511772.98797SDV0.2095310.726547ParametersValuest3.612DF54p≤0.000683Table4:Theresultsofthet-testcomparingthedis-tributionofRRsfortheconsonantandthevowelinventories.4Conclusions,DiscussionandFutureWorkInthispaperwehavemathematicallycapturedtheredundancyobservedacrossthesoundinventoriesoftheworld’slanguages.Westartedbysystematicallydeﬁningthetermredundancyratioandmeasuringthevalueofthesamefortheinventories.Someofourimportantﬁndingsare,1.Redundancyratioisaninvariantpropertyoftheconsonantinventorieswithrespecttotheinventorysize.2.Amorecomplexphenomenonthanmerelytheoccurrencefrequencyresultsinsuchaninvariance.3.Unliketheconsonantinventories,thevowelin-ventoriesarenotindicativeofsuchaninvariance.Untilnowwehaveconcentratedonestablishingtheinvarianceoftheredundancyratioacrosstheconsonantinventoriesratherthanreasoningwhyitcouldhaveemerged.Onepossiblewaytoanswerthisquestionistolookfortheerrorcorrectingca-pabilityoftheencodingschemethatnaturehadem-ployedforcharacterizationoftheconsonants.Ide-ally,ifredundancyhastobeinvariant,thenthisca-pabilityshouldbealmostconstant.Asaproofofconceptwerandomlyselectaconsonantfromin-ventoriesofdifferentsizeandcomputeitshammingdistancefromtherestoftheconsonantsintheinven-tory.Figure7showsforarandomlychosenconso-nantcfromaninventoryofsize10,15,20and30respectively,thenumberoftheconsonantsatapar-ticularhammingdistancefromc(y-axis)versusthehammingdistance(x-axis).Thecurveclearlyindi-catesthatmajorityoftheconsonantsareataham-mingdistanceof4fromc,whichinturnimpliesthattheencodingschemehasalmostaﬁxederrorcor-rectingcapabilityof1bit.Thiscanbetheprecisereasonbehindtheinvarianceoftheredundancyra-111

Figure7:Histogramsshowingthethenumberofconsonantsataparticularhammingdistance(y-axis),fromarandomlychosenconsonantc,versusthehammingdistance(x-axis).tio.Initialstudiesintothevowelinventoriesshowthatforarandomlychosenvowel,itshammingdis-tancefromtheothervowelsinthesameinventoryvarieswiththeinventorysize.Inotherwords,theer-rorcorrectingcapabilityofavowelinventoryseemstobedependentonthesizeoftheinventory.Webelievethattheseresultsaresigniﬁcantaswellasinsightful.Nevertheless,oneshouldbeawareofthefactthattheformulationofRRheavilybanksonthesetoffeaturesthatareusedtorepresentthephonemes.Unfortunately,thereisnoconsensusonthesetofrepresentativefeatures,eventhoughtherearenumeroussuggestionsavailableintheliterature.However,thebasicconceptofRRandtheprocessofanalysispresentedhereisindependentofthechoiceofthefeatureset.InthecurrentstudywehaveusedthebinaryfeaturesprovidedinUPSID,whichcouldbeverywellreplacedbyotherrepresentations,in-cludingmulti-valuedfeaturesystems;welookfor-wardtodothesameasapartofourfuturework.ReferencesB.deBoer.2000.Self-organisationinvowelsystems.JournalofPhonetics,28(4),441–465.P.Boersma.1998.Functionalphonology,Doctoralthe-sis,UniversityofAmsterdam,TheHague:HollandAcademicGraphics.N.Clements.2004.Featuresandsoundinventories.SymposiumonPhonologicalTheory:RepresentationsandArchitecture,CUNY.P.R.Cohen.1995.Empiricalmethodsforartiﬁcialin-telligence,MITPress,Cambridge.L.L.Gatlin.1974.ConservationofShannon’sredun-dancyforproteinsJour.Mol.Evol.,3,189–208.R.Jakobson.1941.Kindersprache,aphasieundall-gemeinelautgesetze,Uppsala,ReprintedinSelectedWritingsI.Mouton,TheHague,1962,328-401.D.C.KrakauerandJ.B.Plotkin.2002.Redundancy,antiredundancy,andtherobustnessofgenomes.PNAS,99(3),1405-1409.A.M.Lesk.2002.Introductiontobioinformatics,Ox-fordUniversityPress,NewYork.P.LadefogedandI.Maddieson.1996.Soundsoftheworld’slanguages,Oxford:Blackwell.J.LiljencrantsandB.Lindblom.1972.Numericalsimu-lationofvowelqualitysystems:theroleofperceptualcontrast.Language,48,839–862.B.LindblomandI.Maddieson.1988.Phoneticuni-versalsinconsonantsystems.Language,Speech,andMind,62–78.I.Maddieson.1984.Patternsofsounds,CambridgeUni-versityPress,Cambridge.A.Martinet1955.`Economiedeschangementsphon´etiques,Berne:A.Francke.A.Mukherjee,M.Choudhury,A.BasuandN.Ganguly.2006.Modelingtheco-occurrenceprinciplesoftheconsonantinventories:Acomplexnetworkapproach.arXiv:physics/0606132(preprint).C.E.ShannonandW.Weaver.1949.Themathematicaltheoryofinformation,Urbana:UniversityofIllinoisPress.N.Trubetzkoy.1931.Diephonologischensysteme.TCLP,4,96–116.N.Trubetzkoy.1969.Principlesofphonology,Berkeley:UniversityofCaliforniaPress.A.Woollard.2005.Geneduplicationsandgeneticre-dundancyinC.elegans,WormBook.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 112–119,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

112

Multilingual Transliteration Using Feature based Phonetic Method  Su-Youn Yoon, Kyoung-Young Kim and Richard Sproat  University of Illinois at Urbana-Champaign {syoon9,kkim36,rws}@uiuc.edu   Abstract In this paper we investigate named entity transliteration based on a phonetic scoring method. The phonetic method is computed using phonetic features and carefully designed pseudo features. The proposed method is tested with four languages – Arabic, Chinese, Hindi and Korean – and one source language – English, using comparable corpora. The proposed method is developed from the phonetic method originally proposed in Tao et al. (2006). In contrast to the phonetic method in Tao et al. (2006) constructed on the basis of pure linguistic knowledge, the method in this study is trained using the Winnow machine learning algorithm. There is salient improvement in Hindi and Arabic compared to the previous study. Moreover, we demonstrate that the method can also achieve comparable results, when it is trained on language data different from the target language. The method can be applied both with minimal data, and without target language data for various languages.  1 Introduction. In this paper, we develop a multi-lingual transliteration system for named entities. Named entity transliteration is the process of producing, for a name in a source language, a set of one or more transliteration candidates in a target language. The correct transliteration of named entities is crucial, since they are frequent and important key words in information retrieval. In addition, requests in retrieving relevant documents in multiple languages require the development of the multi-lingual system.  The system is constructed using paired comparable texts. The comparable texts are about the same or related topics, but are not, in general, translations of each other. Using this data, the transliteration method aims to find transliteration correspondences in the paired languages. For example, if there were an English and Arabic newspaper on the same day, each of the newspapers would contain articles about the same important international events. From these comparable articles across the paired languages, the same named entities are expected to be found. Thus, from the named entities in an English newspaper, the method would find transliteration correspondences in comparable texts in other languages. The multi-lingual transliteration system entails solving several problems which are very challenging. First, it should show stable performance for many unrelated languages. The transliteration will be influenced by the difference in the phonological systems of the language pairs, and the process of transliteration differs according to the languages involved. For example, in Arabic texts, short vowels are rarely written while long vowels are written. When transliterating English names, the vowels are disappeared or written as long vowels. For example London is transliterated as lndn نﺪ(cid:1127)(cid:1127)(cid:1127)(cid:1127)(cid:1127)ﻨﻟ, and both vowels are not represented in the transliteration. However, Washington is often transliterated as  wSnjTwn نوطﻧــــــــــــﺷاو , and the final vowel is realized with long vowel. Transliterations in Chinese are very different from the original English pronunciation due to the 113

limited syllable structure and phoneme inventory of Chinese. For example, Chinese does not allow consonant clusters or coda consonants except [n,N], and this results in deletion, substitution of consonants or insertion of vowels. Thus while a syllable initial /d/ may surface as in Baghdad 巴格达 ba-ge-da, note that the syllable final /d/ is not represented. Multi-lingual transliteration system should solve these language dependent characteristics.  One of the most important concerns in a multilingual transliteration system is its applicability given a small amount of training data, or even no training data: for arbitrary language pairs, one cannot in general assume resources such as name dictionaries. Indeed, for some rarely spoken languages, it is practically impossible to find enough training data. Therefore, the proposed method aims to obtain comparable performance with little training data.  2 Previous Work Previous work — e.g. (Knight and Graehl, 1998; Meng et al., 2001; Al-Onaizan and Knight, 2002; Gao et al., 2004) — has mostly assumed that one has a training lexicon of transliteration pairs, from which one can learn a model, often a source-channel or MaxEnt-based model. Comparable corpora have been studied extensively in the literature, but transliteration in the context of comparable corpora has not been well addressed. In our work, we adopt the method proposed in (Tao et al., 2006) and apply it to the problem of transliteration. Measuring phonetic similarity between words has been studied for a long time. In many studies, two strings are aligned using a string alignment algorithm, and an edit distance (the sum of the cost for each edit operation), is used as the phonetic distance between them. The resulting distance depends on the costs of the edit operation. There are several approaches that use distinctive features to determine the costs of the edit operation. Gildea and Jurafsky (1996) counted the number of features whose values are different, and used them as a substitution cost. However, this approach has a crucial limitation: the cost does not consider the importance of the features. Nerbonne and Heeringa (1997) assigned a weight for each feature based on entropy and information gain, but the results were even less accurate than the method without weight. 3 Phonetic transliteration method In this paper, the phonetic transliteration is performed using the following steps:  1) Generation of the pronunciation for English words and target words: a. Pronunciations for English words are obtained using the Festival text-to-speech system (Taylor et al., 1998).  b. Target words are automatically converted into their phonemic level transcriptions by various language-dependent means. In the case of Mandarin Chinese, this is based on the standard Pinyin transliteration system. Arabic words are converted based on orthography, and the resulting transcriptions are reasonably correct except for the fact that short vowels were not represented. Similarly, the pronunciation of Hindi and Korean can be well-approximated based on the standard orthographic representation. All pronunciations are based on the WorldBet transliteration system (Hieronymus, 1995), an ascii-only version of the IPA. 2) Training a linear classifier using the Winnow algorithm: A linear classifier is trained using the training data which is composed of transliteration pairs and non-transliteration pairs. Transliteration pairs are extracted from the transliteration dictionary, while non-transliteration pairs are composed of an English named entity and a random word from the target language newspaper.  a. For all the training data, the pairs of pronunciations are aligned using standard string alignment algorithm based on Kruskal (1999). The substitution/insertion/deletion cost for the string alignment algorithm is based on the baseline cost from (Tao et al, 2006). b. All phonemes in the pronunciations are decomposed into their features. The features used in this study will be explained in detail in part 3.1.  c. For every phoneme pair (p1, p2) in the aligned pronunciations, a feature xi has a ‘+1’ value or a ‘–1‘ value:  xi =   +1   when p1 and p2  have the same values for feature xi −1   otherwise 114

d. A linear classifier is trained using the Winnow algorithm from the SNoW toolkit (Carlson et al., 1999).   3) Scoring English-target word pair: a. For a given English word, the score between it and a target word is computed using the linear classifier. b. The score ranges from 0 to any positive number, and the candidate with the highest score is selected as the transliteration of the given English name.   3.1  Feature set Halle and Clements (1983)’s distinctive features are used in order to model the substitution/ insertion/deletion costs for the string-alignment algorithm and linear classifier. A distinctive feature is a feature that describes the phonetic characteristics of phonetic segments. However, distinctive features alone are not enough to model the frequent sound change patterns that occur when words are adapted across languages. For example, stop and fricative consonants such as /p, t, k, b, d, g, s, z/ are frequently deleted when they appear in the coda position. This tendency is extremely salient when the target languages do not allow coda consonants or consonant clusters. For example, since Chinese only allows /n, N/ in coda position, stop consonants in the coda position are frequently lost; Stanford is transliterated as sitanfu, with the final /d/ lost. Since traditional distinctive features do not consider the position in the syllable, this pattern cannot be captured by distinctive features alone. To capture these sound change patterns, additional features such as “deletion of stop/fricative consonant in the coda position” must be considered.  Based on the pronunciation error data of learners of English as a second language as reported in (Swan and Smith, 2002), we propose the use of what we will term pseudofeatures. The pseudo features in this study are same as in Tao et al. (2006). Swan & Smith (2002)’s study covers 25 languages including Asian languages such as Thai, Korean, Chinese and Japanese, European languages such as German, Italian, French and Polish, and Middle East languages such as Arabic and Farsi. The substitution/insertion/deletion errors of phonemes were collected from this data. The following types of errors frequently occur in second language learners’ speech production.  (1) Substitution: If the learner’s first language does not have a particular phoneme found in English, it is substituted by the most similar phoneme in their first language. (2) Insertion: If the learner’s first language does not have a particular consonant cluster in English, a vowel is inserted. (3) Deletion: If the learner’s first language does not have a particular consonant cluster in English, one consonant in the consonant cluster is deleted. The same substitution/deletion/insertion patterns in a second language learner’s errors also appear in the transliteration of foreign names. The deletion of the stop consonant which appears in English-Chinese transliterations occurs frequently in the English pronunciation spoken by Chinese speakers. Therefore, the error patterns in second language learners’ can be used in transliteration. Based on (1) ~ (3), 21 pseudo features were designed. All features have binary values. Using these 21 pseudo features and 20 distinctive features, a linear classifier is trained. Some examples of pseudo features are presented in Table 1.   Pseudo-  Feature Description Example Consonant-coda Substitution of consonant feature in coda position  Sonorant-coda Substitution of sonorant feature in coda position Substitution between [N] and [g] in coda position in ArabicLabial-codaSubstitution of labial feature in coda position Substitution between [m] and [n] in coda position in Chinesej-exceptionSubstitution of [j] and [dZ] Spanish/Catalan and Festival errorw-exceptionSubstitution of [v] and [w] Chinese/Farsi and Festival error Table 1. Examples of pseudo features   115

3.2 Scoring the English-target word pair  A linear classifier is trained using the Winnow algorithm from the SNoW toolkit.  The Winnow algorithm is one of the update rules for linear classifier. A linear classifier is an algorithm to find a linear function that best separates the data. For the set of features X and set of weights W, the linear classifier is defined as [1] (Mitchell, T., 1997) 121201122  {,, ... }  {,, ...} ()   1        ...    0                -1  nnnnXxxxWwwwfxifwwxwxwxotherwise===++++>[1]  The linear function assigns label +1 when the paired target language word is the transliteration of given English word, while it assigns label –1 when it is not a transliteration of given English word.  The score of an English word and target word pair is computed using equation [2] which is part of the definition of f(x) in equation [1]. 01niiiwwx=+∑   [2] The output of equation [2] is termed the target node activation. If this value is high, class 1 is more activated, and the pair is more likely to be a transliteration pair. To illustrate, let us assume there are two candidates in target language (t1 and t2) for an English word e. If the score of (e, t1) is higher than the score of (e, t2), the pair (e, t1) has stronger activation than (e, t2). It means that t1  scores higher as the transliteration of e than t2. Therefore, the candidate with the highest score (in this case t1) is selected as the transliteration of the given English name. 4 Experiment and Results The linear function was trained for each language, separately. 500 transliteration pairs were randomly selected from each transliteration dictionary, and used as positive examples in the training procedure. This is quite small compared to previous approaches such as Knight and Graehl (1998) or Gao et al. (2004). In addition, 1500 words were randomly selected from the newspaper in the target languages, and paired with English words in the positive examples. A total of 750,000 pairs (500 English words×1500 target words) were generated, and used as negative examples in the training procedure. Table 2 presents the source of training data for each language.    Transliteration pair Target word ArabicNew Mexico State University Xinhua Arabic newswire ChineseBehavior Design Corporation Xinhua  Chinese  newswire Hindi Naidunia Hindi newswire  Naidunia Hindi newswire Koreanthe National  Institute of the Korean language Chosun  Korean  newspaper Table 2. Sources of the training data The phonetic transliteration method was evaluated using comparable corpora, consisting of newspaper articles in English and the target languages—Arabic, Chinese, Hindi, and Korean–from the same day, or almost the same day. Using comparable corpora, the named-entities for persons and locations were extracted from the English text; in this paper, the English named-entities were extracted using the named-entity recognizer described in Li et al. (2004), based on the SNoW machine learning toolkit (Carlson et al., 1999).  The transliteration task was performed using the following steps:  1) English text was tagged using the named-entity recognizer. The 200 most frequent named entities were extracted from seven days’ worth of the English newswire text. Among pronunciations of words generated by the Festival text-to speech system, 3% contained errors representing monophthongs instead of diphthongs or vice versa. 1.5% of all cases misrepresented single consonant, and 6% showed errors in the vowels. Overall, 10.5% of the tokens contained pronunciation errors which could trigger errors in transliteration. 2) To generate the Arabic and Hindi candidates, all words from the same seven days were extracted. In the case of Korean corpus, the collection of newspapers was from every five days, unlike the other three language corpora which were collected every day; therefore, candidates of Korean were 116

generated from one month of newspapers, since seven days of newspaper articles did not show a sufficient number of transliteration candidates. This caused the total number of candidates to be much bigger than for the other languages.  The words were stemmed all possible ways using simple hand-developed affix lists: for example, given a Hindi word c1c2c3, if both c3 and c2c3 are in the suffix and ending list, then this single word generated three possible candidates: c1, c1c2, and c1c2c3.  3) Segmenting Chinese sentences requires a dictionary or supervised segmenter. Since the goal is to use minimal knowledge or data from the target language, using supervised methods is inappropriate for our approach. Therefore, Chinese sentences were not segmented. Using the 495 characters that are frequently used for transliterating foreign names (Sproat et al., 1996), a sequence of three of more characters from the list was taken as a possible candidate for Chinese. 4) For the given 200 English named entities and target language candidate lists, all the possible pairings of English and target-language name were considered as possible transliteration pairs.  The number of candidates for each target language is presented in Table 3.  Language The number of candidates Arabic 12,466 Chinese 6,291 Hindi 10,169 Korean 42,757 Table 3. Number of candidates for each target language. 5) Node activation scores were calculated for each pair in the test data, and the candidates were ranked by their score. The candidate with the highest node activation score was selected as the transliteration of the given English name.  Some examples of English words and the top three ranking candidates among all of the potential target-language candidates were given in Tables 4, 5. Starred entries are correct.  Candidate English Word RankScript Romanization Arafat *1 2 3 阿拉法特拉法地奥拉维奇 a-la-fa-te la-fa-di-aola-wei-qi Table 4. Examples of the top-3 candidates in the transliteration of English – Chinese Candidate English Word RankScript Romanization *1 베트남 be-thu-nam2 베트남측 be-thu-nam-chug Vietnam 3 표준어와 pyo-jun-e-wa *1 오스트레일리아 o-su-thu-ley-il-li-a 2 웃돌아 us-tol-la Australia3 오스트레일리아에서 o-su-thu-ley-il-li-a-ey-se Table 5. Examples of the top-3 candidates in the transliteration of English-Korean To evaluate the proposed transliteration methods quantitatively, the Mean Reciprocal Rank (MRR), a measure commonly used in information retrieval when there is precisely one correct answer (Kandor and Vorhees, 2000) was measured, following Tao and Zhai (2005).   Since the evaluation data obtained from the comparable corpus was small, the systems were evaluated using both held-out data from the transliteration dictionary and comparable corpus.   First, the results of the held-out data will be presented. For a given English name and target language candidates, all possible combinations were generated. Table 6 presents the size of held-out data, and Table 7 presents MRR of the held-out data.   117

 Number of English named entities Number of Candidates in target language Number of total pairs used in the evaluationArabic 500 1,500 750,000 Chinese 500 1,500 750,000 Hindi 100 1,500 150,000 Korean 100 1,500 150,000 Table 6. Size of the test data Winnow  Baseline  Total feature distinctive feature only Arabic 0.66 0.74 0.70 Chinese 0.74 0.74 0.72 Hindi 0.87 0.91 0.91 Korean 0.82 0.85 0.82 Table 7. MRRs of the phonetic transliteration The baseline was computed using the phonetic transliteration method proposed in Tao et al. (2006). In contrast to the method in this study, the baseline system is purely based on linguistic knowledge. In the baseline system, the edit distance, which was the result of the string alignment algorithm, was used as the score of an English-target word pair. The performance of the edit distance was dependent on insertion/deletion/ substitution costs. These costs were determined based on the distinctive features and pseudo features, based on the pure linguistic knowledge without training data. As illustrated in Table 7, the phonetic transliteration method using features worked adequately for multilingual data, as phonetic features are universal, unlike the phonemes which are composed of them. Adopting phonetic features as the units for transliteration yielded the baseline performance.  In order to evaluate the effectiveness of pseudo features, the method was trained using two different feature sets: a total feature set and a distinctive feature-only set. For Arabic, Chinese and Korean, the MRR of the total feature set was higher than the MRR of the distinctive feature-only set. The improvement of the total set was 4% for Arabic, 2.6% for Chinese, 2.4% for Korean. There was no improvement of the total set in Hindi. In general, the pseudo features improved the accuracy of the transliteration. For all languages, the MRR of the Winnow algorithm with the total feature set was higher than the baseline. There was 7% improvement for Arabic, 0.7% improvement for Chinese, 4% improvement for Hindi and 3% improvement for Korean.   We turn now to the results on comparable corpora. We attempted to create a complete set of answers for the 200 English names in our test set, but part of the English names did not seem to have any standard transliteration in the target language according to the native speaker’s judgment. Accordingly, we removed these names from the evaluation set. Thus, the resulting list was less than 200 English names, as shown in the second column of Table 8; (Table 8 All). Furthermore, some correct transliterations were not found in our candidate list for the target languages, since the answer never occurred in the target news articles; (Table 8 Missing). Thus this results in a smaller number of candidates to evaluate. This smaller number is given in the fourth column of Table 8; (Table 8 Core).   Language# All # Missing #Core Arabic 192 121 71 Chinese 186 92 94 Hindi 144 83 61 Korean 195 114 81 Table 8. Number of evaluated English Name  MRRs were computed on the two sets represented by the count in column 2, and the smaller set represented by the count in column 4. We termed the former MRR “AllMRR” and the latter “CoreMRR”. In Table 9, “CoreMRR” and “AllMRR” of the method were presented.   118

Baseline  Winnow   All-MRR CoreMRR All-MRR CoreMRRArabic 0.20 0.53 0.22 0.61Chinese 0.25 0.49 0.25 0.50Hindi 0.30 0.69 0.36 0.86Korean 0.30 0.71 0.29 0.69Table 9. MRRs of the phonetic transliteration In both methods, CoreMRRs were higher than 0.49 for all languages. That is, if the answer is in the target language texts, then the method finds the correct answer within the top 2 words.  As with the previously discussed results, there were salient improvements in Arabic and Hindi when using the Winnow algorithm. The MRRs of the Winnow algorithm except Korean were higher than the baseline. There was 7% improvement for Arabic and 17% improvement for Hindi in CoreMRR. In contrast to the 3% improvement in held-out data, there was a 2% decrease in Korean: the MRRs of Korean from the Winnow algorithm were lower than baseline, possibly because of the limited size of the evaluation data. Similar to the results of held-out data, the improvement in Chinese was small (1%).  The MRRs of Hindi and the MRRs of Korean were higher than the MRRs of Arabic and Chinese. The lower MRRs of Arabic and Chinese may result from the phonological structures of the languages. In general, transliteration of English word into Arabic and Chinese is much more irregular than the transliteration into Hindi and Korean in terms of phonetics.   To test the applicability to languages for which training data is not available, we also investigated the use of models trained on language pairs different from the target language pair. Thus, for each test language pair, we evaluated the performance of models trained on each of the other language pairs. For example, three models were trained using Chinese, Hindi, and Korean, and they were tested with Arabic data. The CoreMRRs of this experiment were presented in Table 10. Note that the diagonal in this Table represents the within-language-pair training and testing scenario that we reported on above. test data  Arabic Chinese HindiKorean Arabic 0.61 0.50 0.860.63Chinese 0.59 0.50 0.800.66Hindi 0.59 0.54 0.860.67train-ing dataKorean 0.56 0.51 0.760.69Table 10. MRRs for the phonetic transliteration 2  For Arabic, Hindi, and Korean, MRRs were indeed the highest when the methods were trained using data from the same language, as indicated by the boldface MRR scores on the diagonal. In general, however, the MRRs were not saliently lower across the board when using different language data than using same-language data in training and testing. For all languages, MRRs for the cross-language case were best when the methods were trained using Hindi. The differences between MRRs of the method trained from Hindi and MRRs of the method by homogeneous language data were 2% for Arabic and Korean. In the case of Chinese, MRRs of the method trained by Hindi was actually better than MRRs obtained by Chinese training data. Hindi has a large phoneme inventory compared to Korean, Arabic, and Chinese, so the relationship between English phonemes and Hindi phonemes is relatively regular, and only small number of language specific transliteration rules exist. That is, the language specific influences from Hindi are smaller than those from other languages. This characteristic of Hindi may result in the high MRRs for other languages. What these results imply is that named entity transliteration could be performed without training data for the target language with phonetic feature as a unit. This approach is especially valuable for languages for which training data is minimal or lacking.  5 Conclusion In this paper, a phonetic method for multilingual transliteration was proposed. The method was based on string alignment, and linear classifiers trained using the Winnow algorithm. In order to learn both language-universal and language-specific transliteration characteristics, distinctive 119

features and pseudo features were used in training. The method can be trained using a small amount of training data, and the performance decreases only by a small degree when it is trained with a language different from the test data. Therefore, this method is extremely useful for underrepresented languages for which training data is difficult to find. Acknowledgments This work was funded the National Security Agency contract NBCHC040176 (REFLEX) and a Google Research grant.   References Y. Al-Onaizan and K. Knight. 2002. Machine transliteration of names in Arabic text. In Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages, Philadelphia, PA. Andrew J. Carlson, Chad M. Cumby, Jeff L. Rosen, and Dan Roth. 1999. The SNoW learning architecture. Technical Report UIUCDCS-R-99-2101, UIUC CS Dept. Wei Gao, Kam-Fai Wong, and Wai Lam. 2004. Phoneme based transliteration of foreign names for OOV problem. Proceeding of IJCNLP, 374–381. Daniel Gildea and Daniel Jurafsky. 1996. Learning Bias and Phonological-Rule Induction. Computational Linguistics 22(4):497–530. Morris Halle and G.N. Clements. 1983. Problem book in phonology. MIT press, Cambridge. James Hieronymus. 1995. Ascii phonetic symbols for the world’s languages: Worldbet. http://www.ling.ohio-tate.edu/ edwards/worldbet.pdf. Paul B. Kantor and Ellen B. Voorhees. 2000. The TREC-5 confusion track: Comparing retrieval methods for scanned text. Information Retrieval, 2: 165–176. Kevin Knight and Jonathan Graehl. 1998. Machine transliteration. Computational Linguistics, 24(4). Joseph B. Kruskal. 1999. An overview of sequence comparison. Time Warps, String Edits, and Macromolecules, CSLI, 2nd edition, 1–44. Xin Li, Paul Morie, and Dan Roth. 2004. Robust reading: Identification and tracing of ambiguous names. Proceeding of NAACL-2004. H.M. Meng, W.K Lo, B. Chen, and K. Tang. 2001. Generating phonetic cognates to handle named entities in English-Chinese cross-language spoken document retrieval. In Proceedings of the Automatic Speech Recognition and Understanding Workshop. Tom M. Mitchell. 1997. Machine Learning, McCraw-Hill, Boston. John Nerbonne and Wilbert Heeringa. 1997. Measuring Dialect Distance Phonetically. Proceedings of the 3rd Meeting of the ACL Special Interest Group in Computational Phonology. Richard Sproat, Chilin. Shih, William A. Gale, and Nancy Chang. 1996. A stochastic finite-state word-segmentation algorithm for Chinese. Computational Linguistics, 22(3).  Michael Swan and Bernard Smith. 2002. Learner English, Cambridge University Press, Cambridge . Tao Tao and ChengXiang Zhai. 2005. Mining comparable bilingual text corpora for cross-language information integration. Proceeding of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining, 691–696. Tao Tao, Su-Youn Yoon, Andrew Fister, Richard Sproat and ChengXiang Zhai. "Unsupervised Named Entity Transliteration Using Temporal and Phonetic Correlation." EMNLP, July 22-23, 2006, Sydney, Australia. Paul A. Taylor, Alan Black, and Richard Caley. 1998. The architecture of the Festival speech synthesis system. Proceedings of the Third ESCAWorkshop on SpeechSynthesis, 147–151. Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 120–127,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

120

      Abstract Words of foreign origin are referred to as borrowed words or loanwords. A loanword is usually imported to Chinese by phonetic transliteration if a translation is not easily available. Semantic transliteration is seen as a good tradition in introducing foreign words to Chinese. Not only does it preserve how a word sounds in the source language, it also carries forward the word’s original semantic attributes. This paper attempts to automate the semantic transliteration process for the first time. We conduct an inquiry into the feasibility of semantic transliteration and propose a probabilistic model for transliterating personal names in Latin script into Chinese. The results show that semantic transliteration substantially and consistently improves accuracy over phonetic transliteration in all the experiments. 1 Introduction The study of Chinese transliteration dates back to the seventh century when Buddhist scriptures were translated into Chinese. The earliest bit of Chinese translation theory related to transliteration may be the principle of “Names should follow their bearers, while things should follow Chinese.” In other words, names should be transliterated, while things should be translated according to their meanings. The same theory still holds today.  Transliteration has been practiced in several ways, including phonetic transliteration and phonetic-semantic transliteration. By phonetic transliteration, we mean rewriting a foreign word in native grapheme such that its original pronunciation is preserved. For example, London becomes 伦敦 /Lun-Dun/1 which does not carry any clear connotations. Phonetic transliteration represents the common practice in transliteration. Phonetic-semantic transliteration, hereafter referred to as semantic transliteration for short, is an advanced translation technique that is considered as a recommended translation practice for centuries. It translates a foreign word by preserving both its original pronunciation and meaning. For example, Xu Guangqi2 translated geo- in geometry into Chinese as 几何 /Ji-He/, which carries the pronunciation of geo- and expresses the meaning of “a science concerned with measuring the earth”.  Many of the loanwords exist in today’s Chinese through semantic transliteration, which has been well received (Hu and Xu, 2003; Hu, 2004) by the people because of many advantages. Here we just name a few. (1) It brings in not only the sound, but also the meaning that fills in the semantic blank left by phonetic transliteration. This also reminds people that it is a loanword and avoids misleading; (2) It provides etymological clues that make it easy to trace back to the root of the words. For example, a transliterated Japanese name will maintain its Japanese identity in its Chinese appearance; (3) It evokes desirable associations, for example, an English girl’s name is transliterated with Chinese characters that have clear feminine association, thus maintaining the gender identity.                                                  1 Hereafter, Chinese characters are also denoted in Pinyin ro-manization system, for ease of reference.  2 Xu Quangqi (1562–1633) translated The Original Manu-script of Geometry to Chinese jointly with Matteo Ricci. Semantic Transliteration of Personal Names Haizhou Li*,  Khe Chai Sim*,  Jin-Shea Kuo†,  Minghui Dong* *Institute for Infocomm Research Singapore 119613 {hli,kcsim,mhdong}@i2r.a-star.edu.sg †Chung-Hwa Telecom Laboratories Taiwan jskuo@cht.com.tw 121

Unfortunately, most of the reported work in the area of machine transliteration has not ventured into semantic transliteration yet. The Latin-scripted personal names are always assumed to homogeneously follow the English phonic rules in automatic transliteration (Li et al., 2004). Therefore, the same transliteration model is applied to all the names indiscriminatively. This assumption degrades the performance of transliteration because each language has its own phonic rule and the Chinese characters to be adopted depend on the following semantic attributes of a foreign name. (1) Language of origin: An English word is not necessarily of pure English origin. In English news reports about Asian happenings, an English personal name may have been originated from Chinese, Japanese or Korean. The language origin affects the phonic rules and the characters to be used in transliteration3. For example, a Japanese name Matsumoto should be transliterated as 松本 /Song-Ben/, instead of 马茨莫托 /Ma-Ci-Mo-Tuo/ as if it were an English name. (2) Gender association: A given name typically implies a clear gender association in both the source and target languages. For example, the Chinese transliterations of Alice and Alexandra are 爱丽丝 /Ai-Li-Si/ and 亚历山大 /Ya-Li-Shan-Da/ respectively, showing clear feminine and masculine characteristics. Transliterating Alice as 埃里斯 /Ai-Li-Si/ is phonetically correct, but semantically inadequate due to an improper gender association. (3) Surname and given name: The Chinese name system is the original pattern of names in Eastern Asia such as China, Korea and Vietnam, in which a limited number of characters4 are used for surnames while those for given names are less restrictive. Even for English names, the character set for given name transliterations are different from that for surnames. Here are two examples of semantic transliteration for personal names.  George Bush                                                  3 In the literature (Knight  and  Graehl,1998; Qu et al., 2003), translating romanized Japanese or Chinese names to Chinese characters is also known as back-transliteration. For simplic-ity, we consider all conversions from Latin-scripted words to Chinese as transliteration in this paper. 4 The 19 most common surnames cover 55.6% percent of the Chinese population (Ning and Ning 1995). and Yamamoto Akiko are transliterated into 乔治(cid:0)布什and 山本 亚喜子 that arouse to the following associations: 乔治 /Qiao-Zhi/ - male given name, English origin; 布什 /Bu-Shi/ - surname, English origin; 山本 /Shan-Ben/ - surname, Japanese origin; 亚喜子 /Ya-Xi-Zi/ - female given name, Japanese origin.  In Section 2, we summarize the related work. In Section 3, we discuss the linguistic feasibility of semantic transliteration for personal names. Section 4 formulates a probabilistic model for semantic transliteration.  Section 5 reports the experiments. Finally, we conclude in Section 6. 2 Related Work In general, computational studies of transliteration fall into two categories: transliteration modeling and extraction of transliteration pairs. In transliteration modeling, transliteration rules are trained from a large, bilingual transliteration lexicon (Lin and Chen, 2002; Oh and Choi, 2005), with the objective of translating unknown words on the fly in an open, general domain. In the extraction of transliterations, data-driven methods are adopted to extract actual transliteration pairs from a corpus, in an effort to construct a large, up-to-date transliteration lexicon (Kuo et al., 2006; Sproat et al., 2006).  Phonetic transliteration can be considered as an extension to the traditional grapheme-to-phoneme (G2P) conversion (Galescu and Allen, 2001), which has been a much-researched topic in the field of speech processing. If we view the grapheme and phoneme as two symbolic representations of the same word in two different languages, then G2P is a transliteration task by itself. Although G2P and phonetic transliteration are common in many ways, transliteration has its unique challenges, especially as far as E-C transliteration is concerned. E-C transliteration is the conversion between English graphemes, phonetically associated English letters, and Chinese graphemes, characters which represent ideas or meanings. As a Chinese transliteration can arouse to certain connotations, the choice of Chinese characters becomes a topic of interest (Xu et al., 2006). Semantic transliteration can be seen as a subtask of statistical machine translation (SMT) with 122

monotonic word ordering. By treating a letter/character as a word and a group of letters/characters as a phrase or token unit in SMT, one can easily apply the traditional SMT models, such as the IBM generative model (Brown et al., 1993) or the phrase-based translation model (Crego et al., 2005) to transliteration. In transliteration, we face similar issues as in SMT, such as lexical mapping and alignment. However, transliteration is also different from general SMT in many ways. Unlike SMT where we aim at optimizing the semantic transfer, semantic transliteration needs to maintain the phonetic equivalence as well. In computational linguistic literature, much effort has been devoted to phonetic transliteration, such as English-Arabic, English-Chinese (Li et al., 2004), English-Japanese (Knight and Graehl, 1998) and English-Korean. In G2P studies, Font Llitjos and Black (2001) showed how knowledge of language of origin may improve conversion accuracy. Unfortunately semantic transliteration, which is considered as a good tradition in translation practice (Hu and Xu, 2003; Hu, 2004), has not been adequately addressed computationally in the literature. Some recent work (Li et al., 2006; Xu et al., 2006) has attempted to introduce preference into a probabilistic framework for selection of Chinese characters in phonetic transliteration. However, there is neither analytical result nor semantic-motivated transliteration solution being reported. 3 Feasibility of Semantic Transliteration A Latin-scripted personal name is written in letters, which represent the pronunciations closely, whereas each Chinese character represents not only the syllables, but also the semantic associations. Thus, character rendering is a vital issue in trans-literation. Good transliteration adequately projects semantic association while an inappropriate one may lead to undesirable interpretation. Is semantic transliteration possible? Let’s first conduct an inquiry into the feasibility of semantic transliteration on 3 bilingual name corpora, which are summarizied in Table 1 and will be used in experiments. E-C corpus is an augmented version of Xinhua English to Chinese dictionary  for English names (Xinhua, 1992). J-C corpus is a romanized Japanese to Chinese dictionary for Japanese names. The C-C corpus is a Chinese Pinyin to character dictionary for Chinese names. The entries are classified into surname, male and female given name categories. The E-C corpus also contains some entries without gender/surname labels, referred to as unclassified.   E-C J-C5 C-C6 Surname (S) 12,490 36,352 569,403 Given name (M) 3,201 35,767 345,044 Given name (F) 4,275 11,817 122,772 Unclassified 22,562 - - All 42,528 83,936 1,972,851 Table 1: Number of entries in 3 corpora  Phonetic transliteration has not been a problem as Chinese has over 400 unique syllables that are enough to approximately transcribe all syllables in other languages. Different Chinese characters may render into the same syllable and form a range of homonyms. Among the homonyms, those arousing positive meanings can be used for personal names. As discussed elsewhere (Sproat et al., 1996), out of several thousand common Chinese characters, a subset of a few hundred characters tends to be used overwhelmingly for transliterating English names to Chinese, e.g. only 731 Chinese characters are adopted in the E-C corpus. Although the character sets are shared across languages and genders, the statistics in Table 2 show that each semantic attribute is associated with some unique characters. In the C-C corpus, out of the total of 4,507 characters, only 776 of them are for surnames. It is interesting to find that female given names are represented by a smaller set of characters than that for male across 3 corpora.       E-C J-C C-C All S 327 2,129 776 2,612 (19.2%)M 504 1,399 4,340 4,995 (20.0%)F 479 1,178 1,318 2,192 (26.3%)All 731 (44.2%)2,533 (46.2%)4,507 (30.0%) 5,779 (53.6%)Table 2: Chinese character usage in 3 corpora. The numbers in brackets indicate the percentage of characters that are shared by at least 2 corpora.  Note that the overlap of Chinese characters usage across genders is higher than that across languages. For instance, there is a 44.2% overlap                                                  5 http://www.cjk.org 6 http://technology.chtsai.org/namelist 123

across gender for the transcribed English names; but only 19.2% overlap across languages for the surnames. In summary, the semantic attributes of personal names are characterized by the choice of characters, and therefore their n-gram statistics as well. If the attributes are known in advance, then the semantic transliteration is absolutely feasible. We may obtain the semantic attributes from the context through trigger words. For instance, from “Mr Tony Blair”, we realize “Tony” is a male given name while “Blair” is a surname; from  “Japanese Prime Minister Koizumi”, we resolve that “Koizumi” is a Japanese surname. In the case where contextual trigger words are not available, we study detecting the semantic attributes from the personal names themselves in the next section. 4 Formulation of Transliteration Model  Let S and T denote the name written in the source and target writing systems respectively. Within a probabilistic framework, a transliteration system produces the optimum target name, T*, which yields the highest posterior probability given the source name, S, i.e. )|(maxarg*STPTTST∈= (1) where ST is the set of all possible transliterations for the source name, S. The alignment between S and T is assumed implicit in the above formulation.  In a standard phonetic transliteration system, )|(STP, the posterior probability of the hypothe-sized transliteration, T, given the source name, S, is directly modeled without considering any form of semantic information. On the other hand, semantic transliteration described in this paper incorporates language of origin and gender information to cap-ture the semantic structure. To do so, )|(STP is rewritten as (|)PTS = ∑∈∈GLGLSGLTP,)|,,( (2)  = ∑∈∈GLGLSGLPGLSTP,)|,(),,|( (3) where (|,,)PTSLG is the transliteration probabil-ity from source S to target T, given the language of origin (L) and gender (G) labels. L and Gdenote the sets of languages and genders respectively. )|,(SGLP is the probability of the language and the gender given the source, S. Given the alignment between S and T, the transliteration probability given L and G may be written as  ),,|(GLSTP= 1111(|,)IiiiiPtTS−=∏ (4) ≈111(|,,)IiiiiiPttss−−=∏ (5)where is and itare the ith token of S and T respec-tively and I is the total number of tokens in both S and T. kjS and kjT represent the sequence of tokens ()1,,,jjksss+K and ()1,,,jjkttt+K respectively. Eq. (4) is in fact the n-gram likelihood of the token pair ,iits〈〉 sequence and Eq. (5) approximates this probability using a bigram language model. This model is conceptually similar to the joint source-channel model (Li et al., 2004) where the target to-ken it depends on not only its source token isbut also the history 1it−and 1is−. Each character in the target name forms a token. To obtain the source tokens, the source and target names in the training data are aligned using the EM algorithm. This yields a set of possible source tokens and a map-ping between the source and target tokens. During testing, each source name is first segmented into all possible token sequences given the token set. These source token sequences are mapped to the target sequences to yield an N-best list of translit-eration candidates. Each candidate is scored using an n-gram language model given by Eqs. (4) or (5). As in Eq. (3), the transliteration also greatly depends on the prior knowledge, )|,(SGLP. When no prior knowledge is available, a uniform probability distribution is assumed. By expressing )|,(SGLP in the following form, )|(),|()|,(SLPSLGPSGLP= (6) prior knowledge about language and gender may be incorporated. For example, if the language of S is known as sL, we have 1(|)0ssLLPLSLL=⎧=⎨≠⎩ (7) Similarly, if the gender information for S is known as sG, then, 124

1(|,)0ssGGPGLSGG=⎧=⎨≠⎩ (8) Note that personal names have clear semantic associations. In the case where the semantic attribute information is not available, we propose learning semantic information from the names themselves. Using Bayes’ theorem, we have )(),(),|()|,(SPGLPGLSPSGLP= (9) (|,)PSLG can be modeled using an n-gram lan-guage model for the letter sequence of all the Latin-scripted names in the training set. The prior probability, ),(GLP, is typically uniform. )(SP does not depend on L and G, thus can be omitted. Incorporating )|,(SGLPinto Eq. (3) can be viewed as performing a soft decision of the language and gender semantic attributes. By contrast, hard decision may also be performed based on maximum likelihood approach: argmax(|)sLLPSL∈=L (10) argmax(|,)sGGPSLG∈=G (11) where sL and sGare the detected language and gender of S respectively. Therefore, for hard deci-sion,)|,(SGLP is obtained by replacing sL and sG in Eq. (7) and (8) with sL and sGrespec-tively. Although hard decision eliminates the need to compute the likelihood scores for all possible pairs of L and G, the decision errors made in the early stage will propagate to the transliteration stage. This is potentially bad if a poor detector is used (see Table 9 in Section 5.3). If we are unable to model the prior knowledge of semantic attributes)|,(SGLP, then a more general model will be used for (|,,)PTSLG by dropping the dependency on the information that is not available. For example, Eq. (3) is reduced to(|,)(|)LPTSLPLS∈∑L if the gender information is missing. Note that when both language and gender are unknown, the system simplifies to the baseline phonetic transliteration system. 5 Experiments This section presents experiments on database of 3 language origins (Japanese, Chinese and English) and gender information (surname7, male and fe-male). In the experiments of determining the lan-guage origin, we used the full data set for the 3 lan-guages as in shown in Table 1. The training and test data for semantic transliteration are the subset of Table 1 comprising those with surnames, male and female given names labels. In this paper, J, C and E stand for Japanese, Chinese and English; S, M and F represent Surname, Male and Female given names, respectively.   # unique entries L Data set S M F All Train 21.7k 5.6k 1.7k 27.1k J Test 2.6k 518 276 2.9k Train 283 29.6k 9.2k 31.5k C Test 283 2.9k 1.2k 3.1k Train 12.5k 2.8k 3.8k 18.5k E Test 1.4k 367 429 2.1k Table 3: Number of unique entries in training and test sets, categorized by semantic attributes  Table 3 summarizes the number of unique8 name entries used in training and testing. The test sets were randomly chosen such that the amount of test data is approximately 10-20% of the whole corpus. There were no overlapping entries between the training and test data. Note that the Chinese sur-names are typically single characters in a small set; we assume there is no unseen surname in the test set. All the Chinese surname entries are used for both training and testing. 5.1 Language of Origin For each language of origin, a 4-gram language model was trained for the letter sequence of the source names, with a 1-letter shift.  Japanese Chinese English All 96.46 96.44 89.90 94.81 Table 4: Language detection accuracies (%) using a 4-gram language model for the letter sequence of the source name in Latin script.                                                  7 In this paper, surnames are treated as a special class of gen-der. Unlike given names, they do not have any gender associa-tion. Therefore, they fall into a third category which is neither male nor female.  8 By contrast, Table 1 shows the total number of name exam-ples available. For each unique entry, there may be multiple examples. 125

 Table 4 shows the language detection accuracies for all the 3 languages using Eq. (10). The overall detection accuracy is 94.81%. The corresponding Equal Error Rate (EER)9 is 4.52%. The detection results may be used directly to infer the semantic information for transliteration. Alternatively, the language model likelihood scores may be incorporated into the Bayesian framework to improve the transliteration performance, as described in Section 4. 5.2 Gender Association Similarly, gender detection10 was performed by training a 4-gram language model for the letter se-quence of the source names for each language and gender pair.   Language Male Female All Japanese 90.54 80.43 87.03 Chinese 64.34 71.66 66.52 English 75.20 72.26 73.62 Table 5: Gender detection accuracies (%) using a 4-gram language model for the letter sequence of the source name in Latin script.  Table 5 summarizes the gender detection accura-cies using Eq. (11) assuming language of origin is known, argmax(|,)ssGGPSLLG∈==G. The overall detection accuracies are 87.03%, 66.52% and 73.62% for Japanese, Chinese and English respec-tively. The corresponding EER are 13.1%, 21.8% and 19.3% respectively. Note that gender detection is generally harder than language detection. This is because the tokens (syllables) are shared very much across gender categories, while they are quite different from one language to another.  5.3 Semantic Transliteration The performance was measured using the Mean Reciprocal Rank (MRR) metric (Kantor and Voor-hees, 2000), a measure that is commonly used in information retrieval, assuming there is precisely one correct answer. Each transliteration system generated at most 50-best hypotheses for each                                                  9 EER is defined as the error of false acceptance and false re-jection when they are equal. 10 In most writing systems, the ordering of surname and given name is known. Therefore, gender detection is only performed for male and female classes. word when computing MRR. The word and char-acter accuracies of the top best hypotheses are also reported.  We used the phonetic transliteration system as the baseline to study the effects of semantic transliteration. The phonetic transliteration system was trained by pooling all the available training data from all the languages and genders to estimate a language model for the source-target token pairs. Table 6 compares the MRR performance of the baseline system using unigram and bigram language models for the source-target token pairs.   J C E All Unigram 0.5109 0.4869 0.2598 0.4443 Bigram 0.5412 0.5261 0.3395 0.4895 Table 6:  MRR performance of phonetic translit-eration for 3 corpora using unigram and bigram language models.  The MRR performance for Japanese and Chinese is in the range of 0.48-0.55. However, due to the small amount of training and test data, the MRR performance of the English name transliteration is slightly poor (approximately 0.26-0.34). In general, a bigram language model gave an overall relative improvement of 10.2% over a unigram model.   L G Set J C E S 0.5366 0.7426 0.4009 M 0.5992 0.5184 0.2875 F 0.4750 0.4945 0.1779 (cid:50) (cid:50) All 0.5412 0.5261 0.3395 S 0.6500 0.7971 0.7178 M 0.6733 0.5245 0.4978 F 0.5956 0.5191 0.4115 (cid:50) All 0.6491 0.5404 0.6228 S 0.6822 0.9969 0.7382 M 0.7267 0.6466 0.4319 F 0.5856 0.7844 0.4340 (cid:51) (cid:51) All 0.6811 0.7075 0.6294 S 0.6541 0.6733 0.7129 M 0.6974 0.5362 0.4821 F 0.5743 0.6574 0.4138 (cid:99) (cid:99) All 0.6477 0.5764 0.6168 Table 7: The effect of language and gender in-formation on the overall MRR performance of transliteration (L=Language, G=Gender, (cid:50)=unknown, (cid:51)=known, (cid:99)=soft decision).  Next, the scenarios with perfect language and/or gender information were considered. This com-126

parison is summarized in Table 7. All the MRR re-sults are based on transliteration systems using bi-gram language models. The table clearly shows that having perfect knowledge, denoted by “(cid:51)”, of language and gender helps improve the MRR per-formance; detecting semantic attributes using soft decision, denoted by “(cid:99)”, has a clear win over the baseline, denoted by “(cid:50)”, where semantic informa-tion is not used. The results strongly recommend the use of semantic transliteration for personal names in practice. Next let’s look into the effects of automatic language and gender detection on the performance.   J C E All (cid:50) 0.5412 0.5261 0.3395 0.4895 (cid:145) 0.6292 0.5290 0.5780 0.5734 (cid:99) 0.6162 0.5301 0.6088 0.5765 (cid:51) 0.6491 0.5404 0.6228 0.5952 Table 8: The effect of language detection schemes on MRR using bigram language models and unknown gender information (hereafter, (cid:50)=unknown, (cid:51)=known, (cid:145)=hard decision, (cid:99)=soft decision).  Table 8 compares the MRR performance of the semantic transliteration systems with different prior information, using bigram language models. Soft decision refers to the incorporation of the lan-guage model scores into the transliteration process to improve the prior knowledge in Bayesian infer-ence. Overall, both hard and soft decision methods gave similar MRR performance of approximately 0.5750, which was about 17.5% relatively im-provement compared to the phonetic transliteration system with 0.4895 MRR. The hard decision scheme owes its surprisingly good performance to the high detection accuracies (see Table 4).   S M F All (cid:50) 0.6825 0.5422 0.5062 0.5952 (cid:145) 0.7216 0.4674 0.5162 0.5855 (cid:99) 0.7216 0.5473 0.5878 0.6267 (cid:51) 0.7216 0.6368 0.6786 0.6812 Table 9: The effect of gender detection schemes on MRR using bigram language  models with perfect language information.  Similarly, the effect of various gender detection methods used to obtain the prior information is shown in Table 9. The language information was assumed known a-priori. Due to the poorer detection accuracy for the Chinese male given names (see Table 5), hard decision of gender had led to deterioration in MRR performance of the male names compared to the case where no prior information was assumed. Soft decision of gender yielded further gains of 17.1% and 13.9% relative improvements for male and female given names respectively, over the hard decision method.  Overall Accuracy (%) L G MRR Word Character (cid:50) (cid:50) 0.4895 36.87 58.39 (cid:50) 0.5952 46.92 65.18 (cid:51) (cid:51) 0.6812 58.16 70.76 (cid:145)(cid:145)0.5824 47.09 66.84 (cid:99) (cid:99) 0.6122 49.38 69.21 Table 10: Overall transliteration performance using bigram language model with various lan-guage and gender information.  Finally, Table 10 compares the performance of various semantic transliteration systems using bi-gram language models. The baseline phonetic transliteration system yielded 36.87% and 58.39% accuracies at word and character levels respec-tively; and 0.4895 MRR. It can be conjectured from the results that semantic transliteration is sub-stantially superior to phonetic transliteration. In particular, knowing the language information im-proved the overall MRR performance to 0.5952; and with additional gender information, the best performance of 0.6812 was obtained. Furthermore, both hard and soft decision of semantic informa-tion improved the performance, with the latter be-ing substantially better. Both the word and charac-ter accuracies improvements were consistent and have similar trend to that observed for MRR.  The performance of the semantic transliteration using soft decisions (last row of Table 10) achieved 25.1%, 33.9%, 18.5% relative improve-ment in MRR, word and character accuracies respectively over that of the phonetic transliteration (first row of Table 10). In addition, soft decision also presented 5.1%, 4.9% and 3.5% relative improvement over hard decision in MRR, word and character accuracies respectively. 5.4 Discussions It was found that the performance of the baseline phonetic transliteration may be greatly improved by incorporating semantic information such as the language of origin and gender. Furthermore, it was found that the soft decision of language and gender 127

outperforms the hard decision approach. The soft decision method incorporates the semantic scores (,|)PLGSwith transliteration scores(|,,)PTSLG, involving all possible semantic specific models in the decoding process.  In this paper, there are 9 such models (3 languages×3 genders). The hard decision relies on Eqs. (10) and (11) to decide language and gender, which only involves one semantic specific model in the decoding. Neither soft nor hard decision requires any prior information about the names. It provides substantial performance improvement over phonetic transliteration at a reasonable computational cost. If the prior semantic information is known, e.g. via trigger words, then semantic transliteration attains its best performance. 6 Conclusion Transliteration is a difficult, artistic human en-deavor, as rich as any other creative pursuit. Re-search on automatic transliteration has reported promising results for regular transliteration, where transliterations follow certain rules. The generative model works well as it is designed to capture regu-larities in terms of rules or patterns. This paper ex-tends the research by showing that semantic trans-literation of personal names is feasible and pro-vides substantial performance gains over phonetic transliteration.  This paper has presented a success-ful attempt towards semantic transliteration using personal name transliteration as a case study. It formulates a mathematical framework that incor-porates explicit semantic information (prior knowledge), or implicit one (through soft or hard decision) into the transliteration model. Extending the framework to machine transliteration of named entities in general is a topic for further research. References Peter F. Brown and Stephen Della Pietra and Vincent J. Della Pietra and Robert L. Mercer. 1993, The Mathe-matics of Statistical Machine Translation: Parameter Estimation, Computational Linguistics, 19(2), pp. 263-311. J. M. Crego, M. R. Costa-jussa and J. B. Mario and J. A. R. Fonollosa. 2005, N-gram-based versus Phrase-based Statistical Machine Translation, In Proc. of IWSLT, pp. 177-184. Ariadna Font Llitjos, Alan W. Black. 2001. Knowledge of language origin improves pronunciation accuracy of proper names. In Proc. of Eurospeech, Denmark, pp 1919-1922. Lucian Galescu and James F. Allen. 2001, Bi-directional Conversion between Graphemes and Pho-nemes using a Joint N-gram Model, In Proc. 4th ISCA Tutorial and Research Workshop on Speech Synthesis, Scotland, pp. 103-108. Peter Hu, 2004, Adapting English to Chinese, English Today, 20(2), pp. 34-39. Qingping Hu and Jun Xu, 2003, Semantic Translitera-tion: A Good Tradition in Translating Foreign Words into Chinese Babel: International Journal of Transla-tion, Babel, 49(4), pp. 310-326. Paul B. Kantor and Ellen M. Voorhees, 2000, The TREC-5 Confusion Track: Comparing Retrieval Methods for Scanned Text. Informational Retrieval, 2, pp. 165-176. K. Knight and J. Graehl. 1998. Machine Transliteration, Computational Linguistics 24(4), pp. 599-612. J.-S. Kuo, H. Li and Y.-K. Yang. 2006. Learning Trans-literation Lexicons from the Web, In Proc. of 44th ACL, pp. 1129-1136. Haizhou Li, Min Zhang and Jian Su. 2004. A Joint Source Channel Model for Machine Transliteration, In Proc. of 42nd ACL, pp. 159-166. Haizhou Li, Shuanhu Bai, and Jin-Shea Kuo, 2006, Transliteration, In Advances in Chinese Spoken Lan-guage Processing, C.-H. Lee, et al. (eds), World Sci-entific, pp. 341-364. Wei-Hao Lin and Hsin-Hsi Chen, 2002, Backward ma-chine transliteration by learning phonetic similarity, In Proc. of CoNLL , pp.139-145. Yegao Ning and Yun Ning, 1995, Chinese Personal Names, Federal Publications, Singapore. Jong-Hoon Oh and Key-Sun Choi. 2005, An Ensemble of Grapheme and Phoneme for Machine Translitera-tion, In Proc. of IJCNLP, pp.450-461. Y. Qu, G. Grefenstette and D. A. Evans, 2003, Auto-matic Transliteration for Japanese-to-English Text Re-trieval. In Proc. of 26th ACM SIGIR, pp. 353-360. Richard Sproat, C. Chih, W. Gale, and N. Chang. 1996. A stochastic Finite-state Word-segmentation Algo-rithm for Chinese, Computational Linguistics, 22(3), pp. 377-404. Richard Sproat, Tao Tao and ChengXiang Zhai. 2006. Named Entity Transliteration with Comparable Cor-pora, In Proc. of 44th ACL, pp. 73-80. Xinhua News Agency, 1992, Chinese Transliteration of Foreign Personal Names, The Commercial Press. L. Xu, A. Fujii, T. Ishikawa, 2006 Modeling Impression in Probabilistic Transliteration into Chinese, In Proc. of EMNLP 2006, Sydney,  pp. 242–249. Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 128–135,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

128

GeneratingComplexMorphologyforMachineTranslationEinatMinkov∗LanguageTechnologiesInstituteCarnegieMellonUniversityPittsburgh,PA,USAeinatm@cs.cmu.eduKristinaToutanovaMicrosoftResearchRedmond,WA,USAkristout@microsoft.comHisamiSuzukiMicrosoftResearchRedmond,WA,USAhisamis@microsoft.comAbstractWepresentanovelmethodforpredictingin-ﬂectedwordformsforgeneratingmorpho-logicallyrichlanguagesinmachinetrans-lation.Weutilizearichsetofsyntacticandmorphologicalknowledgesourcesfrombothsourceandtargetsentencesinaprob-abilisticmodel,andevaluatetheircontribu-tioningeneratingRussianandArabicsen-tences.Ourresultsshowthattheproposedmodelsubstantiallyoutperformsthecom-monlyusedbaselineofatrigramtargetlan-guagemodel;inparticular,theuseofmor-phologicalandsyntacticfeaturesleadstolargegainsinpredictionaccuracy.Wealsoshowthattheproposedmethodiseffectivewitharelativelysmallamountofdata.1IntroductionMachineTranslation(MT)qualityhasimprovedsubstantiallyinrecentyearsduetoapplyingdataintensivestatisticaltechniques.However,state-of-the-artapproachesareessentiallylexical,consider-ingeverysurfacewordorphraseinboththesourcesentenceandthecorrespondingtranslationasanin-dependententity.Ashortcomingofthisword-basedapproachisthatitissensitivetodatasparsity.Thisisanissueofimportanceasalignedcorporaareanex-pensiveresource,whichisnotabundantlyavailableformanylanguagepairs.Thisisparticularlyprob-lematicformorphologicallyrichlanguages,wherewordstemsarerealizedinmanydifferentsurfaceforms,whichexacerbatesthesparsityproblem.∗Thisresearchwasconductedduringtheauthor’sintern-shipatMicrosoftResearch.Inthispaper,weexploreanapproachinwhichwordsarerepresentedasacollectionofmorpholog-icalentities,andusethisinformationtoaidinMTformorphologicallyrichlanguages.Ourgoalistwo-fold:ﬁrst,toallowgeneralizationovermorphologytoalleviatethedatasparsityprobleminmorphologygeneration.Second,tomodelsyntacticcoherenceintheformofmorphologicalagreementinthetargetlanguagetoimprovethegenerationofmorphologi-callyrichlanguages.Sofar,thisproblemhasbeenaddressedinaverylimitedmannerinMT,mosttyp-icallybyusingatargetlanguagemodel.Intheframeworksuggestedinthispaper,wetrainamodelthatpredictstheinﬂectedformsofase-quenceofwordstemsinatargetsentence,giventhecorrespondingsourcesentence.Weusewordandwordalignmentinformation,aswellaslexi-calresourcesthatprovidemorphologicalinforma-tionaboutthewordsonboththesourceandtargetsides.Givenasentencepair,wealsoobtainsyntacticanalysisinformationforboththesourceandtrans-latedsentences.Wegeneratetheinﬂectedformsofwordsinthetargetsentenceusingalloftheavailableinformation,usingalog-linearmodelthatlearnstherelevantmappingfunctions.Asacasestudy,wefocusontheEnglish-RussianandEnglish-Arabiclanguagepairs.UnlikeEnglish,RussianandArabichaveveryrichsystemsofmor-phology,eachwithdistinctcharacteristics.Trans-latingfromamorphology-poortoamorphology-richlanguageisespeciallychallengingsincede-tailedmorphologicalinformationneedstobede-codedfromalanguagethatdoesnotencodethisin-formationordoessoonlyimplicitly(Koehn,2005).Webelievethattheselanguagepairsarerepresen-129

tativeinthisrespectandthereforedemonstratethegeneralityofourapproach.Thereareseveralcontributionsofthiswork.First,weproposeageneralapproachthatshowspromiseinaddressingthechallengesofMTintomorpholog-icallyrichlanguages.Weshowthattheuseofbothsyntacticandmorphologicalinformationimprovestranslationquality.Wealsoshowtheutilityofsourcelanguageinformationinpredictingthewordformsofthetargetlanguage.Finally,weachievetheseresultswithlimitedmorphologicalresourcesandtrainingdata,suggestingthattheapproachisgenerallyusefulforresource-scarcelanguagepairs.2RussianandArabicMorphologyTable1describesthemorphologicalfeaturesrele-vanttoRussianandArabic,alongwiththeirpossiblevalues.TherightmostcolumninthetablereferstothemorphologicalfeaturesthataresharedbyRus-sianandArabic,includingperson,number,genderandtense.Whilethesefeaturesarefairlygeneric(theyarealsopresentinEnglish),notethatRus-sianincludesanadditionalgender(neuter)andAra-bichasadistinctnumbernotionfortwo(dual).AcentraldimensionofRussianmorphologyiscasemarking,realizedassufﬁxationonnounsandnom-inalmodiﬁers1.TheRussiancasefeatureincludessixpossiblevalues,representingthenotionsofsub-ject,directobject,location,etc.InArabic,likeotherSemiticlanguages,wordsurfaceformsmayincludeprocliticsandenclitics(orpreﬁxesandsufﬁxesaswerefertotheminthispaper),concatenatedtoin-ﬂectedstems.Fornouns,preﬁxesincludeconjunc-tions(wa:“and”,fa:“and,so”),prepositions(bi:“by,with”,ka:“like,suchas”,li:“for,to”)andade-terminer,andsufﬁxesincludepossessivepronouns.Verbalpreﬁxesincludeconjunctionandnegation,andsufﬁxesincludeobjectpronouns.Bothobjectandpossessivepronounsarecapturedbyanindica-torfunctionforitspresenceorabsence,aswellasbythefeaturesthatindicatetheirperson,numberandgender.Ascanbeobservedfromthetable,alargenumberofsurfaceinﬂectedformscanbegen-eratedbythecombinationofthesefeatures,making1CasemarkingalsoexistsinArabic.However,inmanyin-stances,itisrealizedbydiacriticswhichareignoredinstandardorthography.Inourexperiments,weincludecasemarkinginArabiconlywhenitisreﬂectedintheorthography.themorphologicalgenerationoftheselanguagesanon-trivialtask.Morphologicallycomplexlanguagesalsotendtodisplayarichsystemofagreements.InRussian,forexample,adjectivesagreewithheadnounsinnum-ber,genderandcase,andverbsagreewiththesub-jectnouninpersonandnumber(pasttenseverbsagreeingenderandnumber).Arabichasasimilarlyrichsystemofagreement,withuniquecharacteris-tics.Forexample,inadditiontoagreementinvolv-ingperson,numberandgender,italsorequiresade-terminerforeachwordinadeﬁnitenounphrasewithadjectivalmodiﬁers;inanouncompound,adeter-minerisattachedtothelastnouninthechain.Also,non-humansubjectpluralnounsrequiretheverbtobeinﬂectedinasingularfeminineform.Generatingthesemorphologicallycomplexlanguagesisthere-foremoredifﬁcultthangeneratingEnglishintermsofcapturingtheagreementphenomena.3RelatedWorkTheuseofmorphologicalfeaturesinlanguagemod-ellinghasbeenexploredinthepastformorphology-richlanguages.Forexample,(DuhandKirchhoff,2004)showedthatfactoredlanguagemodels,whichconsidermorphologicalfeaturesanduseanopti-mizedbackoffpolicy,yieldlowerperplexity.IntheareaofMT,therehasbeenalargebodyofworkattemptingtomodifytheinputtoatransla-tionsysteminordertoimprovethegeneratedalign-mentsforparticularlanguagepairs.Forexample,ithasbeenshown(Lee,2004)thatdeterminerseg-mentationanddeletioninArabicsentencesinanArabic-to-Englishtranslationsystemimprovessen-tencealignment,thusleadingtoimprovedover-alltranslationquality.Anotherwork(KoehnandKnight,2003)showedimprovementsbysplittingcompoundsinGerman.(NießenandNey,2004)demonstratedthatasimilarlevelofalignmentqual-itycanbeachievedwithsmallercorporaapplyingmorpho-syntacticsourcerestructuring,usinghierar-chicallexiconmodels,intranslatingfromGermanintoEnglish.(Popovi´candNey,2004)experimentedsuccessfullywithtranslatingfrominﬂectionallan-guagesintoEnglishmakinguseofPOStags,wordstemsandsufﬁxesinthesourcelanguage.Morere-cently,(GoldwaterandMcClosky,2005)achievedimprovementsinCzech-EnglishMT,optimizinga130

FeaturesRussianArabicBothPOS(11categories)(18categories)Person1,2,3Numberdualsing(ular),pl(ural)Genderneut(er)masc(uline),fem(inine)Tensegerundpresent,past,future,imperativeMoodsubjunctive,jussiveCasedat(ive),prep(ositional),nom(inative),acc(usative),gen(itive)instr(umental)Negationyes,noDetermineryes,noConjunctionwa,fa,nonePrepositionbi,ka,li,noneObjectPronounyes,noPers/Numb/Gendofpronoun,nonePossessivePronounSameasObjectPronounTable1:MorphologicalfeaturesusedforRussianandArabicsetofpossiblesourcetransformations,incorporat-ingmorphology.Ingeneral,thislineofworkfo-cusedontranslatingfrommorphologicallyrichlan-guagesintoEnglish;therehasbeenlimitedresearchinMTintheoppositedirection.Koehn(2005)in-cludesasurveyofstatisticalMTsystemsinbothdi-rectionsfortheEuroparlcorpus,andpointsoutthechallengesofthistask.Arecentwork(El-KahloutandOﬂazer,2006)experimentedwithEnglish-to-Turkishtranslationwithlimitedsuccess,suggestingthatinﬂectiongenerationgivenmorphologicalfea-turesmaygivepositiveresults.Inthecurrentwork,wesuggestaprobabilisticframeworkformorphologygenerationperformedaspost-processing.Itcanthereforebeconsideredascomplementarytothetechniquesdescribedabove.Ourapproachisgeneralinthatitisnotspeciﬁctoaparticularlanguagepair,andisnovelinthatital-lowsmodellingofagreementonthetargetside.Theframeworksuggestedhereismostcloselyrelatedto(SuzukiandToutanova,2006),whichusesaproba-bilisticmodeltogenerateJapanesecasemarkersforEnglish-to-JapaneseMT.Thisworkcanbeviewedasageneralizationof(SuzukiandToutanova,2006)inthatourmodelgeneratesinﬂectedformsofwords,andisnotlimitedtogeneratingasmall,closedsetofcasemarkers.Inaddition,themorphologygenera-tionproblemismorechallenginginthatitrequireshandlingofcomplexagreementphenomenaalongmultiplemorphologicaldimensions.4InﬂectionPredictionFrameworkInthissection,wedeﬁnethetaskofofmorphologi-calgenerationasinﬂectionprediction,aswellasthelexicaloperationsrelevantforthetask.4.1MorphologyAnalysisandGenerationMorphologicalanalysiscanbeperformedbyap-plyinglanguagespeciﬁcrules.Thesemayincludeafull-scalemorphologicalanalysiswithcontextualdisambiguation,or,whensuchresourcesarenotavailable,simpleheuristicrules,suchasregardingthelastfewcharactersofawordasitsmorphogicalsufﬁx.Inthiswork,weassumethatlexiconsLSandLTareavailableforthesourceandtranslationlan-guages,respectively.Suchlexiconscanbecreatedmanually,orautomaticallyfromdata.Givenalexi-conLandasurfacewordw,wedeﬁnethefollowingoperations:•Stemming-letSw={s1,...,sl}bethesetofpossiblemorphologicalstems(lemmas)ofwaccordingtoL.2•Inﬂection-letIw={i1,...,im}bethesetofsurfaceformwordsthathavethesamestemasw.Thatis,i∈IwiffSiTSw6=∅.•Morphologicalanalysis-letAw={a1,...,av}bethesetofpossiblemorphologicalanalysesforw.Amorphologicalanalysisaisavectorofcategoricalvalues,wherethedimensionsandpossiblevaluesforeachdimensioninthevectorrepresentationspacearedeﬁnedbyL.4.2TheTaskWeassumethatwearegivenalignedsentencepairs,whereasentencepairincludesasourceandatar-2Multiplestemsarepossibleduetoambiguityinmorpho-logicalanalysis.131

NN+sg+nom+neuttheDETallocationofresourceshascompletedNN+sgPREPNN+plAUXV+sgVERB+pastpartраспределениеNN+sg+gen+pl+mascресурсовVERB+perf+pass+part+neut+sgзавершеноraspredelenieresursovzavershenoFigure1:AlignedEnglish-Russiansentencepairwithsyntacticandmorphologicalannotationgetsentence,andlexiconsLSandLTthatsupporttheoperationsdescribedinthesectionabove.Letasentencew1,...wt,...wnbetheoutputofaMTsysteminthetargetlanguage.Thissentencecanbeconvertedintothecorrespondingstemsetse-quenceS1,...St,...Sn,applyingthestemmingop-eration.Thenthetaskis,foreverystemsetStintheoutputsentence,topredictaninﬂectionytfromitsinﬂectionsetIt.Thepredictedinﬂectionsshouldbothreﬂectthemeaningconveyedbythesourcesen-tence,andcomplywiththeagreementrulesofthetargetlanguage.3Figure1showsanexampleofanalignedEnglish-Russiansentencepair:onthesource(English)side,POStagsandworddependencystructureareindi-catedbysolidarcs.ThealignmentsbetweenEn-glishandRussianwordsareindicatedbythedot-tedlines.ThedependencystructureontheRussianside,indicatedbysolidarcs,isgivenbyatreeletMTsysteminourcase(seeSection6.1),projectedfromtheworddependencystructureofEnglishandwordalignmentinformation.NotethattheRussiansen-tencedisplaysagreementinnumberandgenderbe-tweenthesubjectnoun(raspredelenie)andthepred-icate(zaversheno);notealsothatresursovisingen-itivecase,asitmodiﬁesthenounonitsleft.5ModelsforInﬂectionPrediction5.1AProbabilisticModelOurlearningframeworkusesaMaximumEntropyMarkovmodel(McCallumetal.,2000).Themodeldecomposestheoverallprobabilityofapredictedinﬂectionsequenceintoaproductoflocalproba-bilitiesforindividualwordpredictions.Thelocal3Thatis,assumingthatthestemsequencethatisoutputbytheMTsystemiscorrect.probabilitiesareconditionedonthepreviouskpre-dictions.Themodelimplementedhereisofsecondorder:atanydecisionpointtweconditiontheprob-abilitydistributionoverlabelsontheprevioustwopredictionsyt−1andyt−2inadditiontothegiven(static)wordcontextfromboththesourceandtar-getsentences.Thatis,theprobabilityofapredictedinﬂectionsequenceisdeﬁnedasfollows:p(y|x)=nYt=1p(yt|yt−1,yt−2,xt),yt∈ItwherextdenotesthegivencontextatpositiontandItisthesetofinﬂectionscorrespondingtoSt,fromwhichthemodelshouldchooseyt.Thefeaturesweconstructedpairuppredicatesonthecontext(¯x,yt−1,yt−2)andthetargetlabel(yt).Inthesuggestedframework,itisstraightforwardtoencodethemorphologicalpropertiesofaword,inadditiontoitssurfaceinﬂectedform.Forexample,foraparticularinﬂectedwordformytanditscon-text,thederivedpairedfeaturesmayinclude:φk=(cid:26)1ifsurfacewordytisy′ands′∈St+10otherwiseφk+1=n1ifGender(yt)=“Fem”andGender(yt−1)=“Fem”0otherwiseIntheﬁrstexample,agivenneighboringstemsetSt+1isusedasacontextfeatureforpredictingthetargetwordyt.Thesecondfeaturecapturesthegen-deragreementwiththepreviousword.Thisispossi-blebecauseourmodelisofsecondorder.Thus,wecanderivecontextfeaturesdescribingthemorpho-logicalpropertiesofthetwopreviouspredictions.4Notethatourmodelisnotasimplemulti-classclas-siﬁer,becauseourfeaturesaresharedacrossmul-tipletargetlabels.Forexample,thegenderfea-tureaboveappliestomanydifferentinﬂectedforms.Therefore,itisastructuredpredictionmodel,wherethestructureisdeﬁnedbythemorphologicalproper-tiesofthetargetpredictions,inadditiontothewordsequencedecomposition.5.2FeatureCategoriesTheinformationavailableforestimatingthedistri-butionoverytcanbesplitintoseveralcategories,4Notethatwhilewedecomposethepredictiontaskleft-to-right,anappealingalternativeistodeﬁneatop-downdecompo-sition,traversingthedependencytreeofthesentence.However,thisrequiressyntacticanalysisofsufﬁcientquality.132

correspondingtofeaturesource.Theﬁrstma-jordistinctionismonolingualversusbilingualfea-tures:monolingualfeaturesreferonlytothecontext(andpredictedlabel)inthetargetlanguage,whilebilingualfeatureshaveaccesstoinformationinthesourcesentences,obtainedbytraversingthewordalignmentlinksfromtargetwordstoa(setof)sourcewords,asshowninFigure1.Bothmonolingualandbilingualfeaturescanbefurthersplitintothreeclasses:lexical,morpholog-icalandsyntactic.Lexicalfeaturesrefertosurfacewordforms,aswellastheirstems.Sinceourmodelisofsecondorder,ourmonolinguallexicalfea-turesincludethefeaturesofastandardwordtrigramlanguagemodel.Furthermore,sinceourmodelisdiscriminative(predictingwordformsgiventheirstems),themonolinguallexicalmodelcanusestemsinadditiontopredictedwordsfortheleftandcur-rentposition,aswellasstemsfromtherightcon-text.MorphologicalfeaturesarethosethatrefertothefeaturesgiveninTable1.Morphologicalinfor-mationisusedindescribingthetargetlabelaswellasitscontext,andisintendedtocapturemorpho-logicalgeneralizations.Finally,syntacticfeaturescanmakeuseofsyntacticanalysesofthesourceandtargetsentences.Suchanalysesmaybederivedforthetargetlanguage,usingthepre-stemmedsen-tence.Withoutlossofgenerality,wewillusehereadependencyparsingparadigm.Givenasyntacticanalysis,onecanconstructsyntacticfeatures;forex-ample,thestemoftheparentwordofyt.Syntacticfeaturesareexpectedtobeusefulincapturingagree-mentphenomena.5.3FeaturesTable2givesthefullsetofsuggestedfeaturesforRussianandArabic,detailedbytype.Formonolin-guallexicalfeatures,weconsiderthestemsofthepredictedwordanditsimmediatelyadjacentwords,inadditiontotraditionalwordbigramandtrigramfeatures.Formonolingualmorphologicalfeatures,weconsiderthemorphologicalattributesofthetwopreviouslypredictedwordsandthecurrentpredic-tion;formonolingualsyntacticfeatures,weusethestemoftheparentnode.Thebilingualfeaturesincludethesetofwordsalignedtothefocuswordatpositiont,wheretheyaretreatedasbag-of-words,i.e.,eachalignedwordFeaturecategoriesInstantiationsMonolinguallexicalWordstemst−1,st−2,st,st+1Predictedwordyt,yt−1,yt−2Monolingualmorphologicalf:POS,Person,Number,Gender,Tensef(yt−2),f(yt−1),f(yt)Neg,Det,Prep,Conj,ObjPron,PossPronMonolingualsyntacticParentstemsHEAD(t)BilinguallexicalAlignedwordsetAlAlt,Alt−1,Alt+1Bilingualmorph&syntacticf:POS,Person,Number,Gender,Tensef(Alt),f(Alt−1),Neg,Det,Prep,Conj,ObjPron,PossPron,f(Alt+1),f(AlHEAD(t))CompTable2:ThefeaturesetsuggestedforEnglish-RussianandEnglish-Arabicpairsisassignedaseparatefeature.Bilinguallexicalfea-turescanrefertowordsalignedtoytasallaswordsalignedtoitsimmediateneighborsyt−1andyt+1.Bilingualmorphologicalandsyntacticfeaturesre-fertothefeaturesofthesourcelanguage,whichareexpectedtobeusefulforpredictingmorphol-ogyinthetargetlanguage.Forexample,thebilin-gualDet(determiner)featureiscomputedaccord-ingtothesourcedependencytree:ifachildofawordalignedtowtisadeterminer,thenthefea-turevalueisassigneditssurfacewordform(suchasaorthe).ThebilingualPrepfeatureiscom-putedsimilarly,bycheckingtheparentchainofthewordalignedtowtfortheexistenceofapreposi-tion.Thisfeatureishopedtobeusefulforpredict-ingArabicinﬂectedformswithaprepositionalpre-ﬁx,aswellasforpredictingcasemarkinginRus-sian.ThebilingualObjPronandPossPronfeaturesrepresentanyobjectpronounofthewordalignedtowtandaprecedingpossessivepronoun,respectively.ThesefeaturesareexpectedtomaptotheobjectandpossessivepronounfeaturesinArabic.Finally,thebilingualCompoundfeaturecheckswhetherawordappearsaspartofanouncompoundintheEnglishsource.fthisisthecase,thefeatureisassignedthevalueof“head”or“dependent”.Thisfeatureisrel-evantforpredictingagenitivecaseinRussiananddeﬁnitenessinArabic.6ExperimentalSettingsInordertoevaluatetheeffectivenessofthesug-gestedapproach,weperformedreferenceexperi-ments,thatis,usingthealignedsentencepairsof133

DataEng-RusEng-AraAvg.sentlenEngRusEngAraTraining1M470K14.0612.9012.8511.90Development1,0001,00013.7312.9113.4812.90Test1,0001,00013.6112.848.497.50Table3:Datasetstatistics:corpussizeandaveragesentencelength(inwords)referencetranslationsratherthantheoutputofanMTsystemasinput.5Thisallowsustoevaluateourmethodwithareducednoiselevel,asthewordsandwordorderareperfectinreferencetranslations.TheseexperimentsthusconstituteapreliminarystepfortacklingtherealtaskofinﬂectingwordsinMT.6.1DataWeusedacorpusofapproximately1millionalignedsentencepairsforEnglish-Russian,and0.5millionpairsforEnglish-Arabic.Bothcorporaarefromatechnical(softwaremanual)domain,whichwebe-lieveissomewhatrestrictedalongsomemorpho-logicaldimensions,suchastenseandperson.Weused1,000sentencepairseachfordevelopmentandtestingforbothlanguagepairs.ThedetailsofthedatasetsusedaregiveninTable3.Thesentencepairswereword-alignedusingGIZA++(OchandNey,2000)andsubmittedtoatreelet-basedMTsystem(Quirketal.,2005),whichusestheworddependencystructureofthesourcelanguageandprojectsworddependencystructuretothetargetlanguage,creatingthestructureshowninFigure1above.6.2LexiconTable4givessomerelevantstatisticsofthelexiconsweused.ForRussian,ageneral-domainlexiconwasavailabletous,consistingofabout80,000lemmas(stems)and9.4inﬂectedformsperstem.6Limitingthelexicontowordtypesthatareseeninthetrain-ingsetreducesitssizesubstantiallytoabout14,000stems,andanaverageof3.8inﬂectionsperstem.Wewillusethislatter“domain-adapted”lexiconinourexperiments.5Inthiscase,ytshouldequalwt,accordingtothetaskdeﬁ-nition.6TheaveragesreportedinTable4arebytypeanddonotconsiderwordfrequenciesinthedata.SourceStemsAvg(|I|)Avg(|S|)Rus.Lexicon79,3099.4Lexicon∩Train13,9293.81.6Ara.Lexicon∩Train12,6707.01.7Table4:LexiconstatisticsForArabic,asafull-sizeArabiclexiconwasnotavailabletous,weusedtheBuckwaltermorpholog-icalanalyzer(Buckwalter,2004)toderivealexicon.Toacquirethestemmingandinﬂectionoperators,wesubmitallwordsinourtrainingdatatotheBuckwal-teranalyzer.NotethatArabicdisplaysahighlevelofambiguity,eachwordcorrespondingtomanypos-siblesegmentationsandmorphologicalanalyses;weconsideredallofthedifferentstemsreturnedbytheBuckwalteranalyzerincreatingaword’sstemset.Thelexiconcreatedinthismannercontains12,670distinctstemsand89,360inﬂectedforms.Forthegenerationofwordfeatures,weonlycon-sideronedominantanalysisforanysurfacewordforsimplicity.Incaseofambiguity,weconsideredonlytheﬁrst(arbitrary)analysisforRussian.ForArabic,weapplythefollowingheuristic:usethemostfrequentanalysisestimatedfromthegoldstan-dardlabelsintheArabicTreebank(Maamourietal.,2005);ifaworddoesnotappearinthetreebank,wechoosetheﬁrstanalysisreturnedbytheBuckwal-teranalyzer.Ideally,thebestwordanalysisshouldbeprovidedasaresultofcontextualdisambiguation(e.g.,(HabashandRambow,2005));weleavethisforfuturework.6.3BaselineAsabaseline,wepickamorphologicalinﬂectionytatrandomfromIt.Thisrandombaselineservesasanindicatorforthedifﬁcultyoftheproblem.An-othermorecompetitivebaselineweimplementedisawordtrigramlanguagemodel(LM).TheLMsweretrainedusingtheCMUlanguagemodellingtoolkit(ClarksonandRosenfeld,1997)withdefaultsettingsonthetrainingdatadescribedinTable3.6.4ExperimentsIntheexperiments,ourprimarygoalistoevaluatetheeffectivenessoftheproposedmodelusingallfeaturesavailabletous.Additionally,weareinter-estedinknowingthecontributionofeachinforma-tionsource,namelyofmorpho-syntacticandbilin-gualfeatures.Therefore,westudytheperformance134

ofmodelsincludingthefullfeatureschemataaswellasmodelsthatarerestrictedtofeaturesubsetsac-cordingtothefeaturetypesasdescribedinSection5.2.Themodelsareasfollows:Monolingual-Word,includingLM-likeandstemn-gramfeaturesonly;Bilingual-Word,whichalsoincludesbilinguallex-icalfeatures;7Monolingual-All,whichhasaccesstoalltheinformationavailableinthetargetlan-guage,includingmorphologicalandsyntacticfea-tures;andﬁnally,Bilingual-All,whichincludesallfeaturetypesfromTable2.Foreachmodelandlanguage,weperformfeatureselectioninthefollowingmanner.Thefeaturesarerepresentedasfeaturetemplates,suchas”POS=X”,whichgenerateasetofbinaryfeaturescorrespond-ingtodifferentinstantiationsofthetemplate,asin”POS=NOUN”.Inadditiontoindividualfeatures,con-junctionsofuptothreefeaturesarealsoconsideredforselection(e.g.,”POS=NOUN&Number=plural”).Everyconjunctionoffeaturetemplatesconsideredcontainsatleastonepredicateonthepredictionyt,anduptotwopredicatesonthecontext.Thefeatureselectionalgorithmperformsagreedyforwardstep-wisefeatureselectiononthefeaturetemplatessoastomaximizedevelopmentsetaccuracy.Thealgo-rithmissimilartotheonedescribedin(Toutanova,2006).Afterthisprocess,weperformedsomeman-ualinspectionoftheselectedtemplates,andﬁnallyobtained11and36templatesfortheMonolingual-AllandBilingual-AllsettingsforRussian,respec-tively.Thesetemplatesgenerated7.9millionand9.3millionbinaryfeatureinstantiationsintheﬁ-nalmodel,respectively.Thecorrespondingnum-bersforArabicwere27featuretemplates(0.7mil-lionbinaryinstantiations)and39featuretemplates(2.3millionbinaryinstantiations)forMonolingual-AllandBilingual-All,respectively.7ResultsandDiscussionTable5showstheaccuracyofpredictingwordformsforthebaselineandproposedmodels.Wereportac-curacyonlyonwordsthatappearinourlexicons.Thus,punctuation,Englishwordsoccurringinthetargetsentence,andwordswithunknownlemmasareexcludedfromtheevaluation.Thereportedac-curacymeasurethereforeabstractsawayfromtheis-7Overall,thisfeaturesetapproximatestheinformationthatisavailabletoastate-of-the-artstatisticalMTsystem.ModelEng-RusEng-AraRandom31.716.3LM77.631.7MonolingualWord85.169.6BilingualWord87.171.9MonolingualAll87.171.6BilingualAll91.573.3Table5:Accuracy(%)resultsbymodelsueofincompletecoverageofthelexicon.WhenweencounterthesewordsinthetrueMTscenario,wewillmakenopredictionsaboutthem,andsimplyleavethemunmodiﬁed.Inourcurrentexperiments,inRussian,68.2%ofallwordtokenswereinCyril-lic,ofwhich93.8%wereincludedinourlexicon.InArabic,85.5%ofallwordtokenswereinArabiccharacters,ofwhich99.1%wereinourlexicon.8TheresultsinTable5showthatthesuggestedmodelsoutperformthelanguagemodelsubstantiallyforbothlanguages.Inparticular,thecontributionofbothbilingualandnon-lexicalfeaturesisnotewor-thy:addingnon-lexicalfeaturesconsistentlyleadsto1.5%to2%absolutegaininbothmonolingualandbilingualsettingsinbothlanguagepairs.WeobtainaparticularlylargegainintheRussianbilin-gualcase,inwhichtheabsolutegainismorethan4%,translatingto34%errorratereduction.Addingbilingualfeatureshasasimilareffectofgainingabout2%(and4%forRussiannon-lexical)inac-curacyovermonolingualmodels.Theoverallaccu-racyislowerinArabicthaninRussian,reﬂectingtheinherentdifﬁcultyofthetask,asindicatedbytherandombaseline(31.7inRussianvs.16.3inAra-bic).Inordertoevaluatetheeffectivenessofthemodelinalleviatingthedatasparsityprobleminmorpho-logicalgeneration,wetrainedinﬂectionpredictionmodelsonvarioussubsetsofthetrainingdatade-scribedinTable3,andtestedtheiraccuracy.TheresultsaregiveninFigure2.Wecanseethatwithasfewas5,000trainingsentencespairs,themodelob-tainsmuchbetteraccuracythanthelanguagemodel,whichistrainedondatathatislargerbyafewordersofmagnitude.Wealsonotethatthelearningcurve8ForArabic,theinﬂectionambiguitywasextremelyhigh:therewereonaverage39inﬂectedformsperstemsetinourdevelopmentcorpus(pertoken),asopposedto7inRussian.WethereforelimitedtheevaluationofArabictothosestemsthathaveupto30inﬂectedforms,resultingin17inﬂectedformsperstemsetonaverageinthedevelopmentdata.135

5055606570758085905101520253035404550556065707580859095100Training data size (x1,000)Accuracy (%)RUS-bi-wordRUS-bi-allARA-bi-wordARA-bi-allFigure2:Accuracy,varyingtrainingdatasizebecomeslesssteepasweusemoretrainingdata,suggestingthatthemodelsaresuccessfullylearninggeneralizations.Wehavealsomanuallyexaminedsomerepre-sentativecaseswheretheproposedmodelfailedtomakeacorrectprediction.InbothRussianandAra-bic,averycommonpatternwasamistakeinpre-dictingthegender(aswellasnumberandpersoninArabic)ofpronouns.Thismaybeattributedtothefactthatthecorrectchoiceofthepronounrequirescoreferenceresolution,whichisnotavailableinourmodel.Amorethoroughanalysisoftheresultswillbehelpfultobringfurtherimprovements.8ConclusionsandFutureWorkWepresentedaprobabilisticframeworkformor-phologicalgenerationgivenalignedsentencepairs,incorporatingmorpho-syntacticinformationfromboththesourceandtargetsentences.There-sults,usingreferencetranslations,showthatthepro-posedmodelsachievesubstantiallybetteraccuracythanlanguagemodels,evenwitharelativelysmallamountoftrainingdata.Ourmodelsusingmorpho-syntacticinformationalsooutperformedmodelsus-ingonlylexicalinformationbyawidemargin.ThisresultisverypromisingforachievingourultimategoalofimprovingMToutputbyusingaspecial-izedmodelfortargetlanguagemorphologicalgener-ation.Thoughthisgoalisclearlyoutsidethescopeofthispaper,weconductedapreliminaryexperi-mentwhereanEnglish-to-RussianMTsystemwastrainedonastemmedversionofthealigneddataandusedtogeneratestemmedwordsequences,whichweretheninﬂectedusingthesuggestedframework.ThissimpleintegrationoftheproposedmodelwiththeMTsystemimprovedtheBLEUscoreby1.7.Themostobviousnextstepofourresearch,there-fore,istofurtherpursuetheintegrationofthepro-posedmodeltotheend-to-endMTscenario.Therearemultiplepathsforobtainingfurtherim-provementsovertheresultspresentedhere.Theseincludereﬁnementinfeaturedesign,wordanalysisdisambiguation,morphologicalandsyntacticanal-ysisonthesourceEnglishside(e.g.,assigningse-manticroletags),tonameafew.Anotherareaofinvestigationiscapturinglonger-distanceagreementphenomena,whichcanbedonebyimplementingaglobalstatisticalmodel,orbyusingfeaturesfromdependencytreesmoreeffectively.ReferencesTimBuckwalter.2004.Buckwalterarabicmorphologicalana-lyzerversion2.0.PhilipClarksonandRoniRosenfeld.1997.StatisticallanguagemodellingusingtheCMUcambridgetoolkit.InEurospeech.KevinDuhandKathrinKirchhoff.2004.Automaticlearningoflanguagemodelstructure.InCOLING.IlknurDurgarEl-KahloutandKemalOﬂazer.2006.Initialex-plorationsinEnglishtoTurkishstatisticalmachinetransla-tion.InNAACLworkshoponstatisticalmachinetranslation.SharonGoldwaterandDavidMcClosky.2005.Improvingsta-tisticalMTthroughmorphologicalanalysis.InEMNLP.NizarHabashandOwenRambow.2005.Arabictokenization,part-of-speechtaggingandmorphologicaldisambiguationinonefellswoop.InACL.PhilippKoehnandKevinKnight.2003.Empiricalmethodsforcompoundsplitting.InEACL.PhilippKoehn.2005.Europarl:Aparallelcorpusforstatisticalmachinetranslation.InMTSummit.Young-SukLee.2004.Morphologicalanalysisforstatisticalmachinetranslation.InHLT-NAACL.MohamedMaamouri,AnnBies,TimBuckwalter,andHubertJin.2005.ArabicTreebank:Part1v3.0.LinguisticDataConsortium.AndrewMcCallum,DayneFreitag,andFernandoC.N.Pereira.2000.Maximumentropymarkovmodelsforinformationextractionandsegmentation.InICML.SonjaNießenandHermannNey.2004.Statisticalmachinetranslationwithscarceresourcesusingmorpho-syntacticin-formation.ComputationalLinguistics,30(2):181–204.FranzJosefOchandHermannNey.2000.Improvedstatisticalalignmentmodels.InACL.MajaPopovi´candHermannNey.2004.Towardstheuseofwordstemsandsufﬁxesforstatisticalmachinetranslation.InLREC.ChrisQuirk,ArulMenezes,andColinCherry.2005.Depen-dencytreetranslation:SyntacticallyinformedphrasalSMT.InACL.HisamiSuzukiandKristinaToutanova.2006.Learningtopre-dictcasemarkersinJapanese.InCOLING-ACL.KristinaToutanova.2006.CompetitivegenerativemodelswithstructurelearningforNLPclassiﬁcationtasks.InEMNLP.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 136–143,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

136

Assisting Translators in Indirect Lexical Transfer Bogdan Babych, Anthony Hartley, Serge Sharoff  Centre for Translation Studies   University of Leeds, UK {b.babych,a.hartley,s.sharoff}@leeds.ac.ukOlga Mudraya   Department of Linguistics   Lancaster University, UK  o.mudraya@lancs.ac.uk Abstract We present the design and evaluation of a translator’s amenuensis that uses compa-rable corpora to propose and rank non-literal solutions to the translation of expres-sions from the general lexicon. Using dis-tributional similarity and bilingual diction-aries, the method outperforms established techniques for extracting translation equivalents from parallel corpora. The in-terface to the system is available at: http://corpus.leeds.ac.uk/assist/v05/  1 Introduction This paper describes a system designed to assist humans in translating expressions that do not nec-essarily have a literal or compositional equivalent in the target language (TL). In the spirit of (Kay, 1997), it is intended as a translator's amenuensis "under the tight control of a human translator … to help increase his productivity and not to supplant him". One area where human translators particularly appreciate assistance is in the translation of expres-sions from the general lexicon. Unlike equivalent technical terms, which generally share the same part-of-speech (POS) across languages and are in the ideal case univocal, the contextually appropri-ate equivalents of general language expressions are often indirect and open to variation. While the transfer module in RBMT may acceptably under-generate through a many-to-one mapping between source and target expressions, human translators, even in non-literary fields, value legitimate varia-tion. Thus the French expression il faillit échouer (lit.: he faltered to fail) may be variously rendered as he almost/nearly/all but failed; he was on the verge/brink of failing/failure; failure loomed. All of these translations are indirect in that they in-volve lexical shifts or POS transformations. Finding such translations is a hard task that can benefit from automated assistance. 'Mining' such indirect equivalents is difficult, precisely because of the structural mismatch, but also because of the paucity of suitable aligned corpora. The approach adopted here includes the use of comparable cor-pora in source and target languages, which are relatively easy to create. The challenge is to gener-ate a list of usable solutions and to rank them such that the best are at the top. Thus the present system is unlike SMT (Och and Ney, 2003), where lexical selection is effected by a translation model based on aligned, parallel cor-pora, but the novel techniques it has developed are exploitable in the SMT paradigm. It also differs from now traditional uses of comparable corpora for detecting translation equivalents (Rapp, 1999) or extracting terminology (Grefenstette, 2002), which allows a one-to-one correspondence irre-spective of the context. Our system addresses diffi-culties in expressions in the general lexicon, whose translation is context-dependent. The structure of the paper is as follows. In Sec-tion 2 we present the method we use for mining translation equivalents. In Section 3 we present the results of an objective evaluation of the quality of suggestions produced by the system by comparing our output against a parallel corpus. Finally, in Section 4 we present a subjective evaluation focus-ing on the integration of the system into the work-flow of human translators. 2 Methodology The software acts as a decision support system for translators. It integrates different technologies for 137

extracting indirect translation equivalents from large comparable corpora. In the following subsec-tions we give the user perspective on the system and describe the methodology underlying each of its sub-tasks. 2.1 User perspective Unlike traditional dictionaries, the system is a dynamic translation resource in that it can success-fully find translation equivalents for units which have not been stored in advance, even for idiosyn-cratic multiword expressions which almost cer-tainly will not figure in a dictionary. While our system can rectify gaps and omissions in static lexicographical resources, its major advantage is that it is able to cope with an open set of transla-tion problems, searching for translation equivalents in comparable corpora in runtime. This makes it more than just an extended dictionary. Contextual descriptors From the user perspective the system extracts indi-rect translation equivalents as sets of contextual descriptors – content words that are lexically cen-tral in a given sentence, phrase or construction. The choice of these descriptors may determine the general syntactic perspective of the sentence and the use of supporting lexical items. Many transla-tion problems arise from the fact that the mapping between such descriptors is not straightforward. The system is designed to find possible indirect mappings between sets of descriptors and to verify the acceptability of the mapping into the TL. For example, in the following Russian sentence, the bolded contextual descriptors require indirect translation into English. Дети посещают плохо отремонтиро-ванные школы, в которых недостает самого необходимого (Children attend badly repaired schools, in which [it] is missing the most necessary) Combining direct translation equivalents of these words (e.g., translations found in the Oxford Russian Dictionary – ORD) may produce a non-natural English sentence, like the literal translation given above. In such cases human translators usu-ally apply structural and lexical transformations, for instance changing the descriptors’ POS and/or replacing them with near-synonyms which fit to-gether in the context of a TL sentence (Munday, 2001: 57-58). Thus, a structural transformation of плохо отремонтированные (badly repaired) may give in poor repair while a lexical transformation of недостает самого необходимого ([it] is missing the most necessary) gives lacking basic essentials. Our system models such transformations of the descriptors and checks the consistency of the re-sulting sets in the TL. Using the system Human translators submit queries in the form of one or more SL descriptors which in their opinion may require indirect translation. When the transla-tors use the system for translating into their native language, the returned descriptors are usually suf-ficient for them to produce a correct TL construc-tion or phrase around them (even though the de-scriptors do not always form a naturally sounding expression). When the translators work into a non-native language, they often find it useful to gener-ate concordances for the returned descriptors to verify their usage within TL constructions. For example, for the sentence above translators may submit two queries: плохо отремонт-ированные (badly repaired) and недостает необходимого (missing necessary). For the first query the system returns a list of descriptor pairs (with information on their frequency in the English corpus) ranked by distributional proximity to the original query, which we explain in Section 2.2. At the top of the list come: bad repair = 30  (11.005) bad maintenance = 16  (5.301) bad restoration = 2  (5.079) poor repair = 60  (5.026)… Underlined hyperlinks lead translators to actual contexts in the English corpus, e.g., poor repair generates a concordance containing a desirable TL construction which is a structural transformation of the SL query: in such apoor state of repair bridge in aspoor a state of repair as the highways building inpoor repair. dwellings are inpoor repair; Similarly, the result of the second query may give the translators an idea about possible lexical transformation: missing need = 14  (5.035) important missing = 8 (2.930) missing vital = 8  (2.322) lack necessary = 204  (1.982)… essential lack = 86  (0.908)… 138

The concordance for the last pair of descriptors contains the phrase they lack the three essentials, which illustrates the transformation. The resulting translation may be the following: Children attend schools that are in poor re-pair and lacking basic essentials Thus our system supports translators in making decisions about indirect translation equivalents in a number of ways: it suggests possible structural and lexical transformations for contextual descriptors; it verifies which translation variants co-occur in the TL corpus; and it illustrates the use of the transformed TL lexical descriptors in actual con-texts. 2.2 Generating translation equivalents We have generalised the method used in our previ-ous study (Sharoff et al., 2006) for extracting equivalents for continuous multiword expressions (MWEs). Essentially, the method expands the search space for each word and its dictionary trans-lations with entries from automatically computed thesauri, and then checks which combinations are possible in target corpora. These potential transla-tion equivalents are then ranked by their similarity to the original query and presented to the user. The range of retrievable equivalents is now extended from a relatively limited range of two-word con-structions which mirror POS categories in SL and TL to a much wider set of co-occurring lexical content items, which may appear in a different or-der, at some distance from each other, and belong to different POS categories.  The method works best for expressions from the general lexicon, which do not have established equivalents, but not yet for terminology. It relies on a high-quality bilingual dictionary (en-ru ~30k, ru-en ~50K words, combining ORD and the core part of Multitran) and large comparable corpora (~200M En, ~70M Ru) of news texts. For each of the SL query terms q the system generates its dictionary translation Tr(q) and its similarity class S(q) – a set of words with a similar distribution in a monolingual corpus. Similarity is measured as the cosine between collocation vec-tors, whose dimensionality is reduced by SVD us-ing the implementation by Rapp (2004). The de-scriptor and each word in the similarity class are then translated into the TL using ORD or the Mul-titran dictionary, resulting in {Tr(q)∪ Tr(S(q))}. On the TL side we also generate similarity classes, but only for dictionary translations of query terms Tr(q) (not for Tr(S(q)), which can make output too noisy). We refer to the resulting set of TL words as a translation class T.  T = {Tr(q) ∪ Tr(S(q)) ∪ S(Tr(q))} Translation classes approximate lexical and structural transformations which can potentially be applied to each of the query terms. Automatically computed similarity classes do not require re-sources like WordNet, and they are much more suitable for modelling translation transformations, since they often contain a wider range of words of different POS which share the same context, e.g., the similarity class of the word lack contains words such as absence, insufficient, inadequate, lost, shortage, failure, paucity, poor, weakness, inabil-ity, need. This clearly goes beyond the range of traditional thesauri. For multiword queries, the system performs a consistency check on possible combinations of words from different translation classes. In particu-lar, it computes the Cartesian product for pairs of translation classes T1 and T2 to generate the set P of word pairs, where each word (w1 and w2) comes from a different translation class: P = T1 × T2 = {(w1, w2) | w1 ∈ T1 and w2 ∈ T2}  Then the system checks whether each word pair from the set P exists in the database D of discon-tinuous content word bi-grams which actually co-occur in the TL corpus: P’ = P ∩ D The database contains the set of all bi-grams that occur in the corpus with a frequency ≥ 4 within a window of 5 words (over 9M bigrams for each language). The bi-grams in D and in P are sorted alphabetically, so their order in the query is not important. Larger N-grams (N > 2) in queries are split into combinations of bi-grams, which we found to be an optimal solution to the problem of the scarcity of higher order N-grams in the corpus. Thus, for the query gain significant importance the system generates P’1(significant importance), P’2(gain impor-tance), P’3(gain significant) and computes P’ as:  P’ = {(w1,w2,w3)| (w1,w2) ∈ P’1 & (w1, w3) ∈ P’2 & (w2,w3) ∈ P’3 }, which allows the system to find an indirect equiva-lent получить весомое значение (lit.: receive weighty meaning). 139

Even though P’ on average contains about 2% - 4% of the theoretically possible number of bi-grams present in P, the returned number of poten-tial translation equivalents may still be large and contain much noise. Typically there are several hundred elements in P’, of which only a few are really useful for translation. To make the system usable in practice, i.e., to get useful solutions to appear close to the top (preferably on the first screen of the output), we developed methods of ranking and filtering the returned TL contextual descriptor pairs, which we present in the following sections. 2.3 Hypothesis ranking The system ranks the returned list of contextual descriptors by their distributional proximity to the original query, i.e. it uses scores cos(vq, vw) gener-ated for words in similarity classes – the cosine of the angle between the collocation vector for a word and the collocation vector for the query or diction-ary translation of the query. Thus, words whose equivalents show similar usage in a comparable corpus receive the highest scores. These scores are computed for each individual word in the output, so there are several ways to combine them to weight words in translation classes and word com-binations in the returned list of descriptors.  We established experimentally that the best way to combine similarity scores is to multiply weights W(T) computed for each word within its translation class T. The weight W(P’(w1,w2)) for each pair of contextual descriptors (w1, w2)∈P’ is computed as: W(P’(w1,w2)) = W(T(w1)) × W(T(w2)); Computing W(T(w)), however, is not straightfor-ward either, since some words in similarity classes of different translation equivalents for the query term may be the same, or different words from the similarity class of the original query may have the same translation. Therefore, a word w within a translation class may have come by several routes simultaneously, and may have done that several times. For each word w in T there is a possibility that it arrived in T either because it is in Tr(q) or occurs   n times in Tr(S(q)) or k times in S(Tr(q)). We found that the number of occurrences n and k of each word w in each subset gives valuable in-formation for ranking translation candidates. In our experiments we computed the weight W(T) as the sum of similarity scores which w receives in each of the subsets. We also discovered that ranking improves if for each query term we compute in addition a larger (and potentially noisy) space of candidates that includes TL similarity classes of translations of the SL similarity class S(Tr(S(q))). These candidates do not appear in the system out-put, but they play an important role in ranking the displayed candidates. The improvement may be due to the fact that this space is much larger, and may better support relevant candidates since there is a greater chance that appropriate indirect equiva-lents are found several times within SL and TL similarity classes. The best ranking results were achieved when the original W(T) scores were mul-tiplied by 2 and added to the scores for the newly introduced similarity space S(Tr(S(q))): W(T(w))= 2×(1 if w∈Tr(q) )+  2×∑( cos(vq, vTr(w)) | {w | w∈ Tr(S(q)) } ) +  2×∑( cos(vTr(q), vw) | {w | w∈ S(Tr(q)) } ) + ∑(cos(vq, vTr(w))×cos (vTr(q), vw) |  {w | w∈ S(Tr(S(q))) } ) For example, the system gives the following ranking for the indirect translation equivalents of the Russian phrase весомое значение (lit.: weighty meaning) – figures in brackets represent W(P’) scores for each pair of TL descriptors: 1. significant importance = 7 (3.610)  2. significant value = 128    (3.211)  3. measurable value = 6       (2.657)…  8. dramatic importance = 2    (2.028)  9. important significant = 70 (2.014)  10. convincing importance = 6 (1.843) The Russian similarity class for весомый (weighty, ponderous) contains: убедительный (convincing) (0.469), значимый (significant) (0.461), ощутимый (notable) (0.452) драма-тичный (dramatic) (0.371). The equivalent of significant is not at the top of the similarity class of the Russian query, but it appears at the top of the final ranking of pairs in P’, because this hypothesis is supported by elements of the set formed by S(Tr(S(q))); it appears in similarity classes for no-table (0.353) and dramatic (0.315), which contrib-uted these values to the W(T) score of significant: W(T(significant)) =     2 × (Tr(значимый)=significant (0.461))  + (Tr(ощутимый)=notable (0.452)    × S(notable)=significant (0.353)) + (Tr(драматичный)=dramatic (0.371)    × S(dramatic)= significant (0.315)) The word dramatic itself is not usable as a translation equivalent in this case, but its similarity 140

class contains the support for relevant candidates, so it can be viewed as useful noise. On the other hand, the word convincing does not receive such support from the hypothesis space, even though its Russian equivalent is ranked higher in the SL simi-larity class. 2.4 Semantic filtering Ranking of translation candidates can be further improved when translators use an option to filter the returned list by certain lexical criteria, e.g., to display only those examples that contain a certain lexical item, or to require one of the items to be a dictionary translation of the query term. However, lexical filtering is often too restrictive: in many cases translators need to see a number of related words from the same semantic field or subject do-main, without knowing the lexical items in ad-vance. In this section we present the semantic fil-ter, which is based on Russian and English seman-tic taggers which use the same semantic field tax-onomy for both languages. The semantic filter displays only those items which have specified semantic field tags or tag combinations; it can be applied to one or both words in each translation hypothesis in P’. The default setting for the semantic filter is the re-quirement for both words in the resulting TL can-didates to contain any of the semantic field tags from a SL query term. In the next section we present evaluation results for this default setting (which is applied when the user clicks the Semantic Filter button), but human translators have further options – to filter by tags of individual words, to use semantic classes from SL or TL terms, etc. For example, applying the default semantic filter for the output of the query плохо отремон-тированные (badly repaired) removes the high-lighted items from the list:  1. bad repair = 30       (11.005)  [2. good repair = 154     (8.884) ]  3. bad rebuild = 6       (5.920)  [4. bad maintenance = 16  (5.301) ]  5. bad restoration = 2   (5.079)   6. poor repair = 60      (5.026)  [7. good rebuild = 38     (4.779) ]  8. bad construction = 14 (4.779)  Items 2 and 7 are generated by the system be-cause good, well and bad are in the same similar-ity cluster for many words (they often share the same collocations). The semantic filter removes examples with good and well on the grounds that they do not have any of the tags which come from the word плохо (badly): in particular, instead of tag A5– (Evaluation: Negative) they have tag A5+ (Evaluation: Positive). Item 4 is removed on the grounds that the words отремонтированный (repaired) and maintenance do not have any tags in common – they appear ontologically too far apart from the point of view of the semantic tagger. The core of the system’s multilingual semantic tagging is a knowledge base in which single words and MWEs are mapped to their potential semantic field categories. Often a lexical item is mapped to multiple semantic categories, reflecting its poten-tial multiple senses. In such cases, the tags are ar-ranged by the order of likelihood of meanings, with the most prominent first. 3 Objective evaluation In the objective evaluation we tested the perform-ance of our system on a selection of indirect trans-lation problems, extracted from a parallel corpus consisting mostly of articles from English and Russian newspapers (118,497 words in the R-E direction, 589,055 words in the E-R direction). It has been aligned on the sentence level by JAPA (Langlais et al., 1998), and further on the word level by GIZA++ (Och and Ney, 2003). 3.1 Comparative performance The intuition behind the objective evaluation experiment is that the capacity of our tool to find indirect translation equivalents in comparable cor-pora can be compared with the results of automatic alignment of parallel texts used in translation mod-els in SMT: one of the major advantages of the SMT paradigm is its ability to reuse indirect equivalents found in parallel corpora (equivalents that may never come up in hand-crafted dictionar-ies). Thus, automatically generated GIZA++ dic-tionaries with word alignment contain many exam-ples of indirect translation equivalents. We use these dictionaries to simulate the genera-tor of translation classes T, which we recombine to construct their Cartesian product P, similarly to the procedure we use to generate the output of our sys-tem. However, the two approaches generate indi-rect translation equivalence hypotheses on the ba-sis of radically different material: the GIZA dic-tionary uses evidence from parallel corpora of ex-141

isting human translations, while our system re-combines translation candidates on the basis of their distributional similarity in monolingual com-parable corpora. Therefore we took GIZA as a baseline. Translation problems for the objective evalua-tion experiment were manually extracted from two parallel corpora: a section of about 10,000 words of a corpus of English and Russian newspapers, which we also used to train GIZA, and a section of the same length from a corpus of interviews pub-lished on the Euronews.net website. We selected expressions which represented cases of lexical transformations (as illustrated in Section 0), containing at least two content words both in the SL and TL. These expressions were converted into pairs of contextual descriptors – e.g., recent success, reflect success – and submit-ted to the system and to the GIZA dictionary. We compared the ability of our system and of GIZA to find indirect translation equivalents which matched the equivalents used by human translators. The output from both systems was checked to see whether it contained the contextual descriptors used by human translators. We submitted 388 pairs of descriptors extracted from the newspaper trans-lation corpus and 174 pairs extracted from the Eu-ronews interview corpus. Half of these pairs were Russian, and the other half English. We computed recall figures for 2-word combi-nations of contextual descriptors and single de-scriptors within those combinations. We also show the recall of translation variants provided by the ORD on this data set. For example, for the query недостает необходимого ([it] is missing neces-sary [things]) human translators give the solution lacking essentials; the lemmatised descriptors are lack and essential. ORD returns direct translation equivalents missing and necessary. The GIZA dic-tionary in addition contains several translation equivalents for the second term (with alignment probabilities) including: necessary ~0.332, need ~0.226, essential ~0.023. Our system returns both descriptors used in human translation as a pair – lack essential (ranked 41 without filtering and 22 with the default semantic filter). Thus, for a 2-word combination of the descriptors only the output of our system matched the human solution, which we counted as one hit for the system and no hits for ORD or GIZA. For 1-word descriptors we counted 2 hits for our system (both words in the human solution are matched), and 1 hit for GIZA – it matches the word essential ~0.023 (which also il-lustrates its ability to find indirect translation equivalents).  2w descriptors 1w descriptors  news interv news interv ORD 6.7% 4.6% 32.9%29.3% GIZA++ 13.9%3.4% 35.6% 29.0%Our system 21.9% 19.5% 55.8% 49.4%Table 1 Conservative estimate of recall It can be seen from Table 1 that for the newspa-per corpus on which it was trained, GIZA covers a wider set of indirect translation variants than ORD. But our recall is even better both for 2-word and 1-word descriptors. However, note that GIZA’s ability to retrieve from the newspaper corpus certain indirect transla-tion equivalents may be due to the fact that it has previously seen them frequently enough to gener-ate a correct alignment and the corresponding dic-tionary entry. The Euronews interview corpus was not used for training GIZA. It represents spoken language and is expected to contain more ‘radical’ transforma-tions. The small decline in ORD figures here can be attributed to the fact that there is a difference in genre between written and spoken texts and conse-quently between transformation types in them. However, the performance of GIZA drops radi-cally on unseen text and becomes approximately the same as the ORD. This shows that indirect translation equivalents in the parallel corpus used for training GIZA are too sparse to be learnt one by one and successfully applied to unseen data, since solutions which fit one context do not necessarily suit others. The performance of our system stays at about the same level for this new type of text; the decline in its performance is comparable to the decline in ORD figures, and can again be explained by the differences in genre. 3.2 Evaluation of hypothesis ranking As we mentioned, correct ranking of translation candidates improves the usability of the system. Again, the objective evaluation experiment gives only a conservative estimate of ranking, because there may be many more useful indirect solutions further up the list in the output of the system which are legitimate variants of the solutions found in the 142

parallel corpus. Therefore, evaluation figures should be interpreted in a comparative rather then an absolute sense. We use ranking by frequency as a baseline for comparing the ranking described in Section 2.3 – by distributional similarity between a candidate and the original query. Table 2 shows the average rank of human solu-tions found in parallel corpora and the recall of these solutions for the top 300 examples. Since there are no substantial differences between the figures for the newspaper texts and for the inter-views, we report the results jointly for 556 transla-tion problems in both selections (lower rank fig-ures are better).  Recall Average rank 2-word descriptors frequency (baseline) 16.7% rank=93.7distributional similarity 19.5% rank=44.4sim. + semantic filter 14.4% rank=26.71-word descriptors frequency (baseline) 48.2% rank=42.7distributional similarity 52.8% rank=21.6sim. + semantic filter 44.1% rank=11.3Table 2 Ranking: frequency, similarity and filter It can be seen from the table that ranking by similarity yields almost a twofold improvement for the average rank figures compared to the baseline. There is also a small improvement in recall, since there are more relevant examples that appear within the top 300 entries. The semantic filter once again gives an almost twofold improvement in ranking, since it removes many noisy items. The average is now within the top 30 items, which means that there is a high chance that a translation solution will be displayed on the first screen. The price for improved ranking is decline in recall, since it may remove some rele-vant lexical transformations if they appear to be ontologically too far apart. But the decline is smaller: about 26.2% for 2-word descriptors and 16.5% for 1-word descriptors. The semantic filter is an optional tool, which can be used to great ef-fect on noisy output: its improvement of ranking outweighs the decline in recall. Note that the distribution of ranks is not normal, so in Figure 1 we present frequency polygons for rank groups of 30 (which is the number of items that fit on a single screen, i.e., the number of items in the first group (r030) shows solutions that will be displayed on the first screen). The majority of solutions ranked by similarity appear high in the list (in fact, on the first two or three screens). 010203040506070r030r060r090r120r150r180r210r240r270r300similarityfrequency Figure 1 Frequency polygons for ranks 4 Subjective evaluation The objective evaluation reported above uses a single reference translation and is correspondingly conservative in estimating the coverage of the sys-tem. However, many expressions studied have more than one fluent translation. For instance, in poor repair is not the only equivalent for the Rus-sian expression плохо отремонтированные. It is also possible to translate it as unsatisfactory condi-tion, bad state of repair, badly in need of repair, and so on. The objective evaluation shows that the system has been able to find the suggestion used by a particular translator for the problem studied. It does not tell us whether the system has found some other translations suitable for the context. Such legitimate translation variation implies that the per-formance of a system should be studied on the ba-sis of multiple reference translations, though typi-cally just two reference translations are used (Pap-ineni, et al, 2001). This might be enough for the purposes of a fully automatic MT tool, but in the context of a translator's amanuensis which deals with expressions difficult for human translators, it is reasonable to work with a larger range of ac-ceptable target expressions. With this in mind we evaluated the performance of the tool with a panel of 12 professional transla-tors. Problematic expressions were highlighted and the translators were asked to find suitable sugges-tions produced by the tool for these expressions and rank their usability on a scale from 1 to 5 (not acceptable to fully idiomatic, so 1 means that no usable translation was found at all). Sentences themselves were selected from prob-lems discussed on professional translation forums proz.com and forum.lingvo.ru. Given the range of corpora used in the system (reference and newspa-143

per corpora), the examples were filtered to address expressions used in newspapers. The goal of the subjective evaluation experiment was to establish the usefulness of the system for translators beyond the conservative estimate given by the objective evaluation. The intuition behind the experiment is that if there are several admissi-ble translations for the SL contextual descriptors, and system output matches any of these solutions, then the system has generated something useful. Therefore, we computed recall on sets of human solutions rather than on individual solutions. We matched 210 different human solutions to 36 trans-lation problems. To compute more realistic recall figures, we counted cases when the system output matches any of the human solutions in the set. Table 3 compares the conservative estimate of the objective evaluation and the more realistic estimate on a single data set.  2w default 2w with sem filt Conservative  32.4%; r=53.68 21.9%; r=34.67 Realistic 75.0%;   r=7.48 61.1%;   r=3.95 Table 3 Recall and rank for 2-word descriptors Since the data set is different, the figures for the conservative estimate are higher than those for the objective evaluation data set. However, the table shows the there is a gap between the conservative estimate and the realistic coverage of the transla-tion problems by the system, and that real coverage of indirect translation equivalents is potentially much higher. Table 4 shows averages (and standard deviation σ) of the usability scores divided in four groups: (1) solutions that are found both by our system and the ORD; (2) solutions found only by our system; (3) solutions found only by ORD (4) solutions found by neither:  system (+) system (–) ORD (+) 4.03 (0.42) 3.62 (0.89)ORD (–) 4.25 (0.79) 3.15 (1.15)Table 4 Human scores and σ for system output It can be seen from the table that human users find the system most useful for those problems where the solution does not match any of the direct dic-tionary equivalents, but is generated by the system. 5 Conclusions We have presented a method of finding indirect translation equivalents in comparable corpora, and integrated it into a system which assists translators in indirect lexical transfer. The method outper-forms established methods of extracting indirect translation equivalents from parallel corpora. We can interpret these results as an indication that our method, rather than learning individual indirect transformations, models the entire family of transformations entailed by indirect lexical transfer. In other words it learns a translation strat-egy which is based on the distributional similarity of words in a monolingual corpus, and applies this strategy to novel, previously unseen examples. The coverage of the tool and additional filtering techniques make it useful for professional transla-tors in automating the search for non-trivial, indi-rect translation equivalents, especially equivalents for multiword expressions. References Gregory Grefenstette. 2002. Multilingual corpus-based extraction and the very large lexicon. In: Lars Borin, editor, Language and Computers, Parallel corpora, parallel worlds, pages 137-149. Rodopi. Martin Kay. 1997. The proper place of men and ma-chines in language translation. Machine Translation, 12(1-2):3-23. Philippe Langlais, Michel Simard, and Jean Véronis. 1998. Methods and practical issues in evaluating alignment techniques. In Proc. Joint COLING-ACL-98, pages 711-717. Jeremy Munday. 2001. Introducing translation studies. Theories and Applications. Routledge, New York. Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19-51. Papineni, K., Roukos, S., Ward, T., & Zhu, W.-J. (2001). Bleu: a method for automatic evaluation of machine translation, RC22176 W0109-022: IBM. Reinhard Rapp. 1999. Automatic identification of word translations from unrelated English and German cor-pora. In Procs. the 37th ACL, pages 395-398. Reinhard Rapp. 2004. A freely available automatically generated thesaurus of related words. In Procs. LREC 2004, pages 395-398, Lisbon. Serge Sharoff, Bogdan Babych and Anthony Hartley 2006. Using Comparable Corpora to Solve Problems Difficult for Human Translators. In: Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pp. 739-746. Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 144–151,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

144

ForestRescoring:FasterDecodingwithIntegratedLanguageModels∗LiangHuangUniversityofPennsylvaniaPhiladelphia,PA19104lhuang3@cis.upenn.eduDavidChiangUSCInformationSciencesInstituteMarinadelRey,CA90292chiang@isi.eduAbstractEfﬁcientdecodinghasbeenafundamentalprobleminmachinetranslation,especiallywithanintegratedlanguagemodelwhichisessentialforachievinggoodtranslationquality.Wedevelopfasterapproachesforthisproblembasedonk-bestparsingalgo-rithmsanddemonstratetheireffectivenessonbothphrase-basedandsyntax-basedMTsystems.Inbothcases,ourmethodsachievesigniﬁcantspeedimprovements,oftenbymorethanafactoroften,overtheconven-tionalbeam-searchmethodatthesamelev-elsofsearcherrorandtranslationaccuracy.1IntroductionRecenteffortsinstatisticalmachinetranslation(MT)haveseenpromisingimprovementsinout-putquality,especiallythephrase-basedmodels(OchandNey,2004)andsyntax-basedmodels(Chiang,2005;Galleyetal.,2006).However,efﬁcientde-codingundertheseparadigms,especiallywithinte-gratedlanguagemodels(LMs),remainsadifﬁcultproblem.Partofthecomplexityarisesfromtheex-pressivepowerofthetranslationmodel:forexam-ple,aphrase-orword-basedmodelwithfullreorder-inghasexponentialcomplexity(Knight,1999).Thelanguagemodelalso,iffullyintegratedintothede-coder,introducesanexpensiveoverheadformain-tainingtarget-languageboundarywordsfordynamic∗TheauthorswouldliketothankDanGildea,JonathanGraehl,MarkJohnson,KevinKnight,DanielMarcu,BobMooreandHaoZhang.L.H.waspartiallysupportedbyNSFITRgrantsIIS-0428020whilevisitingUSC/ISIandEIA-0205456atUPenn.D.C.waspartiallysupportedundertheGALE/DARPAprogram,contractHR0011-06-C-0022.programming(Wu,1996;OchandNey,2004).Inpractice,onemustprunethesearchspaceaggres-sivelytoreduceittoareasonablesize.AmuchsimpleralternativemethodtoincorporatetheLMisrescoring:weﬁrstdecodewithouttheLM(henceforth−LMdecoding)toproduceak-bestlistofcandidatetranslations,andthenrerankthek-bestlistusingtheLM.Thismethodrunsmuchfasterinpracticebutoftenproducesaconsiderablenumberofsearcherrorssincethetruebesttranslation(takingLMintoaccount)isoftenoutsideofthek-bestlist.Cubepruning(Chiang,2007)isacompromisebe-tweenrescoringandfull-integration:itrescoresksubtranslationsateachnodeoftheforest,ratherthanonlyattherootnodeasinpurerescoring.Byadapt-ingthek-bestparsingAlgorithm2ofHuangandChiang(2005),itachievessigniﬁcantspeed-upoverfull-integrationonChiang’sHierosystem.Wepushtheideabehindthismethodfurtherandmakethefollowingcontributionsinthispaper:•WegeneralizecubepruningandadaptittotwosystemsverydifferentfromHiero:aphrase-basedsystemsimilartoPharaoh(Koehn,2004)andatree-to-stringsystem(Huangetal.,2006).•Wealsodeviseafastervariantofcubepruning,calledcubegrowing,whichusesalazyversionofk-bestparsing(HuangandChiang,2005)thattriestoreducektotheminimumneededateachnodetoobtainthedesirednumberofhypothesesattheroot.Cubepruningandcubegrowingarecollectivelycalledforestrescoringsincetheybothapproxi-matelyrescorethepackedforestofderivationsfrom−LMdecoding.Inpracticetheyrunanorderof145

magnitudefasterthanfull-integrationwithbeamsearch,atthesamelevelofsearcherrorsandtrans-lationaccuracyasmeasuredbyBLEU.2PreliminariesWeestablishinthissectionauniﬁedframeworkfortranslationwithanintegratedn-gramlanguagemodelinbothphrase-basedsystemsandsyntax-basedsystemsbasedonsynchronouscontext-freegrammars(SCFGs).AnSCFG(LewisandStearns,1968)isacontext-freerewritingsystemforgenerat-ingstringpairs.EachruleA→α,βrewritesapairofnonterminalsinbothlanguages,whereαandβarethesourceandtargetsidecomponents,andthereisaone-to-onecorrespondencebetweenthenonter-minaloccurrencesinαandthenonterminaloccur-rencesinβ.Forexample,thefollowingruleVP→PP(1)VP(2),VP(2)PP(1)capturestheswappingofVPandPPbetweenChi-nese(source)andEnglish(target).2.1TranslationasDeductionWewillusethefollowingexamplefromChinesetoEnglishforbothsystemsdescribedinthissection:yˇuwithSh¯al´ongSharonjˇux´ıngholdle[past]hu`ıt´anmeeting‘heldameetingwithSharon’Atypicalphrase-baseddecodergeneratespartialtarget-languageoutputsinleft-to-rightorderintheformofhypotheses(Koehn,2004).Eachhypothesishasacoveragevectorcapturingthesource-languagewordstranslatedsofar,andcanbeextendedintoalongerhypothesisbyaphrase-pairtranslatinganun-coveredsegment.Thisprocesscanbeformalizedasadeduc-tivesystem.Forexample,thefollowingdeduc-tionstepgrowsahypothesisbythephrase-pairhyˇuSh¯al´ong,withSharoni:(•••):(w,“heldatalk”)(•••••):(w+c,“heldatalkwithSharon”)(1)wherea•inthecoveragevectorindicatesthesourcewordatthispositionis“covered”(forsimplicityweomitheretheendingpositionofthelastphrasewhichisneededfordistortioncosts),andwherewandw+caretheweightsofthetwohypotheses,respectively,withcbeingthecostofthephrase-pair.Similarly,thedecodingproblemwithSCFGscanalsobecastasadeductive(parsing)system(Shieberetal.,1995).Basically,weparsetheinputstringus-ingthesourceprojectionoftheSCFGwhilebuild-ingthecorrespondingsubtranslationsinparallel.Apossibledeductionoftheaboveexampleisnotated:(PP1,3):(w1,t1)(VP3,6):(w2,t2)(VP1,6):(w1+w2+c′,t2t1)(2)wherethesubscriptsdenoteindicesintheinputsen-tencejustasinCKYparsing,w1,w2arethescoresofthetwoantecedentitems,andt1andt2arethecorrespondingsubtranslations.Theresultingtrans-lationt2t1istheinvertedconcatenationasspeciﬁedbythetarget-sideoftheSCFGrulewiththeaddi-tionalcostc′beingthecostofthisrule.Thesetwodeductivesystemsrepresentthesearchspaceofdecodingwithoutalanguagemodel.Whenoneisinstantiatedforaparticularinputstring,itde-ﬁnesasetofderivations,calledaforest,representedinacompactstructurethathasastructureofagraphinthephrase-basedcase,ormoregenerally,ahyper-graphinbothcases.Accordinglywecallitemslike(•••••)and(VP1,6)nodesintheforest,andinstan-tiateddeductionslike(•••••)→(•••)withSharon,(VP1,6)→(VP3,6)(PP1,3)wecallhyperedgesthatconnectoneormorean-tecedentnodestoaconsequentnode.2.2AddingaLanguageModelTointegratewithabigramlanguagemodel,wecanusethedynamic-programmingalgorithmsofOchandNey(2004)andWu(1996)forphrase-basedandSCFG-basedsystems,respectively,whichwemaythinkofasdoingaﬁner-grainedversionofthedeductionsabove.Eachnodevintheforestwillbesplitintoasetofaugmenteditems,whichwecall+LMitems.Forphrase-baseddecoding,a+LMitemhastheform(va)whereaisthelastwordofthehypothesis.Thusa+LMversionofDeduc-tion(1)mightbe:(•••talk):(w,“heldatalk”)(•••••Sharon):(w′,“heldatalkwithSharon”)146

1.01.13.51.04.07.02.58.38.52.49.58.49.217.015.2(VPheld⋆meeting3,6)(VPheld⋆talk3,6)(VPhold⋆conference3,6)(PPwith⋆Sharon1,3)(PPalong⋆Sharon1,3)(PPwith⋆Shalong1,3)1.04.07.0(PPwith⋆Sharon1,3)(PPalong⋆Sharon1,3)(PPwith⋆Shalong1,3)2.52.48.3(PPwith⋆Sharon1,3)(PPalong⋆Sharon1,3)(PPwith⋆Shalong1,3)1.04.07.02.52.48.39.59.2(PPwith⋆Sharon1,3)(PPalong⋆Sharon1,3)(PPwith⋆Shalong1,3)1.04.07.02.52.48.39.29.58.5(a)(b)(c)(d)Figure1:Cubepruningalongonehyperedge.(a):thenumbersinthegriddenotethescoreoftheresulting+LMitem,includingthecombinationcost;(b)-(d):thebest-ﬁrstenumerationofthetopthreeitems.Noticethattheitemspoppedin(b)and(c)areoutoforderduetothenon-monotonicityofthecombinationcost.wherethescoreoftheresulting+LMitemw′=w+c−logPlm(with|talk)nowincludesacombinationcostduetothebigramsformedwhenapplyingthephrase-pair.Similarly,a+LMiteminSCFG-basedmodelshastheform(va⋆b),whereaandbareboundarywordsofthehypothesisstring,and⋆isaplaceholdersymbolforanelidedpartofthatstring,indicatingthatapossibletranslationofthepartoftheinputspannedbyvstartswithaandendswithb.Anex-ample+LMversionofDeduction(2)is:(PPwith⋆Sharon1,3):(w1,t1)(VPheld⋆talk3,6):(w2,t2)(VPheld⋆Sharon1,6):(w,t2t1)wherew=w1+w2+c′−logPlm(with|talk)withasimilarcombinationcostformedincombiningad-jacentboundarywordsofantecedents.Thisschemecanbeeasilyextendedtoworkwithageneraln-grammodel(Chiang,2007).Theexperimentsinthispaperusetrigrammodels.Theconventionalfull-integrationapproachtra-versestheforestbottom-upandexploresallpos-sible+LMdeductionsalongeachhyperedge.ThetheoreticalrunningtimeofthisalgorithmisO(|F||T|(m−1))forphrase-basedmodels,andO(|F||T|4(m−1))forbinary-branchingSCFG-basedmodels,where|F|isthesizeoftheforest,and|T|isthenumberofpossibletarget-sidewords.Evenifweassumeaconstantnumberoftranslationsforeachwordintheinput,withatrigrammodel,thisstillamountstoO(n11)forSCFG-basedmodelsandO(2nn2)forphrase-basedmodels.3CubePruningCubepruning(Chiang,2007)reducesthesearchspacesigniﬁcantlybasedontheobservationthatwhentheabovemethodiscombinedwithbeamsearch,onlyasmallfractionofthepossible+LMitemsatanodewillescapebeingpruned,andmore-overwecanselectwithreasonableaccuracythosetop-kitemswithoutcomputingallpossibleitemsﬁrst.Inanutshell,cubepruningworksonthe−LMforest,keepingatmostk+LMitemsateachnode,andusesthek-bestparsingAlgorithm2ofHuangandChiang(2005)tospeedupthecomputation.Forsimplicityofpresentation,wewilluseconcreteSCFG-basedexamples,butthemethodappliestothegeneralhypergraphframeworkinSection2.ConsiderFigure1(a).Herek=3andweuseD(v)todenotethetop-k+LMitems(insortedor-der)ofnodev.SupposewehavecomputedD(u1)andD(u2)forthetwoantecedentnodesu1=(VP3,6)andu2=(PP1,3)respectively.Thenfortheconsequentnodev=(VP1,6)wejustneedtoderivethetop-3fromthe9combinationsof(Di(u1),Dj(u2))withi,j∈[1,3].Sincethean-tecedentitemsaresorted,itisverylikelythatthebestconsequentitemsinthisgridlietowardstheupper-leftcorner.Thissituationisverysimilartok-bestparsingandwecanadapttheAlgorithm2ofHuangandChiang(2005)heretoexplorethisgridinabest-ﬁrstorder.Supposethatthecombinationcostsarenegligible,andthereforetheweightofaconsequentitemisjusttheproductoftheweightsoftheantecedentitems.147

1:functionCUBE(F)⊲theinputisaforestF2:forv∈Fin(bottom-up)topologicalorderdo3:KBEST(v)4:returnD1(TOP)5:procedureKBEST(v)6:cand←{he,1i|e∈IN(v)}⊲foreachincominge7:HEAPIFY(cand)⊲apriorityqueueofcandidates8:buf←∅9:while|cand|>0and|buf|<kdo10:item←POP-MIN(cand)11:appenditemtobuf12:PUSHSUCC(item,cand)13:sortbuftoD(v)14:procedurePUSHSUCC(he,ji,cand)15:eisv→u1...u|e|16:foriin1...|e|do17:j′←j+bi18:if|D(ui)|≥j′ithen19:PUSH(he,j′i,cand)Figure2:Pseudocodeforcubepruning.ThenweknowthatD1(v)=(D1(u1),D1(u2)),theupper-leftcornerofthegrid.Moreover,weknowthatD2(v)isthebetterof(D1(u1),D2(u2))and(D2(u1),D1(u2)),thetwoneighborsoftheupper-leftcorner.Wecontinueinthisway(seeFig-ure1(b)–(d)),enumeratingtheconsequentitemsbest-ﬁrstwhilekeepingtrackofarelativelysmallnumberofcandidates(shadedcellsinFigure1(b),candinFigure2)forthenext-bestitem.However,whenwetakeintoaccountthecombi-nationcosts,thisgridisnolongermonotonicingen-eral,andtheabovealgorithmwillnotalwaysenu-merateitemsinbest-ﬁrstorder.WecanseethisintheﬁrstiterationinFigure1(b),whereanitemwithscore2.5hasbeenenumeratedeventhoughthereisanitemwithscore2.4stilltocome.Thusweriskmakingmoresearcherrorsthanthefull-integrationmethod,butinpracticethelossismuchlesssignif-icantthanthespeedup.Becauseofthisdisordering,wedonotputtheenumerateditemsdirectlyintoD(v);instead,wecollectitemsinabuffer(bufinFigure2)andre-sortthebufferintoD(v)afterithasaccumulatedkitems.1Ingeneralthegrammarmayhavemultiplerulesthatsharethesamesourcesidebuthavedifferenttargetsides,whichwehavetreatedhereasseparate1Noticethatdifferentcombinationsmighthavethesamere-sultingitem,inwhichcaseweonlykeeptheonewiththebetterscore(sometimescalledhypothesisrecombinationinMTliter-ature),sothenumberofitemsinD(v)mightbelessthank.methodk-best+LMrescoring...rescoringAlg.3onlyattherootnodecubepruningAlg.2on-the-ﬂyateachnodecubegrowingAlg.3on-the-ﬂyateachnodeTable1:Comparisonofthethreemethods.hyperedgesinthe−LMforest.InHiero,thesehy-peredgesareprocessedasasingleunitwhichwecallahyperedgebundle.Thedifferenttargetsidesthenconstituteathirddimensionofthegrid,form-ingacubeofpossiblecombinations(Chiang,2007).Nowconsiderthattherearemanyhyperedgesthatderivev,andweareonlyinterestedthetop+LMitemsofvoverallincominghyperedges.FollowingAlgorithm2,weinitializethepriorityqueuecandwiththeupper-leftcorneritemfromeachhyper-edge,andproceedasabove.SeeFigure2forthepseudocodeforcubepruning.Weusethenotationhe,jitoidentifythederivationofvviathehyper-edgeeandthejithbestsubderivationofantecedentui(1≤i≤|j|).Also,welet1standforavec-torwhoseelementsareall1,andbiforthevectorwhosemembersareall0exceptfortheithwhosevalueis1(thedimensionalityofeithershouldbeev-identfromthecontext).Theheartofthealgorithmislines10–12.Lines10–11movethebestderiva-tionhe,jifromcandtobuf,andthenline12pushesitssuccessors{he,j+bii|i∈1...|e|}intocand.4CubeGrowingAlthoughmuchfasterthanfull-integration,cubepruningstillcomputesaﬁxedamountof+LMitemsateachnode,manyofwhichwillnotbeusefulforarrivingatthe1-besthypothesisattheroot.Itwouldbemoreefﬁcienttocomputeasfew+LMitemsateachnodeasareneededtoobtainthe1-besthypoth-esisattheroot.Thisnewmethod,calledcubegrow-ing,isalazyversionofcubepruningjustasAlgo-rithm3ofHuangandChiang(2005),isalazyver-sionofAlgorithm2(seeTable1).Insteadoftraversingtheforestbottom-up,cubegrowingvisitsnodesrecursivelyindepth-ﬁrstor-derfromtherootnode(Figure4).FirstwecallLAZYJTHBEST(TOP,1),whichusesthesameal-gorithmascubepruningtoﬁndthe1-best+LMitemoftherootnodeusingthebest+LMitemsof148

1.01.13.51.04.07.02.15.18.12.25.28.24.67.610.61.04.07.02.52.48.3(a)h-values(b)truecostsFigure3:Exampleofcubegrowingalongonehyper-edge.(a):theh(x)scoresforthegridinFigure1(a),assuminghcombo(e)=0.1forthishyperedge;(b)cubegrowingpreventsearlyrankingofthetop-leftcell(2.5)asthebestiteminthisgrid.theantecedentnodes.However,inthiscasethebest+LMitemsoftheantecedentnodesarenotknown,becausewehavenotvisitedthemyet.Sowere-cursivelyinvokeLAZYJTHBESTontheantecedentnodestoobtainthemasneeded.EachinvocationofLAZYJTHBEST(v,j)willrecursivelycallitselfontheantecedentsofvuntilitisconﬁdentthatthejthbest+LMitemfornodevhasbeenfound.Consideragainthecaseofonehyperedgee.Be-causeofthenonmonotonicitycausedbycombina-tioncosts,theﬁrst+LMitem(he,1i)poppedfromcandisnotguaranteedtobethebestofallcombina-tionsalongthishyperedge(forexample,thetop-leftcellof2.5inFigure1isnotthebestinthegrid).Sowecannotsimplyenumerateitemsjustastheycomeoffofcand.2Instead,weneedtostoreuppoppeditemsinabufferbuf,justasincubepruning,andenumerateanitemonlywhenweareconﬁdentthatitwillneverbesurpassedinthefuture.Inotherwords,wewouldliketohaveanestimateofthebestitemnotexploredyet(analogoustotheheuristicfunc-tioninA*search).Ifwecanestablishalowerboundhcombo(e)onthecombinationcostofany+LMde-ductionviahyperedgee,thenwecanformamono-tonicgrid(seeFigure3(a))oflowerboundsonthegridofcombinations,byusinghcombo(e)inplaceofthetruecombinationcostforeach+LMitemxinthegrid;callthislowerboundh(x).Nowsupposethatthegray-shadedcellsinFig-ure3(a)arethemembersofcand.Thenthemin-imumofh(x)overtheitemsincand,inthisex-2Ifwedid,thentheout-of-orderenumerationof+LMitemsatanantecedentnodewouldcauseanentireroworcolumninthegridtobedisorderedattheconsequentnode,potentiallyleadingtoamultiplicationofsearcherrors.1:procedureLAZYJTHBEST(v,j)2:ifcand[v]isundeﬁnedthen3:cand[v]←∅4:FIRE(e,1,cand)foreache∈IN(v)5:buf[v]←∅6:while|D(v)|<jand|buf[v]|+|D(v)|<kand|cand[v]|>0do7:item←POP-MIN(cand[v])8:PUSH(item,buf[v])9:PUSHSUCC(item,cand[v])10:bound←min{h(x)|x∈cand[v]}11:ENUM(buf[v],D(v),bound)12:ENUM(buf[v],D(v),+∞)13:procedureFIRE(e,j,cand)14:eisv→u1...u|e|15:foriin1...|e|do16:LAZYJTHBEST(ui,ji)17:if|D(ui)|<jithenreturn18:PUSH(he,ji,cand)19:procedurePUSHSUCC(he,ji,cand)20:FIRE(e,j+bi,cand)foreachiin1...|e|21:procedureENUM(buf,D,bound)22:while|buf|>0andMIN(buf)<bounddo23:appendPOP-MIN(buf)toDFigure4:Pseudocodeofcubegrowing.ample,min{2.2,5.1}=2.2isalowerboundonthecostofanyiteminthefutureforthehyperedgee.Indeed,ifcandcontainsitemsfrommultiplehy-peredgesforasingleconsequentnode,thisisstillavalidlowerbound.Moreformally:Lemma1.Foreachnodevintheforest,thetermbound=minx∈cand[v]h(x)(3)isalowerboundonthetruecostofanyfutureitemthatisyettobeexploredforv.Proof.Foranyitemxthatisnotexploredyet,thetruecostc(x)≥h(x),bythedeﬁnitionofh.Andthereexistsanitemy∈cand[v]alongthesamehy-peredgesuchthath(x)≥h(y),duetothemono-tonicityofhwithinthegridalongonehyperedge.Wealsohaveh(y)≥boundbythedeﬁnitionofbound.Thereforec(x)≥bound.NowwecansafelypopthebestitemfrombufifitstruecostMIN(buf)isbetterthanboundandpassituptotheconsequentnode(lines21–23);butother-wise,wehavetowaitformoreitemstoaccumulateinbuftopreventapotentialsearcherror,forexam-ple,inthecaseofFigure3(b),wherethetop-leftcell149

(a)12345(b)12345Figure5:(a)Pharaohexpandsthehypothesesinthecurrentbin(#2)intolongerones.(b)InCubit,hy-pothesesinpreviousbinsarefedviahyperedgebun-dles(solidarrows)intoapriorityqueue(shadedtri-angle),whichemptiesintothecurrentbin(#5).(2.5)isworsethanthecurrentboundof2.2.Theup-dateofboundineachiteration(line10)canbeefﬁ-cientlyimplementedbyusinganotherheapwiththesamecontentsascandbutprioritizedbyhinstead.Inpracticethisisanegligibleoverheadontopofcubepruning.Wenowturntotheproblemofestimatingtheheuristicfunctionhcombo.Inpractice,computingtruelowerboundsofthecombinationcostsistooslowandwouldcompromisethespeedupgainedfromcubegrowing.Soweinsteaduseamuchsim-plermethodthatjustcalculatestheminimumcom-binationcostofeachhyperedgeinthetop-ideriva-tionsoftherootnodein−LMdecoding.Thisisjustanapproximationofthetruelowerbound,andbadestimatescanleadtosearcherrors.However,thehopeisthatbychoosingtherightvalueofi,thesees-timateswillbeaccurateenoughtoaffectthesearchqualityonlyslightly,whichisanalogousto“almostadmissible”heuristicsinA*search(Soricut,2006).5ExperimentsWetestourmethodsontwolarge-scaleEnglish-to-Chinesetranslationsystems:aphrase-basedsystemandourtree-to-stringsystem(Huangetal.,2006).1.01.13.51.04.07.02.58.38.52.49.58.49.217.015.2(•••meeting)(•••talk)(•••conference)withSharonandSharonwithArielSharon...Figure6:Ahyperedgebundlerepresentsall+LMdeductionsthatderivesaniteminthecurrentbinfromthesamecoveragevector(seeFigure5).Thephrasesonthetopdenotethetarget-sidesofappli-cablephrase-pairssharingthesamesource-side.5.1Phrase-basedDecodingWeimplementedCubit,aPythoncloneofthePharaohdecoder(Koehn,2004),3andadaptedcubepruningtoitasfollows.AsinPharaoh,eachbinicontainshypotheses(i.e.,+LMitems)coveringiwordsonthesource-side.Butateachbin(seeFig-ure5),all+LMitemsfrompreviousbinsareﬁrstpartitionedinto−LMitems;thenthehyperedgesleadingfromthose−LMitemsarefurthergroupedintohyperedgebundles(Figure6),whichareplacedintothepriorityqueueofthecurrentbin.OurdatapreparationfollowsHuangetal.(2006):thetrainingdataisaparallelcorpusof28.3MwordsontheEnglishside,andatrigramlanguagemodelistrainedontheChineseside.Weusethesametestsetas(Huangetal.,2006),whichisa140-sentencesub-setoftheNIST2003testsetwith9–36wordsontheEnglishside.Theweightsforthelog-linearmodelaretunedonaseparatedevelopmentset.Wesetthedecoderphrase-tablelimitto100assuggestedin(Koehn,2004)andthedistortionlimitto4.Figure7(a)comparescubepruningagainstfull-integrationintermsofsearchqualityvs.searchef-ﬁciency,undervariouspruningsettings(thresholdbeamsetto0.0001,stacksizevaryingfrom1to200).Searchqualityismeasuredbyaveragemodelcostpersentence(lowerisbetter),andsearchefﬁ-ciencyismeasuredbytheaveragenumberofhy-pothesesgenerated(smallerisfaster).Ateachlevel3Inourtests,CubitalwaysobtainsaBLEUscorewithin0.004ofPharaoh’s(Figure7(b)).Sourcecodeavailableathttp://www.cis.upenn.edu/˜lhuang3/cubit/150

7680848892102103104105106average model costaverage number of hypotheses per sentencefull-integration (Cubit)cube pruning (Cubit)0.2000.2050.2100.2150.2200.2250.2300.2350.2400.245102103104105106BLEU scoreaverage number of hypotheses per sentencePharaohfull-integration (Cubit)cube pruning (Cubit)(a)(b)Figure7:Cubepruningvs.full-integration(withbeamsearch)onphrase-baseddecoding.ofsearchquality,thespeed-upisalwaysbetterthanafactorof10.Thespeed-upatthelowestsearch-errorlevelisafactorof32.Figure7(b)makesasimilarcomparisonbutmeasuressearchqualitybyBLEU,whichshowsanevenlargerrelativespeed-upforagivenBLEUscore,becausetranslationswithverydifferentmodelcostsmighthavesimilarBLEUscores.Italsoshowsthatourfull-integrationimple-mentationinCubitfaithfullyreproducesPharaoh’sperformance.Fixingthestacksizeto100andvary-ingthethresholdyieldedasimilarresult.5.2Tree-to-stringDecodingIntree-to-string(alsocalledsyntax-directed)decod-ing(Huangetal.,2006;Liuetal.,2006),thesourcestringisﬁrstparsedintoatree,whichisthenre-cursivelyconvertedintoatargetstringaccordingtotransferrulesinasynchronousgrammar(Galleyetal.,2006).Forinstance,thefollowingruletranslatesanEnglishpassiveconstructionintoChinese:VPVBDwasVP-Cx1:VBNPPINbyx2:NP-C→b`eix2x1Ourtree-to-stringsystemperformsslightlybet-terthanthestate-of-the-artphrase-basedsystemPharaohontheabovedataset.Althoughdiffer-entfromtheSCFG-basedsystemsinSection2,itsderivationtreesremaincontext-freeandthesearchspaceisstillahypergraph,wherewecanadaptthemethodspresentedinSections3and4.ThedatasetissameasinSection5.1,exceptthatwealsoparsedtheEnglish-sideusingavariantoftheCollins(1997)parser,andthenextracted24.7Mtree-to-stringrulesusingthealgorithmof(Galleyetal.,2006).Sinceourtree-to-stringrulesmayhavemanyvariables,weﬁrstbinarizeeachhyperedgeintheforestonthetargetprojection(Huang,2007).Allthethree+LMdecodingmethodstobecom-paredbelowtakethesebinarizedforestsasinput.Forcubegrowing,weuseanon-duplicatek-bestmethod(Huangetal.,2006)toget100-bestuniquetransla-tionsaccordingto−LMtoestimatethelower-boundheuristics.4Thispreprocessingsteptakesonaver-age0.12secondspersentence,whichisnegligibleincomparisontothe+LMdecodingtime.Figure8(a)comparescubegrowingandcubepruningagainstfull-integrationundervariousbeamsettingsinthesamefashionofFigure7(a).Atthelowestlevelofsearcherror,therelativespeed-upfromcubegrowingandcubepruningcomparedwithfull-integrationisbyafactorof9.8and4.1,respec-tively.Figure8(b)isasimilarcomparisonintermsofBLEUscoresandshowsanevenbiggeradvantageofcubegrowingandcubepruningoverthebaseline.4Ifahyperedgeisnotrepresentedatallinthe100-best−LMderivationsattherootnode,weusethe1-best−LMderivationofthishyperedgeinstead.Here,rulesthatsharethesamesourcesidebuthavedifferenttargetsidesaretreatedasseparatehy-peredges,notcollectedintohyperedgebundles,sincegroupingbecomesdifﬁcultafterbinarization.151

218.2218.4218.6218.8219.0103104105average model costaverage number of +LM items explored per sentencefull-integrationcube pruningcube growing0.2540.2560.2580.2600.262103104105BLEU scoreaverage number of +LM items explored per sentencefull-integrationcube pruningcube growing(a)(b)Figure8:Cubegrowingvs.cubepruningvs.full-integration(withbeamsearch)ontree-to-stringdecoding.6ConclusionsandFutureWorkWehavepresentedanovelextensionofcubeprun-ingcalledcubegrowing,andshownhowbothcanbeseenasgeneralforestrescoringtechniquesapplica-bletobothphrase-basedandsyntax-baseddecoding.Weevaluatedthesemethodsonlarge-scaletransla-tiontasksandobservedconsiderablespeedimprove-ments,oftenbymorethanafactoroften.Weplantoinvestigatehowtoadaptcubegrowingtophrase-basedandhierarchicalphrase-basedsystems.Theseforestrescoringalgorithmshavepotentialapplicationstoothercomputationallyintensivetasksinvolvingcombinationsofdifferentmodels,forexample,head-lexicalizedparsing(Collins,1997);jointparsingandsemanticrolelabeling(SuttonandMcCallum,2005);ortaggingandparsingwithnon-localfeatures.Thusweenvisionforestrescoringasbeingofgeneralapplicabilityforreducingcompli-catedsearchspaces,asanalternativetosimulatedannealingmethods(Kirkpatricketal.,1983).ReferencesDavidChiang.2005.Ahierarchicalphrase-basedmodelforstatisticalmachinetranslation.InProc.ACL.DavidChiang.2007.Hierarchicalphrase-basedtranslation.ComputationalLinguistics,33(2).Toappear.MichaelCollins.1997.Threegenerativelexicalisedmodelsforstatisticalparsing.InProc.ACL.M.Galley,J.Graehl,K.Knight,D.Marcu,S.DeNeefe,W.Wang,andI.Thayer.2006.Scalableinferenceandtrainingofcontext-richsyntactictranslationmodels.InProc.COLING-ACL.LiangHuangandDavidChiang.2005.Betterk-bestparsing.InProc.IWPT.LiangHuang,KevinKnight,andAravindJoshi.2006.Sta-tisticalsyntax-directedtranslationwithextendeddomainoflocality.InProc.AMTA.LiangHuang.2007.Binarization,synchronousbinarization,andtarget-sidebinarization.InProc.NAACLWorkshoponSyntaxandStructureinStatisticalTranslation.S.Kirkpatrick,C.D.Gelatt,andM.P.Vecchi.1983.Optimiza-tionbysimulatedannealing.Science,220(4598):671–680.KevinKnight.1999.Decodingcomplexityinword-replacementtranslationmodels.ComputationalLinguistics,25(4):607–615.PhilippKoehn.2004.Pharaoh:abeamsearchdecoderforphrase-basedstatisticalmachinetranslationmodels.InProc.AMTA,pages115–124.P.M.LewisandR.E.Stearns.1968.Syntax-directedtransduc-tion.J.ACM,15:465–488.YangLiu,QunLiu,andShouxunLin.2006.Tree-to-stringalignmenttemplateforstatisticalmachinetranslation.InProc.COLING-ACL,pages609–616.FranzJosephOchandHermannNey.2004.Thealignmenttemplateapproachtostatisticalmachinetranslation.Com-putationalLinguistics,30:417–449.StuartShieber,YvesSchabes,andFernandoPereira.1995.Principlesandimplementationofdeductiveparsing.J.LogicProgramming,24:3–36.RaduSoricut.2006.NaturalLanguageGenerationusinganInformation-SlimRepresentation.Ph.D.thesis,UniversityofSouthernCalifornia.CharlesSuttonandAndrewMcCallum.2005.Jointparsingandsemanticrolelabeling.InProc.CoNLL2005.DekaiWu.1996.Apolynomial-timealgorithmforstatisticalmachinetranslation.InProc.ACL.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 152–159,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

152

StatisticalMachineTranslationthroughGlobalLexicalSelectionandSentenceReconstructionSrinivasBangalore,PatrickHaffner,StephanKanthakAT&TLabs-Research180ParkAve,FlorhamPark,NJ07932{srini,haffner,skanthak}@research.att.comAbstractMachinetranslationofasourcelanguagesentenceinvolvesselectingappropriatetar-getlanguagewordsandorderingthese-lectedwordstoformawell-formedtar-getlanguagesentence.Mostofthepre-viousworkonstatisticalmachinetransla-tionrelieson(local)associationsoftargetwords/phraseswithsourcewords/phrasesforlexicalselection.Incontrast,inthispa-per,wepresentanovelapproachtolexicalselectionwherethetargetwordsareassoci-atedwiththeentiresourcesentence(global)withouttheneedtocomputelocalassocia-tions.Further,wepresentatechniqueforreconstructingthetargetlanguagesentencefromtheselectedwords.Wecomparethere-sultsofthisapproachagainstthoseobtainedfromaﬁnite-statebasedstatisticalmachinetranslationsystemwhichreliesonlocallex-icalassociations.1IntroductionMachinetranslationcanbeviewedasconsistingoftwosubproblems:(a)lexicalselection,whereappro-priatetargetlanguagelexicalitemsarechosenforeachsourcelanguagelexicalitemand(b)lexicalre-ordering,wherethechosentargetlanguagelexicalitemsarerearrangedtoproduceameaningfultargetlanguagestring.Mostofthepreviousworkonstatis-ticalmachinetranslation,asexempliﬁedin(Brownetal.,1993),employsword-alignmentalgorithm(suchasGIZA++(OchandNey,2003))thatpro-videslocalassociationsbetweensourceandtargetwords.Thesource-to-targetwordalignmentsaresometimesaugmentedwithtarget-to-sourcewordalignmentsinordertoimproveprecision.Further,theword-levelalignmentsareextendedtophrase-levelalignmentsinordertoincreasetheextentoflocalassociations.Thephrasalassociationscompilesomeamountof(local)lexicalreorderingofthetar-getwords–thosepermittedbythesizeofthephrase.Mostofthestate-of-the-artmachinetranslationsys-temsusephrase-levelassociationsinconjunctionwithatargetlanguagemodeltoproducesentences.Thereisrelativelylittleemphasison(global)lexicalreorderingotherthanthelocalreorderingspermit-tedwithinthephrasalalignments.Afewexceptionsarethehierarchical(possiblysyntax-based)trans-ductionmodels(Wu,1997;Alshawietal.,1998;YamadaandKnight,2001;Chiang,2005)andthestringtransductionmodels(Kanthaketal.,2005).Inthispaper,wepresentanalternateapproachtolexicalselectionandlexicalreordering.Forlexicalselection,incontrasttothelocalapproachesofas-sociatingtargettosourcewords,weassociatetar-getwordstotheentiresourcesentence.Theintu-itionisthattheremaybelexico-syntacticfeaturesofthesourcesentence(notnecessarilyasinglesourceword)thatmighttriggerthepresenceofatargetwordinthetargetsentence.Furthermore,itmightbedifﬁculttoexactlyassociateatargetwordtoasourcewordinmanysituations–(a)whenthetranslationsarenotexactbutparaphrases(b)whenthetargetlan-guagedoesnothaveonelexicalitemtoexpressthesameconceptthatisexpressedbyasourceword.Extendingwordtophrasealignmentsattemptstoad-dresssomeofthesesituationswhilealleviatingthenoiseinword-levelalignments.Asaconsequenceofthisgloballexicalselectionapproach,wenolongerhaveatightassociationbe-tweensourceandtargetlanguagewords.There-sultoflexicalselectionissimplyabagofwordsinthetargetlanguageandthesentencehastoberecon-structedusingthisbagofwords.Thewordsinthebag,however,mightbeenhancedwithrichsyntacticinformationthatcouldaidinreconstructingthetar-getsentence.Thisapproachtolexicalselectionand153

Translation modelWFSABilanguagePhrase SegmentedFSA to FSTBilanguageWFSTTransformationBilanguageReorderingLocal PhraseJoint LanguageModelingJoint LanguageAlignmentWordAlignmentSentence AlignedCorpusFigure1:TrainingphasesforoursystemConstructionPermutationPermutation LatticeLexical Choice FST CompositionDecodingSourceSentence/WeightedLatticeTargetDecodingLexical Reodering CompositionFSASentenceModelTranslationModelLanguageTargetFigure2:Decodingphasesforoursystemsentencereconstructionhasthepotentialtocircum-ventlimitationsofword-alignmentbasedmethodsfortranslationbetweenlanguageswithsigniﬁcantlydifferentwordorder(e.g.English-Japanese).Inthispaper,wepresentthedetailsoftrainingagloballexicalselectionmodelusingclassiﬁca-tiontechniquesandsentencereconstructionmod-elsusingpermutationautomata.Wealsopresentastochasticﬁnite-statetransducer(SFST)asanexam-pleofanapproachthatreliesonlocalassociationsanduseittocompareandcontrastourapproach.2SFSTTrainingandDecodingInthissection,wedescribeeachofthecomponentsofourSFSTsystemshowninFigure1.TheSFSTapproachdescribedhereissimilartotheonede-scribedin(BangaloreandRiccardi,2000)whichhassubsequentlybeenadoptedby(Banchsetal.,2005).2.1WordAlignmentTheﬁrststageintheprocessoftrainingalexicalse-lectionmodelisobtaininganalignmentfunction(f)thatgivenapairofsource(s1s2...sn)andtarget(t1t2...tm)languagesentences,mapssourcelan-guagewordsubsequencesintotargetlanguagewordsubsequences,asshownbelow.∀i∃j(f(si)=tj∨f(si)=)(1)Fortheworkreportedinthispaper,wehaveusedtheGIZA++tool(OchandNey,2003)whichim-plementsastring-alignmentalgorithm.GIZA++alignmenthoweverisasymmetricinthatthewordmappingsaredifferentdependingonthedirectionofalignment–source-to-targetortarget-to-source.HenceinadditiontothefunctionsfasshowninEquation1wetrainanotheralignmentfunctiong:∀j∃i(g(tj)=si∨g(tj)=)(2)English:IneedtomakeacollectcallJapanese:ˇ0V	@»?9 Alignment:1503024Figure3:Examplebilingualtextswithalignmentin-formationI:ˇ0need:»?9 to:make:a:collectV	call@Figure4:Bilanguagestringsresultingfromalign-mentsshowninFigure3.2.2BilanguageRepresentationFromthealignmentinformation(seeFigure3),weconstructabilanguagerepresentationofeachsen-tenceinthebilingualcorpus.Thebilanguagestringconsistsofsource-targetsymbolpairsequencesasshowninEquation3.Notethatthetokensofabilan-guagecouldbeeitherorderedaccordingtothewordorderofthesourcelanguageororderedaccordingtothewordorderofthetargetlanguage.Bf=bf1bf2...bfm(3)bfi=(si−1;si,f(si))iff(si−1)==(si,f(si−1);f(si))ifsi−1==(si,f(si))otherwiseFigure4showsanexamplealignmentandthesource-word-orderedbilanguagestringscorrespond-ingtothealignmentshowninFigure3.Wealsoconstructabilanguageusingthealign-mentfunctiongsimilartothebilanguageusingthealignmentfunctionfasshowninEquation3.Thus,thebilanguagecorpusobtainedbycombin-ingthetwoalignmentfunctionsisB=Bf∪Bg.2.3BilingualPhrasesandLocalReorderingWhileword-to-wordtranslationonlyapproximatesthelexicalselectionprocess,phrase-to-phrasemap-pingcangreatlyimprovethetranslationofcolloca-tions,recurrentstrings,etc.Usingphrasesalsoal-lowswordswithinthephrasetobereorderedintothecorrecttargetlanguageorder,thuspartiallysolvingthereorderingproblem.Additionally,SFSTscantakeadvantageofphrasalcorrelationstoimprovethecomputationoftheprobabilityP(WS,WT).Thebilanguagerepresentationcouldresultinsomesourcelanguagephrasestobemappedto154

(emptytargetphrase).Inadditiontothesephrases,wecomputesubsequencesofagivenlengthkonthebilanguagestringandforeachsubsequencewere-orderthetargetwordsofthesubsequencetobeinthesameorderastheyareinthetargetlanguagesen-tencecorrespondingtothatbilanguagestring.Thisresultsinaretokenizationofthebilanguageintoto-kensofsource-targetphrasepairs.2.4SFSTModelFromthebilanguagecorpusB,wetrainann-gramlanguagemodelusingstandardtools(Gofﬁnetal.,2005).Theresultinglanguagemodelisrepresentedasaweightedﬁnite-stateautomaton(S×T→[0,1]).Thesymbolsonthearcsofthisautomaton(siti)areinterpretedashavingthesourceandtargetsymbols(si:ti),makingitintoaweightedﬁnite-statetransducer(S→T×[0,1])thatprovidesaweightedstring-to-stringtransductionfromSintoT:T∗=argmaxTP(si,ti|si−1,ti−1...si−n−1,ti−n−1)2.5DecodingSincewerepresentthetranslationmodelasaweightedﬁnite-statetransducer(TransFST),thedecodingprocessoftranslatinganewsourcein-put(sentenceorweightedlattice(Is))amountstoatransducercomposition(◦)andselectionofthebestprobabilitypath(BestPath)resultingfromthecompositionandprojectingthetargetsequence(π1).T∗=π1(BestPath(Is◦TransFST))(4)However,wehavenoticedthatonthedevelop-mentcorpus,thedecodedtargetsentenceistypicallyshorterthantheintendedtargetsentence.Thismis-matchmaybeduetotheincorrectestimationoftheback-offeventsandtheirprobabilitiesinthetrain-ingphaseofthetransducer.Inordertoalleviatethismismatch,weintroduceanegativewordinser-tionpenaltymodelasamechanismtoproducemorewordsinthetargetsentence.2.6WordInsertionModelThewordinsertionmodelisalsoencodedasaweightedﬁnite-stateautomatonandisincludedinthedecodingsequenceasshowninEquation5.ThewordinsertionFSThasonestateand|PT|numberofarcseachweightedwithaλweightrepresentingthewordinsertioncost.OncompositionasshowninEquation5,thewordinsertionmodelpenalizesorrewardspathswhichhavemorewordsdependingonwhetherλispositiveornegativevalue.T∗=π1(BestPath(Is◦TransFST◦WIP))(5)000010001010021100210103111103110141111432Figure5:Locallyconstraintpermutationautomatonforasentencewith4wordsandwindowsizeof2.2.7GlobalReorderingLocalreorderingasdescribedinSection2.3isre-strictedbythewindowsizekandaccountsonlyfordifferentwordorderwithinphrases.Aspermutingnon-linearautomataistoocomplex,weapplyglobalreorderingbypermutingthewordsofthebesttrans-lationandweightingtheresultbyann-gramlan-guagemodel(seealsoFigure2):T∗=BestPath(perm(T0)◦LMt)(6)Eventhesizeoftheminimalpermutationautoma-tonofalinearautomatongrowsexponentiallywiththelengthoftheinputsequence.Whiledecodingbycompositionsimplyresemblestheprincipleofmem-oization(i.e.here:allstatehypothesesofawholesentencearekeptinmemory),itisnecessarytoei-theruseheuristicforwardpruningorconstrainper-mutationstobewithinalocalwindowofadjustablesize(alsosee(Kanthaketal.,2005)).Wehavecho-sentoconstrainpermutationshere.Figure5showstheresultingminimalpermutationautomatonforaninputsequenceof4wordsandawindowsizeof2.DecodingASRoutputincombinationwithglobalreorderingusesn-bestlistsorextractsthemfromlat-ticesﬁrst.Eachentryofthen-bestlistisdecodedseparatelyandthebesttargetsentenceispickedfromtheunionofthenintermediateresults.3DiscriminantModelsforLexicalSelectionTheapproachfromtheprevioussectionisagenera-tivemodelforstatisticalmachinetranslationrelyingonlocalassociationsbetweensourceandtargetsen-tences.Now,wepresentourapproachforagloballexicalselectionmodelbasedondiscriminativelytrainedclassiﬁcationtechniques.Discriminantmod-elingtechniqueshavebecomethedominantmethodforresolvingambiguityinspeechandotherNLPtasks,outperforminggenerativemodels.Discrimi-nativetraininghasbeenusedmainlyfortranslationmodelcombination(OchandNey,2002)andwiththeexceptionof(Wellingtonetal.,2006;TillmannandZhang,2006),hasnotbeenusedtodirectlytrainparametersofatranslationmodel.Weexpectdis-criminativelytrainedgloballexicalselectionmodels155

tooutperformgenerativelytrainedlocallexicalse-lectionmodelsaswellasprovideaframeworkforincorporatingrichmorpho-syntacticinformation.Statisticalmachinetranslationcanbeformulatedasasearchforthebesttargetsequencethatmaxi-mizesP(T|S),whereSisthesourcesentenceandTisthetargetsentence.Ideally,P(T|S)shouldbeestimateddirectlytomaximizetheconditionallikelihoodonthetrainingdata(discriminantmodel).However,Tcorrespondstoasequencewithaex-ponentiallylargecombinationofpossiblelabels,andtraditionalclassiﬁcationapproachescannotbeuseddirectly.AlthoughConditionalRandomFields(CRF)(Laffertyetal.,2001)trainanexponentialmodelatthesequencelevel,intranslationtaskssuchasoursthecomputationalrequirementsoftrainingsuchmodelsareprohibitivelyexpensive.Weinvestigatetwoapproachestoapproximatingthestringlevelglobalclassiﬁcationproblem,usingdifferentindependenceassumptions.AcomparisonofthetwoapproachesissummarizedinTable1.3.1SequentialLexicalChoiceModelIntheﬁrstapproach,weformulateasequentiallo-calclassiﬁcationproblemasshowninEquations7.ThisapproachissimilartotheSFSTapproachinthatitreliesonlocalassociationsbetweenthesourceandtargetwords(phrases).Wecanuseaconditionalmodel(insteadofajointmodelasbefore)andtheparametersaredeterminedusingdiscriminanttrain-ingwhichallowsforricherconditioningcontext.P(T|S)=YNi=1P(ti|Φ(S,i))(7)whereΦ(S,i)isasetoffeaturesextractedfromthesourcestringS(shortenedasΦintherestofthesection).3.2Bag-of-WordsLexicalChoiceModelThesequentiallexicalchoicemodeldescribedintheprevioussectiontreatstheselectionofalexicalchoiceforasourcewordinthelocallexicalcontextasaclassiﬁcationtask.Thedatafortrainingsuchmodelsisderivedfromwordalignmentsobtainedbye.g.GIZA++.Thedecodedtargetlexicalitemshavetobefurtherreordered,butforcloselyrelatedlanguagesthereorderingcouldbeincorporatedintocorrectlyorderedtargetphrasesasdiscussedprevi-ously.Forpairsoflanguageswithradicallydifferentwordorder(e.g.English-Japanese),thereneedstobeaglobalreorderingofwordssimilartothecaseintheSFST-basedtranslationsystem.Also,forsuchdifferinglanguagepairs,thealignmentalgorithmssuchasGIZA++performpoorly.Theseobservationspromptedustoformulatethelexicalchoiceproblemwithouttheneedforwordalignmentinformation.Werequireasentencealignedcorpusasbefore,butwetreatthetargetsen-tenceasabag-of-wordsorBOWassignedtothesourcesentence.Thegoalis,givenasourcesen-tence,toestimatetheprobabilitythatweﬁndagivenwordinthetargetsentence.Thisiswhy,insteadofproducingatargetsentence,whatweinitiallyobtainisatargetbagofwords.Eachwordinthetargetvo-cabularyisdetectedindependently,sowehavehereaverysimpleuseofbinarystaticclassiﬁers.Train-ingsentencepairsareconsideredaspositiveexam-pleswhenthewordappearsinthetarget,andneg-ativeotherwise.Thus,thenumberoftrainingex-amplesequalsthenumberofsentencepairs,incon-trasttothesequentiallexicalchoicemodelwhichhasonetrainingexampleforeachtokeninthebilin-gualtrainingcorpus.Theclassiﬁeristrainedwithn-gramfeatures(BOgrams(S))fromthesourcesen-tence.Duringdecodingthewordswithconditionalprobabilitygreaterthanathresholdθareconsideredastheresultoflexicalchoicedecoding.BOW∗T={t|P(t|BOgrams(S))>θ}(8)ForreconstructingtheproperorderofwordsinthetargetsentenceweconsiderallpermutationsofwordsinBOW∗Tandweightthembyatargetlan-guagemodel.Thisstepissimilartotheonede-scribedinSection2.7.TheBOWapproachcanalsobemodiﬁedtoallowforlengthadjustmentsoftar-getsentences,ifweaddoptionaldeletionsintheﬁ-nalstepofpermutationdecoding.Theparameterθandanadditionalworddeletionpenaltycanthenbeusedtoadjustthelengthoftranslatedoutputs.InSection6,wediscussseveralissuesregardingthismodel.4ChoosingtheclassiﬁerThissectionaddressesthechoiceoftheclassiﬁ-cationtechnique,andarguesthatonetechniquethatyieldsexcellentperformancewhilescalingwellisbinarymaximumentropy(Maxent)withL1-regularization.4.1Multiclassvs.BinaryClassiﬁcationTheSequentialandBOWmodelsrepresenttwodif-ferentclassiﬁcationproblems.Inthesequentialmodel,wehaveamulticlassproblemwhereeachclasstiisexclusive,therefore,alltheclassiﬁerout-putsP(ti|Φ)mustbejointlyoptimizedsuchthat156

Table1:Acomparisonofthesequentialandbag-of-wordslexicalchoicemodelsSequentialLexicalModelBag-of-WordsLexicalModelOutputtargetTargetwordforeachsourcepositioniTargetwordgivenasourcesentenceInputfeaturesBOgram(S,i−d,i+d):bagofn-gramsBOgram(S,0,|S|):bagofn-gramsinsourcesentenceintheinterval[i−d,i+d]insourcesentenceProbabilitiesP(ti|BOgram(S,i−d,i+d))P(BOW(T)|BOgram(S,0,|S|))IndependenceassumptionbetweenthelabelsNumberofclassesOnepertargetwordorphraseTrainingsamplesOnepersourcetokenOnepersentencePreprocessingSource/TargetwordalignmentSource/TargetsentencealignmentPiP(ti|Φ)=1.Thiscanbeproblematic:withoneclassiﬁerperwordinthevocabulary,evenallo-catingthememoryduringtrainingmayexceedthememorycapacityofcurrentcomputers.IntheBOWmodel,eachclasscanbedetectedindependently,andtwodifferentclassescanbede-tectedatthesametime.Thisisknownasthe1-vs-otherscheme.Thekeyadvantageoverthemulticlassschemeisthatnotallclassiﬁershavetoresideinmemoryatthesametimeduringtrainingwhichal-lowsforparallelization.Fortunatelyforthesequen-tialmodel,wecandecomposeamulticlassclassiﬁ-cationproblemintoseparate1-vs-otherproblems.Intheory,onehastomakeanadditionalindependenceassumptionandtheproblemstatementbecomesdif-ferent.Eachoutputlabeltisprojectedintoabitstringwithcomponentsbj(t)whereprobabilityofeachcomponentisestimatedindependently:P(bj(t)|Φ)=1−P(¯bj(t)|Φ)=11+e−(λj−λ¯j)·ΦInpractice,despitetheapproximation,the1-vs-otherschemehasbeenshowntoperformaswellasthemulticlassscheme(RifkinandKlautau,2004).Asaconsequence,weusethesametypeofbinaryclassiﬁerforthesequentialandtheBOWmodels.TheexcellentresultsrecentlyobtainedwiththeSEARNalgorithm(Daumeetal.,2007)alsosug-gestthatbinaryclassiﬁers,whenproperlytrainedandcombined,seemtobecapableofmatchingmorecomplexstructuredoutputapproaches.4.2Geometricvs.ProbabilisticInterpretationWeseparatethemostpopularclassiﬁcationtech-niquesintotwobroadcategories:•Geometricapproachesmaximizethewidthofaseparationmarginbetweentheclasses.ThemostpopularmethodistheSupportVectorMa-chine(SVM)(Vapnik,1998).•Probabilisticapproachesmaximizethecon-ditionallikelihoodoftheoutputclassgiventheinputfeatures.ThislogisticregressionisalsocalledMaxentasitﬁndsthedistributionwithmaximumentropythatproperlyestimatestheaverageofeachfeatureoverthetrainingdata(Bergeretal.,1996).Inpreviousstudies,wefoundthatthebestaccuracyisachievedwithnon-linear(orkernel)SVMs,attheexpenseofahightesttimecomplexity,whichisun-acceptableformachinetranslation.LinearSVMsandregularizedMaxentyieldsimilarperformance.Intheory,Maxenttraining,whichscaleslinearlywiththenumberofexamples,isfasterthanSVMtraining,whichscalesquadraticallywiththenum-berofexamples.Inourﬁrstexperimentswithlexi-calchoicemodels,weobservedthatMaxentslightlyoutperformedSVMs.UsingasinglethresholdwithSVMs,someclassesofwordswereover-detected.Thissuggeststhat,astheorypredicts,SVMsdonotproperlyapproximatetheposteriorprobability.WethereforechosetouseMaxentasthebestprobabilityapproximator.4.3L1vs.L2regularizationTraditionally,MaxentisregularizedbyimposingaGaussianprioroneachweight:thisL2regulariza-tionﬁndsthesolutionwiththesmallestpossibleweights.However,ontaskslikemachinetranslationwithaverylargenumberofinputfeatures,aLapla-cianL1regularizationthatalsoattemptstomaxi-mizethenumberofzeroweightsishighlydesirable.AnewL1-regularizedMaxentalgorithmswasproposedfordensityestimation(Dudiketal.,2004)andweadaptedittoclassiﬁcation.Wefoundthisal-gorithmtoconvergefasterthanthecurrentstate-of-the-artinMaxenttraining,whichisL2-regularizedL-BFGS(Malouf,2002)1.Moreover,thenumberoftrainedparametersisconsiderablysmaller.5DataandExperimentsWehaveperformedexperimentsontheIWSLT06Chinese-Englishtraininganddevelopmentsetsfrom1Weusedtheimplementationavailableathttp://homepages.inf.ed.ac.uk/s0450736/maxenttoolkit.html157

Table2:Statisticsoftraininganddevelopmentdatafrom2005/2006(∗=ﬁrstofmultipletranslationsonly).Training(2005)Dev2005Dev2006ChineseEnglishChineseEnglishChineseEnglishSentences46,311506489RunningWords351,060376,6153,8263,8975,2146,362∗Vocabulary11,17811,2329318981,1361,134∗Singletons4,3484,866600538619574∗OOVs[%]--0.60.30.91.0ASRWER[%]----25.2-Perplexity--33-86-#References--1672005and2006.Thedataaretravelertaskex-pressionssuchasseekingdirections,expressionsinrestaurantsandtravelreservations.Table2presentssomestatisticsonthedatasets.Itmustbenotedthatwhilethe2005developmentsetmatchesthetrainingdataclosely,the2006developmentsethasbeencollectedseparatelyandshowsslightlydiffer-entstatisticsforaveragesentencelength,vocabularysizeandout-of-vocabularywords.Alsothe2006developmentsetcontainsnopunctuationmarksinChinese,butthecorrespondingEnglishtranslationshavepunctuationmarks.WealsoevaluatedourmodelsontheChinesespeechrecognitionoutputandwereportresultsusing1-bestwithaworder-rorrateof25.2%.Fortheexperiments,wetokenizedtheChinesesentencesintocharacterstringsandtrainedthemod-elsdiscussedintheprevioussections.Also,wetrainedapunctuationpredictionmodelusingMax-entframeworkontheChinesecharacterstringsinordertoinsertpunctuationmarksintothe2006de-velopmentdataset.Theresultingcharacterstringwithpunctuationmarksisusedasinputtothetrans-lationdecoder.Forthe2005developmentset,punc-tuationinsertionwasnotneededsincetheChinesesentencesalreadyhadthetruepunctuationmarks.InTable3wepresenttheresultsofthethreedif-ferenttranslationmodels–FST,SequentialMaxentandBOWMaxent.Thereareafewinterestingob-servationsthatcanbemadebasedontheseresults.First,onthe2005developmentset,thesequentialMaxentmodeloutperformstheFSTmodel,eventhoughthetwomodelsweretrainedstartingfromthesameGIZA++alignment.Thedifference,how-ever,isduetothefactthatMaxentmodelscancopewithincreasedlexicalcontext2andtheparametersofthemodelarediscriminativelytrained.ThemoresurprisingresultisthattheBOWMaxentmodelsig-niﬁcantlyoutperformsthesequentialMaxentmodel.2Weuse6wordstotheleftandrightofasourcewordforsequentialMaxent,butonly2precedingsourceandtargetwordsforFSTapproach.ThereasonisthatthesequentialMaxentmodelre-liesonthewordalignment,which,iferroneous,re-sultsinincorrectpredictionsbythesequentialMax-entmodel.TheBOWmodeldoesnotrelyontheword-levelalignmentandcanbeinterpretedasadis-criminativelytrainedmodelofdictionarylookupforatargetwordinthecontextofasourcesentence.Table3:Results(mBLEU)scoresforthethreedif-ferentmodelsonthetranscriptionsfordevelopmentset2005and2006andASR1-bestfordevelopmentset2006.Dev2005Dev2006TextTextASR1-bestFST51.819.516.5Seq.Maxent53.519.416.3BOWMaxent59.919.316.6Asindicatedinthedatareleasedocument,the2006developmentsetwascollecteddifferentlycom-paredtotheonefrom2005.Duetothismis-match,theperformanceoftheMaxentmodelsarenotverydifferentfromtheFSTmodel,indicatingthelackofgoodgeneralizationacrossdifferentgen-res.However,webelievethattheMaxentframe-workallowsforincorporationoflinguisticfeaturesthatcouldpotentiallyhelpingeneralizationacrossgenres.FortranslationofASR1-best,weseeasys-tematicdegradationofabout3%inmBLEUscorecomparedtotranslatingthetranscription.Inordertocompensateforthemismatchbetweenthe2005and2006datasets,wecomputeda10-foldaveragemBLEUscorebyincluding90%ofthe2006developmentsetintothetrainingsetandusing10%ofthe2006developmentsetfortesting,eachtime.TheaveragemBLEUscoreacrossthese10runsin-creasedto22.8.InFigure6weshowtheimprovementofmBLEUscoreswiththeincreaseinpermutationwindowsize.Wehadtolimittoapermutationwindowsizeof10duetomemorylimitations,eventhoughthecurvehasnotplateaued.Weanticipateusingpruningtech-niqueswecanincreasethewindowsizefurther.158

 0.46 0.48 0.5 0.52 0.54 0.56 0.58 0.6 6 6.5 7 7.5 8 8.5 9 9.5 10Permutation Window SizeFigure6:ImprovementinmBLEUscorewiththeincreaseinsizeofthepermutationwindow5.1UnitedNationsandHansardCorporaInordertotestthescalabilityofthegloballexicalselectionapproach,wealsoperformedlexicalse-lectionexperimentsontheUnitedNations(Arabic-English)corpusandtheHansard(French-English)corpususingtheSFSTmodelandtheBOWMaxentmodel.Weused1,000,000trainingsentencepairsandtestedon994testsentencesfortheUNcorpus.FortheHansardcorpusweusedthesametrainingandtestsplitasin(ZensandNey,2004):1.4milliontrainingsentencepairsand5432testsentences.ThevocabularysizesforthetwocorporaarementionedinTable4.AlsoinTable4,aretheresultsintermsofF-measurebetweenthewordsinthereferencesen-tenceandthedecodedsentences.WecanseethattheBOWmodeloutperformstheSFSTmodelonbothcorporasigniﬁcantly.Thisisduetoasystematic10%relativeimprovementforopenclasswords,astheybeneﬁtfromamuchwidercontext.BOWper-formanceoncloseclasswordsishigherfortheUNcorpusbutlowerfortheHansardcorpus.Table4:LexicalSelectionresults(F-measure)ontheArabic-EnglishUNCorpusandtheFrench-EnglishHansardCorpus.InparenthesisareF-measuresforopenandclosedclasslexicalitems.CorpusVocabularySFSTBOWSourceTargetUN252,57153,00564.669.5(60.5/69.1)(66.2/72.6)Hansard100,27078,33357.460.8(50.6/67.7)(56.5/63.4)6DiscussionTheBOWapproachispromisingasitperformsrea-sonablywelldespiteconsiderablelossesinthetrans-ferofinformationbetweensourceandtargetlan-guage.Theﬁrstandmostobviouslossisaboutwordposition.Theonlyinformationwecurrentlyusetorestorethetargetwordpositionisthetargetlanguagemodel.Informationaboutthegrammaticalroleofawordinthesourcesentenceiscompletelylost.Thelanguagemodelmightfortuitouslyrecoverthisin-formationifthesentencewiththecorrectgrammat-icalroleforthewordhappenstobethemaximumlikelihoodsentenceinthepermutationautomaton.Wearecurrentlyworkingtowardincorporatingsyntacticinformationonthetargetwordssoastobeabletorecoversomeofthegrammaticalroleinfor-mationlostintheclassiﬁcationprocess.Inprelimi-naryexperiments,wehaveassociatedthetargetlex-icalitemswithsupertaginformation(BangaloreandJoshi,1999).Supertagsarelabelsthatprovidelinearorderingconstraintsaswellasgrammaticalrelationinformation.Althoughassociatingsupertagstotar-getwordsincreasestheclasssetfortheclassiﬁer,wehavenoticedthatthedegradationintheF-scoreisontheorderof3%acrossdifferentcorpora.Thesu-pertaginformationcanthenbeexploitedinthesen-tenceconstructionprocess.Theuseofsupertagsinphrase-basedSMTsystemhasbeenshowntoim-proveresults(Hassanetal.,2006).Alessobviouslossisthenumberoftimesawordorconceptappearsinthetargetsentence.Func-tionwordslike”the”and”of”canappearmanytimesinanEnglishsentence.Inthemodeldis-cussedinthispaper,weindexeachoccurrenceofthefunctionwordwithacounter.Inordertoimprovethismethod,wearecurrentlyexploringatechniquewherethefunctionwordsserveasattributes(e.g.deﬁniteness,tense,case)onthecontentfullexicalitems,thusenrichingthelexicalitemwithmorpho-syntacticinformation.AthirdissueconcerningtheBOWmodelistheproblemofsynonyms–targetwordswhichtranslatethesamesourceword.Supposethatinthetrainingdata,targetwordst1andt2are,withequalprobabil-ity,translationsofthesamesourceword.Then,inthepresenceofthissourceword,theprobabilitytodetectthecorrespondingtargetword,whichweas-sumeis0.8,willbe,becauseofdiscriminantlearn-ing,splitequallybetweent1andt2,thatis0.4and0.4.Becauseofthissynonymproblem,theBOWthresholdθhastobesetlowerthan0.5,whichisobservedexperimentally.However,ifwesetthethresholdto0.3,botht1andt2willbedetectedinthetargetsentence,andwefoundthistobeamajorsourceofundesirableinsertions.TheBOWapproachisdifferentfromthepars-ingbasedapproaches(Melamed,2004;ZhangandGildea,2005;Cowanetal.,2006)wherethetransla-tionmodeltightlycouplesthesyntacticandlexicalitemsofthetwolanguages.Thedecouplingofthe159

twostepsinourmodelhasthepotentialforgener-atingparaphrasedsentencesnotnecessarilyisomor-phictothestructureofthesourcesentence.7ConclusionsWeviewmachinetranslationasconsistingoflexi-calselectionandlexicalreorderingsteps.Thesetwostepsneednotnecessarilybesequentialandcouldbetightlyintegrated.Wehavepresentedtheweightedﬁnite-statetransducermodelofmachinetranslationwherelexicalchoiceandalimitedamountoflexicalreorderingaretightlyintegratedintoasingletrans-duction.Wehavealsopresentedanovelapproachtotranslationwherethesetwostepsarelooselycou-pledandtheparametersofthelexicalchoicemodelarediscriminativelytrainedusingamaximumen-tropymodel.Thelexicalreorderingmodelinthisapproachisachievedusingapermutationautoma-ton.Wehaveevaluatedthesetwoapproachesonthe2005and2006IWSLTdevelopmentsetsandshownthatthetechniquesscalewelltoHansardandUNcorpora.ReferencesH.Alshawi,S.Bangalore,andS.Douglas.1998.Automaticacquisitionofhierarchicaltransductionmodelsformachinetranslation.InACL,Montreal,Canada.R.E.Banchs,J.M.Crego,A.Gispert,P.Lambert,andJ.B.Marino.2005.Statisticalmachinetranslationofeuparldatabyusingbilingualn-grams.InWorkshoponBuildingandUsingParallelTexts.ACL.S.BangaloreandA.K.Joshi.1999.Supertagging:Anap-proachtoalmostparsing.ComputationalLinguistics,25(2).S.BangaloreandG.Riccardi.2000.Stochasticﬁnite-statemodelsforspokenlanguagemachinetranslation.InPro-ceedingsoftheWorkshoponEmbeddedMachineTransla-tionSystems,pages52–59.A.L.Berger,StephenA.D.Pietra,D.Pietra,andJ.Vincent.1996.AMaximumEntropyApproachtoNaturalLanguageProcessing.ComputationalLinguistics,22(1):39–71.P.Brown,S.D.Pietra,V.D.Pietra,andR.Mercer.1993.TheMathematicsofMachineTranslation:ParameterEstimation.ComputationalLinguistics,16(2):263–312.D.Chiang.2005.Ahierarchicalphrase-basedmodelforstatis-ticalmachinetranslation.InProceedingsoftheACLCon-ference,AnnArbor,MI.B.Cowan,I.Kucerova,andM.Collins.2006.Adiscrimi-nativemodelfortree-to-treetranslation.InProceedingsofEMNLP.H.Daume,J.Langford,andD.Marcu.2007.Search-basedstructureprediction.submittedtoMachineLearningJour-nal.M.Dudik,S.Phillips,andR.E.Schapire.2004.Perfor-manceGuaranteesforRegularizedMaximumEntropyDen-sityEstimation.InProceedingsofCOLT’04,Banff,Canada.SpringerVerlag.V.Gofﬁn,C.Allauzen,E.Bocchieri,D.Hakkani-Tur,A.Ljolje,S.Parthasarathy,M.Rahim,G.Riccardi,andM.Saraclar.2005.TheAT&TWATSONSpeechRecognizer.InPro-ceedingsofICASSP,Philadelphia,PA.H.Hassan,M.Hearne,K.Sima’an,andA.Way.2006.Syntac-ticphrase-basedstatisticalmachinetranslation.InProceed-ingsofIEEE/ACLﬁrstInternationalWorkshoponSpokenLanguageTechnology(SLT),Aruba,December.S.Kanthak,D.Vilar,E.Matusov,R.Zens,andH.Ney.2005.Novelreorderingapproachesinphrase-basedstatisticalma-chinetranslation.InProceedingsoftheACLWorkshoponBuildingandUsingParallelTexts,pages167–174,AnnAr-bor,Michigan.J.Lafferty,A.McCallum,andF.Pereira.2001.Conditionalrandomﬁelds:Probabilisticmodelsforsegmentingandla-belingsequencedata.InProceedingsofICML,SanFran-cisco,CA.R.Malouf.2002.Acomparisonofalgorithmsformaximumentropyparameterestimation.InProceedingsofCoNLL-2002,pages49–55.Taipei,Taiwan.I.D.Melamed.2004.Statisticalmachinetranslationbypars-ing.InProceedingsofACL.F.J.OchandH.Ney.2002.Discriminativetrainingandmax-imumentropymodelsforstatisticalmachinetranslation.InProceedingsofACL.F.J.OchandH.Ney.2003.Asystematiccomparisonofvari-ousstatisticalalignmentmodels.ComputationalLinguistics,29(1):19–51.RyanRifkinandAldebaroKlautau.2004.Indefenseofone-vs-allclassiﬁcation.JournalofMachineLearningResearch,pages101–141.C.TillmannandT.Zhang.2006.Adiscriminativeglobaltrain-ingalgorithmforstatisticalmt.InCOLING-ACL.V.N.Vapnik.1998.StatisticalLearningTheory.JohnWiley&Sons.B.Wellington,J.Turian,C.Pike,andD.Melamed.2006.Scal-ablepurely-discriminativetrainingforwordandtreetrans-ducers.InAMTA.D.Wu.1997.StochasticInversionTransductionGrammarsandBilingualParsingofParallelCorpora.ComputationalLinguistics,23(3):377–404.K.YamadaandK.Knight.2001.Asyntax-basedstatisticaltranslationmodel.InProceedingsof39thACL.R.ZensandH.Ney.2004.Improvementsinphrase-basedsta-tisticalmachinetranslation.InProceedingsofHLT-NAACL,pages257–264,Boston,MA.H.ZhangandD.Gildea.2005.Stochasticlexicalizedinver-siontransductiongrammarforalignment.InProceedingsofACL.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 160–167,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

160

MildlyContext-SensitiveDependencyLanguagesMarcoKuhlmannProgrammingSystemsLabSaarlandUniversitySaarbrücken,Germanykuhlmann@ps.uni-sb.deMathiasMöhlProgrammingSystemsLabSaarlandUniversitySaarbrücken,Germanymmohl@ps.uni-sb.deAbstractDependency-basedrepresentationsofnatu-rallanguagesyntaxrequireaﬁnebalancebetweenstructuralﬂexibilityandcomputa-tionalcomplexity.Inpreviouswork,severalconstraintshavebeenproposedtoidentifyclassesofdependencystructuresthatarewell-balancedinthissense;thebest-knownbutalsomostrestrictiveoftheseisprojectivity.Mostconstraintsareformulatedonfullyspec-iﬁedstructures,whichmakesthemhardtoin-tegrateintomodelswherestructuresarecom-posedfromlexicalinformation.Inthispaper,weshowhowtwoempiricallyrelevantrelax-ationsofprojectivitycanbelexicalized,andhowcombiningtheresultinglexiconswitharegularmeansofsyntacticcompositiongivesrisetoahierarchyofmildlycontext-sensitivedependencylanguages.1IntroductionSyntacticrepresentationsbasedonword-to-wordde-pendencieshavealongtraditionindescriptivelin-guistics.Lately,theyhavealsobeenusedinmanycomputationaltasks,suchasrelationextraction(Cu-lottaandSorensen,2004),parsing(McDonaldetal.,2005),andmachinetranslation(Quirketal.,2005).Especiallyinrecentworkonparsing,thereisapar-ticularinterestinnon-projectivedependencystruc-tures,inwhichawordanditsdependentsmaybespreadoutoveradiscontinuousregionofthesen-tence.Thesestructuresnaturallyariseinthesyntacticanalysisoflanguageswithﬂexiblewordorder,suchasCzech(Veseláetal.,2004).Unfortunately,mostformalresultsonnon-projectivityarediscouraging:Whilegrammar-drivendependencyparsersthatarerestrictedtoprojectivestructurescanbeasefﬁcientasparsersforlexicalizedcontext-freegrammar(Eis-nerandSatta,1999),parsingisprohibitivelyexpen-sivewhenunrestrictedformsofnon-projectivityarepermitted(NeuhausandBröker,1997).Data-drivendependencyparsingwithnon-projectivestructuresisquadraticwhenallattachmentdecisionsareassumedtobeindependentofoneanother(McDonaldetal.,2005),butbecomesintractablewhenthisassumptionisabandoned(McDonaldandPereira,2006).Insearchofabalancebetweenstructuralﬂexibilityandcomputationalcomplexity,severalauthorshaveproposedconstraintstoidentifyclassesofnon-projec-tivedependencystructuresthatarecomputationallywell-behaved(Bodirskyetal.,2005;Nivre,2006).Inthispaper,wefocusontwooftheseproposals:thegap-degreerestriction,whichputsaboundonthenumberofdiscontinuitiesintheregionofasen-tencecoveredbyawordanditsdependents,andthewell-nestednesscondition,whichconstrainsthear-rangementofdependencysubtrees.Bothconstraintshavebeenshowntobeinverygoodﬁtwithdatafromdependencytreebanks(KuhlmannandNivre,2006).However,likeallothersuchproposals,theyarefor-mulatedonfullyspeciﬁedstructures,whichmakesithardtointegratethemintoagenerativemodel,wheredependencystructuresarecomposedfromelemen-taryunitsoflexicalizedinformation.Consequently,littleisknownaboutthegenerativecapacityandcom-putationalcomplexityoflanguagesoverrestrictednon-projectivedependencystructures.161

ContentsofthepaperInthispaper,weshowhowthegap-degreerestrictionandthewell-nestednessconditioncanbecapturedindependencylexicons,andhowcombiningsuchlexiconswitharegularmeansofsyntacticcompositiongivesrisetoaninﬁ-nitehierarchyofmildlycontext-sensitivelanguages.Thetechnicalkeytotheseresultsisaproceduretoencodearbitrary,evennon-projectivedependencystructuresintotrees(terms)overasignatureoflocalorder-annotations.Theconstructorsofthesetreescanbereadaslexicalentries,andboththegap-de-greerestrictionandthewell-nestednessconditioncanbecouchedassyntacticpropertiesoftheseen-tries.Setsofgap-restricteddependencystructurescanbedescribedusingregulartreegrammars.Thisgivesrisetoanotionofregulardependencylan-guages,andallowsustoestablishaformalrelationbetweenthestructuralconstraintsandmildlycon-text-sensitivegrammarformalisms(Joshi,1985):WeshowthatregulardependencylanguagescorrespondtothesetsofderivationsoflexicalizedLinearCon-text-FreeRewritingSystems(lcfrs)(Vijay-Shankeretal.,1987),andthatthegap-degreemeasureisthestructuralcorrespondentoftheconceptof‘fan-out’inthisformalism(Satta,1992).Wealsoshowthataddingthewell-nestednessconditioncorrespondstotherestrictionoflcfrstoCoupledContext-FreeGrammars(HotzandPitsch,1996),andthatregu-larsetsofwell-nestedstructureswithagap-degreeofatmost1areexactlytheclassofsetsofderiva-tionsofLexicalizedTreeAdjoiningGrammar(ltag).Thisresultgeneralizespreviousworkontherelationbetweenltaganddependencyrepresentations(Ram-bowandJoshi,1997;Bodirskyetal.,2005).StructureofthepaperTheremainderofthispa-perisstructuredasfollows.Section2containssomebasicnotionsrelatedtotreesanddependencystruc-tures.InSection3wepresenttheencodingofdepen-dencystructuresasorder-annotatedtrees,andshowhowthisencodingallowsustogivealexicalizedre-formulationofboththegap-degreerestrictionandthewell-nestednesscondition.Section4introducesthenotionofregulardependencylanguages.InSection5weshowhowdifferentcombinationsofrestrictionsonnon-projectivityintheselanguagescorrespondtodifferentmildlycontext-sensitivegrammarfor-malisms.Section6concludesthepaper.2PreliminariesThroughoutthepaper,wewriteŒnforthesetofallpositivenaturalnumbersuptoandincludingn.ThesetofallstringsoverasetAisdenotedbyA(cid:3),theemptystringisdenotedby",andtheconcatenationoftwostringsxandyisdenotedeitherbyxy,or,wherethisisambiguous,byx(cid:1)y.2.1TreesInthispaper,weregardtreesasterms.Weexpectthereadertobefamiliarwiththebasicconceptsrelatedtothisframework,andonlyintroduceourparticularnotation.Let˙beasetoflabels.Thesetof(ﬁnite,unranked)treesover˙isdeﬁnedrecursivelybytheequationT˙´f(cid:27).x/j(cid:27)2˙;x2T(cid:3)˙g.Thesetofnodesofatreet2T˙isdeﬁnedasN.(cid:27).t1(cid:1)(cid:1)(cid:1)tn//´f"g[fiuji2Œn;u2N.ti/g:Fortwonodesu;v2N.t/,wesaythatugovernsv,andwriteuEv,ifvcanbewrittenasvDux,forsomesequencex2N(cid:3).Notethatthegovernancerelationisbothreﬂexiveandtransitive.Theconverseofgovernmentiscalleddependency,souEvcanalsobereadas‘vdependsonu’.Theyieldofanodeu2N.t/,buc,isthesetofalldependentsofuint:buc´fv2N.t/juEvg.Wealsousethenotationst.u/forthelabelatthenodeuoft,andt=uforthesubtreeoftrootedatu.Atreelanguageover˙isasubsetofT˙.2.2DependencystructuresForthepurposesofthispaper,adependencystructureover˙isapairdD.t;x/,wheret2T˙isatree,andxisalistofthenodesint.WewriteD˙torefertothesetofalldependencystructuresover˙.Independentlyofthegovernancerelationind,thelistxdeﬁnesatotalorderonthenodesint;wewriteu(cid:22)vtodenotethatuprecedesvinthisorder.Notethat,likegovernance,theprecedencerelationisbothreﬂexiveandtransitive.Adependencylanguageover˙isasubsetofD˙.Example.ThelefthalfofFigure1showshowwevisualizedependencystructures:circlesrepresentnodes,arrowsrepresenttherelationof(immediate)governance,theleft-to-rightorderofthenodesrepre-sentstheirorderintheprecedencerelation,andthedottedlinesindicatethelabelling.(cid:3)162

abcdef21111hf;0ihe;01iha;012ihc;0ihd;10ihb;01iFigure1:Aprojectivedependencystructure3LexicalizingtheprecedencerelationInthissection,weshowhowtheprecedencerelationofdependencystructurescanbeencodedas,anddecodedfrom,acollectionofnode-speciﬁcorderannotations.Undertheassumptionthatthenodesofadependencystructurecorrespondtolexemicunits,thisresultdemonstrateshowword-orderinformationcanbecapturedinadependencylexicon.3.1ProjectivestructuresLexicalizingtheprecedencerelationofadependencystructureisparticularlyeasyifthestructureunderconsiderationmeetstheconditionofprojectivity.Adependencystructureisprojective,ifeachofitsyieldsformsanintervalwithrespecttotheprece-denceorder(KuhlmannandNivre,2006).Inaprojectivestructure,theintervalthatcorre-spondstoayieldbucdecomposesintothesingletonintervalŒu;u,andthecollectionoftheintervalsthatcorrespondtotheyieldsoftheimmediatedependentsofu.Toreconstructtheglobalprecedencerelation,itsufﬁcestoannotateeachnodeuwiththerelativeprecedencesamongtheconstituentpartsofitsyield.Werepresentthis‘local’orderasastringoverthealphabetN0,wherethesymbol0representsthesin-gletonintervalŒu;u,andasymboli¤0representstheintervalthatcorrespondstotheyieldoftheithdirectdependentofu.Anorder-annotatedtreeisatreelabelledwithpairsh(cid:27);!i,where(cid:27)isthelabelproper,and!isalocalorderannotation.Inwhatfollows,wewillusethefunctionalnotations(cid:27).u/and!.u/torefertothelabelandorderannotationofu,respectively.Example.Figure1showsaprojectivedependencystructuretogetherwithitsrepresentationasanorder-annotatedtree.(cid:3)Wenowpresentproceduresforencodingprojec-tivedependencystructuresintoorder-annotatedtrees,andforreversingthisencoding.EncodingTherepresentationofaprojectivedepen-dencystructure.t;x/asanorder-annotatedtreecanbecomputedinasingleleft-to-rightsweepoverx.Startingwithacopyofthetreetinwhicheverynodeisannotatedwiththeemptystring,foreachnewnodeuinx,weupdatetheorderannotationofuthroughtheassignment!.u/´!.u/(cid:1)0.IfuDviforsomei2N(thatis,ifuisaninnernode),wealsoupdatetheorderannotationoftheparentvofuthroughtheassignment!.v/´!.v/(cid:1)i.DecodingTodecodeanorder-annotatedtreet,weﬁrstlinearizethenodesoftintoasequencex,andthenremoveallorderannotations.Linearizationpro-ceedsinawaythatisveryclosetoapre-ordertraver-salofthetree,exceptthattherelativepositionoftherootnodeofasubtreeisexplicitlyspeciﬁedintheorderannotation.Speciﬁcally,tolinearizeanor-der-annotatedtree,welookintothelocalorder!.u/annotatedattherootnodeofthetree,andconcatenatethelinearizationsofitsconstituentparts.Asymboliin!.u/representseitherthesingletonintervalŒu;u(iD0),ortheintervalcorrespondingtosomedirectdependentuiofu(i¤0),inwhichcasewepro-ceedrecursively.Formally,thelinearizationofuiscapturedbythefollowingthreeequations:lin.u/Dlin0.u;!.u//lin0.u;i1(cid:1)(cid:1)(cid:1)in/Dlin00.u;i1/(cid:1)(cid:1)(cid:1)lin00.u;in/lin00.u;i/DifiD0thenuelselin.ui/Bothencodinganddecodingcanbedoneintimelinearinthenumberofnodesofthedependencystructureororder-annotatedtree.3.2Non-projectivestructuresItisstraightforwardtoseethatourrepresentationofdependencystructuresisinsufﬁcientifthestructuresunderconsiderationarenon-projective.Towitness,considerthestructureshowninFigure2.Encodingthisstructureusingtheprocedurepresentedaboveyieldsthesameorder-annotatedtreeastheoneshowninFigure1,whichdemonstratesthattheencodingisnotreversible.163

abcdef12111ha;h01212iihc;h0iihe;h0;1iihf;h0iihb;h01;1iihd;h1;0iiFigure2:Anon-projectivedependencystructureBlocksInanon-projectivedependencystructure,theyieldofanodemaybespreadoutovermorethanoneinterval;wewillrefertotheseintervalsasblocks.Twonodesv;wbelongtothesameblockofanodeu,ifallnodesbetweenvandwaregovernedbyu.Example.Considerthenodesb;c;dinthestruc-turesdepictedinFigures1and2.InFigure1,thesenodesbelongtothesameblockofb.InFigure2,thethreenodesarespreadoutovertwoblocksofb(markedbytheboxes):canddareseparatedbyanode(e)notgovernedbyb.(cid:3)Blockshavearecursivestructurethatiscloselyre-latedtotherecursivestructureofyields:theblocksofanodeucanbedecomposedintothesingletonŒu;u,andtheblocksofthedirectdependentsofu.Justasaprojectivedependencystructurecanberepresentedbyannotatingeachyieldwithanorderonitscon-stituents,anunrestrictedstructurecanberepresentedbyannotatingeachblock.ExtendedorderannotationsTorepresentordersonblocks,weextendourannotationschemeasfol-lows.First,insteadofasinglestring,anannotation!.u/nowisatupleofstrings,wherethekthcom-ponentspeciﬁestheorderamongtheconstituentsofthekthblockofu.Second,insteadofone,thean-notationmaynowcontainmultipleoccurrencesofthesamedependent;thekthoccurrenceofiin!.u/representsthekthblockofthenodeui.Wewrite!.u/ktorefertothekthcomponentoftheorderannotationofu.Wealsousethenotation.i#k/utorefertothekthoccurrenceofiin!.u/,andomitthesubscriptwhenthenodeuisimplicit.Example.IntheannotatedtreeshowninFigure2,!.b/1D.0#1/.1#1/,and!.b/2D.1#2/.(cid:3)EncodingToencodeadependencystructure.t;x/asanextendedorder-annotatedtree,wedoapost-ordertraversaloftasfollows.Foragivennodeu,letusrepresentaconstituentofablockofuasatripleiWŒvl;vr,whereidenotesthenodethatcontributestheconstituent,andvlandvrdenotetheconstituent’sleftmostandrightmostelements.Ateachnodeu,wehaveaccesstothesingletonblock0WŒu;u,andtheconstituentblocksoftheimmediatedependentsofu.WesaythattwoblocksiWŒvl;vr;jWŒwl;wrcanbemerged,ifthenodevrimmediatelyprecedesthenodewl.TheresultofthemergerisanewblockijWŒvl;wrthatrepresentstheinformationthatthetwomergedconstituentsbelongtothesameblockofu.Byexhaustivemerging,weobtaintheconstituentstructureofallblocksofu.Fromthisstructure,wecanreadofftheorderannotation!.u/.Example.TheyieldofthenodebinFigure2de-composesinto0WŒb;b,1WŒc;c,and1WŒd;d.Sincebandcareadjacent,theﬁrsttwoofthesecon-stituentscanbemergedintoanewblock01WŒb;c;thethirdconstituentremainsunchanged.Thisgivesrisetotheorderannotationh01;1iforb.(cid:3)Whenusingaglobaldata-structuretokeeptrackoftheconstituentblocks,theencodingprocedurecanbeimplementedtorunintimelinearinthenumberofblocksinthedependencystructure.Inparticular,forprojectivedependencystructures,itstillrunsintimelinearinthenumberofnodes.DecodingTolinearizethekthblockofanodeu,welookintothekthcomponentoftheorderanno-tatedatu,andconcatenatethelinearizationsofitsconstituentparts.Eachoccurrence.i#k/inacom-ponentof!.u/representseitherthenodeuitself(iD0),orthekthblockofsomedirectdependentuiofu(i¤0),inwhichcaseweproceedrecursively:lin.u;k/Dlin0.u;!.u/k/lin0.u;i1(cid:1)(cid:1)(cid:1)in/Dlin00.u;i1/(cid:1)(cid:1)(cid:1)lin00.u;in/lin00.u;.i#k/u/DifiD0thenuelselin.ui;k/Therootnodeofadependencystructurehasonlyoneblock.Therefore,tolinearizeatreet,weonlyneedtolinearizetheﬁrstblockofthetree’srootnode:lin.t/Dlin.";1/.164

ConsistentorderannotationsEverydependencystructureover˙canbeencodedasatreeovertheset˙(cid:2)˝,where˝isthesetofallorderannotations.Theconverseofthisstatementdoesnothold:tobeinterpretableasadependencystructure,treestructureandorderannotationinanorder-annotatedtreemustbeconsistent,inthefollowingsense.PropertyC1:Everyannotation!.u/inatreetcontainsallandonlythesymbolsinthecollectionf0g[fijui2N.t/g,i.e.,onesymbolforu,andonesymbolforeverydirectdependentofu.PropertyC2:Thenumberofoccurrencesofasymboli¤0in!.u/isidenticaltothenumberofcomponentsintheannotationofthenodeui.Further-more,thenumberofcomponentsintheannotationoftherootnodeis1.Withthisnotionofconsistency,wecanprovethefollowingtechnicalresultabouttherelationbetweendependencystructuresandannotatedtrees.Wewrite(cid:25)˙.s/forthetreeobtainedfromatrees2T˙(cid:2)˝byre-labellingeverynodeuwith(cid:27).u/.Proposition1.Foreverydependencystructure.t;x/over˙,thereexistsatreesover˙(cid:2)˝suchthat(cid:25)˙.s/Dtandlin.s/Dx.Conversely,foreveryconsistentlyorder-annotatedtrees2T˙(cid:2)˝,thereexistsauniquelydetermineddependencystruc-ture.t;x/withtheseproperties.(cid:3)3.3LocalversionsofstructuralconstraintsTheencodingofdependencystructuresasorder-an-notatedtreesallowsustoreformulatetwoconstraintsonnon-projectivityoriginallydeﬁnedonfullyspeci-ﬁeddependencystructures(Bodirskyetal.,2005)intermsofsyntacticpropertiesoftheorderannotationsthattheyinduce:Gap-degreeThegap-degreeofadependencystructureisthemaximumoverthenumberofdis-continuitiesinanyyieldofthatstructure.Example.ThestructuredepictedinFigure2hasgap-degree1:theyieldofbhasonediscontinuity,markedbythenodee,andthisisthemaximalnumberofdiscontinuitiesinanyyieldofthestructure.(cid:3)Sinceadiscontinuityinayieldisdelimitedbytwoblocks,andsincethenumberofblocksofanodeuequalsthenumberofcomponentsintheorderanno-tationofu,thefollowingresultisobvious:Proposition2.Adependencystructurehasgap-de-greekifandonlyifthemaximalnumberofcompo-nentsamongtheannotations!.u/iskC1.(cid:3)Inparticular,adependencystructureisprojectiveiffallofitsannotationsconsistofjustonecomponent.Well-nestednessThewell-nestednessconditionconstrainsthearrangementofsubtreesinadepen-dencystructure.Twosubtreest=u1;t=u2interleave,iftherearenodesv1l;v1r2t=u1andv2l;v2r2t=u2suchthatv1l(cid:30)v2l(cid:30)v1r(cid:30)v2r.Adependencystruc-tureiswell-nested,ifnotwoofitsdisjointsubtreesinterleave.Wecanprovethefollowingresult:Proposition3.Adependencystructureiswell-nestedifandonlyifnoannotation!.u/containsasubstringi(cid:1)(cid:1)(cid:1)j(cid:1)(cid:1)(cid:1)i(cid:1)(cid:1)(cid:1)j,fori;j2N.(cid:3)Example.ThedependencystructureinFigure1iswell-nested,thestructuredepictedinFigure2isnot:thesubtreesrootedatthenodesbandeinterleave.Toseethis,noticethatb(cid:30)e(cid:30)d(cid:30)f.Alsonoticethat!.a/containsthesubstring1212.(cid:3)4RegulardependencylanguagesTheencodingofdependencystructuresasorder-an-notatedtreesgivesrisetoanencodingofdependencylanguagesastreelanguages.Morespeciﬁcally,de-pendencylanguagesoveraset˙canbeencodedastreelanguagesovertheset˙(cid:2)˝,where˝isthesetofallorderannotations.Viathisencoding,wecanstudydependencylanguagesusingthetoolsandresultsofthewell-developedformaltheoryoftreelanguages.Inthissection,wediscussdepen-dencylanguagesthatcanbeencodedasregulartreelanguages.4.1RegulartreegrammarsTheclassofregulartreelanguages,REGTforshort,isaverynaturalclasswithmanycharacterizations(GécsegandSteinby,1997):itisgeneratedbyregulartreegrammars,recognizedbyﬁnitetreeautomata,andexpressibleinmonadicsecond-orderlogic.Hereweusethecharacterizationintermsofgrammars.Regulartreegrammarsarenaturalcandidatesfortheformalizationofdependencylexicons,aseachruleinsuchagrammarcanbeseenasthespeciﬁcationofawordandthesyntacticcategoriesorgrammaticalfunctionsofitsimmediatedependents.165

Formally,a(normalized)regulartreegrammarisaconstructGD.NG;˙G;SG;PG/,inwhichNGand˙Gareﬁnitesetsofnon-terminalandtermi-nalsymbols,respectively,SG2NGisadedicatedstartsymbol,andPGisaﬁnitesetofproductionsoftheformA!(cid:27).A1(cid:1)(cid:1)(cid:1)An/,where(cid:27)2˙G,A2NG,andAi2NG,foreveryi2Œn.The(di-rect)derivationrelationassociatedtoGisthebinaryrelation)GonthesetT˙G[NGdeﬁnedasfollows:t2T˙G[NGt=uDA.A!s/2PGt)GtŒu7!sInformally,eachstepinaderivationreplacesanon-terminal-labelledleafbytheright-handsideofamatchingproduction.ThetreelanguagegeneratedbyGisthesetofallterminaltreesthatcaneventu-allybederivedfromthetrivialtreeformedbyitsstartsymbol:L.G/Dft2T˙GjSG)(cid:3)Gtg.4.2RegulardependencygrammarsWecalladependencylanguageregular,ifitsencod-ingasasetoftreesover˙(cid:2)˝formsaregulartreelanguage,andwriteREGDfortheclassofallregulardependencylanguages.ForeveryregulardependencylanguageL,thereisaregulartreegrammarwithter-minalalphabet˙(cid:2)˝thatgeneratestheencodingofL.Similartothesituationwithindividualstruc-tures,theconverseofthisstatementdoesnothold:theconsistencypropertiesmentionedaboveimposecorrespondingsyntacticrestrictionsontherulesofgrammarsGthatgeneratetheencodingofL.PropertyC10:The!-componentofeverypro-ductionA!h(cid:27);!i.A1(cid:1)(cid:1)(cid:1)An/inGcontainsallandonlysymbolsinthesetf0g[fiji2Œng.PropertyC20:Foreverynon-terminalX2NG,thereisauniquelydeterminedintegerdXsuchthatforeveryproductionA!h(cid:27);!i.A1(cid:1)(cid:1)(cid:1)An/inG,dAigivesthenumberofoccurrencesofiin!,dAgivesthenumberofcomponentsin!,anddSGD1.Itturnsoutthatthesepropertiesareinfactsufﬁcienttocharacterizetheclassofregulartreegrammarsthatgenerateencodingsofdependencylanguages.Inbutslightabuseofterminology,wewillrefertosuchgrammarsasregulardependencygrammars.Example.Figure3showsaregulartreegrammarthatgeneratesasetofnon-projectivedependencystructureswithstringlanguagefanbnjn(cid:21)1g.(cid:3)abbbaaBBBSAAS!ha;h01ii.B/jha;h0121ii.A;B/A!ha;h0;1ii.B/jha;h01;21ii.A;B/B!hb;h0iiFigure3:AgrammarforalanguageinREGD.1/5StructuralconstraintsandformalpowerInthissection,wepresentourresultsonthegenera-tivecapacityofregulardependencylanguages,link-ingthemtoalargeclassofmildlycontext-sensitivegrammarformalisms.5.1Gap-restricteddependencylanguagesAdependencylanguageLiscalledgap-restricted,ifthereisaconstantcL(cid:21)0suchthatnostructureinLhasagap-degreehigherthancL.Itisplaintoseethateveryregulardependencylanguageisgap-restricted:thegap-degreeofastructureisdirectlyreﬂectedinthenumberofcomponentsofitsorderannotations,andeveryregulardependencygrammarmakesuseofonlyaﬁnitenumberoftheseannotations.WewriteREGD.k/torefertotheclassofregulardependencylanguageswithagap-degreeboundedbyk.LinearContext-FreeRewritingSystemsGap-re-stricteddependencylanguagesarecloselyrelatedtoLinearContext-FreeRewritingSystems(lcfrs)(Vijay-Shankeretal.,1987),aclassofformalsys-temsthatgeneralizesseveralmildlycontext-sensitivegrammarformalisms.AnlcfrsconsistsofaregulartreegrammarGandaninterpretationoftheterminalsymbolsofthisgrammaraslinear,non-erasingfunc-tionsintotuplesofstrings.Bythesefunctions,eachtreeinL.G/canbeevaluatedtoastring.Example.Hereisanexampleforafunction:f.hx11;x21i;hx12i/Dhax11;x12x21iThisfunctionstatesthatinordertocomputethepairofstringsthatcorrespondstoatreewhoserootnodeislabelledwiththesymbolf,oneﬁrsthastocom-putethepairofstringscorrespondingtotheﬁrstchild166

oftherootnode(hx11;x21i)andthesinglestringcor-respondingtothesecondchild(hx12i),andthencon-catenatetheindividualcomponentsinthespeciﬁedorder,precededbytheterminalsymbola.(cid:3)Wecallafunctionlexicalized,ifitcontributesex-actlyoneterminalsymbol.Inanlcfrsinwhichallfunctionsarelexicalized,thereisaone-to-onecor-respondencebetweenthenodesinanevaluatedtreeandthepositionsinthestringthatthetreeevaluatesto.Therefore,treeandstringimplicitlyformadepen-dencystructure,andwecanspeakofthedependencylanguagegeneratedbyalexicalizedlcfrs.EquivalenceWecanprovethateveryregularde-pendencygrammarcanbetransformedintoalexi-calizedlcfrsthatgeneratesthesamedependencylanguage,andviceversa.Thebasicinsightinthisproofisthateveryorderannotationinaregularde-pendencygrammarcanbeinterpretedasacompactdescriptionofafunctioninthecorrespondinglcfrs.Thenumberofcomponentsintheorder-annotation,andhence,thegap-degreeoftheresultingdepen-dencylanguage,correspondstothefan-outofthefunction:thehighestnumberofcomponentsamongtheargumentsofthefunction(Satta,1992).1Atech-nicaldifﬁcultyiscausedbythefactthatlcfrscanswapcomponents:f.hx11;x21i/Dhax21;x11i.Thiscommutativityneedstobecompiledoutduringthetranslationintoaregulardependencygrammar.WewriteLLCFRL.k/fortheclassofalldepen-dencylanguagesgeneratedbylexicalizedlcfrswithafan-outofatmostk.Proposition4.REGD.k/DLLCFRL.kC1/(cid:3)Inparticular,theclassREGD.0/ofregulardepen-dencylanguagesoverprojectivestructuresisexactlytheclassofdependencylanguagesgeneratedbylexi-calizedcontext-freegrammars.Example.Thegap-degreeofthelanguagegeneratedbythegrammarinFigure3isboundedby1.Therulesforthenon-terminalAcanbetranslatedintothefollowingfunctionsofanequivalentlcfrs:fha;h0;1ii.hx11i/Dha;x11ifha;h01;21ii.hx11;x21i;hx12i/Dhax11;x12x21iThefan-outofthesefunctionsis2.(cid:3)1Moreprecisely,gap-degreeDfan-out(cid:0)1.5.2Well-nesteddependencylanguagesTheabsenceofthesubstringi(cid:1)(cid:1)(cid:1)j(cid:1)(cid:1)(cid:1)i(cid:1)(cid:1)(cid:1)jintheorderannotationsofwell-nesteddependencystruc-turescorrespondstoarestrictionto‘well-bracketed’compositionsofsub-structures.ThisrestrictioniscentraltotheformalismofCoupled-Context-FreeGrammar(ccfg)(HotzandPitsch,1996).Itisstraightforwardtoseethateveryccfgcanbetranslatedintoanequivalentlcfrs.Wecanalsoprovethateverylcfrsobtainedfromaregulardepen-dencygrammarwithwell-nestedorderannotationscanbetranslatedbackintoanequivalentccfg.WewriteREGDwn.k/forthewell-nestedsubclassofREGD.k/,andLCCFL.k/fortheclassofalldepen-dencylanguagesgeneratedbylexicalizedccfgswithafan-outofatmostk.Proposition5.REGDwn.k/DLCCFL.kC1/(cid:3)Asaspecialcase,Coupled-Context-FreeGrammarswithfan-out2areequivalenttoTreeAdjoiningGram-mars(tags)(HotzandPitsch,1996).Thisenablesustogeneralizeapreviousresultontheclassofde-pendencystructuresgeneratedbylexicalizedtags(Bodirskyetal.,2005)totheclassofgeneratedde-pendencylanguages,LTAL.Proposition6.REGDwn.1/DLTAL(cid:3)6ConclusionInthispaper,wehavepresentedalexicalizedrefor-mulationoftwostructuralconstraintsonnon-pro-jectivedependencyrepresentations,andshownthatcombiningdependencylexiconsthatsatisfytheseconstraintswitharegularmeansofsyntacticcom-positionyieldsclassesofmildlycontext-sensitivedependencylanguages.Ourresultsmakeasignif-icantcontributiontoabetterunderstandingoftherelationbetweenthephenomenonofnon-projectivityandnotionsofformalpower.Thecloselinkbetweenrestrictedformsofnon-projectivedependencylanguagesandmildlycontext-sensitivegrammarformalismsprovidesapromisingstartingpointforfuturework.Onthepracticalside,itshouldallowustobeneﬁtfromtheexperienceinbuildingparsersformildlycontext-sensitivefor-malismswhenaddressingthetaskofefﬁcientnon-projectivedependencyparsing,atleastintheframe-167

workofgrammar-drivenparsing.Thismayeven-tuallyleadtoabettertrade-offbetweenstructuralﬂexibilityandcomputationalefﬁciencythanthatob-tainedwithcurrentsystems.Onamoretheoreticallevel,ourresultsprovideabasisforcomparingava-rietyofformallyratherdistinctgrammarformalismswithrespecttothesetsofdependencystructuresthattheycangenerate.Suchacomparisonmaybeempir-icallymoreadequatethanonebasedontraditionalnotionsofgenerativecapacity(Kallmeyer,2006).AcknowledgementsWethankGuidoTack,StefanThater,andtheanonymousreviewersofthispaperfortheirdetailedcomments.TheworkoftheauthorsisfundedbytheGermanResearchFoundation.ReferencesManuelBodirsky,MarcoKuhlmann,andMathiasMöhl.2005.Well-nesteddrawingsasmodelsofsyntacticstructure.InTenthConferenceonFormalGrammarandNinthMeetingonMathematicsofLanguage,Edin-burgh,Scotland,UK.AronCulottaandJeffreySorensen.2004.Dependencytreekernelsforrelationextraction.In42ndAnnualMeetingoftheAssociationforComputationalLinguis-tics(ACL),pages423–429,Barcelona,Spain.JasonEisnerandGiorgioSatta.1999.Efﬁcientparsingforbilexicalcontext-freegrammarsandheadautoma-tongrammars.In37thAnnualMeetingoftheAsso-ciationforComputationalLinguistics(ACL),pages457–464,CollegePark,Maryland,USA.FerencGécsegandMagnusSteinby.1997.Treelan-guages.InGrzegorzRozenbergandArtoSalomaa,editors,HandbookofFormalLanguages,volume3,pages1–68.Springer-Verlag,NewYork,USA.GünterHotzandGiselaPitsch.1996.Onparsingcoupled-context-freelanguages.TheoreticalComputerScience,161:205–233.AravindK.Joshi.1985.Treeadjoininggrammars:Howmuchcontext-sensitivityisrequiredtoprovidereason-ablestructuraldescriptions?InDavidR.Dowty,LauriKarttunen,andArnoldM.Zwicky,editors,NaturalLan-guageParsing,pages206–250.CambridgeUniversityPress,Cambridge,UK.LauraKallmeyer.2006.Comparinglexicalizedgrammarformalismsinanempiricallyadequateway:Thenotionofgenerativeattachmentcapacity.InInternationalConferenceonLinguisticEvidence,pages154–156,Tübingen,Germany.MarcoKuhlmannandJoakimNivre.2006.Mildlynon-projectivedependencystructures.In21stInternationalConferenceonComputationalLinguisticsand44thAn-nualMeetingoftheAssociationforComputationalLin-guistics(COLING-ACL)MainConferencePosterSes-sions,pages507–514,Sydney,Australia.RyanMcDonaldandFernandoPereira.2006.On-linelearningofapproximatedependencyparsingal-gorithms.InEleventhConferenceoftheEuropeanChapteroftheAssociationforComputationalLinguis-tics(EACL),pages81–88,Trento,Italy.RyanMcDonald,FernandoPereira,KirilRibarov,andJanHajiˇc.2005.Non-projectivedependencyparsingusingspanningtreealgorithms.InHumanLanguageTechnol-ogyConference(HLT)andConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP),pages523–530,Vancouver,BritishColumbia,Canada.PeterNeuhausandNorbertBröker.1997.Thecomplexityofrecognitionoflinguisticallyadequatedependencygrammars.In35thAnnualMeetingoftheAssociationforComputationalLinguistics(ACL),pages337–343,Madrid,Spain.JoakimNivre.2006.Constraintsonnon-projectivedepen-dencyparsing.InEleventhConferenceoftheEuropeanChapteroftheAssociationforComputationalLinguis-tics(EACL),pages73–80,Trento,Italy.ChrisQuirk,ArulMenezes,andColinCherry.2005.Dependencytreelettranslation:Syntacticallyinformedphrasalsmt.In43rdAnnualMeetingoftheAssociationforComputationalLinguistics(ACL),pages271–279,AnnArbor,USA.OwenRambowandAravindK.Joshi.1997.Afor-mallookatdependencygrammarsandphrase-structuregrammars.InLeoWanner,editor,RecentTrendsinMeaning-TextTheory,volume39ofStudiesinLan-guage,CompanionSeries,pages167–190.JohnBen-jamins,Amsterdam,TheNetherlands.GiorgioSatta.1992.Recognitionoflinearcontext-freerewritingsystems.In30thAnnualMeetingoftheAs-sociationforComputationalLinguistics(ACL),pages89–95,Newark,Delaware,USA.KaterinaVeselá,JiˇriHavelka,andEvaHajiˇcova.2004.Conditionofprojectivityintheunderlyingdepen-dencystructures.In20thInternationalConferenceonComputationalLinguistics(COLING),pages289–295,Geneva,Switzerland.K.Vijay-Shanker,DavidJ.Weir,andAravindK.Joshi.1987.Characterizingstructuraldescriptionsproducedbyvariousgrammaticalformalisms.In25thAnnualMeetingoftheAssociationforComputationalLinguis-tics(ACL),pages104–111,Stanford,California,USA.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 168–175,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

168

TransformingProjectiveBilexicalDependencyGrammarsintoefﬁciently-parsableCFGswithUnfold-FoldMarkJohnsonMicrosoftResearchBrownUniversityRedmond,WAProvidence,RIt-majoh@microsoft.comMarkJohnson@Brown.eduAbstractThispapershowshowtousetheUnfold-FoldtransformationtotransformProjectiveBilexicalDependencyGrammars(PBDGs)intoambiguity-preservingweaklyequiva-lentContext-FreeGrammars(CFGs).TheseCFGscanbeparsedinO(n3)timeusingaCKYalgorithmwithappropriateindexing,ratherthantheO(n5)timerequiredbyanaiveencoding.Informally,usingtheCKYalgorithmwithsuchaCFGmimicsthestepsoftheEisner-SattaO(n3)PBDGparsingal-gorithm.ThistransformationmakesallofthetechniquesdevelopedforCFGsavailabletoPBDGs.Wedemonstratethisbydescrib-ingamaximumposteriorparsedecoderforPBDGs.1IntroductionProjectiveBilexicalDependencyGrammars(PB-DGs)haveattractedattentionrecentlyfortworea-sons.First,becausetheycapturebilexicalhead-to-headdependenciestheyarecapableofproducingextremelyhigh-qualityparses:state-of-the-artdis-criminativelytrainedPBDGparsersrivaltheaccu-racyoftheverybeststatisticalparsersavailableto-day(McDonald,2006).Second,Eisner-SattaO(n3)PBDGparsingalgorithmsareextremelyfast(Eisner,1996;EisnerandSatta,1999;Eisner,2000).ThispaperinvestigatestherelationshipbetweenContext-FreeGrammar(CFG)parsingandtheEis-ner/SattaPBDGparsingalgorithms,includingtheirextensiontosecond-orderPBDGparsing(McDon-ald,2006;McDonaldandPereira,2006).Speciﬁ-cally,weshowhowtouseanoff-linepreprocessingstep,theUnfold-Foldtransformation,totransformaPBDGintoanequivalentCFGthatcanbeparsedinO(n3)timeusingaversionoftheCKYalgorithmwithsuitableindexing(Younger,1967),andextendthistransformationsothatitcapturessecond-orderPBDGdependenciesaswell.Thetransformationsareambiguity-preserving,i.e.,thereisaone-to-onemappingbetweendependencyparsesandCFGparses,soitispossibletomaptheCFGparsesbacktothePBDGparsestheycorrespondto.ThePBDGtoCFGreductionsmaketechniquesdevelopedforCFGsavailabletoPBDGsaswell.Forexample,incrementalCFGparsingalgorithmscanbeusedwiththeCFGsproducedbythistransform,ascantheInside-Outsideestimationalgorithm(LariandYoung,1990)andmoreexoticmethodssuchasestimatingadjoinedhiddenstates(Matsuzakietal.,2005;Petrovetal.,2006).Asanexampleappli-cation,wedescribeamaximumposteriorparsede-coderforPBDGsinSection8.TheUnfold-Foldtransformationisacalculusfortransformingfunctionalandlogicprogramsintoequivalentbut(hopefully)fasterprograms(BurstallandDarlington,1977).Weuseitheretotrans-formCFGsencodingdependencygrammarsintootherCFGsthataremoreefﬁcientlyparsable.SinceCFGscanbeexpressedasHorn-clauselogicpro-grams(PereiraandShieber,1987)andtheUnfold-Foldtransformationisprovablycorrectforsuchpro-grams(Sato,1992;PettorossiandProeitti,1992),itfollowsthatitsapplicationtoCFGsisprovablycor-rectaswell.TheUnfold-FoldtransformationisusedheretoderivetheCFGschematapresentedinsec-tions5–7.Asystemthatusestheseschemata(suchastheonedescribedinsection8)canimplement169

theseschematadirectly,sotheUnfold-Foldtrans-formationplaysatheoreticalroleinthiswork,justi-fyingtheresultingCFGschemata.TheclosestrelatedworkweareawareofisMcAllester(1999),whichalsodescribesare-ductionofPBDGstoefﬁciently-parsableCFGsanddirectlyinspiredthiswork.However,theCFGsproducedbyMcAllester’stransformationin-cludeepsilon-productionssotheyrequireaspecial-izedCFGparsingalgorithm,whiletheCFGspro-ducedbythetransformationsdescribedherehavebinaryproductionssotheycanbeparsedwithstandardCFGparsingalgorithms.Further,ourapproachextendstosecond-orderPBDGparsing,whileMcAllesteronlydiscussesﬁrst-orderPBDGs.Therestofthispaperisstructuredasfollows.Section2deﬁnesprojectivedependencygraphsandgrammarsandSection3reviewsthe“naive”encod-ingofPBDGsasCFGswithanO(n5)parsetime,wherenisthelengthofthestringtobeparsed.Sec-tion4introducesthe“split-head”CFGencodingofPBDGs,whichhasanO(n4)parsetimeandservesastheinputtotheUnfold-Foldtransform.Section5usestheUnfold-Foldtransformtoobtainaweakly-equivalentCFGencodingofPBDGswhichcanbeparsedinO(n3)time,andpresentstimingresultsshowingthatthetransformationdoesspeedparsing.Sections6and7applyUnfold-FoldinslightlymorecomplexwaystoobtainCFGencodingsofPBDGsthatalsomakesecond-orderdependenciesavailableinO(n3)timeparsableCFGs.Section8appliesaPBDGtoCFGtransformtoobtainamaximumpos-teriordecodingparserforPBDGs.2ProjectivebilexicaldependencyparsesandgrammarsLetΣbeaﬁnitesetofterminals(e.g.,words),andlet0betherootterminalnotinΣ.Ifw=(w1,...,wn)∈Σ⋆,letw⋆=(0,w1,...,wn),i.e.,w⋆isobtainedbypreﬁxingwwith0.AdependencyparseGforwisatreewhoserootislabeled0andwhoseothernverticesarelabeledwitheachofthenterminalsinw.IfGcontainsanarcfromutovthenwesaythatvisadependentofu,andifGcontainsapathfromutovthenwesaythatvisadescendantofu.Ifvisdependentofuthatalsoprecedesuinw⋆thenwesaythatvisaleftdependentofu(rightdependentandleftandrightdescendantsaredeﬁnedsimilarly).0SandygavethedogaboneFigure1:Aprojectivedependencyparseforthesen-tence“Samgavethedogabone”.AdependencyparseGisprojectiveiffwheneverthereisapathfromutovthenthereisalsoapathfromutoeverywordbetweenuandvinw⋆aswell.Figure1depictsaprojectivedependencyparseforthesentence“Samgavethedogabone”.Aprojectivedependencygrammardeﬁnesasetofprojectivedependencyparses.AProjectiveBilexi-calDependencyGrammar(PBDG)consistsoftworelationsand,bothdeﬁnedover(Σ∪{0})×Σ.APBDGgeneratesaprojectivedependencyparseGiffuvforallrightdependencies(u,v)inGandvuforallleftdependencies(u,v)inG.ThelanguagegeneratedbyaPBDGisthesetofstringsthathaveprojectivedependencyparsesgeneratedbythegrammar.Thefollowingdepen-dencygrammargeneratesthedependencyparseinFigure1.0gaveSandygavegavedogthedoggaveboneaboneThispaperdoesnotconsiderstochasticdepen-dencygrammarsdirectly,butseeSection8foranapplicationinvolvingthem.However,itisstraight-forwardtoassociateweightswithdependencies,andsincethedependenciesarepreservedbythetransfor-mations,obtainaweightedCFG.StandardmethodsforconvertingweightedCFGstoequivalentPCFGscanbeusedifrequired(Chi,1999).Alternatively,onecantransformacorpusofdependencyparsesintoacorpusofthecorrespondingCFGparses,andestimateCFGproductionprobabilitiesdirectlyfromthatcorpus.3AnaiveencodingofPBDGsThereisawell-knownmethodforencodingaPBDGasaCFGinwhicheachterminalu∈ΣisassociatedwithacorrespondingnonterminalXuthatexpandstouandallofu’sdescendants.ThenonterminalsofthenaiveencodingCFGconsistofthestartsymbolSandsymbolsXuforeachterminalu∈Σ,and170

theproductionsoftheCFGaretheinstancesofthefollowingschemata:S→Xuwhere0uXu→uXu→XvXuwherevuXu→XuXvwhereuvThedependencyannotationsassociatedwitheachproductionspecifyhowtointerpretalocaltreegen-eratedbythatproduction,andpermitustomapaCFGparsetothecorrespondingdependencyparse.Forexample,thetop-mostlocaltreeinFigure2wasgeneratedbytheproductionS→Xgave,andindi-catethatinthisparse0gave.GivenaterminalvocabularyofsizemtheCFGcontainsO(m2)productions,soitisimpracticaltoenumerateallpossibleproductionsforevenmodestvocabularies.Insteadproductionsrelevanttoapar-ticularsentencearegeneratedontheﬂy.ThenaiveencodingCFGingeneralrequiresO(n5)parsingtimewithaconventionalCKYpars-ingalgorithm,sincetrackingtheheadannotationsuandvmultipliesthestandardO(n3)CFGparsetimerequirementsbyanadditionalfactorproportionaltotheO(n2)productionsexpandingXu.AnadditionalproblemwiththenaiveencodingisthattheresultingCFGingeneralexhibitsspuri-ousambiguities,i.e.,asingledependencyparsemaycorrespondtomorethanoneCFGparse,asshowninFigure2.Informally,thisisbecausetheCFGper-mitsleftandtherightdependenciestobearbitrarilyintermingled.4Split-headencodingofPBDGsThereareseveralwaysofremovingthespuriousam-biguitiesinthenaiveCFGencodingjustdescribed.Thissectionpresentsamethodwecallthe“split-headencoding”,whichremovestheambiguitiesandservesasstartingpointforthegrammartransformsdescribedbelow.Thesplit-headencodingrepresentseachworduintheinputstringwbytwouniqueterminalsulandurintheCFGparse.Asplit-headCFG’ster-minalvocabularyisΣ′={ul,ur:u∈Σ},whereΣisthesetofterminalsofthePBDG.APBDGparsewithyieldw=(u1,...,un)istrans-formedtoasplit-headCFGparsewithyieldw′=(u1,l,u1,r,...,un,l,un,r),so|w′|=2|w|.SthedogXtheXdogXdogXgavegaveXgaveXboneXaaXboneboneXgaveXSandySandyXgaveSthedogXtheXdogXdogXboneXaaXboneboneXgaveXgavegaveXSandySandyXgaveXgaveFigure2:TwoparsesusingthenaiveCFGencod-ingthatbothcorrespondtothedependencyparseofFigure1.Thesplit-headCFGforaPBDGisgivenbythefollowingschemata:S→Xuwhere0uXu→LuuRwhereu∈ΣLu→ulLu→XvLuwherevuuR→uruR→uRXvwhereuvThedependencyparseshowninFigure1corre-spondstothesplit-headCFGparseshowninFig-ure3.EachXuexpandstotwonewcategories,LuanduR.Luconsistsofulandallofu’sleftdescen-dants,whileuRconsistsofurandallofu’srightdescendants.Thespuriousambiguitypresentinthenaiveencodingdoesnotariseinthesplit-headen-codingbecausetheleftandrightdependentsofaheadareassembledindependentlyandcannotinter-mingle.Ascanbeseenbyexaminingthesplit-headschemata,therightmostdescendantofLuiseitherLuorul,whichguaranteesthattherightmosttermi-naldominatedbyLuisalwaysul;similarlytheleft-mostterminaldominatedbyuRisalwaysur.Thus171

dogRXSandyLSandySandylXdoggavergavelgaveRgaveRLaalaRarXaLbonebonelLbonebonerboneRXboneSandyRSandyrLgaveLgaveXgaveSgaveRLthetheltheRtherXtheLdogdoglLdogdogrFigure3:Thesplit-headparsecorrespondingtothedependencygraphdepictedinFigure1.NoticethatulisalwaystherightmostdescendantofLuandurisalwaystheleftmostdescendantofuR,whichmeansthattheseindicesareredundantgiventheconstituentspans.thesesubscriptindicesareredundantgiventhestringpositionsoftheconstituents,whichmeanswedonotneedtotracktheindexuinLuanduRbutcanparsewithjustthetwocategoriesLandR,anddeterminetheindexfromtheconstituent’sspanwhenrequired.Itisstraight-forwardtoextendthesplit-headCFGtoencodetheadditionalstateinformationrequiredbytheheadautomataofEisnerandSatta(1999);thiscorrespondstosplittingthenon-terminalsLuanduR.ForsimplicityweworkwithPBDGsinthispaper,butalloftheUnfold-Foldtransformationsde-scribedbelowextendtosplit-headgrammarswiththeadditionalstatestructurerequiredbyheadau-tomata.Implementationnote:itispossibletodirectlyparsethe“undoubled”inputstringwbymodifyingboththeCKYalgorithmandtheCFGsdescribedinthispaper.ModifyLuanduRsotheybothul-timatelyexpandtothesameterminalu,andspecial-casetheimplementationofproductionXu→LuuRandallproductionsderivedfromittopermitLuanduRtooverlapbytheterminalu.Thesplit-headformulationexplainswhatinitiallyseemunusualpropertiesofexistingPBDGalgo-rithms.Forexample,oneofthestandard“sanitychecks”fortheInside-Outsidealgorithm—thattheoutsideprobabilityofeachterminalisequaltothesentence’sinsideprobability—failsforthesealgo-rithms.Infact,theoutsideprobabilityofeachter-minalisdoublethesentence’sinsideprobabilitybe-causethesealgorithmsimplicitlycollapsethetwoterminalsulandurintoasingleterminalu.5AO(n3)split-headgrammarThesplit-headencodingdescribedintheprevioussectionrequiresO(n4)parsingtimebecausethein-dexvonXvisnotredundant.WecanobtainanequivalentgrammarthatonlyrequiresO(n3)pars-ingtimebytransformingthesplit-headgrammarus-ingUnfold-Fold.WedescribethetransformationonLu;thetransformationofuRissymmetric.WebeginwiththedeﬁnitionofLuinthesplit-headgrammarabove(“|”separatestheright-handsidesofproductions).Lu→ul|XvLuwherevuOurﬁrsttransformationstepistounfoldXvinLu,i.e.,replaceXvbyitsexpansion,producingthefol-lowingdeﬁnitionforLu(ignoretheunderliningfornow).Lu→ul|LvvRLuwherevuThisremovestheoffendingXvinLu,buttheresult-ingdeﬁnitionofLucontainsternaryproductionsandsostillincursO(n4)parsetime.ToaddressthiswedeﬁnenewnonterminalsxMyforeachx,y∈Σ:xMy→xRLyandfoldtheunderlinedchildreninLuintovMu:xMy→xRLywherex,y∈ΣLu→ul|LvvMuwherevu172

SdogrtherLdogtheRtheMdogthelLtheLdogdoglgavergaveRgaveMdoggaveldogRgaveRalaraRbonelLboneaMboneLaLbonegaveMboneboneRbonergaveRLgaveLgaveSandyRSandyMgaveLSandySandylSandyrFigure4:TheO(n3)split-headparsecorrespondingtothedependencygraphofFigure1.TheO(n3)split-headgrammarisobtainedbyun-foldingtheoccurenceofXuintheSproductionanddroppingtheXuschemaasXunolongerappearsontheright-handsideofanyproduction.TheresultingO(n3)split-headgrammarschemataareasfollows:S→LuuRwhere0uLu→ulLu→LvvMuwherevuuR→uruR→uMvvRwhereuvxMy→xRLywherex,y∈ΣAsbefore,thedependencyannotationsonthepro-ductionschematapermitustomapCFGparsestothecorrespondingdependencyparse.ThisgrammarrequiresO(n3)parsingtimetoparsebecausethein-dicesareredundantgiventheconstituent’sstringpo-sitionsforthereasonsdescribedinsection4.Specif-ically,therightmostterminalofLuisalwaysul,theleftmostterminalofuRisalwaysurandtheleft-mostandrightmostterminalsofvMuarevlandurrespectively.TheO(n3)split-headgrammariscloselyrelatedtotheO(n3)PBDGparsingalgorithmgivenbyEis-nerandSatta(1999).Speciﬁcally,thestepsinvolvedinparsingwiththisgrammarusingtheCKYalgo-rithmareessentiallythesameasthoseperformedbytheEisner/Sattaalgorithm.Theprimarydiffer-enceisthattheEisner/SattaalgorithminvolvestwoseparatecategoriesthatarecollapsedintothesinglecategoryMhere.Toconﬁrmtheirrelativeperformanceweimple-mentedstochasticCKYparsersforthethreeCFGschematadescribedsofar.Theproductionschematawerehard-codedforspeed,andtheimplementationtrickdescribedinsection4wasusedtoavoiddou-blingtheterminalstring.Weobtaineddependencyweightsfromourexistingdiscriminatively-trainedPBDGparser(notcitedtopreserveanonymity).Wecomparedtheparsers’runningtimesonsection24ofthePennTreebank.BecauseallthreeCFGsim-plementthesamedependencygrammartheirViterbiparseshavethesamedependencyaccuracy,namely0.8918.Weprecomputethedependencyweights,sothetimesincludejustthedynamicprogrammingcomputationona3.6GHzPentium4.CFGschematasentencesparsed/secondNaiveO(n5)CFG45.4O(n4)CFG406.2O(n3)CFG3580.06AnO(n3)adjacent-headgrammarThissectionshowshowtofurthertransformtheO(n3)grammardescribedaboveintoaformthatencodessecond-orderdependenciesbetweenad-jacentdependentheadsinmuchthewaythataMarkovPCFGdoes(McDonald,2006;McDonaldandPereira,2006).WeprovideaderivationfortheLuconstituents;thereisaparallelderivationforuR.WebeginbyunfoldingXvinthedeﬁnitionofLuinthesplit-headgrammar,producingasbefore:Lu→ul|LvvRLuNowintroduceanewnonterminalvMLu,whichisaspecializedversionofMrequiringthatvisaleft-dependentofu,andfoldtheunderlinedconstituents173

SthertheRtheMLdogthelLtheLdogdogRdogldogrLboneLaaMLboneaRaralbonelbonergavergaveMRdogdogMbonegaveMRboneboneRgaveRgavelSandyrSandyRSandylSandyMLgaveLSandyLgaveFigure5:TheO(n3)adjacent-headparsecorrespondingtothedependencygraphofFigure1.Theboxedlocaltreeindicatesboneisthedependentofgivefollowingthedependentdog,i.e.,givedogbone.intovMLu.vMLu→vRLuwherevuLu→ul|LvvMLuwherevuNowunfoldLuinthedeﬁnitionofvMLu,producing:vMLu→vRul|vRLv′v′MLu;vv′uNotethatintheﬁrstproductionexpandingvMLu,vistheclosestleftdependentofu,andinthesecondproductionvandv′areadjacentleft-dependentsofu.vMLuhasaternaryproduction,soweintroducexMyasbeforetofoldtheunderlinedconstituentsinto.xMy→xRLywherex,y∈ΣvMLu→vRul|vMv′v′MLu;vv′uTheresultinggrammarschemaisasbelow,andasampleparseisgiveninFigure5.S→LuuRwhere0uLu→uluhasnoleftdependentsLu→LvvMLuvisu’slastleftdep.vMLu→vRulvisu’sclosestleftdep.vMLu→vMv′v′MLuvv′uuR→uruhasnorightdependentsuR→uMRvvRvisu’slastrightdep.uMRv→urLvvisu’sclosestrightdep.uMRv→uMRv′v′Mvuv′vxMy→xRLywherex,y∈ΣAsbefore,theindicesonthenonterminalsarere-dundant,astheheadsarealwayslocatedatanedgeofeachconstituent,sotheyneednotbecomputedorstoredandtheCFGcanbeparsedinO(n3)time.ThestepsinvolvedinCKYparsingwiththisgram-marcorrespondcloselytothoseoftheMcDonald(2006)second-orderPBDGparsingalgorithm.7AnO(n3)dependent-headgrammarThissectionshowsadifferentapplicationofUnfold-Foldcancapturehead-to-head-to-headdependen-cies,i.e.,“vertical”second-orderdependencies,ratherthanthe“horizontal”onescapturedbythetransformationdescribedintheprevioussection.Becauseweexpecttheseverticaldependenciestobelessimportantlinguisticallythanthehorizontalones,weonlysketchthetransformationhere.ThederivationdiffersfromtheoneinSection6inthatthedependentvR,ratherthantheheadLu,isun-foldedintheinitialdeﬁnitionofvMLu.Thisresultsinagrammarthattracksvertical,ratherthanhorizon-tal,second-orderdependencies.Sinceleft-handandright-handderivationsareassembledseparatelyinasplit-headgrammar,thegrammarinfactonlytrackszig-zagtypedependencies(e.g.,whereagrandpar-enthasarightdependent,whichinturnhasaleftdependent).Theresultinggrammarisgivenbelow,andasam-pleparseusingthisgrammarisshowninFigure6.BecausethesubscriptsareredundanttheycanbeomittedandtheresultingCFGcanbeparsedin174

gaveMRbonegavergaveRLthethelgaveMthetherdoglLdogtheMLdogdogrdogRgaveRLaalargaveMaaMLboneLbonebonelbonerboneRgaveRLgavegavelSandyrSandyMLgaveSandylLSandygaveMRdogSLgaveFigure6:Then3dependent-headparsecorrespondingtothedependencygraphofFigure1.Theboxedlocaltreeindicatesthataisaleft-dependentofbone,whichisinturnaright-dependentofgave,i.e.,gaveabone.O(n3)timeusingtheCKYalgorithm.S→LuuRwhere0uLu→ulLu→LvvMLuwherevuvMLu→vrLuwherevuvMLu→vMRwwMuwherevwuuR→uruR→uMRvvRwhereuvuMRv→uRvlwhereuvuMRv→uMwwMLvwhereuwuxMy→xRLywherex,y∈Σ8MaximumposteriordecodingAsnotedintheintroduction,oneconsequenceofthePBDGtoCFGreductionspresentedinthispaperisthatCFGparsingandestimationtechniquesarenowavailableforPBDGsaswell.Asanexampleap-plication,thissectiondescribesMaximumPosteriorDecoding(MPD)forPBDGs.Goodman(1996)observedthattheViterbiparseisingeneralnottheoptimalparseforevaluationmetricssuchasf-scorethatarebasedonthenumberofcorrectconstituentsinaparse.HeshowedthatMPDimprovesf-scoremodestlyrelativetoViterbidecodingforPCFGs.Sincedependencyparseaccuracyisjustthepro-portionofdependenciesintheparsethatarecorrect,Goodman’sobservationshouldholdforPBDGpars-ingaswell.MPDforPBDGsselectstheparsethatmaximizesthesumofthemarginalprobabilitiesofeachofthedependenciesintheparse.Suchade-codermightplausiblyproduceparsesthatscorebet-teronthedependencyaccuracymetricthanViterbiparses.MPDisstraightforwardgiventhePBDGtoCFGreductionsdescribedinthispaper.Speciﬁcally,weusetheInside-OutsidealgorithmtocomputetheposteriorprobabilityoftheCFGconstituentscorre-spondingtoeachPBDGdependency,andthenusetheViterbialgorithmtoﬁndtheparsetreethatmax-imizesthesumoftheseposteriorprobabilities.WeimplementedMPDforﬁrst-orderPBDGsusingdependencyweightsfromourexistingdiscriminatively-trainedPBDGparser(notcitedtopreserveanonymity).TheseweightsareestimatedbyanonlineprocedureasinMcDonald(2006),andarenotintendedtodeﬁneaprobabilitydistribution.Inanattempttoheuristicallycorrectforthis,inthisexperimentweusedexp(αwu,v)astheweightofthedependencybetweenheaduanddependentv,wherewu,vistheweightprovidedbythediscriminatively-trainedmodelandαisanadjustablescalingparame-tertunedtooptimizeMPDaccuracyondevelopmentdata.Unfortunatelywefoundnosigniﬁcantdiffer-encebetweentheaccuracyoftheMPDandViterbiparses.OptimizingMPDonthedevelopmentdata(section24ofthePTB)setthescalefactorα=0.21andproducedMPDparseswithanaccuracyof0.8921,whichisapproximatelythesameastheViterbiaccuracyof0.8918.Ontheblindtestdata(section23)thetwoaccuraciesareessentiallyiden-175

tical(0.8997).ThereareseveralpossibleexplanationsforthefailureofMPDtoproducemoreaccurateparsesthanViterbidecoding.PerhapsMPDrequiresweightsthatdeﬁneaprobabilitydistribution(e.g.,aMax-Entmodel).ItisalsopossiblethatdiscriminativetrainingadjuststheweightsinawaythatensuresthattheViterbiparseisclosetothemaximumpos-teriorparse.Thiswasthecaseinourexperiment,andifthisistruewithdiscriminativetrainingingen-eral,thenmaximumposteriordecodingwillnothavemuchtooffertodiscriminativeparsing.9ConclusionThispapershowshowtousetheUnfold-Foldtrans-formtotranslatePBDGsintoCFGsthatcanbeparsedinO(n3)time.Akeycomponentofthisisthesplit-headconstruction,whereeachworduintheinputissplitintotwoterminalsulanduroftheCFGparse.Wealsoshowedhowtosystematicallytrans-formthesplit-headCFGintogrammarswhichtracksecond-orderdependencies.Weprovidedonegram-marwhichcaptureshorizontalsecond-orderdepen-dencies(McDonald,2006),andanotherwhichcap-turesverticalsecond-orderhead-to-head-to-headde-pendencies.Thegrammarsdescribedherejustscratchthesur-faceofwhatispossiblewithUnfold-Fold.Noticethatbothofthesecond-ordergrammarshavemorenonterminalsthantheﬁrst-ordergrammar.Ifoneispreparedtoincreasethenumberofnonterminalsstillfurther,itmaybepossibletotrackadditionalinfor-mationaboutconstituents(althoughifweinsistonO(n3)parsetimewewillbeunabletotrackthein-teractionofmorethanthreeheadsatonce).ReferencesR.M.BurstallandJohnDarlington.1977.Atransformationsystemfordevelopingrecursiveprograms.JournaloftheAssociationforComputingMachinery,24(1):44–67.ZhiyiChi.1999.Statisticalpropertiesofprobabilisticcontext-freegrammars.ComputationalLinguistics,25(1):131–160.JasonEisnerandGiorgioSatta.1999.Efﬁcientparsingforbilexicalcontext-freegrammarsandheadautomatongram-mars.InProceedingsofthe37thAnnualMeetingoftheAssociationforComputationalLinguistics,pages457–480,UniversityofMaryland.JasonEisner.1996.Threenewprobabilisticmodelsfordepen-dencyparsing:Anexploration.InCOLING96:Proceedingsofthe16thInternationalConferenceonComputationalLin-guistics,pages340–345,Copenhagen.CenterforSprogte-knologi.JasonEisner.2000.Bilexicalgrammarsandtheircubic-timeparsingalgorithms.InHarryBuntandAntonNijholt,edi-tors,AdvancesinProbabilisticandOtherParsingTechnolo-gies,pages29–62.KluwerAcademicPublishers.JoshuaT.Goodman.1996.Parsingalgorithmsandmetrics.InProceedingsofthe34thAnnualMeetingoftheAssociationforComputationalLinguistics,pages177–183,SantaCruz,Ca.K.LariandS.J.Young.1990.TheestimationofStochasticContext-FreeGrammarsusingtheInside-Outsidealgorithm.ComputerSpeechandLanguage,4(35-56).TakuyaMatsuzaki,YusukeMiyao,andJun’ichiTsujii.2005.ProbabilisticCFGwithlatentannotations.InProceedingsofthe43rdAnnualMeetingoftheAssociationforCom-putationalLinguistics(ACL’05),pages75–82,AnnArbor,Michigan,June.AssociationforComputationalLinguistics.DavidMcAllester.1999.AreformulationofEisnerandSata’scubictimeparserforsplitheadautomatagrammars.Avail-ablefromhttp://ttic.uchicago.edu/˜dmcallester/.RyanMcDonaldandFernandoPereira.2006.Onlinelearn-ingofapproximatedependencyparsingalgorithms.In11thConferenceoftheEuropeanChapteroftheAssociationforComputationalLinguistics,pages81–88,Trento,Italy.RyanMcDonald.2006.DiscriminativeTrainingandSpanningTreeAlgorithmsforDependencyParsing.Ph.D.thesis,Uni-versityofPennyslvania,Philadelphia,PA.FernandoPereiraandStuartM.Shieber.1987.PrologandNat-uralLanguageAnalysis.CenterfortheStudyofLanguageandInformation,Stanford,CA.SlavPetrov,LeonBarrett,RomainThibaux,andDanKlein.2006.Learningaccurate,compact,andinterpretabletreean-notation.InProceedingsofthe21stInternationalConfer-enceonComputationalLinguisticsand44thAnnualMeet-ingoftheAssociationforComputationalLinguistics,pages433–440,Sydney,Australia,July.AssociationforComputa-tionalLinguistics.A.PettorossiandM.Proeitti.1992.Transformationoflogicprograms.InHandbookofLogicinArtiﬁcialIntelligence,volume5,pages697–787.OxfordUniversityPress.TaisukeSato.1992.Equivalence-preservingﬁrst-orderun-fold/foldtransformationsystems.TheoreticalComputerSci-ence,105(1):57–84.DanielH.Younger.1967.Recognitionandparsingofcontext-freelanguagesintimen3.InformationandControl,10(2):189–208.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 176–183,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

176

ParsingandGenerationasDatalogQueriesMakotoKanazawaNationalInstituteofInformatics2–1–2Hitotsubashi,Chiyoda-ku,Tokyo,101–8430,Japankanazawa@nii.ac.jpAbstractWeshowthattheproblemsofparsingandsur-facerealizationforgrammarformalismswith“context-free”derivations,coupledwithMon-taguesemantics(underacertainrestriction)canbereducedinauniformwaytoDatalogqueryevaluation.Aswellasgivingapolynomial-timealgorithmforcomputingallderivationtrees(intheformofasharedforest)fromanin-putstringorinputlogicalform,thisreductionhasthefollowingcomplexity-theoreticconse-quencesforallsuchformalisms:(i)thede-cisionproblemofrecognizinggrammaticality(surfacerealizability)ofaninputstring(logicalform)isinLOGCFL;and(ii)thesearchprob-lemofﬁndingonelogicalform(surfacestring)fromaninputstring(logicalform)isinfunc-tionalLOGCFL.Moreover,thegeneralizedsup-plementarymagic-setsrewritingoftheDatalogprogramresultingfromthereductionyieldsef-ﬁcientEarley-stylealgorithmsforbothparsingandgeneration.1IntroductionTherepresentationofcontext-freegrammars(aug-mentedwithfeatures)intermsofdeﬁniteclausepro-gramsiswell-known.Inthecaseofabare-boneCFG,thecorrespondingprogramisinthefunction-freesubsetoflogicprogramming,knownasDat-alog.Forexample,determiningwhetherastringJohnfoundaunicornbelongstothelanguageoftheCFGinFigure1isequivalenttodecidingwhethertheDatalogprograminFigure2togetherwiththedatabasein(1)canderivethequery“?−S(0,4).”(1)John(0,1).found(1,2).a(2,3).unicorn(3,4).S→NPVPVP→VNPV→VConjVNP→DetNNP→JohnV→foundV→caughtConj→andDet→aN→unicornFigure1:ACFG.S(i,j):−NP(i,k),VP(k,j).VP(i,j):−V(i,k),NP(k,j).V(i,j):−V(i,k),Conj(k,l),V(l,j).NP(i,j):−Det(i,k),N(k,j).NP(i,j):−John(i,j).V(i,j):−found(i,j).V(i,j):−caught(i,j).Conj(i,j):−and(i,j).Det(i,j):−a(i,j).N(i,j):−unicorn(i,j).Figure2:TheDatalogrepresentationofaCFG.Bynaive(orseminaive)bottom-upevaluation(see,e.g.,Ullman,1988),theanswertosuchaquerycanbecomputedinpolynomialtimeinthesizeofthedatabaseforanyDatalogprogram.Byrecordingruleinstancesratherthanderivedfacts,apackedrep-resentationofthecompletesetofDatalogderivationtreesforagivenquerycanalsobeobtainedinpoly-nomialtimebythesametechnique.SinceaData-logderivationtreeuniquelydeterminesagrammarderivationtree,thisgivesareductionofcontext-freerecognitionandparsingtoqueryevaluationinData-log.Inthispaper,weshowthatasimilarreductiontoDatalogispossibleformorepowerfulgrammarformalismswith“context-free”derivations,suchas(multi-component)tree-adjoininggrammars(JoshiandSchabes,1997;Weir,1988),IOmacrogram-mars(Fisher,1968),and(parallel)multiplecontext-freegrammars(Sekietal.,1991).Forinstance,theTAGinFigure3isrepresentedbytheDatalogpro-graminFigure4.Moreover,themethodofreduc-177

SAANAaAbA∗NAcdFigure3:ATAGwithoneinitialtree(left)andoneauxiliarytree(right)S(p1,p3):−A(p1,p3,p2,p2).A(p1,p8,p4,p5):−A(p2,p7,p3,p6),a(p1,p2),b(p3,p4),c(p5,p6),d(p7,p8).A(p1,p2,p1,p2).Figure4:TheDatalogrepresentationofaTAG.tionextendstotheproblemoftacticalgeneration(surfacerealization)forthesegrammarformalismscoupledwithMontaguesemantics(underacertainrestriction).Ourmethodessentiallyreliesontheen-codingofdiﬀerentformalismsintermsofabstractcategorialgrammars(deGroote,2001).ThereductiontoDatalogmakesitpossibletoap-plytoparsingandgenerationsophisticatedevalu-ationtechniquesforDatalogqueries;inparticular,anapplicationofgeneralizedsupplementarymagic-setsrewriting(BeeriandRamakrishnan,1991)au-tomaticallyyieldsEarley-stylealgorithmsforbothparsingandgeneration.Thereductioncanalsobeusedtoobtainatightupperbound,namelyLOGCFL,onthecomputationalcomplexityoftheproblemofrecognition,bothforgrammaticalityofinputstringsandforsurfacerealizabilityofinputlogicalforms.Withregardtoparsingandrecognitionofin-putstrings,polynomial-timealgorithmsandtheLOGCFLupperboundonthecomputationalcom-plexityarealreadyknownforthegrammarfor-malismscoveredbyourresults(Engelfriet,1986);nevertheless,webelievethatourreductiontoData-logoﬀersvaluableinsights.Concerninggeneration,ourresultsseemtobeentirelynew.12Context-freegrammarsonλ-termsConsideranaugmentationofthegrammarinFig-ure1withMontaguesemantics,wheretheleft-hand1Weonlyconsiderexactgeneration,nottakingintoaccounttheproblemoflogicalformequivalence,whichwillmostlikelyrendertheproblemofgenerationcomputationallyintractable(Moore,2002).S(X1X2)→NP(X1)VP(X2)VP(λx.X2(λy.X1yx))→V(X1)NP(X2)V(λyx.X2(X1yx)(X3yx))→V(X1)Conj(X2)V(X3)NP(X1X2)→Det(X1)N(X2)NP(λu.uJohne)→JohnV(ﬁnde→e→t)→foundV(catche→e→t)→caughtConj(∧t→t→t)→andDet(λuv.∃(e→t)→t(λy.∧t→t→t(uy)(vy)))→aN(unicorne→t)→unicornFigure5:Acontext-freegrammarwithMontaguesemantics.SNPJohnVPVfoundNPDetaNunicornFigure6:Aderivationtree.sideofeachruleisannotatedwithaλ-termthattellshowthemeaningoftheleft-handsideiscomposedfromthemeaningsoftheright-handsidenontermi-nals,representedbyupper-casevariablesX1,X2,...(Figure5).2Themeaningofasentenceiscomputedfromitsderivationtree.Forexample,JohnfoundaunicornhasthederivationtreeinFigure6,andthegrammarrulesassignitsrootnodetheλ-term(λu.uJohn)(λx.(λuv.∃(λy.∧(uy)(vy)))unicorn(λy.ﬁndyx)),whichβ-reducestotheλ-term(2)∃(λy.∧(unicorny)(ﬁndyJohn))encodingtheﬁrst-orderlogicformularepresentingthemeaningofthesentence(i.e.,itslogicalform).Thus,computingthelogicalform(s)ofasentenceinvolvesparsingandλ-termnormalization.Toﬁndasentenceexpressingagivenlogicalform,itsuﬃces2Wefollowstandardnotationalconventionsintypedλ-calculus.Thus,anapplicationM1M2M3(writtenwithoutparen-theses)associatestotheleft,λx.λy.Misabbreviatedtoλxy.M,andα→β→γstandsforα→(β→γ).WereferthereadertoHindley,1997orSørensenandUrzyczyn,2006forstandardnotionsusedinsimplytypedλ-calculus.178

S(X1X2):−NP(X1),VP(X2).VP(λx.X2(λy.X1yx)):−V(X1),NP(X2).V(λyx.X2(X1yx)(X3yx)):−V(X1),Conj(X2),V(X3).NP(X1X2):−Det(X1),N(X2).NP(λu.uJohne).V(ﬁnde→e→t).V(catche→e→t).Conj(∧t→t→t).Det(λuv.∃(e→t)→t(λy.∧t→t→t(uy)(vy))).N(unicorne→t).Figure7:ACFLG.toﬁndaderivationtreewhoserootnodeisassoci-atedwithaλ-termthatβ-reducestothegivenlog-icalform;thedesiredsentencecansimplybereadoﬀfromthederivationtree.Attheheartofbothtasksisthecomputationofthederivationtree(s)thatyieldtheinput.Inthecaseofgeneration,thismaybeviewedasparsingtheinputλ-termwitha“context-free”grammarthatgeneratesasetofλ-terms(innormalform)(Figure7),whichisobtainedfromtheoriginalCFGwithMontaguesemanticsbystrippingoﬀterminalsymbols.Determiningwhetheragivenlogicalformissurfacerealizablewiththeoriginalgrammarisequivalenttorecognitionwiththeresult-ingcontext-freeλ-termgrammar(CFLG).InaCFLGsuchasinFigure7,constantsappear-ingintheλ-termshavepreassignedtypesindicatedbysuperscripts.Thereisamappingσfromnonter-minalstotheirtypes(σ={S(cid:3)→t,NP(cid:3)→(e→t)→t,VP(cid:3)→e→t,V(cid:3)→e→e→t,Conj(cid:3)→t→t→t,Det(cid:3)→(e→t)→(e→t)→t,N(cid:3)→e→t}).ArulethathasAontheleft-handsideandB1,...,Bnasright-handsidenonterminalshasitsleft-handsideannotatedwithawell-formedλ-termMthathastypeσ(A)underthetypeenvironmentX1:σ(B1),...,Xn:σ(Bn)(insym-bols,X1:σ(B1),...,Xn:σ(Bn)(cid:4)M:σ(A)).Whatwehavecalledacontext-freeλ-termgram-marisnothingbutanalternativenotationforanab-stractcategorialgrammar(deGroote,2001)whoseabstractvocabularyissecond-order,withtherestric-tiontolinearλ-termsremoved.3Inthelinearcase,Salvati(2005)hasshowntherecognition/parsingcomplexitytobePTIME,andexhibitedanalgorithmsimilartoEarleyparsingforTAGs.Second-order3Aλ-termisaλI-termifeachoccurrenceofλbindsatleastoneoccurrenceofavariable.AλI-termislinearifnosubtermcontainsmorethanonefreeoccurrenceofthesamevariable.S(λy.X1(λz.z)y):−A(X1).A(λxy.ao→o(X1(λz.bo→o(x(co→oz)))(do→oy))):−A(X1).A(λxy.xy).Figure8:TheCFLGencodingaTAG.linearACGsareknowntobeexpressiveenoughtoencodewell-knownmildlycontext-sensitivegram-marformalismsinastraightforwardway,includ-ingTAGsandmultiplecontext-freegrammars(deGroote,2002;deGrooteandPogodalla,2004).Forexample,thelinearCFLGinFigure8isanencodingoftheTAGinFigure3,whereσ(S)=o→oandσ(A)=(o→o)→o→o(seedeGroote,2002fordetailsofthisencoding).Inencodingastring-generatinggrammar,aCFLGusesoasthetypeofstringpositionando→oasthetypeofstring.Eachterminalsymbolisrepresentedbyaconstantoftypeo→o,andastringa1...anisencodedbytheλ-termλz.ao→o1(...(ao→onz)...),whichhastypeo→o.Astring-generatinggrammarcoupledwithMon-taguesemanticsmayberepresentedbyasyn-chronousCFLG,apairofCFLGswithmatchingrulesets(deGroote2001).Thetransductionbe-tweenstringsandlogicalformsineitherdirectionconsistsofparsingtheinputλ-termwiththesource-sidegrammarandnormalizingtheλ-term(s)con-structedinaccordancewiththetarget-sidegrammarfromthederivationtree(s)outputbyparsing.3ReductiontoDatalogWeshowthatunderaweakerconditionthanlinear-ity,aCFLGcanberepresentedbyaDatalogpro-gram,obtainingatightupperbound(LOGCFL)ontherecognitioncomplexity.Duetospacelimitation,ourpresentationhereiskeptataninformallevel;formaldeﬁnitionsandrigorousproofofcorrectnesswillappearelsewhere.WeusethegrammarinFigure7asanexample,whichisrepresentedbytheDatalogprograminFig-ure9.Notethatallλ-termsinthisgrammarareal-mostlinearinthesensethattheyareλI-termswhereanyvariableoccurringfreemorethanonceinanysubtermmusthaveanatomictype.Ourconstructionisguaranteedtobecorrectonlywhenthisconditionismet.EachDatalogruleisobtainedfromthecorre-spondinggrammarruleinthefollowingway.Let179

S(p1):−NP(p1,p2,p3),VP(p2,p3).VP(p1,p4):−V(p2,p4,p3),NP(p1,p2,p3).V(p1,p4,p3):−V(p2,p4,p3),Conj(p1,p5,p2),V(p5,p4,p3).NP(p1,p4,p5):−Det(p1,p4,p5,p2,p3),N(p2,p3).NP(p1,p1,p2):−John(p2).V(p1,p3,p2):−ﬁnd(p1,p3,p2).V(p1,p3,p2):−catch(p1,p3,p2).Conj(p1,p3,p2):−∧(p1,p3,p2).Det(p1,p5,p4,p3,p4):−∃(p1,p2,p4),∧(p2,p5,p3).N(p1,p2):−unicorn(p1,p2).Figure9:TheDatalogrepresentationofaCFLG.Mbetheλ-termannotatingtheleft-handsideofthegrammarrule.Weﬁrstobtainaprincipal(i.e.,mostgeneral)typingofM.4Inthecaseofthesecondrule,thisisX1:p3→p4→p2,X2:(p3→p2)→p1(cid:4)λx.X2(λy.X1yx):p4→p1.Wethenremove→andparenthesesfromthetypesintheprincipaltypingandwritetheresultingse-quencesofatomictypesinreverse.5WeobtaintheDatalogrulebyreplacingXiandMinthegrammarrulewiththesequencecomingfromthetypepairedwithXiandM,respectively.NotethatatomictypesintheprincipaltypingbecomevariablesintheData-logrule.Whenthereareconstantsintheλ-termM,theyaretreatedlikefreevariables.Inthecaseofthesecond-to-lastrule,theprincipaltypingis∃:(p4→p2)→p1,∧:p3→p5→p2(cid:4)λuv.∃(λy.∧(uy)(vy)):(p4→p3)→(p4→p5)→p1.Ifthesameconstantoccursmorethanonce,distinctoccurrencesaretreatedasdistinctfreevariables.Theconstructionofthedatabaserepresentingtheinputλ-termissimilar,butslightlymorecomplex.Asimplecaseistheλ-term(2),whereeachconstantoccursjustonce.Wecomputeitsprincipaltyping,treatingconstantsasfreevariables.6∃:(4→2)→1,∧:3→5→2,unicorn:4→3,ﬁnd:4→6→5,John:6(cid:4)∃(λy.∧(unicorny)(ﬁndyJohn)):1.4Tobeprecise,wemustﬁrstconvertMtoitsη-longformrelativetothetypeassignedtoitbythegrammar.Forexample,X1X2intheﬁrstruleisconvertedtoX1(λx.X2x).5Thereasonforreversingthesequencesofatomictypesistoreconciletheλ-termencodingofstringswiththeconventionoflistingstringpositionsfromlefttorightindatabaseslike(1).6Weassumethattheinputλ-termisinη-longnormalform.Wethenobtainthecorrespondingdatabase(3)andquery(4)fromtheantecedentandsuccedentofthisjudgment,respectively.Notethathereweareusing1,2,3,...asatomictypes,whichbecomedatabaseconstants.∃(1,2,4).∧(2,5,3).unicorn(3,4).ﬁnd(5,6,4).John(6).(3)?−S(1).(4)Whentheinputλ-termcontainsmorethanoneoc-currenceofthesameconstant,itisnotalwayscor-recttosimplytreatthemasdistinctfreevariables,unlikeinthecaseofλ-termsannotatinggrammarrules.Considertheλ-term(5)(Johnfoundandcaughtaunicorn):(5)∃(λy.∧(unicorny)(∧(ﬁndyJohn)(catchyJohn))).Here,thetwooccurrencesofJohnmustbetreatedasthesamevariable.Theprincipaltypingis(6)andtheresultingdatabaseis(7).∃:(4→2)→1,∧1:3→5→2,unicorn:4→3,∧2:6→8→5,ﬁnd:4→7→6,John:7,catch:4→7→8(cid:4)∃(λy.∧1(unicorny)(∧2(ﬁndyJohn)(catchyJohn))):1.(6)∃(1,2,4).∧(2,5,3).∧(5,8,6).unicron(3,4).ﬁnd(6,7,4).John(7).catch(8,7,4).(7)Itisnotcorrecttoidentifythetwooccurrencesof∧inthisexample.Theruleistoidentifydistinctoccurrencesofthesameconstantjustincasetheyoccurinthesamepositionwithinα-equivalentsub-termsofanatomictype.Thisisanecessarycon-ditionforthoseoccurrencestooriginateasoneandthesameoccurrenceinthenon-normalλ-termattherootofthederivationtree.(Asapreprocessingstep,itisalsonecessarytocheckthatdistinctoccurrencesofaboundvariablesatisfythesamecondition,sothatthegivenλ-termisβ-equaltosomealmostlin-earλ-term.7)7Notethatthewayweobtainadatabasefromaninputλ-termgeneralizesthestandarddatabaserepresentationofastring:fromtheλ-termencodingλz.ao→o1(...(ao→onz)...)ofastringa1...an,weobtainthedatabase{a1(0,1),...,an(n−1,n)}.180

4CorrectnessofthereductionWesketchsomekeypointsintheproofofcor-rectnessofourreduction.Theλ-termNobtainedfromtheinputλ-termbyreplacingoccurrencesofconstantsbyfreevariablesinthemannerdescribedaboveisthenormalformofsomealmostlinearλ-termN(cid:5).Theleftmostreductionfromanalmostlin-earλ-termtoitsnormalformmustbenon-deletingandalmostnon-duplicatinginthesensethatwhenaβ-redex(λx.P)Qiscontracted,Qisnotdeleted,andmoreoveritisnotduplicatedunlessthetypeofxisatomic.WecanshowthattheSubjectEx-pansionTheoremholdsforsuchβ-reduction,sotheprincipaltypingofNisalsotheprincipaltypingofN(cid:5).ByaslightgeneralizationofaresultbyAoto(1999),thistypingΓ(cid:4)N(cid:5):αmustbenegativelynon-duplicatedinthesensethateachatomictypehasatmostonenegativeoccurrenceinit.ByAotoandOno’s(1994)generalizationoftheCoherenceTheorem(seeMints,2000),itfollowsthateveryλ-termPsuchthatΓ(cid:5)(cid:4)P:αforsomeΓ(cid:5)⊆Γmustbeβη-equaltoN(cid:5)(andconsequentlytoN).Giventheone-onecorrespondencebetweenthegrammarrulesandtheDatalogrules,aData-logderivationtreeuniquelydeterminesagrammarderivationtree(seeFigure10asanexample).Thisrelationisnotone-one,becauseaDatalogderiva-tiontreecontainsdatabaseconstantsfromtheinputdatabase.Thisextrainformationdeterminesatyp-ingoftheλ-termPattherootofthegrammarderiva-tiontree(withoccurrencesofconstantsintheλ-termcorrespondingtodistinctfactsinthedatabasere-gardedasdistinctfreevariables):John:6,ﬁnd:4→6→5,∃:(4→2)→1,∧:3→5→2,unicorn:4→3(cid:4)(λu.uJohn)(λx.(λuv.∃(λy.∧(uy)(vy)))unicorn(λy.ﬁndyx)):1.Theantecedentofthistypingmustbeasubsetoftheantecedentoftheprincipaltypingoftheλ-termNfromwhichtheinputdatabasewasobtained.Bythepropertymentionedattheendoftheprecedingpara-graph,itfollowsthatthegrammarderivationtreeisaderivationtreefortheinputλ-term.Conversely,considertheλ-termP(withdistinctoccurrencesofconstantsregardedasdistinctfreevariables)attherootofagrammarderivationtreefortheinputλ-term.WecanshowthatthereisasubstitutionθwhichmapsthefreevariablesofPtothefreevariablesoftheλ-termNusedtobuildtheinputdatabasesuchthatθsendsthenormalformofPtoN.SincePisanalmostlinearλ-term,theleftmostreductionfromPθtoNisnon-deletingandalmostnon-duplicating.BytheSubjectExpansionTheorem,theprincipaltypingofNisalsotheprin-cipaltypingofPθ,andthistogetherwiththegram-marderivationtreedeterminesaDatalogderivationtree.5Complexity-theoreticconsequencesLetuscallaruleA(M):−B1(X1),...,Bn(Xn)inaCFLGan-ruleifn=0andMdoesnotcontainanyconstants.Wecaneliminate-rulesfromanalmostlinearCFLGbythesamemethodthatKanazawaandYoshinaka(2005)usedforlineargrammars,notingthatforanyΓandα,thereareonlyﬁnitelymanyalmostlinearλ-termsMsuchthatΓ(cid:4)M:α.Ifagrammarhasno-rule,anyderivationtreefortheinputλ-termNthathasaλ-termPatitsrootnodecorrespondstoaDatalogderivationtreewhosenum-berofleavesisequaltothenumberofoccurrencesofconstantsinP,whichcannotexceedthenumberofoccurrencesofconstantsinN.ADatalogprogramPissaidtohavethepoly-nomialfringepropertyrelativetoaclassDofdatabasesifthereisapolynomialp(n)suchthatforeverydatabaseDinDofnfactsandeveryqueryqsuchthatP∪Dderivesq,thereisaderivationtreeforqwhosefringe(i.e.,sequenceofleaves)isoflengthatmostp(n).ForsuchPandD,itisknownthat{(D,q)|D∈D,P∪Dderivesq}isinthecomplex-ityclassLOGCFL(UllmanandVanGelder,1988;Kanellakis,1988).Westatewithoutproofthatthedatabase-querypair(D,q)representinganinputλ-termNcanbecomputedinlogspace.BypaddingDwithextrause-lessfactssothatthesizeofDbecomesequaltothenumberofoccurrencesofconstantsinN,weobtainalogspacereductionfromthesetofλ-termsgener-atedbyanalmostlinearCFLGtoasetoftheform{(D,q)|D∈D,P∪D(cid:4)q},wherePhasthepoly-nomialfringepropertyrelativetoD.ThisshowsthattheproblemofrecognitionforanalmostlinearCFLGisinLOGCFL.181

S(1)NP(1,1,6)John(6)VP(1,6)V(5,6,4)ﬁnd(5,6,4)NP(1,5,4)Det(1,5,4,3,4)∃(1,2,4)∧(2,5,3)N(3,4)unicorn(3,4)S((λu.uJohn)(λx.(λuv.∃(λy.∧(uy)(vy)))unicorn(λy.ﬁndyx)))NP(λu.uJohn)VP(λx.(λuv.∃(λy.∧(uy)(vy)))unicorn(λy.ﬁndyx)))V(ﬁnd)NP((λuv.∃(λy.∧(uy)(vy)))unicorn)Det(λuv.∃(λy.∧(uy)(vy)))N(unicorn)Figure10:ADatalogderivationtree(left)andthecorrespondinggrammarderivationtree(right)BythemainresultofGottlobetal.(2002),there-latedsearchproblemofﬁndingonederivationtreefortheinputλ-termisinfunctionalLOGCFL,i.e.,theclassoffunctionsthatcanbecomputedbyalogspace-boundedTuringmachinewithaLOGCFLoracle.InthecaseofasynchronousalmostlinearCFLG,thederivationtreefoundfromthesourceλ-termcanbeusedtocomputeatargetλ-term.Thus,totheextentthattransductionbackandforthbe-tweenstringsandlogicalformscanbeexpressedbyasynchronousalmostlinearCFLG,thesearchprob-lemofﬁndingonelogicalformofaninputsentenceandthatofﬁndingonesurfacerealizationofaninputlogicalformarebothinfunctionalLOGCFL.8Asaconsequence,thereareeﬃcientparallelalgorithmsfortheseproblems.6RegularsetsoftreesasinputAlmostlinearCFLGscanrepresentasubstan-tialfragmentofaMontaguesemanticsforEn-glishandsuch“linear”grammarformalismsas(multi-component)tree-adjoininggrammars(bothasstringgrammarsandastreegrammars)andmul-tiplecontext-freegrammars.However,IOmacrogrammarsandparallelmultiplecontext-freegram-marscannotbedirectlyrepresentedbecauserepre-sentingstringcopyingrequiresmultipleoccurrencesofavariableoftypeo→o.Thisproblemcanbesolvedbyswitchingfromstringstotrees.Wecon-verttheinputstringintotheregularsetofbinarytreeswhoseyieldequalstheinputstring(usingc8Ifthetarget-sidegrammarisnotlinear,thenormalformofthetargetλ-termcannotbeexplicitlycomputedbecauseitssizemaybeexponentialinthesizeofthesourceλ-term.Neverthe-less,atypingthatservestouniquelyidentifythetargetλ-termcanbecomputedfromthederivationtreeinlogspace.Also,ifthetarget-sidegrammarislinearandstring-generating,thetar-getstringcanbeexplicitlycomputedfromthederivationtreeinlogspace(Salvati,2007).asthesolesymbolofrank2),andturnthegram-marintoatreegrammar,replacingallinstancesofstringconcatenationinthegrammarwiththetreeoperationt1,t2(cid:3)→c(t1,t2).Thisway,astringgram-maristurnedintoatreegrammarthatgeneratesasetoftreeswhoseimageundertheyieldfunctionisthelanguageofthestringgrammar.(InthecaseofanIOmacrogrammar,theresultisanIOcontext-freetreegrammar(Engelfriet,1977).)Stringcopy-ingbecomestreecopying,andtheresultinggram-marcanberepresentedbyanalmostlinearCFLGandhencebyaDatalogprogram.Theregularsetofallbinarytreesthatyieldtheinputstringisrepre-sentedbyadatabasethatisconstructedfromadeter-ministicbottom-upﬁnitetreeautomatonrecogniz-ingit.Determinismisimportantforensuringcor-rectnessofthisreduction.Sincethedatabasecanbecomputedfromtheinputstringinlogspace,thecomplexity-theoreticconsequencesofthelastsec-tioncarryoverhere.7MagicsetsandEarley-stylealgorithmsMagic-setsrewritingofaDatalogprogramallowsbottom-upevaluationtoavoidderivinguselessfactsbymimickingtop-downevaluationoftheoriginalprogram.Theresultofthegeneralizedsupplemen-tarymagic-setsrewritingofBeeriandRamakrish-nan(1991)appliedtotheDatalogprogramrepre-sentingaCFGessentiallycoincideswiththededuc-tionsystem(Shieberetal.,1995)oruninstantiatedparsingsystem(Sikkel,1997)forEarleyparsing.ByapplyingthesamerewritingmethodtoDatalogprogramsrepresentingalmostlinearCFLGs,wecanobtaineﬃcientparsingandgenerationalgorithmsforvariousgrammarformalismswithcontext-freederivations.WeillustratethisapproachwiththeprograminFigure4,followingthepresentationofUllman182

(1989a;1989b).Weassumethequerytotaketheform“?−S(0,x).”,sothattheinputdatabasecanbeprocessedincrementally.Theprogramisﬁrstmadesafebyeliminatingthepossibilityofderivingnon-groundatoms:S(p1,p3):−A(p1,p3,p2,p2).A(p1,p8,p4,p5):−A(p2,p7,p3,p6),a(p1,p2),b(p3,p4),c(p5,p6),d(p7,p8).A(p1,p8,p4,p5):−a(p1,p2),b(p2,p4),c(p5,p6),d(p6,p8).Thesubgoalrectiﬁcationremovesduplicateargu-mentsfromsubgoals,creatingnewpredicatesasneeded:S(p1,p3):−B(p1,p3,p2).A(p1,p8,p4,p5):−A(p2,p7,p3,p6),a(p1,p2),b(p3,p4),c(p5,p6),d(p7,p8).A(p1,p8,p4,p5):−a(p1,p2),b(p2,p4),c(p5,p6),d(p6,p8).B(p1,p8,p4):−A(p2,p7,p3,p6),a(p1,p2),b(p3,p4),c(p4,p6),d(p7,p8).B(p1,p8,p4):−a(p1,p2),b(p2,p4),c(p4,p6),d(p6,p8).Wethenattachtopredicatesadornmentsindicatingthefree/boundstatusofargumentsintop-downeval-uation,reorderingsubgoalssothatasmanyargu-mentsaspossiblearemarkedasbound:Sbf(p1,p3):−Bbﬀ(p1,p3,p2).Bbﬀ(p1,p8,p4):−abf(p1,p2),Abﬀf(p2,p7,p3,p6),bbf(p3,p4),cbb(p4,p6),dbf(p7,p8).Bbﬀ(p1,p8,p4):−abf(p1,p2),bbf(p2,p4),cbf(p4,p6),dbf(p6,p8).Abﬀf(p1,p8,p4,p5):−abf(p1,p2),Abﬀf(p2,p7,p3,p6),bbf(p3,p4),cbb(p5,p6),dbf(p7,p8).Abﬀf(p1,p8,p4,p5):−abf(p1,p2),bbf(p2,p4),cﬀ(p5,p6),dbf(p6,p8).Thegeneralizedsupplementarymagic-setsrewritingﬁnallygivesthefollowingruleset:r1:mB(p1):−mS(p1).r2:S(p1,p3):−mB(p1),B(p1,p3,p2).r3:sup2.1(p1,p2):−mB(p1),a(p1,p2).r4:sup2.2(p1,p7,p3,p6):−sup2.1(p1,p2),A(p2,p7,p3,p6).r5:sup2.3(p1,p7,p6,p4):−sup2.2(p1,p7,p3,p6),b(p3,p4).r6:sup2.4(p1,p7,p4):−sup2.3(p1,p7,p6,p4),c(p4,p6).r7:B(p1,p8,p4):−sup2.4(p1,p7,p4),d(p7,p8).r8:sup3.1(p1,p2):−mB(p1),a(p1,p2).r9:sup3.2(p1,p4):−sup3.1(p1,p2),b(p2,p4).r10:sup3.3(p1,p4,p6):−sup3.2(p1,p4),c(p4,p6).r11:B(p1,p8,p4):−sup3.3(p1,p4,p6),d(p6,p8).r12:mA(p2):−sup2.1(p1,p2).r13:mA(p2):−sup4.1(p1,p2).r14:sup4.1(p1,p2):−mA(p1),a(p1,p2).r15:sup4.2(p1,p7,p3,p6):−sup4.1(p1,p2),A(p2,p7,p3,p6).r16:sup4.3(p1,p7,p6,p4):−sup4.2(p1,p7,p3,p6),b(p3,p4).r17:sup4.4(p1,p7,p4,p5):−sup4.3(p1,p7,p6,p4),c(p5,p6).r18:A(p1,p8,p4,p5):−sup4.4(p1,p7,p4,p5),d(p7,p8).r19:sup5.1(p1,p2):−mA(p1),a(p1,p2).r20:sup5.2(p1,p4):−sup5.1(p1,p2),b(p2,p4).r21:sup5.3(p1,p4,p5,p6):−sup5.2(p1,p4),c(p5,p6).r22:A(p1,p8,p4,p5):−sup5.3(p1,p4,p5,p6),d(p6,p8).Thefollowingversionofchartparsingaddscon-trolstructuretothisdeductionsystem:1.()Initializethecharttotheemptyset,theagendatothesingleton{mS(0)},andnto0.2.Repeatthefollowingsteps:(a)Repeatthefollowingstepsuntiltheagendaisexhausted:i.Removeafactfromtheagenda,calledthetrigger.ii.Addthetriggertothechart.iii.Generateallfactsthatareimmediateconsequencesofthetriggertogetherwithallfactsinthechart,andaddtotheagendathosegeneratedfactsthatareneitheralreadyinthechartnorintheagenda.(b)()Removethenextfactfromthein-putdatabaseandaddittotheagenda,in-crementingn.Ifthereisnomorefactintheinputdatabase,gotostep3.3.IfS(0,n)isinthechart,accept;otherwisere-ject.Thefollowingisthetraceofthealgorithmonin-putstringaabbccdd:1.mS(0)2.mB(0)r1,13.a(0,1)4.sup2.1(0,1)r3,2,35.sup3.1(0,1)r8,2,36.mA(1)r12,47.a(1,2)8.sup4.1(1,2)r14,6,79.sup5.1(1,2)r19,6,710.mA(2)r13,811.b(2,3)12.sup5.2(1,3)r20,9,1113.b(3,4)14.c(4,5)15.sup5.3(1,3,4,5)r21,12,1416.c(6,5)17.sup5.3(1,3,5,6)r21,12,1618.d(6,7)19.A(1,7,3,5)r22,17,1820.sup2.2(0,7,3,5)r4,4,1921.sup2.3(0,7,5,4)r5,13,2022.sup2.4(0,7,4)r6,14,2123.d(7,8)24.B(0,8,4)r7,22,2325.S(0,8)r2,2,24NotethatunlikeexistingEarley-styleparsingal-gorithmsforTAGs,thepresentalgorithmisanin-stantiationofageneralschemathatappliestopars-ingwithmorepowerfulgrammarformalismsaswellastogenerationwithMontaguesemantics.8ConclusionOurreductiontoDatalogbringssophisticatedtech-niquesforDatalogqueryevaluationtotheproblems183

ofparsingandgeneration,andestablishesatightboundonthecomputationalcomplexityofrecogni-tionforawiderangeofgrammars.Inparticular,itshowsthattheuseofhigher-orderλ-termsforse-manticrepresentationneednotbeavoidedforthepurposeofachievingcomputationaltractability.ReferencesAoto,Takahito.1999.Uniquenessofnormalproofsinimplicationalintuitionisticlogic.JournalofLogic,LanguageandInformation8,217–242.Aoto,TakahitoandHiroakiraOno.1994.Uniquenessofnormalproofsin{→,∧}-fragmentofNJ.ResearchRe-portIS-RR-94-0024F.SchoolofInformationScience,JapanAdvancedInstituteofScienceandTechnology.Beeri,CatrielandRaghuRamakrishnan.1991.Onthepowerofmagic.JournalofLogicProgramming10,255–299.Engelfriet,J.andE.M.Schmidt.1977.IOandOI,partI.TheJournalofComputerandSystemSciences15,328–353.Engelfriet,Joost.1986.Thecomplexityoflanguagesgeneratedbyattributegrammars.SIAMJournalonComputing15,70–86.Fisher,MichaelJ.1968.GrammarswithMacro-LikeProductions.Ph.D.dissertation.HarvardUniversity.Gottlob,Georg,NicolaLenoe,FrancescoScarcello.2002.ComputingLOGCFLcertiﬁcates.TheoreticalComputerScience270,761–777.deGroote,Philippe.2001.Towardsabstractcatego-rialgrammars.InAssociationforComputationalLin-guistics,39thAnnualMeetingand10thConferenceoftheEuropeanChapter,ProceedingsoftheConference,pages148–155.deGroote,Philippe.2002.Tree-adjoininggram-marsasabstractcategorialgrammars.InProceed-ingsoftheSixthInternationalWorkshoponTreeAd-joiningGrammarandRelatedFrameworks(TAG+6),pages145–150.Universit´adiVenezia.deGroote,PhilippeandSylvainPogodalla.2004.Ontheexpressivepowerofabstractcategorialgrammars:Representingcontext-freeformalisms.JournalofLogic,LanguageandInformation13,421–438.Hindley,J.Roger.1997.BasicSimpleTypeTheory.Cambridge:CambridgeUniversityPress.AravindK.JoshiandYvesSchabes.1997.Tree-adjoininggrammars.InGrzegozRozenbergandArtoSalomaa,editors,HandbookofFormalLanguages,Vol.3,pages69–123.Berlin:Springer.Kanazawa,MakotoandRyoYoshinaka.2005.Lexi-calizationofsecond-orderACGs.NIITechnicalRe-port.NII-2005-012E.NationalInstituteofInformat-ics,Tokyo.Kanellakis,ParisC.1988.Logicprogrammingandparallelcomplexity.InJackMinker,editor,Foun-dationsofDeductiveDatabasesandLogicProgram-ming,pages547–585.LosAltos,CA:MorganKauf-mann.Mints,Grigori.2000.AShortIntroductiontoIntuitionis-ticLogic.NewYork:KluwerAcademic/PlenumPub-lishers.Moore,RobertC.2002.Acomplete,eﬃcientsentence-realizationalgorithmforuniﬁcationgrammar.InPro-ceedings,InternationalNaturalLanguageGenerationConference,Harriman,NewYork,pages41–48.Salvati,Sylvain.2005.Probl`emesdeﬁltrageetprobl`emesd’analysepourlesgrammairescat´egoriellesabstraites.Doctoraldissertation,l’InstitutNationalPolytechniquedeLorraine.Salvati,Sylvain.2007.EncodingsecondorderstringACGwithdeterministictreewalkingtransducers.InShulyWintner,editor,ProceedingsofFG2006:The11thconferenceonFormalGrammar,pages143–156.FGOnlineProceedings.Stanford,CA:CSLIPublica-tions.Seki,Hiroyuki,TakashiMatsumura,MamoruFujii,andTadaoKasami.1991.Onmultiplecontext-freegram-mars.TheoreticalComputerScience88,191–229.Shieber,StuartM.,YvesSchabes,andFernandoC.N.Pereira.1995.Principlesandimplementationsofde-ductiveparsing.JournalofLogicProgramming24,3–36.Sikkel,Klaas.1997.ParsingSchemata.Berlin:Springer.Sørensen,MortenHeineandPawełUrzyczyn.2006.LecturesontheCurry-HowardIsomorphism.Ams-terdam:Elsevier.Ullman,JeﬀreyD.1988.PrinciplesofDatabaseandKnowledge-BaseSystems.VolumeI.Rockville,MD.:ComputerSciencePress.Ullman,JeﬀreyD.1989a.Bottom-upbeatstop-downforDatalog.InProceedingsoftheEighthACMSIGACT-SIGMOD-SIGARTSymposiumonPrinciplesofDatabaseSystems,Philadelphia,pages140–149.Ullman,JeﬀreyD.1989b.PrinciplesofDatabaseandKnowledge-BaseSystems.VolumeII:TheNewTech-nologies.Rockville,MD.:ComputerSciencePress.Ullman,JeﬀreyD.andAllenVanGelder.1988.Par-allelcomplexityoflogicalqueryprograms.Algorith-mica3,5–42.DavidJ.Weir.1988.CharacterizingMildlyContext-SensitiveGrammarFormalisms.Ph.D.dissertation.UniversityofPennsylvania.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 184–191,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

184

OptimizingGrammarsforMinimumDependencyLengthDanielGildeaComputerScienceDept.UniversityofRochesterRochester,NY14627DavidTemperleyEastmanSchoolofMusicUniversityofRochesterRochester,NY14604AbstractWeexaminetheproblemofchoosingwordorderforasetofdependencytreessoastominimizetotaldependencylength.Wepresentanalgorithmforcomputingtheop-timallayoutofasingletreeaswellasanumericalmethodforoptimizingagram-maroforderingsoverasetofdependencytypes.AgrammargeneratedbyminimizingdependencylengthinunorderedtreesfromthePennTreebankisfoundtoagreesurpris-inglywellwithEnglishwordorder,suggest-ingthatdependencylengthminimizationhasinﬂuencedtheevolutionofEnglish.1IntroductionDependencyapproachestolanguageassumethatev-erywordinasentenceisthedependentofoneotherword(exceptforoneword,whichistheglobalheadofthesentence),sothatthewordsofasentenceformanacyclicdirectedgraph.Animportantprincipleoflanguage,supportedbyawiderangeofevidence,isthatthereispreferencefordependenciestobeshort.Thishasbeenofferedasanexplanationfornumer-ouspsycholinguisticphenomena,suchasthegreaterprocessingdifﬁcultyofobjectrelativeclausesver-sussubjectrelativeclauses(Gibson,1998).Depen-dencylengthminimizationisalsoafactorinambi-guityresolution:listenersprefertheinterpretationwithshorterdependencies.Statisticalparsersmakeuseoffeaturesthatcapturedependencylength(e.g.anadjacencyfeatureinCollins(1999),moreexplicitlengthfeaturesinMcDonaldetal.(2005)andEisnerandSmith(2005))andthuslearntofavorparseswithshorterdependencies.InthispaperweattempttomeasuretheextenttowhichbasicEnglishwordorderchoosestominimizedependencylength,ascomparedtoaveragedepen-dencylengthsunderotherpossiblegrammars.Weﬁrstpresentalinear-timealgorithmforﬁndingtheorderingofasingledependencytreewithshortesttotaldependencylength.Then,giventhatwordor-dermustalsobedeterminedbygrammaticalrela-tions,weturntotheproblemofspecifyingagram-marintermsofconstraintsoversuchrelations.Wewishtoﬁndthesetoforderingconstraintsondepen-dencytypesthatminimizesacorpus’stotaldepen-dencylength.Evenassumingthatdependencytreesmustbeprojective,thisproblemisNP-complete,1butweﬁndthatnumericaloptimizationtechniquesworkwellinpractice.Wereorderunordereddepen-dencytreesextractedfromcorporaandcomparetheresultstoEnglishintermsofboththeresultingde-pendencylengthandthestringsthatareproduced.TheoptimizedorderconstraintsshowahighdegreeofsimilaritytoEnglish,suggestingthatdependencylengthminimizationhasinﬂuencedthewordorderchoicesofbasicEnglishgrammar.2TheDependencyLengthPrincipleThisideathatdependencylengthminimizationmaybeageneralprincipleinlanguagehasbeendis-cussedbymanyauthors.Oneexampleconcernsthe1Englishhascrossing(non-projective)dependencies,buttheyarebelievedtobeveryinfrequent.McDonaldetal.(2005)reportthateveninCzech,commonlyviewedasanon-projectivelanguage,fewerthan2%ofdependenciesviolatetheprojectiv-ityconstraint.185

well-knownprinciplethatlanguagestendtobepre-dominantly“head-ﬁrst”(inwhichtheheadofeachdependencyisontheleft)or“head-last”(whereitisontheright).Frazier(1985)suggeststhatthismightservethefunctionofkeepingheadsandde-pendentsclosetogether.Inasituationwhereeachwordhasexactlyonedependent,itcanbeseenthata“head-ﬁrst”arrangementachievesminimaldepen-dencylength,aseachlinkhasalengthofone.Wewillcallahead-ﬁrstdependency“right-branching”andahead-lastdependency“left-branching”;alanguageinwhichmostorallde-pendencieshavethesamebranchingdirectionisa“same-branching”language.Anotherexampleofdependencylengthmini-mizationconcernssituationswhereaheadhasmul-tipledependents.Insuchcases,dependencylengthwillbeminimizediftheshorterdependentisplacedclosertothehead.Hawkins(1994)hasshownthatthisprincipleisreﬂectedingrammaticalrulesacrossmanylanguages.Itisalsoreﬂectedinsituationsofchoice;forexample,incaseswhereaverbisfol-lowedbyaprepositionalphraseandadirectobjectNP,thedirectobjectNPwillusuallybeplacedﬁrst(closertotheverb)butifitislongerthanthePP,itisoftenplacedsecond.Whileonemightsupposethata“same-branching”languageisoptimalfordependency-lengthminimization,thisisnotinfactthecase.Ifawordhasseveraldependents,placingthemallonthesamesidecausesthemtogetinthewayofeachother,sothatamore’balanced”conﬁguration–withsomedependentsoneachside–haslowertotaldependencylength.Itisparticularlydesirableforoneormoreone-worddependentphrasestobe“opposite-branching”(inrelationtotheprevailingbranchingdirectionofthelanguage);opposite-branchingofalongphrasetendstocausealongdependencyfromtheheadofthephrasetotheexternalhead.ExactlythispatternhasbeenobservedbyDryer(1992)innaturallanguages.Dryerarguesthat,whilemostlanguageshaveapredominantbranch-ingdirection,phrasal(multi-word)dependentstendtoadheretothisprevailingdirectionmuchmoreconsistentlythanone-worddependents,whichfre-quentlybranchoppositetotheprevailingdirectionofthelanguage.Englishreﬂectsthispatternquitekw0w1w2w3w4w5w6w7w8Figure1:Separatingadependencylinkintotwopiecesatasubtreeboundary.strongly:Whilealmostallphrasaldependentsareright-branching(prepositionalphrases,objectsofprepositionsandverbs,relativeclauses,etc.),some1-wordcategoriesareleft-branching,notablydeter-miners,nounmodiﬁers,adverbs(sometimes),andattributiveadjectives.Thislinguisticevidencestronglysuggeststhatlanguageshavebeenshapedbyprinciplesofde-pendencylengthminimization.Onemightwon-derhowclosenaturallanguagesaretobeingop-timalinthisregard.Toaddressthisquestion,weextractunordereddependencygraphsfromEnglishandconsiderdifferentalgorithms,whichwecallDe-pendencyLinearizationAlgorithms(DLAs),foror-deringthewords;ourgoalistoﬁndthealgorithmthatisoptimalwithregardtodependencylengthminimization.Webeginwithan“unlabeled”DLA,whichsimplyminimizesdependencylengthwithoutrequiringconsistentorderingofsyntacticrelations.Wethenconsiderthemorerealisticcaseofa“la-beled”DLA,whichisrequiredtohavesyntacticallyconsistentordering.OnceweﬁndtheoptimalDLA,twoquestionscanbeasked.First,howcloseisdependencylengthinEnglishtothatofthisoptimalDLA?Secondly,howsimilaristheoptimalDLAtoEnglishintermsoftheactualrulesthatarise?3TheOptimalUnlabeledDLAFindinglineararrangementsofgraphsthatminimizetotaledgelengthisaclassicproblem,NP-completeforgeneralgraphsbutwithanO(n1.6)algorithmfortrees(Chung,1984).However,thetraditionalprob-lemdescriptiondoesnottakeintoaccountthepro-jectivityconstraintofdependencygrammar.Thisconstraintsimpliﬁestheproblem;inthissectionweshowthatasimplelinear-timealgorithmisguaran-teedtoﬁndanoptimalresult.Anaturalstrategywouldbetoapplydynamicpro-grammingoverthetreestructure,observingthatto-186

taldependencylengthofalinearizationcanbebro-kenintothesumoflinksbelowanynodewinthetree,andthesumoflinksoutsidethenode,bywhichwemeanalllinksnotconnectedtodependentsofthenode.Thesetwoquantitiesinteractonlythroughthepositionofwrelativetotherestofitsdescendants,meaningthatwecanusethispositionasourdy-namicprogrammingstate,computetheoptimallay-outofeachsubtreegiveneachpositionoftheheadwithinthesubtree,andcombinesubtreesbottom-uptocomputetheoptimallinearizationfortheentiresentence.Thiscanbefurtherimprovedbyobservingthatthetotallengthoftheoutsidelinksdependsonthepositionofwonlybecauseitaffectsthelengthofthelinkconnectingwtoitsparent.Allotheroutsidelinkseithercrossaboveallwordsunderw,andde-pendonlyonthetotalsizeofw’ssubtree,orareen-tirelyononesideofw’ssubtree.Thelinkfromwtoitsparentisdividedintotwopieces,whoselengthsadduptothetotallengthofthelink,byslicingthelinkwhereitcrossestheboundaryfromw’ssubtreetotherestofthesentence.IntheexampleinFig-ure1,thedependencyfromw1tow6hastotallengthﬁve,andisdividedintotwocomponentsoflength2.5attheboundaryofw1’ssubtree.Thelengthofthepieceoverw’ssubtreedependsonw’spositionwithinthatsubtree,whiletheotherpiecedoesnotdependontheinternallayoutofw’ssubtree.Thusthetotaldependencylengthfortheentiresentencecanbedividedinto:1.thelengthofalllinkswithinw’ssubtreeplusthelengthoftheﬁrstpieceofw’slinktoitsparent,i.e.thepiecethatisabovedescendantsofw.2.thelengthoftheremainingpieceofw’slinktoitsparentplusthelengthofalllinksoutsidew.wherethesecondquantitycanbeoptimizedin-dependentlyoftheinternallayoutofw’ssubtree.Whilethelinkfromwtoitsparentmaypointeithertotherightorleft,theoptimallayoutforw’ssubtreegiventhatwattachestoitsleftmustbethemirrorimageoftheoptimallayoutgiventhatwattachestoitsright.Thus,onlyonecaseneedbeconsidered,andtheoptimallayoutfortheentiresentencecanbecomputedfromthebottomupusingjustonedy-namicprogrammingstateforeachnodeinthetree.Wenowgoontoshowthat,incomputingtheor-deringofthedichildrenofagivennode,notalldi!possibilitiesneedbeconsidered.Infact,onecansimplyorderthechildrenbyaddingtheminincreas-ingorderofsize,goingfromtheheadoutwards,andalternatingbetweenaddingtotheleftandrightedgesoftheconstituent.Theﬁrstpartofthisproofistheobservationthat,asweprogressfromtheheadoutward,toeithertheleftortheright,thehead’schildsubtreesmustbeplacedinincreasingorderofsize.Ifanytwoad-jacentchildrenappearwiththesmalleronefurtherfromthehead,wecanswapthepositionsofthesetwochildren,reducingthetotaldependencylengthofthetree.Nolinkscrossingoverthetwochil-drenwillchangeinlength,andnolinkswithinei-therchildwillchange.Thusonlythelengthofthelinksfromthetwochildrenwillchange,andasthelinkconnectingtheoutsidechildnowcrossesoverashorterintermediateconstituent,thetotallengthwilldecrease.Next,weshowthatthetwolongestchildrenmustappearonoppositesidesoftheheadintheoptimallinearization.Toseethis,considerthecasewherebothchildi(thelongestchild)andchildi−1(thesecondlongestchild)appearonthesamesideofthehead.Fromthepreviousresult,weknowthati−1andimustbetheoutermostchildrenontheirside.Iftherearenochildrenontheothersideofthehead,thetreecanbeimprovedbymovingeitheriori−1totheotherside.Ifthereisachildontheothersideofthehead,itmustbesmallerthanbothiandi−1,andthetreecanbeimprovedbyswappingthepositionofthechildfromtheothersideandchildi−1.Giventhatthetwolargestchildrenareoutermostandonoppositesidesofthehead,weobservethatthesumofthetwolinksconnectingthesechildrentotheheaddoesnotdependonthearrangementoftheﬁrsti−2children.Anyrearrangementthatde-creasesthelengthofthelinktotheleftoftheheadmustincreasethelengthofthelinktotherightoftheheadbythesameamount.Thus,theoptimallay-outofallichildrencanbefoundbyplacingthetwolargestchildrenoutermostandonoppositesides,thenexttwolargestchildrennextoutermostandonop-187

Figure2:Placingdependentsonalternatingsidesfrominsideoutinorderofincreasinglength.positesides,andsoonuntilonlyoneorzerochil-drenareleft.Ifthereareanoddnumberofchildren,thesideoftheﬁnal(smallest)childmakesnodiffer-ence,becausetheotherchildrenareevenlybalancedonthetwosidessothelastchildwillhavethesamedependency-lengtheningeffectwhicheversideitison.Ourpairwiseapproachimpliesthattherearemanyoptimallinearizations,2⌊i/2⌋infact,butonesimpleandoptimalapproachistoalternatesidesasinFigure2,puttingthesmallestchildnexttothehead,thenextsmallestnexttotheheadontheop-positeside,thenextoutsidetheﬁrstontheﬁrstside,andsoon.Sofarwehavenotconsideredthepieceofthelinkfromtheheadtoitsparentthatisoverthehead’ssubtree.Theargumentabovecanbegeneralizedbyconsideringthislinkasaspecialchild,longerthanthelongestrealchild.Bymakingthespecialchildthelongestchild,wewillbeguaranteedthatitwillbeplacedontheoutside,asisnecessaryforaprojec-tivetree.Asbefore,thespecialchildandthelongestrealchildmustbeplacedoutermostandonoppo-sitesides,thenexttwolongestchildrenimmediatelywithintheﬁrsttwo,andsoon.Usingthealgorithmfromtheprevioussection,itispossibletoefﬁcientlycomputetheoptimalde-pendencylengthfromEnglishsentences.WetakesentencesfromtheWallStreetJournalsectionofthePennTreebank,extractthedependencytreesus-ingthehead-wordrulesofCollins(1999),considerthemtobeunordereddependencytrees,andlin-earizethemtominimizedependencylength.Au-tomaticallyextractingdependenciesfromtheTree-bankcanleadtosomeerrors,inparticularwithcomplexcompoundnouns.Fortunately,compoundnounstendtooccurattheleavesofthetree,andtheheadrulesarereliableforthevastmajorityofstruc-tures.ResultsinTable1showthatobserveddepen-dencylengthsinEnglisharebetweentheminimumDLALengthOptimal33.7Random76.1Observed47.9Table1:DependencylengthsforunlabeledDLAs.achievablegiventheunordereddependenciesandthelengthwewouldﬁndgivenarandomorder-ing,andaremuchclosertotheminimum.Thisal-readysuggeststhatminimizingdependencylengthhasbeenafactorinthedevelopmentofEnglish.However,theoptimal“language”towhichEnglishisbeingcomparedhaslittleconnectiontolinguis-ticreality.Essentially,thismodelrepresentsafreeword-orderlanguage:Head-modiﬁerrelationsareorientedwithoutregardtothegrammaticalrelationbetweenthetwowords.Infact,however,wordorderinEnglishisrelativelyrigid,andamorerealisticex-perimentwouldbetoﬁndtheoptimalalgorithmthatreﬂectsconsistentsyntacticwordorderrules.Wecallthisa“labeled”DLA,asopposedtothe“unla-beled”DLApresentedabove.4LabeledDLAsInthissection,weconsiderlinearizationalgorithmsthatassumeﬁxedwordorderforagivengrammat-icalrelation,butchoosetheordersuchastomini-mizedependencylengthoveralargenumberofsen-tences.Werepresentgrammaticalrelationssimplybyusingthesyntacticcategoriesofthehighestcon-stituentheadedby(maximalprojectionof)thetwowordsinthedependencyrelation.Duetosparsedataconcerns,weremovedallfunctiontagssuchasTMP(temporal),LOC(locative),andCLR(closelyrelated)fromthetreebank.WemadeanexceptionfortheSBJ(subject)tag,aswethoughtitimportanttodistinguishaverb’ssubjectandobjectforthepur-posesofchoosingwordorder.Lookingataheadanditssetofdependents,thecompleteorderingofallde-pendentscanbemodeledasacontext-freegrammarruleoveranonterminalalphabetofmaximalprojec-tioncategories.Aﬁxedword-orderlanguagewillhaveonlyoneruleforeachsetofnonterminalsap-pearingintheright-handside.SearchingoverallsuchDLAswouldbeexponen-tiallyexpensive,butasimpleapproximationofthe188

Dep.len./DLA%correctorderrandom76.1/40.5extractedfromoptimal61.6/55.4weightsfromEnglish50.9/82.2optimizedweights42.5/64.9Table2:Resultsfordifferentmethodsoflineariz-ingunorderedtreesfromsection0oftheWallStreetJournalcorpus.Eachresultisgivenasaveragede-pendencylengthinwords,followedbythepercent-ageofheads(withatleastonedependent)havingalldependentscorrectlyordered.optimallabeledDLAcanfoundusingthefollowingprocedure:1.ComputetheoptimallayoutofallsentencesinthecorpususingtheunlabeledDLA.2.Foreachcombinationofaheadtypeandasetofchildtypes,counttheoccurrencesofeachordering.3.TakethemostfrequentorderingforeachsetastheorderinthenewDLA.Intheﬁrststepweusedthealternatingprocedurefromtheprevioussection,withamodiﬁcationfortheﬁxedword-orderscenario.Inordertomaketheorderofasubtreeindependentofthedirectioninwhichitattachestoitsparent,dependentswereplacedinorderoflengthonalternatingsidesoftheheadfromtheinsideout,alwaysstartingwiththeshortestdependentimmediatelytotheleftofthehead.ResultsinTable2(ﬁrsttwolines)showthataDLAusingrulesextractedfromtheoptimallayoutmatchesEnglishsigniﬁcantlybetterthanarandomDLA,indicatingthatdependencylengthcanbeusedasageneralprincipletopredictwordorder.4.1AnOptimizedLabeledDLAWhiletheDLApresentedaboveisagooddealbet-terthanrandom(intermsofminimizingdependencylength),thereisnoreasontosupposethatitisopti-mal.InthissectionweaddresstheissueofﬁndingtheoptimallabeledDLA.IfwemodelaDLAasasetofcontext-freegram-marrulesoverdependencytypes,specifyingaﬁxedorderingforanysetofdependencytypesattachingtoagivenhead,thespaceofDLAsisenormous,andtheproblemofﬁndingtheoptimalDLAisadifﬁ-cultone.OnewaytobreaktheproblemdownistomodeltheDLAasasetofweightsforeachtypeofdependencyrelation.Underthismodelthewordorderisdeterminedbyplacingalldependentsofawordinorderofincreasingweightfromlefttoright.ThisreducesthenumberofparametersofthemodeltoT,ifthereareTdependencytypes,fromTkifawordmayhaveuptokdependents.Italsoal-lowsustonaturallycapturestatementssuchas“anounphraseconsistsofadeterminer,then(possi-bly)someadjectives,theheadnoun,andthen(pos-sibly)someprepositionalphrases”,by,forexample,settingtheweightforNP→DTto-2,NP→JJto-1,andNP→PPto1.Weassumetheheaditselfhasaweightofzero,meaningnegativelyweighteddependentsappeartothehead’sleft,andpositivelyweighteddependentstothehead’sright.4.1.1ADLAExtractedfromEnglishAsatestofwhetherthismodelisadequatetorepresentEnglishwordorder,weextractedweightsfortheWallStreetJournalcorpus,usedthemtore-orderthesamesetofsentences,andtestedhowoftenwordswithatleastonedependentwereassignedthecorrectorder.Weextractedtheweightsbyassign-ing,foreachdependencyrelationinthecorpus,anintegeraccordingtoitspositionrelativetothehead,-1fortheﬁrstdependenttotheleft,-2forthesec-ondtotheleft,andsoon.Weaveragedthesenum-bersacrossalloccurrencesofeachdependencytype.Thedependencytypesconsistedofthesyntacticcat-egoriesofthemaximalprojectionsofthetwowordsinthedependencyrelation.ReconstructingthewordorderofeachsentencefromthisweightedDLA,weﬁndthat82%ofallwordswithatleastonedependenthavealldepen-dentsorderedcorrectly(thirdlineofTable2).Thisissigniﬁcantlyhigherthantheheuristicdiscussedintheprevioussection,andprobablyasgoodascanbeexpectedfromsuchasimplemodel,particularlyinlightofthefactthatthereissomechoiceinthewordorderformostsentences(amongadjunctsforexam-ple)andthatthismodeldoesnottakethelengthsof189

theindividualconstituentsintoaccountatall.Wenowwishtoﬁndthesetofweightsthatmin-imizethedependencylengthofthecorpus.Whilethesizeofthesearchspaceisstilltoolargetosearchexhaustively,numericaloptimizationtechniquescanbeappliedtoﬁndanapproximatesolution.4.1.2NP-CompletenessTheproblemofﬁndingtheoptimumweightedDLAforasetofinputtreescanbeshowntobeNP-completebyreducingfromtheproblemofﬁndingagraph’sminimumFeedbackArcSet,oneofthe21classicproblemsofKarp(1972).TheinputtotheFeedbackArcSetproblemisadirectedgraph,forwhichwewishtoﬁndanorderingofverticessuchthatthesmallestnumberofedgespointfromlatertoearlierverticesintheordering.Givenaninstanceofthisproblem,wecancreateasetofdependencytreessuchthateachfeedbackarcintheoriginalgraphcausestotaldependencylengthtoincreasebyone,ifweidentifyeachdependencytypewithavertexintheoriginalproblem,andchooseweightsforthedependencytypesaccordingtothevertexorder.24.1.3LocalSearchOursearchprocedureistooptimizeoneweightatatime,holdingallothersﬁxed,anditeratingthroughthesetofweightstobeset.Theobjectivefunctiondescribingthetotaldependencylengthofthecorpusispiecewiseconstant,asthedependencylengthwillnotchangeuntiloneweightcrossesanother,caus-ingtwodependentstoreverseorder,atwhichpointthetotallengthwilldiscontinuouslyjump.Non-differentiabilityimpliesthatmethodsbasedongra-dientascentwillnotapply.Thissettingisreminis-centoftheproblemofoptimizingfeatureweightsforrerankingofcandidatemachinetranslationout-puts,andweemployanoptimizationtechniquesim-ilartothatusedbyOch(2003)formachinetrans-lation.Becausetheobjectivefunctiononlychangesatpointswhereoneweightcrossesanother’svalue,thesetofsegmentsofweightvalueswithdifferentvaluesoftheobjectivefunctioncanbeexhaustivelyenumerated.Infact,theonlysigniﬁcantpointsarethevaluesofotherweightsfordependencytypeswhichoccurinthecorpusattachedtothesamehead2Weomitdetailsduetospace.TestDataTrainingDataWSJSwbdWSJ42.5/64.912.5/63.6Swbd43.9/59.812.2/58.7Table3:Domaineffectsondependencylengthmin-imization:eachresultisformattedasinTable2.asthedependencybeingoptimized.Webuildata-bleofinteractingdependenciesasapreprocessingsteponthedata,andthenwhenoptimizingaweight,considerthesequenceofvaluesbetweenconsecu-tiveinteractingweights.Whencomputingthetotalcorpusdependencylengthatanewweightvalue,wecanfurtherspeedupcomputationbyreorderingonlythosesentencesinwhichadependencytypeisused,bybuildinganindexofwheredependencytypesoc-curasanotherpreprocessingstep.Thisoptimizationprocessisnotguaranteedtoﬁndtheglobalmaximum(forthisreasonwecalltheresultingDLA“optimized”ratherthan“opti-mal”).Theprocedureisguaranteedtoconvergesim-plyfromthefactthatthereareaﬁnitenumberofobjectivefunctionvalues,andtheobjectivefunctionmustincreaseateachstepatwhichweightsaread-justed.Weranthisoptimizationprocedureonsection2through21oftheWallStreetJournalportionofthePennTreebank,initializingallweightstorandomnumbersbetweenzeroandone.Thisinitializationmakesallphraseshead-initialtobeginwith,andhastheeffectofimposingadirectionalbiasonthere-sultinggrammar.Whenoptimizationconverges,weobtainasetofweightswhichachievesanaveragedependencylengthof40.4onthetrainingdata,and42.5onheld-outdatafromsection0(fourthlineofTable2).WhiletheprocedureisunsupervisedwithrespecttotheEnglishwordorder(otherthanthehead-initialbias),itissupervisedwithrespecttodependencylengthminimization;forthisreasonwereportallsubsequentresultsonheld-outdata.Whilerandominitializationsleadtoaninitialaveragede-pendencylengthvaryingfrom60to73withanaver-ageof66overtenruns,allrunswerewithin±.5ofoneanotheruponconvergence.Whentheorderofwords’dependentswascomparedtotherealwordorderonheld-outdata,weﬁndthat64.9%ofwords190

TrainingSentsDep.len./%correctorder10013.70/54.3850012.81/57.75100012.59/58.01500012.34/55.331000012.27/55.925000012.17/58.73Table4:Averagedependencylengthandruleaccu-racyasafunctionoftrainingdatasize,onSwitch-boarddata.withatleastonedependenthavethecorrectorder.4.2DomainVariationWrittenandspokenlanguagediffersigniﬁcantlyintheirstructure,andoneofthemoststrikingdiffer-encesisthemuchgreateraveragesentencelengthofformalwrittenlanguage.TheWallStreetJournalisnotrepresentativeoftypicallanguageuse.Lan-guagewasnotwrittenuntilrelativelyrecentlyinitsdevelopment,andtheWallStreetJournalinparticu-larrepresentsaformalstylewithmuchlongersen-tencesthanareusedinconversationalspeech.Thechangeinthelengthsofsentencesandtheircon-stituentscouldmaketheoptimizedDLAintermsofdependencylengthverydifferentforthetwogenres.Inordertotestthiseffect,weperformedexper-imentsusingboththeWallStreetJournal(written)andSwitchboard(conversationalspeech)portionsofthePennTreebank,andcomparedresultswithdif-ferenttrainingandtestdata.ForSwitchboard,weusedtheﬁrst50,000sentencesofsections2and3asthetrainingdata,andallofsection4asthetestdata.WeﬁndrelativelylittledifferenceindependencylengthaswevarytrainingdatabetweenwrittenandspokenEnglish,asshowninTable3.Fortheac-curacyoftheresultingwordorder,however,train-ingonWallStreetJournaloutperformsSwitchboardevenwhentestingonSwitchboard,perhapsbecausethelongersentencesinWSJprovidemoreinforma-tionfortheoptimizationproceduretoworkwith.4.3LearningCurveHowmanysentencesarenecessarytolearnagoodsetofdependencyweights?Table4showsresultsforSwitchboardasweincreasethenumberofsen-tencesprovidedasinputtotheweightoptimizationprocedure.WhiletheaveragedependencylengthonLabelInterpretationWeightS→NPverb-objectNP0.037S→NP-SBJverb-subjectNP-0.022S→PPverb-PP0.193NP→DTobjectnoun-determiner-0.070NP-SBJ→DTsubjectnoun-determiner-0.052NP→PPobjnoun-PP0.625NP-SBJ→PPsubjnoun-PP0.254NP→SBARobjnoun-rel.clause0.858NP-SBJ→SBARsubjectnoun-rel.clause-0.110NP→JJobjnoun-adjective0.198NP-SBJ→JJsubjnoun-adjective-0.052Table5:SampleweightsfromoptimizedDLA.Neg-ativelyweighteddependentsappeartotheleftoftheirhead.held-outtestdataslowlydecreaseswithmoredata,thepercentageofcorrectlyordereddependentsislesswell-behaved.Itturnsoutthateven100sen-tencesareenoughtolearnaDLAthatisnearlyasgoodasonederivedfromamuchlargerdataset.4.4ComparingtheOptimizedDLAtoEnglishWehaveseenthattheoptimizedDLAmatchesEn-glishtextmuchbetterthanarandomDLAandthatitachievesonlyaslightlylowerdependencylengththanEnglish.ItisalsoofinteresttocomparetheoptimizedDLAtoEnglishinmoredetail.FirstweexaminetheDLA’stendencytowards“opposite-branching1-wordphrases”.Englishreﬂectsthisprincipletoastrikingdegree:ontheWSJtestset,79.4percentofleft-branchingphrasesare1-word,comparedtoonly19.4percentofright-branchingphrases.TheoptimizedDLAalsoreﬂectsthispat-tern,thoughsomewhatlessstrongly:75.5percentofleft-branchingphrasesare1-word,versus36.7per-centofright-branchingphrases.WecanalsocomparetheoptimizedDLAtoEn-glishwithregardtospeciﬁcrules.Asexplainedear-lier,theoptimalDLA’srulesareexpressedintheformofweightsassignedtoeachrelation,withpos-itiveweightsindicatingright-branchingplacement.Table5showssomeimportantrules.Themiddlecolumnshowsthesyntacticsituationinwhichtherelationnormallyoccurs.Wesee,ﬁrstofall,thatobjectNPsaretotherightoftheverbandsubjectNPsaretotheleft,justlikeinEnglish.PPsarealsotherightofverbs;thefactthattheweightisgreaterthanforNPsindicatesthattheyareplacedfurthertotheright,astheynormallyareinEnglish.Turning191

totheinternalstructureofnounphrases,weseethatdeterminersaretotheleftofbothobjectandsub-jectnouns;PPsaretotherightofbothobjectandsubjectnouns.WealsoﬁndsomedifferenceswithEnglish,however.Clausemodiﬁersofnouns(thesearemostlyrelativeclauses)aretotherightofobjectnouns,asinEnglish,buttotheleftofsubjectnouns;adjectivesaretotheleftofsubjectnouns,asinEn-glish,buttotherightofobjectnouns.Ofcourse,thesedifferencespartlyarisefromthefactthatwetreatNPandNP-SBJasdistinctwhereasEnglishdoesnot(withregardtotheirinternalstructure).5ConclusionInthispaperwehavepresentedadependencylin-earizationalgorithmwhichisoptimizedformini-mizingdependencylength,whilestillmaintainingconsistentpositioningforeachgrammaticalrelation.ThefactthatEnglishissomuchlowerthantherandomDLAsindependencylengthgivessuggeststhatdependencylengthminimizationisanimportantgeneralpreferenceinlanguage.TheoutputoftheoptimizedDLAalsoprovestobemuchmoresimilartoEnglishthanarandomDLAinwordorder.Anin-formalcomparisonofsomeimportantrulesbetweenEnglishandtheoptimalDLArevealsanumberofstrikingsimilarities,thoughalsosomedifferences.ThefactthattheoptimizedDLA’sorderingmatchesEnglishononly65%ofwordsshows,notsurprisingly,thatEnglishwordorderisdeterminedbyotherfactorsinadditiontodependencylengthminimization.Insomecases,orderingchoicesinEnglishareunderdeterminedbysyntacticrules.Forexample,amanneradverbmaybeplacedeitherbe-foretheverborafter(“Heranquickly/hequicklyran”).HeretheoptimizedDLArequiresaconsistentorderingwhileEnglishdoesnot.OnemightsupposethatsuchsyntacticchoicesinEnglishareguidedatleastpartlybydependencylengthminimization,andindeedthereisevidenceforthis;forexample,peopletendtoputtheshorteroftwoPPsclosertotheverb(Hawkins,1994).Buttherearealsootherfactorsin-volved–forexample,thetendencytoput“given”discourseelementsbefore“new”ones,whichhasbeenshowntoplayaroleindependentoflength(Arnoldetal.,2000).Inothercases,theoptimizedDLAallowsmoreﬁne-grainedchoicesthanEnglish.Forexample,theoptimizedDLAtreatsNPandNP-SBJasdifferent;thisallowsittohavedifferentsyntacticrulesforthetwocases–apossibilitythatitsometimesexploits,asseenabove.NodoubtthispartlyexplainswhytheoptimizedDLAachieveslowerdependencylengththanEnglish.AcknowledgmentsThisworkwassupportedbyNSFgrantsIIS-0546554andIIS-0325646.ReferencesJ.E.Arnold,T.Wasow,T.Losongco,andR.Ginstrom.2000.Heavinessvs.newness:theeffectsofstructuralcomplexityanddiscoursestatusonconstituentorder-ing.Language,76:28–55.F.R.K.Chung.1984.Onoptimallineararrangementsoftrees.ComputersandMathematicswithApplications,10:43–60.MichaelJohnCollins.1999.Head-drivenStatisticalModelsforNaturalLanguageParsing.Ph.D.thesis,UniversityofPennsylvania,Philadelphia.MatthewDryer.1992.TheGreenbergianwordordercor-relations.Language,68:81–138.JasonEisnerandNoahA.Smith.2005.Parsingwithsoftandhardconstraintsondependencylength.InProceedingsoftheInternationalWorkshoponParsingTechnologies(IWPT),pages30–41.LynFrazier.1985.Syntacticcomplexity.InD.Dowty,L.Karttunen,andA.Zwicky,editors,NaturalLan-guageParsing:Psychological,Computational,andTheoreticalPerspectives,pages129–189.CambridgeUniversityPress,Cambridge.EdwardGibson.1998.Linguisticcomplexity:Localityofsyntacticdependencies.Cognition,68:1–76.JohnHawkins.1994.APerformanceTheoryofOrderandConstituency.CambridgeUniversityPress,Cam-bridge,UK.RichardM.Karp.1972.Reducibilityamongcombina-torialproblems.InR.E.MillerandJ.W.Thatcher,editors,ComplexityofComputerComputations,pages85–103.RyanMcDonald,FernandoPereira,KirilRibarov,andJanHajiˇc.2005.Non-projectivedependencypars-ingusingspanningtreealgorithms.InProceedingsofHLT/EMNLP.FranzJosefOch.2003.Minimumerrorratetrainingforstatisticalmachinetranslation.InProceedingsofACL-03.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 192–199,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

192

Generalizing Semantic Role Annotations  Across Syntactically Similar Verbs   Andrew S. Gordon Reid Swanson Institute for Creative Technologies Institute for Creative Technologies University of Southern California University of Southern California Marina del Rey, CA 90292 USA Marina del Rey, CA 90292 USA gordon@ict.usc.edu swansonr@ict.usc.edu     Abstract Large corpora of parsed sentences with semantic role labels (e.g. PropBank) pro-vide training data for use in the creation of high-performance automatic semantic role labeling systems. Despite the size of these corpora, individual verbs (or role-sets) often have only a handful of in-stances in these corpora, and only a fraction of English verbs have even a sin-gle annotation. In this paper, we describe an approach for dealing with this sparse data problem, enabling accurate semantic role labeling for novel verbs (rolesets) with only a single training example. Our approach involves the identification of syntactically similar verbs found in Prop-Bank, the alignment of arguments in their corresponding rolesets, and the use of their corresponding annotations in Prop-Bank as surrogate training data. 1 Generalizing Semantic Role Annotations A recent release of the PropBank (Palmer et al., 2005) corpus of semantic role annotations of Tree-bank parses contained 112,917 labeled instances of 4,250 rolesets corresponding to 3,257 verbs, as illustrated by this example for the verb buy.  [arg0 Chuck] [buy.01 bought] [arg1 a car] [arg2 from Jerry] [arg3 for $1000].  Annotations similar to these have been used to cre-ate automated semantic role labeling systems (Pradhan et al., 2005; Moschitti et al., 2006) for use in natural language processing applications that require only shallow semantic parsing. As with all machine-learning approaches, the performance of these systems is heavily dependent on the avail-ability of adequate amounts of training data. How-ever, the number of annotated instances in PropBank varies greatly from verb to verb; there are 617 annotations for the want roleset, only 7 for desire, and 0 for any sense of the verb yearn. Do we need to keep annotating larger and larger cor-pora in order to generate accurate semantic label-ing systems for verbs like yearn? A better approach may be to generalize the data that exists already to handle novel verbs. It is rea-sonable to suppose that there must be a number of verbs within the PropBank corpus that behave nearly exactly like yearn in the way that they relate to their constituent arguments. Rather than annotat-ing new sentences that contain the verb yearn, we could simply find these similar verbs and use their annotations as surrogate training data. This paper describes an approach to generalizing semantic role annotations across different verbs, involving two distinct steps. The first step is to order all of the verbs with semantic role annota-tions according to their syntactic similarity to the target verb, followed by the second step of aligning argument labels between different rolesets. To evaluate this approach we developed a simple automated semantic role labeling algorithm based on the frequency of parse-tree paths, and then compared its performance when using real and sur-rogate training data from PropBank. 193

2 Parse Tree Paths A key concept in understanding our approach to both automated semantic role annotation and gen-eralization is the notion of a parse tree path. Parse tree paths were used for semantic role labeling by Gildea and Jurafsky (2002) as descriptive features of the syntactic relationship between predicates and their arguments in the parse tree of a sentence. Predicates are typically assumed to be specific tar-get words (verbs), and arguments are assumed to be spans of words in the sentence that are domi-nated by nodes in the parse tree. A parse tree path can be described as a sequence of transitions up from the target word then down to the node that dominates the argument span (e.g. Figure 1).   Figure 1: An example parse tree path from the predicate ate to the argument NP He, represented as (cid:2)VB(cid:2)VP(cid:2)S(cid:1)NP  Parse tree paths are particularly interesting for automated semantic role labeling because they generalize well across syntactically similar sen-tences. For example, the parse tree path in Figure 1 would still correctly identify the “eater” argument in the given sentence if the personal pronoun “he” were swapped with a markedly different noun phrase, e.g. “the attendees of the annual holiday breakfast.” 3 A Simple Semantic Role Labeler To explore issues surrounding the generalization of semantic role annotations across verbs, we began by authoring a simple automated semantic role la-beling algorithm that assigns labels according to the frequency of the parse tree paths seen in train-ing data. To construct a labeler for a specific role-set, training data consisting of parsed sentences with role-labeled parse tree constituents are ana-lyzed to identify all of the parse tree paths between predicates and arguments, which are then tabulated and sorted by frequency. For example, Table 1 lists the 10 most frequent pairs of arguments and parse tree paths for the want.01 roleset in a recent release of PropBank.  Count Argument Parse tree path 189 ARG0 (cid:1)VBP(cid:1)VP(cid:1)S(cid:2)NP  159 ARG1 (cid:1)VBP(cid:1)VP(cid:2)S  125 ARG0 (cid:1)VBZ(cid:1)VP(cid:1)S(cid:2)NP  110 ARG1 (cid:1)VBZ(cid:1)VP(cid:2)S  102 ARG0 (cid:1)VB(cid:1)VP(cid:1)VP(cid:1)S(cid:2)NP  98 ARG1 (cid:1)VB(cid:1)VP(cid:2)S  96 ARG0 (cid:1)VBD(cid:1)VP(cid:1)S(cid:2)NP  79 ARGM (cid:1)VB(cid:1)VP(cid:1)VP(cid:2)RB  76 ARG1 (cid:1)VBD(cid:1)VP(cid:2)S  43 ARG1 (cid:1)VBP(cid:1)VP(cid:2)NP  Table 1. Top 10 most frequent parse tree paths for arguments of the PropBank want.01 roleset, based on 617 annotations   To automatically assign role labels to an unla-beled parse tree, each entry in the table is consid-ered in order of highest frequency. Beginning from the target word in the sentence (e.g. wants) a check is made to determine if the entry includes a possi-ble parse tree path in the parse tree of the sentence. If so, then the constituent is assigned the role label of the entry, and all subsequent entries in the table that have the same argument label or lead to sub-constituents of the labeled node are invalidated. Only subsequent entries that assign core arguments of the roleset (e.g. ARG0, ARG1) are invalidated, allowing for multiple assignments of non-core la-bels (e.g. ARGM) to a test sentence. In cases where the path leads to more than one node in a sentence, the leftmost path is selected. This process then continues down the list of valid table entries, assigning additional labels to unlabeled parse tree constituents, until the end of the table is reached. This approach also offers a simple means of dealing with multiple-constituent arguments, which occasionally appear in PropBank data. In these cases, the data is listed as unique entries in the frequency table, where each of the parse tree paths to the multiple constituents are listed as a set. The labeling algorithm will assign the argument of the entry only if all parse tree paths in the set are present in the sentence. The expected performance of this approach to semantic role labeling was evaluated using the PropBank data using a leave-one-out cross-validation experimental design. Precision and re-call scores were calculated for each of the 3,086 194

rolesets with at least two annotations. Figure 2 graphs the average precision, recall, and F-score for rolesets according to the number of training examples of the roleset in the PropBank corpus. An additional curve in Figure 2 plots the percent-age of these PropBank rolesets that have the given amount of training data or more. For example, F-scores above 0.7 are first reached with 62 training examples, but only 8% of PropBank rolesets have this much training data available.   Figure 2. Performance of our semantic role label-ing approach on PropBank rolesets 4 Identifying Syntactically Similar Verbs A key part of generalizing semantic role annota-tions is to calculate the syntactic similarity be-tween verbs. The expectation here is that verbs that appear in syntactically similar contexts are going to behave similarly in the way that they relate to their arguments. In this section we describe a fully automated approach to calculating the syntactic similarity between verbs. Our approach is strictly empirical; the similarity of verbs is determined by examining the syntactic contexts in which they appear in a large text cor-pus. Our approach is analogous to previous work in extracting collocations from large text corpora using syntactic information (Lin, 1998). In our work, we utilized the GigaWord corpus of English newswire text (Linguistic Data Consortium, 2003), consisting of nearly 12 gigabytes of textual data. To prepare this corpus for analysis, we extracted the body text from each of the 4.1 million entries in the corpus and applied a maximum-entropy al-gorithm to identify sentence boundaries (Reynar and Ratnaparkhi, 1997). Next we executed a four-step analysis process for each of the 3,257 verbs in the PropBank cor-pus. In the first step, we identified each of the sen-tences in the prepared GigaWord corpus that contained any inflection of the given verb. To automatically identify all verb inflections, we util-ized the English DELA electronic dictionary (Courtois, 2004), which contained all but 21 of the PropBank verbs (for which we provided the inflec-tions ourselves), with old-English verb inflections removed. We extracted GigaWord sentences con-taining these inflections by using the GNU grep program and a template regular expression for each inflection list. The results of these searches were collected in 3,257 files (one for each verb). The largest of these files was for inflections of the verb say (15.9 million sentences), and the smallest was for the verb namedrop (4 sentences). The second step was to automatically generate syntactic parse trees for the GigaWord sentences found for each verb. It was our original intention to parse all of the found sentences, but we found that the slow speed of contemporary syntactic parsers made this impractical. Instead, we focused our ef-forts on the first 100 sentences found for each of the 3,257 verbs with 100 or fewer tokens: a total of 324,461 sentences (average of 99.6 per verb). For this task we utilized the August 2005 release of the Charniak parser with the default speed/accuracy settings (Charniak, 2000), which required roughly 360 hours of processor time on a 2.5 GHz PowerPC G5. The third step was to characterize the syntactic context of the verbs based on where they appeared within the parse trees. For this purpose, we utilized parse tree paths as a means of converting tree structures into a flat, feature-vector representation. For each sentence, we identified all possible parse tree paths that begin from the verb inflection and terminate at a constituent that does not include the verb inflection. For example, the syntactic context of the verb in Figure 1 can be described by the fol-lowing five parse tree paths: 1. (cid:1)VB(cid:1)VP(cid:1)S(cid:2)NP 2. (cid:1)VB(cid:1)VP(cid:1)S(cid:2)NP(cid:2)PRP 3. (cid:1)VB(cid:1)VP(cid:2)NP 4. (cid:1)VB(cid:1)VP(cid:2)NP(cid:2)DT 5. (cid:1)VB(cid:1)VP(cid:2)NP(cid:2)NN Possible parse tree paths were identified for every parsed sentence for a given verb, and the frequencies of each unique path were tabulated 195

into a feature vector representation. Parse tree paths where the first node was not a Treebank part-of-speech tag for a verb were discarded, effectively filtering the non-verb homonyms of the set of in-flections. The resulting feature vectors were nor-malized by dividing the values of each feature by the number of verb instances used to generate the parse tree paths; the value of each feature indicates the proportion of observed inflections in which the parse tree path is possible. As a representative ex-ample, 95 verb forms of abandon were found in the first 100 GigaWord sentences containing any inflection of this verb. For this verb, 4,472 possible parse tree paths were tabulated into 3,145 unique features, 2501 of which occurred only once. The fourth step was to compute the distance be-tween a given verb and each of the 3,257 feature vector representations describing the syntactic con-text of PropBank verbs. We computed and com-pared the performance of a wide variety of possible vector-based distance metrics, including Euclidean, Manhattan, and Chi-square (with un-normalized frequency counts), but found that the ubiquitous cosine measure was least sensitive to variations in sample size between verbs. To facilitate a com-parative performance evaluation (section 6), pair-wise cosine distance measures were calculated between each pair of PropBank verbs and sorted into individual files, producing 3,257 lists of 3,257 verbs ordered by similarity. Table 2 lists the 25 most syntactically similar pairs of verbs among all PropBank verbs. There are a number of notable observations in this list. First is the extremely high similarity between bind and bound. This is partly due to the fact that they share an inflection (bound is the irregular past tense form of bind), so the first 100 instances of GigaWord sentences for each verb overlap signifi-cantly, resulting in overlapping feature vector rep-resentations. Although this problem appears to be restricted to this one pair of verbs, it could be avoided in the future by using the part-of-speech tag in the parse tree to help distinguish between verb lemmas. A second observation of Table 2 is that several verbs appear multiple times in this list, yielding sets of verbs that all have high syntactic similarity. Three of these sets account for 19 of the verbs in this list: 1. plunge, tumble, dive, jump, fall, fell, dip 2. assail, chide, lambaste 3. buffet, embroil, lock, superimpose, whip-saw, pluck, whisk, mar, ensconce The appearance of these sets suggests that our method of computing syntactic similarity could be used to identify distinct clusters of verbs that be-have in very similar ways. In future work, it would be particularly interesting to compare empirically-derived verb clusters to verb classes derived from theoretical considerations (Levin, 1993), and to the automated verb classification techniques that use these classes (Joanis and Stevenson, 2003). A third observation of Table 2 is that the verb pairs with the highest syntactic similarity are often synonyms, e.g. the cluster of assail, chide, and lambaste. As a striking example, the 14 most syn-tactically similar verbs to believe (in order) are think, guess, hope, feel, wonder, theorize, fear, reckon, contend, suppose, understand, know, doubt, and suggest – all mental action verbs. This observation further supports the distributional hy-pothesis of word similarity and corresponding technologies for identifying synonyms by similar-ity of lexical-syntactic context (Lin, 1998).    Verb pairs (instances) Cosine bind (83) bound (95) 0.950 plunge (94) tumble (87) 0.888 dive (36) plunge (94) 0.867 dive (36) tumble (87) 0.866 jump (79) tumble (87) 0.865 fall (84) fell (102) 0.859 intersperse (99) perch (81) 0.859 assail (100) chide (98) 0.859 dip (81) fell (102) 0.858 buffet (72) embroil (100) 0.856 embroil (100) lock (73) 0.856 embroil (100) superimpose (100) 0.856 fell (102) jump (79) 0.855 fell (102) tumble (87) 0.855 embroil (100) whipsaw (63) 0.850 pluck (100) whisk (99) 0.849 acquit (100) hospitalize (99) 0.849 disincline (70) obligate (94) 0.848 jump (79) plunge (94) 0.848 dive (36) jump (79) 0.847 assail (100) lambaste (100) 0.847 festoon (98) strew (100) 0.846 mar (78) whipsaw (63) 0.846 pluck (100) whipsaw (63) 0.846 ensconce (101) whipsaw (63) 0.845 Table 2. Top 25 most syntactically similar pairs of the 3257 verbs in PropBank. Each verb is listed with the number of inflection instances used to calculate the cosine measurement. 196

5 Aligning Arguments Across Rolesets The second key aspect of our approach to general-izing annotations is to make mappings between the argument roles of the novel target verb and the roles used for a given roleset in the PropBank cor-pus. For example, if we’d like to apply the training data for a roleset of the verb desire in PropBank to a novel roleset for the verb yearn, we need to know that the desirer corresponds to the yearner, the de-sired to the yearned-for, etc. In this section, we describe an approach to argument alignment that involves the application of the semantic role label-ing approach described in section 3 to a single training example for the target verb. To simplify the process of aligning argument la-bels across rolesets, we make a number of assump-tions. First, we only consider cases where two rolesets have exactly the same number of argu-ments. The version of the PropBank corpus that we used in this research contained 4250 rolesets, each with 6 or fewer roles (typically two or three). Ac-cordingly, when attempting to apply PropBank data to a novel roleset with a given argument count (e.g. two), we only consider the subset of Prop-Bank data that labels rolesets with exactly the same count. Second, our approach requires at least one fully-annotated training example for the target roleset. A fully-annotated sentence is one that contains a la-beled constituent in its parse tree for each role in the roleset. As an illustration, the example sentence in section 1 (for the roleset buy.01) would not be considered a fully-annotated training example, as only four of the five arguments of the PropBank buy.01 roleset are present in the sentence (it is missing a benefactor, as in “Chuck bought his mother a car from Jerry for $1000”). In both of these simplifying requirements, we ignore role labels that may be assigned to a sen-tence but that are not defined as part of the roleset, specifically the ARGM labels used in PropBank to label standard proposition modifiers (e.g. location, time, manner).  Our approach begins with a list of verbs ordered by their calculated syntactic similarity to the target verb, as described in section 4 of this paper. We subsequently apply two steps that transform this list into an ordered set of rolesets that can be aligned with the roles used in one or more fully-annotated training examples of the target verb. In describing these two steps, we use instigate as an example target verb. Instigate already appears in the PropBank corpus as a two-argument roleset, but it has only a single training example:  [arg0 The Mahatma, or "great souled one,"] [instigate.01 instigated] [arg1 several campaigns of passive resistance against the British government in India].  The syntactic similarity of instigate to all Prop-Bank verbs was calculated in the manner described in the previous section. This resulting list of 3,180 entries begins with the following fourteen verbs: orchestrate, misrepresent, summarize, wreak, rub, chase, refuse, embezzle, harass, spew, thrash, un-earth, snub, and erect. The first step is to replace each of the verbs in the ordered list with corresponding rolesets from PropBank that have the same number of roles as the target verb. As an example, our target roleset for the verb instigate has two arguments, so each verb in the ordered list is replaced with the set of corresponding rolesets that also have two argu-ments, or removed if no two-argument rolesets exist for the verb in the PropBank corpus. The or-dered list of verbs for instigate is transformed into an ordered list of 2,115 rolesets with two argu-ments, beginning with the following five entries: orchestrate.01, chase.01, unearth.01, snub.01, and erect.01.  The second step is to identify the alignments be-tween the arguments of the target roleset and each of the rolesets in the ordered list. Beginning with the first roleset on the list (e.g. orchestrate.01), we build a semantic role labeler (as described in sec-tion 3) using its available training annotations from the PropPank corpus. We then apply this labeler to the single, fully-annotated example sentence for the target verb, treating it as if it were a test exam-ple of the same roleset. We then check to see if any of the core (numbered) role labels overlap with the annotations that are provided. In cases where an annotated constituent of the target test sentence is assigned a label from the source roleset, then the roleset mappings are noted along with the entry in the ordered list. If no mappings are found, the role-set is removed from the ordered list. For example, the roleset for orchestrate.01 con-tains two arguments (ARG0 and ARG1) that corre-spond to the “conductor, manager” and the “things 197

being coordinated or managed”. This roleset is used for only three sentence annotations in the PropBank corpus. Using these annotations as train-ing data, we build a semantic role labeler for this roleset and apply it to the annotated sentence for instigate.01, treating it as if it were a test sentence for the roleset orchestrate.01. The labeler assigns the orchestrate.01 label ARG1 to the same con-stituent labeled ARG1 in the test sentence, but fails to assign a label to the other argument constituent in the test sentence. Therefore, a single mapping is recorded in the ordered list of rolesets, namely that ARG1 of orchestrate.01 can be mapped to ARG1 of instigate.01. After all of the rolesets are considered, we are left with a filtered list of rolesets with their argu-ment mappings, ordered by their syntactic similar-ity to the target verb. For the roleset instigate.01, this list consists of 789 entries, beginning with the following 5 mappings. 1. orchestrate.01, 1:1 2. chase.01, 0:0, 1:1  3. unearth.01, 0:0, 1:1  4. snub.01, 1:1  5. erect.01, 0:0, 1:1  Given this list, arbitrary amounts of PropBank annotations can be used as surrogate training data for the instigate.01 roleset, beginning at the top of the list. To utilize surrogate training data in our semantic role labeling approach (Section 3), we combine parse tree path information for a selected portion of surrogate training data into a single list sorted by frequency, and apply these files to test sentences as normal.  Although we use an existing PropBank roleset (instigate.01) as an example in this section, this approach will work for any novel roleset where one fully-annotated training example is available. For example, arbitrary amounts of surrogate Prop-Bank data can be found for the novel verb yearn by 1) searching for sentences with the verb yearn in the GigaWord corpus, 2) calculating the syntactic similarity between yearn and all PropBank verbs as described in Section 4, 3) aligning the argu-ments in a single fully-annotated example of yearn with ProbBank rolesets with the same number of arguments using the method described in this sec-tion, and 4) selecting arbitrary amounts of Prop-Bank annotations to use as surrogate training data, starting from the top of the resulting list. 6 Evaluation We conducted a large-scale evaluation to deter-mine the performance of our semantic role labeling algorithm when using variable amounts of surro-gate training data, and compared these results to the performance that could be obtained using vari-ous amounts of real training data (as described in section 3). Our hypothesis was that learning-curves for surrogate-trained labelers would be somewhat less steep, but that the availability of large-amounts of surrogate training data would more than make up for the gap.  To test this hypothesis, we conducted an evalua-tion using the PropBank corpus as our testing data as well as our source for surrogate training data. As described in section 5, our approach requires the availability of at least one fully-annotated sentence for a given roleset. Only 28.5% of the PropBank annotations assign labels for each of the numbered arguments in their given roleset, and only 2,858 of the 4,250 rolesets used in PropBank annotations (66.5%) have at least one fully-annotated sentence. Of these, 2,807 rolesets were for verbs that ap-peared at least once in our analysis of the Giga-Word corpus (Section 4). Accordingly, we evaluated our approach using the annotations for this set of 2,807 rolesets as test data. For each of these rolesets, various amounts of surrogate train-ing data were gathered from all 4,250 rolesets rep-resented in PropBank, leaving out the data for whichever roleset was being tested. For each of the target 2,807 rolesets, we gener-ated a list of semantic role mappings ordered by syntactic similarity, using the methods described in sections 4 and 5. In aligning arguments, only a sin-gle training example from the target roleset was used, namely the first annotation within the Prop-Bank corpus where all of the rolesets arguments were assigned. Our approach failed to identify any argument mappings for 41 of the target rolesets, leaving them without any surrogate training data to utilize. Of the remaining 2,766 rolesets, the num-ber of mapped rolesets for a given target ranged from 1,041 to 1 (mean = 608, stdev = 297). For each of the 2,766 target rolesets with aligna-ble roles, we gathered increasingly larger amounts of surrogate training data by descending the or-dered list of mappings translating the PropBank data for each entry according to its argument map-pings. Then each of these incrementally larger sets 198

of training data was then used to build a semantic role labeler as described in section 3. The perform-ance of each of the resulting labelers was then evaluated by applying it to all of the test data available for target roleset in PropBank, using the same scoring methods described in section 3. The performance scores for each labeler were recorded along with the total number of surrogate training examples used to build the labeler. Figure 3 presents the performance result of our semantic role labeling approach using various amounts of surrogate training data. Along with precision, recall, and F-score data, Figure 3 also graphs the percentage of PropBank rolesets for which a given amount of training data had been identified using our approach, of the 2,858 rolesets with at least one fully-annotated training example. For instance, with 120 surrogate annotations our system achieves an F-score above 0.5, and we identified this much surrogate training data for 96% of PropBank rolesets with at least one fully-annotated sentence. This represents 64% of all PropBank rolesets that are used for annotation. Beyond 120 surrogate training examples, F-scores remain around 0.6 before slowly declining after around 700 examples.   Figure 3. Performance of our semantic role label-ing approach on PropBank rolesets using various amounts of surrogate training data   Several interesting comparisons can be made be-tween the results presented in Figure 3 and those in Figure 2, where actual PropBank training data is used instead of surrogate training data. First, the precision obtained with surrogate training data is roughly 10% lower than with real data. Second, the recall performance of surrogate data performs similar to real data at first, but is consistently 10% lower than with real data after the first 50 training examples. Accordingly, F-scores for surrogate training data are 10% lower overall.  Even though the performance obtained using surrogate training data is less than with actual data, there is abundant amounts of it available for most PropBank rolesets. Comparing the “% of rolesets” plots in Figures 2 and 3, the real value of surrogate training data is apparent. Figure 2 suggests that over 20 real training examples are needed to achieve F-scores that are consistently above 0.5, but that less than 20% of PropBank rolesets have this much data available. In contrast, 64% of all PropBank rolesets can achieve this F-score per-formance with the use of surrogate training data. This percentage increases to 96% if every Prop-Bank roleset is given at least one fully annotated sentence, where all of its numbered arguments are assigned to constituents.  In addition to supplementing the real training data available for existing PropBank rolesets, these results predict the labeling performance that can be obtained by applying this technique to a novel roleset with one fully-annotated training example, e.g. for the verb yearn. Using the first 120 surro-gate training examples and our simple semantic role labeling approach, we would expect F-scores that are above 0.5, and that using the first 700 would yield F-scores around 0.6. 7 Discussion The overall performance of our semantic role la-beling approach is not competitive with leading contemporary systems, which typically employ support vector machine learning algorithms with syntactic features (Pradhan et al., 2005) or syntac-tic tree kernels (Moschitti et al., 2006). However, our work highlights a number of characteristics of the semantic role labeling task that will be helpful in improving performance in future systems. Parse tree paths features can be used to achieve high pre-cision in semantic role labeling, but much of this precision may be specific to individual verbs. By generalizing parse tree path features only across syntactically similar verbs, we have shown that the drop in precision can be limited to roughly 10%. The approach that we describe in this paper is not dependent on the use of PropBank rolesets; any large corpus of semantic role annotations could be 199

generalized in this manner. In particular, our ap-proach would be applicable to corpora with frame-specific role labels, e.g. FrameNet (Baker et al., 1998). Likewise, our approach to generalizing parse tree path feature across syntactically similar verbs may improve the performance of automated semantic role labeling systems based on FrameNet data. Our work suggests that feature generalization based on verb-similarity may compliment ap-proaches to generalization based on role-similarity (Gildea and Jurafsky, 2002; Baldewein et al., 2004). There are a number of improvements that could be made to the approach described in this paper. Enhancements to the simple semantic role labeling algorithm would improve the alignment of argu-ments across rolesets, which would help align role-sets with greater syntactic similarity, as well as improve the performance obtained using the surro-gate training data in assigning semantic roles.  This research raises many questions about the relationship between syntactic context and verb semantics. An important area for future research will be to explore the correlation between our dis-tance metric for syntactic similarity and various quantitative measures of semantic similarity (Pedersen, et al., 2004). Particularly interesting would be to explore whether different senses of a given verb exhibited markedly different profiles of syntactic context. A strong syntactic/semantic cor-relation would suggest that further gains in the use of surrogate annotation data could be gained if syn-tactic similarity was computed between rolesets rather than their verbs. However, this would first require accurate word-sense disambiguation both for the test sentences as well as for the parsed cor-pora used to calculate parse tree path frequencies. Alternatively, parse tree path profiles associated with rolesets may be useful for word sense disam-biguation, where the probability of a sense is com-puted as the likelihood that an ambiguous verb's parse tree paths are sampled from the distributions associated with each verb sense. These topics will be the focus of our future work in this area. Acknowledgments The project or effort depicted was or is sponsored by the U.S. Army Research, Development, and Engineering Command (RDECOM), and that the content or information does not necessarily reflect the position or the policy of the Government, and no official endorsement should be inferred. References  Baker, C., Fillmore, C., and Lowe, J. 1998. The Ber-keley FrameNet Project, In Proceedings of COLING-ACL, Montreal. Baldewein, U., Erk, K., Pado, S., and Prescher, D. 2004. Semantic role labeling with similarity-based gener-alization using EM-based clustering. Proceedings of Senseval-3, Barcelona. Charniak, E. 2000. A maximum-entropy-inspired parser, Proceedings NAACL-ANLP, Seattle. Courtois, B. 2004. Dictionnaires électroniques DELAF anglais et français. In C. Leclère, E. Laporte, M. Piot and M. Silberztein (eds.) Syntax, Lexis and Lexicon-Grammar: Papers in Honour of Maurice Gross. Am-sterdam: John Benjamins. Gildea, D. and Jurafsky, D. 2002. Automatic Labeling of Semantic Roles. Computational Linguistics 28:3, 245-288. Joanis, E. and Stevenson, S. 2003. A general feature space for automatic verb classification. Proceedings EACL, Budapest. Levin, B. 1993. English Verb Classes and Alterna-tions: A Preliminary Investigation. Chicago, IL: University of Chicago Press. Lin, D. 1998. Automatic Retrieval and Clustering of Similar Words. COLING-ACL, Montreal. Linguistic Data Consortium. 2003. English Gigaword. Catalog number LDC2003T05. Available from LDC at http://www.ldc.upenn.edu. Moschitti, A., Pighin, D. and Basili, R. 2006. Semantic Role Labeling via Tree Kernel joint inference. Pro-ceedings of CoNLL, New York. Palmer, M., Gildea, D., and Kingsbury, P. 2005. The Proposition Bank: An Annotated Corpus of Semantic Roles. Computational Linguistics 31(1):71-106. Pedersen, T., Patwardhan, S. and Michelizzi, J. 2004. WordNet::Similarity - Measuring the Relatedness of Concepts. Proceedings NAACL-04, Boston, MA. Pradhan, S., Ward, W., Hacioglu, K., Martin, J., and Jurafsky, D. 2005. Semantic role labeling using dif-ferent syntactic views. Proceedings ACL-2005, Ann Arbor, MI. Reynar, J. and Ratnaparkhi, A. 1997. A Maximum En-tropy Approach to Identifying Sentence Boundaries. Proceedings of ANLP, Washington, D.C. Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 200–207,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

200

A Grammar-driven Convolution Tree Kernel for Se-mantic Role Classification  Min ZHANG1     Wanxiang CHE2     Ai Ti AW1     Chew Lim TAN3     Guodong ZHOU1,4     Ting LIU2     Sheng LI2      1Institute for Infocomm Research   {mzhang, aaiti}@i2r.a-star.edu.sg 2Harbin Institute of Technology {car, tliu}@ir.hit.edu.cn  lisheng@hit.edu.cn 3National University of Singapore tancl@comp.nus.edu.sg 4 Soochow Univ., China 215006 gdzhou@suda.edu.cn      Abstract Convolution tree kernel has shown promis-ing results in semantic role classification. However, it only carries out hard matching, which may lead to over-fitting and less ac-curate similarity measure. To remove the constraint, this paper proposes a grammar-driven convolution tree kernel for semantic role classification by introducing more lin-guistic knowledge into the standard tree kernel. The proposed grammar-driven tree kernel displays two advantages over the pre-vious one: 1) grammar-driven approximate substructure matching and 2) grammar-driven approximate tree node matching. The two improvements enable the grammar-driven tree kernel explore more linguistically motivated structure features than the previ-ous one. Experiments on the CoNLL-2005 SRL shared task show that the grammar-driven tree kernel significantly outperforms the previous non-grammar-driven one in SRL. Moreover, we present a composite kernel to integrate feature-based and tree kernel-based methods. Experimental results show that the composite kernel outperforms the previously best-reported methods. 1 Introduction Given a sentence, the task of Semantic Role Label-ing (SRL) consists of analyzing the logical forms expressed by some target verbs or nouns and some constituents of the sentence. In particular, for each predicate (target verb or noun) all the constituents in the sentence which fill semantic arguments (roles) of the predicate have to be recognized. Typical se-mantic roles include Agent, Patient, Instrument, etc. and also adjuncts such as Locative, Temporal, Manner, and Cause, etc. Generally, semantic role identification and classification are regarded as two key steps in semantic role labeling. Semantic role identification involves classifying each syntactic element in a sentence into either a semantic argu-ment or a non-argument while semantic role classi-fication involves classifying each semantic argument identified into a specific semantic role. This paper focuses on semantic role classification task with the assumption that the semantic arguments have been identified correctly. Both feature-based and kernel-based learning methods have been studied for semantic role classi-fication (Carreras and Màrquez, 2004; Carreras and Màrquez, 2005). In feature-based methods, a flat feature vector is used to represent a predicate-argument structure while, in kernel-based methods, a kernel function is used to measure directly the similarity between two predicate-argument struc-tures. As we know, kernel methods are more effec-tive in capturing structured features. Moschitti (2004) and Che et al. (2006) used a convolution tree kernel (Collins and Duffy, 2001) for semantic role classification. The convolution tree kernel takes sub-tree as its feature and counts the number of common sub-trees as the similarity between two predicate-arguments. This kernel has shown very 201

promising results in SRL. However, as a general learning algorithm, the tree kernel only carries out hard matching between any two sub-trees without considering any linguistic knowledge in kernel de-sign. This makes the kernel fail to handle similar phrase structures (e.g., “buy a car” vs. “buy a red car”) and near-synonymic grammar tags (e.g., the POS variations between “high/JJ degree/NN” 1 and “higher/JJR degree/NN”) 2. To some degree, it may lead to over-fitting and compromise performance. This paper reports our preliminary study in ad-dressing the above issue by introducing more lin-guistic knowledge into the convolution tree kernel. To our knowledge, this is the first attempt in this research direction. In detail, we propose a gram-mar-driven convolution tree kernel for semantic role classification that can carry out more linguisti-cally motivated substructure matching. Experimental results show that the proposed method significantly outperforms the standard convolution tree kernel on the data set of the CoNLL-2005 SRL shared task. The remainder of the paper is organized as fol-lows: Section 2 reviews the previous work and Sec-tion 3 discusses our grammar-driven convolution tree kernel. Section 4 shows the experimental re-sults. We conclude our work in Section 5. 2 Previous Work Feature-based Methods for SRL: most features used in prior SRL research are generally extended from Gildea and Jurafsky (2002), who used a linear interpolation method and extracted basic flat fea-tures from a parse tree to identify and classify the constituents in the FrameNet (Baker et al., 1998). Here, the basic features include Phrase Type, Parse Tree Path, and Position. Most of the following work focused on feature engineering (Xue and Palmer, 2004; Jiang et al., 2005) and machine learning models (Nielsen and Pradhan, 2004; Pradhan et al., 2005a). Some other work paid much attention to the robust SRL (Pradhan et al., 2005b) and post infer-ence (Punyakanok et al., 2004). These feature-based methods are considered as the state of the art methods for SRL. However, as we know, the stan-dard flat features are less effective in modeling the                                                            1 Please refer to http://www.cis.upenn.edu/~treebank/ for the detailed definitions of the grammar tags used in the paper. 2 Some rewrite rules in English grammar are generalizations of others: for example, “NP(cid:198) DET JJ NN” is a specialized ver-sion of “NP(cid:198) DET NN”. The same applies to POS. The stan-dard convolution tree kernel is unable to capture the two cases. syntactic structured information. For example, in SRL, the Parse Tree Path feature is sensitive to small changes of the syntactic structures. Thus, a predicate argument pair will have two different Path features even if their paths differ only for one node. This may result in data sparseness and model generalization problems. Kernel-based Methods for SRL: as an alternative, kernel methods are more effective in modeling structured objects. This is because a kernel can measure the similarity between two structured ob-jects using the original representation of the objects instead of explicitly enumerating their features. Many kernels have been proposed and applied to the NLP study. In particular, Haussler (1999) pro-posed the well-known convolution kernels for a discrete structure. In the context of it, more and more kernels for restricted syntaxes or specific do-mains (Collins and Duffy, 2001; Lodhi et al., 2002; Zelenko et al., 2003; Zhang et al., 2006) are pro-posed and explored in the NLP domain. Of special interest here, Moschitti (2004) proposed Predicate Argument Feature (PAF) kernel for SRL under the framework of convolution tree kernel. He selected portions of syntactic parse trees as predicate-argument feature spaces, which include salient sub-structures of predicate-arguments, to define convo-lution kernels for the task of semantic role classifi-cation. Under the same framework, Che et al. (2006) proposed a hybrid convolution tree kernel, which consists of two individual convolution kernels: a Path kernel and a Constituent Structure kernel. Che et al. (2006) showed that their method outperformed PAF on the CoNLL-2005 SRL dataset.  The above two kernels are special instances of convolution tree kernel for SRL. As discussed in Section 1, convolution tree kernel only carries out hard matching, so it fails to handle similar phrase structures and near-synonymic grammar tags. This paper presents a grammar-driven convolution tree kernel to solve the two problems 3 Grammar-driven Convolution Tree Kernel 3.1 Convolution Tree Kernel In convolution tree kernel (Collins and Duffy, 2001), a parse tree T is represented by a vector of integer counts of each sub-tree type (regardless of its ancestors): ()Tφ=( …, # subtreei(T), …), where 202

# subtreei(T) is the occurrence number of the ith sub-tree type (subtreei) in T. Since the number of different sub-trees is exponential with the parse tree size, it is computationally infeasible to directly use the feature vector()Tφ. To solve this computa-tional issue, Collins and Duffy (2001) proposed the following parse tree kernel to calculate the dot product between the above high dimensional vec-tors implicitly. 1122112212121212(,)(),() ()() (,)(()())iisubtreesubtreeinNnNnNnNKTTTTInInnnφφ∈∈∈∈=<>==∆⋅∑∑∑∑∑ where N1 and N2 are the sets of nodes in trees T1 and T2, respectively, and ()isubtreeIn is a function that is 1 iff the subtreei occurs with root at node n and zero otherwise, and 12(,)nn∆ is the number of the com-mon subtrees rooted at n1 and n2, i.e.,  1212(,)()()iisubtreesubtreeinnInIn∆=⋅∑ 12(,)nn∆can be further computed efficiently by the following recursive rules: Rule 1: if the productions (CFG rules) at 1n and 2n are different, 12(,)0nn∆=; Rule 2: else if both1n and 2n are pre-terminals (POS tags), 12(,)1nnλ∆=×; Rule 3: else,  1()12121(,)(1((,),(,)))ncnjnnchnjchnjλ=∆=+∆∏,  where 1()ncnis the child number of 1n, ch(n,j) is the jth child of node n andλ(0<λ<1) is the decay factor in order to make the kernel value less vari-able with respect to the subtree sizes. In addition, the recursive Rule 3 holds because given two nodes with the same children, one can construct common sub-trees using these children and com-mon sub-trees of further offspring. The time com-plexity for computing this kernel is12(||||)ONN⋅. 3.2 Grammar-driven Convolution Tree Kernel This Subsection introduces the two improvements and defines our grammar-driven tree kernel.  Improvement 1: Grammar-driven approximate matching between substructures. The conven-tional tree kernel requires exact matching between two contiguous phrase structures. This constraint may be too strict. For example, the two phrase structures “NP(cid:198)DT JJ NN” (NP(cid:198)a red car) and “NP(cid:198)DT NN” (NP->a car) are not identical, thus they contribute nothing to the conventional kernel although they should share the same semantic role given a predicate. In this paper, we propose a grammar-driven approximate matching mechanism to capture the similarity between such kinds of quasi-structures for SRL. First, we construct reduced rule set by defining optional nodes, for example, “NP->DT [JJ] NP” or “VP-> VB [ADVP]  PP”, where [*] denotes op-tional nodes. For convenience, we call “NP-> DT JJ NP” the original rule and “NP->DT [JJ] NP” the reduced rule. Here, we define two grammar-driven criteria to select optional nodes: 1) The reduced rules must be grammatical. It means that the reduced rule should be a valid rule in the original rule set. For example, “NP->DT [JJ] NP” is valid only when “NP->DT NP” is a valid rule in the original rule set while “NP->DT [JJ NP]” may not be valid since “NP->DT” is not a valid rule in the original rule set. 2) A valid reduced rule must keep the head child of its corresponding original rule and has at least two children. This can make the reduced rules retain the underlying semantic meaning of their corresponding original rules. Given the reduced rule set, we can then formu-late the approximate substructure matching mecha-nism as follows: 11212,(,)((,))abijijTrrijMrrITTλ+=×∑             (1)  where1ris a production rule, representing a sub-tree of depth one3, and1irTis the ith variation of the sub-tree1rby removing one ore more optional nodes4, and likewise for 2rand2jrT. (,)TI••is a function that is 1 iff the two sub-trees are identical and zero otherwise. 1λ(0≤1λ≤1) is a small penalty to penal-                                                           3 Eq.(1) is defined over sub-structure of depth one. The ap-proximate matching between structures of depth more than one can be achieved easily through the matching of sub-structures of depth one in the recursively-defined convolution kernel. We will discuss this issue when defining our kernel. 4 To make sure that the new kernel is a proper kernel, we have to consider all the possible variations of the original sub-trees. Training program converges only when using a proper kernel. 203

ize optional nodes and the two parameters ia and jbstand for the numbers of occurrence of removed optional nodes in subtrees1irTand 2jrT, respectively. 12(,)Mrrreturns the similarity (ie., the kernel value) between the two sub-trees 1rand2r by sum-ming up the similarities between all possible varia-tions of the sub-trees 1rand2r. Under the new approximate matching mecha-nism, two structures are matchable (but with a small penalty1λ) if the two structures are identical after removing one or more optional nodes. In this case, the above example phrase structures “NP->a red car” and “NP->a car” are matchable with a pen-alty1λ in our new kernel. It means that one co-occurrence of the two structures contributes1λ to our proposed kernel while it contributes zero to the traditional one. Therefore, by this improvement, our method would be able to explore more linguistically appropriate features than the previous one (which is formulated as12(,)TIrr). Improvement 2: Grammar-driven tree nodes ap-proximate matching. The conventional tree kernel needs an exact matching between two (termi-nal/non-terminal) nodes. But, some similar POSs may represent similar roles, such as NN (dog) and NNS (dogs). In order to capture this phenomenon, we allow approximate matching between node fea-tures. The following illustrates some equivalent node feature sets:  • JJ, JJR, JJS • VB, VBD, VBG, VBN, VBP, VBZ • …… where POSs in the same line can match each other with a small penalty 0≤2λ≤1. We call this case node feature mutation. This improvement further generalizes the conventional tree kernel to get bet-ter coverage. The approximate node matching can be formulated as: 21212,(,)((,))abijijfijMffIffλ+=×∑          (2) where1fis a node feature,1ifis the ith mutation of1fandiais 0 iff 1ifand1fare identical and 1 oth-erwise, and likewise for2f. (,)fI••is a function that is 1 iff the two features are identical and zero otherwise. Eq. (2) sums over all combinations of feature mutations as the node feature similarity. The same as Eq. (1), the reason for taking all the possibilities into account in Eq. (2) is to make sure that the new kernel is a proper kernel.  The above two improvements are grammar-driven, i.e., the two improvements retain the under-lying linguistic grammar constraints and keep se-mantic meanings of original rules.  The Grammar-driven Kernel Definition: Given the two improvements discussed above, we can de-fine the new kernel by beginning with the feature vector representation of a parse tree T as follows: ()Tφ=′(# subtree1(T), …, # subtreen(T))       where # subtreei(T) is the occurrence number of the ith sub-tree type (subtreei) in T. Please note that, different from the previous tree kernel, here we loosen the condition for the occurrence of a subtree by allowing both original and reduced rules (Im-provement 1) and node feature mutations (Im-provement 2). In other words, we modify the crite-ria by which a subtree is said to occur. For example, one occurrence of the rule “NP->DT JJ NP” shall contribute 1 times to the feature “NP->DT JJ NP” and1λ times to the feature “NP->DT NP” in the new kernel while it only contributes 1 times to the feature “NP->DT JJ NP” in the previous one. Now we can define the new grammar-driven kernel 12(,)GKTTas follows: 1122112212121212(,)(),()()() (,)(()())iiGsubtreesubtreeinNnNnNnNKTTTTInInnnφφ∈∈∈∈′′=<>′′=′=∆⋅∑∑∑∑∑ (3) where N1 and N2 are the sets of nodes in trees T1 and T2, respectively.()isubtreeIn′ is a function that is 12abλλ•iff the subtreei occurs with root at node n and zero otherwise, whereaandbare the numbers of removed optional nodes and mutated node fea-tures, respectively. 12(,)nn′∆ is the number of the common subtrees rooted at n1 and n2, i.e. ,  1212(,)()()iisubtreesubtreeinnInIn′′′∆=⋅∑        (4) Please note that the value of 12(,)nn′∆is no longer an integer as that in the conventional one since op-tional nodes and node feature mutations are consid-ered in the new kernel. 12(,)nn′∆ can be further computed by the following recursive rules:   204

============================================================================ Rule A: if1nand2nare pre-terminals, then: 1212(,)(,)nnMffλ′∆=×                         (5) where1fand2fare features of nodes1nand 2nre-spectively, and 12(,)Mff is defined at Eq. (2).  Rule B: else if both1nand2nare the same non-terminals, then generate all variations of the subtrees of depth one rooted by1nand 2n(denoted by1nT and2nT respectively) by removing different optional nodes, then:  111212,(,)121(,)((,)   (1((,,),(,,)))abijijTnnijncniknnITTchnikchnjkλλ+=′∆=××′×+∆∑∏(6)  where  • 1inTand2jnTstand for the ith and jth variations in sub-tree set 1nTand2nT, respectively. • (,)TI••is a function that is 1 iff the two sub-trees are identical and zero otherwise.  • iaandjbstand for the number of removed op-tional nodes in subtrees1inTand 2jnT, respectively. • 1(,)ncnireturns the child number of1nin its ith subtree variation 1inT. • 1(,,)chnik is the kth child of node1n in its ith variation subtree 1inT, and likewise for2(,,)chnjk. • Finally, the same as the previous tree kernel, λ(0<λ<1) is the decay factor (see the discussion in Subsection 3.1).  Rule C: else 12(,)0nn′∆=   ============================================================================  Rule A accounts for Improvement 2 while Rule B accounts for Improvement 1. In Rule B, Eq. (6) is able to carry out multi-layer sub-tree approxi-mate matching due to the introduction of the recur-sive part while Eq. (1) is only effective for sub-trees of depth one. Moreover, we note that Eq. (4) is a convolution kernel according to the definition and the proof given in Haussler (1999), and Eqs (5) and (6) reformulate Eq. (4) so that it can be com-puted efficiently, in this way, our kernel defined by Eq (3) is also a valid convolution kernel. Finally, let us study the computational issue of the new convolution tree kernel. Clearly, computing Eq. (6) requires exponential time in its worst case. How-ever, in practice, it may only need  12(||||)ONN⋅. This is because there are only 9.9% rules (647 out of the total 6,534 rules in the parse trees) have op-tional nodes and most of them have only one op-tional node. In fact, the actual running time is even much less and is close to linear in the size of the trees since 12(,)0nn′∆= holds for many node pairs (Collins and Duffy, 2001). In theory, we can also design an efficient algorithm to compute Eq. (6) using a dynamic programming algorithm (Mo-schitti, 2006). We just leave it for our future work. 3.3 Comparison with previous work In above discussion, we show that the conventional convolution tree kernel is a special case of the grammar-driven tree kernel. From kernel function viewpoint, our kernel can carry out not only exact matching (as previous one described by Rules 2 and 3 in Subsection 3.1) but also approximate matching (Eqs. (5) and (6) in Subsection 3.2). From feature exploration viewpoint, although they ex-plore the same sub-structure feature space (defined recursively by the phrase parse rules), their feature values are different since our kernel captures the structure features in a more linguistically appropri-ate way by considering more linguistic knowledge in our kernel design. Moschitti (2006) proposes a partial tree (PT) kernel which can carry out partial matching be-tween sub-trees. The PT kernel generates a much larger feature space than both the conventional and the grammar-driven kernels. In this point, one can say that the grammar-driven tree kernel is a spe-cialization of the PT kernel. However, the impor-tant difference between them is that the PT kernel is not grammar-driven, thus many non-linguistically motivated structures are matched in the PT kernel. This may potentially compromise the performance since some of the over-generated features may possibly be noisy due to the lack of linguistic interpretation and constraint. Kashima and Koyanagi (2003) proposed a con-volution kernel over labeled order trees by general-izing the standard convolution tree kernel. The la-beled order tree kernel is much more flexible than the PT kernel and can explore much larger sub-tree features than the PT kernel. However, the same as the PT kernel, the labeled order tree kernel is not grammar-driven. Thus, it may face the same issues 205

(such as over-generated features) as the PT kernel when used in NLP applications.  Shen el al. (2003) proposed a lexicalized tree kernel to utilize LTAG-based features in parse reranking. Their methods need to obtain a LTAG derivation tree for each parse tree before kernel calculation. In contrast, we use the notion of op-tional arguments to define our grammar-driven tree kernel and use the empirical set of CFG rules to de-termine which arguments are optional. 4 Experiments 4.1 Experimental Setting Data: We use the CoNLL-2005 SRL shared task data (Carreras and Màrquez, 2005) as our experi-mental corpus. The data consists of sections of the Wall Street Journal part of the Penn TreeBank (Marcus et al., 1993), with information on predi-cate-argument structures extracted from the Prop-Bank corpus (Palmer et al., 2005). As defined by the shared task, we use sections 02-21 for training, section 24 for development and section 23 for test. There are 35 roles in the data including 7 Core (A0–A5, AA), 14 Adjunct (AM-) and 14 Reference (R-) arguments. Table 1 lists counts of sentences and arguments in the three data sets.    Training Development TestSentences 39,832 1,346 2,416Arguments 239,858 8,34614,077Table 1: Counts on the data set  We assume that the semantic role identification has been done correctly. In this way, we can focus on the classification task and evaluate it more accu-rately. We evaluate the performance with Accu-racy. SVM (Vapnik, 1998) is selected as our classi-fier and the one vs. others strategy is adopted and the one with the largest margin is selected as the final answer. In our implementation, we use the bi-nary SVMLight (Joachims, 1998) and modify the Tree Kernel Tools (Moschitti, 2004) to a grammar-driven one.  Kernel Setup: We use the Constituent, Predicate, and Predicate-Constituent related features, which are reported to get the best-reported performance (Pradhan et al., 2005a), as the baseline features. We use Che et al. (2006)’s hybrid convolution tree ker-nel (the best-reported method for kernel-based SRL) as our baseline kernel. It is defined as (1) (01)hybridpathcsKKKθθθ=+−≤≤(for the de-tailed definitions of pathKandcsK, please refer to Che et al. (2006)). Here, we use our grammar-driven tree kernel to compute pathKandcsK, and we call it grammar-driven hybrid tree kernel while Che et al. (2006)’s is non-grammar-driven hybrid convo-lution tree kernel.  We use a greedy strategy to fine-tune parameters. Evaluation on the development set shows that our kernel yields the best performance when λ(decay factor of tree kernel), 1λand 2λ(two penalty factors for the grammar-driven kernel), θ(hybrid kernel parameter) and c(a SVM training parameter to balance training error and margin) are set to 0.4, 0.6, 0.3, 0.6 and 2.4, respectively. For other parame-ters, we use default setting. In the CoNLL 2005 benchmark data, we get 647 rules with optional nodes out of the total 6,534 grammar rules and de-fine three equivalent node feature sets as below: • JJ, JJR, JJS • RB, RBR, RBS • NN, NNS, NNP, NNPS, NAC, NX  Here, the verb feature set “VB, VBD, VBG, VBN, VBP, VBZ” is removed since the voice information is very indicative to the arguments of ARG0 (Agent, operator) and ARG1 (Thing operated).  Methods Accuracy (%)  Baseline: Non-grammar-driven 85.21  +Approximate Node Matching 86.27  +Approximate Substructure Matching 87.12  Ours: Grammar-driven Substruc-ture and Node Matching 87.96 Feature-based method with poly-nomial kernel (d = 2) 89.92  Table 2: Performance comparison 4.2 Experimental Results Table 2 compares the performances of different methods on the test set. First, we can see that the new grammar-driven hybrid convolution tree kernel significantly outperforms (2χtest with p=0.05) the 206

non-grammar one with an absolute improvement of 2.75 (87.96-85.21) percentage, representing a rela-tive error rate reduction of 18.6% (2.75/(100-85.21)) . It suggests that 1) the linguistically motivated structure features are very useful for semantic role classification and 2) the grammar-driven kernel is much more effective in capturing such kinds of fea-tures due to the consideration of linguistic knowl-edge. Moreover, Table 2 shows that 1) both the grammar-driven approximate node matching and the grammar-driven approximate substructure matching are very useful in modeling syntactic tree structures for SRL since they contribute relative error rate re-duction of 7.2% ((86.27-85.21)/(100-85.21)) and 12.9% ((87.12-85.21)/(100-85.21)), respectively; 2) the grammar-driven approximate substructure matching is more effective than the grammar-driven approximate node matching. However, we find that the performance of the grammar-driven kernel is still a bit lower than the feature-based method. This is not surprising since tree kernel methods only fo-cus on modeling tree structure information. In this paper, it captures the syntactic parse tree structure features only while the features used in the feature-based methods cover more knowledge sources.  In order to make full use of the syntactic structure information and the other useful diverse flat fea-tures, we present a composite kernel to combine the grammar-driven hybrid kernel and feature-based method with polynomial kernel: (1)     (01)comphybridpolyKKKγγγ=+−≤≤ Evaluation on the development set shows that the composite kernel yields the best performance when γ is set to 0.3. Using the same setting, the system achieves the performance of 91.02% in Accuracy in the same test set. It shows statistically significant improvement (χ2 test with p= 0.10) over using the standard features with the polynomial kernel (γ = 0, Accuracy = 89.92%) and using the grammar-driven hybrid convolution tree kernel (γ = 1, Accuracy = 87.96%). The main reason is that the tree kernel can capture effectively more structure features while the standard flat features can cover some other useful features, such as Voice, SubCat, which are hard to be covered by the tree kernel. The ex-perimental results suggest that these two kinds of methods are complementary to each other. In order to further compare with other methods, we also do experiments on the dataset of English PropBank I (LDC2004T14). The training, develop-ment and test sets follow the conventional split of Sections 02-21, 00 and 23. Table 3 compares our method with other previously best-reported methods with the same setting as discussed previously. It shows that our method outperforms the previous best-reported one with a relative error rate reduction of 10.8% (0.97/(100-91)). This further verifies the effectiveness of the grammar-driven kernel method for semantic role classification.   Method Accuracy (%)Ours (Composite Kernel)      91.97 Moschitti (2006): PAF kernel only    87.7 Jiang et al. (2005): feature based    90.50 Pradhan et al. (2005a): feature based    91.0  Table 3: Performance comparison between our method and previous work  Training Time Method   4 Sections  19 SectionsOurs: grammar-driven tree kernel ~8.1 hours ~7.9 days Moschitti (2006): non-grammar-driven tree kernel ~7.9 hours ~7.1 days  Table 4: Training time comparison  Table 4 reports the training times of the two ker-nels. We can see that 1) the two kinds of convolu-tion tree kernels have similar computing time. Al-though computing the grammar-driven one requires exponential time in its worst case, however, in practice, it may only need 12(||||)ONN⋅ or lin-ear and 2) it is very time-consuming to train a SVM classifier in a large dataset.  5 Conclusion and Future Work In this paper, we propose a novel grammar-driven convolution tree kernel for semantic role classifica-tion. More linguistic knowledge is considered in the new kernel design. The experimental results verify that the grammar-driven kernel is more ef-fective in capturing syntactic structure features than the previous convolution tree kernel because it al-lows grammar-driven approximate matching of substructures and node features. We also discuss the criteria to determine the optional nodes in a 207

CFG rule in defining our grammar-driven convolu-tion tree kernel. The extension of our work is to improve the per-formance of the entire semantic role labeling system using the grammar-driven tree kernel, including all four stages: pruning, semantic role identification, classification and post inference. In addition, a more interesting research topic is to study how to integrate linguistic knowledge and tree kernel methods to do feature selection for tree kernel-based NLP applications (Suzuki et al., 2004). In detail, a linguistics and statistics-based theory that can suggest the effectiveness of different substruc-ture features and whether they should be generated or not by the tree kernels would be worked out. References  C. F. Baker, C. J. Fillmore, and J. B. Lowe. 1998. The Berkeley FrameNet Project. COLING-ACL-1998  Xavier Carreras and Lluıs Màrquez. 2004. Introduction to the CoNLL-2004 shared task: Semantic role labeling. CoNLL-2004  Xavier Carreras and Lluıs Màrquez. 2005. Introduction to the CoNLL-2005 shared task: Semantic role labeling. CoNLL-2005  Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings ofNAACL-2000 Wanxiang Che, Min Zhang, Ting Liu and Sheng Li. 2006. A hybrid convolution tree kernel for semantic role labeling. COLING-ACL-2006(poster) Michael Collins and Nigel Duffy. 2001. Convolution kernels for natural language. NIPS-2001  Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-beling of semantic roles. Computational Linguistics, 28(3):245–288 David Haussler. 1999. Convolution kernels on discrete structures. Technical Report UCSC-CRL-99-10 Zheng Ping Jiang, Jia Li and Hwee Tou Ng. 2005. Se-mantic argument classification exploiting argument interdependence. IJCAI-2005 T. Joachims. 1998. Text Categorization with Support Vecor Machine: learning with many relevant fea-tures. ECML-1998 Kashima H. and Koyanagi T. 2003. Kernels for Semi-Structured Data. ICML-2003 Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello Cristianini and Chris Watkins. 2002. Text classifica-tion using string kernels. Journal of Machine Learn-ing Research, 2:419–444 Mitchell P. Marcus, Mary Ann Marcinkiewicz  and Bea-trice Santorini. 1993. Building a large annotated cor-pus of English: the Penn Treebank. Computational Linguistics, 19(2):313–330 Alessandro Moschitti. 2004. A study on convolution ker-nels for shallow statistic parsing. ACL-2004 Alessandro Moschitti. 2006. Syntactic kernels for natu-ral language learning: the semantic role labeling case. HLT-NAACL-2006 (short paper)  Rodney D. Nielsen and Sameer Pradhan. 2004. Mixing weak learners in semantic parsing. EMNLP-2004 Martha Palmer, Dan Gildea and Paul Kingsbury. 2005. The proposition bank: An annotated corpus of seman-tic roles. Computational Linguistics, 31(1) Sameer Pradhan, Kadri Hacioglu, Valeri Krugler, Wayne Ward, James H. Martin and Daniel Jurafsky. 2005a. Support vector learning for semantic argument classi-fication. Journal of Machine Learning Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James Martin and Daniel Jurafsky. 2005b. Semantic role la-beling using different syntactic views. ACL-2005 Vasin Punyakanok, Dan Roth, Wen-tau Yih and Dav Zi-mak. 2004. Semantic role labeling via integer linear programming inference. COLING-2004 Vasin Punyakanok, Dan Roth and Wen Tau Yih. 2005. The necessity of syntactic parsing for semantic role labeling. IJCAI-2005 Libin Shen, Anoop Sarkar and A. K. Joshi. 2003. Using LTAG based features in parse reranking. EMNLP-03 Jun Suzuki, Hideki Isozaki and Eisaku Maede. 2004. Convolution kernels with feature selection for Natu-ral Language processing tasks. ACL-2004 Vladimir N. Vapnik. 1998. Statistical Learning Theory. Wiley Nianwen Xue and Martha Palmer. 2004. Calibrating features for semantic role labeling. EMNLP-2004 Dmitry Zelenko, Chinatsu Aone, and Anthony Rich-ardella. 2003. Kernel methods for relation extraction. Machine Learning Research, 3:1083–1106 Min Zhang, Jie Zhang, Jian Su and Guodong Zhou. 2006. A Composite Kernel to Extract Relations be-tween Entities with both Flat and Structured Fea-tures. COLING-ACL-2006 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 208–215,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

208

LearningPredictiveStructuresforSemanticRoleLabelingofNomBankChangLiuandHweeTouNgDepartmentofComputerScienceNationalUniversityofSingapore3ScienceDrive2,Singapore117543{liuchan1,nght}@comp.nus.edu.sgAbstractThispaperpresentsanovelapplicationofAlternatingStructureOptimization(ASO)tothetaskofSemanticRoleLabeling(SRL)ofnounpredicatesinNomBank.ASOisarecentlyproposedlinearmulti-tasklearn-ingalgorithm,whichextractsthecommonstructuresofmultipletaskstoimproveaccu-racy,viatheuseofauxiliaryproblems.Inthispaper,weexploreanumberofdifferentauxiliaryproblems,andweareabletosig-niﬁcantlyimprovetheaccuracyoftheNom-BankSRLtaskusingthisapproach.Toourknowledge,ourproposedapproachachievesthehighestaccuracypublishedtodateontheEnglishNomBankSRLtask.1IntroductionThetaskofSemanticRoleLabeling(SRL)istoidentifypredicate-argumentrelationshipsinnaturallanguagetextsinadomain-independentfashion.Inrecentyears,theavailabilityoflargehuman-labeledcorporasuchasPropBank(Palmeretal.,2005)andFrameNet(Bakeretal.,1998)hasmadepossibleastatisticalapproachofidentifyingandclassifyingtheargumentsofverbsinnaturallanguagetexts.AlargenumberofSRLsystemshavebeenevalu-atedandcomparedonthestandarddatasetintheCoNLLsharedtasks(CarrerasandMarquez,2004;CarrerasandMarquez,2005),andmanysystemshaveperformedreasonablywell.ComparedtothepreviousCoNLLsharedtasks(nounphrasebracket-ing,chunking,clauseidentiﬁcation,andnameden-tityrecognition),SRLrepresentsasigniﬁcantsteptowardsprocessingthesemanticcontentofnaturallanguagetexts.Althoughverbsareprobablythemostobviouspredicatesinasentence,manynounsarealsoca-pableofhavingcomplexargumentstructures,oftenwithmuchmoreﬂexibilitythanitsverbcounterpart.Forexample,compareaffectandeffect:[subjAutoprices][arg−extgreatly][predaffect][objthePPI].[subjAutoprices]havea[arg−extbig][predeffect][objonthePPI].The[predeffect][subjofautoprices][objonthePPI]is[arg−extbig].[subjTheautoprices’][predeffect][objonthePPI]is[arg−extbig].Theargumentsofnounpredicatescanoftenbemoreeasilyomittedcomparedtotheverbpredi-cates:The[predeffect][subjofautoprices]is[arg−extbig].The[predeffect][objonthePPI]is[arg−extbig].The[predeffect]is[arg−extbig].WiththerecentreleaseofNomBank(Meyersetal.,2004),itbecomespossibletoapplymachinelearningtechniquestothetask.SofarweareawareofonlyoneEnglishNomBank-basedSRLsystem(JiangandNg,2006),whichusesthemaximumentropyclassiﬁer,althoughsimilareffortsarere-portedontheChineseNomBankby(Xue,2006)209

andonFrameNetby(Pradhanetal.,2004)us-ingasmallsetofhand-selectednominalizations.NounpredicatesalsoappearinFrameNetsemanticrolelabeling(GildeaandJurafsky,2002),andmanyFrameNetSRLsystemsareevaluatedinSenseval-3(Litkowski,2004).SemanticrolelabelingofNomBankisamulti-classclassiﬁcationproblembynature.Usingtheone-vs-allarrangement,thatis,onebinaryclassi-ﬁerforeachpossibleoutcome,theSRLtaskcanbetreatedasmultiplebinaryclassiﬁcationproblems.Inthelatterview,wearepresentedwiththeoppor-tunitytoexploitthecommonstructuresofthesere-latedproblems.Thisisknownasmulti-tasklearninginthemachinelearningliterature(Caruana,1997;Ben-DavidandSchuller,2003;EvgeniouandPon-til,2004;MicchelliandPontil,2005;Maurer,2006).Inthispaper,weapplyAlternatingStructureOp-timization(ASO)(AndoandZhang,2005a)tothesemanticrolelabelingtaskonNomBank.ASOisarecentlyproposedlinearmulti-tasklearningalgo-rithmbasedonempiricalriskminimization.Themethodrequirestheuseofmultipleauxiliaryprob-lems,anditseffectivenessmayvarydependingonthespeciﬁcauxiliaryproblemsused.ASOhasbeenshowntobeeffectiveonthefollowingnatu-rallanguageprocessingtasks:textcategorization,namedentityrecognition,part-of-speechtagging,andwordsensedisambiguation(AndoandZhang,2005a;AndoandZhang,2005b;Ando,2006).Thispapermakestwosigniﬁcantcontributions.First,wepresentanovelapplicationofASOtotheSRLtaskonNomBank.Weexploretheeffectofdifferentauxiliaryproblems,andshowthatlearn-ingpredictivestructureswithASOresultsinsigniﬁ-cantlyimprovedSRLaccuracy.Second,weachieveaccuracyhigherthanthatreportedin(JiangandNg,2006)andadvancethestateoftheartinSRLre-search.Therestofthispaperisorganizedasfollows.WegiveanoverviewofNomBankandASOinSec-tions2and3respectively.Thebaselinelinearclas-siﬁerisdescribedindetailinSection4,followedbythedescriptionoftheASOclassiﬁerinSec-tion5,wherewefocusonexploringdifferentauxil-iaryproblems.WeprovidediscussionsinSection6,presentrelatedworkinSection7,andconcludeinSection8.2NomBankNomBankannotatesthesetofargumentsofnounpredicates,justasPropBankannotatestheargu-mentsofverbpredicates.Asmanynounpredicatesarenominalizations(e.g.,replacementvs.replace),thesameframesaresharedwithPropBankasmuchaspossible,thusachievingsomeconsistencywiththelatterregardingtheacceptedargumentsandthemeaningsofeachlabel.UnlikeinPropBank,argumentsinNomBankcanoverlapwitheachotherandwiththepredicate.Forexample:[locationU.S.][pred,subj,objsteelmakers]havesuppliedthesteel.Herethepredicatemakehassubjectsteelmakersandobjectsteel,analogoustoSteelmakersmakesteel.Thedifferenceisthatheremakeandsteelarebothpartofthewordsteelmaker.EachargumentinNomBankisgivenoneormorelabels,outofthefollowing20:ARG0,ARG1,ARG2,ARG3,ARG4,ARG5,ARG8,ARG9,ARGM-ADV,ARGM-CAU,ARGM-DIR,ARGM-DIS,ARGM-EXT,ARGM-LOC,ARGM-MNR,ARGM-MOD,ARGM-NEG,ARGM-PNC,ARGM-PRD,andARGM-TMP.Thus,theabovesentenceisannotatedinNomBankas:[ARGM-LOCU.S.][PRED,ARG0,ARG1steelmak-ers]havesuppliedthesteel.3AlternatingstructureoptimizationThissectiongivesabriefoverviewofASOasimple-mentedinthiswork.Foramorecompletedescrip-tion,see(AndoandZhang,2005a).3.1Multi-tasklinearclassiﬁerGivenasetoftrainingsamplesconsistingofnfea-turevectorsandtheircorrespondingbinarylabels,fXi,Yigfori2f1,...,ngwhereeachXiisap-dimensionalvector,abinarylinearclassiﬁerat-temptstoapproximatetheunknownrelationbyYi=uTXi.Theoutcomeisconsidered+1ifuTXispos-itive,or–1otherwise.Awell-establishedwaytoﬁndtheweightvectoruisempiricalriskminimiza-tionwithleastsquareregularization:ˆu=argminu1nnXi=1L(cid:0)uTXi,Yi(cid:1)+λkuk2(1)210

FunctionL(p,y)isknownasthelossfunction.Itencodesthepenaltyforagivendiscrepancybe-tweenthepredictedlabelandthetruelabel.Inthiswork,weuseamodiﬁcationofHuber’srobustlossfunction,similartothatusedin(AndoandZhang,2005a):L(p,y)=(cid:0)4pyifpy<(cid:0)1(1(cid:0)py)2if(cid:0)1(cid:20)py<10ifpy(cid:21)1(2)Weﬁxtheregularizationparameterλto10−4,similartothatusedin(AndoandZhang,2005a).Theexpressionkuk2isdeﬁnedasPpi=1u2p.Whenmbinaryclassiﬁcationproblemsaretobesolvedtogether,ah(cid:2)pmatrixΘmaybeusedtocap-turethecommonstructuresofthemweightvectorsulforl2f1,...,mg(h(cid:20)m).WemandatethattherowsofΘbeorthonormal,i.e.,ΘΘT=Ih×h.ThehrowsofΘrepresentthehmostsigniﬁcantcomponentssharedbyalltheu’s.Thisrelationshipismodeledbyul=wl+ΘTvl(3)Theparameters[fwl,vlg,Θ]maythenbefoundbyjointempiricalriskminimizationoverallthemproblems,i.e.,theirvaluesshouldminimizethecombinedempiricalrisk:mXl=1 1nnXi=1L(cid:16)(wl+ΘTvl)TXli,Yli(cid:17)+λkwlk2!(4)3.2TheASOalgorithmAnimportantobservationin(AndoandZhang,2005a)isthatthebinaryclassiﬁcationproblemsusedtoderiveΘarenotnecessarilythoseproblemsweareaimingtosolve.Infact,newproblemscanbeinventedforthesolepurposeofobtainingabetterΘ.Thus,wedistinguishbetweentwotypesofproblemsinASO:auxiliaryproblems,whichareusedtoob-tainΘ,andtargetproblems,whicharetheproblemsweareaimingtosolve1.Forinstance,intheargumentidentiﬁcationtask,theonlytargetproblemistoidentifyargumentsvs.1Notethatthisdeﬁnitiondeviatesslightlyfromtheonein(AndoandZhang,2005a).Weﬁndthedeﬁnitionheremoreconvenientforoursubsequentdiscussion.non-arguments,whereasintheargumentclassiﬁca-tiontask,thereare20binarytargetproblems,onetoidentifyeachofthe20labels(ARG0,ARG1,...).Thetargetproblemscanalsobeusedasanaux-iliaryproblem.Inaddition,wecaninventnewaux-iliaryproblems,e.g.,intheargumentidentiﬁcationstage,wecanpredictwhethertherearethreewordsbetweentheconstituentandthepredicateusingthefeaturesofargumentidentiﬁcation.Assumingtherearektargetproblemsandmaux-iliaryproblems,itisshownin(AndoandZhang,2005a)thatbyperformingoneroundofminimiza-tion,anapproximatesolutionofΘcanbeobtainedfrom(4)bythefollowingalgorithm:1.Foreachofthemauxiliaryproblems,learnulasdescribedby(1).2.FindU=[u1,u2,...,um],ap(cid:2)mmatrix.Thisisasimpliﬁedversionofthedeﬁnitionin(AndoandZhang,2005a),madepossiblebe-causethesameλisusedforallauxiliaryprob-lems.3.PerformSingularValueDecomposition(SVD)onU:U=V1DVT2,whereV1isap(cid:2)mma-trix.TheﬁrsthcolumnsofV1arestoredasrowsofΘ.4.GivenΘ,welearnwandvforeachofthektargetproblemsbyminimizingtheempiricalriskoftheassociatedtrainingsamples:1nnXi=1L(cid:16)(w+ΘTv)TXi,Yi(cid:17)+λkwk2(5)5.Theweightvectorofeachtargetproblemcanbefoundby:u=w+ΘTv(6)Bychoosingaconvexlossfunction,e.g.,(2),steps1and4abovecanbeformulatedasconvexop-timizationproblemsandareefﬁcientlysolvable.TheprocedureabovecanbeconsideredasaPrin-cipalComponentAnalysisinthepredictorspace.Step(3)aboveextractsthemostsigniﬁcantcompo-nentssharedbythepredictorsoftheauxiliaryprob-lemsandhopefully,bythepredictorsofthetarget211

problemsaswell.Thehintofpotentialsigniﬁcantcomponentshelps(5)tooutperformthesimplelin-earpredictor(1).4BaselineclassiﬁerTheSRLtaskistypicallyseparatedintotwostages:argumentidentiﬁcationandargumentclassiﬁcation.Duringtheidentiﬁcationstage,eachconstituentinasentence’sparsetreeislabeledaseitherargumentornon-argument.Duringtheclassiﬁcationstage,eachargumentisgivenoneofthe20possiblelabels(ARG0,ARG1,...).Thelinearclassiﬁerdescribedby(1)isusedasthebaselineinbothstages.Forcomparison,theF1scoresofamaximumentropyclassiﬁerarealsoreportedhere.4.1ArgumentidentiﬁcationEighteenbaselinefeaturesandsixadditionalfea-turesareproposedin(JiangandNg,2006)forNom-Bankargumentidentiﬁcation.AstheimprovementoftheF1scoreduetotheadditionalfeaturesisnotstatisticallysigniﬁcant,weusethesetofeighteenbaselinefeaturesforsimplicity.ThesefeaturesarereproducedinTable1foreasyreference.Unlikein(JiangandNg,2006),wedonotpruneargumentsdominatedbyotherargumentsorthosethatoverlapwiththepredicateinthetrainingdata.Accordingly,wedonotmaximizetheprobabilityoftheentirelabeledparsetreeasin(Toutanovaetal.,2005).Afterthefeaturesofeveryconstituentareextracted,eachconstituentissimplyclassiﬁedinde-pendentlyaseitherargumentornon-argument.Thelinearclassiﬁerdescribedaboveistrainedonsections2to21andtestedonsection23.Amax-imumentropyclassiﬁeristrainedandtestedinthesamemanner.TheF1scoresarepresentedintheﬁrstrowofTable3,incolumnslinearandmaxentrespectively.TheJ&Ncolumnpresentstheresultreportedin(JiangandNg,2006)usingbothbase-lineandadditionalfeatures.Thelastcolumnasopresentsthebestresultfromthiswork,tobeex-plainedinSection5.4.2ArgumentclassiﬁcationInNomBank,someconstituentshavemorethanonelabel.Forsimplicity,wealwaysassignexactlyonelabeltoeachidentiﬁedargumentinthisstep.Forthe0.16%argumentswithmultiplelabelsinthetraining1predthestemmedpredicate2subcatgrammarrulethatexpandsthepredicateP’sparent3ptypesyntacticcategory(phrasetype)oftheconstituentC4hwsyntacticheadwordofC5pathsyntacticpathfromCtoP6positionwhetherCistotheleft/rightoforoverlapswithP7ﬁrstwordﬁrstwordspannedbyC8lastwordlastwordspannedbyC9lsis.ptypephrasetypeofleftsister10rsis.hwrightsister’sheadword11rsis.hw.posPOSofrightsister’sheadword12parent.ptypephrasetypeofparent13parent.hwparent’sheadword14partialpathpathfromCtothelowestcom-monancestorwithP15ptype&lengthofpath16pred&hw17pred&path18pred&positionTable1:Featuresusedinargumentidentiﬁcationdata,wepicktheﬁrstanddiscardtherest.(Notethatthesameisnotdoneonthetestdata.)Adiversesetof28featuresisusedin(JiangandNg,2006)forargumentclassiﬁcation.Inthiswork,thenumberoffeaturesisprunedto11,sothatwecanworkwithreasonablymanyauxiliaryproblemsinlaterexperimentswithASO.Toﬁndasmallersetofeffectivefeatures,westartwithallthefeaturesconsideredin(JiangandNg,2006),in(XueandPalmer,2004),andvariouscom-binationsofthem,foratotalof52features.Thesefeaturesarethenprunedbythefollowingalgorithm:1.Foreachfeatureinthecurrentfeatureset,dostep(2).2.Removetheselectedfeaturefromthefeatureset.ObtaintheF1scoreoftheremainingfea-tureswhenappliedtotheargumentclassiﬁca-tiontask,ondevelopmentdatasection24withgoldidentiﬁcation.3.Selectthehighestofallthescoresobtainedin212

1positiontotheleft/rightoforoverlapswiththepredicate2ptypesyntacticcategory(phrasetype)oftheconstituentC3ﬁrstwordﬁrstwordspannedbyC4lastwordlastwordspannedbyC5rsis.ptypephrasetypeofrightsister6nomtypeNOM-TYPEofpredicatesup-pliedbyNOMLEXdictionary7predicate&ptype8predicate&lastword9morphedpredicatestem&headword10morphedpredicatestem&position11nomtype&positionTable2:Featuresusedinargumentclassiﬁcationstep(2).ThecorrespondingfeatureisremovedfromthecurrentfeaturesetifitsF1scoreisthesameasorhigherthantheF1scoreofretainingallfeatures.4.Repeatsteps(1)-(3)untiltheF1scorestartstodrop.The11featuressoobtainedarepresentedinTa-ble2.Usingthesefeatures,alinearclassiﬁerandamaximumentropyclassiﬁeraretrainedonsections2to21,andtestedonsection23.TheF1scoresarepresentedinthesecondrowofTable3,incolumnslinearandmaxentrespectively.TheJ&Ncolumnpresentstheresultreportedin(JiangandNg,2006).4.3FurtherexperimentsanddiscussionInthecombinedtask,weruntheidentiﬁcationtaskwithgoldparsetrees,andthentheclassiﬁcationtaskwiththeoutputoftheidentiﬁcationtask.Thiswaythecombinedeffectoferrorsfrombothstagesontheﬁnalclassiﬁcationoutputcanbeassessed.ThescoresofthiscompleteSRLsystemarepresentedinthethirdrowofTable3.Totesttheperformanceofthecombinedtaskonautomaticparsetrees,weemploytwodifferentcon-ﬁgurations.First,wetrainthevariousclassiﬁersonsections2to21usinggoldargumentlabelsandautomaticparsetreesproducedbyCharniak’sre-rankingparser(CharniakandJohnson,2005),andtestthemonsection23withautomaticparsetrees.Thisisthesameconﬁgurationasreportedin(Prad-hanetal.,2005;JiangandNg,2006).Thescoresarepresentedinthefourthrowautoparse(t&t)inTable3.Next,wetrainthevariousclassiﬁersonsections2to21usinggoldargumentlabelsandgoldparsetrees.Tominimizethediscrepancybetweengoldandautomaticparsetrees,weremoveallthenodesinthegoldtreeswhosePOSare-NONE-,astheydonotspananywordandarethusnevergeneratedbytheautomaticparser.Theresultingclassiﬁersarethentestedonsection23usingautomaticparsetrees.Thescoresarepresentedinthelastrowautoparse(test)ofTable3.Wenotethatautoparse(test)con-sistentlyoutperformsautoparse(t&t).Webelievethatautoparse(test)isamorerealis-ticsettinginwhichtotesttheperformanceofSRLonautomaticparsetrees.Whenpresentedwithsomepreviouslyunseentestdata,weareforcedtorelyonitsautomaticparsetrees.However,forthebestre-sultsweshouldtakeadvantageofgoldparsetreeswheneverpossible,includingthoseofthelabeledtrainingdata.J&Nmaxentlinearasoidentiﬁcation82.5083.5881.3485.32classiﬁcation87.8088.3587.8689.17combined72.7375.3572.6377.04autoparse(t&t)69.1469.6167.3872.11autoparse(test)-71.1969.0572.83Table3:F1scoresofvariousclassiﬁersonNom-BankSRLOurmaximumentropyclassiﬁerconsistentlyout-performs(JiangandNg,2006),whichalsousesamaximumentropyclassiﬁer.TheprimarydifferenceisthatweusealaterversionofNomBank(Septem-ber2006releasevs.September2005release).Inad-dition,weusesomewhatdifferentfeaturesandtreatoverlappingargumentsdifferently.5ApplyingASOtoSRLOurASOclassiﬁerusesthesamefeaturesasthebaselinelinearclassiﬁer.Thedeﬁningcharacteris-tic,andalsothemajorchallengeinsuccessfullyap-plyingtheASOalgorithmistoﬁndrelatedauxiliaryproblemsthatcanrevealcommonstructuresshared213

withthetargetproblem.ToorganizeoursearchforgoodauxiliaryproblemsforSRL,weseparatethemintotwocategories,unobservableauxiliaryprob-lemsandobservableauxiliaryproblems.5.1UnobservableauxiliaryproblemsUnobservableauxiliaryproblemsareproblemswhosetrueoutcomecannotbeobservedfromarawtextcorpusbutmustcomefromanothersource,e.g.,humanlabeling.Forinstance,predictingtheargumentclass(i.e.,ARG0,ARG1,...)ofacon-stituentisanunobservableauxiliaryproblem(whichisalsotheonlyusableunobservableauxiliaryprob-lemhere),becausethetrueoutcomes(i.e.,theargu-mentclasses)areonlyavailablefromhumanlabelsannotatedinNomBank.Forargumentidentiﬁcation,weinventthefollow-ing20binaryunobservableauxiliaryproblemstotakeadvantageofinformationpreviouslyunusedatthisstage:Topredicttheoutcomeofargumentclassi-ﬁcation(i.e.,ARG0,ARG1,...)usingthefeaturesofargumentidentiﬁcation(pred,subcat,...).Thusforargumentidentiﬁcation,wehave20auxil-iaryproblems(oneauxiliaryproblemforpredictingeachoftheargumentclassesARG0,ARG1,...)andonetargetproblem(predictingwhetheraconstituentisanargument)fortheASOalgorithmdescribedinSection3.2.Intheargumentclassiﬁcationtask,the20binarytargetproblemsarealsotheunobservableauxiliaryproblems(oneauxiliaryproblemforpredictingeachoftheargumentclassesARG0,ARG1,...).Thus,weusethesame20problemsasbothauxiliaryprob-lemsandtargetproblems.WetrainanASOclassiﬁeronsections2to21andtestitonsection23.Withthe20unobservableaux-iliaryproblems,weobtaintheF1scoresreportedinthelastcolumnofTable3.Inalltheexperiments,wekeeph=20,i.e.,allthe20columnsofV1arekept.ComparingtheF1scoreofASOagainstthatofthelinearclassiﬁerineverytask(i.e.,identiﬁcation,classiﬁcation,combined,bothautoparseconﬁgura-tions),theimprovementachievedbyASOisstatis-ticallysigniﬁcant(p<0.05)basedontheχ2test.ComparingtheF1scoreofASOagainstthatofthemaximumentropyclassiﬁer,theimprovementinallbutonetask(argumentclassiﬁcation)isstatisticallysigniﬁcant(p<0.05).Forargumentclassiﬁca-tion,theimprovementisnotstatisticallysigniﬁcant(p=0.08).5.2ObservableauxiliaryproblemsObservableauxiliaryproblemsareproblemswhosetrueoutcomecanbeobservedfromarawtextcor-puswithoutadditionalexternallyprovidedlabels.Anexampleistopredictwhetherhw=traderfromaconstituent’sotherfeatures,sincetheheadwordofaconstituentcanbeobtainedfromtherawtextalone.Bydeﬁnition,anobservableauxiliaryprob-lemcanalwaysbeformulatedaspredictingafea-tureofthetrainingdata.Dependingonwhetherthebaselinelinearclassiﬁeralreadyusesthefeaturetobepredicted,wefacetwopossibilities:PredictingausedfeatureInauxiliaryproblemsofthistype,wemusttakecaretoremovethefeatureitselffromthetrainingdata.Forexample,wemustnotusethefeaturepathorpred&pathtopredictpathitself.PredictinganunusedfeatureTheseauxiliaryproblemsprovideinformationthattheclassiﬁerwaspreviouslyunabletoincorporate.Thedesirablecharacteristicsofsuchafeatureare:1.Thefeature,althoughunused,shouldhavebeenconsideredforthetargetproblemsoitisprob-ablyrelatedtothetargetproblem.2.Thefeatureshouldnotbehighlycorrelatedwithausedfeature,e.g.,sincethelastwordfea-tureisusedinargumentidentiﬁcation,wewillnotconsiderpredictinglastword.posasanaux-iliaryproblem.Eachchosenfeaturecancreatethousandsofbi-naryauxiliaryproblems.E.g.,bychoosingtopre-dicthw,wecancreateauxiliaryproblemspredict-ingwhetherhw=to,whetherhw=trader,etc.Tohavemorepositivetrainingsamples,weonlypredictthemostfrequentfeatures.Thuswewillprobablypredictwhetherhw=to,butnotwhetherhw=trader,sincetooccursmorefrequentlythantraderasaheadword.214

5.2.1ArgumentidentiﬁcationInargumentidentiﬁcationusinggoldparsetrees,weexperimentwithpredictingthreeunusedfeaturesasauxiliaryproblems:distance(distancebetweenthepredicateandtheconstituent),parent.lsis.hw(headwordoftheparentconstituent’sleftsister)andparent.rsis.hw(headwordoftheparentconstituent’srightsister).Wethenexperimentwithpredictingfourusedfeatures:hw,lastword,ptypeandpath.TheASOclassiﬁeristrainedonsections2to21,andtestedonsection23.Duetothelargedatasize,weareunabletousemorethan20binaryauxil-iaryproblemsortoexperimentwithcombinationsofthem.TheF1scoresarepresentedinTable4.5.2.2ArgumentclassiﬁcationInargumentclassiﬁcationusinggoldparsetreesandgoldidentiﬁcation,weexperimentwithpre-dictingthreeunusedfeaturespath,partialpath,andchunkseq(concatenationofthephrasetypesoftextchunksbetweenthepredicateandtheconstituent).Wethenexperimentwithpredictingthreeusedfea-tureshw,lastword,andptype.Combinationsoftheseauxiliaryproblemsarealsotested.Inallcombined,weusetheﬁrst100prob-lemsfromeachofthesixgroupsofobservableaux-iliaryproblems.Inselectedcombined,weusetheﬁrst100problemsfromeachofpath,chunkseq,last-wordandptypeproblems.TheASOclassiﬁeristrainedonsections2to21,andtestedonsection23.TheF1scoresareshowninTable5.featuretobepredictedF120mostfrequentdistances81.4820mostfrequentparent.lsis.hws81.5120mostfrequentparent.rsis.hws81.6020mostfrequenthws81.4020mostfrequentlastwords81.3320mostfrequentptypes81.3520mostfrequentpaths81.47linearbaseline81.34Table4:F1scoresofASOwithobservableauxiliaryproblemsonargumentidentiﬁcation.Allh=20.FromTable4and5,weobservethatalthoughtheuseofobservableauxiliaryproblemsconsis-featuretobepredictedF1300mostfrequentpaths87.97300mostfrequentpartialpaths87.95300mostfrequentchunkseqs88.09300mostfrequenthws87.93300mostfrequentlastwords88.01all63ptypes88.05allcombined87.95selectedcombined88.07linearbaseline87.86Table5:F1scoresofASOwithobservableauxiliaryproblemsonargumentclassiﬁcation.Allh=100.tentlyimprovestheperformanceoftheclassiﬁer,thedifferencesaresmallandnotstatisticallysignif-icant.FurtherexperimentscombiningunobservableandobservableauxiliaryproblemsfailtooutperformASOwithunobservableauxiliaryproblemsalone.Insummary,ourworkshowsthatunobservableauxiliaryproblemssigniﬁcantlyimprovetheperfor-manceofNomBankSRL.Incontrast,observableauxiliaryproblemsarenoteffective.6DiscussionsSomeofourexperimentsarelimitedbytheexten-sivecomputingresourcesrequiredforafullerex-ploration.Forinstance,“predictingunusedfeatures”typeofauxiliaryproblemsmightholdsomehopeforfurtherimprovementinargumentidentiﬁcation,ifalargernumberofauxiliaryproblemscanbeused.ASOhasbeendemonstratedtobeaneffec-tivesemi-supervisedlearningalgorithm(AndoandZhang,2005a;AndoandZhang,2005b;Ando,2006).However,wehavebeenunabletouseun-labeleddatatoimprovetheaccuracy.Onepossiblereasonisthecumulativenoisefromthemanycas-cadingstepsinvolvedinautomaticSRLofunlabeleddata:syntacticparse,predicateidentiﬁcation(whereweidentifynounswithatleastoneargument),ar-gumentidentiﬁcation,andﬁnallyargumentclassi-ﬁcation,whichreducestheeffectivenessofaddingunlabeleddatausingASO.7RelatedworkMulti-outputneuralnetworkslearnseveraltaskssi-multaneously.Inadditiontothetargetoutputs,215

(Caruana,1997)discussesconﬁgurationswherebothusedinputsandunusedinputs(duetoexcessivenoise)areutilizedasadditionaloutputs.Incontrast,ourworkconcernslinearpredictorsusingempiricalriskminimization.Avarietyofauxiliaryproblemsaretestedin(AndoandZhang,2005a;AndoandZhang,2005b)inthesemi-supervisedsettings,i.e.,theirauxiliaryproblemsaregeneratedfromunlabeleddata.Thisdifferssigniﬁcantlyfromthesupervisedsettinginourwork,whereonlylabeleddataisused.While(AndoandZhang,2005b)uses“predictingusedfeatures”(previous/current/nextword)asauxiliaryproblemswithgoodresultsinnamedentityrecog-nition,theuseofsimilarobservableauxiliaryprob-lemsinourworkgivesnostatisticallysigniﬁcantim-provements.Morerecently,forthewordsensedisambiguation(WSD)task,(Ando,2006)experimentedwithbothsupervisedandsemi-supervisedauxiliaryproblems,althoughtheauxiliaryproblemssheusedarediffer-entfromours.8ConclusionInthispaper,wehavepresentedanovelapplicationofAlternatingStructureOptimization(ASO)totheSemanticRoleLabeling(SRL)taskonNomBank.Thepossibleauxiliaryproblemsarecategorizedandtestedextensively.Ourresultsoutperformthosere-portedin(JiangandNg,2006).Tothebestofourknowledge,weachievethehighestSRLaccuracypublishedtodateontheEnglishNomBank.ReferencesR.K.AndoandT.Zhang.2005a.Aframeworkforlearningpredictivestructuresfrommultipletasksandunlabeleddata.JournalofMachineLearningResearch.R.K.AndoandT.Zhang.2005b.Ahigh-performancesemi-supervisedlearningmethodfortextchunking.InProc.ofACL.R.K.Ando.2006.Applyingalternatingstructureoptimizationtowordsensedisambiguation.InProc.ofCoNLL.C.F.Baker,C.J.Fillmore,andJ.B.Lowe.1998.TheBerkeleyFrameNetproject.InProc.ofCOLING-ACL.S.Ben-DavidandR.Schuller.2003.Exploitingtaskrelated-nessformultipletasklearning.InProc.ofCOLT.X.CarrerasandL.Marquez.2004.IntroductiontotheCoNLL-2004sharedtask:Semanticrolelabeling.InProc.ofCoNLL.X.CarrerasandL.Marquez.2005.IntroductiontotheCoNLL-2005sharedtask:Semanticrolelabeling.InProc.ofCoNLL.R.Caruana.1997.MultitaskLearning.Ph.D.thesis,SchoolofComputerScience,CMU.E.CharniakandM.Johnson.2005.Coarse-to-ﬁnen-bestpars-ingandMaxEntdiscriminativereranking.InProc.ofACL.T.EvgeniouandM.Pontil.2004.Regularizedmultitasklearn-ing.InProc.ofKDD.D.GildeaandD.Jurafsky.2002.Automaticlabelingofseman-ticroles.ComputationalLinguistics.Z.P.JiangandH.T.Ng.2006.SemanticrolelabelingofNom-Bank:Amaximumentropyapproach.InProc.ofEMNLP.K.C.Litkowski.2004.Senseval-3task:automaticlabelingofsemanticroles.InProc.ofSENSEVAL-3.A.Maurer.2006.Boundsforlinearmultitasklearning.JournalofMachineLearningResearch.A.Meyers,R.Reeves,C.Macleod,R.Szekeley,V.Zielinska,B.Young,andR.Grishman.2004.TheNomBankproject:Aninterimreport.InProc.ofHLT/NAACLWorkshoponFrontiersinCorpusAnnotation.C.A.MicchelliandM.Pontil.2005.Kernelsformultitasklearning.InProc.ofNIPS.M.Palmer,D.Gildea,andP.Kingsbury.2005.ThePropositionBank:anannotatedcorpusofsemanticroles.ComputationalLinguistics.S.S.Pradhan,H.Sun,W.Ward,J.H.Martin,andD.Jurafsky.2004.ParsingargumentsofnominalizationsinEnglishandChinese.InProc.ofHLT/NAACL.S.Pradhan,K.Hacioglu,V.Krugler,W.Ward,J.H.Martin,andD.Jurafsky.2005.Supportvectorlearningforsemanticargumentclassiﬁcation.MachineLearning.K.Toutanova,A.Haghighi,andC.D.Manning.2005.Jointlearningimprovessemanticrolelabeling.InProc.ofACL.N.XueandM.Palmer.2004.Calibratingfeaturesforsemanticrolelabeling.InProc.ofEMNLP.N.Xue.2006.Semanticrolelabelingofnominalizedpredi-catesinChinese.InProc.ofHLT/NAACL.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 216–223,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

216

ASimple,Similarity-basedModelforSelectionalPreferencesKatrinErkUniversityofTexasatAustinkatrin.erk@mail.utexas.eduAbstractWeproposeanew,simplemodelfortheauto-maticinductionofselectionalpreferences,usingcorpus-basedsemanticsimilaritymetrics.Fo-cusingonthetaskofsemanticrolelabeling,wecomputeselectionalpreferencesforseman-ticroles.Inevaluationsthesimilarity-basedmodelshowslowererrorratesthanbothResnik’sWordNet-basedmodelandtheEM-basedclus-teringmodel,buthascoverageproblems.1IntroductionSelectionalpreferences,whichcharacterizetyp-icalargumentsofpredicates,areaveryuse-fulandversatileknowledgesource.Theyhavebeenusedforexampleforsyntacticdisambigua-tion(HindleandRooth,1993),wordsensedis-ambiguation(WSD)(McCarthyandCarroll,2003)andsemanticrolelabeling(SRL)(GildeaandJurafsky,2002).Thecorpus-basedinductionofselectionalpreferenceswasﬁrstproposedbyResnik(1996).Alllaterapproacheshavefollowedthesametwo-stepprocedure,ﬁrstcollectingargumenthead-wordsfromacorpus,thengeneralizingtoother,similarwords.SomeapproacheshaveusedWordNetforthegeneralizationstep(Resnik,1996;ClarkandWeir,2001;AbeandLi,1993),othersEM-basedclustering(Roothetal.,1999).Inthispaperweproposeanew,simplemodelforselectionalpreferenceinductionthatusescorpus-basedsemanticsimilaritymetrics,suchasCosineorLin’s(1998)mutualinformation-basedmetric,forthegeneralizationstep.Thismodeldoesnotrequireanymanuallycreatedlexicalresources.Inaddition,thecorpusforcomputingthesimilaritymetricscanbefreelychosen,allowinggreatervariationinthedomainofgeneralizationthanaﬁxedlexicalresource.Wefocusononeapplicationofselectionalpreferences:semanticrolelabeling.Thear-gumentpositionsforwhichwecomputeselec-tionalpreferenceswillbesemanticrolesintheFrameNet(Bakeretal.,1998)paradigm,andthepredicatesweconsiderwillbesemanticclassesofwordsratherthanindividualwords(whichmeansthatdiﬀerentpreferenceswillbelearnedfordiﬀerentsensesofapredicateword).InSRL,thetwomostpressingissuestodayare(1)thedevelopmentofstrongsemanticfeaturestocomplementthecurrentmostlysyntactically-basedsystems,and(2)theproblemofthedo-maindependence(CarrerasandMarquez,2005).IntheCoNLL-05sharedtask,participatingsys-temsshowedabout10pointsF-scorediﬀerencebetweenin-domainandout-of-domaintestdata.Concerning(1),wefocusonselectionalprefer-encesasthestrongestcandidateforinformativesemanticfeatures.Concerning(2),thecorpus-basedsimilaritymetricsthatweuseforselec-tionalpreferenceinductionopenupinterestingpossibilitiesofmixingdomains.Weevaluatethesimilarity-basedmodelagainstResnik’sWordNet-basedmodelaswellastheEM-basedclusteringapproach.Intheevaluation,thesimilarity-modelshowslowerer-rorratesthanbothResnik’sWordNet-basedmodelandtheEM-basedclusteringmodel.However,theEM-basedclusteringmodelhashighercoveragethanbothotherparadigms.Planofthepaper.Afterdiscussingprevi-217

ousapproachestoselectionalpreferenceinduc-tioninSection2,weintroducethesimilarity-basedmodelinSection3.Section4describesthedatausedfortheexperimentsreportedinSection5,andSection6concludes.2RelatedWorkSelectionalrestrictionsandselectionalprefer-encesthatpredicatesimposeontheirargumentshavelongbeenusedinsemantictheories,(seee.g.(KatzandFodor,1963;Wilks,1975)).TheinductionofselectionalpreferencesfromcorpusdatawaspioneeredbyResnik(1996).Allsub-sequentapproacheshavefollowedthesametwo-stepprocedure,ﬁrstcollectingargumenthead-wordsfromacorpus,thengeneralizingovertheseenheadwordstosimilarwords.ResnikusestheWordNetnounhierarchyforgeneralization.Hisinformation-theoreticapproachmodelstheselectionalpreferencestrengthofanargumentposition1rpofapredicatepasS(rp)=XcP(c|rp)logP(c|rp)P(c)wherethecareWordNetsynsets.Theprefer-encethatrphasforagivensynsetc0,theselec-tionalassociationbetweenthetwo,isthende-ﬁnedasthecontributionofc0torp’sselectionalpreferencestrength:A(rp,c0)=P(c0|rp)logP(c0|rp)P(c0)S(rp)FurtherWordNet-basedapproachestoselec-tionalpreferenceinductionincludeClarkandWeir(2001),andAbeandLi(1993).Brock-mannandLapata(2003)performacomparisonofWordNet-basedmodels.Roothetal.(1999)generalizeoverseenhead-wordsusingEM-basedclusteringratherthanWordNet.TheymodeltheprobabilityofawordwoccurringastheargumentrpofapredicatepasbeingindependentlyconditionedonasetofclassesC:P(rp,w)=Xc∈CP(c,rp,w)=Xc∈CP(c)P(rp|c)P(w|c)1Wewriterptoindicatepredicate-speciﬁcroles,like“thedirectobjectofcatch”,ratherthanjust“obj”.TheparametersP(c),P(rp|c)andP(w|c)areestimatedusingtheEMalgorithm.Whiletherehavebeennoisolatedcompar-isonsofthetwogeneralizationparadigmsthatweareawareof,GildeaandJurafsky’s(2002)task-basedevaluationhasfoundclustering-basedapproachestohavebettercoveragethanWordNetgeneralization,thatis,foragivenroletherearemorewordsforwhichtheycanstateapreference.3ModelTheapproachweareproposingmakesuseoftwocorpora,aprimarycorpusandagener-alizationcorpus(whichmay,butneednot,beidentical).Theprimarycorpusisusedtoextracttuples(p,rp,w)ofapredicate,anargumentpositionandaseenheadword.Thegeneral-izationcorpusisusedtocomputeacorpus-basedsemanticsimilaritymetric.LetSeen(rp)bethesetofseenheadwordsforanargumentrpofapredicatep.ThenwemodeltheselectionalpreferenceSofrpforapossibleheadwordw0asaweightedsumofthesimilari-tiesbetweenw0andtheseenheadwords:Srp(w0)=Xw∈Seen(rp)sim(w0,w)·wtrp(w)sim(w0,w)isthesimilaritybetweentheseenandthepotentialheadword,andwtrp(w)istheweightofseenheadwordw.Similaritysim(w0,w)willbecomputedonthegeneralizationcorpus,againontheba-sisofextractedtuples(p,rp,w).WewillbeusingthesimilaritymetricsshowninTa-ble1:Cosine,theDiceandJaccardcoeﬃcients,andHindle’s(1990)andLin’s(1998)mutualinformation-basedmetrics.Wewritefforfre-quency,Iformutualinformation,andR(w)forthesetofargumentsrpforwhichwoccursasaheadword.Inthispaperweonlystudycorpus-basedmet-rics.Thesimfunctioncanequallywellbein-stantiatedwithaWordNet-basedmetric(foranoverviewseeBudanitskyandHirst(2006)),butwerestrictourexperimentstocorpus-basedmetrics(a)intheinterestofgreatestpossible218

simcosine(w,w0)=Prpf(w,rp)·f(w0,rp)qPrpf(w,rp)2·qPrpf(w0,rp)2simDice(w,w0)=2·|R(w)∩R(w0)||R(w)|+|R(w0)|simLin(w,w0)=Prp∈R(w)∩R(w0)I(w,r,p)I(w0,r,p)Prp∈R(w)I(w,r,p)Prp∈R(w)I(w0,r,p)simJaccard(w,w0)=|R(w)∩R(w0)||R(w)∪R(w0)|simHindle(w,w0)=PrpsimHindle(w,w0,rp)wheresimHindle(w,w0,rp)=min(I(w,rp),I(w0,rp)ifI(w,rp)>0andI(w0,rp)>0abs(max(I(w,rp),I(w0,rp)))ifI(w,rp)<0andI(w0,rp)<00elseTable1:Similaritymeasuresusedresource-independenceand(b)inordertobeabletoshapethesimilaritymetricbythechoiceofgeneralizationcorpus.Fortheheadwordweightswtrp(w),thesim-plestpossibilityistoassumeauniformweightdistribution,i.e.wtrp(w)=1.Inaddition,wetestafrequency-basedweight,i.e.wtrp(w)=f(w,rp),andinversedocumentfrequency,whichweighsawordaccordingtoitsdiscriminativity:wtrp(w)=lognum.wordsnum.wordstowhosecontextwbelongs.Thissimilarity-basedmodelofselectionalpreferencesisastraightforwardimplementa-tionoftheideaofgeneralizationfromseenheadwordstoother,similarwords.Liketheclustering-basedmodel,itisnottiedtotheavailabilityofWordNetoranyothermanuallycreatedresource.Themodelusestwocorpora,aprimarycorpusfortheextractionofseenhead-wordsandageneralizationcorpusforthecom-putationofsemanticsimilaritymetrics.Thisgivesthemodelﬂexibilitytoinﬂuencethesimi-laritymetricthroughthechoiceoftextdomainofthegeneralizationcorpus.Instantiationusedinthispaper.Ouraimistocomputeselectionalpreferencesforseman-ticroles.Sowechooseaparticularinstantia-tionofthesimilarity-basedmodelthatmakesuseofthefactthatthetwo-corporaapproachallowsustousediﬀerentnotionsof“predicate”and“argument”intheprimaryandgeneral-izationcorpus.Ourprimarycorpuswillcon-sistofmanuallysemanticallyannotateddata,andwewillusesemanticverbclassesaspred-icatesandsemanticrolesasarguments.Ex-amplesofextracted(p,rp,w)tuplesare(Moral-ityevaluation,Evaluee,gamblers)and(Placing,Goal,briefcase).Semanticsimilarity,ontheotherhand,willbecomputedonautomaticallysyntacticallyparsedcorpus,wherethepredi-catesarewordsandtheargumentsaresyntac-ticdependents.Examplesofextracted(p,rp,w)tuplesfromthegeneralizationcorpusinclude(catch,obj,frogs)and(intervene,in,deal).2Thisinstantiationofthesimilarity-basedmodelallowsustocomputewordsensespeciﬁcselectionalpreferences,generalizingovermanu-allysemanticallyannotateddatausingautomat-icallysyntacticallyannotateddata.4DataWeuseFrameNet(Bakeretal.,1998),ase-manticlexiconforEnglishthatgroupswordsinsemanticclassescalledframesandlistsse-manticrolesforeachframe.TheFrameNet1.3annotateddatacomprises139,439sentencesfromtheBritishNationalCorpus(BNC).Forourexperiments,wechose100frame-speciﬁcse-manticrolesatrandom,20eachfromﬁvefre-quencybands:50-100annotatedoccurrencesoftherole,100-200occurrences,200-500,500-1000,andmorethan1000occurrences.Theannotateddataforthese100rolescomprised59,608sentences,ourprimarycorpus.Todeter-mineheadwordsofthesemanticroles,thecor-puswasparsedusingtheCollins(1997)parser.OurgeneralizationcorpusistheBNC.ItwasparsedusingMinipar(Lin,1993),whichiscon-siderablyfasterthantheCollinsparserbutfailedtoparseaboutathirdofallsentences.2Fordetailsaboutthesyntacticandsemanticanalysesused,seeSection4.219

Accordingly,theargumentsrextractedfromthegeneralizationcorpusareMinipardepen-dencies,exceptthatpathsthroughprepositionnodeswerecollapsed,usingtheprepositionasthedependencyrelation.Weobtainedparsesfor5,941,811sentencesofthegeneralizationcorpus.TheEM-basedclusteringmodelwascom-putedwithalloftheFrameNet1.3data(139,439sentences)asinput.Resnik’smodelwastrainedontheprimarycorpus(59,608sentences).5ExperimentsInthissectionwedescribeexperimentscom-paringthesimilarity-basedmodelforselectionalpreferencestoResnik’sWordNet-basedmodelandtoanEM-basedclusteringmodel3.Forthesimilarity-basedmodelwetesttheﬁvesimilar-itymetricsandthreeweightingschemeslistedinsection3.ExperimentaldesignLikeRoothetal.(1999)weevaluateselectionalpreferenceinductionapproachesinapseudo-disambiguationtask.Inatestsetofpairs(rp,w),eachheadwordwispairedwithacon-founderw0chosenrandomlyfromtheBNCac-cordingtoitsfrequency4.NounheadwordsarepairedwithnounconfoundersinordernottodisadvantageResnik’smodel,whichonlyworkswithnouns.Theheadword/confounderpairsareonlycomputedonceandreusedinallcross-validationruns.Thetaskistochoosethemorelikelyroleheadwordfromthepair(w,w0).Inthemainpartoftheexperiment,wecountapairascoveredifbothwandw0areassignedsomelevelofpreferencebyamodel(“fullcover-age”).Wecontrastthiswithanothercondition,wherewecountapairascoveredifatleastoneofthetwowordsw,w0isassignedalevelofpref-erencebyamodel(“halfcoverage”).Ifonlyoneisassignedapreference,thatwordiscountedaschosen.Totesttheperformancediﬀerencebetweenmodelsforsigniﬁcance,weuseDietterich’s3WearegratefultoCarstenBrockmannandDetlefPrescherfortheuseoftheirsoftware.4Weexcludepotentialconfoundersthatoccurlessthan30ormorethan3,000times.ErrorRateCoverageCosine0.26670.3284Dice0.19510.3506Hindle0.20590.3530Jaccard0.18580.3506Lin0.16350.2214EM30/200.31150.5460EM40/200.34700.9846Resnik0.39530.3084Table2:Errorrateandcoverage(micro-average),similarity-basedmodelswithuniformweights.5x2cv(Dietterich,1998).Thetestinvolvesﬁve2-foldcross-validationruns.Letdi,j(i∈{1,2},j∈{1,...,5})bethediﬀerenceinerrorratesbetweenthetwomodelswhenusingsplitiofcross-validationrunjastrainingdata.Lets2j=(d1,j−¯dj)2+(d2,j−¯dj)2bethevarianceforcross-validationrunj,with¯dj=d1,j+d2,j2.Thenthe5x2cv˜tstatisticisdeﬁnedas˜t=d1,1q15P5j=1s2jUnderthenullhypothesis,the˜tstatistichasapproximatelyatdistributionwith5degreesoffreedom.5ResultsanddiscussionErrorrates.Table2showserrorratesandcoverageforthediﬀerentselectionalprefer-enceinductionmethods.Theﬁrstﬁvemod-elsaresimilarity-based,computedwithuniformweights.Thenameintheﬁrstcolumnisthenameofthesimilaritymetricused.NextcomeEM-basedclusteringmodels,using30(40)clus-tersand20re-estimationsteps6,andthelastrowliststheresultsforResnik’sWordNet-basedmethod.Resultsaremicro-averaged.Thetableshowsverylowerrorratesforthesimilarity-basedmodels,upto15pointslowerthantheEM-basedmodels.Theerrorrates5Sincethe5x2cvtestfailswhentheerrorratesvarywildly,weexcludedcaseswhereerrorratesdiﬀerby0.8ormoreacrossthe10runs,usingthethresholdrecom-mendedbyDietterich.6TheEM-basedclusteringsoftwaredeterminesgoodvaluesforthesetwoparametersthroughpseudo-disambiguationtestsonthetrainingdata.220

CosDicHinJacLinEM40/20ResnikCos-16(73)-12(73)-18(74)-22(57)11(67)11(74)Dic16(73)2(74)-8(85)-10(64)39(47)27(62)Hin12(73)-2(74)-8(75)-11(63)33(57)16(67)Jac18(74)8(85)8(75)-7(68)42(45)30(62)Lin22(57)10(64)11(63)7(68)29(41)28(51)EM40/20-11(67)-39(47)-33(57)-42(45)-29(41)3(72)Resnik-11(74)-27(62)-16(67)-30(62)-28(51)-3(72)Table3:Comparingsimilaritymeasures:numberofwinsminuslosses(inbracketsnon-signiﬁcantcases)usingDietterich’s5x2cv;uniformweights;condition(1):bothmembersofapairmustbecovered 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0 100 200 300 400 500error_ratenumhwLearning curve: num. headwords, sim_based-Jaccard-Plain, error_rate, allMon Apr 09 02:30:47 20071000-100-200500-1000200-50050-100Figure1:Learningcurve:seenheadwordsver-suserrorratebyfrequencyband,Jaccard,uni-formweights50-100100-200200-500500-10001000-Cos0.31670.32030.27000.25340.2606Jac0.18020.20400.17610.17060.1927Table4:Errorratesforsimilarity-basedmod-els,bysemanticrolefrequencyband.Micro-averages,uniformweightsofResnik’smodelareconsiderablyhigherthanboththeEM-basedandthesimilarity-basedmodels,whichisunexpected.WhileEM-basedmodelshavebeenshowntoworkbetterinSRLtasks(GildeaandJurafsky,2002),thishasbeenattributedtothediﬀerenceincoverage.Inadditiontothefullcoveragecondition,wealsocomputederrorrateandcoverageforthehalfcoveragecase.Inthiscondition,theerrorratesoftheEM-basedmodelsareunchanged,whiletheerrorratesforallsimilarity-basedmodelsaswellasResnik’smodelrisetovaluesbetween0.4and0.6.SotheEM-basedmodeltendstohavepreferencesonlyforthe“right”words.Whythisissoisnotclear.Itmaybeagenuineproperty,oranartifactoftheFrameNetdata,whichonlycontainschosen,illustrativesentencesforeachframe.Itispossiblethatthesesentenceshavefeweroccurrencesofhighlyfrequentbutsemanticallylessinformativeroleheadwordslike“it”or“that”exactlybecauseoftheirillustrativepurpose.Table3inspectsdiﬀerencesbetweenerrorratesusingDietterich’s5x2cv,basicallyconﬁrm-ingTable2.Eachcellshowsthewinsminuslossesforthemethodlistedintherowwhencomparedagainstthemethodinthecolumn.Thenumberofcasesthatdidnotreachsigniﬁ-canceisgiveninbrackets.Coverage.Thecoverageratesofthesimilarity-basedmodels,whilecomparabletoResnik’smodel,areconsiderablylowerthanforEM-basedclustering,whichachievesgoodcoveragewith30andalmostperfectcoveragewith40clusters(Table2).WhilepeculiaritiesoftheFrameNetdatamayhaveinﬂuencedtheresultsintheEM-basedmodel’sfavor(seethediscussionofthehalfcoverageconditionabove),thelowcoverageofthesimilarity-basedmodelsisstillsurprising.Afterall,thegeneralizationcorpusofthesimilarity-basedmodelsisfarlargerthanthecorpususedforclustering.GiventhelearningcurveinFigure1itisunlikelythatthereasonforthelowercover-ageisdatasparseness.However,EM-basedclusteringisasoftclusteringmethod,whichrelateseverypredicateandeveryheadwordtoeverycluster,ifonlywithaverylowprobabil-221

ity.Insimilarity-basedmodels,ontheotherhand,twowordsthathaveneverbeenseeninthesameargumentslotinthegeneralizationcorpuswillhavezerosimilarity.Thatis,asimilarity-basedmodelcanassignalevelofpreferenceforanargumentrpandwordw0onlyifR(w0)∩R(Seen(rp))isnonempty.Sincetheﬂexibilityofsimilarity-basedmodelsextendstothevectorspaceforcomputingsimilarities,oneobviousremedytothecoverageproblemwouldbetheuseofalesssparsevectorspace.Giventhelowerrorratesofsimilarity-basedmodels,itmayevenbeadvisabletousetwovectorspaces,backingoﬀtothedenseroneforwordsnotcoveredbythesparsebuthighlyaccuratespaceusedinthispaper.Parametersofsimilarity-basedmodels.Besidesthesimilaritymetricitself,whichwedis-cussbelow,parametersofthesimilarity-basedmodelsincludethenumberofseenheadwords,theweightingscheme,andthenumberofsimilarwordsforeachheadword.Table4breaksdownerrorratesbysemanticrolefrequencybandfortwoofthesimilarity-basedmodels,micro-averagingoverrolesofthesamefrequencybandandovercross-validationruns.Asthetableshows,therewassomevari-ationacrossfrequencybands,butnotasmuchasbetweenmodels.ThequestionofthenumberofseenheadwordsnecessarytocomputeselectionalpreferencesisfurtherexploredinFigure1.TheﬁgurechartsthenumberofseenheadwordsagainsterrorrateforaJaccardsimilarity-basedmodel(uniformweights).Ascanbeseen,errorratesreachaplateauatabout25seenheadwordsforJaccard.Forothersimilaritymetricstheresultissimilar.Theweightingschemeswtrphadsurprisinglylittleinﬂuenceonresults.ForJaccardsimilar-ity,themodelhadanerrorrateof0.1858foruniformweights,0.1874forfrequencyweight-ing,and0.1806fordiscriminativity.Forothersimilaritymetricstheresultsweresimilar.Acutoﬀwasusedinthesimilarity-basedmodel:Foreachseenheadword,onlythe500mostsimilarwords(accordingtoagivensim-ilaritymeasure)wereincludedinthecomputa-CosDicHinJacLin(a)Freq.sim.1889316729593167860(b)Freq.wins65%73%79%72%58%(c)Num.sim.8160676066(d)Intersec.7.32.37.22.10.5Table5:Comparingsim.metrics:(a)avg.freq.ofsimilarwords;(b)%oftimesthemorefre-quentwordwon;(c)numberofdistinctsimilarwordsperseenheadword;(d)avg.sizeofinter-sectionbetweenrolestion;forallothers,asimilarityof0wasassumed.Experimentstestingarangeofvaluesforthisparametershowthaterrorratesstaystableforparametervalues≥200.Sosimilarity-basedmodelsseemnotoverlysensitivetotheweightingschemeused,thenum-berofseenheadwords,orthenumberofsimilarwordsperseenheadword.Thediﬀerencebe-tweensimilaritymetrics,however,isstriking.Diﬀerencesbetweensimilaritymetrics.AsTable2shows,LinandJaccardworkedbest(thoughLinhasverylowcoverage),DiceandHindlenotasgood,andCosineshowedtheworstperformance.Todeterminepossiblereasonsforthediﬀerence,Table5explorespropertiesoftheﬁvesimilaritymeasures.GivenasetS=Seen(rp)ofseenheadwordsforsomerolerp,eachsimilaritymetricproducesasetlike(S)ofwordsthathavenonzerosimi-laritytoS,thatis,toatleastonewordinS.Line(a)showstheaveragefrequencyofwordsinlike(S).TheresultsconﬁrmthattheLinandCosinemetricstendtoproposelessfrequentwordsassimilar.Line(b)pursuesthequestionofthefrequencybiasfurther,showingthepercentageofhead-word/confounderpairsforwhichthemorefre-quentofthetwowords“won”inthepseudo-disambiguationtask(usinguniformweights).Thisitisanindirectestimateofthefrequencybiasofasimilaritymetric.Notethatthehead-wordactuallywasmorefrequentthanthecon-founderinonly36%ofallpairs.Theseﬁrsttwotestsdonotyieldanyexpla-nationforthelowperformanceofCosine,astheresultstheyshowdonotseparateCosinefrom222

JaccardCosineRidevehicle:Vehicletruck0.05boat0.05coach0.04van0.04ship0.04lorry0.04crea-ture0.04ﬂight0.04guy0.04carriage0.04he-licopter0.04lad0.04Ingestsubstance:Substanceloaf0.04icecream0.03you0.03some0.03that0.03er0.03photo0.03kind0.03he0.03type0.03thing0.03milk0.03Ridevehicle:Vehicleit1.18there0.88they0.43that0.34i0.23ship0.19secondone0.19machine0.19e0.19otherone0.19response0.19second0.19Ingestsubstance:Substancethere1.23that0.50object0.27argument0.27theme0.27version0.27machine0.26result0.26response0.25item0.25concept0.25s0.24Table6:Highest-rankedinducedheadwords(seenheadwordsomitted)fortwosemanticclassesoftheverb“take”:similarity-basedmodels,JaccardandCosine,uniformweights.allothermetrics.Lines(c)and(d),however,dojustthat.Line(c)looksatthesizeoflike(S).Sinceweareusingacutoﬀof500similarwordscomputedperwordinS,thesizeoflike(S)canonlyvaryifthesamewordissuggestedassimilarforseveralseenheadwordsinS.Thisway,thesizeoflike(S)functionsasanindicatorofthedegreeofuniformityorsimilaritythatasim-ilaritymetric“perceives”amongthemembersofS.Tofacilitatecomparisonacrossfrequencybands,line(c)normalizesbythesizeofS,show-ing|like(S)||S|micro-averagedoverallroles.HereweseethatCosineseemsto“perceive”consid-erablylesssimilarityamongtheseenheadwordsthananyoftheothermetrics.Line(d)looksatthesetss25(r)ofthe25mostpreferredpotentialheadwordsofrolesr,show-ingtheaveragesizeoftheintersections25(r)∩s25(r0)betweentworoles(preferencescomputedwithuniformweights).Itindicatesanotherpos-siblereasonforCosine’sproblem:Cosineseemstokeepproposingthesamewordsassimilarfordiﬀerentroles.Wewillseethistendencyalsointhesampleresultswediscussnext.Sampleresults.Table6showssamplesofheadwordsinducedbythesimilarity-basedmodelfortwoFrameNetsensesoftheverb“take”:Ridevehicle(“takethebus”)andIn-gestsubstance(“takedrugs”),asemanticclassthatisexclusivelyaboutingestingcontrolledsubstances.ThesemanticroleVehicleoftheRidevehicleframeandtheroleSubstanceofIn-gestsubstancearebothtypicallyrealizedasthedirectobjectof“take”.Thetableonlyshowsnewinducedheadwords;seenheadwordswereomittedfromthelist.Theparticularimplementationofthesimilarity-basedmodelwehavechosen,usingframesandrolesaspredicatesandargumentsintheprimarycorpus,shouldenablethemodeltocomputepreferencesspeciﬁctowordsenses.ThesampleinTable6showsthatthisisindeedthecase:Thepreferencesdiﬀerconsiderablyforthetwosenses(frames)of“take”,atleastfortheJaccardmetric,whichshowsaclearpreferenceforvehiclesfortheVehiclerole.TheSubstanceroleofIngestsubstanceishardertocharacterize,withverydiverseseenheadwordssuchas“crack”,“lines”,“ﬂuid”,“speed”.Whilethehighest-rankedinducedwordsforJaccarddoincludethreefooditems,thereisnoword,withthepossibleexceptionof“icecream”,thatcouldbeconstruedasacontrolledsubstance.TheinducedheadwordsfortheCosinemetricareconsiderablylesspertinentforbothrolesandshowtheabove-mentionedtendencytorepeatsomehigh-frequencywords.Theinspectionof“take”anecdotallycon-ﬁrmsthatdiﬀerentselectionalpreferencesarelearnedfordiﬀerentsenses.Thispoint(whichcomesdowntotheusabilityofselectionalpref-erencesforWSD)shouldbeveriﬁedinanem-piricalevaluation,possiblyinanotherpseudo-disambiguationtask,choosingasconfoundersseenheadwordsforothersensesofapredicateword.6ConclusionWehaveintroducedthesimilarity-basedmodelforinducingselectionalpreferences.Comput-ingselectionalpreferenceasaweightedsumofsimilaritiestoseenheadwords,itisastraight-223

forwardimplementationoftheideaofgeneral-izationfromseenheadwordstoother,similarwords.Thesimilarity-basedmodelisparticu-larlysimpleandeasytocompute,andseemsnotverysensitivetoparameters.LiketheEM-basedclusteringmodel,itisnotdependentonlexicalresources.Itis,however,moreﬂexibleinthatitinducessimilaritiesfromaseparategeneraliza-tioncorpus,whichallowsustocontrolthesimi-laritieswecomputebythechoiceoftextdomainforthegeneralizationcorpus.Inthispaperwehaveusedthemodeltocomputesense-speciﬁcselectionalpreferencesforsemanticroles.Inapseudo-disambiguationtaskthesimila-rity-basedmodelshowederrorratesdownto0.16,farlowerthanbothEM-basedclusteringandResnik’sWordNetmodel.Howeveritscov-erageisconsiderablylowerthanthatofEM-basedclustering,comparabletoResnik’smodel.Themostprobablereasonforthisisthespar-sityoftheunderlyingvectorspace.Thechoiceofsimilaritymetriciscriticalinsimilarity-basedmodels,withJaccardandLinachievingthebestperformance,andCosinesurprisinglybringinguptherear.Nextstepswillbetotestthesimilarity-basedmodel“invivo”,inanSRLtask;totestthemodelinaWSDtask;toevaluatethemodelonaprimarycorpusthatisnotsemanticallyana-lyzed,forgreatercomparabilitytopreviousap-proaches;toexploreothervectorspacestoad-dressthecoverageissue;andtoexperimentondomaintransfer,usinganappropriategeneral-izationcorpustoinduceselectionalpreferencesforadomaindiﬀerentfromthatoftheprimarycorpus.Thisisespeciallyrelevantinviewofthedomain-dependenceproblemthatSRLfaces.AcknowledgementsManythankstoJasonBaldridge,RazvanBunescu,StefanEvert,RayMooney,UlrikeandSebastianPad´o,andSabineSchulteimWaldeforhelpfuldiscussions.ReferencesN.AbeandH.Li.1993.Learningwordassociationnormsusingtreecutpairmodels.InProceedingsofICML1993.C.Baker,C.Fillmore,andJ.Lowe.1998.TheBerkeleyFrameNetproject.InProceedingsofCOLING-ACL1998,Montreal,Canada.C.BrockmannandM.Lapata.2003.Evaluatingandcombiningapproachestoselectionalpreferenceacqui-sition.InProceedingsofEACL2003,Budapest.A.BudanitskyandG.Hirst.2006.EvaluatingWordNet-basedmeasuresofsemanticdistance.ComputationalLinguistics,32(1).X.CarrerasandL.Marquez.2005.IntroductiontotheCoNLL-2005sharedtask:Semanticrolelabeling.InProceedingsofCoNLL-05,AnnArbor,MI.S.ClarkandD.Weir.2001.Class-basedprobabilityestimationusingasemantichierarchy.InProceedingsofNAACL2001,Pittsburgh,PA.M.Collins.1997.Threegenerative,lexicalisedmodelsforstatisticalparsing.InProceedingsofACL1997,Madrid,Spain.T.Dietterich.1998.Approximatestatisticaltestsforcomparingsupervisedclassiﬁcationlearningalgo-rithms.NeuralComputation,10:1895–1923.D.GildeaandD.Jurafsky.2002.Automaticlabelingofsemanticroles.ComputationalLinguistics,28(3):245–288.D.HindleandM.Rooth.1993.Structuralambiguityandlexicalrelations.ComputationalLinguistics,19(1).D.Hindle.1990.Nounclassiﬁcationfrompredicate-argumentstructures.InProceedingsofACL1990,Pittsburg,Pennsylvania.J.KatzandJ.Fodor.1963.Thestructureofasemantictheory.Language,39(2).D.Lin.1993.Principle-basedparsingwithoutovergen-eration.InProceedingsofACL1993,Columbus,OH.D.Lin.1998.Automaticretrievalandclusteringofsimilarwords.InProceedingsofCOLING-ACL1998,Montreal,Canada.D.McCarthyandJ.Carroll.2003.Disambiguatingnouns,verbsandadjectivesusingautomaticallyac-quiredselectionalpreferences.ComputatinalLinguis-tics,29(4).P.Resnik.1996.Selectionalconstraints:Aninformation-theoreticmodelanditscomputationalre-alization.Cognition,61:127–159.M.Rooth,S.Riezler,D.Prescher,G.Carroll,andF.Beil.1999.InducingansemanticallyannotatedlexiconviaEM-basedclustering.InProceedingsofACL1999,Maryland.Y.Wilks.1975.Preferencesemantics.InE.Keenan,editor,FormalSemanticsofNaturalLanguage.Cam-bridgeUniversityPress.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 224–231,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

224

SVMModelTamperingandAnchoredLearning:ACaseStudyinHebrewNPChunkingYoavGoldbergandMichaelElhadadComputerScienceDepartmentBenGurionUniversityoftheNegevP.O.B653Be’erSheva84105,Israelyoavg,elhadad@cs.bgu.ac.ilAbstractWestudytheissueofportingaknownNLPmethodtoalanguagewithlittleexistingNLPresources,speciﬁcallyHebrewSVM-basedchunking.WeintroducetwoSVM-basedmethods–ModelTamperingandAnchoredLearning.TheseallowﬁnegrainedanalysisofthelearnedSVMmodels,whichprovidesguidancetoidentifyerrorsinthetrainingcor-pus,distinguishtheroleandinteractionoflexicalfeaturesandeventuallyconstructamodelwith∼10%errorreduction.There-sultingchunkerisshowntoberobustinthepresenceofnoiseinthetrainingcorpus,reliesonlesslexicalfeaturesthanwaspreviouslyunderstoodandachievesanF-measureperfor-manceof92.2onautomaticallyPoS-taggedtext.TheSVManalysismethodsalsoprovidegeneralinsightonSVM-basedchunking.1IntroductionWhilehigh-qualityNLPcorporaandtoolsareavail-ableinEnglish,suchresourcesaredifﬁculttoobtaininmostotherlanguages.ThreechallengesmustbemetwhenadaptingresultsestablishedinEnglishtoanotherlanguage:(1)acquiringhighqualityanno-tateddata;(2)adaptingtheEnglishtaskdeﬁnitiontothenatureofadifferentlanguage,and(3)adapt-ingthealgorithmtothenewlanguage.ThispaperpresentsacasestudyintheadaptationofawellknowntasktoalanguagewithfewNLPresourcesavailable.Speciﬁcally,wedealwithSVMbasedHe-brewNPchunking.In(Goldbergetal.,2006),weestablishedthatthetaskisnottriviallytransferabletoHebrew,butreportedthatSVMbasedchunking(KudoandMatsumoto,2000)performswell.Weextendthatworkandstudytheproblemfrom3an-gles:(1)howtodealwithacorpusthatissmallerandwithahigherlevelofnoisethanisavailableinEnglish;weproposetechniquesthathelpidentify‘suspicious’datapointsinthecorpus,andidentifyhowrobustthemodelisinthepresenceofnoise;(2)wecomparethetaskdeﬁnitioninEnglishandinHebrewthroughquantitativeevaluationofthediffer-encesbetweenthetwolanguagesbyanalyzingtherelativeimportanceoffeaturesinthelearnedSVMmodels;and(3)weanalyzethestructureoflearnedSVMmodelstobetterunderstandthecharacteristicsofthechunkingprobleminHebrew.Whilemostworkonchunkingwithmachinelearningtechniquestendtotreattheclassiﬁcationengineasablack-box,wetrytoinvestigatethere-sultingclassiﬁcationmodelinordertounderstanditsinnerworking,strengthsandweaknesses.Wein-troducetwoSVM-basedmethods–ModelTamper-ingandAnchoredLearning–anddemonstratehowaﬁne-grainedanalysisofSVMmodelsprovidesin-sightsonallthreeaccounts.Theunderstandingoftherelativecontributionofeachfeatureinthemodelhelpsusconstructabettermodel,whichachieves∼10%errorreductioninHebrewchunking,aswellasidentifycorpuserrors.ThemethodsalsoprovidegeneralinsightonSVM-basedchunking.2PreviousWorkNPchunkingisthetaskofmarkingthebound-ariesofsimplenoun-phrasesintext.ItisawellstudiedprobleminEnglish,andwasthefocusofCoNLL2000’sSharedTask(SangandBuchholz,225

2000).EarlyattemptsatNPChunkingwererulelearningsystems,suchastheErrorDrivenPrun-ingmethodofPierceandCardie(1998).Follow-ingRamshawandMarcus(1995),thecurrentdom-inantapproachisformulatingchunkingasaclas-siﬁcationtask,inwhicheachwordisclassiﬁedasthe(B)eginning,(I)nsideor(O)outsideofachunk.Featuresforthisclassiﬁcationusuallyinvolvelocalcontextfeatures.KudoandMatsumoto(2000)usedSVMasaclassiﬁcationengineandachievedanF-Scoreof93.79onthesharedtaskNPs.SinceSVMisabinaryclassiﬁer,touseitforthe3-classclassi-ﬁcationofthechunkingtask,3differentclassiﬁers{B/I,B/O,I/O}weretrainedandtheirmajorityvotewastaken.NPchunksinthesharedtaskdataareBaseNPs,whicharenon-recursiveNPs,adeﬁnitionﬁrstpro-posedbyRamshawandMarcus(1995).Thisdeﬁni-tionyieldsgoodNPchunksforEnglish.In(Gold-bergetal.,2006)wearguedthatitisnotapplica-bletoHebrew,mainlybecauseoftheprevalenceoftheHebrew’sconstructstate(smixut).Smixutissimilartoanoun-compoundconstruct,butonethatcanjoinanoun(withaspecialmorphologi-calmarking)withafullNP.Itappearsinabout40%ofHebrewNPs.Weproposedanalterna-tivedeﬁnition(termedSimpleNP)forHebrewNPchunks.ASimpleNPcannotcontainembeddedrel-atives,prepositions,VPsandNP-conjunctions(ex-ceptwhentheyarelicensedbysmixut).Itcancontainsmixut,possessives(evenwhentheyareattachedbythe‘לש/of’preposition)andpartitives(and,therefore,allowsforalimitedamountofre-cursion).WeappliedthisdeﬁnitiontotheHebrewTreeBank(Sima’anetal.,2001),andconstructedamoderatesizecorpus(about5,000sentences)forHebrewSimpleNPchunking.SimpleNPsarediffer-entthanEnglishBaseNPs,andindeedsomemeth-odsthatworkwellforEnglishperformedpoorlyonHebrewdata.However,wefoundthatchunk-ingwithSVMprovidesgoodresultforHebrewSim-pleNPs.WeanalyzedthatthissuccesscomesfromSVM’sabilitytouselexicalfeatures,aswellastwoHebrewmorphologicalfeatures,namely“number”and“construct-state”.OneofthemainissueswhendealingwithHebrewchunkingisthattheavailabletreebankisrathersmall,andsinceitisquitenew,andhasnotbeenusedintensively,itcontainsacertainamountofin-consistenciesandtaggingerrors.Inaddition,theidentiﬁcationofSimpleNPsfromthetreebankalsointroducessomeerrors.Finally,wewanttoinvesti-gatechunkinginascenariowherePoStagsareas-signedautomaticallyandchunksarethencomputed.TheHebrewPoStaggerweuseintroducesabout8%errors(comparedwithabout4%inEnglish).Weare,therefore,interestedinidentifyingerrorsinthechunkingcorpus,andinvestigatinghowthechunkeroperatesinthepresenceofnoiseinthePoStagse-quence.3ModelTampering3.1NotationandTechnicalReviewThissectionpresentsnotationaswellasatechnicalreviewofSVMchunkingdetailsrelevanttothecur-rentstudy.FurtherdetailscanbefoundinKudoandMatsumoto(2000;2003).SVM(Vapnik,1995)isasupervisedbinaryclas-siﬁer.Theinputtothelearnerisasetofltrain-ingsamples(x1,y1),...,(xl,yl),x∈Rn,y∈{+1,−1}.xiisanndimensionalfeaturevec-torrepresentingtheithsample,andyiisthela-belforthatsample.Theresultofthelearningpro-cessisthesetSVofSupportVectors,theasso-ciatedweightsαi,andaconstantb.TheSupportVectorsareasubsetofthetrainingvectors,andto-getherwiththeweightsandbtheydeﬁneahyper-planethatoptimallyseparatesthetrainingsamples.ThebasicSVMformulationisofalinearclassiﬁer,butbyintroducingakernelfunctionKthatnon-linearlytransformsthedatafromRnintoahigherdimensionalspace,SVMcanbeusedtoperformnon-linearclassiﬁcation.SVM’sdecisionfunctionis:y(x)=sgn(cid:16)Pj∈SVyjαjK(xj,x)+b(cid:17)wherexisanndimensionalfeaturevectortobeclassi-ﬁed.Inthelinearcase,Kisadotproductoper-ationandthesumw=Pyjαjxjisanndimen-sionalweightvectorassigningweightforeachofthenfeatures.Theotherkernelfunctionwecon-siderinthispaperisapolynomialkernelofdegree2:K(xi,xj)=(xi·xj+1)2.Whenusingbinaryvaluedfeatures,thiskernelfunctionessentiallyim-pliesthattheclassiﬁerconsidersnotonlytheexplic-itlyspeciﬁedfeatures,butalsoallavailablepairsoffeatures.Inordertocopewithinseparabledata,thelearningprocessofSVMallowsforsomemisclas-siﬁcation,theamountofwhichisdeterminedbya226

parameterC,whichcanbethoughtofasapenaltyforeachmisclassiﬁedtrainingsample.InSVMbasedchunking,eachwordanditscon-textisconsideredalearningsample.Werefertothewordbeingclassiﬁedasw0,andtoitspart-of-speech(PoS)tag,morphology,andB/I/Otagasp0,m0andt0respectively.Theinformationconsid-eredforclassiﬁcationisw−cw...wcw,p−cp...pcp,m−cm...mcmandt−ct...t−1.ThefeaturevectorFisanindexedlistofallthefeaturespresentinthecorpus.Afeaturefioftheformw+1=dogmeansthatthewordfollowingtheonebeingclas-siﬁedis‘dog’.Everylearningsampleisrepre-sentedbyann=|F|dimensionalbinaryvectorx.xi=1iffthefeaturefiisactiveinthegivensample,and0otherwise.Thisencodingleadstoextremelyhighdimensionalvectors,duetothelexicalfeaturesw−cw...wcw.3.2IntroducingModelTamperingAnimportantobservationaboutSVMclassiﬁersisthatfeatureswhicharenotactiveinanyoftheSup-portVectorshavenoeffectontheclassiﬁerdeci-sion.WeintroduceModelTampering,aprocedureinwhichwechangetheSupportVectorsinamodelbyforcingsomevaluesinthevectorsto0.TheresultofthisprocedureisanewModelinwhichthedeletedfeaturesnevertakepartintheclas-siﬁcation.Modeltamperingisdifferentthanfeatureselec-tion:ontheonehand,itisamethodthathelpsusidentifyirrelevantfeaturesinamodelaftertraining;ontheotherhand,andthisisthekeyinsight,re-movingfeaturesaftertrainingisnotthesameasre-movingthembeforetraining.Thepresenceofthelow-relevancefeaturesduringtraininghasanimpactonthegeneralizationperformedbythelearnerasshownbelow.3.3TheRoleofLexicalFeaturesInGoldbergetal.(2006),wehaveestablishedthatusinglexicalfeaturesincreasesthechunkingF-measurefrom78toover92ontheHebrewTree-bank.WereﬁnethisobservationbyusingModelTampering,inordertoassesstheimportanceoflex-icalfeaturesinNPChunking.Weareinterestedinidentifyingwhichspeciﬁclexicalitemsandcontextsimpactthechunkingdecision,andquantifyingtheireffect.Ourmethodistotrainachunkingmodelonagiventrainingcorpus,tamperwiththeresult-ingmodelinvariouswaysandmeasuretheperfor-mance1ofthetamperedmodelsonatestcorpus.3.4ExperimentalSettingWeconductedexperimentsbothforEnglishandHe-brewchunking.FortheHebrewexperiments,weusethecorporaof(Goldbergetal.,2006).TheﬁrstoneisderivedfromtheoriginalTreebankbyprojectingthefullsyntactictree,constructedmanually,ontoasetofNPchunksaccordingtotheSimpleNPrules.WerefertotheresultingcorpusasHEBGoldsincePoStagsarefullyreliable.TheHEBErrversionofthecorpusisobtainedbyprojectingthechunkboundariesonthesequenceofPoSandmorphologytagsobtainedbytheautomaticPoStaggerofAdler&Elhadad(2006).Thiscorpusincludesanerrorrateofabout8%onPoStags.Theﬁrst500sen-tencesareusedfortesting,andtherestfortraining.Thecorpuscontains27KNPchunks.FortheEn-glishexperiments,weusethenow-standardtrainingandtestsetsthatwereintroducedin(MarcusandRamshaw,1995)2.TrainingwasdoneusingKudo’sYAMCHAtoolkit3.BothHebrewandEnglishmod-elsweretrainedusingapolynomialkernelofde-gree2,withC=1.ForEnglish,thefeaturesusedwere:w−2...w2,p−2...p2,t−2...t−1.ThesamefeatureswereusedforHebrew,withtheadditionofm−2...m2.Thesearethesamesettingsasin(KudoandMatsumoto,2000;Goldbergetal.,2006).3.5TamperingsWeexperimentedwiththefollowingtamperings:TopN–WedeﬁnemodelfeaturecounttobethenumberofSupportVectorsinwhichafeatureisac-tiveinagivenclassiﬁer.ThistamperingleavesinthemodelonlythetopNlexicalfeaturesineachclassi-ﬁer,accordingtotheircount.NoPOS–allthelexicalfeaturescorrespondingtoagivenpart-of-speechareremovedfromthemodel.Forexample,inaNoJJtampering,allthefeaturesoftheformwi=Xareremovedfromallthesupportvectorsinwhichpi=JJisactive.Loc6=i–allthelexicalfeatureswithindexiareremovedfromthemodele.g.,inaLoc6=+2tamper-1TheperformancemetricweuseisthestandardPreci-sion/Recall/Fmeasures,ascomputedbytheconllevalprogram:http://www.cnts.ua.ac.be/conll2000/chunking/conlleval.txt2ftp://ftp.cis.upenn.edu/pub/chunker3http://chasen.org/∼taku/software/yamcha/227

ing,featuresoftheformw+2=Xareremoved).Loc=i–allthelexicalfeatureswithanindexotherthaniareremovedfromthemodel.3.6ResultsandDiscussionHighlightsoftheresultsarepresentedinTables(1-3).ThenumbersreportedareFmeasures.TopNHEBGoldHEBErrENGALL93.5892.4893.79N=078.3276.2790.10N=1090.2188.6890.24N=5091.7890.8591.22N=10092.2591.6291.72N=50093.6092.2393.12N=100093.5692.4193.30Table1:ResultsofTopNTampering.TheresultsoftheTopNtamperingsshowthatforbothlanguages,mostofthelexicalfeaturesareirrel-evantfortheclassiﬁcation–thenumbersachievedbyusingallthelexicalfeatures(about30,000inHe-brewand75,000inEnglish)areveryclosetothoseobtainedusingonlyafewlexicalfeatures.Thisﬁndingisveryencouraging,andsuggeststhatSVMbasedchunkingisrobusttocorpusvariations.AnotherconclusionisthatlexicalfeatureshelpbalancethefactthatPoStagscanbenoisy:weknowbothHEBErrandENGincludePoStag-gingerrors(about8%inHebrewand4%inEn-glish).Whileinthecaseof“perfect”PoStagging(HEBGold),averysmallamountoflexicalfeaturesissufﬁcienttoreachthebestF-result(500outof30,264),inthepresenceofPoSerrors,morethanthetop1000lexicalfeaturesareneededtoreachtheresultobtainedwithalllexicalfeatures.MorestrikingisthefactthatinHebrew,thetop10lexicalfeaturesareresponsibleforanim-provementof12.4inF-score.Thewordscov-eredbythese10featuresarethefollowing:StartofSentencemarkerandcomma,quote,‘of/לש’,‘and/ו’,‘the/ה’and‘in/ב’.ThisﬁndingsuggeststhattheHebrewPoStagsetmightnotbeinformativeenoughforthechunkingtask,especiallywherepunctuation4andpreposi-tionsareconcerned.TheresultsinTable2givefur-thersupportforthisclaim.4UnliketheWSJPoStagsetinwhichmostpunctuationsgetuniquetags,ourtagsettreatpunctuationmarksasonegroup.NoPOSHEBGHEBENoPOSHEBGHEBEPrep85.2584.40Pronoun92.9792.14Punct88.9087.66Conjunction92.3191.67Adverb92.0290.72Determiner92.5591.39Table2:ResultsofHebrewNoPOSTampering.Otherscoresare≥93.3(HEBG),≥92.2(HEBE).WhenremovinglexicalfeaturesofaspeciﬁcPoS,themostdramaticlossofF-scoreisreachedforPrepositionsandPunctuationmarks,followedbyAdverbs,andConjunctions.Strikingly,lexi-calinformationformostopen-classPoS(includingProperNamesandNouns)hasverylittleimpactonHebrewchunkingperformance.Fromthisobservation,onecouldconcludethatenrichingamodelbasedonlyonPoSwithlexicalfeaturesforonlyafewclosed-classPoS(prepo-sitionsandpunctuation)couldprovideappropri-ateresultsevenwithasimplerlearningmethod,onethatcannotdealwithalargenumberoffea-tures.WetestedthishypothesisbytrainingtheError-DrivenPruning(EDP)methodof(CardieandPierce,1998)withanextendedsetoffeatures.EDPwithPoSfeaturesonlyproducedanF-resultof76.3onHEBGold.Byaddinglexicalfeaturesonlyforprepositions{מבהכלש},oneconjunction{ו}andpunctuation,theF-scoreonHEBGoldindeedjumpsto85.4.However,whenappliedonHEBErr,EDPfallsdownagainto59.4.Thisstrikingdisparity,bycomparison,letsusappreciatetheresilienceoftheSVMmodeltoPoStaggingerrors,anditsgener-alizationcapabilityevenwithareducednumberoflexicalfeatures.Anotherimplicationofthisdataisthatcommasandquotationmarksplayamajorroleindeter-miningNPboundariesinHebrew.InGoldbergetal.(2006),wenotedtheHebrewTreebankisnotconsistentinitstreatmentofpunctuation,andthusweevaluatedthechunkeronlyafterperformingnor-malizationofchunkboundariesforpunctuations.Wenowhypothesizethat,sincecommasandquo-tationmarksplaysuchanimportantroleintheclas-siﬁcation,performingsuchnormalizationbeforethetrainingstagemightbebeneﬁcial.Indeedresultsonthenormalizedcorpusshowimprovementofabout1.0inFscoreonbothHEBErrandHEBGold.A10-foldcrossvalidationexperimentonpunctuationnormalizedHEBErrresultedinanF-Scoreof92.2,improvingtheresultsreportedby(Goldbergetal.,228

2006)onthesamesetting(91.4).Loc=IHEBEENGLoc6=IHEBEENG-278.2689.79-291.6293.87-176.9690.90-191.8693.03090.3392.37079.4491.16176.9090.47192.3393.30276.5590.06292.1893.65Table3:ResultsofLocTamperings.Wenowturntoanalyzingtheimportanceofcon-textpositions(Table3).Forbothlanguages,themostimportantlexicalfeature(byfar)isatposition0,thatis,thewordcurrentlybeingclassiﬁed.ForEnglish,itisfollowedbypositions1and-1,andthenpositions2and-2.ForHebrew,backcontextseemstohavemoreeffectthanfrontcontext.InHebrew,allthepositionspositivelycontributetothedecision,whileinEnglishremovingw2/−2slightlyimprovestheresults(notealsothatincludingonlyfeaturew2/−2performsworsethanwithnolexicalinformationinEnglish).3.7TheRealRoleofLexicalFeaturesModeltampering(i.e.,removingfeaturesafterthelearningstage)isnotthesameaslearningwithoutthesefeatures.Thisclaimisveriﬁedempirically:trainingontheEnglishcorpuswithoutthelexicalfeaturesatposition–2yieldsworseresultsthanwiththem(93.73vs.93.79)–whileremovingthew−2featuresviatamperingonamodeltrainedwithw−2yieldsbetterresults(93.87).Similarly,forallcor-pora,trainingusingonlythetop1,000features(asdeﬁnedintheTop1000tampering)resultsinlossofabout2inF-Score(ENG92.02,HEBErr90.30,HEBGold91.67),whiletamperingTop1000yieldsaresultveryclosetothebestobtained(93.56,92.41or93.3F).Thisobservationleadsustoaninterestingconclu-sionabouttherealroleoflexicalfeaturesinSVMbasedchunking:rareevents(features)areusedtomemorizehardexamples.Intuitively,bygivingaheavyweighttorareevents,theclassiﬁerlearnsspe-ciﬁcrulessuchas“ifthewordatposition-2isXandthePoSatposition2isY,thenthecurrentwordisInsideanoun-phrase”.Mostoftheserulesareacci-dental–thereisnorealrelationbetweenthepartic-ularword-poscombinationandtheclassofthecur-rentword,itjusthappenstobethiswayinthetrain-ingsamples.Markingtherareoccurrenceshelpsthelearnerachievebettergeneralizationontheother,morecommoncases,whicharesimilartotheoutlieronmostfeatures,exceptthe“irrelevantones”.Astheeventsarerare,suchrulesusuallyhavenoeffectonchunkingaccuracy:theysimplyneveroccurinthetestdata.ThisobservationreﬁnesthecommonconceptionthatSVMchunkingdoesnotsufferfromirrelevantfeatures:inchunking,SVMindeedgener-alizeswellforthecommoncasesbutalsoover-ﬁtsthemodelonoutliers.Modeltamperinghelpsusdesignamodelintwoways:(1)itisawayto“opentheblackbox”ob-tainedwhentraininganSVMandtoanalyzethere-spectiveimportanceoffeatures.Inourcase,thisanalysisallowedustoidentifytheimportanceofpunctuationandprepositionsandimprovethemodelbydeﬁningmorefocusedfeatures(improvingover-allresultby∼1.0F-point).(2)Theanalysisalsoledustotheconclusionthat“featureselection”iscom-plexinthecaseofSVM–irrelevantfeatureshelppreventover-generalizationbyforcingover-ﬁttingonoutliers.Wehavealsoconﬁrmedthatthemodellearnedre-mainsrobustinthepresenceofnoiseinthePoStagsandreliesononlyfewlexicalfeatures.Thisveri-ﬁcationiscriticalinthecontextoflanguageswithfewcomputationalresources,asweexpectthesizeofcorporaandthequalityoftaggerstokeeplaggingbehindthatachievedinEnglish.4AnchoredLearningWepursuetheobservationofhowSVMdealswithoutliersbydevelopingtheAnchoredLearningmethod.TheideabehindAnchoredLearningistoaddauniquefeatureai(ananchor)toeachtrainingsample(weaddasmanynewfeaturestothemodelastherearetrainingsamples).Thesenewfeaturesmakeourdatalinearlyseparable.TheSVMlearnercanthenusetheseanchors(whichwillneveroccuronthetestdata)tomemorizethehardcases,de-creasingthisburdenfrom“real”features.WepresenttwousesforAnchoredLearning.Theﬁrstistheidentiﬁcationofhardcasesandcorpuser-rors,andthesecondisapreliminaryfeatureselec-tionapproachforSVMtoimprovechunkingaccu-racy.4.1MiningforErrorsandHardCasesFollowingtheintuitionthatSVMgivesmoreweighttoanchorfeaturesofhard-to-classifycases,wecan229

activelylookforsuchcasesbytraininganSVMchunkeronanchoreddata(astheanchoreddataisguaranteedtobelinearlyseparable,wecansetaveryhighvaluetotheCparameter,preventinganymis-classiﬁcation),andtheninvestigatingeitherthean-chorswhoseweights5areabovesomethresholdtorthetopNheaviestanchors,andtheircorrespondingcorpuslocations.Theselocationsarethosethatthelearnerconsidershardtoclassify.Theycanbeeithercorpuserrors,orgenuinelyhardcases.Thismethodissimilartothecorpuserrordetec-tionmethodpresentedbyNakagawaandMatsumoto(2002).TheyconstructedanSVMmodelforPoStagging,andconsideredSupportVectorswithhighαvaluestobeindicativeofsuspiciouscorpusloca-tions.Theselocationscanbeeitheroutliers,orcor-rectlylabeledlocationssimilartoanoutlier.Theythenlookedforsimilarcorpuslocationswithadif-ferentlabel,topointoutright-wrongpairswithhighprecision.Usinganchorsimprovestheirmethodinthreeas-pects:(1)withoutanchors,similarexamplesareof-tenindistinguishabletotheSVMlearner,andincasetheyhaveconﬂictinglabelsbothexampleswillbegivenhighweights.Thatis,boththeregularcaseandthehardcasewillbeconsideredashardexam-ples.Moreover,similarcorpuserrorsmightresultinonlyonesupportvectorthatcoverallthegroupofsimilarerrors.Anchorsmitigatetheseeffects,result-inginbetterprecisionandrecall.(2)Themoreer-rorsthereareinthecorpus,thelesslinearlysepara-bleitis.Un-anchoredlearningonerroneouscorpuscantakeunreasonableamountoftime.(3)Anchorsallowlearningwhileremovingsomeoftheimpor-tantfeaturesbutstillallowtheprocesstoconvergeinreasonabletime.Thisletsusanalyzewhichcasesbecomehardtolearnifwedon’tusecertainfeatures,orinotherwords:whatproblematiccasesaresolvedbyspeciﬁcfeatures.Thehardcasesanalysisachievedbyanchoredlearningisdifferentfromtheusualerroranalysiscarriedoutonobservedclassiﬁcationerrors.Thetraditionalmethodsgiveusintuitionsaboutwheretheclassiﬁerfailstogeneralize,whilethemethodwepresentheregivesusintuitionaboutwhattheclassiﬁerconsidershardtolearn,basedonthetrainingexamplesalone.5Aseachanchorappearinonlyonesupportvector,wecantreatthevector’sαvalueastheanchorweightTheintuitionthat“hardtolearn”examplesaresuspectcorpuserrorsisnotnew,andappearsalsoinAbneyetal.(1999),whoconsiderthe“heaviest”samplesintheﬁnaldistributionoftheAdaBoostal-gorithmtobethehardesttoclassifyandthuslikelycorpuserrors.WhileAdaBoostmodelsareeasytointerpret,thisisnotthecasewithSVM.AnchoredlearningallowsustoextractthehardtolearncasesfromanSVMmodel.Interestingly,whilebothAd-aBoostandSVMare‘largemargin’basedclassi-ﬁers,thereislessthan50%overlapinthehardcasesforthetwomethods(intermsofmistakesonthetestdata,therewere234mistakessharedbyAdaBoostandSVM,69errorsuniquetoSVMand126errorsuniquetoAdaBoost)6.Analyzingthedifferenceinwhatthetwoclassiﬁersconsiderhardisinteresting,andwewilladdressitinfuturework.Inthecurrentwork,wenotethatforﬁndingcorpuserrorsthetwomethodsarecomplementary.Experiment1–LocatingHardCasesAlinearSVMmodel(Mfull)wastrainedonthetrainingsubsetoftheanchored,punctuation-normalized,HEBGoldcorpus,withthesamefea-turesasinthepreviousexperiments,andaCvalueof9,999.Corpuslocationscorrespondingtoanchorswithweights>1wereinspected.Therewereabout120suchlocationsoutof4,500sentencesusedinthetrainingset.Decreasingthethresholdtwouldresultinmorecases.Weanalyzedtheselocationsinto3categories:corpuserrors,casesthatchallengetheSimpleNPdeﬁnition,andcaseswherethechunkingdecisionisgenuinelydifﬁculttomakeintheabsenceofglobalsyntacticcontextorworldknowledge.CorpusErrors:Theanalysisrevealedthefol-lowingcorpuserrors:weidentiﬁed29hardcasesrelatedtoconjunctionandapposition(isthecomma,colonorslashinsideanNPorseparatingtwodistinctNPs).14ofthesehardcaseswereindeedmistakesinthecorpus.Thiswasanticipated,aswedistin-guishedappositionsandconjunctivecommasusingheuristics,sincetheTreebankmarkingofconjunc-tionsissomewhatinconsistent.InordertobuildtheChunkNPcorpus,thesyn-tactictreesoftheTreebankwereprocessedtoderivechunksaccordingtotheSimpleNPdeﬁnition.Thehardcasesanalysisidentiﬁed18instanceswherethis6ThesenumbersareforpairwiseLinearSVMandAdaBoostclassiﬁerstrainedonthesamefeatures.230

transformationresultsinerroneouschunks.Forex-ample,nullelementsresultinimproperchunks,suchaschunkscontainingonlyadverbsoronlyadjec-tives.Wealsofound3invalidsentences,6inconsisten-ciesinthetaggingofinterrogativeswithrespecttochunkboundaries,aswellas34otherspeciﬁcmis-takes.Overall,morethanhalfofthelocationsiden-tiﬁedbytheanchorswerecorpuserrors.Lookingforcasessimilartotheerrorsidentiﬁedbyanchors,wefound99morelocations,77ofwhichwereerrors.ReﬁningtheSimpleNPDeﬁnition:ThehardcasesanalysisidentiﬁedexamplesthatchallengetheSimpleNPdeﬁnitionproposedinGoldbergetal.(2006).Themostnotablecasesare:The‘et’marker:‘et’isasyntacticmarkerofdeﬁ-nitedirectobjectsinHebrew.ItwasregardedasapartofSimpleNPsintheirdeﬁnition.Insomecases,thisforcestheresultingSimpleNPtobetooinclu-sive:[תרושקתהוטפשמהתיבתסנכה,הלשממהתא][‘et’(thegovernment,theparliamentandthemedia)]BecauseintheTreebanktheconjunctiondependson‘et’asasingleconstituent,itisfullyembeddedinthechunk.Suchaconjunctionshouldnotbeconsid-eredsimple.Theלשpreposition(‘of’)marksgeneralizedposses-sionandwasconsideredunambiguousandincludedinSimpleNPs.Wefoundcaseswhere‘לש’causesPPattachmentambiguity:[הרטשמה]לש[תעמשמ]ל[ןידהתיבאישנ][president-conshouse-consthe-law]for[discipline]of[thepolice]/ThePoliceDisciplinaryCourtPresidentBecause2prepositionsareinvolvedinthisNP,‘לש’(of)and‘ל’(for),the‘לש’partcannotbeattachedunambiguouslytoitshead(‘court’).Itisunclearwhetherthe‘ל’prepositionshouldbegivenspecialtreatmenttoallowittoentersimpleNPsincertaincontexts,orwhethertheinconsistenthandlingofthe‘לש’thatresultsfromthe‘ל’inter-positionispreferable.Complexdeterminersandquantiﬁers:Inmanycases,complexdeterminersinHebrewaremulti-wordexpressionsthatincludenouns.TheinclusionofsuchdeterminersinsidetheSimpleNPsisnotconsistent.Genuinelyhardcaseswerealsoidentiﬁed.Theseincludeprepositions,conjunctionsandmulti-wordidioms(mostofthemareadjectivesandprepo-sitionswhicharemadeupofnounsanddetermin-ers,e.g.,asthewordunanimouslyisexpressedinHebrewasthemulti-wordexpression‘onemouth’).Also,someadverbialsandadjectivesareimpossibletodistinguishusingonlylocalcontext.Theanchorsanalysishelpedusimprovethechunkingmethodontwoaccounts:(1)itidentiﬁedcorpuserrorswithhighprecision;(2)itmadeusfo-cusonhardcasesthatchallengethelinguisticdeﬁ-nitionofchunkswehaveadopted.Followingtheseﬁndings,weintendtoreﬁnetheHebrewSimpleNPdeﬁnition,andcreateanewversionoftheHebrewchunkingcorpus.Experiment2–determiningtheroleofcontextuallexicalfeaturesTheintentofthisexperimentistounderstandtheroleofthecontextuallexicalfeatures(wi,i6=0).Thisisdonebytraining2additionalanchoredlin-earSVMmodels,Mno−contandMnear.ThesearethesameasMfullexceptforthelexicalfeaturesusedduringtraining.Mno−contusesonlyw0,whileMnearusesw0,w−1,w+1.Anchorsareagainusedtolocatethehardexam-plesforeachclassiﬁer,andthedifferencesareex-amined.TheexamplesthatarehardforMnearbutnotforMfullarethosesolvedbyw−2,w+2.Sim-ilarly,theexamplesthatarehardforMno−contbutnotforMneararethosesolvedbyw−1,w+1.Table4indicatesthenumberofhardcasesidentiﬁedbytheanchormethodforeachmodel.Onewaytointer-prettheseﬁgures,isthattheintroductionoffeaturesw−1,+1solves5timesmorehardcasesthanw−2,+2.ModelNumberofhardcases(t=1)HardcasesforclassiﬁerB-IMfull1202Mnear320(+200)12Mno−cont1360(+1040)164Table4:Numberofhardcasespermodeltype.Qualitativeanalysisofthehardcasessolvedbythecontextuallexicalfeaturesshowsthattheycon-tributemostlytotheidentiﬁcationofchunkbound-ariesincasesofconjunction,apposition,attachmentofadverbsandadjectives,andsomemulti-wordex-pressions.ThenumberofhardcasesspeciﬁctotheB-Iclas-siﬁerindicateshowthefeaturescontributetothede-cisionofsplittingorcontinuingback-to-backNPs.Back-to-backNPsamountto6%oftheNPsinHEBGoldand8%oftheNPsinENG.However,231

whileinEnglishmostofthesecasesareeasilyre-solved,Hebrewphenomenasuchasnull-equativesandfreewordordermakethemharder.Toquantifythedifference:79%oftheﬁrstwordsofthesecondNPinEnglishbelongtooneoftheclosedclassesPOS,DT,WDT,PRP,WP–categorieswhichmostlycannotappearinthemiddleofbaseNPs.Incon-trast,inHebrew,59%areNouns,NumbersorProperNames.Moreover,inEnglishtheratioofuniqueﬁrstwordstonumberofadjacentNPsis0.068,whileinHebrewitis0.47.Thatis,inHebrew,almosteverysecondsuchNPstartswithadifferentword.Theseﬁguresexplainwhysurroundinglexicalin-formationisneededbythelearnerinordertoclas-sifysuchcases.Theyalsosuggestthatthislearningismostlysuperﬁcial,thatis,thelearnerjustmem-orizessomeexamples,butthesewillnotgeneralizewellontestdata.Indeed,themostcommonclassoferrorsreportedinGoldbergetal.,2006areofthesplit/mergetype.Thesearefollowedbyconjunctionrelatederrors,whichsufferfromthesameproblem.Morphologicalfeaturesofsmixutandagreementcanhelptosomeextent,butthisisstillalimitedsolu-tion.Itseemsthatdecidingthe[NP][NP]caseisbeyondthecapabilitiesofchunkingwithlocalcon-textfeaturesalone,andmoreglobalfeaturesshouldbesought.4.2FacilitatingBetterLearningThissectionpresentspreliminaryresultsusingAn-choredLearningforbetterNPchunking.Wepresentasetting(EnglishBaseNPchunking)inwhichselectedfeaturescoupledtogetherwithanchoredlearningshowanimprovementoverpreviousresults.Section3.6hintedthatSVMbasedchunkingmightbehurtbyusingtoomanylexicalfeatures.Speciﬁcally,thefeaturesw−2,w+2wereshowntocausethechunkertooverﬁtinEnglishchunking.Learningwithoutthesefeatures,however,yieldslowerresults.Thiscanbeovercomebyintroduc-inganchorsasasubstitute.Anchorsplaythesameroleasrarefeatureswhenlearning,whileloweringthechanceofmisleadingtheclassiﬁerontestdata.Theresultsoftheexperimentusing5-foldcrossvalidationonENGindicatethattheF-scoreim-provesonaveragefrom93.95to94.10whenusinganchorsinsteadofw±2(+0.15),whilejustignoringthew±2featuresdropstheF-scoreby0.10.Theim-provementisminorbutconsistent.Itsimplicationisthatanchorscansubstitutefor“irrelevant”lexicalfeaturesforbetterlearningresults.Infuturework,wewillexperimentwithbetterinformedsetsoflex-icalfeaturesmixedwithanchors.5ConclusionWehaveintroducedtwonovelmethodstounder-standtheinnerstructureofSVM-learnedmodels.WehaveappliedthesetechniquestoHebrewNPchunking,anddemonstratedthatthelearnedmodelisrobustinthepresenceofnoiseinthePoStags,andreliesononlyafewlexicalfeatures.Wehaveiden-tiﬁedcorpuserrors,betterunderstoodthenatureofthetaskinHebrew–andcompareditquantitativelytothetaskinEnglish.ThemethodsprovidegeneralinsightinthewaySVMclassiﬁcationworksforchunking.ReferencesS.Abney,R.Schapire,andY.Singer.1999.BoostingappliedtotaggingandPPattachment.EMNLP-1999.M.AdlerandM.Elhadad.2006.Anunsupervisedmorpheme-basedhmmforhebrewmorphologicaldis-ambiguation.InCOLING/ACL2006.C.CardieandD.Pierce.1998.Error-drivenpruningoftreebankgrammarsforbasenounphraseidentiﬁcation.InACL-1998.Y.Goldberg,M.Adler,andM.Elhadad.2006.Nounphrasechunkinginhebrew:Inﬂuenceoflexicalandmorphologicalfeatures.InCOLING/ACL2006.T.KudoandY.Matsumoto.2000.Useofsupportvectorlearningforchunkidentiﬁcation.InCoNLL-2000.T.KudoandY.Matsumoto.2003.Fastmethodsforkernel-basedtextanalysis.InACL-2003.M.MarcusandL.Ramshaw.1995.TextChunkingUs-ingTransformation-BasedLearning.InProc.ofthe3rdACLWorkshoponVeryLargeCorpora.T.NakagawaandY.Matsumoto.2002.Detectinger-rorsincorporausingsupportvectormachines.InCOLING-2002.ErikF.TjongKimSangandS.Buchholz.2000.Intro-ductiontotheconll-2000sharedtask:chunking.InCoNLL-2000.K.Sima’an,A.Itai,Y.Winter,A.Altman,andN.Nativ.2001.Buildingatree-bankofmodernhebrewtext.TraitementAutomatiquedesLangues,42(2).V.Vapnik.1995.Thenatureofstatisticallearningthe-ory.Springer-VerlagNewYork,Inc.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 232–239,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

232

FullyUnsupervisedDiscoveryofConcept-SpeciﬁcRelationshipsbyWebMiningDmitryDavidovICNCTheHebrewUniversityJerusalem91904,Israeldmitry@alice.nc.huji.ac.ilAriRappoportInstituteofComputerScienceTheHebrewUniversityJerusalem91904,Israelwww.cs.huji.ac.il/∼arirMosheKoppelDept.ofComputerScienceBar-IlanUniversityRamat-Gan52900,Israelkoppel@cs.biu.ac.ilAbstractWepresentawebminingmethodfordiscov-eringandenhancingrelationshipsinwhichaspeciﬁedconcept(wordclass)participates.Wediscoverawholerangeofrelationshipsfocusedonthegivenconcept,ratherthangenericknownrelationshipsasinmostpre-viouswork.Ourmethodisbasedoncluster-ingpatternsthatcontainconceptwordsandotherwordsrelatedtothem.Weevaluatethemethodonthreedifferentrichconceptsandﬁndthatineachcasethemethodgeneratesabroadvarietyofrelationshipswithgoodpre-cision.1IntroductionThehugeamountofinformationavailableonthewebhasledtoaﬂurryofresearchonmethodsforautomaticcreationofstructuredinformationfromlargeunstructuredtextcorpora.Thechallengeistocreateasmuchinformationaspossiblewhilepro-vidingaslittleinputaspossible.Alotofthisresearchisbasedontheinitialinsight(Hearst,1992)thatcertainlexicalpatterns(‘Xisacountry’)canbeexploitedtoautomaticallygener-atehyponymsofaspeciﬁedword.Subsequentwork(tobediscussedindetailbelow)extendedthisinitialideaalongtwodimensions.Oneobjectivewastorequireassmallauser-providedinitialseedaspossible.Thus,itwasob-servedthatgivenoneormoresuchlexicalpatterns,acorpuscouldbeusedtogenerateexamplesofhy-ponymsthatcouldthen,inturn,beexploitedtogen-eratemorelexicalpatterns.Thelargerandmorereli-ablesetsofpatternsthusgeneratedresultedinlargerandmoreprecisesetsofhyponymsandviceversa.Theinitialstepoftheresultingalternatingbootstrapprocess–theuser-providedinput–couldjustaswellconsistofexamplesofhyponymsasoflexicalpat-terns.Asecondobjectivewastoextendtheinformationthatcouldbelearnedfromtheprocessbeyondhy-ponymsofagivenword.Thus,theapproachwasextendedtoﬁndinglexicalpatternsthatcouldpro-ducesynonymsandotherstandardlexicalrelations.Theserelationscompriseallthosewordsthatstandinsomeknownbinaryrelationwithaspeciﬁedword.Inthispaper,weintroduceanovelextensionofthisproblem:givenaparticularconcept(initiallyrepresentedbytwoseedwords),discoverrelationsinwhichitparticipates,withoutspecifyingtheirtypesinadvance.Wewillgenerateaconceptclassandavarietyofnaturalbinaryrelationsinvolvingthatclass.Anadvantageofourmethodisthatitisparticu-larlysuitableforwebmining,evengiventherestric-tionsonqueryamountsthatexistinsomeoftoday’sleadingsearchengines.Theoutlineofthepaperisasfollows.Inthenextsectionwewilldeﬁnemorepreciselytheproblemweintendtosolve.Insection3,wewillconsiderre-latedwork.Insection4wewillprovideanoverviewofoursolutionandinsection5wewillconsiderthedetailsofthemethod.Insection6wewillillustrateandevaluatetheresultsobtainedbyourmethod.Fi-nally,insection7wewilloffersomeconclusionsandconsiderationsforfurtherwork.233

2ProblemDeﬁnitionInseveralstudies(e.g.,WiddowsandDorow,2002;Panteletal,2004;DavidovandRappoport,2006)ithasbeenshownthatrelativelyunsupervisedandlanguage-independentmethodscouldbeusedtogeneratemanythousandsofsetsofwordswhosesemanticsissimilarinsomesense.Althoughex-aminationofanysuchsetinvariablymakesitclearwhythesewordshavebeengroupedtogetherintoasingleconcept,itisimportanttoemphasizethatthemethoditselfprovidesnoexplicitconceptdeﬁ-nition;insomesense,theimpliedclassisintheeyeofthebeholder.Nevertheless,bothhumanjudgmentandcomparisonwithstandardlistsindicatethatthegeneratedsetscorrespondtoconceptswithhighpre-cision.Wewishnowtobuildonthatresultinthefol-lowingway.Givenalargecorpus(suchastheweb)andtwoormoreexamplesofsomeconceptX,au-tomaticallygenerateexamplesofoneormorerela-tionsR⊂X×Y,whereYissomeconceptandRissomebinaryrelationshipbetweenelementsofXandelementsofY.Wecanthinkoftherelationswewishtogener-ateasbipartitegraphs.Unlikemostearlierwork,thebipartitegraphswewishtogeneratemightbeone-to-one(forexample,countriesandtheircapi-tals),many-to-one(forexample,countriesandtheregionstheyarein)ormany-to-many(forexample,countriesandtheproductstheymanufacture).ForagivenclassX,wewouldliketogeneratenotonebutpossiblymanydifferentsuchrelations.Theonlyinputwerequire,asidefromacorpus,isasmallsetofexamplesofsomeclass.However,sincesuchsetscanbegeneratedinentirelyunsuper-visedfashion,ourchallengeiseffectivelytogener-aterelationsdirectlyfromacorpusgivennoaddi-tionalinformationofanykind.Thekeypointisthatwedonotinanymannerspecifyinadvancewhattypesofrelationswewishtoﬁnd.3RelatedWorkAsfarasweknow,nopreviousworkhasdirectlyaddressedthediscoveryofgenericbinaryrelationsinanunrestricteddomainwithout(atleastimplic-itly)pre-specifyingrelationshiptypes.Mostrelatedworkdealswithdiscoveryofhypernymy(Hearst,1992;Panteletal,2004),synonymy(RoarkandCharniak,1998;WiddowsandDorow,2002;Davi-dovandRappoport,2006)andmeronymy(BerlandandCharniak,1999).Inadditiontothesebasictypes,severalstud-iesdealwiththediscoveryandlabelingofmorespeciﬁcrelationsub-types,includinginter-verbre-lations(ChklovskiandPantel,2004)andnoun-compoundrelationships(Moldovanetal,2004).Studyingrelationshipsbetweentaggednameden-tities,(Hasegawaetal,2004;Hassanetal,2006)proposedunsupervisedclusteringmethodsthatas-signgiven(orsemi-automaticallyextracted)setsofpairsintoseveralclusters,whereeachclustercorre-spondstooneofaknownrelationshiptype.Thesestudies,however,focusedontheclassiﬁcationofpairsthatwereeithergivenorextractedusingsomesupervision,ratherthanondiscoveryanddeﬁnitionofwhichrelationshipsareactuallyinthecorpus.Severalpapersreportonmethodsforusingthewebtodiscoverinstancesofbinaryrelations.How-ever,eachoftheseassumesthattherelationsthem-selvesareknowninadvance(implicitlyorexplic-itly)sothatthemethodcanbeprovidedwithseedpatterns(AgichteinandGravano,2000;Panteletal,2004),pattern-basedrules(Etzionietal,2004),rela-tionkeywords(Sekine,2006),orwordpairsexem-plifyingrelationinstances(Pascaetal,2006;Alfon-secaetal,2006;RosenfeldandFeldman,2006).Insomerecentwork(StrubeandPonzetto,2006),ithasbeenshownthatrelatedpairscanbegener-atedwithoutpre-specifyingthenatureoftherela-tionsought.However,thisworkdoesnotfocusondifferentiatingamongdifferentrelations,sothatthegeneratedrelationsmightconﬂateanumberofdis-tinctones.Itshouldbenotedthatsomeofthesepapersutilizelanguageanddomain-dependentpreprocessingin-cludingsyntacticparsing(Suchaneketal,2006)andnamedentitytagging(Hasegawaetal,2004),whileotherstakeadvantageofhandcrafteddatabasessuchasWordNet(Moldovanetal,2004;Costelloetal,2006)andWikipedia(StrubeandPonzetto,2006).Finally,(Turney,2006)providedapatterndis-tancemeasurewhichallowsafullyunsupervisedmeasurementofrelationalsimilaritybetweentwopairsofwords;however,relationshiptypeswerenotdiscoveredexplicitly.234

4OutlineoftheMethodWewillusetwoconceptwordscontainedinacon-ceptclassCtogenerateacollectionofdistinctre-lationsinwhichCparticipates.Inthissectionweofferabriefoverviewofourmethod.Step1:Useaseedconsistingoftwo(ormore)ex-amplewordstoautomaticallyobtainotherexamplesthatbelongtothesameclass.Calltheseconceptwords.(Forinstance,ifourexamplewordswereFranceandAngola,wewouldgeneratemorecoun-trynames.)Step2:Foreachconceptword,collectinstancesofcontextsinwhichthewordappearstogetherwithoneothercontentword.Callthisotherwordatar-getwordforthatconceptword.(Forexample,forFrancewemightﬁnd‘ParisisthecapitalofFrance’.PariswouldbeatargetwordforFrance.)Step3:Foreachconceptword,groupthecontextsinwhichitappearsaccordingtothetargetwordthatappearsinthecontext.(Thus‘XisthecapitalofY’wouldlikelybegroupedwith‘Y’scapitalisX’.)Step4:Identifysimilarcontextgroupsthatap-pearacrossmanydifferentconceptwords.Mergetheseintoasingleconcept-word-independentclus-ter.(Thegroupincludingthetwocontextsabovewouldappear,withsomevariation,forothercoun-triesaswell,andallthesewouldbemergedintoasingleclusterrepresentingtherelationcapital-of(X,Y).)Step5:Foreachcluster,outputtherelationcon-sistingofall<conceptword,targetword>pairsthatappeartogetherinacontextincludedinthecluster.(Theclusterconsideredabovewouldresultinasetofpairsconsistingofacountryanditscapital.Otherclustersgeneratedbythesameseedmightincludecountriesandtheirlanguages,countriesandthere-gionsinwhichtheyarelocated,andsoforth.)5DetailsoftheMethodInthissectionweconsiderthedetailsofeachoftheabove-enumeratedsteps.Itshouldbenotedthateachstepcanbeperformedusingstandardwebsearches;nospecialpre-processedcorpusisre-quired.5.1GeneralizingtheseedTheﬁrststepistotaketheseed,whichmightcon-sistofasfewastwoconceptwords,andgeneratemany(ideally,all,whentheconceptisaclosedsetofwords)membersoftheclasstowhichtheybe-long.Wedothisasfollows,essentiallyimplement-ingasimpliﬁedversionofthemethodofDavidovandRappoport(2006).ForanypairofseedwordsSiandSj,searchthecorpusforwordpatternsoftheformSiHSj,whereHisahigh-frequencywordinthecorpus(weusedthe100mostfrequentwordsinthecorpus).Ofthese,wekeepallthosepat-terns,whichwecallsymmetricpatterns,forwhichSjHSiisalsofoundinthecorpus.Repeatthispro-cesstoﬁndsymmetricpatternswithanyofthestruc-turesHSHS,SHSHorSHHS.Itwasshownin(DavidovandRappoport,2006)thatpairsofwordsthatoftenappeartogetherinsuchsymmetricpat-ternstendtobelongtothesameclass(thatis,theysharesomenotableaspectoftheirsemantics).Otherwordsintheclasscanthusbegeneratedbysearch-ingasub-corpusofdocumentsincludingatleasttwoconceptwordsforthosewordsXthatappearinasufﬁcientnumberofinstancesofboththepatternsSiHXandXHSi,whereSiisawordintheclass.Thesamecanbedonefortheotherthreepatternstructures.Theprocesscanbebootstrappedasmorewordsareaddedtotheclass.NotethatourmethoddiffersfromthatofDavidovandRappoport(2006)inthathereweprovideanini-tialseedpair,representingourtargetconcept,whiletherethegoalisgroupingofasmanywordsaspos-sibleintoconceptclasses.Thefocusofourpaperisonrelationsinvolvingaspeciﬁcconcept.5.2CollectingcontextsForeachconceptwordS,wesearchthecorpusfordistinctcontextsinwhichSappears.(Forourpur-poses,acontextisawindowwithexactlyﬁvewordsorpunctuationmarksbeforeoraftertheconceptword;wechoose10,000ofthese,ifavailable.)Wecalltheaggregatetextfoundinallthesecontextwin-dowstheS-corpus.Fromamongthesecontexts,wechooseallpat-ternsoftheformH1SH2XH3orH1XH2SH3,where:235

•Xisawordthatappearswithfrequencybelowf1intheS-corpusandthathassufﬁcientlyhighpointwisemutualinformationwithS.WeusethesetwocriteriatoensurethatXisacontentwordandthatitisrelatedtoS.Thelowerthethresholdf1,thelessnoiseweallowin,thoughpossiblyattheexpenseofrecall.Weusedf1=1,000occurrencespermillionwords.•H2isastringofwordseachofwhichoccurswithfrequencyabovef2intheS-corpus.WewantH2toconsistmainlyofwordscommoninthecontextofSinordertorestrictpatternstothosethataresomewhatgeneric.Thus,inthecontextofcountrieswewouldliketoretainwordslikecapitalwhileeliminatingmorespe-ciﬁcwordsthatareunlikelytoexpressgenericpatterns.Weusedf2=100occurrencespermillionwords(thereisroomhereforautomaticoptimization,ofcourse).•H1andH3areeitherpunctuationorwordsthatoccurwithfrequencyabovef3intheS-corpus.ThisismainlytoensurethatXandSaren’tfragmentsofmulti-wordexpressions.Weusedf3=100occurrencespermillionwords.•Wecallthesepatterns,S-patternsandwecallXthetargetoftheS-pattern.TheideaisthatSandXverylikelystandinsomeﬁxedrelationtoeachotherwherethatrelationiscapturedbytheS-pattern.5.3GroupingS-patternsIfSisinfactrelatedtoXinsomeway,theremightbeanumberofS-patternsthatcapturethisrelation-ship.ForeachX,wegroupalltheS-patternsthathaveXasatarget.(NotethattwoS-patternswithtwodifferenttargetsmightbeotherwiseidentical,sothatessentiallythesamepatternmightappearintwodifferentgroups.)Wenowmergegroupswithlarge(morethan2/3)overlap.Wecalltheresultinggroups,S-groups.5.4IdentifyingpatternclustersIftheS-patternsinagivenS-groupactuallycapturesomerelationshipbetweenSandthetarget,thenonewouldexpectthatsimilargroupswouldappearforamultiplicityofconceptwordsS.SupposethatwehaveS-groupsforthreedifferentconceptwordsSsuchthatthepairwiseoverlapamongthethreegroupsismorethan2/3(whereforthispurposetwopatternsaredeemedidenticaliftheydifferonlyatSandX).ThenthesetofpatternsthatappearintwoorthreeoftheseS-groupsiscalledaclustercore.WenowgroupallpatternsinotherS-groupsthathaveanoverlapofmorethan2/3withtheclustercoreintoacandidatepatternpoolP.ThesetofallpatternsinPthatappearinatleasttwoS-groups(amongthosethatformedP)patterncluster.Apatternclusterthathaspatternsinstantiatedbyatleasthalfofthecon-ceptwordsissaidtorepresentarelation.5.5ReﬁningrelationsArelationconsistsofpairs(S,X)whereSisacon-ceptwordandXisthetargetofsomeS-patterninagivenpatterncluster.NotethatforagivenS,theremightbeoneormanyvaluesofXsatisfyingthere-lation.Asaﬁnalreﬁnement,foreachgivenS,werankallsuchXaccordingtopointwisemutualin-formationwithSandretainonlythehighest2/3.IfmostvaluesofShaveonlyasinglecorrespondingXsatisfyingtherelationandtheresthavenone,wetrytoautomaticallyﬁllinthemissingvaluesbysearch-ingthecorpusforrelevantS-patternsforthemissingvaluesofS.(Inourcasethecorpusistheweb,soweperformadditionalclarifyingqueries.)Finally,wedeleteallrelationsinwhichallcon-ceptwordsarerelatedtomosttargetwordsandallrelationsinwhichtheconceptwordsandthetargetwordsareidentical.Suchrelationscancertainlybeofinterest(seeSection7),butarenotourfocusinthispaper.5.6NotesonrequiredWebresourcesInourimplementationweusetheGooglesearchengine.Googlerestrictsindividualusersto1,000queriesperdayand1,000pagesperquery.Ineachstageweconductedqueriesiteratively,eachtimedownloadingall1,000documentsforthequery.Intheﬁrststageourgoalwastodiscoversym-metricrelationshipsfromthewebandconsequentlydiscoveradditionalconceptwords.Forqueriesinthisstageofouralgorithmweinvokedtworequire-ments.First,thequeryshouldcontainatleasttwocon-ceptwords.Thisprovedveryeffectiveinreduc-236

ingambiguity.Thusof1,000documentsforthequerybass,760dealwithmusic,whileifweaddtothequeryasecondwordfromtheintendedconcept(e.g.,barracuda),thennoneofthe1,000documentsdealwithmusicandthevastmajoritydealwithﬁsh,asintended.Second,weavoiddoingoverlappingqueries.TodothisweusedGoogle’sabilitytoexcludefromsearchresultsthosepagescontainingagiventerm(inourcase,oneoftheconceptwords).Weperformedupto300differentqueriesforin-dividualconceptsintheﬁrststageofouralgorithm.Inthesecondstage,weusedwebqueriestoas-sembleS-corpora.Onaverage,about1/3ofthecon-ceptwordsinitiallylackedsufﬁcientdataandweperformeduptotwentyadditionalqueriesforeachrareconceptwordtoﬁllitscorpus.Inthelaststage,whenclustersareconstructed,weusedwebqueriesforﬁllingmissingpairsofone-to-oneorseveral-to-severalrelationships.Theto-talnumberofﬁllingqueriesforaspeciﬁcconceptwasbelow1,000,andweneededonlytheﬁrstre-sultsofthesequeries.Empirically,ittookbetween0.5to6daylimits(i.e.,500–6,000queries)toex-tractrelationshipsforaconcept,dependingonitssize(thenumberofdocumentsusedforeachquerywasatmost100).ObviouslythisstrategycanbeimprovedbyfocusedcrawlingfromprimaryGooglehits,whichcandrasticallyreducetherequirednum-berofqueries.6EvaluationInthissectionwewishtoconsiderthevarietyofre-lationsthatcanbegeneratedbyourmethodfromagivenseedandtomeasurethequalityoftheserela-tionsintermsoftheirprecisionandrecall.Withregardtoprecision,twoclaimsarebeingmade.Oneisthatthegeneratedrelationscorrespondtoidentiﬁablerelations.Theotherclaimisthattotheextentthatageneratedrelationcanbereason-ablyidentiﬁed,thegeneratedpairsdoindeedbelongtotheidentiﬁedrelation.(Thereisasmalldegreeofcircularityinthischaracterizationbutthisisproba-blythebestwecanhopefor.)Asapracticalmatter,itisextremelydifﬁculttomeasureprecisionandrecallforrelationsthathavenotbeenpre-determinedinanyway.Foreachgen-eratedrelation,authoritativeresourcesmustbemar-shaledasagoldstandard.Forpurposesofevalu-ation,weranouralgorithmonthreerepresentativedomains–countries,ﬁshspeciesandstarconstel-lations–andtrackeddowngoldstandardresources(encyclopedias,academictexts,informativeweb-sites,etc)forthebulkoftherelationsgeneratedineachdomain.Thischoiceofdomainsallowedustoexploredifferentaspectsofalgorithmicbehavior.Countryandconstellationdomainsarebothwelldeﬁnedandcloseddomains.Howevertheyaresubstantiallydif-ferent.Countrynamesisarelativelylargedomainwhichhasverylowlexicalambiguity,andalargenumberofpotentiallyusefulrelations.Themainchallengeinthisdomainwastocaptureitwell.Constellationnames,incontrast,arearelativelysmallbuthighlyambiguousdomain.Theyareusedinpropernames,mythology,namesofentertainmentfacilitiesetc.Ourevaluationexaminedhowwellthealgorithmcandealwithsuchambiguity.Theﬁshdomaincontainsaveryhighnumberofmembers.Unlikecountries,itisasemi-opennon-homogenousdomainwithaverylargenumberofsubclassesandgroups.Also,unlikecountries,itdoesnotcontainmanypropernouns,whichareem-piricallygenerallyeasiertoidentifyinpatterns.Sothemainchallengeinthisdomainistoextractun-blurredrelationshipsandnottodivergefromthedo-mainduringtheconceptacquisitionphase.Wedonotshowhereall-to-allrelationshipssuchasﬁshparts(commontoalloralmostallﬁsh),be-causewefocusonrelationshipsthatseparatebe-tweenmembersoftheconceptclass,whicharehardertoacquireandevaluate.6.1CountriesOurseedconsistedoftwocountrynames.Thein-tendedresultfortheﬁrststageofthealgorithmwasalistofcountries.Thereare193countriesintheworld(www.countrywatch.com)someofwhichhavemultiplenamessothatthetotalnumberofcommonlyusedcountrynamesis243.Ofthese,223names(comprising180countries)arecharac-terstringswithnowhitespace.Sinceweconsideronlysinglewordnames,these223arethenameswehopetocaptureinthisstage.237

UsingtheseedwordsFranceandAngola,weobtained202countrynames(comprising167dis-tinctcountries)aswellas32othernames(consistingmostlyofnamesofothergeopoliticalentities).Us-ingthelistof223singlewordcountriesasourgoldstandard,thisgivesprecisionof0.90andrecallof0.86.(Tenotherseedpairsgaveresultsranginginprecision:0.86-0.93andrecall:0.79-0.90.)Thesecondpartofthealgorithmgeneratedasetof31binaryrelations.Ofthese,25wereclearlyidentiﬁablerelationsmanyofwhichareshowninTable1.Notethatforthreeofthesetherearestan-dardexhaustivelistsagainstwhichwecouldmea-surebothprecisionandrecall;fortheothersshown,sourceswereavailableformeasuringprecisionbutnoexhaustivelistwasavailablefromwhichtomea-surerecall,sowemeasuredcoverage(thenumberofcountriesforwhichatleastonetargetconceptisfoundasrelated).Anotherelevenmeaningfulrelationsweregener-atedforwhichwedidnotcomputeprecisionnum-bers.Theseincludecelebrity-from,animal-of,lake-in,borders-onandenemy-of.(ThesetofrelationsgeneratedbyotherseedpairsdifferedonlyslightlyfromthoseshownhereforFranceandAngola.)6.2FishspeciesInoursecondexperiment,ourseedconsistedoftwoﬁshspecies,barracudaandblueﬁsh.Thereare770specieslistedinWordNetofwhich447namesarecharacterstringswithnowhitespace.Theﬁrststageofthealgorithmreturned305ofthespecieslistedinWordnet,another37speciesnotlistedinWord-net,aswellas48othernames(consistingmostlyofotherseacreatures).Thesecondpartoftheal-gorithmgeneratedasetof15binaryrelationsallofwhicharemeaningful.ThoseforwhichwecouldﬁndsomegoldstandardarelistedinTable2.Otherrelationsgeneratedincludeserved-with,bait-for,food-type,spot-type,andgill-type.6.3ConstellationsOurseedconsistedoftwoconstellationnames,OrionandCassiopeia.Thereare88standardconstellations(www.astro.wisc.edu)someofwhichhavemultiplenamessothatthetotalnumberofcom-monlyusedconstellationsis98.Ofthese,87names(77constellations)arestringswithnowhitespace.RelationshipPrec.Rec/CovSamplepattern(Samplepair)capital-of0.92R=0.79in(x),capitalof(y),(Luanda,Angola)language-spoken-in0.92R=0.60to(x)orother(y)speaking(Spain,Spanish)in-region0.73R=0.71throughout(x),from(y)to(America,Canada)city-in0.82C=0.95west(x)–forecastfor(y).(England,London)river-in0.92C=0.68central(x),onthe(y)river(China,Haine)mountain-range-in0.77C=0.69the(x)mountainsin(y),(Chella,Angola)sub-region-of0.81C=0.81the(y)regionof(x),(Veneto,Italy)industry-of0.70C=0.90the(x)industryin(y),(Oil,Russia)island-in0.98C=0.55,(x)island,(y),(Bathurst,Canada)president-of0.86C=0.51president(x)of(y)has(Bush,USA)political-position-in0.81C=0.75former(x)of(y)face(President,Ecuador)political-party-of0.91C=0.53the(x)partyof(y),(Labour,England)festival-of0.90C=0.78the(x)festival,(y),(Tanabata,Japan)religious-denomination-of0.80C=0.62the(x)churchin(y),(Christian,Rome)Table1:Resultsonseed{France,Angola}.238

RelationshipPrec.CovSamplepattern(Samplepair)region-found-in0.830.80best(x)ﬁshingin(y).(Walleye,Canada)sea-found-in0.820.64of(x)catchesinthe(y)sea(Shark,Adriatic)lake-found-in0.790.51lake(y)isfamousfor(x),(Marion,Catﬁsh)habitat-of0.780.92,(x)andother(y)ﬁsh(Menhaden,Saltwater)also-called0.910.58.(y),alsocalled(x),(Lemonﬁsh,Ling)eats0.900.85the(x)eatsthe(y)and(Perch,Minnow)color-of0.950.85the(x)was(y)color(Shark,Gray)used-for-food0.800.53catch(x)–bestfor(y)or(Blueﬁsh,Sashimi)in-family0.950.60the(x)family,includes(y),(Salmonid,Trout)Table2:Resultsonseed{barracud,blueﬁsh}.Theﬁrststageofthealgorithmreturned81constel-lationnames(77distinctconstellations)aswellas38othernames(consistingmostlyofnamesofindi-vidualstars).Usingthelistof87singlewordcon-stellationnamesasourgoldstandard,thisgivespre-cisionof0.68andrecallof0.93.Thesecondpartofthealgorithmgeneratedasetoftenbinaryrelations.Ofthese,oneconcernedtravelandentertainment(constellationsarequitepopularasnamesofhotelsandlounges)andanotherthreewerenotinteresting.Apparently,therequire-mentthathalftheconstellationsappearinarelationlimitedthenumberofviablerelationssincemanyconstellationsarequiteobscure.ThesixinterestingrelationsareshowninTable3alongwithprecisionandcoverage.7DiscussionInthispaperwehaveaddressedanoveltypeofprob-lem:givenaspeciﬁcconcept,discoverinfullyun-supervisedfashion,arangeofrelationsinwhichitparticipates.Thiscanbeextremelyusefulforstudy-ingandresearchingaparticularconceptorﬁeldofstudy.Asothershaveshownaswell,twoconceptwordscanbesufﬁcienttogeneratealmosttheentireclasstowhichthewordsbelongwhentheclassiswell-deﬁned.Withthemethodpresentedinthispaper,usingnofurtheruser-providedinformation,wecan,foragivenconcept,automaticallygenerateadiversecollectionofbinaryrelationsonthisconcept.Theserelationsneednotbepre-speciﬁedinanyway.Re-sultsonthethreedomainsweconsideredindicatethat,takenasanaggregate,therelationsthataregen-eratedforagivendomainpaintaratherclearpictureoftherangeofinformationpertinenttothatdomain.Moreover,allthiswasdoneusingstandardsearchenginemethodsontheweb.Nolanguage-dependenttoolswereused(notevenstemming);infact,were-producedmanyofourresultsusingGoogleinRus-sian.Themethoddependsonanumberofnumericalparametersthatcontrolthesubtletradeoffbetweenquantityandqualityofgeneratedrelations.Thereiscertainlymuchroomfortuningoftheseparameters.Theconceptandtargetwordsusedinthispaperaresinglewords.Extendingthistomultiple-wordexpressionswouldsubstantiallycontributetotheap-plicabilityofourresults.Inthisresearchweeffectivelydisregardmanyre-lationshipsofanall-to-allnature.However,suchrelationshipscanoftenbeveryusefulforontologyconstruction,sinceinmanycasestheyintroducestrongconnectionsbetweentwodifferentconcepts.Thus,forﬁshwediscoveredthatoneoftheall-to-allrelationshipscapturesaprecisesetofﬁshbodyparts,andanothercapturesswimmingverbs.Suchrelationsintroducestronganddistinctconnectionsbetweentheconceptofﬁshandtheconceptsofﬁsh-body-partsandswimming.Suchconnectionsmaybeextremelyusefulforontologyconstruction.239

RelationshipPrec.CovSamplepattern(Samplepair)nearby-constellation0.870.70constellation(x),near(y),(Auriga,Taurus)star-in0.820.76star(x)in(y)is(Antares,Scorpius)shape-of0.900.55,(x)isdepictedas(y).(Lacerta,Lizard)abbreviated-as0.930.90.(x)abbr(y),(Hidra,Hya)cluster-types-in0.921.00famous(x)clusterin(y),(Praesepe,Cancer)location0.820.70,(x)isa(y)constellation(Draco,Circumpolar)Table3:Resultsonseed{Orion,Cassiopeia}.ReferencesAgichtein,E.,Gravano,L.,2000.Snowball:Extractingrelationsfromlargeplain-textcollections.Proceedingsofthe5thACMInternationalConferenceonDigitalLibraries.Alfonseca,E.,Ruiz-Casado,M.,Okumura,M.,Castells,P.,2006.Towardslarge-scalenon-taxonomicrelationextraction:estimatingtheprecisionofroteextractors.WorkshoponOntologyLearningandPopulationatCOLING-ACL’06.Berland,M.,Charniak,E.,1999.Findingpartsinverylargecorpora.ACL’99.ChklovskiT.,PantelP.,2004.VerbOcean:miningthewebforﬁne-grainedsemanticverbrelations.EMNLP’04.Costello,F.,Veale,T.,Dunne,S.,2006.UsingWord-Nettoautomaticallydeducerelationsbetweenwordsinnoun-nouncompounds,COLING-ACL’06.Davidov,D.,Rappoport,A.,2006.Efﬁcientunsuperviseddiscoveryofwordcategoriesusingsymmetricpatternsandhighfrequencywords.COLING-ACL’06.Etzioni,O.,Cafarella,M.,Downey,D.,Popescu,A.,Shaked,T.,Soderland,S.,Weld,D.,Yates,A.,2004.Methodsfordomain-independentinformationextrac-tionfromtheweb:anexperimentalcomparison.AAAI’04.Hasegawa,T.,Sekine,S.,Grishman,R.,2004.Discover-ingrelationsamongnamedentitiesfromlargecorpora.ACL’04.Hassan,H.,Hassan,A.,Emam,O.,2006.unsupervisedinformationextractionapproachusinggraphmutualreinforcement.EMNLP’06.Hearst,M.,1992.Automaticacquisitionofhyponymsfromlargetextcorpora.COLING’92.Moldovan,D.,Badulescu,A.,Tatu,M.,Antohe,D.,Girju,R.,2004.Modelsforthesemanticclassiﬁca-tionofnounphrases.WorkshoponComput.LexicalSemanticsatHLT-NAACL’04.Pantel,P.,Ravichandran,D.,Hovy,E.,2004.Towardsterascaleknowledgeacquisition.COLING’04.Pasca,M.,Lin,D.,Bigham,J.,LifchitsA.,Jain,A.,2006.Namesandsimilaritiesontheweb:factextractioninthefastlane.COLING-ACL’06.Roark,B.,Charniak,E.,1998.Noun-phraseco-occurrencestatisticsforsemi-automaticsemanticlex-iconconstruction.ACL’98.RosenfeldB.,Feldman,R.:URES:anunsupervisedwebrelationextractionsystem.Proceedings,ACL’06PosterSessions.Sekine,S.,2006On-demandinformationextraction.COLING-ACL’06.Strube,M.,Ponzetto,S.,2006.WikiRelate!computingsemanticrelatednessusingWikipedia.AAAI’06.SuchanekF.M.,G.Ifrim,G.Weikum.2006.LEILA:learningtoextractinformationbylinguisticanalysis.WorkshoponOntologyLearningandPopulationatCOLING-ACL’06.Turney,P.,2006.Expressingimplicitsemanticrelationswithoutsupervision.COLING-ACL’06.Widdows,D.,Dorow,B.,2002.Agraphmodelforunsu-pervisedLexicalacquisition.COLING’02.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 240–247,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

240

AddingNounPhraseStructuretothePennTreebankDavidVadasandJamesR.CurranSchoolofInformationTechnologiesUniversityofSydneyNSW2006,Australia dvadas1,james@it.usyd.edu.auAbstractThePennTreebankdoesnotannotatewithinbasenounphrases(NPs),commit-tingonlyto(cid:3)atstructuresthatignorethecomplexityofEnglishNPs.ThismeansthattoolstrainedonTreebankdatacannotlearnthecorrectinternalstructureofNPs.Thispaperdetailstheprocessofaddinggold-standardbracketingwithineachnounphraseinthePennTreebank.Wethenexaminetheconsistencyandreliabil-ityofourannotations.Finally,weusethisresourcetodetermineNPstructureusingseveralstatisticalapproaches,thusdemonstratingtheutilityofthecorpus.ThisaddsdetailtothePennTreebankthatisnecessaryformanyNLPapplications.1IntroductionThePennTreebank(Marcusetal.,1993)isperhapsthemostin(cid:3)uentialresourceinNaturalLanguageProcessing(NLP).Itisusedasastandardtrain-ingandevaluationcorpusinmanysyntacticanalysistasks,rangingfrompartofspeech(POS)taggingandchunking,tofullparsing.Unfortunately,thePennTreebankdoesnotanno-tatetheinternalstructureofbasenounphrases,in-steadleavingthem(cid:3)at.Thissigni(cid:2)cantlysimpli(cid:2)edandspedupthemanualannotationprocess.Therefore,anysystemtrainedonPennTreebankdatawillbeunabletomodelthesyntacticandse-manticstructureinsidebase-NPs.ThefollowingNPisanexampleofthe(cid:3)atstruc-tureofbase-NPswithinthePennTreebank:(NP(NNPAir)(NNPForce)(NNcontract))AirForceisaspeci(cid:2)centityandshouldformasep-arateconstituentunderneaththeNP,asinournewannotationscheme:(NP(NML(NNPAir)(NNPForce))(NNcontract))WeuseNMLtospecifythatAirForcetogetherisanominalmodi(cid:2)erofcontract.Addingthisannota-tionbetterrepresentsthetruesyntacticandseman-ticstructure,whichwillimprovetheperformanceofdownstreamNLPsystems.Ourmaincontributionisagold-standardlabelledbracketingforeveryambiguousnounphraseinthePennTreebank.Wedescribetheannotationguide-linesandprocess,includingtheuseofnameden-titydatatoimproveannotationquality.Wecheckthecorrectnessofthecorpusbymeasuringinter-annotatoragreement,byreannotatingthe(cid:2)rstsec-tion,andbycomparingagainstthesub-NPstructureinDepBank(Kingetal.,2003).WealsogiveananalysisofourextendedTree-bank,quantifyinghowmuchstructurewehaveadded,andhowitisdistributedacrossNPs.Fi-nally,wetesttheutilityoftheextendedTreebankfortrainingstatisticalmodelsontwotasks:NPbracket-ing(Lauer,1995;NakovandHearst,2005)andfullparsing(Collins,1999).Thisnewresourcewillallowanysystemoranno-tatedcorpusdevelopedfromthePennTreebank,torepresentnounphrasestructuremoreaccurately.241

2MotivationManyapproachestoidentifyingbasenounphraseshavebeenexploredaspartofchunking(RamshawandMarcus,1995),butdeterminingsub-NPstruc-tureisrarelyaddressed.Wecouldusemulti-wordexpressions(MWEs)toidentifysomestructure.Forexample,knowingstockmarketisaMWEmayhelpbracketstockmarketpricescorrectly,andNamedEntities(NEs)canbeusedthesameway.However,thisonlyresolvesNPsdominatingMWEsorNEs.Understandingbase-NPstructureisimportant,sinceotherwiseparserswillproposenonsensicalnounphraseslikeForcecontractbydefaultandpassthemontodownstreamcomponents.Forexample,QuestionAnswering(QA)systemsneedtosupplyanNPastheanswertoafactoidquestion,oftenus-ingaparsertoidentifycandidateNPstoreturntotheuser.Iftheparsernevergeneratesthecorrectsub-NPstructure,thenthesystemmayreturnanon-sensicalanswereventhoughthecorrectdominatingnounphrasehasbeenfound.Base-NPstructureisalsoimportantforanno-tateddataderivedfromthePennTreebank.Forinstance,CCGbank(Hockenmaier,2003)wascre-atedbysemi-automaticallyconvertingtheTreebankphrasestructuretoCombinatoryCategorialGram-mar(CCG)(Steedman,2000)derivations.SinceCCGderivationsarebinarybranching,theycannotdi-rectlyrepresentthe(cid:3)atstructureofthePennTree-bankbase-NPs.WithoutthecorrectbracketingintheTreebank,strictlyright-branchingtreeswerecreatedforallbase-NPs.Thishasanunwelcomeeffectwhencon-junctionsoccurwithinanNP(Figure1).Anaddi-tionalgrammarruleisneededjusttogetaparse,butitisstillnotcorrect(Hockenmaier,2003,p.64).Theawkwardconversionresultsinbracketing(a)whichshouldbe(b):(a)(consumer((electronics)and(appliances(retailingchain))))(b)((((consumerelectronics)andappliances)retailing)chain)WehavepreviouslyexperimentedwithusingNEstoimproveparsingperformanceonCCGbank.Duetothemis-alignmentofNEsandright-branchingNPs,theincreaseinperformancewasnegligible.NN/NconsumerNN/NelectronicsNconjandNN/NappliancesNN/NretailingNchainFigure1:CCGderivationfromHockenmaier(2003)3BackgroundTheNPbracketingtaskhasoftenbeenposedintermsofchoosingbetweentheleftorrightbranch-ingstructureofthreewordnouncompounds:(a)(world(oilprices))(cid:150)Right-branching(b)((crudeoil)prices)(cid:150)Left-branchingMostapproachestotheproblemuseunsupervisedmethods,basedoncompetingassociationstrengthbetweentwoofthewordsinthecompound(Mar-cus,1980,p.253).Therearetwopossiblemodelstochoosefrom:dependencyoradjacency.Thede-pendencymodelcomparestheassociationbetweenwords1-2towords1-3,whiletheadjacencymodelcompareswords1-2towords2-3.Lauer(1995)hasdemonstratedsuperiorperfor-manceofthedependencymodelusingatestsetof244(216unique)nouncompoundsdrawnfromGrolier’sencyclopedia.Thisdatahasbeenusedtoevaluatemostresearchsince.HeusesRoget’sthe-saurustosmoothwordsintosemanticclasses,andthencalculatesassociationbetweenclassesbasedontheircountsina(cid:147)trainingset(cid:148)alsodrawnfromGrolier’s.Heachieves80.7%accuracyusingPOStagstoindentifybigramsinthetrainingset.LapataandKeller(2004)deriveestimatesfromwebcounts,andonlycompareatalexicallevel,achieving78.7%accuracy.NakovandHearst(2005)alsousewebcounts,butincorporateadditionalcountsfromseveralvariationsonsimplebigramqueries,includingqueriesforthepairsofwordscon-catenatedorjoinedbyahyphen.Thisresultsinanimpressive89.3%accuracy.Therehavealsobeenattemptstosolvethistaskusingsupervisedmethods,eventhoughthelackofgold-standarddatamakesthisdif(cid:2)cult.Girjuetal.242

(2005)drawatrainingsetfromrawWSJtextanduseittotrainadecisiontreeclassi(cid:2)erachieving73.1%accuracy.Whentheyshuf(cid:3)edtheirdatawithLauer’stocreateanewtestandtrainingsplit,theiraccu-racyincreasedto83.1%whichmaybearesultofthe10%duplicationinLauer’stestset.WehavecreatedanewNPbracketingdatasetfromourextendedTreebankbyextractingallright-mostthreenounsequencesfrombase-NPs.Ourini-tialexperimentsarepresentedinSection6.1.4CorpusCreationAccordingtoMarcusetal.(1993),askingannota-torstomarkupbase-NPstructuresigni(cid:2)cantlyre-ducedannotationspeed,andforthisreasonbase-NPswereleft(cid:3)at.Thebracketingguidelines(Biesetal.,1995)alsomentiontheconsiderabledif(cid:2)cultyofidentifyingthecorrectscopefornominalmodi-(cid:2)ers.Wefoundhowever,thatwhiletherearecer-tainlydif(cid:2)cultcases,thevastmajorityarequitesim-pleandcanbeannotatedreliably.Ourannotationphilosophycanbesummarisedas:1.mostcasesareeasyand(cid:2)tacommonpattern;2.prefertheimplicitright-branchingstructurefordif(cid:2)cultdecisions.Financejargonwasacom-monsourceofthese;3.markverydif(cid:2)culttobracketNPsanddiscusswithotherannotatorslater;Duringthisprocessweidenti(cid:2)ednumerouscasesthatrequireamoresophisticatedannotationscheme.Therearegenuine(cid:3)atcases,primarilynameslikeJohnA.Smith,thatwewouldliketodistinguishfromimplicitlyright-branchingNPsinthenextversionofthecorpus.Althoughourschemeisstilldeveloping,webelievethatthecurrentannotationisalreadyuse-fulforstatisticalmodelling,andwedemonstratethisempiricallyinSection6.4.1AnnotationProcessOurannotationguidelines1arebasedonthosede-velopedforannotatingfullsub-NPstructureinthebiomedicaldomain(Kulicketal.,2004).Theanno-tationguidelinesforthisbiomedicalcorpus(anad-dendumtothePennTreebankguidelines)introducetheuseofNMLnodestomarkinternalNPstructure.1Theguidelinesandcorpusareavailableonourwebpages.Insummary,ourguidelinesleaveright-branchingstructuresuntouched,andinsertlabelledbracketsaroundleft-branchingstructures.ThelabelofthenewlycreatedconstituentisNMLorJJP,dependingonwhetheritsheadisanounoranadjective.WealsochosenottoaltertheexistingPennTreebankannotation,eventhoughtheannotatorsfoundmanyerrorsduringtheannotationprocess.WewantedtokeepourextendedTreebankassimilartotheorigi-nalaspossible,sothattheyremaincomparable.Wedevelopedabracketingtool,whichidenti(cid:2)esambiguousNPsandpresentsthemtotheuserfordisambiguation.AnambiguousNPisany(possiblynon-base)NPwiththreeormorecontiguouschil-drenthatareeithersinglewordsoranotherNP.Cer-taincommonpatterns,suchasthreewordsbegin-ningwithadeterminer,areunambiguous,andwere(cid:2)lteredout.TheannotatorisalsoshowntheentiresentencesurroundingtheambiguousNP.Thebracketingtooloftensuggestsabracket-ingusingrulesbasedmostlyonnamedentitytags,whicharedrawnfromtheBBNcorpus(WeischedelandBrunstein,2005).Forexample,sinceAirForceisgivenORGtags,thetoolsuggeststhattheybebracketedtogether(cid:2)rst.Othersuggestionscomefrompreviousbracketingsofthesamewords,whichhelpstokeeptheannotatorconsistent.Twopostprocesseswerecarriedouttoincreaseannotationconsistencyandcorrectness.915dif(cid:2)-cultNPsweremarkedbytheannotatorandwerethendiscussedwithtwootherexperts.Secondly,cer-tainphrasesthatoccurrednumeroustimesandwerenon-trivialtobracket,e.g.LondonInterbankOf-feredRate,wereidenti(cid:2)ed.Anextrapasswasmadethroughthecorpus,ensuringthateveryinstanceofthesephraseswasbracketedconsistently.4.2AnnotationTimeAnnotationinitiallytookover9hourspersectionoftheTreebank.However,withpracticethiswasre-ducedtoabout3hourspersection.Eachsectioncontainsaround2500ambiguousNPs,i.e.annotat-ingtookapproximately5secondsperNP.MostNPsrequirenobracketing,or(cid:2)tintoastandardpatternwhichtheannotatorsoonbecomesaccustomedto,hencethetaskcanbeperformedquitequickly.FortheoriginalbracketingoftheTreebank,anno-tatorsperformedat375(cid:150)475wordsperhouraftera243

PREC.RECALLF-SCOREBrackets89.1787.5088.33Dependencies96.4096.4096.40Brackets,revised97.5698.0397.79Dependencies,revised99.2799.2799.27Table1:Agreementbetweenannotatorsfewweeks,andincreasedtoabout1000wordsperhouraftergainingmoreexperience(Marcusetal.,1993).Forourannotationprocess,countingeachwordineveryNPshown,ourspeedwasaround800wordsperhour.This(cid:2)gureisnotunexpected,asthetaskwasnotlargeenoughtogetmorethanamonth’sexperience,andthereislessstructuretoannotate.5CorpusAnalysis5.1Inter-annotatorAgreementTheannotationwasperformedbythe(cid:2)rstauthor.AsecondComputationalLinguisticsPhDstudentalsoannotatedSection23,allowinginter-annotatoragreement,andthereliabilityoftheannotations,tobemeasured.Thisalsomaximisedthequalityofthesectionusedforparsertesting.Wemeasuredtheproportionofmatchingbrack-etsanddependenciesbetweenannotators,showninTable1,bothbeforeandaftertheydiscussedcasesofdisagreementandrevisedtheirannotations.Thenumberofdependenciesis(cid:2)xedbythelengthoftheNP,sothedependencyprecisionandrecallarethesame.Countingmatchedbracketsisaharshereval-uation,astherearemanyNPsthatbothannotatorsagreeshouldhavenoadditionalbracketing,whicharenottakenintoaccountbythemetric.Thedisagreementsoccurredforasmallnumberofrepeatedinstances,suchasthiscase:(NP(NP(NNPGoldman)(NML(NNPGoldman)(,,)(,,)(NNPSachs)(NNPSachs))(CC&)(NNPCo))(CC&)(NNPCo))The(cid:2)rstannotatorfeltthatGoldman,SachsshouldformtheirownNMLconstituent,whilethesecondannotatordidnot.WecanalsolookatexactmatchingonNPs,wheretheannotatorsoriginallyagreedin2667of2908cases(91.71%),andafterrevision,in2864of2907cases(98.52%).Theseresultsdemonstratethathighagreementratesareachievablefortheseannotations.MATCHEDTOTAL%Bydependency1409(1315)147995.27(88.91)Bynounphrase562(489)62689.78(78.12)Bydependency,onlyannotatedNPs578(543)62792.19(86.60)Bynounphrase,onlyannotatedNPs186(162)22981.22(70.74)Table2:AgreementwithDepBank5.2DepBankAgreementAnotherapproachtomeasuringannotatorreliabil-ityistocomparewithanindependentlyannotatedcorpusonthesametext.WeusedthePARC700De-pendencyBank(Kingetal.,2003)whichconsistsof700Section23sentencesannotatedwithlabelledde-pendencies.WeusetheBriscoeandCarroll(2006)versionofDepBank,a560sentencesubsetusedtoevaluatetheRASPparser.SometranslationisrequiredtocompareourbracketstoDepBankdependencies.Wemapthebracketstodependenciesby(cid:2)ndingtheheadoftheNP,usingtheCollins(1999)head(cid:2)ndingrules,andthencreatingadependencybetweeneachotherchild’sheadandthishead.Thisdoesnotworkper-fectly,andmismatchesoccurbecauseofwhichde-pendenciesDepBankmarksexplicitly,andhowitchoosesheads.Theerrorsareinvestigatedmanuallytodeterminetheircause.TheresultsareshowninTable2,withthenum-berofagreementsbeforemanualcheckingshowninparentheses.OnceagainthedependencynumbersarehigherthanthoseattheNPlevel.Similarly,whenweonlylookatcaseswherewehaveinsertedsomeannotations,wearelookingatmoredif(cid:2)cultcasesandthescoreisnotashigh.Theresultsofthisanalysisarequitepositive.Overhalfofthedisagreementsthatoccur(inei-thermeasure)arecausedbyhowcompanynamesarebracketed.Whilewehavealwaysseparatedthecompanynamefrompost-modi(cid:2)erssuchasCorpandInc,DepBankdoesnotinmostcases.Theseresultsshowthatconsistentlyandcorrectlybracket-ingnounphrasestructureispossible,andthatinter-annotatoragreementisatanacceptablelevel.5.3CorpusCompositionandConsistencyLookingattheentirePennTreebankcorpus,theannotationtool(cid:2)nds60959ambiguousNPsoutofthe432639NPsinthecorpus(14.09%).22851of244

LEVELCOUNTPOSTAGSEXAMPLE1073JJJJNNSbigredcars1581DTJJNNNNahighinterestrateNP1693JJNNNNShighinterestrates3557NNPNNPNNPJohnA.Smith1468NNNN(interestrate)rises1538JJNN(heavytruck)rentalsNML1650NNPNNPNNP(A.B.C.)Corp8524NNPNNP(JohnSmith)Jr.82JJJJ(darkred)car114RBJJ(veryhigh)ratesJJP122JJCCJJ(bigandred)apples160(cid:147)JJ(cid:148)((cid:147)smart(cid:148))carsTable3:CommonPOStagsequencesthese(37.49%)hadbracketsinsertedbytheannota-tor.Thisisasweexpect,asthemajorityofNPsareright-branching.Ofthebracketsadded,22368wereNMLnodes,while863wereJJP.Tocompare,wecancountthenumberofexistingNPandADJPnodesfoundintheNPsthatthebrack-etingtoolpresents.We(cid:2)ndthereare32772NPchil-dren,and579ADJP,whicharequitesimilarnum-berstotheamountofnodeswehaveadded.Fromthis,wecansaythatourannotationprocesshasin-troducedalmostasmuchstructuralinformationintoNPsastherewasintheoriginalPennTreebank.Table3showsthemostcommonPOStagse-quencesforNP,NMLandJJPnodes.AnexampleisgivenshowingtypicalwordsthatmatchthePOStags.ForNMLandJJP,wealsoshowthewordsbracketed,astheywouldappearunderanNPnode.WecheckedtheconsistencyoftheannotationsbyidentifyingNPswiththesamewordsequenceandcheckingwhethertheywerealwaysbracketediden-tically.Afterthe(cid:2)rstpassthrough,therewere360wordsequenceswithmultiplebracketings,whichoccurredin1923NPinstances.489ofthesein-stancesdifferedfromthemajoritycaseforthatse-quence,andwereprobablyerrors.Theannotatorhadmarkedcertaindif(cid:2)cultandcommonlyrepeatingNPs.Fromthiswegeneratedalistofphrases,andthenmadeanotherpassthroughthecorpus,synchronisingallinstancesthatcon-tainedoneofthesephrases.Afterthis,only150in-stancesdifferedfromthemajoritycase.Inspectingtheseremaininginconsistenciesshowedcaseslike:(NP-TMP(NML(NNPNov.)(CD15))(,,)(CD1999))wherewewereinconsistentininsertingtheNMLnodebecausethePennTreebanksometimesalreadyhasthestructureannotatedunderanNPnode.Sincewedonotmakechangestoexistingbrackets,wecannot(cid:2)xthesecases.Otherinconsistenciesarerare,butwillbeexaminedandcorrectedinafuturerelease.TheannotatormadeasecondpassoverSection00tocorrectchangesmadeafterthebeginningoftheannotationprocess.Comparingthetwopassescangiveussomeideaofhowtheannotatorchangedashegrewmorepracticedatthetask.We(cid:2)ndthattheoldandnewversionsareidenti-calin88.65%ofNPs,withlabelledprecision,recallandF-scorebeing97.17%,76.69%and85.72%re-spectively.Thistellsusthatthereweremanybrack-etsoriginallymissedthatwereaddedinthesecondpass.ThisisnotsurprisingsincethemainproblemwithhowSection00wasannotatedoriginallywasthatcompanynameswerenotseparatedfromtheirpost-modi(cid:2)er(suchasCorp).Besidesthis,itsug-geststhatthereisnotagreatdealofdifferencebe-tweenanannotatorjustlearningthetask,andonewhohashadagreatdealofexperiencewithit.5.4NamedEntitySuggestionsWehavealsoevaluatedhowwellthesuggestionfea-tureoftheannotationtoolperforms.Inparticular,wewanttodeterminehowusefulnamedentitiesareindeterminingthecorrectbracketing.Weranthetoolovertheoriginalcorpus,follow-ingNE-basedsuggestionswherepossible.We(cid:2)ndthatwhenevaluatedagainstourannotations,theF-scoreis50.71%.Weneedtolookcloseratthepre-cisionandrecallthough,astheyarequitedifferent.Theprecisionof93.84%isquitehigh.However,therearemanybracketswheretheentitiesdonothelpatall,andsotherecallofthismethodwasonly34.74%.ThissuggeststhataNEfeaturemayhelptoidentifythecorrectbracketinginonethirdofcases.6ExperimentsHavingbracketedNPsinthePennTreebank,wenowdescribeourinitialexperimentsonhowthisaddi-tionallevelofannotationcanbeexploited.6.1NPBracketingDataTheobvious(cid:2)rsttasktoconsiderisnounphrasebracketingitself.Weimplementasimilarsystemto245

CORPUS#ITEMSLEFTRIGHTPennTreebank558258.99%41.01%Lauer’s24466.80%33.20%Table4:ComparisonofNPbracketingcorporaN-GRAMMATCHUnigrams51.20%Adjacencybigrams6.35%Dependencybigrams3.85%Allbigrams5.83%Trigrams1.40%Table5:LexicaloverlapLauer(1995),describedinSection3,andreportonresultsfromourowndataandLauer’soriginalset.First,weextractedthreewordnounsequencesfromalltheambiguousNPs.Ifthelastthreechil-drenarenouns,thentheybecameanexampleinourdataset.IfthereisaNMLnodecontainingthe(cid:2)rsttwonounsthenitisleft-branching,otherwiseitisright-branching.Becauseweareonlylookingattheright-mostpartoftheNP,weknowthatwearenotextractinganynonsensicalitems.Wealsoremoveallitemswherethenounsareallpartofanamedentitytoeliminate(cid:3)atstructurecases.StatisticsaboutthenewdatasetandLauer’sdatasetaregiveninTable4.Ascanbeseen,thePennTreebankbasedcorpusissigni(cid:2)cantlylarger,andhasamoreevenmixofleftandright-branchingnounphrases.Wealsomeasuredtheamountoflexicaloverlapbetweenthetwocorpora,showninTable5.Thisdisplaysthepercentageofn-gramsinLauer’scorpusthatarealsoinourcorpus.Wecanclearlyseethatthetwocorporaarequitedissimilar,asevenonunigramsbarelyhalfareshared.6.2NPBracketingResultsWithournewdataset,webeganrunningexperi-mentssimilartothosecarriedoutintheliterature(NakovandHearst,2005).Weimplementedbothanadjacencyanddependencymodel,andthreediffer-entassociationmeasures:rawcounts,bigramproba-bility,and.Wedrawourcountsfromacorpusofn-gramcountscalculatedover1trillionwordsfromtheweb(BrantsandFranz,2006).Theresultsfromtheexperiments,onbothourandLauer’sdataset,areshowninTable6.OurresultsASSOC.MEASURELAUERPTBRawcounts,adj.75.41%77.46%Rawcounts,dep.77.05%68.85%Probability,adj.71.31%76.42%Probability,dep.80.33%69.56%,adj.71.31%77.93%,dep.74.59%68.92%Table6:Bracketingtask,unsupervisedresultsFEATURESLAUER10-FOLDCROSSAllfeatures80.74%89.91%(1.04%)Lexical71.31%84.52%(1.77%)n-gramcounts75.41%82.50%(1.49%)Probability72.54%78.34%(2.11%)	75.41%80.10%(1.71%)Adjacencymodel72.95%79.52%(1.32%)Dependencymodel78.69%72.86%(1.48%)Bothmodels76.23%79.67%(1.42%)-Lexical79.92%85.72%(0.77%)-n-gramcounts80.74%89.11%(1.39%)-Probability79.10%89.79%(1.22%)-	80.74%89.79%(0.98%)-Adjacencymodel81.56%89.63%(0.96%)-Dependencymodel81.15%89.72%(0.86%)-Bothmodels81.97%89.63%(0.95%)Table7:Bracketingtask,supervisedresultsonLauer’scorpusaresimilartothosereportedpre-viously,withthedependencymodeloutperformingtheadjacencymodelonallmeasures.Thebigramprobabilityscoreshighestoutofallthemeasures,whilethescoreperformedtheworst.Theresultsonthenewcorpusareevenmoresur-prising,withtheadjacencymodeloutperformingthedependencymodelbyawidemargin.The
mea-suregivesthehighestaccuracy,butstillonlyjustoutperformstherawcounts.Ouranalysisshowsthatthegoodperformanceoftheadjacencymodelcomesfromthelargenumberofnamedentitiesinthecorpus.Whenweremoveallitemsthathaveanywordasanentity,theresultschange,andthede-pendencymodelissuperior.Wealsosuspectthatanothercauseoftheunusualresultsisthedifferentproportionsofleftandright-branchingNPs.Withalargeannotatedcorpus,wecannowrunsupervisedNPbracketingexperiments.Wepresenttwocon(cid:2)gurationsinTable7:trainingonourcorpusandtestingonLauer’sset;andperforming10-foldcrossvalidationusingourcorpusalone.Thefeaturesetweexploreencodestheinforma-tionweusedintheunsupervisedexperiments.Ta-246

OVERALLONLYNMLJJPNOTNMLJJPPREC.RECALLF-SCOREPREC.RECALLF-SCOREPREC.RECALLF-SCOREOriginal88.9388.9088.92(cid:150)(cid:150)(cid:150)88.9388.9088.92NMLandJJPbracketed88.6388.2988.4677.9362.9369.6388.8588.9388.89Relabelledbrackets88.1787.8888.0291.9351.3865.9187.8688.6588.25Table8:Parsingperformanceble7showstheperformancewith:allfeatures,fol-lowedbytheindividualfeatures,and(cid:2)nally,afterremovingindividualfeatures.Thefeaturesetincludes:lexicalfeaturesforeachn-graminthenouncompound;n-gramcountsforunigrams,bigramsandtrigrams;rawprobabilityandassociationscoresforallthreebigramsinthecompound;andtheadjacencyanddependencyre-sultsforallthreeassociationmeasures.Wedis-cretisedthenon-binaryfeaturesusinganimplemen-tationofFayyadandIrani’s(1993)algorithm,andclassifyusingMegaM2.TheresultsonLauer’ssetdemonstratethatthedependencymodelperformswellbyitselfbutnotwiththeotherfeatures.Infact,abetterresultcomesfromusingeveryfeatureexceptthosefromthede-pendencyandadjacencymodels.Itisalsoimpres-sivehowgoodtheperformanceis,consideringthelargedifferencesbetweenourdatasetandLauer’s.Thesedifferencesalsoaccountforthedisparatecross-validation(cid:2)gures.Onthisdata,thelexicalfea-turesperformthebest,whichistobeexpectedgiventhenatureofthecorpus.Thebestmodelinthiscasecomesfromusingallthefeatures.6.3CollinsParsingWecanalsolookattheimpactofournewannota-tionsuponfullstatisticalparsing.WeuseBikel’simplementation(Bikel,2004)ofCollins’parser(Collins,1999)inordertocarryouttheseexperi-ments,usingthenon-de(cid:2)cientCollinssettings.Thenumberswegivearelabelledbracketprecision,re-callandF-scoresforallsentences.Bikelmentionsthatbase-NPsaretreatedverydifferentlyinCollins’parser,andsoitwillbeinterestingtoobservetheresultsusingournewannotations.Firstly,wecomparetheparser’sperformanceontheoriginalPennTreebankandthenewNMLandJJPbracketedversion.Table8showsthatthenewbrack-etsmakeparsingmarginallymoredif(cid:2)cultoverall2Availableathttp://www.cs.utah.edu/hal/megam/(byabout0.5%inF-score).TheperformanceononlythenewNMLandJJPbracketsisnotveryhigh.Thisshowsthedif(cid:2)cultyofcorrectlybracketingNPs.Conversely,the(cid:2)guresforallbracketsexceptNMLandJJPareonlyatinyamountlessinourextendedcorpus.ThismeansthatperformanceforotherphrasesishardlychangedbythenewNPbrackets.WealsorananexperimentwherethenewNMLandJJPlabelswererelabelledasNPandADJP.ThesearethelabelsthatwouldbegivenifNPswereorig-inallybracketedwiththerestofthePennTreebank.Thismeantthemodelwouldnothavetodiscrim-inatebetweentwodifferenttypesofnounandad-jectivestructure.Theperformance,asshowninTa-ble8,wasevenlowerwiththisapproach,suggestingthatthedistinctionislargerthanweanticipated.Ontheotherhand,theprecisiononNMLandJJPcon-stituentswasquitehigh,sotheparserisabletoiden-tifyatleastsomeofthestructureverywell.7ConclusionTheworkpresentedinthispaperisa(cid:2)rststepto-wardsaccuraterepresentationofnounphrasestruc-tureinNLPcorpora.Thereareseveraldistinctionsthatourannotationcurrentlyignoresthatwewouldliketoidentifycorrectlyinthefuture.Firstly,NPswithgenuine(cid:3)atstructurearecurrentlytreatedasimplicitlyrightbranching.Secondly,thereisstillambiguityindeterminingtheheadofanounphrase.AlthoughCollins’head(cid:2)ndingrulesworkinmostNPs,therearecasessuchasIBMAustraliawheretheheadisnottheright-mostnoun.Similarly,ap-positionisverycommoninthePennTreebank,inNPssuchasJohnSmith,IBMpresident.Wewouldliketobeabletoidentifythesemulti-headconstructsproperly,ratherthansimplytreatingthemasasingleentity(orevenworse,astwodifferententities).HavingthecorrectNPstructurealsomeansthatwecannowrepresentthetruestructureinCCGbank,oneoftheproblemswedescribedearlier.Transfer-247

ringourannotationsshouldbefairlysimple,requir-ingjustafewchangestohowNPsaretreatedinthecurrenttranslationprocess.Theadditionofconsistent,gold-standard,nounphrasestructuretoalargecorpusisasigni(cid:2)cantachievement.Wehaveshownthatthetheseanno-tationscanbecreatedinafeasibletimeframewithhighinter-annotatoragreementof98.52%(measur-ingexactNPmatches).Thenewbracketscauseonlyasmalldropinparsingperformance,andnosigni(cid:2)-cantdecreaseontheexistingstructure.AsNEswereusefulforsuggestingbracketsautomatically,wein-tendtoincorporateNEinformationintostatisticalparsingmodelsinthefuture.Ourannotatedcorpuscanimprovetheperfor-manceofanysystemthatreliesonNPsfromparserstrainedonthePennTreebank.ACollins’parsertrainedonourcorpusisnowabletoidentifysub-NPbrackets,makingitofuseinotherNLPsystems.QAsystems,forexample,willbeabletoexploitin-ternalNPstructure.Inthefuture,wewillimprovetheparser’sperformanceonNMLandJJPbrackets.Wehaveprovidedasigni(cid:2)cantlylargercorpusforanalysingNPstructurethanhaseverbeenmadeavailablebefore.Thisisintegratedwithinperhapsthemostin(cid:3)uentialcorpusinNLP.Thelargenum-berofsystemstrainedonPennTreebankdatacanallbene(cid:2)tfromtheextendedresourcewehavecreated.AcknowledgementsWewouldliketothankMatthewHonnibal,oursec-ondannotator,whoalsohelpeddesigntheguide-lines;TobyHawker,forimplementingthedis-cretiser;MarkLauerforreleasinghisdata;andtheanonymousreviewersfortheirhelpfulfeed-back.ThisworkhasbeensupportedbytheAus-tralianResearchCouncilunderDiscoveryProjectsDP0453131andDP0665973.ReferencesAnnBies,MarkFerguson,KarenKatz,andRobertMacIntyre.1995.BracketingguidelinesforTreebankIIstylePennTree-bankproject.Technicalreport,UniversityofPennsylvania.DanBikel.2004.OntheParameterSpaceofGenerativeLexi-calizedStatisticalParsingModels.Ph.D.thesis,UniversityofPennsylvania.ThorstenBrantsandAlexFranz.2006.Web1T5-gramversion1.LinguisticDataConsortium.TedBriscoeandJohnCarroll.2006.EvaluatingtheaccuracyofanunlexicalizedstatisticalparseronthePARCDepBank.InProceedingsofthePosterSessionofCOLING/ACL-06.Sydney,Australia.MichaelCollins.1999.Head-DrivenStatisticalModelsforNat-uralLanguageParsing.Ph.D.thesis,UniversityofPennsyl-vania.UsamaM.FayyadandKekiB.Irani.1993.Multi-intervaldis-cretizationofcontinuous-valuedattributesforclassi(cid:2)cationlearning.InProceedingsofthe13thInternationalJointCon-ferenceonArti(cid:2)calIntelligence(IJCAI(cid:150)93),pages1022(cid:150)1029.Chambery,France.RoxanaGirju,DanMoldovan,MartaTatu,andDanielAntohe.2005.Onthesemanticsofnouncompounds.JournalofComputerSpeechandLanguage-SpecialIssueonMulti-wordExpressions,19(4):313(cid:150)330.JuliaHockenmaier.2003.DataandModelsforStatisticalPars-ingwithCombinatoryCategorialGrammar.Ph.D.thesis,UniversityofEdinburgh.TracyHollowayKing,RichardCrouch,StefanRiezler,MaryDalrymple,andRonaldM.Kaplan.2003.ThePARC700dependencybank.InProceedingsofthe4thInternationalWorkshoponLinguisticallyInterpretedCorpora(LINC-03).Budapest,Hungary.SethKulick,AnnBies,MarkLibeman,MarkMandel,RyanMcDonald,MarthaPalmer,AndrewSchein,andLyleUngar.2004.Integratedannotationforbiomedicalinformationex-traction.InProceedingsoftheHumanLanguageTechnologyConferenceoftheNorthAmericanChapteroftheAssocia-tionforComputationalLinguistics.Boston.MirellaLapataandFrankKeller.2004.Thewebasabase-line:Evaluatingtheperformanceofunsupervisedweb-basedmodelsforarangeofNLPtasks.InProceedingsoftheHu-manLanguageTechnologyConferenceoftheNorthAmeri-canChapteroftheAssociationforComputationalLinguis-tics,pages121(cid:150)128.Boston.MarkLauer.1995.Corpusstatisticsmeetthecompoundnoun:Someempiricalresults.InProceedingsofthe33rdAnnualMeetingoftheAssociationforComputationalLinguistics.Cambridge,MA.MitchellMarcus.1980.ATheoryofSyntacticRecognitionforNaturalLanguage.MITPress,Cambridge,MA.MitchellMarcus,BeatriceSantorini,andMaryMarcinkiewicz.1993.BuildingalargeannotatedcorpusofEnglish:ThePennTreebank.ComputationalLinguistics,19(2):313(cid:150)330.PreslavNakovandMartiHearst.2005.Searchenginestatisticsbeyondthen-gram:Applicationtonouncompoundbrack-eting.InProceedingsofCoNLL-2005,NinthConferenceonComputationalNaturalLanguageLearning.AnnArbor,MI.LanceA.RamshawandMitchellP.Marcus.1995.Textchunk-ingusingtransformation-basedlearning.InProceedingsoftheThirdACLWorkshoponVeryLargeCorpora.CambridgeMA,USA.MarkSteedman.2000.TheSyntacticProcess.MITPress,Cam-bridge,MA.RalphWeischedelandAdaBrunstein.2005.BBNpronouncoreferenceandentitytypecorpus.Technicalreport,Lin-guisticDataConsortium.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 248–255,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

248

Formalism-IndependentParserEvaluationwithCCGandDepBankStephenClarkOxfordUniversityComputingLaboratoryWolfsonBuilding,ParksRoadOxford,OX13QD,UKstephen.clark@comlab.ox.ac.ukJamesR.CurranSchoolofInformationTechnologiesUniversityofSydneyNSW2006,Australiajames@it.usyd.edu.auAbstractAkeyquestionfacingtheparsingcommu-nityishowtocompareparserswhichusedifferentgrammarformalismsandproducedifferentoutput.Evaluatingaparseronthesameresourceusedtocreateitcanleadtonon-comparableaccuracyscoresandanover-optimisticviewofparserperformance.InthispaperweevaluateaCCGparseronDepBank,anddemonstratethedifﬁcultiesinconvertingtheparseroutputintoDep-Bankgrammaticalrelations.Inadditionwepresentamethodformeasuringtheeffec-tivenessoftheconversion,whichprovidesanupperboundonparsingaccuracy.TheCCGparserobtainsanF-scoreof81.9%onlabelleddependencies,againstanupperboundof84.8%.WecomparetheCCGparseragainsttheRASPparser,outperform-ingRASPbyover5%overallandonthema-jorityofdependencytypes.1IntroductionParsershavebeendevelopedforavarietyofgram-marformalisms,forexampleHPSG(Toutanovaetal.,2002;MaloufandvanNoord,2004),LFG(Ka-planetal.,2004;Cahilletal.,2004),TAG(SarkarandJoshi,2003),CCG(HockenmaierandSteed-man,2002;ClarkandCurran,2004b),andvariantsofphrase-structuregrammar(Briscoeetal.,2006),includingthephrase-structuregrammarimplicitinthePennTreebank(Collins,2003;Charniak,2000).Differentparsersproducedifferentoutput,forex-amplephrasestructuretrees(Collins,2003),depen-dencytrees(NivreandScholz,2004),grammati-calrelations(Briscoeetal.,2006),andformalism-speciﬁcdependencies(ClarkandCurran,2004b).Thisvarietyofformalismsandoutputcreatesachal-lengeforparserevaluation.Themajorityofparserevaluationshaveusedtestsetsdrawnfromthesameresourceusedtodeveloptheparser.ThisallowsthemanyparsersbasedonthePennTreebank,forexample,tobemeaningfullycompared.However,therearetwodrawbackstothisapproach.First,parserevaluationsusingdifferentresourcescannotbecompared;forexample,thePar-sevalscoresobtainedbyPennTreebankparserscan-notbecomparedwiththedependencyF-scoresob-tainedbyevaluatingontheParcDependencyBank.Second,usingthesameresourcefordevelopmentandtestingcanleadtoanover-optimisticviewofparserperformance.InthispaperweevaluateaCCGparser(ClarkandCurran,2004b)ontheBriscoeandCarrollver-sionofDepBank(BriscoeandCarroll,2006).TheCCGparserproduceshead-dependencyrelations,soevaluatingtheparsershouldsimplybeamatterofconvertingtheCCGdependenciesintothoseinDep-Bank.Suchconversionshavebeenperformedforotherparsers,includingparsersproducingphrasestructureoutput(Kaplanetal.,2004;Preiss,2003).However,wefoundthatperformingsuchaconver-sionisatime-consumingandnon-trivialtask.Thecontributionsofthispaperareasfollows.First,wedemonstratetheconsiderabledifﬁcultiesassociatedwithformalism-independentparsereval-uation,highlightingtheproblemsinconvertingthe249

outputofaparserfromonerepresentationtoan-other.Second,wedevelopamethodformeasur-inghoweffectivetheconversionprocessis,whichalsoprovidesanupperboundfortheperformanceoftheparser,giventheconversionprocessbeingused;thismethodcanbeadaptedbyotherresearcherstostrengthentheirownparsercomparisons.Andthird,weprovidetheﬁrstevaluationofawide-coverageCCGparseroutsideofCCGbank,obtainingimpressiveresultsonDepBankandoutperformingtheRASPparser(Briscoeetal.,2006)byover5%overallandonthemajorityofdependencytypes.2PreviousWorkThemostcommonformofparserevaluationistoap-plytheParsevalmetricstophrase-structureparsersbasedonthePennTreebank,andthehighestre-portedscoresarenowover90%(Bod,2003;Char-niakandJohnson,2005).However,itisunclearwhetherthesehighscoresaccuratelyreﬂecttheper-formanceofparsersinapplications.Ithasbeenar-guedthattheParsevalmetricsaretooforgivingandthatphrasestructureisnottheidealrepresentationforagoldstandard(Carrolletal.,1998).Also,us-ingthesameresourcefortrainingandtestingmayresultintheparserlearningsystematicerrorswhicharepresentinboththetrainingandtestingmate-rial.AnexampleofthisisfromCCGbank(Hock-enmaier,2003),whereallmodiﬁersinnoun-nouncompoundconstructionsmodifytheﬁnalnoun(be-causethePennTreebank,fromwhichCCGbankisderived,doesnotcontainthenecessaryinformationtoobtainthecorrectbracketing).Thustherearenon-negligible,systematicerrorsinboththetrainingandtestingmaterial,andtheCCGparsersarebeingre-wardedforfollowingparticularmistakes.Thereareparserevaluationsuiteswhichhavebeendesignedtobeformalism-independentandwhichhavebeencarefullyandmanuallycorrected.Carrolletal.(1998)describesuchasuite,consistingofsentencestakenfromtheSusannecorpus,anno-tatedwithGrammaticalRelations(GRs)whichspec-ifythesyntacticrelationbetweenaheadanddepen-dent.Thusallthatisrequiredtousesuchascheme,intheory,isthattheparserbeingevaluatedisabletoidentifyheads.Asimilarresource—theParcDependencyBank(DepBank)(Kingetal.,2003)—hasbeencreatedusingsentencesfromthePennTreebank.BriscoeandCarroll(2006)reannotatedthisresourceusingtheirGRsscheme,andusedittoevaluatetheRASPparser.Kaplanetal.(2004)comparetheCollins(2003)parserwiththeParcLFGparserbymappingLFGF-structuresandPennTreebankparsesintoDepBankdependencies,claimingthattheLFGparseriscon-siderablymoreaccuratewithonlyaslightreduc-tioninspeed.Preiss(2003)comparestheparsersofCollins(2003)andCharniak(2000),theGRﬁnderofBuchholzetal.(1999),andtheRASPparser,us-ingtheCarrolletal.(1998)gold-standard.ThePennTreebanktreesoftheCollinsandCharniakparsers,andtheGRsoftheBuchholzparser,aremappedintotherequiredGRs,withtheresultthattheGRﬁnderofBuchholzisthemostaccurate.Themajorweaknessoftheseevaluationsisthatthereisnomeasureofthedifﬁcultlyoftheconver-sionprocessforeachoftheparsers.Kaplanetal.(2004)clearlyinvestedconsiderabletimeandex-pertiseinmappingtheoutputoftheCollinsparserintotheDepBankdependencies,buttheyalsonotethat“ThisconversionwasrelativelystraightforwardforLFGstructures...However,acertainamountofskillandintuitionwasrequiredtoprovideafaircon-versionoftheCollinstrees”.Withoutsomemeasureofthedifﬁculty—andeffectiveness—ofthecon-version,thereremainsasuspicionthattheCollinsparserisbeingunfairlypenalised.Onewayofprovidingsuchameasureistocon-verttheoriginalgoldstandardonwhichtheparserisbasedandevaluatethatagainstthenewgoldstan-dard(assumingthetworesourcesarebasedonthesamecorpus).InthecaseofKaplanetal.(2004),thetestingprocedurewouldincluderunningtheircon-versionprocessonSection23ofthePennTreebankandevaluatingtheoutputagainstDepBank.Aswellasprovidingsomemeasureoftheeffectivenessoftheconversion,thismethodwouldalsoprovideanupperboundfortheCollinsparser,givingthescorethataperfectPennTreebankparserwouldobtainonDepBank(giventheconversionprocess).WeperformsuchanevaluationfortheCCGparser,withthesurprisingresultthattheupperboundonDepBankisonly84.8%,despitetheconsiderableef-fortinvestedindevelopingtheconversionprocess.250

3TheCCGParserClarkandCurran(2004b)describestheCCGparserusedfortheevaluation.ThegrammarusedbytheparserisextractedfromCCGbank,aCCGversionofthePennTreebank(Hockenmaier,2003).Thegram-marconsistsof425lexicalcategories—expressingsubcategorisationinformation—plusasmallnum-berofcombinatoryruleswhichcombinethecate-gories(Steedman,2000).Asupertaggerﬁrstassignslexicalcategoriestothewordsinasentence,whicharethencombinedbytheparserusingthecombi-natoryrulesandtheCKYalgorithm.Alog-linearmodelscoresthealternativeparses.Weusethenormal-formmodel,whichassignsprobabilitiestosinglederivationsbasedonthenormal-formderiva-tionsinCCGbank.Thefeaturesinthemodelaredeﬁnedoverlocalpartsofthederivationandincludeword-worddependencies.Apackedchartrepresen-tationallowsefﬁcientdecoding,withtheViterbial-gorithmﬁndingthemostprobablederivation.Theparseroutputspredicate-argumentdependen-ciesdeﬁnedintermsofCCGlexicalcategories.Moreformally,aCCGpredicate-argumentdepen-dencyisa5-tuple:hhf,f,s,ha,li,wherehfisthelexicalitemofthelexicalcategoryexpressingthedependencyrelation;fisthelexicalcategory;sistheargumentslot;haistheheadwordofthear-gument;andlencodeswhetherthedependencyislong-range.Forexample,thedependencyencodingcompanyastheobjectofbought(asinIBMboughtthecompany)isrepresentedasfollows:hbought,(S\NP1)/NP2,2,company,−i(1)Thelexicalcategory(S\NP1)/NP2isthecate-goryofatransitiveverb,withtheﬁrstargumentslotcorrespondingtothesubject,andthesecondargu-mentslotcorrespondingtothedirectobject.Theﬁnalﬁeldindicatesthenatureofanylong-rangede-pendency;in(1)thedependencyislocal.Thepredicate-argumentdependencies—includ-inglong-rangedependencies—areencodedinthelexiconbyaddingheadanddependencyannota-tiontothelexicalcategories.Forexample,theexpandedcategoryforthecontrolverbpersuadeis(((S[dcl]persuade\NP1)/(S[to]2\NPX))/NPX,3).Nu-mericalsubscriptsontheargumentcategoriesrep-resentdependencyrelations;theheadoftheﬁnaldeclarativesentenceispersuade;andtheheadoftheinﬁnitivalcomplement’ssubjectisidentiﬁedwiththeheadoftheobject,usingthevariableX,asinstandarduniﬁcation-basedaccountsofcontrol.PreviousevaluationsofCCGparsershaveusedthepredicate-argumentdependenciesfromCCGbankasatestset(HockenmaierandSteedman,2002;ClarkandCurran,2004b),withimpressiveresultsofover84%F-scoreonlabelleddependencies.Inthispaperwereinforcetheearlierresultswiththeﬁrstevalua-tionofaCCGparseroutsideofCCGbank.4DependencyConversiontoDepBankForthegoldstandardwechosetheversionofDep-BankreannotatedbyBriscoeandCarroll(2006),consistingof700sentencesfromSection23ofthePennTreebank.TheB&CschemeissimilartotheoriginalDepBankscheme(Kingetal.,2003),butoverallcontainslessgrammaticaldetail;BriscoeandCarroll(2006)describesthedifferences.Wechosethisresourceforthefollowingreasons:itispub-liclyavailable,allowingotherresearcherstocom-pareagainstourresults;theGRsmakingupthean-notationsharesomesimilaritieswiththepredicate-argumentdependenciesoutputbytheCCGparser;andwecandirectlycompareourparseragainstanon-CCGparser,namelytheRASPparser.WechosenottousethecorpusbasedontheSusannecorpus(Carrolletal.,1998)becausetheGRsarelessliketheCCGdependencies;thecorpusisnotbasedonthePennTreebank,makingcomparisonmoredifﬁ-cultbecauseoftokenisationdifferences,forexam-ple;andthelatestresultsforRASPareonDepBank.TheGRsaredescribedinBriscoeandCarroll(2006)andBriscoeetal.(2006).Table1liststheGRsusedintheevaluation.Asanexample,thesen-tenceTheparentsoldImperialproducesthreeGRs:(detparentThe),(ncsubjsoldparent)and(dobjsoldImperial).NotethatsomeGRs—inthisexamplencsubj—haveasubtypeslot,givingextrainformation.Thesubtypeslotforncsubjisusedtoindicatepassivesubjects,withthenullvalue“”foractivesubjectsandobjforpassivesubjects.OthersubtypeslotsarediscussedinSection4.2.TheCCGdependenciesweretransformedintoGRsintwostages.TheﬁrststagewastocreateamappingbetweentheCCGdependenciesandthe251

GRdescriptionconjcoordinatorauxauxiliarydetdeterminerncmodnon-clausalmodiﬁerxmodunsaturatedpredicativemodiﬁercmodsaturatedclausalmodiﬁerpmodPPmodiﬁerwithaPPcomplementncsubjnon-clausalsubjectxsubjunsaturatedpredicativesubjectcsubjsaturatedclausalsubjectdobjdirectobjectobj2secondobjectiobjindirectobjectpcompPPwhichisaPPcomplementxcompunsaturatedVPcomplementccompsaturatedclausalcomplementtatextualadjunctdelimitedbypunctuationTable1:GRsinB&C’sannotationofDepBankGRs.Thisinvolvedmappingeachargumentslotinthe425lexicalcategoriesintheCCGlexiconontoaGR.Inthesecondstage,theGRscreatedfromtheparseroutputwerepost-processedtocorrectsomeoftheobviousremainingdifferencesbetweentheCCGandGRrepresentations.Intheprocessofperformingthetransformationweencounteredamethodologicalproblem:with-outlookingatexamplesitwasdifﬁculttocreatethemappingandimpossibletoknowwhetherthetworepresentationswereconverging.Briscoeetal.(2006)splitthe700sentencesinDepBankintoatestanddevelopmentset,butthelatteronlyconsistsof140sentenceswhichwasnotenoughtoreliablycre-atethetransformation.TherearesomedevelopmentﬁlesintheRASPreleasewhichprovideexamplesoftheGRs,whichwereusedwhenpossible,buttheseonlycoverasubsetoftheCCGlexicalcategories.OursolutiontothisproblemwastoconvertthegoldstandarddependenciesfromCCGbankintoGRsandusethesetodevelopthetransformation.SowedidinspecttheannotationinDepBank,andcom-paredittothetransformedCCGdependencies,butonlythegold-standardCCGdependencies.Thustheparseroutputwasneverusedduringthisprocess.WealsoensuredthatthedependencymappingandthepostprocessingaregeneraltotheGRsschemeandnotspeciﬁctothetestsetorparser.4.1MappingtheCCGdependenciestoGRsTable2givessomeexamplesofthemapping;%lin-dicatesthewordassociatedwiththelexicalcategoryCCGlexicalcategoryslotGR(S[dcl]\NP1)/NP21(ncsubj%l%f)(S[dcl]\NP1)/NP22(dobj%l%f)(S\NP)/(S\NP)11(ncmod%f%l)(NP\NP1)/NP21(ncmod%f%l)(NP\NP1)/NP22(dobj%l%f)NP[nb]/N11(det%f%l)(NP\NP1)/(S[pss]\NP)21(xmod%f%l)(NP\NP1)/(S[pss]\NP)22(xcomp%l%f)((S\NP)\(S\NP)1)/S[dcl]21(cmod%f%l)((S\NP)\(S\NP)1)/S[dcl]22(ccomp%l%f)((S[dcl]\NP1)/NP2)/NP32(obj2%l%f)(S[dcl]\NP1)/(S[b]\NP)22(aux%f%l)Table2:Examplesofthedependencymappingand%fistheheadoftheconstituentﬁllingtheargu-mentslot.Notethattheorderof%land%fvariesac-cordingtowhethertheGRrepresentsacomplementormodiﬁer,inlinewiththeBriscoeandCarrollan-notation.FormanyoftheCCGdependencies,themappingintoGRsisstraightforward.Forexample,theﬁrsttworowsofTable2showthemappingforthetransitiveverbcategory(S[dcl]\NP1)/NP2:ar-gumentslot1isanon-clausalsubjectandargumentslot2isadirectobject.Creatingthedependencytransformationismoredifﬁcultthantheseexamplessuggest.Theﬁrstprob-lemisthatthemappingfromCCGdependenciestoGRsismany-to-many.Forexample,thetransitiveverbcategory(S[dcl]\NP)/NPappliestothecop-ulainsentenceslikeImperialCorp.istheparentofImperialSavings&Loan.Withthedefaultanno-tation,therelationbetweenisandparentwouldbedobj,whereasinDepBanktheargumentofthecop-ulaisanalysedasanxcomp.Table3givessomeex-amplesofhowweattempttodealwiththisproblem.Theconstraintintheﬁrstexamplemeansthat,when-everthewordassociatedwiththetransitiveverbcat-egoryisaformofbe,thesecondargumentisxcomp,otherwisethedefaultcaseapplies(inthiscasedobj).Thereareanumberofcategorieswithsimilarcon-straints,checkingwhetherthewordassociatedwiththecategoryisaformofbe.Thesecondtypeofconstraint,showninthethirdlineofthetable,checksthelexicalcategoryofthewordﬁllingtheargumentslot.Inthisexample,ifthelexicalcategoryoftheprepositionisPP/NP,thenthesecondargumentof(S[dcl]\NP)/PPmapstoiobj;thusinThelossstemsfromseveralfac-torstherelationbetweentheverbandprepositionis(iobjstemsfrom).Ifthelexicalcategoryof252

CCGlexicalcategoryslotGRconstraintexample(S[dcl]\NP1)/NP22(xcomp%l%f)word=beTheparentisImperial(dobj%l%f)TheparentsoldImperial(S[dcl]\NP1)/PP22(iobj%l%f)cat=PP/NPThelossstemsfromseveralfactors(xcomp%l%f)cat=PP/(S[ng]\NP)Thefuturedependsonbuildingties(S[dcl]\NP1)/(S[to]\NP)22(xcomp%f%l%k)cat=(S[to]\NP)/(S[b]\NP)wantstoweanitselfawayfromTable3:Examplesofthemany-to-manynatureoftheCCGdependencytoGRsmapping,andaternaryGRtheprepositionisPP/(S[ng]\NP),thentheGRisxcomp;thusinThefuturedependsonbuildingtiestherelationbetweentheverbandprepositionis(xcompdependson).ThereareanumberofCCGdependencieswithsimilarconstraints,manyofthemcoveringtheiobj/xcompdistinction.TheseconddifﬁcultyisthatnotalltheGRsarebi-naryrelations,whereastheCCGdependenciesareallbinary.Theprimaryexampleofthisisto-inﬁnitivalconstructions.Forexample,inthesentenceThecompanywantstoweanitselfawayfromexpensivegimmicks,theCCGparserproducestwodependen-ciesrelatingwants,toandwean,whereasthereisonlyoneGR:(xcomptowantswean).Theﬁ-nalrowofTable3givesanexample.Weim-plementthisconstraintbyintroducinga%kvari-ableintotheGRtemplatewhichdenotesthear-gumentofthecategoryintheconstraintcolumn(which,asbefore,isthelexicalcategoryofthewordﬁllingtheargumentslot).Intheexample,thecurrentcategoryis(S[dcl]\NP1)/(S[to]\NP)2,whichisassociatedwithwants;thiscombineswith(S[to]\NP)/(S[b]\NP),associatedwithto;andtheargumentof(S[to]\NP)/(S[b]\NP)iswean.The%kvariableallowsustolookbeyondtheargu-mentsofthecurrentcategorywhencreatingtheGRs.Afurtherdifﬁcultyisthattheheadpassingcon-ventionsdifferbetweenDepBankandCCGbank.Byheadpassingwemeanthemechanismwhichde-terminestheheadsofconstituentsandthemecha-nismbywhichwordsbecomeargumentsoflong-rangedependencies.Forexample,inthesentenceThegroupsaiditwouldconsiderwithholdingroy-altypayments,theDepBankandCCGbankannota-tionscreateadependencybetweensaidandthefol-lowingclause.However,inDepBanktherelationisbetweensaidandconsider,whereasinCCGbanktherelationisbetweensaidandwould.Weﬁxedthisproblembydeﬁningtheheadofwouldconsidertobeconsiderratherthanwould,bychangingthean-notationofalltherelevantlexicalcategoriesintheCCGlexicon(mainlythosecreatingauxrelations).TherearemoresubjectrelationsinCCGbankthanDepBank.Inthepreviousexample,CCGbankhasasubjectrelationbetweenitandconsider,andalsoitandwould,whereasDepBankonlyhastherelationbetweenitandconsider.Inpracticethismeansig-noringanumberofthesubjectdependenciesoutputbytheCCGparser.Anotherexamplewherethedependenciesdifferisthetreatmentofrelativepronouns.Forexample,inSen.Mitchell,whohadproposedthestreamlin-ing,thesubjectofproposedisMitchellinCCGbankbutwhoinDepBank.Again,weimplementedthischangebyﬁxingtheheadannotationinthelexicalcategorieswhichapplytorelativepronouns.4.2PostprocessingoftheGRoutputToobtainsomeideaofwhethertheschemeswereconverging,weperformedthefollowingoracleex-periment.WetooktheCCGderivationsfromCCGbankcorrespondingtothesentencesinDep-Bank,andforcedtheparsertoproducegold-standardderivations,outputtingthenewlycreatedGRs.TreatingtheDepBankGRsasagold-standard,andcomparingthesewiththeCCGbankGRs,gaveprecisionandrecallscoresofonly76.23%and79.56%respectively(usingtheRASPevaluationtool).Thusgiventhecurrentmapping,theperfectCCGbankparserwouldachieveanF-scoreofonly77.86%whenevaluatedagainstDepBank.Oninspectingtheoutput,itwasclearthatanumberofgeneralrulescouldbeappliedtobringtheschemesclosertogether,whichwasimple-mentedasapost-processingscript.Theﬁrstsetofchangesdealswithcoordination.Onesig-niﬁcantdifferencebetweenDepBankandCCG-bankisthetreatmentofcoordinationsasargu-ments.ConsidertheexampleThepresidentandchiefexecutiveofﬁcersaidthelossstemsfromsev-eralfactors.ForbothDepBankandthetrans-formedCCGbanktherearetwoconjGRsarising253

fromthecoordination:(conjandpresident)and(conjandofficer).Thedifferencearisesinthesubjectofsaid:inDepBankthesubjectisand:(ncsubjsaidand),whereasinCCGbanktherearetwosubjects:(ncsubjsaidpresident)and(ncsubjsaidofficer).Wedealwiththisdif-ferencebyreplacinganypairsofGRswhichdifferonlyintheirarguments,andwheretheargumentsarecoordinateditems,withasingleGRcontainingthecoordinationtermastheargument.AmpersandsareafrequentlyoccurringprobleminWSJtext.Forexample,theCCGbankanalysisofStandard&Poor’sindexassignsthelexicalcat-egoryN/NtobothStandardand&,treatingthemasmodiﬁersofPoor,whereasDepBanktreats&asacoordinatingterm.WeﬁxedthisbycreatingconjGRsbetweenany&andthetwowordseitherside;removingthemodiﬁerGRbetweenthetwowords;andreplacinganyGRsinwhichthewordseithersideofthe&areargumentswithasingleGRinwhich&istheargument.Thetarelation,whichidentiﬁestextadjunctsde-limitedbypunctuation,isdifﬁculttoassigncor-rectlytotheparseroutput.Thesimplepunctuationrulesusedbytheparserdonotcontainenoughin-formationtodistinguishbetweenthevariouscasesofta.Thustheonlyrulewehaveimplemented,whichissomewhatspeciﬁctothenewspapergenre,istoreplaceGRsoftheform(cmodsayarg)with(taquoteargsay),wheresaycanbeanyofsay,saidorsays.Thisruleappliestoonlyasmallsubsetofthetacasesbuthashighenoughprecisiontobeworthyofinclusion.Acommonsourceoferroristhedistinctionbe-tweeniobjandncmod,whichisnotsurprisinggiventhedifﬁcultythathumanannotatorshaveindistin-guishingargumentsandadjuncts.TherearemanycaseswhereanargumentinDepBankisanadjunctinCCGbank,andviceversa.TheonlychangewehavemadeistoturnallncmodGRswithofasthemodiﬁerintoiobjGRs(unlessthencmodisapar-titivepredeterminer).Thiswasfoundtohavehighprecisionandappliestoalargenumberofcases.TherearesomedependenciesinCCGbankwhichdonotappearinDepBank.Examplesincludeanydependenciesinwhichapunctuationmarkisoneofthearguments;thesewereremovedfromtheoutput.WeattempttoﬁllthesubtypeslotforsomeGRs.ThesubtypeslotspeciﬁesadditionalinformationabouttheGR;examplesincludethevalueobjinapassivencsubj,indicatingthatthesubjectisanun-derlyingobject;thevaluenuminncmod,indicatinganumericalquantity;andprtinncmodtoindicateaverbparticle.Thepassivecaseisidentiﬁedasfol-lows:anylexicalcategorywhichstartsS[pss]\NPindicatesapassiveverb,andwealsomarkanyverbsPOStaggedVBNandassignedthelexicalcategoryN/Naspassive.Boththeseruleshavehighpreci-sion,butstillleavemanyofthecasesinDepBankunidentiﬁed.Thenumericalcaseisidentiﬁedusingtworules:thenumsubtypeisaddedifanyargumentinaGRisassignedthelexicalcategoryN/N[num],andifanyoftheargumentsinanncmodisPOStaggedCD.prtisaddedtoanncmodifthemodi-ﬁeehasanyoftheverbPOStagsandifthemodiﬁerhasPOStagRP.TheﬁnalcolumnsofTable4showtheaccuracyofthetransformedgold-standardCCGbankdepen-dencieswhencomparedwithDepBank;thesim-plepost-processingruleshaveincreasedtheF-scorefrom77.86%to84.76%.ThisF-scoreisanupperboundontheperformanceoftheCCGparser.5ResultsTheresultsinTable4wereobtainedbyparsingthesentencesfromCCGbankcorrespondingtothoseinthe560-sentencetestsetusedbyBriscoeetal.(2006).WeusedtheCCGbanksentencesbecausethesedifferinsomewaystotheoriginalPennTree-banksentences(therearenoquotationmarksinCCGbank,forexample)andtheparserhasbeentrainedonCCGbank.Evenhereweexperiencedsomeunexpecteddifﬁculties,sincesomeoftheto-kenisationisdifferentbetweenDepBankandCCG-bankandtherearesomesentencesinDepBankwhichhavebeensigniﬁcantlyshortenedcomparedtotheoriginalPennTreebanksentences.Wemod-iﬁedtheCCGbanksentences—andtheCCGbankanalysessincethesewereusedfortheoracleex-periments—tobeasclosetotheDepBanksen-tencesaspossible.Alltheresultswereobtainedus-ingtheRASPevaluationscripts,withtheresultsfortheRASPparsertakenfromBriscoeetal.(2006).TheresultsforCCGbankwereobtainedusingtheoraclemethoddescribedabove.254

RASPCCGparserCCGbankRelationPrecRecFPrecRecFPrecRecF#GRsaux93.3391.0092.1594.2089.2591.6696.4790.3393.30400conj72.3972.2772.3379.7377.9878.8483.0780.2781.65595ta42.6151.3746.5852.3111.6419.0562.0712.5920.93292det87.7390.4889.0995.2595.4295.3497.2794.0995.661114ncmod75.7269.9472.7275.7579.2777.4778.8880.6479.753550xmod53.2146.6349.7043.4652.2547.4556.5460.6758.54178cmod45.9530.3636.5651.5061.3155.9864.7769.0966.86168pmod30.7733.3332.000.000.000.000.000.000.0012ncsubj79.1667.0672.6183.9275.9279.7288.8678.5183.371354xsubj33.3328.5730.770.000.000.0050.0028.5736.367csubj12.5050.0020.000.000.000.000.000.000.002dobj83.6379.0881.2987.0389.4088.2092.1190.3291.211764obj223.0830.0026.0965.0065.0065.0066.6760.0063.1620iobj70.7776.1073.3477.6070.0473.6283.5969.8176.08544xcomp76.8877.6977.2876.6877.6977.1880.0078.4979.24381ccomp46.4469.4255.5579.5572.1675.6880.8176.3178.49291pcomp72.7366.6769.570.000.000.000.000.000.0024macroaverage62.1263.7762.9465.6163.2864.4371.7365.8568.67microaverage77.6674.9876.2982.4481.2881.8686.8682.7584.76Table4:AccuracyonDepBank.F-scoreisthebalancedharmonicmeanofprecision(P)andrecall(R):2PR/(P+R).#GRsisthenumberofGRsinDepBank.TheCCGparserresultsarebasedonautomati-callyassignedPOStags,usingtheCurranandClark(2003)tagger.ThecoverageoftheparseronDep-Bankis100%.ForaGRintheparseroutputtobecorrect,ithastomatchthegold-standardGRexactly,includinganysubtypeslots;however,itispossibleforaGRtobeincorrectatonelevelbutcorrectatasubsuminglevel.1Forexample,ifanncmodGRisincorrectlylabelledwithxmod,butisotherwisecor-rect,itwillbecorrectforalllevelswhichsubsumebothncmodandxmod,forexamplemod.Themicro-averagedscoresarecalculatedbyaggregatingthecountsforalltherelationsinthehierarchy,includingthesubsumingrelations;themacro-averagedscoresarethemeanoftheindividualscoresforeachrela-tion(Briscoeetal.,2006).TheresultsshowthattheperformanceoftheCCGparserishigherthanRASPoverall,andalsohigheronthemajorityofGRtypes(especiallythemorefrequenttypes).RASPusesanunlexicalisedpars-ingmodelandhasnotbeentunedtonewspapertext.Ontheotherhandithashadmanyyearsofdevelop-ment;thusitprovidesastrongbaselineforthistestset.TheoverallF-scorefortheCCGparser,81.86%,isonly3pointsbelowthatforCCGbank,whichpro-1TheGRsarearrangedinahierarchy,withthoseinTable1attheleaves;asmallnumberofmoregeneralGRssubsumethese(BriscoeandCarroll,2006).videsanupperboundfortheCCGparser(giventheconversionprocessbeingused).6ConclusionAcontributionofthispaperhasbeentohigh-lightthedifﬁcultiesassociatedwithcross-formalismparsercomparison.NotethatthedifﬁcultiesarenotuniquetoCCG,andmanywouldapplytoanycross-formalismcomparison,especiallywithparsersusingautomaticallyextractedgrammars.Parserevalua-tionhasimprovedontheoriginalParsevalmeasures(Carrolletal.,1998),butthechallengeremainstodeveloparepresentationandevaluationsuitewhichcanbeeasilyappliedtoawidevarietyofparsersandformalisms.Despitethedifﬁculties,wehavegiventheﬁrstevaluationofaCCGparseroutsideofCCGbank,outperformingtheRASPparserbyover5%overallandonthemajorityofdependencytypes.CantheCCGparserbecomparedwithparsersotherthanRASP?BriscoeandCarroll(2006)givearoughcomparisonofRASPwiththeParcLFGparseronthedifferentversionsofDepBank,obtainingsim-ilarresultsoverall,buttheyacknowledgethatthere-sultsarenotstrictlycomparablebecauseofthedif-ferentannotationschemesused.ComparisonwithPennTreebankparserswouldbedifﬁcultbecause,formanyconstructions,thePennTreebanktreesand255

CCGderivationsaredifferentshapes,andreversingthemappingHockenmaierusedtocreateCCGbankwouldbeverydifﬁcult.HencewechallengeotherparserdeveloperstomaptheirownparseoutputintotheversionofDepBankusedhere.Oneaspectofparserevaluationnotcoveredinthispaperisefﬁciency.TheCCGparsertookonly22.6secondstoparsethe560sentencesinDepBank,withtheaccuracygivenearlier.Usingaclusterof18ma-chineswehavealsoparsedtheentireGigawordcor-pusinlessthanﬁvedays.Hence,weconcludethataccurate,large-scale,linguistically-motivatedNLPisnowpracticalwithCCG.AcknowledgementsWewouldliketothankstheanonymousreview-ersfortheirhelpfulcomments.JamesCurranwasfundedunderARCDiscoverygrantsDP0453131andDP0665973.ReferencesRensBod.2003.AnefﬁcientimplementationofanewDOPmodel.InProceedingsofthe10thMeetingoftheEACL,pages19–26,Budapest,Hungary.TedBriscoeandJohnCarroll.2006.EvaluatingtheaccuracyofanunlexicalizedstatisticalparseronthePARCDepBank.InProceedingsofthePosterSessionofCOLING/ACL-06,Sydney,Australia.TedBriscoe,JohnCarroll,andRebeccaWatson.2006.ThesecondreleaseoftheRASPsystem.InProceedingsoftheInteractiveDemoSessionofCOLING/ACL-06,Sydney,Australia.SabineBuchholz,JornVeenstra,andWalterDaelemans.1999.Cascadedgrammaticalrelationassignment.InProceedingsofEMNLP/VLC-99,pages239–246,UniversityofMary-land,June21-22.A.Cahill,M.Burke,R.O’Donovan,J.vanGenabith,andA.Way.2004.Long-distancedependencyresolutioninau-tomaticallyacquiredwide-coveragePCFG-basedLFGap-proximations.InProceedingsofthe42ndMeetingoftheACL,pages320–327,Barcelona,Spain.JohnCarroll,TedBriscoe,andAntonioSanﬁlippo.1998.Parserevaluation:asurveyandanewproposal.InProceed-ingsofthe1stLRECConference,pages447–454,Granada,Spain.EugeneCharniakandMarkJohnson.2005.Coarse-to-ﬁnen-bestparsingandmaxentdiscriminativereranking.InPro-ceedingsofthe43rdAnnualMeetingoftheACL,UniversityofMichigan,AnnArbor.EugeneCharniak.2000.Amaximum-entropy-inspiredparser.InProceedingsofthe1stMeetingoftheNAACL,pages132–139,Seattle,WA.StephenClarkandJamesR.Curran.2004a.Theimportanceofsupertaggingforwide-coverageCCGparsing.InProceed-ingsofCOLING-04,pages282–288,Geneva,Switzerland.StephenClarkandJamesR.Curran.2004b.ParsingtheWSJusingCCGandlog-linearmodels.InProceedingsofthe42ndMeetingoftheACL,pages104–111,Barcelona,Spain.MichaelCollins.2003.Head-drivenstatisticalmodelsfornaturallanguageparsing.ComputationalLinguistics,29(4):589–637.JamesR.CurranandStephenClark.2003.InvestigatingGISandsmoothingformaximumentropytaggers.InProceed-ingsofthe10thMeetingoftheEACL,pages91–98,Bu-dapest,Hungary.JuliaHockenmaierandMarkSteedman.2002.GenerativemodelsforstatisticalparsingwithCombinatoryCategorialGrammar.InProceedingsofthe40thMeetingoftheACL,pages335–342,Philadelphia,PA.JuliaHockenmaier.2003.DataandModelsforStatisticalParsingwithCombinatoryCategorialGrammar.Ph.D.the-sis,UniversityofEdinburgh.RonKaplan,StefanRiezler,TracyH.King,JohnT.MaxwellIII,AlexanderVasserman,andRichardCrouch.2004.Speedandaccuracyinshallowanddeepstochasticparsing.InProceedingsoftheHLTConferenceandthe4thNAACLMeeting(HLT-NAACL’04),Boston,MA.TracyH.King,RichardCrouch,StefanRiezler,MaryDalrym-ple,andRonaldM.Kaplan.2003.ThePARC700Depen-dencyBank.InProceedingsoftheLINC-03Workshop,Bu-dapest,Hungary.RobertMaloufandGertjanvanNoord.2004.Widecoverageparsingwithstochasticattributevaluegrammars.InPro-ceedingsoftheIJCNLP-04Workshop:Beyondshallowanal-yses-Formalismsandstatisticalmodelingfordeepanalyses,HainanIsland,China.JoakimNivreandMarioScholz.2004.Deterministicdepen-dencyparsingofEnglishtext.InProceedingsofCOLING-2004,pages64–70,Geneva,Switzerland.JuditaPreiss.2003.Usinggrammaticalrelationstocompareparsers.InProceedingsofthe10thMeetingoftheEACL,pages291–298,Budapest,Hungary.AnoopSarkarandAravindJoshi.2003.Tree-adjoininggram-marsanditsapplicationtostatisticalparsing.InRensBod,RemkoScha,andKhalilSima’an,editors,Data-orientedparsing.CSLI.MarkSteedman.2000.TheSyntacticProcess.TheMITPress,Cambridge,MA.KristinaToutanova,ChristopherManning,StuartShieber,DanFlickinger,andStephanOepen.2002.ParsedisambiguationforarichHPSGgrammar.InProceedingsoftheFirstWork-shoponTreebanksandLinguisticTheories,pages253–263,Sozopol,Bulgaria.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 256–263,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

256

FrustratinglyEasyDomainAdaptationHalDaum´eIIISchoolofComputingUniversityofUtahSaltLakeCity,Utah84112me@hal3.nameAbstractWedescribeanapproachtodomainadapta-tionthatisappropriateexactlyinthecasewhenonehasenough“target”datatodoslightlybetterthanjustusingonly“source”data.Ourapproachisincrediblysimple,easytoimplementasapreprocessingstep(10linesofPerl!)andoutperformsstate-of-the-artapproachesonarangeofdatasets.Moreover,itistriviallyextendedtoamulti-domainadaptationproblem,whereonehasdatafromavarietyofdifferentdomains.1IntroductionThetaskofdomainadaptationistodeveloplearn-ingalgorithmsthatcanbeeasilyportedfromonedomaintoanother—say,fromnewswiretobiomed-icaldocuments.Thisproblemisparticularlyinter-estinginNLPbecauseweareofteninthesituationthatwehavealargecollectionoflabeleddatainone“source”domain(say,newswire)buttrulydesireamodelthatperformswellinasecond“target”do-main.Theapproachwepresentinthispaperisbasedontheideaoftransformingthedomainadaptationlearningproblemintoastandardsupervisedlearn-ingproblemtowhichanystandardalgorithmmaybeapplied(eg.,maxent,SVMs,etc.).Ourtransfor-mationisincrediblysimple:weaugmentthefeaturespaceofboththesourceandtargetdataandusetheresultasinputtoastandardlearningalgorithm.Thereareroughlytwovarietiesofthedomainadaptationproblemthathavebeenaddressedintheliterature:thefullysupervisedcaseandthesemi-supervisedcase.Thefullysupervisedcasemod-elsthefollowingscenario.Wehaveaccesstoalarge,annotatedcorpusofdatafromasourcedo-main.Inaddition,wespendalittlemoneytoanno-tateasmallcorpusinthetargetdomain.Wewanttoleveragebothannotateddatasetstoobtainamodelthatperformswellonthetargetdomain.Thesemi-supervisedcaseissimilar,butinsteadofhavingasmallannotatedtargetcorpus,wehavealargebutunannotatedtargetcorpus.Inthispaper,wefocusexclusivelyonthefullysupervisedcase.Oneparticularlynicepropertyofourapproachisthatitisincrediblyeasytoimplement:theAp-pendixprovidesa10line,194characterPerlscriptforperformingthecompletetransformation(avail-ableathttp://hal3.name/easyadapt.pl.gz).Inadditiontothissimplicity,ouralgorithmperformsaswellas(or,insomecases,betterthan)currentstateofthearttechniques.2ProblemFormalizationandPriorWorkTofacilitatediscussion,weﬁrstintroducesomeno-tation.DenotebyXtheinputspace(typicallyeitherarealvectororabinaryvector),andbyYtheoutputspace.WewillwriteDstodenotethedistributionoversourceexamplesandDttodenotethedistri-butionovertargetexamples.WeassumeaccesstoasamplesDs∼Dsofsourceexamplesfromthesourcedomain,andsamplesDt∼Dtoftargetex-amplesfromthetargetdomain.WewillassumethatDsisacollectionofNexamplesandDtisacol-lectionofMexamples(where,typically,N≫M).Ourgoalistolearnafunctionh:X→Ywithlowexpectedlosswithrespecttothetargetdomain.257

Forthepurposesofdiscussion,wewillsupposethatX=RFandthatY={−1,+1}.However,mostofthetechniquesdescribedinthissection(aswellasourowntechnique)aremoregeneral.Thereareseveral“obvious”waystoattackthedomainadaptationproblemwithoutdevelopingnewalgorithms.Manyofthesearepresentedandevalu-atedbyDaum´eIIIandMarcu(2006).TheSRCONLYbaselineignoresthetargetdataandtrainsasinglemodel,onlyonthesourcedata.TheTGTONLYbaselinetrainsasinglemodelonlyonthetargetdata.TheALLbaselinesimplytrainsastandardlearningalgorithmontheunionofthetwodatasets.ApotentialproblemwiththeALLbaselineisthatifN≫M,thenDsmay“washout”anyaffectDtmighthave.Wewilldiscussthisprobleminmoredetaillater,butonepotentialsolutionistore-weightexamplesfromDs.Forinstance,ifN=10×M,wemayweighteachexamplefromthesourcedomainby0.1.Thenextbase-line,WEIGHTED,isexactlythisapproach,withtheweightchosenbycross-validation.ThePREDbaselineisbasedontheideaofusingtheoutputofthesourceclassiﬁerasafeatureinthetargetclassiﬁer.Speciﬁcally,weﬁrsttrainaSRCONLYmodel.ThenweruntheSRCONLYmodelonthetargetdata(training,developmentandtest).WeusethepredictionsmadebytheSRCONLYmodelasadditionalfeaturesandtrainasecondmodelonthetargetdata,aug-mentedwiththisnewfeature.IntheLININTbaseline,welinearlyinterpolatethepredictionsoftheSRCONLYandtheTG-TONLYmodels.Theinterpolationparameterisadjustedbasedontargetdevelopmentdata.Thesebaselinesareactuallysurprisinglydifﬁculttobeat.Todate,therearetwomodelsthathavesuccessfullydefeatedthemonahandfulofdatasets.Theﬁrstmodel,whichweshallrefertoasthePRIORmodel,wasﬁrstintroducedbyChelbaandAcero(2004).TheideaofthismodelistousetheSR-CONLYmodelasapriorontheweightsforasec-ondmodel,trainedonthetargetdata.ChelbaandAcero(2004)describethisapproachwithinthecon-textofamaximumentropyclassiﬁer,buttheideaismoregeneral.Inparticular,formanylearningalgorithms(maxent,SVMs,averagedperceptron,naiveBayes,etc.),oneregularizestheweightvec-tortowardzero.Inotherwords,allofthesealgo-rithmscontainaregularizationtermontheweightswoftheformλ||w||22.InthegeneralizedPRIORmodel,wesimplyreplacethisregularizationtermwithλ||w−ws||22,wherewsistheweightvectorlearnedintheSRCONLYmodel.1Inthisway,themodeltrainedonthetargetdata“prefers”tohaveweightsthataresimilartotheweightsfromtheSR-CONLYmodel,unlessthedatademandsotherwise.Daum´eIIIandMarcu(2006)provideempiricalevi-denceonfourdatasetsthatthePRIORmodeloutper-formsthebaselineapproaches.Morerecently,Daum´eIIIandMarcu(2006)pre-sentedanalgorithmfordomainadaptationformax-imumentropyclassiﬁers.Thekeyideaoftheirap-proachistolearnthreeseparatemodels.Onemodelcaptures“sourcespeciﬁc”information,onecaptures“targetspeciﬁc”informationandonecaptures“gen-eral”information.Thedistinctionbetweenthesethreesortsofinformationismadeonaper-examplebasis.Inthisway,eachsourceexampleisconsid-eredeithersourcespeciﬁcorgeneral,whileeachtargetexampleisconsideredeithertargetspeciﬁcorgeneral.Daum´eIIIandMarcu(2006)presentanEMalgorithmfortrainingtheirmodel.Thismodelcon-sistentlyoutperformedallthebaselineapproachesaswellasthePRIORmodel.Unfortunately,despitetheempiricalsuccessofthisalgorithm,itisquitecomplextoimplementandisroughly10to15timesslowerthantrainingthePRIORmodel.3AdaptationbyFeatureAugmentationInthissection,wedescribeourapproachtothedo-mainadaptationproblem.Essentially,allwearego-ingtodoistakeeachfeatureintheoriginalproblemandmakethreeversionsofit:ageneralversion,asource-speciﬁcversionandatarget-speciﬁcversion.Theaugmentedsourcedatawillcontainonlygeneralandsource-speciﬁcversions.Theaugmentedtarget1Forthemaximumentropy,SVMandnaiveBayeslearn-ingalgorithms,modifyingtheregularizationtermissimplebe-causeitappearsexplicitly.Fortheperceptronalgorithm,onecanobtainanequivalentregularizationbyperformingstandardperceptronupdates,butusing(w+ws)⊤xformakingpredic-tionsratherthansimplyw⊤x.258

datacontainsgeneralandtarget-speciﬁcversions.Tostatethismoreformally,ﬁrstrecallthenota-tionfromSection2:XandYaretheinputandoutputspaces,respectively;Dsisthesourcedo-maindatasetandDtisthetargetdomaindataset.SupposeforsimplicitythatX=RFforsomeF>0.Wewilldeﬁneouraugmentedinputspaceby˘X=R3F.Then,deﬁnemappingsΦs,Φt:X→˘Xformappingthesourceandtargetdatarespectively.ThesearedeﬁnedbyEq(1),where0=h0,0,...,0i∈RFisthezerovector.Φs(x)=hx,x,0i,Φt(x)=hx,0,xi(1)Beforeweproceedwithaformalanalysisofthistransformation,letusconsiderwhyitmightbeex-pectedtowork.Supposeourtaskispartofspeechtagging,oursourcedomainistheWallStreetJournalandourtargetdomainisacollectionofreviewsofcomputerhardware.Here,awordlike“the”shouldbetaggedasadeterminerinbothcases.However,awordlike“monitor”ismorelikelytobeaverbintheWSJandmorelikelytobeanouninthehard-warecorpus.ConsiderasimplecasewhereX=R2,wherex1indicatesifthewordis“the”andx2indi-catesifthewordis“monitor.”Then,in˘X,˘x1and˘x2willbe“general”versionsofthetwoindicatorfunc-tions,˘x3and˘x4willbesource-speciﬁcversions,and˘x5and˘x6willbetarget-speciﬁcversions.Now,considerwhatalearningalgorithmcoulddotocapturethefactthattheappropriatetagfor“the”remainsconstantacrossthedomains,andthetagfor“monitor”changes.Inthiscase,themodelcansetthe“determiner”weightvectortosomethinglikeh1,0,0,0,0,0i.Thisplaceshighweightonthecom-monversionof“the”andindicatesthat“the”ismostlikelyadeterminer,regardlessofthedomain.Ontheotherhand,theweightvectorfor“noun”mightlooksomethinglikeh0,0,0,0,0,1i,indicatingthattheword“monitor”isanounonlyinthetargetdo-main.Similar,theweightvectorfor“verb”mightlooklikeh0,0,0,1,0,0i,indicatingthe“monitor”isaverbonlyinthesourcedomain.Notethatthisexpansionisactuallyredundant.WecouldequallywelluseΦs(x)=hx,xiandΦt(x)=hx,0i.However,itturnsoutthatitiseas-iertoanalyzetheﬁrstcase,sowewillstickwiththat.Moreover,theﬁrstcasehasthenicepropertythatitisstraightforwardtogeneralizeittothemulti-domainadaptationproblem:whentherearemorethantwodomains.Ingeneral,forKdomains,theaugmentedfeaturespacewillconsistofK+1copiesoftheoriginalfeaturespace.3.1AKernelizedVersionItisstraightforwardtoderiveakernelizedversionoftheaboveapproach.Wedonotexploitthispropertyinourexperiments—allareconductedwithasimplelinearkernel.However,byderivingthekernelizedversion,wegainsomeinsightintothemethod.Forthisreason,wesketchthederivationhere.SupposethatthedatapointsxaredrawnfromareproducingkernelHilbertspaceXwithkernelK:X×X→R,withKpositivesemi-deﬁnite.Then,Kcanbewrittenasthedotproduct(inX)oftwo(perhapsinﬁnite-dimensional)vectors:K(x,x′)=hΦ(x),Φ(x′)iX.DeﬁneΦsandΦtintermsofΦ,as:Φs(x)=hΦ(x),Φ(x),0i(2)Φt(x)=hΦ(x),0,Φ(x)iNow,wecancomputethekernelproductbe-tweenΦsandΦtintheexpandedRKHSbymak-inguseoftheoriginalkernelK.Wedenotetheex-pandedkernelby˘K(x,x′).Itissimplesttoﬁrstde-scribe˘K(x,x′)whenxandx′arefromthesamedomain,thenanalyzethecasewhenthedomaindiffers.Whenthedomainisthesame,weget:˘K(x,x′)=hΦ(x),Φ(x′)iX+hΦ(x),Φ(x′)iX=2K(x,x′).Whentheyarefromdifferentdomains,weget:˘K(x,x′)=hΦ(x),Φ(x′)iX=K(x,x′).Puttingthistogether,wehave:˘K(x,x′)=(cid:26)2K(x,x′)samedomainK(x,x′)diff.domain(3)Thisisanintuitivelypleasingresult.Whatitsaysisthat—consideringthekernelasameasureofsimilarity—datapointsfromthesamedomainare“bydefault”twiceassimilarasthosefromdiffer-entdomains.Looselyspeaking,thismeansthatdatapointsfromthetargetdomainhavetwiceasmuchinﬂuenceassourcepointswhenmakingpredictionsabouttesttargetdata.259

3.2AnalysisWeﬁrstnoteanobviouspropertyofthefeature-augmentationapproach.Namely,itdoesnotmakelearningharder,inaminimumBayeserrorsense.Amoreinterestingstatementwouldbethatitmakeslearningeasier,alongthelinesoftheresultof(Ben-Davidetal.,2006)—note,however,thattheirre-sultsareforthe“semi-supervised”domainadapta-tionproblemandsodonotapplydirectly.Asyet,wedonotknowaproperformalisminwhichtoan-alyzethefullysupervisedcase.Itturnsoutthatthefeature-augmentationmethodisremarkablysimilartothePRIORmodel2.Sup-posewelearnfeature-augmentedweightsinaclas-siﬁerregularizedbyanℓ2norm(eg.,SVMs,maxi-mumentropy).Wecandenotebywsthesumofthe“source”and“general”componentsofthelearnedweightvector,andbywtthesumofthe“target”and“general”components,sothatwsandwtarethepre-dictiveweightsforeachtask.Then,theregulariza-tionconditionontheentireweightvectorisapprox-imately||wg||2+||ws−wg||2+||wt−wg||2,withfreeparameterwgwhichcanbechosentominimizethissum.Thisleadstoaregularizerproportionalto||ws−wt||2,akintothePRIORmodel.Giventhissimilaritybetweenthefeature-augmentationmethodandthePRIORmodel,onemightwonderwhyweexpectourapproachtodobetter.Ourbeliefisthatthisoccursbecauseweop-timizewsandwtjointly,notsequentially.First,thismeansthatwedonotneedtocross-validatetoes-timategoodhyperparametersforeachtask(thoughinourexperiments,wedonotuseanyhyperparam-eters).Second,andmoreimportantly,thismeansthatthesinglesupervisedlearningalgorithmthatisrunisallowedtoregulatethetrade-offbetweensource/targetandgeneralweights.InthePRIORmodel,weareforcedtousethepriorvarianceoninthetargetlearningscenariotodothisourselves.3.3Multi-domainadaptationOurformulationisagnostictothenumberof“source”domains.Inparticular,itmaybethecasethatthesourcedataactuallyfallsintoavarietyofmorespeciﬁcdomains.Thisissimpletoaccountforinourmodel.Inthetwo-domaincase,weex-2Thanksananonymousreviewerforpointingthisout!pandedthefeaturespacefromRFtoR3F.ForaK-domainproblem,wesimplyexpandthefeaturespacetoR(K+1)Fintheobviousway(the“+1”cor-respondstothe“generaldomain”whileeachoftheother1...Kcorrespondtoasingletask).4ResultsInthissectionwedescribeexperimentalresultsonawidevarietyofdomains.Firstwedescribethetasks,thenwepresentexperimentalresults,andﬁnallywelookmorecloselyatafewoftheexperiments.4.1TasksAlltasksweconsideraresequencelabelingtasks(eithernamed-entityrecognition,shallowparsingorpart-of-speechtagging)onthefollowingdatasets:ACE-NER.Weusedatafromthe2005AutomaticContentExtractiontask,restrictingourselvestothenamed-entityrecognitiontask.The2005ACEdatacomesfrom5domains:Broad-castNews(bn),BroadcastConversations(bc),Newswire(nw),Weblog(wl),Usenet(un)andConverstaionalTelephoneSpeech(cts).CoNLL-NE.SimilartoACE-NER,anamed-entityrecognitiontask.Thedifferenceis:weusethe2006ACEdataasthesourcedomainandtheCoNLL2003NERdataasthetargetdomain.PubMed-POS.Apart-of-speechtaggingproblemonPubMedabstractsintroducedbyBlitzeretal.(2006).Therearetwodomains:thesourcedomainistheWSJportionofthePennTree-bankandthetargetdomainisPubMed.CNN-Recap.Thisisarecapitalizationtaskintro-ducedbyChelbaandAcero(2004)andalsousedbyDaum´eIIIandMarcu(2006).Thesourcedomainisnewswireandthetargetdo-mainistheoutputofanASRsystem.Treebank-Chunk.ThisisashallowparsingtaskbasedonthedatafromthePennTreebank.Thisdatacomesfromavarietyofdomains:thestan-dardWSJdomain(weusethesamedataasforCoNLL2000),theATISswitchboarddomain,andtheBrowncorpus(whichis,itself,assem-bledfromsixsubdomains).Treebank-Brown.ThisisidenticaltotheTreebank-Chunktask,exceptthatweconsideralloftheBrowncorpustobeasingledomain.260

TaskDom#Tr#De#Te#Ftbn52,9986,6256,62680kbc38,0734,7594,761109kACE-nw44,3645,5465,547113kNERwl35,8834,4854,487109kun35,0834,3854,38796kcts39,6774,9604,96154kCoNLL-src256,145--368kNERtgt29,7915,2588,80688kPubMed-src950,028--571kPOStgt11,2641,98714,55439kCNN-src2,000,000--368kRecaptgt39,6847,0038,07588kwsj191,20929,45538,44094kswbd345,2825,59641,84055kbr-cf58,2018,3077,607144kTreebr-cg67,4299,4446,897149kbank-br-ck51,3796,0619,451121kChunkbr-cl47,3825,1015,88095kbr-cm11,6961,3241,59451kbr-cn56,0576,7517,847115kbr-cp55,3187,4775,977112kbr-cr16,7422,5222,71265kTable1:Taskstatistics;columnsaretask,domain,sizeofthetraining,developmentandtestsets,andthenumberofuniquefeaturesinthetrainingset.Inallcases(exceptforCNN-Recap),weuseroughlythesamefeatureset,whichhasbecomesomewhatstandardized:lexicalinformation(words,stems,capitalization,preﬁxesandsufﬁxes),mem-bershipongazetteers,etc.FortheCNN-Recaptask,weuseidenticalfeaturetothoseusedbybothChelbaandAcero(2004)andDaum´eIIIandMarcu(2006):thecurrent,previousandnextword,and1-3letterpreﬁxesandsufﬁxes.StatisticsonthetasksanddatasetsareinTable1.Inallcases,weusetheSEARNalgorithmforsolv-ingthesequencelabelingproblem(Daum´eIIIetal.,2007)withanunderlyingaveragedperceptronclas-siﬁer;implementationdueto(Daum´eIII,2004).Forstructuralfeatures,wemakeasecond-orderMarkovassumptionandonlyplaceabiasfeatureonthetran-sitions.Forsimplicity,weoptimizeandreportonlyonlabelaccuracy(butrequirethatouroutputsbeparsimonious:wedonotallow“I-NP”tofollow“B-PP,”forinstance).Wedothisforthreerea-sons.First,ourfocusinthisworkisonbuildingbetterlearningalgorithmsandintroducingamorecomplicatedmeasureonlyservestomasktheseef-fects.Second,itisarguablethatameasurelikeF1isinappropriateforchunkingtasks(Manning,2006).Third,wecaneasilycomputestatisticalsigniﬁcanceoveraccuraciesusingMcNemar’stest.4.2ExperimentalResultsThefull—somewhatdaunting—tableofresultsispresentedinTable2.Theﬁrsttwocolumnsspec-ifythetaskanddomain.Forthetaskswithonlyasinglesourceandtarget,wesimplyreportresultsonthetarget.Forthemulti-domainadaptationtasks,wereportresultsforeachsettingofthetarget(whereallotherdata-setsareusedasdifferent“source”do-mains).Thenextsetofeightcolumnsaretheerrorratesforthetask,usingoneofthedifferenttech-niques(“AUGMENT”isourproposedtechnique).Foreachrow,theerrorrateofthebestperformingtechniqueisbolded(asarealltechniqueswhoseper-formanceisnotstatisticallysigniﬁcantlydifferentatthe95%level).The“T<S”columniscontainsa“+”wheneverTGTONLYoutperformsSRCONLY(thiswillbecomeimportantshortly).TheﬁnalcolumnindicateswhenAUGMENTcomesinﬁrst.3Thereareseveraltrendstonoteintheresults.Ex-cludingforamomentthe“br-*”domainsontheTreebank-Chunktask,ourtechniquealwaysper-formsbest.Stillexcluding“br-*”,theclearsecond-placecontestantisthePRIORmodel,aﬁndingcon-sistentwithpriorresearch.WhenwerepeattheTreebank-Chunktask,butlumpingallofthe“br-*”datatogetherintoasingle“brown”domain,thestoryrevertstowhatweexpectedbefore:ouralgorithmperformsbest,followedbythePRIORmethod.Importantly,thissimplestorybreaksdownontheTreebank-ChunktaskfortheeightsectionsoftheBrowncorpus.Forthese,ourAUGMENTtechniqueperformsratherpoorly.Moreover,thereisnoclearwinningapproachonthistask.OurhypothesisisthatthecommonfeatureoftheseexamplesisthattheseareexactlythetasksforwhichSRCONLYout-performsTGTONLY(withoneexception:CoNLL).Thisseemslikeaplausibleexplanation,sinceitim-pliesthatthesourceandtargetdomainsmaynotbethatdifferent.Ifthedomainsaresosimilarthatalargeamountofsourcedataoutperformsasmallamountoftargetdata,thenitisunlikelythatblow-3Oneadvantageofusingtheaveragedperceptronforallex-perimentsisthattheonlytunablehyperparameteristhenumberofiterations.Inallcases,werun20iterationsandchoosetheonewiththelowesterrorondevelopmentdata.261

TaskDomSRCONLYTGTONLYALLWEIGHTPREDLININTPRIORAUGMENTT<SWinbn4.982.372.292.232.112.212.061.98++bc4.544.073.553.533.894.013.473.47++ACE-nw4.783.713.863.653.563.793.683.39++NERwl2.452.452.122.122.452.332.412.12=+un3.672.462.482.402.182.102.031.91++cts2.080.460.400.400.460.440.340.32++CoNLLtgt2.492.951.801.752.131.771.891.76+PubMedtgt12.024.155.434.154.143.953.993.61++CNNtgt10.293.823.673.453.463.443.353.37++wsj6.634.354.334.304.324.324.274.11++swbd315.904.154.504.104.134.093.603.51++br-cf5.166.274.854.804.784.725.225.15Treebr-cg4.325.364.164.154.274.304.254.90bank-br-ck5.056.325.054.985.015.055.275.41Chunkbr-cl5.666.605.425.395.395.535.995.73br-cm3.576.593.143.113.153.314.084.89br-cn4.605.564.274.224.204.194.484.42br-cp4.825.624.634.574.554.554.874.78br-cr5.789.135.715.195.205.156.716.30Treebank-brown6.355.754.804.754.814.724.724.65++Table2:Taskresults.ingupthefeaturespacewillhelp.WeadditionallyrantheMEGAMmodel(Daum´eIIIandMarcu,2006)onthesedata(thoughnotinthemulti-conditionalcase;forthis,weconsid-eredthesinglesourceastheunionofallsources).TheresultsarenotdisplayedinTable2tosavespace.Forthemajorityofresults,MEGAMper-formedroughlycomparablytothebestofthesys-temsinthetable.Inparticular,itwasnotsta-tisticallysigniﬁcantlydifferentthatAUGMENTon:ACE-NER,CoNLL,PubMed,Treebank-chunk-wsj,Treebank-chunk-swbd3,CNNandTreebank-brown.ItdidoutperformAUGMENTontheTreebank-chunkontheTreebank-chunk-br-*datasets,butonlyout-performedthebestothermodelonthesedatasetsforbr-cg,br-cmandbr-cp.However,despiteitsadvantagesonthesedatasets,itwasquitesigniﬁ-cantlyslowertotrain:asinglerunrequiredabouttentimeslongerthananyoftheothermodels(includingAUGMENT),andalsorequiredﬁve-to-teniterationsofcross-validationtotuneitshyperparameterssoastoachievetheseresults.4.3ModelIntrospectionOneexplanationofourmodel’simprovedperfor-manceissimplythatbyaugmentingthefeaturespace,wearecreatingamorepowerfulmodel.Whilethismaybeapartialexplanation,hereweshowthatwhatthemodellearnsaboutthevarious*bnbcnwwlunctsPERGPEORGLOCFigure1:Hintondiagramforfeature/Aa+/atcur-rentposition.domainsactuallymakessomeplausiblesense.WeperformthisanalysisonlyontheACE-NERdatabylookingspeciﬁcallyatthelearnedweights.Thatis,foranygivenfeaturef,therewillbesevenversionsoff:onecorrespondingtothe“cross-domain”fandsevencorrespondingtoeachdomain.Wevisualizetheseweights,usingHintondiagrams,toseehowtheweightsvaryacrossdomains.Forexample,considerthefeature“currentwordhasaninitialcapitalletterandisthenfollowedbyoneormorelower-caseletters.”Thisfeatureispre-sumablyuselessfordatathatlackscapitalizationin-formation,butpotentiallyquiteusefulforotherdo-mains.InFigure1weshownaHintondiagramforthisﬁgure.Eachcolumninthisﬁgurecorrespondtoadomain(thetoprowisthe“generaldomain”).262

*bnbcnwwlunctsPERGPEORGLOCFigure2:Hintondiagramforfeature/bush/atcur-rentposition.Eachrowcorrespondstoaclass.4Blackboxescor-respondtonegativeweightsandwhiteboxescorre-spondtopositiveweights.Thesizeoftheboxde-pictstheabsolutevalueoftheweight.AswecanseefromFigure1,the/Aa+/featureisaverygoodindicatorofentity-hood(it’svalueisstronglypositiveforallfourentityclasses),regard-lessofdomain(i.e.,forthe“*”domain).Thelackofboxesinthe“bn”columnmeansthat,beyondthesettingsin“*”,thebroadcastnewsisagnosticwithrespecttothisfeature.Thismakessense:thereisnocapitalizationinbroadcastnewsdomain,sotherewouldbenosenseissettingtheseweightstoany-thingbyzero.Theusenetcolumnisﬁlledwithneg-ativeweights.Whilethismayseemstrange,itisduetothefactthatmanyemailaddressesandURLsmatchthispattern,butarenotentities.Figure2depictsasimilarﬁgureforthefeature“wordis’bush’atthecurrentposition”(thisﬁgureiscasesensitive).5Theseweightsaresomewhathardertointerpret.Whatishappeningisthat“bydefault”theword“bush”isgoingtobeaperson—thisisbe-causeitrarelyappearsreferringtoaplantandsoeveninthecapitalizeddomainslikebroadcastcon-versations,ifitappearsatall,itisaperson.Theexceptionisthatintheconversationsdata,peopledoactuallytalkaboutbushesasplants,andsotheweightsaresetaccordingly.Theweightsarehighintheusenetdomainbecausepeopletendtotalkaboutthepresidentwithoutcapitalizinghisname.4Technicallytherearemanymoreclassesthanareshownhere.Wedonotdepictthesmallestclasses,andhavemergedthe“Begin-*”and“In-*”weightsforeachentitytype.5Thescaleofweightsacrossfeaturesisnotcomparable,sodonottrytocompareFigure1withFigure2.*bnbcnwwlunctsPERGPEORGLOCFigure3:Hintondiagramforfeature/the/atcurrentposition.*bnbcnwwlunctsPERGPEORGLOCFigure4:Hintondiagramforfeature/the/atprevi-ousposition.Figure3presentstheHintondiagramforthefea-ture“wordatthecurrentpositionis’the”’(again,case-sensitive).Ingeneral,itappears,“the”isacommonwordinentitiesinalldomainexceptforbroadcastnewsandconversations.Theexceptionsarebroadcastnewsandconversations.Theseexcep-tionscropupbecauseofthecapitalizationissue.InFigure4,weshowthediagramforthefeature“previouswordis’the’.”Theonlydomainforwhichthisisagoodfeatureofentity-hoodisbroadcastconversations(toamuchlesserextent,newswire).Thisoccursbecauseoffourphrasesverycommoninthebroadcastconversationsandrareelsewhere:“theIraqipeople”(“Iraqi”isaGPE),“thePentagon”(anORG),“theBush(cabinet|advisors|...)”(PER),and“theSouth”(LOC).Finally,Figure5showstheHintondiagramforthefeature“thecurrentwordisonalistofcom-monnames”(thisfeatureiscase-insensitive).Allaround,thisisagoodfeatureforpickingoutpeopleandnothingelse.Thetwoexceptionsare:itisalsoagoodfeatureforotherentitytypesforbroadcast263

*bnbcnwwlunctsPERGPEORGLOCFigure5:Hintondiagramformembershiponalistofnamesatcurrentposition.newsanditisnotquitesogoodforpeopleinusenet.Theﬁrstiseasilyexplained:inbroadcastnews,itisverycommontorefertocountriesandorganiza-tionsbythenameoftheirrespectiveleaders.Thisisessentiallyametonymyissue,butasthedataisan-notated,thesearemarkedbytheirtruereferent.Forusenet,itisbecausethelistofnamescomesfromnewsdata,butusenetnamesaremorediverse.Ingeneral,theweightsdepicteforthesefeaturesmakesomeintuitivesense(inasmuchasweightsforanylearnedalgorithmmakeintuitivesense).Itisparticularlyinterestingtonotethatwhiletherearesomeregularitiestothepatternsintheﬁvediagrams,itisdeﬁnitelynotthecasethatthereare,eg.,twodomainsthatbehaveidenticallyacrossallfeatures.Thissupportsthehypothesisthatthereasonoural-gorithmworkssowellonthisdataisbecausethedomainsareactuallyquitewellseparated.5DiscussionInthispaperwehavedescribedanincrediblysim-pleapproachtodomainadaptationthat—underacommonandeasy-to-verifycondition—outperformspreviousapproaches.Whileitissomewhatfrus-tratingthatsomethingsosimpledoessowell,itisperhapsnotsurprising.Byaugmentingthefea-turespace,weareessentiallyforcingthelearningalgorithmtodotheadaptationforus.Goodsuper-visedlearningalgorithmshavebeendevelopedoverdecades,andsoweareessentiallyjustleveragingallthatpreviouswork.Ourhopeisthatthisapproachissosimplethatitcanbeusedformanymorereal-worldtasksthanwehavepresentedherewithlittleeffort.Finally,itisveryinterestingtonotethatus-ingourmethod,shallowparsingerrorrateontheCoNLLsectionofthetreebankimprovesfrom5.35to5.11.Whilethisimprovementissmall,itisreal,andmaycarryovertofullparsing.Themostimpor-tantavenueoffutureworkistodevelopaformalframeworkunderwhichwecananalyzethis(andothersuperviseddomainadaptationmodels)theo-retically.Currentlyourresultsonlystatethatthisaugmentationproceduredoesn’tmakethelearningharder—wewouldliketoknowthatitactuallymakesiteasier.Anadditionalfuturedirectionistoexplorethekernelizationinterpretationfurther:whyshouldweuse2asthe“similarity”betweendomains—wecouldintroduceahyperparamterαthatindicatesthesimilaritybetweendomainsandcouldbetunedviacross-validation.Acknowledgments.Wethankthethreeanony-mousreviewers,aswellasRyanMcDonaldandJohnBlitzerforveryhelpfulcommentsandinsights.ReferencesShaiBen-David,JohnBlitzer,KobyCrammer,andFernandoPereira.2006.Analysisofrepresentationsfordomainadap-tation.InAdvancesinNeuralInformationProcessingSys-tems(NIPS).JohnBlitzer,RyanMcDonald,andFernandoPereira.2006.Domainadaptationwithstructuralcorrespondencelearning.InProceedingsoftheConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP).CiprianChelbaandAlexAcero.2004.Adaptationofmax-imumentropyclassiﬁer:Littledatacanhelpalot.InPro-ceedingsoftheConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP),Barcelona,Spain.HalDaum´eIIIandDanielMarcu.2006.Domainadaptationforstatisticalclassiﬁers.JournalofArtiﬁcialIntelligenceResearch,26.HalDaum´eIII,JohnLangford,andDanielMarcu.2007.Search-basedstructuredprediction.MachineLearningJour-nal(submitted).HalDaum´eIII.2004.NotesonCGandLM-BFGSopti-mizationoflogisticregression.Paperavailableathttp://pub.hal3.name/#daume04cg-bfgs,implemen-tationavailableathttp://hal3.name/megam/,Au-gust.ChristopherManning.2006.Doingnamedentityrecognition?Don’toptimizeforF1.PostontheNLPersBlog,25August.http://nlpers.blogspot.com/2006/08/doing-named-entity-recognition-dont.html.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 264–271,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

264

InstanceWeightingforDomainAdaptationinNLPJingJiangandChengXiangZhaiDepartmentofComputerScienceUniversityofIllinoisatUrbana-ChampaignUrbana,IL61801,USA{jiang4,czhai}@cs.uiuc.eduAbstractDomainadaptationisanimportantprobleminnaturallanguageprocessing(NLP)duetothelackoflabeleddatainnoveldomains.Inthispaper,westudythedomainadaptationproblemfromtheinstanceweightingper-spective.Weformallyanalyzeandcharac-terizethedomainadaptationproblemfromadistributionalview,andshowthattherearetwodistinctneedsforadaptation,cor-respondingtothedifferentdistributionsofinstancesandclassiﬁcationfunctionsinthesourceandthetargetdomains.Wethenproposeageneralinstanceweightingframe-workfordomainadaptation.Ourempir-icalresultsonthreeNLPtasksshowthatincorporatingandexploitingmoreinforma-tionfromthetargetdomainthroughinstanceweightingiseffective.1IntroductionManynaturallanguageprocessing(NLP)problemssuchaspart-of-speech(POS)tagging,namedentity(NE)recognition,relationextraction,andseman-ticrolelabeling,arecurrentlysolvedbysupervisedlearningfrommanuallylabeleddata.Abottleneckproblemwiththissupervisedlearningapproachisthelackofannotateddata.Asaspecialcase,weoftenfacethesituationwherewehaveasufﬁcientamountoflabeleddatainonedomain,buthavelittleornolabeleddatainanotherrelateddomainwhichweareinterestedin.Wethusfacethedomainadap-tationproblem.Following(Blitzeretal.,2006),wecalltheﬁrstthesourcedomain,andthesecondthetargetdomain.Thedomainadaptationproblemiscommonlyen-counteredinNLP.Forexample,inPOStagging,thesourcedomainmaybetaggedWSJarticles,andthetargetdomainmaybescientiﬁcliteraturethatcon-tainsscientiﬁcterminology.InNErecognition,thesourcedomainmaybeannotatednewsarticles,andthetargetdomainmaybepersonalblogs.Anotherexampleispersonalizedspamﬁltering,wherewemayhavemanylabeledspamandhamemailsfrompubliclyavailablesources,butweneedtoadaptthelearnedspamﬁltertoanindividualuser’sinboxbe-causetheuserhasherown,andpresumablyverydif-ferent,distributionofemailsandnotionofspams.DespitetheimportanceofdomainadaptationinNLP,currentlytherearenostandardmethodsforsolvingthisproblem.Animmediatepossiblesolu-tionissemi-supervisedlearning,wherewesimplytreatthetargetinstancesasunlabeleddatabutdonotdistinguishthetwodomains.However,giventhatthesourcedataandthetargetdataarefromdif-ferentdistributions,weshouldexpecttodobetterbyexploitingthedomaindifference.Recentlytherehavebeensomestudiesaddressingdomainadapta-tionfromdifferentperspectives(RoarkandBacchi-ani,2003;ChelbaandAcero,2004;Florianetal.,2004;Daum´eIIIandMarcu,2006;Blitzeretal.,2006).However,therehavenotbeenmanystudiesthatfocusonthedifferencebetweentheinstancedis-tributionsinthetwodomains.AdetaileddiscussiononrelatedworkisgiveninSection5.Inthispaper,westudythedomainadaptationproblemfromtheinstanceweightingperspective.265

Ingeneral,thedomainadaptationproblemariseswhenthesourceinstancesandthetargetinstancesarefromtwodifferent,butrelateddistributions.Weformallyanalyzeandcharacterizethedomainadaptationproblemfromthisdistributionalview.Suchananalysisrevealsthattherearetwodistinctneedsforadaptation,correspondingtothediffer-entdistributionsofinstancesandthedifferentclas-siﬁcationfunctionsinthesourceandthetargetdo-mains.Basedonthisanalysis,weproposeagen-eralinstanceweightingmethodfordomainadapta-tion,whichcanberegardedasageneralizationofanexistingapproachtosemi-supervisedlearning.Theproposedmethodimplementsseveraladapta-tionheuristicswithauniﬁedobjectivefunction:(1)removingmisleadingtraininginstancesinthesourcedomain;(2)assigningmoreweightstolabeledtar-getinstancesthanlabeledsourceinstances;(3)aug-mentingtraininginstanceswithtargetinstanceswithpredictedlabels.WeevaluatedtheproposedmethodwiththreeadaptationproblemsinNLP,includingPOStagging,NEtypeclassiﬁcation,andspamﬁlter-ing.Theresultsshowthatregularsemi-supervisedandsupervisedlearningmethodsdonotperformaswellasournewmethod,whichexplicitlycapturesdomaindifference.Ourresultsalsoshowthatin-corporatingandexploitingmoreinformationfromthetargetdomainismuchmoreusefulforimprov-ingperformancethanexcludingmisleadingtrainingexamplesfromthesourcedomain.Therestofthepaperisorganizedasfollows.InSection2,weformallyanalyzethedomainadapta-tionproblemanddistinguishtwotypesofadapta-tion.InSection3,wethenproposeageneralin-stanceweightingframeworkfordomainadaptation.InSection4,wepresenttheexperimentresults.Fi-nally,wecompareourframeworkwithrelatedworkinSection5beforeweconcludeinSection6.2DomainAdaptationInthissection,wedeﬁneandanalyzedomainadap-tationfromatheoreticalpointofview.Weshowthattheneedfordomainadaptationarisesfromtwofac-tors,andthesolutionsaredifferentforeachfactor.WerestrictourattentiontothoseNLPtasksthatcanbecastintomulticlassclassiﬁcationproblems,andweonlyconsiderdiscriminativemodelsforclassiﬁ-cation.SincebotharecommonpracticeinNLP,ouranalysisisapplicabletomanyNLPtasks.LetXbeafeaturespacewechoosetorepresenttheobservedinstances,andletYbethesetofclasslabels.Inthestandardsupervisedlearningsetting,wearegivenasetoflabeledinstances{(xi,yi)}Ni=1,wherexi∈X,yi∈Y,and(xi,yi)aredrawnfromanunknownjointdistributionp(x,y).Ourgoalistorecoverthisunknowndistributionsothatwecanpre-dictunlabeledinstancesdrawnfromthesamedistri-bution.Indiscriminativemodels,weareonlycon-cernedwithp(y|x).Followingthemaximumlikeli-hoodestimationframework,westartwithaparame-terizedmodelfamilyp(y|x;θ),andthenﬁndthebestmodelparameterθ∗thatmaximizestheexpectedloglikelihoodofthedata:θ∗=argmaxθZXXy∈Yp(x,y)logp(y|x;θ)dx.Sincewedonotknowthedistributionp(x,y),wemaximizetheempiricalloglikelihoodinstead:θ∗≈argmaxθZXXy∈Y˜p(x,y)logp(y|x;θ)dx=argmaxθ1NNXi=1logp(yi|xi;θ).Notethatsinceweusetheempiricaldistribution˜p(x,y)toapproximatep(x,y),theestimatedθ∗isdependenton˜p(x,y).Ingeneral,aslongaswehavesufﬁcientlabeleddata,thisapproximationisﬁnebe-causetheunlabeledinstanceswewanttoclassifyarefromthesamep(x,y).2.1TwoFactorsforDomainAdaptationLetusnowturntothecaseofdomainadaptationwheretheunlabeledinstanceswewanttoclassifyarefromadifferentdistributionthanthelabeledin-stances.Letps(x,y)andpt(x,y)bethetrueun-derlyingdistributionsforthesourceandthetargetdomains,respectively.Ourgeneralideaistouseps(x,y)toapproximatept(x,y)sothatwecanex-ploitthelabeledexamplesinthesourcedomain.Ifwefactorp(x,y)intop(x,y)=p(y|x)p(x),wecanseethatpt(x,y)candeviatefromps(x,y)intwodifferentways,correspondingtotwodifferentkindsofdomainadaptation:266

Case1(LabelingAdaptation):pt(y|x)deviatesfromps(y|x)toacertainextent.Inthiscase,itisclearthatourestimationofps(y|x)fromthelabeledsourcedomaininstanceswillnotbeagoodestima-tionofpt(y|x),andthereforedomainadaptationisneeded.Werefertothiskindofadaptationasfunc-tion/labelingadaptation.Case2(InstanceAdaptation):pt(y|x)ismostlysimilartops(y|x),butpt(x)deviatesfromps(x).Inthiscase,itmayappearthatourestimatedps(y|x)canstillbeusedinthetargetdomain.However,aswehavepointedout,theestimationofps(y|x)de-pendsontheempiricaldistribution˜ps(x,y),whichdeviatesfrompt(x,y)duetothedeviationofps(x)frompt(x).Ingeneral,theestimationofps(y|x)wouldbemoreinﬂuencedbytheinstanceswithhigh˜ps(x,y)(i.e.,high˜ps(x)).Ifpt(x)isverydiffer-entfromps(x),thenweshouldexpectpt(x,y)tobeverydifferentfromps(x,y),andthereforedifferentfrom˜ps(x,y).Wethuscannotexpecttheestimatedps(y|x)toworkwellontheregionswherept(x,y)ishigh,butps(x,y)islow.Therefore,inthiscase,westillneeddomainadaptation,whichwerefertoasinstanceadaptation.Becausetheneedfordomainadaptationarisesfromtwodifferentfactors,weneeddifferentsolu-tionsforeachfactor.2.2SolutionsforLabelingAdaptationIfpt(y|x)deviatesfromps(y|x)tosomeextent,wehaveoneofthefollowingchoices:Changeofrepresentation:Itmaybethecasethatifwechangetherep-resentationoftheinstances,i.e.,ifwechooseafeaturespaceX0differentfromX,wecanbridgethegapbetweenthetwodistributionsps(y|x)andpt(y|x).Forexample,considerdomainadaptiveNErecognitionwherethesourcedomaincontainscleannewswiredata,whilethetargetdomaincon-tainsbroadcastnewsdatathathasbeentranscribedbyautomaticspeechrecognitionandlackscapital-ization.SupposeweuseanaiveNEtaggerthatonlylooksattheworditself.Ifweconsidercapi-talization,thentheinstanceBushisrepresenteddif-ferentlyfromtheinstancebush.Inthesourcedo-main,ps(y=Person|x=Bush)ishighwhileps(y=Person|x=bush)islow,butinthetargetdomain,pt(y=Person|x=bush)ishigh.Ifweignorethecapitalizationinformation,theninbothdomainsp(y=Person|x=bush)willbehighpro-videdthatthesourcedomaincontainsmuchfewerinstancesofbushthanBush.Adaptationthroughprior:Whenweuseaparameterizedmodelp(y|x;θ)toapproximatep(y|x)andestimateθbasedonthesourcedomaindata,wecanplacesomeprioronthemodelparameterθsothattheestimateddistributionp(y|x;ˆθ)willbeclosertopt(y|x).ConsideragaintheNEtaggingexample.Ifweusecapitalizationasafeature,inthesourcedomainwherecapitalizationinformationisavailable,thisfeaturewillbegivenalargeweightinthelearnedmodelbecauseitisveryuseful.Ifweplaceapriorontheweightforthisfea-turesothatalargeweightwillbepenalized,thenwecanpreventthelearnedmodelfromrelyingtoomuchonthisdomainspeciﬁcfeature.Instancepruning:Ifweknowtheinstancesxforwhichpt(y|x)isdifferentfromps(y|x),wecanactivelyremovetheseinstancesfromthetrainingdatabecausetheyare“misleading”.Forallthethreesolutionsgivenabove,weneedeithersomepriorknowledgeaboutthetargetdo-main,orsomelabeledtargetdomaininstances;fromonlytheunlabeledtargetdomaininstances,wewouldnotknowwhereandwhypt(y|x)differsfromps(y|x).2.3SolutionsforInstanceAdaptationInthecasewherept(y|x)issimilartops(y|x),butpt(x)deviatesfromps(x),wemayusethe(unla-beled)targetdomaininstancestobiastheestimateofps(x)towardabetterapproximationofpt(x),andthusachievedomainadaptation.Weexplaintheideabelow.Ourgoalistoobtainagoodestimateofθ∗tthatisoptimizedaccordingtothetargetdomaindistribu-tionpt(x,y).Theexactobjectivefunctionisthusθ∗t=argmaxθZXXy∈Ypt(x,y)logp(y|x;θ)dx=argmaxθZXpt(x)Xy∈Ypt(y|x)logp(y|x;θ)dx.267

Ourideaofdomainadaptationistoexploitthela-beledinstancesinthesourcedomaintohelpobtainθ∗t.LetDs={(xsi,ysi)}Nsi=1denotethesetofla-beledinstanceswehavefromthesourcedomain.Assumethatwehavea(small)setoflabeledanda(large)setofunlabeledinstancesfromthetar-getdomain,denotedbyDt,l={(xt,lj,yt,lj)}Nt,lj=1andDt,u={xt,uk}Nt,uk=1,respectively.Wenowshowthreewaystoapproximatetheobjectivefunctionabove,correspondingtousingthreedifferentsetsofin-stancestoapproximatetheinstancespaceX.UsingDs:Usingps(y|x)toapproximatept(y|x),weobtainθ∗t≈argmaxθZXpt(x)ps(x)ps(x)Xy∈Yps(y|x)logp(y|x;θ)dx≈argmaxθZXpt(x)ps(x)˜ps(x)Xy∈Y˜ps(y|x)logp(y|x;θ)dx=argmaxθ1NsNsXi=1pt(xsi)ps(xsi)logp(ysi|xsi;θ).HereweuseonlythelabeledinstancesinDsbutweadjusttheweightofeachinstancebypt(x)ps(x).Themajordifﬁcultyishowtoaccuratelyestimatept(x)ps(x).UsingDt,l:θ∗t≈argmaxθZX˜pt,l(x)Xy∈Y˜pt,l(y|x)logp(y|x;θ)dx=argmaxθ1Nt,lNt,lXj=1logp(yt,lj|xt,lj;θ)Notethatthisisthestandardsupervisedlearningmethodusingonlythesmallamountoflabeledtar-getinstances.Themajorweaknessofthisapproxi-mationisthatwhenNt,lisverysmall,theestimationisnotaccurate.UsingDt,u:θ∗t≈argmaxθZX˜pt,u(x)Xy∈Ypt(y|x)logp(y|x;θ)dx=argmaxθ1Nt,uNt,uXk=1Xy∈Ypt(y|xt,uk)logp(y|xt,uk;θ),Thechallengehereisthatpt(y|xt,uk;θ)isunknowntous,thusweneedtoestimateit.OnepossibilityistoapproximateitwithamodelˆθlearnedfromDsandDt,l.Forexample,wecansetpt(y|x,θ)=p(y|x;ˆθ).Alternatively,wecanalsosetpt(y|x,θ)to1ify=argmaxy0p(y0|x;ˆθ)and0otherwise.3AFrameworkofInstanceWeightingforDomainAdaptationThetheoreticalanalysiswegiveinSection2sug-geststhatonewaytosolvethedomainadaptationproblemisthroughinstanceweighting.WeproposeaframeworkthatincorporatesinstancepruninginSection2.2andthethreeapproximationsinSec-tion2.3.Beforeweshowtheformalframework,weﬁrstintroducesomeweightingparametersandex-plaintheintuitionsbehindtheseparameters.First,foreach(xsi,ysi)∈Ds,weintroduceapa-rameterαitoindicatehowlikelypt(ysi|xsi)isclosetops(ysi|xsi).Largeαimeansthetwoprobabilitiesareclose,andthereforewecantrustthelabeledin-stance(xsi,ysi)forthepurposeoflearningaclas-siﬁerforthetargetdomain.Smallαimeansthesetwoprobabilitiesareverydifferent,andthereforeweshouldprobablydiscardtheinstance(xsi,ysi)inthelearningprocess.Second,againforeach(xsi,ysi)∈Ds,weintro-duceanotherparameterβithatideallyisequaltopt(xsi)ps(xsi).FromtheapproximationinSection2.3thatusesonlyDs,itisclearthatsuchaparameterisuse-ful.Next,foreachxt,ui∈Dt,u,andforeachpossiblelabely∈Y,weintroduceaparameterγi(y)thatindicateshowlikelywewouldliketoassignyasatentativelabeltoxt,uiandinclude(xt,ui,y)asatrain-ingexample.Finally,weintroducethreeglobalparametersλs,λt,landλt,uthatarenotinstance-speciﬁcbutareas-sociatedwithDs,Dt,landDt,u,respectively.ThesethreeparametersallowustocontrolthecontributionofeachofthethreeapproximationmethodsinSec-tion2.3whenwelinearlycombinethemtogether.Wenowformallydeﬁneourinstanceweightingframework.GivenDs,Dt,landDt,u,tolearnaclas-siﬁerforthetargetdomain,weﬁndaparameterˆθthatoptimizesthefollowingobjectivefunction:268

ˆθ=argmaxθ•λs·1CsNsXi=1αiβilogp(ysi|xsi;θ)+λt,l·1Ct,lNt,lXj=1logp(yt,lj|xt,lj;θ)+λt,u·1Ct,uNt,uXk=1Xy∈Yγk(y)logp(y|xt,uk;θ)+logp(θ)‚,whereCs=PNsi=1αiβi,Ct,l=Nt,l,Ct,u=PNt,uk=1Py∈Yγk(y),andλs+λt,l+λt,u=1.Thelastterm,logp(θ),isthelogofaGaussianpriordis-tributionofθ,commonlyusedtoregularizethecom-plexityofthemodel.Ingeneral,wedonotknowtheoptimalvaluesoftheseparametersforthetargetdomain.Neverthe-less,theintuitionsbehindtheseparametersserveasguidelinesforustodesignheuristicstosetthesepa-rameters.Intherestofthissection,weintroduceseveralheuristicsthatweusedinourexperimentstosettheseparameters.3.1SettingαFollowingtheintuitionthatifpt(y|x)differsmuchfromps(y|x),then(x,y)shouldbediscardedfromthetrainingset,weusethefollowingheuristictosetαs.First,withstandardsupervisedlearning,wetrainamodelˆθt,lfromDt,l.Weconsiderp(y|x;ˆθt,l)tobeacrudeapproximationofpt(y|x).Then,weclassify{xsi}Nsi=1usingˆθt,l.Thetopkinstancesthatareincorrectlypredictedbyˆθt,l(rankedbytheirpredictionconﬁdence)arediscarded.Inanotherword,αsiofthetopkinstancesforwhichysi6=argmaxyp(y|xsi;ˆθt,l)aresetto0,andαiofalltheothersourceinstancesaresetto1.3.2SettingβAccuratelysettingβinvolvesaccuratelyestimatingps(x)andpt(x)fromtheempiricaldistributions.FormanyNLPclassiﬁcationtasks,wedonothaveagoodparametricmodelforp(x).Wethusneedtore-sorttonon-parametricdensityestimationmethods.However,formanyNLPtasks,xresidesinahighdimensionalspace,whichmakesithardtoapplystandardnon-parametricdensityestimationmeth-ods.Wehavenotexploredthisdirection,andinourexperiments,wesetβto1forallsourceinstances.3.3SettingγSettingγiscloselyrelatedtosomesemi-supervisedlearningmethods.Oneoptionistosetγk(y)=p(y|xt,uk;θ).Inthiscase,γisnolongeraconstantbutisafunctionofθ.Thiswayofsettingγcorre-spondstotheentropyminimizationsemi-supervisedlearningmethod(GrandvaletandBengio,2005).Anotherwaytosetγcorrespondstobootstrappingsemi-supervisedlearning.First,letˆθ(n)beamodellearnedfromthepreviousroundoftraining.WethenselectthetopkinstancesfromDt,uthathavethehighestpredictionconﬁdence.Fortheseinstances,wesetγk(y)=1fory=argmaxy0p(y0|xt,uk;ˆθ(n)),andγk(y)=0forallothery.Inanotherword,weselectthetopkconﬁdentlypredictedinstances,andincludetheseinstancestogetherwiththeirpredictedlabelsinthetrainingset.AllotherinstancesinDt,uarenotconsidered.Inourexperiments,weonlycon-sideredthisbootstrappingwayofsettingγ.3.4Settingλλs,λt,landλt,ucontrolthebalanceamongthethreesetsofinstances.Usingstandardsupervisedlearn-ing,λsandλt,laresetproportionallytoCsandCt,l,thatis,eachinstanceisweightedthesamewhetheritisinDsorinDt,l,andλt,uissetto0.Similarly,usingstandardbootstrapping,λt,uissetproportion-allytoCt,u,thatis,eachtargetinstanceaddedtothetrainingsetisalsoweightedthesameasasourceinstance.Inneithercasearethetargetinstancesem-phasizemorethansourceinstances.However,fordomainadaptation,wewanttofocusmoreonthetargetdomaininstances.Sointuitively,wewanttomakeλt,landλt,usomehowlargerrelativetoλs.AswewillshowinSection4,thisisindeedbeneﬁcial.Ingeneral,theframeworkprovidesgreatﬂexibil-ityforimplementingdifferentadaptationstrategiesthroughtheseinstanceweightingparameters.4Experiments4.1TasksandDataSetsWechosethreedifferentNLPtaskstoevaluateourinstanceweightingmethodfordomainadaptation.TheﬁrsttaskisPOStagging,forwhichweused269

6166WSJsentencesfromSections00and01ofPennTreebankasthesourcedomaindata,and2730PubMedsentencesfromtheOncologysectionofthePennBioIEcorpusasthetargetdomaindata.Thesecondtaskisentitytypeclassiﬁcation.ThesetupisverysimilartoDaum´eIIIandMarcu(2006).Weassumethattheentityboundarieshavebeencor-rectlyidentiﬁed,andwewanttoclassifythetypesoftheentities.WeusedACE2005trainingdataforthistask.Forthesourcedomain,weusedthenewswirecollection,whichcontains11256exam-ples,andforthetargetdomains,weusedthewe-blog(WL)collection(5164examples)andthecon-versationaltelephonespeech(CTS)collection(4868examples).Thethirdtaskispersonalizedspamﬁl-tering.WeusedtheECML/PKDD2006discov-erychallengedataset.Thesourcedomaincontains4000spamandhamemailsfrompubliclyavailablesources,andthetargetdomainsarethreeindividualusers’inboxes,eachcontaining2500emails.Foreachtask,weconsidertwoexperimentset-tings.Intheﬁrstsetting,weassumethereareasmallnumberoflabeledtargetinstancesavailable.ForPOStagging,weusedanadditional300Oncologysentencesaslabeledtargetinstances.ForNEtyp-ing,weused500labeledtargetinstancesand2000unlabeledtargetinstancesforeachtargetdomain.Forspamﬁltering,weused200labeledtargetin-stancesand1800unlabeledtargetinstances.Inthesecondsetting,weassumethereisnolabeledtargetinstance.Wethususedallavailabletargetinstancesfortestinginallthreetasks.Weusedlogisticregressionasourmodelofp(y|x;θ)becauseitisarobustlearningalgorithmandwidelyused.Wenowdescribethreesetsofexperiments,cor-respondingtothreeheuristicwaysofsettingα,λt,landλt,u.4.2Removing“Misleading”SourceDomainInstancesIntheﬁrstsetofexperiments,wegraduallyremove“misleading”labeledinstancesfromthesourcedo-main,usingthesmallnumberoflabeledtargetin-stanceswehave.Wefollowtheheuristicwede-scribedinSection3.1,whichsetstheαforthetopkmisclassiﬁedsourceinstancesto0,andtheαforalltheothersourceinstancesto1.Wealsosetλt,landλt,lto0inordertofocusonlyontheeffectofremoving“misleading”instances.Wecomparewithabaselinemethodwhichusesallsourceinstanceswithequalweightbutnotargetinstances.There-sultsareshowninTable1.Fromthetable,wecanseethatinmostexper-iments,removingthesepredicted“misleading”ex-amplesimprovedtheperformanceoverthebaseline.Insomeexperiments(Oncology,CTS,u00,u01),thelargestimprovementwasachievedwhenallmisclas-siﬁedsourceinstanceswereremoved.InthecaseofweblogNEtypeclassiﬁcation,however,removingthesourceinstanceshurttheperformance.Apos-siblereasonforthisisthatthesetoflabeledtargetinstancesweuseisabiasedsamplefromthetargetdomain,andthereforethemodeltrainedonthesein-stancesisnotalwaysagoodpredictorof“mislead-ing”sourceinstances.4.3AddingLabeledTargetDomainInstanceswithHigherWeightsThesecondsetofexperimentsistoaddthelabeledtargetdomaininstancesintothetrainingset.Thiscorrespondstosettingλt,ltosomenon-zerovalue,butstillkeepingλt,uas0.Ifweignorethedo-maindifference,theneachlabeledtargetinstanceisweightedthesameasalabeledsourceinstance(λu,lλs=Cu,lCs),whichiswhathappensinregularsu-pervisedlearning.However,basedonourtheoret-icalanalysis,wecanexpectthelabeledtargetin-stancestobemorerepresentativeofthetargetdo-mainthanthesourceinstances.Wecanthereforeassignhigherweightsforthetargetinstances,byad-justingtheratiobetweenλt,landλs.Inourexperi-ments,wesetλt,lλs=aCt,lCs,wherearangesfrom2to20.TheresultsareshowninTable2.Asshownfromthetable,addingsomelabeledtar-getinstancescangreatlyimprovetheperformanceforalltasks.Andinalmostallcases,weightingthetargetinstancesmorethanthesourceinstancesper-formedbetterthanweightingthemequally.Wealsotestedanothersettingwhereweﬁrstremovedthe“misleading”sourceexamplesasweshowedinSection4.2,andthenaddedthelabeledtargetinstances.TheresultsareshowninthelastrowofTable2.However,althoughbothremoving“misleading”sourceinstancesandaddinglabeled270

POSNETypeSpamkOncologykCTSkWLku00u01u0200.863000.781500.704500.63060.69500.764440000.86758000.82456000.70701500.64170.70780.795080000.870916000.864012000.69753000.66110.72280.8222120000.871324000.882518000.68304500.71060.78060.8239160000.871430000.882524000.67956000.79110.83220.8328all0.8720all0.8830all0.6600all0.81060.85170.8067Table1:Accuracyonthetargetdomainafterremoving“misleading”sourcedomaininstances.POSNETypeSpammethodOncologymethodCTSWLmethodu00u01u02Dsonly0.8630Dsonly0.78150.7045Dsonly0.63060.69500.7644Ds+Dt,l0.9349Ds+Dt,l0.93400.7735Ds+Dt,l0.95720.95720.9461Ds+5Dt,l0.9411Ds+2Dt,l0.93550.7810Ds+2Dt,l0.96060.96000.9533Ds+10Dt,l0.9429Ds+5Dt,l0.93600.7820Ds+5Dt,l0.9628096110.9601Ds+20Dt,l0.9443Ds+10Dt,l0.93550.7840Ds+10Dt,l0.96390.96280.9633D0s+20Dt,l0.9422D0s+10Dt,l0.89500.6670D0s+10Dt,l0.97170.94780.9494Table2:Accuracyontheunlabeledtargetinstancesafteraddingthelabeledtargetinstances.targetinstancesworkwellindividually,whencom-bined,theperformanceinmostcasesisnotasgoodaswhennosourceinstancesareremoved.Wehy-pothesizethatthisisbecauseafterweaddedsomelabeledtargetinstanceswithlargeweights,weal-readygainedagoodbalancebetweenthesourcedataandthetargetdata.Furtherremovingsourcein-stanceswouldpushtheemphasismoreonthesetoflabeledtargetinstances,whichisonlyabiasedsampleofthewholetargetdomain.ThePOSdatasetandtheCTSdatasethavepre-viouslybeenusedfortestingotheradaptationmeth-ods(Daum´eIIIandMarcu,2006;Blitzeretal.,2006),thoughthesetupthereisdifferentfromours.Ourperformanceusinginstanceweightingiscom-parabletotheirbestperformance(slightlyworseforPOSandbetterforCTS).4.4BootstrappingwithHigherWeightsInthethirdsetofexperiments,weassumethatwedonothaveanylabeledtargetinstances.Wetriedtwobootstrappingmethods.Theﬁrstisastandardbootstrappingmethod,inwhichwegraduallyaddedthemostconﬁdentlypredictedunlabeledtargetin-stanceswiththeirpredictedlabelstothetrainingset.Sincewebelievethatthetargetinstancesshouldingeneralbegivenmoreweightbecausetheybet-terrepresentthetargetdomainthanthesourcein-stances,inthesecondmethod,wegavetheaddedtargetinstancesmoreweightintheobjectivefunc-tion.Inparticular,wesetλt,u=λssuchthatthetotalcontributionoftheaddedtargetinstancesisequaltothatofallthelabeledsourceinstances.Wecallthissecondmethodthebalancedbootstrappingmethod.Table3showstheresults.Aswecansee,whilebootstrappingcangenerallyimprovetheperformanceoverthebaselinewherenounlabeleddataisused,thebalancedbootstrap-pingmethodperformedslightlybetterthanthestan-dardbootstrappingmethod.Thisagainshowsthatweightingthetargetinstancesmoreisarightdirec-tiontogofordomainadaptation.5RelatedWorkTherehavebeenseveralstudiesinNLPthataddressdomainadaptation,andmostofthemneedlabeleddatafromboththesourcedomainandthetargetdo-main.Herewehighlightafewrepresentativeones.Forgenerativesyntacticparsing,RoarkandBac-chiani(2003)haveusedthesourcedomaindatatoconstructaDirichletpriorforMAPestimationofthePCFGforthetargetdomain.ChelbaandAcero(2004)usetheparametersofthemaximumentropymodellearnedfromthesourcedomainasthemeansofaGaussianpriorwhentraininganewmodelonthetargetdata.Florianetal.(2004)ﬁrsttrainaNEtaggeronthesourcedomain,andthenusethetagger’spredictionsasfeaturesfortrainingandtestingonthetargetdomain.Theonlyworkweareawareofthatdirectlymod-271

POSNETypeSpammethodOncologyCTSWLu00u01u02supervised0.86300.77810.73510.64760.69760.8068standardbootstrap0.87280.89170.74980.87200.92120.9760balancedbootstrap0.87500.89230.75230.88160.92560.9772Table3:Accuracyonthetargetdomainwithoutusinglabeledtargetinstances.Inbalancedbootstrapping,moreweightsareputonthetargetinstancesintheobjectivefunctionthaninstandardbootstrapping.elsthedifferentdistributionsinthesourceandthetargetdomainsisbyDaum´eIIIandMarcu(2006).Theyassumea“trulysourcedomain”distribution,a“trulytargetdomain”distribution,anda“generaldomain”distribution.Thesource(target)domaindataisgeneratedfromamixtureofthe“trulysource(target)domain”distributionandthe“generaldo-main”distribution.Incontrast,wedonotassumesuchamixturemodel.Noneoftheabovemethodswouldworkiftherewerenolabeledtargetinstances.Indeed,alltheabovemethodsdonotmakeuseoftheunlabeledinstancesinthetargetdomain.Incontrast,ourin-stanceweightingframeworkallowsunlabeledtargetinstancestocontributetothemodelestimation.Blitzeretal.(2006)proposeadomainadaptationmethodthatusestheunlabeledtargetinstancestoinferagoodfeaturerepresentation,whichcanbere-gardedasweightingthefeatures.Incontrast,weweighttheinstances.Theideaofusingpt(x)ps(x)toweightinstanceshasbeenstudiedinstatistics(Shi-modaira,2000),buthasnotbeenappliedtoNLPtasks.6ConclusionsandFutureWorkDomainadaptationisaveryimportantproblemwithapplicationstomanyNLPtasks.Inthispaper,weformallyanalyzethedomainadaptationproblemandproposeageneralinstanceweightingframeworkfordomainadaptation.Theframeworkisﬂexibletosupportmanydifferentstrategiesforadaptation.Inparticular,itcansupportadaptationwithsometargetdomainlabeledinstancesaswellasthatwithoutanylabeledtargetinstances.ExperimentresultsonthreeNLPtasksshowthatwhileregularsemi-supervisedlearningmethodsandsupervisedlearningmethodscanbeappliedtodomainadaptationwithoutcon-sideringdomaindifference,theydonotperformaswellasournewmethod,whichexplicitlycapturesdomaindifference.Ourresultsalsoshowthatincor-poratingandexploitingmoreinformationfromthetargetdomainismuchmoreusefulthanexcludingmisleadingtrainingexamplesfromthesourcedo-main.Theframeworkopensupmanyinterestingfutureresearchdirections,especiallythoserelatedtohowtomoreaccuratelyset/estimatethoseweightingparameters.AcknowledgmentsThisworkwasinpartsupportedbytheNationalSci-enceFoundationunderawardnumbers0425852and0428472.Wethanktheanonymousreviewersfortheirvaluablecomments.ReferencesJohnBlitzer,RyanMcDonald,andFernandoPereira.2006.Domainadaptationwithstructuralcorrespon-dencelearning.InProc.ofEMNLP,pages120–128.CiprianChelbaandAlexAcero.2004.Adaptationofmaximumentropycapitalizer:Littledatacanhelpalot.InProc.ofEMNLP,pages285–292.HalDaum´eIIIandDanielMarcu.2006.Domainadapta-tionforstatisticalclassiﬁers.J.ArtiﬁcialIntelligenceRes.,26:101–126.R.Florian,H.Hassan,A.Ittycheriah,H.Jing,N.Kamb-hatla,X.Luo,N.Nicolov,andS.Roukos.2004.Astatisticalmodelformultilingualentitydetectionandtracking.InProc.ofHLT-NAACL,pages1–8.Y.GrandvaletandY.Bengio.2005.Semi-supervisedlearningbyentropyminimization.InNIPS.BrianRoarkandMichielBacchiani.2003.SupervisedandunsupervisedPCFGadaptatintonoveldomains.InProc.ofHLT-NAACL,pages126–133.HidetoshiShimodaira.2000.Improvingpredictivein-ferenceundercovariateshiftbyweightingthelog-likelihoodfunction.JournalofStatisticalPlanningandInference,90:227–244.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 272–279,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

272

TheInﬁniteTreeJennyRoseFinkel,TrondGrenager,andChristopherD.ManningComputerScienceDepartment,StanfordUniversityStanford,CA94305{jrfinkel,grenager,manning}@cs.stanford.eduAbstractHistorically,unsupervisedlearningtech-niqueshavelackedaprincipledtechniqueforselectingthenumberofunseencompo-nents.Researchintonon-parametricpriors,suchastheDirichletprocess,hasenabledin-steadtheuseofinﬁnitemodels,inwhichthenumberofhiddencategoriesisnotﬁxed,butcangrowwiththeamountoftrainingdata.Herewedeveloptheinﬁnitetree,anewinﬁ-nitemodelcapableofrepresentingrecursivebranchingstructureoveranarbitrarilylargesetofhiddencategories.Speciﬁcally,wedevelopthreeinﬁnitetreemodels,eachofwhichenforcesdifferentindependenceas-sumptions,andforeachmodelwedeﬁneasimpledirectassignmentsamplinginferenceprocedure.Wedemonstratetheutilityofourmodelsbydoingunsupervisedlearningofpart-of-speechtagsfromtreebankdepen-dencyskeletonstructure,achievinganaccu-racyof75.34%,andbydoingunsupervisedsplittingofpart-of-speechtags,whichin-creasestheaccuracyofagenerativedepen-dencyparserfrom85.11%to87.35%.1IntroductionModel-basedunsupervisedlearningtechniqueshavehistoricallylackedgoodmethodsforchoosingthenumberofunseencomponents.Forexample,k-meansorEMclusteringrequireadvancespeciﬁca-tionofthenumberofmixturecomponents.ButtheintroductionofnonparametricpriorssuchastheDirichletprocess(Ferguson,1973)enableddevelop-mentofinﬁnitemixturemodels,inwhichthenum-berofhiddencomponentsisnotﬁxed,butemergesnaturallyfromthetrainingdata(Antoniak,1974).Tehetal.(2006)proposedthehierarchicalDirich-letprocess(HDP)asawayofapplyingtheDirichletprocess(DP)tomorecomplexmodelforms,soastoallowmultiple,group-speciﬁc,inﬁnitemixturemod-elstosharetheirmixturecomponents.ThecloselyrelatedinﬁnitehiddenMarkovmodelisanHMMinwhichthetransitionsaremodeledusinganHDP,enablingunsupervisedlearningofsequencemodelswhenthenumberofhiddenstatesisunknown(Bealetal.,2002;Tehetal.,2006).Weextendthisworkbyintroducingtheinﬁnitetreemodel,whichrepresentsrecursivebranchingstructureoverapotentiallyinﬁnitesetofhiddenstates.Suchmodelsareappropriateforthesyntacticdependencystructureofnaturallanguage.Thehid-denstatesrepresentwordcategories(“tags”),theob-servationstheygeneraterepresentthewordsthem-selves,andthetreestructurerepresentssyntacticde-pendenciesbetweenpairsoftags.Tovalidatethemodel,wetestunsupervisedlearn-ingoftagsconditionedonagivendependencytreestructure.Thisisuseful,becausecoarse-grainedsyntacticcategories,suchasthoseusedinthePennTreebank(PTB),makeinsufﬁcientdistinctionstobethebasisofaccuratesyntacticparsing(Charniak,1996).Hence,state-of-the-artparserseithersupple-mentthepart-of-speech(POS)tagswiththelexicalformsthemselves(Collins,2003;Charniak,2000),manuallysplitthetagsetintoaﬁner-grainedone(KleinandManning,2003a),orlearnﬁnergrainedtagdistinctionsusingaheuristiclearningprocedure(Petrovetal.,2006).WedemonstratethatthetagslearnedwithourmodelarecorrelatedwiththePTBPOStags,andfurthermorethattheyimprovetheac-curacyofanautomaticparserwhenusedintraining.2FiniteTreesWebeginbypresentingthreeﬁnitetreemodels,eachwithdifferentindependenceassumptions.273

CρπkHφkz1z2z3x1x2x3Figure1:AgraphicalrepresentationoftheﬁniteBayesiantreemodelwithindependentchildren.Theplate(rectangle)indicatesthatthereisonecopyofthemodelparametervariablesforeachstatek≤C.2.1IndependentChildrenIntheﬁrstmodel,childrenaregeneratedindepen-dentlyofeachother,conditionedontheparent.Lettdenoteboththetreeanditsrootnode,c(t)thelistofchildrenoft,ci(t)theithchildoft,andp(t)theparentoft.Eachtreethasahiddenstatezt(inasyn-taxtree,thetag)andanobservationxt(theword).1Theprobabilityofatreeisgivenbytherecursivedeﬁnition:2Ptr(t)=P(xt|zt)Yt′∈c(t)P(zt′|zt)Ptr(t′)TomakethemodelBayesian,wemustdeﬁneran-domvariablestorepresenteachofthemodel’spa-rameters,andspecifypriordistributionsforthem.LeteachofthehiddenstatevariableshaveCpossi-blevalueswhichwewillindexwithk.Eachstatekhasadistinctdistributionoverobservations,param-eterizedbyφk,whichisdistributedaccordingtoapriordistributionovertheparametersH:φk|H∼HWegenerateeachobservationxtfromsomedistri-butionF(φzt)parameterizedbyφztspeciﬁctoitscorrespondinghiddenstatezt.IfF(φk)saremulti-nomials,thenanaturalchoiceforHwouldbeaDirichletdistribution.3Thehiddenstatezt′ofeachchildisdistributedaccordingtoamultinomialdistributionπztspeciﬁctothehiddenstateztoftheparent:xt|zt∼F(φzt)zt′|zt∼Multinomial(πzt)1Tomodellength,everychildlistendswithadistinguishedstopnode,whichhasasitsstateadistinguishedstopstate.2Wealsodeﬁneadistinguishednodet0,whichgeneratestherootoftheentiretree,andP(xt0|zt0)=1.3ADirichletdistributionisadistributionoverthepossibleparametersofamultinomialdistributions,andisdistinctfromtheDirichletprocess.Eachmultinomialoverchildrenπkisdistributedac-cordingtoaDirichletdistributionwithparameterρ:πk|ρ∼Dirichlet(ρ,...,ρ)ThismodelispresentedgraphicallyinFigure1.2.2SimultaneousChildrenTheindependentchildmodeladoptsstrongindepen-denceassumptions,andwemayinsteadwantmod-elsinwhichthechildrenareconditionedonmorethanjusttheparent’sstate.Oursecondmodelthusgeneratesthestatesofallofthechildrenc(t)simul-taneously:Ptr(t)=P(xt|zt)P((zt′)t′∈c(t)|zt)Yt′∈c(t)Ptr(t′)where(zt′)t′∈c(t)indicatesthelistoftagsofthechil-drenoft.Toparameterizethismodel,wereplacethemultinomialdistributionπkoverstateswithamulti-nomialdistributionλkoverlistsofstates.42.3MarkovChildrenTheverylargedomainsizeofthechildlistsinthesimultaneouschildmodelmaycauseproblemsofsparseestimation.Anotheralternativeistouseaﬁrst-orderMarkovprocesstogeneratechildren,inwhicheachchild’sstateisconditionedontheprevi-ouschild’sstate:Ptr(t)=P(xt|zt)Y|c(t)|i=1P(zci(t)|zci−1(t),zt)Ptr(t′)Forthismodel,weaugmentallchildlistswithadis-tinguishedstartnode,c0(t),whichhasasitsstateadistinguishedstartstate,allowingustocapturetheuniquebehavioroftheﬁrst(observed)child.Toparameterizethismodel,notethatwewillneedtodeﬁneC(C+1)multinomials,oneforeachparentstateandprecedingchildstate(oradistinguishedstartstate).3ToInﬁnity,andBeyond...Thissectionreviewsneededbackgroundmaterialforourapproachtomakingourtreemodelsinﬁnite.3.1TheDirichletProcessSupposewemodeladocumentasabagofwordsproducedbyamixturemodel,wherethemixturecomponentsmightbetopicssuchasbusiness,pol-itics,sports,etc.Usingthismodelwecangeneratea4Thisrequiresstipulatingamaximumlistlength.274

00.20.40.60.8100.20.40.60.81P(xi = "game")P(xi = "profit")Figure2:PlotofthedensityfunctionofaDirich-letdistributionH(thesurface)aswellasadrawG(theverticallines,orsticks)fromaDirichletprocessDP(α0,H)whichhasHasabasemea-sure.Bothdistributionsaredeﬁnedoverasim-plexinwhicheachpointcorrespondstoaparticularmultinomialdistributionoverthreepossiblewords:“proﬁt”,“game”,and“election”.TheplacementofthesticksisdrawnfromthedistributionH,andisindependentoftheirlengths,whichisdrawnfromastick-breakingprocesswithparameterα0.documentbyﬁrstgeneratingadistributionovertop-icsπ,andthenforeachpositioniinthedocument,generatingatopiczifromπ,andthenawordxifromthetopicspeciﬁcdistributionφzi.TheworddistributionsφkforeachtopickaredrawnfromabasedistributionH.InSection2,wesampleCmultinomialsφkfromH.Intheinﬁnitemixturemodelwesampleaninﬁnitenumberofmultinomi-alsfromH,usingtheDirichletprocess.Formally,givenabasedistributionHandacon-centrationparameterα0(looselyspeaking,thiscon-trolstherelativesizesofthetopics),aDirichletpro-cessDP(α0,H)isthedistributionofadiscreteran-domprobabilitymeasureGoverthesame(possiblycontinuous)spacethatHisdeﬁnedover;thusitisameasureovermeasures.InFigure2,thesticks(ver-ticallines)showadrawGfromaDirichletprocesswherethebasemeasureHisaDirichletdistributionover3words.Adrawcomprisesofaninﬁnitenum-berofsticks,andeachcorrespondingtopic.WefactorGintotwocoindexeddistributions:π,adistributionovertheintegers,wheretheintegerrepresentstheindexofaparticulartopic(i.e.,theheightofthesticksintheﬁgurerepresenttheproba-bilityofthetopicindexedbythatstick)andφ,rep-resentingtheworddistributionofeachofthetop-N∞α0Hπφkzixiπ|α0∼GEM(α0)φk|H∼Hzi|π∼πxi|zi,φ∼F(φzi)N∞γα0βHπjφkzjixji(a)(b)Figure3:AgraphicalrepresentationofasimpleDirichletprocessmixturemodel(left)andahierar-chicalDirichletprocessmodel(right).Notethatweshowthestick-breakingrepresentationsofthemod-els,inwhichwehavefactoredG∼DP(α0,H)intotwosetsofvariables:πandφ.ics(i.e.,thelocationofthesticksintheﬁgure).Togenerateπweﬁrstgenerateaninﬁnitesequenceofvariablesπ′=(π′k)∞k=1,eachofwhichisdistributedaccordingtotheBetadistribution:π′k|α0∼Beta(1,α0)Thenπ=(πk)∞k=1isdeﬁnedas:πk=π′kYk−1i=1(1−π′i)FollowingPitman(2002)werefertothisprocessasπ∼GEM(α0).ItshouldbenotedthatP∞k=1πk=1,5andP(i)=πi.Then,accordingtotheDP,P(φi)=πi.Thecompletemodel,isshowngraphi-callyinFigure3(a).Tobuildintuition,wewalkthroughtheprocessofgeneratingfromtheinﬁnitemixturemodelforthedocumentexample,wherexiisthewordatposi-tioni,andziisitstopic.Fisamultinomialdis-tributionparameterizedbyφ,andHisaDirichletdistribution.Insteadofgeneratingalloftheinﬁnitemixturecomponents(πk)∞k=1atonce,wecanbuildthemupincrementally.IfthereareKknowntop-ics,werepresentonlytheknownelements(πk)Kk=1andrepresenttheremainingprobabilitymassπu=5Thisiscalledthestick-breakingconstruction:westartwithastickofunitlength,representingtheentireprobabilitymass,andsuccessivelybreakbitsofftheendofthestick,wheretheproportionalamountbrokenoffisrepresentedbyπ′kandtheabsoluteamountisrepresentedbyπk.275

φ1φ2φ3φ4φ5φ6φ7...β:πj:...Figure4:Agraphicalrepresentationofπj,abrokenstick,whichisdistributedaccordingtoaDPwithabrokenstickβasabasemeasure.Eachβkcorre-spondstoaφk.1−(PKk=1πk).Initiallywehaveπu=1andφ=().Fortheithpositioninthedocument,weﬁrstdrawatopiczi∼π.Ifzi6=u,thenweﬁndthecoin-dexedtopicφzi.Ifzi=u,theunseentopic,wemakeadrawb∼Beta(1,α0)andsetπK+1=bπuandπnewu=(1−b)πu.Thenwedrawaparame-terφK+1∼Hforthenewtopic,resultinginπ=(π1,...,πK+1,πnewu)andφ=(φ1,...,φK+1).Awordisthendrawnfromthistopicandemittedbythedocument.3.2TheHierarchicalDirichletProcessLet’sgeneralizeourpreviousexampletoacorpusofdocuments.Asbefore,wehaveasetofsharedtopics,butnoweachdocumenthasitsowncharac-teristicdistributionoverthesetopics.Werepresenttopicdistributionsbothlocally(foreachdocument)andglobally(acrossalldocuments)byuseofahier-archicalDirichletprocess(HDP),whichhasalocalDPforeachdocument,inwhichthebasemeasureisitselfadrawfromanother,global,DP.ThecompleteHDPmodelisrepresentedgraphi-callyinFigure3(b).LiketheDP,ithasglobalbro-kenstickβ=(βk)∞k=1andtopicspeciﬁcworddis-tributionparametersφ=(φk)∞k=1,whicharecoin-dexed.ItdiffersfromtheDPinthatitalsohaslo-calbrokensticksπjforeachgroupj(inourcasedocuments).Whiletheglobalstickβ∼GEM(γ)isgeneratedasbefore,thelocalsticksπjaredis-tributedaccordingtoaDPwithbasemeasureβ:πj∼DP(α0,β).WeillustratethisgenerationprocessinFigure4.Theupperunitlinerepresentsβ,wherethesizeofsegmentkrepresentsthevalueofelementβk,andthelowerunitlinerepresentsπj∼DP(α0,β)foraparticulargroupj.Eachelementofthelowerstickwassampledfromaparticularelementoftheupperstick,andelementsoftheupperstickmaybesam-pledmultipletimesornotatall;onaverage,largerelementswillbesampledmoreoften.Eachelementβk,aswellasallelementsofπjthatweresampledfromit,correspondstoaparticularφk.Critically,severaldistinctπjcanbesampledfromthesameβkandhenceshareφk;thisishowcomponentsaresharedamonggroups.Forconcreteness,weshowhowtogenerateacor-pusofdocumentsfromtheHDP,generatingonedocumentatatime,andincrementallyconstruct-ingourinﬁniteobjects.Initiallywehaveβu=1,φ=(),andπju=1forallj.Westartwiththeﬁrstpositionoftheﬁrstdocumentanddrawalocaltopicy11∼π1,whichwillreturnuwithprobabil-ity1.Becausey11=uwemustmakeadrawfromthebasemeasure,β,which,becausethisistheﬁrstdocument,willalsoreturnuwithprobability1.Wemustnowbreakβuintoβ1andβnewu,andbreakπ1uintoπ11andπnew1uinthesamemannerpresentedfortheDP.Sinceπ11nowcorrespondstoglobaltopic1,wesamplethewordx11∼Multinomial(φ1).Tosampleeachsubsequentwordi,weﬁrstsamplethelocaltopicy1i∼π1.Ify1i6=u,andπ1y1icorre-spondstoβkintheglobalstick,thenwesamplethewordx1i∼Multinomial(φk).Oncetheﬁrstdocu-menthasbeensampled,subsequentdocumentsaresampledinasimilarmanner;initiallyπju=1fordocumentj,whileβcontinuestogrowasmoredoc-umentsaresampled.4InﬁniteTreesWenowusethetechniquesfromSection3tocreateinﬁniteversionsofeachtreemodelfromSection2.4.1IndependentChildrenThechangesrequiredtomaketheBayesianinde-pendentchildrenmodelinﬁnitedon’taffectitsba-sicstructure,ascanbewitnessedbycomparingthegraphicaldepictionoftheinﬁnitemodelinFigure5withthatoftheﬁnitemodelinFigure1.Thein-stancevariablesztandxtareparameterizedasbe-fore.Theprimarychangeisthatthenumberofcopiesofthestateplateisinﬁnite,asarethenumberofvariablesπkandφk.Notealsothateachdistributionoverpossiblechildstatesπkmustalsobeinﬁnite,sincethenum-berofpossiblechildstatesispotentiallyinﬁnite.Weachievethisbyrepresentingeachoftheπkvariablesasabrokenstick,andadoptthesameapproachof276

β|γ∼GEM(γ)πk|α0,β∼DP(α0,β)φk|H∼H∞γβα0πkHφkz1z2z3x1x2x3Figure5:Agraphicalrepresentationoftheinﬁniteindependentchildmodel.samplingeachπkfromaDPwithbasemeasureβ.Forthedependencytreeapplication,φkisavectorrepresentingtheparametersofamultinomialoverwords,andHisaDirichletdistribution.TheinﬁnitehiddenMarkovmodel(iHMM)orHDP-HMM(Bealetal.,2002;Tehetal.,2006)isamodelofsequencedatawithtransitionsmodeledbyanHDP.6TheiHMMcanbeviewedasaspecialcaseofthismodel,whereeachstate(exceptthestopstate)producesexactlyonechild.4.2SimultaneousChildrenThekeyprobleminthedeﬁnitionofthesimulta-neouschildrenmodelisthatofdeﬁningadistribu-tionoverthelistsofchildrenproducedbyeachstate,sinceeachchildinthelisthasasitsdomaintheposi-tiveintegers,representingtheinﬁnitesetofpossiblestates.OursolutionistoconstructadistributionLkoverlistsofstatesfromthedistributionoverindivid-ualstatesπk.Theobviousapproachistosamplethestatesateachpositioni.i.d.:P((zt′)t′∈c(t)|π)=Yt′∈c(t)P(zt′|π)=Yt′∈c(t)πzt′However,wewantourmodeltobeabletorep-resentthefactthatsomechildlists,ct,aremoreorlessprobablethantheproductoftheindividualchildprobabilitieswouldindicate.Toaddressthis,wecansampleastate-conditionaldistributionoverchildlistsλkfromaDPwithLkasabasemeasure.6TheoriginaliHMMpaper(Bealetal.,2002)predates,andwasthemotivationfor,theworkpresentedinTehetal.(2006),andistheoriginofthetermhierarchicalDirichletprocess.However,theyusedthetermtomeansomethingslightlydiffer-entthantheHDPpresentedinTehetal.(2006),andpresentedasamplingschemeforinferencethatwasaheuristicapproxima-tionofaGibbssampler.Thus,weaugmentthebasicmodelgiveninthepre-vioussectionwiththevariablesζ,Lk,andλk:Lk|πk∼Deterministic,asdescribedaboveλk|ζ,Lk∼DP(ζ,Lk)ct|λk∼λkAnimportantconsequenceofdeﬁningLklocally(insteadofglobally,usingβinsteadoftheπks)isthatthemodelcapturesnotonlywhatsequencesofchildrenastateprefers,butalsotheindividualchil-drenthatstateprefers;ifastategiveshighproba-bilitytosomeparticularsequenceofchildren,thenitislikelytoalsogivehighprobabilitytootherse-quencescontainingthosesamestates,orasubsetthereof.4.3MarkovChildrenIntheMarkovchildrenmodel,morecopiesofthevariableπareneeded,becauseeachchildstatemustbeconditionedbothontheparentstateandonthestateoftheprecedingchild.Weuseanewsetofvariablesπki,whereπisdeterminedbythepar-entstatekandthestateoftheprecedingsiblingi.Eachoftheπkiisdistributedasπkwasinthebasicmodel:πki∼DP(α0,β).5InferenceOurgoalininferenceistodrawasamplefromtheposterioroverassignmentsofstatestoobservations.WepresentaninferenceprocedurefortheinﬁnitetreethatisbasedonGibbssamplinginthedirectassignmentrepresentation,sonamedbecausewedi-rectlyassignglobalstateindicestoobservations.7Beforewepresenttheprocedure,wedeﬁneafewcountvariables.RecallfromFigure4thateachstatekhasalocalstickπk,eachelementofwhichcor-respondstoanelementofβ.Inoursamplingpro-cedure,weonlykeepelementsofπkandβwhichcorrespondtostatesobservedinthedata.Wedeﬁnethevariablemjktobethenumberofelementsoftheﬁniteobservedportionofπkwhichcorrespondtoβjandnjktobethenumberofobservationswithstatekwhoseparent’sstateisj.Wealsoneedafewmodel-speciﬁccounts.Forthesimultaneouschildrenmodelweneednjz,whichis7WeadaptoneofthesamplingschemesmentionedbyTehetal.(2006)foruseintheiHMM.Thispapersuggeststwosamplingschemesforinference,butdoesnotexplicitlypresentthem.Upondiscussionwithoneoftheauthors(Y.W.Teh,2006,p.c.),itbecameclearthatinferenceusingtheaugmentedrepresentationismuchmorecomplicatedthaninitiallythought.277

thenumberoftimesthestatesequencezoccurredasthechildrenofstatej.FortheMarkovchil-drenmodelweneedthecountvariableˆnjikwhichisthenumberofobservationsforanodewithstatekwhoseparent’sstateisjandwhoseprevioussib-ling’sstateisi.Inallcaseswerepresentmarginalcountsusingdot-notation,e.g.,n·kisthetotalnum-berofnodeswithstatek,regardlessofparent.Ourprocedurealternatesbetweenthreedistinctsamplingstages:(1)samplingthestateassignmentsz,(2)samplingthecountsmjk,and(3)samplingtheglobalstickβ.Theonlymodiﬁcationofthepro-cedurethatisrequiredforthedifferenttreemod-elsisthemethodforcomputingtheprobabilityofthechildstatesequencegiventheparentstateP((zt′)t′∈c(t)|zt),deﬁnedseparatelyforeachmodel.Samplingz.Inthisstagewesampleastateforeachtreenode.Theprobabilityofnodetbeingas-signedstatekisgivenby:P(zt=k|z−t,β)∝P(zt=k,(zt′)t′∈s(t)|zp(t))·P((zt′)t′∈c(t)|zt=k)·f−xtk(xt)wheres(t)denotesthesetofsiblingsoft,f−xtk(xt)denotestheposteriorprobabilityofobservationxtgivenallotherobservationsassignedtostatek,andz−tdenotesallstateassignmentsexceptzt.Inotherwords,theprobabilityisproportionaltotheproductofthreeterms:theprobabilityofthestatesoftanditssiblingsgivenitsparentzp(t),theprobabilityofthestatesofthechildrenc(t)givenzt,andthepos-teriorprobabilityofobservationxtgivenzt.Notethatifwesamplezttobeapreviouslyunseenstate,wewillneedtoextendβasdiscussedinSection3.2.NowwegivetheequationsforP((zt′)t′∈c(t)|zt)foreachofthemodels.Intheindependentchildmodeltheprobabilityofgeneratingeachchildis:Pind(zci(t)=k|zt=j)=njk+α0βknj·+α0Pind((zt′)t′∈c(t)|zt=j)=Yt′∈c(t)Pind(zt′|zt=j)Forthesimultaneouschildmodel,theprobabilityofgeneratingasequenceofchildren,z,takesintoac-counthowmanytimesthatsequencehasbeengen-erated,alongwiththelikelihoodofregeneratingit:Psim((zt′)t′∈c(t)=z|zt=j)=njz+ζPind(z|zt=j)nj·+ζRecallthatζdenotestheconcentrationparameterforthesequencegeneratingDP.Lastly,wehavetheDTNNINDTNNVBDPRP$NNTOVBNNEOSThemaninthecornertaughthisdachshundtoplaygolfEOSFigure6:Anexampleofasyntacticdependencytreewherethedependenciesarebetweentags(hiddenstates),andeachtaggeneratesaword(observation).Markovchildmodel:Pm(zci(t)=k|zci−1(t)=i,zt=j)=ˆnjik+α0βkˆnji·+α0Pm((zt′)t′∈c(t)|zt)=Y|c(t)|i=1Pm(zci(t)|zci−1(t),zt)Finally,wegivetheposteriorprobabilityofanob-servation,giventhatF(φk)isMultinomial(φk),andthatHisDirichlet(ρ,...,ρ).LetNbethevocab-ularysizeand˙nkbethenumberofobservationsxwithstatek.Then:f−xtk(xt)=˙nxtk+ρ˙n·k+NρSamplingm.Weusethefollowingprocedure,whichslightlymodiﬁesonefrom(Y.W.Teh,2006,p.c.),tosampleeachmjk:SAMPLEM(j,k)1ifnjk=02thenmjk=03elsemjk=14fori←2tonjk5doifrand()<α0α0+i−16thenmjk=mjk+17returnmjkSamplingβ.Lastly,wesampleβusingtheDi-richletdistribution:(β1,...,βK,βu)∼Dirichlet(m·1,...,m·K,α0)6ExperimentsWedemonstrateinﬁnitetreemodelsontwodis-tinctsyntaxlearningtasks:unsupervisedPOSlearn-ingconditionedonuntaggeddependencytreesandlearningasplitofanexistingtagset,whichimprovestheaccuracyofanautomaticsyntacticparser.Forbothtasks,weuseasimplemodiﬁcationofthebasicmodelstructure,toallowthetreestogen-eratedependentsontheleftandtherightwithdif-ferentdistributions–asisusefulinmodelingnatu-rallanguage.Themodiﬁcationoftheindependentchildtreeistrivial:wehavetwocopiesofeachof278

thevariablesπk,oneeachfortheleftandtheright.Generationofdependentsontherightiscompletelyindependentofthatfortheleft.Themodiﬁcationsoftheothermodelsaresimilar,butnowtherearesepa-ratesetsofπkvariablesfortheMarkovchildmodel,andseparateLkandλkvariablesforthesimultane-ouschildmodel,foreachoftheleftandright.Forbothexperiments,weuseddependencytreesextractedfromthePennTreebank(Marcusetal.,1993)usingtheheadrulesanddependencyextrac-torfromYamadaandMatsumoto(2003).Asisstan-dard,weusedWSJsections2–21fortraining,sec-tion22fordevelopment,andsection23fortesting.6.1UnsupervisedPOSLearningIntheﬁrstexperiment,wedounsupervisedpart-of-speechlearningconditionedondependencytrees.Tobeclear,theinputtoouralgorithmisthede-pendencystructureskeletonofthecorpus,butnotthePOStags,andtheoutputisalabelingofeachofthewordsinthetreeforwordclass.SincethemodelknowsnothingaboutthePOSannotation,thenewclasseshavearbitraryintegernames,andarenotguaranteedtocorrelatewiththePOStagdef-initions.Wefoundthatthechoiceofα0andβ(theconcentrationparameters)didnotaffecttheout-putmuch,whilethevalueofρ(theparameterforthebaseDirichletdistribution)madeamuchlargerdifference.Forallreportedexperiments,wesetα0=β=10andvariedρ.Weuseseveralmetricstoevaluatethewordclasses.First,weusethestandardapproachofgreedilyassigningeachofthelearnedclassestothePOStagwithwhichithasthegreatestoverlap,andthencomputingtaggingaccuracy(SmithandEisner,2005;HaghighiandKlein,2006).8Additionally,wecomputethemutualinformationofthelearnedclus-terswiththegoldtags,andwecomputetheclusterF-score(Ghosh,2003).SeeTable1forresultsofthedifferentmodels,parametersettings,andmet-rics.Giventhevarianceinthenumberofclasseslearneditisalittledifﬁculttointerprettheseresults,butitisclearthattheMarkovchildmodelisthebest;itachievessuperiorperformancetotheinde-pendentchildmodelonallmetrics,whilelearningfewerwordclasses.Thepoorperformanceofthesimultaneousmodelwarrantsfurtherinvestigation,butweobservedthatthedistributionslearnedbythat8Theadvantageofthismetricisthatit’scomprehensible.Thedisadvantageisthatit’seasytoinﬂatebyaddingclasses.Modelρ#ClassesAcc.MIF1Indep.0.0194367.892.0048.290.001174473.612.2340.800.0001243774.642.2739.47Simul.0.0118321.360.3121.570.00143015.770.0913.800.000154916.680.1214.29Markov0.0161368.532.1249.820.00189475.342.3148.73Table1:ResultsofpartunsupervisedPOStaggingonthedifferentmodels,usingagreedyaccuracymeasure.modelarefarmorespiked,potentiallyduetodoublecountingoftags,sincethesequenceprobabilitiesarealreadybasedonthelocalprobabilities.Forcomparison,HaghighiandKlein(2006)re-portanunsupervisedbaselineof41.3%,andabestresultof80.5%fromusinghand-labeledprototypesanddistributionalsimilarity.However,theytrainonlessdata,andlearnfewerwordclasses.6.2UnsupervisedPOSSplittingInthesecondexperimentweusetheinﬁnitetreemodelstolearnareﬁnementofthePTBtags.WeinitializethesetofhiddenstatestothesetofPTBtags,andthen,duringinference,constrainthesam-plingdistributionoverhiddenstateztateachnodettoincludeonlystatesthatareareﬁnementofthean-notatedPTBtagatthatposition.TheoutputofthistrainingprocedureisanewannotationofthewordsinthePTBwiththelearnedtags.WethencomparetheperformanceofagenerativedependencyparsertrainedonthenewreﬁnedtagswithonetrainedonthebasePTBtagset.Weusethegenerativede-pendencyparserdistributedwiththeStanfordfac-toredparser(KleinandManning,2003b)forthecomparison,sinceitperformssimultaneoustaggingandparsingduringtesting.Inthisexperiment,un-labeled,directed,dependencyparsingaccuracyforthebestmodelincreasedfrom85.11%to87.35%,a15%errorreduction.SeeTable2forthefullresultsoverallmodelsandparametersettings.7RelatedWorkTheHDP-PCFG(Liangetal.,2007),developedatthesametimeasthiswork,aimstolearnstatesplitsforabinary-branchingPCFG.Itissimilartooursimultaneouschildmodel,butwithseveralimpor-tantdistinctions.AsdiscussedinSection4.2,inourmodeleachstatehasaDPoversequences,withabasedistributionthatisdeﬁnedoverthelocalchild279

ModelρAccuracyBaseline–85.11Independent0.0186.180.00185.88Markov0.0187.150.00187.35Table2:Resultsofuntyped,directeddependencyparsing,wherethePOStagsinthetrainingdatahavebeensplitaccordingtothevariousmodels.Attesttime,thePOStaggingandparsingaredonesimulta-neouslybytheparser.stateprobabilities.Incontrast,Liangetal.(2007)deﬁneaglobalDPoversequences,withthebasemeasuredeﬁnedovertheglobalstateprobabilities,β;locally,eachstatehasanHDP,withthisglobalDPasthebasemeasure.Webelieveourchoicetobemorelinguisticallysensible:inourmodel,foraparticularstate,dependentsequenceswhicharesim-ilartooneanotherincreaseoneanother’slikelihood.Additionally,theirmodelingdecisionmadeitdifﬁ-culttodeﬁneaGibbssampler,andinsteadtheyusevariationalinference.Earlier,Johnsonetal.(2007)presentedadaptorgrammars,whichisaverysimi-larmodeltotheHDP-PCFG.Howevertheydidnotconﬁnethemselvestoabinarybranchingstructureandpresentedamoregeneralframeworkfordeﬁn-ingtheprocessforsplittingthestates.8DiscussionandFutureWorkWehavepresentedasetofnovelinﬁnitetreemodelsandassociatedinferencealgorithms,whicharesuit-ableforrepresentingsyntacticdependencystructure.Becausethemodelsrepresentapotentiallyinﬁnitenumberofhiddenstates,theypermitunsupervisedlearningalgorithmswhichnaturallyselectanum-berofwordclasses,ortags,basedonqualitiesofthedata.Althoughtheyrequiresubstantialtechni-calbackgroundtodevelop,thelearningalgorithmsbasedonthemodelsareactuallysimpleinform,re-quiringonlythemaintenanceofcounts,andthecon-structionofsamplingdistributionsbasedonthesecounts.Ourexperimentalresultsarepreliminarybutpromising:theydemonstratethatthemodeliscapa-bleofcapturingimportantsyntacticstructure.Muchremainstobedoneinapplyinginﬁnitemodelstolanguagestructure,andaninterestingex-tensionwouldbetodevelopinferencealgorithmsthatpermitcompletelyunsupervisedlearningofde-pendencystructure.AcknowledgmentsManythankstoYehWhyeTehforseveralenlight-eningconversations,andtothefollowingmem-bers(andhonorarymember)oftheStanfordNLPgroupforcommentsonanearlierdraft:ThadHughes,DavidHall,SurabhiGupta,AniNenkova,SebastianRiedel.ThisworkwassupportedbyaScottishEnterpriseEdinburgh-StanfordLinkgrant(R37588),aspartoftheEASIEproject,andbytheAdvancedResearchandDevelopmentActivity(ARDA)’sAdvancedQuestionAnsweringforIntel-ligence(AQUAINT)PhaseIIProgram.ReferencesC.E.Antoniak.1974.MixturesofDirichletprocesseswithap-plicationstoBayesiannonparametrics.AnnalsofStatistics,2:1152–1174.M.J.Beal,Z.Ghahramani,andC.E.Rasmussen.2002.TheinﬁnitehiddenMarkovmodel.InAdvancesinNeuralInfor-mationProcessingSystems,pages577–584.E.Charniak.1996.Tree-bankgrammars.InAAAI1996,pages1031–1036.E.Charniak.2000.Amaximum-entropy-inspiredparser.InHLT-NAACL2000,pages132–139.M.Collins.2003.Head-drivenstatisticalmodelsfornaturallan-guageparsing.ComputationalLinguistics,29(4):589–637.T.S.Ferguson.1973.ABayesiananalysisofsomenonpara-metricproblems.AnnalsofStatistics,1:209–230.J.Ghosh.2003.Scalableclusteringmethodsfordatamining.InN.Ye,editor,HandbookofDataMining,chapter10,pages247–277.LawrenceErlbaumAssoc.A.HaghighiandD.Klein.2006.Prototype-drivenlearningforsequencemodels.InHLT-NAACL2006.M.Johnson,T.Grifﬁths,andS.Goldwater.2007.Adaptorgrammars:Aframeworkforspecifyingcompositionalnon-parametricBayesianmodels.InNIPS2007.D.KleinandC.D.Manning.2003a.Accurateunlexicalizedparsing.InACL2003.D.KleinandC.D.Manning.2003b.FactoredA*searchformodelsoversequencesandtrees.InIJCAI2003.P.Liang,S.Petrov,D.Klein,andM.Jordan.2007.Nonpara-metricPCFGsusingDirichletprocesses.InEMNLP2007.M.P.Marcus,B.Santorini,andM.A.Marcinkiewicz.1993.BuildingalargeannotatedcorpusofEnglish:ThePennTreebank.ComputationalLinguistics,19(2):313–330.S.Petrov,L.Barrett,R.Thibaux,andD.Klein.2006.Learningaccurate,compact,andinterpretabletreeannotation.InACL44/COLING21,pages433–440.J.Pitman.2002.Poisson-DirichletandGEMinvariantdistribu-tionsforsplit-and-mergetransformationsofanintervalpar-tition.Combinatorics,ProbabilityandComputing,11:501–514.N.A.SmithandJ.Eisner.2005.Contrastiveestimation:Train-inglog-linearmodelsonunlabeleddata.InACL2005.Y.W.Teh,M.I.Jordan,M.J.Beal,andD.M.Blei.2006.Hier-archicalDirichletprocesses.JournaloftheAmericanStatis-ticalAssociation,101:1566–1581.H.YamadaandY.Matsumoto.2003.Statisticaldependencyanalysiswithsupportvectormachines.InProceedingsofIWPT,pages195–206.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 280–287,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

280

GuidingSemi-SupervisionwithConstraint-DrivenLearningMing-WeiChangLevRatinovDanRothDepartmentofComputerScienceUniversityofIllinoisatUrbana-ChampaignUrbana,IL61801{mchang21,ratinov2,danr}@uiuc.eduAbstractOverthelastfewyears,twoofthemainresearchdirectionsinmachinelearningofnaturallanguageprocessinghavebeenthestudyofsemi-supervisedlearningalgo-rithmsasawaytotrainclassi(cid:2)erswhenthelabeleddataisscarce,andthestudyofwaystoexploitknowledgeandglobalinformationinstructuredlearningtasks.Inthispaper,wesuggestamethodforincorporatingdo-mainknowledgeinsemi-supervisedlearn-ingalgorithms.Ournovelframeworkuni(cid:2)esandcanexploitseveralkindsoftaskspeci(cid:2)cconstraints.Theexperimentalresultspre-sentedintheinformationextractiondomaindemonstratethatapplyingconstraintshelpsthemodeltogeneratebetterfeedbackduringlearning,andhencetheframeworkallowsforhighperformancelearningwithsignif-icantlylesstrainingdatathanwaspossiblebeforeonthesetasks.1IntroductionNaturalLanguageProcessing(NLP)systemstypi-callyrequirelargeamountsofknowledgetoachievegoodperformance.Acquiringlabeleddataisadif-(cid:2)cultandexpensivetask.Therefore,anincreasingattentionhasbeenrecentlygiventosemi-supervisedlearning,wherelargeamountsofunlabeleddataareusedtoimprovethemodelslearnedfromasmalltrainingset(CollinsandSinger,1999;ThelenandRiloff,2002).Thehopeisthatsemi-supervisedorevenunsupervisedapproaches,whengivenenoughknowledgeaboutthestructureoftheproblem,willbecompetitivewiththesupervisedmodelstrainedonlargetrainingsets.However,inthegeneralcase,semi-supervisedapproachesgivemixedre-sults,andsometimesevendegradethemodelper-formance(Nigametal.,2000).Inmanycases,im-provingsemi-supervisedmodelswasdonebyseed-ingthesemodelswithdomaininformationtakenfromdictionariesorontology(CohenandSarawagi,2004;CollinsandSinger,1999;HaghighiandKlein,2006;ThelenandRiloff,2002).Ontheotherhand,inthesupervisedsetting,ithasbeenshownthatincorporatingdomainandproblemspeci(cid:2)cstruc-turedinformationcanresultinsubstantialimprove-ments(Toutanovaetal.,2005;RothandYih,2005).Thispaperproposesanovelconstraints-basedlearningprotocolforguidingsemi-supervisedlearn-ing.Wedevelopaformalismforconstraints-basedlearningthatuni(cid:2)esseveralkindsofconstraints:unary,dictionarybasedandn-aryconstraints,whichencodestructuralinformationandinterdependenciesamongpossiblelabels.Oneadvantageofourfor-malismisthatitallowscapturingdifferentlevelsofconstraintviolation.Ourprotocolcanbeusedinthepresenceofanylearningmodel,includingthosethatacquireadditionalstatisticalconstraintsfromobserveddatawhilelearning(seeSection5.IntheexperimentalpartofthispaperweuseHMMsastheunderlyingmodel,andexhibitsigni(cid:2)cantreductioninthenumberoftrainingexamplesrequiredintwoinformationextractionproblems.Asisoftenthecaseinsemi-supervisedlearning,thealgorithmcanbeviewedasaprocessthatim-provesthemodelbygeneratingfeedbackthrough281

labelingunlabeledexamples.Ouralgorithmpushesthisintuitionfurther,inthattheuseofconstraintsallowsustobetterexploitdomaininformationasawaytolabel,alongwiththecurrentlearnedmodel,unlabeledexamples.Givenasmallamountofla-beleddataandalargeunlabeledpool,ourframe-workinitializesthemodelwiththelabeleddataandthenrepeatedly:(1)Usesconstraintsandthelearnedmodeltolabeltheinstancesinthepool.(2)Updatesthemodelbynewlylabeleddata.Thisway,wecangeneratebetter(cid:147)training(cid:148)ex-amplesduringthesemi-supervisedlearningprocess.Thecoreofourapproach,(1),isdescribedinSec-tion5.ThetaskisdescribedinSection3andtheExperimentalstudyinSection6.Itisshowntherethattheimprovementonthetrainingexamplesviatheconstraintsindeedbooststhelearnedmodelandtheproposedmethodsigni(cid:2)cantlyoutperformsthetraditionalsemi-supervisedframework.2RelatedWorkInthesemi-superviseddomaintherearetwomainapproachesforinjectingdomainspeci(cid:2)cknowledge.Oneisusingthepriorknowledgetoaccuratelytailorthegenerativemodelsothatitcapturesthedomainstructure.Forexample,(Grenageretal.,2005)pro-posesDiagonalTransitionModelsforsequentialla-belingtaskswhereneighboringwordstendtohavethesamelabels.ThisisdonebyconstrainingtheHMMtransitionmatrix,whichcanbedonealsoforothermodels,suchasCRF.However(RothandYih,2005)showedthatreasoningwithmoreexpressive,non-sequentialconstraintscanimprovetheperfor-manceforthesupervisedprotocol.Asecondapproachhasbeentouseasmallhigh-accuracysetoflabeledtokensasawaytoseedandbootstrapthesemi-supervisedlearning.Thiswasused,forexample,by(ThelenandRiloff,2002;CollinsandSinger,1999)ininformationextraction,andby(SmithandEisner,2005)inPOStagging.(HaghighiandKlein,2006)extendsthedictionary-basedapproachtosequentiallabelingtasksbyprop-agatingtheinformationgivenintheseedswithcon-textualwordsimilarity.Thisfollowsaconceptuallysimilarapproachby(CohenandSarawagi,2004)thatusesalargenamed-entitydictionary,wherethesimilaritybetweenthecandidatenamed-entityanditsmatchingprototypeinthedictionaryisencodedasafeatureinasupervisedclassi(cid:2)er.Inourframework,dictionarylookupapproachesareviewedasunaryconstraintsontheoutputstates.Weextendthesekindsofconstraintsandallowformoregeneral,n-aryconstraints.Inthesupervisedlearningsettingithasbeenes-tablishedthatincorporatingglobalinformationcansigni(cid:2)cantlyimproveperformanceonseveralNLPtasks,includinginformationextractionandsemanticrolelabeling.(Punyakanoketal.,2005;Toutanovaetal.,2005;RothandYih,2005).Ourformalismismostrelatedtothislastwork.But,wedevelopasemi-supervisedlearningprotocolbasedonthisfor-malism.Wealsomakeuseofsoftconstraintsand,furthermore,extendthenotionofsoftconstraintstoaccountformultiplelevelsofconstraints’violation.Conceptually,althoughnottechnically,themostre-latedworktooursis(Shenetal.,2005)that,inasomewhatad-hocmannerusessoftconstraintstoguideanunsupervisedmodelthatwascraftedformentiontracking.Tothebestofourknowledge,wearethe(cid:2)rsttosuggestageneralsemi-supervisedprotocolthatisdrivenbysoftconstraints.Weproposelearningwithconstraints-aframe-workthatcombinestheapproachesdescribedaboveinauni(cid:2)edandintuitiveway.3Tasks,ExamplesandDatasetsInSection4wewilldevelopageneralframeworkforsemi-supervisedlearningwithconstraints.How-ever,itisusefultoillustratetheideasonconcreteproblems.Therefore,inthissection,wegiveabriefintroductiontothetwodomainsonwhichwetestedouralgorithms.Westudytwoinformationextrac-tionproblemsineachofwhich,giventext,asetofpre-de(cid:2)ned(cid:2)eldsistobeidenti(cid:2)ed.Sincethe(cid:2)eldsaretypicallyrelatedandinterdependent,thesekindsofapplicationsprovideagoodtestcaseforanap-proachlikeours.1The(cid:2)rsttaskistoidentify(cid:2)eldsfromcitations(McCallumetal.,2000).Thedataoriginallyin-cluded500labeledreferences,andwaslaterex-tendedwith5,000unannotatedcitationscollectedfrompapersfoundontheInternet(Grenageretal.,2005).Givenacitation,thetaskistoextractthe1Thedataforbothproblemsisavailableat:http://www.stanford.edu/grenager/data/unsupie.tgz282

(a)[AUTHORLarsOleAndersen.][TITLEProgramanalysisandspecializationfortheCprogramminglanguage.][TECH-REPORTPhDthesis,][INSTITUTIONDIKU,UniversityofCopenhagen,][DATEMay1994.](b)[AUTHORLarsOleAndersen.Programanalysisand][TITLEspecializationforthe][EDITORC][BOOKTITLEProgramminglanguage][TECH-REPORT.PhDthesis,][INSTITUTIONDIKU,UniversityofCopenhagen,May][DATE1994.]Figure1:ErroranalysisofaHMMmodel.Thelabelsareannotatedbyunderlineandaretotherightofeachopenbracket.Thecorrectassignmentwasshownin(a).Whilethepredictedlabelassignment(b)isgenerallycoherent,someconstraintsareviolated.Mostobviously,punctuationmarksareignoredascuesforstatetransitions.Theconstraint(cid:147)Fieldscannotendwithstopwords(suchas(cid:147)the(cid:148))(cid:148)maybealsogood.(cid:2)eldsthatappearinthegivenreference.SeeFig.1.Thereare13possible(cid:2)eldsincludingauthor,title,location,etc.Togainaninsighttohowtheconstraintscanguidesemi-supervisedlearning,assumethatthesentenceshowninFigure1appearsintheunlabeleddatapool.Part(a)ofthe(cid:2)gureshowsthecorrectla-beledassignmentandpart(b)showstheassignmentlabeledbyaHMMtrainedon30labels.However,ifweapplytheconstraintthatstatetransitioncanoccuronlyonpunctuationmarks,thesameHMMmodelparameterswillresultinthecorrectlabeling(a).Therefore,byaddingtheimprovedlabeledas-signmentwecangeneratebettertrainingsamplesduringsemi-supervisedlearning.Infact,thepunc-tuationmarksareonlysomeoftheconstraintsthatcanbeappliedtothisproblem.ThesetofconstraintsweusedinourexperimentsappearsinTable1.Notethatsomeoftheconstraintsarenon-localandareveryintuitiveforpeople,yetitisverydif(cid:2)culttoinjectthisknowledgeintomostmodels.Thesecondproblemweconsiderisextracting(cid:2)eldsfromadvertisements(Grenageretal.,2005).Thedatasetconsistsof8,767advertisementsforapartmentrentalsintheSanFranciscoBayAreadownloadedinJune2004fromtheCraigslistweb-site.Inthedataset,only302entrieshavebeenla-beledwith12(cid:2)elds,includingsize,rent,neighbor-hood,features,andsoon.Thedatawasprepro-cessedusingregularexpressionsforphonenumbers,emailaddressesandURLs.Thelistofthecon-straintsforthisdomainisgiveninTable1.Weim-plementsomeglobalconstraintsandincludeunaryconstraintswhichwerelargelyimportedfromthelistofseedwordsusedin(HaghighiandKlein,2006).Weslightlymodi(cid:2)edtheseedwordsduetodifferenceinpreprocessing.4NotationandDe(cid:2)nitionsConsiderastructuredclassi(cid:2)cationproblem,wheregivenaninputsequencex=(x1,...,xN),thetaskisto(cid:2)ndthebestassignmenttotheoutputvariablesy=(y1,...,yM).WedenoteXtobethespaceofthepossibleinputsequencesandYtobethesetofpossibleoutputsequences.Wede(cid:2)neastructuredoutputclassi(cid:2)erasafunc-tionh:X!Ythatusesaglobalscoringfunctionf:X(cid:2)Y!Rtoassignscorestoeachpossiblein-put/outputpair.Givenaninputx,adesiredfunctionfwillassignthecorrectoutputythehighestscoreamongallthepossibleoutputs.Theglobalscoringfunctionisoftendecomposedasaweightedsumoffeaturefunctions,f(x,y)=MXi=1λifi(x,y)=λ(cid:1)F(x,y).ThisdecompositionappliesbothtodiscriminativelinearmodelsandtogenerativemodelssuchasHMMsandCRFs,inwhichcasethelinearsumcorrespondstologlikelihoodassignedtothein-put/outputpairbythemodel(fordetailssee(Roth,1999)fortheclassi(cid:2)cationcaseand(Collins,2002)forthestructuredcase).Evenwhennotdictatedbythemodel,thefeaturefunctionsfi(x,y)usedarelocaltoallowinferencetractability.Localfeaturefunctioncancapturesomecontextforeachinputoroutputvariable,yetitisverylimitedtoallowdy-namicprogrammingdecodingduringinference.Now,considerascenariowherewehaveasetofconstraintsC1,...,CK.Wede(cid:2)neaconstraintC:X(cid:2)Y!f0,1gasafunctionthatindicateswhethertheinput/outputsequenceviolatessomede-siredproperties.Whentheconstraintsarehard,thesolutionisgivenbyargmaxy∈1C(x)λ(cid:1)F(x,y),283

(a)-Citations1)Each(cid:2)eldmustbeaconsecutivelistofwords,andcanappearatmostonceinacitation.2)Statetransitionsmustoccuronpunctuationmarks.3)Thecitationcanonlystartwithauthororeditor.4)Thewordspp.,pagescorrespondtoPAGE.5)Fourdigitsstartingwith20xxand19xxareDATE.6)Quotationscanappearonlyintitles.7)Thewordsnote,submitted,appearareNOTE.8)ThewordsCA,Australia,NYareLOCATION.9)Thewordstech,technicalareTECHREPORT.10)Thewordsproc,journal,proceedings,ACMareJOUR-NALorBOOKTITLE.11)Thewordsed,editorscorrespondtoEDITOR.(b)-Advertisements1)Statetransitionscanoccuronlyonpunctuationmarksorthenewlinesymbol.2)Each(cid:2)eldmustbeatleast3wordslong.3)Thewordslaundry,kitchen,parkingareFEATURES.4)Thewordssq,ft,bdrmareSIZE.5)Theword$,*MONEY*areRENT.6)Thewordsclose,near,shoppingareNEIGHBORHOOD.7)Thewordslaundrykitchen,parkingareFEATURES.8)The(normalized)wordsphone,emailareCONTACT.9)Thewordsimmediately,begin,cheaperareAVAILABLE.10)Thewordsroommates,respectful,dramaareROOM-MATES.11)Thewordssmoking,dogs,catsareRESTRICTIONS.12)Thewordhttp,image,linkarePHOTOS.13)Thewordsaddress,carlmont,st,crossareADDRESS.14)Thewordsutilities,pays,electricityareUTILITIES.Table1:Thelistofconstraintsforextracting(cid:2)eldsfromcitationsandadvertisements.Someconstraints(representedinthe(cid:2)rstblockofeachdomain)areglobalandarerelativelydif(cid:2)culttoinjectintotradi-tionalmodels.Whilealltheconstraintsholdforthevastmajorityofthedata,someofthemareviolatedbysomecorrectlabeledassignments.where1C(x)isasubsetofYforwhichallCias-signthevalue1forthegiven(x,y).Whentheconstraintsaresoft,wewanttoin-cursomepenaltyfortheirviolation.Moreover,wewanttoincorporateintoourcostfunctionamea-surefortheamountofviolationincurredbyvi-olatingtheconstraint.Agenericwaytocapturethisintuitionistointroduceadistancefunctiond(y,1Ci(x))betweenthespaceofoutputsthatre-specttheconstraint,1Ci(x),andthegivenoutputse-quencey.Onepossiblewaytoimplementthisdis-tancefunctionisastheminimalHammingdistancetoasequencethatrespectstheconstraintCi,thatis:d(y,1Ci(x))=min(y0∈1C(x))H(y,y0).IfthepenaltyforviolatingthesoftconstraintCiisρi,wewritethescorefunctionas:argmaxyλ(cid:1)F(x,y)(cid:0)KXi=1ρid(y,1Ci(x))(1)Werefertod(y,1C(x))asthevaluationoftheconstraintCon(x,y).Theintuitionbehind(1)isasfollows.Insteadofmerelymaximizingthemodel’slikelihood,wealsowanttobiasthemodelusingsomeknowledge.The(cid:2)rsttermof(1)isusedtolearnfromdata.Thesecondtermbiasesthemodebyusingtheknowledgeencodedintheconstraints.Notethatwedonotnormalizeourobjectivefunctiontobeatrueprobabilitydistribution.5LearningandInferencewithConstraintsInthissectionwepresentanewconstraint-drivenlearningalgorithm(CODL)forusingconstraintstoguidesemi-supervisedlearning.Thetaskistolearntheparametervectorλbyusingthenewobjectivefunction(1).Whileourformulationallowsustotrainalsothecoef(cid:2)cientsoftheconstraintsvalua-tion,ρi,wechoosenottodoit,sinceweviewthisasawaytobias(orenforce)thepriorknowledgeintothelearnedmodel,ratherthanallowingthedatatobrushitaway.Ourexperimentsdemonstratethattheproposedapproachisrobusttoinaccurateapproxi-mationofthepriorknowledge(assigningthesamepenaltytoalltheρi).Wenotethatinthepresenceofconstraints,theinferenceprocedure(for(cid:2)ndingtheoutputythatmaximizesthecostfunction)isusuallydonewithsearchtechniques(ratherthanViterbidecoding,see(Toutanovaetal.,2005;RothandYih,2005)foradiscussion),wechosebeamsearchdecoding.Thesemi-supervisedlearningwithconstraintsisdonewithanEM-likeprocedure.Weinitializethemodelwithtraditionalsupervisedlearning(ignoringtheconstraints)onasmalllabeledset.Givenanun-labeledsetU,intheestimationstep,thetraditionalEMalgorithmassignsadistributionoverlabeledas-signmentsYofeachx2U,andinthemaximizationstep,thesetofmodelparametersislearnedfromthedistributionsassignedintheestimationstep.However,inthepresenceofconstraints,assigningthecompletedistributionsintheestimationstepisinfeasiblesincetheconstraintsreshapethedistribu-tioninanarbitraryway.Asinexistingmethodsfortrainingamodelbymaximizingalinearcostfunc-tion(maximizelikelihoodordiscriminativemaxi-284

mization),thedistributionoverYisrepresentedasthesetofscoresassignedtoit;ratherthanconsid-eringthescoreassignedtoally0s,wetruncatethedistributiontothetopKassignmentsasreturnedbythesearch.GivenasetofKtopassignmentsy1,...,yK,weapproximatetheestimationstepbyassigninguniformprobabilitytothetopKcandi-dates,andzerototheotheroutputsequences.Wedenotethisalgorithmtop-KhardEM.Inthispa-per,weusebeamsearchtogenerateKcandidatesaccordingto(1).OurtrainingalgorithmissummarizedinFigure2.Severalthingsaboutthealgorithmshouldbeclari-(cid:2)ed:theTop-K-Inferenceprocedureinline7,thelearningprocedureinline9,andthenewparameterestimationinline9.TheTop-K-InferenceisaprocedurethatreturnstheKlabeledassignmentsthatmaximizethenewobjectivefunction(1).Inourcaseweusedthetop-Kelementsinthebeam,butthiscouldbeappliedtoanyotherinferenceprocedure.Thefactthattheconstraintsareusedintheinferenceprocedure(inparticular,forgeneratingnewtrainingexamples)al-lowsustousealearningalgorithmthatignorestheconstraints,whichisalotmoreef(cid:2)cient(althoughalgorithmsthatdotaketheconstraintsintoaccountcanbeusedtoo).Weusedmaximumlikelihoodes-timationofλbut,ingeneral,perceptronorquasi-Newtoncanalsobeused.Itisknownthattraditionalsemi-supervisedtrain-ingcandegradethelearnedmodel’sperformance.(Nigametal.,2000)hassuggestedtobalancethecontributionoflabeledandunlabeleddatatothepa-rameters.Theintuitionisthatwheniterativelyesti-matingtheparameterswithEM,wedisallowthepa-rameterstodrifttoofarfromthesupervisedmodel.Theparameterre-estimationinline9,usesasimilarintuition,butinsteadofweightingdatainstances,weintroducedasmoothingparameterγwhichcontrolstheconvexcombinationofmodelsinducedbythela-beledandtheunlabeleddata.UnlikethetechniquementionedabovewhichfocusesonnaiveBayes,ourmethodallowsustoweightlinearmodelsgeneratedbydifferentlearningalgorithms.Anotherwaytolookthealgorithmisfromtheself-trainingperspective(McCloskyetal.,2006).Similarlytoself-training,weusethecurrentmodeltogeneratenewtrainingexamplesfromtheunla-Input:Cycles:learningcyclesTr={x,y}:labeledtrainingset.U:unlabeleddatasetF:setoffeaturefunctions.{ρi}:setofpenalties.{Ci}:setofconstraints.γ:balancingparameterwiththesupervisedmodel.learn(Tr,F):supervisedlearningalgorithmTop-K-Inference:returnstop-Klabeledscoredbythecostfunction(1)CODL:1.Initialize(cid:21)0=learn(Tr,F).2.(cid:21)=(cid:21)0.3.ForCyclesiterationsdo:4.T=φ5.Foreachx∈U6.{(x,y1),...,(x,yK)}=7.Top-K-Inference(x,(cid:21),F,{Ci},{ρi})8.T=T∪{(x,y1),...,(x,yK)}9.(cid:21)=γ(cid:21)0+(1−γ)learn(T,F)Figure2:COnstraintDrivenLearning(CODL).InTop-K-Inference,weusebeamsearchto(cid:2)ndtheK-bestsolutionaccordingtoEq.(1).beledset.However,therearetwoimportantdiffer-ences.Oneisthatinself-training,onceanunlabeledsamplewaslabeled,itisneverlabeledagain.Inourcaseallthesamplesarerelabeledineachiter-ation.Inself-trainingitisoftenthecasethatonlyhigh-con(cid:2)dencesamplesareaddedtothelabeleddatapool.Whileweincludeallthesamplesinthetrainingpool,wecouldalsolimitourselvestothehigh-con(cid:2)dencesamples.TheseconddifferenceisthateachunlabeledexamplegeneratesKlabeledin-stances.Thecaseofoneiterationoftop-1hardEMisequivalenttoselftraining,wherealltheunlabeledsamplesareaddedtothelabeledpool.Thereareseveralpossiblebene(cid:2)tstousingK>1samples.(1)IteffectivelyincreasesthetrainingsetbyafactorofK(albeitbysomewhatnoisyexam-ples).Inthestructuredscenario,eachofthetop-Kassignmentsislikelytohavesomegoodcomponentssogeneratingtop-Kassignmentshelpsleveragingthenoise.(2)Givenanassignmentthatdoesnotsat-isfysomeconstraints,usingtop-Kallowsformul-tiplewaystocorrectit.Forexample,considertheoutput11101000withtheconstraintthatitshouldbelongtothelanguage1∗0∗.Ifthetwotopscoringcorrectionsare11111000and11100000,consider-ingonlyoneofthosecannegativelybiasthemodel.285

6ExperimentsandResultsInthissection,wepresentempiricalresultsofouralgorithmsontwodomains:citationsandadver-tisements.Bothproblemsaremodeledwithasim-pletoken-basedHMM.Westressthattoken-basedHMMcannotrepresentmanyofourconstraints.Thefunctiond(y,1C(x))usedisanapproximationofaHammingdistancefunction,discussedinSection7.Forbothdomains,andalltheexperiments,γwassetto0.1.Theconstraintsviolationpenaltyρissetto(cid:0)log10−4and(cid:0)log10−1forcitationsandad-vertisements,resp.2Notethatallconstraintssharethesamepenalty.Thenumberofsemi-supervisedtrainingcycles(line3ofFigure2)wassetto5.TheconstraintsforthetwodomainsarelistedinTable1.Wetrainedmodelsontrainingsetsofsizevary-ingfrom5to300forthecitationsandfrom5to100fortheadvertisements.Additionally,inallthesemi-supervisedexperiments,1000unlabeledexam-plesareused.Wereporttoken-based3accuracyon100held-outexamples(whichdonotoverlapneitherwiththetrainingnorwiththeunlabeleddata).Weran5experimentsineachsetting,randomlychoos-ingthetrainingset.Theresultsreportedbelowaretheaveragesoverthese5runs.Toverifyourclaimsweimplementedseveralbaselines.The(cid:2)rstbaselineisthesupervisedlearn-ingprotocoldenotedbysup.Thesecondbaselinewasatraditionaltop-1HardEMalsoknownastruncatedEM4(denotedbyHforHard).Inthethirdbaseline,denotedH&W,webalancedtheweightofthesupervisedandunsupervisedmodelsasde-scribedinline9ofFigure2.Wecomparethesebase-linestoourproposedprotocol,H&W&C,whereweaddedtheconstraintstoguidetheH&Wprotocol.Weexperimentedwithtwo(cid:3)avorsofthealgorithm:thetop-1andthetop-Kversion.Inthetop-Kver-sion,thealgorithmusesK-bestpredictions(K=50)foreachinstanceinordertoupdatethemodelasde-scribedinFigure2.TheexperimentalresultsforbothdomainsareingivenTable2.Ashypothesized,hardEMsometimes2Theguidingintuitionisthat(cid:21)F(x,y)correspondstoalog-likelihoodofaHMMmodelandρtoacrudeestimationofthelogprobabilitythataconstraintdoesnothold.ρwastunedonadevelopmentsetandkept(cid:2)xedinallexperiments.3Eachtoken(wordorpunctuationmark)isassignedastate.4Wealsoexperimentedwith(soft)EMwithoutconstraints,buttheresultsweregenerallyworse.(a)-CitationsNInf.sup.HH&WH&W&CH&W&C(Top-1)(Top-K)5noI55.160.963.670.671.0I66.669.072.576.077.810noI64.666.869.876.576.7I78.178.181.083.483.815noI68.770.673.778.679.4I81.381.984.185.586.220noI70.172.475.079.679.4I81.182.484.086.186.125noI72.773.277.081.682.0I84.384.286.287.487.6300noI86.180.787.188.288.2I92.589.693.493.693.5(b)-AdvertisementsNInf.sup.HH&WH&W&CH&W&C(Top-1)(Top-K)5noI55.261.860.566.066.0I59.465.263.669.369.610noI61.669.267.070.870.9I66.673.271.674.774.715noI66.371.770.173.073.0I70.475.674.576.676.920noI68.172.872.074.574.6I71.976.775.777.978.125noI70.073.873.074.974.8I73.777.776.678.478.5100noI76.376.277.678.578.6I80.480.581.281.881.7Table2:Experimentalresultsforextracting(cid:2)eldsfromcitationsandadvertisements.Nisthenumberoflabeledsamples.Histhetraditionalhard-EMandH&Wweighslabeledandunlabeleddataasmen-tionedinSec.5.OurproposedmodelisH&W&C,whichusesconstraintsinthelearningprocedure.Ireferstousingconstraintsduringinferenceateval-uationtime.Notethataddingconstraintsimprovestheaccuracyduringbothlearningandinference.degradetheperformance.Indeed,with300labeledexamplesinthecitationsdomain,theperformancedecreasesfrom86.1to80.7.Theusefulnessofin-jectingconstraintsinsemi-supervisedlearningisex-hibitedinthetworightmostcolumns:usingcon-straintsH&W&CimprovestheperformanceoverH&Wquitesigni(cid:2)cantly.Wecarefullyexaminedthecontributionofus-ingconstraintstothelearningstageandthetestingstage,andtwoseparateresultsarepresented:test-ingwithconstraints(denotedIforinference)andwithoutconstraints(noI).TheIresultsareconsis-tentlybetter.And,itisalsoclearfromTable2,thatusingconstraintsintrainingalwaysimproves286

themodelandtheamountofimprovementdependsontheamountoflabeleddata.Figure3comparestwoprotocolsontheadver-tisementsdomain:H&W+I,wherewe(cid:2)rstruntheH&Wprotocolandthenapplytheconstraintsdur-ingtestingstage,andH&W&C+I,whichusescon-straintstoguidethemodelduringlearningandusesitalsointesting.Althoughinjectingconstraintsinthelearningprocesshelps,testingwithconstraintsismoreimportantthanusingconstraintsduringlearn-ing,especiallywhenthelabeleddatasizeislarge.Thiscon(cid:2)rmsresultsreportedforthesupervisedlearningcasein(Punyakanoketal.,2005;RothandYih,2005).However,asshown,ourproposedal-gorithmH&W&Cfortrainingwithconstraintsiscriticalwhentheamountlabeleddataissmall.Figure4furtherstrengthensthispoint.Inthecita-tionsdomain,H&W&C+Iachieveswith20labeledsamplessimilarperformancetothesupervisedver-sionwithoutconstraintswith300labeledsamples.(Grenageretal.,2005)and(HaghighiandKlein,2006)alsoreportresultsforsemi-supervisedlearn-ingforthesedomains.However,duetodiffer-entpreprocessing,thecomparisonisnotstraight-forward.Forthecitationdomain,when20labeledand300unlabeledsamplesareavailable,(Grenageretal.,2005)observedanincreasefrom65.2%to71.3%.Ourimprovementisfrom70.1%to79.4%.Fortheadvertisementdomain,theyobservednoim-provement,whileourmodelimprovesfrom68.1%to74.6%with20labeledsamples.Moreover,wesuccessfullyuseout-of-domaindata(webdata)toimproveourmodel,whiletheyreportthatthisdatadidnotimprovetheirunsupervisedmodel.(HaghighiandKlein,2006)alsoworkedononeofourdatasets.Theirunderlyingmodel,MarkovRan-domFields,allowsmoreexpressivefeatures.Nev-ertheless,whentheyuseonlyunaryconstraintstheyget53.75%.Whentheyusetheir(cid:2)nalmodel,alongwithamechanismforextendingtheprototypestoothertokens,theygetresultsthatarecomparabletoourmodelwith10labeledexamples.Additionally,intheirframework,itisnotclearhowtousesmallamountsoflabeleddatawhenavailable.Ourmodeloutperformstheirsonceweadd10moreexamples. 0.65 0.7 0.75 0.8 0.85100252015105H+N+IH+N+C+IFigure3:ComparisonbetweenH&W+IandH&W&C+Iontheadvertisementsdomain.Whenthereisalotoflabeleddata,inferencewithcon-straintsismoreimportantthanusingconstraintsdur-inglearning.However,itisimportanttotrainwithconstraintswhentheamountoflabeleddataissmall. 0.7 0.75 0.8 0.85 0.9 0.95100252015105sup. (300)H+N+C+IFigure4:With20labeledcitations,ouralgorithmperformscompetitivelytothesupervisedversiontrainedon300samples.7SoftConstraintsThissectiondiscussestheimportanceofusingsoftconstraintsratherthanhardconstraints,thechoiceofHammingdistanceford(y,1C(x))andhowweapproximateit.Weusetwoconstraintstoillustratetheideas.(C1):(cid:147)statetransitionscanonlyoccuronpunctuationmarksornewlines(cid:148),and(C2):(cid:147)the(cid:2)eldTITLEmustappear(cid:148).First,weclaimthatde(cid:2)ningd(y,1C(x))tobetheHammingdistanceissuperiortousingabinaryvalue,d(y,1C(x))=0ify21C(x)and1other-wise.Consider,forexample,theconstraintC1intheadvertisementsdomain.Whilethevastmajorityoftheinstancessatisfytheconstraint,someviolateitinmorethanoneplace.Therefore,oncethebinarydistanceissetto1,thealgorithmloosestheabilitytodiscriminateconstraintviolationsinotherlocations287

ofthesameinstance.Thismayhurttheperformanceinboththeinferenceandthelearningstage.ComputingtheHammingdistanceexactlycanbeacomputationallyhardproblem.Further-more,itisunreasonabletoimplementtheex-actcomputationforeachconstraint.Therefore,weimplementedagenericapproximationforthehammingdistanceassumingonlythatwearegivenabooleanfunctionφC(yN)thatreturnswhetherlabelingthetokenxNwithstateyNvio-latesconstraintwithrespecttoanalreadylabeledsequence(x1,...,xN−1,y1,...,yN−1).Thend(y,1C(x))=PNi=1φC(yi).Forexample,considerthepre(cid:2)xx1,x2,x3,x4,whichcon-tainsnopunctuationornewlinesandwaslabeledAUTH,AUTH,DATE,DATE.ThislabelingviolatesC1,theminimalhammingdistanceis2,andourapproximationgives1,(sincethereisonlyonetransitionthatviolatestheconstraint.)Forconstraintswhichcannotbevalidatedbasedonpre(cid:2)xinformation,ourapproximationresortstobinaryviolationcount.Forinstance,theconstraintC2cannotbeimplementedwithpre(cid:2)xinformationwhentheassignmentisnotcomplete.Otherwise,itwouldmeanthatthe(cid:2)eldTITLEshouldappearasearlyaspossibleintheassignment.While(RothandYih,2005)showedthesignif-icanceofusinghardconstraints,ourexperimentsshowthatusingsoftconstraintsisasuperiorop-tion.Forexample,intheadvertisementsdomain,C1holdsforthelargemajorityofthegold-labeledinstances,butissometimesviolated.Insupervisedtrainingwith100labeledexamplesonthisdomain,supgave76.3%accuracy.Whentheconstraintvio-lationpenaltyρwasin(cid:2)nity(equivalenttohardcon-straint),theaccuracyimprovedto78.7%,butwhenthepenaltywassetto(cid:0)log(0.1),theaccuracyofthemodeljumpedto80.6%.8ConclusionsandFutureWorkWeproposedtouseconstraintsasawaytoguidesemi-supervisedlearning.Theframeworkdevel-opedisgeneralbothintermsoftherepresentationandexpressivenessoftheconstraints,andintermsoftheunderlyingmodelbeinglearned(cid:150)HMMinthecurrentimplementation.Moreover,ourframe-workisausefultoolwhenthedomainknowledgecannotbeexpressedbythemodel.Theresultsshowthatconstraintsimprovenotonlytheperformanceofthe(cid:2)nalinferencestagebutalsopropagateusefulinformationduringthesemi-supervisedlearningprocessandthattrainingwiththeconstraintsisespeciallysigni(cid:2)cantwhenthenumberoflabeledtrainingdataissmall.Acknowledgments:ThisworkissupportedbyNSFSoD-HCER-0613885andbyagrantfromBoeing.PartofthisworkwasdonewhileDanRothvisitedtheTechnion,Israel,sup-portedbyaLadyDavisFellowship.ReferencesW.CohenandS.Sarawagi.2004.Exploitingdictionariesinnamedentityextraction:Combiningsemi-markovextractionprocessesanddataintegrationmethods.InProc.oftheACMSIGKDD.M.CollinsandY.Singer.1999.Unsupervisedmodelsfornamedentityclassi(cid:2)cation.InProc.ofEMNLP.M.Collins.2002.DiscriminativetrainingmethodsforhiddenMarkovmodels:Theoryandexperimentswithperceptronalgorithms.InProc.ofEMNLP.T.Grenager,D.Klein,andC.Manning.2005.Unsupervisedlearningof(cid:2)eldsegmentationmodelsforinformationextrac-tion.InProc.oftheAnnualMeetingoftheACL.A.HaghighiandD.Klein.2006.Prototype-drivenlearningforsequencemodels.InProc.ofHTL-NAACL.A.McCallum,D.Freitag,andF.Pereira.2000.Maximumentropymarkovmodelsforinformationextractionandseg-mentation.InProc.ofICML.D.McClosky,E.Charniak,andM.Johnson.2006.Effectiveself-trainingforparsing.InProceedingsofHLT-NAACL.K.Nigam,A.McCallum,S.Thrun,andT.Mitchell.2000.Textclassi(cid:2)cationfromlabeledandunlabeleddocumentsusingEM.MachineLearning,39(2/3):103(cid:150)134.V.Punyakanok,D.Roth,W.Yih,andD.Zimak.2005.Learn-ingandinferenceoverconstrainedoutput.InProc.ofIJCAI.D.RothandW.Yih.2005.Integerlinearprogramminginfer-enceforconditionalrandom(cid:2)elds.InProc.ofICML.D.Roth.1999.Learninginnaturallanguage.InProc.ofIJCAI,pages898(cid:150)904.W.Shen,X.Li,andA.Doan.2005.Constraint-basedentitymatching.InProc.ofAAAI).N.SmithandJ.Eisner.2005.Contrastiveestimation:Traininglog-linearmodelsonunlabeleddata.InProc.oftheAnnualMeetingoftheACL.M.ThelenandE.Riloff.2002.Abootstrappingmethodforlearningsemanticlexiconsusingextractionpatterncontexts.InProc.ofEMNLP.K.Toutanova,A.Haghighi,andC.D.Manning.2005.Jointlearningimprovessemanticrolelabeling.InProc.oftheAnnualMeetingoftheACL.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 288–295,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

288

SupertaggedPhrase-BasedStatisticalMachineTranslationHanyHassanSchoolofComputing,DublinCityUniversity,Dublin9,Irelandhhasan@computing.dcu.ieKhalilSima’anLanguageandComputation,UniversityofAmsterdam,Amsterdam,TheNetherlandssimaan@science.uva.nlAndyWaySchoolofComputing,DublinCityUniversity,Dublin9,Irelandaway@computing.dcu.ieAbstractUntilquiterecently,extendingPhrase-basedStatisticalMachineTranslation(PBSMT)withsyntacticstructurecausedsystemper-formancetodeteriorate.Inthisworkweshowthatincorporatinglexicalsyntacticde-scriptionsintheformofsupertagscanyieldsigniﬁcantlybetterPBSMTsystems.Wede-scribeanovelPBSMTmodelthatintegratessupertagsintothetargetlanguagemodelandthetargetsideofthetranslationmodel.Twokindsofsupertagsareemployed:thosefromLexicalizedTree-AdjoiningGrammarandCombinatoryCategorialGrammar.De-spitethedifferencesbetweenthesetwoap-proaches,thesupertaggersgivesimilarim-provements.Inadditiontosupertagging,wealsoexploretheutilityofasurfaceglobalgrammaticalitymeasurebasedoncombina-toryoperators.Weperformvariousexperi-mentsontheArabictoEnglishNIST2005testsetaddressingissuessuchassparseness,scalabilityandtheutilityofsystemsubcom-ponents.Ourbestresult(0.4688BLEU)improvesby6.1%relativetoastate-of-the-artPBSMTmodel,whichcomparesveryfavourablywiththeleadingsystemsontheNIST2005task.1IntroductionWithintheﬁeldofMachineTranslation,byfarthemostdominantparadigmisPhrase-basedStatisticalMachineTranslation(PBSMT)(Koehnetal.,2003;Tillmann&Xia,2003).However,unlikeinrule-andexample-basedMT,ithasprovendifﬁculttodatetoincorporatelinguistic,syntacticknowledgeinordertoimprovetranslationquality.Onlyquiterecentlyhave(Chiang,2005)and(Marcuetal.,2006)shownthatincorporatingsomeformofsyntacticstructurecouldshowimprovementsoverabaselinePBSMTsystem.While(Chiang,2005)availsofstructurewhichisnotlinguisticallymotivated,(Marcuetal.,2006)employsyntacticstructuretoenrichtheen-triesinthephrasetable.InthispaperweexploreanovelapproachtowardsextendingastandardPBSMTsystemwithsyntacticdescriptions:weinjectlexicaldescriptionsintoboththetargetsideofthephrasetranslationtableandthetargetlanguagemodel.Crucially,thekindoflexicaldescriptionsthatweemployarethosethatarecom-monlydevisedwithinlexicon-drivenapproachestolinguisticsyntax,e.g.LexicalizedTree-AdjoiningGrammar(Joshi&Schabes,1992;Bangalore&Joshi,1999)andCombinaryCategorialGrammar(Steedman,2000).Intheselinguisticapproaches,itisassumedthatthegrammarconsistsofaveryrichlexiconandatiny,impoverished1setofcombina-toryoperatorsthatassemblelexicalentriestogetherintoparse-trees.Thelexicalentriesconsistofsyn-tacticconstructs(‘supertags’)thatdescribeinforma-tionsuchasthePOStagoftheword,itssubcatego-rizationinformationandthehierarchyofphrasecat-egoriesthatthewordprojectsupwards.Inthisworkweemploythelexicalentriesbutexchangetheal-gebraiccombinatoryoperatorswiththemorerobust1Theseoperatorsneithercarrynorpresupposefurtherlin-guisticknowledgebeyondwhatthelexiconcontains.289

andefﬁcientsupertaggingapproach:likestandardtaggers,supertaggersemployprobabilitiesbasedonlocalcontextandcanbeimplementedusingﬁnitestatetechnology,e.g.HiddenMarkovModels(Ban-galore&Joshi,1999).Therearecurrentlytwosupertaggingapproachesavailable:LTAG-based(Bangalore&Joshi,1999)andCCG-based(Clark&Curran,2004).BoththeLTAG(Chenetal.,2006)andtheCCGsupertagsets(Hockenmaier,2003)wereacquiredfromtheWSJsectionofthePenn-IITreebankusinghand-builtextractionrules.HerewetestboththeLTAGandCCGsupertaggers.Weinterpolate(log-linearly)thesupertaggedcomponents(languagemodelandphrasetable)withthecomponentsofastandardPBSMTsystem.OurexperimentsontheArabic–EnglishNIST2005testsuiteshowthateachofthesupertaggedsystemssigniﬁcantlyimprovesoverthebaselinePBSMTsystem.Interestingly,combiningthetwotaggerstogetherdiminishesthebeneﬁtsofsupertaggingseenwiththeindividualLTAGandCCGsystems.Inthispaperwediscusstheseandotherempiricalissues.Theremainderofthepaperisorganisedasfol-lows:insection2wediscusstherelatedworkonen-richingPBSMTwithsyntacticstructure.Insection3,wedescribethebaselinePBSMTsystemwhichourworkextends.Insection4,wedetailourap-proach.Section5describestheexperimentscarriedout,togetherwiththeresultsobtained.Section6concludes,andprovidesavenuesforfurtherwork.2RelatedWorkUntilveryrecently,theexperiencewithaddingsyn-taxtoPBSMTsystemswasnegative.Forexample,(Koehnetal.,2003)demonstratedthataddingsyn-taxactuallyharmedthequalityoftheirSMTsystem.Amongtheﬁrsttodemonstrateimprovementwhenaddingrecursivestructurewas(Chiang,2005),whoallowsforhierarchicalphraseprobabilitiesthathan-dlearangeofreorderingphenomenainthecorrectfashion.Chiang’sderivedgrammardoesnotrelyonanylinguisticannotationsorassumptions,sothatthe‘syntax’inducedisnotlinguisticallymotivated.Comingrightuptodate,(Marcuetal.,2006)demonstratethat‘syntactiﬁed’targetlanguagephrasescanimprovetranslationqualityforChinese–English.Theyemployastochastic,top-downtrans-ductionprocessthatassignsajointprobabilitytoasourcesentenceandeachofitsalternativetrans-lationswhenrewritingthetargetparse-treeintoasourcesentence.Therewriting/transductionprocessisdrivenby“xRSrules”,eachconsistingofapairofasourcephraseanda(possiblyonlypartially)lexicalizedsyntactiﬁedtargetphrase.InordertoextractxRSrules,theword-to-wordalignmentin-ducedfromtheparalleltrainingcorpusisusedtoguideheuristictree‘cutting’criteria.Whiletheresearchof(Marcuetal.,2006)hasmuchincommonwiththeapproachproposedhere(suchasthesyntactiﬁedtargetphrases),therere-mainanumberofsigniﬁcantdifferences.Firstly,ratherthaninducemillionsofxRSrulesfrompar-alleldata,weextractphrasepairsinthestandardway(Och&Ney,2003)andassociatewitheachphrase-pairasetoftargetlanguagesyntacticstruc-turesbasedonsupertagsequences.Relativetousingarbitraryparse-chunks,thepowerofsupertagsliesinthefactthattheyare,syntacticallyspeaking,richlexicaldescriptions.Asupertagcanbeassignedtoeverywordinaphrase.Ontheonehand,thecor-rectsequenceofsupertagscouldbeassembledto-gether,usingonlyimpoverishedcombinatoryopera-tors,intoasmallsetofconstituents/parses(‘almost’aparse).Ontheotherhand,becausesupertagsarelexicalentries,theyfacilitaterobustsyntacticpro-cessing(usingMarkovmodels,forinstance)whichdoesnotnecessarilyaimatbuildingafullycon-nectedgraph.AsecondmajordifferencewithxRSrulesisthatoursupertag-enrichedtargetphrasesneednotbegeneralizedinto(xRSoranyother)rulesthatworkwithabstractcategories.Finally,likePOStagging,supertaggingismoreefﬁcientthanactualparsingortreetransduction.3BaselinePhrase-BasedSMTSystemWepresentthebaselinePBSMTmodelwhichweextendwithsupertagsinthenextsection.OurbaselinePBSMTmodelusesGIZA++2toobtainword-levelalignmentsinbothlanguagedirections.Thebidirectionalwordalignmentisusedtoobtainphrasetranslationpairsusingheuristicspresentedin2http://www.fjoch.com/GIZA++.html290

(Och&Ney,2003)and(Koehnetal.,2003),andtheMosesdecoderwasusedforphraseextractionanddecoding.3Lettandsbethetargetandsourcelanguagesentencesrespectively.Any(targetorsource)sen-tencexwillconsistoftwoparts:abagofelements(words/phrasesetc.)andanorderoverthatbag.Inotherwords,x=hφx,Oxi,whereφxstandsforthebagofphrasesthatconstitutex,andOxfortheorderofthephrasesasgiveninx(Oxcanbeimplementedasafunctionfromabagoftokensφxtoasetwithaﬁnitenumberofpositions).Hence,wemayseparateorderfromcontent:argmaxtP(t|s)=argmaxtP(s|t)P(t)(1)=argmaxhφt,OtiTMz}|{P(φs|φt)distortionz}|{P(Os|Ot)LMz}|{Pw(t)(2)Here,Pw(t)isthetargetlanguagemodel,P(Os|Ot)representstheconditional(order)lineardistortionprobability,andP(φs|φt)standsforaprobabilis-tictranslationmodelfromtargetlanguagebagsofphrasestosourcelanguagebagsofphrasesusingaphrasetranslationtable.AscommonlydoneinPB-SMT,weinterpolatethesemodelslog-linearly(us-ingdifferentλweights)togetherwithawordpenaltyweightwhichallowsforcontroloverthelengthofthetargetsentencet:argmaxhφt,OtiP(φs|φt)P(Os|Ot)λoPw(t)λlmexp|t|λwForconvenienceofnotation,theinterpolationfactorforthebagofphrasestranslationmodelisshowninformula(3)atthephraselevel(butthatdoesnoten-tailanydifference).Forabagofphrasesφtconsist-ingofphrasesti,andbagφsconsistingofphrasessi,thephrasetranslationmodelisgivenby:P(φs|φt)=YsitiP(si|ti)P(si|ti)=Pph(si|ti)λt1Pw(si|ti)λt2Pr(ti|si)λt3(3)wherePphandPrarethephrase-translationproba-bilityanditsreverseprobability,andPwisthelexi-caltranslationprobability.3http://www.statmt.org/moses/4OurApproach:SupertaggedPBSMTWeextendthebaselinemodelwithlexicallinguis-ticrepresentations(supertags)bothinthelanguagemodelaswellasinthephrasetranslationmodel.Be-forewedescribehowourmodelextendsthebase-line,weshortlyreviewthesupertaggingapproachesinLexicalizedTree-AdjoiningGrammarandCom-binatoryCategorialGrammar.4.1Supertags:LexicalSyntaxNPDTheNPNPNpurchaseNPNPNpriceSNPVPVincludesNPNPNtaxesFigure1:AnLTAGsupertagsequenceforthesen-tenceThepurchasepriceincludestaxes.Thesub-categorizationinformationismostclearlyavailableintheverbincludeswhichtakesasubjectNPtoitsleftandanobjectNPtoitsright.Modernlinguistictheoryproposesthatasyntacticparserhasaccesstoanextensivelexiconofword-structurepairsandasmall,impoverishedsetofoper-ationstomanipulateandcombinethelexicalentriesintoparses.ExamplesofformalinstantiationsofthisideaincludeCCGandLTAG.Thelexicalentriesaresyntacticconstructs(graphs)thatspecifyinforma-tionsuchasPOStag,subcategorization/dependencyinformationandothersyntacticconstraintsatthelevelofagreementfeatures.Oneimportantwayofportrayingsuchlexicaldescriptionsisviathesu-pertagsdevisedintheLTAGandCCGframeworks(Bangalore&Joshi,1999;Clark&Curran,2004).Asupertag(seeFigure1)representsacomplex,linguisticwordcategorythatencodesasyntacticstructureexpressingaspeciﬁclocalbehaviourofaword,intermsoftheargumentsittakes(e.g.sub-ject,object)andthesyntacticenvironmentinwhichitappears.Infact,inLTAGasupertagisanelemen-tarytreeandinCCGitisaCCGlexicalcategory.Bothdescriptionscanbeviewedascloselyrelatedfunctionaldescriptions.Theterm“supertagging”(Bangalore&Joshi,1999)referstotaggingthewordsofasentence,each291

withasupertag.Whenwell-formed,anorderedse-quenceofsupertagscanbeviewedasacompactrepresentationofasmallsetofconstituents/parsesthatcanbeobtainedbyassemblingthesupertagstogetherusingtheappropriatecombinatoryopera-tors(suchassubstitutionandadjunctioninLTAGorfunctionapplicationandcombinationinCCG).AkintoPOStagging,theprocessofsupertagginganinpututteranceproceedswithstatisticsthatarebasedontheprobabilityofaword-supertagpairgiventheirMarkovianorlocalcontext(Bangalore&Joshi,1999;Clark&Curran,2004).Thisisthemaindifferencewithfullparsing:supertaggingtheinpututteranceneednotresultinafullyconnectedgraph.TheLTAG-basedsupertaggerof(Bangalore&Joshi,1999)isastandardHMMtaggerandconsistsofa(second-order)Markovlanguagemodeloversu-pertagsandalexicalmodelconditioningtheproba-bilityofeverywordonitsownsupertag(justlikestandardHMM-basedPOStaggers).TheCCGsupertagger(Clark&Curran,2004)isbasedonlog-linearprobabilitiesthatconditionasu-pertagonfeaturesrepresentingitscontext.TheCCGsupertaggerdoesnotconstitutealanguagemodelnoraretheMaximumEntropyestimatesdirectlyin-terpretableassuch.InourmodelweemploytheCCGsupertaggertoobtainthebestsequencesofsu-pertagsforacorpusofsentencesfromwhichweob-tainlanguagemodelstatistics.Besidesthediffer-enceinprobabilitiesandstatisticalestimates,thesetwosupertaggersdifferinthewaythesupertagsareextractedfromthePennTreebank,cf.(Hocken-maier,2003;Chenetal.,2006).Bothsupertaggersachieveasupertaggingaccuracyof90–92%.ThreeaspectsmakesupertagsattractiveinthecontextofSMT.Firstly,supertagsarerichsyntac-ticconstructsthatexistforindividualwordsandsotheyareeasytointegrateintoSMTmodelsthatcanbebasedonanylevelofgranularity,beitword-orphrase-based.Secondly,supertagsspecifythelocalsyntacticconstraintsforaword,whichres-onateswellwithsequential(ﬁnitestate)statistical(e.g.Markov)models.Finally,becausesupertagsarerichlexicaldescriptionsthatrepresentunder-speciﬁcationinparsing,itispossibletohavesomeofthebeneﬁtsoffullparsingwithoutimposingthestrictconnectednessrequirementsthatitdemands.4.2ASupertag-BasedSMTmodelWeemploytheaforementionedsupertaggerstoen-richtheEnglishsideoftheparalleltrainingcor-puswithasinglesupertagsequencepersentence.Thenweextractphrase-pairstogetherwiththeco-occuringEnglishsupertagsequencefromthiscor-pusviathesamephraseextractionmethodusedinthebaselinemodel.Thiswaywedirectlyextendthebaselinemodeldescribedinsection3withsu-pertagsbothinthephrasetranslationtableandinthelanguagemodel.Nextwedeﬁnetheprobabilisticmodelthataccompaniesthissyntacticenrichmentofthebaselinemodel.LetSTrepresentasupertagsequenceofthesamelengthasatargetsentencet.Equation(2)changesasfollows:argmaxtXSTP(s|t,ST)PST(t,ST)≈argmaxht,STiTMw.sup.tagsz}|{P(φs|φt,ST)distortionz}|{P(Os|Ot)λoLMw.sup.tagsz}|{PST(t,ST)word−penaltyz}|{exp|t|λwTheapproximationsmadeinthisformulaareoftwokinds:thestandardsplitintocomponentsandthesearchforthemostlikelyjointprobabilityofatar-gethypothesisandasupertagsequencecooccuringwiththesourcesentence(akindofViterbiapproachtoavoidthecomplexoptimizationinvolvingthesumoversupertagsequences).ThedistortionandwordpenaltymodelsarethesameasthoseusedinthebaselinePBSMTmodel.SupertaggedLanguageModelThe‘languagemodel’PST(t,ST)isasupertaggerassigningprob-abilitiestosequencesofword–supertagpairs.Thelanguagemodelisfurthersmoothedbylog-linearinterpolationwiththebaselinelanguagemodeloverwordsequences.SupertagsinPhraseTablesThesupertaggedphrasetranslationprobabilityconsistsofacombina-tionofsupertaggedcomponentsanalogoustotheircounterpartsinthebaselinemodel(equation(3)),i.e.itconsistsofP(s|t,ST),itsreverseandaword-levelprobability.Wesmooththisproba-bilitybylog-linearinterpolationwiththefactored292

JohnboughtquicklysharesNNP_NNVBD_(S[dcl]\NP)/NPRB|(S\NP)\(S\NP)NNS_N2ViolationsFigure2:ExampleCCGoperatorviolations:V=2andL=3,andsothepenaltyfactoris1/3.backoffversionP(s|t)P(s|ST),whereweim-portthebaselinephrasetableprobabilityandex-ploittheprobabilityofasourcephrasegiventhetar-getsupertagsequence.AmodelinwhichweomitP(s|ST)turnsouttobeslightlylessoptimalthanthisone.Asinmoststate-of-the-artPBSMTsystems,weuseGIZA++toobtainword-levelalignmentsinbothlanguagedirections.Thebidirectionalwordalign-mentisusedtoobtainlexicalphrasetranslationpairsusingheuristicspresentedin(Och&Ney,2003)and(Koehnetal.,2003).Giventhecollectedphrasepairs,weestimatethephrasetranslationprobabilitydistributionbyrelativefrequencyasfollows:ˆPph(s|t)=count(s,t)Pscount(s,t)Foreachextractedlexicalphrasepair,weextractthecorrespondingsupertaggedphrasepairsfromthesu-pertaggedtargetsequenceinthetrainingcorpus(cf.section5).Foreachlexicalphrasepair,thereisatleastonecorrespondingsupertaggedphrasepair.Theprobabilityofthesupertaggedphrasepairises-timatedbyrelativefrequencyasfollows:Pst(s|t,st)=count(s,t,st)Pscount(s,t,st)4.3LMswithaGrammaticalityFactorThesupertagsusuallyencodedependencyinforma-tionthatcouldbeusedtoconstructan‘almostparse’withthehelpoftheCCG/LTAGcompositionoper-ators.Then-gramlanguagemodeloversupertagsappliesakindofstatistical‘compositionalitycheck’butduetosmoothingeffectsthiscouldmaskcru-cialviolationsofthecompositionalityoperatorsofthegrammarformalism(CCGinthiscase).Itisinterestingtoobservetheeffectofintegratingintothelanguagemodelapenaltyimposedwhenformalcompostionoperatorsareviolated.Wecombinethen-gramlanguagemodelwithapenaltyfactorthatmeasuresthenumberofencounteredcombinatoryoperatorviolationsinasequenceofsupertags(cf.Figure2).Forasupertagsequenceoflength(L)whichhas(V)operatorviolations(asmeasuredbytheCCGsystem),thelanguagemodelPwillbead-justedasP∗=P×(1−VL).Thisisofcoursenolongerasimplesmoothedmaximum-likelihoodes-timatenorisitatrueprobability.Nevertheless,thismechanismprovidesasimple,efﬁcientintegrationofaglobalcompositionality(grammaticality)mea-sureintothen-gramlanguagemodeloversupertags.DecoderThedecoderusedinthisworkisMoses,alog-lineardecodersimilartoPharaoh(Koehn,2004),modiﬁedtoaccommodatesupertagphraseprobabilitiesandsupertaglanguagemodels.5ExperimentsInthissectionwepresentanumberofexperimentsthatdemonstratetheeffectoflexicalsyntaxontrans-lationquality.WecarriedoutexperimentsontheNISTopendomainnewstranslationtaskfromAra-bicintoEnglish.Weperformedanumberofex-perimentstoexaminetheeffectofsupertaggingap-proaches(CCGorLTAG)withvaryingdatasizes.DataandSettingsTheexperimentswerecon-ductedforArabictoEnglishtranslationandtestedontheNIST2005evaluationset.ThesystemsweretrainedontheLDCArabic–Englishparallelcorpus;weusethenewspart(130Ksentences,about5mil-lionwords)totrainsystemswithwhatwecallthesmalldataset,andthenewsandalargepartoftheUNdata(2millionsentences,about50millionwords)forexperimentswithlargedatasets.Then-gramtargetlanguagemodelwasbuiltus-ing250MwordsfromtheEnglishGigaWordCor-pususingtheSRILMtoolkit.4Taking10%oftheEnglishGigaWordCorpususedforbuildingourtar-getlanguagemodel,thesupertag-basedtargetlan-guagemodelswerebuiltfrom25Mwordsthatweresupertagged.FortheLTAGsupertagsexperiments,weusedtheLTAGEnglishsupertagger5(Bangalore4http://www.speech.sri.com/projects/srilm/5http://www.cis.upenn.edu/˜xtag/gramrelease.html293

&Joshi,1999)totagtheEnglishpartoftheparalleldataandthesupertaglanguagemodeldata.FortheCCGsupertagexperiments,weusedtheCCGsu-pertaggerof(Clark&Curran,2004)andtheEdin-burghCCGtools6totagtheEnglishpartofthepar-allelcorpusaswellastheCCGsupertaglanguagemodeldata.TheNISTMT03testsetisusedfordevelopment,particularlyforoptimizingtheinterpolationweightsusingMinimumErrorRatetraining(Och,2003).BaselineSystemThebaselinesystemisastate-of-the-artPBSMTsystemasdescribedinsec-tion3.Webuilttwobaselinesystemswithtwodifferent-sizedtrainingsets:‘Base-SMALL’(5mil-lionwords)and‘Base-LARGE’(50millionwords)asdescribedabove.Bothsystemsuseatrigramlan-guagemodelbuiltusing250millionwordsfromtheEnglishGigaWordCorpus.Table1presentstheBLEUscores(Papinenietal.,2002)ofbothsystemsontheNIST2005MTEvaluationtestset.SystemBLEUScoreBase-SMALL0.4008Base-LARGE0.4418Table1:Baselinesystems’BLEUscores5.1Baselinevs.SupertagsonSmallDataSetsWecomparedthetranslationqualityofthebaselinesystemswiththeLTAGandCCGsupertagssystems(LTAG-SMALLandCCG-SMALL).TheresultsareSystemBLEUScoreBase-SMALL0.4008LTAG-SMALL0.4205CCG-SMALL0.4174Table2:LTAGandCCGsystemsonsmalldatagiveninTable2.Allsystemsweretrainedonthesameparalleldata.TheLTAGsupertag-basedsys-temoutperformsthebaselineby1.97BLEUpointsabsolute(or4.9%relative),whiletheCCGsupertag-basedsystemscores1.66BLEUpointsoverthe6http://groups.inf.ed.ac.uk/ccg/software.htmlbaseline(4.1%relative).Thesesigniﬁcantimprove-mentsindicatethattherichinformationinsupertagshelpsselectbettertranslationcandidates.POSTagsvs.SupertagsAsupertagisacomplextagthatlocalizesthedependencyandthesyntaxin-formationfromthecontext,whereasanormalPOStagjustdescribesthegeneralsyntacticcategoryofthewordwithoutfurtherconstraints.Inthisexperi-mentwecomparedtheeffectofusingsupertagsandPOStagsontranslationquality.AscanbeseenSystemBLEUScoreBase-SMALL0.4008POS-SMALL0.4073LTAG-SMALL.0.4205Table3:ComparingtheeffectofsupertagsandPOStagsinTable3,whilethePOStagshelp(0.65BLEUpoints,or1.7%relativeincreaseoverthebaseline),theyclearlyunderperformcomparedtothesupertagmodel(by3.2%).TheUsefulnessofaSupertaggedLMIntheseexperimentswestudytheeffectofthetwoaddedfeature(cost)functions:supertaggedtranslationandlanguagemodels.Wecomparethebaselinesystemtothesupertagssystemwiththesupertagphrase-tableprobabilitybutwithoutthesupertagLM.Ta-ble4liststhebaselinesystem(Base-SMALL),theLTAGsystemwithoutsupertaggedlanguagemodel(LTAG-TM-ONLY)andtheLTAG-SMALLsys-temwithbothsupertaggedtranslationandlanguagemodels.TheresultspresentedinTable4indi-SystemBLEUScoreBase-SMALL0.4008LTAG-TM-ONLY0.4146LTAG-SMALL.0.4205Table4:Theeffectofsupertaggedcomponentscatethattheimprovementisasharedcontributionbetweenthesupertaggedtranslationandlanguagemodels:addingtheLTAGTMimprovesBLEUscoreby1.38points(3.4%relative)overthebase-line,withtheLTAGLMimprovingBLEUscoreby294

afurther0.59points(afurther1.4%increase).5.2Scalability:LargerTrainingCorporaOutperformingaPBSMTsystemonsmallamountsoftrainingdataislessimpressivethandoingsoonreallylargesets.TheissuehereisscalabilityaswellaswhetherthePBSMTsystemisabletobridgetheperformancegapwiththesupertaggedsystemwhenreasonablylargesizesoftrainingdataareused.Tothisend,wetrainedthesystemson2millionsen-tencesofparalleldata,deployingLTAGsupertagsandCCGsupertags.Table5presentsthecompari-sonbetweenthesesystemsandthebaselinetrainedonthesamedata.TheLTAGsystemimprovesby1.17BLEUpoints(2.6%relative),buttheCCGsys-temgivesanevenlargerincrease:1.91BLEUpoints(4.3%relative).Whilethisisslightlylowerthanthe4.9%relativeimprovementwiththesmallerdatasets,thesustainedincreaseisprobablyduetoob-servingmoredatawithdifferentsupertagcontexts,whichenablesthemodeltoselectbettertargetlan-guagephrases.SystemBLEUScoreBase-LARGE0.4418LTAG-LARGE0.4535CCG-LARGE0.4609Table5:TheeffectofmoretrainingdataAddingagrammaticalityfactorAsdescribedinsection4.3,weintegrateanimpoverishedgrammat-icalityfactorbasedontwostandardCCGcombi-nationoperations,namelyForwardandBackwardApplication.Table6comparestheresultsofthebaseline,theCCGwithann-gramLM-onlysystem(CCG-LARGE)andCCG-LARGEwiththis‘gram-maticalized’LMsystem(CCG-LARGE-GRAM).Weseethatbringingthegrammaticalityteststobearontothesupertaggedsystemgivesafurtherim-provementof0.79BLEUpoints,a1.7%relativeincrease,culminatinginanoverallincreaseof2.7BLEUpoints,ora6.1%relativeimprovementoverthebaselinesystem.5.3DiscussionAnaturalquestiontoaskiswhetherLTAGandCCGsupertagsareplayingsimilar(overlapping,orcon-SystemBLEUScoreBase-LARGE0.4418CCG-LARGE0.4609CCG-LARGE-GRAM0.4688Table6:ComparingtheeffectofCCG-GRAMﬂicting)rolesinpractice.UsinganoracletochoosethebestoutputofthetwosystemsgivesaBLEUscoreof0.441,indicatingthatthecombinationpro-videssigniﬁcantroomforimprovement(cf.Ta-ble2).However,oureffortstobuildasystemthatbeneﬁtsfromthecombinationusingasimplelog-linearcombinationofthetwomodelsdidnotgiveanysigniﬁcantperformancechangerelativetothebaselineCCGsystem.Obviously,moreinformedwaysofcombiningthetwocouldresultinbetterper-formancethanasimplelog-linearinterpolationofthecomponents.Figure3showssomeexamplesystemoutput.Whilethebaselinesystemomitstheverbgiving“theauthoritiesthatithad...”,boththeLTAGandCCGfoundaformulation“authoritiesreportedthat”withaclosermeaningtothereferencetranslation“Theauthoritiessaidthat”.Omittingverbsturnsouttobeaproblemforthebaselinesystemwhentrans-latingthenotoriousverblessArabicsentences(seeFigure4).Thesupertaggedsystemshaveamoregrammaticallystrictlanguagemodelthanastandardword-levelMarkovmodel,therebyexhibitingapref-erence(intheCCGsystemespecially)fortheinser-tionofaverbwithasimilarmeaningtothatcon-tainedinthereferencesentence.6ConclusionsSMTpractitionershaveonthewholefounditdif-ﬁculttointegratesyntaxintotheirsystems.Inthiswork,wehavepresentedanovelmodelofPBSMTwhichintegratessupertagsintothetargetlanguagemodelandthetargetsideofthetranslationmodel.UsingLTAGsupertagsgivesthebestimprove-mentoverastate-of-the-artPBSMTsystemforasmallerdataset,whileCCGsupertagsworkbestonalarge2million-sentencepairtrainingset.Addinggrammaticalityfactorsbasedonalgebraiccomposi-tionaloperatorsgivesthebestresult,namely0.4688BLEU,ora6.1%relativeincreaseoverthebaseline.295

Reference:Theauthoritiessaidhewasallowedtocontactfamilymembersbyphonefromthearmoredvehiclehewasin.Baseline:theauthoritiesthatithadallowedhimtocommunicatebyphonewithhisfamilyofthearmoredcarwhereLTAG:authoritiesreportedthatithadallowedhimtocontactbytelephonewithhisfamilyofarmoredcarwhereCCG:authoritiesreportedthatithadenabledhimtocommunicatebyphonehisfamilymembersofthearmoredcarwhereFigure3:SampleoutputfromdifferentsystemsSource:wmnAlmErwfAnAl$EbAlSynymHbllslAm.Ref:ItiswellknownthattheChinesepeoplearepeaceloving.Baseline:ItisknownthattheChinesepeopleapeace-loving.LTAG:ItisknownthattheChinesepeopleapeaceloving.CCG:ItisknownthattheChinesepeoplearepeaceloving.Figure4:VerblessArabicsentenceandsampleoutputfromdifferentsystemsThisresultcomparesfavourablywiththebestsys-temsontheNIST2005Arabic–Englishtask.Weexpectmoreworkonsystemintegrationtoimproveresultsstillfurther,andanticipatethatsimilarin-creasesaretobeseenforotherlanguagepairs.AcknowledgementsWewouldliketothankSrinivasBangaloreandtheanonymousreviewersforusefulcommentsonearlierversionsofthispaper.Thisworkispar-tiallyfundedbyScienceFoundationIrelandPrinci-palInvestigatorAward05/IN/1732,andNetherlandsOrganizationforScientiﬁcResearch(NWO)VIDIAward.ReferencesS.BangaloreandA.Joshi,“Supertagging:AnAp-proachtoAlmostParsing”,ComputationalLinguistics25(2):237–265,1999.J.Chen,S.Bangalore,andK.Vijay-Shanker,“Au-tomatedextractionoftree-adjoininggrammarsfromtreebanks”.NaturalLanguageEngineering,12(3):251–299,2006.D.Chiang,“AHierarchicalPhrase-BasedModelforSta-tisticalMachineTranslation”,inProceedingsofACL2005,AnnArbor,MI.,pp.263–270,2005.S.ClarkandJ.Curran,“TheImportanceofSupertaggingforWide-CoverageCCGParsing”,inProceedingsofCOLING-04,Geneva,Switzerland,pp.282–288,2004.J.Hockenmaier,DataandModelsforStatisticalParsingwithCombinatoryCategorialGrammar,PhDthesis,UniversityofEdinburgh,UK,2003.A.JoshiandY.Schabes,“TreeAdjoiningGrammarsandLexicalizedGrammars”inM.NivatandA.Podelski(eds.)TreeAutomataandLanguages,Amsterdam,TheNetherlands:North-Holland,pp.409–431,1992.P.Koehn,“Pharaoh:ABeamSearchDecoderforphrase-basedStatisticalMachineTranslationModels”,inPro-ceedingsofAMTA-04,Berlin/Heidelberg,Germany:SpringerVerlag,pp.115–124,2004.P.Koehn,F.Och,andD.Marcu,“StatisticalPhrase-BasedTranslation”,inProceedingsofHLT-NAACL2003,Edmonton,Canada,pp.127–133,2003.D.Marcu,W.Wang,A.EchihabiandK.Knight,“SPMT:StatisticalMachineTranslationwithSyntactiﬁedTar-getLanguagePhrases”,inProceedingsofEMNLP,Sydney,Australia,pp.44–52,2006.D.MarcuandW.Wong,“APhrase-Based,JointProbabil-ityModelforStatisticalMachineTranslation”,inPro-ceedingsofEMNLP,Philadelphia,PA.,pp.133–139,2002.F.Och,“MinimumErrorRateTraininginStatisticalMa-chineTranslation”,inProceedingsofACL2003,Sap-poro,Japan,pp.160–167,2003.F.OchandH.Ney,“ASystematicComparisonofVar-iousStatisticalAlignmentModels”,ComputationalLinguistics29:19–51,2003.K.Papineni,S.Roukos,T.WardandW-J.Zhu,“BLEU:AMethodforAutomaticEvaluationofMachineTranslation”,inProceedingsofACL2002,Philadel-phia,PA.,pp.311–318,2002.L.Rabiner,“ATutorialonHiddenMarkovModelsandSelectedApplicationsinSpeechRecognition”,inA.Waibel&F-K.Lee(eds.)ReadingsinSpeechRecog-nition,SanMateo,CA.:MorganKaufmann,pp.267–296,1990.M.Steedman,TheSyntacticProcess.Cambridge,MA:TheMITPress,2000.C.TillmannandF.Xia,“APhrase-basedUnigramModelforStatisticalMachineTranslation”,inProceedingsofHLT-NAACL2003,Edmonton,Canada.pp.106–108,2003.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 296–303,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

296

RegressionforSentence-LevelMTEvaluationwithPseudoReferencesJoshuaS.AlbrechtandRebeccaHwaDepartmentofComputerScienceUniversityofPittsburgh{jsa8,hwa}@cs.pitt.eduAbstractManyautomaticevaluationmetricsforma-chinetranslation(MT)relyonmakingcom-parisonstohumantranslations,aresourcethatmaynotalwaysbeavailable.Wepresentamethodfordevelopingsentence-levelMTevaluationmetricsthatdonotdirectlyrelyonhumanreferencetranslations.Ourmet-ricsaredevelopedusingregressionlearn-ingandarebasedonasetofweakerindi-catorsofﬂuencyandadequacy(pseudoref-erences).Experimentalresultssuggestthattheyrivalstandardreference-basedmetricsintermsofcorrelationswithhumanjudg-mentsonnewtestinstances.1IntroductionAutomaticassessmentoftranslationqualityisachallengingproblembecausetheevaluationtask,atitscore,isbasedonsubjectivehumanjudgments.Reference-basedmetricssuchasBLEU(Papinenietal.,2002)haverephrasedthissubjectivetaskasasomewhatmoreobjectivequestion:howcloselydoesthetranslationresemblesentencesthatareknowntobegoodtranslationsforthesamesource?Thisapproachrequirestheparticipationofhumantranslators,whoprovidethe“goldstandard”refer-encesentences.However,keepinghumansintheevaluationlooprepresentsasigniﬁcantexpenditurebothintermsoftimeandresources;thereforeitisworthwhiletoexplorewaysofreducingthedegreeofhumaninvolvement.Tothisend,Gamonetal.(2005)proposedalearning-basedevaluationmetricthatdoesnotcom-pareagainstreferencetranslations.Underalearn-ingframework,theinput(i.e.,thesentencetobeevaluated)isrepresentedasasetoffeatures.Thesearemeasurementsthatcanbeextractedfromthein-putsentence(andmaybeindividualmetricsthem-selves).Thelearningalgorithmcombinesthefea-turestoformamodel(acompositeevaluationmet-ric)thatproducestheﬁnalscorefortheinput.With-outhumanreferences,thefeaturesinthemodelpro-posedbyGamonetal.wereprimarilylanguagemodelfeaturesandlinguisticindicatorsthatcouldbedirectlyderivedfromtheinputsentencealone.Al-thoughtheirinitialresultswerenotcompetitivewithstandardreference-basedmetrics,theirstudiessug-gestedthatareferencelessmetricmaystillprovideusefulinformationabouttranslationﬂuency.How-ever,apotentialpitfallisthatsystemsmight“gamethemetric”byproducingﬂuentoutputsthatarenotadequatetranslationsofthesource.ThispaperproposesanalternativeapproachtoevaluateMToutputswithoutcomparingagainsthu-manreferences.Whileourmetricsarealsotrained,ourmodelconsistsofdifferentfeaturesandistrainedunderadifferentlearningregime.Crucially,ourmodelincludesfeaturesthatcapturesomeno-tionsofadequacybycomparingtheinputagainstpseudoreferences:sentencesfromotherMTsys-tems(suchascommercialoff-the-shelfsystemsoropensourcedresearchsystems).Toimproveﬂu-encyjudgments,themodelalsoincludesfeaturesthatcomparetheinputagainsttarget-language“ref-erences”suchaslargetextcorporaandtreebanks.Unlikehumantranslationsusedbystandardreference-basedmetrics,pseudoreferencesarenot297

“goldstandards”andcanbeworsethanthesen-tencesbeingevaluated;therefore,these“references”in-and-ofthemselvesarenotnecessarilyinformativeenoughforMTevaluation.Themaininsightofourapproachisthatthroughregression,thetrainedmet-ricscanmakemorenuancedcomparisonsbetweentheinputandpseudoreferences.Morespeciﬁcally,ourregressionobjectiveistoinferafunctionthatmapsafeaturevector(whichmeasuresaninput’ssimilaritytothepseudoreferences)toascorethatindicatesthequalityoftheinput.Thisisachievedbyoptimizingthemodel’soutputtocorrelateagainstasetoftrainingexamples,whicharetranslationsen-tenceslabeledwithquantitativeassessmentsoftheirqualitybyhumanjudges.Althoughthisapproachdoesincursomehumaneffort,itisprimarilyforthedevelopmentoftrainingdata,which,ideally,canbeamortizedoveralongperiodoftime.Todeterminethefeasibilityoftheproposedap-proach,weconductedempiricalstudiesthatcom-pareourtrainedmetricsagainststandardreference-basedmetrics.Wereportthreemainﬁndings.First,pseudoreferencesareinformativecompar-isonpoints.Experimentalresultssuggestthataregression-trainedmetricthatcomparesagainstpseudoreferencescanhavehighercorrelationswithhumanjudgmentsthanapplyingstandardmetricswithmultiplehumanreferences.Second,thelearn-ingmodelthatusesbothadequacyandﬂuencyfea-turesperformedthebest,withadequacybeingthemoreimportantfactor.Third,whenthepseudoref-erencesaremultipleMTsystems,theregression-trainedmetricispredictiveevenwhentheinputisfromabetterMTsystemthanthoseprovidingthereferences.WeconjecturethatcomparingMTout-putsagainstotherimperfecttranslationsallowsforamorenuanceddiscriminationofquality.2BackgroundandRelatedWorkForaformallyorganizedevent,suchastheannualMTEvaluationsponsoredbyNationalInstituteofStandardandTechnology(NISTMTEval),itmaybeworthwhiletorecruitmultiplehumantranslatorstotranslateafewhundredsentencesforevaluationreferences.However,therearesituationsinwhichmultiplehumanreferencesarenotpracticallyavail-able(e.g.,thesourcemaybeofalargequantity,andnohumantranslationexists).Onesuchinstanceistranslationqualityassurance,inwhichonewishestoidentifypooroutputsinalargebodyofmachinetranslatedtextautomaticallyforhumantopost-edit.Anotherinstanceisinday-to-dayMTresearchanddevelopment,wherenewtestsetwithmultipleref-erencesarealsohardtocomeby.OnecouldworkwithpreviousdatasetsfromeventssuchastheNISTMTEvals,butthereisadangerofover-ﬁtting.Onealsocouldextractasinglereferencefromparallelcorpora,althoughitisknownthatautomaticmetricsaremorereliablewhencomparingagainstmultiplereferences.Theaimofthisworkistodevelopatrainableau-tomaticmetricforevaluationwithouthumanrefer-ences.Thiscanbeseenasaformofconﬁdenceesti-mationonMToutputs(Blatzetal.,2003;Uefﬁngetal.,2003;Quirk,2004).Themaindistinctionisthatconﬁdenceestimationistypicallyperformedwithaparticularsysteminmind,andmayrelyonsystem-internalinformationinestimation.Inthisstudy,wedrawononlysystem-independentindicatorssothattheresultingmetricmaybemoregenerallyapplied.Thisallowsustohaveaclearerpictureofthecon-tributingfactorsastheyinteractwithdifferenttypesofMTsystems.Alsorelevantispreviousworkthatappliedma-chinelearningapproachestoMTevaluation,bothwithhumanreferences(Corston-Oliveretal.,2001;KuleszaandShieber,2004;AlbrechtandHwa,2007;LiuandGildea,2007)andwithout(Gamonetal.,2005).Onemotivationforthelearningapproachistheeaseofcombiningmultiplecriteria.Literatureintranslationevaluationreportsamyriadofcriteriathatpeopleuseintheirjudgments,butitisnotclearhowthesefactorsshouldbecombinedmathemati-cally.Machinelearningoffersaprincipledanduni-ﬁedframeworktoinduceacomputationalmodelofhuman’sdecisionprocess.Disparateindicatorscanbeencodedasoneormoreinputfeatures,andthelearningalgorithmtriestoﬁndamappingfrominputfeaturestoascorethatquantiﬁestheinput’squalitybyoptimizingthemodeltomatchhumanjudgmentsontrainingexamples.TheframeworkisattractivebecauseitsobjectivedirectlycapturesthegoalofMTevaluation:howwouldauserratethequalityofthesetranslations?Thisworkdiffersfrompreviousapproachesin298

twoaspects.Oneistherepresentationofthemodel;ourmodeltreatsthemetricasadistancemeasureeventhoughtherearenohumanreferences.An-otheristhetrainingofthemodel.Moresothanwhenhumanreferencesareavailable,regressioniscentraltothesuccessoftheapproach,asitdeter-mineshowmuchwecantrustthedistancemeasuresagainsteachpseudoreferencesystem.Whileourmodeldoesnotusehumanreferencesdirectly,itsfeaturesareadaptedfromthefollowingdistance-basedmetrics.Thewell-knownBLEU(Pa-pinenietal.,2002)isbasedonthenumberofcom-monn-gramsbetweenthetranslationhypothesisandhumanreferencetranslationsofthesamesentence.MetricssuchasROUGE,HeadWordChain(HWC),METEOR,andotherrecentlyproposedmethodsallofferdifferentwaysofcomparingmachineandhu-mantranslations.ROUGEutilizes’skipn-grams’,whichallowformatchesofsequencesofwordsthatarenotnecessarilyadjacent(LinandOch,2004a).METEORusesthePorterstemmerandsynonym-matchingviaWordNettocalculaterecallandpre-cisionmoreaccurately(BanerjeeandLavie,2005).TheHWCmetricscomparedependencyandcon-stituencytreesforbothreferenceandmachinetrans-lations(LiuandGildea,2005).3MTEvaluationwithPseudoReferencesusingRegressionReference-basedmetricsaretypicallythoughtofasmeasurementsof“similaritytogoodtranslations”becausehumantranslationsareusedasreferences,butinmoregeneralterms,theyaredistancemea-surementsbetweentwosentences.Thedistancebe-tweenatranslationhypothesisandanimperfectref-erenceisstillsomewhatinformative.Asatoyex-ample,consideraone-dimensionallinesegment.Adistancefromtheend-pointuniquelydeterminesthepositionofapoint.Whenthereferencelocationisanywhereelseonthelinesegment,arelativedis-tancetothereferencedoesnotuniquelyspecifyalocationonthelinesegment.However,thepositionofapointcanbeuniquelydeterminedifwearegivenitsrelativedistancestotworeferencelocations.TheproblemspaceforMTevaluation,thoughmorecomplex,isnotdissimilartothetoyscenario.Therearetwomaindifferences.First,wedonotknowtheactualdistancefunction–thisiswhatwearetryingtolearn.Thedistancefunctionswehaveatourdisposalareallheuristicapproximationstothetruetranslationaldistancefunction.Second,unlikehumanreferences,whosequalityvalueisassumedtobemaximum,thequalityofapseudoreferencesen-tenceisnotknown.Infact,priortotraining,wedonotevenknowthequalityofthereferencesystems.Althoughthedirectwaytocalibrateareferencesys-temistoevaluateitsoutputs,thisisnotpracticallyideal,sincehumanjudgmentswouldbeneededeachtimewewishtoincorporateanewreferencesystem.OurproposedalternativeistocalibratethereferencesystemsagainstanexistingsetofhumanjudgmentsforarangeofoutputsfromdifferentMTsystems.Thatis,ifmanyofthereferencesystem’soutputsaresimilartothoseMToutputsthatreceivedlowassessments,weconcludethisreferencesystemmaynotbeofhighquality.Thus,ifanewtranslationisfoundtobesimilarwiththisreferencesystem’sout-put,itismorelikelyforthenewtranslationtoalsobebad.Bothissuesofcombiningevidencesfromheuris-ticdistancesandcalibratingthequalityofpseudoreferencesystemscanbeaddressedbyaprobabilis-ticlearningmodel.Inparticular,weuseregressionbecauseitsproblemformulationﬁtsnaturallywiththeobjectiveofMTevaluations.Inregressionlearn-ing,weareinterestedinapproximatingafunctionfthatmapsamulti-dimensionalinputvector,x,toacontinuousrealvalue,y,suchthattheerroroverasetofmtrainingexamples,{(x1,y1),...,(xm,ym)},isminimizedaccordingtoalossfunction.InthecontextofMTevaluation,yisthe“true”quantitativemeasureoftranslationqualityforanin-putsentence1.Thefunctionfrepresentsamathe-maticalmodelofhumanjudgmentsoftranslations;aninputsentenceisrepresentedasafeaturevector,x,whichcontainstheinformationthatcanbeex-tractedfromtheinputsentence(possiblyincludingcomparisonsagainstsomereferencesentences)thatarerelevanttocomputingy.Determiningthesetofrelevantfeaturesforthismodelingison-goingre-1Perhapsevenmoresothangrammaticalityjudgments,thereisvariabilityinpeople’sjudgmentsoftranslationquality.How-ever,likegrammaticalityjudgments,peopledosharesomesim-ilaritiesintheirjudgmentsatacoarse-grainedlevel.Ideally,whatwerefertoasthetruevalueoftranslationalqualityshouldreﬂecttheconsensusjudgmentsofallpeople.299

search.Inthiswork,weconsidersomeofthemorewidelyusedmetricsasfeatures.Ourfullfeaturevectorconsistsofr×18adequacyfeatures,whereristhenumberofreferencesystemsused,and26ﬂuencyfeatures:Adequacyfeatures:Theseincludefeaturesde-rivedfromBLEU(e.g.,n-gramprecision,where1≤n≤5,lengthratios),PER,WER,fea-turesderivedfromMETEOR(precision,recall,fragmentation),andROUGE-relatedfeatures(non-consecutivebigramswithagapsizeofg,where1≤g≤5andlongestcommonsubsequence).Fluencyfeatures:Weconsiderbothstring-levelfeaturessuchascomputingn-gramprecisionagainstatarget-languagecorpusaswellasseveralsyntax-basedfeatures.Weparseeachinputsentenceintoadependencytreeandcomparedaspectsofitagainstalargetarget-languagedependencytreebank.Inaddi-tiontoadaptingtheideaofHeadWordChains(LiuandGildea,2005),wealsocomparedtheinputsen-tence’sargumentstructuresagainstthetreebankforcertainsyntacticcategories.Duetothelargefeaturespacetoexplore,wechosetoworkwithsupportvectorregressionasthelearningalgorithm.Asitslossfunction,supportvec-torregressionusesan-insensitiveerrorfunction,whichallowsforerrorswithinamarginofasmallpositivevalue,,tobeconsideredashavingzeroer-ror(cf.Bishop(2006),pp.339-344).Likeitsclassi-ﬁcationcounterpart,thisisakernel-basedalgorithmthatﬁndssparsesolutionssothatscoresfornewtestinstancesareefﬁcientlycomputedbasedonasubsetofthemostinformativetrainingexamples.Inthiswork,Gaussiankernelsareused.Thecostofregressionlearningisthatitrequirestrainingexamplesthataremanuallyassessedbyhu-manjudges.However,comparedtothecostofcre-atingnewreferenceswhenevernew(test)sentencesareevaluated,theeffortofcreatinghumanassess-menttrainingdataisalimited(ideally,one-time)cost.Moreover,thereisalreadyasizablecollectionofhumanassesseddataforarangeofMTsystemsthroughmultipleyearsoftheNISTMTEvalefforts.Ourexperimentssuggestthatthereisenoughas-sesseddatatotraintheproposedregressionmodel.Asidefromreducingthecostofdevelopinghu-manreferencetranslations,theproposedmetricalsoprovidesanalternativeperspectiveonautomaticMTevaluationthatmaybeinformativeinitsownright.Weconjecturethatametricthatcomparesinputsagainstadiversepopulationofdifferentlyimperfectsentencesmaybemorediscriminativeinjudgingtranslationsystemsthansolelycomparingagainstgoldstandards.Thatis,twosentencesmaybeconsideredequallybadfromtheperspectiveofagoldstandard,butsubtledifferencesbetweenthemmaybecomemoreprominentiftheyarecomparedagainstsentencesintheirpeergroup.4ExperimentsWeconductedexperimentstodeterminethefeasibil-ityoftheproposedapproachandtoaddressthefol-lowingquestions:(1)Howinformativearepseudoreferencesin-and-ofthemselves?Doesvaryingthenumberand/orthequalityofthereferenceshaveanimpactonthemetrics?(2)Whatarethecontribu-tionsoftheadequacyfeaturesversustheﬂuencyfea-turestothelearning-basedmetric?(3)Howdothequalityanddistributionofthetrainingexamples,to-getherwiththequalityofthepseudoreferences,im-pactthemetrictraining?(4)Dothesefactorsimpactthemetric’sabilityinassessingsentencesproducedwithinasingleMTsystem?Howdoesthatsystem’squalityaffectmetricperformance?4.1DatapreparationandExperimentalSetupTheimplementationofsupportvectorregressionusedfortheseexperimentsisSVM-Light(Joachims,1999).Weperformedallexperimentsusingthe2004NISTChineseMTEvaldataset.Itconsistsof447sourcesentencesthatweretranslatedbyfourhu-mantranslatorsaswellastenMTsystems.Eachmachinetranslatedsentencewasevaluatedbytwohumanjudgesfortheirﬂuencyandadequacyona5-pointscale2.Toremovethebiasinthedistribu-tionsofscoresbetweendifferentjudges,wefollowthenormalizationproceduredescribedbyBlatzetal.(2003).Thetwojudge’stotalscores(i.e.,sumofthenormalizedﬂuencyandadequacyscores)arethenaveraged.2TheNISThumanjudgesusehumanreferencetranslationswhenmakingassessments;however,ourapproachisgenerallyapplicablewhenthejudgesarebilingualspeakerswhocomparesourcesentenceswithtranslationoutputs.300

WechosetoworkwiththisNISTdatasetbecauseitcontainsnumeroussystemsthatspanoverarangeofperformancelevels(seeTable1forarankingofthesystemsandtheiraveragedhumanassessmentscores).Thisallowsustohavecontroloverthevari-abilityoftheexperimentswhileansweringtheques-tionsweposedabove(suchasthequalityofthesys-temsprovidingthepseudoreferences,thequalityofMTsystemsbeingevaluated,andthediversityoverthedistributionoftrainingexamples).Speciﬁcally,wereservedfoursystems(MT2,MT5,MT6,andMT9)fortheroleofpseudoref-erences.Sentencesproducedbytheremainingsixsystemsareusedasevaluativedata.Thissetin-cludesthebestandworstsystemssothatwecanseehowwellthemetricsperformsonsentencesthatarebetter(orworse)thanthepseudoreferences.Met-ricsthatrequirenolearningaredirectlyappliedontoallsentencesoftheevaluativeset.Forthelearning-basedmetrics,weperformsix-foldcrossvalidationontheevaluativedataset.Eachfoldconsistsofsen-tencesfromoneMTsystem.Inaroundrobinfash-ion,eachfoldservesasthetestsetwhiletheotherﬁveareusedfortrainingandheldout.Thus,thetrainedmodelshaveseenneitherthetestinstancesnorotherinstancesfromtheMTsystemthatpro-ducedthem.AmetricisevaluatedbasedonitsSpearmanrankcorrelationcoefﬁcientbetweenthescoresitgavetotheevaluativedatasetandhumanassessmentsforthesamedata.Thecorrelationcoefﬁcientisarealnumberbetween-1,indicatingperfectnegativecor-relations,and+1,indicatingperfectpositivecor-relations.Tocomparetherelativequalityofdif-ferentmetrics,weapplybootstrappingre-samplingonthedata,andthenusepairedt-testtodeter-minethestatisticalsigniﬁcanceofthecorrelationdifferences(Koehn,2004).Fortheresultswere-port,unlessexplicitlymentioned,allstatedcompar-isonsarestatisticallysigniﬁcantwith99.8%con-ﬁdence.Weincludetwostandardreference-basedmetrics,BLEUandMETEOR,asbaselinecompar-isons.BLEUissmoothed(LinandOch,2004b),anditconsidersonlymatchinguptobigramsbecausethishashighercorrelationswithhumanjudgmentsthanwhenhigher-orderedn-gramsareincluded.SysIDHuman-assessmentscoreMT10.661MT20.626MT30.586MT40.578MT50.537MT60.530MT70.530MT80.375MT90.332MT100.243Table1:Thehuman-judgedqualityoftenpartici-patingsystemsintheNIST2004ChineseMTEval-uation.Weusedfoursystemsasreferences(high-lightedinboldface)andthedatafromtheremainingsixfortrainingandevaluation.4.2PseudoReferenceVariationsvs.MetricsWeﬁrstcomparedifferentmetrics’performanceonthesix-systemevaluativedatasetunderdifferentconﬁgurationsofhumanand/orpseudoreferences.Forthecasewhenonlyonehumanreferenceisused,thereferencewaschosenatrandomfromthe2004NISTEvaldataset3.ThecorrelationresultsontheevaluativedatasetaresummarizedinTable2.Sometrendsareasexpected:comparingwithinametric,havingfourreferencesisbetterthanhavingjustone;havinghumanreferencesisbetterthananequalnumberofsystemreferences;havingahighqualitysystemasreferenceisbetterthanonewithlowquality.Perhapsmoresurprisingistheconsis-tenttrendthatmetricsdosigniﬁcantlybetterwithfourMTreferencesthanwithonehumanreference,andtheydoalmostaswellasusingfourhumanref-erences.Theresultsshowthatpseudoreferencesareinformative,asstandardmetricswereabletomakeuseofthepseudoreferencesandachievehighercor-relationsthanjudgingfromﬂuencyalone.How-ever,highercorrelationsareachievedwhenlearningwithregression,suggestingthatthetrainedmetricsarebetteratinterpretingcomparisonsagainstpseudoreferences.Comparingwithineachreferenceconﬁguration,theregression-trainedmetricthatincludesbothad-3Onerevieweraskedaboutthequalitythishuman’strans-lations.Althoughwewerenotgivenofﬁcialrankingsofthehumanreferences,wecomparedeachpersonagainsttheotherthreeusingMTevaluationmetricsandfoundthisparticulartranslatortorankthird,thoughthequalityofallfouraresig-niﬁcantlyhigherthaneventhebestMTsystems.301

equacyandﬂuencyfeaturesalwayshasthehighestcorrelations.Ifthemetricconsistsofonlyadequacyfeatures,itsperformancedegradeswiththedecreas-ingqualityofthereferences.Atanotherextreme,ametricbasedonlyonﬂuencyfeatureshasanover-allcorrelationrateof0.459,whichislowerthanmostcorrelationsreportedinTable2.Thisconﬁrmstheimportanceofmodelingadequacy;evenasin-glemid-qualityMTsystemmaybeaninformativepseudoreference.Finally,wenotethataregression-trainedmetricwiththefullfeaturessetthatcom-paresagainst4pseudoreferenceshasahighercor-relationthanBLEUwithfourhumanreferences.Theseresultssuggestthatthefeedbackfromthehu-manassessedtrainingexampleswasabletohelpthelearningalgorithmtocombinedifferentfeaturestoformabettercompositemetric.4.3Sentence-LevelEvaluationonSingleSystemsToexploretheinteractionbetweenthequalityofthereferenceMTsystemsandthatofthetestMTsystems,wefurtherstudythefollowingpseudoref-erenceconﬁgurations:allfoursystems,ahigh-qualitysystemwithamediumqualitysystem,twosystemsofmedium-quality,onemediumwithonepoorsystem,andonlythehigh-qualitysystem.Foreachpseudoreferenceconﬁguration,weconsiderthreemetrics:BLEU,METEOR,andtheregression-trainedmetric(usingthefullfeatureset).Eachmetricevaluatessentencesfromfourtestsystemsofvaryingquality:thebestsysteminthedataset(MT1),theworstintheset(MT10),andtwomid-rangedsystems(MT4andMT7).ThecorrelationcoefﬁcientsaresummarizedinTable3.Eachrowspeciﬁesametric/reference-typecombination;eachcolumnspeciﬁesanMTsystembeingevaluated(us-ingsentencesfromallothersystemsastrainingex-amples).Theﬂuency-onlymetricandstandardmet-ricsusingfourhumanreferencesarebaselines.Theoveralltrendsatthedatasetlevelgenerallyalsoholdfortheper-systemcomparisons.WiththeexceptionoftheevaluationofMT10,regression-basedmetricsalwayshashighercorrelationsthanstandardmetricsthatusethesamereferencecon-ﬁguration(comparingcorrelationcoefﬁcientswithineachcell).WhenthebestMTreferencesystem(MT2)isincludedaspseudoreferences,regression-basedmetricsaretypicallybetterthanornotstatisti-callydifferentfromstandardapplicationsofBLEUandMETEORwith4humanreferences.Usingthetwomid-qualityMTsystemsasreferences(MT5andMT6),regressionmetricsyieldcorrelationsthatareonlyslightlylowerthanstandardmetricswithhumanreferences.Theseresultssupportourcon-jecturethatcomparingagainstmultiplesystemsisinformative.Thepoorerperformancesoftheregression-basedmetricsonMT10pointoutanasymmetryinthelearningapproach.Theregressionmodelaimstolearnafunctionthatapproximateshumanjudgmentsoftranslatedsentencesthroughtrainingexamples.InthespaceofallpossibleMToutputs,theneigh-borhoodofgoodtranslationsismuchsmallerthanthatofbadtranslations.Thus,aslongastheregres-sionmodelsseessomeexamplesofsentenceswithhighassessmentscoresduringtraining,itshouldhaveamuchbetterestimationofthecharacteristicsofgoodtranslations.Thisideaissupportedbytheexperimentaldata.Considerthescenarioofeval-uatingMT1whileusingtwomid-qualityMTsys-temsasreferences.Althoughthereferencesystemsarenotashighqualityasthesystemunderevalu-ation,andalthoughthetrainingexamplesshowntotheregressionmodelwerealsogeneratedbysystemswhoseoverallqualitywasratedlower,thetrainedmetricwasreasonablygoodatrankingsentencesproducedbyMT1.Incontrast,thetaskofevaluatingsentencesfromMT10ismoredifﬁcultforthelearn-ingapproach,perhapsbecauseitissufﬁcientlydif-ferentfromalltrainingandreferencesystems.Cor-relationsmightbeimprovedwithadditionalrefer-encesystems.4.4DiscussionsThedesignoftheseexperimentsaimstosimulatepracticalsituationstouseourproposedmetrics.Forthemorefrequentlyencounteredlanguagepairs,itshouldbepossibletoﬁndatleasttwomid-quality(orbetter)MTsystemstoserveaspseudorefer-ences.Forexample,onemightusecommercialoff-the-shelfsystems,someofwhicharefreeovertheweb.Forlesscommonlyusedlanguages,onemightuseopensourceresearchsystems(Al-Onaizanetal.,1999;Burbanketal.,2005).Datasetsfromformalevaluationeventssuchas302

Reftypeand#RefSys.BLEU-S(2)METEORRegr(adq.only)Regr(full)4Humansallhumans0.6280.5910.5880.6441HumanHRef#30.5360.5120.4870.5974SystemsallMTRefs0.6140.5830.5840.6322SystemsBest2MTRefs0.6030.5770.5730.620Mid2MTRefs0.5790.5550.5280.608Worst2MTRefs0.5410.5080.4670.5811SystemBestMTRef0.5760.5590.5340.596MidMTRef(MT5)0.5380.5280.4740.577WorstMTRef0.3710.3290.1510.495Table2:Comparisonsofmetrics(columns)usingdifferenttypesofreferences(rows).Thefullregression-trainedmetrichasthehighestcorrelation(showninboldface)whenfourhumanreferencesareused;ithasthesecondhighestcorrelationrate(showninitalic)whenfourMTsystemreferencesareusedinstead.Aregression-trainedmetricwithonlyﬂuencyfeatureshasacorrelationcoefﬁcientof0.459.RefTypeMetricMT-1MT-4MT-7MT-10NorefRegr.0.3670.3160.301-0.0454humanrefsRegr.0.538*0.473*0.459*0.247BLEU-S(2)0.4660.4190.3970.321*METEOR0.4640.4180.4100.3124MTRefsRegr.0.4980.4290.4210.243BLEU-S(2)0.3860.3490.4040.240METEOR0.4450.3540.3330.243Best2MTRefsRegr.0.4920.4180.4030.201BLEU-S(2)0.3910.3300.3940.268METEOR0.4300.3330.3270.267Mid2MTRefsRegr.0.4500.4130.3880.219BLEU-S(2)0.3620.3140.3100.282METEOR0.3910.3150.2840.274Worst2MTRefsRegr.0.4300.3860.3650.158BLEU-S(2)0.3200.2980.3160.223METEOR0.3510.3060.3020.228BestMTRefRegr.0.4610.4010.4140.122BLEU-S(2)0.3710.3300.3800.242METEOR0.3750.3180.3920.283Table3:Correlationcomparisonsofmetricsbytestsystems.Foreachtestsystem(columns)theoverallhighestcorrelationsisdistinguishedbyanasterisk(*);correlationshigherthanstandardmetricsusinghuman-referencesarehighlightedinboldface;thosethatarestatisticallycomparabletothemareitalicized.NISTMTEvals,whichcontainshumanassessedMToutputsforavarietyofsystems,canbeusedfortrainingexamples.Alternatively,onemightdi-rectlyrecruithumanjudgestoassesssamplesen-tencesfromthesystem(s)tobeevaluated.Thisshouldresultinbettercorrelationsthanwhatwere-portedhere,sincethehumanassessedtrainingex-ampleswillbemoresimilartothetestinstancesthanthesetupinourexperiments.IndevelopingnewMTsystems,pseudorefer-encesmaysupplementthesinglehumanreferencetranslationsthatcouldbeextractedfromaparalleltext.UsingthesamesetupasExp.1(seeTable2),addingpseudoreferencesdoesimprovecorrelations.Addingfourpseudoreferencestothesinglehumanreferenceraisesthecorrelationcoefﬁcientto0.650(from0.597)fortheregressionmetric.Addingthemtofourhumanreferencesresultsinacorrelationco-efﬁcientof0.660(from0.644)4.5ConclusionInthispaper,wehavepresentedamethodforde-velopingsentence-levelMTevaluationmetricswith-outusinghumanreferences.Weshowedthatbylearningfromhumanassessedtrainingexamples,4BLEUwithfourhumanreferenceshasacorrelationof0.628.AddingfourpseudoreferencesincreasesBLEUto0.650.303

theregression-trainedmetriccanevaluateaninputsentencebycomparingitagainstmultiplemachine-generatedpseudoreferencesandothertargetlan-guageresources.Ourexperimentalresultssuggestthattheresultingmetricsarerobustevenwhenthesentencesunderevaluationarefromasystemofhigherqualitythanthesystemsservingasrefer-ences.Weobservethatregressionmetricsthatusemultiplepseudoreferencesoftenhavecomparableorhighercorrelationrateswithhumanjudgmentsthanstandardreference-basedmetrics.Ourstudysuggeststhatinconjunctionwithregressiontraining,multipleimperfectreferencesmaybeasinformativeasgold-standardreferences.AcknowledgmentsThisworkhasbeensupportedbyNSFGrantsIIS-0612791andIIS-0710695.WewouldliketothankRicCrabbe,DanGildea,AlonLavie,StuartShieber,andNoahSmithandtheanonymousreviewersfortheirsuggestions.WearealsogratefultoNISTformakingtheirassessmentdataavailabletous.ReferencesYaserAl-Onaizan,JanCurin,MichaelJahr,KevinKnight,JohnLafferty,I.DanMelamed,Franz-JosefOch,DavidPurdy,NoahA.Smith,andDavidYarowsky.1999.Statisticalmachinetranslation.Technicalreport,JHU.citeseer.nj.nec.com/al-onaizan99statistical.html.JoshuaS.AlbrechtandRebeccaHwa.2007.Are-examinationofmachinelearningapproachesforsentence-levelMTeval-uation.InProceedingsofthe45thAnnualMeetingoftheAssociationforComputationalLinguistics(ACL-2007).SatanjeevBanerjeeandAlonLavie.2005.Meteor:Anauto-maticmetricforMTevaluationwithimprovedcorrelationwithhumanjudgments.InACL2005WorkshoponIntrinsicandExtrinsicEvaluationMeasuresforMachineTranslationand/orSummarization,June.ChristopherM.Bishop.2006.PatternRecognitionandMa-chineLearning.SpringerVerlag.JohnBlatz,ErinFitzgerald,GeorgeFoster,SimonaGandrabur,CyrilGoutte,AlexKulesza,AlbertoSanchis,andNicolaUefﬁng.2003.Conﬁdenceestimationformachinetrans-lation.TechnicalReportNaturalLanguageEngineeringWorkshopFinalReport,JohnsHopkinsUniversity.AndreaBurbank,MarineCarpuat,StephenClark,MarkusDreyer,DeclanGrovesPamela.Fox,KeithHall,MaryHearne,I.DanMelamed,YihaiShen,AndyWay,BenWellington,andDekaiWu.2005.Finalreportofthe2005languageengineeringworkshoponstatisticalmachinetrans-lationbyparsing.TechnicalReportNaturalLanguageEngi-neeringWorkshopFinalReport,”JHU”.SimonCorston-Oliver,MichaelGamon,andChrisBrockett.2001.Amachinelearningapproachtotheautomaticeval-uationofmachinetranslation.InProceedingsofthe39thAnnualMeetingoftheAssociationforComputationalLin-guistics,July.MichaelGamon,AnthonyAue,andMartineSmets.2005.Sentence-levelMTevaluationwithoutreferencetranslations:Beyondlanguagemodeling.InEuropeanAssociationforMachineTranslation(EAMT),May.ThorstenJoachims.1999.Makinglarge-scaleSVMlearningpractical.InBernhardSch¨oelkopf,ChristopherBurges,andAlexanderSmola,editors,AdvancesinKernelMethods-SupportVectorLearning.MITPress.PhilippKoehn.2004.Statisticalsigniﬁcancetestsformachinetranslationevaluation.InProceedingsofthe2004Confer-enceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP-04).AlexKuleszaandStuartM.Shieber.2004.Alearningap-proachtoimprovingsentence-levelMTevaluation.InPro-ceedingsofthe10thInternationalConferenceonTheoreticalandMethodologicalIssuesinMachineTranslation(TMI),Baltimore,MD,October.Chin-YewLinandFranzJosefOch.2004a.Automaticevalu-ationofmachinetranslationqualityusinglongestcommonsubsequenceandskip-bigramstatistics.InProceedingsofthe42ndAnnualMeetingoftheAssociationforComputa-tionalLinguistics,July.Chin-YewLinandFranzJosefOch.2004b.Orange:amethodforevaluatingautomaticevaluationmetricsforma-chinetranslation.InProceedingsofthe20thInternationalConferenceonComputationalLinguistics(COLING2004),August.DingLiuandDanielGildea.2005.Syntacticfeaturesforevaluationofmachinetranslation.InACL2005WorkshoponIntrinsicandExtrinsicEvaluationMeasuresforMachineTranslationand/orSummarization,June.DingLiuandDanielGildea.2007.Source-languagefeaturesandmaximumcorrelationtrainingformachinetranslationevaluation.InProceedingsoftheHLT/NAACL-2007,April.KishorePapineni,SalimRoukos,ToddWard,andWei-JingZhu.2002.Bleu:amethodforautomaticevaluationofma-chinetranslation.InProceedingsofthe40thAnnualMeetingoftheAssociationforComputationalLinguistics,Philadel-phia,PA.ChristopherQuirk.2004.Trainingasentence-levelmachinetranslationconﬁdencemeasure.InProceedingsofLREC2004.NicolaUefﬁng,KlausMacherey,andHermannNey.2003.Conﬁdencemeasuresforstatisticalmachinetranslation.InMachineTranslationSummitIX,pages394–401,September.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 304–311,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

304

BootstrappingWordAlignmentviaWordPackingYanjunMa,NicolasStroppa,AndyWaySchoolofComputingDublinCityUniversityGlasnevin,Dublin9,Ireland{yma,nstroppa,away}@computing.dcu.ieAbstractWeintroduceasimplemethodtopackwordsforstatisticalwordalignment.Ourgoalistosimplifythetaskofautomaticwordalign-mentbypackingseveralconsecutivewordstogetherwhenwebelievetheycorrespondtoasinglewordintheoppositelanguage.Thisisdoneusingthewordaligneritself,i.e.bybootstrappingonitsoutput.WeevaluatetheperformanceofourapproachonaChinese-to-Englishmachinetranslationtask,andreporta12.2%relativeincreaseinBLEUscoreoverastate-of-theartphrase-basedSMTsystem.1IntroductionAutomaticwordalignmentcanbedeﬁnedastheproblemofdeterminingatranslationalcorrespon-denceatwordlevelgivenaparallelcorpusofalignedsentences.Mostcurrentstatisticalmodels(Brownetal.,1993;Vogeletal.,1996;DengandByrne,2005)treatthealignedsentencesinthecorpusasse-quencesoftokensthataremeanttobewords;thegoalofthealignmentprocessistoﬁndlinksbe-tweensourceandtargetwords.Beforeapplyingsuchaligners,wethusneedtosegmentthesentencesintowords–ataskwhichcanbequitehardforlan-guagessuchasChineseforwhichwordboundariesarenotorthographicallymarked.Moreimportantly,however,thissegmentationisoftenperformedinamonolingualcontext,whichmakesthewordalign-menttaskmoredifﬁcultsincedifferentlanguagesmayrealizethesameconceptusingvaryingnum-bersofwords(seee.g.(Wu,1997)).Moreover,asegmentationconsideredtobe“good”fromamono-lingualpointofviewmaybeunadaptedfortrainingalignmentmodels.Althoughsomestatisticalalignmentmodelsal-lowfor1-to-nwordalignmentsforthosereasons,theyrarelyquestionthemonolingualtokenizationandthebasicunitofthealignmentprocessremainstheword.Inthispaper,wefocuson1-to-nalign-mentswiththegoalofsimplifyingthetaskofauto-maticwordalignersbypackingseveralconsecutivewordstogetherwhenwebelievetheycorrespondtoasinglewordintheoppositelanguage;byidentifyingenoughsuchcases,wereducethenumberof1-to-nalignments,thusmakingthetaskofwordalignmentbotheasierandmorenatural.Ourapproachconsistsofusingtheoutputfromanexistingstatisticalwordalignertoobtainasetofcandidatesforwordpacking.Weevaluatethere-liabilityofthesecandidates,usingsimplemetricsbasedonco-occurencefrequencies,similartothoseusedinassociativeapproachestowordalignment(KitamuraandMatsumoto,1996;Melamed,2000;Tiedemann,2003).Wethenmodifythesegmenta-tionofthesentencesintheparallelcorpusaccord-ingtothispackingofwords;thesemodiﬁedsen-tencesarethengivenbacktothewordaligner,whichproducesnewalignments.WeevaluatethevalidityofourapproachbymeasuringtheinﬂuenceofthealignmentprocessonaChinese-to-EnglishMachineTranslation(MT)task.Theremainderofthispaperisorganizedasfol-lows.InSection2,westudythecaseof1-to-nwordalignment.Section3introducesanauto-maticmethodtopacktogethergroupsofconsecutive305

1:01:11:21:31:n(n>3)IWSLTChinese–English21.6463.769.493.361.75IWSLTEnglish–Chinese29.7757.4710.031.651.08IWSLTItalian–English13.7172.879.773.230.42IWSLTEnglish–Italian20.4571.087.020.90.55EuroparlDutch–English24.7167.045.351.41.5EuroparlEnglish–Dutch23.7669.074.851.21.12Table1:Distributionofalignmenttypesfordifferentlanguagepairs(%)wordsbasedontheoutputfromawordaligner.InSection4,theexperimentalsettingisdescribed.InSection5,weevaluatetheinﬂuenceofourmethodonthealignmentprocessonaChinesetoEnglishMTtask,andexperimentalresultsarepresented.Section6concludesthepaperandgivesavenuesforfuturework.2TheCaseof1-to-nAlignmentThesameconceptcanbeexpressedindifferentlan-guagesusingvaryingnumbersofwords;forexam-ple,asingleChinesewordmaysurfaceasacom-poundoracollocationinEnglish.Thisisfre-quentforlanguagesasdifferentasChineseandEn-glish.Toquickly(andapproximately)evaluatethisphenomenon,wetrainedthestatisticalIBMword-alignmentmodel4(Brownetal.,1993),1usingtheGIZA++software(OchandNey,2003)forthefol-lowinglanguagepairs:Chinese–English,Italian–English,andDutch–English,usingtheIWSLT-2006corpus(Takezawaetal.,2002;Paul,2006)fortheﬁrsttwolanguagepairs,andtheEuroparlcorpus(Koehn,2005)forthelastone.Theseasymmet-ricmodelsproduce1-to-nalignments,withn≥0,inbothdirections.Here,itisimportanttomentionthatthesegmentationofsentencesisperformedto-tallyindependentlyofthebilingualalignmentpro-cess,i.e.itisdoneinamonolingualcontext.ForEu-ropeanlanguages,weapplythemaximum-entropybasedtokenizerofOpenNLP2;theChinesesen-tenceswerehumansegmented(Paul,2006).InTable1,wereportthefrequenciesofthedif-ferenttypesofalignmentsforthevariouslanguagesanddirections.Asexpected,thenumberof1:n1Morespeciﬁcally,weperformed5iterationsofModel1,5iterationsofHMM,5iterationsofModel3,and5iterationsofModel4.2http://opennlp.sourceforge.net/.alignmentswithn6=1ishighforChinese–English(’40%),andsigniﬁcantlyhigherthanfortheEu-ropeanlanguages.Thecaseof1-to-nalignmentsis,therefore,obviouslyanimportantissuewhendeal-ingwithChinese–Englishwordalignment.32.1TheTreatmentof1-to-nAlignmentsFertility-basedmodelssuchasIBMmodels3,4,and5allowforalignmentsbetweenonewordandsev-eralwords(1-to-nor1:nalignmentsinwhatfol-lows),inparticularforthereasonsspeciﬁedabove.TheycanbeseenasextensionsofthesimplerIBMmodels1and2(Brownetal.,1993).Similarly,DengandByrne(2005)proposeanHMMframe-workcapableofdealingwith1-to-nalignment,whichisanextensionoftheoriginalmodelof(Vogeletal.,1996).However,thesemodelsrarelyquestionthemono-lingualtokenization,i.e.thebasicunitofthealign-mentprocessistheword.4Onealternativetoex-tendingtheexpressivityofonemodel(andusuallyitscomplexity)istofocusontheinputrepresenta-tion;inparticular,wearguethatthealignmentpro-cesscanbeneﬁtfromasimpliﬁcationoftheinput,whichconsistsoftryingtoreducethenumberof1-to-nalignmentstoconsider.Notethattheneedtoconsidersegmentationandalignmentatthesametimeisalsomentionedin(Tiedemann,2003),andrelatedissuesarereportedin(Wu,1997).2.2NotationWhileinthispaper,wefocusonChinese–English,themethodproposedisapplicabletoanylanguage3Notethata1:0alignmentmaydenoteafailuretocapturea1:nalignmentwithn>1.4Interestingly,thisisactuallyeventhecaseforapproachesthatdirectlymodelalignmentsbetweenphrases(MarcuandWong,2002;Birchetal.,2006).306

pair–evenforcloselyrelatedlanguages,weex-pectimprovementstobeseen.Thenotationhow-everassumeChinese–EnglishMT.GivenaChi-nesesentencecJ1consistingofJwords{c1,...,cJ}andanEnglishsentenceeI1consistingofIwords{e1,...,eI},AC→E(resp.AE→C)willdenoteaChinese-to-English(resp.anEnglish-to-Chinese)wordalignmentbetweencJ1andeI1.Sinceweareprimarilyinterestedin1-to-nalignments,AC→Ecanberepresentedasasetofpairsaj=hcj,EjidenotingalinkbetweenonesingleChinesewordcjandafewEnglishwordsEj(andsimilarlyforAE→C).ThesetEjisemptyifthewordcjisnotalignedtoanywordineI1.3AutomaticWordRepackingOurapproachconsistsofpackingconsecutivewordstogetherwhenwebelievetheycorrespondtoasin-glewordintheotherlanguage.Thisbilinguallymotivatedpackingofwordschangesthebasicunitofthealignmentprocess,andsimpliﬁesthetaskofautomaticwordalignment.Wethusminimizethenumberof1-to-nalignmentsinordertoobtainmorecomparablesegmentationsinthetwolanguages.Inthissection,wepresentanautomaticmethodthatbuildsupontheoutputfromanexistingautomaticwordaligner.Morespeciﬁcally,we(i)useawordalignertoobtain1-to-nalignments,(ii)extractcan-didatesforwordpacking,(iii)estimatethereliabilityofthesecandidates,(iv)replacethegroupsofwordstopackbyasingletokenintheparallelcorpus,and(v)re-iteratethealignmentprocessusingtheup-datedcorpus.Theﬁrstthreestepsareperformedinbothdirections,andproducetwobilingualdic-tionaries(source-targetandtarget-source)ofgroupsofwordstopack.3.1CandidateExtractionInthefollowing,weassumetheavailabilityofanautomaticwordalignerthatcanoutputalignmentsAC→EandAE→Cforanysentencepair(cJ1,eI1)inaparallelcorpus.WealsoassumethatAC→EandAE→Ccontain1:nalignments.Ourmethodforrepackingwordsisverysimple:wheneverasinglewordisalignedwithseveralconsecutivewords,theyareconsideredcandidatesforrepacking.Formally,givenanalignmentAC→EbetweencJ1andeI1,ifaj=hcj,Eji∈AC→E,withEj={ej1,...,ejm}and∀k∈J1,m−1K,jk+1−jk=1,thenthealign-mentajbetweencjandthesequenceofwordsEjisconsideredacandidateforwordrepacking.ThesamegoesforAE→C.Someexamplesofsuch1-to-nalignmentsbetweenChineseandEnglish(inbothdirections)wecanderiveautomaticallyaredis-playedinFigure1.白葡萄酒: white wine百货公司: department store抱歉: excuse me报警: call the police杯: cup of必须: have toclosest: 最 近fifteen: 十 五fine: 很 好flight: 次 航班 get: 拿 到here:  在 这里Figure1:Exampleof1-to-nwordalignmentsbe-tweenChineseandEnglish3.2CandidateReliabilityEstimationOfcourse,theprocessdescribedaboveiserror-proneandifwewanttochangetheinputtogivetothewordaligner,weneedtomakesurethatwearenotmakingharmfulmodiﬁcations.5Wethusaddi-tionallyevaluatethereliabilityofthecandidatesweextractandﬁlterthembeforeinclusioninourbilin-gualdictionary.Toperformthisﬁltering,weusetwosimplestatisticalmeasures.Inthefollowing,aj=hcj,Ejidenotesacandidate.Theﬁrstmeasureweconsiderisco-occurrencefrequency(COOC(cj,Ej)),i.e.thenumberoftimescjandEjco-occurinthebilingualcorpus.Thisverysimplemeasureisfrequentlyusedinas-sociativeapproaches(Melamed,1997;Tiedemann,2003).Thesecondmeasureisthealignmentconﬁ-dence,deﬁnedasAC(aj)=C(aj)COOC(cj,Ej),whereC(aj)denotesthenumberofalignmentspro-posedbythewordalignerthatareidenticaltoaj.Inotherwords,AC(aj)measureshowoftenthe5Consequently,ifwecompareourapproachtotheproblemofcollocationidentiﬁcation,wemaysaythatwearemorein-terestedinprecisionthanrecall(Smadjaetal.,1996).However,notethatourgoalisnotrecognizingspeciﬁcsequencesofwordssuchascompoundsorcollocations;itismaking(bilinguallymotivated)changesthatsimplifythealignmentprocess.307

aligneralignscjandEjwhentheyco-occur.Wealsoimposethat|Ej|≤k,wherekisaﬁxedinte-gerthatmaydependonthelanguagepair(between3and5inpractice).Therationalebehindthisisthatitisveryraretogetreliablealignmentbetweenonewordandkconsecutivewordswhenkishigh.Thecandidatesareincludedinourbilingualdic-tionaryifandonlyiftheirmeasuresareabovesomeﬁxedthresholdstcoocandtac,whichallowforthecontrolofthesizeofthedictionaryandthequalityofitscontents.Someothermeasures(includingtheDicecoefﬁcient)couldbeconsidered;however,ithastobenotedthatwearemoreinterestedhereintheﬁlteringthaninthediscoveryofalignment,sinceourmethodbuildsuponanexistingaligner.More-over,wewillseethateventhesesimplemeasurescanleadtoanimprovementofthealignmentpro-cessinaMTcontext(cf.Section5).3.3BootstrappedWordRepackingOncethecandidatesareextracted,werepackthewordsinthebilingualdictionariesconstructedusingthemethoddescribedabove;thisprovidesuswithanupdatedtrainingcorpus,inwhichsomewordse-quenceshavebeenreplacedbyasingletoken.Thisupdateistotallynaive:ifanentryaj=hcj,Ejiispresentinthedictionaryandmatchesonesentencepair(cJ1,eI1)(i.e.cjandEjarerespectivelycon-tainedincJ1andeI1),thenwereplacethesequenceofwordsEjwithasingletokenwhichbecomesanewlexicalunit.6NotethatthisreplacementoccursevenifnoalignmentwasfoundbetweencjandEjforthepair(cJ1,eI1).Thisismotivatedbythefactthattheﬁlteringdescribedaboveisquiteconserva-tive;wetrusttheentryaitobecorrect.Thisupdateisperformedinbothdirections.Itisthenpossibletorunthewordalignerusingtheupdated(simpliﬁed)parallelcorpus,inordertogetnewalignments.Byperformingadeterministicwordpacking,weavoidthecomputationofthefertilityparametersassoci-atedwithfertility-basedmodels.Wordpackingcanbeappliedseveraltimes:oncewehavegroupedsomewordstogether,theybecomethenewbasicunittoconsider,andwecanre-runthesamemethodtogetadditionalgroupings.How-6Incaseofoverlapbetweenseveralgroupsofwordstore-place,weselecttheonewithhighestconﬁdence(accordingtotac).ever,wehavenotseeninpracticemuchbeneﬁtfromrunningitmorethantwice(fewnewcandidatesareextractedaftertwoiterations).Itisalsoimportanttonotethatthisprocessisbilinguallymotivatedandstronglydependsonthelanguagepair.Forexample,whitewine,excuseme,callthepolice,andcupof(cf.Figure1)translatere-spectivelyasvinblanc,excusez-moi,appellezlapo-lice,andtassedeinFrench.ThosegroupingswouldnotbefoundforalanguagepairsuchasFrench–English,whichisconsistentwiththefactthattheyarelessusefulforFrench–EnglishthanforChinese–EnglishinaMTperspective.3.4UsingManuallyDevelopedDictionariesWewantedtocomparethisautomaticapproachtomanuallydevelopedresources.Forthispurpose,weusedadictionarybuiltbytheMTgroupofHarbinInstituteofTechnology,asapreprocessingsteptoChinese–Englishwordalignment,andmoti-vatedbyseveralyearsofChinese–EnglishMTprac-tice.SomeexamplesextractedfromthisresourcearedisplayedinFigure2.有: there is 想要: want to不必: need not前面: in front of一: as soon as看: look atFigure2:Examplesofentriesfromthemanuallyde-velopeddictionary4ExperimentalSetting4.1EvaluationTheintrinsicqualityofwordalignmentcanbeas-sessedusingtheAlignmentErrorRate(AER)met-ric(OchandNey,2003),thatcomparesasystem’salignmentoutputtoasetofgold-standardalign-ment.Whilethismethodgivesadirectevaluationofthequalityofwordalignment,itisfacedwithsev-erallimitations.First,itisreallydifﬁculttobuildareliableandobjectivegold-standardset,especiallyforlanguagesasdifferentasChineseandEnglish.Second,anincreaseinAERdoesnotnecessarilyim-plyanimprovementintranslationquality(Liangetal.,2006)andvice-versa(Vilaretal.,2006).The308

relationshipbetweenwordalignmentsandtheirim-pactonMTisalsoinvestigatedin(AyanandDorr,2006;LopezandResnik,2006;FraserandMarcu,2006).Consequently,wechosetoextrinsicallyeval-uatetheperformanceofourapproachviathetransla-tiontask,i.e.wemeasuretheinﬂuenceofthealign-mentprocessontheﬁnaltranslationoutput.ThequalityofthetranslationoutputisevaluatedusingBLEU(Papinenietal.,2002).4.2DataTheexperimentswerecarriedoutusingtheChinese–EnglishdatasetsprovidedwithintheIWSLT2006evaluationcampaign(Paul,2006),ex-tractedfromtheBasicTravelExpressionCorpus(BTEC)(Takezawaetal.,2002).Thismultilingualspeechcorpuscontainssentencessimilartothosethatareusuallyfoundinphrase-booksfortouristsgoingabroad.Trainingwasperformedusingthede-faulttrainingset,towhichweaddedthesetsde-vset1,devset2,anddevset3.7TheEnglishsideofthetestsetwasnotavailableatthetimewecon-ductedourexperiments,sowesplitthedevelopmentset(devset4)intotwoparts:onewaskeptfortesting(200alignedsentences)withtherest(289alignedsentences)usedfordevelopmentpurposes.Asapre-processingstep,theEnglishsentencesweretokenizedusingthemaximum-entropybasedtokenizeroftheOpenNLPtoolkit,andcaseinfor-mationwasremoved.ForChinese,thedatapro-videdweretokenizedaccordingtotheoutputformatofASRsystems,andhuman-corrected(Paul,2006).Sincesegmentationsarehuman-corrected,wearesurethattheyaregoodfromamonolingualpointofview.Table2containsthevariouscorpusstatistics.4.3BaselineWeuseastandardlog-linearphrase-basedstatisticalmachinetranslationsystemasabaseline:GIZA++implementationofIBMwordalignmentmodel4(Brownetal.,1993;OchandNey,2003),8there-ﬁnementandphrase-extractionheuristicsdescribedin(Koehnetal.,2003),minimum-error-ratetraining7Morespeciﬁcally,wechoosetheﬁrstEnglishreferencefromthe7referencesandtheChinesesentencetoconstructnewsentencepairs.8TrainingisperformedusingthesamenumberofiterationsasinSection2.ChineseEnglishTrainSentences41,465Runningwords361,780375,938Vocabularysize11,4279,851Dev.Sentences289(7refs.)Runningwords3,35026,223Vocabularysize8971,331Eval.Sentences200(7refs.)Runningwords1,86414,437Vocabularysize5691,081Table2:Chinese–Englishcorpusstatistics(Och,2003)usingPhramer(Olteanuetal.,2006),a3-gramlanguagemodelwithKneser-Neysmooth-ingtrainedwithSRILM(Stolcke,2002)ontheEn-glishsideofthetrainingdataandPharaoh(Koehn,2004)withdefaultsettingstodecode.Thelog-linearmodelisalsobasedonstandardfeatures:condi-tionalprobabilitiesandlexicalsmoothingofphrasesinbothdirections,andphrasepenalty(ZensandNey,2004).5ExperimentalResults5.1ResultsTheinitialwordalignmentsareobtainedusingthebaselineconﬁgurationdescribedabove.Fromthese,webuildtwobilingual1-to-ndictionaries(oneforeachdirection),andthetrainingcorpusisupdatedbyrepackingthewordsinthedictionaries,usingthemethodpresentedinSection2.Aspreviouslymen-tioned,thisprocesscanberepeatedseveraltimes;ateachstep,wecanalsochoosetoexploitonlyoneofthetwoavailabledictionaries,ifsodesired.Wethenextractalignedphrasesusingthesameprocedureasforthebaselinesystem;theonlydifferenceistheba-sicunitweareconsidering.Oncethephrasesareex-tracted,weperformtheestimationofthefeaturesofthelog-linearmodelandunpackthegroupedwordstorecovertheinitialwords.Finally,minimum-error-ratetraininganddecodingareperformed.Thevariousparametersofthemethod(k,tcooc,tac,cf.Section2)havebeenoptimizedonthedevel-opmentset.Wefoundoutthatitwasenoughtoper-formtwoiterationsofrepacking:theoptimalsetofvalueswasfoundtobek=3,tac=0.5,tcooc=20fortheﬁrstiteration,andtcooc=10forthesecond309

BLEU[%]Baseline15.14n=1.withC-Edict.15.92n=1.withE-Cdict.15.77n=1.withboth16.59n=2.withC-Edict.16.99n=2.withE-Cdict.16.59n=2.withboth16.88Table3:InﬂuenceofwordrepackingonChinese-to-EnglishMTiteration,forbothdirections.9InTable3,wereporttheresultsobtainedonthetestset,wherendenotestheiteration.WeﬁrstconsideredtheinclusionofonlytheChinese–Englishdictionary,thenonlytheEnglish–Chinesedictionary,andthenboth.Aftertheﬁrststep,wecanalreadyseeanim-provementoverthebaselinewhenconsideringoneofthetwodictionaries.Whenusingboth,weob-serveanincreaseof1.45BLEUpoints,whichcor-respondstoa9.6%relativeincrease.Moreover,wecangainfromperforminganotherstep.However,theinclusionoftheEnglish–Chinesedictionaryisharmfulinthiscase,probablybecause1-to-nalign-mentsarelessfrequentforthisdirection,andhavebeencapturedduringtheﬁrststep.ByincludingtheChinese–Englishdictionaryonly,wecanachieveanincreaseof1.85absoluteBLEUpoints(12.2%rela-tive)overtheinitialbaseline.10QualityoftheDictionariesToassessthequal-ityoftheextractionprocedure,wesimplymanu-allyevaluatedtheratioofincorrectentriesinthedictionaries.Afteronestepofwordpacking,theChinese–EnglishandtheEnglish–Chinesedictio-nariesrespectivelycontain7.4%and13.5%incor-rectentries.Aftertwostepsofpacking,theyonlycontain5.9%and10.3%incorrectentries.5.2AlignmentTypesIntuitively,thewordalignmentsobtainedafterwordpackingaremorelikelytobe1-to-1thanbefore.In-9Theparametersk,tac,andtcoocareoptimizedforeachstep,andthealignmentobtainedusingthebestsetofparametersforagivenstepareusedasinputforthefollowingstep.10Notethatthissetting(usingbothdictionariesfortheﬁrststepandonlytheChinesedictionaryforthesecondstep)isalsothebestsettingonthedevelopmentset.deed,thewordsequencesinonelanguagethatusu-allyaligntoonesinglewordintheotherlanguagehavebeengroupedtogethertoformonesingleto-ken.Table4showsthedetailofthedistributionofalignmenttypesafteroneandtwostepsofautomaticrepacking.Inparticular,wecanobservethatthe1:11:01:11:21:31:n(n>3)C-EBase.21.6463.769.493.361.75n=119.6969.436.322.791.78n=219.6771.574.872.121.76E-CBase.29.7757.4710.031.651.08n=126.5961.958.821.551.09n=225.1062.739.381.681.12Table4:Distributionofalignmenttypes(%)alignmentsaremorefrequentaftertheapplicationofrepacking:theratioofthistypeofalignmenthasincreasedby7.81%forChinese–Englishand5.26%forEnglish–Chinese.5.3InﬂuenceofWordSegmentationTotesttheinﬂuenceoftheinitialwordsegmenta-tionontheprocessofwordpacking,weconsideredanadditionalsegmentationconﬁguration,basedonanautomaticsegmentercombiningrule-basedandstatisticaltechniques(Zhaoetal.,2001).BLEU[%]Originalsegmentation15.14Originalsegmentation+Wordpacking16.99Automaticsegmentation14.91Automaticsegmentation+Wordpacking17.51Table5:InﬂuenceofChinesesegmentationTheresultsobtainedaredisplayedinTable5.Asexpected,theautomaticsegmenterleadstoslightlylowerresultsthanthehuman-correctedsegmenta-tion.However,theproposedmethodseemstobebeneﬁcialirrespectiveofthechoiceofsegmentation.Indeed,wecanalsoobserveanimprovementinthenewsetting:2.6pointsabsoluteincreaseinBLEU(17.4%relative).1111Wecouldactuallyconsideranextremecase,whichwouldconsistofsplittingthesentencesintocharacters,i.e.eachchar-acterwouldbeblindlytreatedasoneword.Thesegmentation310

5.4ExploitingManuallyDevelopedResourcesWealsocomparedourtechniqueforautomaticpack-ingofwordswiththeexploitationofmanuallydevelopedresources.Morespeciﬁcally,weuseda1-to-nChinese–Englishbilingualdictionary,de-scribedinSection3.4,anduseditinplaceoftheautomaticallyacquireddictionary.Wordsarethusgroupedaccordingtothisdictionary,andwethenapplythesamewordalignerasforpreviousexperi-ments.Inthiscase,sincewearenotbootstrappingfromtheoutputofawordaligner,thiscanactuallybeseenasapre-processingsteppriortoalignment.Theseresourcesfollowmoreorlessthesamefor-matastheoutputofthewordsegmentermentionedinSection5.1.2(Zhaoetal.,2001),sotheexperi-mentsarecarriedoutusingthissegmentation.BLEU[%]Baseline14.91Automaticwordpacking17.51Packingwith“manual”dictionary16.15Table6:ExploitingmanuallydevelopedresourcesTheresultsobtainedaredisplayedinTable6.Wecanobservethattheuseofthemanuallydevelopeddictionaryprovidesuswithanimprovementintrans-lationquality:1.24BLEUpointsabsolute(8.3%rel-ative).However,theredoesnotseemtobeacleargainwhencomparedwiththeautomaticmethod.Evenifthosemanualresourceswereextended,wedonotbelievetheimprovementissufﬁcientenoughtojustifythisadditionaleffort.6ConclusionandFutureWorkInthispaper,wehaveintroducedasimpleyeteffec-tivemethodtopackwordstogetherinordertogiveadifferentandsimpliﬁedinputtoautomaticwordaligners.Weuseabootstrapapproachinwhichweﬁrstextract1-to-nwordalignmentsusinganexist-ingwordaligner,andthenestimatetheconﬁdenceofthosealignmentstodecidewhetherornotthenwordshavetobegrouped;ifso,thisgroupiscon-wouldthusbecompletelydrivenbythebilingualalignmentpro-cess(seealso(Wu,1997;Tiedemann,2003)forrelatedconsid-erations).Inthiscase,ourapproachwouldbesimilartotheapproachof(Xuetal.,2004),exceptfortheestimationofcan-didates.sideredanewbasicunittoconsider.Wecanﬁnallyre-applythewordalignertotheupdatedsentences.Wehaveevaluatedtheperformanceofourap-proachbymeasuringtheinﬂuenceofthisprocessonaChinese-to-EnglishMTtask,basedontheIWSLT2006evaluationcampaign.Wereporta12.2%relativeincreaseinBLEUscoreoverastan-dardphrase-basedSMTsystem.Wehaveveriﬁedthatthisprocessactuallyreducesthenumberof1:nalignmentswithn6=1,andthatitisratherindepen-dentfromthe(Chinese)segmentationstrategy.Asforfuturework,weﬁrstplantoconsiderdif-ferentconﬁdencemeasuresfortheﬁlteringofthealignmentcandidates.Wealsowanttobootstrapondifferentwordaligners;inparticular,onepossibilityistousetheﬂexibleHMMword-to-phrasemodelofDengandByrne(2005)inplaceofIBMmodel4.Finally,wewouldliketoapplythismethodtoothercorporaandlanguagepairs.AcknowledgmentThisworkissupportedbyScienceFoundationIre-land(grantnumberOS/IN/1732).Prof.TiejunZhaoandDr.MuyunYangfromtheMTgroupofHarbinInstituteofTechnology,andYajuanLvfromtheIn-stituteofComputingTechnology,ChineseAcademyofSciences,arekindlyacknowledgedforprovid-inguswiththeChinesesegmenterandthemanuallydevelopedbilingualdictionaryusedinourexperi-ments.ReferencesNecipFazilAyanandBonnieJ.Dorr.2006.Goingbe-yondaer:Anextensiveanalysisofwordalignmentsandtheirimpactonmt.InProceedingsofCOLING-ACL2006,pages9–16,Sydney,Australia.AlexandraBirch,ChrisCallison-Burch,andMilesOs-borne.2006.Constrainingthephrase-based,jointprobabilitystatisticaltranslationmodel.InProceed-ingsofAMTA2006,pages10–18,Boston,MA.PeterF.Brown,StephenA.DellaPietra,VincentJ.DellaPietra,andRobertL.Mercer.1993.Themathematicsofstatisticalmachinetranslation:Parameterestima-tion.ComputationalLinguistics,19(2):263–311.YonggangDengandWilliamByrne.2005.HMMwordandphrasealignmentforstatisticalmachinetransla-tion.InProceedingsofHLT-EMNLP2005,pages169–176,Vancouver,Canada.311

AlexanderFraserandDanielMarcu.2006.Measuringwordalignmentqualityforstatisticalmachinetransla-tion.TechnicalReportISI-TR-616,ISI/UniversityofSouthernCalifornia.MihokoKitamuraandYujiMatsumoto.1996.Auto-maticextractionofwordsequencecorrespondencesinparallelcorpora.InProceedingsofthe4thWorkshoponVeryLargeCorpora,pages79–87,Copenhagen,Denmark.PhilipKoehn,FranzOch,andDanielMarcu.2003.Sta-tisticalphrase-basedtranslation.InProceedingsofHLT-NAACL2003,pages48–54,Edmonton,Canada.PhilipKoehn.2004.Pharaoh:Abeamsearchdecoderforphrase-basedstatisticalmachinetranslationmod-els.InProceedingsofAMTA2004,pages115–124,Washington,DistrictofColumbia.PhilippKoehn.2005.Europarl:Aparallelcorpusforstatisticalmachinetranslation.InMachineTransla-tionSummitX,pages79–86,Phuket,Thailand.PercyLiang,BenTaskar,andDanKlein.2006.Align-mentbyagreement.InProceedingsofHLT-NAACL2006,pages104–111,NewYork,NY.AdamLopezandPhilipResnik.2006.Word-basedalignment,phrase-basedtranslation:What’sthelink?InProceedingsofAMTA2006,pages90–99,Cam-bridge,MA.DanielMarcuandWilliamWong.2002.Aphrase-based,jointprobabilitymodelforstatisticalmachinetransla-tion.InProceedingsofEMNLP2002,pages133–139,Morristown,NJ.I.DanMelamed.1997.Automaticdiscoveryofnon-compositionalcompoundsinparalleldata.InPro-ceedingsofEMNLP1997,pages97–108,Somerset,NewJersey.I.DanMelamed.2000.Modelsoftranslationalequiv-alenceamongwords.ComputationalLinguistics,26(2):221–249.FranzOchandHermannNey.2003.Asystematiccom-parisonofvariousstatisticalalignmentmodels.Com-putationalLinguistics,29(1):19–51.FranzOch.2003.Minimumerrorratetraininginstatisti-calmachinetranslation.InProceedingsofACL2003,pages160–167,Sapporo,Japan.MarianOlteanu,ChrisDavis,IonutVolosen,andDanMoldovan.2006.Phramer-anopensourcestatis-ticalphrase-basedtranslator.InProceedingsoftheNAACL2006WorkshoponStatisticalMachineTrans-lation,pages146–149,NewYork,NY.KishorePapineni,SalimRoukos,ToddWard,andWei-JingZhu.2002.BLEU:amethodforautomaticeval-uationofmachinetranslation.InProceedingsofACL2002,pages311–318,Philadelphia,PA.MichaelPaul.2006.OverviewoftheIWSLT2006Eval-uationCampaign.InProceedingsofIWSLT2006,pages1–15,Kyoto,Japan.FrankSmadja,KathleenR.McKeown,andVasileiosHatzivassiloglou.1996.Translatingcollocationsforbilinguallexicons:Astatisticalapproach.Computa-tionalLinguistics,22(1):1–38.AndreaStolcke.2002.SRILM–Anextensiblelan-guagemodelingtoolkit.InProceedingsoftheInter-nationalConferenceonSpokenLanguageProcessing,pages901–904,Denver,Colorado.T.Takezawa,E.Sumita,F.Sugaya,H.Yamamoto,andS.Yamamoto.2002.Towardabroad-coveragebilin-gualcorpusforspeechtranslationoftravelconversa-tionsintherealworld.InProceedingsofLREC2002,pages147–152,LasPalmas,Spain.J¨orgTiedemann.2003.Combiningcluesforwordalign-ment.InProceedingsofEACL2003,pages339–346,Budapest,Hungary.DavidVilar,MajaPopovic,andHermannNey.2006.AER:Doweneedto”improve”ouralignments?InProceedingsofIWSLT2006,pages205–212,Kyoto,Japan.StefanVogel,HermannNey,andChristophTillmann.1996.HMM-basedwordalignmentinstatisticaltrans-lation.InProceedingsofCOLING1996,pages836–841,Copenhagen,Denmark.DekaiWu.1997.Stochasticinversiontransductiongrammarsandbilingualparsingofparallelcorpora.ComputationalLinguistics,23(3):377–403.JiaXu,RichardZens,andHermannNey.2004.Doweneedchinesewordsegmentationforstatisticalmachinetranslation?InProceedingsoftheThirdSIGHANWorkshoponChineseLanguageLearning,pages122–128,Barcelona,Spain.RichardZensandHermannNey.2004.Improvementsinphrase-basedstatisticalmachinetranslation.InProceedingsofHLT-NAACL2004,pages257–264,Boston,MA.TiejunZhao,YajuanL¨u,andHaoYu.2001.Increas-ingaccuracyofchinesesegmentationwithstrategyofmulti-stepprocessing.JournalofChineseInformationProcessing,15(1):13–18.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 312–319,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

312

ImprovedWord-LevelSystemCombinationforMachineTranslationAntti-VeikkoI.RostiandSpyrosMatsoukasandRichardSchwartzBBNTechnologies,10MoultonStreetCambridge,MA02138 arosti,smatsouk,schwartz@bbn.comAbstractRecently,confusionnetworkdecodinghasbeenappliedinmachinetranslationsystemcombination.Duetoerrorsinthehypoth-esisalignment,decodingmayresultinun-grammaticalcombinationoutputs.Thispa-perdescribesanimprovedconfusionnet-workbasedmethodtocombineoutputsfrommultipleMTsystems.Inthisapproach,ar-bitraryfeaturesmaybeaddedlog-linearlyintotheobjectivefunction,thusallowinglanguagemodelexpansionandre-scoring.Also,anovelmethodtoautomaticallyse-lectthehypothesiswhichotherhypothesesarealignedagainstisproposed.Agenericweighttuningalgorithmmaybeusedtoop-timizevariousautomaticevaluationmetricsincludingTER,BLEUandMETEOR.Theexperimentsusingthe2005ArabictoEn-glishandChinesetoEnglishNISTMTeval-uationtasksshowsigniﬁcantimprovementsinBLEUscorescomparedtoearlierconfu-sionnetworkdecodingbasedmethods.1IntroductionSystemcombinationhasbeenshowntoimproveclassiﬁcationperformanceinvarioustasks.Thereareseveralapproachesforcombiningclassiﬁers.Inensemblelearning,acollectionofsimpleclassiﬁersisusedtoyieldbetterperformancethananysingleclassiﬁer;forexampleboosting(Schapire,1990).Anotherapproachistocombineoutputsfromafewhighlyspecializedclassiﬁers.Theclassiﬁersmaybebasedonthesamebasicmodelingtechniquesbutdifferby,forexample,alternativefeaturerepre-sentations.Combinationofspeechrecognitionout-putsisanexampleofthisapproach(Fiscus,1997).Inspeechrecognition,confusionnetworkdecoding(Manguetal.,2000)hasbecomewidelyusedinsys-temcombination.Unlikespeechrecognition,currentstatisticalma-chinetranslation(MT)systemsarebasedonvariousdifferentparadigms;forexamplephrasal,hierarchi-calandsyntax-basedsystems.Theideaofcombin-ingoutputsfromdifferentMTsystemstoproduceconsensustranslationsinthehopeofgeneratingbet-tertranslationshasbeenaroundforawhile(Fred-erkingandNirenburg,1994).Recently,confusionnetworkdecodingforMTsystemcombinationhasbeenproposed(Bangaloreetal.,2001).Togenerateconfusionnetworks,hypotheseshavetobealignedagainsteachother.In(Bangaloreetal.,2001),Lev-enshteinalignmentwasusedtogeneratethenet-work.Asopposedtospeechrecognition,thewordorderbetweentwocorrectMToutputsmaybedif-ferentandtheLevenshteinalignmentmaynotbeabletoalignshiftedwordsinthehypotheses.In(Matusovetal.,2006),differentwordorderingsaretakenintoaccountbytrainingalignmentmodelsbyconsideringallhypothesispairsasaparallelcorpususingGIZA++(OchandNey,2003).Thesizeofthetestsetmayinﬂuencethequalityofthesealign-ments.Thus,systemoutputsfromdevelopmentsetsmayhavetobeaddedtoimprovetheGIZA++align-ments.AmodiﬁedLevenshteinalignmentallowingshiftsasincomputationofthetranslationeditrate(TER)(Snoveretal.,2006)wasusedtoalignhy-313

pothesesin(Simetal.,2007).ThealignmentsfromTERareconsistentastheydonotdependonthetestsetsize.Also,amoreheuristicalignmentmethodhasbeenproposedinadifferentsystemcombina-tionapproach(JayaramanandLavie,2005).Afullcomparisonofdifferentalignmentmethodswouldbedifﬁcultasmanyapproachesrequireasigniﬁcantamountofengineering.Confusionnetworksaregeneratedbychoosingonehypothesisasthe“skeleton”,andotherhypothe-sesarealignedagainstit.Theskeletondeﬁnesthewordorderofthecombinationoutput.MinimumBayesrisk(MBR)wasusedtochoosetheskeletonin(Simetal.,2007).TheaverageTERscorewascomputedbetweeneachsystem’s -besthypothesisandallotherhypotheses.TheMBRhypothesisistheonewiththeminimumaverageTERandthus,maybeviewedastheclosesttoallotherhypothe-sesintermsofTER.Thisworkwasextendedin(Rostietal.,2007)byintroducingsystemweightsforwordconﬁdences.However,thesystemweightsdidnotinﬂuencetheskeletonselection,soahypoth-esisfromasystemwithzeroweightmighthavebeenchosenastheskeleton.Inthiswork,confusionnet-worksaregeneratedbyusingthe -bestoutputfromeachsystemastheskeleton,andpriorprobabili-tiesforeachnetworkareestimatedfromtheaverageTERscoresbetweentheskeletonandotherhypothe-ses.Allresultingconfusionnetworksareconnectedinparallelintoajointlatticewherethepriorproba-bilitiesarealsomultipliedbythesystemweights.Thecombinationoutputsfromconfusionnetworkdecodingmaybeungrammaticalduetoalignmenterrors.Alsotheword-leveldecodingmaybreakcoherentphrasesproducedbytheindividualsys-tems.Inthiswork,log-posteriorprobabilitiesareestimatedforeachconfusionnetworkarcinsteadofusingvotesorsimplewordconﬁdences.Thisallowsalog-linearadditionofarbitraryfeaturessuchaslanguagemodel(LM)scores.TheLMscoresshouldincreasethetotallog-posteriorofmoregrammaticalhypotheses.Powell’smethod(Brent,1973)isusedtotunethesystemandfeatureweightssimultane-ouslysoastooptimizevariousautomaticevaluationmetricsonadevelopmentset.Tuningisfullyauto-matic,asopposedto(Matusovetal.,2006)whereglobalsystemweightsweresetmanually.Thispaperisorganizedasfollows.Threeevalu-ationmetricsusedinweightstuningandreportingthetestsetresultsarereviewedinSection2.Sec-tion3describesconfusionnetworkdecodingforMTsystemcombination.Theextensionstoaddfeatureslog-linearlyandimprovetheskeletonselectionarepresentedinSections4and5,respectively.Section6detailstheweightsoptimizationalgorithmandtheexperimentalresultsarereportedinSection7.Con-clusionsandfutureworkarediscussedinSection8.2EvaluationMetricsCurrently,themostwidelyusedautomaticMTeval-uationmetricistheNISTBLEU-4(Papinenietal.,2002).Itiscomputedasthegeometricmeanof-gramprecisionsupto-gramsbetweenthehypoth-esisandreferenceasfollows
	(1)  "!$#&%('*),+ -/.10-where0243 isthebrevitypenaltyand+ -arethe-gramprecisions.Whenmul-tiplereferencesareprovided,the-gramcountsagainstallreferencesareaccumulatedtocomputetheprecisions.Similarly,fulltestsetscoresareob-tainedbyaccumulatingcountsoverallhypothesisandreferencepairs.TheBLEUscoresarebetween5and ,higherbeingbetter.OftenBLEUscoresarereportedaspercentagesand“oneBLEUpointgain”usuallymeansaBLEUincreaseof5675 .OtherevaluationmetricshavebeenproposedtoreplaceBLEU.IthasbeenarguedthatMETEORcorrelatesbetterwithhumanjudgmentduetohigherweightonrecallthanprecision(BanerjeeandLavie,2005).METEORisbasedontheweightedharmonicmeanoftheprecisionandrecallmeasuredonuni-grammatchesasfollows8:9-;<= 5">?A@BDC"? FE56HGJI=K>/L.(2)where>isthetotalnumberofunigrammatches,?M@isthehypothesislength,?isthereferencelengthandIistheminimumnumberof-grammatchesthatcoversthealignment.Thesecondtermisafragmentationpenaltywhichpenalizestheharmonicmeanbyafactorofupto56HGwhenI>;i.e.,314

therearenomatching-gramshigherthan .Bydefault,METEORscriptcountsthewordsthatmatchexactly,andwordsthatmatchafterasimplePorterstemmer.Additionalmatchingmodulesin-cludingWordNetstemmingandsynonymymayalsobeused.Whenmultiplereferencesareprovided,thelowestscoreisreported.Fulltestsetscoresareob-tainedbyaccumulatingstatisticsoveralltestsen-tences.TheMETEORscoresarealsobetween5and ,higherbeingbetter.Thescoresintheresultssec-tionarereportedaspercentages.Translationeditrate(TER)(Snoveretal.,2006)hasbeenproposedasmoreintuitiveevaluationmet-ricsinceitisbasedontherateofeditsrequiredtotransformthehypothesisintothereference.TheTERscoreiscomputedasfollows9-	;<-$B%B	
AB	?(3)where?isthereferencelength.Theonlydiffer-encetoworderrorrateisthattheTERallowsshifts.Ashiftofasequenceofwordsiscountedasasin-gleedit.Theminimumtranslationeditalignmentisusuallyfoundthroughabeamsearch.Whenmulti-plereferencesareprovided,theeditsfromtheclos-estreferencearedividedbytheaveragereferencelength.Fulltestsetscoresareobtainedbyaccumu-latingtheeditsandtheaveragereferencelengths.TheperfectTERscoreis0,andotherwisehigherthanzero.TheTERscoremayalsobehigherthan1duetoinsertions.AlsoTERisreportedasapercent-ageintheresultssection.3ConfusionNetworkDecodingConfusionnetworkdecodinginMThastopickonehypothesisastheskeletonwhichdeterminesthewordorderofthecombination.Theotherhypothe-sesarealignedagainsttheskeleton.Eithervotesorsomeformofconﬁdencesareassignedtoeachwordinthenetwork.Forexampleusing“catsatthemat”astheskeleton,aligning“catsittingonthemat”and“hatonamat”againstitmightyieldthefollowingalignments:catsatthematcatsittingonthemathatonamatwhererepresentsaNULLword.Ingraphicalform,theresultingconfusionnetworkisshowninFigure1.Eacharcrepresentsanalternativewordatthatpositioninthesentenceandthenumberofvotesforeachwordismarkedinparentheses.Confusionnet-workdecodingusuallyrequiresﬁndingthepathwiththehighestconﬁdenceinthenetwork.Basedonvotecounts,therearethreealternativesintheexample:“catsatonthemat”,“catonthemat”and“catsit-tingonthemat”,eachhavingaccumulated10votes.Thealignmentprocedureplaysanimportantrole,asbyswitchingthepositionoftheword‘sat’andthefollowingNULLintheskeleton,therewouldbeasinglehighestscoringpaththroughthenetwork;thatis,“catonthemat”.123456cat (2)hat (1)e (1)sitting (1)e (1)on (2)a (1)the (2)sat (1)mat (3)Figure1:Exampleconsensusnetworkwithvotesonwordarcs.Differentalignmentmethodsyielddifferentcon-fusionnetworks.ThemodiﬁedLevenshteinalign-mentasusedinTERismorenaturalthansimpleeditdistancesuchasworderrorratesincemachinetrans-lationhypothesesmayhavedifferentwordorderswhilehavingthesamemeaning.Astheskeletondeterminesthewordorder,thequalityofthecom-binationoutputalsodependsonwhichhypothesisischosenastheskeleton.SincethemodiﬁedLeven-shteinalignmentproducesTERscoresbetweentheskeletonandtheotherhypotheses,anaturalchoiceforselectingtheskeletonistheminimumaverageTERscore.ThehypothesisresultinginthelowestaverageTERscorewhenalignedagainstallotherhypothesesischosenastheskeletonasfollows) "!#%$&!$#9-	;A&('(4)where?isthenumberofsystems.Thisisequiv-alenttominimumBayesriskdecodingwithuni-formposteriorprobabilities(Simetal.,2007).OtherevaluationmetricsmayalsobeusedastheMBRlossfunction.ForBLEUandMETEOR,thelossfunctionwouldbe E
	<&)'and E89-;A&)'.Ithasbeenfoundthatmultiplehypothesesfromeachsystemmaybeusedtoimprovethequalityof315

thecombinationoutput(Simetal.,2007).Whenusing?-bestlistsfromeachsystem,thewordsmaybeassignedadifferentscorebasedontherankofthehypothesis.In(Rostietal.,2007),simple K B scorewasassignedtothewordcomingfromthe th-besthypothesis.DuetothecomputationalburdenoftheTERalignment,only -besthypotheseswereconsideredaspossibleskeletons,and 5hy-pothesespersystemwerealigned.Similarapproachtoestimatewordposteriorsisadoptedinthiswork.Systemweightsmaybeusedtoassignasystemspeciﬁcconﬁdenceoneachwordinthenetwork.Theweightsmaybebasedonthesystems’relativeperformanceonaseparatedevelopmentsetortheymaybeautomaticallytunedtooptimizesomeevalu-ationmetriconthedevelopmentset.In(Rostietal.,2007),thetotalconﬁdenceofthethbestconfusionnetworkhypothesis& ,includingNULLwords,giventhethsourcesentence&wasgivenbyI*& &(5)#	
#'!$##%$!$#I'B,? & where?&isthenumberofnodesintheconfu-sionnetworkforthesourcesentence&,?isthenumberoftranslationsystems,isthethsystemweight,I'istheaccumulatedconﬁdenceforwordproducedbysystembetweennodesandB ,andisaweightforthenumberofNULLlinksalongthehypothesis? & .Thewordcon-ﬁdencesI'wereincreasedby K B ifthewordalignsbetweennodesandB inthenet-work.IfnowordalignsbetweennodesandB ,theNULLwordconﬁdenceatthatpositionwasin-creasedby K B .ThelasttermcontrolsthenumberofNULLwordsgeneratedintheoutputandmaybeviewedasaninsertionpenalty.Eacharcintheconfusionnetworkcarriesthewordlabeland?scoresI'.ThedecoderoutputsthehypothesiswiththehighestI*& &giventhecurrentsetofweights.3.1DiscussionThereareseveralproblemswiththepreviouscon-fusionnetworkdecodingapproaches.First,thedecodingcangenerateungrammaticalhypothesesduetoalignmenterrorsandphrasesbrokenbytheword-leveldecoding.Forexample,twosynony-mouswordsmaybealignedtootherwordsnotal-readyaligned,whichmayresultinrepetitiveoutput.Second,theadditiveconﬁdencescoresinEquation5havenoprobabilisticmeaningandcannotthere-forebecombinedwithlanguagemodelscores.Lan-guagemodelexpansionandre-scoringmayhelpbyincreasingtheprobabilityofmoregrammaticalhy-pothesesindecoding.Third,thesystemweightsareindependentoftheskeletonselection.Therefore,ahypothesisfromasystemwithaloworzeroweightmaybechosenastheskeleton.4Log-LinearCombinationwithArbitraryFeaturesToaddresstheissuewithungrammaticalhypothesesandallowlanguagemodelexpansionandre-scoring,thehypothesisconﬁdencecomputationismodiﬁed.InsteadofsummingarbitraryconﬁdencescoresasinEquation5,wordposteriorprobabilitiesareusedasfollows%'*)
+& &(6)#	
#'!$#%'*)#%$!$#+///.B! #"& B$,? & B%*?&')(& where isthelanguagemodelweight,"& istheLMlog-probabilityand?*'(& isthenumberofwordsinthehypothesis& .Thewordposteriors+//areestimatedbyscalingthecon-ﬁdencesI'tosumtooneforeachsystemoverallwordsinbetweennodesandB .Thesystemweightsarealsoconstrainedtosumtoone.Equation6maybeviewedasalog-linearsumofsentence-levelfeatures.Theﬁrstfeatureisthesumofwordlog-posteriors,thesecondistheLMlog-probability,thethirdisthelog-NULLscoreandthelastisthelog-lengthscore.Thelasttwotermsarenotcom-pletelyindependentbutseemtohelpbasedonex-perimentalresults.Thenumberofpathsthroughaconfusionnet-workgrowsexponentiallywiththenumberofnodes.Thereforeexpandinganetworkwithan-gramlan-guagemodelmayresultinhugelatticesifishigh.Insteadofhighorder-gramswithheavypruning,abi-grammayﬁrstbeusedtoexpandthelattice.Af-teroptimizingonesetofweightsfortheexpanded316

confusionnetwork,asecondsetofweightsfor?-bestlistre-scoringwithahigherorder-grammodelmaybeoptimized.Onatestset,theﬁrstsetofweightsisusedtogeneratean?-bestlistfromthebi-gramexpandedlattice.This?-bestlististhenre-scoredwiththehigherorder-gram.Thesecondsetofweightsisusedtoﬁndtheﬁnal -bestfromthere-scored?-bestlist.5MultipleConfusionNetworkDecodingAsdiscussedinSection3,thereisadisconnectbe-tweentheskeletonselectionandconﬁdenceestima-tion.Topreventthe -bestfromasystemwithaloworzeroweightbeingselectedastheskeleton,confu-sionnetworksaregeneratedforeachsystemandtheaverageTERscoreinEquation4isusedtoestimateapriorprobabilityforthecorrespondingnetwork.All?confusionnetworksareconnectedtoasinglestartnodewithNULLarcswhichcontainthepriorprobabilityfromthesystemusedastheskeletonforthatnetwork.AllconfusionnetworkareconnectedtoacommonendnodewithNULLarcs.Theﬁnalarcshaveaprobabilityofone.Thepriorprobabil-itiesinthearcsleavingtheﬁrstnodewillbemul-tipliedbythecorrespondingsystemweightswhichguaranteesthatapaththroughanetworkgeneratedarounda -bestfromasystemwithazeroweightwillnotbechosen.ThepriorprobabilitiesareestimatedbyviewingthenegativeaverageTERscoresbetweentheskele-tonandotherhypothesesaslog-probabilities.Theselog-probabilitiesarescaledsothatthepriorssumtoone.Thereisaconcernthatthepriorprobabilitiesestimatedthiswaymaybeinaccurate.Therefore,thepriorsmayhavetobesmoothedbyatunableexponent.However,theoptimizationexperimentsshowedthatthebestperformancewasobtainedbyhavingasmoothingfactorof1whichisequivalenttotheoriginalpriors.Thus,nosmoothingwasusedintheexperimentspresentedlaterinthispaper.AnexamplejointnetworkwiththepriorsisshowninFigure2.Thisexamplehasthreeconfu-sionnetworkswithpriors56HG,56 and56.Theto-talnumberofnodesinthenetworkisrepresentedby?.Similarcombinationofmultipleconfusionnetworkswaspresentedin(Matusovetal.,2006).However,thisapproachdidnotincludesentencee (1)e (1)e (1)e (0.2)e (0.3)e (0.5)1NaFigure2:Threeconfusionnetworkswithpriorprob-abilities.speciﬁcpriorestimates,wordposteriorestimates,anddidnotallowjointoptimizationofthesystemandfeatureweights.6WeightsOptimizationTheoptimizationofthesystemandfeatureweightsmaybecarriedoutusing?-bestlistsasin(Osten-dorfetal.,1991).Aconfusionnetworkmayberep-resentedbyawordlatticeandstandardtoolsmaybeusedtogenerate?-besthypothesislistsincludingwordconﬁdencescores,languagemodelscoresandotherfeatures.The?-bestlistmaybere-orderedusingthesentence-levelposteriors+& &fromEquation6forthethsourcesentence&andthecorrespondingthhypothesis& .Thecurrent -besthypothesis&givenasetofweights #666#%$ %mayberepresentedasfol-lows&&) 	+& &(7)Theobjectiveistooptimizethe -bestscoreonadevelopmentsetgivenasetofreferencetransla-tions.Forexample,estimatingweightswhichmini-mizeTERbetweenasetof -besthypothesis
andreferencetranslations
canbewrittenas)9-	;

=(8)Thisobjectivefunctionisverycomplicated,sogradient-basedoptimizationmethodsmaynotbeused.Inthiswork,modiﬁedPowell’smethodasproposedby(Brent,1973)isused.Thealgorithmexploresbetterweightsiterativelystartingfromasetofinitialweights.First,eachdimensionisop-timizedusingagrid-basedlineminimizationalgo-rithm.Then,anewdirectionbasedonthechangesintheobjectivefunctionisestimatedtospeedupthesearch.Toimprovethechancesofﬁndinga317

globaloptimum,19randomperturbationsoftheini-tialweightsareusedinparalleloptimizationruns.Sincethe?-bestlistrepresentsonlyasmallportionofallhypothesesintheconfusionnetwork,theop-timizedweightsfromoneiterationmaybeusedtogenerateanew?-bestlistfromthelatticeforthenextiteration.Similarly,weightswhichmaximizeBLEUorMETEORmaybeoptimized.ThesamePowell’smethodhasbeenusedtoes-timatefeatureweightsofastandardfeature-basedphrasalMTdecoderin(Och,2003).Amoreefﬁ-cientalgorithmforlog-linearmodelswasalsopro-posed.Inthiswork,boththesystemandfeatureweightsarejointlyoptimized,sotheefﬁcientalgo-rithmforthelog-linearmodelscannotbeused.7ResultsTheimprovedsystemcombinationmethodwascomparedtoasimpleconfusionnetworkdecodingwithoutsystemweightsandthemethodproposedin(Rostietal.,2007)ontheArabictoEnglishandChinesetoEnglishNISTMT05tasks.SixMTsys-temswerecombined:three(A,C,E)werephrase-basedsimilarto(Koehn,2004),two(B,D)werehierarchicalsimilarto(Chiang,2005)andone(F)wassyntax-basedsimilarto(Galleyetal.,2006).Allsystemsweretrainedonthesamedataandtheoutputsusedthesametokenization.ThedecoderweightsforsystemsAandBweretunedtooptimizeTER,andothersweretunedtooptimizeBLEU.AlldecoderweighttuningwasdoneontheNISTMT02task.Thejointconfusionnetworkwasexpandedwithabi-gramlanguagemodelanda"5*5-bestlistwasgeneratedfromthelatticeforeachtuningiteration.ThesystemandfeatureweightsweretunedontheunionofNISTMT03andMT04tasks.Allfourref-erencetranslationsavailableforthetuningandtestsetswereused.Aﬁrstsetofweightswiththebi-gramLMwasoptimizedwiththreeiterations.Asecondsetofweightswastunedfor5-gram?-bestlistre-scoring.Thebi-gramand5-gramEnglishlan-guagemodelsweretrainedonabout7billionwords.Theﬁnalcombinationoutputsweredetokenizedandcasedbeforescoring.ThetuningsetresultsontheArabictoEnglishNISTMT03+MT04taskareshowninTable1.TheArabictuningTERBLEUMTRsystemA44.9345.7166.09systemB46.4143.0764.79systemC46.1046.4165.33systemD44.3646.8366.91systemE45.3545.4465.69systemF47.1044.5265.28noweights42.3548.9167.76baseline42.1949.8668.34TERtuned41.8851.4568.62BLEUtuned42.1251.7268.59MTRtuned54.0838.9371.42Table1:Mixed-caseTERandBLEU,andlower-caseMETEORscoresonArabicNISTMT03+MT04.ArabictestTERBLEUMTRsystemA42.9849.5869.86systemB43.7947.0668.62systemC43.9247.8766.97systemD40.7552.0971.23systemE42.1950.8670.02systemF44.3050.1569.75noweights39.3353.6671.61baseline39.2954.5172.20TERtuned39.1055.3072.53BLEUtuned39.1355.4872.81MTRtuned51.5641.7374.79Table2:Mixed-caseTERandBLEU,andlower-caseMETEORscoresonArabicNISTMT05.bestscoreoneachmetricisshowninboldfacefonts.TherowlabeledasnoweightscorrespondstoEquation5withuniformsystemweightsandzeroNULLweight.ThebaselinecorrespondstoEquation5withTERtunedweights.Thefollow-ingthreerowscorrespondtotheimprovedconfusionnetworkdecodingwithdifferentoptimizationmet-rics.Asexpected,thescoresonthemetricusedintuningarethebestonthatmetric.Also,thecombi-nationresultsarebetterthananysinglesystemonallmetricsinthecaseofTERandBLEUtuning.How-ever,theMETEORtuningyieldsextremelyhighTERandlowBLEUscores.ThismustbeduetothehigherweightontherecallcomparedtoprecisionintheharmonicmeanusedtocomputetheMETEOR318

ChinesetuningTERBLEUMTRsystemA56.5629.3954.54systemB55.8830.4554.36systemC58.3532.8856.72systemD57.0936.1857.11systemE57.6933.8558.28systemF56.1136.6458.90noweights53.1137.7759.19baseline53.4038.5259.56TERtuned52.1336.8757.30BLEUtuned53.0339.9958.97MTRtuned70.2728.6063.10Table3:Mixed-caseTERandBLEU,andlower-caseMETEORscoresonChineseNISTMT03+MT04.score.EventhoughMETEORhasbeenshowntobeagoodmetriconagivenMToutput,tuningtoop-timizeMETEORresultsinahighinsertionrateandlowprecision.TheArabictestsetresultsareshowninTable2.TheTERandBLEUoptimizedcom-binationresultsbeatallsinglesystemscoresonallmetrics.Thebestresultsonagivenmetricareagainobtainedbythecombinationoptimizedforthecorre-spondingmetric.ItshouldbenotedthattheTERop-timizedcombinationhassigniﬁcantlyhigherBLEUscorethantheTERoptimizedbaseline.ComparedtothebaselinesystemwhichisalsooptimizedforTER,theBLEUscoreisimprovedby0.97points.Also,theMETEORscoreusingtheMETEORop-timizedweightsisveryhigh.However,theotherscoresareworseincommonwiththetuningsetre-sults.ThetuningsetresultsontheChinesetoEnglishNISTMT03+MT04taskareshowninTable3.Thebaselinecombinationweightsweretunedtoopti-mizeBLEU.Again,thebestscoresoneachmet-ricareobtainedbythecombinationtunedforthatmetric.OnlytheMETEORscoreoftheTERtunedcombinationisworsethantheMETEORscoresofsystemsEandF-othercombinationsarebetterthananysinglesystemonallmetricsapartfromtheME-TEORtunedcombinations.Thetestsetresultsfol-lowclearlythetuningresultsagain-theTERtunedcombinationisthebestintermsofTER,theBLEUtunedintermsofBLEU,andtheMETEORtunedinChinesetestTERBLEUMTRsystemA56.5729.6356.63systemB56.3029.6255.61systemC59.4831.3257.71systemD58.3233.7757.92systemE58.4632.4059.75systemF56.7935.3060.82noweights53.8036.1760.75baseline54.3436.4461.05TERtuned52.9035.7658.60BLEUtuned54.0537.9160.31MTRtuned72.5926.9664.35Table4:Mixed-caseTERandBLEU,andlower-caseMETEORscoresonChineseNISTMT05.termsofMETEOR.Comparedtothebaseline,theBLEUscoreoftheBLEUtunedcombinationisim-provedby1.47points.Again,theMETEORtunedweightshurttheothermetricssigniﬁcantly.8ConclusionsAnimprovedconfusionnetworkdecodingmethodcombiningthewordposteriorswitharbitraryfea-tureswaspresented.Thisallowstheadditionoflanguagemodelscoresbyexpandingthelatticesorre-scoring?-bestlists.TheLMintegrationshouldresultinmoregrammaticalcombinationoutputs.Also,confusionnetworksgeneratedbyusingthe -besthypothesisfromallsystemsastheskeletonwereusedwithpriorprobabilitiesderivedfromtheaverageTERscores.Thisguaranteesthatthebestpathwillnotbefoundfromanetworkgeneratedforasystemwithzeroweight.Comparedtotheearliersystemcombinationapproaches,thismethodisfullyautomaticandrequiresverylittleadditionalinfor-mationontopofthedevelopmentsetoutputsfromtheindividualsystemstotunetheweights.ThenewmethodwasevaluatedontheArabictoEnglishandChinesetoEnglishNISTMT05tasks.Comparedtothebaselinefrom(Rostietal.,2007),thenewmethodimprovestheBLEUscoressignif-icantly.Thecombinationweightsweretunedtooptimizethreeautomaticevaluationmetrics:TER,BLEUandMETEOR.TheTERtuningseemstoyieldverygoodresultsonArabic-theBLEUtun-ingseemstobebetteronChinese.Italsoseemslike319

METEORshouldnotbeusedintuningduetohighinsertionrateandlowprecision.Itwouldbeinterest-ingtoknowwhichtuningmetricresultsinthebesttranslationsintermsofhumanjudgment.However,thiswouldrequiretimeconsumingevaluationssuchashumanmediatedTERpost-editing(Snoveretal.,2006).Theimprovedconfusionnetworkdecodingap-proachallowsarbitraryfeaturestobeusedinthecombination.Newfeaturesmaybeaddedinthefu-ture.Hypothesisalignmentisalsoveryimportantinconfusionnetworkgeneration.Betteralignmentmethodswhichtakesynonymyintoaccountshouldbeinvestigated.Thismethodcouldalsobeneﬁtfrommoresophisticatedwordposteriorestimation.AcknowledgmentsThisworkwassupportedbyDARPA/IPTOContractNo.HR0011-06-C-0022undertheGALEprogram(approvedforpublicrelease,distributionunlimited).TheauthorswouldliketothankISIandUniversityofEdinburghforsharingtheirMTsystemoutputs.ReferencesSatanjeevBanerjeeandAlonLavie.2005.METEOR:AnautomaticmetricforMTevaluationwithimprovedcorrelationwithhumanjudgments.InProc.ACLWorkshoponIntrinsicandExtrinsicEvaluationMea-suresforMachineTranslationand/orSummarization,pages65–72.SrinivasBangalore,GermanBordel,andGiuseppeRic-cardi.2001.Computingconsensustranslationfrommultiplemachinetranslationsystems.InProc.ASRU,pages351–354.RichardP.Brent.1973.AlgorithmsforMinimizationWithoutDerivatives.Prentice-Hall.DavidChiang.2005.Ahierarchicalphrase-basedmodelforstatisticalmachinetranslation.InProc.ACL,pages263–270.JonathanG.Fiscus.1997.Apost-processingsystemtoyieldreducedworderrorrates:Recognizeroutputvot-ingerrorreduction(ROVER).InProc.ASRU,pages347–354.RobertFrederkingandSergeiNirenburg.1994.Threeheadsarebetterthanone.InProc.ANLP,pages95–100.MichelGalley,JonathanGraehl,KevinKnight,DanielMarcu,SteveDeNeefe,WeiWang,andIgnacioThayer.2006.Scalableinferencesandtrainingofcontext-richsyntaxtranslationmodels.InProc.COL-ING/ACL,pages961–968.ShyamsundarJayaramanandAlonLavie.2005.Multi-enginemachinetranslationguidedbyexplicitwordmatching.InProc.EAMT,pages143–152.PhilippKoehn.2004.Pharaoh:abeamsearchdecoderforphrase-basedstatisticalmachinetranslationmod-els.InProc.AMTA,pages115–124.LidiaMangu,EricBrill,andAndreasStolcke.2000.Findingconsensusinspeechrecognition:Worderrorminimizationandotherapplicationsofconfusionnet-works.ComputerSpeechandLanguage,14(4):373–400.EvgenyMatusov,NicolaUefﬁng,andHermannNey.2006.Computingconsensustranslationfrommultiplemachinetranslationsystemsusingenhancedhypothe-sesalignment.InProc.EACL,pages33–40.FranzJ.OchandHermannNey.2003.Asystematiccomparisonofvariousstatisticalalignmentmodels.ComputationalLinguistics,29(1):19–51.FranzJ.Och.2003.Minimumerrorratetraininginsta-tisticalmachinetranslation.InProc.ACL,pages160–167.MariOstendorf,AshvinKannan,SteveAustin,OwenKimball,RichardSchwartz,andJanRobinRohlicek.1991.Integrationofdiverserecognitionmethodolo-giesthroughreevaluationofN-bestsentencehypothe-ses.InProc.HLT,pages83–87.KishorePapineni,SalimRoukos,ToddWard,andWei-JingZhu.2002.BLEU:amethodforautomaticeval-uationofmachinetranslation.InProc.ACL,pages311–318.Antti-VeikkoI.Rosti,BingXiang,SpyrosMatsoukas,RichardSchwartz,NecipFazilAyan,andBonnieJ.Dorr.2007.Combiningoutputsfrommultiplema-chinetranslationsystems.InProc.NAACL-HLT2007,pages228–235.RobertE.Schapire.1990.Thestrengthofweaklearn-ability.MachineLearning,5(2):197–227.KheChaiSim,WilliamJ.Byrne,MarkJ.F.Gales,HichemSahbi,andPhilC.Woodland.2007.Consen-susnetworkdecodingforstatisticalmachinetransla-tionsystemcombination.InProc.ICASSP,volume4,pages105–108.MatthewSnover,BonnieDorr,RichardSchwartz,Lin-neaMicciula,andJohnMakhoul.2006.Astudyoftranslationeditratewithtargetedhumanannotation.InProc.AMTA,pages223–231.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 320–327,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

320

GeneratingConstituentOrderinGermanClausesKatjaFilippovaandMichaelStrubeEMLResearchgGmbHSchloss-Wolfsbrunnenweg3369118Heidelberg,Germanyhttp://www.eml-research.de/nlpAbstractWeinvestigatethefactorswhichdetermineconstituentorderinGermanclausesandpro-poseanalgorithmwhichperformsthetaskintwosteps:First,thebestcandidatefortheinitialsentencepositionischosen.Then,theorderfortheremainingconstituentsisdetermined.The(cid:2)rsttaskismoredif(cid:2)cultthanthesecondonebecauseofpropertiesoftheGermansentence-initialposition.Ex-perimentsshowasigni(cid:2)cantimprovementovercompetingapproaches.Ouralgorithmisalsomoreef(cid:2)cientthanthese.1IntroductionManynaturallanguagesallowvariationinthewordorder.Thisisachallengefornaturallanguagegen-erationandmachinetranslationsystems,orfortextsummarizers.E.g.,intext-to-textgeneration(Barzi-lay&McKeown,2005;Marsi&Krahmer,2005;Wanetal.,2005),newsentencesarefusedfromde-pendencystructuresofinputsentences.Thelaststepofsentencefusionislinearizationoftheresultingparse.EvenforEnglish,whichisalanguagewith(cid:2)xedwordorder,thisisnotatrivialtask.Germanhasarelativelyfreewordorder.Thisconcernstheorderofconstituents1withinsentenceswhiletheorderofwordswithinconstituentsisrela-tivelyrigid.Thegrammaronlypartiallyprescribeshowconstituentsdependentontheverbshouldbeordered,andformanyclauseseachofthen!possi-blepermutationsofnconstituentsisgrammatical.1Henceforth,wewillusethistermtorefertoconstituentsdependentontheclausaltopnode,i.e.averb,only.InspiteofthepermanentinterestinGermanwordorderinthelinguisticscommunity,moststudieshavelimitedtheirscopetotheorderofverbargu-mentsandfewresearchershaveimplemented(cid:150)andevenlessevaluated(cid:150)agenerationalgorithm.Inthispaper,wepresentanalgorithm,whichordersnotonlyverbargumentsbutallkindsofconstituents,andevaluateitonacorpusofbiographies.Foreachparsedsentenceinthetestset,ourmaximum-entropy-basedalgorithmaimsatreproducingtheor-derfoundintheoriginaltext.Weinvestigatetheimportanceofdifferentlinguisticfactorsandsug-gestanalgorithmtoconstituentorderingwhich(cid:2)rstdeterminesthesentenceinitialconstituentandthenorderstheremainingones.Weprovideevidencethatthetaskrequireslanguage-speci(cid:2)cknowledgetoachievebetterresultsandpointtothemostdif(cid:2)-cultpartofit.SimilartoLangkilde&Knight(1998)weutilizestatisticalmethods.Unlikeovergenera-tionapproaches(Varges&Mellish,2001,interalia)whichselectthebestofallpossibleoutputsoursismoreef(cid:2)cient,becausewedonotneedtogenerateeverypermutation.2TheoreticalPremises2.1BackgroundIthasbeensuggestedthatseveralfactorshaveanin-(cid:3)uenceonGermanconstituentorder.Apartfromtheconstraintsposedbythegrammar,informationstructure,surfaceform,anddiscoursestatushavealsobeenshowntoplayarole.Ithasalsobeenobservedthattherearepreferencesforaparticularorder.Thepreferencessummarizedbelowhavemo-321

tivatedourchoiceoffeatures:•constituentsinthenominativecaseprecedethoseinothercases,anddativeconstituentsoftenprecedethoseintheaccusativecase(Uszkoreit,1987;Keller,2000);•theverbarguments’orderdependsontheverb’ssubcategorizationproperties(Kurz,2000);•constituentswithade(cid:2)nitearticleprecedethosewithaninde(cid:2)niteone(Weber&M¤uller,2004);•pronominalizedconstituentsprecedenon-pronominalizedones(Kempen&Harbusch,2004);•animatereferentsprecedeinanimateones(Pap-pertetal.,2007);•shortconstituentsprecedelongerones(Kim-ball,1973);•thepreferredtopicpositionisrightaftertheverb(Frey,2004);•theinitialpositionisusuallyoccupiedbyscene-settingelementsandtopics(Speyer,2005).•thereisadefaultorderbasedonsemanticprop-ertiesofconstituents(Sgalletal.,1986):Actor<Temporal<SpaceLocative<Means<Ad-dressee<Patient<Source<Destination<PurposeNotethatmostofthesepreferenceswereidenti(cid:2)edincorpusstudiesandexperimentswithnativespeak-ersandconcerntheorderofverbargumentsonly.Littlehasbeensaidsofarabouthownon-argumentsshouldbeordered.Germanisaverbsecondlanguage,i.e.,thepo-sitionoftheverbinthemainclauseisdeterminedexclusivelybythegrammarandisinsensitivetootherfactors.Thus,theGermanmainclauseisdi-videdintotwopartsbythe(cid:2)niteverb:Vorfeld(VF),whichcontainsexactlyoneconstituent,andMit-telfeld(MF),wheretheremainingconstituentsarelocated.ThesubordinateclausenormallyhasonlyMF.TheVFandMFaremarkedwithbracketsinExample1:(1)[Au(cid:223)erdem]Apartfromthatentwickeltedeveloped[LummerLummereineaQuecksilberdamp(cid:3)ampe,Mercury-vaporlampumtomonochromatischesmonochromeLichtlightherzustellen].produce.’Apartfromthat,LummerdevelopedaMercury-vaporlamptoproducemonochromelight’.2.2OurHypothesisTheessentialcontributionofourstudyisthatwetreatpreverbalandpostverbalpartsofthesentencedifferently.Thesentence-initialposition,whichinGermanistheVF,hasbeenshowntobecognitivelymoreprominentthanotherpositions(Gernsbacher&Hargreaves,1988).MotivatedbythetheoreticalworkbyChafe(1976)andJacobs(2001),weviewtheVFastheplaceforelementswhichmodifythesituationdescribedinthesentence,i.e.forsocalledframe-settingtopics(Jacobs,2001).Forexample,temporalorlocationalconstituents,oranaphoricad-verbsaregoodcandidatesfortheVF.Wehypoth-esizethatthereasonswhichbringaconstituenttotheVFaredifferentfromthosewhichplaceit,say,tothebeginningoftheMF,fortheorderintheMFhasbeenshowntoberelativelyrigid(Keller,2000;Kempen&Harbusch,2004).Speakershavethefreedomofselectingtheoutgoingpointforasen-tence.Oncetheyhaveselectedit,theremainingcon-stituentsarearrangedintheMF,mainlyaccordingtotheirgrammaticalproperties.Thislastobservationmotivatesanotherhypothe-siswemake:Thecumulationofthepropertiesofaconstituentdeterminesitssalience.Thissaliencecanbecalculatedandusedfororderingwithasim-plerulestatingthatmoresalientconstituentsshouldprecedelesssalientones.Inthiscasethereisnoneedtogenerateallpossibleordersandrankthem.Thebestordercanbeobtainedfromarandomonebysorting.Ourexperimentssupportthisview.Atwo-stepapproach,which(cid:2)rstselectsthebestcan-didatefortheVFandthenarrangestheremainingconstituentsintheMFwithrespecttotheirsalienceperformsbetterthanalgorithmswhichgeneratetheorderforasentenceasawhole.322

3RelatedWorkUszkoreit(1987)addressestheproblemfromamostlygrammar-basedperspectiveandsuggestsweightedconstraints,suchas[+NOM]≺[+DAT],[+PRO]≺[(cid:150)PRO],[(cid:150)FOCUS]≺[+FOCUS],etc.Kruijffetal.(2001)describeanarchitecturewhichsupportsgeneratingtheappropriatewordor-derfordifferentlanguages.Inspiredbythe(cid:2)ndingsofthePragueSchool(Sgalletal.,1986)andSys-temicFunctionalLinguistics(Halliday,1985),theyfocusontherolethatinformationstructureplaysinconstituentordering.Kruijff-Korbayov·aetal.(2002)addressthetaskofwordordergenerationinthesamevein.Similartoours,theiralgorithmrec-ognizesthespecialroleofthesentence-initialpo-sitionwhichtheyreserveforthetheme(cid:150)thepointofdepartureofthemessage.Unfortunately,theydidnotimplementtheiralgorithm,anditishardtojudgehowwellthesystemwouldperformonrealdata.Harbuschetal.(2006)presentagenerationwork-bench,whichhasthegoalofproducingnotthemostappropriateorder,butallgrammaticalones.Theyalsodonotprovideexperimentalresults.TheworkofUchimotoetal.(2000)isdoneonthefreewordorderlanguageJapanese.Theyde-terminetheorderofphrasalunitsdependentonthesamemodi(cid:2)ee.Theirapproachissimilartooursinthattheyaimatregeneratingtheoriginalorderfromadependencyparse,butdiffersinthescopeoftheproblemastheyregeneratetheorderofmodifersforallandnotonlyforthetopclausalnode.Usingamaximumentropyframework,theychoosethemostprobableorderfromthesetofallpermutationsofnwordsbythefollowingformula:P(1|h)=P({Wi,i+j=1|1≤i≤n−1,1≤j≤n−i}|h)≈n−1Yi=1n−iYj=1P(Wi,i+j=1|hi,i+j)=n−1Yi=1n−iYj=1PME(1|hi,i+j)(1)Foreachpermutation,foreverypairofwords,theymultiplytheprobabilityoftheirbeinginthecorrect2ordergiventhehistoryh.RandomvariableWi,i+j2Onlyreferenceordersareassumedtobecorrect.is1ifwordwiprecedeswi+jinthereferencesen-tence,0otherwise.ThefeaturestheyuseareakintothosewhichplayaroleindeterminingGermanwordorder.Weusetheirapproachasanon-trivialbaselineinourstudy.Ringgeretal.(2004)aimatregeneratingtheor-derofconstituentsaswellastheorderwithinthemforGermanandFrenchtechnicalmanuals.Utilizingsyntactic,semantic,sub-categorizationandlengthfeatures,theytestseveralstatisticalmodelsto(cid:2)ndtheorderwhichmaximizestheprobabilityofanor-deredtree.Using(cid:147)Markovgrammars(cid:148)asthestart-ingpointandconditioningonthesyntacticcategoryonly,theyexpandanon-terminalnodeCbypredict-ingitsdaughtersfromlefttoright:P(C|h)=nYi=1P(di|di−1,...,di−j,c,h)(2)Here,cisthesyntacticcategoryofC,dandharethesyntacticcategoriesofC’sdaughtersandthedaughterwhichistheheadofCrespectively.Intheirsimplestsystem,whoseperformanceisonly2.5%worsethantheperformanceofthebestone,theyconditiononbothsyntacticcategoriesandsemanticrelations(ψ)accordingtotheformula:P(C|h)=nYi=1»P(ψi|di−1,ψi−1,...di−j,ψi−j,c,h)×P(di|ψi,di−1,ψi−1...,di−j,ψi−j,c,h)–(3)AlthoughtheytesttheirsystemonGermandata,itishardtocomparetheirresultstooursdirectly.First,themetrictheyusedoesnotdescribetheper-formanceappropriately(seeSection6.1).Second,whilethewordorderwithinNPsandPPsaswellastheverbpositionareprescribedbythegrammartoalargeextent,theconstituentscantheoreticallybeor-deredinanyway.Thus,bygeneratingtheorderforeverynon-terminalnode,theycombinetwotasksofdifferentcomplexityandmixtheresultsofthemoredif(cid:2)culttaskwiththoseoftheeasierone.4DataThedataweworkwithisacollectionofbiogra-phiesfromtheGermanversionofWikipedia3.Fullyautomaticpreprocessinginoursystemcomprisesthefollowingsteps:First,alistofpeopleofacertainWikipediacategoryistakenandanarticleisextractedforeveryperson.Second,sentence3http://de.wikipedia.org323

entwickelteumherzustellenSUBmonochromatischesLichteineQuecksilberdamp(cid:3)ampeOBJAau(cid:223)erdemADV(conn)LummerSUBJ(pers)Figure1:TherepresentationofthesentenceinExample1boundariesareidenti(cid:2)edwithaPerlCPANmod-ule4whoseperformanceweimprovedbyextend-ingthelistofabbreviations.Next,thesentencesaresplitintotokens.TheTnTtagger(Brants,2000)andtheTreeTagger(Schmid,1997)areusedfortag-gingandlemmatization.Finally,thearticlesareparsedwiththeCDGdependencyparser(Foth&Menzel,2006).Namedentitiesareclassi(cid:2)edaccord-ingtotheirsemantictypeusinglistsandcategoryinformationfromWikipedia:person(pers),location(loc),organization(org),orunde(cid:2)nednamedentity(undefne).Temporalexpressions(Oktober1915,danach(afterthat)etc.)areidenti(cid:2)edautomaticallybyasetofpatterns.Inevitableduringautomatican-notation,errorsatoneofthepreprocessingstagescauseerrorsattheorderingstage.Distinguishingbetweenmainandsubordinateclauses,wesplitthetotalofabout19000sentencesintotraining,developmentandtestsets(Table1).Clauseswithoneconstituentaresortedoutastrivial.ThedistributionofbothtypesofclausesaccordingtotheirlengthinconstituentsisgiveninTable2.traindevtestmain1432433441683sub3304777408total1762841212091Table1:Sizeofthedatasetsinclauses23456+main20%35%27%12%6%sub49%35%11%2%3%Table2:Proportionofclauseswithcertainlengths4http://search.cpan.org/(cid:152)holsten/Lingua-DE-Sentence-0.07/Sentence.pmGiventhesentenceinExample1,we(cid:2)rsttrans-formitsdependencyparseintoamoregeneralrepresentation(Figure15)andthen,basedonthepredictionsofourlearner,arrangethefourcon-stituents.Forevaluation,wecomparethearrangedorderagainsttheoriginalone.Notethatwepredictneitherthepositionoftheverb,northeorderwithinconstituentsastheformerisexplicitlydeterminedbythegrammar,andthelat-terismuchmorerigidthantheorderofconstituents.5BaselinesandAlgorithmsWecomparetheperformanceoftwoouralgorithmswithfourbaselines.5.1RandomWeimproveatrivialrandombaseline(RAND)bytwosyntax-orientedrules:the(cid:2)rstpositionisre-servedforthesubjectandthesecondforthedirectobjectifthereisany;theorderoftheremainingcon-stituentsisgeneratedrandomly(RANDIMP).5.2StatisticalBigramModelSimilartoRinggeretal.(2004),we(cid:2)ndtheorderwiththehighestprobabilityconditionedonsyntac-ticandsemanticcategories.Unlikethemweusede-pendencyparsesandcomputetheprobabilityofthetopnodeonly,whichismodi(cid:2)edbyallconstituents.WiththeseadjustmentstheprobabilityofanorderOgiventhehistoryh,ifconditionedonsyntacticfunctionsofconstituents(s1...sn),issimply:P(O|h)=nYi=1P(si|si−1,h)(4)Ringgeretal.(2004)donotmakeexplicit,whattheirsetofsemanticrelationsconsistsof.Fromthe5OBJAstandsfortheaccusativeobject.324

exampleinthepaper,itseemsthattheseareamix-tureoflexicalandsyntacticinformation6.Ouranno-tationdoesnotspecifysemanticrelations.Instead,someoftheconstituentsarecategorizedaspers,loc,temp,orgorundefneiftheirheadsbearoneoftheselabels.Byjoiningthesewithpossiblesyntacticfunc-tions,weobtainalargersetofsyntactic-semantictagsas,e.g.,subj-pers,pp-loc,adv-temp.Wetrans-formeachclauseinthetrainingsetintoasequenceofsuchtags,plusthreetagsfortheverbposition(v),thebeginning(b)andtheend(e)oftheclause.Thenwecomputethebigramprobabilities7.Forourthirdbaseline(BIGRAM),weselectfromallpossibleorderstheonewiththehighestprobabil-ityascalculatedbythefollowingformula:P(O|h)=nYi=1P(ti|ti−1,h)(5)wheretiisfromthesetofjoinedtags.ForExample1,possibletagsequences(i.e.orders)are’bsubj-persvadvobjasube’,’badvvsubj-persobjasube’,’bobjavadvsubsubj-perse’,etc.5.3UchimotoForthefourthbaseline(UCHIMOTO),weutilizedamaximumentropylearner(OpenNLP8)andreim-plementedthealgorithmofUchimotoetal.(2000).Foreverypossiblepermutation,itsprobabilityises-timatedaccordingtoFormula(1).Thebinaryclas-si(cid:2)er,whosetaskwastopredicttheprobabilitythattheorderofapairofconstituentsiscorrect,wastrainedonthefollowingfeaturesdescribingtheverborhc(cid:150)theheadofaconstituentc9:vlex,vpass,vmodthelemmaoftherootoftheclause(non-auxiliaryverb),thevoiceoftheverbandthenumberofconstituentstoorder;lexthelemmaofhcor,ifhcisafunctionalword,thelemmaofthewordwhichdependsonit;pospart-of-speechtagofhc;6E.g.DefDet,Coords,Possr,werden7WeusetheCMUToolkit(Clarkson&Rosenfeld,1997).8http://opennlp.sourceforge.net9Wedisregardedfeatureswhichuseinformationspeci(cid:2)ctoJapaneseandnon-applicabletoGerman(e.g.onpostpositionalparticles).semifde(cid:2)ned,thesemanticclassofc;e.g.imApril1900andmitAlbertEinstein(withAlbertEin-stein)areclassi(cid:2)edtempandpersrespectively;syn,samethesyntacticfunctionofhcandwhetheritisthesameforthetwoconstituents;modnumberofmodi(cid:2)ersofhc;repwhetherhcappearsintheprecedingsentence;prowhetherccontainsa(anaphoric)pronoun.5.4MaximumEntropyThe(cid:2)rstcon(cid:2)gurationofoursystemisanextendedversionoftheUCHIMOTObaseline(MAXENT).Tothefeaturesdescribingcweaddedthefollowingones:detthekindofdeterminermodifyinghc(def,indef,non-appl);relwhetherhcismodi(cid:2)edbyarelativeclause(yes,no,non-appl);depthedepthofc;lenthelengthofcinwords.The(cid:2)rsttwofeaturesdescribethediscoursestatusofaconstituent;theothertwoprovideinformationonits(cid:147)weight(cid:148).Sinceourlearnertreatsallvaluesasnominal,wediscretizedthevaluesofdepandlenwithaC4.5classi(cid:2)er(Kohavi&Sahami,1996).Anothermodi(cid:2)cationconcernstheef(cid:2)ciencyofthealgorithm.Insteadofcalculatingprobabilitiesforallpairs,weobtaintherightorderfromarandomonebysorting.Wecompareadjacentelementsbyconsultingthelearnerasifwewouldsortanarrayofnumbers.Giventwoadjacentconstituents,ci<cj,wechecktheprobabilityoftheirbeingintherightorder,i.e.thatciprecedescj:Ppre(ci,cj).Ifitislessthan0.5,wetransposethetwoandcompareciwiththenextone.Sincethesortingmethodpresupposesthatthepre-dictedrelationistransitive,wecheckedwhetherthisisreallysoonthedevelopmentandtestdatasets.Welookedforthreeconstituentsci,cj,ckfromasen-tenceS,suchthatPpre(ci,cj)>0.5,Ppre(cj,ck)>0.5,Ppre(ci,ck)<0.5andfoundnone.Therefore,unlikeUCHIMOTO,whereoneneedstomakeexactlyN!∗N(N−1)/2comparisons,wehavetomakeN(N−1)/2comparisonsatmost.325

5.5TheTwo-StepApproachThemaindifferencebetweenour(cid:2)rstalgorithm(MAXENT)andthesecondone(TWO-STEP)isthatwegeneratetheorderintwosteps10(bothclassi(cid:2)ersaretrainedonthesamefeatures):1.FortheVF,usingtheOpenNLPmaximumen-tropylearnerforabinaryclassi(cid:2)cation(VFvs.MF),weselecttheconstituentcwiththehigh-estprobabilityofbeingintheVF.2.FortheMF,theremainingconstituentsareputintoarandomorderandthensortedthewayitisdoneforMAXENT.ThetrainingdataforthesecondtaskwasgeneratedonlyfromtheMFofclauses.6Results6.1EvaluationMetricsWeuseseveralmetricstoevaluateoursystemsandthebaselines.The(cid:2)rstisper-sentenceaccuracy(acc)whichistheproportionofcorrectlyregener-atedsentences.Kendall’sτ,whichhasbeenusedforevaluatingsentenceorderingtasks(Lapata,2006),isthesecondmetricweuse.τiscalculatedas1−4tN(N−1),wheretisthenumberofinterchangesofconsecutiveelementstoarrangeNelementsintherightorder.τissensitivetonearmissesandassignsabdc(almostcorrectorder)ascoreof0.66whiledcba(inverseorder)gets−1.Notethatitisquestionablewhetherthismetricisasappropriateforwordorderingtasksasforsentenceorderingonesbecauseanearmissmightturnouttobeungrammat-icalwhereasamoredifferentorderstaysacceptable.Apartfromaccandτ,wealsoadoptthemetricsusedbyUchimotoetal.(2000)andRinggeretal.(2004).Theformeruseagreementrate(agr)cal-culatedas2pN(N−1):thenumberofcorrectlyorderedpairsofconstituentsoverthetotalnumberofallpos-siblepairs,aswellascompleteagreementwhichisbasicallyper-sentenceaccuracy.Unlikeτ,whichhas−1asthelowestscore,agrrangesfrom0to1.Ringgeretal.(2004)evaluatetheperformanceonlyintermsofper-constituenteditdistancecalculatedasmN,wheremistheminimumnumberofmoves1110SincesubordinateclausesdonothaveaVF,the(cid:2)rststepisnotneeded.11Amoveisadeletioncombinedwithaninsertion.neededtoarrangeNconstituentsintherightorder.Thismeasureseemslessappropriatethanτoragrbecauseitdoesnottakethedistanceofthemoveintoaccountandscoresabcedandeabcdequally(0.2).Sinceτandagr,unlikeeditdistance,givehigherscorestobetterorders,wecomputeinversedistance:inv=1(cid:150)editdistanceinstead.Thus,allthreemet-rics(τ,agr,inv)givethemaximumof1ifcon-stituentsareorderedcorrectly.However,likeτ,agrandinvcangiveapositivescoretoanungrammat-icalorder.Hence,noneoftheevaluationmetricsdescribestheperformanceperfectly.Humaneval-uationwhichreliablydistinguishesbetweenappro-priate,acceptable,grammaticalandingrammaticalorderswasoutofchoicebecauseofitshighcost.6.2ResultsTheresultsonthetestdataarepresentedinTable3.TheperformanceofTWO-STEPissigni(cid:2)cantlybetterthananyothermethod(χ2,p<0.01).TheperformanceofMAXENTdoesnotsigni(cid:2)cantlydif-ferfromUCHIMOTO.BIGRAMperformedaboutasgoodasUCHIMOTOandMAXENT.WealsocheckedhowwellTWO-STEPperformsoneachofthetwosub-tasks(Table4)andfoundthattheVFselectionisconsiderablymoredif(cid:2)cultthanthesortingpart.accτagrinvRAND15%0.020.510.64RANDIMP23%0.240.620.71BIGRAM51%0.600.800.83UCHIMOTO50%0.650.820.83MAXENT52%0.670.840.84TWO-STEP61%0.720.860.87Table3:Per-clausemeanoftheresultsThemostimportantconclusionwedrawfromtheresultsisthatthegainof9%accuracyisduetotheVFselectiononly,becausethefeaturesetsareiden-ticalforMAXENTandTWO-STEP.Fromthisfol-lowsthatdoingfeatureselectionwithoutsplittingthetaskintwoisineffective,becausetheimportanceofafeaturedependsonwhethertheVFortheMFisconsidered.FortheMF,featureselectionhasshownsynandpostobethemostrelevantfeatures.TheyalonebringtheperformanceintheMFupto75%.Incontrast,thesetwofeaturesexplainonly56%ofthe326

casesintheVF.ThisimpliesthattheorderintheMFmainlydependsongrammaticalfeatures,whilefortheVFallfeaturesareimportantbecauseremovalofanyfeaturecausedalossinaccuracy.accτagrinvTWO-STEPVF68%---TWO-STEPMF80%0.920.960.95Table4:MeanoftheresultsfortheVFandtheMFAnotherimportant(cid:2)ndingisthatthereisnoneedtoovergenerateto(cid:2)ndtherightorder.Insigni(cid:2)cantforclauseswithtwoorthreeconstituents,forclauseswith10constituents,thenumberofcomparisonsisreduceddrasticallyfrom163,296,000to45.Accordingtotheinvmetric,ourresultsarecon-siderablyworsethanthosereportedbyRinggeretal.(2004).AsmentionedinSection3,thefactthattheygeneratetheorderforeverynon-terminalnodese-riouslyin(cid:3)atestheirnumbers.Apartfromthat,theydonotreportaccuracy,anditisunknown,howmanysentencestheyactuallyreproducedcorrectly.6.3ErrorAnalysisTorevealthemainerrorsources,weanalyzedincor-rectpredictionsconcerningtheVFandtheMF,onehundredforeach.MosterrorsintheVFdidnotleadtounacceptabilityorungrammaticality.Fromlexi-calandsemanticfeatures,theclassi(cid:2)erlearnedthatsomeexpressionsareoftenusedinthebeginningofasentence.ThesearetemporalorlocationalPPs,anaphoricadverbials,someconnectivesorphrasesstartingwithunlikeX,togetherwithX,asX,etc.SuchelementswereplacedintheVFinsteadofthesubjectandcausedanerroralthoughbothvariantswereequallyacceptable.Inothercasestheclassi-(cid:2)ercouldnot(cid:2)ndabettercandidatebutthesubjectbecauseitcouldnotconcludefromtheprovidedfea-turesthatanotherconstituentwouldnicelyintroducethesentenceintothediscourse.Mainlythiscon-cernsrecognizinginformationfamiliartothereadernotbyanalreadymentionedentity,butonewhichisinferrablefromwhathasbeenread.IntheMF,manyordershadaPPtransposedwiththedirectobject.Insomecasesthepredictedorderseemedasgoodasthecorrectone.Oftenthealgo-rithmfailedatidentifyingverb-speci(cid:2)cpreferences:E.g.,someverbstakePPswiththelocationalmean-ingasanargumentandnormallyhavethemrightnexttothem,whereasothersdonot.Anotherfre-quenterrorwasthewrongplacementofsuper(cid:2)ciallyidenticalconstituents,e.g.twoPPsofthesamesize.Tohandlethiserror,thesystemneedsmorespe-ci(cid:2)csemanticinformation.Someerrorswerecausedbytheparser,whichcreatedextraconstituents(e.g.falsePPoradverbattachment)orconfusedthesub-jectwiththedirectverb.Weretrainedoursystemonacorpusofnewspaperarticles(Telljohannetal.,2003,T¤uBa-D/Z)whichismanuallyannotatedbutencodesnosemanticknowl-edge.TheresultsfortheMFwerethesameasonthedatafromWikipedia.TheresultsfortheVFweremuchworse(45%)becauseofthelackofsemanticinformation.7ConclusionWepresentedanovelapproachtoorderingcon-stituentsinGerman.Theresultsindicatethatalinguistically-motivatedtwo-stepsystem,which(cid:2)rstselectsaconstituentfortheinitialpositionandthenorderstheremainingones,workssigni(cid:2)cantlybetterthanapproacheswhichdonotmakethisseparation.Ourresultsalsocon(cid:2)rmthehypothesis(cid:150)whichhasbeenattestedinseveralcorpusstudies(cid:150)thattheor-derintheMFisratherrigidanddependentongram-maticalproperties.Wehavealsodemonstratedthatthereisnoneedtoovergenerateto(cid:2)ndthebestorder.Onaprac-ticalside,this(cid:2)ndingreducestheamountofworkconsiderably.Theoretically,itletsusconcludethattherelatively(cid:2)xedorderintheMFdependsonthesaliencewhichcanbepredictedmainlyfromgram-maticalfeatures.ItismuchhardertopredictwhichelementshouldbeplacedintheVF.Wesupposethatthisdif(cid:2)cultycomesfromthedoublefunctionoftheinitialpositionwhichcaneitherintroducethead-dressationtopic,orbethescene-orframe-settingposition(Jacobs,2001).Acknowledgements:ThisworkhasbeenfundedbytheKlausTschiraFoundation,Heidelberg,Ger-many.The(cid:2)rstauthorhasbeensupportedbyaKTFgrant(09.009.2004).WewouldalsoliketothankElkeTeichandthethreeanonymousreviewersfortheirusefulcomments.327

ReferencesBarzilay,R.&K.R.McKeown(2005).Sentencefusionformultidocumentnewssummarization.ComputationalLin-guistics,31(3):297(cid:150)327.Brants,T.(2000).TnT(cid:150)AstatisticalPart-of-Speechtagger.InProceedingsofthe6thConferenceonAppliedNaturalLan-guageProcessing,Seattle,Wash.,29April(cid:150)4May2000,pp.224(cid:150)231.Chafe,W.(1976).Givenness,contrastiveness,de(cid:2)niteness,sub-jects,topics,andpointofview.InC.Li(Ed.),SubjectandTopic,pp.25(cid:150)55.NewYork,N.Y.:AcademicPress.Clarkson,P.&R.Rosenfeld(1997).Statisticallanguagemod-elingusingtheCMU-Cambridgetoolkit.InProceedingsofthe5thEuropeanConferenceonSpeechCommunicationandTechnology,Rhodes,Greece,22-25September1997,pp.2707(cid:150)2710.Foth,K.&W.Menzel(2006).Hybridparsing:Usingproba-bilisticmodelsaspredictorsforasymbolicparser.InPro-ceedingsofthe21stInternationalConferenceonComputa-tionalLinguisticsand44thAnnualMeetingoftheAssocia-tionforComputationalLinguistics,Sydney,Australia,17(cid:150)21July2006,pp.321(cid:150)327.Frey,W.(2004).AmedialtopicpositionforGerman.Linguis-tischeBerichte,198:153(cid:150)190.Gernsbacher,M.A.&D.J.Hargreaves(1988).Accessingsen-tenceparticipants:Theadvantageof(cid:2)rstmention.JournalofMemoryandLanguage,27:699(cid:150)717.Halliday,M.A.K.(1985).IntroductiontoFunctionalGram-mar.London,UK:Arnold.Harbusch,K.,G.Kempen,C.vanBreugel&U.Koch(2006).Ageneration-orientedworkbenchforperformancegrammar:CapturinglinearordervariabilityinGermanandDutch.InProceedingsoftheInternationalWorkshoponNaturalLan-guageGeneration,Sydney,Australia,15-16July2006,pp.9(cid:150)11.Jacobs,J.(2001).Thedimensionsoftopic-comment.Linguis-tics,39(4):641(cid:150)681.Keller,F.(2000).GradienceinGrammar:ExperimentalandComputationalAspectsofDegreesofGrammaticality,(Ph.D.thesis).UniversityofEdinburgh.Kempen,G.&K.Harbusch(2004).How(cid:3)exibleiscon-stituentorderinthemid(cid:2)eldofGermansubordinateclauses?Acorpusstudyrevealingunexpectedrigidity.InProceed-ingsoftheInternationalConferenceonLinguisticEvidence,T¤ubingen,Germany,29(cid:150)31January2004,pp.81(cid:150)85.Kimball,J.(1973).Sevenprinciplesofsurfacestructureparsinginnaturallanguage.Cognition,2:15(cid:150)47.Kohavi,R.&M.Sahami(1996).Error-basedandentropy-baseddiscretizationofcontinuousfeatures.InProceedingsofthe2ndInternationalConferenceonDataMiningandKnowl-edgeDiscovery,Portland,Oreg.,2(cid:150)4August,1996,pp.114(cid:150)119.Kruijff,G.-J.,I.Kruijff-Korbayov·a,J.Bateman&E.Teich(2001).Linearorderashigher-leveldecision:Informationstructureinstrategicandtacticalgeneration.InProceedingsofthe8thEuropeanWorkshoponNaturalLanguageGener-ation,Toulouse,France,6-7July2001,pp.74(cid:150)83.Kruijff-Korbayov·a,I.,G.-J.Kruijff&J.Bateman(2002).Gen-erationofappropriatewordorder.InK.vanDeemter&R.Kibble(Eds.),InformationSharing:ReferenceandPre-suppositioninLanguageGenerationandInterpretation,pp.193(cid:150)222.Stanford,Cal.:CSLI.Kurz,D.(2000).AstatisticalaccountonwordordervariationinGerman.InA.Abeill·e,T.Brants&H.Uszkoreit(Eds.),ProceedingsoftheCOLINGWorkshoponLinguisticallyIn-terpretedCorpora,Luxembourg,6August2000.Langkilde,I.&K.Knight(1998).Generationthatexploitscorpus-basedstatisticalknowledge.InProceedingsofthe17thInternationalConferenceonComputationalLinguisticsand36thAnnualMeetingoftheAssociationforComputa-tionalLinguistics,Montr·eal,Qu·ebec,Canada,10(cid:150)14August1998,pp.704(cid:150)710.Lapata,M.(2006).Automaticevaluationofinformationorder-ing:Kendall’stau.ComputationalLinguistics,32(4):471(cid:150)484.Marsi,E.&E.Krahmer(2005).Explorationsinsentencefu-sion.InProceedingsoftheEuropeanWorkshoponNat-uralLanguageGeneration,Aberdeen,Scotland,8(cid:150)10Au-gust,2005,pp.109(cid:150)117.Pappert,S.,J.Schliesser,D.P.Janssen&T.Pechmann(2007).Corpus-andpsycholinguisticinvestigationsoflinguisticconstraintsonGermanwordorder.InA.Steube(Ed.),Thediscoursepotentialofunderspeci(cid:2)edstructures:Eventstructuresandinformationstructures.Berlin,NewYork:MoutondeGruyter.Inpress.Ringger,E.,M.Gamon,R.C.Moore,D.Rojas,M.Smets&S.Corston-Oliver(2004).Linguisticallyinformedstatisticalmodelsofconstituentstructurefororderinginsentencereal-ization.InProceedingsofthe20thInternationalConferenceonComputationalLinguistics,Geneva,Switzerland,23(cid:150)27August2004,pp.673(cid:150)679.Schmid,H.(1997).ProbabilisticPart-of-Speechtaggingusingdecisiontrees.InD.Jones&H.Somers(Eds.),NewMethodsinLanguageProcessing,pp.154(cid:150)164.London,UK:UCLPress.Sgall,P.,E.Hajiˇcov·a&J.Panevov·a(1986).TheMeaningoftheSentenceinItsSemanticandPragmaticAspects.Dordrecht,TheNetherlands:D.Reidel.Speyer,A.(2005).CompetingconstraintsonVorfeldbesetzunginGerman.InProceedingsoftheConstraintsinDiscourseWorkshop,Dortmund,3(cid:150)5July2005,pp.79(cid:150)87.Telljohann,H.,E.W.Hinrichs&S.K¤ubler(2003).StylebookfortheT¤ubingentreebankofwrittenGerman(T¤uBa-D/Z.TechnicalReport:Seminarf¤urSprachwissenschaft,Univer-sit¤atT¤ubingen,T¤ubingen,Germany.Uchimoto,K.,M.Murata,Q.Ma,S.Sekine&H.Isahara(2000).Wordorderacquisitionfromcorpora.InProceedingsofthe18thInternationalConferenceonComputationalLin-guistics,Saarbr¤ucken,Germany,31July(cid:150)4August2000,pp.871(cid:150)877.Uszkoreit,H.(1987).WordOrderandConstituentStructureinGerman.CSLILectureNotes.Stanford:CSLI.Varges,S.&C.Mellish(2001).Instance-basednaturallan-guagegeneration.InProceedingsofthe2ndConferenceoftheNorthAmericanChapteroftheAssociationforCompu-tationalLinguistics,Pittsburgh,Penn.,2(cid:150)7June,2001,pp.1(cid:150)8.Wan,S.,R.Dale,M.Dras&C.Paris(2005).Searchingforgrammaticalityandconsistency:PropagatingdependenciesintheViterbialgorithm.InProceedingsofthe10thEuro-peanWorkshoponNaturalLanguageGeneration,Aberdeen,Scotland,8(cid:150)10August,2005,pp.211(cid:150)216.Weber,A.&K.M¤uller(2004).WordordervariationinGer-manmainclauses:Acorpusanalysis.InProceedingsofthe5thInternationalWorkshoponLinguisticallyInterpretedCorpora,29August,2004,Geneva,Switzerland,pp.71(cid:150)77.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 328–335,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

328

ASymbolicApproachtoNear-DeterministicSurfaceRealisationusingTreeAdjoiningGrammarClaireGardentCNRS/LORIANancy,Franceclaire.gardent@loria.frEricKowINRIA/LORIA/UHPNancy,Franceeric.kow@loria.frAbstractSurfacerealisersdivideintothoseusedingeneration(NLGgearedrealisers)andthosemirroringtheparsingprocess(Reversiblere-alisers).Whiletheﬁrstrelyongrammarsnoteasilyusableforparsing,itisunclearhowthesecondtypeofrealiserscouldbeparam-eterisedtoyieldfromamongthesetofpos-sibleparaphrases,theparaphraseappropri-atetoagivengenerationcontext.Inthispa-per,wepresentasurfacerealiserwhichcom-binesareversiblegrammar(usedforpars-inganddoingsemanticconstruction)withasymbolicmeansofselectingparaphrases.1IntroductionIngeneration,thesurfacerealisationtaskconsistsinmappingasemanticrepresentationintoagrammati-calsentence.Dependingontheiruse,ontheirdegreeofnon-determinismandonthetypeofgrammartheyas-sume,existingsurfacerealiserscanbedividedintotwomaincategoriesnamely,NLG(NaturalLan-guageGeneration)gearedrealisersandreversiblerealisers.NLGgearedrealisersaremeantasmodulesinafull-blowngenerationsystemandassuch,theyareconstrainedtobedeterministic:agenerationsystemmustoutputexactlyonetext,noless,nomore.Inor-dertoensurethisdeterminism,NLGgearedrealisersgenerallyrelyontheoriesofgrammarwhichsys-tematicallylinkformtofunctionsuchassystemicfunctionalgrammar(SFG,(MatthiessenandBate-man,1991))and,toalesserextent,MeaningTextTheory(MTT,(Mel’cuk,1988)).Inthesetheories,asentenceisassociatednotjustwithasemanticrep-resentationbutwithasemanticrepresentationen-richedwithadditionalsyntactic,pragmaticand/ordiscourseinformation.Thisadditionalinformationisthenusedtoconstraintherealiseroutput.1OnedrawbackoftheseNLGgearedrealisershowever,isthatthegrammarusedisnotusuallyreversiblei.e.,cannotbeusedbothforparsingandforgeneration.Giventhetimeandexpertiseinvolvedindevelopingagrammar,thisisanon-trivialdrawback.Reversiblerealisersontheotherhand,aremeanttomirrortheparsingprocess.Theyareusedonagrammardevelopedforparsingandequippedwithacompositionalsemantics.Givenastringandsuchagrammar,aparserwillassigntheinputstringallthesemanticrepresentationsassociatedwiththatstringbythegrammar.Conversely,givenaseman-ticrepresentationandthesamegrammar,arealiserwillassigntheinputsemanticsallthestringsas-sociatedwiththatsemanticsbythegrammar.Insuchapproaches,non-determinismisusuallyhan-dledbystatisticalﬁltering:treebankinducedprob-abilitiesareusedtoselectfromamongthepossibleparaphrases,themostprobableone.Sincethemostprobableparaphraseisnotnecessarilythemostap-propriateoneinagivencontext,itisunclearhow-ever,howsuchrealiserscouldbeintegratedintoagenerationsystem.Inthispaper,wepresentasurfacerealiserwhich1Ontheotherhand,oneofourreviewersnotedthat“de-terminism”oftencomesmorefromdefaultswheninputcon-straintsarenotsupplied.Onemightseetheserealisersasbeinglessdeterministicthanadvertised;however,thepointisthatitispossibletosupplytheconstraintsthatensuredeterminism.329

combinesreversibilitywithasymbolicapproachtodeterminism.Thegrammarusedisfullyreversible(itisusedforparsing)andtherealisationalgorithmcanbeconstrainedbytheinputsoastoensureauniqueoutputconformingtotherequirementofagiven(generation)context.Weshowboththatthegrammarusedhasagoodparaphrasticpower(itisdesignedinsuchawaythatgrammaticalpara-phrasesareassignedthesamesemanticrepresenta-tions)andthattherealisationalgorithmcanbeusedeithertogenerateallthegrammaticalparaphrasesofagiveninputorjustoneprovidedtheinputisade-quatelyconstrained.Thepaperisstructuredasfollows.Section2in-troducesthegrammarusednamely,aFeatureBasedLexicalisedTreeAdjoiningGrammarenrichedwithacompositionalsemantics.Importantly,thisgram-mariscompiledfromamoreabstractspeciﬁcation(aso-called“meta-grammar”)andasweshallsee,itisthisfeaturewhichpermitsanaturalandsystem-aticcouplingofsemanticliteralswithsyntactican-notations.Section3deﬁnesthesurfacerealisationalgorithmusedtogeneratesentencesfromsemanticformulae.Thisalgorithmisnon-deterministicandproducesallparaphrasesassociatedbythegram-marwiththeinputsemantics.Wethengoontoshow(section4)howthisalgorithmcanbeusedonasemanticinputenrichedwithsyntacticormoreabstractcontrolannotationsandfurther,howtheseannotationscanbeusedtoselectfromamongthesetofadmissibleparaphrasespreciselythesewhichobeytheconstraintsexpressedintheaddedannota-tions.Section5reportsonaquantitativeevaluationbasedontheuseofacoretreeadjoininggrammarforFrench.Theevaluationgivesanindicationoftheparaphrasingpowerofthegrammarusedaswellassomeevidenceofthedeterministicnatureofthere-aliser.Section6relatestheproposedapproachtoexistingworkandsection7concludeswithpointersforfurtherresearch.2ThegrammarWeuseauniﬁcationbasedversionofLTAGnamely,Feature-basedTAG.AFeature-basedTAG(FTAG,(Vijay-ShankerandJoshi,1988))consistsofasetof(auxiliaryorinitial)elementarytreesandoftwotreecompositionoperations:substitutionandad-junction.Initialtreesaretreeswhoseleavesarela-belledwithsubstitutionnodes(markedwithadow-narrow)orterminalcategories.Auxiliarytreesaredistinguishedbyafootnode(markedwithastar)whosecategorymustbethesameasthatoftherootnode.Substitutioninsertsatreeontoasubstitutionnodeofsomeothertreewhileadjunctioninsertsanauxiliarytreeintoatree.InanFTAG,thetreenodesarefurthermoredecoratedwithtwofeaturestruc-tures(calledtopandbottom)whichareuniﬁeddur-ingderivationasfollows.Onsubstitution,thetopofthesubstitutionnodeisuniﬁedwiththetopoftherootnodeofthetreebeingsubstitutedin.Onadjunc-tion,thetopoftherootoftheauxiliarytreeisuni-ﬁedwiththetopofthenodewhereadjunctiontakesplace;andthebottomfeaturesofthefootnodeareuniﬁedwiththebottomfeaturesofthisnode.Attheendofaderivation,thetopandbottomofallnodesinthederivedtreeareuniﬁed.Toassociatesemanticrepresentationswithnatu-rallanguageexpressions,theFTAGismodiﬁedasproposedin(GardentandKallmeyer,2003).NPjJohnname(j,john)SNP↓sVPrVrunsrun(r,s)VPxoftenVP*often(x)⇒name(j,john),run(r,j),often(r)Figure1:FlatSemanticsfor“Johnoftenruns”Eachelementarytreeisassociatedwithaﬂatse-manticrepresentation.Forinstance,inFigure1,2thetreesforJohn,runsandoftenareassociatedwiththesemanticsname(j,john),run(r,s)andoften(x)re-spectively.Importantly,theargumentsofasemanticfunctorarerepresentedbyuniﬁcationvariableswhichoccurbothinthesemanticrepresentationofthisfunctorandonsomenodesoftheassociatedsyntactictree.ForinstanceinFigure1,thesemanticindexsoc-curringinthesemanticrepresentationofrunsalsooccursonthesubjectsubstitutionnodeoftheasso-ciatedelementarytree.2Cx/CxabbreviateanodewithcategoryCandatop/bottomfeaturestructureincludingthefeature-valuepair{index:x}.330

Thevalueofsemanticargumentsisdeterminedbytheuniﬁcationsresultingfromadjunctionandsub-stitution.Forinstance,thesemanticindexsinthetreeforrunsisuniﬁedduringsubstitutionwiththesemanticindiceslabellingtherootnodesofthetreeforJohn.Asaresult,thesemanticsofJohnoftenrunsis(1){name(j,john),run(r,j),often(r)}ThegrammaruseddescribesacorefragmentofFrenchandcontainsaround6000elementarytrees.Itcoverssome35basicsubcategorisationframesandforeachoftheseframes,thesetofargumentre-distributions(active,passive,middle,neuter,reﬂex-ivisation,impersonal,passiveimpersonal)andofar-gumentrealisations(cliticisation,extraction,omis-sion,permutations,etc.)possibleforthisframe.Asaresult,itcapturesmostgrammaticalparaphrasesthatis,paraphrasesduetodivergingargumentreal-isationsortodifferentmeaningpreservingalterna-tion(e.g.,active/passiveorclefted/non-cleftedsen-tence).3Thesurfacerealiser,GenIThebasicsurfacerealisationalgorithmusedisabot-tomup,tabularrealisationalgorithm(Kay,1996)optimisedforTAGs.Itfollowsathreestepstrat-egywhichcanbesummarisedasfollows.Givenanemptyagenda,anemptychartandaninputseman-ticsφ:Lexicalselection.Selectallelementarytreeswhosesemanticssubsumes(partof)φ.Storethesetreesintheagenda.Auxiliarytreesdevoidofsubstitutionnodesarestoredinaseparateagendacalledtheauxiliaryagenda.Substitutionphase.Retrieveatreefromtheagenda,addittothechartandtrytocombineitbysubstitutionwithtreespresentinthechart.Addanyresultingderivedtreetotheagenda.Stopwhentheagendaisempty.Adjunctionphase.Movethecharttreestotheagendaandtheauxiliaryagendatreestothechart.Retrieveatreefromtheagenda,addittothechartandtrytocombineitbyadjunctionwithtreespresentinthechart.Addanyresult-ingderivedtreetotheagenda.Stopwhentheagendaisempty.Whenprocessingstops,theyieldofanysyntacti-callycompletetreewhosesemanticsisφyieldsanoutputi.e.,asentence.Theworkingsofthisalgorithmcanbeillustratedbythefollowingexample.Supposethattheinputse-manticsis(1).Inaﬁrststep(lexicalselection),theelementarytreesselectedaretheonesforJohn,runs,often.Theirsemanticssubsumespartoftheinputse-mantics.ThetreesforJohnandrunsareplacedontheagenda,theoneforoftenisplacedontheauxil-iaryagenda.Thesecondstep(thesubstitutionphase)consistsinsystematicallyexploringthepossibilityofcom-biningtwotreesbysubstitution.Here,thetreeforJohnissubstitutedintotheoneforruns,andthere-sultingderivedtreeforJohnrunsisplacedontheagenda.Treesontheagendaareprocessedonebyoneinthisfashion.Whentheagendaisempty,in-dicatingthatallcombinationshavebeentried,weprepareforthenextphase.Allitemscontaininganemptysubstitutionnodeareerasedfromthechart(here,thetreeanchoredbyruns).Theagendaisthenreinitialisedtothecontentofthechartandthecharttothecontentoftheaux-iliaryagenda(hereoften).Theadjunctionphaseproceedsmuchlikethepreviousphase,exceptthatnowallpossibleadjunctionsareperformed.Whentheagendaisemptyoncemore,theitemsinthechartwhosesemanticsmatchestheinputsemanticsarese-lected,andtheirstringsprintedout,yieldinginthiscasethesentenceJohnoftenruns.4ParaphraseselectionThesurfacerealisationalgorithmjustsketchedisnon-deterministic.Givenasemanticformula,itmightproduceseveraloutputs.Forinstance,giventheappropriategrammarforFrench,theinputin(2a)willgeneratethesetofparaphrasespartlygivenin(2b-2k).(2)a.lj:jean(j)la:aime(e,j,m)lm:marie(m)b.JeanaimeMariec.Marieestaim´eeparJeand.C’estJeanquiaimeMariee.C’estJeanparquiMarieestaim´eef.C’estparJeanqu’estaim´eeMarieg.C’estJeandontestaim´eeMarieh.C’estJeandontMarieestaim´eei.C’estMariequiestaim´eeparJean331

j.C’estMariequ’aimeJeank.C’estMariequeJeanaimeToselectfromamongallpossibleparaphrasesofagiveninput,exactlyoneparaphrase,NLGgearedrealisersusesymbolicinformationtoencodesyn-tactic,stylisticorpragmaticconstraintsontheout-put.Thusforinstance,bothREALPRO(LavoieandRambow,1997)andSURGE(ElhadadandRobin,1999)assumethattheinputassociatessemanticlit-eralswithlowlevelsyntacticandlexicalinforma-tionmostlyleavingtherealisertojusthandlein-ﬂection,wordorder,insertionofgrammaticalwordsandagreement.Similarly,KPML(MatthiessenandBateman,1991)assumesaccesstoideational,inter-personalandtextualinformationwhichroughlycor-respondstosemantic,mood/voice,theme/rhemeandfocus/groundinformation.Inwhatfollows,weﬁrstshowthatthesemanticinputassumedbytherealisersketchedintheprevi-oussectioncanbesystematicallyenrichedwithsyn-tacticinformationsoastoensuredeterminism.Wethenindicatehowthesatisﬁabilityofthisenrichedinputcouldbecontrolled.4.1AtmostonerealisationIntherealisationalgorithmsketchedinSection3,non-determinismstemsfromlexicalambiguity:3foreach(combinationof)literal(s)lintheinputthereusuallyismorethanoneTAGelementarytreewhosesemanticssubsumesl.Thuseach(combinationof)literal(s)intheinputselectsasetofelementarytreesandtherealiseroutputisthesetofcombi-nationsofselectedlexicaltreeswhicharelicensedbythegrammaroperations(substitutionandadjunc-tion)andwhosesemanticsistheinput.Onewaytoenforcedeterminismconsistsinen-suringthateachliteralintheinputselectsexactlyoneelementarytree.Forinstance,supposewewanttogenerate(2b),repeatedhereas(3a),ratherthan3GiventwoTAGtrees,theremightalsobeseveralwaysofcombiningthemtherebyinducingmorenon-determinism.Howeverinpracticewefoundthatmostofthisnon-determinismisdueeithertoover-generation(caseswherethegrammarisnotsufﬁcientlyconstrainedandallowsforonetreetoadjointoanothertreeinseveralplaces)ortospuriousderiva-tion(distinctderivationswithidenticalsemantics).Thefewre-mainingcasesthatarelinguisticallycorrectareduetovaryingmodiﬁerpositionsandcouldbeconstrainedbyasophisticatedfeaturedecorationsintheelementarytree.anyoftheparaphraseslistedin(2c-2k).Intuitively,thesyntacticconstraintstobeexpressedarethosegivenin(3b).(3)a.JeanaimeMarieb.CanonicalNominalSubject,Activeverbform,CanonicalNominalObjectc.lj:jean(j)la:aime(e,j,m)lm:marie(m)Thequestionishowpreciselytoformulatetheseconstraints,howtoassociatethemwiththeseman-ticinputassumedinSection3andhowtoensurethattheconstraintsuseddoenforceuniquenessofselection(i.e.,thatforeachinputliteral,exactlyoneelementarytreeisselected)?Toanswerthis,werelyonafeatureofthegrammarused,namelythateachelementarytreeisassociatedwithalinguisticallymeaningfuluniqueidentiﬁer.Thereasonforthisisthatthegrammariscom-piledfromahigherleveldescriptionwheretreefrag-mentsareﬁrstencapsulatedintoso-calledclassesandthenexplicitlycombined(byinheritance,con-junctionanddisjunction)toproducethegrammarelementarytrees(cf.(Crabb´eandDuchier,2004)).Moregenerally,eachelementarytreeinthegram-marisassociatedwiththesetofclassesusedtopro-ducethattreeandimportantly,thissetofclasses(wewillcallthisthetreeidentiﬁer)providesadis-tinguishingdescription(auniqueidentiﬁer)forthattree:atreeisdeﬁnedbyaspeciﬁccombinationofclassesandconversely,aspeciﬁccombinationofclassesyieldsauniquetree.4Thusthesetofclassesassociatedbythecompilationprocesswithagivenelementarytreecanbeusedtouniquelyidentifythattree.Giventhis,surfacerealisationisconstrainedasfollows.1.EachtreeidentiﬁerId(tree)ismappedintoasimpliﬁedsetoftreepropertiesTPt.Therearetworeasonsforthissimpliﬁcation.First,someclassesareirrelevant.Forinstance,theclassusedtoenforcesubject-verbagreementisneededtoensurethisagreementbutdoesnothelpinselectingamongcompetingtrees.Second,agivenclassCcanbedeﬁnedtobe4Thisisnotabsolutelytrueasatreeidentiﬁeronlyreﬂectspartofthecompilationprocess.Inpractice,theyarefewex-ceptionsthoughsothatdistincttreeswhosetreeidentiﬁersareidenticalcanbemanuallydistinguished.332

equivalenttothecombinationofotherclassesC1...CnandconsequentlyatreeidentiﬁercontainingC,C1...Cncanbereducedtoin-cludeeitherCorC1...Cn.2.EachliteralliintheinputisassociatedwithatreepropertysetTPi(i.e.,theinputwegener-atefromisenrichedwithsyntacticinformation)3.Duringrealisation,foreachliteral/treepropertypairhli:TPiiintheenrichedinputsemantics,lexicalselectionisconstrainedtoretrieveonlythosetrees(i)whosesemanticssubsumesliand(ii)whosetreepropertiesareTPiSinceeachliteralisassociatedwitha(simpli-ﬁed)treeidentiﬁerandeachtreeidentiﬁeruniquelyidentiﬁesanelementarytree,realisationproducesatmostonerealisation.Examples4a-4cillustratesthekindofconstraintsusedbytherealiser.(4)a.lj:jean(j)/ProperNamela:aime(e,j,m)/[CanonicalNominalSubject,ActiveVerbForm,CanonicalNominalObject]lm:marie(m)/ProperNameJeanaimeMarie*Jeanestaim´edeMarieb.lc:le(c)/Detlc:chien(c)/Nounld:dort(e1,c)/RelativeSubjectlr:ronﬂe(e2,c)/CanonicalSubjectLechienquidortronﬂe*Lechienquironﬂedortc.lj:jean(j)/ProperNamelp:promise(e1,j,m,e2)/[CanonicalNominalSubject,ActiveVerbForm,CompletiveObject]lm:marie(m)/ProperNamele2:partir(e2,j)/InﬁnitivalVerbJeanpromet`amariedepartir*Jeanpromet`amariequ’ilpartira4.2AtleastonerealisationForarealisertobeusablebyagenerationsystem,theremustbesomemeanstoensurethatitsinputissatisﬁablei.e.,thatitcanberealised.Howcanthisbedonewithoutactuallycarryingoutrealisationi.e.,withoutcheckingthattheinputissatisﬁable?Existingrealisersindicatetwotypesofanswerstothatdilemma.Aﬁrstpossibilitywouldbetodrawon(Yangetal.,1991)’sproposalandcomputetheenrichedin-putbasedonthetraversalofasystemicnetwork.Morespeciﬁcally,onepossibilitywouldbetocon-siderasystemicnetworksuchasNIGEL,precom-pileallthefunctionalfeaturesassociatedwitheachpossibletraversalofthenetwork,mapthemontothecorrespondingtreepropertiesandusetheresultingsetoftreepropertiestoensurethesatisﬁabilityoftheenrichedinput.Anotheroptionwouldbetocheckthewellformednessoftheinputatsomelevelofthelinguis-tictheoryonwhichtherealiserisbased.Thusforinstance,REALPROassumesasinputawellformeddeepsyntacticstructure(DSyntS)asdeﬁnedbyMeaningTextTheory(MTT)andsimilarly,SURGEtakesasinputafunctionaldescription(FD)whichinessenceisanunderspeciﬁedgrammaticalstructurewithintheSURGEgrammar.Inbothcases,thereisnoguaranteethattheinputbesatisﬁablesincealltheotherlevelsofthelinguistictheorymustbeveriﬁedforthistobetrue.InMTT,theDSyntSmustﬁrstbemappedontoasurfacesyntacticstruc-tureandthensuccessivelyontotheotherlevelsofthetheorywhileinSURGE,theinputFDcanbere-alisedonlyifitprovidesconsistentinformationforacompletetop-downtraversalofthegrammarrightdowntothelexicallevel.Inshort,inbothcases,thewellformednessoftheinputcanbecheckedwithrespecttosomecriteria(e.g.,wellformednessofadeepsyntacticstructureinMTT,wellformednessofaFDinSURGE)butthiswellformednessdoesnotguaranteesatisﬁability.Nonethelessthisbasicwellformednesscheckisimportantasitprovidessomeguidanceastowhatanacceptableinputtothere-alisershouldlooklike.Weadoptasimilarstrategyandresorttotheno-tionofpolarityneutralinputtocontrolthewellformednessoftheenrichedinput.Theproposaldrawsonideasfrom(KollerandStriegnitz,2002;GardentandKow,2005)andaimstodeterminewhetherforagiveninput(asetofTAGelemen-tarytreeswhosesemanticsequatetheinputseman-tics),syntacticrequirementsandresourcescancelout.Morespeciﬁcally,theaimistodeterminewhethergiventheinputsetofelementarytrees,eachsubstitutionandeachadjunctionrequirementissat-isﬁedbyexactlyoneelementarytreeoftheappro-priatesyntacticcategoryandsemanticindex.333

Roughly,5thetechniqueconsistsin(automati-cally)associatingwitheachelementarytreeapo-laritysignaturereﬂectingitssubstitution/adjunctionrequirementsandresourcesandincomputingthegrandpolarityofeachpossiblecombinationoftreescoveringtheinputsemantics.Eachsuchcombina-tionwhosetotalpolarityisnon-nullisthenﬁlteredout(notconsideredforrealisation)asitcannotpos-siblyleadtoavalidderivation(eitherarequirementcannotbesatisﬁedoraresourcecannotbeused).Inthecontextofagenerationsystem,polaritycheckingcanbeusedtocheckthesatisﬁabilityoftheinputormoreinterestingly,tocorrectanillformedinputi.e.,aninputwhichcanbedetectedasbeingunsatisﬁable.Tocheckagiveninput,itsufﬁcestocomputeitspolaritycount.Ifitisnon-null,theinputisunsatis-ﬁableandshouldberevised.Thisisnotveryusefulhowever,astheenrichedinputensuresdeterminismandtherebymakerealisationveryeasy,indeedal-mostaseasyaspolaritychecking.Moreinterestingly,polaritycheckingcanbeusedtosuggestwaysofﬁxinganillformedinput.Insuchacase,theenrichedinputisstrippedofitscontrolannotations,realisationproceedsonthebasisofthissimpliﬁedinputandpolaritycheckingisusedtopre-selectallpolarityneutralcombinationsofelemen-tarytrees.Aclosestmatch(i.e.thepolarityneutralcombinationwiththegreatestnumberofcontrolan-notationsincommonwiththeillformedinput)totheillformedinputisthenproposedasaprobablysatisﬁablealternative.5EvaluationToevaluateboththeparaphrasticpowerofthere-aliserandtheimpactofthecontrolannotationsonnon-determinism,weusedagraduatedtest-suitewhichwasbuiltby(i)parsingasetofsentences,(ii)selectingthecorrectmeaningrepresentationsfromtheparseroutputand(iii)generatingfromthesemeaningrepresentations.Thegradationinthetestsuitecomplexitywasobtainedbypartitioningtheinputintosentencescontainingone,twoorthreeﬁ-niteverbsandbychoosingcasesallowingfordiffer-entparaphrasingpatterns.Morespeciﬁcally,thetest5Lackofspacepreventsusfromgivingmuchdetailshere.Wereferthereaderto(KollerandStriegnitz,2002;GardentandKow,2005)formoredetails.suiteincludescasesinvolvingthefollowingtypesofparaphrases:•Grammaticalvariationsintherealisationsofthearguments(cleft,cliticisation,question,rel-ativisation,subject-inversion,etc.)oroftheverb(active/passive,impersonal)•Variationsintherealisationofmodiﬁers(e.g.,relativeclausevsadjective,predicativevsnon-predicativeadjective)•Variationsinthepositionofmodiﬁers(e.g.,pre-vspost-nominaladjective)•Variationslicensedbyamorpho-derivationallink(e.g.,toarrive/arrival)Onatestsetof80cases,theparaphrasticlevelvariesbetween1andover50withanaverageof18paraphrasesperinput(taking36asuppercutoffpointintheparaphrasescount).Figure5givesamoredetaileddescriptionofthedistributionoftheparaphrasticvariation.Inessence,42%ofthesentenceswithoneﬁniteverbaccept1to3para-phrases(casesofintransitiveverbs),44%accept4to28paraphrases(verbsofarity2)and13%yieldmorethan29paraphrases(ditransitives).Forsen-tencescontainingtwoﬁniteverbs,theratiois5%for1to3paraphrases,36%for4to14paraphrasesand59%formorethan14paraphrases.Finally,sen-tencescontaining3ﬁniteverbsallacceptmorethan29paraphrases.Twothingsareworthnotinghere.First,thepara-phraseﬁguresmightseemlowwrttoe.g.,workby(VelldalandOepen,2006)whichmentionsseveralthousandoutputsforonegiveninputandanaveragenumberofrealisationsperinputvaryingbetween85.7and102.2.Admittedly,theFrenchgrammarweareusinghasamuchmorelimitedcoveragethantheERG(thegrammarusedby(VelldalandOepen,2006))anditispossiblethatitsparaphrasticpowerislower.However,thecountswegiveonlytakeintoaccountvalidparaphrasesoftheinput.Inotherwords,overgenerationandspuriousderivationsareexcludedfromthetoll.Thisdoesnotseemtobethecasein(VelldalandOepen,2006)’sapproachwherethecountseemstoincludeallsentencesassociatedbythegrammarwiththeinputsemantics.Second,althoughthetestsetmayseemsmallitisimportanttokeepinmindthatitrepresents80inputs334

withdistinctgrammaticalandparaphrasticproper-ties.Ineffect,these80testcasesyields1528dis-tinctwell-formedsentences.ThisﬁgurecomparesfavourablywiththesizeofthelargestregressiontestsuiteusedbyasymbolicNLGrealisernamely,theSURGEtestsuitewhichcontains500inputeachcorrespondingtoasinglesentence.Italsocomparesreasonablywithothermorerecentevaluations(Call-away,2003;Langkilde-Geary,2002)whichderivetheirinputdatafromthePennTreebankbytrans-formingeachsentencetreeintoaformatsuitablefortherealiser(Callaway,2003).Fortheseapproaches,thetestsetsizevariesbetweenroughly1000andalmost3000sentences.Butagain,itisworthstress-ingthattheseevaluationsaimatassessingcoverageandcorrectness(doestherealiserﬁndthesentenceusedtoderivetheinputbyparsingit?)ratherthantheparaphrasticpowerofthegrammar.Theyfailtoprovideasystematicassessmentofhowmanydis-tinctgrammaticalparaphrasesareassociatedwitheachgiveninput.Toverifytheclaimthattreepropertiescanbeusedtoensuredeterminism(cf.footnote4),westartedbyeliminatingfromtheoutputallill-formedsen-tences.Wethenautomaticallyassociatedeachwell-formedoutputwithitssetoftreeproperties.Finally,foreachinputsemantics,wedidasystematicpair-wisecomparisonofthetreepropertysetsassociatedwiththeinputrealisationsandwecheckedwhetherforanygiveninput,thereweretwo(ormore)dis-tinctparaphraseswhosetreepropertieswerethesame.Wefoundthatsuchcasesrepresentedslightlyover2%ofthetotalnumberof(input,realisations)pairs.Closerinvestigationofthefaultydataindi-catestwomainreasonsfornon-determinismnamely,treeswithalternatingorderofargumentsandderiva-tionswithdistinctmodiﬁeradjunctions.Bothcasescanbehandledbymodifyingthegrammarinsuchawaythatthosedifferencesarereﬂectedinthetreeproperties.6RelatedworkTheapproachpresentedherecombinesareversiblegrammarrealiserwithasymbolicapproachtopara-phraseselection.Wenowcompareittoexistingsur-facesrealisers.NLGgearedrealisers.ProminentgeneralpurposeNLGgearedrealisersincludeREALPRO,SURGE,KPML,NITROGENandHALOGEN.Fur-thermore,HALOGENhasbeenshowntoachievebroadcoverageandhighqualityoutputonasetof2400inputautomaticallyderivedfromthePenntree-bank.ThemaindifferencebetweentheseandthepresentapproachisthatourapproachisbasedonareversiblegrammarwhilstNLGgearedrealisersarenot.Thishasseveralimportantconsequences.First,itmeansthatoneandthesamegrammarandlexiconcanbeusedbothforparsingandforgener-ation.Giventhecomplexityinvolvedindevelopingsuchresources,thisisanimportantfeature.Second,asdemonstratedintheRedwoodLingoTreebank,reversibilitymakesiteasytorapidlycre-ateverylargeevaluationsuites:itsufﬁcestoparseasetofsentencesandselectfromtheparseroutputthecorrectsemantics.Incontrast,NLGgearedrealis-erseitherworkonevaluationsetsofrestrictedsize(500inputforSURGE,210forKPML)orrequirethetimeexpensiveimplementationofapreprocessortransforminge.g.,PennTreebanktreesintoaformatsuitablefortherealisers.Forinstance,(Callaway,2003)reportsthattheimplementationofsuchapro-cessorforSURGEwasthemosttimeconsumingpartoftheevaluationwiththeresultingcomponentcon-taining4000linesofcodeand900rules.Third,areversiblegrammarcanbeexploitedtosupportnotonlyrealisationbutalsoitsreverse,namelysemanticconstruction.Indeed,reversibilityisensuredthroughacompositionalsemanticsthatis,throughatightcouplingbetweensyntaxandseman-tics.Incontrast,NLGgearedrealisersoftenhavetoreconstructthisassociationinratheradhocways.Thusforinstance,(Yangetal.,1991)resortstoad335

hoc“mappingtables”toassociatesubstitutionnodeswithsemanticindicesand“fr-nodes”toconstrainadjunctiontothecorrectnodes.Moregenerally,thelackofaclearlydeﬁnedcompositionalsemanticsinNLGgearedrealisersmakesitdifﬁculttoseehowthegrammartheyusecouldbeexploitedtoalsosup-portsemanticconstruction.Fourth,thegrammarcanbeusedbothtogener-ateandtodetectparaphrases.Itcouldbeusedforinstance,incombinationwiththeparserandthese-manticconstructionmoduledescribedin(GardentandParmentier,2005),tosupporttextualentailmentrecognitionoranswerdetectioninquestionanswer-ing.Reversiblerealisers.Therealiserpresentedherediffersinmainlytwowaysfromexistingreversiblerealiserssuchas(White,2004)’sCCGsystemortheHPSGERGbasedrealiser(CarrollandOepen,2005).First,itpermitsasymbolicselectionoftheout-putparaphrase.Incontrast,existingreversiblere-alisersusestatisticalinformationtoselectfromtheproducedoutputthemostplausibleparaphrase.Second,particularattentionhasbeenpaidtothetreatmentofparaphrasesinthegrammar.RecallthatTAGelementarytreesaregroupedintofamiliesandfurther,thatthespeciﬁcTAGweuseiscom-piledfromahighlyfactoriseddescription.Werelyonthesefeaturestoassociateoneandthesamese-mantictolargesetsoftreesdenotingsemanticallyequivalentbutsyntacticallydistinctconﬁgurations(cf.(Gardent,2006)).7ConclusionTherealiserpresentedhere,GENI,exploitsagram-marwhichisproducedsemi-automaticallybycom-pilingahighlevelgrammardescriptionintoaTreeAdjoiningGrammar.Wehavearguedthataside-effectofthiscompilationprocess–namely,theas-sociationwitheachelementarytreeofasetoftreeproperties–canbeusedtoconstraintherealiseroutput.Theresultingsystemcombinestheadvan-tagesoftwoorthogonalapproaches.Fromthere-versibleapproach,ittakesthereusability,theabilitytorapidlycreateverylargetestsuitesandthecapac-itytobothgenerateanddetectparaphrases.FromtheNLGgearedparadigm,ittakestheabilitytosymbolicallyconstraintherealiseroutputtoagivengenerationcontext.GENIisfree(GPL)softwareandisavailableathttp://trac.loria.fr/˜geni.ReferencesCharlesB.Callaway.2003.Evaluatingcoverageforlargesym-bolicNLGgrammars.In18thIJCAI,pages811–817,Aug.J.CarrollandS.Oepen.2005.Highefﬁciencyrealizationforawide-coverageuniﬁcationgrammar.2ndIJCNLP.B.Crabb´eandD.Duchier.2004.Metagrammarredux.InCSLP,Copenhagen.M.ElhadadandJ.Robin.1999.SURGE:acomprehensiveplug-insyntacticrealizationcomponentfortextgeneration.ComputationalLinguistics.C.GardentandL.Kallmeyer.2003.SemanticconstructioninFTAG.In10thEACL,Budapest,Hungary.C.GardentandE.Kow.2005.Generatingandselectinggram-maticalparaphrases.ENLG,Aug.C.GardentandY.Parmentier.2005.Largescalesemanticcon-structionforTreeAdjoiningGrammars.LACL05.C.Gardent.2006.Integrationd’unedimensionsemantiquedanslesgrammairesd’arbresadjoints.TALN.M.Kay.1996.ChartGeneration.In34thACL,pages200–204,SantaCruz,California.A.KollerandK.Striegnitz.2002.Generationasdependencyparsing.In40thACL,Philadelphia.I.Langkilde-Geary.2002.Anempiricalveriﬁcationofcover-ageandcorrectnessforageneral-purposesentencegenera-tor.InProceedingsoftheINLG.B.LavoieandO.Rambow.1997.RealPro–afast,portablesentencerealizer.ANLP’97.C.MatthiessenandJ.A.Bateman.1991.Textgenerationandsystemic-functionallinguistics:experiencesfromEn-glishandJapanese.FrancesPinterPublishersandSt.Mar-tin’sPress,LondonandNewYork.I.A.Mel’cuk.1988.DependencySyntax:TheorieandPrac-tice.StateUniversityPressofNewYork.ErikVelldalandStephanOepen.2006.Statisticalrankingintacticalgeneration.InEMNLP,Sydney,Australia.K.Vijay-ShankerandAKJoshi.1988.FeatureStructuresBasedTreeAdjoiningGrammars.Proceedingsofthe12thconferenceonComputationallinguistics,55:v2.M.White.2004.ReininginCCGchartrealization.InINLG,pages182–191.G.Yang,K.McKoy,andK.Vijay-Shanker.1991.Fromfunc-tionalspeciﬁcationtosyntacticstructure.ComputationalIn-telligence,7:207–219.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 336–343,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

336

SentencegenerationasaplanningproblemAlexanderKollerCenterforComputationalLearningSystemsColumbiaUniversitykoller@cs.columbia.eduMatthewStoneComputerScienceRutgersUniversityMatthew.Stone@rutgers.eduAbstractWetranslatesentencegenerationfromTAGgrammarswithsemanticandpragmaticin-formationintoaplanningproblembyencod-ingthecontributionofeachworddeclara-tivelyandexplicitly.Thisallowsustoex-ploittheperformanceofoff-the-shelfplan-ners.Italsoopensupnewperspectivesonreferringexpressiongenerationandtherela-tionshipbetweenlanguageandaction.1IntroductionSystemsthatproducenaturallanguagemustsynthe-sizetheprimitivesoflinguisticstructureintowell-formedutterancesthatmakedesiredcontributionstodiscourse.Thisisfundamentallyaplanningprob-lem:Eachlinguisticprimitivemakescertaincon-tributionswhilepotentiallyintroducingnewgoals.Inthispaper,wemakethisperspectiveexplicitbytranslatingthesentencegenerationproblemofTAGgrammarswithsemanticandpragmaticinformationintoaplanningproblemstatedinthewidelyusedPlanningDomainDeﬁnitionLanguage(PDDL,Mc-Dermott(2000)).Theencodingprovidesacleanseparationbetweencomputationandlinguisticmod-ellingandisopentofutureextensions.Italsoallowsustobeneﬁtfromthepastandongoingadvancesintheperformanceofoff-the-shelfplanners(BlumandFurst,1997;KautzandSelman,1998;HoffmannandNebel,2001).Whiletherehavebeenprevioussystemsthaten-codegenerationasplanning(CohenandPerrault,1979;Appelt,1985;HeemanandHirst,1995),ourapproachisdistinguishedfromthesesystemsbyitsfocusonthegrammaticallyspeciﬁedcontributionsofeachindividualword(andtheTAGtreeitan-chors)tosyntax,semantics,andlocalpragmatics(Hobbsetal.,1993).Forexample,wordsdirectlyachievecontentgoalsbyaddingacorrespondingse-manticprimitivetotheconversationalrecord.Wedeliberatelyavoidreasoningaboututterancesasco-ordinatedrationalbehavior,asearliersystemsdid;thisallowsustogetbywithamuchsimplerlogic.Theproblemwesolveencompassesthegenera-tionofreferringexpressions(REs)asaspecialcase.Unlikesomeapproaches(DaleandReiter,1995;HeemanandHirst,1995),wedonothavetodis-tinguishbetweengeneratingNPsandexpressionsofothersyntacticcategories.Wedevelopanewper-spectiveonthelifecycleofadistractor,whichallowsustogeneratemoresuccinctREsbytakingtherestoftheutteranceintoaccount.Moregenerally,wedonotsplittheprocessofsentencegenerationintotwoseparatestepsofsentenceplanningandrealization,asmostothersystemsdo,butsolvethejointprob-leminasingleintegratedstep.Thiscanpotentiallyallowustogeneratehigher-qualitysentences.WesharetheseadvantageswithsystemssuchasSPUD(Stoneetal.,2003).Crucially,however,ourapproachdescribesthedynamicsofinterpretationexplicitlyanddeclara-tively.Wedonotneedtoassumeextramachin-erybeyondtheencodingofwordsasPDDLplan-ningoperators;forexample,ourplanningopera-torsgiveaself-containeddescriptionofhoweachindividualwordcontributestoresolvingreferences.Thismakesourencodingmoredirectandtranspar-entthanthoseinworklikeThomasonandHobbs(1997)andStoneetal.(2003).Wepresentourencodinginasequenceofsteps,eachofwhichaddsmorelinguisticinformationto337

theplanningoperators.AfterabriefreviewofLTAGandPDDL,weﬁrstfocusonsyntaxaloneandshowhowtocasttheproblemofgeneratinggrammaticallywell-formedLTAGtreesasaplanningprobleminSection2.InSection3,weaddsemanticstotheele-mentarytreesandaddgoalstocommunicatespeciﬁccontent(thiscorrespondstosurfacerealization).Wecompletetheaccountbymodelingreferringexpres-sionsandgothroughanexample.Finally,weassessthepracticalefﬁciencyofourapproachanddiscussfutureworkinSection4.2GrammaticalityasplanningWestartbyreviewingtheLTAGgrammarformal-ismandgivinganintuitionofhowLTAGgen-erationisplanning.WethenaddsemanticrolestotheLTAGelementarytreesinordertodistin-guishdifferentsubstitutionnodes.Finally,were-viewthePDDLplanningspeciﬁcationlanguageandshowhowLTAGgrammaticalitycanbeencodedasaPDDLproblemandhowwecanreconstructanLTAGderivationfromtheplan.2.1Tree-adjoininggrammarsThegrammarformalismweusehereisthatoflex-icalizedtree-adjoininggrammars(LTAG;JoshiandSchabes(1997)).AnLTAGgrammarconsistsofaﬁnitesetoflexicalizedelementarytreesasshowninFig.1a.Eachelementarytreecontainsexactlyoneanchornode,whichislabelledbyaword.Elemen-tarytreescancontainsubstitutionnodes,whicharemarkedbydownarrows(↓).Thoseelementarytreesthatareauxiliarytreesalsocontainexactlyonefootnode,whichismarkedwithanasterisk(∗).Treesthatarenotauxiliarytreesarecalledinitialtrees.Elementarytreescanbecombinedbysubstitutionandadjunctiontoformlargertrees.Substitutionistheoperationofreplacingasubstitutionnodeofsometreebyanotherinitialtreewiththesamerootlabel.Adjunctionistheoperationofsplicinganaux-iliarytreeintosomenodevofatree,insuchawaythattherootoftheauxiliarytreebecomesthechildofv’sparent,andthefootnodebecomestheparentofv’schildren.Ifanodecarriesanulladjunctionconstraint(indicatedbyno-adjoin),noadjunctionisallowedatthisnode;ifitcarriesanobligatoryad-junctionconstraint(indicatedbyadjoin!),anauxil-SNP ↓ VPVlikesNPtheNP * PNMaryNwhiteN * Marylikes rabbitwhite(a)(b)(c)NP ↓ NrabbitNPadjoin!NPSNP VPVlikesNP PNMaryNPtheNwhiteNrabbittheno-adjoinFigure1:Buildingaderived(b)andaderivationtree(c)bycombiningelementarytrees(a).iarytreemustbeadjoinedthere.InFig.1a,wehavecombinedsomeele-mentarytreesbysubstitution(indicatedbythedashed/magentaarrows)andadjunction(dotted/bluearrows).TheresultoftheseoperationsisthederivedtreeinFig.1b.ThederivationtreeinFig.1crep-resentsthetreecombinationoperationsweusedbyhavingonenodeperelementarytreeanddrawingasolidedgeifwecombinedthetwotreesbysubstitu-tion,andadashededgeforadjunctions.2.2ThebasicideaConsidertheprocessofconstructingaderivationtreetop-down.TobuildthetreeinFig.1c,say,westartwiththeemptyderivationtreeandanobligationtogenerateanexpressionofcategoryS.Wesatisfythisobligationbyaddingthetreefor“likes”astherootofthederivation;butindoingso,wehavein-troducednewunﬁlledsubstitutionnodesofcategoryNP,i.e.thederivationtreeisnotcomplete.WeusetheNPtreefor“Mary”toﬁllonesubstitutionnodeandtheNPtreefor“rabbit”toﬁlltheother.Thisﬁllsbothsubstitutionnodes,butthe“rabbit”treein-troducesanobligatoryadjunctionconstraint,whichwemustsatisfybyadjoiningtheauxiliarytreefor“the”.Wenowhaveagrammaticalderivationtree,butwearefreetocontinuebyaddingmoreauxiliarytrees,suchastheonefor“white”.Aswehavejustpresentedit,thegenerationofderivationtreesisessentiallyaplanningproblem.Aplanningprobleminvolvesstatesandactionsthatcanmovefromonestatetoanother.Thetaskistoﬁndasequenceofactionsthatmovesusfromthe338

initialstatetoastatethatsatisﬁesallthegoals.Inourcase,thestatesaredeﬁnedbytheunﬁlledsub-stitutionnodes,theunsatisﬁedobligatoryadjunctionconstraints,andthenodesthatareavailableforad-junctioninsome(possiblyincomplete)derivationtree.Eachactionaddsasingleelementarytreetothederivation,removingsomeofthese“opennodes”whileintroducingnewones.Theinitialstateisasso-ciatedwiththeemptyderivationtreeandarequire-menttogenerateanexpressionforthegivenrootcat-egory.Thegoalisforthecurrentderivationtreetobegrammaticallycomplete.2.3SemanticrolesFormalizingthisintuitionrequiresuniquenamesforeachnodeinthederivedtree.Suchnamesarenec-essarytodistinguishthedifferentopensubstitutionnodesthatstillneedtobeﬁlled,orthedifferentavailableadjunctionsites;intheexample,theplan-nerneededtobeawarethat“likes”introducestwoseparateNPsubstitutionnodestoﬁll.Therearemanywaystoassignthesenames.OnethatworksparticularlywellinthecontextofPDDL(aswewillseebelow)istoassumethateachnodeinanelementarytree,exceptforoneswithnullad-junctionconstraints,ismarkedwithasemanticrole,andthatallsubstitutionnodesaremarkedwithdif-ferentroles.Nothinghingesontheparticularrolein-ventory;hereweassumeaninventoryincludingtherolesagfor“agent”andpatfor“patient”.Wealsoassumeonespecialroleself,whichmustbeusedfortherootofeachelementarytreeandmustneverbeusedforsubstitutionnodes.Wecannowassignauniquenametoeverysub-stitutionnodeinaderivedtreebyassigningarbitrarybutdistinctindicestoeachuseofanelementarytree,andgivingthesubstitutionnodewithrolerintheel-ementarytreewithindexitheidentityi.r.Intheex-ample,let’ssaythe“likes”treehasindex1andthesemanticrolesforthesubstitutionnodeswereagandpat,respectively.TheplanneractionthataddsthistreewouldthenrequiresubstitutionofoneNPwithidentity1.agandanotherNPwithidentity1.pat;the“Mary”treewouldsatisfytheﬁrstrequirementandthe“rabbit”treethesecond.Ifweassumethatnoelementarytreecontainstwointernalnodeswiththesamecategoryandrole,wecanrefertoadjunctionopportunitiesinasimilarway.ActionS-likes-1(u).Precond:subst(S,u),step(1)Effect:¬subst(S,u),subst(NP,1.ag),subst(NP,1.pat),¬step(1),step(2)ActionNP-Mary-2(u).Precond:subst(NP,u),step(2)Effect:¬subst(NP,u),¬step(2),step(3)ActionNP-rabbit-3(u).Precond:subst(NP,u),step(3)Effect:¬subst(NP,u),canadjoin(NP,u),mustadjoin(NP,u),¬step(3),step(4)ActionNP-the-4(u).Precond:canadjoin(NP,u),step(4)Effect:¬mustadjoin(NP,u),¬step(4),step(5)Figure2:SomeactionsforthegrammarinFig.1.2.4EncodinginPDDLNowwearereadytoencodetheproblemofgenerat-inggrammaticalLTAGderivationtreesintoPDDL.PDDL(McDermott,2000)isthestandardinputlan-guageformodernplanningsystems.Itisbasedonthewell-knownSTRIPSlanguage(FikesandNils-son,1971).Inthisparadigm,aplanningstateisdeﬁnedasaﬁnitesetofgroundatomsofpredicatelogicthataretrueinthisstate;allotheratomsareas-sumedtobefalse.Actionshaveanumberofparam-eters,aswellasapreconditionandeffect,bothofwhicharelogicalformulas.Whenaplannertriestoapplyanaction,itwillﬁrstcreateanactioninstancebybindingallparameterstoconstantsfromthedo-main.Itmustthenverifythatthepreconditionoftheactioninstanceissatisﬁedinthecurrentstate.Ifso,theactioncanbeapplied,inwhichcasetheeffectisprocessedinordertochangethestate.InSTRIPS,thepreconditionandeffectbothhadtobeconjunc-tionsofatomsornegatedatoms;positiveeffectsareinterpretedasmakingtheatomtrueinthenewstate,andnegativeonesasmakingitfalse.PDDLper-mitsnumerousextensionstotheformulasthatcanbeusedaspreconditionsandeffects.Eachactioninourplanningproblemencodestheeffectofaddingsomeelementarytreetothederiva-tiontree.AninitialtreewithrootcategoryAtrans-latestoanactionwithaparameterufortheiden-tityofthenodethatthecurrenttreeissubstitutedinto.Theactioncarriesthepreconditionsubst(A,u),andsocanonlybeappliedifuisanopensubsti-tutionnodeinthecurrentderivationwiththecor-rectcategoryA.Auxiliarytreesareanalogous,butcarrythepreconditioncanadjoin(A,u).Theeffectofaninitialtreeistoremovethesubstconditionfromtheplanningstate(torecordthatthesubstitu-339

S-likes-1 (1.self)subst(S,1.self)subst(NP,1.ag)NP-Mary-2   (1.ag)subst(NP,1.pat)NP-rabbit-3  (1.pat)mustadjoin(NP,1.pat)NP-the-4(1.pat)canadjoin(NP,1.pat)subst(NP,1.pat)canadjoin(NP,1.pat)step(1)step(2)step(3)step(4)step(5)Figure3:AplanfortheactionsinFig.2.tionnodeuisnowﬁlled);anauxiliarytreehasaneffect¬mustadjoin(A,u)toindicatethatanyoblig-atoryadjunctionconstraintissatisﬁedbutleavesthecanadjoinconditioninplacetoallowmultiplead-junctionsintothesamenode.Inbothcases,effectsaddsubst,canadjoinandmustadjoinatomsrepre-sentingthesubstitutionnodesandadjunctionsitesthatareintroducedbythenewelementarytree.Oneremainingcomplicationisthatanactionmustassignnewidentitiestothenodesitintroduces;thusitmusthaveaccesstoatreeindexthatwasnotusedinthederivationtreesofar.Weusethenumberofthecurrentplanstepastheindex.Weaddanatomstep(1)totheinitialstateoftheplanningproblem,andweintroducekdifferentcopiesoftheactionsforeachelementarytree,wherekissomeupperlimitontheplansize.Theseactionsareidentical,exceptthatthei-thcopyhasanextrapreconditionstep(i)andeffects¬step(i)andstep(i+1).Itisnorestric-tiontoassumeanupperlimitontheplansize,asmostmodernplannerssearchforplanssmallerthanagivenmaximumlengthanyway.Fig.2showssomeoftheactionsintowhichthegrammarinFig.1translates.Wedisplayonlyonecopyofeachactionandhaveleftoutmostofthecanadjoineffects.Inaddition,weuseaninitialstatecontainingtheatomssubst(S,1.self)andstep(1)andaﬁnalstateconsistingofthefollowinggoal:∀A,u.¬subst(A,u)∧∀A,u.¬mustadjoin(A,u).Wecanthensendtheactionsandtheinitialstateandgoalspeciﬁcationstoanyoff-the-shelfplannerandobtaintheplaninFig.3.Thestraightarrowsinthepicturelinktheactionstotheirpreconditionsand(positive)effects;thecurvedarrowsindicateatomsthatcarryoverfromonestatetothenextwithoutbeingchangedbytheaction.Atomsareprintedinboldfaceifftheycontradictthegoal.Thisplancanbereadasaderivationtreethathasonenodeforeachactioninstanceintheplan,andanedgefromnodeutonodevifuestablishesasubstorcanadjoinfactthatisapreconditionofv.ThesecausallinksaredrawnasboldedgesinFig.3.Themappingisuniqueforsubstitutionedgesbecausesubstatomsareremovedbyeveryactionthathasthemastheirprecondition.Theremaybemultipleactioninstancesintheplanthatintroducethesameatomcanadjoin(A,u).Inthiscase,wecanfreelychooseoneoftheseinstancesastheparent.3SentencegenerationasplanningNowweextendthisencodingtodealwithsemanticsandreferringexpressions.3.1CommunicativegoalsInordertousetheplannerasasurfacerealiza-tionalgorithmforTAGalongthelinesofKollerandStriegnitz(2002),weattachsemanticcontenttoeachelementarytreeandrequirethatthesentenceachievesacertaincommunicativegoal.Wealsouseaknowledgebasethatspeciﬁesthespeaker’sknowl-edge,andrequirethatwecanonlyusetreesthatex-pressinformationinthisknowledgebase.WefollowStoneetal.(2003)informalizingthesemanticcontentofalexicalizedelementarytreetasaﬁnitesetofatoms;butunlikeinearlierapproaches,weusethesemanticrolesintastheargumentsoftheseatoms.Forinstance,thesemanticcontentofthe“likes”treeinFig.1is{like(self,ag,pat)}(seealsothesemconentriesinFig.4).Theknowledgebaseissomeﬁnitesetofgroundatoms;intheexam-ple,itcouldcontainsuchentriesaslike(e,m,r)andrabbit(r).Finally,thecommunicativegoalissomesubsetoftheknowledgebase,suchas{like(e,m,r)}.Weimplementunsatisﬁedcommunicativegoalsasﬂawsthattheplanmustremedy.Tothisend,weaddanatomcg(P,a1,...,an)foreachelementP(a1,...,an)ofthecommunicativegoaltotheini-tialstate,andweaddacorrespondingconjunct∀P,x1,...,xn.¬cg(P,x1,...,xn)tothegoal.Inad-dition,weaddanatomskb(P,a1,...,an)totheinitialstateforeachelementP(a1,...,an)ofthe(speaker’s)knowledgebase.340

Wethenaddparametersx1,...,xntoeachactionwithnsemanticroles(includingself).Thesenewparametersareintendedtobeboundtoindividualconstantsintheknowledgebasebytheplanner.Foreachelementarytreetandpossiblestepindexi,weestablishtherelationshipbetweentheseparametersandtherolesintwosteps.Firstweﬁxafunctionidthatmapsthesemanticrolesofttonodeidentities.Itmapsselftouandeachotherrolertoi.r.Second,weﬁxafunctionrefthatmapstheoutputsofidbi-jectivelytotheparametersx1,...,xn,insuchawaythatref(u)=x1.Wecanthencapturethecontributionofthei-thactionforttothecommunicativegoalbygivingitaneffect¬cg(P,ref(id(r1)),...,ref(id(rn)))foreachelementP(r1,...,rn)oftheelementarytree’sseman-ticcontent.Werestrictourselvestoonlyexpressingtruestatementsbygivingtheactionapreconditionskb(P,ref(id(r1)),...,ref(id(rn)))foreachelementofthesemanticcontent.Inordertokeeptrackoftheconnectionbetweennodeidentitiesandindividualsforfuturereference,eachactiongetsaneffectreferent(id(r),ref(id(r)))foreachsemanticrolerexceptself.Weenforcetheconnectionbetweenuandx1byaddingaprecondi-tionreferent(u,x1).Intheexample,themostinterestingactioninthisrespectistheonefortheelementarytreefor“likes”.Thisactionlooksasfollows:ActionS-likes-1(u,x1,x2,x3).Precond:subst(S,u),step(1),referent(u,x1),skb(like,x1,x2,x3)Effect:¬subst(S,u),subst(NP,1.ag),subst(NP,1.pat),¬step(1),step(2),referent(1.ag,x2),referent(1.pat,x3),¬cg(like,x1,x2,x3)Wecanrunaplannerandinterprettheplanasabove;themaindifferenceisthatcompleteplansnotonlycorrespondtogrammaticalderivationtrees,butalsoexpressallcommunicativegoals.Noticethatthisencodingmodelssomeaspectsoflexicalchoice:Thesemanticcontentsetsoftheelementarytreesneednotbesingletons,andsotheremaybemultiplewaysofpartitioningthecommunicativegoalintothecontentsetsofvariouselementarytrees.3.2ReferringexpressionsFinally,weextendthesystemtodealwiththegen-erationofreferringexpressions.Whilethisprob-lemistypicallytakentorequirethegenerationofanounphrasethatrefersuniquelytosomeindividual,wedon’tneedtomakeanyassumptionsaboutthesyntacticcategoryhere.Moreover,weconsidertheprobleminthewidercontextofgeneratingreferringexpressionswithinasentence,whichcanallowustogeneratemoresuccinctexpressions.Becauseareferringexpressionmustallowthehearertoidentifytheintendedreferentuniquely,wekeeptrackofthehearer’sknowledgebasesep-arately.Weuseatomshkb(P,a1,...,an),aswithskbabove.Inaddition,weassumepragmaticinformationoftheformpkb(P,a1,...,an).Thethreepragmaticpredicatesthatwewilluseherearehearer-new,indicatingthatthehearerdoesnotknowabouttheexistenceofanindividualandcan’tinferit(Stoneetal.,2003),hearer-oldfortheopposite,andcontextset.Thecontextsetofanintendedreferentisthesetofallindividualsthatthehearermightpossi-blyconfuseitwith(DeVaultetal.,2004).Itisemptyforhearer-newindividuals.Tosaythatbisina’scontextset,weputtheatompkb(contextset,a,b)intotheinitialstate.Inadditiontothesemanticcontent,weequipev-eryelementarytreeinthegrammarwithaseman-ticrequirementandapragmaticcondition(Stoneetal.,2003).Thesemanticrequirementisasetofatomsspellingoutpresuppositionsofanelementarytreethatcanhelptheheareridentifywhatitsargu-mentsreferto.Forinstance,“likes”hastheselec-tionalrestrictionthatitsagentmustbeanimate;thusthehearerwillnotconsiderinanimateindividualsasdistractorsforthereferringexpressioninagentposi-tion.Thepragmaticconditionisasetofatomsoverthepredicatesinthepragmaticknowledgebase.Inoursetting,everysubstitutionnodethatisin-troducedduringthederivationintroducesanewre-ferringexpression.Thismeansthatwecandis-tinguishthereferringexpressionsbytheidentityofthesubstitutionnodethatintroducedthem.Foreachreferringexpressionu(whereuisanodeiden-tity),wekeeptrackofthedistractorsinatomsoftheformdistractor(u,x).Thepresenceofanatomdistractor(u,a)insomeplanningstaterepre-sentsthefactthatthecurrentderivationtreeisnotyetinformativeenoughtoallowthehearertoiden-tifytheintendedreferentforuuniquely;aisan-otherindividualthatisnottheintendedreferent,341

butconsistentwiththepartialreferringexpressionwehaveconstructedsofar.Weenforceuniquenessofallreferringexpressionsbyaddingtheconjunct∀u,x¬distractor(u,x)totheplanninggoal.Nowwheneveranactionintroducesanewsubsti-tutionnodeu,itwillalsointroducesomedistractoratomstorecordtheinitialdistractorsfortherefer-ringexpressionatu.Anindividualaisintheinitialdistractorsetforthesubstitutionnodewithrolerif(a)itisnottheintendedreferent,(b)itisinthecontextsetoftheintendedreferent,and(c)thereisachoiceofindividualsfortheotherparametersoftheactionthatsatisﬁesthesemanticrequirementtogetherwitha.Thisisexpressedbyaddingthefollowingeffectforeachsubstitutionnode;thecon-junctionisovertheelementsP(r1,...,rn)ofthese-manticrequirement,andthereisoneuniversalquan-tiﬁerforyandforeachparameterxjoftheactionexceptforref(id(r)).∀y,x1,...,xn(y6=ref(id(r))∧pkb(contextset,ref(id(r)),y)∧Vhkb(P,ref(id(r1)),...,ref(id(rn)))[y/ref(id(r))])→distractor(id(r),y)Ontheotherhand,adistractoraforareferringex-pressionintroducedatuisremovedwhenwesubsti-tuteoradjoinanelementarytreeintouwhichrulesaout.Forinstance,theelementarytreefor“rabbit”willremoveallnon-rabbitsfromthedistractorsetofthesubstitutionnodeintowhichitissubstituted.Weachievethisbyaddingthefollowingeffecttoeachaction;heretheconjunctionisoverallelementsofthesemanticcontent.∀y.(¬Vhkb(P,ref(id(r1)),...,ref(id(rn))))[y/x1]→¬distractor(u,y),Finally,eachactiongetsitspragmaticconditionasaprecondition.3.3TheexampleBywayofexample,Fig.5showsthefullversionsoftheactionsfromFig.2,fortheextendedgram-marinFig.4.Let’ssaythatthehearerknowsabouttworabbitsr(whichiswhite)andr0(whichisnot),aboutapersonmwiththenameMary,andaboutanevente,andthatthecontextsetofris{r,r0,m,e}.Let’salsosaythatourcommunicativegoalis{like(e,m,r)}.Inthiscase,theﬁrstactioninstanceinFig.3,S-likes-1(1.self,e,m,r),intro-ducesasubstitutionnodewithidentity1.pat.TheS:selfNP:ag ↓ VP:selfV:selflikesNP:self theNP:self *NP:selfaNP:self *NP:selfPN:selfMaryN:selfrabbitN:selfwhiteN:self * semcon: {like(self,ag,pat)}semreq: {animate(ag)}semcon: { }semreq: { }pragcon: {hearer-old(self)}semcon: { }semreq: { }pragcon: {hearer-new(self)}semcon: {white(self)}semcon: {name(self, mary)}semcon: {rabbit(self)}NP:pat ↓ adjoin!NP:selfFigure4:Theextendedexamplegrammar.initialdistractorsetofthisnodeis{r0,m}–thesetofallindividualsinr’scontextsetexceptforinan-imateobjects(whichviolatethesemanticrequire-ment)andritself.TheNP-rabbit-3actionremovesmfromthedistractorset,butattheendoftheplaninFig.3,r0isstilladistractor,i.e.wehavenotreachedagoalstate.Wecancompletetheplanbyperform-ingaﬁnalactionNP-white-5(1.pat,r),whichwillremovethisdistractorandachievetheplanninggoal.WecanstillreconstructaderivationtreefromthecompleteplanliterallyasdescribedinSection2.Nowlet’ssaythatthehearerdidnotknowabouttheexistenceoftheindividualrbeforetheutterancewearegenerating.Wemodelthisbymarkingrashearer-newinthepragmaticknowledgebaseandas-signingitanemptycontextset.Inthiscase,there-ferringexpression1.patwouldbeinitializedwithanemptydistractorset.ThisentitlesustousetheactionNP-a-4andgeneratethefour-stepplancorrespond-ingtothesentence“Marylikesarabbit.”4DiscussionandfutureworkInconclusion,let’slookinmoredetailatcomputa-tionalissuesandtheroleofmutuallyconstrainingreferringexpressions.342

ActionS-likes-1(u,x1,x2,x3).Precond:referent(u,x1),skb(like,x1,x2,x3),subst(S,u),step(1)Effect:¬cg(like,x1,x2,x3),¬subst(S,u),¬step(1),step(2),subst(NP,1.ag),subst(NP,1.pat),∀y.¬hkb(like,y,x2,x3)→¬distractor(u,y),∀y,x1,x3.x26=y∧pkb(contextset,x2,y)∧animate(y)→distractor(1.ag,y),∀y,x1,x2.x36=y∧pkb(contextset,x3,y)→distractor(1.pat,y)ActionNP-Mary-2(u,x1).Precond:referent(u,x1),skb(name,x1,mary),subst(NP,u),step(2)Effect:¬cg(name,x1,mary),¬subst(NP,u),¬step(2),step(3),∀y.¬hkb(name,y,mary)→¬distractor(u,y)ActionNP-rabbit-3(u,x1).Precond:referent(u,x1),skb(rabbit,x1),subst(N,u),step(3)Effect:¬cg(rabbit,x1),¬subst(N,u),¬step(3),step(4),canadjoin(NP,u),mustadjoin(NP,u),∀y.¬hkb(rabbit,y)→¬distractor(u,y)ActionNP-the-4(u,x1).Precond:referent(u,x1),canadjoin(NP,u),step(4),pkb(hearer-old,x1)Effect:¬mustadjoin(NP,u),¬step(4),step(5)ActionNP-a-4(u,x1).Precond:referent(u,x1),canadjoin(NP,u),step(4),pkb(hearer-new,x1)Effect:¬mustadjoin(NP,u),¬step(4),step(5)ActionNP-white-5(u,x1).Precond:referent(u,x1),skb(white,x1),canadjoin(NP,u),step(5)Effect:¬cg(white,x1),¬mustadjoin(NP,u),¬step(5),step(6),∀y.¬hkb(white,y)→¬distractor(u,y)Figure5:SomeoftheactionscorrespondingtothegrammarinFig.4.4.1ComputationalissuesWelackthespacetopresenttheformaldeﬁnitionofthesentencegenerationproblemweencodeintoPDDL.However,thisproblemisNP-complete,byreductionofHamiltonianCycle–unsurprisingly,giventhatitencompassesrealization,andtheverysimilarrealizationprobleminKollerandStriegnitz(2002)isNP-hard.Soanyalgorithmforourprob-lemmustbepreparedforexponentialruntimes.Wehaveimplementedthetranslationdescribedinthispaperandexperimentedwithanumberofdiffer-entgrammars,knowledgebases,andplanners.TheFFplanner(HoffmannandNebel,2001)cancom-putetheplansinSection3.3inunder100msus-ingthegrammarinFig.4.Ifweadd10morelex-iconentriestothegrammar,theruntimegrowsto190ms;andfor20moreentries,to360ms.Theruntimealsogrowswiththeplanlength:Ittakes410mstogenerateasentence“MarylikestheAdj...Adjrabbit”withfouradjectivesand890msforsixadjectives,correspondingtoaplanlengthof10.Wecomparedtheseresultsagainstaplanning-basedreimplementationofSPUD’sgreedysearchheuris-tic(Stoneetal.,2003).ThissystemisfasterthanFFforsmallinputs(360msforfouradjectives),butbe-comesslowerasinputsgrowlarger(1000msforsixadjectives);butnoticethatwhileFFisalsoaheuris-ticplanner,itisguaranteedtoﬁndasolutionifoneexists,unlikeSPUD.Plannershavemadetremendousprogressinefﬁ-ciencyinthepastdecade,andbyencodingsentencegenerationasaplanningproblem,wearesettoproﬁtfromanyfutureimprovements;itisanadvantageoftheplanningapproachthatwecancompareverydifferentsearchstrategieslikeFF’sandSPUD’sinthesameframework.However,ourPDDLproblemsarechallengingformodernplannersbecausemostplannersstartbycomputingallinstancesofatomsandactions.Inourexperiments,FFgenerallyspentonlyabout10%oftheruntimeonsearchandtherestoncomputingtheinstances;thatis,thereisalotofroomforoptimization.Forlargergrammarsandknowledgebases,thenumberofinstancescaneasilygrowintothebillions.Infuturework,wewillthere-forecollaboratewithexpertsonplanningsystemstocomputeactioninstancesonlybyneed.4.2ReferringexpressionsInouranalysisofreferringexpressions,thetreetthatintroducesthenewsubstitutionnodestypicallyinitializesthedistractorsetswithpropersubsetsoftheentiredomain.Thisallowsustogeneratesuc-cinctdescriptionsbyencodingt’spresuppositionsassemanticrequirements,andlocalizestheinter-actionsbetweenthereferringexpressionsgeneratedfordifferentsubstitutionnodeswithint’saction.343

However,animportantdetailintheencodingofreferringexpressionsaboveisthatanindividualacountsasadistractorfortherolerifthereisanytupleofvaluesthatsatisﬁesthesemanticrequire-mentandhasainther-component.Thisiscorrect,butcansometimesleadtooverlycomplicatedrefer-ringexpressions.Anexampleistheconstruction“XtakesYfromZ”,whichpresupposesthatYisinZ.Inascenariothatinvolvesmultiplerabbits,multiplehats,andmultipleindividualsthatareinsideotherindividuals,butonlyonepairofarabbitrinsideahath,theexpression“Xtakestherabbitfromthehat”issufﬁcienttoreferuniquelytorandh(StoneandWebber,1998).Oursystemwouldtrytogen-erateanexpressionforYthatsufﬁcesbyitselftodistinguishrfromalldistractors,andsimilarlyforZ.Wewillexplorethisissuefurtherinfuturework.5ConclusionInthispaper,wehaveshownhowsentencegener-ationwithTAGgrammarsandsemanticandprag-maticinformationcanbeencodedintoPDDL.Ourencodingisdeclarativeinthatitcanbeusedwithanycorrectplanningalgorithm,andexplicitinthattheactionscapturethecompleteeffectofawordonthesyntactic,semantic,andlocalpragmaticgoals.Intermsofexpressivepower,itcapturesthecoreofSPUD,exceptforitsinferencecapabilities.Thisworkispracticallyrelevantbecauseitopensupthepossibilityofusingefﬁcientplannerstomakegeneratorsfasterandmoreﬂexible.Conversely,ourPDDLproblemsareachallengeforcurrentplan-nersandopenupNLGasanapplicationdomainthatplanningresearchitselfcantarget.Theoretically,ourencodingprovidesanewframeworkforunderstandingandexploringthegen-eralrelationshipsbetweenlanguageandaction.ItsuggestsnewwaysofgoingbeyondSPUD’sexpres-sivepower,toformulateutterancesthatdescribeanddisambiguateconcurrentreal-worldactionsorex-ploitthedynamicsoflinguisticcontextwithinandacrosssentences.Acknowledgments.ThisworkwasfundedbyaDFGre-searchfellowshipandtheNSFgrantsHLC0308121,IGERT0549115,andHSD0624191.WeareindebtedtoHenryKautzforhisadviceonplanningsystems,andtoOwenRambow,Bon-nieWebber,andtheanonymousreviewersforfeedback.ReferencesD.Appelt.1985.PlanningEnglishSentences.Cam-bridgeUniversityPress,CambridgeEngland.A.BlumandM.Furst.1997.Fastplanningthroughgraphanalysis.ArtiﬁcialIntelligence,90:281–300.P.R.CohenandC.R.Perrault.1979.Elementsofaplan-basedtheoryofspeechacts.CognitiveScience,3(3):177–212.R.DaleandE.Reiter.1995.Computationalinterpreta-tionsoftheGriceanmaximsinthegenerationofrefer-ringexpressions.CognitiveScience,19.D.DeVault,C.Rich,andC.Sidner.2004.Naturallan-guagegenerationanddiscoursecontext:Computingdistractorsetsfromthefocusstack.InProc.FLAIRS.R.FikesandN.Nilsson.1971.STRIPS:Anewapproachintheapplicationoftheoremprovingtoproblemsolv-ing.ArtiﬁcialIntelligence,2:189–208.P.HeemanandG.Hirst.1995.Collaboratingonreferringexpressions.ComputationalLinguistics,21(3):351–382.J.Hobbs,M.Stickel,D.Appelt,andP.Martin.1993.Interpretationasabduction.ArtiﬁcialIntelligence,63:69–142.J.HoffmannandB.Nebel.2001.TheFFplanningsystem:Fastplangenerationthroughheuristicsearch.JournalofArtiﬁcialIntelligenceResearch,14.A.JoshiandY.Schabes.1997.Tree-AdjoiningGram-mars.InG.RozenbergandA.Salomaa,editors,HandbookofFormalLanguages,chapter2,pages69–123.Springer-Verlag,Berlin.H.KautzandB.Selman.1998.Blackbox:Anewap-proachtotheapplicationoftheoremprovingtoprob-lemsolving.InWorkshopPlanningasCombinatorialSearch,AIPS-98.A.KollerandK.Striegnitz.2002.Generationasdepen-dencyparsing.InProc.40thACL,Philadelphia.D.V.McDermott.2000.The1998AIPlanningSystemsCompetition.AIMagazine,21(2):35–55.M.StoneandB.Webber.1998.Textualeconomythroughclosecouplingofsyntaxandsemantics.InProc.INLG.M.Stone,C.Doran,B.Webber,T.Bleam,andM.Palmer.2003.Microplanningwithcommunicativeinten-tions:TheSPUDsystem.ComputationalIntelligence,19(4):311–381.R.ThomasonandJ.Hobbs.1997.Interrelatinginterpre-tationandgenerationinanabductiveframework.InAAAIFallSymposiumonCommunicativeAction.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 344–351,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

344

GLEU:AutomaticEvaluationofSentence-LevelFluencyAndrewMutton∗MarkDras∗StephenWan∗,†RobertDale∗∗CentreforLanguageTechnology†InformationandCommunicationTechnologiesMacquarieUniversityCSIRONSW2109AustraliaNSW2109Australiamadras@ics.mq.edu.auAbstractInevaluatingtheoutputoflanguagetech-nologyapplications—MT,naturallanguagegeneration,summarisation—automaticeval-uationtechniquesgenerallyconﬂatemea-surementoffaithfulnesstosourcecontentwithﬂuencyoftheresultingtext.Inthispaperwedevelopanautomaticevaluationmetrictoestimateﬂuencyalone,byexamin-ingtheuseofparseroutputsasmetrics,andshowthattheycorrelatewithhumanjudge-mentsofgeneratedtextﬂuency.Wethende-velopamachinelearnerbasedonthese,andshowthatthisperformsbetterthantheindi-vidualparsermetrics,approachingalowerboundonhumanperformance.Weﬁnallylookatdifferentlanguagemodelsforgener-atingsentences,andshowthatwhileindivid-ualparsermetricscanbe‘fooled’dependingongenerationmethod,themachinelearnerprovidesaconsistentestimatorofﬂuency.1IntroductionIntrinsicevaluationoftheoutputofmanylanguagetechnologiescanbecharacterisedashavingatleasttwoaspects:howwellthegeneratedtextreﬂectsthesourcedata,whetheritbetextinanotherlan-guageformachinetranslation(MT),anaturallan-guagegeneration(NLG)inputrepresentation,adoc-umenttobesummarised,andsoon;andhowwellitconformstonormalhumanlanguageusage.Thesetwoaspectsareoftenmadeexplicitinapproachestocreatingthetext.Forexample,instatisticalMTthetranslationmodelandthelanguagemodelaretreatedseparately,characterisedasfaithfulnessandﬂuencyrespectively(asinthetreatmentinJurafskyandMartin(2000)).Similarly,theultrasummarisa-tionmodelofWitbrockandMittal(1999)consistsofacontentmodel,modellingtheprobabilitythatawordinthesourcetextwillbeinthesummary,andalanguagemodel.Evaluationmethodscanbesaidtofallintotwocate-gories:acomparisontogoldreference,oranappealtohumanjudgements.Automaticevaluationmeth-odscarryingoutacomparisontogoldreferencetendtoconﬂatethetwoaspectsoffaithfulnessandﬂu-encyingivingagoodnessscoreforgeneratedout-put.BLEU(Papinenietal.,2002)isacanonicalex-ample:inmatchingn-gramsinacandidatetransla-tiontextwiththoseinareferencetext,themetricmeasuresfaithfulnessbycountingthematches,andﬂuencybyimplicitlyusingthereferencen-gramsasalanguagemodel.Oftenweareinterestedinknow-ingthequalityofthetwoaspectsseparately;manyhumanjudgementframeworksaskspeciﬁcallyforseparatejudgementsonelementsofthetaskthatcor-respondtofaithfulnessandtoﬂuency.Inaddition,theneedforreferencetextsforanevaluationmetriccanbeproblematic,andintuitivelyseemsunneces-saryforcharacterisinganaspectoftextqualitythatisnotrelatedtoitscontentsourcebuttotheuseoflanguageitself.Itisagoalofthispapertoprovideanautomaticevaluationmethodforﬂuencyalone,withouttheuseofareferencetext.Onemightconsiderusingametricbasedonlan-guagemodelprobabilitiesforsentences:ineval-345

uatingalanguagemodelon(alreadyexisting)testdata,ahigherprobabilityforasentence(andlowerperplexityoverawholetestcorpus)indicatesbet-terlanguagemodelling;perhapsahigherprobabilitymightindicateabettersentence.However,herewearelookingatgeneratedsentences,whichhavebeengeneratedusingtheirownlanguagemodel,ratherthanhuman-authoredsentencesalreadyexistinginatestcorpus;andsoitisnotobviouswhatlanguagemodelwouldbeanobjectiveassessmentofsentencenaturalness.Inthecaseofevaluatingasinglesys-tem,usingthelanguagemodelthatgeneratedthesentencewillonlyconﬁrmthatthesentencedoesﬁtthelanguagemodel;insituationssuchascom-paringtwosystemswhicheachgeneratetextusingadifferentlanguagemodel,itisnotobviousthatthereisaprincipledwayofdecidingonafairlan-guagemodel.QuiteadifferentideawassuggestedinWanetal.(2005),ofusingthegrammaticaljudge-mentofaparsertoassessﬂuency,givingameasureindependentofthelanguagemodelusedtogener-atethetext.Theideaisthat,assumingtheparserhasbeentrainedonanappropriatecorpus,thepoorperformanceoftheparserononesentencerelativetoanothermightbeanindicatorofsomedegreeofungrammaticalityandpossiblydisﬂuency.Inthatwork,however,correlationwithhumanjudgementswasleftuninvestigated.Thegoalofthispaperistotakethisideaandde-velopit.InSection2welookatsomerelatedworkonmetrics,inparticularforNLG.InSection3,weverifywhetherparseroutputscanbeusedasesti-matorsofgeneratedsentenceﬂuencybycorrelatingthemwithhumanjudgements.InSection4,wepro-poseanSVM-basedmetricusingparseroutputsasfeatures,andcompareitscorrelationagainsthumanjudgementswiththatoftheindividualparsers.InSection5,weinvestigatetheeffectsonthevariousmetricsfromdifferenttypesoflanguagemodelforthegeneratedtext.TheninSection6weconclude.2RelatedWorkIntermsofhumanevaluation,thereisnouniformviewonwhatconstitutesthenotionofﬂuency,oritsrelationshiptogrammaticalityorsimilarconcepts.Wementionafewexamplesheretoillustratetherangeofusage.InMT,the2005NISTMTEvalu-ationPlanusesguidelines1forjudgestoassess‘ad-equacy’and‘ﬂuency’on5pointscales,wheretheyareaskedtoprovideintuitivereactionsratherthanponderingtheirdecisions;forﬂuency,thescalede-scriptionsarefairlyvague(5:ﬂawlessEnglish;4:goodEnglish;3:non-nativeEnglish;2:disﬂuentEnglish;1:incomprehensible)andinstructionsareshort,withsomeexamplesprovidedinappendices.Zajicetal.(2002)usesimilarscalesforsummari-sation.Bycontrast,PanandShaw(2004),fortheirNLGsystemSEGUEtiedthenotionofﬂuencymoretightlytogrammaticality,givingtwohumanevalu-atorsthreegradeoptions:good,minorgrammaticalerror,majorgrammatical/pragmaticerror.Asafur-thercontrast,theanalysisofCoch(1996)wasverycomprehensiveandﬁne-grained,inacomparisonofthreetext-productiontechniques:heused14humanjudges,eachjudging60letters(20pergenerationsystem),andrequiredthemtoassessthelettersforcorrectspelling,goodgrammar,rhythmandﬂow,appropriatenessoftone,andseveralotherspeciﬁccharacteristicsofgoodtext.Intermsofautomaticevaluation,wearenotawareofanytechniquethatmeasuresonlyﬂuencyorsim-ilarcharacteristics,ignoringcontent,apartfromthatofWanetal.(2005).EveninNLG,where,giventhevariabilityoftheinputrepresentations(andhencedifﬁcultyinverifyingfaithfulness),itmightbeex-pectedthatsuchmeasureswouldbeavailable,theavailablemetricsstillconﬂatecontentandform.Forexample,themetricsproposedinBangaloreetal.(2000),suchasSimpleAccuracyandGenerationAccuracy,measurechangeswithrespecttoarefer-encestringbasedontheideaofstring-editdistance.Similarly,BLEUhasbeenusedinNLG,forexamplebyLangkilde-Geary(2002).3ParsersasEvaluatorsTherearethreepartstoverifyingtheusefulnessofparsersasevaluators:choosingtheparsersandthemetricsderivedfromthem;generatingsometextsforhumanandparserevaluation;and,thekeypart,gettinghumanjudgementsonthesetextsandcorre-latingthemwithparsermetrics.1http://projects.ldc.upenn.edu/TIDES/Translation/TranAssessSpec.pdf346

3.1TheParsersIntestingtheideaofusingparserstojudgeﬂuency,weusethreeparsers,fromwhichwederivefourparsermetrics,toinvestigatethegeneralapplicabil-ityoftheidea.ThosechosenweretheConnexorparser,2theCollinsparser(Collins,1999),andtheLinkGrammarparser(Grinbergetal.,1995).Eachproducesoutputthatcanbetakenasrepresentingdegreeofungrammaticality,althoughthisoutputisquitedifferentforeach.Connexorisacommerciallyavailabledependencyparserthatreturnshead–dependantrelationsaswellasstemminginformation,partofspeech,andsoon.Inthecaseofanungrammaticalsentence,Connexorreturnstreefragments,wherethesefragmentsaredeﬁnedbytransitivehead–dependantrelations:forexample,forthesentenceEverybodylikesbigcakesdoitreturnsfragmentsforEverybodylikesbigcakesandfordo.Weexpectthatthenumberoffragmentsshouldcorrelateinverselywiththequalityofasen-tence.Forametric,wenormalisethisnumberbythelargestnumberoffragmentsforagivendataset.(NormalisationmattersmostforthemachinelearnerinSection4.)TheCollinsparserisastatisticalchartparserthataimstomaximisetheprobabilityofaparseusingdy-namicprogramming.Theparsetreeproducedisan-notatedwithlogprobabilities,includingoneforthewholetree.Inthecaseofungrammaticalsentences,theparserwillassignalowprobabilitytoanyparse,includingthemostlikelyone.Weexpectthatthelogprobability(becomingmorenegativeasthesen-tenceislesslikely)shouldcorrelatepositivelywiththequalityofasentence.Forametric,wenormalisethisbythemostnegativevalueforagivendataset.LikeConnexor,theLinkGrammarparserreturnsin-formationaboutwordrelationships,forminglinks,withtheprovisothatlinkscannotcrossandthatinagrammaticalsentencealllinksareindirectlycon-nected.Foranungrammaticalsentence,theparserwilldeletewordsuntilitcanproduceaparse;thenumberitdeletesiscalledthe‘nullcount’.Weex-pectthatthisshouldcorrelateinverselywithsen-tencequality.Forametric,wenormalisethisbythesentencelength.Inaddition,theparserproduces2http://www.connexor.comanothervariablepossiblyofinterest.Ingeneratingaparse,theparserproducesmanycandidatesandrulessomeoutbyaposterioriconstraintsonvalidparses.Initsoutputtheparserreturnsthenumberofinvalidparses.Foranungrammaticalsentence,thisnumbermaybehigher;however,theremayalsobemoreparses.Forametric,wenormalisethisbythetotalnumberofparsesfoundforthesentence.Thereisnostrongintuitionaboutthedirectionofcorrela-tionhere,butweinvestigateitinanycase.3.2TextGenerationMethodTotestwhethertheseparsersareabletodiscriminatesentence-lengthtextsofvaryingdegreesofﬂuency,weneedﬁrsttogeneratetextsthatweexpectwillbediscriminableinﬂuencyqualityrangingfromgoodtoverypoor.Belowwedescribeourmethodforgen-eratingtext,andthenourpreliminarycheckonthediscriminabilityofthedatabeforegivingthemtohu-manjudges.Ourapproachtogenerating‘sentences’ofaﬁxedlengthistotakewordsequencesofdifferentlengthsfromacorpusandgluethemtogetherprobabilisti-cally:theintuitionisthatafewlongersequencesgluedtogetherwillbemoreﬂuentthanmanyshortersequences.Moreprecisely,togenerateasentenceoflengthn,wetakesequencesoflengthl(suchthatldividesn),withsequenceioftheformwi,1...wi,l,wherewi,isawordorpunctuationmark.Westartbyselectingsequence1,ﬁrstbyrandomlychoos-ingitsﬁrstwordaccordingtotheunigramprobabil-ityP(w1,1),andthenthesequenceuniformlyran-domlyoverallsequencesoflengthlstartingwithw1,1;weselectsubsequentsequencesj(2≤j≤n/l)randomlyaccordingtothebigramprobabilityP(wj,1|wj−1,l).TakingasourcorpustheReuterscorpus,3forlengthn=24,wegeneratesentencesforsequencesizesl=1,2,4,8,24asinFigure1.So,forinstance,thesequence-size8examplewasconstructedbystringingtogetherthethreeconsecu-tivesequencesoflength8(There...to;be...have;to....)takenfromthecorpus.Theseexamples,andothersgenerated,appeartobeofvariablequalityinaccordancewithourintu-ition.However,toconﬁrmthispriortotestingthem3http://trec.nist.gov/data/reuters/reuters.html347

Extracted(Sequence-size24)GinebrafaceFormulaShellinasudden-deathplayoffonSun-daytodecidewhowillfaceAlaskainabest-of-sevenseriesforthetitle.Sequence-size8Thereissomethinkinginthegovernmenttobenearlyasdra-maticassomepeoplehavetobeslaughteredtoeradicatetheepidemic.Sequence-size4MostofBanharn’smovecomesafteritcanstillbeavertedthecrashifitshouldagainbecomeapolicestatementsaid.Sequence-size2Masseysaidinlinewithlosses,Nordbankeniswell-placedtobeneﬁtabusewasloadedwithCzechprimeministerAndrisShkele,said.Sequence-size1Thewarwe’rehereinaspokesmanJeffSluman86percentjumpthatSpaintowhatwasbooked,expressalsosaid.Figure1:SamplesentencesfromtheﬁrsttrialDescriptionCorrelationSmall0.10to0.29Medium0.30to0.49Large0.50to1.00Table1:Correlationcoefﬁcientinterpretationoutfordiscriminabilityinahumantrial,wewantedseewhethertheyarediscriminablebysomemethodotherthanourownjudgement.WeusedtheparsersdescribedinSection3.1,inthehopeofﬁndinganon-zerocorrelationbetweentheparseroutputsandthesequencelengths.Regardingtheinterpretationoftheabsolutevalueof(Pearson’s)correlationcoefﬁcients,bothhereandintherestofthepaper,weadoptCohen’sscale(Co-hen,1988)foruseinhumanjudgements,giveninTable1;weusethisasmostofthisworkistodowithhumanjudgementsofﬂuency.Fordata,wegener-ated1000sentencesoflength24foreachsequencelengthl=1,2,3,4,6,8,24,giving7000sentencesintotal.Thecorrelationswiththefourparserout-putsareasinTable2,withthemediumcorrelationsforCollinsandLinkGrammar(nulledtokens)indi-catingthatthesentencesareindeeddiscriminabletosomeextent,andhencetheapproachislikelytobeusefulforgeneratingsentencesforhumantrials.3.3HumanJudgementsThenextstepisthentoobtainasetofhumanjudge-mentsforthisdata.Humanjudgescanonlybeex-pectedtojudgeareasonablysizedamountofdata,MetricCorr.CollinsParser0.3101Connexor-0.2332LinkGrammarNulledTokens-0.3204LinkGrammarInvalidParses0.1776GLEU0.4144Table2:Parservssequencesizefororiginaldatasetsoweﬁrstreducedthesetofsequencesizestobejudged.Todothiswedeterminedforthe7000generatedsentencesthescoresaccordingtothe(ar-bitrarilychosen)Collinsparser,andcalculatedthemeansforeachsequencesizeandthe95%conﬁ-denceintervalsaroundthesemeans.Wethenchoseasubsetofsequencesizessuchthattheconﬁdenceintervalsdidnotoverlap:1,2,4,8,24;theideawasthatthiswouldbelikelytogivemaximallydiscrim-inablesentences.Foreachofthesesequencessizes,wechoserandomly10sentencesfromtheinitialset,givingasetforhumanjudgementofsize50.Thejudgesconsistedoftwentyvolunteers,allnativeEnglishspeakerswithoutexplicitlinguistictraining.Wegavethemgeneralguidelinesaboutwhatconsti-tutedﬂuency,mentioningthattheyshouldconsidergrammaticalitybutdeliberatelynotgivingdetailedinstructionsonthemannerfordoingthis,aswewereinterestedinthelevelofagreementofintuitiveun-derstandingofﬂuency.Weinstructedthemalsothattheyshouldevaluatethesentencewithoutconsider-ingitscontent,usingColourlessgreenideassleepfuriouslyasanexampleofanonsensicalbutper-fectlyﬂuentsentence.Thejudgeswerethenpre-sentedwiththe50sentencesinrandomorder,andaskedtoscorethesentencesaccordingtotheirownscale,asinmagnitudeestimation(Bardetal.,1996);thesescoreswerethennormalisedintherange[0,1].Somejudgesnotedthatthetaskwasdifﬁcultbe-causeofitssubjectivity.Notwithstandingthissub-jectivityandvariationintheirapproachtothetask,thepairwisecorrelationsbetweenjudgeswerehigh,asindicatedbythemaximum,minimumandmeanvaluesinTable3,indicatingthatourassumptionthathumanshadanintuitivenotionofﬂuencyandneededonlyminimalinstructionwasjustiﬁed.Lookingatmeanscoresforeachsequencesize,judgesgenerallyalsorankedsentencesbysequencesize;seeFigure2.Comparinghumanjudgement348

StatisticCorr.Maximumcorrelation0.8749Minimumcorrelation0.4710Meancorrelation0.7040Standarddeviation0.0813Table3:DataoncorrelationbetweenhumansFigure2:Meanscoresforhumanjudgescorrelationsagainstsequencesizewiththesamecor-relationsfortheparsermetrics(asforTable2,butonthehumantrialdata)givesTable4,indicatingthathumanscanalsodiscriminatethedifferentgeneratedsentencetypes,infact(notsurprisingly)betterthantheautomaticmetrics.Now,havingbothhumanjudgementscoresofsomereliabilityforsentences,andscoringmetricsfromthreeparsers,wegivecorrelationsinTable5.GivenCohen’sinterpretation,theCollinsandLinkGram-mar(nulledtokens)metricsshowmoderatecorrela-tion,theConnexormetricalmostso;theLinkGram-mar(invalidparses)metriccorrelationisbyfartheweakest.Theconsistencyandmagnitudeoftheﬁrstthreeparsermetrics,however,lendssupporttotheideaofWanetal.(2005)tousesomethingliketheseasindicatorsofgeneratedsentenceﬂuency.Theaimofthenextsectionistobuildabetterpredictorthantheindividualparsermetricsalone.MetricCorr.Humans0.6529CollinsParser0.4057Connexor-0.3804LinkGrammarNulledTokens-0.3310LinkGrammarInvalidParses0.1619GLEU0.4606Table4:CorrelationwithsequencesizeforhumantrialdatasetMetricCorr.CollinsParser0.3057Connexor-0.3445Link-GrammarNulledTokens-0.2939LinkGrammarInvalidParses0.1854GLEU0.4014Table5:Correlationbetweenmetricsandhumanevaluators4AnSVM-BasedMetricInMT,oneproblemwithmostmetricslikeBLEUisthattheyareintendedtoapplyonlytodocument-lengthtexts,andanyapplicationtoindividualsen-tencesisinaccurateandcorrelatespoorlywithhumanjudgements.Aneatsolutiontopoorsentence-levelevaluationproposedbyKuleszaandShieber(2004)istouseaSupportVectorMachine,usingfeaturessuchasworderrorrate,toestimatesentence-leveltranslationquality.Thetwomainin-sightsinapplyingSVMshereare,ﬁrst,notingthathumantranslationsaregenerallygoodandmachinetranslationspoor,thatbinarytrainingdatacanbecreatedbytakingthehumantranslationsasposi-tivetraininginstancesandmachinetranslationsasnegativeones;andsecond,thatanon-binarymetricoftranslationgoodnesscanbederivedbythedis-tancefromatestinstancetothesupportvectors.Inanempiricalevaluation,KuleszaandShieberfoundthattheirSVMgaveacorrelationof0.37,whichwasanimprovementofaroundhalfthegapbetweentheBLEUcorrelationswiththehumanjudgements(0.25)andthelowestpairwisehumaninter-judgecorrelation(0.46)(Turianetal.,2003).Wetakeasimilarapproachhere,usingasfeaturesthefourparsermetricsdescribedinSection3.WetrainedanSVM,4takingaspositivetrainingdatathe1000instancesofsentencesofsequencelength24(i.e.sentencesextractedfromthecorpus)andasnegativetrainingdatathe1000sentencesofse-quencelength1.WecallthislearnerGLEU.5AsacheckontheabilityoftheGLEUSVMtodis-tinguishthese‘positive’sentencesfrom‘negative’ones,weevaluateditsclassiﬁcationaccuracyona(new)testsetofsize300,splitevenlybetweensen-tencesofsequencelength24andsequencelength1.4WeusedthepackageSVM-light(Joachims,1999).5ForGrammaticaLityEvaluationUtility.349

Thisgave81%,againstarandombaselineof50%,indicatingthattheSVMcanclassifysatisfactorily.Wenowmovefromlookingatclassiﬁcationaccu-racytothemainpurposeoftheSVM,usingdistancefromsupportvectorasametric.ResultsaregivenforcorrelationofGLEUagainstsequencesizesforalldata(Table2)andforthehumantrialdataset(Table4);andalsoforcorrelationofGLEUagainstthehumanjudges’scores(Table5).Thislastindi-catesthatGLEUcorrelatesbetterwithhumanjudge-mentsthananyoftheparsersindividually,andiswellwithinthe‘moderate’rangeforcorrelationin-terpretation.Inparticular,fortheGLEU–humancor-relation,thescoreof0.4014isapproachingthemin-imumpairwisehumancorrelationof0.4710.5DifferentTextGenerationMethodsThemethodusedtogeneratetextinSection3.2isavariationofthestandardn-gramlanguagemodel.Aquestionthatarisesis:Areanyofthemetricsde-ﬁnedabovestronglyinﬂuencedbythetypeoflan-guagemodelusedtogeneratethetext?Itmaybethecase,forexample,thataparserimplementationusesitsownlanguagemodelthatpredisposesittofavourasimilarmodelinthetextgenerationprocess.ThisisaphenomenonseeninMT,whereBLEUseemstofavourtextthathasbeenproducedusingasimilarstatisticaln-gramlanguagemodeloverothersym-bolicmodels(Callison-Burchetal.,2006).Ourpreviousapproachusedonlysequencesofwordsconcatenatedtogether.Todeﬁnesomenewmethodsforgeneratingtext,weintroducedvaryingamountsofstructureintothegenerationprocess.5.1StructuralGenerationMethodsPoStagIntheﬁrstofthese,weconstructedaroughapproximationoftypicalsentencegrammarstructurebytakingbigramsoverpart-of-speechtags.6Then,givenastringofPoStagsoflengthn,t1...tn,westartbyassigningtheprobabilitiesforthewordinposition1,w1,accordingtothecon-ditionalprobabilityP(w1|t1).Then,forpositionj(2≤j≤n),weassigntocandidatewordsthevalueP(wj|tj)×P(wj|wj−1)toscorewordsequences.6WeusedthesupertaggerofBangaloreandJoshi(1999).So,forexample,wemightgeneratethePoStagtem-plateDetNNAdjAdv,takeallthewordscorre-spondingtoeachofthesepartsofspeech,andcom-binebigramwordsequenceprobabilitywiththecon-ditionalprobabilityofwordswithrespecttothesepartsofspeech.WethenuseaViterbi-stylealgo-rithmtoﬁndthemostlikelywordsequence.InthismodelweviolatetheMarkovassumptionofindependenceinmuchthesamewayasWitbrockandMittal(1999)intheircombinationofcontentandlanguagemodelprobabilities,bybacktrackingateverystateinordertodiscouragerepeatedwordsandavoidloops.SupertagThisisavariantoftheapproachabove,butusingsupertags(BangaloreandJoshi,1999)in-steadofPoStags.Theideaisthatthesupertagsmightgiveamoreﬁne-graineddeﬁnitionofstruc-ture,usingpartialtreesratherthanpartsofspeech.CFGWeextractedaCFGfromthe∼10%ofthePennTreebankfoundintheNLTK-litecorpora.7ThisCFGwasthenaugmentedwithproductionsde-rivedfromthePoS-taggeddatausedabove.Wethengeneratedatemplateoflengthnpre-terminalcate-goriesusingthisCFG.Toavoidloopswebiasedtheselectiontowardsterminalsovernon-terminals.5.2HumanJudgementsWegeneratedsentencesaccordingtoamixoftheinitialmethodofSection3.2,forcalibration,andthenewmethodsabove.Weagainusedasentencelengthof24,andsequencelengthsfortheinitialmethodofl=1,8,24.Asampleofsentencesgen-eratedforeachofthesesixtypesisinFigure3.Forourdata,wegenerated1000sentencespergen-erationmethod,givingacorpusof6000sentences.Forthehumanjudgementswealsoagaintook10sentencespergenerationmethod,giving60sen-tencesintotal.Thesamejudgesweregiventhesameinstructionsaspreviously.Beforecorrelatingthehumanjudges’scoresandtheparseroutputs,itisinterestingtolookathoweachparsertreatsthesentencegenerationmethods,andhowthiscompareswithhumanratings(Ta-ble6).Inparticular,notethattheCollinsparserratesthePoStag-andSupertag-generatedsentencesmore7http://nltk.sourceforge.net350

Extracted(Sequence-size24)Afteranearthree-hourmeetingandlast-minutetalkswithPres-identLennartMeri,theReformPartycouncilvotedoverwhelm-inglytoleavethegovernment.Sequence-size8IfDenmarkiscloselylinkedtotheEuroDisneyreportedanetproﬁtof85millionnote:theﬁgureswereroundedoff.Sequence-size1Israelistherewouldseekapprovalforall-partypeacenowcom-plainthatthisyear,whichshowsdemandfollowingyearand56billionpounds.POS-tag,Viterbi-mappedHesaidearlierthe9yearsandholdingcompany’sgovernment,including69.62pointsasanumberoflastyearbutmarket.Supertag,Viterbi-mappedThat97sayinghesaidinitssharesofthemarket74.53percent,addingtoallowforeignexchange:Ithinkpeople.Context-FreeGrammarTheproductionmoderatedChernomyrdinwhichleveledgov-ernmentbacknearown52overeveryacurrentatfromthesaidbylatertheother.Figure3:Samplesentencesfromthesecondtrialsent.types-24s-8s-1PoSsup.CFGCollins0.520.480.410.600.570.36Connexor0.120.160.240.260.250.43LG(null)0.020.060.100.090.110.18LG(invalid)0.780.670.560.620.660.53GLEU1.070.32-0.960.28-0.06-2.48Human0.930.670.440.390.440.31Table6:Meannormalisedscorespersentencetypehighlyeventhanrealsentences(inbold).ThesearethetwomethodsthatusetheViterbi-stylealgo-rithm,suggestingthatthisprobabilitymaximisationhasfooledtheCollinsparser.Thepairwisecorrela-tionbetweenjudgeswasaroundthesameonaverageasinSection3.3,butwithwidervariation(Table7).Themainresults,determiningthecorrelationofthevariousparsermetricsplusGLEUagainstthenewdata,areinTable8.Thisconﬁrmstheveryvari-ableperformanceoftheCollinsparser,whichhasdroppedsigniﬁcantly.GLEUperformsquiteconsis-tentlyhere,thistimealittlebehindtheLinkGram-mar(nulledtokens)result,butstillwithabettercorrelationwithhumanjudgementthanatleasttwoStatisticCorr.Maximumcorrelation0.9048Minimumcorrelation0.3318Meancorrelation0.7250Standarddeviation0.0980Table7:DataoncorrelationbetweenhumansMetricCorr.CollinsParser0.1898Connexor-0.3632Link-GrammarNulledTokens-0.4803LinkGrammarInvalidParses0.1774GLEU0.4738Table8:CorrelationbetweenparsersandhumanevaluatorsonnewhumantrialdataMetricCorr.CollinsParser0.2313Connexor-0.2042Link-GrammarNulledTokens-0.1289LinkGrammarInvalidParses-0.0084GLEU0.4312Table9:Correlationbetweenparsersandhumanevaluatorsonallhumantrialdatajudgeswitheachother.(NotealsothattheGLEUSVMwasnotretrainedonthenewsentencetypes.)Lookingatallthedatatogether,however,iswhereGLEUparticularlydisplaysitsconsistency.Aggre-gatingtheoldhumantrialdata(Section3.3)andthenewdata,anddeterminingcorrelationsagainstthemetrics,wegetthedatainTable9.AgaintheSVM’sperformanceisconsistent,butisnowalmosttwiceashighasitsnearestalternative,Collins.5.3DiscussionIngeneral,thereisatleastoneparserthatcorrelatesquitewellwiththehumanjudgesforeachsentencetype.Withwell-structuredsentences,theprobabilis-ticCollinsparserperformsbest;onsentencesthataregeneratedbyapoorprobabilisticmodellead-ingtopoorstructure,LinkGrammar(nulledtokens)performsbest.Thissupportstheuseofamachinelearnertakingasfeaturesoutputsfromseveralparsertypes;empiricallythisisconﬁrmedbythelargead-vantageGLEUhasonoveralldata(Table9).ThegeneratedtextitselffromtheViterbi-basedgen-eratorsasimplementedhereisquitedisappoint-ing,givenanexpectationthatintroducingstructurewouldmakesentencesmorenaturalandhenceleadtoarangeofsentencequalities.Inhindsight,thisisnotsosurprising;ingeneratingthestructuretem-plate,onlysequences(overtags)ofsize1wereused,whichisperhapswhythehumanjudgesdeemedthemfairlyclosetosentencesgeneratedbytheorigi-351

nalmethodusingsequencesize1,thepoorestofthatinitialdataset.6ConclusionInthispaperwehaveinvestigatedanewapproachtoevaluatingtheﬂuencyofindividualgeneratedsen-tences.Thenotionofwhatconstitutesﬂuencyisanimpreciseone,buttrialswithhumanjudgeshaveshownthatevenifitcannotbeexactlydeﬁned,orevenarticulatedbythejudges,thereisahighlevelofagreementaboutwhatisﬂuentandwhatisnot.Giventhisdata,metricsderivedfromparserout-putshavebeenfoundusefulformeasuringﬂuency,correlatinguptomoderatelywellwiththesehumanjudgements.Abetterapproachistocombinetheseinamachinelearner,asinourSVMGLEU,whichoutperformsindividualparsermetrics.Interestingly,wehavefoundthattheparsermetricscanbefooledbythemethodofsentencegeneration;GLEU,how-ever,givesaconsistentestimateofﬂuencyregard-lessofgenerationtype;and,acrossalltypesofgen-eratedsentencesexaminedinthispaper,issuperiortoindividualparsermetricsbyalargemargin.Thisallsuggeststhattheapproachhaspromise,butitneedstobedevelopedfurtherforpraticaluse.TheSVMpresentedinthispaperhasonlyfourfeatures;morefeatures,andinparticularawiderrangeofparsers,shouldraisecorrelations.Intermsofthedata,welookedonlyatsentencesgeneratedwithseveralparametersﬁxed,suchassentencelength,duetoourlimitedpoolofjudges.Infuturewewouldliketoexaminethespaceofsentencetypesmorefully.Inparticular,wewilllookatpredictingtheﬂu-encyofnear-humanqualitysentences.Moregener-ally,wewouldliketolookalsoathowtheapproachofthispaperwouldrelatetoaperplexity-basedmet-ric;howitcomparesagainstBLEUorsimilarmea-suresasapredictorofﬂuencyinacontextwhereref-erencesentencesareavailable;andwhetherGLEUmightbeusefulinapplicationssuchasrerankingofcandidatesentencesinMT.AcknowledgementsWethankBenHutchinsonandMirellaLapatafordiscussions,andSrinivasBangalorefortheTAGsupertagger.Thesec-ondauthoracknowledgesthesupportofARCDiscoveryGrantDP0558852.ReferencesSrinivasBangaloreandAravindJoshi.1999.Supertagging:Anapproachtoalmostparsing.ComputationalLinguistics,25(2):237–265.SrinivasBangalore,OwenRambow,andSteveWhittaker.2000.Evaluationmetricsforgeneration.InProceedingsoftheFirstInternationalNaturalLanguageGenerationConference(INLG2000),MitzpeRamon,Israel.E.Bard,D.Robertson,andA.Sorace.1996.Magnitudeesti-mationandlinguisticacceptability.Language,72(1):32–68.ChrisCallison-Burch,MilesOsborne,andPhilippKoehn.2006.Re-evaluatingtheRoleofBleuinMachineTranslationResearch.InProceedingsofEACL,pages249–256.Jos´eCoch.1996.Evaluatingandcomparingthreetext-productionstrategies.InProceedingsofthe16thInternationalConferenceonComputationalLinguistics(COLING’96),pages249–254.J.Cohen.1988.Statisticalpoweranalysisforthebehavioralsciences.Erlbaum,Hillsdale,NJ,US.MichaelCollins.1999.Head-DrivenStatisticalModelsforNaturalLanguageParsing.Ph.D.thesis,UniversityofPenn-sylvania.DennisGrinberg,JohnLafferty,andDanielSleator.1995.Arobusparsingalgorithmforlinkgrammars.InProceedingsoftheFourthInternationalWorkshoponParsingTechnologies.ThorstenJoachims.1999.MakingLarge-ScaleSVMLearningPractical.MITPress.DanielJurafskyandJamesMartin.2000.SpeechandLan-guageProcessing:AnIntroductiontoNaturalLangugePro-cessing,ComputationalLinguistics,andSpeechRecognition.Prentice-Hall.AlexKuleszaandStuartShieber.2004.Alearningapproachtoimprovingsentence-levelMTevaluation.InProceedingsofthe10thInternationalConferenceonTheoreticalandMethodolog-icalIssuesinMachineTranslation,Baltimore,MD,US.IreneLangkilde-Geary.2002.Anempiricalveriﬁcationofcov-erageandcorrectnessforageneral-purposesentencegenerator.InProceedingsoftheInternationalNaturalLanguageGenera-tionConference(INLG)2002,pages17–24.ShimeiPanandJamesShaw.2004.Segue:Ahybridcase-basedsurfacenaturallanguagegenerator.InProceedingsoftheInternationalConferenceonNaturalLanguageGeneration(INLG)2004,pages130–140.KishorePapineni,SalimRoukos,ToddWard,andWei-JingZhu.2002.BLEU:aMethodforAutomaticEvaluationofMa-chineTranslation.TechnicalReportRC22176,IBM.JosephTurian,LukeShen,andI.DanMelamed.2003.Evalua-tionofMachineTranslationanditsevaluation.InProceedingsofMTSummitIX,pages23–28.StephenWan,RobertDale,MarkDras,andC´ecileParis.2005.Searchingforgrammaticality:PropagatingdependenciesintheViterbialgorithm.InProceedingsofthe10thEuropeanNaturalLanguageProcessingWworkshop,Aberdeen,UK.MichaelWitbrockandVibhuMittal.1999.Ultra-summarization:Astatisticalapproachtogeneratinghighlycon-densednon-executivesummaries.InProceedingsofthe22ndInternationalConferenceonResearchandDevelopmentinIn-formationRetrieval(SIGIR’99).DavidZajic,BonnieDorr,andRichardSchwartz.2002.Au-tomaticheadlinegenerationfornewspaperstories.InPro-ceedingsoftheACL-2002WorkshoponTextSummarization(DUC2002),pages78–85.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 352–359,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

352

ConditionalModalityFusionforCoreferenceResolutionJacobEisensteinandRandallDavisComputerScienceandArtiﬁcialIntelligenceLaboratoryMassachusettsInstituteofTechnologyCambridge,MA02139USA{jacobe,davis}@csail.mit.eduAbstractNon-verbalmodalitiessuchasgesturecanimproveprocessingofspontaneousspokenlanguage.Forexample,similarhandges-turestendtopredictsemanticsimilarity,sofeaturesthatquantifygesturalsimilaritycanimprovesemantictaskssuchascoreferenceresolution.However,notallhandmove-mentsareinformativegestures;psycholog-icalresearchhasshownthatspeakersaremorelikelytogesturemeaningfullywhentheirspeechisambiguous.Ideally,onewouldattendtogestureonlyinsuchcir-cumstances,andignoreotherhandmove-ments.Wepresentconditionalmodalityfusion,whichformalizesthisintuitionbytreatingtheinformativenessofgestureasahiddenvariabletobelearnedjointlywiththeclasslabel.Appliedtocoreferenceresolution,conditionalmodalityfusionsig-niﬁcantlyoutperformsbothearlyandlatemodalityfusion,whicharecurrenttech-niquesformodalitycombination.1IntroductionNon-verbalmodalitiessuchasgestureandprosodycanincreasetherobustnessofNLPsystemstotheinevitabledisﬂuencyofspontaneousspeech.Forex-ample,considerthefollowingexcerptfromadia-logueinwhichthespeakerdescribesamechanicaldevice:“Sothismovesup,andit–everythingmovesup.Andthistoponeclearsthisareahere,andgoesallthewayuptothetop.”Thereferencesinthispassagearedifﬁculttodisambiguate,butthegesturesshowninFigure1makethemeaningmoreclear.However,non-verbalmodalitiesareoftennoisy,andtheirinteractionswithspeecharecomplex(McNeill,1992).Ges-ture,forexample,issometimescommunicative,butothertimesmerelydistracting.Whilepeoplehavelittledifﬁcultydistinguishingbetweenmeaningfulgesturesandirrelevanthandmotions(e.g.,self-touching,adjustingglasses)(GoodwinandGood-win,1986),NLPsystemsmaybeconfusedbysuchseeminglyrandommovements.Ourgoalistoin-cludenon-verbalfeaturesonlyinthespeciﬁccaseswhentheyarehelpfulandnecessary.Wepresentamodelthatlearnsinanunsupervisedfashionwhennon-verbalfeaturesareuseful,allow-ingittogatethecontributionofthosefeatures.Therelevanceofthenon-verbalfeaturesistreatedasahiddenvariable,whichislearnedjointlywiththeclasslabelinaconditionalmodel.Wedemonstratethatthisimprovesperformanceonbinarycorefer-enceresolution,thetaskofdeterminingwhetheranounphrasesreferstoasinglesemanticentity.Con-ditionalmodalityfusionyieldsarelativeincreaseof73%inthecontributionofhand-gesturefeatures.Themodelisnotspeciﬁcallytailoredtogesture-speechintegration,andmayalsobeapplicabletoothernon-verbalmodalities.2RelatedworkMostoftheexistingworkonintegratingnon-verbalfeaturesrelatestoprosody.Forexample,Shribergetal.(2000)exploretheuseofprosodicfeaturesforsentenceandtopicsegmentation.Theﬁrstmodal-353

Andthistoponeclearsthisareahere,andgoesallthewayuptothetop...2Sothismovesup.Andit–everythingmovesup.1Figure1:Anexamplewheregesturehelpstodisambiguatemeaning.itycombinationtechniquethattheyconsidertrainsasingleclassiﬁerwithallmodalitiescombinedintoasinglefeaturevector;thisissometimescalled“earlyfusion.”Shribergetal.alsoconsidertrainingsepa-rateclassiﬁersandcombiningtheirposteriors,eitherthroughweightedadditionormultiplication;thisissometimescalled“latefusion.”Latefusionisalsoemployedforgesture-speechcombinationin(Chenetal.,2004).Experimentsinboth(Shribergetal.,2000)and(Kimetal.,2004)ﬁndnoconclusivewin-neramongearlyfusion,additivelatefusion,andmultiplicativelatefusion.ToyamaandHorvitz(2000)introduceaBayesiannetworkapproachtomodalitycombinationforspeakeridentiﬁcation.Asinlatefusion,modality-speciﬁcclassiﬁersaretrainedindependently.How-ever,theBayesianapproachalsolearnstopredictthereliabilityofeachmodalityonagiveninstance,andincorporatesthisinformationintotheBayesnet.Whilemoreﬂexiblethantheinterpolationtech-niquesdescribedin(Shribergetal.,2000),trainingmodality-speciﬁcclassiﬁersseparatelyisstillsub-optimalcomparedtotrainingthemjointly,becauseindependenttrainingofthemodality-speciﬁcclassi-ﬁersforcesthemtoaccountfordatathattheycan-notpossiblyexplain.Forexample,ifthespeakerisnotgesturingmeaningfully,itiscounterproductivetotrainagesture-modalityclassiﬁeronthefeaturesatthisinstant;doingsocanleadtooverﬁttingandpoorgeneralization.Ourapproachcombinesaspectsofbothearlyandlatefusion.Asinearlyfusion,classiﬁersforallmodalitiesaretrainedjointly.ButasinToyamaandHorvitz’sBayesianlatefusionmodel,modalitiescanbeweightedbasedontheirpredictivepowerforspe-ciﬁcinstances.Inaddition,ourmodelistrainedtomaximizeconditionallikelihood,ratherthanjointlikelihood.3ConditionalmodalityfusionThegoalofourapproachistolearntoweightthenon-verbalfeaturesxnvonlywhentheyarerele-vant.Todothis,weintroduceahiddenvariablem∈{−1,1},whichgovernswhetherthenon-verbalfeaturesareincluded.p(m)isconditionedonasubsetoffeaturesxm,whichmaybelongtoanymodality;p(m|xm)islearnedjointlywiththeclasslabelp(y|x),withy∈{−1,1}.Forourcoreferenceresolutionmodel,ycorrespondstowhetheragivenpairofnounphrasesreferstothesameentity.Inalog-linearmodel,parameterizedbyweightsw,wehave:p(y|x;w)=Xmp(y,m|x;w)=Pmexp(ψ(y,m,x;w))Py0,mexp(ψ(y0,m,x;w)).Here,ψisapotentialfunctionrepresentingthecompatibilitybetweenthelabely,thehiddenvari-ablem,andtheobservationsx;thispotentialispa-rameterizedbyavectorofweights,w.Thenumera-torexpressesthecompatibilityofthelabelyandob-servationsx,summedoverallpossiblevaluesofthehiddenvariablem.Thedenominatorsumsoverbothmandallpossiblelabelsy0,yieldingtheconditionalprobabilityp(y|x;w).Theuseofhiddenvariables354

inaconditionally-trainedmodelfollows(Quattonietal.,2004).Thismodelcanbetrainedbyagradient-basedoptimizationtomaximizetheconditionallog-likelihoodoftheobservations.Theunregularizedlog-likelihoodandgradientaregivenby:l(w)=Xiln(p(yi|xi;w))(1)=XilnPmexp(ψ(yi,m,xi;w))Py0,mexp(ψ(y0,m,xi;w))(2)∂li∂wj=Xmp(m|yi,xi;w)∂∂wjψ(yi,m,xi;w)−Xy0,mp(m,y0|xi;w)∂∂wjψ(y0,m,xi;w)Theformofthepotentialfunctionψiswhereourintuitionsabouttheroleofthehiddenvariableareformalized.Ourgoalistoincludethenon-verbalfeaturesxnvonlywhentheyarerelevant;conse-quently,theweightforthesefeaturesshouldgotozeroforsomesettingsofthehiddenvariablem.Inaddition,verballanguageisdifferentwhenusedincombinationwithmeaningfulnon-verbalcommu-nicationthanwhenitisusedunimodally(Kehler,2000;MelingerandLevelt,2004).Thus,welearnadifferentsetoffeatureweightsforeachcase:wv,1whenthenon-verbalfeaturesareincluded,andwv,2otherwise.Theformaldeﬁnitionofthepotentialfunctionforconditionalmodalityfusionis:ψ(y,m,x;w)≡(y(wTv,1xv+wTnvxnv)+wTmxmm=1ywTv,2xv−wTmxmm=−1.(3)4ApplicationtocoreferenceresolutionWeapplyconditionalmodalityfusiontocorefer-enceresolution–theproblemofpartitioningthenounphrasesinadocumentintoclusters,whereallmembersofaclusterrefertothesamesemanticen-tity.Coreferenceresolutionontextdatasetsiswell-studied(e.g.,(CardieandWagstaff,1999)).Thispriorworkprovidesthedeparturepointforourin-vestigationofcoreferenceresolutiononspontaneousandunconstrainedspeechandgesture.4.1FormofthemodelTheformofthemodelusedinthisapplicationisslightlydifferentfromthatshowninEquation3.Whendeterminingwhethertwonounphrasescore-fer,thefeaturesateachutterancemustbeconsid-ered.Forexample,ifwearetocomparethesimi-larityofthegesturesthataccompanythetwonounphrases,itshouldbethecasethatgestureisrelevantduringbothtimeperiods.Forthisreason,wecreatetwohiddenvariables,m1andm2;theyindicatetherelevanceofges-tureovertheﬁrst(antecedent)andsecond(anaphor)nounphrases,respectively.SincegesturesimilarityisonlymeaningfulifthegestureisrelevantduringbothNPs,thegesturefeaturesareincludedonlyifm1=m2=1.Similarly,thelinguisticfeatureweightswv,1areusedwhenm1=m2=1;oth-erwisetheweightswv,2areused.ThisyieldsthemodelshowninEquation4.Thevectorofmetafeaturesxm1includesallsingle-phraseverbalandgesturefeaturesfromTa-ble1,computedattheantecedentnounphrase;xm2includesthesingle-phraseverbalandgesturefeatures,computedattheanaphoricnounphrase.Thelabel-dependentverbalfeaturesxvincludebothpairwiseandsinglephraseverbalfeaturesfromthetable,whilethelabel-dependentnon-verbalfeaturesxnvincludeonlythepairwisegesturefeatures.Thesingle-phrasenon-verbalfeatureswerenotincludedbecausetheywerenotthoughttobeinformativeastowhethertheassociatednoun-phrasewouldpartic-ipateincoreferencerelations.4.2VerbalfeaturesWeemployasetofverbalfeaturesthatissimilartothefeaturesusedbystate-of-the-artcoreferenceresolutionsystemsthatoperateontext(e.g.,(CardieandWagstaff,1999)).Pairwiseverbalfeaturesin-clude:severalstring-matchvariants;distancefea-tures,measuredintermsofthenumberofinterven-ingnounphrasesandsentencesbetweenthecandi-dateNPs;andsomesyntacticfeaturesthatcanbecomputedfrompartofspeechtags.Single-phraseverbalfeaturesdescribethetypeofthenounphrase(deﬁnite,indeﬁnite,demonstrative(e.g.,thisball),orpronoun),thenumberoftimesitappearedinthedocument,andwhethertherewereanyadjecti-355

ψ(y,m1,m2,x;w)≡(y(wTv,1xv+wTnvxnv)+m1wTmxm1+m2wTmxm2,m1=m2=1ywTv,2xv+m1wTmxm1+m2wTmxm2,otherwise.(4)valmodiﬁers.Thecontinuous-valuedfeatureswerebinnedusingasupervisedtechnique(FayyadandIrani,1993).Notethatsomefeaturescommonlyusedforcoref-erenceontheMUCandACEcorporaarenotappli-cablehere.Forexample,gazetteerslistingnamesofnationsorcorporationsarenotrelevanttoourcor-pus,whichfocusesondiscussionsofmechanicalde-vices(seesection5).Becauseweareworkingfromtranscriptsratherthantext,featuresdependentonpunctuationandcapitalization,suchasapposition,arealsonotapplicable.4.3Non-verbalfeaturesOurnon-verbalfeaturesattempttocapturesimilar-itybetweenthespeaker’shandgestures;similarges-turesarethoughttosuggestsemanticsimilarity(Mc-Neill,1992).Forexample,twonounphrasesmaybemorelikelytocoreferiftheyareaccompaniedbyidentically-locatedpointinggestures.Inthissection,wedescribefeaturesthatquantifyvariousaspectsofgesturalsimilarity.ThemoststraightforwardmeasureofsimilarityistheEuclideandistancebetweentheaveragehandpo-sitionduringeachnounphrase–wecallthistheFOCUS-DISTANCEfeature.Euclideandistancecap-turescasesinwhichthespeakerisperformingages-tural“hold”inroughlythesamelocation(McNeill,1992).However,Euclideandistancemaynotcorrelatedirectlywithsemanticsimilarity.Forexample,whengesturingatadetailedpartofadiagram,verysmallchangesinhandpositionmaybese-manticallymeaningful,whileinotherregionsposi-tionalsimilaritymaybedeﬁnedmoreloosely.Ide-ally,wewouldcomputeasemanticfeaturecap-turingtheobjectofthespeaker’sreference(e.g.,“theredblock”),butthisisnotpossibleingen-eral,sinceacompletetaxonomyofallpossibleob-jectsofreferenceisusuallyunknown.Instead,weuseahiddenMarkovmodel(HMM)toperformaspatio-temporalclusteringonhandpositionandve-locity.TheSAME-CLUSTERfeaturereportswhetherthehandpositionsduringtwonounphraseswereusuallygroupedinthesameclusterbytheHMM.JS-DIVreportstheJensen-Shannondivergence,acontinuous-valuedfeatureusedtomeasurethesimi-larityinclusterassignmentprobabilitiesbetweenthetwogestures(Lin,1991).Thegesturefeaturesdescribedthusfarcapturethesimilaritybetweenstaticgestures;thatis,gesturesinwhichthehandpositionisnearlyconstant.How-ever,thesefeaturesdonotcapturethesimilaritybe-tweengesturetrajectories,whichmayalsobeusedtocommunicatemeaning.Forexample,adescrip-tionoftwoidenticalmotionsmightbeexpressedbyverysimilargesturetrajectories.Tomeasurethesimilaritybetweengesturetrajectories,weusedy-namictimewarping(Huangetal.,2001),whichgivesasimilaritymetricfortemporaldatathatisinvarianttospeed.ThisisreportedintheDTW-DISTANCEfeature.Allfeaturesarecomputedfromhandandbodypixelcoordinates,whichareobtainedviacomputervision;ourvisionsystemissimilarto(Deutscheretal.,2000).Thefeaturesetcurrentlysupportsonlysingle-handgestures,usingthehandthatisfarthestfromthebodycenter.Aswiththeverbalfeatureset,supervisedbinningwasappliedtothecontinuous-valuedfeatures.4.4MetafeaturesTheroleofthemetafeaturesistodeterminewhetherthegesturefeaturesarerelevantatagivenpointintime.Tomakethisdetermination,bothverbalandnon-verbalfeaturesareapplied;theonlyrequire-mentisthattheybecomputableatasingleinstantintime(unlikefeaturesthatmeasurethesimilaritybetweentwoNPsorgestures).VerbalmetafeaturesMeaningfulgesturehasbeenshowntobemorefrequentwhentheassociatedspeechisambiguous(MelingerandLevelt,2004).Kehlerﬁndsthatfully-speciﬁednounphrasesarelesslikelytoreceivemultimodalsupport(Kehler,2000).Theseﬁndingsleadustoexpectthatpro-356

Pairwiseverbalfeaturesedit-distanceanumericalmeasureofthestringsimi-laritybetweenthetwoNPsexact-matchtrueifthetwoNPshaveidenticalsur-faceformsstr-matchtrueiftheNPsareidenticalafterre-movingarticlesnonpro-strtrueifiandjarenotpronouns,andstr-matchistruepro-strtrueifiandjarepronouns,andstr-matchistruej-substring-itrueiftheanaphorjisasubstringoftheantecedentii-substring-jtrueifiisasubstringofjoverlaptrueifthereareanysharedwordsbe-tweeniandjnp-distthenumberofnounphrasesbetweeniandjinthedocumentsent-distthenumberofsentencesbetweeniandjinthedocumentboth-subjtrueifbothiandjprecedetheﬁrstverboftheirsentencessame-verbtrueiftheﬁrstverbinthesentencesforiandjisidenticalnumber-matchtrueifiandjhavethesamenumberSingle-phraseverbalfeaturespronountrueiftheNPisapronouncountnumberoftimestheNPappearsinthedocumenthas-modiﬁerstrueiftheNPhasadjectivemodiﬁersindef-nptrueiftheNPisanindeﬁniteNP(e.g.,aﬁsh)def-nptrueiftheNPisadeﬁniteNP(e.g.,thescooter)dem-nptrueiftheNPbeginswiththis,that,these,orthoselexicalfeatureslexicalfeaturesaredeﬁnedforthemostcommonpronouns:it,that,this,andtheyPairwisegesturefeaturesfocus-distancetheEuclideandistanceinpixelsbe-tweentheaveragehandpositionduringthetwoNPsDTW-agreementameasureoftheagreementofthehand-trajectoriesduringthetwoNPs,com-putedusingdynamictimewarpingsame-clustertrueifthehandpositionsduringthetwoNPsfallinthesameclusterJS-divtheJensen-Shannondivergencebe-tweentheclusterassignmentlikeli-hoodsSingle-phrasegesturefeaturesdist-to-restdistanceofthehandfromrestpositionjittersumofinstantaneousmotionacrossNPspeedtotaldisplacementoverNP,dividedbydurationrest-clustertrueifthehandisusuallyintheclusterassociatedwithrestpositionmovement-clustertrueifthehandisusuallyintheclusterassociatedwithmovementTable1:Thefeaturesetnounsshouldbelikelytoco-occurwithmeaningfulgestures,whiledeﬁniteNPsandnounphrasesthatincludeadjectivalmodiﬁersshouldbeunlikelytodoso.Tocapturetheseintuitions,allsingle-phrasever-balfeaturesareincludedasmetafeatures.Non-verbalmetafeaturesResearchongesturehasshownthatsemanticallymeaningfulhandmo-tionsusuallytakeplaceawayfrom“restposition,”whichislocatedatthespeaker’slaporsides(Mc-Neill,1992).Effortfulmovementsawayfromthesedefaultpositionscanthusbeexpectedtopredictthatgestureisbeingusedtocommunicate.Weiden-tifyrestpositionasthecenterofthebodyonthex-axis,andataﬁxed,predeﬁnedlocationonthey-axis.TheDIST-TO-RESTfeaturecomputestheav-erageEuclideandistanceofthehandsfromtherestposition,overthedurationoftheNP.Asnotedintheprevioussection,aspatio-temporalclusteringwasperformedonthehandpo-sitionsandvelocities,usinganHMM.TheREST-CLUSTERfeaturetakesthevalue“true”iffthemostfrequentlyoccupiedclusterduringtheNPistheclusterclosesttorestposition.Inaddition,pa-rametertyingintheHMMforcesallclustersbutonetorepresentstatichold,withtheremainingclusteraccountingforthetransitionmovementsbe-tweenholds.Onlythislastclusterispermittedtohaveanexpectednon-zerospeed;ifthehandismostfrequentlyinthisclusterduringtheNP,thentheMOVEMENT-CLUSTERfeaturetakesthevalue“true.”4.5ImplementationTheobjectivefunction(Equation1)isoptimizedusingaJavaimplementationofL-BFGS,aquasi-Newtonnumericaloptimizationtechnique(LiuandNocedal,1989).StandardL2-normregulariza-tionisemployedtopreventoverﬁtting,withcross-validationtoselecttheregularizationconstant.Al-thoughstandardlogisticregressionoptimizesacon-vexobjective,theinclusionofthehiddenvariablerendersourobjectivenon-convex.Thus,conver-gencetoaglobalminimumisnotguaranteed.5EvaluationsetupDatasetOurdatasetconsistsofsixteenshortdia-logues,inwhichparticipantsexplainedthebehavior357

ofmechanicaldevicestoafriend.Thereareninedifferentpairsofparticipants;eachcontributedtwodialogues,withtwothrownoutduetorecordinger-rors.Oneparticipant,the“speaker,”sawashortvideodescribingthefunctionofthedevicepriortothedialogue;theotherparticipantwastestedoncomprehensionofthedevice’sbehaviorafterthedi-alogue.Thespeakerwasgivenapre-printeddia-gramtoaidinthediscussion.Forsimplicity,onlythespeaker’sutteranceswereincludedintheseex-periments.Thedialogueswerelimitedtothreeminutesindu-ration,andmostoftheparticipantsusedtheentireallottedtime.“Markable”nounphrases–thosethatarepermittedtoparticipateincoreferencerelations–wereannotatedbytheﬁrstauthor,inaccordancewiththeMUCtaskdeﬁnition(HirschmanandChin-chor,1997).Atotalof1141“markable”NPsweretranscribed,roughlyhalfthesizeoftheMUC6de-velopmentset,whichincludes2072markableNPsover30documents.EvaluationmetricCoreferenceresolutionisof-tenperformedintwophases:abinaryclassiﬁ-cationphase,inwhichthelikelihoodofcorefer-enceforeachpairofnounphrasesisassessed;andapartitioningphase,inwhichtheclustersofmutually-coreferringNPsareformed,maximizingsomeglobalcriterion(CardieandWagstaff,1999).Ourmodeldoesnotaddresstheformationofnoun-phraseclusters,butonlythequestionofwhethereachpairofnounphrasesinthedocumentcorefer.Consequently,weevaluateonlythebinaryclassiﬁ-cationphase,andreportresultsintermsoftheareaundertheROCcurve(AUC).Asthesmallsizeofthecorpusdidnotpermitdedicatedtestanddevel-opmentsets,resultsarecomputedusingleave-one-outcross-validation,withonefoldforeachofthesixteendocumentsinthecorpus.BaselinesThreetypesofbaselineswerecomparedtoourconditionalmodalityfusion(CMF)technique:•Earlyfusion.Theearlyfusionbaselinein-cludesallfeaturesinasinglevector,ignor-ingmodality.Thisisequivalenttostandardmaximum-entropyclassiﬁcation.Earlyfusionisimplementedwithaconditionally-trainedlinearclassiﬁer;itusesthesamecodeastheCMFmodel,butalwaysincludesallfeatures.•Latefusion.Thelatefusionbaselinestrainseparateclassiﬁersforgestureandspeech,andthencombinetheirposteriors.Themodality-speciﬁcclassiﬁersareconditionally-trainedlin-earmodels,andagainusethesamecodeastheCMFmodel.Forsimplicity,aparametersweepidentiﬁestheinterpolationweightsthatmaxi-mizeperformanceonthetestset.Thus,itislikelythattheseresultssomewhatoverestimatetheperformanceofthesebaselinemodels.Wereportresultsforbothadditiveandmultiplica-tivecombinationofposteriors.•Nofusion.Thesebaselinesincludethefea-turesfromonlyasinglemodality,andagainbuildaconditionally-trainedlinearclassiﬁer.ImplementationusesthesamecodeastheCMFmodel,butweightsonfeaturesoutsidethetar-getmodalityareforcedtozero.Althoughacomparisonwithexistingstate-of-the-artcoreferencesystemswouldbeideal,allsuchavailablesystemsuseverbalfeaturesthatareinap-plicabletoourdataset,suchaspunctuation,capital-ization,andgazetteers.Theverbalfeaturesthatwehaveincludedarearepresentativesamplefromtheliterature(e.g.,(CardieandWagstaff,1999)).The“nofusion,verbalfeaturesonly”baselinethuspro-videsareasonablerepresentationofpriorworkoncoreference,byapplyingamaximum-entropyclas-siﬁertothissetoftypicalverbalfeatures.ParametertuningContinuousfeaturesarebinnedseparatelyforeachcross-validationfold,usingonlythetrainingdata.Theregularizationconstantisselectedbycross-validationwithineachtrainingsubset.6ResultsConditionalmodalityfusionoutperformsallotherapproachesbyastatisticallysigniﬁcantmargin(Ta-ble2).Comparedwithearlyfusion,CMFoffersanabsoluteimprovementof1.20%inareaundertheROCcurve(AUC).1Apairedt-testshowsthatthis1AUCquantiﬁestherankingaccuracyofaclassiﬁer.IftheAUCis1,allpositively-labeledexamplesarerankedhigherthanallnegative-labeledones.358

modelAUCConditionalmodalityfusion.8226Earlyfusion.8109Latefusion,multiplicative.8103Latefusion,additive.8068Nofusion(verbalfeaturesonly).7945Nofusion(gesturefeaturesonly).6732Table2:Results,intermsofareasundertheROCcurve23456780.790.7950.80.8050.810.8150.820.8250.83log of regularization constantAUCCMFEarly FusionSpeech OnlyFigure2:Conditionalmodalityfusionisrobusttovariationsintheregularizationconstant.resultisstatisticallysigniﬁcant(p<.002,t(15)=3.73).CMFobtainshigherperformanceonfourteenofthesixteentestfolds.Bothadditiveandmulti-plicativelatefusionperformonparwithearlyfu-sion.Earlyfusionwithgesturefeaturesissuperiortounimodalverbalclassiﬁcationbyanabsoluteim-provementof1.64%AUC(p<4∗10−4,t(15)=4.45).Thus,whilegesturefeaturesimprovecoref-erenceresolutiononthisdataset,theireffectivenessisincreasedbyarelative73%whenconditionalmodalityfusionisapplied.Figure2showshowper-formancevarieswiththeregularizationconstant.7DiscussionThefeatureweightslearnedbythesystemtodeter-minecoreferencelargelyconﬁrmourlinguisticin-tuitions.Amongthetextualfeatures,alargepos-itiveweightwasassignedtothestringmatchfea-tures,whilealargenegativeweightwasassignedtofeaturessuchasnumberincompatibility(i.e.,sin-pronoundefdemindef"this""it""that""they"modifiers−0.6−0.5−0.4−0.3−0.2−0.100.10.20.30.4Weights learned with verbal meta featuresFigure3:Weightsforverbalmetafeaturesgularversusplural).Thesystemalsolearnedthatgestureswithsimilarhandpositionsandtrajectorieswerelikelytoindicatecoreferringnounphrases;allofoursimilaritymetricswerecorrelatedpositivelywithcoreference.Achi-squaredanalysisfoundthattheEDITDISTANCEwasthemostinformativever-balfeature.ThemostinformativegesturefeaturewasDTW-AGREEMENTfeature,whichmeasuresthesimilaritybetweengesturetrajectories.Asdescribedinsection4,bothtextualandgestu-ralfeaturesareusedtodeterminewhethertheges-tureisrelevant.Amongtextualfeatures,deﬁniteandindeﬁnitenounphraseswereassignednega-tiveweights,suggestinggesturewouldnotbeuse-fultodisambiguatecoreferenceforsuchNPs.Pro-nounswereassignedpositiveweights,with“this”andthemuchlessfrequentlyused“they”receivingthestrongestweights.“It”and“that”receivedlowerweights;weobservedthatthesepronounswerefre-quentlyusedtorefertotheimmediatelyprecedingnounphrase,somultimodalsupportwasoftenun-necessary.Last,wenotethatNPswithadjectivalmodiﬁerswereassignednegativeweights,support-ingtheﬁndingof(Kehler,2000)thatfully-speciﬁedNPsarelesslikelytoreceivemultimodalsupport.AsummaryoftheweightsassignedtotheverbalmetafeaturesisshowninFigure3.Amonggesturemetafeatures,theweightslearnedbythesystemindicatethatnon-movinghandgesturesawayfromthebodyaremostlikelytobeinformativeinthisdataset.359

8FutureworkWehaveassumedthattherelevanceofgesturetosemanticsisdependentonlyonthecurrentlyavail-ablefeatures,andnotconditionedonpriorhistory.Inreality,meaningfulgesturesoccurovercontigu-ousblocksoftime,ratherthanatrandomlydis-tributedinstances.Indeed,thepsychologyliteraturedescribesaﬁnite-statemodelofgesture,proceed-ingfrom“preparation,”to“stroke,”“hold,”andthen“retraction”(McNeill,1992).Theseunitsarecalledmovementphases.Therelevanceofvariousgesturefeaturesmaybeexpectedtodependonthemove-mentphase.Duringstrokes,thetrajectoryofthegesturemaybethemostrelevantfeature,whiledur-ingholds,staticfeaturessuchashandpositionandhandshapemaydominate;duringpreparationandretraction,gesturefeaturesarelikelytobeirrelevant.Theidentiﬁcationofthesemovementphasesshouldbeindependentofthespeciﬁcproblemofcoreferenceresolution.Thus,additionallabelsforotherlinguisticphenomena(e.g.,topicsegmenta-tion,disﬂuency)couldbecombinedintothemodel.Ideally,eachadditionalsetoflabelswouldtransferperformancegainstotheotherlabelingproblems.9ConclusionsWehavepresentedanewmethodforcombiningmultiplemodalities,whichwefeelisespeciallyrel-evanttonon-verbalmodalitiesthatareusedtocom-municateonlyintermittently.Ourmodeltreatstherelevanceofthenon-verbalmodalityasahiddenvariable,learnedjointlywiththeclasslabels.Ap-pliedtocoreferenceresolution,thismodelyieldsarelativeincreaseof73%inthecontributionofthegesturefeatures.Thisgainisattainedbyidentify-inginstancesinwhichgesturefeaturesareespeciallyrelevant,andweighingtheircontributionmoreheav-ily.Wenextplantoinvestigatemodelswithatem-poralcomponent,sothatthebehaviorofthehiddenvariableisgovernedbyaﬁnite-statetransducer.AcknowledgmentsWethankAaronAdler,ReginaBarzilay,S.R.K.Branavan,SonyaCates,ErdongChen,MichaelCollins,LisaGuttentag,MichaelOltmans,andTomOuyang.ThisresearchissupportedinpartbyMITProjectOxy-gen.ReferencesClaireCardieandKiriWagstaff.1999.Nounphrasecorefer-enceasclustering.InProceedingsofEMNLP,pages82–89.LeiChen,YangLiu,MaryP.Harper,andElizabethShriberg.2004.Multimodalmodelintegrationforsentenceunitde-tection.InProceedingsofICMI,pages121–128.JonathanDeutscher,AndrewBlake,andIanReid.2000.Artic-ulatedbodymotioncapturebyannealedparticleﬁltering.InProceedingsofCVPR,volume2,pages126–133.UsamaM.FayyadandKekiB.Irani.1993.Multi-intervaldiscretizationofcontinuousvaluedattributesforclassiﬁca-tionlearning.InProceedingsofIJCAI-93,volume2,pages1022–1027.MorganKaufmann.M.H.GoodwinandC.Goodwin.1986.Gestureandco-participationintheactivityofsearchingforaword.Semiot-ica,62:51–75.LynetteHirschmanandNancyChinchor.1997.MUC-7coref-erencetaskdeﬁnition.InProceedingsoftheMessageUn-derstandingConference.XuedongHuang,AlexAcero,andHsiao-WuenHon.2001.SpokenLanguageProcessing.PrenticeHall.AndrewKehler.2000.Cognitivestatusandformofreferenceinmultimodalhuman-computerinteraction.InProceedingsofAAAI,pages685–690.JoungbumKim,SarahE.Schwarm,andMariOsterdorf.2004.Detectingstructuralmetadatawithdecisiontreesandtransformation-basedlearning.InProceedingsofHLT-NAACL’04.ACLPress.JianhuaLin.1991.Divergencemeasuresbasedontheshannonentropy.IEEEtransactionsoninformationtheory,37:145–151.DongC.LiuandJorgeNocedal.1989.OnthelimitedmemoryBFGSmethodforlargescaleoptimization.MathematicalProgramming,45:503–528.DavidMcNeill.1992.HandandMind.TheUniversityofChicagoPress.AlissaMelingerandWillemJ.M.Levelt.2004.Gestureandcommunicativeintentionofthespeaker.Gesture,4(2):119–141.AriadnaQuattoni,MichaelCollins,andTrevorDarrell.2004.Conditionalrandomﬁeldsforobjectrecognition.InNeuralInformationProcessingSystems,pages1097–1104.ElizabethShriberg,AndreasStolcke,DilekHakkani-Tur,andGokhanTur.2000.Prosody-basedautomaticsegmentationofspeechintosentencesandtopics.SpeechCommunication,32.KentaroToyamaandEricHorvitz.2000.Bayesianmodal-ityfusion:Probabilisticintegrationofmultiplevisional-gorithmsforheadtracking.InProceedingsofACCV’00,FourthAsianConferenceonComputerVision.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 360–367,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

360

The Utility of a Graphical Representation of Discourse Structure in Spoken Dialogue Systems Mihai Rotaru University of Pittsburgh Pittsburgh, USA mrotaru@cs.pitt.edu Diane J. Litman University of Pittsburgh Pittsburgh, USA litman@cs.pitt.edu   Abstract In this paper we explore the utility of the Navigation Map (NM), a graphical repre-sentation of the discourse structure. We run a user study to investigate if users perceive the NM as helpful in a tutoring spoken dia-logue system. From the users’ perspective, our results show that the NM presence al-lows them to better identify and follow the tutoring plan and to better integrate the in-struction. It was also easier for users to concentrate and to learn from the system if the NM was present. Our preliminary analysis on objective metrics further strengthens these findings. 1 Introduction With recent advances in spoken dialogue system technologies, researchers have turned their atten-tion to more complex domains (e.g. tutoring (Litman and Silliman, 2004; Pon-Barry et al., 2006), technical support (Acomb et al., 2007), medication assistance (Allen et al., 2006)). These domains bring forward new challenges and issues that can affect the usability of such systems: in-creased task complexity, user’s lack of or limited task knowledge, and longer system turns. In typical information access dialogue systems, the task is relatively simple: get the information from the user and return the query results with minimal complexity added by confirmation dia-logues. Moreover, in most cases, users have knowledge about the task. However, in complex domains things are different. Take for example tutoring. A tutoring dialogue system has to discuss concepts, laws and relationships and to engage in complex subdialogues to correct user misconcep-tions. In addition, it is very likely that users of such systems are not familiar or are only partially famil-iar with the tutoring topic. The length of system turns can also be affected as these systems need to make explicit the connections between parts of the underlying task. Thus, interacting with such systems can be char-acterized by an increased user cognitive load asso-ciated with listening to often lengthy system turns and the need to integrate the current information to the discussion overall (Oviatt et al., 2004). We hypothesize that one way to reduce the user’s cognitive load is to make explicit two pieces of information: the purpose of the current system turn, and how the system turn relates to the overall discussion. This information is implicitly encoded in the intentional structure of a discourse as pro-posed in the Grosz & Sidner theory of discourse (Grosz and Sidner, 1986). Consequently, in this paper we propose using a graphical representation of the discourse structure as a way of improving the performance of com-plex-domain dialogue systems (note that graphical output is required). We call it the Navigation Map (NM). The NM is a dynamic representation of the discourse segment hierarchy and the discourse seg-ment purpose information enriched with several features (Section 3). To make a parallel with geog-raphy, as the system “navigates” with the user through the domain, the NM offers a cartographic view of the discussion. While a somewhat similar graphical representation of the discourse structure has been explored in one previous study (Rich and Sidner, 1998), to our knowledge we are the first to test its benefits (see Section 6). 361

As a first step towards understanding the NM ef-fects, here we focus on investigating whether users prefer a system with the NM over a system without the NM and, if yes, what are the NM usage pat-terns. We test this in a speech based computer tutor (Section 2). We run a within-subjects user study in which users interacted with the system both with and without the NM (Section 4). Our analysis of the users’ subjective evaluation of the system indicates that users prefer the version of the system with the NM over the version with-out the NM on several dimensions. The NM pres-ence allows the users to better identify and follow the tutoring plan and to better integrate the instruc-tion. It was also easier for users to concentrate and to learn from the system if the NM was present. Our preliminary analysis on objective metrics fur-ther strengthens these findings. 2 ITSPOKE ITSPOKE (Litman and Silliman, 2004) is a state-of-the-art tutoring spoken dialogue system for con-ceptual physics. When interacting with ITSPOKE, users first type an essay answering a qualitative physics problem using a graphical user interface. ITSPOKE then engages the user in spoken dialogue (using head-mounted microphone input and speech output) to correct misconceptions and elicit more complete explanations, after which the user revises the essay, thereby ending the tutoring or causing another round of tutoring/essay revision. All dialogues with ITSPOKE follow a question-answer format (i.e. system initiative): ITSPOKE asks a question, users answer and then the process is repeated. Deciding what question to ask, in what order and when to stop is hand-authored before-hand in a hierarchical structure. Internally, system questions are grouped in question segments. In Figure 1, we show the transcript of a sample interaction with ITSPOKE. The system is discussing the problem listed in the upper right corner of the figure and it is currently asking the question Tu-tor5. The left side of the figure shows the interac-tion transcript (not available to the user at run-time). The right side of the figure shows the NM which will be discussed in the next section. Our system behaves as follows. First, based on the analysis of the user essay, it selects a question segment to correct misconceptions or to elicit more complete explanations. Next the system asks every question from this question segment. If the user answer is correct, the system simply moves on to the next question (e.g. Tutor2→Tutor3). For incor-rect answers there are two alternatives. For simple questions, the system will give out the correct an-swer accompanied by a short explanation and move on to the next question (e.g. Tutor1→Tutor2). For complex questions (e.g. applying physics laws), ITSPOKE will engage into a remediation subdialogue that attempts to remediate user’s lack of knowledge or skills (e.g. Tutor4→Tutor5). The remediation subdialogue for each complex ques-tion is specified in another question segment. Our system exhibits some of the issues we linked in Section 1 with complex-domain systems. Dialogues with our system can be long and com-plex (e.g. the question segment hierarchical struc-ture can reach level 6) and sometimes the system’s turn can be quite long (e.g. Tutor2). User’s reduced knowledge of the task is also inherent in tutoring.  3 The Navigation Map (NM) We use the Grosz & Sidner theory of discourse (Grosz and Sidner, 1986) to inform our NM de-sign. According to this theory, each discourse has a discourse purpose/intention. Satisfying the main discourse purpose is achieved by satisfying several smaller purposes/intentions organized in a hierar-chical structure. As a result, the discourse is seg-mented into discourse segments each with an asso-ciated discourse segment purpose/intention. This theory has inspired several generic dialogue man-agers for spoken dialogue systems (e.g. (Rich and Sidner, 1998)). The NM requires that we have the discourse structure information at runtime. To do that, we manually annotate the system’s internal representa-tion of the tutoring task with discourse segment purpose and hierarchy information. Based on this annotation, we can easily construct the discourse structure at runtime. In this section we describe our annotation and the NM design choices we made. Figure 1 shows the state of the NM after turn Tu-tor5 as the user sees it on the interface (NM line numbering is for exposition only). Note that Figure 1 is not a screenshot of the actual system interface. The NM is the only part from the actual system interface. Figure 2 shows the NM after turn Tutor1. We manually annotated each system ques-tion/explanation for its intention(s)/purpose(s). Note that some system turns have multiple inten-362

tions/purposes thus multiple discourse segments were created for them. For example, in Tutor1 the system first identifies the time frames on which the analysis will be performed (Figure 1&2, NM2). Next, the system indicates that it will discuss about the first time frame (Figure 1&2, NM3) and then it asks the actual question (Figure 2, NM4).  Thus, in addition to our manual annotation of the discourse segment purpose information, we manually organized all discourse segments from a question segment in a hierarchical structure that reflects the discourse structure. At runtime, while discussing a question seg-ment, the system has only to follow the annotated hierarchy, displaying and highlighting the dis-course segment purposes associated with the ut-tered content. For example, while uttering Tutor1, the NM will synchronously highlight NM2, NM3 and NM4. Remediation question segments (e.g. NM12) or explanations (e.g. NM5) activated by in-correct answers are attached to the structure under the corresponding discourse segment. 3.1 NM Design Choices In our graphical representation of the discourse structure, we used a left to right indented layout. In addition, we made several design choices to enrich the NM information content and usability. Figure 2. NM state after turn Tutor1 Correct answers. In Figure 2 we show the state of the NM after uttering Tutor1. The current dis-course segment purpose (NM4) indicates that the system is asking about the relationship between the two velocities. While we could have kept the same information after the system was done with this discourse segment, we thought that users will benefit from having the correct answer on the screen (recall NM4 in Figure 1). Thus, the NM was enhanced to display the correct answer after the system is done with each question. We extracted the correct answer from the system specifications for each question and manually created a new ver-sion of the discourse segment purpose that includes this information. 123456TUTOR1: To analyze this problem we will first de-scribe the motion of the person and his keys while heis holding them. Then we will look at the motion of the person and his keys after he lets go of them.   Let's begin by looking at the motion of the man andhis keys while he is holding them.   How does his velocity compare to that of his keys?User answer1: man’s velocity is bigger [incorrect] TUTOR2: Well… The problem statement says that theman is holding his keys motionless in front of hisface. That means they are both falling together andare not moving with respect to each other. Hence theirvelocities are the same.   Now let's see what happens when he releases them.  To do so we will first find the individual forcesexerted on the man and his keys, then both net forces,both accelerations, and finally we will describe andcompare the velocities.   So what are the forces exerted on the man after hereleases his keys? Please, specify their directions. User answer2: gravity vertically down [correct] TUTOR3: Good. Because there is just one force, grav-ity's, it is trivial to determine the NET force. So whatis the direction of the NET force on the person? User answer3: down [correct] TUTOR4: Ok. Considering the net force, does the per-son have an acceleration? If yes, specify its direction.User answer4: the man has no acceleration [incorrect] TUTOR5: Well… To analyze the man's accelera-tion we will use Newton's second law. What isthe definition of this law? Problem: Suppose a man is in a free-falling elevator and is holding his keys motionless right in front of his face.  He then lets go. What will be the position of the keys relative to the man's face as time passes?  Explain. 12345678910111213141516171819Figure 1. Transcript of a sample ITSPOKE speech interaction (left). The NM as the user sees it after turn Tutor5363

Limited horizon. Since in our case the system drives the conversation (i.e. system initiative), we always know what questions would be discussed next. We hypothesized that by having access to this information, users will have a better idea of where instruction is heading, thus facilitating their understanding of the relevance of the current topic to the overall discussion. To prevent information overload, we only display the next discourse seg-ment purpose at each level in the hierarchy (see Figure 1, NM14, NM16, NM17 and NM19; Figure 2, NM5); additional discourse segments at the same level are signaled through a dotted line. To avoid helping the students answer the current question in cases when the next discourse segment hints/de-scribes the answer, each discourse segment has an additional purpose annotation that is displayed when the segment is part of the visible horizon.  Auto-collapse. To reduce the amount of infor-mation on the screen, discourse segments dis-cussed in the past are automatically collapsed by the system. For example, in Figure 1, NM Line 3 is collapsed in the actual system and Lines 4 and 5 are hidden (shown in Figure1 to illustrate our dis-course structure annotation.). The user can expand nodes as desired using the mouse. Information highlight. Bold and italics font were used to highlight important information (what and when to highlight was manually annotated). For example, in Figure 1, NM2 highlights the two time frames as they are key steps in approaching this problem. Correct answers are also highlighted. We would like to reiterate that the goal of this study is to investigate if making certain types of discourse information explicitly available to the user provides any benefits. Thus, whether we have made the optimal design choices is of secondary importance. While, we believe that our annotation is relatively robust as the system questions follow a carefully designed tutoring plan, in the future we would like to investigate these issues. 4 User Study We designed a user study focused primarily on user’s perception of the NM presence/absence. We used a within-subject design where each user re-ceived instruction both with and without the NM. Each user went through the same experimental procedure: 1) read a short document of background material, 2) took a pretest to measure initial phys-ics knowledge, 3) worked through 2 problems with ITSPOKE 4) took a posttest similar to the pretest, 5) took a NM survey, and 6) went through a brief open-question interview with the experimenter. In the 3rd step, the NM was enabled in only one problem. Note that in both problems, users did not have access to the system turn transcript. After each problem users filled in a system question-naire in which they rated the system on various dimensions; these ratings were designed to cover dimensions the NM might affect (see Section 5.1). While the system questionnaire implicitly probed the NM utility, the NM survey from the 5th step explicitly asked the users whether the NM was use-ful and on what dimensions (see Section 5.1) To account for the effect of the tutored problem on the user’s questionnaire ratings, users were ran-domly assigned to one of two conditions. The users in the first condition (F) had the NM enabled in the first problem and disabled in the second problem, while users in the second condition (S) had the op-posite. Thus, if the NM has any effect on the user’s perception of the system, we should see a decrease in the questionnaire ratings from problem 1 to problem 2 for F users and an increase for S users. Other factors can also influence our measure-ments. To reduce the effect of the text-to-speech component, we used a version of the system with human prerecorded prompts. We also had to ac-count for the amount of instruction as in our sys-tem the top level question segment is tailored to what users write in the essay. Thus the essay analysis component was disabled; for all users, the system started with the same top level question segment which assumed no information in the es-say. Note that the actual dialogue depends on the correctness of the user answers. After the dialogue, users were asked to revise their essay and then the system moved on to the next problem. The collected corpus comes from 28 users (13 in F and 15 in S). The conditions were balanced for gender (F: 6 male, 7 female; S: 8 male, 7 female). There was no significant differences between the two conditions in terms of pretest (p<0.63); in both conditions users learned (significant difference between pretest and posttest, p<0.01). 5 Results 5.1 Subjective metrics Our main resource for investigating the effect of the NM was the system questionnaires given after 364

each problem. These questionnaires are identical and include 16 questions that probed user’s percep-tion of ITSPOKE on various dimensions. Users were asked to answer the questions on a scale from 1-5 (1 – Strongly Disagree, 2 – Disagree, 3 – Somewhat Agree, 4 – Agree, 5 – Strongly Agree). If indeed the NM has any effect we should observe differences between the ratings of the NM problem and the noNM problem (i.e. the NM is disabled). Table 1 lists the 16 questions in the question-naire order. The table shows for every question the average rating for all condition-problem combina-tions (e.g. column 5: condition F problem 1 with the NM enabled). For all questions except Q7 and Q11 a higher rating is better. For Q7 and Q11 (italicized in Table 1) a lower rating is better as they gauge negative factors (high level of concen-tration and task disorientation). They also served as a deterrent for negligence while rating. To test if the NM presence has a significant ef-fect, a repeated-measure ANOVA with between-subjects factors was applied. The within-subjects factor was the NM presence (NMPres) and the between-subjects factor was the condition (Cond)1. The significance of the effect of each factor and their combination (NMPres*Cond) is listed in the table with significant and trend effects highlighted in bold (see columns 2-4). Post-hoc t-tests between the NM and noNM ratings were run for each con-dition (“s”/“t”marks significant/trend differences). Results for Q1-6 Questions Q1-6 were inspired by previous work on spoken dialogue system evaluation (e.g. (Walker et al., 2000)) and measure user’s overall perception of the system. We find that the NM presence significantly improves user’s perception of the system in terms of their ability to concen-trate on the instruction (Q3), in terms of their incli-nation to reuse the system (Q6) and in terms of the system’s matching of their expectations (Q4). There is a trend that it was easier for them to learn from the NM enabled version of the system (Q2). Results for Q7-13 Q7-13 relate directly to our hypothesis that users                                                  1 Since in this version of ANOVA the NM/noNM rat-ings come from two different problems based on the condition, we also run an ANOVA in which the within-subjects factor was the problem (Prob). In this case, the NM effect corresponds to an effect from Prob*Cond which is identical in significance with that of NMPres. benefit from access to the discourse structure in-formation. These questions probe the user’s per-ception of ITSPOKE during the dialogue. We find that for 6 out 7 questions the NM presence has a significant/trend effect (Table 1, column 2).  Structure. Users perceive the system as having a structured tutoring plan significantly2 more in the NM problems (Q8). Moreover, it is significantly easier for them to follow this tutoring plan if the NM is present (Q11). These effects are very clear for F users where their ratings differ significantly between the first (NM) and the second problem (noNM). A difference in ratings is present for S users but it is not significant. As with most of the S users’ ratings, we believe that the NM presentation order is responsible for the mostly non-significant differences. More specifically, assuming that the NM has a positive effect, the S users are asked to rate first the poorer version of the system (noNM) and then the better version (NM). In contrast, F users’ task is easier as they already have a high reference point (NM) and it is easier for them to criticize the second problem (noNM). Other factors that can blur the effect of the NM are domain learning and user’s adaptation to the system. Integration. Q9 and Q10 look at how well users think they integrate the system questions in both a forward-looking fashion (Q9) and a backward looking fashion (Q10). Users think that it is sig-nificantly easier for them to integrate the current system question to what will be discussed in the future if the NM is present (Q9). Also, if the NM is present, it is easier for users to integrate the current question to the discussion so far (Q10, trend). For Q10, there is no difference for F users but a sig-nificant one for S users. We hypothesize that do-main learning is involved here: F users learn better from the first problem (NM) and thus have less issues solving the second problem (noNM). In con-trast, S users have more difficulties in the first problem (noNM), but the presence of the NM eases their task in the second problem. Correctness. The correct answer NM feature is useful for users too. There is a trend that it is easier for users to know the correct answer if the NM is present (Q13). We hypothesize that speech recog-nition and language understanding errors are re-                                                 2 We refer to the significance of the NMPres factor (Ta-ble 1, column 2). When discussing individual experi-mental conditions, we refer to the post-hoc t-tests. 365

sponsible for the non-significant NM effect on the dimension captured by Q12. Concentration. Users also think that the NM enabled version of the system requires less effort in terms of concentration (Q7). We believe that hav-ing the discourse segment purpose as visual input allows the users to concentrate more easily on what the system is uttering. In many of the open ques-tion interviews users stated that it was easier for them to listen to the system when they had the dis-course segment purpose displayed on the screen. Results for Q14-16 Questions Q14-16 were included to probe user’s post tutoring perceptions. We find a trend that in the NM problems it was easier for users to under-stand the system’s main point (Q14). However, in terms of identifying (Q15) and correcting (Q16) problems in their essay the results are inconclusive. We believe that this is due to the fact that the essay interpretation component was disabled in this ex-periment. As a result, the instruction did not match the initial essay quality. Nonetheless, in the open-question interviews, many users indicated using the NM as a reference while updating their essay. In addition to the 16 questions, in the system questionnaire after the second problem users were asked to choose which version of the system they preferred the most (i.e. the first or the second prob-lem version). 24 out 28 users (86%) preferred the NM enabled version. In the open-question inter-view, the 4 users that preferred the noNM version (2 in each condition) indicated that it was harder for them to concurrently concentrate on the audio and the visual input (divided attention problem) and/or that the NM was changing too fast. To further strengthen our conclusions from the system questionnaire analysis, we would like to note that users were not asked to directly compare the two versions but they were asked to individu-ally rate two versions which is a noisier process (e.g. users need to recall their previous ratings). The NM survey While the system questionnaires probed users’ NM usage indirectly, in the second to last step in the experiments, users had to fill a NM survey Table 1. System questionnaire results QuestionOverallNMPresCondNMPres*Cond1. The tutor increased my understanding of the subject0.5180.8980.8624.0>3.94.0>3.92. It was easy to learn from the tutor0.1000.8130.9473.9>3.63.9>3.53. The tutor helped me to concentrate0.0160.1560.8543.5>3.03.9>t3.44. The tutor worked the way I expected it to0.0340.8860.1573.5>3.43.9>s3.15. I enjoyed working with the tutor0.1540.5130.9173.5>3.23.7>3.46. Based on my experience using the tutor to learn physics, I would like to use such a tutor regularly0.0040.6930.9883.7>s3.23.5>s3.0During the conversation with the tutor:7. ... a high level of concentration is required to follow the tutor0.0040.5340.5453.5<s4.23.9<t4.38. ... the tutor had a clear and structured agenda behind its explanations0.0080.3400.1044.4>s3.64.3>4.19. ... it was easy to figure out where the tutor's instruction was leading me0.0170.4720.5934.0>s3.44.1>3.710. ... when the tutor asked me a question I knew why it was asking me that question 0.0540.1910.0543.5~3.54.3>s3.511. ... it was easy to loose track of where I was in the interaction with the tutor 0.0120.7660.0482.5<s3.52.9<3.012. ... I knew whether my answer to the tutor's question was correct or incorrect0.3580.6350.8043.5>3.33.7>3.413. ... whenever I answered incorrectly, it was easy to know the correct answer after the tutor corrected me0.0850.0440.8173.8>3.54.3>3.9At the end of the conversation with the tutor:14. ... it was easy to understand the tutor's main point0.0710.0560.8944.0>3.64.4>4.115. ... I knew what was wrong or missing from my essay0.3400.9650.3403.9~3.93.7<4.016. ... I knew how to modify my essay0.7910.4780.3274.1>3.93.7<3.8P1       P2NM     noNMP2       P1NM     noNMAverage ratingANOVAF conditionS condition366

which explicitly asked how the NM helped them, if at all. The answers were on the same 1 to 5 scale. We find that the majority of users (75%-86%) agreed or strongly agreed that the NM helped them follow the dialogue, learn more easily, concentrate and update the essay. These findings are on par with those from the system questionnaire analysis. 5.2 Objective metrics Our analysis of the subjective user evaluations shows that users think that the NM is helpful. We would like to see if this perceived usefulness is reflected in any objective metrics of performance. Due to how our experiment was designed, the ef-fect of the NM can be reliably measured only in the first problem as in the second problem the NM is toggled3; for the same reason, we can not use the pretest/posttest information. Our preliminary investigation4 found several dimensions on which the two conditions differed in the first problem (F users had NM, S users did not). We find that if the NM was present the inter-action was shorter on average and users gave more correct answers; however these differences are not statistically significant (Table 2). In terms of speech recognition performance, we looked at two metrics: AsrMis and SemMis (ASR/Semantic Mis-recognition). A user turn is labeled as AsrMis if the output of the speech recognition is different from the human transcript (i.e. a binary version of Word Error Rate). SemMis are AsrMis that change the correctness interpretation. We find that if the NM was present users had fewer AsrMis and fewer SemMis (trend for SemMis, p<0.09). In addition, a χ2 dependency analysis showed that the NM presence interacts significantly with both AsrMis (p<0.02) and SemMis (p<0.001), with fewer than expected AsrMis and SemMis in the                                                  3 Due to random assignment to conditions, before the first problem the F and S populations are similar (e.g. no difference in pretest); thus any differences in metrics can be attributed to the NM presence/absence. However, in the second problem, the two populations are not simi-lar anymore as they have received different forms of instruction; thus any difference has to be attributed to the NM presence/absence in this problem as well as to the NM absence/presence in the previous problem.  4 Due to logging issues, 2 S users are excluded from this analysis (13 F and 13 S users remaining). We run the subjective metric analysis from Section 5.1 on this sub-set and the results are similar. NM condition. The fact that in the second problem the differences are much smaller (e.g. 2% for AsrMis) and that the NM-AsrMis and NM-SemMis interactions are not significant anymore, suggests that our observations can not be attributed to a difference in population with respect to sys-tem’s ability to recognize their speech. We hy-pothesize that these differences are due to the NM text influencing users’ lexical choice. Metric F (NM) S (noNM) p # user turns 21.8 (5.3) 22.8 (6.5) 0.65 % correct turns 72% (18%) 67% (22%) 0.59 AsrMis 37% (27%) 46% (28%) 0.46 SemMis 5% (6%) 12% (14%) 0.09 Table 2. Average (standard deviation) for  objective metrics in the first problem 6 Related work Discourse structure has been successfully used in non-interactive settings (e.g. understanding spe-cific lexical and prosodic phenomena (Hirschberg and Nakatani, 1996) , natural language generation (Hovy, 1993), essay scoring (Higgins et al., 2004) as well as in interactive settings (e.g. predic-tive/generative models of postural shifts (Cassell et al., 2001), generation/interpretation of anaphoric expressions (Allen et al., 2001), performance mod-eling (Rotaru and Litman, 2006)). In this paper, we study the utility of the dis-course structure on the user side of a dialogue sys-tem. One related study is that of (Rich and Sidner, 1998). Similar to the NM, they use the discourse structure information to display a segmented inter-action history (SIH): an indented view of the inter-action augmented with purpose information. This paper extends over their work in several areas. The most salient difference is that here we investigate the benefits of displaying the discourse structure information for the users. In contrast, (Rich and Sidner, 1998) never test the utility of the SIH. Their system uses a GUI-based interaction (no speech/text input, no speech output) while we look at a speech-based system. Also, their underlying task (air travel domain) is much simpler than our tutoring task. In addition, the SIH is not always available and users have to activate it manually. Other visual improvements for dialogue-based computer tutors have been explored in the past (e.g. talking heads (Graesser et al., 2003)). How-ever, implementing the NM in a new domain re-quires little expertise as previous work has shown 367

that naïve users can reliably annotate the informa-tion needed for the NM (Passonneau and Litman, 1993). Our NM design choices should also have an equivalent in a new domain (e.g. displaying the recognized user answer can be the equivalent of the correct answers). Other NM usages can also be imagined: e.g. reducing the length of the system turns by removing text information that is implic-itly represented in the NM. 7 Conclusions & Future work In this paper we explore the utility of the Naviga-tion Map, a graphical representation of the dis-course structure. As our first step towards under-standing the benefits of the NM, we ran a user study to investigate if users perceive the NM as useful. From the users’ perspective, the NM pres-ence allows them to better identify and follow the tutoring plan and to better integrate the instruction. It was also easier for users to concentrate and to learn from the system if the NM was present. Our preliminary analysis on objective metrics shows that users’ preference for the NM version is re-flected in more correct user answers and less speech recognition problems in the NM version. These findings motivate future work in under-standing the effects of the NM. We would like to continue our objective metrics analysis (e.g. see if users are better in the NM condition at updating their essay and at answering questions that require combining facts previously discussed). We also plan to run an additional user study with a be-tween-subjects experimental design geared towards objective metrics. The experiment will have two conditions: NM present/absent for all problems. The conditions will then be compared in terms of various objective metrics. We would also like to know which information sources represented in the NM (e.g. discourse segment purpose, limited hori-zon, correct answers) has the biggest impact. Acknowledgements This work is supported by NSF Grants 0328431 and 0428472. We would like to thank Shimei Pan, Pamela Jordan and the ITSPOKE group. References K. Acomb, J. Bloom, K. Dayanidhi, P. Hunter, P. Krogh, E. Levin and R. Pieraccini. 2007. Technical Support Dialog Systems: Issues, Problems, and Solu-tions. In Proc. of Workshop on Bridging the Gap: Academic and Industrial Research in Dialog Technologies. J. Allen, G. Ferguson, B. N., D. Byron, N. Chambers, M. Dzikovska, L. Galescu and M. Swift. 2006. Ches-ter: Towards a Personal Medication Advisor. Journal of Biomedical Informatics, 39(5). J. Allen, G. Ferguson and A. Stent. 2001. An architec-ture for more realistic conversational systems. In Proc. of Intelligent User Interfaces. J. Cassell, Y. I. Nakano, T. W. Bickmore, C. L. Sidner and C. Rich. 2001. Non-Verbal Cues for Discourse Structure. In Proc. of ACL. A. Graesser, K. Moreno, J. Marineau, A. Adcock, A. Olney and N. Person. 2003. AutoTutor improves deep learning of computer literacy: Is it the dialog or the talking head? In Proc. of Artificial Intelligence in Education (AIED). B. Grosz and C. L. Sidner. 1986. Attentions, intentions and the structure of discourse. Computational Lin-guistics, 12(3). D. Higgins, J. Burstein, D. Marcu and C. Gentile. 2004. Evaluating Multiple Aspects of Coherence in Student Essays. In Proc. of HLT-NAACL. J. Hirschberg and C. Nakatani. 1996. A prosodic analy-sis of discourse segments in direction-giving mono-logues. In Proc. of ACL. E. Hovy. 1993. Automated discourse generation using discourse structure relations. Articial Intelligence, 63(Special Issue on NLP). D. Litman and S. Silliman. 2004. ITSPOKE: An intelli-gent tutoring spoken dialogue system. In Proc. of HLT/NAACL. S. Oviatt, R. Coulston and R. Lunsford. 2004. When Do We Interact Multimodally? Cognitive Load and Mul-timodal Communication Patterns. In Proc. of Interna-tional Conference on Multimodal Interfaces. R. Passonneau and D. Litman. 1993. Intention-based segmentation: Human reliability and correlation with linguistic cues. In Proc. of ACL. H. Pon-Barry, K. Schultz, E. O. Bratt, B. Clark and S. Peters. 2006. Responding to Student Uncertainty in Spoken Tutorial Dialogue Systems. International Journal of Artificial Intelligence in Education, 16. C. Rich and C. L. Sidner. 1998. COLLAGEN: A Col-laboration Manager for Software Interface Agents. User Modeling and User-Adapted Interaction, 8(3-4). M. Rotaru and D. Litman. 2006. Exploiting Discourse Structure for Spoken Dialogue Performance Analy-sis. In Proc. of EMNLP. M. Walker, D. Litman, C. Kamm and A. Abella. 2000. Towards Developing General Models of Usability with PARADISE. Natural Language Engineering. Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 368–375,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

368

AutomatedVocabularyAcquisitionandInterpretationinMultimodalConversationalSystemsYiLiuJoyceY.ChaiRongJinDepartmentofComputerScienceandEngineeringMichiganStateUniversityEastLansing,MI48824,USA{liuyi3,jchai,rongjin}@cse.msu.eduAbstractMotivatedbypsycholinguisticﬁndingsthateyegazeistightlylinkedtohumanlan-guageproduction,wedevelopedanunsuper-visedapproachbasedontranslationmodelstoautomaticallylearnthemappingsbetweenwordsandobjectsonagraphicdisplaydur-inghumanmachineconversation.Theex-perimentalresultsindicatethatusereyegazecanprovideusefulinformationtoestablishsuchmappings,whichhaveimportantimpli-cationsinautomaticallyacquiringandinter-pretinguservocabulariesforconversationalsystems.1IntroductionTofacilitateeffectivehumanmachineconversation,itisimportantforaconversationalsystemtohaveknowledgeaboutuservocabulariesandunderstandhowthesevocabulariesaremappedtotheinternalentitiesforwhichthesystemhasrepresentations.Forexample,inamultimodalconversationalsystemthatallowsuserstoconversewithagraphicinter-face,thesystemneedstoknowwhatvocabulariesuserstendtousetodescribeobjectsonthegraphicdisplayandwhat(typeof)object(s)auserisattend-ingtowhenaparticularwordisexpressed.Here,weuseacquisitiontorefertotheprocessofacquir-ingrelevantvocabulariesdescribinginternalentities,andinterpretationtorefertotheprocessofautomat-icallyidentifyinginternalentitiesgivenaparticularword.Bothacquisitionandinterpretationhavebeentraditionallyapproachedbyeitherknowledgeengi-neering(e.g.,manuallycreatedlexicons)orsuper-visedlearningfromannotateddata.Inthispaper,wedescribeanunsupervisedapproachthatreliesonnaturallyco-occurredeyegazeandspokenutter-ancesduringhumanmachineconversationtoauto-maticallyacquireandinterpretvocabularies.Motivatedbypsycholinguisticstudies(JustandCarpenter,1976;GrifﬁnandBock,2000;Tenenhausetal.,1995)andrecentinvestigationsoncomputa-tionalmodelsforlanguageacquisitionandground-ing(Siskind,1995;RoyandPentland,2002;YuandBallard,2004),weareparticularlyinterestedintwouniquequestionsrelatedtomultimodalconver-sationalsystems:(1)Inamultimodalconversationthatinvolvesmorecomplextasks(e.g.,bothuserinitiatedtasksandsysteminitiatedtasks),isthereareliabletemporalalignmentbetweeneyegazeandspokenreferencessothatthecoupledinputscanbeusedforautomatedvocabularyacquisitionandinter-pretation?(2)Ifsuchanalignmentexists,howcanwemodelthisalignmentandautomaticallyacquireandinterpretthevocabularies?Toaddresstheﬁrstquestion,weconductedanempiricalstudytoexaminethetemporalrelation-shipsbetweeneyeﬁxationsandtheircorrespond-ingspokenreferences.Asshownlaterinsection4,althoughalargervariance(comparedtotheﬁnd-ingsfrompsycholinguisticstudies)existsintermsofhoweyegazeislinkedtospeechproductionduringhumanmachineconversation,eyeﬁxationsandthecorrespondingspokenreferencesstilloccurinaveryclosevicinitytoeachother.Thisnaturalcouplingbetweeneyegazeandspeechprovidesanopportu-nitytoautomaticallylearnthemappingsbetween369

wordsandobjectswithoutanyhumansupervision.Becauseofthelargervariance,itisdifﬁculttoapplyrule-basedapproachestoquantifythisalign-ment.Therefore,toaddressthesecondquestion,wedevelopedanapproachbasedonstatisticaltrans-lationmodelstoexploretheco-occurrencepatternsbetweeneyeﬁxatedobjectsandspokenreferences.Ourpreliminaryexperimentresultsindicatethatthetranslationmodelcanreliablycapturethemappingsbetweentheeyeﬁxatedobjectsandthecorrespond-ingspokenreferences.Givenanobject,thismodelcanprovidepossiblewordsdescribingthisobject,whichrepresentstheacquisitionprocess;givenaword,thismodelcanalsoprovidepossibleobjectsthatarelikelytobedescribed,whichrepresentstheinterpretationprocess.Inthefollowingsections,weﬁrstreviewsomere-latedworkandintroducetheproceduresusedtocol-lecteyegazeandspeechdataduringhumanmachineconversation.Wethendescribeourempiricalstudyandtheunsupervisedapproachbasedontranslationmodels.Finally,wepresentexperimentresultsanddiscusstheirimplicationsinnaturallanguagepro-cessingapplications.2RelatedWorkOurworkismotivatedbypreviousworkinthefol-lowingthreeareas:psycholinguisticsstudies,multi-modalinteractivesystems,andcomputationalmod-elingoflanguageacquisitionandgrounding.Previouspsycholinguisticsstudieshaveshownthatthedirectionofgazecarriesinformationaboutthefocusoftheuser’sattention(JustandCarpenter,1976).Speciﬁcally,inhumanlanguageprocessingtasks,eyegazeistightlylinkedtolanguageproduc-tion.Theperceivedvisualcontextinﬂuencesspo-kenwordrecognitionandmediatessyntacticpro-cessing(Tenenhausetal.,1995).Additionally,be-forespeakingaword,theeyesusuallymovetotheobjectstobementioned(GrifﬁnandBock,2000).Thesepsycholinguisticsﬁndingshaveprovidedafoundationforourinvestigation.Inresearchonmultimodalinteractivesystems,re-centworkindicatesthatthespeechandgazeinte-grationpatternscanbemodeledreliablyforindi-vidualusersandthereforebeusedtoimprovemul-timodalsystemperformances(Kauretal.,2003).Studieshavealsoshownthateyegazehasapoten-tialtoimproveresolutionofunderspeciﬁedreferringexpressionsinspokendialogsystems(Campanaetal.,2001)andtodisambiguatespeechinput(Tanaka,1999).Incontrasttotheseearlierstudies,ourworkfocusesonadifferentgoalofusingeyegazeforau-tomatedvocabularyacquisitionandinterpretation.Thethirdareaofresearchthatinﬂuencedourworkiscomputationalmodelingoflanguageacqui-sitionandgrounding.Recentstudieshaveshownthatmultisensoryinformation(e.g.,throughvisionandlanguageprocessing)canbecombinedtoeffec-tivelyacquirewordstotheirperceptuallygroundedobjectsintheenvironment(Siskind,1995;RoyandPentland,2002;YuandBallard,2004).Especiallyin(YuandBallard,2004),anunsupervisedapproachbasedonagenerativecorrespondencemodelwasdevelopedtocapturethemappingbetweenspokenwordsandtheoccurringperceptualfeaturesofob-jects.Thisapproachismostsimilartothetransla-tionmodelusedinourwork.However,comparedtothisworkwheremultisensoryinformationcomesfromvisionandlanguageprocessing,ourworkfo-cusesonadifferentaspect.Here,insteadofapplyingvisionprocessingonobjects,weareinterestedineyegazebehaviorwhenusersinteractwithagraphicdis-play.Eyegazeisanimplicitandsubconsciousinputmodalityduringhumanmachineinteraction.Eyegazedatainevitablycontainasigniﬁcantamountofnoise.Therefore,itisthegoalofthispapertoexam-inewhetherthismodalitycanbeutilizedforvocab-ularyacquisitionforconversationalsystems.3DataCollectionWeusedasimpliﬁedmultimodalconversationalsys-temtocollectsynchronizedspeechandeyegazedata.Aroominteriorscenewasdisplayedonacom-puterscreen,asshowninFigure1.Whilewatchingthegraphicaldisplay,userswereaskedtocommuni-catewiththesystemontopicsabouttheroomdec-orations.Atotalof28objects(e.g.,multiplelampsandpictureframes,abed,twochairs,acandle,adresser,etc.,asmarkedinFigure1)areexplicitlymodeledinthisscene.Thesystemissimpliﬁedinthesensethatitonlysupports14tasksduringhumanmachineinteraction.Thesetasksaredesignedtocoverbothopen-endedutterances(e.g.,thesystem370

Figure1:Theroominteriorsceneforuserstudies.Foreasyreference,wegiveeachobjectanID.TheseIDsarehiddenfromthesystemusers.asksuserstodescribetheroom)andmorerestrictedutterances(e.g.,thesystemaskstheuserwhetherhe/shelikesthebed)thatarecommonlysupportedinconversationalsystems.Sevenhumansubjectspar-ticipatedinourstudy.UserspeechinputswererecordedusingtheAu-dacitysoftware1,witheachutterancetime-stamped.EyemovementswererecordedusinganEyeLinkIIeyetrackersampledat250Hz.Theeyetrackerau-tomaticallysavedtwo-dimensionalcoordinatesofauser’seyeﬁxationsaswellasthetime-stampswhentheﬁxationsoccurred.Thecollectedrawgazedataisextremelynoisy.Toreﬁnethegazedata,wefurthereliminatedin-validandsaccadicgazepoints(knownas“saccadicsuppression”invisionstudies).Sinceeyesdonotstaystillbutrathermakesmall,frequentjerkymove-ments,wealsosmoothedthedatabyaveragingnearbygazelocationstoidentifyﬁxations.4EmpiricalStudyonSpeech-GazeAlignmentBasedonthedatacollected,weinvestigatedthetem-poralalignmentbetweenco-occurredeyegazeandspokenutterances.Inparticular,weexaminedthetemporalalignmentbetweeneyegazeﬁxationsandthecorrespondingspokenreferences(i.e.,thespo-kenwordsthatareusedtorefertotheobjectsonthegraphicdisplay).Accordingtothetime-stampinformation,wecan1http://audacity.sourceforge.net/measurethelengthoftimegapbetweenauser’seyeﬁxationfallingonanobjectandthecorrespondingspokenreferencebeinguttered(whichwerefertoas“lengthoftimegap”forbrevity).Also,wecancountthenumberoftimesthatuserﬁxationshap-pentochangetheirtargetobjectsduringthistimegap(whichwerefertoas“numberofﬁxatedobjectchanges”forbrevity).Theninemostfrequentlyoc-curredspokenreferencesinutterancesfromallusers(asshowninTable1)arechosenforthisempiricalstudy.Foreachofthosespokenreferences,weusehumanjudgmenttodecidewhichobjectisreferredto.Then,frombothbeforeandaftertheonsetofthespokenreference,weﬁndtheclosestoccurrenceoftheﬁxationfallingonthatparticularobject.Al-togetherwehave96suchspeech-gazepairs.In54pairs,theeyegazeﬁxationoccurredbeforethecor-respondingspeechreferencewasuttered;andintheother42pairs,theeyeﬁxationoccurredafterthecorrespondingspeechreferencewasuttered.Thisobservationsuggeststhatinhumanmachineconver-sation,eyeﬁxationonanobjectdoesnotnecessarilyalwaysproceedtheutteranceofthecorrespondingspeechreference.Further,wecomputedtheaverageabsolutelengthofthetimegapandtheaveragenumberofﬁxatedobjectchanges,aswellastheirvariancesforeachof5selectedusers2asshowninTable1.FromTable1,itiseasytoobservethat:(I)Aspokenreferenceal-waysappearswithinashortperiodoftime(usually1-2seconds)beforeorafterthecorrespondingeyegazeﬁxation.But,theexactlengthoftheperiodisfarfromconstant.(II)Itisnotnecessaryforausertoutterthecorrespondingspokenreferenceimme-diatelybeforeoraftertheeyegazeﬁxationfallsonthatparticularobject.Eyegazeﬁxationsmaymovebackandforth.Betweenthetimeanobjectisﬁxatedandthecorrespondingspokenreferenceisuttered,auser’seyegazemayﬁxateonafewotherobjects(reﬂectedbytheaveragenumberofeyeﬁxatedob-jectchangesshowninthetable).(III)Thereisalargevarianceinboththelengthoftimegapandthenumberofﬁxatedobjectchangesintermsof1)thesameuserandthesamespokenreferenceatdiffer-enttime-stamps,2)thesameuserbutdifferentspo-2Theothertwousersarenotselectedbecausetheninese-lectedwordsdonotappearfrequentlyintheirutterances.371

SpokenAverageAbsoluteLengthofTimeGap(inseconds)AverageNumberofEyeFixatedObjectChangesReferenceUser1User2User3User4User5User1User2User3User4User5bed1.27±1.401.02±0.650.32±0.210.59±0.772.57±3.252.1±3.22.1±2.20.4±0.51.4±2.25.3±7.9tree-0.24±0.24----0.0±0.0---window-0.67±0.74--1.95±3.20-0.0±0.0--3.3±5.9mirror-1.04±1.36----1.0±1.4---candle--3.64±0.59----8.5±2.1--waterfall1.80±1.12----5.5±4.9----painting0.10±0.10----0.2±0.4----lamp0.74±0.541.70±0.990.26±0.351.98±1.722.84±2.421.3±1.31.8±1.50.3±0.64.8±4.32.7±2.2door2.47±0.84--2.49±1.906.36±2.295.0±2.6--6.7±5.513.3±6.7Table1:Theaverageabsolutelengthoftimeandthenumberofeyeﬁxatedobjectchangeswithinthetimegapofeyegazeandcorrespondingspokenreferences.Variancesarealsolisted.Someoftheentriesarenotavailablebecausethespokenreferenceswereneverorrarelyusedbythecorrespondingusers.kenreferences,and3)thesamespokenreferencebutdifferentusers.Webelievethisisduetothedifferentdialogscenariosanduserlanguagehabits.Tosummarizeourempiricalstudy,weﬁndthatinhumanmachineconversation,therestillexistsanaturaltemporalcouplingbetweenuserspeechandeyegaze,i.e.thespokenreferenceandthecorre-spondingeyeﬁxationhappenwithinaclosevicinityofeachother.However,alargevarianceisalsoob-servedintermsofthesetemporalvicinities,whichindicatesanintrinsicallymorecomplexgaze-speechpattern.Therefore,itishardtodirectlyquantifythetemporalororderingrelationshipbetweenspo-kenreferencesandcorrespondingeyeﬁxatedobjects(forexample,throughrules).Tobetterhandlethecomplexityinthegaze-speechpattern,weproposetousestatisticaltransla-tionmodels.Givenatimewindowofenoughlength,aspeechinputthatcontainsalistofspokenrefer-ences(e.g.,deﬁnitenounphrases)isalwaysaccom-paniedbyalistofnaturallyoccurredeyeﬁxationsandthereforealistofobjectsreceivingthoseﬁxa-tions.Allthosepairsofspeechreferencesandcor-respondingﬁxatedobjectscouldbeviewedasparal-lel,i.e.theyco-occurwithinthetimewindow.Thissituationisverysimilartothetrainingprocessoftranslationmodelsinstatisticalmachinetranslation(Brownetal.,1993),whereparallelcorpusisusedtoﬁndthemappingsbetweenwordsfromdifferentlan-guagesbyexploitingtheirco-occurrencepatterns.Thesameideacanbeborrowedhere:byexploringtheco-occurrencestatistics,wehopetouncovertheexactmappingbetweenthoseeyeﬁxatedobjectsandspokenreferences.Theintuitionisthat,themoreof-tenaﬁxationisfoundtoexclusivelyco-occurwithaspokenreference,themorelikelyamappingshouldbeestablishedbetweenthem.5TranslationModelsforVocabularyAcquisitionandInterpretationFormally,wedenotethesetofobservationsbyD={wi,oi}Ni=1wherewiandoireferstothei-thspeechutterance(i.e.,alistofwordsofspokenreferences)andthei-thcorrespondingeyegazepattern(i.e.,alistofeyeﬁxatedob-jects)respectively.Whenwestudytheprob-lemofmappinggivenobjectstowords(forvo-cabularyacquisition),theparameterspaceΘ={Pr(wj|ok),1≤j≤mw,1≤k≤mo}consistsofthemappingprobabilitiesofanarbitrarywordwjtoanarbitraryobjectok,wheremwandmorepre-sentthetotalnumberofuniquewordsandobjectsrespectively.Thosemappingprobabilitiesaresub-jecttoconstraintsPmwj=1Pr(wj|ok)=1.NotethatPr(wj|ok)=0ifthecorrespondingwordwjandokneverco-occurinanyobservedlistpair(wi,oi).Letlwiandloidenotethelengthoflistswiandoirespectively.Todistinguishwiththenotationswjandokwhosesubscriptsareindicesforuniquewordsandobjectsrespectively,weuse˜wi,jtode-notethewordinthej-thpositionofthelistwiand˜oi,ktodenotetheobjectinthek-thpositionofthelistoi.Intranslationmodels,weassumethatanywordinthelistwiismappedtoanobjectinthecor-respondinglistoioranullobject(wereservetheposition0foritineveryobjectlist).Todenotealltheword-objectmappingsinthei-thlistpair,wein-troduceanalignmentvectorai,whoseelementai,jtakesthevaluekiftheword˜wi,jismappedto˜oi,k.Then,thelikelihoodoftheobservationsgiventhe372

parameterscanbecomputedasfollowsPr(D;Θ)=NYi=1Pr(wi|oi)=NYi=1XaiPr(wi,ai|oi)=NYi=1XaiPr(lwi|oi)(loi+1)lwilwiYj=1Pr(˜wi,j|˜oai,j)=NYi=1Pr(lwi|oi)(loi+1)lwiXailwiYj=1Pr(˜wi,j|˜oai,j)Notethatthefollowingequationholds:lwiYj=1loiXk=0Pr(˜wi,j|˜oi,k)=loiXai,1=1···loiXai,lwi=1lwiYj=1Pr(˜wi,j|˜oai,j)wheretheright-handsideisactuallytheexpansionofPaiQlwijPr(˜wi,j|˜oai,j).Therefore,thelikelihoodcanbesimpliﬁedasPr(D;Θ)=NYi=1Pr(lwi|oi)(loi+1)lwilwiYj=1loiXk=0Pr(˜wi,j|˜oi,k)Switchingtothenotationswjandok,wehavePr(D;Θ)=NYi=1Pr(lwi|oi)(loi+1)lwimwYj=1"moXk=0Pr(wj|ok)δoi,k#δwi,jwhereδwi,j=1if˜wi,j∈wiandδwi,j=0otherwise,andδoi,k=1if˜oi,k∈oiandδoi,k=0otherwise.Finally,thetranslationmodelcanbeformalizedasthefollowingoptimizationproblemargmaxΘlogPr(D;Θ)s.t.mwXj=1Pr(wj|ok)=1,∀kThisoptimizationproblemcanbesolvedbytheEMalgorithm(Brownetal.,1993).Theabovemodelisdevelopedinthecon-textofmappinggivenobjectstowords,i.e.,itssolutionyieldsasetofconditionalprobabilities{Pr(wj|ok),∀j}foreachobjectok,indicatinghowlikelyeverywordismappedtoit.Similarly,wecandevelopthemodelinthecontextofmappinggivenwordstoobjects(forvocabularyinterpreta-tion),whosesolutionleadstoanothersetofprob-abilities{Pr(ok|wj),∀k}foreachwordwjindicat-inghowlikelyeveryobjectismappedtoit.Inourexperiments,bothmodelsareimplementedandwewillpresenttheresultslater.6ExperimentsWeexperimentedourproposedstatisticaltranslationmodelonthecollecteddatamentionedinSection3.6.1PreprocessingThemainpurposeofpreprocessingistocreatea“parallelcorpus”fortrainingatranslationmodel.Here,the“parallelcorpus”referstoaseriesofspeech-gazepairs,eachofthemconsistingofalistofwordsfromthespokenreferencesintheuserut-terancesandalistofobjectsthatareﬁxateduponwithinthesametimewindow.Speciﬁcally,weﬁrsttranscribedtheuserspeechintoscriptsbyautomaticspeechrecognitionsoft-wareandthenreﬁnedthemmanually.Atime-stampwasassociatedwitheachwordinthespeechscript.Further,wedetectedlongpausesinthespeechscriptassplittingpointstocreatetimewindows,sincealongpauseusuallymarksthestartofasentencethatindicatesauser’sattentionshift.Inourexper-iment,wesetthethresholdofjudgingalongpausetobe1second.Fromallthedatagatheredfrom7users,weget357suchtimewindows(whichtypi-callycontain10-20spokenwordsand5-10ﬁxatedobjectchanges).Givenatimewindow,wethenfoundtheobjectsbeingﬁxateduponbyeyegaze(representedbytheirIDsasshowninFigure1).Consideringthateyegazeﬁxationcouldoccurduringthepausesinspeech,weexpandedeachtimewindowbyaﬁxedlengthatbothitsstartandendtoﬁndtheﬁxations.Inourexperi-ments,theexpansionlengthissetto0.5seconds.Finally,weappliedapart-of-speechtaggertoeachsentenceintheuserscriptandonlysingledoutnounsaspotentialspokenreferencesinthewordlist.ThePorterstemmingalgorithmwasalsousedtogetthenormalizedformsofthosenouns.Thetranslationmodelwastrainedbasedonthispreprocessedparalleldata.6.2EvaluationMetricsAsdescribedinSection5,byusingastatisticaltranslationmodelwecangetasetoftranslationprobabilities,eitherfromanygivenspokenwordtoalltheobjects,orfromanygivenobjecttoallthespokenwords.Toevaluatethetwosetsoftrans-lationprobabilities,weuseprecisionandrecallas373

#RankPrecisionRecall#RankPrecisionRecall10.66670.259360.23020.537020.45240.351970.20410.555630.38100.444480.19050.592640.30950.481590.17990.629650.26670.5185100.16190.6296Table2:Averageprecision/recallofmappinggivenobjectstowords(i.e.,acquisition)#RankPrecisionRecall#RankPrecisionRecall10.78260.321460.30430.750020.58700.482170.26710.767930.46380.571480.24460.803640.38040.625090.22930.839350.34780.7143100.21240.8571Table3:Averageprecision/recallofmappinggivenwordstoobjects.(i.e.,interpretation)evaluationmetrics.Speciﬁcally,foragivenobjectokthetrans-lationmodelwillyieldasetofprobabilities{Pr(wj|ok),∀j}.Wecansorttheprobabilitiesandgetarankedlist.Letusassumethatwehavethegroundtruthaboutallthespokenwordstowhichthegivenobjectshouldbemapped.Then,atagivennumbernoftoprankedwords,theprecisionofmap-pingthegivenobjectoktowordsisdeﬁnedas#wordsthatokiscorrectlymappedto#wordsthatokismappedtoandtherecallisdeﬁnedas#wordsthatokiscorrectlymappedto#wordsthatokshouldbemappedtoAllthecountingaboveisdonewithinthetopnrank.Therefore,wecangetdifferentprecision/recallatdifferentranks.Ateachrank,theoverallperfor-mancecanbeevaluatedbyaveragingthepreci-sion/recallforallthegivenobjects.Humanjudg-mentisusedtodecidewhetheranobject-wordmap-pingiscorrectornot,asgroundtruthforevaluation.Similarly,basedonthesetofprobabilitiesofmap-pingagivenobjectwithspokenwords,wecanﬁndarankedlistofobjectsforagivenword,i.e.{Pr(ok|wj),∀k}.Thus,atagivenrankthepreci-sionandrecallofmappingagivenwordwjtoob-jectscanbemeasured.6.3ExperimentResultsVocabularyacquisitionistheprocessofﬁndingtheappropriateword(s)foranygivenobject.Forthesakeofstatisticalsigniﬁcance,ourevaluationisdoneon21objectsthatwerementionedatleast3timesbytheusers.Table2givestheaverageprecision/recallevalu-atedatthetop10ranks.Aswecansee,ifweusethemostprobablewordacquiredforeachobject,about66.67%ofthemareappropriate.Withtherankincreasing,moreandmoreappropriatewordscanbeacquired.About62.96%ofalltheappropri-atewordsareincludedwithinthetop10probablewordsfound.Theresultsindicatethatbyusingatranslationmodel,wecanobtainthewordsthatareusedbytheuserstodescribetheobjectswithrea-sonableaccuracy.Table4presentsthetop3mostprobablewordsfoundforeachobject.Itshowsthatalthoughtheremaybemorethanonewordappropriatetodescribeagivenobject,thosewordswithhighestprobabil-itiesalwayssuggestthemostpopularwayofde-scribingthecorrespondingobjectamongtheusers.Forexample,fortheobjectwithID26,thewordcandlegetsahigherprobabilitythanthewordcandlestick,whichisinaccordancewithourobservationthatinouruserstudy,onmostoccasionsuserstendtousethewordcandleratherthanthewordcandlestick.Vocabularyinterpretationistheprocessofﬁnd-ingtheappropriateobject(s)foranygivenspokenword.Outof176nounsintheuservocabulary,weonlyevaluatethoseusedatleastthreetimesforstatisticalsigniﬁcanceconcerns.Further,abstractwords(suchasreason,position)andgeneralwords(suchasroom,furniture)arenoteval-uatedsincetheydonotrefertoanyparticularobjectsinthescene.Finally,23nounsremainforevalua-tion.Wemanuallyenumeratedalltheobject(s)thatthose23nounsrefertoasthegroundtruthinourevaluation.Notethatagivennouncanpossiblybeusedtorefertomultipleobjects,suchaslamp,sincewehaveseverallamps(withobjectID3,8,17,and23)intheexperimentsetting,andbed,sincebedframe,bedspread,andpillows(withobjectID19,21,and20respectively)areallpartofabed.Also,anobjectcanbereferredtobymultiplenouns.Forexample,thewordspainting,picture,orwaterfallcanallbeusedtorefertotheob-jectwithID15.374

ObjectRank1Rank2Rank31paint(0.254)*wall(0.191)left(0.150)2pictur(0.305)*girl(0.122)niagara(0.095)*3wall(0.109)lamp(0.093)*ﬂoor(0.084)4upsid(0.174)*left(0.151)*paint(0.149)*5pictur(0.172)window(0.157)*wall(0.116)6window(0.287)*curtain(0.115)pictur(0.076)7chair(0.287)*tabl(0.088)bird(0.083)9mirror(0.161)*dresser(0.137)bird(0.098)*12room(0.131)lamp(0.127)left(0.069)14hang(0.104)favourit(0.085)natur(0.064)15thing(0.066)size(0.059)queen(0.057)16paint(0.211)*pictur(0.116)*forest(0.076)*17lamp(0.354)*end(0.154)tabl(0.097)18bedroom(0.158)side(0.128)bed(0.104)19bed(0.576)*room(0.059)candl(0.049)20bed(0.396)*queen(0.211)*size(0.176)21bed(0.180)*chair(0.097)orang(0.078)22bed(0.282)door(0.235)*chair(0.128)25chair(0.215)*bed(0.162)candlestick(0.124)26candl(0.145)*chair(0.114)candlestick(0.092)*27tree(0.246)*chair(0.107)ﬂoor(0.096)Table4:Wordsfoundforgivenobjects.Eachrowliststhetop3mostprobablespokenwords(beingstemmed)forthecorrespondinggivenobject,withthemappingprobabilitiesinparentheses.Asterisksindicatecorrectlyidentiﬁedspokenwords.Notethatsomeobjectsareheavilyoverlapped,sothecor-respondingwordsareconsideredcorrectforalltheoverlappingobjects,suchasbedbeingconsideredcorrectforobjectswithID19,20,and21.WordRank1Rank2Rank3Rank4curtain6(0.305)*5(0.305)*7(0.133)1(0.121)candlestick25(0.147)*28(0.135)24(0.131)22(0.117)lamp22(0.126)12(0.094)17(0.093)*25(0.093)dresser12(0.298)*9(0.294)*13(0.173)*7(0.104)queen20(0.187)*21(0.182)*22(0.136)19(0.136)*door22(0.200)*27(0.124)25(0.108)24(0.106)tabl9(0.152)*12(0.125)*13(0.112)*22(0.107)mirror9(0.251)*12(0.238)8(0.109)13(0.081)girl2(0.173)22(0.128)16(0.099)10(0.074)chair22(0.132)25(0.099)*28(0.085)24(0.082)waterfal6(0.226)5(0.215)1(0.118)9(0.083)candl19(0.156)22(0.139)28(0.134)24(0.131)niagara4(0.359)*2(0.262)*1(0.226)7(0.045)plant27(0.230)*22(0.181)23(0.131)28(0.117)tree27(0.352)*22(0.218)26(0.100)13(0.062)upsid4(0.204)*12(0.188)9(0.153)1(0.104)*bird9(0.142)*10(0.138)12(0.131)7(0.121)desk12(0.170)*9(0.141)*19(0.118)8(0.118)bed19(0.207)*22(0.141)20(0.111)*28(0.090)upsidedown4(0.243)*3(0.219)6(0.203)5(0.188)paint4(0.188)*16(0.148)*1(0.137)*15(0.118)*window6(0.305)*5(0.290)*3(0.085)22(0.065)lampshad3(0.223)*7(0.137)11(0.137)10(0.137)Table5:Objectsfoundforgivenwords.Eachrowliststhe4mostprobableobjectIDsforthecorre-spondinggivenwords(beingstemmed),withthemappingprobabilitiesinparentheses.Asterisksin-dicatecorrectlyidentiﬁedobjects.Notethatsomeobjectsareheavilyoverlapped,suchasthecandle(withobjectID26)andthechair(withobjectID25),andbothwereconsideredcorrectforthere-spectivespokenwords.Table3givestheaverageprecision/recallevalu-atedatthetop10ranks.Aswecansee,ifweusethemostprobableobjectfoundforeachspeechword,about78.26%ofthemareappropriate.Withtherankincreasing,moreandmoreappropriateobjectscanbefound.About85.71%ofalltheappropriateob-jectsareincludedwithinthetop10probableobjectsfound.Theresultsindicatethatbyusingatrans-lationmodel,wecanpredicttheobjectsfromuserspokenwordswithreasonableaccuracy.Table5liststhetop4probableobjectsfoundforeachspokenwordbeingevaluated.Acloselookre-vealsthatingeneral,thetoprankedobjectstendtogatheraroundthecorrectobjectforagivenspokenword.Thisisconsistentwiththefactthateyegazetendstomovebackandforth.Italsoindicatesthatthemappingsestablishedbythetranslationmodelcaneffectivelyﬁndtheapproximateareaofthecor-respondingﬁxatedobject,evenifitcannotﬁndtheobjectduetothenoisyandjerkynatureofeyegaze.Theprecision/recallinvocabularyacquisitionisnotashighasthatinvocabularyinterpretation,par-tiallyduetotherelativelysmallscaleofourexper-imentdata.Forexample,withonly7users’speechdataon14conversationaltasks,somewordswereonlyspokenafewtimestorefertoanobject,whichpreventedthemfromgettingasigniﬁcantportionofprobabilitymassamongallthewordsinthevocab-ulary.Thisdegradesbothprecisionandrecall.Webelievethatinlargescaleexperimentsorreal-worldapplications,theperformancewillbeimproved.7DiscussionandConclusionPreviouspsycholinguisticﬁndingshaveshownthateyegazeistightlylinkedwithhumanlanguagepro-duction.Duringhumanmachineconversation,ourstudyshowsthatalthoughalargervarianceisob-servedonhoweyeﬁxationsareexactlylinkedwithcorrespondingspokenreferences(comparedtothepsycholinguisticﬁndings),eyegazeingeneraliscloselycoupledwithcorrespondingreferringex-pressionsintheutterances.Thisclosecouplingna-turebetweeneyegazeandspeechutterancespro-videsanopportunityforthesystemtoautomatically375

acquiredifferentwordsrelatedtodifferentobjectswithoutanyhumansupervision.Tofurtherexplorethisidea,wedevelopedanovelunsupervisedap-proachusingstatisticaltranslationmodels.Ourexperimentalresultshaveshownthatthisap-proachcanreasonablyuncoverthemappingsbe-tweenwordsandobjectsonthegraphicaldisplay.Themainadvantagesofthisapproachinclude:1)Itisanunsupervisedapproachwithminimumhumaninference;2)Itdoesnotneedanypriorknowledgetotrainastatisticaltranslationmodel;3)Ityieldsprob-abilitiesthatindicatethereliabilityofthemappings.Certainly,ourcurrentapproachisbuiltuponsim-pliﬁedassumptions.Itisquitechallengingtoin-corporateeyegazeinformationsinceitisextremelynoisywithlargevariances.Recentworkhasshownthattheeffectofeyegazeinfacilitatingspokenlan-guageprocessingvariesamongdifferentusers(QuandChai,2007).Inaddition,visualpropertiesoftheinterfacealsoaffectusergazebehaviorandthusinﬂuencethepredicationofattention(Prasovetal.,2007)basedoneyegaze.Ourfutureworkwillde-velopmodelstoaddressthesevariations.Nevertheless,theresultsfromourcurrentworkhaveseveralimportantimplicationsinbuildingro-bustconversationalinterfaces.Firstofall,mostconversationalsystemsarebuiltwithstaticknowl-edgespace(e.g.,vocabularies)andcanonlybeup-datedbythesystemdevelopers.Ourapproachcanpotentiallyallowthesystemtoautomaticallyac-quireknowledgeandvocabulariesbasedonthenat-uralinteractionswiththeuserswithouthumanin-tervention.Furthermore,theautomaticallyacquiredmappingsbetweenwordsandobjectscanalsohelplanguageinterpretationtaskssuchasreferenceres-olution.Giventherecentadvancesineyetrack-ingtechnology(Duchowski,2002),integratingnon-intrusiveandhighperformanceeyetrackerswithconversationalinterfacesbecomesfeasible.Theworkreportedherecanpotentiallybeintegratedinpracticalsystemstoimprovetheoverallrobustnessofhumanmachineconversation.AcknowledgmentThisworkwassupportedbyfundingfromNationalScienceFoundation(IIS-0347548,IIS-0535112,andIIS-0643494)andDisruptiveTechnologyOf-ﬁce.TheauthorswouldliketothankZaharPrasovforhiscontributiontodatacollection.ReferencesP.F.Brown,S.A.DellaPietra,V.J.DellaPietra,andR.L.Mercer.1993.Themathematicsofstatisticalmachinetranslation:Parameterestimation.Computa-tionalLinguistics,19(2):263–311.E.Campana,J.Baldridge,J.Dowding,B.A.Hockey,R.Remington,andL.S.Stone.2001.Usingeyemovementstodeterminereferentsinaspokendialogsystem.InProceedingsofPUI’01.A.T.Duchowski.2002.Abreath-ﬁrstsurveyofeyetrackingapplications.BehaviorResearchmethods,In-struments,andComputers,33(4).Z.M.GrifﬁnandK.Bock.2000.Whattheeyessayaboutspeaking.PsychologicalScience,11:274–279.M.A.JustandP.A.Carpenter.1976.Eyeﬁxationsandcognitiveprocesses.CognitivePsychology,8:441–480.M.Kaur,M.Tremaine,N.Huang,J.Wilder,Z.Gacovski,F.Flippo,andC.S.Mantravadi.2003.Whereis“it”?Eventsynchronizationingaze-speechinputsystems.InProceedingsofICMI’03,pages151–157.Z.Prasov,J.Y.Chai,andH.Jeong.2007.Eyegazeforattentionpredictioninmultimodalhuman-machineconversation.In2007SpringSymposiumonInter-actionChallengesforArtiﬁcialAssistants,PaloAlto,California,March.S.QuandJ.Y.Chai.2007.Anexplorationofeyegazeinspokenlanguageprocessingformultimodalcon-versationalinterfaces.InNAACL’07,pages284–291,Rochester,NewYork,April.D.RoyandA.Pentland.2002.Learningwordsfromsightsandsounds,acomputationalmodel.CognitiveScience,26(1):113–1146.J.M.Siskind.1995.Groundinglanguageinperception.ArtiﬁcialIntelligenceReview,8:371–391.K.Tanaka.1999.Arobustselectionsystemusingreal-timemulti-modaluser-agentinteractions.InProceed-ingsofIUI’99,pages105–108.M.K.Tenenhaus,M.Sivey-Knowlton,E.Eberhard,andJ.Sedivy.1995.Integrationofvisualandlinguisticinformationduringspokenlanguagecomprehension.Science,268:1632–1634.C.YuandD.H.Ballard.2004.Ontheintegrationofgroundinglanguageandlearningobjects.ProceedingsofAAAI’04.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 376–383,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

376

A Multimodal Interface for Access to Content in the Home Michael Johnston AT&T Labs  Research, Florham Park, New Jersey, USA johnston@ research. att.com Luis Fernando D’Haro Universidad Politécnica de Madrid,  Madrid, Spain lfdharo@die. upm.es Michelle Levine AT&T Labs  Research, Florham Park,  New Jersey, USA mfl@research.att.com Bernard Renger AT&T Labs  Research, Florham Park,  New Jersey, USA renger@ research. att.com  Abstract In order to effectively access the rapidly increasing range of media content available in the home, new kinds of more natural in-terfaces are needed.  In this paper, we ex-plore the application of multimodal inter-face technologies to searching and brows-ing a database of movies.  The resulting system allows users to access movies using speech, pen, remote control, and dynamic combinations of these modalities. An ex-perimental evaluation, with more than 40 users, is presented contrasting two variants of the system: one combining speech with traditional remote control input and a sec-ond where the user has a tablet display supporting speech and pen input. 1 Introduction As traditional entertainment channels and the internet converge through the advent of technolo-gies such as broadband access, movies-on-demand, and streaming video, an increasingly large range of content is available to consumers in the home.  However, to benefit from this new wealth of con-tent, users need to be able to rapidly and easily find what they are actually interested in, and do so ef-fortlessly while relaxing on the couch in their liv-ing room — a location where they typically do not have easy access to the keyboard, mouse, and close-up screen display typical of desktop web browsing.  Current interfaces to cable and satellite televi-sion services typically use direct manipulation of a graphical user interface using a remote control. In order to find content, users generally have to either navigate a complex, pre-defined, and often deeply embedded menu structure or type in titles or other key phrases using an onscreen keyboard or triple tap input on a remote control keypad. These inter-faces are cumbersome and do not scale well as the range of content available increases (Berglund, 2004; Mitchell, 1999).   Figure 1 Multimodal interface on tablet In this paper we explore the application of multi-modal interface technologies (See André (2002) for an overview) to the creation of more effective systems used to search and browse for entertain-ment content in the home.  A number of previous systems have investigated the addition of unimodal spoken search queries to a graphical electronic program guide (Ibrahim and Johansson, 2002 377

(NokiaTV); Goto et al., 2003; Wittenburg et al., 2006). Wittenburg et al experiment with unre-stricted speech input for electronic program guide search, and use a highlighting mechanism to pro-vide feedback to the user regarding the “relevant” terms the system understood and used to make the query. However, their usability study results show this complex output can be confusing to users and does not correspond to user expectations. Others have gone beyond unimodal speech input and added multimodal commands combining speech with pointing (Johansson, 2003; Portele et al, 2006). Johansson (2003) describes a movie re-commender system MadFilm where users can use speech and pointing to accept/reject recommended movies.  Portele et al (2006) describe the Smart-Kom-Home system which includes multimodal electronic program guide on a tablet device. In our work we explore a broader range of inter-action modalities and devices. The system provides users with the flexibility to interact using spoken commands, handwritten commands, unimodal pointing (GUI) commands, and multimodal com-mands combining speech with one or more point-ing gestures made on a display. We compare two different interaction scenarios. The first utilizes a traditional remote control for direct manipulation and pointing, integrated with a wireless micro-phone for speech input. In this case, the only screen is the main TV display (far screen). In the second scenario, the user also has a second graphi-cal display (close screen) presented on a mobile tablet which supports speech and pen input, includ-ing both pointing and handwriting (Figure 1).  Our application task also differs, focusing on search and browsing of a large database of movies-on-demand and supporting queries over multiple si-multaneous dimensions.  This work also differs in the scope of the evaluation. Prior studies have pri-marily conducted qualitative evaluation with small groups of users (5 or 6). A quantitative and qualita-tive evaluation was conducted examining the inter-action of 44 naïve users with two variants of the system.  We believe this to be the first broad scale experimental evaluation of a flexible multimodal interface for searching and browsing large data-bases of movie content.  In Section 2, we describe the interface and illus-trate the capabilities of the system. In Section 3, we describe the underlying multimodal processing architecture and how it processes and integrates user inputs.  Section 4 describes our experimental evaluation and comparison of the two systems. Section 5 concludes the paper. 2 Interacting with the system The system described here is an advanced user in-terface prototype which provides multimodal ac-cess to databases of media content such as movies or television programming.  The current database is harvested from publicly accessible web sources and contains over 2000 popular movie titles along with associated metadata such as cast, genre, direc-tor, plot, ratings, length, etc. The user interacts through a graphical interface augmented with speech, pen, and remote control input modalities. The remote control can be used to move the current focus and select items.  The pen can be used both for selecting items (pointing at them) and for handwritten input. The graphical user interface has three main screens. The main screen is the search screen (Figure 2). There is also a control screen used for setting system parameters and a third comparison display used for showing movie details side by side (Figure 4).  The user can select among the screens using three icons in the navigation bar at the top left of the screen. The ar-rows provide ‘Back’ and ‘Next’ for navigation through previous searches.  Directly below, there is a feedback window which indicates whether the system is listening and provides feedback on speech recognition and search.  In the tablet vari-ant, the microphone and speech recognizer are ac-tivated by tapping on ‘CLICK TO SPEAK’ with the pen. In the remote control version, the recog-nizer can also be activated using a button on the remote control.  The main section of the search display (Figure 2) contains two panels.  The right panel (results panel) presents a scrollable list of thumbnails for the movies retrieved by the current search.  The left panel (details panel) provides de-tails on the currently selected title in the results panel.  These include the genre, plot summary, cast, and director.  The system supports a speech modality, a hand-writing modality, pointing (unimodal GUI) modal-ity, and composite multimodal input where the user utters a spoken command which is combined with pointing ‘gestures’ the user has made towards screen icons using the pen or the remote control.   378

     Figure 2 Graphical user interface Speech: The system supports speech search over multiple different dimensions such as title, genre, cast, director, and year. Input can be more tele-graphic with searches such as “Legally Blonde”, “Romantic comedy”, and “Reese Witherspoon”, or more verbose natural language queries such as “I’m looking for a movie called Legally Blonde” and “Do you have romantic comedies”.  An impor-tant advantage of speech is that it makes it easy to combine multiple constraints over multiple dimen-sions within a single query (Cohen, 1992). For ex-ample, queries can indicate co-stars: “movies star-ring Ginger Rogers and Fred Astaire”, or constrain genre and cast or director at the same time: “Meg Ryan Comedies”, “show drama directed by Woody Allen” and “show comedy movies directed by Woody Allen and starring Mira Sorvino”.  Handwriting: Handwritten pen input can also be used to make queries.  When the user’s pen ap-proaches the feedback window, it expands allow-ing for freeform pen input. In the example in Fig-ure 3, the user requests comedy movies with Bruce Willis using unimodal handwritten input. This is an important input modality as it is not impacted by ambient noise such as crosstalk from other viewers or currently playing content.  Figure 3 Handwritten query  Navigation Bar Feedback Window Pointing/GUI:  In addition to the recognition-based modalities, speech and handwriting, the in-terface also supports more traditional graphical user interface (GUI) commands. In the details panel, the actors and directors are presented as but-tons. Pointing at (i.e., clicking on) these buttons results in a search for all of the movies with that particular actor or director, allowing users to quickly navigate from an actor or director in a spe-cific title to other material they may be interested in. The buttons in the results panel can be pointed at (clicked on) in order to view the details in the left panel for that particular title.    Actor/Director Buttons Details Results Figure 4 Comparison screen Composite multimodal input: The system also supports true composite multimodality when spo-ken or handwritten commands are integrated with pointing gestures made using the pen (in the tablet version) or by selecting items (in the remote con-trol version).  This allows users to quickly execute more complex commands by combining the ease of reference of pointing with the expressiveness of spoken constraints.  While by unimodally pointing at an actor button you can search for all of the ac-tor’s movies, by adding speech you can narrow the search to, for example, all of their comedies by saying: “show comedy movies with THIS actor”.  Multimodal commands with multiple pointing ges-tures are also supported, allowing the user to ‘glue’ together references to multiple actors or directors in order to constrain the search.  For example, they can say “movies with THIS actor and THIS direc-tor” and point at the ‘Alan Rickman’ button and then the ‘John McTiernan’ button in turn (Figure 2). Comparison commands can also be multimo-379

dal; for example, if the user says “compare THIS movie and THIS movie” and clicks on the two but-tons on the right display for ‘Die Hard’ and the ‘The Fifth Element’ (Figure 2), the resulting dis-play shows the two movies side-by-side in the comparison screen (Figure 4).  3 Underlying multimodal architecture The system consists of a series of components which communicate through a facilitator compo-nent (Figure 5). This develops and extends upon the multimodal architecture underlying the MATCH system (Johnston et al., 2002).  Multimodal UIASRServerASRServerMultimodalNLUMultimodalNLUMovie DB(XML)NLU ModelGrammar TemplateASR ModelWordsGesturesSpeechClientSpeechClientMeaningGrammarCompilerGrammarCompilerFACILITATORHandwritingHandwritingRecognition Figure 5 System architecture The underlying database of movie information is stored in XML format.  When a new database is available, a Grammar Compiler component ex-tracts and normalizes the relevant fields from the database. These are used in conjunction with a pre-defined multimodal grammar template and any available corpus training data to build a multimo-dal understanding model and speech recognition language model.   The user interacts with the multimodal user in-terface client (Multimodal UI), which provides the graphical display.  When the user presses ‘CLICK TO SPEAK’ a message is sent to the Speech Cli-ent, which activates the microphone and ships au-dio to a speech recognition server.  Handwritten inputs are processed by a handwriting recognizer embedded within the multimodal user interface client. Speech recognition results, pointing ges-tures made on the display, and handwritten inputs, are all passed to a multimodal understanding server which uses finite-state multimodal language proc-essing techniques (Johnston and Bangalore, 2005) to interpret and integrate the speech and gesture. This model combines alignment of multimodal inputs, multimodal integration, and language un-derstanding within a single mechanism. The result-ing combined meaning representation (represented in XML) is passed back to the multimodal user interface client, which translates the understanding results into an XPATH query and runs it against the movie database to determine the new series of results.  The graphical display is then updated to represent the latest query. The system first attempts to find an exact match in the database for all of the search terms in the user’s query.  If this returns no results, a back off and query relaxation strategy is employed. First the system tries a search for movies that have all of the search terms, except stop words, independent of the order (an AND query). If this fails, then it backs off further to an OR query of the search terms and uses an edit machine, using Levenshtein distance, to retrieve the most similar item to the one requested by the user.  4 Evaluation After designing and implementing our initial proto-type system, we conducted an extensive multimo-dal data collection and usability study with the two different interaction scenarios: tablet versus remote control.  Our main goals for the data collection and statistical analysis were three-fold: collect a large corpus of natural multimodal dialogue for this me-dia selection task, investigate whether future sys-tems should be paired with a remote control or tab-let-like device, and determine which types of search and input modalities are more or less desir-able. 4.1 Experimental set up The system evaluation took place in a conference room set up to resemble a living room (Figure 6). The system was projected on a large screen across the room from a couch. An adjacent conference room was used for data collection (Figure 7). Data was collected in sound files, videotapes, and text logs. Each subject’s spo-ken utterances were recorded by three micro-phones: wireless, array and stand alone. The wire-less microphone was connected to the system while the array and stand alone microphones were 380

around 10 feet away.1 Test sessions were recorded with two video cameras – one captured the sys-tem’s screen using a scan converter while the other recorded the user and couch area. Lastly, the user’s interactions and the state of the system were cap-tured by the system’s logger. The logger is an addi-tional agent added to the system architecture for the purposes of the evaluation.  It receives log mes-sages from different system components as interac-tion unfolds and stores them in a detailed XML log file. For the specific purposes of this evaluation, each log file contains: general information about the system’s components, a description and time-stamp for each system event and user event, names and timestamps for the system-recorded sound files, and timestamps for the start and end of each scenario.  Figure 6 Data collection environment Forty-four subjects volunteered to participate in this evaluation.  There were 33 males and 11 fe-males, ranging from 20 to 66 years of age.  Each user interacted with both the remote control and tablet variants of the system, completing the same two sets of scenarios and then freely interacting with each system.  For counterbalancing purposes, half of the subjects used the tablet and then the re-mote control and the other half used the remote                                                  1 Here we report results for the wireless microphone only. Analysis of the other microphone conditions is ongoing. control and then the tablet.  The scenario set as-signed to each version was also counterbalanced.    Figure 7 Data collection room Each set of scenarios consisted of seven defined tasks, four user-specialized tasks and five open-ended tasks. Defined tasks were presented in chart form and had an exact answer, such as the movie title that two specified actors/actresses starred in. For example, users had to find the movie in the database with Matthew Broderick and Denzel Washington. User-specialized tasks relied on the specific user’s preferences, such as “What type of movie do you like to watch on a Sunday evening?  Find an example from that genre and write down the title”. Open-ended tasks prompted users to search for any type of information with any input modality. The tasks in the two sets paralleled each other. For example, if one set of tasks asked the user to find the highest ranked comedy movie with Reese Witherspoon, the other set of tasks asked the user to find the highest ranked comedy movie with Will Smith. Within each task set, the defined tasks appeared first, then the user-specialized tasks and lastly the open-ended tasks. However, for each par-ticipant, the order of defined tasks was random-ized, as well as the order of user-specialized tasks. At the beginning of the session, users read a short tutorial about the system’s GUI, the experi-ment, and available input modalities. Before inter-acting with each version, users were given a man-ual on operating the tablet/remote control. To minimize bias, the manuals gave only a general overview with few examples and during the ex-periment users were alone in the room.  At the end of each session, users completed a user-satisfaction/preference questionnaire and then a qualitative interview. The questionnaire consisted 381

of 25 statements about the system in general, the two variants of the system, input modality options and search options. For example, statements ranged from “If I had [the system], I would use the tablet with it” to “If my spoken request was mis-understood, I would want to try again with speak-ing”.  Users responded to each statement with a 5-point Likert scale, where 1 = ‘I strongly agree’, 2 = ‘I mostly agree’, 3 = ‘I can’t say one way or the other’, 4 = ‘I mostly do not agree’ and 5 = ‘I do not agree at all’. The qualitative interview allowed for more open-ended responses, where users could discuss reasons for their preferences and their likes and dislikes regarding the system. 4.2 Results Data was collected from all 44 participants. Due to technical problems, five participants’ logs or sound files were not recorded in parts of the experiment.  All collected data was used for the overall statistics but these five participants had to be excluded from analyses comparing remote control to tablet. Spoken utterances: After removing empty sound files, the full speech corpus consists of 3280 spoken utterances.  Excluding the five participants subject to technical problems, the total is 3116 ut-terances (1770 with the remote control and 1346 with the tablet).   The set of 3280 utterances averages 3.09 words per utterance.  There was not a significant differ-ence in utterance length between the remote con-trol and tablet conditions.  Users’ averaged 2.97 words per utterance with the remote control and 3.16 words per utterance with the tablet, paired t (38) = 1.182, p = n.s.  However, users spoke sig-nificantly more often with the remote control.  On average, users spoke 34.51 times with the tablet and 45.38 times with the remote control, paired t (38) = -3.921, p < .01. ASR performance: Over the full corpus of 3280 speech inputs, word accuracy was 44% and sentence accuracy 38%.  In the tablet condition, word accuracy averaged 46% and sentence accu-racy 41%.  In the remote control condition, word accuracy averaged 41% and sentence accuracy 38%.  The difference across conditions was only significant for word accuracy, paired t (38) = 2.469, p < .02.  In considering the ASR perform-ance, it is important to note that 55% of the 3280 speech inputs were out of grammar, and perhaps more importantly 34% were out of the functional-ity of the system entirely.  On within functionality inputs, word accuracy is 62% and sentence accu-racy 57%.  On the in grammar inputs, word accu-racy is 86% and sentence accuracy 83%. The vo-cabulary size was 3851 for this task. In the corpus, there are a total of 356 out-of-vocabulary words.  Handwriting recognition: Performance was de-termined by manual inspection of screen capture video recordings.2  There were a total of 384 handwritten requests with overall 66% sentence accuracy and 76% word accuracy. Task completion:  Since participants had to re-cord the task answers on a paper form, task com-pletion was calculated by whether participants wrote down the correct answer.  Overall, users had little difficulty completing the tasks.  On average, participants completed 11.08 out of the 14 defined tasks and 7.37 out of the 8 user-specialized tasks.  The number of tasks completed did not differ across system variants.3 For the seven defined tasks within each condition, users averaged 5.69 with the remote control and 5.40 with the tablet, paired t (34) = -1.203, p = n.s.  For the four user-specialized task within each condition, users aver-aged 3.74 on the remote control and 3.54 on the tablet, paired t (34) = -1.268, p = n.s. Input modality preference: During the inter-view, 55% of users reported preferring the pointing (GUI) input modality over speech and multimodal input. When asked about handwriting, most users were hesitant to place it on the list.  They also dis-cussed how speech was extremely important, and given a system with a low error speech recognizer, using speech for input probably would be their first choice. In the questionnaire, the majority of users (93%) ‘strongly agree’ or ‘mostly agree’ with the importance of making a pointing request. The im-portance of making a request by speaking had the next highest average, where 57% ‘strongly agree’ or ‘mostly agree’ with the statement. The impor-tance of multimodal and handwriting requests had the lowest averages, where 39% agreed with the former and 25% for the latter.  However, in the open-ended interview, users mentioned handwrit-ing as an important back-up input choice for cases when the speech recognizer fails.                                                  2 One of the 44 participants videotape did not record and so is not included in the statistics.     3 Four participants did not properly record their task answers and had to be eliminated from the 39 participants being used in the remote control versus tablet statistics.   382

Further support for input modality preference was gathered from the log files, which showed that par-ticipants mostly searched using unimodal speech commands and GUI buttons.  Out of a total of 6082 user inputs to the systems, 48% were unimo-dal speech and 39% were unimodal GUI (pointing and clicking). Participants requested information with composite multimodal commands 7% of the time and with handwriting 6% of the time. Search preference: Users most strongly agreed with movie title being the most important way to search. For searching by title, more than half the users chose ‘strongly agree’ and 91% of users chose ‘strongly agree’ or ‘mostly agree’.  Slightly more than half chose ‘strongly agree’ with search-ing by actor/actress and slightly less than half chose ‘strongly agree’ with the importance of searching by genre. During the open ended inter-view, most users reported title as the most impor-tant means for searching. Variant preference:  Results from the qualita-tive interview indicate that 67% of users preferred the remote control over the tablet variant of the system. The most common reported reasons were familiarity, physical comfort and ease of use. Re-mote control preference is further supported from the user-preference questionnaire, where 68% of participants ‘mostly agree’ or ‘strongly agree’ with wanting to use the remote control variant of the system, compared to 30% of participants choosing ‘mostly agree’ or ‘strongly agree’ with wanting to use the tablet version of the system. 5 Conclusion  With the range of entertainment content available to consumers in their homes rapidly expanding, the current access paradigm of direct manipulation of complex graphical menus and onscreen keyboards, and remote controls with way too many buttons is increasingly ineffective and cumbersome. In order to address this problem, we have developed a highly flexible multimodal interface that allows users to search for content using speech, handwrit-ing, pointing (using pen or remote control), and dynamic multimodal combinations of input modes. Results are presented in a straightforward graphical interface similar to those found in current systems but with the addition of icons for actors and direc-tors that can be used both for unimodal GUI and multimodal commands. The system allows users to search for movies over multiple different dimen-sions of classification (title, genre, cast, director, year) using the mode or modes of their choice. We have presented the initial results of an extensive multimodal data collection and usability study with the system. Users in the study were able to successfully use speech in order to conduct searches. Almost half of their inputs were unimodal speech (48%) and the majority of users strongly agreed with the impor-tance of using speech as an input modality for this task. However, as also reported in previous work (Wittenburg et al 2006), recognition accuracy re-mains a serious problem. To understand the per-formance of speech recognition here, detailed error analysis is important. The overall word accuracy was 44% but the majority of errors resulted from requests from users that lay outside the functional-ity of the underlying system, involving capabilities the system did not have or titles/cast absent from the database (34% of the 3280 spoken and multi-modal inputs). No amount of speech and language processing can resolve these problems. This high-lights the importance of providing more detailed help and tutorial mechanisms in order to appropri-ately ground users’ understanding of system capa-bilities. Of the remaining 66% of inputs (2166) which were within the functionality of the system, 68% were in grammar. On the within functionality portion of the data, the word accuracy was 62%, and on in grammar inputs it is 86%.  Since this was our initial data collection, an un-weighted finite-state recognition model was used. The perform-ance will be improved by training stochastic lan-guage models as data become available and em-ploying robust understanding techniques. One in-teresting issue in this domain concerns recognition of items that lie outside of the current database. Ideally the system would have a far larger vocabu-lary than the current database so that it would be able to recognize items that are outside the data-base. This would allow feedback to the user to dif-ferentiate between lack of results due to recogni-tion or understanding problems versus lack of items in the database. This has to be balanced against degradation in accuracy resulting from in-creasing the vocabulary.  In practice we found that users, while acknowl-edging the value of handwriting as a back-up mode, generally preferred the more relaxed and familiar style of interaction with the remote con-trol. However, several factors may be at play here. 383

The tablet used in the study was the size of a small laptop and because of cabling had a fixed location on one end of the couch. In future, we would like to explore the use of a smaller, more mobile, tablet that would be less obtrusive and more conducive to leaning back on the couch. Another factor is that the in-lab data collection environment is somewhat unrealistic since it lacks the noise and disruptions of many living rooms. It remains to be seen whether in a more realistic environment we might see more use of handwritten input. Another factor here is familiarity. It may be that users have more familiarity with the concept of speech input than handwriting. Familiarity also appears to play a role in user preferences for remote control versus tablet. While the tablet has additional capabilities such handwriting and easier use of multimodal com-mands, the remote control is more familiar to users and allows for a more relaxed interaction since they can lean back on the couch. Also many users are concerned about the quality of their handwrit-ing and may avoid this input mode for that reason.   Another finding is that it is important not to un-derestimate the importance of GUI input. 39% of user commands were unimodal GUI (pointing) commands and 55% of users reported a preference for GUI over speech and handwriting for input. Clearly, the way forward for work in this area is to determine the optimal way to combine more tradi-tional graphical interaction techniques with the more conversational style of spoken interaction. Most users employed the composite multimodal commands, but they make up a relatively small proportion of the overall number of user inputs in the study data (7%). Several users commented that they did not know enough about the multimodal commands and that they might have made more use of them if they had understood them better. This, along with the large number of inputs that were out of functionality, emphasizes the need for more detailed tutorial and online help facilities. The fact that all users were novices with the sys-tem may also be a factor. In future, we hope to conduct a longer term study with repeat users to see how previous experience influences use of newer kinds of inputs such as multimodal and handwriting.   Acknowledgements Thanks to Keith Bauer, Simon Byers, Harry Chang, Rich Cox, David Gibbon, Mazin Gilbert, Stephan Kanthak, Zhu Liu, Antonio Moreno, and Behzad Shahraray for their help and support.  Thanks also to the Di-rección General de Universidades e Investigación - Consejería de Educación - Comunidad de Madrid, España for sponsoring D’Haro’s visit to AT&T. References Elisabeth André. 2002. Natural Language in Multimodal and Multimedia systems. In Ruslan Mitkov (ed.) Ox-ford Handbook of Computational Linguistics. Oxford University Press. Aseel Berglund. 2004. Augmenting the Remote Control: Studies in Complex Information Navigation for Digi-tal TV. Linköping Studies in Science and Technol-ogy, Dissertation no. 872. Linköping University. Philip R. Cohen. 1992. The Role of Natural Language in a Multimodal Interface. In Proceedings of ACM UIST Symposium on User Interface Software and Technology. pp. 143-149. Jun Goto, Kazuteru Komine, Yuen-Bae Kim and Nori-yoshi Uratan. 2003. A Television Control System based on Spoken Natural Language Dialogue. In Proceedings of 9th International Conference on Hu-man-Computer Interaction. pp. 765-768. Aseel Ibrahim and Pontus Johansson. 2002. Multimodal Dialogue Systems for Interactive TV Applications. In Proceedings of 4th IEEE International Conference on Multimodal Interfaces. pp. 117-222. Pontus Johansson. 2003. MadFilm - a Multimodal Ap-proach to Handle Search and Organization in a Movie Recommendation System. In Proceedings of the 1st Nordic Symposium on Multimodal Communi-cation. Helsingör, Denmark. pp. 53-65. Michael Johnston, Srinivas Bangalore, Guna Vasireddy, Amanda Stent, Patrick Ehlen, Marilyn Walker, Steve Whittaker, Preetam Maloor. 2002. MATCH: An Ar-chitecture for Multimodal Dialogue Systems. In Pro-ceedings of the 40th ACL. pp. 376-383. Michael Johnston and Srinivas Bangalore. 2005. Finite-state Multimodal Integration and Understanding. Journal of Natural Language Engineering 11.2. Cambridge University Press. pp. 159-187. Russ Mitchell. 1999. TV’s Next Episode. U.S. News and World Report. 5/10/99. Thomas Portele, Silke Goronzy, Martin Emele, Andreas Kellner, Sunna Torge, and Jüergen te Vrugt. 2006. SmartKom–Home: The  Interface to Home Enter-tainment. In Wolfgang Wahlster (ed.) SmartKom: Foundations of Multimodal Dialogue Systems. Springer.  pp. 493-503. Kent Wittenburg, Tom Lanning, Derek Schwenke, Hal Shubin and Anthony Vetro. 2006. The Prospects for Unrestricted Speech Input for TV Content Search. In Proceedings of  AVI’06. pp. 352-359. Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 384–391,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

384

FastUnsupervisedIncrementalParsingYoavSeginerInstituteforLogic,LanguageandComputationUniversiteitvanAmsterdamPlantageMuidergracht241018TVAmsterdamTheNetherlandsyseginer@science.uva.nlAbstractThispaperdescribesanincrementalparserandanunsupervisedlearningalgorithmforinducingthisparserfromplaintext.Theparserusesarepresentationforsyntacticstructuresimilartodependencylinkswhichiswell-suitedforincrementalparsing.Incontrasttopreviousunsupervisedparsers,theparserdoesnotusepart-of-speechtagsandbothlearningandparsingarelocalandfast,requiringnoexplicitclusteringorglobaloptimization.Theparserisevalu-atedbyconvertingitsoutputintoequivalentbracketingandimprovesonpreviouslypub-lishedresultsforunsupervisedparsingfromplaintext.1IntroductionGrammarinduction,thelearningofthegrammarofalanguagefromunannotatedexamplesentences,haslongbeenofinteresttolinguistsbecauseofitsrelevancetolanguageacquisitionbychildren.Inrecentyears,interestinunsupervisedlearningofgrammarhasalsoincreasedamongcomputationallinguists,asthedifﬁcultyandcostofconstructingannotatedcorporaledresearcherstolookforwaystotrainparsersonunannotatedtext.Thiscanei-therbesemi-supervisedparsing,usingbothanno-tatedandunannotateddata(McCloskyetal.,2006)orunsupervisedparsing,trainingentirelyonunan-notatedtext.Thepastfewyearshaveseenconsiderableim-provementintheperformanceofunsupervisedparsers(KleinandManning,2002;KleinandMan-ning,2004;Bod,2006a;Bod,2006b)and,fortheﬁrsttime,unsupervisedparsershavebeenabletoimproveontheright-branchingheuristicforpars-ingEnglish.Alltheseparserslearnandparsefromsequencesofpart-of-speechtagsandselect,foreachsentence,thebinaryparsetreewhichmaxi-mizessomeobjectivefunction.Learningisbasedonglobalmaximizationofthisobjectivefunctionoverthewholecorpus.InthispaperIpresentanunsupervisedparserfromplaintextwhichdoesnotuseparts-of-speech.Learningislocalandparsingis(locally)greedy.Asaresult,bothlearningandparsingarefast.Theparserisincremental,usinganewlinkrepresenta-tionforsyntacticstructure.Incrementalparsingwaschosenbecauseitconsiderablyrestrictsthesearchspaceforbothlearningandparsing.Therepresen-tationtheparserusesisdesignedforincrementalparsingandallowsapreﬁxofanutterancetobeparsedbeforethefullutterancehasbeenread(seesection3).Therepresentationtheparseroutputscanbeconvertedintobracketing,thusallowingevalua-tionoftheparseronstandardtreebanks.Toachievecompletelyunsupervisedparsing,standardunsupervisedparsers,workingfrompart-of-speechsequences,needﬁrsttoinducetheparts-of-speechfortheplaintexttheyneedtoparse.Thereareseveralalgorithmsfordoingso(Sch¨utze,1995;Clark,2000),whichclusterwordsintoclassesbasedonthemostfrequentneighborsofeachword.ThisstepbecomessuperﬂuousinthealgorithmIpresenthere:thealgorithmcollectslistsoflabelsforeachword,basedonneighboringwords,andthendirectly385

usestheselabelstoparse.Noclusteringisper-formed,butduetotheZipﬁandistributionofwords,highfrequencywordsdominatetheselistsandpars-ingdecisionsforwordsofsimilardistributionareguidedbythesamelabels.Section2describesthesyntacticrepresentationused,section3describesthegeneralparseralgo-rithmandsections4and5completethedetailsbydescribingthelearningalgorithm,thelexiconitcon-structsandthewaytheparserusesthislexicon.Sec-tion6givesexperimentalresults.2CommonCoverLinksTherepresentationofsyntacticstructurewhichIin-troduceinthispaperisbasedonlinksbetweenpairsofwords.Givenanutteranceandabracketingofthatutterance,shortestcommoncoverlinksetsforthebracketingaredeﬁned.Theoriginalbracketingcanbereconstructedfromanyoftheselinksets.2.1BasicDeﬁnitionsAnutteranceisasequenceofwordshx1,...,xniandabracketisanysub-sequencehxi,...,xjiofconsecutivewordsintheutterance.AsetBofbrack-etsoveranutteranceUisabracketingofUifeverywordinUisinsomebracketandforanyX,Y∈BeitherX∩Y=∅,X⊆YorY⊆X(non-crossingbrackets).Thedepthofawordx∈UunderabracketB∈B(x∈B)isthemaxi-malnumberofbracketsX1,...,Xn∈Bsuchthatx∈X1⊂...⊂Xn⊂B.AwordxisageneratorofdepthdofBinBifxisofminimaldepthunderB(amongallwordsinB)andthatdepthisd.Abracketmayhavemorethanonegenerator.2.2CommonCoverLinkSetsAcommoncoverlinkoveranutteranceUisatriplexd→ywherex,y∈U,x6=yanddisanon-negativeinteger.Thewordxisthebaseofthelink,thewordyisitsheadanddisthedepthofthelink.ThecommoncoverlinksetRBassociatedwithabracketingBisthesetofcommoncoverlinksoverUsuchthatxd→y∈RBiffthewordxisagener-atorofdepthdofthesmallestbracketB∈Bsuchthatx,y∈B(seeﬁgure1(a)).GivenRB,asimplealgorithmreconstructsthebracketingB:foreachwordxanddepth0≤d,(a)[[w]1;;1<<1==[x1zz0!!0  [y//0z]]]oo(b)[[w][x1zz0!!0  [y//0z]]]oo(c)[[w][x1zz0!![y//0z]]]ooFigure1:(a)ThecommoncoverlinksetRBofabracketingB,(b)arepresentativesubsetRofRB,(c)theshortestcommoncoverlinksetbasedonR.createabracketcoveringxandallysuchthatforsomed0≤d,xd0→y∈RB.SomeofthelinksinthecommoncoverlinksetRBareredundant.Theﬁrstredundancyistheresultofbracketshavingmorethanonegenerator.Thebracketingreconstructionalgorithmoutlinedabovecanconstructabracketfromthelinksbasedatanyofitsgenerators.ThebracketingBcanthereforebereconstructedfromasubsetR⊆RBif,foreverybracketB∈B,Rcontainsthelinksbasedatleastatonegenerator1ofB.SuchasetRisarepresentativesubsetofRB(seeﬁgure1(b)).AsecondredundancyinthesetRBfollowsfromthelineartransitivityofRB:Lemma1Ifyisbetweenxandz,xd1→y∈RBandyd2→z∈RBthenxd→z∈RBwhereifthereisalinkyd0→x∈RBthend=max(d1,d2)andd=d1otherwise.Thispropertyimpliesthatlongerlinkscanbede-ducedfromshorterlinks.Itis,therefore,sufﬁcienttoleaveonlytheshortestnecessarylinksintheset.GivenarepresentativesubsetRofRB,ashortestcommoncoverlinksetofRBisconstructedbyre-movinganylinkwhichcanbededucedfromshorterlinksbylineartransitivity.ForeachrepresentativesubsetR⊆RB,thisdeﬁnesauniqueshortestcom-moncoverlinkset(seeﬁgure1(c)).GivenashortestcommoncoverlinksetS,thebracketingwhichitrepresentscanbecalculatedby1Fromthebracketreconstructionalgorithmitcanbeseenthatlinksofdepth0mayneverbedropped.386

[[I][know{{%%[[theboy]oo[sleeps]]]]}}(a)dependencystructure[[I][know1{{0%%0""[[the0//boy]oo[sleeps]]]]1}}(b)shortestcommoncoverlinksetFigure2:Adependencystructureandshortestcom-moncoverlinksetofthesamesentence.ﬁrstusinglineartransitivitytodeducemissinglinksandthenapplyingthebracketreconstructionalgo-rithmoutlinedaboveforRB.2.3ComparisonwithDependencyStructuresHavingdeﬁnedalink-basedrepresentationofsyn-tacticstructure,itisnaturaltowonderwhattherela-tionisbetweenthisrepresentationandstandardde-pendencystructures.Themaindifferencesbetweenthetworepresentationscanallbeseeninﬁgure2.TheﬁrstdifferenceisinthelinkingoftheNPtheboy.WhiletheshortestcommoncoverlinksethasanexocentricconstructionforthisNP(thatis,linksgoingbackandforthbetweenthetwowords),thedependencystructureforcesustodecidewhichofthetwowordsintheNPisitshead.ConsideringthatlinguistshavenotbeenabletoagreewhetheritisthedeterminerorthenounthatistheheadofanNP,itmaybeeasierforalearningalgorithmifitdidnothavetomakesuchachoice.Theseconddifferencebetweenthestructurescanbeseeninthelinkfromknowtosleeps.Intheshort-estcommoncoverlinkset,thereisapathoflinksconnectingknowtoeachofthewordsseparatingitfromsleeps,whileinthedependencystructurenosuchlinksexist.Thisproperty,whichIwillrefertoasadjacencyplaysanimportantroleinincrementalparsing,asexplainedinthenextsection.Thelastmaindifferencebetweentherepresen-tationsistheassignmentofdepthtothecommoncoverlinks.Inthepresentexample,thisallowsustodistinguishbetweentheattachmentoftheexternal(subject)andtheinternal(object)argumentsoftheverb.Dependenciescannotcapturethisdifferencewithoutadditionallabelingofthelinks.Inwhatfol-lows,Iwillrestrictcommoncoverlinkstohavingdepth0or1.Thisrestrictionmeansthatanytreerepresentedbyashortestcommoncoverlinksetwillbeskewed-everysubtreemusthaveashortbranch.Itseemsthatthisisindeedapropertyofthesyntaxofnaturallanguages.Buildingthisrestrictionintothesyntacticrepresentationconsiderablyreducesthesearchspaceforbothparsingandlearning.3IncrementalParsingTocalculateashortestcommoncoverlinkforanutterance,Iwilluseanincrementalparser.Incre-mentalitymeansthattheparserreadsthewordsoftheutteranceonebyoneand,aseachwordisread,theparserisonlyallowedtoaddlinkswhichhaveoneoftheirendsatthatword.Wordswhichhavenotyetbeenreadarenotavailabletotheparseratthisstage.Thisrestrictionisinspiredbypsycholin-guisticresearchwhichsuggeststhathumansprocesslanguageincrementally(Crockeretal.,2000).Iftheincrementalityoftheparserroughlyresemblesthatofhumanprocessing,theresultisasigniﬁcantre-strictionofparsersearchspacewhichdoesnotleadtotoomanyparsingerrors.Theadjacencypropertydescribedintheprevioussectionmakesshortestcommoncoverlinksetses-peciallysuitableforincrementalparsing.Considertheexamplegiveninﬁgure2.Whenthewordtheisread,theparsercanalreadyconstructalinkfromknowtothewithoutworryingaboutthecontinuationofthesentence.ThislinkispartofthecorrectparsewhetherthesentenceturnsouttobeIknowtheboyorIknowtheboysleeps.Adependencyparser,ontheotherhand,cannotmakesuchadecisionbeforetheendofthesentenceisreached.IfthesentenceisIknowtheboythenadependencylinkhastobecre-atedfromknowtoboywhileifthesentenceisIknowtheboysleepsthensuchalinkiswrong.Thisprob-lemisknowninpsycholinguisticsastheproblemofreanalysis(SturtandCrocker,1996).Assumetheincrementalparserisprocessingapreﬁxhx1,...,xkiofanutteranceandhasalreadydeducedasetoflinksLforthispreﬁx.Itcannowonlyaddlinkswhichhaveoneoftheirendsatxkanditmayneverremoveanylinks.Fromthedeﬁnitionsinsection2.2itispossibletoderiveanexactchar-acterizationofthelinkswhichmaybeaddedateachstepsuchthattheresultinglinksetrepresentssome387

bracketing.Itcanbeshownthatanyshortestcom-moncoverlinksetcanbeconstructedincrementallyundertheseconditions.Asthefullspeciﬁcationoftheseconditionsisbeyondthescopeofthispaper,Iwillonlygivethemaincondition,whichisbasedonadjacency.Itstatesthatalinkmaybeaddedfromxtoyonlyifforeveryzbetweenxandythereisapathoflinks(inL)fromxtozbutnolinkfromztoy.Intheexampleinﬁgure2thismeansthatwhenthewordsleepsisﬁrstread,alinktosleepscanbecreatedfromknow,theandboybutnotfromI.Giventheseconditions,theparsingprocessissimple.Ateachstep,theparsercalculatesanon-negativeweight(section5)foreverylinkwhichmaybeaddedbetweenthepreﬁxhx1,...,xk−1iandxk.Itthenaddsthelinkwiththestrongestpositiveweightandrepeatstheprocess(addingalinkcanchangethesetoflinkswhichmaybeadded).Whenallpossiblelinksareassignedazeroweightbytheparser,theparserreadsthenextwordoftheutter-anceandrepeatstheprocess.Thisisagreedyalgo-rithmwhichoptimizeseverystepseparately.4LearningTheweightfunctionwhichassignsaweighttoacan-didatelinkislexicalized:theweightiscalculatedbasedonthelexicalentriesofthewordswhicharetobeconnectedbythelink.Itisthetaskofthelearn-ingalgorithmtolearnthelexicon.4.1TheLexiconThelexiconstoresforeachwordxalexicalen-try.Eachsuchlexicalentryisasequenceofadja-cencypoints,holdingstatisticsrelevanttothedeci-sionwhethertolinkxtosomeotherword.Thesestatisticsaregivenasweightsassignedtolabelsandlinkingproperties.Eachadjacencypointdescribesadifferentlinkbasedatx,similartothespeciﬁcationoftheargumentsofawordindependencyparsing.LetWbethesetofwordsinthecorpus.ThesetoflabelsL(W)=W×{0,1}consistsoftwolabelsbasedoneverywordw:aclassla-bel(w,0)(denotedby[w])andanadjacencyla-bel(w,1)(denotedby[w]or[w]).Thetwola-bels(w,0)and(w,1)aresaidtobeoppositela-belsand,forl∈L(W),Iwritel−1fortheop-positeofl.Inadditiontothelabels,thereisalsoaﬁnitesetP={Stop,In∗,In,Out}oflink-ingproperties.TheStopspeciﬁesthestrengthofnon-attachment,InandOutspecifythestrengthofinboundandoutboundlinksandIn∗isanin-termediatevalueintheinductionofinboundandoutboundstrengths.AlexiconLisafunctionwhichassignseachwordw∈Walexicalentry(...,Aw−2,Aw−1,Aw1,Aw2,...).EachoftheAwiisanadjacencypoint.EachAwiisafunctionAwi:L(W)∪P→RwhichassignseachlabelinL(W)andeachlinkingpropertyinParealvaluedstrength.ForeachAwi,#(Awi)isthecountoftheadjacencypoint:thenum-beroftimestheadjacencypointwasupdated.Basedonthiscount,IalsodeﬁneanormalizedversionofAwi:¯Awi(l)=Awi(l)/#(Awi).4.2TheLearningProcessGivenasequenceoftrainingutterances(Ut)0≤t,thelearnerconstructsasequenceoflexicons(Ls)0≤sbeginningwiththezerolexiconL0(whichassignsazerostrengthtoalllabelsandlinkingproperties).Ateachstep,thelearnerusestheparsingfunctionPLsbasedonthepreviouslylearnedlexiconLstoextendtheparseLofanutteranceUt.Itthenusestheresultofthisparsestep(togetherwiththelexi-conLs)tocreateanewlexiconLs+1(itmaybethatLs=Ls+1).Thisoperationisalexiconupdate.TheprocessthencontinueswiththenewlexiconLs+1.AnyofthelexiconsLsconstructedbythelearnermaybeusedforparsinganyutteranceU,butassincreases,parsingaccuracyshouldimprove.Thislearningprocessisopen-ended:additionaltrainingtextcanalwaysbeaddedwithouthavingtore-runthelearneronprevioustrainingdata.4.3LexiconUpdateTodeﬁnealexiconupdate,IextendthedeﬁnitionofanutterancetobeU=h∅l,x1,...,xn,∅riwhere∅land∅rareboundarymarkers.Thepropertyofadja-cencycannowbeextendedtoincludetheboundarymarkers.Asymbolα∈UisadjacenttoawordxrelativetoasetoflinksLoverUifforeverywordzbetweenxandαthereisapathoflinksinLfromxtozbutthereisnolinkfromztoα.Inthefollowingexample,theadjacenciesofx1are∅l,x2andx3:x10//x2x3x4388

Ifalinkisaddedfromx2tox3,x4becomesadjacenttox1insteadofx3(theadjacenciesofx1arethen∅l,x2andx4):x10//x20//x3x4Thepositionsintheutteranceadjacenttoawordxareindexedbyanindexisuchthati<0totheleftofx,i>0totherightofxand|i|increaseswiththedistancefromx.Theparsermayonlyaddalinkfromawordxtoawordyadjacenttox(relativetothesetoflinksal-readyconstructed).Therefore,thelexicalentryofxshouldcollectstatisticsabouteachoftheadjacencypositionsofx.Asseenabove,adjacencypositionsmaymove,sothelearnerwaitsuntiltheparsercom-pletesparsingtheutteranceandthenupdateseachadjacencypointAxiwiththesymbolαattheithad-jacencypositionofx(relativetotheparsegeneratedbytheparser).Itshouldbestressedthatthisupdatedoesnotdependonwhetheralinkwascreatedfromxtoα.Inparticular,whateverlinkstheparseras-signs,Ax(−1)andAx1arealwaysupdatedbythesym-bolswhichappearimmediatelybeforeandafterx.Thefollowingexampleshouldclarifythepicture.Considerthefragment:put0//the//0boxooonAllthelinksinthisexample,includingtheabsenceofalinkfromboxtoon,dependonadjacencypointsoftheformAx(−1)andAx1whichareupdatedinde-pendentlyofanylinks.Basedonthisaloneandre-gardlessofwhetheralinkiscreatedfromputtoon,Aput2willbeupdatedbythewordon,whichisin-deedthesecondargumentoftheverbput.4.4AdjacencyPointUpdateTheupdateofAxibyαisgivenbyoperationsAxi(p)+=f(Aα(−1),Aα1)whichmakethevalueofAxi(p)inthenewlexiconLs+1equaltothesumAxi(p)+f(Aα(−1),Aα1)intheoldlexiconLs.LetSign(i)be1if0<iand−1otherwise.Let•Aαi=trueif@l∈L(W):Aαi(l)>Aαi(Stop)falseotherwiseTheupdateofAxibyαbeginsbyincrementingthecount:#(Axi)+=1Ifαisaboundarysymbol(∅lor∅r)orifxandαarewordsseparatedbystoppingpunctuation(fullstop,questionmark,exclamationmark,semicolon,commaordash):Axi(Stop)+=1Otherwise,foreveryl∈L(W):Axi(l−1)+=(cid:26)1ifl=[α]¯AαSign(−i)(l)otherwise(Inpractice,onlyl=[α]andthe10strongestlabelsinAαSign(−i)areupdated.Becauseoftheexponen-tialdecayinthestrengthoflabelsinAαSign(−i),thisisagoodapproximation.)Ifi=−1,1andαisnotaboundaryorblockedbypunctuation,simplebootstrappingtakesplacebyupdatingthefollowingproperties:Axi(In∗)+=−1if•AαSign(−i)+1if¬•AαSign(−i)∧•AαSign(i)0otherwiseAxi(Out)+=¯AαSign(−i)(In∗)Axi(In)+=¯AαSign(−i)(Out)4.5DiscussionTounderstandthewaythelabelsandpropertiesarecalculated,itisbesttolookatanexample.ThefollowingtablegivesthelinkingpropertiesandstrongestlabelsforthedeterminertheaslearnedfromthecompleteWallStreetJournalcorpus(onlyAthe(−1)andAthe1areshown):theA−1A1Stop12897Stop8In∗14898In∗18914In8625In4764Out-13184Out21922[the]10673[the]16461[of]6871[a]3107[in]5520[the]2787[a]3407[of]2347[for]2572[company]2094[to]2094[’s]1686Astrongclasslabel[w]indicatesthatthewordwfrequentlyappearsincontextswhicharesimilartothe.Astrongadjacencylabel[w](or[w])indicates389

thatweitherfrequentlyappearsnexttotheorthatwfrequentlyappearsinthesamecontextsaswordswhichappearnexttothe.ThepropertyStopcountsthenumberoftimesaboundaryappearednexttothe.Becausethecanof-tenappearatthebeginningofanutterancebutmustbefollowedbyanounoranadjective,itisnotsur-prisingthatStopisstrongerthananylabelontheleftbutweakerthanalllabelsontheright.Ingen-eral,itisunlikelythatawordhasanoutboundlinkonthesideonwhichitsStopstrengthisstrongerthanthatofanylabel.Theoppositeisnottrue:alabelstrongerthanStopindicatesanattachmentbutthismayalsobetheresultofaninboundlink,asinthefollowingentryforto,wherethestronglabelsontheleftarearesultofaninboundlink:toA−1A1Stop822Stop48In∗-4250In∗-981In-57In-1791Out-3053Out4010[to]5912[to]7009[%]848[the]3851[in]844[be]2208[the]813[will]1414[of]624[a]1158[a]599[the]954Forthisreason,thelearningprocessisbasedontheproperty•Axiwhichindicateswherealinkisnotpossible.Sinceanoutboundlinkononewordisin-boundontheother,theinbound/outboundpropertiesofeachwordarethencalculatedbyasimpleboot-strappingprocessasanaverageoftheoppositeprop-ertiesoftheneighboringwords.5TheWeightFunctionAteachstep,theparsermustassignanon-negativeweighttoeverycandidatelinkxd→ywhichmaybeaddedtoanutterancepreﬁxhx1,...,xki,andthelinkwiththelargest(non-zero)weight(withapref-erenceforlinksbetweenxk−1andxk)isaddedtotheparse.TheweightcouldbeassigneddirectlybasedontheInandOutpropertiesofeitherxorybutthismethodisnotsatisfactoryforthreerea-sons:ﬁrst,thevaluesofthesepropertiesonlowfre-quencywordsarenotreliable;second,thevaluesofthepropertiesonxandymayconﬂict;third,somewordsareambiguousandrequiredifferentlinkingindifferentcontexts.Tosolvetheseproblems,theweightofthelinkistakenfromthevaluesofInandOutonthebestmatchinglabelbetweenxandy.Thislabeldependsonbothwordsandisusuallyafrequentwordwithreliablestatistics.Itservesasaprototypefortherelationbetweenxandy.5.1BestMatchingLabelAlabellisamatchinglabelbetweenAxiandAySign(−i)ifAxi(l)>Axi(Stop)andeitherl=(y,1)orAySign(−i)(l−1)>0.ThebestmatchinglabelatAxiisthematchinglabellsuchthatthematchstrengthmin(¯Axi(l),¯AySign(−i)(l−1))ismaximal(ifl=(y,1)then¯AySign(−i)(l−1)isdeﬁnedtobe1).Inpractice,asbefore,onlythetop10labelsinAxiandAySign(−i)areconsidered.ThebestmatchinglabelfromxtoyiscalculatedbetweenAxiandAySign(−i)suchthatAxiisonthesamesideofxasyandwaseitheralreadyusedtocreatealinkoristheﬁrstadjacencypointonthatsideofxwhichwasnotyetused.Thismeansthattheadjacencypointsoneachsidehavetobeusedonebyone,butmaybeusedmorethanonce.Thereasonisthatoptionalargumentsofxusuallydonothaveanadjacencypointoftheirownbuthavethesamelabelsasobligatoryargumentsofxandcansharetheiradjacencypoint.TheAxiwiththestrongestmatchinglabelisselected,withaprefer-encefortheunusedadjacencypoint.Asinthelearningprocess,labelmatchingisblockedbetweenwordswhichareseparatedbystop-pingpunctuation.5.2CalculatingtheLinkWeightThebestmatchinglabell=(w,δ)fromxtoycanbeeitheraclass(δ=0)oranadjacency(δ=1)la-belatAxi.Ifitisaclasslabel,wcanbeseenastak-ingtheplaceofxandallwordsseparatingitfromy(whicharealreadylinkedtox).Iflisanadjacencylabel,wcanbeseentotaketheplaceofy.Thecal-culationoftheweightWt(xd→y)ofthelinkfromxtoyisthereforebasedonthestrengthsoftheInandOutpropertiesofAwσwhereσ=Sign(i)ifl=(w,0)andσ=Sign(−i)ifl=(w,1).Inad-dition,theweightisboundedfromabovebythebestlabelmatchstrength,s(l):•Ifl=(w,0)andAwσ(Out)>0:Wt(x0→y)=min(s(l),¯Awσ(Out))390

WSJ10WSJ40Negra10Negra40ModelUPURUF1UPURUF1UPURUF1UPURUF1Right-branching55.170.061.735.447.440.533.960.143.317.635.023.4Right-branching+punct.59.174.465.844.557.750.235.462.545.220.940.427.6ParsingfromPOSCCM64.281.671.948.185.561.6DMV+CCM(POS)69.388.077.649.689.763.9U-DOP70.888.278.563.951.290.565.4UML-DOP82.966.467.0ParsingfromplaintextDMV+CCM(DISTR.)65.282.872.9Incremental75.676.275.958.955.957.451.069.859.034.848.940.6Incremental(righttoleft)75.972.574.259.352.255.650.468.358.032.945.538.2Table1:ParsingresultsonWSJ10,WSJ40,Negra10andNegra40.•Ifl=(w,1):◦IfAwσ(In)>0:Wt(xd→y)=min(s(l),¯Awσ(In))◦Otherwise,ifAwσ(In∗)≥|Awσ(In)|:Wt(xd→y)=min(s(l),¯Awσ(In∗))whereifAwσ(In∗)<0andAwσ(Out)≤0thend=1andotherwised=0.•IfAwσ(Out)≤0andAwσ(In)≤0andeitherl=(w,1)orAwσ(Out)=0:Wt(x0→y)=s(l)•Inallothercases,Wt(xd→y)=0.Alinkx1→yattachesxtoybutdoesnotplaceyinsidethesmallestbracketcoveringx.Suchlinksarethereforecreatedinthesecondcaseabove,whentheattachmentindicationismixed.Toexplainthethirdcase,recallthats(l)>0meansthatthelabellisstrongerthanStoponAxi.Thisimpliesalinkunlessthepropertiesofwblockit.Onewayinwhichwcanblockthelinkistohaveapositivestrengthforthelinkintheoppositedirec-tion.Anotherwayinwhichthepropertiesofwcanblockthelinkisifl=(w,0)andAwσ(Out)<0,thatis,ifthelearningprocesshasexplicitlydeter-minedthatnooutboundlinkfromw(whichrepre-sentsxinthiscase)ispossible.Thesameconclu-sioncannotbedrawnfromanegativevaluefortheInpropertywhenl=(w,1)because,aswithstan-darddependencies,aworddeterminesitsoutboundlinksmuchmorestronglythanitsinboundlinks.6ExperimentsTheincrementalparserwastestedontheWallStreetJournalandNegraCorpora.2ParsingaccuracywasevaluatedonthesubsetsWSJXandNegraXofthesecorporacontainingsentencesoflengthatmostX(excludingpunctuation).Someofthesesubsetswereusedforscoringin(KleinandManning,2004;Bod,2006a;Bod,2006b).Ialsousethesamepreci-sionandrecallmeasuresusedinthosepapers:mul-tiplebracketsandbracketscoveringasinglewordwerenotcounted,butthetopbracketwas.Theincrementalparserlearnswhileparsing,anditcould,inprinciple,simplybeevaluatedforasin-glepassofthedata.But,becausethequalityoftheparsesoftheﬁrstsentenceswouldbelow,Iﬁrsttrainedonthefullcorpusandthenmeasuredpars-ingaccuracyonthecorpussubset.Bytrainingonthefullcorpus,theprocedurediffersfromthatofKlein,ManningandBodwhoonlytrainonthesub-setofboundedlengthsentences.However,thisex-cludestheinductionofparts-of-speechforparsingfromplaintext.WhenKleinandManninginducetheparts-of-speech,theydosofromamuchlargercorpuscontainingthefullWSJtreebanktogetherwithadditionalWSJnewswire(KleinandManning,2002).Thecomparisonbetweenthealgorithmsre-mains,therefore,valid.Table1givestwobaselinesandtheparsingre-sultsforWSJ10,WSJ40,Negra10andNegra40forrecentunsupervisedparsingalgorithms:CCM2IalsotestedtheincrementalparserontheChineseTree-bankversion5.0,achievinganF1scoreof54.6onCTB10and38.0onCTB40.Becausethisversionofthetreebankisnewerandclearlydifferentfromthatusedbypreviouspapers,there-sultsarenotcomparableandonlygivenhereforcompleteness.391

andDMV+CCM(KleinandManning,2004),U-DOP(Bod,2006b)andUML-DOP(Bod,2006a).Themiddlepartofthetablegivesresultsforpars-ingfrompart-of-speechsequencesextractedfromthetreebankwhilethebottompartofthetablegivenresultsforparsingfromplaintext.Resultsforthein-crementalparseraregivenforlearningandparsingfromlefttorightandfromrighttoleft.Theﬁrstbaselineisthestandardright-branchingbaseline.Thesecondbaselinemodiﬁesright-branchingbyusingpunctuationinthesamewayastheincrementalparser:brackets(exceptthetopone)arenotallowedtocontainstoppingpunctuation.Itcanbeseenthatpunctuationaccountsformerelyasmallpartoftheincrementalparser’simprovementovertheright-branchingheuristic.Comparingthetwoalgorithmsparsingfromplaintext(ofWSJ10),itcanbeseenthattheincrementalparserhasasomewhathighercombinedF1score,withbetterprecisionbutworserecall.Thisisbe-causeKleinandManning’salgorithms(aswellasBod’s)alwaysgeneratebinaryparsetrees,whileherenosuchconditionisimposed.Thesmalldiffer-encebetweentherecall(76.2)andprecision(75.6)oftheincrementalparsershowsthatthenumberofbracketsinducedbytheparserisveryclosetothatofthecorpus3andthattheparsercapturesthesamedepthofsyntacticstructureasthatwhichwasusedbythecorpusannotators.Incrementalparsingfromrighttoleftachievesre-sultsclosetothoseofparsingfromlefttoright.Thisshowsthattheincrementalparserhasnobuilt-inbiasforrightbranchingstructures.4Theslightdegra-dationinperformancemaysuggestthatlanguageshouldnot,afterall,beprocessedbackwards.Whileachievingstateoftheartaccuracy,thealgo-rithmalsoprovedtobefast,parsing(ona1.86GHzCentrinolaptop)atarateofaround4000words/sec.andlearning(includingparsing)atarateof3200–3600words/sec.Theeffectofsentencelengthonparsingspeedissmall:thefullWSJcorpuswasparsedat3900words/sec.whileWSJ10wasparsedat4300words/sec.3Thealgorithmproduced35588bracketscomparedwith35302bracketsinthecorpus.4IwouldliketothankAlexanderClarkforsuggestingthistest.7ConclusionsTheunsupervisedparserIpresentedhereattemptstomakeuseofseveraluniversalpropertiesofnat-urallanguages:itcapturestheskewnessofsyntac-tictreesinitssyntacticrepresentation,restrictsthesearchspacebyprocessingutterancesincrementally(ashumansdo)andreliesontheZipﬁandistributionofwordstoguideitsparsingdecisions.Itusesanelementarybootstrappingprocesstodeducetheba-sicpropertiesofthelanguagebeingparsed.Theal-gorithmseemstosuccessfullycapturesomeofthesebasicproperties,butcanbefurtherreﬁnedtoachievehighqualityparsing.Thecurrentalgorithmisagoodstartingpointforsuchreﬁnementbecauseitissoverysimple.AcknowledgmentsIwouldliketothankDickdeJonghformanyhoursofdiscussion,andRemkoScha,ReutTsarfatyandJelleZuidemaforreadingandcommentingonvariousversionsofthispaper.ReferencesRensBod.2006a.Anall-subtreesapproachtounsuper-visedparsing.InProceedingsofCOLING-ACL2006.RensBod.2006b.UnsupervisedparsingwithU-DOP.InProceedingsofCoNLL10.AlexanderClark.2000.Inducingsyntacticcategoriesbycontextdistributionclustering.InProceedingsofCoNLL4.MatthewW.Crocker,MartinPickering,andCharlesClifton.2000.ArchitecturesandMechanismsforLanguageProcessing.CambridgeUniversityPress.DanKleinandChristopherD.Manning.2002.Agener-ativeconstituent-contextmodelforimprovedgrammarinduction.InProceedingsofACL40,pages128–135.DanKleinandChristopherD.Manning.2004.Corpus-basedinductionofsyntacticstructure:Modelsofde-pendencyandconstituency.InProceedingsofACL42.DavidMcClosky,EugeneCharniak,andMarkJohnson.2006.Effectiveself-trainingforparsing.InProceed-ingsofHLT-NAACL2006.HinrichSch¨utze.1995.Distributionalpart-of-speechtagging.InProceedingsofEACL7.PatrickSturtandMatthewW.Crocker.1996.Mono-tonicsyntacticprocessing:Across-linguisticstudyofattachmentandreanalysis.LanguageandCognitiveProcesses,11(5):449–492.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 392–399,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

392

k-bestSpanningTreeParsingKeithHallCenterforLanguageandSpeechProcessingJohnsHopkinsUniversityBaltimore,MD21218keithhall@jhu.eduAbstractThispaperintroducesaMaximumEntropydependencyparserbasedonanefﬁcientk-bestMaximumSpanningTree(MST)algo-rithm.Althoughrecentworksuggeststhattheedge-factoredconstraintsoftheMSTal-gorithmsigniﬁcantlyinhibitparsingaccu-racy,weshowthatgeneratingthe50-bestparsesaccordingtoanedge-factoredmodelhasanoracleperformancewellabovethe1-bestperformanceofthebestdependencyparsers.Thismotivatesourparsingap-proach,whichisbasedonrerankingthek-bestparsesgeneratedbyanedge-factoredmodel.Oracleparseaccuracyresultsarepresentedfortheedge-factoredmodeland1-bestresultsforthererankeroneightlan-guages(sevenfromCoNLL-XandEnglish).1IntroductionTheMaximumSpanningTreealgorithm1wasre-centlyintroducedasaviablesolutionfornon-projectivedependencyparsing(McDonaldetal.,2005b).Thedependencyparsingproblemisnat-urallyaspanningtreeproblem;however,efﬁ-cientspanning-treeoptimizationalgorithmsassumeacostfunctionwhichassignsscoresindependentlytoedgesofthegraph.Independencyparsing,thiseffectivelyconstrainsthesetofmodelstothosewhichindependentlygenerateparent-childpairs;1InthispaperwedealonlywithMSTsondirectedgraphs.Theseareoftenreferredtointhegraph-theoryliteratureasMax-imumSpanningArborescences.theseareknownasedge-factoredmodels.Thesemodelsarelimitedtorelativelysimplefeatureswhichexcludelinguisticconstructssuchasverbsub-categorization/valency,lexicalselectionalpref-erences,etc.2Inordertoexplorearichsetofsyntacticfea-turesintheMSTframework,wecaneitherapprox-imatetheoptimalnon-projectivesolutionasinMc-DonaldandPereira(2006),orwecanusethecon-strainedMSTmodeltoselectasubsetofthesetofdependencyparsestowhichwethenapplyless-constrainedmodels.Anefﬁcientalgorithmforgen-eratingthek-bestparsetreesforaconstituency-basedparserwaspresentedinHuangandChiang(2005);avariationofthatalgorithmwasusedforgeneratingprojectivedependencytreesforparsinginDreyeretal.(2006)andfortraininginMcDonaldetal.(2005a).However,priortothispaper,anefﬁ-cientnon-projectivek-bestMSTdependencyparserhasnotbeenproposed.3Inthispaperweshowthatthena¨ıveedge-factoredmodelsareeffectiveatselectingsetsofparsesonwhichtheoracleparseaccuracyishigh.Theor-acleparseaccuracyforasetofparsetreesisthehighestaccuracyforanyindividualtreeintheset.Weshowthatthe1-bestaccuracyandoracleaccu-racycandifferbyasmuchasanabsolute9%whentheoracleiscomputedoverasmallsetgeneratedbyedge-factoredmodels(k=50).2Labelededge-factoredmodelscancaptureselectionalpref-erence;however,theunlabeledmodelspresentedherearelim-itedtomodelinghead-childrelationshipswithoutpredictingthetypeofrelationship.3TheworkofMcDonaldetal.(2005b)wouldalsobeneﬁtfromak-bestnon-projectiveparserfortraining.393

ROOTtwoshareahousealmostdevoidoffurniture.Figure1:AdependencygraphforanEnglishsen-tenceinourdevelopmentset(PennWSJsection24):Twoshareahousealmostdevoidoffurniture.Thecombinationoftwodiscriminativelytrainedmodels,ak-bestMSTparserandaparsetreereranker,resultsinanefﬁcientparserthatincludescomplextree-basedfeatures.Intheremainderofthepaper,weﬁrstdescribethecoreofourparser,thek-bestMSTalgorithm.Wethenintroducethefea-turesthatweusetocomputeedge-factoredscoresaswellastree-basedscores.Following,weoutlinethetechnicaldetailsofourtrainingprocedureandﬁ-nallywepresentempiricalresultsfortheparseronsevenlanguagesfromtheCoNLL-Xshared-taskandadependencyversionoftheWSJPennTreebank.2MSTinDependencyParsingWorkonstatisticaldependencyparsinghasutilizedeitherdynamic-programming(DP)algorithmsorvariantsoftheEdmonds/Chu-LiuMSTalgorithm(seeTarjan(1977)).TheDPalgorithmsaregener-allyvariantsoftheCKYbottom-upchartparsingal-gorithmsuchasthatproposedbyEisner(1996).TheEisneralgorithmefﬁciently(O(n3))generatespro-jectivedependencytreesbyassemblingstructuresovercontiguouswordsinacleverwaytominimizebook-keeping.OtherDPsolutionsuseconstituency-basedparserstoproducephrase-structuretrees,fromwhichdependencystructuresareextracted(Collinsetal.,1999).AshortcomingoftheDP-basedap-proachesisthattheyareunabletogeneratenon-projectivestructures.However,non-projectivityisnecessarytocapturesyntacticphenomenainmanylanguages.McDonaldetal.(2005b)introducedamodelfordependencyparsingbasedontheEdmonds/Chu-Liualgorithm.Theworkwepresenthereextendstheirworkbyexploringak-bestversionoftheMSTalgo-rithm.Inparticular,weconsideranalgorithmpro-posedbyCamerinietal.(1980)whichhasaworst-casecomplexityofO(kmlog(n)),wherekisthenumberofparseswewant,nisthenumberofwordsintheinputsentence,andmisthenumberofedgesinthehypothesisgraph.ThiscanbereducedtoO(kn2)indensegraphs4bychoosingappropriatedatastructures(Tarjan,1977).Underthemodelsconsideredhere,allpairsofwordsareconsideredascandidateparents(children)ofanother,resultinginafullyconnectedgraph,thusm=n2.Inordertoincorporatesecond-orderfeatures(speciﬁcally,siblingfeatures),McDonaldetal.pro-posedadependencyparserbasedontheEisneralgo-rithm(McDonaldandPereira,2006).Thesecond-orderfeaturesallowformorecomplexphrasalrela-tionshipsthantheedge-factoredfeatureswhichonlyincludeparent/childfeatures.TheiralgorithmﬁndsthebestsolutionaccordingtotheEisneralgorithmandthensearchesforthesinglevalidedgechangethatincreasesthetreescore.Thealgorithmiter-atesuntilnobettersingleedgesubstitutioncanim-provethescoreofthetree.Thisgreedyapproxi-mationallowsforsecond-orderconstraintsandnon-projectivity.TheyfoundthatapplyingthismethodtotreesgeneratedbytheEisneralgorithmusingsecond-orderfeaturesperformsbetterthanapplyingittothebesttreeproducedbytheMSTalgorithmwithﬁrst-order(edge-factored)features.Inthispaperweprovideanewevaluationoftheefﬁcacyofedge-factoredmodels,k-bestoraclere-sults.Weshowthatevenwhenkissmall,theedge-factoredmodelsselectk-bestsetswhichcon-taingoodparses.Furthermore,thesegoodparsesareevenbetterthantheparsesselectedbythebestdependencyparsers.2.1k-bestMSTAlgorithmThek-bestMSTalgorithmweintroduceinthispa-peristhealgorithmdescribedinCamerinietal.(1980).Forproofsofcomplexityandcorrectness,wedefertotheoriginalpaper.Thissectionisin-tendedtoprovidetheintuitionsbehindthealgo-rithmandallowforanunderstandingofthekeydata-structuresnecessarytoensurethetheoreticalguar-antees.4Adensegraphisoneinwhichthenumberofedgesisclosetothenumberofedgesinafullyconnectedgraph(i.e.,n2).394

BC49851110115v1v2v3R4-2-35-101-5v4v3Rv1v2v1v210v1v21011v4v1v2v3v1v2v3v3v4v3-2v1v2v4v3v4v3-25-7-4-3v5Re32e23eR2eR1e13e314-2-35-101-5v4v3Re32e23eR2eR1e13e314-2-35-101-5v4v3Re32e23eR2eR1e13e31eR1eR2eR3v1v2v4v3v1v2v4v3v548851110115v1v2v3R-7-4-3v5ReR1eR2eR3v5R-3eR1e23e31e31Gv1v2v4v3v5S1S2S3S4S5S6S7Figure2:Simulated1-bestMSTalgorithm.LetG={V,E}beadirectedgraphwhereV={R,v1,...,vn}andE={e11,e12,...,e1n,e21,...,enn}.Werefertoedgeeijastheedgethatisdirectedfromviintovjinthegraph.TheinitialdependencygraphinFigure2(columnG)containsthreeregularnodesandarootnode.Algorithm1isaversionoftheMSTalgorithmaspresentedbyCamerinietal.(1980);subtletiesofthealgorithmhavebeenomitted.ArgumentsY(abranching5)andZ(asetofedges)areconstraintsontheedgesthatcanbepartofthesolution,A.EdgesinYarerequiredtobeinthesolutionandedgesin5Abranchingisasubgraphthatcontainsnocyclesandnomorethanoneedgedirectedintoeachnode.Algorithm1Sketchof1-bestMSTalgorithmprocedureBEST(G,Y,Z)G=(G∪Y)−ZB=∅C=V5:forunvisitedvertexvi∈Vdomarkviasvisitedgetbestin-edgeb∈{ejk:k=i}forviB=B∪bβ(vi)=b10:ifBcontainsacycleCthencreateanewnodevn+1C=C∪vn+1makeallnodesofCchildrenofvn+1inCCOLLAPSEallnodesofCintovn+115:ADDvn+1tolistofunvisitedverticesn=n+1B=B−Cendifendfor20:EXPANDCchoosingbestwaytobreakcyclesReturnbestA={b∈E|∃v∈V:β(v)=b}andCendprocedureZcannotbepartofthesolution.ThebranchingCstoresahierarchicalhistoryofcyclecollapses,en-capsulatingembeddedcyclesandallowingforanex-pandingprocedure,whichbreakscycleswhilemain-taininganoptimalsolution.Figure2presentsaviewofthealgorithmwhenrunonathreenodegraphs(plusaspeciﬁedrootnode).StepsS1,S2,S4,andS5depicttheprocess-ingoflines5to8,recordinginβthebestinputedgesforeachvertex.StepsS3andS6showtheprocessofcollapsingacycleintoanewnode(lines10to16).Themainloopofthealgorithmprocesseseachvertexthathasnotyetbeenvisited.Welookupthebestincomingedge(whichisstoredinapriority-queue).ThisvalueisrecordedinβandtheedgeisaddedtothecurrentbestgraphB.WethenchecktoseeifaddingthisnewedgewouldcreateacycleinB.Ifso,wecreateanewnodeandcollapsethecycleintoit.ThiscanbeseeninStepS3inFigure2.Theprocessofcollapsingacycleintoanodein-volvesremovingtheedgesinthecyclefromB,andadjustingtheweightsofalledgesdirectedintoanynodeinthecycle.Theweightsareadjustedsothattheyreﬂecttherelativedifferenceofchoosingthenewin-edgeratherthantheedgeinthecycle.InstepS3,observethatedgeeR1hadaweightof5,butnowthatitpointsintothenewnodev4,wesubtracttheweightoftheedgee21thatalsopointedintov1,395

whichwas10.Additionally,werecordinCthere-lationshipbetweenthenewnodev4andtheoriginalnodesv1andv2.Thisprocesscontinuesuntilwehavevisitedalloriginalandnewlycreatednodes.Atthatpoint,weexpandthecyclesencodedinC.ForeachnodenotoriginallyinG(e.g.,v5,v4),weretrievetheedgeerpointingintothisnode,recordedinβ.WeidentifythenodevstowhicherpointedintheoriginalgraphGandsetβ(vs)=er.Algorithm2Sketchofnext-bestMSTalgorithmprocedureNEXT(G,Y,Z,A,C)δ←+∞forunvisitedvertexvdogetbestin-edgebforv5:ifb∈A−Ythenf←alternateedgeintovifswappingfwithbresultsinsmallerδthenupdateδ,lete←fendif10:endififbformsacyclethenResolveasin1-bestendifendfor15:ReturnedgeeandδendprocedureAlgorithm2returnsthesingleedge,e,ofthe1-bestsolutionAthat,whenremovedfromthegraph,resultsinagraphforwhichthebestsolutionisthenextbestsolutionafterA.Additionally,itreturnsδ,thedifferenceinscorebetweenAandthenextbesttree.ThebranchingCispassedinfromAlgo-rithm1andisusedheretoefﬁcientlyidentifyalter-nateedges,f,foredgee.YandZinAlgorithms1and2areusedtocon-structthenextbestsolutionsefﬁciently.WecallGY,Zaconstrainedgraph;theconstraintsbeingthatYrestrictsthein-edgesforasubsetofnodes:foreachvertexwithanin-edgeinY,onlytheedgeofYcanbeanin-edgeofthevertex.Also,edgesinZareremovedfromthegraph.Aconstrainedspan-ningtreeforGY,Z(atreecoveringallnodesinthegraph)mustsatisfy:Y⊆A⊆E−Z.LetAbethe(constrained)solutiontoa(con-strained)graphandletebetheedgethatleadstothenextbestsolution.Thethird-bestsolutioniseitherthesecond-bestsolutiontoGY,{Z∪e}orthesecond-bestsolutiontoG{Y∪e},Z.Thek-bestrankingal-gorithmusesthisfacttoincrementallypartitionthesolutionspace:foreachsolution,thenextbesteitherwillincludeeorwillnotincludee.Algorithm3k-bestMSTrankingalgorithmprocedureRANK(G,k)A,C←best(E,V,∅,∅)(e,δ)←next(E,V,∅,∅,A,C)bestList←A5:Q←enqueue(s(A)−δ,e,A,C,∅,∅)forj←2tokdo(s,e,A,C,Y,Z)=dequeue(Q)Y0=Y∪eZ0=Z∪e10:A0,C0←best(E,V,Y,Z0)bestList←A0e0,δ0←next(E,V,Y0,Z,A0,C0)Q←enqueue(s(A)−δ0,e0,A0,C0,Y0,Z)e0,δ0←next(E,V,Y,Z0,A0,C0)15:Q←enqueue(s(A)−δ0,e0,A0,C0,Y,Z0)endforReturnbestListendprocedureThek-bestrankingproceduredescribedinAlgo-rithm3usesapriorityqueue,Q,keyedontheﬁrstparametertoenqueuetokeeptrackofthehorizonofnextbestsolutions.Thefunctions(A)returnsthescoreassociatedwiththetreeA.Notethatineachiterationtherearetwonewelementsenqueuedrep-resentingthesetsGY,{Z∪e}andG{Y∪e},Z.BothAlgorithms1and2runinO(mlog(n))timeandcanruninquadratictimefordensegraphswiththeuseofanefﬁcientpriority-queue6(i.e.,basedonaFibonacciheap).Algorithm3runsincon-stanttime,resultinginanO(kmlogn)algorithm(orO(kn2)fordensegraphs).3DependencyModelsEachofthetwostagesofourparserisbasedonadis-criminativetrainingprocedure.Theedge-factoredmodelisbasedonaconditionallog-linearmodeltrainedusingtheMaximumEntropyconstraints.3.1Edge-factoredMSTModelOnewayinwhichdependencyparsingdiffersfromconstituencyparsingisthatthereisaﬁxedamountofstructureineverytree.Adependencytreeforasen-tenceofnwordshasexactlynedges,7eachrepre-6Eachvertexkeepsapriorityqueueofcandidateparents.Whenacyclesiscollapsed,thenewvertexinheritstheunionofqueuesassociatedwiththeverticesofthecycle.7Weassumeeachtreehasarootnode.396

sentingasyntacticorsemanticrelationship,depend-ingonthelinguisticmodelassumedforannotation.Aspanningtree(equivalently,adependencyparse)isasubgraphforwhicheachnodehasonein-edge,therootnodehaszeroin-edges,andtherearenocy-cles.Edge-factoredfeaturesaredeﬁnedovertheedgeandtheinputsentence.Foreachofthen2par-ent/childpairs,weextractthefollowingfeatures:Node-typeTherearethreebasicnode-typefea-tures:wordform,morphologicallyreducedlemma,andpart-of-speech(POS)tag.TheCoNLL-Xdataformat8describestwopart-of-speechtagtypes,wefoundthatfeaturesderivedfromthecoarsetagsaremorereliable.Wecon-siderbothunigram(parentorchild)andbigram(compositeparent/child)features.Werefertoparentfeatureswiththepreﬁxp-andchildfea-turewiththepreﬁxc-;forexample:p–pos,p–form,c–pos,andc–form.InourmodelweusebothwordformandPOStagandincludethecompositeform/POSfeatures:p–form/c–posandp–pos/c–form.BranchAbinaryfeaturewhichindicateswhetherthechildistotheleftorrightoftheparentintheinputstring.Additionally,weprovidecompositefeaturesp–pos/branchandp–pos/c–pos/branch.DistanceThenumberofwordsoccurringbetweentheparentandchildword.Thesedistancesarebucketedinto7buckets(1through6plusanad-ditionalsinglebucketfordistancesgreaterthan6).Additionally,thisfeatureiscombinedwithnode-typefeatures:p–pos/dist,c–pos/dist,p–pos/c–pos/dist.InsidePOStagsofthewordsbetweentheparentandchild.Acountofeachtagthatoccursisrecorded,thefeatureisidentiﬁedbythetagandthefeaturevalueisdeﬁnedbythecount.Addi-tionalcompositefeaturesareincludedcombin-ingtheinsideandnode-type:foreachtypetithecompositefeaturesare:p–pos/ti,c–pos/ti,p–pos/c–pos/ti.8The2006CoNLL-Xdataformatcanbefoundon-lineat:http://nextens.uvt.nl/˜conll/.OutsideExactlythesameastheInsidefeatureex-ceptthatitisdeﬁnedoverthefeaturestotheleftandrightofthespancoveredbythisparent-childpair.Extra-FeatsAttribute-valuepairsfromtheCoNLLFEATSﬁeldincludingcombinationswithpar-ent/childnode-types.Thesefeaturesrepresentword-levelannotationsprovidedinthetree-bankandincludemorphologicalandlexical-semanticfeatures.ThesedonotexistintheEn-glishdata.InsideEdgeSimilartoInsidefeatures,butonlyincludesnodesimmediatelytoleftandrightwithinthespancoveredbytheparent/childpair.WeincludethefollowingfeatureswhereilandiraretheinsideleftandrightPOStagsandipistheinsidePOStagclosesttothepar-ent:il/ir,p–pos/ip,p–pos/il/ir/c–pos,OutsideEdgeAnOutsideversionoftheInsideEdgefeaturetype.ManyofthefeaturesabovewereintroducedinMcDonaldetal.(2005a);speciﬁcally,thenode-type,inside,andedgefeatures.Thenumberoffea-turescangrowquitelargewhenformorlemmafea-turesareincluded.Inordertohandlelargetrainingsetswithalargenumberoffeaturesweintroduceabagging-basedapproach,describedinSection4.2.3.2Tree-basedRerankingModelThesecondstageofourdependencyparserisarerankerthatoperatesontheoutputofthek-bestMSTparser.Featuresinthismodelarenotcon-strainedasintheedge-factoredmodel.Manyofthemodelfeatureshavebeeninspiredbytheconstituency-basedfeaturespresentedinCharniakandJohnson(2005).Wehavealsoincludedfeaturesthatexploitnon-projectivitywherepossible.Thenode-typeisthesameasdeﬁnedfortheMSTmodel.MSTscoreThescoreofthisparsegivenbytheﬁrst-stageMSTmodel.SiblingThePOS-tagofimmediatesiblings.In-tendedtocapturethepreferenceforparticularimmediatesiblingssuchasmodiﬁers.ValencyCountofthenumberofchildrenforeachword(indexedbyPOS-tagoftheword).These397

countsarebucketedinto4buckets.Forex-ample,afeaturemaylooklikep–pos=VB/v=4,meaningthePOStagoftheparentis‘VB’andithad4dependents.Sub-categorizationAstringrepresentingthese-quenceofchildPOStagsforeachparentPOS-tag.AncestorGrandparentandgreatgrandparentPOS-tagforeachword.Compositefeaturesaregen-eratedwiththelabelc–pos/p–pos/gp–posandc–pos/p–pos/ggp–pos(wheregpisthegrand-parentandggpisthegreatgrand-parent).EdgePOS-tagtotheleftandrightofthesubtree,bothinsideandoutsidethesubtree.Forexam-ple,sayasubtreewithparentPOS-tagp–posspansfromitoj,weincludecompositeout-sidefeatures:p–pos/ni−1–pos/nj+1–pos,p–pos/ni−1–pos,p–pos/nj+1–pos;andcompositeinsidefeatures:p–pos/ni+1–pos/nj−1–pos,p–pos/ni+1–pos,p–pos/nj−1–pos.BranchingFactorAveragenumberofleft/rightbranchingnodesperPOS-tag.Additionally,weincludeabooleanfeatureindicatingtheoverallleft/rightpreference.DepthDepthofthetreeanddepthnormalizedbysentencelength.HeavyNumberofdominatednodesperPOS-tag.WealsoincludetheaveragenumberofnodesdominatedbyeachPOS-tag.4MaxEntTrainingWehaveadoptedtheconditionalMaximumEntropy(MaxEnt)modelingparadigmasoutlinedinChar-niakandJohnson(2005)andRiezleretal.(2002).Wecanpartitionthetrainingexamplesintoindepen-dentsubsets,Ys:fortheedge-factoredMSTmodels,eachsetrepresentsawordanditscandidateparents;forthereranker,eachsetrepresentsthek-besttreesforaparticularsentence.Wewishtoestimatetheconditionaldistributionoverhypothesesinthesetyi,giventheset:p(yi|Ys)=exp(Pkλkfik)Pj:yj∈Ysexp(Pk0λk0fjk0),wherefikisthekthfeaturefunctioninthemodelforexampleyi.4.1MSTTrainingOurMSTparsertrainingprocedureinvolvesenu-meratingthen2potentialtreeedges(parent/childpairs).UnlikethetrainingprocedureemployedbyMcDonaldetal.(2005b)andMcDonaldandPereira(2006),weprovidepositiveandnegativeexamplesinthetrainingdata.Anodecanhaveatmostoneparent,providinganaturalsplitofthen2trainingexamples.Foreachnodeni,wewishtoestimateadistributionovernnodes9aspotentialparents,p(vi,eji|ei),theprobabilityofthecorrectparentofvibeingvjgiventhesetofedgesassociatedwithitscandidateparentsei.Wecallthistheparent-predictionmodel.4.2MSTBaggingThecomplexityofthetrainingprocedureisafunc-tionofthenumberoffeaturesandthenumberofex-amples.Forlargedatasets,weuseanensembletech-niqueinspiredbyBagging(Breiman,1996).Bag-gingisgenerallyusedtomitigatehighvarianceindatasetsbysampling,withreplacement,fromthetrainingset.Giventhatwewishtoincludesomeofthelessfrequentexamplesandthereforearenotnecessarilyavoidinghighvariance,wepartitionthedataintodisjointsets.Foreachofthesets,wetrainamodelindepen-dently.Furthermore,weonlyallowtheparame-terstobechangedforthosefeaturesobservedinthetrainingset.Atinferencetime,weapplyeachmodeltothetrainingdataandthencombinethepredictionprobabilities.˜pθ(yi|Ys)=maxmpθm(yi|Ys)(1)˜pθ(yi|Ys)=1MXmpθm(yi|Ys)(2)˜pθ(yi|Ys)= Ympθm(yi|Ys)!1/M(3)˜pθ(yi|Ys)=MPm1pθm(yi|Ys)(4)Equations1,2,3,and4arethemaximum,aver-age,geometricmean,andharmonicmean,respec-tively.Weperformedanexplorationoftheseonthe9Recallthatinadditiontothen−1othernodesinthegraph,thereisarootnodeforwhichweknowhasnoparents.398

developmentdataandfoundthatthegeometricmeanproducesthebestresults(Equation3);however,weobservedonlyverysmalldifferencesintheaccuracyamongmodelswhereonlythecombinationfunctiondiffered.4.3RerankerTrainingThesecondstageofparsingisperformedbyourtree-basedreranker.Theinputtothererankerisalistofkparsesgeneratedbythek-bestMSTparser.Foreachinputsentence,thehypothesissetisthekparses.Atinferencetime,predictionsaremadein-dependentlyforeachhypothesissetYsandthereforethenormalizationfactorcanbeignored.5EmpiricalEvaluationTheCoNLL-Xsharedtaskondependencyparsingprovideddataforanumberoflanguagesinacom-mondataformat.Wehaveselectedsevenoftheselanguagesforwhichthedataisavailabletous.Ad-ditionally,wehaveautomaticallygeneratedadepen-dencyversionofthePennWSJtreebank.10Asweareonlyinterestedinthestructuralcomponentofaparseinthispaper,wepresentresultsforunlabeleddependencyparsing.Asecondlabelingstagecanbeappliedtogetlabeleddependencystructuresasde-scribedin(McDonaldetal.,2006).InTable1wereporttheaccuracyforsevenoftheCoNLLlanguagesandEnglish.11Already,atk=50,weseetheoraclerateclimbasmuchas9.25%overthe1-bestresult(Dutch).Continuingtoincreasethesizeofthek-bestlistsaddstotheoracleaccuracy,buttherelativeimprovementappearstobeincreasingatalogarithmicrate.Thek-bestparserisusedbothtotrainthek-bestrerankerand,atinfer-encetime,toselectasetofhypothesestorerank.Itisnotnecessarythattrainingisdonewiththesamesizehypothesissetastest,weexplorethematchedandmismatchedconditionsinourrerankingexperi-ments.10ThePennWSJtreebankwasconvertedusingthecon-versionprogramdescribedin(JohanssonandNugues,2007)andavailableonthewebat:http://nlp.cs.lth.se/pennconverter/11TheBestReportedresultsisfromtheCoNLL-Xcompeti-tion.ThebestresultreportedforEnglishistheCharniakparser(withoutreranking)onSection23oftheWSJTreebankusingthesamehead-ﬁndingrulesasfortheevaluationdata.Table2showsthererankingresultsforthesetoflanguages.Foreachlanguage,weselectmodelpa-rametersonadevelopmentsetpriortorunningonthetestdata.Theseparametersincludeafeaturecountthreshold(theminimumnumberofobserva-tionsofafeaturebeforeitisincludedinamodel)andamixtureweightcontrollingthecontributionofaquadraticregularizer(usedinMaxEnttraining).ForCzech,German,andEnglish,weusetheMSTbaggingtechniquewith10bags.Thesetestresultsareforthemodelswhichperformedbestonthede-velopmentset(using50-bestparses).Weseeminorimprovementsoverthe1-bestbase-lineMSToutput(repeatedinthistableforcompar-ison).Webelievethisisduetotheoverwhelmingnumberofparametersinthererankingmodelsandtherelativelysmallamountoftrainingdata.Inter-estingly,increasingthenumberofhypotheseshelpsforsomelanguagesandhurtstheothers.6ConclusionAlthoughtheedge-factoredconstraintsofMSTparsersinhibitaccuracyin1-bestparsing,edge-factoredmodelsareeffectiveatselectinghighaccu-racyk-bestsets.WehaveintroducedtheCamerinietal.(1980)k-bestMSTalgorithmandhaveshownhowtoefﬁcientlytrainMaxEntmodelsfordepen-dencyparsing.Additionally,wepresentedauni-ﬁedmodelingandtrainingsettingforourtwo-stageparser;MaxEnttrainingisusedtoestimatethepa-rametersinbothmodels.Wehaveintroducedaparticularensembletechniquetoaccommodatethelargetrainingsetsgeneratedbytheﬁrst-stageedge-factoredmodelingparadigm.Finally,wehavepre-sentedarerankerwhichattemptstoselectthebesttreefromthek-bestset.Infutureworkwewishtoexploremorerobustfeaturesetsandexperimentwithfeatureselectiontechniquestoaccommodatethem.AcknowledgmentsThisworkwaspartiallysupportedbyU.S.NSFgrantsIIS–9982329andOISE–0530118.WethankRyanMcDonaldfordirectingustotheCamerinietal.paperandLiangHuangforinsightfulcomments.399

LanguageBestOracleAccuracyReportedk=1k=10k=50k=100k=500Arabic79.3477.9280.7282.1883.0384.47Czech87.3083.5688.5090.8891.8093.50Danish90.5889.1292.8994.7995.2996.59Dutch83.5781.0587.4390.3091.2893.12English92.3685.0489.0491.1291.8793.42German90.3887.0291.5193.3994.0795.47Portuguese91.3689.8693.1194.8595.3996.47Swedish89.5486.5091.2093.3793.8395.42Table1:k-bestMSToracleresults.The1-bestresultsrepresenttheperformanceoftheparserinisolation.ResultsarereportedfortheCoNLLtestsetandforEnglish,onSection23ofthePennWSJTreebank.LanguageBestRerankedAccuracyReported1-best10-best50-best100-best500-bestArabic79.3477.6178.0678.0277.9477.76Czech87.3083.5683.9484.1484.4884.46Danish90.5889.1289.4889.7689.6889.74Dutch83.5781.0582.0182.9182.8383.21English92.3685.0486.5487.2287.3887.81German90.3887.0288.2488.7288.7688.90Portuguese91.3689.3890.0089.9890.0290.02Swedish89.5486.5087.8788.2188.2688.53Table2:Second-stageresultsfromthek-bestparserandreranker.TheBestReportedand1-bestﬁeldsarecopiedfromtable1.Onlynon-lexicalfeatureswereusedforthererankingmodels.ReferencesLeoBreiman.1996.Baggingpredictors.MachineLearning,26(2):123–140.PaoloM.Camerini,LuigiFratta,andFrancescoMafﬁoli.1980.Thekbestspanningarborescencesofanetwork.Networks,10:91–110.EugeneCharniakandMarkJohnson.2005.Coarse-to-ﬁnen-bestparsingandMaxEntdiscriminativereranking.InPro-ceedingsofthe43rdAnnualMeetingoftheAssociationforComputationalLinguistics.MichaelCollins,LanceRamshaw,JanHajiˇc,andChristophTillmann.1999.AstatisticalparserforCzech.InPro-ceedingsofthe37thannualmeetingoftheAssociationforComputationalLinguistics,pages505–512.MarkusDreyer,DavidA.Smith,andNoahA.Smith.2006.Vineparsingandminimumriskrerankingforspeedandpre-cision.InProceedingsoftheTenthConferenceonCompu-tationalNaturalLanguageLearning.JasonEisner.1996.Threenewprobabilisticmodelsforde-pendencyparsing:Anexploration.InProceedingsofthe16thInternationalConferenceonComputationalLinguistics(COLING),pages340–345.LiangHuangandDavidChiang.2005.Betterk-bestparsing.InProceedingsofthe9thInternationalWorkshoponParsingTechnologies.RichardJohanssonandPierreNugues.2007.Extendedconstituent-to-dependencyconversionforEnglish.InPro-ceedingsofNODALIDA2007,Tartu,Estonia,May25-26.Toappear.RyanMcDonaldandFernandoPereira.2006.Onlinelearningofapproximatedependencyparsingalgorithms.InProceed-ingsoftheAnnualMeetingoftheEuropeanAssociationforComputationalLinguistics.RyanMcDonald,KobyCrammer,andFernandoPereira.2005a.Onlinelarge-margintrainingofdependencyparsers.InProceedingsofthe43ndAnnualMeetingoftheAssocia-tionforComputationalLinguistics.RyanMcDonald,FernandoPereira,KirilRibarov,andJanHajiˇc.2005b.Non-projectivedependencyparsingusingspanningtreealgorithms.InProceedingsofHumanLan-guageTechnologyConferenceandConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages523–530,October.RyanMcDonald,KevinLerman,andFernandoPereira.2006.Multilingualdependencyparsingwithatwo-stagediscrimi-nativeparser.InConferenceonNaturalLanguageLearning.StefanRiezler,TracyH.King,RonaldM.Kaplan,RichardCrouch,JohnT.IIIMaxwell,andMarkJohnson.2002.ParsingtheWallStreetJournalusingalexical-functionalgrammaranddiscriminativeestimationtechniques.InPro-ceedingsofthe40thAnnualMeetingoftheAssociationforComputationalLinguistics.MorganKaufmann.R.E.Tarjan.1977.Findingoptimalbranchings.Networks,7:25–35.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 400–407,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

400

Is the End of Supervised Parsing in Sight?   Rens Bod School of Computer Science University of St Andrews, ILLC, University of Amsterdam rb@cs.st-and.ac.uk     Abstract How far can we get with unsupervised parsing if we make our training corpus several orders of magnitude larger than has hitherto be attempted? We present a new algorithm for unsupervised parsing using an all-subtrees model, termed U-DOP*, which parses directly with packed forests of all binary trees. We train both on Penn’s WSJ data and on the (much larger) NANC corpus, showing that U-DOP* outperforms a treebank-PCFG on the standard WSJ test set. While U-DOP* performs worse than state-of-the-art supervised parsers on hand-annotated sentences, we show that the model outperforms supervised parsers when evaluated as a language model in syntax-based machine translation on Europarl. We argue that supervised parsers miss the fluidity between constituents and non-constituents and that in the field of syntax-based language modeling the end of supervised parsing has come in sight. 1    Introduction  A major challenge in natural language parsing is the unsupervised induction of syntactic structure. While most parsing methods are currently supervised or semi-supervised (McClosky et al. 2006; Henderson 2004; Steedman et al. 2003), they depend on hand-annotated data which are difficult to come by and which exist only for a few languages. Unsupervised parsing methods are becoming increasingly important since they operate with raw, unlabeled data of which unlimited quantities are available. There has been a resurgence of interest in unsupervised parsing during the last few years. Where van Zaanen (2000) and Clark (2001) induced unlabeled phrase structure for small domains like the ATIS, obtaining around 40% unlabeled f-score, Klein and Manning (2002) report 71.1% f-score on Penn WSJ part-of-speech strings ≤ 10 words (WSJ10) using a constituent-context model called CCM. Klein and Manning (2004) further show that a hybrid approach which combines constituency and dependency models, yields 77.6% f-score on WSJ10. While Klein and Manning’s approach may be described as an “all-substrings” approach to unsupervised parsing, an even richer model consists of an “all-subtrees” approach to unsupervised parsing, called U-DOP (Bod 2006). U-DOP initially assigns all unlabeled binary trees to a training set, efficiently stored in a packed forest, and next trains subtrees thereof on a held-out corpus, either by taking their relative frequencies, or by iteratively training the subtree parameters using the EM algorithm (referred to as “UML-DOP”). The main advantage of an all-subtrees approach seems to be the direct inclusion of discontiguous context that is not captured by (linear) substrings. Discontiguous context is important not only for learning structural dependencies but also for learning a variety of non-contiguous constructions such as nearest … to… or take … by surprise. Bod (2006) reports 82.9% unlabeled f-score on the same WSJ10 as used by Klein and Manning (2002, 2004). Unfortunately, his experiments heavily depend on a priori sampling of subtrees, and the model becomes highly inefficient if larger corpora are used or longer sentences are included. In this paper we will also test an alternative model for unsupervised all-subtrees 401

parsing, termed U-DOP*, which is based on the DOP* estimator by Zollmann and Sima’an (2005), and which computes the shortest derivations for sentences from a held-out corpus using all subtrees from all trees from an extraction corpus. While we do not achieve as high an f-score as the UML-DOP model in Bod (2006), we will show that U-DOP* can operate without subtree sampling, and that the model can be trained on corpora that are two orders of magnitude larger than in Bod (2006). We will extend our experiments to 4 million sentences from the NANC corpus (Graff 1995), showing that an f-score of 70.7% can be obtained on the standard Penn WSJ test set by means of unsupervised parsing. Moreover, U-DOP* can be directly put to use in bootstrapping structures for concrete applications such as syntax-based machine translation and speech recognition. We show that U-DOP* outperforms the supervised DOP model if tested on the German-English Europarl corpus in a syntax-based MT system. In the following, we first explain the DOP* estimator and discuss how it can be extended to unsupervised parsing. In section 3, we discuss how a PCFG reduction for supervised DOP  can be applied to packed parse forests. In section 4, we will go into an experimental evaluation of U-DOP* on annotated corpora, while in section 5 we will evaluate U-DOP* on unlabeled corpora in an MT application.   2     From DOP* to U-DOP*  DOP* is a modification of the DOP model in Bod (1998) that results in a statistically consistent estimator and in an efficient training procedure (Zollmann and Sima’an 2005). DOP* uses the all-subtrees idea from DOP: given a treebank, take all subtrees, regardless of size, to form a stochastic tree-substitution grammar (STSG). Since a parse tree of a sentence may be generated by several (leftmost) derivations, the probability of a tree is the sum of the probabilities of the derivations producing that tree. The probability of a derivation is the product of the subtree probabilities. The original DOP model in Bod (1998) takes the occurrence frequencies of the subtrees in the trees normalized by their root frequencies as subtree parameters. While efficient algorithms have been developed for this DOP model by converting it into a PCFG reduction (Goodman 2003), DOP’s estimator was shown to be inconsistent by Johnson (2002). That is, even with unlimited training data, DOP's estimator is not guaranteed to converge to the correct distribution.  Zollmann and Sima’an (2005) developed a statistically consistent estimator for DOP which is based on the assumption that maximizing the joint probability of the parses in a treebank can be approximated by maximizing the joint probability of their shortest derivations (i.e. the derivations consisting of the fewest subtrees). This assumption is in consonance with the principle of simplicity, but there are also empirical reasons for the shortest derivation assumption: in Bod (2003) and Hearne and Way (2006), it is shown that DOP models that select the preferred parse of a test sentence using the shortest derivation criterion perform very well. On the basis of this shortest-derivation assumption, Zollmann and Sima’an come up with a model that uses held-out estimation: the training corpus is randomly split into two parts proportional to a fixed ratio: an extraction corpus EC and a held-out corpus HC. Applied to DOP, held-out estimation would mean to extract fragments from the trees in EC and to assign their weights such that the likelihood of HC is maximized. If we combine their estimation method with Goodman’s reduction of DOP, Zollman and Sima’an’s procedure operates as follows:  (1) Divide a treebank into an EC and HC (2) Convert the subtrees from EC into a PCFG reduction (3) Compute the shortest derivations for the sentences in HC (by simply assigning each subtree equal weight and applying Viterbi 1-best) (4) From those shortest derivations, extract the subtrees and their relative frequencies in HC to form an STSG  Zollmann and Sima’an show that the resulting estimator is consistent. But equally important is the fact that this new DOP* model does not suffer from a decrease in parse accuracy if larger subtrees are included, whereas the original DOP model needs to be redressed by a correction factor to maintain this property (Bod 2003). Moreover, DOP*’s estimation procedure is very efficient, while the EM training procedure for UML-DOP 402

proposed in Bod (2006) is particularly time consuming and can only operate by randomly sampling trees.  Given the advantages of DOP*, we  will generalize this model in the current paper to unsupervised parsing. We will use the same all-subtrees methodology as in Bod (2006), but now by applying the efficient and consistent DOP*-based estimator. The resulting model, which we will call U-DOP*, roughly operates as follows:  (1) Divide a corpus into an EC and HC (2) Assign all unlabeled binary trees to the sentences in EC, and store them in a shared parse forest (3) Convert the subtrees from the parse forests into a compact PCFG reduction (see next section) (4) Compute the shortest derivations for the sentences in HC (as in DOP*) (5) From those shortest derivations, extract the subtrees and their relative frequencies in HC to form an STSG (6) Use the STSG to compute the most probable parse trees for new test data by means of Viterbi n-best (see next section)  We will use this U-DOP* model to investigate our main research question: how far can we get with unsupervised parsing if we make our training corpus several orders of magnitude larger than has hitherto be attempted?   3  Converting shared parse forests into PCFG reductions  The main computational problem is how to deal with the immense number of subtrees in U-DOP*. There exists already an efficient supervised algorithm that parses a sentence by means of all subtrees from a treebank. This algorithm was extensively described in Goodman (2003) and converts a DOP-based STSG into a compact PCFG reduction that generates eight rules for each node in the treebank. The reduction is based on the following idea: every node in every treebank tree is assigned a unique number which is called its address. The notation A@k denotes the node at address k where A is the nonterminal labeling that node. A new nonterminal is created for each node in the training data. This nonterminal is called Ak. Let aj represent the number of subtrees headed by the node A@j, and let a represent the number of subtrees headed by nodes with nonterminal A, that is a = Σj aj. Then there is a PCFG with the following property: for every subtree in the training corpus headed by A, the grammar will generate an isomorphic subderivation. For example, for a node (A@j (B@k, C@l)), the following eight PCFG rules in figure 1 are generated, where the number following a rule is its weight.   Aj → BC       (1/aj) A → BC        (1/a) Aj → BkC      (bk/aj) A → BkC      (bk/a) Aj → BCl      (cl/aj) A → BCl         (cl/a) Aj → BkCl     (bkcl/aj) A → BkCl       (bkcl/a)  Figure 1. PCFG reduction of supervised DOP  By simple induction it can be shown that this construction produces PCFG derivations isomorphic to DOP derivations (Goodman 2003: 130-133). The PCFG reduction is linear in the number of nodes in the corpus. While Goodman’s reduction method was developed for supervised DOP where each training sentence is annotated with exactly one tree, the method can be generalized to a corpus where each sentence is annotated with all possible binary trees (labeled with the generalized category X), as long as we represent these trees by a shared parse forest. A shared parse forest can be obtained by adding pointers from each node in the chart (or tabular diagram) to the nodes that caused it to be placed in the chart. Such a forest can be represented in cubic space and time (see Billot and Lang 1989). Then, instead of assigning a unique address to each node in each tree, as done by the PCFG reduction for supervised DOP, we now assign a unique address to each node in each parse forest for each sentence. However, the same node may be part of more than one tree. A shared parse forest is an AND-OR graph where AND-nodes correspond to the usual parse tree nodes, while OR-nodes correspond to distinct subtrees occurring in the same context. The total number of nodes is cubic in sentence length n. This means that there are O(n3) many nodes that receive a unique address as described above, to which next our PCFG reduction is applied. This is a huge reduction compared to Bod (2006) where 403

the number of subtrees of all trees increases with the Catalan number, and only ad hoc sampling could make the method work. Since U-DOP* computes the shortest derivations (in the training phase) by combining subtrees from unlabeled binary trees, the PCFG reduction in figure 1 can be represented as in figure 2, where X refers to the generalized category while B and C either refer to part-of-speech categories or are equivalent to X. The equal weights follow from the fact that the shortest derivation is equivalent to the most probable derivation if all subtrees are assigned equal probability (see Bod 2000; Goodman 2003).  Xj → BC        1  X → BC        0.5 Xj → BkC      1  X → BkC       0.5 Xj → BCl       1  X → BCl         0.5 Xj → BkCl      1  X → BkCl       0.5  Figure 2. PCFG reduction for U-DOP*  Once we have parsed HC with the shortest derivations by the PCFG reduction in figure 2, we extract the subtrees from HC to form an STSG. The number of subtrees in the shortest derivations is linear in the number of nodes (see Zollmann and Sima’an 2005, theorem 5.2). This means that U-DOP* results in an STSG which is much more succinct than previous DOP-based STSGs. Moreover, as in Bod (1998, 2000), we use an extension of Good-Turing to smooth the subtrees and to deal with ‘unknown’ subtrees. Note that the direct conversion of parse forests into a PCFG reduction also allows us to efficiently implement the maximum likelihood extension of U-DOP known as UML-DOP (Bod 2006). This can be accomplished by training the PCFG reduction on the held-out corpus HC by means of the expectation-maximization algorithm, where the weights in figure 1 are taken as initial parameters. Both U-DOP*’s and UML-DOP’s estimators are known to be statistically consistent. But while U-DOP*’s training phase merely consists of the computation of the shortest derivations and the extraction of subtrees, UML-DOP involves iterative training of the parameters. Once we have extracted the STSG, we compute the most probable parse for new sentences by Viterbi n-best, summing up the probabilities of derivations resulting in the same tree (the exact computation of the most probable parse is NP hard – see Sima’an 1996). We have incorporated the technique by Huang and Chiang (2005) into our implementation which allows for efficient Viterbi n-best parsing.   4    Evaluation on hand-annotated corpora  To evaluate U-DOP* against UML-DOP and other unsupervised parsing models, we started out with three corpora that are also used in Klein and Manning (2002, 2004) and Bod (2006): Penn’s WSJ10 which contains 7422 sentences ≤ 10 words after removing empty elements and punctuation, the German NEGRA10 corpus and the Chinese Treebank CTB10 both containing 2200+ sentences ≤ 10 words after removing punctuation. As with most other unsupervised parsing models, we train and test on p-o-s strings rather than on word strings. The extension to word strings is straightforward as there exist highly accurate unsupervised part-of-speech taggers (e.g. Schütze 1995) which can be directly combined with unsupervised parsers, but for the moment we will stick to p-o-s strings (we will come back to word strings in section 5). Each corpus was divided into 10 training/test set splits of 90%/10% (n-fold testing), and each training set was randomly divided into two equal parts, that serve as EC and HC and vice versa. We used the same evaluation metrics for unlabeled precision (UP) and unlabeled recall (UR) as in Klein and Manning (2002, 2004). The two metrics of UP and UR are combined by the unlabeled f-score F1 = 2*UP*UR/(UP+UR). All trees in the test set were binarized beforehand, in the same way as in Bod (2006).  For UML-DOP the decrease in cross-entropy became negligible after maximally 18 iterations. The training for U-DOP* consisted in the computation of the shortest derivations for the HC from which the subtrees and their relative frequencies were extracted. We used the technique in Bod (1998, 2000) to include ‘unknown’ subtrees. Table 1 shows the f-scores for U-DOP* and UML-DOP against the f-scores for U-DOP reported in Bod (2006), the CCM model in Klein and Manning (2002), the DMV dependency model in Klein and Manning (2004) and their combined model DMV+CCM.  404

 Model English (WSJ10) German (NEGRA10) Chinese (CTB10) CCM 71.9 61.6 45.0 DMV 52.1 49.5 46.7 DMV+CCM 77.6 63.9 43.3 U-DOP 78.5 65.4 46.6 U-DOP* 77.9 63.8 42.8 UML-DOP 79.4 65.2 45.0  Table 1. F-scores of U-DOP* and UML-DOP compared to other models on the same data.  It should be kept in mind that an exact comparison can only be made between U-DOP* and UML-DOP in table 1, since these two models were tested on 90%/10% splits, while the other models were applied to the full WSJ10, NEGRA10 and CTB10 corpora. Table 1 shows that U-DOP* performs worse than UML-DOP in all cases, although the differences are small and was statistically significant only for WSJ10 using paired t-testing. As explained above, the main advantage of U-DOP* over UML-DOP is that it works with a more succinct grammar extracted from the shortest derivations of HC. Table 2 shows the size of the grammar (number of rules or subtrees) of the two models for resp. Penn WSJ10, the entire Penn WSJ and the first 2 million sentences from the NANC (North American News Text) corpus which contains a total of approximately 24 million sentences from different news sources.  Model Size of STSG for WSJ10 Size of STSG for Penn WSJ  Size of STSG for 2,000K NANC  U-DOP* 2.2 x 104 9.8 x 105 7.2 x 106 UML-DOP 1.5 x 106 8.1 x 107 5.8 x 109  Table 2. Grammar size of U-DOP* and UML-DOP for WSJ10 (7,7K sentences), WSJ (50K sentences) and the first 2,000K sentences from NANC.  Note that while U-DOP* is about 2 orders of magnitudes smaller than UML-DOP for the WSJ10, it is almost 3 orders of magnitudes smaller for the first 2 million sentences of the NANC corpus. Thus even if U-DOP* does not give the highest f-score in table 1, it is more apt to be trained on larger data sets. In fact, a well-known advantage of unsupervised methods over supervised methods is the availability of almost unlimited amounts of text. Table 2 indicates that U-DOP*’s grammar is still of manageable size even for text corpora that are (almost) two orders of magnitude larger than Penn’s WSJ. The NANC corpus contains approximately 2 million WSJ sentences that do not overlap with Penn’s WSJ and has been previously used by McClosky et al. (2006) in improving a supervised parser by self-training. In our experiments below we will start by mixing subsets from the NANC’s WSJ data with Penn’s WSJ data. Next, we will do the same with 2 million sentences from the LA Times in the NANC corpus, and finally we will mix all data together for inducing a U-DOP* model. From Penn’s WSJ, we only use sections 2 to 21 for training (just as in supervised parsing) and section 23 (≤100 words) for testing, so as to compare our unsupervised results with some binarized supervised parsers. The NANC data was first split into sentences by means of a simple discriminitive model. It was next p-o-s tagged with the the TnT tagger (Brants 2000) which was trained on the Penn Treebank such that the same tag set was used. Next, we added subsets of increasing size from the NANC p-o-s strings to the 40,000 Penn WSJ p-o-s strings. Each time the resulting corpus was split into two halfs and the shortest derivations were computed for one half by using the PCFG-reduction from the other half and vice versa. The resulting trees were used for extracting an STSG which in turn was used to parse section 23 of Penn’s WSJ. Table 3 shows the results.  # sentences added  f-score by adding WSJ data f-score by adding LA Times data 0 (baseline) 62.2 62.2 100k 64.7 63.0 250k 66.2 63.8 500k 67.9 64.1 1,000k 68.5 64.6 2,000k 69.0 64.9  Table 3. Results of U-DOP* on section 23 from Penn’s WSJ by adding sentences from NANC’s WSJ and NANC’s LA Times  405

Table 3 indicates that there is a monotonous increase in f-score on the WSJ test set if NANC text is added to our training data in both cases, independent of whether the sentences come from the WSJ domain or the LA Times domain. Although the effect of adding LA Times data is weaker than adding WSJ data, it is noteworthy that the unsupervised induction of trees from the LA Times domain still improves the f-score even if the test data are from a different domain.  We also investigated the effect of adding the LA Times data to the total mix of Penn’s WSJ and NANC’s WSJ. Table 4 shows the results of this experiment, where the baseline of 0 sentences thus starts with the 2,040k sentences from the combined Penn-NANC WSJ data.  Sentences added from LA Times to Penn-NANC WSJ f-score by adding LA Times data 0 69.0 100k 69.4 250k 69.9 500k 70.2 1,000k 70.4 2,000k 70.7  Table 4. Results of U-DOP* on section 23 from Penn’s WSJ by mixing sentences from the combined Penn-NANC WSJ with additions from NANC’s LA Times.  As seen in table 4, the f-score continues to increase even when adding LA Times data to the large combined set of Penn-NANC WSJ sentences. The highest f-score is obtained by adding 2,000k sentences, resulting in a total training set of 4,040k sentences. We believe that our result is quite promising for the future of unsupervised parsing.  In putting our best f-score in table 4 into perspective, it should be kept in mind that the gold standard trees from Penn-WSJ section 23 were binarized. It is well known that such a binarization has a negative effect on the f-score. Bod (2006) reports that an unbinarized treebank grammar achieves an average 72.3% f-score on WSJ sentences ≤ 40 words, while the binarized version achieves only 64.6% f-score. To compare U-DOP*’s results against some supervised parsers, we additionally evaluated a PCFG treebank grammar and the supervised DOP* parser using the same test set. For these supervised parsers, we employed the standard training set, i.e. Penn’s WSJ sections 2-21, but only by taking the p-o-s strings as we did for our unsupervised U-DOP* model. Table 5 shows the results of this comparison.  Parser f-score U-DOP* 70.7 Binarized treebank PCFG 63.5 Binarized DOP* 80.3  Table 5. Comparison between the (best version of) U-DOP*, the supervised treebank PCFG and the supervised DOP* for section 23 of Penn’s WSJ  As seen in table 5, U-DOP* outperforms the binarized treebank PCFG on the WSJ test set. While a similar result was obtained in Bod (2006), the absolute difference between unsupervised parsing and the treebank grammar was extremely small in Bod (2006): 1.8%, while the difference in table 5 is 7.2%, corresponding to 19.7% error reduction. Our f-score remains behind the supervised version of DOP* but the gap gets narrower as more training data is being added to U-DOP*.   5   Evaluation on unlabeled corpora in a practical application  Our experiments so far have shown that despite the addition of large amounts of unlabeled training data, U-DOP* is still outperformed by the supervised DOP* model when tested on hand-annotated corpora like the Penn Treebank. Yet it is well known that any evaluation on hand-annotated corpora unreasonably favors supervised parsers. There is thus a quest for designing an evaluation scheme that is independent of annotations. One way to go would be to compare supervised and unsupervised parsers as a syntax-based language model in a practical application such as machine translation (MT) or speech recognition.   In Bod (2007), we compared U-DOP* and DOP* in a syntax-based MT system known as Data-Oriented Translation or DOT (Poutsma 2000; Groves et al. 2004). The DOT model starts with a bilingual treebank where each tree pair constitutes an example translation and where translationally equivalent constituents are linked. Similar to DOP, 406

the DOT model uses all linked subtree pairs from the bilingual treebank to form an STSG of linked subtrees, which are used to compute the most probable translation of a target sentence given a source sentence (see Hearne and Way 2006).   What we did in Bod (2007) is to let both DOP* and U-DOP* compute the best trees directly for the word strings in the German-English Europarl corpus (Koehn 2005), which contains about 750,000 sentence pairs. Differently from U-DOP*, DOP* needed to be trained on annotated data, for which we used respectively the Negra and the Penn treebank. Of course, it is well-known that a supervised parser’s f-score decreases if it is transferred to another domain: for example, the (non-binarized) WSJ-trained DOP model in Bod (2003) decreases from around 91% to 85.5% f-score if tested on the Brown corpus. Yet, this score is still considerably higher than the accuracy obtained by the unsupervised U-DOP model, which achieves 67.6% unlabeled f-score on Brown sentences. Our main question of interest is in how far this difference in accuracy on hand-annotated corpora carries over when tested in the context of a concrete application like MT. This is not a trivial question, since U-DOP* learns ‘constituents’ for word sequences such as Ich möchte (“I would like to”) and There are (Bod 2007), which are usually hand-annotated as non-constituents. While U-DOP* is punished for this ‘incorrect’ prediction if evaluated on the Penn Treebank, it may be rewarded for this prediction if evaluated in the context of machine translation using the Bleu score (Papineni et al. 2002). Thus similar to Chiang (2005), U-DOP can discover non-syntactic phrases, or simply “phrases”, which are typically neglected by linguistically syntax-based MT systems. At the same time, U-DOP* can also learn discontiguous constituents that are neglected by phrase-based MT systems (Koehn et al. 2003). In our experiments, we used both U-DOP* and DOP* to predict the best trees for the German-English Europarl corpus. Next, we assigned links between each two nodes in the respective trees for each sentence pair. For a 2,000 sentence test set from a different part of the Europarl corpus we computed the most probable target sentence (using Viterbi n best). The Bleu score was used to measure translation accuracy, calculated by the NIST script with its default settings. As a baseline we compared our results with the publicly available phrase-based system Pharaoh (Koehn et al. 2003), using the default feature set. Table 6 shows for each system the Bleu score together with a description of the productive units. ‘U-DOT’ refers to ‘Unsupervised DOT’ based on U-DOP*, while DOT is based on DOP*.  System Productive Units Bleu-score U-DOT / U-DOP* Constituents and Phrases 0.280 DOT / DOP* Constituents only 0.221 Pharaoh Phrases only 0.251  Table 6. Comparing U-DOP* and DOP* in syntax-based MT on the German-English Europarl corpus against the Pharaoh system.  The table shows that the unsupervised U-DOT model outperforms the supervised DOT model with 0.059. Using Zhang’s significance tester (Zhang et al. 2004), it turns out that this difference is statistically significant (p < 0.001). Also the difference between U-DOT and the baseline Pharaoh is statistically significant (p < 0.008). Thus even if supervised parsers like DOP* outperform unsupervised parsers like U-DOP* on hand-parsed data with >10%, the same supervised parser is outperformed by the unsupervised parser if tested in an MT application. Evidently, U-DOP’s capacity to capture both constituents and phrases pays off in a concrete application and shows the shortcomings of models that only allow for either constituents (such as linguistically syntax-based MT) or phrases (such as phrase-based MT). In Bod (2007) we also show that U-DOT obtains virtually the same Bleu score as Pharaoh after eliminating subtrees with discontiguous yields.  6    Conclusion: future of supervised parsing  In this paper we have shown that the accuracy of unsupervised parsing under U-DOP* continues to grow when enlarging the training set with additional data. However, except for the simple treebank PCFG, U-DOP* scores worse than supervised parsers if evaluated on hand-annotated data. At the same time U-DOP* significantly outperforms the supervised DOP* if evaluated in a practical application like MT. We argued that this can be explained by the fact that U-DOP learns 407

both constituents and (non-syntactic) phrases while supervised parsers learn constituents only. What should we learn from these results? We believe that parsing, when separated from a task-based application, is mainly an academic exercise. If we only want to mimick a treebank or implement a linguistically motivated grammar, then supervised, grammar-based parsers are preferred to unsupervised parsers. But if we want to improve a practical application with a syntax-based language model, then an unsupervised parser like U-DOP* might be superior.  The problem with most supervised (and semi-supervised) parsers is their rigid notion of constituent which excludes ‘constituents’ like the German Ich möchte or the French Il y a. Instead, it has become increasingly clear that the notion of constituent is a fluid which may sometimes be in agreement with traditional syntax, but which may just as well be in opposition to it. Any sequence of words can be a unit of combination, including non-contiguous word sequences like closest X to Y. A parser which does not allow for this fluidity may be of limited use as a language model. Since supervised parsers seem to stick to categorical notions of constituent, we believe that in the field of syntax-based language models the end of supervised parsing has come in sight.  Acknowledgements   Thanks to Willem Zuidema and three anonymous reviewers for useful comments and suggestions on the future of supervised parsing.  References Billot, S. and B. Lang, 1989. The Structure of Shared Forests in Ambiguous Parsing. In ACL 1989. Bod, R. 1998. Beyond Grammar: An Experience-Based Theory of Language, CSLI Publications. Bod, R. Parsing with the Shortest Derivation. In COLING 2000, Saarbruecken. Bod, R. 2003. An efficient implementation of a new DOP model. In EACL 2003, Budapest. Bod, R. 2006. An All-Subtrees Approach to Unsupervised Parsing. In ACL-COLING 2006, Sydney. Bod, R. 2007. Unsupervised Syntax-Based Machine Translation. Submitted for publication. Brants, T. 2000. TnT - A Statistical Part-of-Speech Tagger. In ANLP 2000. Chiang, D. 2005. A Hierarchical Phrase-Based Model for Statistical Machine Translation. In ACL 2005, Ann Arbor. Clark, A. 2001. Unsupervised induction of stochastic context-free grammars using distributional clustering. In CONLL 2001. Goodman, J. 2003. Efficient algorithms for the DOP model. In R. Bod, R. Scha and K. Sima'an (eds.). Data-Oriented Parsing, CSLI Publications. Graff, D. 1995. North American News Text Corpus. Linguistic Data Consortium. LDC95T21. Groves, D., M. Hearne and A. Way, 2004. Robust Sub-Sentential Alignment of Phrase-Structure Trees. In COLING 2004, Geneva. Hearne, M and A. Way, 2006. Disambiguation Strategies for Data-Oriented Translation. Proceedings of the 11th Conference of the European Association for Machine Translation, Oslo. Henderson, J. 2004. Discriminative training of a neural network statistical parser. In ACL 2004, Barcelona. Huang, L. and D. Chiang 2005. Better k-best parsing. In IWPT 2005, Vancouver. Johnson, M. 2002. The DOP estimation method is biased and inconsistent. Computational Linguistics 28, 71-76. Klein, D. and C. Manning 2002. A general constituent-context model for improved grammar induction. In ACL 2002, Philadelphia. Klein, D. and C. Manning 2004. Corpus-based induction of syntactic structure: models of dependency and constituency. ACL 2004, Barcelona. Koehn, P., Och, F. J., and Marcu, D. 2003. Statistical phrase based translation. In HLT-NAACL 2003. Koehn, P. 2005. Europarl: a parallel corpus for statistical machine translation. In MT Summit 2005. McClosky, D., E. Charniak and M. Johnson 2006. Effective self-training for parsing. In HLT-NAACL 2006, New York. Poutsma, A. 2000. Data-Oriented Translation. In COLING 2000, Saarbruecken. Schütze, H. 1995. Distributional part-of-speech tagging. In ACL 1995, Dublin. Sima'an, K. 1996. Computational complexity of probabilistic disambiguation by means of tree grammars. In COLING 1996, Copenhagen. Steedman, M. M. Osborne, A. Sarkar, S. Clark, R. Hwa, J. Hockenmaier, P. Ruhlen, S. Baker, and J. Crim. 2003. Bootstrapping statistical parsers from small datasets. In EACL 2003, Budapest. van Zaanen, M. 2000. ABL: Alignment-Based Learning. In COLING 2000, Saarbrücken. Zhang, Y., S. Vogel and A. Waibel, 2004. Interpreting BLEU/NIST scores: How much improvement do we need to have a better system? Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC). Zollmann, A. and K. Sima'an 2005. A consistent and efficient estimator for data-oriented parsing. Journal of Automata, Languages and Combinatorics, Vol. 10 (2005) Number 2/3, 367-388. Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 408–415,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

408

AnEnsembleMethodforSelectionofHighQualityParsesRoiReichartICNCHebrewUniversityofJerusalemroiri@cs.huji.ac.ilAriRappoportInstituteofComputerScienceHebrewUniversityofJerusalemarir@cs.huji.ac.ilAbstractWhiletheaverageperformanceofstatisti-calparsersgraduallyimproves,theystillat-tachtomanysentencesannotationsofratherlowquality.Thenumberofsuchsentencesgrowswhenthetrainingandtestdataaretakenfromdifferentdomains,whichisthecaseformajorwebapplicationssuchasin-formationretrievalandquestionanswering.InthispaperwepresentaSampleEnsem-bleParseAssessment(SEPA)algorithmfordetectingparsequality.Weuseafunctionoftheagreementamongseveralcopiesofaparser,eachofwhichtrainedonadiffer-entsamplefromthetrainingdata,toassessparsequality.Weexperimentedwithbothgenerativeandrerankingparsers(Collins,CharniakandJohnsonrespectively).Weshowsuperiorresultsoverseveralbaselines,bothwhenthetrainingandtestdataarefromthesamedomainandwhentheyarefromdifferentdomains.Foratestsettingusedbypreviouswork,weshowanerrorreductionof31%asopposedtotheir20%.1IntroductionManyalgorithmsformajorNLPapplicationssuchasinformationextraction(IE)andquestionanswer-ing(QA)utilizetheoutputofstatisticalparsers(see(Yatesetal.,2006)).Whiletheaverageper-formanceofstatisticalparsersgraduallyimproves,thequalityofmanyoftheparsestheyproduceistoolowforapplications.Whenthetrainingandtestdataaretakenfromdifferentdomains(theparseradaptationscenario)theratioofsuchlowqualityparsesbecomesevenhigher.Figure1demonstratesthesephenomenafortwoleadingmodels,Collins(1999)model2,agenerativemodel,andCharniakandJohnson(2005),arerankingmodel.Theparseradaptationscenarioistheruleratherthantheexcep-tionforQAandIEsystems,becausetheseusuallyoperateoverthehighlyvariableWeb,makingitverydifﬁculttocreatearepresentativecorpusformanualannotation.Mediumqualityparsesmayseriouslyharmtheperformanceofsuchsystems.Inthispaperweaddresstheproblemofassess-ingparsequality,usingaSampleEnsembleParseAssessment(SEPA)algorithm.Weusethelevelofagreementamongseveralcopiesofaparser,eachofwhichtrainedonadifferentsamplefromthetrainingdata,topredictthequalityofaparse.Thealgorithmdoesnotassumeuniformityoftrainingandtestdata,andisthussuitabletoweb-basedapplicationssuchasQAandIE.Generativestatisticalparserscomputeaprobabil-ityp(a,s)foreachsentenceannotation,sotheim-mediatetechniquethatcomestomindforassess-ingparsequalityistosimplyusep(a,s).Anotherseeminglytrivialmethodistoassumethatshortersentenceswouldbeparsedbetterthanlongerones.However,thesetechniquesproduceresultsthatarefarfromoptimal.InSection5weshowthesuperi-orityofourmethodovertheseandotherbaselines.Surprisingly,asfarasweknowthereisonlyonepreviousworkexplicitlyaddressingthisproblem(Yatesetal.,2006).TheirWOODWARDalgorithmﬁltersouthighqualityparsesbyperformingseman-409

808590951000.20.40.60.81F scoreFraction of parses  Collins, IDCollins, Adap.Charniak, IDCharniak,Adap.Figure1:F-scorevs.thefractionofparseswhosef-scoreisatleastthatf-score.Forthein-domainscenario,theparsersaretestedonsec23oftheWSJPennTreebank.Fortheparseradaptationscenario,theyaretestedontheBrowntestsection.Inbothcasestheyaretrainedonsections2-21ofWSJ.ticanalysis.Thepresentpaperprovidesadetailedcomparisonbetweenthetwoalgorithms,showingboththatSEPAproducessuperiorresultsandthatitoperatesunderlessrestrictiveconditions.Weexperimentwithboththegenerativeparsingmodelnumber2ofCollins(1999)andthererankingparserofCharniakandJohnson(2005),bothwhenthetrainingandtestdatabelongtothesamedomain(thein-domainscenario)andintheparseradapta-tionscenario.Inallfourcases,weshowsubstantialimprovementoverthebaselines.Thepresentpaperistheﬁrsttousearerankingparserandtheﬁrsttoaddresstheadaptationscenarioforthisproblem.Section2discussesrelevantpreviouswork,Sec-tion3describestheSEPAalgorithm,Sections4and5presenttheexperimentalsetupandresults,andSection6discussescertainaspectsoftheseresultsandcomparesSEPAtoWOODWARD.2RelatedWorkTheonlypreviousworkweareawareofthatexplic-itlyaddressedtheproblemofdetectinghighqualityparsesintheoutputofstatisticalparsersis(Yatesetal.,2006).Basedontheobservationthatincorrectparsesoftenresultinimplausiblesemanticinterpre-tationsofsentences,theydesignedtheWOODWARDﬁlteringsystem.Itﬁrstmapstheparseproducedbytheparsertoalogic-basedrepresentation(relationalconjunction(RC))andthenemploysfourmethodsforsemanticallyanalyzingwhetheraconjunctintheRCislikelytobereasonable.Theﬁltersuseseman-ticinformationobtainedfromtheWeb.Measuringerrorsusingﬁlterf-score(seeSection3)andusingtheCollinsgenerativemodel,WOODWARDreduceserrorsby67%onasetofTRECquestionsandby20%onasetofa100WSJsentences.Section5providesadetailedcomparisonwithouralgorithm.Rerankingalgorithms(KooandCollins,2005;CharniakandJohnson,2005)searchthelistofbestparsesoutputbyagenerativeparsertoﬁndaparseofhigherqualitythantheparseselectedbythegenera-tiveparser.Thus,thesealgorithmsineffectassessparsequalityusingsyntacticandlexicalfeatures.TheSEPAalgorithmdoesnotusesuchfeatures,andissuccessfulindetectinghighqualityparsesevenwhenworkingontheoutputofareranker.Rerank-ingandSEPAarethusrelativelyindependent.Bagging(Breiman,1996)usesanensembleofin-stancesofamodel,eachtrainedonasampleofthetrainingdata1.Baggingwassuggestedinordertoenhanceclassiﬁers;theclassiﬁcationoutcomewasdeterminedusingamajorityvoteamongthemod-els.InNLP,baggingwasusedforactivelearningfortextclassiﬁcation(Argamon-EngelsonandDa-gan,1999;McCallumandNigam,1998).Specif-icallyinparsing,(HendersonandBrill,2000)ap-pliedaconstituentlevelvotingschemetoanen-sembleofbaggedmodelstoincreaseparserperfor-mance,and(BeckerandOsborne,2005)suggestedanactivelearningtechniqueinwhichtheagreementamonganensembleofbaggedparsersisusedtopre-dictexamplesvaluableforhumanannotation.Theyreportedexperimentswithsmalltrainingsetsonly(upto5,000sentences),andtheiragreementfunc-tionisverydifferentfromours.Bothworksexperi-mentedwithgenerativeparsingmodelsonly.NgaiandYarowsky(2000)usedanensemblebasedonbaggingandpartitioningforactivelearningforbaseNPchunking.Theyselecttopitemswith-outanygradedassessment,andtheirf-complementfunction,whichslightlyresemblesourMF(seethenextsection),isappliedtotheoutputofaclassiﬁer,whileourfunctionisappliedtostructuredoutput.Asurveyofseveralpapersdealingwithmapping1Eachsampleiscreatedbysampling,withreplacement,Lexamplesfromthetrainingpool,whereListhesizeofthetrain-ingpool.Conversely,eachofoursamplesissmallerthanthetrainingset,andiscreatedbysamplingwithoutreplacement.SeeSection3(‘regardingS’)foradiscussionofthisissue.410

predictorsinclassiﬁers’outputtoposteriorproba-bilitiesisgivenin(CaruanaandNiculescu-Mizil,2006).Asfarasweknow,theapplicationofasam-plebasedparserensembleforassessingparsequal-ityisnovel.ManyIEandQAsystemsrelyontheoutputofparsers(Kwoketal.,2001;Attardietal.,2001;Moldovanetal.,2003).Thelattertriestoaddressincorrectparsesusingcomplexrelaxationmethods.Knowingthequalityofaparsecouldgreatlyim-provetheperformanceofsuchsystems.3TheSampleEnsembleParseAssessment(SEPA)AlgorithmInthissectionwedetailourparseassessmentalgo-rithm.ItsinputconsistsofaparsingalgorithmA,anannotatedtrainingsetTR,andanunannotatedtestsetTE.Theoutputprovides,foreachtestsentence,theparsegeneratedforitbyAwhentrainedonthefulltrainingset,andagradeassessingtheparse’squality,onacontinuousscalebetween0to100.Ap-plicationsarethenfreetoselectasentencesubsetthatsuitstheirneedsusingourgrades,e.g.bykeep-ingonlyhigh-qualityparses,orbyremovinglow-qualityparsesandkeepingtherest.Thealgorithmhasthefollowingstages:1.ChooseNrandomsamplesofsizeSfromthetrainingsetTR.Eachsampleisselectedwith-outreplacement.2.TrainNcopiesoftheparsingalgorithmA,eachwithoneofthesamples.3.ParsethetestsetwitheachoftheNmodels.4.Foreachtestsentence,computethevalueofanagreementfunctionFbetweenthemodels.5.SortthetestsetaccordingtoF’svalue.Thealgorithmusesthelevelofagreementamongseveralcopiesofaparser,eachtrainedonadifferentsamplefromthetrainingdata,topredictthequal-ityofaparse.Thehighertheagreement,thehigherthequalityoftheparse.Ourapproachassumesthatiftheparametersofthemodelarewelldesignedtoannotateasentencewithahighqualityparse,thenitislikelythatthemodelwilloutputthesame(orahighlysimilar)parseevenifthetrainingdataissomewhatchanged.Inotherwords,werelyonthestabilityoftheparametersofstatisticalparsers.Al-thoughthisisnotalwaysthecase,ourresultscon-ﬁrmthatstrongcorrelationbetweenagreementandparsequalitydoesexist.Weexploredseveralagreementfunctions.TheonethatshowedthebestresultsisMeanF-score(MF)2,deﬁnedasfollows.Denotethemodelsbym1...mN,andtheparseprovidedbymiforsen-tencesasmi(s).Werandomlychooseamodelml,andcomputeMF(s)=1N−1Xi∈[1...N],i6=lfscore(mi,ml)(1)WeusetwomeasurestoevaluatethequalityofSEPAgrades.BothmeasuresaredeﬁnedusingathresholdparameterT,addressingonlysentenceswhoseSEPAgradesarenotsmallerthanT.WerefertothesesentencesasT-sentences.Theﬁrstmeasureistheaveragef-scoreoftheparsesofT-sentences.Notethatwecomputethef-scoreofeachoftheselectedsentencesandthenaveragetheresults.Thisstandsincontrasttothewayf-scoreisordinarilycalculated,bycomputingthelabeledprecisionandrecalloftheconstituentsinthewholesetandusingtheseastheargumentsofthef-scoreequation.Theordinaryf-scoreiscom-putedthatwaymostlyinordertoovercomethefactthatsentencesdifferinlength.However,forappli-cationssuchasIEandQA,whichworkatthesinglesentencelevelandwhichmightreacherroneousde-cisionduetoaninaccurateparse,normalizingoversentencelengthsislessofafactor.Forthisreason,inthispaperwepresentdetailedgraphsfortheaver-agef-score.Forcompleteness,Table4alsoprovidessomeoftheresultsusingtheordinaryf-score.Thesecondmeasureisageneralizationoftheﬁl-terf-scoremeasuresuggestedbyYatesetal.(2006).Theydeﬁneﬁlterprecisionastheratioofcorrectlyparsedsentencesintheﬁlteredset(thesetthealgo-rithmchoose)tototalsentencesintheﬁlteredsetandﬁlterrecallastheratioofcorrectlyparsedsentencesintheﬁlteredsettocorrectlyparsedsentencesinthe2Recallthatsentencef-scoreisdeﬁnedas:f=2×P×RP+R,wherePandRarethelabeledprecisionandrecallofthecon-stituentsinthesentencerelativetoanotherparse.411

wholesetofsentencesparsedbytheparser(unﬁl-teredsetortestset).Correctlyparsedsentencesaresentenceswhoseparsegotf-scoreof100%.Sincerequiringa100%maybetoorestrictive,wegeneralizethismeasuretoﬁlterf-scorewithparam-eterk.Inourmeasure,theﬁlterrecallandprecisionarecalculatedwithregardtosentencesthatgetanf-scoreofkormore,ratherthantocorrectlyparsedsentences.Filteredf-scoreisthusaspecialcaseofourﬁlteredf-score,withparameter100.Wenowdiscusstheeffectofthenumberofmod-elsNandthesamplesizeS.Thediscussionisbasedonexperiments(usingdevelopmentdata,seeSec-tion4)inwhichalltheparametersareﬁxedexceptfortheparameterinquestion,usingourdevelopmentsections.RegardingN(seeFigure2):Asthenumberofmodelsincreases,thenumberofT-sentencesse-lectedbySEPAdecreasesandtheirqualityim-proves,intermsofbothaveragef-scoreandﬁlterf-score(withk=100).Thefactthatmoremod-elstrainedondifferentsamplesofthetrainingdataagreeonthesyntacticannotationofasentenceim-pliesthatthissyntacticpatternislesssensitivetoperturbationsinthetrainingdata.Thenumberofsuchsentencesissmallanditislikelytheparserwillcorrectlyannotatethem.ThesmallerT-setsizeleadstoadecreaseinﬁlterrecall,whilethebetterqualityleadstoanincreaseinﬁlterprecision.Sincethein-creaseinﬁlterprecisionissharperthanthedecreaseinﬁlterrecall,ﬁlterf-scoreincreaseswiththenum-berofmodelsN.RegardingS3:Asthesamplesizeincreases,thenumberofT-sentencesincreases,andtheirqual-itydegradesintermsofaveragef-scorebutim-provesintermsofﬁlterf-score(again,withparam-eterk=100).Theoverlapamongsmallersam-plesissmallandthedatatheysupplyissparse.Ifseveralmodelstrainedonsuchsamplesattachtoasentencethesameparse,thissyntacticpatternmustbeveryprominentinthetrainingdata.Thenum-berofsuchsentencesissmallanditislikelythattheparserwillcorrectlyannotatethem.ThereforesmallersamplesizeleadstosmallerT-setswithhighaveragef-score.Asthesamplesizeincreases,theT-setbecomeslargerbuttheaveragef-scoreofaparse3Graphsarenotshownduetolackofspace.51015209091929394Average f scoreNumber of models − N51015205456586062Filter f score, k = 10005101520657075808590Filter recall, k = 10005101520354045505560Filter precision, k = 100Number of models − NFigure2:TheeffectofthenumberofmodelsNonSEPA(Collins’model).Thescenarioisin-domain,samplesizeS=33,000andT=100.Wesee:averagef-scoreofT-sentences(left,solidcurveandlefty-axis),ﬁlterf-scorewithk=100(left,dashedcurveandrighty-axis),ﬁlterrecallwithk=100(right,solidcurveandlefty-axis),andﬁlterpreci-sionwithk=100(right,dashedcurveandrighty-axis).decreases.ThelargerT-setsizeleadstoincreaseinﬁlterrecall,whiletheloweraveragequalityleadstodecreaseinﬁlterprecision.Sincetheincreaseinﬁlterrecallissharperthanthedecreaseinﬁlterpre-cision,theresultisthatﬁlterf-scoreincreaseswiththesamplesizeS.Thisdiscussiondemonstratestheimportanceofusingbothaveragef-scoreandﬁlterf-score,sincethetwomeasuresreﬂectcharacteristicsofthese-lectedsamplethatarenotnecessarilyhighly(orpos-itively)correlated.4ExperimentalSetupWeperformedexperimentswithtwoparsingmod-els,theCollins(1999)generativemodelnumber2andtheCharniakandJohnson(2005)rerankingmodel.Fortheﬁrstweusedareimplementation(?).Weperformedexperimentswitheachmodelintwoscenarios,in-domainandparseradaptation.Inbothexperimentsthetrainingdataaresections02-21oftheWSJPennTreebank(about40Ksen-tences).Inthein-domainexperimentthetestdataissection23(2416sentences)ofWSJandintheparseradaptationscenariothetestdataisBrowntestsection(2424sentences).DevelopmentsectionsareWSJsection00forthein-domainscenario(1981sentences)andBrowndevelopmentsectionfortheadaptationscenario(2424sentences).Following412

(Gildea,2001),theBrowntestanddevelopmentsec-tionsconsistof10%ofBrownsentences(the9thand10thofeach10consecutivesentencesinthedevel-opmentandtestsectionsrespectively).Weperformedexperimentswithmanyconﬁgu-rationsoftheparametersN(numberofmodels),S(samplesize)andF(agreementfunction).DuetospacelimitationswedescribeonlyexperimentswherethevaluesoftheparametersN,SandFareﬁxed(FisMF,NandSaregiveninSection5)andthethresholdparameterTischanged.5ResultsWeﬁrstexplorethequalityoftheselectedsetintermsofaveragef-score.InSection3wereportedthatthequalityofaselectedT-setofparsesincreasesasthenumberofmodelsNincreasesandsamplesizeSdecreases.WethereforeshowtheresultsforrelativelyhighN(20)andrelativelylowS(13,000,whichisaboutathirdofthetrainingset).DenotethecardinalityofthesetselectedbySEPAbyn(itisactuallyafunctionofTbutweomittheTinordertosimplifynotations).Weuseseveralbaselinemodels.Theﬁrst,conﬁ-dencebaseline(CB),containsthensentenceshav-ingthehighestparserassignedprobability(whentrainedonthewholetrainingset).Thesecond,min-imumlength(ML),containsthenshortestsentencesinthetestset.Sincemanytimesitiseasiertoparseshortsentences,atrivialwaytoincreasetheaver-agef-scoremeasureofasetissimplytoselectshortsentences.Thethird,following(Yatesetal.,2006),ismaximumrecall(MR).MRsimplypredictsthatalltestsetsentencesshouldbecontainedintheselectedT-set.Theoutputsetofthismodelgetsﬁlterrecallof1foranykvalue,butitsprecisionislower.TheMRbaselineisnotrelevanttotheaveragef-scoremea-sure,becauseitselectsallofthesentencesinaset,whichleadstothesameaverageasarandomselec-tion(seebelow).Inordertominimizevisualclutter,fortheﬁlterf-scoremeasureweusethemaximumrecall(MR)baselineratherthantheminimumlength(ML)baseline,sincetheformeroutperformsthelat-ter.Thus,MLisonlyshownfortheaveragef-scoremeasure.Wehavealsoexperimentedwitharandombaselinemodel(containingnrandomlyselectedtestsentences),whoseresultsaretheworstandwhichisshownforreference.ReadersofthissectionmaygetconfusedbetweentheagreementthresholdparameterTandtheparam-eterkoftheﬁlterf-scoremeasure.Pleasenote:astoT,SEPAsortsthetestsetbythevaluesoftheagree-mentfunction.OnecanthenselectonlysentenceswhoseagreementscoreisatleastT.T’svaluesareonacontinuousscalefrom0to100.Astok,theﬁl-terf-scoremeasuregivesagrade.Thisgradecom-binesthreevalues:(1)thenumberofsentencesintheset(selectedbyanalgorithm)whosef-scorerel-ativetothegoldstandardparseisatleastk,(2)thesizeoftheselectedset,and(3)thetotalnumberofsentenceswithsuchaparseinthewholetestset.Wedidnotintroduceseparatenotationsforthesevalues.Figure3(top)showsaveragef-scoreresultswhereSEPAisappliedtoCollins’generativemodelinthein-domain(left)andadaptation(middle)scenarios.SEPAoutperformsthebaselinesforallvaluesoftheagreementthresholdparameterT.Furthermore,asTincreases,notonlydoestheSEPAsetqualityin-crease,butthequalitydifferencesbetweenthissetandthebaselinesetsincreasesaswell.ThegraphsontherightshowthenumberofsentencesinthesetsselectedbySEPAforeachTvalue.Asexpected,thisnumberdecreasesasTincreases.Figure3(bottom)showsthesamepatternofre-sultsfortheCharniakrerankingparserinthein-domain(left)andadaptation(middle)scenarios.WeseethattheeffectsofthererankerandSEPAarerel-ativelyindependent.Evenaftersomeoftheerrorsofthegenerativemodelwerecorrectedbythererankerbyselectingparsesofhigherqualityamongthe50-best,SEPAcandetectparsesofhighqualityfromthesetofparsedsentences.Toexplorethequalityoftheselectedsetintermsofﬁlterf-score,werecallthatthequalityofase-lectedsetofparsesincreasesasboththenumberofmodelsNandthesamplesizeSincrease,andwithT.Therefore,fork=85...100weshowthevalueofﬁlterf-scorewithparameterkwhentheparame-tersconﬁgurationisarelativelyhighN(20),rela-tivelyhighS(33,000,whichareabout80%ofthetrainingset),andthehighestT(100).Figure4(top)showsﬁlterf-scoreresultsforCollins’generativemodelinthein-domain(left)andadaptation(middle)scenarios.Asthesegraphsshow,SEPAoutperformsCBandrandomforallval-413

uesoftheﬁlterf-scoreparameterk,andoutper-formstheMRbaselinewherethevalueofkis95ormore.AlthoughforsmallkvaluesMRgetsahigherf-scorethanSEPA,theﬁlterprecisionofSEPAismuchhigher(right,shownforadaptation.Thein-domainpatternissimilarandnotshown).ThisstemsfromthedeﬁnitionoftheMRbaseline,whichsim-plypredictsanysentencetobeintheselectedset.Furthermore,sincetheselectedsetismeanttobetheinputforsystemsthatrequirehighqualityparses,whatmattersmostisthatSEPAoutperformstheMRbaselineatthehighkranges.Figure4(bottom)showsthesamepatternofre-sultsfortheCharniakrerankingparserinthein-domain(left)andadaptation(middle)scenarios.Asfortheaveragef-scoremeasure,itdemonstratesthattheeffectsofthererankerandSEPAalgorithmarerelativelyindependent.Tables1and2showtheerrorreductionachievedbySEPAfortheﬁlterf-scoremeasurewithparam-etersk=95,97,100(Table1)andfortheaver-agef-scoremeasurewithseveralSEPAagreementthreshold(T)values(Table2).TheerrorreductionsachievedbySEPAforbothmeasuresaresubstantial.Table3comparesSEPAandWOODWARDontheexactsametestsetusedby(Yatesetal.,2006)(takenfromWSJsec23).SEPAachieveserrorre-ductionof31%overtheMRbaselineonthisset,comparedtoonly20%achievedbyWOODWARD.Notshowninthetable,intermsofordinaryf-scoreWOODWARDachieveserrorreductionof37%whileSEPAachieves43%.Thesenumbersweretheonlyonesreportedin(Yatesetal.,2006).Forcompletenessofreference,Table4showsthesuperiorityofSEPAoverCBintermsoftheusualf-scoremeasureusedbytheparsingcommunity(num-bersarecountedforconstituentsﬁrst).Resultsforotherbaselinesareevenmoreimpressive.Thecon-ﬁgurationissimilartothatofFigure3.6DiscussionInthispaperweintroducedSEPA,anovelalgorithmforassessingparsequalityintheoutputofastatis-ticalparser.SEPAistheﬁrstalgorithmshowntobesuccessfulwhenarerankingparserisconsidered,eventhoughsuchmodelsusearerankertodetectandﬁxsomeoftheerrorsmadebythebasegener-Filterf-scoreIn-domainAdaptationkvalue95971009597100Coll.MR3.520.129.222.829.833.6Coll.CB11.611.73.414.29.97.4Char.MR1.3513.623.4421.93032.5Char.CB21.916.811.92520.216.2Table1:Errorreductionintheﬁlterf-scoremea-sureobtainedbySEPAwithCollins’(toptwolines)andCharniak’s(bottomtwolines)model,inthetwoscenarios(in-domainandadaptation),vs.themaximumrecall(MRlines1and3)andconﬁ-dence(CB,lines2and4)baselines,usingN=20,T=100andS=33,000.Shownarepa-rametervaluesk=95,97,100.Errorreductionnumberswerecomputedby100×(fscoreSEPA−fscorebaseline)/(1−fscorebaseline).Averagef-scoreIn-domainAdaptationT95971009597100Coll.ML32.637.260.846.852.770.7Coll.CB26.531.453.946.953.670Char.ML25.133.258.546.958.477.1Char.CB20.4305244.455.573.5Table2:Errorreductionintheaveragef-scoremea-sureobtainedbySEPAwithCollins(toptwolines)andCharniak(bottomtwolines)model,inthetwoscenarios(in-domainandadaptation),vs.themin-imumlength(MLlines1and3)andconﬁdence(CB,lines2and4)baselines,usingN=20andS=13,000.Shownareagreementthrehsoldpa-rametervaluesT=95,97,100.Errorreductionnumberswerecomputedby100×(fscoreSEPA−fscorebaseline)/(1−fscorebaseline).SEPAWOODWARDCBER31%20%-31%Table3:ErrorreductioncomparedtotheMRbase-line,measuredbyﬁlterf-scorewithparameter100.ThedataistheWSJsec23testsetusdby(Yatesetal.,2006).AllthreemethodsuseCollins’model.SEPAusesN=20,S=33,000,T=100.ativemodel.WOODWARD,theonlypreviouslysug-gestedalgorithmforthisproblem,wastestedwithCollins’generativemodelonly.Furthermore,thisistheﬁrsttimethatanalgorithmforthisproblemsuc-ceedsinadomainadaptationscenario,regardlessof414

859095100889092949698Agreement thresholdAverage fscore  SEPACBMLRand.85909510080859095100Agreement thresholdAverage fscore  SEPACBMLRand.85909510005001000150020002500Agreement thresholdNumber of sentences  In domainAdaptation85909510092939495969798Agreement thresholdAverage fscore  SEPACBMLRand.859095100859095100Agreement thresholdAverage fscore  SEPACBMLRand.8590951005001000150020002500Agreement thresholdNumber of sentences  In domainAdaptationFigure3:AgreementthresholdTvs.averagef-score(leftandmiddle)andnumberofsentencesinthese-lectedset(right),forSEPAwithCollins’generativemodel(top)andtheCharniakrerankingmodel(bottom).SEPAparametersareS=13,000,N=20.Inbothrows,SEPAresultsforthein-domain(left)andadap-tation(middle)scenariosarecomparedtotheconﬁdence(CB)andminimumlength(ML)baselines.Thegraphsontherightshowthenumberofsentencesintheselectedsetforbothscenarios.8590951000.30.40.50.60.70.8KFilter fscore with parameter k  SEPACBMRRand.8590951000.40.50.60.70.80.9KFilter fscore with parameter k  SEPACBMRRand.8590951000.20.40.60.81KFilter precision with parameter k  SEPACBMRRand.8590951000.40.50.60.70.80.9KFilter fscore with parameter k  SEPACBMRRand.8590951000.40.50.60.70.80.91KFilter fscore with parameter k  SEPACBMRRand.8590951000.20.40.60.81KFilter precision with parameter k  SEPACBMRRand.Figure4:Parameterkvs.ﬁlterf-score(leftandmiddle)andﬁlterprecision(right)withthatparameter,forSEPAwithCollins’generativemodel(top)andtheCharniakrerankingmodel(bottom).SEPAparametersareS=33,000,N=20,T=100.Inbothrows,resultsforthein-domain(left)andadaptation(middle)scenarios.Intwoleftmostgraphs,theperformanceofthealgorithmiscomparedtotheconﬁdencebaseline(CB)andmaximumrecall(MR).ThegraphsontherightcomparetheﬁlterprecisionofSEPAwiththatoftheMRandCBbaselines.415

theparsingmodel.IntheWebenvironmentthisisthecommonsituation.TheWSJandBrownexperimentsperformedwithSEPAaremuchbroaderthanthoseperformedwithWOODWARD,consideringallsentencesofWSJsec23andBrowntestsectionratherthanasubsetofcarefullyselectedsentencesfromWSJsec23.However,wedidnotperformaTRECexperiment,as(Yatesetal.,2006)did.OurWSJandBrownresultsoutperformedseveralbaselines.Moreover,WSJ(orBrown)sentencesthatcontainconjunctionswereavoidedintheexperimentsof(Yatesetal.,2006).Wehaveveriﬁedthatouralgorithmshowssubstantialerrorreductionoverthebaselinesforthistypeofsentences(intheranges13−46%fortheﬁlterf-scorewithk=100,and30−60%fortheaveragef-score).AsTable3shows,onaWSJsec23testsetsimilartothatusedby(Yatesetal.,2006),SEPAachieves31%errorreductioncomparedto20%ofWOOD-WARD.WOODWARDworksunderseveralassumptions.Speciﬁcally,itrequiresacorpuswhosecontentover-lapsatleastinpartwiththecontentoftheparsedsentences.Thiscorpusisusedtoextractsemanti-callyrelatedstatisticsforitsﬁlters.Furthermore,theﬁltersofthisalgorithm(exceptoftheQAﬁlter)arefocusedonverbandprepositionrelations.Thus,itismorenaturalforittodealwithmistakescontainedinsuchrelations.ThisisreﬂectedintheWSJbasedtestsetonwhichitistested.SEPAdoesnotmakeanyoftheseassumptions.Itdoesnotuseanyexter-nalinformationsourceandisshowntoselecthighqualityparsesfromdiversesets.In-domainAdaptationFERFERSEPACollins97.0944.36%95.3866.38%CBCollins94.77–86.3–SEPAChar-niak97.2135.69%96.354.66%CBCharniak95.6–91.84–Table4:SEPAerrorreductionvs.theCBbase-lineinthein-domainandadaptationscenarios,us-ingthetraditionalf-scoreoftheparsingliterature.N=20,S=13,000,T=100.Forfuturework,integratingSEPAintothererank-ingprocessseemsapromisingdirectionforenhanc-ingoverallparserperformance.Acknowledgement.WewouldliketothankDanRothforhisconstructivecommentsonthispaper.ReferencesShlomoArgamon-EngelsonandIdoDagan,1996.committee-basedsampleselectionforprobabilisticclassiﬁers.JournalofArtiﬁcialIntelligenceResearch,11:335–360.GiuseppeAttardi,AntonioCisternino,FrancescoFormica,MariaSimiandAlessandroTommasi,2001.PiQASso:Pisaquestionansweringsystem.TREC’01.MarkusBeckerandMilesOsborne,2005.Atwo-stagemethodforactivelearningofstatisticalgrammars.IJ-CAI’05.DanielBikel,2004.CodedevelopedatUniversityofPennsylvania.http://www.cis.upenn.edu.bikel.LeoBreiman,1996.Baggingpredictors.MachineLearning,24(2):123–140.RichCaruanaandAlexandruNiculescu-Mizil,2006.Anempiricalcomparisonofsupervisedlearningalgo-rithms.ICML’06.EugeneCharniakandMarkJohnson,2005.Coarse-to-ﬁnen-bestparsingandmaxentdiscriminativererank-ing.ACL’05.MichaelCollins,1999.Head-drivenstatisticalmodelsfornaturallanguageparsing.Ph.D.thesis,UniversityofPennsylvania.DanielGildea,2001.Corpusvariationandparserperfor-mance.EMNLP’01.JohnC.HendersonandEricBrill,2000.Baggingandboostingatreebankparser.NAACL’00.TerryKooandMichaelCollins,2005.Hidden-variablemodelsfordiscriminativereranking.EMNLP’05.CodyKwok,OrenEtzioniandDanielS.Weld,2001.Scalingquestionansweringtotheweb.WWW’01.AndrewMcCallumandKamalNigam,1998.EmployingEMandpool-basedactivelearningfortextclassiﬁca-tion.ICML’98.DanMoldovan,ChristineClark,SandaHarabagiuandSteveMaiorano,2003.Cogex:Alogicproverforquestionanswering.HLT-NAACL’03.GraceNgaiandDavidYarowsky,2000.Rulewritingorannotation:cost-efﬁcientresourceusageforbasenounphrasechunking.ACL’00.AlexanderYates,StefanSchoenmackersandOrenEt-zioni,2006.Detectingparsererrorsusingweb-basedsemanticﬁlters.EMNLP’06.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 416–423,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

416

OpinionMiningUsingEconometrics:ACaseStudyonReputationSystemsAnindyaGhosePanagiotisG.IpeirotisDepartmentofInformation,Operations,andManagementSciencesLeonardN.SternSchoolofBusiness,NewYorkUniversity{aghose,panos,arun}@stern.nyu.eduArunSundararajanAbstractDerivingthepolarityandstrengthofopinionsisanimportantresearchtopic,attractingsig-niﬁcantattentionoverthelastfewyears.Inthiswork,tomeasurethestrengthandpo-larityofanopinion,weconsidertheeco-nomiccontextinwhichtheopinioniseval-uated,insteadofusinghumanannotatorsorlinguisticresources.Werelyonthefactthattextinon-linesystemsinﬂuencesthebehav-iorofhumansandthiseffectcanbeobservedusingsomeeasy-to-measureeconomicvari-ables,suchasrevenuesorproductprices.Byreversingthelogic,weinferthesemanticori-entationandstrengthofanopinionbytracingthechangesintheassociatedeconomicvari-able.Ineffect,weuseeconometricstoiden-tifythe“economicvalueoftext”andassigna“dollarvalue”toeachopinionphrase,measur-ingsentimenteffectivelyandwithouttheneedformanuallabeling.Wearguethatbyinter-pretingopinionsusingeconometrics,wehavetheﬁrstobjective,quantiﬁable,andcontext-sensitiveevaluationofopinions.WemakethediscussionconcretebypresentingresultsonthereputationsystemofAmazon.com.Weshowthatuserfeedbackaffectsthepricingpowerofmerchantsandbymeasuringtheirpricingpowerwecaninferthepolarityandstrengthoftheunderlyingfeedbackpostings.1IntroductionAsigniﬁcantnumberofwebsitestodayallowuserstopostarticleswheretheyexpressopinionsaboutprod-ucts,ﬁrms,people,andsoon.Forexample,usersonAmazom.compostreviewsaboutproductstheyboughtandusersoneBay.compostfeedbackdescrib-ingtheirexperienceswithsellers.Thegoalofopinionminingsystemsistoidentifysuchpiecesofthetextthatexpressopinions(Brecketal.,2007;K¨onigandBrill,2006)andthenmeasurethepolarityandstrengthoftheexpressedopinions.Whileintuitivelythetaskseemsstraightforward,therearemultiplechallengesinvolved.•Whatmakesanopinionpositiveornegative?Isthereanobjectivemeasureforthistask?•Howcanwerankopinionsaccordingtotheirstrength?Canwedeﬁneanobjectivemeasureforrankingopinions?•Howdoesthecontextchangethepolarityandstrengthofanopinionandhowcanwetakethecontextintoconsideration?Toevaluatethepolarityandstrengthofopinions,mostoftheexistingapproachesrelyeitherontrain-ingfromhuman-annotateddata(HatzivassiloglouandMcKeown,1997),oruselinguisticresources(HuandLiu,2004;KimandHovy,2004)likeWordNet,orrelyonco-occurrencestatistics(Turney,2002)be-tweenwordsthatareunambiguouslypositive(e.g.,“excellent”)andunambiguouslynegative(e.g.,“hor-rible”).Finally,otherapproachesrelyonreviewswithnumericratingsfromwebsites(PangandLee,2002;Daveetal.,2003;PangandLee,2004;Cuietal.,2006)andtrain(semi-)supervisedlearningalgorithmstoclassifyreviewsaspositiveornegative,orinmoreﬁne-grainedscales(PangandLee,2005;Wilsonetal.,2006).Implicitly,thesupervisedlearningtechniquesassumethatnumericratingsfullyencapsulatethesen-timentofthereview.417

Inthispaper,wetakeadifferentapproachandin-steadconsidertheeconomiccontextinwhichanopin-ionisevaluated.Weobservethatthetextinon-linesystemsinﬂuencethebehaviorofthereaders.Thiseffectcanbemeasuredbyobservingsomeeasy-to-measureeconomicvariable,suchasproductprices.Forinstance,onlinemerchantsoneBaywith“posi-tive”feedbackcansellproductsforhigherpricesthancompetitorswith“negative”evaluations.Therefore,eachofthese(positiveornegative)evaluationshasa(positiveornegative)effectonthepricesthatthemerchantcancharge.Forexample,everythingelsebeingequal,asellerwith“speedy”deliverymaybeabletocharge$10morethanasellerwith“slow”de-livery.Usingthisinformation,wecanconcludethat“speedy”isbetterthan“slow”whenappliedto“deliv-ery”andtheirdifferenceis$10.Thus,wecaninferthesemanticorientationandthestrengthofanevaluationfromthechangesintheobservedeconomicvariable.Followingthisidea,weusetechniquesfromecono-metricstoidentifythe“economicvalueoftext”andassigna“dollarvalue”toeachtextsnippet,measuringsentimentstrengthandpolarityeffectivelyandwith-outtheneedforlabelingoranyotherresource.Wearguethatbyinterpretingopinionswithinaneconometricframework,wehavetheﬁrstobjectiveandcontext-sensitiveevaluationofopinions.Forexample,considerthecomment“goodpackaging,”postedbyabuyertoevaluateamerchant.Thiscommentwouldhavebeenconsideredunambiguouslypositivebytheexistingopinionminingsystems.Weobserved,though,thatwithinelectronicmarkets,suchaseBay,apostingthatcontainsthewords“goodpack-aging”hasactuallynegativeeffectonthepowerofamerchanttochargehigherprices.Thissurprisingef-fectreﬂectsthenatureofthecommentsinonlinemar-ketplaces:buyerstendtousesuperlativesandhighlyenthusiasticlanguagetopraiseagoodmerchant,andalukewarm“goodpackaging”isinterpretedasneg-ative.Byintroducingtheeconometricinterpretationofopinionswecaneffortlesslycapturesuchchalleng-ingscenarios,somethingthatisimpossibletoachievewiththeexistingapproaches.Wefocusourpaperonreputationsystemsinelec-tronicmarketsandweexaminetheeffectofopinionsonthepricingpowerofmerchantsinthemarketplaceofAmazon.com.(WediscussmoreapplicationsinSection7.)Wedemonstratethevalueofourtechniqueusingadatasetwith9,500transactionsthattookplaceover180days.Weshowthattextualfeedbackaffectsthepowerofmerchantstochargehigherpricesthanthecompetition,forthesameproduct,andstillmakeasale.Wethenreversethelogicanddeterminethecon-tributionofeachcommentinthepricingpowerofamerchant.Thus,wediscoverthepolarityandstrengthofeachevaluationwithouttheneedforhumananno-tationoranyotherformoflinguisticresource.Thestructureoftherestofthepaperisasfol-lows.Section2givesthebasicbackgroundonrep-utationsystems.Section3describesourmethodol-ogyforconstructingthedatasetthatweuseinourexperiments.Section4showshowwecombineestab-lishedtechniquesfromeconometricswithtextminingtechniquestoidentifythestrengthandpolarityofthepostedfeedbackevaluations.Section5presentstheexperimentalevaluationsofourtechniques.Finally,Section6discussesrelatedworkandSection7dis-cussesfurtherapplicationsandconcludesthepaper.2ReputationSystemsandPricePremiumsWhenbuyerspurchaseproductsinanelectronicmar-ket,theyassessandpaynotonlyfortheproducttheywishtopurchasebutforasetoffulﬁllmentcharacter-isticsaswell,e.g.,packaging,delivery,andtheextenttowhichtheproductdescriptionmatchestheactualproduct.Electronicmarketsrelyonreputationsys-temstoensurethequalityofthesecharacteristicsforeachmerchant,andtheimportanceofsuchsystemsiswidelyrecognizedintheliterature(Resnicketal.,2000;Dellarocas,2003).Typically,merchants’rep-utationinelectronicmarketsisencodedbya“repu-tationproﬁle”thatincludes:(a)thenumberofpasttransactionsforthemerchant,(b)asummaryofnu-mericratingsfrombuyerswhohavecompletedtrans-actionswiththeseller,and(c)achronologicallistoftextualfeedbackprovidedbythesebuyers.Studiesofonlinereputation,thusfar,baseamer-chant’sreputationonthenumericratingthatcharac-terizestheseller(e.g.,averagenumberofstarsandnumberofcompletedtransactions)(MelnikandAlm,2002).Thegeneralconclusionofthesestudiesshowthatmerchantswithhigher(numeric)reputationcanchargehigherpricesthanthecompetition,forthesameproducts,andstillmanagetomakeasale.Thispricepremiumthatthemerchantscancommandoverthecompetitionisameasureoftheirreputation.Deﬁnition2.1Considerasetofmerchantss1,...,snsellingaproductforpricesp1,...,pn.Ifsimakes418

Figure1:AsetofmerchantsonAmazon.comsellinganidenticalproductfordifferentpricesthesaleforpricepi,thensicommandsapricepre-miumequaltopi−pjoversjandarelativepricepremiumequaltopi−pjpi.Hence,atransactionthatin-volvesncompetingmerchantsgeneratesn−1pricepremiums.1Theaveragepricepremiumforthetrans-actionisPj6=i(pi−pj)n−1andtheaveragerelativepricepremiumisPj6=i(pi−pj)pi(n−1).2Example2.1ConsiderthecaseinFigure1wherethreemerchantssellthesameproductfor$631.95,$632.26,and$637.05,respectively.IfGameHogsellstheproduct,thenthepricepremiumagainstXPPass-portis$4.79(=$637.05−$632.26)andagainstthemerchantBuyPCsoftis$5.10.Therelativepricepre-miumis0.75%and0.8%,respectively.Similarly,theaveragepricepremiumforthistransactionis$4.95andtheaveragerelativepricepremium0.78%.2Differentsellersinthesemarketsderivetheirrepu-tationfromdifferentcharacteristics:somesellershaveareputationforfastdelivery,whilesomeothershaveareputationofhavingthelowestpriceamongtheirpeers.Similarly,whilesomesellersarepraisedfortheirpackaginginthefeedback,othersgetgoodcom-mentsforsellinghigh-qualitygoodsbutarecriticizedforbeingratherslowwithshipping.Eventhoughpre-viousstudieshaveestablishedthepositivecorrelationbetweenhigher(numeric)reputationandhigherpricepremiums,theyignoredcompletelytheroleofthetex-tualfeedbackand,inturn,themulti-dimensionalna-tureofreputationinelectronicmarkets.Weshowthatthetextualfeedbackaddssigniﬁcantadditionalvaluetothenumericalscores,andaffectsthepricingpowerofthemerchants.1Asanalternativedeﬁnitionwecanignorethenegativepricepremiums.Theexperimentalresultsaresimilarforbothversions.3DataWecompiledadatasetusingsoftwareresellersfrompubliclyavailableinformationonsoftwareproductlistingsatAmazon.com.Ourdatasetincludes280individualsoftwaretitles.Thesellers’reputationmat-terswhensellingidenticalgoods,andthepricevaria-tionobservedcanbeattributedprimarilytovariationinthemerchant’sreputation.Wecollectedthedataus-ingAmazonWebServicesoveraperiodof180days,betweenOctober2004andMarch2005.Wedescribebelowthetwocategoriesofdatathatwecollected.TransactionData:TheﬁrstpartofourdatasetcontainsdetailsofthetransactionsthattookplaceonthemarketplaceofAmazon.comforeachofthesoft-waretitles.TheAmazonWebServicesassociatesauniquetransactionIDforeachuniqueproductlistedbyaseller.ThistransactionIDenablesustodistin-guishbetweenmultipleorsuccessivelistingsofiden-ticalproductssoldbythesamemerchant.Keepingwiththemethodologyinpriorresearch(Ghoseetal.,2006),wecrawltheAmazon’sXMLlistingsevery8hoursandwhenatransactionIDassociatedwithaparticularlistingisremoved,weinferthatthelistedproductwassuccessfullysoldintheprior8hourwin-dow.2Foreachtransactionthattakesplace,wekeepthepriceatwhichtheproductwassoldandthemer-chant’sreputationatthetimeofthetransaction(moreonthislater).Additionally,foreachofthecompetinglistingsforidenticalproducts,wekeepthelistedpricealongwiththecompetitorsreputation.Usingthecol-lecteddata,wecomputethepricepremiumvariablesforeachtransaction3usingDeﬁnition2.1.Overall,ourdatasetcontains1,078merchants,9,484uniquetransactionsand107,922pricepremiums(recallthateachtransactiongeneratesmultiplepricepremiums).ReputationData:Thesecondpartofourdatasetcontainsthereputationhistoryofeachmerchantthathada(monitored)productforsaleduringour180-daywindow.Eachofthesemerchantshasafeedbackpro-ﬁle,whichconsistsofnumericalscoresandtext-basedfeedback,postedbybuyers.Wehadanaverageof4,932postingspermerchant.Thenumericalratings2AmazonindicatesthattheirsellerlistingsremainonthesiteindeﬁnitelyuntiltheyaresoldandsellerscanchangethepriceoftheproductwithoutalteringthetransactionID.3Ideally,wewouldalsoincludethetaxandshippingcostchargedbyeachmerchantinthecomputationofthepricepre-miums.Unfortunately,wecouldnotcapturethesecostsusingourmethodology.Assumingthatthefeesforshippingandtaxareindependentofthemerchants’reputation,ouranalysisisnotaffected.419

areprovidedonascaleofonetoﬁvestars.Theserat-ingsareaveragedtoprovideanoverallscoretotheseller.Notethatwecollectallfeedback(bothnumeri-calandtextual)associatedwithasellerovertheentirelifetimeofthesellerandwereconstructeachseller’sexactfeedbackproﬁleatthetimeofeachtransaction.4Econometrics-basedOpinionMiningInthissection,wedescribehowwecombineecono-metrictechniqueswithNLPtechniquestoderivethesemanticorientationandstrengthofthefeedbackevaluations.Section4.1describeshowwestructurethetextualfeedbackandSection4.2showshowweuseeconometricstoestimatethepolarityandstrengthoftheevaluations.4.1RetrievingtheDimensionsofReputationWecharacterizeamerchantusingavectorofreputa-tiondimensionsX=(X1,X2,...,Xn),representingitsabilityoneachofndimensions.Weassumethateachofthesendimensionsisexpressedbyanoun,nounphrase,verb,oraverbphrasechosenfromthesetofallfeedbackpostings,andthatamerchantisevaluatedonthesendimensions.Forexample,di-mension1mightbe“shipping”,dimension2mightbe“packaging”andsoon.Inourmodel,eachofthesedimensionsisassignedanumericalscore.Ofcourse,whenpostingtextualfeedback,buyersdonotassignexplicitnumericscorestoanydimension.Rather,theyusemodiﬁers(typicallyadjectivesoradverbs)toeval-uatetheselleralongeachofthesedimensions(wede-scribehowweassignnumericscorestoeachmodiﬁerinSection4.2).Oncewehaveidentiﬁedthesetofalldimensions,wecanthenparseeachofthefeedbackpostings,associateamodiﬁerwitheachdimension,andrepresentafeedbackpostingasann-dimensionalvectorφofmodiﬁers.Example4.1Supposedimension1is“delivery,”di-mension2is“packaging,”anddimension3is“ser-vice.”Thefeedbackposting“Iwasimpressedbythespeedydelivery!Greatservice!”isthenencodedasφ1=[speedy,NULL,great],whiletheposting“Theitemarrivedinawfulpackaging,andthedeliverywasslow”isencodedasφ2=[slow,awful,NULL].2LetM={NULL,µ1,...,µM}bethesetofmodi-ﬁersandconsiderasellersiwithppostingsinitsrep-utationproﬁle.Wedenotewithµijk∈Mthemodiﬁerthatappearsinthej-thpostingandisusedtoassessthek-threputationdimension.Wethenstructurethemerchant’sfeedbackasann×pmatrixM(si)whoserowsarethepencodedvectorsofmodiﬁersassociatedwiththeseller.WeconstructM(si)asfollows:1.Retrievethepostingsassociatedwithamerchant.2.Parsethepostingstoidentifythedimensionsacrosswhichthebuyerevaluatesaseller,keep-ing4thenouns,nounphrases,verbs,andverbalphrasesasreputationcharacteristics.5.3.Retrieveadjectivesandadverbsthatreferto6di-mensions(Step2)andconstructtheφvectors.Wehaveimplementedthisalgorithmonthefeed-backpostingsofeachofoursellers.Ouranalysisyields151uniquedimensions,andatotalof142mod-iﬁers(notethatthesamemodiﬁercanbeusedtoeval-uatemultipledimensions).4.2ScoringtheDimensionsofReputationAsdiscussedabove,thetextualfeedbackproﬁleofmerchantsiisencodedasan×pmatrixM(si);theelementsofthismatrixbelongtothesetofmodiﬁersM.Inourcase,weareinterestedincomputingthe“score”a(µ,d,j)thatamodiﬁerµ∈Massignstothedimensiond,whenitappearsinthej-thposting.Sincebuyerstendtoreadonlytheﬁrstfewpagesoftext-basedfeedback,weweighthighertheinﬂu-enceofrecenttextpostings.Wemodelthisbyas-sumingthatKisthenumberofpostingsthatappearoneachpage(K=25onAmazon.com),andthatcistheprobabilityofclickingonthe“Next”linkandmovingthenextpageofevaluations.7Thisassignsaposting-speciﬁcweightrj=cbjKc/Ppq=1cbqKcforthejthposting,wherejistherankoftheposting,Kisthenumberofpostingsperpage,andpisthetotalnumberofpostingsforthegivenseller.Then,weseta(µ,d,j)=rj·a(µ,d)wherea(µ,d)isthe“global”scorethatmodiﬁerµassignstodimensiond.Finally,sinceeachreputationdimensionhaspoten-tiallyadifferentweight,weuseaweightvectorwto4Weeliminatealldimensionsappearingintheproﬁlesoflessthan50(outof1078)merchants,sincewecannotextractstatisti-callymeaningfulresultsforsuchsparsedimensions5Thetechniqueasdescribedinthispaper,considerswordslike“shipping”and“delivery”asseparatedimensions,althoughtheyrefertothesame“real-life”dimension.WecanuseLatentDirich-letAllocation(Bleietal.,2003)toreducethenumberofdimen-sions,butthisisoutsidethescopeofthispaper.6Toassociatetheadjectivesandadverbswiththecorrectdi-mensions,weusetheCollinsHeadFindercapabilityoftheStan-fordNLPParser.7Wereportonlyresultsforc=0.5.Weconductedexperi-mentsothervaluesofcaswellandtheresultsaresimilar.420

weightthecontributionofeachreputationdimensiontotheoverall“reputationscore”Π(si)ofsellersi:Π(si)=rT·A(M(si))·w(1)whererT=[r1,r2,...rp]isthevectoroftheposting-speciﬁcweightsandA(M(i))isamatrixthatcon-tainsaselementthescorea(µj,dk)whereM(si)con-tainsthemodiﬁerµjinthecolumnofthedimen-siondk.Ifwemodelthebuyers’preferencesasinde-pendentlydistributedalongeachdimensionandeachmodiﬁerscorea(µ,dk)alsoasanindependentran-domvariable,thentherandomvariableΠ(si)isasumofrandomvariables.Speciﬁcally,wehave:Π(si)=MXj=1nXk=1(wk·a(µj,dk))R(µj,dk)(2)whereR(µj,dk)isequaltothesumoftheriweightsacrossallpostingsinwhichthemodiﬁerµjmodiﬁesdimensiondk.WecaneasilycomputetheR(µj,dk)valuesbysimplycountingappearancesandweightingeachappearanceusingthedeﬁnitionofri.Thequestionis,ofcourse,howtoestimatetheval-uesofwk·a(µj,dk),whichdeterminethepolarityandintensityofthemodiﬁerµjmodifyingthedimen-siondk.Forthis,weobservethattheappearanceofsuchmodiﬁer-dimensionopinionphraseshasanef-fectonthepricepremiumsthatamerchantcancharge.Hence,thereisacorrelationbetweenthereputationscoresΠ(·)ofthemerchantsandthepricepremi-umsobservedforeachtransaction.Todiscoverthelevelofassociation,weuseregression.Sincewearedealingwithpaneldata,weestimateordinary-least-squares(OLS)regressionwithﬁxedeffects(Greene,2002),wherethedependentvariableisthepricepre-miumvariable,andtheindependentvariablesarethereputationscoresΠ(·)ofthemerchants,togetherwithafewothercontrolvariables.Generally,weestimatemodelsoftheform:PricePremiumij=Xβc·Xcij+fij+ij+βt1·Π(merchant)ij+βt2·Π(competitor)ij(3)wherePricePremiumijisoneofthevariationsofpricepremiumasgiveninDeﬁnition2.1forasellersiandproductj,βc,βt1,andβt2aretheregressorco-efﬁcients,Xcarethecontrolvariables,Π(·)arethetextreputationscores(seeEquation1),fijdenotestheﬁxedeffectsandistheerrorterm.InSection5,wegivethedetailsaboutthecontrolvariablesandthere-gressionsettings.Interestingly,ifweexpandtheΠ(·)variablesac-cordingtoEquation2,wecanruntheregressionus-ingthemodiﬁer-dimensionpairsasindependentvari-ables,whosevaluesareequaltotheR(µj,dk)val-ues.Afterrunningtheregression,thecoefﬁcientsas-signedtoeachmodiﬁer-dimensionpaircorrespondtothevaluewk·a(µj,dk)foreachmodiﬁer-dimensionpair.Therefore,wecaneasilyestimateineconomictermsthe“value”ofaparticularmodiﬁerwhenusedtoevaluateaparticulardimension.5ExperimentalEvaluationInthissection,weﬁrstpresenttheexperimentalset-tings(Section5.1),andthenwedescribetheresultsofourexperimentalevaluation(Section5.2).5.1RegressionSettingsInEquation3wepresentedthegeneralformoftheregressionforestimatingthescoresa(µj,dk).Sincewewanttoeliminatetheeffectofanyotherfactorsthatmayinﬂuencethepricepremiums,wealsouseasetofcontrolvariables.Afterallthecontrolfactorsaretakenintoconsideration,themodiﬁerscoresre-ﬂecttheadditionalvalueofthetextopinions.Speciﬁ-cally,weusedascontrolvariablestheproduct’spriceonAmazon,theaveragestarratingofthemerchant,thenumberofmerchant’spasttransactions,andthenumberofsellersfortheproduct.First,weranOLSregressionswithproduct-sellerﬁxedeffectscontrollingforunobservedheterogene-ityacrosssellersandproducts.Theseﬁxedeffectscontrolforaverageproductqualityanddifferencesinsellercharacteristics.Werunmultiplevariationsofourmodel,usingdifferentversionsofthe“pricepremium”variableaslistedinDeﬁnition2.1.Wealsotestedvariationswhereweincludeasindepen-dentvariablenottheindividualreputationscoresbutthedifferenceΠ(merchant)−Π(competitor).Allre-gressionsyieldedqualitativelysimilarresults,soduetospacerestrictionsweonlyreportresultsforthere-gressionsthatincludeallthecontrolvariablesandallthetextvariables;wereportresultsusingthepricepremiumasthedependentvariable.Ourregressionsinthissettingcontain107,922observations,andato-talof547independentvariables.5.2ExperimentalResultsRecallofExtraction:Theﬁrststepofourexperi-mentalevaluationistoexaminewhethertheopinionextractiontechniqueofSection4.1indeedcapturesallthereputationcharacteristicsexpressedinthefeed-421

DimensionHumanRecallComputerRecallProductCondition0.760.76Price0.910.61Package0.960.66OverallExperience0.650.55DeliverySpeed0.960.92ItemDescription0.220.43ProductSatisfaction0.680.58ProblemResponse0.300.37CustomerService0.570.50Average0.660.60Table1:Therecallofourtechniquecomparedtotherecallofthehumanannotatorsback(recall)andwhetherthedimensionsthatwecap-tureareaccurate(precision).Toexaminetherecallquestion,weusedtwohumanannotators.Theannota-torsreadarandomsampleof1,000feedbackpostings,andidentiﬁedthereputationdimensionsmentionedinthetext.Then,theyexaminedtheextractedmodiﬁer-dimensionpairsforeachpostingandmarkedwhetherthemodiﬁer-dimensionpairscapturedtheidentiﬁedrealreputationdimensionsmentionedinthepostingandwhichpairswerespurious,non-opinionphrases.Bothannotatorsidentiﬁedninereputationdimen-sions(seeTable1).Sincetheannotatorsdidnotagreeinallannotations,wecomputedtheaveragehumanrecallhRecd=agreeddalldforeachdimensiond,whereagreeddisthenumberofpostingsforwhichbothan-notatorsidentiﬁedthereputationdimensiond,andalldisthenumberofpostingsinwhichatleastoneannotatoridentiﬁedthedimensiond.Basedontheannotations,wecomputedtherecallofouralgorithmagainsteachannotator.Wereporttheaveragerecallforeachdimension,togetherwiththehumanrecallinTable1.Therecallofourtechniqueisonlyslightlyinferiortotheperformanceofhumans,indicatingthatthetechniqueofSection4.1extractsthemajorityofthepostedevaluations.8Interestingly,precisionisnotanissueinoursetting.Inourframework,ifanparticularmodiﬁer-dimensionpairisjustnoise,thenitisalmostimpossibletohaveastatisticallysigniﬁcantcorrelationwiththepricepre-miums.Thenoisyopinionphrasesarestatisticallyguaranteedtobeﬁlteredoutbytheregression.EstimatingPolarityandStrength:InTable2,8Inthecaseof“ItemDescription,”wherethecomputerrecallwashigherthanthehumanrecall,ourtechniqueidentiﬁedalmostallthephrasesofoneannotator,buttheotherannotatorhadamoreliberalinterpretationof“ItemDescription”dimensionandannotatedsigniﬁcantlymorepostingswiththedimension“ItemDescription”thantheotherannotator,thusdecreasingthehumanrecall.wepresentthemodiﬁer-dimensionpairs(positiveandnegative)thathadthestrongest“dollarvalue”andwerestatisticallysigniﬁcantacrossallregressions.(Duetospaceissues,wecannotlistthevaluesforallpairs.)Thesevaluesreﬂectchangesinthemerchants’spricingpoweraftertakingtheiraveragenumericalscoreandlevelofexperienceintoaccount,andalsohighlighttheadditionalthevaluecontainedintext-basedreputation.Theexamplesthatwelisthereil-lustratethatourtechniquegeneratesanaturalrankingoftheopinionphrases,inferringthestrengthofeachmodiﬁerwithinthecontextinwhichthisopinionisevaluated.Thisholdstrueevenformisspelledevalua-tionsthatwouldbreakexistingtechniquesbasedonannotationoronresourceslikeWordNet.Further-more,thesevaluesreﬂectthecontextinwhichtheopinionisevaluated.Forexample,thepairgoodpack-aginghasadollarvalueof-$0.58.Eventhoughthisseemscounterintuitive,itactuallyreﬂectsthenatureofanonlinemarketplacewheremostofthepositiveevaluationscontainsuperlatives,andamere“good”isactuallyinterpretedbythebuyersasalukewarm,slightlynegativeevaluation.Existingtechniquescan-notcapturesuchphenomena.PricePremiumsvs.Ratings:Oneofthenaturalcomparisonsistoexaminewhetherwecouldreachsimilarresultsbyjustusingtheaveragestarratingas-sociatedwitheachfeedbackpostingtoinferthescoreofeachopinionphrase.Theunderlyingassumptionbehindusingtheratingsisthatthereviewisper-fectlysummarizedbythestarrating,andhencethetextplaysmainlyanexplanatoryroleandcarriesnoextrainformation,giventhestarrating.Forthis,weexaminedtheR2ﬁtoftheregression,withandwith-outtheuseofthetextvariables.Withouttheuseoftextvariables,theR2was0.35,whilewhenusingonlythetext-basedregressors,theR2ﬁtincreasedto0.63.Thisresultclearlyindicatesthattheactualtextcon-tainssigniﬁcantlymoreinformationthantheratings.Wealsoexperimentedwithpredictingwhichmer-chantwillmakeasale,iftheysimultaneouslysellthesameproduct,basedontheirlistedpricesandontheirnumericandtextreputation.OurC4.5classi-ﬁer(Quinlan,1992)takesapairofmerchantsandde-cideswhichofthetwowillmakeasale.Weusedastrainingsetthetransactionsthattookplaceintheﬁrstfourmonthsandastestsetthetransactionsinthelasttwomonthsofourdataset.Table3summarizestheresultsfordifferentsetsoffeaturesused.The55%422

ModiﬁerDimensionDollarValue[wonderfulexperience]$5.86[outstandingseller]$5.76[excellantservice]$5.27[lightningdelivery]$4.84[highlyrecommended]$4.15[bestseller]$3.80[perfectlypackaged]$3.74[excellentcondition]$3.53[excellentpurchase]$3.22[excellentseller]$2.70[excellentcommunication]$2.38[perfectitem]$1.92[terriﬁccondition]$1.87[topquality]$1.67[awesomeservice]$1.05[A+++seller]$1.03[greatmerchant]$0.93[friendlyservice]$0.81[easyservice]$0.78[neverreceived]-$7.56[defectiveproduct]-$6.82[horibleexperience]-$6.79[neversent]-$6.69[neverrecieved]-$5.29[badexperience]-$5.26[cancelledorder]-$5.01[neverresponded]-$4.87[wrongproduct]-$4.39[notasadvertised]-$3.93[poorpackaging]-$2.92[lateshipping]-$2.89[wrongitem]-$2.50[notyetreceived]-$2.35[stillwaiting]-$2.25[wrongaddress]-$1.54[neverbuy]-$1.48Table2:Thehighestscoringopinionphrases,asde-terminedbytheproductwk·a(µj,dk).accuracywhenusingonlypricesasfeaturesindicatesthatcustomersrarelychooseaproductbasedsolelyonprice.Rather,asindicatedbythe74%accuracy,theyalsoconsiderthereputationofthemerchants.How-ever,therealvalueofthepostingsreliesonthetextandnotonthenumericratings:theaccuracyis87%-89%whenusingthetextualreputationvariables.Infact,textsubsumesthenumericvariablesbutnotviceversa,asindicatedbytheresultsinTable3.6RelatedWorkTothebestofourknowledge,ourworkistheﬁrsttouseeconomicsformeasuringtheeffectofopinionsandderivingtheirpolarityandstrengthinanecono-metricmanner.Afewpapersinthepasttriedtocombinetextanalysiswitheconomics(DasandChen,2006;LewittandSyverson,2005),butthetextanal-ysiswaslimitedtotokencountinganddidnotuseFeaturesAccuracyonTestSetPrice55%Price+NumericReputation74%Price+NumericReputation89%+TextReputationPrice+TextReputation87%Table3:Predictingthemerchantwhomakesthesale.anyNLPtechniques.ThetechniqueofSection4.1isbasedonexistingresearchinsentimentanalysis.Forinstance,(HatzivassiloglouandMcKeown,1997;NigamandHurst,2004)useannotateddatatocreateasupervisedlearningtechniquetoidentifythesemanticorientationofadjectives.WefollowtheapproachbyTurney(2002),whonotethatthesemanticorientationofanadjectivedependsonthenounthatitmodiﬁesandsuggestusingadjective-nounoradverb-verbpairstoextractsemanticorientation.However,wedonotrelyonlinguisticresources(KampsandMarx,2002)oronsearchengines(TurneyandLittman,2003)todeterminethesemanticorientation,butratherrelyoneconometricsforthistask.HuandLiu(2004),whosestudyistheclosesttoourwork,useWordNettocom-putethesemanticorientationofproductevaluationsandtrytosummarizeuserreviewsbyextractingthepositiveandnegativeevaluationsofthedifferentprod-uctfeatures.Similarly,SnyderandBarzilay(2007)decomposeanopinionacrossseveraldimensionsandcapturethesentimentacrosseachdimension.Otherworkinthisareaincludes(Lee,2004;PopescuandEtzioni,2005)whichusestextmininginthecontextproductreviews,butnoneusestheeconomiccontexttoevaluatetheopinions.7ConclusionandFurtherApplicationsWedemonstratedthevalueofusingeconometricsforextractingaquantitativeinterpretationofopin-ions.Ourtechnique,additionally,takesintocon-siderationthecontextwithinwhichtheseopinionsareevaluated.Ourexperimentalresultsshowthatourtechniquescancapturethepragmaticmean-ingoftheexpressedopinionsusingsimpleeco-nomicvariablesasaformoftrainingdata.Thesourcecodewithourimplementationtogetherwiththedatasetusedinthispaperareavailablefromhttp://economining.stern.nyu.edu.Therearemanyotherapplicationsbeyondreputa-tionsystems.Forexample,usingsalesrankdatafromAmazon.com,wecanexaminetheeffectofproductreviewsonproductsalesanddetecttheweightthat423

customersputondifferentproductfeatures;further-more,wecandiscoverhowcustomerevaluationsonindividualproductfeaturesaffectproductsalesandextractthepragmaticmeaningoftheseevaluations.Anotherapplicationistheanalysisoftheeffectofnewsstoriesonstockprices:wecanexaminewhatnewstopicsareimportantforthestockmarketandseehowtheviewsofdifferentopinionholdersandthewordingthattheyusecancausethemarkettomoveupordown.Inaslightlydifferenttwist,wecanana-lyzenewsstoriesandblogsinconjunctionwithresultsfrompredictionmarketsandextractthepragmaticef-fectofnewsandblogsonelectionsorotherpoliticalevents.Anotherresearchdirectionistoexaminetheeffectofsummarizingproductdescriptionsonprod-uctsales:shortdescriptionsreducethecognitiveloadofconsumersbutincreasetheiruncertaintyabouttheunderlyingproductcharacteristics;alongerdescrip-tionhastheoppositeeffect.Theoptimumdescriptionlengthistheonethatbalancesbotheffectsandmaxi-mizesproductsales.Similarapproachescanimprovethestateofartinbotheconomicsandcomputationallinguistics.Ineco-nomicsandinsocialsciencesingeneral,mostre-searchershandletextualdatamanuallyorwithsim-plistictokencountingtechniques;intheworstcasetheyignoretextdataaltogether.Incomputationallinguistics,researchersoftenrelyonhumanannota-torstogeneratetrainingdata,alaboriousanderror-pronetask.Webelievethatcross-fertilizationofideasbetweentheﬁeldsofcomputationallinguisticsandeconometricscanbebeneﬁcialforbothﬁelds.AcknowledgmentsTheauthorswouldliketothankElenaFilatovafortheusefuldiscussionsandthepointerstorelatedlit-erature.WealsothankSanjeevDewan,AlokGupta,BinGu,andseminarparticipantsatCarnegieMel-lonUniversity,ColumbiaUniversity,MicrosoftRe-search,NewYorkUniversity,PolytechnicUniversity,andUniversityofFloridafortheircommentsandfeedback.WethankRhongZhengforassistanceindatacollection.ThisworkwaspartiallysupportedbyaMicrosoftLiveLabsSearchAward,aMicrosoftVir-tualEarthAward,andbyNSFgrantsIIS-0643847andIIS-0643846.Anyopinions,ﬁndings,andconclusionsexpressedinthismaterialarethoseoftheauthorsanddonotnecessarilyreﬂecttheviewsoftheMicrosoftCorporationoroftheNationalScienceFoundation.ReferencesD.M.Blei,A.Y.Ng,andM.I.Jordan.2003.LatentDirichletallocation.JMLR,3:993–1022.E.Breck,Y.Choi,andC.Cardie.2007.Identifyingexpressionsofopinionincontext.InIJCAI-07,pages2683–2688.H.Cui,V.Mittal,andM.Datar.2006.Comparativeexperi-mentsonsentimentclassiﬁcationforonlineproductreviews.InAAAI-2006.S.RanjanDasandM.Chen.2006.Yahoo!forAmazon:Senti-mentextractionfromsmalltalkontheweb.WorkingPaper,SantaClaraUniversity.K.Dave,S.Lawrence,andD.M.Pennock.2003.Miningthepeanutgallery:Opinionextractionandsemanticclassiﬁcationofproductreviews.InWWW12,pages519–528.C.Dellarocas.2003.Thedigitizationofword-of-mouth:Promiseandchallengesofonlinereputationmechanisms.ManagementScience,49(10):1407–1424.A.Ghose,M.D.Smith,andR.Telang.2006.Internetexchangesforusedbooks:Anempiricalanalysisforproductcannibal-izationandsocialwelfare.InformationSystemsResearch,17(1):3–19.W.H.Greene.2002.EconometricAnalysis.5thedition.V.HatzivassiloglouandK.R.McKeown.1997.Predictingthesemanticorientationofadjectives.InACL’97,pages174–181.M.HuandB.Liu.2004.Miningandsummarizingcustomerreviews.InKDD-2004,pages168–177.J.KampsandM.Marx.2002.Wordswithattitude.InProceed-ingsoftheFirstInternationalConferenceonGlobalWordNet.S.-M.KimandE.Hovy.2004.Determiningthesentimentofopinions.InCOLING2004,pages1367–1373.A.C.K¨onigandE.Brill.2006.Reducingthehumanoverheadintextcategorization.InKDD-2006,pages598–603.T.Lee.2004.Use-centricminingofcustomerreviews.InWITS.S.LewittandC.Syverson.2005.Marketdistortionswhenagentsarebetterinformed:Thevalueofinformationinrealestatetransactions.WorkingPaper,UniversityofChicago.M.I.MelnikandJ.Alm.2002.Doesaseller’sreputationmat-ter?EvidencefromeBayauctions.JournalofIndustrialEco-nomics,50(3):337–350,September.K.NigamandM.Hurst.2004.Towardsarobustmetricofopin-ion.InAAAISpringSymposiumonExploringAttitudeandAffectinText,pages598–603.B.PangandL.Lee.2002.Thumbsup?Sentimentclassiﬁcationusingmachinelearningtechniques.InEMNLP2002.B.PangandL.Lee.2004.Asentimentaleducation:Sentimentanalysisusingsubjectivitysummarizationbasedonminimumcuts.InACL2004,pages271–278.B.PangandL.Lee.2005.Seeingstars:Exploitingclassrelation-shipsforsentimentcategorizationwithrespecttoratingscales.InACL2005.A.-M.PopescuandO.Etzioni.2005.Extractingproductfeaturesandopinionsfromreviews.InHLT/EMNLP2005.B.SnyderandR.Barzilay.2007.Multipleaspectrankingusingthegoodgriefalgorithm.InHLT-NAACL2007.J.R.Quinlan.1992.C4.5:ProgramsforMachineLearning.MorganKaufmannPublishers,Inc.P.Resnick,K.Kuwabara,R.Zeckhauser,andE.Friedman.2000.Reputationsystems.CACM,43(12):45–48,December.P.D.TurneyandM.L.Littman.2003.Measuringpraiseandcriticism:Inferenceofsemanticorientationfromassociation.ACMTransactionsonInformationSystems,21(4):315–346.P.D.Turney.2002.Thumbsuporthumbsdown?Semanticori-entationappliedtounsupervisedclassiﬁcationofreviews.InACL2002,pages417–424.T.Wilson,J.Wiebe,andR.Hwa.2006.Recognizingstrongandweakopinionclauses.ComputationalIntell.,22(2):73–99.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 424–431,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

424

PageRankingWordNetSynsets:AnApplicationtoOpinionMining∗AndreaEsuliandFabrizioSebastianiIstitutodiScienzaeTecnologiedell’InformazioneConsiglioNazionaledelleRicercheViaGiuseppeMoruzzi,1–56124Pisa,Italy{andrea.esuli,fabrizio.sebastiani}@isti.cnr.itAbstractThispaperpresentsanapplicationofPageR-ank,arandom-walkmodeloriginallyde-visedforrankingWebsearchresults,torankingWordNetsynsetsintermsofhowstronglytheypossessagivensemanticprop-erty.Thesemanticpropertiesweuseforex-emplifyingtheapproacharepositivityandnegativity,twopropertiesofcentralimpor-tanceinsentimentanalysis.TheideaderivesfromtheobservationthatWordNetmaybeseenasagraphinwhichsynsetsarecon-nectedthroughthebinaryrelation“atermbelongingtosynsetskoccursintheglossofsynsetsi”,andonthehypothesisthatthisrelationmaybeviewedasatransmit-terofsuchsemanticproperties.ThedataforthisrelationcanbeobtainedfromeX-tendedWordNet,apubliclyavailablesense-disambiguatedversionofWordNet.Wear-guethatthisrelationisstructurallyakintotherelationbetweenhyperlinkedWebpages,andthuslendsitselftoPageRankanalysis.Wereportexperimentalresultssupportingourintuitions.1IntroductionRecentyearshavewitnessedanexplosionofworkonopinionmining(akasentimentanalysis),thedis-∗ThisworkwaspartiallysupportedbyProjectONTOTEXT“FromTexttoKnowledgefortheSemanticWeb”,fundedbytheProvinciaAutonomadiTrentounderthe2004–2006“FondoUnicoperlaRicerca”fundingscheme.ciplinethatdealswiththequantitativeandqualita-tiveanalysisoftextforthepurposeofdeterminingitsopinion-relatedproperties(ORPs).Animportantpartofthisresearchhasbeentheworkontheauto-maticdeterminationoftheORPsofterms,ase.g.,indeterminingwhetheranadjectivetendstogiveapositive,anegative,oraneutralnaturetothenounphraseitappearsin.Whilemanyworks(EsuliandSebastiani,2005;HatzivassiloglouandMcKeown,1997;Kampsetal.,2004;Takamuraetal.,2005;TurneyandLittman,2003)viewthepropertiesofpositivityandnegativityascategorical(i.e.,atermiseitherpositiveoritisnot),others(AndreevskaiaandBergler,2006b;Grefenstetteetal.,2006;KimandHovy,2004;SubasicandHuettner,2001)viewthemasgraded(i.e.,atermmaybepositivetoacertaindegree),withtheunderlyinginterpretationvaryingfromfuzzytoprobabilistic.Someauthorsgoastepfurtherandattachthesepropertiesnottotermsbuttotermsenses(typ-ically:WordNetsynsets),ontheassumptionthatdifferentsensesofthesametermmayhavedif-ferentopinion-relatedproperties(AndreevskaiaandBergler,2006a;EsuliandSebastiani,2006b;Ide,2006;WiebeandMihalcea,2006).InthispaperwecontributetothislatterliteraturewithanovelmethodforrankingtheentiresetofWordNetsynsets,irrespectivelyofPOS,accordingtotheirORPs.Tworankingsareproduced,oneac-cordingtopositivityandoneaccordingtonegativity.Thetworankingsareindependent,i.e.,itisnotthecasethatoneistheinverseoftheother,sincee.g.,theleastpositivesynsetsmaybenegativeorneutralsynsetsalike.425

ThemainhypothesisunderlyingourmethodisthatthepositivityandnegativityofWordNetsynsetscanbedeterminedbyminingtheirglosses.ItcruciallyreliesontheobservationthattheglossofaWordNetsynsetcontainstermsthatthem-selvesbelongtosynsets,andonthehypothesisthattheglossesofpositive(resp.negative)synsetswillmostlycontaintermsbelongingtopositive(nega-tive)synsets.ThismeansthatthebinaryrelationsiIsk(“theglossofsynsetsicontainsatermbelongingtosynsetsk”),whichinducesadirectedgraphonthesetofWordNetsynsets,maybethoughtofasachannelthroughwhichpositivityandnega-tivityﬂow,fromthedeﬁniendum(thesynsetsibe-ingdeﬁned)tothedeﬁniens(asynsetskthatcon-tributestothedeﬁnitionofsibyvirtueofitsmembertermsoccurringintheglossofsi).Inotherwords,ifasynsetsiisknowntobepositive(negative),thiscanbeviewedasanindicationthatthesynsetssktowhichthetermsoccurringintheglossofsibelong,arethemselvespositive(negative).WeobtainthedataoftheIrelationfromeX-tendedWordNet(Harabagiuetal.,1999),anauto-maticallysense-disambiguatedversionofWordNetinwhicheverytermoccurrenceineveryglossislinkedtothesynsetitisdeemedtobelongto.InordertocomputehowpolarityﬂowsinthegraphofWordNetsynsetsweusethewellknownPageRankalgorithm(BrinandPage,1998).PageR-ank,arandom-walkmodelforrankingWebsearchresultswhichliesatthebasisoftheGooglesearchengine,isprobablythemostimportantsinglecontri-butiontotheﬁeldsofinformationretrievalandWebsearchofthelasttenyears,andwasoriginallyde-visedinordertodetecthowauthoritativenessﬂowsintheWebgraphandhowitisconferredontoWebsites.TheadvantagesofPageRankareitsstrongtheoreticalfoundations,itsfastconvergenceproper-ties,andtheeffectivenessofitsresults.ThereasonwhyPageRank,amongallrandom-walkalgorithms,isparticularlysuitedtoourapplicationwillbedis-cussedintherestofthepaper.Notehoweverthatourmethodisnotlimitedtorankingsynsetsbypositivityandnegativity,andcouldinprinciplebeappliedtothedeterminationofothersemanticpropertiesofsynsets,suchasmem-bershipinadomain,sinceformanyotherpropertieswemayhypothesizetheexistenceofasimilar“hy-draulics”betweensynsets.Wethusseepositivityandnegativityonlyasproofs-of-conceptforthepo-tentialofthemethod.Therestofthepaperisorganizedasfollows.Sec-tion2reportsonrelatedworkontheORPsoflex-icalitems,highlightingthesimilaritiesanddiffer-encesbetweenthediscussedmethodsandourown.InSection3weturntodiscussingourmethod;inor-dertomakethepaperself-contained,westartwithabriefintroductionofPageRank(Section3.1)andofthestructureofeXtendedWordNet(Section3.2).Section4describesthestructureofourexperiments,whileSection5discussestheresultswehaveob-tained,comparingthemwithotherresultsfromtheliterature.Section6concludes.2RelatedworkSeveralworkshaverecentlytackledtheautomateddeterminationoftermpolarity.HatzivassiloglouandMcKeown(1997)determinethepolarityofadjec-tivesbyminingpairsofconjoinedadjectivesfromtext,andobservingthatconjunctionssuchasandtendtoconjoinadjectivesofthesamepolaritywhileconjunctionssuchasbuttendtoconjoinadjectivesofoppositepolarity.TurneyandLittman(2003)de-terminethepolarityofgenerictermsbycomputingthepointwisemutualinformation(PMI)betweenthetargettermandeachofasetof“seed”termsofknownpositivityornegativity,wherethemarginalandjointprobabilitiesneededforPMIcomputationareequatedtothefractionsofdocumentsfromagivencorpusthatcontaintheterms,individuallyorjointly.Kampsetal.(2004)determinethepolarityofadjectivesbycheckingwhetherthetargetadjec-tiveisclosertothetermgoodortothetermbadinthegraphinducedonWordNetbythesynonymyrelation.KimandHovy(2004)determinethepo-larityofgenerictermsbymeansoftwoalternativelearning-freemethodsthatusetwosetsofseedtermsofknownpositivityandnegativity,andarebasedonthefrequencywithwhichsynonymsofthetargettermalsoappearintherespectiveseedsets.Amongtheseworks,(TurneyandLittman,2003)hasprovenbyfarthemosteffective,butitisalsobyfarthemostcomputationallyintensive.Somerecentworkshaveemployed,asinthepresentpaper,theglossesfromonlinedictionar-426

iesfortermpolaritydetection.AndreevskaiaandBerger(2006a)extendasetoftermsofknownpos-itivity/negativitybyaddingtothemallthetermswhoseglossescontainthem;thisalgorithmdoesnotviewglossesasasourceforagraphofterms,andisbasedonadifferentintuitionthanours.EsuliandSebastiani(2005;2006a)determinetheORPsofgenerictermsbylearning,inasemi-supervisedway,abinarytermclassiﬁerfromasetoftrainingtermsthathavebeengivenvectorialrepresentationsbyin-dexingtheirWordNetglosses.ThesameauthorslaterextendtheirworktodeterminingtheORPsofWordNetsynsets(EsuliandSebastiani,2006b).However,thereisasubstantialdifferencebetweentheseworksandthepresentone,inthattheformersimplyviewtheglossesassourcesoftextualrepre-sentationsfortheterms/synsets,andnotasinducingagraphofsynsetsasweinsteadviewthemhere.TheworkclosestinspirittothepresentoneisprobablythatbyTakamuraetal.(2005),whode-terminethepolarityoftermsbyapplyingintuitionsfromthetheoryofelectronspins:twotermsthatap-pearoneintheglossoftheotherareviewedasakintotwoneighbouringelectrons,whichtendtoacquirethesame“spin”(anotionviewedasakintopolarity)duetotheirbeingneighbours.Thisworkissimi-lartoourssinceagraphbetweentermsisgeneratedfromdictionaryglosses,andsinceaniterativealgo-rithmthatconvergestoastablestateisused,butthealgorithmisverydifferent,andbasedonintuitionsfromverydifferentwalksoflife.Somerecentworkshavetackledtheattributionofopinion-relatedpropertiestowordsensesorsynsets(Ide,2006;WiebeandMihalcea,2006)1;however,theydonotuseglossesinanysigniﬁcantway,andarethusverydifferentfromourmethod.Theinterestedreadermayalsoconsult(Mihalcea,2006)forotherapplicationsofrandom-walkmodelstocomputationallinguistics.3RankingWordNetsynsetsbyPageRank3.1ThePageRankalgorithmLetG=hN,Libeadirectedgraph,withNitssetofnodesandLitssetofdirectedlinks;letW0be1AndreevskaiaandBerger(2006a)alsoworkontermsenses,ratherthanterms,buttheyevaluatetheirworkontermsonly.Thisisthereasonwhytheyarelistedintheprecedingparagraphandnothere.the|N|×|N|adjacencymatrixofG,i.e.,thema-trixsuchthatW0[i,j]=1iffthereisalinkfromnodenitonodenj.WewilldenotebyB(i)={nj|W0[j,i]=1}thesetofthebackwardneigh-boursofni,andbyF(i)={nj|W0[i,j]=1}thesetoftheforwardneighboursofni.LetWbetherow-normalizedadjacencymatrixofG,i.e.,thematrixsuchthatW[i,j]=1|F(i)|iffW0[i,j]=1andW[i,j]=0otherwise.TheinputtoPageRankistherow-normalizedad-jacencymatrixW,anditsoutputisavectora=ha1,...,a|N|i,whereairepresentsthe“score”ofnodeni.WhenusingPageRankforsearchresultsranking,niisaWebsiteandaimeasuresitscom-putedauthoritativeness;inourapplicationniisin-steadasynsetandaimeasuresthedegreetowhichnihasthesemanticpropertyofinterest.PageRankiterativelycomputesvectorabasedontheformulaa(k)i←αXj∈B(i)a(k−1)j|F(j)|+(1−α)ei(1)wherea(k)idenotesthevalueofthei-thentryofvec-toraatthek-thiteration,eiisaconstantsuchthatPie|N|i=1=1,and0≤α≤1isacontrolparameter.Invectorialform,Equation1canbewrittenasa(k)=αa(k−1)W+(1−α)e(2)Theunderlyingintuitionisthatanodenihasahighscorewhen(recursively)ithasmanyhigh-scoringbackwardneighbourswithfewforwardneighbourseach;anodenjthuspassesitsscoreajalongtoitsforwardneighboursF(j),butthisscoreissub-dividedequallyamongthemembersofF(j).Thismechanism(thatisrepresentedbythesummationinEquation1)isthen“smoothed”bytheeiconstants,whoseroleis(see(Bianchinietal.,2005)forde-tails)toavoidthatscoresﬂowandgettrappedintoso-called“ranksinks”(i.e.,cliqueswithbackwardneighboursbutnoforwardneighbours).ThecomputationalpropertiesofthePageRankal-gorithm,andhowtocomputeitefﬁciently,havebeenwidelystudied;theinterestedreadermaycon-sult(Bianchinietal.,2005).IntheoriginalapplicationofPageRankforrank-ingWebsearchresultstheelementsofeareusuallytakentobeallequalto1|N|.However,itispossible427

togivedifferentvaluestodifferentelementsine.Infact,thevalueofeiamountstoaninternalsourceofscorefornithatisconstantacrosstheiterationsandindependentfromitsbackwardneighbours.Forinstance,attributinganulleivaluetoallbutafewWebpagesthatareaboutagiventopiccanbeusedinordertobiastherankingofWebpagesinfavourofthistopic(Haveliwala,2003).InthisworkweusetheeivaluesasinternalsourcesofagivenORP(positivityornegativity),byattributinganulleivaluetoallbutafew“seed”synsetsknowntopossessthatORP.PageRankwillthusmaketheORPﬂowfromtheseedsynsets,atarateconstantthroughouttheiterations,intoothersynsetsalongtheIrelation,untilastablestateisreached;theﬁnalaivaluescanbeusedtorankthesynsetsintermsofthatORP.Ourmethodthusre-quirestworunsofPageRank;intheﬁrstehasnon-nullscoresforthepositiveseedsynsets,whileinthesecondthesamehappensforthenegativeones.3.2eXtendedWordNetThetransformationofWordNetintoagraphbasedontheIrelationwouldofcoursebenon-trivial,butisluckilyprovidedbyeXtendedWord-Net(Harabagiuetal.,1999),apubliclyavailableversionofWordNetinwhich(amongotherthings)eachtermskoccurringinaWordNetgloss(ex-ceptthoseinexamplephrases)islemmatizedandmappedtothesynsetinwhichitbelongs2.WeuseeXtendedWordNetversion2.0-1.1,whichreferstoWordNetversion2.0.TheeXtendedWordNetresourcehasbeenautomaticallygenerated,whichmeansthattheassociationsbetweentermsandsynsetsarelikelytobesometimesincorrect,andthisofcourseintroducesnoiseinourmethod.3.3PageRank,(eXtended)WordNet,andORPﬂowWenowdiscusstheapplicationofPageRanktorankingWordNetsynsetsbypositivityandnegativ-ity.Ouralgorithmconsistsinthefollowingsteps:1.ThegraphG=hN,LionwhichPageRankwillbeappliedisgenerated.WedeﬁneNtobethesetofallWordNetsynsets;inWordNet2.0thereare115,424ofthem.WedeﬁneLto2http://xwn.hlt.utdallas.edu/containalinkfromsynsetsitosynsetskifftheglossofsicontainsatleastatermbelongingtosk(termsoccurringintheexamplesphrasesandtermsoccurringafteratermthatexpressesnegationarenotconsidered).Numbers,articlesandprepositionsoccurringintheglossesarediscarded,sincetheycanbeassumedtocarrynopositivityandnegativity,andsincetheydonotbelongtoasynsetoftheirown.Thisleavesonlynouns,adjectives,verbs,andadverbs.2.ThegraphG=hN,Liis“pruned”byremov-ing“self-loops”,i.e.,linksgoingfromasynsetsiintoitself(sinceweassumethatthereisnoﬂowofsemanticsfromaconceptuntoitself).Therow-normalizedadjacencymatrixWofGisderived.3.Theeivaluesareloadedintotheevector;allsynsetsotherthantheseedsynsetsofrenownedpositivity(negativity)aregivenavalueof0.Theαcontrolparameterissettoaﬁxedvalue.Weexperimentwithseveraldifferentversionsoftheevectorandseveraldifferentvaluesofα;seeSection4.3fordetails.4.PageRankisexecutedusingWande,iter-atinguntilapredeﬁnedterminationconditionisreached.Theterminationconditionweuseinthisworkconsistsinthefactthattheco-sineoftheanglebetweena(k)anda(k+1)isaboveapredeﬁnedthresholdχ(herewehavesetχ=1−10−9).5.WerankallthesynsetsofWordNetindescend-ingorderoftheiraiscore.Theprocessisruntwice,onceforpositivityandoncefornegativity.Thelastquestiontobeansweredis:“whyPageR-ank?”ArethecharacteristicsofPageRankmoresuitabletotheproblemofrankingsynsetsthanotherrandom-walkalgorithms?Theanswerisyes,sinceitseemsreasonablethat:1.Iftermscontainedinsynsetskoccurintheglossesofmanypositivesynsets,andifthepos-itivityscoresofthesesynsetsarehigh,thenitislikelythatskisitselfpositive(thesamehap-pensfornegativity).Thisjustiﬁesthesumma-tionofEquation1.428

2.Iftheglossofapositivesynsetthatcontainsaterminsynsetskalsocontainsmanyotherterms,thenthisisaweakerindicationthatskisitselfpositive(thisjustiﬁesdividingby|F(j)|inEquation1).3.TherankingresultingfromthealgorithmneedstobebiasedinfavourofaspeciﬁcORP;thisjustiﬁesthepresenceofthe(1−α)eifactorinEquation1).ThefactthatPageRankisthe“right”random-walkalgorithmforourapplicationisalsoconﬁrmedbysomeexperiments(notreportedhereforreasonsofspace)wehaverunwithslightlydifferentvariantsofthemodel(e.g.,oneinwhichwechallengeintuition2aboveandthusavoiddividingby|F(j)|inEqua-tion1).TheseexperimentshavealwaysreturnedinferiorresultswithrespecttostandardPageRank,therebyconﬁrmingthecorrectnessofourintuitions.4Experiments4.1ThebenchmarkToevaluatethequalityoftherankingsproducedbyourexperimentswehaveusedtheMicro-WNOpcorpus(Cerinietal.,2007)asabenchmark3.Micro-WNOpconsistsinasetof1,105WordNetsynsets,eachofwhichwasmanuallyassignedatripletofscores,oneofpositivity,oneofnegativity,oneofneutrality.TheevaluationwasperformedbyﬁveMScstudentsoflinguistics,proﬁcientsecond-languagespeakersofEnglish.Micro-WNOpisrep-resentativeofWordNetwithrespecttothedifferentpartsofspeech,inthesensethatitcontainssynsetsofthedifferentpartsofspeechinthesamepropor-tionsasintheentireWordNet.However,itisnotrepresentativeofWordNetwithrespecttoORPs,sincethiswouldhavebroughtaboutacorpuslargelycomposedofneutralsynsets,whichwouldbeprettyuselessasabenchmarkfortestingautomaticallyde-rivedlexicalresourcesforopinionmining.Itwasthusgeneratedbyrandomlyselecting100positive+100negative+100neutraltermsfromtheGeneralInquirerlexicon(see(TurneyandLittman,2003)fordetails)andincludingallthesynsetsthatcontained3http://www.unipv.it/wnop/atleastonesuchterm,withoutpayingattentiontoPOS.See(Cerinietal.,2007)formoredetails.Thecorpusisdividedintothreeparts:•Common:110synsetswhichalltheevaluatorsevaluatedbyworkingtogether,soastoaligntheirevaluationcriteria.•Group1:496synsetswhichwereeachinde-pendentlyevaluatedbythreeevaluators.•Group2:499synsetswhichwereeachinde-pendentlyevaluatedbytheothertwoevalua-tors.Eachofthesethreepartshasthesamebalance,intermsofbothpartsofspeechandORPs,ofMicro-WNOpasawhole.Weobtainthepositivity(nega-tivity)rankingfromMicro-WNOpbyaveragingthepositivity(negativity)scoresassignedbytheevalua-torsofeachgroupintoasinglescore,andbysortingthesynsetsaccordingtotheresultingscore.WeuseGroup1asavalidationset,i.e.,inordertoﬁne-tuneourmethod,andGroup2asatestset,i.e.,inordertoevaluateourmethodoncealltheparametershavebeenoptimizedonthevalidationset.TheresultofapplyingPageRanktothegraphGinducedbytheIrelation,givenavectoreofin-ternalsourcesofpositivity(negativity)scoreandavaluefortheαparameter,isarankingofalltheWordNetsynsetsintermsofpositivity(negativity).Byusingdifferentevectorsanddifferentvaluesofαweobtaindifferentrankings,whosequalityweevaluatebycomparingthemagainsttherankingob-tainedfromMicro-WNOp.4.2TheeffectivenessmeasureAranking(cid:22)isapartialorderonasetofobjectsN={o1...o|N|}.Givenapair(oi,oj)ofobjects,oimayprecedeoj(oi(cid:22)oj),itmayfollowoi(oi(cid:23)oj),oritmaybetiedwithoj(oi≈oj).ToevaluatetherankingsproducedbyPageRankwehaveusedthep-normalizedKendallτdistance(notedτp–seee.g.,(Faginetal.,2004))betweentheMicro-WNOprankingsandthosepredictedbyPageRank.Astandardfunctionfortheevaluationofrankingswithties,τpisdeﬁnedasτp=nd+p·nuZ(3)429

wherendisthenumberofdiscordantpairs,i.e.,pairsofobjectsorderedonewayinthegoldstan-dardandtheotherwayintheprediction;nuisthenumberofpairsordered(i.e.,nottied)inthegoldstandardandtiedintheprediction,andpisapenal-izationtobeattributedtoeachsuchpair;andZisanormalizationfactor(equaltothenumberofpairsthatareorderedinthegoldstandard)whoseaimistomaketherangeofτpcoincidewiththe[0,1]in-terval.Notethatpairstiedinthegoldstandardarenotconsideredintheevaluation.Thepenalizationfactorissettop=12,whichisequaltotheprobabilitythatarankingalgorithmcorrectlyordersthepairbyrandomguessing;thereisthusnoadvantagetobegainedfromeitherran-domguessingorassigningtiesbetweenobjects.Forapredictionwhichperfectlycoincideswiththegoldstandardτpequals0;forapredictionwhichisex-actlytheinverseofthegoldstandardτpequals1.4.3SetupInordertoproducearankingbypositivity(nega-tivity)weneedtoprovideanevectorasinputtoPageRank.Wehaveexperimentedwithseveraldif-ferentdeﬁnitionsofe,eachforbothpositivityandnegativity.Forreasonsofspace,weonlyreportre-sultsfromtheﬁvemostsigniﬁcantones.Wehaveﬁrsttestedavector(hereafterdubbede1)withallvaluesuniformlysetto1|N|.Thisistheevectororiginallyusedin(BrinandPage,1998)forWebpageranking,andbringsaboutanunbiased(thatis,withrespecttoparticularproperties)rank-ingofWordNet.Ofcourse,itisnotmeanttobeusedforrankingbypositivityornegativity;wehaveuseditasabaselineinordertoevaluatetheimpactofproperty-biasedvectors.Theﬁrstsensible,albeitminimalistic,deﬁnitionofewehaveused(dubbede2)isthatofavec-torwithuniformnon-nulleiscoresassignedtothesynsetsthatcontaintheadjectivegood(bad),andnullscoresforallothersynsets.Afurther,stillfairlyminimalisticdeﬁnitionwehaveused(dubbede3)isthatofavectorwithuniformnon-nulleiscoresas-signedtothesynsetsthatcontainatleastoneoftheseven“paradigmatic”positive(negative)adjectivesusedasseedsin(TurneyandLittman,2003)4,and4Thesevenpositiveadjectivesaregood,nice,excellent,nullscoresforallothersynsets.Wehavealsotestedamorecomplexversionofe,witheiscoresobtainedfromrelease1.0ofSenti-WordNet(EsuliandSebastiani,2006b)5.ThislatterisalexicalresourceinwhicheachWordNetsynsetisgivenapositivityscore,anegativityscore,andaneutralityscore.Weproducedanevector(dubbede4)inwhichthescoreassignedtoasynsetispropor-tionaltothepositivity(negativity)scoreassignedtoitbySentiWordNet,andinwhichallentriessumupto1.Inasimilarwaywealsoproducedafurtherevector(dubbede5)throughthescoresofanewerre-leaseofSentiWordNet(release1.1),resultingfromaslightmodiﬁcationoftheapproachthathadbroughtaboutrelease1.0(EsuliandSebastiani,2007b).PageRankisparametriconα,whichdeterminesthebalancebetweenthecontributionsofthea(k−1)vectorandtheevector.Avalueofα=0makesthea(k)vectorcoincidewithe,andcorrespondstodiscardingthecontributionoftherandom-walkal-gorithm.Conversely,settingα=1correspondstodiscardingthecontributionofe,andmakesa(k)uniquelydependonthetopologyofthegraph;theresultisan“unbiased”ranking.Thedesirablecasesare,ofcourse,inbetween.AsﬁrsthintedinSec-tion4.1,wethusoptimizetheαparameteronthesynsetsinGroup1,andthentestthealgorithmwiththeoptimalvalueofαonthesynsetsinGroup2.Allthe101valuesofαfrom0.0to1.0withastepof.01havebeentestedintheoptimizationphase.Op-timizationisperformedanewforeachexperiment,whichmeansthatdifferentvaluesofαmaybeeven-tuallyselectedfordifferentevectors.5ResultsTheresultsshowthattheuseofPageRankincom-binationwithsuitablevectorsealmostalwaysim-provestheranking,sometimessigniﬁcantlyso,withrespecttotheoriginalrankingembodiedbytheevector.Forpositivity,therankingsproducedusingPageRankandanyofthevectorsfrome2toe5allimproveontheoriginalrankings,witharelativeim-provement,measuredastherelativedecreaseinτp,positive,fortunate,correct,superior,andthesevennegativeonesarebad,nasty,poor,negative,unfortunate,wrong,in-ferior.5http://sentiwordnet.isti.cnr.it/430

rangingfrom−4.88%(e5)to−6.75%(e4).Theserankingsarealsoallbetterthantherankingspro-ducedbyusingPageRankandtheuniform-valuedvectore1,withaminimumrelativeimprovementof−5.04%(e3)andamaximumof−34.47%(e4).Thissuggeststhatthekeytogoodperformanceisindeedacombinationofpositivityﬂowandinternalsourceofscore.Forthenegativityrankings,theperformanceofbothSentiWordNet-basedvectorsisstillgood,pro-ducinga−4.31%(e4)anda−3.45%(e5)improve-mentwithrespecttotheoriginalrankings.The“minimalistic”vectors(i.e.,e2ande3)arenotasgoodastheirpositivecounterparts.Thereasonseemstobethatthegenerationofarankingbyneg-ativityseemsasomehowhardertaskthanthegen-erationofarankingbypositivity;thisisalsoshownbytheresultsobtainedwiththeuniform-valuedvec-tore1,inwhichtheapplicationofPageRankim-proveswithrespecttoe1forpositivitybutdeteri-oratesfornegativity.However,againstthebaselineconstitutedbytheresultsobtainedwiththeuniform-valuedvectore1fornegativity,ourrankingsshowarelevantimprovement,rangingfrom−8.56%(e2)to−48.27%(e4).Ourresultsareparticularlysigniﬁcantforthee4vectors,derivedbySentiWordNet1.0,foranum-berofreasons.First,e4bringsaboutthebestvalueofτpobtainedinallourexperiments(.325forpos-itivity,.284fornegativity).Second,therelativeim-provementwithrespecttoe4isthemostmarkedamongthevariouschoicesfore(6.75%forpositiv-ity,4.31%fornegativity).Third,theimprovementisobtainedwithrespecttoanalreadyhigh-qualityresource,obtainedbythesametechniquesthat,atthetermlevel,arestillthebestperformersforpo-laritydetectiononthewidelyusedGeneralInquirerbenchmark(EsuliandSebastiani,2005).Finally,observethatthefactthate4outperformsallotherchoicesfore(ande2inparticular)wasnotnecessarilytobeexpected.Infact,SentiWordNet1.0wasbuiltbyasemi-supervisedlearningmethodthatusesvectorse2asitsonlyinitialtrainingdata.Thispaperthusshowsthat,startingfrome2astheonlymanuallyannotateddata,thebestresultsareobtainedneitherbythesemi-supervisedmethodthatgeneratedSentiWordNet1.0,norbyPageRank,butbytheconcatenationoftheformerwiththelatter.PositivityNegativityePageRank?τp∆τp∆e1before.500.500after.496(-0.81%).549(9.83%)e2before.500.500after.467(-6.65%).502(0.31%)e3before.500.500after.471(-5.79%).495(-0.92%)e4before.349.296after.325(-6.75%).284(-4.31%)e5before.400.407after.380(-4.88%).393(-3.45%)Table1:Valuesofτpbetweenpredictedrankingsandgoldstandardrankings(smallerisbetter).Foreachexperimenttheﬁrstlineindicatestherankingobtainedfromtheoriginalevector(beforetheap-plicationofPageRank),whilethesecondlineindi-catestherankingobtainedaftertheapplicationofPageRank,withtherelativeimprovement(anega-tivepercentageindicatesimprovement).6ConclusionsWehaveinvestigatedtheapplicabilityofarandom-walkmodeltotheproblemofrankingsynsetsac-cordingtopositivityandnegativity.However,weconjecturethatthismodelcanbeofmoregeneraluse,i.e.,forthedeterminationofotherpropertiesoftermsenses,suchasmembershipinadomain.Thispaperthuspresentsaproof-of-conceptofthemodel,andtheresultsofexperimentssupportourintuitions.Also,weseethisworkasaproofofconceptfortheapplicabilityofgeneralrandom-walkalgo-rithms(andnotjustPageRank)tothedeterminationofthesemanticpropertiesofsynsets.Inamorere-centpaper(EsuliandSebastiani,2007a)wehaveinvestigatedarelatedrandom-walkmodel,oneinwhich,symmetricallytotheintuitionsofthemodelpresentedinthispaper,semanticsﬂowsfromthedeﬁnienstothedeﬁniendum;ametaphorthatprovesnolesspowerfulthantheonewehavechampionedinthispaper.ReferencesAlinaAndreevskaiaandSabineBergler.2006a.MiningWord-Netforfuzzysentiment:SentimenttagextractionfromWordNetglosses.InProceedingsofthe11thConferenceoftheEuropeanChapteroftheAssociationforComputationalLinguistics(EACL’06),pages209–216,Trento,IT.AlinaAndreevskaiaandSabineBergler.2006b.SentimenttagextractionfromWordNetglosses.InProceedingsof431

the5thConferenceonLanguageResourcesandEvaluation(LREC’06),Genova,IT.MonicaBianchini,MarcoGori,andFrancoScarselli.2005.In-sidePageRank.ACMTransactionsonInternetTechnology,5(1):92–128.SergeyBrinandLawrencePage.1998.Theanatomyofalarge-scalehypertextualWebsearchengine.ComputerNetworksandISDNSystems,30(1-7):107–117.SabrinaCerini,ValentinaCompagnoni,AliceDemontis,MaicolFormentelli,andCaterinaGandini.2007.Micro-WNOp:Agoldstandardfortheevaluationofautomati-callycompiledlexicalresourcesforopinionmining.InAn-dreaSans`o,editor,Languageresourcesandlinguisticthe-ory:Typology,secondlanguageacquisition,Englishlinguis-tics.FrancoAngeliEditore,Milano,IT.Forthcoming.AndreaEsuliandFabrizioSebastiani.2005.Determiningthesemanticorientationoftermsthroughglossanalysis.InPro-ceedingsofthe14thACMInternationalConferenceonIn-formationandKnowledgeManagement(CIKM’05),pages617–624,Bremen,DE.AndreaEsuliandFabrizioSebastiani.2006a.Determiningtermsubjectivityandtermorientationforopinionmining.InProceedingsofthe11thConferenceoftheEuropeanChapteroftheAssociationforComputationalLinguistics(EACL’06),pages193–200,Trento,IT.AndreaEsuliandFabrizioSebastiani.2006b.SENTIWORD-NET:Apubliclyavailablelexicalresourceforopinionmin-ing.InProceedingsofthe5thConferenceonLanguageRe-sourcesandEvaluation(LREC’06),pages417–422,Gen-ova,IT.AndreaEsuliandFabrizioSebastiani.2007a.Random-walkmodelsoftermsemantics:Anapplicationtoopinion-relatedproperties.TechnicalReportISTI-009/2007,Isti-tutodiScienzaeTecnologiedell’Informazione,ConsiglioNazionaledellleRicerche,Pisa,IT.AndreaEsuliandFabrizioSebastiani.2007b.SENTIWORD-NET:Ahigh-coveragelexicalresourceforopinionmining.TechnicalReport2007-TR-02,IstitutodiScienzaeTecnolo-giedell’Informazione,ConsiglioNazionaledelleRicerche,Pisa,IT.RonaldFagin,RaviKumar,MohammadMahdiany,D.Sivaku-mar,andErikVeez.2004.Comparingandaggregatingrank-ingswithties.InProceedingsofACMInternationalConfer-enceonPrinciplesofDatabaseSystems(PODS’04),pages47–58,Paris,FR.GregoryGrefenstette,YanQu,DavidA.Evans,andJamesG.Shanahan.2006.Validatingthecoverageoflexicalre-sourcesforaffectanalysisandautomaticallyclassifyingnewwordsalongsemanticaxes.InJamesG.Shanahan,YanQu,andJanyceWiebe,editors,ComputingAttitudeandAffectinText:TheoriesandApplications,pages93–107.Springer,Heidelberg,DE.SandaH.Harabagiu,GeorgeA.Miller,andDanI.Moldovan.1999.WordNet2:Amorphologicallyandsemanticallyen-hancedresource.InProceedingsoftheACLSIGLEXWork-shoponStandardizingLexicalResources,pages1–8,Col-legePark,US.VasileiosHatzivassiloglouandKathleenR.McKeown.1997.Predictingthesemanticorientationofadjectives.InPro-ceedingsofthe35thAnnualMeetingoftheAssociationforComputationalLinguistics(ACL’97),pages174–181,Madrid,ES.TaherH.Haveliwala.2003.Topic-sensitivePageRank:Acontext-sensitiverankingalgorithmforWebsearch.IEEETransactionsonKnowledgeandDataEngineering,15(4):784–796.NancyIde.2006.Makingsenses:Bootstrappingsense-taggedlistsofsemantically-relatedwords.InProceedingsofthe7thInternationalConferenceonComputationalLinguisticsandIntelligentTextProcessing(CICLING’06),pages13–27,MexicoCity,MX.JaapKamps,MaartenMarx,RobertJ.Mokken,andMaartenDeRijke.2004.UsingWordNettomeasuresemanticori-entationofadjectives.InProceedingsofthe4thInterna-tionalConferenceonLanguageResourcesandEvaluation(LREC’04),volumeIV,pages1115–1118,Lisbon,PT.Soo-MinKimandEduardHovy.2004.Determiningthesentimentofopinions.InProceedingsofthe20thInter-nationalConferenceonComputationalLinguistics(COL-ING’04),pages1367–1373,Geneva,CH.RadaMihalcea.2006.Randomwalksontextstructures.InProceedingsofthe7thInternationalConferenceonCom-putationalLinguisticsandIntelligentTextProcessing(CI-CLING’06),pages249–262,MexicoCity,MX.PeroSubasicandAlisonHuettner.2001.Affectanalysisoftextusingfuzzysemantictyping.IEEETransactionsonFuzzySystems,9(4):483–496.HiroyaTakamura,TakashiInui,andManabuOkumura.2005.Extractingemotionalpolarityofwordsusingspinmodel.InProceedingsofthe43rdAnnualMeetingoftheAssoci-ationforComputationalLinguistics(ACL’05),pages133–140,AnnArbor,US.PeterD.TurneyandMichaelL.Littman.2003.Measur-ingpraiseandcriticism:Inferenceofsemanticorientationfromassociation.ACMTransactionsonInformationSys-tems,21(4):315–346.JanyceWiebeandRadaMihalcea.2006.Wordsenseandsub-jectivity.InProceedingsofthe44thAnnualMeetingoftheAssociationforComputationalLinguistics(ACL’06),pages1065–1072,Sydney,AU.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 432–439,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

432

StructuredModelsforFine-to-CoarseSentimentAnalysisRyanMcDonald∗KerryHannanTylerNeylonMikeWellsJeffReynarGoogle,Inc.76NinthAvenueNewYork,NY10011∗Contactemail:ryanmcd@google.comAbstractInthispaperweinvestigateastructuredmodelforjointlyclassifyingthesentimentoftextatvaryinglevelsofgranularity.Infer-enceinthemodelisbasedonstandardse-quenceclassiﬁcationtechniquesusingcon-strainedViterbitoensureconsistentsolu-tions.Theprimaryadvantageofsuchamodelisthatitallowsclassiﬁcationdeci-sionsfromonelevelinthetexttoinﬂuencedecisionsatanother.Experimentsshowthatthismethodcansigniﬁcantlyreduceclassiﬁ-cationerrorrelativetomodelstrainediniso-lation.1IntroductionExtractingsentimentfromtextisachallengingprob-lemwithapplicationsthroughoutNaturalLanguageProcessingandInformationRetrieval.Previousworkonsentimentanalysishascoveredawiderangeoftasks,includingpolarityclassiﬁcation(Pangetal.,2002;Turney,2002),opinionextraction(PangandLee,2004),andopinionsourceassignment(Choietal.,2005;Choietal.,2006).Furthermore,thesesystemshavetackledtheproblematdiffer-entlevelsofgranularity,fromthedocumentlevel(Pangetal.,2002),sentencelevel(PangandLee,2004;MaoandLebanon,2006),phraselevel(Tur-ney,2002;Choietal.,2005),aswellasthespeakerlevelindebates(Thomasetal.,2006).Theabil-itytoclassifysentimentonmultiplelevelsisimpor-tantsincedifferentapplicationshavedifferentneeds.Forexample,asummarizationsystemforproductreviewsmightrequirepolarityclassiﬁcationatthesentenceorphraselevel;aquestionansweringsys-temwouldmostlikelyrequirethesentimentofpara-graphs;andasystemthatdetermineswhicharticlesfromanonlinenewssourceareeditorialinnaturewouldrequireadocumentlevelanalysis.Thisworkfocusesonmodelsthatjointlyclassifysentimentonmultiplelevelsofgranularity.Considerthefollowingexample,ThisistheﬁrstMp3playerthatIhaveused...Ithoughtitsoundedgreat...Afteronlyafewweeks,itstartedhavingtroublewiththeearphoneconnec-tion...Iwon’tbebuyinganother.Mp3playerreviewfromAmazon.comThisexcerptexpressesanoverallnegativeopinionoftheproductbeingreviewed.However,notallpartsofthereviewarenegative.Theﬁrstsentencemerelyprovidessomecontextonthereviewer’sexperiencewithsuchdevicesandthesecondsentenceindicatesthat,atleastinoneregard,theproductperformedwell.Wecalltheproblemofidentifyingthesenti-mentofthedocumentandofallitssubcomponents,whetherattheparagraph,sentence,phraseorwordlevel,ﬁne-to-coarsesentimentanalysis.Thesimplestapproachtoﬁne-to-coarsesentimentanalysiswouldbetocreateaseparatesystemforeachlevelofgranularity.Thereare,however,obvi-ousadvantagestobuildingasinglemodelthatclas-siﬁeseachlevelintandem.Considerthesentence,My11yearolddaughterhasalsobeenusingitanditisalotharderthanitlooks.Inisolation,thissentenceappearstoconveynegativesentiment.However,itispartofafavorablereview433

forapieceofﬁtnessequipment,wherehardessen-tiallymeansgoodworkout.Inthisdomain,hard’ssentimentcanonlybedeterminedincontext(i.e.,hardtoassembleversusahardworkout).Iftheclas-siﬁerknewtheoverallsentimentofadocument,thendisambiguatingsuchcaseswouldbeeasier.Conversely,documentlevelanalysiscanbeneﬁtfromﬁnerlevelclassiﬁcationbytakingadvantageofcommondiscoursecues,suchasthelastsentencebeingareliableindicatorforoverallsentimentinre-views.Furthermore,duringtraining,themodelwillnotneedtomodifyitsparameterstoexplainphe-nomenalikethetypicallypositivewordgreatap-pearinginanegativetext(asisthecaseabove).Themodelcanalsoavoidoverﬁttingtofeaturesderivedfromneutralorobjectivesentences.Infact,ithasal-readybeenestablishedthatsentencelevelclassiﬁca-tioncanimprovedocumentlevelanalysis(PangandLee,2004).Thislineofreasoningsuggeststhatacascadedapproachwouldalsobeinsufﬁcient.Valu-ableinformationispassedinbothdirections,whichmeansanymodelofﬁne-to-coarseanalysisshouldaccountforthis.InSection2wedescribeasimplestructuredmodelthatjointlylearnsandinferssentimentondif-ferentlevelsofgranularity.Inparticular,wereducetheproblemofjointsentenceanddocumentlevelanalysistoasequentialclassiﬁcationproblemus-ingconstrainedViterbiinference.Extensionstothemodelthatmovebeyondjusttwo-levelsofanalysisarealsopresented.InSection3anempiricaleval-uationofthemodelisgiventhatshowssigniﬁcantgainsinaccuracyoverbothsinglelevelclassiﬁersandcascadedsystems.1.1RelatedWorkThemodelsinthisworkfallintothebroadclassofglobalstructuredmodels,whicharetypicallytrainedwithstructuredlearningalgorithms.HiddenMarkovmodels(Rabiner,1989)areoneoftheearlieststruc-turedlearningalgorithms,whichhaverecentlybeenfollowedbydiscriminativelearningapproachessuchasconditionalrandomﬁelds(CRFs)(Laffertyetal.,2001;SuttonandMcCallum,2006),thestructuredperceptron(Collins,2002)anditslarge-marginvari-ants(Taskaretal.,2003;Tsochantaridisetal.,2004;McDonaldetal.,2005;Daum´eIIIetal.,2006).Thesealgorithmsareusuallyappliedtosequentiallabelingorchunking,buthavealsobeenappliedtoparsing(Taskaretal.,2004;McDonaldetal.,2005),machinetranslation(Liangetal.,2006)andsumma-rization(Daum´eIIIetal.,2006).Structuredmodelshavepreviouslybeenusedforsentimentanalysis.Choietal.(2005,2006)useCRFstolearnaglobalsequencemodeltoclassifyandassignsourcestoopinions.MaoandLebanon(2006)usedasequentialCRFregressionmodeltomeasurepolarityonthesentencelevelinordertodeterminethesentimentﬂowofauthorsinreviews.Hereweshowthatﬁne-to-coarsemodelsofsenti-mentcanoftenbereducedtothesequentialcase.Cascadedmodelsforﬁne-to-coarsesentimentanalysiswerestudiedbyPangandLee(2004).Inthatworkaninitialmodelclassiﬁedeachsentenceasbeingsubjectiveorobjectiveusingaglobalmin-cutinferencealgorithmthatconsideredlocallabel-ingconsistencies.Thetopsubjectivesentencesaretheninputintoastandarddocumentlevelpolarityclassiﬁerwithimprovedresults.ThecurrentworkdiffersfromthatinPangandLeethroughtheuseofasinglejointstructuredmodelforbothsentenceanddocumentlevelanalysis.Manyproblemsinnaturallanguageprocessingcanbeimprovedbylearningand/orpredictingmul-tipleoutputsjointly.Thisincludesparsingandrela-tionextraction(Milleretal.,2000),entitylabelingandrelationextraction(RothandYih,2004),andpart-of-speechtaggingandchunking(Suttonetal.,2004).OneinterestingworkonsentimentanalysisisthatofPopescuandEtzioni(2005)whichattemptstoclassifythesentimentofphraseswithrespecttopossibleproductfeatures.Todothisaniterativeal-gorithmisusedthatattemptstogloballymaximizetheclassiﬁcationofallphraseswhilesatisfyinglocalconsistencyconstraints.2StructuredModelInthissectionwepresentastructuredmodelforﬁne-to-coarsesentimentanalysis.Westartbyexam-iningthesimplecasewithtwo-levelsofgranularity–thesentenceanddocument–andshowthattheproblemcanbereducedtosequentialclassiﬁcationwithconstrainedinference.Wethendiscussthefea-turespaceandgiveanalgorithmforlearningthepa-rametersbasedonlarge-marginstructuredlearning.434

Extensionstothemodelarealsoexamined.2.1ASentence-DocumentModelLetY(d)beadiscretesetofsentimentlabelsatthedocumentlevelandY(s)beadiscretesetofsentimentlabelsatthesentencelevel.Asinputasystemisgivenadocumentcontainingsentencess=s1,...,snandmustproducesentimentlabelsforthedocument,yd∈Y(d),andeachindivid-ualsentence,ys=ys1,...,ysn,whereysi∈Y(s)∀1≤i≤n.Deﬁney=(yd,ys)=(yd,ys1,...,ysn)asthejointlabelingofthedocumentandsentences.Forinstance,inPangandLee(2004),ydwouldbethepolarityofthedocumentandysiwouldindicatewhethersentencesiissubjectiveorobjective.Themodelspresentedherearecompatiblewitharbitrarysetsofdiscreteoutputlabels.Figure1presentsamodelforjointlyclassifyingthesentimentofboththesentencesandthedocu-ment.Inthisundirectedgraphicalmodel,thelabelofeachsentenceisdependentonthelabelsofitsneighbouringsentencesplusthelabelofthedocu-ment.Thelabelofthedocumentisdependentonthelabelofeverysentence.Notethattheedgesbetweentheinput(eachsentence)andtheoutputlabelsarenotsolid,indicatingthattheyaregivenasinputandarenotbeingmodeled.Thefactthatthesentimentofsentencesisdependentnotonlyonthelocalsentimentofothersentences,butalsotheglobaldocumentsentiment–andviceversa–al-lowsthemodeltodirectlycapturetheimportanceofclassiﬁcationdecisionsacrosslevelsinﬁne-to-coarsesentimentanalysis.ThelocaldependenciesbetweensentimentlabelsonsentencesissimilartotheworkofPangandLee(2004)wheresoftlocalconsistencyconstraintswerecreatedbetweeneverysentenceinadocumentandinferencewassolvedus-ingamin-cutalgorithm.However,jointlymodelingthedocumentlabelandallowingfornon-binaryla-belscomplicatesmin-cutstylesolutionsasinferencebecomesintractable.Learningandinferenceinundirectedgraphicalmodelsisawellstudiedprobleminmachinelearn-ingandNLP.Forexample,CRFsdeﬁnetheprob-abilityoverthelabelsconditionedontheinputus-ingthepropertythatthejointprobabilitydistribu-tionoverthelabelsfactorsovercliquepotentialsinundirectedgraphicalmodels(Laffertyetal.,2001).Figure1:Sentenceanddocumentlevelmodel.Inthisworkwewillusestructuredlinearclassi-ﬁers(Collins,2002).Wedenotethescoreofala-belingyforaninputsasscore(y,s)anddeﬁnethisscoreasthesumofscoresovereachclique,score(y,s)=score((yd,ys),s)=score((yd,ys1,...,ysn),s)=nXi=2score(yd,ysi−1,ysi,s)whereeachcliquescoreisalinearcombinationoffeaturesandtheirweights,score(yd,ysi−1,ysi,s)=w·f(yd,ysi−1,ysi,s)(1)andfisahighdimensionalfeaturerepresentationofthecliqueandwacorrespondingweightvector.Notethatsisincludedineachscoresinceitisgivenasinputandcanalwaysbeconditionedon.Ingeneral,inferenceinundirectedgraphicalmod-elsisintractable.However,forthecommoncaseofsequences(a.k.a.linear-chainmodels)theViterbial-gorithmcanbeused(Rabiner,1989;Laffertyetal.,2001).FortunatelythereisasimpletechniquethatreducesinferenceintheabovemodeltosequenceclassiﬁcationwithaconstrainedversionofViterbi.2.1.1InferenceasSequentialLabelingTheinferenceproblemistoﬁndthehighestscor-inglabelingyforaninputs,i.e.,argmaxyscore(y,s)Ifthedocumentlabelydisﬁxed,theninferenceinthemodelfromFigure1reducestothesequen-tialcase.Thisisbecausethesearchspaceisonlyoverthesentencelabelsysi,whosegraphicalstruc-tureformsachain.Thustheproblemofﬁndingthe435

Input:s=s1,...,sn1.y=null2.foreachyd∈Y(d)3.ys=argmaxysscore((yd,ys),s)4.y0=(yd,ys)5.ifscore(y0,s)>score(y,s)ory=null6.y=y07.returnyFigure2:InferencealgorithmformodelinFigure1.Theargmaxinline3canbesolvedusingViterbi’salgorithmsinceydisﬁxed.highestscoringsentimentlabelsforallsentences,givenaparticulardocumentlabelyd,canbesolvedefﬁcientlyusingViterbi’salgorithm.Thegeneralinferenceproblemcanthenbesolvedbyiteratingovereachpossibleyd,ﬁndingysmax-imizingscore((yd,ys),s)andkeepingthesinglebesty=(yd,ys).ThisalgorithmisoutlinedinFig-ure2andhasaruntimeofO(|Y(d)||Y(s)|2n),duetorunningViterbi|Y(d)|timesoveralabelspaceofsize|Y(s)|.Thealgorithmcanbeextendedtopro-duceexactk-bestlists.Thisisachievedbyusingk-bestViterbitechniquestoreturnthek-bestgloballabelingsforeachdocumentlabelinline3.Mergingthesesetswillproducetheﬁnalk-bestlist.ItispossibletoviewtheinferencealgorithminFigure2asaconstrainedViterbisearchsinceitisequivalenttoﬂatteningthemodelinFigure1toasequentialmodelwithsentencelabelsfromthesetY(s)×Y(d).TheresultingViterbisearchwouldthenneedtobeconstrainedtoensureconsistentsolutions,i.e.,thelabelassignmentsagreeonthedocumentlabeloverallsentences.Ifviewedthisway,itisalsopossibletorunaconstrainedforward-backwardalgorithmandlearntheparametersforCRFsaswell.2.1.2FeatureSpaceInthissectionwedeﬁnethefeaturerepresenta-tionforeachclique,f(yd,ysi−1,ysi,s).AssumethateachsentencesiisrepresentedbyasetofbinarypredicatesP(si).Thissetcancontainanypredicateovertheinputs,butforthepresentpurposesitwillincludealltheunigram,bigramandtrigramsinthesentencesiconjoinedwiththeirpart-of-speech(obtainedfromanautomaticclassiﬁer).Back-offsofeachpredicatearealsoincludedwhereoneormorewordisdiscarded.Forinstance,ifP(si)con-tainsthepredicatea:DTgreat:JJproduct:NN,thenitwouldalsohavethepredicatesa:DTgreat:JJ*:NN,a:DT*:JJproduct:NN,*:DTgreat:JJproduct:NN,a:DT*:JJ*:NN,etc.Eachpredicate,p,isthenconjoinedwiththelabelinformationtoconstructabinaryfeature.Forexam-ple,ifthesentencelabelsetisY(s)={subj,obj}andthedocumentsetisY(d)={pos,neg},thenthesystemmightcontainthefollowingfeature,f(j)(yd,ysi−1,ysi,s)=1ifp∈P(si)andysi−1=objandysi=subjandyd=neg0otherwiseWheref(j)isthejthdimensionofthefeaturespace.Foreachfeature,asetofback-offfeaturesarein-cludedthatonlyconsiderthedocumentlabelyd,thecurrentsentencelabelysi,thecurrentsentenceanddocumentlabelysiandyd,andthecurrentandpre-vioussentencelabelsysiandysi−1.Notethatthroughtheseback-offfeaturesthejointmodelsfeaturesetwillsubsumethefeaturesetofanyindividuallevelmodel.Onlyfeaturesobservedinthetrainingdatawereconsidered.Dependingonthedataset,thedi-mensionofthefeaturevectorfrangedfrom350Kto500K.Thoughthefeaturevectorscanbesparse,thefeatureweightswillbelearnedusinglarge-margintechniquesthatarewellknowntoberobusttolargeandsparsefeaturerepresentations.2.1.3TrainingtheModelLetY=Y(d)×Y(s)nbethesetofallvalidsentence-documentlabelingsforaninputs.Theweights,w,aresetusingtheMIRAlearningal-gorithm,whichisaninferencebasedonlinelarge-marginlearningtechnique(CrammerandSinger,2003;McDonaldetal.,2005).Anadvantageofthisalgorithmisthatitreliesonlyoninferencetolearntheweightvector(seeSection2.1.1).MIRAhasbeenshowntoprovidestate-of-the-artaccuracyformanylanguageprocessingtasksincludingparsing,chunkingandentityextraction(McDonald,2006).ThebasicalgorithmisoutlinedinFigure3.Thealgorithmworksbyconsideringasingletrainingin-stanceduringeachiteration.Theweightvectorwisupdatedinline4throughaquadraticprogrammingproblem.Thisupdatemodiﬁestheweightvectorso436

Trainingdata:T={(yt,st)}Tt=11.w(0)=0;i=02.forn:1..N3.fort:1..T4.w(i+1)=argminw*‚‚‚w*−w(i)‚‚‚s.t.score(yt,st)−score(y0,s)≥L(yt,y0)relativetow*∀y0∈C⊂Y,where|C|=k5.i=i+16.returnw(N×T)Figure3:MIRAlearningalgorithm.thatthescoreofthecorrectlabelingislargerthanthescoreofeverylabelinginaconstraintsetCwithamarginproportionaltotheloss.TheconstraintsetCcanbechosenarbitrarily,butitisusuallytakentobetheklabelingsthathavethehighestscoreundertheoldweightvectorw(i)(McDonaldetal.,2005).Inthismanner,thelearningalgorithmcanupdateitsparametersrelativetothoselabelingsclosesttothedecisionboundary.Ofalltheweightvectorsthatsat-isfytheseconstraints,MIRAchoosestheonethatisascloseaspossibletothepreviousweightvectorinordertoretaininformationaboutpreviousupdates.ThelossfunctionL(y,y0)isapositiverealval-uedfunctionandisequaltozerowheny=y0.Thisfunctionistaskspeciﬁcandisusuallythehamminglossforsequenceclassiﬁcationproblems(Taskaretal.,2003).Experimentswithdifferentlossfunctionsforthejointsentence-documentmodelonadevelop-mentdatasetindicatedthatthehamminglossoversentencelabelsmultipliedbythe0-1lossoverdoc-umentlabelsworkedbest.Animportantmodiﬁcationthatwasmadetothelearningalgorithmdealswithhowthekconstraintsarechosenfortheoptimization.Typicallythesecon-straintsarethekhighestscoringlabelingsunderthecurrentweightvector.However,earlyexperimentsshowedthatthemodelquicklylearnedtodiscardanylabelingwithanincorrectdocumentlabelfortheinstancesinthetrainingset.Asaresult,thecon-straintsweredominatedbylabelingsthatonlydif-feredoversentencelabels.Thisdidnotallowtheal-gorithmadequateopportunitytosetparametersrel-ativetoincorrectdocumentlabelingdecisions.Tocombatthis,kwasdividedbythenumberofdoc-umentlabels,togetanewvaluek0.Foreachdoc-umentlabel,thek0highestscoringlabelingswereFigure4:AnextensiontothemodelfromFigure1incorporatingparagraphlevelanalysis.extracted.Eachofthesesetswerethencombinedtoproducetheﬁnalconstraintset.Thisallowedcon-straintstobeequallydistributedamongstdifferentdocumentlabels.BasedonperformanceonthedevelopmentdatasetthenumberoftrainingiterationswassettoN=5andthenumberofconstraintstok=10.Weightaveragingwasalsoemployed(Collins,2002),whichhelpedimproveperformance.2.2BeyondTwo-LevelModelsTothispoint,wehavefocusedsolelyonamodelfortwo-levelﬁne-to-coarsesentimentanalysisnotonlyforsimplicity,butbecausetheexperimentsinSec-tion3dealexclusivelywiththisscenario.Inthissection,webrieﬂydiscusspossibleextensionsformorecomplexsituations.Forexample,longerdoc-umentsmightbeneﬁtfromananalysisonthepara-graphlevelaswellasthesentenceanddocumentlevels.OnepossiblemodelforthiscaseisgiveninFigure4,whichessentiallyinsertsanadditionallayerbetweenthesentenceanddocumentlevelfromtheoriginalmodel.Sentencelevelanalysisisde-pendentonneighbouringsentencesaswellastheparagraphlevelanalysis,andtheparagraphanal-ysisisdependentoneachofthesentenceswithinit,theneighbouringparagraphs,andthedocumentlevelanalysis.Thiscanbeextendedtoanarbitrarylevelofﬁne-to-coarsesentimentanalysisbysimplyinsertingnewlayersinthisfashiontocreatemorecomplexhierarchicalmodels.Theadvantageofusinghierarchicalmodelsofthisformisthattheyarenested,whichkeepsin-ferencetractable.Observethateachpairofadja-centlevelsinthemodelisequivalenttotheorigi-nalmodelfromFigure1.Asaresult,thescoresoftheeverylabelateachnodeinthegraphcanbecalculatedwithastraight-forwardbottom-updy-namicprogrammingalgorithm.Detailsareomitted437

SentenceStatsDocumentStatsPosNegNeuTotPosNegTotCar47244326411799880178Fit56863537115749297189Mp348546421411639889187Tot152515428493916288266554Table1:Datastatisticsforcorpus.Pos=positivepolarity,Neg=negativepolarity,Neu=nopolarity.forspacereasons.Othermodelsarepossiblewheredependenciesoccuracrossnon-neighbouringlevels,e.g.,byin-sertingedgesbetweenthesentencelevelnodesandthedocumentlevelnode.Inthegeneralcase,infer-enceisexponentialinthesizeofeachclique.BoththemodelsinFigure1andFigure4havemaximumcliquesizesofthree.3Experiments3.1DataTotestthemodelwecompiledacorpusof600on-lineproductreviewsfromthreedomains:carseatsforchildren,ﬁtnessequipment,andMp3players.Oftheoriginal600reviewsthatweregathered,wedis-cardedduplicatereviews,reviewswithinsufﬁcienttext,andspam.Allreviewswerelabeledbyon-linecustomersashavingapositiveornegativepolar-ityonthedocumentlevel,i.e.,Y(d)={pos,neg}.Eachreviewwasthensplitintosentencesandev-erysentenceannotatedbyasingleannotatorasei-therbeingpositive,negativeorneutral,i.e.,Y(s)={pos,neg,neu}.DatastatisticsforthecorpusaregiveninTable1.Allsentenceswereannotatedbasedontheircon-textwithinthedocument.Sentenceswereanno-tatedasneutraliftheyconveyednosentimentorhadindeterminatesentimentfromtheircontext.Manyneutralsentencespertaintothecircumstancesun-derwhichtheproductwaspurchased.Acommonclassofsentenceswerethosecontainingproductfeatures.Thesesentenceswereannotatedashavingpositiveornegativepolarityifthecontextsupportedit.Thiscouldincludepunctuationsuchasexcla-mationpoints,smiley/frownyfaces,questionmarks,etc.Thesupportingevidencecouldalsocomefromanothersentence,e.g.,“Iloveit.Ithas64Mbofmemoryandcomeswithasetofearphones”.3.2ResultsThreebaselinesystemswerecreated,•Document-Classiﬁerisaclassiﬁerthatlearnstopredictthedocumentlabelonly.•Sentence-Classiﬁerisaclassiﬁerthatlearnstopredictsentencelabelsinisolationofoneanother,i.e.,withoutconsiderationforeitherthedocumentorneighbouringsentencessen-timent.•Sentence-Structuredisanothersentenceclas-siﬁer,butthisclassiﬁerusesasequentialchainmodeltolearnandclassifysentences.ThethirdbaselineisessentiallythemodelfromFig-ure1withoutthetopleveldocumentnode.Thisbaselinewillhelptogagetheempiricalgainsofthedifferentcomponentsofthejointstructuredmodelonsentencelevelclassiﬁcation.ThemodeldescribedinSection2willbecalledJoint-Structured.Allmodelsusethesameba-sicpredicatespace:unigram,bigram,trigramcon-joinedwithpart-of-speech,plusback-offsofthese(seeSection2.1.2formore).However,duetothestructureofthemodelanditslabelspace,thefeaturespaceofeachmightbedifferent,e.g.,thedocumentclassiﬁerwillonlyconjoinpredicateswiththedoc-umentlabeltocreatethefeatureset.AllmodelsaretrainedusingtheMIRAlearningalgorithm.ResultsforeachmodelaregivenintheﬁrstfourrowsofTable2.Theseresultsweregatheredusing10-foldcrossvalidationwithonefoldfordevelop-mentandtheotherninefoldsforevaluation.Thistableshowsthatclassifyingsentencesinisolationfromoneanotherisinferiortoaccountingforamoreglobalcontext.Asigniﬁcantincreaseinperfor-mancecanbeobtainedwhenlabelingdecisionsbe-tweensentencesaremodeled(Sentence-Structured).Moreinterestingly,evenfurthergainscanbehadwhendocumentleveldecisionsaremodeled(Joint-Structured).Inmanycases,theseimprovementsarehighlystatisticallysigniﬁcant.Onthedocumentlevel,performancecanalsobeimprovedbyincorporatingsentenceleveldecisions–thoughtheseimprovementsarenotconsistent.Thisinconsistencymaybearesultofthemodeloverﬁttingonthesmallsetoftrainingdata.We438

suspectthisbecausethedocumentlevelerrorrateontheMp3trainingsetconvergestozeromuchmorerapidlyfortheJoint-StructuredmodelthantheDocument-Classiﬁer.ThissuggeststhattheJoint-Structuredmodelmightberelyingtoomuchonthesentencelevelsentimentfeatures–inordertominimizeitserrorrate–insteadofdistributingtheweightsacrossallfeaturesmoreevenly.Oneinterestingapplicationofsentencelevelsen-timentanalysisissummarizingproductreviewsonretailwebsiteslikeAmazon.comorreviewaggrega-torslikeYelp.com.Inthissettingthecorrectpolar-ityofadocumentisoftenknown,butwewishtolabelsentimentonthesentenceorphraseleveltoaidingeneratingacohesiveandinformativesum-mary.Thejointmodelcanbeusedtoclassifysen-tencesinthissettingbyconstraininginferencetotheknownﬁxeddocumentlabelforareview.Ifthisisdone,thensentimentaccuracyonthesentencelevelincreasessubstantiallyfrom62.6%to70.3%.FinallyweshouldnotethatexperimentsusingCRFstotrainthestructuredmodelsandlogisticre-gressiontotrainthelocalmodelsyieldedsimilarre-sultstothoseinTable2.3.2.1CascadedModelsAnotherapproachtoﬁne-to-coarsesentimentanalysisistouseacascadedsystem.Insuchasys-tem,asentencelevelclassiﬁermightﬁrstberunonthedata,andthentheresultsinputintoadocu-mentlevelclassiﬁer–orvice-versa.1Twocascadedsystemswerebuilt.TheﬁrstusestheSentence-Structuredclassiﬁertoclassifyallthesentencesfromareview,thenpassesthisinformationtothedocumentclassiﬁerasinput.Inparticular,forev-erypredicateintheoriginaldocumentclassiﬁer,anadditionalpredicatethatspeciﬁesthepolarityofthesentenceinwhichthispredicateoccurredwascre-ated.Thesecondcascadedsystemusesthedocu-mentclassiﬁertodeterminetheglobalpolarity,thenpassesthisinformationasinputintotheSentence-Structuredmodel,constructingpredicatesinasimi-larmanner.TheresultsforthesetwosystemscanbeseeninthelasttworowsofTable2.Inbothcasesthere1Alternatively,decisionsfromthesentenceclassiﬁercanguidewhichinputisseenbythedocumentlevelclassiﬁer(PangandLee,2004).isaslightimprovementinperformancesuggestingthataniterativeapproachmightbebeneﬁcial.Thatis,asystemcouldstartbyclassifyingdocuments,usethedocumentinformationtoclassifysentences,usethesentenceinformationtoclassifydocuments,andrepeatuntilconvergence.However,experimentsshowedthatthisdidnotimproveaccuracyoverasin-gleiterationandoftenhurtperformance.Improvementsfromthecascadedmodelsarefarlessconsistentthanthosegivenfromthejointstruc-turemodel.Thisisbecausedecisionsinthecas-cadedsystemarepassedtothenextlayerasthe“gold”standardattesttime,whichresultsinerrorsfromtheﬁrstclassiﬁerpropagatingtoerrorsinthesecond.Thiscouldbeimprovedbypassingalatticeofpossibilitiesfromtheﬁrstclassiﬁertothesecondwithcorrespondingconﬁdences.However,solutionssuchasthesearereallyjustapproximationsofthejointstructuredmodelthatwaspresentedhere.4FutureWorkOneimportantextensiontothisworkistoaugmentthemodelsforpartiallylabeleddata.Itisrealistictoimagineatrainingsetwheremanyexamplesdonothaveeverylevelofsentimentannotated.Forexample,therearethousandsofonlineproductre-viewswithlabeleddocumentsentiment,butamuchsmalleramountwheresentencesarealsolabeled.WorkonlearningwithhiddenvariablescanbeusedforbothCRFs(Quattonietal.,2004)andforin-ferencebasedlearningalgorithmslikethoseusedinthiswork(Liangetal.,2006).Anotherareaoffutureworkistoempiricallyin-vestigatetheuseofthesemodelsonlongerdocu-mentsthatrequiremorelevelsofsentimentanal-ysisthanproductreviews.Inparticular,therela-tivepositionofaphrasetoacontrastivediscourseconnectiveoracuephraselike“inconclusion”or“tosummarize”mayleadtoimprovedperformancesincehigherlevelclassiﬁcationscanlearntoweighinformationpassedfromtheselowerlevelcompo-nentsmoreheavily.5DiscussionInthispaperwehaveinvestigatedtheuseofaglobalstructuredmodelthatlearnstopredictsentimentondifferentlevelsofgranularityforatext.Wede-439

SentenceAccuracyDocumentAccuracyCarFitMp3TotalCarFitMp3TotalDocument-Classiﬁer----72.880.187.280.3Sentence-Classiﬁer54.856.849.453.1----Sentence-Structured60.561.455.758.8----Joint-Structured63.5∗65.2∗∗60.1∗∗62.6∗∗81.5∗81.985.082.8CascadedSentence→Document60.561.455.758.875.980.786.181.1CascadedDocument→Sentence59.761.058.359.572.880.187.280.3Table2:Fine-to-coarsesentimentaccuracy.SigniﬁcancecalculatedusingMcNemar’stestbetweentoptwoperformingsystems.∗Statisticallysigniﬁcantp<0.05.∗∗Statisticallysigniﬁcantp<0.005.scribedasimplemodelforsentence-documentanal-ysisandshowedthatinferenceinitistractable.Ex-perimentsshowthatthismodelobtainshigherac-curacythanclassiﬁerstrainedinisolationaswellascascadedsystemsthatpassinformationfromoneleveltoanotherattesttime.Furthermore,extensionstothesentence-documentmodelwerediscussedanditwasarguedthatanestedhierarchicalstructurewouldbebeneﬁcialsinceitwouldallowforefﬁcientinferencealgorithms.ReferencesY.Choi,C.Cardie,E.Riloff,andS.Patwardhan.2005.Identi-fyingsourcesofopinionswithconditionalrandomﬁeldsandextractionpatterns.InProc.HLT/EMNLP.Y.Choi,E.Breck,andC.Cardie.2006.Jointextractionofenti-tiesandrelationsforopinionrecognition.InProc.EMNLP.M.Collins.2002.DiscriminativetrainingmethodsforhiddenMarkovmodels:Theoryandexperimentswithperceptronalgorithms.InProc.EMNLP.K.CrammerandY.Singer.2003.Ultraconservativeonlinealgorithmsformulticlassproblems.JMLR.HalDaum´eIII,JohnLangford,andDanielMarcu.2006.Search-basedstructuredprediction.InSubmission.J.Lafferty,A.McCallum,andF.Pereira.2001.Conditionalrandomﬁelds:Probabilisticmodelsforsegmentingandla-belingsequencedata.InProc.ICML.P.Liang,A.Bouchard-Cote,D.Klein,andB.Taskar.2006.Anend-to-enddiscriminativeapproachtomachinetranslation.InProc.ACL.Y.MaoandG.Lebanon.2006.Isotonicconditionalrandomﬁeldsandlocalsentimentﬂow.InProc.NIPS.R.McDonald,K.Crammer,andF.Pereira.2005.Onlinelarge-margintrainingofdependencyparsers.InProc.ACL.R.McDonald.2006.DiscriminativeTrainingandSpanningTreeAlgorithmsforDependencyParsing.Ph.D.thesis,Uni-versityofPennsylvania.S.Miller,H.Fox,L.A.Ramshaw,andR.M.Weischedel.2000.Anoveluseofstatisticalparsingtoextractinformationfromtext.InProcNAACL,pages226–233.B.PangandL.Lee.2004.Asentimentaleducation:Sen-timentanalysisusingsubjectivitysummarizationbasedonminimumcuts.InProc.ACL.B.Pang,L.Lee,andS.Vaithyanathan.2002.Thumbsup?Sentimentclassiﬁcationusingmachinelearningtechniques.InEMNLP.A.PopescuandO.Etzioni.2005.Extractingproductfeaturesandopinionsfromreviews.InProc.HLT/EMNLP.A.Quattoni,M.Collins,andT.Darrell.2004.Conditionalrandomﬁeldsforobjectrecognition.InProc.NIPS.L.R.Rabiner.1989.AtutorialonhiddenMarkovmodelsandselectedapplicationsinspeechrecognition.ProceedingsoftheIEEE,77(2):257–285,February.D.RothandW.Yih.2004.Alinearprogrammingformula-tionforglobalinferenceinnaturallanguagetasks.InProc.CoNLL.C.SuttonandA.McCallum.2006.Anintroductiontocon-ditionalrandomﬁeldsforrelationallearning.InL.GetoorandB.Taskar,editors,IntroductiontoStatisticalRelationalLearning.MITPress.C.Sutton,K.Rohanimanesh,andA.McCallum.2004.Dy-namicconditionalrandomﬁelds:Factorizedprobabilisticmodelsforlabelingandsegmentingsequencedata.InProc.ICML.B.Taskar,C.Guestrin,andD.Koller.2003.Max-marginMarkovnetworks.InProc.NIPS.B.Taskar,D.Klein,M.Collins,D.Koller,andC.Manning.2004.Max-marginparsing.InProc.EMNLP.M.Thomas,B.Pang,andL.Lee.2006.Getoutthevote:Determiningsupportoroppositionfromcongressionalﬂoor-debatetranscripts.InProc.EMNLP.I.Tsochantaridis,T.Hofmann,T.Joachims,andY.Altun.2004.Supportvectorlearningforinterdependentandstructuredoutputspaces.InProc.ICML.P.Turney.2002.Thumbsuporthumbsdown?Sentimentori-entationappliedtounsupervisedclassiﬁcationofreviews.InEMNLP.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 440–447,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

440

Biographies,Bollywood,Boom-boxesandBlenders:DomainAdaptationforSentimentClassiﬁcationJohnBlitzerMarkDredzeDepartmentofComputerandInformationScienceUniversityofPennsylvania{blitzer|mdredze|pereria@cis.upenn.edu}FernandoPereiraAbstractAutomaticsentimentclassiﬁcationhasbeenextensivelystudiedandappliedinrecentyears.However,sentimentisexpresseddif-ferentlyindifferentdomains,andannotatingcorporaforeverypossibledomainofinterestisimpractical.Weinvestigatedomainadap-tationforsentimentclassiﬁers,focusingononlinereviewsfordifferenttypesofprod-ucts.First,weextendtosentimentclassiﬁ-cationtherecently-proposedstructuralcor-respondencelearning(SCL)algorithm,re-ducingtherelativeerrorduetoadaptationbetweendomainsbyanaverageof30%overtheoriginalSCLalgorithmand46%overasupervisedbaseline.Second,weidentifyameasureofdomainsimilaritythatcorre-lateswellwiththepotentialforadaptationofaclassiﬁerfromonedomaintoanother.Thismeasurecouldforinstancebeusedtoselectasmallsetofdomainstoannotatewhosetrainedclassiﬁerswouldtransferwelltomanyotherdomains.1IntroductionSentimentdetectionandclassiﬁcationhasreceivedconsiderableattentionrecently(Pangetal.,2002;Turney,2002;GoldbergandZhu,2004).Whilemoviereviewshavebeenthemoststudieddomain,sentimentanalysishasextendedtoanumberofnewdomains,rangingfromstockmessageboardstocongressionalﬂoordebates(DasandChen,2001;Thomasetal.,2006).ResearchresultshavebeendeployedindustriallyinsystemsthatgaugemarketreactionandsummarizeopinionfromWebpages,discussionboards,andblogs.Withsuchwidely-varyingdomains,researchersandengineerswhobuildsentimentclassiﬁcationsystemsneedtocollectandcuratedataforeachnewdomaintheyencounter.Eveninthecaseofmarketanalysis,ifautomaticsentimentclassiﬁcationweretobeusedacrossawiderangeofdomains,theef-forttoannotatecorporaforeachdomainmaybe-comeprohibitive,especiallysinceproductfeatureschangeovertime.Weenvisionascenarioinwhichdevelopersannotatecorporaforasmallnumberofdomains,trainclassiﬁersonthosecorpora,andthenapplythemtoothersimilarcorpora.However,thisapproachraisestwoimportantquestions.First,itiswellknownthattrainedclassiﬁersloseaccuracywhenthetestdatadistributionissigniﬁcantlydiffer-entfromthetrainingdatadistribution1.Second,itisnotclearwhichnotionofdomainsimilarityshouldbeusedtoselectdomainstoannotatethatwouldbegoodproxiesformanyotherdomains.Weproposesolutionstothesetwoquestionsandevaluatethemonacorpusofreviewsforfourdiffer-enttypesofproductsfromAmazon:books,DVDs,electronics,andkitchenappliances2.First,weshowhowtoextendtherecentlyproposedstructuralcor-1Forsurveysofrecentresearchondomainadaptation,seetheICML2006WorkshoponStructuralKnowledgeTransferforMachineLearning(http://gameairesearch.uta.edu/)andtheNIPS2006WorkshoponLearningwhentestandtraininginputshavedifferentdistribution(http://ida.first.fraunhofer.de/projects/different06/)2Thedatasetwillbemadeavailablebytheauthorsatpubli-cationtime.441

respondencelearning(SCL)domainadaptational-gorithm(Blitzeretal.,2006)foruseinsentimentclassiﬁcation.AkeystepinSCListheselectionofpivotfeaturesthatareusedtolinkthesourceandtar-getdomains.Wesuggestselectingpivotsbasednotonlyontheircommonfrequencybutalsoaccordingtotheirmutualinformationwiththesourcelabels.Fordataasdiverseasproductreviews,SCLcansometimesmisalignfeatures,resultingindegrada-tionwhenweadaptbetweendomains.Inoursecondextensionweshowhowtocorrectmisalignmentsus-ingaverysmallnumberoflabeledinstances.Second,weevaluatetheA-distance(Ben-Davidetal.,2006)betweendomainsasmeasureofthelossduetoadaptationfromonetotheother.TheA-distancecanbemeasuredfromunlabeleddata,anditwasdesignedtotakeintoaccountonlydivergenceswhichaffectclassiﬁcationaccuracy.Weshowthatitcorrelateswellwithadaptationloss,indicatingthatwecanusetheA-distancetoselectasubsetofdo-mainstolabelassources.InthenextsectionwebrieﬂyreviewSCLandin-troduceournewpivotselectionmethod.Section3describesdatasetsandexperimentalmethod.Sec-tion4givesresultsforSCLandthemutualinforma-tionmethodforselectingpivotfeatures.Section5showshowtocorrectfeaturemisalignmentsusingasmallamountoflabeledtargetdomaindata.Sec-tion6motivatestheA-distanceandshowsthatitcorrelateswellwithadaptability.WediscussrelatedworkinSection7andconcludeinSection8.2StructuralCorrespondenceLearningBeforereviewingSCL,wegiveabriefillustrativeexample.Supposethatweareadaptingfromre-viewsofcomputerstoreviewsofcellphones.Whilemanyofthefeaturesofagoodcellphonereviewarethesameasacomputerreview–thewords“excel-lent”and“awful”forexample–manywordsareto-tallynew,like“reception”.Atthesametime,manyfeatureswhichwereusefulforcomputers,suchas“dual-core”arenolongerusefulforcellphones.Ourkeyintuitionisthatevenwhen“good-qualityreception”and“fastdual-core”arecompletelydis-tinctforeachdomain,iftheybothhavehighcorrela-tionwith“excellent”andlowcorrelationwith“aw-ful”onunlabeleddata,thenwecantentativelyalignthem.Afterlearningaclassiﬁerforcomputerre-views,whenweseeacell-phonefeaturelike“good-qualityreception”,weknowitshouldbehaveinaroughlysimilarmannerto“fastdual-core”.2.1AlgorithmOverviewGivenlabeleddatafromasourcedomainandun-labeleddatafrombothsourceandtargetdomains,SCLﬁrstchoosesasetofmpivotfeatureswhichoc-curfrequentlyinbothdomains.Then,itmodelsthecorrelationsbetweenthepivotfeaturesandallotherfeaturesbytraininglinearpivotpredictorstopredictoccurrencesofeachpivotintheunlabeleddatafrombothdomains(AndoandZhang,2005;Blitzeretal.,2006).The‘thpivotpredictorischaracterizedbyitsweightvectorw‘;positiveentriesinthatweightvectormeanthatanon-pivotfeature(like“fastdual-core”)ishighlycorrelatedwiththecorrespondingpivot(like“excellent”).ThepivotpredictorcolumnweightvectorscanbearrangedintoamatrixW=[w‘]n‘=1.Letθ∈Rk×dbethetopkleftsingularvectorsofW(heredindi-catesthetotalnumberoffeatures).Thesevectorsaretheprincipalpredictorsforourweightspace.Ifwechoseourpivotfeatureswell,thenweexpecttheseprincipalpredictorstodiscriminateamongpositiveandnegativewordsinbothdomains.Attrainingandtesttime,supposeweobserveafeaturevectorx.Weapplytheprojectionθxtoob-tainknewreal-valuedfeatures.Nowwelearnapredictorfortheaugmentedinstancehx,θxi.Ifθcontainsmeaningfulcorrespondences,thenthepre-dictorwhichusesθwillperformwellinbothsourceandtargetdomains.2.2SelectingPivotswithMutualInformationTheefﬁcacyofSCLdependsonthechoiceofpivotfeatures.ForthepartofspeechtaggingproblemstudiedbyBlitzeretal.(2006),frequently-occurringwordsinbothdomainsweregoodchoices,sincetheyoftencorrespondtofunctionwordssuchasprepositionsanddeterminers,whicharegoodindi-catorsofpartsofspeech.Thisisnotthecaseforsentimentclassiﬁcation,however.Therefore,were-quirethatpivotfeaturesalsobegoodpredictorsofthesourcelabel.Amongthosefeatures,wethenchoosetheoneswithhighestmutualinformationtothesourcelabel.Table1showstheset-symmetric442

SCL,notSCL-MISCL-MI,notSCLbookone<num>soallamustawonderfulloveditveryabouttheylikeweakdon’twasteawfulgoodwhenhighlyrecommendedandeasyTable1:ToppivotsselectedbySCL,butnotSCL-MI(left)andvice-versa(right)differencesbetweenthetwomethodsforpivotselec-tionwhenadaptingaclassiﬁerfrombookstokitchenappliances.WereferthroughouttherestofthisworktoourmethodforselectingpivotsasSCL-MI.3DatasetandBaselineWeconstructedanewdatasetforsentimentdomainadaptationbyselectingAmazonproductreviewsforfourdifferentproducttypes:books,DVDs,electron-icsandkitchenappliances.Eachreviewconsistsofarating(0-5stars),areviewernameandlocation,aproductname,areviewtitleanddate,andthere-viewtext.Reviewswithrating>3werelabeledpositive,thosewithrating<3werelabeledneg-ative,andtherestdiscardedbecausetheirpolaritywasambiguous.Afterthisconversion,wehad1000positiveand1000negativeexamplesforeachdo-main,thesamebalancedcompositionasthepolaritydataset(Pangetal.,2002).Inadditiontothelabeleddata,weincludedbetween3685(DVDs)and5945(kitchen)instancesofunlabeleddata.ThesizeoftheunlabeleddatawaslimitedprimarilybythenumberofreviewswecouldcrawlanddownloadfromtheAmazonwebsite.Sincewewereabletoobtainla-belsforallofthereviews,wealsoensuredthattheywerebalancedbetweenpositiveandnegativeexam-ples,aswell.Whilethepolaritydatasetisapopularchoiceintheliterature,wewereunabletouseitforourtask.OurmethodrequiresmanyunlabeledreviewsanddespitealargenumberofIMDBreviewsavailableonline,theextensivecurationrequirementsmadepreparingalargeamountofdatadifﬁcult3.Forclassiﬁcation,weuselinearpredictorsonun-igramandbigramfeatures,trainedtominimizetheHuberlosswithstochasticgradientdescent(Zhang,3Foradescriptionoftheconstructionofthepolaritydataset,seehttp://www.cs.cornell.edu/people/pabo/movie-review-data/.2004).Onthepolaritydataset,thismodelmatchestheresultsreportedbyPangetal.(2002).WhenwereportresultswithSCLandSCL-MI,werequirethatpivotsoccurinmorethanﬁvedocumentsineachdo-main.Wesetk,thenumberofsingularvectorsoftheweightmatrix,to50.4ExperimentswithSCLandSCL-MIEachlabeleddatasetwassplitintoatrainingsetof1600instancesandatestsetof400instances.Alltheexperimentsuseaclassiﬁertrainedonthetrain-ingsetofonedomainandtestedonthetestsetofapossiblydifferentdomain.Thebaselineisalin-earclassiﬁertrainedwithoutadaptation,whilethegoldstandardisanin-domainclassiﬁertrainedonthesamedomainasitistested.Figure1givesaccuraciesforallpairsofdomainadaptation.Thedomainsareorderedclockwisefromthetopleft:books,DVDs,electronics,andkitchen.Foreachsetofbars,theﬁrstletteristhesourcedomainandthesecondletteristhetargetdomain.Thethickhorizontalbarsaretheaccura-ciesofthein-domainclassiﬁersforthesedomains.Thustheﬁrstsetofbarsshowsthatthebaselineachieves72.8%accuracyadaptingfromDVDstobooks.SCL-MIachieves79.7%andthein-domaingoldstandardis80.4%.Wesaythattheadaptationlossforthebaselinemodelis7.6%andtheadapta-tionlossfortheSCL-MImodelis0.7%.TherelativereductioninerrorduetoadaptationofSCL-MIforthistestis90.8%.Wecanobservefromtheseresultsthatthereisaroughgroupingofourdomains.BooksandDVDsaresimilar,asarekitchenappliancesandelectron-ics,butthetwogroupsaredifferentfromonean-other.AdaptingclassiﬁersfrombookstoDVDs,forinstance,iseasierthanadaptingthemfrombookstokitchenappliances.Wenotethatwhentransfer-ringfromkitchentoelectronics,SCL-MIactuallyoutperformsthein-domainclassiﬁer.Thisispossi-blesincetheunlabeleddatamaycontaininformationthatthein-domainclassiﬁerdoesnothaveaccessto.AtthebeginningofSection2wegaveexam-plesofhowfeaturescanchangebehavioracrossdo-mains.Theﬁrsttypeofbehavioriswhenpredictivefeaturesfromthesourcedomainarenotpredictiveordonotappearinthetargetdomain.Thesecondis443

657075808590D->BE->BK->BB->DE->DK->DbaselineSCLSCL-MIbooks72.876.879.770.775.475.470.966.168.680.482.477.274.075.870.674.376.272.775.476.9dvd657075808590B->ED->EK->EB->KD->KE->Kelectronicskitchen70.877.575.973.074.174.182.783.786.884.487.774.578.778.974.079.481.484.084.485.9Figure1:AccuracyresultsfordomainadaptationbetweenallpairsusingSCLandSCL-MI.Thickblacklinesaretheaccuraciesofin-domainclassiﬁers.domain\polaritynegativepositivebooksplot<num>pagespredictablereadergrishamengagingreadingthispage<num>mustreadfascinatingkitchentheplasticpoorlydesignedexcellentproductespressoleakingawkwardtodefectiveareperfectyearsnowabreezeTable2:CorrespondencesdiscoveredbySCLforbooksandkitchenappliances.Thetoprowshowsfeaturesthatonlyappearinbooksandthebottomfeaturesthatonlyappearinkitchenappliances.Theleftandrightcolumnsshownegativeandpositivefeaturesincorrespondence,respectively.whenpredictivefeaturesfromthetargetdomaindonotappearinthesourcedomain.ToshowhowSCLdealswiththosedomainmismatches,welookattheadaptationfrombookreviewstoreviewsofkitchenappliances.Weselectedthetop1000mostinfor-mativefeaturesinbothdomains.Inbothcases,be-tween85and90%oftheinformativefeaturesfromonedomainwerenotamongthemostinformativeoftheotherdomain4.SCLaddressesbothoftheseissuessimultaneouslybyaligningfeaturesfromthetwodomains.4Thereisathirdtype,featureswhicharepositiveinonedo-mainbutnegativeinanother,buttheyappearveryinfrequentlyinourdatasets.Table2illustratesonerowoftheprojectionma-trixθforadaptingfrombookstokitchenappliances;thefeaturesoneachrowappearonlyinthecorre-spondingdomain.Asupervisedclassiﬁertrainedonbookreviewscannotassignweighttothekitchenfeaturesinthesecondrowoftable2.Incon-trast,SCLassignsweighttothesefeaturesindirectlythroughtheprojectionmatrix.Whenweobservethefeature“predictable”withanegativebookre-view,weupdateparameterscorrespondingtotheentireprojection,includingthekitchen-speciﬁcfea-tures“poorlydesigned”and“awkwardto”.Whilesomerowsoftheprojectionmatrixθare444

usefulforclassiﬁcation,SCLcanalsomisalignfea-tures.Thiscausesproblemswhenaprojectionisdiscriminativeinthesourcedomainbutnotinthetarget.Thisisthecaseforadaptingfromkitchenappliancestobooks.Sincethebookdomainisquitebroad,manyprojectionsinbooksmodeltopicdistinctionssuchasbetweenreligiousandpoliticalbooks.Theseprojections,whichareuninforma-tiveastothetargetlabel,areputintocorrespon-dencewiththefewerdiscriminatingprojectionsinthemuchnarrowerkitchendomain.Whenweadaptfromkitchentobooks,weassignweighttotheseun-informativeprojections,degradingtargetclassiﬁca-tionaccuracy.5CorrectingMisalignmentsWenowshowhowtouseasmallamountoftargetdomainlabeleddatatolearntoignoremisalignedprojectionsfromSCL-MI.UsingthenotationofAndoandZhang(2005),wecanwritethesupervisedtrainingobjectiveofSCLonthesourcedomainasminw,vXiL(cid:0)w0xi+v0θxi,yi(cid:1)+λ||w||2+µ||v||2,whereyisthelabel.Theweightvectorw∈Rdweighstheoriginalfeatures,whilev∈Rkweighstheprojectedfeatures.AndoandZhang(2005)andBlitzeretal.(2006)suggestλ=10−4,µ=0,whichwehaveusedinourresultssofar.Supposenowthatwehavetrainedsourcemodelweightvectorswsandvs.Asmallamountoftar-getdomaindataisprobablyinsufﬁcienttosignif-icantlychangew,butwecancorrectv,whichismuchsmaller.Weaugmenteachlabeledtargetin-stancexjwiththelabelassignedbythesourcedo-mainclassiﬁer(Florianetal.,2004;Blitzeretal.,2006).Thenwesolveminw,vPjL(w0xj+v0θxj,yj)+λ||w||2+µ||v−vs||2.Sincewedon’twanttodeviatesigniﬁcantlyfromthesourceparameters,wesetλ=µ=10−1.Figure2showsthecorrectedSCL-MImodelus-ing50targetdomainlabeledinstances.Wechosethisnumbersincewebelieveittobeareasonableamountforasingleengineertolabelwithminimaleffort.Forreasonsofspace,foreachtargetdomaindom\modelbasebasesclscl-miscl-mi+targ+targbooks8.99.07.45.84.4dvd8.98.97.86.15.3electron8.38.56.05.54.8kitchen10.29.97.05.65.1average9.19.17.15.84.9Table3:Foreachdomain,weshowthelossduetotransferforeachmethod,averagedoveralldomains.Thebottomrowshowstheaveragelossoverallruns.weshowadaptationfromonlythetwodomainsonwhichSCL-MIperformedtheworstrelativetothesupervisedbaseline.Forexample,thebookdomainshowsonlyresultsfromelectronicsandkitchen,butnotDVDs.Asabaseline,weusedthelabelofthesourcedomainclassiﬁerasafeatureinthetarget,butdidnotuseanySCLfeatures.Wenotethatthebase-lineisveryclosetojustusingthesourcedomainclassiﬁer,becausewithonly50targetdomainin-stanceswedonothaveenoughdatatorelearnalloftheparametersinw.Aswecansee,though,relearn-ingthe50parametersinvisquitehelpful.Thecor-rectedmodelalwaysimprovesoverthebaselineforeverypossibletransfer,includingthosenotshownintheﬁgure.Theideaofusingtheregularizerofalinearmodeltoencouragethetargetparameterstobeclosetothesourceparametershasbeenusedpreviouslyindo-mainadaptation.Inparticular,ChelbaandAcero(2004)showedhowthistechniquecanbeeffectiveforcapitalizationadaptation.Themajordifferencebetweenourapproachandtheirsisthatweonlype-nalizedeviationfromthesourceparametersfortheweightsvofprojectedfeatures,whiletheyworkwiththeweightsoftheoriginalfeaturesonly.Foroursmallamountoflabeledtargetdata,attemptingtopenalizewusingwsperformednobetterthanourbaseline.Becauseweonlyneedtolearntoig-noreprojectionsthatmisalignfeatures,wecanmakemuchbetteruseofourlabeleddatabyadaptingonly50parameters,ratherthan200,000.Table3summarizestheresultsofsections4and5.Structuralcorrespondencelearningreducestheerrorduetotransferby21%.Choosingpivotsbymutualinformationallowsustofurtherreducetheerrorto36%.Finally,byadding50instancesoftar-getdomaindataandusingthistocorrectthemis-alignedprojections,weachieveanaveragerelative445

657075808590E->BK->BB->DK->DB->ED->EB->KE->Kbase+50-targSCL-MI+50-targbookskitchen70.976.070.776.878.572.780.487.776.670.876.673.077.974.380.784.3dvdelectronics82.484.473.285.9Figure2:Accuracyresultsfordomainadaptationwith50labeledtargetdomaininstances.reductioninerrorof46%.6MeasuringAdaptabilitySections2-5focusedonhowtoadapttoatargetdo-mainwhenyouhadalabeledsourcedataset.Wenowtakeastepbacktolookattheproblemofse-lectingsourcedomaindatatolabel.Westudyaset-tingwhereanengineerknowsroughlyherdomainsofinterestbutdoesnothaveanylabeleddatayet.Inthatcase,shecanaskthequestion“WhichsourcesshouldIlabeltoobtainthebestperformanceoverallmydomains?”Onourproductdomains,forex-ample,ifweareinterestedinclassifyingreviewsofkitchenappliances,weknowfromsections4-5thatitwouldbefoolishtolabelreviewsofbooksorDVDsratherthanelectronics.HereweshowhowtoselectsourcedomainsusingonlyunlabeleddataandtheSCLrepresentation.6.1TheA-distanceWeproposetomeasuredomainadaptabilitybyus-ingthedivergenceoftwodomainsaftertheSCLprojection.Wecancharacterizedomainsbytheirinduceddistributionsoninstancespace:themoredifferentthedomains,themoredivergentthedistri-butions.HerewemakeuseoftheA-distance(Ben-Davidetal.,2006).ThekeyintuitionbehindtheA-distanceisthatwhiletwodomainscandifferinarbitraryways,weareonlyinterestedinthediffer-encesthataffectclassiﬁcationaccuracy.LetAbethefamilyofsubsetsofRkcorrespond-ingtocharacteristicfunctionsoflinearclassiﬁers(setsonwhichalinearclassiﬁerreturnspositivevalue).ThentheAdistancebetweentwoprobabilitydistributionsisdA(D,D0)=2supA∈A|PrD[A]−PrD0[A]|.Thatis,weﬁndthesubsetinAonwhichthedistri-butionsdifferthemostintheL1sense.Ben-Davidetal.(2006)showthatcomputingtheA-distanceforaﬁnitesampleisexactlytheproblemofminimiz-ingtheempiricalriskofaclassiﬁerthatdiscrimi-natesbetweeninstancesdrawnfromDandinstancesdrawnfromD0.Thisisconvenientforus,sinceital-lowsustouseclassiﬁcationmachinerytocomputetheA-distance.6.2UnlabeledAdaptabilityMeasurementsWefollowBen-Davidetal.(2006)andusetheHu-berlossasaproxyfortheA-distance.Ourproce-dureisasfollows:Giventwodomains,wecomputetheSCLrepresentation.Thenwecreateadatasetwhereeachinstanceθxislabeledwiththeidentityofthedomainfromwhichitcameandtrainalinearclassiﬁer.Foreachpairofdomainswecomputetheempiricalaverageper-instanceHuberloss,subtractitfrom1,andmultiplytheresultby100.WerefertothisquantityastheproxyA-distance.Whenitis100,thetwodomainsarecompletelydistinct.Whenitis0,thetwodomainsareindistinguishableusingalinearclassiﬁer.Figure3isacorrelationplotbetweentheproxyA-distanceandtheadaptationerror.Supposewewantedtolabeltwodomainsoutofthefourinsucha446

024681012146065707580859095100Proxy A-distanceAdaptation LossEKBDDEDKBE, BKFigure3:TheproxyA-distancebetweeneachdo-mainpairplottedagainsttheaverageadaptationlossofasmeasuredbyourbaselinesystem.Eachpairofdomainsislabeledbytheirﬁrstletters:EKindicatesthepairelectronicsandkitchen.wayastominimizeourerroronallthedomains.Us-ingtheproxyA-distanceasacriterion,weobservethatwewouldchooseonedomainfromeitherbooksorDVDs,butnotboth,sincethenwewouldnotbeabletoadequatelycoverelectronicsorkitchenappli-ances.Similarlywewouldalsochooseonedomainfromeitherelectronicsorkitchenappliances,butnotboth.7RelatedWorkSentimentclassiﬁcationhasadvancedconsiderablysincetheworkofPangetal.(2002),whichweuseasourbaseline.Thomasetal.(2006)usediscoursestructurepresentincongressionalrecordstoperformmoreaccuratesentimentclassiﬁcation.PangandLee(2005)treatsentimentanalysisasanordinalrankingproblem.Inourworkweonlyshowim-provementforthebasicmodel,butallofthesenewtechniquesalsomakeuseoflexicalfeatures.Thuswebelievethatouradaptationmethodscouldbealsoappliedtothosemorereﬁnedmodels.Whileworkondomainadaptationforsenti-mentclassiﬁersissparse,itisworthnotingthatotherresearchershaveinvestigatedunsupervisedandsemisupervisedmethodsfordomainadaptation.TheworkmostsimilarinspirittooursthatofTur-ney(2002).Heusedthedifferenceinmutualin-formationwithtwohuman-selectedfeatures(thewords“excellent”and“poor”)toscorefeaturesinacompletelyunsupervisedmanner.Thenheclas-siﬁeddocumentsaccordingtovariousfunctionsofthesemutualinformationscores.Westressthatourmethodimprovesasupervisedbaseline.Whilewedonothaveadirectcomparison,wenotethatTur-ney(2002)performsworseonmoviereviewsthanonhisotherdatasets,thesametypeofdataasthepolaritydataset.WealsonotetheworkofAueandGamon(2005),whoperformedanumberofempiricaltestsondo-mainadaptationofsentimentclassiﬁers.Mostofthesetestswereunsuccessful.Webrieﬂynotetheirresultsoncombininganumberofsourcedomains.Theyobservedthatsourcedomainsclosertothetar-gethelpedmore.Inpreliminaryexperimentsweconﬁrmedtheseresults.Addingmorelabeleddataalwayshelps,butdiversifyingtrainingdatadoesnot.Whenclassifyingkitchenappliances,foranyﬁxedamountoflabeleddata,itisalwaysbettertodrawfromelectronicsasasourcethanusesomecombi-nationofallthreeotherdomains.Domainadaptationaloneisagenerallywell-studiedarea,andwecannotpossiblyhopetocoverallofithere.AswenotedinSection5,weareabletosigniﬁcantlyoutperformbasicstructuralcor-respondencelearning(Blitzeretal.,2006).WealsonotethatwhileFlorianetal.(2004)andBlitzeretal.(2006)observethatincludingthelabelofasourceclassiﬁerasafeatureonsmallamountsoftargetdatatendstoimproveoverusingeitherthesourcealoneorthetargetalone,wedidnotobservethatforourdata.Webelievethemostimportantreasonforthisisthattheyexplorestructuredpredictionproblems,wherelabelsofsurroundingwordsfromthesourceclassiﬁermaybeveryinformative,evenifthecur-rentlabelisnot.Incontrastoursimplebinarypre-dictionproblemdoesnotexhibitsuchbehavior.ThismayalsobethereasonthatthemodelofChelbaandAcero(2004)didnotaidinadaptation.FinallywenotethatwhileBlitzeretal.(2006)didcombineSCLwithlabeledtargetdomaindata,theyonlycomparedusingthelabelofSCLornon-SCLsourceclassiﬁersasfeatures,followingtheworkofFlorianetal.(2004).ByonlyadaptingtheSCL-relatedpartoftheweightvectorv,weareabletomakebetteruseofoursmallamountofunlabeleddatathantheseprevioustechniques.447

8ConclusionSentimentclassiﬁcationhasseenagreatdealofat-tention.Itsapplicationtomanydifferentdomainsofdiscoursemakesitanidealcandidatefordomainadaptation.Thisworkaddressedtwoimportantquestionsofdomainadaptation.First,weshowedthatforagivensourceandtargetdomain,wecansigniﬁcantlyimproveforsentimentclassiﬁcationthestructuralcorrespondencelearningmodelofBlitzeretal.(2006).Wechosepivotfeaturesusingnotonlycommonfrequencyamongdomainsbutalsomutualinformationwiththesourcelabels.Wealsoshowedhowtocorrectstructuralcorrespondencemisalign-mentsbyusingasmallamountoflabeledtargetdo-maindata.Second,weprovidedamethodforselectingthosesourcedomainsmostlikelytoadaptwelltogiventargetdomains.TheunsupervisedA-distancemea-sureofdivergencebetweendomainscorrelateswellwithlossduetoadaptation.ThuswecanusetheA-distancetoselectsourcedomainstolabelwhichwillgivelowtargetdomainerror.Inthefuture,wewishtoincludesomeofthemorerecentadvancesinsentimentclassiﬁcation,aswellasaddressingthemorerealisticproblemofrank-ing.Wearealsoactivelysearchingforalargerandmorevariedsetofdomainsonwhichtotestourtech-niques.AcknowledgementsWethankNikhilDineshforhelpfuladvicethrough-outthecourseofthiswork.ThismaterialisbaseduponworkpartiallysupportedbytheDefenseAd-vancedResearchProjectsAgency(DARPA)un-derContractNo.NBCHD03001.Anyopinions,ﬁndings,andconclusionsorrecommendationsex-pressedinthismaterialarethoseoftheauthorsanddonotnecessarilyreﬂecttheviewsofDARPAortheDepartmentofInterior-NationalBusinessCenter(DOI-NBC).ReferencesRieAndoandTongZhang.2005.Aframeworkforlearningpredictivestructuresfrommultipletasksandunlabeleddata.JMLR,6:1817–1853.AnthonyAueandMichaelGamon.2005.Customiz-ingsentimentclassiﬁerstonewdomains:acasestudy.http://research.microsoft.com/anthaue/.ShaiBen-David,JohnBlitzer,KobyCrammer,andFer-nandoPereira.2006.Analysisofrepresentationsfordomainadaptation.InNeuralInformationProcessingSystems(NIPS).JohnBlitzer,RyanMcDonald,andFernandoPereira.2006.Domainadaptationwithstructuralcorrespon-dencelearning.InEmpiricalMethodsinNaturalLan-guageProcessing(EMNLP).CiprianChelbaandAlexAcero.2004.Adaptationofmaximumentropycapitalizer:Littledatacanhelpalot.InEMNLP.SanjivDasandMikeChen.2001.Yahoo!forama-zon:Extractingmarketsentimentfromstockmessageboards.InProceedingsofAtheAsiaPaciﬁcFinanceAssociationAnnualConference.R.Florian,H.Hassan,A.Ittycheriah,H.Jing,N.Kamb-hatla,X.Luo,N.Nicolov,andS.Roukos.2004.Astatisticalmodelformultilingualentitydetectionandtracking.InofHLT-NAACL.AndrewGoldbergandXiaojinZhu.2004.Seeingstarswhentherearen’tmanystars:Graph-basedsemi-supervisedlearningforsentimentcategorization.InHLT-NAACL2006WorkshoponTextgraphs:Graph-basedAlgorithmsforNaturalLanguageProcessing.BoPangandLillianLee.2005.Seeingstars:Exploitingclassrelationshipsforsentimentcategorizationwithrespecttoratingscales.InProceedingsofAssociationforComputationalLinguistics.BoPang,LillianLee,andShivakumarVaithyanathan.2002.Thumbsup?sentimentclassiﬁcationusingma-chinelearningtechniques.InProceedingsofEmpiri-calMethodsinNaturalLanguageProcessing.MattThomas,BoPang,andLillianLee.2006.Getoutthevote:Determiningsupportoroppositionfromcon-gressionalﬂoor-debatetranscripts.InEmpiricalMeth-odsinNaturalLanguageProcessing(EMNLP).PeterTurney.2002.Thumbsuporthumbsdown?se-manticorientationappliedtounsupervisedclassiﬁca-tionofreviews.InProceedingsofAssociationforComputationalLinguistics.TongZhang.2004.Solvinglargescalelinearpredic-tionproblemsusingstochasticgradientdescental-gorithms.InInternationalConferenceonMachineLearning(ICML).Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 448–455,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

448

ClusteringClausesforHigh-LevelRelationDetection:AnInformation-theoreticApproachSamuelBrodySchoolofInformaticsUniversityofEdinburghs.brody@sms.ed.ac.ukAbstractRecently,therehasbeenariseofin-terestinunsuperviseddetectionofhigh-levelsemanticrelationsinvolvingcom-plexunits,suchasphrasesandwholesentences.Typicallysuchapproachesarefacedwithtwomainobstacles:datasparsenessandcorrectlygeneralizingfromtheexamples.Inthiswork,wedescribetheClusteredClauserepresen-tation,whichutilizesinformation-basedclusteringandinter-sentencedependen-ciestocreateasimpliﬁedandgeneralizedrepresentationofthegrammaticalclause.Weimplementanalgorithmwhichusesthisrepresentationtodetectapredeﬁnedsetofhigh-levelrelations,anddemon-strateourmodel’seﬀectivenessinover-comingboththeproblemsmentioned.1IntroductionThesemanticrelationshipbetweenwords,andtheextractionofmeaningfromsyntacticdatahasbeenoneofthemainpointsofresearchintheﬁeldofcomputationallinguistics(seeSec-tion5andreferencestherein).Untilrecently,thefocushasremainedlargelyeitheratthesin-glewordorsentencelevel(forinstance:depen-dencyextraction,word-to-wordsemanticsimi-larityfromsyntax,etc.)oronrelationsbetweenunitsataveryhighcontextlevelsuchastheentireparagraphordocument(e.g.categorizingdocumentsbytopic).Recentlytherehavebeenseveralattemptstodeﬁneframeworksfordetectingandstudyingin-teractionsatanintermediatecontextlevel,andinvolvingwholeclausesorsentences.Daganetal.(2005)haveemphasizedtheimportanceofdetectingtextual-entailment/implicationbe-tweentwosentences,anditsplaceasakeycom-ponentinmanyreal-worldapplications,suchasInformationRetrievalandQuestionAnswering.Whendesigningsuchaframework,oneisfacedwithseveralobstacles.Asweapproachhigherlevelsofcomplexity,theproblemofdeﬁn-ingthebasicunitswestudy(e.g.words,sen-tencesetc.)andtheincreasingamountofin-teractionsmakethetaskverydiﬃcult.Inaddi-tion,thedatasparsenessproblembecomesmoreacuteasthedataunitsbecomemorecomplexandhaveanincreasingnumberofpossibleval-ues,despitethefactthatmanyofthesevalueshavesimilaroridenticalmeaning.Inthispaperwedemonstrateanapproachtosolvingthecomplexityanddatasparse-nessproblemsinthetaskofdetectingrela-tionsbetweensentencesorclauses.WepresenttheClusteredClausestructure,whichutilizesinformation-basedclusteringanddependencieswithinthesentencetocreateasimpliﬁedandgeneralizedrepresentationofthegrammaticalclauseandisdesignedtoovercomeboththeseproblems.Theclusteringmethodweemployisaninte-gralpartofthemodel.Weevaluateourclustersagainstsemanticsimilaritymeasuresdeﬁnedonthehuman-annotatedWordNetstructure(Fell-baum,1998).Theresultsofthesecomparisonsshowthatourclustermembersareverysimilarsemantically.Wealsodeﬁneahigh-levelrela-tiondetectiontaskinvolvingrelationsbetweenclauses,evaluateourresults,anddemonstrate449

theeﬀectivenessofusingourmodelinthistask.ThisworkextendsselectedpartsofBrody(2005),wherefurtherdetailscanbefound.2ModelConstructionWhendesigningourframework,wemustad-dressthecomplexityandsparsenessproblemsencounteredwhendealingwithwholesentences.Oursolutiontotheseissuescombinestwoele-ments.First,toreducecomplexity,wesimplifyagrammaticalclausetoitsprimarycomponents-thesubject,verbandobject.Secondly,topro-videageneralizationframeworkwhichwillen-ableustoovercomedata-sparseness,weclustereachpartoftheclauseusingdatafromwithintheclauseitself.BycombiningthesimpliﬁedclausestructureandtheclusteringweproduceourClusteredClausemodel-atripletofclustersrepresentingageneralizedclause.TheSimpliﬁedClause:Inordertoextractclausesfromthetext,weuseLin’sparserMINI-PAR(Lin,1994).Theoutputoftheparserisadependencytreeofeachsentence,alsocon-taininglemmatizedversionsofthecomponentwords.Weextracttheverb,subjectandobjectofeveryclause(includingsubordinateclauses),andusethistripletofvalues,thesimpliﬁedclause,inplaceoftheoriginalcompleteclause.AsseeninFigure1,thesecomponentsmakeupthetop(root)triangleoftheclauseparsetree.Wealsousethelemmatizedformofthewordsprovidedbytheparser,tofurtherreducecom-plexity.Figure1:Theparsetreeforthesentence“Johnfoundasolutiontotheproblem”.Thesubject-verb-objecttripletismarkedwithaborder.ClusteringClauseComponents:Forourmodel,weclusterthedatatoprovidebothgen-eralization,byusingaclustertorepresentamoregeneralizedconceptsharedbyitscompo-nentwords,andaformofdimensionalityreduc-tion,byusingfewerunits(clusters)torepresentamuchlargeramountofwords.WechosetousetheSequentialInformationBottleneckalgorithm(Slonimetal.,2002)forourclusteringtasks.TheinformationBottle-neckprincipleviewstheclusteringtaskasanoptimizationproblem,wheretheclusteringalgo-rithmattemptstogrouptogethervaluesofonevariablewhileretainingasmuchinformationaspossibleregardingthevaluesofanother(target)variable.Thereisatrade-oﬀbetweenthecom-pactnessoftheclusteringandtheamountofre-tainedinformation.Thisalgorithm(andothersbasedontheIBprinciple)isespeciallysuitedforusewithgraphicalmodelsordependencystruc-tures,sincethedistancemeasureitemploysintheclusteringisdeﬁnedsolelybythedepen-dencyrelationbetweentwovariables,andthere-forerequirednoexternalparameters.Theval-uesofonevariableareclusteredusingtheirco-occurrencedistributionwithregardtothevaluesofthesecond(target)variableinthedependencyrelation.Asanexample,considerthefollowingsubject-verbco-occurrencematrix:S\Vtellscratchdrinkdog045John409cat063man612Thevalueincell(i,j)indicatesthenumberoftimesthenounioccurredasthesubjectoftheverbj.CalculatingtheMutualInformationbetweenthesubjectsvariable(S)andverbsvari-able(V)inthistable,wegetMI(S,V)=0.52bits.SupposewewishtoclusterthesubjectnounsintotwoclusterswhilepreservingthehighestMutualInformationwithregardtotheverbs.Thefollowingco-occurrencematrixistheoptimalclustering,andretainsaM.I.valueof0.4bits(77%oforiginal):ClusteredS\Vtellscratchdrink{dog,cat}0108{John,man}10111Noticethatalthoughthevaluesinthedrinkcolumnarehigherthaninothers,andwemaybe450

temptedtoclustertogetherdogandJohnbasedonthiscolumn,theinformativenessofthisverbissmaller-ifweknowtheverbistellwecanbesurethenounisnotdogorcat,whereasifweknowitisdrink,wecanonlysayitisslightlymoreprobablethatthenounisJohnordog.Ourdependencystructureconsistsofthreevariables:subject,verb,andobject,andwetakeadvantageofthesubject-verbandverb-objectdependenciesinourclustering.Theclusteringwasperformedoneachvariableseparately,inatwophaseprocedure(seeFigure2).Intheﬁrststage,weclusteredthesubjectvariableinto200clusters1,usingthesubject-verbdependency(i.e.theverbvariablewasthetarget).Thesamewasdonewiththeobjectvariable,usingtheverb-objectdependency.Inthesecondphase,wewishtoclustertheverbvalueswithregardtoboththesubjectandobjectvariables.Wecouldnotuseallpairsofsubjectsandobjectsvaluesasthetargetvariableinthistask,sincetoomanysuchcombinationsexist.Instead,weusedavari-ablecomposedofallthepairsofsubjectandob-jectclustersasthetargetfortheverbclustering.Inthisfashionweproduced100verbclusters.Figure2:Thetwoclusteringphases.Arrowsrep-resentdependenciesbetweenthevariableswhichareusedintheclustering.CombiningtheModelElements:Havingobtainedourthreeclusteredvariables,ourorig-inalsimpliﬁedclausetripletcannowbeusedtoproducetheClusteredClausemodel.Thismodelrepresentsaclauseinthedatabyatripletofclusterindexes,oneclusterindexforeachclusteredvariable.Inordertomapaclausein1Thechosennumbersofclustersaresuchthateachtheresultingclusteredvariablespreservedapproximatelyhalfoftheco-occurrencemutualinformationthatexistedbetweentheoriginal(unclustered)variableanditstarget.thetexttoitscorrespondingclusteredclause,itisﬁrstparsedandlemmatizedtoobtainthesubject,verbandobjectvalues,asdescribedabove,andthenassignedtotheclusteredclauseinwhichthesubjectclusterindexisthatoftheclustercontainingthesubjectwordoftheclause,andthesamefortheverbandobjectwords.Forexample,thesentence“Theterroristthrewthegrenade”wouldbeconvertedtothetriplet(terrorist,throw,grenade)andassignedtotheclusteredclausecomposedofthethreeclusterstowhichthesewordsbelong.Othertripletsassignedtothisclusteredclausemightinclude(fundamentalist,throw,bomb)or(mil-itant,toss,explosive).Applyingthisproceduretotheentiretextcorpusresultsinadistilla-tionofthetextintoaseriesofclusteredclausescontainingtheessentialinformationabouttheactionsdescribedinthetext.TechnicalSpeciﬁcations:ForthisworkwechosetousetheentireReutersCorpus(En-glish,release2000),containing800,000newsarticlescollecteduniformlyfrom20/8/1996to19/8/1997.Beforeclustering,severalprepro-cessingstepsweretaken.WehadaverylargeamountofwordvaluesforeachoftheSub-ject(85,563),Verb(4,593)andObject(74,842)grammaticalcategories.Manyofthewordswereinfrequentpropernounsorrareverbsandwereoflittleinterestinthepatternrecognitiontask.Wethereforeremovedthelessfrequentwords-thoseappearingintheircategorylessthanonehundredtimes.Wealsocleanedourdatabyremovingallwordsthatwereoneletterinlength,otherthantheword‘I’.Theseweremostlyinitialsinnamesofpeopleorcompa-nies,whichwereuninformativewithoutthesur-roundingcontext.Thisprocessingstepbroughtustotheﬁnalcountof2,874,763clausetriplets(75.8%oftheoriginalnumber),containing3,153distinctsubjects,1,716distinctverbs,and3,312distinctobjects.Thesevalueswereclusteredasdescribedabove.Theclusterswereusedtocon-vertthesimpliﬁedclausesintoclusteredclauses.3EvaluatingClusterQualityExamplesofsomeoftheresultingclustersareprovidedinTable1.Whenmanuallyexamin-451

“TechnicalDevelopements”(SubjectCluster160):treatment,drug,method,tactic,version,technology,software,design,device,vaccine,ending,tool,mechanism,technique,instrument,therapy,concept,model“Ideals/Virtues”(ObjectCluster14):sovereignty,dominance,logic,validity,legitimacy,freedom,discipline,viability,referendum,wisdom,innocence,credential,integrity,independence“EmphasisVerbs”(VerbCluster92):im-ply,signify,highlight,mirror,exacerbate,mark,sig-nal,underscore,compound,precipitate,mask,illus-trate,herald,reinforce,suggest,underline,aggra-vate,reﬂect,demonstrate,spell,indicate,deepen“Plans”(ObjectCluster33):journey,ar-rangement,trip,eﬀort,attempt,revolution,pull-out,handover,sweep,preparation,ﬁling,start,play,repatriation,redeployment,landing,visit,push,transition,processTable1:Exampleclusters(labeledmanually).ingtheclusters,wenoticedthe“ﬁne-tuning”ofsomeoftheclusters.Forinstance,wehadaclusterofcountriesinvolvedinmilitarycon-ﬂicts,andanotherforothercountries;aclusterforwinninggamescores,andanotherforties;etc.Thefactthatthealgorithmseparatedtheseclustersindicatesthatthedistinctionbetweenthemisimportantwithregardtotheinterac-tionswithintheclause.Forinstance,intheﬁrstexample,thecontextinwhichcountriesfromtheﬁrstclusterappearisverydiﬀerentfromthatin-volvingcountriesinthesecondcluster.Theeﬀectofthedependenciesweuseisalsostronglyfelt.Manyclusterscanbedescribedbylabelssuchas“thingsthatarethrown”(rock,ﬂower,bottle,grenadeandothers),or“verbsdescribingattacks”(spearhead,foil,intensify,mount,repulseandothers).Whilesuchcrite-riamaynotbetheﬁrstchoiceofsomeonewhoisaskedtoclusterverbsornouns,theyrepre-sentunifyingthemeswhichareveryappropri-atetopatterndetectiontasks,inwhichwewishtodetectconnectionsbetweenactionsdescribedintheclauses.Forinstance,wewouldliketodetecttherelationbetweenthrowingandmil-itary/policeaction(muchofthethrowingde-scribedinthenewsreportsﬁtsthisrelation).Inordertodothis,wemusthaveclusterswhichunitethewordsrelevanttothoseactions.Othercriteriaforclusteringwouldmostlikelynotbesuitable,sincetheywouldprobablynotputegg,bottleandrockinthesamecategory.Inthisre-spect,ourclusteringmethodprovidesamoreeﬀectivemodelingofthedomainknowledge.3.1EvaluationviaSemanticResourceSincethesuccessofourpatterndetectiontaskdependstoalargeextentonthequalityofourclusters,weperformedanexperimentdesignedtoevaluatesemanticsimilaritybetweenmem-bersofourclusters.ForthispurposewemadeuseoftheWordNetSimilaritypackage(Peder-senetal.,2004).Thispackagecontainsmanysimilaritymeasures,andweselectedthreeofthem(Resnik(1995),LeacockandChodorow(1997),HirstandSt-Onge(1997)),whichmakeuseofdiﬀerentaspectsofWordNet(hierarchyandgraphstructure).Wemeasuredtheaveragepairwisesimilaritybetweenanytwowordsap-pearinginthesamecluster.Wethenperformedthesamecalculationonarandomgroupingofthewords,andcomparedthetwoscores.There-sults(Fig.3)showthatourclustering,basedonco-occurrencestatisticsanddependencieswithinthesentence,correlateswithapurelysemanticsimilarityasrepresentedbytheWordNetstruc-ture,andcannotbeattributedtochance.Figure3:Inter-clustersimilarity(averagepair-wisesimilaritybetweenclustermembers)inourclustering(light)andarandomone(dark).Ran-domclusteringwasperformed10times.Aver-agevaluesareshownwitherrorbarstoindicatestandarddeviation.OnlyHirst&St-Ongemea-sureverbsimilarity.4RelationDetectionTaskMotivation:Inordertodemonstratetheuseofourmodel,wechosearelationdetectiontask.Theworkshoponentailmentmentionedintheintroductionwasmainlyfocusedondetectingwhetherornotanentailmentrelationexistsbe-tweentwotexts.Inthisworkwepresentacom-452

plementaryapproach-amethoddesignedtoau-tomaticallydetectrelationsbetweenportionsoftextandgenerateaknowledgebaseofthede-tectedrelationsinageneralizedform.Asstatedby(Daganetal.,2005),suchrelationsareim-portantforIRapplications.Inaddition,thepat-ternsweemployarelikelytobeusefulinotherlinguistictasksinvolvingwholeclauses,suchasparaphraseacquisition.PatternDeﬁnition:Forourrelationdetec-tiontask,wesearchedforinstancesofprede-ﬁnedpatternsindicatingarelationbetweentwoclusteredclauses.Werestrictedthesearchtoclausepairswhichco-occurwithinadistanceoftenclauses2fromeachother.Inadditiontothedistancerestriction,werequiredananchor:anounthatappearsinbothclauses,tofurtherstrengthentherelationbetweenthem.Nounan-chorsestablishthefactthatthetwocompo-nentactionsdescribedbythepatterninvolvethesameentities,implyingadirectconnectionbe-tweenthem.Theuseofverbanchorswasalsotested,butfoundtobelesshelpfulindetect-ingsigniﬁcantpatterns,sinceinmostcasesitsimplyfoundverbswhichtendtorepeatthem-selvesfrequentlyinacontext.Themethodwedescribeassumesthatstatisticallysigniﬁcantco-occurrencesindicatearelationshipbetweentheclauses,butdoesnotattempttodeterminethetypeofrelation.SigniﬁcanceCalculation:Thepatternsde-tectedbythesystemwerescoredusingthesta-tisticalp-valuemeasure.Thisvaluerepresentstheprobabilityofdetectingacertainnumberofoccurrencesofagivenpatterninthedataundertheindependenceassumption,i.e.assum-ingthereisnoconnectionbetweenthetwohalvesofthepattern.Ifthesystemhasdetectedkinstancesofacertainpattern,wecalculatetheprobabilityofencounteringthisnumberofinstancesundertheindependenceassumption.Thesmallertheprobability,thehigherthesig-niﬁcance.Weconsiderpatternswithachanceprobabilitylowerthan5%tobesigniﬁcant.WeassumeaGaussian-likedistributionofoc-2Ourexperimentsshowedthatincreasingthedistancebeyondthispointdidnotresultinsigniﬁcantincreaseinthenumberofdetectedpatterns.currenceprobabilityforeachpattern3.Inor-dertoestimatethemeanandstandarddevia-tionvalues,wecreated100simulatedsequencesoftriplets(representingclusteredclauses)whichwereindependentlydistributedandvariedonlyintheiroverallprobabilityofoccurrence.Wethenestimatedthemeanandstandarddevia-tionforanypairofclausesintheactualdatausingthesimulatedsequences.(X,VC36,OC7)→10(X,VC57,OC85)storm,lash,province...storm,cross,Cubaquake,shake,city...quake,hit,Iranearthquake,jolt,city...earthquake,hit,Iran(X,VC40,OC165)→10(X,VC52,OC152)police,arrest,leader...police,search,mosquepolice,detain,leader...police,search,mosquepolice,arrest,member...police,raid,enclave(SC39,VC21,X)→10(X,beat4,OC155)sun,report,earnings...earnings,beat,expectationxerox,report,earnings...earnings,beat,forecastmicrosoft,release,result...result,beat,forecast(X,VC57,OC7)→10(X,cause4,OC153)storm,hit,coast...storm,cause,damagecyclone,near,coast...cyclone,cause,damageearthquake,hit,northwest...earthquake,cause,damagequake,hit,northwest...quake,cause,casualtyearthquake,hit,city...earthquake,cause,damageTable2:ExamplePatterns4.1PatternDetectionResultsInTable2wepresentseveralexamplesofhighranking(i.e.signiﬁcance)patternswithdiﬀerentanchoringsdetectedbyourmethod.Thedetectedpatternsarerepresentedusingthenotationoftheform(SCi,VCj,X)→n(X,VCi0,OCj0).Xindicatestheanchoringword.Intheexamplenotation,theanchoringwordistheobjectoftheﬁrstclauseandthesubjectofthesecond(O-Sforshort).nindicatesthemaximaldistancebetweenthetwoclauses.ThetermsSC,VCorOCwithasubscriptedindexrepresenttheclustercontainingthesub-ject,verborobject(respectively)oftheappro-priateclause.Forinstance,intheﬁrstexampleinTable2,VC36indicatesverbclusterno.36,containingtheverbslash,shakeandjolt,amongothers.3BasedonGwaderaetal.(2003),dealingwithasim-ilar,thoughsimpler,case.4Intwoofthepatterns,insteadofaclusterfortheverb,wehaveasingleword-beatorcause.Thisistheresultofanautomaticpost-processingstageintendedtopreventover-generalization.Ifalltheinstancesofthepat-453

AnchoringNumberofSystemPatternsFoundSubject-Subject428Object-Object291Subject-Object180Object-Subject178Table3:Numbersofpatternsfound(p<5%)Table3liststhenumberofpatternsfound,foreachanchoringsystem.Thediﬀerentanchor-ingsystemsproducequantitativelydiﬀerentre-sults.Anchoringbetweenthesamecategoriesproducesmorepatternsthanbetweenthesamenounindiﬀerentgrammaticalroles.Thisisex-pected,sincemanynounscanonlyplayacertainpartintheclause(forinstance,manyverbscan-nothaveaninanimateentityastheirsubject).Thenumberofinstancesofpatternswefoundfortheanchoredtemplatemightbeconsideredlow,anditislikelythatsomepatternsweremissedsimplybecausetheiroccurrenceproba-bilitywasverylowandnotenoughinstancesofthepatternoccurredinthetext.InSection4westatedthatinthistask,weweremoreinterestedinprecisionthaninrecall.Inordertodetectawiderrangeofpatterns,alessrestricteddeﬁni-tionofthepatterns,oradiﬀerentsigniﬁcanceindicator,shouldbeused(seeSec.6).HumanEvaluation:Inordertobetterde-terminethequalityofpatternsdetectedbyoursystem,andconﬁrmthatthestatisticalsignif-icancetestingisconsistentwithhumanjudg-ment,weperformedanevaluationexperimentwiththehelpof22humanjudges.Wepresentedeachofthejudgeswith60examplegroups,15foreachtypeofanchoring.Eachexamplegroupcontainedthreeclausepairsconformingtotheanchoringrelation.Theclauseswerepresentedinanormalizedformconsistingonlyofasub-ject,objectandverbconvertedtopasttense,withtheadditionofnecessarydeterminersandprepositions.Forexample,thetriplet(police,detain,leader)wasconvertedto“Thepolicede-tainedtheleader”.Inhalfthecases(randomlyterninthetextcontainedthesamewordinacertainpo-sition(intheseexamples-theverbpositioninthesecondclause),thiswordwasplacedinthatpositioninthegen-eralizedpattern,ratherthantheclusteritbelongedto.Sincewehavenoevidenceforthefactthatotherwordsintheclustercanﬁtthatposition,usingtheclusterin-dicatorwouldbeover-generalizing.selected),theseclausepairswereactualexam-ples(instances)ofapatterndetectedbyoursys-tem(instancesgroup),suchasthoseappearinginTable2.Intheotherhalf,welistedthreeclausepairs,eachofwhichconformedtotheanchoringspeciﬁcationlistedinSection4,butwhichwererandomlysampledfromthedata,andsohadnoconnectiontooneanother(base-linegroup).Weaskedthejudgestorateonascaleof1-5whethertheythoughttheclausepairswereagoodsetofexamplesofacommonrelationlinkingtheﬁrstclauseineachpairtothesecondone.InstancesInstancesBaselineBaselineScoreStdDevScoreStdDevAll3.54610.47802.63410.4244O-S3.92660.60582.87610.5096O-O3.49380.51442.74640.6205S-O3.47460.73402.57580.6314S-S3.23980.48922.35840.5645Table4:ResultsforhumanevaluationTable4reportstheoverallaveragescoresforbaselineandinstancesgroups,andforeachofthefouranchoringtypesindividually.Thescoreswereaveragedoverallexamplesandalljudges.AnANOVAshowedthediﬀerenceinscoresbe-tweenthebaselineandinstancegroupstobesigniﬁcant(p<0.001)inallfourcases.AchievementofModelGoals:Weem-ployedclusteringinourmodeltoovercomedata-sparseness.Theimportanceofthisdecisionwasevidentinourresults.Forexample,thesecondpatternshowninTable2appearedonlyfourtimesinthetext.Intheseinstances,verbcluster40wasrepresentedtwicebytheverbarrestandtwicebydetain.Twoappearancesarewithinthestatisticaldeviationofallbuttherarestwords,andwouldnothavebeendetectedassigniﬁcantwithouttheclusteringeﬀect.Thismeansthepatternwouldhavebeenoverlooked,despitethestronglyintuitiveconnectionitrepresents.Thesystemdetectedseveralsuchpatterns.Theotherreasonforclusteringwasgeneral-ization.Evenincaseswherepatternsinvolvingsinglewordscouldhavebeendetected,itwouldhavebeenimpossibletounifysimilarpatternsintogeneralizedones.Inaddition,whenencoun-teringanewclausewhichdiﬀersslightlyfrom454

theoneswerecognizedintheoriginaldata,therewouldbenowaytorecognizeitanddrawtheap-propriateconclusions.Forexample,therewouldbenowaytorelatethesentence“Thetyphoonapproachedthecoast”tothefourthexamplepat-tern,andtheconnectionwiththeresultingdam-agewouldnotberecognized.5ComparisonwithPreviousWorkTherelationshipbetweentextualfeaturesandsemanticsandtheuseofsyntaxasanindica-torofsemanticshasbeenwidespread.FollowingtheideaproposedinHarris’DistributionalHy-pothesis(Harris,1985),thatwordsoccurringinsimilarcontextsaresemanticallysimilar,manyworkshaveuseddiﬀerentdeﬁnitionsofcontexttoidentifyvarioustypesofsemanticsimilarity.Hindle(1990)usesamutual-informationbasedmetricderivedfromthedistributionofsubject,verbandobjectinalargecorpustoclassifynouns.Pereiraetal.(1993)clusternounsac-cordingtotheirdistributionasdirectobjectsofverbs,usinginformation-theoretictools(thepredecessorsofthetoolsweuseinthiswork).Theysuggestthatinformationtheoreticmea-surescanalsomeasuresemanticrelatedness.Theseworksfocusonlyonrelatednessofindi-vidualwordsanddonotdescribehowtheau-tomaticestimationofsemanticsimilaritycanbeusefulinreal-worldtasks.Inourworkwedemonstratethatusingclustersasgeneralizedwordunitshelpsovercomethesparsenessandgeneralizationproblemstypicallyencounteredwhenattemptingtoextracthigh-levelpatternsfromtext,asrequiredformanyapplications.TheDIRTsystem(LinandPantel,2001)dealswithinferencerules,andemploystheno-tionofpathsbetweentwonounsinasentence’sparsetree.Thesystemextractssuchpathstruc-turesfromtext,andprovidesasimilaritymea-surebetweentwosuchpathsbycomparingthewordswhichﬁllthesameslotsinthetwopaths.Afterextractingthepaths,thesystemﬁndsgroupsofsimilarpaths.Thisapproachbearsseveralsimilaritiestotheideasdescribedinthispaper,sinceourstructurecanbeseenasaspeciﬁcpathintheparsetree(probablythemostbasicone,seeFig.1).Inoursetup,sim-ilarclausesareclusteredtogetherinthesameClustered-Clause,whichcouldbecomparedtoclusteringDIRT’spathsusingitssimilaritymea-sure.Despitethesesimilarities,thereareseveralimportantdiﬀerencesbetweenthetwosystems.Ourmethodusesonlytherelationshipsinsidethepathorclauseintheclusteringprocedure,sothesimilarityisbasedonthestructureit-self.Furthermore,LinandPanteldidnotcreatepathclustersorgeneralizedpaths,sothatwhiletheirmethodallowedthemtocomparephrasesforsimilarity,thereisnoconvenientwaytoiden-tifyhighlevelcontextualrelationshipsbetweentwonearbysentences.Thisisoneofthesigniﬁ-cantadvantagesthatclusteringhasoversimilar-itymeasures-itallowsagroupofsimilarobjectstoberepresentedbyasingleunit.Therehavebeenseveralattemptstoformu-lateanddetectrelationshipsatahighercontextlevel.TheVerbOceanproject(ChklovskiandPantel,2004)dealswithrelationsbetweenverbs.Itpresentsanautomaticallyacquirednetworkofsuchrelations,similartotheWordNetframe-work.Thoughthepatternsusedtoacquiretherelationsareusuallypartsofasinglesentence,therelationshipsthemselvescanalsobeusedtodescribeconnectionsbetweendiﬀerentsen-tences,especiallytheenablementandhappens-beforerelations.Sinceverbsarethecentralpartoftheclause,VerbOceancanbeviewedasde-tectingrelationsbetweenclausesaswholeunits,aswellasthosebetweenindividualwords.Asasolutiontothedatasparsenessproblem,webqueriesareused.Torisawa(2006)addressesasimilarproblem,butfocusesontemporalre-lations,andmakesuseofthephenomenaofJapanesecoordinatesentences.Neitheroftheseapproachesattempttocreategeneralizedrela-tionsorgroupverbsintoclusters,thoughinthecaseofVerbOceanthiscouldpresumablybedoneusingthesimilarityandstrengthvalueswhicharedeﬁnedanddetectedbythesystem.6FutureWorkTheclusteredclausemodelpresentsmanydi-rectionsforfurtherresearch.Itmaybeproduc-tivetoextendthemodelfurther,andincludeotherpartsofthesentence,suchasadjectives455

andadverbs.Clusteringnounsbytheadjectivesthatdescribethemmayprovideamoreintu-itivegrouping.Theadditionoffurtherelementstothestructuremayalsoallowthedetectionofnewkindsofrelations.Thenews-orienteddomainofthecorpusweusedstronglyinﬂuencedourresults.Ifwewereinterestedinmoremundanerelations,involvingday-to-dayactionsofindividuals,aliterarycor-puswouldprobablybemoresuitable.Indeﬁningourpatterntemplate,severalele-mentsweretailoredspeciﬁcallytoourtask.Welimitedourselvestoaveryrestrictedsetofpat-ternsinordertobetterdemonstratetheeﬀec-tivenessofourmodel.Foramoregeneralknowl-edgeacquisitiontask,severaloftheserestric-tionsmayberelaxed,allowingamuchlargersetofrelationstobedetected.Forexample,alessstrictsigniﬁcanceﬁlter,suchasthesupportandconﬁdencemeasurescommonlyusedindatamining,maybepreferable.Thesecanbesettodiﬀerentthresholds,accordingtotheuser’spref-erence.Inourcurrentwork,inordertopreventover-generalization,weemployedasinglesteppost-processingalgorithmwhichdetectedtheincor-rectuseofanentireclusterinplaceofasingleword(seefootnoteforTable2).Thismethodallowsonlytwolevelsofgeneralization-sin-glewordsandwholeclusters.Amoreappro-priatewaytohandlegeneralizationwouldbetouseahierarchicalclusteringalgorithm.TheAgglomerativeInformationBottleneck(SlonimandTishby,1999)isanexampleofsuchanal-gorithm,andcouldbeemployedforthistask.Useofahierarchicalmethodwouldresultinseverallevelsofclusters,representingdiﬀerentlevelsofgeneralization.Itwouldberelativelyeasytomodifyourproceduretoreducegeneral-izationtothelevelindicatedbythepatternex-amplesinthetext,producingamoreaccuratedescriptionofthepatternsdetected.AcknowledgmentsTheauthoracknowledgesthesupportofEPSRCgrantEP/C538447/1.TheauthorwouldliketothankNaftaliTishbyandMirellaLapatafortheirsupervisionandas-sistanceonlargeportionsoftheworkpresentedhere.Iwouldalsoliketothanktheanonymousreviewersandmyfriendsandcolleaguesfortheirhelpfulcomments.ReferencesBrody,Samuel.2005.Cluster-BasedPatternRecognitioninNaturalLanguageText.Master’sthesis,HebrewUniversity,Jerusalem,Israel.Chklovski,T.andP.Pantel.2004.Verbocean:Miningthewebforﬁne-grainedsemanticverbrelations.InProc.ofEMNLP.pages33–40.Dagan,I.,O.Glickman,andB.Magnini.2005.Thepascalrecognisingtextualentailmentchallenge.InProceedingsofthePASCALChallengesWorkshoponRecognisingTextualEntailment.Fellbaum,Christiane,editor.1998.WordNet:AnElec-tronicDatabase.MITPress,Cambridge,MA.Gwadera,R.,M.Atallah,andW.Szpankowski.2003.Reliabledetectionofepisodesineventsequences.InICDM.Harris,Z.1985.Distributionalstructure.Katz,J.J.(ed.)ThePhilosophyofLinguisticspages26–47.Hindle,Donald.1990.Nounclassiﬁcationfrompredicate-argumentstructures.InMeetingoftheACL.pages268–275.Hirst,G.andD.St-Onge.1997.Lexicalchainsasrepre-sentationofcontextforthedetectionandcorrectionofmalapropisms.InWordNet:AnElectronicLexicalDatabase,ed.,ChristianeFellbaum.MITPress.Leacock,C.andM.Chodorow.1997.Combininglocalcontextandwordnetsimilarityforwordsenseidenti-ﬁcation.InWordNet:AnElectronicLexicalDatabase,ed.,ChristianeFellbaum.MITPress.Lin,Dekang.1994.Principar-aneﬃcient,broad-coverage,principle-basedparser.InCOLING.pages482–488.Lin,DekangandPatrickPantel.2001.DIRT-discoveryofinferencerulesfromtext.InKnowledgeDiscoveryandDataMining.pages323–328.Pedersen,T.,S.Patwardhan,andJ.Michelizzi.2004.Wordnet::similarity-measuringtherelatednessofcon-cepts.InProc.ofAAAI-04.Pereira,F.,N.Tishby,andL.Lee.1993.Distributionalclusteringofenglishwords.InMeetingoftheAssoci-ationforComputationalLinguistics.pages183–190.Resnik,Philip.1995.Usinginformationcontenttoeval-uatesemanticsimilarityinataxonomy.InIJCAI.pages448–453.Slonim,N.,N.Friedman,andN.Tishby.2002.Unsu-perviseddocumentclassiﬁcationusingsequentialin-formationmaximization.InProc.ofSIGIR’02.Slonim,N.andN.Tishby.1999.Agglomerativeinforma-tionbottleneck.InProc.ofNIPS-12.Torisawa,Kentaro.2006.Acquiringinferenceruleswithtemporalconstraintsbyusingjapanesecoordinatedsentencesandnoun-verbco-occurrences.InProceed-ingsofNAACL.pages57–64.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 456–463,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

456

Instance-basedEvaluationofEntailmentRuleAcquisitionIdanSzpektor,EyalShnarch,IdoDaganDept.ofComputerScienceBarIlanUniversityRamatGan,Israel{szpekti,shey,dagan}@cs.biu.ac.ilAbstractObtaininglargevolumesofinferenceknowl-edge,suchasentailmentrules,hasbecomeamajorfactorinachievingrobustseman-ticprocessing.Whiletherehasbeensub-stantialresearchonlearningalgorithmsforsuchknowledge,theirevaluationmethod-ologyhasbeenproblematic,hinderingfur-therresearch.Weproposeanovelevalua-tionmethodologyforentailmentruleswhichexplicitlyaddressestheirsemanticproper-tiesandyieldssatisfactoryhumanagreementlevels.Themethodologyisusedtocomparetwostateoftheartlearningalgorithms,ex-posingcriticalissuesforfutureprogress.1IntroductionInmanyNLPapplications,suchasQuestionAn-swering(QA)andInformationExtraction(IE),itiscrucialtorecognizethataparticulartargetmean-ingcanbeinferredfromdifferenttextvariants.Forexample,aQAsystemneedstoidentifythat“As-pirinlowerstheriskofheartattacks”canbeinferredfrom“Aspirinpreventsheartattacks”inordertoan-swerthequestion“Whatlowerstheriskofheartat-tacks?”.Thistypeofreasoninghasbeenrecognizedasacoresemanticinferencetaskbythegenerictex-tualentailmentframework(Daganetal.,2006).Amajorobstacleforfurtherprogressinseman-ticinferenceisthelackofbroad-scaleknowledge-basesforsemanticvariabilitypatterns(Bar-Haimetal.,2006).Oneprominenttypeofinferenceknowl-edgerepresentationisinferencerulessuchaspara-phrasesandentailmentrules.Wedeﬁneanentail-mentruletobeadirectionalrelationbetweentwotemplates,textpatternswithvariables,e.g.‘Xpre-ventY→XlowertheriskofY’.Theleft-hand-sidetemplateisassumedtoentailtheright-hand-sidetemplateincertaincontexts,underthesamevariableinstantiation.Paraphrasescanbeviewedasbidirectionalentailmentrules.Suchrulescapturebasicinferencesandareusedasbuildingblocksformorecomplexentailmentinference.Forexample,giventheaboverule,theanswer“Aspirin”canbeidentiﬁedintheexampleabove.Theneedforlarge-scaleinferenceknowledge-basestriggeredextensiveresearchonautomaticac-quisitionofparaphraseandentailmentrules.Yetthecurrentprecisionofacquisitionalgorithmsistyp-icallystillmediocre,asillustratedinTable1forDIRT(LinandPantel,2001)andTEASE(Szpek-toretal.,2004),twoprominentacquisitionalgo-rithmswhoseoutputsarepubliclyavailable.Thecurrentperformancelevelonlystressestheobviousneedforsatisfactoryevaluationmethodologiesthatwoulddrivefutureresearch.Theprominentapproachintheliteratureforeval-uatingrules,termedheretherule-basedapproach,istopresenttherulestohumanjudgesaskingwhethereachruleiscorrectornot.However,itisdifﬁculttoexplicitlydeﬁnewhenalearnedruleshouldbecon-sideredcorrectunderthismethodology,andthiswasmainlyleftundeﬁnedinpreviousworks.Asthecri-terionforevaluatingaruleisnotwelldeﬁned,usingthisapproachoftencausedlowagreementbetweenhumanjudges.Indeed,thestandardsforevaluationinthisﬁeldarelowerthanotherﬁelds:manypapers457

don’treportonhumanagreementatallandthosethatdoreportratherlowagreementlevels.Yetitiscrucialtoreliablyassessrulecorrectnessinor-dertomeasureandcomparetheperformanceofdif-ferentalgorithmsinareplicablemanner.Lackingagoodevaluationmethodologyhasbecomeabarrierforfurtheradvancesintheﬁeld.Inordertoprovideawell-deﬁnedevaluationmethodologyweﬁrstexplicitlyspecifywhenentail-mentrulesshouldbeconsideredcorrect,followingthespiritoftheirusageinapplications.Wethenproposeanewinstance-basedevaluationapproach.Underthisscheme,judgesarenotpresentedonlywiththerulebutratherwithasampleofsentencesthatmatchitslefthandside.Thejudgesthenassesswhethertheruleholdsundereachspeciﬁcexample.Aruleisconsideredcorrectonlyifthepercentageofexamplesassessedascorrectissufﬁcientlyhigh.WehaveexperimentedwithasampleofinputverbsforbothDIRTandTEASE.Ourresultsshowsigniﬁcantimprovementinhumanagreementovertherule-basedapproach.Itisalsotheﬁrstcompar-isonbetweensuchtwostate-of-the-artalgorithms,whichshowedthattheyarecomparableinprecisionbutlargelycomplementaryintheircoverage.Additionally,theevaluationshowedthatbothal-gorithmslearnmostlyone-directionalrulesratherthan(symmetric)paraphrases.WhilemostNLPap-plicationsneeddirectionalinference,previousac-quisitionworkstypicallyexpectedthatthelearnedruleswouldbeparaphrases.Undersuchanexpec-tation,unidirectionalruleswereassessedasincor-rect,underestimatingthetruepotentialofthesealgo-rithms.Inaddition,weobservedthatmanylearnedrulesarecontextsensitive,stressingtheneedtolearncontextualconstraintsforruleapplications.2Background:EntailmentRulesandtheirEvaluation2.1EntailmentRulesAnentailmentrule‘L→R’isadirectionalrela-tionbetweentwotemplates,LandR.Forexam-ple,‘XacquireY→XownY’or‘XbeatY→XplayagainstY’.Templatescorrespondtotextfragmentswithvariables,andaretypicallyeitherlin-earphrasesorparsesub-trees.Thegoalofentailmentrulesistohelpapplica-InputCorrectIncorrect(↔)XmodifyYXadoptYXchangeY(←)XamendYXcreateY(DIRT)(←)XreviseYXsticktoY(↔)XalterYXmaintainYXchangeY(→)XaffectYXfollowY(TEASE)(←)XextendYXuseYTable1:ExamplesoftemplatessuggestedbyDIRTandTEASEashavinganentailmentrelation,insomedirection,withtheinputtemplate‘XchangeY’.Theentailmentdirectionarrowswerejudgedmanuallyandaddedforreadability.tionsinferonetextvariantfromanother.ArulecanbeappliedtoagiventextonlywhenLcanbein-ferredfromit,withappropriatevariableinstantia-tion.Then,usingtherule,theapplicationdeducesthatRcanalsobeinferredfromthetextunderthesamevariableinstantiation.Forexample,therule‘XlosetoY→YbeatX’canbeusedtoinfer“Liv-erpoolbeatChelsea”from“ChelsealosttoLiver-poolinthesemiﬁnals”.Entailmentrulesshouldtypicallybeappliedonlyinspeciﬁccontexts,whichwetermrelevantcon-texts.Forexample,therule‘XacquireY→XbuyY’canbeusedinthecontextof‘buying’events.However,itshouldn’tbeappliedfor“Stu-dentsacquiredanewlanguage”.Inthesameman-ner,therule‘XacquireY→XlearnY’shouldbeappliedonlywhenYcorrespondstosomesortofknowledge,asinthelatterexample.Someexistingentailmentacquisitionalgorithmscanaddcontextualconstraintstothelearnedrules(Sekine,2005),butmostdon’t.However,NLPap-plicationsusuallyimplicitlyincorporatesomecon-textualconstraintswhenapplyingarule.Forex-ample,whenansweringthequestion“Whichcom-paniesdidIBMbuy?”aQAsystemwouldapplytherule‘XacquireY→XbuyY’correctly,sincethephrase“IBMacquireX”islikelytobefoundmostlyinrelevanteconomiccontexts.Wethusex-pectthatanevaluationmethodologyshouldconsidercontextrelevanceforentailmentrules.Forexample,wewouldlikeboth‘XacquireY→XbuyY’and‘XacquireY→XlearnY’tobeassessedascor-rect(thesecondruleshouldnotbedeemedincorrect458

justbecauseitisnotapplicableinfrequenteconomiccontexts).Finally,wehighlightthatthecommonnotionof“paraphraserules”canbeviewedasaspecialcaseofentailmentrules:aparaphrase‘L↔R’holdsifbothtemplatesentaileachother.Followingthetex-tualentailmentformulation,weobservethatmanyappliedinferencesettingsrequireonlydirectionalentailment,andarequirementforsymmetricpara-phraseisusuallyunnecessary.Forexample,inor-dertoanswerthequestion“WhoownsOverture?”itsufﬁcestouseadirectionalentailmentrulewhoserighthandsideis‘XownY’,suchas‘XacquireY→XownY’,whichisclearlynotaparaphrase.2.2EvaluationofAcquisitionAlgorithmsManymethodsforautomaticacquisitionofruleshavebeensuggestedinrecentyears,rangingfromdistributionalsimilaritytoﬁndingsharedcontexts(LinandPantel,2001;RavichandranandHovy,2002;Shinyamaetal.,2002;BarzilayandLee,2003;Szpektoretal.,2004;Sekine,2005).How-ever,thereisstillnocommonacceptedframeworkfortheirevaluation.Furthermore,allthesemethodslearnrulesaspairsoftemplates{L,R}inasym-metricmanner,withoutaddressingruledirectional-ity.Accordingly,previousworks(except(Szpektoretal.,2004))evaluatedthelearnedrulesundertheparaphrasecriterion,whichunderestimatestheprac-ticalutilityofthelearnedrules(seeSection2.1).Oneapproachwhichwasusedforevaluatingau-tomaticallyacquiredrulesistomeasuretheircontri-butiontotheperformanceofspeciﬁcsystems,suchasQA(RavichandranandHovy,2002)orIE(Sudoetal.,2003;Romanoetal.,2006).Whilemeasuringtheimpactoflearnedrulesonapplicationsishighlyimportant,itcannotserveastheprimaryapproachforevaluatingacquisitionalgorithmsforseveralrea-sons.First,developersofacquisitionalgorithmsof-tendonothaveaccesstothedifferentapplicationsthatwilllaterusethelearnedrulesasgenericmod-ules.Second,thelearnedrulesmayaffectindividualsystemsdifferently,thusmakingobservationsthatarebasedondifferentsystemsincomparable.Third,withinacomplexsystemitisdifﬁculttoassesstheexactqualityofentailmentrulesindependentlyofeffectsofothersystemcomponents.Thus,asinmanyotherNLPlearningsettings,adirectevaluationisneeded.Indeed,thepromi-nentapproachforevaluatingthequalityofruleac-quisitionalgorithmsisbyhumanjudgmentofthelearnedrules(LinandPantel,2001;Shinyamaetal.,2002;BarzilayandLee,2003;Pangetal.,2003;Szpektoretal.,2004;Sekine,2005).Inthisevalua-tionscheme,termedheretherule-basedapproach,asampleofthelearnedrulesispresentedtothejudgeswhoevaluatewhethereachruleiscorrectornot.Thecriterionforcorrectnessisnotexplicitlydescribedinmostpreviousworks.Bythecommonviewofcon-textrelevanceforrules(seeSection2.1),arulewasconsideredcorrectifthejudgecouldthinkofrea-sonablecontextsunderwhichitholds.Wehavereplicatedtherule-basedmethodologybutdidnotmanagetoreacha0.6Kappaagree-mentlevelbetweenpairsofjudges.Thisapproachturnsouttobeproblematicbecausetherulecorrect-nesscriterionisnotsufﬁcientlywelldeﬁnedandishardtoapply.Whilesomerulesmightobviouslybejudgedascorrectorincorrect(seeTable1),judg-mentisoftenmoredifﬁcultduetocontextrelevance.Onejudgemightcomeupwithacertaincontextthat,toheropinion,justiﬁestherule,whileanotherjudgemightnotimaginethatcontextorthinkthatitdoesn’tsufﬁcientlysupportrulecorrectness.Forexample,inourexperimentsoneofthejudgesdidnotidentifythevalid“religiousholidays”contextforthecorrectrule‘XobserveY→XcelebrateY’.Indeed,onlyfewearlierworksreportedinter-judgeagreementlevel,andthosethatdidreportedratherlowKappavalues,suchas0.54(BarzilayandLee,2003)and0.55-0.63(Szpektoretal.,2004).Toconclude,theprominentrule-basedmethodol-ogyforentailmentruleevaluationisnotsufﬁcientlywelldeﬁned.Itresultsinlowinter-judgeagreementwhichpreventsreliableandconsistentassessmentsofdifferentalgorithms.3Instance-basedEvaluationMethodologyAsdiscussedinSection2.1,anevaluationmethodol-ogyforentailmentrulesshouldreﬂecttheexpectedvalidityoftheirapplicationwithinNLPsystems.Followingthatline,anentailmentrule‘L→R’shouldberegardedascorrectifinall(oratleastmost)relevantcontextsinwhichtheinstantiatedtemplateLisinferredfromthegiventext,theinstan-459

RuleSentenceJudgment1XseekY→XdiscloseYIfheisarrested,hecanimmediatelyseekbail.Leftnotentailed2XclarifyY→XprepareYHedidn’tclarifyhispositiononthesubject.Leftnotentailed3XhitY→XapproachYOtherearthquakeshavehitLebanonsince’82.Irrelevantcontext4XloseY→XsurrenderYBreadhasrecentlylostitssubsidy.Irrelevantcontext5XregulateY→XreformYTheSRAregulatesthesaleofsugar.Noentailment6XresignY→XshareYLopezresignedhispostatVWlastweek.Noentailment7XsetY→XallowYThecommitteesetthefollowingrefunds.Entailmentholds8XstressY→XstateYBenYahiaalsostressedtheneedforaction.EntailmentholdsTable2:Ruleevaluationexamplesandtheirjudgment.tiatedtemplateRisalsoinferredfromthetext.Thisreasoningcorrespondstothecommondeﬁnitionofentailmentinsemantics,whichspeciﬁesthatatextLentailsanothertextRifRistrueineverycircum-stance(possibleworld)inwhichListrue(ChierchiaandMcConnell-Ginet,2000).Itfollowsthatinordertoassessifaruleiscor-rectweshouldjudgewhetherRistypicallyen-tailedfromthosesentencesthatentailL(withinrel-evantcontextsfortherule).Wethuspresentanewevaluationschemeforentailmentrules,termedtheinstance-basedapproach.Attheheartofthisap-proach,humanjudgesarepresentednotonlywitharulebutratherwithasampleofexamplesoftherule’susage.Insteadofthinkingupvalidcontextsfortherulethejudgesneedtoassesstherule’sva-lidityunderthegivencontextineachexample.Theessenceofourproposalisa(apparentlynon-trivial)protocolofasequenceofquestions,whichdeter-minesrulevalidityinagivensentence.Weshallnextdescribehowwecollectasampleofexamplesforevaluationandtheevaluationprocess.3.1SamplingExamplesGivenarule‘L→R’,ourgoalistogenerateevalua-tionexamplesbyﬁndingasampleofsentencesfromwhichLisentailed.Wedothatbyautomaticallyre-trieving,fromagivencorpus,sentencesthatmatchLandarethuslikelytoentailit,asexplainedbelow.Foreachexamplesentence,weautomaticallyex-tracttheargumentsthatinstantiateLandgeneratetwophrases,termedleftphraseandrightphrase,whichareconstructedbyinstantiatingthelefttem-plateLandtherighttemplateRwiththeextractedarguments.Forexample,theleftandrightphrasesgeneratedforexample1inTable2are“heseekbail”and“hedisclosebail”,respectively.FindingsentencesthatmatchLcanbeperformedatdifferentlevels.Inthispaperwematchlexical-syntactictemplatesbyﬁndingasub-treeofthesen-tenceparsethatisidenticaltothetemplatestructure.Ofcourse,thismatchingmethodisnotperfectandwillsometimesretrievesentencesthatdonotentailtheleftphraseforvariousreasons,suchasincorrectsentenceanalysisorsemanticaspectslikenegation,modalityandconditionals.Seeexamples1-2inTa-ble2forsentencesthatsyntacticallymatchLbutdonotentailtheinstantiatedleftphrase.SinceweshouldassessR’sentailmentonlyfromsentencesthatentailL,suchsentencesshouldbeignoredbytheevaluationprocess.3.2JudgmentQuestionsForeachexamplegeneratedforarule,thejudgesarepresentedwiththegivensentenceandtheleftandrightphrases.Theyprimarilyanswertwoquestionsthatassesswhetherentailmentholdsinthisexample,followingthesemanticsofentailmentruleapplica-tionasdiscussedabove:Qle:Istheleftphraseentailedfromthesentence?Apositive/negativeanswercorrespondstoa‘Leftentailed/notentailed’judgment.Qre:Istherightphraseentailedfromthesentence?Apositive/negativeanswercorrespondstoan‘Entailmentholds/Noentailment’judgment.Theﬁrstquestionidentiﬁessentencesthatdonoten-tailtheleftphrase,andthusshouldbeignoredwhenevaluatingtherule’scorrectness.Whileinappropri-atematchesoftheruleleft-hand-sidemayhappen460

andharmanoverallsystemprecision,sucherrorsshouldbeaccountedforasystem’srulematchingmoduleratherthanfortherules’precision.Thesec-ondquestionassesseswhethertheruleapplicationisvalidornotforthecurrentexample.Seeexamples5-8inTable2forcaseswhereentailmentdoesordoesn’thold.Thus,thejudgesfocusonlyonthegivensentenceineachexample,sothetaskisactuallytoevaluatewhethertextualentailmentholdsbetweenthesen-tence(text)andeachoftheleftandrightphrases(hypotheses).Followingpastexperienceintextualentailmentevaluation(Daganetal.,2006)weexpectareasonableagreementlevelbetweenjudges.AsdiscussedinSection2.1,wemaywanttoig-noreexampleswhosecontextisirrelevantfortherule.Tooptionallycapturethisdistinction,thejudgesareaskedanotherquestion:Qrc:IstherightphrasealikelyphraseinEnglish?Apositive/negativeanswercorrespondstoa‘Relevant/Irrelevantcontext’evaluation.IftherightphraseisnotlikelyinEnglishthenthegivencontextisprobablyirrelevantfortherule,be-causeitseemsinherentlyincorrecttoinferanim-plausiblephrase.Examples3-4inTable2demon-stratecasesofirrelevantcontexts,whichwemaychoosetoignorewhenassessingrulecorrectness.3.3EvaluationProcessForeachexample,thejudgesarepresentedwiththethreequestionsaboveinthefollowingorder:(1)Qle(2)Qrc(3)Qre.Iftheanswertoacertainquestionisnegativethenwedonotneedtopresentthenextquestionstothejudge:iftheleftphraseisnoten-tailedthenweignorethesentencealtogether;andifthecontextisirrelevantthentherightphrasecannotbeentailedfromthesentenceandsotheanswertoQreisalreadyknownasnegative.Theaboveentailmentjudgmentsassumethatwecanactuallyaskwhethertheleftorrightphrasesarecorrectgiventhesentence,thatis,weassumethatatruthvaluecanbeassignedtobothphrases.Thisisthecasewhentheleftandrighttemplatescorrespond,asexpected,tosemanticrelations.Yetsometimeslearnedtemplatesare(erroneously)notrelational,e.g.‘X,Y,IBM’(representingalist).Wethereforeletthejudgesinitiallymarkrulesthatincludesuchtemplatesasnon-relational,inwhichcasetheirexamplesarenotevaluatedatall.3.4RulePrecisionWecomputetheprecisionofarulebythepercent-ageofexamplesforwhichentailmentholdsoutofall“relevant”examples.Wecancalculatetheprecisionintwoways,asdeﬁnedbelow,dependingonwhetherweignoreirrelevantcontextsornot(obtaininglowerprecisionifwedon’t).Whensystemsansweraninformationneed,suchasaqueryorquestion,irrelevantcontextsaresometimesnotencounteredthankstoadditionalcontextwhichispresentinthegiveninput(seeSection2.1).Thus,thefollowingtwomeasurescanbeviewedasupperandlowerboundsfortheexpectedprecisionoftheruleapplicationsinactualsystems:upperboundprecision:#Entailmentholds#Relevantcontextlowerboundprecision:#Entailmentholds#Leftentailedwhere#denotesthenumberofexampleswiththecorrespondingjudgment.Finally,weconsideraruletobecorrectonlyifitsprecisionisatleast80%,whichseemssensiblefortypicalappliedsettings.Thisyieldstwoalterna-tivesetsofcorrectrules,correspondingtotheupperboundandlowerboundprecisionmeasures.Eventhoughjudgesmaydisagreeonspeciﬁcexamplesforarule,theirjudgmentsmaystillagreeoverallontherule’scorrectness.Wethereforeexpecttheagree-mentlevelonrulecorrectnesstobehigherthantheagreementonindividualexamples.4ExperimentalSettingsWeappliedtheinstance-basedmethodologytoeval-uatetwostate-of-the-artunsupervisedacquisitional-gorithms,DIRT(LinandPantel,2001)andTEASE(Szpektoretal.,2004),whoseoutputispubliclyavailable.DIRTidentiﬁessemanticallyrelatedtem-platesinalocalcorpususingdistributionalsim-ilarityoverthetemplates’variableinstantiations.TEASEacquiresentailmentrelationsfromtheWebforagiveninputtemplateIbyidentifyingcharac-teristicvariableinstantiationssharedbyIandothertemplates.461

FortheexperimentweusedthepublishedDIRTandTEASEknowledge-bases1.Foreverygivenin-puttemplateI,eachknowledge-baseprovidesalistoflearnedoutputtemplates{Oj}nI1,wherenIisthenumberofoutputtemplateslearnedforI.Eachout-puttemplateissuggestedasholdinganentailmentrelationwiththeinputtemplateI,butthealgorithmsdonotspecifytheentailmentdirection(s).Thus,eachpair{I,Oj}inducestwocandidatedirectionalentailmentrules:‘I→Oj’and‘Oj→I’.4.1TestSetConstructionThetestsetconstructionconsistsofthreesamplingsteps:selectingasetofinputtemplatesforthetwoalgorithms,selectingasampleofoutputrulestobeevaluated,andselectingasampleofsentencestobejudgedforeachrule.First,werandomlyselected30transitiveverbsoutofthe1000mostfrequentverbsintheReutersRCV1corpus2.Foreachverbwemanuallyconstructedalexical-syntacticinputtemplatebyaddingsubjectandobjectvariables.Forexam-ple,fortheverb‘seek’weconstructedthetemplate‘Xsubj←−−seekobj−−→Y’.Next,foreachinputtemplateIweconsideredthelearnedtemplates{Oj}nI1fromeachknowledge-base.SinceDIRThasalongtailoftemplateswithalowscoreandverylowprecision,DIRTtemplateswhosescoreisbelowathresholdof0.1wereﬁlteredout3.Wethensampled10%ofthetemplatesineachoutputlist,limitingthesamplesizetobebetween5-20templatesforeachlist(thusbalancingbetweensufﬁcientevaluationdataandjudgmentload).ForeachsampledtemplateOweevaluatedbothdirec-tionalrules,‘I→O’and‘O→I’.Intotal,wesam-pled380templates,inducing760directionalrulesoutofwhich754ruleswereunique.Last,werandomlyextractedasampleofexamplesentencesforeachrule‘L→R’byutilizingasearchengineovertheﬁrstCDofReutersRCV1.First,weretrievedallsentencescontainingalllexicaltermswithinL.TheretrievedsentenceswereparsedusingtheMinipardependencyparser(Lin,1998),keep-ingonlysentencesthatsyntacticallymatchL(as1Availableathttp://aclweb.org/aclwiki/index.php?title=Te-xtualEntailmentResourcePool2http://about.reuters.com/researchandstandards/corpus/3FollowingadvicebyPatrickPantel,DIRT’sco-author.explainedinSection3.1).Asampleof15match-ingsentenceswasrandomlyselected,orallmatch-ingsentencesiflessthan15werefound.Finally,anexampleforjudgmentwasgeneratedfromeachsampledsentenceanditsleftandrightphrases(seeSection3.1).Wedidnotﬁndsentencesfor108rules,andthusweendedupwith646uniquerulesthatcouldbeevaluated(with8945examplestobejudged).4.2EvaluatingtheTest-SetTwohumanjudgesevaluatedtheexamples.Werandomlysplittheexamplesbetweenthejudges.100rules(1287examples)werecrossannotatedforagreementmeasurement.ThejudgesfollowedtheprocedureinSection3.3andthecorrectnessofeachrulewasassessedbasedonbothitsupperandlowerboundprecisionvalues(Section3.4).5MethodologyEvaluationResultsWeassessedtheinstance-basedmethodologybymeasuringtheagreementlevelbetweenjudges.Thejudgesagreedon75%ofthe1287sharedexam-ples,correspondingtoareasonableKappavalueof0.64.Asimilarkappavalueof0.65wasobtainedfortheexamplesthatwerejudgedaseitherentail-mentholds/noentailmentbybothjudges.Yet,ourevaluationtargetistoassessrules,andtheKappavaluesfortheﬁnalcorrectnessjudgmentsofthesharedruleswere0.74and0.68forthelowerandupperboundevaluations.TheseKappascoresareregardedas‘substantialagreement’andaresubstan-tiallyhigherthanpublishedagreementscoresandthosewemanagedtoobtainusingthestandardrule-basedapproach.Asexpected,theagreementonrulesishigherthanonexamples,sincejudgesmaydisagreeonacertainexamplebuttheirjudgementswouldstillyieldthesameruleassessment.Table3illustratessomedisagreementsthatwerestillexhibitedwithintheinstance-basedevaluation.Theprimaryreasonfordisagreementswasthedif-ﬁcultytodecidewhetheracontextisrelevantforaruleornot,resultinginsomeconfusionbetween‘Irrelevantcontext’and‘Noentailment’.Thismayexplaintheloweragreementfortheupperboundprecision,forwhichexamplesjudgedas’Irrelevantcontext’areignored,whileforthelowerboundboth462

RuleSentenceJudge1Judge2XsignY→XsetYIraqandTurkeysignagreementtoincreasetradecooperationEntailmentholdsIrrelevantcontextXworsenY→XslowYNewsofthestrikeworsenedthesituationIrrelevantcontextNoentailmentXgetY→XwantYHewillgethisparadeonTuesdayEntailmentholdsNoentailmentTable3:Examplesfordisagreementbetweenthetwojudges.judgmentsareconﬂatedandrepresentnoentailment.Ourﬁndingssuggestthatbetterwaysfordistin-guishingrelevantcontextsmaybesoughtinfutureresearchforfurtherreﬁnementoftheinstance-basedevaluationmethodology.About43%ofallexampleswerejudgedas’Leftnotentailed’.Therelativelylowmatchingprecision(57%)madeuscollectmoreexamplesthanneeded,since’Leftnotentailed’examplesareignored.Bet-termatchingcapabilitieswillallowcollectingandjudgingfewerexamples,thusimprovingtheefﬁ-ciencyoftheevaluationprocess.6DIRTandTEASEEvaluationResultsDIRTTEASEPYPYRules:UpperBound30.5%33.528.4%40.3LowerBound18.6%20.417%24.1Templates:UpperBound44%22.638%26.9LowerBound27.3%14.123.6%16.8Table4:AveragePrecision(P)andYield(Y)attheruleandtemplatelevels.Weevaluatedthequalityoftheentailmentrulesproducedbyeachalgorithmusingtwoscores:(1)microaveragePrecision,thepercentageofcorrectrulesoutofalllearnedrules,and(2)averageYield,theaveragenumberofcorrectruleslearnedforeachinputtemplateI,asextrapolatedbasedonthesam-ple4.SinceDIRTandTEASEdonotidentifyruledirectionality,wealsomeasuredthesescoresatthe4Sincetherulesarematchedagainstthefullcorpus(asinIRevaluations),itisdifﬁculttoevaluatetheirtruerecall.templatelevel,whereanoutputtemplateOiscon-sideredcorrectifatleastoneoftherules‘I→O’or‘O→I’iscorrect.TheresultsarepresentedinTa-ble4.ThemajorﬁndingisthattheoverallqualityofDIRTandTEASEisverysimilar.UnderthespeciﬁcDIRTcutoffthresholdchosen,DIRTexhibitssome-whathigherPrecisionwhileTEASEhassomewhathigherYield(recallthatthereisnoparticularnaturalcutoffpointforDIRT’soutput).Sinceapplicationstypicallyapplyrulesinaspe-ciﬁcdirection,thePrecisionforrulesreﬂectstheirexpectedperformancebetterthanthePrecisionfortemplates.Obviously,futureimprovementinpre-cisionisneededforrulelearningalgorithms.Mean-while,manualﬁlteringofthelearnedrulescanproveeffectivewithinlimiteddomains,whereourevalua-tionapproachcanbeutilizedforreliableﬁlteringaswell.Thesubstantialyieldobtainedbythesealgo-rithmssuggestthattheyareindeedlikelytobevalu-ableforrecallincreaseinsemanticapplications.Inaddition,wefoundthatonlyabout15%ofthecorrecttemplateswerelearnedbybothalgorithms,whichimpliesthatthetwoalgorithmslargelycom-plementeachotherintermsofcoverage.Oneex-planationmaybethatDIRTisfocusedonthedo-mainofthelocalcorpusused(newsarticlesforthepublishedDIRTknowledge-base),whereasTEASElearnsfromtheWeb,extractingrulesfrommultipledomains.SincePrecisioniscomparableitmaybebesttousebothalgorithmsintandem.WealsomeasuredwhetherOisaparaphraseofI,i.e.whetherboth‘I→O’and‘O→I’arecor-rect.Only20-25%ofallcorrecttemplateswereas-sessedasparaphrases.Thisstressesthesigniﬁcanceofevaluatingdirectionalrulesratherthanonlypara-phrases.Furthermore,itshowsthatinordertoim-proveprecision,acquisitionalgorithmsmustiden-tifyruledirectionality.463

About28%ofall‘Leftentailed’exampleswereevaluatedas‘Irrelevantcontext’,yieldingthelargedifferenceinprecisionbetweentheupperandlowerprecisionbounds.Thisresultshowsthatinordertogetclosertotheupperboundprecision,learningalgorithmsandapplicationsneedtoidentifytherel-evantcontextsinwhicharuleshouldbeapplied.Last,wenotethattheinstance-basedqualityas-sessmentcorrespondstothecorpusfromwhichtheexamplesentencesweretaken.Itisthereforebesttoevaluatetherulesusingacorpusofthesamedomainfromwhichtheywerelearned,orthetargetapplica-tiondomainforwhichtheruleswillbeapplied.7ConclusionsAccuratelearningofinferenceknowledge,suchasentailmentrules,hasbecomecriticalforfurtherprogressofappliedsemanticsystems.However,evaluationofsuchknowledgehasbeenproblematic,hinderingfurtherdevelopments.Theinstance-basedevaluationapproachproposedinthispaperobtainedacceptableagreementlevels,whicharesubstantiallyhigherthanthoseobtainedforthecommonrule-basedapproach.Wealsoconductedtheﬁrstcomparisonbetweentwostate-of-the-artacquisitionalgorithms,DIRTandTEASE,usingthenewmethodology.Wefoundthattheirqualityiscomparablebuttheyeffectivelycomplementeachotherintermsofrulecoverage.Also,wefoundthatmostlearnedrulesarenotpara-phrasesbutratherone-directionalentailmentrules,andthatmanyoftherulesarecontextsensitive.Theseﬁndingssuggestinterestingdirectionsforfu-tureresearch,inparticularlearningruledirection-alityandrelevantcontexts,issuesthatwerehardlyexploredtillnow.Suchdevelopmentscanbethenevaluatedbytheinstance-basedmethodology,whichwasdesignedtocapturethesetwoimportantaspectsofentailmentrules.AcknowledgementsTheauthorswouldliketothankEphiSachsandIddoGreentalfortheirevaluation.ThisworkwaspartiallysupportedbyISFgrant1095/05,theISTProgrammeoftheEuropeanCommunityunderthePASCALNetworkofExcellenceIST-2002-506778,andtheITC-irst/UniversityofHaifacollaboration.ReferencesRoyBar-Haim,IdoDagan,BillDolan,LisaFerro,DaniloGiampiccolo,BernardoMagnini,andIdanSzpektor.2006.Thesecondpascalrecognisingtextualentail-mentchallenge.InSecondPASCALChallengeWork-shopforRecognizingTextualEntailment.ReginaBarzilayandLillianLee.2003.Learningtoparaphrase:Anunsupervisedapproachusingmultiple-sequencealignment.InProceedingsofNAACL-HLT.GennaroChierchiaandSallyMcConnell-Ginet.2000.MeaningandGrammar(2nded.):anintroductiontosemantics.MITPress,Cambridge,MA.IdoDagan,OrenGlickman,andBernardoMagnini.2006.Thepascalrecognisingtextualentailmentchal-lenge.LectureNotesinComputerScience,3944:177–190.DekangLinandPatrickPantel.2001.Discoveryofinfer-encerulesforquestionanswering.NaturalLanguageEngineering,7(4):343–360.DekangLin.1998.Dependency-basedevaluationofminipar.InProceedingsoftheWorkshoponEvalu-ationofParsingSystemsatLREC.BoPang,KevinKnight,andDanielMarcu.2003.Syntax-basedalignmentofmultipletranslations:Ex-tractingparaphrasesandgeneratingnewsentences.InProceedingsofHLT-NAACL.DeepakRavichandranandEduardHovy.2002.Learningsurfacetextpatternsforaquestionansweringsystem.InProceedingsofACL.LorenzaRomano,MilenKouylekov,IdanSzpektor,IdoDagan,andAlbertoLavelli.2006.Investigatingagenericparaphrase-basedapproachforrelationextrac-tion.InProceedingsofEACL.SatoshiSekine.2005.Automaticparaphrasediscoverybasedoncontextandkeywordsbetweennepairs.InProceedingsofIWP.YusukeShinyama,SatoshiSekine,KiyoshiSudo,andRalphGrishman.2002.Automaticparaphraseacqui-sitionfromnewsarticles.InProceedingsofHLT.KiyoshiSudo,SatoshiSekine,andRalphGrishman.2003.AnimprovedextractionpatternrepresentationmodelforautomaticIEpatternacquisition.InPro-ceedingsofACL.IdanSzpektor,HristoTanev,IdoDagan,andBonaven-turaCoppola.2004.Scalingweb-basedacquisitionofentailmentrelations.InProceedingsofEMNLP.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 464–471,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

464

StatisticalMachineTranslationforQueryExpansioninAnswerRetrievalStefanRiezler,AlexanderVasserman,IoannisTsochantaridis,VibhuMittalandYiLiuGoogleInc.,1600AmphitheatreParkway,MountainView,CA94043{riezler|avasserm|ioannis|vibhu|yliu}@google.comAbstractWepresentanapproachtoqueryexpan-sioninanswerretrievalthatusesStatisti-calMachineTranslation(SMT)techniquestobridgethelexicalgapbetweenques-tionsandanswers.SMT-basedqueryex-pansionisdonebyi)usingafull-sentenceparaphrasertointroducesynonymsincon-textoftheentirequery,andii)bytrans-latingquerytermsintoanswertermsus-ingafull-sentenceSMTmodeltrainedonquestion-answerpairs.Weevaluatetheseglobal,context-awarequeryexpansiontech-niquesontﬁdfretrievalfrom10millionquestion-answerpairsextractedfromFAQpages.ExperimentalresultsshowthatSMT-basedexpansionimprovesretrievalperfor-manceoverlocalexpansionandoverre-trievalwithoutexpansion.1IntroductionOneofthefundamentalproblemsinQuestionAn-swering(QA)hasbeenrecognizedtobethe“lexi-calchasm”(Bergeretal.,2000)betweenquestionstringsandanswerstrings.Thisproblemismani-festedinamismatchbetweenquestionandanswervocabularies,andisaggravatedbytheinherentam-biguityofnaturallanguage.Severalapproacheshavebeenpresentedthatapplynaturallanguageprocess-ingtechnologytoclosethisgap.Forexample,syn-tacticinformationhasbeendeployedtoreformu-latequestions(Hermjakobetal.,2002)ortore-placequestionsbysyntacticallysimilarones(LinandPantel,2001);lexicalontologiessuchasWord-net1havebeenusedtoﬁndsynonymsforquestionwords(Burkeetal.,1997;Hovyetal.,2000;Prageretal.,2001;Harabagiuetal.,2001),andstatisti-calmachinetranslation(SMT)modelstrainedonquestion-answerpairshavebeenusedtorankcan-didateanswersaccordingtotheirtranslationprob-abilities(Bergeretal.,2000;EchihabiandMarcu,2003;SoricutandBrill,2006).Informationretrieval(IR)isfacedbyasimilarfundamentalproblemof“termmismatch”betweenqueriesanddocuments.AstandardIRsolution,queryexpansion,attemptstoincreasethechancesofmatchingwordsinrelevantdocumentsbyaddingtermswithsimilarstatisticalpropertiestothoseintheoriginalquery(Voorhees,1994;QiuandFrei,1993;XuandCroft,1996).InthispaperwewillconcentrateonthetaskofanswerretrievalfromFAQpages,i.e.,anIRprob-lemwhereuserqueriesarematchedagainstdocu-mentsconsistingofquestion-answerpairsfoundinFAQpages.Equivalently,thisisaQAproblemthatconcentratesonﬁndinganswersgivenFAQdocu-mentsthatareknowntocontaintheanswers.Ourapproachtoclosethelexicalgapinthissettingat-temptstomarryQAandIRtechnologybydeploy-ingSMTmethodsforqueryexpansioninanswerretrieval.WepresenttwoapproachestoSMT-basedqueryexpansion,bothofwhichareimplementedintheframeworkofphrase-basedSMT(OchandNey,2004;Koehnetal.,2003).Ourﬁrstqueryexpansionmodeltrainsanend-to-endphrase-basedSMTmodelon10millionquestion-answerpairsextractedfromFAQpages.1http://wordnet.princeton.edu465

Thegoalofthissystemistolearnlexicalcorrela-tionsbetweenwordsandphrasesinquestionsandanswers,forexamplebyallowingformultipleun-alignedwordsinautomaticwordalignment,anddis-regardingissuessuchaswordorder.Theabilitytotranslatephrasesinsteadofwordsandtheuseofalargelanguagemodelserveasrichcontexttomakeprecisedecisionsinthecaseofambiguoustransla-tions.Queryexpansionisperformedbyaddingcon-tentwordsthathavenotbeenseenintheoriginalqueryfromthen-besttranslationsofthequery.OursecondqueryexpansionmodelisbasedontheuseofSMTtechnologyforfull-sentencepara-phrasing.Aphrasetableofparaphrasesisextractedfrombilingualphrasetables(BannardandCallison-Burch,2005),andparaphrasingqualityisimprovedbyadditionaldiscriminativetrainingonmanuallycreatedparaphrases.Thisapproachutilizeslargebilingualphrasetablesasinformationsourcetoex-tractatableofpara-phrases.Synonymsforqueryexpansionarereadofffromthen-bestparaphrasesoffullqueriesinsteadoffromparaphrasesofsep-aratewordsorphrases.Thisallowsthemodeltotakeadvantageoftherichcontextofalargen-gramlanguagemodelwhenaddingtermsfromthen-bestparaphrasestotheoriginalquery.Inourexperimentalevaluationwedeployadatabaseofquestion-answerpairsextractedfromFAQpagesforbothtrainingaquestion-answertranslationmodel,andforacomparativeevalua-tionofdifferentsystemsonthetaskofanswerre-trieval.RetrievalisbasedonthetﬁdfframeworkofJijkounanddeRijke(2005),andqueryexpan-sionisdonestraightforwardlybyaddingexpansiontermstothequeryforasecondretrievalcycle.Wecompareourglobal,context-awarequeryexpansiontechniqueswithJijkounanddeRijke’s(2005)tﬁdfmodelforanswerretrievalandalocalqueryexpan-siontechnique(XuandCroft,1996).Experimen-talresultsshowasigniﬁcantimprovementofSMT-basedqueryexpansionoverbothbaselines.2RelatedWorkQAhasapproachedtheproblemofthelexicalgapbyvarioustechniquesforquestionreformulation,includingrule-basedsyntacticandsemanticrefor-mulationpatterns(Hermjakobetal.,2002),refor-mulationsbasedonshareddependencyparses(LinandPantel,2001),orvarioususesoftheWord-Netontologytoclosethelexicalgapword-by-word(Hovyetal.,2000;Prageretal.,2001;Harabagiuetal.,2001).Anotheruseofnaturallanguagepro-cessinghasbeenthedeploymentofSMTmodelsonquestion-answerpairsfor(re)rankingcandidatean-swerswhichwereeitherassumedtobecontainedinFAQpages(Bergeretal.,2000)orretrievedbybaselinesystems(EchihabiandMarcu,2003;Sori-cutandBrill,2006).IRhasapproachedthetermmismatchproblembyvariousapproachestoqueryexpansion(Voorhees,1994;QiuandFrei,1993;XuandCroft,1996).Inconclusiveresultshavebeenreportedfortech-niquesthatexpandquerytermsseparatelybyaddingstronglyrelatedtermsfromanexternalthesaurussuchasWordNet(Voorhees,1994).Signiﬁcantimprovementsinretrievalperformancecouldbeachievedbyglobalexpansiontechniquesthatcom-putecorpus-widestatisticsandtaketheentirequery,orqueryconcept(QiuandFrei,1993),intoaccount,orbylocalexpansiontechniquesthatselectexpan-siontermsfromthetoprankeddocumentsretrievedbytheoriginalquery(XuandCroft,1996).AsimilarpictureemergesforqueryexpansioninQA:Mixedresultshavebeenreportedforword-by-wordexpansionbasedonWordNet(Burkeetal.,1997;Hovyetal.,2000;Prageretal.,2001;Harabagiuetal.,2001).ConsiderableimprovementshavebeenreportedfortheuseofthelocalcontextanalysismodelofXuandCroft(1996)intheQAsystemofIttycheriahetal.(2001),orforthesys-temsofAgichteinetal.(2004)orHarabagiuandLacatusu(2004)thatuseFAQdatatolearnhowtoexpandquerytermsbyanswerterms.TheSMT-basedapproachespresentedinthispa-percanbeseenasglobalqueryexpansiontech-niquesinthatourquestion-answertranslationmodelusesthewholequestion-answercorpusasinforma-tionsource,andourapproachtoparaphrasingde-ployslargeamountsofbilingualphrasesashigh-coverageinformationsourceforsynonymﬁnding.Furthermore,bothapproachestaketheentirequerycontextintoaccountwhenproposingtoaddnewtermstotheoriginalquery.TheapproachesthatareclosesttoourmodelsaretheSMTapproachofRadevetal.(2001)andtheparaphrasingapproach466

webpagesFAQpagesQApairscount4billion795,48310,568,160Table1:CorpusstatisticsofQApairdataofDuboueandChu-Carroll(2006).Noneoftheseapproachesdeﬁnestheproblemofthelexicalgapasaqueryexpansionproblem,andbothapproachesusemuchsimplerSMTmodelsthanoursystems,e.g.,Radevetal.(2001)neglecttousealanguagemodeltoaiddisambiguationoftranslationchoices,andDuboueandChu-Carroll(2006)useSMTasblackboxaltogether.Insum,ourapproachdiffersfrompreviousworkinQAandIRintheuseSMTtechnologyforqueryexpansion,andshouldbeapplicableinbothareaseventhoughexperimentalresultsareonlygivenfortherestricteddomainofretrievalfromFAQpages.3Question-AnswerPairsfromFAQPagesLarge-scalecollectionofquestion-answerpairshasbeenhamperedinpreviousworkbythesmallsizesofpubliclyavailableFAQcollectionsorbyrestrictedaccesstoretrievalresultsviapublicAPIsofsearchengines.JijkounanddeRijke(2005)neverthelessmanagedtoextractaround300,000FAQpagesand2.8millionquestion-answerpairsbyrepeatedlyqueryingsearchengineswith“intitle:faq”and“inurl:faq”.SoricutandBrill(2006)coulddeployaproprietaryURLcollectionof1billionURLstoextract2.3millionFAQpagescontain-ingtheuncasedstring“faq”intheurlstring.Theextractionofquestion-answerpairsamountedtoadatabaseof1millionpairsintheirexperiment.However,inspectionofthepubliclyavailableWeb-FAQcollectionprovidedbyJijkounanddeRijke2showedagreatamountofnoiseintheretrievedFAQpagesandquestion-answerpairs,andyettheindexedquestion-answerpairsshowedaseriousre-callprobleminthatnoanswercouldberetrievedformanywell-formedqueries.Forourexperiment,wedecidedtopreferprecisionoverrecallandtoattemptaprecision-orientedFAQandquestion-answerpairextractionthatbeneﬁtsthetrainingofquestion-answertranslationmodels.2http://ilps.science.uva.nl/Resources/WazDah/AsshowninTable1,theFAQpagesusedinourexperimentwereextractedfroma4billionpagesubsetofthewebusingthequeries“inurl:faq”and“inurl:faqs”tomatchthetokens“faq”or“faqs”intheurls.Thisextractionresultedin2.6millionwebpages(0.07%ofthecrawl).SincenotallthosepagesareactuallyFAQs,wemanuallyla-beled1,000ofthosepagestotrainanonlinepassive-aggressiveclassiﬁcier(Crammeretal.,2006)ina10-foldcrossvalidationsetup.Trainingwasdoneusing20featurefunctionsonoccurrencesquestionmarksandkeywordsindifferentﬁeldsofwebpages,andresultedinanF1scoreofaround90%forFAQclassiﬁcation.Applicationoftheclassiﬁertotheextractedwebpagesresultedinaclassiﬁcationof795,483pagesasFAQpages.Theextractionofquestion-answerpairsfromthisdatabaseofFAQpageswasperformedagaininaprecision-orientedmanner.Thegoalofthisstepwastoextracturl,title,question,andanswersﬁeldsfromthequestion-answerpairsinFAQpages.Thiswasachievedbyusingfeaturefunctionsonpunc-tuations,HTMLtags(e.g.,<p>,<BR>),listingmarkers(e.g.,Q:,(1)),andlexicalcues(e.g.,What,How),andanalgorithmsimilartoJoachims(2003)topropagateinitiallabelsacrosssimilartextpieces.Theresultofthisextractionstepisadatabaseofabout10millionquestionanswerpairs(13.3pairsperFAQpage).Amanualevaluationof100documents,containing1,303question-answerpairs,achievedaprecisionof98%andarecallof82%forextractingquestion-answerpairs.4SMT-BasedQueryExpansionOurSMT-basedqueryexpansiontechniquesarebasedonarecentimplementationofthephrase-basedSMTframework(Koehnetal.,2003;OchandNey,2004).TheprobabilityoftranslatingaforeignsentencefintoEnglisheisdeﬁnedinthenoisychan-nelmodelasargmaxep(e|f)=argmaxep(f|e)p(e)(1)Thisallowsforaseparationofalanguagemodelp(e),andatranslationmodelp(f|e).Translationprobabilitiesarecalculatedfromrelativefrequenciesofphrases,whichareextractedviavariousheuris-ticsaslargerblocksofalignedwordsfrombestword467

alignments.Wordalignmentsareestimatedbymod-elssimilartoBrownetal.(1993).ForasequenceofIphrases,thetranslationprobabilityinequation(1)canbedecomposedintop(fIi|eIi)=IYi=1p(fi|ei)(2)RecentSMTmodelshaveshownsigniﬁcantim-provementsintranslationqualitybyimprovedmod-elingoflocalwordorderandidiomaticexpressionsthroughtheuseofphrases,andbythedeploymentoflargen-gramlanguagemodelstomodelﬂuencyandlexicalchoice.4.1Question-AnswerTranslationOurﬁrstapproachtoqueryexpansiontreatsthequestionsandanswersinthequestion-answercor-pusastwodistinctlanguages.Thatis,the10millionquestion-answerpairsextractedfromFAQpagesarefedasparalleltrainingdataintoanSMTtrainingpipeline.Thistrainingprocedureincludesvariousstandardproceduressuchaspreprocessing,sentenceandchunkalignment,wordalignment,andphraseextraction.Thegoalofquestion-answertranslationistolearnassociationsbetweenquestionwordsandsynonymousanswerwords,ratherthanthetrans-lationofquestionsintoﬂuentanswers.Thuswedidnotconductdiscriminativetrainingoffeatureweightsfortranslationprobabilitiesorlanguagemodelprobabilities,butweheldout4,000question-answerpairsformanualdevelopmentandtestingofthesystem.Forexample,thesystemwasadjustedtoaccountforthedifferenceinsentencelengthbe-tweenquestionsandanswersbysettingthenull-wordprobabilityparameterinwordalignmentto0.9.Thisallowedustoconcentratethewordalign-mentstoasmallnumberofkeywords.Furthermore,extractionofphraseswasbasedontheintersectionofalignmentsfrombothtranslationdirections,thusfavoringprecisionoverrecallalsoinphrasealign-ment.Table2showsuniquetranslationsofthequery“howtolivewithcatallergies”onthephrase-level,withcorrespondingsourceandtargetphrasesshowninbrackets.Expansiontermsaretakenfromphrasetermsthathavenotbeenseenintheoriginalquery,andarehighlightedinboldface.4.2SMT-BasedParaphrasingOurSMT-basedparaphrasingsystemisbasedontheapproachpresentedinBannardandCallison-Burch(2005).Thecentralideainthisapproachistoiden-tifyparaphrasesorsynonymsatthephraselevelbypivotingonanotherlanguage.Forexample,givenatableofChinese-to-Englishphrasetranslations,phrasalsynonymsinthetargetlanguagearedeﬁnedasthoseEnglishphrasesthatarealignedtothesameChinesesourcephrases.Translationprobabilitiesforextractedpara-phrasescanbeinferredfrombilin-gualtranslationprobabilitiesasfollows:GivenanEnglishpara-phrasepair(trg,syn),theprobabilityp(syn|trg)thattrgtranslatesintosynisdeﬁnedasthejointprobabilitythattheEnglishphrasetrgtranslatesintotheforeignphrasesrc,andthattheforeignphrasesrctranslatesintotheEnglishphrasesyn.Underanindependenceassumptionofthosetwoevents,thisprobabilityandthereversetransla-tiondirectionp(trg|syn)canbedeﬁnedasfollows:p(syn|trg)=maxsrcp(src|trg)p(syn|src)(3)p(trg|syn)=maxsrcp(src|syn)p(trg|src)Sincethesamepara-phrasepaircanbeobtainedbypivotingonmultipleforeignlanguagephrases,asummationormaximizationoverforeignlanguagephrasesisnecessary.Inordernottoputtoomuchprobabilitymassontopara-phrasetranslationsthatcanbeobtainedfrommultipleforeignlanguagephrases,wemaximizeinsteadofsummingoversrc.Inourexperiments,weemployedequation(3)toinferforeachpara-phrasepairtranslationmodelprobabilitiespφ(syn|trg)andpφ0(trg|syn)fromrelativefrequenciesofphrasesinbilingualtables.IncontrasttoBannardandCallison-Burch(2005),weappliedthesameinferencesteptoinferalsolexicaltranslationprobabilitiespw(syn|trg)andpw0(trg|syn)asdeﬁnedinKoehnetal.(2003)forpara-phrases.Furthermore,wedeployedfeaturesforthenumberofwordslw,numberofphrasescφ,areorderingscorepd,andascorefora6-gramlan-guagemodelpLMtrainedonEnglishwebdata.Theﬁnalmodelcombinesthesefeaturesinalog-linearmodelthatdeﬁnestheprobabilityofparaphrasingafullsentence,consistingofasequenceofIphrases468

qa-translation(how,how)(to,to)(live,live)(with,with)(cat,pet)(allergies,allergies)(how,how)(to,to)(live,live)(with,with)(cat,cat)(allergies,allergy)(how,how)(to,to)(live,live)(with,with)(cat,cat)(allergies,food)(how,how)(to,to)(live,live)(with,with)(cat,cats)(allergies,allergies)paraphrasing(how,how)(tolive,tolive)(withcat,withcat)(allergies,allergy)(how,ways)(tolive,tolive)(withcat,withcat)(allergies,allergies)(how,how)(tolivewith,tolivewith)(cat,feline)(allergies,allergies)(howto,howto)(live,living)(withcat,withcat)(allergies,allergies)(howto,howto)(live,life)(withcat,withcat)(allergies,allergies)(how,way)(tolive,tolive)(withcat,withcat)(allergies,allergies)(how,how)(tolive,tolive)(withcat,withcat)(allergies,allergens)(how,how)(tolive,tolive)(withcat,withcat)(allergies,allergen)Table2:Uniquen-bestphrase-leveltranslationsofquery“howtolivewithcatallergies”.asfollows:p(synI1|trgI1)=(IYi=1pφ(syni|trgi)λφ(4)×pφ0(trgi|syni)λφ0×pw(syni|trgi)λw×pw0(trgi|syni)λw0×pd(syni,trgi)λd)×lw(synI1)λl×cφ(synI1)λc×pLM(synI1)λLMForestimationofthefeatureweights~λdeﬁnedinequation(4)weemployedminimumerrorrate(MER)trainingundertheBLEUmeasure(Och,2003).TrainingdataforMERtrainingweretakenfrommultiplemanualEnglishtranslationsofChi-nesesourcesfromtheNIST2006evaluationdata.TheﬁrstoffourreferencetranslationsforeachChi-nesesentencewastakenassourceparaphrase,therestasreferenceparaphrases.Discriminativetrain-ingwasconductedon1,820sentences;ﬁnalevalua-tionon2,390sentences.Abaselineparaphrasetableconsistingof33millionEnglishpara-phrasepairswasextractedfrom1billionphrasepairsfromthreedifferentlanguages,atacutoffofpara-phraseprob-abilitiesof0.0025.Queryexpansionisdonebyaddingtermsintro-ducedinn-bestparaphrasesofthequery.Table2showsexampleparaphrasesforthequery“howtolivewithcatallergies”withnewlyintroducedtermshighlightedinboldface.5ExperimentalEvaluationOurbaselineanswerretrievalsystemismodeledaf-terthetﬁdfretrievalmodelofJijkounanddeRi-jke(2005).Theirmodelcalculatesalinearcom-binationofvectorsimilarityscoresbetweentheuserqueryandseveralﬁeldsinthequestion-answerpair.Weusedthecosinesimilaritymetricwithlogarithmicallyweightedtermanddocumentfre-quencyweightsinordertoreproducetheLucene3modelusedinJijkounanddeRijke(2005).Forindexingofﬁelds,weadoptedthesettingsthatwerereportedtobeoptimalinJijkounanddeRijke(2005).Thesesettingscomprisetheuseof8question-answerpairﬁelds,andaweightvec-torh0.0,1.0,0.0,0.0,0.5,0.5,0.2,0.3iforﬁeldsor-deredasfollows:(1)fullFAQdocumenttext,(2)questiontext,(3)answertext,(4)titletext,(5)-(8)eachoftheabovewithoutstopwords.Thesecondﬁeldthustakestakeswh-words,whichwouldtyp-icallybeﬁlteredout,intoaccount.Allotherﬁeldsarematchedwithoutstopwords,withhigherweightassignedtodocumentandquestionthantoanswerandtitleﬁelds.Wedidnotusephrase-matchingorstemminginourexperiments,similartoJijkounanddeRijke(2005),whocouldnotﬁndpositiveeffectsforthesefeaturesintheirexperiments.Expansiontermsaretakenfromthosetermsinthen-besttranslationsofthequerythathavenotbeenseenintheoriginalquerystring.Forparaphrasing-basedqueryexpansion,a50-bestlistofparaphrasesoftheoriginalquerywasused.Forthenoisierquestion-answertranslation,expan-siontermsandphraseswereextractedfroma10-3http://lucene.apache.org469

S2@10S2@20S1,2@10S1,2@20baselinetﬁdf27355865localexpansion30(+11.1)40(+14.2)57(-1)63(-3)SMT-basedexpansion38(+40.7)43(+22.8)5865Table3:Successrateat10or20resultsforretrievalofadequate(2)ormaterial(1)answers;relativechangeinbrackets.bestlistofquerytranslations.Termstakenfromqueryparaphraseswerematchedwiththesameﬁeldweightvectorh0.0,1.0,0.0,0.0,0.5,0.5,0.2,0.3iasabove.Termstakenfromquestion-answertrans-lationwerematchedwiththeweightvectorh0.0,1.0,0.0,0.0,0.5,0.2,0.5,0.3i,preferringan-swerﬁeldsoverquestionﬁelds.Afterstopwordremoval,theaveragenumberofexpansiontermsproducedwas7.8forparaphrasing,and3.1forquestion-answertranslation.Thelocalexpansiontechniqueusedinourexper-imentsfollowsXuandCroft(1996)intakingex-pansiontermsfromthetopnanswersthatwerere-trievedbythebaselinetﬁdfsystem,andbyincorpo-ratingcooccurrenceinformationwithqueryterms.Thisisdonebycalculatingtermfrequenciesforex-pansiontermsbysummingupthetﬁdfweightsoftheanswersinwhichtheyoccur,thusgivinghigherweighttotermsthatoccurinanswersthatreceiveahighersimilarityscoretotheoriginalquery.Inourexperiments,expansiontermsarerankedaccord-ingtothismodiﬁedtﬁdfcalculationoverthetop20answersretrievedbythebaselineretrievalrun,andmatchedasecondtimewiththeﬁeldweightvectorh0.0,1.0,0.0,0.0,0.5,0.2,0.5,0.3ithatprefersan-swerﬁeldsoverquestionﬁelds.Afterstopwordre-moval,theaveragenumberofexpansiontermspro-ducedbythelocalexpansiontechniquewas9.25.ThetestqueriesweusedforretrievalaretakenfromquerylogsoftheMetaCrawlersearchen-gine4andwereprovidedtousbyValentinJijk-oun.Inordertomaximizerecallforthecomparativeevaluationofsystems,weselected60queriesthatwerewell-formednaturallanguagequestionswith-outmetacharactersandspellingerrors.However,foronethirdofthesewell-formedqueriesnoneoftheﬁvecomparedsystemscouldretrieveananswer.Ex-amplesare“howdoyoumakeacornhuskdoll”,4http://www.metacrawler.com“whatistheideaofmaterialization”,or“whatdoes8xcertiﬁedmean”,pointingtoasevererecallprob-lemofthequestion-answerdatabase.Evaluationwasperformedbymanuallabelingoftop20answersretrievedforeachof60queriesforeachsystembytwoindependentjudges.Forthesakeofconsistency,wechosenottousetheassessmentsprovidedbyJijkounanddeRijke.Instead,thejudgeswereaskedtoﬁndagreementontheexamplesonwhichtheydisagreedaftereachevaluationround.Theratingstogetherwiththequestion-answerpairidwerestoredandmergedintotheretrievalresultsforthenextsystemevaluation.Inthiswayconsis-tencyacrosssystemevaluationscouldbeensured,andtheeffortofmanuallabelingcouldbesubstan-tiallyreduced.ThequalityofretrievalresultswasassessedaccordingtoJijkounanddeRijke’s(2005)threepointscale:•adequate(2):answeriscontained•material(1):noexactanswer,butimportantin-formationgiven•unsatisfactory(0):user’sinformationneedisnotaddressedTheevaluationmeasureusedinJijkounanddeRijke(2005)isthesuccessrateat10or20an-swers,i.e.,S2@nisthepercentageofquerieswithatleastoneadequateanswerinthetopnretrievedquestion-answerpairs,andS1,2@nisthepercentageofquerieswithatleastoneadequateormaterialan-swerinthetopnresults.Thisevaluationmeasureac-countsforimprovementsincoverage,i.e.,itrewardscaseswhereanswersarefoundforqueriesthatdidnothaveanadequateormaterialanswerbefore.Incontrast,themeanreciprocalrank(MRR)measurestandardlyusedinQAcanhavetheeffectofprefer-ringsystemsthatﬁndanswersonlyforasmallsetofqueries,butrankthemhigherthansystemswith470

(1)query:howtolivewithcatallergieslocalexpansion(-):allergensallergicinfectionsﬁlterplasmaclusterrhinitisintroductioneffectivereplacementqa-translation(+):allergycatspetfoodparaphrasing(+):wayallergenslifeallergyfelinewayslivingallergen(2)query:howtodesignmodelrocketslocalexpansion(-):modelsrepresentedorientationdrawingsanalysiselementenvironmentdifferentstructureqa-translation(+):modelsrocketparaphrasing(+):missilesmissilerocketgrenadesarrowdesigningprototypemodelswaysparadigm(3)query:whatisdnahybridizationlocalexpansion(-):instructionsindividualblueprintcharacteristicschromosomesdeoxyribonucleicinformationbiologicalgeneticmoleculeqa-translation(+):slidesclonecdnasittingsequencesparaphrasing(+):hibridizationhybridshybridationanythinghibridacionhybridisingadnhybridisationnothing(4)query:howtoenhancecompetitivenessofindianindustrieslocalexpansion(+):resourcesproductionqualityprocessingestablishedinvestmentdevelopmentfacilitiesinstitutionalqa-translation(+):increaseindustryparaphrasing(+):promoteraiseimproveincreaseindustrystrengthen(5)query:howtoinducelabourlocalexpansion(-):experienceinductionpracticeimaginationconcentrationinformationconsciousnessdifferentmeditationrelaxationqa-translation(-):birthindustrialinducedinducesparaphrasing(-):wayworkersinducingemploymentwayslaborworkingchildworkjobactionunionsTable4:Examplesforqueriesandexpansiontermsyieldingimproved(+),decreased(-),orunchanged(0)retrievalperformancecomparedtoretrievalwithoutexpansion.highercoverage.ThismakesMRRlessadequateforthelow-recallsetupofFAQretrieval.Table3showssuccessratesat10and20retrievedquestion-answerpairsforﬁvedifferentsystems.Theresultsforthebaselinetﬁdfsystem,followingJijk-ounanddeRijke(2005),areshowninrow2.Row3presentsresultsforourvariantoflocalexpansionbypseudo-relevancefeedback(XuandCroft,1996).ResultsforSMT-basedexpansionaregiveninrow4.Acomparisonofsuccessratesforretrievingatleastoneadequateanswerinthetop10resultsshowsrel-ativeimprovementsoverthebaselineof11.1%forlocalqueryexpansion,andof40.7%forcombinedSMT-basedexpansion.Successratesattop20re-sultsshowsimilarrelativeimprovementsof14.2%forlocalqueryexpansion,andof22.8%forcom-binedSMT-basedexpansion.Ontheeasiertaskofretrievingamaterialoradequateanswer,successratesdropbyasmallamountforlocalexpansion,andstayunchangedforSMT-basedexpansion.Theseresultscanbeexplainedbyinspectingafewsamplequeryexpansions.Examples(1)-(3)inTa-ble4illustratecaseswhereSMT-basedqueryexpan-sionimprovesresultsoverbaselineperformance,butlocalexpansiondecreasesperformancebyintroduc-ingirrelevantterms.In(4)retrievalperformanceisimprovedoverthebaselineforbothexpansiontech-niques.In(5)bothlocalandSMT-basedexpansionintroducetermsthatdecreaseretrievalperformancecomparedtoretrievalwithoutexpansion.6ConclusionWepresentedtwotechniquesforqueryexpansioninanswerretrievalthatarebasedonSMTtechnology.Ourmethodforquestion-answertranslationusesalargecorpusofquestion-answerpairsextractedfromFAQpagestolearnatranslationmodelfromques-tionstoanswers.SMT-basedparaphrasingutilizeslargeamountsofbilingualdataasanewinforma-tionsourcetoextractphrase-levelsynonyms.BothSMT-basedtechniquestaketheentirequerycontextintoaccountwhenaddingnewtermstotheorig-inalquery.InanexperimentalcomparisonwithabaselinetﬁdfapproachandalocalqueryexpansiontechniqueonthetaskofanswerretrievalfromFAQpages,weshowedasigniﬁcantimprovementofbothSMT-basedqueryexpansionoverbothbaselines.Despitethesmall-scalenatureofourcurrentex-perimentalresults,wehopetoapplythepresentedtechniquestogeneralwebretrievalinfuturework.Anothertaskforfutureworkistoscaleuptheex-tractionofquestion-answerpairdatainordertoprovideanimprovedresourceforquestion-answertranslation.471

ReferencesEugeneAgichtein,SteveLawrence,andLuisGravano.2004.Learningtoﬁndanswerstoquestionsontheweb.ACMTransactionsonInternetTechnology,4(2):129–162.ColinBannardandChrisCallison-Burch.2005.Para-phrasingwithbilingualparallelcorpora.InProceed-ingsof(ACL’05),AnnArbor,MI.AdamL.Berger,RichCaruana,DavidCohn,DayneFre-itag,andVibhuMittal.2000.Bridgingthelexicalchasm:Statisticalapproachestoanswer-ﬁnding.InProceedingsofSIGIR’00,Athens,Greece.PeterF.Brown,StephenA.DellaPietra,VincentJ.DellaPietra,andRobertL.Mercer.1993.Themathemat-icsofstatisticalmachinetranslation:Parameteresti-mation.ComputationalLinguistics,19(2):263–311.RobinB.Burke,KristianJ.Hammond,andVladimirA.Kulyukin.1997.Questionansweringfromfrequently-askedquestionﬁles:ExperienceswiththeFAQﬁndersystem.AIMagazine,18(2):57–66.KobyCrammer,OferDekel,JosephKeshet,ShaiShalev-Shwartz,andYo-ramSinger.2006.Onlinepassive-agressivealgorithms.MachineLearning,7:551–585.PabloArielDuboueandJenniferChu-Carroll.2006.An-sweringthequestionyouwishtheyhadasked:Theim-pactofparaphrasingforquestionanswering.InPro-ceedingsof(HLT-NAACL’06),NewYork,NY.AbdessamadEchihabiandDanielMarcu.2003.Anoisy-channelapproachtoquestionanswering.InProceedingsof(ACL’03),Sapporo,Japan.SandaHarabagiuandFinleyLacatusu.2004.Strategiesforadvancedquestionanswering.InProceedingsoftheHLT-NAACL’04WorkshoponPragmaticsofQues-tionAnswering,Boston,MA.SandaHarabagiu,DanMoldovan,MariusPas¸ca,RadaMihalcea,MihaiSurdeanu,R˘azvanBunescu,RoxanaGˆırju,VasileRus,andPaulMor˘arescu.2001.Theroleoflexico-semanticfeedbackinopen-domaintex-tualquestion-answering.InProceedingsof(ACL’01),Toulouse,France.UlfHermjakob,AbdessamadEchihabi,andDanielMarcu.2002.Naturallanguagebasedreformulationresourceandwebexploitationforquestionanswering.InProceedingsofTREC-11,Gaithersburg,MD.EduardHovy,LaurieGerber,UlfHermjakob,MichaelJunk,andChin-YewLin.2000.Questionansweringinwebclopedia.InProceedingsofTREC9,Gaithers-burg,MD.AbrahamIttycheriah,MartinFranz,andSalimRoukos.2001.IBM’sstatisticalquestionansweringsystem.InProceedingsofTREC10,Gaithersburg,MD.ValentinJijkounandMaartendeRijke.2005.Retrievinganswersfromfrequentlyaskedquestionspagesontheweb.InProceedingsoftheTenthACMConferenceonInformationandKnowledgeManagement(CIKM’05),Bremen,Germany.ThorstenJoachims.2003.Transductivelearningviaspectralgraphpartitioning.InProceedingsofICML’03,Washington,DC.PhilippKoehn,FranzJosefOch,andDanielMarcu.2003.Statisticalphrase-basedtranslation.InProceed-ingsof(HLT-NAACL’03),Edmonton,Cananda.DekangLinandPatrickPantel.2001.Discoveryofinfer-encerulesforquestionanswering.JournalofNaturalLanguageEngineering,7(3):343–360.FranzJosefOchandHermannNey.2004.Thealign-menttemplateapproachtostatisticalmachinetransla-tion.ComputationalLinguistics,30(4):417–449.FranzJosefOch.2003.Minimumerrorratetraininginstatisticalmachinetranslation.InProceedingsof(HLT-NAACL’03),Edmonton,Cananda.JohnPrager,JenniferChu-Carroll,andKrysztofCzuba.2001.Useofwordnethypernymsforansweringwhat-isquestions.InProceedingsofTREC10,Gaithers-burg,MD.YonggangQiuandH.P.Frei.1993.Conceptbasedqueryexpansion.InProceedingsofSIGIR’93,Pittsburgh,PA.DragomirR.Radev,HongQi,ZhipingZheng,SashaBlair-Goldensohn,ZhuZhang,WeigoFan,andJohnPrager.2001.Miningthewebforanswerstonatu-rallanguagequestions.InProceedingsof(CIKM’01),Atlanta,GA.RaduSoricutandEricBrill.2006.Automaticquestionansweringusingtheweb:Beyondthefactoid.JournalofInformationRetrieval-SpecialIssueonWebInfor-mationRetrieval,9:191–206.EllenM.Voorhees.1994.Queryexpansionusinglexical-semanticrelations.InProceedingsofSI-GIR’94,Dublin,Ireland.JinxiXuandW.BruceCroft.1996.Queryexpansionusinglocalandglobaldocumentanalysis.InProceed-ingsofSIGIR’96,Zurich,Switzerland.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 472–479,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

472

AComputationalModelofTextReuseinAncientLiteraryTextsJohnLeeSpokenLanguageSystemsMITComputerScienceandArti(cid:2)cialIntelligenceLaboratoryCambridge,MA02139,USAjsylee@csail.mit.eduAbstractWeproposeacomputationalmodeloftextreusetailoredforancientliterarytexts,avail-abletousoftenonlyinsmallandnoisysam-ples.Themodeltakesintoaccountsourcealternationpatterns,soastobeabletoalignevensentenceswithlowsurfacesimilarity.WedemonstrateitsabilitytocharacterizetextreuseintheGreekNewTestament.1IntroductionTextreuseisthetransformationofasourcetextintoatargettextinordertoserveadifferentpurpose.Pastresearchhasaddressedavarietyoftext-reuseappli-cations,including:journaliststurninganewsagencytextintoanewspaperstory(Cloughetal.,2002);ed-itorsadaptinganencyclopediaentrytoanabridgedversion(BarzilayandElhadad,2003);andplagia-rizersdisguisingtheirsourcesbyremovingsurfacesimilarities(Uzuneretal.,2005).Acommonassumptionintherecoveryoftextreuseistheconservationofsomedegreeoflexi-calsimilarityfromthesourcesentencetothede-rivedsentence.Asimpleapproach,then,istode-(cid:2)nealexicalsimilaritymeasureandestimateascorethreshold;givenasentenceinthetargettext,ifthehighest-scoringsentenceinthesourcetextisabovethethreshold,thentheformerisconsideredtobede-rivedfromthelatter.Obviously,theeffectivenessofthisbasicapproachdependsonthedegreeoflexicalsimilarity:sourcesentencesthatarequotedverba-timareeasiertoidentifythanthosethathavebeentransformedbyaskillfulplagiarizer.Thecruxofthequestion,therefore,ishowtoidentifysourcesentencesdespitetheirlackofsur-facesimilaritytothederivedsentences.Ancientlit-erarytexts,whicharethefocusofthispaper,presentsomedistinctivechallengesinthisrespect.1.1AncientLiteraryTexts(cid:147)Borrowedmaterialembeddedinthe(cid:3)owofawriter’stextisacommonphenomenoninAntiq-uity.(cid:148)(vandenHoek,1996).Ancientwritersrarelyacknowledgedtheirsources.Duetothescarcityofbooks,theyoftenneededtoquotefrommem-ory,resultingininexactquotations.Furthermore,theycombinedmultiplesources,sometimesinsert-ingnewmaterialorsubstantiallyparaphrasingtheirsourcestosuittheirpurpose.Tocompoundthenoise,theversionofthesourcetextavailabletoustodaymightnotbethesameastheoneoriginallyconsultedbytheauthor.Beforetheageoftheprint-ingpress,documentsweresusceptibletocorruptionsintroducedbycopyists.Identifyingthesourcesofancienttextsisuse-fulinmanyways.Ithelpsestablishtheirrelativedates.Ittracestheevolutionofideas.Thematerialquoted,leftoutoralteredinacompositionprovidesmuchinsightintotheagendaofitsauthor.AmongthemorefrequentlyquotedancientbooksarethegospelsintheNewTestament.Threeofthem(cid:151)thegospelsofMatthew,Mark,andLuke(cid:151)arecalledtheSynopticGospelsbecauseofthesubstantialtextreuseamongthem.473

Targetverses(Englishtranslation)Targetverses(originalGreek)Sourceverses(originalGreek)Luke9:30-33Luke9:30-33Mark9:4-5(9:30)And,behold,(9:30)kaiidou(9:4)kaiﬂophthﬂeautoistheretalkedwithhimtwomen,andresduosunelalounautﬂoﬂEliassunMﬂouseikaiwhichwereMosesandElias.hoitinesﬂesanMﬂousﬂeskaiﬂEliasﬂesansullalountestﬂoIﬂesou(9:31)Whoappearedinglory,...(9:31)hoiophthentesendoxﬂe...(noobvioussourceverse)(9:32)ButPeterandtheythatwerewithhim...(9:32)hodePetroskaihoisunautﬂo...(noobvioussourceverse)(9:33)Anditcametopass,(9:33)kaiegenetoentﬂodiachﬂorizesthaiastheydepartedfromhim,autousap’autoueipenhoPetros(9:5)kaiapokritheishoPetrosPetersaiduntoJesus,Master,prostonIﬂesounepistatalegeitﬂoIﬂesourabbiitisgoodforustobehere:kalonestinhﬂemashﬂodeeinaikalonestinhﬂemashﬂodeeinaiandletusmakekaipoiﬂesﬂomenskﬂenastreiskaipoiﬂesﬂomentreisskﬂenasthreetabernacles;oneforthee,miansoikaimianMﬂouseisoimiankaiMﬂouseimianandoneforMoses,andoneforElias:kaimianﬂEliakaiﬂEliamiannotknowingwhathesaid.mﬂeeidﬂosholegeiTable1:Luke9:30-33andtheirsourceversesintheGospelofMark.TheGreekwordswithcommonstemsinthetargetandsourceversesarebolded.TheKingJamesVersionEnglishtranslationisincludedforreference.§1.2commentsonthetextreuseintheseverses.1.2SynopticGospelsThenatureoftextreuseamongtheSynopticsspansawidespectrum.Ontheonehand,somereveredverses,suchasthesayingsofJesusortheapostles,werepreservedverbatim.SuchisthecasewithPe-ter’sshortspeechinthesecondhalfofLuke9:33(seeTable1).Ontheotherhand,unimportantde-tailsmaybedeleted,andnewinformationweavedinfromothersourcesororaltraditions.Forex-ample,(cid:147)Lukeofteneditstheintroductionstonewsectionswiththegreatestindependence(cid:148)(Taylor,1972).Tocomplicatematters,itisbelievedbysomeresearchersthattheversionoftheGospelofMarkusedbyLukewasamoreprimitiveversion,cus-tomarilycalledProto-Mark,whichisnolongerex-tant(Boismard,1972).ContinuingourexampleinTable1,verses9:31-32havenoobviouscounter-partsintheGospelofMark.SomeresearchershaveattributedthemtoanearlierversionofMark(Bo-ismard,1972)ortoLuke’s(cid:147)redactionaltenden-cies(cid:148)(Bovon,2002).Theresultisthatsomeversesbearlittleresem-blancetotheirsources,duetoextensiveredaction,ortodiscrepanciesbetweendifferentversionsofthesourcetext.Inthe(cid:2)rstcase,anysurfacesimilarityscorealoneisunlikelytobeeffective.Inthesecond,evendeepsemanticanalysismightnotsuf(cid:2)ce.1.3GoalsOnepropertyoftextreusethathasnotbeenexploredinpastresearchissourcealternationpatterns.Forexample,(cid:147)itiswellknownthatsectionsofLukede-rivedfromMarkandthoseofotheroriginsarear-rangedincontinuousblocks(cid:148)(Cadbury,1920).Thisnotioncanbeformalizedwithfeaturesontheblocksandorderofthesourcesentences.The(cid:2)rstgoalofthispaperistoleveragesourcealternationpatternstooptimizetheglobaltextreusehypothesis.Scholarsofancienttextstendtoexpresstheiranalysesqualitatively.Weattempttotranslatetheirinsightsintoaquantitativemodel.Toourbestknowledge,thisisthe(cid:2)rstsentence-level,quantita-tivetext-reusemodelproposedforancienttexts.Oursecondgoalisthustobringaquantitativeapproachtosourceanalysisofancienttexts.2PreviousWorkTextreuseisanalyzedatthedocumentlevelin(Cloughetal.,2002),whichclassi(cid:2)esnewspaperarticlesaswholly,partially,ornon-derivedfromanewsagencytext.Thehapaxlegomena,andsentencealignmentbasedonN-gramoverlap,arefoundtobethemostusefulfeatures.Consideringadocumentasawholemitigatestheproblemoflowsimilarityscoresforsomeofthederivedsentences.474

12345678910111213141516123456789101112131415161718192021222324MarkLukeFigure1:Adot-plotofthecosinesimilaritymea-surebetweentheGospelofLukeandtheGospelofMark.Thenumberontheaxesrepresentchapters.Thethickdiagonallinesre(cid:3)ectregionsofhighlexi-calsimilaritybetweenthetwogospels.Atthelevelofshortpassagesorsentences,(Hatzi-vassiloglouetal.,1999)goesbeyondN-gram,tak-ingadvantageofWordNetsynonyms,aswellasor-deringanddistancebetweensharedwords.(Barzi-layandElhadad,2003)showsthatthesimplecosinesimilarityscorecanbeeffectivewhenusedincon-junctionwithparagraphclustering.Amoredetailedcomparisonwiththisworkfollowsin§4.2.Inthehumanities,reusedmaterialinthewrit-ingsofPlutarch(HelmboldandO’Neil,1959)andClement(vandenHoek,1996)havebeenmanuallyclassi(cid:2)edasquotations,reminiscences,referencesorparaphrases.StudiesontheSynopticshavebeenlimitedtoN-gramoverlap,notably(Honor·e,1968)and(Miyakeetal.,2004).TextHypothesisResearcherModelLtrainLtrain.B(Bovon,2002)BLtrain.J(Jeremias,1966)JLtestLtest.B(Bovon,2003)Ltest.J(Jeremias,1966)Table2:TwomodelsoftextreuseofMarkinLtrainaretrainedontwodifferenttext-reusehypotheses:TheBmodelisonthehypothesisin(Bovon,2002),andtheJmodel,on(Jeremias,1966).Thesetwomodelsthenpredictthetext-reuseinLtest.3DataWeassumetheTwo-DocumentTheory1,whichhy-pothesizesthattheGospelofLukeandtheGospelofMatthewhaveastheircommonsourcestwodoc-uments:theGospelofMark,andalosttextcustom-arilydenotedQ.Inparticular,wewillconsidertheGospelofLuke2asthetargettext,andtheGospelofMarkasthesourcetext.WeuseaGreekNewTestamentcorpuspreparedbytheCenterforComputerAnalysisofTextsattheUniversityofPennsylvania3,basedonthetextvari-antfromtheUnitedBibleSociety.Thetext-reusehypotheses(i.e.,listsofversesdeemedtobede-rivedfromMark)ofFranc‚oisBovon(Bovon,2002;Bovon,2003)andJoachimJeremias(Jeremias,1966)areused.Table2presentsournotations.Luke1:1to9:50(Ltrain,458verses)Chapters1and2,narrativesofthebirthsofJesusandJohntheBaptist,arebasedonnon-Markansources.Verses3:1to9:50describeJesus’activitiesinGalilee,asubstantialpartofwhichisderivedfromMark.LukeChapters22to24(Ltest,179verses)Thesechapters,knownasthePassionNarrative,serveasourtesttext.Markansourceswerebehind38%oftheverses,accordingtoBovon,and7%accordingtoJeremias.1Thistheory(Streeter,1930)iscurrentlyacceptedbyama-jorityofresearchers.Itguidesourchoiceofexperimentaldata,butourmodeldoesnotdependonitsvalidity.2WedonotconsidertheGospelofMattheworQinthisstudy.VersesfromLuke9:51totheendofchapter21arealsonotconsidered,sincetheirsourcesaredif(cid:2)culttoascertain(Bovon,2002).3ObtainedthroughPeterBallard(personalcommunication)475

4ApproachForeachverseinthetargettext(a(cid:147)targetverse(cid:148)),wewouldliketodeterminewhetheritisderivedfromaverseinthesourcetext(a(cid:147)sourceverse(cid:148))and,ifso,whichone.Followingtheframeworkofgloballinearmodelsin(Collins,2002),wecastthistaskaslearningamappingFfrominputversesx∈Xtoatext-reusehypothesisy∈Y∪{}.Xisthesetofversesinthetargettext.Inourcase,xtrain=(x1,...,x458)isthesequenceofversesinLtrain,andxtestisthatofLtest.Yisthesetofversesinthesourcetext.Saythesequencey=(y1,...,yn)isthetext-reusehypothesisforx=(x1,...,xn).Ifyiis,thenxiisnotderivedfromthesourcetext;otherwise,yiisthesourceverseforxi.ThesetofcandidatesGEN(x)containsallpossiblesequencesfory,andΘistheparametervector.ThemappingFisthus:F(x)=argmaxy∈GEN(x)Φ(x,y)·Θ4.1FeaturesGiventhesmallamountoftrainingdataavailable4,thefeaturespacemustbekeptsmalltoavoidover(cid:2)t-ting.Startingwiththecosinesimilarityscoreasthebaselinefeature,weprogressivelyenrichthemodelwiththefollowingfeatures:CosineSimilarity[Sim]Treatingatargetverseasaquerytothesetofsourceverses,wecom-putethecosinesimilarity,weightedwithtf.idf,foreachpairofsourceverseandtargetverse5.Thisstandardbag-of-wordsapproachisappro-priateforGreek,arelativelyfreeword-orderlanguage.Figure1plotsthisfeatureonLukeandMark.Non-derivedversesareassignedaconstantscoreinlieuofthecosinesimilarity.Wewillrefertothisconstantasthecosinethreshold(C):whentheSimfeaturealoneisused,theconstanteffectivelyactsasthethresholdabovewhichtargetversesareconsideredtobede-rived.Ifwi,wjarethevectorsofwordsofa4Notethatthetrainingsetconsistsofonlyonextrain(cid:151)theGospelofLuke.Luke’sonlyotherbook,theActsoftheApostles,containsfewidenti(cid:2)ablereusedmaterial.5Atargertverseisalsoallowedtomatchtwoconsecutivesourceverses.targetverseandacandidatesourceverse,then:sim(i,j)=(wi·wjkwik·kwjkifderivedCotherwiseNumberofBlocks[Block]LukecanbeviewedasalternatingbetweenMarkandnon-Markanmaterial,andhe(cid:147)preferstopickupal-ternativelyentireblocksratherthanisolatedunits.(cid:148)(Bovon,2002)WewillusethetermMarkanblocktorefertoasequenceofversesthatarederivedfromMark.Aversewithalowcosinescore,butpositionedinthemid-dleofaMarkanblock,islikelytobederived.Conversely,anisolatedverseinthemiddleofanon-Markanblock,evenwithahighcosinescore,isunlikelytobeso.Theheaviertheweightofthisfeature,thefewerblocksarepre-ferred.SourceProximity[Prox]Whentwoderivedversesareclosetooneanother,theirrespectivesourceversesarealsolikelytobeclosetooneanother;inotherwords,derivedversestendtoform(cid:147)continuousblocks(cid:148)(Cadbury,1920).Wede(cid:2)nedistanceasthenumberofversessep-aratingtwoverses.Foreachpairofconsec-utivetargetverses,wetaketheinverseofthedistancebetweentheirsourceverses.Thisfea-tureisthusintendedtodiscourageaderivedversefrombeingalignedwithasourceversethatsharessomelexicalsimilaritiesbychance,butisfarawayfromothersourceversesintheMarkanblock.SourceOrder[Order](cid:147)WheneverLukefollowstheMarkannarrativeinhisowngospelhefollowspainstakinglytheMarkanorder(cid:148),andhence(cid:147)deviationsintheorderofthematerialmustthereforeberegardedasindicationsthatLukeisnotfollowingMark.(cid:148)(Jeremias,1966).Thisfeatureisabinaryfunctionontwoconsec-utivederivedverses,indicatingwhethertheirsourceversesareinorder.Apositiveweightforthisfeaturewouldfavoranalignmentthatrespectstheorderofthesourcetext.Incaseswheretherearenoobvioussourceverses,suchasLuke9:30-31inTable1,thesourceorder476

andproximitywouldbedisrupted.Tomitigatethisissue,weallowtheProxandOrderfeaturestheoptionofskippinguptotwoverseswithinaMarkanblockinthetargettext.Inourexample,Luke9:30canskipto9:32,preservingthesourceproximityandorderbetweentheirsourceverses,Mark9:4and9:5.AnotherpotentialfeatureistheoccurrenceoffunctionwordscharacteristicofLuke(Rehkopf,1959),alongthesamelinesasinthestudyoftheFederalistPapers(MostellerandWallace,1964).Thesestylisticindicators,however,areunlikelytobeashelpfulonthesentencelevelasonthedocumentlevel.Furthermore,Luke(cid:147)reworks[hissources]toanextentthat,withinhisentirecomposi-tion,thesourcesrarelycometolightintheiroriginalindependentform(cid:148)(Bovon,2002).Thesigni(cid:2)canceofthepresenceoftheseindicators,therefore,isdi-minished.4.2DiscussionThismodelisbothasimpli(cid:2)cationofandanex-tensiontotheoneadvocatedin(BarzilayandEl-hadad,2003).Ontheonehand,weperformnopara-graphclusteringormappingbeforesentencealign-ment.Ancienttextsarerarelydividedintopara-graphs,noraretheylikelytobelargeenoughforstatisticalmethodsonclustering.Instead,werelyontheProxfeaturetoencouragesourceversestostayclosetoeachotherinthealignment.Ontheotherhand,ourmodelmakestwoexten-sionstothe(cid:147)MicroAlignment(cid:148)stepin(BarzilayandElhadad,2003).First,weaddtheBlockandProxfeaturestocapturesourcealternationpatterns.Second,weplacenohardrestrictionsonthere-orderingofthesourcetext,optinginsteadforasoftpreferenceformaintainingthesourceorderthroughtheOrderfeature.Incontrast,deviationfromthesourceorderislimitedto(cid:147)(cid:3)ips(cid:148)betweentwosen-tencesin(BarzilayandElhadad,2003),anassump-tionthatisnotvalidintheSynoptics6.4.3EvaluationMetricOurmodelcanmaketwotypesoferrors:sourceer-ror,whenitpredictsanon-derivedtargetversetobederived,orviceversa;andalignmenterror,when6Forexample,Luke6:12-19transposesMark3:7-12andMark3:13-19(Bovon,2002).itcorrectlypredictsatargetversetobederived,butalignsittothewrongsourceverse.Correspondingly,weinterprettheoutputofourmodelattwolevels:asabinaryoutput,i.e.,thetargetverseiseither(cid:147)derived(cid:148)or(cid:147)non-derived(cid:148);or,asanalignmentofthetargetversetoasourceverse.Wemeasuretheprecisionandrecallofthetargetversesatbothlevels,yieldingtwoF-measures,FsourceandFalign7.LiterarydependenciesintheSynopticsaretypi-callyexpressedaspairsofpericopes(short,coher-entpassages),forexample,(cid:147)Luke22:47-53//Mark14:43-52(cid:148).Likewise,forFalign,weconsidertheoutputcorrectifthehypothesizedsourceverselieswithinthepericope8.5ExperimentsThissectionpresentsexperimentsforevaluatingourtext-reusemodel.§5.1givessomeimplementa-tiondetails.§5.2describesthetrainingprocess,whichusestext-reusehypothesesoftwodifferentre-searchers(Ltrain.BandLtrain.J)onthesametrain-ingtext.ThetworesultingmodelsthusrepresenttwodifferentopinionsonhowLukere-usedMark;theythenproducetwohypothesesonthetesttext(ˆLtest.BandˆLtest.J).Evaluationsofthesehypothesesfollow.In§5.3,wecomparethemwiththehypothesesofthesametworesearchersonthetesttext(Ltest.BandLtest.J).In§5.3,wecomparethemwiththehypothesesofsevenotherrepresentativeresearchers(Neirynck,1973).Ideally,whenthemodelistrainedonapar-ticularresearcher’shypothesisonthetraintext,itshypothesisonthetesttextshouldbeclosesttotheoneproposedbythesameresearcher.5.1ImplementationSupposewealigntheithtargetversetothekthsourceverseorto.Usingdynamicprogramming,theirscoreisthecosinesimilarityscoresim(i,k),addedtothebestalignmentstateuptothe(i−1−skip)thtargetverse,whereskipcanvaryfrom0to2(see§4.1).Ifthejthsourceverseisthealigned7NotethatFalignisneverhigherthanFsourcesinceitpe-nalizesbothsourceandalignmenterrors.8Amore(cid:2)ne-grainedmetricisindividualversealignment.Thisisunfortunatelydif(cid:2)culttomeasure.Asdiscussedin§1.2,manyderivedverseshavenoclearsourceverses.477

ModelBJTrainHypLtrain.BLtrain.JMetricFsourceFalignFsourceFalignSim0.7600.6460.7480.635+Block0.9610.7280.9770.743All0.9850.9490.9830.936Table3:Performanceonthetrainingtext,Ltrain.Thefeaturesareaccumulative;Allreferstothefullfeatureset.verseinthisstate,thenscore(i,k)is:sim(i,k)+maxj,skip{score(i−1−skip,j)+wprox·prox(j,k)+worder·order(j,k)−wblock·block(j,k)}Ifbothjandkarealigned(i.e.,not),then:prox(j,k)=1dist(j,k)order(j,k)=1ifj≥kblock(j,k)=1ifstartingnewblockOtherwisethesearesettozero.5.2TrainingResultsThemodeltakesonlyfourparameters:theweightsfortheBlock,ProxandOrderfeatures,aswellasthecosinethreshold(C).Theyareempiricallyoptimized,accurateto0.01,onthetwotraininghy-potheseslistedinTable2,yieldingtwomodels,BandJ.Table3showstheincreasingaccuracyofbothmodelsindescribingthetextreuseinLtrainasmorefeaturesareincorporated.TheBlockfea-turecontributesmostinpredictingtheblockbound-aries,asseeninthejumpofFsourcefromSimto+Block.TheProxandOrderfeaturessubstan-tiallyimprovethealignment,boostingtheFalignfrom+BlocktoAll.BothmodelsBandJ(cid:2)ttheirrespectivehypothe-sestoveryhighdegrees.ForB,theonlysigni(cid:2)cantsourceerroroccursinLuke8:1-4,whicharederivedverseswithlowsimilarityscores.Theyaretransi-tionalversesatthebeginningofaMarkanblock.ForModelBJTestHypLtest.BLtest.JMetricFsourceFalignFsourceFalignSim0.5790.3820.1860.144+Block0.6710.3290.7430.400All0.7790.5650.8390.839Table5:Performanceonthetesttext,Ltest.J,thepericopeLuke6:12-16iswronglypredictedasderived.Mostalignmenterrorsaremisalignmentstoaneighboringpericope,typicallyforverseslocatedneartheboundarybetweentwopericopes.Duetotheirlowsimilarityscores,themodelwasunabletodecideiftheybelongtotheendoftheprecedingpericopeortothebeginningofthefollowingone.5.3TestResultsThetwomodelstrainedin§5.2,BandJ,areintendedtocapturethecharacteristicsoftextreuseinLtrainaccordingtotwodifferentresearchers.Whenap-pliedonthetesttext,Ltest,theyproducetwohy-potheses,ˆLtest.BandˆLtest.J.Ideally,theyshouldbesimilartothehypothesesofferedbythesamere-searchers(namely,Ltest.BandLtest.J),anddissim-ilartothosebyotherresearchers.Weanalyzethe(cid:2)rstaspectin§5.3,andthesecondaspectin§5.3.ComparisonwithBovonandJeremiasTable4showstheoutputofBandJonLtest.Asmorefeaturesareadded,theiroutputincreasinglyresembleLtest.BandLtest.J,asshowninTable5.BothˆLtest.BandˆLtest.JcontainthesamenumberofMarkanblocksasthe(cid:147)reference(cid:148)hypothesespro-posedbytherespectivescholars.Inbothcases,thepericopeLuke22:24-30iscorrectlyassignedasnon-derived,despitetheirrelativelyhighcosinescores.ThisillustratestheeffectoftheBlockfeature.Asforsourceerrors,bothBandJmistakenlyas-signLuke22:15-18asMarkan,attractedbythehighsimilarityscoreofLuke22:18withMark14:25.B,inaddition,attributesanotherpericopetoMarkwhereBovondoesnot.Despitethepenaltyoflowersourceproximity,itwronglyalignedLuke23:37-38toMark15:2,misledbyaspeci(cid:2)ctitleofJesusthathappenstobepresentinboth.478

Chp22.....................................................................23.....................................................Simxx--x-x-xxxxxx-xxxxx-xx----------x---xxx-x---xx--x-xxx-xxxxxx-xx-x-xxxxx-x--x--xx-----x--xxx--xx------x-x-xxx---xxxx---xxxxx--Allxxxxxxxxxxxxxxxxxx-------------------------------xxxxxxxxxxxxxxxxxxxxxxxxxxxx-----------------------------xxxxxxxxxxxxxxxxx---Bovxxxxxxxxxxxxxx--------------------------------xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx-------------------------------------xxxxxxxxxxxxxSimxx--x-x-xxxxxx-xxxxx-xx----------x---xxx-x---xx--x-xxx-xxxxxx-xx-x-xxxxx-x--x--xx-----x--xxx--xx------x-x-xxx---xxxx---xxxxx--Allxxxxxxxxxxxxxxxxxx------------------------------------------------------------------------------------------------------------Jerxxxxxxxxxxxxx-----------------------------------------------------------------------------------------------------------------Gruxxxxxxxxxxxxx-------xxx---------xx-----------------xx--------------------x-----------------------------xx--x-----xx-x---xxx---Hawxxxxxxxxxxxxx----x---x-------------------x---xx----xxx------x---------x--------------------x---x-------x---------xxx-----xx---Rehxxxxxxxxxxxxx------x-------------------------------xx------------------------------------------x------------------------------Snd--------------------xxx---------xx-----------------xxxxxxxxxx-------xxx-------------------------------------------------------Srmxxxxxxxxxxxxxx------xxx---------xx--------------------------------------------------------------------------------------------Strxxxxxxxxxxxxx----x---x-------------------x---xx----xxxxxxxxxx---------x--x-----------------x--xx------xx---x-----xxx-----xx---Tayxxxxxxxxxxxxx--------x-----------x-----------x---x-xxxxxxxxxx------------x---------------------x-------x---x-----xx---xxxxxx--Chp24...................................................Simxxx---x-xx---------x---x-------xxx-x---x--x-xxx-x----(ModelBSim)All-----------------------------------------------------(ModelBAll)Bovxxxxxxxxxxx------------------------------------------(Bovon)Simxxx---x-xx---------x---x-------xxx-x---x--x-xxx-x----(ModelJSim)All-----------------------------------------------------(ModelJAll)Jer-----------------------------------------------------(Jeremias)Gru-x---x-----------------------------------------------(Grundmann)Haw-----x-----------------------------------------------(Hawkins)Reh-----------------------------------------------------(Rehkopf)Snd-x---x---x-------------------------------------------(Schneider)Srm-----------------------------------------------------(Sch¤urmann)Str-----x-----------------------------------------------(Streeter)Tay---------x-------------------------------------------(Taylor)Table4:OutputofmodelsBandJ,andscholarlyhypothesesonthetesttext,Ltest.Thesymbol‘x’indicatesthattheverseisderivedfromMark,and‘-’indicatesthatitisnot.Thehypothesisfrom(Bovon,2003),labelled‘Bov’,iscomparedwiththeSim(baseline)outputandtheAlloutputofmodelB,asdetailedinTable5.Thehypothesisfrom(Jeremias,1966),‘Jer’,issimilarlycomparedwithoutputsofmodelJ.Sevenotherscholarlyhypothesesarealsolisted.Elsewhere,BismoreconservativethanBovoninproposingMarkanderivation.Forinstance,theperi-copeLuke24:1-11isdeemednon-derived,anopin-ion(partially)sharedbysomeoftheothersevenre-searchers.ComparisonwithOtherHypothesesAnotherwayofevaluatingtheoutputofBandJistocomparethemwiththehypothesesofotherresearchers.AsshowninTable6,ˆLtest.BismoresimilartoLtest.Bthantothehypothesisofotherresearchers9.Inotherwords,whenthemodelistrainedonBovon’stext-reusehypothesisonthetraintext,itspredictiononthetesttextmatchesmostcloselywiththatofthesameresearcher,Bovon.9ThisisthelistofresearcherswhoseopinionsonLtestareconsideredrepresentativeby(Neirynck,1973).Wehavesimpli(cid:2)edtheirhypotheses,consideringthose(cid:147)partiallyassim-ilated(cid:148)and(cid:147)re(cid:3)ectthein(cid:3)uenceofMark(cid:148)tobenon-derivedfromMark.HypothesisB(ˆLtest.B)J(ˆLtest.J)Bovon(Ltest.B)0.8380.676Jeremias(Ltest.J)0.7210.972Grundmann0.7260.866Hawkins0.7370.877Rehkopf0.7210.950Schneider0.6760.782Sch¤urmann0.6980.950Streeter0.7710.821Taylor0.7930.821Table6:ComparisonoftheoutputofthemodelsBandJwithhypothesesbyprominentresearcherslistedin(Neirynck,1973).Themetricistheper-centageofversesdeemedbybothhypothesestobe(cid:147)derived(cid:148),or(cid:147)non-derived(cid:148).479

ThedifferencesbetweenBovonandthenexttwomostsimilarhypotheses,TaylorandStreeter,arenotstatisticallysigni(cid:2)cantaccordingtoMcNemar’stest(p=0.27andp=0.10respectively),possi-blyare(cid:3)ectionofthesmallsizeofLtest;thedif-ferencesaresigni(cid:2)cant,however,withallotherhy-potheses(p<0.05).SimilarresultsareobservedforJeremiasandˆLtest.J.6Conclusion&FutureWorkWehaveproposedatext-reusemodelforancientliterarytexts,withnovelfeaturesthataccountforsourcealternationpatterns.Thesefeatureswereval-idatedontheLukanPassionNarrative,aninstanceoftextreuseintheGreekNewTestament.Themodel’spredictionsonthispassagearecom-paredtoninescholarlyhypotheses.Whentunedonthetext-reusehypothesisofacertainresearcheronthetraintext,itfavorsthehypothesisofthesamepersononthetesttext.Thisdemonstratesthemodel’sabilitytocapturetheresearcher’sparticularunderstandingoftextreuse.Whileacomputationalmodelaloneisunlikelytoprovidede(cid:2)nitiveanswers,itcanserveasasup-plementtolinguisticandliterary-criticalapproachestotext-reuseanalysis,andcanbeespeciallyhelp-fulwhendealingwithalargeamountofcandidatesourcetexts.AcknowledgementsThisworkgrewoutofatermprojectinthecourse(cid:147)GospelofLuke(cid:148),taughtbyProfessorFranc‚oisBovonatHarvardDivinitySchool.Ithasalsobene-(cid:2)tedmuchfromdiscussionswithDr.StevenLulich.ReferencesR.BarzilayandN.Elhadad.2003.SentenceAlign-mentforMonolingualComparableCorpora.Proc.EMNLP.M.E.Boismard.1972.SynopsedesquatreEvangilesenfranc‚ais,TomeII.EditionsduCerf,Paris,France.F.Bovon.2002.LukeI:ACommentaryontheGospelofLuke1:1-9:50.Hermeneia.FortressPress.Min-neapolis,MN.F.Bovon.2003.TheLukanStoryofthePassionofJesus(Luke22-23).StudiesinEarlyChristianity.BakerAcademic,GrandRapids,MI.H.J.Cadbury.1920.TheStyleandLiteraryMethodofLuke.HarvardTheologicalStudies,NumberVI.GeorgeF.MooreandJamesH.RopesandKirsoppLake(ed).HarvardUniversityPress,Cambridge,MA.P.Clough,R.Gaizauskas,S.S.L.PiaoandY.Wilks.2002.METER:MEasuringTExtReuse.Proc.ACL.M.Collins.2002.DiscriminativeTrainingMethodsforHiddenMarkovModels:TheoryandExperimentswithPerceptronAlgorithms.Proc.EMNLP.V.Hatzivassiloglou,J.L.KlavansandE.Eskin.1999.DetectingTextSimilarityoverShortPassages:Ex-ploringLinguisticFeatureCombinationsviaMachineLearning.Proc.EMNLP.W.C.HelmboldandE.N.O’Neil.1959.Plutarch’sQuotations.PhilologicalMonographsXIX,AmericanPhilologicalAssociation.A.M.Honor·e.1968.AStatisticalStudyoftheSynopticProblem.NovumTestamentum,Vol.10,p.95-147.J.Jeremias.1966.TheEucharisticWordsofJesus.Scribner’s,NewYork,NY.M.Miyake,H.Akama,M.Sato,M.NakagawaandN.Makoshi.2004.Tele-SynopsisforBiblicalResearch:DevelopmentofNLPbasedSynopticSoftwareforTextAnalysisasaMediatorofEducationalTechnologyandKnowledgeDiscovery.Proc.IEEEInternationalCon-ferenceonAdvancedLearningTechnologies(ICALT).F.MostellerandD.L.Wallace.1964.InferenceandDis-putedAuthorship:TheFederalist.AddisonWesley,Reading,MA.F.Neirynck.1973.Lamati(cid:30)eremarciennedansl’·evangiledeLuc.L’·EvangiledeLuc,Probl(cid:30)emeslitt·erairesetth·eologiques.EditionsDuculot,Belgium.F.Rehkopf.1959.DielukanischeSonderquelle.Wis-senschaftlicheUntersuchungenzumNeuenTestament,Vol.5.T¤ubingen,Germany.B.H.Streeter.1930.TheFourGospels:AStudyofOri-gins.MacMillan.London,England.V.Taylor.1972.ThePassionNarrativeofSt.Luke:ACriticalandHistoricalInvestigation.SocietyforNewTestamentStudiesMonographSeries,Vol.19.Cam-bridgeUniversityPress,Cambridge,England.O.Uzuner,B.KatzandT.Nahnsen.2005.UsingSyn-tacticInformationtoIdentifyPlagiarism.Proc.2ndWorkshoponBuildingEducationalApplicationsusingNLP.AnnArbor,MI.A.vandenHoek.1996.TechniquesofQuotationinClementofAlexandria(cid:151)AViewofAncientLiteraryWorkingMethods.VigiliaeChristianae,Vol50,p.223-243.E.J.Brill,Leiden,TheNetherlands.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 480–487,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

480

FindingdocumenttopicsforimprovingtopicsegmentationOlivierFerretCEALIST,LIC2M18routeduPanorama,BP6FontenayauxRoses,F-92265Franceferreto@zoe.cea.frAbstractTopicsegmentationandidentiﬁcationareof-tentackledasseparateproblemswhereastheyarebothpartoftopicanalysis.Inthisarticle,westudyhowtopicidentiﬁcationcanhelptoimproveatopicsegmenterbasedonwordreiteration.Weﬁrstpresentanunsu-pervisedmethodfordiscoveringthetopicsofatext.Then,wedetailhowthesetopicsareusedbysegmentationforﬁndingtopicalsimilaritiesbetweentextsegments.Finally,weshowthroughtheresultsofanevaluationdonebothforFrenchandEnglishtheinter-estofthemethodwepropose.1IntroductionInthisarticle,weaddresstheproblemoflineartopicsegmentation,whichconsistsinsegmentingdoc-umentsintotopicallyhomogeneoussegmentsthatdoesnotoverlapeachother.ThispartoftheDis-courseAnalysisﬁeldhasreceivedaconstantinterestsincetheinitialworkinthisdomainsuchas(Hearst,1994).Onecriterionforclassifyingtopicsegmen-tationsystemsisthekindofknowledgetheyde-pendon.Mostofthemonlyrelyonsurfacefeaturesofdocuments:wordreiterationin(Hearst,1994;Choi,2000;UtiyamaandIsahara,2001;Galleyetal.,2003)ordiscoursecuesin(PassonneauandLit-man,1997;Galleyetal.,2003).Assuchsystemsdonotrequireexternalknowledge,theyarenotsensi-tivetodomainsbuttheyarelimitedbythetypeofdocumentstheycanbeappliedto:lexicalreiterationisreliableonlyifconceptsarenottoofrequentlyex-pressedbyseveralmeans(synonyms,etc.)anddis-coursecuesareoftenrareandcorpus-speciﬁc.Toovercomethesedifﬁculties,somesystemsmakeuseofdomain-independentknowledgeaboutlexicalcohesion:alexicalnetworkbuiltfromadic-tionaryin(Kozima,1993);athesaurusin(Mor-risandHirst,1991);alargesetoflexicalco-occurrencescollectedfromacorpusin(Choietal.,2001).Toacertainextent,theselexicalnetworksenabletopicsegmenterstoexploitasortofconceptreiteration.However,theirlackofanyexplicittopi-calstructuremakesthiskindofknowledgedifﬁculttousewhenlexicalambiguityishigh.Themostsimplesolutiontothisproblemistoex-ploitknowledgeaboutthetopicsthatmayoccurindocuments.Suchtopicmodelsaregenerallybuiltfromalargesetofexampledocumentsasin(Yam-ronetal.,1998),(BleiandMoreno,2001)orinonecomponentof(Beefermanetal.,1999).Thesesta-tisticaltopicmodelsenablesegmenterstoimprovetheirprecisionbuttheyalsorestricttheirscope.Hybridsystemsthatcombinetheapproacheswehavepresentedwerealsodevelopedandillus-tratedtheinterestofsuchacombination:(Job-binsandEvett,1998)combinedwordrecurrence,co-occurrencesandathesaurus;(Beefermanetal.,1999)reliedonbothlexicalmodelinganddiscoursecues;(Galleyetal.,2003)madeuseofwordreitera-tionthroughlexicalchainsanddiscoursecues.Theworkwereportinthisarticletakesplaceintheﬁrstcategorywehavepresented.Itdoesnotrelyonanyaprioriknowledgeandexploitswordusageratherthandiscoursecues.Moreprecisely,wepresentanewmethodforenhancingtheresults481

ofsegmentationsystemsbasedonwordreiterationwithoutrelyingonanyexternalknowledge.2PrinciplesInmostofthealgorithmsinthetextsegmentationﬁeld,documentsarerepresentedassequencesofba-sicdiscourseunits.Whentheyarewrittentexts,theseunitsaregenerallysentences,whichisalsothecaseinourwork.Eachunitisturnedintoavectorofwords,followingtheprinciplesoftheVectorSpacemodel.Then,thesimilaritybetweenthebasicunitsofatextisevaluatedbycomputingasimilaritymea-surebetweenthevectorsthatrepresentthem.Suchasimilarityisconsideredasrepresentativeofthetop-icalclosenessofthecorrespondingunits.Thisprin-cipleisalsoappliedtogroupsofbasicunits,suchastextsegments,becauseofthepropertiesoftheVec-torSpacemodel.Segmentsareﬁnallydelimitedbylocatingtheareaswherethesimilaritybetweenunitsorgroupsofunitsisweak.Thisquickoverviewhighlightstheimportantroleoftheevaluationofthesimilaritybetweendiscourseunitsinthesegmentationprocess.Whennoexter-nalknowledgeisused,thissimilarityisonlybasedonthestrictreiterationofwords.Butitcanbeen-hancedbytakingintoaccountsemanticrelationsbe-tweenwords.Thiswasdoneforinstancein(JobbinsandEvett,1998)bytakingsemanticrelationsfromRoget’sThesaurus.Thisresourcewasalsousedin(MorrisandHirst,1991)wherethesimilaritybe-tweendiscourseunitswasmoreindirectlyevaluatedthroughthelexicalchainstheyshare.Thesameap-proachwasadoptedin(Stokesetal.,2002)butwithWordNetasthereferencesemanticresource.Inthisarticle,weproposetoimprovethedetec-tionoftopicalsimilaritybetweentextsegmentsbutwithoutrelyingonanyexternalknowledge.Foreachtexttosegment,weﬁrstidentifyitstopicsbyper-forminganunsupervisedclusteringofitswordsac-cordingtotheirco-occurrentsinthetext.Thus,eachofitstopicsisrepresentedbyasubsetofitsvocab-ulary.Whenthesimilaritybetweentwosegmentsisevaluatedduringsegmentation,thewordstheyshareareﬁrstconsideredbutthepresenceofwordsofthesametopicisalsotakenintoaccount.Thismakesitpossibletoﬁndsimilartwosegmentsthatrefertothesametopicalthoughtheydonotsharealotofwords.Itisalsoawaytoexploitlong-rangerela-tionsbetweenwordsatalocallevel.Moreglobally,ithelpstoreducethefalsedetectionoftopicshifts.3UnsupervisedTopicIdentiﬁcationTheapproachweproposeﬁrstrequirestodiscoverthetopicsoftexts.Forperformingsuchataskwith-outusingaprioriknowledge,weassumethatthemostrepresentativewordsofeachofthetopicsofatextoccurinsimilarcontexts.Hence,foreachwordofthetextwithaminimalfrequency,wecol-lectitsco-occurrents,weevaluatethepairwisesimi-larityoftheseselectedtextwordsbyrelyingontheirco-occurrentsandﬁnally,webuildtopicsbyapply-inganunsupervisedclusteringmethodtothem.3.1BuildingthesimilaritymatrixoftextwordsTheﬁrststepfordiscoveringthetopicsofatextisalinguisticpre-processingofit.Thispre-processingsplitsthetextintosentencesandrepresentseachofthemasthesequenceofitslemmatizedplainwords,thatis,nouns(properandcommonnouns),verbsandadjectives.Afterﬁlteringthelowfrequencywordsofthetext(frequency<3),theco-occurrentsoftheremainingwordsareclassicallycollectedbyrecordingtheco-occurrencesinaﬁxed-sizewin-dow(15plainwords)movedoverthepre-processedtext.Asaresult,eachtextwordisrepresentedbyavectorthatcontainsitsco-occurrentsandtheirco-occurrencefrequency.Thepairwisesimilaritybe-tweenalltheselectedtextwordsisthenevaluatedforbuildingtheirsimilaritymatrix.WeclassicallyapplytheCosinemeasurebetweenthevectorsthatrepresentthemforthisevaluation.3.2FromasimilaritymatrixtotexttopicsTheﬁnalstepfordiscoveringthetopicsofatextistheunsupervisedclusteringofitswordsfromtheirsimilaritymatrix.Werelyforthistaskonanadap-tationoftheSharedNearestNeighbor(SNN)algo-rithmdescribedin(Ertözetal.,2001).Thisalgo-rithmparticularlyﬁtsourneedsasitautomaticallydeterminesthenumberofclusters–inourcasethenumberoftopicsofatext–anddoesnottakeintoaccounttheelementsthatarenotrepresentativeoftheclustersitbuilds.Thislastpointisimportantforourapplicationasalltheplainwordsofatextarenotrepresentativeofitstopics.TheSNNalgorithm482

yearyearlybovinecasebecomeBEShumanmarketpairskiswissanimalcarcassdeclarefederalinfectdiseasedirectorindicateshakingcowmakerproductionstreulelastmadcompanystockliFigure1:Similaritygraphafteritssparsiﬁcation(seeAlgorithm1)performsclusteringbydetectinghigh-densityareasinasimilaritygraph.Inourcase,thesimilaritygraphisdirectlybuiltfromthesimi-laritymatrix:eachvertexrepresentsatextwordandanedgelinkstwowordswhosesimilarityisnotnull.TheSNNalgorithmsplitsupintotwomainstages:theﬁrstoneﬁndstheelementsthatarethemostrep-resentativeoftheirneighborhood.Theseelementsaretheseedsoftheﬁnalclustersthatarebuiltinthesecondstagebyaggregatingtheremainingelementstothoseselectedbytheﬁrststage.ThisﬁrststageAlgorithm1SNNalgorithm1.sparsiﬁcationofthesimilaritygraph2.buildingoftheSNNgraph3.computationofthedistributionofstronglinks4.searchfortopicseedsandﬁlteringofnoise5.buildingoftexttopics6.removalofinsigniﬁcanttopics7.extensionoftexttopicsstartsbysparsifyingthesimilaritygraph,whichisdonebykeepingonlythelinkstowardsthek(k=10)mostsimilarneighborsofeachtextword(step1).Figure1showstheresultinggraphforatwo-topicdocumentofourevaluationframework(seeSec-tion5.1).Then,thesimilaritygraphistransposedintoasharednearestneighbor(SNN)graph(step2).Inthisgraph,thesimilaritybetweentwowordsisgivenbythenumberofdirectneighborstheyshareinthesimilaritygraph.Thistranspositionmakesthesimilarityvaluesmorereliable,especiallyforhigh-dimensionaldataliketextualdata.StronglinksintheSNNgraphareﬁnallydetectedbyapplyingaﬁxedthresholdtothedistributionofsharedneigh-bornumbers(step3).Awordwithahighnumberofstronglinksistakenastheseedofatopicasitisrepresentativeofthesetofwordsthatarelinkedtoit.Onthecontrary,awordwithfewstronglinksissupposedtobeoutlier(step4).ThesecondstageoftheSNNalgorithmﬁrstbuildstexttopicsbyassociatingtotopicseedstheremainingwordsthatarethemostsimilartothemprovidedthattheirnumberofsharedneighborsishighenough(step5).Moreover,theseedsthatarejudgedastooclosetoeachotherarealsogroupedduringthisstepinaccordancewiththesamecrite-ria.Thelasttwostepsbringsmallimprovementstotheresultsofthisclustering.First,whenthenum-berofwordsofatopicistoosmall(size<3),thistopicisjudgedasinsigniﬁcantanditisdiscarded(step6).Itswordsareaddedtothesetofwordswith-outtopicafterstep5.WeaddedthissteptotheSNNalgorithmtobalancethefactthatwithoutanyex-ternalknowledge,allthesemanticrelationsbetweentextwordscannotbefoundbyrelyingonlyonco-occurrence.Finally,theremainingtexttopicsareextendedbyassociatingtothemthewordsthatareneithernoisenoralreadypartofatopic(step7).Astopicsaredeﬁnedatthispointmorepreciselythanatstep4,theintegrationofwordsthatarenotstronglylinkedtoatopicseedcanbesafelyperformedbyrelyingontheaveragestrengthoftheirlinksintheSNNgraphwiththewordsofthetopic.AftertheSNNalgorithmisapplied,asetoftopicsisassoci-atedtothetexttosegment,eachofthembeingde-ﬁnedasasubsetofitsvocabulary.4UsingTextTopicsforSegmentation4.1TopicsegmentationusingwordreiterationAsTextTiling,thetopicsegmentationmethodofHearst(Hearst,1994),thetopicsegmenterwepro-pose,calledF06,ﬁrstevaluatesthelexicalcohesionoftextsandthenﬁndstheirtopicshiftsbyiden-tifyingbreaksinthiscohesion.Theﬁrststepofthisprocessisthelinguisticpre-processingoftexts,whichisidenticalfortopicsegmentationtothepre-483

processingdescribedinSection3.1forthediscover-ingoftexttopics.Theevaluationofthelexicalcohe-sionofatextreliesasforTextTilingonaﬁxed-sizefocuswindowthatismovedoverthetexttosegmentandstopsateachsentencebreak.Thecohesioninthepartoftextdelimitedbythiswindowisevalu-atedbymeasuringthewordreiterationbetweenitstwosides.ThisisdoneinourcasebyapplyingtheDicecoefﬁcientbetweenthetwosidesofthefocuswindow,following(JobbinsandEvett,1998).Thiscohesionvalueisassociatedtothesentencebreakatthetransitionbetweenthetwosidesofthewindow.Moreprecisely,ifWlreferstothevocabularyoftheleftsideofthefocuswindowandWrreferstothevocabularyofitsrightside,thecohesioninthewin-dowatpositionxisgivenby:LCrec(x)=2·card(Wl∩Wr)card(Wl)+card(Wr)(1)ThismeasurewasadoptedinsteadoftheCosinemeasureusedinTextTilingbecauseitsdeﬁnitionintermsofsetsmakesiteasiertoextendfortakingintoaccountothertypesofrelations,asin(JobbinsandEvett,1998).Acohesionvalueiscomputedforeachsentencebreakofthetexttosegmentandtheﬁnalresultisacohesiongraphofthetext.ThelastpartofouralgorithmismainlytakenfromtheLCsegsystem(Galleyetal.,2003)andisdividedintothreesteps:•computationofascoreevaluatingtheprobabil-ityofeachminimumofthecohesiongraphtobeatopicshift;•removalofsegmentswithatoosmallsize;•selectionoftopicshifts.Thecomputationofthescoreofaminimummbe-ginsbyﬁndingthepairofmaximalandraroundit.Thisscoreisthengivenby:score(m)=LC(l)+LC(r)−2·LC(m)2(2)Thisscore,whosevaluesarebetween0and1,isameasureofhowhighisthedifferencebetweentheminimumandthemaximaaroundit.Hence,itfa-vorsaspossibletopicshiftsminimathatcorrespondtosharpfallsoflexicalcohesion.Thenextstepisdonebyremovingasapossibletopicshifteachminimumthatisnotfartherthan2sentencesfromitsprecedingneighbor.Finally,theselectionoftopicshiftsisperformedbyapplyingathresholdcomputedfromthedistributionofmini-mumscores.Thus,aminimummiskeptasatopicshiftifscore(m)>µ−α·σ,whereµistheaverageofminimumscores,σtheirstandarddeviationandαisamodulator(α=0.6inourexperiments).4.2UsingtexttopicstoenhancesegmentationTheheartofthealgorithmwehavepresentedaboveistheevaluationoflexicalcohesioninthefocuswin-dow,asgivenbyEquation1.Thisevaluationisalsoaweakpointascard(Wl∩Wr)onlyreliesonwordreiteration.Asaconsequence,twodifferentwordsthatrespectivelybelongstoWlandWrbutalsobelongtothesametexttopiccannotcontributetotheidentiﬁcationofapossibletopicalsimilaritybetweenthetwosidesofthefocuswindow.ThealgorithmF06Tisbasedonthesameprinci-plesasF06butitextendstheevaluationoflexicalcohesionbytakingintoaccountthetopicalproxim-ityofwords.ThereferencetopicsforjudgingthisproximityareofcoursethetexttopicsdiscoveredbythemethodofSection3.Inthisextendedversion,theevaluationofthecohesioninthefocuswindowismadeofthreesteps:•computationofthewordreiterationcohesion;•determinationofthetopic(s)ofthewindow;•computationofthecohesionbasedontexttop-icsandfusionofthetwokindsofcohesion.TheﬁrststepisidenticaltothecomputationofthecohesioninF06.Thesecondoneaimsatrestrict-ingthesetoftopicsthatareusedinthelaststeptothetopicsthatareactuallyrepresentativeofthecontentofthefocuswindow,i.e.representativeofthecurrentcontextofdiscourse.Thispointisespe-ciallyimportantintheareaswherethecurrenttopicischangingbecauseamplifyingtheinﬂuenceofthesurroundingtopicscanleadtothetopicshiftbeingmissed.Hence,atopicisconsideredasrepresen-tativeofthecontentofthefocuswindowonlyifitmatcheseachsideofthiswindow.Inpractice,thismatchingisevaluatedbyapplyingtheCosinemea-surebetweenthevectorthatrepresentsonesideof484

thewindowandthevectorthatrepresentsthetopic1andbytestingiftheresultingvalueishigherthanaﬁxedthreshold(equalto0.1intheexperimentsofSection5).Itmustbenotedthatseveraltopicsmaybeassociatedtothefocuswindow.Asthediscov-eringoftexttopicsisdoneinanunsupervisedwayandwithoutanyexternalknowledge,athemeofatextmaybescatteredoverseveralidentiﬁedtopicsandthen,itspresencecanbecharacterizedbysev-eralofthem.Thelaststepofthecohesionevaluationﬁrstcon-sistsindeterminingforeachsideofthefocuswin-dowthenumberofitswordsthatbelongtooneofthetopicsassociatedtothewindow.ThecohesionofthewindowisthengivenbyEquation3,thates-timatesthesigniﬁcanceofthepresenceofthetexttopicsinthewindow:LCtop(x)=card(TWl)+card(TWr)card(Wl)+card(Wr)(3)whereTWi∈{l,r}=(Wi∩Tw)−(Wl∩Wr)andTwistheunionofalltherepresentationsofthetopicsassociatedtothewindow.TWicorrespondstothewordsoftheisideofthewindowthatbelongtothetopicsofthewindow(Wi∩Tw)butarenotpartofthevocabularyfromwhichthelexicalcohesionbasedonwordreiterationiscomputed(Wl∩Wr).Finally,theglobalcohesioninthefocuswindowiscomputedasthesumofthetwokindsofcohesion,theonecomputedfromwordreiteration(seeEqua-tion1)andtheonecomputedfromtexttopics(seeEquation3).5Evaluation5.1EvaluationframeworkThemainobjectiveofourevaluationwastoverifythattakingintoaccounttexttopicsdiscoveredwith-outrelyingonexternalknowledgecanactuallyim-proveatopicsegmentationalgorithmthatisinitiallybasedonwordreiteration.SincetheworkofChoi(Choi,2000),theevaluationframeworkheproposedhasbecomeakindofstandardfortheevaluationoftopicsegmentationalgorithms.Thisframeworkis1Eachwordofthetopicvectorhasaweightequalto1.Inthewindowvector,thisweightisequaltothefrequencyofthewordinthecorrespondingsideofthewindow.basedonthebuildingofartiﬁcialtextsmadeofseg-mentsextractedfromdifferentdocuments.Ithasatleasttwoadvantages:thereferencecorpusiseasytobuildasitdoesnotrequirehumanannotations;parameterssuchasthesizeofthedocumentsorthesegmentscanbepreciselycontrolled.Butithasalsoanobviousdrawback:itstextsareartiﬁcial.Thisisaprobleminourcaseasouralgorithmfordiscoveringtexttopicsexploitsthefactthatthewordsofatopictendtoco-occuratthedocumentscale.Thishypoth-esisisnolongervalidfordocumentsbuiltaccord-ingtotheprocedureofChoi.Itiswhyweadaptedhisframeworkforhavingmorerealisticdocumentswithoutlosingitsadvantages.Thisadaptationcon-FrenchEnglish#sourcedoc.12887#sourcetopics113segments/doc.10(84%)10(97%)8(16%)8(3%)sentences/doc.6568plainwords/doc.797604Table1:Dataaboutourevaluationcorporacernsthewaythedocumentsegmentsareselected.Insteadoftakingeachsegmentfromadifferentdoc-ument,weonlyusetwosourcedocuments.Eachofthemissplitintoasetofsegmentswhosesizeisbe-tween3and11sentences,asforChoi,andaneval-uationdocumentisbuiltbyconcatenatingtheseseg-mentsinanalternatewayfromthebeginningofthesourcedocuments,i.e.onesegmentfromasourcedocumentandthefollowingfromtheotherone,un-til10segmentsareextracted.Moreover,inordertobesurethattheboundarybetweentwoadjacentsegmentsofanevaluationdocumentactuallycorre-spondstoatopicshift,thesourcedocumentsarese-lectedinsuchawaythattheyrefertodifferenttop-ics.ThispointwascontrolledinourcasebytakingdocumentsfromthecorpusoftheCLEF2003eval-uationforcrosslingualinformationretrieval:eachevaluationdocumentwasbuiltfromtwosourcedoc-umentsthathadbeenjudgedasrelevantfortwodif-ferentCLEF2003topics.Twoevaluationcorporamadeof100documentseach,oneinFrenchandoneinEnglish,werebuiltfollowingthisprocedure.Ta-ble1showstheirmaincharacteristics.485

5.2TopicidentiﬁcationAsF06Texploitsdocumenttopics,wealsoevalu-atedourmethodfortopicidentiﬁcation.Thisevalu-ationisbasedonthecorpusoftheprevioussection.Foreachofitsdocuments,areferencetopicisbuiltfromeachgroupofsegmentsthatcomefromthesamesourcedocumentbygatheringthewordsthatonlyappearinthesesegments.Areferencetopicisassociatedtothediscoveredtopicthatshareswithitthelargestnumberofwords.Threecomplementarymeasureswerecomputedtoevaluatethequalityofdiscoveredtopics.Themainoneispurity,whichisclassicallyusedforunsupervisedclustering:Purity=kXi=1viVP(Tdi)(4)whereP(Tdi),thepurityofthediscoveredtopicTdi,isequaltothefractionofthevocabularyofTdithatispartofthevocabularyofthereferencetopicTdiisassignedto,Visthevocabularyofallthedis-coveredtopicsandviisthevocabularyofTdi.Thesecondmeasureevaluatestowhatextenttherefer-encetopicsarerepresentedamongthediscoveredtopicsandisequaltotheratiobetweenthenum-berofdiscoveredtopicsthatareassignedtoarefer-encetopic(assigneddiscoveredtopics)andthenum-berofreferencetopics.Thelastmeasureestimateshowstronglythevocabularyofreferencetopicsispresentamongthediscoveredtopicsandisequaltotheratiobetweenthesizeofthevocabularyoftheassigneddiscoveredtopicsandthesizeofthevo-cabularyofreferencetopics.Table2givesthemeanpurityreferencetopics(%)ref.topicvocab.(%)French0.771(0.117)89.5(23.9)29.9(7.8)English0.766(0.082)99.0(10.0)31.6(5.3)Table2:Evaluationoftopicidentiﬁcationofeachmeasure,followedbyitsstandarddeviation.ResultsaregloballysimilarforFrenchandEnglish.Theyshowthatourmethodfortopicidentiﬁcationbuildstopicsthatareratherpure,i.e.eachofthemisstronglytiedtoareferencetopic,buttheircontentisrathersparseincomparisonwiththecontentoftheirassociatedreferencetopics.5.3TopicsegmentationForvalidatingthehypothesisthatunderliesourwork,weappliedF06andF06Ttoﬁndthetopicboundsinthedocumentsofourtwoevaluationcor-pora.Moreover,wealsotestedfourwellknownseg-mentersonourcorporatocomparetheresultsofF06andF06Twithstate-of-the-artalgorithms.Weclas-sicallyusedtheerrormetricPkproposedin(Beefer-manetal.,1999)tomeasuresegmentationaccuracy.Pkevaluatestheprobabilitythatarandomlycho-senpairofsentences,separatedbyksentences,iswronglyclassiﬁed,i.e.theyarefoundinthesamesegmentwhiletheyareactuallyindifferentones(miss)ortheyarefoundindifferentsegmentswhiletheyareactuallyinthesameone(falsealarm).WealsogivethevalueofWindowDiff(WD),avariantofPkproposedin(PevznerandHearst,2002)thatcor-rectssomeofitsinsufﬁciencies.Tables3and4showsystemsPkpval(F06)pval(F06T)WDU0025.910.0031.3e-0727.42C9927.574.2e-053.6e-1035.42TextTiling*21.080.6990.03727.43LCseg20.550.4390.11128.31F0621.58(cid:30)0.01327.83F06T18.460.013(cid:30)24.05Table3:EvaluationoftopicsegmentationfortheFrenchcorpus(PkandWDaspercentages)theresultsofourevaluationsfortopicsegmentation(smallestvaluesarebestresults).U00isthesys-temdescribedin(UtiyamaandIsahara,2001),C99theoneproposedin(Choi,2000)andLCsegispre-sentedin(Galleyetal.,2003).TextTiling*isavari-antofTextTilinginwhichtheﬁnalidentiﬁcationoftopicshiftsistakenfrom(Galleyetal.,2003).AllthesesystemswereusedasF06andF06Twithoutﬁxingthenumberoftopicshiftstoﬁnd.Moreover,theirparametersweretunedforourevaluationcor-pustoobtaintheirbestresults.Foreachresult,wealsogivethesigniﬁcancelevelpvalofitsdifferenceforPkwithF06andF06T,evaluatedbyaone-sidet-testwithanullhypothesisofequalmeans.Lev-elslowerthan0.05areconsideredasstatisticallysigniﬁcant(bold-facedvalues).Theﬁrstimportantpointtonoticeaboutthesetablesisthefactthat486

systemsPkpval(F06)pval(F06T)WDU0019.420.0484.3e-0521.22C9921.631.2e-041.8e-0930.64TextTiling*15.810.3080.11119.80LCseg14.780.0430.49619.73F0616.90(cid:30)0.01020.93F06T14.060.010(cid:30)18.31Table4:EvaluationoftopicsegmentationfortheEnglishcorpus(PkandWDaspercentages)F06ThassigniﬁcantlybetterresultsthanF06,bothforFrenchandEnglish.Hence,itconﬁrmsourhy-pothesisabouttheinterestoftakingintoaccountthetopicsofatextforitssegmentation,evenifthesetopicswerediscoveredinanunsupervisedwayandwithoutusingexternalknowledge.Moreover,F06Thavethebestresultsamongallthetestedalgorithms,withasigniﬁcantdifferenceinmostofthecases.Anothernotablepointabouttheseresultsistheirstabilityacrossourtwocorpora,evenifthesecor-poraarequitesimilar.WhereasF06andF06TwereinitiallydevelopedonacorpusinFrench,theirre-sultsontheEnglishcorpusarecomparabletotheirresultsontheFrenchtestcorpus,bothforthedif-ferencebetweenthemandthedifferencewiththefourotheralgorithms.Thecomparisonwiththesealgorithmsalsoillustratestherelationshipsbetweenthem:TextTiling*,LCseg,F06andF06TsharealargenumberofprinciplesandtheiroverallresultsaresigniﬁcantlyhigherthantheresultsofU00andC99.ThistrendisdifferentfromtheoneobservedfromtheChoicorpusforwhichalgorithmssuchC99orU00havegoodresults(PkforC99,U00,F06andF06Tisrespectivelyequalto12%,10%,14%and14%).ThismeansprobablythatalgorithmswithgoodresultsonacorpusbuiltastheChoicorpuswillnotnecessarilyhavegoodresultson“true”texts,whichagreeswith(Georgesculetal.,2006).Finally,wecanobservethatallthesealgorithmshavebetterresultsontheEnglishcorpusthanontheFrenchone.Asthetwocorporaarequitesimilar,thisdifferenceseemstocomefromtheirdifferenceoflanguage,perhapsbecauserepetitionsaremorediscouragedinFrenchthaninEnglishfromastylisticviewpoint.Thistendstobeconﬁrmedbytheratiobetweenthesizeofthelemmatizedvocabularyofeachcorpusandtheirnumberoftokens,equalto8%fortheFrenchcorpusandto5.6%fortheEnglishcorpus.6RelatedWorkOneofthemainproblemsaddressedbyourworkisthedetectionofthetopicalsimilarityoftwotextunits.Wehavetackledthisproblemfollowinganendogenousapproach,whichisnewinthetopicseg-mentationﬁeldtoourknowledge.Themainadvan-tageofthisoptionisthatitdoesnotrequireexternalknowledge.Moreover,itcanintegraterelationsbe-tweenwords,suchaspropernounsforinstance,thatareunlikelytobefoundinanexternalresource.Othersolutionshavebeenalreadyproposedtosolvetheproblemweconsider.Mostofthemconsistoftwosteps:ﬁrst,theyautomaticallybuildaseman-ticrepresentationofwordsfromtheco-occurrencescollectedfromalargecorpus;then,theyusethisrepresentationforenhancingtherepresentationofeachtextunittocompare.Thisoverallprincipleisimplementedwithdifferentformsbyseveraltopicsegmenters.InCWM(Choietal.,2001),avariantofC99,eachwordofasentenceisreplacedbyitsrepresentationinaLatentSemanticAnalysis(LSA)space.IntheworkofPonteandCroft(PonteandCroft,1997),therepresentationsofsentencesareex-pandedbyaddingtothemwordsselectedfromanexternalcorpusbythemeansoftheLocalContextAnalysis(LCA)method.Finallyin(Cailletetal.,2004),asetofconceptsarelearntfromacorpusinanunsupervisedwaybyusingtheX-meansclus-teringalgorithmandtheparagraphsofdocumentsarerepresentedinthespacedeﬁnedbythesecon-cepts.Infact,thewayweuserelationsbetweenwordsiscloserto(JobbinsandEvett,1998),eveniftherelationsinthisworkcomefromanetworkofco-occurrencesorathesaurusratherthanfromtexttopics.Inbothcasesthesimilarityoftwotextunitsisdeterminedbytheproportionoftheirwordsthatarepartofarelationacrossthetwounits.Moreglobally,ourworkexploitsthetopicsofatextforitssegmentation.Thiskindofapproachwasalsoexploredin(BleiandMoreno,2001)whereprobabilistictopicmodelswerebuiltinanunsuper-visedway.Morerecently,(Purveretal.,2006)hasalsoproposedamethodforunsupervisedtopicmod-elingtoaddressbothtopicsegmentationandidenti-487

ﬁcation.(Purveretal.,2006)isclosertoourworkthan(BleiandMoreno,2001)becauseitdoesnotre-quiretobuildtopicmodelsfromacorpusbutasinourcase,itsresultsdonotoutperformLCseg(Galleyetal.,2003)whileitsmodelisfarmorecomplex.7ConclusionandFutureWorkInthisarticle,wehaveﬁrstproposedanunsuper-visedmethodfordiscoveringthetopicsofatextwithoutrelyingonexternalknowledge.Then,wehaveshownhowthesetopicscanbeusedforim-provingatopicsegmentationmethodbasedonwordreiteration.Moreover,wehaveproposedanadapta-tionoftheevaluationframeworkofChoithataimsatbuildingmorerealisticevaluationdocuments.Fi-nally,wehavedemonstratedtheinterestofthemethodwepresentthroughitsevaluationbothonaFrenchandanEnglishcorpus.However,thesolutionwehaveproposedforim-provingtheidentiﬁcationoftopicalsimilaritiesbe-tweentextexcerptscannotcompletelymakeupfornotusinganyexternalknowledge.Hence,weplantouseanetworkoflexicalco-occurrences,whichisasourceofknowledgethatiseasytobuildautomati-callyfromalargecorpus.Moreprecisely,weintendtoextendourmethodfordiscoveringtexttopicsbycombiningtheco-occurrencegraphofadocumentwithsuchanetwork.Thisnetworkcouldalsobeusedmoredirectlyfortopicsegmentationasin(Job-binsandEvett,1998).ReferencesDougBeeferman,AdamBerger,andJohnLafferty.1999.Statisticalmodelsfortextsegmentation.Ma-chineLearning,34(1):177–210.DavidM.BleiandPedroJ.Moreno.2001.Topicseg-mentationwithanaspecthiddenmarkovmodel.In24thACMSIGIR,pages343–348.MarcCaillet,Jean-FrançoisPessiot,MassihAmini,andPatrickGallinari.2004.Unsupervisedlearningwithtermclusteringforthematicsegmentationoftexts.InRIAO’04,pages1–11.FreddyY.Y.Choi,PeterWiemer-Hastings,andJohannaMoore.2001.Latentsemanticanalysisfortextseg-mentation.InEMNLP’01,pages109–117.FreddyY.Y.Choi.2000.Advancesindomaininde-pendentlineartextsegmentation.InNAACL’00,pages26–33.LeventErtöz,MichaelSteinbach,andVipinKuma.2001.Findingtopicsincollectionsofdocuments:Asharednearestneighborapproach.InTextMine’01,Work-shopofthe1stSIAMInternationalConferenceonDataMining.MichelGalley,KathleenMcKeown,EricFosler-Lussier,andHongyanJing.2003.Discoursesegmentationofmulti-partyconversation.InACL’03,pages562–569.MariaGeorgescul,AlexanderClark,andSusanArm-strong.2006.Ananalysisofquantitativeaspectsintheevaluationofthematicsegmentationalgorithms.In7thSIGdialWorkshoponDiscourseandDialogue,pages144–151.MartiA.Hearst.1994.Multi-paragraphsegmentationofexpositorytext.InACL’94,pages9–16.AmandaC.JobbinsandLindsayJ.Evett.1998.Textseg-mentationusingreiterationandcollocation.InACL-COLING’98,pages614–618.HidekiKozima.1993.Textsegmentationbasedonsim-ilaritybetweenwords.InACL’93(StudentSession),pages286–288.JaneMorrisandGraemeHirst.1991.Lexicalcohe-sioncomputedbythesauralrelationsasanindicatorofthestructureoftext.ComputationalLinguistics,17(1):21–48.RebeccaJ.PassonneauandDianeJ.Litman.1997.Dis-coursesegmentationbyhumanandautomatedmeans.ComputationalLinguistics,23(1):103–139.LevPevznerandMartiA.Hearst.2002.Acritiqueandimprovementofanevaluationmetricfortextsegmen-tation.ComputationalLinguistics,28(1):19–36.JayM.PonteandBruceW.Croft.1997.Textsegmen-tationbytopic.InFirstEuropeanConferenceonre-searchandadvancedtechnologyfordigitallibraries.MatthewPurver,KonradP.Körding,ThomasL.Grif-ﬁths,andJoshuaB.Tenenbaum.2006.Unsupervisedtopicmodellingformulti-partyspokendiscourse.InCOLING-ACL2006,pages17–24.N.Stokes,J.Carthy,andA.F.Smeaton.2002.Segment-ingbroadcastnewsstreamsusinglexicalchains.InSTAIRS’02,pages145–154.MasaoUtiyamaandHitoshiIsahara.2001.Astatisticalmodelfordomain-independenttextsegmentation.InACL’01,pages491–498.J.P.Yamron,I.Carp,L.Gillick,S.Lowe,andP.vanMul-bregt.1998.Ahiddenmarkovmodelapproachtotextsegmentationandeventtracking.InICASSP,pages333–336.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 488–495,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

488

Theutilityofparse-derivedfeaturesforautomaticdiscoursesegmentationSeegerFisherandBrianRoarkCenterforSpokenLanguageUnderstanding,OGISchoolofScience&EngineeringOregonHealth&ScienceUniversity,Beaverton,Oregon,97006USA{fishers,roark}@cslu.ogi.eduAbstractWeinvestigatedifferentfeaturesetsforperformingautomaticsentence-leveldis-coursesegmentationwithinageneralma-chinelearningapproach,includingfeaturesderivedfromeitherﬁnite-stateorcontext-freeannotations.Weachievethebestre-portedperformanceonthistask,anddemon-stratethatourSPADE-inspiredcontext-freefeaturesarecriticaltoachievingthislevelofaccuracy.Thiscountersrecentresultssug-gestingthatpurelyﬁnite-stateapproachescanperformcompetitively.1IntroductionDiscoursestructureannotationshavebeendemon-stratedtobeofhighutilityforanumberofNLPapplications,includingautomatictextsummariza-tion(Marcu,1998;Marcu,1999;Cristeaetal.,2005),sentencecompression(SporlederandLap-ata,2005),naturallanguagegeneration(Prasadetal.,2005)andquestionanswering(Verberneetal.,2006).Theseannotationsincludesentencesegmen-tationintodiscourseunitsalongwiththelinkingofdiscourseunits,bothwithinandacrosssentenceboundaries,intoalabeledhierarchicalstructure.Forexample,thetreeinFigure1showsasentence-leveldiscoursetreeforthestring“Priceshavedroppedbutremainquitehigh,accordingtoCEOSmith,”whichhasthreediscoursesegments,eachlabeledwithei-ther“Nucleus”or“Satellite”dependingonhowcen-tralthesegmentistothecoherenceofthetext.Thereareanumberofcorporaannotatedwithdiscoursestructure,includingthewell-knownRSTTreebank(Carlsonetal.,2002);theDiscourseGraphBank(WolfandGibson,2005);andthePennDiscourseTreebank(Miltsakakietal.,2004).Whiletheannotationapproachesdifferacrossthesecor-pora,therequirementofsentencesegmentationintoRootHHHHHHNucleusHHHHHNucleusPPPPPriceshavedroppedSatellitePPPPbutremainquitehighSatellitePPPPPaccordingtoCEOSmithFigure1:ExampleNucleus/Satellitelabeledsentence-leveldiscoursetree.sub-sententialdiscourseunitsissharedacrossallap-proaches.Theseresourceshavefacilitatedresearchintostochasticmodelsandalgorithmsforautomaticdiscoursestructureannotationinrecentyears.UsingtheRSTTreebankastrainingandevalua-tiondata,SoricutandMarcu(2003)demonstratedthattheirautomaticsentence-leveldiscoursepars-ingsystemcouldachievenear-humanlevelsofac-curacy,ifitwasprovidedwithmanualsegmenta-tionsandmanualparsetrees.Manualsegmenta-tionwasprimarilyresponsibleforthisperformanceboostovertheirfullyautomaticsystem,thusmak-ingthecasethatautomaticdiscoursesegmentationistheprimaryimpedimenttohighaccuracyautomaticsentence-leveldiscoursestructureannotation.Theirmodelsandalgorithm–subsequentlypackagedto-getherintothepubliclyavailableSPADEdiscourseparser1–makeuseoftheoutputoftheCharniak(2000)parsertoderivesyntacticindicatorfeaturesforsegmentationanddiscourseparsing.SporlederandLapata(2005)alsousedtheRSTTreebankastrainingdatafordata-drivendiscourseparsingalgorithms,thoughtheirfocus,incontrasttoSoricutandMarcu(2003),wastoavoidcontext-freeparsingandrelyexclusivelyonfeaturesintheirmodelthatcouldbederivedviaﬁnite-statechunkersandtaggers.Theannotationsthattheyderivearedis-1http://www.isi.edu/publications/licensed-sw/spade/489

course“chunks”,i.e.,sentence-levelsegmentationandnon-hierarchicalnucleus/spanlabelingofseg-ments.TheydemonstratethattheirmodelsachievecomparableresultstoSPADEwithouttheuseofanycontext-freefeatures.Onceagain,segmentationisthepartoftheprocesswheretheautomaticalgo-rithmsmostseriouslyunderperform.InthispaperwetakeupthequestionposedbytheresultsofSporlederandLapata(2005):howmuch,ifany,accuracyreductionshouldweexpectifwechoosetouseonlyﬁnite-statederivedfea-tures,ratherthanthosederivedfromfullcontext-freeparses?Iflittleaccuracyislost,astheirre-sultssuggest,thenitwouldmakesensetoavoidrel-ativelyexpensivecontext-freeparsing,particularlyiftheamountoftexttobeprocessedislargeoriftherearereal-timeprocessingconstraintsonthesys-tem.If,however,theaccuracylossissubstantial,onemightchoosetoavoidcontext-freeparsingonlyinthemosttime-constrainedscenarios.WhileSporlederandLapata(2005)demonstratedthattheirﬁnite-statesystemcouldperformaswellastheSPADEsystem,whichusescontext-freeparsetrees,thisdoesnotdirectlyanswerthequestionoftheutilityofcontext-freederivedfeaturesforthistask.SPADEmakesuseofaparticularkindoffea-turefromtheparsetrees,anddoesnottrainagen-eralclassiﬁermakinguseofotherfeaturesbeyondtheparse-derivedindicatorfeatures.Asweshallshow,itsperformanceisnotthehighestthatcanbeachievedviacontext-freeparserderivedfeatures.Inthispaper,wetrainaclassiﬁerusingagen-eralmachinelearningapproachandarangeofﬁnite-stateandcontext-freederivedfeatures.Weinvesti-gatetheimpactondiscoursesegmentationperfor-mancewhenonefeaturesetisusedversusanother,insuchawayestablishingtheutilityoffeaturesde-rivedfromcontext-freeparses.Inthecourseofsodoing,weachievethebestreportedperformanceonthistask,anabsoluteF-scoreimprovementof5.0%overSPADE,whichrepresentsamorethan34%rel-ativeerrorratereduction.Byfocusingonsegmentation,weprovideanap-proachthatisgenerallyapplicabletoallofthevariousannotationapproaches,giventhesimilari-tiesbetweenthevarioussentence-levelsegmenta-tionguidelines.Giventhatsegmentationhasbeenshowntobeaprimaryimpedimenttohighaccu-racysentence-leveldiscoursestructureannotation,thisrepresentsalargestepforwardinourabilitytoautomaticallyparsethediscoursestructureoftext,whateverannotationapproachwechoose.2Methods2.1DataForourexperimentsweusetheRhetoricalStructureTheoryDiscourseTreebank(Carlsonetal.,2002),whichwewilldenoteRST-DT,acorpusannotatedwithdiscoursesegmentationandrelationsaccordingtoRhetoricalStructureTheory(MannandThomp-son,1988).TheRST-DTconsistsof385docu-mentsfromtheWallStreetJournal,about176,000words,whichoverlapswiththePennWallSt.Jour-nal(WSJ)Treebank(Marcusetal.,1993).ThesegmentationofsentencesintheRST-DTisintoclause-likeunits,knownaselementarydis-courseunits,oredus.Wewillusethetwoterms‘edu’and‘segment’interchangeablythroughouttherestofthepaper.Humanagreementforthissegmen-tationtaskisquitehigh,withagreementbetweentwoannotatorsatanF-scoreof98.3forunlabeledsegmentation(SoricutandMarcu,2003).TheRST-DTcorpusannotatesedubreaks,whichtypicallyincludesentenceboundaries,butsentenceboundariesarenotexplicitlyannotatedinthecorpus.Toperformsentence-levelprocessingandevalua-tion,wealignedtheRST-DTdocumentstothesamedocumentsinthePennWSJTreebank,andusedthesentenceboundariesfromthatcorpus.2Anaddi-tionalbeneﬁtofthisalignmentisthatthePennWSJTreebanktokenizationisthenavailableforparsingpurposes.Simpleminimumeditdistancealignmenteffectivelyallowedfordifferencesinpunctuationrepresentation(e.g.,doublequotes)andtokenizationwhenderivingtheoptimalalignment.TheRST-DTcorpusispartitionedintoatrain-ingsetof347documentsandatestsetof38doc-uments.Thistestsetconsistsof991sentenceswith2,346segments.Fortrainingpurposes,wecreatedaheld-outdevelopmentsetbyselectingeverytenthsentenceofthetrainingset.Thisdevelopmentsetwasusedforfeaturedevelopmentandforselectingthenumberofiterationsusedwhentrainingmodels.2.2EvaluationPreviousresearchintoRST-DTsegmentationandparsinghasfocusedonsubsetsofthe991sentencetestsetduringevaluation.SoricutandMarcu(2003)2AsmallnumberofdocumentﬁnalparentheticalsareintheRST-DTandnotinthePennWSJTreebank,whichouralign-mentapproachtakesintoaccount.490

omittedsentencesthatwerenotexactlyspannedbyasubtreeofthetreebank,sothattheycouldfo-cusonsentence-leveldiscourseparsing.Byourcount,thiseliminates40ofthe991sentencesinthetestsetfromconsideration.SporlederandLapata(2005)wentfurtherandestablishedasmallersub-setof608sentences,whichomittedsentenceswithonlyonesegment,i.e.,sentenceswhichthemselvesareatomicedus.Sincetheprimaryfocusofthispaperisonseg-mentation,thereisnostrongreasontoomitanysen-tencesfromthetestset,henceourresultswilleval-uateonall991testsentences,withtwoexceptions.First,inSection2.3,wecompareSPADEresultsun-derourconﬁgurationwithresultsfromSporlederandLapata(2005)inordertoestablishcompara-bility,andthisisdoneontheir608sentencesub-set.Second,inSection3.2,weinvestigatefeed-ingoursegmentationintotheSPADEsystem,inor-dertoevaluatetheimpactofsegmentationimprove-mentsontheirsentence-leveldiscourseparsingper-formance.Forthosetrials,the951sentencesubsetfromSoricutandMarcu(2003)isused.Allothertrialsusethefull991sentencetestset.Segmentationevaluationisdonewithprecision,recallandF1-scoreofsegmentationboundaries.Givenawordstringw1...wk,wecanindexwordboundariesfrom0tok,sothateachwordwifallsbetweenboundariesi−1andi.Forsentence-basedsegmentation,indices0andk,representingthebe-ginningandendofthestring,areknowntobeseg-mentboundaries.HenceSoricutandMarcu(2003)evaluatewithrespecttosentenceinternalsegmenta-tionboundaries,i.e.,withindicesjsuchthat0<j<kforasentenceoflengthk.Letgbethenumberofsentence-internalsegmentationboundariesinthegoldstandard,tthenumberofsentence-internalseg-mentationboundariesinthesystemoutput,andmthenumberofcorrectsentence-internalsegmenta-tionboundariesinthesystemoutput.ThenP=mtR=mgandF1=2PRP+R=2mg+tInSporlederandLapata(2005),theywerepri-marilyinterestedinlabeledsegmentation,wherethesegmentinitialboundarywaslabeledwiththeseg-menttype.Insuchascenario,theboundaryatin-dex0isnolongerknown,hencetheirevaluationin-cludedthoseboundaries,evenwhenreportingun-labeledresults.Thus,insection2.3,forcompar-isonwithreportedresultsinSporlederandLapata(2005),ourF1-scoreisdeﬁnedaccordingly,i.e.,seg-SegmentationsystemF1SporlederandLapatabest(reported)88.40SPADESporlederandLapataconﬁguration(reported):87.06currentconﬁguration:91.04Table1:SegmentationresultsontheSporlederandLapata(2005)dataset,withaccuracydeﬁnedtoincludesentenceinitialsegmentationboundaries.mentationboundariesjsuchthat0≤j<k.Inaddition,wewillreportunlabeledbracketingprecision,recallandF1-score,asdeﬁnedinthePARSEVALmetrics(Blacketal.,1991)andeval-uatedviathewidelyusedevalbpackage.Wealsouseevalbwhenreportinglabeledandunlabeleddis-courseparsingresultsinSection3.2.2.3BaselineSPADEsetupThepubliclyavailableSPADEpackage,whichen-codestheapproachinSoricutandMarcu(2003),istakenasthebaselineforthispaper.Wemadeseveralmodiﬁcationstothescriptfromthedefault,whichaccountforbetterbaselineperformancethanisachievedwiththedefaultconﬁguration.First,wemodiﬁedthescripttotakegivenparsetreesasinput,ratherthanrunningtheCharniakparseritself.Thisallowedustomaketwomodiﬁcationsthatimprovedperformance:turningofftokenizationintheChar-niakparser,andreranking.ThedefaultscriptthatcomeswithSPADEdoesnotturnofftokenizationinsideoftheparser,whichleadstodegradedperfor-mancewhentheinputhasalreadybeentokenizedinthePennTreebankstyle.Secondly,CharniakandJohnson(2005)showedhowrerankingofthe50-bestoutputoftheCharniak(2000)parsergivessub-stantialimprovementsinparsingaccuracy.ThesetwomodiﬁcationstotheCharniakparsingoutputusedbytheSPADEsystemleadtoimprovementsinitsperformancecomparedtopreviouslyreportedresults.Table1comparessegmentationresultsofthreesystemsontheSporlederandLapata(2005)608sentencesubsetoftheevaluationdata:(1)theirbestreportedsystem;(2)theSPADEsystemresultsre-portedinthatpaper;and(3)theSPADEsystemre-sultswithourcurrentconﬁguration.TheevaluationusestheunlabeledF1measureasdeﬁnedinthatpa-per,whichcountssentenceinitialboundariesinthescoring,asdiscussedintheprevioussection.Ascanbeseenfromtheseresults,ourimprovedconﬁgu-rationofSPADEgivesuslargeimprovementsoverthepreviouslyreportedSPADEperformanceonthissubset.Asaresult,wefeelthatwecanuseSPADE491

asaverystrongbaselineforevaluationontheentiretestset.Additionally,wemodiﬁedtheSPADEscripttoal-lowustoprovideoursegmentationstothefulldis-courseparsingthatitperforms,inordertoevalu-atetheimprovementstodiscourseparsingyieldedbyanyimprovementstosegmentation.2.4SegmentationclassiﬁerForthispaper,wetrainedabinaryclassiﬁer,whichwasappliedindependentlyateachwordwiinthestringw1...wk,todecidewhetherthatwordisthelastinasegment.Notethatwkisthelastwordinthestring,andishenceignored.Weusedalog-linearmodelwithnoMarkovdependencybetweenadjacenttags,3andtrainedtheparametersofthemodelwiththeperceptronalgorithm,withaverag-ingtocontrolforover-training(Collins,2002).LetC={E,I}bethesetofclasses:seg-mentationboundary(E)ornon-boundary(I).Letf(c,i,w1...wk)beafunctionthattakesasin-putaclassvaluec,awordindexiandthewordstringw1...wkandreturnsad-dimensionalvectoroffeaturevaluesforthatwordindexinthatstringwiththatclass.Forexample,onefeaturemightbe(c=E,wi=the),whichreturnsthevalue1whenc=Eandthecurrentwordis‘the’,andreturns0otherwise.Givenad-dimensionalparametervec-torφ,theoutputoftheclassiﬁeristhatclasswhichmaximizesthedotproductbetweenthefeatureandparametervectors:ˆc(i,w1...wk)=argmaxc∈Cφ·f(c,i,w1...wk)(1)Intraining,theweightsinφareinitializedto0.Formepochs(passesoverthetrainingdata),foreachwordinthetrainingdata(exceptsentenceﬁnalwords),themodelisupdated.Letibethecurrentwordpositioninstringw1...wkandsupposethattherehavebeenj−1previousupdatestothemodelparameters.Let¯cibethetrueclasslabel,andletˆcibeshorthandforˆc(i,w1...wk)inequation1.Thentheparametervectorφjatstepjisupdatedasfol-lows:φj=φj−1−f(ˆc,i,w1...wk)+f(¯c,i,w1...wk)(2)AsstatedinSection2.1,wereservedeverytenthsentenceasheld-outdata.Aftereachpassoverthetrainingdata,weevaluatedthesystemperformance3Becauseofthesparsityofboundarytags,Markovdepen-denciesbetweentagsbuynoadditionalsystemaccuracy.onthisheld-outdata,andchosethemodelthatop-timizedaccuracyonthatset.Theaveragedpercep-tronwasusedonheld-outandevaluationsets.SeeCollins(2002)formoredetailsonthisapproach.2.5FeaturesToteaseaparttheutilityofﬁnite-statederivedfea-turesandcontext-freederivedfeatures,weconsiderthreefeaturesets:(1)basicﬁnite-statefeatures;(2)context-freefeatures;and(3)ﬁnite-stateapproxima-tiontocontext-freefeatures.Notethateveryfeaturemustincludeexactlyoneclasslabelcinordertodiscriminatebetweenclassesinequation1.Hencewhenpresentingfeatures,itcanbeassumedthattheclasslabelispartofthefeature,evenifitisnotex-plicitlymentioned.Thethreefeaturesetsarenotcompletelydisjoint.Weincludesimpleposition-basedfeaturesineverysystem,deﬁnedasfollows.Becauseedusaretypi-callymulti-wordstrings,itislesslikelyforawordnearthebeginningorendofasentencetobeataneduboundary.Thusitisreasonabletoexpectthepositionwithinasentenceofatokentobeahelpfulfeature.Wecreated101indicatorfeatures,repre-sentingpercentagesfrom0to100.Forastringoflengthk,atpositioni,weroundi/ktotwodecimalplacesandprovideavalueof1forthecorrespondingquantizedpositionfeatureand0fortheotherposi-tionfeatures.2.5.1Basicﬁnite-statefeaturesOurbaselineﬁnite-statefeaturesetincludessimpletaggerderivedfeatures,aswellasfeaturesbasedonpositioninthestringandn-grams4.Weannotatetagsequencesontothewordsequenceviaacompet-itivediscriminativelytrainedtagger(Hollingsheadetal.,2005),trainedforeachoftwokindsoftagsequences:part-of-speech(POS)tagsandshallowparsetags.Theshallowparsetagsdeﬁnenon-hierarchicalbaseconstituents(“chunks”),asdeﬁnedfortheCoNLL-2000sharedtask(TjongKimSangandBuchholz,2000).Thesecaneitherbeusedastagorchunksequences.Forexample,thetreeinFigure2representsashallow(non-hierarchical)parsetree,withfourbaseconstituents.EachbaseconstituentXbeginswithawordlabeledwithBX,whichsigniﬁesthatthiswordbeginstheconstituent.AllotherwordswithinaconstituentXarelabeled4Wetriedusingalistof311cuephrasesfromKnott(1996)todeﬁnefeatures,butdidnotderiveanysystemimprovementthroughthislist,presumablybecauseoursimplen-gramfea-turesalreadycapturemanysuchlexicalcues.492

ROOT   @@@PPPPPPPPPNPHHBNPDTtheINPNNbrokerVPHHBVPMDwillIVPVBDsellNPHHBNPDTtheINPNNSstocksNPBNPNNtomorrowFigure2:Treerepresentationofshallowparses,withB(egin)andI(nside)tagsIX,andwordsoutsideofanybaseconstituentarela-beledO.Insuchaway,eachwordislabeledwithbothaPOS-tagandaB/I/Otag.Forourthreesequences(lexical,POS-tagandshallowtag),wedeﬁnen-gramfeaturessurround-ingthepotentialdiscourseboundary.Ifthecurrentwordiswi,thehypothesizedboundarywilloccurbetweenwiandwi+1.Forthisboundaryposition,the6-gramincludingthethreewordsbeforeandthethreewordsaftertheboundaryisincludedasafea-ture;additionally,alln-gramsforn<6suchthateitherwiorwi+1(orboth)isinthen-gramarein-cludedasfeatures.Inotherwords,alln-gramsinasixwordwindowofboundarypositioniareincludedasfeatures,exceptthosethatincludeneitherwinorwi+1inthen-gram.TheidenticalfeaturetemplatesareusedwithPOS-tagandshallowtagsequencesaswell,todeﬁnetagn-gramfeatures.ThisfeaturesetisveryclosetothatusedinSporlederandLapata(2005),butnotidentical.Theirn-gramfeaturedeﬁnitionsweredifferent(thoughsimilar),andtheymadeuseofcuephrasesfromKnott(1996).Inaddition,theyusedarule-basedclauserthatwedidnot.Despitesuchdiffer-ences,thisfeaturesetisquiteclosetowhatisde-scribedinthatpaper.2.5.2Context-freefeaturesTodescribeourcontext-freefeatures,weﬁrstpresenthowSPADEmadeuseofcontext-freeparsetreeswithintheirsegmentationalgorithm,sincethisformsthebasisofourfeatures.TheSPADEfeaturesarebasedonproductionsextractedfromfullsyntac-ticparsesofthegivensentence.Theprimaryfeatureforadiscourseboundaryafterwordwiisbasedonthelowestconstituentinthetreethatspanswordswm...wnsuchthatm≤i<n.Forexample,intheparsetreeschematicinFigure3,theconstituentlabeledwithAisthelowestconstituentinthetreewhosespancrossesthepotentialdiscoursebound-aryafterwi.TheprimaryfeatureistheproductionA   @@@PPPPPPPPB1...Bj−1HHHC1...CnHH...TiwiBj...BmFigure3:Parsetreeschematicfordescribingcontext-freeseg-mentationfeaturesthatexpandsthisconstituentinthetree,withtheproposedsegmentationboundarymarked,whichinthiscaseis:A→B1...Bj−1||Bj...Bm,where||denotesthesegmentationboundary.InSPADE,theproductionislexicalizedbytheheadwordsofeachconstituent,whicharedeterminedusingstan-dardhead-percolationtechniques.Thisfeatureisusedtopredictaboundaryasfollows:iftherelativefrequencyestimateofaboundarygiventheproduc-tionfeatureinthecorpusisgreaterthan0.5,thenaboundaryispredicted;otherwisenot.Iftheproduc-tionhasnotbeenobservedfrequentlyenough,thelexicalizationisremovedandtherelativefrequencyofaboundarygiventheunlexicalizedproductionisusedforprediction.Iftheobservationsoftheunlex-icalizedproductionarealsotoosparse,thenonlythechildrenadjacenttotheboundaryaremaintainedinthefeature,e.g.,A→∗Bj−1||Bj∗where∗repre-sentszeroormorecategories.Furthersmoothingisusedwheneventhisisunobserved.Weusethesefeaturesasthestartingpointforourcontext-freefeatureset:thelexicalizedproductionA→B1...Bj−1||Bj...Bm,asdeﬁnedaboveforSPADE,isafeatureinourmodel,asistheunlexi-calizedversionoftheproduction.Aswiththeotherfeaturesthatwehavedescribed,thisfeatureisusedasanindicatorfeatureintheclassiﬁerappliedatthewordwiprecedingthehypothesizedboundary.Inadditiontothesefullproductionfeatures,weusetheproductionwithonlychildrenadjacenttothebound-ary,denotedbyA→∗Bj−1||Bj∗.Thisproductionisusedinfourways:fullylexicalized;unlexicalized;onlycategoryBj−1lexicalized;andonlycategoryBjlexicalized.WealsouseA→∗Bj−2Bj−1||∗andA→∗||BjBj+1∗features,bothunlexicalizedandwiththeboundary-adjacentcategorylexical-ized.IfthereisnocategoryBj−2orBj+1,theyarereplacedwith“N/A”.Inadditiontothesefeatures,weﬁrethesamefea-turesforallproductionsonthepathfromAdown493

SegmentBoundaryaccuracyBracketingaccuracySegmentationsystemRecallPrecisionF1RecallPrecisionF1SPADE85.485.585.577.777.977.8Classiﬁer:Basicﬁnite-state81.583.382.473.674.574.0Classiﬁer:Fullﬁnite-state84.187.986.078.080.079.0Classiﬁer:Context-free84.791.187.880.383.782.0Classiﬁer:Allfeatures89.791.390.584.985.885.3Table2:Segmentationresultsonall991sentencesintheRST-DTtestset.Segmentboundaryaccuracyisforsentenceinternalboundariesonly,followingSoricutandMarcu(2003).Bracketingaccuracyisforunlabeledﬂatbracketingofthesamesegments.Whileboundaryaccuracycorrectlydepictssegmentationresults,theharsherﬂatbracketingmetricbetterpredictsdiscourseparsingperformance.tothewordwi.Fortheseproductions,theseg-mentationboundary||willoccurafterallchildrenintheproduction,e.g.,Bj−1→C1...Cn||,whichisthenusedinbothlexicalizedandunlexicalizedforms.Forthefeaturewithonlycategoriesadja-centtotheboundary,weagainuse“N/A”todenotethefactthatnocategoryoccurstotherightoftheboundary:Bj−1→∗Cn||N/A.Onceagain,thesearelexicalizedasdescribedabove.2.5.3Finite-stateapproximationfeaturesAnapproximationtoourcontext-freefeaturescanbemadebyusingtheshallowparsetree,asshowninFigure2,inlieuofthefullhierarchicalparsetree.Forexample,ifthecurrentwordwas“sell”inthetreeinFigure2,theprimaryfeaturewouldbeROOT→NPVP||NPNP,anditwouldhaveanunlexicalizedversionandthreelexicalizedversions:thecategoryimmediatelypriortotheboundarylex-icalized;thecategoryimmediatelyafterthebound-arylexicalized;andbothlexicalized.Forlexicaliza-tion,wechoosetheﬁnalwordintheconstituentasthelexicalheadfortheconstituent.Thisisarea-sonableﬁrstapproximation,becausesuchtypicallyleft-headedcategoriesasPPandVPlosetheirargu-mentsintheshallowparse.Asapracticalmatter,welimitthenumberofcat-egoriesintheﬂatproductionto8totheleftand8totherightoftheboundary.Inamannersimilartothen-gramfeaturesthatwedeﬁnedinSection2.5.1,weallowallcombinationswithlessthan8contiguouscategoriesoneachside,providedthatatleastoneoftheadjacentcategoriesisincludedinthefeature.Eachfeaturehasanunlexicalizedandthreelexical-izedversions,asdescribedabove.3ExperimentsWeperformedanumberofexperimentstodeter-minetherelativeutilityoffeaturesderivedfromfullcontext-freesyntacticparsesandthosederivedsolelyfromshallowﬁnite-statetagging.Ourpri-maryconcerniswithintra-sententialdiscourseseg-mentation,butwearealsointerestedinhowmuchtheimprovedsegmentationhelpsdiscourseparsing.Thesyntacticparserweuseforallcontext-freesyntacticparsesusedineitherSPADEorourclas-siﬁeristheCharniakparserwithreranking,asde-scribedinCharniakandJohnson(2005).TheChar-niakparserandrerankerweretrainedonthesectionsofthePennTreebanknotincludedintheRST-DTtestset.Allstatisticalsigniﬁcancetestingisdoneviathestratiﬁedshufﬂingtest(Yeh,2000).3.1SegmentationTable2presentssegmentationresultsforSPADEandfourversionsofourclassiﬁer.The“Basicﬁnite-state”systemusesonlyﬁnite-statesequencefea-turesasdeﬁnedinSection2.5.1,whilethe“Fullﬁnite-state”alsoincludestheﬁnite-stateapproxima-tionfeaturesfromSection2.5.3.The“Context-free”systemusestheSPADE-inspiredfeaturesdetailedinSection2.5.2,butnoneofthefeaturesfromSections2.5.1or2.5.3.Finally,the“Allfeatures”sectionin-cludesfeaturesfromallthreesections.5Notethatthefullﬁnite-statesystemisconsider-ablybetterthanthebasicﬁnite-statesystem,demon-stratingtheutilityoftheseapproximationsoftheSPADE-likecontext-freefeatures.Theperformanceoftheresulting“Full”ﬁnite-statesystemisnotsta-tisticallysigniﬁcantlydifferentfromthatofSPADE(p=0.193),despitenorelianceonfeaturesderivedfromcontext-freeparses.Thecontext-freefeatures,however,evenwithoutanyoftheﬁnite-statesequencefeatures(evenlex-icaln-grams)outperformsthebestﬁnite-statesys-tembyalmosttwopercentabsolute,andthesys-temwithallfeaturesimprovesonthebestﬁnite-statesystembyoverfourpercentabsolute.Thesystem5Inthe“Allfeatures”condition,theﬁnite-stateapproxima-tionfeaturesdeﬁnedinSection2.5.3onlyincludeamaximumof3childrentotheleftandrightoftheboundary,versusamax-imumof8forthe“Fullﬁnite-state”system.Thiswasfoundtobeoptimalonthedevelopmentset.494

SegmentationUnlabeledNuc/SatSPADE76.970.2Classiﬁer:Fullﬁnitestate78.171.1Classiﬁer:Allfeatures83.576.1Table3:Discourseparsingresultsonthe951sentenceSori-cutandMarcu(2003)evaluationset,usingSPADEforparsing,andvariousmethodsforsegmentation.Scoresareunlabeledandlabeled(Nucleus/Satellite)bracketingaccuracy(F1).TheﬁrstlineshowsSPADEperformingbothsegmentationanddis-courseparsing.TheothertwolinesshowSPADEperformingdiscourseparsingwithsegmentationsproducedbyourclassi-ﬁerusingdifferentcombinationsoffeatures.withallfeaturesisstatisticallysigniﬁcantlybetterthanbothSPADEandthe“Fullﬁnite-state”classi-ﬁersystem,atp<0.001.Thislargeimprovementdemonstratesthatthecontext-freefeaturescanpro-videaverylargesystemimprovement.3.2DiscourseparsingIthasbeenshownthataccuratediscoursesegmen-tationwithinasentencegreatlyimprovestheover-allparsingaccuracytonearhumanlevels(Sori-cutandMarcu,2003).Givenourimprovedseg-mentationresultspresentedintheprevioussection,improvementswouldbeexpectedinfullsentence-leveldiscourseparsing.Toachievethis,wemodi-ﬁedtheSPADEscripttoacceptoursegmentationswhenbuildingthefullyhierarchicaldiscoursetree.TheresultsforthreesystemsarepresentedinTa-ble3:SPADE,our“Fullﬁnite-state”system,andoursystemwithallfeatures.Resultsforunlabeledbracketingarepresented,alongwithresultsforla-beledbracketing,wherethelabeliseitherNucleusorSatellite,dependinguponwhetherornotthenodeismorecentral(Nucleus)tothecoherenceofthetextthanitssibling(s)(Satellite).Thislabelsethasbeenshowntobeofparticularutilityforindicatingwhichsegmentsaremoreimportanttoincludeinanauto-maticallycreatedsummaryorcompressedsentence(SporlederandLapata,2005;Marcu,1998;Marcu,1999;Cristeaetal.,2005).Onceagain,theﬁnite-statesystemdoesnotperformstatisticallysigniﬁcantlydifferentfromSPADEoneitherlabeledorunlabeleddiscourseparsing.Usingallfeatures,however,resultsingreaterthan5%absoluteaccuracyimprovementoverbothofthesesystems,whichissigniﬁcant,inallcases,atp<0.001.4DiscussionandfuturedirectionsOurresultsshowthatcontext-freeparsederivedfea-turesarecriticalforachievingthehighestlevelofaccuracyinsentence-leveldiscoursesegmentation.Giventhatedusarebydeﬁnitionclause-likeunits,itisnotsurprisingthataccuratefullsyntacticparsetreesprovidehighlyrelevantinformationunavail-ablefromﬁnite-stateapproaches.Addingcontext-freefeaturestoourbestﬁnite-statefeaturemodelreduceserrorinsegmentationby32.1%,anin-creaseinabsoluteF-scoreof4.5%.Theseincreasesareagainstaﬁnite-statesegmentationmodelthatispowerfulenoughtobestatisticallyindistinguishablefromSPADE.Ourexperimentsalsoconﬁrmthatincreasedseg-mentationaccuracyyieldssigniﬁcantlybetterdis-courseparsingaccuracy,aspreviouslyshowntobethecasewhenprovidingreferencesegmentationstoaparser(SoricutandMarcu,2003).Thesegmen-tationreductioninerrorof34.5%propagatestoa28.6%reductioninerrorforunlabeleddiscourseparsetrees,anda19.8%reductioninerrorfortreeslabeledwithNucleusandSatellite.Wehaveseveralkeydirectionsinwhichtocon-tinuethiswork.First,giventhatageneralma-chinelearningapproachallowedustoimproveuponSPADE’ssegmentationperformance,wealsobe-lievethatitwillproveusefulforimprovingfulldiscourseparsing,bothatthesentencelevelandbeyond.Forefﬁcientinter-sententialdiscourseparsing,weseetheneedforanadditionallevelofsegmentationattheparagraphlevel.Whereasmostsentencescorrespondtoawell-formedsubtree,SporlederandLascarides(2004)reportthatover20%oftheparagraphboundariesintheRST-DTdonotcorrespondtoawell-formedsubtreeinthehu-manannotateddiscourseparseforthatdocument.Therefore,toperformaccurateandefﬁcientpars-ingoftheRST-DTattheparagraphlevel,thetextshouldbesegmentedintoparagraph-likesegmentsthatconformtothehuman-annotatedsubtreebound-aries,justassentencesaresegmentedintoedus.Wealsointendtobeginworkontheotherdis-courseannotatedcorpora.Whilemostworkontex-tualdiscourseparsinghasmadeuseoftheRST-DTcorpus,theDiscourseGraphBankcorpusprovidesacompetingannotationthatisnotconstrainedtotreestructures(WolfandGibson,2005).Onceaccuratelevelsofsegmentationandparsingforbothcorporaareattained,itwillbepossibletoperformextrinsicevaluationstodeterminetheirrelativeutilityfordif-ferentNLPtasks.Recentworkhasshownpromis-ingpreliminaryresultsforrecognizingandlabelingrelationsofGraphBankstructures(Wellneretal.,2006),inthecasethatthealgorithmisprovidedwith495

manuallysegmentedsentences.Sentence-levelseg-mentationintheGraphBankisverysimilartothatintheRST-DT,sooursegmentationapproachshouldworkwellforDiscourseGraphBankstyleparsing.ThePennDiscourseTreebank(Miltsakakietal.,2004),orPDTB,usesarelativelyﬂatannotationofdiscoursestructure,incontrasttothehierarchicalstructuresfoundintheothertwocorpora.Itcontainsannotationsfordiscourseconnectivesandtheirargu-ments,whereanargumentcanbeassmallasanom-inalizationoraslargeasseveralsentences.Thisap-proachobviatestheneedtocreateasetofdiscourserelations,butsentenceinternalsegmentationisstillanecessarystep.ThoughsegmentationinthePDTBtendstolargerunitsthanedus,ourapproachtoseg-mentationshouldbestraightforwardlyapplicabletotheirsegmentationstyle.AcknowledgmentsThankstoCarolineSporlederandMirellaLapatafortheirtestdataandhelpfulcomments.ThanksalsotoRaduSoricutforhelpfulinput.ThisresearchwassupportedinpartbyNSFGrant#IIS-0447214.Anyopinions,ﬁndings,conclusionsorrecommendationsexpressedinthispublicationarethoseoftheauthorsanddonotnecessarilyreﬂecttheviewsoftheNSF.ReferencesE.Black,S.Abney,D.Flickenger,C.Gdaniec,R.Grish-man,P.Harrison,D.Hindle,R.Ingria,F.Jelinek,J.Kla-vans,M.Liberman,M.P.Marcus,S.Roukos,B.Santorini,andT.Strzalkowski.1991.Aprocedureforquantita-tivelycomparingthesyntacticcoverageofenglishgram-mars.InDARPASpeechandNaturalLanguageWorkshop,pages306–311.L.Carlson,D.Marcu,andM.E.Okurowski.2002.RSTdis-coursetreebank.LinguisticDataConsortium,Catalog#LDC2002T07.ISBNLDC2002T07.E.CharniakandM.Johnson.2005.Coarse-to-ﬁnen-bestpars-ingandMaxEntdiscriminativereranking.InProceedingsofthe43rdAnnualMeetingofACL,pages173–180.E.Charniak.2000.Amaximum-entropy-inspiredparser.InProceedingsofthe1stConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics,pages132–139.M.J.Collins.2002.DiscriminativetrainingmethodsforhiddenMarkovmodels:Theoryandexperimentswithperceptronalgorithms.InProceedingsoftheConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP),pages1–8.D.Cristea,O.Postolache,andI.Pistol.2005.Summarisationthroughdiscoursestructure.In6thInternationalConferenceonComputationalLinguisticsandIntelligentTextProcess-ing(CICLing).K.Hollingshead,S.Fisher,andB.Roark.2005.Comparingandcombiningﬁnite-stateandcontext-freeparsers.InPro-ceedingsofHLT-EMNLP,pages787–794.A.Knott.1996.AData-DrivenMethodologyforMotivatingaSetofCoherenceRelations.Ph.D.thesis,DepartmentofArtiﬁcialIntelligence,UniversityofEdinburgh.W.C.MannandS.A.Thompson.1988.Rhetoricalstructuretheory:Towardafunctionaltheoryoftextorganization.Text,8(3):243–281.D.Marcu.1998.Improvingsummarizationthroughrhetoricalparsingtuning.InThe6thWorkshoponVeryLargeCorpora.D.Marcu.1999.Discoursetreesaregoodindicatorsofim-portanceintext.InI.ManiandM.Maybury,editors,Ad-vancesinAutomaticTextSummarization,pages123–136.MITPress,Cambridge,MA.M.P.Marcus,B.Santorini,andM.A.Marcinkiewicz.1993.BuildingalargeannotatedcorpusofEnglish:ThePennTreebank.ComputationalLinguistics,19(2):313–330.E.Miltsakaki,R.Prasad,A.Joshi,andB.Webber.2004.ThePennDiscourseTreeBank.InProceedingsoftheLanguageResourcesandEvaluationConference.R.Prasad,A.Joshi,N.Dinesh,A.Lee,E.Miltsakaki,andB.Webber.2005.ThePennDiscourseTreeBankasare-sourcefornaturallanguagegeneration.InProceedingsoftheCorpusLinguisticsWorkshoponUsingCorporaforNat-uralLanguageGeneration.R.SoricutandD.Marcu.2003.Sentenceleveldiscoursepars-ingusingsyntacticandlexicalinformation.InHumanLan-guageTechnologyConferenceoftheNorthAmericanAsso-ciationforComputationalLinguistics(HLT-NAACL).C.SporlederandM.Lapata.2005.Discoursechunkinganditsapplicationtosentencecompression.InHumanLanguageTechnologyConferenceandtheConferenceonEmpiricalMethodsinNaturalLanguageProcessing(HLT-EMNLP),pages257–264.C.SporlederandA.Lascarides.2004.Combininghierarchi-calclusteringandmachinelearningtopredicthigh-leveldis-coursestructure.InProceedingsoftheInternationalConfer-enceinComputationalLinguistics(COLING),pages43–49.E.F.TjongKimSangandS.Buchholz.2000.IntroductiontotheCoNLL-2000sharedtask:Chunking.InProceedingsofCoNLL,pages127–132.S.Verberne,L.Boves,N.Oostdijk,andP.A.Coppen.2006.Discourse-basedansweringofwhy-questions.TraitementAutomatiquedesLangues(TAL).B.Wellner,J.Pustejovsky,C.Havasi,A.Rumshisky,andR.Sauri.2006.Classiﬁcationofdiscoursecoherencere-lations:Anexploratorystudyusingmultipleknowledgesources.InProceedingsofthe7thSIGdialWorkshoponDis-courseandDialogue.F.WolfandE.Gibson.2005.Representingdiscoursecoher-ence:Acorpus-basedanalysis.ComputationalLinguistics,31(2):249–288.A.Yeh.2000.Moreaccuratetestsforthestatisticalsigniﬁ-canceofresultdifferences.InProceedingsofthe18thInter-nationalCOLING,pages947–953.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 496–503,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

496

PERSONAGE:PersonalityGenerationforDialogueFranc¸oisMairesseDepartmentofComputerScienceUniversityofShefﬁeldShefﬁeld,S14DP,UnitedKingdomF.Mairesse@sheffield.ac.ukMarilynWalkerDepartmentofComputerScienceUniversityofShefﬁeldShefﬁeld,S14DP,UnitedKingdomM.A.Walker@sheffield.ac.ukAbstractOverthelastﬁftyyears,the“BigFive”modelofpersonalitytraitshasbecomeastandardinpsychology,andresearchhassystematicallydocumentedcorrelationsbe-tweenawiderangeoflinguisticvariablesandtheBigFivetraits.Adistinctlineofresearchhasexploredmethodsforautomati-callygeneratinglanguagethatvariesalongpersonalitydimensions.WepresentPER-SONAGE(PERSONAlityGEnerator),theﬁrsthighlyparametrizablelanguagegener-atorforextraversion,animportantaspectofpersonality.Weevaluatetwopersonal-itygenerationmethods:(1)directgenera-tionwithparticularparametersettingssug-gestedbythepsychologyliterature;and(2)overgenerationandselectionusingstatisticalmodelstrainedfromjudge’sratings.Resultsshowthatbothmethodsreliablygenerateut-terancesthatvaryalongtheextraversiondi-mension,accordingtohumanjudges.1IntroductionOverthelastﬁftyyears,the“BigFive”modelofper-sonalitytraitshasbecomeastandardinpsychology(extraversion,neuroticism,agreeableness,conscien-tiousness,andopennesstoexperience),andresearchhassystematicallydocumentedcorrelationsbetweenawiderangeoflinguisticvariablesandtheBigFivetraits(Mehletal.,2006;Norman,1963;Oberlan-derandGill,2006;PennebakerandKing,1999).Adistinctlineofresearchhasexploredmethodsforautomaticallygeneratinglanguagethatvariesalongpersonalitydimensions,targetingapplicationssuchascomputergamingandeducationalvirtualworlds(Andr´eetal.,2000;Isardetal.,2006;LoyallandBates,1997;Piwek,2003;Walkeretal.,1997)interalia.Otherworksuggestsaclearutilityforgener-atinglanguagemanifestingpersonality(ReevesandNass,1996).However,todate,(1)researchingener-ationhasnotsystematicallyexploitedthepsycholin-guisticﬁndings;and(2)therehasbeenlittleevalua-tionshowingthatautomaticgeneratorscanproducelanguagewithrecognizablepersonalityvariation.AltRealizationExtra5Err...itseemstomethatLeMaraisisn’tasbadastheothers.1.834Right,Imean,LeMaraisistheonlyrestaurantthatisanygood.2.838Ok,Imean,LeMaraisisaquitefrench,kosherandsteakhouseplace,youknowandtheatmo-sphereisn’tnasty,ithasniceatmosphere.Ithasfriendlyservice.Itseemstomethattheserviceisnice.Itisn’tasbadastheothers,isit?5.179Well,itseemstomethatIamsureyouwouldlikeLeMarais.Ithasgoodfood,thefoodissortofrathertasty,theambienceisnice,theat-mosphereisn’tsortofnasty,itfeaturesratherfriendlyserversanditspriceisaround44dol-lars.5.833IamsureyouwouldlikeLeMarais,youknow.Theatmosphereisacceptable,theserversareniceandit’safrench,kosherandsteakhouseplace.Actually,thefoodisgood,evenifitspriceis44dollars.6.0010ItseemstomethatLeMaraisisn’tasbadastheothers.It’safrench,kosherandsteakhouseplace.Ithasfriendlyservers,youknowbutit’ssomewhatexpensive,youknow!6.172Basically,actually,IamsureyouwouldlikeLeMarais.Itfeaturesfriendlyserviceandaccept-ableatmosphereandit’safrench,kosherandsteakhouseplace.Evenifitspriceis44dollars,itjusthasreallygoodfood,nicefood.6.17Table1:Recommendationsalongtheextraver-siondimension,withtheaverageextraversionratingfromhumanjudgesonascalefrom1to7.Alt-2and3arefromtheextravertset,Alt-4and5arefromtheintrovertset,andotherswererandomlygenerated.Ouraimistoproduceahighlyparameterizablegeneratorwhoseoutputsvaryalongpersonalitydi-mensions.Wehypothesizethatsuchlanguagecan497

begeneratedbyvaryingparameterssuggestedbypsycholinguisticresearch.So,wemustﬁrstmapthepsychologicalﬁndingstoparametersofanaturallanguagegenerator(NLG).However,thispresentsseveralchallenges:(1)Theﬁndingsresultfromstudiesofgenresoflanguage,suchasstream-of-consciousnessessays(PennebakerandKing,1999),andinformalconversations(Mehletal.,2006),andthusmaynotapplytoﬁxedcontentdomainsusedinNLG;(2)Mostﬁndingsarebasedonself-reportsofpersonality,butwewanttoaffectobserver’spercep-tions;(3)Theﬁndingsconsistofweakbutsigniﬁ-cantcorrelations,sothatindividualparametersmaynothaveastrongenougheffecttoproducerecog-nizablevariationwithinasingleutterance;(4)Therearemanypossiblemappingsoftheﬁndingstogen-erationparameters;and(5)Itisunclearwhetheronlyspeciﬁcspeech-acttypesmanifestpersonalityorwhetherallutterancesdo.Thusthispapermakesseveralcontributions.First,Section2summarizesthelinguisticreﬂexesofextraversion,organizedbythemodulesinastandardNLGsystem,andproposeamappingfromtheseﬁndingstoNLGparameters.Toourknowledgethisistheﬁrstattempttoputforwardasystematicframe-workforgeneratinglanguagemanifestingpersonal-ity.Westartwiththeextraversiondimensionbe-causeitisanimportantpersonalityfactor,withmanyassociatedlinguisticvariables.WebelievethatourframeworkwillgeneralizetotheotherdimensionsintheBigFivemodel.Second,Sections3and4describethePERSONAGE(PERSONAlityGEner-ator)generatorandits29parameters.Table1showsexamplesgeneratedbyPERSONAGEforrecom-mendationsintherestaurantdomain,alongwithhumanextraversionjudgments.Third,Sections5and6describeexperimentsevaluatingtwogenera-tionmethods.Weﬁrstshowthat(1)theparame-tersgenerateutterancesthatvarysigniﬁcantlyontheextraversiondimension,accordingtohumanjudg-ments;and(2)wecantrainastatisticalmodelthatmatcheshumanperformanceinassigningextraver-sionratingstogenerationoutputsproducedwithran-domparametersettings.Section7sumsupanddis-cussesfuturework.2PsycholinguisticFindingsandPERSONAGEParametersWehypothesizethatpersonalitycanbemademan-ifestinevaluativespeechactsinanydialoguedo-main,i.e.utterancesrespondingtorequeststoREC-OMMENDorCOMPAREdomainentities,suchasrestaurantsormovies(Isardetal.,2006;Stentetal.,2004).Thus,westartwiththeSPaRKygenera-tor1,whichproducesevaluativerecommendationsandcomparisonsintherestaurantdomain,foradatabaseofrestaurantsinNewYorkCity.Thereareeightattributesforeachrestaurant:thenameandaddress,scalarattributesforprice,foodquality,at-mosphere,andserviceandcategoricalattributesforneighborhoodandtypeofcuisine.SPaRKyisbasedonthestandardNLGarchitecture(ReiterandDale,2000),andconsistsofthefollowingmodules:1.ContentPlanning:reﬁnecommunicativegoals,selectandstructurecontent;2.Sentenceplanning;chooselinguisticresources(lexicon,syntax)toachievegoals;3.Realization:usegrammar(syntax,morphology)togen-eratesurfaceutterances.GiventheNLGarchitecture,speech-acttypes,anddomain,theﬁrststepthenistosummarisepsy-chologicalﬁndingsonextraversionandmapthemtothisarchitecture.ThecolumnNLGmodulesofTable2givestheproposedmapping.Theﬁrstrowspeciﬁesﬁndingsforthecontentplanningmoduleandtheotherrowsareaspectsofsentenceplanning.RealizationisachievedwiththeRealProsurfacere-alizer(LavoieandRambow,1997).Anexamina-tionoftheintrovertandextravertﬁndingsinTable2highlightsthechallengesabove,i.e.exploitingtheseﬁndingsinasystematicwaywithinaparameteriz-ableNLGsystem.ThecolumnParameterinTable2proposespa-rameters(explainedinSections3and4)thatarema-nipulatedwithineachmoduletorealizetheﬁndingsintheothercolumns.Eachparametervariescon-tinuouslyfrom0to1,whereendpointsaremeanttoproduceextremebutplausibleoutput.Giventhechallengesabove,itisimportanttonotethattheseparametersrepresenthypothesesabouthowaﬁnd-ingcanbemappedintoanyNLGsystem.TheIntroandExtracolumnsattherighthandsideofthePa-rametercolumnindicatearangeofsettingsforthisparameter,suggestedbythepsychologicalﬁndings,toproduceintrovertedvs.extravertedlanguage.SPaRKyproducescontentplansforrestaurantrecommendationsandcomparisonsthataremodi-ﬁedbytheparameters.ThesamplecontentplanforarecommendationinFigure1correspondstotheoutputsinTable1.WhileTable1showsthatPERSONAGE’sparametershavevariouspragmaticeffects,theypreservethemeaningattheGriceanin-tentionlevel(dialoguegoal).Eachcontentplancon-tainsaclaim(nucleus)abouttheoverallqualityof1Availablefordownloadfromwww.dcs.shef.ac.uk/cogsys/sparky.html498

NLGmodulesIntrovertﬁndingsExtravertﬁndingsParameterIntroExtraContentSingletopicManytopicsVERBOSITYlowhighselectionStrictselectionThinkoutloud*RESTATEMENTSlowhighandREPETITIONSlowlowstructureProblemtalk,Pleasuretalk,agreement,CONTENTPOLARITYlowhighdissatisfactioncomplimentREPETITIONSPOLARITYlowhighCLAIMPOLARITYlowhighCONCESSIONSavgavgCONCESSIONSPOLARITYlowhighPOLARISATIONlowhighPOSITIVECONTENTFIRSTlowhighSyntacticFewself-referencesManyself-referencesSELF-REFERENCESlowhightemplatesElaboratedconstructionsSimpleconstructions*CLAIMCOMPLEXITYhighlowselectionManyarticlesFewarticlesAggregationManywordsperFewwordsperRELATIVECLAUSEShighlowOperationssentence/clausesentence/clauseWITHCUEWORDhighlowCONJUNCTIONlowhighManyunﬁlledpausesFewunﬁlledpausesPERIODhighlow...PragmatictransformationsManynouns,adjectives,prepo-sitions(explicit)Manyverbs,adverbs,pronouns(implicit)SUBJECTIMPLICITNESSlowhighManynegationsFewnegationsNEGATIONINSERTIONhighlowManytentativewordsFewtentativewordsDOWNTONERHEDGES:·SORTOF,SOMEWHAT,QUITE,RATHER,ERR,ITHINKTHAT,ITSEEMSTHAT,ITSEEMSTOMETHAT,IMEANhighlow·AROUNDavgavgFormalInformal·KINDOF,LIKElowhighACKNOWLEDGMENTS:·YEAHlowhigh·RIGHT,OK,ISEE,WELLhighlowRealismExaggeration*EMPHASIZERHEDGES:·REALLY,BASICALLY,ACTUALLY,JUSTHAVE,JUSTIS,EXCLAMATIONlowhigh·YOUKNOWlowhighNopolitenessformPositivefaceredressment*TAGQUESTIONINSERTIONlowhighLowerwordcountHigherwordcountHEDGEVARIATIONlowavgHEDGEREPETITIONlowlowLexicalRichPoorLEXICONFREQUENCYlowhighchoiceFewpositiveemotionwordsManypositiveemotionwordsseepolarityparametersManynegativeemotionwordsFewnegativeemotionwordsseepolarityparametersTable2:Summaryoflanguagecuesforextraversion,basedonDewaeleandFurnham(1999);Furnham(1990);Mehletal.(2006);OberlanderandGill(2006);PennebakerandKing(1999),aswellasPERSON-AGE’scorrespondinggenerationparameters.Asterisksindicatehypotheses,ratherthanresults.Fordetailsonaggregationparameters,seeSection4.2.Relations:JUSTIFY(nuc:1,sat:2);JUSTIFY(nuc:1,sat:3);JUSTIFY(nuc:1,sat:4);JUSTIFY(nuc:1,sat:5);JUSTIFY(nuc:1,sat:6)Content:1.assert(best(LeMarais))2.assert(is(LeMarais,cuisine(French)))3.assert(has(LeMarais,food-quality(good)))4.assert(has(LeMarais,service(good)))5.assert(has(LeMarais,decor(decent)))6.assert(is(LeMarais,price(44dollars)))Figure1:Acontentplanforarecommendation.theselectedrestaurant(s),supportedbyasetofsatel-litecontentitemsdescribingtheirattributes.SeeTa-ble1.Claimscanbeexpressedindifferentways,suchasRESTAURANTNAMEisthebest,whiletheattributesatellitesfollowthepatternRESTAU-RANTNAMEhasMODIFIERATTRIBUTENAME,asinLeMaraishasgoodfood.RecommendationsarecharacterizedbyaJUSTIFYrhetoricalrelationassociatingtheclaimwithallothercontentitems,whicharelinkedtogetherthroughanINFERrelation.Incomparisons,theattributesofmultiplerestaurantsarecomparedusingaCONTRASTrelation.Anop-tionalclaimaboutthequalityofallrestaurantscanalsobeexpressedasthenucleusofanELABORATErelation,withtherestofthecontentplantreeasasatellite.3ContentPlanningContentplanningselectsandstructuresthecontenttobecommunicated.Table2speciﬁes10param-etershypothesizedtoaffectthisprocesswhichareexplainedbelow.Contentsize:Extravertsaremoretalkativethanintroverts(Furnham,1990;PennebakerandKing,1999),althoughitisnotclearwhethertheyactu-allyproducemorecontent,orarejustredundantandwordy.Thusvariousparametersrelatetotheamountandtypeofcontentproduced.TheVERBOSITYpa-rametercontrolsthenumberofcontentitemsse-lectedfromthecontentplan.Forexample,Alt-5inTable1isterse,whileAlt-2expressesalltheitemsinthecontentplan.TheREPETITIONparameteraddsanexactrepetition:thecontentitemisduplicatedandlinkedtotheoriginalcontentbyaRESTATE499

rhetoricalrelation.Inasimilarway,theRESTATE-MENTparameteraddsparaphrasesofcontentitemstotheplan,thatareobtainedfromtheinitialhand-craftedgenerationdictionary(seeSection4.1)andbyautomaticallysubstitutingcontentwordswiththemostfrequentWordNetsynonym(seeSection4.4).Alt-9inTable1containsrestatementsforthefoodqualityandtheatmosphereattributes.Polarity:Extravertstendtobemorepositive;in-trovertsarecharacterizedasengaginginmore‘prob-lemtalk’andexpressionsofdissatisfaction(Thorne,1987).Tocontrolforpolarity,contentitemsaredeﬁnedaspositiveornegativebasedonthescalarvalueofthecorrespondingattribute.Thetypeofcui-sineandneighborhoodattributeshaveneutralpolar-ity.Therearemultipleparametersassociatedwithpolarity.TheCONTENTPOLARITYparametercon-trolswhetherthecontentismostlynegative(e.g.Xhasmediocrefood),neutral(e.g.XisaThairestaurant),orpositive.Fromtheﬁlteredsetofcontentitems,thePOLARISATIONparameterdeter-mineswhethertheﬁnalcontentincludesitemswithextremescalarvalues(e.g.Xhasfantasticstaff).Inaddition,polaritycanalsobeimpliedmoresub-tlythroughrhetoricalstructure.TheCONCESSIONSparametercontrolshownegativeandpositiveinfor-mationispresented,i.e.whethertwocontentitemswithdifferentpolarityarepresentedobjectively,orifoneisforegroundedandtheotherbackgrounded.Iftwoopposedcontentitemsareselectedforacon-cession,aCONCESSrhetoricalrelationisinsertedbetweenthem.WhiletheCONCESSIONSparam-etercapturesthetendencytoputinformationintoperspective,theCONCESSIONPOLARITYparametercontrolswhetherthepositiveorthenegativecontentisconcessed,i.e.markedasthesatelliteoftheCON-CESSrelation.ThelastsentenceofAlt-3inTable1illustratesapositiveconcession,inwhichthegoodfoodqualityisputbeforethehighprice.Contentordering:Althoughextravertsusemorepositivelanguage(PennebakerandKing,1999;Thorne,1987),itisunclearhowtheypositionthepositivecontentwithintheirutterances.Addition-ally,thepositionoftheclaimaffectsthepersuasive-nessofanargument(CareniniandMoore,2000):startingwiththeclaimfacilitatesthehearer’sunder-standing,whileﬁnishingwiththeclaimismoreef-fectiveifthehearerdisagrees.ThePOSITIVECON-TENTFIRSTparameterthereforecontrolswhetherpositivecontentitems–includingtheclaim–appearﬁrstorlast,andtheorderinwhichthecontentitemsareaggregated.However,someoperationscanstillimposeaspeciﬁcordering(e.g.BECAUSEcuewordtorealizetheJUSTIFYrelation,seeSection4.2).4SentencePlanningSentenceplanningchoosesthelinguisticresourcesfromthelexiconandthesyntacticanddiscoursestructurestoachievethecommunicativegoalsspec-iﬁedintheinputcontentplan.Table2speciﬁesfoursetsofﬁndingsandparametersfordifferentaspectsofsentenceplanningdiscussedbelow.4.1SyntactictemplateselectionPERSONAGE’sinputgenerationdictionaryismadeof27DeepSyntacticStructures(DSyntS):9fortherecommendationclaim,12forthecomparisonclaim,andoneperattribute.SelectingaDSyntSre-quiresassigningitautomaticallytoapointinathreedimensionalspacedescribedbelow.AllparametervaluesarenormalizedoveralltheDSyntS,sotheDSyntSclosesttothetargetvaluecanbecomputed.Syntacticcomplexity:Furnham(1990)suggeststhatintrovertsproducemorecomplexconstructions:theCLAIMCOMPLEXITYparametercontrolsthedepthofthesyntacticstructurechosentorepresenttheclaim,e.g.theclaimXisthebestisratedaslesscomplexthanXisoneofmyfavoriterestaurants.Self-references:Extravertsmakemoreself-referencesthanintroverts(PennebakerandKing,1999).TheSELF-REFERENCEparametercontrolswhethertheclaimismadeintheﬁrstperson,basedonthespeaker’sownexperience,orwhethertheclaimisreportedasobjectiveorinformationob-tainedelsewhere.Theself-referencevalueisob-tainedfromthesyntacticstructurebycountingthenumberofﬁrstpersonpronouns.Forexample,theclaimofAlt-2inTable1,i.e.IamsureyouwouldlikeLeMarais,willberatedhigherthanLeMaraisisn’tasbadastheothersinAlt-5.Polarity:Whilepolaritycanbeexpressedbycon-tentselectionandstructure,itcanalsobedirectlyassociatedwiththeDSyntS.TheCLAIMPOLARITYparameterdeterminestheDSyntSselectedtorealizetheclaim.DSyntSaremanuallyannotatedforpo-larity.Forexample,Alt-4’sclaiminTable1,i.e.LeMaraisistheonlyrestaurantthatisanygood,hasalowerpolaritythanAlt-2.4.2AggregationoperationsSPaRKyaggregationoperationsareused(SeeStentetal.(2004)),withadditionaloperationsforconces-sionsandrestatements.SeeTable2.Theprobabil-ityoftheoperationsbiasestheproductionofcom-plexclauses,periodsandformalcuewordsforin-troverts,toexpresstheirpreferenceforcomplexsyn-500

tacticconstructions,longpausesandrichvocabulary(Furnham,1990).Thus,theintrovertparametersfa-voroperationssuchasRELATIVECLAUSEfortheINFERrelation,PERIODHOWEVERCUEWORDforCONTRAST,andALTHOUGHADVERBIALCLAUSEforCONCESS,thatwehypothesizetoresultinmoreformallanguage.Extravertaggregationproduceslongersentenceswithsimplerconstructionsandin-formalcuewords.ThusextravertutterancestendtouseoperationssuchasaCONJUNCTIONtorealizetheINFERandRESTATErelations,andtheEVENIFADVERBIALCLAUSEforCONCESSrelations.4.3PragmatictransformationsThissectiondescribestheinsertionofmarkersintheDSyntStoproducevariouspragmaticeffects.Hedges:Hedgescorrelatewithintroversion(Pen-nebakerandKing,1999)andaffectpoliteness(BrownandLevinson,1987).Thusthereareparam-etersforinsertingawiderangeofhedges,bothaf-fectiveandepistemic,suchaskindof,sortof,quite,rather,somewhat,like,around,err,Ithinkthat,itseemsthat,itseemstomethat,andImean.Alt-5inTable1showshedgeserranditseemstomethat.Tomodelextravertsuseofmoresociallanguage,agreementandbackchannelbehavior(DewaeleandFurnham,1999;PennebakerandKing,1999),weuseinformalacknowledgmentssuchasyeah,right,ok.AcknowledgmentsthatmayaffectintroversionareIsee,expressingself-referenceandcognitiveload,andthewellcuewordimplyingreservationfromthespeaker(seeAlt-9).Tomodelsocialconnectionandemotionweaddedmechanismsforinsertingemphasizerssuchasyouknow,basically,actually,justhave,justis,andexclamations.Alt-3inTable1showstheinsertionofyouknowandactually.Althoughsimilarhedgescanbegroupedtogether,eachhedgehasauniquepragmaticeffect.Forex-ample,youknowimpliespositive-faceredressment,whileactuallydoesn’t.Aparameterforeachhedgecontrolsthelikelihoodofitsselection.Tocontrolthegenerallevelofhedging,aHEDGEVARIATIONparameterdeﬁneshowmanydifferenthedgesareselected(maximumof5),whilethefre-quencyofanindividualhedgeiscontrolledbyaHEDGEREPETITIONparameter,uptoamaximumof2identicalhedgesperutterance.Thesyntacticstructureofhedgesaredeﬁnedaswellasconstraintsontheirinsertionpointintheut-terance’ssyntacticstructure.Eachtimeahedgeisselected,itisrandomlyinsertedatoneoftheinser-tionpointsrespectingtheconstraints,untilthespec-iﬁedfrequencyisreached.Forexample,aconstraintonthehedgekindofisthatitmodiﬁesadjectives.Tagquestions:Tagquestionsarealsopolite-nessmarkers(BrownandLevinson,1987).Theyredressthehearer’spositivefacebyclaimingcom-monground.ATAGQUESTIONINSERTIONparam-eterleadstonegatingtheauxiliaryoftheverbandpronominalizingthesubject,e.g.Xhasgreatfoodresultsintheinsertionofdoesn’tit?,asinAlt-8.Negations:Introvertsusesigniﬁcantlymorenegations(PennebakerandKing,1999).Althoughthecontentparametersselectmorenegativepolaritycontentitemsforintrovertutterances,wealsoma-nipulatenegations,whilekeepingthecontentcon-stant,byconvertingadjectivestothenegativeoftheirantonyms,e.g.theatmosphereisnicewastransformedtonotnastyinAlt-9inTable1.Subjectimplicitness:HeylighenandDewaele(2002)foundthatextravertsusemoreimplicitlan-guagethanintroverts.Tocontrolthelevelofimplic-itness,theSUBJECTIMPLICITNESSparameterdeter-mineswhetherpredicatesdescribingrestaurantat-tributesareexpressedwiththerestaurantinthesub-ject,orwiththeattributeitself(e.g.,ithasgoodfoodvs.thefoodistastyinAlt-9).4.4LexicalchoiceIntrovertsusearichervocabulary(DewaeleandFurnham,1999),sotheLEXICONFREQUENCYpa-rameterselectslexicalitemsbytheirnormalizedfre-quencyintheBritishNationalCorpus.WordNetsynonymsareusedtoobtainapoolofsynonyms,aswellasadjectivesextractedfromacorpusofrestau-rantreviewsforalllevelsofpolarity(e.g.thead-jectivetastyinAlt-9isahighpolaritymodiﬁerofthefoodattribute).Synonymsaremanuallycheckedtomakesuretheyareinterchangeable.Forexample,thecontentitemexpressedoriginallyasithasdecentserviceistransformedtoitfeaturesfriendlyserviceinAlt-2,andtotheserversareniceinAlt-3.5ExperimentalMethodandHypothesesOurprimaryhypothesisisthatlanguagegeneratedbyvaryingparameterssuggestedbypsycholinguis-ticresearchcanberecognizedasextravertorin-trovert.Totestthishypothesis,threeexpertjudgesevaluatedasetofgeneratedutterancesasiftheyhadbeenutteredbyafriendrespondinginadialoguetoarequesttorecommendrestaurants.Theseutteranceshadbeengeneratedtosystematicallymanipulateex-traversion/introversionparameters.Thejudgesratedeachutteranceforperceivedex-traversion,byansweringthetwoquestionsmeasur-501

ingthattraitfromtheTen-ItemPersonalityInven-tory,asthisinstrumentwasshowntobepsychome-tricallysuperiortoa‘singleitempertrait’question-naire(Goslingetal.,2003).Theanswersareaver-agedtoproduceanextraversionratingrangingfrom1(highlyintrovert)to7(highlyextravert).BecauseitwasunclearwhetherthegenerationparametersinTable2wouldproducenaturalsoundingutterances,thejudgesalsoevaluatedthenaturalnessofeachut-teranceonthesamescale.Thejudgesrated240ut-terances,groupedinto20setsof12utterancesgen-eratedfromthesamecontentplan.Theyratedonerandomlyorderedsetatatime,butviewedall12utterancesinthatsetbeforeratingthem.Theut-terancesweregeneratedtomeettwoexperimentalgoals.First,totestthedirectcontroloftheper-ceptionofextraversion.2introvertutterancesand2extravertutterancesweregeneratedforeachcon-tentplan(80intotal)usingtheparametervaluesinTable2.Multipleoutputsweregeneratedwithbothparametersettingsnormallydistributedwitha15%standarddeviation.Second,8utterancesforeachcontentplan(160intotal)weregeneratedwithrandomparametervalues.Theserandomutterancesmakeitpossibleto:(1)improvePERSONAGE’sdi-rectoutputbycalibratingitsparametersmorepre-cisely;and(2)buildastatisticalmodelthatselectsutterancesmatchinginputpersonalityvaluesafteranovergenerationphase(seeSection6.2).Theinter-rateragreementforextraversionbetweenthejudgesoverall240utterances(averagePearson’scorrela-tionof0.57)showsthatthemagnitudeofthediffer-encesofperceptionbetweenjudgesisalmostcon-stant(σ=.037).Alowagreementcanyieldahighcorrelation(e.g.ifallvaluesdifferbyaconstantfactor),sowealsocomputetheintraclasscorrela-tioncoefﬁcientrbasedonatwo-wayrandomeffectmodel.Weobtainarof0.79,whichissigniﬁcantatthep<.001level(reliabilityofaveragemea-sures,identicaltoCronbach’salpha).Thisiscom-parabletotheagreementofjudgmentsofpersonalityinMehletal.(2006)(meanr=0.84).6ExperimentalResults6.1HypothesizedparametersettingsTable1providesexamplesofPERSONAGE’sout-putandextraversionratings.ToassesswhetherPERSONAGEgenerateslanguagethatcanberec-ognizedasintrovertandextravert,wedidaindepen-dentsamplet-testbetweentheaverageratingsofthe40introvertand40extravertutterances(parameterswith15%standarddeviationasinTable2).Table3RatingIntrovertExtravertRandomExtraversion2.965.985.02Naturalness4.935.784.51Table3:Averageextraversionandnaturalnessrat-ingsfortheutterancesgeneratedwithintrovert,ex-travert,andrandomparameters.showsthatintrovertutteranceshaveanaveragerat-ingof2.96outof7whileextravertutteranceshaveanaverageratingof5.98.Theseratingsaresigniﬁ-cantlydifferentatthep<.001level(two-tailed).Inaddition,ifwedividethedataintotwoequal-widthbinsaroundtheneutralextravertrating(4outof7),thenPERSONAGE’sutteranceratingsfallinthebinpredictedbytheparameterset89.2%ofthetime.Extravertutterancearealsoslightlymorenat-uralthantheintrovertones(p<.001).Table3alsoshowsthatthe160randomparame-terutterancesproduceanaverageextraversionratingof5.02,bothsigniﬁcantlyhigherthantheintrovertsetandlowerthantheextravertset(p<.001).In-terestingly,therandomutterances,whichmaycom-binelinguisticvariablesassociatedwithbothintro-vertsandextraverts,arelessnaturalthantheintro-vert(p=.059)andextravertsets(p<.001).6.2StatisticalmodelsevaluationWealsoinvestigateasecondapproach:overgener-ationwithrandomparametersettings,followedbyrankingviaastatisticalmodeltrainedonthejudges’feedback.Thisapproachsupportsgeneratingutter-ancesforanyinputextraversionvalue,aswellasde-terminingwhichparametersaffectthejudges’per-ception.Wemodelperceivedpersonalityratings(1...7)withregressionmodelsfromtheWekatoolbox(Wit-tenandFrank,2005).Weusedthefulldatasetof160averagedratingsfortherandomparameterutter-ances.Eachutterancewasassociatedwithafeaturevectorwiththegenerationdecisionsforeachparam-eterinSection2.Toreducedatasparsity,weselectfeaturesthatcorrelatesigniﬁcantlywiththeratings(p<.10)withacoefﬁcienthigherthan0.1.Regressionmodelsareevaluatedusingthemeanabsoluteerrorandthecorrelationbetweenthepre-dictedscoreandtheactualaveragerating.Table4showsthemeanabsoluteerroronascalefrom1to7overten10-foldcross-validationsforthe4bestregressionmodels:LinearRegression(LR),M5’modeltree(M5),andSupportVectorMachines(i.e.SMOreg)withlinearkernels(SMO1)andradial-502

basisfunctionkernels(SMOr).Allmodelssignif-icantlyoutperformthebaseline(0.83meanabsoluteerror,p<.05),butsurprisinglythelinearmodelperformsthebestwithameanabsoluteerrorof0.65.Thebestmodelproducesacorrelationcoefﬁcientof0.59withthejudges’ratings,whichishigherthanthecorrelationsbetweenpairsofjudges,suggestingthatthemodelperformsaswellasahumanjudge.MetricLRM5SMO1SMOrAbsoluteerror0.650.660.720.70Correlation0.590.560.540.57Table4:Meanabsoluteregressionerrors(scalefrom1to7)andcorrelationcoefﬁcientsoverten10-foldcross-validations,for4models:LinearRegression(LR),M5’modeltree(M5),SupportVectorMa-chineswithlinearkernels(SMO1)andradial-basisfunctionkernels(SMOr).Allmodelssigniﬁcantlyoutperformthemeanbaseline(0.83error,p<.05).TheM5’regressiontreeinFigure2assignsarat-inggiventhefeatures.Verbosityplaysthemostim-portantrole:utteranceswith4ormorecontentitemsaremodeledasmoreextravert.Givenalowver-bosity,lexicalfrequencyandrestatementsdeterminetheextraversionlevel,e.g.utteranceswithlessthan4contentitemsandinfrequentwordsareperceivedasveryintroverted(ratingof2.69outof7).Forverboseutterances,theyouknowhedgeindicatesextraversion,aswellasconcessions,restatements,self-references,andpositivecontent.Althoughrel-ativelysimple,thesemodelsareusefulforidentify-ingnewpersonalitymarkers,aswellascalibratingparametersinthedirectgenerationmodel.7DiscussionandConclusionsWepresentandevaluatePERSONAGE,aparame-terizablegeneratorthatproducesoutputsthatvaryalongtheextraversionpersonalitydimension.Thispapermakesfourcontributions:1.Wepresentasystematicreviewofpsycholinguisticﬁnd-ings,organizedbytheNLGreferencearchitecture;2.WeproposeamappingfromtheseﬁndingstogenerationparametersforeachNLGmoduleandareal-timeimple-mentationofageneratorusingtheseparameters2.Toourknowledgethisistheﬁrstattempttoputforwardasys-tematicframeworkforgeneratinglanguagethatmanifestspersonality;3.Wepresentanevaluationexperimentshowingthatwecancontroltheparameterstoproducerecognizablelinguis-ticvariationalongtheextraversionpersonalitydimen-sion.Thus,weshowthattheweakcorrelationsreported2Anonlinedemoisavailableatwww.dcs.shef.ac.uk/cogsys/personage.htmlinothergenresoflanguage,andforself-reportsratherthanobservers,carryovertotheproductionofsingleeval-uativeutteranceswithrecognizablepersonalityinare-stricteddomain;4.Wepresenttheresultsofatrainingexperimentshowingthatgivenanoutput,wecantrainamodelthatmatcheshumanperformanceinassigninganextraversionratingtothatoutput.Someofthechallengesdiscussedintheintroduc-tionremain.Wehaveshownthatevaluativeutter-ancesintherestaurantdomaincanmanifestperson-ality,butmoreresearchisneededonwhichspeechactsrecognisablymanifestpersonalityinarestricteddomain.Wealsoshowedthatthemappingwehy-pothesisedofﬁndingstogenerationparameterswaseffective,buttheremaybeadditionalparametersthatthepsycholinguisticﬁndingscouldbemappedto.OurworkwaspartiallyinspiredbytheICONO-CLASTandPAULINEparameterizablegenerators(Bouayad-Aghaetal.,2000;Hovy,1988),whichvarythestyle,ratherthanthepersonality,ofthegen-eratedtexts.Walkeretal.(1997)describeagen-eratorintendedtoaffectperceptionsofpersonality,basedonBrownandLevinson’stheoryofpolite-ness(BrownandLevinson,1987),thatusessomeofthelinguisticconstructionsimplementedhere,suchastagquestionsandhedges,butitwasneverevaluated.ResearchbyAndr´eetal.(2000);Piwek(2003)usespersonalityvariablestoaffectthelin-guisticbehaviourofconversationalagents,buttheydidnotsystematicallymanipulateparameters,andtheirgeneratorswerenotevaluated.ReevesandNass(1996)demonstratethatmanipulationsofper-sonalityaffectmanyaspectsofuser’sperceptions,buttheirexperimentsusehandcraftedutterances,ratherthangeneratedutterances.CassellandBick-more(2003)showthatextravertsprefersystemsuti-lizingdiscourseplansthatincludesmalltalk.PaivaandEvans’trainablegenerator(2005)producesout-putsthatcorrespondtoasetoflinguisticvariablesmeasuredinacorpusoftargettexts.Theirmethodissimilartoourstatisticalmethodusingregressiontrees,butprovidesdirectcontrol.Themethodre-portedinMairesseandWalker(2005)fortrainingindividualizedsentenceplannersrankstheoutputsproducedbyanovergenerationphase,ratherthandi-rectlypredictingascalarvalue,aswedohere.TheclosestworktooursisprobablyIsardetal.’sCRAG-2system(2006),whichovergeneratesandranksus-ingngramlanguagemodelstrainedonacorpusla-belledforallBigFivepersonalitydimensions.How-ever,CRAG-2hasnoexplicitparametercontrol,andithasyettobeevaluated.503

Max BNC FrequencyRestatementsVerbosity> 0.022.693.524.474.123.26> 0.1> 2.5> 0.64<= 0.643.74Max BNC FrequencyConcessions> 0.87Self−referencesRestatements> 0.5> 0.55.33Verbosity5.085.53> 5.55.85> 0.55.00> 0.5..<= 0.02<= 2.5<= 0.5<= 0.5<= 0.5<= 0.5<= 0.5<= 5.5> 0.5> 3.5<= 0.87> 0.5<= 0.1Max BNC FrequencyVerbosityVerbosity> 4.5<= 4.5.Infer aggregation:       Period<= 0.54.52<= 3.5Hedge: ‘you know’5.93Content5.545.78PolarityFigure2:M5’regressiontree.Theoutputrangesfrom1to7,where7meansstronglyextravert.Infuturework,wehopetodirectlycomparethedirectgenerationmethodofSection6.1withtheovergenerateandrankmethodofSection6.2,andtousetheseresultstoreﬁnePERSONAGE’sparame-tersettings.WealsohopetoextendPERSONAGE’sgenerationcapabilitiestootherBigFivetraits,iden-tifyadditionalfeaturestoimprovethemodel’sper-formance,andevaluatetheeffectofpersonalityvari-ationonusersatisfactioninvariousapplications.ReferencesE.Andr´e,T.Rist,S.vanMulken,M.Klesen,andS.Baldes.2000.Theautomateddesignofbelievabledialoguesforanimatedpresentationteams.InEmbodiedconversationalagents,p.220–255.MITPress,Cambridge,MA.N.Bouayad-Agha,D.Scott,andR.Power.2000.Integratingcontentandstyleindocuments:acasestudyofpatientin-formationleaﬂets.InformationDesignJournal,9:161–176.P.BrownandS.Levinson.1987.Politeness:Someuniversalsinlanguageusage.CambridgeUniversityPress.G.CareniniandJ.D.Moore.2000.Astrategyforgeneratingevaluativearguments.InProc.ofInternationalConferenceonNaturalLanguageGeneration,p.47–54.J.CassellandT.Bickmore.2003.Negotiatedcollusion:Model-ingsociallanguageanditsrelationshipeffectsinintelligentagents.UserModelingandUser-AdaptedInteraction,13(1-2):89–132.J-M.DewaeleandA.Furnham.1999.Extraversion:theunlovedvariableinappliedlinguisticresearch.LanguageLearning,49(3):509–544.A.Furnham.1990.Languageandpersonality.InHandbookofLanguageandSocialPsychology.Winley.S.D.Gosling,P.J.Rentfrow,andW.B.SwannJr.2003.Averybriefmeasureofthebigﬁvepersonalitydomains.JournalofResearchinPersonality,37:504–528.F.HeylighenandJ-M.Dewaele.2002.Variationinthecon-textualityoflanguage:anempiricalmeasure.ContextinContext,FoundationsofScience,7(3):293–340.E.Hovy.1988.GeneratingNaturalLanguageunderPragmaticConstraints.LawrenceErlbaumAssociates.A.Isard,C.Brockmann,andJ.Oberlander.2006.Individualityandalignmentingenerateddialogues.InProc.ofINLG.B.LavoieandO.Rambow.1997.Afastandportablerealizerfortextgenerationsystems.InProc.ofANLP.A.LoyallandJ.Bates.1997.Personality-richbelievableagentsthatuselanguage.InProc.oftheFirstInternationalConfer-enceonAutonomousAgents,p.106–113.F.MairesseandM.Walker.2005.Learningtopersonalizespo-kengenerationfordialoguesystems.InProc.oftheInter-speech-Eurospeech,p.1881–1884.M.Mehl,S.Gosling,andJ.Pennebaker.2006.Personalityinitsnaturalhabitat:Manifestationsandimplicitfolktheoriesofpersonalityindailylife.JournalofPersonalityandSocialPsychology,90:862–877.W.T.Norman.1963.Towardanadequatetaxonomyofper-sonalityattributes:Replicatedfactorstructureinpeernom-inationpersonalityrating.JournalofAbnormalandSocialPsychology,66:574–583.J.OberlanderandA.Gill.2006.Languagewithcharacter:Astratiﬁedcorpuscomparisonofindividualdifferencesine-mailcommunication.DiscourseProcesses,42:239–270.D.PaivaandR.Evans.2005.Empirically-basedcontrolofnat-urallanguagegeneration.InProc.ofACL.J.W.PennebakerandL.A.King.1999.Linguisticstyles:Lan-guageuseasanindividualdifference.JournalofPersonalityandSocialPsychology,77:1296–1312.P.Piwek.2003.Aﬂexiblepragmatics-drivenlanguagegenera-torforanimatedagents.InProc.ofEACL.B.ReevesandC.Nass.1996.TheMediaEquation.UniversityofChicagoPress.E.ReiterandR.Dale.2000.BuildingNaturalLanguageGen-erationSystems.CambridgeUniversityPress.A.Stent,R.Prasad,andM.Walker.2004.Trainablesentenceplanningforcomplexinformationpresentationinspokendi-alogsystems.InProc.ofACL.A.Thorne.1987.Thepressofpersonality:Astudyofconver-sationsbetweenintrovertsandextraverts.JournalofPerson-alityandSocialPsychology,53:718–726.M.Walker,J.Cahn,andS.Whittaker.1997.Improvisinglin-guisticstyle:Socialandaffectivebasesforagentpersonality.InProc.oftheConferenceonAutonomousAgents.I.H.WittenandE.Frank.2005.DataMining:Practicalma-chinelearningtoolsandtechniques.MorganKaufmann.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 504–511,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

504

MakingSenseofSound:UnsupervisedTopicSegmentationoverAcousticInputIgorMalioutov,AlexPark,ReginaBarzilay,andJamesGlassMassachusettsInstituteofTechnology{igorm,malex,regina,glass}@csail.mit.eduAbstractWeaddressthetaskofunsupervisedtopicsegmentationofspeechdataoperatingoverrawacousticinformation.Incontrasttoex-istingalgorithmsfortopicsegmentationofspeech,ourapproachdoesnotrequirein-puttranscripts.Ourmethodpredictstopicchangesbyanalyzingthedistributionofre-occurringacousticpatternsinthespeechsig-nalcorrespondingtoasinglespeaker.Thealgorithmrobustlyhandlesnoiseinherentinacousticmatchingbyintelligentlyaggregat-inginformationaboutthesimilarityproﬁlefrommultiplelocalcomparisons.Ourex-perimentsshowthataudio-basedsegmen-tationcomparesfavorablywithtranscript-basedsegmentationcomputedovernoisytranscripts.Theseresultsdemonstratethedesirabilityofourmethodforapplicationswhereaspeechrecognizerisnotavailable,oritsoutputhasahighworderrorrate.1IntroductionAnimportantpracticalapplicationoftopicsegmen-tationistheanalysisofspokendata.Paragraphbreaks,sectionmarkersandotherstructuralcuescommoninwrittendocumentsareentirelymissinginspokendata.Insertionofthesestructuralmarkerscanbeneﬁtmultiplespeechprocessingapplications,includingaudiobrowsing,retrieval,andsummariza-tion.Notsurprisingly,avarietyofmethodsfortopicsegmentationhavebeendevelopedinthepast(Beefermanetal.,1999;Galleyetal.,2003;DielmannandRenals,2005).Thesemethodstypi-callyassumethatasegmentationalgorithmhasac-cessnotonlytoacousticinput,butalsotoitstran-script.Thisassumptionisnaturalforapplicationswherethetranscripthastobecomputedaspartofthesystemoutput,oritisreadilyavailablefromothersystemcomponents.However,forsomedomainsandlanguages,thetranscriptsmaynotbeavailable,ortherecognitionperformancemaynotbeadequatetoachievereliablesegmentation.Inordertoprocesssuchdata,weneedamethodfortopicsegmentationthatdoesnotrequiretranscribedinput.Inthispaper,weexploreamethodfortopicseg-mentationthatoperatesdirectlyonarawacousticspeechsignal,withoutusinganyinputtranscripts.Thismethodpredictstopicchangesbyanalyzingthedistributionofreoccurringacousticpatternsinthespeechsignalcorrespondingtoasinglespeaker.Inthesamewaythatunsupervisedsegmentationalgo-rithmspredictboundariesbasedonchangesinlexi-caldistribution,ouralgorithmisdrivenbychangesinthedistributionofacousticpatterns.Thecentralhypothesishereisthatsimilarsoundingacousticse-quencesproducedbythesamespeakercorrespondtosimilarlexicographicsequences.Thus,byana-lyzingthedistributionofacousticpatternswecouldapproximateatraditionalcontentanalysisbasedonthelexicaldistributionofwordsinatranscript.Analyzinghigh-levelcontentstructurebasedonlow-levelacousticfeaturesposesinterestingcompu-tationalandlinguisticchallenges.Forinstance,weneedtohandlethenoiseinherentinmatchingbasedonacousticsimilarity,becauseofpossiblevaria-505

tionsinspeakingrateorpronunciation.Moreover,intheabsenceofhigher-levelknowledge,informa-tionaboutwordboundariesisnotalwaysdiscerniblefromtherawacousticinput.Thiscausesproblemsbecausewehavenoobviousunitofcomparison.Fi-nally,noiseinherentintheacousticmatchingpro-cedurecomplicatesthedetectionofdistributionalchangesinthecomparisonmatrix.Thealgorithmpresentedinthispaperdemon-stratesthefeasibilityoftopicsegmentationoverrawacousticinputcorrespondingtoasinglespeaker.Weﬁrstapplyavariantofthedynamictimewarpingal-gorithmtoﬁndsimilarfragmentsinthespeechinputthroughalignment.Next,weconstructacompari-sonmatrixthataggregatestheoutputofthealign-mentstage.Sincealignedutterancesareseparatedbygapsanddifferinduration,thisrepresentationgivesrisetosparseandirregularinput.Toobtainro-bustsimilaritychangedetection,weinvokeaseriesoftransformationstosmoothandreﬁnethecompar-isonmatrix.Finally,weapplytheminimum-cutseg-mentationalgorithmtothetransformedcomparisonmatrixtodetecttopicboundaries.Wecomparetheperformanceofourmethodagainsttraditionaltranscript-basedsegmentational-gorithms.Asexpected,theperformanceofthelat-terdependsontheaccuracyoftheinputtranscript.Whenamanualtranscriptionisavailable,thegapbetweenaudio-basedsegmentationandtranscript-basedsegmentationissubstantial.However,inamorerealisticscenariowhenthetranscriptsarefraughtwithrecognitionerrors,thetwoapproachesexhibitsimilarperformance.Theseresultsdemon-stratethataudio-basedalgorithmsareaneffectiveandefﬁcientsolutionforapplicationswheretran-scriptsareunavailableorhighlyerrorful.2RelatedWorkSpeech-basedTopicSegmentationAvarietyofsupervisedandunsupervisedmethodshavebeenemployedtosegmentspeechinput.Someofthesealgorithmshavebeenoriginallydevelopedforpro-cessingwrittentext(Beefermanetal.,1999).Othersarespeciﬁcallyadaptedforprocessingspeechinputbyaddingrelevantacousticfeaturessuchaspauselengthandspeakerchange(Galleyetal.,2003;Diel-mannandRenals,2005).Inparallel,researchersex-tensivelystudytherelationshipbetweendiscoursestructureandintonationalvariation(HirschbergandNakatani,1996;Shribergetal.,2000).However,alloftheexistingsegmentationmethodsrequireasinputaspeechtranscriptofreasonablequality.Incontrast,themethodpresentedinthispaperdoesnotassumetheavailabilityoftranscripts,whichpre-ventsusfromusingsegmentationalgorithmsdevel-opedforwrittentext.Atthesametime,ourworkiscloselyrelatedtounsupervisedapproachesfortextsegmentation.Thecentralassumptionhereisthatsharpchangesinlex-icaldistributionsignalthepresenceoftopicbound-aries(Hearst,1994;Choietal.,2001).Theseap-proachesdeterminesegmentboundariesbyidenti-fyinghomogeneousregionswithinasimilarityma-trixthatencodespairwisesimilaritybetweentextualunits,suchassentences.Oursegmentationalgo-rithmoperatesoveradistortionmatrix,buttheunitofcomparisonisthespeechsignaloveratimein-terval.Thischangeinrepresentationgivesrisetomultiplechallengesrelatedtotheinherentnoiseofacousticmatching,andrequiresthedevelopmentofnewmethodsforsignaldiscretization,intervalcom-parisonandmatrixanalysis.PatternInductioninAcousticDataOurworkisrelatedtoresearchonunsupervisedlexicalacqui-sitionfromcontinuousspeech.Thesemethodsaimtoinfervocabularyfromunsegmentedaudiostreamsbyanalyzingregularitiesinpatterndistribution(deMarcken,1996;Brent,1999;Venkataraman,2001).Traditionally,thespeechsignalisﬁrstconvertedintoastring-likerepresentationsuchasphonemesandsyllablesusingaphoneticrecognizer.ParkandGlass(2006)haverecentlyshownthefeasibilityofanaudio-basedapproachforworddis-covery.Theyinducethevocabularyfromtheau-diostreamdirectly,avoidingtheneedforphonetictranscription.Theirmethodcanaccuratelydiscoverwordswhichappearwithhighfrequencyintheau-diostream.WhiletheresultsobtainedbyParkandGlassinspireourapproach,wecannotdirectlyusetheiroutputasproxiesforwordsintopicsegmen-tation.Manyofthecontentwordsoccurringonlyafewtimesinthetextareprunedawaybythismethod.Ourresultsshowthatthisdatathatistoosparseandnoisyforrobustlydiscerningchangesin506

lexicaldistribution.3AlgorithmTheaudio-basedsegmentationalgorithmidentiﬁestopicboundariesbyanalyzingchangesinthedis-tributionofacousticpatterns.Theanalysisisper-formedinthreesteps.First,weidentifyrecurringpatternsintheaudiostreamandcomputedistortionbetweenthem(Section3.1).Theseacousticpatternscorrespondtohigh-frequencywordsandphrases,buttheyonlycoverafractionofthewordsthatap-pearintheinput.Asaresult,thedistributionalpro-ﬁleobtainedduringthisprocessistoosparsetode-liverrobusttopicanalysis.Second,wegenerateanacousticcomparisonmatrixthataggregatesinfor-mationfrommultiplepatternmatches(Section3.2).Additionalmatrixtransformationsduringthisstepreducethenoiseandirregularitiesinherentinacous-ticmatching.Third,wepartitionthematrixtoiden-tifysegmentswithahomogeneousdistributionofacousticpatterns(Section3.3).3.1ComparingAcousticPatternsGivenarawacousticwaveform,weextractasetofacousticpatternsthatoccurfrequentlyinthespeechdocument.Continuousspeechincludesmanywordsequencesthatlackclearlow-levelacousticcuestodenotewordboundaries.Therefore,wecannotper-formthistaskthroughsimplecountingofspeechsegmentsseparatedbysilence.Instead,weusealo-calalignmentalgorithmtosearchforsimilarspeechsegmentsandquantifytheamountofdistortionbe-tweenthem.Inwhatfollows,weﬁrstpresentavec-torrepresentationusedinthiscomputation,andthenspecifythealignmentalgorithmthatﬁndssimilarsegments.MFCCRepresentationWestartbytransformingtheacousticsignalintoavectorrepresentationthatfacilitatesthecomparisonofacousticsequences.First,weperformsilencedetectionontheoriginalwaveformbyregisteringapauseiftheenergyfallsbelowacertainthresholdforadurationof2s.Thisenablesustobreakuptheacousticstreamintocon-tinuousspokenutterances.Thisstepisnecessaryasiteliminatesspuriousalignmentsbetweensilentregionsoftheacousticwaveform.Notethatsilencedetectionisnotequiv-alenttowordboundarydetection,assegmentationbysilencedetectionaloneonlyaccountsfor20%ofwordboundariesinourcorpus.Next,weconverteachutteranceintoatimese-riesofvectorsconsistingofMel-scalecepstralco-efﬁcients(MFCCs).Thiscompactlow-dimensionalrepresentationiscommonlyusedinspeechprocess-ingapplicationsbecauseitapproximateshumanau-ditorymodels.TheprocessofextractingMFCCsfromthespeechsignalcanbesummarizedasfollows.First,the16kHzdigitizedaudiowaveformisnormalizedbyre-movingthemeanandscalingthepeakamplitude.Next,theshort-timeFouriertransformistakenataframeintervalof10msusinga25.6msHam-mingwindow.ThespectralenergyfromtheFouriertransformisthenweightedbyMel-frequencyﬁl-ters(Huangetal.,2001).Finally,thediscretecosinetransformofthelogoftheseMel-frequencyspec-tralcoefﬁcientsiscomputed,yieldingaseriesof14-dimensionalMFCCvectors.Wetaketheadditionalstepofwhiteningthefeaturevectors,whichnormal-izesthevarianceanddecorrelatesthedimensionsofthefeaturevectors(Bishop,1995).Thiswhitenedspectralrepresentationenablesustousethestan-dardunweightedEuclideandistancemetric.Afterthistransformation,thedistancesineachdimensionwillbeuncorrelatedandhaveequalvariance.AlignmentNow,ourgoalistoidentifyacousticpatternsthatoccurmultipletimesintheaudiowave-form.Thepatternsmaynotberepeatedexactly,butwillmostlikelyreoccurinvariedforms.Wecapturethisinformationbyextractingpairsofpatternswithanassociateddistortionscore.Thecomputationisperformedusingasequencealignmentalgorithm.Table1showsexamplesofalignmentsautomati-callycomputedbyouralgorithm.Thecorrespond-ingphonetictranscriptions1demonstratethatthematchingprocedurecanrobustlyhandlevariationsinpronunciations.Forexample,twoinstancesoftheword“direction”arematchedtooneanotherdespitedifferentpronunciations,(“day”vs.“dax”intheﬁrstsyllable).Atthesametime,somealignedpairsformerroneousmatches,suchas“myprediction”matching“ydirection”duetotheirhighacoustic1Phonetictranscriptionsarenotusedbyouralgorithmandareprovidedforillustrativepurposesonly.507

AlignedWord(s)PhoneticTranscriptionthexdirectiondhiyehkclksdcldaxrehkclshaxnDiyEk^ksd^d@rEk^S@ntheydirectiondhaxwaydcldayrehkclshepienD@wayd^ayrEk^kS@nofmypredictionaxvmaykclkriyliykclkshaxn@vmayk^kriyliyk^kS@naccelerationehkclksehlaxreyshepienEk^ksEl@rEyS-n"accelerationaxkclksahnaxrehnepishepien@k^ks2n@rEn-S-n"thederivationdcldihdxihzdcldheyshepiend^dIRIzd^DEyS-n"ademonstrationuhdcldehmaxnepistcltreyshenUd^dEm@n-st^trEySn"Table1:AlignedWordPaths.Eachgroupofrowsrepresentsaudiosegmentsthatwerealignedtooneanother,alongwiththeircorrespondingphonetictranscriptionsusingTIMITconventions(Garofoloetal.,1993)andtheirIPAequivalents.similarity.Thealignmentalgorithmoperatesontheaudiowaveformrepresentedbyalistofsilence-freeutter-ances(u1,u2,...,un).Eachutteranceu0isatimeseriesofMFCCvectors(~x01,~x02,...,~x0m).Giventwoinpututterancesu0andu00,thealgorithmout-putsasetofalignmentsbetweenthecorrespondingMFCCvectors.ThealignmentdistortionscoreiscomputedbysummingtheEuclideandistancesofmatchingvectors.Tocomputetheoptimalalignmentweuseavari-antofthedynamictimewarpingalgorithm(Huangetal.,2001).Foreverypossiblestartingalignmentpoint,weoptimizethefollowingdynamicprogram-mingobjective:D(ik,jk)=d(ik,jk)+minD(ik(cid:0)1,jk)D(ik,jk(cid:0)1)D(ik(cid:0)1,jk(cid:0)1)Intheequationabove,ikandjkarealignmentend-pointsinthek-thsubproblemofdynamicprogram-ming.Thisobjectivecorrespondstoadescentthroughadynamicprogrammingtrellisbychoosingright,down,ordiagonalstepsateachstage.Duringthesearchprocess,weconsidernotonlythealignmentdistortionscore,butalsotheshapeofthealignmentpath.Tolimittheamountoftemporalwarping,weenforcethefollowingconstraint:(cid:12)(cid:12)(cid:0)ik(cid:0)i1(cid:1)(cid:0)(cid:0)jk(cid:0)j1(cid:1)(cid:12)(cid:12)(cid:20)R,8k,(1)ik(cid:20)Nxandjk(cid:20)Ny,whereNxandNyarethenumberofMFCCsamplesineachutterance.Thevalue2R+1isthewidthofthediagonalbandthatcontrolstheextentoftempo-ralwarping.TheparameterRistunedonadevelop-mentset.Thisalignmentproceduremayproducepathswithhighdistortionsubpaths.Therefore,wetrimeachpathtoretainthesubpathwithlowestaveragedis-tortionandlengthatleastL.Moreformally,givenanalignmentoflengthN,weseektoﬁndmandnsuchthat:argmin1≤m≤n≤N1n(cid:0)m+1nXk=md(ik,jk)n(cid:0)m(cid:21)LWeaccomplishthisbycomputingthelengthcon-strainedminimumaveragedistortionsubsequenceofthepathsequenceusinganO(Nlog(L))algo-rithmproposedbyLinetal(2002).Thelengthparameter,L,allowsustoavoidovertrimmingandcontrolthelengthofalignmentsthatarefound.Af-tertrimming,thedistortionofeachalignmentpathisnormalizedbythepathlength.Alignmentswithadistortionexceedingaprespec-iﬁedthresholdareprunedawaytoensurethatthealignedphrasalunitsarecloseacousticmatches.Thisparameteristunedonadevelopmentset.Inthenextsection,wedescribehowtoaggregateinformationfrommultiplenoisymatchesintoarep-resentationthatfacilitatesboundarydetection.3.2ConstructionofAcousticComparisonMatrixThegoalofthisstepistoconstructanacousticcom-parisonmatrixthatwillguidetopicsegmentation.Thismatrixencodesvariationsinthedistributionofacousticpatternsforagivenspeechdocument.Weconstructthismatrixbyﬁrstdiscretizingtheacousticsignalintoconstant-lengthblocksandthencomput-ingthedistortionbetweenpairsofblocks.508

Figure1:a)SimilaritymatrixforaPhysicslectureconstructedusingamanualtranscript.b)Similaritymatrixforthesamelectureconstructedfromacousticdata.Theintensityofapixelindicatesthedegreeofblocksimilarity.c)Acousticcomparisonmatrixafter2000iterationsofanisotropicdiffusion.Verticallinescorrespondtothereferencesegmentation.Unfortunately,thepathsanddistortionsgeneratedduringthealignmentstep(Section3.1)cannotbemappeddirectlytoanacousticcomparisonmatrix.Sincewecompareonlycommonlyrepeatedacous-ticpatterns,someportionsofthesignalcorrespondtogapsbetweenalignmentpaths.Infact,inourcor-pusonly67%ofthedataiscoveredbyalignmentpathsfoundduringthealignmentstage.Moreover,manyofthesepathsarenotdisjoint.Forinstance,ourexperimentsshowthat74%ofthemoverlapwithatleastoneadditionalalignmentpath.Finally,thesealignmentsvarysigniﬁcantlyinduration,rangingfrom0.350msto2.7msinourcorpus.DiscretizationandDistortionComputationTocompensatefortheirregulardistributionofalign-mentpaths,wequantizethedatabysplittingthein-putsignalintouniformcontiguoustimeblocks.Atimeblockdoesnotnecessarilycorrespondtoanyonediscoveredalignmentpath.Itmaycontainsev-eralcompletepathsandalsoportionsofotherpaths.WecomputetheaggregatedistortionscoreD(x,y)oftwoblocksxandybysummingthedistortionsofallalignmentpathsthatfallwithinxandy.MatrixSmoothingEquippedwithablockdis-tortionmeasure,wecannowconstructanacousticcomparisonmatrix.Inprinciple,thismatrixcanbeprocessedemployingstandardmethodsdevelopedfortextsegmentation.However,asFigure1illus-trates,thestructureoftheacousticmatrixisquitedifferentfromtheoneobtainedfromtext.Inatran-scriptsimilaritymatrixshowninFigure1a),refer-enceboundariesdelimithomogeneousregionswithhighinternalsimilarity.Ontheotherhand,lookingattheacousticsimilaritymatrix2showninFigure1b),itisdifﬁculttoobserveanyblockstructurecor-respondingtothereferencesegmentation.Thisdeﬁciencycanbeattributedtothesparsityofacousticalignments.Consider,forexample,thecasewhenasegmentisinterspersedwithblocksthatcon-tainveryfewornocompletepaths.Eventhoughtherestoftheblocksinthesegmentcouldbecloselyrelated,thesepath-freeblocksdilutesegmenthomo-geneity.Thisisproblematicbecauseitisnotalwayspossibletotellwhetherasuddenshiftinscoressig-niﬁesatransitionorifitisjustanartifactofirreg-ularitiesinacousticmatching.Withoutadditionalmatrixprocessing,theseirregularitieswillleadthesystemastray.Wefurtherreﬁnetheacousticcomparisonmatrixusinganisotropicdiffusion.Thistechniquehasbeendevelopedforenhancingedgedetectionaccuracyinimageprocessing(PeronaandMalik,1990),andhasbeenshowntobeaneffectivesmoothingmethodintextsegmentation(JiandZha,2003).Whenap-pliedtoacomparisonmatrix,anisotropicdiffusionreducesscorevariabilitywithinhomogeneousre-2Weconvertedtheoriginalcomparisondistortionmatrixtothesimilaritymatrixbysubtractingthecomponentdistortionsfromthemaximumalignmentdistortionscore.509

gionsofthematrixandmakesedgesbetweentheseregionsmorepronounced.Consequently,thistrans-formationfacilitatesboundarydetection,potentiallyincreasingsegmentationaccuracy.InFigure1c),wecanobservethattheboundarystructureinthedif-fusedcomparisonmatrixbecomesmoresalientandcorrespondsmorecloselytothereferencesegmen-tation.3.3MatrixPartitioningGivenatargetnumberofsegmentsk,thegoalofthepartitioningstepistodivideamatrixintoksquaresubmatricesalongthediagonal.Thispro-cessisguidedbyanoptimizationfunctionthatmax-imizesthehomogeneitywithinasegmentormini-mizesthehomogeneityacrosssegments.Thisopti-mizationproblemcanbesolvedusingoneofmanyunsupervisedsegmentationapproaches(Choietal.,2001;JiandZha,2003;MalioutovandBarzilay,2006).Inourimplementation,weemploytheminimum-cutsegmentationalgorithm(ShiandMalik,2000;MalioutovandBarzilay,2006).Inthisgraph-theoreticframework,segmentationiscastasaprob-lemofpartitioningaweightedundirectedgraphthatminimizesthenormalized-cutcriterion.Theminimum-cutmethodachievesrobustanalysisbyjointlyconsideringallpossiblepartitioningsofadocument,movingbeyondlocalizeddecisions.Thisallowsustoaggregatecomparisonsfrommultiplelocations,therebycompensatingforthenoiseofin-dividualmatches.4EvaluationSet-UpDataWeuseapubliclyavailable3corpusofintro-ductoryPhysicslecturesdescribedinourpreviouswork(MalioutovandBarzilay,2006).Thismate-rialisaparticularlyappealingapplicationareaforanaudio-basedsegmentationalgorithm—manyaca-demicsubjectslacktranscribeddatafortraining,whileahighratioofin-domaintechnicaltermslim-itstheuseofout-of-domaintranscripts.Thiscorpusisalsochallengingfromthesegmentationperspec-tivebecausethelecturesarelongandtransitionsbe-tweentopicsaresubtle.3Seehttp://www.csail.mit.edu/˜igorm/acl06.htmlThecorpusconsistsof33lectures,withanaver-agelengthof8500wordsandanaveragedurationof50minutes.Onaverage,alecturewasanno-tatedwithsixsegments,andatypicalsegmentcor-respondstotwopagesofatranscript.Threelecturesfromthissetwereusedfordevelopment,and30lec-tureswereusedfortesting.Thelecturesweredeliv-eredbythesamespeaker.Toevaluatetheperformanceoftraditionaltranscript-basedsegmentationalgorithmsonthiscorpus,wealsouseseveraltypesoftranscriptsatdifferentlevelsofrecognitionaccuracy.Inaddi-tiontomanualtranscripts,ourcorpuscontainstwotypesofautomatictranscripts,oneobtainedusingspeaker-dependent(SD)modelsandtheotherob-tainedusingspeaker-independent(SI)models.Thespeaker-independentmodelwastrainedon85hoursofout-of-domaingenerallecturematerialandcon-tainednospeechfromthespeakerinthetestset.Thespeaker-dependentmodelwastrainedbyus-ing38hoursofaudiodatafromotherlecturesgivenbythespeaker.Bothrecognizersincorporatedwordstatisticsfromtheaccompanyingclasstextbookintothelanguagemodel.Theworderrorratesforthespeaker-independentandspeaker-dependentmodelsare44.9%and19.4%,respectively.EvaluationMetricsWeusethePkandWindowD-iffmeasurestoevaluateoursystem(Beefermanetal.,1999;PevznerandHearst,2002).ThePkmea-sureestimatestheprobabilitythatarandomlycho-senpairofwordswithinawindowoflengthkwordsisinconsistentlyclassiﬁed.TheWindowDiffmet-ricisavariantofthePkmeasure,whichpenalizesfalsepositivesandnearmissesequally.Forbothofthesemetrics,lowerscoresindicatebettersegmen-tationaccuracy.BaselineWeusethestate-of-the-artmincutseg-mentationsystembyMalioutovandBarzilay(2006)asourpointofcomparison.Thismodelisanappro-priatebaseline,becauseithasbeenshowntocom-parefavorablywithothertop-performingsegmenta-tionsystems(Choietal.,2001;UtiyamaandIsa-hara,2001).Weusethepubliclyavailableimple-mentationofthesystem.Asadditionalpointsofcomparison,wetesttheuniformandrandombaselines.Thesecorrespondtosegmentationsobtainedbyuniformlyplacing510

PkWindowDiffMAN0.2980.311SD0.3400.351AUDIO0.3580.370SI0.3780.390RAND0.4720.497UNI0.4760.484Table2:Segmentationaccuracyforaudio-basedsegmentor(AUDIO),random(RAND),uniform(UNI)andthreetranscript-basedsegmentationalgo-rithmsthatusemanual(MAN),speaker-dependent(SD)andspeaker-independent(SI)transcripts.Forallofthealgorithms,thetargetnumberofsegmentsissettothereferencenumberofsegments.boundariesalongthespanofthelectureandselect-ingrandomboundaries,respectively.Tocontrolforsegmentationgranularity,wespec-ifythenumberofsegmentsinthereferencesegmen-tationforbothoursystemandthebaselines.ParameterTuningWetunedthenumberofquan-tizedblocks,theedgecutoffparameterofthemin-imumcutalgorithm,andtheanisotropicdiffusionparametersonaheldoutsetofthreedevelopmentlectures.Weusedthesamedevelopmentsetforthebaselinesegmentationsystems.5ResultsThegoalofourevaluationexperimentsistwo-fold.First,weareinterestedinunderstandingthecondi-tionsinwhichanaudio-basedsegmentationisad-vantageousoveratranscript-basedone.Second,weaimtoanalyzetheimpactofvariousdesigndeci-sionsontheperformanceofouralgorithm.ComparisonwithTranscript-BasedSegmenta-tionTable2showsthesegmentationaccuracyoftheaudio-basedsegmentationalgorithmandthreetranscript-basedsegmentorsonthesetof30Physicslectures.OuralgorithmyieldsanaveragePkmea-sureof0.358andanaverageWindowDiffmea-sureof0.370.Thisresultismarkedlybetterthanthescoresattainedbyuniformandrandomseg-mentations.Asexpected,thebestsegmentationre-sultsareobtainedusingmanualtranscripts.How-ever,thegapbetweenaudio-basedsegmentationandtranscript-basedsegmentationnarrowswhentherecognitionaccuracydecreases.Infact,perfor-manceoftheaudio-basedsegmentationbeatsthetranscript-basedsegmentationbaselineobtainedus-ingspeaker-independent(SI)models(0.358forAU-DIOversusPkmeasurementsof0.378forSI).AnalysisofAudio-basedSegmentationAcen-tralchallengeinaudio-basedsegmentationishowtoovercomethenoiseinherentinacousticmatching.Weaddressedthisissuebyusinganisotropicdiffu-siontoreﬁnethecomparisonmatrix.Wecanquan-tifytheeffectsofthissmoothingtechniquebygener-atingsegmentationsdirectlyfromthesimilarityma-trix.Weobtainsimilaritiesfromthedistortionsinthecomparisonmatrixbysubtractingthedistortionscoresfromthemaximumdistortion:S(x,y)=maxsi,sj[D(si,sj)](cid:0)D(x,y)Usingthismatrixwiththemin-cutalgorithm,seg-mentationaccuracydropstoaPkmeasureof0.418(0.450WindowDiff).Thisdifferenceinperfor-manceshowsthatanisotropicdiffusioncompensatesfornoiseintroducedduringacousticmatching.Analternativesolutiontotheproblemofirregu-laritiesinaudio-basedmatchingistocomputeclus-tersofacousticallysimilarutterances.Eachofthederivedclusterscanbethoughtofasauniquewordtype.4Wecomputetheseclusters,employingamethodforunsupervisedvocabularyinductionde-velopedbyParkandGlass(2006).Usingtheout-putoftheiralgorithm,thecontinuousaudiostreamistransformedintoasequenceofword-likeunits,whichinturncanbesegmentedusinganystan-dardtranscript-basedsegmentationalgorithm,suchastheminimum-cutsegmentor.Onourcorpus,thismethodachievesdisappointingresults—aPkmea-sureof0.423(0.424WindowDiff).Theresultcanbeattributedtothesparsityofclusters5generatedbythismethod,whichfocusesprimarilyondiscoveringthefrequentlyoccurringcontentwords.6ConclusionandFutureWorkWepresentedanunsupervisedalgorithmforaudio-basedtopicsegmentation.Incontrasttoexisting4Inpractice,aclustercancorrespondtoaphrase,word,orwordfragment(SeeTable1forexamples).5Wetunedthenumberofclustersonthedevelopmentset.511

algorithmsforspeechsegmentation,ourapproachdoesnotrequireaninputtranscript.Thus,itcanbeusedindomainswhereaspeechrecognizerisnotavailableoritsoutputistoonoisy.Ourap-proachapproximatesthedistributionofcohesiontiesbyconsideringthedistributionofacousticpat-terns.Ourexperimentalresultsdemonstratetheutil-ityofthisapproach:audio-basedsegmentationcom-paresfavorablywithtranscript-basedsegmentationcomputedovernoisytranscripts.Thesegmentationalgorithmpresentedinthispa-perfocusesononesourceoflinguisticinformationfordiscourseanalysis—lexicalcohesion.Multiplestudiesofdiscoursestructure,however,haveshownthatprosodiccuesarehighlypredictiveofchangesintopicstructure(HirschbergandNakatani,1996;Shribergetal.,2000).Inasupervisedframework,wecanfurtherenhanceaudio-basedsegmentationbycombiningfeaturesderivedfrompatternanaly-siswithprosodicinformation.Wecanalsoexploreanunsupervisedfusionofthesetwosourcesofin-formation;forinstance,wecaninduceinformativeprosodiccuesbyusingdistributionalevidence.Anotherinterestingdirectionforfutureresearchliesincombiningtheresultsofnoisyrecogni-tionwithinformationobtainedfromdistributionofacousticpatterns.Wehypothesizethatthesetwosourcesprovidecomplementaryinformationabouttheaudiostream,andthereforecancompensateforeachother’smistakes.Thiscombinationcanbepar-ticularlyfruitfulwhenprocessingspeechdocumentswithmultiplespeakersorbackgroundnoise.7AcknowledgementsTheauthorsacknowledgethesupportoftheMicrosoftFacultyFellowshipandtheNationalScienceFoundation(CAREERgrantIIS-0448168,grantIIS-0415865,andtheNSFGraduateFellowship).Anyopinions,ﬁndings,conclusionsorrecom-mendationsexpressedinthispublicationarethoseoftheau-thor(s)anddonotnecessarilyreﬂecttheviewsoftheNationalScienceFoundation.WewouldliketothankT.J.HazenforhisassistancewiththespeechrecognizerandtoacknowledgeTaraSainath,NatashaSingh,BenSnyder,ChaoWang,LukeZettlemoyerandthethreeanonymousreviewersfortheirvalu-ablecommentsandsuggestions.ReferencesD.Beeferman,A.Berger,J.D.Lafferty.1999.Statisticalmod-elsfortextsegmentation.MachineLearning,34(1-3):177–210.C.Bishop,1995.NeuralNetworksforPatternRecognition,pg.38.OxfordUniversityPress,NewYork,1995.M.R.Brent.1999.Anefﬁcient,probabilisticallysoundalgo-rithmforsegmentationandworddiscovery.MachineLearn-ing,34(1-3):71–105.F.Choi,P.Wiemer-Hastings,J.Moore.2001.Latentsemanticanalysisfortextsegmentation.InProceedingsofEMNLP,109–117.C.G.deMarcken.1996.UnsupervisedLanguageAcquisition.Ph.D.thesis,MassachusettsInstituteofTechnology.A.Dielmann,S.Renals.2005.MultistreamdynamicBayesiannetworkformeetingsegmentation.InProceedingsMul-timodalInteractionandRelatedMachineLearningAlgo-rithmsWorkshop(MLMI–04),76–86.M.Galley,K.McKeown,E.Fosler-Lussier,H.Jing.2003.Discoursesegmentationofmulti-partyconversation.InPro-ceedingsoftheACL,562–569.J.Garofolo,L.Lamel,W.Fisher,J.Fiscus,D.Pallet,N.Dahlgren,V.Zue.1993.TIMITAcoustic-PhoneticCon-tinuousSpeechCorpus.LinguisticDataConsortium,1993.M.Hearst.1994.Multi-paragraphsegmentationofexpositorytext.InProceedingsoftheACL,9–16.J.Hirschberg,C.H.Nakatani.1996.Aprosodicanalysisofdiscoursesegmentsindirection-givingmonologues.InPro-ceedingsoftheACL,286–293.X.Huang,A.Acero,H.-W.Hon.2001.SpokenLanguagePro-cessing.PrenticeHall.X.Ji,H.Zha.2003.Domain-independenttextsegmentationusinganisotropicdiffusionanddynamicprogramming.InProceedingsofSIGIR,322–329.Y.-L.Lin,T.Jiang,K.-M.Chao.2002.Efﬁcientalgorithmsforlocatingthelength-constrainedheaviestsegmentswithapplicationstobiomolecularsequenceanalysis.J.ComputerandSystemSciences,65(3):570–586.I.Malioutov,R.Barzilay.2006.Minimumcutmodelforspokenlecturesegmentation.InProceedingsoftheCOL-ING/ACL,25–32.A.Park,J.R.Glass.2006.Unsupervisedwordacquisitionfromspeechusingpatterndiscovery.InProceedingsofICASSP.P.Perona,J.Malik.1990.Scale-spaceandedgedetectionusinganisotropicdiffusion.IEEETransactionsonPatternAnaly-sisandMachineIntelligence,12(7):629–639.L.Pevzner,M.Hearst.2002.Acritiqueandimprovementofanevaluationmetricfortextsegmentation.ComputationalLinguistics,28(1):19–36.J.Shi,J.Malik.2000.Normalizedcutsandimagesegmenta-tion.IEEETransactionsonPatternAnalysisandMachineIntelligence,22(8):888–905.E.Shriberg,A.Stolcke,D.Hakkani-Tur,G.Tur.2000.Prosody-basedautomaticsegmentationofspeechintosen-tencesandtopics.SpeechCommunication,32(1-2):127–154.M.Utiyama,H.Isahara.2001.Astatisticalmodelfordomain-independenttextsegmentation.InProceedingsoftheACL,499–506.A.Venkataraman.2001.Astatisticalmodelforworddis-coveryintranscribedspeech.ComputationalLinguistics,27(3):353–372.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 512–519,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

512

RandomisedLanguageModellingforStatisticalMachineTranslationDavidTalbotandMilesOsborneSchoolofInformatics,UniversityofEdinburgh2BuccleuchPlace,Edinburgh,EH89LW,UKd.r.talbot@sms.ed.ac.uk,miles@inf.ed.ac.ukAbstractABloomﬁlter(BF)isarandomiseddatastructureforsetmembershipqueries.Itsspacerequirementsaresigniﬁcantlybelowlosslessinformation-theoreticlowerboundsbutitproducesfalsepositiveswithsomequantiﬁableprobability.HereweexploretheuseofBFsforlanguagemodellinginstatis-ticalmachinetranslation.WeshowhowaBFcontainingn-gramscanenableustousemuchlargercorporaandhigher-ordermodelscomplementingacon-ventionaln-gramLMwithinanSMTsys-tem.Wealsoconsider(i)howtoincludeap-proximatefrequencyinformationefﬁcientlywithinaBFand(ii)howtoreducetheer-rorrateofthesemodelsbyﬁrstcheckingforlower-ordersub-sequencesincandidaten-grams.Oursolutionsinbothcasesretaintheone-sidederrorguaranteesoftheBFwhiletakingadvantageoftheZipf-likedistributionofwordfrequenciestoreducethespacere-quirements.1IntroductionLanguagemodelling(LM)isacrucialcomponentinstatisticalmachinetranslation(SMT).Standardn-gramlanguagemodelsassignprobabilitiestotrans-lationhypothesesinthetargetlanguage,typicallyassmoothedtrigrammodels,e.g.(Chiang,2005).Al-thoughitiswell-knownthathigher-orderLMsandmodelstrainedonadditionalmonolingualcorporacanyieldbettertranslationperformance,thechal-lengesindeployinglargeLMsarenottrivial.In-creasingtheorderofann-grammodelcanresultinanexponentialincreaseinthenumberofparameters;forcorporasuchastheEnglishGigawordcorpus,forinstance,thereare300milliondistincttrigramsandover1.2billion5-grams.SinceaLMmaybequeriedmillionsoftimespersentence,itshouldideallyre-sidelocallyinmemorytoavoidtime-consumingre-moteordisk-basedlook-ups.Againstthisbackground,weconsideraradicallydifferentapproachtolanguagemodelling:insteadofexplicitlystoringalldistinctn-grams,westorearandomisedrepresentation.Inparticular,weshowthattheBloomﬁlter(Bloom(1970);BF),asim-plespace-efﬁcientrandomiseddatastructureforrep-resentingsets,maybeusedtorepresentstatisticsfromlargercorporaandforhigher-ordern-gramstocomplementaconventionalsmoothedtrigrammodelwithinanSMTdecoder.1ThespacerequirementsofaBloomﬁlterarequitespectacular,fallingsigniﬁcantlybelowinformation-theoreticerror-freelowerboundswhilequerytimesareconstant.Thisefﬁciency,however,comesatthepriceoffalsepositives:theﬁltermayerroneouslyreportthatanitemnotinthesetisamember.Falsenegatives,ontheotherhand,willneveroccur:theerrorissaidtobeone-sided.Inthispaper,weshowthataBloomﬁltercanbeusedeffectivelyforlanguagemodellingwithinanSMTdecoderandpresentthelog-frequencyBloomﬁlter,anextensionofthestandardBooleanBFthat1Forextensionsoftheframeworkpresentedheretostand-alonesmoothedBloomﬁlterlanguagemodels,wereferthereadertoacompanionpaper(TalbotandOsborne,2007).513

takesadvantageoftheZipf-likedistributionofcor-pusstatisticstoallowfrequencyinformationtobeassociatedwithn-gramsintheﬁlterinaspace-efﬁcientmanner.Wethenproposeamechanism,sub-sequenceﬁltering,forreducingtheerrorratesofthesemodelsbyusingthefactthatann-gram’sfrequencyisboundfromabovebythefrequencyofitsleastfrequentsub-sequence.Wepresentmachinetranslationexperimentsus-ingthesemodelstorepresentinformationregardinghigher-ordern-gramsandadditionallargermono-lingualcorporaincombinationwithconventionalsmoothedtrigrammodels.Wealsorunexperimentswiththesemodelsinisolationtohighlighttheim-pactofdifferentordern-gramsonthetranslationprocess.FinallyweprovidesomeempiricalanalysisoftheeffectivenessofboththelogfrequencyBloomﬁlterandsub-sequenceﬁltering.2TheBloomﬁlterInthissection,wegiveabriefoverviewoftheBloomﬁlter(BF);refertoBroderandMitzenmacher(2005)foramoreindetailedpresentation.ABFrep-resentsasetS={x1,x2,...,xn}withnelementsdrawnfromauniverseUofsizeN.ThestructureisattractivewhenN(cid:29)n.Theonlysigniﬁcantstor-ageusedbyaBFconsistsofabitarrayofsizem.Thisisinitiallysettoholdzeroes.Totraintheﬁlterwehasheachiteminthesetktimesusingdistincthashfunctionsh1,h2,...,hk.Eachfunctionisas-sumedtobeindependentfromeachotherandtomapitemsintheuniversetotherange1tomuniformlyatrandom.Thekbitsindexedbythehashvaluesforeachitemaresetto1;theitemisthendiscarded.Onceabithasbeensetto1itremainssetforthelife-timeoftheﬁlter.Distinctitemsmaynotbehashedtokdistinctlocationsintheﬁlter;weignorecol-lisons.Bitsintheﬁltercan,therefore,besharedbydistinctitemsallowingsigniﬁcantspacesavingsbutintroducinganon-zeroprobabilityoffalsepositivesattesttime.ThereisnowayofdirectlyretrievingorennumeratingtheitemsstoredinaBF.Attesttimewewishtodiscoverwhetheragivenitemwasamemberoftheoriginalset.Theﬁlterisqueriedbyhashingthetestitemusingthesamekhashfunctions.Ifallbitsreferencedbythekhashvaluesare1thenweassumethattheitemwasamember;ifanyofthemare0thenweknowitwasnot.Truemembersarealwayscorrectlyidentiﬁed,butafalsepositivewilloccurifallkcorrespondingbitsweresetbyotheritemsduringtrainingandtheitemwasnotamemberofthetrainingset.Thisisknownasaone-sidederror.Theprobabilityofafalsepostive,f,isclearlytheprobabilitythatnoneofkrandomlyselectedbitsintheﬁlterarestill0aftertraining.Lettingpbetheproportionofbitsthatarestillzeroafterthesenele-mentshavebeeninserted,thisgives,f=(1−p)k.Asnitemshavebeenenteredintheﬁlterbyhashingeachktimes,theprobabilitythatabitisstillzerois,p0=(cid:18)1−1m(cid:19)kn≈e−knmwhichistheexpectedvalueofp.Hencethefalsepositiveratecanbeapproximatedas,f=(1−p)k≈(1−p0)k≈(cid:16)1−e−knm(cid:17)k.Bytakingthederivativeweﬁndthatthenumberoffunctionsk∗thatminimizesfis,k∗=ln2·mn.whichleadstotheintuitiveresultthatexactlyhalfthebitsintheﬁlterwillbesetto1whentheoptimalnumberofhashfunctionsischosen.ThefundmentaldifferencebetweenaBloomﬁl-ter’sspacerequirementsandthatofanylosslessrep-resentationofasetisthattheformerdoesnotdependonthesizeofthe(exponential)universeNfromwhichthesetisdrawn.Alosslessrepresentationscheme(forexample,ahashmap,trieetc.)mustde-pendonNsinceitassignsadistinctrepresentationtoeachpossiblesetdrawnfromtheuniverse.3LanguagemodellingwithBloomﬁltersInourexperimentswemakeuseofbothstandard(i.e.Boolean)BFscontainingn-gramtypesdrawnfromatrainingcorpusandanovelBFscheme,thelog-frequencyBloomﬁlter,thatallowsfrequencyinformationtobeassociatedefﬁcientlywithitemsstoredintheﬁlter.514

Algorithm1TrainingfrequencyBFInput:Strain,{h1,...hk}andBF=∅Output:BFforallx∈Straindoc(x)←frequencyofn-gramxinStrainqc(x)←quantisationofc(x)(Eq.1)forj=1toqc(x)dofori=1tokdohi(x)←hashofevent{x,j}underhiBF[hi(x)]←1endforendforendforreturnBF3.1Log-frequencyBloomﬁlterTheefﬁciencyofourschemeforstoringn-gramstatisticswithinaBFreliesontheZipf-likedistribu-tionofn-gramfrequenciesinnaturallanguagecor-pora:mosteventsoccuranextremelysmallnumberoftimes,whileasmallnumberareveryfrequent.Wequantiserawfrequencies,c(x),usingaloga-rithmiccodebookasfollows,qc(x)=1+blogbc(x)c.(1)Theprecisionofthiscodebookdecaysexponentiallywiththerawcountsandthescaleisdeterminedbythebaseofthelogarithmb;weexaminetheeffectofthisparameterinexperimentsbelow.Giventhequantisedcountqc(x)forann-gramx,theﬁlteristrainedbyenteringcompositeeventscon-sistingofthen-gramappendedbyanintegercounterjthatisincrementedfrom1toqc(x)intotheﬁlter.Toretrievethequantisedcountforann-gram,itisﬁrstappendedwithacountof1andhashedunderthekfunctions;ifthistestspositive,thecountisin-crementedandtheprocessrepeated.Theprocedureterminatesassoonasanyofthekhashfunctionshitsa0andthepreviouscountisreported.Theone-sidederroroftheBFandthetrainingschemeensurethattheactualquantisedcountcannotbelargerthanthisvalue.Asthecountsarequantisedlogarithmically,thecounterwillbeincrementedonlyasmallnumberoftimes.ThetrainingandtestingroutinesaregivenhereasAlgorithms1and2respectively.Errorsforthelog-frequencyBFschemeareone-sided:frequencieswillneverbeunderestimated.Algorithm2TestfrequencyBFInput:x,MAXQCOUNT,{h1,...hk}andBFOutput:Upperboundonqc(x)∈Strainforj=1toMAXQCOUNTdofori=1tokdohi(x)←hashofevent{x,j}underhiifBF[hi(x)]=0thenreturnj−1endifendforendforTheprobabilityofoverestimatinganitem’sfre-quencydecaysexponentiallywiththesizeoftheoverestimationerrord(i.e.asfdford>0)sinceeacherroneousincrementcorrespondstoasinglefalsepositiveanddsuchindependenteventsmustoccurtogether.3.2Sub-sequenceﬁlteringTheerroranalysisinSection2focusedonthefalsepositiverateofaBF;ifwedeployaBFwithinanSMTdecoder,however,theactualerrorratewillalsodependontheapriorimembershipprobabilityofitemspresentedtoit.TheerrorrateErris,Err=Pr(x/∈Strain|Decoder)f.Thisimpliesthat,unlikeaconventionallosslessdatastructure,themodel’saccuracydependsonothercomponentsinsystemandhowitisqueried.Wetakeadvantageofthemonotonicityofthen-grameventspacetoplaceupperboundsonthefre-quencyofann-grampriortotestingforitintheﬁlterandpotentiallytruncatetheouterloopinAlgorithm2whenweknowthatthetestcouldonlyreturnpos-tiveinerror.Speciﬁcally,ifwehavestoredlower-ordern-gramsintheﬁlter,wecaninferthatann-gramcan-notpresent,ifanyofitssub-sequencestestnega-tive.Sinceourschemeforstoringfrequenciescanneverunderestimateanitem’sfrequency,thisrela-tionwillgeneralisetofrequencies:ann-gram’sfre-quencycannotbegreaterthanthefrequencyofitsleastfrequentsub-sequenceasreportedbytheﬁlter,c(w1,...,wn)≤min{c(w1,...,wn−1),c(w2,...,wn)}.WeusethistoreducetheeffectiveerrorrateofBF-LMsthatweuseintheexperimentsbelow.515

3.3BloomﬁlterlanguagemodeltestsAstandardBFcanimplementaBoolean‘languagemodel’test:haveweseensomefragmentoflan-guagebefore?Thisdoesnotuseanyfrequencyin-formation.TheBooleanBF-LMisastandardBFcontainingalln-gramsofacertainlengthinthetrainingcorpus,Strain.Itimplementsthefollowingbinaryfeaturefunctioninalog-lineardecoder,φbool(x)≥δ(x∈Strain)SeparateBooleanBF-LMscanbeincludedfordifferentordernandassigneddistinctlog-linearweightsthatarelearnedaspartofaminimumerrorratetrainingprocedure(seeSection4).Thelog-frequencyBF-LMimplementsamultino-mialfeaturefunctioninthedecoderthatreturnsthevalueassociatedwithann-grambyAlgorithm2.φlogfreq(x)≥qc(x)∈StrainSub-sequenceﬁlteringcanbeperformedbyusingtheminimumvaluereturnedbylower-ordermodelsasanupper-boundonthehigher-ordermodels.Byboostingthescoreofhypothesescontainingn-gramsobservedinthetrainingcorpuswhileremain-ingagnosticforunseenn-grams(withtheexceptionoferrors),thesefeaturefunctionshavemoreincom-monwithmaximumentropymodelsthanconven-tionallysmoothedn-grammodels.4ExperimentsWeconductedarangeofexperimentstoexploretheeffectivenessandtheerror-spacetrade-offofBloomﬁltersforlanguagemodellinginSMT.Thespace-efﬁciencyofthesemodelsalsoallowsustoinves-tigatetheimpactofusingmuchlargercorporaandhigher-ordern-gramsontranslationquality.WhileourmainexperimentsusetheBloomﬁltermodelsinconjunctionwithaconventionalsmoothedtrigrammodel,wealsopresentexperimentswiththesemod-elsinisolationtohighlighttheimpactofdifferentordern-gramsonthetranslationprocess.Finally,wepresentsomeempiricalanalysisofboththelog-frequencyBloomﬁlterandthesub-sequenceﬁlter-ingtechniquewhichmaybeofindependentinterest.ModelEP-KN-3EP-KN-4AFP-KN-3Memory64M99M1.3Ggzipsize21M31M481M1-gms62K62K871K2-gms1.3M1.3M16M3-gms1.1M1.0M31M4-gmsN/A1.1MN/ATable1:BaselineandComparisonModels4.1Experimentalset-upAllofourexperimentsusepublicallyavailablere-sources.WeusetheFrench-EnglishsectionoftheEuroparl(EP)corpusforparalleldataandlanguagemodelling(Koehn,2003)andtheEnglishGiga-wordCorpus(LDC2003T05;GW)foradditionallanguagemodelling.Decodingiscarried-outusingtheMosesdecoder(KoehnandHoang,2007).Weholdout500testsen-tencesand250developmentsentencesfromthepar-alleltextforevaluationpurposes.Thefeaturefunc-tionsinourmodelsareoptimisedusingminimumerrorratetrainingandevaluationisperformedusingtheBLEUscore.4.2BaselineandcomparisonmodelsOurbaselineLMandothercomparisonmodelsareconventionaln-grammodelssmoothedusingmodi-ﬁedKneser-NeyandbuiltusingtheSRILMToolkit(Stolcke,2002);asisstandardpracticethesemodelsdropentriesforn-gramsofsize3andabovewhenthecorrespondingdiscountedcountislessthan1.Thebaselinelanguagemodel,EP-KN-3,isatrigrammodeltrainedontheEnglishportionoftheparallelcorpus.Foradditionalcomparisonswealsotrainedasmoothed4-grammodelonthisEuroparldata(EP-KN-4)andatrigrammodelontheAgenceFrancePresssectionoftheGigawordCorpus(AFP-KN-3).Table1showstheamountofmemorythesemod-elstakeupondiskandcompressedusingthegziputilityinparenthesesaswellasthenumberofdis-tinctn-gramsofeachorder.Wegivethegzipcom-pressedsizeasanoptimisticlowerboundonthesizeofanylosslessrepresentationofeachmodel.22Note,inparticular,thatgzipcompressedﬁlesdonotsup-portdirectrandomaccessasrequiredbyourapplication.516

CorpusEuroparlGigaword1-gms61K281K2-gms1.3M5.4M3-gms4.7M275M4-gms9.0M599M5-gms10.3M842M6-gms10.7M957MTable2:Numberofdistinctn-grams4.3Bloomﬁlter-basedmodelsTocreateBloomﬁlterLMswegatheredn-gramcountsfromboththeEuroparl(EP)andthewholeoftheGigawordCorpus(GW).Table2showsthenumbersofdistinctn-gramsinthesecorpora.Notethatweusenopruningforthesemodelsandthatthenumbersofdistinctn-gramsisofthesameor-derasthatoftherecentlyreleasedGoogleNgramsdataset(LDC2006T13).Inourexperimentswecre-atearangeofmodelsreferredtobythecorpusused(EPorGW),theorderofthen-gram(s)enteredintotheﬁlter(1to10),whetherthemodelisBoolean(Bool-BF)orprovidesfrequencyinformation(Freq-BF),whetherornotsub-sequenceﬁlteringwasused(FTR)andwhetheritwasusedinconjunctionwiththebaselinetrigram(+EP-KN-3).4.4MachinetranslationexperimentsOurﬁrstsetofexperimentsexaminestherelation-shipbetweenmemoryallocatedtotheBFandBLEUscore.WepresentresultsusingtheBooleanBF-LMinisolationandthenboththeBooleanandlog-frequencyBF-LMStoadd4-gramstoourbaseline3-grammodel.Oursecondsetofexperimentsadds3-gramsand5-gramsfromtheGigawordCorpustoourbaseline.HereweconstrasttheBooleanBF-LMwiththelog-frequencyBF-LMwithdifferentquantisationbases(2=ﬁne-grainedand5=coarse-grained).Wethenevaluatethesub-sequenceﬁl-teringapproachtoreducingtheactualerrorrateofthesemodelsbyaddingboth3and4-gramsfromtheGigawordCorpustothebaseline.SincetheBF-LMseasilyallowustodeployveryhigh-ordern-grammodels,weusethemtoevaluatetheimpactofdif-ferentordern-gramsonthetranslationprocesspre-sentingresultsusingtheBooleanandlog-frequencyBF-LMinisolationforn-gramsoforder1to10.ModelEP-KN-3EP-KN-4AFP-KN-3BLEU28.5129.2429.17Memory64M99M1.3Ggzipsize21M31M481MTable3:BaselineandComparisonModels4.5AnalysisofBFextensionsWeanalyseourlog-frequencyBFschemeintermsoftheadditionalmemoryitrequiresandtheerrorratecomparedtoanon-redundantscheme.Thenon-redundantschemeinvolvesenteringjusttheexactquantisedcountforeachn-gramandthensearchingovertherangeofpossiblecountsattesttimestartingwiththecountwithmaximumaprioriprobability(i.e.1)andincrementinguntilacountisfoundorthewholecodebookhasbeensearched(herethesizeis16).Wealsoanalysethesub-sequenceﬁlteringschemedirectlybycreatingaBFwithonly3-gramsandaBFcontainingboth2-gramsand3-gramsandcomparingtheiractualerrorrateswhenpresentedwith3-gramsthatareallknowntobenegatives.5Results5.1MachinetranslationexperimentsTable3showstheresultsofthebaseline(EP-KN-3)andotherconventionaln-grammodelstrainedonlargercorpora(AFP-KN-3)andusinghigher-orderdependencies(EP-KN-4).Thelargermodelsim-provesomewhatonthebaselineperformance.Figure1showstherelationshipbetweenspaceal-locatedtotheBFmodelsandBLEUscore(left)andfalsepositiverate(right)respectively.Theseexperi-mentsdonotincludethebaselinemodel.Wecanseeaclearcorrelationbetweenmemory/falsepositiverateandtranslationperformance.Adding4-gramsintheformofaBooleanBForalog-frequencyBF(seeFigure2)improvesonthe3-grambaselinewithlittleadditionalmemory(around4MBs)whileperformingonaparwithorabovetheEuroparl4-grammodelwitharound10MBs;thissuggeststhatalossyrepresentationoftheun-prunedsetof4-gramscontainsmoreusefulinforma-tionthanalosslessrepresentationoftheprunedset.33AnunprunedmodiﬁedKneser-Ney4-grammodelontheEurpoparldatascoresslightlyhigher-29.69-whiletakingup489MB(132MBgzipped).517

 29 28 27 26 25 10 8 6 4 2 1 0.8 0.6 0.4 0.2 0BLEU ScoreFalse positive rateMemory in MBEuroparl Boolean BF 4-gram (alone)BLEU Score Bool-BF-EP-4False positive rateFigure1:Space/Errorvs.BLEUScore. 30.5 30 29.5 29 28.5 28 9 7 5 3 1BLEU ScoreMemory in MBEP-Bool-BF-4 and Freq-BF-4 (with EP-KN-3)EP-Bool-BF-4 + EP-KN-3EP-Freq-BF-4 + EP-KN-3EP-KN-4 comparison (99M / 31M gzip)EP-KN-3 baseline (64M / 21M gzip)Figure2:Adding4-gramswithBloomﬁlters.Asthefalsepositiverateexceeds0.20theperfor-manceisseverlydegraded.Adding3-gramsdrawnfromthewholeoftheGigawordcorpusratherthansimplytheAgenceFrancePresssectionresultsinslightlyimprovedperformancewithsignﬁcantlylessmemorythantheAFP-KN-3model(seeFigure3).Figure4showstheresultsofadding5-gramsdrawnfromtheGigawordcorpustothebaseline.ItalsocontraststheBooleanBFandthelog-frequencyBFsuggestinginthiscasethatthelog-frequencyBFcanprovideusefulinformationwhenthequantisa-tionbaseisrelativelyﬁne-grained(base2).TheBooleanBFandthebase5(coarse-grainedquan-tisation)log-frequencyBFperformapproximatelythesame.Thebase2quantisationperformsworse 30.5 30 29.5 29 28.5 28 27.5 27 1 0.8 0.6 0.4 0.2 0.1BLEU ScoreMemory in GBGW-Bool-BF-3 and GW-Freq-BF-3 (with EP-KN-3)GW-Bool-BF-3 + EP-KN-3GW-Freq-BF-3 + EP-KN-3AFP-KN-3 + EP-KN-3Figure3:AddingGW3-gramswithBloomﬁlters. 30.5 30 29.5 29 28.5 28 1 0.8 0.6 0.4 0.2 0.1BLEU ScoreMemory in GBGW-Bool-BF-5 and GW-Freq-BF-5 (base 2 and 5) (with EP-KN-3)GW-Bool-BF-5 + EP-KN-3GW-Freq-BF-5 (base 2) + EP-KN-3GW-Freq-BF-5 (base 5) + EP-KN-3AFP-KN-3 + EP-KN-3Figure4:Comparisonofdifferentquantisationrates.forsmalleramountsofmemory,possiblyduetothelargersetofeventsitisrequiredtostore.Figure5showssub-sequenceﬁlteringresultinginasmallincreaseinperformancewhenfalsepositiveratesarehigh(i.e.lessmemoryisallocated).Webelievethistobetheresultofanincreasedapri-orimembershipprobabilityforn-gramspresentedtotheﬁlterunderthesub-sequenceﬁlteringscheme.Figure6showsthatforthistaskthemostusefuln-gramsizesarebetween3and6.5.2AnalysisofBFextensionsFigure8comparesthememoryrequirementsofthelog-frequenceyBF(base2)andtheBoolean518

 31 30 29 28 1 0.8 0.6 0.4 0.2BLEU ScoreMemory in MBGW-Bool-BF-3-4-FTR and GW-Bool-BF-3-4 (with EP-KN-3)GW-Bool-BF-3-4-FTR + EP-KN-3GW-Bool-BF-3-4 + EP-KN-3Figure5:Effectofsub-sequenceﬁltering. 27 26 25 24 10 9 8 7 6 5 4 3 2 1BLEU ScoreN-gram orderEP-Bool-BF and EP-Freq-BF with different order N-grams (alone)EP-Bool-BFEP-Freq-BFFigure6:Impactofn-gramsofdifferentsizes.BFforvariousordern-gramsetsfromtheGiga-wordCorpuswiththesameunderlyingfalseposi-tiverate(0.125).Theadditionalspacerequiredbyourschemeforstoringfrequencyinformationislessthanafactorof2comparedtothestandardBF.Figure7showsthenumberandsizeoffrequencyestimationerrorsmadebyourlog-frequencyBFschemeandanon-redundantschemethatstoresonlytheexactquantisedcount.Wepresented500Knega-tivestotheﬁlterandrecordedthefrequencyofover-estimationerrorsofeachsize.AsshowninSection3.1,theprobabilityofoverestimatinganitem’sfre-quencyunderthelog-frequencyBFschemedecaysexponentiallyinthesizeofthisoverestimationer-ror.Althoughthenon-redundantschemerequires 0 10 20 30 40 50 60 70 80 16 15 14 13 12 11 10 9 8 7 6 5 4 3 2 1Frequency (K)Size of overestimation errorFrequency Estimation Errors on 500K NegativesLog-frequency BF (Bloom error = 0.159)Non-redundant scheme (Bloom error = 0.076)Figure7:Frequencyestimationerrors. 0 100 200 300 400 500 600 700 1 2 3 4 5 6 7Memory (MB)N-gram order (Gigaword)Memory requirements for 0.125 false positive rateBool-BFFreq-BF (log base-2 quantisation)Figure8:Comparisonofmemoryrequirements.feweritemsbestoredintheﬁlterand,therefore,hasalowerunderlyingfalsepositiverate(0.076versus0.159),inpracticeitincursamuchhighererrorrate(0.717)withmanylargeerrors.Figure9showstheimpactofsub-sequenceﬁlter-ingontheactualerrorrate.Although,thefalsepos-itiveratefortheBFcontaining2-grams,inaddition,to3-grams(ﬁltered)ishigherthanthefalsepositiverateoftheunﬁlteredBFcontainingonly3-grams,theactualerrorrateoftheformerislowerformod-elswithlessmemory.Bytestingfor2-gramspriortoqueryingforthe3-grams,wecanavoidperform-ingsomequeriesthatmayotherwisehaveincurrederrorsusingthefactthata3-gramcannotbepresentifoneofitsconstituent2-gramsisabsent.519

 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.5 1 1.5 2 2.5 3 3.5Error rateMemory (MB)Error rate with sub-sequence filteringFiltered false positive rateUnfiltered false pos rate / actual error rateFiltered actual error rateFigure9:Errorratewithsub-sequenceﬁltering.6RelatedWorkWearenottheﬁrstpeopletoconsiderbuildingverylargescaleLMs:Kumaretal.usedafour-gramLMforre-ranking(Kumaretal.,2005)andinun-publishedwork,Googleusedsubstantiallylargern-gramsintheirSMTsystem.DeployingsuchLMsrequireseitheraclusterofmachines(andtheover-headsofremoteprocedurecalls),per-sentenceﬁl-tering(whichagain,isslow)and/ortheuseofsomeotherlossycompression(GoodmanandGao,2000).Ourapproachcancomplementallthesetechniques.Bloomﬁltershavebeenwidelyusedindatabaseapplicationsforreducingcommunicationsover-headsandwererecentlyappliedtoencodewordfrequenciesininformationretrieval(LinariandWeikum,2006)usingamethodthatresemblesthenon-redundantschemedescribedabove.Exten-sionsoftheBFtoassociatefrequencieswithitemsinthesethavebeenproposede.g.,(CormodeandMuthukrishn,2005);whiletheseschemesaremoregeneralthanours,theyincurgreaterspaceoverheadsforthedistributionsthatweconsiderhere.7ConclusionsWehaveshownthatBloomFilterscanformtheba-sisforspace-efﬁcientlanguagemodellinginSMT.ExtendingthestandardBFstructuretoencodecor-pusfrequencyinformationanddevelopingastrat-egyforreducingtheerrorratesofthesemodelsbysub-sequenceﬁltering,ourmodelsenablehigher-ordern-gramsandlargermonolingualcorporatobeusedmoreeasilyforlanguagemodellinginSMT.Inacompanionpaper(TalbotandOsborne,2007)wehaveproposedaframeworkforderivingcon-ventionalsmoothedn-grammodelsfromthelog-frequencyBFschemeallowingustodoawayen-tirelywiththestandardn-grammodelinanSMTsystem.Wehopethepresentworkwillhelpestab-lishtheBloomﬁlterasapracticalalternativetocon-ventionalassociativedatastructuresusedincompu-tationallinguistics.Theframeworkpresentedhereshowsthatwithsomeconsiderationforitsworkings,therandomisednatureoftheBloomﬁlterneednotbeasigniﬁcantimpedimenttoisuseinapplications.ReferencesB.Bloom.1970.Space/timetradeoffsinhashcodingwithallowableerrors.CACM,13:422–426.A.BroderandM.Mitzenmacher.2005.Networkapplicationsofbloomﬁlters:Asurvey.InternetMathematics,1(4):485–509.DavidChiang.2005.Ahierarchicalphrase-basedmodelforstatisticalmachinetranslation.InProceedingsofthe43rdAnnualMeetingoftheAssociationforComputationalLin-guistics(ACL’05),pages263–270,AnnArbor,Michigan.G.CormodeandS.Muthukrishn.2005.Animproveddatastreamsummary:thecount-minsketchanditsapplications.JournalofAlgorithms,55(1):58–75.J.GoodmanandJ.Gao.2000.Languagemodelsizereductionbypruningandclustering.InICSLP’00,Beijing,China.PhilippKoehnandHieuHoang.2007.Factoredtranslationmodels.InProc.ofthe2007ConferenceonEmpiricalMeth-odsinNaturalLanguageProcessing(EMNLP/Co-NLL).P.Koehn.2003.Europarl:Amultilingualcorpusforeval-uationofmachinetranslationphilippkoehn,draft.Availableat:http://people.csail.mit.edu/koehn/publications/europarl.ps.S.Kumar,Y.Deng,andW.Byrne.2005.JohnsHopkinsUni-versity-CambridgeUniversityChinese-EnglishandArabic-English2005NISTMTEvaluationSystems.InProceedingsof2005NISTMTWorkshop,June.AlessandroLinariandGerhardWeikum.2006.Efﬁcientpeer-to-peersemanticoverlaynetworksbasedonstatisticallan-guagemodels.InProceedingsoftheInternationalWorkshoponIRinPeer-to-PeerNetworks,pages9–16,Arlington.AndreasStolcke.2002.SRILM–Anextensiblelanguagemod-elingtoolkit.InProc.oftheIntl.Conf.onSpokenLang.Processing,2002.DavidTalbotandMilesOsborne.2007.SmoothedBloomﬁl-terlanguagemodels:Tera-scaleLMsonthecheap.InPro-ceedingsofthe2007ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP/Co-NLL),June.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 520–527,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

520

Bilingual-LSABasedLMAdaptationforSpokenLanguageTranslationYik-CheungTamandIanLaneandTanjaSchultzInterACT,LanguageTechnologiesInstituteCarnegieMellonUniversityPittsburgh,PA15213{yct,ian.lane,tanja}@cs.cmu.eduAbstractWeproposeanovelapproachtocrosslinguallanguagemodel(LM)adaptationbasedonbilingualLatentSemanticAnalysis(bLSA).AbLSAmodelisintroducedwhichenableslatenttopicdistributionstobeefﬁcientlytransferredacrosslanguagesbyenforcingaone-to-onetopiccorrespondenceduringtraining.UsingtheproposedbLSAframe-workcrosslingualLMadaptationcanbeper-formedby,ﬁrst,inferringthetopicposte-riordistributionofthesourcetextandthenapplyingtheinferreddistributiontothetar-getlanguageN-gramLMviamarginaladap-tation.Theproposedframeworkalsoen-ablesrapidbootstrappingofLSAmodelsfornewlanguagesbasedonasourceLSAmodelfromanotherlanguage.OnChinesetoEnglishspeechandtexttranslationtheproposedbLSAframeworksuccessfullyre-ducedwordperplexityoftheEnglishLMbyover27%foraunigramLMandupto13.6%fora4-gramLM.Furthermore,thepro-posedapproachconsistentlyimprovedma-chinetranslationqualityonbothspeechandtextbasedadaptation.1IntroductionLanguagemodeladaptationiscrucialtonumerousspeechandtranslationtasksasitenableshigher-levelcontextualinformationtobeeffectivelyincor-poratedintoabackgroundLMimprovingrecogni-tionortranslationperformance.OneapproachistoemployLatentSemanticAnalysis(LSA)tocap-turein-domainwordunigramdistributionswhicharethenintegratedintothebackgroundN-gramLM.Thisapproachhasbeensuccessfullyappliedinautomaticspeechrecognition(ASR)(TamandSchultz,2006)usingtheLatentDirichletAlloca-tion(LDA)(Bleietal.,2003).TheLDAmodelcanbeviewedasaBayesiantopicmixturemodelwiththetopicmixtureweightsdrawnfromaDirichletdistribution.ForLMadaptation,thetopicmixtureweightsareestimatedbasedonin-domainadapta-tiontext(e.g.ASRhypotheses).Theadaptedmix-tureweightsarethenusedtointerpolateatopic-dependentunigramLM,whichisﬁnallyintegratedintothebackgroundN-gramLMusingmarginaladaptation(Kneseretal.,1997)Inthispaper,weproposeaframeworktoper-formLMadaptationacrosslanguages,enablingtheadaptationofaLMfromonelanguagebasedontheadaptationtextofanotherlanguage.Instatisticalmachinetranslation(SMT),oneapproachistoap-plyLMadaptationonthetargetlanguagebasedonaninitialtranslationofinputreferences(KimandKhudanpur,2003;Pauliketal.,2005).Thisschemeislimitedbythecoverageofthetranslationmodel,andoverallbythequalityoftranslation.SincethisapproachonlyallowstoapplyLMadaptationaf-tertranslation,availableknowledgecannotbeap-pliedtoextendthecoverage.WeproposeabilingualLSAmodel(bLSA)forcrosslingualLMadaptationthatcanbeappliedbeforetranslation.ThebLSAmodelconsistsoftwoLSAmodels:oneforeachsideofthelanguagetrainedonparalleldocumentcorpora.ThekeypropertyofthebLSAmodelisthat521

thelatenttopicofthesourceandtargetLSAmod-elscanbeassumedtobeaone-to-onecorrespon-denceandthusshareacommonlatenttopicspacesincethetrainingcorporaconsistofbilingualparal-leldata.Forinstance,saytopic10oftheChineseLSAmodelisaboutpolitics.Thentopic10oftheEnglishLSAmodelissettoalsocorrespondtopol-iticsandsoforth.DuringLMadaptation,weﬁrstinferthetopicmixtureweightsfromthesourcetextusingthesourceLSAmodel.ThenwetransfertheinferredmixtureweightstothetargetLSAmodelandthusobtainthetargetLSAmarginals.Thechal-lengeistoenforcetheone-to-onetopiccorrespon-dence.OurproposalistosharecommonvariationalDirichletposteriorsoverthetopicmixtureweightsofadocumentpairintheLDA-stylemodel.ThebeautyofthebLSAframeworkisthatthemodelsearchesforacommonlatenttopicspaceinanun-supervisedfashion,ratherthantorequiremanualin-teraction.Sincethetopicspaceislanguageindepen-dent,ourapproachsupportstopictransferinmulti-plelanguagepairsinO(N)whereNisthenumberoflanguages.RelatedworkincludestheBilingualTopicAd-mixtureModel(BiTAM)forwordalignmentpro-posedby(ZhaoandXing,2006).Basically,theBiTAMmodelconsistsoftopic-dependenttransla-tionlexiconsmodelingPr(c|e,k)wherec,eandkdenotesthesourceChineseword,targetEnglishwordandthetopicindexrespectively.Ontheotherhand,thebLSAframeworkmodelsPr(c|k)andPr(e|k)whichisdifferentfromtheBiTAMmodel.Bytheirdifferentmodelingnature,thebLSAmodelusuallysupportsmoretopicsthantheBiTAMmodel.Anotherworkby(KimandKhudanpur,2004)employedcrosslingualLSAusingsingularvaluedecompositionwhichconcatenatesbilingualdocumentsintoasingleinputsupervectorbeforeprojection.Weorganizethepaperasfollows:InSection2,weintroducethebLSAframeworkincludingLa-tentDirichlet-TreeAllocation(LDTA)(TamandSchultz,2007)asacorrelatedLSAmodel,bLSAtrainingandcrosslingualLMadaptation.InSec-tion3,wepresenttheeffectofLMadaptationonwordperplexity,followedbySMTexperimentsre-portedinBLEUonbothspeechandtextinputinSection3.3.Section4describesconclusionsandfu-ASR hypoChinese LSAEnglish LSAChinese N−gram LMEnglish N−gram LMChinese ASRChinese−>English SMTChinese−EnglishAdaptAdaptMT hypoTopic distributionParallel document corpusChinese textEnglish textFigure1:TopictransferinbilingualLSAmodel.tureworks.2BilingualLatentSemanticAnalysisThegoalofabLSAmodelistoenforceaone-to-onetopiccorrespondencebetweenmonolingualLSAmodels,eachofwhichcanbemodeledusinganLDA-stylemodel.TheroleofthebLSAmodelistotransfertheinferredlatenttopicdistributionfromthesourcelanguagetothetargetlanguageas-sumingthatthetopicdistributionsonbothsidesareidentical.Theassumptionisreasonableforparalleldocumentpairswhicharefaithfultranslations.Fig-ure1illustratestheideaoftopictransferbetweenmonolingualLSAmodelsfollowedbyLMadapta-tion.Oneobservationisthatthetopictransfercanbebi-directionalmeaningthatthe“ﬂow”oftopiccanbefromASRtoSMTorviceversa.Inthispaper,weonlyfocusonASR-to-SMTdirection.Ourtar-getistominimizethewordperplexityonthetargetlanguagethroughLMadaptation.Beforeweintro-ducetheheuristicofenforcingaone-to-onetopiccorrespondence,wedescribetheLatentDirichlet-TreeAllocation(LDTA)forLSA.2.1LatentDirichlet-TreeAllocationTheLDTAmodelextendstheLDAmodelinwhichcorrelationamonglatenttopicsarecapturedusingaDirichlet-Treeprior.Figure2illustratesadepth-twoDirichlet-Tree.AtreeofdepthonesimplyfallsbacktotheLDAmodel.TheLDTAmodelisagenerativemodelwiththefollowinggenerativeprocess:1.Sampleavectorofbranchprobabilitiesbj∼522

Dir(.)Dir(.)Dir(.)Dir(.)topic 1topic 4Latent topicstopic Kj=1j=2j=3Figure2:Dirichlet-Treepriorofdepthtwo.Dir(αj)foreachnodej=1...Jwhereαjde-notestheparameter(akathepseudo-countsofitsoutgoingbranches)oftheDirichletdistribu-tionatnodej.2.Computethetopicproportionsas:θk=Yjcbδjc(k)jc(1)whereδjc(k)isanindicatorfunctionwhichsetstounitywhenthec-thbranchofthej-thnodeleadstotheleafnodeoftopickandzeroother-wise.Thek-thtopicproportionθkiscomputedastheproductofbranchprobabilitiesfromtherootnodetotheleafnodeoftopick.3.Generateadocumentusingthetopicmultino-mialforeachwordwi:zi∼Mult(θ)wi∼Mult(β.zi)whereβ.zidenotesthetopic-dependentuni-gramLMindexedbyzi.Thejointdistributionofthelatentvariables(topicsequencezn1andtheDirichletnodesoverchildbranchesbj)andanobserveddocumentwn1canbewrittenasfollows:p(wn1,zn1,bJ1)=p(bJ1|{αj})nYiβwizi·θziwherep(bJ1|{αj})=JYjDir(bj;αj)∝Yjcbαjc−1jcSimilartoLDAtraining,weapplythevariationalBayesapproachbyoptimizingthelowerboundofthemarginalizeddocumentlikelihood:L(wn1;Λ,Γ)=Eq[logp(wn1,zn1,bJ1;Λ)q(zn1,bJ1;Γ)]=Eq[logp(wn1|zn1)]+Eq[logp(zn1|bJ1)q(zn1)]+Eq[logp(bJ1;{αj})q(bJ1;{γj})]whereq(zn1,bJ1;Γ)=Qniq(zi)·QJjq(bj)isafac-torizablevariationalposteriordistributionoverthelatentvariablesparameterizedbyΓwhicharedeter-minedintheE-step.ΛisthemodelparametersforaDirichlet-Tree{αj}andthetopic-dependentuni-gramLM{βwk}.TheLDTAmodelhasanE-stepsimilartotheLDAmodel:E-Step:γjc=αjc+nXiKXkqik·δjc(k)(2)qik∝βwik·eEq[logθk](3)whereEq[logθk]=Xjcδjc(k)Eq[logbjc]=Xjcδjc(k) Ψ(γjc)−Ψ(Xcγjc)!whereqikdenotesq(zi=k)meaningthevariationaltopicposteriorofwordwi.Eqn2andEqn3areexecutediterativelyuntilconvergenceisreached.M-Step:βwk∝nXiqik·δ(wi,w)(4)whereδ(wi,w)isaKroneckerDeltafunction.ThealphaparameterscanbeestimatedwithiterativemethodssuchasNewton-Raphsonorsimplegradi-entascentprocedure.2.2BilingualLSAtrainingForthefollowingexplanations,weassumethatoursourceandtargetlanguagesareChineseandEn-glishrespectively.ThebLSAmodeltrainingisa523

two-stageprocedure.Attheﬁrststage,wetrainaChineseLSAmodelusingtheChinesedocu-mentsinparallelcorpora.Weappliedthevaria-tionalEMalgorithm(Eqn2–4)totrainaChineseLSAmodel.ThenweusedthemodeltocomputethetermeEq[logθk]neededinEqn3foreachChinesedocumentinparallelcorpora.Atthesecondstage,weapplythesameeEq[logθk]tobootstrapanEnglishLSAmodel,whichisthekeytoenforceaone-to-onetopiccorrespondence.Nowthehyper-parametersofthevariationalDirichletposteriorsofeachnodeintheDirichlet-TreearesharedamongtheChineseandEnglishmodel.Precisely,weapplyonlyEqn3withﬁxedeEq[logθk]intheE-stepandEqn4intheM-stepon{βwk}tobootstrapanEnglishLSAmodel.No-ticethattheE-stepisnon-iterativeresultinginrapidLSAtraining.Inshort,givenamonolingualLSAmodel,wecanrapidlybootstrapLSAmodelsofnewlanguagesusingparalleldocumentcorpora.NoticethattheEnglishandChinesevocabularysizesdonotneedtobesimilar.Inoursetup,theChinesevo-cabularycomesfromtheASRsystemwhiletheEn-glishvocabularycomesfromtheEnglishpartoftheparallelcorpora.Sincethetopictransfercanbebi-directional,wecanperformthebLSAtraininginareversemanner,i.e.traininganEnglishLSAmodelfollowedbybootstrappingaChineseLSAmodel.2.3CrosslingualLMadaptationGivenasourcetext,weapplytheE-steptoestimatevariationalDirichletposteriorofeachnodeintheDirichlet-Tree.Weestimatethetopicweightsonthesourcelanguageusingthefollowingequation:ˆθ(CH)k∝Yjc(cid:18)γjcPc′γjc′(cid:19)δjc(k)(5)ThenweapplythetopicweightsintothetargetLSAmodeltoobtainanin-domainLSAmarginals:PrEN(w)=KXk=1β(EN)wk·ˆθ(CH)k(6)WeintegratetheLSAmarginalintothetargetback-groundLMusingmarginaladaptation(Kneseretal.,1997)whichminimizestheKullback-Leiblerdiver-gencebetweentheadaptedLMandthebackgroundLM:Pra(w|h)∝(cid:18)Prldta(w)Prbg(w)(cid:19)β·Prbg(w|h)(7)Likewise,LMadaptationcantakeplaceonthesourcelanguageaswellduetothebi-directionalna-tureofthebLSAframeworkwhentarget-sideadap-tationtextisavailable.Inthispaper,wefocusonLMadaptationonthetargetlanguageforSMT.3ExperimentalSetupWeevaluatedourbLSAmodelusingtheChinese–EnglishparalleldocumentcorporaconsistingoftheXinhuanews,HongKongnewsandSinanews.Thecombinedcorporacontains67kparalleldocumentswith35MChinese(CH)wordsand43MEnglish(EN)words.Ourspokenlanguagetranslationsys-temtranslatesfromChinesetoEnglish.TheChinesevocabularycomesfromtheASRdecoderwhiletheEnglishvocabularyisderivedfromtheEnglishpor-tionoftheparalleltrainingcorpora.ThevocabularysizesforChineseandEnglishare108kand69kre-spectively.OurbackgroundEnglishLMisa4-gramLMtrainedwiththemodiﬁedKneser-Neysmooth-ingschemeusingtheSRILMtoolkitonthesametrainingtext.WeexplorethebLSAtraininginbothdirections:EN→CHandCH→ENmeaningthatanEnglishLSAmodelistrainedﬁrstandaChineseLSAmodelisbootstrappedorviceversa.Exper-imentsexplorewhichbootstrappingdirectionyieldbestresultsmeasuredintermsofEnglishwordper-plexity.Thenumberoflatenttopicsissetto200andabalancedbinaryDirichlet-Treepriorisused.WithanincreasinginterestintheASR-SMTcou-plingforspokenlanguagetranslation,wealsoeval-uatedourapproachwithChineseASRhypothesesandcomparedwithChinesemanualtranscriptions.Weareinterestedtoseetheimpactduetorecog-nitionerrorsontheASRhypothesescomparedtothemanualtranscriptions.WeemployedtheCMU-InterACTASRsystemdevelopedfortheGALE2006evaluation.Wetrainedacousticmodelswithover500hoursofquicklytranscribedspeechdatare-leasedbytheGALEprogramandtheLMwithover800M-wordChinesecorpora.ThecharactererrorratesontheCCTV,RFAandNTDTVshowsintheRT04testsetare7.4%,25.5%and13.1%respec-tively.524

TopicindexTopwords“CH-40”ﬂying,submarine,aircraft,air,pilot,land,mission,brand-new“EN-40”air,sea,submarine,aircraft,ﬂight,ﬂying,ship,test“CH-41”satellite,han-tian,launch,space,china,technology,astronomy“EN-41”space,satellite,china,technology,satellites,science“CH-42”ﬁre,airport,services,marine,accident,air“EN-42”ﬁre,airport,services,department,marine,air,serviceTable1:ParalleltopicsextractedbythebLSAmodel.TopwordsontheChinesesidearetranslatedintoEnglishforillustrationpurpose.-3.05e+08-3e+08-2.95e+08-2.9e+08-2.85e+08-2.8e+08-2.75e+08-2.7e+08 2 4 6 8 10 12 14 16 18 20Training log likelihood# of training iterationsbootstrapped EN LSAmonolingual EN LSAFigure3:ComparisonoftrainingloglikelihoodofEnglishLSAmodelsbootstrappedfromaChineseLSAandfromaﬂatmonolingualEnglishLSA.3.1AnalysisofthebLSAmodelByexaminingthetop-wordsoftheextractedparal-leltopics,weverifythevalidityoftheheuristicde-scribedinSection2.2whichenforcesaone-to-onetopiccorrespondenceinthebLSAmodel.Table1showsthelatenttopicsextractedbytheCH→ENbLSAmodel.WecanseethattheChinese-Englishtopicwordshavestrongcorrelations.Manyofthemareactuallytranslationpairswithsimilarwordrank-ings.Fromthisviewpoint,wecaninterpretbLSAasacrosslingualwordtriggermodel.Theresultindi-catesthatourheuristiciseffectivetoextractparallellatenttopics.Asasanitycheck,wealsoexaminethelikelihoodofthetrainingdatawhenanEnglishLSAmodelisbootstrapped.WecanseefromFigure3thatthelikelihoodincreasesmonotonicallywiththenumberoftrainingiterations.TheﬁgurealsoshowsthatbysharingthevariationalDirichletposteriorsfromtheChineseLSAmodel,wecanbootstrapanEnglishLSAmodelrapidlycomparedtomonolin-gualEnglishLSAtrainingwithbothtrainingproce-duresstartedfromthesameﬂatmodel.LM(43M)CCTVRFANTDTVBGENunigram106512201549+CH→EN(CHref)7558801113+EN→CH(CHref)7628961111+CH→EN(CHhypo)7578851126+EN→CH(CHhypo)7668961129+CH→EN(ENref)7318381075+EN→CH(ENref)7478481087Table2:Englishwordperplexity(PPL)ontheRT04testsetusingaunigramLM.3.2LMadaptationresultsWetrainedthebLSAmodelsonbothCH→ENandEN→CHdirectionsandcomparedtheirLMadapta-tionperformanceusingtheChineseASRhypothe-ses(hypo)andthemanualtranscriptions(ref)asin-put.WeadaptedtheEnglishbackgroundLMusingtheLSAmarginalsdescribedinSection2.3foreachshowonthetestset.WeﬁrstevaluatedtheEnglishwordperplexityus-ingtheENunigramLMgeneratedbythebLSAmodel.Table2showsthatthebLSA-basedLMadaptationreducesthewordperplexitybyover27%relativecomparedtoanunadaptedENunigramLM.TheresultsindicatethatthebLSAmodelsuccess-fullyleveragesthetextfromthesourcelanguageandimprovesthewordperplexityonthetargetlanguage.Weobservethatthereisalmostnoperformancedif-ferencewheneithertheASRhypothesesortheman-ualtranscriptionsareusedforadaptation.TheresultisencouragingsincethebLSAmodelmaybein-sensitivetomoderaterecognitionerrorsthroughtheprojectionoftheinputadaptationtextintothelatenttopicspace.WealsoapplyanEnglishtranslationreferenceforadaptationtoshowanoracleperfor-mance.TheresultsusingtheChinesehypothesesarenottoofarofffromtheoracleperformance.AnotherobservationisthattheCH→ENbLSAmodelseemstogivebetterperformancethantheEN→CHbLSAmodel.However,theirdifferencesarenotsigniﬁ-cant.TheresultmayimplythatthedirectionofthebLSAtrainingisnotimportantsincethelatenttopicspacecapturedbyeitherlanguageissimilarwhenparalleltrainingcorporaareused.Table3showsthewordperplexitywhenthebackground4-gramEn-glishLMisadaptedwiththetuningparameterβset525

LM(43M,β=0.7)CCTVRFANTDTVBGEN4-gram118212203+CH→EN(CHref)102191179+EN→CH(CHref)102198179+CH→EN(CHhypo)102193180+EN→CH(CHhypo)103198180+CH→EN(ENref)100186176+EN→CH(ENref)101190176Table3:Englishwordperplexity(PPL)ontheRT04testsetusinga4-gramLM. 100 105 110 115 120 125 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1English Word PerplexityBetaCCTV (CER=7.4%)BG 4-gram+bLSA (CH reference)+bLSA (CH ASR hypo)+bLSA (EN reference)Figure4:WordperplexitywithdifferentβusingmanualreferenceorASRhypothesesonCCTV.to0.7.Figure4showsthechangeofperplexitywithdifferentβ.WeseethattheadaptationperformanceusingtheASRhypothesesorthemanualtranscrip-tionsarealmostidenticalondifferentβwithanop-timalvalueataround0.7.Theresultsshowthattheproposedapproachsuccessfullyreducestheperplex-ityintherangeof9–13.6%relativecomparedtoanunadaptedbaselineondifferentshowswhenASRhypothesesareused.Moreover,weobservesimi-larperformanceusingASRhypothesesormanualChinesetranscriptionswhichisconsistentwiththeresultsonTable2.Ontheotherhand,itisinterest-ingtoseethattheperformancegapfromtheoracleadaptationissomewhatrelatedtothedegreeofmis-matchbetweenthetestshowandthetrainingcondi-tion.ThegaplookswiderontheRFAandNTDTVshowscomparedtotheCCTVshow.3.3IncorporatingbLSAintoSpokenLanguageTranslationToinvestigatetheeffectivenessofbLSALMadap-tationforspokenlanguagetranslation,weincorpo-ratedtheproposedapproachintoourstate-of-the-artphrase-basedSMTsystem.TranslationperformancewasevaluatedontheRT04broadcastnewsevalua-tionsetwhenappliedtoboththemanualtranscrip-tionsand1-bestASRhypotheses.Duringevalua-tiontwoperformancemetrics,BLEU(Papinenietal.,2002)andNIST,werecomputed.Inbothcases,asingleEnglishreferencewasusedduringscoring.InthetranscriptioncasetheoriginalEnglishreferenceswereused.FortheASRcase,asutterancesegmen-tationwasperformedautomatically,thenumberofsentencesgeneratedbyASRandSMTdifferedfromthenumberofEnglishreferences.Inthiscase,Lev-enshteinalignmentwasusedtoalignthetranslationoutputtotheEnglishreferencesbeforescoring.3.4BaselineSMTSetupThebaselineSMTsystemconsistedofanonadap-tivesystemtrainedusingthesameChinese-Englishparalleldocumentcorporausedinthepreviousex-periments(Sections3.1and3.2).Forphraseextrac-tionacleanedsubsetofthesecorpora,consistingof1MChinese-Englishsentencepairs,wasused.SMTdecodingparameterswereoptimizedusingman-ualtranscriptionsandtranslationsof272utterancesfromtheRT04developmentset(LDC2006E10).SMTtranslationwasperformedintwostagesus-inganapproachsimilartothatin(Vogel,2003).First,atranslationlatticewasconstructedbymatch-ingallpossiblebilingualphrase-pairs,extractedfromthetrainingcorpora,totheinputsentence.Phraseextractionwasperformedusingthe“PESA”(PhrasePairExtractionasSentenceSplitting)ap-proachdescribedin(Vogel,2005).Next,asearchwasperformedtoﬁndthebestpaththroughthelat-tice,i.e.thatwithmaximumtranslation-score.Dur-ingsearchreorderingwasallowedonthetargetlan-guageside.Theﬁnaltranslationresultwasthathypothesiswithmaximumtranslation-score,whichisalog-linearcombinationof10scoresconsist-ingofTargetLMprobability,DistortionPenalty,Word-CountPenalty,Phrase-CountandsixPhrase-Alignmentscores.WeightsforeachcomponentscorewereoptimizedtomaximizeBLEU-scoreonthedevelopmentsetusingMERoptimizationasde-scribedin(Venugopaletal.,2005).526

TranslationQuality-BLEU(NIST)SMTTargetLMCCTVRFANTDTVALLManualTranscriptionBaselineLM:0.162(5.212)0.087(3.854)0.140(4.859)0.132(5.146)bLSA(bLSA-AdaptedLM):0.164(5.212)0.087(3.897)0.143(4.864)0.134(5.162)1-bestASROutputCER(%)7.425.513.114.9BaselineLM:0.129(4.15)0.051(2.77)0.086(3.50)0.095(3.90)bLSA(bLSA-AdaptedLM):0.132(4.16)0.050(2.79)0.089(3.53)0.096(3.91)Table4:TranslationperformanceofbaselineandbLSA-AdaptedChinese-EnglishSMTsystemsonmanualtranscriptionsand1-bestASRhypotheses3.5PerformanceofBaselineSMTSystemFirst,thebaselinesystemperformancewasevalu-atedbyapplyingthesystemdescribedabovetothereferencetranscriptionsand1-bestASRhypothesesgeneratedbyourMandarinspeechrecognitionsys-tem.ThetranslationaccuracyintermsofBLEUandNISTforeachindividualshow(“CCTV”,“RFA”,and“NTDTV”),andforthecompletetest-set,areshowninTable4(BaselineLM).WhenappliedtothereferencetranscriptionsanoverallBLEUscoreof0.132wasobtained.BLEU-scoresrangedbe-tween0.087and0.162forthe“RFA”,“NTDTV”and“CCTV”shows,respectively.Asthe“RFA”showcontainedalargesegmentofconversationalspeech,translationqualitywasconsiderablylowerforthisshowduetogenremismatchwiththetrainingcor-poraofnewspapertext.Forthe1-bestASRhypotheses,anoverallBLEUscoreof0.095wasachieved.FortheASRcase,therelativereductioninBLEUscoresfortheRFAandNTDTVshowsislarge,duetothesigniﬁcantlylowerrecognitionaccuraciesfortheseshows.BLEUscoreisalsodegradedduetopooralignmentofref-erencesduringscoring.3.6IncorporationofbLSAAdaptationNext,theeffectivenessofbLSAbasedLMadapta-tionwasevaluated.ForeachshowthetargetEn-glishLMwasadaptedusingbLSA-adaptation,asdescribedinSection2.3.SMTwasthenappliedus-inganidenticalsetuptothatusedinthebaselineex-periments.ThetranslationaccuracywhenbLSAadaptationwasincorporatedisshowninTable4.Whenap-00.020.040.060.080.10.120.140.16CCTVRFANTDTVAll showsBLEUBaseline-LMbLSA Adapted LMFigure5:BLEUscoreforthose25%utteranceswhichresultedindifferenttranslationsafterbLSAadaptation(manualtranscriptions)pliedtothemanualtranscriptions,bLSAadaptationimprovedtheoverallBLEU-scoreby1.7%relative(from0.132to0.134).ForallthreeshowsbLSAadaptationgainedhigherBLEUandNISTmetrics.Asimilartrendwasalsoobservedwhenthepro-posedapproachwasappliedtothe1-bestASRout-put.OntheevaluationsetarelativeimprovementinBLEUscoreof1.0%wasgained.Thesemanticinterpretationofthemajorityofut-terancesinbroadcastnewsarenotaffectedbytopiccontext.Intheexperimentalevaluationitwasob-servedthatonly25%ofutterancesproduceddiffer-enttranslationoutputwhenbLSAadaptationwasperformedcomparedtothetopic-independentbase-line.Althoughtheimprovementintranslationqual-ity(BLEU)wassmallwhenevaluatedovertheen-tiretestset,theimprovementinBLEUscorefor527

these25%utteranceswassigniﬁcant.Thetrans-lationqualityforthebaselineandbLSA-adaptivesystemwhenevaluatedonlyontheseutterancesisshowninFigure5forthemanualtranscriptioncase.OnthissubsetofutterancesanoverallimprovementinBLEUof0.007(5.7%relative)wasgained,withagainof0.012(10.6%relative)pointsforthe“NT-DTV”show.Asimilartrendwasobservedwhenap-pliedtothe1-bestASRoutput.Inthiscasearel-ativeimprovementinBLEUof12.6%wasgainedfor“NTDTV”,andfor“Allshows”0.007(3.7%)wasgained.Currentevaluationmetricsfortrans-lation,suchas“BLEU”,donotconsidertherela-tiveimportanceofspeciﬁcwordsorphrasesduringtranslationandthusareunabletohighlightthetrueeffectivenessoftheproposedapproach.Infuturework,weintendtoinvestigateotherevaluationmet-ricswhichconsidertherelativeinformationalcon-tentofwords.4ConclusionsWeproposedabilinguallatentsemanticmodelforcrosslingualLMadaptationinspokenlanguagetranslation.ThebLSAmodelconsistsofasetofmonolingualLSAmodelsinwhichaone-to-onetopiccorrespondenceisenforcedbetweentheLSAmodelsthroughthesharingofvariationalDirich-letposteriors.BootstrappingaLSAmodelforanewlanguagecanbeperformedrapidlywithtopictransferfromawell-trainedLSAmodelofanotherlanguage.Wetransfertheinferredtopicdistribu-tionfromtheinputsourcetexttothetargetlan-guageeffectivelytoobtainanin-domaintargetLSAmarginalsforLMadaptation.Resultsshowedthatourapproachsigniﬁcantlyreducesthewordper-plexityonthetargetlanguageinbothcasesusingASRhypothesesandmanualtranscripts.Interest-ingly,theadaptationperformanceisnotmuchaf-fectedwhenASRhypotheseswereused.Weeval-uatedtheadaptedLMonSMTandfoundthattheevaluationmetricsarecrucialtoreﬂecttheactualimprovementinperformance.Futuredirectionsin-cludetheexplorationofstory-dependentLMadap-tationwithautomaticstorysegmentationinsteadofshow-dependentadaptationduetothepossibilityofmultiplestorieswithinashow.Wewillinvestigatetheincorporationofmonolingualdocumentsforpo-tentiallybetterbilingualLSAmodeling.AcknowledgmentThisworkispartlysupportedbytheDefenseAd-vancedResearchProjectsAgency(DARPA)underContractNo.HR0011-06-2-0001.Anyopinions,ﬁndingsandconclusionsorrecommendationsex-pressedinthismaterialarethoseoftheauthorsanddonotnecessarilyreﬂecttheviewsofDARPA.ReferencesD.Blei,A.Ng,andM.Jordan.2003.LatentDirichletAllocation.InJournalofMachineLearningResearch,pages1107–1135.W.KimandS.Khudanpur.2003.LMadaptationusingcross-lingualinformation.InProc.ofEurospeech.W.KimandS.Khudanpur.2004.Cross-linguallatentsemanticanalysisforLM.InProc.ofICASSP.R.Kneser,J.Peters,andD.Klakow.1997.Languagemodeladaptationusingdynamicmarginals.InProc.ofEurospeech,pages1971–1974.K.Papineni,S.Roukos,T.Ward,andW.Zhu.2002.BLEU:Amethodforautomaticevaluationofmachinetranslation.InProc.ofACL.M.Paulik,C.F¨ugen,T.Schaaf,T.Schultz,S.St¨uker,andA.Waibel.2005.Documentdrivenmachinetransla-tionenhancedautomaticspeechrecognition.InProc.ofInterspeech.Y.C.TamandT.Schultz.2006.Unsupervisedlanguagemodeladaptationusinglatentsemanticmarginals.InProc.ofInterspeech.Y.C.TamandT.Schultz.2007.Correlatedlatentseman-ticmodelforunsupervisedlanguagemodeladaptation.InProc.ofICASSP.A.Venugopal,A.Zollmann,andA.Waibel.2005.Train-ingandevaluationerrorminimizationrulesforstatis-ticalmachinetranslation.InProc.ofACL.S.Vogel.2003.SMTdecoderdissected:Wordreorder-ing.InProc.ofICNLPKE.S.Vogel.2005.PESA:Phrasepairextractionassentencesplitting.InProc.oftheMachineTranslationSummit.B.ZhaoandE.P.Xing.2006.BiTAM:Bilingualtopicadmixturemodelsforwordalignment.InProc.ofACL.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 528–535,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

528

CoreferenceResolutionUsingSemanticRelatednessInformationfromAutomaticallyDiscoveredPatternsXiaofengYangJianSuInstituteforInfocommResearch21HengMuiKengTerrace,Singapore,119613{xiaofengy,sujian}@i2r.a-star.edu.sgAbstractSemanticrelatednessisaveryimportantfac-torforthecoreferenceresolutiontask.Toobtainthissemanticinformation,corpus-basedapproachescommonlyleveragepat-ternsthatcanexpressaspeciﬁcsemanticrelation.Thepatterns,however,arede-signedmanuallyandthusarenotnecessar-ilythemosteffectiveonesintermsofac-curacyandbreadth.Todealwiththisprob-lem,inthispaperweproposeanapproachthatcanautomaticallyﬁndtheeffectivepat-ternsforcoreferenceresolution.Weexplorehowtoautomaticallydiscoverandevaluatepatterns,andhowtoexploitthepatternstoobtainthesemanticrelatednessinformation.TheevaluationonACEdatasetshowsthatthepatternbasedsemanticinformationishelpfulforcoreferenceresolution.1IntroductionSemanticrelatednessisaveryimportantfactorforcoreferenceresolution,asnounphrasesusedtore-fertothesameentityshouldhaveacertainsemanticrelation.Toobtainthissemanticinformation,previ-ousworkonreferenceresolutionusuallyleveragesasemanticlexiconlikeWordNet(VieiraandPoe-sio,2000;Harabagiuetal.,2001;Soonetal.,2001;NgandCardie,2002).However,thedrawbackofWordNetisthatmanyexpressions(especiallyforpropernames),wordsensesandsemanticrelationsarenotavailablefromthedatabase(VieiraandPoe-sio,2000).Inrecentyears,increasinginteresthasbeenseeninminingsemanticrelationsfromlargetextcorpora.Onecommonsolutionistoutilizeapatternthatcanrepresentaspeciﬁcsemanticrela-tion(e.g.,“XsuchasY”foris-arelation,and“XandotherY”forother-relation).Instantiatedwithtwogivennounphrases,thepatternissearchedinalargecorpusandtheoccurrencenumberisusedasameasureoftheirsemanticrelatedness(Markertetal.,2003;Modjeskaetal.,2003;Poesioetal.,2004).However,inthepreviouspatternbasedap-proaches,theselectionofthepatternstorepresentaspeciﬁcsemanticrelationisdoneinanadhocway,usuallybylinguisticintuition.Themanuallyse-lectedpatterns,nevertheless,arenotnecessarilythemosteffectiveonesforcoreferenceresolutionfromthefollowingtwoconcerns:•Accuracy.Canthepatterns(e.g.,“XsuchasY”)ﬁndasmanyNPpairsofthespeciﬁcse-manticrelation(e.g.is-a)aspossible,withahighprecision?•Breadth.Canthepatternscoverawidevarietyofsemanticrelations,notjustis-a,bywhichcoreferencerelationshipisrealized?Forex-ample,insomeannotationschemeslikeACE,“Beijing:China”arecoreferentialasthecapitalandthecountrycouldbeusedtorepresentthegovernment.Thepatternforthecommon“is-a”relationwillfailtoidentifytheNPpairsofsucha“capital-country”relation.Todealwiththisproblem,inthispaperwepro-poseanapproachwhichcanautomaticallydiscovereffectivepatternstorepresentthesemanticrelations529

forcoreferenceresolution.Weexploretwoissuesinourstudy:(1)Howtoautomaticallyacquireandevaluatethepatterns?WeutilizeasetofcoreferentialNPpairsasseeds.Foreachseedpair,wesearchalargecorpusforthetextswherethetwonounphrasesco-occur,andcollectthesurroundingwordsasthesur-facepatterns.Weevaluateapatternbasedonitscommonalityorassociationwiththepositiveseedpairs.(2)Howtominethepatternstoobtaintheseman-ticrelatednessinformationforcoreferenceresolu-tion?Wepresenttwostrategiestoexploitthepat-terns:choosingthetopbestpatternsasasetofpat-ternfeatures,orcomputingthereliabilityofseman-ticrelatednessasasinglefeature.Ineitherstrategy,theobtainedfeaturesareappliedtodocoreferenceresolutioninasupervised-learningway.Toourknowledge,ourworkistheﬁrsteffortthatsystematicallyexplorestheseissuesinthecorefer-enceresolutiontask.WeevaluateourapproachonACEdataset.Theexperimentalresultsshowthatthepatternbasedsemanticrelatednessinformationishelpfulforthecoreferenceresolution.Theremainderofthepaperisorganizedasfol-lows.Section2givessomerelatedwork.Section3introducestheframeworkforcoreferenceresolution.Section4presentsthemodeltoobtainthepattern-basedsemanticrelatednessinformation.Section5discussestheexperimentalresults.Finally,Section6summarizestheconclusions.2RelatedWorkEarlierworkoncoreferenceresolutioncommonlyreliesonsemanticlexiconsforsemanticrelatednessknowledge.InthesystembyVieiraandPoesio(2000),forexample,WordNetisconsultedtoobtainthesynonymy,hypernymyandmeronymyrelationsforresolvingthedeﬁniteanaphora.In(Harabagiuetal.,2001),thepathpatternsinWordNetareuti-lizedtocomputethesemanticconsistencybetweenNPs.Recently,PonzettoandStrube(2006)suggesttominesemanticrelatednessfromWikipedia,whichcandealwiththedatasparsenessproblemsufferedbyusingWordNet.Insteadofleveragingexistinglexicons,manyresearchershaveinvestigatedcorpus-basedap-proachestominesemanticrelations.GareraandYarowsky(2006)proposeanunsupervisedmodelwhichextractshypernymrelationforreslovingdef-initeNPs.TheirmodelassumesthatadeﬁniteNPanditshypernymwordsusuallyco-occurintexts.Thus,foradeﬁnite-NPanaphor,aprecedingNPthathasahighco-occurrencestatisticsinalargecorpusispreferredfortheantecedent.BeanandRiloff(2004)presentasystemcalledBABARthatusescontextualroleknowledgetodocoreferenceresolution.TheyapplyanIEcomponenttounannotatedtextstogenerateasetofextractioncaseframes.Eachcaseframerepresentsalinguis-ticexpressionandasyntacticposition,e.g.“mur-derof<NP>”,“killed<patient>”.Fromthecase-frames,theyderivedifferenttypesofcontextualroleknowledgeforresolution,forexample,whetherananaphorandanantecedentcandidatecanbeﬁlledintoco-occurringcaseframes,orwhethertheyaresubstitutableforeachotherintheircaseframes.Dif-ferentfromtheirsystem,ourapproachaimstoﬁndsurfacepatternsthatcandirectlyindicatethecoref-erencerelationbetweentwoNPs.Hearst(1998)presentsamethodtoautomatethediscoveryofWordNetrelations,bysearchingforthecorrespondingpatternsinlargetextcorpora.Sheex-ploresseveralpatternsforthehyponymyrelation,including“XsuchasY”“Xand/orotherY”,“Xincluding/especiallyY”andsoon.TheuseofHearst’sstylepatternscanbeseenforthereferenceresolutiontask.Modjeskaetal.(2003)exploretheuseoftheWebtodotheother-anaphoraresolution.Intheirapproach,apattern“XandotherY”isused.Givenananaphorandacandidateantecedent,thepatternisinstantiatedwiththetwoNPsandformsaquery.ThequeryissubmittedtotheGooglesearch-ingengine,andthereturnedhitnumberisutilizedtocomputethesemanticrelatednessbetweenthetwoNPs.Intheirwork,thesemanticinformationisusedasafeatureforthelearner.Markertetal.(2003)andPoesioetal.(2004)adoptasimilarstrategyforthebridginganaphoraresolution.In(Hearst,1998),theauthoralsoproposestodis-covernewpatternsinsteadofusingthemanuallydesignedones.Sheemploysabootstrappingalgo-rithmtolearnnewpatternsfromthewordpairswithaknownrelation.BasedonHearst’swork,Pan-telandPennacchiotti(2006)furthergiveamethod530

whichmeasuresthereliabilityofthepatternsbasedonthestrengthofassociationbetweenpatternsandinstances,employingthepointwisemutualinforma-tion(PMI).3FrameworkofCoreferenceResolutionOurcoreferenceresolutionsystemadoptsthecommonlearning-basedframeworkasemployedbySoonetal.(2001)andNgandCardie(2002).Inthelearningframework,atrainingortestinginstancehastheformofi{NPi,NPj},inwhichNPjisapossibleanaphorandNPiisoneofitsan-tecedentcandidates.Aninstanceisassociatedwithavectoroffeatures,whichisusedtodescribethepropertiesofthetwonounphrasesaswellastheirrelationships.Inourbaselinesystem,weadoptthecommonfeaturesforcoreferenceresolutionsuchaslexicalproperty,distance,string-matching,name-alias,apposition,grammaticalrole,number/genderagreementandsoon.Thesamefeaturesetisde-scribedin(NgandCardie,2002)forreference.Duringtraining,foreachencounteredanaphorNPj,onesinglepositivetraininginstanceiscreatedforitsclosestantecedent.AndagroupofnegativetraininginstancesiscreatedforeveryinterveningnounphrasesbetweenNPjandtheantecedent.Basedonthetraininginstances,abinaryclassiﬁercanbegeneratedusinganydiscriminativelearningalgorithm,likeC5inourstudy.Forresolution,aninputdocumentisprocessedfromtheﬁrstNPtothelast.ForeachencounteredNPj,atestinstanceisformedforeachantecedentcandidate,NPi1.Thisinstanceispresentedtotheclassiﬁertodeterminethecoreferencerelationship.NPjwillberesolvedtothecandidatethatisclassiﬁedaspositive(ifany)andhasthehighestconﬁdencevalue.Inourstudy,weaugmentthecommonframeworkbyincorporatingnon-anaphorsintotraining.Wefo-cusonthenon-anaphorsthattheoriginalclassiﬁerfailstoidentify.Speciﬁcally,weapplythelearnedclassiﬁertoallthenon-anaphorsinthetrainingdoc-uments.Foreachnon-anaphorthatisclassiﬁedaspositive,anegativeinstanceiscreatedbypairingthenon-anaphoranditsfalseantecedent.Theseneg-1Forresolutionofpronouns,onlytheprecedingNPsincur-rentandprevioustwosentencesareconsideredasantecedentcandidates.Forresolutionofnon-pronouns,alltheprecedingnon-pronounsareconsidered.ativeinstancesareaddedintotheoriginaltraininginstancesetforlearning,whichwillgenerateaclas-siﬁerwiththecapabilityofnotonlyantecedentiden-tiﬁcation,butalsonon-anaphoricallyidentiﬁcation.Thenewclassierisappliedtothetestingdocumenttodocoreferenceresolutionasusual.4PatternedBasedSemanticRelatedness4.1AcquiringthePatternsToderivepatternstoindicateaspeciﬁcsemanticre-lation,asetofseedNPpairsthathavetherelationofinterestisneeded.Asdescribedintheprevioussec-tion,wehaveasetoftraininginstancesformedbyNPpairswithknowncoreferencerelationships.WecanjustusethissetofNPpairsastheseeds.Thatis,aninstancei{NPi,NPj}willbecomeaseedpair(Ei:Ej)inwhichNPicorrespondstoEiandNPjcorrespondstoEj.Increatingtheseed,foracom-monnoun,onlytheheadwordisretainedwhileforapropername,thewholestringiskept.Forex-ample,instancei{“BillClinton”,“theformerpres-ident”}willbeconvertedtoaNPpair(“BillClin-ton”:“president”).Wecreatetheseedpairforeverytraininginstancei{NPi,NPj},exceptwhen(1)NPiorNPjisapronoun;or(2)NPiandNPjhavethesameheadword.WedenoteS+andS-thesetofseedpairsderivedfromthepositiveandthenegativetraininginstances,respectively.NotethataseedpairmaypossiblybelongtoS+canS-atthesametime.ForeachoftheseedNPpairs(Ei:Ej),wesearchinalargecorpusforthestringsthatmatchthereg-ularexpression“Ei***Ej”or“Ej***Ei”,where*isawildcardforanywordorsymbol.Theregularexpressionisdeﬁnedassuchthatalltheco-occurrencesofEiandEjwithatmostthreewords(orsymbols)inbetweenareretrieved.Foreachretrievedstring,weextractasurfacepat-ternbyreplacingexpressionEiwithamark<#t1#>andEjwith<#t2#>.Ifthestringisfollowedbyasymbol,thesymbolwillbealsoincludedinthepat-tern.Thisistocreatepatternslike“X***Y[,.?]”whereY,withahighpossibility,istheheadword,butnotamodiﬁerofanothernounphrase.Asanexample,considerthepair(“BillClin-ton”:“president”).Supposethattwosentencesinacorpuscanbematchedbytheregularexpressions:531

(S1)“BillClintoniselectedPresidentoftheUnitedStates.”(S2)“TheUSPresident,MrBillClinton,to-dayadvisedIndiatomovetowardsnuclearnon-proliferationandbeginadialoguewithPakistanto...”.Thepatternstobeextractedfor(S1)and(S2),re-spectively,areP1:<#t1#>iselected<#t2#>P2:<#t2#>,Mr<#t1#>,Werecordthenumberofstringsmatchedbyapat-ternpinstantiatedwith(Ei:Ej),noted|(Ei,p,Ej)|,forlateruse.Foreachseedpair,wegeneratealistofsurfacepatternsintheaboveway.Wecollectallthepat-ternsderivedfromthepositiveseedpairsasasetofreferencepatterns,whichwillbescoredandusedtoevaluatethesemanticrelatednessforanynewNPpair.4.2ScoringthePatterns4.2.1FrequencyOnepossiblescoringschemeistoevaluateapat-ternbasedonitscommonalitytopositiveseedpairs.Theintuitionhereisthatthemoreoftenapatternisseenforthepositiveseedpairs,themoreindicativethepatternistoﬁndpositivecoreferentialNPpairs.Basedonthisidea,wescoreapatternbycalculatingthenumberofpositiveseedpairswhosepatternlistcontainsthepattern.Formally,supposingthepat-ternlistassociatedwithaseedpairsisPList(s),thefrequencyscoreofapatternpisdeﬁnedasFreqency(p)=|{s|s∈S+,p∈PList(s)}|(1)4.2.2ReliabilityAnotherpossiblewaytoevaluateapatternisbasedonitsreliability,i.e.,thedegreethatthepat-ternisassociatedwiththepositivecoreferentialNPs.Inourstudy,weusepointwisemutualinforma-tion(CoverandThomas,1991)tomeasureassoci-ationstrength,whichhasbeenprovedeffectiveinthetaskofsemanticrelationidentiﬁcation(PantelandPennacchiotti,2006).Underpointwisemutualinformation(PMI),thestrengthofassociationbe-tweentwoeventsxandyisdeﬁnedasfollows:pmi(x,y)=logP(x,y)P(x)P(y)(2)Thustheassociationbetweenapatternpandapositiveseedpairs:(Ei:Ej)is:pmi(p,(Ei:Ej))=log|(Ei,p,Ej)||(∗,∗,∗)||(Ei,∗,Ej)||(∗,∗,∗)||(∗,p,∗)||(∗,∗,∗)|(3)where|(Ei,p,Ej)|isthecountofstringsmatchedbypatternpinstantiatedwithEiandEj.Asterisk*representsawildcard,thatis:|(Ei,∗,Ej)|=Xp∈PList(Ei:Ej)|(Ei,p,Ej)|(4)|(∗,p,∗)|=X(Ei:Ej)∈S+∪S−|(Ei,p,Ej)|(5)|(∗,∗,∗)|=X(Ei:Ej)∈S+∪S−;p∈Plist(Ei:Ej)|(Ei,p,Ej)|(6)Thereliabilityofpatternistheaveragestrengthofassociationacrosseachpositiveseedpair:r(p)=Ps∈S+pmi(p,s)maxpmi|S+|(7)Heremaxpmiisusedforthenormalizationpur-pose,whichisthemaximumPMIbetweenallpat-ternsandallpositiveseedpairs.4.3ExploitingthePatterns4.3.1PatternsFeaturesOnestrategyistodirectlyusethereferencepat-ternsasasetoffeaturesforclassiﬁerlearningandtesting.Toselectthemosteffectivepatternsforthelearner,werankthepatternsaccordingtotheirscoresandthenchoosethetoppatterns(ﬁrst100inourstudy)asthefeatures.Asmentioned,thefrequencyscoreisbasedonthecommonalityofapatterntothepositiveseedpairs.However,ifapatternalsooccursfrequentlyforthenegativeseedpairs,itshouldbenotdeemedagoodfeatureasitmayleadtomanyfalsepositivepairsduringrealresolution.Totakethisfactorintoac-count,weﬁlterthepatternsbasedontheiraccuracy,whichisdeﬁnedasfollows:Accuracy(p)=|{s|s∈S+,p∈PList(s)}||{s|s∈S+∪S−,p∈PList(s)}|(8)Apatternwithanaccuracybelowthreshold0.5iseliminatedfromthereferencepatternset.There-mainingpatternsaresortedasnormal,fromwhichthetop100patternsareselectedasfeatures.532

NWireNPaperBNewsRPFRPFRPFNormalFeatures54.580.364.956.676.064.952.775.362.0+”XsuchasY”propernames55.179.064.956.876.165.052.675.161.9alltypes55.178.364.756.874.764.453.074.461.9+“XandotherY”propernames54.779.964.956.475.964.752.674.961.8alltypes54.879.865.056.475.964.752.873.361.4+patternfeatures(frequency)propernames58.775.866.257.573.964.754.071.161.4alltypes59.767.363.357.462.459.855.957.756.8+patternfeatures(ﬁlteredfrequency)propernames57.879.166.856.975.164.754.172.461.9alltypes58.177.466.456.871.263.255.068.160.9+patternfeatures(PMIreliability)propernames58.876.966.658.173.865.054.372.061.9alltypes59.670.464.658.761.660.156.058.857.4+singlereliabilityfeaturepropernames57.480.867.156.676.265.054.074.762.7alltypes57.776.465.756.775.964.955.169.561.5Table1:TheresultsofdifferentsystemsforcoreferenceresolutionEachselectedpatternpisusedasasinglefea-ture,PFp.Foraninstancei{NPi,NPj},alistofpatternsisgeneratedfor(Ei:Ej)inthesamewayasdescribedinSection4.1.ThevalueofPFpfortheinstanceissimply|(Ei,p,Ej)|.Thesetofpatternfeaturesisusedtogetherwiththeothernormalfeaturestodothelearningandtest-ing.Thus,theactualimportanceofapatternincoreferenceresolutionisautomaticallydeterminedinasupervisedlearningway.4.3.2SemanticRelatednessFeatureAnotherstrategyistouseonlyonesemanticfea-turewhichisabletoreﬂectthereliabilitythataNPpairisrelatedinsemantics.Intuitively,aNPpairwithstrongsemanticrelatednessshouldbehighlyassociatedwithasmanyreliablepatternsaspossi-ble.Basedonthisidea,wedeﬁnethesemanticre-latednessfeature(SRel)asfollows:SRel(i{NPi,NPj})=1000∗Pp∈PList(Ei:Ej)pmi(p,(Ei:Ej))∗r(p)(9)wherepmi(p,(Ei:Ej))isthepointwisemutualin-formationbetweenpatternpandaNPpair(Ei:Ej),asdeﬁnedinEq.3.r(p)isthereliabilityscoreofp(Eq.7).Asarelatednessvalueisalwaysbelow1,wemultipleitby1000sothatthefeaturevaluewillbeofintegertypewitharangefrom0to1000.NotethatamongPList(Ei:Ej),onlythereferencepatternsareinvolvedinthefeaturecomputing.5ExperimentsandDiscussion5.1ExperimentalsetupInourstudywedidevaluationontheACE-2V1.0corpus(NIST,2003),whichcontainstwodataset,traininganddevtest,usedfortrainingandtestingre-spectively.Eachofthesesetsisfurtherdividedbythreedomains:newswire(NWire),newspaper(NPa-per),andbroadcastnews(BNews).Aninputrawtextwaspreprocessedautomati-callybyapipelineofNLPcomponents,includ-ingsentenceboundarydetection,POS-tagging,TextChunkingandNamed-EntityRecognition.Twodif-ferentclassiﬁerswerelearnedrespectivelyforre-solvingpronounsandnon-pronouns.Asmentioned,thepatternbasedsemanticinformationwasonlyap-pliedtothenon-pronounresolution.Forevaluation,Vilainetal.(1995)’sscoringalgorithmwasadoptedtocomputetherecallandprecisionofthewholecoreferenceresolution.Forpatternextractionandfeaturecomputing,weusedWikipedia,aweb-basedfree-contentencyclo-pedia,asthetextcorpus.WecollectedtheEnglishWikipediadatabasedumpinNovember2006(re-fertohttp://download.wikimedia.org/).Afterallthehyperlinksandotherhtmltagswereremoved,thewholepuretextcontainsabout220Millionwords.5.2ResultsandDiscussionTable1liststheperformanceofdifferentcoref-erenceresolutionsystems.Theﬁrstlineofthetableshowsthebaselinesystemthatusesonlythecommonfeaturesproposedin(NgandCardie,2002).Fromthetable,ourbaselinesystemcan533

NOFrequencyFrequency(Filtered)PMIReliabilty1<#t1><#t2><#t2>||<#t1>|<#t1>:<#t2>2<#t2><#t1><#t1>)isa<#t2><#t2>:<#t1>3<#t1>,<#t2><#t1>)isan<#t2><#t1>.the<#t2>4<#t2>,<#t1><#t2>)isan<#t1><#t2>(<#t1>)5<#t1>.<#t2><#t2>)isa<#t1><#t1>(<#t2>6<#t1>and<#t2><#t1>orthe<#t2><#t1>(<#t2>)7<#t2>.<#t1><#t1>(the<#t2><#t1>||<#t2>|8<#t1>.the<#t2><#t1>.duringthe<#t2><#t2>||<#t1>|9<#t2>and<#t1><#t1>|<#t2><#t2>,the<#t1>10<#t1>,the<#t2><#t1>,an<#t2><#t1>,the<#t2>11<#t2>.the<#t1><#t1>)wasa<#t2><#t2>(<#t1>12<#t2>,the<#t1><#t1>inthe<#t2>-<#t1>,<#t2>13<#t2><#t1>,<#t1>-<#t2><#t1>andthe<#t2>14<#t1><#t2>,<#t1>)wasan<#t2><#t1>.<#t2>15<#t1>:<#t2><#t1>,many<#t2><#t1>)isa<#t2>16<#t1><#t2>.<#t2>)wasa<#t1><#t1>duringthe<#t2>17<#t2><#t1>.<#t1>(<#t2>.<#t1><#t2>.18<#t1>(<#t2>)<#t2>|<#t1><#t1>)isan<#t2>19<#t1>andthe<#t2><#t1>,notthe<#t2><#t2>in<#t1>.20<#t2>(<#t1>)<#t2>,many<#t1><#t2>,<#t1>............Table2:Toppatternschosenunderdifferentscoringschemesachieveagoodprecision(above75%-80%)witharecallaround50%-60%.TheoverallF-measureforNWire,NPaperandBNewsis64.9%,64.9%and62.0%respectively.Theresultsarecomparabletothosereportedin(Ng,2005)whichusessimilarfea-turesandgetsanF-measureofabout62%forthesamedataset.TherestlinesofTable1areforthesystemsus-ingthepatternbasedinformation.Inallthesys-tems,weexaminetheutilityofthesemanticinfor-mationinresolvingdifferenttypesofNPPairs:(1)NPPairscontainingpropernames(i.e.,Name:NameorName:Deﬁnites),and(2)NPPairsofalltypes.InTable1(Line2-5),wealsolisttheresultsofincorporatingtwocommonlyusedpatterns,“X(s)suchasY”and“XandotherY(s)”.Wecanﬁndthatneitherofthemanuallydesignedpatternshassignif-icantimpactontheresolutionperformance.Forallthedomains,themanualpatternsjustachieveslightimprovementinrecall(below0.6%),indicatingthatcoverageofthepatternsisnotbroadenough.5.2.1PatternFeaturesInSection4.3.1weproposeastrategythatdi-rectlyusesthepatternsasfeatures.Table2liststhetoppatternsthataresortedbasedonfrequency,ﬁl-teredfrequency(byaccuracy),andPMIreliability,ontheNWiredomainforillustration.Fromthetable,evaluatedonlybasedonfre-quency,thetoppatternsarethosethatindicatetheappositivestructurelike“X,an/a/theY”.However,ifﬁlteredbyaccuracy,patternsofsuchakindwillberemoved.Instead,thetoppatternswithbothhighfrequencyandhighaccuracyarethoseforthecopulastructure,like“Xis/was/areY”.SortedbyPMIreli-ability,patternsfortheabovetwostructurescanbeseeninthetopofthelist.Theseresultsareconsis-tentwiththeﬁndingsin(CimianoandStaab,2004)thattheappositiveandcopulastructuresareindica-tivetoﬁndtheis-arelation.Also,thetwocommonlyusedpatterns“X(s)suchasY”and“XandotherY(s)”werefoundinthefeaturelists(notshowninthetable).Theirimportanceforcoreferenceresolu-tionwillbedeterminedautomaticallybythelearn-ingalgorithm.Aninterestingpatternseeninthelistsis“X||Y|”,whichrepresentsthecaseswhenYandXappearinthesameoflineofatableinWikipedia.Forexam-ple,thefollowingtext“American||UnitedStates|WashingtonD.C.|...”isfoundinthetable“listofempires”.Thusthepair“American:UnitedStates”,whichisdeemedcoref-erentialinACE,canbeidentiﬁedbythepattern.ThesixthtilltheeleventhlinesofTable1listtheresultsofthesystemwithpatternfeatures.Fromthetable,addingthepatternfeaturesbringstheimprove-mentoftherecallagainstthebaseline.Takethesys-tembasedonﬁlteredfrequencyasanexample.Wecanobservethattherecallincreasesbyupto3.3%(forNWire).However,weseetheprecisiondrops(upto1.2%forNWire)atthesametime.Over-allthesystemachievesanF-measurebetterthanthebaselineinNWire(1.9%),whileequal(±0.2%)inNPaperandBNews.Amongthethreerankingschemes,simplyusingfrequencyleadstothelowestprecision.Bycontrast,usingﬁlteredfrequencyyieldsthehighestprecisionwithneverthelessthelowestrecall.Itisreasonablesincethelowaccuracyfeaturespronetofalseposi-534

NameAlias=1:...NameAlias=0::..Appositive=1:...Appositive=0::..P014>0::...P003<=4:0(3):P003>4:1(25)P014<=0::..P004>0:...P004<=0::..P027>0:1(25/7)P027<=0::..P002>0:...P002<=0::..P005>0:1(49/22)P005<=0::..String_Match=1:.String_Match=0:.//p002:<t1>)isa<t2>//P003:<t1>)isan<t2>//P004:<t2>)isan<t1>//p005:<t2>)isa<t1>//P014:<t1>)wasan<t2>//p027:<t1>,(<t2>,Figure1:Thedecisiontree(NWiredomain)forthesystemusingpatternfeatures(ﬁlteredfrequency)(featureStringMatchrecordswhetherthestringofanaphorNPjmatchesthatofacandidateantecedentNPi)tiveNPpairsareeliminated,atthepriceofrecall.UsingPMIReliabilitycanachievethehighestre-callwithamediumlevelofprecision.However,wedonotﬁndsigniﬁcantdifferenceintheoverallF-measureforallthesethreeschemes.Thisshouldbeduetothefactthatthepatternfeaturesneedtobefurtherchosenbythelearningalgorithm,andonlythosepatternsdeemedeffectivebythelearnerwillreallymatterintherealresolution.Fromthetable,thepatternfeaturesonlyworkwellforNPpairscontainingpropernames.Ap-pliedonalltypesofNPpairs,thepatternfeaturesfurtherboosttherecallofthesystems,butinthemeanwhiledegradetheprecisionsigniﬁcantly.TheF-measureofthesystemsisevenworsethanthatofthebaseline.Ourerroranalysisshowsthatanon-anaphorisoftenwronglyresolvedtoafalsean-tecedentoncethetwoNPshappentosatisfyapat-ternfeature,whichaffectsprecisionlargely(asanevidence,thedecreaseofprecisionislesssigniﬁcantwhenusingﬁlteredfrequencythanusingfrequency).Still,theseresultssuggestthatwejustapplythepat-ternbasedsemanticinformationinresolvingpropernameswhich,infact,ismorecompellingasthese-manticinformationofcommonnounscouldbemoreeasilyretrievedfromWordNet.WealsonoticethatthepatternedbasedsemanticinformationseemsmoreeffectiveintheNWiredo-mainthantheothertwo.EspeciallyforNPaper,theimprovementinF-measureislessthan0.1%forallthesystemstested.Theerroranalysisindicatesitmaybebecause(1)therearelessNPpairsinNPa-perthaninNWirethatrequiretheexternalseman-ticknowledgeforresolution;and(2)FormanyNPpairsthatrequirethesemanticknowledge,noco-occurrencecanbefoundintheWikipediacorpus.Toaddressthisproblem,wecouldresorttotheWebwhichcontainsalargervolumeoftextsandthuscouldleadtomoreinformativepatterns.Wewouldliketoexplorethisissueinourfuturework.InFigure1,weplotthedecisiontreelearnedwiththepatternfeaturesfornon-pronounresolution(NWiredomain,ﬁlteredfrequency),whichvisuallyillustrateswhichfeaturesareusefulinthereferencedetermination.Wecanﬁndthepatternfeaturesoc-curinthetopofthedecisiontree,amongthefeaturesfornamealias,appositionandstring-matchingthatarecrucialforcoreferenceresolutionasreportedinpreviouswork(Soonetal.,2001).Mostofthepat-ternfeaturesdeemedimportantbythelearnerareforthecopulastructure.5.2.2SingleSemanticRelatednessFeatureSection4.3.2presentsanotherstrategytoexploitthepatterns,whichusesasinglefeaturetoreﬂectthesemanticrelatednessbetweenNPpairs.ThelasttwolinesofTable1listtheresultsofsuchasystem.Observedfromthetable,thesystemwiththesin-glesemanticrelatednessfeaturebeatsthosewithothersolutions.Comparedwiththebaseline,thesystemcangetimprovementinrecall(upto2.9%asinNWire),withasimilarorevenhigherpreci-sion.TheoverallF-measureitproducesis67.1%,65.0%and62.7%,betterthanthebaselineinallthedomains.EspeciallyintheNWiredomain,wecanseethesigniﬁcant(t-test,p≤0.05)improvementof2.1%inF-measure.WhenappliedonAll-TypeNPpairs,thedegradeofperformanceislesssigniﬁcantasusingpatternfeatures.Theresultingperformanceisbetterthanthebaselineorequal.Comparedwiththesystemsusingthepatternfeatures,itcanstillachieveahigherprecisionandF-measure(withalit-tlelossinrecall).Thereareseveralreasonswhythesingleseman-ticrelatednessfeature(SRel)canperformbetterthanthesetofpatternfeatures.Firstly,thefeaturevalueofSReltakesintoconsiderationtheinformationofallthepatterns,insteadofonlytheselectedpatterns.Secondly,sincetheSRelfeatureiscomputedbasedonallthepatterns,itreducestheriskoffalseposi-535

NameAlias=1:...NameAlias=0::..Appositive=1:...Appositive=0::..SRel>28::..SRel>47:...:SRel<=47:...SRel<=28::..String_Match=1:...String_Match=0:...Figure2:Thedecisiontree(Nwire)forthesystemusingthesinglesemanticrelatednessfeaturetivewhenaNPpairhappenstosatisfyoneorseveralpatternfeatures.Lastly,fromthepointofviewofmachinelearning,usingonlyonesemanticfeature,insteadofhundredsofpatternfeatures,canavoidoverﬁttingandthusbeneﬁttheclassiﬁerlearning.InFigure2,wealsoshowthedecisiontreelearnedwiththesemanticrelatednessfeature.Weobservethatthedecisiontreeissimplerthanthatwithpat-ternfeaturesasdepictedinFigure1.Afterfeaturename-aliasandapposite,theclassiﬁerchecksdif-ferentrangesoftheSRelvalueandmakedifferentresolutiondecisionaccordingly.Thisﬁgurefurtherillustratestheimportanceofthesemanticfeature.6ConclusionsInthispaperwepresentapatternbasedapproachtocoreferenceresolution.Differentfromthepreviousworkwhichutilizesmanuallydesignedpatterns,ourapproachcanautomaticallydiscoverthepatternsef-fectiveforthecoreferenceresolutiontask.Inourstudy,weexplorehowtoacquireandevaluatepat-terns,andinvestigatehowtoexploitthepatternstominesemanticrelatednessinformationforcorefer-enceresolution.TheevaluationonACEdatasetshowsthatthepatternedbasedfeatures,whenap-pliedonNPpairscontainingpropernames,canef-fectivelyhelptheperformanceofcoreferenceres-olutionintherecall(upto4.3%)andtheoverallF-measure(upto2.1%).Theresultsalsoindicatethatusingthesinglesemanticrelatednessfeaturehasmoreadvantagesthanusingasetofpatternfeatures.Forfuturework,weintendtoinvestigateourapproachinmoredifﬁculttaskslikethebridginganaphoraresolution,inwhichthesemanticrelationsinvolvedaremorecomplicated.Also,wewouldliketoexploretheapproachintechnical(e.g.,biomedi-cal)domains,wherejargonsarefrequentlyseenandtheneedforexternalknowledgeismorecompelling.AcknowledgementsThisresearchissupportedbyaSpeciﬁcTargetedResearchProject(STREP)oftheEuropeanUnion’s6thFrameworkProgrammewithinISTcall4,Boot-strappingOfOntologiesandTerminologiesSTrategicREsearchProject(BOOTStrep).ReferencesD.BeanandE.Riloff.2004.Unsupervisedlearningofcontex-tualroleknowledgeforcoreferenceresolution.InProceed-ingsofNAACL,pages297–304.P.CimianoandS.Staab.2004.Learningbygoogling.SIGKDDExplorationsNewsletter,6(2):24–33.T.CoverandJ.Thomas.1991.ElementsofInformationThe-ory.HohnWiley&Sons.N.GareraandD.Yarowsky.2006.Resolvingandgeneratingdeﬁniteanaphorabymodelinghypernymyusingunlabeledcorpora.InProceedingsofCoNLL,pages37–44.S.Harabagiu,R.Bunescu,andS.Maiorano.2001.Textknowl-edgeminingforcoreferenceresolution.InProceedingsofNAACL,pages55–62.M.Hearst.1998.Automateddiscoveryofwordnetrelations.InChristianeFellbaum,editor,WordNet:AnElectronicLexicalDatabaseandSomeofitsApplications.MITPress,Cam-bridge,MA.K.Markert,M.Nissim,andN.Modjeska.2003.Usingthewebfornominalanaphoraresolution.InProceedingsoftheEACLworkshoponComputationalTreatmentofAnaphora,pages39–46.N.Modjeska,K.Markert,andM.Nissim.2003.Usingthewebinmachinelearningforother-anaphoraresolution.InProceedingsofEMNLP,pages176–183.V.NgandC.Cardie.2002.Improvingmachinelearningap-proachestocoreferenceresolution.InProceedingsofACL,pages104–111,Philadelphia.V.Ng.2005.Machinelearningforcoreferenceresolution:Fromlocalclassiﬁcationtoglobalranking.InProceedingsofACL,pages157–164.P.PantelandM.Pennacchiotti.2006.Espresso:Leveraginggenericpatternsforautomaticallyharvestingsemanticrela-tions.InProceedingsofACL,pages113–1200.M.Poesio,R.Mehta,A.Maroudas,andJ.Hitzeman.2004.Learningtoresolvebridgingreferences.InProceedingsofACL,pages143–150.S.PonzettoandM.Strube.2006.Exploitingsemanticrolelabeling,wordnetandwikipediaforcoreferenceresolution.InProceedingsofNAACL,pages192–199.W.Soon,H.Ng,andD.Lim.2001.Amachinelearningap-proachtocoreferenceresolutionofnounphrases.Computa-tionalLinguistics,27(4):521–544.R.VieiraandM.Poesio.2000.Anempiricallybasedsystemforprocessingdeﬁnitedescriptions.ComputationalLinguis-tics,27(4):539–592.M.Vilain,J.Burger,J.Aberdeen,D.Connolly,andL.Hirschman.1995.Amodel-theoreticcoreferencescoringscheme.InProceedingsoftheSixthMessageunderstand-ingConference(MUC-6),pages45–52,SanFrancisco,CA.MorganKaufmannPublishers.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 536–543,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

536

SemanticClassInductionandCoreferenceResolutionVincentNgHumanLanguageTechnologyResearchInstituteUniversityofTexasatDallasRichardson,TX75083-0688vince@hlt.utdallas.eduAbstractThispaperexamineswhetheralearning-basedcoreferenceresolvercanbeimprovedusingsemanticclassknowledgethatisau-tomaticallyacquiredfromaversionofthePennTreebankinwhichthenounphrasesarelabeledwiththeirsemanticclasses.Ex-perimentsontheACEtestdatashowthataresolverthatemployssuchinducedsemanticclassknowledgeyieldsastatisticallysignif-icantimprovementof2%inF-measureoveronethatexploitsheuristicallycomputedse-manticclassknowledge.Inaddition,thein-ducedknowledgeimprovestheaccuracyofcommonnounresolutionby2-6%.1IntroductionInthepastdecade,knowledge-leanapproacheshavesigni(cid:2)cantlyin(cid:3)uencedresearchinnounphrase(NP)coreferenceresolution(cid:151)theproblemofdeter-miningwhichNPsrefertothesamereal-worlden-tityinadocument.Inknowledge-leanapproaches,coreferenceresolversemployonlymorpho-syntacticcuesasknowledgesourcesintheresolutionprocess(e.g.,Mitkov(1998),Tetreault(2001)).Whiletheseapproacheshavebeenreasonablysuccessful(seeMitkov(2002)),Kehleretal.(2004)speculatethatdeeperlinguisticknowledgeneedstobemadeavail-abletoresolversinordertoreachthenextlevelofperformance.Infact,semanticsplaysacruciallyim-portantroleintheresolutionofcommonNPs,allow-ingustoidentifythecoreferencerelationbetweentwolexicallydissimilarcommonnouns(e.g.,talksandnegotiations)andtoeliminateGeorgeW.Bushfromthelistofcandidateantecedentsofthecity,forinstance.Asaresult,researchershavere-adoptedtheonce-popularknowledge-richapproach,investi-gatingavarietyofsemanticknowledgesourcesforcommonnounresolution,suchasthesemanticrela-tionsbetweentwoNPs(e.g.,Jietal.(2005)),theirsemanticsimilarityascomputedusingWordNet(e.g.,Poesioetal.(2004))orWikipedia(PonzettoandStrube,2006),andthecontextualroleplayedbyanNP(seeBeanandRiloff(2004)).Anothertypeofsemanticknowledgethathasbeenemployedbycoreferenceresolversisthese-manticclass(SC)ofanNP,whichcanbeusedtodis-allowcoreferencebetweensemanticallyincompat-ibleNPs.However,learning-basedresolvershavenotbeenabletobene(cid:2)tfromhavinganSCagree-mentfeature,presumablybecausethemethodusedtocomputetheSCofanNPistoosimplistic:whiletheSCofapropernameiscomputedfairlyaccu-ratelyusinganamedentity(NE)recognizer,manyresolverssimplyassigntoacommonnounthe(cid:2)rst(i.e.,mostfrequent)WordNetsenseasitsSC(e.g.,Soonetal.(2001),MarkertandNissim(2005)).Itisnoteasytomeasuretheaccuracyofthisheuristic,butthefactthattheSCagreementfeatureisnotusedbySoonetal.’sdecisiontreecoreferenceclassi(cid:2)erseemstosuggestthattheSCvaluesoftheNPsarenotcomputedaccuratelybythis(cid:2)rst-senseheuristic.Motivatedinpartbythisobservation,weexam-inewhetherautomaticallyinducedsemanticclassknowledgecanimprovetheperformanceofalearning-basedcoreferenceresolver,reportingeval-uationresultsonthecommonly-usedACEcorefer-537

encecorpus.Ourinvestigationproceedsasfollows.TrainaclassiﬁerforlabelingtheSCofanNP.InACE,weareprimarilyconcernedwithclassify-inganNPasbelongingtooneoftheACEseman-ticclasses.Forinstance,partoftheACEPhase2evaluationinvolvesclassifyinganNPasPERSON,ORGANIZATION,GPE(ageographical-politicalre-gion),FACILITY,LOCATION,orOTHERS.Weadoptacorpus-basedapproachtoSCdetermination,re-castingtheproblemasasix-classclassi(cid:2)cationtask.DerivetwoknowledgesourcesforcoreferenceresolutionfromtheinducedSCs.The(cid:2)rstknowledgesource(KS)issemanticclassagreement(SCA).FollowingSoonetal.(2001),werepresentSCAasabinaryvaluethatindicateswhetherthein-ducedSCsofthetwoNPsinvolvedarethesameornot.ThesecondKSismention,whichisrepresentedasabinaryvaluethatindicateswhetheranNPbe-longstooneofthe(cid:2)veACESCsmentionedabove.Hence,thementionvalueofanNPcanbereadilyderivedfromitsinducedSC:thevalueisNOifitsSCisOTHERS,andYESotherwise.ThisKScouldbeusefulforACEcoreference,sinceACEiscon-cernedwithresolvingonlyNPsthatarementions.Incorporatethetwoknowledgesourcesinacoreferenceresolver.Next,weinvestigatewhetherthesetwoKSscanimprovealearning-basedbase-lineresolverthatemploysafairlystandardfeatureset.Since(1)thetwoKSscaneachberepre-sentedintheresolverasaconstraint(for(cid:2)lteringnon-mentionsordisallowingcoreferencebetweensemanticallyincompatibleNPs)orasafeature,and(2)theycanbeappliedtotheresolverinisolationorincombination,wehaveeightwaysofincorporatingtheseKSsintothebaselineresolver.InourexperimentsontheACEPhase2coref-erencecorpus,wefoundthat(1)ourSCinduc-tionmethodyieldsasigni(cid:2)cantimprovementof2%inaccuracyoverSoonetal.’s(cid:2)rst-senseheuristicmethodasdescribedabove;(2)thecoreferencere-solverthatincorporatesourinducedSCknowledgebymeansofthetwoKSsmentionedaboveyieldsasigni(cid:2)cantimprovementof2%inF-measureovertheresolverthatexploitstheSCknowledgecom-putedbySoonetal.’smethod;(3)thementionKS,whenusedinthebaselineresolverasaconstraint,improvestheresolverbyapproximately5-7%inF-measure;and(4)SCA,whenemployedasafeaturebythebaselineresolver,improvestheaccuracyofcommonnounresolutionbyabout5-8%.2RelatedWorkMentiondetection.ManyACEparticipantshavealsoadoptedacorpus-basedapproachtoSCdeter-minationthatisinvestigatedaspartofthementiondetection(MD)task(e.g.,Florianetal.(2006)).Brie(cid:3)y,thegoalofMDistoidentifytheboundaryofamention,itsmentiontype(e.g.,pronoun,name),anditssemantictype(e.g.,person,location).Un-likethem,(1)wedonotperformthefullMDtask,asourgoalistoinvestigatetheroleofSCknowl-edgeincoreferenceresolution;and(2)wedonotusetheACEtrainingdataforacquiringourSCclas-si(cid:2)er;instead,weusetheBBNEntityTypeCorpus(WeischedelandBrunstein,2005),whichconsistsofallthePennTreebankWallStreetJournalarticleswiththeACEmentionsmanuallyidenti(cid:2)edandan-notatedwiththeirSCs.Thisprovidesuswithatrain-ingsetthatisapproximately(cid:2)vetimesbiggerthanthatofACE.Moreimportantly,theACEparticipantsdonotevaluatetheroleofinducedSCknowledgeincoreferenceresolution:manyofthemevaluatecoreferenceperformanceonperfectmentions(e.g.,Luoetal.(2004));andforthosethatdoreportper-formanceonautomaticallyextractedmentions,theydonotexplainwhetherorhowtheinducedSCinfor-mationisusedintheircoreferencealgorithms.Jointprobabilisticmodelsofcoreference.Re-cently,therehasbeenasurgeofinterestinim-provingcoreferenceresolutionbyjointlymodelingcoreferencewitharelatedtasksuchasMD(e.g.,Daum·eandMarcu(2005)).However,jointmodelstypicallyneedtobetrainedondatathatissimulta-neouslyannotatedwithinformationrequiredbyalloftheunderlyingmodels.Forinstance,Daum·eandMarcu’smodelassumesasinputacorpusannotatedwithbothMDandcoreferenceinformation.Ontheotherhand,wetacklecoreferenceandSCinductionseparately(ratherthanjointly),sincewetrainourSCdeterminationmodelontheBBNEntityTypeCor-pus,wherecoreferenceinformationisabsent.3SemanticClassInductionThissectiondescribeshowwetrainandevaluateaclassi(cid:2)erfordeterminingtheSCofanNP.538

3.1TrainingtheClassiﬁerTrainingcorpus.Asmentionedbefore,weusetheBBNEntityTypeCorpusfortrainingtheSCclassi(cid:2)er.ThiscorpuswasoriginallydevelopedtosupporttheACEandAQUAINTprogramsandcon-sistsofannotationsof12namedentitytypesandninenominalentitytypes.Nevertheless,wewillonlymakeuseoftheannotationsofthe(cid:2)veACEsemantictypesthatarepresentinourACEPhase2coreferencecorpus,namely,PERSON,ORGANIZA-TION,GPE,FACILITY,andLOCATION.Traininginstancecreation.Wecreateonetrain-inginstanceforeachproperorcommonNP(ex-tractedusinganNPchunkerandanNErecognizer)ineachtrainingtext.Eachinstanceisrepresentedbyasetoflexical,syntactic,andsemanticfeatures,asdescribedbelow.IftheNPunderconsiderationisannotatedasoneofthe(cid:2)veACESCsinthecorpus,thentheclassi(cid:2)cationoftheassociatedtrainingin-stanceissimplytheACESCvalueoftheNP.Other-wise,theinstanceislabeledasOTHERS.Thisresultsin310063instancesinthetrainingset.Features.Werepresentthetraininginstanceforanounphrase,NPi,usingseventypesoffeatures:(1)WORD:ForeachwordwinNPi,wecreateaWORDfeaturewhosevalueisequaltow.Nofea-turesarecreatedfromstopwords,however.(2)SUBJVERB:IfNPiisinvolvedinasubject-verbrelation,wecreateaSUBJVERBfeaturewhosevalueistheverbparticipatingintherelation.WeuseLin’s(1998b)MINIPARdependencyparsertoextractgrammaticalrelations.Ourmotivationhereistocoarselymodelsubcategorization.(3)VERBOBJ:AVERBOBJfeatureiscreatedinasimilarfashionasSUBJVERBifNPiparticipatesinaverb-objectrelation.Again,thisrepresentsourattempttocoarselymodelsubcategorization.(4)NE:WeuseBBN’sIdentiFinder(Bikeletal.,1999),aMUC-styleNErecognizertodeterminetheNEtypeofNPi.IfNPiisdeterminedtobeaPERSONorORGANIZATION,wecreateanNEfeaturewhosevalueissimplyitsMUCNEtype.However,ifNPiisdeterminedtobeaLOCATION,wecreateafeaturewithvalueGPE(becausemostoftheMUCLOCA-TIONNEsareACEGPENEs).Otherwise,noNEfeaturewillbecreated(becausewearenotinterestedintheotherMUCNEtypes).ACESCKeywordsPERSONpersonORGANIZATIONsocialgroupFACILITYestablishment,construction,building,facil-ity,workplaceGPEcountry,province,government,town,city,administration,society,island,communityLOCATIONdryland,region,landmass,bodyofwater,geographicalarea,geologicalformationTable1:ListofkeywordsusedinWordNetsearchforgeneratingWNCLASSfeatures.(5)WNCLASS:ForeachkeywordwshownintherightcolumnofTable1,wedeterminewhethertheheadnounofNPiisahyponymofwinWordNet,usingonlythe(cid:2)rstWordNetsenseofNPi.1Ifso,wecreateaWNCLASSfeaturewithwasitsvalue.Thesekeywordsarepotentiallyusefulfeaturesbe-causesomeofthemaresubclassesoftheACESCsshownintheleftcolumnofTable1,whileothersappeartobecorrelatedwiththeseACESCs.2(6)INDUCEDCLASS:Sincethe(cid:2)rst-senseheuris-ticusedinthepreviousfeaturemaynotbeaccurateincapturingtheSCofanNP,weemployacorpus-basedmethodforinducingSCsthatismotivatedbyresearchinlexicalsemantics(e.g.,Hearst(1992)).Givenalarge,unannotatedcorpus3,weuseIdenti-FindertolabeleachNEwithitsNEtypeandMINI-PARtoextractalltheappositiverelations.Anex-ampleextractionwouldbe<EasternAirlines,thecarrier>,wherethe(cid:2)rstentryisapropernounla-beledwitheitheroneofthesevenMUC-styleNEtypes4orOTHERS5andthesecondentryisacom-monnoun.WetheninfertheSCofacommonnounasfollows:(1)wecomputetheprobabilitythatthecommonnounco-occurswitheachoftheeightNEtypes6basedontheextractedappositiverelations,and(2)ifthemostlikelyNEtypehasaco-occurrenceprobabilityaboveacertainthreshold(wesetitto0.7),wecreateaINDUCEDCLASSfea-1ThisismotivatedbyLin’s(1998c)observationthatacoref-erenceresolverthatemploysonlythe(cid:2)rstWordNetsenseper-formsslightlybetterthanonethatemploysmorethanonesense.2ThekeywordsareobtainedviaourexperimentationwithWordNetandtheACESCsoftheNPsintheACEtrainingdata.3Weused(1)theBLLIPcorpus(30Mwords),whichcon-sistsofWSJarticlesfrom1987to1989,and(2)theReutersCorpus(3.7GBdata),whichhas806,791Reutersarticles.4Person,organization,location,date,time,money,percent.5ThisindicatesthepropernounisnotaMUCNE.6Forsimplicity,OTHERSisviewedasanNEtypehere.539

tureforNPiwhosevalueisthemostlikelyNEtype.(7)NEIGHBOR:Researchinlexicalsemanticssug-geststhattheSCofanNPcanbeinferredfromitsdistributionallysimilarNPs(seeLin(1998a)).Mo-tivatedbythisobservation,wecreateforeachofNPi’stenmostsemanticallysimilarNPsaNEIGH-BORfeaturewhosevalueisthesurfacestringoftheNP.Todeterminethetennearestneighbors,weusethesemanticsimilarityvaluesprovidedbyLin’sdependency-basedthesaurus,whichisconstructedusingadistributionalapproachcombinedwithaninformation-theoreticde(cid:2)nitionofsimilarity.Learningalgorithms.Weexperimentwithfourlearnerscommonlyemployedinlanguagelearning:DecisionList(DL):WeusetheDLlearnerasde-scribedinCollinsandSinger(1999),motivatedbyitssuccessintherelatedtasksofwordsensedis-ambiguation(Yarowsky,1995)andNEclassi(cid:2)ca-tion(CollinsandSinger,1999).Weapplyadd-onesmoothingtosmooththeclassposteriors.1-NearestNeighbor(1-NN):Weusethe1-NNclas-si(cid:2)erasimplementedinTiMBL(Daelemansetal.,2004),employingdotproductasthesimilarityfunc-tion(whichde(cid:2)nessimilarityasthenumberofcom-monfeature-valuepairsbetweentwoinstances).Allotherparametersaresettotheirdefaultvalues.MaximumEntropy(ME):WeemployLin’sMEimplementation7,usingaGaussianpriorforsmooth-ingandrunningthealgorithmuntilconvergence.NaiveBayes(NB):Weuseanin-houseimplementa-tionofNB,usingadd-onesmoothingtosmooththeclasspriorsandtheclass-conditionalprobabilities.Inaddition,wetrainanSVMclassi(cid:2)erforSCdeterminationbycombiningtheoutputof(cid:2)veclas-si(cid:2)cationmethods:DL,1-NN,ME,NB,andSoonetal.’smethodasdescribedintheintroduction,8withthegoalofexaminingwhetherSCclassi(cid:2)ca-tionaccuracycanbeimprovedbycombiningtheoutputofindividualclassi(cid:2)ersinasupervisedman-ner.Speci(cid:2)cally,we(1)use80%oftheinstancesgeneratedfromtheBBNEntityTypeCorpustotrainthefourclassi(cid:2)ers;(2)applythefourclassi(cid:2)ersandSoonetal.’smethodtoindependentlymakepredic-7Seehttp://www.cs.ualberta.ca/∼lindek/downloads.htm8InourimplementationofSoon’smethod,welabelanin-stanceasOTHERSifnoNEorWNCLASSfeatureisgenerated;otherwiseitslabelisthevalueoftheNEfeatureortheACESCthathastheWNCLASSfeaturesasitskeywords(seeTable1).PERORGGPEFACLOCOTHTraining19.89.611.41.61.256.3Test19.59.09.61.81.159.0Table2:DistributionofSCsintheACEcorpus.tionsfortheremaining20%oftheinstances;and(3)trainanSVMclassi(cid:2)er(usingtheLIBSVMpack-age(ChangandLin,2001))onthese20%ofthein-stances,whereeachinstance,i,isrepresentedbyasetof31binaryfeatures.Morespeci(cid:2)cally,letLi={li1,li2,li3,li4,li5}bethesetofpredictionsthatweobtainedforiinstep(2).Torepresenti,wegenerateonefeaturefromeachnon-emptysubsetofLi.3.2EvaluatingtheClassiﬁersForevaluation,weusetheACEPhase2coreferencecorpus,whichcomprises422trainingtextsand97testtexts.EachtexthasitsmentionsannotatedwiththeirACESCs.WecreateourtestinstancesfromtheACEtextsinthesamewayasthetrainingin-stancesdescribedinSection3.1.Table2showsthepercentagesofinstancescorrespondingtoeachSC.Table3showstheaccuracyofeachclassi(cid:2)er(seerow1)fortheACEtrainingset(54641NPs,with16414properNPsand38227commonNPs)andtheACEtestset(13444NPs,with3713properNPsand9731commonNPs),aswellastheirperformanceontheproperNPs(row2)andthecommonNPs(row3).WeemployasourbaselinesystemtheSoonetal.method(seeFootnote8),whoseaccuracyisshownundertheSooncolumn.Aswecansee,DL,1-NN,andSVMshowastatisticallysigni(cid:2)cantimprove-mentoverthebaselineforbothdatasets,whereasMEandNBperformsigni(cid:2)cantlyworse.9Addi-tionalexperimentsareneededtodeterminetherea-sonforMEandNB’spoorperformance.Inanattempttogainadditionalinsightintotheperformancecontributionofeachtypeoffeatures,weconductfeatureablationexperimentsusingtheDLclassi(cid:2)er(DLischosensimplybecauseitisthebestperformerontheACEtrainingset).ResultsareshowninTable4,whereeachrowshowstheaccu-racyoftheDLtrainedonalltypesoffeaturesexceptfortheoneshowninthatrow(All),aswellasaccu-raciesontheproperNPs(PN)andthecommonNPs(CN).Foreasyreference,theaccuracyoftheDL9WeuseNoreen’s(1989)ApproximateRandomizationtestforsigni(cid:2)cancetesting,withpsetto.05unlessotherwisestated.540

TrainingSetTestSetSoonDL1-NNMENBSVMSoonDL1-NNMENBSVM1Overall83.185.084.054.571.384.281.182.983.153.070.383.32ProperNPs83.184.181.054.265.582.279.682.079.855.864.480.43CommonNPs83.185.485.254.673.885.181.683.384.351.972.684.4Table3:SCclassi(cid:2)cationaccuraciesofdifferentmethodsfortheACEtrainingsetandtestset.TrainingSetTestSetFeatureTypePNCNAllPNCNAllAllfeatures84.185.485.082.083.382.9-WORD84.285.485.082.083.182.8-SUBJVERB84.185.485.082.083.382.9-VERBOBJ84.185.485.082.083.382.9-NE72.985.381.674.183.280.7-WNCLASS84.185.985.381.984.183.5-INDUCEDC84.085.685.182.083.683.2-NEIGHBOR82.884.984.380.282.982.1Table4:Resultsforfeatureablationexperiments.TrainingSetTestSetFeatureTypePNCNAllPNCNAllWORD64.083.977.966.582.478.0SUBJVERB24.070.256.328.870.559.0VERBOBJ24.070.256.328.870.559.0NE81.172.174.878.471.473.3WNCLASS25.678.862.830.478.965.5INDUCEDC25.881.164.530.080.366.3NEIGHBOR67.785.880.468.084.479.8Table5:Accuraciesofsingle-featureclassi(cid:2)ers.classi(cid:2)ertrainedonalltypesoffeaturesisshowninrow1ofthetable.Aswecansee,accuracydropssigni(cid:2)cantlywiththeremovalofNEandNEIGHBOR.Asexpected,removingNEprecipitatesalargedropinproperNPaccuracy;somewhatsurprisingly,re-movingNEIGHBORalsocausesproperNPaccuracytodropsigni(cid:2)cantly.Toourknowledge,therearenopriorresultsonusingdistributionallysimilarneigh-borsasfeaturesforsupervisedSCinduction.Note,however,thattheseresultsdonotimplythattheremainingfeaturetypesarenotusefulforSCclassi(cid:2)cation;theysimplysuggest,forinstance,thatWORDisnotimportantinthepresenceofotherfeaturetypes.Togetabetterideaoftheutilityofeachfeaturetype,weconductanotherexperimentinwhichwetrainsevenclassi(cid:2)ers,eachofwhichem-ploysexactlyonetypeoffeatures.Theaccuraciesoftheseclassi(cid:2)ersareshowninTable5.Aswecansee,NEIGHBORhasthelargestcontribution.Thisagaindemonstratestheeffectivenessofadistribu-tionalapproachtosemanticsimilarity.ItssuperiorperformancetoWORD,thesecondlargestcontribu-tor,couldbeattributedtoitsabilitytocombatdatasparseness.TheNEfeature,asexpected,iscrucialtotheclassi(cid:2)cationofproperNPs.4ApplicationtoCoreferenceResolutionWecannowderivefromtheinducedSCinforma-tiontwoKSs(cid:151)semanticclassagreementandmen-tion(cid:151)andincorporatethemintoourlearning-basedcoreferenceresolverineightdifferentways,asde-scribedintheintroduction.Thissectionexamineswhetherourcoreferenceresolvercanbene(cid:2)tfromanyoftheeightwaysofincorporatingtheseKSs.4.1ExperimentalSetupAsinSCinduction,weusetheACEPhase2coref-erencecorpusforevaluationpurposes,acquiringthecoreferenceclassi(cid:2)ersonthe422trainingtextsandevaluatingtheiroutputonthe97testtexts.Were-portperformanceintermsoftwometrics:(1)theF-measurescoreascomputedbythecommonly-usedMUCscorer(Vilainetal.,1995),and(2)theaccu-racyontheanaphoricreferences,computedasthefractionofanaphoricreferencescorrectlyresolved.FollowingPonzettoandStrube(2006),weconsiderananaphoricreference,NPi,correctlyresolvedifNPianditsclosestantecedentareinthesamecorefer-encechainintheresultingpartition.Inallofourexperiments,weuseNPsautomaticallyextractedbyanin-houseNPchunkerandIdentiFinder.4.2TheBaselineCoreferenceSystemOurbaselinecoreferencesystemusestheC4.5deci-siontreelearner(Quinlan,1993)toacquireaclassi-(cid:2)eronthetrainingtextsfordeterminingwhethertwoNPsarecoreferent.Followingpreviouswork(e.g.,Soonetal.(2001)andPonzettoandStrube(2006)),wegeneratetraininginstancesasfollows:apositiveinstanceiscreatedforeachanaphoricNP,NPj,anditsclosestantecedent,NPi;andanegativeinstanceiscreatedforNPjpairedwitheachoftheinterveningNPs,NPi+1,NPi+2,...,NPj−1.Eachinstanceisrep-resentedby33lexical,grammatical,semantic,and541

positionalfeaturesthathavebeenemployedbyhigh-performingresolverssuchasNgandCardie(2002)andYangetal.(2003),asdescribedbelow.Lexicalfeatures.NinefeaturesallowdifferenttypesofstringmatchingoperationstobeperformedonthegivenpairofNPs,NPxandNPy10,including(1)exactstringmatchforpronouns,propernouns,andnon-pronominalNPs(bothbeforeandafterde-terminersareremoved);(2)substringmatchforpropernounsandnon-pronominalNPs;and(3)headnounmatch.Inaddition,onefeaturetestswhetherallthewordsthatappearinoneNPalsoappearintheotherNP.Finally,anationalitymatchingfeatureisusedtomatch,forinstance,BritishwithBritain.Grammaticalfeatures.22featurestestthegram-maticalpropertiesofoneorbothoftheNPs.TheseincludetenfeaturesthattestwhethereachofthetwoNPsisapronoun,ade(cid:2)niteNP,aninde(cid:2)niteNP,anestedNP,andaclausalsubject.Asimilarsetof(cid:2)vefeaturesisusedtotestwhetherbothNPsarepronouns,de(cid:2)niteNPs,nestedNPs,propernouns,andclausalsubjects.Inaddition,(cid:2)vefeaturesdeter-minewhetherthetwoNPsarecompatiblewithre-specttogender,number,animacy,andgrammaticalrole.Furthermore,twofeaturestestwhetherthetwoNPsareinappositionorparticipateinapredicatenominalconstruction(i.e.,theIS-Arelation).Semanticfeatures.MotivatedbySoonetal.(2001),wehaveasemanticfeaturethattestswhetheroneNPisanamealiasoracronymoftheother.Positionalfeature.Wehaveafeaturethatcom-putesthedistancebetweenthetwoNPsinsentences.Aftertraining,thedecisiontreeclassi(cid:2)erisusedtoselectanantecedentforeachNPinatesttext.FollowingSoonetal.(2001),weselectasthean-tecedentofeachNP,NPj,theclosestprecedingNPthatisclassi(cid:2)edascoreferentwithNPj.IfnosuchNPexists,noantecedentisselectedforNPj.Row1ofTable6andTable7showstheresultsofthebaselinesystemintermsofF-measure(F)andaccuracyinresolving4599anaphoricreferences(All),respectively.Forfurtheranalysis,wealsore-portthecorrespondingrecall(R)andprecision(P)inTable6,aswellastheaccuraciesofthesysteminresolving1769pronouns(PRO),1675properNPs(PN),and1155commonNPs(CN)inTable7.As10WeassumethatNPxprecedesNPyintheassociatedtext.wecansee,thebaselineachievesanF-measureof57.0andaresolutionaccuracyof48.4.Togetabettersenseofhowstrongourbaselineis,were-implementtheSoonetal.(2001)corefer-enceresolver.Thissimplyamountstoreplacingthe33featuresinthebaselineresolverwiththe12fea-turesemployedbySoonetal.’ssystem.ResultsofourDuplicatedSoonetal.systemareshowninrow2ofTables6and7.Incomparisontoourbaseline,theDuplicatedSoonetal.systemperformsworseaccordingtobothmetrics,andalthoughthedropinF-measureseemsmoderate,theperformancediffer-enceisinfacthighlysigni(cid:2)cant(p=0.002).114.3CoreferencewithInducedSCKnowledgeRecallfromtheintroductionthatourinvestigationoftheroleofinducedSCknowledgeinlearning-basedcoreferenceresolutionproceedsinthreesteps:LabeltheSCofeachNPineachACEdocument.Ifanounphrase,NPi,isaproperorcommonNP,thenitsSCvalueisdeterminedusinganSCclassi-(cid:2)erthatweacquiredinSection3.Ontheotherhand,ifNPiisapronoun,thenwewillbeconservativeandposititsSCvalueasUNCONSTRAINED(i.e.,itisse-manticallycompatiblewithallotherNPs).12DerivetwoKSsfromtheinducedSCs.Recallthatour(cid:2)rstKS,Mention,isde(cid:2)nedonanNP;itsvalueisYESiftheinducedSCoftheNPisnotOTHERS,andNOotherwise.Ontheotherhand,oursecondKS,SCA,isde(cid:2)nedonapairofNPs;itsvalueisYESifthetwoNPshavethesameinducedSCthatisnotOTHERS,andNOotherwise.IncorporatethetwoKSsintothebaselinere-solver.Recallthatthereareeightwaysofincor-poratingthesetwoKSsintoourresolver:theycaneachberepresentedasaconstraintorasafeature,andtheycanbeappliedtotheresolverinisolationandincombination.Constraintsareapplieddur-ingtheantecedentselectionstep.Speci(cid:2)cally,whenemployedasaconstraint,theMentionKSdisallowscoreferencebetweentwoNPsifatleastoneofthemhasaMentionvalueofNO,whereastheSCAKSdis-allowscoreferenceiftheSCAvalueofthetwoNPsinvolvedisNO.Whenencodedasafeaturefortheresolver,theMentionfeatureforanNPpairhasthe11Again,weuseApproximateRandomizationwithp=.05.12TheonlyexceptionispronounswhoseSCvaluecanbeeas-ilydeterminedtobePERSON(e.g.,he,him,his,himself).542

SystemVariationRPFRPFRPFRPF1Baselinesystem60.953.657.0(cid:150)(cid:150)(cid:150)(cid:150)(cid:150)(cid:150)(cid:150)(cid:150)(cid:150)2DuplicatedSoonetal.56.154.455.3(cid:150)(cid:150)(cid:150)(cid:150)(cid:150)(cid:150)(cid:150)(cid:150)(cid:150)AddtotheBaselineSoon’sSCMethodDecisionListSVMPerfectInformation3Mention(C)only56.969.762.659.570.664.659.570.764.661.283.170.54Mention(F)only60.954.057.261.252.956.760.953.657.062.333.743.85SCA(C)only56.470.062.557.771.263.758.970.764.361.386.171.66SCA(F)only62.052.857.062.553.557.663.053.357.771.133.045.17Mention(C)+SCA(C)56.470.062.557.771.263.758.970.864.361.386.171.68Mention(C)+SCA(F)58.266.462.060.966.863.761.466.563.871.176.773.89Mention(F)+SCA(C)56.469.862.457.771.363.858.970.664.362.785.372.310Mention(F)+SCA(F)62.052.757.062.652.857.363.252.657.471.830.342.6Table6:CoreferenceresultsobtainedviatheMUCscoringprogramfortheACEtestset.SystemVariationPROPNCNAllPROPNCNAllPROPNCNAll1Baselinesystem59.254.822.548.4(cid:150)(cid:150)(cid:150)(cid:150)(cid:150)(cid:150)(cid:150)(cid:150)2DuplicatedSoonetal.53.445.716.941.4(cid:150)(cid:150)(cid:150)(cid:150)(cid:150)(cid:150)(cid:150)(cid:150)AddtotheBaselineSoon’sSCMethodDecisionListSVM3Mention(C)only58.551.316.545.359.154.120.247.559.153.920.647.54Mention(F)only59.255.022.548.559.256.122.448.859.455.222.648.65SCA(C)only58.150.116.444.758.151.817.145.558.552.019.646.36SCA(F)only59.254.927.849.760.456.730.151.560.856.429.451.37Mention(C)+SCA(C)58.150.116.444.758.151.817.145.558.551.919.546.38Mention(C)+SCA(F)58.952.022.347.260.255.928.150.660.755.327.450.49Mention(F)+SCA(C)58.150.316.344.858.152.416.745.658.652.419.746.610Mention(F)+SCA(F)59.255.027.649.760.456.830.151.560.856.529.551.4Table7:ResolutionaccuraciesfortheACEtestset.valueYESifandonlyiftheMentionvalueforbothNPsisYES,whereastheSCAfeatureforanNPpairhasitsvaluetakenfromtheSCAKS.Now,wecanevaluatetheimpactofthetwoKSsontheperformanceofourbaselineresolver.Speci(cid:2)-cally,rows3-6ofTables6and7showtheF-measureandtheresolutionaccuracy,respectively,whenex-actlyoneofthetwoKSsisemployedbythebaselineaseitheraconstraint(C)orafeature(F),androws7-10ofthetwotablesshowtheresultswhenbothKSsareappliedtothebaseline.Furthermore,eachrowofTable6containsfoursetsofresults,eachofwhichcorrespondstoadifferentmethodfordeter-miningtheSCvalueofanNP.Forinstance,the(cid:2)rstsetisobtainedbyusingSoonetal.’smethodasde-scribedinFootnote8tocomputeSCvalues,servingassortofabaselineforourresultsusinginducedSCvalues.ThesecondandthirdsetsareobtainedbasedontheSCvaluescomputedbytheDLandtheSVMclassi(cid:2)er,respectively.13Thelastsetcorrespondstoanoracleexperimentinwhichtheresolverhasac-cesstoperfectSCinformation.Rows3-10ofTable13Resultsusingotherlearnersarenotshownduetospacelim-itations.DLandSVMarechosensimplybecausetheyachievethehighestSCclassi(cid:2)cationaccuraciesontheACEtrainingset.7canbeinterpretedinasimilarmanner.FromTable6,wecanseethat(1)incomparisontothebaseline,F-measureincreasessigni(cid:2)cantlyinthe(cid:2)vecaseswhereatleastoneoftheKSsisemployedasaconstraintbytheresolver,andsuchimprove-mentsstemmainlyfromsigni(cid:2)cantgainsinpreci-sion;(2)inthese(cid:2)vecases,theresolversthatuseSCsinducedbyDLandSVMachievesigni(cid:2)cantlyhigherF-measurescoresthantheircounterpartsthatrelyonSoon’smethodforSCdetermination;and(3)noneoftheresolversappearstobene(cid:2)tfromSCAin-formationwhenevermentionisusedasaconstraint.Moreover,notethatevenwithperfectlycomputedSCinformation,theperformanceofthebaselinesys-temdoesnotimprovewhenneitherMDnorSCAisemployedasaconstraint.Theseresultsprovidefur-therevidencethatthedecisiontreelearnerisnotex-ploitingthesetwosemanticKSsinanoptimalman-ner,whethertheyarecomputedautomaticallyorper-fectly.Hence,inmachinelearningforcoreferenceresolution,itisimportanttodeterminenotonlywhatlinguisticKSstouse,butalsohowtousethem.WhilethecoreferenceresultsinTable6seemtosuggestthatSCAandmentionshouldbeemployedasconstraints,theresolutionresultsinTable7sug-543

gestthatSCAisbetterencodedasafeature.Speci(cid:2)-cally,(1)incomparisontothebaseline,theaccuracyofcommonNPresolutionimprovesbyabout5-8%whenSCAisencodedasafeature;and(2)wheneverSCAisemployedasafeature,theoverallresolutionaccuracyissigni(cid:2)cantlyhigherforresolversthatuseSCsinducedbyDLandSVMthanthosethatrelyonSoon’smethodforSCdetermination,withimprove-mentsinresolutionobservedonallthreeNPtypes.Overall,theseresultsprovidesuggestiveevidencethatbothKSsareusefulforlearning-basedcorefer-enceresolution.Inparticular,mentionshouldbeem-ployedasaconstraint,whereasSCAshouldbeusedasafeature.Interestingly,thisisconsistentwiththeresultsthatweobtainedwhentheresolverhasaccesstoperfectSCinformation(seeTable6),wherethehighestF-measureisachievedbyemployingmen-tionasaconstraintandSCAasafeature.5ConclusionsWehaveshownthat(1)bothmentionandSCAcanbeusefullyemployedtoimprovetheperformanceofalearning-basedcoreferencesystem,and(2)em-ployingSCknowledgeinducedinasupervisedman-nerenablesaresolvertoachievebetterperformancethanemployingSCknowledgecomputedbySoonetal.’ssimplemethod.Inaddition,wefoundthattheMUCscoringprogramisunabletorevealtheusefulnessoftheSCAKS,which,whenencodedasafeature,substantiallyimprovestheaccuracyofcommonNPresolution.Thisunderscorestheim-portanceofreportingbothresolutionaccuracyandclustering-levelaccuracywhenanalyzingtheperfor-manceofacoreferenceresolver.ReferencesD.BeanandE.Riloff.2004.Unsupervisedlearningofcontex-tualroleknowledgeforcoreferenceresolution.InProc.ofHLT/NAACL,pages297(cid:150)304.D.M.Bikel,R.Schwartz,andR.M.Weischedel.1999.Analgorithmthatlearnswhat’sinaname.MachineLearning34(1(cid:150)3):211(cid:150)231.C.-C.ChangandC.-J.Lin,2001.LIBSVM:alibraryforsupportvectormachines.Softwareavailableathttp://www.csie.ntu.edu.tw/∼cjlin/libsvm.M.CollinsandY.Singer.1999.Unsupervisedmodelsfornamedentityclassi(cid:2)cation.InProc.ofEMNLP/VLC.W.Daelemans,J.Zavrel,K.vanderSloot,andA.vandenBosch.2004.TiMBL:TilburgMemoryBasedLearner,ver-sion5.1,ReferenceGuide.ILKTechnicalReport.H.Daum·eIIIandD.Marcu.2005.Alarge-scaleexplorationofeffectiveglobalfeaturesforajointentitydetectionandtrackingmodel.InProc.ofHLT/EMNLP,pages97(cid:150)104.R.Florian,H.Jing,N.Kambhatla,andI.Zitouni.2006.Fac-torizingcomplexmodels:Acasestudyinmentiondetection.InProc.ofCOLING/ACL,pages473(cid:150)480.M.Hearst.1992.Automaticacquisitionofhyponymsfromlargetextcorpora.InProc.ofCOLING.H.Ji,D.Westbrook,andR.Grishman.2005.Usingseman-ticrelationstore(cid:2)necoreferencedecisions.InProc.ofHLT/EMNLP,pages17(cid:150)24.A.Kehler,D.Appelt,L.Taylor,andA.Simma.2004.The(non)utilityofpredicate-argumentfrequenciesforpronouninterpretation.InProc.ofNAACL,pages289(cid:150)296.D.Lin.1998a.Automaticretrievalandclusteringofsimilarwords.InProc.ofCOLING/ACL,pages768(cid:150)774.D.Lin.1998b.Dependency-basedevaluationofMINIPAR.InProc.oftheLRECWorkshopontheEvaluationofParsingSystems,pages48(cid:150)56.D.Lin.1998c.Usingcollocationstatisticsininformationex-traction.InProc.ofMUC-7.X.Luo,A.Ittycheriah,H.Jing,N.Kambhatla,andS.Roukos.2004.Amention-synchronouscoreferenceresolutionalgo-rithmbasedontheBelltree.InProc.oftheACL.K.MarkertandM.Nissim.2005.Comparingknowledgesourcesfornominalanaphoraresolution.ComputationalLinguistics,31(3):367(cid:150)402.R.Mitkov.2002.AnaphoraResolution.Longman.R.Mitkov.1998.Robustpronounresolutionwithlimitedknowledge.InProc.ofCOLING/ACL,pages869(cid:150)875.V.NgandC.Cardie.2002.Improvingmachinelearningap-proachestocoreferenceresolution.InProc.oftheACL.E.W.Noreen.1989.ComputerIntensiveMethodsforTestingHypothesis:AnIntroduction.JohnWiley&Sons.M.Poesio,R.Mehta,A.Maroudas,andJ.Hitzeman.2004.Learningtoresolvebridgingreferences.InProc.oftheACL.S.P.PonzettoandM.Strube.2006.Exploitingsemanticrolelabeling,WordNetandWikipediaforcoreferenceresolution.InProc.ofHLT/NAACL,pages192(cid:150)199.J.R.Quinlan.1993.C4.5:ProgramsforMachineLearning.MorganKaufmann,SanMateo,CA.W.M.Soon,H.T.Ng,andD.Lim.2001.Amachinelearningapproachtocoreferenceresolutionofnounphrases.Compu-tationalLinguistics,27(4):521(cid:150)544.J.Tetreault.2001.Acorpus-basedevaluationofcenteringandpronounresolution.ComputationalLinguistics,27(4).M.Vilain,J.Burger,J.Aberdeen,D.Connolly,andL.Hirschman.1995.Amodel-theoreticcoreferencescor-ingscheme.InProc.ofMUC-6,pages45(cid:150)52.R.WeischedelandA.Brunstein.2005.BBNpronouncorefer-enceandentitytypecorpus.LinguisticaDataConsortium.X.Yang,G.Zhou,J.Su,andC.L.Tan.2003.Coreferenceresolutionusingcompetitivelearningapproach.InProc.oftheACL,pages176(cid:150)183.D.Yarowsky.1995.Unsupervisedwordsensedisambiguationrivalingsupervisedmethods.InProc.oftheACL.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 544–551,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

544

GeneratingaTable-of-ContentsS.R.K.Branavan,PawanDeshpandeandReginaBarzilayMassachusettsInstituteofTechnology{branavan,pawand,regina}@csail.mit.eduAbstractThispaperpresentsamethodfortheauto-maticgenerationofatable-of-contents.Thistypeofsummarycouldserveasaneffec-tivenavigationtoolforaccessinginforma-tioninlongtexts,suchasbooks.Togen-erateacoherenttable-of-contents,weneedtocapturebothglobaldependenciesacrossdifferenttitlesinthetableandlocalcon-straintswithinsections.Ouralgorithmef-fectivelyhandlesthesecomplexdependen-ciesbyfactoringthemodelintolocalandglobalcomponents,andincrementallycon-structingthemodel’soutput.Theresultsofautomaticevaluationandmanualassessmentconﬁrmthebeneﬁtsofthisdesign:oursys-temisconsistentlyrankedhigherthannon-hierarchicalbaselines.1IntroductionCurrentresearchinsummarizationfocusesonpro-cessingshortarticles,primarilyinthenewsdomain.Whileinpracticetheexistingsummarizationmeth-odsarenotlimitedtothismaterial,theyarenotuniversal:textsinmanydomainsandgenrescan-notbesummarizedusingthesetechniques.Apar-ticularlysigniﬁcantchallengeisthesummarizationoflongertexts,suchasbooks.Therequirementforhighcompressionratesandtheincreasedneedforthepreservationofcontextualdependenciesbe-tweensummarysentencesplacessummarizationofsuchtextsbeyondthescopeofcurrentmethods.Inthispaper,weinvestigatetheautomaticgener-ationoftables-of-contents,atypeofindicativesum-maryparticularlysuitedforaccessinginformationinlongtexts.Atypicaltable-of-contentsliststopicsdescribedinthesourcetextandprovidesinforma-tionabouttheirlocationinthetext.Thehierarchicalorganizationofinformationinthetablefurtherre-ﬁnesinformationaccessbyspecifyingtherelationsbetweendifferenttopicsandprovidingrichcontex-tualinformationduringbrowsing.Commonlyfoundinbooks,tables-of-contentscanalsofacilitateaccesstoothertypesoftexts.Forinstance,thistypeofsummarycouldserveasaneffectivenavigationtoolforunderstandingalong,unstructuredtranscriptforanacademiclectureorameeting.Givenatext,ourgoalistogenerateatreewhereinanoderepresentsasegmentoftextandatitlethatsummarizesitscontent.Thisprocessinvolvestwotasks:thehierarchicalsegmentationofthetext,andthegenerationofinformativetitlesforeachsegment.Theﬁrsttaskcanbeaddressedbyusingthehier-archicalstructurereadilyavailableinthetext(e.g.,chapters,sectionsandsubsections)orbyemploy-ingexistingtopicsegmentationalgorithms(Hearst,1994).Inthispaper,wetaketheformerapproach.Asforthesecondtask,anaiveapproachwouldbetoemployexistingmethodsoftitlegenerationtoeachsegment,andcombinetheresultsintoatreestruc-ture.However,thelatterapproachcannotguaranteethatthegeneratedtable-of-contentsformsacoher-entrepresentationoftheentiretext.Sincetitlesofdifferentsegmentsaregeneratedinisolation,someofthegeneratedtitlesmayberepetitive.Evennon-repetitivetitlesmaynotprovidesufﬁcientinforma-tiontodiscriminatebetweenthecontentofoneseg-545

ScientiﬁccomputingRemarkablerecursivealgorithmformultiplyingmatricesDivideandconqueralgorithmdesignMakingarecursivealgorithmSolvingsystemsoflinearequationsComputinganLUPdecompositionForwardandbacksubstitutionSymmetricpositivedeﬁnitematricesandleastsquaresapproximationFigure1:Afragmentofatable-of-contentsgeneratedbyourmethod.mentandanother.Therefore,itisessentialtogen-erateanentiretable-of-contentstreeinaconcertedfashion.Thispaperpresentsahierarchicaldiscriminativeapproachfortable-of-contentsgeneration.Figure1showsafragmentofatable-of-contentsautomat-icallygeneratedbythisalgorithm.Ourmethodhastwoimportantpointsofdeparturefromexist-ingtechniques.First,weintroduceastructureddis-criminativemodelfortable-of-contentsgenerationthataccountsforawiderangeofphrase-basedandcollocationalfeatures.Theﬂexibilityofthismodelresultsinimprovedsummaryquality.Second,ourmodelcapturesbothglobaldependenciesacrossdif-ferenttitlesinthetreeandlocaldependencieswithinsections.Wedecomposethemodelintolocalandglobalcomponentsthathandledifferentclassesofdependencies.Wefurtherreducethesearchspacethroughincrementalconstructionofthemodel’sout-putbyconsideringonlythepromisingpartsofthedecisionspace.Weapplyourmethodtoprocessa1,180pageal-gorithmstextbook.Toassessthecontributionofourhierarchicalmodel,wecompareourmethodwithstate-of-the-artmethodsthatgenerateeachsegmenttitleindependently.1Theresultsofautomaticeval-uationandmanualassessmentoftitlequalityshowthattheoutputofoursystemisconsistentlyrankedhigherthanthatofnon-hierarchicalbaselines.2RelatedWorkAlthoughmostcurrentresearchinsummarizationfocusesonnewspaperarticles,anumberofap-proacheshavebeendevelopedforprocessinglongertexts.Mostoftheseapproachesaretailoredtoapar-1Thecodeandfeaturevectordataforourmodelandthebaselinesareavailableathttp://people.csail.mit.edu/branavan/code/toc.ticulardomain,suchasmedicalliteratureorscien-tiﬁcarticles.Bymakingstrongassumptionsabouttheinputstructureandthedesiredformatoftheout-put,thesemethodsachieveahighcompressionratewhilepreservingsummarycoherence.Forinstance,TeufelandMoens(2002)summarizescientiﬁcarti-clesbyselectingrhetoricalelementsthatarecom-monlypresentinscientiﬁcabstracts.ElhadadandMcKeown(2001)generatesummariesofmedicalar-ticlesbyfollowingacertainstructuraltemplateincontentselectionandrealization.Ourwork,however,isclosertodomain-independentmethodsforsummarizinglongtexts.Typically,theseapproachesemploytopicsegmen-tationtoidentifyalistoftopicsdescribedinadocument,andthenproduceasummaryforeachpart(BoguraevandNeff,2000;Anghelutaetal.,2002).Incontrasttoourmethod,theseapproachesperformeithersentenceorphraseextraction,ratherthansummarygeneration.Moreover,extractionforeachsegmentisperformedinisolation,andglobalconstraintsonthesummaryarenotenforced.Finally,ourworkisalsorelatedtoresearchonti-tlegeneration(Bankoetal.,2000;JinandHaupt-mann,2001;Dorretal.,2003).Sinceworkinthisareafocusesongeneratingtitlesforonearticleatatime(e.g.,newspaperreports),theissueofhierarchi-calgeneration,whichisuniquetoourtask,doesnotarise.However,thisisnottheonlynovelaspectoftheproposedapproach.Ourmodellearnstitlegener-ationinafullydiscriminativeframework,incontrasttothecommonlyusednoisy-channelmodel.Thus,insteadofindependentlymodelingtheselectionandgrammaticalityconstraints,welearnbothtypesoffeaturesinasingleframework.Thisjointtrainingregimesupportsgreaterﬂexibilityinmodelingfea-tureinteraction.546

3ProblemFormulationWeformalizetheproblemoftable-of-contentsgen-erationasasupervisedlearningtaskwherethegoalistomapatreeoftextsegmentsStoatreeoftitlesT.Asegmentmaycorrespondtoachapter,sectionorsubsection.Sincethefocusofourworkisonthegenerationaspectoftable-of-contentsconstruction,weassumethatthehierarchicalsegmentationofatextispro-videdintheinput.Thisdivisioncaneitherbeau-tomaticallycomputedusingoneofthemanyavail-abletextsegmentationalgorithms(Hearst,1994),oritcanbebasedondemarcationsalreadypresentintheinput(e.g.,paragraphmarkers).Duringtraining,thealgorithmisprovidedwithasetofpairs(Si,Ti)fori=1,...,p,whereSiistheithtreeoftextsegments,andTiisthetable-of-contentsforthattree.Duringtesting,thealgorithmgeneratestables-of-contentsforunseentreesoftextsegments.Wealsoassumethatduringtestingthedesiredtitlelengthisprovidedasaparametertothealgo-rithm.4AlgorithmTogenerateacoherenttable-of-contents,weneedtotakeintoaccountmultipleconstraints:thetitlesshouldbegrammatical,theyshouldadequatelyrep-resentthecontentoftheirsegments,andthetable-of-contentsasawholeshouldclearlyconveythere-lationsbetweenthesegments.Takingadiscrimina-tiveapproachformodelingthistaskwouldallowustoachievethisgoal:wecaneasilyintegratearangeofconstraintsinaﬂexiblemanner.Sincethenum-berofpossiblelabels(i.e.,tables-of-contents)ispro-hibitivelylargeandthelabelsthemselvesexhibitarichinternalstructure,weemployastructureddis-criminativemodelthatcaneasilyhandlecomplexdependencies.Oursolutionreliesontwoorthogo-nalstrategiestobalancethetractabilityandtherich-nessofthemodel.First,wefactorthemodelintolocalandglobalcomponents.Second,weincremen-tallyconstructtheoutputofeachcomponentusingasearch-baseddiscriminativealgorithm.Bothofthesestrategieshavetheeffectofintelligentlyprun-ingthedecisionspace.Ourmodelfactorizationisdrivenbythedifferenttypesofdependencieswhicharecapturedbythetwocomponents.Theﬁrstmodelislocal:foreachseg-ment,itgeneratesalistofcandidatetitlesrankedbytheirindividuallikelihoods.Thismodelfocusesongrammaticalityandwordselectionconstraints,butitdoesnotconsiderrelationsamongdifferenttitlesinthetable-of-contents.Theselatterdependenciesarecapturedintheglobalmodelthatconstructsatable-of-contentsbyselectingtitlesforeachsegmentfromtheavailablecandidates.Evenafterthisfactoriza-tion,thedecisionspaceforeachmodelislarge:forthelocalmodel,itisexponentialinthelengthofthesegmenttitle,andfortheglobalmodelitisexponen-tialinthesizeofthetree.Therefore,weconstructtheoutputforeachofthesemodelsincrementallyusingbeamsearch.Thealgorithmmaintainsthemostpromisingpartialout-putstructures,whichareextendedateveryitera-tion.Themodelincorporatesthisdecodingpro-cedureintothetrainingprocess,therebylearningmodelparametersbestsuitedforthespeciﬁcdecod-ingalgorithm.Similarmodelshavebeensuccess-fullyappliedinthepasttoothertasksincludingpars-ing(CollinsandRoark,2004),chunking(Daum´eandMarcu,2005),andmachinetranslation(Cowanetal.,2006).4.1ModelStructureThemodeltakesasinputatreeoftextsegmentsS.Eachsegments∈SanditstitlezarerepresentedasalocalfeaturevectorΦloc(s,z).Eachcompo-nentofthisvectorstoresanumericalvalue.Thisfeaturevectorcantrackanyfeatureofthesegmentstogetherwithitstitlez.Forinstance,theithcompo-nentofthisvectormayindicatewhetherthebigram(z[j]z[j+1])occursins,wherez[j]isthejthwordinz:(Φloc(s,z))i=(cid:26)1if(z[j]z[j+1])∈s0otherwiseInaddition,ourmodelcapturesdependenciesamongmultipletitlesthatappearinthesametable-of-contents.WerepresentatreeofsegmentsSpairedwithtitlesTwiththeglobalfeaturevectorΦglob(S,T).Thecomponentsherearealsonumer-icalfeatures.Forexample,theithcomponentofthevectormayindicatewhetheratitleisrepeatedinthetable-of-contentsT:547

(Φglob(S,T))i=(cid:26)1repeatedtitle0otherwiseOurmodelconstructsatable-of-contentsintwobasicsteps:StepOneThegoalofthisstepistogeneratealistofkcandidatetitlesforeachsegments∈S.Todoso,foreachpossibletitlez,themodelmapsthefeaturevectorΦloc(s,z)toarealnumber.Thismappingcantaketheformofalinearmodel,Φloc(s,z)·αlocwhereαlocisthelocalparametervector.Sincethenumberofpossibletitlesisexponen-tial,wecannotconsiderallofthem.Instead,weprunethedecisionspacebyincrementallyconstruct-ingpromisingtitles.Ateachiterationj,thealgo-rithmmaintainsabeamQofthetopkpartiallygen-eratedtitlesoflengthj.Duringiterationj+1,anewsetofcandidatesisgrownbyappendingawordfromstotherightofeachmemberofthebeamQ.WethensorttheentriesinQ:z1,z2,...suchthatΦloc(s,zi)·αloc≥Φloc(s,zi+1)·αloc,∀i.Onlythetopkcandidatesareretained,formingthebeamforthenextiteration.Thisprocesscontinuesuntilatitleofthedesiredlengthisgenerated.Finally,thelistofkcandidatesisreturned.StepTwoGivenasetofcandidatetitlesz1,z2,...,zkforeachsegments∈S,ourgoalistoconstructatable-of-contentsTbyselectingthemostappropriatetitlefromeachsegment’scandi-datelist.Todoso,ourmodelcomputesascoreforthepair(S,T)basedontheglobalfeaturevectorΦglob(S,T):Φglob(S,T)·αglobwhereαglobistheglobalparametervector.Aswiththelocalmodel(stepone),thenum-berofpossibletables-of-contentsistoolargetobeconsideredexhaustively.Therefore,weincremen-tallyconstructatable-of-contentsbytraversingthetreeofsegmentsinapre-orderwalk(i.e.,theor-derinwhichsegmentsappearinthetext).Inthiscase,thebeamcontainspartiallygeneratedtables-of-contents,whichareexpandedbyonesegmentti-tleatatime.Tofurtherreducethesearchspace,duringdecodingonlythetopﬁvecandidatetitlesforasegmentaregiventotheglobalmodel.4.2TrainingtheModelTrainingforStepOneWenowdescribehowthelocalparametervectorαlocisestimatedfromtrain-ingdata.Wearegivenasetoftrainingexamples(si,yi)fori=1,...,l,wheresiistheithtextseg-ment,andyiisthetitleofthissegment.Thislinearmodelislearnedusingavariantoftheincrementalperceptronalgorithm(CollinsandRoark,2004;Daum´eandMarcu,2005).Thison-linealgorithmtraversesthetrainingsetmultipletimes,updatingtheparametervectorαlocaftereachtrainingexampleincaseofmis-predictions.Theal-gorithmencouragesasettingoftheparametervectorαlocthatassignsthehighestscoretothefeaturevec-torassociatedwiththecorrecttitle.Thepseudo-codeofthealgorithmisshowninFig-ure2.Givenatextsegmentsandthecorrespondingtitley,thetrainingalgorithmmaintainsabeamQcontainingthetopkpartialtitlesoflengthj.Thebeamisupdatedoneachiterationusingthefunc-tionsGROWandPRUNE.Foreverywordinseg-mentsandforeverypartialtitleinQ,GROWcre-atesanewtitlebyappendingthiswordtothetitle.PRUNEretainsonlythetoprankedcandidatesbasedonthescoringfunctionΦloc(s,z)·αloc.Ify[1...j](i.e.,thepreﬁxofyoflengthj)isnotinthemodi-ﬁedbeamQ,thenαlocisupdated2asshowninline4ofthepseudo-codeinFigure2.Inaddition,Qisreplacedwithabeamcontainingonlyy[1...j](line5).Thisprocessisperformed|y|times.Werepeatthisprocessforalltrainingexamplesover50train-ingiterations.3TrainingforStepTwoTotraintheglobalparam-etervectorαglob,wearegiventrainingexamples(Si,Ti)fori=1,...,p,whereSiistheithtreeoftextsegments,andTiisthetable-of-contentsforthattree.However,wecannotdirectlyusethesetables-of-contentsfortrainingourglobalmodel:sincethismodelselectsoneofthecandidatetitleszi1,...,zikreturnedbythelocalmodel,thetruetitleoftheseg-mentmaynotbeamongthesecandidates.There-fore,todetermineanewtargettitleforthesegment,weneedtoidentifythetitleinthesetofcandidates2Ifthewordinthejthpositionofydoesnotoccurins,thentheparameterupdateisnotperformed.3Fordecoding,αlocisaveragedoverthetrainingiterationsasinCollinsandRoark(2004).548

s–segmenttext.y–segmenttitle.y[1...j]–preﬁxofyoflengthj.Q–beamcontainingpartialtitles.1.forj=1...|y|2.Q=PRUNE(GROW(s,Q))3.ify[1...j]/∈Q4.αloc=αloc+Φloc(s,y[1...j])−Pz∈QΦloc(s,z)|Q|5.Q={y[1...j]}Figure2:Thetrainingalgorithmforthelocalmodel.thatisclosesttothetruetitle.WeemploytheL1distancemeasuretocomparethecontentwordoverlapbetweentwotitles.4Foreachinput(S,T),andeachsegments∈S,weiden-tifythesegmenttitleclosestintheL1measuretothetruetitley5:z∗=argminiL1(zi,y)Onceallthetrainingtargetsinthecorpushavebeenidentiﬁedthroughthisprocedure,thegloballinearmodelΦglob(S,T)·αglobislearnedusingthesameperceptronalgorithmasinstepone.Ratherthanmaintainingthebeamofpartiallygeneratedti-tles,thebeamQholdspartiallygeneratedtables-of-contents.Also,theloopinline1ofFigure2iteratesoversegmenttitlesratherthanwords.Theglobalmodelistrainedover200iterations.5FeaturesLocalFeaturesOurlocalmodelaimstogeneratetitleswhichadequatelyrepresentthemeaningofthesegmentandaregrammatical.Selectionandcontex-tualpreferencesareencodedinthelocalfeatures.Thefeaturesthatcaptureselectionconstraintsarespeciﬁedatthewordlevel,andcontextualfeaturesareexpressedatthewordsequencelevel.Theselectionfeaturescapturethepositionoftheword,itsTF*IDF,andpart-of-speechinformation.Inaddition,theyalsorecordwhetherthewordoc-cursinthebodyofneighboringsegments.Wealso4ThismeasureisclosetoROUGE-1whichinadditioncon-siderstheoverlapinauxiliarywords.5Inthecaseofties,oneofthetitlesispickedarbitrarily.SegmenthasthesametitleasitssiblingSegmenthasthesametitleasitsparentTwoadjacentsiblingtitleshavethesameheadTwoadjacentsiblingtitlesstartwiththesamewordRankgiventothetitlebythelocalmodelTable1:Examplesofglobalfeatures.generateconjunctivefeaturesbycombiningfeaturesofdifferenttypes.Thecontextualfeaturesrecordthebigramandtri-gramlanguagemodelscores,bothforwordsandforpart-of-speechtags.Thetrigramscoresareaver-agedoverthetitle.ThelanguagemodelsaretrainedusingtheSRILMtoolkit.Anothertypeofcontex-tualfeaturemodelsthecollocationalpropertiesofnounphrasesinthetitle.Thisfeatureaimstoelim-inategenericphrases,suchas“thefollowingsec-tion”fromthegeneratedtitles.6Toachievethisef-fect,foreachnounphraseinthetitle,wemeasuretheratiooftheirfrequencyinthesegmenttotheirfrequencyinthecorpus.GlobalFeaturesOurglobalmodeldescribestheinteractionbetweendifferenttitlesinthetree(SeeTable1).Theseinteractionsareencodedinthreetypesofglobalfeatures.Theﬁrsttypeofglobalfeatureindicateswhethertitlesinthetreearere-dundantatvariouslevelsofthetreestructure.Thesecondtypeoffeatureencouragesparallelconstruc-tionswithinthesametree.Forinstance,titlesofad-joiningsegmentsmaybeverbalizedasnounphraseswiththesamehead(e.g.,“Bubblesortalgorithm”,“Mergesortalgorithm”).Wecapturethispropertybycomparingwordsthatappearincertainpositionsinadjacentsiblingtitles.Finally,ourglobalmodelalsousestherankofthetitleprovidedbythelocalmodel.Thisfeatureenablestheglobalmodeltoac-countforthepreferencesofthelocalmodelinthetitleselectionprocess.6EvaluationSet-UpDataWeapplyourmethodtoanundergraduateal-gorithmstextbook.FordetailedstatisticsonthedataseeTable2.Wesplititstable-of-contentsintoaset6Unfortunately,wecouldnotusemoresophisticatedsyntac-ticfeaturesduetothelowaccuracyofstatisticalparsersonourcorpus.549

NumberofTitles540NumberofTrees39TreeDepth4NumberofWords269,650Avg.TitleLength3.64Avg.Branching3.29Avg.TitleDuplicates21Table2:Statisticsonthecorpususedintheexperi-ments.ofindependentsubtrees.Givenatable-of-contentsofdepthnwitharootbranchingfactorofr,wegen-eratersubtrees,withadepthofatmostn−1.Werandomlyselect80%ofthesetreesfortraining,andtherestareusedfortesting.Inourexperiments,weusetendifferentrandomizationstocompensateforthesmallnumberofavailabletrees.Admittedly,thismethodofgeneratingtrainingandtestingdataomitssomedependenciesatthelevelofthetable-of-contentsasawhole.However,thesubtreesusedinourexperimentsstillexhibitasufﬁcientlydeephierarchicalstructure,richwithcontextualdependencies.BaselinesAsanalternativetoourhierarchicaldis-criminativemethod,weconsiderthreebaselinesthatbuildatable-of-contentsbygeneratingatitleforeachsegmentindividually,withouttakingintoac-countthetreestructure,andonehierarchicalgener-ativebaseline.Theﬁrstmethodgeneratesatitleforasegmentbyselectingthenounphrasefromthatseg-mentwiththehighestTF*IDF.Thissimplemethodiscommonlyusedtogeneratekeywordsforbrows-ingapplicationsininformationretrieval,andhasbeenshowntobeeffectiveforsummarizingtechni-calcontent(Wacholderetal.,2001).Thesecondbaselineisbasedonthenoisy-channelgenerative(ﬂatgenerative,FG)modelproposedbyBankoetal.,(2000).Similartoourlocalmodel,thismethodcapturesbothselectionandgrammati-calconstraints.However,theseconstraintsaremod-eledseparately,andthencombinedinagenerativeframework.Weuseourlocalmodel(FlatDiscriminativemodel,FD)asthethirdbaseline.Likethesecondbaseline,thismodelomitsglobaldependencies,andonlyfocusesonfeaturesthatcapturerelationswithinindividualsegments.Inthehierarchicalgenerative(HG)baselinewerunourglobalmodelontherankedlistoftitlespro-ducedforeachsectionbythenoisy-channelgenera-tivemodel.Thelastthreebaselinesandouralgorithmarepro-videdwiththetitlelengthasaparameter.Inourexperiments,thealgorithmsusethereferencetitlelength.ExperimentalDesign:Comparisonwithrefer-encetables-of-contentsReferencebasedevalu-ationiscommonlyusedtoassessthequalityofmachine-generatedheadlines(Wangetal.,2005).Wecompareoursystem’soutputwiththetable-of-contentsfromthetextbookusingROUGEmetrics.Weemployapubliclyavailablesoftwarepackage,7withalltheparameterssettodefaultvalues.ExperimentalDesign:HumanassessmentThejudgeswereeachgiven30segmentsrandomlyse-lectedfromasetof359testsegments.Foreachtestsegment,thejudgeswerepresentedwithitstext,and3alternativetitlesconsistingofthereferenceandthetitlesproducedbythehierarchicaldiscriminativemodel,andthebestperformingbaseline.Inaddi-tion,thejudgeshadaccesstoallofthesegmentsinthebook.Atotalof498titlesfor166uniqueseg-mentswereranked.Thesystemidentitieswerehid-denfromthejudges,andthetitleswerepresentedinrandomorder.Thejudgesrankedthetitlesbasedonhowwelltheyrepresentthecontentofthesegment.Titleswererankedequaliftheywerejudgedtobeequallyrepresentativeofthesegment.Sixpeopleparticipatedinthisexperiment.Alltheparticipantsweregraduatestudentsincomputersci-encewhohadtakenthealgorithmsclassinthepastandwerereasonablyfamiliarwiththematerial.7ResultsFigure3showsfragmentsofthetables-of-contentsgeneratedbyourmethodandthefourbaselinesalongwiththereferencecounterpart.Theseextractsillustratethreegeneralphenomenathatweobservedinthetestcorpus.First,thetitlesproducedbykey-wordextractionexhibitahighdegreeofredundancy.Infact,40%ofthetitlesproducedbythismethodarerepeatedmorethanonceinthetable-of-contents.In7http://www.isi.edu/licensed-sw/see/rouge/550

Reference:hashtablesdirectaddresstableshashtablescollisionresolutionbychaininganalysisofhashingwithchainingopenaddressinglinearprobingquadraticprobingdoublehashingFlatGenerative:linkedlistworstcasetimewastedspaceworstcaserunningtimetoshowthattherearedynamicsetoccupiedslotquadraticfunctiondoublehashingFlatDiscriminative:dictionaryoperationsuniverseofkeyscomputermemoryelementinthelisthashtablewithloadfactorhashtablehashfunctionhashfunctiondoublehashingKeywordExtraction:hashtabledynamicsethashfunctionworstcaseexpectednumberhashtablehashfunctionhashtabledoublehashingHierarchicalGenerative:dictionaryoperationsworstcasetimewastedspaceworstcaserunningtimetoshowthattherearecollisionresolutionlineartimequadraticfunctiondoublehashingHierarchicalDiscriminative:dictionaryoperationsdirectaddresstablecomputermemoryworstcaserunningtimehashtablewithloadfactoraddresstablehashfunctionquadraticprobingdoublehashingFigure3:Fragmentsoftables-of-contentsgeneratedbyourmethodandthefourbaselinesalongwiththecorrespondingreference.Rouge-1Rouge-LRouge-WFullMatchHD0.2560.2490.21613.5FD0.2410.2340.20313.1HG0.1390.1330.1175.8FG0.0940.0900.0794.1Keyword0.1680.1680.1576.3Table3:Titlequalityascomparedtothereferenceforthehierarchicaldiscriminative(HD),ﬂatdis-criminative(FD),hierarchicalgenerative(HG),ﬂatgenerative(FG)andKeywordmodels.Theimprove-mentgivenbyHDoverFDinallthreeRougemea-suresissigniﬁcantatp≤0.03basedontheSigntest.betterworseequalHDvs.FD683249Referencevs.HD1151322Referencevs.FD123720Table4:Overallpairwisecomparisonsoftherank-ingsgivenbythejudges.Theimprovementinti-tlequalitygivenbyHDoverFDissigniﬁcantatp≤0.0002basedontheSigntest.contrast,ourmethodyields5.5%ofthetitlesasdu-plicates,ascomparedto9%inthereferencetable-of-contents.8Second,thefragmentsshowthatthetwodiscrim-inativemodels—FlatandHierarchical—haveanumberofcommontitles.However,addingglobaldependenciestoreranktitlesgeneratedbythelocalmodelchanges30%ofthetitlesinthetestset.Comparisonwithreferencetables-of-contentsTable3showstheaverageROUGEscoresoverthetenrandomizationsfortheﬁveautomaticmeth-ods.Thehierarchicaldiscriminativemethodconsis-tentlyoutperformsthefourbaselinesaccordingtoallROUGEmetrics.Atthesametime,theseresultsalsoshowthatonlyasmallratiooftheautomaticallygeneratedtitlesareidenticaltothereferenceones.Insomecases,themachine-generatedtitlesareverycloseinmean-ingtothereference,butareverbalizeddifferently.Examplesincludepairssuchas(“MinimumSpan-ningTrees”,“SpanningTreeProblem”)and(“Wal-laceTree”,“MultiplicationCircuit”).9Whilemea-sureslikeROUGEcancapturethesimilarityintheﬁrstpair,theycannotidentifysemanticproximity8Titlessuchas“Analysis”and“ChapterOutline”arere-peatedmultipletimesinthetext.9AWallaceTreeisacircuitthatmultipliestwointegers.551

betweenthetitlesinthesecondpair.Therefore,wesupplementtheresultsofthisexperimentwithamanualassessmentoftitlequalityasdescribedbe-low.HumanassessmentWeanalyzethehumanrat-ingsbyconsideringpairwisecomparisonsbetweenthemodels.Giventwomodels,AandB,threeout-comesarepossible:AisbetterthanB,Bisbet-terthanA,ortheyareofequalquality.There-sultsofthecomparisonaresummarizedinTable4.Theseresultsindicatethatusinghierarchicalinfor-mationyieldsstatisticallysigniﬁcantimprovement(atp≤0.0002basedontheSigntest)overaﬂatcounterpart.8ConclusionandFutureWorkThispaperpresentsamethodfortheautomaticgen-erationofatable-of-contents.Thekeystrengthofourmethodliesinitsabilitytotrackdependenciesbetweengenerationdecisionsacrossdifferentlevelsofthetreestructure.Theresultsofautomaticevalu-ationandmanualassessmentconﬁrmthebeneﬁtsofjointtreelearning:oursystemisconsistentlyrankedhigherthannon-hierarchicalbaselines.Wealsoplantoexpandourmethodforthetaskofslidegeneration.Liketables-of-contents,slidebulletsareorganizedinahierarchicalfashionandarewritteninrelativelyshortphrases.Fromthelanguageviewpoint,however,slidesexhibitmorevariabilityandcomplexitythanatypicaltable-of-contents.Toaddressthischallenge,wewillexploremorepowerfulgenerationmethodsthattakeintoac-countsyntacticinformation.AcknowledgmentsTheauthorsacknowledgethesupportoftheNa-tionalScienceFoundation(CAREERgrantIIS-0448168andgrantIIS-0415865).Wewouldalsoliketoacknowledgethemanypeoplewhotookpartinhumanevaluations.ThankstoMichaelCollins,BenjaminSnyder,IgorMalioutov,JacobEisenstein,LukeZettlemoyer,TerryKoo,ErdongChen,Zo-ranDzunicandtheanonymousreviewersforhelpfulcommentsandsuggestions.Anyopinions,ﬁndings,conclusionsorrecommendationsexpressedabovearethoseoftheauthorsanddonotnecessarilyre-ﬂecttheviewsoftheNSF.ReferencesRoxanaAngheluta,RikDeBusser,andMarie-FrancineMoens.2002.Theuseoftopicsegmentationforauto-maticsummarization.InProceedingsoftheACL-2002WorkshoponAutomaticSummarization.MicheleBanko,VibhuO.Mittal,andMichaelJ.Wit-brock.2000.Headlinegenerationbasedonstatisticaltranslation.InProceedingsoftheACL,pages318–325.BranimirBoguraevandMaryS.Neff.2000.Discoursesegmentationinaidofdocumentsummarization.InProceedingsofthe33rdHawaiiInternationalConfer-enceonSystemSciences,pages3004–3014.MichaelCollinsandBrianRoark.2004.Incrementalparsingwiththeperceptronalgorithm.InProceedingsoftheACL,pages111–118.BrookeCowan,IvonaKucerova,andMichaelCollins.2006.Adiscriminativemodelfortree-to-treetrans-lation.InProceedingsoftheEMNLP,pages232–241.HalDaum´eandDanielMarcu.2005.Learningassearchoptimization:Approximatelargemarginmethodsforstructuredprediction.InProceedingsoftheICML,pages169–176.BonnieDorr,DavidZajic,andRichardSchwartz.2003.Hedgetrimmer:aparse-and-trimapproachtoheadlinegeneration.InProceedingsoftheHLT-NAACL03onTextsummarizationworkshop,pages1–8.NoemieElhadadandKathleenR.McKeown.2001.To-wardsgeneratingpatientspeciﬁcsummariesofmed-icalarticles.InProceedingsofNAACLWorkshoponAutomaticSummarization,pages31–39.MartiHearst.1994.Multi-paragraphsegmentationofexpositorytext.InProceedingsoftheACL,pages9–16.RongJinandAlexanderG.Hauptmann.2001.Auto-matictitlegenerationforspokenbroadcastnews.InProceedingsoftheHLT,pages1–3.SimoneTeufelandMarcMoens.2002.Summariz-ingscientiﬁcarticles:Experimentswithrelevanceandrhetoricalstatus.ComputationalLinguistics,28(4):409–445.NinaWacholder,DavidK.Evans,andJudithKlavans.2001.Automaticidentiﬁcationandorganizationofin-dextermsforinteractivebrowsing.InJCDL,pages126–134.R.Wang,J.Dunnion,andJ.Carthy.2005.Machinelearningapproachtoaugmentingnewsheadlinegen-eration.InProceedingsoftheIJCNLP.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 552–559,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

552

Towards an Iterative Reinforcement Approach for Simultaneous Document Summarization and Keyword Extraction Xiaojun Wan                     Jianwu Yang                     Jianguo Xiao Institute of Computer Science and Technology  Peking University, Beijing 100871, China {wanxiaojun,yangjianwu,xiaojianguo}@icst.pku.edu.cnAbstract Though both document summarization and keyword extraction aim to extract concise representations from documents, these two tasks have usually been investigated inde-pendently. This paper proposes a novel it-erative reinforcement approach to simulta-neously extracting summary and keywords from single document under the assump-tion that the summary and keywords of a document can be mutually boosted. The approach can naturally make full use of the reinforcement between sentences and key-words by fusing three kinds of relation-ships between sentences and words, either homogeneous or heterogeneous. Experi-mental results show the effectiveness of the proposed approach for both tasks. The cor-pus-based approach is validated to work almost as well as the knowledge-based ap-proach for computing word semantics.  1Introduction Text summarization is the process of creating a compressed version of a given document that de-livers the main topic of the document. Keyword extraction is the process of extracting a few salient words (or phrases) from a given text and using the words to represent the text. The two tasks are simi-lar in essence because they both aim to extract concise representations for documents. Automatic text summarization and keyword extraction have drawn much attention for a long time because they both are very important for many text applications, including document retrieval, document clustering, etc.  For example, keywords of a document can be used for document indexing and thus benefit to improve the performance of document retrieval, and document summary can help to facilitate users to browse the search results and improve users’ search experience.  Text summaries and keywords can be either query-relevant or generic. Generic summaryand keyword should reflect the main topics of the document without any additional clues and prior knowledge. In this paper, we focus on generic document summarization and keyword extraction for single documents. Document summarization and keyword extrac-tion have been widely explored in the natural lan-guage processing and information retrieval com-munities. A series of workshops and conferences on automatic text summarization (e.g. SUMMAC, DUC and NTCIR) have advanced the technology and produced a couple of experimental online sys-tems. In recent years, graph-based ranking algo-rithms have been successfully used for document summarization (Mihalcea and Tarau, 2004, 2005; ErKan and Radev, 2004) and keyword extraction (Mihalcea and Tarau, 2004). Such algorithms make use of “voting” or “recommendations” between sentences (or words) to extract sentences (or key-words). Though the two tasks essentially share much in common, most algorithms have been de-veloped particularly for either document summari-zation or keyword extraction.  Zha (2002) proposes a method for simultaneous keyphrase extraction and text summarization by using only the heterogeneous sentence-to-word relationships. Inspired by this, we aim to take into account all the three kinds of relationships among sentences and words (i.e. the homogeneous rela-tionships between words, the homogeneous rela-tionships between sentences, and the heterogene-ous relationships between words and sentences) in 553

aunified framework for both document summari-zation and keyword extraction. The importance of asentence (word) is determined by both the impor-tance of related sentences (words) and the impor-tance of related words (sentences). The proposed approach can be considered as a generalized form of previous graph-based ranking algorithms and Zha’s work (Zha, 2002).  In this study, we propose an iterative reinforce-ment approach to realize the above idea. The pro-posed approach is evaluated on the DUC2002 dataset and the results demonstrate its effectiveness for both document summarization and keyword extraction. Both knowledge-based approach and corpus-based approach have been investigated to compute word semantics and they both perform very well.  The rest of this paper is organized as follows: Section 2 introduces related works. The details of the proposed approach are described in Section 3. Section 4 presents and discusses the evaluation results. Lastly we conclude our paper in Section 5. 2Related Works 2.1 Document Summarization Generally speaking, single document summariza-tion methods can be either extraction-based or ab-straction-based and we focus on extraction-based methods in this study. Extraction-based methods usually assign a sali-ency score to each sentence and then rank the sen-tences in the document. The scores are usually computed based on a combination of statistical and linguistic features, including term frequency, sen-tence position, cue words, stigma words, topic sig-nature (Hovy and Lin, 1997; Lin and Hovy, 2000), etc. Machine learning methods have also been em-ployed to extract sentences, including unsupervised methods (Nomoto and Matsumoto, 2001) and su-pervised methods (Kupiec et al., 1995; Conroy and O’Leary, 2001; Amini and Gallinari, 2002; Shen et al., 2007). Other methods include maximal mar-ginal relevance (MMR) (Carbonell and Goldstein, 1998), latent semantic analysis (LSA) (Gong and Liu, 2001). In Zha (2002), the mutual reinforce-ment principle is employed to iteratively extract key phrases and sentences from a document.   Most recently, graph-based ranking methods, in-cluding TextRank ((Mihalcea and Tarau, 2004, 2005) and LexPageRank (ErKan and Radev, 2004) have been proposed for document summarization. Similar to Kleinberg’s HITS algorithm (Kleinberg, 1999) or Google’s PageRank (Brin and Page, 1998), these methods first build a graph based on the similarity between sentences in a document and then the importance of a sentence is determined by taking into account global information on the graph recursively, rather than relying only on local sentence-specific information. 2.2 Keyword Extraction Keyword (or keyphrase) extraction usually in-volves assigning a saliency score to each candidate keyword by considering various features. Krulwich and Burkey (1996) use heuristics to extract key-phrases from a document. The heuristics are based on syntactic clues, such as the use of italics, the presence of phrases in section headers, and the use of acronyms. Muñoz (1996) uses an unsupervised learning algorithm to discover two-word key-phrases. The algorithm is based on Adaptive Reso-nance Theory (ART) neural networks. Steier and Belew (1993) use the mutual information statistics to discover two-word keyphrases. Supervised machine learning algorithms have been proposed to classify a candidate phrase into either keyphrase or not. GenEx (Turney, 2000) and Kea (Frank et al., 1999; Witten et al., 1999) are two typical systems, and the most important fea-tures for classifying a candidate phrase are the fre-quency and location of the phrase in the document. More linguistic knowledge (such as syntactic fea-tures) has been explored by Hulth (2003). More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank keywords based on the co-occurrence links between words. 3Iterative Reinforcement Approach 3.1 Overview The proposed approach is intuitively based on the following assumptions: Assumption 1:Asentence should be salient if it is heavily linked with other salient sentences, and a word should be salient if it is heavily linked with other salient words. Assumption 2:Asentence should be salient if it contains many salient words, and a word should be salient if it appears in many salient sentences. The first assumption is similar to PageRank which makes use of mutual “recommendations” 554

between homogeneous objects to rank objects. The second assumption is similar to HITS if words and sentences are considered as authorities and hubs respectively. In other words, the proposed ap-proach aims to fuse the ideas of PageRank and HITS in a unified framework.  In more detail, given the heterogeneous data points of sentences and words, the following three kinds of relationships are fused in the proposed approach: SS-Relationship:It reflects the homogeneous relationships between sentences, usually computed by their content similarity. WW-Relationship:It reflects the homogeneous relationships between words, usually computed by knowledge-based approach or corpus-based ap-proach. SW-Relationship:It reflects the heterogeneous relationships between sentences and words, usually computed as the relative importance of a word in a sentence. Figure 1 gives an illustration of the relationships.   Figure 1. Illustration of the Relationships  The proposed approach first builds three graphs to reflect the above relationships respectively, and then iteratively computes the saliency scores of the sentences and words based on the graphs. Finally, the algorithm converges and each sentence or word gets its saliency score. The sentences with high saliency scores are chosen into the summary, and the words with high saliency scores are combined to produce the keywords. 3.2 Graph Building 3.2.1  Sentence-to-Sentence Graph ( SS-Graph)  Given the sentence collection S={si|1IiIm}of a document,  if each sentence is considered as a node, the sentence collection can be modeled as an undi-rected graph by generating an edge between two sentences if their content similarity exceeds 0, i.e. an undirected link between siand sj(iKj)is con-structed and the associated weight is their content similarity. Thus, we construct an undirected graph GSS to reflect the homogeneous relationship be-tween sentences. The content similarity between two sentences is computed with the cosine measure. We use an adjacency matrix Uto describe GSS with each entry corresponding to the weight of a link in the graph. U=[Uij]m×mis defined as follows: (cid:1)(cid:2)(cid:1)(cid:3)(cid:4)(cid:185)¥(cid:215)=otherwise,j,if issssUjijiij0rrrr(1) where isand jsrare the corresponding term vec-tors of sentences siand sjrespectively. The weight associated with term tis calculated with tft.isft,where tftis the frequency of term tin the sentence and isftis the inverse sentence frequency of term t,i.e. 1+log(N/nt), where Nis the total number of sentences and ntis the number of sentences con-taining term tin a background corpus. Note that other measures (e.g. Jaccard, Dice, Overlap, etc.) can also be explored to compute the content simi-larity between sentences, and we simply choose the cosine measure in this study. Then Uis normalized to U~as follows to make the sum of each row equal to 1: (cid:1)(cid:2)(cid:1)(cid:3)(cid:4)(cid:185)=(cid:5)(cid:5)==erwise ,oth U,if UUUmjijmjijijij00~11(2) 3.2.2  Word-to-Word Graph ( WW-Graph)  Given the word collection T={tj|1IjIn}of a docu-ment1,the semantic similarity between any two words tiand tjcan be computed using approaches that are either knowledge-based or corpus-based (Mihalcea et al., 2006).   Knowledge-based measures of word semantic similarity try to quantify the degree to which two words are semantically related using information drawn from semantic networks. WordNet (Fell-baum, 1998) is a lexical database where each  1The stopwords defined in the Smart system have been re-moved from the collection. sentencewordSSWWSW555

unique meaning of a word is represented by a synonym set or synset.Each synset has a gloss that defines the concept that it represents. Synsets are connected to each other through explicit semantic relations that are defined in WordNet. Many ap-proaches have been proposed to measure semantic relatedness based on WordNet. The measures vary from simple edge-counting to attempt to factor in peculiarities of the network structure by consider-ing link direction, relative path, and density, such as  vector,lesk,hso, lch,wup,path,res, lin and jcn (Pedersen et al., 2004). For example, “cat” and “dog” has higher semantic similarity than “cat” and “computer”. In this study, we implement the vector measure to efficiently evaluate the similari-ties of a large number of word pairs. The vector measure (Patwardhan, 2003) creates a co–occurrence matrix from a corpus made up of the WordNet glosses. Each content word used in a WordNet gloss has an associated context vector. Each gloss is represented by a gloss vector that is the average of all the context vectors of the words found in the gloss. Relatedness between concepts is measured by finding the cosine between a pair of gloss vectors.  Corpus-based measures of word semantic simi-larity try to identify the degree of similarity be-tween words using information exclusively derived from large corpora. Such measures as mutual in-formation (Turney 2001), latent semantic analysis (Landauer et al., 1998), log-likelihood ratio (Dun-ning, 1993) have been proposed to evaluate word semantic similarity based on the co-occurrence information on a large corpus. In this study, we simply choose the mutual information to compute the semantic similarity between word tiand tjas follows: )()()(log)(jijijitptp,ttpN,ttsim¥¥=(3) which indicates the degree of statistical depend-ence between tiand tj.Here, Nis the total number of words in the corpus and p(ti)and p(tj)are re-spectively the probabilities of the occurrences of tiand tj,i.e. count(ti)/N and count(tj)/N,where count(ti)and count(tj)are the frequencies of tiand tj.p(ti,tj)is the probability of the co-occurrence of tiand tjwithin a window with a predefined size k,i.e. count(ti,tj)/N,where count(ti,tj)is the number of the times tiand tjco-occur within the window.  Similar to the SS-Graph, we can build an undi-rected graph GWW to reflect the homogeneous rela-tionship between words, in which each node corre-sponds to a word and the weight associated with the edge between any different word tiand tjis computed by either the WordNet-based vector measure or the corpus-based mutual information measure. We use an adjacency matrix Vto de-scribe GWW with each entry corresponding to the weight of a link in the graph. V=[Vij]n×n,where Vij =sim(ti,tj)if iKjand Vij=0 if i=j.Then Vis similarly normalized to V~to make the sum of each row equal to 1. 3.2.3  Sentence-to-Word Graph ( SW-Graph)  Given the sentence collection S={si|1IiIm}and the word collection T={tj|1IjIn}of a document, we can build a weighted bipartite graph GSW from Sand Tin the following way: if word tjappears in sentence si,we then create an edge between siand tj.Anonnegative weight aff(si,tj)is specified on the edge, which is proportional to the importance of word tjin sentence si,computed as follows: (cid:5)Œ¥¥=ijjstttttjiisftfisftf,tsaff)( (4)where trepresents a unique term in siand tft,isftare respectively the term frequency in the sentence and the inverse sentence frequency.  We use an adjacency (affinity) matrix W=[Wij]m×nto describe GSW  with each entry Wij corresponding to aff(si,tj). Similarly, Wis normal-ized to W~to make the sum of each row equal to 1. In addition, we normalize the transpose of W,i.e. WT,to Wˆto make the sum of each row in WTequal to 1. 3.3 Reinforcement Algorithm We use two column vectors u=[u(si)]m×1 and v=[v(tj)]n×1 to denote the saliency scores of the sen-tences and words in the specified document. The assumptions introduced in Section 3.1 can be ren-dered as follows: (cid:5)µjjjiisuUsu)(~)((5) (cid:5)µiiijjtvVtv)(~)((6) (cid:5)µjjjiitvWsu)(ˆ)((7) 556

(cid:5)µiiijjsuWtv)(~)((8) After fusing the above equations, we can obtain the following iterative forms: (cid:5)(cid:5)==+=njjjimjjjiitvW)suU*su11)(ˆ)(~)((9)(cid:5)(cid:5)==+=miiijniiijjsuW)tvV*tv11)(~)(~)((10)And the matrix form is: vWuUuTT)*ˆ~+=(11)uWvVvTT)*~~+=(12) where *and )specify the relative contributions to the final saliency scores from the homogeneous nodes and the heterogeneous nodes and we have *+)=1. In order to guarantee the convergence of the iterative form, uand vare normalized after each iteration. For numerical computation of the saliency scores, the initial scores of all sentences and words are set to 1 and the following two steps are alter-nated until convergence, 1. Compute and normalize the scores of sen-tences: )(n-T)(n-T(n))*11ˆ~vWuUu+=,1(n)(n)(n)/uuu=2. Compute and normalize the scores of words: )(n-T)(n-T(n))*11~~uWvVv+=,1(n)(n)(n)/vvv=where u(n)and v(n)denote the vectors computed at the n-th iteration.   Usually the convergence of the iteration algo-rithm is achieved when the difference between the scores computed at two successive iterations for any sentences and words falls below a given threshold (0.0001 in this study).  4Empirical Evaluation 4.1 Summarization Evaluation 4.1.1 Evaluation Setup We used task 1 of DUC2002 (DUC, 2002) for evaluation. The task aimed to evaluate generic summaries with a length of approximately 100 words or less. DUC2002 provided 567 English news articles collected from TREC-9 for single-document summarization task. The sentences in each article have been separated and the sentence information was stored into files.  In the experiments, the background corpus for using the mutual information measure to compute word semantics simply consisted of all the docu-ments from DUC2001 to DUC2005, which could be easily expanded by adding more documents. The stopwords were removed and the remaining words were converted to the basic forms based on WordNet. Then the semantic similarity values be-tween the words were computed.   We used the ROUGE (Lin and Hovy, 2003) toolkit (i.e.ROUGEeval-1.4.2 in this study) for evaluation, which has been widely adopted by DUC for automatic summarization evaluation. It measured summary quality by counting overlap-ping units such as the n-gram, word sequences and word pairs between the candidate summary and the reference summary. ROUGE toolkit reported sepa-rate scores for 1, 2, 3 and 4-gram, and also for longest common subsequence co-occurrences. Among these different scores, unigram-based ROUGE score (ROUGE-1) has been shown to agree with human judgment most (Lin and Hovy, 2003). We showed three of the ROUGE metrics in the experimental results: ROUGE-1 (unigram-based), ROUGE-2 (bigram-based), and ROUGE-W(based on weighted longest common subse-quence, weight=1.2).  In order to truncate summaries longer than the length limit, we used the “-l” option2in the ROUGE toolkit. 4.1.2 Evaluation Results For simplicity, the parameters in the proposed ap-proach are simply set to *=)=0.5, which means that the contributions from sentences and words are equally important. We adopt the WordNet-based vector measure (WN) and the corpus-based mutual information measure (MI) for computing the semantic similarity between words.  When us-ing the mutual information measure, we heuristi-cally set the window size kto 2, 5 and 10, respec-tively.  The proposed approaches with different word similarity measures (WN and MI) are compared  2The “-l” option is very important for fair comparison. Some previous works not adopting this option are likely to overes-timate the ROUGE scores.  557

with two solid baselines: SentenceRank and Mutu-alRank. SentenceRank is proposed in Mihalcea and Tarau (2004) to make use of only the sentence-to-sentence relationships to rank sentences, which outperforms most popular summarization methods. MutualRank is proposed in Zha (2002) to make use of only the sentence-to-word relationships to rank sentences and words. For all the summarization methods, after the sentences are ranked by their saliency scores, we can apply a variant form of the MMR algorithm to remove redundancy and choose both the salient and novel sentences to the sum-mary. Table 1 gives the comparison results of the methods before removing redundancy and Table 2 gives the comparison results of the methods after removing redundancy.  System ROUGE-1 ROUGE-2ROUGE-WOur Approach(WN) 0.47100*# 0.20424*# 0.16336#Our Approach(MI:k=2) 0.46711#0.20195#0.16257#Our Approach(MI:k=5) 0.46803#0.20259#0.16310#Our Approach(MI:k=10) 0.46823#0.20301#0.16294#SentenceRank0.45591 0.19201 0.15789 MutualRank 0.43743 0.17986 0.15333 Table 1. Summarization Performance before Re-moving Redundancy (w/o MMR)  System ROUGE-1 ROUGE-2ROUGE-WOur Approach(WN) 0.47329*# 0.20249#0.16352#Our Approach(MI:k=2) 0.47281#0.20281#0.16373#Our Approach(MI:k=5) 0.47282#0.20249#0.16343#Our Approach(MI:k=10) 0.47223#0.20225#0.16308#SentenceRank0.46261 0.19457 0.16018 MutualRank 0.43805 0.17253 0.15221 Table 2. Summarization Performance after Remov-ing Redundancy (w/ MMR)  (*indicates that the improvement over SentenceRank is sig-nificant and #indicates that the improvement over Mutual-Rank is significant, both by comparing the 95% confidence intervals provided by the ROUGE package.)Seen from Tables 1 and 2, the proposed ap-proaches always outperform the two baselines over all three metrics with different word semantic measures. Moreover, no matter whether the MMR algorithm is applied or not, almost all performance improvements over MutualRank are significant and the ROUGE-1 performance improvements over SentenceRank are significant when using WordNet-based measure (WN). Word semantics can be naturally incorporated into the computation process, which addresses the problem that Sen-tenceRank cannot take into account word seman-tics, and thus improves the summarization per-formance. We also observe that the corpus-based measure (MI) works almost as well as the knowl-edge-based measure (WN) for computing word semantic similarity. In order to better understand the relative contri-butions from the sentence nodes and the word nodes, the parameter *is varied from 0 to 1. The larger *is, the more contribution is given from the sentences through the SS-Graph, while the less contribution is given from the words through the SW-Graph. Figures 2-4 show the curves over three ROUGE scores with respect to *.Without loss of generality, we use the case of k=5 for the MI measure as an illustration. The curves are similar to Figures 2-4 when k=2 and k=10.    0.4350.440.4450.450.4550.460.4650.470.47500.10.20.30.40.50.60.70.80.91*ROUGE-1MI(w/o MMR)MI(w/ MMR)WN(w/o MMR)WN(w/ MMR)Figure 2. ROUGE-1 vs. *0.170.1750.180.1850.190.1950.20.2050.2100.10.20.30.40.50.60.70.80.91*ROUGE-2MI(w/o MMR)MI(w/ MMR)WN(w/o MMR)WN(w/ MMR)Figure 3. ROUGE-2 vs. *558

0.1510.1530.1550.1570.1590.1610.1630.16500.10.20.30.40.50.60.70.80.91*ROUGE-WMI(w/o MMR)MI(w/ MMR)WN(w/o MMR)WN(w/ MMR)Figure 4. ROUGE-W vs. *Seen from Figures 2-4, no matter whether the MMR algorithm is applied or not (i.e. w/o MMR or w/ MMR), the ROUGE scores based on either word semantic measure (MI or WN) achieves the peak when *is set between 0.4 and 0.6. The per-formance values decrease sharply when *is very large (near to 1) or very small (near to 0). The curves demonstrate that both the contribution from the sentences and the contribution from the words are important for ranking sentences; moreover, the contributions are almost equally important. Loss of either contribution will much deteriorate the final performance.  Similar results and observations have been ob-tained on task 1 of DUC2001 in our study and the details are omitted due to page limit. 4.2 Keyword Evaluation 4.1.1   Evaluation Setup In this study we performed a preliminary evalua-tion of keyword extraction. The evaluation was conducted on the single word level instead of the multi-word phrase (n-gram) level, in other words, we compared the automatically extracted unigrams (words) and the manually labeled unigrams (words). The reasons were that: 1) there existed partial matching between phrases and it was not trivial to define an accurate measure to evaluate phrase quality; 2) each phrase was in fact com-posed of a few words, so the keyphrases could be obtained by combining the consecutive keywords.  We used 34 documents in the first five docu-ment clusters in DUC2002 dataset (i.e. d061-d065).  At most 10 salient words were manually labeled for each document to represent the document and the average number of manually assigned key-words was 6.8. Each approach returned 10 words with highest saliency scores as the keywords. The extracted 10 words were compared with the manu-ally labeled keywords. The words were converted to their corresponding basic forms based on WordNet before comparison. The precision p,re-call r,F-measure (F=2pr/(p+r)) were obtained for each document and then the values were averaged over all documents for evaluation purpose. 4.1.2 Evaluation Results Table 3 gives the comparison results. The proposed approaches are compared with two baselines: WordRank and MutualRank. WordRank is pro-posed in Mihalcea and Tarau (2004) to make use of only the co-occurrence relationships between words to rank words, which outperforms tradi-tional keyword extraction methods. The window size kfor WordRank is also set to 2, 5 and 10, re-spectively.  System Precision Recall F-measureOur Approach(WN) 0.413 0.504 0.454 Our Approach(MI:k=2) 0.428 0.485 0.455 Our Approach(MI:k=5) 0.425 0.491 0.456 Our Approach(MI:k=10) 0.393 0.455 0.422 WordRank (k=2) 0.373 0.412 0.392 WordRank (k=5) 0.368 0.422 0.393 WordRank (k=10) 0.379 0.407 0.393 MutualRank 0.355 0.397 0.375 Table 3. The Performance of Keyword Extraction  Seen from the table, the proposed approaches significantly outperform the baseline approaches. Both the corpus-based measure (MI) and the knowledge-based measure (WN) perform well on the task of keyword extraction. Arunning example is given below to demon-strate the results: Document ID:D062/AP891018-0301 Labeled keywords:insurance earthquake insurer damage california Francisco pay Extracted keywords:WN:insurance earthquake insurer quake california spokesman cost million wednesday damage MI(k=5):insurance insurer earthquake percent benefit california property damage estimate rate 559

5Conclusion and Future Work In this paper we propose a novel approach to si-multaneously document summarization and key-word extraction for single documents by fusing the sentence-to-sentence, word-to-word, sentence-to-word relationships in a unified framework. The semantics between words computed by either cor-pus-based approach or knowledge-based approach can be incorporated into the framework in a natural way. Evaluation results demonstrate the perform-ance improvement of the proposed approach over the baselines for both tasks. In this study, only the mutual information meas-ure and the vector measure are employed to com-pute word semantics, and in future work many other measures mentioned earlier will be investi-gated in the framework in order to show the ro-bustness of the framework. The evaluation of key-word extraction is preliminary in this study, and we will conduct more thorough experiments to make the results more convincing. Furthermore, the proposed approach will be applied to multi-document summarization and keyword extraction, which are considered more difficult than single document summarization and keyword extraction. Acknowledgements This work was supported by the National Science Foundation of China (60642001). References M. R. Amini and P. Gallinari. 2002. The use of unlabeled data to improve supervised learning for text summarization. In Pro-ceedings of SIGIR2002,105-112. S. Brin and L. Page. 1998. The anatomy of a large-scale hypertex-tual Web search engine. Computer Networks and ISDN Sys-tems,30(1–7). J. Carbonell and J. Goldstein. 1998. The use of MMR, diversity-based reranking for reordering documents and producing summaries. In Proceedings of SIGIR-1998,335-336. J. M. Conroy and D. P. O’Leary. 2001. Text summarization via Hidden Markov Models. In Proceedings of SIGIR2001,406-407. DUC. 2002. The Document Understanding Workshop 2002. http://www-nlpir.nist.gov/projects/duc/guidelines/2002.html T. Dunning. 1993. Accurate methods for the statistics of surprise and coincidence. Computational Linguistics 19,61–74. G. ErKan and D. R. Radev. 2004. LexPageRank: Prestige in multi-document text summarization. In Proceedings of EMNLP2004.C. Fellbaum. 1998. WordNet: An Electronic Lexical Database. The MIT Press.  E. Frank, G. W. Paynter, I. H. Witten, C. Gutwin, and C. G. Nevill-Manning. 1999. Domain-specific keyphrase extraction. Proceedings of IJCAI-99,pp. 668-673.  Y. H. Gong and X. Liu. 2001. Generic text summarization using Relevance Measure and Latent Semantic Analysis. In Proceed-ings of SIGIR2001,19-25. E. Hovy and C. Y. Lin. 1997. Automated text summarization in SUMMARIST. In Proceeding of ACL’1997/EACL’1997 Wor-shop on Intelligent Scalable Text Summarization.A. Hulth. 2003. Improved automatic keyword extraction given more linguistic knowledge. In Proceedings of EMNLP2003,Japan, August. J. M. Kleinberg. 1999. Authoritative sources in a hyperlinked environment. Journal of the ACM,46(5):604–632. B. Krulwich and C. Burkey. 1996. Learning user information interests through the extraction of semantically significant phrases. In AAAI 1996 Spring Symposium on Machine Learn-ing in Information Access.J. Kupiec, J. Pedersen, and F. Chen. 1995. A.trainable document summarizer. In Proceedings of SIGIR1995, 68-73. T. K. Landauer, P. Foltz, and D. Laham. 1998. Introduction to latent semantic analysis. Discourse Processes 25. C. Y. Lin and  E. Hovy. 2000. The automated acquisition of topic signatures for text Summarization. In Proceedings of ACL-2000,495-501. C.Y. Lin and E.H. Hovy. 2003. Automatic evaluation of summa-ries using n-gram co-occurrence statistics. In Proceedings of HLT-NAACL2003,Edmonton, Canada, May. R. Mihalcea, C. Corley, and C. Strapparava. 2006. Corpus-based and knowledge-based measures of text semantic similarity. In Proceedings of AAAI-06.R. Mihalcea and P. Tarau. 2004. TextRank: Bringing order into texts. In Proceedings of EMNLP2004.R. Mihalcea and P.Tarau. 2005. A language independent algo-rithm for single and multiple document summarization. In Proceedings of IJCNLP2005.A. Muñoz. 1996. Compound key word generation from document databases using a hierarchical clustering ART model. Intelli-gent Data Analysis, 1(1). T. Nomoto and Y. Matsumoto. 2001. A new approach to unsuper-vised text summarization. In Proceedings of SIGIR2001,26-34. S. Patwardhan. 2003. Incorporating dictionary and corpus infor-mation into a context vector measure of semantic relatedness. Master’s thesis,Univ. of Minnesota, Duluth. T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004. Word-Net::Similarity – Measuring the relatedness of concepts. In Proceedings of AAAI-04.D. Shen, J.-T. Sun, H. Li, Q. Yang, and Z. Chen. 2007. Document Summarization using Conditional Random Fields. In Proceed-ings of IJCAI 07.A. M. Steier and R. K. Belew. 1993. Exporting phrases: A statisti-cal analysis of topical language.  In Proceedings of Second Symposium on Document Analysis and Information Retrieval,pp. 179-190. P. D. Turney. 2000. Learning algorithms for keyphrase extraction. Information Retrieval,2:303-336. P. Turney. 2001. Mining the web for synonyms: PMI-IR versus LSA on TOEFL. In Proceedings of ECML-2001.I. H. Witten, G. W. Paynter, E. Frank, C. Gutwin, and C. G. Nevill-Manning. 1999. KEA: Practical automatic keyphrase extraction. Proceedings of Digital Libraries 99 (DL'99), pp. 254-256. H. Y. Zha. 2002. Generic summarization and keyphrase extraction using mutual reinforcement principle and sentence clustering. In Proceedings of SIGIR2002,pp.113-120. Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 560–567,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

560

FastSemanticExtractionUsingaNovelNeuralNetworkArchitectureRonanCollobertNECLaboratoriesAmerica,Inc.4IndependenceWaySuite200,Princeton,NJ08540collober@nec-labs.comJasonWestonNECLaboratoriesAmerica,Inc.4IndependenceWaySuite200,Princeton,NJ08540jasonw@nec-labs.comAbstractWedescribeanovelneuralnetworkarchi-tecturefortheproblemofsemanticrolela-beling.Manycurrentsolutionsarecompli-cated,consistofseveralstagesandhand-builtfeatures,andaretooslowtobeappliedaspartofrealapplicationsthatrequiresuchsemanticlabels,partlybecauseoftheiruseofasyntacticparser(Pradhanetal.,2004;GildeaandJurafsky,2002).Ourmethodin-steadlearnsadirectmappingfromsourcesentencetosemantictagsforagivenpred-icatewithouttheaidofaparserorachun-ker.Ourresultingsystemobtainsaccuraciescomparabletothecurrentstate-of-the-artatafractionofthecomputationalcost.1IntroductionSemanticunderstandingplaysanimportantroleinmanyend-userapplicationsinvolvingtext:forinfor-mationextraction,web-crawlingsystems,questionandanswerbasedsystems,aswellasmachinetrans-lation,summarizationandsearch.Suchapplicationstypicallyhavetobecomputationallycheaptodealwithanenormousquantityofdata,e.g.web-basedsystemsprocesslargenumbersofdocuments,whilstinteractivehuman-machineapplicationsrequireal-mostinstantresponse.Anotherissueisthecostofproducinglabeledtrainingdatarequiredforstatisti-calmodels,whichisexacerbatedwhenthosemodelsalsodependonsyntacticfeatureswhichmustthem-selvesbelearnt.Toachievethegoalofsemanticunderstanding,thecurrentconsensusistodivideandconquerthe[Thecompany]ARG0[bought]REL[sugar]ARG1[ontheworldmarket]ARGM-LOC[tomeetexportcommitments]ARGM-PNCFigure1:ExampleofSemanticRoleLabelingfromthePropBankdataset(Palmeretal.,2005).ARG0istypicallyanactor,RELanaction,ARG1anob-ject,andARGMdescribevariousmodiﬁerssuchaslocation(LOC)andpurpose(PNC).problem.Researcherstackleseverallayersofpro-cessingtasksrangingfromthesyntactic,suchaspart-of-speechlabelingandparsing,tothesemantic:word-sensedisambiguation,semanticrole-labeling,namedentityextraction,co-referenceresolutionandentailment.Noneofthesetasksareendgoalsinthemselvesbutcanbeseenaslayersoffeatureex-tractionthatcanhelpinalanguage-basedendap-plication,suchastheonesdescribedabove.Un-fortunately,thestate-of-the-artsolutionsofmanyofthesetasksaresimplytooslowtobeusedintheap-plicationspreviouslydescribed.Forexample,state-of-the-artsyntacticparserstheoreticallyhavecubiccomplexityinthesentencelength(Younger,1967)1andseveralsemanticextractionalgorithmsusetheparsetreeasaninitialfeature.Inthiswork,wedescribeanoveltypeofneuralnetworkarchitecturethatcouldhelptosolvesomeoftheseissues.Wefocusourexperimentalstudyonthesemanticrolelabelingproblem(Palmeretal.,2005):beingabletogiveasemanticroletoasyn-1Eventhoughsomeparserseffectivelyexhibitlinearbe-haviorinsentencelength(Ratnaparkhi,1997),faststatisticalparserssuchas(Henderson,2004)stilltakearound1.5secondsforsentencesoflength35inteststhatwemade.561

tacticconstituentofasentence,i.e.annotatingthepredicateargumentstructureintext(seeforexam-pleFigure1).Becauseofitsnature,rolelabelingseemstorequirethesyntacticanalysisofasentencebeforeattributingsemanticlabels.Usingthisintu-ition,state-of-the-artsystemsﬁrstbuildaparsetree,andsyntacticconstituentsarethenlabeledbyfeed-inghand-builtfeaturesextractedfromtheparsetreetoamachinelearningsystem,e.g.theASSERTsys-tem(Pradhanetal.,2004).Thisisratherslow,tak-ingafewsecondspersentenceattesttime,partlybecauseoftheparsetreecomponent,andpartlybe-causeoftheuseofSupportVectorMachines(Boseretal.,1992),whichhavelinearcomplexityintest-ingtimewithrespecttothenumberoftrainingex-amples.Thismakesithardtoapplythismethodtointerestingenduserapplications.Here,weproposearadicallydifferentapproachthatavoidsthemorecomplextaskofbuildingafullparsetree.Fromamachinelearningpointofview,ahumandoesnotneedtobetaughtaboutparsetreestotalk.Itispossible,however,thatourbrainsmayimplicitlylearnfeatureshighlycorrelatedwiththoseextractedfromaparsetree.Weproposetodevelopanarchitecturethatimplementsthiskindofimplicitlearning,ratherthanusingexplicitlyengineeredfea-tures.Inpractice,oursystemalsoprovidessemantictagsatafractionofthecomputationalcostofothermethods,takingonaverage0.02secondstolabelasentencefromthePennTreebank,withalmostnolossinaccuracy.Therestofthearticleisasfollows.First,wede-scribetheproblemofshallowsemanticparsinginmoredetail,aswellasexistingsolutionstothisprob-lem.Wethendetailouralgorithmicapproach–theneuralnetworkarchitectureweemploy–followedbyexperimentsthatevaluateourmethod.Finally,weconcludewithasummaryanddiscussionoffu-turework.2ShallowSemanticParsingFrameNet(Bakeretal.,1998)andthePropositionBank(Palmeretal.,2005),orPropBankforshort,arethetwomainsystemscurrentlydevelopedforsemanticrole-labelingannotation.WefocushereonPropBank.PropBankencodesrolelabelsbyse-manticallytaggingthesyntacticstructuresofhandannotatedparsesofsentences.Thecurrentversionofthedatasetgivessemantictagsforthesamesen-tencesasinthePennTreebank(Marcusetal.,1993),whichareexcerptsfromtheWallStreetJournal.Thecentralideaisthateachverbinasentenceisla-beledwithitspropositionalarguments,wheretheabstractnumberedargumentsareintendedtoﬁlltyp-icalroles.Forexample,ARG0istypicallytheactor,andARG1istypicallythethingactedupon.Thepreciseusageofthenumberingsystemislabeledforeachparticularverbasso-calledframes.Addition-ally,semanticrolescanalsobelabeledwithoneof13ARGMadjunctlabels,suchasARGM-LOCorARGM-TMPforadditionallocationalortemporalinformationrelativetosomeverb.Shallowsemanticparsinghasimmediateapplica-tionsintaskssuchasmeta-dataextraction(e.g.fromwebdocuments)andquestionandanswerbasedsys-tems(e.g.callcentersystems),amongstothers.3PreviousWorkSeveralauthorshavealreadyattemptedtobuildma-chinelearningapproachesforthesemanticrole-labelingproblem.In(GildeaandJurafsky,2002)theauthorspresentedastatisticalapproachtolearn-ing(forFrameNet),withsomesuccess.Theypro-posedtotakeadvantageofthesyntactictreestruc-turethatcanbepredictedbyaparser,suchasChar-niak’sparser(Charniak,2000).Theiraimis,givenanodeintheparsetree,toassignasemanticrolelabeltothewordsthatarethechildrenofthatnode.Theyextractseveralkeytypesoffeaturesfromtheparsetreetobeusedinastatisticalmodelforpre-diction.Thesesamefeaturesalsoprovedcrucialtosubsequentapproaches,e.g.(Pradhanetal.,2004).Thesefeaturesinclude:•Thepartsofspeechandsyntacticlabelsofwordsandnodesinthetree.•Thenode’sposition(leftorright)inrelationtotheverb.•Thesyntacticpathtotheverbintheparsetree.•Whetheranodeintheparsetreeispartofanounorverbphrase(bylookingattheparentnodesofthatnode).562

•Thevoiceofthesentence:activeorpassive(partofthePropBankgoldannotation);aswellasseveralotherfeatures(predicate,headword,verbsub-categorization,...).Theauthorsof(Pradhanetal.,2004)usedasimilarstructure,butaddedmorefeatures,notablyheadwordpart-of-speech,thepredictednameden-tityclassoftheargument,wordsensedisambigua-tionoftheverbandverbclustering,andothers(theyadd25variantsof12newfeaturetypesoverall.)Theirsystemalsousesaparser,asbefore,andthenapolynomialSupportVectorMachine(SVM)(Boseretal.,1992)isusedintwofurtherstages:toclas-sifyeachnodeinthetreeasbeingasemanticar-gumentornotforagivenverb;andthentoclas-sifyeachsemanticargumentintooneoftheclasses(ARG1,ARG2,etc.).TheﬁrstSVMsolvesatwo-classproblem,thesecondsolvesamulti-classprob-lemusingaone-vs-the-restapproach.Theﬁnalsys-tem,calledASSERT,givesstate-of-the-artperfor-manceandisalsofreelyavailableat:http://oak.colorado.edu/assert/.WecomparetothissysteminourexperimentalresultsinSec-tion5.Severalothercompetingmethodsexist,e.g.theonesthatparticipatedintheCONLL2004and2005challenges(http://www.lsi.upc.edu/˜srlconll/st05/st05.html).InthispaperwefocusonacomparisonwithASSERTbecausesoftwaretore-runitisavailableonline.Thisalsogivesusatimingresultforcomparisonpurposes.Thethree-stepprocedureusedinASSERT(calcu-latingaparsetreeandthenapplyingSVMstwice)leadstogoodclassiﬁcationperformance,buthasseveraldrawbacks.Firstinspeed:predictingaparsetreeisextremelydemandingincomputingre-sources.Secondly,choosingthefeaturesnecessaryforSVMclassiﬁcationrequiresextensiveresearch.Finally,theSVMclassiﬁcationalgorithmusedinex-istingapproachesisratherslow:SVMtrainingisatleastquadraticintimewithrespecttothenumberoftrainingexamples.Thenumberofsupportvec-torsinvolvedintheSVMdecisionfunctionalsoin-creaseslinearlywiththenumberoftrainingexam-ples.ThismakesSVMsslowonlarge-scaleprob-lems,bothduringtrainingandtestingphases.Toalleviatetheburdenofparsetreecomputation,severalattemptshavebeenmadetoremovethefullparsetreeinformationfromthesemanticrolelabel-ingsystem,infactthesharedtaskofCONLL2004wasdevotedtothisgoal,buttheresultswerenotcompletelysatisfactory.Previously,in(GildeaandPalmer,2001),theauthorstriedtoshowthattheparsetreeisnecessaryforgoodgeneralizationbyshowingthatsegmentsderivedfromashallowsyn-tacticparserorchunkerdonotperformaswellforthisgoal.Afurtheranalysisofusingchunkers,withimprovedresultswasalsogivenin(Punyakanoketal.,2005),butstillconcludedthefullparsetreeismostuseful.4NeuralNetworkArchitectureIdeally,wewantanend-to-endfastlearningsystemtooutputsemanticrolesforsyntacticconstituentswithoutusingatimeconsumingparsetree.Also,asexplainedbefore,weareinterestinginexploringwhethermachinelearningapproachescanlearnstructureimplicitly.Hence,evenifthereisadeeprelationshipbetweensyntaxandsemantics,weprefertoavoidhand-engineeredfeaturesthatexploitthis,andseeifwecandevelopamodelthatcanlearnthesefeaturesinstead.Wearethusnotinterestedinchunker-basedtechniques,eventhoughtheyarefasterthanparser-basedtechniques.Weproposehereaneuralnetworkbasedarchitec-turewhichachievesthesetwogoals.4.1BasicArchitectureThetypeofneuralnetworkthatweemployisaMultiLayerPerceptron(MLP).MLPshavebeenusedformanyyearsinthemachinelearningﬁeldandslowlyabandonedforseveralreasons:partlybecauseofthedifﬁcultyofsolvingthenon-convexoptimizationproblemsassociatedwithlearning(LeCunetal.,1998),andpartlybecauseofthedifﬁcultyoftheirtheoreticalanalysiscomparedtoalternativeconvexapproaches.AnMLPworksbysuccessivelyprojectingthedatatobeclassiﬁedintodifferentspaces.Theseprojectionsaredoneinwhatiscalledhiddenlay-ers.Givenaninputvectorz,ahiddenlayerappliesalineartransformation(matrixM)followedbyasquashingfunctionh:z7→Mz7→h(Mz).(1)563

Atypicalsquashingfunctionisthehyperbolictan-genth(·)=tanh(·).Thelastlayer(theoutputlayer)linearlyseparatestheclasses.Thecomposi-tionoftheprojectionsinthehiddenlayerscouldbeviewedastheworkdonebythekernelinSVMs.Howeverthereisaveryimportantdifference:thekernelinSVMisﬁxedandarbitrarilychosen,whilethehiddenlayersinanMLParetrainedandadaptedtotheclassiﬁcationtask.Thisallowsustocreatemuchmoreﬂexibleclassiﬁcationarchitectures.Ourmethodforsemanticrolelabelingclassiﬁeseachwordofasentenceseparately.Wedonotuseanysemanticconstituentinformation:ifthemodelispowerfulenough,wordsinthesamesemanticconstituentshouldhavethesameclasslabel.Thismeanswealsodonotseparatetheproblemintoanidentiﬁcationandclassiﬁcationphase,butrathersolveinasinglestep.4.1.1NotationWerepresentwordsasindices.Weconsideraﬁ-nitedictionaryofwordsD⊂N.Letusrepresentasentenceofnwwordstobeanalyzedasafunctions(·).Theithwordinthesentenceisgivenbytheindexs(i):1≤i≤nws(i)∈D.Weareinterestedinpredictingthesemanticrolela-belofthewordatpositionposw,givenaverbatpo-sitionposv(1≤posw,posv≤nw).Amathemati-caldescriptionofournetworkarchitectureschemat-icallyshowninFigure2follows.4.1.2TransformingwordsintofeaturevectorsOurﬁrstconcerninsemanticrolelabelingisthatwehavetodealwithwords,andthatasimplein-dexi∈Ddoesnotcarryanyinformationspeciﬁctoaword:foreachwordweneedasetoffeaturesrelevantforthetask.Asdescribedearlier,previousmethodsconstructaparsetree,andthencomputehand-builtfeatureswhicharethenfedtoaclassi-ﬁcationalgorithm.Inordertobypasstheuseofaparsetree,weconverteachwordi∈Dintoapar-ticularvectorwi∈Rdwhichisgoingtobelearntforthetaskweareinterestedin.Thisapproachhasalreadybeenusedwithgreatsuccessinthedomainoflanguagemodels(BengioandDucharme,2001;SchwenkandGauvain,2002).      Lookup Tabled...dLinear Layer with sentence−adapted columnsdC(position w.r.t. cat, position w.r.t. sat)Softmax Squashing Layer...ARG1ARG2ARGMLOCARGMTMPClassical Linear LayerTanh Squashing LayernhuCiws(6)ws(2)s(1)w...C1C2C6Classical Linear Layerws(6)...ws(2)s(1)ws(1)  s(2)   ...                   s(6)sattheInput Sentenceon the matcatFigure2:MLParchitectureforshallowsemanticparsing.Theinputsequenceisatthetop.Theout-putclassprobabilitiesforthewordofinterest(“cat”)giventheverbofinterest(“sat”)aregivenatthebot-tom.TheﬁrstlayerofourMLPisthusalookuptablewhichreplacesthewordindicesintoaconcatenationofvectors:{s(1),...,s(nw)}7→(ws(1)...ws(nw))∈Rnwd.(2)Theweights{wi|i∈D}forthislayerareconsid-eredduringthebackpropagationphaseoftheMLP,andthusadaptedautomaticallyforthetaskweareinterestedin.4.1.3IntegratingtheverbpositionFeedingwordvectorsalonetoalinearclassiﬁca-tionlayerasin(BengioandDucharme,2001)leads564

toverypooraccuracybecausethesemanticclassiﬁ-cationofagivenwordalsodependsontheverbinquestion.WeneedtoprovidetheMLPwithinfor-mationabouttheverbpositionwithinthesentence.Forthatpurposeweuseakindoflinearlayerwhichisadaptedtothesentenceconsidered.Ittakestheform:(ws(1)...ws(nw))7→MwTs(1)...wTs(nw),whereM∈Rnhu×nwd,andnhuisthenumberofhiddenunits.ThespeciﬁcnatureofthislayeristhatthematrixMhasaspecialblock-columnformwhichdependsonthesentence:M=(C1|...|Cnw),whereeachcolumnCi∈Rnhu×ddependsonthepositionoftheithwordins(·),withrespecttothepositionposwofthewordofinterest,andwithre-specttothepositionposvoftheverbofinterest:Ci=C(i−posw,i−posv),whereC(·,·)isafunctiontobechosen.InourexperimentsC(·,·)wasalinearlayerwithdiscretizedinputs(i−posw,i−posv)whichweretransformedintotwobinaryvectorsofsizewsz,whereabitissetto1ifitcorrespondstothepo-sitiontoencode,and0otherwise.Thesetwobinaryvectorsarethenconcatenatedandfedtothelinearlayer.Wechosethe“windowsize”wsz=11.Ifapositionliesoutsidethewindow,thenwestillsettheleftmostorrightmostbitto1.Theparametersin-volvedinthisfunctionarealsoconsideredduringthebackpropagation.Withsuchanarchitectureweal-lowourMLPtoautomaticallyadapttheimportanceofawordinthesentencegivenitsdistancetothewordwewanttoclassify,andtotheverbwearein-terestedin.Thisideaisthemajornoveltyinthiswork,andiscrucialforthesuccessoftheentirearchitecture,aswewillseeintheexperiments.4.1.4LearningclassprobabilitiesThelastlayerinourMLPisaclassicallinearlayerasdescribedin(1),withasoftmaxsquashingfunction(Bridle,1990).Considering(1)andgiven˜z=Mz,wehavehi(˜z)=exp˜ziPjexp˜zj.Thisallowsustointerpretoutputsasprobabilitiesforeachsemanticrolelabel.Thetrainingofthewholesystemisachievedusinganormalstochasticgradientdescent.4.2WordrepresentationAswehaveseen,inourmodelwearelearningoneddimensionalvectortorepresenteachword.Ifthedatasetwerelargeenough,thiswouldbeanelegantsolution.InpracticemanywordsoccurinfrequentlywithinPropBank,so(independentofthesizeofd)wecanstillonlylearnaverypoorrepresentationforwordsthatonlyappearafewtimes.Hence,tocon-trolthecapacityofourmodelwetaketheoriginalwordandreplaceitwithitspart-of-speechifitisaverb,noun,adjective,adverbornumberasdeter-minedbyapart-of-speechclassiﬁer,andkeepthewordsforallotherpartsofspeech.Thisclassiﬁerisitselfaneuralnetwork.Thiswaywekeeplinkingwordswhichareimportantforthistask.Wedonotdothisreplacementforthepredicateitself.5ExperimentsWeusedSections02-21ofthePropBankdatasetversion1fortrainingandvalidationandSection23fortestingasstandardinallourexperiments.Weﬁrstdescribethepart-of-speechtaggerweem-ploy,andthendescribeoursemanticrolelabelingexperiments.Softwareforourmethod,SENNA(Se-manticExtractionusingaNeuralNetworkArchi-tecture),moredetailsonitsimplementation,anon-lineappletandtestsetpredictionsofoursystemincomparisontoASSERTcanbefoundathttp://ml.nec-labs.com/software/senna.Part-Of-SpeechTaggerThepart-of-speechclas-siﬁerweemployisaneuralnetworkarchitectureofthesametypeasinSection4,wherethefunctionCi=C(i−posw)dependsnowonlyonthewordposition,andnotonaverb.Moreprecisely:Ci=(cid:26)0if2|i−posw|>wsz−1Wi−poswotherwise,565

whereWk∈Rnhu×dandwszisawindowsize.Wechosewsz=5inourexperiments.Thed-dimensionalvectorslearnttakeintoaccountthecapitalizationofaword,andthepreﬁxandsuf-ﬁxcalculatedusingPorter-Stemmer.Seehttp://ml.nec-labs.com/software/sennaformoredetails.WetrainedonthetrainingsetofProp-BanksupplementedwiththeBrowncorpus,result-inginatestaccuracyonthetestsetofPropBankof96.85%whichcomparesto96.66%usingtheBrilltagger(Brill,1992).SemanticRoleLabelingInourexperimentsweconsidereda23-classproblemofNULL(nola-bel),thecoreargumentsARG0-5,REL,ARGA,andARGM-alongwiththe13secondarymodiﬁerlabelssuchasARGM-LOCandARGM-TMP.Wesimpli-ﬁedR-ARGnandC-ARGntobewrittenasARGn,andpost-processedASSERTtodothisaswell.WecomparedoursystemtothefreelyavailableASSERTsystem(Pradhanetal.,2004).Bothsys-temsarefedonlytheinputsentenceduringtesting,withtracesremoved,sotheycannotmakeuseofmanyPropBankfeaturessuchasframesetidentiti-ﬁer,person,tense,aspect,voice,andformoftheverb.Asouralgorithmoutputsasemantictagforeachwordofasentence,wedirectlycomparethisper-wordaccuracywithASSERT.BecauseASSERTusesaparser,andbecausePropBankwasbuiltbyla-belingthenodesofahand-annotatedparsetree,per-nodeaccuracyisusuallyreportedinpaperssuchas(Pradhanetal.,2004).Unfortunatelyourapproachisbasedonacompletelydifferentpremise:wetagwords,notsyntacticconstituentscomingfromtheparser.WediscussthisfurtherinSection5.2.Theper-wordaccuracycomparisonresultscanbeseeninTable5.Beforelabelingthesemanticrolesofeachpredicate,onemustﬁrstidentifythepred-icatesthemselves.Ifapredicateisnotidentiﬁed,NULLtagsareassignedtoeachwordforthatpred-icate.Theﬁrstlineofresultsinthetabletakesintoaccountthisidentiﬁcationprocess.Fortheneuralnetwork,weusedourpart-of-speechtaggertoper-formthisasaverb-detectiontask.WenoticedASSERTfailedtoidentifyrelativelymanypredicates.Inparticular,itseemspredicatessuchas“is”aresometimeslabeledasAUXbythepart-of-speechtagger,andsubsequentlyignored.Weinformedtheauthorsofthis,butwedidnotre-ceivearesponse.Todealwiththis,weconsideredtheadditionalaccuracy(secondrowinthetable)measuredoveronlythosesentenceswherethepred-icatewasidentiﬁedbyASSERT.TimingresultsTheper-sentencecomputetimeisalsogiveninTable5,averagedoverallsentencesinthetestset.Ourmethodisaround250timesfasterthanASSERT.ItisnotreallyfeasibletorunAS-SERTformostapplications.MeasurementNNASSERTPer-wordaccuracy(allverbs)83.64%83.46%Per-wordaccuracy(ASSERTverbs)84.09%86.06%Per-sentencecomputetime(secs)0.02secs5.08secsTable1:ExperimentalcomparisonwithASSERT5.1AnalysisofourMLPWhilewegaveanintuitivejustiﬁcationofthearchi-tecturechoicesofourmodelinSection4,wenowgiveasystematicempiricalstudyofthosechoices.Firstofall,providingthepositionofthewordandthepredicateinfunctionC(·,·)isessential:thebestmodelweobtainedwithawindowaroundthewordonlygave51.3%,assumingcorrectidentiﬁcationofallpredicates.Ourbestmodelachieves83.95%inthissetting.Ifwedonotclusterthewordsaccordingtotheirpart-of-speech,wealsolosesomeperformance,ob-taining78.6%atbest.Ontheotherhand,clusteringallwords(suchasCC,DT,INpart-of-speechtags)alsogivesweakerresults(81.1%accuracyatbest).Webelievethatincludingallwordswouldgiveverygoodperformanceifthedatasetwaslargeenough,buttrainingonlyonPropBankleadstooverﬁtting,manywordsbeinginfrequent.Clusteringisawaytoﬁghtagainstoverﬁtting,bygroupinginfrequentwords:forexample,wordswiththelabelNNP,JJ,RB(whichwecluster)appearonaverage23,22and72timesrespectivelyinthetrainingset,whileCC,DT,IN(whichwedonotcluster)appear2420,5659and1712timesrespectively.566

Eventhoughsomeverbsareinfrequent,onecan-notclusterallverbsintoasinglegroup,aseachverbdictatesthetypesofsemanticrolesinthesentence,dependingonitsframe.Clusteringallwordsintotheirpart-of-speech,includingthepredicate,givesapoor73.8%comparedwith81.1%,whereevery-thingisclusteredapartfromthepredicate.Figure3givessomeanecdotalexamplesoftestsetpredictionsofourﬁnalmodelcomparedtoASSERT.5.2ArgumentClassiﬁcationAccuracySofarwehavenotusedthesameaccuracymeasuresasinpreviouswork(GildeaandJurafsky,2002;Pradhanetal.,2004).Currentlyourarchitectureisdesignedtolabelonaper-wordbasis,whileexistingsystemsperformasegmentationprocess,andthenlabelsegments.Whilewedonotoptimizeourmodelforthesamecriteria,itisstillpossibletomeasuretheaccuracyusingthesamemetrics.Wemeasuredtheargumentclassiﬁcationaccuracyofournetwork,as-sumingthecorrectsegmentationisgiventooursys-tem,asin(Pradhanetal.,2004),bypost-processingourper-wordtagstoformamajorityvoteovereachsegment.Thisgives83.18%accuracyforournet-workwhenwesupposethepredicatemustalsobeidentiﬁed,and80.53%fortheASSERTsoftware.MeasuringonlyonpredicatesidentiﬁedbyASSERTweinsteadobtain84.32%accuracyforournetwork,and87.02%forASSERT.6DiscussionWehaveintroducedaneuralnetworkarchitecturethatcanprovidecomputationallyefﬁcientsemanticroletagging.Itisalsoageneralarchitecturethatcouldbeappliedtootherproblemsaswell.Becauseournetworkcurrentlyoutputslabelsonaper-wordbasisitisdifﬁculttoassessexistingaccuracymea-sures.However,itshouldbepossibletocombineourapproachwithashallowparsertoenhanceper-formance,andmakecomparisonsmoredirect.Weconsiderthisworkasastartingpointfordif-ferentresearchdirections,includingthefollowingareas:•Incorporatinghand-builtfeaturesCurrently,theonlypriorknowledgeoursystemencodescomesfrompart-of-speechtags,instarkcon-trasttoothermethods.Ofcourse,performanceTRUTH:Hecampedoutatahigh-technervecenterontheﬂoorof[theBigBoard,where]ARGM-LOC[he]ARG0[could]ARGM-MOD[watch]REL[updatesonpricesandpend-ingstockorders]ARG1.ASSERT(68.7%):Hecampedoutatahigh-technervecenterontheﬂooroftheBigBoard,[where]ARGM-LOC[he]ARG0[could]ARGM-MOD[watch]REL[updates]ARG1onpricesandpendingstockorders.NN(100%):Hecampedoutatahigh-technervecenterontheﬂoorof[theBigBoard,where]ARGM-LOC[he]ARG0[could]ARGM-MOD[watch]REL[updatesonpricesandpend-ingstockorders]ARG1.TRUTH:[UnitedAutoWorkersLocal1069,which]ARG0[represents]REL[3,000workersatBoeing’shelicopterunitinDelawareCounty,Pa.]ARG1,saiditagreedtoextenditscontractonaday-by-daybasis,witha10-daynotiﬁcationtocancel,whileitcontinuesbargaining.ASSERT(100%):[UnitedAutoWorkersLocal1069,which]ARG0[represents]REL[3,000workersatBoeing’shelicopterunitinDelawareCounty,Pa.]ARG1,saiditagreedtoextenditscontractonaday-by-daybasis,witha10-daynotiﬁcationtocancel,whileitcontinuesbargaining.NN(89.1%):[UnitedAutoWorkersLocal1069,which]ARG0[represents]REL[3,000workersatBoeing’shelicopterunit]ARG1[inDelawareCounty]ARGM-LOC,Pa.,saiditagreedtoextenditscontractonaday-by-daybasis,witha10-daynotiﬁcationtocancel,whileitcontinuesbargaining.Figure3:TwoexamplesfromthePropBanktestset,showingNeuralNetandASSERTandgoldstandardlabelings,withper-wordaccuracyinbrackets.Notethateventhoughourlabelingdoesnotmatchthehand-annotatedoneinthesecondsentenceitstillseemstomakesomesenseas“inDelawareCounty”islabeledasalocationmodiﬁer.Thecompletesetofpredictionsonthetestsetcanbefoundathttp://ml.nec-labs.com/software/senna.wouldimprovewithmorehand-builtfeatures.Forexample,simplyaddingwhethereachwordispartofanounorverbphraseusingthehand-annotatedparsetree(theso-called“GOV”fea-turefrom(GildeaandJurafsky,2002))im-provestheperformanceofoursystemfrom83.95%to85.8%.Onemusttradethegener-alityofthemodelwithitsspeciﬁcity,andalsotakeintoaccounthowlongthefeaturestaketocompute.•IncorporatingsegmentinformationOursystemhasnopriorknowledgeaboutsegmentationintext.Thiscouldbeencodedinmanyways:mostobviouslybyusingachunker,butalsoby567

designingadifferentnetworkarchitecture,e.g.byencodingcontiguityconstraints.Toshowthelatterisuseful,usinghand-annotatedseg-mentstoforcecontiguitybymajorityvoteleadstoanimprovementfrom83.95%to85.6%.•Incorporatingknowninvariancesviavirtualtrainingdata.Inimagerecognitionproblemsitiscommontocreateartiﬁcialtrainingdatabytakingintoaccountinvariancesintheimages,e.g.viarotationandscale.Suchdataimprovesgeneralizationsubstantially.Itmaybepossibletoachievesimilarresultsfortext,by“warp-ing”trainingdatatocreatenewsentences,orbyconstructingsentencesfromscratchusingahand-builtgrammar.•Unlabeleddata.Ourrepresentationofwordsisasddimensionalvectors.Wecouldtrytoimprovethisrepresentationbylearningalan-guagemodelfromunlabeleddata(BengioandDucharme,2001).AsmanywordsinProp-Bankonlyappearafewtimes,therepresenta-tionmightimprove,eventhoughthelearningisunsupervised.ThismayalsomakethesystemgeneralizebettertotypesofdataotherthantheWallStreetJournal.•TransductiveInference.Finally,onecanalsouseunlabeleddataaspartofthesupervisedtrainingprocess,whichiscalledtransductionorsemi-supervisedlearning.Inparticular,weﬁndthepossibilityofusingun-labeleddata,invariancesandtheuseoftransduc-tionexciting.Thesepossibilitiesnaturallyﬁtintoourframework,whereasscalabilityissueswilllimittheirapplicationincompetingmethods.ReferencesC.F.Baker,C.J.Fillmore,andJ.B.Lowe.1998.TheBerkeleyFrameNetproject.ProceedingsofCOLING-ACL,98.Y.BengioandR.Ducharme.2001.Aneuralprobabilis-ticlanguagemodel.InAdvancesinNeuralInforma-tionProcessingSystems,NIPS13.B.E.Boser,I.M.Guyon,andV.N.Vapnik.1992.Atrain-ingalgorithmforoptimalmarginclassiﬁers.Proceed-ingsoftheﬁfthannualworkshoponComputationallearningtheory,pages144–152.J.S.Bridle.1990.Probabilisticinterpretationoffeed-forwardclassiﬁcationnetworkoutputs,withrelation-shipstostatisticalpatternrecognition.InF.FogelmanSouli´eandJ.H´erault,editors,Neurocomputing:Al-gorithms,ArchitecturesandApplications,pages227–236.NATOASISeries.E.Brill.1992.Asimplerule-basedpartofspeechtag-ger.ProceedingsoftheThirdConferenceonAppliedNaturalLanguageProcessing,pages152–155.E.Charniak.2000.Amaximum-entropy-inspiredparser.ProceedingsoftheﬁrstconferenceonNorthAmericanchapteroftheAssociationforComputationalLinguis-tics,pages132–139.D.GildeaandD.Jurafsky.2002.Automaticlabel-ingofsemanticroles.ComputationalLinguistics,28(3):245–288.D.GildeaandM.Palmer.2001.Thenecessityofpars-ingforpredicateargumentrecognition.Proceedingsofthe40thAnnualMeetingonAssociationforCom-putationalLinguistics,pages239–246.J.Henderson.2004.Discriminativetrainingofaneuralnetworkstatisticalparser.InProceedingsofthe42ndMeetingofAssociationforComputationalLinguistics.Y.LeCun,L.Bottou,G.B.Orr,andK.-R.M¨uller.1998.Efﬁcientbackprop.InG.B.OrrandK.-R.M¨uller,ed-itors,NeuralNetworks:TricksoftheTrade,pages9–50.Springer.M.P.Marcus,M.A.Marcinkiewicz,andB.Santorini.1993.BuildingalargeannotatedcorpusofEn-glish:thepenntreebank.ComputationalLinguistics,19(2):313–330.M.Palmer,D.Gildea,andP.Kingsbury.2005.Thepropositionbank:Anannotatedcorpusofsemanticroles.Comput.Linguist.,31(1):71–106.S.Pradhan,W.Ward,K.Hacioglu,J.Martin,andD.Ju-rafsky.2004.Shallowsemanticparsingusingsupportvectormachines.ProceedingsofHLT/NAACL-2004.V.Punyakanok,D.Roth,andW.Yih.2005.Thene-cessityofsyntacticparsingforsemanticrolelabeling.ProceedingsofIJCAI’05,pages1117–1123.A.Ratnaparkhi.1997.Alinearobservedtimestatisticalparserbasedonmaximumentropymodels.Proceed-ingsofEMNLP.H.SchwenkandJ.L.Gauvain.2002.Connection-istlanguagemodelingforlargevocabularycontinu-ousspeechrecognition.ProceedingsofICASSP’02.D.H.Younger.1967.Recognitionandparsingofcontext-freelanguagesintimen3.InformationandControl,10.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 568–575,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

568

ImprovingtheInterpretationofNounPhraseswithCross-linguisticInformationRoxanaGirjuUniversityofIllinoisatUrbana-Champaigngirju@uiuc.eduAbstractThispaperaddressestheautomaticclassiﬁ-cationofsemanticrelationsinnounphrasesbasedoncross-linguisticevidencefromasetofﬁveRomancelanguages.AsetofnovelsemanticandcontextualEnglish–RomanceNPfeaturesisderivedbasedonempiricalobservationsonthedistributionofthesyntaxandmeaningofnounphrasesontwocorporaofdifferentgenre(EuroparlandCLUVI).ThefeatureswereemployedinaSupportVectorMachinesalgorithmwhichachievedanaccuracyof77.9%(Eu-roparl)and74.31%(CLUVI),animprove-mentcomparedwithtwostate-of-the-artmodelsreportedintheliterature.1IntroductionSemanticknowledgeisveryimportantforanyap-plicationthatrequiresadeepunderstandingofnatu-rallanguage.Theautomaticacquisitionofsemanticinformationintexthasbecomeincreasinglyimpor-tantinontologydevelopment,informationextrac-tion,questionanswering,andotheradvancednaturallanguageprocessingapplications.Inthispaperwepresentamodelfortheauto-maticsemanticinterpretationofnounphrases(NPs),whichisthetaskofdeterminingthesemanticre-lationamongthenounconstituents.Forexample,familyestateencodesaPOSSESSIONrelation,whiledressofsilkreferstoPART-WHOLE.Theproblem,whilesimpletostateishardtosolve.Therea-sonisthatthemeaningoftheseconstructionsismostofthetimeambiguousorimplicit.InterpretingNPscorrectlyrequiresvarioustypesofinformationfromworldknowledgetocomplexcontextfeatures.Moreover,theextensionofthistasktoothernatu-rallanguagesbringsforwardnewissuesandprob-lems.Forinstance,beerglasstranslatesintotarrodecervezainSpanish,bicchieredabirrainItalian,verre`abi`ereinFrench,andpahardebereinRoma-nian.Thus,animportantresearchquestionishowdothesyntacticconstructionsinthetargetlanguagecontributetothepreservationofmeaningincontext.Inthispaperweinvestigatenounphrasesbasedoncross-linguisticevidenceandpresentadomaininde-pendentmodelfortheirsemanticinterpretation.WeaimatuncoveringthegeneralaspectsthatgovernthesemanticsofNPsinEnglishbasedonasetofﬁveRomancelanguages:Spanish,Italian,French,Portuguese,andRomanian.ThefocusonRomancelanguagesiswellmotivated.ItismostlytruethatEnglishnounphrasestranslateintoconstructionsoftheformNPNinRomancelanguageswhere,aswewillshowbelow,theP(preposition)variesinwaysthatcorrelatewiththesemantics.ThusRo-mancelanguageswillgiveusanothersourceofevi-dencefordisambiguatingthesemanticrelationsinEnglishNPs.Wealsopresentempiricalobserva-tionsonthedistributionofthesyntaxandmeaningofnounphrasesontwodifferentcorporabasedontwostate-of-the-artclassiﬁcationtagsets:Lauer’ssetof8prepositions(Lauer,1995)andourlistof22semanticrelations.Weshowthatvariouscrosslin-gualcuescanhelpintheNPinterpretationtaskwhenemployedinanSVMmodel.Theresultsarecom-paredagainsttwostateoftheartapproaches:asu-569

pervisedmachinelearningmodel,SemanticScatter-ing(MoldovanandBadulescu,2005),andaweb-basedprobabilisticmodel(LapataandKeller,2004).Thepaperisorganizedasfollows.InSection2wepresentasummaryofthepreviouswork.Sec-tion3liststhesyntacticandsemanticinterpretationcategoriesusedalongwithobservationsregardingtheirdistributiononthetwodifferentcross-lingualcorpora.Sections4and5presentalearningmodelandresultsfortheinterpretationofEnglishnounphrases.Finally,inSection6weoffersomedis-cussionandconclusions.2RelatedWorkCurrently,thebest-performingNPinterpretationmethodsincomputationallinguisticsfocusmostlyontwoconsecutivenouninstances(nouncom-pounds)andrelyeitheronratherad-hoc,domain-speciﬁcsemantictaxonomies,oronstatisticalmod-elsonlargecollectionsofunlabeleddata.Recentresultshaveshownthatsymbolicnouncompoundinterpretationsystemsusingmachinelearningtech-niquescoupledwithalargelexicalhierarchyper-formwithverygoodaccuracy,buttheyaremostofthetimetailoredtoaspeciﬁcdomain(RosarioandHearst,2001).Ontheotherhand,themajorityofcorpusstatisticsapproachestonouncompoundin-terpretationcollectstatisticsontheoccurrencefre-quencyofthenounconstituentsandusetheminaprobabilisticmodel(Lauer,1995).Morerecently,(LapataandKeller,2004)showedthatsimpleunsu-pervisedmodelsperformsigniﬁcantlybetterwhenthefrequenciesareobtainedfromtheweb,ratherthanfromalargestandardcorpus.Otherresearchers(PantelandPennacchiotti,2006),(Snowetal.,2006)useclusteringtechniquescoupledwithsyntacticde-pendencyfeaturestoidentifyIS-Arelationsinlargetextcollections.(KimandBaldwin,2006)and(Tur-ney,2006)focusonthelexicalsimilarityofunseennouncompoundswiththosefoundintraining.However,althoughtheweb-basedsolutionmightovercomethedatasparsenessproblem,thecurrentprobabilisticmodelsarelimitedbythelackofdeeplinguisticinformation.Inthispaperweinvestigatetheroleofcross-linguisticinformationinthetaskofEnglishNPsemanticinterpretationandshowtheimportanceofasetofnovellinguisticfeatures.3CorpusAnalysisForabetterunderstandingofthemeaningoftheNNandNPNinstances,weanalyzedtheseman-ticbehavioroftheseconstructionsonalargecross-linguisticcorporaofexamples.Weareinterestedinwhatsyntacticconstructionsareusedtotrans-latetheEnglishinstancestothetargetRomancelan-guagesandvice-versa,whatsemanticrelationsdotheseconstructionsencode,andwhatisthecorpusdistributionofthesemanticrelations.3.1ListsofsemanticclassiﬁcationrelationsAlthoughtheNPinterpretationproblemhasbeenstudiedforalongtime,researchershaven’tagreedonthenumberandthelevelofabstractionofthesesemanticcategories.Theycanvaryfromafewprepositions(Lauer,1995)tohundredsorthousandsspeciﬁcsemanticrelations(Finin,1980).Themoreabstractthecategories,themorenounphrasesarecovered,butalsothemoreroomforvariationastowhichcategoryaphraseshouldbeassigned.InthispaperweexperimentwithtwostateoftheartclassiﬁcationsetsusedinNPinterpretation.Theﬁrstisacoresetof22semanticrelations(22SRs)identiﬁedbyusfromthecomputationallinguisticsliterature.Thislist,presentedinTable1alongwithexamplesisgeneralenoughtocoveralargemajor-ityoftextsemanticswhilekeepingthesemanticre-lationstoamanageablenumber.ThesecondsetisLauer’slistof8prepositions(8PP)andcanbeap-pliedonlytonouncompounds(of,for,with,in,on,at,about,andfrom–e.g.,accordingtothisclassiﬁ-cation,lovestorycanbeclassiﬁedasstoryaboutlove).Weselectedthesesetsastheyareofdifferentsizeandcontainsemanticclassiﬁcationcategoriesatdifferentlevelsofabstraction.Lauer’slistismoreabstractand,thuscapableofencodingalargenum-berofnouncompoundinstances,whilethe22-SRlistcontainsﬁnergrainedsemanticcategories.Weshowbelowthecoverageofthesesemanticlistsontwodifferentcorporaandhowwelltheysolvetheinterpretationproblemofnounphrases.3.2ThedataThedatawascollectedfromtwotextcollectionswithdifferentdistributionsandofdifferentgenre,570

POSSESSION(familyestate);KINSHIP(sisteroftheboy);PROPERTY(lubricantviscosity);AGENT(returnofthenatives);THEME(acquisitionofstock);TEMPORAL(morningnews);DEPICTION-DEPICTED(apictureofmyniece);PART-WHOLE(brushhut);HYPERNYMY(IS-A)(daisyﬂower);CAUSE(screamofpain);MAKE/PRODUCE(chocolatefactory);INSTRUMENT(lasertreatment);LOCATION(castleinthedesert);PURPOSE(coughsyrup);SOURCE(grapefruitoil);TOPIC(weatherreport);MANNER(performancewithpassion);beneﬁciary(rightsofcitizens);MEANS(busservice);EXPERIENCER(fearofthegirl);MEASURE(cupofsugar);TYPE(frameworklaw);Table1:Thelistof22semanticrelations(22-SRs).Europarl1andCLUVI2.TheEuroparldatawasas-sembledbycombiningtheSpanish-English,Italian-English,French-EnglishandPortuguese-EnglishcorporawhichwereautomaticallyalignedbasedonexactmatchesofEnglishtranslations.Then,weconsideredonlytheEnglishsentenceswhichap-pearedverbatiminallfourlanguagepairs.There-sultingEnglishcorpuscontained10,000sentenceswhichweresyntacticallyparsed(Charniak,2000).Fromtheseweextractedtheﬁrst3,000NPinstances(NN:48.82%andNPN:51.18%).CLUVIisanopentextrepositoryofparallelcor-poraofcontemporaryoralandwrittentextsinsomeoftheRomancelanguages.Here,wefocusedonlyontheEnglish-PortugueseandEnglish-Spanishpar-alleltextsfromtheworksofJohnSteinbeck,H.G.Wells,J.Salinger,andothers.UsingtheCLUVIsearchinterfacewecreatedasentence-alignedpar-allelcorpusof2,800English-SpanishandEnglish-Portuguesesentences.TheEnglishversionswereautomaticallyparsedafterwhicheachNNandNPNinstancethusidentiﬁedwasmanuallymappedtothecorrespondingtranslations.Theresultingcor-puscontains2,200Englishinstanceswithadistribu-tionof26.77%NNand73.23%NPN.3.3CorpusAnnotationForeachcorpus,eachNPinstancewaspresentedseparatelytotwoexperiencedannotatorsinawebinterfaceincontextalongwiththeEnglishsentenceanditstranslations.Sincethecorporadonotcoversomeofthelanguages(RomanianinEuroparlandCLUVI,andItalianandFrenchinCLUVI),threeothernativespeakersoftheselanguagesandﬂu-entinEnglishprovidedthetranslationswhichwere1http://www.isi.edu/koehn/europarl/.Thiscorpuscontainsover20millionwordsinelevenofﬁciallanguagesoftheEuro-peanUnioncoveringtheproceedingsoftheEuropeanParlia-mentfrom1996to2001.2CLUVI-LinguisticCorpusoftheUniversityofVigo-Par-allelCorpus2.1-http://sli.uvigo.es/CLUVI/addedtothelist.ThetwocomputationalsemanticsannotatorshadtotageachEnglishconstituentnounwithitscorrespondingWordNetsenseandeachin-stancewiththecorrespondingsemanticcategory.IfthewordwasnotfoundinWordNettheinstancewasnotconsidered.Whenevertheannotatorsfoundanexampleencodingasemanticcategoryotherthanthoseprovidedortheydidn’tknowwhatinterpre-tationtogive,theyhadtotagitas“OTHER-SR”,andrespectively“OTHER-PP”3.Thedetailsoftheanno-tationtaskandtheobservationsdrawnfromtherearepresentedinacompanionpaper(Girju,2007).Thecorpusinstancesusedinthecorpusanaly-sisphasehavethefollowingformat:<NPEn;NPEs;NPIt;NPFr;NPPort;NPRo;target>.Thewordtargetisoneofthe23(22+OTHER-SR)seman-ticrelationsandoneoftheeightprepositionscon-sideredorOTHER-PP(withtheexceptionofthoseNPNinstancesthatalreadycontainapreposi-tion).Forexample,<developmentcooperation;cooperaci´onparaeldesarrollo;cooperazioneallosviluppo;coop´erationaud´eveloppement;cooperarepentrudezvoltare;PURPOSE/FOR>.Theannotators’agreementwasmeasuredusingKappastatistics:K=Pr(A)−Pr(E)1−Pr(E),wherePr(A)istheproportionoftimestheannotatorsagreeandPr(E)istheprobabilityofagreementbychance.TheKappavalueswereobtainedonEuroparl(NN:0.80for8-PPand0.61for22-SR;NPN:0.67for22-SR)andCLUVI(NN:0.77for8-PPand0.56for22-SR;NPN:0.68for22-SR).WealsocomputedthenumberofpairsthatweretaggedwithOTHERbybothannotatorsforeachsemanticrelationandprepositionparaphrase,overthenumberofexam-plesclassiﬁedinthatcategorybyatleastoneofthejudges(inEuroparl:91%for8-PPand78%for22-SR;inCLUVI:86%for8-PPand69%for22-SR).TheagreementobtainedontheEuroparlcorpusis3Theannotatedcorporaresultedinthisresearchisavailableathttp://apfel.ai.uiuc.edu.571

higherthantheoneonCLUVIonbothclassiﬁcationsets.Thisispartiallyexplainedbythedistributionofsemanticrelationsinbothcorpora,aswillbeshowninthenextsubsection.3.4Cross-linguisticdistributionofSyntacticConstructionsFromthesetsof2,954(Europarl)and2,168(CLUVI)instancesresultedafterannotation,thedatashowthatover83%ofthetranslationpatternsforbothtextcorporaonalllanguageswereofthetypeNNandNPN.However,whiletheirdistribu-tionisbalancedintheEuroparlcorpus(about45%,witha64%NPN–26%NNratioforRomanian),inCLUVItheNPNconstructionsoccurinmorethan85%ofthecases(again,withtheexceptionofRomanian–50%).ItisinterestingtonoteherethatsomeoftheEnglishNPsaretranslatedintobothnoun–nounandnoun–adjectivecompoundsinthetargetlanguages.Forexample,loveaffairtranslatesinItalianasstoriad’amoreorthenoun–adjectivecompoundrelazioneamorosa.Therearealsoin-stancesthathavejustonewordcorrespondentinthetargetlanguage(e.g.,anklebootisbottineinFrench).Therestofthedataisencodedbyothersyntacticparaphrases(e.g.,bombsiteisluogodove`eesplosalabomba(It.)).4.FromtheinitialcorpusweconsideredthoseEn-glishinstancesthathadallthetranslationsencodedonlybyNNandNPN.Outofthese,weselectedonly1,023Europarland1,008CLUVIinstancesen-codedbyNNandNPNinalllanguagesconsideredandresultedafteragreement.4Model4.1FeaturespaceWehaveidentiﬁedandexperimentedwith13NPfeaturespresentedbelow.WiththeexceptionsoffeaturesF1-F5(Girjuetal.,2005),alltheotherfea-turesarenovel.A.EnglishFeaturesF1andF2.NounsemanticclassspeciﬁestheWord-Netsenseofthehead(F1)andmodiﬁernoun(F2)andimplicitlypointstoallitshypernyms.Forex-ample,thehypernymsofcar#1are:{motorvehi-4“theplacewherethebombisexploded”(It.)cle},..{entity}.Thisfeaturehelpsgeneralizeoverthesemanticclassesofthetwonounsinthecorpus.F3andF4.WordNetderivationallyrelatedformspeciﬁesifthehead(F3)andthemodiﬁer(F4)nounsarerelatedtoacorrespondingWordNetverb(e.g.statementderivedfromtostate;cryfromtocry).F5.PrepositionalcuesthatlinkthetwonounsinanNP.Thesecanbeeithersimpleorcomplexpreposi-tionssuchas“of”or“accordingto”.IncaseofNNinstances,thisfeatureis“–”(e.g.,frameworklaw).F6andF7.Typeofnominalizednounindicatesthespeciﬁcclassofnounsthehead(F6)ormodiﬁer(F7)belongstodependingontheverbitderivesfrom.First,wecheckifthenounisanominalization.ForEnglishweusedNomLex-Plus(Meyersetal.,2004)tomapnounstocorrespondingverbs.5Forexam-ple,“destructionofthecity”,wheredestructionisanominalization.F6andF7mayoverlapwithfea-turesF3andF4whichareusedincasethenountobecheckeddoesnothaveanentryintheNomLex-Plusdictionary.Thesefeaturesareofparticularimpor-tancesincetheyimposesomeconstraintsonthepos-siblesetofrelationstheinstancecanencode.Theytakethefollowingvalues(identiﬁedbasedonlistofverbsextractedfromVerbNet(Kipperetal.,2000)):a.Activeformnounswhichhaveanintrinsicactivevoicepredicate-argumentstructure.(GiorgiandLongobardi,1991)arguethatinEnglishthisisanecessaryrestriction.Mostofthetime,theyrep-resentstatesofemotion,suchasfear,desire,etc.ThesenounsmarktheirinternalargumentthroughofandrequiremostofthetimeprepositionslikeporandnotdewhentranslatedinRomance.Ourobser-vationsontheRomaniantranslations(capturedbyfeaturesF12andF13below)showthatthepossiblecasesofambiguityaresolvedbythetypeofsyntac-ticconstructionused.Forexample,NNgenitive-markedconstructionsareusedforEXPERIENCER–encodinginstances,whileNdeNorNpentruN(NforN)areusedforotherrelations.Suchexamplesaretheloveofchildren–THEME(andnotthelovebythechildren).(GiorgiandLongobardi,1991)men-tionthatwithsuchnounsthatresistpassivisation,5NomLex-Plusisahand-codeddatabaseof5,000verbnom-inalizations,de-adjectival,andde-adverbialnounsincludingthecorrespondingsubcategorizationframes(verb-argumentstruc-tureinformation).572

theprepositionintroducingtheinternalargument,evenifitisof,hasalwaysasemanticcontent,andisnotabarecase-markerrealizingthegenitivecase.b.Unaccusative(ergative)nounswhicharede-rivedfromergativeverbsthattakeonlyinternalar-guments(e.g.,notagentiveones).Forexample,thetransitiveverbtodisbandallowsthesubjecttobedeletedasinthefollowingsentences(1)“Theleadsingerdisbandedthegroupin1991.”and(2)“Thegroupdisbanded.”.Thus,thecorrespondingerga-tivenominalizationthedisbandmentofthegroupen-codesaTHEMErelationandnotAGENT.c.Unergative(intransitive)nounsarederivedfromintransitiveverbsandtakeonlyAGENTseman-ticrelations.Forexample,thedepartureofthegirl.d.Inherentlypassivenounssuchasthecap-tureofthesoldier.Thesenouns,liketheverbstheyarederivedfrom,assumeadefaultAGENT(subject)andbeingtransitive,associatetotheirinternalargu-ment(introducedby“of”intheexampleabove)theTHEMErelation.B.RomanceFeaturesF8,F9,F10,F11andF12.Prepositionalcuesthatlinkthetwonounsareextractedfromeachtransla-tionoftheEnglishinstance:F8(Es.),F9(Fr.),F10(It.),F11(Port.),andF12(Ro.).Thesecanbeeithersimpleorcomplexprepositions(e.g.,de,inmateriade(Es.))inallﬁveRomancelanguages,ortheRo-maniangenitivalarticlea/ai/ale.InRomanianthegenitivecaseisassignedbythedeﬁnitearticleoftheﬁrstnountothesecondnoun,caserealizedasasuf-ﬁxifthesecondnounisprecededbythedeﬁnitearti-cleorasoneofthegenitivalarticlesa/ai/ale.Forex-ample,thenounphrasethebeautyofthegirlistrans-latedasfrumuset¸eafetei(beauty-thegirl-gen),andthebeautyofagirlasfrumuset¸eauneifete(beauty-thegengirl).ForNNinstances,thisfeatureis“–”.F13.NouninﬂectionisdeﬁnedonlyforRomanianandshowsifthemodiﬁernounisinﬂected(indicatesthegenitivecase).Thisfeatureisusedtohelpdiffer-entiatebetweeninstancesencodingIS-AandothersemanticrelationsinNNcompoundsinRomanian.ItalsohelpsinfeaturesF6andF7,casea)whenthechoiceofsyntacticconstructionreﬂectsdifferentse-manticcontent.Forexample,iubireapentrucopii(NPN)(theloveforchildren)andnotiubireacopi-ilor(NN)(loveexpressedbythechildren).4.2LearningModelsWehaveexperimentedwiththesupportvectorma-chines(SVM)model6andcomparedtheresultsagainsttwostate-of-the-artmodels:asupervisedmodel,SemanticScattering(SS),(MoldovanandBadulescu,2005),andaweb-basedunsupervisedmodel(LapataandKeller,2004).TheSVMandSSmodelsweretrainedandtestedontheEuroparlandCLUVIcorporausinga8:2ratio.Thetestdatasetwasrandomlyselectedfromeachcorpusandthetestnouns(onlyforEnglish)weretaggedwiththecor-respondingsenseincontextusingastateoftheartWSDtool(MihalceaandFaruque,2004).AftertheinitialNPinstancesinthetrainingandtestcorporawereexpandedwiththecorrespondingfeatures,wehadtopreparethemforSVMandSS.ThemethodconsistsofasetofautomaticiterativeproceduresofspecializationoftheEnglishnounsontheWordNetIS-Ahierarchy.Thus,afterasetofnec-essaryspecializationiterations,themethodproducesspecializedexampleswhichthroughsupervisedma-chinelearningaretransformedintosetsofseman-ticrules.Thisspecializationprocedureimprovesthesystem’sperformancesinceitefﬁcientlysepa-ratesthepositiveandnegativenoun-nounpairsintheWordNethierarchy.Initially,thetrainingcorpusconsistsofexamplesintheformatexempliﬁedbythefeaturespace.NotethatfortheEnglishNPinstances,eachnouncon-stituentwasexpandedwiththecorrespondingWord-Nettopsemanticclass.Atthispoint,thegeneral-izedtrainingcorpuscontainstwotypesofexamples:unambiguousandambiguous.Thesecondsituationoccurswhenthetrainingcorpusclassiﬁesthesamenoun–nounpairintomorethanonesemanticcat-egory.Forexample,bothrelationships“chocolatecake”-PART-WHOLEand“chocolatearticle”-TOPICaremappedintothemoregeneraltype<entity#1,entity#1,PART-WHOLE/TOPIC>7.Werecursivelyspecializetheseexamplestoeliminatetheambigu-ity.Byspecialization,thesemanticclassisreplacedwiththecorrespondinghyponymforthatparticularsense,i.e.theconceptimmediatelybelowinthehi-erarchy.Thesestepsarerepeateduntilthereareno6WeusedthepackageLIBSVMwitharadial-basedkernelhttp://www.csie.ntu.edu.tw/∼cjlin/libsvm/7Thespecializationprocedureappliesonlytofeatures1,2.573

moreambiguousexamples.Fortheexampleabove,thespecializationstopsattheﬁrsthyponymofen-tity:physicalentity(forcake)andabstractentity(forarticle).Fortheunambiguousexamplesinthegeneralizedtrainingcorpus(thosethatareclassiﬁedwithasinglesemanticrelation),constraintsarede-terminedusingcrossvalidationonSVM.A.SemanticScatteringusesatrainingdatasettoestablishaboundaryG∗onWordNetnounhier-archiessuchthateachfeaturepairofnoun–nounsensesfijonthisboundarymapsuniquelyintooneofapredeﬁnedlistofsemanticrelations,andanyfeaturepairabovetheboundarymapsintomorethanonesemanticrelation.Foranynewpairofnoun–nounsenses,themodelﬁndstheclosestWordNetboundarypair.TheauthorsdeﬁnewithSCm={fmi}andSCh={fhj}thesetsofsemanticclassfeaturesformodiﬁernounand,respectivelyheadnoun.Apairof<modiﬁer–head>nounsmapsuniquelyintoasemanticclassfeaturepair<fmi,fhj>,denotedasfij.Theprobabilityofasemanticre-lationrgivenfeaturepairfij,P(r|fij)=n(r,fij)n(fij),isdeﬁnedastheratiobetweenthenumberofoc-currencesofarelationrinthepresenceoffea-turepairfijoverthenumberofoccurrencesoffeaturepairfijinthecorpus.Themostproba-blesemanticrelationˆrisargmaxr∈RP(r|fij)=argmaxr∈RP(fij|r)P(r).B.(LapataandKeller,2004)’sweb-basedun-supervisedmodelclassiﬁesnoun-nouninstancesbasedonLauer’slistof8prepositionsandusesthewebastrainingcorpus.Theyshowthatthebestperformanceisobtainedwiththetrigrammodelf(n1,p,n2).ThecountusedforagiventrigramisthenumberofpagesreturnedbyAltavistaonthetri-gramcorrespondingqueries.Forexample,forthetestinstancewarstories,thebestnumberofhitswasobtainedwiththequerystoriesaboutwar.FortheEuroparlandCLUVItestsets,werepli-catedLapata&Keller’sexperimentsusingGoogle8.Weformedinﬂectedquerieswiththepatternstheyproposedandsearchedtheweb.8AsGooglelimitsthenumberofqueriesto1,000perday,werepeatedtheexperimentforanumberofdays.Although(LapataandKeller,2004)usedAltavistaintheirexperiments,theyshowedthereisalmostnodifferencebetweenthecorrela-tionsachievedusingGoogleandAltavistacounts.5ExperimentalresultsTable2showstheresultsobtainedagainstSSandLapata&Keller’smodelonbothcorporaandthecontributionthefeaturesexempliﬁedinonebaselineandsixversionsoftheSVMmodel.ThebaselineisdeﬁnedonlyfortheEnglishpartoftheNPfeaturesetandmeasuresthethecontributionoftheWord-NetIS-Alexicalhierarchyspecialization.Thebase-linedoesnotdifferentiatebetweenunambiguousandambiguoustrainingexamples(afterjustonelevelspecialization)andthus,doesnotspecializetheam-biguousones.Moreover,herewewantedtoseewhatisthedifferencebetweenSSandSVM,andwhatisthecontributionoftheotherEnglishfeatures,suchasprepositionandnominalization(F1–F7).Thetableshowsthat,overalltheperformanceisbetterfortheEuroparlcorpusthanforCLUVI.FortheBaselineandSVM1,SS[F1+F2]givesbet-terresultsthanSVM.TheinclusionofotherEnglishfeatures(SVM[F1–F7])addsmorethan15%(withahigherincreaseinEuroparl)forSVM1.ThecontributionofRomancelinguisticfeatures.SinceourintuitionisthatthemoretranslationsareprovidedforanEnglishnounphraseinstance,thebettertheresults,wewantedtoseewhatistheim-pactofeachRomancelanguageontheoverallper-formance.Thus,SVM2showstheresultsobtainedforEnglishandtheRomancelanguagethatcon-tributedtheleasttotheperformance(F1–F12).HerewecomputedtheperformanceonallﬁveEnglish–RomancelanguagecombinationsandchosetheRo-mancelanguagethatprovidedthebestresult.Thus,SVM#2,#3,#4,#5,and#6addSpanish,French,Italian,Portuguese,andRomanianinthisorderandshowthecontributionofeachRomanceprepositionandallfeaturesforEnglish.ThelanguagerankinginTable2showsthatRo-mancelanguagesconsideredherehaveadifferentcontributiontotheoverallperformance.WhiletheadditionofItalianinEuroparldecreasestheper-formance,Portuguesedoesn’taddanything.How-ever,acloseranalysisofthedatashowsthatthisismostlyduetothedistributionofthecorpusin-stances.Forexample,French,Italian,Spanish,andPortuguesearemostofthetimeconsistentinthechoiceofpreposition(e.g.mostofthetime,ifthepreposition’de’(’of’)isusedinFrench,thenthe574

LearningmodelsResults[%]CLUVIEuroparl8-PP22-SR8-PP22-SRBaseline(En.)(nospecializ.)SS(F1+F2)44.1148.0338.738SVM(F1+F2)36.3740.6731.1834.81SVM(F1-F7)–52.15–47.37SVM1(En.)SS(F1+F2)56.2261.3353.156.81SVM(F1+F2)45.0846.140.2342.2SVM(F1-F7)–62.54–74.19SVM2(En.+Es.)SVM(F1-F8)–64.18–75.74SVM3(En.+Es.+Fr.)SVM(F1-F9)–67.8–76.52SVM4(En.+Es.+Fr.+It.)SVM(F1-F10)–66.31–75.74SVM5(En.+Es.+Fr.+It+Port.)SVM(F1-F11)–67.12–75.74SVM6(En.+Romance:F1–F13)–74.31–77.9Lapata&Keller’sunsupervisedmodel(En.)44.15–45.31–Table2:Theperformanceofthecross-linguisticSVMmodelscomparedagainstonebaseline,SSmodelandLapata&Keller’sunsupervisedmodel.Accuracy(numberofcorrectlylabeledinstancesoverthenumberofinstancesinthetestset).correspondingprepositionisusedintheotherfourlanguagetranslations).AnotableexceptionhereisRomanianwhichprovidestwopossibleconstruc-tions:theNPNandthegenitive-markedNN.Thetableshows(intheincreaseinperformancebetweenSVM5andSVM6)thatthischoiceisnotrandom,butinﬂuencedbythemeaningoftheinstances(fea-turesF12,F13).Thisobservationisalsosupportedbythecontributionofeachfeaturetotheoverallper-formance.Forexample,inEuroparl,theWordNetverbandnominalizationfeaturesoftheheadnoun(F3,F6)haveacontributionof4.08%,whileforthemodiﬁernounsitdecreasesbyabout2%.Theprepo-sition(F5)contributes4.41%(Europarl)and5.24%(CLUVI)totheoverallperformance.AcloseranalysisofthedatashowsthatinEu-roparlmostoftheNNinstanceswerenamingnouncompoundssuchasframeworklaw(TYPE)and,mostofthetime,areencodedbyNNpatternsinthetargetlanguages(e.g.,leggequadro(It.)).IntheCLUVIcorpus,ontheotherhand,theNNRo-mancetranslationsrepresentedonly1%ofthedata.AnotableexceptionhereisRomanianwheremostNPsarerepresentedasgenitive–markednouncom-pounds.However,thereareinstancesthatareen-codedmostlyoronlyasNPNconstructionsandthischoicecorrelateswiththemeaningoftheinstance.Forexample,themilkglass(PURPOSE)translatesaspaharuldelapte(glass-theofmilk)andnotaspaharullaptelui(glass-themilk-gen),theoliveoil(SOURCE)translatesasuleiuldemˇasline(oil-theofolive)andnotasuleiulmˇaslinei(oil-theolive-gen).OtherexamplesincludeCAUSEandTOPIC.Lauer’ssetof8prepositionsrepresents94.5%(Europarl)and97%(CLUVI)oftheNPNin-stances.Fromthese,themostfrequentprepositionis“of”withacoverageof70.31%(Europarl)and85.08%(CLUVI).Moreover,intheEuroparlcor-pus,26.39%oftheinstancesaresyntheticphrases(whereoneofthenounsisanominalization)encod-ingAGENT,EXPERIENCER,THEME,BENEFICIARY.Outoftheseinstances,74.81%usetheprepositionof.InCLUVI,11.71%oftheexampleswerever-bal,fromwhichtheprepositionofhasacoverageof82.20%.Themany-to-manymappingsoftheprepo-sitions(especiallyof/de)tothesemanticclassesaddstothecomplexityoftheinterpretationtask.Thus,fortheinterpretationoftheseconstructionsasystemmustrelyonthesemanticinformationoftheprepo-sitionandtwoconstituentnounsinparticular,andoncontextingeneral.InEuroparl,themostfrequentlyoccurringre-lationsarePURPOSE,TYPE,andTHEMEthatto-getherrepresentabout57%ofthedatafollowedbyPART-WHOLE,PROPERTY,TOPIC,AGENT,andLO-CATIONwithanaveragecoverageofabout6.23%.Moreover,otherrelationssuchasKINSHIP,DE-PICTION,MANNER,MEANSdidnotoccurinthiscorpusand5.08%representedOTHER-SRrelations.ThissemanticdistributioncontrastswiththeoneinCLUVI,whichusesamoredescriptivelan-guage.Here,themostfrequentrelationbyfar575

isPART-WHOLE(32.14%),followedbyLOCATION(12.40%),THEME(9.23%)andOTHER-SR(7.74%).Itisinterestingtonoteherethatonly5.70%oftheTYPErelationinstancesinEuroparlwereunique.Thisisincontrastwiththeotherrelationsinbothcorpora,whereinstancesweremostlyunique.WealsoreporthereourobservationsonLap-ata&Keller’sunsupervisedmodel.Ananalysisoftheseresultsshowedthattheorderofthecon-stituentnounsintheNPNparaphraseplaysanim-portantrole.Forexample,asearchforbloodves-selsgeneratedsimilarfrequencycountsforvesselsofbloodandbloodinvessels.About30%noun-nounparaphrasablepairspreservedtheorderinthecorrespondingNPNparaphrases.WealsomanuallycheckedtheﬁrstﬁveentriesgeneratedbyGoogleforeachmostfrequentprepositionalparaphrasefor50instancesandnoticedthatabout35%ofthemwerewrongduetosyntacticand/orsemanticambiguities.Thus,sincewewantedtomeasuretheimpactoftheseambiguitiesofnouncompoundsontheinter-pretationperformance,wefurthertestedtheprob-abilisticweb-basedmodelonfourdistincttestsetsselectedfromEuroparl,eachcontaining30noun-nounpairsencodingdifferenttypesofambiguity:inset#1thenounconstituentshadonlyonepartofspeechandoneWordNetsense;inset#2thenounshadatleasttwopossiblepartsofspeechandweresemanticallyunambiguous,inset#3thenounswereambiguousonlysemantically,andinset#4theywereambiguousbothsyntacticallyandsemantically.Forunambiguousnoun-nounpairs(set#1),themodelobtainedanaccuracyof35.01%,whileformorese-manticallyambiguouscompoundsitobtainedanac-curacyofabout48.8%.Thisshowsthatformoresemanticallyambiguousnoun-nounpairs,theweb-basedprobabilisticmodelintroducesasigniﬁcantnumberoffalsepositives.Thus,themoreabstractthecategories,themorenouncompoundsarecov-ered,butalsothemoreroomforvariationastowhichcategoryacompoundshouldbeassigned.6DiscussionandConclusionsInthispaperwepresentedasupervised,knowledge-intensiveinterpretationmodelwhichtakesadvan-tageofnewlinguisticinformationfromEnglishandalistofﬁveRomancelanguages.OurapproachtoNPinterpretationisnovelinseveralways.Wede-ﬁnedtheprobleminacross-linguisticframeworkandprovidedempiricalobservationsonthedistribu-tionofthesyntaxandmeaningofnounphrasesontwodifferentcorporabasedontwostate-of-the-artclassiﬁcationtagsets.AsfutureworkweconsidertheinclusionofotherfeaturessuchasthesemanticclassesofRomancenounsfromalignedEuroWordNets,andothersen-tencefeatures.SincetheresultsobtainedcanbeseenasanupperboundonNPinterpretationduetoper-fectEnglish-RomanceNPalignment,wewillex-perimentwithautomatictranslationsgeneratedforthetestdata.Moreover,weliketoextendtheanal-ysistoothersetoflanguageswhosestructuresareverydifferentfromEnglishandRomance.ReferencesT.W.Finin.1980.TheSemanticInterpretationofCompoundNominals.Ph.D.thesis,UniversityofIllinoisatUrbana-Champaign.A.GiorgiandG.Longobardi.1991.Thesyntaxofnounphrases.CambridgeUniversityPress.R.Girju,D.Moldovan,M.Tatu,andD.Antohe.2005.Onthesemanticsofnouncompounds.ComputerSpeechandLanguage,19(4):479–496.R.Girju.2007.Experimentswithanannotationschemeforaknowledge-richnounphraseinterpretationsystem.TheLin-guisticAnnotationWorkshopatACL,Prague.SuNamKimandT.Baldwin.2006.Interpretingsemanticrela-tionsinnouncompoundsviaverbsemantics.COLING-ACL.K.Kipper,H.Dong,andM.Palmer.2000.Class-basedcon-structionofaverblexicon.AAAIConference,Austin.M.LapataandF.Keller.2004.TheWebasabaseline:Eval-uatingtheperformanceofunsupervisedWeb-basedmodelsforarangeofNLPtasks.HLT-NAACL.M.Lauer.1995.Corpusstatisticsmeetthenouncompound:Someempiricalresults.ACL,Cambridge,Mass.A.Meyers,R.Reeves,C.Macleod,R.SzekeleyV.Zielinska,andB.Young.2004.Thecross-breedingofdictionaries.LREC-2004,Lisbon,Portugal.R.MihalceaandE.Faruque.2004.Senselearner:Minimallysupervisedwordsensedisambiguationforallwordsinopentext.ACL/SIGLEXSenseval-3,Barcelona,Spain.D.MoldovanandA.Badulescu.2005.Asemanticscat-teringmodelfortheautomaticinterpretationofgenitives.HLT/EMNLPConference,Vancouver,Canada.P.PantelandM.Pennacchiotti.2006.Espresso:Leveraginggenericpatternsforautomaticallyharvestingsemanticrela-tions.COLING/ACL,Sydney,Australia.B.RosarioandM.Hearst.2001.Classifyingthesemanticrela-tionsinnouncompounds.EMNLPConference.R.Snow,D.Jurafsky,andA.Ng.2006.Semantictaxonomyinductionfromheterogenousevidence.COLING-ACL.P.Turney.2006.Expressingimplicitsemanticrelationswithoutsupervision.COLING/ACL,Sydney,Australia.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 576–583,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

576

LearningtoExtractRelationsfromtheWebusingMinimalSupervisionRazvanC.BunescuDepartmentofComputerSciencesUniversityofTexasatAustin1UniversityStationC0500Austin,TX78712razvan@cs.utexas.eduRaymondJ.MooneyDepartmentofComputerSciencesUniversityofTexasatAustin1UniversityStationC0500Austin,TX78712mooney@cs.utexas.eduAbstractWepresentanewapproachtorelationex-tractionthatrequiresonlyahandfuloftrain-ingexamples.Givenafewpairsofnamedentitiesknowntoexhibitornotexhibitaparticularrelation,bagsofsentencescon-tainingthepairsareextractedfromtheweb.Weextendanexistingrelationextractionmethodtohandlethisweakerformofsu-pervision,andpresentexperimentalresultsdemonstratingthatourapproachcanreliablyextractrelationsfromwebdocuments.1IntroductionAgrowingbodyofrecentworkininformationextractionhasaddressedtheproblemofrelationextraction(RE),identifyingrelationshipsbetweenentitiesstatedintext,suchasLivesIn(Person,Location)orEmployedBy(Person,Company).SupervisedlearninghasbeenshowntobeeffectiveforRE(Zelenkoetal.,2003;CulottaandSorensen,2004;BunescuandMooney,2006);however,anno-tatinglargecorporawithexamplesoftherelationstobeextractedisexpensiveandtedious.Inthispaper,weintroduceasupervisedlearningapproachtoREthatrequiresonlyahandfuloftrainingexamplesandusesthewebasacorpus.Givenafewpairsofwell-knownentitiesthatclearlyexhibitordonotexhibitaparticularre-lation,suchasCorpAcquired(Google,YouTube)andnot(CorpAcquired(Yahoo,Microsoft)),asearchengineisusedtoﬁndsentencesonthewebthatmentionbothoftheentitiesineachofthepairs.Althoughnotallofthesentencesforpositivepairswillstatethedesiredrelationship,manyofthemwill.Presumably,noneofthesentencesfornegativepairsstatethetargetedrelation.Multipleinstancelearning(MIL)isamachinelearningframeworkthatexploitsthissortofweaksupervision,inwhichapositivebagisasetofinstanceswhichisguaranteedtocontainatleastonepositiveexample,andanegativebagisasetofinstancesallofwhicharenegative.MILwasoriginallyintroducedtosolveaprobleminbiochemistry(Dietterichetal.,1997);however,ithassincebeenappliedtoproblemsinotherareassuchasclassifyingimageregionsincomputervision(Zhangetal.,2002),andtextcategorization(Andrewsetal.,2003;RayandCraven,2005).Wehaveextendedanexistingapproachtorela-tionextractionusingsupportvectormachinesandstringkernels(BunescuandMooney,2006)tohan-dlethisweakerformofMILsupervision.Thisap-proachcansometimesbemisledbytextualfeaturescorrelatedwiththespeciﬁcentitiesinthefewtrain-ingpairsprovided.Therefore,wealsodescribeamethodforweightingfeaturesinordertofocusonthosecorrelatedwiththetargetrelationratherthanwiththeindividualentities.Wepresentexperimen-talresultsdemonstratingthatourapproachisabletoaccuratelyextractrelationsfromthewebbylearningfromsuchweaksupervision.2ProblemDescriptionWeaddressthetaskoflearningarelationextrac-tionsystemtargetedtoaﬁxedbinaryrelationshipR.Theonlysupervisiongiventothelearningalgo-577

rithmisasmallsetofpairsofnamedentitiesthatareknowntobelong(positive)ornotbelong(negative)tothegivenrelationship.Table1showsfourposi-tiveandtwonegativeexamplepairsforthecorpo-rateacquisitionrelationship.Foreachpair,abagofsentencescontainingthetwoargumentscanbeex-tractedfromacorpusoftextdocuments.Thecorpusisassumedtobesufﬁcientlylargeanddiversesuchthat,ifthepairispositive,itishighlylikelythatthecorrespondingbagcontainsatleastonesentencethatexplicitlyassertstherelationshipRbetweenthetwoarguments.InSection6wedescribeamethodforextractingbagsofrelevantsentencesfromtheweb.+/−Arga1Arga2+GoogleYouTube+AdobeSystemsMacromedia+ViacomDreamWorks+NovartisEonLabs−YahooMicrosoft−PﬁzerTevaTable1:CorporateAcquisitionPairs.Usingalimitedsetofentitypairs(e.g.thoseinTable1)andtheirassociatedbagsastrainingdata,theaimistoinducearelationextractionsystemthatcanreliablydecidewhethertwoentitiesmentionedinthesamesentenceexhibitthetargetrelationshipornot.Inparticular,whentestedontheexamplesentencesfromFigure1,thesystemshouldclassifyS1,S3,andS4aspositive,andS2andS5asnegative.+/S1:SearchenginegiantGooglehasboughtvideo-sharingwebsiteYouTubeinacontroversial$1.6billiondeal.−/S2:ThecompanieswillmergeGoogle’ssearchex-pertisewithYouTube’svideoexpertise,pushingwhatexecutivesbelieveisahotemergingmarketofvideoofferedovertheInternet.+/S3:Googlehasacquiredsocialmediacompany,YouTubefor$1.65billioninastock-for-stocktransactionasannouncedbyGoogleInc.onOctober9,2006.+/S4:DruggiantPﬁzerInc.hasreachedanagreementtobuytheprivatebiotechnologyﬁrmRinatNeuroscienceCorp.,thecompaniesannouncedThursday.−/S5:HehasalsoreceivedconsultingfeesfromAl-pharma,EliLillyandCompany,Pﬁzer,WyethPharmaceu-ticals,RinatNeuroscience,ElanPharmaceuticals,andFor-estLaboratories.Figure1:Sentenceexamples.Asformulatedabove,thelearningtaskcanbeseenasaninstanceofmultipleinstancelearning.However,thereareimportantpropertiesthatsetitapartfromproblemspreviouslyconsideredinMIL.Themostdistinguishingcharacteristicisthatthenumberofbagsisverysmall,whiletheaveragesizeofthebagsisverylarge.3MultipleInstanceLearningSinceitsintroductionbyDietterich(1997),anex-tensiveandquitediversesetofmethodshavebeenproposedforsolvingtheMILproblem.Forthetaskofrelationextraction,weconsideronlyMILmeth-odswherethedecisionfunctioncanbeexpressedintermsofkernelscomputedbetweenbaginstances.Thischoicewasmotivatedbythecomparativelyhighaccuracyobtainedbykernel-basedSVMswhenappliedtovariousnaturallanguagetasks,andinpar-ticulartorelationextraction.Throughtheuseofker-nels,SVMs(Vapnik,1998;Sch¨olkopfandSmola,2002)canworkefﬁcientlywithinstancesthatim-plicitlybelongtoahighdimensionalfeaturespace.Whenusedforclassiﬁcation,thedecisionfunctioncomputedbythelearningalgorithmisequivalenttoahyperplaneinthisfeaturespace.OverﬁttingisavoidedintheSVMformulationbyrequiringthatpositiveandnegativetraininginstancesbemaxi-mallyseparatedbythedecisionhyperplane.Gartneretal.(2002)adaptedSVMstotheMILsettingusingvariousmulti-instancekernels.Twoofthese–thenormalizedsetkernel,andthestatis-tickernel–havebeenexperimentallycomparedtoothermethodsbyRayandCraven(2005),withcom-petitiveresults.Alternatively,asimpleapproachtoMIListotransformitintoastandardsupervisedlearningproblembylabelingallinstancesfrompos-itivebagsaspositive.AninterestingoutcomeofthestudyconductedbyRayandCraven(2005)wasthat,despitetheclassnoiseintheresultingpositiveex-amples,suchasimpleapproachoftenobtainscom-petitiveresultswhencomparedagainstothermoresophisticatedMILmethods.WebelievethatanMILmethodbasedonmulti-instancekernelsisnotappropriatefortrainingdatasetsthatcontainjustafew,verylargebags.Inamulti-instancekernelapproach,onlybags(andnotinstances)areconsideredastrainingexamples,578

whichmeansthatthenumberofsupportvectorsisgoingtobeupperboundedbythenumberoftrain-ingbags.TakingthebagsfromTable1asasam-pletrainingset,thedecisionfunctionisgoingtobespeciﬁedbyatmostsevenparameters:thecoefﬁ-cientsforatmostsixsupportvectors,plusanop-tionalbiasparameter.Ahypothesisspacecharacter-izedbysuchasmallnumberofparametersislikelytohaveinsufﬁcientcapacity.Basedontheseobservations,wedecidedtotrans-formtheMILproblemintoastandardsupervisedproblemasdescribedabove.Theuseofthisap-proachisfurthermotivatedbyitssimplicityanditsobservedcompetitiveperformanceonverydiversedatasets(RayandCraven,2005).LetXbethesetofbagsusedfortraining,Xp⊆Xthesetofposi-tivebags,andXn⊆Xthesetofnegativebags.Foranyinstancex∈XfromabagX∈X,letφ(x)bethe(implicit)featurevectorrepresentationofx.ThenthecorrespondingSVMoptimizationproblemcanbeformulatedasinFigure2:minimize:J(w,b,ξ)=12kwk2+CL(cid:18)cpLnLΞp+cnLpLΞn(cid:19)Ξp=XX∈XpXx∈XξxΞn=XX∈XnXx∈Xξxsubjectto:wφ(x)+b≥+1−ξx,∀x∈X∈Xpwφ(x)+b≤−1+ξx,∀x∈X∈Xnξx≥0Figure2:SVMOptimizationProblem.ThecapacitycontrolparameterCisnormalizedbythetotalnumberofinstancesL=Lp+Ln=PX∈Xp|X|+PX∈Xn|X|,sothatitremainsin-dependentofthesizeofthedataset.Theadditionalnon-negativeparametercp(cn=1−cp)controlstherelativeinﬂuencethatfalsenegativevs.falseposi-tiveerrorshaveonthevalueoftheobjectivefunc-tion.Becausenotallinstancesfrompositivebagsarerealpositiveinstances,itmakessensetohavefalsenegativeerrorsbepenalizedlessthanfalsepos-itiveerrors(i.e.cp<0.5).Inthedualformulationoftheoptimizationprob-lemfromFigure2,baginstancesappearonlyinsidedotproductsoftheformK(x1,x2)=φ(x1)φ(x2).ThekernelKisinstantiatedtoasubsequenceker-nel,asdescribedinthenextsection.4RelationExtractionKernelThetrainingbagsconsistofsentencesextractedfromonlinedocuments,usingthemethodologyde-scribedinSection6.Parsingwebdocumentsinordertoobtainasyntacticanalysisoftengivesun-reliableresults–thetypeofnarrativecanvarygreatlyfromonewebdocumenttoanother,andsen-tenceswithgrammaticalerrorsarefrequent.There-fore,fortheinitialexperiments,weusedamodi-ﬁedversionofthesubsequencekernelofBunescuandMooney(2006),whichdoesnotrequiresyn-tacticinformation.Thiskernelcomputesthenum-berofcommonsubsequencesoftokensbetweentwosentences.Thesubsequencesareconstrainedtobe“anchored”atthetwoentitynames,andthereisamaximumnumberoftokensthatcanappearinasequence.Forexample,asubsequencefeatureforthesentenceS1inFigure1is˜s=“he1i...bought...he2i...in...billion...deal”,wherehe1iandhe2iaregenericplaceholdersforthetwoentitynames.Thesubsequencekernelinducesafeaturespacewhereeachdimensioncorrespondstoasequenceofwords.Anysuchsequencethatmatchesasubsequenceofwordsinasentenceexam-pleisdown-weightedasafunctionofthetotallengthofthegapsbetweeneverytwoconsecutivewords.Moreexactly,lets=w1w2...wkbeasequenceofkwords,and˜s=w1g1w2g2...wk−1gk−1wkamatchingsubsequenceinarelationexample,wheregistandsforanysequenceofwordsbetweenwiandwi+1.Thenthesequenceswillberepresentedintherelationexampleasafeaturewithweightcomputedasτ(s)=λg(˜s).Theparameterλcontrolsthemag-nitudeofthegappenalty,whereg(˜s)=Pi|gi|isthetotalgap.Manyrelations,liketheonesthatweexploreintheexperimentalevaluation,cannotbeexpressedwithoutusingatleastonecontentword.Wethere-foremodiﬁedthekernelcomputationtooptionallyignoresubsequencepatternsformedexclusivelyof579

stopwordsandpunctuationsigns.InSection5.1,weintroduceanewweightingscheme,whereinaweightisassignedtoeverytoken.Correspondingly,everysequencefeaturewillhaveanadditionalmul-tiplicativeweight,computedastheproductoftheweightsofallthetokensinthesequence.Theaimofthisnewweightingscheme,asdetailedinthenextsection,istoeliminatethebiascausedbythespecialstructureoftherelationextractionMILproblem.5TwoTypesofBiasAsalreadyhintedattheendofSection2,thereisoneimportantpropertythatdistinguishesthecur-rentMILsettingforrelationextractionfromotherMILproblems:thetrainingdatasetcontainsveryfewbags,andeachbagcanbeverylarge.Con-sequently,anapplicationofthelearningmodelde-scribedinSections3&4isboundtobeaffectedbythefollowingtwotypesofbias:(cid:4)[TypeIBias]Bydeﬁnition,allsentencesinsideabagareconstrainedtocontainthesametwoar-guments.Wordsthataresemanticallycorrelatedwitheitherofthetwoargumentsarelikelytooc-curinmanysentences.Forexample,considerthesentencesS1andS2fromthebagassociatedwith“Google”and“YouTube”(asshowninFigure1).Theybothcontainthewords“search”–highlycor-relatedwith“Google”,and“video”–highlycorre-latedwith“YouTube”,anditislikelythatasigniﬁ-cantpercentageofsentencesinthisbagcontainoneofthetwowords(orboth).ThetwoentitiescanbementionedinthesamesentenceforreasonsotherthanthetargetrelationR,andthesenoisytrainingsentencesarelikelytocontainwordsthatarecorre-latedwiththetwoentities,withoutanyrelationshiptoR.Alearningmodelwherethefeaturesarebasedonwords,orwordsequences,isgoingtogivetoomuchweighttowordsorcombinationsofwordsthatarecorrelatedwitheitherofindividualarguments.Thisoverweightingwilladverselyaffectextractionperformancethroughanincreasednumberoferrors.Amethodforeliminatingthistypeofbiasisintro-ducedinSection5.1.(cid:4)[TypeIIBias]WhileTypeIbiasisduetowordsthatarecorrelatedwiththeargumentsofarelationinstance,theTypeIIbiasiscausedbywordsthatarespeciﬁctotherelationinstanceitself.UsingFrameNetterminology(Bakeretal.,1998),thesecorrespondtoinstantiatedframeelements.Forex-ample,thecorporateacquisitionframecanbeseenasasubtypeofthe“Getting”frameinFrameNet.ThecoreelementsinthisframearetheRecipi-ent(e.g.Google)andtheTheme(e.g.YouTube),whichfortheacquisitionrelationshipcoincidewiththetwoarguments.Theydonotcontributeanybias,sincetheyarereplacedwiththegenerictagshe1iandhe2iinallsentencesfromthebag.Therearehoweverotherframeelements–peripheral,orextra-thematic–thatcanbeinstantiatedwiththesamevalueinmanysentences.InFigure1,forin-stance,sentenceS3containstwonon-coreframeele-ments:theMeanselement(e.g“inastock-for-stocktransaction”)andtheTimeelement(e.g.“onOc-tober9,2006”).Wordsfromtheseelements,like“stock”,or“October”,arelikelytooccurveryoftenintheGoogle-YouTubebag,andbecausethetrain-ingdatasetcontainsonlyafewotherbags,subse-quencepatternscontainingthesewordswillbegiventoomuchweightinthelearnedmodel.Thisisprob-lematic,sincethesewordscanappearinmanyotherframes,andthusthelearnedmodelislikelytomakeerrors.Instead,wewouldlikethemodeltofo-cusonwordsthattriggerthetargetrelationship(inFrameNet,thesearethelexicalunitsassociatedwiththetargetframe).5.1ASolutionforTypeIBiasInordertoaccountforhowstronglythewordsinasequencearecorrelatedwitheitheroftheindividualargumentsoftherelation,wemodifytheformulaforthesequenceweightτ(s)byfactoringinaweightτ(w)foreachwordinthesequence,asillustratedinEquation1.τ(s)=λg(˜s)·Yw∈sτ(w)(1)Givenapredeﬁnedsetofweightsτ(w),itisstraight-forwardtoupdatetherecursivecomputationofthesubsequencekernelsothatitreﬂectsthenewweightingscheme.Ifallthewordweightsaresetto1,thenthenewkernelisequivalenttotheoldone.Whatwewant,however,isasetofweightswherewordsthatarecorrelatedwitheitherofthetwoargumentsaregivenlowerweights.Foranyword,thedecreaseinweight580

shouldreﬂectthedegreeofcorrelationbetweenthatwordandthetwoarguments.Beforeshowingtheformulausedforcomputingthewordweights,weﬁrstintroducesomenotation:•LetX∈Xbeanarbitrarybag,andletX.a1andX.a2bethetwoargumentsassociatedwiththebag.•LetC(X)bethesizeofthebag(i.e.thenum-berofsentencesinthebag),andC(X,w)thenumberofsentencesinthebagXthatcontainthewordw.LetP(w|X)=C(X,w)/C(X).•LetP(w|X.a1∨X.a2)betheprobabilitythatthewordwappearsinasentencedueonlytothepresenceofX.a1orX.a2,assumingX.a1andX.a2areindependentcausesforw.Thewordweightsarecomputedasfollows:τ(w)=C(X,w)−P(w|X.a1∨X.a2)·C(X)C(X,w)=1−P(w|X.a1∨X.a2)P(w|X)(2)ThequantityP(w|X.a1∨X.a2)·C(X)representstheexpectednumberofsentencesinwhichwwouldoccur,iftheonlycauseswereX.a1orX.a2,inde-pendentofeachother.Wewanttodiscardthisquan-tityfromthetotalnumberofoccurrencesC(X,w),sothattheeffectofcorrelationswithX.a1orX.a2iseliminated.WestillneedtocomputeP(w|X.a1∨X.a2).Be-causeinthedeﬁnitionofP(w|X.a1∨X.a2),thear-gumentsX.a1andX.a2wereconsideredindepen-dentcauses,P(w|X.a1∨X.a2)canbecomputedwiththenoisy-oroperator(Pearl,1986):P(·)=1−(1−P(w|a1))·(1−P(w|a2))(3)=P(w|a1)+P(w|a2)−P(w|a1)·P(w|a2)ThequantityP(w|a)representstheprobabilitythatthewordwappearsinasentencedueonlytothepresenceofa,anditcouldbeestimatedusingcountsonasufﬁcientlylargecorpus.Forourexperimen-talevaluation,weusedthefollowingapproxima-tion:givenanargumenta,asetofsentencescon-tainingaareextractedfromwebdocuments(de-tailsinSection6).ThenP(w|a)issimplyapproxi-matedwiththeratioofthenumberofsentencescon-tainingwoverthetotalnumberofsentences,i.e.P(w|a)=C(w,a)/C(a).Becausethismaybeanoverestimate(wmayappearinasentencecontain-ingaduetocausesotherthana),andalsobecauseofdatasparsity,thequantityτ(w)maysometimesresultinanegativevalue–inthesecasesitissetto0,whichisequivalenttoignoringthewordwinallsubsequencepatterns.6MILRelationExtractionDatasetsForthepurposeofevaluation,wecreatedtwodatasets:oneforcorporateacquisitions,asshowninTable2,andonefortheperson-birthplacerela-tion,withtheexamplepairsfromTable3.Inbothtables,thetoppartshowsthetrainingpairs,whilethebottompartshowsthetestpairs.+/−Arga1Arga2Size+GoogleYouTube1375+AdobeSystemsMacromedia622+ViacomDreamWorks323+NovartisEonLabs311−YahooMicrosoft163−PﬁzerTeva247+PﬁzerRinatNeuroscience50(41)+YahooInktomi433(115)−GoogleApple281−ViacomNBC231Table2:CorporateAcquisitionPairs.+/−Arga1Arga2Size+FranzKafkaPrague552+AndreAgassiLasVegas386+CharlieChaplinLondon292+GeorgeGershwinNewYork260−LucBessonNewYork74−WolfgangA.MozartVienna288+LucBessonParis126(6)+MarieAntoinetteVienna105(39)−CharlieChaplinHollywood266−GeorgeGershwinLondon104Table3:Person-BirthplacePairs.Givenapairofarguments(a1,a2),thecorre-spondingbagofsentencesiscreatedasfollows:(cid:4)Aquerystring“a1∗∗∗∗∗∗∗a2”containingsevenwildcardsymbolsbetweenthetwoargumentsissubmittedtoGoogle.ThepreferencesaresettosearchonlyforpageswritteninEnglish,withSafe-searchturnedon.Thistypeofquerywillmatchdoc-umentswhereanoccurrenceofa1isseparatedfromanoccurrenceofa2byatmostsevencontentwords.Thisisanapproximationofouractualinformation581

need:“returnalldocumentscontaininga1anda2inthesamesentence”.(cid:4)Thereturneddocuments(limitedbyGoogletotheﬁrst1000)aredownloaded,andthenthetextisextractedusingtheHTMLparserfromtheJavaSwingpackage.Wheneverpossible,theappropriateHTMLtags(e.g.BR,DD,P,etc.)areusedashardend-of-sentenceindicators.Thetextisfurtherseg-mentedintosentenceswiththeOpenNLP1package.(cid:4)Sentencesthatdonotcontainbothargumentsa1anda2arediscarded.Foreveryremainingsentence,weﬁndtheoccurrencesofa1anda2thatareclos-esttoeachother,andcreatearelationexamplebyreplacinga1withhe1ianda2withhe2i.Allotheroccurrencesofa1anda2arereplacedwithanulltokenignoredbythesubsequencekernel.ThenumberofsentencesineverybagisshowninthelastcolumnofTables2&3.BecauseGooglealsocountspagesthataredeemedtoosimilarintheﬁrst1000,someofthebagscanberelativelysmall.AsdescribedinSection5.1,theword-argumentcorrelationsaremodeledthroughthequantityP(w|a)=C(w,a)/C(a),estimatedastheratiobe-tweenthenumberofsentencescontainingwanda,andthenumberofsentencescontaininga.Thesecountsarecomputedoverabagofsentencescon-taininga,whichiscreatedbyqueryingGooglefortheargumenta,andthenbyprocessingtheresultsasdescribedabove.7ExperimentalEvaluationEachdatasetissplitintotwosetsofbags:onefortrainingandonefortesting.Thetestdatasetwaspurposefullymadedifﬁcultbyincludingneg-ativebagswithargumentsthatduringtrainingwereusedinpositivebags,andvice-versa.Inordertoevaluatetherelationextractionperformanceatthesentencelevel,wemanuallyannotatedallinstancesfromthepositivetestbags.ThelastcolumninTa-bles2&3shows,betweenparentheses,howmanyinstancesfromthepositivetestbagsarerealpos-itiveinstances.Thecorporateacquisitiontestsethasatotalof995instances,outofwhich156arepositive.Theperson-birthplacetestsethasatotalof601instances,andonly45ofthemarepositive.Extrapolatingfromthetestsetdistribution,thepos-1http://opennlp.sourceforge.netitivebagsintheperson-birthplacedatasetaresig-niﬁcantlysparserinrealpositiveinstancesthanthepositivebagsinthecorporateacquisitiondataset.ThesubsequencekerneldescribedinSection4wasusedasacustomkernelfortheLibSVM2Javapackage.Whenrunwiththedefaultparameters,theresultswereextremelypoor–toomuchweightwasgiventotheslacktermintheobjectivefunc-tion.Minimizingtheregularizationtermisessen-tialinordertocapturesubsequencepatternssharedamongpositivebags.ThereforeLibSVMwasmod-iﬁedtosolvetheoptimizationproblemfromFig-ure2,wherethecapacityparameterCisnormal-izedbythesizeofthetransformeddataset.Inthisnewformulation,Cissettoitsdefaultvalueof1.0–changingittoothervaluesdidnotresultinsigniﬁ-cantimprovement.Thetrade-offbetweenfalsepos-itiveandfalsenegativeerrorsiscontrolledbytheparametercp.Whensettoitsdefaultvalueof0.5,false-negativeerrorsandfalsepositiveerrorshavethesameimpactontheobjectivefunction.Asex-pected,settingcptoasmallervalue(0.1)resultedinbetterperformance.Testswithevenlowervaluesdidnotimprovetheresults.Wecomparethefollowingfoursystems:(cid:4)SSK–MIL:ThiscorrespondstotheMILformu-lationfromSection3,withtheoriginalsubsequencekerneldescribedinSection4.(cid:4)SSK–T1:ThisistheSSK–MILsystemaug-mentedwithwordweights,sothattheTypeIbiasisreduced,asdescribedinSection5.1.(cid:4)BW-MIL:Thisisabag-of-wordskernel,inwhichtherelationexamplesareclassiﬁedbasedontheunorderedwordscontainedinthesentence.Thisbaselineshowstheperformanceofastandardtext-classiﬁcationapproachtotheproblemusingastate-of-theartalgorithm(SVM).(cid:4)SSK–SIL:Thiscorrespondstotheoriginalsub-sequencekerneltrainedwithtraditional,singlein-stancelearning(SIL)supervision.Forevaluation,wetrainonthemanuallylabeledinstancesfromthetestbags.Weuseacombinationofonepositivebagandonenegativebagfortraining,whiletheothertwobagsareusedfortesting.Theresultsareaver-agedoverallfourpossiblecombinations.NotethatthesupervisionprovidedtoSSK–SILrequiressig-2http://www.csie.ntu.edu.tw/˜cjlin/libsvm582

 0 10 20 30 40 50 60 70 80 90 100 0 10 20 30 40 50 60 70 80 90 100Precision (%)Recall (%)SSK-T1SSK-MILBW-MIL 0 10 20 30 40 50 60 70 80 90 100 0 10 20 30 40 50 60 70 80 90 100Precision (%)Recall (%)SSK-T1SSK-MILBW-MIL(a)CorporateAcquisitions(b)Person-BirthplaceFigure3:Precision-Recallgraphsonthetwodatasets.niﬁcantlymoreannotationeffort,therefore,givenasufﬁcientamountoftrainingexamples,weexpectthissystemtoperformatleastaswellasitsMILcounterpart.InFigure3,precisionisplottedagainstrecallbyvaryingathresholdonthevalueoftheSVMdeci-sionfunction.Toavoidclutter,weshowonlythegraphsfortheﬁrstthreesystems.InTable4weshowtheareaundertheprecisionrecallcurvesofallfoursystems.Overall,thelearnedrelationextrac-torsareabletoidentifytherelationshipinnovelsen-tencesquiteaccuratelyandsigniﬁcantlyout-performabag-of-wordsbaseline.ThenewversionofthesubsequencekernelSSK–T1issigniﬁcantlymoreaccurateintheMILsettingthantheoriginalsub-sequencekernelSSK–MIL,andisalsocompetitivewithSSK–SIL,whichwastrainedusingareason-ableamountofmanuallylabeledsentenceexamples.DatasetSSK–MILSSK–T1BW–MILSSK–SIL(a)CA76.9%81.1%45.9%80.4%(b)PB72.5%78.2%69.2%73.4%Table4:AreaUnderPrecision-RecallCurve.8FutureWorkAninterestingpotentialapplicationofourapproachisawebrelation-extractionsystemsimilartoGoogleSets,inwhichtheuserprovidesonlyahandfulofpairsofentitiesknowntoexhibitornottoexhibitaparticularrelation,andthesystemisusedtoﬁndotherpairsofentitiesexhibitingthesamerelation.Ideally,theuserwouldonlyneedtoprovidepos-itivepairs.Sentencescontainingoneoftherela-tionargumentscouldbeextractedfromtheweb,andlikelynegativesentenceexamplesautomaticallycre-atedbypairingthisentitywithothernamedenti-tiesmentionedinthesentence.Inthisscenario,thetrainingsetcancontainbothfalsepositiveandfalsenegativenoise.OneusefulsideeffectisthatTypeIbiasispartiallyremoved–somebiasstillremainsduetocombinationsofatleasttwowords,eachcor-relatedwithadifferentargumentoftherelation.WearealsoinvestigatingmethodsforreducingTypeIIbias,eitherbymodifyingthewordweights,orbyintegratinganappropriatemeasureofworddistri-butionacrosspositivebagsdirectlyintheobjectivefunctionfortheMILproblem.Alternatively,im-plicitnegativeevidencecanbeextractedfromsen-tencesinpositivebagsbyexploitingthefactthat,be-sidesthetworelationarguments,asentencefromapositivebagmaycontainotherentitymentions.Anypairofentitiesdifferentfromtherelationpairisverylikelytobeanegativeexampleforthatrelation.ThisissimilartotheconceptofnegativeneighborhoodsintroducedbySmithandEisner(2005),andhasthepotentialofeliminatingbothTypeIandTypeIIbias.9RelatedWorkOneoftheearliestIEmethodsdesignedtoworkwithareducedamountofsupervisionisthatofHearst(1992),whereasmallsetofseedpatternsisusedinabootstrappingfashiontominepairsof583

hypernym-hyponymnouns.Bootstrappingisactu-allyorthogonaltoourmethod,whichcouldbeusedasthepatternlearnerineverybootstrappingitera-tion.AmorerecentIEsystemthatworksbyboot-strappingrelationextractionpatternsfromthewebisKNOWITALL(Etzionietal.,2005).Foragiventar-getrelation,supervisioninKNOWITALLisprovidedasaruletemplatecontainingwordsthatdescribetheclassofthearguments(e.g.“company”),andasmallsetofseedextractionpatterns(e.g.“hasacquired”).Inourapproach,thetypeofsupervisionisdifferent–weaskonlyforpairsofentitiesknowntoexhibitthetargetrelationornot.Also,KNOWITALLrequireslargenumbersofsearchenginequeriesinordertocollectandvalidateextractionpatterns,thereforeex-perimentscantakeweekstocomplete.Compara-tively,theapproachpresentedinthispaperrequiresonlyasmallnumberofqueries:onequeryperrela-tionpair,andonequeryforeachrelationargument.CravenandKumlien(1999)createanoisytrain-ingsetforthesubcellular-localizationrelationbyminingMedlineforsentencesthatcontaintuplesextractedfromrelevantmedicaldatabases.Toourknowledge,thisistheﬁrstapproachthatisusinga“weakly”labeleddatasetforrelationextraction.Theresultingbagshoweverareverydenseinpositiveex-amples,andtheyarealsomanyandsmall–conse-quently,thetwotypesofbiasarenotlikelytohavesigniﬁcantimpactontheirsystem’sperformance.10ConclusionWehavepresentedanewapproachtorelationex-tractionthatleveragesthevastamountofinforma-tionavailableontheweb.ThenewREsystemistrainedusingonlyahandfulofentitypairsknowntoexhibitandnotexhibitthetargetrelationship.Wehaveextendedanexistingrelationextractionker-neltolearninthissettingandtoresolveproblemscausedbytheminimalsupervisionprovided.Exper-imentalresultsdemonstratethatthenewapproachcanreliablyextractrelationsfromwebdocuments.AcknowledgmentsWewouldliketothanktheanonymousreviewersfortheirhelpfulsuggestions.ThisworkwassupportedbygrantIIS-0325116fromtheNSF,andagiftfromGoogleInc.ReferencesStuartAndrews,IoannisTsochantaridis,andThomasHofmann.2003.Supportvectormachinesformultiple-instancelearn-ing.InNIPS15,pages561–568,Vancouver,BC.MITPress.CollinF.Baker,CharlesJ.Fillmore,andJohnB.Lowe.1998.TheBerkeleyFrameNetproject.InProc.ofCOLING–ACL’98,pages86–90,SanFrancisco,CA.MorganKaufmannPublishers.RazvanC.BunescuandRaymondJ.Mooney.2006.Sub-sequencekernelsforrelationextraction.InY.Weiss,B.Sch¨olkopf,andJ.Platt,editors,NIPS18.M.CravenandJ.Kumlien.1999.Constructingbiologi-calknowledgebasesbyextractinginformationfromtextsources.InProc.ofISMB’99,pages77–86,Heidelberg,Germany.AronCulottaandJeffreySorensen.2004.Dependencytreekernelsforrelationextraction.InProc.ofACL’04,pages423–429,Barcelona,Spain,July.ThomasG.Dietterich,RichardH.Lathrop,andTomasLozano-Perez.1997.Solvingthemultipleinstanceproblemwithaxis-parallelrectangles.ArtiﬁcialIntelligence,89(1-2):31–71.OrenEtzioni,MichaelCafarella,DougDowney,Ana-MariaPopescu,TalShaked,StephenSoderland,DanielS.Weld,andAlexanderYates.2005.Unsupervisednamed-entityex-tractionfromtheweb:anexperimentalstudy.ArtiﬁcialIn-telligence,165(1):91–134.T.Gartner,P.A.Flach,A.Kowalczyk,andA.J.Smola.2002.Multi-instancekernels.InInProc.ofICML’02,pages179–186,Sydney,Australia,July.MorganKaufmann.M.A.Hearst.1992.Automaticacquisitionofhyponymsfromlargetextcorpora.InProc.ofACL’92,Nantes,France.JudeaPearl.1986.Fusion,propagation,andstructuringinbe-liefnetworks.ArtiﬁcialIntelligence,29(3):241–288.SoumyaRayandMarkCraven.2005.Supervisedversusmul-tipleinstancelearning:Anempiricalcomparison.InProc.ofICML’05,pages697–704,Bonn,Germany.BernhardSch¨olkopfandAlexanderJ.Smola.2002.Learningwithkernels-supportvectormachines,regularization,opti-mizationandbeyond.MITPress,Cambridge,MA.N.A.SmithandJ.Eisner.2005.Contrastiveestimation:Train-inglog-linearmodelsonunlabeleddata.InProc.ofACL’05,pages354–362,AnnArbor,Michigan.VladimirN.Vapnik.1998.StatisticalLearningTheory.JohnWiley&Sons.D.Zelenko,C.Aone,andA.Richardella.2003.Kernelmeth-odsforrelationextraction.JournalofMachineLearningResearch,3:1083–1106.Q.Zhang,S.A.Goldman,W.Yu,andJ.Fritts.2002.Content-basedimageretrievalusingmultiple-instancelearning.InProc.ofICML’02,pages682–689.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 584–591,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

584

 A Seed-driven Bottom-up Machine Learning Framework  for Extracting Relations of Various Complexity Feiyu Xu, Hans Uszkoreit and Hong Li Language Technology Lab, DFKI GmbH Stuhlsatzenhausweg 3, D-66123 Saarbruecken {feiyu,uszkoreit,hongli}@dfki.de   Abstract A minimally supervised machine learning framework is described for extracting rela-tions of various complexity. Bootstrapping starts from a small set of n-ary relation in-stances as “seeds”, in order to automati-cally learn pattern rules from parsed data, which then can extract new instances of the relation and its projections. We propose a novel rule representation enabling the composition of n-ary relation rules on top of the rules for projections of the relation. The compositional approach to rule con-struction is supported by a bottom-up pat-tern extraction method. In comparison to other automatic approaches, our rules can-not only localize relation arguments but also assign their exact target argument roles. The method is evaluated in two tasks: the extraction of Nobel Prize awards and management succession events. Perfor-mance for the new Nobel Prize task is strong. For the management succession task the results compare favorably with those of existing pattern acquisition ap-proaches.  1 Introduction Information extraction (IE) has the task to discover n-tuples of relevant items (entities) belonging to an n-ary relation in natural language documents. One of the central goals of the ACE program1 is to de-velop a more systematically grounded approach to IE starting from elementary entities, binary rela-                                                 1 http://projects.ldc.upenn.edu/ace/ tions to n-ary relations such as events. Current semi- or unsupervised approaches to automatic pattern acquisition are either limited to a certain linguistic representation (e.g., subject-verb-object), or only deal with binary relations, or cannot assign slot filler roles to the extracted arguments, or do not have good selection and filtering methods to handle the large number of tree patterns (Riloff, 1996; Agichtein and Gravano, 2000; Yangarber, 2003; Sudo et al., 2003; Greenwood and Stevenson, 2006; Stevenson and Greenwood, 2006). Most of these approaches do not consider the linguistic in-teraction between relations and their projections on k dimensional subspaces where 1≤k<n, which is important for scalability and reusability of rules.  Stevenson and Greenwood (2006) present a sys-tematic investigation of the pattern representation models and point out that substructures of the lin-guistic representation and the access to the embed-ded structures are important for obtaining a good coverage of the pattern acquisition. However, all considered representation models (subject-verb-object, chain model, linked chain model and sub-tree model) are verb-centered. Relations embedded in non-verb constructions such as a compound noun cannot be discovered: (1)  the 2005  Nobel Peace Prize  (1) describes a ternary relation referring to three properties of a prize: year, area and prize name. We also observe that the automatically acquired patterns in Riloff (1996), Yangarber (2003), Sudo et al. (2003), Greenwood and Stevenson (2006) cannot be directly used as relation extraction rules because the relation-specific argument role infor-mation is missing. E.g., in the management succes-sion domain that concerns the identification of job changing events, a person can either move into a 585

job (called Person_In) or leave a job (called Per-son_Out). (2) is a simplified example of patterns extracted by these systems: (2) <subject: person> verb <object:organisation>  In (2), there is no further specification of whether the person entity in the subject position is Per-son_In or Person_Out.   The ambitious goal of our approach is to provide a general framework for the extraction of relations and events with various complexity. Within this framework, the IE system learns extraction pat-terns automatically and induces rules of various complexity systematically, starting from sample relation instances as seeds. The arity of the seed determines the complexity of extracted relations. The seed helps us to identify the explicit linguistic expressions containing mentionings of relation in-stances or instances of their k-ary projections where 1≤k<n. Because our seed samples are not linguistic patterns, the learning system is not re-stricted to a particular linguistic representation and is therefore suitable for various linguistic analysis methods and representation formats. The pattern discovery is bottom-up and compositional, i.e., complex patterns can build on top of simple pat-terns for projections.  We propose a rule representation that supports this strategy. Therefore, our learning approach is seed-driven and bottom-up. Here we use depend-ency trees as input for pattern extraction. We con-sider only trees or their subtrees containing seed arguments. Therefore, our method is much more efficient than the subtree model of Sudo et al., (2003), where all subtrees containing verbs are taken into account. Our pattern rule ranking and filtering method considers two aspects of a pattern: its domain relevance and the trustworthiness of its origin. We tested our framework in two domains: Nobel Prize awards and management succession. Evaluations have been conducted to investigate the performance with respect to the seed parameters: the number of seeds and the influence of data size and its redundancy property.  The whole system has been evaluated for the two domains consider-ing precision and recall. We utilize the evaluation strategy “Ideal Matrix” of Agichtein and Gravano (2000) to deal with unannotated test data.   The remainder of the paper is organised as fol-lows: Section 2 provides an overview of the system architecture. Section 3 discusses the rule represen-tation. In Section 4, a detailed description of the seed-driven bottom-up pattern acquisition is pre-sented. Section 5 describes our experiments with pattern ranking, filtering and rule induction. Sec-tion 6 presents the experiments and evaluations for the two application domains. Section 7 provides a conclusion and an outline of future work.   2 System Architecture Given the framework, our system architecture can be depicted as follows:  Figure 1. Architecture  This architecture has been inspired by several existing seed-oriented minimally supervised ma-chine learning systems, in particular by Snowball (Agichtein and Gravano, 2000) and ExDisco (Yangarber et al., 2000). We call our system DARE, standing for “Domain Adaptive Relation Extraction based on Seeds”. DARE contains four major components: linguistic annotation, classifier, rule learning and relation extraction. The first com-ponent only applies once, while the last three com-ponents are integrated in a bootstrapping loop.  At each iteration, rules will be learned based on the seed and then new relation instances will be ex-tracted by applying the learned rules. The new re-lation instances are then used as seeds for the next iteration of the learning cycle.  The cycle termi-nates when no new relations can be acquired. The linguistic annotation is responsible for en-riching the natural language texts with linguistic information such as named entities and depend-ency structures.  In our framework, the depth of the linguistic annotation can be varied depending on the domain and the available resources. The classifier has the task to deliver relevant paragraphs and sentences that contain seed ele-ments. It has three subcomponents: document re-586

trieval, paragraph retrieval and sentence retrieval. The document retrieval component utilizes the open source IR-system Lucene2. A translation step is built in to convert the seed into the proper IR query format. As explained in Xu et al. (2006), we generate all possible lexical variants of the seed arguments to boost the retrieval coverage and for-mulate a boolean query where the arguments are connected via conjunction and the lexical variants are associated via disjunction. However, the trans-lation could be modified. The task of paragraph retrieval is to find text snippets from the relevant documents where the seed relation arguments co-occur. Given the paragraphs, a sentence containing at least two arguments of a seed relation will be regarded as relevant. As mentioned above, the rule learning compo-nent constitutes the core of our system. It identifies patterns from the annotated documents inducing extraction rules from the patterns, and validates them.  In section 4, we will give a detailed expla-nation of this component.  The relation extraction component applies the newly learned rules to the relevant documents and extracts relation instances. The validated relation instances will then be used as new seeds for the next iteration.  3 DARE Rule Representation  Our rule representation is designed to specify the location and the role of the arguments w.r.t. the target relation in a linguistic construction. In our framework, the rules should not be restricted to a particular linguistic representation and should be adaptable to various NLP tools on demand.  A DARE rule is allowed to call further DARE rules that extract a subset of the arguments. Let us step through some example rules for the prize award domain. One of the target relations in the domain is about a person who obtains a special prize in a cer-tain area in a certain year, namely, a quaternary tuple, see (3). (4) is a domain relevant sentence.  (3) <recipient, prize, area, year> (4) Mohamed ElBaradei won the 2005 Nobel Peace Prize on Friday for his efforts to limit the spread of atomic weapons. (5) is a rule that extracts a ternary projection in-stance <prize, area, year>  from a  noun phrase                                                  2 http://www.lucene.de compound, while (6) is a rule which triggers (5) in its object argument and extracts all four arguments. (5) and (6) are useful rules for  extracting argu-ments from (4). (5)    (6)   Next we provide a definition of a DARE rule: A DARE rule has three components  1. rule name: ri; 2. output: a set A containing the n arguments of the n-ary relation, labelled with their ar-gument roles; 3. rule body in AVM format containing: - specific linguistic labels or attributes (e.g., subject, object, head, mod), de-rived from the linguistic analysis, e.g., dependency structures and the named en-tity information - rule: its value is a DARE rule which ex-tracts a subset of arguments of A  The rule in (6) is a typical DARE rule. Its sub-ject and object descriptions call appropriate DARE rules that extract a subset of the output relation arguments.  The advantages of this rule representa-tion strategy are that (1) it supports the bottom-up rule composition; (2) it is expressive enough for the representation of rules of various complexity; (3) it reflects the precise linguistic relationship among the relation arguments and reduces the template merging task in the later phase; (4) the rules for the subset of arguments may be reused for other relation extraction tasks.  The rule representation models for automatic or unsupervised pattern rule extraction discussed by 587

Stevenson and Greenwood (2006) do not account for these considerations.  4 Seed-driven Bottom-up Rule Learning  Two main approaches to seed construction have been discussed in the literature: pattern-oriented (e.g., ExDisco) and semantics-oriented (e.g., Snowball) strategies. The pattern-oriented method suffers from poor coverage because it makes the IE task too dependent on one linguistic representation construction (e.g., subject-verb-object) and has moreover ignored the fact that semantic relations and events could be dispersed over different sub-structures of the linguistic representation. In prac-tice, several tuples extracted by different patterns can contribute to one complex relation instance.   The semantics-oriented method uses relation in-stances as seeds. It can easily be adapted to all re-lation/event instances. The complexity of the target relation is not restricted by the expressiveness of the seed pattern representation. In Brin (1998) and Agichtein and Gravano (2000),  the semantics-oriented methods have proved to be effective in learning patterns for some general binary relations such as booktitle-author and company-headquarter relations. In Xu et al. (2006), the authors show that at least for the investigated task it is more effective to start with the most complex relation instance, namely, with an n-ary sample for the target n-ary relation as seed, because the seed arguments are often centred in a relevant textual snippet where the relation is mentioned.  Given the bottom-up extracted patterns, the task of the rule induction is to cluster and generalize the patterns. In compari-son to the bottom-up rule induction strategy (Califf and Mooney, 2004), our method works also in a compositional way. For reasons of space this part of the work will be reported in Xu and Uszkoreit (forthcoming).  4.1 Pattern Extraction Pattern extraction in DARE aims to find linguistic patterns which do not only trigger the relations but also locate the relation arguments. In DARE, the patterns can be extracted from a phrase, a clause or a sentence, depending on the location and the dis-tribution of the seed relation arguments.    Figure 2. Pattern extraction step 1  Figure 3. Pattern extraction step 2  Figures 2 and 3 depict the general steps of bot-tom-up pattern extraction from a dependency tree t where three seed arguments arg1, arg2 and arg3 are located. All arguments are assigned their relation roles r1, r2 and r3. The pattern-relevant subtrees are trees in which seed arguments are embedded: t1, t2 and t3. Their root nodes are n1, n2 and n3.  Figure 2 shows the extraction of a unary pattern n2_r3_i, while Figure 3 illustrates the further extraction and construction of a binary pattern n1_r1_r2_j and a ternary pattern n3_r1_r2_r3_k. In practice, not all branches in the subtrees will be kept. In the follow-ing, we give a general definition of our seed-driven bottom-up pattern extraction algorithm: input:  (i) relation = <r1, r2, ..., rn>: the target rela-tion tuple with n argument roles.  T: a set of linguistic analysis trees anno-tated with i seed relation arguments (1≤i≤n) output: P: a set of pattern instances which can ex-tract i or a subset of i arguments.  Pattern extraction:  for each tree t ∈T 588

Step 1: (depicted in Figure 2) 1. replace all terminal nodes that are instanti-ated with the seed arguments by new nodes. Label these new nodes with the seed argument roles and possibly the cor-responding entity classes; 2. identify the set of the lowest nonterminal nodes N1 in t that dominate only one ar-gument (possibly among other nodes). 3. substitute N1 by nodes labelled with the seed argument roles and their entity classes 4. prune the subtrees dominated by N1 from t and add these subtrees into P. These sub-trees are assigned the argument role infor-mation and a unique id. Step2: For i=2 to n: (depicted in Figure 3) 1. find the set of the lowest nodes N1 in t that dominate in addition to other children only i seed arguments; 2. substitute N1 by nodes labelled with the i seed argument role combination informa-tion (e.g., ri_rj) and with a unique id. 3. prune the subtrees Ti dominated by Ni in t; 4. add Ti to P together with the argument role combination information and the unique id  With this approach, we can learn rules like (6) in a straightforward way. 4.2 Rule Validation: Ranking and Filtering Our ranking strategy has incorporated the ideas proposed by Riloff (1996), Agichtein and Gravano (2000), Yangarber (2003) and Sudo et al. (2003). We take two properties of a pattern into account: • domain relevance: its distribution in the rele-vant documents and irrelevant documents (documents in other domains); • trustworthiness of its origin: the relevance score of the seeds from which it is extracted.   In Riloff (1996) and Sudo et al. (2003), the rele-vance of a pattern is mainly dependent on its oc-currences in the relevant documents vs. the whole corpus.  Relevant patterns with lower frequencies cannot float to the top. It is known that some com-plex patterns are relevant even if they have low occurrence rates. We propose a new method for calculating the domain relevance of a pattern. We assume that the domain relevance of a pattern is dependent on the relevance of the lexical terms (words or collocations) constructing the pattern, e.g., the domain relevance of (5) and (6) are de-pendent on the terms “prize” and “win” respec-tively. Given n different domains, the domain rele-vance score (DR) of a term t in a domain di is: DR(t, di)= 0, if df(t, di) =0; df(t,di)N×D×LOG(n×df(t,di)df(t,dj)j=1n∑), otherwise where • df(t, di): is the document frequency of a term t in the domain di • D: the number of the documents in di • N: the total number of the terms in di Here the domain relevance of a term is dependent both on its document frequency and its document frequency distribution in other domains. Terms mentioned by more documents within the domain than outside are more relevant (Xu et al., 2002).   In the case of n=3 such different domains might be, e.g., management succession, book review or biomedical texts. Every domain corpus should ide-ally have the same number of documents and simi-lar average document size. In the calculation of the trustworthiness of the origin, we follow Agichtein and Gravano (2000) and Yangarber (2003). Thus, the relevance of a pattern is dependent on the rele-vance of its terms and the score value of the most trustworthy seed from which it origins. Finally, the score of a pattern p is calculated as follows: score(p)=}:)(max{)(0SeedsssscoretDRTii∈×∑= where    |T|> 0 and ti ∈ T • T: is the set of the terms occur in p; • Seeds: a set of seeds from which the pat-tern is extracted; • score(s): is the score of the seed s; This relevance score is not dependent on the distri-bution frequency of a pattern in the domain corpus. Therefore, patterns with lower frequency, in par-ticular, some complex patterns, can be ranked higher when they contain relevant domain terms or come from reliable seeds. 589

5 Top down Rule Application After the acquisition of pattern rules, the DARE system applies these rules to the linguistically an-notated corpus. The rule selection strategy moves from complex to simple. It first matches the most complex pattern to the analyzed sentence in order to extract the maximal number of relation argu-ments. According to the duality principle (Yangar-ber 2001), the score of the new extracted relation instance S is dependent on the patterns from which it origins. Our score method is a simplified version of that defined by Agichtein and Gravano (2000): score(S)=1−(1−score(Pi)i=0P∏) where P={Pi} is the set of patterns that extract S.  The extracted instances can be used as potential seeds for the further pattern extraction iteration, when their scores are validated.  The initial seeds obtain 1 as their score. 6 Experiments and Evaluation  We apply our framework to two application do-mains: Nobel Prize awards and management suc-cession events.  Table 1 gives an overview of our test data sets. Data Set Name Doc Number Data Amount Nobel Prize A  (1999-2005) 2296 12,6 MB Nobel Prize B (1981-1998)  1032 5,8 MB MUC-6 199 1 MB Table1. Overview of Test Data Sets.  For the Nobel Prize award scenario, we use two test data sets with different sizes: Nobel Prize A and Nobel Prize B. They are Nobel Prize related articles from New York Times, online BBC and CNN news reports.   The target relation for the ex-periment is a quaternary relation as mentioned in (3), repeated here again: <recipient, prize, area, year>  Our test data is not annotated with target rela-tion instances. However, the entire list of Nobel Prize award events is available for the evaluation from the Nobel Prize official website3. We use it as our reference relation database for building our Ideal table (Agichtein and Gravano, 2000).      For the management succession scenario, we use the test data from MUC-6 (MUC-6, 1995) and de-                                                 3 http://nobelprize.org/ fine a simpler relation structure than the MUC-6 scenario template with four arguments:  <Person_In, Person_Out, Position, Organisation> In the following tables, we use PI for Person_In, PO for Person_Out, POS for Position and ORG for Organisation. In our experiments, we attempt to investigate the influence of the size of the seed and the size of the test data on the performance. All these documents are processed by named entity recognition (Drozdzynski et al., 2004) and depend-ency parser MINIPAR (Lin, 1998).      6.1 Nobel Prize Domain Evaluation For this domain, three test runs have been evalu-ated, initialized by one randomly selected relation instance as seed each time.  In the first run, we use the largest test data set Nobel Prize A. In the sec-ond and third runs, we have compared two random selected seed samples with 50% of the data each, namely Nobel Prize B. For data sets in this do-main, we are faced with an evaluation challenge pointed out by DIPRE (Brin, 1998) and Snowball (Agichtein and Gravano, 2000), because there is no gold-standard evaluation corpus available. We have adapted the evaluation method suggested by Agichtein and Gravano, i.e., our system is success-ful if we capture one mentioning of a Nobel Prize winner event through one instance of the relation tuple or its projections. We constructed two tables (named Ideal) reflecting an approximation of the maximal detectable relation instances: one for No-bel Prize A and another for Nobel Prize B. The Ideal tables contain the Nobel Prize winners that co-occur with the word “Nobel” in the test corpus. Then precision is the correctness of the extracted relation instances, while recall is the coverage of the extracted tuples that match with the Ideal table. In Table 2 we show the precision and recall of the three runs and their random seed sample: Recall Data Set Seed Preci-sion total time interval Nobel Prize A[Zewail, Ahmed H], nobel, chemistry,1999 71,6% 50,7% 70,9% (1999-2005) Nobel Prize B[Sen, Amartya], no-bel, economics, 1998 87,3% 31% 43% (1981-1998) Nobel Prize B[Arias, Oscar],  nobel, peace, 1987 83,8% 32% 45% (1981-1998) Table 2. Precision, Recall against the Ideal Table  The first experiment with the full test data has achieved much higher recall than the two experi-ments with the set Nobel Prize B. The two experi-ments with the Nobel Prize B corpus show similar 590

performance. All three experiments have better recalls when taking only the relation instances dur-ing the report years into account, because there are more mentionings during these years in the corpus.  Figure (6) depicts the pattern learning and new seed extracting behavior during the iterations for the first experiment. Similar behaviours are ob-served in the other two experiments.    Figure 6. Experiment with Nobel Prize A  6.2 Management Succession Domain The MUC-6 corpus is much smaller than the Nobel Prize corpus. Since the gold standard of the target relations is available, we use the standard IE preci-sion and recall method. The total gold standard table contains 256 event instances, from which we randomly select seeds for our experiments. Table 3 gives an overview of performance of the experi-ments. Our tests vary between one seed, 20 seeds and 55 seeds. Initial Seed Nr.  Precision Recall A 12.6% 7.0% 1  B 15.1% 21.8% 20  48.4%  34.2% 55  62.0% 48.0% Table 3. Results for various initial seed sets  The first two one-seed tests achieved poor per-formance. With 55 seeds, we can extract additional  67 instances to obtain the half size of the instances occurring in the corpus. Table 4 show evaluations of the single arguments. B works a little better be-cause the randomly selected single seed appears a better sample for finding the pattern for extracting PI argument.  Arg precision (A) precision (B) Recall (A) Recall (B) PI 10.9% 15.1% 8.6% 34.4% PO 28.6% - 2.3% 2.3% ORG 25.6% 100% 2.6% 2.6% POS 11.2% 11.2% 5.5% 5.5% Table 4. Evaluation of one-seed tests (A and B) Table 5 shows the performance with 20 and 55 seeds respectively. Both of them are better than the one-seed tests, while 55 seeds deliver the best per-formance in average, in particular, the recall value.   arg precision (20) precision (55) recall (20) recall (55) PI 84% 62.8% 27.9% 56.1% PO 41.2% 59% 34.2% 31.2% ORG 82.4% 58.2% 7.4% 20.2% POS 42% 64.8% 25.6% 30.6% Table 5. Evaluation of 20 and 55 seeds tests Our result with 20 seeds (precision of 48.4% and recall of 34.2%) is comparable with the best result reported by Greenwood and Stevenson (2006) with the linked chain model (precision of 0.434 and re-call of 0.265). Since the latter model uses patterns as seeds, applying a similarity measure for pattern ranking, a fair comparison is not possible. Our re-sult is not restricted to binary relations and our model also assigns the exact argument role to the Person role, i.e. Person_In or Person_Out.   We have also evaluated the top 100 event-independent binary relations such as Person-Organisation and Position-Organisation. The preci-sion of these by-product relations of our IE system is above 98%.  7 Conclusion and Future Work Several parameters are relevant for the success of a seed-based bootstrapping approach to relation extraction. One of these is the arity of the relation.  Another one is the locality of the relation instance in an average mentioning. A third one is the types of the relation arguments:  Are they  named entities in the classical sense? Are they lexically marked? Are there several arguments of the same type? Both tasks we explored involved extracting quater-nary relations. The Nobel Prize domain shows bet-ter lexical marking because of the prize name.  The management succession domain has two slots of the same NE type, i.e., persons. These differences are relevant for any relation extraction approach.   The success of the bootstrapping approach cru-cially depends on the nature of the training data base.  One of the most relevant properties of this data base is the ratio of documents to relation in-stances. Several independent reports of an instance usually yield a higher number of patterns.   The two tasks we used to investigate our method drastically differ in this respect.  The Nobel Prize 591

domain we selected as a learning domain for gen-eral award events since it exhibits a high degree of redundancy in reporting.  A Nobel Prize triggers more news reports than most other prizes.  The achieved results met our expectations.  With one randomly selected seed, we could finally extract most relevant events in some covered time interval. However, it turns out that it is not just the aver-age number of reports per events that matters but also the distribution of reportings to events.  Since the Nobel prizes data exhibit a certain type of skewed distribution, the graph exhibits properties of scale-free graphs.  The distances between events are shortened to a few steps. Therefore, we can reach most events in a few iterations. The situation is different for the management succession task where the reports came from a single newspaper.  The ratio of events to reports is close to one.  This lack of informational redundancy requires a higher number of seeds.  When we started the bootstrap-ping with a single event, the results were rather poor.  Going up to twenty seeds, we still did not get the performance we obtain in the Nobel Prize task but our results compare favorably to the per-formance of existing bootstrapping methods.  The conclusion, we draw from the observed dif-ference between the two tasks is simple:  We shall always try to find a highly redundant training data set.  If at all possible, the training data should ex-hibit a skewed distribution of reports to events.  Actually, such training data may be the only realis-tic chance for reaching a large number of rare pat-terns.  In future work we will try to exploit the web as training resource for acquiring patterns while using the parsed domain data as the source for ob-taining new seeds in bootstrapping the rules before applying these to any other nonredundant docu-ment base.  This is possible because our seed tu-ples can be translated into simple IR queries and further linguistic processing is limited to the re-trieved candidate documents.   Acknowledgement The presented research was partially supported by a grant from the German Federal Ministry of Education and Research to the project Hylap (FKZ: 01IWF02) and EU–funding for the project RASCALLI. Our special thanks go to Doug Appelt and an anonymous reviewer for their thorough and highly valuable comments.  References E. Agichtein and L. Gravano. 2000. Snowball: extract-ing relations from large plain-text collections. In ACM 2000, pages 85–94, Texas, USA. S. Brin. Extracting patterns and relations from the World-Wide Web. In Proc. 1998 Int'l Workshop on the Web and Databases (WebDB '98), March 1998. M. E. Califf and R. J. Mooney. 2004. Bottom-Up Rela-tional Learning of Pattern Matching Rules for Infor-mation Extraction. Journal of Machine Learning Re-search, MIT Press. W. Drozdzynski,  H.-U.Krieger, J.  Piskorski; U. Schä-fer, and F. Xu. 2004. Shallow Processing with Unifi-cation and Typed Feature Structures — Foundations and Applications. Künstliche Intelligenz 1:17—23.  M. A. Greenwood and M. Stevenson. 2006. Improving Semi-supervised Acquisition of Relation Extraction Patterns. In Proc. of the Workshop on Information Extraction Beyond  the Document, Australia.  D. Lin. 1998. Dependency-based evaluation of  MINI-PAR. In Workshop on the Evaluation of Parsing Sys-tems, Granada, Spain. MUC. 1995. Proceedings of the Sixth Message Under-standing Conference (MUC-6), Morgan Kaufmann. E. Riloff. 1996. Automatically Generating Extraction Patterns from Untagged Text. In Proc. of the Thir-teenth National Conference on Articial Intelligence, pages 1044–1049, Portland, OR, August. M. Stevenson and Mark A. Greenwood. 2006. Compar-ing Information Extraction Pattern Models. In Proc. of the Workshop on Information Extraction Beyond  the Document, Sydney, Australia.  K. Sudo, S. Sekine, and R. Grishman. 2003. An Im-proved Extraction Pattern Representation Model for Automatic IE Pattern Acquisition. In Proc. of ACL-03, pages 224–231, Sapporo, Japan. R. Yangarber, R. Grishman, P. Tapanainen, and S. Hut-tunen. 2000. Automatic Acquisition of Domain Knowledge for Information Extraction. In Proc. of COLING 2000, Saarbrücken, Germany. R. Yangarber. 2003. Counter-training in the Discovery of Semantic Patterns. In Proceedings of ACL-03, pages 343–350, Sapporo, Japan. F. Xu, D. Kurz, J. Piskorski and S. Schmeier. 2002. A Domain Adaptive Approach to Automatic Acquisition of Domain Relevant Terms and their Relations with Bootstrapping.  In Proc. of LREC 2002, May 2002.  F. Xu, H. Uszkoreit and H. Li. 2006. Automatic Event and Relation Detection with Seeds of Varying Com-plexity. In Proceedings of AAAI 2006 Workshop Event Extraction and Synthesis, Boston, July, 2006.  Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 592–599,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

592

A Multi-resolution Framework for Information Extraction from Free TextMstislav Maslennikov and Tat-Seng Chua Department of Computer Science National University of Singapore {maslenni,chuats}@comp.nus.edu.sg Abstract Extraction of relations between entities is an important part of Information Extraction on free text. Previous methods are mostly based on statistical correlation and depend-ency relations between entities. This paper re-examines the problem at the multi-resolution layers of phrase, clause and sen-tence using dependency and discourse rela-tions. Our multi-resolution framework ARE (Anchor and Relation) uses clausal relations in 2 ways: 1) to filter noisy de-pendency paths; and 2) to increase reliabil-ity of dependency path extraction. The re-sulting system outperforms the previous approaches by 3%, 7%, 4% on MUC4, MUC6 and ACE RDC domains respec-tively. 1 Introduction Information Extraction (IE) is the task of identify-ing information in texts and converting it into a predefined format. The possible types of informa-tion include entities, relations or events. In this paper, we follow the IE tasks as defined by the conferences MUC4, MUC6 and ACE RDC: slot-based extraction, template filling and relation ex-traction, respectively. Previous approaches to IE relied on co-occurrence (Xiao et al., 2004) and dependency (Zhang et al., 2006) relations between entities. These relations enable us to make reliable extrac-tion of correct entities/relations at the level of a single clause. However, Maslennikov et al. (2006) reported that the increase of relation path length will lead to considerable decrease in performance. In most cases, this decrease in performance occurs because entities may belong to different clauses.  Since clauses in a sentence are connected by clausal relations (Halliday and Hasan, 1976), it is thus important to perform discourse analysis of a sentence.  Discourse analysis may contribute to IE in sev-eral ways. First, Taboada and Mann (2005) re-ported that discourse analysis helps to decompose long sentences into clauses. Therefore, it helps to distinguish relevant clauses from non-relevant ones. Second, Miltsakaki (2003) stated that entities in subordinate clauses are less salient. Third, the knowledge of textual structure helps to interpret the meaning of entities in a text (Grosz and Sidner 1986). As an example, consider the sentences “ABC Co. appointed a new chairman. Addition-ally, the current CEO was retired”. The word ‘ad-ditionally’ connects the event in the second sen-tence to the entity ‘ABC Co.’ in the first sentence. Fourth, Moens and De Busser (2002) reported that discourse segments tend to be in a fixed order for structured texts such as court decisions or news. Hence, analysis of discourse order may reduce the variability of possible relations between entities. To model these factors, we propose a multi-resolution framework ARE that integrates both discourse and dependency relations at 2 levels. ARE aims to filter noisy dependency relations from training and support their evaluation with discourse relations between entities. Additionally, we encode semantic roles of entities in order to utilize semantic relations. Evaluations on MUC4, MUC6 and ACE RDC 2003 corpora demonstrates that our approach outperforms the state-of-art sys-tems mainly due to modeling of discourse rela-tions. The contribution of this paper is in applying dis-course relations to supplement dependency rela-tions in a multi-resolution framework for IE. The 593

framework enables us to connect entities in differ-ent clauses and thus improve the performance on long-distance dependency paths.  Section 2 describes related work, while Section 3 presents our proposed framework, including the extraction of anchor cues and various types of rela-tions, integration of extracted relations, and com-plexity classification. Section 4 describes our ex-perimental results, with the analysis of results in Section 5. Section 6 concludes the paper. 2 Related work Recent work in IE focuses on relation-based, se-mantic parsing-based and discourse-based ap-proaches. Several recent research efforts were based on modeling relations between entities. Cu-lotta and Sorensen (2004) extracted relationships using dependency-based kernel trees in Support Vector Machines (SVM). They achieved an F1-measure of 63% in relation detection. The authors reported that the primary source of mistakes comes from the heterogeneous nature of non-relation in-stances. One possible direction to tackle this prob-lem is to carry out further relationship classifica-tion. Maslennikov et al. (2006) classified relation path between candidate entities into simple, aver-age and hard cases. This classification is based on the length of connecting path in dependency parse tree. They reported that dependency relations are not reliable for the hard cases, which, in our opin-ion, need the extraction of discourse relations to supplement dependency relation paths. Surdeanu et al. (2003) applied semantic parsing to capture the predicate-argument sentence struc-ture. They suggested that semantic parsing is use-ful to capture verb arguments, which may be con-nected by long-distance dependency paths. How-ever, current semantic parsers such as the ASSERT are not able to recognize support verb construc-tions such as “X conducted an attack on Y” under the verb frame “attack” (Pradhan et al. 2004). Hence, many useful predicate-argument structures will be missed. Moreover, semantic parsing be-longs to the intra-clausal level of sentence analysis, which, as in the dependency case, will need the support of discourse analysis to bridge inter-clausal relations. Webber et al. (2002) reported that discourse structure helps to extract anaphoric relations. How-ever, their set of grammatical rules is heuristic. Our task needs construction of an automated approach to be portable across several domains. Cimiano et al. (2005) employed a discourse-based analysis for IE. However, their approach requires a predefined domain-dependent ontology in the format of ex-tended logical description grammar as described by Cimiano and Reely (2003). Moreover, they used discourse relations between events, whereas in our approach, discourse relations connect entities. 3 Motivation for using discourse relations  Our method is based on Rhetorical Structure The-ory (RST) by Taboada and Mann (2005). RST splits the texts into 2 parts: a) nuclei, the most im-portant parts of texts; and b) satellites, the secon-dary parts. We can often remove satellites without losing the meaning of text. Both nuclei and satel-lites are connected with discourse relations in a hierarchical structure. In our work, we use 16 classes of discourse relations between clauses: At-tribution, Background, Cause, Comparison, Condi-tion, Contrast, Elaboration, Enablement, Evalua-tion, Explanation, Joint, Manner-Means, Topic-Comment, Summary, Temporal, Topic-Change. The additional 3 relations impose a tree structure: textual-organization, span and same-unit. All the discourse relation classes are potentially useful, since they encode some knowledge about textual structure. Therefore, we decide to include all of them in the learning process to learn patterns with best possible performance. We consider two main rationales for utilizing discourse relations to IE. First, discourse relations help to narrow down the search space to the level of a single clause. For example, the sentence “[<Soc-A1>Trudeau</>'s <Soc-A2>son</> told everyone], [their prime minister was his father], [who took him to a secret base in the arctic] [and let him peek through a window].” contains 4 clauses and 7 anchor cues (key phrases) for the type Social, which leads to 21 possible variants. Splitting this sentence into clauses reduces the combinations to 4 possible variants. Additionally, this reduction eliminates the long and noisy de-pendency paths.  Second, discourse analysis enables us to connect entities in different clauses with clausal relations. As an example, we consider a sentence “It’s a dark comedy about a boy named <AT-A1>Marshal</> played by Amourie Kats who discovers all kinds of 594

on and scary things going on in <AT-A2>a seem-ingly quiet little town</>”. In this example, we need to extract the relation “At” between the enti-ties “Marshal” and “a seemingly quiet little town”. The discourse structure of this sentence is given in .Figure 1    Figure 1. Example of discourse parsing The discourse path “Marshal <-elaboration- _ <-span- _ -elaboration-> _ -elaboration-> town” is relatively short and captures the necessary rela-tions. At the same time, prediction based on de-pendency path “Marshal <–obj- _ <-i- _ <-fc- _ <-pnmod- _ <-pred- _ <-i- _ <-null- _ -null-> _ -rel-> _ -i-> _ -mod-> _ -pcomp-n-> town” is un-reliable, since the relation path is long. Thus, it is important to rely on discourse analysis in this ex-ample. In addition, we need to evaluate both the score and reliability of prediction by relation path of each type. 4 Anchors and Relations In this section, we define the key components that we use in ARE: anchors, relation types and general architecture of our system. Some of these compo-nents are also presented in detail in our previous work (Maslennikov et al., 2006). 4.1 Anchors The first task in IE is to identify candidate phrases (which we call anchor or anchor cue) of a pre-defined type  (anchor  type)  to  fill  a  desired  slot  in  an   IE   template.   The   example   anchor   for   the   phrase   “Marshal” is shown in Figure 2. Given a training set of sentences, we extract the anchor cues ACj = [A1, …, ANanch] of type Cj using the procedures described in Maslennikov et al. (2006). The linguistic features of these an-chors  for  the  anchor  types  of  Per-  petrator, Action, Victim and Target for the MUC4 domain are given in Table 1.  Anchor  typesFeature Perpetrator_Cue (A) Action_Cue (D) Victim_Cue (A) Target_Cue (A) Lexical  (Head noun) terrorists,  individuals,  Soldiers attacked, murder,  Massacre Mayor, general, priests bridge,  house,  Ministry Part-of-SpeechNoun Verb Noun Noun Named Enti-ties Soldiers  (PERSON) - Jesuit priests (PERSON) WTC  (OBJECT) Synonyms Synset 130, 166 Synset 22 Synset 68 Synset 71 Concept Class ID 2, 3 ID 9  ID 22, 43 ID 61, 48 Co-referenced entity He -> terrorist, soldier - They -> peasants - Clausal type Nucleus Satellite Nucleus, Satellite Nucleus, Satellite Nucleus, Satellite Argument type Arg0, Arg1Root  Target, -, ArgM-MNR Arg0, Arg1Arg1, ArgM-MNR Table 1. Linguistic features for anchor extraction  Given an input phrase P from a test sentence, we need to classify if the phrase belongs to anchor cue type Cj. We calculate the entity score as:   Entity_Score(P) =∑  δi * Feature_Scorei(P,Cj) (1)   where Feature_Score(P,Cj) is a score function for a particular linguistic feature representation of type Cj, and δi is the corresponding weight for that rep-resentation in the overall entity score.  The weights are learned automatically using Expectation Maxi-mization (Dempster et al., 1977). The Fea-ture_Scorei(P,Cj) is estimated from the training set as the number of slots containing the correct fea-ture representation type versus all the slots:   Feature_Scorei(P,Cj) = #(positive slots) / #(all slots) (2)  We classify the phrase P as belonging to an anchor type Cj when its Entity_score(P) is above an em-pirically determined threshold ω. We refer to this anchor as Aj. We allow a phrase to belong to mul-tiple anchor types and hence the anchors alone are not enough for filling templates. 4.2 Relations To resolve the correct filling of phrase P of type Ci in a desired slot in the template, we need to con-sider the relations between multiple candidate phrases of related slots. To do so, we consider sev-eral types of relations between anchors: discourse, dependency and semantic relations. These relations capture the interactions between anchors and are therefore useful for tackling the paraphrasing and alignment problems (Maslennikov et al., 2006). Given 2 anchors Ai and Aj of anchor types Ci and Cj, we consider a relation Pathl = [Ai, Rel1,…, Reln, Aj] between them, such that there are no an-chors between Ai and Aj. Additionally, we assume that the relations between anchors are represented in the form of a tree Tl, where l = {s, c, d} refers to Satellite who discovers all kinds of on and scary things going on in a seem-ingly quiet little town. Nucleus It's a dark comedy about a boy Satellite named Mar-shal Nucleus played by Amourie Kats Nucleus Satellitespan elaboration span elaboration elaboration span Figure 2. Exam-ple of anchor Anchor Ai  Marshal pos_NNP list_personWord Cand_AtArg1 Minipar_obj Arg2 Spade_Satellite 595

discourse, dependency and semantic relation types respectively. We describe the nodes and edges of Tl separately for each type, because their represen-tations are different: 1) The nodes of discourse tree Tc consist of clauses [Clause1, …, ClauseNcl]; and their relation edges are obtained from the Spade system described in Soricut and Marcu (2003). This system performs RST-based parsing at the sentence level. The re-ported accuracy of Spade is 49% on the RST-DT corpus. To obtain a clausal path, we map each anchor Ai to its clause in Spade. If anchors Ai and Aj belong to the same clause, we assign them the relation same-clause. es.  2) The nodes of dependency tree Td consist of words in sentences; and their relation edges are obtained from Minipar by Lin (1997). Lin (1997) reported a parsing performance of Preci-sion = 88.5% and Recall = 78.6% on the SU-SANNE corpus. 3) The nodes of semantic tree Ts consist of argu-ments [Arg0, …, ArgNarg] and targets [Target1, …, TargetNtarg]. Both arguments and targets are obtained from the ASSERT parser developed by Pradhan (2004). The reported performance of ASSERT is F1=83.8% on the identification and classification task for all arguments, evaluated using PropBank and AQUAINT as the training and testing corpora, respectively. Since the rela-tion edges have a form Targetk -> Argl, the rela-tion path in semantic frame contains only a sin-gle relation. Therefore, we encode semantic rela-tions as part of the anchor features.  In later parts of this paper, we consider only dis-course and dependency relation paths Pathl, where l={c, d}.   Figure 3. Architecture of the system 4.3 Architecture of ARE system In order to perform IE, it is important to extract candidate entities (anchors) of appropriate anchor types, evaluate the relationships between them, further evaluate all possible candidate templates, and output the final template. For the case of rela-tion extraction task, the final templates are the same as an extracted binary relation. The overall architecture of ARE is given in Figure 3. The focus of this paper is in applying discourse relations for binary relationship evaluation. 5 Overall approach In this section, we describe our relation-based ap-proach to IE. We start with the evaluation of rela-tion paths (single relation ranking, relation path ranking) to assess the suitability of their anchors as entities to template slots. Here we want to evaluate given a single relation or relation path, whether the two anchors are correct in filling the appropriate slots in a template. This is followed by the integra-tion of relation paths and evaluation of templates. 5.1 Evaluation of relation path In the first stage, we evaluate from training data the relevance of relation path Pathl = [Ai, Rel1,…, Reln, Aj] between candidate anchors Ai and Aj of types Ci and Cj. We divide this task into 2 steps. The first step ranks each single relation Relk ∈ Pathl; while the second step combines the evalua-tions of Relk to rank the whole relation path Pathl.  Single relation ranking Let Seti and Setj be the set of linguistic features of anchors Ai and Aj respectively. To evaluate Relk, we consider 2 characteristics: (1) the direction of relation Relk as encoded in the tree structure; and (2) the linguistic features, Seti and Setj, of anchors Ai and Aj. We need to construct multiple single relation classifiers, one for each anchor pair of types Ci and Cj, to evaluate the relevance of Relk with respect to these 2 anchor typPreprocessing Corpus  (a) Construction of classifiers. The training data to each classifier consists of anchor pairs of types Ci and Cj extracted from the training corpus. We use these anchor pairs to construct each classifier in four stages. First, we compose the set of possi-ble patterns in the form P+ = { Pm = <Si –Rel-> Sj> | Si ∈ Seti , Sj ∈ Setj }. The construction of Pm Anchor evaluation Templates Anchor NEs Template evaluation Sentences Binary relationship evaluation Candidate templates 596

conforms to the 2 characteristics given above. Figure 4 illustrates several discourse and depend-ency patterns of P+ constructed from a sample sen-tence.    Figure 4.  Examples of discourse and dependency patterns Second, we identify the candidate anchor A, whose type matches slot C in a template. Third, we find the correct patterns for the following 2 cases: 1) Ai, Aj are of correct anchor types; and 2) Ai is an action anchor, while Aj is a correct anchor. Any other patterns are considered as incorrect. We note that the discourse and dependency paths between anchors Ai and Aj are either correct or wrong si-multaneously.   Fourth, we evaluate the relevance of each pat-tern Pm ∈ P+. Given the training set, let PairSetm be the set of anchor pairs extracted by Pm; and PairSet+(Ci, Cj) be the set of correct anchor pairs of types Ci, Cj. We evaluate both precision and recall of Pm as     |||||),(||)(mjimmPairSetCCPairsSetPairSetPrecisionP|=+Ι  (3)      ||),(|||),(||)(jijimmCCPairsSetCCPairsSetPairSetPecallR++|=Ι  (4)  These values are stored and used in the training model for use during testing.  (b) Evaluation of relation. Here we want to evaluate whether relation InputRel belongs to a path between anchors InputAi and InputAj. We employ the constructed classifier for the anchor types InputCi and InputCj in 2 stages. First, we find a subset P(0) = { Pm = <Si –InputRel-> Sj> ∈ P+  | Si ∈ InputSeti, Sj ∈ InputSetj } of applicable patterns. Second, we utilize P(0) to find the pattern Pm(0) with maximal precision:     Precision(Pm(0)) = argmaxPm∈P(0) Precision (Pm)(5)  A problem arises if Pm(0) is evaluated only on a small amount of training instances. For example, we noticed that patterns that cover 1 or 2 instances may lead to Precision=1, whereas on the testing corpus their accuracy becomes less than 50%. Therefore, it is important to additionally consider the recall parameter of Pm(0). Relation path ranking In this section, we want to evaluate relation path connecting template slots Ci and Cj. We do this independently for each relation of type discourse and dependency. Let Recallk and Precisionk be the recall  and precision values of Relk in Path = [Ai, Rel1,…, Reln, Aj], both obtained from the previous step. First, we calculate the average recall of the involved relations:   W = (1/LengthPath) * ∑Relk∈Path Recallk(6)  W gives the average recall of the involved rela-tions and can be used as a measure of reliability of the relation Path. Next, we compute a combined score of average Precisionk weighted by Recallk:    Score = 1/(W*LengthPath)*∑Relk∈Path Recallk*Precisionk(7)  We use all Precisionk values in the path here, be-cause omitting a single relation may turn a correct path into the wrong one, or vice versa. The com-bined score value is used as a ranking of the rela-tion path. Experiments show that we need to give priority to scores with higher reliability W. Hence we use (W, Score) to evaluate each Path.  5.2 Integration of different relation path types The purpose of this stage is to integrate the evalua-tions for different types of relation paths. The input to this stage consists of evaluated relation paths PathC and PathD for discourse and dependency relations respectively. Let (Wl, Scorel) be an evaluation for Pathl, l ∈ [c, d]. We first define an integral path PathI between Ai and Aj as: 1) PathI is enabled if at least one of Pathl, l ∈ [c, d], is en-abled; and 2) PathI is correct if at least one of Pathl is correct. To evaluate PathI, we consider the average recall Wl of each Pathl, because Wl esti-elaboration obj Anchor Aj  town pos_NN Cand_AtArg2 Minipar_pcompnArgM-Loc Spade_SatelliteAnchor Ai  Marshal pos_NNP list_personWord Cand_AtArg1 Minipar_obj Arg2 SpadeSatellite pcomp-nfc span Discourse path Dependency path i elaboration Input sentence Marshal… named <At-A1></> played by Amourie Kats who discovers all kinds of on and scary things going on in <At-A2>Dependency patterns  Minipar_obj <–i- ArgM-Loc Minipar_obj <–obj- ArgM-Loc Minipar_obj –pcompn-> Minipar_pcompnMinipar_obj –mod-> Minipar_pcompn …a seemingly quiet little town</> ... elaboration pnmod pred i null null rel imod Discourse patterns  list_personWord <–elaboration- pos_NN list_personWord –elaboration-> town list_personWord <–span- town list_personWord <–elaboration- town … 597

mates the reliability of Scorel. We define a weighted average for Pathl as:   WI = WC + WD(8)   ScoreI = 1/WI * ∑l Wl*Scorel(9)  Next, we want to determine the threshold score ScoreIO above which ScoreI is acceptable. This score may be found by analyzing the integral paths on the training corpus. Let SI = { PathI } be the set of integral paths between anchors Ai and Aj on the training set. Among the paths in SI, we need to de-fine a set function SI(X) = { PathI | ScoreI(PathI) ≥ X } and find the optimal threshold for X. We find the optimal threshold based on F1-measure, be-cause precision and recall are equally important in IE. Let SI(X)+ ⊂ SI(X) and S(X)+ ⊂ S(X) be sets of correct path extractions. Let FI(X) be F1-measure of SI(X):       ||)(||||)(||)(XSXSXPIII+= (10)  ||)(||||)(||)(++=XSXSXRII (11)     )()()(*)(*2)(XRXPXRXPXFIIIII+= (12)  Based on the computed values FI(X) for each X on the training data, we determine the optimal thresh-old as Score = argmax F (X)IOXI, which corre-sponds to the maximal expected F1-measure of anchor pair Ai and Aj.  5.3 Evaluation of templates At this stage, we have a set of accepted integral relation paths between any anchor pair Ai and Aj. The next task is to merge appropriate set of an-chors into candidate templates. Here we follow the methodology of Maslennikov et al. (2006). For each sentence, we compose a set of candidate tem-plates T using the extracted relation paths between each Ai and Aj. To evaluate each template Ti∈T, we combine the integral scores from relation paths between its anchors Ai and Aj into the overall Rela-tion_ScoreT:     MAAScoreTScoreelationRKjijiIiT∑≤≤=,1),()(_ (13)  where K is the number of extracted slots, M is the number of extracted relation paths between an-chors Ai and Aj, and ScoreI(Ai, Aj) is obtained from Equation (9). Next, we calculate the extracted entity score based on the scores of all the anchors in Ti:   ∑≤≤=KkkiTKAScoreEntityTScoreEntity1/)(_)(_ (14)  where Entity_Score(Ai) is taken from Equation (1).  Finally, we obtain the combined evaluation for a template:      ScoreT(Ti) = (1- λ) * Entity_ScoreT (Ti) +                            λ  * Relation_ScoreT (Ti) (15)  where λ is a predefined constant. In order to decide whether the template Ti should be accepted or rejected, we need to deter-mine a threshold ScoreTO from the training data. If anchors of a candidate template match slots in a correct template, we consider the candidate tem-plate as correct. Let TrainT = { Ti }  be the set of candidate templates extracted from the training data, TrainT+ ⊂ TrainT be the subset of correct candidate templates, and TotalT+ be the total set of correct templates in the training data. Also, let TrainT(X) = { Ti | ScoreT(Ti) ≥ X, Ti ∈ TrainT } be the set of candidate templates with score above X and TrainT+(X) ⊂ TrainT(X) be the subset of cor-rect candidate templates. We define the measures of precision, recall and F1 as follows:      ||)(||||)(||)(XTrainTXTrainTXPT+= (16) ||||||)(||)(++=TotalTXTrainTXRT(17)      )()()()(*2)(XRXPXRXPXFTTTTT+=   (18)  Since the performance in IE is measured in F1-measure, an appropriate threshold to be used for the most prominent candidate templates is:   ScoreTO = argmaxX FT (X) (19)  The value ScoreTO is used as a training model. During testing, we accept a candidate template In-putTi if ScoreT(InputTi) > ScoOre. TAs an additional remark, we note that domains MUC4, MUC6 and ACE RDC 2003 are signifi-cantly different in the evaluation methodology for the candidate templates. While the performance of the MUC4 domain is measured for each slot indi-vidually; the MUC6 task measures the perform-ance on the extracted templates; and the ACE RDC 2003 task evaluates performance on the matching relations. To overcome these differences, we con-struct candidate templates for all the domains and measure the required type of performance for each domain. Our candidate templates for the ACE RDC 2003 task consist of only 2 slots, which cor-respond to entities of the correct relations. 598

6 Experimental results We carry out our experiments on 3 domains: MUC4 (Terrorism), MUC6 (Management Succes-sion), and ACE-Relation Detection and Characteri-zation (2003). The MUC4 corpus contains 1,300 documents as training set and 200 documents (TST3 and TST4) as official testing set. We used a modified version of the MUC6 corpus described by Soderland (1999). This version includes 599 documents as training set and 100 documents as testing set. Following the methodology of Zhang et al. (2006), we use only the English portion of ACE RDC 2003 training data. We used 97 documents for testing and the remaining 155 documents for training. Our task is to extract 5 major relation types and 24 subtypes.  Case (%) P R F1GRID 52% 62% 57% Riloff’05 46% 51% 48% ARE (2006) 58% 61% 60% ARE 65% 61% 63% Table 2. Results on MUC4 To compare the results on the terrorism domain in MUC4, we choose the recent state-of-art sys-tems GRID by Xiao et al. (2004), Riloff et al. (2005) and ARE (2006) by Maslennikov et al. (2006) which does not utilize discourse and seman-tic relations. The comparative results are given in Table 2. It shows that our enhanced ARE results in 3% improvement in F1 measure over ARE (2006) that does not use clausal relations. The improve-ment was due to the use of discourse relations on long paths, such as “X distributed leaflets claiming responsibility for murder of Y”. At the same time, for many instances, it would be useful to store the extracted anchors for another round of learning. For example, the extracted features of discourse pattern “murder –same_clause-> HUM_PERSON” may boost the score for patterns that correspond to relation path “X <-span- _ -Elaboration->  mur-der”. In this way, high-precision patterns will sup-port the refinement of patterns with average recall and low precision. This observation is similar to that described in Ciravegna’s work on (LP)2 (Ciravegna 2001).  Case (%) P R F1 Chieu et al.’02 75% 49% 59% ARE (2006) 73% 58% 65% ARE 73% 70% 72% Table 3. Results on MUC6 Next, we present the performance of our system on MUC6 corpus (Management Succession) as shown in Table 3. The improvement of 7% in F1 is mainly due to the filtering of irrelevant depend-ency relations. Additionally, we noticed that 22% of testing sentences contain 2 answer templates, and entities in many of such templates are inter-twined. One example is the sentence “Mr. Bronc-zek who is 39 years old succeeds Kenneth Newell 55 who was named to the new post of senior vice president”, which refers to 2 positions. We there-fore we need to extract 2 templates “PersonIn: Bronczek, PersonOut: Newell” and “PersonIn: Newell, Post: senior vice president”. The discourse analysis is useful to extract the second template, while rejecting another long-distance template “PersonIn: Bronczek, PersonOut: Newell, Post: seniour vice president”. Another remark is that it is important to assign 2 anchors of ‘Cand_PersonIn’ and ‘Cand_PersonOut’ for the phrase “Kenneth Newell”.  The characteristic of the ACE corpus is that it contains a large amount of variations, while only 2% of possible dependency paths are correct. Since many of the relations occur only at the level of sin-gle clause (for example, most instances of relation At), the discourse analysis is used to eliminate long-distance dependency paths. It allows us to significantly decrease the dimensionality of the problem. We noticed that 38% of relation paths in ACE contain a single relation, 28% contain 2 rela-tions and 34% contain  ≥ 3 relations. For the case of  ≥ 3 relations, the analysis of dependency paths alone is not sufficient to eliminate the unreliable paths. Our results for general types and specific subtypes are presented in Tables 6 and 7, respec-tively.   Case (%) P R F1 Zhang et al.’06 77% 65% 70% ARE 79% 66% 73% Table 4. Results on ACE RDC’03, general types Based on our results in Table 4, discourse and dependency relations support each other in differ-ent situations. We also notice that multiple in-stances require modeling of entities in the path. Thus, in our future work we need to enrich the search space for relation patterns. This observation corresponds to that reported in Zhang et al. (2006). Discourse parsing is very important to reduce the amount of variations for specific types on ACE  599

RDC’03, as there are 48 possible anchor types.   Case (%) P R F1Zhang et al.’06 64% 51% 57% ARE 67% 54% 61% Table 5. Results on ACE RDC’03, specific types The relatively small improvement of results in Table 5 may be attributed to the following reasons: 1) it is important to model the commonality rela-tions, as was done by Zhou et al. (2006); and 2) our relation paths do not encode entities. This is different from Zhang et al. (2006), who were using entities in their subtrees. Overall, the results indicate that the use of dis-course relations leads to improvement over the state-of-art systems.  7 Conclusion We presented a framework that permits the inte-gration of discourse relations with dependency re-lations. Different from previous works, we tried to use the information about sentence structure based on discourse analysis. Consequently, our system improves the performance in comparison with the state-of-art IE systems. Another advantage of our approach is in using domain-independent parsers and features. Therefore, ARE may be easily port-able into new domains.  Currently, we explored only 2 types of relation paths: dependency and discourse. For future re-search, we plan to integrate more relations in our multi-resolution framework. References P. Cimiano and U. Reyle. 2003. Ontology-based semantic construction, underspecification and disambiguation. In Proc of the Prospects and Advances in the Syntax-Semantic Interface Workshop. P. Cimiano, U. Reyle and J. Saric. 2005. Ontology-driven discourse analysis for information extraction. Data & Knowledge Engineering, 55(1):59-83. H.L. Chieu and H.T. Ng. 2002. A Maximum Entropy Ap-proach to Information Extraction from Semi-Structured and Free Text. In Proc of AAAI-2002. F. Ciravegna. 2001. Adaptive Information Extraction from Text by Rule Induction and Generalization. In Proc of IJCAI-2001. A. Culotta and J. Sorensen J. 2004. Dependency tree ker-nels for relation extraction. In Proc of ACL-2004. A. Dempster, N. Laird, and D. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society B, 39(1):1–38. B. Grosz and C. Sidner. 1986. Attention, Intentions and  the Structure of Discourse. Computational Linguistics, 12(3):175-204. M. Halliday and R. Hasan. 1976. Cohesion in English. Longman, London. D. Lin. 1997. Dependency-based Evaluation of Minipar. In Workshop on the Evaluation of Parsing systems. M. Maslennikov, H.K. Goh and T.S. Chua. 2006. ARE: Instance Splitting Strategies for Dependency Relation-based Information Extraction. In Proc of ACL-2006. E. Miltsakaki. 2003. The Syntax-Discourse Interface: Ef-fects of the Main-Subordinate Distinction on Attention Structure. PhD thesis. M.F. Moens and R. De Busser. 2002. First steps in building a model for the retrieval of court decisions. International Journal of Human-Computer Studies, 57(5):429-446. S. Pradhan, W. Ward, K. Hacioglu, J. Martin and D. Juraf-sky. 2004. Shallow Semantic Parsing using Support Vector Machines. In Proc of HLT/NAACL-2004. E. Riloff, J. Wiebe, and W. Phillips. 2005. Exploiting Sub-jectivity Classification to Improve Information Extrac-tion. In Proc of AAAI-2005. S. Soderland. 1999. Learning Information Extraction Rules for Semi-Structured and Free Text. Machine Learning, 34:233-272. R. Soricut and D. Marcu. 2003. Sentence Level Discourse Parsing using Syntactic and Lexical Information. In Proc of HLT/NAACL. M. Surdeanu, S. Harabagiu, J. Williams, P. Aarseth. 2003. Using Predicate Arguments Structures for Information Extraction. In Proc of ACL-2003. M. Taboada and W. Mann. 2005. Applications of Rhetori-cal Structure Theory. Discourse studies, 8(4). B. Webber, M. Stone, A. Joshi and A. Knott. 2002. Anaphora and Discourse Structure. Computational Lin-guistics, 29(4). J. Xiao, T.S. Chua and H. Cui. 2004. Cascading Use of Soft and Hard Matching Pattern Rules for Weakly Su-pervised Information Extraction. In Proc of COLING-2004. M. Zhang, J. Zhang, J. Su and G. Zhou. 2006. A Compos-ite Kernel to Extract Relations between Entities with both Flat and Structured Features. In Proc of ACL-2006. G. Zhou, J. Su and M. Zhang. 2006. Modeling Commonal-ity among Related Classes in Relation Extraction. In Proc of ACL-2006. Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 600–607,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

600

Using Corpus Statistics on Entities to Improve Semi-supervised  Relation Extraction from the Web Benjamin Rosenfeld Information Systems HU School of Business, Hebrew University, Jerusalem, Israel grurgrur@gmail.com Ronen Feldman Information Systems HU School of Business, Hebrew University, Jerusalem, Israel ronen.feldman@huji.ac.il   Abstract Many errors produced by unsupervised and semi-supervised relation extraction (RE) systems occur because of wrong recogni-tion of entities that participate in the rela-tions. This is especially true for systems that do not use separate named-entity rec-ognition components, instead relying on general-purpose shallow parsing. Such sys-tems have greater applicability, because they are able to extract relations that contain attributes of unknown types. However, this generality comes with the cost in accuracy. In this paper we show how to use corpus statistics to validate and correct the arguments of extracted relation instances, improving the overall RE performance. We test the methods on SRES – a self-supervised Web relation extraction system. We also compare the performance of corpus-based methods to the performance of validation and correc-tion methods based on supervised NER components.   1 Introduction Information Extraction (IE) is the task of extract-ing factual assertions from text. Most IE systems rely on knowledge engineering or on machine learning to generate the “task model” that is subse-quently used for extracting instances of entities and relations from new text. In the knowledge engi-neering approach the model (usually in the form of extraction rules) is created manually, and in the machine learning approach the model is learned automatically from a manually labeled training set of documents. Both approaches require substantial human effort, particularly when applied to the broad range of documents, entities, and relations on the Web.  In order to minimize the manual ef-fort necessary to build Web IE systems, semi-supervised and completely unsupervised systems are being developed by many researchers.  The task of extracting facts from the Web has significantly different aims than the regular infor-mation extraction. The goal of regular IE is to identify and label all mentions of all instances of the given relation type inside a document or inside a collection of documents. Whereas, in the Web Extraction (WE) tasks we are only interested in extracting relation instances and not interested in particular mentions. This difference in goals leads to a difference in the methods of performance evaluation. The usual measures of performance of regular IE systems are precision, recall, and their combinations – the breakeven point and F-measure. Unfortunately, the true recall usually cannot be known for WE tasks. Consequently, for evaluating the performance of WE systems, the recall is substituted by the num-ber of extracted instances. WE systems usually order the extracted in-stances by the system’s confidence in their cor-rectness. The precision of top-confidence extrac-tions is usually very high, but it gets progressively lower when lower-confidence candidates are con-sidered. The curve that plots the number of extrac-tions against precision level is the best indicator of system’s quality. Naturally, for a comparision be-601

tween different systems to be meaningful, the evaluations must be performed on the same corpus. In this paper we are concerned with Web RE systems that extract binary relations between named entities. Most of such systems utilize sepa-rate named entity recognition (NER) components, which are usualy trained in a supervised way on a separate set of manually labeled documents. The NER components recognize and extract the values of relation attributes (also called arguments, or slots), while the RE systems are concerned with patterns of contexts in which the slots appear. However, good NER components only exist for common and very general entity types, such as Person, Organization, and Location. For some re-lations, the types of attributes are less common, and no ready NER components (or ready labeled training sets) exist for them. Also, some Web RE systems (e.g., KnowItAll (Etzioni, Cafarella et al. 2005)) do not use separate NER components even for known entity types, because such components are usually domain-specific and may perform poorly on cross-domain text collections extracted from the Web. In such cases, the values for relation attributes must be extracted by generic methods – shallow parsing (extracting noun phrases), or even simple substring extraction. Such methods are naturally much less precise and produce many entity-recognition errors (Feldman and Rosenfeld 2006). In this paper we propose several methods of us-ing corpus statistics to improve Web RE precision by validating and correcting the entities extracted by generic methods. The task of Web Extraction is particularly suited for the corpus statistics-based methods because of very large size of the corpora involved, and because the system is not required to identify individual mentions of the relations. Our methods of entity validation and correction are based on the following two observations: First, the entities that appear in target relations will often also appear in many other contexts, some of which may strongly discriminate in favor of entities of specific type. For example, assume the system encounters a sentence “Oracle bought PeopleSoft.” If the system works without a NER component, it only knows that “Oracle” and “Peo-pleSoft” are proper noun phrases, and its confi-dence in correctness of a candidate relation in-stance  Acquisition(Oracle, PeopleSoft)  cannot be very high. However, both entities occur many times elsewhere in the corpus, sometimes in strongly discriminating contexts, such as “Oracle is a company that…” or “PeopleSoft Inc.” If the system somehow learned that such contexts indi-cate entities of the correct type for the Acquisition relation (i.e., companies), then the system would be able to boost its confidence in both entities (“Oracle” and “PeopleSoft”) being of correct types and, consequently, in (Oracle, PeopleSoft) being a correct instance of the Acquisition relation. Another observation that we can use is the fact that the entities, in which we are interested, usually have sufficient frequency in the corpus for statisti-cal term extraction methods to perform reasonably well. These methods may often correct a wrongly placed entity boundary, which is a common mis-take of general-purpose shallow parsers. In this paper we show how to use these observa-tions to supplement a Web RE system with an en-tity validation and correction component, which is able to significantly improve the system’s accu-racy. We evaluate the methods using SRES (Feldman and Rosenfeld 2006) – a Web RE sys-tem, designed to extend and improve KnowItAll (Etzioni, Cafarella et al. 2005). The contributions of this paper are as follows: • We show how to automatically generate the validating patterns for the target relation arguments, and how to integrate the results produced by the validating patterns into the whole relation extraction system. • We show how to use corpus statistics and term extraction methods to correct the boundaries of relation arguments. • We experimentally compare the improve-ment produced by the corpus-based entity validation and correction methods with the improvements produced by two alternative validators – a CRF-based NER system trained on a separate labeled corpus, and a small manually-built rule-based NER com-ponent. The rest of the paper is organized as follows:  Section 2 describes previous work.  Section 3 out-lines the general design principles of SRES and briefly describes its components. Section 4 de-scribes in detail the different entity validation and correction methods, and Section 5 presents their 602

experimental evaluation. Section 6 contains con-clusions and directions for future work. 2 Related Work We are not aware of any work that deals specifi-cally with validation and/or correction of entity recognition for the purposes of improving relation extraction accuracy. However, the background techniques of our methods are relatively simple and known. The validation is based on the same ideas that underlie semi-supervised entity extrac-tion (Etzioni, Cafarella et al. 2005), and uses a simplified SRES code. The boundary correction process utilizes well-known term extraction meth-ods, e.g., (Su, Wu et al. 1994). We also recently became aware of the work by Downey, Broadhead and Etzioni (2007) that deals with locating entities of arbitrary types in large corpora using corpus statistics. The IE systems most similar to SRES are based on bootstrap learning: Mutual Bootstrapping (Riloff and Jones 1999), the DIPRE system (Brin 1998), and the Snowball system (Agichtein and Gravano 2000). Ravichandran and Hovy (Ravichandran and Hovy 2002) also use bootstrap-ping, and learn simple surface patterns for extract-ing binary relations from the Web. Unlike these systems, SRES surface patterns al-low gaps that can be matched by any sequences of tokens. This makes SRES patterns more general, and allows to recognize instances in sentences in-accessible to the simple surface patterns of systems such as (Brin 1998; Riloff and Jones 1999; Ravi-chandran and Hovy 2002). Another direction for unsupervised relation learning was taken in (Hasegawa, Sekine et al. 2004; Chen, Ji et al. 2005). These systems use a NER system to identify frequent pairs of entities and then cluster the pairs based on the types of the entities and the words appearing between the enti-ties. The main benefit of this approach is that all relations between two entity types can be discov-ered simultaneously and there is no need for the user to supply the relations definitions. 3 Description of SRES The goal of SRES is extracting instances of speci-fied relations from the Web without human super-vision. Accordingly, the supervised input to the system is limited to the specifications of the target relations. A specification for a given relation con-sists of the relation schema and a small set of seeds – known true instances of the relation. In the full-scale SRES, the seeds are also generated automati-cally, by using a set of generic patterns instantiated with the relation schema. However, the seed gen-eration is not relevant to this paper. A relation schema specifies the name of the rela-tion, the names and types of its arguments, and the arguments ordering. For example, the schema of the Acquisition relation Acquisition(Buyer=ProperNP,                    Acquired=ProperNP)  ordered  specifies that Acquisition has two slots, named Buyer and Acquired, which must be filled with en-tities of type ProperNP. The order of the slots is important (as signified by the word “ordered”, and as opposed to relations like Merger, which are “unordered” or, in binary case, “symmetric”). The baseline SRES does not utilize a named en-tity recognizer, instead using a shallow parser for exracting the relation slots. Thus, the only allowed entity types are ProperNP, CommonNP, and AnyNP, which mean the heads of, respectively, proper, common, and arbitrary noun phrases. In the experimental section we compare the baseline SRES to its extensions containing additional NER components. When using those components we allow further subtypes of ProperNP, and the rela-tion schema above becomes … (Buyer=Company, Acquired=Company) … The main components of SRES are the Pattern Learner, the Instance Extractor, and the Classifier. The Pattern Learner uses the seeds to learn likely patterns of relation occurrences. Then, the Instance Extractor uses the patterns to extract the candidate instances from the sentences. Finally, the Classifier assigns the confidence score to each extraction. We shall now briefly describe these components. 3.1 Pattern Learner The Pattern Learner receives a relation schema and a set of seeds. Then it finds the occurences of seeds inside a large (unlabeled) text corpus, ana-lyzes their contexts, and extracts common patterns among these contexts. The details of the patterns language and the process of pattern learning are not significant for this paper, and are described fully in (Feldman and Rosenfeld 2006). 603

3.2 Instance Extractor The Instance Extractor applies the patterns gener-ated by the Pattern Learner to the text corpus. In order to be able to match the slots of the patterns, the Instance Extractor utilizes an external shallow parser from the OpenNLP package (http://opennlp.sourceforge.net/), which is able to find all proper and common noun phrases in a sen-tence. These phrases are matched to the slots of the patterns. In other respects, the pattern matching and extraction process is straightforward. 3.3 Classifier The goal of the final classification stage is to filter the list of all extracted instances, keeping the cor-rect extractions, and removing mistakes that would always occur regardless of the quality of the pat-terns. It is of course impossible to know which ex-tractions are correct, but there exist properties of patterns and pattern matches that increase or de-crease the confidence in the extractions that they produce. These properties are turned into a set of binary features, which are processed by a linear feature-rich classifier. The classifier receives a feature vec-tor for a candidate, and produces a confidence score between 0 and 1.  The set of features is small and is not specific to any particular relation. This allows to train a model using a small amount of labeled data for one rela-tion, and then use the model for scoring the candi-dates of all other relations. Since the supervised training stage needs to be run only once, it is a part of the system development, and the complete sys-tem remains unsupervised, as demonstrated in (Feldman and Rosenfeld 2006). 4 Entity Validation and Correction In this paper we describe three different methods of validation and correction of relation arguments in the extracted instances. Two of them are “classi-cal” and are based, respectively, on the knowledge-engineering, and on the statistical supervised ap-proaches to the named entity recognition problems. The third is our novel approach, based on redun-dancy and corpus statistics. The methods are implemented as components for SRES, called Entity Validators, inserted be-tween the Instance Extractor and the Classifier. The result of applying Entity Validator to a candi-date instance is an (optionally) fixed instance, with validity values attached to all slots. There are three validity values: valid, invalid, and uncertain. The Classifier uses the validity values by con-verting them into two additional binary features, which are then able to influence the confidence of extractions. We shall now describe the three different valida-tors in details. 4.1 Small Rule-based NER validator This validator is a small Perl script that checks whether a character string conforms to a set of simple regular expression patterns, and whether it appears inside lists of known named entities. There are two sets of regular expression patterns – for Person and for Company entity types, and three large lists – for known personal names, known companies, and “other known named entities”, cur-rently including locations, universities, and gov-ernment agencies. The manually written regular expression repre-sent simple regularities in the internal structure of the entity types. For example, the patterns for Per-son include: Person = KnownFirstName  [Initial]  LastName Person = Honorific [FirstName] [Initial] LastName Honorific = (“Mr” | “Ms” | “Dr” |…) [“.”] Initial = CapitalLetter [“.”] KnownFirstName = member of                                        KnownPersonalNamesList FirstName = CapitalizedWord  LastName = CapitalizedWord LastName = CapitalizedWord [“–”CapitalizedWord] LastName = (“o” | “de” | …) “`”CapitalizedWord           … while the patterns for Company include: Company = KnownCompanyName Company = CompanyName CompanyDesignator Company = CompanyName FrequentCompanySfx KnownCompanyName = member of                                               KnownCompaniesList CompanyName = CapitalizedWord + CompanyDesignator = “inc” | “corp” | “co” | … FrequentCompanySfx = “systems” | “software” | …           … The validator works in the following way: it re-ceives a sentence with a labeled candidate entity of a specified entity type (which can be either Person or Company). It then applies all of the regular ex-pression patterns to the labeled text and to its en-604

closing context. It also checks for membership in the lists of known entities. If a boundary is incor-rectly placed according to the patterns or to the lists, it is fixed. Then, the following result is re-turned: Valid, if some pattern/list of the right entity type matched the candidate entity, while there were no matches for patterns/lists of other entity types. Invalid, if no pattern/list of the right entity type matched the candidate entity, while there were matches for patterns/lists of other entity types. Uncertain, otherwise, that is either if there were no matches at all, or if both correct and in-correct entity types matched. The number of patterns is relatively small, and the whole component consists of about 300 lines in Perl and costs several person-days of knowledge engineering work. Despite its simplicity, we will show in the experimental section that it is quite effective, and even often outperforms the CRF-based NER component, described below. 4.2 CRF-based NER validator This validator is built using a feature-rich CRF-based sequence classifier, trained upon an English dataset of the CoNLL 2003 shared task (Rosenfeld, Fresko et al. 2005). For the gazetteer lists it uses the same large lists as the rule-based component described above. The validator receives a sentence with a labeled candidate entity of a specified entity type (which can be either Person or Company). It then sends the sentence to the CRF-based classifier, which labels all named entities it knows – Dates, Times, Percents, Persons, Organizations, and Locations. If the CRF classifier places the entity boundaries differently, they are fixed. Then, the following re-sult is returned: Valid, if CRF classification of the entity accords with the expected argument type. Invalid, if CRF classification of the entity is dif-ferent from the expected argument type. Uncertain, otherwise, that is if the CRF classi-fier didn’t recognize the entity at all. 4.3 Corpus-based NER validator The goal of building the corpus-based NER valida-tor is to provide the same level of performance as the supervised NER components, while requiring neither additional human supervision nor addi-tional labeled corpora or other resources. There are several important facts that help achieve this goal. First, the relation instances that are used as seeds for the pattern learning are known to contain cor-rect instances of the right entity type. These in-stances can be used as seeds in their own right, for learning the patterns of occurrence of the corre-sponding entity types. Second, the entities in which we are interested usually appear in the corpus with a sufficient frequency. The validation is based on the first observation, while the boundary fixing on the second. Corpus-based entity validation There is a preparation stage, during which the information required for validation is extracted from the corpus. This information is the lists of all entities of every type that appears in the target rela-tions. In order to extract these lists we use a simpli-fied SRES. The entities are considered to be unary relations, and the seeds for them are taken from the slots of the target binary relations seeds. We don’t use the Classifier on the extracted entity instances. Instead, for every extracted instance we record the number of different sentences the entity was ex-tracted from. During the validation process, the validator’s task is to evaluate a given candidate entity in-stance. The validator compares the number of times the instance was extracted (during the prepa-ration stage) by the patterns for the correct entity type, and by the patterns for all other entity types. The validator then returns Valid, if the number of times the entity was ex-tracted for the specified entity type is at least 5, and at least two times bigger than the number of times it was extracted for all other entity types. Invalid, if the number of times the instance was extracted for the specified entity type is less than 5, and at least 2 times smaller than the number of times it was extracted for all other entity types. 605

Uncertain, otherwise, that is if it was never ex-tracted at all, or extracted with similar fre-quency for both correct and wrong entity types. Corpus-based correction of entity boundaries Our entity boundaries correction mechanism is similar to the known statistical term extraction techniques (Su, Wu et al. 1994). It is based on the assumption that the component words of a term (an entity in our case) are more tightly bound to each other than to the context. In the statistical sense, this fact is expressed by a high mutual information between the adjacent words belonging to the same term. There are two possible boundary fixes: remov-ing words from the candidate entity, or adding words from the context to the entity. There is a significant practical difference between the two cases. Assume that an entity boundary was placed too broadly, and included extra words. If this was a chance occurrence (and only such cases can be found by statistical methods), then the resulting sequence of tokens will be very infrequent, while its parts will have relatively high frequency. For example, consider a sequence “Formerly Microsoft Corp.”, which is produced by mistakenly labeling “Formerly” as a proper noun by the PoS tagger. While it is easy to know from the frequencies that a boundary mistake was made, it is unclear (to the system) which part is the correct entity. But since the entity (one of the parts of the candidate) has a high frequency, there is a chance that the relation instance, in which the entity appears, will be re-peated elsewhere in the corpus and will be ex-tracted correctly there. Therefore, in such case, the simplest recourse is to simply label the entity as Invalid, and not to try fixing the boundaries. On the other hand, if a word was missed from an entity (e.g., “Beverly O”, instead of “Beverly O ' Neill”), the resulting sequence will be frequent. Moreover, it is quite probable that the same boundary mistake is made in many places, because the same sequence of tokens is being analyzed in all those places. Therefore, it makes sense to try to fix the bounary in this case, especially since it can be done simply and  reliably: a word (or several words) is attached to the entity string if both their frequencies and their mutual information are above a threshold. 5 Experimental Evaluation The experiments described in this paper aim to confirm the effectiveness of the proposed corpus-based relation argument validation and correction method, and to compare its performance with the classical knowledge-engineering-based and super-vised-training-based methods. The experiments were performed with five relations: Acquisition(BuyerCompany, AcquiredCompany), Merger(Company1, Company2), CEO_Of(Company, Person), MayorOf(City, Person), InventorOf(Person, Invention). The data for the experiments were collected by the KnowItAll crawler. The data for the Acquisition and Merger consist of about 900,000 sentences for each of the two relations. The data for the bound relations consist of sentences, such that each con-tains one of a hundred values of the first (bound) attribute. Half of the hundred are frequent entities (>100,000 search engine hits), and another half are rare (<10,000 hits). For evaluating the validators we randomly se-lected a set of 10000 sentences from the corpora for each of the relations, and manually evaluated the SRES results generated from these sentences. Four sets of results were evaluated: the baseline results produced without any NER validator, and three sets of results produced using three different NER validators. For the InventorOf relation, only the corpus-based validator results can be produced, since the other two NER components cannot be adapted to validate/correct entities of type Inven-tion. The results for the five relations are shown in the Figure 1. Several conclusions can be drawn from the graphs. First, all of the NER validators improve over the baseline SRES, sometimes as much as doubling the recall at the same level of precision. In most cases the three validators show roughly similar levels of performance. A notable difference is the CEO_Of relation, where the sim-ple rule-based component performs much better than CRF, which performs yet better than the cor-pus-based component. The CEO_Of relation is tested as bound, which means that only the second relation argument, of type Person, is validated. The Person entities have much more rigid internal structure than the other entities – Companies and Inventions. Consequently, the best performing of 606

Acquisition0.500.600.700.800.901.00050100150Correct ExtractionsPrecisionBaselineRB-NERCRFCorpusMerger0.500.600.700.800.901.00050100150Correct ExtractionsPrecisionBaselineRB-NERCRFCorpus CeoOf0.500.600.700.800.901.00020406080100120Correct ExtractionsPrecisionBaselineRB-NERCRFCorpus InventorOf0.500.600.700.800.901.00020406080100120Correct ExtractionsPrecisionBaselineCorpus  Figure 1. Comparison between Baseline-SRES and its extensions with three different NER validators:  a simple Rule-Based one, a CRF-based statistical one, and a Corpus-based one.   the three validators is the rule-based, which di-rectly tests this internal structure. The CRF-based validator is also able to take advantage of the struc-ture, although in a weaker manner. The Corpus-based validator, however, works purely on the ba-sis of context, entirely disregarding the internal structure of entities, and thus performs worst of all in this case. On the other hand, the Corpus-based validator is able to improve the results for the In-ventor relation, which the other two validators are completely unable to do. It is also of interest to compare the performance of CRF-based and the rule-based NER components in other cases. As can be seen, in most cases the rule-based component, despite its simplicity, out-performs the CRF-based one. The possible reason for this is that relation extraction setting is signifi-cantly different from the classical named entity recognition setting. A classical NER system is set to maximize  the F1 measure of all mentions of all entities in the corpus. A relation argument extrac-tor, on the other hand, should maximize its per-formance on relation arguments, and apparently their statistical properties are often significantly different. 6 Conclusions We have presented a novel method for validation and correction of relation arguments for the state-of-the-art unsupervised Web relation extraction system SRES. The method is based on corpus sta-tistics and requires no human supervision and no additional corpus resources beyond the corpus that is used for relation extraction. We showed experimentally the effectiveness of our method, which performed comparably to both simple rule-based NER and a statistical CRF-based NER in the task of validating Companies, and somewhat worse in the task of validating Persons, 607

due to its complete disregard of internal structure of entities. The ways to learn and use this structure in an unsupervised way are left for future research. Our method also successfully validated the Invention entities, which are inaccessible to the other methods due to the lack of training data. In our experiments we made use of a unique fea-ture of SRES system – a feature-rich classifier that assigns confidence score to the candidate in-stances, basing its decisions on various features of the patterns and of the contexts from which the candidates were extracted. This architecture allows easy integration of the entity validation compo-nents as additional feature generators. We believe, however, that our results have greater applicability, and that the corpus statistics-based components can be added to RE systems with other architectures as well. References Agichtein, E. and L. Gravano (2000). Snowball: Ex-tracting Relations from Large Plain-Text Collections. Proceedings of the 5th ACM International Confer-ence on Digital Libraries (DL). Brin, S. (1998). Extracting Patterns and Relations from the World Wide Web. WebDB Workshop at 6th In-ternational Conference on Extending Database Tech-nology, EDBT’98, Valencia, Spain. Chen, J., D. Ji, C. L. Tan and Z. Niu (2005). Unsuper-vised Feature Selection for Relation Extraction. IJCNLP-05, Jeju Island, Korea. Downey, D., M. Broadhead and O. Etzioni (2007). Lo-cating Complex Named Entities in Web Text. IJCAI-07. Etzioni, O., M. Cafarella, D. Downey, A. Popescu, T. Shaked, S. Soderland, D. Weld and A. Yates (2005). Unsupervised named-entity extraction from the Web: An experimental study. Artificial Intelligence 165(1): 91-134. Feldman, R. and B. Rosenfeld (2006). Boosting Unsu-pervised Relation Extraction by Using NER. EMNLP-06, Sydney, Australia. Feldman, R. and B. Rosenfeld (2006). Self-Supervised Relation Extraction from the Web. ISMIS-2006, Bari, Italy. Hasegawa, T., S. Sekine and R. Grishman (2004). Dis-covering Relations among Named Entities from Large Corpora. ACL 2004. Ravichandran, D. and E. Hovy (2002). Learning Sur-face Text Patterns for a Question Answering System. 40th ACL Conference. Riloff, E. and R. Jones (1999). Learning Dictionaries for Information Extraction by Multi-level Boot-strapping. AAAI-99. Rosenfeld, B., M. Fresko and R. Feldman (2005). A Systematic Comparison of Feature-Rich Probabilis-tic Classifiers for NER Tasks. PKDD. Su, K.-Y., M.-W. Wu and J.-S. Chang (1994). A Cor-pus-based Approach to Automatic Compound Ex-traction. Meeting of the Association for Computa-tional Linguistics: 242-247.   Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 608–615,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

608

BeyondProjectivity:MultilingualEvaluationofConstraintsandMeasuresonNon-ProjectiveStructuresJiˇr´ıHavelkaInstituteofFormalandAppliedLinguisticsCharlesUniversityinPragueCzechRepublichavelka@ufal.mff.cuni.czAbstractDependencyanalysisofnaturallanguagehasgainedimportanceforitsapplicabilitytoNLPtasks.Non-projectivestructuresarecommonindependencyanalysis,there-foreweneedﬁne-grainedmeansofdescrib-ingthem,especiallyforthepurposesofmachine-learningorientedapproacheslikeparsing.Wepresentanevaluationontwelvelanguageswhichexploresseveralconstraintsandmeasuresonnon-projectivestructures.Wepursueanedge-basedap-proachconcentratingonpropertiesofin-dividualedgesasopposedtopropertiesofwholetrees.Inourevaluation,weincludepreviouslyunreportedmeasurestakingintoaccountlevelsofnodesindependencytrees.Ourempiricalresultscorroboratetheoreti-calresultsandshowthatanedge-basedap-proachusinglevelsofnodesprovidesanaccurateandatthesametimeexpressivemeansforcapturingnon-projectivestruc-turesinnaturallanguage.1IntroductionDependencyanalysisofnaturallanguagehasbeengaininganeverincreasinginterestthankstoitsap-plicabilityinmanytasksofNLP—arecentexampleisthedependencyparsingworkofMcDonaldetal.(2005),whichintroducesanapproachbasedonthesearchformaximumspanningtrees,capableofhan-dlingnon-projectivestructuresnaturally.Thestudyofdependencystructuresoccurringinnaturallanguagecanbeapproachedfromtwosides:bytryingtodelimitpermissibledependencystruc-turesthroughformalconstraints(forarecentreviewpaper,seeKuhlmannandNivre(2006)),orbypro-vidingtheirlinguisticdescription(seee.g.Vesel´aetal.(2004)andHajiˇcov´aetal.(2004)foralinguisticanalysisofnon-projectiveconstructionsinCzech.1)Wethinkthatitisworthbearinginmindthatneithersyntacticstructuresindependencytree-banks,norstructuresarisinginmachine-learningap-proaches,suchasMSTdependencyparsing,needapriorifallintoanyformalsubclassofdependencytrees.Weshouldthereforeaimatformalmeansca-pableofdescribingallnon-projectivestructuresthatarebothexpressiveandﬁne-grainedenoughtobeusefulinstatisticalapproaches,andatthesametimesuitableforanadequatelinguisticdescription.2Holanetal.(1998)ﬁrstdeﬁnedaninﬁnitehierar-chyofclassesofdependencytrees,goingfrompro-jectivetounrestricteddependencytrees,basedonthenotionofgapdegreeforsubtrees(cf.Section3).Holanetal.(2000)presentlinguisticconsiderationsconcerningCzechandEnglishwithrespecttothishierarchy(cf.alsoSection6).Inthispaper,weconsiderallconstraintsandmea-suresevaluatedbyKuhlmannandNivre(2006)—withsomeminorvariations,cf.Section4.2.Ad-1Thesetwopaperscontainanerrorconcerninganalternativeconditionofprojectivity,whichisrectiﬁedinHavelka(2005).2Theimportanceofsuchmeansbecomesmoreevidentfromtheasymptoticallynegligibleproportionofprojectivetreestoalldependencytrees;therearesuper-exponentiallymanyunre-strictedtreescomparedtoexponentiallymanyprojectivetreesonnnodes.Unrestricteddependencytrees(i.e.labelledrootedtrees)andprojectivedependencytreesarecountedbysequencesA000169andA006013(offset1),respectively,intheOn-LineEncyclopediaofSequences(Sloane,2007).609

ditionally,weintroduceseveralmeasuresnotcon-sideredintheirwork.WealsoextendtheempiricalbasisfromCzechandDanishtotwelvelanguages,whichweremadeavailableintheCoNLL-Xsharedtaskondependencyparsing.Inourevaluation,wedonotaddresstheissueofwhatpossibleeffectstheannotationsand/orconver-sionsusedwhencreatingthedatamighthaveonnon-projectivestructuresinthedifferentlanguages.Thenewlyconsideredmeasureshavetheﬁrstorbothofthefollowingdesiderata:theyarebasedonpropertiesofindividualnon-projectiveedges(cf.Deﬁnition3);andtheytakeintoaccountlevelsofnodesindependencytreesexplicitly.NoneoftheconstraintsandmeasuresinKuhlmannandNivre(2006)takeintoaccountlevelsofnodesexplicitly.Leveltypesofnon-projectiveedges,introducedbyHavelka(2005),havebothdesiderata.Theypro-videanedge-basedmeansofcharacterizingallnon-projectivestructures;theyalsohavesomefurtherin-terestingformalproperties.Weproposeanovel,moredetailedmeasure,levelsignaturesofnon-projectiveedges,combininglev-elsofnodeswiththepartitioningofgapsofnon-projectiveedgesintocomponents.Wederiveafor-malpropertyofthesesignaturesthatlinksthemtotheconstraintofwell-nestedness,whichisanexten-sionoftheresultforleveltypes(seealsoHavelka(2007b)).Thepaperisorganizedasfollows:Section2con-tainsformalpreliminaries;inSection3wereviewtheconstraintofprojectivityanddeﬁnerelatedno-tionsnecessaryinSection4,wherewedeﬁneanddiscussallevaluatedconstraintsandmeasures;Sec-tion5describesourdataandexperimentalsetup;empiricalresultsarepresentedinSection6.2FormalpreliminariesHereweprovidebasicdeﬁnitionsandnotationusedinsubsequentsections.Deﬁnition1Adependencytreeisatriple(V,→,(cid:22)),whereVisaﬁnitesetofnodes,→ade-pendencyrelationonV,and(cid:22)atotalorderonV.33Weadoptthefollowingconvention:nodesaredrawntop-downaccordingtotheirincreasinglevel,withnodesonthesamelevelbeingthesamedistancefromtheroot;nodesaredrawnfromlefttorightaccordingtothetotalorderonnodes;edgesaredrawnassolidlines,pathsasdottedcurves.Relation→modelslinguisticdependency,andsorepresentsadirected,rootedtreeonV.Therearemanywaysofcharacterizingrootedtrees,wegivehereacharacterizationviathepropertiesof→:thereisarootr∈Vsuchthatr→∗vforallv∈Vandthereisauniqueedgep→vforallv∈V,v6=r,andnoedgeintor.Relation→∗isthereﬂexivetransitiveclosureof→andisusuallycalledsubordination.Foreachnodeiwedeﬁneitslevelasthelengthofthepathr→∗i;wedenoteitleveli.Thesymmetriza-tion↔=→∪→−1makesitpossibletotalkaboutedges(pairsofnodesi,jsuchthati→j)withoutexplicitlyspecifyingtheparent(head;ihere)andthechild(dependent;jhere);so→representsdi-rectededgesand↔undirectededges.Toretaintheabilitytotalkaboutthedirectionofedges,wedeﬁneParenti↔j=(cid:26)iifi→jjifj→iandChildi↔j=(cid:26)jifi→jiifj→i.Tomaketheexpositionclearerbyavoidingoveruseofthesymbol→,weintroducenotationforrootedsubtreesnotonlyfornodes,butalsoforedges:Subtreei={v∈V|i→∗v},Subtreei↔j={v∈V|Parenti↔j→∗v}(notethatthesubtreeofanedgeisdeﬁnedrelativetoitsparentnode).Tobeabletotalkconciselyaboutthetotalorderonnodes(cid:22),wede-ﬁneopenintervalswhoseendpointsneednotbeinaprescribedorder(i,j)={v∈V|min(cid:22){i,j}≺v≺max(cid:22){i,j}}.3ConditionofprojectivityProjectivityofadependencytreecanbecharacter-izedboththroughthepropertiesofitssubtreesandthroughthepropertiesofitsedges.4Deﬁnition2AdependencytreeT=(V,→,(cid:22))isprojectiveifitsatisﬁesthefollowingequivalentcon-ditions:i→j&v∈(i,j)=⇒v∈Subtreei,(Harper&Hays)j∈Subtreei&v∈(i,j)=⇒v∈Subtreei,(Lecerf&Ihm)j1,j2∈Subtreei&v∈(j1,j2)=⇒v∈Subtreei.(Fitialov)OtherwiseTisnon-projective.4Therearemanyotherequivalentcharacterizationsofpro-jectivity,wegiveonlythreehistoricallyprominentones.610

ItwasMarcus(1965)whoprovedtheequivalenceoftheconditionsinDeﬁnition2,proposedintheearly1960’s(wedenotethembythenamesofthosetowhomMarcusattributestheirauthorship).Weseethattheantecedentsoftheprojectiv-ityconditionsmovefromedge-focusedtosubtree-focused(i.e.fromtalkingaboutdependencytotalk-ingaboutsubordination).ItistheconditionofFitialovthathasbeenmostlyexploredwhenstudyingso-calledrelaxationsofpro-jectivity.(Theconditionisusuallywordedasfol-lows:Adependencytreeisprojectiveifthenodesofallitssubtreesconstitutecontiguousintervalsinthetotalorderonnodes.)However,weﬁndtheconditionofHarper&Haystobethemostappealingfromthelinguisticpointofviewbecauseitgivesprominencetotheprimarynotionofdependencyedgesoverthederivednotionofsubordination.Wethereforeuseanedge-basedapproachwheneverweﬁnditsuitable.Tothatend,weneedthenotionofanon-projectiveedgeanditsgap.Deﬁnition3Foranyedgei↔jinadependencytreeTwedeﬁneitsgapasfollowsGapi↔j={v∈V|v∈(i,j)&v/∈Subtreei↔j}.Anedgewithanemptygapisprojective,anedgewhosegapisnon-emptyisnon-projective.5Weseethatnon-projectivearethoseedgesi↔jforwhichthereisanodevsuchthattogethertheyviolatetheconditionofHarper&Hays;wegroupallsuchnodesvintoGapi↔j,thegapofthenon-projectiveedgei↔j.Thenotionofgapisdeﬁneddifferentlyforsub-treesofadependencytree(Holanetal.,1998;Bodirskyetal.,2005).Thereitisdeﬁnedthroughthenodesofthewholedependencytreenotintheconsideredsubtreethatintervenebetweenitsnodesinthetotalorderonnodes(cid:22).4Relaxationsofprojectivity:evaluatedconstraintsandmeasuresInthissectionwepresentallconstraintsandmea-suresondependencytreesthatweevaluateempir-5Inﬁgureswithsampleconﬁgurationsweadoptthiscon-vention:foranon-projectiveedge,wedrawallnodesinitsgapexplicitlyandassumethatnonodeonanypathcrossingthespanoftheedgeliesintheintervaldelimitedbyitsendpoints.icallyinSection6.Firstwegivedeﬁnitionsofglobalconstraintsondependencytrees,thenwepresentmeasuresofnon-projectivitybasedonprop-ertiesofindividualnon-projectiveedges(someoftheedge-basedmeasureshavecorrespondingtree-basedcounterparts,howeverwedonotdiscussthemindetail).4.1TreeconstraintsWeconsiderthefollowingthreeglobalconstraintsondependencytrees:projectivity,planarity,andwell-nestedness.Allthreeconstraintscanbeappliedtomoregeneralstructures,e.g.dependencyforestsorevengeneraldirectedgraphs.Hereweadheretotheirprimaryapplicationtodependencytrees.Deﬁnition4AdependencytreeTisnon-planariftherearetwoedgesi1↔j1,i2↔j2inTsuchthati1∈(i2,j2)&i2∈(i1,j1).OtherwiseTisplanar.Planarityisarelaxationofprojectivitythatcor-respondstothe“nocrossingedges”constraint.Al-thoughitmightgetconfusedwithprojectivity,itisinfactastrictlyweakerconstraint.Planarityisequiv-alenttoprojectivityfordependencytreeswiththeirrootnodeateithertheleftorrightfringeofthetree.Planarityisarecentnameforaconstraintstud-iedunderdifferentnamesalreadyinthe1960’s—weareawareofindependentworkintheUSSR(weaklynon-projectivetrees;seethesurveypaperbyDikovskyandModina(2000)forreferences)andinCzechoslovakia(smoothtrees;Nebesk´y(1979)presentsasurveyofhisresults).Deﬁnition5AdependencytreeTisill-nestediftherearetwonon-projectiveedgesi1↔j1,i2↔j2inTsuchthati1∈Gapi2↔j2&i2∈Gapi1↔j1.OtherwiseTiswell-nested.Well-nestednesswasproposedbyBodirskyetal.(2005).Theoriginalformulationforbidsinterleav-ingofdisjointsubtreesinthetotalorderonnodes;wepresentanequivalentformulationintermsofnon-projectiveedges,derivedin(Havelka,2007b).Figure1illustratesthesubsethierarchybetweenclassesofdependencytreessatisfyingtheparticularconstraints:projective(planar(well-nested(unrestricted611

projectiveplanarwell-nestedunrestrictedFigure1:Sampledependencytrees(treessatisfycorre-spondingconstraintsandviolateallprecedingones)4.2EdgemeasuresTheﬁrsttwomeasuresarebasedontwowaysofpartitioningthegapofanon-projectiveedge—intointervalsandintocomponents.Thethirdmeasure,leveltype,isbasedonlevelsofnodes.Wealsopro-poseanovelmeasurecombininglevelsofnodesandthepartitioningofgapsintocomponents.Deﬁnition6Foranyedgei↔jinadependencytreeTwedeﬁneitsintervaldegreeasfollowsidegi↔j=numberofintervalsinGapi↔j.Byanintervalwemeanacontiguousintervalin(cid:22),i.e.amaximalsetofnodescomprisingallnodesbe-tweenitsendpointsinthetotalorderonnodes(cid:22).Thismeasurecorrespondstothetree-basedgapdegreemeasurein(KuhlmannandNivre,2006),whichwasﬁrstintroducedin(Holanetal.,1998)—thereitisdeﬁnedasthemaximumovergapdegreesofallsubtreesofadependencytree(thegapdegreeofasubtreeisthenumberofcontiguousintervalsinthegapofthesubtree).Theintervaldegreeofanedgeisboundedfromabovebythegapdegreeofthesubtreerootedinitsparentnode.Deﬁnition7Foranyedgei↔jinadependencytreeTwedeﬁneitscomponentdegreeasfollowscdegi↔j=numberofcomponentsinGapi↔j.Byacomponentwemeanaconnectedcomponentintherelation↔,inotherwordsaweakcomponentintherelation→(weconsiderrelationsinducedonthesetGapi↔jbyrelationsonT).ThismeasurewasintroducedbyNivre(2006);KuhlmannandNivre(2006)callitedgedegree.Again,theydeﬁneitasthemaximumoveralledges.Eachcomponentofagapcanberepresentedbyasinglenode,itsrootinthedependencyrelationin-ducedonthenodesofthegap(i.e.anodeofthecom-ponentclosesttotherootofthewholetree).Notethatacomponentneednotconstituteafullsubtreepositivetypetype0negativetypeFigure2:Sampleconﬁgurationswithnon-projectiveedgesofdifferentleveltypesofthedependencytree(theremaybenodesinthesubtreeofthecomponentrootthatlieoutsidethespanoftheparticularnon-projectiveedge).Deﬁnition8Theleveltype(orjusttype)ofanon-projectiveedgei↔jinadependencytreeTisde-ﬁnedasfollowsTypei↔j=levelChildi↔j−minn∈Gapi↔jleveln.Theleveltypeofanedgeistherelativedistanceinlevelsofitschildnodeandanodeinitsgapclosesttotheroot;theremaybemorethanonenodewit-nessinganedge’stype.ForsampleconﬁgurationsseeFigure2.PropertiesofleveltypesarepresentedinHavelka(2005;2007b).6Weproposeanewmeasurecombiningleveltypesandcomponentdegrees.(Wedonotuseintervalde-grees,i.e.thepartitioningofgapsintointervals,be-causewecannotspecifyauniquerepresentativeofanintervalwithrespecttothetreestructure.)Deﬁnition9Thelevelsignature(orjustsignature)ofanedgei↔jinadependencytreeTisamappingSignaturei↔j:P(V)→ZN0deﬁnedasfollowsSignaturei↔j={levelChildi↔j−levelr|riscomponentrootinGapi↔j}.(Theright-handsideisconsideredasamultiset,i.e.elementsmayrepeat.)Wecalltheelementsofasig-naturecomponentlevels.Thesignatureofanedgeisamultisetconsistingoftherelativedistancesinlevelsofallcomponentrootsinitsgapfromitschildnode.Further,wedisregardanypossibleorderingsonsignaturesandconcentrateonlyontherelativedis-tancesinlevels.Wepresentsignaturesasnon-6Forexample,presenceofnon-projectiveedgesofnonnega-tiveleveltypeinequivalenttonon-projectivityofadependencytree;moreover,allsuchedgescanbefoundinlineartime.612

decreasingsequencesandwritetheminanglebrack-etshi,componentlevelsseparatedbycommas(bydoingso,weavoidcombinatorialexplosion).Noticethatlevelsignaturessubsumeleveltypes:theleveltypeofanon-projectiveedgeisthecom-ponentlevelofanyofpossiblyseveralcomponentrootsclosesttotherootofthewholetree.Inotherwords,theleveltypeofanedgeisequaltothelargestcomponentleveloccurringinitslevelsignature.Levelsignaturesshareinterestingformalproper-tieswithleveltypesofnon-projectiveedges.ThefollowingresultisadirectextensionoftheresultspresentedinHavelka(2005;2007b).Theorem10Leti↔jbeanon-projectiveedgeinadependencytreeT.ForanycomponentcinGapi↔jrepresentedbyrootrcwithcomponentlevellc≤0(<0)thereisanon-projectiveedgev→rcinTwithTypev↔rc≥0(>0)suchthateitheri∈Gapv↔rc,orj∈Gapv↔rc.PROOF.Fromtheassumptionslc≤0andrc∈Gapi↔jtheparentvofnodercliesoutsidethespanoftheedgei↔j,hencev/∈Gapi↔j.Thuseitheri∈(v,rc),orj∈(v,rc).Sincelevelv≥levelParenti↔j,wehavethatParenti↔j/∈Subtreev,andsoeitheri∈Gapv↔rc,orj∈Gapv↔rc.Finallyfromlc=levelChildi↔j−levelrc≤0(<0)wegetlevelrc−levelChildi↔j≥0(>0),henceTypev↔rc≥0(>0).Thisresultlinkslevelsignaturestowell-nestedness:ittellsusthatwheneveranedge’ssig-naturecontainsanonpositivecomponentlevel,thewholedependencytreeisill-nested(becausethentherearetwoedgessatisfyingDeﬁnition5).Alldiscussededgemeasurestakeintegervalues:intervalandcomponentdegreestakeonlynonneg-ativevalues,leveltypesandlevelsignaturestakeintegervalues(inallcases,theirabsolutevaluesareboundedbythesizeofthewholedependencytree).Bothintervalandcomponentdegreesarede-ﬁnedalsoforprojectiveedges(forwhichtheytakevalue0),leveltypeisundeﬁnedforprojectiveedges,howeverthelevelsignatureofprojectiveedgesisdeﬁned—itistheemptymultiset/sequence.5DataandexperimentalsetupWeevaluateallconstraintsandmeasuresdescribedintheprevioussectionon12languages,whosetree-banksweremadeavailableintheCoNLL-XsharedFigure3:Samplenon-projectivetreeconsideredplanarinempiricalevaluationtaskondependencyparsing(BuchholzandMarsi,2006).Inalphabeticalordertheyare:Arabic,Bul-garian,Czech,Danish,Dutch,German,Japanese,Portuguese,Slovene,Spanish,Swedish,andTurk-ish(Hajiˇcetal.,2004;Simovetal.,2005;B¨ohmov´aetal.,2003;Kromann,2003;vanderBeeketal.,2002;Brantsetal.,2002;KawataandBartels,2000;Afonsoetal.,2002;Dˇzeroskietal.,2006;CivitTor-ruellaandMart´ıAnton´ın,2002;Nilssonetal.,2005;Oﬂazeretal.,2003).7WedonotincludeChinese,whichisalsoavailableinthisdataformat,becausealltreesinthisdatasetareprojective.Wetakethedata“asis”,althoughweareawarethatstructuresoccurringindifferentlanguagesde-pendontheannotationsand/orconversionsused(somelanguageswerenotoriginallyannotatedwithdependencysyntax,butonlyconvertedtoauniﬁeddependencyformatfromotherrepresentations).TheCoNLLdataformatisasimpletabularfor-matforcapturingdependencyanalysesofnaturallanguagesentences.Foreachsentence,itusesatechnicalrootnodetowhichdependencyanalysesofpartsofthesentence(possiblyseveral)areattached.Equivalently,therepresentationofasentencecanbeviewedasaforestconsistingofdependencytrees.Byconjoiningpartialdependencyanalysesunderonetechnicalrootnode,weletalltheiredgesinter-act.Sincethetechnicalrootcomesbeforethesen-tenceitself,nonewnon-projectiveedgesareintro-duced.However,edgesfromtechnicalrootsmayintroducenon-planarity.Therefore,inourempiricalevaluationwedisregardallsuchedgeswhencount-ingtreesconformingtotheplanarityconstraint;wealsoexcludethemfromthetotalnumbersofedges.Figure3exempliﬁeshowthismayaffectcountsofnon-planartrees;8cf.alsotheremarkafterDeﬁni-tion4.Countsofwell-nestedtreesarenotaffected.7AlldatasetsarethetrainpartsoftheCoNLL-Xsharedtask.8Thesampletreeisnon-planaraccordingtoDeﬁnition4,howeverwedonotconsideritassuch,becauseallpairsof“crossingedges”involveanedgefromthetechnicalroot(edgesfromthetechnicalrootaredepictedasdottedlines).613

6EmpiricalresultsOurcompleteresultsforglobalconstraintsonde-pendencytreesaregiveninTable1.TheyconﬁrmtheﬁndingsofKuhlmannandNivre(2006):pla-narityseemstobealmostasrestrictiveasprojectiv-ity;well-nestedness,ontheotherhand,coverslargeproportionsoftreesinalllanguages.Incontrasttoglobalconstraints,propertiesofin-dividualnon-projectiveedgesallowustopinpointthecausesofnon-projectivity.Thereforetheypro-videameansforamuchmoreﬁne-grainedclassiﬁ-cationofnon-projectivestructuresoccurringinnatu-rallanguage.Table2presentshighlightsofouranal-ysisofedgemeasures.Bothintervalandcomponentdegreestakegen-erallylowvalues.Ontheotherhand,Holanetal.(1998;2000)showthatatleastforCzechneitherofthesetwomeasurescaninprinciplebebounded.Takinglevelsofnodesintoaccountseemstobringbothbetteraccuracyandexpressivity.Sincelevelsignaturessubsumeleveltypesastheirlastcompo-nents,weonlyprovidecountsofedgesofpositive,nonpositive,andnegativeleveltypes.Forlackofspace,wedonotpresentfulldistributionsofleveltypesnoroflevelsignatures.Positiveleveltypesgiveanevenbetterﬁtwithreallinguisticdatathantheglobalconstraintofwell-nestedness(anill-nestedtreeneednotcontainanon-projectiveedgeofnonpositiveleveltype;cf.The-orem10).Forexample,inGermanlessthanonetenthofill-nestedtreescontainanedgeofnonpos-itiveleveltype.MinimumnegativeleveltypesforCzech,Slovene,Swedish,andTurkisharerespec-tively−1,−5,−2,and−4.Levelsignaturescombineleveltypesandcompo-nentdegrees,andsogiveanevenmoredetailedpic-tureofthegapsofnon-projectiveedges.Insomelanguagestheactuallyoccurringsignaturesarequitelimited,inothersthereisalargevariation.Becauseweconsideritlinguisticallyrelevant,wealsocounthowmanynon-projectiveedgescontainintheirgapsacomponentrootedinanancestoroftheedge(anancestorofanedgeisanynodeonthepathfromtherootofthewholetreetotheparentnodeoftheedge).Theproportionsofsuchnon-projectiveedgesvarywidelyamonglanguagesandforsomethispropertyseemshighlyimportant.Empiricalevidenceshowsthatedgemeasuresofnon-projectivitytakingintoaccountlevelsofnodesﬁtverywellwithlinguisticdata.Thissupportsourtheoreticalresultsandconﬁrmsthatpropertiesofnon-projectiveedgesprovideamoreaccurateaswellasexpressivemeansfordescribingnon-projectivestructuresinnaturallanguagethantheconstraintsandmeasuresconsideredbyKuhlmannandNivre(2006).7ConclusionInthispaper,weevaluateseveralconstraintsandmeasuresonnon-projectivedependencystructures.Wepursueanedge-basedapproachgivingpromi-nencetopropertiesofindividualedges.Atthesametime,weconsiderlevelsofnodesindependencytrees.Weﬁndanedge-basedapproachalsomoreappealinglinguisticallythantraditionalapproachesbasedonpropertiesofwholedependencytreesortheirsubtrees.Furthermore,edge-basedpropertiesallowmachine-learningtechniquestomodelglobalphenomenalocally,resultinginlesssparsemodels.Weproposeanewedgemeasureofnon-projectivity,levelsignaturesofnon-projectiveedges.Weprovethat,analogouslytoleveltypes,theyrelatetotheconstraintofwell-nestedness.Ourempiricalresultsontwelvelanguagescanbesummarizedasfollows:Amongtheglobalcon-straints,well-nestednessﬁtsbestwithlinguisticdata.Amongedgemeasures,thepreviouslyunre-portedmeasurestakingintoaccountlevelsofnodesstandout.Theyprovideboththebestﬁtwithlin-guisticdataofallconstraintsandmeasureswehaveconsidered,aswellasasubstantiallymoredetailedcapabilityofdescribingnon-projectivestructures.Theinterestedreadercanﬁndamorein-depthandbroader-coveragediscussionofpropertiesofdepen-dencytreesandtheirapplicationtonaturallanguagesyntaxin(Havelka,2007a).Asfuturework,weplantoinvestigatemorelan-guagesandcarryoutlinguisticanalysesofnon-projectivestructuresinsomeofthem.WewillalsoapplyourresultstostatisticalapproachestoNLPtasks,suchasdependencyparsing.AcknowledgementTheresearchreportedinthispaperwassupportedbyProjectNo.1ET201120505oftheMinistryofEducationoftheCzechRepublic.614

LanguageArabicBulgarianCzechDanishDutchGermanJapanesePortugueseSloveneSpanishSwedishTurkishill-nested179615416737114non-planar1506771378378741151086511713283561076556non-projective163690168318114865108839021718340571079580proportionofall(%)11.16%5.38%23.15%15.63%36.44%27.75%5.29%18.94%22.16%1.72%9.77%11.6%all146012823727035190133493921617044907115343306110424997Table1:Countsofdependencytreesviolatingglobalconstraintsofwell-nestedness,planarity,andprojectivity;thelastlinegivesthetotalnumbersofdependencytrees.(Anemptycellmeanscountzero.)LanguageArabicBulgarianCzechDanishDutchGermanJapanesePortugueseSloveneSpanishSwedishTurkishideg=121172423376940102091460515702398548581829813ideg=211895349119881272214627ideg=33837122491cdeg=120072323190842102641310714842466531591546623cdeg=210129278238220614315111204146cdeg=311662247434266427655Type>021172523495942105641580316672699547591847833Type≤075324133508Type<042152Signature/counth1i/92h2i/674h2i/18507h2i/555h2i/8061h2i/8407h1i/466h2i/1670h2i/384h2i/46h2i/823h2i/341h2i/56h3i/32h1i/2886h1i/115h3i/1461h1i/3112h2i/209h1i/571h1i/67h3i/7h1i/530h1i/189h3i/18h1i/10h3i/1515h3i/100h1i/512h1,1i/1503h4i/186h3i/208h3i/45h4i/4h3i/114h1,1i/91h4i/10h4i/5h4i/154h1,1i/63h4i/201h3i/1397h3i/183h1,1i/113h4i/13h1i/2h1,1i/94h3i/53h1,1i/8h5i/2h1,1i/115h4i/41h1,1i/118h2,2i/476h5i/126h1,1,1i/44h5i/12h0i/31h2,2i/31h5i/7h1,1,1i/1h0i/70h5i/16h2,2i/52h1,1,1i/312h6i/113h2,2i/29h1,1i/6h1,3i/27h1,1,1i/29h6i/6h1,1i/1h2,2i/58h1,1,1i/16h1,1,1i/25h4i/136h7i/78h2,2,2i/13h6i/4h1,1,1i/25h4i/19h7i/4h1,1,1i/48h2,2i/7h5i/23h3,3i/98h1,1i/63h4i/12h1,1,1,1i/4h4i/21h2,2,2i/10h2,2i/2h2,4i/44h6i/6h1,3i/16h2,2,2i/69h8i/49h1,1,1,1i/7h7i/2h1,2i/19h3,3i/6h9i/1h1,3i/32h2,2,2i/6h3,3i/15h1,1,1,1i/59h9i/35h1,1,1,1,1i/6h1,1,3i/2h2,2i/16h2,2,2,2i/6..............................ancestorcomp.root39711200357039781101280183239257950345onlyancestorcomp.r.3971119913685969795260182038657857340non-projective21172523570945105661584416672702550591897841proportionofall(%)0.42%0.41%2.13%1.06%5.9%2.4%1.32%1.37%2.13%0.07%1.05%1.61%all50097177394110543789171179063660394126511197607257778602818042552273Table2:Countsforedgemeasuresintervaldegree,componentdegree(forvaluesfrom1to3;largervaluesarenotincluded),leveltype(forpositive,nonpositive,andnegativevalues),levelsignature(upto10mostfrequentvalues),andnumbersofedgeswithancestorcomponentrootsintheirgapsandsolelywithancestorcomponentrootsintheirgaps;thesecondtolastlinegivesthetotalnumbersofnon-projectiveedges,thelastlinegivesthetotalnumbersofalledges—weexcludeedgesfromtechnicalroots.(Thelistingsneednotbeexhaustive;anemptycellmeanscountzero.)615

ReferencesA.Abeill´e,editor.2003.Treebanks:BuildingandUsingParsedCorpora,volume20ofText,SpeechandLanguageTechnology.KluwerAcademicPublishers,Dordrecht.S.Afonso,E.Bick,R.Haber,andD.Santos.2002.“Florestasint´a(c)tica”:atreebankforPortuguese.InProceedingsofthe3rdIntern.Conf.onLanguageResourcesandEvaluation(LREC),pages1698–1703.ManuelBodirsky,MarcoKuhlmann,andMatthiasM¨ohl.2005.Well-nesteddrawingsasmodelsofsyntacticstructure.InProceedingsofTenthConferenceonFormalGrammarandNinthMeeringonMathematicsofLanguage.A.B¨ohmov´a,J.Hajiˇc,E.Hajiˇcov´a,andB.Hladk´a.2003.ThePDT:a3-levelannotationscenario.InAbeill´e(2003),chap-ter7.S.Brants,S.Dipper,S.Hansen,W.Lezius,andG.Smith.2002.TheTIGERtreebank.InProceedingsofthe1stWorkshoponTreebanksandLinguisticTheories(TLT).S.BuchholzandE.Marsi.2006.CoNLL-Xsharedtaskonmultilingualdependencyparsing.InProceedingsofCoNLL-X.SIGNLL.M.CivitTorruellaandMaA.Mart´ıAnton´ın.2002.DesignprinciplesforaSpanishtreebank.InProceedingsofthe1stWorkshoponTreebanksandLinguisticTheories(TLT).AlexanderDikovskyandLarissaModina.2000.DependenciesontheothersideoftheCurtain.TraitementAutomatiquedesLangues(TAL),41(1):67–96.S.Dˇzeroski,T.Erjavec,N.Ledinek,P.Pajas,Z.ˇZabokrtsky,andA.ˇZele.2006.TowardsaSlovenedependencytreebank.InProceedingsofthe5thIntern.Conf.onLanguageResourcesandEvaluation(LREC).J.Hajiˇc,O.Smrˇz,P.Zem´anek,J.ˇSnaidauf,andE.Beˇska.2004.PragueArabicdependencytreebank:Developmentindataandtools.InProceedingsoftheNEMLARIntern.Conf.onArabicLanguageResourcesandTools,pages110–117.EvaHajiˇcov´a,Jiˇr´ıHavelka,PetrSgall,KateˇrinaVesel´a,andDanielZeman.2004.IssuesofProjectivityinthePragueDependencyTreebank.PragueBulletinofMathematicalLinguistics,81:5–22.Jiˇr´ıHavelka.2005.ProjectivityinTotallyOrderedRootedTrees:AnAlternativeDeﬁnitionofProjectivityandOptimalAlgorithmsforDetectingNon-ProjectiveEdgesandProjec-tivizingTotallyOrderedRootedTrees.PragueBulletinofMathematicalLinguistics,84:13–30.Jiˇr´ıHavelka.2007a.MathematicalPropertiesofDependencyTreesandtheirApplicationtoNaturalLanguageSyntax.Ph.D.thesis,InstituteofFormalandAppliedLinguistics,CharlesUniversityinPrague,CzechRepublic.Jiˇr´ıHavelka.2007b.RelationshipbetweenNon-ProjectiveEdges,TheirLevelTypes,andWell-Nestedness.InPro-ceedingsofHLT/NAACL;CompanionVolume,ShortPapers,pages61–64.Tom´aˇsHolan,VladislavKuboˇn,KarelOliva,andMartinPl´atek.1998.TwoUsefulMeasuresofWordOrderComplexity.InAlainPolgu`ereandSylvainKahane,editors,ProceedingsofDependency-BasedGrammarsWorkshop,COLING/ACL,pages21–28.Tom´aˇsHolan,VladislavKuboˇn,KarelOliva,andMartinPl´atek.2000.OnComplexityofWordOrder.TraitementAutoma-tiquedesLangues(TAL),41(1):273–300.Y.KawataandJ.Bartels.2000.StylebookfortheJapanesetreebankinVERBMOBIL.Verbmobil-Report240,Seminarf¨urSprachwissenschaft,Universit¨atT¨ubingen.M.T.Kromann.2003.TheDanishdependencytreebankandtheunderlyinglinguistictheory.InProceedingsofthe2ndWorkshoponTreebanksandLinguisticTheories(TLT).MarcoKuhlmannandJoakimNivre.2006.MildlyNon-ProjectiveDependencyStructures.InProceedingsofCOL-ING/ACL,pages507–514.SolomonMarcus.1965.Surlanotiondeprojectivit´e[Onthenotionofprojectivity].Zeitschriftf¨urMathematischeLogikundGrundlagenderMathematik,11:181–192.RyanMcDonald,FernandoPereira,KirilRibarov,andJanHajiˇc.2005.Non-ProjectiveDependencyParsingusingSpanningTreeAlgorithms.InProceedingsofHLT/EMNLP,pages523–530.LadislavNebesk´y.1979.Graphtheoryandlinguistics(chapter12).InR.J.WilsonandL.W.Beineke,editors,ApplicationsofGraphTheory,pages357–380.AcademicPress.J.Nilsson,J.Hall,andJ.Nivre.2005.MAMBAmeetsTIGER:ReconstructingaSwedishtreebankfromantiquity.InPro-ceedingsoftheNODALIDASpecialSessiononTreebanks.JoakimNivre.2006.ConstraintsonNon-ProjectiveDepen-dencyParsing.InProceedingsofEACL,pages73–80.K.Oﬂazer,B.Say,D.ZeynepHakkani-T¨ur,andG.T¨ur.2003.BuildingaTurkishtreebank.InAbeill´e(2003),chapter15.K.Simov,P.Osenova,A.Simov,andM.Kouylekov.2005.DesignandimplementationoftheBulgarianHPSG-basedtreebank.InJournalofResearchonLanguageandCom-putation–SpecialIssue,pages495–522.KluwerAcademicPublishers.NeilJ.A.Sloane.2007.On-LineEncyclopediaofIntegerSequences.Publishedelectronicallyatwww.research.att.com/˜njas/sequences/.L.vanderBeek,G.Bouma,R.Malouf,andG.vanNoord.2002.TheAlpinodependencytreebank.InComputationalLinguisticsintheNetherlands(CLIN).KateˇrinaVesel´a,Jiˇr´ıHavelka,andEvaHajiˇcov´a.2004.Con-ditionofProjectivityintheUnderlyingDependencyStruc-tures.InProceedingsofCOLING,pages289–295.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 616–623,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

616

Self-TrainingforEnhancementandDomainAdaptationofStatisticalParsersTrainedonSmallDatasetsRoiReichartICNCHebrewUniversityofJerusalemroiri@cs.huji.ac.ilAriRappoportInstituteofComputerScienceHebrewUniversityofJerusalemarir@cs.huji.ac.ilAbstractCreatinglargeamountsofannotateddatatotrainstatisticalPCFGparsersisexpensive,andtheperformanceofsuchparsersdeclineswhentrainingandtestdataaretakenfromdifferentdomains.Inthispaperweuseself-traininginordertoimprovethequalityofaparserandtoadaptittoadifferentdo-main,usingonlysmallamountsofmanuallyannotatedseeddata.Wereportsigniﬁcantimprovementbothwhentheseedandtestdataareinthesamedomainandintheout-of-domainadaptationscenario.Inparticu-lar,weachieve50%reductioninannotationcostforthein-domaincase,yieldinganim-provementof66%overpreviouswork,anda20-33%reductionforthedomainadaptationcase.Thisistheﬁrsttimethatself-trainingwithsmalllabeleddatasetsisappliedsuc-cessfullytothesetasks.Wewerealsoabletoformulateacharacterizationofwhenself-trainingisvaluable.1IntroductionStateoftheartstatisticalparsers(Collins,1999;Charniak,2000;KooandCollins,2005;CharniakandJohnson,2005)aretrainedonmanuallyanno-tatedtreebanksthatarehighlyexpensivetocreate.Furthermore,theperformanceoftheseparsersde-creasesasthedistancebetweenthegenresoftheirtrainingandtestdataincreases.Therefore,enhanc-ingtheperformanceofparserswhentrainedonsmallmanuallyannotateddatasetsisofgreatimpor-tance,bothwhentheseedandtestdataaretakenfromthesamedomain(thein-domainscenario)andwhentheyaretakenfromdifferentdomains(theout-of-domainorparseradaptationscenario).Sincetheproblemistheexpenseinmanualannotation,wede-ﬁne‘small’tobe100-2,000sentences,whicharethesizesofsentencesetsthatcanbemanuallyannotatedbyconstituentstructureinafewhours1.Self-trainingisamethodforusingunannotateddatawhentrainingsupervisedmodels.Themodelisﬁrsttrainedusingmanuallyannotated(‘seed’)data,thenthemodelisusedtoautomaticallyannotateapoolofunannotated(‘self-training’)data,andthenthemanuallyandautomaticallyannotateddatasetsarecombinedtocreatethetrainingdatafortheﬁ-nalmodel.Self-trainingofparserstrainedonsmalldatasetsisofenormouspotentialpracticalimpor-tance,duetothehugeamountsofunannotateddatathatarebecomingavailabletodayandtothehighcostofmanualannotation.Inthispaperweuseself-trainingtoenhancetheperformanceofagenerativestatisticalPCFGparser(Collins,1999)forboththein-domainandtheparseradaptationscenarios,usingonlysmallamountsofmanuallyannotateddata.Weperformfourexperi-ments,examiningallcombinationsofin-domainandout-of-domainseedandself-trainingdata.Ourresultsshowthatself-trainingisofsubstantialbeneﬁtfortheproblem.Inparticular,wepresent:•50%reductioninannotationcostwhentheseedandtestdataaretakenfromthesamedomain,whichis66%higherthananypreviousresultwithsmallmanuallyannotateddatasets.1Wenoteinpassingthatquantitativeresearchonthecostofannotationusingvariousannotationschemesisclearlylacking.617

•Theﬁrsttimethatself-trainingimprovesagen-erativeparserwhentheseedandtestdataarefromthesamedomain.•20-33%reductioninannotationcostwhentheseedandtestdataarefromdifferentdomains.•Theﬁrsttimethatself-trainingsucceedsinadaptingagenerativeparserbetweendomainsusingasmallmanuallyannotateddataset.•Theﬁrstformulation(relatedtothenumberofunknownwordsinasentence)ofwhenself-trainingisvaluable.Section2discussespreviouswork,andSection3comparesin-depthourprotocoltoapreviousone.Sections4and5presenttheexperimentalsetupandourresults,andSection6analyzestheresultsinanattempttoshedlightonthephenomenonofself-training.2RelatedWorkSelf-trainingmightseemastrangeidea:whyshouldaparsertrainedonitsownoutputlearnanythingnew?Indeed,(Clarketal.,2003)appliedself-trainingtoPOS-taggingwithpoorresults,and(Charniak,1997)appliedittoagenerativestatisti-calPCFGparsertrainedonalargeseedset(40Ksentences),withoutanygaininperformance.Recently,(McCloskyetal.,2006a;McCloskyetal.,2006b)havesuccessfullyappliedself-trainingtovariousparseradaptationscenariosusingthererank-ingparserof(CharniakandJohnson,2005).Arerankingparser(seealso(KooandCollins,2005))isalayeredmodel:thebaselayerisagenerativesta-tisticalPCFGparserthatcreatesarankedlistofkparses(say,50),andthesecondlayerisarerankerthatreorderstheseparsesusingmoredetailedfea-tures.McCloskyetal(2006a)usesections2-21oftheWSJPennTreebankasseeddataandbetween50Kto2,500KunlabeledNANCcorpussentencesasself-trainingdata.TheytrainthePCFGparserandthererankerwiththemanuallyannotatedWSJdata,andparsetheNANCdatawiththe50-bestPCFGparser.Thentheyproceedintwodirections.Intheﬁrst,theyreorderthe50-bestparselistwiththererankertocreateanew1-bestlist.Inthesecond,theyleavethe1-bestlistproducedbythegenera-tivePCFGparseruntouched.Thentheycombinethe1-bestlist(eachdirectionhasitsownlist)withtheWSJtrainingset,toretrainthePCFGparser.TheﬁnalPCFGmodelandthereranker(trainedonlyonannotatedWSJmaterial)arethenusedtoparsethetestsection(23)ofWSJ.Therearetwomajordifferencesbetweenthesepa-persandthecurrentone,stemmingfromtheirusageofarerankerandoflargeseeddata.First,whentheir1-bestlistofthebasePCFGparserwasusedasselftrainingdataforthePCFGparser(thesec-onddirection),theperformanceofthebaseparserdidnotimprove.Ithadimprovedonlywhenthe1-bestlistofthererankerwasused.Inthispaperweshowhowthe1-bestlistofabase(generative)PCFGparsercanbeusedasaself-trainingmaterialforthebaseparseritselfandenhanceitsperformance,with-outusinganyreranker.ThisrevealsanoteworthycharacteristicofgenerativePCFGmodelsandoffersapotentialdirectionforparserimprovement,sincethequalityofaparser-rerankercombinationcriti-callydependsonthatofthebaseparser.Second,thesepapersdidnotexploreself-trainingwhentheseedissmall,ascenariowhoseimportancehasbeendiscussedabove.Ingeneral,PCFGmod-elstrainedonsmalldatasetsarelesslikelytoparsetheself-trainingdatacorrectly.Forexample,thef-scoreofWSJdataparsedbythebasePCFGparserof(CharniakandJohnson,2005)whentrainedonthetrainingsectionsofWSJisbetween89%to90%,whilethef-scoreofWSJdataparsedwiththeCollins’modelthatweuse,andasmallseed,isbe-tween40%and80%.Asaresult,thegoodresultsof(McCloskyetal,2006a;2006b)withlargeseedsetsdonotimmediatelyimplysuccesswithsmallseedsets.Demonstrationofsuchsuccessisacontribu-tionofthepresentpaper.Bacchianietal(2006)exploredthescenarioofout-of-domainseeddata(theBrowntrainingsetcontainingabout20Ksentences)andin-domainself-trainingdata(between4Kto200KsentencesfromtheWSJ)andshowedanimprovementoverthebaselineoftrainingtheparserwiththeseeddataonly.However,theydidnotexplorethecaseofsmallseeddatasets(theeffortinmanuallyannotating20Kissubstantial)andtheirworkaddressesonlyoneofourscenarios(OI,seebelow).618

Aworkcloselyrelatedtooursis(Steedmanetal.,2003a),whichappliedco-training(BlumandMitchell,1998)andself-trainingtoCollins’pars-ingmodelusingasmallseeddataset(500sentencesforbothmethodsand1,000sentencesforco-trainingonly).Theseed,self-trainingandtestdatasetstheyusedaresimilartothoseweuseinourIIexperi-ment(seebelow),buttheself-trainingprotocolsaredifferent.TheyﬁrsttraintheparserwiththeseedsentencessampledfromWSJsections2-21.Then,iteratively,30sentencesaresampledfromthesesec-tions,parsedbytheparser,andthe20bestsentences(intermsofparserconﬁdencedeﬁnedasprobabilityoftopparse)areselectedandcombinedwiththepre-viouslyannotateddatatoretraintheparser.Theco-trainingprotocolissimilarexceptthateachparseristrainedwiththe20bestsentencesoftheotherparser.Self-trainingdidnotimproveparserperfor-manceontheWSJtestsection(23).Steedmanetal(2003b)followedasimilarco-trainingprotocolexceptthattheselectionfunction(threefunctionswereexplored)consideredthedifferencesbetweentheconﬁdencescoresofthetwoparsers.Inthispa-perweshowaself-trainingprotocolthatachievesbetterresultsthanallofthesemethods(Table2).Thenextsectiondiscussespossibleexplanationsforthedifferenceinresults.Steedmanetal(2003b)andHwaetal,(2003)alsousedseveralversionsofcor-rectedco-trainingwhicharenotcomparabletooursandothersuggestedmethodsbecausetheirevalua-tionrequiresdifferentmeasures(e.g.reviewedandcorrectedconstituentsareseparatelycounted).Asfarasweknow,(BeckerandOsborne,2005)istheonlyadditionalworkthattriestoimproveagenerativePCFGparsersusingsmallseeddata.Thetechniquesusedarebasedonactivelearning(Cohnetal.,1994).Theauthorstesttwonovelmethods,alongwiththetreeentropy(TE)methodof(Hwa,2004).Theseed,theunannotatedandthetestsets,aswellastheparserusedinthatwork,aresimilartothoseweuseinourIIexperiment.Ourresultsaresuperior,asshowninTable3.3Self-TrainingProtocolsTherearemanypossiblewaystodoself-training.Amaingoalofthispaperistoidentifyaself-trainingprotocolmostsuitableforenhancementanddomainadaptationofstatisticalparserstrainedonsmalldatasets.Nopreviousworkhassucceededinidentifyingsuchaprotocolforthistask.Inthissec-tionwetrytounderstandwhy.Intheprotocolweapply,theself-trainingsetcon-tainsseveralthousandsentencesAparsertrainedwithasmallseedsetparsestheself-trainingset,andthenthewholeautomaticallyannotatedself-trainingsetiscombinedwiththemanuallyannotatedseedsettoretraintheparser.ThisprotocolandthatofSteedmanetal(2003a)wereappliedtotheproblem,withthesameseed,self-trainingandtestsets.Asweshowbelow(seeSection4andSection5),whileSteedman’sprotocoldoesnotimproveoverthebase-lineofusingonlytheseeddata,ourprotocoldoes.Therearefourdifferencesbetweentheprotocols.First,Steedmanetal’sseedsetconsistsofconsecu-tiveWSJsentences,whileweselectthemrandomly.Inthenextsectionweshowthatthisdifferenceisimmaterial.Second,Steedmanetal’sprotocollooksforsentencesofhighqualityparse,whileourpro-tocolpreferstousemanysentenceswithoutcheck-ingtheirparsequality.Third,theirprotocolisitera-tivewhileoursusesasinglestep.Fourth,ourself-trainingsetisordersofmagnitudelargerthantheirs.Toexaminetheparsequalityissue,weperformedtheirexperimentusingtheirsettingbutselectingthehighqualityparsesentencesusingtheirf-scorerel-ativetothegoldstandardannotationfromsecs2-21ratherthanaqualityestimate.Noimprovementoverthebaselinewasachievedevenwiththisor-acle.Thustheproblemwiththeirprotocoldoesnotliewiththeparsequalityassessmentfunction;nootherfunctionwouldproduceresultsbetterthantheoracle.Toexaminetheiterationissue,weper-formedtheirexperimentinasinglestep,selectingatoncetheoracle-best2,000among3,000sentences2,whichproducedonlyamediocreimprovement.Wethusconcludethatthesizeoftheself-trainingsetisamajorfactorresponsibleforthedifferencebetweentheprotocols.4ExperimentalSetupWeusedareimplementationofCollins’parsingmodel2(Bikel,2004).Weperformedfourexperi-ments,II,IO,OI,andOO,twowithin-domainseed2Correspondingtoa100iterationsof30sentenceseach.619

(II,IO)andtwowithout-of-domainseed(OI,OO),examiningin-domainself-training(II,OI)andout-of-domainself-training(IO,OO).Notethatbeing‘in’or‘out’ofdomainisdeterminedbythetestdata.Eachexperimentcontained19runs.Ineachrunadifferentseedsizewasused,from100sentenceson-wards,instepsof100.Forstatisticalsigniﬁcance,werepeatedeachexperimentﬁvetimes,ineachrep-etitionrandomlysamplingdifferentmanuallyanno-tatedsentencestoformtheseeddataset3.TheseeddataweretakenfromWSJsections2-21.ForIIandIO,thetestdataisWSJsection23(2416sentences)andtheself-trainingdataareeitherWSJsections2-21(inII,excludingtheseedsen-tences)ortheBrowntrainingsection(inIO).ForOIandOO,thetestdataistheBrowntestsection(2424sentences),andtheself-trainingdataiseithertheBrowntrainingsection(inOI)orWSJsections2-21(inOO).Weremovedthemanualannotationsfromtheself-trainingsectionsbeforeusingthem.FortheBrowncorpus,webasedourdivisionon(Bacchianietal.,2006;McCloskyetal.,2006b).Thetestandtrainingsectionsconsistofsentencesfromallofthegenresthatformthecorpus.Thetrainingdivisionconsistsof90%(9ofeach10con-secutivesentences)ofthedata,andthetestsectionaretheremaining10%(Wedidnotuseanyheldoutdata).Parsingperformanceismeasuredbyf-score,f=2×P×RP+R,whereP,Rarelabeledprecisionandrecall.Tofurtherdemonstrateourresultsforparseradap-tation,wealsoperformedtheOIexperimentwhereseeddataistakenfromWSJsections2-21andbothself-trainingandtestdataaretakenfromtheSwitch-boardcorpus.Thedistancebetweenthedomainsofthesecorporaismuchgreaterthanthedistancebe-tweenthedomainsofWSJandBrown.TheBrownandSwitchboardcorporaweredividedtosectionsinthesameway.WehavealsoperformedallfourexperimentswiththeseeddatatakenfromtheBrowntrainingsection.3(Steedmanetal.,2003a)usedtheﬁrst500sentencesofWSJtrainingsectionasseeddata.Fordirectcomparison,weperformedourprotocolintheIIscenariousingtheﬁrst500or1000sentencesofWSJtrainingsectionasseeddataandgotsimilarresultstothosereportedbelowforourprotocolwithran-domselection.WealsoappliedtheprotocolofSteedmanetaltoscenarioIIwith500randomlyselectedsentences,gettingnoimprovementovertherandombaseline.Theresultswereverysimilarandwillnotbedetailedhereduetospaceconstraints.5Results5.1In-domainseeddataInthesetwoexperimentsweshowthatwhentheseedandtestdataaretakenfromthesamedomain,averysigniﬁcantenhancementofparserperformancecanbeachieved,whethertheself-trainingmaterialisin-domain(II)orout-of-domain(IO).Figure1showstheimprovementinparserf-scorewhenself-trainingdataisused,comparedtowhenitisnotused.Table1showsthereductioninmanuallyan-notatedseeddataneededtoachievecertainf-scorelevels.Theenhancementinperformanceisveryim-pressiveinthein-domainself-trainingdatascenario–areductionof50%inthenumberofmanuallyan-notatedsentencesneededforachieving75and80f-scorevalues.Asigniﬁcantimprovementisachievedintheout-of-domainself-trainingscenarioaswell.Table2comparesourresultswithself-trainingandco-trainingresultsreportedby(Steedmanetal,20003a;2003b).Asstatedearlier,theexperimentalsetupoftheseworksissimilartoours,buttheself-trainingprotocolsaredifferent.Forself-training,ourIIimprovesanabsolute3.74%overtheir74.3%result,whichconstitutesa14.5%reductioninerror(from25.7%).Thetableshowsthatforbothseedsizesourselftrainingprotocoloutperformsboththeself-trainingandco-trainingprotocolsof(Steedmanetal,20003a;2003b).Resultsarenotincludedinthetableonlyiftheyarenotreportedintherelevantpa-per.Theself-trainingprotocolof(Steedmanetal.,2003a)doesnotactuallyimproveoverthebaselineofusingonlytheseeddata.Section3discussedapossibleexplanationtothedifferenceinresults.InTable3wecompareourresultstotheresultsofthemethodstestedin(BeckerandOsborne,2005)(includingTE)4.Todothat,wecomparethereduc-tioninmanuallyannotateddataneededtoachieveanf-scorevalueof80onWSJsection23achievedbyeachmethod.Wechosethismeasuresinceitis4Themeasureisconstituentsandnotsentencesbecausethisishowresultsarereportedin(BeckerandOsborne,2005).However,thesamereductionisobtainedwhensentencesarecounted,becausethenumberofconstituentsisaveragedwhentakingmanysentences.620

f-score7580Seeddataonly600(0%)1400(0%)II300(50%)700(50%)IO500(17%)1200(14.5%)Table1:Numberofin-domainseedsentencesneededforachievingcertainf-scores.Reductionscomparedtonoself-training(line1)aregiveninparentheses.SeedsizeourIIourIOSteedmanSTSteedmanCTSteedmanCT2003a2003b500sent.78.0475.8174.376.9—-1,000sent.81.4379.49—-7981.2Table2:F-scoresofourin-domain-seedself-trainingvs.self-training(ST)andco-training(CT)of(Steedmanetal,20003a;2003b).theonlyexplicitlyreportednumberinthatwork.Asthetableshows,ourmethodissuperior:ourreduc-tionof50%constitutesanimprovementof66%overtheirbestreductionof30.6%.Whenapplyingself-trainingtoaparsertrainedwithasmalldatasetweexpectthecoverageoftheparsertoincrease,sincethecombinedtrainingsetshouldcontainitemsthattheseeddatasetdoesnot.Ontheotherhand,sincetheaccuracyofannota-tionofsuchaparserispoor(seethenoself-trainingcurveinFigure1)thecombinedtrainingsetsurelyincludesinaccuratelabelsthatmightharmparserperformance.Figure2(left)showstheincreaseincoverageachievedforin-domainandout-of-domainself-trainingdata.Theimprovementsinducedbybothmethodsaresimilar.Thisisquitesurpris-inggiventhattheBrownsectionsweusedasself-trainingdatacontainscience,ﬁction,humor,ro-mance,mysteryandadventuretextswhilethetestsectionintheseexperiments,WSJsection23,con-tainsonlynewsarticles.Figure2alsocomparesrecall(middle)andpreci-sion(right)forthedifferentmethods.ForIIthereisasigniﬁcantimprovementinbothprecisionandrecalleventhoughmanymoresentencesareparsed.ForIO,thereisalargegaininrecallandamuchsmallerlossinprecision,yieldingasubstantialim-provementinf-score(Figure1).F-scoreThiswork-IIBeckerunparsedBeckeren-tropy/unparsedHwaTE8050%29.4%30.6%-5.7%Table3:Reductionofthenumberofmanuallyanno-tatedconstituentsneededforachievingfscorevalueof80onsection23oftheWSJ.Inallcasestheseedandadditionalsentencesselectedtotraintheparseraretakenfromsections02-21ofWSJ.5.2Out-of-domainseeddataInthesetwoexperimentsweshowthatself-trainingisvaluableforadaptingparsersfromonedomaintoanother.Figure3comparesout-of-domainseeddatausedwithin-domain(OI)orout-of-domain(OO)self-trainingdataagainstthebaselineoftrainingonlywiththeout-of-domainseeddata.Theleftgraphshowsasigniﬁcantimprovementinf-score.Inthemiddleandrightgraphsweexam-inethequalityoftheparsesproducedbythemodelbyplottingrecallandprecisionvs.seedsize.Re-gardingprecision,thedifferencebetweenthethreeconditionsissmallrelativetothef-scoredifferenceshownintheleftgraph.Theimprovementintherecallmeasureismuchgreaterthantheprecisiondifferences,andthisisreﬂectedinthef-scorere-sult.Thegainincoverageachievedbybothmeth-ods,whichisnotshownintheﬁgure,issimilartothatreportedforthein-domainseedexperiments.Theleftgraphalongwiththeincreaseincoverageshowthepowerofself-traininginparseradaptationwhensmallseeddatasetsareused:notonlydoOOandOIparsemanymoresentencesthanthebaseline,buttheirf-scorevaluesareconsistentlybetter.Toseehowmuchmanuallyannotateddatacanbesavedbyusingout-of-domainseed,wetraintheparsingmodelwithmanuallyannotateddatafromtheBrowntrainingsection,asdescribedinSec-tion4.Weassumethatgivenaﬁxednumberoftrainingsentencesthebestperformanceoftheparserwithoutself-trainingwilloccurwhenthesesen-tencesareselectedfromthedomainofthetestsec-tion,theBrowncorpus.Wecomparetheamountsofmanuallyannotateddataneededtoachievecertainf-scorelevelsinthisconditionwiththecorrespondingamountsofdataneededbyOIandOO.TheresultsaresummarizedinTable4.Wecomparetotwobase-linesusingin-andout-of-domainseeddatawithout621

02004006008001000405060708090number of manually annotated sentencesf score  no self trainingwsj self−trainingbrown self−training10001200140016001800200078798081828384number of manually annotated sentencesf score  no self−trainingwsj self−trainingbrown self−trainingFigure1:Numberofseedsentencesvs.f-score,forthetwoin-domainseedexperiments:II(triangles)andIO(squares),andforthenoself-trainingbaseline.Self-trainingprovidesasubstantialimprovement.05001000150020001000150020002500number of manually annotated sentencesnumber of covered sentences  no self−trainingwsj self−trainingbrown self−training050010001500200020406080100number of manually annotated sentencesrecall  no self−trainingwsj self−trainingbrown self−training05001000150020006570758085number of manually annotated sentencesprecision  no self−trainingwsj self−trainingbrown self−trainingFigure2:Numberofseedsentencesvs.coverage(left),recall(middle)andprecision(right)forthetwoin-domainseedexperiments:II(triangles)andIO(squares),andforthenoself-trainingbaseline.anyself-training.Thesecondline(ID)servesasareferencetocomputehowmuchmanualannotationofthetestdomainwassaved,andtheﬁrstline(OD)servesasareferencetoshowbyhowmuchself-trainingimprovestheout-of-domainbaseline.Thetablestopsatanf-scoreof74becausethatisthebestthatthebaselinescando.AsigniﬁcantreductioninannotationcostovertheIDbaselineisachievedwheretheseedsizeisbe-tween100and1200.ImprovementovertheODbaselineisforthewholerangeofseedsizes.BothOOandOIachieve20-33%reductioninmanualan-notationcomparedtotheIDbaselineandenhancetheperformanceoftheparserbyasmuchas42.9%.Theonlypreviousworkthatadaptsaparsertrainedonasmalldatasetbetweendomainsisthatof(Steedmanetal.,2003a),whichusedco-training(noself-trainingresultswerereportedthereorelse-where).Inordertocomparewiththatwork,weper-formedOIwithseedtakenfromtheBrowncorpusandself-trainingandtesttakenfromWSJ,whichisthesetuptheyuse,obtainingasimilarimprove-menttothatreportedthere.However,co-trainingisamorecomplexmethodthatrequiresanadditionalparser(LTAGintheircase).Tofurthersubstantiateourresultsfortheparseradaptationscenario,weusedanadditionalcorpus,Switchboard.Figure4showstheresultsofanOIexperimentwithWSJseedandSwitchboardself-trainingandtestdata.Althoughthedomainsofthesetwocorporaareverydifferent(moresothanWSJandBrown),self-trainingprovidesasubstantialim-provement.WehavealsoperformedallfourexperimentswithBrownandWSJtradingplaces.Theresultsobtainedwereverysimilartothosereportedhere,andwillnotbedetailedduetolackofspace.6AnalysisInthissectionwetrytobetterunderstandtheben-eﬁtinusingself-trainingwithsmallseeddatasets.Weformulatethefollowingcriterion:thenumberofwordsinatestsentencethatdonotappearintheseeddata(‘unknownwords’)isastrongindicator622

0500100015002000304050607080number of manually annotated sentencesf score  no self−trainingwsj self−trainingbrown self−training050010001500200020304050607080number of manually annotated sentencesrecall  no self−trainingwsj self−trainingbrown self−training0500100015002000727476788082number of manually annotated sentencesprecision  no self−trainingwsj self−trainingbrown self−trainingFigure3:Numberofseedsentencesvs.f-score(left),recall(middle)andprecision(right),forthetwoout-of-domainseeddataexperiments:OO(triangles)andOI(squares),andforthenoself-trainingbaseline.f-sc.6668707274OD6008001,0001,400–ID6007008001,0001,200OO400500600800110033,3328.6,37.533,4020,42.98,–OI4005006008001,30033,3328.6,37.533,4020,42.9−8,–Table4:Numberofmanuallyannotatedseedsen-tencesneededforachievingcertainf-scorevalues.Theﬁrsttwolinesshowtheout-of-domainandin-domainseedbaselines.ThereductionscomparedtothebaselinesisgivenasID,OD.05001000150020001020304050number of manually annotated sentencesf score  switchboard self−trainingno self−trainingFigure4:Numberofseedsentencesvs.f-score,fortheOIexperimentusingWSJseeddataandSwitchBoardself-trainingandtestdata.Inspiteofthestrongdissimilaritybetweenthedomains,self-trainingprovidesasubstantialimprovement.towhetheritisworthwhiletousesmallseedself-training.Figure5showsthenumberofunknownwordsinasentencevs.theprobabilitythattheself-trainingmodelwillparseasentencenoworse(up-percurve)orbetter(lowercurve)thanthebaselinemodel.Theuppercurveshowsthatregardlessofthe0102030405000.20.40.60.81number of unknown wordsprobability  ST > baselineST >= baselineFigure5:Forsentenceshavingthesamenumberofunknownwords,weshowtheprobabilitythattheself-trainingmodelparsesasentencefromthesetnoworse(uppercurve)orbetter(lowercurve)thanthebaselinemodel.numberofunknownwordsinthesentence,thereismorethan50%chancethattheself-trainingmodelwillnotharmtheresult.Thisprobabilitydecreasesfromalmost1foraverysmallnumberofunknownwordstoabout0.55for50unknownwords.Thelowercurveshowsthatwhenthenumberofun-knownwordsincreases,theprobabilitythattheself-trainingmodelwilldobetterthanthebaselinemodelincreasesfromalmost0(foraverysmallnumberofunknownwords)toabout0.55.Hence,thenumberofunknownwordsisanindicationforthepotentialbeneﬁt(valueonthelowercurve)andrisk(1minusthevalueontheuppercurve)inusingtheself-trainingmodelcomparedtousingthebaselinemodel.Unknownwordswerenotidentiﬁedin(McCloskyetal.,2006a)asausefulpredictorforthebeneﬁtofself-training.623

Wealsoidentiﬁedalengtheffectsimilartothatstudiedby(McCloskyetal.,2006a)forself-training(usingarerankerandlargeseed,asdetailedinSec-tion2).Duetospacelimitationswedonotdiscussithere.7DiscussionSelf-trainingisusuallynotconsideredtobeavalu-abletechniqueinimprovingtheperformanceofgen-erativestatisticalparsers,especiallywhentheman-uallyannotatedseedsentencedatasetissmall.In-deed,intheIIscenario,(Steedmanetal.,2003a;McCloskyetal.,2006a;Charniak,1997)reportednoimprovementofthebaseparserforsmall(500sentences,intheﬁrstpaper)andlarge(40Ksen-tences,inthelasttwopapers)seeddatasetsrespec-tively.IntheII,OO,andOIscenarios,(McCloskyetal,2006a;2006b)succeededinimprovingtheparserperformanceonlywhenarerankerwasusedtore-orderthe50-bestlistofthegenerativeparser,withaseedsizeof40Ksentences.Bacchianietal(2006)improvedtheparserperformanceintheOIscenariobuttheirseedsizewaslarge(about20Ksentences).Inthispaperwehaveshownthatself-trainingcanenhancetheperformanceofgenerativeparsers,withoutareranker,infourin-andout-of-domainscenariosusingasmallseeddataset.FortheII,IOandOOscenarios,wearetheﬁrsttoshowimprove-mentbyself-trainingforgenerativeparsers.Weachieveda50%(20-33%)reductioninannotationcostforthein-domain(out-of-domain)seeddatascenarios.PreviousworkwithsmallseeddatasetsconsideredonlytheIIandOIscenarios.Ourresultsfortheformerarebetterthananypreviousmethod,andourresultsforthelatter(whicharetheﬁrstreportedself-trainingresults)aresimilartoprevi-ousresultsforco-training,amorecomplexmethod.Wedemonstratedourresultsusingthreecorporaofvaryingdegreesofdomaindifference.Adirectionforfutureresearchiscombiningself-trainingdatafromvariousdomainstoenhanceparseradaptation.Acknowledgement.WewouldliketothankDanRothforhisconstructivecommentsonthispaper.ReferencesMichielBacchiani,MichaelRiley,BrianRoark,andRichardSproat,2006.MAPadaptationofstochas-ticgrammars.ComputerSpeechandLanguage,20(1):41–68.MarkusBeckerandMilesOsborne,2005.Atwo-stagemethodforactivelearningofstatisticalgrammars.IJ-CAI’05.DanielBikel,2004.CodedevelopedatUniversityofPennsylvania.http://www.cis.upenn.edu.bikel.AvrimBlumandTomM.Mitchell,1998.Combiningla-beledandunlabeleddatawithco-training.COLT’98.EugeneCharniak,1997.Statisticalparsingwithacontext-freegrammarandwordstatistics.AAAI’97.EugeneCharniak,2000.Amaximum-entropy-inspiredparser.ANLP’00.EugeneCharniakandMarkJohnson,2005.Coarse-to-ﬁnen-bestparsingandmaxentdiscriminativererank-ing.ACL’05.StephenClark,JamesCurran,andMilesOsborne,2003.Bootstrappingpostaggersusingunlabelleddata.CoNLL’03.DavidA.Cohn,LesAtlas,andRichardE.Ladner,1994.Improvinggeneralizationwithactivelearning.Ma-chineLearning,15(2):201–221.MichaelCollins,1999.Head-drivenstatisticalmodelsfornaturallanguageparsing.Ph.D.thesis,UniversityofPennsylvania.RebeccaHwa,MilesOsborne,AnoopSarkarandMarkSteedman,2003.Correctedco-trainingforstatisticalparsers.InICML’03,WorkshopontheContinuumfromLabeledtoUnlabeledDatainMachineLearningandDataMining.RebeccaHwa,2004.Sampleselectionforstatisticalparsing.ComputationalLinguistics,30(3):253–276.TerryKooandMichaelCollins,2005.Hidden-variablemodelsfordiscriminativereranking.EMNLP’05.DavidMcClosky,EugeneCharniak,andMarkJohn-son,2006a.Effectiveself-trainingforparsing.HLT-NAACL’06.DavidMcClosky,EugeneCharniak,andMarkJohnson,2006b.Rerankingandself-trainingforparseradapta-tion.ACL-COLING’06.MarkSteedman,AnoopSarkar,MilesOsborne,RebeccaHwa,StephenClark,JuliaHockenmaier,PaulRuhlen,StevenBaker,andJeremiahCrim,2003a.Bootstrap-pingstatisticalparsersfromsmalldatasets.EACL’03.MarkSteedman,RebeccaHwa,StephenClark,MilesOsborne,AnoopSarkar,JuliaHockenmaier,PaulRuhlen,StevenBaker,andJeremiahCrim,2003b.Ex-ampleselectionforbootstrappingstatisticalparsers.NAACL’03.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 624–631,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

624

HPSGParsingwithShallowDependencyConstraintsKenjiSagae1andYusukeMiyao1andJun’ichiTsujii1,2,31DepartmentofComputerScienceUniversityofTokyoHongo7-3-1,Bunkyo-ku,Tokyo,Japan2SchoolofComputerScience,UniversityofManchester3NationalCenterforTextMining{sagae,yusuke,tsujii}@is.s.u-tokyo.ac.jpAbstractWepresentanovelframeworkthatcom-binesstrengthsfromsurfacesyntacticpars-inganddeepsyntacticparsingtoincreasedeepparsingaccuracy,speciﬁcallybycom-biningdependencyandHPSGparsing.Weshowthatbyusingsurfacedependenciestoconstraintheapplicationofwide-coverageHPSGrules,wecanbeneﬁtfromanum-berofparsingtechniquesdesignedforhigh-accuracydependencyparsing,whileactu-allyperformingdeepsyntacticanalysis.Ourframeworkresultsina1.4%absoluteim-provementoverastate-of-the-artapproachforwidecoverageHPSGparsing.1IntroductionSeveralefﬁcient,accurateandrobustapproachestodata-drivendependencyparsinghavebeenproposedrecently(NivreandScholz,2004;McDonaldetal.,2005;BuchholzandMarsi,2006)forsyntacticanal-ysisofnaturallanguageusingbilexicaldependencyrelations(Eisner,1996).Muchoftheappealoftheseapproachesistiedtotheuseofasimpleformalism,whichallowsfortheuseofefﬁcientparsingalgo-rithms,aswellasstraightforwardwaystotraindis-criminativemodelstoperformdisambiguation.Atthesametime,thereisgrowinginterestinpars-ingwithmoresophisticatedlexicalizedgrammarformalisms,suchasLexicalFunctionalGrammar(LFG)(Bresnan,1982),LexicalizedTreeAdjoin-ingGrammar(LTAG)(Schabesetal.,1988),Head-drivenPhraseStructureGrammar(HPSG)(PollardandSag,1994)andCombinatoryCategorialGram-mar(CCG)(Steedman,2000),whichrepresentdeepsyntacticstructuresthatcannotbeexpressedinashallowerformalismdesignedtorepresentonlyas-pectsofsurfacesyntax,suchasthedependencyformalismusedincurrentmainstreamdependencyparsing.Wepresentanovelframeworkthatcombinesstrengthsfromsurfacesyntacticparsinganddeepsyntacticparsing,speciﬁcallybycombiningdepen-dencyandHPSGparsing.Weshowthat,byus-ingsurfacedependenciestoconstraintheapplica-tionofwide-coverageHPSGrules,wecanbene-ﬁtfromanumberofparsingtechniquesdesignedforhigh-accuracydependencyparsing,whileactu-allyperformingdeepsyntacticanalysis.FromthepointofviewofHPSGparsing,accuracycanbeim-provedsigniﬁcantlythroughtheuseofhighlyac-curatediscriminativedependencymodels,withoutthedifﬁcultiesinvolvedinadaptingthesemodelstoamorecomplexandlinguisticallysophisticatedformalism.Inaddition,improvementsindepen-dencyparsingaccuracyareconverteddirectlyintoimprovementsinHPSGparsingaccuracy.Fromthepointofviewofdependencyparsing,theapplica-tionofHPSGrulestostructuresgeneratedbyasur-facedependencymodelprovidesaprincipledandlinguisticallymotivatedwaytoidentifydeepsyntac-ticphenomena,suchaslong-distancedependencies,raisingandcontrol.WebeginbydescribingourdependencyandHPSGparsingapproachesinsection2.Insection3,wepresentourframeworkforHPSGparsingwithshallowdependencyconstraints,andinsection4we625

Figure1:HPSGparsingevaluatethisframeworkempirically.Sections5and6discussrelatedworkandconclusions.2Fastdependencyparsingandwide-coverageHPSGparsing2.1Data-drivendependencyparsingBecauseweusedependencyparsingasastepindeepparsing,itisimportantthatwechooseapars-ingapproachthatisnotonlyaccurate,butalsoefﬁ-cient.Thedeterministicshift/reduceclassiﬁer-baseddependencyparsingapproach(NivreandScholz,2004)hasbeenshowntoofferstate-of-the-artaccu-racy(Nivreetal.,2006)withhighefﬁciencyduetoagreedysearchstrategy.OurapproachisbasedonNivreandScholz’sapproach,usingsupportvectormachinesforclassiﬁcationofshift/reduceactions.2.2Wide-coverageHPSGparsingHPSG(PollardandSag,1994)isasyntacticthe-orybasedonlexicalizedgrammarformalism.InHPSG,asmallnumberofschemasexplaingeneralconstructionrules,andalargenumberoflexicalen-triesexpressword-speciﬁcsyntactic/semanticcon-straints.Figure1showsanexampleoftheprocessofHPSGparsing.First,lexicalentriesareassignedtoeachwordinasentence.InFigure1,lexicalentriesexpresssubcategorizationframesandpred-icateargumentstructures.Parsingproceedsbyap-plyingschemastolexicalentries.Inthisexample,theHead-ComplementSchemaisappliedtothelex-icalentriesof“tried”and“running”.Wethenobtainaphrasalstructurefor“triedrunning”.Byrepeat-edlyapplyingschemastolexical/phrasalstructures,Figure2:ExtractingHPSGlexicalentriesfromthePennTreebankweﬁnallyobtainanHPSGparsetreethatcoverstheentiresentence.Inthispaper,weuseanHPSGparserdevelopedbyMiyaoandTsujii(2005).Thisparserhasawide-coverageHPSGlexiconwhichisextractedfromthePennTreebank.Figure2illustratestheirmethodforextractionofHPSGlexicalentries.First,givenaparsetreefromthePennTreebank(top),HPSG-styleconstraintsareaddedandanHPSG-styleparsetreeisobtained(middle).Lexicalentriesarethenex-tractedfromtheterminalnodesoftheHPSGparsetree(bottom).Thisway,inadditiontoawide-coveragelexicon,wealsoobtainanHPSGtreebank,whichcanbeusedastrainingdatafordisambigua-tionmodels.Thedisambiguationmodelofthisparserisbasedonamaximumentropymodel(Bergeretal.,1996).Theprobabilityp(T|W)ofanHPSGparsetreeTforthesentenceW=hw1,...,wniisgivenas:p(T|W)=p(T|L,W)p(L|W)=1Zexp Xiλifi(T)!Yjp(lj|W),whereL=hl1,...,lniarelexicalentriesand626

p(li|W)isthesupertaggingprobability,i.e.,theprobabilityofassigniningthelexicalentrylitowi(Ninomiyaetal.,2006).Theprobabilityp(T|L,W)isamaximumentropymodelonHPSGparsetrees,whereZisanormalizationfactor,andfeaturefunc-tionsfi(T)representsyntacticcharacteristics,suchasheadwords,lengthsofphrases,andappliedschemas.GiventheHPSGtreebankastrainingdata,themodelparametersλiareestimatedsoastomaxi-mizethelog-likelihoodofthetrainingdata(Malouf,2002).3HPSGparsingwithdependencyconstraintsWhileanumberoffairlystraightforwardmodelscanbeappliedsuccessfullytodependencyparsing,de-signingandtrainingHPSGparsingmodelshasbeenregardedasasigniﬁcantlymorecomplextask.Al-thoughitseemsintuitivethatamoresophisticatedlinguisticformalismshouldbemoredifﬁculttopa-rameterizeproperly,wearguethatthedifferenceincomplexitybetweenHPSGanddependencystruc-turescanbeseenasincremental,andthattheuseofaccurateandefﬁcienttechniquestodeterminethesurfacedependencystructureofasentenceprovidesvaluableinformationthataidsHPSGdisambigua-tion.ThisislargelybecauseHPSGisbasedonalex-icalizedgrammarformalism,andassuchitssyntac-ticstructureshaveanunderlyingdependencyback-bone.However,HPSGsyntacticstructuresincludeslong-distancedependencies,andtheunderlyingde-pendencystructuredescribedbyandHPSGstructureisadirectedacyclicgraph,notadependencytree(asusedbymainstreamapproachestodata-drivende-pendencyparsing).Thisdifferencemanifestsitselfinwordsthathavemultipleheads.Forexample,inthesentenceItriedtorun,thepronounIisadepen-dentoftriedandofrun.ThismakesitpossibletorepresentthatIisthesubjectofbothverbs,preciselythekindofinformationthatcannotberepresentedindependencyparsing.Ifweignorelong-distancede-pendencies,however,HPSGstructurescanbeseenaslexicalizedtreesthatcanbeeasilyconvertedintodependencytrees.GiventhatforanHPSGrepresentationofthesyn-tacticstructureofasentencewecandetermineadependencytreebyremovinglong-distancedepen-dencies,wecanusedependencyparsingtechniques(suchasthedeterministicdependencyparsingap-proachmentionedinsection2.1)todeterminetheunderlyingdependencytreesinHPSGstructures.Thisisthebasisfortheparsingframeworkpresentedhere.Inthisapproach,deepdependencyanalysisisdoneintwostages.First,adependencyparserdeterminestheshallowdependencytreeforthein-putsentence.Thisshallowdependencytreecorre-spondstotheunderlyingdependencygraphoftheHPSGstructurefortheinputsentence,withoutde-pendenciesthatroughlycorrespondtodeepsyntax.ThesecondstepistoperformHPSGparsing,asdescribedinsection2.2,butusingtheshallowde-pendencytreetoconstraintheapplicationofHPSGrules.Wenowdiscussthesetwostepsinmoredetail.3.1DeterminingshallowdependenciesinHPSGstructuresusingdependencyparsingInordertoapplyadata-drivendependencyap-proachtothetaskofidentifyingtheshallowde-pendencytreeinHPSGstructures,weﬁrstneedacorpusofsuchdependencytreestoserveastrain-ingdata.WecreatedadependencytrainingcorpusbasedonthePennTreebank(Marcusetal.,1993),ormorespeciﬁcallyontheHPSGTreebankgener-atedfromthePennTreebank(seesection2.2).ForeachHPSGstructureintheHPSGTreebank,ade-pendencytreeisextractedintwosteps.First,theHPSGtreeisconvertedintoaCFG-styletree,sim-plybyremovinglong-distancedependencylinksbe-tweennodes.AdependencytreeisthenextractedfromtheresultinglexicalizedCFG-styletree,asiscommonlydoneforconvertingconstituenttreesintodependencytreesaftertheapplicationofahead-percolationtable(Collins,1999).Onceadependencytrainingcorpusisavailable,itisusedtotrainadependencyparserasdescribedinsection2.1.Thisisdonebytrainingaclassiﬁertodetermineparseractionsbasedonlocalfeaturesthatrepresentthecurrentstateoftheparser(NivreandScholz,2004;SagaeandLavie,2005).Train-ingdatafortheclassiﬁerisobtainedbyapplyingtheparsingalgorithmoverthetrainingsentences(forwhichthecorrectdependencystructuresareknown)andrecordingtheappropriateparseractionsthatre-sultintheformationofthecorrectdependencytrees,coupledwiththefeaturesthatrepresentthestateof627

theparsermentionedinsection2.1.AnevaluationoftheresultingdependencyparseranditsefﬁcacyinaidingHPSGparsingispresentedinsection4.3.2ParsingwithdependencyconstraintsGivenasetofdependencies,thebottom-upprocessofHPSGparsingcanbeconstrainedsothatitdoesnotviolatethegivendependencies.Thiscanbeachievedbyasimpleextensionoftheparsingalgo-rithm,asfollows.Duringparsing,westorethelex-icalheadofeachpartialparsetree.Ineachschemaapplication,wecandeterminewhichchildisthehead;forexample,theleftchildistheheadwhenweapplytheHead-ComplementSchema.Giventhisinformationandlexicalheads,theparsercaniden-tifythedependencyproducedbythisschemaappli-cation,andcanthereforejudgewhethertheschemaapplicationviolatesthedependencyconstraints.ThismethodforcestheHPSGparsertoproduceparsetreesthatstrictlyconformtotheoutputofthedependencyparser.However,thismeansthattheHPSGparseroutputsnosuccessfulparseresultswhenitcannotﬁndtheparsetreethatiscompletelyconsistentwiththegivendependencies.Thissitu-ationmayoccurwhenthedependencyparserpro-ducesstructuresthatarenotcoveredintheHPSGgrammar.Thisisespeciallylikelywithafullydata-drivendependencyparserthatuseslocalclassiﬁca-tion,sinceitsoutputmaynotbegloballyconsistentgrammatically.Inaddition,theHPSGgrammarisextractedfromtheHPSGTreebankusingacorpus-basedprocedure,anditdoesnotnecessarilycoverallpossiblegrammaticalphenomenainunseentext(MiyaoandTsujii,2005).Wethereforeproposeanextensionofthisap-proachthatusespredetermineddependenciesassoftconstraints.Violationsofschemaapplicationsaredetectedinthesamewayasbefore,butinsteadofstrictlyprohibitingschemaapplications,wepenal-izethelog-likelihoodofpartialparsetreescreatedbyschemaapplicationsthatviolatethedependen-ciesconstraints.Givenanegativevalueα,weaddαtothelog-probabilityofapartialparsetreewhentheschemaapplicationviolatesthedependencycon-straints.Thatis,whenaparsetreeviolatesndepen-dencies,thelog-probabilityoftheparsetreeislow-eredbynα.Themetaparameterαisdeterminedsoastomaximizetheaccuracyonthedevelopmentset.Softdependencyconstraintscanbeimplementedasexplainedaboveasastraightforwardextensionoftheparsingalgorithm.Inaddition,itiseasilyinte-gratedwithbeamthresholdingmethodsofparsing.Becausebeamthresholdingdiscardspartialparsetreesthathavelowlog-probabilities,wecanex-pectthattheparserwoulddiscardpartialparsetreesbasedonviolationofthedependencyconstraints.4ExperimentsWeevaluatetheaccuracyofHPSGparsingwithde-pendencyconstraintsontheHPSGTreebank(Miyaoetal.,2003),whichisextractedfromtheWallStreetJournalportionofthePennTreebank(Marcusetal.,1993)1.Sections02-21wereusedfortraining(forHPSGanddependencyparsers),section22wasusedasdevelopmentdata,andﬁnaltestingwasper-formedonsection23.Followingpreviousworkonwide-coverageparsingwithlexicalizedgrammarsusingthePennTreebank,weevaluatetheparserbymeasuringtheaccuracyofpredicate-argumentrela-tionsintheparser’soutput.Apredicate-argumentrelationisdeﬁnedasatuplehσ,wh,a,wai,whereσisthepredicatetype(e.g.adjective,intransitiveverb),whistheheadwordofthepredicate,aistheargumentlabel(MODARG,ARG1,...,ARG4),andwaistheheadwordoftheargument.Labeledpre-cision(LP)/labeledrecall(LR)istheratiooftuplescorrectlyidentiﬁedbytheparser.Thesepredicate-argumentrelationscoverthefullrangeofsyntacticdependenciesproducedbytheHPSGparser(includ-ing,long-distancedependencies,raisingandcontrol,inadditiontosurfacedependencies).Intheexperimentspresentedinthissection,in-putsentenceswereautomaticallytaggedwithparts-of-speechwithabout97%accuracy,usingamax-imumentropyPOStagger.WealsoreportresultsonparsingtextwithgoldstandardPOStags,whereexplicitlynoted.Thisprovidesanupper-boundonwhatcanbeexpectedifamoresophisticatedmulti-taggingscheme(JamesR.CurranandVadas,2006)isused,insteadofhardassignmentofsingletagsinapreprocessingstepasdonehere.1Theextractionsoftwarecanbeobtainedfromhttp://www-tsujii.is.s.u-tokyo.ac.jp/enju.628

4.1BaselineHPSGparsingresultsusingthesameHPSGgram-marandtreebankhaverecentlybeenreportedbyMiyaoandTsujii(2005)andNinomiaetal.(2006).ByrunningtheHPSGparserdescribedinsection2.2onthedevelopmentdatawithoutdependencycon-straints,weobtainsimilarvaluesofLP(86.8%)andLR(85.6%)asthosereportedbyMiyaoandTsu-jii(MiyaoandTsujii,2005).Usingtheextremelylexicalizedframeworkof(Ninomiyaetal.,2006)byperformingsupertaggingbeforeparsing,weobtainsimilaraccuracyasNinomiyaetal.(87.1%LPand85.9%LR).4.2DependencyconstraintsandthepenaltyparameterParsingthedevelopmentdatawithharddependencyconstraintsconﬁrmedtheintuitionthatthesecon-straintsoftendescribedependencystructuresthatdonotconformtoHPSGschemausedinparsing,re-sultinginparsefailures.Todeterminetheupper-boundonHPSGparsingwithharddependencycon-straints,wesettheHPSGparsertodisallowtheap-plicationofanyrulesthatresultinthecreationofdependenciesthatviolategoldstandarddependen-cies.Thisresultsinhighprecision(96.7%),butre-callislow(82.3%)duetoparsefailurescausedbylackofgrammaticalcoverage2.Usingdependen-ciesproducedbytheshift-reduceSVMparser,weobtain91.5%LPand65.7%LR.Thisrepresentsalargegaininprecisionoverthebaseline,butanevengreaterlossinrecall,whichlimitstheusefulnessoftheparser,andseverelyhurtstheappealofhardcon-straints.Wefocustherestofourexperimentsonparsingwithsoftdependencyconstraints.Asexplainedinsection3,thisinvolvessettingthepenaltyparame-terα.Duringparsing,wesubtractαfromthelog-probabilityofapplyinganyschemathatviolatesthedependencyconstraintsgiventotheHPSGparser.Figure3illustratestheeffectofαwhengoldstan-darddependencies(andgoldstandardPOStags)areused.Wenotethatsettingα=0causestheparser2AlthoughtheHPSGgrammardoesnothaveperfectcov-erageofunseentext,itsupportscompleteandmostlycorrectanalysesforallsentencesinthedevelopmentset.However,whenwerequirecompletelycorrectanalysesbyusinghardcon-straints,lackofcoveragemaycauseparsefailures.899091929394959605101520253035PenaltyAccuracyPrecisionRecallF-scoreFigure3:TheeffectofαonHPSGparsingcon-strainedbygoldstandarddependencies.toignoredependencyconstraints,providingbase-lineperformance.Conversely,settingahighenoughvalue(α=30issufﬁcient,inpractice)causesanysubstructuresthatviolatethedependencyconstraintstobeusedonlywhentheyareabsolutelyneces-sarytoproduceavalidparsefortheinputsentence.Inﬁgure3,thiscorrespondstoanupper-boundontheaccuracyofparsingwithsoftdependencycon-straints(94.7%f-score),sincegoldstandarddepen-denciesareused.Wesetαempiricallywithsimplehillclimbingonthedevelopmentset.Becauseitisexpectedthattheoptimalvalueofαdependsontheaccuracyofthesurfacedependencyparser,wesetseparatevaluesforparsingwithaPOStaggerorwithgoldstandardPOStags.Figure4showstheaccuracyofHPSGpredicate-argumentrelationsobtainedwithdepen-dencyconstraintsdeterminedbydependencypars-ingwithgoldstandardPOStags.Withbothau-tomaticallyassignedandgoldstandardPOStags,weobserveanimprovementofabout0.6%inpre-cision,recallandf-score,whentheoptimalαvalueisusedineachcase.Whilethiscorrespondstoarel-ativeerrorreductionofover6%(or12%,ifwecon-sidertheupper-bounddictatedbyimperfectgram-maticalcoverage),amoreinterestingaspectofthisframeworkisthatitallowstechniquesdesignedforimprovingdependencyaccuracytoimproveHPSGparsingaccuracydirectly,asweillustratenext.629

89.489.689.89090.290.490.690.89100.511.522.533.5PenaltyAccuracyPrecisionRecallF-scoreFigure4:TheeffectofαonHPSGparsingcon-strainedbytheoutputofadependencyparserusinggoldstandardPOStags.4.3DeterminingconstraintswithdependencyparsercombinationParsercombinationhasbeenshowntobeapower-fulwaytoobtainveryhighaccuracyindependencyparsing(SagaeandLavie,2006).UsingdependencyconstraintsallowsustoimproveHPSGparsingac-curacysimplybyusinganexistingparsercombina-tionapproach.Asaﬁrststep,wetraintwoaddi-tionalparserswiththedependenciesextractedfromtheHPSGTreebank.Theﬁrstusesthesameshift-reduceframeworkdescribedinsection2.1,butitprocesstheinputfromrighttoleft(RL).Thishasbeenfoundtoworkwellinpreviousworkondepen-dencyparsercombination(ZemanandˇZabokrtsk´y,2005;SagaeandLavie,2006).ThesecondparserisMSTParser,thelarge-marginmaximumspanningtreeparserdescribedin(McDonaldetal.,2005)3.Weexaminetheuseoftwocombinationschemes:oneusingtwoparsers,andoneusingthreeparsers.Theﬁrstcombinationapproachistokeeponlyde-pendenciesforwhichthereisagreementbetweenthetwoparsers.Inotherwords,dependenciesthatareproposedbyoneparserbutnottheotheraresimplydiscarded.Usingtheleft-to-rightshift-reduceparserandMSTParser,weﬁndthatthisresultsinveryhighprecisionofsurfacedependenciesonthedevelop-mentdata.Inthesecondapproach,combinationof3Downloadedfromhttp://sourceforge.net/projects/mstparserthethreedependencyparsersisdoneaccordingtothemaximumspanningtreecombinationschemeofSagaeandLavie(2006),whichresultsinhighaccu-racyofsurfacedependencies.Foreachofthecom-binationapproaches,weusetheresultingdependen-ciesasconstraintsforHPSGparsing,determiningtheoptimalvalueofαonthedevelopmentsetinthesamewayasdoneforasingleparser.Table1summarizesourexperimentsondevelopmentdatausingparsercombinationstoproducedependencyconstraints4.ThetwocombinationapproachesaredenotedasC1andC2.ParserDepαHPSGDiffnone(baseline)––86.5–LRshift-reduce91.21.587.10.6RLshift-reduce90.1––MSTParser91.0––C1(agreement)96.8*2.587.40.9C2(MST)92.42.587.40.9Table1:Summaryofresultsondevelopmentdata.*TheshallowaccuracyofcombinationC1corre-spondstothedependencyprecision(nodependen-cieswerereportedfor8%ofallwordsinthedevel-opmentset).4.4ResultsHavingdeterminedαvaluesondevelopmentdatafortheshift-reducedependencyparser,thetwo-parseragreementcombination,andthethree-parsermaximumspanningtreecombination,weparsethetestdata(section23)usingthesethreedifferentsourcesofdependencyconstraintsforHPSGpars-ing.Ourﬁnalresultsareshownintable2,wherewealsoincludetheresultspublishedin(Ninomiyaetal.,2006)forcomparisonpurposes,andtheresultofusingdependencyconstraintsobtainedwithgoldstandardPOStags.Byusingtwounlabeleddependencyparserstoprovidesoftdependencyconstraints,weobtaina1%absoluteimprovementinprecisionandrecallofpredicate-argumentidentiﬁcationinHPSGparsingoverastrongbaseline.Ourbaselineapproachout-performedpreviouslypublishedresultsonthistest4Theaccuracyﬁguresforthedependencyparsersisex-pressedasunlabeledaccuracyofthesurfacedependenciesonly,andarenotcomparabletotheHPSGparsingaccuracyﬁgures630

ParserLPLRF-scoreHPSGBaseline87.487.087.2Shift-Reduce+HPSG88.287.787.9C1+HPSG88.588.088.2C2+HPSG88.487.988.1Baseline(gold)89.889.489.6Shift-Reduce(gold)90.6290.2390.42C1+HPSG(gold)90.990.490.6C2+HPSG(gold)90.890.490.6MiyaoandTsujii,200585.084.384.6Ninomiyaetal.,200687.486.386.8Table2:Finalresultsontestset.TheﬁrstsetofresultsshowourHPSGbaselineandHPSGwithsoftdependencyconstraintsusingthreedifferentsourcesofdependencyconstraints.Thesecondsetofresultsshowtheaccuracyofthesameparserswhengoldpart-of-speechtagsareused.Thethirdsetofresultsisfromexistingpublishedmodelsonthesamedata.set,andourbestperformingcombinationschemeobtainsanabsoluteimprovementof1.4%overthebestpreviouslypublishedresultsusingtheHPSGTreebank.Itisinterestingtonotethattheresultsob-tainedwithdependencyparsercombinationsC1andC2wereverysimilar,eventhoughinC1onlytwoparserswereused,andconstraintswereprovidedforabout92%ofshallowdependencies(withaccuracyhigherthan96%).Clearly,precisioniscrucialinde-pendencyconstraints.Finally,althoughitisnecessarytoperformde-pendencyparsingtopre-computedependencycon-straints,thetotaltimerequiredtoperformtheen-tireprocessofHPSGparsingwithdependencycon-straintsisclosetothatofthebaselineHPSGap-proach.Thisisduetotworeasons:(1)thede-pendencyparsingapproachesusedtopre-computeconstraintsareseveraltimesfasterthanthebaselineHPSGapproach,and(2)theHPSGportionoftheprocessissigniﬁcantlyfasterwhendependencycon-straintsareused,sincetheconstraintshelpsharpenthesearchspace,makingsearchmoreefﬁcient.Us-ingthebaselineHPSGapproach,ittakesapprox-imately25minutestoparsethetestset.Theto-taltimerequiredtoparsethetestsetusingHPSGwithdependencyconstraintsgeneratedbytheshift-reduceparseris27minutes.WithcombinationC1,parsingtimeincreasesto30minutes,sincetwode-pendencyparsersareusedsequentially.5RelatedworkThereareotherapproachesthatcombineshallowprocessingwithdeepparsing(Crysmannetal.,2002;Franketal.,2003;Daumetal.,2003)toim-proveparsingefﬁciency.Typically,shallowparsingisusedtocreaterobustminimalrecursionseman-tics,whichareusedasconstraintstolimitambigu-ityduringparsing.Ourapproach,incontrast,usessyntacticdependenciestoachieveasigniﬁcantim-provementintheaccuracyofwide-coverageHPSGparsing.Additionally,ourapproachisinmanywayssimilartosupertagging(BangaloreandJoshi,1999),whichusessequencelabelingtechniquesasanefﬁcientwaytopre-computeparsingconstraints(speciﬁcally,theassignmentoflexicalentriestoin-putwords).6ConclusionWehavepresentedanovelframeworkfortakingad-vantageofthestrengthsofashallowparsingap-proachandadeepparsingapproach.WehaveshownthatbyconstrainingtheapplicationofrulesinHPSGparsingaccordingtoresultsfromadepen-dencyparser,wecansigniﬁcantlyimprovetheac-curacyofdeepparsingbyusingshallowsyntacticanalyses.Toillustratehowthisframeworkallowsforim-provementsintheaccuracyofdependencyparsingtobeuseddirectlytoimprovetheaccuracyofHPSGparsing,weshowedthatbycombiningtheresultsofdifferentdependencyparsersusingthesearch-basedparsingensembleapproachof(SagaeandLavie,2006),weobtainimprovedHPSGparsingaccuracyasaresultoftheimproveddependencyaccuracy.AlthoughwehavefocusedontheuseofHPSGanddependencyparsing,thegeneralframeworkpre-sentedherecanbeappliedtootherlexicalizedgram-marformalisms,suchasLTAG,CCGandLFG.AcknowledgementsThisresearchwaspartiallysupportedbyGrant-in-AidforSpeciallyPromotedResearch18002007.631

ReferencesSrinivasBangaloreandAravindK.Joshi.1999.Su-pertagging:anapproachtoalmostparsing.Compu-tationalLinguistics,25(2):237–265.A.Berger,S.A.DellaPietra,andV.J.DellaPietra.1996.Amaximumentropyapproachtonaturallanguagepro-cessing.ComputationalLinguistics,22(1):39–71.JoanBresnan.1982.Thementalrepresentationofgram-maticalrelations.MITPress.SabineBuchholzandErwinMarsi.2006.Conll-xsharedtaskonmultilingualdependencyparsing.InProceed-ingsoftheTenthConferenceonNaturalLanguageLearning.NewYork,NY.M.Collins.1999.Head-DrivenModelsforNaturalLan-guageParsing.Phdthesis,UniversityofPennsylva-nia.BertholdCrysmann,AnetteFrank,BerndKiefer,StefanMueller,GuenterNeumann,JakubPiskorski,UlrichSchaefer,MelanieSiegel,HansUszkoreit,FeiyuXu,MarkusBecker,andHans-UlrichKrieger.2002.Anintegratedarchitectureforshallowanddeepprocess-ing.InProceedingsofthe40thAnnualMeetingoftheAssociationforComputationalLinguistics(ACL2002).MichaelDaum,KilianA.Foth,andWolfgangMenzel.2003.Constraint-basedintegrationofdeepandshal-lowparsingtechniques.InProceedingsofthe10thConferenceoftheEuropeanChapteroftheAssocia-tionforComputationalLinguistics(EACL2003).JasonEisner.1996.Threenewprobabilisticmodelsfordependencyparsing:Anexploration.InProceedingsoftheInternationalConferenceonComputationalLin-guistics(COLING’96).Copenhagen,Denmark.AnetteFrank,MarkusBecker,BertholdCrysmann,BerndKiefer,andUlrichSchaefer.2003.Integratedshallowanddeepparsing:TopPmeetsHPSG.InPro-ceedingsofthe41stAnnualMeetingoftheAssocia-tionforComputationalLinguistics(ACL2003),pages104–111.StephenClarkJamesR.CurranandDavidVadas.2006.Multi-taggingforlexicalized-grammarparsing.InProceedingsofCOLING/ACL2006.Sydney,Aus-tralia.RobertMalouf.2002.Acomparisonofalgorithmsformaximumentropyparameterestimation.InProceed-ingsofthe2002ConferenceonNaturalLanguageLearning.M.P.Marcus,B.Santorini,andM.A.Marcinkiewics.1993.Buildingalargeannotatedcorpusofenglish:Thepenntreebank.ComputationalLinguistics,19.RyanMcDonald,FernandoPereira,K.Ribarov,andJ.Hajic.2005.Non-projectivedependencypars-ingusingspanningtreealgorithms.InProceedingsoftheConferenceonHumanLanguageTechnolo-gies/EmpiricalMethodsinNaturalLanguageProcess-ing(HLT-EMNLP).Vancouver,Canada.YusukeMiyaoandJun’ichiTsujii.2005.Probabilisticdisambiguationmodelsforwide-coveragehpsgpars-ing.InProceedingsofthe42ndMeetingoftheAssoci-ationforComputationalLinguistics.AnnArbor,MI.YusukeMiyao,TakashiNinomiya,andJun’ichiTsu-jii.2003.Corpusorientedgrammardevelopmentforaquiringahead-drivenphrasestructuregrammarfromthepenntreebank.InProceedingsoftheTenthCon-ferenceonNaturalLanguageLearning.T.Ninomiya,T.Matsuzaki,Y.Tsuruoka,Y.Miyao,andJ.Tsujii.2006.Extremelylexicalizedmodelsforac-curateandfasthpsgparsing.InProceedingsofthe2006ConferenceonEmpiricalMethodsforNaturalLanguageProcessing(EMNLP2006).JoakimNivreandMarioScholz.2004.Deterministicdependencyparsingofenglishtext.InProceedingsofthe20thInternationalConferenceonComputationalLinguistics,pages64–70.Geneva,Switzerland.J.Nivre,J.Hall,J.Nilsson,G.Eryigit,andS.Marinov.2006.Labeledpseudo-projectivedependencypars-ingwithsupportvectormachines.InProceedingsoftheTenthConferenceonNaturalLanguageLearning.NewYork,NY.C.PollardandI.A.Sag.1994.Head-DrivenPhraseStructureGrammar.UniversityofChicagoPress.KenjiSagaeandAlonLavie.2005.Aclassiﬁer-basedparserwithlinearrun-timecomplexity.InProceed-ingsoftheNinthInternationalWorkshoponParsingTechnologies.Vancouver,BC.KenjiSagaeandAlonLavie.2006.Parsercombinationbyreparsing.InProceedingsofthe2006MeetingoftheNorthAmericanACL.NewYork,NY.YvesSchabes,AnneAbeille,andAravindJoshi.1988.Parsingstrategieswithlexicalizedgrammars:Appli-cationtotreeadjoininggrammars.InProceedingsof12thCOLING.MarkSteedman.2000.TheSyntacticProcess.MITPress.DanielZemanandZdenekˇZabokrtsk´y.2005.Improvingparsingaccuracybycombiningdiversedependencyparsers.InProceedingsoftheInternationalWorkshoponParsingTechnologies.Vancouver,Canada.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 632–639,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

632

ConstituentParsingwithIncrementalSigmoidBeliefNetworksIvanTitovDepartmentofComputerScienceUniversityofGeneva24,rueG´en´eralDufourCH-1211Gen`eve4,Switzerlandivan.titov@cui.unige.chJamesHendersonSchoolofInformaticsUniversityofEdinburgh2BuccleuchPlaceEdinburghEH89LW,UnitedKingdomjames.henderson@ed.ac.ukAbstractWeintroduceaframeworkforsyntacticparsingwithlatentvariablesbasedonaformofdynamicSigmoidBeliefNetworkscalledIncrementalSigmoidBeliefNetworks.Wedemonstratethatapreviousfeed-forwardneuralnetworkparsingmodelcanbeviewedasacoarseapproximationtoinferencewiththisclassofgraphicalmodel.Byconstruct-ingamoreaccuratebutstilltractableap-proximation,wesigniﬁcantlyimprovepars-ingaccuracy,suggestingthatISBNsprovideagoodidealizationforparsing.Thisgener-ativemodelofparsingachievesstate-of-the-artresultsonWSJtextand8%errorreduc-tionoverthebaselineneuralnetworkparser.1IntroductionLatentvariablemodelshaverecentlybeenofin-creasinginterestinNaturalLanguageProcessing,andinparsinginparticular(e.g.(KooandCollins,2005;Matsuzakietal.,2005;Riezleretal.,2002)).Latentvariablesprovideaprincipledwaytoin-cludefeaturesinaprobabilitymodelwithoutneed-ingtohavedatalabeledwiththosefeaturesinad-vance.Instead,alabelingwiththesefeaturescanbeinducedaspartofthetrainingprocess.Thedifﬁcultywithlatentvariablemodelsisthatevensmallnumbersoflatentvariablescanleadtocom-putationallyintractableinference(a.k.a.decoding,parsing).InthispaperweproposeasolutiontothisproblembasedondynamicSigmoidBeliefNet-works(SBNs)(Neal,1992).ThedynamicSBNswhichwepeopose,calledIncrementalSigmoidBe-liefNetworks(ISBNs)havelargenumbersoflatentvariables,whichmakesexactinferenceintractable.However,theycanbeapproximatedsufﬁcientlywelltobuildfastandaccuratestatisticalparserswhichin-ducefeaturesduringtraining.WeuseSBNsinagenerativehistory-basedmodelofconstituentstructureparsing.Theprobabilityofanunboundedstructureisdecomposedintoase-quenceofprobabilitiesforindividualderivationde-cisions,eachdecisionconditionedontheunboundedhistoryofpreviousdecisions.Themostcommonap-proachtohandlingtheunboundednatureofthehis-toriesistochooseapre-deﬁnedsetoffeatureswhichcanbeunambiguouslyderivedfromthehistory(e.g.(Charniak,2000;Collins,1999)).Decisionprob-abilitiesarethenassumedtobeindependentofallinformationnotrepresentedbythisﬁnitesetoffea-tures.Anotherpreviousapproachistouseneuralnetworkstocomputeacompressedrepresentationofthehistoryandconditiondecisionsonthisrepresen-tation(Henderson,2003;Henderson,2004).Itispossiblethatanunboundedamountofinformationisencodedinthecompressedrepresentationviaitscontinuousvalues,butitisnotclearwhetherthisisactuallyhappeningduetothelackofanyprincipledinterpretationforthesecontinuousvalues.Liketheformerapproach,weassumethatthereareaﬁnitesetoffeatureswhichencodetherelevantinformationabouttheparsehistory.Butunlikethatapproach,weallowfeaturevaluestobeambiguous,andrepresenteachfeatureasadistributionover(bi-nary)values.Inotherwords,thesehistoryfeaturesaretreatedaslatentvariables.Unfortunately,inter-633

pretingthehistoryrepresentationsasdistributionsoverdiscretevaluesoflatentvariablesmakestheex-actcomputationofdecisionprobabilitiesintractable.Exactcomputationrequiresmarginalizingoutthela-tentvariables,whichinvolvessummingoverallpos-siblevectorsofdiscretevalues,whichisexponentialinthelengthofthevector.Weproposetwoformsofapproximationfordy-namicSBNs,aneuralnetworkapproximationandaformofmeanﬁeldapproximation(SaulandJor-dan,1999).Weﬁrstshowthatthepreviousneuralnetworkmodelof(Henderson,2003)canbeviewedasacoarseapproximationtoinferencewithISBNs.Wethenproposeanincrementalmeanﬁeldmethod,whichresultsinanimprovedapproximationovertheneuralnetworkbutremainstractable.There-sultingparserachievessigniﬁcantlyhigheraccuracythantheneuralnetworkparser(90.0%F-measurevs89.1%).Wearguethatthiscorrelationbetweenbet-terapproximationandbetteraccuracysuggeststhatdynamicSBNsareagoodabstractmodelfornaturallanguageparsing.2SigmoidBeliefNetworksAbeliefnetwork,oraBayesiannetwork,isadi-rectedacyclicgraphwhichencodesstatisticalde-pendenciesbetweenvariables.EachvariableSiinthegraphhasanassociatedconditionalprobabilitydistributionsP(Si|Par(Si))overitsvaluesgiventhevaluesofitsparentsPar(Si)inthegraph.ASigmoidBeliefNetwork(Neal,1992)isaparticu-lartypeofbeliefnetworkswithbinaryvariablesandconditionalprobabilitydistributionsintheformofthelogisticsigmoidfunction:P(Si=1|Par(Si))=11+exp(−PSj∈Par(Si)JijSj),whereJijistheweightfortheedgefromvariableSjtovariableSi.Inthispaperweconsideragen-eralizedversionofSBNswhereweallowvariableswithanyrangeofdiscretevalues.Wethusgeneral-izethelogisticsigmoidfunctiontothenormalizedexponential(a.k.a.softmax)functiontodeﬁnetheconditionalprobabilitiesfornon-binaryvariables.ExactinferencewithallbutverysmallSBNsisnottractable.Initiallysamplingmethodswereused(Neal,1992),butthisisalsonotfeasibleforlargenetworks,especiallyforthedynamicmodelsofthetypedescribedinsection2.2.Variationalmethodshavealsobeenproposedforapproximat-ingSBNs(SaulandJordan,1999).Themainideaofvariationalmethods(Jordanetal.,1999)is,roughly,toconstructatractableapproximatemodelwithanumberoffreeparameters.Thefreeparametersaresetsothattheresultingapproximatemodelisascloseaspossibletotheoriginalgraphicalmodelforagiveninferenceproblem.2.1MeanFieldApproximationMethodsThesimplestexampleofavariationmethodisthemeanﬁeldmethod,originallyintroducedinstatis-ticalmechanicsandlaterappliedtounsupervisedneuralnetworksin(Hintonetal.,1995).Letusde-notethesetofvisiblevariablesinthemodel(i.e.theinputsandoutputs)byVandhiddenvariablesbyH=h1,...,hl.ThemeanﬁeldmethodusesafullyfactorizeddistributionQastheapproximatemodel:Q(H|V)=YiQi(hi|V).whereeachQiisthedistributionofanindividuallatentvariable.Theindependencebetweenthevari-ableshiinthisapproximatedistributionQdoesnotimplyindependenceofthefreeparameterswhichdeﬁnetheQi.Theseparametersaresettomin-imizetheKullback-Leiblerdivergence(CoverandThomas,1991)betweentheapproximatedistribu-tionQ(H|V)andthetruedistributionP(H|V):KL(QkP)=XHQ(H|V)lnQ(H|V)P(H|V),(1)or,equivalently,tomaximizetheexpression:LV=XHQ(H|V)lnP(H,V)Q(H|V).(2)TheexpressionLVisalowerboundonthelog-likelihoodlnP(V).Itisusedinthemeanﬁeldtheory(SaulandJordan,1999)toapproximatethelikelihood.However,inourcaseofdynamicgraph-icalmodels,wehavetouseadifferentapproachwhichallowsustoconstructanincrementalparsingmethodwithoutneedingtointroducetheadditionalparametersproposedin(SaulandJordan,1999).Wewilldescribeourmodiﬁcationofthemeanﬁeldmethodinsection3.3.634

2.2DynamicsDynamicBayesiannetworksareBayesiannetworksappliedtoarbitrarilylongsequences.Anewsetofvariablesisinstantiatedforeachpositioninthese-quence,buttheedgesandweightsforthesevariablesarethesameasinotherpositions.Theedgeswhichconnectvariablesinstantiatedfordifferentpositionsmustbedirectedforwardinthesequence,therebyallowingatemporalinterpretationofthesequence.TypicallyadynamicBayesianNetworkwillonlyin-volveedgesbetweenadjacentpositionsinthese-quence(i.e.theyareMarkovian),butinourparsingmodelsthepatternofinterconnectionisdeterminedbystructurallocality,ratherthansequencelocality,asintheneuralnetworksof(Henderson,2003).UsingstructurallocalitytodeﬁnethegraphinadynamicSBNmeansthatthesubgraphofedgeswithdestinationsatagivenpositioncannotbedetermineduntilalltheparserdecisionsforpreviouspositionshavebeenchosen.WethereforecallthesemodelsIncrementalSBNs,because,atanygivenpositionintheparse,weonlyknowthegraphofedgesforthatpositionandpreviouspositionsintheparse.Forexampleinﬁgure1,discussedbelow,itwouldnotbepossibletodrawtheportionofthegraphaftert,becausewedonotyetknowthedecisiondtk.Theincrementalspeciﬁcationofmodelstructuremeansthatwecannotuseanundirectedgraphicalmodel,suchasConditionalRandomFields.Withadirecteddynamicmodel,alledgesconnectingtheknownportionofthegraphtotheunknownportionofthegrapharedirectedtowardtheunknownpor-tion.Alsotherearenovariablesintheunknownportionofthegraphwhosevaluesareknown(i.e.novisiblevariables),becauseateachstepinahistory-basedmodelthedecisionprobabilityisconditionedonlyontheparsinghistory.Onlyvisiblevariablescanresultininformationbeingreﬂectedbackwardthroughadirectededge,soitisimpossibleforany-thingintheunknownportionofthegraphtoaffecttheprobabilitiesintheknownportionofthegraph.Thereforeinferencecanbeperformedbysimplyig-noringtheunknownportionofthegraph,andthereisnoneedtosumoverallpossiblestructuresfortheunknownportionofthegraph,aswouldbeneces-saryforanundirectedgraphicalmodel.Figure1:IllustrationofanISBN.3TheProbabilisticModelofParsingInthissectionwepresentourframeworkforsyn-tacticparsingwithdynamicSigmoidBeliefNet-works.WeﬁrstspecifytheformofSBNwepropose,namelyISBNs,andthentwomethodsforapprox-imatingtheinferenceproblemsrequiredforpars-ing.Weonlyconsidergenerativemodelsofpars-ing,sincegenerativeprobabilitymodelsaresimplerandwearefocusedonprobabilityestimation,notdecisionmaking.Althoughthemostaccuratepars-ingmodels(CharniakandJohnson,2005;Hender-son,2004;Collins,2000)arediscriminative,allthemostaccuratediscriminativemodelsmakeuseofagenerativemodel.Moreaccurategenerativemodelsshouldmakethediscriminativemodelswhichusethemmoreaccurateaswell.Also,therearesomeapplications,suchaslanguagemodeling,whichre-quiregenerativemodels.3.1TheGraphicalModelInISBNs,weuseahistory-basedmodel,whichde-composestheprobabilityoftheparseas:P(T)=P(D1,...,Dm)=YtP(Dt|D1,...,Dt−1),whereTistheparsetreeandD1,...,Dmisitsequivalentsequenceofparserdecisions.InsteadoftreatingeachDtasatomicdecisions,itisconvenienttofurthersplitthemintoasequenceofelementarydecisionsDt=dt1,...,dtn:P(Dt|D1,...,Dt−1)=YkP(dtk|h(t,k)),whereh(t,k)denotestheparsinghistoryD1,...,Dt−1,dt1,...,dtk−1.Forexample,a635

decisiontocreateanewconstituentcanbedividedintwoelementarydecisions:decidingtocreateaconstituentanddecidingwhichlabeltoassigntoit.Weuseagraphicalmodeltodeﬁneourproposedclassofprobabilitymodels.AnexamplegraphicalmodelforthecomputationofP(dtk|h(t,k))isillustratedinﬁgure1.Thegraphicalmodelisorganizedintovectorsofvariables:latentstatevariablevectorsSt0=st01,...,st0n,representinganintermediatestateoftheparseratderivationstept0,anddecisionvariablevectorsDt0=dt01,...,dt0l,representingaparserde-cisionatderivationstept0,wheret0≤t.Variableswhosevaluearegivenatthecurrentdecision(t,k)areshadedinﬁgure1,latentandoutputvariablesareleftunshaded.Asillustratedbythearrowsinﬁgure1,theprob-abilityofeachstatevariablest0idependsonallthevariablesinaﬁnitesetofrelevantpreviousstateanddecisionvectors,buttherearenodirectdependen-ciesbetweenthedifferentvariablesinasinglestatevector.Whichpreviousstateanddecisionvectorsareconnectedtothecurrentstatevectorisdeter-minedbyasetofstructuralrelationsspeciﬁedbytheparserdesigner.Forexample,wecouldselectthemostrecentstatewherethesameconstituentwasonthetopofthestack,andadecisionvariablerep-resentingtheconstituent’slabel.Eachsuchselectedrelationhasitsowndistinctweightmatrixfortheresultingedgesinthegraph,butthesameweightmatrixisusedateachderivationpositionwheretherelationisrelevant.Asindicatedinﬁgure1,theprobabilityofeachelementarydecisiondt0kdependsbothonthecurrentstatevectorSt0andonthepreviouslychosenele-mentaryactiondt0k−1fromDt0.Thisprobabilitydis-tributionhastheformofanormalizedexponential:P(dt0k=d|St0,dt0k−1)=Φh(t0,k)(d)ePjWdjst0jPd0Φh(t0,k)(d0)ePjWd0jst0j,(3)whereΦh(t0,k)istheindicatorfunctionofasetofelementarydecisionsthatmaypossiblyfollowtheparsinghistoryh(t0,k),andtheWdjaretheweights.Forourexperiments,wereplicatedthesamepat-ternofinterconnectionbetweenstatevariablesasdescribedin(Henderson,2003).1Wealsousedthe1Intheneuralnetworkof(Henderson,2003),ourvariablessameleft-cornerparsingstrategy,andthesamesetofdecisions,features,andstates.Wereferthereaderto(Henderson,2003)fordetails.Exactcomputationwiththismodelisnottractable.Samplingofparsetreesfromthemodelisnotfeasible,becauseagenerativemodeldeﬁnesajointmodelofbothasentenceandatree,therebyre-quiringsamplingoverthespaceofsentences.Gibbssampling(GemanandGeman,1984)isalsoimpos-sible,becauseofthehugespaceofvariablesandneedtoresampleaftermakingeachnewdecisioninthesequence.Thus,weknowofnoreasonablealter-nativestotheuseofvariationalmethods.3.2AFeed-ForwardApproximationTheﬁrstmodelweconsiderisastrictlyincrementalcomputationofavariationalapproximation,whichwewillcallthefeed-forwardapproximation.Itcanbeviewedasthesimplestformofmeanﬁeldapprox-imation.Asinanymeanﬁeldapproximation,eachofthelatentvariablesisindependentlydistributed.Butunlikethegeneralcaseofmeanﬁeldapproxi-mation,inthefeed-forwardapproximationweonlyallowtheparametersofthedistributionsQitode-pendonthedistributionsoftheirparents.Thisaddi-tionalconstraintincreasesthepotentialforalargeKullback-Leiblerdivergencewiththetruemodel,deﬁnedinexpression(1),butitsigniﬁcantlysimpli-ﬁesthecomputations.ThesetofhiddenvariablesHinourgraphicalmodelconsistsofallthestatevectorsSt0,t0≤t,andthelastdecisiondtk.Allthepreviouslyobserveddecisionsh(t,k)comprisethesetofvisiblevari-ablesV.Theapproximatefullyfactorisabledistri-butionQ(H|V)canbewrittenas:Q(H|V)=qtk(dtk)Yt0,i(cid:16)µt0i(cid:17)st0i(cid:16)1−µt0i(cid:17)1−st0i.whereµt0iisthefreeparameterwhichdeterminesthedistributionofstatevariableiatpositiont0,namelyitsmean,andqtk(dtk)isthefreeparameterwhichde-terminesthedistributionoverdecisionsdtk.Becauseweareonlyallowedtouseinformationaboutthedistributionsoftheparentvariablestomaptotheir“units”,andourdependencies/edgesmaptotheir“links”.636

computethefreeparametersµt0i,theoptimalassign-mentofvaluestotheµt0iis:µt0i=σ(cid:16)ηt0i(cid:17),whereσdenotesthelogisticsigmoidfunctionandηt0iisaweightedsumoftheparentvariables’means:ηt0i=Xt00∈RS(t0)XjJτ(t0,t00)ijµt00j+Xt00∈RD(t0)XkBτ(t0,t00)idt00k,(4)whereRS(t0)isthesetofpreviouspositionswithedgesfromtheirstatevectorstothestatevectoratt0,RD(t0)isthesetofpreviouspositionswithedgesfromtheirdecisionvectorstothestatevectoratt0,τ(t0,t00)istherelevantrelationbetweenthepositiont00andthepositiont0,andJτijandBτidareweightmatrices.Inordertomaximize(2),theapproximatedistri-butionofthenextdecisionsqtk(d)shouldbesettoqtk(d)=Φh(t,k)(d)ePjWdjµtjPd0Φh(t,k)(d0)ePjWd0jµtj,(5)asfollowsfromexpression(3).Theresultingesti-mateofthetreeprobabilityisgivenby:P(T)≈Yt,kqtk(dtk).Thisapproximationmethodreplicatesexactlythecomputationofthefeed-forwardneuralnetworkin(Henderson,2003),wheretheabovemeansµt0iareequivalenttotheneuralnetworkhiddenunitacti-vations.Thus,thatneuralnetworkprobabilitymodelcanberegardedasasimpleapproximationtothegraphicalmodelintroducedinsection3.1.Inadditiontothedrawbackssharedbyanymeanﬁeldapproximationmethod,thisfeed-forwardap-proximationcannotcapturebackwardreasoning.Bybackward(a.k.a.top-down)reasoningwemeantheneedtoupdatethestatevectormeansµt0iafterobservingadecisiondtk,fort0≤t.Thenextsectiondiscusseshowbackwardreasoningcanbeincorpo-ratedintheapproximatemodel.3.3AMeanFieldApproximationThissectionproposesamoreaccuratewaytoap-proximateISBNswithmeanﬁeldmethods,whichwewillcallthemeanﬁeldapproximation.Again,weareinterestedinﬁndingthedistributionQwhichmaximizesthequantityLVinexpression(2).Thedecisiondistributionqtk(dtk)maximizesLVwhenithasthesamedependenceonthestatevectormeansµtkasinthefeed-forwardapproximation,namelyex-pression(5).However,aswementionedabove,thefeed-forwardcomputationdoesnotallowustocom-putetheoptimalvaluesofstatemeansµt0i.Optimally,aftereachnewdecisiondtk,weshouldrecomputeallthemeansµt0iforallthestatevec-torsSt0,t0≤t.However,thiswouldmakethemethodintractable,duetothelengthofderivationsinconstituentparsingandtheinterdependencebe-tweenthesemeans.Instead,aftermakingeachdeci-siondtkandaddingittothesetofvisiblevariablesV,werecomputeonlymeansofthecurrentstatevectorSt.Thedenominatorofthenormalizedexponentialfunctionin(3)doesnotallowustocomputeLVex-actly.Instead,weuseasimpleﬁrstorderapproxi-mation:EQ[lnXdΦh(t,k)(d)exp(XjWdjstj)]≈lnXdΦh(t,k)(d)exp(XjWdjµtj),(6)wheretheexpectationEQ[...]istakenoverthestatevectorStdistributedaccordingtotheapproximatedistributionQ.Unfortunately,evenwiththisassumptionthereisnoanalyticwaytomaximizeLVwithrespecttothemeansµtk,soweneedtousenumericalmethods.Assuming(6),wecanrewritetheexpression(2)asfollows,substitutingthetrueP(H,V)deﬁnedbythegraphicalmodelandtheapproximatedistribu-tionQ(H|V),omittingpartsindependentofµtk:Lt,kV=Xi−µtilnµti−(1−µti)ln(cid:16)1−µti(cid:17)+µtiηti+Xk0<kΦh(t,k0)(dtk0)XjWdtk0jµtj−Xk0<klnXdΦh(t,k0)(d)exp(XjWdjµtj),(7)here,ηtiiscomputedfromthepreviousrelevantstatemeansanddecisionsasin(4).Thisexpressionis637

concavewithrespecttotheparametersµti,sotheglobalmaximumcanbefound.Weusecoordinate-wiseascent,whereeachµtiisselectedbyanefﬁcientlinesearch(Pressetal.,1996),whilekeepingotherµti0ﬁxed.3.4ParameterEstimationWetrainthesemodelstomaximizetheﬁtoftheapproximatemodeltothedata.Weusegradientdescentandamaximumlikelihoodobjectivefunc-tion.Thisrequirescomputationofthegradientoftheapproximatelog-likelihoodwithrespecttothemodelparameters.Inordertocomputethesederiva-tives,theerrorshouldbepropagatedallthewaybackthroughthestructureofthegraphicalmodel.Forthefeed-forwardapproximation,computationofthederivativesisstraightforward,asinneuralnet-works.Butforthemeanﬁeldapproximation,itre-quirescomputationofthederivativesofthemeansµtiwithrespecttotheotherparametersinexpres-sion(7).Theuseofanumericalsearchinthemeanﬁeldapproximationmakestheanalyticalcomputa-tionofthesederivativesimpossible,soadifferentmethodneedstobeusedtocomputetheirvalues.IfmaximizationofLt,kVisdoneuntilconvergence,thenthederivativesofLt,kVwithrespecttoµtiareclosetozero:Ft,ki=∂Lt,kV∂µti≈0foralli.Thissystemofequationsallowsustouseimplicitdifferentiationtocomputetheneededderivatives.4ExperimentalEvaluationInthissectionweevaluatethetwoapproximationstodynamicSBNsdiscussedintheprevioussection,thefeed-forwardmethodequivalenttotheneuralnetworkof(Henderson,2003)(NNmethod)andthemeanﬁeldmethod(MFmethod).Thehypothesiswewishtotestisthatthemoreaccurateapproxima-tionofdynamicSBNswillresultinamoreaccuratemodelofconstituentstructureparsing.Ifthisistrue,thenitsuggeststhatdynamicSBNsoftheformpro-posedhereareagoodabstractmodelofthenatureofnaturallanguageparsing.WeusedthePennTreebankWSJcorpus(Marcusetal.,1993)toperformtheempiricalevaluationoftheconsideredapproaches.ItisexpensivetotrainRPF1Bikel,200487.988.888.3Taskaretal.,200489.189.189.1NNmethod89.189.289.1TurianandMelamed,200689.389.689.4MFmethod89.390.790.0Charniak,200090.090.290.1Table1:Percentagelabeledconstituentrecall(R),precision(P),combinationofboth(F1)onthetest-ingset.theMFapproximationonthewholeWSJcorpus,soinsteadweuseonlysentencesoflengthatmost15,asin(Taskaretal.,2004)and(TurianandMelamed,2006).Thestandardsplitofthecorpusintotraining(sections2–22,9,753sentences),validation(section24,321sentences),andtesting(section23,603sen-tences)wasperformed.2Asin(Henderson,2003;TurianandMelamed,2006)weusedapubliclyavailabletagger(Ratna-parkhi,1996)toprovidethepart-of-speechtagforeachwordinthesentence.Foreachtag,thereisanunknown-wordvocabularyitemwhichisusedforallthosewordswhicharenotsufﬁcientlyfrequentwiththattagtobeincludedindividuallyinthevocabu-lary.Weonlyincludedaspeciﬁctag-wordpairinthevocabularyifitoccurredatleast20timeinthetrain-ingset,which(withtag-unknown-wordpairs)ledtotheverysmallvocabularyof567tag-wordpairs.DuringparsingwithboththeNNmethodandtheMFmethod,weusedbeamsearchwithapost-wordbeamof10.Increasingthebeamsizebeyondthisvaluedidnotsigniﬁcantlyeffectparsingaccuracy.Forbothofthemodels,thestatevectorsizeof40wasused.AlltheparametersforboththeNNandMFmodelsweretunedonthevalidationset.Asin-glebestmodelofeachtypewasthenappliedtotheﬁnaltestingset.Table1liststheresultsoftheNNapproximationandtheMFapproximation,alongwithresultsofdif-2TrainingofourMFmethodonthissubsetofWSJtooklessthan6daysonastandarddesktopPC.WewouldexpectthatamodelfortheentireWSJcorpuscanbetrainedinabout3monthstime.Thetrainingtimeisaboutlinearwiththenum-berofwords,butalargerstatevectorisneededtoaccommo-datealltheinformation.ThelongtrainingtimesontheentireWSJwouldnotallowustotunethemodelparametersproperly,whichwouldhaveincreasedtherandomnessoftheempiricalcomparison,althoughitwouldbefeasibleforbuildingasys-tem.638

ferentgenerativeanddiscriminativeparsingmeth-ods(Bikel,2004;Taskaretal.,2004;TurianandMelamed,2006;Charniak,2000)evaluatedinthesameexperimentalsetup.TheMFmodelimprovesoverthebaselineNNapproximation,withanerrorreductioninF-measureexceeding8%.Thisim-provementisstaticallysigniﬁcant.3TheMFmodelachievesresultswhichdonotappeartobesigniﬁ-cantlydifferentfromtheresultsofthebestmodelinthelist(Charniak,2000).Itshouldalsobenotedthatthemodel(Charniak,2000)isthemostaccu-rategenerativemodelonthestandardWSJparsingbenchmark,whichconﬁrmstheviabilityofourgen-erativemodel.TheseexperimentalresultssuggestthatIncre-mentalSigmoidBeliefNetworksareanappropriatemodelfornaturallanguageparsing.Evenapproxi-mationssuchasthosetestedhere,withaverystrongfactorisabilityassumption,allowustobuildquiteaccurateparsingmodels.Themaindrawbackofourproposedmeanﬁeldapproachistherelativecompu-tationalcomplexityofthenumericalprocedureusedtomaximizeLt,kV.Butthisapproximationhassuc-ceededinshowingthatamoreaccurateapproxima-tionofISBNsresultsinamoreaccurateparser.Webelievethisprovidesstrongjustiﬁcationformoreac-curateapproximationsofISBNsforparsing.5RelatedWorkTherehasnotbeenmuchpreviousworkongraph-icalmodelsforfullparsing,althoughrecentlysev-erallatentvariablemodelsforparsinghavebeenproposed(KooandCollins,2005;Matsuzakietal.,2005;Riezleretal.,2002).In(KooandCollins,2005),anundirectedgraphicalmodelisusedforparsereranking.DependencyparsingwithdynamicBayesiannetworkswasconsideredin(PeshkinandSavova,2005),withlimitedsuccess.Theirmodelisverydifferentfromours.Roughly,itconsideredthewholesentenceatatime,withthegraphicalmodelbeingusedtodecidewhichwordscorrespondtoleavesofthetree.Thechosenwordsarethenremovedfromthesentenceandthemodelisrecur-sivelyappliedtothereducedsentence.Undirectedgraphicalmodels,inparticularCondi-3Wemeasuredsigniﬁcanceofalltheexperimentsinthispa-perwiththerandomizedsigniﬁcancetest(Yeh,2000).tionalRandomFields,arethestandardtoolsforshal-lowparsing(ShaandPereira,2003).However,shal-lowparsingiseffectivelyasequencelabelingprob-lemandthereforedifferssigniﬁcantlyfromfullpars-ing.Asdiscussedinsection2.2,undirectedgraph-icalmodelsdonotseemtobesuitableforhistory-basedfullparsingmodels.SigmoidBeliefNetworkswereusedoriginallyforcharacterrecognitiontasks,butlateradynamicmodiﬁcationofthismodelwasappliedtotherein-forcementlearningtask(Sallans,2002).However,theirgraphicalmodel,approximationmethod,andlearningmethoddiffersigniﬁcantlyfromthoseofthispaper.6ConclusionsThispaperproposesanewgenerativeframeworkforconstituentparsingbasedondynamicSigmoidBeliefNetworkswithvectorsoflatentvariables.Exactinferencewiththeproposedgraphicalmodel(calledIncrementalSigmoidBeliefNetworks)isnottractable,buttwoapproximationsareconsid-ered.First,itisshownthattheneuralnetworkparserof(Henderson,2003)canbeconsideredasasimplefeed-forwardapproximationtothegraphicalmodel.Second,amoreaccuratebutstilltractableapproximationbasedonmeanﬁeldtheoryispro-posed.Bothmethodsareempiricallycompared,andthemeanﬁeldapproachachievessigniﬁcantlybetterresults,whicharenon-signiﬁcantlydifferentfromtheresultsofthemostaccurategenerativeparsingmodel(Charniak,2000)onourtestingset.ThefactthatamoreaccurateapproximationleadstoamoreaccurateparsersuggeststhatISBNsareagoodab-stractmodelforconstituentstructureparsing.Thisempiricalresultmotivatesresearchintomoreaccu-rateapproximationsofdynamicSBNs.Wefocusedinthispaperongenerativemodelsofparsing.Theresultsofsuchagenerativemodelcanbeeasilyimprovedbyadiscriminativererank-ingmodel,evenwithoutanyadditionalfeatureen-gineering.Forexample,thediscriminativetrain-ingtechniquessuccessfullyappliedin(Henderson,2004)tothefeed-forwardneuralnetworkmodelcanbedirectlyappliedtothemeanﬁeldmodelpro-posedinthispaper.Thesameistrueforrerank-ingwithdata-deﬁnedkernels,withwhichwewould639

expectsimilarimprovementsaswereachievedwiththeneuralnetworkparser(HendersonandTitov,2005).Suchimprovementsshouldsituatetheresult-ingmodelamongthebestcurrentparsingmodels.ReferencesDanM.Bikel.2004.IntricaciesofCollins’parsingmodel.ComputationalLinguistics,30(4).EugeneCharniakandMarkJohnson.2005.Coarse-to-ﬁnen-bestparsingandMaxEntdiscriminativererank-ing.InProc.ACL,pages173–180,AnnArbor,MI.EugeneCharniak.2000.Amaximum-entropy-inspiredparser.InProc.ACL,pages132–139,Seattle,Wash-ington.MichaelCollins.1999.Head-DrivenStatisticalModelsforNaturalLanguageParsing.Ph.D.thesis,Univer-sityofPennsylvania,Philadelphia,PA.MichaelCollins.2000.Discriminativererankingfornat-urallanguageparsing.InProc.ICML,pages175–182,Stanford,CA.ThomasM.CoverandJoyA.Thomas.1991.ElementsofInformationTheory.JohnWiley,NewYork,NY.S.GemanandD.Geman.1984.Stochasticrelaxation,Gibbsdistributions,andtheBayesianrestorationofimages.IEEETransactionsonPatternAnalysisandMachineIntelligence,6:721–741.JamesHendersonandIvanTitov.2005.Data-deﬁnedkernelsforparsererankingderivedfromprobabilisticmodels.InProc.ACL,AnnArbor,MI.JamesHenderson.2003.Inducinghistoryrepresenta-tionsforbroadcoveragestatisticalparsing.InProc.HLT-NAACL,pages103–110,Edmonton,Canada.JamesHenderson.2004.Discriminativetrainingofaneuralnetworkstatisticalparser.InProc.ACL,Barcelona,Spain.G.Hinton,P.Dayan,B.Frey,andR.Neal.1995.Thewake-sleepalgorithmforunsupervisedneuralnet-works.Science,268:1158–1161.M.I.Jordan,Z.Ghahramani,T.S.Jaakkola,andL.K.Saul.1999.Anintroductiontovariationalmethodsforgraphicalmodels.InMichaelI.Jordan,editor,Learn-inginGraphicalModels.MITPress,Cambridge,MA.TerryKooandMichaelCollins.2005.Hidden-variablemodelsfordiscriminativereranking.InProc.EMNLP,Vancouver,B.C.,Canada.MitchellP.Marcus,BeatriceSantorini,andMaryAnnMarcinkiewicz.1993.Buildingalargeannotatedcor-pusofEnglish:ThePennTreebank.ComputationalLinguistics,19(2):313–330.TakuyaMatsuzaki,YusukeMiyao,andJun’ichiTsujii.2005.ProbabilisticCFGwithlatentannotations.InProc.ACL,AnnArbor,MI.RadfordNeal.1992.Connectionistlearningofbeliefnetworks.ArtiﬁcialIntelligence,56:71–113.LeonPeshkinandVirginiaSavova.2005.Dependencyparsingwithdynamicbayesiannetwork.InAAAI,20thNationalConferenceonArtiﬁcialIntelligence,Pittsburgh,Pennsylvania.W.Press,B.Flannery,S.Teukolsky,andW.Vetterling.1996.NumericalRecipes.CambridgeUniversityPress,Cambridge,UK.AdwaitRatnaparkhi.1996.Amaximumentropymodelforpart-of-speechtagging.InProc.EMNLP,pages133–142,Univ.ofPennsylvania,PA.StefanRiezler,TracyH.King,RonaldM.Kaplan,RichardCrouch,JohnT.Maxwell,andMarkJohn-son.2002.ParsingtheWallStreetJournalusingaLexical-FunctionalGrammaranddiscriminativeesti-mationtechniques.InProc.ACL,Philadelphia,PA.BrianSallans.2002.ReinforcementLearningforFac-toredMarkovDecisionProcesses.Ph.D.thesis,Uni-versityofToronto,Toronto,Canada.LawrenceK.SaulandMichaelI.Jordan.1999.Ameanﬁeldlearningalgorithmforunsupervisedneu-ralnetworks.InMichaelI.Jordan,editor,LearninginGraphicalModels,pages541–554.MITPress,Cam-bridge,MA.FeiShaandFernandoPereira.2003.Shallowparsingwithconditionalrandomﬁelds.InProc.HLT-NAACL,Edmonton,Canada.BenTaskar,DanKlein,MichaelCollins,DaphneKoller,andChristopherManning.2004.Max-marginpars-ing.InProc.EMNLP,Barcelona,Spain.JosephTurianandDanMelamed.2006.Advancesindiscriminativeparsing.InProc.COLING-ACL,Syd-ney,Australia.AlexanderYeh.2000.Moreaccuratetestsforthesta-tisticalsigniﬁcanceoftheresultdifferences.InProc.COLING,pages947–953,Saarbruken,Germany.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 640–647,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

640

CorpusEffectsontheEvaluationofAutomatedTransliterationSystemsSarvnazKarimiAndrewTurpinFalkScholerSchoolofComputerScienceandInformationTechnologyRMITUniversity,GPOBox2476V,Melbourne3001,Australia{sarvnaz,aht,fscholer}@cs.rmit.edu.auAbstractMostcurrentmachinetransliterationsys-temsemployacorpusofknownsource-targetwordpairstotraintheirsystem,andtypicallyevaluatetheirsystemsonasimilarcorpus.Inthispaperweexploretheperfor-manceoftransliterationsystemsoncorporathatarevariedinacontrolledway.Inpartic-ular,wecontrolthenumber,andpriorlan-guageknowledgeofhumantransliteratorsusedtoconstructthecorpora,andtheoriginofthesourcewordsthatmakeupthecor-pora.Weﬁndthatthewordaccuracyofau-tomatedtransliterationsystemscanvarybyupto30%(inabsoluteterms)dependingonthecorpusonwhichtheyarerun.Wecon-cludethatatleastfourhumantransliteratorsshouldbeusedtoconstructcorporaforeval-uatingautomatedtransliterationsystems;andthatalthoughabsolutewordaccuracymetricsmaynottranslateacrosscorpora,therelativerankingsofsystemperformancere-mainsstableacrossdifferingcorpora.1IntroductionMachinetransliterationistheprocessoftransform-ingawordwritteninasourcelanguageintoawordinatargetlanguagewithouttheaidofabilingualdictionary.Wordpronunciationispreserved,asfaraspossible,butthescriptusedtorenderthetargetwordisdifferentfromthatofthesourcelanguage.Transliterationisappliedtopropernounsandout-of-vocabularytermsaspartofmachinetranslationandcross-lingualinformationretrieval(CLIR)(Ab-dulJaleelandLarkey,2003;Pirkolaetal.,2006).Severaltransliterationmethodsarereportedintheliteratureforavarietyoflanguages,withtheirper-formancebeingevaluatedonmultilingualcorpora.Source-targetpairsareeitherextractedfrombilin-gualdocumentsordictionaries(AbdulJaleelandLarkey,2003;BilacandTanaka,2005;OhandChoi,2006;ZelenkoandAone,2006),orgatheredex-plicitlyfromhumantransliterators(Al-OnaizanandKnight,2002;ZelenkoandAone,2006).Someeval-uationsoftransliterationmethodsdependonasingleuniquetransliterationforeachsourceword,whileotherstakemultipletargetwordsforasinglesourcewordintoaccount.IntheirworkontransliteratingEnglishtoPersian,Karimietal.(2006)observedthatthecontentofthecorpususedforevaluatingsystemscouldhavedramaticaffectsonthereportedaccuracyofmethods.Theeffectsofcorpuscompositionontheevalua-tionoftransliterationsystemshasnotbeenspecif-icallystudied,withonlyimplicitexperimentsorclaimsmadeintheliteraturesuchasintroduc-ingtheeffectsofdifferenttransliterationmod-els(AbdulJaleelandLarkey,2003),languagefam-ilies(Lind´en,2005)orapplicationbased(CLIR)evaluation(Pirkolaetal.,2006).Inthispaper,were-portourexperimentsdesignedtoexplicitlyexaminetheeffectthatvaryingtheunderlyingcorpususedinbothtrainingandtestingsystemshasontranslitera-tionaccuracy.Speciﬁcally,wevarythenumberofhumantransliteratorsthatareusedtoconstructthecorpus;andtheoriginoftheEnglishwordsusedinthecorpus.Ourexperimentsshowthatthewordaccuracyofautomatedtransliterationsystemscanvarybyupto30%(inabsoluteterms),dependingonthecorpusused.Despitethewiderangeofabsolutevalues641

inperformance,therankingofourtwotranslitera-tionsystemswaspreservedonallcorpora.Wealsoﬁndthatahuman’sconﬁdenceinthelanguagefromwhichtheyaretransliteratingcanaffectthecorpusinsuchawaythatwordaccuracyratesarealtered.2BackgroundMachinetransliterationmethodsaredividedintographeme-based(AbdulJaleelandLarkey,2003;Lind´en,2005),phoneme-based(Jungetal.,2000;VirgaandKhudanpur,2003)andcombinedtech-niques(BilacandTanaka,2005;OhandChoi,2006).Grapheme-basedmethodsderivetransforma-tionrulesforcharactercombinationsinthesourcetextfromatrainingdataset,whilephoneme-basedmethodsuseanintermediatephonetictransforma-tion.Inthispaper,weusetwographeme-basedmethodsforEnglishtoPersiantransliteration.Dur-ingatrainingphase,bothmethodsderiverulesfortransformingcharactercombinations(segments)inthesourcelanguageintocharactercombinationsinthetargetlanguagewithsomeprobability.Duringtransliteration,thesourcewordsiisseg-mentedandrulesarechosenandappliedtoeachseg-mentaccordingtoheuristics.Theprobabilityofaresultingwordistheproductoftheprobabilitiesoftheappliedrules.Theresultisalistoftargetwordssortedbytheirassociatedprobabilities,Li.Theﬁrstsystemweuse(SYS-1)isann-gramapproachthatusesthelastcharacteroftheprevi-oussourcesegmenttoconditionthechoiceoftheruleforthecurrentsourcesegment.Thissystemhasbeenshowntooutperformothern-grambasedmeth-odsforEnglishtoPersiantransliteration(Karimietal.,2006).Thesecondsystemweemploy(SYS-2)makesuseofsomeexplicitknowledgeofourchosenlan-guagepair,EnglishandPersian,andisalsoonthecollapsed-vowelschemepresentedbyKarimietal.(2006).Inparticular,itexploitsthetendencyforrunsofEnglishvowelstobecollapsedintoasinglePersiancharacter,orperhapsomittedfromthePer-sianaltogether.Assuch,segmentsarechosenbasedonsurroundingconsonantsandvowels.Thefullde-tailsofthissystemarenotimportantforthispaper;herewefocusontheperformanceevaluationofsys-tems,notthesystemsthemselves.2.1SystemEvaluationInordertoevaluatethelistLioftargetwordspro-ducedbyatransliterationsystemforsourcewordsi,atestcorpusisconstructed.Thetestcorpuscon-sistsofasourceword,si,andalistofpossibletargetwords{tij},where1≤j≤di,thenumberofdis-tincttargetwordsforsourcewordsi.Associatedwitheachtijisacountnijwhichisthenumberofhumantransliteratorswhotransliteratedsiintotij.Oftenthetestcorpusisaproportionofalargercorpus,theremainderofwhichhasbeenusedfortrainingthesystem’srulebase.Inthisworkweadoptthestandardten-foldcrossvalidationtech-niqueforallofourresults,where90%ofacorpusisusedfortrainingand10%fortesting.Thepro-cessisrepeatedtentimes,andthemeanresulttaken.Forthwith,weusethetermcorpustorefertothesin-glecorpusfromwhichbothtrainingandtestsetsaredrawninthisfashion.Oncethecorpusisdecidedupon,ametrictomea-surethesystem’saccuracyisrequired.Theappro-priatemetricdependsonthescenarioinwhichthetransliterationsystemistobeused.Forexample,inamachinetranslationapplicationwhereonlyonetargetwordcanbeinsertedinthetexttorepresentasourceword,itisimportantthatthewordatthetopofthesystemgeneratedlistoftargetwords(bydef-initionthemostprobable)isoneofthewordsgen-eratedbyahumaninthecorpus.Moreformally,theﬁrstwordgeneratedforsourcewordsi,Li1,mustbeoneoftij,1≤j≤di.Itmayevenbedesirablethatthisisthetargetwordmostcommonlyusedforthissourceword;thatis,Li1=tijsuchthatnij≥nik,forall1≤k≤di.Alternately,inaCLIRappli-cation,allvariantsofasourcewordmightbere-quired.Forexample,ifausersearchesforanEn-glishterm“Tom”inPersiandocuments,thesearchengineshouldtryandlocatedocumentsthatcontainboth“A(cid:16)”(3letters:(cid:16) --)and”Õç(cid:16)”(2letters:(cid:16) -),twopossibletransliterationsof“Tom”thatwouldbegeneratedbyhumantransliterators.Inthiscase,ametricthatcountsthenumberoftijthatappearinthetopdielementsofthesystemgeneratedlist,Li,mightbeappropriate.Inthispaperwefocusonthe“Top-1”case,whereitisimportantforthemostprobabletargetwordgen-eratedbythesystem,Li1tobeeitherthemostpop-642

ulartij(labeledtheMajority,withtiesbrokenar-bitrarily),orjustoneofthetij’s(labeledUniformbecauseallpossibletransliterationsareequallyre-warded).Athirdscheme(labeledWeighted)isalsopossiblewheretherewardfortijappearingasLi1isnij/(cid:229)dij=1nij;here,eachtargetwordisgivenaweightproportionaltohowoftenahumantranslit-eratorchosethattargetword.Duetospaceconsid-erations,wefocusontheﬁrsttwovariantsonly.Ingeneral,therearetwocommonlyusedmet-ricsfortransliterationevaluation:wordaccuracy(WA)andcharacteraccuracy(CA)(HallandDowl-ing,1980).Inallofourexperiments,CAbasedmetricscloselymirroredWAbasedmetrics,andsoconclusionsdrawnfromthedatawouldbethesamewhetherWAmetricsorCAmetricswereused.HenceweonlydiscussandreportWAbasedmetricsinthispaper.ForeachsourcewordinthetestcorpusofKwords,wordaccuracycalculatesthepercentageofcorrectlytransliteratedterms.Henceforthemajor-itycase,whereeverysourcewordinthecorpusonlyhasonetargetword,thewordaccuracyisdeﬁnedasMWA=|{si|Li1=ti1,1≤i≤K}|/K,andfortheUniformcase,whereeverytargetvariantisincludedwithequalweightinthecorpus,thewordaccuracyisdeﬁnedasUWA=|{si|Li1∈{tij},1≤i≤K,1≤j≤di}|/K.2.2HumanEvaluationToevaluatethelevelofagreementbetweentranslit-erators,weuseanagreementmeasurebasedonMunandEye(2004).Foranysourcewordsi,therearedidifferenttransliterationsmadebythenihumantranslitera-tors(ni=(cid:229)dij=1nij,wherenijisthenumberoftimessourcewordsiwastransliteratedintotargetwordtij).Whenanytwotransliteratorsagreeonthesametargetword,therearetwoagreementsbeingmade:transliteratoroneagreeswithtransliteratortwo,andviceversa.Ingeneral,therefore,theto-talnumberofagreementsmadeonsourcewordsiis(cid:229)dij=1nij(nij−1).HencethetotalnumberofactualagreementsmadeontheentirecorpusofKwordsisAact=K(cid:229)i=1di(cid:229)j=1nij(nij−1).Thetotalnumberofpossibleagreements(thatis,whenallhumantransliteratorsagreeonasingletar-getwordforeachsourceword),isAposs=K(cid:229)i=1ni(ni−1).TheproportionofoverallagreementisthereforePA=AactAposs.2.3CorporaSeventransliterators(T1,T2,...,T7:allnativePer-sianspeakersfromIran)wererecruitedtotransliter-ate1500propernamesthatweprovided.ThenamesweretakenfromlistsofnameswritteninEnglishonEnglishWebsites.FivehundredofthesenamesalsoappearedinlistsofnamesonArabicWebsites,andﬁvehundredonDutchnamelists.Thetransliteratorswerenottoldoftheoriginofeachword.Theen-tirecorpus,therefore,waseasilyseparatedintothreesub-corporaof500wordseachbasedontheoriginofeachword.Todistinguishthesecollections,weuseE7,A7andD7todenotetheEnglish,ArabicandDutchsub-corpora,respectively.Thewhole1500wordcorpusisreferredtoasEDA7.DutchandArabicwerechosenwithanassump-tionthatmostIranianPersianspeakershavelittleknowledgeofDutch,whiletheirfamiliaritywithArabicshouldbeinthesecondrankafterEnglish.AlloftheparticipantsheldatleastaBachelorsde-gree.Table1summarizestheinformationaboutthetransliteratorsandtheirperceptionofthegiventask.Participantswereaskedtoscalethedifﬁcultyofthetransliterationofeachsub-corpus,indicatedasascalefrom1(hard)to3(easy).Similarly,theparticipants’conﬁdenceinperformingthetaskwasratedfrom1(noconﬁdence)to3(quiteconﬁdent).Theleveloffamiliaritywithsecondlanguageswasalsoreportedbasedonascaleofzero(notfamiliar)to3(excellentknowledge).Theinformationprovidedbyparticipantscon-ﬁrmsourassumptionoftransliteratorsknowledgeofsecondlanguages:highfamiliaritywithEnglish,someknowledgeofArabic,andlittleornopriorknowledgeofDutch.Also,themajorityofthemfoundthetransliterationofEnglishtermsofmediumdifﬁculty,Dutchwasconsideredmostlyhard,andArabicaseasytomedium.643

SecondLanguageKnowledgeDifﬁculty,ConﬁdenceTransliteratorEnglishDutchArabicOtherEnglishDutchArabic1201-1,11,22,32202-2,22,33,33201-2,21,22,24201-2,22,13,35202Turkish2,21,13,26201-2,21,13,37201-2,21,12,2Table1:Transliterator’slanguageknowledge(0=notfamiliarto3=excellentknowledge),perceptionofdifﬁculty(1=hardto3=easy)andconﬁdence(1=noconﬁdenceto3=quiteconﬁdent)increatingthecorpus.E7D7A7EDA7Corpus020406080100Word Accuracy (%) UWA (SYS-2)UWA (SYS-1)MWA (SYS-2)MWA (SYS-1)Figure1:Comparisonofthetwoevaluationmetricsusingthetwosystemsonfourcorpora.(Lineswereaddedforclarity,anddonotrepresentdatapoints.)020406080100Corpus020406080100Word Accuracy (%) UWA (SYS-2)UWA (SYS-1)MWA (SYS-2)MWA (SYS-1)Figure2:Comparisonofthetwoevaluationmetricsusingthetwosystemson100randomlygeneratedsub-corpora.3ResultsFigure1showsthevaluesofUWAandMWAforE7,A7,D7andEDA7usingthetwotransliterationsystems.Immediatelyobviousisthatvaryingthecorpora(x-axis)resultsindifferentvaluesforwordaccuracy,whetherbytheUWAorMWAmethod.Forexample,ifyouchosetoevaluateSYS-2withtheUWAmetricontheD7corpus,youwouldobtainaresultof82%,butifyouchosetoevaluateitwiththeA7corpusyouwouldreceivearesultofonly73%.Thismakescomparingsystemsthatreportresultsobtainedondifferentcorporaverydifﬁcult.Encour-agingly,however,SYS-2consistentlyoutperformstheSYS-1onallcorporaforbothmetricsexceptMWAonE7.Thisimpliesthatrankingsystemper-formanceonthesamecorpusmostlikelyyieldsasystemrankingthatistransferabletoothercorpora.Tofurtherinvestigatethis,werandomlyextracted100corporaof500wordpairsfromEDA7andranthetwosystemsonthemandevaluatedtheresultsusingbothMWAandUWA.Bothofthemeasuresrankedthesystemsconsistentlyusingallthesecor-pora(Figure2).Asexpected,theUWAmetricisconsistentlyhigherthantheMWAmetric;itallowsforthetoptransliterationtoappearinanyofthepossiblevari-antsforthatwordinthecorpus,unliketheMWAmetricwhichinsistsuponasingletargetword.Forexample,fortheE7corpususingtheSYS-2ap-proach,UWAis76.4%andMWAis47.0%.Eachofthethreesub-corporacanbefurtherdi-videdbasedonthesevenindividualtransliterators,indifferentcombinations.Thatis,constructasub-corpusfromT1’stransliterations,T2’s,andsoon;thentakeallcombinationsoftwotransliterators,thenthree,andsoon.Ingeneralwecanconstruct7Crsuchcorporafromrtransliteratorsinthisfash-ion,allofwhichhave500sourcewords,butmayhavebetweenonetosevendifferenttransliterationsforeachofthosewords.Figure3showstheMWAforthesesub-corpora.Thex-axisshowsthenumberoftransliteratorsusedtoformthesub-corpora.Forexample,whenx=3,theperformanceﬁguresplottedareachievedoncor-porawhentakingalltriplesoftheseventranslitera-tor’stransliterations.Fromtheboxplotsitcanbeseenthatperformancevariesconsiderablywhenthenumberoftransliter-atorsusedtodetermineamajorityvoteisvaried.644

12345672030405060D7                                                                                                                                                                                    12345672030405060Number of TransliteratorsEDA712345672030405060Word Accuracy (%)E7                                                                                                                                                                            12345672030405060Number of TransliteratorsWord Accuracy (%)A7Figure3:Performanceonsub-corporaderivedbycombiningthenumberoftransliteratorsshownonthex-axis.Boxesshowthe25thand75thpercentileoftheMWAforall7CxcombinationsoftransliteratorsusingSYS-2,withwhiskersshowingextremevalues.However,thechangesdonotfollowaﬁxedtrendacrossthelanguages.ForE7,therangeofaccuraciesachievedishighwhenonlytwoorthreetranslitera-torsareinvolved,rangingfrom37.0%to50.6%inSYS-2methodandfrom33.8%to48.0%inSYS-1(notshown)whenonlytwotransliterators’dataareavailable.Whenmorethanthreetransliteratorsareused,therangeofperformanceisnoticeablysmaller.Henceifatleastfourtransliteratorsareused,thenitismorelikelythatasystem’sMWAwillbestable.ThisﬁndingissupportedbyPapinenietal.(2002)whorecommendthatfourpeopleshouldbeusedforcollectingjudgmentsformachinetranslationexper-iments.ThecorporaderivedfromA7showconsistentme-dianincreasesasthenumberoftransliteratorsin-creases,butthemedianaccuracyislowerthanforotherlanguages.TheD7collectiondoesnotshowanystableresultsuntilatleastsixtransliterator’sareused.Theresultsindicatethatcreatingacollectionusedfortheevaluationoftransliterationsystems,basedona“goldstandard”createdbyonlyonehumantransliteratormayleadtowordaccuracyresultsthatcouldshowa10%absolutedifferencecomparedtoresultsonacorpusderivedusingadifferenttranslit-E7D7A7EDA7Corpus0204060Word Accuracy (%) T1T2T3T4T5T6T7SYS-2Figure4:Wordaccuracyonthesub-corporausingonlyasingletransliterator’stransliterations.erator.Thisisevidencedbytheleftmostboxineachpaneloftheﬁgurewhichhasawiderangeofresults.Figure4showsthisboxinmoredetailforeachcollection,plottingthewordaccuracyforeachuserforallsub-corporaforSYS-2.Theaccuracyachievedvariessigniﬁcantlybetweentranslitera-tors;forexample,forE7collections,wordaccuracyvariesfrom37.2%forT1to50.0%forT5.ThisvarianceismoreobviousfortheD7datasetwherethedifferencerangesfrom23.2%forT1to56.2%forT3.Originlanguagealsohasaneffect:accuracyfortheArabiccollection(A7)isgenerallylessthanthatofEnglish(E7).TheDutchcollection(D7),showsanunstabletrendacrosstransliterators.Inotherwords,accuracydiffersinanarrowerrangeforArabicandEnglish,butinwiderrangeforDutch.645

ThisislikelyduetothefactthatmosttransliteratorsfoundDutchadifﬁcultlanguagetoworkwith,asreportedinTable1.3.1TransliteratorConsistencyToinvestigatetheeffectofinvididualtransliteratorconsistencyonsystemaccuracy,weconsiderthenumberofPersiancharactersusedbyeachtransliter-atoroneachsub-corpus,andtheaveragenumberofrulesgeneratedbySYS-2onthetentrainingsetsde-rivedintheten-foldcrossvalidationprocess,whichareshowninTable2.Forexample,whentranslit-eratingwordsfromE7intoPersian,T3onlyeverused21outof32charactersavailableinthePersianalphabet;T7,ontheotherhand,used24differentPersiancharacters.Itisexpectedthatanincreaseinnumberofcharactersorrulesprovidesmore“noise”fortheautomatedsystem,hencemayleadtoloweraccuracy.Superﬁciallytheoppositeseemstrueforrules:themeannumberofrulesgeneratedbySYS-2ismuchhigherfortheEDA7corpusthanfortheA7corpus,andyetFigure1showsthatwordaccuracyishigherontheEDA7corpus.Acorrelationtest,however,revealsthatthereisnosigniﬁcantrelation-shipbetweeneitherthenumberofcharactersused,northenumberofrulesgenerated,andtheresult-ingwordaccuracyofSYS-2(Spearmancorrelation,p=0.09(characters)andp=0.98(rules)).Abetterindicationof“noise”inthecorpusmaybegivenbytheconsistencywithwhichatranslit-eratorappliesacertainrule.Forexample,alargenumberofrulesgeneratedfromaparticulartranslit-erator’scorpusmaynotbeproblematicifmanyoftherulesgetappliedwithalowprobability.If,ontheotherhand,thereweremanyruleswithapprox-imatelyequalprobabilities,thesystemmayhavedifﬁcultydistinguishingwhentoapplysomerules,andnotothers.Onewaytoquantifythiseffectistocomputetheselfentropyoftheruledistribu-tionforeachsegmentinthecorpusforanindi-vidual.Ifpijistheprobabilityofapplyingrule1≤j≤mwhenconfrontedwithsourcesegmenti,thenHi=−(cid:229)mj=1pijlog2pijistheentropyoftheprobabilitydistributionforthatrule.Hismaximizedwhentheprobabilitiespijareallequal,andmini-mizedwhentheprobabilitiesareveryskewed(Shan-non,1948).Asanexample,considertherules:t→<(cid:16) ,0.5>,t→< ,0.3>andt→<X,0.2>;forwhichHt=0.79.Theexpectedentropycanbeusedtoobtainasin-gleentropyvalueoverthewholecorpus,E=−R(cid:229)i=1fiSHi,whereHiistheentropyoftheruleprobabilitiesforsegmenti,Risthetotalnumberofsegments,fiisthefrequencywithwhichsegmentioccursatanypositioninallsourcewordsinthecorpus,andSisthesumofallfi.TheexpectedentropyforeachtransliteratorisshowninFigure5,separatedbycorpus.Compar-isonofthisgraphwithFigure4showsthatgen-erallytransliteratorsthathaveusedrulesinconsis-tentlygenerateacorpusthatleadstolowaccuracyforthesystems.Forexample,T1whohasthelow-estaccuracyforallthecollectionsinbothmethods,alsohasthehighestexpectedentropyofrulesforallthecollections.FortheE7collection,themax-imumaccuracyof50.0%,belongstoT5whohastheminimumexpectedentropy.ThesameappliestotheD7collection,wherethemaximumaccuracyof56.2%andtheminimumexpectedentropybothbelongtoT3.TheseobservationsareconﬁrmedbyastatisticallysigniﬁcantSpearmancorrelationbetweenexpectedruleentropyandwordaccuracy(r=−0.54,p=0.003).Therefore,theconsistencywithwhichtransliteratorsemploytheirowninternalrulesindevelopingacorpushasadirecteffectonsystemperformancemeasures.3.2Inter-TransliteratorAgreementandPerceivedDifﬁcultyHerewepresentvariousagreementproportions(PAfromSection2.2),whichgiveameasureofconsis-tencyinthecorporaacrossallusers,asopposedtotheentropymeasurewhichgivesaconsistencymea-sureforasingleuser.ForE7,PAwas33.6%,forA7itwas33.3%andforD7,agreementwas15.5%.Ingeneral,humansagreelessthan33%ofthetimewhentransliteratingEnglishtoPersian.Inaddition,weexaminedagreementamongtransliteratorsbasedontheirperceptionofthetaskdifﬁcultyshowninTable1.ForA7,agreementamongthosewhofoundthetaskeasywashigher(22.3%)thanthosewhofounditinmediumlevel646

E7D7A7EDA7CharRulesCharRulesCharRulesCharRulesT1235232362328330311075T222487255502930432956T321466205002828031870T423497225242830730956T521492225082829629896T624493215632531329968T724495215292829930952Mean23493225422830430953Table2:NumberofcharactersusedandrulesgeneratedusingSYS-2,pertransliterator.(18.8%).PAis12.0%forthosewhofoundtheD7collectionhardtotransliterate;whilethesixtransliteratorswhofoundtheE7collectiondifﬁcultymediumhadPA=30.2%.Hence,theharderpar-ticipantsratedthetransliterationtask,thelowertheagreementscorestendtobeforthederivedcorpus.Finally,inTable3weshowwordaccuracyresultsforthetwosystemsoncorporaderivedfromtranslit-eratorsgroupedbyperceivedlevelofdifﬁcultyonA7.ItisreadilyapparentthatSYS-2outperformsSYS-1onthecorpuscomprisedofhumantranslit-erationsfrompeoplewhosawthetaskaseasywithbothwordaccuracymetrics;therelativeimprove-mentofover50%isstatisticallysigniﬁcant(pairedt-testonten-foldcrossvalidationruns).However,onthecorpuscomposedoftransliterationsthatwereperceivedasmoredifﬁcult,“Medium”,theadvan-tageofSYS-2issigniﬁcantlyeroded,butisstillstatisticallysigniﬁcantforUWA.Hereagain,usingonlyonetransliteration,MWA,didnotdistinguishtheperformanceofeachsystem.4DiscussionWehaveevaluatedtwoEnglishtoPersiantranslit-erationsystemsonavarietyofcontrolledcorporausingevaluationmetricsthatappearinprevioustransliterationstudies.Varyingtheevaluationcor-pusinacontrolledfashionhasrevealedseveralin-terestingfacts.WereportthathumanagreementontheEnglishtoPersiantransliterationtaskisabout33%.Theef-fectthatthislevelofdisagreementontheevalua-tionofsystemshas,canbeseeninFigure4,wherewordaccuracyiscomputedoncorporaderivedfromsingletransliterators.Accuracycanvarybyupto30%inabsolutetermsdependingonthetranslitera-torchosen.Toourknowledge,thisistheﬁrstpaperE7D7A7EDA7Corpus0.00.20.40.6Entropy T1T2T3T4T5T6T7Figure5:Entropyofthegeneratedsegmentsbasedonthecollectionscreatedbydifferenttransliterators.toreporthumanagreement,andexamineitseffectsontransliterationaccuracy.Inordertoalleviatesomeoftheseeffectsonthestabilityofwordaccuracymeasuresacrosscorpora,werecommendthatatleastfourtransliteratorsareusedtoconstructacorpus.Figure3showsthatcon-structingacorpuswithfourormoretransliterators,therangeofpossiblewordaccuraciesachievedislessthanthatofusingfewertransliterators.Somepaststudiesdonotusemorethanasin-gletargetwordforeverysourcewordinthecor-pus(BilacandTanaka,2005;OhandChoi,2006).Ourresultsindicatethatitisunlikelythatthesere-sultswouldtranslateontoacorpusotherthantheoneusedinthesestudies,exceptinrarecaseswherehumantransliteratorsarein100%agreementforagivenlanguagepair.GiventhenatureoftheEnglishlanguage,anEn-glishcorpuscancontainEnglishwordsfromavari-etyofdifferentorigins.InthisstudywehaveusedEnglishwordsfromanArabicandDutchorigintoshowthatwordaccuracyofthesystemscanvarybyupto25%(inabsoluteterms)dependingontheori-ginofEnglishwordsinthecorpus,asdemonstratedinFigure1.Inadditiontocomputingagreement,wealsoin-647

RelativePerceptionSYS-1SYS-2Improvement(%)UWAEasy33.455.454.4(p<0.001)Medium44.648.48.52(p<0.001)MWAEasy23.236.256.0(p<0.001)Medium30.637.422.2(p=0.038)Table3:SystemperformancewhenA7issplitintosub-corporabasedontransliteratorsperceptionofthetask(EasyorMedium).vestigatedthetransliterator’sperceptionofdifﬁcultyofthetransliterationtaskwiththeensuingwordac-curacyofthesystems.Interestingly,whenusingcor-porabuiltfromtransliteratorsthatperceivethetasktobeeasy,thereisalargedifferenceinthewordaccuracybetweenthetwosystems,butoncorporabuiltfromtransliteratorswhoperceivethetasktobemoredifﬁcult,thegapbetweenthesystemsnarrows.Hence,acorpusappliedforevaluationoftransliter-ationshouldeitherbemadecarefullywithtranslit-eratorswithavarietyofbackgrounds,orshouldbelargeenoughandbegatheredfromvarioussourcessoastosimulatedifferentexpectationsofitsex-pectednon-homogeneoususers.Theselfentropyofruleprobabilitydistributionsderivedbytheautomatedtransliterationsystemcanbeusedtomeasuretheconsistencywithwhichin-dividualtransliteratorsapplytheirownrulesincon-structingacorpus.Itwasdemonstratedthatwhensystemsareevaluatedoncorporabuiltbytransliter-atorswhoarelessconsistentintheirapplicationoftransliterationrules,wordaccuracyisreduced.Giventhelargevariationsinsystemaccuracythataredemonstratedbythevaryingcorporausedinthisstudy,werecommendthatextremecarebetakenwhenconstructingcorporaforevaluatingtranslitera-tionsystems.Studiesshouldalsogivedetailsoftheircorporathatwouldallowanyoftheeffectsobservedinthispapertobetakenintoaccount.AcknowledgmentsThisworkwassupportedinpartbytheAustraliangovernmentIPRSprogram(SK).ReferencesNasreenAbdulJaleelandLeahS.Larkey.2003.StatisticaltransliterationforEnglish-Arabiccross-languageinforma-tionretrieval.InConferenceonInformationandKnowledgeManagement,pages139–146.YaserAl-OnaizanandKevinKnight.2002.Machinetranslit-erationofnamesinArabictext.InProceedingsoftheACL-02workshoponComputationalapproachestosemiticlan-guages,pages1–13.SlavenBilacandHozumiTanaka.2005.Directcombinationofspellingandpronunciationinformationforrobustback-transliteration.InConferenceonComputationalLinguisticsandIntelligentTextProcessing,pages413–424.PatrickA.V.HallandGeoffR.Dowling.1980.Approximatestringmatching.ACMComputingSurvey,12(4):381–402.SungYoungJung,SungLimHong,andEunokPaek.2000.AnEnglishtoKoreantransliterationmodelofextendedMarkovwindow.InConferenceonComputationalLinguistics,pages383–389.SarvnazKarimi,AndrewTurpin,andFalkScholer.2006.En-glishtoPersiantransliteration.InStringProcessingandIn-formationRetrieval,pages255–266.KristerLind´en.2005.Multilingualmodelingofcross-lingualspellingvariants.InformationRetrieval,9(3):295–310.EunYoungMunandAlexanderVonEye,2004.AnalyzingRaterAgreement:ManifestVariableMethods.LawrenceErlbaumAssociates.Jong-HoonOhandKey-SunChoi.2006.Anensembleoftransliterationmodelsforinformationretrieval.InformationProcessingManagement,42(4):980–1002.KishorePapineni,SalimRoukos,ToddWard,andWei-JingZhu.2002.Bleu:amethodforautomaticevaluationofmachinetranslation.InThe40thAnnualMeetingofAssoci-ationforComputationalLinguistics,pages311–318.AriPirkola,JarmoToivonen,HeikkiKeskustalo,andKalervoJ¨arvelin.2006.FITE-TRT:ahighqualitytranslationtech-niqueforOOVwords.InProceedingsofthe2006ACMSymposiumonAppliedComputing,pages1043–1049.ClaudeElwoodShannon.1948.Amathematicaltheoryofcommunication.BellSystemTechnicalJournal,27:379–423.PaolaVirgaandSanjeevKhudanpur.2003.Transliterationofpropernamesincross-languageapplications.InACMSIGIRConferenceonResearchandDevelopmentonInformationRetrieval,pages365–366.DmitryZelenkoandChinatsuAone.2006.Discriminativemethodsfortransliteration.InProceedingsofthe2006Con-ferenceonEmpiricalMethodsinNaturalLanguageProcess-ing,pages612–617.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 648–655,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

648

CollapsedConsonantandVowelModels:NewApproachesforEnglish-PersianTransliterationandBack-TransliterationSarvnazKarimiFalkScholerAndrewTurpinSchoolofComputerScienceandInformationTechnologyRMITUniversity,GPOBox2476V,Melbourne3001,Australia{sarvnaz,fscholer,aht}@cs.rmit.edu.auAbstractWeproposeanovelalgorithmforEnglishtoPersiantransliteration.Previousmeth-odsproposedforthislanguagepairapplyawordalignmenttoolfortraining.Bycontrast,weintroduceanalignmentalgo-rithmparticularlydesignedfortranslitera-tion.OurnewmodelimprovestheEnglishtoPersiantransliterationaccuracyby14%overann-grambaseline.Wealsoproposeanovelback-transliterationmethodforthislanguagepair,apreviouslyunstudiedprob-lem.Experimentalresultsdemonstratethatouralgorithmleadstoanabsoluteimprove-mentof25%overstandardtransliterationapproaches.1IntroductionTranslationofatextfromasourcelanguagetoatargetlanguagerequiresdealingwithtechnicaltermsandpropernames.Theseoccurinalmostanytext,butrarelyappearinbilingualdictionar-ies.Thesolutionisthetransliterationofsuchout-of-dictionaryterms:awordfromthesourcelanguageistransformedtoawordinthetargetlanguage,pre-servingitspronunciation.Recoveringtheoriginalwordfromthetransliteratedtargetiscalledback-transliteration.Automatictransliterationisimpor-tantformanydifferentapplications,includingma-chinetranslation,cross-lingualinformationretrievalandcross-lingualquestionanswering.Transliterationmethodscanbecategorizedintographeme-based(AbdulJaleelandLarkey,2003;Lietal.,2004),phoneme-based(KnightandGraehl,1998;Jungetal.,2000),andcombined(BilacandTanaka,2005)approaches.Grapheme-basedmeth-odsperformadirectorthographicalmappingbe-tweensourceandtargetwords,whilephoneme-basedapproachesuseanintermediatephoneticrep-resentation.Bothgrapheme-orphoneme-basedmethodsusuallybeginbybreakingthesourcewordintosegments,andthenuseasourcesegmenttotar-getsegmentmappingtogeneratethetargetword.Therulesofthismappingareobtainedbyaligningalreadyavailabletransliteratedwordpairs(trainingdata);alternatively,suchrulescanbehandcrafted.Fromthisperspective,pastworkisroughlydividedintothosemethodswhichapplyawordalignmenttoolsuchasGIZA++(OchandNey,2003),andap-proachesthatcombinethealignmentstepintotheirmaintransliterationprocess.Transliterationislanguagedependent,andmeth-odsthatareeffectiveforonelanguagepairmaynotworkaswellforanother.Inthispaper,weinvestigatetheEnglish-Persiantransliterationprob-lem.Persian(Farsi)isanIndo-Europeanlanguage,writteninArabicscriptfromrighttoleft,butwithanextendedalphabetanddifferentpronunciationfromArabic.OurpreviousapproachtoEnglish-Persiantransliterationintroducedthegrapheme-basedcollapsed-vowelmethod,employingGIZA++forsourcetotargetalignment(Karimietal.,2006).Weproposeanewtransliterationapproachthatex-tendsthecollapsed-vowelmethod.TomeetPer-sianlanguagetransliterationrequirements,wealsoproposeanovelalignmentalgorithminourtrainingstage,whichmakesuseofstatisticalinformationof649

thecorpus,transliterationspeciﬁcations,andsimplelanguageproperties.Thisapproachhandlespossi-bleconsequencesofelision(omissionofsoundstomakethewordeasiertoread)andepenthesis(addingextrasoundstoawordtomakeitﬂuent)inwrittentargetwordsthathappenduetothechangeoflan-guage.Ourmethodshowsanabsoluteaccuracyim-provementof14.2%overann-grambaseline.Inaddition,weinvestigatetheproblemofback-transliterationfromPersiantoEnglish.Toourknowledge,thisistheﬁrstreportofsuchastudy.TherearetwochallengesinPersiantoEnglishtransliterationthatmakesitparticularlydifﬁcult.First,writtenPersianomitsshortvowels,whileonlylongvowelsappearintexts.Second,monophthon-gization(changingdiphthongstomonophthongs)ispopularamongPersianspeakerswhenadaptingfor-eignwordsintotheirlanguage.Totaketheseintoaccount,weproposeanovelmethodtoformtrans-formationrulesbychangingthenormalsegmenta-tionalgorithm.Weﬁndthatthismethodsigniﬁ-cantlyimprovesthePersiantoEnglishtranslitera-tioneffectiveness,demonstratinganabsoluteperfor-mancegainof25.1%overstandardtransliterationapproaches.2BackgroundIngeneral,transliterationconsistsofatrainingstage(runningonabilingualtrainingcorpus),andagen-eration–alsocalledtesting–stage.Thetrainingstepofatransliterationdevelopstransformationrulesmappingcharactersinthesourcetocharactersinthetargetlanguageusingknowledgeofcorrespondingcharactersintranslit-eratedpairsprovidedbyanalignment.Forexample,forthesource-targetwordpair(pat, (cid:18)(cid:16) ),analign-mentmaymap“p”to“ (cid:18)”and“a”to“”,andthetrainingstagemaydeveloptherulepa→,with“”asthetransliterationof“a”inthecontextof“pa”.Thegenerationstageappliestheserulesonaseg-mentedsourceword,transformingittoawordinthetargetlanguage.Previousworkontransliterationeitheremploysawordalignmenttool(usuallyGIZA++),ordevelopsspeciﬁcalignmentstrategies.Transliterationmeth-odsthatuseGIZA++astheirwordpairaligner(Ab-dulJaleelandLarkey,2003;VirgaandKhudanpur,2003;Karimietal.,2006)havebasedtheirworkontheassumptionthattheprovidedalignmentsarere-liable.Gaoetal.(2004)arguethatprecisealign-mentcanimprovetransliterationeffectiveness,ex-perimentingonEnglish-Chinesedataandcompar-ingIBMmodels(Brownetal.,1993)withphoneme-basedalignmentsusingdirectprobabilities.Othertransliterationsystemsfocusonalignmentfortransliteration,forexamplethejointsource-channelmodelsuggestedbyLietal.(2004).TheirmethodoutperformsthenoisychannelmodelindirectorthographicalmappingforEnglish-Chinesetransliteration.Lietal.alsoﬁndthatgrapheme-basedmethodsthatusethejointsource-channelmodelaremoreeffectivethanphoneme-basedmeth-odsduetoremovingtheintermediatephonetictransformationstep.Alignmenthasalsobeenin-vestigatedfortransliterationbyadoptingCoving-ton’salgorithmoncognateidentiﬁcation(Coving-ton,1996);thisisacharacteralignmentalgorithmbasedonmatchingorskippingofcharacters,withamanuallyassignedcostofassociation.Coving-tonconsidersconsonanttoconsonantandvoweltovowelcorrespondencemorevalidthanconsonanttovowel.KangandChoi(2000)revisethismethodfortransliterationwhereaskipisdeﬁnedasinsertinganullinthetargetstringwhentwocharactersdonotmatchbasedontheirphoneticsimilaritiesortheirconsonantandvowelnature.OhandChoi(2002)revisethismethodbyintroducingbinding,inwhichmanytomanycorrespondencesareallowed.How-ever,alloftheseapproachesrelyonthemanuallyassignedpenaltiesthatneedtobedeﬁnedforeachpossiblematching.Inaddition,somerecentstudiesinvestigatedis-criminativetransliterationmethods(KlementievandRoth,2006;ZelenkoandAone,2006)inwhicheachsegmentofthesourcecanbealignedtoeachseg-mentofthetarget,wheresomerestrictiveconditionsbasedonthedistanceofthesegmentsandphoneticsimilaritiesareapplied.3TheProposedAlignmentApproachWeproposeanalignmentmethodbasedonsegmentoccurrencefrequencies,therebyavoidingpredeﬁnedmatchingpatternsandpenaltyassignments.Wealsoapplytheobservedtendencyofaligningconsonants650

toconsonants,andvowelstovowels,asasubsti-tuteforphoneticsimilarities.Manytomany,onetomany,onetonullandmanytoonealignmentscanbegenerated.3.1FormulationOuralignmentapproachconsistsoftwosteps:theﬁrstisbasedontheconsonantandvowelnatureoftheword’sletters,whilethesecondusesafrequency-basedsequentialsearch.Deﬁnition1AbilingualcorpusBistheset{(S,T)},whereS=s1..sℓ,T=t1..tm,siisaletterinthesourcelanguagealphabet,andtjisaletterinthetargetlanguagealphabet.Deﬁnition2Givensomeword,w,theconsonant-vowelsequencep=(C|V)+forwisobtainedbyreplacingeachconsonantwithCandeachvowelwithV.Deﬁnition3Givensomeconsonant-vowelse-quence,p,areducedconsonant-vowelsequenceqreplacesallrunsofC’swithC,andallrunsofV’swithV;henceq=q′|q′′,q′=V(CV)∗(C|ǫ)andq′′=C(VC)∗(V|ǫ).Foreachnaturallanguageword,wecandeterminetheconsonant-vowelsequence(p)fromwhichthereducedconsonant-vowelsequence(q)canbede-rived,givingacommonnotationbetweentwodif-ferentlanguages,nomatterwhichscripteitherofthemuse.Tosimplify,semi-vowelsandapproxi-mants(soundsintermediatebetweenconsonantsandvowels,suchas“w”and“y”inEnglish)aretreatedaccordingtotheirtargetlanguagecounterparts.Ingeneral,forallthewordpairs(S,T)inacorpusB,analignmentcanbeachievedusingthefunctionf:B→A;(S,T)7→(ˆS,ˆT,r).Thefunctionfmapsthewordpair(S,T)∈Btothetriple(ˆS,ˆT,r)∈AwhereˆSandˆTaresub-stringsofSandTrespectively.Thefrequencyofthiscorrespondenceisdenotedbyr.Arepresentsasetofsubstringalignments,andweuseaperwordalignmentnotationofae2pwhenaligningEnglishtoPersianandap2eforPersiantoEnglish.3.2AlgorithmDetailsOuralgorithmconsistsoftwosteps.Step1(Consonant-Vowelbased)Foranywordpair(S,T)∈B,thecorrespondingreducedconsonant-vowelsequences,qSandqT,aregenerated.Ifthesequencesmatch,thenthealignedconsonantclustersandvowelsequencesareaddedtothealignmentsetA.IfqSdoesnotmatchwithqT,thewordpairremainsunalignedinStep1.Theassumptioninthisstepisthattransliterationofeachvowelsequenceofthesourceisavowelse-quenceinthetargetlanguage,andsimilarlyforcon-sonants.However,consonantsdonotalwaysmaptoconsonants,orvowelstovowels(forexample,theEnglishletter“s”maybewrittenas“€”inPersianwhichconsistsofonevowelandoneconsonant).Al-ternatively,theymightbeomittedaltogether,whichcanbespeciﬁedasthenullstring,ε.Wethereforerequireasecondstep.Step2(Frequencybased)Formostnaturallanguages,themaximumlengthofcorrespondingphonemesofeachgraphemeisadigraph(twoletters)oratmostatrigraph.Hence,alignmentcanbedeﬁnedasasearchproblemthatseeksforunitswithamaximumlengthoftwoorthreeinbothstringsthatneedtobealigned.Inourapproach,wesearchbasedonstatisticaloccurrencedataavailablefromStep1.InStep2,onlythosewordsthatremainunalignedattheendofStep1needtobeconsidered.Foreachpairofwords(S,T),matchingproceedsfromlefttoright,examiningoneofthethreepossibleoptionsoftransliteration:singlelettertosingleletter,digraphtosingleletterandsinglelettertodigraph.Trigraphsareunnecessaryinalignmentastheycanbeeffec-tivelycapturedduringtransliterationgeneration,asweexplainbelow.Wedeﬁnefourdifferentvalidalignmentsforthesource(S=s1s2...si...sl)andtarget(T=t1t2...tj...tm)strings:(si,tj,r),(sisi+1,tj,r),(si,tjtj+1,r)and(si,ε,r).Thesefouroptionsareconsideredastheonlypossiblevalidalignments,andthemostfrequentlyoccurringalignment(high-estr)ischosen.Thesefrequenciesaredynamicallyupdatedaftersuccessfullyaligningapair.Forex-ceptionalsituations,wherethereisnocharacterinthetargetstringtomatchwiththesourcecharactersi,itisalignedwiththeemptystring.Itispossiblethatnoneofthefourvalidalignment651

optionshaveoccurredpreviously(thatis,r=0foreach).Thissituationcanariseintwoways:ﬁrst,suchatuplemaysimplynothaveoccurredinthetrainingdata;and,second,thepreviousalign-mentinthecurrentstringpairmayhavebeenincor-rect.Toaccountforthissecondpossibility,apar-tialbacktrackingisconsidered.Mostmisalignmentsarederivedfromthesimultaneouscomparisonofalignmentpossibilities,givingthehighestprioritytothemostfrequent.ForexampleifS=bbc,T= .€andA={(b, .,100),(bb, .,40),(c,€,60)},startingfromtheinitialpositions1andt1,theﬁrstalignmentchoiceis(b, .,101).Howeverimmediatelyafter,wefacetheproblemofaligningthesecond“b”.Therearetwosolutions:insertingεandaddingthetriple(b,ε,1),orbacktrackingthepreviousalignmentandsubstitutingthatwiththelessfrequentbutpossiblealignmentof(bb, .,41).Thesecondsolutionisabetterchoiceasitaddslessambiguousalignmentscontainingε.Attheend,thealignmentsetisup-datedasA={(b, .,100),(bb, .,41),(c,€,61)}.Incaseofequalfrequencies,wecheckpossiblesubsequentalignmentstodecideonwhichalign-mentshouldbechosen.Forexample,if(b, .,100)and(bb, .,100)bothexistaspossibleoptions,weconsiderifchoosingtheformerleadstoasubse-quentεinsertion.Ifso,weoptforthelatter.Attheendofastring,ifjustonecharacterinthetargetstringremainsunalignedwhilethelastalign-mentisaεinsertion,thatﬁnalalignmentwillbesub-stitutedforε.Thisusuallyhappenswhenthealign-mentofﬁnalcharactersisnotyetregisteredinthealignmentset,mainlybecausePersianspeakerstendtotransliteratetheﬁnalvowelstoconsonantstopre-servetheirexistenceintheword.Forexample,intheword“Jose”theﬁnal“e”mightbetransliteratedto“è”whichisaconsonant(“h”)andthereforeisnotcapturedinStep1.BackparsingTheprocessofaligningwordsexplainedabovecanhandlewordswithalreadyknowncomponentsinthealignmentsetA(thefrequencyofoccurrenceisgreaterthanzero).However,whenthisisnotthecase,thesystemmayrepeatedlyinsertεwhilepartorallofthetargetcharactersareleftintact(unsuc-cessfulalignment).Insuchcases,processingthesourceandtargetbackwardshelpstoﬁndtheprob-lematicsubstrings:backparsing.Thepoorlyalignedsubstringsofthesourceandtargetaretakenasnewpairsofstrings,whicharethenreintroducedintothesystemasnewentries.Notethattheythemselvesarenotsubjecttoback-parsing.Moststringsofrepeatingnullscanbebro-kenupthisway,andintheworstcasewillremainasonetupleinthealignmentset.Toclarify,considertheexamplegiveninFigure1.Forthewordpair(patricia, (cid:18)(cid:16) ø(cid:17)€ø),whereanassociationbetween“c”and“(cid:17)€”isnotyetregis-tered.Forwardparsing,asshownintheﬁgure,doesnotresolvealltargetcharacters;aftertheincorrectalignmentof“c”with“ε”,subsequentcharactersarealsoalignedwithnull,andthesubstring“(cid:17)€ø”re-mainsintact.Backwardparsing,showninthenextlineoftheﬁgure,isalsonotsuccessful.Itisabletocorrectlyalignthelasttwocharactersofthestring,beforegeneratingrepeatednullalignments.There-fore,thecentralregion—substringsofthesourceandtargetwhichremainedunalignedplusoneextraalignedsegmenttotheleftandright—isenteredasanewpairtothesystem(ici,ø(cid:17)€ø),asshowninthelinelabelledInput2intheﬁgure.ThisnewinputmeetsStep1requirements,andisalignedsuc-cessfully.TheresultingtuplesarethenmergedwiththealignmentsetA.Anadvantageofourbackparsingstrategyisthatittakescareofcasualtransliterationshappeningduetoelisionandepenthesis(addingorremovingex-trasounds).Itisnotonlyintranslationthatpeoplemayaddextrawordstomakeﬂuenttargettext;fortransliterationalso,itispossiblethatspuriouschar-actersareintroducedforﬂuency.However,thisof-tenfollowspatterns,suchasaddingvowelstothetargetform.Theseirregularitiesareconsistentlycoveredinthebackparsingstrategy,wheretheyre-mainconnectedtotheirpreviouscharacter.4TransliterationMethodTransliterationalgorithmsusealigneddata(theout-putfromthealignmentprocess,ae2porap2ealign-menttuples)fortrainingtoderivetransformationrules.Theserulesarethenusedtogenerateatar-getwordTgivenanewinputsourcewordS.652

Initialalignmentset:A={(p, (cid:18),42),(a,,320),(a,ε,99),(a,ø,10),(a,ø,35),(r,,200),(i,ø,60),(i,ε,5),(c,€,80),(c,h(cid:18),25),(t,(cid:16) ,51)}Input:(patricia, (cid:18)(cid:16) ø(cid:17)€ø)qS=CVCVCVqT=CVCVStep1:qS6=qTForwardalignment:(p, (cid:18),43),(a,ε,100),(t,(cid:16) ,52),(r,,201),(i,ø,61),(c,ε,1),(i,ε,6),(a,ε,100)Backwardalignment:(a,,321),(i,ø,61),(c,ε,1),(i,ε,6),(r,ε,1),(t,ε,1),(a,ε,100),(p,ε,1)Input2:(ici,ø(cid:17)€ø)qS=VCVqT=VCVStep1:(i,ø,61),(c,(cid:17)€,1),(i,ø,61)FinalAlignment:ae2p=((p, (cid:18)),(a,ε),(t,(cid:16) ),((r,),(i,ø),(c,(cid:17)€),(i,ø),(a,))Updatedalignmentset:A={(p, (cid:18),43),(a,,321),(a,ε,100),(a,ø,10),(a,ø,35),(r,,201),(i,ø,62),(i,ε,5),(c,€,80),(c,h(cid:18),25),(c,(cid:17)€,1),(t,(cid:16) ,52)}Figure1:Abackparsingexample.NotemiddletuplesinforwardandbackwardparsingsarenotmergedinAtillthealignmentissuccessfullycompleted.MethodIntermediateSequenceSegment(Pattern)BackoffBigramN/A#s,sh,he,el,ll,le,eys,h,e,l,e,yCV-MODEL1CCVCCVsh(CC),hel(CVC),ll(CC),lley(CV)s(C),h(C),e(V),l(C),e(V),y(V)CV-MODEL2CCVCCVsh(CC),e(CVC),ll(CC),ey(CV)AsAbove.CV-MODEL3CVCV#sh(C),e(CVC),ll(C),ey(CV)sh(C),s(C),h(C),e(V),l(C),e(V),y(V)Figure2:Anexampleoftransliterationforthewordpair(shelley,(cid:17)€Èø).Underlinedcharactersareactuallytransliteratedforeachsegment.4.1BaselineMosttransliterationmethodsreportedinthelitera-ture—eithergrapheme-orphoneme-based—usen-grams(AbdulJaleelandLarkey,2003;Jungetal.,2000).Then-gram-basedmethodsdiffermainlyinthewaythatwordsaresegmented,bothfortrain-ingandtransliterationgeneration.Asimplen-grambasedmethodworksonlyonsinglecharac-ters(unigram)andtransformationrulesaredeﬁnedassi→tj,whileanadvancedmethodmaytakethesurroundingcontextintoaccount(Jungetal.,2000).Wefoundthatusingonepastsymbol(bigrammodel)worksbetterthanothern-grambasedmeth-odsforEnglishtoPersiantransliteration(Karimietal.,2006).Ourcollapsed-vowelmethodsconsiderlanguageknowledgetoimprovethestringsegmentationofn-gramtechniques(Karimietal.,2006).Thepro-cessbeginsbygeneratingtheconsonant-vowelse-quence(Deﬁnition2)ofasourceword.Forex-ample,theword“shelley”isrepresentedbythese-quencep=CCVCCVV.Then,followingthecol-lapsedvowelconcept(Deﬁnition3),thissequencebecomes“CCVCCV”.Theseapproaches,whichwerefertoasCV-MODEL1andCV-MODEL2re-spectively,partitionthesesequencesusingbasicpat-terns(CandV)andmainpatterns(CC,CVC,VCandCV).Inthetrainingphase,transliterationrulesareformedaccordingtotheboundariesofthede-ﬁnedpatternsandtheiralignedcounterparts(basedonae2porap2e)inthetargetlanguagewordT.Simi-larsegmentationisappliedduringthetransliterationgenerationstage.4.2TheProposedTransliterationApproachTherestrictiononthecontextlengthofconsonantsimposedbyCV-MODEL1andCV-MODEL2makesthetransliterationofconsecutiveconsonantsmap-pingtoaparticularcharacterinthetargetlanguagedifﬁcult.Forexample,“ght”inEnglishmapstoonlyonecharacterinPersian:“(cid:16) ”.Dealingwithlanguageswhichhavedifferentalphabets,andforwhichthenumberofcharactersintheiralphabetsalsodiffers(suchas26and32forEnglishandPer-sian),increasesthepossibilityoffacingthesecases,especiallywhenmovingfromthelanguagewithsmalleralphabetsizetotheonewithalargersize.Tomoreeffectivelyaddressthis,weproposeacol-lapsedconsonantandvowelmethod(CV-MODEL3)whichusesthefullreducedsequence(Deﬁnition3),ratherthansimplyreducedvowelsequences.Al-thoughrecognitionofconsonantsegmentsisbasedonthevowelpositions,consonantsareconsideredasindependentblocksineachstring.Conversely,vow-elsaretransliteratedinthecontextofsurrounding653

consonants,asdemonstratedintheexamplebelow.Aspecialsymbolisusedtoindicatethestartand/orendofeachwordifthebeginningandendofthewordisaconsonantrespectively.Therefore,forthewordsstartingorendingwithconsonants,thesymbol“#”isadded,whichistreatedasaconsonantandthereforegroupedintheconsonantsegment.AnexampleofapplyingthistechniqueisshowninFigure2forthestring“shelley”.Inthisexample,“sh”and“ll”aretreatedastwoconsonantsegments,wherethetransliterationofindividualcharactersin-sideasegmentisdependentontheothermembersbutnotthesurroundingsegments.However,thisisnotthecaseforvowelsequenceswhichincorporatealevelofknowledgeaboutanysegmentneighbours.Therefore,fortheexample“shelley”,theﬁrstseg-mentis“sh”whichbelongstoCpattern.Duringtransliteration,if“#sh”doesnotappearinanyex-istingrules,abackoffsplitsthesegmenttosmallersegments:“#”and“sh”,or“s”and“h”.Thesecondsegmentcontainsthevowel“e”.Sincethisvowelissurroundedbyconsonants,thesegmentpatternisCVC.Inthiscase,backoffonlyappliesforvowelsasconsonantsaresupposedtobepartoftheirownin-dependentsegments.Thatis,ifsearchintherulesofpatternCVCwasunsuccessful,itlooksfor“e”inVpattern.Similarly,segmentationforthiswordcon-tinueswith“ll”inCpatternand“ey”inCVpattern(“y”isanapproximant,andthereforeconsideredasavowelwhentransliteratingEnglishtoPersian).4.3RulesforBack-TransliterationWrittenPersianignoresshortvowels,andonlylongvowelsappearintext.ThiscausesmostEnglishvowelstodisappearwhentransliteratingfromEn-glishtoPersian;hence,thesevowelsmustbere-storedduringback-transliteration.WhentheinitialtransliterationhappensfromEn-glishtoPersian,thetransliterator(whetherhu-manormachine)usestherulesoftransliterat-ingfromEnglishasthesourcelanguage.There-fore,transliteratingbacktotheoriginallanguageshouldconsidertheoriginalprocess,toavoidlos-ingessentialinformation.Intermsofsegmenta-tionincollapsed-vowelmodels,differentpatternsdeﬁnesegmentboundariesinwhichvowelsarenecessaryclues.Althoughwedonothavemostofthesevowelsinthetransliterationgenerationphase,itispossibletobeneﬁtfromtheirexistenceinthetrainingphase.Forexample,usingCV-MODEL3,thepair(È,merkel)withqS=Candap2e=((,me),(,r),(,ke),(È,l)),producesjustonetransformationrule“È→merkel”basedonaCpattern.Thatis,thePersianstringcontainsnovowelcharacters.If,duringthetransliterationgen-erationphase,asourceword“É¿	Ó”(S=È)isentered,therewouldbeoneandonlyoneoutputof“merkel”,whileanalternativesuchas“mercle”mightberequiredinstead.Toavoidoverﬁttingthesystembylongconsonantclusters,weperformseg-mentationbasedontheEnglishqsequence,butcate-gorisetherulesbasedontheirPersiansegmentcoun-terparts.Thatis,forthepair(È,merkel)withae2p=((m,),(e,ε),(r,),(k,),(e,ε),(l,È)),theserulesaregenerated(withcategorypatternsgiveninparen-thesis):→m(C),→rk(C),È→l(C),→merk(C),È→rkel(C).Wecallthesuggestedtrainingapproachreversesegmentation.Reversesegmentationavoidsclusteringalltheconsonantsinonerule,sincemanyEnglishwordsmightbetransliteratedtoall-consonantPersianwords.4.4TransliterationGenerationandRankingInthetransliterationgenerationstage,thesourcewordissegmentedfollowingthesameprocessofsegmentingwordsintrainingstage,andaprobabil-ityiscomputedforeachgeneratedtargetword:P(T|S)=|K|Yk=1P(ˆTk|ˆSk),where|K|isthenumberofdistinctsourceseg-ments.P(ˆTk|ˆSk)istheprobabilityoftheˆSk→ˆTktransformationrule,asobtainedfromthetrainingstage:P(ˆTk|ˆSk)=frequencyofˆSk→ˆTkfrequencyofˆSk,wherefrequencyofˆSkisthenumberofitsoc-currenceinthetransformationrules.Weapplyatreestructure,followingDijkstra’sα-shortestpath,togeneratetheαhighestscoring(mostprobable)transliterations,rankedbasedontheirprobabilities.654

CorpusBaselineCV-MODEL3BigramCV-MODEL1CV-MODEL2GIZA++NewAlignmentSmallCorpusTOP-158.0(2.2)61.7(3.0)60.0(3.9)67.4(5.5)72.2(2.2)TOP-585.6(3.4)80.9(2.2)86.0(2.8)90.9(2.1)92.9(1.6)TOP-1089.4(2.9)82.0(2.1)91.2(2.5)93.8(2.1)93.5(1.7)LargeCorpusTOP-147.2(1.0)50.6(2.5)47.4(1.0)55.3(0.8)59.8(1.1)TOP-577.6(1.4)79.8(3.4)79.2(1.0)84.5(0.7)85.4(0.8)TOP-1083.3(1.5)84.9(3.1)87.0(0.9)89.5(0.4)92.6(0.7)Table1:Mean(standarddeviation)wordaccuracy(%)forEnglishtoPersiantransliteration.5ExperimentsToinvestigatetheeffectivenessofCV-MODEL3andthenewalignmentapproachontransliteration,weﬁrstcompareCV-MODEL3withbaselinesystems,employingGIZA++foralignmentgenerationduringsystemtraining.Wethenevaluatethesamesys-tems,usingournewalignmentapproach.Back-transliterationisalsoinvestigated,applyingbothalignmentsystemsandreversesegmentation.Inallourexperiments,weusedten-foldcross-validation.Thestatisticalsigniﬁcanceofdifferentperformancelevelsareevaluatedusingapairedt-test.Theno-tationTOP-XindicatestheﬁrstXtransliterationsprodcuedbytheautomaticmethods.WeusedtwocorporaofwordpairsinEnglishandPersian:theﬁrst,calledLarge,contains16,670wordpairs;thesecond,Small,contains1,857wordpairs,andaredescribedfullyinourpreviouspaper(Karimietal.,2006).Theresultsoftransliterationexperimentsareeval-uatedusingwordaccuracy(KangandChoi,2000)whichmeasurestheproportionoftransliterationsthatarecorrectoutofthetestcorpus.5.1AccuracyofTransliterationApproachesTheresultsofourexperimentsfortransliteratingEn-glishtoPersian,usingGIZA++foralignmentgen-eration,areshowninTable1.CV-MODEL3out-performsallthreebaselinesystemssigniﬁcantlyinTOP-1andTOP-5results,forbothPersiancorpora.TOP-1resultswereimprovedby9.2%to16.2%(p<0.0001,pairedt-test)relativetothebaselinesys-temsfortheSmallcorpus.FortheLargecorpus,CV-MODEL3was9.3%to17.2%(p<0.0001)moreaccuraterelativetothebaselinesystems.Theresultsofapplyingournewalignmental-gorithmarepresentedinthelastcolumnofTa-ble1,comparingwordaccuracyofCV-MODEL3us-ingGIZA++andthenewalignmentforEnglishtoPersiantransliteration.Transliterationaccuracyin-creasesinTOP-1forbothcorpora(arelativeincreaseof7.1%(p=0.002)fortheSmallcorpusand8.1%(p<0.0001)fortheLargecorpus).TheTOP-10re-sultsoftheLargecorpusagainshowarelativein-creaseof3.5%(p=0.004).Althoughthenewalign-mentalsoincreasestheperformanceforTOP-5andTOP-10oftheSmallcorpus,theseincreasesarenotstatisticallysigniﬁcant.5.2AccuracyofBack-TransliterationTheresultsofback-transliterationareshowninTa-ble2.WeﬁrstconsiderperformanceimprovementsgainedfromusingCV-MODEL3:CV-MODEL3usingGIZA++outperformsBigram,CV-MODEL1andCV-MODEL2by12.8%to40.7%(p<0.0001)inTOP-1fortheSmallcorpus.Thecorrespondingim-provementfortheLargecorpusis12.8%to74.2%(p<0.0001).Theﬁfthcolumnofthetableshowstheperfor-manceincreasewhenusingCV-MODEL3withthenewalignmentalgorithm:fortheLargecorpus,thenewalignmentapproachgivesarelativeincreaseinaccuracyof15.5%forTOP-5(p<0.0001)and10%forTOP-10(p=0.005).ThenewalignmentmethoddoesnotshowasigniﬁcantdifferenceusingCV-MODEL3fortheSmallcorpus.TheﬁnalcolumnofTable2showstheperfor-manceoftheCV-MODEL3withthenewreverseseg-mentationapproach.ReversesegmentationleadstoasigniﬁcantimprovementoverthenewalignmentapproachinTOP-1resultsfortheSmallcorpusby40.1%(p<0.0001),and49.4%(p<0.0001)fortheLargecorpus.655

CorpusBigramCV-MODEL1CV-MODEL2CV-MODEL3GIZA++NewAlignmentReverseSmallCorpusTOP-123.1(2.0)28.8(4.6)24.9(2.8)32.5(3.6)34.4(3.8)48.2(2.9)TOP-540.8(3.1)51.0(4.8)52.9(3.4)56.0(3.5)54.8(3.7)68.1(4.9)TOP-1050.1(4.1)58.2(5.3)63.2(3.1)64.2(3.2)63.8(3.6)75.7(4.2)LargeCorpusTOP-110.1(0.6)15.6(1.0)12.0(1.0)17.6(0.8)18.0(1.2)26.9(0.7)TOP-520.6(1.2)31.7(0.9)28.0(0.7)36.2(0.5)41.8(1.2)41.3(1.7)TOP-1027.2(1.0)40.1(1.1)37.4(0.8)46.0(0.8)50.6(1.1)49.3(1.6)Table2:Comparisonofmean(standarddeviation)wordaccuracy(%)forPersiantoEnglishtransliteration.6ConclusionsWehavepresentedanewalgorithmforEnglishtoPersiantransliteration,andanovelalignmental-gorithmapplicablefortransliteration.Ournewtransliterationmethod(CV-MODEL3)outperformsthepreviousapproachesforEnglishtoPersian,in-creasingwordaccuracybyarelative9.2%to17.2%(TOP-1),whenusingGIZA++foralignmentintrain-ing.Thismethodshowsfurther7.1%to8.1%in-creaseinwordaccuracy(TOP-1)withournewalign-mentalgorithm.PersiantoEnglishback-transliterationisalsoin-vestigated,withCV-MODEL3signiﬁcantlyoutper-formingothermethods.Enrichingthismodelwithanewreversesegmentationalgorithmgivesrisetofurtheraccuracygainsincomparisontodirectlyap-plyingEnglishtoPersianmethods.Infutureworkwewillinvestigatewhetherpho-neticinformationcanhelpreﬁneourCV-MODEL3,andexperimentwithmanuallyconstructedrulesasabaselinesystem.AcknowledgmentsThisworkwassupportedinpartbytheAustraliangovernmentIPRSprogram(SK)andanARCDis-coveryProjectGrant(AT).ReferencesNasreenAbdulJaleelandLeahS.Larkey.2003.StatisticaltransliterationforEnglish-Arabiccrosslanguageinforma-tionretrieval.InConferenceonInformationandKnowledgeManagement,pages139–146.SlavenBilacandHozumiTanaka.2005.Directcombinationofspellingandpronunciationinformationforrobustback-transliteration.InConferencesonComputationalLinguis-ticsandIntelligentTextProcessing,pages413–424.PeterF.Brown,VincentJ.DellaPietra,StephenA.DellaPietra,andRobertL.Mercer.1993.Themathematicsofstatisti-calmachinetranslation:Parameterestimation.ComputionalLinguistics,19(2):263–311.MichaelA.Covington.1996.Analgorithmtoalignwordsforhistoricalcomparison.ComputationalLinguistics,22(4):481–496.WeiGao,Kam-FaiWong,andWaiLam.2004.Improvingtransliterationwithprecisealignmentofphonemechunksandusingcontextualfeatures.InAsiaInformationRetrievalSymposium,pages106–117.SungYoungJung,SungLimHong,andEunokPaek.2000.AnEnglishtoKoreantransliterationmodelofextendedMarkovwindow.InConferenceonComputationalLinguistics,pages383–389.Byung-JuKangandKey-SunChoi.2000.Automatictranslit-erationandback-transliterationbydecisiontreelearning.InConferenceonLanguageResourcesandEvaluation,pages1135–1411.SarvnazKarimi,AndrewTurpin,andFalkScholer.2006.En-glishtoPersiantransliteration.InStringProcessingandIn-formationRetrieval,pages255–266.AlexandreKlementievandDanRoth.2006.Weaklysuper-visednamedentitytransliterationanddiscoveryfrommul-tilingualcomparablecorpora.InAssociationforComputa-tionalLinguistics,pages817–824.KevinKnightandJonathanGraehl.1998.Machinetranslitera-tion.ComputationalLinguistics,24(4):599–612.HaizhouLi,MinZhang,andJianSu.2004.Ajointsource-channelmodelformachinetransliteration.InAssociationforComputationalLinguistics,pages159–166.FranzJosefOchandHermannNey.2003.Asystematiccom-parisonofvariousstatisticalalignmentmodels.Computa-tionalLinguistics,29(1):19–51.Jong-HoonOhandKey-SunChoi.2002.AnEnglish-Koreantransliterationmodelusingpronunciationandcontextualrules.InConferenceonComputationalLinguistics.PaolaVirgaandSanjeevKhudanpur.2003.Transliterationofpropernamesincross-languageapplications.InACMSIGIRConferenceonResearchandDevelopmentonInformationRetrieval,pages365–366.DmitryZelenkoandChinatsuAone.2006.Discriminativemethodsfortransliteration.InProceedingsofthe2006Con-ferenceonEmpiricalMethodsinNaturalLanguageProcess-ing.,pages612–617.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 656–663,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

656

Alignment-BasedDiscriminativeStringSimilarityShaneBergsmaandGrzegorzKondrakDepartmentofComputingScienceUniversityofAlbertaEdmonton,Alberta,Canada,T6G2E8{bergsma,kondrak}@cs.ualberta.caAbstractAcharacter-basedmeasureofsimilarityisanimportantcomponentofmanynatu-rallanguageprocessingsystems,includingapproachestotransliteration,coreference,wordalignment,spellingcorrection,andtheidenti(cid:2)cationofcognatesinrelatedvocabu-laries.Weproposeanalignment-baseddis-criminativeframeworkforstringsimilarity.Wegatherfeaturesfromsubstringpairscon-sistentwithacharacter-basedalignmentofthetwostrings.Thisapproachachievesexceptionalperformance;onnineseparatecognateidenti(cid:2)cationexperimentsusingsixlanguagepairs,wemorethandoublethepre-cisionoftraditionalorthographicmeasureslikeLongestCommonSubsequenceRatioandDice’sCoef(cid:2)cient.Wealsoshowstrongimprovementsoverotherrecentdiscrimina-tiveandheuristicsimilarityfunctions.1IntroductionStringsimilarityisoftenusedasameansofquan-tifyingthelikelihoodthattwopairsofstringshavethesameunderlyingmeaning,basedpurelyonthecharactercompositionofthetwowords.Strubeetal.(2002)useEditDistanceasafeatureforde-terminingiftwowordsarecoreferent.Taskaretal.(2005)useFrench-Englishcommonletterse-quencesasafeaturefordiscriminativewordalign-mentinbilingualtexts.BrillandMoore(2000)learnmisspelled-wordtocorrectly-spelled-wordsimilari-tiesforspellingcorrection.Ineachoftheseexam-ples,asimilaritymeasurecanmakeuseoftherecur-rentsubstringpairingsthatreliablyoccurbetweenwordshavingthesamemeaning.Acrossnaturallanguages,theserecurrentsub-stringcorrespondencesarefoundinwordpairsknownascognates:wordswithacommonformandmeaningacrosslanguages.Cognatesariseei-therfromwordsinacommonancestorlanguage(e.g.light/Licht,night/NachtinEnglish/German)orfromforeignwordborrowings(e.g.trampo-line/toranporininEnglish/Japanese).Knowledgeofcognatesisusefulforanumberofapplications,in-cludingsentencealignment(Melamed,1999)andlearningtranslationlexicons(MannandYarowsky,2001;KoehnandKnight,2002).Weproposeanalignment-based,discriminativeapproachtostringsimilarityandevaluatethisap-proachoncognateidenti(cid:2)cation.Section2de-scribespreviousapproachesandtheirlimitations.InSection3,weexplainourtechniqueforautomati-callycreatingacognate-identi(cid:2)cationtrainingset.Anovelaspectofthissetistheinclusionofcompetitivecounter-examplesforlearning.Section4showshowdiscriminativefeaturesarecreatedfromacharacter-based,minimum-edit-distancealignmentofapairofstrings.InSection5,wedescribeourbitextanddictionary-basedexperimentsonsixlanguagepairs,includingthreebasedonnon-Romanalphabets.InSection6,weshowsigni(cid:2)cantimprovementsovertraditionalapproaches,aswellassigni(cid:2)cantgainsovermorerecenttechniquesbyRistadandYiani-los(1998),Tiedemann(1999),Kondrak(2005),andKlementievandRoth(2006).2RelatedWorkStringsimilarityisafundamentalconceptinava-rietyof(cid:2)eldsandhencearangeoftechniques657

havebeendeveloped.Wefocusonapproachesthathavebeenappliedtowords,i.e.,uninterruptedsequencesofcharactersfoundinnaturallanguagetext.Themostwell-knownmeasureofthesimi-larityoftwostringsistheEditDistanceorLev-enshteinDistance(Levenshtein,1966):thenumberofinsertions,deletionsandsubstitutionsrequiredtotransformonestringintoanother.Inourexperi-ments,weuseNormalizedEditDistance(NED):EditDistancedividedbythelengthofthelongerword.OtherpopularmeasuresincludeDice’sCoef-(cid:2)cient(DICE)(AdamsonandBoreham,1974),andthelength-normalizedmeasuresLongestCommonSubsequenceRatio(LCSR)(Melamed,1999),andLongestCommonPre(cid:2)xRatio(PREFIX)(Kondrak,2005).Thesebaselineapproacheshavetheimpor-tantadvantageofnotrequiringtrainingdata.Wecanalsoincludeinthenon-learningcategoryKon-drak(2005)’sLongestCommonSubsequenceFor-mula(LCSF),aprobabilisticmeasuredesignedtomitigateLCSR’spreferenceforshorterwords.Althoughsimpletouse,theuntrainedmeasurescannotadapttothespeci(cid:2)cspellingdifferencesbe-tweenapairoflanguages.Researchershavethere-foreinvestigatedadaptivemeasuresthatarelearnedfromasetofknowncognatepairs.RistadandYiani-los(1998)developedastochastictransducerversionofEditDistancelearnedfromunalignedstringpairs.MannandYarowsky(2001)sawlittleimprovementoverEditDistancewhenapplyingthistransducertocognates,evenwhen(cid:2)lteringthetransducer’sproba-bilitiesintodifferentweightclassestobetterapprox-imateEditDistance.Tiedemann(1999)usedvariousmeasurestolearntherecurrentspellingchangesbe-tweenEnglishandSwedish,andusedthesechangestore-weightLCSRtoidentifymorecognates,withmodestperformanceimprovements.MulloniandPekar(2006)developedasimilartechniquetoim-proveNEDforEnglish/German.Essentially,allthesetechniquesimproveonthebaselineapproachesbyusingasetofpositive(true)cognatepairstore-weightthecostsofeditop-erationsorthescoreofsequencematches.Ide-ally,wewouldpreferamore(cid:3)exibleapproachthatcanlearnpositiveornegativeweightsonsubstringpairingsinordertobetteridentifyrelatedstrings.Onesystemthatcanpotentiallyprovidethis(cid:3)exi-bilityisadiscriminativestring-similarityapproachtonamed-entitytransliterationbyKlementievandRoth(2006).Althoughnotcomparedtoothersimi-laritymeasuresintheoriginalpaper,weshowthatthisdiscriminativetechniquecanstronglyoutper-formtraditionalmethodsoncognateidenti(cid:2)cation.Unlikemanyrecentgenerativesystems,theKle-mentievandRothapproachdoesnotexploittheknownpositionsinthestringswherethecharactersmatch.Forexample,BrillandMoore(2000)com-bineacharacter-basedalignmentwiththeExpec-tationMaximization(EM)algorithmtodevelopanimprovedprobabilisticerrormodelforspellingcor-rection.RappoportandLevent-Levi(2006)applythisapproachtolearnsubstringcorrespondencesforcognates.ZelenkoandAone(2006)recentlyshowedaKlementievandRoth(2006)-stylediscriminativeapproachtobesuperiortoalignment-basedgenera-tivetechniquesfornametransliteration.Ourworksuccessfullyusesthealignment-basedmethodologyofthegenerativeapproachestoenhancethefeaturesetfordiscriminativestringsimilarity.3TheCognateIdentiﬁcationTaskGiventwostringlists,EandF,thetaskofcog-nateidenti(cid:2)cationisto(cid:2)ndallpairsofstrings(e,f)thatarecognate.Inothersimilarity-drivenapplica-tions,EandFcouldbemisspelledandcorrectlyspelledwords,ortheorthographicandthephoneticrepresentationofwords,etc.ThetaskremainstolinkstringswithcommonmeaninginEandFus-ingonlythestringsimilaritymeasure.Wecanfacilitatetheapplicationofstringsimi-laritytocognatesbyusingade(cid:2)nitionofcognationnotdependentonetymologicalanalysis.Forex-ample,MannandYarowsky(2001)de(cid:2)neawordpair(e,f)tobecognateiftheyareatranslationpair(samemeaning)andtheirEditDistanceislessthanthree(sameform).Weadoptanimprovedde(cid:2)nition(suggestedbyMelamed(1999)fortheFrench-EnglishCanadianHansards)thatdoesnotover-proposeshorterwordpairs:(e,f)arecog-nateiftheyaretranslationsandtheirLCSR(cid:21)0.58.Notethatthiscutoffissomewhatconser-vative:theEnglish/Germancognateslight/Licht(LCSR=0.8)areincluded,butnotthecognateseight/acht(LCSR=0.4).IftwowordsmusthaveLCSR(cid:21)0.58tobecog-658

ForeignLanguageFWordsf2FCognatesEf+FalseFriendsEf−Japanese(R(cid:136)omaji)napukinnapkinnanking,pumpkin,snacking,sneakingFrenchabondammentabundantlyabandonment,abatement,...wondermentGermanprozyklischeprocyclicalpolished,prophylactic,prophylaxisTable1:Foreign-Englishcognatesandfalsefriendtrainingexamples.nate,thenforagivenwordf2F,weneedonlyconsideraspossiblecognatesthesubsetofwordsinEhavinganLCSRwithflargerthan0.58,asetwecallEf.TheportionofEfwiththesamemeaningasf,Ef+,arecognates,whilethepartwithdiffer-entmeanings,Ef−,arenotcognates.ThewordsEf−withsimilarspellingbutdifferentmeaningaresometimescalledfalsefriends.Thecognateidenti-(cid:2)cationtaskis,foreverywordf2F,andalistofsimilarlyspelledwordsEf,todistinguishthecog-natesubsetEf+fromthefalsefriendsetEf−.Tocreatetrainingdataforourlearningap-proaches,andtogenerateahigh-qualitylabelledtestset,weneedtoannotatesomeofthe(f,ef2Ef)wordpairsforwhetherornotthewordsshareacommonmeaning.InSection5,weexplainourtwohigh-precisionautomaticannotationmethods:checkingifeachpairofwords(a)werealignedinaword-alignedbitext,or(b)werelistedastransla-tionpairsinabilingualdictionary.Table1providessomelabelledexampleswithnon-emptycognateandfalsefriendlists.Notethatdespitetheseexamples,thisisnotarankingtask:eveninhighlyrelatedlanguages,mostwordsinFhaveemptyEf+lists,andmanyhaveemptyEf−aswell.Thusonenaturalformulationforcognateidenti(cid:2)cationisapairwise(andsymmetric)cogna-tionclassi(cid:2)cationthatlooksateachpair(f,ef)sep-aratelyandindividuallymakesadecision:+(napukin,napkin)–(napukin,nanking)–(napukin,pumpkin)Inthisformulation,thebene(cid:2)tsofadiscrimina-tiveapproachareclear:itmust(cid:2)ndsubstringsthatdistinguishcognatepairsfromwordpairswithoth-erwisesimilarform.KlementievandRoth(2006),althoughusingadiscriminativeapproach,donotprovidetheirin(cid:2)nite-attributeperceptronwithcom-petitivecounter-examples.Theyinsteadusetranslit-erationsaspositivesandrandomly-pairedEnglishandRussianwordsasnegativeexamples.Inthefol-lowingsection,wealsoimproveonKlementievandRoth(2006)byusingacharacter-basedstringalign-menttofocusthefeaturesfordiscrimination.4FeaturesforDiscriminativeSimilarityDiscriminativelearningworksbyprovidingatrain-ingsetoflabelledexamples,eachrepresentedasasetoffeatures,toamodulethatlearnsaclassi(cid:2)er.Intheprevioussectionweshowedhowlabelledwordpairscanbecollected.Wenowaddressmethodsofrepresentingthesewordpairsassetsoffeaturesuse-fulfordeterminingcognation.ConsidertheR(cid:136)omajiJapanese/Englishcognates:(sutoresu,stress).TheLCSRis0.625.NotethattheLCSRofsutoresuwiththeEnglishfalsefriendsto-riesishigher:0.75.LCSRaloneistooweakafea-turetopickoutcognates.Weneedtolookattheactualcharactersubstrings.KlementievandRoth(2006)generatefeaturesforapairofwordsbysplittingbothwordsintoallpos-siblesubstringsofuptosizetwo:sutoresu)fs,u,t,o,r,e,s,u,su,ut,to,...sugstress)fs,t,r,e,s,s,st,tr,re,es,ssgThen,afeaturevectorisbuiltfromallsubstringpairsfromthetwowordssuchthatthedifferenceinposi-tionsofthesubstringsiswithinone:fs-s,s-t,s-st,su-s,su-t,su-st,su-tr...r-s,r-s,r-es...gThisfeaturevectorprovidesthefeaturerepresenta-tionusedinsupervisedmachinelearning.ThisexamplealsohighlightsthelimitationsoftheKlementievandRothapproach.Thelearnercanpro-videweighttofeatureslikes-sors-statthebegin-ningoftheword,butbecauseofthegradualaccu-mulationofpositionaldifferences,thelearnerneverseesthetor-trandes-escorrespondencesthatreallyhelpindicatethewordsarecognate.Oursolutionistousetheminimum-edit-distancealignmentofthetwostringsasthebasisforfea-tureextraction,ratherthanthepositionalcorrespon-dences.Wealsoincludebeginning-of-word((cid:136))andend-of-word($)markers(referredtoasboundary659

markers)tohighlightcorrespondencesatthosepo-sitions.Thepair(sutoresu,stress)canbealigned:Forthefeaturerepresentation,weonlyextractsub-stringpairsthatareconsistentwiththisalignment.1Thatis,thelettersinourpairscanonlybealignedtoeachotherandnottolettersoutsidethepairing:fˆ-ˆ,ˆs-ˆs,s-s,su-s,ut-t,t-t,...es-es,s-s,su-ss...gWede(cid:2)nephrasepairstobethepairsofsubstringsconsistentwiththealignment.Asimilaruseoftheterm(cid:147)phrase(cid:148)existsinmachinetranslation,wherephrasesareoftenpairsofwordsequencesconsistentwithword-basedalignments(Koehnetal.,2003).Bylimitingthesubstringstoonlythosepairsthatareconsistentwiththealignment,wegener-atefewer,more-informativefeatures.UsingmoreprecisefeaturesallowsalargermaximumsubstringsizeLthanisfeasiblewiththepositionalapproach.Largersubstringsallowustocaptureimportantre-curringdeletionslikethe(cid:147)u(cid:148)insut-st.Tiedemann(1999)andothershaveshowntheim-portanceofusingthemismatchingportionsofcog-natepairstolearntherecurrentspellingchangesbe-tweentwolanguages.Inordertocapturemismatch-ingsegmentslongerthanourmaximumsubstringsizewillallow,weincludespecialfeaturesinourrepresentationcalledmismatches.Mismatchesarephrasesthatspantheentiresequenceofunalignedcharactersbetweentwopairsofalignedendchar-acters(similartothe(cid:147)rules(cid:148)extractedbyMulloniandPekar(2006)).Intheaboveexample,su$-ss$isamismatchwith(cid:147)s(cid:148)and(cid:147)$(cid:148)asthealignedendcharacters.Twosetsoffeaturesaretakenfromeachmismatch,onethatincludesthebeginning/endingalignedcharactersascontextandonethatdoesnot.Forexample,fortheendingsoftheFrench/Englishpair(´economique,economic),weincludeboththesubstringpairsique$:ic$andque:casfeatures.Oneconsiderationiswhethersubstringfeaturesshouldbebinarypresence/absence,orthecountofthefeatureinthepairnormalizedbythelengthofthelongerword.Weinvestigatebothoftheseap-1Ifthewordsarefromdifferentalphabets,wecangetthealignmentbymappingtheletterstotheirclosestRomanequiv-alent,orbyusingtheEMalgorithmtolearntheedits(RistadandYianilos,1998).proachesinourexperiments.Also,thereisnorea-sonnottoincludethescoresofbaselineapproacheslikeNED,LCSR,PREFIXorDICEasfeaturesintherepresentationaswell.Featureslikethelengthsofthetwowordsandthedifferenceinlengthsofthewordshavealsoprovedtobeusefulinpreliminaryexperiments.Semanticfeatureslikefrequencysimi-larityorcontextualsimilaritymightalsobeincludedtohelpdeterminecognationbetweenwordsthatarenotpresentinatranslationlexiconorbitext.5ExperimentsSection3introducedtwohigh-precisionmethodsforgeneratinglabelledcognatepairs:usingthewordalignmentsfromabilingualcorpusorusingtheen-triesinatranslationlexicon.Weinvestigatebothofthesemethodsinourexperiments.Ineachcase,wegeneratesetsoflabelledwordpairsfortraining,test-ing,anddevelopment.Theproportionofpositiveex-amplesinthebitext-labelledtestsetsrangebetween1.4%and1.8%,whilerangingbetween1.0%and1.6%forthedictionarydata.2Forthediscriminativemethods,weuseapopu-larSupportVectorMachine(SVM)learningpack-agecalledSVMlight(Joachims,1999).SVMsaremaximum-marginclassi(cid:2)ersthatachievegoodper-formanceonarangeoftasks.Ineachcase,welearnalinearkernelonthetrainingsetpairsandtunetheparameterthattrades-offtrainingerrorandmarginonthedevelopmentset.Weapplyourclassi-(cid:2)ertothetestsetandscorethepairsbytheirpos-itivedistancefromtheSVMclassi(cid:2)cationhyper-plane(alsodonebyBilenkoandMooney(2003)withtheirtoken-basedSVMsimilaritymeasure).Wealsoscorethetestsetsusingtraditionalortho-graphicsimilaritymeasuresPREFIX,DICE,LCSR,andNED,anaverageofthesefour,andKondrak(2005)’sLCSF.Wealsousethelogoftheeditprob-abilityfromthestochasticdecoderofRistadandYianilos(1998)(normalizedbythelengthofthelongerword)andTiedemann(1999)’shighestper-formingsystem(Approach#3).Bothuseonlythepositiveexamplesinourtrainingset.Ourevaluationmetricis11-ptaverageprecisiononthescore-sortedpairlists(alsousedbyKondrakandSherif(2006)).2Thecognatedatasetsusedinourexperimentsareavailableathttp://www.cs.ualberta.ca/(cid:152)bergsma/Cognates/660

5.1BitextExperimentsForthebitext-basedannotation,weusepublicly-availablewordalignmentsfromtheEuroparlcorpus,automaticallygeneratedbyGIZA++forFrench-English(Fr),Spanish-English(Es)andGerman-English(De)(KoehnandMonz,2006).Initialclean-ingofthesenoisywordpairsisnecessary.Wethusremoveallpairswithnumbers,punctuation,acapi-talizedEnglishword,andallwordsthatoccurfewerthantentimes.Wealsoremovemanyincorrectlyalignedwordsby(cid:2)lteringpairswherethepairwiseMutualInformationbetweenthewordsislessthan7.5.Thisprocessingleavesvocabularysizesof39KforFrench,31KforSpanish,and60KforGerman.OurlabelledsetisthengeneratedfrompairswithLCSR(cid:21)0.58(usingthecutofffromMelamed(1999)).Eachlabelledsetentryisatripleofa)theforeignwordf,b)thecognatesEf+andc)thefalsefriendsEf−.Foreachlanguagepair,werandomlytake20Ktriplesfortraining,5Kfordevelopmentand5Kfortesting.Eachtripleisconvertedtoasetofpairwiseexamplesforlearningandclassi(cid:2)cation.5.2DictionaryExperimentsForthedictionary-basedcognateidenti(cid:2)cation,weuseFrench,Spanish,German,Greek(Gr),Japanese(Jp),andRussian(Rs)toEnglishtranslationpairsfromtheFreelangprogram.3Thelatterthreepairswerechosensothatwecanevaluateonmoredistantlanguagesthatusenon-Romanalphabets(althoughtheR(cid:136)omajiJapaneseisRomanizedbyde(cid:2)nition).Wetake10Klabelled-settriplesfortraining,2Kfortestingand2Kfordevelopment.Thebaselineapproachesandourde(cid:2)nitionofcognationrequirecomparisoninacommonalpha-bet.Thusweuseasimplecontext-freemappingtoconverteveryRussianandGreekcharacterinthewordpairstotheirnearestRomanequivalent.WethenlabelatranslationpairascognateiftheLCSRbetweenthewords’Romanizedrepresentationsisgreaterthan0.58.Wealsooperateallofourcom-parisonsystemsontheseRomanizedpairs.6ResultsWewereinterestedinwhetherourworkingde(cid:2)ni-tionofcognation(translationsandLCSR(cid:21)0.58)3http://www.freelang.net/dictionary/Figure1:LCSRhistogramandpolynomialtrendlineofFrench-Englishdictionarypairs.SystemPrecKlementiev-Roth(KR)L(cid:20)258.6KRL(cid:20)2(normalized,boundarymarkers)62.9phrasesL(cid:20)261.0phrasesL(cid:20)365.1phrasesL(cid:20)3+mismatches65.6phrasesL(cid:20)3+mismatches+NED65.8Table2:BitextFrench-Englishdevelopmentsetcog-nateidenti(cid:2)cation11-ptaverageprecision(%).re(cid:3)ectstrueetymologicalrelatedness.WelookedattheLCSRhistogramfortranslationpairsinoneofourtranslationdictionaries(Figure1).Thetrendlinesuggestsabimodaldistribution,withtwodistinctdistributionsoftranslationpairsmakingupthedic-tionary:incidentalletteragreementgiveslowLCSRforthelarger,non-cognateportionandhighLCSRcharacterizesthelikelycognates.Athresholdof0.58capturesmostofthecognatedistributionwhileexcludingnon-cognatepairs.Thishypothesiswascon(cid:2)rmedbycheckingtheLCSRvaluesofalistofknownFrench-Englishcognates(randomlycol-lectedfromadictionaryforanotherproject):87.4%wereabove0.58.Wealsocheckedcognationon100randomly-sampled,positively-labelledFrench-Englishpairs(i.e.translatedoralignedandhavingLCSR(cid:21)0.58)fromboththedictionaryandbitextdata.100%ofthedictionarypairsand93%ofthebitextpairswerecognate.Next,weinvestigatevariouscon(cid:2)gurationsofthediscriminativesystemsononeofourcognateiden-ti(cid:2)cationdevelopmentsets(Table2).Theorigi-nalKlementievandRoth(2006)(KR)systemcan661

BitextDictionarySystemFrEsDeFrEsDeGrJpRsPREFIX34.727.336.345.534.725.528.516.129.8DICE33.728.233.544.333.721.330.620.133.6LCSR34.028.728.548.336.518.430.224.236.6NED36.531.932.350.140.323.333.928.241.4PREFIX+DICE+LCSR+NED38.731.839.351.640.128.633.722.937.9Kondrak(2005):LCSF29.828.929.139.936.625.030.533.445.5Ristad&Yanilos(1998)37.732.534.656.146.936.938.052.751.8Tiedemann(1999)38.833.034.755.349.024.937.633.945.8Klementiev&Roth(2006)61.155.553.273.462.348.351.462.064.4Alignment-BasedDiscriminative66.563.264.177.772.165.665.782.076.9Table3:Bitext,DictionaryForeign-to-Englishcognateidenti(cid:2)cation11-ptaverageprecision(%).beimprovedbynormalizingthefeaturecountbythelongerstringlengthandincludingthebound-arymarkers.Thisisthereforedonewithallthealignment-basedapproaches.Also,becauseofthewayitsfeaturesareconstructed,theKRsystemislimitedtoamaximumsubstringlengthoftwo(L(cid:20)2).Amaximumlengthofthree(L(cid:20)3)intheKRframeworkproducesmillionsoffeaturesandpro-hibitivetrainingtimes,whileL(cid:20)3iscomputation-allyfeasibleinthephrasalcase,andincreasespre-cisionby4.1%overthephrasesL(cid:20)2system.4In-cludingmismatchesresultsinanothersmallboostinperformance(0.5%),whileusinganEditDistancefeatureagainincreasesperformancebyaslightmar-gin(0.2%).Thisrankingofcon(cid:2)gurationsisconsis-tentacrossallthebitext-baseddevelopmentsets;wethereforetakethecon(cid:2)gurationofthehighestscor-ingsystemasourAlignment-BasedDiscriminativesystemfortheremainderofthispaper.WenextcomparetheAlignment-BasedDiscrim-inativescorertothevariousotherimplementedap-proachesacrossthethreebitextandsixdictionary-basedcognateidenti(cid:2)cationtestsets(Table3).Thetablehighlightsthetopsystemamongboththenon-adaptiveandadaptivesimilarityscorers.5In4Preliminaryexperimentsusingevenlongerphrases(be-yondL≤3)currentlyproduceacomputationallyprohibitivenumberoffeaturesforSVMlearning.Deployingcurrentfea-tureselectiontechniquesmightenabletheuseofevenmoreex-pressiveandpowerfulfeaturesetswithlongerphraselengths.5UsingthetrainingdataandtheSVMtoweightthecom-ponentsofthePREFIX+DICE+LCSR+NEDscorerresultedinnegligibleimprovementsoverthesimpleaverageonourdevel-opmentdata.eachlanguagepair,thealignment-baseddiscrimi-nativeapproachoutperformsallotherapproaches,buttheKRsystemalsoshowsstronggainsovernon-adaptivetechniquesandtheirre-weightedex-tensions.Thisisincontrasttopreviouscompar-isonswhichhaveonlydemonstratedminorimprove-mentswithadaptiveovertraditionalsimilaritymea-sures(KondrakandSherif,2006).WeconsistentlyfoundthattheoriginalKRperfor-mancecouldbesurpassedbyasystemthatnormal-izestheKRfeaturecountandaddsboundarymark-ers.Acrossallthetestsets,thismodi(cid:2)cationresultsina6%averagegaininperformanceoverbaselineKR,butisstillonaverage5%belowtheAlignment-BasedDiscriminativetechnique,withastatisticallysigni(cid:2)cantlydifferenceoneachoftheninesets.6Figure2showstherelationshipbetweentrain-ingdatasizeandperformanceinourbitext-basedFrench-Englishdata.NoteagainthattheTiedemannandRistad&Yanilossystemsonlyusethepositiveexamplesinthetrainingdata.Ouralignment-basedsimilarityfunctionoutperformsalltheothersystemsacrossnearlytheentirerangeoftrainingdata.Notealsothatthediscriminativelearningcurvesshownosignsofslowingdown:performancegrowslogarith-micallyfrom1Kto846Kwordpairs.Forinsightintothepowerofourdiscrimina-tiveapproach,weprovidesomeofourclassi(cid:2)ers’highestandlowest-weightedfeatures(Table4).6FollowingEvert(2004),signi(cid:2)cancewascomputedusingFisher’sexacttest(atp=0.05)tocomparethen-bestwordpairsfromthescoredtestsets,wherenwastakenasthenumberofpositivepairsintheset.662

 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 1000 10000 100000 1e+0611-pt Average PrecisionNumber of training pairsNEDTiedemannRistad-YanilosKlementiev-RothAlignment-Based Discrim.Figure2:BitextFrench-Englishcognateidenti(cid:2)ca-tionlearningcurve.Lang.Feat.Wt.ExampleFr(Bitext)·ees-ed+8.0v·eri(cid:2)·ees:veri(cid:2)edJp(Dict.)ru-l+5.9penaruti:penaltyDe(Bitext)k-c+5.5kreativ:creativeRs(Dict.)irov-+4.9motivirovat:motivateGr(Dict.)f-ph+4.1symfonia:symphonyGr(Dict.)kos-c+3.3anarchikos:anarchicGr(Dict.)os$-y$-2.5anarchikos:anarchyJp(Dict.)ou-ou-2.6handoutai:handoutEs(Dict.)-un-3.1balance:unbalanceFr(Dict.)er$-er$-5.0former:formerEs(Bitext)mos-s-5.1toleramos:toleratesTable4:Examplefeaturesandweightsforvar-iousAlignment-BasedDiscriminativeclassi(cid:2)ers(Foreign-English,negativepairsinitalics).NotetheexpectedcorrespondencesbetweenforeignspellingsandEnglish(k-c,f-ph),butalsofeaturesthatleveragederivationalandin(cid:3)ectionalmorphol-ogy.Forexample,Greek-Englishpairswiththeadjective-endingcorrespondencekos-c,e.g.anar-chikos:anarchic,arefavoured,butpairswiththead-jectiveendinginGreekandnounendinginEnglish,os$-y$,arepenalized;indeed,byourde(cid:2)nition,an-archikos:anarchyisnotcognate.Inabitext,thefeature´ees-edcapturesthatfeminine-pluralin(cid:3)ec-tionofpasttenseverbsinFrenchcorrespondstoregularpasttenseinEnglish.Ontheotherhand,wordsendingintheSpanish(cid:2)rstpersonpluralverbsuf(cid:2)x-amosarerarelytranslatedtoEnglishwordsendingwiththesuf(cid:2)x-s,causingmos-stobepe-Gr-En(Dict.)Es-En(Bitext)alkali:alkaliagenda:agendamakaroni:macaroninatural:naturaladrenalini:adrenalinem·argenes:margins(cid:3)amingko:(cid:3)amingohormonal:hormonalspasmodikos:spasmodicrad·on:radonamvrosia:ambrosiahigi·enico:hygienicTable5:HighestscoredpairsbyAlignment-BasedDiscriminativeclassi(cid:2)er(negativepairsinitalics).nalized.Theabilitytoleveragenegativefeatures,learnedfromappropriatecounterexamples,isakeyinnovationofourdiscriminativeframework.Table5givesthetoppairsscoredbyoursystemontwoofthesets.Noticethatunliketraditionalsim-ilaritymeasuresthatalwaysscoreidenticalwordshigherthanallotherpairs,byvirtueofourfeatureweighting,ourdiscriminativeclassi(cid:2)erpreferssomepairswithverycharacteristicspellingchanges.Weperformederroranalysisbylookingatallthepairsoursystemscoredquitecon(cid:2)dently(highlypositiveorhighlynegativesimilarity),butwhichwerelabelledoppositely.Highly-scoredfalsepos-itivesaroseequallyfrom1)actualcognatesnotlinkedastranslationsinthedata,2)relatedwordswithdivergedmeanings,e.g.theerrorinTable5:makaroniinGreekactuallymeansspaghettiinEn-glish,and3)thesamewordstem,adifferentpartofspeech(e.g.theGreek/Englishadjective/nounsynonymos:synonym).Meanwhile,inspectionofthehighly-con(cid:2)dentfalsenegativesrevealedsome(of-tenerroneously-alignedinthebitext)positivepairswithincidentallettermatch(e.g.theFrench/Englishrecettes:proceeds)thatwewouldnotactuallydeemtobecognate.Thustheerrorsthatoursystemmakesareofteneitherlinguisticallyinterestingorpointoutmistakesinourautomatically-labelledbitextand(toalesserextent)dictionarydata.7ConclusionThisisthe(cid:2)rstresearchtoapplydiscriminativestringsimilaritytothetaskofcognateidenti(cid:2)cation.Wehaveintroducedandsuccessfullyappliedanalignment-basedframeworkfordiscriminativesim-ilaritythatconsistentlydemonstratesimprovedper-formanceinbothbitextanddictionary-basedcog-663

nateidenti(cid:2)cationonsixlanguagepairs.Ourim-provedapproachcanbeappliedinanyofthedi-verseapplicationswheretraditionalsimilaritymea-sureslikeEditDistanceandLCSRareprevalent.Wehavealsomadeavailableourcognateidenti(cid:2)cationdatasets,whichwillbeofinteresttogeneralstringsimilarityresearchers.Furthermore,wehaveprovidedanaturalframe-workforfuturecognateidenti(cid:2)cationresearch.Pho-netic,semantic,orsyntacticfeaturescouldbein-cludedwithinourdiscriminativeinfrastructuretoaidintheidenti(cid:2)cationofcognatesintext.Inparticu-lar,weplantoinvestigateapproachesthatdonotre-quirethebilingualdictionariesorbitextstogeneratetrainingdata.Forexample,researchershaveauto-maticallydevelopedtranslationlexiconsbyseeingifwordsfromeachlanguagehavesimilarfrequen-cies,contexts(KoehnandKnight,2002),bursti-ness,inversedocumentfrequencies,anddatedis-tributions(SchaferandYarowsky,2002).Semanticandstringsimilaritymightbelearnedjointlywithaco-trainingorbootstrappingapproach(KlementievandRoth,2006).Wemayalsocomparealignment-baseddiscriminativestringsimilaritywithamorecomplexdiscriminativemodelthatlearnsthealign-mentsaslatentstructure(McCallumetal.,2005).AcknowledgmentsWegratefullyacknowledgesupportfromtheNatu-ralSciencesandEngineeringResearchCouncilofCanada,theAlbertaIngenuityFund,andtheAlbertaInformaticsCircleofResearchExcellence.ReferencesGeorgeW.AdamsonandJillianBoreham.1974.Theuseofanassociationmeasurebasedoncharacterstructuretoiden-tifysemanticallyrelatedpairsofwordsanddocumenttitles.InformationStorageandRetrieval,10:253(cid:150)260.MikhailBilenkoandRaymondJ.Mooney.2003.Adaptivedu-plicatedetectionusinglearnablestringsimilaritymeasures.InKDD,pages39(cid:150)48.EricBrillandRobertMoore.2000.Animprovederrormodelfornoisychannelspellingcorrection.InACL.286(cid:150)293.StefanEvert.2004.Signi(cid:2)cancetestsfortheevaluationofrankingmethods.InCOLING,pages945(cid:150)951.ThorstenJoachims.1999.Makinglarge-scaleSupportVectorMachinelearningpractical.InAdvancesinKernelMethods:SupportVectorMachines,pages169(cid:150)184.MIT-Press.AlexandreKlementievandDanRoth.2006.Namedentitytransliterationanddiscoveryfrommultilingualcomparablecorpora.InHLT-NAACL,pages82(cid:150)88.PhilippKoehnandKevinKnight.2002.Learningatransla-tionlexiconfrommonolingualcorpora.InACLWorkshoponUnsupervisedLexicalAcquistion.PhilippKoehnandChristofMonz.2006.Manualandauto-maticevaluationofmachinetranslationbetweenEuropeanlanguages.InNAACLWorkshoponStatisticalMachineTranslation,pages102(cid:150)121.PhilippKoehn,FranzJosefOch,andDanielMarcu.2003.Statisticalphrase-basedtranslation.InHLT-NAACL,pages127(cid:150)133.GrzegorzKondrakandTarekSherif.2006.Evaluationofseveralphoneticsimilarityalgorithmsonthetaskofcog-nateidenti(cid:2)cation.InCOLING-ACLWorkshoponLinguis-ticDistances,pages37(cid:150)44.GrzegorzKondrak.2005.Cognatesandwordalignmentinbitexts.InMTSummitX,pages305(cid:150)312.VladimirI.Levenshtein.1966.Binarycodescapableofcor-rectingdeletions,insertions,andreversals.SovietPhysicsDoklady,10(8):707(cid:150)710.GideonS.MannandDavidYarowsky.2001.Multipathtrans-lationlexiconinductionviabridgelanguages.InNAACL,pages151(cid:150)158.AndrewMcCallum,KedarBellare,andFernandoPereira.2005.Aconditionalrandom(cid:2)eldfordiscriminatively-trained(cid:2)nite-statestringeditdistance.InUAI.388(cid:150)395.I.DanMelamed.1999.Bitextmapsandalignmentviapatternrecognition.ComputationalLinguistics,25(1):107(cid:150)130.AndreaMulloniandViktorPekar.2006.Automaticdetec-tionoforthographiccuesforcognaterecognition.InLREC,pages2387(cid:150)2390.AriRappoportandTsahiLevent-Levi.2006.Inductionofcross-languageaf(cid:2)xandlettersequencecorrespondence.InEACLWorkshoponCross-LanguageKnowledgeInduction.EricSvenRistadandPeterN.Yianilos.1998.Learningstring-editdistance.IEEETransactionsonPatternAnalysisandMachineIntelligence,20(5):522(cid:150)532.CharlesSchaferandDavidYarowsky.2002.Inducingtransla-tionlexiconsviadiversesimilaritymeasuresandbridgelan-guages.InCoNLL,pages207(cid:150)216.MichaelStrube,StefanRapp,andChristophM¤uller.2002.Thein(cid:3)uenceofminimumeditdistanceonreferenceresolution.InEMNLP,pages312(cid:150)319.BenTaskar,SimonLacoste-Julien,andDanKlein.2005.Adiscriminativematchingapproachtowordalignment.InHLT-EMNLP,pages73(cid:150)80.J¤orgTiedemann.1999.Automaticconstructionofweightedstringsimilaritymeasures.InEMNLP-VLC,pages213(cid:150)219.DmitryZelenkoandChinatsuAone.2006.Discriminativemethodsfortransliteration.InEMNLP,pages612(cid:150)617.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 664–671,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

664

BilingualTerminologyMining–UsingBrain,notbrawncomparablecorporaE.Morin,B.DailleUniversitédeNantesLINAFRECNRS27292,ruedelaHoussinièreBP92208F-44322NantesCedex03{morin-e,daille-b}@univ-nantes.frK.TakeuchiOkayamaUniversity3-1-1,TsushimanakaOkayama-shi,Okayama,700-8530,Japankoichi@cl.it.okayama-u.ac.jpK.KageuraGraduateSchoolofEducationTheUniversityofTokyo7-3-1Hongo,Bunkyo-ku,Tokyo,113-0033,Japankyo@p.u-tokyo.ac.jpAbstractCurrentresearchintextminingfavoursthequantityoftextsovertheirquality.Butforbilingualterminologymining,andformanylanguagepairs,largecomparablecorporaarenotavailable.Moreimportantly,astermsaredeﬁnedvis-à-visaspeciﬁcdomainwitharestrictedregister,itisexpectedthatthequalityratherthanthequantityofthecorpusmattersmoreinterminologymining.Ourhypothesis,therefore,isthatthequalityofthecorpusismoreimportantthanthequan-tityandensuresthequalityoftheacquiredterminologicalresources.Weshowhowim-portantthetypeofdiscourseisasacharac-teristicofthecomparablecorpus.1IntroductionTwomainapproachesexistforcompilingcorpora:“Bigisbeautiful”or“Insecurityinlargecollec-tions”.Textminingresearchcommonlyadoptstheﬁrstapproachandfavorsdataquantityoverqual-ity.Thisisnormallyjustiﬁedontheonehandbytheneedforlargeamountsofdatainordertomakeuseofstatisticorstochasticmethods(ManningandSchütze,1999),andontheotherbythelackofoper-ationalmethodstoautomatizethebuildingofacor-pusansweringtoselectedcriteria,suchasdomain,register,media,styleordiscourse.Forlexicalalignmentfromcomparablecorpora,goodresultsonsinglewordscanbeobtainedfromlargecorpora—severalmillionswords—theaccu-racyofproposedtranslationisabout80%forthetop10-20candidates(Fung,1998;Rapp,1999;ChiaoandZweigenbaum,2002).(CaoandLi,2002)haveachieved91%accuracyforthetopthreecandidatesusingtheWebasacomparablecorpus.Butforspe-ciﬁcdomains,andmanypairsoflanguages,suchhugecorporaarenotavailable.Moreimportantly,astermsaredeﬁnedvis-à-visaspeciﬁcdomainwitharestrictedregister,itisexpectedthatthequalityratherthanthequantityofthecorpusmattersmoreinterminologymining.Forterminologymining,there-fore,ourhypothesisisthatthequalityofthecorporaismoreimportantthanthequantityandthatthisen-suresthequalityoftheacquiredterminologicalre-sources.Comparablecorporaare“setsoftextsindifferentlanguages,thatarenottranslationsofeachother”(BowkerandPearson,2002,p.93).Thetermcom-parableisusedtoindicatethatthesetextssharesomecharacteristicsorfeatures:topic,period,me-dia,author,register(Biber,1994),discourse...Thiscorpuscomparabilityisdiscussedbylexicalalign-mentresearchersbutneverdemonstrated:itisof-tenreducedtoaspeciﬁcdomain,suchasthemed-ical(ChiaoandZweigenbaum,2002)orﬁnancialdomains(Fung,1998),ortoaregister,suchasnewspaperarticles(Fung,1998).Forterminology665

mining,thecomparabilityofthecorpusshouldbebasedonthedomainorthesub-domaine,butalsoonthetypeofdiscourse.Indeed,discourseactssemanticallyuponthelexicalunits.Foradeﬁnedtopic,sometermsarespeciﬁctoonediscourseoranother.Forexample,forFrench,withinthesub-domainofobesityinthedomainofmedicine,weﬁndthetermexcèsdepoids(overweight)onlyin-sidetextssharingapopularsciencediscourse,andthesynonymexcèspondéral(overweight)onlyinscientiﬁcdiscourse.Inordertoevaluatehowimpor-tantthediscoursecriterionisforbuildingbilingualterminologicallists,wecarriedoutexperimentsonFrench-Japanesecomparablecorporainthedomainofmedicine,morepreciselyonthetopicofdiabetesandnutrition,usingtextscollectedfromtheWebandmanuallyselectedandclassiﬁedintotwodiscoursecategories:onecontainsonlyscientiﬁcdocumentsandtheothercontainsbothscientiﬁcandpopularsciencedocuments.Weusedastate-of-the-artmultilingualterminol-ogyminingchaincomposedoftwotermextractionprograms,oneineachlanguage,andanalignmentprogram.Thetermextractionprogramsarepub-liclyavailableandbothextractmulti-wordtermsthataremorepreciseandspeciﬁctoaparticularsci-entiﬁcdomainthansinglewordterms.Thealign-mentprogrammakesuseofthedirectcontext-vectorapproach(Fung,1998;PetersandPicchi,1998;Rapp,1999)slightlymodiﬁedtohandlebothsingle-andmulti-wordterms.Weevaluatedthecandidatetranslationsofmulti-wordtermsusingareferencelistcompiledfrompubliclyavailableresources.Wefoundthattakingdiscoursetypeintoaccountre-sultedincandidatetranslationsofabetterqualityevenwhenthecorpussizeisreducedbyhalf.Thus,evenusingastate-of-the-artalignmentmethodwell-knownasdatagreedy,wereachedtheconclusionthatthequantityofdataisnotsufﬁcienttoobtainaterminologicallistofhighqualityandthatarealcomparabilityofcorporaisrequired.2MultilingualterminologyminingchainTakingasinputacomparablecorpora,themultilin-gualterminologychainoutputsalistofsingle-andmulti-wordcandidatetermsalongwiththeircandi-datetranslations.ItsarchitectureissummarizedinFigure1andcomprisestermextractionandalign-mentprograms.2.1TermextractionprogramsTheterminologyextractionprogramsareavail-ableforbothFrench1(Daille,2003)andJapanese2(Takeuchietal.,2004).Theterminologicalunitsthatareextractedaremulti-wordtermswhosesyn-tacticpatternscorrespondeithertoacanonicaloravariationstructure.Thepatternsareexpressedus-ingpart-of-speechtags:forFrench,Brill’sPOStag-ger3andtheFLEMlemmatiser4areutilised,andforJapanese,CHASEN5.ForFrench,themainpatternsareNN,NPrepNetNAdjandforJapanese,NN,NSuff,AdjNandPrefN.Thevariantshandledaremorphologicalforbothlanguages,syntacticalonlyforFrench,andcompoundingonlyforJapanese.Weconsiderasamorphologicalvariantamorphologicalmodiﬁcationofoneofthecomponentsofthebaseform,asasyntacticalvarianttheinsertionofanotherwordintothecomponentsofthebaseform,andasacompoundingvarianttheagglutinationofanotherwordtooneofthecomponentsofthebaseform.Forexample,inFrench,thecandidateMWTsécrétiond’insuline(insulinsecretion)appearsinthefollow-ingforms: baseformofNPrepNpattern:sécrétiond’insuline(insulinsecretion); inﬂexionalvariant:sécrétionsd’insuline(in-sulinsecretions); syntacticvariant(insertioninsidethebaseformofamodiﬁer):sécrétionpancréatiqued’insuline(pancreaticinsulinsecretion); syntacticvariant(expansioncoordinationofbaseform):secrétiondepeptideetd’insuline(insulinandpeptidesecretion).TheMWTcandidatessecrétioninsulinique(insulinsecretion)andhypersécrétioninsulinique(insulin1http://www.sciences.univ-nantes.fr/info/perso/permanents/daille/andreleaseLINUX.2http://research.nii.ac.jp/~koichi/study/hotal/3http://www.atilf.fr/winbrill/4http://www.univ-nancy2.fr/pers/namer/5http://chasen.org/$\sim$taku/software/mecab/666

WEBdictionarybilingualJapanese documents French documentsterminologyextractionterminologyextractionlexical contextextractionlexical contextextractionprocesstranslatedterms to betranslationscandidatehaverstingdocumentslexical alignmentFigure1:Architectureofthemultilingualterminologyminingchainhypersecretion)havealsobeenidentiﬁedandleadtogetherwithsécrétiond’insuline(insulinsecretion)toaclusterofsemanticallylinkedMWTs.InJapanese,theMWT .
	6(in-sulinsecretion)appearsinthefollowingforms: baseformofNNpattern: /N.	/N(insulinsecretion); compoundingvariant(agglutinationofawordattheendofthebaseform): /N.	/N./N(insulinsecretionability)Atpresent,theJapanesetermextractionprogramdoesnotclusterterms.2.2TermalignmentThelexicalalignmentprogramadaptsthedirectcontext-vectorapproachproposedby(Fung,1998)forsingle-wordterms(SWTs)tomulti-wordterms(MWTs).ItalignssourceMWTswithtargetsingle6ForallJapaneseexamples,weexplicitlysegmentthecom-poundintoitscomponentpartsthroughtheuseofthe“.”sym-bol.words,SWTsorMWTs.Fromnowon,wewillrefertolexicalunitsaswords,SWTsorMWTs.2.2.1Implementationofthedirectcontext-vectormethodOurimplementationofthedirectcontext-vectormethodconsistsofthefollowing4steps:1.Wecollectallthelexicalunitsinthecontextofeachlexicalunitandcounttheiroccurrencefrequencyinawindowofwordsaround.Foreachlexicalunitofthesourceandthetargetlanguage,weobtainacontextvectorwhichgathersthesetofco-occurrenceunitsassociatedwiththenumberoftimesthatandoccurtogether !".Wenormalisecontextvec-torsusinganassociationscoresuchasMutualInformationorLog-likelihood.Inordertore-ducethearityofcontextvectors,wekeeponlytheco-occurrenceswiththehighestassociationscores.2.Usingabilingualdictionary,wetranslatethelexicalunitsofthesourcecontextvector.667

3.Forawordtobetranslated,wecomputethesimilaritybetweenthetranslatedcontextvectorandalltargetvectorsthroughvectordistancemeasuressuchasCosine(SaltonandLesk,1968)orJaccard(Tanimoto,1958).4.Thecandidatetranslationsofalexicalunitarethetargetlexicalunitsclosesttothetranslatedcontextvectoraccordingtovectordistance.2.2.2TranslationoflexicalunitsThetranslationofthelexicalunitsofthecontextvectors,whichdependsonthecoverageofthebilin-gualdictionaryvis-à-visthecorpus,isanimportantstepofthedirectapproach:moreelementsofthecontextvectoraretranslatedmorethecontextvectorwillbediscrimatingforselectingtranslationsinthetargetlanguage.Ifthebilingualdictionaryprovidesseveraltranslationsforalexicalunit,weconsiderallofthembutweightthedifferenttranslationsbytheirfrequencyinthetargetlanguage.IfanMWTcannotbedirectlytranslated,wegeneratepossibletrans-lationsbyusingacompositionalmethod(Grefen-stette,1999).ForeachelementoftheMWTfoundinthebilingualdictionary,wegenerateallthetrans-latedcombinationsidentiﬁedbythetermextractionprogram.Forexample,inthecaseoftheMWTfa-tiguechronique(chronicfatigue),wehavethefol-lowingfourtranslationsforfatigue: , ,,
	andthefollowingtwotranslationsforchronique:,.Next,wegenerateallcombinationsoftranslatedelements(SeeTable17)andselectthosewhichrefertoanexistingMWTinthetargetlanguage.Here,onlyonetermhasbeenidentiﬁedbytheJapaneseterminologyextrac-tionprogram:. .Inthisapproach,whenitisnotpossibletotranslateallpartsofanMWT,orwhenthetranslatedcombinationsarenotidenti-ﬁedbythetermextractionprogram,theMWTisnottakenintoaccountinthetranslationprocess.Thisapproachdiffersfromthatusedby(Ro-bitailleetal.,2006)forFrench/Japanesetranslation.TheyﬁrstdecomposetheFrenchMWTintocom-binationsofshortermulti-wordunits(MWU)ele-ments.ThisapproachmakesthedirecttranslationofasubpartoftheMWTpossibleifitispresentinthe7theFrenchwordorderisinvertedtotakeintoaccountthedifferentconstraintsbetweenFrenchandJapanese.chroniquefatigue    		Table1:Illustrationofthecompositionalmethod.TheunderlinedJapaneseMWTactuallyexists.bilingualdictionary.ForanMWToflength,(Ro-bitailleetal.,2006)produceallthecombinationsofMWUelementsofalengthlessthanorequalto.Forexample,theFrenchtermsyndromedefatiguechronique(chronicfatiguedisease)yieldsthefol-lowingfourcombinations:i)syndromedefatiguechronique,ii)syndromedefatiguechronique,iii)syndromefatiguechroniqueandiv)syndromefatiguechronique.Welimitourselvestothecom-binationoftypeiv)abovesince90%ofthecandidatetermsprovidedbythetermextractionprocess,afterclustering,areonlycomposedoftwocontentwords.3LinguisticresourcesInthissectionweoutlinethedifferenttextualre-sourcesusedforourexperiments:thecomparablecorpora,bilingualdictionaryandreferencelexicon.3.1ComparablecorporaTheFrenchandJapanesedocumentswereharvestedfromtheWebbynativespeakersofeachlanguagewhoarenotdomainspecialists.Thetextsarefromthemedicaldomain,withinthesub-domainofdia-betesandnutrition.Documentharvestingwascar-riedoutbyadomain-basedsearch,thenbyman-ualselection.Thesearchfordocumentssharingthesamedomaincanbeachievedusingkeywordsre-ﬂectingthespecializeddomain:forFrench,diabèteandobésité(diabetesandobesity);forJapanese, !"and#$.Thenthedocumentswereclassiﬁedaccordingtothetypeofdiscourse:scientiﬁcorpop-ularizedscience.Atpresent,theselectionandclas-siﬁcationphasesarecarriedoutmanuallyalthough668

researchintohowtoautomatizethesetwostepsisongoing.Table2showsthemainfeaturesoftheharvestedcomparablecorpora:thenumberofdoc-uments,andthenumberofwordsforeachlanguageandeachtypeofdiscourse.FrenchJapanesedoc.wordsdoc.wordsScientiﬁc65425,781119234,857Popular183267,885419572,430scienceTotal248693,666538807,287Table2:ComparablecorporastatisticsFromthesedocuments,wecreatedtwocompara-blecorpora: scientiﬁccorpora,composedonlyofscientiﬁcdocuments; mixedcorpora,composedofbothpopularandscientiﬁcdocuments.3.2BilingualdictionaryTheFrench-Japanesebilingualdictionaryrequiredforthetranslationphaseiscomposedoffourdic-tionariesfreelyavailablefromtheWeb8,andoftheFrench-JapaneseScientiﬁcDictionary(1989).Itcontainsabout173,156entries(114,461singlewordsand58,695multiwords)withanaverageof2.1translationsperentry.3.3TerminologyreferencelistsToevaluatethequalityoftheterminologymin-ingchain,webuilttwobilingualterminologyrefer-encelistswhichincludeeitherSWTsorSMTsandMWTs: lexicon1100FrenchSWTsofwhichthetranslationareJapaneseSWTs. lexicon260FrenchSWTsandMWTsofwhichthetranslationcouldbeJapaneseSWTsorMWTs.8http://kanji.free.fr/,http://quebec-japon.com/lexique/index.php?a=index&d=25,http://dico.fj.free.fr/index.php,http://quebec-japon.com/lexique/index.php?a=index&d=3Theselexiconscontainstermsthatoccuratleasttwiceinthescientiﬁccorpus,havebeenidentiﬁedmonolinguallybyboththeFrenchandtheJapanesetermextractionprograms,andarefoundineithertheUMLS9thesaurusorintheFrenchpartoftheGranddictionnaireterminologique10inthedomainofmedicine.Theseconstraintspreventedusfromobtaining100FrenchSWTsandMWTsforlexicon2.ThemainreasonsforthisarethesmallnumberofUMLStermsdealingwiththesub-domainofdi-abetesandthegreatdifferencebetweenthelinguis-ticstructuresofFrenchandJapaneseterms:FrenchpatterndeﬁnitionstendtocovermorephrasalunitswhileJapanesepatterndeﬁnitionsfocusmorenar-rowlyoncompounds.So,evenifmonolinguallythesamepercentageoftermsaredetectedinbothlanguages,thisdoesnotguaranteeagoodresultinbilingualterminologyextraction.Forexample,theFrenchtermdiabètedetype1(DiabetesmellitustypeI)extractedbytheFrenchtermextractionpro-gramandfoundinUMLSwasnotextractedbytheJapanesetermextractionprogramalthoughitap-pearsfrequentlyintheJapanesecorpus(  !").Inbilingualterminologyminingfromspecializedcomparablecorpora,theterminologyreferencelistsareoftencomposedofahundredwords(180SWTsin(DéjeanandGaussier,2002)and97SWTsin(ChiaoandZweigenbaum,2002)).4ExperimentsInordertoevaluatetheinﬂuenceofdiscoursetypeonthequalityofbilingualterminologyextraction,twoexperimentswerecarriedout.Sincethemainstudiesrelatingtobilinguallexiconextractionfromcomparablecorporaconcentrateonﬁndingtransla-tioncandidatesforSWTs,weﬁrstperformanex-perimentusinglexicon1,whichiscomposedofSWTs.Inordertoevaluatethehypothesisofthisstudy,wethenconductedasecondexperimentusinglexicon2,whichiscomposedofMWTs.4.1Alignmentresultsforlexicon1Table3showstheresultsobtained.Theﬁrstthreecolumnsindicatethenumberoftranslationsfound9http://www.nlm.nih.gov/research/umls10http://www.granddictionnaire.com/669

 
	 "!$# "!#scientiﬁccorpora6411.620.24952mixedcorpora7611.516.35160Table3:Bilingualterminologyextractionresultsforlexicon1 
	 "!$# "!#scientiﬁccorpora3216.121.91825mixedcorpora3223.927.61720Table4:Bilingualterminologyextractionresultsforlexicon2( %	&),andtheaverage()andstandarddeviation(')positionsforthetransla-tionsintherankedlistofcandidatetranslations.TheothertwocolumnsindicatethepercentageofFrenchtermsforwhichthecorrecttranslationwasobtainedamongthetoptenandtoptwentycandi-dates(!$#, "!#).Theresultsofthisexperiment(seeTable3)showthatthetermsbelongingtolexicon1weremoreeasilyidentiﬁedinthecorpusofscientiﬁcandpop-ulardocuments(51%and60%respectivelyfor "!$#and !#)thaninthecorpusofscien-tiﬁcdocuments(49%and52%).Sincelexicon1iscomposedofSWTs,thesetermsarenotmorechar-acteristicofpopulardiscoursethanscientiﬁcdis-course.Thefrequencyofthetermstobetranslatedisanimportantfactorinthevectorialapproach.Infact,thehigherthefrequencyofthetermtobetranslated,themoretheassociatedcontextvectorwillbedis-criminant.Table5conﬁrmsthishypothesissincethemostfrequentterms,suchasinsuline(#occ.364-insulin: ),obésité(#occ.333-obe-sity:#$),andprévention(#occ.120-prevention:(*)),werethebesttranslated.[2,10][11,50][51,100][101,...]fr3/1712/2917/2328/31jp4/2632/4114/2010/13Table5:Frequencyincorpus2ofthetermstrans-latedbelongingtolexicon1(for "!#)Asabaseline,(Déjeanetal.,2002)obtain43%and51%fortheﬁrst10and20candidatesrespec-tivelyina100,000-wordmedicalcorpus,and79%and84%inamulti-domain8million-wordcor-pus.Forsingle-itemFrench-Englishwordsappliedonamedicalcorpusof0.66millionwords,(ChiaoandZweigenbaum,2002)obtained61%and94%precisiononthetop-10andtop-20candidates.Inourcase,weobtained51%and60%precisionforthetop10and20candidatesina1.5million-wordFrench/Japanesecorpus.4.2Alignmentresultsforlexicon2Theanalysisresultsintable4indicateonlyasmallnumberofthetermsinlexicon2werefound.Sinceweworkwithsmall-sizecorpora,thisresultisnotsurprising.Becausemulti-wordtermsaremorespeciﬁcthansingle-wordterms,theytendtooccurlessfrequentlyinacorpusandaremoredifﬁ-culttotranslate.Here,thetermsbelonginglexicon2weremoreaccuratelyidentiﬁedfromthecorpuswhichconsistsofscientiﬁcdocumentsthanthecor-puswhichconsistsofscientiﬁcandpopulardoc-uments.Inthisinstance,weobtained30%and42%precisionforthetop10andtop20candi-datesina0.84million-wordscientiﬁccorpus.More-over,ifwecountthenumberoftermswhicharecorrectlytranslatedbetweenscientiﬁccorporaandmixedcorpora,weﬁndthemajorityofthetrans-latedtermswithmixedcorporainthoseobtainedwithscientiﬁccorpora11Bycombiningparameters11Here,+,.-0/214357683
9;:<3
=,+,.-?>@1
A4=B6A%C;:D3E5andF GHJILKEM%NAO6NA.:PA%5.670

C=3C3=ACA4=C=3EC3=A%CA=·······nbr.win.C=3C3=ACA4=C=3EC3=A%CA=·······nbr.win.(a)parameter:Log-likelihood&cosinus(b)parameter:Log-likelihood&jaccardC=3C3=ACA4=C=3EC3=A%CA=·······nbr.win.C=3C3=ACA4=C=3EC3=A%CA=·······nbr.win.(c)parameter:MI&cosinus(d)parameter:MI&jaccardFigure2:Evolutionofthenumberoftranslationsfoundin "!#accordingtothesizeofthecontextualwindowforseveralcombinationsofparameterswithlexicon2(scientiﬁccorpora—–;mixedcorpora---,thepointsindicatedarethecomputedvalues)suchasthewindowsizeofthecontextvector,as-sociationscore,andvectordistancemeasure,thetermswereoftenidentiﬁedwithmoreprecisionfromthecorpusconsistingofscientiﬁcdocumentsthanthecorpusconsistingofscientiﬁcandpopulardocu-ments(seeFigure2).Hereagain,themostfrequentterms(seeTable6),suchasdiabète(#occ.899-diabetes: !."),facteurderisque(#occ.267-riskfactor:.	
),hyperglycémie(#occ.127-hyperglycaemia:. ),tissuadipeux(#occ.62-adiposetissue:.)werethebesttranslated.Ontheotherhand,sometermswithlowfrequency,suchasédul-corant(#occ.13-sweetener:.)andchoixal-imentaire(#occ.11-feedingpreferences:.),orverylowfrequency,suchasobésitémassive(#occ.6-massiveobesity:.#$),werealsoidentiﬁedwiththisapproach.[2,10][11,50][51,100][101,...]fr1/1111/256/147/10jp5/2113/255/92/5Table6:Frequencyinscientiﬁccorporaoftrans-latedtermsbelongingtolexicon2(for !#)5ConclusionThisarticledescribesaﬁrstattemptatcompilingFrench-Japaneseterminologyfromcomparablecor-poratakingintoaccountbothsingle-andmulti-wordterms.Ourclaimwasthatarealcomparabilityofthecorporaisrequiredtoobtainrelevanttermsofthedomain.Thiscomparabilityshouldbebasednotonlyonthedomainandthesub-domainbutalsoonthetypeofdiscourse,whichactssemanticallyuponthelexicalunits.Thediscoursecategorizationofdocumentsallowslexicalacquisitiontoincreasepre-671

cisiondespitethedatasparsityproblemthatisof-tenencounteredforterminologyminingandforlan-guagepairsnotinvolvingtheEnglishlanguage,suchasFrench-Japanese.Wecarriedoutexperimentsus-ingtwocorporaofthespecialiseddomainconcern-ingdiabetesandnutrition:onegatheringdocumentsfrombothscientiﬁcandpopularsciencediscourses,theotherlimitedtoscientiﬁcdiscourse.Ouralign-mentresultsareclosetopreviousworksinvolvingtheEnglishlanguage,andareofbetterqualityforthescientiﬁccorpusdespiteacorpussizethatwasreducedbyhalf.Theresultsdemonstratethatthemorefrequentatermanditstranslation,thebetterthequalityofthealignmentwillbe,butalsothatthedatasparsityproblemcouldbepartiallysolvedbyusingcomparablecorporaofhighquality.ReferencesDouglasBiber.1994.Representativenessincorpusde-sign.InA.Zampolli,N.Calzolari,andM.Palmer,editors,CurrentIssuesinComputationalLinguistics:inHonourofDonWalker,pages377–407.Pisa:Giar-dini/Dordrecht:Kluwer.LynneBowkerandJenniferPearson.2002.WorkingwithSpecializedLanguage:APracticalGuidetoUs-ingCorpora.London/NewYork:Routledge.YunboCaoandHangLi.2002.BaseNounPhraseTrans-lationUsingWebDataandtheEMAlgorithm.InProceedingsofthe19thInternationalConferenceonComputationalLinguistics(COLING’02),pages127–133,Tapei,Taiwan.Yun-ChuangChiaoandPierreZweigenbaum.2002.Lookingforcandidatetranslationalequivalentsinspe-cialized,comparablecorpora.InProceedingsofthe19thInternationalConferenceonComputationalLin-guistics(COLING’02),pages1208–1212,Tapei,Tai-wan.BéatriceDaille.2003.TerminologyMining.InMariaTeresaPazienza,editor,InformationExtractionintheWebEra,pages29–44.Springer.HervéDéjeanandÉricGaussier.2002.Unenouvelleap-proche l’extractiondelexiquesbilingues partirdecorpuscomparables.Lexicometrica,Alignementlexi-caldanslescorpusmultilingues,pages1–22.HervéDéjean,FatiaSadat,andÉricGaussier.2002.Anapproachbasedonmultilingualthesauriandmodelcombinationforbilinguallexiconextraction.InPro-ceedingsofthe19thInternationalConferenceonComputationalLinguistics(COLING’02),pages218–224,Tapei,Taiwan.French-JapaneseScientiﬁcDictionary.1989.Hakusu-isha.4thedition.PascaleFung.1998.AStatisticalViewonBilingualLexiconExtraction:FromParallelCorporatoNon-parallelCorpora.InDavidFarwell,LaurieGerber,andEduardHovy,editors,Proceedingsofthe3rdCon-ferenceoftheAssociationforMachineTranslationintheAmericas(AMTA’98),pages1–16,Langhorne,PA,USA.Springer.GregoryGrefenstette.1999.TheWordWideWebasaResourceforExample-BasedMachineTranslationTasks.InASLIB’99TranslatingandtheComputer21,London,UK.ChristopherD.ManningandHinrichSchütze.1999.FoundationsofStatisticalNaturalLanguageProcess-ing.MITPress,Cambridge,MA.CarolPetersandEugenioPicchi.1998.Cross-languageinformationretrieval:Asystemforcomparablecor-pusquerying.InGregoryGrefenstette,editor,Cross-languageinformationretrieval,chapter7,pages81–90.Kluwer.ReinhardRapp.1999.AutomaticIdentiﬁcationofWordTranslationsfromUnrelatedEnglishandGermanCor-pora.InProceedingsofthe37thAnnualMeetingoftheAssociationforComputationalLinguistics(ACL’99),pages519–526,CollegePark,Maryland,USA.XavierRobitaille,XavierSasaki,MasatsuguTonoike,SatoshiSato,andSatoshiUtsuro.2006.Compil-ingFrench-JapaneseTerminologiesfromtheWeb.InProceedingsofthe11thConferenceoftheEuropeanChapteroftheAssociationforComputationalLinguis-tics(EACL’06),pages225–232,Trento,Italy.GerardSaltonandMichaelE.Lesk.1968.Computerevaluationofindexingandtextprocessing.Jour-naloftheAssociationforComputationalMachinery,15(1):8–36.KoichiTakeuchi,KyoKageura,BéatriceDaille,andLau-rentRomary.2004.Constructionofgrammarbasedtermextractionmodelforjapanese.InSophiaAnana-diouandPierreZweigenbaum,editors,ProceedingoftheCOLING2004,3rdInternationalWorkshoponComputationalTerminology(COMPUTERM’04),pages91–94,Geneva,Switzerland.T.T.Tanimoto.1958.Anelementarymathematicalthe-oryofclassiﬁcation.Technicalreport,IBMResearch.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 672–679,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

672

Unsupervised Language Model Adaptation Incorporating  Named Entity Information Feifan Liu and Yang Liu Department of Computer Science The University of Texas at Dallas, Richardson, TX, USA {ffliu,yangl}@hlt.utdallas.edu   Abstract Language model (LM) adaptation is im-portant for both speech and language processing. It is often achieved by com-bining a generic LM with a topic-specific model that is more relevant to the target document.  Unlike previous work on un-supervised LM adaptation, this paper in-vestigates how effectively using named entity (NE) information, instead of con-sidering all the words, helps LM adapta-tion. We evaluate two latent topic analysis approaches in this paper, namely, cluster-ing and Latent Dirichlet Allocation (LDA). In addition, a new dynamically adapted weighting scheme for topic mix-ture models is proposed based on LDA topic analysis. Our experimental results show that the NE-driven LM adaptation framework outperforms the baseline ge-neric LM. The best result is obtained us-ing the LDA-based approach by expanding the named entities with syntac-tically filtered words, together with using a large number of topics, which yields a perplexity reduction of 14.23% compared to the baseline generic LM. 1 Introduction Language model (LM) adaptation plays an impor-tant role in speech recognition and many natural language processing tasks, such as machine trans-lation and information retrieval. Statistical N-gram LMs have been widely used; however, they capture only local contextual information. In addition, even with the increasing amount of LM training data, there is often a mismatch problem because of dif-ferences in domain, topics, or styles. Adaptation of LM, therefore, is very important in order to better deal with a variety of topics and styles. Many studies have been conducted for LM ad-aptation. One method is supervised LM adaptation, where topic information is typically available and a topic specific LM is interpolated with the generic LM (Kneser and Steinbiss, 1993; Suzuki and Gao, 2005). In contrast, various unsupervised ap-proaches perform latent topic analysis for LM ad-aptation. To identify implicit topics from the unlabeled corpus, one simple technique is to group the documents into topic clusters by assigning only one topic label to a document (Iyer and Ostendorf, 1996). Recently several other methods in the line of latent semantic analysis have been proposed and used in LM adaptation, such as latent semantic analysis (LSA) (Bellegarda, 2000), probabilistic latent semantic analysis (PLSA) (Gildea and Hof-mann, 1999), and LDA (Blei et al., 2003). Most of these existing approaches are based on the “bag of words” model to represent documents, where all the words are treated equally and no relation or association between words is considered.  Unlike prior work in LM adaptation, this paper investigates how to effectively leverage named entity information for latent topic analysis. Named entities are very common in domains such as newswire or broadcast news, and carry valuable information, which we hypothesize is topic indica-tive and useful for latent topic analysis. We com-pare different latent topic generation approaches as well as model adaptation methods, and propose an LDA based dynamic weighting method for the topic mixture model. Furthermore, we expand 673

named entities by incorporating other content words, in order to capture more topic information. Our experimental results show that the proposed method of incorporating named information in LM adaptation is effective. In addition, we find that for the LDA based adaptation scheme, adding more content words and increasing the number of topics can further improve the performance significantly. The paper is organized as follows. In Section 2 we review some related work. Section 3 describes in detail our unsupervised LM adaptation approach using named entities. Experimental results are pre-sented and discussed in Section 4. Conclusion and future work appear in Section 5. 2 Related Work There has been a lot of previous related work on LM adaptation. Suzuki and Gao (2005) compared different supervised LM adaptation approaches, and showed that three discriminative methods sig-nificantly outperform the maximum a posteriori (MAP) method. For unsupervised LM adaptation, an earlier attempt is a cache-based model (Kuhn and Mori, 1990), developed based on the assump-tion that words appearing earlier in a document are likely to appear again. The cache concept has also been used to increase the probability of unseen but topically related words, for example, the trigger-based LM adaptation using the maximum entropy approach (Rosenfeld, 1996). Latent topic analysis has recently been investi-gated extensively for language modeling. Iyer and Ostendorf (1996) used hard clustering to obtain topic clusters for LM adaptation, where a single topic is assigned to each document. Bellegarda (2000) employed Latent Semantic Analysis (LSA) to map documents into implicit topic sub-spaces and demonstrated significant reduction in perplex-ity and word error rate (WER). Its probabilistic extension, PLSA, is powerful for characterizing topics and documents in a probabilistic space and has been used in LM adaptation. For example, Gildea and Hofmann (1999) reported noticeable perplexity reduction via a dynamic combination of many unigram topic models with a generic trigram model. Proposed by Blei et al. (2003), Latent Dirichlet Allocation (LDA) loosens the constraint of the document-specific fixed weights by using a prior distribution and has quickly become one of the most popular probabilistic text modeling tech-niques. LDA can overcome the drawbacks in the PLSA model, and has been shown to outperform PLSA in corpus perplexity and text classification experiments (Blei et al., 2003). Tam and Schultz (2005) successfully applied the LDA model to un-supervised LM adaptation by interpolating the background LM with the dynamic unigram LM estimated by the LDA model. Hsu and Glass (2006) investigated using hidden Markov model with LDA to allow for both topic and style adaptation. Mrva and Woodland (2006) achieved WER reduc-tion on broadcast conversation recognition using an LDA based adaptation approach that effectively combined the LMs trained from corpora with dif-ferent styles: broadcast news and broadcast con-versation data. In this paper, we investigate unsupervised LM adaptation using clustering and LDA based topic analysis. Unlike the clustering based interpolation method as in (Iyer and Ostendorf, 1996), we ex-plore different distance measure methods for topic analysis. Different from the LDA based framework as in (Tam and Schultz, 2005), we propose a novel dynamic weighting scheme for the topic adapted LM. More importantly, the focus of our work is to investigate the role of named entity information in LM adaptation, which to our knowledge has not been explored.  3 Unsupervised LM Adaptation Integrat-ing Named Entities (NEs) 3.1 Overview of the NE-driven LM Adapta-tion Framework Figure 1 shows our unsupervised LM adaptation framework using NEs. For training, we use the text collection to train the generic word-based N-gram LM. Then we apply named entity recognition (NER) and topic analysis to train multiple topic specific N-gram LMs. During testing, NER is per-formed on each test document, and then a dynami-cally adaptive LM based on the topic analysis result is combined with the general LM. Note that in this figure, we evaluate the performance of LM adaptation using the perplexity measure. We will evaluate this framework for N-best or lattice res-coring in speech recognition in the future. In our experiments, different topic analysis methods combined with different topic matching and adaptive schemes result in several LM adapta-674

tion paradigms, which are described below in de-tails.  Training TextTest TextNERNERLatent Topic AnalysisCompute PerplexityGeneric N-gram TrainingTopic Model TrainingTopic MatchingTopic Model AdaptationModel Interpolation Figure 1. Framework of NE-driven LM adaptation.  3.2 NE-based Clustering for LM Adaptation Clustering is a simple unsupervised topic analysis method. We use NEs to construct feature vectors for the documents, rather than considering all the words as in most previous work. We use the CLUTO1 toolkit to perform clustering. It finds a predefined number of clusters based on a specific criterion, for which we chose the following func-tion: ∑∑=∈=KiSuvkiuvsimSSS1,*21),(maxarg)(L where K is the desired number of clusters, Si is the set of documents belonging to the ith cluster, v and u represent two documents, and sim(v, u) is the similarity between them. We use the cosine dis-tance to measure the similarity between two docu-ments: ||||||||),(uvuvuvsimrrrr⋅⋅=                        (1) where vr and ur are the feature vectors represent-ing the two documents respectively, in our experi-ments composed of NEs. For clustering, the elements in every feature vector are scaled based on their term frequency and inverse document fre-                                                           1 Available at http://glaros.dtc.umn.edu/gkhome/views/cluto quency, a concept widely used in information re-trieval.   After clustering, we train an N-gram LM, called a topic LM, for each cluster using the documents in it. During testing, we identify the ‘topic’ for the test document, and interpolate the topic specific LM with the background LM, that is, if the test document belongs to the cluster S*, we can predict a word wk in the document given the word’s his-tory hk using the following equation: )|()1()|()|(*kkSTopickkGeneralkkhwphwphwp−−+=λλ      (2) where λ is the interpolation weight. We investigate two approaches to find the topic assignment S* for a given test document. (A) cross-entropy measure For a test document d=w1,w2,…,wn with a word distribution pd(w) and a cluster S with a topic LM ps(w), the cross entropy CE(d, S) can be computed as: ∑=−==niisidsdwpwpppHSdCE12))((log)(),(),(    From the information theoretic perspective, the cluster with the lower cross entropy value is ex-pected to be more topically correlated to the test document. For each test document, we compute the cross entropy values according to different clusters, and select the cluster S* that satisfies: ),(minarg1*iKiSdCES≤≤= (B) cosine similarity  For each cluster, its centroid can be obtained by: ∑==inkikiiuncv11 where uik is the vector for the kth document in the ith cluster, and ni is the number of documents in the ith cluster. The distance between the test document and a cluster can then be easily measured by the cosine similarity function as in Equation (1). Our goal here is to find the cluster S* which the test document is closest to, that is, ||||||||maxarg1*iiKicvdcvdS⋅⋅=≤≤rr 675

where dris the feature vector for the test document.   3.3 NE-based LDA for LM Adaptation LDA model (Blei et al., 2003) has been introduced as a new, semantically consistent generative model, which overcomes overfitting and the problem of generating new documents in PLSA. It is a three-level hierarchical Bayesian model. Based on the LDA model, a document d is generated as follows. • Sample a vector of K topic mixture weights θ from a prior Dirichlet distribution with parameter α: ∏=−=Kkkkf11);(αθαθ • For each word w in d, pick a topic k from the multinomial distribution θ. • Pick a word w from the multinomial distri-bution kw,β given the kth topic. For a document d=w1,w2,…wn, the LDA model assigns it the following probability: ∫∏∑⎟⎟⎠⎞⎜⎜⎝⎛⋅===θθαθθβdfdpniKkkkwi);()(11 We use the MATLAB topic Toolbox 1.3 (Grif-fiths et al., 2004) in the training set to obtain the document-topic matrix, DP, and the word-topic matrix, WP. Note that here “words” correspond to the elements in the feature vector used to represent the document (e.g., NEs). In the DP matrix, an en-try cik represents the counts of words in a document di that are from a topic zk (k=1,2,…,K). In the WP matrix, an entry fjk represents the frequency of a word wj generated from a topic zk (k=1,2,…,K) over the training set.  For training, we assign a topic zi* to a document di such that ikKkicz≤≤=1*maxarg. Based on the docu-ments belonging to the different topics, K topic N-gram LMs are trained. This “hard clustering” strat-egy allows us to train an LM that accounts for all the words rather than simply those NEs used in LDA analysis, as well as use higher order N-gram LMs, unlike the ‘unigram’ based LDA in previous work. For a test document d = w1,w2,…,wn that is gen-erated by multiple topics under the LDA assump-tion, we formulate a dynamically adapted topic model using the mixture of LMs from different topics: ∑=−×=KikkzikkadaptLDAhwphwpi1)|()|(γ where )|(kkzhwpi stands for the ith topic LM, and γi is the mixture weight. Different from the idea of dynamic topic adaptation in (Tam and Schultz, 2005), we propose a new weighting scheme to cal-culate γi that directly uses the two resulting matri-ces from LDA analysis during training: ∑==njjjkkdwpwzp1)|()|(γ ∑∑====nqqjjKpjpjkjkwfreqwfreqdwpffwzp11)()()|(,)|( where freq(wj) is the frequency of a word wj in the document d. Other notations are consistent with the previous definitions.  Then we interpolate this adapted topic model with the generic LM, similar to Equation (2): )|()1()|()|(kkadaptLDAkkGeneralkkhwphwphwp−−+=λλ      (3) 4 Experiments 4.1 Experimental Setup  # of files# of words # of NEsTraining Data23,985 7,345,644 590,656Test Data 2,661 831,283 65,867 Table 1. Statistics of our experimental data.  The data set we used is the LDC Mandarin TDT4 corpus, consisting of 337 broadcast news shows with transcriptions. These files were split into small pieces, which we call documents here, ac-cording to the topic segmentation information marked in the LDC’s transcription. In total, there are 26,646 such documents in our data set. We randomly chose 2661 files as the test data (which is balanced for different news sources). The rest was used for topic analysis and also generic LM training. Punctuation marks were used to deter-mine sentences in the transcriptions. We used the NYU NE tagger (Ji and Grishman, 2005) to recog-nize four kinds of NEs: Person, Location, Organi-676

zation, and Geo-political. Table 1 shows the statis-tics of the data set in our experiments.  We trained trigram LMs using the SRILM tool-kit (Stolcke, 2002). A fixed weight (i.e., λ in Equation (2) and (3)) was used for the entire test set when interpolating the generic LM with the adapted topic LM. Perplexity was used to measure the performance of different adapted LMs in our experiments.  4.2 Latent Topic Analysis Results   Topic # of  Files Top 10 Descriptive Items  (Translated from Chinese) 1 3526 U.S., Israel, Washington, Palestine, Bush, Clinton, Gore, Voice of Amer-ica, Mid-East, Republican Party 2 3067 Taiwan, Taipei, Mainland, Taipei City, Chinese People’s Broadcasting Station, Shuibian Chen,  the Execu-tive Yuan, the Legislative Yuan, De-mocratic Progressive Party, Nationalist Party 3 4857 Singapore, Japan, Hong Kong, Indo-nesia, Asia, Tokyo, Malaysia, Thai-land, World, China 4 4495 World, German, Landon, Russia, France, England, Xinhua News Agency, Europe, U.S., Italy Cluster-ing Based 5 7586 China, Beijing, Nation, China Central Television Station, Xinhua News Agency, Shanghai, World, State Council, Zemin Jiang, Beijing City1 5859 China, Japan, Hong Kong, Beijing, Shanghai, World, Zemin Jiang, Ma-cao,  China Central Television Sta-tion, Africa 2 3794 U.S., Bush, World,  Gore,  South Korea, North Korea, Clinton, George Walker Bush, Asia, Thailand 3 4640 Singapore, Indonesia, Team, Israel, Europe, Germany, England, France, Palestine, Wahid 4 4623 Taiwan, Russia, Mainland, India, Taipei, Shuibian Chen, Philippine, Estrada, Communist Party of China, RUS. LDA Based 5 4729 Xinhua News Agency, Nation, Bei-jing, World, Canada, Sydney, Brazil, Beijing City, Education Ministry, Cuba Table 2.  Topic analysis results using clustering and LDA (the number of documents and the top 10 words (NEs) in each cluster).  For latent topic analysis, we investigated two ap-proaches using named entities, i.e., clustering and LDA. 5 latent topics were used in both approaches. Table 2 illustrates the resulting topics using the top 10 words in each topic. We can see that the words in the same cluster share some similarity and that the words in different clusters seem to be ‘topi-cally’ different. Note that errors from automatic NE recognition may impact the clustering results. For example, ‘队/team’ in the table (in topic 3 in LDA results) is an error and is less discriminative for topic analysis. Table 3 shows the perplexity of the test set us-ing the background LM (baseline) and each of the topic LMs, from clustering and LDA respectively. We can see that for the entire test set, a topic LM generally performs much worse than the generic LM. This is expected, since the size of a topic clus-ter is much smaller than that of the entire training set, and the test set may contain documents from different topics. However, we found that when us-ing an optimal topic model (i.e., the topic LM that yields the lowest perplexity among the 5 topic LMs), 23.45% of the documents in the test set have a lower perplexity value than that obtained from the generic LM. This suggests that a topic model could benefit LM adaptation and motivates a dy-namic topic adaptation approach for different test documents.   Perplexity Baseline 502.02 CL-1 1054.36 CL-2 1399.16 CL-3 919.237 CL-4 962.996 CL-5 981.072 LDA-1 1224.54 LDA-2 1375.97 LDA-3 1330.44 LDA-4 1328.81 LDA-5 1287.05 Table 3. Perplexity results using the baseline LM vs. the single topic LMs.  4.3 Clustering vs. LDA Based LM Adaptation In this section, we compare three LM adaptation paradigms. As we discussed in Section 3, two of them are clustering based topic analysis, but using different strategies to choose the optimal cluster; and the third one is based on LDA analysis that 677

uses a dynamic weighting scheme for adapted topic mixture model.  Figure 2 shows the perplexity results using dif-ferent interpolation parameters with the general LM.  5 topics were used in both clustering and LDA based approaches (as in Section 4.2). “CL-CE” means clustering based topic analysis via cross entropy criterion, “CL-Cos” represents clus-tering based topic analysis via cosine distance cri-terion, and “LDA-MIX” denotes LDA based topic mixture model, which uses 5 mixture topic LMs.  4404504604704804905005105205305400.40.50.60.70.8λPerplexityBaselineCL-CECL-CosLDA-MIX Figure 2. Perplexity using different LM adaptation approaches and different interpolation weightsλ with the general LM.  We observe that all three adaptation approaches outperform the baseline when using a proper inter-polation weight. “CL-CE” yields the best perplex-ity of 469.75 when λ is 0.5, a reduction of 6.46% against the baseline perplexity of 502.02. For clus-tering based adaptation, between the two strategies used to determine the topic for a test document, “CL-CE” outperforms “CL-Cos”. This indicates that the cosine distance measure using only names is less effective than cross entropy for LM adapta-tion. In addition, cosine similarity does not match perplexity as well as the CE-based distance meas-ure. Similarly, for the LDA based approach, using only NEs may not be sufficient to find appropriate weights for the topic model. This also explains the bigger interpolation weight for the general LM in CL-Cos and LDA-MIX than that in “CL-CE”.   For a fair comparison between the clustering and LDA based LM adaptation approaches, we also evaluated using the topic mixture model for the clustering based approach and using only one topic in the LDA based method. For clustering based adaptation, we constructed topic mixture models using the weights obtained from a linear normalization of the two distance measures pre-sented in Section 3.2. In order to use only one topic model in LDA based adaptation, we chose the topic cluster that has the largest weight in the adapted topic mixture model (as in Sec 3.3). Table 4 shows the perplexity for the three approaches (CL-Cos, CL-CE, and LDA) using the mixture topic models versus a single topic LM. We observe similar trends as in Figure 2 when changing the interpolation weight λwith the generic LM; there-fore, in Table 4 we only present results for one op-timal interpolation weight.   Single-Topic Mixture-TopicCL-Cos (λ=0.7)498.01 497.86 CL-CE (λ=0.5)469.75 483.09 LDA (λ=0.7) 488.96 489.14 Table 4. Perplexity results using the adapted topic model (single vs. mixture) for clustering and LDA based approaches.  We can see from Table 4 that using the mixture model in clustering based adaptation does not im-prove performance. This may be attributed to how the interpolation weights are calculated. For ex-ample, only names are used in cosine distance, and the normalized distance may not be appropri-ate weights. We also notice negligible difference when only using one topic in the LDA based framework. This might be because of the small number of topics currently used. Intuitively, using a mixture model should yield better performance, since LDA itself is based on the assumption of generating words from multiple topics. We will investigate the impact of the number of topics on LM adaptation in Section 4.5. 4.4 Effect of Different Feature Configura-tions on LM Adaptation We suspect that using only named entities may not provide enough information about the ‘topics’ of the documents, therefore we investigate expanding the feature vectors with other words. Since gener-ally content words are more indicative of the topic of a document than function words, we used a POS tagger (Hillard et al., 2006) to select words for la-tent topic analysis. We kept words with three POS classes: noun (NN, NR, NT), verb (VV), and modi-678

fier (JJ), selected from the LDC POS set2. This is similar to the removal of stop words widely used in information retrieval.  Figure 3 shows the perplexity results for three different feature configurations, namely, all-words (w), names (n), and names plus syntactically fil-tered items (n+), for the CL-CE and LDA based approaches. The LDA based LM adaptation para-digm supports our hypothesis. Using named infor-mation instead of all the words seems to efficiently eliminate redundant information and achieve better performance. In addition, expanding named enti-ties with syntactically filtered items yields further improvement. For CL-CE, using named informa-tion achieves the best result among the three con-figurations. This might be because that the clustering method is less powerful in analyzing the principal components as well as dealing with re-dundant information than the LDA model.  4604654704754804854904955005050.40.50.60.70.8λPerplexityCL-CE(w)CL-CE(n)CL-CE(n+)LDA-MIX(w)LDA-MIX(n)LDA-MIX(n+) Figure 3. Comparison of perplexity using different feature configurations. 4.5 Impact of Predefined Topic Number on LM Adaptation LDA based topic analysis typically uses a large number of topics to capture the fine grained topic space. In this section, we evaluate the effect of the number of topics on LM adaptation. For compari-son, we evaluate this for both LDA and CL-CE, similar to Section 4.3. We use the “n+” feature configuration as in Section 4.4, that is, names plus POS filtered items. When using a single-topic adapted model in the LDA or CL-CE based ap-proach, finer-grained topic analysis (i.e., increasing the number of topics) leads to worse performance mainly because of the smaller clusters for each topic; therefore, we only show results here using                                                            2 See http://www.cis.upenn.edu/~chinese/posguide.3rd.ch.pdf the mixture topic adapted models. Figure 4 shows the perplexity results using different numbers of topics. The interpolation weightλwith the general LM is 0.5 in all the experiments. For the topic mix-ture LMs, we used a maximum of 9 mixtures (a limitation in the current SRILM toolkit) when the number of topics is greater than 9.  We observe that as the number of topics in-creases, the perplexity reduces significantly for LDA. When the number of topics is 50, the adapted LM using LDA achieves a perplexity re-duction of 11.35% compared to using 5 topics, and 14.23% against the baseline generic LM. Therefore, using finer-grained multiple topics in dynamic ad-aptation improves system performance. When the number of topics increases further, e.g., to 100, the performance degrades slightly. This might be due to the limitation of the number of the topic mix-tures used. A similar trend is observable for the CL-CE approach, but the effect of the topic num-ber is much greater in LDA than CL-CE.    435.2477.2430.6445.8485.7477.3471.8467.2483.1485.1400420440460480500n=5n=10n=20n=50n=100# of TopicsPerplexityLDACL-CE Figure 4. Perplexity results using different prede-fined numbers of topics for LDA and CL-CE.  4.6 Discussion As we know, although there is an increasing amount of training data available for LM training, it is still only for limited domains and styles. Creat-ing new training data for different domains is time consuming and labor intensive, therefore it is very important to develop algorithms for LM adaptation. We investigate leveraging named entities in the LM adaptation task. Though some errors of NER may be introduced, our experimental results have shown that exploring named information for topic analysis is promising for LM adaptation.  Furthermore, this framework may have other advantages. For speech recognition, using NEs for topic analysis can be less vulnerable to recognition 679

errors. For instance, we may add a simple module to compute the similarity between two NEs based on the word tokens or phonetics, and thus compen-sate the recognition errors inside NEs. Whereas, word-based models, such as the traditional cache LMs, may be more sensitive to recognition errors that are likely to have a negative impact on the prediction of the current word. From this point of view, our framework can potentially be more ro-bust in the speech processing task. In addition, the number of NEs in a document is much smaller than that of the words, as shown in Table 1; hence, us-ing NEs can also reduce the computational com-plexity, in particular in topic analysis for training. 5 Conclusion and Future Work We compared several unsupervised LM adaptation methods leveraging named entities, and proposed a new dynamic weighting scheme for topic mixture model based on LDA topic analysis. Experimental results have shown that the NE-driven LM adapta-tion approach outperforms using all the words, and yields perplexity reduction compared to the base-line generic LM. In addition, we find that for the LDA based method, adding other content words, combined with an increased number of topics, can further improve the performance, achieving up to 14.23% perplexity reduction compared to the base-line LM. The experiments in this paper combine models primarily through simple linear interpolation. Thus one direction of our future work is to develop algo-rithms to automatically learn appropriate interpola-tion weights. In addition, our work in this paper has only showed promising results in perplexity reduction. We will investigate using this frame-work of LM adaptation for N-best or lattice rescor-ing in speech recognition. Acknowledgements We thank Mari Ostendorf, Mei-Yuh Hwang, and Wen Wang for useful discussions, and Heng Ji for sharing the Mandarin named entity tagger. This work is supported by DARPA under Contract No. HR0011-06-C-0023. Any opinions expressed in this material are those of the authors and do not necessarily reflect the views of DARPA.   References  J. Bellegarda. 2000. Exploiting Latent Semantic Infor-mation in Statistical Language Modeling. In IEEE Transactions on Speech and Audio Processing. 88(80):1279-1296.   D. Blei, A. Ng, and M. Jordan. 2003. Latent Dirichlet Allocation. Journal of Machine Learning Research. 3:993-1022. D. Gildea and T. Hofmann. 1999. Topic-Based Lan-guage Models using EM. In Proc. of Eurospeech. T. Griffiths, M. Steyvers, D. Blei, and J. Tenenbaum. 2004. Integrating Topics and Syntax. Adv. in Neural Information Processing Systems. 17:537-544. D. Hillard, Z. Huang, H. Ji, R. Grishman, D. Hakkani-Tur, M. Harper, M. Ostendorf, and W. Wang. 2006. Impact of Automatic Comma Prediction on POS/Name Tagging of Speech. In Proc. of the First Workshop on Spoken Language Technology (SLT).  P. Hsu and J. Glass. 2006. Style & Topic Language Model Adaptation using HMM-LDA. In Proc. of EMNLP, pp:373-381. R. Iyer and M. Ostendorf. 1996. Modeling Long Dis-tance Dependence in Language: Topic Mixtures vs. Dynamic Cache Models. In Proc. of ICSLP. H. Ji and R. Grishman. 2005. Improving NameTagging by Reference Resolution and Relation Detection. In Proc. of ACL. pp: 411-418. R. Kneser and V. Steinbiss. 1993. On the Dynamic Ad-aptation of Stochastic language models. In Proc. of ICASSP, Vol 2, pp: 586-589. R. Kuhn and R.D. Mori. 1990. A Cache-Based Natural Language Model for Speech Recognition. In IEEE Transactions on Pattern Analysis and Machine Intel-ligence, 12: 570-583.  D. Mrva and P.C. Woodland. 2006. Unsupervised Lan-guage Model Adaptation for Mandarin Broadcast Conversation Transcription. In Proc. of INTERSPEECH, pp:2206-2209. R. Rosenfeld. 1996. A Maximum Entropy Approach to Adaptive Statistical Language Modeling. Computer, Speech and Language, 10:187-228. A. Stolcke. 2002. SRILM – An Extensible Language Modeling Toolkit. In Proc. of ICSLP. H. Suzuki and J. Gao. 2005. A Comparative Study on Language Model Adaptation Techniques Using New Evaluation Metrics, In Proc. of HLT/EMNLP. Y.C. Tam and T. Schultz. 2005. Dynamic Language Model Adaptation Using Variational Bayes Inference. In Proc. of INTERSPEECH, pp:5-8. Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 680–687,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

680

CoordinateNounPhraseDisambiguationinaGenerativeParsingModelDeirdreHogan∗ComputerScienceDepartmentTrinityCollegeDublinDublin2,Irelanddhogan@computing.dcu.ieAbstractInthispaperwepresentmethodsforim-provingthedisambiguationofnounphrase(NP)coordinationwithintheframeworkofalexicalisedhistory-basedparsingmodel.Aswellasreducingnoiseinthedata,welookatmodellingtwomainsourcesofinformationfordisambiguation:symmetryinconjunctstructure,andthedependencybetweencon-junctlexicalheads.Ourchangestothebase-linemodelresultinanincreaseinNPcoor-dinationdependencyf-scorefrom69.9%to73.8%,whichrepresentsarelativereductioninf-scoreerrorof13%.1IntroductionCoordinationdisambiguationisarelativelylittlestudiedarea,yetthecorrectbracketingofcoordina-tionconstructionsisoneofthemostdifﬁcultprob-lemsfornaturallanguageparsers.IntheCollinsparser(Collins,1999),forexample,dependenciesinvolvingcoordinationachieveanf-scoreaslowas61.8%,byfartheworstperformanceofalldepen-dencytypes.Takethephrasebusloadsofexecutivesandtheirwives(takenfromtheWSJtreebank).Thecoordi-natingconjunction(CC)andandthenounphrasetheirwivescouldattachtothenounphraseexec-utives,asillustratedinTree1,Figure1.Alterna-tively,theirwivescouldbeincorrectlyconjoinedtothenounphrasebusloadsofexecutivesasinTree2,Figure1.∗NowattheNationalCentreforLanguageTechnology,DublinCityUniversity,Ireland.AswithPPattachment,mostpreviousattemptsattacklingcoordinationasasubproblemofparsinghavetreateditasaseparatetasktoparsinganditisnotalwaysobvioushowtointegratethemethodsproposedfordisambiguationintoexistingparsingmodels.Wethereforeapproachcoordinationdisam-biguation,notasaseparatetask,butfromwithintheframeworkofagenerativeparsingmodel.Asnounphrasecoordinationaccountsforover50%ofcoordinationdependencyerrorinourbase-linemodelwefocusonNPcoordination.Us-ingamodelbasedonthegenerativeparsingmodelof(Collins,1999)Model1,weattempttoimprovetheabilityoftheparsingmodeltomakethecorrectcoordinationdecisions.Thisisdoneinthecontextofparsereranking,wherethen-bestparsesoutputfromBikel’sparser(Bikel,2004)arererankedac-cordingtoagenerativehistory-basedmodel.InSection2wesummarisepreviousworkonco-ordinationdisambiguation.Thereisoftenaconsid-erablebiastowardsymmetryinthesyntacticstruc-tureoftwoconjunctsandinSection3weintroducenewparameterclassestoallowthemodeltoprefersymmetryinconjunctstructure.Section4iscon-cernedwithmodellingthedependencybetweencon-junctheadwordsandbeginsbylookingathowthedifferenthandlingofcoordinationinnounphrasesandbasenounphrases(NPB)affectscoordinationdisambiguation.1Welookathowwemightimprovethemodel’shandlingofcoordinatehead-headde-pendenciesbyalteringthemodelsothatacommon1Abasenounphrase,asdeﬁnedin(Collins,1999),isanounphrasewhichdoesnotdirectlydominateanothernounphrase,unlessthatnounphraseispossessive.681

1.NPNPNPBbusloadsPPofNPNPNPBexecutivesandNPNPBtheirwives2.NPNPNPBbusloadsPPofNPNPBexecutivesandNPNPBtheirwivesFigure1:Tree1.Thecorrectnounphraseparse.Tree2.Theincorrectparseforthenounphrase.parameterclassisusedforcoordinatewordprob-abilityestimationinbothNPsandNPBs.InSec-tion4.2wefocusonimprovingtheestimationofthisparameterclassbyincorporatingBNCdata,andameasureofwordsimilaritybasedonvectorcosinesimilarity,toreducedatasparseness.InSection5wesuggestanewhead-ﬁndingruleforNPBssothatthelexicalisationprocessforcoordinateNPBsismoresimilartothatofotherNPs.Section6examinesinconsistenciesintheannota-tionofcoordinateNPsinthePennTreebankwhichcanleadtoerrorsincoordinationdisambiguation.Weshowhowsomecoordinatenounphraseincon-sistenciescanbeautomaticallydetectedandcleanedfromthedatasets.Section7detailshowthemodelisevaluated,presentstheexperimentsmadeandgivesabreakdownofresults.2PreviousWorkMostpreviousattemptsattacklingcoordinationhavefocusedonaparticulartypeofNPcoordinationtodisambiguate.BothResnik(1999)andNakovandHearst(2005)considerNPcoordinationsoftheformn1andn2n3wheretwostructuralanalysesarepos-sible:((n1andn2)n3)and((n1)and(n2n3)).TheyaimtoshowmorestructurethanisshownintreesfollowingthePennguidelines,whereasinourap-proachweaimtoreproducePennguidelinetrees.Toresolvetheambiguities,Resnikcombinesnum-beragreementinformationofcandidateconjoinednouns,aninformationtheoreticmeasureofsemanticsimilarity,andameasureoftheappropriatenessofnoun-nounmodiﬁcation.NakovandHearst(2005)disambiguatebycombiningWeb-basedstatisticsonheadwordco-occurrenceswithothermainlyheuris-ticinformationsources.Aprobabilisticapproachispresentedin(Gold-berg,1999),whereanunsupervisedmaximumen-tropystatisticalmodelisusedtodisambiguatecoor-dinatenounphrasesoftheformn1prepositionn2ccn3.Heretheproblemisframedasanattachmentdecision:doesn3attach‘high’totheﬁrstnoun,n1,or‘low’ton2?In(AgarwalandBoggess,1992)thetaskistoidentifypre-CCconjunctswhichappearintextthathasbeenpart-of-speech(POS)taggedandsemi-parsed,aswellastaggedwithsemanticlabelsspe-ciﬁctothedomain.Theidentiﬁcationofthepre-CCconjunctisbasedonheuristicswhichchoosethepre-CCconjunctthatmaximisesthesymmetrybe-tweenpre-andpost-CCconjuncts.Insofaraswedonotseparatecoordinationdis-ambiguationfromtheoverallparsingtask,ourap-proachresemblestheeffortstoimprovecoordi-nationdisambiguationin(Kurohashi,1994;Rat-naparkhi,1994;CharniakandJohnson,2005).In(Kurohashi,1994)coordinationdisambiguationiscarriedoutastheﬁrstcomponentofaJapanesedependencyparserusingatechniquewhichcalcu-latessimilaritybetweenseriesofwordsfromtheleftandrightofaconjunction.SimilarityismeasuredbasedonmatchingPOStags,matchingwordsandathesaurus-basedmeasureofsemanticsimilarity.InboththediscriminativererankerofRatnaparkhietal.(1994)andthatofCharniakandJohnson(2005)featuresareincludedtocapturesyntacticparallelismacrossconjunctsatvariousdepths.3ModellingSymmetryBetweenConjunctsThereisoftenaconsiderablebiastowardsymme-tryinthesyntacticstructureoftwoconjuncts,seeforexample(Dubeyetal.,2005).TakeFigure2:Ifwetakeaslevel0thelevelinthecoordinatesub-682

NP1(plains)NP2(plains)NP3(plains)DT6theJJ5highNNS4plainsPP7(of)IN8ofNP9(Texas)NNP10TexasCC11andNP11(states)NP12(states)DT15theJJ14northernNNS13statesPP16(of)IN17ofNP18(Delta)DT20theNNP19DeltaFigure2:Exampleofsymmetryinconjunctstructureinalexicalisedsubtree.treewherethecoordinatingconjunctionCCoccurs,thenthereisexactsymmetryinthetwoconjunctsintermsofnon-terminallabelsandheadwordpart-of-speechtagsforlevels0,1and2.Learningabiastowardparallelisminconjunctsshouldimprovetheparsingmodel’sabilitytocorrectlyattachacoordi-nationconjunctionandsecondconjuncttothecor-rectpositioninthetree.Inhistory-basedmodels,featuresarelimitedtobeingfunctionsofthetreegeneratedsofar.Thetaskistoincorporateafeatureintothemodelthatcap-turesaparticularbiasyetstilladherestoderivation-basedrestrictions.Parsesaregeneratedtop-down,head-ﬁrst,left-to-right.EachnodeinthetreeinFigure2isannotatedwiththeorderthenodesaregenerated(weomit,forthesakeofclarity,thegen-erationoftheSTOPnodes).Notethatwhenthedecisiontoattachthesecondconjuncttotheheadconjunctisbeingmade(i.e.Step11,whentheCCandNP(states)nodesarebeinggenerated)thesub-treerootedatNP(states)hasnotyetbeengenerated.Thusatthepointthattheconjunctattachmentde-cisionismadeitisnotpossibletouseinformationaboutsymmetryofconjunctstructure,asthestruc-tureofthesecondconjunctisnotyetknown.Itispossible,however,toconditiononstructureofthealreadygeneratedheadconjunctwhenbuild-ingtheinternalstructureofthesecondconjunct.Inourmodelwhenthestructureofthesecondconjunctisbeinggeneratedweconditiononfeatureswhicharefunctionsoftheﬁrstconjunct.Whengenerat-inganodeNiinthesecondconjunct,weretrievethecorrespondingnodeNipreCCintheﬁrstconjunct,viaalefttorighttraversaloftheﬁrstconjunct.Forexample,fromFigure2thepre-CCnodeNP(Texas)isthenodecorrespondingtoNP(Delta)inthepost-CCconjunct.FromNipreCCweextractinformation,suchasitspart-of-speech,foruseasafeaturewhenpredictingaPOStagforthecorrespondingnodeinthepost-CCconjunct.Whengeneratingasecondconjunct,insteadoftheusualparameterclassesforestimatingtheprob-abilityoftheheadlabelChandthePOSlabelofadependentnodeti,wecreatedtwonewparameterclasseswhichareusedonlyinthegenerationofsec-ondconjunctnodes:PccCh(Ch|γ(headC),Cp,wp,tp,tgp,depth)(1)Pccti(ti|α(headC),dir,Cp,wp,tp,dist,ti1,ti2,depth)(2)whereγ(headC)returnsthenon-terminallabelofNipreCCforthenodeinquestionandα(headC)re-turnsthePOStagofNipreCC.Bothfunctionsreturn+NOMATCH+ifthereisnoNipreCCforthenode.Depthisthelevelofthepost-CCconjunctnodebe-inggenerated.4ModellingCoordinateHeadWordsSomenounpairsaremorelikelytobeconjoinedthanothers.TakeagainthetreesinFigure1.ThetwoheadnounscoordinatedinTree1areexecu-tivesandwives,andinTree2:busloadsandwives.Clearly,theformerpairofheadnounsismorelikelyand,forthepurposeofdiscrimination,themodelwouldbeneﬁtifitcouldlearnthatexecutivesandwivesisamorelikelycombinationthanbusloadsandwives.Bilexicalhead-headdependenciesofthetypefoundincoordinatestructuresareasomewhatdif-683

ferentclassofdependencytomodiﬁer-headdepen-dencies.Inthefatcat,forexample,thereisclearlyoneheadtothenounphrase:cat.Incatsanddogshowevertherearetwoheads,thoughintheparsingmodeljustoneischosen,somewhatarbitrarily,toheadtheentirenounphrase.Inthebaselinemodelthereisessentiallyonepa-rameterclassfortheestimationofwordprobabili-ties:Pword(wi|H(i))(3)wherewiisthelexicalheadofconstituentiandH(i)isthehistoryoftheconstituent.Thehistoryismadeupofconditioningfeatureschosenfromstruc-turethathasalreadybeendeterminedinthetop-downderivationofthetree.InSection4.1wediscusshowthoughthecoordi-natehead-headdependencyiscapturedforNPs,itisnotcapturedforNPBs.Welookathowwemightimprovethemodel’shandlingofcoordinatehead-headdependenciesbyalteringthemodelsothatacommonparameterclassin(4)isusedforcoordi-natewordprobabilityestimationinbothNPsandNPBs.PcoordWord(wi|wp,H(i))(4)InSection4.2wefocusonimprovingtheestimationofthisparameterclassbyreducingdatasparseness.4.1ExtendingPcoordWordtoCoordinateNPBsInthebaselinemodeleachnodeinthetreeisan-notatedwithacoordinationﬂagwhichissettotrueforthenodeimmediatelyfollowingthecoordinatingconjunction.ForcoordinateNPsthehead-headde-pendencyiscapturedwhenthisﬂagissettotrue.InFigure1,discardingforsimplicitytheotherfeaturesinthehistory,theprobabilityofthecoordinateheadwives,isestimatedinTree1as:Pword(wi=wives|coord=true,wp=executives,...)(5)andinTree2:Pword(wi=wives|coord=true,wp=busloads,...)(6)wherewpistheheadwordofthenodetowhichthenodeheadedbywiisattachingandcoordistheco-ordinationﬂag.UnlikeNPs,inNPBs(i.e.ﬂat,non-recursiveNPs)thecoordinationﬂagisnotusedtomarkwhetheranodeisacoordinatedheadornot.ThisﬂagisalwayssettofalseforNPBs.Inaddition,modiﬁerswithinNPBsareconditionedonthepreviouslygeneratedmodiﬁerratherthantheheadofthephrase.2ThismeansthatinanNPBsuchas(catsanddogs),theestimateforthewordcatswilllooklike:Pword(wi=cats|coord=false,wp=and,...)(7)Inournewmodel,forNPs,whenthecoordinationﬂagissettotrue,weusetheparameterclassin(4)toestimatetheprobabilityofonelexicalheadnoun,givenanother.ForNPBs,ifanounisgenerateddi-rectlyafteraCCthenitistakentobeacoordinatehead,wi,andconditionedonthenoungeneratedbe-forethecoordinatingconjunction,whichischosenaswp,andalsoestimatedusing(4).4.2EstimatingthePcoordWordparameterclassDataforbilexicalstatisticsareparticularlysparse.Inordertodecreasethesparsenessofthecoordinateheadnoundata,weextractedfromtheBNCexam-plesofcoordinateheadnounpairs.Weextractedallnounpairsoccurringinapatternoftheform:nounccnoun,aswellaslistsofanynumberofnounsseparatedbycommasandendinginccnoun.3TothisdataweaddedallheadnounpairsfromtheWSJthatoccurredtogetherinacoordinatenounphrase,identiﬁedwhenthecoordinationﬂagwassettotrue.EveryoccurrenceniCCnjwasalsocountedasanoccurrenceofnjCCni.Thisfurtherhelpsreducesparseness.Theprobabilityofonenoun,nibeingcoordinatedwithanothernjcanbecalculatedsimplyas:Plex(ni|nj)=|ninj||nj|(8)Againtoreducedatasparseness,weintroduceameasureofwordsimilarity.Awordcanberep-resentedasavectorwhereeverydimensionofthevectorrepresentsanotherwordtype.Thevaluesofthevectorcomponents,thetermweights,arederivedfromwordco-occurrencecounts.Cosinesimilar-itybetweentwowordvectorscanthenbeusedtomeasurethesimilarityoftwowords.Measuresof2Afullexplanationofthehandlingofcoordinationinthemodelisgivenin(Bikel,2004).3ExtractingcoordinatenounpairsfromtheBNCinsuchafashionfollowsworkonnetworksofconceptsdescribedin(Widdows,2004).684

similaritybetweenwordsbasedonsimilarityofco-occurrencevectorshavebeenusedbefore,forexam-ple,forwordsensedisambiguation(Sch¨utze,1998)andforPP-attachmentdisambiguation(ZhaoandLin,2004).Ourmeasureresemblesthatof(Cara-ballo,99)whereco-occurrenceisalsodeﬁnedwithrespecttocoordinationpatterns,althoughtheexper-imentaldetailsintermsofdatacollectionandvectortermweightsdiffer.Wecannowincorporatethesimilaritymeasureintotheprobabilityestimateof(8)togiveanewk-NNstylemethodofestimatingbilexicalstatisticsbasedonweightingeventsaccordingtothewordsimilaritymeasure:Psim(ni|nj)=Pnx∈N(nj)sim(nj,nx)|ninx|Pnx∈N(nj)sim(nj,nx)|nx|(9)wheresim(nj,nx)isasimilarityscorebetweenwordsnjandnxandN(nj)isthesetofwordsintheneighbourhoodofnj.Thisneighbourhoodcanbebasedonthek-nearestneighboursofnj,wherenearnessismeasuredwiththesimilarityfunction.Inordertosmooththebilexicalestimatein(9)wecombineitwithanotherestimate,trainedfromWSJdata,bywayoflinearinterpolation:PcoordWord(ni|nj)=λnjPsim(ni|nj)+(1−λnj)PMLE(ni|ti)(10)wheretiisthePOStagofwordni,PMLE(ni|ti)isthemaximum-likelihoodestimatecalculatedfromannotatedWSJdata,andλnjiscalculatedasin(11).In(11)weadapttheWitten-Bellmethodforthecalculationoftheweightλ,asusedintheCollinsparser,sothatitincorporatesthesimilaritymeasureforallwordsintheneighbourhoodofnj.λnj=Pnx∈N(nj)sim(nj,nx)|nx|Pnx∈N(nj)sim(nj,nx)(|nx|+CD(nx))(11)whereCisaconstantthatcanbeoptimisedusingheld-outdataandD(nj)isthediversityofawordnj:thenumberofdistinctwordswithwhichnjhasbeencoordinatedinthetrainingset.Theestimatein(9)canbeviewedastheestimatewiththemoregeneralhistorycontextthanthatof(8)becausethecontextincludesnotonlynjbutalsowordssimilartonj.TheﬁnalprobabilityestimateforPcoordWordiscalculatedasthemostspeciﬁces-timate,Plex,combinedviaregularWitten-Bellinter-polationwiththeestimatein(10).5NPBHead-FindingRulesHead-ﬁndingrulesforcoordinateNPBsdifferfromcoordinateNPs.4Takethefollowingtwoversionsofthenounphrasehardworkandharmony:(c)(NP(NPBhardworkandharmony))and(d)(NP(NP(NPBhardwork))and(NP(NPBharmony))).Intheﬁrstexample,harmonyischosenasheadwordoftheNP;inexample(d)theheadoftheentireNPiswork.Thechoiceofheadaffectsthevariousdependenciesinthemodel.However,inthecaseoftwocoordinateNPswhich,asintheaboveexample,coverthesamespanofwordsanddifferonlyinwhetherthecoordi-natenounphraseisﬂatasin(c)orstructuredasin(d),thechoiceofheadforthephraseisnotparticu-larlyinformative.Inbothcasestheheadwordsbe-ingcoordinatedarethesameandeitherwordcouldplausiblyheadthephrase;discriminationbetweentreesinsuchcasesshouldnotbeinﬂuencedbythechoiceofhead,butratherbyother,salientfeaturesthatdistinguishthetrees.5Wewouldliketoalterthehead-ﬁndingrulesforcoordinateNPBssothat,incaseslikethoseabove,thewordchosentoheadtheentirecoordinatenounphrasewouldbethesameforbothbaseandnon-basenounphrases.Weexperimentwithslightlymodiﬁedhead-ﬁndingrulesforcoordinateNPBs.InanNPBsuchasNPB→n1CCn2n3,theheadrulesremainunchangedandtheheadofthephraseis(usu-ally)therightmostnouninthephrase.Thus,whenn2isimmediatelyfollowedbyanothernounthede-faultistoassumenominalmodiﬁercoordinationandtheheadrulesstaythesame.ThemodiﬁcationwemaketotheheadrulesforNPBsisasfollows:whenn2isnotimmediatelyfollowedbyanounthenthenounchosentoheadtheentirephraseisn1.6InconsistenciesinWSJCoordinateNPAnnotationAninspectionofNPcoordinationerrorinthebase-linemodelrevealedinconsistenciesinWSJannota-4See(Collins,1999)fortherulesusedinthebaselinemodel.5Forexample,itwouldbebetterifdiscriminationwaslargelybasedonwhetherhardmodiﬁesbothworkandharmony(c),orwhetheritmodiﬁesworkalone(d).685

tion.Inthissectionweoutlinesometypesofco-ordinateNPinconsistencyandoutlineamethodfordetectingsomeoftheseinconsistencies,whichwelaterusetoautomaticallycleannoisefromthedata.Eliminatingnoisefromtreebankshasbeenprevi-ouslyusedsuccessfullytoincreaseoverallparserac-curacy(DickinsonandMeurers,2005).TheannotationofNPsinthePennTreebank(Biesetal.,1995)followssomewhatdifferentguidelinestothatofothersyntacticcategories.Becausetheirinterpretationissoambiguous,nointernalstructureisshownfornominalmodiﬁers.ForNPswithmorethanoneheadnoun,iftheonlyunsharedmodiﬁersintheconstituentarenominalmodiﬁers,thenaﬂatstructureisalsogiven.Thusin(NPtheManhattanphonebookandtourguide)6aﬂatstructureisgivenbecausealthoughtheisanon-nominalmodiﬁer,itisshared,modifyingbothtourguideandphonebook,andallothermodiﬁersinthephrasearenominal.However,wefoundthatoutof1,417examplesofNPcoordinationinsections02to21,involvingphrasescontainingonlynouns(commonnounsoramixtureofcommonandpropernouns)andtheco-ordinatingconjunction,asmanyas21.3%,contrarytotheguidelines,weregiveninternalstructure,in-steadofaﬂatannotation.Whenallpropernounsareinvolvedthisphenomenonisevenmorecommon.7Anothercommonsourceofinconsistencyinco-ordinatenounphrasebracketingoccurswhenanon-nominalmodiﬁerappearsinthecoordinatenounphrase.Aspreviouslydiscussed,accordingtotheguidelinesthemodiﬁerisannotatedﬂatifitisshared.Whenthenon-nominalmodiﬁerisun-shared,moreinternalstructureisshown,asin:(NP(NP(NNSfangs))(CCand)(NP(JJpointed)(NNSears))).However,thefollowingtwostruc-turedphrases,forexample,weregivenacom-pletelyﬂatstructureinthetreebank:(a)(NP(NP(NNoversight))(CCand)(NP(JJdisciplinary)(NNSprocedures))),(b)(NP(ADJP(JJmoderate)(CCand)(JJlow-cost))(NNhousing)).IfwefollowtheguidelinesthenanycoordinateNPBwhichendswiththefollowingtagsequencecanbeautomat-icallydetectedasincorrectlybracketed:CC/non-nominalmodiﬁer/noun.Thisisbecauseeitherthe6InthissectionwedonotshowtheNPBlevels.7Intheguidelinesitisrecognisedhoweverthatpropernamesarefrequentlyannotatedwithinternalstructure.non-nominalmodiﬁer,whichisunambiguouslyun-shared,ispartofanounphraseas(a)above,oritconjoinedwithanothermodiﬁerasin(b).Wefound202examplesofthisinthetrainingset,outofatotalof4,895coordinatebasenounphrases.Finally,inconsistenciesinPOStaggingcanalsoleadtoproblemswithcoordination.Takethebi-gramexecutiveofﬁcer.Wefound151examplesinthetrainingsetofabasenounphrasewhichendedwiththisbigram.48%ofthecaseswerePOStaggedJJNN,52%taggedNNNN.8Thishasrepercussionsforcoordinatenounphrasestructure,asthepresenceofanadjectivalpre-modiﬁerindicatesastructuredannotationshouldbegiven.Theseinconsistenciesposeproblemsbothfortrainingandtesting.Witharelativelylargeamountofnoiseinthetrainingsetthemodellearnstogivestructures,whichshouldbeveryunlikely,toohighaprobability.Intesting,giveninconsistenciesinthegoldstandardtrees,itbecomesmoredifﬁculttojudgehowwellthemodelisdoing.AlthoughitwouldbedifﬁculttoautomaticallydetectthePOStaggingerrors,theotherinconsistenciesoutlinedabovecanbedetectedautomaticallybysimplepat-ternmatching.Automaticallyeliminatingsuchex-amplesisasimplemethodofcleaningthedata.7ExperimentalEvaluationWeuseaparsingmodelsimilartothatdescribedin(Hogan,2005)whichisbasedon(Collins,1999)Model1andusesk-NNforparameterestimation.Then-bestoutputfromBikel’sparser(Bikel,2004)isrerankedaccordingtothisk-NNparsingmodel,whichachievesanf-scoreof89.4%onsection23.Forthecoordinationexperiments,sections02to21areusedfortraining,section23fortestingandtheremainingsectionsforvalidation.Resultsareforsentencescontaining40wordsorless.AsoutlinedinSection6,thetreebankguide-linesaresomewhatambiguousastotheappropriatebracketingforcoordinateNPswhichconsistentirelyofpropernouns.Wethereforedonotinclude,inthecoordinationtestandvalidationsets,coordinateNPswhereinthegoldstandardNPtheleafnodesconsistentirelyofpropernouns(orCCsorcommas).Indo-8AccordingtothePOSbracketingguidelines(Santorini,1991)thecorrectsequenceofPOStagsshouldbeNNNN.686

ingsowehopetoavoidasituationwherebythesuc-cessofthemodelismeasuredinpartbyhowwellitcanpredicttheofteninconsistentbracketingdeci-sionsmadeforaparticularportionofthetreebank.Inaddition,andforthesamereasons,ifagoldstandardtreeisinconsistentwiththeguidelinesineitherofthefollowingtwowaysthetreeisnotusedwhencalculatingcoordinateprecisionandrecallofthemodel:thegoldtreeisanounphrasewhichendswiththesequenceCC/non-nominalmodiﬁer/noun;thegoldtreeisastructuredcoordinatenounphrasewhereeachwordinthenounphraseisanoun.9Calltheseinconsistenciestypeaandtypebrespectively.Thisleftuswithacoordinationvalidationsetcon-sistingof1064coordinatenounphrasesandatestsetof416coordinateNPsfromsection23.Acoordinatephrasewasdeemedcorrectiftheparentconstituentlabel,andthetwoconjunctnodelabels(atlevel0)matchthoseinthegoldsubtreeandif,inaddition,eachoftheconjunctheadwordsarethesameinbothtestandgoldtree.Thisfollowsthedeﬁnitionofacoordinatedependencyin(Collins,1999).Basedonthesecriteria,thebaselinef-scoresfortestandvalidationsetwere69.1%and67.1%re-spectively.Thecoordinationf-scorefortheoracletreesonsection23is83.56%.Inotherwords:ifan‘oracle’weretochoosefromeachsetofn-besttreesthetreethatmaximisedconstituentprecisionandre-call,thentheresultingsetoforacletreeswouldhaveaNPcoordinationdependencyf-scoreof83.56%.Forthevalidationsettheoracletreescoordinationdependencyf-scoreis82.47%.7.1ExperimentsandResultsWeﬁrsteliminatedfromthetrainingsetallcoordi-natenounphrasesubtrees,oftypeaandtypebde-scribedinSection7.Theeffectofthisonthevali-dationsetisoutlinedinTable1,step2.Forthenewparameterclassin(1)wefoundthatthebestresultsoccurredwhenitwasusedonlyinconjunctsofdepth1and2,althoughthecasebaseforthisparameterclasscontainedheadeventsfromallpost-CCconjunctdepths.Parameterclass(2)wasusedforpredictingPOStagsatlevel1inright-of-headconjuncts,althoughagainthesamplecontained9Recallfrom§6thatforthislattercasethenounphraseshouldbeﬂat-anNPB-ratherthananounphrasewithinternalstructure.Modelf-scoresigniﬁcance1.Baseline67.12.NoiseElimination68.7(cid:29)13.Symmetry69.9>2,(cid:29)14.NPBheadrule70.6NOT>3,>2,(cid:29)15.PcoordWordWSJ71.7NOT>4,>3,(cid:29)26.BNCdata72.1NOT>5,>4,(cid:29)37.sim(wi,wp)72.4NOT>6,NOT>5,(cid:29)4Table1:ResultsontheValidationSet.1064coordi-natenounphrasedependencies.Inthesigniﬁcancecolumn>meansatlevel.05and(cid:29)meansatlevel.005,forMcNemar’stestofsigniﬁcance.Resultsarecumulative.eventsfromalldepths.ForthePcoordWordparameterclassweextracted9961coordinatenounpairsfromtheWSJtrain-ingsetand815,323pairsfromtheBNC.Aspairsareconsideredsymmetricthisresultedinatotalof1,650,568coordinatenounevents.Thetermweightsforthewordvectorsweredampenedco-occurrencecounts,oftheform:1+log(count).Forthees-timationofPsim(ni|nj)wefoundittoocomputa-tionallyexpensivetocalculatesimilaritymeasuresbetweennjandeachwordtokencollected.Thebestresultswereobtainedwhentheneighbourhoodofnjwastakentobethek-nearestneighboursofnjfromamongthesetofwordthathadpreviouslyoccurredinacoordinationpatternwithnj,wherekis1000.Table1showstheeffectofthePcoordWordparame-terclassestimatedfromWSJdataonly(step5),withtheadditionofBNCdata(step6)andﬁnallywiththewordsimilaritymeasure(step7).Theresultoftheseexperiments,aswellasthatinvolvingthechangeinthehead-ﬁndingheuristics,outlinedinSection5,wasanincreaseincoordinatenounphrasef-scorefrom69.9%to73.8%onthetestset.Thisrepresentsa13%relativereductioninco-ordinatef-scoreerroroverthebaseline,and,usingMcNemar’stestforsigniﬁcance,issigniﬁcantatthe0.05level(p=0.034).Thererankerf-scoreforallconstituents(notexcludinganycoordinateNPs)forsection23roseslightlyfrom89.4%to89.6%,asmallbutsigniﬁcantincreaseinf-score.10Finally,wereportresultsonanunalteredcoor-dinationtestset,thatis,atestsetfromwhichno10Signiﬁcancewascalculatedusingthesoftwareavailableatwww.cis.upenn.edu/dbikel/software.html.687

noisyeventswereeliminated.Thebaselinecoordi-nationdependencyf-scoreforallNPcoordinationdependencies(550dependencies)fromsection23is69.27%.Thisrisesto72.74%whenallexperimentsdescribedinSection7areapplied,whichisalsoastatisticallysigniﬁcantincrease(p=0.042).8ConclusionandFutureWorkThispaperoutlinedanovelmethodformodellingsymmetryinconjunctstructure,formodellingthedependencybetweennounphraseconjunctheadwordsandforincorporatingameasureofwordsim-ilarityintheestimationofamodelparameter.WealsodemonstratedhowsimplepatternmatchingcanbeusedtoreducenoiseinWSJnounphrasecoor-dinationdata.Combined,thesetechniquesresultedinastatisticallysigniﬁcantimprovementinnounphrasecoordinationaccuracy.Coordinationdisambiguationnecessitatesin-formationfromavarietyofsources.AnotherinformationsourceimportanttoNPcoordinatedisambiguationisthedependencybetweennon-nominalmodiﬁersandnounswhichcrossCCsinNPBs.Forexample,modellingthistypeofdependencycouldhelpthemodellearnthatthephrasethecatsanddogsshouldbebracketedﬂat,whereasthephrasetheU.S.andWashingtonshouldbegivenstructure.AcknowledgementsWearegratefultotheTCDBroadCurriculumFellowshipschemeandtotheSFIBasicResearchGrant04/BR/CS370forfund-ingthisresearch.ThankstoP´adraigCunningham,SaturninoLuz,JenniferFosterandGerardHoganforhelpfuldiscussionsandfeedbackonthiswork.ReferencesRajeevAgarwalandLoisBoggess.1992.ASimplebutUsefulApproachtoConjunctIdentiﬁcation.InProceedingsofthe30thACL.AnnBies,MarkFerguson,KarenKatzandRobertMacIntyre.1995.BracketingGuidelinesforTreebankIIStylePennTreebankProject.TechnicalReport.UniversityofPenn-sylvania.DanBikel.2004.OnTheParameterSpaceofGenerativeLex-icalizedStatisticalParsingModels.Ph.D.thesis,UniversityofPennsylvania.SharonCaraballo.1999.Automaticconstructionofahypernym-labelednounhierarchyfromtext.InProceedingsofthe37thACL.EugeneCharniakandMarkJohnson.2005.Coarse-to-ﬁnen-bestParsingandMaxEntDiscriminativeReranking.InPro-ceedingsofthe43rdACL.MichaelCollins.1999.Head-DrivenStatisticalModelsforNaturalLanguageParsing.Ph.D.thesis,UniversityofPennsylvania.MarkusDickinsonandW.DetmarMeurers.2005.Prunedis-easedbranchestogethealthytrees!Howtoﬁnderroneouslocaltreesinatreebankandwhyitmatters.InProceedingsoftheFourthWorkshoponTreebanksandLinguisticTheo-ries(TLT).AmitDubey,PatrickSturtandFrankKeller.2005.ParallelisminCoordinationasanInstanceofSyntacticPriming:Evi-dencefromCorpus-basedModeling.InProceedingsoftheHLT/EMNP-05.MiriamGoldberg.1999.AnUnsupervisedModelforStatis-ticallyDeterminingCoordinatePhraseAttachment.InPro-ceedingsofthe27thACL.DeirdreHogan.2005.k-NNforLocalProbabilityEstimationinGenerativeParsingModels.InProceedingsoftheIWPT-05.SadaoKurohashiandMakotoNagao.1994.ASyntacticAnal-ysisMethodofLongJapaneseSentencesBasedontheDe-tectionofConjunctiveStructures.InComputationalLin-guistics,20(4).PreslavNakovandMartiHearst.2005.UsingtheWebasanImplicitTrainingSet:ApplicationtoStructuralAmbiguityResolution.InProceedingsoftheHLT/EMNLP-05.AdwaitRatnaparkhi,SalimRoukosandR.ToddWard.1994.AMaximumEntropyModelforParsing.InProceedingsoftheInternationalConferenceonSpokenLanguageProcessing.PhilipResnik.1999.SemanticSimilarityinaTaxonomy:AnInformation-BasedMeasureanditsApplicationtoProblemsofAmbiguityinNaturalLanguage.InJournalofArtiﬁcialIntelligenceResearch,11:95-130,1999.BeatriceSantorini.1991.Part-of-SpeechTaggingGuidelinesforthePennTreebankProject.TechnicalReport.UniversityofPennsylvania.HinrichSch¨utze.1998.AutomaticWordSenseDiscrimination.ComputationalLinguistics,24(1):97-123.DominicWiddows.2004.GeometryandMeaning.CSLIPub-lications,Stanford,USA.ShaojunZhaoandDekangLin.2004.ANearest-NeighborMethodforResolvingPP-AttachmentAmbiguity.InPro-ceedingsoftheIJCNLP-04.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 688–695,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

688

A Unified Tagging Approach to Text Normalization Conghui Zhu Harbin Institute of Technology Harbin, China chzhu@mtlab.hit.edu.cn Jie Tang Department of Computer ScienceTsinghua University, China jietang@tsinghua.edu.cnHang Li Microsoft Research Asia  Beijing, China hangli@microsoft.comHwee Tou Ng Department of Computer Science National University of Singapore, Singapore nght@comp.nus.edu.sg Tiejun Zhao Harbin Institute of Technology Harbin, China tjzhao@mtlab.hit.edu.cn   Abstract This paper addresses the issue of text nor-malization, an important yet often over-looked problem in natural language proc-essing. By text normalization, we mean converting ‘informally inputted’ text into the canonical form, by eliminating ‘noises’ in the text and detecting paragraph and sen-tence boundaries in the text. Previously, text normalization issues were often under-taken in an ad-hoc fashion or studied sepa-rately. This paper first gives a formaliza-tion of the entire problem. It then proposes a unified tagging approach to perform the task using Conditional Random Fields (CRF). The paper shows that with the in-troduction of a small set of tags, most of the text normalization tasks can be per-formed within the approach. The accuracy of the proposed method is high, because the subtasks of normalization are interde-pendent and should be performed together. Experimental results on email data cleaning show that the proposed method signifi-cantly outperforms the approach of using cascaded models and that of employing in-dependent models. 1 Introduction More and more ‘informally inputted’ text data be-comes available to natural language processing, such as raw text data in emails, newsgroups, fo-rums, and blogs. Consequently, how to effectively process the data and make it suitable for natural language processing becomes a challenging issue. This is because informally inputted text data is usually very noisy and is not properly segmented. For example, it may contain extra line breaks, extra spaces, and extra punctuation marks; and it may contain words badly cased. Moreover, the bounda-ries between paragraphs and the boundaries be-tween sentences are not clear. We have examined 5,000 randomly collected emails and found that 98.4% of the emails contain noises (based on the definition in Section 5.1). In order to perform high quality natural lan-guage processing, it is necessary to perform ‘nor-malization’ on informally inputted data first, spe-cifically, to remove extra line breaks, segment the text into paragraphs, add missing spaces and miss-ing punctuation marks, eliminate extra spaces and extra punctuation marks, delete unnecessary tokens, correct misused punctuation marks, restore badly cased words, correct misspelled words, and iden-tify sentence boundaries. Traditionally, text normalization is viewed as an engineering issue and is conducted in a more or less ad-hoc manner. For example, it is done by us-ing rules or machine learning models at different levels. In natural language processing, several is-sues of text normalization were studied, but were only done separately. This paper aims to conduct a thorough investiga-tion on the issue. First, it gives a formalization of 689

the problem; specifically, it defines the subtasks of the problem. Next, it proposes a unified approach to the whole task on the basis of tagging. Specifi-cally, it takes the problem as that of assigning tags to the input texts, with a tag representing deletion, preservation, or replacement of a token. As the tagging model, it employs Conditional Random Fields (CRF). The unified model can achieve better performances in text normalization, because the subtasks of text normalization are often interde-pendent. Furthermore, there is no need to define specialized models and features to conduct differ-ent types of cleaning; all the cleaning processes have been formalized and conducted as assign-ments of the three types of tags. Experimental results indicate that our method significantly outperforms the methods using cas-caded models or independent models on normali-zation. Our experiments also indicate that with the use of the tags defined, we can conduct most of the text normalization in the unified framework. Our contributions in this paper include: (a) for-malization of the text normalization problem, (b) proposal of a unified tagging approach, and (c) empirical verification of the effectiveness of the proposed approach. The rest of the paper is organized as follows. In Section 2, we introduce related work. In Section 3, we formalize the text normalization problem. In Section 4, we explain our approach to the problem and in Section 5 we give the experimental results. We conclude the paper in Section 6. 2 Related Work Text normalization is usually viewed as an engineering issue and is addressed in an ad-hoc manner. Much of the previous work focuses on processing texts in clean form, not texts in informal form. Also, prior work mostly focuses on processing one type or a small number of types of errors, whereas this paper deals with many different types of errors. Clark (2003) has investigated the problem of preprocessing noisy texts for natural language processing. He proposes identifying token bounda-ries and sentence boundaries, restoring cases of words, and correcting misspelled words by using a source channel model. Minkov et al. (2005) have investigated the prob-lem of named entity recognition in informally in-putted texts. They propose improving the perform-ance of personal name recognition in emails using two machine-learning based methods: Conditional Random Fields and Perceptron for learning HMMs. See also (Carvalho and Cohen, 2004). Tang et al. (2005) propose a cascaded approach for email data cleaning by employing Support Vec-tor Machines and rules. Their method can detect email headers, signatures, program codes, and ex-tra line breaks in emails. See also (Wong et al., 2007). Palmer and Hearst (1997) propose using a Neu-ral Network model to determine whether a period in a sentence is the ending mark of the sentence, an abbreviation, or both. See also (Mikheev, 2000; Mikheev, 2002). Lita et al. (2003) propose employing a language modeling approach to address the case restoration problem. They define four classes for word casing: all letters in lower case, first letter in uppercase, all letters in upper case, and mixed case, and formal-ize the problem as assigning class labels to words in natural language texts. Mikheev (2002) proposes using not only local information but also global information in a document in case restoration. Spelling error correction can be formalized as a classification problem. Golding and Roth (1996) propose using the Winnow algorithm to address the issue. The problem can also be formalized as that of data conversion using the source channel model. The source model can be built as an n-gram language model and the channel model can be con-structed with confusing words measured by edit distance. Brill and Moore, Church and Gale, and Mayes et al. have developed different techniques for confusing words calculation (Brill and Moore, 2000; Church and Gale, 1991; Mays et al., 1991). Sproat et al. (1999) have investigated normaliza-tion of non-standard words in texts, including numbers, abbreviations, dates, currency amounts, and acronyms. They propose a taxonomy of non-standard words and apply n-gram language models, decision trees, and weighted finite-state transduc-ers to the normalization. 3 Text Normalization In this paper we define text normalization at three levels: paragraph, sentence, and word level. The subtasks at each level are listed in Table 1. For ex-ample, at the paragraph level, there are two sub-690

tasks: extra line-break deletion and paragraph boundary detection. Similarly, there are six (three) subtasks at the sentence (word) level, as shown in Table 1. Unnecessary token deletion refers to dele-tion of tokens like ‘-----’ and ‘====’, which are not needed in natural language processing. Note that most of the subtasks conduct ‘cleaning’ of noises, except paragraph boundary detection and sentence boundary detection. Level Task Percentages of NoisesExtra line break deletion 49.53 Paragraph Paragraph boundary detection  Extra space deletion 15.58 Extra punctuation mark deletion 0.71 Missing space insertion 1.55 Missing punctuation mark insertion3.85 Misused punctuation mark correction0.64 Sentence Sentence boundary detection  Case restoration 15.04 Unnecessary token deletion 9.69 Word Misspelled word correction 3.41 Table 1. Text Normalization Subtasks As a result of text normalization, a text is seg-mented into paragraphs; each paragraph is seg-mented into sentences with clear boundaries; and each word is converted into the canonical form. After normalization, most of the natural language processing tasks can be performed, for example, part-of-speech tagging and parsing. We have manually cleaned up some email data (cf., Section 5) and found that nearly all the noises can be eliminated by performing the subtasks de-fined above. Table 1 gives the statistics. 1.  i’m thinking about buying a pocket 2.  pc    device for my wife this christmas,. 3.  the worry that i have is that she won’t 4.  be able to sync it to her outlook express  5.  contacts… Figure 1. An example of informal text I’m thinking about buying a Pocket PC device for my wife this Christmas.// The worry that I have is that she won’t be able to sync it to her Outlook Express contacts.// Figure 2. Normalized text Figure 1 shows an example of informally input-ted text data. It includes many typical noises. From line 1 to line 4, there are four extra line breaks at the end of each line. In line 2, there is an extra comma after the word ‘Christmas’. The first word in each sentence and the proper nouns (e.g., ‘Pocket PC’ and ‘Outlook Express’) should be capitalized. The extra spaces between the words ‘PC’ and ‘device’ should be removed. At the end of line 2, the line break should be removed and a space is needed after the period. The text should be segmented into two sentences. Figure 2 shows an ideal output of text normali-zation on the input text in Figure 1. All the noises in Figure 1 have been cleaned and paragraph and sentence endings have been identified. We must note that dependencies (sometimes even strong dependencies) exist between different types of noises. For example, word case restoration needs help from sentence boundary detection, and vice versa. An ideal normalization method should consider processing all the tasks together. 4 A Unified Tagging Approach 4.1 Process In this paper, we formalize text normalization as a tagging problem and employ a unified approach to perform the task (no matter whether the processing is at paragraph level, sentence level, or word level). There are two steps in the method: preprocess-ing and tagging. In preprocessing, (A) we separate the text into paragraphs (i.e., sequences of tokens), (B) we determine tokens in the paragraphs, and (C) we assign possible tags to each token. The tokens form the basic units and the paragraphs form the sequences of units in the tagging problem. In tag-ging, given a sequence of units, we determine the most likely corresponding sequence of tags by us-ing a trained tagging model. In this paper, as the tagging model, we make use of CRF. Next we describe the steps (A)-(C) in detail and explain why our method can accomplish many of the normalization subtasks in Table 1. (A). We separate the text into paragraphs by tak-ing two or more consecutive line breaks as the end-ings of paragraphs. (B). We identify tokens by using heuristics. There are five types of tokens: ‘standard word’, ‘non-standard word’, punctuation mark, space, and line break. Standard words are words in natural language. Non-standard words include several general ‘special words’ (Sproat et al., 1999), email address, IP address, URL, date, number, money, percentage, unnecessary tokens (e.g., ‘===‘ and 691

‘###’), etc. We identify non-standard words by using regular expressions. Punctuation marks in-clude period, question mark, and exclamation mark. Words and punctuation marks are separated into different tokens if they are joined together. Natural spaces and line breaks are also regarded as tokens. (C). We assign tags to each token based on the type of the token. Table 2 summarizes the types of tags defined. Token Type Tag Description PRV Preserve line break RPA Replace line break by space Line break DEL Delete line break PRV Preserve space Space DEL Delete space PSB Preserve punctuation mark and view it as sentence ending PRV Preserve punctuation mark without viewing it as sentence ending Punctuation mark DEL Delete punctuation mark AUC Make all characters in uppercase ALC Make all characters in lowercase FUC Make the first character in uppercaseWord AMC Make characters in mixed case PRV Preserve the special token Special token DEL Delete the special token Table 2. Types of tags  Figure 3. An example of tagging Figure 3 shows an example of the tagging proc-ess. (The symbol ‘’ indicates a space). In the fig-ure, a white circle denotes a token and a gray circle denotes a tag. Each token can be assigned several possible tags. Using the tags, we can perform most of the text normalization processing (conducting seven types of subtasks defined in Table 1 and cleaning 90.55% of the noises). In this paper, we do not conduct three subtasks, although we could do them in principle. These in-clude missing space insertion, missing punctuation mark insertion, and misspelled word correction. In our email data, it corresponds to 8.81% of the noises. Adding tags for insertions would increase the search space dramatically. We did not do that due to computation consideration. Misspelled word correction can be done in the same framework eas-ily. We did not do that in this work, because the percentage of misspelling in the data is small. We do not conduct misused punctuation mark correction as well (e.g., correcting ‘.’ with ‘?’). It consists of 0.64% of the noises in the email data. To handle it, one might need to parse the sentences. 4.2 CRF Model We employ Conditional Random Fields (CRF) as the tagging model. CRF is a conditional probability distribution of a sequence of tags given a sequence of tokens, represented as P(Y|X) , where X denotes the token sequence and Y the tag sequence (Lafferty et al., 2001). In tagging, the CRF model is used to find the sequence of tags Y* having the highest likelihood Y* = maxYP(Y|X), with an efficient algorithm (the Viterbi algorithm). In training, the CRF model is built with labeled data and by means of an iterative algorithm based on Maximum Likelihood Estimation. Transition Features yi-1=y’, yi=y yi-1=y’, yi=y, wi=w yi-1=y’, yi=y, ti=t State Features wi=w, yi=y wi-1=w, yi=y wi-2=w, yi=y wi-3=w, yi=y wi-4=w, yi=y wi+1=w, yi=y wi+2=w, yi=y wi+3=w, yi=y wi+4=w, yi=y wi-1=w’, wi=w, yi=ywi+1=w’, wi=w, yi=y ti=t, yi=y ti-1=t, yi=y ti-2=t, yi=y ti-3=t, yi=y ti-4=t, yi=y ti+1=t, yi=y ti+2=t, yi=y ti+3=t, yi=y ti+4=t, yi=y ti-2=t’’, ti-1=t’, yi=y ti-1=t’, ti=t, yi=y ti=t, ti+1=t’, yi=y ti+1=t’, ti+2=t’’, yi=y ti-2=t’’, ti-1=t’, ti=t, yi=y ti-1=t’’, ti=t, ti+1=t’, yi=y ti=t, ti+1=t’, ti+2=t’’, yi=y Table 3. Features used in the unified CRF model 692

4.3 Features Two sets of features are defined in the CRF model: transition features and state features. Table 3 shows the features used in the model. Suppose that at position i in token sequence x, wi is the token, ti the type of token (see Table 2), and yi the possible tag. Binary features are defined as described in Table 3. For example, the transition feature yi-1=y’, yi=y implies that if the current tag is y and the previous tag is y’, then the feature value is true; otherwise false. The state feature wi=w, yi=y implies that if the current token is w and the current label is y, then the feature value is true; otherwise false. In our experiments, an actual fea-ture might be the word at position 5 is ‘PC’ and the current tag is AUC. In total, 4,168,723 features were used in our experiments. 4.4 Baseline Methods We can consider two baseline methods based on previous work, namely cascaded and independent approaches. The independent approach performs text normalization with several passes on the text. All of the processes take the raw text as input and output the normalized/cleaned result independently. The cascaded approach also performs normaliza-tion in several passes on the text. Each process car-ries out cleaning/normalization from the output of the previous process. 4.5 Advantages Our method offers some advantages. (1) As indicated, the text normalization tasks are interdependent. The cascaded approach or the in-dependent approach cannot simultaneously per-form the tasks. In contrast, our method can effec-tively overcome the drawback by employing a uni-fied framework and achieve more accurate per-formances. (2) There are many specific types of errors one must correct in text normalization. As shown in Figure 1, there exist four types of errors with each type having several correction results. If one de-fines a specialized model or rule to handle each of the cases, the number of needed models will be extremely large and thus the text normalization processing will be impractical. In contrast, our method naturally formalizes all the tasks as as-signments of different types of tags and trains a unified model to tackle all the problems at once. 5 Experimental Results 5.1 Experiment Setting Data Sets We used email data in our experiments. We ran-domly chose in total 5,000 posts (i.e., emails) from 12 newsgroups. DC, Ontology, NLP, and ML are from newsgroups at Google (http://groups-beta.google.com/groups). Jena is a newsgroup at Ya-hoo (http://groups.yahoo.com/group/jena-dev). Weka is a newsgroup at Waikato University (https://list. scms.waikato.ac.nz). Protégé and OWL are from a project at Stanford University (http://protege.stanford.edu/). Mobility, WinServer, Windows, and PSS are email collections from a company. Five human annotators conducted normalization on the emails. A spec was created to guide the an-notation process. All the errors in the emails were labeled and corrected. For disagreements in the annotation, we conducted “majority voting”.  For example, extra line breaks, extra spaces, and extra punctuation marks in the emails were labeled. Un-necessary tokens were deleted. Missing spaces and missing punctuation marks were added and marked. Mistakenly cased words, misspelled words, and misused punctuation marks were corrected. Fur-thermore, paragraph boundaries and sentence boundaries were also marked. The noises fell into the categories defined in Table 1. Table 4 shows the statistics in the data sets. From the table, we can see that a large number of noises (41,407) exist in the emails. We can also see that the major noise types are extra line breaks, extra spaces, casing errors, and unnecessary tokens. In the experiments, we conducted evaluations in terms of precision, recall, F1-measure, and accu-racy (for definitions of the measures, see for ex-ample (van Rijsbergen, 1979; Lita et al., 2003)). Implementation of Baseline Methods We used the cascaded approach and the independ-ent approach as baselines. For the baseline methods, we defined several basic prediction subtasks: extra line break detec-tion, extra space detection, extra punctuation mark detection, sentence boundary detection, unneces-sary token detection, and case restoration. We compared the performances of our method with those of the baseline methods on the subtasks. 693

Data Set Number of Email Number of Noises Extra Line Break Extra Space Extra Punc.MissingSpaceMissingPunc.CasingErrorSpellingErrorMisused Punc.Unnece-ssary Token Number of Paragraph Boundary Number of Sentence BoundaryDC 100 702 476 31 8 3 24 53 14 2 91 457 291 Ontology 100 2,731 2,132 24 3 10 68 205 79 15 195 677 1,132 NLP 60 861 623 12 1 3 23 135 13 2 49 244 296 ML 40 980 868 17 0 2 13 12 7 0 61 240 589 Jena 700 5,833 3,066 117 42 38 234 888 288 59 1,101 2,999 1,836 Weka 200 1,721 886 44 0 30 37 295 77 13 339 699 602 Protégé 700 3,306 1,770 127 48 151 136 552 116 9 397 1,645 1,035 OWL 300 1,232 680 43 24 47 41 152 44 3 198 578 424 Mobility 400 2,296 1,292 64 22 35 87 495 92 8 201 891 892 WinServer 400 3,487 2,029 59 26 57 142 822 121 21 210 1,232 1,151 Windows 1,000 9,293 3,416 3,056 60 116 348 1,309291 67 630 3,581 2,742 PSS 1,000 8,965 3,348 2,880 59 153 296 1,331276 66 556 3,411 2,590 Total 5,000 41,407 20,586 6,474 293645 1,4496,2491,418265 4,028 16,654 13,580 Table 4. Statistics on data sets For the case restoration subtask (processing on token sequence), we employed the TrueCasing method (Lita et al., 2003). The method estimates a tri-gram language model using a large data corpus with correctly cased words and then makes use of the model in case restoration. We also employed Conditional Random Fields to perform case restoration, for comparison purposes. The CRF based casing method estimates a conditional probabilistic model using the same data and the same tags defined in TrueCasing. For unnecessary token deletion, we used rules as follows. If a token consists of non-ASCII charac-ters or consecutive duplicate characters, such as ‘===‘, then we identify it as an unnecessary token. For each of the other subtasks, we exploited the classification approach. For example, in extra line break detection, we made use of a classification model to identify whether or not a line break is a paragraph ending. We employed Support Vector Machines (SVM) as the classification model (Vap-nik, 1998). In the classification model we utilized the same features as those in our unified model (see Table 3 for details). In the cascaded approach, the prediction tasks are performed in sequence, where the output of each task becomes the input of each immediately following task. The order of the prediction tasks is: (1) Extra line break detection: Is a line break a paragraph ending? It then separates the text into paragraphs using the remaining line breaks. (2) Extra space detection: Is a space an extra space? (3) Extra punctuation mark detection: Is a punctuation mark a noise? (4) Sentence boundary detection: Is a punctuation mark a sentence boundary? (5) Un-necessary token deletion: Is a token an unnecessary token? (6) Case restoration. Each of steps (1) to (4) uses a classification model (SVM), step (5) uses rules, whereas step (6) uses either a language model (TrueCasing) or a CRF model (CRF). In the independent approach, we perform the prediction tasks independently. When there is a conflict between the outcomes of two classifiers, we adopt the result of the latter classifier, as de-termined by the order of classifiers in the cascaded approach. To test how dependencies between different types of noises affect the performance of normali-zation, we also conducted experiments using the unified model by removing the transition features. Implementation of Our Method In the implementation of our method, we used the tool CRF++, available at http://chasen.org/~taku /software/CRF++/. We made use of all the default settings of the tool in the experiments. 5.2 Text Normalization Experiments Results We evaluated the performances of our method (Unified) and the baseline methods (Cascaded and Independent) on the 12 data sets. Table 5 shows the five-fold cross-validation results. Our method outperforms the two baseline methods. Table 6 shows the overall performances of text normalization by our method and the two baseline methods. We see that our method outperforms the two baseline methods. It can also be seen that the performance of the unified method decreases when removing the transition features (Unified w/o Transition Features). 694

We conducted sign tests for each subtask on the results, which indicate that all the improvements of Unified over Cascaded and Independent are statis-tically significant (p << 0.01). Detection Task Prec. Rec. F1 Acc.Independent 95.16 91.52 93.3093.81Cascaded 95.16 91.52 93.3093.81Extra Line Break  Unified 93.87 93.63 93.7594.53Independent 91.85 94.64 93.2299.87Cascaded 94.54 94.56 94.5599.89Extra Space Unified 95.17 93.98 94.5799.90Independent 88.63 82.69 85.5699.66Cascaded 87.17 85.37 86.2699.66Extra  Punctuation Mark Unified 90.94 84.84 87.7899.71Independent 98.46 99.62 99.0498.36Cascaded 98.55 99.20 98.8798.08Sentence Boundary  Unified 98.76 99.61 99.1898.61Independent 72.51 100.0 84.0684.27Cascaded 72.51 100.0 84.0684.27Unnecessary Token Unified 98.06 95.47 96.7596.18Independent 27.32 87.44 41.6396.22Case  Restoration (TrueCasing) Cascaded 28.04 88.21 42.5596.35Independent 84.96 62.79 72.2199.01Cascaded 85.85 63.99 73.3399.07Case  Restoration (CRF) Unified 86.65 67.09 75.6399.21Table 5. Performances of text normalization (%) Text Normalization Prec. Rec. F1 Acc.Independent (TrueCasing) 69.54 91.33 78.9697.90Independent (CRF) 85.05 92.52 88.6398.91Cascaded (TrueCasing) 70.29 92.07 79.7297.88Cascaded (CRF) 85.06 92.70 88.7298.92Unified w/o Transition Features 86.03 93.45 89.5999.01Unified 86.46 93.92 90.0499.05Table 6. Performances of text normalization (%) Discussions Our method outperforms the independent method and the cascaded method in all the subtasks, espe-cially in the subtasks that have strong dependen-cies with each other, for example, sentence bound-ary detection, extra punctuation mark detection, and case restoration. The cascaded method suffered from ignorance of the dependencies between the subtasks. For ex-ample, there were 3,314 cases in which sentence boundary detection needs to use the results of extra line break detection, extra punctuation mark detec-tion, and case restoration. However, in the cas-caded method, sentence boundary detection is con-ducted after extra punctuation mark detection and before case restoration, and thus it cannot leverage the results of case restoration. Furthermore, errors of extra punctuation mark detection can lead to errors in sentence boundary detection. The independent method also cannot make use of dependencies across different subtasks, because it conducts all the subtasks from the raw input data. This is why for detection of extra space, extra punctuation mark, and casing error, the independ-ent method cannot perform as well as our method. Our method benefits from the ability of model-ing dependencies between subtasks. We see from Table 6 that by leveraging the dependencies, our method can outperform the method without using dependencies (Unified w/o Transition Features) by 0.62% in terms of F1-measure. Here we use the example in Figure 1 to show the advantage of our method compared with the inde-pendent and the cascaded methods. With normali-zation by the independent method, we obtain: I’m thinking about buying a pocket PC   device for my wife this Christmas, The worry that I have is that she won’t be able to sync it to her outlook express contacts.// With normalization by the cascaded method, we obtain: I’m thinking about buying a pocket PC device for my wife this Christmas, the worry that I have is that she won’t be able to sync it to her outlook express contacts.// With normalization by our method, we obtain: I’m thinking about buying a Pocket PC device for my wife this Christmas.// The worry that I have is that she won’t be able to sync it to her Outlook Express contacts.// The independent method can correctly deal with some of the errors. For instance, it can capitalize the first word in the first and the third line, remove extra periods in the fifth line, and remove the four extra line breaks. However, it mistakenly removes the period in the second line and it cannot restore the cases of some words, for example ‘pocket’ and ‘outlook express’. In the cascaded method, each process carries out cleaning/normalization from the output of the pre-vious process and thus can make use of the cleaned/normalized results from the previous proc-ess. However, errors in the previous processes will also propagate to the later processes. For example, the cascaded method mistakenly removes the pe-riod in the second line. The error allows case resto-ration to make the error of keeping the word ‘the’ in lower case. 695

TrueCasing-based methods for case restoration suffer from low precision (27.32% by Independent and 28.04% by Cascaded), although their recalls are high (87.44% and 88.21% respectively). There are two reasons: 1) About 10% of the errors in Cascaded are due to errors of sentence boundary detection and extra line break detection in previous steps; 2) The two baselines tend to restore cases of words to the forms having higher probabilities in the data set and cannot take advantage of the de-pendencies with the other normalization subtasks. For example, ‘outlook’ was restored to first letter capitalized in both ‘Outlook Express’ and ‘a pleas-ant outlook’. Our method can take advantage of the dependencies with other subtasks and thus correct 85.01% of the errors that the two baseline methods cannot handle. Cascaded and Independent methods employing CRF for case restoration improve the accuracies somewhat. However, they are still infe-rior to our method. Although we have conducted error analysis on the results given by our method, we omit the de-tails here due to space limitation and will report them in a future expanded version of this paper. We also compared the speed of our method with those of the independent and cascaded methods. We tested the three methods on a computer with two 2.8G Dual-Core CPUs and three Gigabyte memory. On average, it needs about 5 hours for training the normalization models using our method and 25 seconds for tagging in the cross-validation experiments. The independent and the cascaded methods (with TrueCasing) require less time for training (about 2 minutes and 3 minutes respectively) and for tagging (several seconds). This indicates that the efficiency of our method still needs improvement. 6 Conclusion In this paper, we have investigated the problem of text normalization, an important issue for natural language processing. We have first defined the problem as a task consisting of noise elimination and boundary detection subtasks. We have then proposed a unified tagging approach to perform the task, specifically to treat text normalization as as-signing tags representing deletion, preservation, or replacement of the tokens in the text. Experiments show that our approach significantly outperforms the two baseline methods for text normalization. References E. Brill and R. C. Moore. 2000. An Improved Error Model for Noisy Channel Spelling Correction, Proc. of ACL 2000. V. R. Carvalho and W. W. Cohen. 2004. Learning to Extract Signature and Reply Lines from Email, Proc. of CEAS 2004. K. Church and W. Gale. 1991. Probability Scoring for Spelling Correction, Statistics and Computing, Vol. 1. A. Clark. 2003. Pre-processing Very Noisy Text, Proc. of Workshop on Shallow Processing of Large Cor-pora. A. R. Golding and D. Roth. 1996. Applying Winnow to Context-Sensitive Spelling Correction, Proc. of ICML’1996. J. Lafferty, A. McCallum, and F. Pereira. 2001. Condi-tional Random Fields: Probabilistic Models for Seg-menting and Labeling Sequence Data, Proc. of ICML 2001. L. V. Lita, A. Ittycheriah, S. Roukos, and N. Kambhatla. 2003. tRuEcasIng, Proc. of ACL 2003. E. Mays, F. J. Damerau, and R. L. Mercer. 1991. Con-text Based Spelling Correction, Information Process-ing and Management, Vol. 27, 1991. A. Mikheev. 2000. Document Centered Approach to Text Normalization, Proc. SIGIR 2000. A. Mikheev. 2002. Periods, Capitalized Words, etc. Computational Linguistics, Vol. 28, 2002. E. Minkov, R. C. Wang, and W. W. Cohen. 2005. Ex-tracting Personal Names from Email: Applying Named Entity Recognition to Informal Text, Proc. of EMNLP/HLT-2005. D. D. Palmer and M. A. Hearst. 1997. Adaptive Multi-lingual Sentence Boundary Disambiguation, Compu-tational Linguistics, Vol. 23. C.J. van Rijsbergen. 1979. Information Retrieval. But-terworths, London. R. Sproat, A. Black, S. Chen, S. Kumar, M. Ostendorf, and C. Richards. 1999. Normalization of non-standard words, WS’99 Final Report. http://www.clsp.jhu.edu/ws99/projects/normal/. J. Tang, H. Li, Y. Cao, and Z. Tang. 2005. Email data cleaning, Proc. of SIGKDD’2005. V. Vapnik. 1998. Statistical Learning Theory, Springer. W. Wong, W. Liu, and M. Bennamoun. 2007. Enhanced Integrated Scoring for Cleaning Dirty Texts, Proc. of IJCAI-2007 Workshop on Analytics for Noisy Un-structured Text Data. Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 696–703,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

696

SparseInformationExtraction:UnsupervisedLanguageModelstotheRescueDougDowney,StefanSchoenmackers,andOrenEtzioniTuringCenter,DepartmentofComputerScienceandEngineeringUniversityofWashington,Box352350Seattle,WA98195,USA{ddowney,stef,etzioni}@cs.washington.eduAbstractEveninamassivecorpussuchastheWeb,asubstantialfractionofextractionsappearin-frequently.Thispapershowshowtoassessthecorrectnessofsparseextractionsbyuti-lizingunsupervisedlanguagemodels.TheREALMsystem,whichcombinesHMM-basedandn-gram-basedlanguagemodels,rankscandidateextractionsbythelikeli-hoodthattheyarecorrect.OurexperimentsshowthatREALMreducesextractionerrorby39%,onaverage,whencomparedwithpreviouswork.BecauseREALMpre-computeslanguagemodelsbasedonitscorpusanddoesnotre-quireanyhand-taggedseeds,itisfarmorescalablethanapproachesthatlearnmod-elsforeachindividualrelationfromhand-taggeddata.Thus,REALMisideallysuitedforopeninformationextractionwheretherelationsofinterestarenotspeciﬁedinad-vanceandtheirnumberispotentiallyvast.1IntroductionInformationExtraction(IE)fromtextisfarfromin-fallible.Inresponse,researchershavebeguntoex-ploittheredundancyinmassivecorporasuchastheWebinordertoassesstheveracityofextractions(e.g.,(Downeyetal.,2005;Etzionietal.,2005;Feldmanetal.,2006)).Inessence,suchmethodsuti-lizeextractionpatternstogeneratecandidateextrac-tions(e.g.,“Istanbul”)andthenassesseachcandi-datebycomputingco-occurrencestatisticsbetweentheextractionandwordsorphrasesindicativeofclassmembership(e.g.,“citiessuchas”).However,Zipf’sLawgovernsthedistributionofextractions.Thus,eventheWebhaslimitedredun-dancyforlessprominentinstancesofrelations.In-deed,50%oftheextractionsinthedatasetsem-ployedby(Downeyetal.,2005)appearedonlyonce.Asaresult,Downeyetal.’smodel,andre-latedmethods,hadnowayofassessingwhichex-tractionismorelikelytobecorrectforfullyhalfoftheextractions.Thisproblemisparticularlyacutewhenmovingbeyondunaryrelations.Werefertothischallengeasthetaskofassessingsparseextrac-tions.Thispaperintroducestheideathatlanguagemod-elingtechniquessuchasn-gramstatistics(ManningandSch¨utze,1999)andHMMs(Rabiner,1989)canbeusedtoeffectivelyassesssparseextractions.ThepaperintroducestheREALMsystem,andhighlightsitsuniqueproperties.Notably,REALMdoesnotrequireanyhand-taggedseeds,whichenablesittoscaletoOpenIE—extractionwheretherelationsofinterestarenotspeciﬁedinadvance,andtheirnum-berispotentiallyvast(Bankoetal.,2007).REALMisbasedontwokeyhypotheses.TheKnowItAllhypothesisisthatextractionsthatoc-curmorefrequentlyindistinctsentencesinthecorpusaremorelikelytobecorrect.Forexam-ple,thehypothesissuggeststhattheargumentpair(Giuliani,NewYork)isrelativelylikelytobeappropriatefortheMayorrelation,simplybecausethispairisextractedfortheMayorrelationrela-tivelyfrequently.Second,weemployaninstanceofthedistributionalhypothesis(Harris,1985),which697

canbephrasedasfollows:differentinstancesofthesamesemanticrelationtendtoappearinsim-ilartextualcontexts.Weassesssparseextractionsbycomparingthecontextsinwhichtheyappeartothoseofmorecommonextractions.Sparseextrac-tionswhosecontextsaremoresimilartothoseofcommonextractionsarejudgedmorelikelytobecorrectbasedontheconjunctionoftheKnowItAllandthedistributionalhypotheses.Thecontributionsofthepaperareasfollows:•Thepaperintroducestheinsightthatthesub-ﬁeldoflanguagemodelingprovidesunsuper-visedmethodsthatcanbeleveragedtoassesssparseextractions.Thesemethodsaremorescalablethanpreviousassessmenttechniques,andrequirenohandtaggingwhatsoever.•ThepaperintroducesanHMM-basedtech-niqueforcheckingwhethertwoargumentsareofthepropertypeforarelation.•Thepaperintroducesarelationaln-grammodelforthepurposeofdeterminingwhetherasentencethatmentionsmultipleargumentsactuallyexpressesaparticularrelationshipbe-tweenthem.•Thepaperintroducesanovellanguage-modelingsystemcalledREALMthatcombinesbothHMM-basedmodelsandrelationaln-grammodels,andshowsthatREALMreduceserrorbyanaverageof39%overpreviousmeth-ods,whenappliedtosparseextractiondata.Theremainderofthepaperisorganizedasfol-lows.Section2introducestheIEassessmenttask,anddescribestheREALMsystemindetail.Section3reportsonourexperimentalresultsfollowedbyadiscussionofrelatedworkinSection4.Finally,weconcludewithadiscussionofscalabilityandwithdirectionsforfuturework.2IEAssessmentThissectionformalizestheIEassessmenttaskanddescribestheREALMsystemforsolvingit.AnIEassessortakesasinputalistofcandidateextractionsmeanttodenoteinstancesofarelation,andoutputsarankingoftheextractionswiththegoalthatcor-rectextractionsrankhigherthanincorrectones.Acorrectextractionisdeﬁnedtobeatrueinstanceoftherelationmentionedintheinputtext.Moreformally,thelistofcandidateextrac-tionsforarelationRisdenotedasER={(a1,b1),...,(am,bm)}.Anextraction(ai,bi)isanorderedpairofstrings.TheextractioniscorrectifandonlyiftherelationRholdsbetweentheargu-mentsnamedbyaiandbi.Forexample,forR=Headquartered,apair(ai,bi)iscorrectiffthereexistsanorganizationaithatisinfactheadquarteredinthelocationbi.1ERisgeneratedbyapplyinganextractionmech-anism,typicallyasetofextraction“patterns”,toeachsentenceinacorpus,andrecordingtheresults.Thus,manyelementsofERareidenticalextractionsderivedfromdifferentsentencesinthecorpus.Thistaskdeﬁnitionisnotablefortheminimalinputsrequired—IEassessmentdoesnotrequireknowingtherelationnamenordoesitrequirehand-taggedseedexamplesoftherelation.Thus,anIEAssessorisapplicabletoOpenIE.2.1SystemOverviewInthissection,wedescribetheREALMsystem,whichutilizeslanguagemodelingtechniquestoper-formIEAssessment.REALMtakesasinputasetofextractionsER,andoutputsarankingofthoseextractions.ThealgorithmREALMfollowsisoutlinedinFigure1.REALMbeginsbyautomaticallyselectingfromERasetofbootstrappedseedsSRintendedtoserveascorrectexamplesoftherelationR.REALMutilizestheKnowItAllhypothesis,settingSRequaltothehelementsinERextractedmostfrequentlyfromtheunderlyingcorpus.Thisresultsinanoisysetofseeds,butthemethodsthatusetheseseedsarenoisetolerant.REALMthenproceedstoranktheremaining(non-seed)extractionsbyutilizingtwolanguage-modelingcomponents.Ann-gramlanguagemodelisaprobabilitydistributionP(w1,...,wn)overcon-secutivewordsequencesoflengthninacorpus.Formally,ifweassumeaseed(s1,s2)isacorrectextractionofarelationR,thedistributionalhypoth-esisstatesthatthecontextdistributionaroundtheseedextraction,P(w1,...,wn|wi=s1,wj=s2)for1≤i,j≤ntendstobe“moresimilar”to1Forclarity,ourdiscussionfocusesonrelationsbetweenpairsofarguments.However,themethodsweproposecanbeextendedtorelationsofanyarity.698

P(w1,...,wn|wi=e1,wj=e2)whentheextrac-tion(e1,e2)iscorrect.Naivelycomparingcontextdistributionsisproblematic,however,becausetheargumentstoarelationoftenappearseparatedbyseveralinterveningwords.Inourexperiments,wefoundthatwhenrelationargumentsappeartogetherinasentence,75%ofthetimetheargumentsareseparatedbyatleastthreewords.Thisimpliesthatnmustbelarge,andforsparseargumentpairsitisnotpossibletoestimatesuchalargelanguagemodelaccurately,becausethenumberofmodelingparam-etersisproportionaltothevocabularysizeraisedtothenthpower.Tomitigatesparsity,REALMutilizessmallerlanguagemodelsinitstwocomponentsasameansof“backing-off’fromestimatingcontextdis-tributionsexplicitly,asdescribedbelow.First,REALMutilizesanHMMtoestimatewhethereachextractionhasargumentsofthepropertypefortherelation.EachrelationRhasasetoftypesforitsarguments.Forexample,therela-tionAuthorOf(a,b)requiresthatitsﬁrstar-gumentbeanauthor,andthatitssecondbesomekindofwrittenwork.Knowingwhetherextractedargumentsareofthepropertypeforarelationcanbequiteinformativeforassessingextractions.Thechallengeis,however,thatthistypeinformationisnotgiventothesystemsincetherelations(andthetypesofthearguments)arenotknowninadvance.REALMsolvesthisproblembycomparingthedis-tributionsoftheseedargumentsandextractionar-guments.Typecheckingmitigatesdatasparsitybyleveragingeveryoccurrenceoftheindividualextrac-tionargumentsinthecorpus,ratherthanonlythosecasesinwhichargumentpairsoccurneareachother.Althoughargumenttypecheckingisinvalu-ableforextractionassessment,itisnotsuf-ﬁcientforextractingrelationshipsbetweenar-guments.Forexample,anIEsystemus-ingonlytypeinformationmightdeterminethatIntelisacorporationandthatSeattleisacity,andthereforeerroneouslyconcludethatHeadquartered(Intel,Seattle)iscor-rect.Thus,REALM’ssecondstepistoemployann-gram-basedlanguagemodeltoassesswhethertheextractedargumentssharetheappropriaterelation.Again,thisinformationisnotgiventothesystem,soREALMcomparesthecontextdistributionsoftheextractionstothoseoftheseeds.AsdescribedinREALM(ExtractionsER={e1,...,em})SR=thehmostfrequentextractionsinERUR=ER-SRTypeRankings(UR)←HMM-T(SR,UR)RelationRankings(UR)←REL-GRAMS(SR,UR)returnarankingofERwiththeelementsofSRatthetop(rankedbyfrequency)followedbytheelementsofUR={u1,...,um−h}rankedinascendingorderofTypeRanking(ui)∗RelationRanking(ui).Figure1:PseudocodeforREALMatrun-time.ThelanguagemodelsusedbytheHMM-TandREL-GRAMScomponentsareconstructedinapre-processingstep.Section2.3,REALMemploysarelationaln-gramlanguagemodelinordertoaccuratelycomparecon-textdistributionswhenextractionsaresparse.REALMexecutesthetypecheckingandrelationassessmentcomponentsseparately;eachcomponenttakestheseedandnon-seedextractionsasargumentsandreturnsarankingofthenon-seeds.REALMthencombinesthetwocomponents’assessmentsintoasingleranking.Althoughseveralsuchcombinationsarepossible,REALMsimplyrankstheextractionsinascendingorderoftheproductoftheranksassignedbythetwocomponents.ThefollowingsubsectionsdescribeREALM’stwocomponentsindetail.Weidentifythepropernounsinourcorpusus-ingtheLEXmethod(Downeyetal.,2007).Inad-ditiontolocatingthepropernounsinthecorpus,LEXalsoconcatenateseachmulti-tokenpropernoun(e.g.,LosAngeles)togetherintoasingletoken.BothofREALM’scomponentsconstructlanguagemodelsfromthistokenizedcorpus.2.2TypeCheckingwithHMM-TInthissection,wedescribeourtype-checkingcom-ponent,whichtakestheformofaHiddenMarkovModelandisreferredtoasHMM-T.HMM-TranksthesetURofnon-seedextractions,withagoalofrankingthoseextractionswithargumentsofpropertypeforRaboveextractionscontainingtypeerrors.Formally,letURidenotethesetoftheithargumentsoftheextractionsinUR.LetSRibedeﬁnedsimi-larlyfortheseedsetSR.Ourtypecheckingtechniqueexploitsthedistri-butionalhypothesis—inthiscase,theintuitionthat699

Intel,headquarteredinSanta+ClaraFigure2:GraphicalmodelemployedbyHMM-T.Shownisthecaseinwhichk=2.Corpuspre-processingresultsinthepropernounSantaClarabeingconcatenatedintoasingletoken.extractionargumentsinURiofthepropertypewilllikelyappearincontextssimilartothoseinwhichtheseedargumentsSRiappear.Inordertoiden-tifytermsthataredistributionallysimilar,wetrainaprobabilisticgenerativeHiddenMarkovModel(HMM),whichtreatseachtokeninthecorpusasgeneratedbyasinglehiddenstatevariable.Here,thehiddenstatestakeintegralvaluesfrom{1,...,T},andeachhiddenstatevariableisitselfgeneratedbysomenumberkofprevioushiddenstates.2For-mally,thejointdistributionofthecorpus,repre-sentedasavectoroftokensw,givenacorrespond-ingvectorofstatestis:P(w|t)=YiP(wi|ti)P(ti|ti−1,...,ti−k)(1)ThedistributionsontherightsideofEquation1canbelearnedfromacorpusinanunsupervisedmanner,suchthatwordswhicharedistributedsim-ilarlyinthecorpustendtobegeneratedbysimi-larhiddenstates(Rabiner,1989).ThegenerativemodelisdepictedasaBayesiannetworkinFigure2.TheﬁgurealsoillustratestheonewayinwhichourimplementationisdistinctfromastandardHMM,namelythatpropernounsaredetectedaprioriandmodeledassingletokens(e.g.,SantaClaraisgeneratedbyasinglehiddenstate).Thisallowsthetypecheckertocomparethestatedistributionsofdifferentpropernounsdirectly,evenwhenthepropernounscontaindifferingnumbersofwords.TogeneratearankingofURusingthelearnedHMMparameters,weranktheargumentseiaccord-ingtohowsimilartheirstatedistributionsP(t|ei)2Ourimplementationmakesthesimplifyingassumptionthateachsentenceinthecorpusisgeneratedindependently.aretothoseoftheseedarguments.3Speciﬁcally,wedeﬁneafunction:f(e)=Xei∈eKL(Pw0∈SRiP(t|w0)|SRi|,P(t|ei))(2)whereKLrepresentsKLdivergence,andtheoutersumistakenovertheargumentseioftheextractione.WeranktheelementsofURinascendingorderoff(e).HMM-Thastwoadvantagesoveramoretradi-tionaltypecheckingapproachofsimplycountingthenumberoftimesinthecorpusthateachextrac-tionappearsinacontextinwhichaseedalsoap-pears(cf.(Ravichandranetal.,2005)).TheﬁrstadvantageofHMM-Tisefﬁciency,asthetraditionalapproachinvolvesacomputationallyexpensivestepofretrievingthepotentiallylargesetofcontextsinwhichtheextractionsandseedsappear.Inourex-periments,usingHMM-Tinsteadofacontext-basedapproachresultsina10-50xreductionintheamountofdatathatisretrievedtoperformtypechecking.Secondly,onsparsedataHMM-Thasthepoten-tialtoimprovetypecheckingaccuracy.Forexam-ple,considercomparingPickerington,asparsecandidateargumentofthetypeCity,totheseedargumentChicago,forwhichthefollowingtwophrasesappearinthecorpus:(i)“Pickerington,Ohio”(ii)“Chicago,Illinois”Inthesephrases,thetextualcontextssurroundingChicagoandPickeringtonarenotidentical,sotothetraditionalapproachthesecontextsoffernoevidencethatPickeringtonandChicagoareofthesametype.ForasparsetokenlikePickerington,thisisproblematicbecausethetokenmayneveroccurinacontextthatpreciselymatchesthatofaseed.Incontrast,intheHMM,thenon-sparsetokensOhioandIllinoisarelikelytohavesimilarstatedistributions,astheyareboththenamesofU.S.States.Thus,inthestatespaceemployedbytheHMM,thecontextsinphrases(i)and(ii)areinfactquitesimilar,allowingHMM-TtodetectthatPickeringtonandChicagoarelikelyofthesametype.Ourexperimentsquan-tifytheperformanceimprovementsthatHMM-Tof-3ThedistributionP(t|ei)foranyeicanbeobtainedfromtheHMMparametersusingBayesRule.700

fersoverthetraditionalapproachfortypecheckingsparsedata.ThetimerequiredtolearnHMM-T’sparametersscalesproportionaltoTk+1timesthecorpussize.Thus,fortractability,HMM-TusesarelativelysmallstatespaceofT=20statesandalimitedkvalueof3.Whilethesesettingsaresufﬁcientfortypechecking(e.g.,determiningthatSantaClaraisacity)theyaretoocoarse-grainedtoassessrelationsbetweenarguments(e.g.,determiningthatSantaClaraistheparticularcityinwhichIntelisheadquartered).WenowturntotheREL-GRAMScomponent,whichperformsthelattertask.2.3RelationAssessmentwithREL-GRAMSREALM’srelationassessmentcomponent,calledREL-GRAMS,testswhethertheextractedargumentshaveadesiredrelationship,butgivenREALM’smin-imalinputithasnoaprioriinformationabouttherelationship.REL-GRAMSreliesinsteadonthedis-tributionalhypothesistotesteachextraction.AsarguedinSection2.1,itisintractabletobuildanaccuratelanguagemodelforcontextdistributionssurroundingsparseargumentpairs.Toovercomethisproblem,weintroducerelationaln-grammod-els.Ratherthansimplymodelingthecontextdistri-butionaroundagivenargument,arelationaln-grammodelspeciﬁesseparatecontextdistributionsforanargumentsconditionedoneachoftheotherargu-mentswithwhichitappears.Therelationaln-grammodelallowsustoestimatecontextdistributionsforpairsofarguments,evenwhentheargumentsdonotappeartogetherwithinaﬁxedwindowofnwords.Further,byconsideringonlyconsecutiveargumentpairs,thenumberofdistinctargumentpairsinthemodelgrowsatmostlinearlywiththenumberofsentencesinthecorpus.Thus,therelationaln-grammodelcanscale.Formally,forapairofarguments(e1,e2),are-lationaln-grammodelestimatesthedistributionsP(w1,...,wn|wi=e1,e1↔e2)foreach1≤i≤n,wherethenotatione1↔e2indicatestheeventthate2isthenextargumenttoeithertherightortheleftofe1inthecorpus.REL-GRAMSbeginsbybuildingarelationaln-grammodeloftheargumentsinthecorpus.Fornotationalconvenience,werepresentthemodel’sdistributionsintermsof“contextvectors”foreachpairofarguments.Formally,foragivensentencecontainingargumentse1ande2consecutively,wedeﬁneacontextoftheorderedpair(e1,e2)tobeanywindowofntokensarounde1.LetC={c1,c2,...,c|C|}bethesetofallcontextsofallar-gumentpairsfoundinthecorpus.4Forapairofar-guments(ej,ek),wemodeltheirrelationshipusinga|C|dimensionalcontextvectorv(ej,ek),whosei-thdimensioncorrespondstothenumberoftimescon-textcioccurredwiththepair(ej,ek)inthecorpus.Thesecontextvectorsaresimilartodocumentvec-torsfromInformationRetrieval(IR),andwelever-ageIRresearchtocomparethem,asdescribedbe-low.Toassesseachextraction,wedeterminehowsim-ilaritscontextvectoristoacanonicalseedvec-tor(createdbysummingthecontextvectorsoftheseeds).Whiletherearemanypotentialmethodsfordeterminingsimilarity,inthisworkwerankex-tractionsbydecreasingvaluesoftheBM25dis-tancemetric.BM25isaTF-IDFvariantintro-ducedinTREC-3(Robertsonetal.,1992),whichoutperformedboththestandardcosinedistanceandasmoothedKLdivergenceonourdata.3ExperimentalResultsThissectiondescribesourexperimentsonIEassess-mentforsparsedata.Westartbydescribingourexperimentalmethodology,andthenpresentourre-sults.TheﬁrstexperimentteststhehypothesisthatHMM-Toutperformsann-gram-basedmethodonthetaskoftypechecking.ThesecondexperimentteststhehypothesisthatREALMoutperformsmulti-pleapproachesfrompreviouswork,andalsooutper-formseachofitsHMM-TandREL-GRAMScompo-nentstakeninisolation.3.1ExperimentalMethodologyThecorpususedforourexperimentsconsistedofasampleofsentencestakenfromWebpages.FromaninitialcrawlofninemillionWebpages,wese-lectedsentencescontainingrelationsbetweenpropernouns.Theresultingtextcorpusconsistedofabout4Pre-computingthesetCrequiresidentifyinginadvancethepotentialrelationargumentsinthecorpus.WeconsiderthepropernounsidentiﬁedbytheLEXmethod(seeSection2.1)tobethepotentialarguments.701

threemillionsentences,andwastokenizedasde-scribedinSection2.Fortractability,beforeandafterperformingtokenization,wereplacedeachtokenoc-curringfewerthanﬁvetimesinthecorpuswithoneoftwo“unknownword”markers(oneforcapital-izedwords,andoneforuncapitalizedwords).Thispreprocessingresultedinacorpuscontainingaboutsixty-ﬁvemilliontotaltokens,and214,787uniquetokens.Weevaluatedperformanceonfourrelations:Conquered,Founded,Headquartered,andMerged.Thesefourrelationswerechosenbecausetheytypicallytakepropernounsasarguments,andincludedalargenumberofsparseextractions.ForeachrelationR,thecandidateextractionlistERwasobtainedusingTEXTRUNNER(Bankoetal.,2007).TEXTRUNNERisanIEsystemthatcomputesanin-dexofallextractedrelationshipsitrecognizes,intheformof(object,predicate,object)triples.Foreachofourtargetrelations,weexecutedasinglequerytotheTEXTRUNNERindexforextractionswhosepredicatecontainedaphraseindicativeoftherela-tion(e.g.,“foundedby”,“headquarteredin”),andtheresultsformedourextractionlist.Foreachrela-tion,the10mostfrequentextractionsservedasboot-strappedseeds.Allofthenon-seedextractionsweresparse(noargumentpairswereextractedmorethantwiceforagivenrelation).Thesetestsetscontainedatotalof361extractions.3.2TypeCheckingExperimentsAsdiscussedinSection2.2,onsparsedataHMM-Thasthepotentialtooutperformtypecheckingmeth-odsthatrelyontextualsimilaritiesofcontextvec-tors.Toevaluatethisclaim,wetestedtheHMM-TsystemagainstanN-GRAMStypecheckingmethodonthetaskoftype-checkingtheargumentstoare-lation.TheN-GRAMSmethodcomparesthecontextvectorsofextractionsinthesamewayastheREL-GRAMSmethoddescribedinSection2.3,butisnotrelational(N-GRAMSconsidersthedistributionofeachextractionargumentindependently,similartoHMM-T).Wetaggedanextractionastypecorrectiffbothargumentswerevalidfortherelation,ignoringwhethertherelationheldbetweenthearguments.TheresultsofourtypecheckingexperimentsareshowninTable1.Foralltypes,HMM-Toutper-formsN-GRAMS,andHMM-Treduceserror(mea-TypeHMM-TN-GRAMSConquered0.9170.767Founded0.8270.636Headquartered0.7340.589Merged0.9200.854Average0.8490.712Table1:TypeCheckingPerformance.Listedisareaundertheprecision/recallcurve.HMM-Toutper-formsN-GRAMSforallrelations,andreducestheerrorintermsofmissingareaunderthecurveby46%onaverage.suredinmissingareaundertheprecision/recallcurve)by46%.Theperformancedifferenceoneachrelationisstatisticallysigniﬁcant(p<0.01,two-sampledt-test),usingthemethodologyformeasur-ingthestandarddeviationofareaunderthepreci-sion/recallcurvegivenin(RichardsonandDomin-gos,2006).N-GRAMS,likeREL-GRAMS,employstheBM-25metrictomeasuredistributionalsimilar-itybetweenextractionsandseeds.ReplacingBM-25withcosinedistancecutsHMM-T’sadvantageoverN-GRAMS,butHMM-T’serrorrateisstill23%loweronaverage.3.3ExperimentswithREALMTheREALMsystemcombinesthetypecheckingandrelationassessmentcomponentstoassessex-tractions.Here,wetesttheabilityofREALMtoimprovetherankingofastateoftheartIEsystem,TEXTRUNNER.Fortheseexperiments,weevalu-ateREALMagainsttheTEXTRUNNERfrequency-basedordering,apattern-learningapproach,andtheHMM-TandREL-GRAMScomponentstakeniniso-lation.TheTEXTRUNNERfrequency-basedorder-ingranksextractionsindecreasingorderoftheirex-tractionfrequency,andimportantly,forourtaskthisorderingisessentiallyequivalenttothatproducedbythe“Urns”(Downeyetal.,2005)andPointwiseMu-tualInformation(Etzionietal.,2005)approachesemployedinpreviouswork.Thepattern-learningapproach,denotedasPL,ismodeledafterSnowball(Agichtein,2006).Theal-gorithmandparametersettingsforPLwerethosemanuallytunedfortheHeadquarteredrelationinpreviouswork(Agichtein,2005).Asensitivityanalysisoftheseparametersindicatedthatthere-702

ConqueredFoundedHeadquarteredMergedAverageAvg.Prec.0.6980.5780.4000.7420.605TEXTRUNNER0.7380.6990.7100.7840.733PL0.8850.6330.6510.8520.785PL+HMM-T0.8830.7220.7270.9000.808HMM-T0.8300.7760.6780.8640.787REL-GRAMS0.929(39%)0.7130.7580.8860.822REALM0.907(19%)0.781(27%)0.810(35%)0.908(38%)0.851(39%)Table2:PerformanceofREALMforassessmentofsparseextractions.Listedisareaunderthepreci-sion/recallcurveforeachmethod.Inparenthesesisthepercentagereductioninerroroverthestrongestbaselinemethod(TEXTRUNNERorPL)foreachrelation.“Avg.Prec.”denotesthefractionofcorrectexamplesinthetestsetforeachrelation.REALMoutperformsitsREL-GRAMSandHMM-Tcomponentstakeninisolation,aswellastheTEXTRUNNERandPLsystemsfrompreviouswork.sultsaresensitivetotheparametersettings.How-ever,wefoundnoparametersettingsthatperformedsigniﬁcantlybetter,andmanysettingsperformedsigniﬁcantlyworse.Assuch,webelieveourre-sultsreasonablyreﬂecttheperformanceofapatternlearningsystemonthistask.BecausePLperformsrelationassessment,wealsoattemptedcombiningPLwithHMM-Tinahybridmethod(PL+HMM-T)analogoustoREALM.TheresultsoftheseexperimentsareshowninTa-ble2.REALMoutperformstheTEXTRUNNERandPLbaselinesforallrelations,andreducesthemiss-ingareaunderthecurvebyanaverageof39%rel-ativetothestrongestbaseline.TheperformancedifferencesbetweenREALMandTEXTRUNNERarestatisticallysigniﬁcantforallrelations,asarediffer-encesbetweenREALMandPLforallrelationsex-ceptConquered(p<0.01,two-sampledt-test).ThehybridREALMsystemalsooutperformseachofitscomponentsinisolation.4RelatedWorkToourknowledge,REALMistheﬁrstsystemtouselanguagemodelingtechniquesforIEAssessment.Redundancy-basedapproachestopattern-basedIEassessment(Downeyetal.,2005;Etzionietal.,2005)requirethatextractionsappearrelativelyfre-quentlywithalimitedsetofpatterns.Incontrast,REALMutilizesallcontextstobuildamodelofex-tractions,ratherthanalimitedsetofpatterns.OurexperimentsdemonstratethatREALMoutperformstheseapproachesonsparsedata.Typecheckingusingnamed-entitytaggershasbeenpreviouslyshowntoimprovetheprecisionofpattern-basedIEsystems(Agichtein,2005;Feld-manetal.,2006),buttheHMM-Ttype-checkingcomponentwedevelopdiffersfromthisworkinim-portantways.Named-entitytaggersarelimitedinthattheytypicallyrecognizeonlysmallsetoftypes(e.g.,ORGANIZATION,LOCATION,PERSON),andtheyrequirehand-taggedtrainingdataforeachtype.HMM-T,bycontrast,performstypecheck-ingforanytype.Finally,HMM-Tdoesnotrequirehand-taggedtrainingdata.Patternlearningisacommontechniqueforex-tractingandassessingsparsedata(e.g.(Agichtein,2005;RiloffandJones,1999;Pas¸caetal.,2006)).OurexperimentsdemonstratethatREALMoutper-formsapatternlearningsystemcloselymodeledaf-ter(Agichtein,2005).REALMisinspiredbypat-ternlearningtechniques(inparticular,bothusethedistributionalhypothesistoassesssparsedata)butisdistinctinimportantways.Patternlearningtech-niquesrequiresubstantialprocessingofthecorpusaftertherelationstheyassesshavebeenspeciﬁed.Becauseofthis,patternlearningsystemsareun-suitedtoOpenIE.Unlikethesetechniques,REALMpre-computeslanguagemodelswhichallowittoas-sessextractionsforarbitraryrelationsatrun-time.Inessence,pattern-learningmethodsrunintimelin-earinthenumberofrelationswhereasREALM’sruntimeisconstantinthenumberofrelations.Thus,REALMscalesreadilytolargenumbersofrelationswhereaspattern-learningmethodsdonot.703

AseconddistinctionofREALMisthatitstypechecker,unlikethenamedentitytaggersemployedinpatternlearningsystems(e.g.,Snowball),canbeusedtoidentifyarbitrarytypes.AﬁnaldistinctionisthatthelanguagemodelsREALMemploysrequirefewerparametersandheuristicsthanpatternlearn-ingtechniques.SimilardistinctionsexistbetweenREALMandarecentsystemdesignedtoassesssparseextractionsbybootstrappingaclassiﬁerforeachtargetrelation(Feldmanetal.,2006).Asinpatternlearning,con-structingtheclassiﬁersrequiressubstantialprocess-ingafterthetargetrelationshavebeenspeciﬁed,andasetofhand-taggedexamplesperrelation,makingitunsuitableforOpenIE.5ConclusionsThispaperdemonstratedthatunsupervisedlanguagemodels,asembodiedintheREALMsystem,areaneffectivemeansofassessingsparseextractions.AnotherattractivefeatureofREALMisitsscal-ability.Scalabilityisaparticularlyimportantcon-cernforOpenInformationExtraction,thetaskofex-tractinglargenumbersofrelationsthatarenotspec-iﬁedinadvance.BecauseHMM-TandREL-GRAMSbothpre-computelanguagemodels,REALMcanbequeriedefﬁcientlytoperformIEAssessment.Fur-ther,thelanguagemodelsareconstructedindepen-dentlyofthetargetrelations,allowingREALMtoperformIEAssessmentevenwhenrelationsarenotspeciﬁedinadvance.Infuturework,weplantodevelopaprobabilisticmodeloftheinformationcomputedbyREALM.Wealsoplantoevaluatetheuseofnon-localcontextforIEAssessmentbyintegratingdocument-levelmod-elingtechniques(e.g.,LatentDirichletAllocation).AcknowledgementsThisresearchwassupportedinpartbyNSFgrantsIIS-0535284andIIS-0312988,DARPAcontractNBCHD030010,ONRgrantN00014-05-1-0185aswellasagiftfromGoogle.Theﬁrstauthorissup-portedbyanMSRgraduatefellowshipsponsoredbyMicrosoftLiveLabs.WethankMicheleBanko,JeffBilmes,KatrinKirchhoff,andAlexYatesforhelpfulcomments.ReferencesE.Agichtein.2005.ExtractingRelationsFromLargeTextCollections.Ph.D.thesis,DepartmentofCom-puterScience,ColumbiaUniversity.E.Agichtein.2006.Conﬁdenceestimationmethodsforpartiallysupervisedrelationextraction.InSDM2006.M.Banko,M.Cararella,S.Soderland,M.Broadhead,andO.Etzioni.2007.Openinformationextractionfromtheweb.InProcs.ofIJCAI2007.D.Downey,O.Etzioni,andS.Soderland.2005.AProb-abilisticModelofRedundancyinInformationExtrac-tion.InProcs.ofIJCAI2005.D.Downey,M.Broadhead,andO.Etzioni.2007.Locat-ingcomplexnamedentitiesinwebtext.InProcs.ofIJCAI2007.O.Etzioni,M.Cafarella,D.Downey,S.Kok,A.Popescu,T.Shaked,S.Soderland,D.Weld,andA.Yates.2005.Unsupervisednamed-entityextractionfromtheweb:Anexperimentalstudy.ArtiﬁcialIntelligence,165(1):91–134.R.Feldman,B.Rosenfeld,S.Soderland,andO.Etzioni.2006.Self-supervisedrelationextractionfromtheweb.InISMIS,pages755–764.Z.Harris.1985.Distributionalstructure.InJ.J.Katz,editor,ThePhilosophyofLinguistics,pages26–47.NewYork:OxfordUniversityPress.C.D.ManningandH.Sch¨utze.1999.FoundationsofStatisticalNaturalLanguageProcessing.M.Pas¸ca,D.Lin,J.Bigham,A.Lifchits,andA.Jain.2006.Namesandsimilaritiesontheweb:Factextrac-tioninthefastlane.InProcs.ofACL/COLING2006.L.R.Rabiner.1989.Atutorialonhiddenmarkovmodelsandselectedapplicationsinspeechrecognition.Pro-ceedingsoftheIEEE,77(2):257–286.D.Ravichandran,P.Pantel,andE.H.Hovy.2005.Ran-domizedAlgorithmsandNLP:UsingLocalitySensi-tiveHashFunctionsforHighSpeedNounClustering.InProcs.ofACL2005.M.RichardsonandP.Domingos.2006.MarkovLogicNetworks.MachineLearning,62(1-2):107–136.E.RiloffandR.Jones.1999.LearningDictionariesforInformationExtractionbyMulti-levelBoot-strapping.InProcs.ofAAAI-99,pages1044–1049.S.E.Robertson,S.Walker,M.Hancock-Beaulieu,A.Gull,andM.Lau.1992.OkapiatTREC-3.InTextREtrievalConference,pages21–30.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 704–711,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

704

Forest-to-StringStatisticalTranslationRulesYangLiu,YunHuang,QunLiuandShouxunLinKeyLaboratoryofIntelligentInformationProcessingInstituteofComputingTechnologyChineseAcademyofSciencesP.O.Box2704,Beijing100080,China{yliu,huangyun,liuqun,sxlin}@ict.ac.cnAbstractInthispaper,weproposeforest-to-stringrulestoenhancetheexpressivepoweroftree-to-stringtranslationmodels.Aforest-to-stringruleiscapableofcapturingnon-syntacticphrasepairsbydescribingthecor-respondencebetweenmultipleparsetreesandonestring.Tointegratetheserulesintotree-to-stringtranslationmodels,auxil-iaryrulesareintroducedtoprovideagen-eralizationlevel.Experimentalresultsshowthat,ontheNIST2005Chinese-Englishtestset,thetree-to-stringmodelaugmentedwithforest-to-stringrulesachievesarelativeim-provementof4.3%intermsofBLEUscoreovertheoriginalmodelwhichallowstree-to-stringrulesonly.1IntroductionThepasttwoyearshavewitnessedtherapidde-velopmentoflinguisticallysyntax-basedtranslationmodels(Quirketal.,2005;Galleyetal.,2006;Marcuetal.,2006;Liuetal.,2006),whichinducetree-to-stringtranslationrulesfromparalleltextswithlinguisticannotations.Theydemonstratedverypromisingresultswhencomparedwiththestateoftheartphrase-basedsystem(OchandNey,2004)intheNIST2006machinetranslationevaluation1.WhileGalleyetal.(2006)andMarcuetal.(2006)putemphasisontargetlanguageanalysis,Quirketal.(2005)andLiuetal.(2006)showbeneﬁtsfrommodelingthesyntaxofsourcelanguage.1Seehttp://www.nist.gov/speech/tests/mt/Onemajorproblemwithlinguisticallysyntax-basedmodels,however,isthattree-to-stringrulesfailtosyntactifynon-syntacticphrasepairsbecausetheyrequireasyntaxtreefragmentoverthephrasetobesyntactiﬁed.Here,wedistinguishbetweensyn-tacticandnon-syntacticphrasepairs.By“syntactic”wemeanthatthephrasepairissubsumedbysomesyntaxtreefragment.Thephrasepairswithouttreesoverthemarenon-syntactic.Marcuetal.(2006)reportthatapproximately28%ofbilingualphrasesarenon-syntacticontheirEnglish-Chinesecorpus.Webelievethatitisimportanttomakeavailabletosyntax-basedmodelsallthebilingualphrasesthataretypicallyavailabletophrase-basedmodels.Ononehand,phraseshavebeenproventobeasimpleandpowerfulmechanismformachinetranslation.Theyexcelatcapturingtranslationsofshortidioms,providinglocalre-orderingdecisions,andincorpo-ratingcontextinformationstraightforwardly.Chi-ang(2005)showssigniﬁcantimprovementbykeep-ingthestrengthsofphraseswhileincorporatingsyn-taxintostatisticaltranslation.Ontheotherhand,theperformanceoflinguisticallysyntax-basedmod-elscanbehinderedbymakinguseofonlysyntac-ticphrasepairs.Studiesrevealthatlinguisticallysyntax-basedmodelsaresensitivetosyntacticanal-ysis(QuirkandCorston-Oliver,2006),whichisstillnotreliableenoughtohandlereal-worldtextsduetolimitedsizeanddomainoftrainingdata.Varioussolutionsareproposedtotackletheprob-lem.Galleyetal.(2004)handlenon-constituentphrasaltranslationbytraversingthetreeupwardsuntilreachesanodethatsubsumesthephrase.Marcuetal.(2006)arguethatthischoiceisinap-705

propriatebecauselargeapplicabilitycontextsarere-quired.Foranon-syntacticphrasepair,Marcuetal.(2006)createaxRSruleheadedbyapseudo,non-syntacticnonterminalsymbolthatsubsumesthephraseandcorrespondingmulti-headedsyntacticstructure;andonesiblingxRSrulethatexplainshowthenon-syntacticnonterminalsymbolcanbecom-binedwithothergenuinenonterminalssoastoob-taingenuineparsetrees.Thenameofthepseudononterminalisdesignedtoreﬂecthowthecorre-spondingrulecanbefullyrealized.However,theyneglectalignmentconsistencywhencreatingsiblingrules.Inaddition,itishardforthenamingmecha-nismtodealwithmorecomplexphenomena.Liuetal.(2006)treatbilingualphrasesaslexi-calizedTATs(Tree-to-stringAlignmentTemplate).Abilingualphrasecanbeusedindecodingifthesourcephraseissubsumedbytheinputparsetree.Althoughthissolutiondoeshelp,onlysyntacticbilingualphrasesareavailabletotheTAT-basedmodel.Moreover,itisproblematictocombinethetranslationprobabilitiesofbilingualphrasesandTATs,whichareestimatedindependently.Inthispaper,weproposeforest-to-stringruleswhichdescribethecorrespondencebetweenmulti-pleparsetreesandastring.Theycannotonlycap-turenon-syntacticphrasepairsbutalsohavetheca-pabilityofgeneralization.Tointegratetheserulesintotree-to-stringtranslationmodels,auxiliaryrulesareintroducedtoprovideageneralizationlevel.Asthereisnopseudonodeornamingmechanism,theintegrationofforest-to-stringrulesisﬂexible,rely-ingonlyontheirrootnodes.Theforest-to-stringandauxiliaryrulesenabletree-to-stringmodelstoderiveinamoregeneralway,whilethestrengthsofcon-ventionaltree-to-stringrulesstillremain.2Forest-to-StringTranslationRulesWedeﬁneatree-to-stringrulerasatriple(cid:2)˜T,˜S,˜A(cid:3),whichdescribesthealignment˜Abetweenasourceparsetree˜T=T(fJ(cid:2)1)andatargetstring˜S=eI(cid:2)1.AsourcestringfJ(cid:2)1,whichisthesequenceofleafnodesofT(fJ(cid:2)1),consistsofbothterminals(sourcewords)andnonterminals(phrasalcategories).Atar-getstringeI(cid:2)1isalsocomposedofbothterminals(targetwords)andnonterminals(placeholders).AnIPNPNN (cid:0)VPSB(cid:0)VPNPNN  VV(cid:0)(cid:2)PU(cid:0)Thegunmanwaskilledbypolice.Figure1:AnEnglishsentencealignedwithaChi-neseparsetree.alignment˜AisdeﬁnedasasubsetoftheCartesianproductofsourceandtargetsymbolpositions:˜A⊆{(j,i):j=1,...,J(cid:2);i=1,...,I(cid:2)}Aderivationθ=r1◦r2◦...◦rnisaleft-mostcompositionoftranslationrulesthatexplainshowasourceparsetreeT=T(fJ1),atargetsen-tenceS=eI1,andthewordalignmentAaresyn-chronouslygenerated.Forexample,Table1demon-stratesaderivationcomposedofonlytree-to-stringrulesforthe(cid:2)T,S,A(cid:3)tupleinFigure12.Aswementionedbefore,tree-to-stringrulescannotsyntactifyphrasepairsthatarenotsubsumedbyanysyntaxtreefragments.Forexample,forthephrasepair(cid:2)“ (cid:0)(cid:0)”,“Thegunmanwas”(cid:3)inFig-ure1,itisimpossibletoextractanequivalenttree-to-stringrulethatsubsumesthesamephrasepairbecausevalidtree-to-stringrulescannotbemulti-headed.Toaddressthisproblem,weproposeforest-to-stringrules3tosubsumethenon-syntacticphrasepairs.Aforest-to-stringruler4isatriple(cid:2)˜F,˜S,˜A(cid:3),whichdescribesthealignment˜AbetweenKsourceparsetrees˜F=˜TK1andatargetstring˜S.ThesourcestringfJ(cid:2)1isthereforethesequenceofleafnodesof˜F.Auxiliaryrulesareintroducedtointegrateforest-to-stringrulesintotree-to-stringtranslationmodels.Anauxiliaryruleisaspecialunlexicalizedtree-to-stringrulethatallowsmultiplesourcenonterminals2Weuse“X”todenoteanonterminalinthetargetstring.Iftherearemorethanonenonterminals,theyareindexed.3Theterm“forest”referstoanorderedandﬁnitesetoftrees.4Westilluse“r”torepresentaforest-to-stringruletoreducenotationaloverhead.706

No.Rule(1)(IP(NP)(VP)(PU))X1X2X31:12:23:3(2)(NP(NN (cid:0)))Thegunman1:11:2(3)(VP(SB(cid:0))(VP(NP(NN))(VV(cid:0)(cid:2))))waskilledbyX1:12:43:2(4)(NN  )police1:1(5)(PU(cid:0)).1:1Table1:Aderivationcomposedofonlytree-to-stringrulesforFigure1.No.Rule(1)(IP(NP)(VP(SB)(VP))(PU))X1X21:12:13:24:2(2)(NP(NN (cid:0)))(SB(cid:0))Thegunmanwas1:11:22:3(3)(VP(NP)(VV(cid:0)(cid:2)))(PU(cid:0))killedbyX.1:32:13:4(4)(NP(NN  ))police1:1Table2:Aderivationcomposedoftree-to-string,forest-to-string,andauxiliaryrulesforFigure1.tocorrespondtoonetargetnonterminal,suggestingthattheforest-to-stringrulesthatarerootedatsuchsourcenonterminalscanbeintegrated.Forexample,Table2showsaderivationcom-posedoftree-to-string,forest-to-string,andauxil-iaryrulesforthe(cid:2)T,S,A(cid:3)tupleinFigure1.r1isanauxiliaryrule,r2andr3areforest-to-stringrules,andr4isaconventionaltree-to-stringrule.FollowingMarcuetal.(2006),wedeﬁnetheprobabilityofatuple(cid:2)T,S,A(cid:3)asthesumoverallderivationsθi∈Θthatareconsistentwiththetuple,c(Θ)=(cid:2)T,S,A(cid:3).Theprobabilityofeachderiva-tionθiisgivenbytheproductoftheprobabilitiesofalltherulesp(rj)inthederivation.Pr(T,S,A)=(cid:2)θi∈Θ,c(Θ)=(cid:4)T,S,A(cid:5)(cid:3)rj∈θip(rj)(1)3TrainingWeobtaintree-to-stringandforest-to-stringrulesfromword-aligned,sourcesideparsedbilingualcor-pus.TheextractionalgorithmisshowninFigure2.NotethatT(cid:2)denoteseitheratreeoraforest.Foreachspan,the(cid:2)tree/forest,string,alignment(cid:3)triplesareidentiﬁedﬁrst.Ifatripleisconsistentwiththealignment,theskeletonofthetripleiscomputedthen.Askeletonsisarulesatisfyingthefollowing:1.s∈R(t),sisinducedfromt.2.node(T(s))≥2,thetree/forestofscontainstwoormorenodes.3.∀r∈R(t)∧node(T(r))≥2,T(s)⊆T(r),thetree/forestofsisthesubgraphofthatofanyrcontainingtwoormorenodes.1:Input:asourcetreeT=T(fJ1),atargetstringS=eI1,andwordalignmentAbetweenthem2:R:=∅3:foru:=0toJ−1do4:forv:=1toJ−udo5:identifythetriplesetTcorrespondingtospan(v,v+u)6:foreachtriplet=(cid:3)T(cid:2),S(cid:2),A(cid:2)(cid:4)∈Tdo7:if(cid:3)T(cid:2),S(cid:2)(cid:4)isnotconsistentwithAthen8:continue9:endif10:ifu=0∧node(T(cid:2))=1then11:addttoR12:add(cid:3)root(T(cid:2)),“X”,1:1(cid:4)toR13:else14:computetheskeletonsofthetriplet15:registerrulesthatarebuiltonsusingrulesextractedfromthesub-triplesoft:R:=R∪build(s,R)16:endif17:endfor18:endfor19:endfor20:Output:rulesetRFigure2:Ruleextractionalgorithm.Giventheskeletonandrulesextractedfromthesub-triples,therulesforthetriplecanbeacquired.Forexample,thealgorithmidentiﬁesthefollow-ingtripleforspan(1,2)inFigure1:(cid:3)(NP(NN (cid:0)))(SB(cid:0)),“Thegunmanwas”,1:11:22:3(cid:4)Theskeletonofthetripleis:(cid:3)(NP)(SB),“X1X2”,1:12:2(cid:4)Asthealgorithmproceedsbottom-up,ﬁveruleshavealreadybeenextractedfromthesub-triples,rootedat“NP”and“SB”respectively:(cid:3)(NP),“X”,1:1(cid:4)(cid:3)(NP(NN)),“X”,1:1(cid:4)(cid:3)(NP(NN (cid:0))),“Thegunman”,1:11:2(cid:4)707

(cid:3)(SB),“X”,1:1(cid:4)(cid:3)(SB(cid:0)),“was”,1:1(cid:4)Hence,wecanobtainnewrulesbyreplacingthesourceandtargetsymbolsoftheskeletonwithcorre-spondingrulesandalsobymodifyingthealignmentinformation.Fortheabovetriple,thecombinationoftheﬁverulesproduces2×3=6newrules:(cid:3)(NP)(SB),“X1X2”,1:12:2(cid:4)(cid:3)(NP)(SB(cid:0)),“Xwas”,1:12:2(cid:4)(cid:3)(NP(NN))(SB),“X1X2”,1:12:2(cid:4)(cid:3)(NP(NN))(SB(cid:0)),“Xwas”,1:12:2(cid:4)(cid:3)(NP(NN (cid:0)))(SB),“ThegunmanX”,1:11:2(cid:4)(cid:3)(NP(NN (cid:0)))(SB(cid:0)),“Thegunmanwas”,1:11:22:3(cid:4)Sinceweneedonlytocheckthealignmentcon-sistency,inprincipleallphrasepairscanbecapturedbytree-to-stringandforest-to-stringrules.Tolowerthecomplexityforbothtraininganddecoding,weimposefourrestrictions:1.Boththeﬁrstandthelastsymbolsinthetargetstringmustbealignedtosomesourcesymbols.2.Theheightofatreeorforestisnogreaterthanh.3.Thenumberofdirectdescendantsofanodeisnogreaterthanc.4.Thenumberofleafnodesisnogreaterthanl.Althoughpossible,itisinfeasibletolearnaux-iliaryrulesfromtrainingdata.Toextractanauxil-iaryrulewhichintegratesatleastoneforest-to-stringrule,oneneedtraversetheparsetreeupwardsuntilonereachesanodethatsubsumestheentireforestwithoutviolatingthealignmentconsistency.Thisusuallyresultsinverycomplexauxiliaryrules,es-peciallyonreal-worldtrainingdata,makingbothtraininganddecodingveryslow.Asaresult,weconstructauxiliaryrulesindecodinginstead.4DecodingGivenasourceparsetreeT(fJ1),ourdecoderﬁndsthetargetyieldofthesinglebestderivationthathassourceyieldofT(fJ1):ˆS=argmaxS,APr(T,S,A)=argmaxS,A(cid:2)θi∈Θ,c(Θ)=(cid:4)T,S,A(cid:5)(cid:3)rj∈θip(rj)1:Input:asourceparsetreeT=T(fJ1)2:foru:=0toJ−1do3:forv:=1toJ−udo4:foreachT(cid:2)spanningfromvtov+udo5:ifT(cid:2)isatreethen6:foreachusabletree-to-stringrulerdo7:foreachderivationθinferredfromrandderivationsinmatrixdo8:addθtomatrix[v,v+u,root(T(cid:2))]9:endfor10:endfor11:searchsubcelldivisionsD[v,v+u]12:foreachsubcelldivisiond∈D[v,v+u]do13:ifdcontainsatleastoneforestcellthen14:constructauxiliaryrulera15:foreachderivationθinferredfromraandderivationsinmatrixdo16:addθtomatrix[v,v+u,root(T(cid:2))]17:endfor18:endif19:endfor20:else21:foreachusableforest-to-stringrulerdo22:foreachderivationθinferredfromrandderivationsinmatrixdo23:addθtomatrix[v,v+u,“”]24:endfor25:endfor26:searchsubcelldivisionsD[v,v+u]27:endif28:endfor29:endfor30:endfor31:ﬁndthebestderivationˆθinmatrix[1,J,root(T)]andgetthebesttranslationˆS=e(ˆθ)32:Output:atargetstringˆSFigure3:Decodingalgorithm.≈argmaxS,A,θ(cid:3)rj∈θ,c(θ)=(cid:4)T,S,A(cid:5)p(rj)(2)Figure3demonstratesthedecodingalgorithm.Itorganizesthederivationsintoanarraymatrixwhosecellsmatrix[j1,j2,X]aresetsofderivations.[j1,j2,X]representsatree/forestrootedatXspan-ningfromj1toj2.Weusetheemptystring“”todenotethepseudorootofaforest.Next,wewillexplainhowtoinferderivationsforatree/forestprovidedausablerule.IfT(r)=T(cid:2),thereisonlyonederivationwhichcontainsonlytheruler.Thisusuallyhappensforleafnodes.IfT(r)⊂T(cid:2),therulerresortstoderivationsfromsubcellstoinfernewderivations.SupposethatthedecoderistotranslatethesourcetreeinFigure1andﬁndsausablerulefor[1,5,“IP”]:(cid:3)(IP(NP)(VP)(PU)),“X1X2X3”,1:12:23:3(cid:4)708

SubcellDivisionAuxiliaryRule[1,1][2,2][3,5](IP(NP)(VP(SB)(VP))(PU))X1X2X31:12:23:34:3[1,2][3,4][5,5](IP(NP)(VP(SB)(VP))(PU))X1X2X31:12:13:24:3[1,3][4,5](IP(NP)(VP(SB)(VP(NP)(VV)))(PU))X1X21:12:13:14:25:2[1,1][2,5](IP(NP)(VP)(PU))X1X21:12:23:2Table3:SubcelldivisionsandcorrespondingauxiliaryrulesforthesourcetreeinFigure1Sincethedecodingalgorithmproceedsinabottom-upfashion,theuncoveredportionshaveal-readybeentranslated.For[1,1,“NP”],supposethatwecanﬁndaderivationinmatrix:(cid:3)(NP(NN (cid:0))),“Thegunman”,1:11:2(cid:4)For[2,4,“VP”],weﬁndaderivationinmatrix:(cid:3)(VP(SB(cid:0))(VP(NP(NN))(VV(cid:0)(cid:2)))),“waskilledbyX”,1:12:43:2(cid:4)(cid:3)(NN 	),“police”,1:1(cid:4)For[5,5,“PU”],weﬁndaderivationinmatrix:(cid:3)(PU(cid:0)),“.”,1:1(cid:4)Henceforth,wegetaderivationfor[1,5,“IP”],showninTable1.Atranslationrulerissaidtobeusabletoaninputtree/forestT(cid:2)ifandonlyif:1.T(r)⊆T(cid:2),thetree/forestofristhesubgraphofT(cid:2).2.root(T(r))=root(T(cid:2)),therootsequenceofT(r)isidenticaltothatofT(cid:2).Forexample,thefollowingrulesareusabletothetree“(NP(NR(cid:0))(NN(cid:0)))”:(cid:3)(NP(NR)(NN)),“X1X2”,1:22:1(cid:4)(cid:3)(NP(NR(cid:0))(NN)),“ChinaX”,1:12:2(cid:4)(cid:3)(NP(NR(cid:0))(NN(cid:2))),“Chinaeconomy”,1:12:2(cid:4)Similarly,theforest-to-stringrule(cid:3)((NP(NR)(NN))(VP)),“X1X2X3”,1:22:13:3(cid:4)isusabletotheforest(NP(NR(cid:2))(NN))(VP(VV	)(NN (cid:2)))Aswementionedbefore,auxiliaryrulesarespe-cialunlexicalizedtree-to-stringrulesthatarebuiltindecodingratherthanlearntfromreal-worlddata.Togetanauxiliaryruleforacell,weneedﬁrstidentifyitssubcelldivision.Acellsequencec1,c2,...,cnisreferredtoasasubcelldivisionofacellcifandonlyif:1.c1.begin=c.begin1:Input:acell[j1,j2],thederivationarraymatrix,thesubcelldivisionarrayD2:ifj1=j2then3:ˆp:=04:foreachderivationθinmatrix[j1,j2,·]do5:ˆp:=max(p(θ),ˆp)6:endfor7:add{[j1,j2]}:ˆptoD[j1,j2]8:else9:if[j1,j2]isaforestcellthen10:ˆp:=011:foreachderivationθinmatrix[j1,j2,·]do12:ˆp:=max(p(θ),ˆp)13:endfor14:add{[j1,j2]}:ˆptoD[j1,j2]15:endif16:forj:=j1toj2−1do17:foreachdivisiond1∈D[j1,j]do18:foreachdivisiond2∈D[j+1,j2]do19:createanewdivision:d:=d1⊕d220:adddtoD[j1,j2]21:endfor22:endfor23:endfor24:endif25:Output:subcelldivisionsD[j1,j2]Figure4:Subcelldivisionsearchalgorithm.2.cn.end=c.end3.cj.end+1=cj+1.begin,1≤j<nGivenasubcelldivision,itiseasytoconstructtheauxiliaryruleforacell.Foreachsubcell,oneneedtransversetheparsetreeupwardsuntilonereachesnodesthatsubsumeit.Alldescendantsofthesenodesaredropped.Thetargetstringconsistsofonlynonterminals,thenumberofwhichisidenticaltothatofsubcells.Tolimitthesearchspace,weas-sumethatthealignmentbetweenthesourcetreeandthetargetstringismonotone.Table3showssomesubcelldivisionsandcorre-spondingauxiliaryrulesconstructedforthesourcetreeinFigure1.Forsimplicity,weignoretherootnodelabel.Thereare2n−1subcelldivisionsforacellwhichhasalengthofn.Weneedonlyconsiderthesub-709

celldivisionswhichcontainatleastoneforestcellbecausetree-to-stringruleshavealreadyexploredthosecontainonlytreecells.TheactualsearchalgorithmforsubcelldivisionsisshowninFigure4.Weusematrix[j1,j2,·]tode-notealltreesorforestsspanningfromj1toj2.ThesubcelldivisionsandtheirassociatedprobabilitiesarestoredinanarrayD.Wedeﬁneanoperator⊕betweentwodivisions:theircellsequencesarecon-catenatedandtheprobabilitiesareaccumulated.Assometimestherearenousablerulesavailable,weintroducedefaultrulestoensurethatwecanal-waysgetatranslationforanyinputparsetree.Ade-faultruleisatree-to-stringrule5,builtintwoways:1.Iftheinputtreecontainsonlyonenode,thetargetstringofthedefaultruleisequaltothesourcestring.2.Iftheheightoftheinputtreeisgreaterthanone,thetreeofthedefaultrulecontainsonlytherootnodeanditsdirectdescendantsoftheinputtree,thestringcontainsonlynontermi-nals,andthealignmentismonotone.Tospeedupthedecoder,welimitthesearchspacebyreducingthenumberofrulesusedforeachcell.Therearetwowaystolimittheruletablesize:byaﬁxedlimitaofhowmanyrulesareretrievedforeachcell,andbyaprobabilitythresholdαthatspec-ifythattheruleprobabilityhastobeabovesomevalue.Also,insteadofkeepingthefulllistofderiva-tionsforacell,westoreatop-scoringsubsetofthederivations.Thiscanalsobedonebyaﬁxedlimitborathresholdβ.ThesubcelldivisionarrayD,inwhichdivisionscontainingforestcellshavepriorityoverthosecomposedofonlytreecells,isprunedbykeepingonlya-bestdivisions.FollowingOchandNey(2002),webaseourmodelonlog-linearframeworkandadoptthesevenfeaturefunctionsdescribedin(Liuetal.,2006).Itisveryimportanttobalancethepreferencebetweenconventionaltree-to-stringrulesandthenewly-introducedforest-to-stringandauxiliaryrules.Astheprobabilitiesofauxiliaryrulesarenotlearntfromtrainingdata,weaddafeaturethatsumsupthe5Therearenodefaultrulesforforestsbecauseonlytree-to-stringrulesareessentialtotree-to-stringtranslationmodels.nodecountofauxiliaryrulesofaderivationtope-nalizetheuseofforest-to-stringandauxiliaryrules.5ExperimentsInthissection,wereportonexperimentswithChinese-to-Englishtranslation.Thetrainingcorpusconsistsof31,149sentencepairswith843,256Chi-nesewordsand949,583Englishwords.Forthelanguagemodel,weusedSRILanguageModelingToolkit(Stolcke,2002)totrainatrigrammodelwithmodiﬁedKneser-Neysmoothing(ChenandGood-man,1998)onthe31,149Englishsentences.Weselected571shortsentencesfromthe2002NISTMTEvaluationtestsetasourdevelopmentcorpus,andusedthe2005NISTMTEvaluationtestsetasourtestcorpus.OurevaluationmetricisBLEU-4(Papinenietal.,2002),ascalculatedbythescriptmteval-v11b.plwithitsdefaultsettingexceptthatweusedcase-sensitivematchingofn-grams.Toperformminimumerrorratetraining(Och,2003)totunethefeatureweightstomaximizethesys-tem’sBLEUscoreondevelopmentset,weusedthescriptoptimizeV5IBMBLEU.m(VenugopalandVo-gel,2005).WeranGIZA++(OchandNey,2000)onthetrainingcorpusinbothdirectionsusingitsdefaultsetting,andthenappliedthereﬁnementrule“diag-and”describedin(Koehnetal.,2003)toobtainasinglemany-to-manywordalignmentforeachsen-tencepair.Next,weemployedaChineseparserwrittenbyDeyiXiong(Xiongetal.,2005)toparseallthe31,149Chinesesentences.Theparserwastrainedonarticles1-270ofPennChineseTreebankversion1.0andachieved79.4%intermsofF1mea-sure.Giventheword-aligned,sourcesideparsedbilin-gualcorpus,weobtainedbilingualphrasesusingthetrainingtoolkitspubliclyreleasedbyPhilippKoehnwithitsdefaultsetting.Then,weappliedextrac-tionalgorithmdescribedinFigure2toextractbothtree-to-stringandforest-to-stringrulesbyrestrictingh=3,c=5,andl=7.Alltherules,includingbilingualphrases,tree-to-stringrules,andforest-to-stringrules,areﬁlteredforthedevelopmentandtestsets.Accordingtodifferentlevelsoflexicalization,wedividetranslationrulesintothreecategories:710

RuleLPUTotalBP251,17300251,173TR56,98341,0273,529101,539FR16,609254,34625,051296,006Table4:Numberofrulesusedinexperiments(BP:bilingualphrase,TR:tree-to-stringrule,FR:forest-to-stringrule;L:lexicalized,P:partiallexicalized,U:unlexicalized).SystemRuleSetBLEU4PharaohBP0.2182±0.0089BP0.2059±0.0083TR0.2302±0.0089LynxTR+BP0.2346±0.0088TR+FR+AR0.2402±0.0087Table5:ComparisonofPharaohandLynxwithdif-ferentrulesets.1.lexicalized:allsymbolsinboththesourceandtargetstringsareterminals2.unlexicalized:allsymbolsinboththesourceandtargetstringsarenonterminals3.partiallexicalized:otherwiseTable4showsthestatisticsofrulesusedinourex-periments.Weﬁndthateventhoughforest-to-stringrulesareintroducedthetotalnumber(i.e.73,592)oflexicalizedtree-to-stringandforest-to-stringrulesisstillfarlessthanthat(i.e.251,173)ofbilingualphrases.Thisdifferenceresultsfromtherestrictionweimposeintrainingthatboththeﬁrstandlastsym-bolsinthetargetstringmustbealignedtosomesourcesymbols.Fortheforest-to-stringrules,par-tiallexicalizedonesareinthemajority.WecomparedoursystemLynxagainstafreelyavailablephrase-baseddecoderPharaoh(Koehnetal.,2003).ForPharaoh,weseta=20,α=0,b=100,β=10−5,anddistortionlimitdl=4.ForLynx,weseta=20,α=0,b=100,andβ=0.Twopostprocessingproceduresrantoimprovetheoutputsofbothsystems:OOVsremovalandrecapi-talization.Table5showsresultsontestsetusingPharaohandLynxwithdifferentrulesets.NotethatLynxiscapableofusingonlybilingualphrasesplusde-Forest-to-StringRuleSetBLEU4None0.2225±0.0085L0.2297±0.0081P0.2279±0.0083U0.2270±0.0087L+P+U0.2312±0.0082Table6:Effectoflexicalized,partiallexicalized,andunlexicalizedforest-to-stringrules.faultrulestoperformmonotonesearch.The95%conﬁdenceintervalswerecomputedusingZhang’ssigniﬁcancetester(Zhangetal.,2004).Wemod-iﬁedittoconformtoNIST’scurrentdeﬁnitionoftheBLEUbrevitypenalty.WeﬁndthatLynxout-performsPharaohsigniﬁcantly.Theintegrationofforest-to-stringrulesachievesanabsoluteimprove-mentof1.0%(4.3%relative)overusingtree-to-stringrulesonly.Thisdifferenceisstatisticallysig-niﬁcant(p<0.01).Italsoachievesbetterresultthantreatingbilingualphrasesaslexicalizedtree-to-stringrules.Toproducethebestresultof0.2402,Lynxmadeuseof26,082tree-to-stringrules,9,219defaultrules,5,432forest-to-stringrules,and2,919auxiliaryrules.Thissuggeststhattree-to-stringrulesstillplayacentralrole,althoughtheintegra-tionofforest-to-stringandauxiliaryrulesisreallybeneﬁcial.Table6demonstratestheeffectofforest-to-stringruleswithdifferentlexicalizationlevels.Weseta=3,α=0,b=10,andβ=0.Thesecondrow“None”showstheresultofusingonlytree-to-stringrules.“L”denotesusingtree-to-stringrulesandlex-icalizedforest-to-stringrules.Similarly,“L+P+U”denotesusingtree-to-stringrulesandallforest-to-stringrules.Weﬁndthatlexicalizedforest-to-stringrulesaremoreuseful.6ConclusionInthispaper,weintroduceforest-to-stringrulestocapturenon-syntacticphrasepairsthatareusuallyunaccessibletotraditionaltree-to-stringtranslationmodels.Withthehelpofauxiliaryrules,forest-to-stringrulescanbeintegratedintotree-to-stringmod-elstooffermoregeneralderivations.Experimentre-sultsshowthatthetree-to-stringmodelaugmentedwithforest-to-stringrulessigniﬁcantlyoutperforms711

theoriginalmodelwhichallowstree-to-stringrulesonly.Ourcurrentruleextractionalgorithmattachestheunalignedtargetwordstothenearestascendantsthatsubsumethem.Thisconstrainthamperstheexpres-sivepowerofourmodel.Wewilltryamoregeneralwayassuggestedin(Galleyetal.,2006),makingnoaprioriassumptionaboutassignmentandusingEMtrainingtolearntheprobabilitydistribution.Wewillalsoconductexperimentsonlargescaletrainingdatatofurtherexamineourdesignphilosophy.AcknowledgementThisworkwassupportedbyNationalNaturalSci-enceFoundationofChina,ContractNo.60603095and60573188.ReferencesStanleyF.ChenandJoshuaGoodman.1998.Anempir-icalstudyofsmoothingtechniquesforlanguagemod-eling.Technicalreport,HarvardUniversityCenterforResearchinComputingTechnology.DavidChiang.2005.Ahierarchicalphrase-basedmodelforstatisticalmachinetranslation.InProceedingsofACL2005,pages263–270,AnnArbor,Michigan,June.MichelGalley,MarkHopkins,KevinKnight,andDanielMarcu.2004.What’sinatranslationrule?InProceedingsofHLT/NAACL2004,pages273–280,Boston,Massachusetts,USA,May.MichelGalley,JonathanGraehl,KevinKnight,DanielMarcu,SteveDeNeefe,WeiWang,andIgnacioThayer.2006.Scalableinferenceandtrainingofcontext-richsyntactictranslationmodels.InProceed-ingsofCOLING/ACL2006,pages961–968,Sydney,Australia,July.PhilippKoehn,FranzJosephOch,andDanielMarcu.2003.Statisticalphrase-basedtranslation.InProceed-ingsofHLT/NAACL2003,pages127–133,Edmonton,Canada,May.YangLiu,QunLiu,andShouxunLin.2006.Tree-to-stringalignmenttemplateforstatisticalmachinetrans-lation.InProceedingsofCOLING/ACL2006,pages609–616,Sydney,Australia,July.DanielMarcu,WeiWang,AbdessamadEchihabi,andKevinKnight.2006.Spmt:Statisticalmachinetrans-lationwithsyntactiﬁedtargetlanguagephrases.InProceedingsofEMNLP2006,pages44–52,Sydney,Australia,July.FranzJ.OchandHermannNey.2000.Improvedstatis-ticalalignmentmodels.InProceedingsofACL2000,pages440–447.FranzJ.OchandHermannNey.2002.Discriminativetrainingandmaximumentropymodelsforstatisticalmachinetranslation.InProceedingsofACL2002,pages295–302.FranzJ.OchandHermannNey.2004.Thealignmenttemplateapproachtostatisticalmachinetranslation.ComputationalLinguistics,30(4):417–449.FranzJ.Och.2003.Minimumerrorratetraininginsta-tisticalmachinetranslation.InProceedingsofACL2003,pages160–167.KishorePapineni,SalimRoukos,ToddWard,andWei-JingZhu.2002.Bleu:amethodforautomaticeval-uationofmachinetranslation.InProceedingsofACL2002,pages311–318,Philadephia,USA,July.ChrisQuirkandSimonCorston-Oliver.2006.Theim-pactofparsequalityonsyntactically-informedstatis-ticalmachinetranslation.InProceedingsofEMNLP2006,pages62–69,Sydney,Australia,July.ChrisQuirk,ArulMenezes,andColinCherry.2005.De-pendencytreelettranslation:SyntacticallyinformedphrasalSMT.InProceedingsofACL2005,pages271–279,AnnArbor,Michigan,June.AndreasStolcke.2002.Srilm-anextensiblelan-guagemodelingtoolkit.InProceedingsofInterna-tionalConferenceonSpokenLanguageProcessing,volume30,pages901–904.AshishVenugopalandStephanVogel.2005.Consid-erationsinmaximummutualinformationandmini-mumclassiﬁcationerrortrainingforstatisticalma-chinetranslation.InProceedingsoftheTenthConfer-enceoftheEuropeanAssociationforMachineTrans-lation,pages271–279.DeyiXiong,ShuanglongLi,QunLiu,andShouxunLin.2005.Parsingthepennchinesetreebankwithseman-ticknowledge.InProceedingsofIJCNLP2005,pages70–81.YingZhang,StephanVogel,andAlexWaibel.2004.In-terpretingbleu/nistscoreshowmuchimprovementdoweneedtohaveabettersystem?InProceedingsofFourthInternationalConferenceonLanguageRe-sourcesandEvaluation,pages2051–2054.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 712–719,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

712

OrderingPhraseswithFunctionWordsHendraSetiawanandMin-YenKanSchoolofComputingNationalUniversityofSingaporeSingapore117543{hendrase,kanmy}@comp.nus.edu.sgHaizhouLiInstituteforInfocommResearch21HengMuiKengTerraceSingapore119613hli@i2r.a-star.edu.sgAbstractThispaperpresentsaFunctionWordcen-tered,Syntax-based(FWS)solutiontoad-dressphraseorderinginthecontextofstatisticalmachinetranslation(SMT).Mo-tivatedbytheobservationthatfunctionwordsoftenencodegrammaticalrelation-shipamongphraseswithinasentence,weproposeaprobabilisticsynchronousgram-martomodeltheorderingoffunctionwordsandtheirleftandrightarguments.Weim-provephraseorderingperformancebylexi-calizingtheresultingrulesinasmallnumberofcasescorrespondingtofunctionwords.TheexperimentsshowthattheFWSap-proachconsistentlyoutperformsthebase-linesysteminorderingfunctionwords’ar-gumentsandimprovingtranslationqualityinbothperfectandnoisywordalignmentscenarios.1IntroductionThefocusofthispaperisonfunctionwords,aclassofwordswithlittleintrinsicmeaningbutisvitalinexpressinggrammaticalrelationshipsamongwordswithinasentence.Suchencodedgrammaticalinfor-mation,oftenimplicit,makesfunctionwordspiv-otalinmodelingstructuraldivergences,asproject-ingthemindifferentlanguagesoftenresultinlong-rangestructuralchangestotherealizedsentences.Justasaforeignlanguagelearneroftenmakesmistakesinusingfunctionwords,weobservethatcurrentmachinetranslation(MT)systemsoftenper-formpoorlyinorderingfunctionwords’arguments;lexicallycorrecttranslationsoftenendupreorderedincorrectly.Thus,weareinterestedinmodelingthestructuraldivergenceencodedbysuchfunctionwords.Akeyﬁndingofourworkisthatmodelingtheorderingofthedependentargumentsoffunctionwordsresultsinbettertranslationquality.Mostcurrentsystemsusestatisticalknowledgeobtainedfromcorporainfavorofrichnaturallan-guageknowledge.Insteadofusingsyntacticknowl-edgetodeterminefunctionwords,weapproximatethisbyequatingthemostfrequentwordsasfunc-tionwords.Byexplicitlymodelingphraseorderingaroundthesefrequentwords,weaimtocapturethemostimportantandprevalentorderingproductions.2RelatedWorkAgoodtranslationshouldbebothfaithfulwithade-quatelexicalchoicetothesourcelanguageandﬂu-entinitswordorderingtothetargetlanguage.Inpursuitofbettertranslation,phrase-basedmodels(OchandNey,2004)havesigniﬁcantlyimprovedthequalityoverclassicalword-basedmodels(Brownetal.,1993).Thesemultiwordphrasalunitscontributetoﬂuencybyinherentlycapturingintra-phrasere-ordering.However,despitethisprogress,inter-phrasereordering(especiallylongdistanceones)stillposesagreatchallengetostatisticalmachinetranslation(SMT).Thebasicphrasereorderingmodelisasimpleunlexicalized,context-insensitivedistortionpenaltymodel(Koehnetal.,2003).Thismodelassumeslittleornostructuraldivergencebetweenlanguagepairs,preferringtheoriginal,translatedorderbype-nalizingreordering.Thissimplemodelworkswellwhenproperlycoupledwithawell-trainedlanguage713

model,butisotherwiseimpoverishedwithoutanylexicalevidencetocharacterizethereordering.Toaddressthis,lexicalizedcontext-sensitivemodelsincorporatecontextualevidence.Thelocalpredictionmodel(TillmannandZhang,2005)mod-elsstructuraldivergenceastherelativepositionbe-tweenthetranslationoftwoneighboringphrases.Otherfurthergeneralizationsoforientationincludetheglobalpredictionmodel(Nagataetal.,2006)anddistortionmodel(Al-OnaizanandPapineni,2006).However,thesemodelsareoftenfullylexicalizedandsensitivetoindividualphrases.Asaresult,theyarenotrobusttounseenphrases.Acarefulapprox-imationisvitaltoavoiddatasparseness.Proposalstoalleviatethisproblemincludeutilizingbilingualphraseclusterorwordsatthephraseboundary(Na-gataetal.,2006)asthephraseidentity.Thebeneﬁtofintroducinglexicalevidencewith-outbeingfullylexicalizedhasbeendemonstratedbyarecentstate-of-the-artformallysyntax-basedmodel1,Hiero(Chiang,2005).Hieroperformsphraseorderingbyusinglinkednon-terminalsym-bolsinitssynchronousCFGproductionrulescou-pledwithlexicalevidence.However,sinceitisdif-ﬁculttospecifyawell-deﬁnedrule,Hierohastorelyonweakheuristics(i.e.,length-basedthresholds)toextractrules.Asaresult,Hieroproducesgrammarsofenormoussize.Watanabeetal.(2006)furtherreducesthegrammar’ssizebyenforcingallrulestocomplywithGreibachNormalForm.Takingthelexicalizationanintuitiveastepfor-ward,weproposeanovel,ﬁner-grainedsolutionwhichmodelsthecontentandcontextinformationencodedbyfunctionwords-approximatedbyhighfrequencywords.Inspiredbythesuccessofsyntax-basedapproaches,weproposeasynchronousgram-marthataccommodatesgappingproductionrules,whilefocusingonthestatisticalmodelinginrela-tiontofunctionwords.WerefertoourapproachastheFunctionWord-centeredSyntax-basedap-proach(FWS).OurFWSapproachisdifferentfromHierointwokeyaspects.First,weuseonlyasmallsetofhighfrequencylexicalitemstolexi-calizenon-terminalsinthegrammar.ThisresultsinamuchsmallersetofrulescomparedtoHiero,1Chiang(2005)usedtheterm“formal”toindicatetheuseofsynchronousgrammarbutwithoutlinguisticcommitment៨ת೤ᒧᲃκሇಚ௳᧢ԗߨሇᰴىaformisacoll.ofdataentryﬁeldsonapage((((((((((((((((PPPPPPPPP``````````````Figure1:AChinese-Englishsentencepair.greatlyreducingthecomputationaloverheadthatariseswhenmovingfromphrase-basedtosyntax-basedapproach.Furthermore,bymodelingonlyhighfrequencywords,weareabletoobtainreliablestatisticseveninsmalldatasets.Second,asopposedtoHiero,wherephraseorderingisdoneimplicitlyalongsidephrasetranslationandlexicalweighting,wedirectlymodelthereorderingprocessusingori-entationstatistics.TheFWSapproachisalsoakinto(Xiongetal.,2006)inusingasynchronousgrammarasareorder-ingconstraint.InsteadofusingInversionTransduc-tionGrammar(ITG)(Wu,1997)directly,wewilldiscussanITGextensiontoaccommodategapping.3PhraseOrderingaroundFunctionWordsWeusethefollowingChinese(c)toEnglish(e)translationinFig.1asanillustrationtoconductaninquirytotheproblem.Notethatthesentencetrans-lationrequiressometranslationsofEnglishwordstobeorderedfarfromtheiroriginalpositioninChi-nese.RecoveringthecorrectEnglishorderingre-quirestheinversionoftheChinesepostpositionalphrase,followedbytheinversionoftheﬁrstsmallernounphrase,andﬁnallytheinversionofthesec-ondlargernounphrase.Nevertheless,thecorrectorderingcanberecoveredifthepositionandthese-manticrolesoftheargumentsoftheboxedfunctionwordswereknown.Suchafunctionwordcenteredapproachalsohingesonknowingthecorrectphraseboundariesforthefunctionwords’argumentsandwhichreorderingsaregivenprecedence,incaseofconﬂicts.Weproposemodelingthesesourcesofknowl-edgeusingastatisticalformalism.Itincludes1)amodeltocapturebilingualorientationsoftheleftandrightargumentsofthesefunctionwords;2)amodeltoapproximatecorrectreorderingsequence;and3)amodelforﬁndingconstituentboundariesof714

theleftandrightarguments.Assumingthatthemostfrequentwordsinalanguagearefunctionwords,wecanapplyorientationstatisticsassociatedwiththesewordstoreordertheiradjacentleftandrightneighbors.Wefollowthenotationin(Nagataetal.,2006)anddeﬁnethefollowingbilingualori-entationvaluesgiventwoneighboringsource(Chi-nese)phrases:Monotone-Adjacent(MA);Reverse-Adjacent(RA);Monotone-Gap(MG);andReverse-Gap(RG).Theﬁrstclause(monotone,reverse)in-dicateswhetherthetargetlanguagetranslationorderfollowsthesourceorder;thesecond(adjacent,gap)indicateswhetherthesourcephrasesareadjacentorseparatedbyaninterveningphraseonthetargetside.Table1showstheorientationstatisticsforseveralfunctionwords.Notethatweseparatethestatisticsforleftandrightargumentstoaccountfordiffer-encesinargumentstructures:somefunctionwordstakeasingleargument(e.g.,prepositions),whileotherstaketwoormore(e.g.,copulas).Tohan-dleotherreorderingdecisionsnotexplicitlyencoded(i.e.,lexicalized)inourFWSmodel,weintroduceauniversaltokenU,tobeusedasabackoffstatisticwhenfunctionwordsareabsent.Forexample,orientationstatisticsfor೤(tobe)overwhelminglysuggeststhattheEnglishtransla-tionofitssurroundingphrasesisidenticaltoitsChi-neseordering.Thisreﬂectsthefactthattheargu-mentsofcopulasinbothlanguagesarerealizedinthesameorder.Theorientationstatisticsforpost-positionκ(on)suggestsinversionwhichcapturesthedivergencebetweenChinesepostpositiontotheEnglishpreposition.Similarly,thedominantorien-tationforparticleሇ(of)suggeststhenoun-phraseshiftfrommodiﬁed-modiﬁertomodiﬁer-modiﬁed,whichiscommonwhentranslatingChinesenounphrasestoEnglish.Takingallpartsofthemodel,whichwedetaillater,togetherwiththeknowledgeinTable1,wedemonstratethestepstakentotranslatetheexam-pleinFig.2.Wehighlightthefunctionwordswithboxedcharactersandencapsulatecontentwordsasindexedsymbols.Asshown,orientationstatisticsfromfunctionwordsaloneareadequatetorecovertheEnglishordering-inpractice,contentwordsalsoinﬂuencethereorderingthroughalanguagemodel.OnecanthinkoftheFWSapproachasaforeignlan-guagelearnerwithlimitedknowledgeaboutChinesegrammarbutfairlyknowledgableabouttheroleofChinesefunctionwords.៨ת೤ᒧᲃκሇಚ௳᧢ԗߨሇᰴىX1೤X2κሇX3ሇX4HHjκX2?9XXXXXzX3ሇX5?)XXXXXXXzX4ሇX6???X1೤X7X1೤X4ሇX3ሇκX2៨ת೤ᰴىሇಚ௳᧢ԗߨሇκᒧᲃaformisacoll.ofdataentryﬁeldsonapage#1#2#3?????????Figure2:InStep1,functionwords(boxedchar-acters)andcontentwords(indexedsymbols)areidentiﬁed.Step2reordersphrasesaccordingtoknowledgeembeddedinfunctionwords.Anewin-dexedsymbolisintroducedtoindicatepreviouslyreorderedphrasesforconciseness.Step3ﬁnallymapsChinesephrasestotheirEnglishtranslation.4TheFWSModelWeﬁrstdiscusstheextensionofstandardITGtoaccommodategappingandthendetailthestatisticalcomponentsofthemodellater.4.1SingleGapITG(SG-ITG)TheFWSmodelemploysasynchronousgrammartodescribetheadmissibleorderings.TheutilityofITGasareorderingconstraintformostlanguagepairs,iswell-knownbothempirically(ZensandNey,2003)andanalytically(Wu,1997),howeverITG’sstraight(monotone)andinverted(re-verse)rulesexhibitstrongcohesiveness,whichisin-adequatetoexpressorientationsthatrequiregaps.WeproposeSG-ITGthatfollowsWellingtonetal.(2006)’ssuggestiontomodelatmostonegap.WeshowtherulesforSG-ITGbelow.Rules1-3areidenticaltothosedeﬁnedinstandardITG,inwhichmonotoneandreverseorderingsarerepre-sentedbysquareandanglebrackets,respectively.715

RankWordunigramMALRALMGLRGLMARRARMGRRGR1ሇ0.05800.450.520.010.020.440.520.010.032ƥ0.05070.850.120.020.010.840.120.020.023ˊ0.05500.990.010.000.000.920.080.000.004ǽ0.01550.870.100.020.000.820.120.050.025Ǽ0.01530.840.110.010.040.880.110.010.016ڔ0.01380.950.020.010.010.970.020.010.007ч֩0.01230.730.120.100.040.510.140.140.208ظм0.01140.780.120.030.070.860.050.080.019ୈ0.00990.950.020.020.010.960.010.020.0110य0.00910.870.100.010.020.880.100.010.0021೤0.00560.850.110.020.020.850.040.090.0237κ0.00350.330.650.020.010.310.630.030.03-U0.00020.760.140.060.050.740.130.070.06Table1:OrientationstatisticsandunigramprobabilityofselectedfrequentChinesewordsintheHITcorpus.SubscriptsL/Rreferstolexicalunit’sorientationwithrespecttoitsleft/rightneighbor.Uistheuniversaltokenusedinback-offforN=128.Dominantorientationsofeachwordareinbold.(1)X→c/e(2)X→[XX](3)X→hXXi(4)X(cid:5)→[X(cid:5)X](5)X(cid:5)→hX(cid:5)Xi(6)X→[X∗X](7)X→hX∗XiSG-ITGintroducestwonewsetsofrules:gap-ping(Rules4-5)anddovetailing(Rules6-7)thatdealspeciﬁcallywithgaps.OntheRHSofthegap-pingrules,adiamondsymbol((cid:5))indicatesagap,whileontheLHS,itemitsasuperscriptedsymbolX(cid:5)toindicateagappedphrase(plainXswithoutsuperscriptsarethuscontiguousphrases).GapsinX(cid:5)areeventuallyﬁlledbyactualphrasesviadove-tailing(markedwithan∗ontheRHS).Fig.3illustratesgappinganddovetailingrulesusinganexamplewheretwoChineseadjectivalphrasesaretranslatedintoasingleEnglishsubordi-nateclause.SG-ITGcangeneratethecorrectorder-ingbyemployinggappingfollowedbydovetailing,asshowninthefollowingsimpliﬁedtrace:X(cid:5)1→h1997ሇᎁδႝ,V.1(cid:5)1997iX(cid:5)2→h1998ሇᎁЁႝ,V.2(cid:5)1998iX3→[X1∗X2]→[1997ሇᎁδႝڔ1998ሇᎁЁႝ,V.1(cid:5)1997∗V.2(cid:5)1998]→1997ሇᎁδႝڔ1998ሇᎁЁႝ,V.1andV.2thatwerereleasedin1997and1998whereX(cid:5)1andX(cid:5)2eachgeneratethetranslationoftheirrespectiveChinesenounphraseusinggappingandX3generatestheEnglishsubclausebydovetail-ingthetwogappedphrasestogether.Thusfar,thegrammarisunlexicalized,anddoes1997৯ؤ៞ሇᎁδႝڔ1998৯ؤ៞ሇᎁЁႝV.1andV.2thatwerereleasedin1997and1998.!!!!!!(((((((((((((hhhhhhhhhhhhhPPPPPPPFigure3:Anexampleofanalignmentthatcanbegeneratedonlybyallowinggaps.notincorporateanylexicalevidence.Nowwemod-ifythegrammartointroducelexicalizedfunctionwordstoSG-ITG.Inpractice,weintroduceanewsetoflexicalizednon-terminalsymbolsYi,i∈{1...N},torepresentthetopNmost-frequentwordsinthevocabulary;theexistingunlexicalizedXisnowreservedforcontentwords.Thisdifferencedoesnotinherentlyaffectthestructureofthegram-mar,butratherlexicalizesthestatisticalmodel.Inthisway,althoughdifferentYisfollowthesameproductionrules,theyareassociatedwithdifferentstatistics.ThisisreﬂectedinRules8-9.Rule8emitsthefunctionword;Rule9reorderstheargumentsaroundthefunctionword,resemblingourorienta-tionmodel(seeSection4.2)whereafunctionwordinﬂuencestheorientationofitsleftandrightargu-ments.Forclarity,weomitnotationthatdenoteswhichruleshavebeenapplied(monotone,reverse;gapping,dovetailing).(8)Yi→c/e(9)X→XYiXInpractice,wereplaceRule9withitsequivalent2-normalformsetofrules(Rules10-13).Finally,weintroducerulestohandleback-off(Rules14-16)andupgrade(Rule17).TheseallowSG-ITGtore-716

vertfunctionwordstonormalwordsandviceversa.(10)R→YiX(11)L→XYi(12)X→LX(13)X→XR(14)Yi→X(15)R→X(16)L→X(17)X→YUBack-offrulesareneededwhenthegrammarhastoreordertwoadjacentfunctionwords,whereonesetoforientationstatisticsmusttakeprecedenceovertheother.TheexampleinFig.1illustratessuchacasewheretheorientationofκ(on)andሇ(of)competeforinﬂuence.Inthiscase,thegrammarchoosestouseሇ(of)andrevertsthefunctionwordκ(on)totheunlexicalizedform.Theupgraderuleisusedforcaseswheretherearetwoadjacentphrases,bothofwhicharenotfunctionwords.Upgradingallowseitherphrasetoactasafunctionword,makinguseoftheuniversalword’sorientationstatisticstoreorderitsneighbor.4.2StatisticalmodelWenowformulatetheFWSmodelasastatisticalframework.WereplacethedeterministicrulesinourSG-ITGgrammarwithprobabilisticones,elevatingittoastochasticgrammar.Inparticular,wedevelopthethreesubmodels(seeSection3)whichinﬂuencethechoiceofproductionrulesfororderingdecision.Thesemodelsoperateonthe2-normrules,wheretheRHScontainsonefunctionwordanditsargument(exceptinthecaseofthephraseboundarymodel).Weprovidetheintuitionforthesemodelsnext,buttheiractualformwillbediscussedinthenextsectionontraining.1)OrientationModelori(o|H,Yi):ThismodelcapturesthepreferenceofafunctionwordYitoaparticularorientationo∈{MA,RA,MG,RG}inreorderingitsH∈{left,right}argumentX.TheparameterHdetermineswhichsetofYi’sstatisticstouse(leftorright);themodelconsultsYi’sleftori-entationstatisticforRules11and13whereXpre-cedesYi,otherwiseYi’srightorientationstatisticisusedforRules10and12.2)PreferenceModelpref(Yi):Thismodelar-bitratesreorderinginthecaseswheretwofunctionwordsareadjacentandthebackoffruleshavetode-cidewhichfunctionwordtakesprecedence,revert-ingtheothertotheunlexicalizedXform.Thismodelprefersthefunctionwordwithhigheruni-gramprobabilitytotaketheprecedence.3)PhraseBoundaryModelpb(X):Thismodelisapenalty-basedmodel,favoringtheresultingalign-mentthatconformstothesourceconstituentbound-ary.ItpenalizesRule1iftheterminalruleXemitsaChinesephrasethatviolatestheboundary(pb=e−1),otherwiseitisinactive(pb=1).ThesethreesubmodelsactasfeaturesalongsidesevenotherstandardSMTfeaturesinalog-linearmodel,resultinginthefollowingsetoffeatures{f1,...,f10}:f1)orientationori(o|H,Yi);f2)preferencepref(Yi);f3)phraseboundarypb(X);f4)languagemodellm(e);f5−f6)phrasetrans-lationscoreφ(e|c)anditsinverseφ(c|e);f7−f8)lexicalweightlex(e|c)anditsinverselex(c|e);f9)wordpenaltywp;andf10)phrasepenaltypp.ThetranslationisthenobtainedfromthemostprobablederivationofthestochasticSG-ITG.TheformulaforasinglederivationisshowninEq.(18),whereX1,X2,...,XLisasequenceofruleswithw(Xl)beingtheweightofeachparticularruleXl.w(Xl)isestimatedthroughalog-linearmodel,asinEq.(19),withalltheabovementionedfeatureswhereλjreﬂectsthecontributionofeachfeaturefj.P(X1,...,XL)=YLl=1w(Xl)(18)w(Xl)=Y10j=1fj(Xl)λj(19)5TrainingWetraintheorientationandpreferencemodelsfromstatisticsofatrainingcorpus.Tothisend,weﬁrstderivetheeventcountsandthencomputetherela-tivefrequencyofeachevent.Theremainingphraseboundarymodelcanbemodeledbytheoutputofastandardtextchunker,asinpracticeitissimplyaconstituentboundarydetectionmechanismtogetherwithapenaltyscheme.Theeventsofinteresttotheorientationmodelare(Yi,o)tuples,whereo∈{MA,RA,MG,RG}isanorientationvalueofaparticularfunctionwordYi.Notethatthesetuplesarenotdirectlyobservablefromtrainingdata.Hence,weneedanalgorithmtoderive(Yi,o)tuplesfromaparallelcorpus.Sincebothleftandrightstatisticsshareidenticaltrainingsteps,thusweomitreferencestothem.Thealgorithmtoderive(Yi,o)involvesseveralsteps.First,weestimatethebi-directionalalignment717

byrunningGIZA++andapplyingthe“grow-diag-ﬁnal”heuristic.Then,thealgorithmenumeratesallYianddeterminesitsorientationowithrespecttoitsargumentXtoderive(Yi,o).Todetermineo,thealgorithminspectsthemonotonicity(monotoneorreverse)andadjacency(adjacentorgap)betweenYi’sandX’salignments.MonotonicitycanbedeterminedbylookingattheYi’salignmentwithrespecttothemostﬁne-grainedlevelofX(i.e.,wordlevelalignment).However,suchaheuristicmayinaccuratelysuggestgapori-entation.Figure1illustratesthisproblemwhende-rivingtheorientationforthesecondሇ(of).Look-ingonlyatthewordalignmentofitsleftargumentߨ(ﬁelds)incorrectlysuggestsagappedorientation,wherethealignmentofಚ௳᧢ԗ(dataentry)in-tervened.Itisdesirabletolookatthealignmentofಚ௳᧢ԗߨ(dataentryﬁelds)atthephraselevel,whichsuggeststhecorrectadjacentorientationin-stead.Toaddressthisissue,thealgorithmusesgap-pingconservativelybyutilizingtheconsistencycon-straint(OchandNey,2004)tosuggestphraselevelalignmentofX.Thealgorithmexhaustivelygrowsconsistentblockscontainingthemostﬁne-grainedlevelofXnotincludingYi.Subsequently,itmergeseachhypotheticalargumentwiththeYi’salignment.ThealgorithmdecidesthatYihasagappedorienta-tiononlyifallmergedblocksviolatetheconsistencyconstraint,concludinganadjacentorientationother-wise.WiththeeventcountsC(Yi,o)oftuple(Yi,o),weestimatetheorientationmodelforYiandUusingEqs.(20)and(21).Wealsoestimatetheprefer-encemodelwithwordunigramcountsC(Yi)usingEqs.(22)and(23),whereVindicatesthevocabu-larysize.ori(o|Yi)=C(Yi,o)/C(Yi,·),i6N(20)ori(o|U)=Xi>NC(Yi,o)/Xi>NC(Yi,·)(21)pref(Yi)=C(Yi)/C(·),i6N(22)pref(U)=1/(V−N)Xi>NC(Yi)/C(·)(23)SamplesofthesestatisticsarefoundinTable1andhavebeenusedintherunningexamples.Forinstance,thestatisticori(RAL|ሇ)=0.52,whichisthedominantone,suggeststhatthegrammarin-verselyorderሇ(of)’sleftargument;whileinourillustrationofbackoffrulesinFig.1,thegrammarchoosesሇ(of)totakeprecedencesincepref(ሇ)>pref(κ).6DecodingWeemployabottom-upCKYparserwithabeamtoﬁndthederivationofaChinesesentencewhichmaximizesEq.(18).TheEnglishtranslationisthenobtainedbypost-processingthebestparse.Wesetthebeamsizeto30inourexperimentandfurtherconstrainreorderingtooccurwithinawin-dowof10words.Ourdecoderalsoprunesentriesthatviolatethefollowingconstraints:1)eachentrycontainsatmostonegap;2)anygappedentriesmustbedovetailedatthenextlevelhigher;3)anentryspanningthewholesentencemustnotcontaingaps.Thescoreofeachnewly-createdentryisderivedfromthescoresofitspartsaccordingly.Whenscor-ingentries,wetreatgappedentriesascontiguousphrasesbyignoringthegapsymbolandrelyontheorientationmodeltopenalizesuchentries.Thisal-lowsafairscorecomparisonbetweengappedandcontiguousentries.7ExperimentsWewouldliketostudyhowtheFWSmodelaffects1)theorderingofphrasesaroundfunctionwords;2)theoveralltranslationquality.WeachievethisbyevaluatingtheFWSmodelagainstabaselinesystemusingtwometrics,namely,orientationaccuracyandBLEUrespectively.Wedeﬁnetheorientationaccuracyofa(function)wordastheaccuracyofassigningcorrectorientationvaluestobothitsleftandrightarguments.Wereporttheaggregateforthetop1024mostfrequentwords;thesewordscover90%ofthetestset.Wedeviseaseriesofexperimentsandrunitintwoscenarios-manualandautomaticalignment-toas-sesstheeffectsofusingperfectorreal-worldinput.WeutilizetheHITbilingualcomputermanualcor-pus,whichhasbeenmanuallyaligned,toperformChinese-to-Englishtranslation(seeTable2).Man-ualalignmentisessentialasweneedtomeasureori-entationaccuracywithrespecttoagoldstandard.718

ChineseEnglishtrainwords145,731135,032(7Ksentences)vocabulary5,2678,064devwords13,98614,638(1Ksentences)untranslatable486(3.47%)testwords27,73228,490(2Ksentences)untranslatable935(3.37%)Table2:StatisticsfortheHITcorpus.AlanguagemodelistrainedusingtheSRILM-Toolkit,andatextchunker(Chenetal.,2006)isap-pliedtotheChinesesentencesinthetestanddevsetstoextracttheconstituentboundariesnecessaryforthephraseboundarymodel.Werunminimumer-rorratetrainingondevsetusingChiang’stoolkittoﬁndasetofparametersthatoptimizesBLEUscore.7.1PerfectLexicalChoiceHere,thetaskissimpliﬁedtorecoveringthecorrectorderoftheEnglishsentencefromthescrambledChineseorder.Wetrainedtheorientationmodelus-ingmanualalignmentasinput.Theaforementioneddecoderisusedwithphrasetranslation,lexicalmap-pingandpenaltyfeaturesturnedoff.Table4comparesorientationaccuracyandBLEUbetweenourFWSmodelandthebaseline.Thebaseline(lm+d)employsalanguagemodelanddistortionpenaltyfeatures,emulatingthestandardPharaohmodel.WestudythebehavioroftheFWSmodelwithdifferentnumbersoflexicalizeditemsN.Westartwiththelanguagemodelalone(N=0)andincrementallyaddtheorientation(+ori),preference(+ori+pref)andphraseboundarymodels(+ori+pref+pb).Asshown,thelanguagemodelaloneisrela-tivelyweak,assigningthecorrectorientationinonly62.28%ofthecases.Acloserinspectionrevealsthatthelmcomponentaggressivelypromotesreversere-orderings.Includingadistortionpenaltymodel(thebaseline)improvestheaccuracyto72.55%.ThistrendisalsoapparentfortheBLEUscore.WhenweincorporatetheFSWmodel,includingjustthemostfrequentword(Y1=ሇ),weseeim-provement.Thismodelpromotesnon-monotonere-orderingconservativelyaroundY1(wherethedom-inantstatisticsuggestsreverseordering).IncreasingthevalueofNleadstogreaterimprovement.Themosteffectiveimprovementisobtainedbyincreas-pharaoh(dl=5)22.44±0.94+ori23.80±0.98+ori+pref23.85±1.00+ori+pref+pb23.86±1.08Table3:BLEUscorewiththe95%conﬁdencein-tervalsbasedon(ZhangandVogel,2004).Allim-provementoverthebaseline(row1)arestatisticallysigniﬁcantunderpairedbootstrapresampling.ingNto128.Additional(marginal)improvementisobtainedattheexpenseofmodelinganadditional900+lexicalitems.Weseetheseresultsasvalidat-ingourclaimthatmodelingthetopfewmostfre-quentwordscapturesmostimportantandprevalentorderingproductions.Lastly,westudytheeffectoftheprefandpbfea-tures.Theinclusionofbothsubmodelshaslittleaf-fectonorientationaccuracy,butitimprovesBLEUconsistently(althoughnotsigniﬁcantly).Thissug-geststhatbothmodelscorrectthemistakesmadebytheorimodelwhilepreservingthegain.Theyarenotaseffectiveastheadditionofthebasicorienta-tionmodelastheyonlyplayarolewhentwolexi-calizedentriesareadjacent.7.2FullSMTexperimentsHere,allknowledgeisautomaticallytrainedonthetrainset,andasaresult,theinputwordalignmentisnoisy.Asabaseline,weusethestate-of-the-artphrase-basedPharaohdecoder.Forafaircompari-son,werunminimumerrorratetrainingfordifferentdistortionlimitsfrom0to10andreportthebestpa-rameter(dl=5)asthebaseline.Weusethephrasetranslationtablefromthebase-lineandperformanidenticalsetofexperimentsastheperfectlexicalchoicescenario,exceptthatweonlyreporttheresultforN=128,duetospacecon-straint.Table3reportstheresultingBLEUscores.Asshown,theFWSmodelimprovesBLEUscoresigniﬁcantlyoverthebaseline.Weobservethesametrendastheoneinperfectlexicalchoicescenariowheretop128mostfrequentwordsprovidesthema-jorityofimprovement.However,thepbfeaturesyieldsnonoticeableimprovementunlikeinprefectlexicalchoicescenario;thisissimilartotheﬁndingsin(Koehnetal.,2003).719

N=0N=1N=4N=16N=64N=128N=256N=1024OrientationAcc.(%)lm+d72.55+ori62.2876.5276.5877.3877.5478.1777.7678.38+ori+pref76.6676.8277.5777.7478.1377.9478.54+ori+pref+pb76.7076.8577.5877.7078.2077.9478.56BLEUlm+d75.13+ori66.5477.5477.5778.2278.4878.7678.5879.20+ori+pref77.6077.7078.2978.6578.7778.7079.30+ori+pref+pb77.6977.8078.3478.6578.9378.7979.30Table4:Resultsusingperfectalignedinput.Here,(lm+d)isthebaseline;(+ori),(+ori+pref)and(+ori+pref+pb)aredifferentFWSconﬁgurations.Theresultsofthemodel(whereNisvaried)thatfea-turesthelargestgainarebold,whereasthehighestscoreisitalicized.8ConclusionInthispaper,wepresentastatisticalmodeltocap-turethegrammaticalinformationencodedinfunc-tionwords.Formally,wedeveloptheFunctionWordSyntax-based(FWS)model,aprobabilisticsyn-chronousgrammar,toencodetheorientationstatis-ticsofargumentstofunctionwords.Ourexperimen-talresultsshowsthattheFWSmodelsigniﬁcantlyimprovesthestate-of-the-artphrase-basedmodel.Wehavetouchedonlythesurfacebeneﬁtsofmod-elingfunctionwords.Inparticular,ourproposalislimitedtomodelingfunctionwordsinthesourcelanguage.Webelievethatconditioningonbothsourceandtargetpairwouldresultinmoreﬁne-grained,accurateorientationstatistics.Fromourerroranalysis,weobservethat1)re-orderingmayspanseverallevelsandthepreferencemodeldoesnothandlethisphenomenawell;2)cor-rectlyreorderedphraseswithincorrectboundariesseverelyaffectsBLEUscoreandthephrasebound-arymodelisinadequatetocorrecttheboundarieses-peciallyforcasesoflongphrase.Infuture,wehopetoaddresstheseissueswhilemaintainingthebene-ﬁtsofferedbymodelingfunctionwords.ReferencesBenjaminWellington,SonjiaWaxmonsky,andI.DanMelamed.2006.EmpiricalLowerBoundsontheComplexityofTranslationalEquivalence.InACL/COLING2006,pp.977–984.ChristophTillmanandTongZhang.2005.ALocalizedPredictionModelforStatisticalMachineTranslation.InACL2005,pp.557–564.DavidChiang.2005.AHierarchicalPhrase-BasedModelforStatisticalMachineTranslation.InACL2005,pp.263–270.DekaiWu.1997.StochasticInversionTransductionGrammarsandBilingualParsingofParallelCorpora.ComputationalLinguistics,23(3):377–403.DeyiXiong,QunLiu,andShouxunLin.2006.Maxi-mumEntropyBasedPhraseReorderingModelforSta-tisticalMachineTranslation.InACL/COLING2006,pp.521–528.FranzJ.OchandHermannNey.2004.TheAlignmentTemplateApproachtoStatisticalMachineTranslation.ComputationalLinguistics,30(4):417–449.MasaakiNagata,KunikoSaito,KazuhideYamamoto,andKazuteruOhashi.2006.AClusteredGlobalPhraseReorderingModelforStatisticalMachineTranslation.InACL/COLING2006,pp.713–720.PeterF.Brown,StephenA.DellaPietra,VincentJ.DellaPietra,RobertL.Mercer1993.TheMathematicsofStatisticalMachineTranslation:ParameterEstima-tion.ComputationalLinguistics,19(2):263–311.PhilippKoehn,FranzJ.Och,andDanielMarcu.2003.StatisticalPhrase-BasedTranslation.InHLT-NAACL2003,pp.127–133.RichardZensandHermannNey.2003.ACompara-tiveStudyonReorderingConstraintsinStatisticalMa-chineTranslation.InACL2003.TaroWatanabe,HajimeTsukada,andHidekiIsozaki.2006.Left-to-RightTargetGenerationforHierarchi-calPhrase-BasedTranslation.InACL/COLING2006,pp.777–784.WenliangChen,YujieZhangandHitoshiIsahara2006.AnEmpiricalStudyofChineseChunkingInACL2006PosterSessions,pp.97–104.YaserAl-OnaizanandKishorePapineni.2006.Distor-tionModelsforStatisticalMachineTranslation.InACL/COLING2006,pp.529–536.YingZhangandStephanVogel.2004.MeasuringConﬁ-denceIntervalsfortheMachineTranslationEvaluationMetrics.InTMI2004.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 720–727,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

720

AProbabilisticApproachtoSyntax-basedReorderingforStatisticalMachineTranslationChi-HoLi,DongdongZhang,MuLi,MingZhouMicrosoftResearchAsiaBeijing,Chinachl,dozhang@microsoft.commuli,mingzhou@microsoft.comMinghuiLi,YiGuanHarbinInstituteofTechnologyHarbin,Chinamhli@insun.hit.edu.cnguanyi@insun.hit.edu.cnAbstractInspiredbypreviouspreprocessingap-proachestoSMT,thispaperproposesanovel,probabilisticapproachtoreorderingwhichcombinesthemeritsofsyntaxandphrase-basedSMT.Givenasourcesentenceanditsparsetree,ourmethodgenerates,bytreeoperations,ann-bestlistofre-orderedinputs,whicharethenfedtostan-dardphrase-baseddecodertoproducetheoptimaltranslation.Experimentsshowthat,fortheNISTMT-05taskofChinese-to-Englishtranslation,theproposalleadstoBLEUimprovementof1.56%.1IntroductionThephrase-basedapproachhasbeenconsideredthedefaultstrategytoStatisticalMachineTranslation(SMT)inrecentyears.Itiswidelyknownthatthephrase-basedapproachispowerfulinlocallexicalchoiceandwordreorderingwithinshortdistance.However,long-distancereorderingisproblematicinphrase-basedSMT.Forexample,thedistance-basedreorderingmodel(Koehnetal.,2003)al-lowsadecodertotranslateinnon-monotonousor-der,undertheconstraintthatthedistancebetweentwophrasestranslatedconsecutivelydoesnotex-ceedalimitknownasdistortionlimit.Intheorythedistortionlimitcanbeassignedaverylargevaluesothatallpossiblereorderingsareallowed,yetinpractiseitisobservedthattoohighadistortionlimitnotonlyharmsefﬁciencybutalsotranslationper-formance(Koehnetal.,2005).Inourownexper-imentsetting,thebestdistortionlimitforChinese-Englishtranslationis4.However,someidealtrans-lationsexhibitreorderingslongerthansuchdistor-tionlimit.ConsiderthesentencepairinNISTMT-2005testsetshowninﬁgure1(a):aftertranslatingtheword“һ៧/mend”,thedecodershould‘jump’acrosssixwordsandtranslatethelastphrase“ԟᐄ᠅ᒈ/ﬁssuresintherelationship”.Therefore,whileshort-distancereorderingisunderthescopeofthedistance-basedmodel,long-distancereorder-ingissimplyoutofthequestion.Aterminologicalremark:Intherestofthepaper,wewillusethetermsglobalreorderingandlocalreorderinginplaceoflong-distancereorderingandshort-distancereorderingrespectively.Thedistinc-tionbetweenlongandshortdistancereorderingissolelydeﬁnedbydistortionlimit.Syntax1iscertainlyapotentialsolutiontoglobalreordering.Forexample,forthelasttwoChinesephrasesinﬁgure1(a),simplyswappingthetwochil-drenoftheNPnodewillproducethecorrectwordorderontheEnglishside.However,therearealsoreorderingswhichdonotagreewithsyntacticanal-ysis.Figure1(b)showshowourphrase-basedde-coder2obtainsagoodEnglishtranslationbyreorder-ingtwoblocks.ItshouldbenotedthatthesecondChineseblock“ᑄ൉೎”anditsEnglishcounterpart“attheendof”arenotconstituentsatall.Inthispaper,ourinterestisthevalueofsyntaxinreordering,andthemajorstatementisthatsyntacticinformationisusefulinhandlingglobalreordering1Herebysyntaxitismeantlinguisticsyntaxratherthanfor-malsyntax.2Thedecoderisintroducedinsection6.721

Figure1:Examplesonhowsyntax(a)helpsand(b)harmsreorderinginChinese-to-EnglishtranslationThelinesandnodesonthetophalfoftheﬁguresshowthephrasestructureoftheChinesesentences,whilethelinksonthebottomhalfoftheﬁguresshowthealignmentsbetweenChineseandEnglishphrases.Squarebracketsindicatetheboundariesofblocksfoundbyourdecoder.anditachievesbetterMTperformanceontheba-sisofthestandardphrase-basedmodel.Toproveit,wedevelopedahybridapproachwhichpreservesthestrengthofphrase-basedSMTinlocalreorderingaswellasthestrengthofsyntaxinglobalreordering.Ourmethodisinspiredbypreviouspreprocessingapproacheslike(XiaandMcCord,2004),(Collinsetal.,2005),and(Costa-juss`aandFonollosa,2006),whichsplittranslationintotwostages:S→S0→T(1)whereasentenceofthesourcelanguage(SL),S,isﬁrstreorderedwithrespecttothewordorderofthetargetlanguage(TL),andthenthereorderedSLsentenceS0istranslatedasaTLsentenceTbymonotonoustranslation.Ourﬁrstcontributionisanewtranslationmodelasrepresentedbyformula2:S→n×S0→n×T→ˆT(2)whereann-bestlistofS0,insteadofonlyoneS0,isgenerated.Thereasonofsuchchangewillbegiveninsection2.NotealsothatthetranslationprocessS0→Tisnotmonotonous,sincethedistance-basedmodelisneededforlocalreordering.Oursecondcontributionisourdeﬁnitionofthebesttranslation:argmaxTexp(λrlogPr(S→S0)+XiλiFi(S0→T))whereFiarethefeaturesinthestandardphrase-basedmodelandPr(S→S0)isournewfeature,viz.theprobabilityofreorderingSasS0.Thede-tailsofthismodelareelaboratedinsections3to6.Thesettingsandresultsofexperimentsonthisnewmodelaregiveninsection7.2RelatedWorkTherehavebeenvariousattemptstosyntax-basedSMT,suchas(YamadaandKnight,2001)and(Quirketal.,2005).Wedonotadoptthesemodelssincealotofsubtleissueswouldthenbein-troducedduetothecomplexityofsyntax-basedde-coder,andtheimpactofsyntaxonreorderingwillbedifﬁculttosingleout.Therehavebeenmanyreorderingstrategiesun-derthephrase-basedcamp.Anotableapproachislexicalizedreordering(Koehnetal.,2005)and(Till-mann,2004).Itshouldbenotedthatthisapproachachievesthebestresultwithincertaindistortionlimitandisthereforenotagoodmodelforglobalreorder-ing.Thereareafewattemptstothepreprocessingapproachtoreordering.Themostnotableonesare(XiaandMcCord,2004)and(Collinsetal.,2005),bothofwhichmakeuseoflinguisticsyntaxinthepreprocessingstage.(Collinsetal.,2005)an-alyzeGermanclausestructureandproposesixtypes722

ofrulesfortransformingGermanparsetreeswithrespecttoEnglishwordorder.Insteadofrelyingonmanualrules,(XiaandMcCord,2004)proposeamethodinlearningpatternsofrewritingSLsen-tences.ThismethodparsestrainingdataandusessomeheuristicstoalignSLphraseswithTLones.Fromsuchalignmentitcanextractrewritingpat-terns,ofwhichtheunitsarewordsandPOSs.ThelearnedrewritingrulesarethenappliedtorewriteSLsentencesbeforemonotonoustranslation.Despitetheencouragingresultsreportedinthesepapers,thetwoattemptssharethesameshortcomingthattheirreorderingisdeterministic.Aspointedoutin(Al-OnaizanandPapineni,2006),thesestrategiesmakeharddecisionsinreorderingwhichcannotbeundoneduringdecoding.Thatis,thechoiceofre-orderingisindependentfromothertranslationfac-tors,andonceareorderingmistakeismade,itcan-notbecorrectedbythesubsequentdecoding.Toovercomethisweakness,wesuggestamethodto‘soften’theharddecisionsinpreprocessing.Theessenceisthatourpreprocessingmodulegeneratesn-bestS0sratherthanmerelyoneS0.AvarietyofreorderedSLsentencesarefedtothedecodersothatthedecodercanconsider,tocertainextent,theinteractionbetweenreorderingandotherfactorsoftranslation.Theentireprocesscanbedepictedbyformula2,recapitulatedasfollows:S→n×S0→n×T→ˆT.Apartfromtheirdeterministicnature,thetwopreviouspreprocessingapproacheshavetheirownweaknesses.(Collinsetal.,2005)countonman-ualrulesanditissuspiciousifreorderingrulesforotherlanguagepairscanbeeasilymade.(XiaandMcCord,2004)proposeawaytolearnrewritingpatterns,neverthelesstheunitsofsuchpatternsarewordsandtheirPOSs.Althoughthereisnolimittothelengthofrewritingpatterns,duetodatasparse-nessmostpatternsbeingappliedwouldbeshortones.Manyinstancesofglobalreorderingarethere-foreleftunhandled.3TheAcquisitionofReorderingKnowledgeToavoidthisproblem,wegiveupusingrewritingpatternsanddesignaformofreorderingknowledgewhichcanbedirectlyappliedtoparsetreenodes.GivenanodeNontheparsetreeofanSLsentence,therequiredreorderingknowledgeshouldenablethepreprocessingmoduletodeterminehowprobablethechildrenofNarereordered.3Forsimplicity,letusﬁrstconsiderthecaseofbinarynodesonly.LetN1andN2,whichyieldphrasesp1andp2respec-tively,bethechildnodesofN.Wewanttodeter-minetheorderofp1andp2withrespecttotheirTLcounterparts,T(p1)andT(p2).Theknowledgeformakingsuchadecisioncanbelearnedfromaword-alignedparallelcorpus.Therearetwoquestionsin-volvedinobtainingtraininginstances:•HowtodeﬁneT(pi)?•HowtodeﬁnetheorderofT(pi)s?Fortheﬁrstquestion,weadoptasimilarmethodasin(Fox,2002):givenanSLphraseps=s1...si...snandawordalignmentmatrixA,wecanenumeratethesetofTLwords{ti:tiA(si)},andthenarrangethewordsintheorderastheyap-pearintheTLsentence.Letﬁrst(t)betheﬁrstwordinthissortedsetandlast(t)bethelastword.T(ps)isdeﬁnedasthephraseﬁrst(t)...last(t)intheTLsentence.NotethatT(ps)maycontainwordsnotintheset{ti}.ThequestionoftheorderoftwoTLphrasesisnotatrivialone.Sinceawordalignmentmatrixusu-allycontainsalotofnoisesaswellasone-to-manyandmany-to-manyalignments,twoTLphrasesmayoverlapwitheachother.Forthesakeofthequalityofreorderingknowledge,ifT(p1)andT(p2)over-lap,thenthenodeNwithchildrenN1andN2isnottakenasatraininginstance.Obviouslyitwillgreatlyreducetheamountoftraininginput.Torem-edydatasparseness,lessprobablealignmentpointsareremovedsoastominimizeoverlappingphrases,since,afterremovingsomealignmentpoint,oneoftheTLphrasesmaybecomeshorterandthetwophrasesmaynolongeroverlap.Theimplementationissimilartotheideaoflexicalweightin(Koehnetal.,2003):allpointsinthealignmentmatricesoftheentiretrainingcorpusarecollectedtocalculatetheprobabilisticdistribution,P(t|s),ofsomeTLword3SomereadersmayprefertheexpressionthesubtreerootedatnodeNtonodeN.Thelattertermisusedinthispaperforsimplicity.723

tgivensomeSLwords.AnypairofoverlappingT(pi)swillberedeﬁnedbyiterativelyremovinglessprobablewordalignmentsuntiltheynolongerover-lap.Iftheystilloverlapafterallone/many-to-manyalignmentshavebeenremoved,thenthereﬁnementwillstopandN,whichcoverspis,isnolongertakenasatraininginstance.Insum,givenabilingualtrainingcorpus,aparserfortheSL,andawordalignmenttool,wecancollectallbinaryparsetreenodes,eachofwhichmaybeaninstanceoftherequiredreorderingknowledge.Thenextquestioniswhatkindofreorderingknowledgecanbeformedoutofthesetraininginstances.Twoformsofreorderingknowledgeareinvestigated:1.ReorderingRules,whichhavetheformZ:XY⇒(XYPr(IN-ORDER)YXPr(INVERTED)whereZisthephraselabelofabinarynodeandXandYarethephraselabelsofZ’schil-dren,andPr(INVERTED)andPr(IN-ORDER)aretheprobabilitythatXandYareinvertedonTLsideandthatnotinverted,respectively.TheprobabilityﬁguresareestimatedbyMaximumLikelihoodEstimation.2.MaximumEntropy(ME)Model,whichdoesthebinaryclassiﬁcationwhetherabinarynode’schildrenareinvertedornot,basedonasetoffeaturesovertheSLphrasescorrespond-ingtothetwochildrennodes.Thefeaturesthatweinvestigatedincludetheleftmost,rightmost,head,andcontextwords4,andtheirPOSs,oftheSLphrases,aswellasthephraselabelsoftheSLphrasesandtheirparent.4TheApplicationofReorderingKnowledgeAfterlearningreorderingknowledge,theprepro-cessingmodulecanapplyittotheparsetree,tS,ofanSLsentenceSandobtainthen-bestlistofS0.SincearankingofS0isneeded,weneedsomewaytoscoreeachS0.Hereprobabilityisusedasthescoringmetric.Inthissectionitisexplained4ThecontextwordsoftheSLphrasesarethewordtotheleftoftheleftphraseandthewordtotherightoftherightphrase.howthen-bestreorderingsofSandtheirassociatedscores/probabilitesarecomputed.Letusﬁrstlookintothescoringofaparticularreordering.LetPr(p→p0)betheprobabilityofre-orderingaphrasepintop0.Foraphraseqyieldedbyanon-binarynode,thereisonlyone‘reordering’ofq,viz.qitself,thusPr(q→q)=1.ForaphrasepyieldedbyabinarynodeN,whoseleftchildN1hasreorderingspi1andrightchildN2hasthereorder-ingspj2(1≤i,j≤n),p0hastheformpi1pj2orpj2pi1.Therefore,Pr(p→p0)=(Pr(IN-ORDER)×Pr(pi1→pi01)×Pr(pj2→pj02)Pr(INVERTED)×Pr(pj2→pj02)×Pr(pi1→pi01)TheﬁguresPr(IN-ORDER)andPr(INVERTED)areobtainedfromthelearnedreorderingknowledge.Ifreorderingknowledgeisrepresentedasrules,thentherequiredprobabilityistheprobabilityassociatedwiththerulethatcanapplytoN.IfreorderingknowledgeisrepresentedasanMEmodel,thentherequiredprobabilityis:P(r|N)=exp(Piλifi(N,r))Pr0exp(Piλifi(N,r0))wherer{IN-ORDER,INVERTED},andfi’sarefea-turesusedintheMEmodel.Letusturntothecomputationofthen-bestre-orderinglist.LetR(N)bethenumberofreorder-ingsofthephraseyieldedbyN,then:R(N)=(2R(N1)R(N2)ifNhaschildrenN1,N21otherwiseItiseasilyseenthatthenumberofS0sincreasesex-ponentially.Fortunately,whatweneedismerelyann-bestlistratherthanafulllistofreorderings.Start-ingfromtheleavesoftS,foreachnodeNcoveringphrasep,weonlykeeptrackofthenp0sthathavethehighestreorderingprobability.ThusR(N)≤n.Thereareatmost2n2reorderingsforanynodeandonlythetop-scorednreorderingsarerecorded.Then-bestreorderingsofS,i.e.then-bestreorderingsoftheyieldoftherootnodeoftS,canbeobtainedbythisefﬁcientbottom-upmethod.5TheGeneralizationofReorderingKnowledgeInthelasttwosectionsreorderingknowledgeislearnedfromandappliedtobinaryparsetreenodes724

only.Itisnotdifﬁculttogeneralizethetheoryofreorderingknowledgetonodesofotherbranchingfactors.Thecaseofbinarynodesissimpleasthereareonlytwopossiblereorderings.Thecaseof3-arynodesisabitmorecomplicatedastherearesix.5Ingeneral,ann-arynodehasn!possiblereorderingsofitschildren.Themaximumentropymodelhasthesameformasinthebinarycase,exceptthattherearemoreclassesofreorderingpatternsasnincreases.Theformofreorderingrules,andthecalculationofreorderingprobabilityforaparticularnode,canalsobegeneralizedeasily.6Theonlyproblemforthegeneralizedreorderingknowledgeisthat,astherearemoreclasses,datasparsenessbecomesmorese-vere.6TheDecoderThelastthreesectionsexplainhowtheS→n×S0partofformula2isdone.TheS0→Tpartissimplydonebyourre-implementationofPHARAOH(Koehn,2004).Notethatnon-monotonoustranslationisusedheresincethedistance-basedmodelisneededforlocalreordering.Forthen×T→ˆTpart,thefactorsinconsiderationincludethescoreofTreturnedbythedecoder,andthereorderingprobabilityPr(S→S0).Inordertoconformtothelog-linearmodelusedinthede-coder,weintegratethetwofactorsbydeﬁningthetotalscoreofTasformula3:exp(λrlogPr(S→S0)+XiλiFi(S0→T))(3)Theﬁrsttermcorrespondstothecontributionofsyntax-basedreordering,whilethesecondtermthatofthefeaturesFiusedinthedecoder.Allthefea-tureweights(λs)weretrainedusingourimplemen-tationofMinimumErrorRateTraining(Och,2003).TheﬁnaltranslationˆTistheTwiththehighesttotalscore.5Namely,N1N2N3,N1N3N2,N2N1N3,N2N3N1,N3N1N2,andN3N2N1,ifthechildnodesintheoriginalorderareN1,N2,andN3.6Forexample,thereorderingprobabilityofaphrasep=p1p2p3generatedbya3-arynodeNisPr(r)×Pr(pi1)×Pr(pj2)×Pr(pk3)whererisoneofthesixreorderingpatternsfor3-arynodes.Itisobservedinpilotexperimentsthat,foralotoflongsentencescontainingseveralclauses,onlyoneoftheclausesisreordered.Thatis,ourgreedyre-orderingalgorithm(c.f.section4)hasatendencytofocusonlyonaparticularclauseofalongsentence.Theproblemwasremediedbymodifyingourde-codersuchthatitnolongertranslatesasentenceatonce;insteadthenewdecoderdoes:1.splitaninputsentenceSintoclauses{Ci};2.obtainthereorderingsamong{Ci},{Sj};3.foreachSj,do(a)foreachclauseCiinSj,doi.reorderCiinton-bestC0is,ii.translateeachC0iintoT(C0i),iii.selectˆT(C0i);(b)concatenate{ˆT(C0i)}intoTj;4.selectˆTj.Step1isdonebycheckingtheparsetreeifthereareanyIPorCPnodes7immediatelyundertherootnode.Ifyes,thenalltheseIPs,CPs,andtheremain-ingsegmentsaretreatedasclauses.Ifno,thentheentireinputistreatedasonesingleclause.Step2andstep3(a)(i)stillfollowthealgorithminsec-tion4.Step3(a)(ii)istrivial,butthereisasubtlepointaboutthecalculationoflanguagemodelscore:thelanguagemodelscoreofatranslatedclauseisnotindependentfromotherclauses;itshouldtakeintoaccountthelastfewwordsoftheprevioustranslatedclause.ThebesttranslatedclauseˆT(C0i)isselectedinstep3(a)(iii)byequation3.Instep4thebesttranslationˆTjisargmaxTjexp(λrlogPr(S→Sj)+Xiscore(T(C0i))).7Experiments7.1CorporaOurexperimentsareaboutChinese-to-Englishtranslation.TheNISTMT-2005testdatasetisusedforevaluation.(Case-sensitive)BLEU-4(Papinenietal.,2002)isusedastheevaluationmetric.The7IPstandsforinﬂectionalphraseandCPforcomplementizerphrase.ThesetwotypesofphrasesareclausesintermsoftheGovernmentandBindingTheory.725

BranchingFactor23>3Count1229431731280Percentage73.4118.957.64Table1:DistributionofParseTreeNodeswithDif-ferentBranchingFactorsNotethatnodeswithonlyonechildareexcludedfromthesurveyasreorderingdoesnotapplytosuchnodes.testsetanddevelopmentsetofNISTMT-2002aremergedtoformourdevelopmentset.ThetrainingdataforbothreorderingknowledgeandtranslationtableistheoneforNISTMT-2005.TheGIGA-WORDcorpusisusedfortraininglanguagemodel.TheChinesesideofallcorporaaresegmentedintowordsbyourimplementationof(Gaoetal.,2003).7.2ThePreprocessingModuleAsmentionedinsection3,thepreprocessingmod-uleforreorderingneedsaparseroftheSL,awordalignmenttool,andaMaximumEntropytrainingtool.WeusetheStanfordparser(KleinandMan-ning,2003)withitsdefaultChinesegrammar,theGIZA++(OchandNey,2000)alignmentpackagewithitsdefaultsettings,andtheMEtooldevelopedby(Zhang,2004).Section5mentionsthatourreorderingmodelcanapplytonodesofanybranchingfactor.Itisinter-estingtoknowhowmanybranchingfactorsshouldbeincluded.Thedistributionofparsetreenodesasshownintable1isbasedontheresultofpars-ingtheChinesesideofNISTMT-2002testsetbytheStanfordparser.Itiseasilyseenthatthemajor-ityofparsetreenodesarebinaryones.Nodeswithmorethan3childrenseemtobenegligible.The3-arynodesoccupyacertainproportionofthedistri-bution,andtheirimpactontranslationperformancewillbeshowninourexperiments.7.3ThedecoderThedataneededbyourPharaoh-likedecoderaretranslationtableandlanguagemodel.Our5-gramlanguagemodelistrainedbytheSRIlanguagemod-elingtoolkit(Stolcke,2002).Thetranslationtableisobtainedasdescribedin(Koehnetal.,2003),i.e.thealignmenttoolGIZA++isrunoverthetrainingdatainbothtranslationdirections,andthetwoalign-TestSettingBLEUB1standardphrase-basedSMT29.22B2(B1)+clausesplitting29.13Table2:ExperimentBaselineTestSettingBLEUBLEU2-ary2,3-ary1rule29.7730.312ME(phraselabel)29.9330.493ME(left,right)30.1030.534ME((3)+head)30.2430.715ME((3)+phraselabel)30.1230.306ME((4)+context)30.2430.76Table3:TestsonVariousReorderingModelsThe3rdcolumncomprisestheBLEUscoresobtainedbyre-orderingbinarynodesonly,the4thcolumnthescoresbyre-orderingbothbinaryand3-arynodes.ThefeaturesusedintheMEmodelsareexplainedinsection3.mentmatricesareintegratedbytheGROW-DIAG-FINALmethodintoonematrix,fromwhichphrasetranslationprobabilitiesandlexicalweightsofbothdirectionsareobtained.Themostimportantsystemparameteris,ofcourse,distortionlimit.Pilotexperimentsusingthestandardphrase-basedmodelshowthattheoptimaldistortionlimitis4,whichwasthereforeselectedforallourexperiments.7.4ExperimentResultsandAnalysisThebaselineofourexperimentsisthestandardphrase-basedmodel,whichachieves,asshownbytable2,theBLEUscoreof29.22.Fromthesametablewecanalsoseethattheclausesplittingmech-anismintroducedinsection6doesnotsigniﬁcantlyaffecttranslationperformance.Twosetsofexperimentswererun.Theﬁrstset,ofwhichtheresultsareshownintable3,teststheeffectofdifferentformsofreorderingknowledge.Inallthesetestsonlythetop10reorderingsofeachclausearegenerated.Thecontrastbetweentests1and2showsthatMEmodelingofreorderingoutperformsreorderingrules.Tests3and4showthatphraselabelscanachieveasgoodperformanceasthelexicalfeaturesofmereleftmostandright-mostwords.However,whenmorelexicalfeatures726

Inputཏ׬ሲ2005৯ᨄयᑖᑛࠖ֨नԚԞദ֩ڔየѕЀχ߰ኦᡷ಴ஃᤨReferenceHainanprovincewillcontinuetoincreaseitsinvestmentinthepublicservicesandsocialservicesinfrastructuresin2005BaselineHainanProvincein2005willcontinuetoincreaseforthepublicserviceandsocialinfrastructureinvestmentTranslationwithPreprocessingHainanProvincein2005willcontinuetoincreaseinvestmentinpublicservicesandsocialinfrastructureTable4:TranslationExample1TestSettingBLEUalengthconstraint30.52bDL=030.48cn=10030.78Table5:TestsonVariousConstraintsareadded(tests4and6),phraselabelscannolongercompetewithlexicalfeatures.Surprisingly,test5showsthatthecombinationofphraselabelsandlex-icalfeaturesisevenworsethanusingeitherphraselabelsorlexicalfeaturesonly.Apartfromquantitativeevaluation,letuscon-siderthetranslationexampleoftest6showninta-ble4.Togeneratethecorrecttranslation,aphrase-baseddecodershould,aftertranslatingtheword“ࠖ֨”as“increase”,jumptothelastword“ஃᤨ(investment)”.Thisisobviouslyoutofthecapa-bilityofthebaselinemodel,andourapproachcanaccomplishthedesiredreorderingasexpected.Byandlarge,theexperimentresultsshowthatnomatterwhatkindofreorderingknowledgeisused,thepreprocessingofsyntax-basedreorderingdoesgreatlyimprovetranslationperformance,andthatthereorderingof3-arynodesiscrucial.Thesecondsetofexperimentstesttheeffectofsomeconstraints.Thebasicsettingisthesameasthatoftest6intheﬁrstexperimentset,andreorder-ingisappliedtobothbinaryand3-arynodes.Theresultsareshownintable5.Intest(a),theconstraintisthatthemoduledoesnotconsideranyreorderingofanodeiftheyieldofthisnodecontainsnotmorethanfourwords.Theunderlyingrationaleisthatreorderingwithindistortionlimitshouldbelefttothedistance-basedmodelduringdecoding,andsyntax-basedreorder-ingshouldfocusonglobalreorderingonly.Theresultshowsthatthishypothesisdoesnothold.Inpracticesyntax-basedreorderingalsohelpslo-calreordering.Considerthetranslationexampleoftest(a)shownintable6.Boththebaselinemodelandourmodeltranslateinthesamewayuptotheword“އᑆ”(whichisincorrectlytranslatedas“and”).Fromthispoint,theproposedpreprocess-ingmodelcorrectlyjumptothelastphrase“ᨆ៞Ͻᡤᡴ/discussed”,whilethebaselinemodelfailtodosoforthebesttranslation.Itshouldbenoted,how-ever,thatthereareonlyfourwordsbetween“އᑆ”andthelastphrase,andthedesiredorderofdecod-ingiswithinthecapabilityofthebaselinesystem.Withthefeatureofsyntax-basedglobalreordering,aphrase-baseddecoderperformsbetterevenwithrespecttolocalreordering.Itisbecausesyntax-basedreorderingaddsmoreweighttoahypothesisthatmoveswordsacrosslongerdistance,whichispenalizedbythedistance-basedmodel.Intest(b)distortionlimitissetas0;i.e.reorder-ingisdonemerelybysyntax-basedpreprocessing.Theworseresultisnotsurprisingsince,afterall,preprocessingdiscardsmanypossibilitiesandthusreducethesearchspaceofthedecoder.Somelocalreorderingmodelisstillneededduringdecoding.Finally,test(c)showsthattranslationperfor-mancedoesnotimprovesigniﬁcantlybyraisingthenumberofreorderings.Thisimpliesthatourap-proachisveryefﬁcientinthatonlyasmallvalueofniscapableofcapturingthemostimportantglobalreorderingpatterns.8ConclusionandFutureWorkThispaperproposesanovel,probabilisticapproachtoreorderingwhichcombinesthemeritsofsyntaxandphrase-basedSMT.Ontheonehand,globalreordering,whichcannotbeaccomplishedbythe727

Inputξ๢ٌ೎,़Ԍ጖ڔ֭ୡއᑆᐵਢಱಈਇᯅᲢᨆ៞ϽᡤᡴReferenceMeanwhile,Yushchenkoandhisassistantsdiscussedissuesconcerningtheestab-lishmentofanewgovernmentBaselineThesametime,YushchenkoassistantsandanewGovernmentonissuesdiscussedTranslationwithPreprocessingThesametime,YushchenkoassistantsandhelddiscussionsontheissueofanewgovernmentTable6:TranslationExample2phrase-basedmodel,isenabledbythetreeopera-tionsinpreprocessing.Ontheotherhand,localre-orderingispreservedandevenstrengthenedinourapproach.Experimentsshowthat,fortheNISTMT-05taskofChinese-to-Englishtranslation,thepro-posalleadstoBLEUimprovementof1.56%.Despitetheencouragingexperimentresults,itisstillnotveryclearhowthesyntax-basedanddistance-basedmodelscomplementeachotherinimprovingwordreordering.Infutureweneedtoinvestigatetheirinteractionandidentifythecontri-butionofeachcomponent.Moreover,itisobservedthattheparsetreesreturnedbyafullparserliketheStanfordparsercontaintoomanynodeswhichseemnotbeinvolvedindesiredreorderings.Shal-lowparsersshouldbetriedtoseeiftheyimprovethequalityofreorderingknowledge.ReferencesYaserAl-Onaizan,andKishorePapineni.2006.Distor-tionModelsforStatisticalMachineTranslation.Pro-ceedingsforACL2006.MichaelCollins,PhilippKoehn,andIvonaKucerova.2005.ClauseRestructuringforStatisticalMachineTranslation.ProceedingsforACL2005.M.R.Costa-juss`a,andJ.A.R.Fonollosa.2006.Statis-ticalMachineReordering.ProceedingsforEMNLP2006.HeidiFox.2002.PhraseCohesionandStatisticalMa-chineTranslation.ProceedingsforEMNLP2002.JianfengGao,MuLi,andChang-NingHuang2003.ImprovedSource-ChannelModelsforChineseWordSegmentation.ProceedingsforACL2003.DanKleinandChristopherD.Manning.2003.AccurateUnlexicalizedParsing.ProceedingsforACL2003.PhilippKoehn,FranzJ.Och,andDanielMarcu.2003.StatisticalPhrase-basedTranslation.ProceedingsforHLT-NAACL2003.PhilippKoehn.2004.Pharaoh:aBeamSearchDe-coderforPhrase-BasedStatisticalMachineTransla-tionModels.ProceedingsforAMTA2004.PhilippKoehn,AmittaiAxelrod,AlexandraBirchMayne,ChrisCallison-Burch,MilesOsborne,andDavidTalbot2005.EdinburghSystemDescriptionforthe2005IWSLTSpeechTranslationEvaluation.ProceedingsforIWSLT2005.FranzJ.Och.2003.MinimumErrorRateTraininginStatisticalMachineTranslation.ProceedingsforACL2003.FranzJ.Och,andHermannNey.2000.ImprovedStatis-ticalAlignmentModels.ProceedingsforACL2000.KishorePapineni,SalimRoukos,ToddWard,andWei-JingZhu.2002.BLEU:aMethodforAutomaticEval-uationofMachineTranslation.ProceedingsforACL2002.ChrisQuirk,ArulMenezes,andColinCherry.2005.De-pendencyTreeletTranslation:SyntacticallyInformedPhrasalSMT.ProceedingsforACL2005.AndreasStolcke.2002.SRILM-AnExtensibleLan-guageModelingToolkit.ProceedingsfortheInterna-tionalConferenceonSpokenLanguageUnderstand-ing2002.ChristophTillmann.2004.AUnigramOrientationModelforStatisticalMachineTranslation.Proceed-ingsforACL2004.FeiXia,andMichaelMcCord2004.ImprovingaStatis-ticalMTSystemwithAutomaticallyLearnedRewritePatterns.ProceedingsforCOLING2004.KenjiYamada,andKevinKnight.2001.Asyntax-basedstatisticaltranslationmodel.ProceedingsforACL2001.LeZhang.2004.MaximumEntropyModelingToolkitforPythonandC++.http://homepages.inf.ed.ac.uk/s0450736/maxenttoolkit.html.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 728–735,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

728

MachineTranslationbyTriangulation:MakingEffectiveUseofMulti-ParallelCorporaTrevorCohnandMirellaLapataHumanComputerResearchCentre,SchoolofInformaticsUniversityofEdinburgh{tcohn,mlap}@inf.ed.ac.ukAbstractCurrentphrase-basedSMTsystemsperformpoorlywhenusingsmalltrainingsets.Thisisaconsequenceofunreliabletranslationes-timatesandlowcoverageoversourceandtargetphrases.Thispaperpresentsamethodwhichalleviatesthisproblembyexploit-ingmultipletranslationsofthesamesourcephrase.Centraltoourapproachistriangula-tion,theprocessoftranslatingfromasourcetoatargetlanguageviaanintermediatethirdlanguage.Thisallowstheuseofamuchwiderrangeofparallelcorporafortrain-ing,andcanbecombinedwithastandardphrase-tableusingconventionalsmoothingmethods.ExperimentalresultsdemonstrateBLEUimprovementsfortriangulatedmod-elsoverastandardphrase-basedsystem.1IntroductionStatisticalmachinetranslation(Brownetal.,1993)hasseenmanyimprovementsinrecentyears,mostnotablythetransitionfromword-tophrase-basedmodels(Koehnetal.,2003).ModernSMTsys-temsarecapableofproducinghighqualitytransla-tionswhenprovidedwithlargequantitiesoftrainingdata.Withonlyasmalltrainingsample,thetrans-lationoutputisofteninferiortotheoutputfromus-inglargercorporabecausethetranslationalgorithmmustrelyonmoresparseestimatesofphrasefre-quenciesandmustalso‘back-off’tosmallersizedphrases.Thisoftenleadstopoorchoicesoftargetphrasesandreducesthecoherenceoftheoutput.Un-fortunately,parallelcorporaarenotreadilyavailableinlargequantities,exceptforasmallsubsetoftheworld’slanguages(seeResnikandSmith(2003)fordiscussion),thereforelimitingthepotentialuseofcurrentSMTsystems.Inthispaperweprovideameansforobtainingmorereliabletranslationfrequencyestimatesfromsmalldatasets.Wemakeuseofmulti-parallelcor-pora(sentencealignedparalleltextsoverthreeormorelanguages).Suchcorporaareoftencreatedbyinternationalorganisations,theUnitedNations(UN)beingaprimeexample.Theypresentachal-lengeforcurrentSMTsystemsduetotheirrela-tivelymoderatesizeanddomainvariability(exam-plesofUNtextsincludepolicydocuments,proceed-ingsofmeetings,letters,etc.).Ourmethodtranslateseachtargetphrase,t,ﬁrsttoanintermediatelan-guage,i,andthenintothesourcelanguage,s.Wecallthistwo-stagetranslationprocesstriangulation(Kay,1997).Wepresentaprobabilisticformulationthroughwhichwecanestimatethedesiredphrasetranslationdistribution(phrase-table)bymarginali-sation,p(s|t)=Pip(s,i|t).Aswithconventionalsmoothingmethods(Koehnetal.,2003;Fosteretal.,2006),triangulationin-creasestherobustnessofphrasetranslationesti-mates.Incontrasttosmoothing,ourmethodallevi-atesdatasparsenessbyexploringadditionalmulti-paralleldataratherthanadjustingtheprobabilitiesofexistingdata.Importantly,triangulationprovidesuswithseparatelyestimatedphrase-tableswhichcouldbefurthersmoothedtoprovidemorereliabledis-tributions.Moreover,thetriangulatedphrase-tablescanbeeasilycombinedwiththestandardsource-targetphrase-table,therebyimprovingthecoverageoverunseensourcephrases.Asanexample,considerFigure1whichshowsthecoverageofunigramsandlargern-gramphraseswhenusingastandardsourcetargetphrase-table,atriangulatedphrase-tablewithone(it)andninelan-guages(all),andacombinationofstandardandtri-angulatedphrase-tables(all+standard).ThephraseswereharvestedfromasmallFrench-Englishbitext729

andevaluatedagainstatestset.Althoughveryfewsmallphrasesareunknown,themajorityoflargerphrasesareunseen.TheItalianandallresultsshowthattriangulationalonecanprovidesimilarorim-provedcoveragecomparedtothestandardsource-targetmodel;furtherimprovementisachievedbycombiningthetriangulatedandstandardmodels(all+standard).ThesemodelsanddatasetswillbedescribedindetailinSection3.Wealsodemonstratethattriangulationcanbeusedonitsown,thatiswithoutasource-targetdis-tribution,andstillyieldacceptabletranslationout-put.Thisisparticularlyheartening,asitprovidesameansoftranslatingbetweenthemany“lowden-sity”languagepairsforwhichwedon’tyethaveasource-targetbitext.ThisallowsSMTtobeappliedtoamuchlargersetoflanguagepairsthanwaspre-viouslypossible.Inthefollowingsectionweprovideanoverviewofrelatedwork.Section3introducesagenerativeformulationoftriangulation.Wepresentourevalua-tionframeworkinSection4andresultsinSection5.2RelatedWorkTheideaofusingmultiplesourcelanguagesforimprovingthetranslationqualityofthetargetlan-guagedatesbackatleasttoKay(1997),whoob-servedthatambiguitiesintranslatingfromonelan-guageontoanothermayberesolvedifatransla-tionintosomethirdlanguageisavailable.Systemswhichhaveusedthisnotionoftriangulationtypi-callycreateseveralcandidatesententialtargettrans-lationsforsourcesentencesviadifferentlanguages.Asingletranslationisthenselectedbyﬁndingthecandidatethatyieldsthebestoverallscore(OchandNey,2001;UtiyamaandIsahara,2007)orbyco-training(Callison-BurchandOsborne,2003).ThistiesinwithrecentworkonensemblecombinationsofSMTsystems,whichhaveusedalignmenttech-niques(Matusovetal.,2006)orsimpleheuristics(Eisele,2005)toguidetargetsentenceselectionandgeneration.BeyondSMT,theuseofanintermediatelanguageasatranslationaidhasalsofoundappli-cationincross-lingualinformationretrieval(GollinsandSanderson,2001).Callison-Burchetal.(2006)proposetheuseofparaphrasesasameansofdealingwithunseensourcephrases.Theirmethodacquiresparaphrasesbyidentifyingcandidatephrasesinthesourcelan-123456phrase lengthproportion of test events in phrase table0.0050.010.020.050.10.20.51standardItalianallall + standardFigure1:Coverageoffr→entestphrasesusinga10,000sen-tencebitext.Thestandardmodelisshownalongsidetriangu-latedmodelsusingone(Italian)ornineotherlanguages(all).guage,translatingthemintomultipletargetlan-guages,andthenbacktothesource.Unknownsourcephrasesaresubstitutedbytheback-translatedparaphrasesandtranslationproceedsonthepara-phrases.Inlinewithpreviouswork,weexploitmulti-plesourcecorporatoalleviatedatasparsenessandincreasetranslationcoverage.However,wedifferinseveralimportantrespects.Ourmethodoper-atesoverphrasesratherthansentences.Weproposeagenerativeformulationwhichtreatstriangulationnotasapost-processingstepbutaspartofthetrans-lationmodelitself.Theinducedphrase-tableentriesarefeddirectlyintothedecoder,thusavoidingtheadditionalinefﬁcienciesofmergingtheoutputofseveraltranslationsystems.AlthoughrelatedtoCallison-Burchetal.(2006)ourmethodisconceptuallysimplerandmoregen-eral.Phrase-tableentriesarecreatedviamultiplesourcelanguageswithouttheintermediatestepofparaphraseextraction,therebyreducingtheexpo-suretocompoundingerrors.Ourphrase-tablesmaywellcontainparaphrasesbutthesearenaturallyin-ducedaspartofourmodel,withoutextraprocessingeffort.Furthermore,weimprovethetranslationesti-matesforbothseenandunseenphrase-tableentries,whereasCallison-Burchetal.concentratesolelyonunknownphrases.IncontrasttoUtiyamaandIsa-hara(2007),weemployalargenumberofinter-mediatelanguagesanddemonstratehowtriangu-latedphrase-tablescanbecombinedwithstandardphrase-tablestoimprovetranslationoutput.730

en varm kartoffeleen hete aardappeluma batata quenteune patateune patate chauddélicateune question délicatea hot potatosourceintermediatetargetFigure2:TriangulationbetweenEnglish(source)andFrench(target),showingthreephrasesinDutch,DanishandPortuguese,respectively.Arrowsdenotephrasesalignedinalanguagepairandalsothegenerativetranslationprocess.3TriangulationWestartwithamotivatingexamplebeforeformalis-ingthemechanicsoftriangulation.Considertrans-latingtheEnglishphraseahotpotato1intoFrench,asshowninFigure2.InourcorpusthisEnglishphraseoccursonlythreetimes.DuetoerrorsinthewordalignmentthephrasewasnotincludedintheEnglish-Frenchphrase-table.Triangulationﬁrsttranslateshotpotatointoasetofintermediatelan-guages(Dutch,DanishandPortugueseareshownintheﬁgure),andthenthesephrasesarefurthertrans-latedintothetargetlanguage(French).Intheex-ample,fourdifferenttargetphrasesareobtained,allofwhichareusefulphrase-tableentries.Wearguethattheredundancyintroducedbyalargesuiteofotherlanguagescancorrectforerrorsinthewordalignmentsandalsoprovidegreatergeneralisation,sincethetranslationdistributionisestimatedfromarichersetofdata-points.Forexample,instancesoftheDanishenvarmkartoffelmaybeusedtotrans-lateseveralEnglishphrases,notonlyahotpotato.Ingeneralweexpectthatawiderrangeofpos-sibletranslationsarefoundforanysourcephrase,simplyduetotheextralayerofindirection.So,ifasourcephrasetendstoalignwithtwodifferenttar-getphrases,thenwewouldalsoexpectittoalignwithtwophrasesinthe‘intermediate’language.Theseintermediatephrasesshouldtheneachalignwithtwotargetphrases,yieldinguptofourtargetphrases.Consequently,triangulationwilloftenpro-ducemorevariedtranslationdistributionsthanthestandardsource-targetapproach.3.1FormalisationWenowformalisetriangulationasagenerativeprobabilisticprocessoperatingindependentlyonphrasepairs.Westartwiththeconditionaldistri-butionoverthreelanguages,p(s,i|t),wherethear-gumentsdenotephrasesinthesource,intermediate1Anidiommeaningasituationforwhichnoonewantstoclaimresponsibility.andtargetlanguage,respectively.Fromthisdistri-bution,wecanﬁndthedesiredconditionaloverthesource-targetpairbymarginalisingouttheinterme-diatephrases:2p(s|t)=Xip(s|i,t)p(i|t)≈Xip(s|i)p(i|t)(1)where(1)imposesasimplifyingconditionalinde-pendenceassumption:theintermediatephrasefullyrepresentstheinformation(semantics,syntax,etc.)inthesourcephrase,renderingthetargetphrasere-dundantinp(s|i,t).Equation(1)requiresthatallphrasesintheintermediate-targetbitextmustalsobefoundinthesource-intermediatebitext,suchthatp(s|i)isde-ﬁned.Clearlythiswilloftennotbethecase.Inthesesituationswecouldback-offtoanotherdistribution(bydiscardingpart,orall,oftheconditioningcon-text),howeverwetakeamorepragmaticapproachandignorethemissingphrases.Thisproblemofmissingcontextsisuncommoninmulti-parallelcor-pora,butismorecommonwhenthetwobitextsaredrawnfromdifferentsources.Whiletriangulationisintuitivelyappealing,itmaysufferfromafewproblems.Firstly,aswithanySMTapproach,thetranslationestimatesarebasedonnoisyautomaticwordalignments.Thisleadstomanyerrorsandomissionsinthephrase-table.Withastandardsource-targetphrase-tabletheseerrorsareonlyencounteredonce,howeverwithtriangulationtheyareencounteredtwice,andthereforetheerrorswillcompound.Thisleadstomorenoisyestimatesthaninthesource-targetphrase-table.Secondly,theincreasedexposuretonoisemeansthattriangulationwillomitagreaterproportionoflargeorrarephrasesthanthestandardmethod.An2Equation(1)isusedwiththesourceandtargetargumentsreversedtogivep(t|s).731

alignmenterrorineitherofthesource-intermediateorintermediate-targetbitextscanpreventtheextrac-tionofasource-targetphrasepair.ThiseffectcanbeseeninFigure1,wherethecoverageoftheItaliantriangulatedphrase-tableisworsethanthestandardsource-targetmodel,despitethetwomodelsusingthesamesizedbitexts.Asweexplaininthenextsection,theseproblemscanbeamelioratedbyus-ingthetriangulatedphrase-tableinconjunctionwithastandardphrase-table.Finally,anotherpotentialproblemstemsfromtheindependenceassumptionin(1),whichmaybeanoversimpliﬁcationandleadtoalossofinformation.TheexperimentsinSection5showthatthiseffectisonlymild.3.2Mergingthephrase-tablesOnceinduced,thetriangulatedphrase-tablecanbeusefullycombinedwiththestandardsource-targetphrase-table.Thesimplestapproachistouselinearinterpolationtocombinethetwo(ormore)distribu-tions,asfollows:p(s,t)=Xjλjpj(s,t)(2)whereeachjointdistribution,pj,hasanon-negativeweight,λj,andtheweightssumtoone.Thejointdistributionfortriangulatedphrase-tablesisdeﬁnedinananalogouswaytoEquation(1).Weexpectthatthestandardphrase-tableshouldbeallocatedahigherweightthantriangulatedphrase-tables,asitwillbelessnoisy.Thejointdistributionisnowconditionalisedtoyieldp(s|t)andp(t|s),whicharebothusedasfeaturesinthedecoder.Notethatthere-sultingconditionaldistributionwillbedrawnsolelyfromoneinputdistributionwhentheconditioningcontextisunseenintheremainingdistributions.Thismayleadtoanover-relianceonunreliabledistribu-tions,whichcanbeamelioratedbysmoothing(e.g.,Fosteretal.(2006)).Asanalternativetolinearinterpolation,wealsoemployaweightedproductforphrase-tablecombi-nation:p(s|t)∝Yjpj(s|t)λj(3)Thishasthesameformusedforlog-lineartrainingofSMTdecoders(Och,2003),whichallowsustotreateachdistributionasafeature,andlearnthemix-ingweightsautomatically.Notethatwemustindi-viduallysmooththecomponentdistributionsin(3)tostopzerosfrompropagating.ForthisweuseSimpleGood-Turingsmoothing(GaleandSamp-son,1995)foreachdistribution,whichprovideses-timatesforzerocountevents.4ExperimentalDesignCorporaWeusedtheEuroparlcorpus(Koehn,2005)forexperimentation.Thiscorpusconsistsofabout700,000sentencesofparliamentaryproceed-ingsfromtheEuropeanUnioninelevenEuropeanlanguages.Wepresentresultsonthefullcorpusforarangeoflanguagepairs.Inaddition,wehavecreatedsmallerparallelcorporabysub-sampling10,000sentencebitextsforeachlanguagepair.Thesecor-poraarelikelytohaveminimaloverlap—about1.5%ofthesentenceswillbesharedbetweeneachpair.However,thephrasaloverlapismuchgreater(10to20%),whichallowsfortriangulationusingthesecommonphrases.Thistrainingsettingwaschosentosimulatetranslatingtoorfroma“lowdensity”language,whereonlyafewsmallindepen-dentlysourcedparallelcorporaareavailable.Thesebitextswereusedfordirecttranslationandtriangula-tion.AllexperimentalresultswereevaluatedontheACL/WMT20053setof2,000sentences,andarereportedinBLEUpercentage-points.DecodingPharaoh(Koehn,2003),abeam-searchdecoder,wasusedtomaximise:T∗=argmaxTYjfj(T,S)λj(4)whereTandSdenoteatargetandsourcesentencerespectively.Theparameters,λj,weretrainedusingminimumerrorratetraining(Och,2003)tomax-imisetheBLEUscore(Papinenietal.,2002)ona150sentencedevelopmentset.Weusedastan-dardsetoffeatures,comprisinga4-gramlanguagemodel,distancebaseddistortionmodel,forwardandbackwardtranslationprobabilities,forwardandbackwardlexicaltranslationscoresandthephrase-andword-counts.Thetranslationmodelsandlex-icalscoreswereestimatedonthetrainingcorpuswhichwasautomaticallyalignedusingGiza++(Ochetal.,1999)inbothdirectionsbetweensourceandtargetandsymmetrisedusingthegrowingheuristic(Koehnetal.,2003).3Fordetailsseehttp://www.statmt.org/wpt05/mt-shared-task.732

LexicalweightsThelexicaltranslationscoreisusedforsmoothingthephrase-tabletranslationesti-mate.Thisrepresentsthetranslationprobabilityofaphrasewhenitisdecomposedintoaseriesofinde-pendentword-for-wordtranslationsteps(Koehnetal.,2003),andhasprovenaveryeffectivefeature(ZensandNey,2004;Fosteretal.,2006).Pharaoh’slexicalweightsrequireaccesstoword-alignments;calculatingthesealignmentsbetweenthesourceandtargetwordsinaphrasewouldprovedifﬁcultforatriangulatedmodel.Thereforeweuseamodiﬁedlexicalscore,correspondingtothemaximumIBMmodel1scoreforthephrasepair:lex(t|s)=1ZmaxaYkp(tk|sak)(5)wherethemaximisation4rangesoverallone-to-manyalignmentsandZnormalisesthescorebythenumberofpossiblealignments.Thelexicalprobabilityisobtainedbyinterpo-latingarelativefrequencyestimateonthesource-targetbitextwithestimatesfromtriangulation,inthesamemannerusedforphrasetranslationsin(1)and(2).Theadditionofthelexicalprobabilityfea-tureyieldedasubstantialgainofuptotwoBLEUpointsoverabasicfeatureset.5ExperimentalResultsTheevaluationofourmethodwasmotivatedbythreequestions:(1)Howdodifferenttrainingre-quirementsaffecttheperformanceofthetriangu-latedmodelspresentedinthispaper?Weexpectperformancegainswithtriangulationonsmallandmoderatedatasets.(2)Ismachinetranslationout-putinﬂuencedbythechoiceoftheintermediatelan-guage/s?Here,wewouldliketoevaluatewhetherthenumberandchoiceofintermediatelanguagesmatters.(3)Whatisthequalityofthetriangulatedphrase-table?Inparticular,weareinterestedintheresultingdistributionandwhetheritissufﬁcientlydistinctfromthestandardphrase-table.5.1TrainingrequirementsBeforereportingourresults,webrieﬂydiscussthespeciﬁcchoiceofmodelforourexperiments.AsmentionedinSection3,ourmethodcombinesthe4Themaximisationin(5)canbereplacedwithasumwithsimilarexperimentalresults.standardinterp+indicseparateen→de12.0312.6612.9512.25fr→en23.0224.6323.8623.43Table1:Differentfeaturesetsusedwiththe10Ktrainingcorpora,usingasinglelanguage(es)fortriangulation.Thecolumnsrefertostandard,uniforminterpolation,interpolationwith0-1indicatorfeatures,andseparatephrase-tables,respec-tively.triangulatedphrase-tablewiththestandardsource-targetone.Thisisdesiredinordertocompensateforthenoiseincurredbythetriangulationprocess.Weusedtwocombinationmethods,namelylinearinter-polation(see(2))andaweightedgeometricmean(see(3)).Table1reportstheresultsfortwotranslationtaskswhentriangulatingwithasinglelanguage(es)us-ingthreedifferentfeaturesets,eachwithdifferenttranslationfeatures.Theinterpolationmodelusesuniformlinearinterpolationtomergethestandardandtriangulatedphrase-tables.Non-uniformmix-turesdidnotprovideconsistentgains,although,asexpected,biasingtowardsthestandardphrase-tablewasmoreeffectivethanagainst.Theindicatormodelusesthesameinterpolateddistributionalongwithaseriesof0-1indicatorfeaturestoidentifythesourceofeachevent,i.e.,ifeach(s,t)pairispresentinphrase-tablej.Wealsotriedper-contextfeatureswithsimilarresults.Theseparatemodelhasasepa-ratefeatureforeachphrase-table.Allthreefeaturesetsimproveoverthestandardsource-targetsystem,whiletheinterpolatedfeaturesprovidedthebestoverallperformance.Therela-tivelypoorerperformanceoftheseparatemodelisperhapssurprising,asitisabletodifferentiallyweightthecomponentdistributions;thisisprobablyduetoMERTnotproperlyhandlingthelargerfea-turesets.Inallsubsequentexperimentswereportresultsusinglinearinterpolation.Asaproofofconcept,weﬁrstassessedtheef-fectoftriangulationoncorporaconsistingof10,000sentencebitexts.Weexpecttriangulationtode-liverperformancegainsonsmallcorpora,sincealargenumberofphrase-tableentrieswillbeun-seen.InTable2eachentryshowstheBLEUscorewhenusingthestandardphrase-tableandtheab-soluteimprovementwhenusingtriangulation.Herewehaveusedthreelanguagesfortriangulation(it∪{de,en,es,fr}\{s,t}).Thesource-targetlan-guageswerechosensoastomirrortheevaluationsetupofNAACL/WMT.Thetranslationtasksrange733

s↓t→deenesfrde-17.5816.8418.06-+1.20+1.99+1.94en12.45-23.8324.05+1.22-+1.04+1.48es12.3123.83-32.69+2.24+1.35-+0.85fr11.7623.0231.22-+2.41+2.24+1.30-Table2:BLEUimprovementsoverthestandardphrase-table(top)wheninterpolatingwiththreetriangulatedphrase-tables(bottom)onthesmalltrainingsample.fromeasy(es→fr)toveryhard(de→en).Inallcasestriangulationresultedinanimprovementintranslationquality,withthehighestgainsobservedforthemostdifﬁculttasks(toandfromGerman).Forthesetasksthestandardsystemshavepoorcov-erage(dueinparttothesizeablevocabularyofGer-manphrases)andthereforethegaincanbelargelyexplainedbytheadditionalcoverageaffordedbythetriangulatedphrase-tables.TotestwhethertriangulationcanalsoimproveperformanceoflargercorporaweransixseparatetranslationtasksonthefullEuroparlcorpus.TheresultsarepresentedinTable3,forasingletrian-gulationlanguageusedalone(triang)oruniformlyinterpolatedwiththestandardphrase-table(interp).Theseresultsshowthattriangulationcanproducehighqualitytranslationsonitsown,whichisnote-worthy,asitallowsforSMTbetweenamuchlargersetoflanguagepairs.Usingtriangulationincon-junctionwiththestandardphrase-tableimprovedoverthestandardsysteminmostinstances,andonlydegradedperformanceonce.TheimprovementislargestfortheGermantaskswhichcanbeex-plainedbytriangulationprovidingbetterrobustnesstonoisyalignments(whichareoftenquitepoorforGerman)andbetterestimatesoflow-countevents.ThedifﬁcultyofaligningGermanwiththeotherlan-guagesisapparentfromtheGiza++perplexity:theﬁnalModel4perplexitiesforGermanarequitehigh,asmuchasdoubletheperplexityformoreeasilyalignedlanguagepairs(e.g.,Spanish-French).Figure3showstheeffectoftriangulationondif-ferentsizedcorporaforthelanguagepairfr→en.Itpresentslearningcurvesforthestandardsystemandatriangulatedsystemusingonelanguage(es).Ascanbeseen,gainsfromtriangulationonlydi-minishslightlyforlargertrainingcorpora,andthattaskstandardintermtrianginterpde→en23.85es23.4824.36en→de17.24es16.2817.42es→en30.48fr29.0630.52en→es29.09fr28.1929.09fr→en29.66es29.5930.36en→fr30.07es28.9429.62Table3:Resultsonthefulltrainingsetshowingtriangulationwithasinglelanguage,bothalone(triang)andalongsideastan-dardmodel(interp).llllsize of training bitext(s)BLEU score10K40K160K700K2224262830lstandardtrianginterpFigure3:Learningcurveforfr→entranslationforthestandardsource-targetmodelandatriangulatedmodelusingSpanishasanintermediatelanguage.thepurelytriangulatedmodelshaveverycompeti-tiveperformance.Thegainfrominterpolationwithatriangulatedmodelisroughlyequivalenttohavingtwiceasmuchtrainingdata.Finally,noticethattriangulationmaybeneﬁtwhenthesentencesineachbitextaredrawnfromthesamesource,inthattherearenounseen‘intermedi-ate’phrases,andtherefore(1)canbeeasilyevalu-ated.Weinvestigatethisbyexaminingtherobust-nessofourmethodinthefaceofdisjointbitexts.Theconceptscontainedineachbitextwillbemorevaried,potentiallyleadingtobettercoverageofthetargetlanguage.Inlieuofastudyondifferentdo-mainbitextswhichweplanforthefuture,webi-sectedtheEuroparlcorpusforfr→en,triangulat-ingwithSpanish.Thetriangulatedmodelswerepre-sentedwithfr-esandes-enbitextsdrawnfromeitherthesamehalfofthecorpusorfromdifferenthalves,resultinginscoresof28.37and28.13,respectively.5Theseresultsindicatethattriangulationiseffective5Thebaselinesource-targetsystemononehalfhasascoreof28.85.734

trianginterpBLEU score19202122232425fi (−14.26)dadadedeelelesesfiititnlnlptptsvsvFigure4:Comparisonofdifferenttriangulationlanguagesforfr→entranslation,relativetothestandardmodel(10Ktrainingsample).Thebarforﬁhasbeentruncatedtoﬁtonthegraph.fordisjointbitexts,althoughideallywewouldtestthiswithindependentlysourcedparalleltexts.5.2ThechoiceofintermediatelanguagesThepreviousexperimentsusedanad-hocchoiceof‘intermediate’language/sfortriangulation,andwenowexaminewhichlanguagesaremosteffec-tive.Figure4showstheefﬁcacyoftheremainingninelanguageswhentranslatingfr→en.Minimumerror-ratetrainingwasnotusedforthisexperiment,orthenextshowninFigure5,inordertohighlighttheeffectofthechangingtranslationestimates.Ro-mancelanguages(es,it,pt)givethebestresults,bothontheirownandwhenusedtogetherwiththestandardphrase-table(usinguniforminterpolation);Germaniclanguages(de,nl,da,sv)areadistantsec-ond,withthelessrelatedGreekandFinnishtheleastuseful.Interpolationyieldsanimprovementforall‘intermediate’languages,evenFinnish,whichhasaverylowscorewhenusedalone.Thesameexperimentwasrepeatedforen→detranslationwithsimilartrends,exceptthattheGermaniclanguagesout-scoredtheRomancelan-guages.Theseﬁndingssuggestthat‘intermediate’languageswhichexhibitahighdegreeofsimilaritywiththesourceortargetlanguagearedesirable.Weconjecturethatthisisaconsequenceofbetterauto-maticwordalignmentsandagenerallyeasiertrans-lationtask,aswellasabetterpreservationofinfor-mationbetweenalignedphrases.Usingasinglelanguagefortriangulationclearlyimprovesperformance,butcanwerealisefurtherimprovementsbyusingadditionallanguages?Fig-123456789# intermediate languagesBLEU score2223242526trianginterpFigure5:Increasingthenumberofintermediatelanguagesusedfortriangulationincreasesperformanceforfr→en(10Ktrain-ingsample).ThedashedlineshowstheBLEUscoreforthestandardphrase-table.ure5showstheperformanceproﬁleforfr→enwhenaddinglanguagesinaﬁxedorder.Thelan-guageswereorderedbyfamily,withRomancebe-foreGermanicbeforeGreekandFinnish.Eachad-ditionresultsinanincreaseinperformance,evenfortheﬁnallanguages,fromwhichweexpectlittlein-formation.Thepurelytriangulated(triang)andin-terpolatedscores(interp)areconverging,suggestingthatthesource-targetbitextisredundantgivensuf-ﬁcienttriangulateddata.Weobtainedsimilarresultsforen→de.5.3Evaluatingthequalityofthephrase-tableOurexperimentalresultssofarhaveshownthattriangulationisnotamereapproximationofthesource-targetphrase-table,butthatitextractsaddi-tionalusefultranslationinformation.Wenowas-sessthephrase-tablequalitymoredirectly.Com-parativestatisticsofastandardandatriangulatedphrase-tablearegiveninTable4.Thecoverageoversourceandtargetphrasesismuchhigherinthestan-dardtablethanthetriangulatedtables,whichreﬂectsthereducedabilityoftriangulationtoextractlargephrases—despitethelargeincreaseinthenum-berofevents.Thetablealsoshowstheoverlappingprobabilitymasswhichmeasuresthesumofprob-abilityinonetableforwhichtheeventsarepresentintheother.Thisshowsthatthemajorityofmassissharedbybothtables(asjointdistributions),al-thoughtherearesigniﬁcantdifferences.TheJensen-Shannondivergenceisperhapsmoreappropriateforthecomparison,givingarelativelyhighdivergence735

standardtriangsourcephrases(M)82.5targetphrases(M)72.5events(M)1270overlappingmass0.6460.750Table4:Comparativestatisticsofthestandardtriangulatedtableonfr→enusingthefulltrainingsetandSpanishasaninter-mediatelanguage.of0.3937.Thisaugurswellforthecombinationofstandardandtriangulatedphrase-tables,wheredi-versityisvalued.Thedecodingresults(showninTable3forfr→en)indicatethatthetwometh-odshavesimilarefﬁcacy,andthattheirinterpolatedcombinationprovidesthebestoverallperformance.6ConclusionInthispaperwehavepresentedanovelmethodforobtainingmorereliabletranslationestimatesfromsmalldatasets.Thekeypremiseofourworkisthatmulti-paralleldatacanbeusefullyexploitedforim-provingthecoverageandqualityofphrase-basedSMT.Ourtriangulationmethodtranslatesfromasourcetoatargetviaoneormanyintermediatelan-guages.Wepresentagenerativeformulationofthisprocessandshowhowitcanbeusedtogetherwiththeentriesofastandardsource-targetphrase-table.Weobservelargeperformancegainswhentrans-latingwithtriangulatedmodelstrainedonsmalldatasets.Furthermore,whencombinedwithastan-dardphrase-table,ourmodelsalsoyieldperfor-manceimprovementsonlargerdatasets.Ourexper-imentsrevealedthattriangulationbeneﬁtsfromalargesetofintermediatelanguagesandthatperfor-manceisincreasedwhenlanguagesofthesamefam-ilytothesourceortargetareusedasintermediates.Wehavejustscratchedthesurfaceofthepossi-bilitiesfortheframeworkdiscussedhere.Importantfuturedirectionslieincombiningtriangulationwithrichermeansofconventionalsmoothingandusingtriangulationtotranslatebetweenlow-densitylan-guagepairs.AcknowledgementsTheauthorsacknowledgethesupportofEPSRC(grantsGR/T04540/01andGR/T04557/01).SpecialthankstoMarkusBecker,ChrisCallison-Burch,DavidTalbotandMilesOsbornefortheirhelpfulcomments.ReferencesP.F.Brown,V.J.D.Pietra,S.A.D.Pietra,R.L.Mercer.1993.Themathematicsofstatisticalmachinetranslation:Parame-terestimation.ComputationalLinguistics,19(2):263–311.C.Callison-Burch,M.Osborne.2003.Bootstrappingparallelcorpora.InProceedingsoftheNAACLWorkshoponBuild-ingandUsingParallelTexts:DataDrivenMachineTrans-lationandBeyond,Edmonton,Canada.C.Callison-Burch,P.Koehn,M.Osborne.2006.Improvedsta-tisticalmachinetranslationusingparaphrases.InProceed-ingsoftheHLT/NAACL,17–24,NewYork,NY.A.Eisele.2005.Firststepstowardsmulti-enginemachinetranslation.InProceedingsoftheACLWorkshoponBuild-ingandUsingParallelTexts,155–158,AnnArbor,MI.G.Foster,R.Kuhn,H.Johnson.2006.Phrase-tablesmooth-ingforstatisticalmachinetranslation.InProceedingsoftheEMNLP,53–61,Sydney,Australia.W.A.Gale,G.Sampson.1995.Good-turingfrequencyesti-mationwithouttears.JournalofQuantitativeLinguistics,2(3):217–237.T.Gollins,M.Sanderson.2001.Improvingcrosslanguageretrievalwithtriangulatedtranslation.InProceedingsoftheSIGIR,90–95,NewOrleans,LA.M.Kay.1997.Theproperplaceofmenandmachinesinlan-guagetranslation.MachineTranslation,12(1–2):3–23.P.Koehn,F.J.Och,D.Marcu.2003.Statisticalphrase-basedtranslation.InProceedingsoftheHLT/NAACL,48–54,Edomonton,Canada.P.Koehn.2003.NounPhraseTranslation.Ph.D.thesis,Uni-versityofSouthernCalifornia,LosAngeles,California.P.Koehn.2005.Europarl:Aparallelcorpusforevaluationofmachinetranslation.InProceedingsofMTSummit,Phuket,Thailand.E.Matusov,N.Uefﬁng,H.Ney.2006.Computingconsesustranslationfrommultiplemachinetranslationsystemsus-ingenhancedhypothesesalignment.InProceedingsoftheEACL,33–40,Trento,Italy.F.J.Och,H.Ney.2001.Statisticalmulti-sourcetranslation.InProceedingsoftheMTSummit,253–258,SantiagodeCom-postela,Spain.F.J.Och,C.Tillmann,H.Ney.1999.Improvedalignmentmodelsforstatisticalmachinetranslation.InProceedingsoftheEMNLPandVLC,20–28,UniversityofMaryland,Col-legePark,MD.F.J.Och.2003.Minimumerrorratetraininginstatisticalma-chinetranslation.InProceedingsoftheACL,160–167,Sap-poro,Japan.K.Papineni,S.Roukos,T.Ward,W.-J.Zhu.2002.BLEU:Amethodforautomaticevaluationofmachinetranslation.InProceedingsoftheACL,311–318,Philadelphia,PA.P.Resnik,N.A.Smith.2003.TheWebasaparallelcorpus.ComputationalLinguistics,29(3):349–380.M.Utiyama,H.Isahara.2007.Acomparisonofpivotmethodsforphrase-basedstatisticalmachinetranslation.InProceed-ingsoftheHLT/NAACL,484–491,Rochester,NY.R.Zens,H.Ney.2004.Improvementsinphrase-basedstatisti-calmachinetranslation.InD.M.SusanDumais,S.Roukos,eds.,ProceedingsoftheHLT/NAACL,257–264,Boston,MA.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 736–743,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

736

AMaximumExpectedUtilityFrameworkforBinarySequenceLabelingMartinJansche∗jansche@acm.orgAbstractWeconsidertheproblemofpredictiveinfer-enceforprobabilisticbinarysequencelabel-ingmodelsunderF-scoreasutility.Forasimpleclassofmodels,weshowthatthenumberofhypotheseswhoseexpectedF-scoreneedstobeevaluatedislinearinthesequencelengthandpresentaframeworkforefﬁcientlyevaluatingtheexpectationofmanycommonloss/utilityfunctions,includingtheF-score.Thisframeworkincludesbothexactandfasterinexactcalculationmethods.1Introduction1.1MotivationandScopeTheweightedF-score(vanRijsbergen,1974)playsanimportantroleintheevaluationofbinaryclassi-ﬁers,asitneatlysummarizesaclassiﬁer’sabilitytoidentifythepositiveclass.Avarietyofmethodsex-istsfortrainingclassiﬁersthatoptimizetheF-score,orsomesimilartrade-offbetweenfalsepositivesandfalsenegatives,precisionandrecall,sensitivityandspeciﬁcity,typeIerrorandtypeIIerrorrate,etc.AmongthemostgeneralmethodsarethoseofMozeretal.(2001),whoseconstrainedoptimizationtech-niqueissimilartothosein(Gaoetal.,2006;Jansche,2005).Morespecializedmethodsalsoexist,forex-ampleforsupportvectormachines(Musicantetal.,2003)andforconditionalrandomﬁelds(Grossetal.,2007;Suzukietal.,2006).Allofthesemethodsareaboutclassiﬁertraining.Inthispaperwefocusprimarilyontherelated,butorthogonal,issueofpredictiveinferencewithafullytrainedprobabilisticclassiﬁer.UsingtheweightedF-scoreasourutilityfunction,predictiveinferenceamountstochoosinganoptimalhypothesiswhichmaximizestheexpectedutility.Werefertothisas∗Currentafﬁliation:GoogleInc.Formerafﬁliation:CenterofComputationalLearningSystems,ColumbiaUniversity.thepredictionordecodingtask.Ingeneral,decodingcanbeahardcomputationalproblem(CasacubertaanddelaHiguera,2000;Knight,1999).InthispaperweshowthatthemaximumexpectedF-scoredecod-ingproblemcanbesolvedinpolynomialtimeundercertainassumptionsabouttheunderlyingprobabil-itymodel.OnekeyingredientinoursolutionisaverygeneralframeworkforevaluatingtheexpectedF-score,andindeedmanyotherutilityfunctions,ofaﬁxedhypothesis.1Thisframeworkcanalsobeap-pliedtodiscriminativeclassiﬁertraining.1.2BackgroundandNotationWeformulateourapproachintermsofsequencela-beling,althoughithasapplicationsbeyondthat.Thisismotivatedbythefactthatourframeworkforevalu-atingexpectedutilityisindeedapplicabletogeneralsequencelabelingtasks,whileourdecodingmethodismorerestricted.AnotherreasonisthattheF-scoreisonlymeaningfulforcomparingtwo(multi)setsortwobinarysequences,butthenotationformultisetsisslightlymoreawkward.Alltasksconsideredhereinvolvestringsofbinarylabels.Wewritethelengthofagivenstringy∈{0,1}nas|y|=n.Itisconvenienttoviewsuchstringsasrealvectors–whosecomponentshappentobe0or1–withthedotproductdeﬁnedasusual.Theny·yisthenumberofonesthatoccurinthestringy.Fortwostringsx,yofthesamelength|x|=|y|thenumberofonesthatoccuratcorrespondingindicesisx·y.Givenahypothesiszandagoldstandardlabelsequencey,wedeﬁnethefollowingquantities:1.T=y·y,thegenuinepositives;2.P=z·z,thepredictedpositives;3.A=z·y,thetruepositives(predictedpositivesthataregenuinelypositive);1Aproof-of-conceptimplementationisavailableathttp://purl.org/net/jansche/meu_framework/.737

4.Recl=A/T,recall(a.k.a.sensitivityorpower);5.Prec=A/P,precision.Theβ-weightedF-scoreisthendeﬁnedastheweightedharmonicmeanofrecallandprecision.ThissimpliﬁestoFβ=(β+1)AP+βT(β>0)(1)whereweassumeforconveniencethat0/0def=1toavoidexplicitlydealingwiththespecialcaseofthedenominatorbeingzero.WewillwritetheweightedF-scorefromnowonasF(z,y)toemphasizethatitisafunctionofzandy.1.3ExpectedF-ScoreInSection3wewilldevelopamethodforevaluatingtheexpectationoftheF-score,whichcanalsobeusedasasmoothapproximationoftherawF-scoreduringclassiﬁertraining:inthattask(whichwewillnotdiscussfurtherinthispaper),zarethesupervisedlabels,yistheclassiﬁeroutput,andthechallengeisthatF(z,y)doesnotdependsmoothlyontheparam-etersoftheclassiﬁer.Gradient-basedoptimizationtechniquesarenotapplicableunlesssomeofthequan-titiesdeﬁnedabovearereplacedbyapproximationsthatdependsmoothlyontheclassiﬁer’sparameters.Forexample,theconstrainedoptimizationmethodof(Mozeretal.,2001)reliesonapproximationsofsensitivity(whichtheycallCA)andspeciﬁcity2(theirCR);relatedtechniques(Gaoetal.,2006;Jansche,2005)relyonapproximationsoftruepositives,falsepositives,andfalsenegatives,and,indirectly,recallandprecision.UnlikethesemethodswecomputetheexpectedF-scoreexactly,withoutrelyingonadhocapproximationsofthetruepositives,etc.BeingabletoefﬁcientlycomputetheexpectedF-scoreisaprerequisiteformaximizingitduringde-coding.Moreprecisely,wecomputetheexpectationofthefunctiony7→F(z,y),(2)whichisaunaryfunctionobtainedbyholdingtheﬁrstargumentofthebinaryfunctionFﬁxed.ItwillhenceforthbeabbreviatedasF(z,·),andwewillde-noteitsexpectedvaluebyE[F(z,·)]=∑y∈{0,1}|z|F(z,y)Pr(y).(3)2Deﬁnedas[(~1−z)·(~1−y)](cid:14)[(~1−y)·(~1−y)].Thisexpectationistakenwithrespecttoaprobabilitymodeloverbinarylabelsequences,writtenasPr(y)forsimplicity.Thisprobabilitymodelmaybecondi-tional,thatis,ingeneralitwilldependoncovariatesxandparametersθ.Wehavesuppressedbothinournotation,sincexisﬁxedduringtraininganddecod-ing,andweassumethatthemodelisfullyidentiﬁedduringdecoding.Thisisforclarityonlyanddoesnotlimittheclassofmodels,thoughwewillintroduceadditional,limitingassumptionsshortly.Wearenowreadytotackletheinferencetaskformally.2MaximumExpectedF-ScoreInference2.1ProblemStatementOptimalpredictiveinferenceunderF-scoreutilityrequiresustoﬁndanhypothesisˆzoflengthnwhichmaximizestheexpectedF-scorerelativetoagivenprobabilisticsequencelabelingmodel:ˆz=argmaxz∈{0,1}nE[F(z,·)]=argmaxz∈{0,1}n∑yF(z,y)Pr(y).(4)Werequiretheprobabilitymodeltofactorintoinde-pendentBernoullicomponents(Markovorderzero):Pr(y=(y1,...,yn))=n∏i=1pyii(1−pi)1−yi.(5)Inpracticalapplicationswemightchoosetheoverallprobabilitydistributiontobetheproductofindepen-dentlogisticregressionmodels,forexample.Ordi-naryclassiﬁcationarisesasaspecialcasewhentheyiarei.i.d.,thatis,asingleprobabilisticclassiﬁerisusedtoﬁndPr(yi=1|xi).Forourpresentpurposesitissufﬁcienttoassumethattheinferencealgorithmtakesasitsinputthevector(p1,...,pn),wherepiistheprobabilitythatyi=1.Thediscretemaximizationproblem(4)cannotbesolvednaively,sincethenumberofhypothesesthatwouldneedtobeevaluatedinabrute-forcesearchforanoptimalhypothesisˆzisexponentialinthesequencelengthn.Weshowbelowthatinfactonlyafewhypotheses(n+1insteadof2n)needtobeexaminedinordertoﬁndanoptimalone.Theinferencealgorithmistheintuitiveone,analo-goustothefollowingsimpleobservation:Startwiththehypothesisz=00...0andevaluateitsrawF-scoreF(z,y)relativetoaﬁxedbutunknownbinary738

stringy.Thenzwillhaveperfectprecision(noposi-tivelabelsmeansnochancetomakemistakes),andzerorecall(unlessy=z).Switchonanybitofzthatiscurrentlyoff.Thenprecisionwilldecreaseorre-mainequal,whilerecallwillincreaseorremainequal.Repeatuntilz=11...1isreached,inwhichcasere-callwillbeperfectandprecisionatitsminimum.TheinferencealgorithmforexpectedF-scorefollowsthesamestrategy,andinparticularitswitchesonthebitsofzinorderofnon-increasingprobability:startwith00...0,thenswitchonthebiti1=argmaxipi,etc.until11...1isreached.Wenowshowthatthisintuitivestrategyisindeedadmissible.2.2OuterandInnerMaximizationIngeneral,maximizationcanbecarriedoutpiece-wise,sinceargmaxx∈Xf(x)=argmaxx∈{argmaxy∈Yf(y)|Y∈π(X)}f(x),whereπ(X)isanyfamily(Y1,Y2,...)ofnonemptysubsetsofXwhoseunionSiYiisequaltoX.(Recur-siveapplicationwouldleadtoadivide-and-conqueralgorithm.)Duplicationofeffortisavoidedifπ(X)isapartitionofX.Herewepartitiontheset{0,1}nintoequivalenceclassesbasedonthenumberofonesinastring(viewedasarealvector).DeﬁneSmtobethesetSm={s∈{0,1}n|s·s=m}consistingofallbinarystringsofﬁxedlengthnthatcontainexactlymones.Thenthemaximizationprob-lem(4)canbetransformedintoaninnermaximiza-tionˆs(m)=argmaxs∈SmE[F(s,·)],(6)followedbyanoutermaximizationˆz=argmaxz∈{ˆs(0),...,ˆs(n)}E[F(z,·)].(7)2.3Closed-FormInnerMaximizationThekeyinsightisthattheinnermaximizationprob-lem(6)canbesolvedanalytically.Givenavectorp=(p1,...,pn)ofprobabilities,deﬁnez(m)tobethebinarylabelsequencewithexactlymonesandn−mzeroeswhereforallindicesi,kwehavehz(m)i=1∧z(m)k=0i→pi≥pk.Algorithm1MaximizingtheExpectedF-Score.1:Input:probabilitiesp=(p1,...,pn)2:I←indicesofpsortedbynon-increasingprobability3:z←0...04:a←05:v←expectF(z,p)6:forj←1tondo7:i←I[j]8:z[i]←1//switchontheithbit9:u←expectF(z,p)10:ifu>vthen11:a←j12:v←u13:forj←a+1tondo14:z[I[j]]←015:return(z,v)Inotherwords,themostprobablembits(accordingtop)inz(m)aresetandtheleastprobablen−mbitsareoff.Werelyonthefollowingresult,whoseproofisdeferredtoAppendixA:Theorem1.(∀s∈Sm)E[F(z(m),·)]≥E[F(s,·)].Becausez(m)ismaximalinSm,wemayequatez(m)=argmaxs∈SmE[F(s,·)]=ˆs(m)(moduloties,whichcanalwaysarisewithargmax).2.4PedestrianOuterMaximizationWiththeinnermaximization(6)thussolved,theoutermaximization(7)canbecarriedoutnaively,sinceonlyn+1hypothesesneedtobeevaluated.ThisispreciselywhatAlgorithm1does,whichkeepstrackofthemaximumvalueinv.Onterminationz=argmaxsE[F(s,·)].Correctnessfollowsdirectlyfromourresultsinthissection.Algorithm1runsintimeO(nlogn+nf(n)).AtotalofO(nlogn)timeisrequiredforaccessingthevectorpinsortedorder(line2).ThisdominatestheO(n)timerequiredtoexplicitlygeneratetheoptimalhypothesis(lines13–14).ThealgorithminvokesasubroutineexpectF(z,p)atotalofn+1times.Thissubroutine,whichisthetopicofthenextsection,evaluates,intimef(n),theexpectedF-score(withrespecttop)ofagivenhypothesiszoflengthn.3ComputingtheExpectedF-Score3.1ProblemStatementWenowturntotheproblemofcomputingtheex-pectedvalue(3)oftheF-scoreforagivenhypothesiszrelativetoafullyidentiﬁedprobabilitymodel.Themethodpresentedheredoesnotstrictlyrequirethe739

zeroth-orderMarkovassumption(5)instatedearlier(ahigher-orderMarkovassumptionwillsufﬁce),butitshallremainineffectforsimplicity.Aswiththemaximizationproblem(4),thesumin(3)isoverexponentiallymanytermsandcannotbecomputednaively.ButobservethattheF-score(1)isa(rational)functionofintegercountswhicharebounded,soitcantakeononlyaﬁnite,andindeedsmall,numberofdistinctvalues.Weshallseeshortlythatthefunction(2)whoseexpectationwewishtocomputehasadomainwhosecardinalityisexponen-tialinn,butthecardinalityofitsrangeispolynomialinn.Thelatterissufﬁcienttoensurethatitsex-pectationcanbecomputedinpolynomialtime.ThemethodweareabouttodevelopisinfactverygeneralandappliestomanyotherlossandutilityfunctionsbesidestheF-score.3.2ExpectedF-ScoreasanIntegralAfewnotionsfromrealanalysisarehelpfulbecausetheyhighlighttheimportanceofthinkingaboutfunc-tionsintermsoftheirrange,levelsets,andtheequiv-alenceclassestheyinduceontheirdomain(thekernelofthefunction).Afunctiong:Ω→Rissaidtobesimpleifitcanbeexpressedasalinearcombinationofindicatorfunctions(characteristicfunctions):g(x)=∑k∈KakχBk(x),whereKisaﬁniteindexset,ak∈R,andBk⊆Ω.(χS:S→{0,1}isthecharacteristicfunctionofsetS.)LetΩbeacountablesetandPbeaprobabilitymeasureonΩ.ThentheexpectationofgisgivenbytheLebesgueintegralofg.Inthecaseofasimplefunctiongasdeﬁnedabove,theintegral,andhencetheexpectation,isdeﬁnedasE[g]=ZΩgdP=∑k∈KakP(Bk).(8)ThisgivesusageneralrecipeforevaluatingE[g]whenΩismuchlargerthantherangeofg.Insteadofcomputingthesum∑y∈Ωg(y)P({y})wecancom-putethesumin(8)above.ThisdirectlyyieldsanefﬁcientalgorithmwheneverKissufﬁcientlysmallandP(Bk)canbeevaluatedefﬁciently.TheexpectedF-scoreisthustheLebesgueintegralofthefunction(2).Lookingatthedeﬁnitionofthe0,0Y:n, n:n1,1Y:Y0,1n:YY:n, n:n2,2Y:Y1,2n:YY:n, n:nY:Y0,2n:YY:n, n:n3,3Y:Y2,3n:YY:n, n:nY:Y1,3n:YY:n, n:nY:Y0,3n:YY:n, n:nY:n, n:nY:n, n:nY:n, n:nFigure1:FiniteStateClassiﬁerh0.F-scorein(1)weseethattheonlyexpressionswhichdependonyareA=z·yandT=y·y(P=z·zisﬁxedbecausezis).But0≤z·y≤y·y≤n=|z|.ThereforeF(z,·)takesonatmost(n+1)(n+2)/2,i.e.quadraticallymany,distinctvalues.ItisasimplefunctionwithK={(A,T)∈N0×N0|A≤T≤|z|,A≤z·z}a(A,T)=(β+1)Az·z+βTwhere0/0def=1B(A,T)={y|z·y=A,y·y=T}.3.3ComputingMembershipinBkObservethatthefamilyofsets(cid:0)B(A,T)(cid:1)(A,T)∈Kisapartition(namelythekernelofF(z,·))ofthesetΩ={0,1}nofalllabelsequencesoflengthn.Inturnitgivesrisetoafunctionh:Ω→Kwhereh(y)=kiffy∈Bk.Thefunctionhcanbecomputedbyadeterministicﬁniteautomaton,viewedasasequenceclassiﬁer:ratherthanassigningbinaryaccept/rejectlabels,itassignsarbitrarylabelsfromaﬁniteset,inthiscasetheindexsetK.Forsimplicityweshowtheinitialportionofaslightlymoregeneraltwo-tapeautomatonh0inFigure1.Itreadsthetwosequenceszandyonitstwoinputtapesandcountsthenumberofmatchingpositivelabels(representedasY)aswellasthenumberofpositivelabelsonthesecondtape.Itsbehavioristhereforeh0(z,y)=(z·y,y·y).Thefunctionhisobtainedasaspecialcasewhenz(theﬁrsttape)isﬁxed.Notethatthisonlyappliestothespecialcasewhen740

Algorithm2SimpleFunctionInstanceforF-Score.defstart():return(0,0)deftransition(k,z,i,yi):(A,T)←kifyi=1thenT←T+1ifz[i]=1thenA←A+1return(A,T)defa(k,z):(A,T)←kF←(β+1)Az·z+βT//where0/0def=1returnFAlgorithm3ValueofaSimpleFunction.1:Input:instancegofthesimplefunctioninterface,stringszandyoflengthn2:k←g.start()3:fori←1tondo4:k←g.transition(k,z,i,y[i])5:returng.a(k,z)thefamilyB=(Bk)k∈KisapartitionofΩ.Itisal-wayspossibletoexpressanysimplefunctioninthisway,butingeneraltheremaybeanexponentialin-creaseinthesizeofKwhenthefamilyBisrequiredtobeapartition.Howeverforthespecialcasesweconsiderherethisproblemdoesnotarise.3.4TheSimpleFunctionTrickIngeneral,whatwewillcallthesimplefunctiontrickamountstorepresentingthesimplefunctiongwhoseexpectationwewanttocomputeby:1.aﬁniteindexsetK(perhapsimplicit),2.adeterministicﬁnitestateclassiﬁerh:Ω→K,3.andavectorofcoefﬁcients(ak)k∈K.Inpractice,thismeansinstantiatinganinterfacewiththreemethods:thestartandtransitionfunctionofthetransducerwhichcomputesh0(andfromwhichhcanbederived),andanaccessormethodforthecoefﬁ-cientsa.Algorithm2showstheF-scoreinstance.Anysimplefunctiongexpressedasaninstanceofthisinterfacecanthenbeevaluatedverysimplyasg(x)=ah(x).ThisisshowninAlgorithm3.EvaluatingE[g]isalsostraightforward:ComposetheDFAhwiththeprobabilitymodelpanduseanal-gebraicpathalgorithmtocomputethetotalprobabil-itymassP(Bk)foreachﬁnalstatekoftheresultingautomaton.Ifpfactorsintoindependentcomponentsasrequiredby(5),thecompositionisgreatlysim-Algorithm4ExpectationofaSimpleFunction.1:Input:instancegofthesimplefunctioninterface,stringzandprobabilityvectorpoflengthn2:M←Map()3:M[g.start()]←14:fori←1tondo5:N←Map()6:for(k,P)∈Mdo7://transitiononyi=08:k0←g.transition(k,z,i,0)9:ifk0/∈Nthen10:N[k0]←011:N[k0]←N[k0]+P×(1−p[i])12://transitiononyi=113:k1←g.transition(k,z,i,1)14:ifk1/∈Nthen15:N[k1]←016:N[k1]←N[k1]+P×p[i]17:M←N18:E←019:for(k,P)∈Mdo20:E←E+g.a(k,z)×P21:returnEpliﬁed.Ifpincorporateslabelhistory(higher-orderMarkovassumption),nothingchangesinprinciple,thoughthefollowingalgorithmassumesforsimplic-itythatthestrongerassumptionisineffect.Algorithm4expandsthefollowingcomposedau-tomaton,representedimplicitly:theﬁnite-statetrans-ducerh0speciﬁedaspartofthesimplefunctionobjectgiscomposedontheleftwiththestringz(yieldingh)andontherightwiththeprobabilitymodelp.Theouterloopvariableiisanindexintozandhenceastateintheautomatonthatacceptsz;thevariablekkeepstrackofthestatesoftheautomatonimple-mentedbyg;andtheprobabilitymodelhasasinglestatebyassumption,whichdoesnotneedtoberep-resentedexplicitly.Exploringthestatesinorderofincreasingiputsthemintopologicalorder,whichmeansthatthealgebraicpathproblemcanbesolvedintimelinearinthesizeofthecomposedautomaton.ThemapsMandNkeeptrackofthealgebraicdis-tancefromthestartstatetoeachintermediatestate.Onterminationoftheﬁrstouterloop(lines4–17),themapMcontainstheﬁnalstatestogetherwiththeirdistances.ThealgebraicdistanceofaﬁnalstatekisnowequaltoP(Bk),sotheexpectedvalueEcanbecomputedinthesecondloop(lines18–20)assuggestedby(8).WhentheutilityfunctioninterfacegisinstantiatedasinAlgorithm2torepresenttheF-score,therun-timeofAlgorithm4iscubicinn,withverysmall741

constants.3Theﬁrstmainloopiteratesovern.Theinnerloopiteratesoverthestatesexpandedatitera-tioni,ofwhichthereareO(i2)manywhendealingwiththeF-score.Thesecondmainloopiteratesovertheﬁnalstates,whosenumberisquadraticinninthiscase.Theoverallcubicruntimeoftheﬁrstloopdominatesthecomputation.3.5OtherUtilityFunctionsWithotherfunctionsgtheruntimeofAlgorithm4willdependontheasymptoticsizeoftheindexsetK.Ifthereareasymptoticallyasmanyintermediatestatesatanypointasthereareﬁnalstates,thenthegeneralasymptoticruntimeisO(n|K|).Manyloss/utilityfunctionsaresubsumedbythepresentframework.Zero–onelossistrivial:theau-tomatonhastwostates(success,failure);itstartsandremainsinthesuccessstateaslongasthesymbolsreadonbothtapesmatch;ontheﬁrstmismatchittransitionsto,andremainsin,thefailurestate.Hamming(1950)distanceissimilartozero–oneloss,butcountsthenumberofmismatches(boundedbyn),whereaszero–onelossonlycountsuptoathresholdofone.AmoreinterestingcaseisgivenbythePk-score(Beefermanetal.,1999)anditsgeneralizations,whichmovesaslidingwindowofsizekoverapairoflabelsequences(z,y)andcountsthenumberofwindowswhichcontainasegmentboundaryononeofthesequencesbutnottheother.Tocomputeitsexpectationinourframework,allwehavetodoisexpresstheslidingwindowmechanismasanautoma-ton,whichcanbedoneverynaturally(seetheproof-of-conceptimplementationforfurtherdetails).4FasterInexactComputationsBecausetheexactcomputationoftheexpectedF-scorebyAlgorithm4requirescubictime,theoverallruntimeofAlgorithm1(thedecoder)isquartic.43Atightupperboundonthetotalnumberofstatesofthecom-posedautomatonintheworstcaseisj112n3+58n2+1712n+1k.4Itispossibletospeedupthedecodingalgorithminabsoluteterms,thoughnotasymptotically,byexploitingthefactthatitexploresverysimilarhypothesesinsequence.Algorithm4canbemodiﬁedtostoreandreturnallofitsintermediatemapdata-structures.Thismodiﬁedalgorithmthenrequirescubicspaceinsteadofquadraticspace.Thisadditionalstoragecostpaysoffwhenthealgorithmiscalledasecondtime,withitsformalparameterzboundtoastringthatdiffersfromtheoneoftheFasterdecodingcanbeachievedbymodifyingAl-gorithm4tocomputeanapproximation(infact,alowerbound)oftheexpectedF-score.5ThisisdonebyintroducinganadditionalparameterLwhichlimitsthenumberofintermediatestatesthatgetexpanded.Insteadofiteratingoverallstatesandtheirassoci-atedprobabilities(innerloopstartingatline6),oneiteratesoverthetopLstatesonly.WerequirethatL≥1forthistobemeaningful.BeforeenteringtheinnerlooptheentriesofthemapMareexpandedand,usingthelineartimeselectionalgorithm,thetopLentriesareselected.Becauseeachstatethatgetsexpandedintheinnerloophasout-degree2,thenewstatemapNwillcontainatmost2Lstates.Thismeansthatwehaveanadditionalloopinvariant:thesizeofMisalwayslessthanorequalto2L.There-foretheselectionalgorithmrunsintimeO(L),andsodoestheabridgedinnerloop,aswellasthesec-ondouterloop.TheoverallruntimeofthismodiﬁedalgorithmisthereforeO(nL).IfLisaconstantfunction,theinexactcomputationoftheexpectedF-scorerunsinlineartimeandtheoveralldecodingalgorithminquadratictime.Inpar-ticularifL=1theapproximateexpectedF-scoreisequaltotheF-scoreoftheMAPhypothesis,andthemodiﬁedinferencealgorithmreducestoavariantofViterbidecoding.IfLisalinearfunctionofn,theoveralldecodingalgorithmrunsincubictime.Weexperimentallycomparedtheexactquartic-timedecodingalgorithmwiththeapproximatedecod-ingalgorithmforL=2nandforL=1.WecomputedtheabsolutedifferencebetweentheexpectedF-scoreoftheoptimalhypothesis(asfoundbytheexactal-gorithm)andtheexpectedF-scoreofthewinninghypothesisfoundbytheapproximatedecodingalgo-rithm.Fordifferentsequencelengthsn∈{1,...,50}weperformed10runsofthedifferentdecodingal-gorithmsonrandomlygeneratedprobabilityvectorsp,whereeachpiwasrandomlydrawnfromacontin-uousuniformdistributionon(0,1),or,inasecondexperiment,fromaBeta(1/2,1/2)distribution(tosimulateanover-trainedclassiﬁer).ForL=1thereisasubstantialdifferenceofaboutprecedingruninjustoneposition.Thismeansthatthemapdata-structuresonlyneedtoberecomputedfromthatpositionforward.However,thisdoesnotleadtoanasymptoticallyfasteralgorithmintheworstcase.5Forerrorbounds,seetheproof-of-conceptimplementation.742

0.6betweentheexpectedF-scoresofthewinninghypothesiscomputedbytheexactalgorithmandbytheapproximatealgorithm.Neverthelesstheapprox-imatedecodingalgorithmfoundtheoptimalhypoth-esismorethan99%ofthetime.Thisispresumablyduetotheadditionalregularizationinherentinthediscretemaximizationofthedecoderproper:eventhoughthecomputedexpectedF-scoresmaybefarfromtheirexactvalues,thisdoesnotnecessarilyaf-fectthebehaviorofthedecoderverymuch,sinceitonlyneedstoﬁndthemaximumamongasmallnum-berofsuchscores.Theerrorintroducedbytheap-proximationwouldhavetobelargeenoughtodisturbtheorderofthehypothesesexaminedbythedecoderinsuchawaythatthetruemaximumisreordered.Thisgenerallydoesnotseemtohappen.ForL=2nthecomputedapproximateexpectedF-scoreswereindistinguishablefromtheirexactvalues.Consequentlytheapproximatedecoderfoundthetruemaximumeverytime.5ConclusionandRelatedWorkWehavepresentedefﬁcientalgorithmsformaximumexpectedF-scoredecoding.Ourexactalgorithmrunsinquartictime,butanapproximatecubic-timevariantisindistinguishableinpractice.Aquadratic-timeapproximationmakesveryfewmistakesandremainspracticallyuseful.Wehavefurtherdescribedageneralframeworkforcomputingtheexpectationsofcertainloss/utilityfunctions.Ourmethodreliesonthefactthatmanyfunctionsaresparse,inthesenseofhavingaﬁniterangethatismuchsmallerthantheircodomain.Toevaluatetheirexpectations,wecanusethesimplefunctiontrickandconcentrateontheirlevelsets:itsufﬁcestoevaluatetheprobabilityofthosesets/events.Thefactthatthecommonlyusedutilityfunc-tionsliketheF-scorehaveonlypolynomiallymanylevelsetsissufﬁcient(butnotnecessary)toensurethatourmethodisefﬁcient.Becausethecoefﬁcientsakcanbearbitrary(infact,theycanbegeneralizedtobeelementsofavectorspaceoverthereals),wecandealwithfunctionsthatgobeyondsimplecounts.LikethemethodsdevelopedbyAllauzenetal.(2003)andCortesetal.(2003)ourtechniqueincor-poratesﬁniteautomata,butusesadirectthreshold-countingtechnique,ratherthananondeterministiccountingtechniquewhichreliesonpathmultiplici-ties.Thismakesiteasytoformulatethesimultaneouscountingoftwodistinctquantities,suchasourAandT,andtoreasonabouttheresultingautomata.ThemethoddescribedhereissimilarinspirittothoseofGaoetal.(2006)andJansche(2005),whodiscussmaximumexpectedF-scoretrainingofdeci-siontreesandlogisticregressionmodels.However,thepresentworkisconsiderablymoregeneralintwoways:(1)theexpectedutilitycomputationspresentedherearenottiedinanywaytoparticularclassiﬁers,butcanbeusedwithlargeclassesofprobabilisticmodels;and(2)ourframeworkextendsbeyondthecomputationofF-scores,whichfalloutasaspecialcase,tootherlossandutilityfunctions,includingthePkscore.Moreimportantly,expectedF-scorecom-putationaspresentedherecanbeexact,ifdesired,whereasthecitedworksalwaysuseanapproximationtothequantitieswehavecalledAandT.AcknowledgementsMostofthisresearchwasconductedwhileIwasafﬁlatedwiththeCenterforComputationalLearningSystems,ColumbiaUni-versity.IwouldliketothankmycolleaguesatGoogle,inpartic-ularRyanMcDonald,aswellastwoanonymousreviewersforvaluablefeedback.ReferencesCyrilAllauzen,MehryarMohri,andBrianRoark.2003.Gen-eralizedalgorithmsforconstructinglanguagemodels.InProceedingsofthe41stAnnualMeetingoftheAssociationforComputationalLinguistics.DougBeeferman,AdamBerger,andJohnLafferty.1999.Sta-tisticalmodelsfortextsegmentation.MachineLearning,34(1–3):177–210.FranciscoCasacubertaandColindelaHiguera.2000.Computa-tionalcomplexityofproblemsonprobabilisticgrammarsandtransducers.In5thInternationalColloquiumonGrammaticalInference.CorinnaCortes,PatrickHaffner,andMehryarMohri.2003.Ra-tionalkernels.InAdvancesinNeuralInformationProcessingSystems,volume15.ShengGao,WenWu,Chin-HuiLee,andTai-SengChua.2006.Amaximalﬁgure-of-merit(MFoM)-learningapproachtoro-bustclassiﬁerdesignfortextcategorization.ACMTransac-tionsonInformationSystems,24(2):190–218.AlsoinICML2004.SamuelS.Gross,OlgaRussakovsky,ChuongB.Do,andSer-aﬁmBatzoglou.2007.Trainingconditionalrandomﬁeldsformaximumlabelwiseaccuracy.InAdvancesinNeuralInformationProcessingSystems,volume19.R.W.Hamming.1950.Errordetectinganderrorcorrectingcodes.TheBellSystemTechnicalJournal,26(2):147–160.MartinJansche.2005.MaximumexpectedF-measuretrainingoflogisticregressionmodels.InProceedingsofHumanLan-guageTechnologyConferenceandConferenceonEmpiricalMethodsinNaturalLanguageProcessing.743

KevinKnight.1999.Decodingcomplexityinword-replacementtranslationmodels.ComputationalLinguistics,25(4):607–615.MichaelC.Mozer,RobertDodier,MichaelD.Colagrosso,C´esarGuerra-Salcedo,andRichardWolniewicz.2001.Prod-dingtheROCcurve:Constrainedoptimizationofclassiﬁerperformance.InAdvancesinNeuralInformationProcessingSystems,volume14.DavidR.Musicant,VipinKumar,andAyselOzgur.2003.OptimizingF-measurewithsupportvectormachines.InProceedingsoftheSixteenthInternationalFloridaArtiﬁcialIntelligenceResearchSocietyConference.JunSuzuki,ErikMcDermott,andHidekiIsozaki.2006.Train-ingconditionalrandomﬁeldswithmultivariateevaluationmeasures.InProceedingsofthe21stInternationalConfer-enceonComputationalLinguisticsand44thAnnualMeetingoftheAssociationforComputationalLinguistics.C.J.vanRijsbergen.1974.Foundationofevaluation.JournalofDocumentation,30(4):365–373.AppendixAProofofTheorem1TheproofofTheorem1employsthefollowinglemma:Theorem2.Forﬁxednandp,lets,t∈Smforsomemwith1≤m<n.Furtherassumethatsandtdifferonlyintwobits,iandk,insuchawaythatsi=1,sk=0;ti=0,tk=1;andpi≥pk.ThenE[F(s,·)]≥E[F(t,·)].Proof.ExpresstheexpectedF-scoreE[F(s,·)]asasumandsplitthesummationintotwoparts:∑yF(s,y)Pr(y)=∑yyi=ykF(s,y)Pr(y)+∑yyi6=ykF(s,y)Pr(y).Ifyi=ykthenF(s,y)=F(t,y),forthreereasons:thenumberofonesinsandtisthesame(namelym)byassumption;yisconstant;andthenumberoftruepositivesisthesame,thatiss·y=t·y.Thelatterholdsbecausesandyagreeeverywhereexceptoniandk;ifyi=yk=0,thentherearenotruepositivesatiandk;andifyi=yk=1thensiisatruepositivebutskisnot,andconverselytkisbuttiisnot.Therefore∑yyi=ykF(s,y)Pr(y)=∑yyi=ykF(t,y)Pr(y).(9)Focusonthosesummandswhereyi6=yk.Speciﬁcallygroupthemintopairs(y,z)whereyandzareidenticalexceptthatyi=1andyk=0,butzi=0andzk=1.Inotherwords,thetwosummationsontheright-handsideofthefollowingequalityarecarriedoutinparallel:∑yyi6=ykF(s,y)Pr(y)=∑yyi=1yk=0F(s,y)Pr(y)+∑zzi=0zk=1F(s,z)Pr(z).Then,focusingonsﬁrst:F(s,y)Pr(y)+F(s,z)Pr(z)=(β+1)(A+1)m+βTPr(y)+(β+1)Am+βTPr(z)=[(A+1)pi(1−pk)+A(1−pi)pk](β+1)m+βTC=[pi+(pi+pk−2pipk)A−pipk](β+1)m+βTC=[pi+C0]C1,whereA=s·zisthenumberoftruepositivesbetweensandz(sandyhaveanadditionaltruepositiveatibyconstruction);T=y·y=z·zisthenumberofpositivelabelsinyandz(identicalbyassumption);andC=Pr(y)pi(1−pk)=Pr(z)(1−pi)pkistheprobabilityofyandzevaluatedonallpositionsexceptforiandk.Thisequalityholdsbecauseofthezeroth-orderMarkovassumption(5)imposedonPr(y).C0andC1areconstantsthatallowustofocusontheessentialaspects.Thesituationfortissimilar,exceptforthetruepositives:F(t,y)Pr(y)+F(t,z)Pr(z)=(β+1)Am+βTPr(y)+(β+1)(A+1)m+βTPr(z)=[Api(1−pk)+(A+1)(1−pi)pk](β+1)m+βTC=[pk+(pi+pk−2pipk)A−pipk](β+1)m+βTC=[pk+C0]C1whereallconstantshavethesamevaluesasabove.Butpi≥pkbyassumption,pk+C0≥0,andC1≥0,sowehaveF(s,y)Pr(y)+F(s,z)Pr(z)=[pi+C0]C1≥F(t,y)Pr(y)+F(t,z)Pr(z)=[pk+C0]C1,andtherefore∑yyi6=ykF(s,y)Pr(y)≥∑yyi6=ykF(t,y)Pr(y).(10)Thetheoremfollowsfromequality(9)andinequality(10).ProofofTheorem1:(∀s∈Sm)E[F(z(m),·)]≥E[F(s,·)].Observethatz(m)∈Smbydeﬁnition(seeSection2.3).Form=0andm=nthetheoremholdstriviallybecauseSmisasingletonset.Inthenontrivialcases,Theorem2isappliedrepeatedly.Thestringz(m)canbetransformedintoanyotherstrings∈Smbyrepeatedlyclearingamorelikelysetbitandsettingalesslikelyunsetbit.Inparticularthiscanbedoneasfollows:First,ﬁndtheindiceswherez(m)andsdisagree.Byconstructiontheremustbeanevennumberofsuchindices;indeedthereareequinumeroussetsni(cid:12)(cid:12)z(m)i=1∧si=0o≈nj(cid:12)(cid:12)z(m)j=0∧sj=1o.Thisholdsbecausethetotalnumberofonesisﬁxedandidenticalinz(m)ands,andsoisthetotalnumberofzeroes.Next,sortthoseindicesbynon-increasingprobabilityandrepresentthemasi1,...,ikandj1,...,jk.Lets0=z(m).Thenlets1beidenticaltos0exceptthatsi1=0andsj1=1.Forms2,...,skalongthesamelinesandobservethatsk=sbyconstruction.Bydeﬁnitionofz(m)itmustbethecasethatpir≥pjrforallr∈{1,...,k}.ThereforeTheorem2appliesateverystepalongthewayfromz(m)=s0tosk=s,andsotheexpectedutilityisnon-increasingalongthatpath.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 744–751,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

744

AFullyBayesianApproachtoUnsupervisedPart-of-SpeechTagging∗SharonGoldwaterDepartmentofLinguisticsStanfordUniversitysgwater@stanford.eduThomasL.GrifﬁthsDepartmentofPsychologyUCBerkeleytomgriffiths@berkeley.eduAbstractUnsupervisedlearningoflinguisticstructureisadifﬁcultproblem.Acommonapproachistodeﬁneagenerativemodelandmax-imizetheprobabilityofthehiddenstruc-turegiventheobserveddata.Typically,thisisdoneusingmaximum-likelihoodes-timation(MLE)ofthemodelparameters.Weshowusingpart-of-speechtaggingthatafullyBayesianapproachcangreatlyim-proveperformance.Ratherthanestimatingasinglesetofparameters,theBayesianap-proachintegratesoverallpossibleparame-tervalues.Thisdifferenceensuresthatthelearnedstructurewillhavehighprobabilityoverarangeofpossibleparameters,andper-mitstheuseofpriorsfavoringthesparsedistributionsthataretypicalofnaturallan-guage.OurmodelhasthestructureofastandardtrigramHMM,yetitsaccuracyisclosertothatofastate-of-the-artdiscrimi-nativemodel(SmithandEisner,2005),upto14percentagepointsbetterthanMLE.Weﬁndimprovementsbothwhentrainingfromdataalone,andusingataggingdictionary.1IntroductionUnsupervisedlearningoflinguisticstructureisadif-ﬁcultproblem.Recently,severalnewmodel-basedapproacheshaveimprovedperformanceonavari-etyoftasks(KleinandManning,2002;Smithand∗ThisworkwassupportedbygrantsNSF0631518andONRMURIN000140510388.WewouldalsoliketothankNoahSmithforprovidinguswithhisdatasets.Eisner,2005).Nearlyalloftheseapproacheshaveoneaspectincommon:thegoaloflearningistoidentifythesetofmodelparametersthatmaximizessomeobjectivefunction.Valuesforthehiddenvari-ablesinthemodelarethenchosenbasedonthelearnedparameterization.Here,weproposeadif-ferentapproachbasedonBayesianstatisticalprin-ciples:ratherthansearchingforanoptimalsetofparametervalues,weseektodirectlymaximizetheprobabilityofthehiddenvariablesgiventheob-serveddata,integratingoverallpossibleparame-tervalues.Usingpart-of-speech(POS)taggingasanexampleapplication,weshowthattheBayesianapproachprovideslargeperformanceimprovementsovermaximum-likelihoodestimation(MLE)forthesamemodelstructure.Twofactorscanexplaintheimprovement.First,integratingoverparameterval-uesleadstogreaterrobustnessinthechoiceoftagsequence,sinceitmusthavehighprobabilityoverarangeofparameters.Second,integrationpermitstheuseofpriorsfavoringsparsedistributions,whicharetypicalofnaturallanguage.Thesekindsofpri-orscanleadtodegeneratesolutionsiftheparametersareestimateddirectly.Beforedescribingourapproachinmoredetail,webrieﬂyreviewpreviousworkonunsupervisedPOStagging.Perhapsthemostwell-knownisthatofMerialdo(1994),whousedMLEtotrainatri-gramhiddenMarkovmodel(HMM).MorerecentworkhasshownthatimprovementscanbemadebymodifyingthebasicHMMstructure(BankoandMoore,2004),usingbettersmoothingtechniquesoraddedconstraints(WangandSchuurmans,2005),orusingadiscriminativemodelratherthananHMM745

(SmithandEisner,2005).Non-model-basedap-proacheshavealsobeenproposed(Brill(1995);seealsodiscussioninBankoandMoore(2004)).AllofthisworkisreallyPOSdisambiguation:learningisstronglyconstrainedbyadictionarylistingtheal-lowabletagsforeachwordinthetext.SmithandEisner(2005)alsopresentresultsusingadiluteddictionary,whereinfrequentwordsmayhaveanytag.HaghighiandKlein(2006)useasmalllistoflabeledprototypesandnodictionary.Adifferenttraditiontreatstheidentiﬁcationofsyntacticclassesasaknowledge-freeclusteringproblem.Distributionalclusteringanddimen-sionalityreductiontechniquesaretypicallyappliedwhenlinguisticallymeaningfulclassesaredesired(Sch¨utze,1995;Clark,2000;Finchetal.,1995);probabilisticmodelshavebeenusedtoﬁndclassesthatcanimprovesmoothingandreduceperplexity(Brownetal.,1992;SaulandPereira,1997).Unfor-tunately,duetoalackofstandardandinformativeevaluationtechniques,itisdifﬁculttocomparetheeffectivenessofdifferentclusteringmethods.Inthispaper,wehopetounifytheproblemsofPOSdisambiguationandsyntacticclusteringbypre-sentingresultsforconditionsrangingfromafulltagdictionarytonodictionaryatall.Weintroducetheuseofanewinformation-theoreticcriterion,varia-tionofinformation(Meilˇa,2002),whichcanbeusedtocompareagoldstandardclusteringtotheclus-teringinducedfromatagger’soutput,regardlessoftheclusterlabels.Wealsoevaluateusingtagac-curacywhenpossible.OursystemoutperformsanHMMtrainedwithMLEonbothmetricsinallcir-cumstancestested,oftenbyawidemargin.Itsac-curacyinsomecasesisclosetothatofSmithandEisner’s(2005)discriminativemodel.OurresultsshowthattheBayesianapproachisparticularlyuse-fulwhenlearningislessconstrained,eitherbecauselessevidenceisavailable(corpussizeissmall)orbecausethedictionarycontainslessinformation.Inthefollowingsection,wediscussthemotiva-tionforaBayesianapproachandpresentourmodelandsearchprocedure.Section3givesresultsillus-tratinghowtheparametersoftheprioraffectre-sults,andSection4describeshowtoinferagoodchoiceofparametersfromunlabeleddata.Section5presentsresultsforarangeofcorpussizesanddic-tionaryinformation,andSection6concludes.2ABayesianHMM2.1MotivationInmodel-basedapproachestounsupervisedlan-guagelearning,theproblemisformulatedintermsofidentifyinglatentstructurefromdata.Wede-ﬁneamodelwithparametersθ,someobservedvari-ablesw(thelinguisticinput),andsomelatentvari-ablest(thehiddenstructure).Thegoalistoas-signappropriatevaluestothelatentvariables.Stan-dardapproachesdosobyselectingvaluesforthemodelparameters,andthenchoosingthemostprob-ablevariableassignmentbasedonthoseparame-ters.Forexample,maximum-likelihoodestimation(MLE)seeksparametersˆθsuchthatˆθ=argmaxθP(w|θ),(1)whereP(w|θ)=PtP(w,t|θ).Sometimes,anon-uniformpriordistributionoverθisintroduced,inwhichcaseˆθisthemaximumaposteriori(MAP)solutionforθ:ˆθ=argmaxθP(w|θ)P(θ).(2)ThevaluesofthelatentvariablesarethentakentobethosethatmaximizeP(t|w,ˆθ).Incontrast,theBayesianapproachweadvocateinthispaperseekstoidentifyadistributionoverlatentvariablesdirectly,withouteverﬁxingparticularval-uesforthemodelparameters.Thedistributionoverlatentvariablesgiventheobserveddataisobtainedbyintegratingoverallpossiblevaluesofθ:P(t|w)=ZP(t|w,θ)P(θ|w)dθ.(3)Thisdistributioncanbeusedinvariousways,in-cludingchoosingtheMAPassignmenttothelatentvariables,orestimatingexpectedvaluesforthem.Toseewhyintegratingoverpossibleparametervaluescanbeusefulwheninducinglatentstructure,considerthefollowingexample.Wearegivenacoin,whichmaybebiased(t=1)orfair(t=0),eachwithprobability.5.Letθbetheprobabilityofheads.Ifthecoinisbiased,weassumeauniformdistributionoverθ,otherwiseθ=.5.Weobservew,theoutcomesof10coinﬂips,andwewishtode-terminewhetherthecoinisbiased(i.e.thevalueof746

t).Assumethatwehaveauniformprioronθ,withp(θ)=1forallθ∈[0,1].First,weapplythestan-dardmethodologyofﬁndingtheMAPestimateforθandthenselectingthevalueoftthatmaximizesP(t|w,ˆθ).Inthiscase,anelementarycalculationshowsthattheMAPestimateisˆθ=nH/10,wherenHisthenumberofheadsinw(likewise,nTisthenumberoftails).Consequently,P(t|w,ˆθ)favorst=1foranysequencethatdoesnotcontainexactlyﬁveheads,andassignsequalprobabilitytot=1andt=0foranysequencethatdoescontainexactlyﬁveheads—acounterintuitiveresult.Incontrast,usingsomestandardresultsinBayesiananalysiswecanshowthatapplyingEquation3yieldsP(t=1|w)=1/(cid:18)1+11!nH!nT!210(cid:19)(4)whichissigniﬁcantlylessthan.5whennH=5,andonlyfavorst=1forsequenceswherenH≥8ornH≤2.ThisintuitivelysensiblepredictionresultsfromthefactthattheBayesianapproachissensitivetotherobustnessofachoiceofttothevalueofθ,asillustratedinFigure1.EventhoughasequencewithnH=6yieldsaMAPestimateofˆθ=0.6(Figure1(a)),P(t=1|w,θ)isonlygreaterthan0.5forasmallrangeofθaroundˆθ(Figure1(b)),meaningthatthechoiceoft=1isnotveryrobusttovariationinθ.Incontrast,asequencewithnH=8favorst=1forawiderangeofθaroundˆθ.Byintegratingoverθ,Equation3takesintoaccounttheconsequencesofpossiblevariationinθ.Anotheradvantageofintegratingoverθisthatitpermitstheuseoflinguisticallyappropriatepri-ors.Inmanylinguisticmodels,includingHMMs,thedistributionsovervariablesaremultinomial.Foramultinomialwithparametersθ=(θ1,...,θK),anaturalchoiceofprioristheK-dimensionalDirich-letdistribution,whichisconjugatetothemultino-mial.1Forsimplicity,weinitiallyassumethatallKparameters(alsoknownashyperparameters)oftheDirichletdistributionareequaltoβ,i.e.theDirichletissymmetric.Thevalueofβdetermineswhichparametersθwillhavehighprobability:whenβ=1,allparametervaluesareequallylikely;whenβ>1,multinomialsthatareclosertouniformare1Apriorisconjugatetoadistributioniftheposteriorhasthesameformastheprior.00.10.20.30.40.50.60.70.80.91θ P( θ | w )  00.10.20.30.40.50.60.70.80.9100.51θ P( t = 1 | w, θ )   w = HHTHTTHHTH w = HHTHHHTHHH w = HHTHTTHHTH w = HHTHHHTHHH(a)(b)Figure1:TheBayesianapproachtoestimatingthevalueofalatentvariable,t,fromobserveddata,w,choosesavalueoftrobusttouncertaintyinθ.(a)Posteriordistributiononθgivenw.(b)Probabilitythatt=1givenwandθasafunctionofθ.preferred;andwhenβ<1,highprobabilityisas-signedtosparsemultinomials,whereoneormoreparametersareatornear0.Typically,linguisticstructuresarecharacterizedbysparsedistributions(e.g.,POStagsarefollowedwithhighprobabilitybyonlyafewothertags,andhavehighlyskewedoutputdistributions).Conse-quently,itmakessensetouseaDirichletpriorwithβ<1.However,asnotedbyJohnsonetal.(2007),thischoiceofβleadstodifﬁcultieswithMAPesti-mation.Forasequenceofdrawsx=(x1,...,xn)fromamultinomialdistributionθwithobservedcountsn1,...,nK,asymmetricDirichlet(β)prioroverθyieldstheMAPestimateθk=nk+β−1n+K(β−1).Whenβ≥1,standardMLEtechniquessuchasEMcanbeusedtoﬁndtheMAPestimatesimplybyadding“pseudocounts”ofsizeβ−1toeachoftheexpectedcountsnkateachiteration.However,whenβ<1,thevaluesofθthatsetoneormoreoftheθkequalto0canhaveinﬁnitelyhighposte-riorprobability,meaningthatMAPestimationcanyielddegeneratesolutions.If,insteadofestimatingθ,weintegrateoverallpossiblevalues,wenolongerencountersuchdifﬁculties.Instead,theprobabilitythatoutcomexitakesvaluekgivenpreviousout-comesx−i=(x1,...,xi−1)isP(k|x−i,β)=ZP(k|θ)P(θ|x−i,β)dθ=nk+βi−1+Kβ(5)747

wherenkisthenumberoftimeskoccurredinx−i.SeeMacKayandPeto(1995)foraderivation.2.2ModelDeﬁnitionOurmodelhasthestructureofastandardtrigramHMM,withtheadditionofsymmetricDirichletpri-orsoverthetransitionandoutputdistributions:ti|ti−1=t,ti−2=t′,τ(t,t′)∼Mult(τ(t,t′))wi|ti=t,ω(t)∼Mult(ω(t))τ(t,t′)|α∼Dirichlet(α)ω(t)|β∼Dirichlet(β)wheretiandwiaretheithtagandword.Weassumethatsentenceboundariesaremarkedwithadistin-guishedtag.ForamodelwithTpossibletags,eachofthetransitiondistributionsτ(t,t′)hasTcompo-nents,andeachoftheoutputdistributionsω(t)hasWtcomponents,whereWtisthenumberofwordtypesthatarepermissibleoutputsfortagt.Wewilluseτandωtorefertotheentiretransitionandout-putparametersets.Thismodelassumesthattheprioroverstatetransitionsisthesameforallhis-tories,andtheprioroveroutputdistributionsisthesameforallstates.WerelaxthelatterassumptioninSection4.Underthismodel,Equation5givesusP(ti|t−i,α)=n(ti−2,ti−1,ti)+αn(ti−2,ti−1)+Tα(6)P(wi|ti,t−i,w−i,β)=n(ti,wi)+βn(ti)+Wtiβ(7)wheren(ti−2,ti−1,ti)andn(ti,wi)arethenumberofoccurrencesofthetrigram(ti−2,ti−1,ti)andthetag-wordpair(ti,wi)inthei−1previouslygener-atedtagsandwords.Notethat,byintegratingouttheparametersτandω,weinducedependenciesbetweenthevariablesinthemodel.Theprobabil-ityofgeneratingaparticulartrigramtagsequence(likewise,output)dependsonthenumberoftimesthatsequence(output)hasbeengeneratedprevi-ously.Importantly,trigrams(andoutputs)remainexchangeable:theprobabilityofasetoftrigrams(outputs)isthesameregardlessoftheorderinwhichitwasgenerated.Thepropertyofexchangeabilityiscrucialtotheinferencealgorithmwedescribenext.2.3InferenceToperforminferenceinourmodel,weuseGibbssampling(GemanandGeman,1984),astochasticprocedurethatproducessamplesfromtheposteriordistributionP(t|w,α,β)∝P(w|t,β)P(t|α).Weinitializethetagsatrandom,theniterativelyresam-pleeachtagaccordingtoitsconditionaldistributiongiventhecurrentvaluesofallothertags.Exchange-abilityallowsustotreatthecurrentcountsoftheothertagtrigramsandoutputsas“previous”obser-vations.Theonlycomplicationisthatresamplingatagchangestheidentityofthreetrigramsatonce,andwemustaccountforthisincomputingitscondi-tionaldistribution.ThesamplingdistributionfortiisgiveninFigure2.InBayesianstatisticalinference,multiplesamplesfromtheposteriorareoftenusedinordertoobtainstatisticssuchastheexpectedvaluesofmodelvari-ables.ForPOStagging,estimatesbasedonmulti-plesamplesmightbeusefulifwewereinterestedin,forexample,theprobabilitythattwowordshavethesametag.However,computingsuchprobabilitiesacrossallpairsofwordsdoesnotnecessarilyleadtoaconsistentclustering,andtheresultwouldbedifﬁ-culttoevaluate.Usingasinglesamplemakesstan-dardevaluationmethodspossible,butyieldssub-optimalresultsbecausethevalueforeachtagissam-pledfromadistribution,andsometagswillbeas-signedlow-probabilityvalues.OursolutionistotreattheGibbssamplerasastochasticsearchpro-cedurewiththegoalofidentifyingtheMAPtagse-quence.Thiscanbedoneusingtempering(anneal-ing),whereatemperatureofφisequivalenttorais-ingtheprobabilitiesinthesamplingdistributiontothepowerof1φ.Asφapproaches0,evenasinglesamplewillprovideagoodMAPestimate.3FixedHyperparameterExperiments3.1MethodOurinitialexperimentsfollowinthetraditionbegunbyMerialdo(1994),usingatagdictionarytocon-strainthepossiblepartsofspeechallowedforeachword.(ThisalsoﬁxesWt,thenumberofpossiblewordsfortagt.)Thedictionarywasconstructedbylisting,foreachword,alltagsfoundforthatwordintheentireWSJtreebank.Fortheexperimentsinthissection,weuseda24,000-wordsubsetofthetree-748

P(ti|t−i,w,α,β)∝n(ti,wi)+βnti+Wtiβ·n(ti−2,ti−1,ti)+αn(ti−2,ti−1)+Tα·n(ti−1,ti,ti+1)+I(ti−2=ti−1=ti=ti+1)+αn(ti−1,ti)+I(ti−2=ti−1=ti)+Tα·n(ti,ti+1,ti+2)+I(ti−2=ti=ti+2,ti−1=ti+1)+I(ti−1=ti=ti+1=ti+2)+αn(ti,ti+1)+I(ti−2=ti,ti−1=ti+1)+I(ti−1=ti=ti+1)+TαFigure2:Conditionaldistributionforti.Here,t−ireferstothecurrentvaluesofalltagsexceptforti,I(.)isafunctionthattakesonthevalue1whenitsargumentistrueand0otherwise,andallcountsnxarewithrespecttothetagtrigramsandtag-wordpairsin(t−i,w−i).bankasourunlabeledtrainingcorpus.54.5%ofthetokensinthiscorpushaveatleasttwopossibletags,withtheaveragenumberoftagspertokenbeing2.3.Wevariedthevaluesofthehyperparametersαandβandevaluatedoveralltaggingaccuracy.Forcom-parisonwithourBayesianHMM(BHMM)inthisandfollowingsections,wealsopresentresultsfromtheViterbidecodingofanHMMtrainedusingMLEbyrunningEMtoconvergence(MLHMM).Wheredirectcomparisonispossible,welistthescoresre-portedbySmithandEisner(2005)fortheircondi-tionalrandomﬁeldmodeltrainedusingcontrastiveestimation(CRF/CE).2Forallexperiments,weranourGibbssamplingalgorithmfor20,000iterationsovertheentiredataset.Thealgorithmwasinitializedwitharandomtagassignmentandatemperatureof2,andthetemper-aturewasgraduallydecreasedto.08.Sinceourin-ferenceprocedureisstochastic,ourreportedresultsareanaverageover5independentruns.Resultsfromourmodelforarangeofhyperpa-rametersarepresentedinTable1.Withthebestchoiceofhyperparameters(α=.003,β=1),weachieveaveragetaggingaccuracyof86.8%.ThisfarsurpassestheMLHMMperformanceof74.5%,andisclosertothe90.1%accuracyofCRF/CEonthesamedatasetusingoracleparameterselection.Theeffectsofα,whichdeterminestheprobabil-2ResultsofCRF/CEdependonthesetoffeaturesusedandthecontrastneighborhood.Inallcases,welistthebestscorereportedforanycontrastneighborhoodusingtrigram(butnospelling)features.Toensurepropercomparison,allcorporausedinourexperimentsconsistofthesamerandomizedsetsofsentencesusedbySmithandEisner.Notethattrainingonsetsofcontiguoussentencesfromthebeginningofthetreebankcon-sistentlyimprovesourresults,oftenby1-2percentagepointsormore.MLHMMscoresshowlessdifferencebetweenrandom-izedandcontiguouscorpora.ValueValueofβofα.001.003.01.03.1.31.0.00185.085.786.186.086.286.586.6.00385.585.585.886.686.786.786.8.0185.385.585.685.986.486.486.2.0385.985.886.186.286.686.886.4.185.285.085.285.184.985.584.9.384.484.484.684.484.585.785.31.083.183.083.283.383.583.783.9Table1:PercentageofwordstaggedcorrectlybyBHMMasafunctionofthehyperparametersαandβ.Resultsareaveragedover5runsonthe24kcor-puswithfulltagdictionary.Standarddeviationsinmostcasesarelessthan.5.ityofthetransitiondistributions,arestrongerthantheeffectsofβ,whichdeterminestheprobabilityoftheoutputdistributions.Theoptimalvalueof.003forαreﬂectsthefactthatthetruetransitionprobabilitymatrixforthiscorpusisindeedsparse.Asαgrowslarger,themodelprefersmoreuniformtransitionprobabilities,whichcausesittoperformworse.Althoughthetrueoutputdistributionstendtobesparseaswell,thelevelofsparsenessdependsonthetag(considerfunctionwordsvs.contentwordsinparticular).Therefore,avalueofβthataccu-ratelyreﬂectsthemostprobableoutputdistributionsforsometagsmaybeapoorchoiceforothertags.Thisleadstothesmallereffectofβ,andsuggeststhatperformancemightbeimprovedbyselectingadifferentβforeachtag,aswedointhenextsection.Aﬁnalpointworthnotingisthatevenwhenα=β=1(i.e.,theDirichletpriorsexertnoinﬂu-ence)theBHMMstillperformsmuchbetterthantheMLHMM.Thisresultunderscorestheimportanceofintegratingovermodelparameters:theBHMMidentiﬁesasequenceoftagsthathavehighproba-749

bilityoverarangeofparametervalues,ratherthanchoosingtagsbasedonthesinglebestsetofpara-meters.TheimprovedresultsoftheBHMMdemon-stratethatselectingasequencethatisrobusttovari-ationsintheparametersleadstobetterperformance.4HyperparameterInferenceInourinitialexperiments,weexperimentedwithdif-ferentﬁxedvaluesofthehyperparametersandre-portedresultsbasedontheiroptimalvalues.How-ever,choosinghyperparametersinthiswayistime-consumingatbestandimpossibleatworst,ifthereisnogoldstandardavailable.Luckily,theBayesianapproachallowsustoautomaticallyselectvaluesforthehyperparametersbytreatingthemasaddi-tionalvariablesinthemodel.Weaugmentthemodelwithpriorsoverthehyperparameters(here,weas-sumeanimproperuniformprior),anduseasin-gleMetropolis-Hastingsupdate(Gilksetal.,1996)toresamplethevalueofeachhyperparameteraftereachiterationoftheGibbssampler.Informally,toupdatethevalueofhyperparameterα,wesampleaproposednewvalueα′fromanormaldistributionwithµ=αandσ=.1α.Theprobabilityofac-ceptingthenewvaluedependsontheratiobetweenP(t|w,α)andP(t|w,α′)andatermcorrectingfortheasymmetricproposaldistribution.Performinginferenceonthehyperparametersal-lowsustorelaxtheassumptionthateverytaghasthesameprioronitsoutputdistribution.Intheex-perimentsreportedinthefollowingsection,weusedtwodifferentversionsofourmodel.Theﬁrstver-sion(BHMM1)usesasinglevalueofβforallwordclasses(asabove);thesecondversion(BHMM2)usesaseparateβjforeachtagclassj.5InferredHyperparameterExperiments5.1VaryingcorpussizeInthissetofexperiments,weusedthefulltagdictio-nary(asabove),butperformedinferenceonthehy-perparameters.FollowingSmithandEisner(2005),wetrainedonfourdifferentcorpora,consistingoftheﬁrst12k,24k,48k,and96kwordsoftheWSJcorpus.Forallcorpora,thepercentageofambigu-oustokensis54%-55%andtheaveragenumberoftagspertokenis2.3.Table2showsresultsforthevariousmodelsandarandombaseline(averagedCorpussizeAccuracy12k24k48k96krandom64.864.664.664.6MLHMM71.374.576.778.3CRF/CE86.288.688.489.4BHMM185.885.283.685.0BHMM285.884.485.785.8σ<.7.2.6.2Table2:Percentageofwordstaggedcorrectlybythevariousmodelsondifferentsizedcorpora.BHMM1andBHMM2usehyperparameterinfer-ence;CRF/CEusesparameterselectionbasedonanunlabeleddevelopmentset.Standarddeviations(σ)fortheBHMMresultsfellbelowthoseshownforeachcorpussize.over5randomtagassignments).Hyperparameterinferenceleadstoslightlylowerscoresthanareob-tainedbyoraclehyperparameterselection,butbothversionsofBHMMarestillfarsuperiortoMLHMMforallcorpussizes.Notsurprisingly,theadvantagesofBHMMaremostpronouncedonthesmallestcor-pus:theeffectsofparameterintegrationandsensiblepriorsarestrongerwhenlessevidenceisavailablefromtheinput.Inthelimitascorpussizegoestoin-ﬁnity,theBHMMandMLHMMwillmakeidenticalpredictions.5.2VaryingdictionaryknowledgeInunsupervisedlearning,itisnotalwaysreasonabletoassumethatalargetagdictionaryisavailable.Todeterminetheeffectsofreducedorabsentdictionaryinformation,weranasetofexperimentsinspiredbythoseofSmithandEisner(2005).First,wecol-lapsedthesetof45treebanktagsontoasmallersetof17(thesamesetusedbySmithandEisner).Wecreatedafulltagdictionaryforthissetoftagsfromtheentiretreebank,andalsocreatedseveralreduceddictionaries.Eachreduceddictionarycontainsthetaginformationonlyforwordsthatappearatleastdtimesinthetrainingcorpus(the24kcorpus,fortheseexperiments).Allotherwordsarefullyam-biguousbetweenall17classes.Werantestswithd=1,2,3,5,10,and∞(i.e.,knowledge-freesyn-tacticclustering).Withstandardaccuracymeasures,itisdifﬁcultto750

ValueofdAccuracy123510∞random69.656.751.045.238.6MLHMM83.270.665.559.050.9CRF/CE90.477.071.7BHMM186.076.471.064.358.0BHMM287.379.665.059.249.7σ<.2.8.6.31.4VIrandom2.653.964.384.755.137.29MLHMM1.132.513.003.413.896.50BHMM11.092.442.823.193.474.30BHMM21.041.782.312.492.974.04σ<.02.03.04.03.07.17Corpusstats%ambig.49.061.366.370.975.8100tags/token1.94.45.56.88.317Table3:Percentageofwordstaggedcorrectlyandvariationofinformationbetweenclusteringsin-ducedbytheassignedandgoldstandardtagsastheamountofinformationinthedictionaryisvaried.Standarddeviations(σ)fortheBHMMresultsfellbelowthoseshownineachcolumn.Thepercentageofambiguoustokensandaveragenumberoftagspertokenforeachvalueofdisalsoshown.evaluatethequalityofasyntacticclusteringwhennodictionaryisused,sinceclusternamesareinter-changeable.Wethereforeintroduceanotherevalua-tionmeasurefortheseexperiments,adistancemet-riconclusteringsknownasvariationofinformation(Meilˇa,2002).Thevariationofinformation(VI)be-tweentwoclusteringsC(thegoldstandard)andC′(thefoundclustering)ofasetofdatapointsisasumoftheamountofinformationlostinmovingfromCtoC′,andtheamountthatmustbegained.Itisde-ﬁnedintermsofentropyHandmutualinformationI:VI(C,C′)=H(C)+H(C′)−2I(C,C′).Evenwhenaccuracycanbemeasured,VImaybemorein-formative:twodifferenttagassignmentsmayhavethesameaccuracybutdifferentVIwithrespecttothegoldstandardiftheerrorsinoneassignmentarelessconsistentthanthoseintheother.Table3givestheresultsforthissetofexperi-ments.OneorbothversionsofBHMMoutperformMLHMMintermsoftagaccuracyforallvaluesofd,althoughthedifferencesarenotasgreatasinear-lierexperiments.ThedifferencesinVIaremorestriking,particularlyastheamountofdictionaryin-formationisreduced.Whenambiguityisgreater,bothversionsofBHMMshowlessconfusionwithrespecttothetruetagsthandoesMLHMM,andBHMM2performsthebestinallcircumstances.TheconfusionmatricesinFigure3provideamoreintu-itivepictureoftheverydifferentsortsofclusteringsproducedbyMLHMMandBHMM2whennotagdictionaryisavailable.Similardifferencesholdtoalesserdegreewhenapartialdictionaryisprovided.WithMLHMM,differenttokensofthesamewordtypeareusuallyassignedtothesamecluster,buttypesareassignedtoclustersmoreorlessatran-dom,andallclustershaveapproximatelythesamenumberoftypes(542onaverage,withastandarddeviationof174).TheclustersfoundbyBHMM2tendtobemorecoherentandmorevariableinsize:inthe5runsofBHMM2,theaveragenumberoftypesperclusterrangedfrom436to465(i.e.,to-kensofthesamewordarespreadoverfewerclus-tersthaninMLHMM),withastandarddeviationbetween460and674.Determiners,prepositions,thepossessivemarker,andvariouskindsofpunc-tuationaremostlyclusteredcoherently.Nounsarespreadoverafewclusters,partlyduetoadistinctionfoundbetweencommonandpropernouns.Like-wise,modalverbsandthecopulaaremostlysep-aratedfromotherverbs.Errorsareoftensensible:adjectivesandnounsarefrequentlyconfused,asareverbsandadverbs.ThekindsofresultsproducedbyBHMM1andBHMM2aremoresimilartoeachotherthantotheresultsofMLHMM,butthedifferencesarestillinformative.RecallthatBHMM1learnsasinglevalueforβthatisusedforalloutputdistribu-tions,whileBHMM2learnsseparatehyperparame-tersforeachcluster.Thisleadstodifferenttreat-mentsofdifﬁcult-to-classifylow-frequencyitems.InBHMM1,theseitemstendtobespreadevenlyamongallclusters,sothatallclustershavesimi-larlysparseoutputdistributions.InBHMM2,thesystemcreatesoneortwoclustersconsistingen-tirelyofveryinfrequentitems,wherethepriorsontheseclustersstronglypreferuniformoutputs,andallotherclusterspreferextremelysparseoutputs(andaremorecoherentthaninBHMM1).ThisexplainsthedifferenceinVIbetweenthetwosys-tems,aswellasthehigheraccuracyofBHMM1ford≥3:thesingleβdiscouragesplacinglow-frequencyitemsintheirowncluster,sotheyaremorelikelytobeclusteredwithitemsthathavesim-751

1234567891011121314151617NINPUNCADJVDETPREPENDPUNCVBGCONJVBNADVTOWHPRTPOS LPUNCRPUNC (a) BHMM2Found TagsTrue Tags1234567891011121314151617NINPUNCADJVDETPREPENDPUNCVBGCONJVBNADVTOWHPRTPOS LPUNCRPUNC (b) MLHMMFound TagsTrue TagsFigure3:Confusionmatricesforthedictionary-freeclusteringsfoundby(a)BHMM2and(b)MLHMM.ilartransitionprobabilities.TheproblemofjunkclustersinBHMM2mightbealleviatedbyusinganon-uniformprioroverthehyperparameterstoen-couragesomedegreeofsparsityinallclusters.6ConclusionInthispaper,wehavedemonstratedthat,forastan-dardtrigramHMM,takingaBayesianapproachtoPOStaggingdramaticallyimprovesperformanceovermaximum-likelihoodestimation.Integratingoverpossibleparametervaluesleadstomorerobustsolutionsandallowstheuseofpriorsfavoringsparsedistributions.TheBayesianapproachisparticularlyhelpfulwhenlearningislessconstrained,eitherbe-causelessdataisavailableorbecausedictionaryinformationislimitedorabsent.Forknowledge-freeclustering,ourapproachcanalsobeextendedthroughtheuseofinﬁnitemodelssothatthenum-berofclustersneednotbespeciﬁedinadvance.WehopethatoursuccesswithPOStaggingwillinspirefurtherresearchintoBayesianmethodsforothernat-urallanguagelearningtasks.ReferencesM.BankoandR.Moore.2004.Astudyofunsupervisedpart-of-speechtagging.InProceedingsofCOLING’04.E.Brill.1995.Unsupervisedlearningofdisambiguationrulesforpartofspeechtagging.InProceedingsofthe3rdWork-shoponVeryLargeCorpora,pages1–13.P.Brown,V.DellaPietra,V.deSouza,J.Lai,andR.Mer-cer.1992.Class-basedn-grammodelsofnaturallanguage.ComputationalLinguistics,18:467–479.A.Clark.2000.Inducingsyntacticcategoriesbycontextdis-tributionclustering.InProceedingsoftheConferenceonNaturalLanguageLearning(CONLL).S.Finch,N.Chater,andM.Redington.1995.Acquiringsyn-tacticinformationfromdistributionalstatistics.InJ.InLevy,D.Bairaktaris,J.Bullinaria,andP.Cairns,editors,Connec-tionistModelsofMemoryandLanguage.UCLPress,Lon-don.S.GemanandD.Geman.1984.Stochasticrelaxation,GibbsdistributionsandtheBayesianrestorationofimages.IEEETransactionsonPatternAnalysisandMachineIntelligence,6:721–741.W.R.Gilks,S.Richardson,andD.J.Spiegelhalter,editors.1996.MarkovChainMonteCarloinPractice.ChapmanandHall,Suffolk.A.HaghighiandD.Klein.2006.Prototype-drivenlearningforsequencemodels.InProceedingsofHLT-NAACL.M.Johnson,T.Grifﬁths,andS.Goldwater.2007.BayesianinferenceforPCFGsviaMarkovchainMonteCarlo.D.KleinandC.Manning.2002.Agenerativeconstituent-contextmodelforimprovedgrammarinduction.InProceed-ingsoftheACL.D.MacKayandL.BaumanPeto.1995.AhierarchicalDirich-letlanguagemodel.NaturalLanguageEngineering,1:289–307.M.Meilˇa.2002.Comparingclusterings.TechnicalReport418,UniversityofWashingtonStatisticsDepartment.B.Merialdo.1994.TaggingEnglishtextwithaprobabilisticmodel.ComputationalLinguistics,20(2):155–172.L.SaulandF.Pereira.1997.Aggregateandmixed-ordermarkovmodelsforstatisticallanguageprocessing.InPro-ceedingsoftheSecondConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP).H.Sch¨utze.1995.Distributionalpart-of-speechtagging.InProceedingsoftheEuropeanChapteroftheAssociationforComputationalLinguistics(EACL).N.SmithandJ.Eisner.2005.Contrastiveestimation:Traininglog-linearmodelsonunlabeleddata.InProceedingsofACL.I.WangandD.Schuurmans.2005.Improvedestimationforunsupervisedpart-of-speechtagging.InProceedingsoftheIEEEInternationalConferenceonNaturalLanguageProcessingandKnowledgeEngineering(IEEENLP-KE).Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 752–759,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

752

ComputationallyEfﬁcientM-EstimationofLog-LinearStructureModels∗NoahA.SmithandDouglasL.VailandJohnD.LaffertySchoolofComputerScienceCarnegieMellonUniversityPittsburgh,PA15213USA{nasmith,dvail2,lafferty}@cs.cmu.eduAbstractWedescribeanewlossfunction,duetoJeonandLin(2006),forestimatingstructuredlog-linearmodelsonarbitraryfeatures.Thelossfunctioncanbeseenasa(generative)al-ternativetomaximumlikelihoodestimationwithaninterestinginformation-theoreticin-terpretation,anditisstatisticallyconsis-tent.Itissubstantiallyfasterthanmaximum(conditional)likelihoodestimationofcondi-tionalrandomﬁelds(Laffertyetal.,2001;anorderofmagnitudeormore).Wecom-pareitsperformanceandtrainingtimetoanHMM,aCRF,anMEMM,andpseudolike-lihoodonashallowparsingtask.Theseex-perimentshelpteaseapartthecontributionsofrichfeaturesanddiscriminativetraining,whichareshowntobemorethanadditive.1IntroductionLog-linearmodelsareaverypopulartoolinnaturallanguageprocessing,andareoftenlaudedforper-mittingtheuseof“arbitrary”and“correlated”fea-turesofthedatabyamodel.Usersoflog-linearmodelsknow,however,thatthisclaimrequiressomequaliﬁcation:anyfeatureispermittedinprinciple,buttraininglog-linearmodels(anddecodingunderthem)istractableonlywhenthemodel’sindepen-denceassumptionspermitefﬁcientinferenceproce-dures.Forexample,intheoriginalconditionalran-domﬁelds(Laffertyetal.,2001),featureswerecon-∗ThisworkwassupportedbyNSFgrantIIS-0427206andtheDARPACALOproject.Theauthorsaregratefulforfeed-backfromDavidSmithandfromthreeanonymousACLre-viewers,andhelpfuldiscussionswithCharlesSutton.ﬁnedtolocally-factoredindicatorsonlabelbigramsandlabelunigrams(withanyoftheobservation).Evenincaseswhereinferenceinlog-linearmod-elsistractable,itrequiresthecomputationofaparti-tionfunction.Moreformally,alog-linearmodelforrandomvariablesXandYoverX,Ydeﬁnes:pw(x,y)=ew>f(x,y)Px0,y0∈X×Yew>f(x0,y0)=ew>f(x,y)Z(w)(1)wheref:X×Y→Rmisthefeaturevector-functionandw∈Rmisaweightvectorthatparameterizesthemodel.InNLP,werarelytrainthismodelbymaximizinglikelihood,becausethepartitionfunc-tionZ(w)isexpensivetocomputeexactly.Z(w)canbeapproximated(e.g.,usingGibbssampling;Rosenfeld,1997).Inthispaper,weproposetheuseofanewlossfunctionthatiscomputationallyefﬁcientandstatis-ticallyconsistent(§2).Notably,repeatedinferenceisnotrequiredduringestimation.Thislossfunc-tioncanbeseenasacaseofM-estimation1thatwasoriginallydevelopedbyJeonandLin(2006)fornonparametricdensityestimation.Thispapergivesaninformation-theoreticmotivationthathelpseluci-datetheobjectivefunction(§3),showshowtoap-plythenewestimatortostructuredmodelsusedinNLP(§4),andcomparesittoastate-of-the-artnounphrasechunker(§5).Wediscussimplicationsandfuturedirectionsin§6.2LossFunctionAsbefore,letXbearandomvariableoverahigh-dimensionalspaceX,andsimilarlyYoverY.X1“M-estimation”isageneralizationofMLE(vanderVaart,1998);spacedoesnotpermitafulldiscussion.753

mightbethesetofallsentencesinalanguage,andYthesetofallPOStagsequencesorthesetofallparsetrees.Letq0bea“base”distributionthatisourﬁrstapproximationtothetruedistributionoverX×Y.HMMsandPCFGs,whilelessaccurateaspredictorsthantherich-featuredlog-linearmodelswedesire,mightbeusedtodeﬁneq0.Themodelweestimatewillhavetheformpw(x,y)∝q0(x,y)ew>f(x,y)(2)Noticethatpw(x,y)=0wheneverq0(x,y)=0.Itisthereforeimportantforq0tobesmooth,sincethesupportofpwisasubsetofthesupportofq0.NoticethatwehavenotwrittenthepartitionfunctionexplicitlyinEq.2;itwillneverneedtobecomputedduringestimationorinference.Theunnormalizeddistributionwillsufﬁceforallcomputation.Supposewehaveobservationshx1,x2,...,xniwithannotationshy1,...,yni.The(unregularized)lossfunction,duetoJeonandLin(2006),is2‘(w)=1nnXi=1e−w>f(xi,yi)+Xx,yq0(x,y)(cid:16)w>f(x,y)(cid:17)(3)=1nnXi=1e−w>f(xi,yi)+w>Xx,yq0(x,y)f(x,y)=1nnXi=1e−w>f(xi,yi)+w>Eq0(X,Y)[f(X,Y)]|{z}constant(w)Beforeexplainingthisobjective,wepointoutsomeattractivecomputationalproperties.Noticethatf(xi,yi)(foralli)andtheexpectationsofthefeaturevectorsunderq0areconstantwithrespecttow.ComputingthefunctioninEq.3,then,re-quiresnoinferenceandnodynamicprogramming,onlyO(nm)ﬂoating-pointoperations.3AnInterpretationHerewegiveanaccountofthelossfunctionasawayof“cleaningup”amediocremodel(q0).We2Wegiveonlythediscreteversionhere,becauseitismostrelevantforanACLaudience.Also,ourlinearfunctionw>f(xi,yi)isasimplecase;anotherkernel(forexample)couldbeused.showthatthisestimateaimstomodelapresumedperturbationthatcreatedq0,byminimizingtheKLdivergencebetweenq0andaperturbedversionofthesampledistribution˜p.ConsiderEq.2.Givenatrainingdataset,maxi-mizinglikelihoodunderthismodelmeansassumingthatthereissomew∗forwhichthetruedistribu-tionp∗(x,y)=pw∗(x,y).CarryingoutMLE,how-ever,wouldrequirecomputingthepartitionfunctionPx0,y0q0(x0,y0)ew>f(x0,y0),whichisingeneralin-tractable.RearrangingEq.2slightly,wehaveq0(x,y)∝p∗(x,y)e−w>f(x,y)(4)Ifq0isclosetothetruemodel,e−w>f(x,y)shouldbecloseto1andwclosetozero.Inthesequencemodelsetting,forexample,ifq0isanHMMthatex-plainsthedatawell,thentheadditionalfeaturesarenotnecessary(equivalently,theirweightsshouldbe0).Ifq0isimperfect,wemightwishtomakeitmorepowerfulbyaddingfeatures(e.g.,f),butq0nonethe-lessprovidesareasonable“startingpoint”fordeﬁn-ingourmodel.Soinsteadofmaximizinglikelihood,wewillmin-imizetheKLdivergencebetweenthetwosidesofEq.4.3DKL(q0(x,y)kp∗(x,y)e−w>f(x,y))(5)=Xx,yq0(x,y)logq0(x,y)p∗(x,y)e−w>f(x,y)(6)+Xx,yp∗(x,y)e−w>f(x,y)−Xx,yq0(x,y)=−H(q0)+Xx,yp∗(x,y)e−w>f(x,y)−1−Xx,yq0(x,y)log(cid:16)p∗(x,y)e−w>f(x,y)(cid:17)=constant(w)+Xx,yp∗(x,y)e−w>f(x,y)+Xx,yq0(x,y)(cid:16)w>f(x,y)(cid:17)3TheKLdivergencehereisgeneralizedforunnormalizeddistributions,followingO’Sullivan(1998):DKL(ukv)=Pj“ujlogujvj−uj+vj”whereuandvarenonnegativevectorsdeﬁningunnormal-izeddistributionsoverthesameeventspace.NotethatwhenPjuj=Pjvj=1,thisformulatakesonthemorefamiliarform,as−PjujandPjvjcancel.754

Ifwereplacep∗withtheempirical(sampled)dis-tribution˜p,minimizingtheaboveKLdivergenceisequivalenttominimizing‘(w)(Eq.3).Itmaybehelpfultothinkof−wastheparametersofaprocessthat“damage”thetruemodelp∗,producingq0,andtheestimationofwaslearningtoundothatdamage.Intheremainderofthepaper,weusethegeneralterm“M-estimation”torefertotheminimizationof‘(w)asawayoftrainingalog-linearmodel.4AlgorithmsforModelsofSequencesandTreesWediscussheresomeimplementationaspectsoftheapplicationofM-estimationtoNLPmodels.4.1Expectationsunderq0Thebasedistributionq0entersintoimplementationintwoplaces:Eq0(X,Y)[f(X,Y)]mustbecomputedfortraining,andq0(x,y)isafactorinthemodelusedindecoding.Ifq0isafamiliarstochasticgrammar,suchasanHMMoraPCFG,oranygenerativemodelfromwhichsamplingisstraightforward,itispossibletoestimatethefeatureexpectationsbysamplingfromthemodeldirectly;forsampleh(˜xi,˜yi)isi=1let:Eq0(X,Y)[fj(X,Y)]←1ssXi=1fj(˜xi,˜yi)(7)Ifthefeaturespaceissparseunderq0(likelyinmostsettings),thensmoothingmayberequired.Ifq0isanHMMoraPCFG,theexpectationvec-torcanbecomputedexactlybysolvingasystemofequations.Wewillseethatforthecommoncaseswherefeaturesarelocalsubstructures,inferenceisstraightforward.WebrieﬂydescribehowthiscanbedoneforabigramHMMandaPCFG.4.1.1ExpectationsunderanHMMLetSbethestatespaceofaﬁrst-orderHMM.Ifs=hs1,...,skiisastatesequenceandx=hx1,...,xkiisanobservedsequenceofemissions,then:q0(s,x)= kYi=1tsi−1(si)esi(xi)!tsk(stop)(8)(Assumes0=startisthesingle,silent,initialstate,andstopistheonlystopstate,alsosilent.Weas-sumenootherstatesaresilent.)Theﬁrststepistocomputepath-sumsintoandoutofeachstate,undertheHMMq0.Todothis,deﬁneisasthetotalweightofstate-preﬁxes(beginninginstart)endinginsandosasthetotalweightofstate-sufﬁxesbeginningins(andendinginstop):4istart=ostop=1(9)∀s∈S\{start,stop}:is=∞Xn=1Xhs1,...,sni∈Sn nYi=1tsi−1(si)!tsn(s)=Xs0∈Sis0ts0(s)(10)os=∞Xn=1Xhs1,...,sni∈Snts(s1) nYi=2tsi−1(si)!=Xs0∈Sts(s0)os0(11)Thisamountstotwolinearsystemsgiventhetran-sitionprobabilitiest,wherethevariablesarei•ando•,respectively.Ineachsystemthereare|S|vari-ablesand|S|equations.Oncesolved,expectedcountsoftransitionandemissionfeaturesunderq0arestraightforward:Eq0[stransit→s0]=ists(s0)os0Eq0[semit→x]=ises(x)osGiveniando,Eq0canbecomputedforotherfea-turesinthemodelinasimilarway,providedtheycorrespondtocontiguoussubstructures.Forexam-ple,afeaturef627thatcountsoccurrencesof“Si=sandXi+3=x”hasexpectedvalueEq0[f627]=Xs0,s00,s000∈Sists(s0)ts0(s00)ts00(s000)es000(x)os000(12)Non-contiguoussubstructurefeatureswith“gaps”requiresummingoverpathsbetweenanypairofstates.Thisisstraightforward(weomititforspace),butofcourseusingsuchfeatures(whileinteresting)wouldcomplicateinferenceindecoding.4Itmaybehelpfultothinkofiasforwardprobabilities,butfortheobservationsetY∗ratherthanaparticularobservationy.oarelikebackwardprobabilities.Notethat,becausesomecountedpreﬁxesarepreﬁxesofothers,icanbe>1;similarlyforo.755

4.1.2ExpectationsunderaPCFGIngeneral,theexpectationsforaPCFGrequiresolvingaquadraticsystemofequations.Theanal-ogythistimeistoinsideandoutsideprobabilities.LetthePCFGhavenonterminalsetN,startsymbolS∈N,terminalalphabetΣ,andrulesoftheformA→BCandA→x.(WeassumeChomskynor-malformforclarity;thegeneralizationisstraight-forward.)LetrA(BC)andrA(x)denotetheproba-bilitiesofnonterminalArewritingtochildsequenceBCorx,respectively.Then∀A∈N:oA=XB∈NXC∈NoBiC[rB(AC)+rB(CA)]+(cid:26)1ifA=S0otherwiseiA=XB∈NXC∈NrA(BC)iBiC+XxrA(x)ixox=XA∈NoArA(x),∀x∈Σix=1,∀x∈ΣInmostpracticalapplications,thePCFGwillbe“tight”(BoothandThompson,1973;ChiandGe-man,1998).Informally,thismeansthattheproba-bilityofaderivationrootedinSfailingtoterminateiszero.Ifthatisthecase,theniA=1forallA∈N,andthesystembecomeslinear(seealsoCorazzaandSatta,2006).5Iftightnessisnotguaranteed,iterativepropagationofweights,followingStolcke(1995),workswellinourexperienceforsolvingthequadraticsystem,andconvergesquickly.AsintheHMMcase,expectedcountsofarbitrarycontiguoustreesubstructurescanbecomputedasproductsofprobabilitiesofrulesappearingwithinthestructure,factoringintheovalueofthestruc-ture’srootandtheivaluesofthestructure’sleaves.4.2OptimizationTocarryoutM-estimation,weminimizethefunc-tion‘(w)inEq.3.Toapplygradientde-scentoraquasi-Newtonnumericaloptimizationmethod,6itsufﬁcestospecifytheﬁxedquantities5ThesameistrueforHMMs:iftheprobabilityofnon-terminationiszero,thenforalls∈S,os=1.6WeuseL-BFGS(LiuandNocedal,1989)asimplementedintheRlanguage’soptimfunction.f(xi,yi)(foralli∈{1,2,...,n})andthevectorEq0(X,Y)[f(X,Y)].Thegradientis:7∂‘∂wj=−nXi=1e−w>f(xi,yi)fj(xi,yi)+Eq0[fj](13)TheHessian(matrixofsecondderivatives)canalsobecomputedwithrelativeease,thoughthespacere-quirementcouldbecomeprohibitive.Forproblemswheremisrelativelysmall,thiswouldallowtheuseofsecond-orderoptimizationmethodsthatarelikelytoconvergeinfeweriterations.ItiseasytoseethatEq.3isconvexinw.There-fore,convergencetoaglobaloptimumisguaranteedanddoesnotdependontheinitializingvalueofw.4.3RegularizationRegularizationisatechniquefrompatternrecogni-tionthataimstokeepparameters(likew)fromover-ﬁttingthetrainingdata.Itiscrucialtotheperfor-manceofmoststatisticallearningalgorithms,andourexperimentsshowithasamajoreffectonthesuccessoftheM-estimator.Hereweuseaquadraticregularizer,minimizing‘(w)+(w>w)/2c.Notethatthisisalsoconvexanddifferentiableifc>0.Thevalueofccanbechosenusingatuningdataset.Thisregularizeraimstokeepeachcoordinateofwclosetozero.IntheM-estimator,regularizationisparticularlyimportantwhentheexpectationofsomefeaturefj,Eq0(X,Y)[fj(X,Y)]isequaltozero.Thiscanhap-peneitherduetosamplingerror(fjsimplyfailedtoappearwithapositivevalueintheﬁnitesample)orbecauseq0assignszeroprobabilitymasstoanyx∈X,y∈Ywherefj(x,y)6=0.Withoutregular-ization,theweightwjwilltendtoward±∞,butthequadraticpenaltytermwillpreventthatundesirabletendency.Justastheadditionofaquadraticregular-izertolikelihoodcanbeinterpretedasazero-meanGaussianprioronw(ChenandRosenfeld,2000),itcanbeso-interpretedhere.Theregularizedobjectiveisanalogoustomaximumaposterioriestimation.5ShallowParsingWecomparedM-estimationtoahiddenMarkovmodelandothertrainingmethodsonEnglishnoun7Takingthelimitasn→∞andsettingequaltozero,wehavethebasisforaproofthat‘(w)isstatisticallyconsistent.756

HMMCRFMEMMPLM-est.2 sec.64:183:409:351:04Figure1:Walltime(hours:minutes)oftrainingtheHMMand100L-BFGSiterationsforeachoftheextended-featuremodelsona2.2GHzSunOpteronwith8GBRAM.Seediscussionintextfordetails.phrase(NP)chunking.ThedatasetcomesfromtheConferenceonNaturalLanguageLearning(CoNLL)2000shallowparsingsharedtask(TjongKimSangandBuchholz,2000);weapplythemodeltoNPchunkingonly.About900sentenceswerere-servedfortuningregularizationparameters.Baseline/q0Inthisexperiment,thesimplebase-lineisasecond-orderHMM.Thestatescorrespondto{B,I,O}labels,denotingthebeginning,inside,andoutsideofnounphrases.Eachstateemitsatagandaword(independentofeachothergiventhestate).WereplacedtheﬁrstoccurrenceofeverytagandofeverywordinthetrainingdatawithanOOVsymbol,givingaﬁxedtagvocabularyof46andaﬁxedwordvocabularyof9,014.Transitiondistribu-tionswereestimatedusingMLE,andtag-andword-emissiondistributionswereestimatedusingadd-1smoothing.TheHMMhad27,213parameters.ThisHMMachieves86.3%F1-measureonthedevelop-mentdataset(slightlybetterthanthelowest-scoringoftheCoNLL-2000systems).Heavierorweakersmoothing(anorderofmagnitudedifferenceinadd-λ)oftheemissiondistributionshadverylittleeffect.NotethatHMMtrainingtimeisnegligible(roughly2seconds);itrequirescountingevents,smoothingthecounts,andnormalizing.ExtendedFeatureSetShaandPereira(2003)ap-pliedaconditionalrandomﬁeldtotheNPchunk-ingtask,achievingexcellentresults.ToimprovetheperformanceoftheHMMandtestdifferentestima-tionmethods,weuseShaandPereira’sfeaturetem-plates,whichincludesubsequencesoflabels,tags,andwordsofdifferentlengthsandoffsets.Here,weuseonlyfeaturesobservedtooccuratleastonceinthetrainingdata,accounting(inadditiontoourOOVtreatment)fortheslightdropinperformanceprec.recallF1HMMfeatures:HMM85.6088.6887.11CRF90.4089.5689.98PL80.3181.3780.84MEMM86.0388.6287.31M-est.85.5788.6587.08extendedfeatures:CRF94.0493.6893.86PL91.8891.7991.83MEMM90.8992.1591.51M-est.88.8890.4289.64Table1:NPchunkingaccuracyontestdataus-ingdifferenttrainingmethods.Theeffectsofdis-criminativetraining(CRF)andextendedfeaturesets(lowersection)aremorethanadditive.comparedtowhatShaandPereirareport.Thereare630,862suchfeatures.UsingtheoriginalHMMfeaturesetandtheex-tendedfeatureset,wetrainedfourmodelsthatcanusearbitraryfeatures:conditionalrandomﬁelds(anear-replicationofShaandPereira,2003),maxi-mumentropyMarkovmodels(MEMMs;McCal-lumetal.,2000),pseudolikelihood(Besag,1975;seeToutanovaetal.,2003,forataggingapplica-tion),andourM-estimatorwiththeHMMasq0.CRFsandMEMMsarediscriminatively-trainedtomaximizeconditionallikelihood(theformerispa-rameterizedusingasequence-normalizedlog-linearmodel,thelatterusingalocally-normalizedlog-linearmodel).Pseudolikelihoodisaconsistentesti-matorforthejointlikelihood,likeourM-estimator;itsobjectivefunctionisasumoflogprobabilities.Ineachcase,wetrainedsevenmodelsforeachfeaturesetwithquadraticregularizersc∈[10−1,10],spacedatequalintervalsinthelog-scale,plusanunregularizedmodel(c=∞).Asdiscussedin§4.2,wetrainedusingL-BFGS;trainingcontin-ueduntilrelativeimprovementfellwithinmachineprecisionor100iterations,whichevercameﬁrst.Aftertraining,thevalueofcischosenthatmaxi-mizesF1accuracyonthetuningset.RuntimeFig.1comparesthewalltimeofcarefully-timedtrainingrunsonadedicatedserver.NotethatDyna,ahigh-levelprogramminglanguage,wasusedfordynamicprogramming(intheCRF)757

andsummations(MEMMandpseudolikelihood).TheruntimeoverheadincurredbyusingDynaises-timatedasaslow-downfactorof3–5againstahand-tunedimplementation(Eisneretal.,2005),thoughtheslow-downfactorisalmostcertainlylessfortheMEMMandpseudolikelihood.Alltraining(excepttheHMM,ofcourse)wasdoneusingtheRlanguageimplementationofL-BFGS.Inourimplementation,theM-estimatortrainedsubstantiallyfasterthantheothermethods.Ofthe64minutesrequiredtotraintheM-estimator,6minuteswerespentprecomput-ingEq0(X,Y)[f(X,Y)](thisneednotberepeatediftheregularizationsettingsarealtered).AccuracyTab.1showshowNPchunkingaccu-racycomparesamongthemodels.WithHMMfeatures,theM-estimatorisaboutthesameastheHMMandMEMM(betterthanPLandworsethantheCRF).Withextendedfeatures,theM-estimatorlagsbehindtheslowermethods,butperformsaboutthesameastheHMM-featuredCRF(2.5–3pointsovertheHMM).Thefull-featuredCRFimprovesperformancebyanother4points.PerformanceasafunctionoftrainingsetsizeisplottedinFig.2;thedifferentmethodsbehaverelativelysimilarlyasthetrainingdataarereduced.Fig.3plotsaccuracy(ontuningdata)againsttrainingtime,foravari-etyoftrainingdatasetsizesandregularizatonset-tings,underdifferenttrainingmethods.Thisillus-tratesthetraining-time/accuracytradeoff:theM-estimator,whenwell-regularized,isconsiderablyfasterthantheothermethods,attheexpenseofac-curacy.Thisexperimentgivessomeinsightintotherelativeimportanceofextendedfeaturesversuses-timationmethods.TheM-estimatedmodelis,likethemaximumlikelihood-estimatedHMM,agener-ativemodel.UnliketheHMM,itusesamuchlargersetoffeatures–thesamefeaturesthatthediscrimina-tivemodelsuse.Ourresultsupportstheclaimthatgoodfeaturesarenecessaryforstate-of-the-artper-formance,butsoisgoodtraining.5.1EffectoftheBaseDistributionWenowturntothequestionofthebasedistributionq0:howaccuratedoesitneedtobe?GiventhattheM-estimatorisconsistent,itshouldbeclearthat,inthelimitandassumingthatourmodelfamilypiscorrect,q0shouldnotmatter(exceptinitssupport).q0selectionprec.recallF1HMMF1,prec.88.8890.4289.64l.u.F172.9157.5664.33prec.84.4037.6852.10emp.F184.3889.4386.83Table2:NPchunkingaccuracyontestdatausingdifferentbasemodelsfortheM-estimator.The“se-lection”columnshowswhichaccuracymeasurewasoptimizedwhenselectingthehyperparameterc.InNLP,wedealwithﬁnitedatasetsandimperfectmodels,soq0mayhavepracticalimportance.Wenextconsideranalternativeq0thatisfarlesspowerful;infact,itisuninformativeaboutthevari-abletobepredicted.Letxbeasequenceofwords,tbeasequenceofpart-of-speechtags,andybeasequenceof{B,I,O}-labels.Themodelis:ql.u.0(x,t,y)def=|x|Yi=1puni(xi)puni(ti)1Nyi−11Ny|x|(14)whereNyisthenumberoflabels(includingstop)thatcanfollowy(3forOandy0=start,4forBandI).puniarethetagandwordunigramdistri-butions,estimatedusingMLEwithadd-1smooth-ing.Thismodelignorestemporaleffects.Onitsown,thismodelachieves0%precisionandrecall,becauseitlabelseverywordO(themostlikelylabelsequenceisO|x|).Wecallthismodell.u.(“locallyuniform”).Tab.2showsthat,whileanM-estimatethatusesql.u.0isnotnearlyasaccurateastheonebasedonanHMM,theM-estimatordidmanagetoimproveconsiderablyoverql.u.0.SotheM-estimatorisfarbetterthannothing,andinthiscase,tuningctomaximizeprecision(ratherthanF1)ledtoanM-estimatedmodelwithprecisioncompetitivewiththeHMM.Wepointthisoutbecause,inapplicationsin-volvingverylargecorpora,amodelwithgoodpreci-sionmaybeusefulevenifitscoverageismediocre.Anotherquestionaboutq0iswhetheritshouldtakeintoaccountallpossiblevaluesoftheinputvariables(here,xandt),oronlythoseseenintrain-ing.Considerthefollowingmodel:qemp0(x,t,y)def=q0(y|x,t)˜p(x,t)(15)Hereweusetheempiricaldistributionovertag/word758

7075808590951000200040006000800010000training set sizeF1CRFPLMEMMM-est.HMMFigure2:Learningcurvesfordifferentestimators;alloftheseestimatorsexcepttheHMMusetheex-tendedfeatureset.6570758085909510001101001000100001000001000000training time (seconds)F1M-est.CRFHMMPLMEMMFigure3:Accuracy(tuningdata)vs.trainingtime.TheM-estimatortrainsnotablyfaster.Thepointsinagivencurvecorrespondtodifferentregulariza-tionstrengths(c);M-estimationismoredamagedbyweakthanstrongregularization.sequences,andtheHMMtodeﬁnethedistri-butionoverlabelsequences.TheexpectationsEqemp0(X)[f(X)]canbecomputedusingdynamicprogrammingoverthetrainingdata(recallthatthisonlyneedstobedoneonce,cf.theCRF).Strictlyspeaking,qemp0assignsprobabilityzerotoanyse-quencenotseenintraining,butwecanignorethe˜pmarginalatdecodingtime.AsshowninTab.2,thismodelslightlyimprovesrecallovertheHMM,butdamagesprecision;thegainsofM-estimationseenwiththeHMMasq0,arenotreproduced.Fromtheseexperiments,weconcludethattheM-estimatormightperformconsiderablybetter,givenabetterq0.5.2Input-OnlyFeaturesWepresentbrieﬂyonenegativeresult.NotingthattheM-estimatorisamodelingtechniquethatesti-matesadistributionoverbothinputandoutputvari-ables(i.e.,agenerativemodel),wewantedawaytomaketheobjectivemorediscriminativewhilestillmaintainingthecomputationalpropertythatinfer-ence(ofanykind)notberequiredduringtheinnerloopofiterativetraining.Theideaistoreducethepredictiveburdenonthefeatureweightsforf.WhendesigningaCRF,featuresthatdonotdependontheoutputvariable(here,y)areunnecessary.Theycannotdistinguishbetweencompetinglabelingsforaninput,andsotheirweightswillbesettozeroduringconditionalestimation.ThefeaturevectorfunctioninShaandPereira’schunkingmodeldoesnotincludesuchfeatures.InM-estimation,however,addingsuch“input-only”featuresmightpermitbettermodelingofthedataand,moreimportantly,usetheorigi-nalfeaturesprimarilyforthediscriminativetaskofmodelingygiventheinput.Addingunigram,bigram,andtrigramfeaturestofforM-estimationresultedinaverysmallde-creaseinperformance:selectingforF1,thismodelachieves89.33F1ontestdata.6DiscussionM-estimationﬁllsagapintheplethoraoftrain-ingtechniquesthatareavailableforNLPmod-elstoday:itpermitsarbitraryfeatures(likeso-calledconditional“maximumentropy”modelssuchasCRFs)butestimatesagenerativemodel(permit-ting,amongotherthings,classiﬁcationoninputvari-ablesandmeaningfulcombinationwithothermod-els).Itissimilarinspirittopseudolikelihood(Be-sag,1975),towhichitcomparesfavorablyontrain-ingruntimeandunfavorablyonaccuracy.Further,sincenoinferenceisrequiredduringtraining,anyfeaturesreallyarepermitted,solongastheirexpectedvaluescanbeestimatedunderthebasemodelq0.Indeed,M-estimationisconsider-ablyeasiertoimplementthanconditionalestima-tion.Bothrequirefeaturecountsfromthetrain-ingdata;M-estimationreplacesrepeatedcalculationanddifferentiationofnormalizingconstantswithin-ferenceorsampling(once)underabasemodel.So759

theM-estimatorismuchfastertotrain.Generativeanddiscriminativemodelshavebeencomparedanddiscussedagreatdeal(NgandJordan,2002),includingforNLPmodels(Johnson,2001;KleinandManning,2002).SuttonandMcCallum(2005)presentapproximatemethodsthatkeepadis-criminativeobjectivewhileavoidingfullinference.WeseeM-estimationasaparticularlypromisingmethodinsettingswhereperformancedependsonhigh-dimensional,highly-correlatedfeaturespaces,wherethedesiredfeatures“large,”makingdiscrimi-nativetrainingtootime-consuming—acompellingexampleismachinetranslation.Further,insomesettingsalocally-normalizedconditionallog-linearmodel(likeanMEMM)maybedifﬁculttodesign;ourestimatoravoidsnormalizationaltogether.8TheM-estimatormayalsobeusefulasatoolindesign-ingandselectingfeaturecombinations,sincemoretrialscanberuninlesstime.Afterselectingafea-turesetunderM-estimation,discriminativetrainingcanbeappliedonthatset.TheM-estimatormightalsoserveasaninitializertodiscriminativemod-els,perhapsreducingthenumberoftimesinferencemustbeperformed—thiscouldbeparticularlyuse-fulinvery-largedatascenarios.InfutureworkwehopetoexploretheuseoftheM-estimatorwithinhiddenvariablelearning,suchastheExpectation-Maximizationalgorithm(Dempsteretal.,1977).7ConclusionsWehavepresentedanewlossfunctionforgenera-tivelyestimatingtheparametersoflog-linearmod-els.TheM-estimatorisfasttotrain,requiringnorepeated,expensivecalculationofnormalizationterms.Itwasshowntoimproveperformanceonashallowparsingtaskoverabaseline(generative)HMM,butitisnotcompetitivewiththestate-of-the-art.Oursequencemodelingexperimentssupportthewidelyacceptedclaimthatdiscriminative,rich-featuremodelingworksaswellasitdoesnotjustbecauseofrichfeaturesinthemodel,butalsobe-causeofdiscriminativetraining.Ourtechniqueﬁllsanimportantgapinthespectrumoflearningmeth-odsforNLPmodelsandshowspromiseforapplica-tionwhendiscriminativemethodsaretooexpensive.8NotethatMEMMsalsorequirelocalpartitionfunctions—whichmaybeexpensive—tobecomputedatdecodingtime.ReferencesJ.E.Besag.1975.Statisticalanalysisofnon-latticedata.TheStatistician,24:179–195.T.L.BoothandR.A.Thompson.1973.Applyingprobabil-itymeasurestoabstractlanguages.IEEETransactionsonComputers,22(5):442–450.S.ChenandR.Rosenfeld.2000.Asurveyofsmoothingtech-niquesforMEmodels.IEEETransactionsonSpeechandAudioProcessing,8(1):37–50.Z.ChiandS.Geman.1998.Estimationofprobabilis-ticcontext-freegrammars.ComputationalLinguistics,24(2):299–305.A.CorazzaandG.Satta.2006.Cross-entropyandestimationofprobabilisticcontext-freegrammars.InProc.ofHLT-NAACL.A.Dempster,N.Laird,andD.Rubin.1977.Maximumlikeli-hoodestimationfromincompletedataviatheEMalgorithm.JournaloftheRoyalStatisticalSocietyB,39:1–38.J.Eisner,E.Goldlust,andN.A.Smith.2005.CompilingCompLing:PracticalweighteddynamicprogrammingandtheDynalanguage.InProc.ofHLT-EMNLP.Y.JeonandY.Lin.2006.Aneffectivemethodforhigh-dimensionallog-densityANOVAestimation,withapplica-tiontononparametricgraphicalmodelbuilding.StatisticalSinica,16:353–374.M.Johnson.2001.Jointandconditionalestimationoftaggingandparsingmodels.InProc.ofACL.D.KleinandC.D.Manning.2002.Conditionalstructurevs.conditionalestimationinNLPmodels.InProc.ofEMNLP.J.Lafferty,A.McCallum,andF.Pereira.2001.Conditionalrandomﬁelds:Probabilisticmodelsforsegmentingandla-belingsequencedata.InProc.ofICML.D.C.LiuandJ.Nocedal.1989.OnthelimitedmemoryBFGSmethodforlargescaleoptimization.Math.Programming,45:503–528.A.McCallum,D.Freitag,andF.Pereira.2000.MaximumentropyMarkovmodelsforinformationextractionandseg-mentation.InProc.ofICML.A.NgandM.Jordan.2002.Ondiscriminativevs.generativeclassiﬁers:Acomparisonoflogisticregressionandna¨ıveBayes.InNIPS14.J.A.O’Sullivan.1998.Alternatingminimizationalgo-rithms:fromBlahut-ArmijotoExpectation-Maximization.InA.Vardy,editor,Codes,Curves,andSignals:CommonThreadsinCommunications,pages173–192.Kluwer.R.Rosenfeld.1997.Awholesentencemaximumentropylan-guagemodel.InProc.ofASRU.F.ShaandF.Pereira.2003.Shallowparsingwithconditionalrandomﬁelds.InProc.ofHLT-NAACL.A.Stolcke.1995.Anefﬁcientprobabilisticcontext-freepars-ingalgorithmthatcomputespreﬁxprobabilities.Computa-tionalLinguistics,21(2):165–201.C.SuttonandA.McCallum.2005.Piecewisetrainingofundi-rectedmodels.InProc.ofUAI.E.F.TjongKimSangandS.Buchholz.2000.IntroductiontotheCoNLL-2000sharedtask:Chunking.InProc.ofCoNLL.K.Toutanova,D.Klein,C.D.Manning,andY.Singer.2003.Feature-richpart-of-speechtaggingwithacyclicdepen-dencynetwork.InProc.ofHLT-NAACL.A.W.vanderVaart.1998.AsymptoticStatistics.CambridgeUniversityPress.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 760–767,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

760

GuidedLearningforBidirectionalSequenceClassiﬁcationLibinShenBBNTechnologiesCambridge,MA02138,USAlshen@bbn.comGiorgioSattaDept.ofInf.Eng’g.UniversityofPaduaI-35131Padova,Italysatta@dei.unipd.itAravindK.JoshiDepartmentofCISUniversityofPennsylvaniaPhiladelphia,PA19104,USAjoshi@seas.upenn.eduAbstractInthispaper,weproposeguidedlearning,anewlearningframeworkforbidirectionalsequenceclassiﬁcation.Thetasksoflearn-ingtheorderofinferenceandtrainingthelocalclassiﬁeraredynamicallyincorporatedintoasinglePerceptronlikelearningalgo-rithm.Weapplythisnovellearningalgo-rithmtoPOStagging.Itobtainsanerrorrateof2.67%onthestandardPTBtestset,whichrepresents3.3%relativeerrorreductionoverthepreviousbestresultonthesamedataset,whileusingfewerfeatures.1IntroductionManyNLPtaskscanbemodeledasasequenceclas-siﬁcationproblem,suchasPOStagging,chunking,andincrementalparsing.Atraditionalmethodtosolvethisproblemistodecomposethewholetaskintoasetofindividualtasksforeachtokeninthein-putsequence,andsolvethesesmalltasksinaﬁxedorder,usuallyfromlefttoright.Inthisway,theout-putoftheprevioussmalltaskscanbeusedastheinputofthelatertasks.HMMandMaxEntMarkovModelareexamplesofthismethod.Laffertyetal.(2001)showedthatthisapproachsufferedfromthesocalledlabelbiasproblem(Bot-tou,1991).TheyproposedConditionalRandomFields(CRF)asageneralsolutionforsequenceclas-siﬁcation.CRFmodelsasequenceasanundirectedgraph,whichmeansthatalltheindividualtasksaresolvedsimultaneously.Taskaretal.(2003)improvedtheCRFmethodbyemployingthelargemarginmethodtoseparatethegoldstandardsequencela-belingfromincorrectlabellings.However,thecom-plexityofquadraticprogrammingforthelargemar-ginapproachpreventeditfrombeingusedinlargescaleNLPtasks.Collins(2002)proposedaPerceptronlikelearn-ingalgorithmtosolvesequenceclassiﬁcationinthetraditionalleft-to-rightorder.Thissolutiondoesnotsufferfromthelabelbiasproblem.Comparedtotheundirectedmethods,thePerceptronlikealgorithmisfasterintraining.Inthispaper,wewillimproveuponCollins’algorithmbyintroducingabidirec-tionalsearchingstrategy,soastoeffectivelyutilizemorecontextinformationatlittleextracost.Whenabidirectionalstrategyisused,themainproblemishowtoselecttheorderofinference.Tsu-ruokaandTsujii(2005)proposedtheeasiest-ﬁrstap-proachwhichgreatlyreducedthecomputationcom-plexityofinferencewhilemaintainingtheaccuracyonlabeling.However,theeasiest-ﬁrstapproachonlyservesasaheuristicrule.TheorderofinferenceisnotincorporatedintothetrainingoftheMaxEntclas-siﬁerforindividuallabeling.Here,wewillproposeanovellearningframe-work,namelyguidedlearning,tointegrateclassiﬁ-cationofindividualtokensandinferenceorderselec-tionintoasinglelearningtask.WeproposedaPer-ceptronlikelearningalgorithm(CollinsandRoark,2004;Daum´eIIIandMarcu,2005)forguidedlearn-ing.WeapplythisalgorithmtoPOStagging,aclas-sicsequencelearningproblem.Oursystemreportsanerrorrateof2.67%onthestandardPTBtestset,arelative3.3%errorreductionofthepreviousbestsystem(Toutanovaetal.,2003)byusingfewerfea-tures.Byusingdeterministicsearch,itobtainsanerrorrateof2.73%,a5.9%relativeerrorreduction761

overthepreviousbestdeterministicalgorithm(Tsu-ruokaandTsujii,2005).ThenewPOStaggerissimilarto(Toutanovaetal.,2003;TsuruokaandTsujii,2005)inthewaythatweemploycontextfeatures.Weuseabidi-rectionalsearchstrategy(Woods,1976;SattaandStock,1994),andouralgorithmisbasedonPercep-tronlearning(Collins,2002).Auniquecontributionofourworkisontheintegrationofindividualclas-siﬁcationandinferenceorderselection,whicharelearnedsimultaneously.2GuidedLearningforBidirectionalLabelingWeﬁrstpresentanexampleofPOStaggingtoshowtheideaofbidirectionallabeling.Thenwepresenttheinferencealgorithmandthelearningalgorithm.2.1AnExampleofPOStaggingSupposethatwehaveaninputsentenceAgathafoundthatbookinterestingw1w2w3w4w5(Step0)Ifwescanfromlefttoright,wemayﬁnditdifﬁculttoresolvetheambiguityofthelabelforthat,whichcouldbeeitherDT(determiner),orIN(prepositionorsubordinatingconjunction)inthePennTreebank.However,ifweresolvethelabelsforbookandinteresting,itwouldberelativelyeasytoﬁgureoutthecorrectlabelforthat.Now,weshowhowbidirectionalinferenceworksonthissample.Supposeweusebeamsearchwithwidthof2,andweuseawindowof(-2,2)forcon-textfeatures.Fortheﬁrststep,weenumeratehypothesesforeachword.Forexample,foundcouldhavealabelVBNorVBD.Supposethatatthispointthemostfavorableaction,outofthecandidatehypotheses,istheassignmentofNNtobook,accordingtothecon-textfeaturesdeﬁnedonwords.Then,weresolvethelabelforbookﬁrst.Wemaintainthetoptwohy-pothesesasshownbelow.Here,thesecondmostfa-vorablelabelforbookisVB.NNVBAgathafoundthatbookinterestingw1w2w3w4w5(Step1)Atthesecondstep,assumethemostfavorableac-tionistheassignmentoflabelJJtointerestinginthecontextofNNforbook.Thenwemaintainthetoptwohypothesesforspanbookinterestingasshownbelow.ThesecondmostfavorablelabelforinterestingisstillJJ,butinthecontextofVBforbook.NN------JJVB------JJAgathafoundthatbookinterestingw1w2w3w4w5(Step2)Then,supposewearemostconﬁdentforassigninglabelsVBDandVBNtofound,inthatorder.Wegettwoseparatedtaggedspansasshownbelow.VBDNN------JJVBNVB------JJAgathafoundthatbookinterestingw1w2w3w4w5(Step3)Inthenextstep,supposewearemostconﬁdentforassigninglabelDTtothatunderthecontextofVBDontheleftandNN-JJontherightside,asshownbelow(secondmostfavorableaction,notdiscussedhere,isalsodisplayed).Aftertaggingw3,twosep-aratedspansmergeintoone,startingfromfoundtointeresting.VBD---DT---NN------JJVBD---IN---NN------JJAgathafoundthatbookinterestingw1w2w3w4w5(Step4)Forthelaststep,weassignlabelNNPtoAgatha,whichcouldbeanout-of-vocabularyword,underthecontextofVBD-DTontheright.NNP---VBD---DT---NN------JJNNP---VBD---IN---NN------JJAgathafoundthatbookinterestingw1w2w3w4w5(Step5)Thissimpleexamplehasshowntheadvantageofadoptingaﬂexiblesearchstrategy.However,itisstillunclearhowwemaintainthehypotheses,howwekeepcandidatesandacceptedlabelsandspans,andhowweemploydynamicprogramming.Wewillanswerthesequestionsintheformaldeﬁnitionoftheinferencealgorithminthenextsection.762

2.2InferenceAlgorithmTerminology:Lettheinputsequencebew1w2···wn.Foreachtokenwi,weareexpectedtoassignalabelti∈T,withTthelabelset.Asubsequencewi···wjiscalledaspan,andisdenoted[i,j].Eachspanpconsideredbytheal-gorithmisassociatedwithoneormorehypotheses,thatis,sequencesoverThavingthesamelengthasp.Partofthelabelsequenceofeachhypothesisisusedasacontextforlabelingtokensoutsidethespanp.Forexample,ifatri-grammodelisadopted,weusethetwolabelsontheleftboundaryandthetwolabelsontherightboundaryofthehypothesisforla-belingoutsidetokens.Thelefttwolabelsarecalledtheleftinterface,andtherighttwolabelsarecalledtherightinterface.Leftandrightinterfaceshaveonlyonelabelincaseofspansoflengthone.Apairs=(Ileft,Iright)withaleftandarightinterfaceiscalledastate.Wepartitionthehypothe-sesassociatedwithspanpintosetscompatiblewiththesamestate.Inpractice,forspanp,weuseama-trixMpindexedbystates,sothatMp(s),s=(Ileft,Iright),isthesetofallhypothesesassociatedwithpthatarecompatiblewithIleftandIright.Foraspanpandastates,wedenotetheassociatedtophypothesisass.T=argmaxh∈Mp(s)V(h),whereVisthescoreofahypothesis(deﬁnedin(1)below).Similarly,wedenotethetopstateforpasp.S=argmaxs:Mp(s)6=∅V(s.T).Therefore,foreachspanp,wehaveatophypothe-sisp.S.T,whosescoreisthehighestamongallthehypothesesforspanp.Hypothesesarestartedandgrownbymeansoflabelingactions.Foreachhypothesishassociatedwithaspanpwemaintainitsmostrecentlabelingactionh.A,involvingsometokenwithinp,aswellasthestatesh.SLandh.SRthathavebeenusedascontextbysuchanaction,ifany.Notethath.SLandh.SRrefertospansthataresubsequencesofp.WerecursivelycomputethescoreofhasV(h)=V(h.SL.T)+V(h.SR.T)+U(h.A),(1)Algorithm1InferenceAlgorithmRequire:tokensequencew1···wn;Require:beamwidthB;Require:weightvectorw;1:InitializeP,thesetofacceptedspans;2:InitializeQ,thequeueofcandidatespans;3:repeat4:spanp0←argmaxp∈QU(p.S.T.A);5:UpdatePwithp0;6:UpdateQwithp0andP;7:until(Q=∅)whereUisthescoreofanaction.Inotherwords,thescoreofanhypothesisisthesumofthescoreofthemostrecentactionh.Aandthescoresofthetophypothesesofthecontextstates.Thescoreofanactionh.Aiscomputedthroughalinearfunctionwhoseweightvectorisw,asU(h.A)=w·f(h.A),(2)wheref(h.A)isthefeaturevectorofactionh.A,whichdependsonh.SLandh.SR.Algorithm:Algorithm1istheinferencealgorithm.Wearegiventheinputsequenceandtwoparame-ters,beamwidthBtodeterminethenumberofstatesmaintainedforeachspan,andweightvectorwusedtocomputethescoreofanaction.WeﬁrstinitializethesetPofacceptedspanswiththeemptyset.ThenweinitializethequeueQofcandidatespanswithspan[i,i]foreachtokenwi,andforeacht∈TassignedtowiwesetM[i,i]((t,t))={i→t},wherei→trepresentsthehypothesisconsistingofasingleactionwhichassignslabelttowi.Thispro-videsthesetofstartinghypotheses.AsfortheexampleAgathafoundthatbookinterestingintheprevioussubsection,wehave•P=∅•Q={[1,1],[2,2],[3,3],[4,4],[5,5]}SupposeNNandVBarethetwopossiblePOStagsforw4book.Wehave•M[4,4](NN,NN)={h441=4→NN}•M[4,4](VB,VB)={h442=4→VB}Themostrecentactionofhypothesish441istoas-signNNtow4.AccordingtoEquation(2),thescore763

ofthisactionU(h441.A)dependsonthefeaturesde-ﬁnedonthelocalcontextofaction.Forexample,f1001(h441.A)=(cid:26)1ift=NN∧w−1=that0otherwise,wherew−1representstheleftword.Itshouldbenotedthat,forallthefeaturesdependingontheneighboringtags,thevalueisalways0,sincethosetagsarestillunknowninthestepofinitialization.Sincethisoperationdoesnotdependonsolvedtags,wehaveV(h441)=U(h411.A),accordingtoEqua-tion(1).Thecoreofthealgorithmrepeatedlyselectsacan-didatespanfromQ,andusesittoupdatePandQ,untilaspancoveringthewholesequenceisaddedtoPandQbecomesempty.Thisisexplainedindetailbelow.Ateachstep,weremovefromQthespanp0suchthattheaction(nothypothesis)scoreofitstophy-pothesis,p0.S.T,isthehighest.Thisrepresentsthelabelingactionforthenextmovethatwearemostconﬁdentabout.NowweneedtoupdatePandQwiththeselectedspanp0.Weaddp0toP,andre-movefromPthespansincludedinp0,ifany.LetSbethesetofremovedspans.WeremovefromQeachspanwhichtakesoneofthespansinSascon-text,andreplaceitwithanewcandidatespantakingp0(andanotheracceptedspan)ascontext.WealwaysmaintainBdifferentstatesforeachspan.Backtothepreviousexample,afterStep3iscom-pleted,w2found,w4bookandw5interestinghavebeentaggedandwehave•P={[2,2],[4,5]}•Q={[1,2],[2,5]}TherearetwocandidatespansinQ,eachwithitsas-sociatedhypothesesandmostrecentactions.Morespeciﬁcally,wecaneithersolvew1basedonthecon-texthypothesesfor[2,2],resultinginspan[1,2],orelsesolvew3basedonthecontexthypothesesin[2,2]and[4,5],resultinginspan[2,5].Thetoptwostatesforspan[2,2]are•M[2,2](VBD,VBD)={h221=2→VBD}•M[2,2](VBN,VBN)={h222=2→VBN}andthetoptwostatesforspan[4,5]are•M[4,5](NN-JJ,NN-JJ)={h451=(NN,NN)5→JJ}•M[4,5](VB-JJ,VB-JJ)={h452=(VB,VB)5→JJ}Here(NN,NN)5→JJrepresentsthehypothesiscomingfromtheactionofassigningJJtow5undertheleftcontextstateof(NN,NN).(VB,VB)5→JJhasasimilarmeaning.1WeﬁrstcomputethehypothesesresultingfromallpossiblePOStagassignmentstow3,underallpossi-blestatecombinationsoftheneighboringspans[2,2]and[4,5].SupposethehighestscoreactionconsistsintheassignmentofDTundertheleftcontextstate(VBD,VBD)andtherightcontextstate(NN-JJ,NN-JJ).Weobtainhypothesish251=(VBD,VBD)3→DT(NN-JJ,NN-JJ)withV(h251)=V((VBD,VBD).T)+V((NN-JJ,NN-JJ).T)+U(h251.A)=V(h221)+V(h451)+w·f(h251.A)Here,featuresforactionh251.AmaydependonthelefttagVBDandrighttagsNN-JJ,whichhavebeensolvedbefore.Moredetailsofthefeaturefunc-tionsaregiveninSection4.2.Forexample,wecanhavefeatureslikef2002(h251.A)=(cid:26)1ift=DT∧t+2=JJ0otherwise,Wemaintainthetoptwostateswiththehighesthypothesisscores,ifthebeamwidthissettotwo.Wehave•M[2,5](VBD-DT,NN-JJ)={h251=(VBD,VBD)3→DT(NN-JJ,NN-JJ)}•M[2,5](VBD-IN,NN-JJ)={h252=(VBD,VBD)3→IN(NN-JJ,NN-JJ)}Similarly,wecomputethetophypothesesandstatesforspan[1,2].Supposenowthehypothesiswiththehighestactionscoreish251.Thenweup-datePbyadding[2,5]andremoving[2,2]and[4,5],whicharecoveredby[2,5].WealsoupdateQbyre-moving[2,5]and[1,2],2andaddnewcandidatespan[1,5]resultingin•P={[2,5]}•Q={[1,5]}1Itshouldbenotedthat,inthesecases,eachstatecon-tainsonlyonehypothesis.However,ifthespanislongerthan4words,theremayexistmultiplehypothesesforthesamestate.Forexample,hypothesesDT-NN-VBD-DT-JJandDT-NN-VBN-DT-JJhavethesameleftinterfaceDT-NNandrightinterfaceDT-JJ.2Span[1,2]dependson[2,2]and[2,2]hasbeenremovedfromP.SoitisnolongeravalidcandidategiventheacceptedspansinP.764

Thealgorithmisespeciallydesignedinsuchawaythat,ateachstep,somenewspanisaddedtoPorelsesomespansalreadypresentinPareextendedbysometoken(s).Furthermore,nopairofoverlap-pingspansiseverfoundinP,andthenumberofpairsofoverlappingspansthatmaybefoundinQisalwaysboundedbyaconstant.Thismeansthatthealgorithmperformsatmostniterations,anditsrun-ningtimeisthereforeO(B2n),thatis,linearinthelengthoftheinputsequence.2.3LearningAlgorithmInthissection,weproposeguidedlearning,aPer-ceptronlikealgorithm,tolearntheweightvectorw,asshowninAlgorithm2.Weusep0.Gtorepresentthegoldstandardhypothesisonspanp0.ForeachinputsequenceXrandthegoldstandardsequenceoflabelingYr,weﬁrstinitializePandQasintheinferencealgorithm.ThenweselectthespanforthenextmoveasinAlgorithm1.Ifp0.S.T,thetophypothesisoftheselectedspanp0,iscom-patiblewiththegoldstandard,weupdatePandQasinAlgorithm1.Otherwise,weupdatetheweightvectorinthePerceptronstyle,bypromotingthefea-turesofthegoldstandardaction,anddemotingthefeaturesoftheactionofthetophypothesis.Thenwere-generatethequeueQwithPandtheupdatedweightvectorw.Speciﬁcally,weﬁrstremovealltheelementsinQ,andthengeneratehypothesesforallthepossiblespansbasedonthecontextspansinP.Hypothesisscoresandactionscoresarecalculatedwiththeupdatedweightvectorw.AspecialaspectofAlgorithm2isthatwemain-taintwoscores:thescoreoftheactionrepresentstheconﬁdenceforthenextmove,andthescoreofthehypothesisrepresentstheoverallqualityofapartialresult.Theselectionforthenextactiondirectlyde-pendsonthescoreoftheaction,butnotonthescoreofthehypothesis.Ontheotherhand,thescoreofthehypothesisisusedtomaintaintoppartialresultsforeachspan.WebrieﬂydescribethesoundnessoftheGuidedLearningAlgorithmintermsoftwoaspects.First,inAlgorithm2weightupdateisactivatedwheneverthereexistsanincorrectstates,theactionscoreofwhosetophypothesiss.Tishigherthanthatofanystateineachspan.Wedemotethisactionandpro-motethegoldstandardactiononthesamespan.Algorithm2GuidedLearningAlgorithmRequire:trainingsequencepairs{(Xr,Yr)}1≤r≤R;Require:beamwidthBanditerationsI;1:w←0;2:for(i←1;i≤I;i++)do3:for(r←1;r≤R;r++)do4:LoadsequenceXrandgoldlabelingYr.5:InitializeP,thesetofacceptedspans6:InitializeQ,thequeueofcandidatespans;7:repeat8:p0←argmaxp∈QU(p.S.T.A);9:if(p0.S.T=p0.G)then10:UpdatePwithp0;11:UpdateQwithp0andP;12:else13:promote(w,f(p0.G.A));14:demote(w,f(p0.S.T.A));15:Re-generateQwithwandP;16:endif17:until(Q=∅)18:endfor19:endforHowever,wedonotautomaticallyadoptthegoldstandardactiononthisspan.Instead,inthenextstep,thetophypothesisofanotherspanmightbese-lectedbasedonthescoreofaction,whichmeansthatitbecomesthemostfavorableactionaccordingtotheupdatedweights.Asasecondaspect,iftheactionscoreofagoldstandardhypothesisishigherthanthatofanyoth-ers,thishypothesisandthecorrespondingspanareguaranteedtobeselectedatline8ofAlgorithm2.Thereasonforthisisthatthescoresofthecontexthypothesesofagoldstandardhypothesismustbenolessthanthoseofotherhypothesesofthesamespan.ThiscouldbeshownrecursivelywithrespecttoEquation1,becausethecontexthypothesesofagoldstandardhypothesisarealsocompatiblewiththegoldstandard.Furthermore,ifwetake(xi=f(p0.G.A)−f(p0.S.T.A),yi=+1)asapositivesample,and(xj=f(p0.S.T.A)−f(p0.G.A),yj=−1)asanegativesample,theweightupdatesatlines13765

and14areastochasticapproximationofgradientde-scentthatminimizesthesquarederrorsofthemis-classiﬁedsamples(WidrowandHoff,1960).Whatisspecialwithourlearningalgorithmisthestrategyusedtoselectsamplesfortraining.Ingeneral,thisnovellearningframeworkliesbe-tweensupervisedlearningandreinforcementlearn-ing.Guidedlearningismoredifﬁcultthansuper-visedlearning,becausewedonotknowtheorderofinference.Theorderislearnedautomatically,andpartialoutputisinturnusedtotrainthelocalclas-siﬁer.Therefore,theorderofinferenceandthelo-calclassiﬁcationaredynamicallyincorporatedinthelearningphase.Guidedlearningisnotashardasreinforcementlearning.Ateachlocalstepinlearning,wealwaysknowtheundesirablelabelingactionsaccordingtothegoldstandard,althoughwedonotknowwhichisthemostdesirable.Inthisapproach,wecaneas-ilycollecttheautomaticallygeneratednegativesam-ples,andusetheminlearning.Thesenegativesam-plesareexactlythosewewillfaceduringinferencewiththecurrentweightvector.Inourexperiments,wehaveusedAveragedPer-ceptron(Collins,2002;FreundandSchapire,1999)andPerceptronwithmargin(KrauthandM´ezard,1987)toimproveperformance.3RelatedWorksTsuruokaandTsujii(2005)proposedabidirectionalPOStagger,inwhichtheorderofinferenceishan-dledwiththeeasiest-ﬁrstheuristic.Gim´enezandM`arquez(2004)combinedtheresultsofaleft-to-rightscanandaright-to-leftscan.Inourmodel,theorderofinferenceisdynamicallyincorporatedintothetrainingofthelocalclassiﬁer.Toutanovaetal.(2003)reportedaPOStaggerbasedoncyclicdependencynetwork.Intheirwork,theorderofinferenceisﬁxedasfromlefttoright.Inthisapproach,largebeamwidthisrequiredtomain-taintheambiguoushypotheses.Inourapproach,wecanhandletokensthatwearemostconﬁdentaboutﬁrst,sothatoursystemdoesnotneedalargebeam.AsshowninSection4.2,evendeterministicinfer-enceshowsrathergoodresults.OurguidedlearningcanbemodeledasasearchalgorithmwithPerceptronlikelearning(Daum´eIIIandMarcu,2005).However,asfarasweknow,DataSetSectionsSentencesTokensTraining0-1838,219912,344Develop19-215,527131,768Test22-245,462129,654Table1:Datasetsplitsthemechanismofbidirectionalsearchwithanon-linelearningalgorithmhasnotbeeninvestigatedbe-fore.In(Daum´eIIIandMarcu,2005),aswellasothersimilarworks(Collins,2002;CollinsandRoark,2004;ShenandJoshi,2005),onlyleft-to-rightsearchwasemployed.Ourguidedlearningal-gorithmprovidesmoreﬂexibilityinsearchwithanautomaticallylearnedorder.Inaddition,ourtreat-mentofthescoreofactionandthescoreofhypoth-esisisunique(seediscussioninSection2.3).Furthermore,comparedtotheaboveworks,ourguidedlearningalgorithmismoreaggressiveonlearning.In(CollinsandRoark,2004;ShenandJoshi,2005),asearchstopsifthereisnohypothe-siscompatiblewiththegoldstandardinthequeueofcandidates.In(Daum´eIIIandMarcu,2005),thesearchisresumedaftersomegoldstandardcompat-iblehypothesesareinsertedintoaqueueforfutureexpansion,andtheweightsareupdatedcorrespond-ingly.However,thereisnoguaranteethattheup-datedweightsassignahigherscoretothoseinsertedgoldstandardcompatiblehypotheses.Inouralgo-rithm,thegoldstandardcompatiblehypothesesareusedforweightupdateonly.Asaresult,aftereachsentenceisprocessed,theweightvectorcanusuallysuccessfullypredictthegoldstandardparse.There-foreourlearningalgorithmisaggressiveonweightupdate.Asfarasthisaspectisconcerned,ouralgorithmissimilartotheMIRAalgorithmin(CrammerandSinger,2003).InMIRA,onealwaysknowsthecor-recthypothesis.Inourcase,wedonotknowthecorrectorderofoperations.Soweuseourformofweightupdatetoimplementaggressivelearning.4ExperimentsonPOSTagging4.1SettingsWeapplyourguidedlearningalgorithmtoPOStag-ging.WecarryoutexperimentsonthestandarddatasetofthePennTreebank(PTB)(Marcusetal.,1994).Following(Ratnaparkhi,1996;Collins,2002;Toutanovaetal.,2003;TsuruokaandTsujii,2005),766

FeatureSetsTemplatesError%ARatnaparkhi’s3.05BA+[t0,t1],[t0,t−1,t1],[t0,t1,t2]2.92CB+[t0,t−2],[t0,t2],[t0,t−2,w0],[t0,t−1,w0],[t0,t1,w0],[t0,t2,w0],[t0,t−2,t−1,w0],[t0,t−1,t1,w0],[t0,t1,t2,w0]2.84DC+[t0,w−1,w0],[t0,w1,w0]2.78ED+[t0,X=preﬁxorsufﬁxofw0],4<|X|≤92.72Table2:Experimentsonthedevelopmentdatawithbeamwidthof3wecutthePTBintothetraining,developmentandtestsetsasshowninTable1.WeusetoolsprovidedbyCoNLL-20053toextractPOStagsfromthemrgﬁlesofPTB.Sothedatasetisthesameaspreviouswork.Weusethedevelopmentsettoselectfeaturesandestimatethenumberofiterationsintraining.Inourexperiments,weenumerateallthePOStagsforeachwordinsteadofusingadictionaryasin(Ratna-parkhi,1996),sincethesizeofthetagsetistractableandourlearningalgorithmisefﬁcientenough.4.2ResultsEffectofFeatures:Weﬁrstruntheexperimentstoevaluatetheeffectoffeatures.Weusetemplatestodeﬁnefeatures.Forthissetofexperiments,wesetthebeamwidthB=3asabalancebetweenspeedandaccuracy.Theguidedlearningalgorithmusuallyconvergesonthedevelopmentdatasetin4-8itera-tionsoverthetrainingdata.Table2showstheerrorrateonthedevelopmentsetwithdifferentfeatures.Weﬁrstusethesamefea-turesetusedin(Ratnaparkhi,1996),whichincludesasetofpreﬁx,sufﬁxandlexicalfeatures,aswellassomebi-gramandtri-gramcontextfeatures.Fol-lowing(Collins,2002),wedonotdistinguishrarewords.OnsetA,Ratnaparkhi’sfeatureset,oursys-temreportsanerrorrateof3.05%onthedevelop-mentdataset.WithsetB,weincludeafewfeaturetemplateswhicharesymmetrictothoseinRatnaparkhi’sset,butareonlyavailablewithbidirectionalsearch.WithsetC,weaddmorebi-gramandtri-gramfeatures.WithsetD,weincludebi-lexicalfeatures.WithsetE,weusepreﬁxesandsufﬁxesoflengthupto9,asin(Toutanovaetal.,2003;TsuruokaandTsujii,2005).Weobtain2.72%oferrorrate.Wewillusethisfea-turesetonourﬁnalexperimentsonthetestdata.EffectofSearchandLearningStrategies:Forthesecondsetofexperiments,weevaluatetheeffectof3http://www.lsi.upc.es/˜srlconll/soft.html,packagesrlconll-1.1.tgz.SearchAggressive?Beam=1Beam=3L-to-RYes2.942.82L-to-RNo3.242.75Bi-DirYes2.842.72Bi-DirNodoesnotconvergeTable3:Experimentsonthedevelopmentdatasearchmethods,learningstrategies,andbeamwidth.WeusefeaturesetEforthissetofexperiments.Ta-ble3showstheerrorratesonthedevelopmentdatasetwithbothleft-to-right(L-to-R)andbidirectional(Bi-Dir)searchmethods.Wealsotestedbothaggres-sivelearningandnon-aggressivelearningstrategieswithbeamwidthof1and3.First,withnon-aggressivelearningonbidirec-tionalsearch,theerrorratedoesnotconvergetoacomparablenumber.Thisisduetothefactthatthesearchspaceistoolargeinbidirectionalsearch,ifwedonotuseaggressivelearningtoconstrainthesamplesforlearning.Withaggressivelearning,thebidirectionalap-proachalwaysshowsadvantagesoverleft-to-rightsearch.However,thegapisnotlarge.ThisisduetothefactthattheaccuracyofPOStaggingisveryhigh.Asaresult,wecanalwayskeepthegold-standardtagsinthebeamevenwithleft-to-rightsearchintraining.Thiscanalsoexplainwhytheperformanceofleft-to-rightsearchwithnon-aggressivelearningisclosetobidirectionalsearchifthebeamislargeenough.However,withbeamwidth=1,non-aggressivelearningoverleft-to-rightsearchperformsmuchworse,becauseinthiscaseitismorelikelythatthegold-standardtagisnotinthebeam.Thissetofexperimentsshowthatguidedlearn-ingismorepreferablefortaskswithhigherambi-guities.Inourrecentwork(ShenandJoshi,2007),wehaveappliedavariantofthisalgorithmtodepen-dencyparsing,andshowedsigniﬁcantimprovementoverleft-to-rightnon-aggressivelearningstrategy.Comparison:Table4showsthecomparisonwiththepreviousworksonthePTBtestsections.767

SystemBeamError%(Ratnaparkhi,1996)53.37(TsuruokaandTsujii,2005)12.90(Collins,2002)-2.89GuidedLearning,featureB32.85(TsuruokaandTsujii,2005)all2.85(Gim´enezandM`arquez,2004)-2.84(Toutanovaetal.,2003)-2.76GuidedLearning,featureE12.73GuidedLearning,featureE32.67Table4:ComparisonwiththepreviousworksAccordingtotheexperimentsshownabove,webuildourbestsystembyusingfeaturesetEwithbeamwidthB=3.Thenumberofiterationsonthetrainingdataisestimatedwithrespecttothede-velopmentdata.Weobtainanerrorrateof2.67%onthetestdata.Withdeterministicsearch,orbeamwithB=1,weobtainanerrorrateof2.73%.Comparedtopreviousbestresultonthesamedataset,2.76%by(Toutanovaetal.,2003),ourbestre-sultshowsarelativeerrorreductionof3.3%.Thisresultisverypromising,sincewehavenotusedanyspeciallydesignedfeaturesinourexperiments.Itisreportedin(Toutanovaetal.,2003)thatacrudecom-panynamedetectorwasusedtogeneratefeatures,anditgaverisetosigniﬁcantimprovementinper-formance.However,itisdifﬁcultforustoduplicateexactlythesamefeatureforthepurposeofcompari-son,althoughitisconvenienttousefeatureslikethatinourframework.5ConclusionsInthispaper,weproposeguidedlearning,anewlearningframeworkforbidirectionalsequenceclas-siﬁcation.Thetasksoflearningtheorderofinfer-enceandtrainingthelocalclassiﬁeraredynamicallyincorporatedintoasinglePerceptronlikealgorithm.WeapplythisnovelalgorithmtoPOStagging.Itobtainsanerrorrateof2.67%onthestandardPTBtestset,whichrepresents3.3%relativeerrorreduc-tionoverthepreviousbestresult(Toutanovaetal.,2003)onthesamedataset,whileusingfewerfea-tures.Byusingdeterministicsearch,itobtainsanerrorrateof2.73%,a5.9%relativeerrorreductionoverthepreviousbestdeterministicalgorithm(Tsu-ruokaandTsujii,2005).Itshouldbenotedthattheerrorrateisclosetotheinter-annotatordiscrepancyonPTB,thestandardtestsetforPOStagging,there-foreitisverydifﬁculttoachieveimprovement.ReferencesL.Bottou.1991.Uneapprocheth´eoriquedel’apprentissageconnexionniste:Applications`alareconnaissancedelapa-role.Ph.D.thesis,Universit´edeParisXI.M.CollinsandB.Roark.2004.Incrementalparsingwiththeperceptronalgorithm.InACL-2004.M.Collins.2002.Discriminativetrainingmethodsforhiddenmarkovmodels:Theoryandexperimentswithperceptronal-gorithms.InEMNLP-2002.K.CrammerandY.Singer.2003.Ultraconservativeonlinealgorithmsformulticlassproblems.JournalofMachineLearningResearch,3:951–991.H.Daum´eIIIandD.Marcu.2005.Learningassearchopti-mization:Approximatelargemarginmethodsforstructuredprediction.InICML-2005.Y.FreundandR.E.Schapire.1999.Largemarginclassiﬁ-cationusingtheperceptronalgorithm.MachineLearning,37(3):277–296.J.Gim´enezandL.M`arquez.2004.Svmtool:Ageneralpostag-gergeneratorbasedonsupportvectormachines.InLREC-2004.W.KrauthandM.M´ezard.1987.Learningalgorithmswithoptimalstabilityinneuralnetworks.JournalofPhysicsA,20:745–752.J.Lafferty,A.McCallum,andF.Pereira.2001.Conditionalrandomﬁelds:Probabilisticmodelsforsegmentationandla-belingsequencedata.InICML-2001.M.P.Marcus,B.Santorini,andM.A.Marcinkiewicz.1994.BuildingalargeannotatedcorpusofEnglish:ThePennTree-bank.ComputationalLinguistics,19(2):313–330.A.Ratnaparkhi.1996.Amaximumentropypart-of-speechtag-ger.InEMNLP-1996.G.SattaandO.Stock.1994.Bi-DirectionalContext-FreeGrammarParsingforNaturalLanguageProcessing.Artiﬁ-cialIntelligence,69(1-2).L.ShenandA.K.Joshi.2005.IncrementalLTAGParsing.InEMNLP-2005.L.ShenandA.K.Joshi.2007.BidirectionalLTAGDepen-dencyParsing.TechnicalReport07-02,IRCS,UPenn.B.Taskar,C.Guestrin,andD.Koller.2003.Max-marginmarkovnetworks.InNIPS-2003.K.Toutanova,D.Klein,C.Manning,andY.Singer.2003.Feature-richpart-of-speechtaggingwithacyclicdependencynetwork.InNAACL-2003.Y.TsuruokaandJ.Tsujii.2005.Bidirectionalinferencewiththeeasiest-ﬁrststrategyfortaggingsequencedata.InEMNLP-2005.B.WidrowandM.E.Hoff.1960.Adaptiveswitchingcircuits.IREWESCONConventionRecord,part4.W.Woods.1976.Parsersinspeechunderstandingsystems.TechnicalReport3438,Vol.4,1–21,BBNInc.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 768–775,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

768

DifferentStructuresforEvaluatingAnswerstoComplexQuestions:PyramidsWon’tTopple,andNeitherWillHumanAssessorsHoaTrangDangInformationAccessDivisionNationalInstituteofStandardsandTechnologyGaithersburg,MD20899hoa.dang@nist.govJimmyLinCollegeofInformationStudiesUniversityofMarylandCollegePark,MD20742jimmylin@umd.eduAbstractTheideaof“nuggetpyramids”hasre-centlybeenintroducedasareﬁnementtothenugget-basedmethodologyusedtoevaluateanswerstocomplexquestionsintheTRECQAtracks.Thispaperexaminesdatafromthe2006evaluation,theﬁrstlarge-scalede-ploymentofthenuggetpyramidsscheme.Weshowthatthismethodofcombiningjudgmentsofnuggetimportancefrommulti-pleassessorsincreasesthestabilityanddis-criminativepoweroftheevaluationwhilein-troducingonlyasmalladditionalburdenintermsofmanualassessment.Wealsocon-sideranalternativemethodforcombiningassessoropinions,whichyieldsadistinctionsimilartomicro-andmacro-averaginginthecontextofclassiﬁcationtasks.Whilethetwoapproachesdifferintermsofunderly-ingassumptions,theirresultsareneverthe-lesshighlycorrelated.1IntroductionTheemergenceofquestionanswering(QA)systemsforaddressingcomplexinformationneedshasne-cessitatedthedevelopmentandreﬁnementofnewmethodologiesforevaluatingandcomparingsys-tems.IntheTextREtrievalConference(TREC)QAtracksorganizedbytheU.S.NationalInstituteofStandardsandTechnology(NIST),improvementsinevaluationprocesseshavekeptpacewiththeevolu-tionofQAtasks.Forthepastseveralyears,NISThasimplementedanevaluationmethodologybasedonthenotionof“informationnuggets”toassessan-swerstocomplexquestions.Asithasbecomethedefactostandardforevaluatingsuchsystems,theresearchcommunitystandstobeneﬁtfromabetterunderstandingofthecharacteristicsofthisevalua-tionmethodology.Thispaperexploresrecentreﬁnementstothenugget-basedevaluationmethodologydevelopedbyNIST.Inparticular,weexaminetherecentso-called“pyramidextension”thatincorporatesrelevancejudgmentsfrommultipleassessorstoimproveeval-uationstability(LinandDemner-Fushman,2006).Weorganizeourdiscussionasfollows:Thenextsectionbeginsbyprovidingabriefoverviewofnugget-basedevaluationsandthepyramidexten-sion.Section3presentsresultsfromtheﬁrstlarge-scaleimplementationofnuggetpyramidsforQAevaluationinTREC2006.Analysisshowsthatthisextensionimprovesbothstabilityanddiscriminativepower.InSection4,wediscussanalternativeforcombiningmultiplejudgmentsthatparallelsthedis-tinctionbetweenmicro-andmacro-averagingoftenseeninclassiﬁcationtasks.Experimentsrevealthatthemethodsyieldalmostexactlythesameresults,despiteoperatingondifferentgranularities(individ-ualnuggetsvs.individualusers).2EvaluatingComplexQuestionsComplexquestionsaredistinguishedfromfactoidquestionssuchas“WhoshotAbrahamLincoln?”inthattheycannotbeansweredbynamedentities(e.g.,persons,organizations,dates,etc.).Typically,theseinformationneedsareembeddedinthecontextofascenario(i.e.,usertask)andoftenrequiresystemsto769

synthesizeinformationfrommultipledocumentsortogenerateanswersthatcannotbeeasilyextracted(e.g.,byleveraginginferencecapabilities).Todate,NISThasalreadyconductedseverallarge-scaleevaluationsofcomplexquestions:def-initionquestionsinTREC2003,“Other”ques-tionsinTREC2004–2006,“relationship”questionsinTREC2005,andthecomplex,interactiveQA(ciQA)taskinTREC2006.DeﬁnitionandOtherquestionsaresimilarinthattheybothrequestnovelfactsabout“targets”,whichcanbepersons,orga-nizations,things,andevents.Relationshipques-tionsevolvedintotheciQAtaskandfocusonin-formationneedssuchas“Whatﬁnancialrelation-shipsexistbetweenSouthAmericandrugcartelsandbanksinLiechtenstein?”Suchcomplexquestionsfocusonties(ﬁnancial,military,familial,etc.)thatconnecttwoormoreentities.Alloftheseevalua-tionshaveemployedthenugget-basedmethodology,whichdemonstratesitsversatilityandapplicabilitytoawiderangeofinformationneeds.2.1BasicSetupIntheTRECQAevaluations,ananswertoacomplexquestionconsistsofanunorderedsetof[document-id,answerstring]pairs,wherethestringsarepresumedtoprovidesomerelevantinformationthataddressesthequestion.Althoughnoexplicitlimitisplacedonthelengthoftheanswer,theﬁnalmetricpenalizesverbosity(seebelow).Evaluationofsystemoutputproceedsintwosteps.First,answerstringsfromallsubmissionsaregatheredtogetherandpresentedtoasingleas-sessor.Thesourceofeachanswerstringisblindedsothattheassessorcannotobviouslytellwhichsystemsgeneratedwhatoutput.Usingthesean-swersandsearchesperformedduringquestionde-velopment,theassessorcreatesalistofrelevantnuggets.Anuggetisapieceofinformation(i.e.,“fact”)thataddressesoneaspectoftheuser’sques-tion.Nuggetsshouldbeatomic,inthesensethatanassessorshouldbeabletomakeabinaryde-cisionastowhetherthenuggetappearsinanan-swerstring.Althoughanuggetrepresentsacon-ceptualentity,theassessorprovidesanaturallan-guagedescription—primarilyasamemoryaidforthesubsequentevaluationsteps.Thesedescriptionsrangefromsentence-lengthdocumentextractstor=#ofvitalnuggetsreturneda=#ofokaynuggetsreturnedR=#ofvitalnuggetsintheanswerkeyl=#ofnon-whitespacecharactersinentirerunrecall:R=r/Rallowance:α=100×(r+a)precision:P=(1ifl<α1−l−αlotherwiseF(β)=(β2+1)×P×Rβ2×P+RFigure1:OfﬁcialdeﬁnitionofF-scorefornuggetevaluationinTREC.keyphrasestotelegraphicshort-handnotes—theirreadabilitygreatlyvariesfromassessortoassessor.Theassessoralsomanuallyclassiﬁeseachnuggetaseithervitalorokay(non-vital).Vitalnuggetsrep-resentconceptsthatmustbepresentina“good”an-swer.Okaynuggetsmaycontaininterestinginfor-mation,butarenotessential.Inthesecondstep,thesameassessorwhocre-atedthenuggetsreadseachsystem’soutputinturnandmarkstheappearanceofthenuggets.Anan-swerstringcontainsanuggetifthereisaconceptualmatch;thatis,thematchisindependentofthepartic-ularwordingusedinthesystem’soutput.Anuggetmatchismarkedatmostonceperrun—i.e.,asys-temisnotrewardedforretrievinganuggetmultipletimes.Ifthesystem’soutputcontainsmorethanonematchforanugget,thebestmatchisselectedandtherestareleftunmarked.Asingle[document-id,answerstring]pairinasystemresponsecanmatch0,1,ormultiplenuggets.TheﬁnalF-scoreforanansweriscalculatedinthemannerdescribedinFigure1,andtheﬁnalscoreofarunistheaverageacrosstheF-scoresofallques-tions.Themetricisaweightedharmonicmeanbe-tweennuggetprecisionandnuggetrecall,wherere-callisheavilyfavored(controlledbytheβparame-ter,usuallysettothree).Nuggetrecalliscalculatedsolelyonvitalnuggets,whilenuggetprecisionisap-proximatedbyalengthallowancebasedonthenum-berofbothvitalandokaynuggetsreturned.Inan770

earlierpilotstudy,researchersdiscoveredthatitwasnotpossibleforassessorstoconsistentlyenumer-atethetotalsetofnuggetscontainedinananswer,whichcorrespondstothedenominatorinaprecisioncalculation(Voorhees,2003).Thus,apenaltyforverbosityservesasasurrogateforprecision.2.2ThePyramidExtensionThevital/okaydistinctionhasbeenidentiﬁedasaweaknessintheTRECnugget-basedevalua-tionmethodology(Hildebrandtetal.,2004;LinandDemner-Fushman,2005;LinandDemner-Fushman,2006).Theredonotappeartobeanyre-liableindicatorsforpredictingnuggetimportance,whichmakesitchallengingtodevelopalgorithmssensitivetothisconsideration.Sinceonlyvitalnuggetsaffectnuggetrecall,itisdifﬁcultforsys-temstoachievenon-zeroscoresontopicswithfewvitalnuggetsintheanswerkey.Thus,scoresareeasilyaffectedbyassessorerrorsandotherrandomvariationsinevaluationconditions.OnedirectconsequenceisthatinpreviousTRECevaluations,themedianscoreformanyquestionsturnedouttobezero.Abinarydistinctiononnuggetimportanceisinsufﬁcienttodiscriminatebetweenthequalityofrunsthatreturnnovitalnuggetsbutdifferentnumbersofokaynuggets.Also,ascoredistributionheavilyskewedtowardszeromakesmeta-analysesofevaluationstabilitydifﬁculttoper-form(Voorhees,2005).Thepyramidextension(LinandDemner-Fushman,2006)wasproposedtoaddresstheissuesmentionedabove.Theideawasrelativelysimple:bysolicitingvital/okayjudgmentsfrommultipleasses-sors(afterthelistofnuggetshasbeenproducedbyaprimaryassessor),itispossibletodeﬁnenuggetimportancewithgreatergranularity.Eachnuggetisassignedaweightbetweenzeroandonethatispro-portionaltothenumberofassessorswhojudgedittobevital.NuggetrecallfromFigure1canberede-ﬁnedtoincorporatetheseweights:R=Pm∈AwmPn∈VwnWhereAisthesetofreferencenuggetsthatarematchedinasystem’soutputandVisthesetofallreferencenuggets;wmandwnaretheweightsofnuggetsmandn,respectively.1Thecalculationofnuggetprecisionremainsthesame.3NuggetPyramidsinTREC2006LinandDemner-Fushman(2006)presentexper-imentalevidenceinsupportofnuggetpyramidsbyapplyingtheproposaltoresultsfrompreviousTRECQAevaluations.Theirsimulationstudiesap-peartosupporttheassertionthatpyramidsaddressmanyoftheissuesraisedinSection2.2.Basedontheresults,NISTproceededwithatrialdeploymentofnuggetpyramidsintheTREC2006QAtrack.Al-thoughscoresbasedonthebinaryvital/okaydistinc-tionwereretainedasthe“ofﬁcial”metric,pyramidscoresweresimultaneouslycomputed.Thispro-videdanopportunitytocomparethetwomethod-ologiesonalargescale.3.1TheDataThebasicunitofevaluationforthemainQAtaskatTREC2006wasthe“questionseries”.Eachse-riesfocusedona“target”,whichcouldbeaperson,organization,thing,orevent.Individualquestionsinaseriesinquiredaboutdifferentfacetsofthetar-get,andwereexplicitlyclassiﬁedasfactoid,list,orOther.OnecompleteseriesisshowninFigure2.TheOtherquestionscanbebestparaphrasedas“TellmeinterestingthingsaboutXthatIhaven’talreadyexplicitlyaskedabout.”Itwasthesystem’stasktoretrieveinterestingnuggetsaboutthetarget(intheopinionoftheassessor),butcreditwasnotgivenforretrievingfactsalreadyexplicitlyaskedforinthefactoidandlistquestions.TheOtherquestionswereevaluatedusingthenugget-basedmethodology,andarethesubjectofthisanalysis.TheQAtestsetinTREC2006contained75se-ries.Ofthe75targets,19werepersons,19wereorganizations,19wereevents,and18werethings.Theseriescontainedatotalof75Otherquestions(onepertarget).Eachseriescontained6–9ques-tions(countingtheOtherquestion),withmostse-riescontaining8questions.ThetaskemployedtheAQUAINTcollectionofnewswiretext(LDCcat-alognumberLDC2002T31),consistingofEnglishdatadrawnfromthreesources:theNewYorkTimes,1Notethatthisnewscoringmodelcapturestheexistingbinaryvital/okaydistinctioninastraightforwardway:vitalnuggetsgetascoreofone,andokaynuggetszero.771

147Britain’sPrinceEdwardmarries147.1FACTOIDWhendidPrinceEdwardengagetomarry?147.2FACTOIDWhodidthePrincemarry?147.3FACTOIDWheredidtheyhoneymoon?147.4FACTOIDWherewasEdwardinlineforthethroneatthetimeofthewedding?147.5FACTOIDWhatwasthePrince’soccupation?147.6FACTOIDHowmanypeopleviewedtheweddingontelevision?147.7LISTWhatindividualswereatthewedding?147.8OTHERFigure2:SamplequestionseriesfromTREC2006.Nugget012345678Thecouplehadalongcourtship100000110QueenElizabethIIwasdelightedwiththematch010100001QueennamedcoupleEarlandContessaofWessex010011100AllmarriagesofEdward’ssiblingsendedindivorce000001001EdwardarrangedforWilliamtoappearmorecheerfulinphoto000000000theyweremarriedinSt.GeorgesChapel,Windsor111010110Figure3:Multipleassessors’judgmentsofnuggetimportanceforSeries147(vital=1,okay=0).Assessor2wasthesameastheprimaryassessor(assessor0),butjudgmentswereelicitedatdifferenttimes.theAssociatedPress,andtheXinhuaNewsService.Thereareapproximatelyonemillionarticlesinthecollection,totalingroughlythreegigabytes.Into-tal,59runsfrom27participantsweresubmittedtoNIST.Formoredetails,see(Dangetal.,2006).FortheOtherquestions,ninesetsofjudgmentswereelicitedfromeightjudges(theprimaryassessorwhooriginallycreatedthenuggetslaterannotatedthenuggetsonceagain).Eachassessorwasaskedtoassignthevital/okaylabelinarapidfashion,withoutgivingeachdecisionmuchthought.Figure3givesanexampleofthemultiplejudgmentsfornuggetsinSeries147.Thereisvariationinnotionsofimpor-tancenotonlybetweendifferentassessors,butalsoforasingleassessorovertime.3.2ResultsAfterthehumanannotationprocess,nuggetpyra-midswerebuiltinthemannerdescribedbyLinandDemner-Fushman(2006).Twoscoreswerecom-putedforeachrunsubmittedtotheTREC2006mainQAtask:onebasedonthevital/okayjudgmentsoftheprimaryassessor(whichwecallthebinaryF-score)andonebasedonthenuggetpyramids(thepyramidF-score).Thecharacteristicsofthepyra-midmethodcanbeinferredbycomparingthesetwosetsofscores.Figure4plotstheaveragebinaryandaveragepyramidF-scoresforeachrun(whichrepresentsav-erageperformanceacrossallseries).Eventhoughthenuggetpyramiddoesnotrepresentanysinglerealuser(apointwereturntolater),pyramidF-scoresdocorrelatehighlywiththebinaryF-scores.ThePearson’scorrelationis0.987,witha95%con-ﬁdenceintervalof[0.980,1.00].WhiletheaverageF-scoreforarunisstablegivenasufﬁcientnumberofquestions,theF-scoreforasingleOtherquestionexhibitsgreatervariabilityacrossassessors.ThisisshowninFigure5,whichplotsbinaryandpyramidF-scoresforindividualquestionsfromallruns.Inthiscase,thePearsoncorrelationis0.870,witha95%conﬁdenceintervalof[0.863,1.00].For16.4%ofallOtherquestions,thenuggetpyra-midassignedanon-zeroF-scorewheretheorigi-nalbinaryF-scorewaszero.ThiscanbeseeninthebandofpointsontheleftedgeoftheplotinFigure5.Thishighlightsthestrengthofnugget772

0.000.050.100.150.200.250.000.050.100.150.200.25Average binary F−scoreAverage pyramid F−scoreFigure4:ScatterplotcomparingthebinaryandpyramidF-scoresforeachrun.pyramids—theirabilitytosmoothoutassessordif-ferencesandmoreﬁnelydiscriminateamongsys-temoutputs.Thisisakeycapabilitythatisusefulforsystemdevelopers,particularlysincealgorithmicimprovementsareoftenincrementalandsmall.Becauseitismorestablethanthesingle-assessormethodofevaluation,thepyramidmethodalsoap-pearstohavegreaterdiscriminativepower.Weﬁtatwo-wayanalysisofvariancemodelwiththese-riesandrunasfactors,andthebinaryF-scoreasthedependentvariable.Wefoundsigniﬁcantdiffer-encesbetweenseriesandbetweenruns(pessentiallyequalto0forbothfactors).Todeterminewhichrunsweresigniﬁcantlydifferentfromeachother,weper-formedamultiplecomparisonusingTukey’shon-estlysigniﬁcantdifferencecriterionandcontrollingfortheexperiment-wiseTypeIerrorsothattheprob-abilityofdeclaringadifferencebetweentworunstobesigniﬁcant,whenitisactuallynot,isatmost5%.With59runs,thereareC592=1711differentpairsthatcanbecompared.Thesingle-assessormethodwasabletodeclareoneruntobesigniﬁcantlybetterthantheotherin557ofthesepairs.Usingthepyra-midF-scores,itwaspossibletoﬁndsigniﬁcantdif-ferencesinperformancebetweenrunsin617pairs.3.3DiscussionAnyevaluationrepresentsacompromisebetweeneffort(whichcorrelateswithcost)andinsightful-nessofresults.Thelevelofdetailandmeaning-0.00.20.40.60.80.00.20.40.60.8Binary F−scorePyramid F−scoreFigure5:ScatterplotcomparingthebinaryandpyramidF-scoresforeachOtherquestion.fulnessofevaluationsareconstantlyintensionwiththeavailabilityofresources.Modiﬁcationstoexist-ingprocessesusuallycomeatacostthatneedstobeweighedagainstpotentialgains.Basedonthesecon-siderations,thebalancesheetfornuggetpyramidsshowsafavorableorientation.IntheTREC2006QAevaluation,solicitingvital/okayjudgmentsfrommultipleassessorswasnotverytime-consuming(acoupleofhoursperassessor).Analysisconﬁrmsthatpyramidscoresconfermanybeneﬁtsatanac-ceptablecost,thusarguingforitsadoptioninfutureevaluations.Costconsiderationsprecludedexploringotherre-ﬁnementstothenugget-basedevaluationmethodol-ogy.Onepossiblealternativewouldinvolveask-ingmultipleassessorstocreatedifferentsetsofnuggetsfromscratch.Notonlywouldthisbetime-consuming,onewouldthenneedtodealwiththeadditionalcomplexitiesofaligningeachassessor’snuggetslist.Thisincludesresolvingissuessuchasnuggetgranularity,overlapininformationcontent,implicatureandotherrelationsbetweennuggets,etc.4ExplorationofAlternativeStructuresDespitethedemonstratedeffectivenessofnuggetpyramids,thereareafewpotentialdrawbacksthatareworthdiscussing.Onedownsideisthatthenuggetpyramiddoesnotrepresentasingleassessor.Thenuggetweightsreﬂecttheaggregationofopin-ionsacrossasamplepopulation,butthereisnoguar-773

anteethatthemethodforcomputingthoseweightsactuallycapturesanyaspectofrealuserbehavior.ItcanbearguedthatthebinaryF-scoreismorere-alisticsinceitreﬂectstheopinionofarealuser(theprimaryassessor),whereasthepyramidF-scoretriestomodeltheopinionofamythicalaverageuser.Althoughthispointmayseemsomewhatcounter-intuitive,itrepresentsawell-establishedtraditionintheinformationretrievalliterature(Voorhees,2002).Indocumentretrieval,forexample,relevancejudgmentsareprovidedbyasingleassessor—eventhoughitiswellknownthattherearelargeindi-vidualdifferencesinnotionsofrelevance.IRre-searchersbelievethathumanidiosyncrasiesareaninescapablefactpresentinanysystemdesignedforhumanusers,andhenceanyattempttoremovethoseelementsintheevaluationsetupisactuallyundesir-able.Itistheresponsibilityofresearcherstodevelopsystemsthatarerobustandﬂexible.Thispremise,however,doesnotmeanthatIRevaluationresultsareunstableorunreliable.Analyseshaveshownthatdespitelargevariationsinhumanopinions,sys-temrankingsareremarkablystable(Voorhees,2000;Sormunen,2002)—thatis,onecanusuallybeconﬁ-dentaboutsystemcomparisons.ThephilosophyinIRsharplycontrastswithworkinNLPannotationtaskssuchasparsing,wordsensedisambiguation,andsemanticrolelabeling—whereresearchersstriveforhighlevelsofinterannota-toragreement,oftenthroughelaborateguidelines.ThedifferenceinphilosophiesarisesbecauseunliketheseNLPannotationtasks,wheretheproductsareusedprimarilybyotherNLPsystemcomponents,IR(andlikewiseQA)isanend-usertask.Thesesys-temsareintendedforrealworlduse.Sincepeoplediffer,systemsmustbeabletoaccommodatethesedifferences.Hence,thereisastrongpreferenceinQAforevaluationsthatmaintainamodelofthein-dividualuser.4.1Micro-vs.Macro-AveragingThecurrentnuggetpyramidmethodleveragesmul-tiplejudgmentstodeﬁneaweightforeachindivid-ualnugget,andthenincorporatesthisweightintotheF-scorecomputation.Asanalternative,wepro-poseanothermethodforcombiningtheopinionsofmultipleassessors:evaluatesystemresponsesindi-viduallyagainstNsetsofbinaryjudgments,andthencomputethemeanacrossthosescores.Wede-ﬁnethemacro-averagedbinaryF-scoreoverasetA={a1,...,aN}ofNassessorsas:F=Pa∈AFaNWhereFaisthebinaryF-scoreaccordingtothevital/okayjudgmentsofassessora.Thediffer-encesbetweenthepyramidF-scoreandthemacro-averagedbinaryF-scorecorrespondtothedistinc-tionbetweenmicro-andmacro-averagingdiscussedinthecontextoftextclassiﬁcation(Lewis,1991).Inthoseapplications,bothmeasuresaremean-ingfuldependingonfocus:individualinstancesorentireclasses.Intaskswhereitisimportanttocorrectlyclassifyindividualinstances,micro-averagingismoreappropriate.Intaskswhereitisimportanttocorrectlyidentifyaclass,macro-averagingbetterquantiﬁesperformance.Inclassi-ﬁcationtasks,imbalanceintheprevalenceofeachclasscanleadtolargedifferencesinmacro-andmicro-averagedscores.Analogizingtoourwork,theoriginalformulationofnuggetpyramidscorre-spondstomicro-averaging(sincewefocusonindi-vidualnuggets),whilethealternativecorrespondstomacro-averaging(sincewefocusontheassessor).Weadditionallynotethatthetwomethodsen-codedifferentassumptions.Macro-averagingas-sumesthatthereisnothingintrinsicallyinterestingaboutanugget—itissimplyamatterofaparticularuserwithparticularneedsﬁndingaparticularnuggettobeofinterest.Micro-averaging,ontheotherhand,assumesthatsomenuggetsareinherentlyinterest-ing,independentoftheparticularinterestsofusers.2Eachapproachhascharacteristicsthatmakeitdesirable.Fromtheperspectiveofevaluators,themacro-averagedbinaryF-scoreispreferablebe-causeitmodelsrealusers;eachsetofbinaryjudg-mentsrepresentstheinformationneedofarealuser,eachbinaryF-scorerepresentshowwellananswerwillsatisfyarealuser,andthemacro-averagedbi-naryF-scorerepresentshowwellananswerwillsat-isfy,onaverage,asamplepopulationofrealusers.FromtheperspectiveofQAsystemdevelopers,themicro-averagednuggetpyramidF-scoreisprefer-ablebecauseitallowsﬁnerdiscriminationinin-2Wearegratefultoananonymousreviewerforthisinsight.774

dividualnuggetperformance,whichenablesbettertechniquesforsystemtrainingandoptimization.Themacro-averagedbinaryF-scorehasthesamedesirablepropertiesasthemicro-averagedpyramidF-scoreinthatfewerresponseswillhavezeroF-scoresascomparedtothesingle-assessorbinaryF-score.Wedemonstratethisasfollows.LetXbearesponsethatreceivesanon-zeropyramidF-score.LetA={a1,a2,a3,...,aN}bethesetofNasses-sors.ThenitcanbeproventhatXalsoreceivesanon-zeromacro-averagedbinaryF-score:1.Thereexistssomenuggetvwithweightgreaterthan0,suchthatananswerstringrinXmatchesv.(def.ofpyramidrecall)2.Thereexistssomeassessorap∈Awhomarkedvasvital.(def.ofpyramidnuggetweight)3.ToshowthatXwillalsoreceiveanon-zeromacro-averagedbinaryscore,itissufﬁcienttoshowthatthereissomeassessoram∈AsuchthatXreceivesanon-zeroF-scorewhenevalu-atedusingjustthevital/okayjudgmentsofam.(def.ofmacro-averagedbinaryF-score)4.But,suchanassessordoesexist,namelyasses-sorap:ConsiderthebinaryF-scoreassignedtoXaccordingtojustassessorap.There-callofXisgreaterthanzero,sinceXcontainstheresponserthatmatchesthenuggetvthatwasmarkedasvitalbyap(from(2),(1),andthedef.ofrecall).Theprecisionmustalsobegreaterthanzero(def.ofprecision).Therefore,themacro-averagedbinaryF-scoreofXisnon-zero.(def.ofF-score)4.2AnalysisfromTREC2006Whilethemacro-averagedmethodisguaranteedtoproducenomorezero-valuedscoresthanthemicro-averagedpyramidmethod,itisnotguaranteedthatthescoreswillbethesameforanygivenresponse.Whataretheempiricalcharacteristicsofeachap-proach?Toexplorethisquestion,weonceagainex-amineddatafromTREC2006.Figure6showsascatterplotofthepyramidF-scoreandmacro-averagedbinaryF-scoreforeveryOtherquestionsinallrunsfromtheTREC2006QAtrackmaintask.Despitefocusingondiffer-entaspectsoftheevaluationsetup,thesemeasures0.00.20.40.60.80.00.20.40.60.8Pyramid F−scoreMacro−averaged binary F−scoreFigure6:Scatterplotcomparingthepyramidandmacro-averagedbinaryF-scoresforallquestions.binarymicromacrobinary1.000/1.0000.870/0.9870.861/0.988micro-1.000/1.0000.985/0.996macro--1.000/1.000Table1:Pearson’scorrelationofF-scores,byques-tionandbyrun.arehighlycorrelated,evenatthelevelofindivid-ualquestions.Table1providesasummaryofthecorrelationsbetweentheoriginalbinaryF-score,the(micro-averaged)pyramidF-score,andthemacro-averagedbinaryF-score.Pearson’srisgivenforF-scoresattheindividualquestionlevel(ﬁrstnum-ber)andattherunlevel(secondnumber).Thecor-relationbetweenallthreevariantsareaboutequalatthelevelofsystemruns.Atthelevelofindividualquestions,themicro-andmacro-averagedF-scores(usingmultiplejudgments)arestillhighlycorrelatedwitheachother,buteachislesscorrelatedwiththesingle-assessorbinaryF-score.4.3DiscussionThedifferencesbetweenmacro-andmicro-averagingmethodsinvokeamoregeneraldiscus-siononnotionsofnuggetimportance.Thereareactuallytwodifferentissuesweareattemptingtoaddresswithourdifferentapproaches:theﬁrstisamoregranularscaleofnuggetimportance,thesecondisvariationsacrossapopulationofusers.In775

themicro-averagedpyramidF-scores,weachievetheﬁrstbyleveragingthesecond,i.e.,binaryjudgmentsfromalargepopulationarecombinedtoyieldweightsforindividualnuggets.Inthemacro-averagedbinaryF-score,wefocussolelyonpopulationeffectswithoutaddressinggranularityofnuggetimportance.Exploringthisthreadofargument,wecanfor-mulateadditionalapproachesfortacklingtheseis-sues.Wecould,forexample,solicitmoregranularindividualjudgmentsoneachnuggetfromeachas-sessor,perhapsonaLikertscaleorasacontinuousquantityrangingfromzerotoone.ThiswouldyieldtwomoremethodsforcomputingF-scores,bothamacro-averagedandamicro-averagedvariant.Themacro-averagedvariantwouldbeespeciallyattrac-tivebecauseitreﬂectsrealusersandyetindividualF-scoresremaindiscriminative.Despiteitspossi-bleadvantages,thisextensionisrejectedbasedonresourceconsiderations;makingsnapbinaryjudg-mentsonindividualnuggetsismuchquickerthanamulti-scaledvalueassignment—atleastatpresent,theadditionalcostsarenotsufﬁcienttooffsetthepotentialgains.5ConclusionTheimportantrolethatlarge-scaleevaluationsplayinguidingresearchinhumanlanguagetechnologiesmeansthatthecommunitymust“getitright.”Thiswouldordinarilycallforamoreconservativeap-proachtoavoidchangesthatmighthaveunintendedconsequences.However,evaluationmethodologiesmustevolvetoreﬂecttheshiftinginterestsofthere-searchcommunitytoremainrelevant.Thus,orga-nizersofevaluationsmustwalkaﬁnelinebetweenprogressandchaos.Nevertheless,theintroductionofnuggetpyramidsintheTRECQAevaluationpro-videsacasestudyshowinghowthisﬁnebalancecanindeedbeachieved.Theadditionofmultiplejudg-mentsofnuggetimportanceyieldsanevaluationthatisbothmorestableandmorediscriminativethantheoriginalsingle-assessorevaluation,whilerequiringonlyasmalladditionalcostintermsofhumanlabor.Wehaveexploredtwodifferentmethodsforcom-biningjudgmentsfrommultipleassessorstoaddressshortcomingsintheoriginalnugget-basedevalua-tionsetup.Althoughtheymakedifferentassump-tionsabouttheevaluation,resultsfrombothap-proachesarehighlycorrelated.Thus,wecancon-tinueemployingthepyramid-basedmethod,whichiswell-suitedfordevelopingsystems,andstillbeas-suredthattheresultsremainconsistentwithaneval-uationmethodthatmaintainsamodelofrealindi-vidualusers.AcknowledgmentsThisworkhasbeensupportedinpartbyDARPAcontractHR0011-06-2-0001(GALE).ThesecondauthorwouldliketothankKiriandEstherfortheirkindsupport.ReferencesH.Dang,J.Lin,andD.Kelly.2006.OverviewoftheTREC2006questionansweringtrack.InProc.ofTREC2006.W.Hildebrandt,B.Katz,andJ.Lin.2004.Answeringdeﬁnitionquestionswithmultipleknowledgesources.InProc.HLT/NAACL2004.D.Lewis.1991.Evaluatingtextcategorization.InProc.oftheSpeechandNaturalLanguageWorkshop.J.LinandD.Demner-Fushman.2005.Automaticallyevaluatinganswerstodeﬁnitionquestions.InProc.ofHLT/EMNLP2005.J.LinandD.Demner-Fushman.2006.Willpyramidsbuiltofnuggetstoppleover?InProc.ofHLT/NAACL2006.E.Sormunen.2002.LiberalrelevancecriteriaofTREC—countingonnegligibledocuments?InProc.ofSIGIR2002.E.Voorhees.2000.Variationsinrelevancejudgmentsandthemeasurementofretrievaleffectiveness.IP&M,36(5):697–716.E.Voorhees.2002.Thephilosophyofinformationre-trievalevaluation.InProc.ofCLEFWorkshop.E.Voorhees.2003.OverviewoftheTREC2003ques-tionansweringtrack.InProc.ofTREC2003.E.Voorhees.2005.Usingquestionseriestoevaluatequestionansweringsystemeffectiveness.InProc.ofHLT/EMNLP2005.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 776–783,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

776

ExploitingSyntacticandShallowSemanticKernelsforQuestion/AnswerClassiﬁcationAlessandroMoschittiUniversityofTrento38050PovodiTrentoItalymoschitti@dit.unitn.itSilviaQuarteroniTheUniversityofYorkYorkYO105DDUnitedKingdomsilvia@cs.york.ac.ukRobertoBasili“TorVergata”UniversityViadelPolitecnico100133Rome,Italybasili@info.uniroma2.itSureshManandharTheUniversityofYorkYorkYO105DDUnitedKingdomsuresh@cs.york.ac.ukAbstractWestudytheimpactofsyntacticandshallowsemanticinformationinautomaticclassiﬁ-cationofquestionsandanswersandanswerre-ranking.Wedeﬁne(a)newtreestruc-turesbasedonshallowsemanticsencodedinPredicateArgumentStructures(PASs)and(b)newkernelfunctionstoexploittherepresentationalpowerofsuchstructureswithSupportVectorMachines.Ourex-perimentssuggestthatsyntacticinformationhelpstaskssuchasquestion/answerclassiﬁ-cationandthatshallowsemanticsgivesre-markablecontributionwhenareliablesetofPASscanbeextracted,e.g.fromanswers.1IntroductionQuestionanswering(QA)isasaformofinforma-tionretrievalwhereoneormoreanswersarere-turnedtoaquestioninnaturallanguageintheformofsentencesorphrases.ThetypicalQAsystemar-chitectureconsistsofthreephases:questionpro-cessing,documentretrievalandanswerextraction(Kwoketal.,2001).Questionprocessingisoftencenteredonquestionclassiﬁcation,whichselectsoneofkexpectedan-swerclasses.Mostaccuratemodelsapplysuper-visedmachinelearningtechniques,e.g.SNoW(LiandRoth,2005),wherequestionsareencodedus-ingvariouslexical,syntacticandsemanticfeatures.Theretrievalandanswerextractionphasesconsistinretrievingrelevantdocuments(Collins-Thompsonetal.,2004)andselectingcandidateanswerpassagesfromthem.Afurtheranswerre-rankingphaseisop-tionallyapplied.Here,too,thesyntacticstructureofasentenceappearstoprovidemoreusefulinfor-mationthanabagofwords(Chenetal.,2006),al-thoughthecorrectwaytoexploititisstillanopenproblem.Aneffectivewaytointegratesyntacticstructuresinmachinelearningalgorithmsistheuseoftreeker-nel(TK)functions(CollinsandDuffy,2002),whichhavebeensuccessfullyappliedtoquestionclassiﬁ-cation(ZhangandLee,2003;Moschitti,2006)andothertasks,e.g.relationextraction(Zelenkoetal.,2003;Moschitti,2006).Inmorecomplextaskssuchascomputingtherelatednessbetweenquestionsandanswersinanswerre-ranking,toourknowledgenostudyuseskernelfunctionstoencodesyntacticin-formation.Moreover,thestudyofshallowsemanticinformationsuchaspredicateargumentstructuresannotatedinthePropBank(PB)project(KingsburyandPalmer,2002)(www.cis.upenn.edu/∼ace)isapromisingresearchdirection.Wearguethatseman-ticstructurescanbeusedtocharacterizetherelationbetweenaquestionandacandidateanswer.Inthispaper,weextensivelystudynewstructuralrepresentations,encodingparsetrees,bag-of-words,POStagsandpredicateargumentstructures(PASs)forquestionclassiﬁcationandanswerre-ranking.WedeﬁnenewtreerepresentationsforbothsimpleandnestedPASs,i.e.PASswhoseargumentsareotherpredicates(Section2).Moreover,wedeﬁnenewkernelfunctionstoexploitPASs,whichweau-tomaticallyderivewithourSRLsystem(Moschittietal.,2005)(Section3).OurexperimentsusingSVMsandtheaboveker-777

nelsanddata(Section4)showsthefollowing:(a)ourapproachreachesstate-of-the-artaccuracyonquestionclassiﬁcation.(b)PBpredicativestructuresarenoteffectiveforquestionclassiﬁcationbutshowpromisingresultsforanswerclassiﬁcationonacor-pusofanswerstoTREC-QA2001descriptionques-tions.WecreatedsuchdatasetbyusingYourQA(QuarteroniandManandhar,2006),ourbasicWeb-basedQAsystem1.(c)Theanswerclassiﬁerin-creasestherankingaccuracyofourQAsystembyabout25%.OurresultsshowthatPASandsyntacticparsingarepromisingmethodstoaddresstasksaffectedbydatasparsenesslikequestion/answercategorization.2EncodingShallowSemanticStructuresTraditionally,informationretrievaltechniquesarebasedonthebag-of-words(BOW)approachaug-mentedbylanguagemodeling(Allanetal.,2002).Whenthetaskrequirestheuseofmorecomplexse-mantics,theaboveapproachesareofteninadequatetoperformﬁne-leveltextualanalysis.AnimprovementonBOWisgivenbytheuseofsyntacticparsetrees,e.g.forquestionclassiﬁcation(ZhangandLee,2003),butthese,tooareinadequatewhendealingwithdeﬁnitionalanswersexpressedbylongandarticulatedsentencesorevenparagraphs.Onthecontrary,shallowsemanticrepresentations,bearingamore“compact”information,couldpre-ventthesparsenessofdeepstructuralapproachesandtheweaknessofBOWmodels.InitiativessuchasPropBank(PB)(KingsburyandPalmer,2002)havemadepossiblethedesignofaccurateautomaticSemanticRoleLabeling(SRL)systems(CarrerasandM`arquez,2005).AttemptinganapplicationofSRLtoQAhenceseemsnatural,aspinpointingtheanswertoaquestionreliesonadeepunderstandingofthesemanticsofboth.LetusconsiderthePBannotation:[ARG1Antigens]were[AM−TMPoriginally][reldefined][ARG2asnon-selfmolecules].Suchannotationcanbeusedtodesignashallowsemanticrepresentationthatcanbematchedagainstothersemanticallysimilarsentences,e.g.[ARG0Researchers][reldescribe][ARG1antigens][ARG2asforeignmolecules][ARGM−LOCin1Demoat:http://cs.york.ac.uk/aig/aqua.PASreldeﬁneARG1antigensARG2moleculesARGM-TMPoriginallyPASreldescribeARG0researchersARG1antigensARG2moleculesARGM-LOCbodyFigure1:Compactpredicateargumentstructuresoftwodifferentsentences.thebody].Forthispurpose,wecanrepresenttheaboveanno-tatedsentencesusingthetreestructuresdescribedinFigure1.Inthiscompactrepresentation,hereafterPredicate-ArgumentStructures(PAS),argumentsarereplacedwiththeirmostimportantword–oftenreferredtoasthesemantichead.ThisreducesdatasparsenesswithrespecttoatypicalBOWrepresentation.However,sentencesrarelycontainasinglepred-icate;ithappensmoregenerallythatpropositionscontainoneormoresubordinateclauses.Forinstanceletusconsideraslightmodiﬁcationoftheﬁrstsentence:“Antigenswereoriginallydeﬁnedasnon-selfmoleculeswhichboundspeciﬁcallytoantibodies2.”Here,themainpredicateis“deﬁned”,followedbyasubordinatepredicate“bound”.OurSRLsystemoutputsthefollowingtwoannotations:(1)[ARG1Antigens]were[ARGM−TMPoriginally][reldefined][ARG2asnon-selfmoleculeswhichboundspecificallytoantibodies].(2)Antigenswereoriginallydefinedas[ARG1non-selfmolecules][R−A1which][relbound][ARGM−MNRspecifically][ARG2toantibodies].givingthePASsinFigure2.(a)resp.2.(b).AsvisibleinFigure2.(a),whenanargumentnodecorrespondstoanentiresubordinateclause,welabelitsleafwithPAS,e.g.theleafofARG2.SuchPASnodeisactuallytherootofthesubordinateclauseinFigure2.(b).Takenasstandalone,suchPASsdonotexpressthewholemeaningofthesentence;itismoreaccuratetodeﬁneasinglestructureencod-ingthedependencybetweenthetwopredicatesasin2Thisisanactualanswerto”Whatareantibodies?”fromourquestionansweringsystem,YourQA.778

PASreldeﬁneARG1antigensARG2PASAM-TMPoriginally(a)PASrelboundARG1moleculesR-ARG1whichAM-ADVspeciﬁcallyARG2antibodies(b)PASreldeﬁneARG1antigensARG2PASrelboundARG1moleculesR-ARG1whichAM-ADVspeciﬁcallyARG2antibodiesAM-TMPoriginally(c)Figure2:TwoPASscomposingaPASNFigure2.(c).WerefertonestedPASsasPASNs.ItisworthtonotethatsemanticallyequivalentsentencessyntacticallyexpressedindifferentwayssharethesamePBargumentsandthesamePASs,whereassemanticallydifferentsentencesresultindifferentPASs.Forexample,thesentence:“Anti-genswereoriginallydeﬁnedasantibodieswhichboundspeciﬁcallytonon-selfmolecules”,usesthesamewordsas(2)buthasdifferentmeaning.ItsPBannotation:(3)Antigenswereoriginallydefinedas[ARG1antibodies][R−A1which][relbound][ARGM−MNRspecifically][ARG2tonon-selfmolecules],clearlydiffersfrom(2),asARG2isnownon-selfmolecules;consequently,thePASsarealsodifferent.OncewehaveassumedthatparsetreesandPASscanimproveonthesimpleBOWrepresentation,wefacetheproblemofrepresentingtreestructuresinlearningmachines.Section3introducesaviableap-proachbasedontreekernels.3SyntacticandSemanticKernelsforTextAsmentionedabove,encodingsyntactic/semanticinformationrepresentedbymeansoftreestructuresinthelearningalgorithmisproblematic.Aﬁrstso-lutionistouseallitspossiblesubstructuresasfea-tures.Giventhecombinatorialexplosionofconsid-eringsubparts,theresultingfeaturespaceisusuallyverylarge.Atreekernel(TK)functionwhichcom-putesthenumberofcommonsubtreesbetweentwosyntacticparsetreeshasbeengivenin(CollinsandDuffy,2002).Unfortunately,suchsubtreesaresub-jecttotheconstraintthattheirnodesaretakenwithallornoneofthechildrentheyhaveintheoriginaltree.ThismakestheTKfunctionnotwellsuitedforthePAStreesdeﬁnedabove.Forinstance,althoughthetwoPASsofFigure1sharemostofthesubtreesrootedinthePASnode,CollinsandDuffy’skernelwouldcomputenomatch.Inthenextsectionwedescribeanewkernelde-rivedfromtheabovetreekernel,abletoevaluatethemeaningfulsubstructuresforPAStrees.Moreover,asasinglePASmaynotbesufﬁcientfortextrep-resentation,weproposeanewkernelthatcombinesthecontributionsofdifferentPASs.3.1TreekernelsGiventwotreesT1andT2,let{f1,f2,..}=Fbethesetofsubstructures(fragments)andIi(n)beequalto1iffiisrootedatnoden,0otherwise.CollinsandDuffy’skernelisdeﬁnedasTK(T1,T2)=Pn1∈NT1Pn2∈NT2∆(n1,n2),(1)whereNT1andNT2arethesetsofnodesinT1andT2,respectivelyand∆(n1,n2)=P|F|i=1Ii(n1)Ii(n2).Thelatterisequaltothenumberofcommonfragmentsrootedinnodesn1andn2.∆canbecomputedasfollows:(1)iftheproductions(i.e.thenodeswiththeirdirectchildren)atn1andn2aredifferentthen∆(n1,n2)=0;(2)iftheproductionsatn1andn2arethesame,andn1andn2onlyhaveleafchildren(i.e.theyarepre-terminalsymbols)then∆(n1,n2)=1;(3)iftheproductionsatn1andn2arethesame,andn1andn2arenotpre-terminalsthen∆(n1,n2)=Qnc(n1)j=1(1+∆(cjn1,cjn2)),wherenc(n1)isthenum-berofchildrenofn1andcjnisthej-thchildofn.Suchtreekernelcanbenormalizedandaλfactorcanbeaddedtoreducetheweightoflargestructures(referto(CollinsandDuffy,2002)foracompletedescription).Thecriticalaspectofsteps(1),(2)and(3)isthattheproductionsoftwoevaluatednodeshavetobeidenticaltoallowthematchoffurtherde-scendants.Thismeansthatcommonsubstructurescannotbecomposedbyanodewithonlysomeofits779

PASSLOTreldeﬁneSLOTARG1antigens*SLOTARG2PAS*SLOTARGM-TMPoriginally*(a)PASSLOTreldeﬁneSLOTARG1antigens*SLOTnullSLOTnull(b)PASSLOTreldeﬁneSLOTnullSLOTARG2PAS*SLOTnull(c)Figure3:APASwithsomeofitsfragments.childrenasaneffectivePASrepresentationwouldrequire.WesolvethisproblembydesigningtheShallowSemanticTreeKernel(SSTK)whichallowstomatchportionsofaPAS.3.2TheShallowSemanticTreeKernel(SSTK)TheSSTKisbasedontwoideas:ﬁrst,wechangethePAS,asshowninFigure3.(a)byaddingSLOTnodes.Theseaccommodateargumentlabelsinaspeciﬁcorder,i.e.weprovideaﬁxednumberofslots,possiblyﬁlledwithnullarguments,thaten-codeallpossiblepredicatearguments.Forsimplic-ity,theﬁgureshowsastructureofjust4arguments,butmorecanbeaddedtoaccommodatethemax-imumnumberofargumentsapredicatecanhave.Leafnodesareﬁlledwiththewildcardcharacter*buttheymayalternativelyaccommodateadditionalinformation.TheslotnodesareusedinsuchawaythattheadoptedTKfunctioncangeneratefragmentscon-tainingoneormorechildrenlikeforexamplethoseshowninframes(b)and(c)ofFigure3.Aspre-viouslypointedout,iftheargumentsweredirectlyattachedtotherootnode,thekernelfunctionwouldonlygeneratethestructurewithallchildren(orthestructurewithnochildren,i.e.empty).Second,astheoriginaltreekernelwouldgeneratemanymatcheswithslotsﬁlledwiththenulllabel,wehavesetanewstep0:(0)ifn1(orn2)isapre-terminalnodeanditschildlabelisnull,∆(n1,n2)=0;andsubtractoneunitto∆(n1,n2),instep3:(3)∆(n1,n2)=Qnc(n1)j=1(1+∆(cjn1,cjn2))−1,Theabovechangesgenerateanew∆which,whensubstituted(inplaceoftheoriginal∆)inEq.1,givesthenewShallowSemanticTreeKernel.ToshowthatSSTKiseffectiveincountingthenumberofrelationssharedbytwoPASs,weproposethefol-lowing:Proposition1Thenew∆functionappliedtothemodiﬁedPAScountsthenumberofallpossiblek-aryrelationsderivablefromasetofkarguments,i.e.Pki=1(cid:0)ki(cid:1)relationsofarityfrom1tok(thepred-icatebeingconsideredasaspecialargument).ProofWeobservethatakernelappliedtoatreeanditselfcomputesallitssubstructures,thusifweeval-uateSSTKbetweenaPASanditselfwemustobtainthenumberofgeneratedk-aryrelations.Weprovebyinductiontheaboveclaim.Forthebasecase(k=0):weuseaPASwithnoarguments,i.e.allitsslotsareﬁlledwithnullla-bels.LetrbethePASroot;sincerisnotapre-terminal,step3isselectedand∆isrecursivelyap-pliedtoallr’schildren,i.e.theslotnodes.Forthelatter,step0assigns∆(cjr,cjr)=0.Asaresult,∆(r,r)=Qnc(r)j=1(1+0)−1=0andthebasecaseholds.Forthegeneralcase,ristherootofaPASwithk+1arguments.∆(r,r)=Qnc(r)j=1(1+∆(cjr,cjr))−1=Qkj=1(1+∆(cjr,cjr))×(1+∆(ck+1r,ck+1r))−1.Forkarguments,weassumebyinductionthatQkj=1(1+∆(cjr,cjr))−1=Pki=1(cid:0)ki(cid:1),i.e.thenumberofk-aryrelations.Moreover,(1+∆(ck+1r,ck+1r))=2,thus∆(r,r)=Pki=1(cid:0)ki(cid:1)×2=2k×2=2k+1=Pk+1i=1(cid:0)k+1i(cid:1),i.e.alltherelationsuntilarityk+12TKfunctionscanbeappliedtosentenceparsetrees,thereforetheirusefulnessfortextprocessingapplications,e.g.questionclassiﬁcation,isevident.Onthecontrary,theSSTKappliedtoonePASex-tractedfromatextfragmentmaynotbemeaningfulsinceitsrepresentationneedstotakeintoaccountallthePASsthatitcontains.Weaddresssuchproblem780

bydeﬁningakernelonmultiplePASs.LetPtandPt0bethesetsofPASsextractedfromthetextfragmenttandt0.Wedeﬁne:Kall(Pt,Pt0)=Xp∈PtXp0∈Pt0SSTK(p,p0),(2)Whileduringtheexperiments(Sect.4)theKallkernelisusedtohandlepredicateargumentstruc-tures,TK(Eq.1)isusedtoprocessparsetreesandthelinearkerneltohandlePOSandBOWfeatures.4ExperimentsThepurposeofourexperimentsistostudytheim-pactofthenewrepresentationsintroducedearlierforQAtasks.Inparticular,wefocusonquestionclas-siﬁcationandanswerre-rankingforWeb-basedQAsystems.Inthequestionclassiﬁcationtask,weextendpre-viousstudies,e.g.(ZhangandLee,2003;Moschitti,2006),bytestingasetofpreviouslydesignedker-nelsandtheircombinationwithournewShallowSe-manticTreeKernel.Intheanswerre-rankingtask,weapproachtheproblemofdetectingdescriptionanswers,amongthemostcomplexintheliterature(Cuietal.,2005;Kazawaetal.,2001).Therepresentationsthatweadoptare:bag-of-words(BOW),bag-of-POStags(POS),parsetree(PT),predicateargumentstructure(PAS)andnestedPAS(PASN).BOWandPOSareprocessedbymeansofalinearkernel,PTisprocessedwithTK,PASandPASNareprocessedbySSTK.Weimple-mentedtheproposedkernelsintheSVM-light-TKsoftwareavailableatai-nlp.info.uniroma2.it/moschitti/whichencodestreekernelfunctionsinSVM-light(Joachims,1999).4.1QuestionclassiﬁcationAsaﬁrstexperiment,wefocusonquestionclassi-ﬁcation,forwhichbenchmarksandbaselineresultsareavailable(ZhangandLee,2003;LiandRoth,2005).Wedesignaquestionmulti-classiﬁerbycombiningnbinarySVMs3accordingtotheONE-vs-ALLscheme,wheretheﬁnaloutputclassistheoneassociatedwiththemostprobableprediction.ThePASswereautomaticallyderivedbyourSRL3Weadoptedthedefaultregularizationparameter(i.e.,theaverageof1/||~x||)andtriedafewcost-factorvaluestoadjusttheratebetweenPrecisionandRecallonthedevelopmentset.systemwhichachievesa76%F1-measure(Mos-chittietal.,2005).Asbenchmarkdata,weusethequestiontrain-ingandtestsetavailableat:l2r.cs.uiuc.edu/∼cogcomp/Data/QA/QC/,wherethetestsetarethe500TREC2001testquestions(Voorhees,2001).WerefertothissplitasUIUC.Theperformanceofthemulti-classiﬁerandtheindividualbinaryclassi-ﬁersismeasuredwithaccuracyresp.F1-measure.Tocollectstatisticallysigniﬁcantinformation,werun10-foldcrossvalidationonthe6,000questions.FeaturesAccuracy(UIUC)Accuracy(c.v.)PT90.484.8±1.2BOW90.684.7±1.2PAS34.243.0±1.9POS26.432.4±2.1PT+BOW91.886.1±1.1PT+BOW+POS91.884.7±1.5PAS+BOW90.082.1±1.3PAS+BOW+POS88.881.0±1.5Table1:Accuracyofthequestionclassiﬁerwithdif-ferentfeaturecombinationsQuestionclassiﬁcationresultsTable1showstheaccuracyofdifferentquestionrepresentationsontheUIUCsplit(Column1)andtheaverageaccuracy±thecorrespondingconﬁdencelimit(at90%signiﬁ-cance)onthecrossvalidationsplits(Column2).(i)TheTKonPTandthelinearkernelonBOWpro-duceaveryhighresult,i.e.about90.5%.Thisishigherthanthebestoutcomederivedin(ZhangandLee,2003),i.e.90%,obtainedwithakernelcombin-ingBOWandPTonthesamedata.CombinedwithPT,BOWreaches91.8%,veryclosetothe92.5%accuracyreachedin(LiandRoth,2005)usingcom-plexsemanticinformationfromexternalresources.(ii)ThePASfeatureprovidesnoimprovement.Thisismainlybecauseatleasthalfofthetrainingandtestquestionsonlycontainthepredicate“tobe”,forwhichaPAScannotbederivedbyaPB-basedshal-lowsemanticparser.(iii)The10-foldcross-validationexperimentscon-ﬁrmthetrendsobservedintheUIUCsplit.Thebestmodel(accordingtostatisticalsigniﬁcance)isPT+BOW,achievingan86.1%averageaccuracy4.4ThisvalueislowerthantheUIUCsplitoneastheUIUCtestsetisnotconsistentwiththetrainingset(itcontainsthe781

4.2AnswerclassiﬁcationQuestionclassiﬁcationdoesnotallowtofullyex-ploitthePASpotentialsincequestionstendtobeshortandwithfewverbalpredicates(i.e.theonlyonesthatourSRLsystemcanextract).Adiffer-entscenarioisanswerclassiﬁcation,i.e.decidingifapassage/sentencecorrectlyanswersaquestion.Here,thesemanticstobegeneratedbytheclassi-ﬁerarenotconstrainedtoasmalltaxonomyandan-swerlengthmaymakethePT-basedrepresentationtoosparse.WelearnanswerclassiﬁcationwithabinarySVMwhichdeterminesifanansweriscorrectforthetar-getquestion:here,theclassiﬁcationinstancesarehquestion,answeripairs.EachpaircomponentcanbeencodedwithPT,BOW,PASandPASNrepre-sentations(processedbypreviouskernels).Astestdata,wecollectedthe138TREC2001testquestionslabeledas“description”andforeach,weobtainedalistofanswerparagraphsextractedfromWebdocumentsusingYourQA.Eachparagraphsen-tencewasmanuallyevaluatedbasedonwhetheritcontainedananswertothecorrespondingquestion.Moreover,tosimplifytheclassiﬁcationproblem,weisolatedforeachparagraphthesentencewhichob-tainedthemaximaljudgment(incasemorethanonesentenceintheparagraphhadthesamejudgment,wechosetheﬁrstone).Wecollectedacorpuscon-taining1309sentences,416ofwhich–labeled“+1”–answeredthequestioneitherconciselyorwithnoise;therest–labeled“-1”–wereeitherirrele-vanttothequestionorcontainedhintsrelatingtothequestionbutcouldnotbejudgedasvalidanswers5.AnswerclassiﬁcationresultsTotesttheimpactofourmodelsonanswerclassiﬁcation,weran5-foldcross-validation,withtheconstraintthattwopairshq,a1iandhq,a2iassociatedwiththesameques-tionqcouldnotbesplitbetweentrainingandtest-ing.Hence,eachreportedvalueistheaverageover5differentoutcomes.ThestandarddeviationsrangedTREC2001questions)andincludesalargerpercentageofeas-ilyclassiﬁedquestiontypes,e.g.thenumeric(22.6%)andde-scriptionclasses(27.6%)whosepercentageintrainingis16.4%resp.16.2%.5Forinstance,giventhequestion“Whatareinvertebrates?”,thesentence“Atleast99%ofallanimalspeciesareinverte-brates,comprising...”waslabeled“-1”,while“Invertebratesareanimalswithoutbackbones.”waslabeled“+1”.            		  
 !"# ! !"#$%! !"#$%& !$%& !"#$%& !$%!"#$%!$%!"#$%& !$%!"# !Figure4:ImpactoftheBOWandPTfeaturesonanswerclassiﬁcation'()*'()+'+)*'+)+'')*'')+',)*',)+'-)*'-)+'.)*/)+0)*0)+1)*1)+()*()++)*+)+')*')+,)*2345678953:;<=>?@ABC?DEFGHIJKEFGHIDEFGHIJKELMNFGHIDEFGHIJKEFGHNLMNLKOIDEFGHIJKEFGHNLMNLKOPIDEFGHIJKEFGHNLKOIDEFGHIJKEFGHNLKOPIFigure5:ImpactofthePASandPASNfeaturescombinedwiththeBOWandPTfeaturesonanswerclassiﬁcationQRSTQRSUQVSTQVSUUTSTUTSUUWSTUWSUUXSTWSUXSTXSUYSTYSUQSTQSUUSTUSUZSTZSU[ST\]^_`abc_]defghijklminopqrstuovuwsnopqrstuovuwxsFigure6:ComparisonbetweenPASandPASNwhenusedasstandalonefeaturesfortheansweronanswerclassiﬁcation782

approximatelybetween2.5and5.Theexperimentswereorganizedasfollows:First,weexaminedthecontributionsofBOWandPTrepresentationsastheyprovedveryimportantforquestionclassiﬁcation.Figure4reportstheplotoftheF1-measureofanswerclassiﬁerstrainedwithallcombinationsoftheabovemodelsaccordingtodif-ferentvaluesofthecost-factorparameter,adjustingtheratebetweenPrecisionandRecall.Weseeherethatthemostaccurateclassiﬁersaretheonesusingboththeanswer’sBOWandPTfeatureandeitherthequestion’sPTorBOWfeature(i.e.Q(BOW)+A(PT,BOW)resp.Q(PT)+A(PT,BOW)combina-tions).WhenPTisusedfortheanswerthesim-pleBOWmodelisoutperformedby2to3points.Hence,weinferthatboththeanswer’sPTandBOWfeaturesareveryusefulintheclassiﬁcationtask.However,PTdoesnotseemtoprovideadditionalinformationtoBOWwhenusedforquestionrepre-sentation.Thiscanbeexplainedbyconsideringthatanswerclassiﬁcation(restrictedtodescriptionques-tions)doesnotrequirequestiontypeclassiﬁcationsinceitsmainpurposeistodetectquestion/answerrelations.Inthisscenario,thequestion’ssyntacticstructuredoesnotseemtoprovidemuchmoreinfor-mationthanBOW.Secondly,weevaluatedtheimpactofthenewlydeﬁnedPASandPASNfeaturescombinedwiththebestperformingpreviousmodel,i.e.Q(BOW)+A(PT,BOW).Figure5illustratestheF1-measureplotsagainaccordingtothecost-factorparam-eter.WeobserveherethatmodelQ(BOW)+A(PT,BOW,PAS)greatlyoutperformsmodelQ(BOW)+A(PT,BOW),provingthatthePASfea-tureisveryusefulforanswerclassiﬁcation,i.e.theimprovementisabout2to3pointswhilethedifferencewiththeBOWmodel,i.e.Q(BOW)+A(BOW),exceeds3points.TheQ(BOW)+A(PT,BOW,PASN)modelisnotmoreeffectivethanQ(BOW)+A(PT,BOW,PAS).ThissuggestseitherthatPASismoreeffectivethanPASNorthatwhenthePTinformationisadded,thePASNcontributionfadesout.Tofurtherinvestigatethepreviousissue,weﬁ-nallycomparedthecontributionofthePASandPASNwhencombinedwiththequestion’sBOWfeaturealone,i.e.noPTisused.Theresults,re-portedinFigure6,showthatthistimePASNper-formsbetterthanPAS.Thissuggeststhatthedepen-denciesbetweenthenestedPASsareinsomewaycapturedbythePTinformation.Indeed,itshouldbenotedthatwejoinpredicatesonlyincaseoneissubordinatetotheother,thusconsideringonlyare-strictedsetofallpossiblepredicatedependencies.However,theimprovementoverPASconﬁrmsthatPASNistherightdirectiontoencodeshallowse-manticsfromdifferentsentencepredicates.BaselinePRF1-measureGg@539.22±3.5933.15±4.2235.92±3.95QA@539.72±3.4434.22±3.6336.76±3.56Gg@all31.58±0.5810048.02±0.67QA@all31.58±0.5810048.02±0.67GgQARe-rankerMRR48.97±3.7756.21±3.1881.12±2.12Table2:BaselineclassiﬁersaccuracyandMRRofYourQA(QA),Google(Gg)andthebestre-ranker4.3Answerre-rankingTheoutputoftheanswerclassiﬁercanbeusedtore-rankthelistofcandidateanswersofaQAsys-tem.Startingfromthetopanswer,eachinstancecanbeclassiﬁedbasedonitscorrectnesswithrespecttothequestion.Ifitisclassiﬁedascorrectitsrankisunchanged;otherwiseitispusheddown,untilalowerrankedincorrectanswerisfound.WeusedtheanswerclassiﬁerwiththehighestF1-measureonthedevelopmentsetaccordingtodiffer-entcost-factorvalues6.WeappliedsuchmodeltotheGoogleranksandtotheranksofourWeb-basedQAsystem,i.e.YourQA.ThelatterusesWebdocu-mentscorrespondingtothetop20Googleresultsforthequestion.Then,eachsentenceineachdocumentiscomparedtothequestionviaablendofsimilar-itymetricsusedintheanswerextractionphasetoselectthemostrelevantsentence.Apassageofupto750bytesisthencreatedaroundthesentenceandreturnedasananswer.Table2illustratestheresultsoftheanswerclassi-ﬁersderivedbyexploitingGoogle(Gg)andYourQA(QA)ranks:thetopNrankedresultsareconsideredascorrectdeﬁnitionsandtheremainingonesasin-6However,byobservingthecurvesinFig.5,theselectedparametersappearaspessimisticestimatesforthebestmodelimprovement:theoneforBOWistheabsolutemaximum,butanaverageoneisselectedforthebestmodel.783

correctfordifferentvaluesofN.WeshowN=5andthemaximumN(all),i.e.alltheavailablean-swers.EachmeasureistheaverageofthePrecision,RecallandF1-measurefromcrossvalidation.TheF1-measureofGoogleandYourQAaregreatlyout-performedbyouranswerclassiﬁer.ThelastrowofTable2reportstheMRR7achievedbyGoogle,YourQA(QA)andYourQAaf-terre-ranking(Re-ranker).WenotethatGoogleisoutperformedbyYourQAsinceitsranksarebasedonwholedocuments,notonsinglepassages.ThusGooglemayrankadocumentcontainingseveralsparselydistributedquestionwordshigherthandoc-umentswithseveralwordsconcentratedinonepas-sage,whicharemoreinteresting.WhentheanswerclassiﬁerisappliedtoimprovetheYourQAranking,theMRRreaches81.1%,risingbyabout25%.Finally,itisworthtonotethattheanswerclas-siﬁerbasedonQ(BOW)+A(BOW,PT,PAS)model(parameterizedasdescribed)gavea4%higherMRRthantheonebasedonthesimpleBOWfeatures.Asanexample,forquestion“Whatisforeclosure?”,thesentence“Foreclosuremeansthatthelendertakespossessionofyourhomeandsellsitinordertogetitsmoneyback.”wascorrectlyclassiﬁedbythebestmodel,whileBOWfailed.5ConclusionInthispaper,wehaveintroducednewstructurestorepresenttextualinformationinthreequestionan-sweringtasks:questionclassiﬁcation,answerclassi-ﬁcationandanswerre-ranking.Wehavedeﬁnedtreestructures(PASandPASN)torepresentpredicate-argumentrelations,whichweautomaticallyextractusingourSRLsystem.Wehavealsointroducedtwofunctions,SSTKandKall,toexploittheirrepre-sentativepower.OurexperimentswithSVMsandtheabovemodelssuggestthatsyntacticinformationhelpstaskssuchasquestionclassiﬁcationwhereassemanticinforma-tioncontainedinPASandPASNgivespromisingre-sultsinanswerclassiﬁcation.Inthefuture,weaimtostudywaystocapturere-lationsbetweenpredicatessothatmoregeneralse-7TheMeanReciprocalRankisdeﬁnedas:MRR=1nPni=11ranki,wherenisthenumberofquestionsandrankiistherankoftheﬁrstcorrectanswertoquestioni.manticscanbeencodedbyPASN.Formsofgeneral-izationforpredicatesandargumentswithinPASNslikeLSAclusters,WordNetsynsetsandFrameNet(rolesandframes)informationalsoappearasapromisingresearcharea.AcknowledgmentsWethanktheanonymousreviewersfortheirhelpfulsugges-tions.AlessandroMoschittiwouldliketothanktheAMI2labattheUniversityofTrentoandtheEUprojectLUNA“spokenLanguageUNderstandinginmultilinguAlcommunicationsys-tems”contractno33549forsupportingpartofhisresearch.ReferencesJ.Allan,J.Aslam,N.Belkin,andC.Buckley.2002.Chal-lengesinIRandlanguagemodeling.InReportofaWork-shopattheUniversityofAmherst.X.CarrerasandL.M`arquez.2005.IntroductiontotheCoNLL-2005sharedtask:SRL.InCoNLL-2005.Y.Chen,M.Zhou,andS.Wang.2006.RerankinganswersfromdeﬁnitionalQAusinglanguagemodels.InACL’06.M.CollinsandN.Duffy.2002.Newrankingalgorithmsforparsingandtagging:Kernelsoverdiscretestructures,andthevotedperceptron.InACL’02.K.Collins-Thompson,J.Callan,E.Terra,andC.L.A.Clarke.2004.TheeffectofdocumentretrievalqualityonfactoidQAperformance.InSIGIR’04.ACM.H.Cui,M.Kan,andT.Chua.2005.Genericsoftpatternmod-elsfordeﬁnitionalQA.InSIGIR’05.ACM.T.Joachims.1999.Makinglarge-scaleSVMlearningpractical.InAdvancesinKernelMethods-SupportVectorLearning.H.Kazawa,H.Isozaki,andE.Maeda.2001.NTTquestionansweringsysteminTREC2001.InTREC’01.P.KingsburyandM.Palmer.2002.FromTreebanktoProp-Bank.InLREC’02.C.C.T.Kwok,O.Etzioni,andD.S.Weld.2001.Scalingquestionansweringtotheweb.InWWW’01.X.LiandD.Roth.2005.Learningquestionclassiﬁers:theroleofsemanticinformation.Journ.Nat.Lang.Eng.A.Moschitti,B.Coppola,A.Giuglea,andR.Basili.2005.Hierarchicalsemanticrolelabeling.InCoNLL2005sharedtask.A.Moschitti.2006.Efﬁcientconvolutionkernelsfordepen-dencyandconstituentsyntactictrees.InECML’06.S.QuarteroniandS.Manandhar.2006.UsermodellingforAdaptiveQuestionAnsweringandInformationRetrieval.InFLAIRS’06.E.M.Voorhees.2001.OverviewoftheTREC2001QAtrack.InTREC’01.D.Zelenko,C.Aone,andA.Richardella.2003.Kernelmeth-odsforrelationextraction.Journ.ofMach.Learn.Res.D.ZhangandW.Lee.2003.Questionclassiﬁcationusingsup-portvectormachines.InSIGIR’03.ACM.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 784–791,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

784

Language-independentProbabilisticAnswerRankingforQuestionAnsweringJeongwooKo,TerukoMitamura,EricNybergLanguageTechnologiesInstituteSchoolofComputerScienceCarnegieMellonUniversity{jko,teruko,ehn}@cs.cmu.eduAbstractThispaperpresentsalanguage-independentprobabilisticanswerrankingframeworkforquestionanswering.Theframeworkesti-matestheprobabilityofanindividualan-swercandidategiventhedegreeofanswerrelevanceandtheamountofsupportingevi-denceprovidedinthesetofanswercandi-datesforthequestion.Ourapproachwasevaluatedbycomparingthecandidatean-swersetsgeneratedbyChineseandJapaneseanswerextractorswiththere-rankedanswersetsproducedbytheanswerrankingframe-work.EmpiricalresultsfromtestingonNT-CIRfactoidquestionsshowa40%perfor-manceimprovementinChineseanswerse-lectionanda45%improvementinJapaneseanswerselection.1IntroductionQuestionanswering(QA)systemsaimatﬁnd-ingpreciseanswerstonaturallanguagequestionsfromlargedocumentcollections.TypicalQAsys-tems(Prageretal.,2000;Clarkeetal.,2001;Harabagiuetal.,2000)adoptapipelinearchitec-turethatincorporatesfourmajorsteps:(1)questionanalysis,(2)documentretrieval,(3)answerextrac-tionand(4)answerselection.Questionanalysisisaprocesswhichanalyzesaquestionandproducesalistofkeywords.Documentretrievalisastepthatsearchesforrelevantdocumentsorpassages.An-swerextractionextractsalistofanswercandidatesfromtheretrieveddocuments.Answerselectionisaprocesswhichpinpointscorrectanswer(s)fromtheextractedcandidateanswers.SincetheﬁrstthreestepsintheQApipelinemayproduceerroneousoutputs,theﬁnalanswerselec-tionstepoftenentailsidentifyingcorrectanswer(s)amongstmanyincorrectones.Forexample,giventhequestion“WhichChinesecityhasthelargestnumberofforeignﬁnancialcompanies?”,thean-swerextractioncomponentproducesarankedlistofﬁveanswercandidates:Beijing(AP880603-0268)1,HongKong(WSJ920110-0013),Shanghai(FBIS3-58),Taiwan(FT942-2016)andShanghai(FBIS3-45320).Duetoimprecisioninanswerextraction,anincorrectanswer(“Beijing”)canberankedintheﬁrstposition,andthecorrectanswer(“Shang-hai”)wasextractedfromtwodifferentdocumentsandrankedinthethirdandtheﬁfthpositions.Inor-dertorank“Shanghai”inthetopposition,wehavetoaddresstwointerestingchallenges:•AnswerSimilarity.Howdoweexploitsimi-larityamonganswercandidates?Forexample,whenthecandidateslistcontainsredundantan-swers(e.g.,“Shanghai”asabove)orseveralan-swerswhichrepresentasingleinstance(e.g.“U.S.A.”and“theUnitedStates”),howmuchshouldweboosttherankoftheredundantan-swers?•AnswerRelevance.Howdoweidentifyrelevantanswer(s)amongstirrelevantones?Thistaskmayinvolvesearchingforevi-denceofarelationshipbetweentheanswer1AnswercandidatesareshownwiththeidentiﬁeroftheTRECdocumentwheretheywerefound.785

andtheanswertypeoraquestionkey-word.Forexample,wemightwishtoqueryaknowledgebasetodetermineif“Shang-hai”isacity(IS-A(Shanghai,city)),ortodetermineifShanghaiisinChina(IS-IN(Shanghai,China)).Theﬁrstchallengeistoexploitredundancyinthesetofanswercandidates.Asanswercandidatesareextractedfromdifferentdocuments,theymaycon-tainidentical,similarorcomplementarytextsnip-pets.Forexample,“U.S.”canappearas“UnitedStates”or“USA”indifferentdocuments.Itisim-portanttodetectredundantinformationandboostanswerconﬁdence,especiallyforlistquestionsthatrequireasetofuniqueanswers.Oneapproachistoperformanswerclustering(Nybergetal.,2002;Jijkounetal.,2006).However,theuseofcluster-ingraisesadditionalquestions:howtocalculatethescoreoftheclusteredanswers,andhowtoselecttheclusterlabel.Toaddressthesecondquestion,severalanswerselectionapproacheshaveusedexternalknowledgeresourcessuchasWordNet,CYCandgazetteersforanswervalidationoranswerreranking.Answercan-didatesareeitherremovedordiscountediftheyarenotoftheexpectedanswertype(Xuetal.,2002;Moldovanetal.,2003;Chu-Carrolletal.,2003;Echihabietal.,2004).TheWebalsohasbeenusedforanswerrerankingbyexploitingsearchenginere-sultsproducedbyqueriescontainingtheanswercan-didateandquestionkeywords(Magninietal.,2002).Thisapproachhasbeenusedinvariouslanguagesforanswervalidation.Wikipedia’sstructuredin-formationwasusedforSpanishanswertypecheck-ing(BuscaldiandRosso,2006).AlthoughmanyQAsystemshaveincorporatedin-dividualfeaturesand/orresourcesforanswerselec-tioninasinglelanguage,therehasbeenlittlere-searchonageneralizedprobabilisticframeworkthatsupportsanswerrankinginmultiplelanguagesusinganyanswerrelevanceandanswersimilarityfeaturesthatareappropriateforthelanguageinquestion.Inthispaper,wedescribeaprobabilisticanswerrankingframeworkformultiplelanguages.Theframeworkuseslogisticregressiontoestimatetheprobabilitythatananswercandidateiscorrectgivenmultipleanswerrelevancefeaturesandanswersim-ilarityfeatures.AnexistingframeworkwhichwasoriginallydevelopedforEnglish(Koetal.,2007)wasextendedforChineseandJapaneseanswerrankingbyincorporatinglanguage-speciﬁcfeatures.EmpiricalresultsonNTCIRChineseandJapanesefactoidquestionsshowthattheframeworksigniﬁ-cantlyimprovedanswerselectionperformance;Chi-neseperformanceimprovedby40%overthebase-line,andJapaneseperformanceimprovedby45%overthebaseline.Theremainderofthispaperisorganizedasfol-lows:Section2containsanoverviewoftheanswerrankingtask.Section3summarizestheanswerrank-ingframework.InSection4,weexplainhowweextendedtheframeworkbyincorporatinglanguage-speciﬁcfeatures.Section5describestheexperimen-talmethodologyandresults.Finally,Section6con-cludeswithsuggestionsforfutureresearch.2AnswerRankingTaskTherelevanceofananswertoaquestioncanbees-timatedbytheprobabilityP(correct(Ai)|Ai,Q),whereQisaquestionandAiisananswercan-didate.Toexploitanswersimilarity,weestimatetheprobabilityP(correct(Ai)|Ai,Aj),whereAjissimilartoAi.Sincebothprobabilitiesinﬂuenceoverallanswerrankingperformance,itisimportanttocombinetheminauniﬁedframeworkandes-timatetheprobabilityofananswercandidateas:P(correct(Ai)|Q,A1,...,An).Theestimatedprobabilityisusedtorankanswercandidatesandselectﬁnalanswersfromthelist.Forfactoidquestions,thetopanswerisselectedasaﬁ-nalanswertothequestion.Inaddition,wecanusetheestimatedprobabilitytoclassifyincorrectan-swers:iftheprobabilityofananswercandidateislowerthan0.5,itisconsideredtobeawronganswerandisﬁlteredoutoftheanswerlist.Thisisusefulindecidingwhetherornotavalidanswertoaques-tionexistsinagivencorpus(Voorhees,2002).Theestimatedprobabilitycanalsobeusedinconjunc-tionwithacutoffthresholdwhenselectingmultipleanswerstolistquestions.3AnswerRankingFrameworkThissectionsummarizesouranswerrankingframe-work,originallydevelopedforEnglishanswers(Ko786

P(correct(Ai)|Q,A1,...,An)≈P(correct(Ai)|rel1(Ai),...,relK1(Ai),sim1(Ai),...,simK2(Ai))=exp(α0+K1Pk=1βkrelk(Ai)+K2Pk=1λksimk(Ai))1+exp(α0+K1Pk=1βkrelk(Ai)+K2Pk=1λksimk(Ai))where,simk(Ai)=NXj=1(j6=i)sim0k(Ai,Aj).Figure1:Estimatingcorrectnessofananswercandidategivenaquestionandasetofanswercandidatesetal.,2007).Themodeluseslogisticregressiontoestimatetheprobabilityofananswercandidate(Figure1).Eachrelk(Ai)isafeaturefunctionusedtoproduceananswerrelevancescoreforanan-swercandidateAi.Eachsim0k(Ai,Aj)isasimilar-ityfunctionusedtocalculateananswersimilaritybetweenAiandAj.K1andK2arethenumberofanswerrelevanceandanswersimilarityfeatures,re-spectively.Nisthenumberofanswercandidates.Toincorporatemultiplesimilarityfeatures,eachsimk(Ai)isobtainedfromanindividualsimilaritymetric,sim0k(Ai,Aj).Forexample,ifLevenshteindistanceisusedasonesimilaritymetric,simk(Ai)iscalculatedbysummingN-1Levenshteindistancesbetweenoneanswercandidateandallothercandi-dates.Theparametersα,β,λwereestimatedfromtrain-ingdatabymaximizingtheloglikelihood.WeusedtheQuasi-Newtonalgorithm(Minka,2003)forpa-rameterestimation.Multiplefeatureswereusedtogenerateanswerrelevancescoresandanswersimilarityscores;thesearediscussedbelow.3.1AnswerRelevanceFeaturesAnswerrelevancefeaturescanbeclassiﬁedintoknowledge-basedfeaturesordata-drivenfeatures.1)Knowledge-basedfeaturesGazetteers:Gazetteersprovidegeographicinfor-mation,whichallowsustoidentifystringsasin-stancesofcountries,theircities,continents,capitals,etc.Foranswerranking,threegazetteerresourceswereused:theTipsterGazetteer,theCIAWorldFactbookandinformationabouttheUSstatespro-videdby50states.com.Theseresourceswereusedtoassignananswerrelevancescorebetween-1and1toeachcandidate.Forexample,giventhequestion“WhichcityinChinahasthelargestnumberoffor-eignﬁnancialcompanies?”,thecandidate“Shang-hai”receivesascoreof0.5becauseitisacityinthegazetteers.But“Taiwan”receivesascoreof-1.0be-causeitisnotacityinthegazetteers.Ascoreof0meansthegazetteersdidnotcontributetotheanswerselectionprocessforthatcandidate.Ontology:OntologiessuchasWordNetcontaininformationaboutrelationshipsbetweenwordsandgeneralmeaningtypes(synsets,semanticcategories,etc.).WordNetwasusedtoidentifyanswerrele-vanceinamanneranalogoustotheuseofgazetteers.Forexample,giventhequestion“Whowrotethebook’SongofSolomon’?”,thecandidate“MarkTwain”receivesascoreof0.5becauseitshyper-nymsinclude“writer”.2)Data-drivenfeaturesWikipedia:Wikipediawasusedtogenerateanan-swerrelevancescore.IfthereisaWikipediadocu-mentwhosetitlematchesananswercandidate,thedocumentisanalyzedtoobtainthetermfrequency(tf)andtheinversetermfrequency(idf)ofthecan-didate,fromwhichatf.idfscoreiscalculated.Whenthereisnomatcheddocument,eachquestionkey-wordisalsoprocessedasaback-offstrategy,andtheanswerrelevancescoreiscalculatedbysummingthetf.idfscoresobtainedfromindividualkeywords.Google:FollowingMagninietal.(2002),aqueryconsistingofananswercandidateandquestionkey-787

wordswassenttotheGooglesearchengine.Thenthetop10textsnippetsreturnedbyGooglewereanalyzedtogenerateananswerrelevancescorebycomputingtheminimumnumberofwordsbetweenakeywordandtheanswercandidate.3.2AnswerSimilarityFeaturesAnswersimilarityiscalculatedusingmultiplestringdistancemetricsandalistofsynonyms.StringDistanceMetrics:StringdistancemetricssuchasLevenshtein,Jaro-Winkler,andCosinesim-ilaritywereusedtocalculatethesimilaritybetweentwoEnglishanswercandidates.Synonyms:Synonymscanbeusedasanothermetrictocalculateanswersimilarity.Ifoneanswerissynonymofanotheranswer,thescoreis1.Other-wisethescoreis0.Togetalistofsynonyms,threeknowledgebaseswereused:WordNet,WikipediaandtheCIAWorldFactbook.Inaddition,manuallygeneratedruleswereusedtoobtainsynonymsfordifferenttypesofanswercandidates.Forexample,“April121914”and“12thApr.1914”areconvertedinto“1914-04-12”andtreatedassynonyms.4ExtensionsforMultipleLanguagesWeextendedtheframeworkforChineseandJapaneseQA.Thissectiondetailshowweincor-poratedlanguage-speciﬁcresourcesintotheframe-work.Aslogisticregressionisbasedonaproba-bilisticframework,themodeldoesnotneedtobechangedtosupportotherlanguages.Weonlyre-trainedthemodelforindividuallanguages.Tosup-portChineseandJapaneseQA,weincorporatednewfeaturesforindividuallanguages.4.1AnswerRelevanceFeaturesWereplacedtheEnglishgazetteersandWordNetwithlanguage-speciﬁcresourcesforJapaneseandChinese.AsWikipediaandtheWebsupportmul-tiplelanguages,thesamealgorithmwasusedinsearchinglanguage-speciﬁccorporaforthetwolan-guages.1)Knowledge-basedfeaturesTheknowledge-basedfeaturesinvolvesearchingforfactsinaknowledgebasesuchasgazetteersandWordNet.WeutilizedcomparableresourcesforChineseandJapanese.Usinglanguage-speciﬁcre-#ArticlesLanguageNov.2005Aug.2006English1,811,5543,583,699Japanese201,703446,122Chinese69,936197,447Table1:ArticlesinWikipediafordifferentlan-guagessources,thesamealgorithmswereappliedtogener-ateananswerrelevancescorebetween-1and1.Gazetteers:TherearefewavailablegazetteersforChineseandJapanese.Therefore,weextractedlocationdatafromlanguage-speciﬁcresources.ForJapanese,weextractedJapaneselocationinforma-tionfromYahoo2,whichcontainsmanylocationnamesinJapanandtherelationshipsamongthem.ForChinese,weextractedlocationnamesfromtheWeb.Inaddition,wetranslatedcountrynamespro-videdbytheCIAWorldFactbookandtheTipstergazetteersintoChineseandJapanesenames.Asthereismorethanonetranslation,top3translationswereused.Ontology:ForChinese,weusedHowNet(Dong,2000)whichisaChineseversionofWordNet.Itcontains65,000Chineseconceptsand75,000correspondingEnglishequivalents.ForJapanese,weusedsemanticclassesprovidedbyGengoGoiTaikei3.GengoGoiTaikeiisaJapaneselexiconcontaining300,000Japanesewordswiththeirasso-ciated3,000semanticclasses.Thesemanticinfor-mationprovidedbyHowNetandGengoGoiTaikeiwasusedtoassignananswerrelevancescorebe-tween-1and1.2)Data-drivenfeaturesWikipedia:AsWikipediasupportsmorethan200languageeditions,theapproachusedinEnglishcanbeusedfordifferentlanguageswithoutanymodiﬁ-cation.Table1showsthenumberoftextarticlesinthreedifferentlanguages.Wikipedia’scurrentcov-erageinJapaneseandChinesedoesnotmatchitscoverageinEnglish,butcoverageintheselanguagescontinuestoimprove.TosupplementthesmallcorpusofChi-nesedocumentsavailable,weusedBaidu2http://map.yahoo.co.jp/3http://www.kecl.ntt.co.jp/mtg/resources/GoiTaikei788

(http://baike.baidu.com),whichissimilartoWikipediabutcontainsmorearticleswritteninChinese.WeﬁrstsearchforChineseWikipedia.WhenthereisnomatchingdocumentinWikipedia,eachanswercandidateissenttoBaiduandtheretrieveddocumentisanalyzedinthesamewaytoanalyzeWikipediadocuments.Theidfscorewascalculatedusingwordstatis-ticsfromJapaneseYomiurinewspapercorpusandtheNTCIRChinesecorpus.Google:Thesamealgorithmwasappliedtoana-lyzeJapaneseandChinesesnippetsreturnedfromGoogle.ButwerestrictedthelanguagetoChi-neseorJapanesesothatGooglereturnedonlyChi-neseorJapanesedocuments.Tocalculatethedis-tancebetweenananswercandidateandquestionkeywords,segmentationwasdonewithlinguistictools.ForJapanese,Chasen4wasused.ForChinesesegmentation,amaximum-entropybasedparserwasused(Wangetal.,2006).3)ManualFilteringOtherthanthefeaturesmentionedabove,weman-uallycreatedmanyrulesfornumericandtemporalquestionstoﬁlteroutinvalidanswers.Forexample,whenthequestionislookingforayearasananswer,ananswercandidatewhichcontainsonlythemonthreceivesascoreof-1.Otherwise,thescoreis0.4.2AnswerSimilarityFeaturesThesamefeaturesusedforEnglishwereappliedtocalculatethesimilarityofChinese/Japanesean-swercandidates.Toidentifysynonyms,WikipediawereusedforbothChineseandJapanese.EIJIROdictionarywasusedtoobtainJapanesesynonyms.EIJIROisaEnglish-Japanesedictionarycontain-ing1,576,138wordsandprovidessynonymsforJapanesewords.Asthereareseveraldifferentwaystorepresenttemporalandnumericexpressions(Nybergetal.,2002;Greenwood,2006),language-speciﬁcconver-sionruleswereappliedtoconvertthemintoacanon-icalformat;forexample,aruletoconvertJapaneseKanjicharacterstoArabicnumbersisshowninFig-ure2.4http://chasen.aist-nara.ac.jp/hiki/ChaSen0.25四分の一1993-07-041993 年 7 月4 日50 %５割1993-07-04一九九三年七月四日3E+11 円3,000億円3E+11 円三千 億 円Normalized answer stringOriginal answer stringFigure2:Exampleofnormalizedanswerstrings5ExperimentsThissectiondescribestheexperimentstoevaluatetheextendedanswerrankingframeworkforChineseandJapaneseQA.5.1ExperimentalSetupWeusedChineseandJapanesequestionsprovidedbytheNTCIR(NIITestCollectionforIRSys-tems),whichfocusesonevaluatingcross-lingualandmonolingualQAtasksforChinese,JapaneseandEnglish.ForChinese,atotalof550fac-toidquestionsfromtheNTCIR5-6QAevaluationsservedasthedataset.Amongthem,200questionswereusedtotraintheChineseanswerextractorand350questionswereusedtoevaluateouranswerrankingframework.ForJapanese,700questionsfromtheNTCIR5-6QAevaluationsservedasthedataset.Amongthem,300questionswereusedtotraintheJapaneseanswerextractorand400ques-tionswereusedtoevaluateourframework.BoththeChineseandJapaneseanswerextractorsusemaximum-entropytoextractanswercandidatesbasedonmultiplefeaturessuchasnamedentity,de-pendencystructuresandsomelanguage-dependentfeatures.Performanceoftheanswerrankingframeworkwasmeasuredbyaverageansweraccuracy:thenumberofcorrecttopanswersdividedbythenum-berofquestionswhereatleastonecorrectanswerexistsinthecandidatelistprovidedbyanextrac-tor.MeanReciprocalRank(MRR5)wasalsousedtocalculatetheaveragereciprocalrankoftheﬁrstcorrectanswerinthetop5answers.Thebaselineforaverageansweraccuracywascalculatedusingtheanswercandidatelikelihoodscoresprovidedbyeachindividualextractor;the789

TOP1TOP3MRR50.00.10.20.30.40.50.60.70.80.91.0Japanese Answer Selection  Baseline FrameworkTOP1TOP3MRR50.00.10.20.30.40.50.60.70.80.91.0Chinese Answer Selection Avgerage Accuracy Baseline FrameworkFigure3:PerformanceoftheanswerrankingframeworkforChineseandJapaneseanswerselection(TOP1:averageaccuracyoftopanswer,TOP3:averageaccuracyoftop3answers,MRR5:averageofmeanrecip-rocalrankoftop5answers)answerwiththebestextractorscorewaschosen,andnovalidationorsimilarityprocessingwasper-formed.3-foldcross-validationwasperformed,andweusedaversionofWikipediadownloadedinAug2006.5.2ResultsandAnalysisWeﬁrstanalyzedtheaverageaccuracyoftop1,top3andtop5answers.Figure3comparestheaverageaccuracyusingthebaselineandtheanswerselec-tionframework.Ascanbeseen,theanswerrank-ingframeworksigniﬁcantlyimprovedperformanceonbothChineseandJapaneseanswerselection.Asfortheaveragetopansweraccuracy,therewere40%improvementoverthebaseline(Chinese)and45%improvementoverthebaseline(Japanese).Wealsoanalyzedthedegreetowhichtheaverageaccuracywasaffectedbyanswersimilarityandrel-evancefeatures.Table2comparestheaveragetopansweraccuracyusingthebaseline,theanswerrel-evancefeatures,theanswersimilarityfeaturesandallfeaturecombinations.Boththesimilarityandtherelevancefeaturessigniﬁcantlyimprovedanswerse-lectionperformancecomparedtothebaseline,andcombiningbothsetsoffeaturestogetherproducedthebestperformance.Wefurtheranalyzedtheutilityofindividualrele-vancefeatures(Figure4).Forbothlanguages,ﬁlter-ingwasusefulinrulingoutwronganswers.Theim-BaselineRelSimAllChinese0.4420.4820.5970.619Japanese0.3670.4630.5020.532Table2:Averagetopansweraccuracyofindivid-ualfeatures(Rel:mergingrelevancefeatures,Sim:mergingsimilarityfeatures,ALL:mergingallfea-tures).pactoftheontologywasmorepositiveforJapanese;weassumethatthisisbecausetheChineseontol-ogy(HowNet)containsmuchlessinformationover-allthantheJapaneseontology(GengoGoiTaikei).ThecomparativeimpactofWikipediawassimilar.ForChinese,thereweremanyfewerWikipediadoc-umentsavailable.EventhoughweusedBaiduasasupplementalresourceforChinese,thisdidnotim-proveanswerselectionperformance.Ontheotherhand,theuseofWikipediawasveryhelpfulforJapanese,improvingperformanceby26%overthebaseline.Thisshowsthatthequalityofanswerrelevanceestimationissigniﬁcantlyaffectedbyre-sourcecoverage.Whencomparingthedata-drivenfeatureswiththeknowledge-basedfeatures,thedata-drivenfeatures(suchasWikipediaandGoogle)tendedtoincreaseperformancemorethantheknowledge-basedfea-tures(suchasgazetteersandWordNet).Table3showstheeffectofindividualsimilar-ityfeaturesonChineseandJapaneseanswerselec-790

BaselineFILONTGAZGLWIKIAll0.300.350.400.450.500.55Avg. Top Answer Accuracy Chinese JapaneseFigure4:Averagetopansweraccuracyofindivid-ualanswerrelevancefeatures.(FIL:ﬁltering,ONT,ontology,GAZ:gazetteers,GL:Google,WIKI:Wikipedia,ALL:combinationofallrelevancefea-tures)ChineseJapanese0.30.50.30.5Cosine0.5970.5970.4880.488Jaro-Winkler0.5440.5180.4100.415Levenshtein0.5580.5440.4340.449Synonyms0.5270.5270.4930.493All0.5880.5800.5020.488Table3:Averageaccuracyusingindividualsimilar-ityfeaturesunderdifferentthresholds:0.3and0.5(“All”:combinationofallsimilaritymetrics)tion.Assomestringsimilarityfeatures(e.g.,Lev-enshteindistance)produceanumberbetween0and1(where1meanstwostringsareidenticaland0meanstheyaredifferent),similarityscoreslessthanathresholdcanbeignored.Weusedtwothresh-olds:0.3and0.5.Inourexperiments,using0.3asathresholdproducedbetterresultsinChinese.InJapanese,0,5wasabetterthresholdforindivid-ualfeatures.Amongthreedifferentstringsimilar-ityfeatures(Levenshtein,Jaro-WinklerandCosinesimilarity),cosinesimilaritytendedtoperformbet-terthantheothers.Whencomparingsynonymfeatureswithstringsimilarityfeatures,synonymsperformedbetterthanstringsimilarityinJapanese,butnotinChinese.WehadmanymoresynonymsavailableforJapaneseData-drivenfeaturesAllfeaturesChinese0.6060.619Japanese0.5170.532Table4:Averagetopansweraccuracywhenusingdata-drivenfeaturesv.s.whenusingallfeatures.andtheyhelpedthesystemtobetterexploitanswerredundancy.Wealsoanalyzedanswerselectionperformancewhencombiningallfoursimilarityfeatures(“All”inTable3).Combiningallsimilarityfeaturesim-provedtheperformanceinJapanese,buthurttheperformanceinChinese,becauseaddingasmallsetofsynonymstothestringmetricsworsenedtheper-formanceoflogisticregression.5.3Utilityofdata-drivenfeaturesInourexperimentsweuseddata-drivenfea-turesaswellasknowledge-basedfeatures.Asknowledge-basedfeaturesneedmanualefforttoac-cesslanguage-speciﬁcresourcesforindividuallan-guages,weconductedanadditionalexperimentonlywithdata-drivenfeaturesinordertoseehowmuchperformancegainisavailablewithoutthemanualwork.AsGoogle,Wikipediaandstringsimilaritymetricscanbeusedwithoutanyadditionalmanualeffortwhenextendedtootherlanguages,weusedthesethreefeaturesandcomparedtheperformance.Table4showstheperformancewhenusingdata-drivenfeaturesv.s.allfeatures.Itcanbeseenthatdata-drivenfeaturesaloneachievedsigniﬁcantim-provementoverthebaseline.Thisindicatesthattheframeworkcaneasilybeextendedtoanylanguagewhereappropriatedataresourcesareavailable,evenifknowledge-basedfeaturesandresourcesforthelanguagearestillunderdevelopment.6ConclusionInthispaper,wepresentedageneralizedanswerse-lectionframeworkwhichwasappliedtoChineseandJapanesequestionanswering.Anempiricalevalu-ationusingNTCIRtestquestionsshowedthattheframeworksigniﬁcantlyimprovesbaselineanswerselectionperformance.ForChinese,theperfor-manceimprovedby40%overthebaseline.ForJapanese,theperformanceimprovedby45%over791

thebaseline.Thisshowsthatourprobabilisticframeworkcanbeeasilyextendedformultiplelan-guagesbyreusingdata-drivenfeatures(withnewcorpora)andaddinglanguage-speciﬁcresources(ontologies,gazetteers)forknowledge-basedfea-tures.Inourpreviouswork,weevaluatedtheperfor-manceoftheframeworkforEnglishQAusingques-tionsfrompastTRECevaluations(Koetal.,2007).Theexperimentalresultsshowedthatthecombina-tionofallanswerrankingfeaturesimprovedper-formancebyanaverageof102%overthebaseline.Therelevancefeaturesimprovedperformancebyanaverageof99%overthebaseline,andthesimilar-ityfeaturesimprovedperformancebyanaverageof46%overthebaseline.OurhypothesisisthatanswerrelevancefeatureshadagreaterimpactforEnglishQAbecausethequalityandcoverageofthedatare-sourcesavailableforEnglishanswervalidationismuchhigherthanthequalityandcoverageofex-istingresourcesforJapaneseandChinese.Infuturework,wewillcontinuetoevaluatetherobustnessoftheframework.ItisalsoclearfromourcomparisonwithEnglishQAthatmoreworkcanandshouldbedoneinacquiringdataresourcesforanswervalida-tioninChineseandJapanese.AcknowledgmentsWewouldliketothankHidekiShima,MengqiuWang,FrankLin,JustinBetteridge,MatthewBilotti,AndrewSchlaikjerandLuoSifortheirvalu-ablesupport.ThisworkwassupportedinpartbyARDA/DTOAQUAINTprogramawardnumberNBCHC040164.ReferencesD.BuscaldiandP.Rosso.2006.MiningKnowledgefromWikipediafortheQuestionAnsweringtask.InProceedingsoftheInternationalConferenceonLan-guageResourcesandEvaluation.J.Chu-Carroll,J.Prager,C.Welty,K.Czuba,andD.Fer-rucci.2003.AMulti-StrategyandMulti-SourceAp-proachtoQuestionAnswering.InProceedingsofTextREtrievalConference.C.Clarke,G.Cormack,andT.Lynam.2001.Exploitingredundancyinquestionanswering.InProceedingsofSIGIR.ZhendongDong.2000.Hownet:http://www.keenage.com.A.Echihabi,U.Hermjakob,E.Hovy,D.Marcu,E.Melz,andD.Ravichandran.2004.Howtoselectananswerstring?InT.StrzalkowskiandS.Harabagiu,editors,AdvancesinTextualQuestionAnswering.Kluwer.MarkA.Greenwood.2006.Open-DomainQuestionAn-swering.Thesis.S.Harabagiu,D.Moldovan,M.Pasca,R.Mihalcea,M.Surdeanu,R.Bunsecu,R.Girju,V.Rus,andP.Morarescu.2000.Falcon:Boostingknowledgeforanswerengines.InProceedingsofTREC.V.Jijkoun,J.vanRantwijk,D.Ahn,E.TjongKimSang,andM.deRijke.2006.TheUniversityofAmsterdamatCLEF@QA2006.InWorkingNotesCLEF.J.Ko,L.Si,andE.Nyberg.2007.AProbabilisticFrame-workforAnswerSelectioninQuestionAnswering.InProceedingsofNAACL/HLT.B.Magnini,M.Negri,R.Pervete,andH.Tanev.2002.Comparingstatisticalandcontent-basedtechniquesforanswervalidationontheweb.InProceedingsoftheVIIIConvegnoAI*IA.T.Minka.2003.AComparisonofNumericalOptimizersforLogisticRegression.Unpublisheddraft.D.Moldovan,D.Clark,S.Harabagiu,andS.Maiorano.2003.Cogex:Alogicproverforquestionanswering.InProceedingsofHLT-NAACL.E.Nyberg,T.Mitamura,J.Carbonell,J.Callan,K.Collins-Thompson,K.Czuba,M.Duggan,L.Hiyakumoto,N.Hu,Y.Huang,J.Ko,L.Lita,S.Murtagh,V.Pedro,andD.Svoboda.2002.TheJAVELINQuestion-AnsweringSystematTREC2002.InProceedingsofTextREtrievalConference.J.Prager,E.Brown,A.Coden,andD.Radev.2000.Questionansweringbypredictiveannotation.InPro-ceedingsofSIGIR.E.Voorhees.2002.OverviewoftheTREC2002ques-tionansweringtrack.InProceedingsofTextREtrievalConference.M.Wang,K.Sagae,andT.Mitamura.2006.AFast,Ac-curateDeterministicParserforChinese.InProceed-ingsofCOLING/ACL.J.Xu,A.Licuanan,J.May,S.Miller,andR.Weischedel.2002.TREC2002QAatBBN:AnswerSelectionandConﬁdenceEstimation.InProceedingsofTextRE-trievalConference.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 792–799,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

792

LearningtoComposeEffectiveStrategiesfromaLibraryofDialogueComponentsMartijnSpitters†MarcoDeBoni‡JakubZavrel†RemkoBonnema††TextkernelBV,Nieuwendammerkade28/a17,1022ABAmsterdam,NL{spitters,zavrel,bonnema}@textkernel.nl‡UnileverCorporateResearch,ColworthHouse,Sharnbrook,Bedford,UKMK441LQmarco.de-boni@unilever.comAbstractThispaperdescribesamethodforautomat-icallylearningeffectivedialoguestrategies,generatedfromalibraryofdialoguecontent,usingreinforcementlearningfromuserfeed-back.Thislibraryincludesgreetings,so-cialdialogue,chit-chat,jokesandrelation-shipbuilding,aswellasthemoreusualclar-iﬁcationandveriﬁcationcomponentsofdia-logue.Wetestedthemethodthroughamo-tivationaldialoguesystemthatencouragestake-upofexerciseandshowthatitcanbeusedtoconstructgooddialoguestrategieswithlittleeffort.1IntroductionInteractionsbetweenhumansandmachineshavebe-comequitecommoninourdailylife.Manyser-vicesthatusedtobeperformedbyhumanshavebeenautomatedbynaturallanguagedialoguesys-tems,includinginformationseekingfunctions,asintimetableorbankingapplications,butalsomorecomplexareassuchastutoring,healthcoachingandsaleswherecommunicationismuchricher,embed-dingtheprovisionandgatheringofinformationine.g.socialdialogue.Inthelattercategoryofdia-loguesystems,ahighlevelofnaturalnessofinterac-tionandtheoccurrenceoflongerperiodsofsatisfac-toryengagementwiththesystemareaprerequisitefortaskcompletionandusersatisfaction.Typically,suchsystemsarebasedonadialoguestrategythatismanuallydesignedbyanexpertbasedonknowledgeofthesystemandthedomain,andoncontinuousexperimentationwithtestusers.Inthisprocess,theexperthastomakemanyde-signchoiceswhichinﬂuencetaskcompletionandusersatisfactioninamannerwhichishardtoassess,becausetheeffectivenessofastrategydependsonmanydifferentfactors,suchasclassiﬁcation/ASRperformance,thedialoguedomainandtask,and,perhapsmostimportantly,personalitycharacteris-ticsandknowledgeoftheuser.Webelievethatthekeytomaximumdialogueef-fectivenessistolistentotheuser.Thispaperde-scribesthedevelopmentofanadaptivedialoguesys-temthatusesthefeedbackofuserstoautomaticallyimproveitsstrategy.Thesystemstartswithalibraryofgenericandtask-/domain-speciﬁcdialoguecom-ponents,includingsocialdialogue,chit-chat,enter-tainingparts,proﬁlingquestions,andinformativeanddiagnosticparts.Giventhisvarietyofpossi-bledialogueactions,thesystemcanfollowmanydifferentstrategieswithinthedialoguestatespace.Weconductedtrainingsessionsinwhichusersinter-actedwithaversionofthesystemwhichrandomlygeneratesapossibledialoguestrategyforeachin-teraction(restrictedbyglobaldialogueconstraints).Aftereachinteraction,theuserswereaskedtore-warddifferentaspectsoftheconversation.Weap-pliedreinforcementlearningtousethisfeedbacktocomputetheoptimaldialoguepolicy.Thefollowingsectionprovidesabriefoverviewofpreviousresearchrelatedtothisareaandhowourworkdiffersfromthesestudies.Wethenproceedwithaconcisedescriptionofthedialoguesystemusedforourexperimentsinsection3.Section4isaboutthetrainingprocessandtherewardmodel.Section5goesintodetailaboutdialoguepolicyop-793

timizationwithreinforcementlearning.Insection6wediscussourexperimentalresults.2RelatedWorkPreviousworkhasexaminedlearningofeffectivedialoguestrategiesforinformationseekingspo-kendialoguesystems,andinparticulartheuseofreinforcementlearningmethodstolearnpoliciesforactionselectionindialoguemanagement(seee.g.Levinetal.,2000;Walker,2000;SchefﬂerandYoung,2002;PeekandChickering,2005;FramptonandLemon,2006),forselectinginitiativeandcon-ﬁrmationstrategies(Singhetal.,2002);fordetect-ingspeechrecognitionproblem(LitmanandPan,2002);changingthedialogueaccordingtotheex-pertiseoftheuser(MaloorandChai,2000);adapt-ingresponsesaccordingtopreviousinteractionswiththeusers(Rudaryetal.,2004);optimizingmixedinitiativeincollaborativedialogue(EnglishandHeeman,2005),andoptimizingconﬁrmations(Cuay´ahuitletal.,2006).Otherresearchershavefocussedtheirattentiononthelearningaspectofthetask,examining,forexamplehybridreinforce-ment/supervisedlearning(Hendersonetal.,2005).Previousworkonlearningdialoguemanagementstrategieshashowevergenerallybeenlimitedtowelldeﬁnedareasofthedialogue,inparticulardealingwithspeechrecognitionandclariﬁcationproblems,withsmallstatespacesandalimitedsetofactionstochoosefrom(Hendersonetal.,2005).Inanum-berofcontexts,however,dialoguesneedtohaveafargreaterdegreeofcomplexitynotjustinthenum-berofstatesandpossibleactionsbutalsointheva-rietyofdialogueacts:forexampleinmotivationaldialoguesystemswherethetaskisnotlimitedtoinformationgathering,slot-ﬁllingorqueryingofadatabase,andwheredialoguesmustcontainmoresocialandrelationalelementstobesuccessful(fortheusefulnessofsocialdialogueseee.g.Bickmore,2003;LiuandPicard,2005).Onlylittleefforthasbeendirectedtothequestionwhatdialoguecompo-nentsshouldmakeupthedialogue,involvingdeci-sionslikehowmuchandwhattypeofsocialinterac-tionshouldbeused,differentwaysofformingare-lationshipwiththeusersuchasusingchit-chat(forexampleaskingaboutauser’shobbiesoraskingfortheuser’sname),usinghumour,aswellasthemoreconventionaltasksofclarifyinguserinput,estab-lishingcommongroundandensuringsystemrepliesareappropriate.Ourworkhasfocusedontheseas-pectsofdialoguestrategyconstruction,inordertocreategooddialoguestrategiesincorporatingappro-priatelevelsofsocialinteraction,humour,chit-chat,aswellassuccessfulinformationgatheringandpro-vision.3AMotivationalDialogueSystemThedomainofoursystemisphysicalexercise.Thesystemissetupasanexerciseadvisorthataskstheuserwhatispreventinghim/herfromexercis-ingmore.Aftertheuserhaswordedhis/herexercise‘barrier’,thesystemwillgivemotivationaladviceforhowtoovercomethisbarrier.Asanillustration,Table1showsanexampledialogue,generatedbyoursystem.Ourdialoguesystemistext-based,soitdoesnotinvolvespeechrecognition.Thissectiondescribesthesystem’sthreemainmodules:thedi-aloguestate/actionmanager,theutteranceclassiﬁerandthedialoguecomponentlibrary.SpeakerUtterancesystemGoodevening!systemWhat’syourname?usermynameisBrendasystemSoBrenda,howareyoutoday?userﬁnethankyousystemTellmewhatispreventingyoufromexercisingmore.userIdon’thaveanyonetoexercisewith...systemSoyourbarrierisLoneliness(e.g.Youdon’tlikeexercisingonyourown).usersortof,yeahsystemAsolutionforthisbarrieris:Trygoingonyourownandseewhetherit’sreallythatdifferenttogoingwithafriend.Trytalkingtootherpeoplewhoareexercising,you’relikelytoﬁndtheywelcomealittleconversation!systemDidyouﬁndthissolutionuseful?userIguess...systemOk.Trytoworkonthistoovercomeyourbarrier.systemBye!Table1:Exampledialogue.3.1Dialoguestate/actionmanagementThedialoguestateisuniqueateverystageoftheconversationandisrepresentedasavectoroffeature-values.Weuseonlyalimitedsetoffea-turesbecause,asalsonotedin(Singhetal.,2002;Levinetal.,2000),itisimportanttokeepthestatespaceassmallaspossible(butwithenoughdistinc-794

tivepowertosupportlearning)sowecanconstructanon-sparseMarkovdecisionprocess(seesection5)basedonourlimitedtrainingdialogues.ThestatefeaturesarelistedinTable2.FeatureValuesDescriptioncurnodec∈Nthecurrentdialoguenodeactiontypeutt,transactiontypetriggert∈Tutteranceclassiﬁercategoryconﬁdence1,0categoryconﬁdenceproblem1,0communicationproblemearlierTable2:Dialoguestatefeatures.Ineachdialoguestate,thedialoguemanagerwilllookupthenextactionthatshouldbetaken.Inoursystem,anactioniseitherasystemutteranceoratransitioninthedialoguestructure.Intheinitialsystem,thedialoguestructurewasmanuallycon-structed.Inmanystates,thenextactionrequiresachoicetobemade.Dialoguestatesinwhichthesystemcanchooseamongseveralpossibleactionsarecalledchoice-states.Forexample,inoursys-tem,immediatelyaftergreetingtheuser,thedia-loguestructureallowsfordifferentdirections:thesystemcanﬁrstasksomepersonalquestions,oritcanimmediatelydiscussthemaintopicwithoutanydigressions.Utteranceactionsmayalsore-quireachoice(e.g.directiveversusopenformula-tionofaquestion).Intrainingmode,thesystemwillmakerandomchoicesinthechoice-states.Thisap-proachwillgeneratemanydifferentdialoguestrate-gies,i.e.pathsthroughthedialoguestructure.Userrepliesaresenttoanutteranceclassiﬁer.Thecategoryassignedbythisclassiﬁerisreturnedtothedialoguemanagerandtriggersatransitiontothenextnodeinthedialoguestructure.Thesystemalsoaccommodatesasimplerule-basedextractionmod-ule,whichcanbeusedtoextractinformationfromuserutterances(e.g.theuser’sname,whichistem-platedinsubsequentsystempromptsinordertoper-sonalizethedialogue).3.2UtteranceclassiﬁcationThe(memory-based)classiﬁerusesarichsetoffea-turesforaccurateclassiﬁcation,includingwords,phrases,regularexpressions,domain-speciﬁcword-relations(usingataxonomy-plugin)andsyntacti-callymotivatedexpressions.Forutterancepars-ingweusedamemory-basedshallowparser,calledMBSP(Daelemansetal.,1999).Thisparserpro-videspartofspeechlabels,chunkbrackets,subject-verb-objectrelations,andhasbeenenrichedwithde-tectionofnegationscopeandclauseboundaries.Thefeature-matchingmechanisminourclassiﬁ-cationsystemcanmatchtermsorphrasesatspeci-ﬁedpositionsinthetokenstreamoftheutterance,alsoincombinationwithsyntacticandsemanticclasslabels.Thisallowsustodeﬁnefeaturesthatareparticularlyusefulforresolvingconfusinglinguis-ticphenomenalikeambiguityandnegation.Abasefeaturesetwasgeneratedautomatically,butquitealotoffeaturesweremanuallytunedoraddedtocopewithcertaincommondialoguesituations.Theoverallclassiﬁcationaccuracy,measuredonthedia-loguesthatwereproducedduringthetrainingphase,is93.6%.Averageprecision/recallis98.6/97.3%forthenon-barriercategories(conﬁrmation,negation,unwillingness,etc.),and99.1/83.4%forthebarriercategories(injury,lackofmotivation,etc.).3.3DialogueComponentLibraryThedialoguecomponentlibrarycontainsgenericaswellastask-/domain-speciﬁcdialoguecontent,combiningdifferentaspectsofdialogue(task/topicstructure,communicationgoals,etc.).Table3listsallcomponentsinthelibrarythatwasusedfortrain-ingourdialoguesystem.Adialoguecomponentisbasicallyacoherentsetofdialoguenoderepresen-tationswithacertaindialoguefunction.Thelibraryissetupinaﬂexible,genericway:newcomponentscaneasilybepluggedintotesttheirusefulnessindifferentdialoguecontextsorfornewdomains.4TrainingtheDialogueSystem4.1RandomstrategygenerationInitstrainingmode,thedialoguesystemusesran-domexploration:itgeneratesdifferentdialoguestrategiesbychoosingrandomlyamongtheallowedactionsinthechoice-states.Notethatdialoguegen-erationisconstrainedtocontaincertainﬁxedactionsthatareessentialfortaskcompletion(e.g.askingtheexercisebarrier,givingasolution,closingtheses-sion).Thisexcludesavastnumberofuselessstrate-giesfromexplorationbythesystem.Still,givenallactionchoicesandpossibleuserreactions,thetotalnumberofuniquedialoguesthatcanbegeneratedby795

ComponentDescriptionpapeStartSessionDialogueopenings,includingvariousgreetings••PersonalQuestionnairePersonalquestions,e.g.name;age;hobbies;interests,howareyoutoday?•ElizaChitChatEliza-likechit-chat,e.g.pleasegoon...ExerciseChitChatChit-chataboutexercise,e.g.haveyoubeendoinganyexercisethisweek?◦BarrierPromptsconcerningthebarrier,e.g.askthebarrier;barrierveriﬁcation;askarephrase••SolutionPromptsconcerningthesolution,e.g.givethesolution;verifyusefulness••GiveBeneﬁtsTalkaboutthebeneﬁtsofexercisingAskCommitmentAskusertocommithisimplementationofthegivensolution•EncourageEncouragetheusertoworkonthegivensolution••GiveJokeThehumorcomponent:askiftheuserwantstohearajoke;tellrandomjokes◦•VerifyCloseSessionVeriﬁcationforclosingthesession(areyousureyouwanttoclosethissession?)◦◦CloseSessionDialogueendings,includingvariousfarewells••Table3:Componentsinthedialoguecomponentlibrary.Thelasttwocolumnsshowwhichofthecompo-nentswasusedinthelearnedpolicy(pa)andtheexpertpolicy(pe),discussedinsection6.•meansthecomponentisalwaysused,◦meansitissometimesused,dependingonthedialoguestate.thesystemisapproximately345000(manyofwhichareunlikelytoeveroccur).Duringtraining,thesys-temgenerated490differentstrategies.Thereare71choice-statesthatcanactuallyoccurinadialogue.Inourtrainingdialogues,theopeningstatewasob-viouslyvisitedmostfrequently(572times),almost60%ofallstateswasvisitedatleast50times,andonly16stateswerevisitedlessthan10times.4.2TherewardmodelWhenthedialoguehasreacheditsﬁnalstate,asur-veyispresentedtotheuserfordialogueevaluation.Thesurveyconsistsofﬁvestatementsthatcaneachberatedonaﬁve-pointscale(indicatingtheuser’slevelofagreement).Theresponsesaremappedtorewardsof-2to2.Thestatementsweusedarepartlybasedontheusersurveythatwasusedin(Singhetal.,2002).Weconsideredthesestatementstoreﬂectthemostimportantaspectsofconversationthatarerelevantforlearningagooddialoguepolicy.Theﬁvestatementsweusedarelistedbelow.M1Overall,thisconversationwentwellM2ThesystemunderstoodwhatIsaidM3IknewwhatIcouldsayateachpointinthedialogueM4IfoundthisconversationengagingM5Thesystemprovidedusefuladvice4.3Trainingset-upEightsubjectscarriedoutatotalof572conversa-tionswiththesystem.Becauseofthevarietyofpos-sibleexercisebarriersknownbythesystem(52intotal)andthefactthatsomeofthesebarriersaremorecomplexorhardertodetectthanothers,thesystem’sclassiﬁcationaccuracydependslargelyontheuser’sbarrier.Topreventclassiﬁcationaccuracydistortingtheuserevaluations,weaskedthesubjectstoactasiftheyhadoneofﬁvepredeﬁnedexercisebarriers(e.g.Imaginethatyoudon’tfeelcomfort-ableexercisinginpublic.Seewhattheadvisorrec-ommendsforthisbarriertoyourexercise).5DialoguePolicyOptimizationwithReinforcementLearningReinforcementlearningreferstoaclassofmachinelearningalgorithmsinwhichanagentexploresanenvironmentandtakesactionsbasedonitscurrentstate.Incertainstates,theenvironmentprovidesareward.Reinforcementlearningalgorithmsat-tempttoﬁndtheoptimalpolicy,i.e.thepolicythatmaximizescumulativerewardfortheagentoverthecourseoftheproblem.Inourcase,apolicycanbeseenasamappingfromthedialoguestatestothepossibleactionsinthosestates.TheenvironmentistypicallyformulatedasaMarkovdecisionprocess(MDP).Theideaofusingreinforcementlearningtoau-tomatethedesignofstrategiesfordialoguesystemswasﬁrstproposedbyLevinetal.(2000)andhassubsequentlybeenappliedina.o.(Walker,2000;Singhetal.,2002;FramptonandLemon,2006;Williamsetal.,2005).5.1MarkovdecisionprocessesWefollowpastlinesofresearch(suchasLevinetal.,2000;Singhetal.,2002)byrepresentingadia-logueasatrajectoryinthestatespace,determined796

bytheuserresponsesandsystemactions:s1a1,r1−−−→s2a2,r2−−−→...snan,rn−−−→sn+1,inwhichsiai,ri−−−→si+1meansthatthesystemperformedactionaiinstatesi,received1rewardriandchangedtostatesi+1.Inoursystem,astateisadialoguecontextvectoroffeaturevalues.Thisfeaturevectorcontainstheavailableinformationaboutthedialoguesofarthatisrelevantfordecidingwhatactiontotakenextinthecurrentdialoguestate.Wewantthesystemtolearntheoptimaldecisions,i.e.tochoosetheactionsthatmaximizetheexpectedreward.5.2Q-valueiterationTheﬁeldofreinforcementlearningincludesmanyalgorithmsforﬁndingtheoptimalpolicyinanMDP(seeSuttonandBarto,1998).Weappliedthealgo-rithmof(Singhetal.,2002),astheirexperimentalset-upissimilartoours,constistingof:generationof(limited)exploratorydialoguedata,usingatrain-ingsystem;creatinganMDPfromthesedataandtherewardsassignedbythetrainingusers;off-linepolicylearningbasedonthisMDP.TheQ-functionforacertainactiontakeninacer-tainstatedescribesthetotalrewardexpectedbe-tweentakingthatactionandtheendofthedialogue.Foreachstate-actionpair(s,a),wecalculatedthisexpectedcumulativerewardQ(s,a)oftakingactionafromstates,withthefollowingequation(SuttonandBarto,1998;Singhetal.,2002):Q(s,a)=R(s,a)+γXs′P(s′|s,a)maxa′Q(s′,a′)(1)where:P(s′|s,a)istheprobabilityofatransitionfromstatestostates′bytakingactiona,andR(s,a)istheexpectedrewardobtainedwhentak-ingactionainstates.γisaweight(0≤γ≤1),thatdiscountsrewardsobtainedlaterintimewhenitissettoavalue<1.Inoursystem,γwassetto1.Equation1isrecursive:theQ-valueofacer-tainstateiscomputedintermsoftheQ-valuesofitssuccessorstates.TheQ-valuescanbeestimatedtowithinadesiredthresholdusingQ-valueiteration(SuttonandBarto,1998).Oncethevalueiteration1Inourexperiments,wedidnotmakeuseofimmediatere-warding(e.g.ateveryturn)duringtheconversation.Rewardsweregivenaftertheﬁnalstateofthedialoguehadbeenreached.processiscompleted,byselectingtheactionwiththemaximumQ-value(themaximumexpectedfu-turereward)ateachchoice-state,wecanobtaintheoptimaldialoguepolicyπ.6ResultsandDiscussion6.1RewardanalysisFigure1showsagraphofthedistributionoftheﬁvedifferentevaluationmeasuresinthetrainingdata(seesection4.2forthestatementwordings).M1isprobablythemostimportantmeasureofsuccess.Thedistributionofthisrewardisquitesymmetri-cal,withaslightlyhigherpeakinthepositivearea.ThedistributionofM2showsthatM1andM2arerelated.FromthedistributionofM4wecancon-cludethatthemajorityofdialoguesduringthetrain-ingphasewasnotveryengaging.Usersobviouslyhadagoodfeelingaboutwhattheycouldsayateachpointinthedialogue(M3),whichimpliesgoodqual-ityofthesystemprompts.Thejudgementabouttheusefulnessoftheprovidedadviceisprettyaverage,tendingabitmoretonegativethantopositive.Wedothinkthatthismeasuremightbedistortedbythefactthatweaskedthesubjectstoimaginethattheyhavethegivenexercisebarriers.Furthermore,theyweresometimesconfrontedwithadvicethathadal-readybeenpresentedtotheminearlierconversa-tions. 0 50 100 150 200 250-2-1 0 1 2Number of dialoguesRewardReward distributionsM1M2M3M4M5Figure1:Rewarddistributionsinthetrainingdata.Inouranalysisoftheusers’rewardingbehavior,wefoundseveralsigniﬁcantcorrelations.Wefoundthatlongerdialogues(>3userturns)areappreci-atedmorethanshortones(<4userturns),whichseemsratherlogical,asdialoguesinwhichtheuser797

barelygetstosayanythingareneithernaturalnorengaging.Wealsolookedattherelationshipbetweenuserinputveriﬁcationandthegivenrewards.Ourintu-itionisthatthechoiceofbarrierveriﬁcationisoneofthemostimportantchoicesthesystemcanmakeinthedialogue.Wefoundthatitismuchbettertoﬁrstverifythedetectedbarrierthantoimmediatelygiveadvice.Thepercentageofappropriateadviceprovidedindialogueswithbarrierveriﬁcationissig-niﬁcantlyhigherthanindialogueswithoutveriﬁca-tion.Inseveralstatesofthedialogue,weletthesys-temchoosefromdifferentwordingsofthesystemprompt.Oneofthesechoicesiswhethertouseanopenquestiontoaskwhattheuser’sbarrieris(HowcanIhelpyou?),oradirectivequestion(Tellmewhatispreventingyoufromexercisingmore.).Themotivationbehindtheopenquestionisthattheusergetstheinitiativeandisbasicallyfreetotalkaboutanythinghe/shelikes.Naturally,theadvantageofdirectivequestionsisthatthechanceofmakingclas-siﬁcationerrorsismuchlowerthanwithopenques-tionsbecausetheuserwillbebetterabletoassesswhatkindofanswerthesystemexpects.Dialoguesinwhichthekey-question(askingtheuser’sbarrier)wasdirective,wererewardedmorepositivelythandialogueswiththeopenquestion.6.2LearneddialoguepoliciesWelearnedadifferentpolicyforeachevaluationmeasureseparately(byonlyusingtherewardsgivenforthatparticularmeasure),andapolicybasedonacombination(sum)oftherewardsforallevalu-ationmeasures.Wefoundthatthelearnedpolicybasedonthecombinationofallmeasures,andthepolicybasedonmeasureM1alone(Overall,thisconversationwentwell)werenearlyidentical.Ta-ble4comparesthemostimportantdecisionsofthedifferentpolicies.Forconvenienceofcomparison,weonlylistedthemain,structuralchoices.Table3showswhichofthedialoguecomponentsintheli-brarywereusedinthelearnedandtheexpertpolicy.Notethat,forthesakeofclarity,thestatedescrip-tionsinTable4arebasicallysummariesofasetofmorespeciﬁcstatessinceastateisaspeciﬁcrepre-sentationofthedialoguecontextataparticularmo-ment(composedofthevaluesofthefeatureslistedinTable2).Forinstance,inthepapolicy,thedeci-sioninthelastrowofthetable(giveajokeornot),dependsonwhetherornottherehasbeenaclassiﬁ-cationfailure(i.e.acommunicationproblemearlierinthedialogue).Iftherehasbeenaclassiﬁcationfailure,thepolicyprescribesthedecisionnottogiveajoke,asitwasnotappreciatedbythetrainingusersinthatcontext.Otherwise,iftherewerenocommu-nicationproblemsduringtheconversation,theusersdidappreciateajoke.6.3EvaluationWecomparedthelearneddialoguepolicywithapol-icywhichwasindependentlyhand-designedbyex-perts2forthissystem.Thedecisionsmadeinthelearnedstrategywereverysimilartotheonesmadebytheexperts,withonlyafewdifferences,indicat-ingthattheautomatedmethodwouldindeedper-formaswellasanexpert.Themaindifferencesweretheinclusionofapersonalquestionnaireforre-lationbuildingatthebeginningofthedialogueandacommitmentquestionattheendofthedialogue.Anotherdifferencewasthemorerestricteduseofthehumourelement,describedinsection6.2whichturnsouttobeintuitivelybetterthantheexpert’sde-cisiontosimplyalwaysincludeajoke.Ofcourse,wecanonlydrawconclusionswithregardtotheef-fectivenessofthesetwopoliciesifweempiricallycomparethemwithrealtestusers.Suchevaluationsareplannedaspartofourfutureresearch.Assomeadditionalevidenceagainstthepossibil-itythatthelearnedpolicywasgeneratedbychance,weperformedasimpleexperimentinwhichwetookseveralrandomsamplesof300trainingdialoguesfromthecompletetrainingset.Foreachsample,welearnedtheoptimalpolicy.Wemutuallycomparedthesepoliciesandfoundthattheywereverysimilar:onlyin15-20%ofthestates,thepoliciesdisagreedonwhichactiontotakenext.Oncloserinspectionwefoundthatthisdisagreementmainlyconcernedstatesthatwerepoorlyvisited(1-10times)inthesesamples.Theseresultssuggestthatthelearnedpol-icyisunreliableatinfrequentlyvisitedstates.Notehowever,thatallmaindecisionslistedinTable4are2Theexpertswereateammadeupofpsychologistswithexperienceinthepsychologyofhealthbehaviourchangeandascientistwithexperienceinthedesignofautomateddialoguesystems.798

StatedescriptionActionchoicesp1p2p3p4p5papeAftergreetingtheuser-asktheexercisebarrier•••-askpersonalinformation••••-chit-chataboutexerciseWhenaskingthebarrier-useadirectivequestion•••••••-useanopenquestionUsergivesexercisebarrier-verifydetectedbarrier•••••••-givesolutionUserrephrasedbarrier-verifydetectedbarrier••••••-givesolution•Beforepresentingsolution-askiftheuserwantstoseeasolutionforthebarrier•-giveasolution••••••Afterpresentingsolution-verifysolutionusefulness••••••-encouragetheusertoworkonthegivensolution•-askusertocommitsolutionimplementationUserfoundsolutionuseful-encouragetheusertoworkonthesolution••••-askusertocommitsolutionimplementation•••Userfoundsolutionnotuseful-giveanothersolution•••••••-asktheuserwantstoproposehisownsolutionAftergivingsecondsolution-verifysolutionusefulness••-encouragetheusertoworkonthegivensolution••••-askusertocommitsolutionimplementation•Endofdialogue-closethesession•••-askiftheuserwantstohearajoke••••Table4:Comparisonofthemostimportantdecisionsmadebythelearnedpolicies.pnisthepolicybasedonevaluationmeasuren;paisthepolicybasedonallmeasures;pecontainsthedecisionsmadebyexpertsinthemanuallydesignedpolicy.madeatfrequentlyvisitedstates.Theonlydisagree-mentinfrequentlyvisitedstatesconcernedsystem-promptchoices.Wemightconcludethatthesepar-ticular(oftenverysubtle)system-promptchoices(e.g.carefulversusdirectformulationoftheexercisebarrier)arehardertolearnthanthemorenoticabledialoguestructure-relatedchoices.7ConclusionsandFutureWorkWehaveexploredreinforcementlearningforauto-maticdialoguepolicyoptimizationinaquestion-basedmotivationaldialoguesystem.Oursystemcanautomaticallycomposeadialoguestrategyfromali-braryofdialoguecomponents,thatisverysimilartoamanuallydesignedexpertstrategy,bylearningfromuserfeedback.Thus,inordertobuildanewdialoguesystem,dialoguesystemengineerswillhavetosetuparoughdialoguetemplatecontainingseveral‘multi-plechoice’-actionnodes.Atthesenodes,variousdialoguecomponentsorpromptwordings(e.g.en-tertainingparts,clariﬁcationquestions,socialdia-logue,personalquestions)fromanexistingorself-madelibrarycanbepluggedinwithoutknowingbe-forehandwhichofthemwouldbemosteffective.Theautomaticallygenerateddialoguepolicyisverysimilar(seeTable4)–butarguablyimprovedinmanydetails–tothehand-designedpolicyforthissystem.Automaticallylearningdialoguepoliciesalsoallowsustotestanumberofinterestingissuesinparallel,forexample,wehavelearnedthatusersappreciateddialoguesthatwerelonger,startingwithsomepersonalquestions(e.gWhatisyourname?,Whatareyourhobbies?).Wethinkthataltogether,thisrelationbuildingcomponentgavethedialogueamorenaturalandengagingcharacter,althoughitwasleftoutintheexpertstrategy.Wethinkthatthemethodologydescribedinthispapermaybeabletoyieldmoreeffectivedialoguepoliciesthanexperts.Especiallyincomplicateddi-aloguesystemswithlargestatespaces.Inoursys-tem,staterepresentationsarecomposedofmultiplecontextfeaturevalues(e.g.communicationproblemearlierinthedialogue,theconﬁdenceoftheutter-anceclassiﬁer).Ourexperimentsshowedthatsome-timesdifferentdecisionswerelearnedindialoguecontextswhereonlyoneofthesefeatureswasdiffer-ent(forexampleusehumouronlyifthesystemhasbeensuccessfulinrecognisingauser’sexercisebar-rier):allcontextfeaturesareimplicitlyusedtolearntheoptimaldecisionsandwhenhand-designingadi-799

aloguepolicy,expertscanimpossiblytakeintoac-countallpossibledifferentdialoguecontexts.Withrespecttofuturework,weplantoexaminetheimpactofdifferentstaterepresentations.Wedidnotyetempiricallycomparetheeffectsofeachfea-tureonpolicylearningorexperimentwithotherfea-turesthantheoneslistedinTable2.AsTetreaultandLitman(2006)show,incorporatingmoreordifferentinformationintothestaterepresentationmighthow-everresultindifferentpolicies.Furthermore,wewillevaluatetheactualgeneric-ityofourapproachbyapplyingittodifferentdo-mains.Aspartofthat,wewilllookatautomaticallymininglibrariesofdialoguecomponentsfromex-istingdialoguetranscriptdata(e.g.availablescriptsortranscriptsofﬁlms,tvseriesandinterviewscon-tainingreal-lifeexamplesofdifferenttypesofdia-logue).Thesecomponentscanthenbepluggedintoourcurrentadaptivesysteminordertodiscoverwhatworksbestindialoguefornewdomains.Weshouldnoteherethatextendingthesystem’sdialoguecom-ponentlibrarywillautomaticallyincreasethestatespaceandthuspolicygenerationandoptimizationwillbecomemoredifﬁcultandrequiremoretrain-ingdata.Itwillthereforebeveryimportanttocare-fullycontrolthesizeofthestatespaceandtheglobalstructureofthedialogue.AcknowledgementsTheauthorswouldliketothankPiroskaLendvaiRudenko,WalterDaelemans,andBobHurlingfortheircontributionsandhelpfulcomments.Wealsothanktheanonymousreviewersfortheirusefulcom-mentsontheinitialversionofthispaper.ReferencesTimothyW.Bickmore.2003.RelationalAgents:Ef-fectingChangethroughHuman-ComputerRelationships.Ph.D.Thesis,MIT,Cambridge,MA.HeribertoCuay´ahuitl,SteveRenals,OliverLemon,andHiroshiShimodaira.2006.Learningmulti-goaldialoguestrate-giesusingreinforcementlearningwithreducedstate-actionspaces.ProceedingsofInterspeech-ICSLP.WalterDaelemans,SabineBuchholz,andJornVeenstra.1999.Memory-BasedShallowParsing.ProceedingsofCoNLL-99,Bergen,Norway.MichaelS.EnglishandPeterA.Heeman2005.Learn-ingMixedInitiativeDialogStrategiesByUsingReinforce-mentLearningOnBothConversants.ProceedingsofHLT/NAACL.MatthewFramptonandOliverLemon.2006.LearningMoreEffectiveDialogueStrategiesUsingLimitedDialogueMoveFeatures.ProceedingsoftheAnnualMeetingoftheACL.JamesHenderson,OliverLemon,andKallirroiGeorgila.2005.HybridReinforcement/SupervisedLearningforDialoguePoliciesfromCOMMUNICATORData.IJCAIworkshoponKnowledgeandReasoninginPracticalDialogueSystems.EstherLevin,RobertoPieraccini,andWielandEckert.2000.AStochasticModelofHuman-MachineInteractionforLearn-ingDialogStrategies.IEEETrans.onSpeechandAudioProcessing,Vol.8,No.1,pp.11-23.DianeJ.LitmanandShimeiPan.2002.DesigningandEval-uatinganAdaptiveSpokenDialogueSystem.UserModel-ingandUser-AdaptedInteraction,Volume12,Issue2-3,pp.111-137.KarenK.LiuandRosalindW.Picard.2005.EmbeddedEm-pathyinContinuous,InteractiveHealthAssessment.CHIWorkshoponHCIChallengesinHealthAssessment,Port-land,Oregon.PreetamMaloorandJoyceChai.2000.DynamicUserLevelandUtilityMeasurementforAdaptiveDialoginaHelp-DeskSystem.Proceedingsofthe1stSigdialWorkshop.TimPaekandDavidM.Chickering.2005.TheMarkovAs-sumptioninSpokenDialogueManagement.ProceedingsofSIGDIAL2005.MatthewRudary,SatinderSingh,andMarthaE.Pollack.2004.Adaptivecognitiveorthotics:Combiningreinforce-mentlearningandconstraint-basedtemporalreasoning.Pro-ceedingsofthe21stInternationalConferenceonMachineLearning.KonradSchefﬂerandSteveYoung.2002.Automaticlearningofdialoguestrategyusingdialoguesimulationandreinforce-mentlearning.ProceedingsofHLT-2002.SatinderSingh,DianeLitman,MichaelKearns,andMarilynWalker.2002.OptimizingDialogueManagementwithRe-inforcementLearning:ExperimentswiththeNJFunSystem.JournalofArtiﬁcialIntelligenceResearch(JAIR),Volume16,pages105-133.RichardS.SuttonandAndrewG.Barto.1998.ReinforcementLearning.MITPress.JoelR.TetreaultandDianeJ.Litman2006.ComparingtheUtilityofStateFeaturesinSpokenDialogueUsingRe-inforcementLearning.ProceedingsofHLT/NAACL,NewYork.MarilynA.Walker2000.AnApplicationofReinforcementLearningtoDialogueStrategySelectioninaSpokenDia-logueSystemforEmail.JournalofArtiﬁcialIntelligenceResearch,Vol12.,pp.387-416.JasonD.Williams,PascalPoupart,andSteveYoung.2005.PartiallyObservableMarkovDecisionProcesseswithCon-tinuousObservationsforDialogueManagement.Proceed-ingsofthe6thSigDialWorkshop,September2005,Lisbon.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 800–807,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

800

Ontheroleofcontextandprosodyintheinterpretationof‘okay’Agust´ınGravano,StefanBenus,H´ectorCh´avez,JuliaHirschberg,LaurenWilcoxDepartmentofComputerScienceColumbiaUniversity,NewYork,NY,USA{agus,sbenus,hrc2009,julia,lgw23}@cs.columbia.eduAbstractWeexaminetheeffectofcontextualandacousticcuesinthedisambiguationofthreediscourse-pragmaticfunctionsofthewordokay.Resultsofaperceptionstudyshowthatcontextualcuesarestrongerpredictorsofdiscoursefunctionthanacousticcues.However,acousticfeaturescapturingthepitchexcursionattherightedgeofokayfea-tureprominentlyindisambiguation,whetherothercontextualcuesarepresentornot.1IntroductionCUEPHRASES(alsoknownasDISCOURSEMARK-ERS)arelinguisticexpressionsthatcanbeusedtoconveyexplicitinformationaboutthestructureofadiscourseortoconveyasemanticcontribution(GroszandSidner,1986;Reichman,1985;Cohen,1984).Forexample,thewordokaycanbeusedtoconveya‘satisfactory’evaluationofsomeentityinthediscourse(themoviewasokay);asabackchan-nelinadialoguetoindicatethatoneinterlocutorisstillattendingtoanother;toconveyacknowledg-mentoragreement;or,inits‘cue’use,tostartor(cid:2)n-ishadiscoursesegment(Jefferson,1972;SchegloffandSacks,1973;Kowtko,1997;WardandTsuka-hara,2000).Amajorquestionishowspeakersindi-cateandlistenersinterpretsuchvariationinmean-ing.Fromapracticalperspective,understandinghowspeakersandlistenersdisambiguatecuephrasesisimportanttospokendialoguesystems,sothatsys-temscanconveypotentiallyambiguoustermswiththeirintendedmeaningandcaninterpretuserinputcorrectly.Thereisconsiderableevidencethatthedifferentusesofindividualcuephrasescanbedistinguishedbyvariationintheprosodywithwhichtheyarere-alized.Forexample,(HirschbergandLitman,1993)foundthatcuephrasesingeneralcouldbedisam-biguatedbetweentheir‘semantic’andtheir‘dis-coursemarker’usesintermsofthetypeofpitchaccentbornebythecuephrase,thepositionofthephraseintheintonationalphrase,andtheamountofadditionalinformationinthephrase.Despitethefrequenceofthewordokayinnaturaldialogues,relativelylittleattentionhasbeenpaidtotherela-tionshipbetweenitsuseanditsprosodicrealization.(Hockey,1993)did(cid:2)ndthatokaydiffersintermsofthepitchcontourspeakersuseinutteringit,suggest-ingthata(cid:2)nalrisingpitchcontour(cid:147)categoricallymarksaturnchange,(cid:148)whileadownsteppedfallingpitchcontourusuallyindicatesadiscoursesegmentboundary.However,itisnotclearwhich,ifany,oftheprosodicdifferencesidenti(cid:2)edinthisstudyareactuallyusedbylistenersininterpretingthesepo-tentiallyambiguousitems.Inthisstudy,weaddressthequestionofhowhear-ersdisambiguatetheinterpretationofokay.Ourgoalistoidentifytheacoustic,prosodicandphoneticfea-turesofokaytokensforwhichlistenersassigndiffer-entmeanings.Additionally,wewanttodeterminetherolethatdiscoursecontextplaysinthisclassi-(cid:2)cation:i.e.,cansubjectsclassifyokaytokensreli-ablyfromthewordaloneordotheyrequireaddi-tionalcontext?Belowwedescribeaperceptionstudyinwhichlistenerswerepresentedwithanumberofspokenproductionsofokay,takenfromacorpusofdia-loguesbetweensubjectsplayingacomputergame.Thetokenswerepresentedbothinisolationandincontext.Userswereaskedtoselectthemeaning801

ofeachtokenfromthreeofthemeaningsthatokaycantakeon:ACKNOWLEDGEMENT/AGREEMENT,BACKCHANNEL,andCUEOFANINITIALDIS-COURSESEGMENT.Subsequently,weexaminedtheacoustic,prosodicandphoneticcorrelatesoftheseclassi(cid:2)cationstotrytoinferwhatcueslistenersusedtointerpretthetokens,andhowthesevariedbycon-textcondition.Section2describesourcorpus.Sec-tion3describestheperceptionexperiment.InSec-tion4weanalyzeinter-subjectagreement,introduceanovelrepresentationofsubjectjudgments,andex-aminetheacoustic,prosodic,phoneticandcontex-tualcorrelatesofsubjectclassi(cid:2)cationofokays.InSection5wediscussourresultsandfuturework.2CorpusThematerialsforourperceptionstudywereselectedfromaportionoftheColumbiaGamesCorpus,acollectionof12spontaneoustask-orienteddyadicconversationselicitedfromspeakersofStandardAmericanEnglish.ThecorpuswascollectedandannotatedjointlybytheSpokenLanguageGroupatColumbiaUniversityandtheDepartmentofLin-guisticsatNorthwesternUniversity.Subjectswerepaidtoplaytwoseriesofcom-putergames(theCARDSGAMESandtheOBJECTSGAMES),requiringcollaborationbetweenpartnerstoachieveacommongoal.Participantssatinfrontoflaptopsinasoundproofboothwithacurtainbe-tweenthem,sothatallcommunicationwouldbever-bal.Eachplayerplayedwithtwodifferentpartnersintwodifferentsessions.Onaverage,eachsessiontook45m39s,totalling9h8mofdialogueforthewholecorpus.Allinteractionswererecorded,digi-tized,anddownsampledto16K.Therecordingswereorthographicallytranscribedandwordswerealignedbyhandbytrainedannota-torsinaToBI(BeckmanandHirschberg,1994)or-thographictierusingPraat(BoersmaandWeenink,2001)tomanipulatewaveforms.Thecorpuscon-tains2239uniquewords,with73,831wordsintotal.NearlyalloftheObjectsGamespartofthecorpushasbeenintonationallytranscribed,usingtheToBIconventions.Pitch,energyanddurationinformationhasbeenextractedfortheentirecorpusautomati-cally,usingPraat.IntheObjectsGamesportionofthecorpuseachplayer’slaptopdisplayedagameboardcontaining5(cid:150)7objects(Figure1).Ineachsegmentofthegame,bothplayerssawthesamesetofobjectsatthesamepositiononeachscreen,exceptforoneobject(theTARGET).Foroneplayer(theDESCRIBER),thistar-getappearedinarandomlocationamongotherob-jectsonthescreen.Fortheotherplayer(theFOL-LOWER),thetargetobjectappearedatthebottomofthescreen.Thedescriberwasinstructedtodescribethepositionofthetargetobjectontheirscreensothatthefollowercouldmovetheirrepresentationofthetargettothesamelocationontheirownscreen.Aftertheplayershadnegotiatedwhattheydeter-minedtobethebestlocation,theywereawardedupto100pointsbasedontheactualmatchofthetargetlocationonthetwoscreens.Thegamepro-ceededinthiswaythrough14tasks,withdescriberandfolloweralternatingroles.Onaverage,theOb-jectsGamesportionofeachsessiontook21m36s,resultingin4h19mofdialogueforthetwelveses-sionsinthecorpus.Thereare1484uniquewordsinthisportionofthecorpus,and36,503wordsintotal.Figure1:SamplescreenoftheObjectsGames.ThroughouttheObjectsGames,wenotedthatsubjectsmadefrequentuseofaf(cid:2)rmativecuewords,suchasokay,yeah,alright,whichappearedtovaryinmeaning.Toinvestigatethediscoursefunctionsofsuchwords,we(cid:2)rstaskedthreelabelerstoinde-pendentlyclassifyalloccurrencesofalright,gotcha,huh,mmhm,okay,right,uhhuh,yeah,yep,yes,yupintheentireGamesCorpusintooneoftencate-gories,includingacknowledgment/agreement,cuebeginningorendingdiscoursesegment,backchan-nel,andliteralmodi(cid:2)er.Labelerswereaskedto802

choosethemostappropriatecategoryforeachto-ken,orindicatewith‘?’iftheycouldnotmakeadecision.Theywereallowedtoreadthetranscriptsandlistentothespeechastheylabeled.Forourperceptionexperimentwechosematerialsfromthetokensofthemostfrequentofourlabeledaf(cid:2)rmativewords,okay,fromtheObjectsGames,whichcontainedmostofthesetokens.Altogether,thereare1151instancesofokayinthispartofthecorpus;itisthethirdmostfrequentword,follow-ingthe,with4565instances,andof,with1534.Atleasttwolabelersagreedonthefunctionalcat-egoryof902(78%)okaytokens.Ofthosetokens,286(32%)wereclassi(cid:2)edasBACKCHANNEL,255(28%)asACKNOWLEDGEMENT/AGREEMENT,141(16%)asCUEBEGINNING,116(13%)asPIVOTBEGINNING(afunctionthatcombinesAcknowl-edgement/agreementandCuebeginning),and104(11%)asoneoftheotherfunctions.WesampledfromtokenstheannotatorshadlabeledasCuebe-ginningdiscoursesegment,Backchannel,andAc-knowledgement/agreement,themostfrequentcate-goriesinthecorpus;wewillrefertothesebelowsimplyas‘C’,‘B’,and‘A’classes,respectively.3ExperimentWenextdesignedaperceptionexperimenttoex-aminenaivesubjects’perceptionofthesetokensofokay.Toobtaingoodcoveragebothofthe(labeled)A,B,andCclasses,aswellasthedegreesofpo-tentialambiguityamongtheseclasses,weidenti(cid:2)ed9categoriesofokaytokenstoincludeintheexperi-ment:3classes(A,B,C)(cid:2)3levelsoflabeleragree-ment(UNANIMOUS,MAJORITY,NO-AGREEMENT).‘Unanimous’referstotokensassignedtoaparticu-larclasslabelbyall3labelers,‘majority’totokensassignedtothisclassby2ofthe3labelers,and‘no-agreement’totokensassignedtothisclassbyonly1labeler.Todecreasevariabilityinthestimuli,weselectedtokensonlyfromspeakerswhoproducedatleastonetokenforeachofthe9conditions.Therewere6suchspeakers(3female,3male),whichgaveusatotalof54tokens.Toseewhethersubjects’classi(cid:2)cationsofokayweredependentuponcontextualinformationornot,wepreparedtwoversionsofeachtoken.Theiso-latedversionsconsistedofonlythewordokayex-tractedfromthewaveform.Forthecontextualizedversions,weextractedtwofullspeakerturnsforeachokayincludingthefullturn1containingthetar-getokayplusthefullturnofthepreviousspeaker.Inthefollowingthreesamplecontexts,pausesareindi-catedwith‘#’,andthetargetokaysareunderlined:SpeakerA:yeah#umthere’slikethere’ssomespacethere’sSpeakerB:okay#IthinkIgotitSpeakerA:butit’sgonnabebelowtheonionSpeakerB:okaySpeakerA:okay#alright#I’lltryit#okaySpeakerB:okaytheowlisblinkingTheisolatedokaytokensweresinglechannelau-dio(cid:2)les;thecontextualizedokaytokenswerefor-mattedsothateachspeakerwaspresentedtosub-jectsonadifferentchannel,withthespeakerutteringthetargetokayconsistentlyonthesamechannel.Theperceptionstudywasdividedintotwoparts.Inthe(cid:2)rstpart,eachsubjectwaspresentedwiththe54isolatedokaytokens,inadifferentran-domorderingforeachsubject.TheyweregivenaforcedchoicetasktoclassifythemasA,B,orC,withthecorrespondinglabels(Acknowledge-ment/agreement,Backchannel,andCuebeginning)alsopresentedinarandomorderforeachtoken.Inthesecondpart,thesamesubjectwasgiven54con-textualizedtokens,presentedinadifferentrandomorder,andaskedtomakethesamechoice.Werecruited20(paid)subjectsforthestudy,10female,and10male,allbetweentheagesof20and60.AllsubjectswerenativespeakersofStandardAmericanEnglish,exceptforonesubjectwhowasborninJamaicabutanativespeakerofEnglish.Allsubjectsreportednohearingproblems.Subjectsper-formedthestudyinaquietlabusingheadphonestolistentothetokensandindicatingtheirclassi(cid:2)cationdecisionsinaGUIinterfaceonalabworkstation.Theyweregiveninstructionsonhowtousethein-terfacebeforeeachofthetwosectionsofthestudy.Forthestudyitself,foreachtokenintheisolatedcondition,subjectswereshownascreenwiththethreerandomlyorderedclassesandalinktotheto-ken’ssound(cid:2)le.Theycouldlistentothesound(cid:2)lesasmanytimesastheywishedbutwereinstructednottobeconcernedwithansweringthequestions1Wede(cid:2)neaTURNasamaximalsequenceofwordsspokenbythesamespeakerduringwhichthespeakerholdsthe(cid:3)oor.803

(cid:147)correctly(cid:148),buttoanswerwiththeirimmediatere-sponseifpossible.However,theywereallowedtochangetheirselectionasmanytimesastheylikedbeforemovingtothenextscreen.Inthecontex-tualizedcondition,theywerealsoshownanortho-graphictranscriptionofpartofthecontextualizedto-ken,tohelpthemidentifythetargetokay.Themeandurationofthe(cid:2)rstpartofthestudywas25minutes,andofthesecondpart,27minutes.4Results4.1SubjectratingsThedistributionofclasslabelsineachexperimentalconditionisshowninTable1.Whilethisdistribu-tionroughlymirrorsourselectionofequalnumbersoftokensfromeachpreviously-labeledclass,inbothpartsofthestudymoretokenswerelabeledasA(acknowledgment/agreement)thanasB(backchan-nel)orC(cuetotopicbeginning).Thissupportsthehypothesisthatacknowledgment/agreementmayfunctionasthedefaultinterpretationofokay.IsolatedContextualizedA426(39%)452(42%)B324(30%)306(28%)C330(31%)322(30%)Total1080(100%)1080(100%)Table1:Distributionoflabelclassesineachstudycondition.Weexaminedinter-subjectagreementusingFleiss’κmeasureofinter-rateragreementformul-tipleraters(Fleiss,1971).2Table2showsFleiss’κcalculatedforeachindividuallabelvs.theothertwolabelsandforallthreelabels,inbothstudycondi-tions.Fromthistableweseethat,whilethereisverylittleoverallagreementamongsubjectsabouthowtoclassifytokensintheisolatedcondition,agree-mentishigherinthecontextualizedcondition,withamoderateagreementforclassC(κscoreof.497).Thissuggeststhatcontexthelpsdistinguishthecuebeginningdiscoursesegmentfunctionmorethantheothertwofunctionsofokay.2Thismeasureofagreementabovechanceisinterpretedasfollows:0=None,0-0.2=Small,0.2-0.4=Fair,0.4-0.6=Moderate,0.6-0.8=Substantial,0.8-1=Almostperfect.IsolatedContextualizedAvs.rest.089.227Bvs.rest.118.164Cvs.rest.157.497all.120.293Table2:Fleiss’κforeachlabelclassineachstudycondition.RecallfromSection3thattheokaytokenswerechoseninequalnumbersfromthreeclassesaccord-ingtothelevelofagreementofourthreeoriginallabelers(unanimous,majority,andno-agreement),whohadthefulldialoguecontexttouseinmakingtheirdecisions.Table3showsFleiss’κmeasurenowgroupedbyamountofagreementoftheorig-inallabelers,againpresentedforeachcontextcon-dition.Weseeherethattheinter-subjectagreementIsolatedContext.OLno-agreement.085.104-majority.092.299-unanimous.158.452-all.120.293.312Table3:Fleiss’κineachstudycondition,groupedbyagreementofthethreeoriginallabelers(‘OL’).alsomirrorstheagreementofthethreeoriginalla-belers.Inbothstudyconditions,tokenswhichtheoriginallabelersagreedonalsohadthehighestκscores,followedbytokensinthemajorityandno-agreementclasses,inthatorder.Inallcases,tokenswhichsubjectsheardincontextshowedmoreagree-mentthanthosetheyheardinisolation.Theoverallκissmallat.120fortheisolatedcon-dition,andfairat.293forthecontextualizedcon-dition.Thethreeoriginallabelersalsoachievedfairagreementat.312.3Thesimilaritybetweenthelat-tertwoκscoressuggeststhatthefullcontextavail-abletotheoriginallabelersandthelimitedcontextpresentedtotheexperimentsubjetsoffercompara-bleamountsofinformationtodisambiguatebetweenthethreefunctions,althoughlackofanycontextclearlyaffectedsubjects’decisions.Weconclude3Forthecalculationofthisκ,weconsideredfourlabelclasses:A,B,C,andafourthclass‘other’thatcomprisestheremaining7wordfunctionsmentionedinSection2.Inconse-quence,theseκscoresshouldbecomparedwithcaution.804

fromtheseresultsthatcontextisofconsiderableim-portanceintheinterpretationofthewordokay,al-thoughevenaverylimitedcontextappearstosuf-(cid:2)ce.4.2RepresentingsubjectjudgmentsInthissection,wepresentagraphicalrepresenta-tionofsubjectdecisions,usefulforinterpreting,vi-sualizing,andcomparingthewayoursubjectsin-terpretedthedifferenttokensofokay.Foreachin-dividualokayinthestudy,wede(cid:2)neanassociatedthree-dimensionalVOTEVECTOR,whosecompo-nentsaretheproportionsofsubjectsthatclassi(cid:2)edthetokenasA,BorC.Forexample,ifaparticu-larokaywaslabeledasAby5subjects,asBby3,andasCby12,thenitsassociatedvotevectoris(cid:0)520,320,1220(cid:1)=(0.25,0.15,0.6).Followingthisdef-inition,thevectorsA=(1,0,0),B=(0,1,0)andC=(0,0,1)correspondtotheidealsituationsinwhichall20subjectsagreedonthelabel.WecallthesevectorstheUNANIMOUS-VOTEVECTORS.Figure2.ishowsatwo-dimensionalrepresenta-tionthatillustratesthesede(cid:2)nitions.TheblackdotFigure2:2Drepresentationofavotevector(i)andoftheclustercentroids(ii).representsthevotevectorforourexampleokay,theverticesofthetrianglecorrespondtothethreeunanimous-votevectors(A,BandC),andthecrossinthecenterofthetrianglerepresentsthevotevectorofathree-waytiebetweenthelabelers(cid:0)13,13,13(cid:1).WearethusabletocalculatetheEuclideandis-tanceofavotevectortoeachoftheunanimous-votevectors.Theshortestofthesedistancescorrespondstothelabelassignedbytheplurality4ofsubjects.Also,thesmallerthatdistance,thehighertheinter-subjectagreementforthatparticulartoken.Forour4Pluralityisalsoknownassimplemajority:thecandidatewhogetsmorevotesthananyothercandidateisthewinner.exampleokay,thedistancestoA,BandCare0.972,1.070and0.495,respectively;itspluralitylabelisC.Inourexperiment,eachokayhastwoassociatedvotevectors,oneforeachcontextcondition.Toillustratetherelationshipbetweendecisionsintheisolatedandthecontextualizedconditions,we(cid:2)rstgroupedeachcondition’s54votevectorsintothreeclusters,accordingtotheirpluralitylabel.Figure2.iishowstheclustercentroidsinatwo-dimensionalrepresentationofvotevectors.The(cid:2)lleddotscorre-spondtotheclustercentroidsoftheisolatedcondi-tion,andtheemptydots,tothecentroidsofthecon-textualizedcondition.Table4showsthedistancesineachconditionfromtheclustercentroids(denotedAc,Bc,Cc)totherespectiveunanimous-votevec-tors(A,B,C),andalsothedistancebetweeneachpairofclustercentroids.IsolatedContextualizedd(Ac,A).54.44((cid:150)18%)d(Bc,B).57.52((cid:150)10%)d(Cc,C).52.28((cid:150)47%)d(Ac,Bc).41.48(+17%)d(Ac,Cc).49.86(+75%)d(Bc,Cc).54.91(+69%)Table4:Distancesfromtheclustercentroids(Ac,Bc,Cc)totheunanimous-votevectors(A,B,C)andbetweenclustercentroids,ineachcondition.Intheisolatedcondition,thethreeclustercen-troidsareapproximatelyequidistantfromeachother(cid:151)thatis,thethreewordfunctionsappeartobeequallyconfusable.Inthecontextualizedcondi-tion,whileCcisfurtherapartfromtheothertwocentroids,thedistancebetweenAcandBcremainspracticallythesame.Thissuggeststhat,withsomecontextavailable,AandBtokensarestillfairlycon-fusable,whilebotharemoreeasilydistinguishedfromCtokens.Weposittwopossibleexplanationsforthis:First,Cistheonlyfunctionforwhichthespeakerutteringtheokaynecessarilycontinuesspeaking;thustheroleofcontextindisambiguat-ingseemsquiteclear.Second,bothAandBhaveacommonelementof‘acknowledgement’thatmightaffectinter-subjectagreement.805

4.3FeaturesoftheokaytokensInthissection,wedescribeasetofacoustic,prosodic,phoneticandcontextualfeatureswhichmayhelptoexplainwhysubjectsinterpretokaydif-ferently.Acousticfeatureswereextractedautomat-icallyusingPraat.Phoneticandprosodicfeatureswerehand-labeledbyexpertannotators.Contextualfeatureswereconsideredonlyintheanalysisofthecontextualizedcondition,sincetheywerenotavail-abletosubjectsintheisolatedcondition.Weexaminedanumberofphoneticfeaturestode-terminewhetherthesecorrelatedwithsubjectclas-si(cid:2)cations.We(cid:2)rstlookedattheproductionofthethreephonemesinthetargetokay(/oU/,/k/,/eI/),notingthefollowingpossiblevariations:(cid:15)/oU/:[],[A],[5],[O],[OU],[m],[N],[@],[@U].(cid:15)/k/:[G],[k],[kx],[q],[x].(cid:15)/eI/:[e],[eI],[E],[e@].Wealsocalculatedthedurationofeachphoneandofthevelarclosure.Whetherthetargetokaywasatleastpartiallywhisperedornot,andwhethertherewasglottalizationinthetargetokaywerealsonoted.Foreachtargetokay,wealsoexamineditsdu-rationanditsmaximum,meanandminimumpitchandintensity,aswellasthespeaker-normalizedver-sionsofthesevalues.5Weconsidereditspitchslope,intensityslope,andstylizedpitchslope,calculatedoverthewholetargetokay,itslast50,80and100milliseconds,itssecondhalf,itssecondsyllable,andthesecondhalfofitssecondsyllable,aswell.WeusedtheToBIlabelingscheme(Pitrellietal.,1994)tolabeltheprosodyofthetargetokaysandtheirsurroundingcontext.(cid:15)Pitchaccent,ifany,ofthetargetokay(e.g.,H*,H+!H*,L*).(cid:15)Breakindexafterthetargetokay(0-4).(cid:15)Phraseaccentandboundarytone,ifany,fol-lowingthetargetokay(e.g.,L-L%,!H-H%).Forcontextualizedtokens,weincludedseveralfea-turesrelatedtotheexchangebetweenthespeakerutteringthetargetokay(SpeakerB)andtheotherspeaker(SpeakerA).5Speaker-normalizedfeatureswerenormalizedbycomput-ingz-scores(z=(X−mean)/st.dev)forthefeature,wheremeanandst.devwerecalculatedfromallokaysutteredbythespeakerinthesession.(cid:15)NumberofwordsutteredbySpeakerAinthecontext,beforeandafterthetargetokay.SameforSpeakerB.(cid:15)LatencyofSpeakerAbeforeSpeakerB’sturn.(cid:15)DurationofsilenceofSpeakerBbeforeandaf-terthetargetokay.(cid:15)DurationofspeechbySpeakerBimmediatelybeforeandafterthetargetokayanduptoasi-lence.4.4CuestointerpretationWeconductedaseriesofPearson’steststolookforcorrelationsbetweentheproportionofsubjectsthatchoseeachlabelandthenumericfeaturesdescribedinSection4.3,togetherwithtwo-sidedt-teststo(cid:2)ndwhethersuchcorrelationsdifferedsigni(cid:2)cantlyfromzero.Tables5and6showthesigni(cid:2)cantresults(two-sidedt-tests,p<0.05)fortheisolatedandcontextualizedconditions,respectively.Acknowledgement/agreementrdurationofrealizationof/k/(cid:150)0.299Backchannelrstylizedpitchslopeover2ndhalf2ndsyl.0.752pitchslopeover2ndhalfof2ndsyllable0.409speaker-normalizedmaximumintensity(cid:150)0.372pitchslopeoverlast80ms0.349speaker-normalizedmeanintensity(cid:150)0.327durationofrealizationof/eI/0.278wordduration0.277Cuetodiscoursesegmentbeginningrstylizedpitchslopeoverthewholeword(cid:150)0.380pitchslopeoverthewholeword(cid:150)0.342pitchslopeover2ndhalfof2ndsyllable(cid:150)0.319Table5:Featurescorrelatedtotheproportionofvotesforeachlabel.Isolatedcondition.Table5showsthatintheisolatedcondition,sub-jectstendedtoclassifytokensofokayasAcknowl-edgment/agreement(A)whichhadalongerrealiza-tionofthe/k/phoneme.TheytendedtoclassifytokensasBackchannels(B)whichhadalowerin-tensity,alongerduration,alongerrealizationofthe/eI/phoneme,anda(cid:2)nalrisingpitch.TheytendedtoclassifytokensasC(cuetotopicbeginning)thatendedwithfallingpitch.806

Acknowledgement/agreementrlatencyofSpkrAbeforeSpkrB’sturn(cid:150)0.528durationofsilencebySpkrBbeforeokay(cid:150)0.404numberofwordsbySpkrBafterokay(cid:150)0.277Backchannelrpitchslopeover2ndhalfof2ndsyllable0.520pitchslopeoverlast80ms0.455numberofwordsbySpkrAbeforeokay0.451numberofwordsbySpkrBafterokay(cid:150)0.433durationofspeechbySpkrBafterokay(cid:150)0.413latencyofSpkrAbeforeSpkrB’sturn(cid:150)0.385durationofsilencebySpkrBbeforeokay0.295intensityslopeover2ndsyllable(cid:150)0.279CuetodiscoursesegmentbeginningrlatencyofSpkrAbeforeSpkrB’sturn0.645numberofwordsbySpkrBafterokay0.481numberofwordsbySpkrAbeforeokay(cid:150)0.426pitchslopeover2ndhalfof2ndsyllable(cid:150)0.385pitchslopeoverlast80ms(cid:150)0.377durationofspeechbySpkrBafterokay0.338Table6:Featurescorrelatedtotheproportionofvotesforeachlabel.Contextualizedcondition.Inthecontextualizedcondition,we(cid:2)ndverydif-ferentcorrelations.Table6showsthatnearlyallofthestrongcorrelationsinthisconditioninvolvecon-textualfeatures,suchasthelatencybeforeSpeakerB’sturn,orthenumberofwordsbyeachspeakerbe-foreandafterthetargetokay.Notably,onlyoneofthefeaturesthatshowstrongcorrelationsintheiso-latedconditionshowsthesamestrongcorrelationinthecontextualizedcondition:thepitchslopeattheendoftheword.Inbothconditionssubjectstendedtolabeltokenswitha(cid:2)nalrisingpitchcontourasB,andtokenswitha(cid:2)nalfallingpitchcontourasC.Thissupports(Hockey,1993)’s(cid:2)ndingsontheroleofpitchcontourindisambiguatingokay.Wenextconductedaseriesoftwo-sidedFisher’sexactteststo(cid:2)ndcorrelationsbetweensubjects’la-belingsofokayandthenominalfeaturesdescribedinSection4.3.Wefoundsigni(cid:2)cantassociationsbe-tweentherealizationofthe/oU/phonemeandtheokayfunctionintheisolatedcondition(p<0.005).Table7showsthat,inparticular,[m]seemstobethepreferredrealizationforBokays,while[@]seemstobethepreferredoneforAokays,and[OU]and[O]forAandCokays.?[A][5][OU][O][N][@U][@][][m]A0056400800B2041010115C1123401300Table7:Realizationofthe/oU/phoneme,groupedbysubjectpluralitylabel.Isolatedconditiononly.Notably,wedidnot(cid:2)ndsuchsigni(cid:2)cantasso-ciationsinthecontextualizedcondition.Wedid(cid:2)ndsigni(cid:2)cantcorrelationsinbothconditions,how-ever,betweenokayclassi(cid:2)cationsandthetypeofphraseaccentandboundarytonefollowingthetarget(Fisher’sExactTest,p<0.05fortheisolatedcon-dition,p<0.005forthecontextualizedcondition).Table8showsthatL-L%tendstobeassociatedwithAandCclasses,H-H%withBclasses,andL-H%withAandBclasses.Inthiscase,suchcorrelationsarepresentintheisolatedcondition,andsustainedorenhancedinthecontextualizedcondition.H-H%H-L%L-H%L-L%otherIsolatedA02489B33153C11085Context.A0231010B43212C010105Table8:Phraseaccentandboundarytone,groupedbysubjectpluralitylabel.Summingup,whensubjectslistenedtotheokaytokensinisolation,withonlytheiracoustic,prosodicandphoneticpropertiesavailable,afewfeaturesseemtostronglycorrelatewiththeperceptionofwordfunction;forexample,maximumintensity,wordduration,andrealizingthe/oU/phonemeas[m]tendtobeassociatedwithbackchannel,whilethedurationoftherealizationofthe/k/phoneme,andrealizingthe/oU/phonemeas[@]tendtobeas-sociatedwithacknowledgment/agreement.Inthesecondpartofthestudy,whensubjectslistenedtocontextualizedversionsofthesameto-kensofokay,mostofthestrongcorrelationsofwordfunctionwithacoustic,prosodicandphoneticfea-tureswerereplacedbycorrelationswithcontextualfeatures,likelatencyandturnduration.Inotherwords,theseresultssuggestthatcontextualfeatures807

mightoverridetheeffectofmostacoustic,prosodicandphoneticfeaturesofokay.Thereisnonethe-lessonenotableexception:word(cid:2)nalintonation(cid:151)capturedbythepitchslopeandtheToBIlabelsforphraseaccentandboundarytone(cid:151)seemstoplayacentralroleintheinterpretationofbothisolatedandcontextualizedokays.5ConclusionandfutureworkInthisstudy,wehavepresentedevidenceofdiffer-encesintheinterpretationofthefunctionofisolatedandcontextualizedokays.Wehaveshownthatword(cid:2)nalintonationstronglycorrelateswiththesubjects’classi(cid:2)cationofokaysinbothconditions.Addition-ally,thehigherdegreeofinter-subjectagreementinthecontextualizedcondition,alongwiththestrongcorrelationsfoundforcontextualizedfeatures,sug-geststhatcontext,whenavailable,playsacentralroleinthedisambiguationofokay.(Note,how-ever,thatfurtherresearchisneededinordertoassesswhetherthesefeaturesareindeed,infact,perceptu-allyimportant,bothindividuallyandcombined.)Wehavealsopresentedresultssuggestingthatac-knowledgment/agreementactsasadefaultfunctionforbothisolatedancontextualizedokays.Further-more,whilethatfunctionremainsconfusablewithbackchannelinbothconditions,theavailabilityofsomecontexthelpsindistinguishingthosetwofunc-tionsfromcuetotopicbeginning.Theseresultsarerelevanttospokendialoguesys-temsinsuggestinghowsystemscanconveythecuewordokaywiththeintendedmeaningandcaninter-pretusers’productionsofokaycorrectly.Howtheseresultsextendtoothercuewordsandtootherwordfunctionsremainsanopenquestion.Asfuturework,wewillextendthisstudytoin-cludetheover5800occurrencesofalright,gotcha,huh,mmhm,okay,right,uhhuh,yeah,yep,yes,yupintheentireGamesCorpus,andall10discoursefunctionsmentionedinSection2,asannotatedbyourthreeoriginallabelers.Sincewehaveobservedconsiderabledifferencesinconversationstyleinthetwopartsofthecorpus(theObjectsGameselicitedmore‘dynamic’conversations,withmoreoverlapsandinterruptionsthantheCardsGames),wewillcomparecuephraseusageinthesetwosettings.Fi-nally,wearealsointerestedinexaminingspeakerentrainmentincuephraseusage,orhowsubjectsadapttheirchoiceandproductionofcuephrasestotheirconversationpartner’s.AcknowledgmentsThisworkwasfundedinpartbyNSFIIS-0307905.WethankGregoryWard,ElisaSneed,andMichaelMulleyfortheirvaluablehelpincollectingandla-belingthedata,andtheanonymousreviewersforhelpfulcommentsandsuggestions.ReferencesMaryE.BeckmanandJuliaHirschberg.1994.TheToBIannotationconventions.OhioStateUniversity.PaulBoersmaandDavidWeenink.2001.Praat:Doingphoneticsbycomputer.http://www.praat.org.RobinCohen.1984.Acomputationaltheoryofthefunc-tionofcluewordsinargumentunderstanding.22ndConferenceoftheACL,pages251(cid:150)258.JosephL.Fleiss.1971.Measuringnominalscaleagree-mentamongmanyraters.PsychologicalBulletin,76(5):378(cid:150)382.BarbaraJ.GroszandCandaceL.Sidner.1986.Atten-tion,Intentions,andtheStructureofDiscourse.Com-putationalLinguistics,12(3):175(cid:150)204.JuliaHirschbergandDianeLitman.1993.EmpiricalStudiesontheDisambiguationofCuePhrases.Com-putationalLinguistics,19(3):501(cid:150)530.BethAnnHockey.1993.Prosodyandtheroleofokayanduh-huhindiscourse.ProceedingsoftheEasternStatesConferenceonLinguistics,pages128(cid:150)136.GailJefferson.1972.Sidesequences.Studiesinsocialinteraction,294:338.JacquelineC.Kowtko.1997.Thefunctionofintonationintask-orienteddialogue.Ph.D.thesis,UniversityofEdinburgh.JohnPitrelli,MaryBeckman,andJuliaHirschberg.1994.EvaluationofprosodictranscriptionlabelingreliabilityintheToBIframework.InICSLP94,vol-ume2,pages123(cid:150)126,Yokohama,Japan.RachelReichman.1985.GettingComputerstoTalkLikeYouandMe:DiscourseContext,Focus,andSeman-tics:(anATNModel).MITPress.EmanuelA.SchegloffandHarveySacks.1973.Openingupclosings.Semiotica,8(4):289(cid:150)327.NigelWardandWataruTsukahara.2000.Prosodicfea-tureswhichcueback-channelresponsesinEnglishandJapanese.JournalofPragmatics,23:1177(cid:150)1207.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 808–815,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

808

PredictingSuccessinDialogueDavidReitterandJohannaD.Mooredreitter|jmoore@inf.ed.ac.ukSchoolofInformaticsUniversityofEdinburghUnitedKingdomAbstractTask-solvingindialoguedependsonthelin-guisticalignmentoftheinterlocutors,whichPickering&Garrod(2004)havesuggestedtobebasedonmechanisticrepetitionef-fects.Inthispaper,weseekconﬁrmationofthishypothesisbylookingatrepetitionincorpora,andwhetherrepetitioniscor-relatedwithtasksuccess.Weshowthattherelevantrepetitiontendencyisbasedonslowadaptationratherthanshort-termprim-inganddemonstratethatlexicalandsyntac-ticrepetitionisareliablepredictoroftasksuccessgiventheﬁrstﬁveminutesofatask-orienteddialogue.1IntroductionWhilehumansareremarkablyefﬁcient,ﬂexibleandreliablecommunicators,wearefarfromperfect.Ourdialoguesdifferinhowsuccessfullyinforma-tionisconveyed.Intask-orienteddialogue,wheretheinterlocutorsarecommunicatingtosolveaprob-lem,tasksuccessisacrucialindicatorofthesuccessofthecommunication.Anautomaticmeasureoftasksuccesswouldbeusefulforevaluatingconversationsamonghumans,e.g.,forevaluatingagentsinacallcenter.Inhuman-computerdialogues,predictingthetasksuccessafterjustaﬁrstfewturnsoftheconversationcouldavoiddisappointment:iftheconversationisn’tgoingwell,acallermaybepassedontoahumanoperator,orthesystemmayswitchdialoguestrategies.Asaﬁrststep,wefocusonhuman-humandialogue,sincecur-rentspokendialoguesystemsdonotyetyieldlong,syntacticallycomplexconversations.Inthispaper,weusesyntacticandlexicalfeaturestopredicttasksuccessinanenvironmentwhereweassumenospeakermodel,nosemanticinformationandnoinformationtypicalforahuman-computerdialoguesystem,e.g.,ASRconﬁdence.Thefea-turesweusearebasedonapsychologicaltheory,linkingalignmentbetweendialogueparticipantstolow-levelsyntacticpriming.Anexaminationofthisprimingrevealsdifferencesbetweenshort-termandlong-termeffects.1.1RepetitionsupportsdialogueIntheirInteractiveAlignmentModel(IAM),Pick-eringandGarrod(2004)suggestthatdialoguebe-tweenhumansisgreatlyaidedbyaligningrepre-sentationsonseverallinguisticandconceptuallev-els.Thiseffectisassumedtobedrivenbyacas-cadeoflinguisticprimingeffects,whereinterlocu-torstendtore-uselexical,syntacticandotherlin-guisticstructuresaftertheirintroduction.Suchre-useleadsspeakerstoagreeonacommonsitua-tionmodel.Severalstudieshaveshownthatspeak-erscopytheirinterlocutor’ssyntax(Braniganetal.,1999).Thiseffectisusuallyreferredtoasstructural(or:syntactic)priming.Thesepersistenceeffectsareinter-related,aslexicalrepetitionimpliespref-erencesforsyntacticchoices,andsyntacticchoicesleadtopreferredsemanticinterpretations.Withoutdemandingadditionalcognitiveresources,theef-fectsformacausalchainthatwillbeneﬁttheinter-locutor’spurposes.Or,attheveryleast,itwillbeeasierforthemtorepeatlinguisticchoicesthanto809

activelydiscusstheirterminologyandkeeptrackofeachother’scurrentknowledgeofthesituationinordertocometoamutualunderstanding.1.2StructuralprimingTherepetitioneffectatthecenterofthispaper,prim-ing,isdeﬁnedasatendencytorepeatlinguisticde-cisions.Priminghasbeenshowntoaffectlanguageproductionand,toalesserextent,comprehension,atdifferentlevelsoflinguisticanalysis.Thistendencymayshowupinvariousways,forinstanceinthecaseoflexicalprimingasashorterresponsetimeinlexicaldecisionmakingtasks,orasapreferenceforonesyntacticconstructionoveranalternativeoneinsyntacticpriming(Bock,1986).Inanexperimentalstudy(Braniganetal.,1999),subjectswereprimedbycompletingeithersentence(1a)or(1b):1a.Theracingdrivershowedthetornoverall...1b.Theracingdrivershowedthehelpfulmechanic...Sentence(1a)wastobecompletedwithaprepo-sitionalobject(“tothehelpfulmechanic”),while(1b)requiredadoubleobjectconstruction(“thetornoverall”).Subsequently,subjectswereallowedtofreelycompleteasentencesuchasthefollowingone,describingapicturetheywereshown:2.Thepatientshowed...Subjectsweremorelikelytocomplete(2)withadouble-objectconstructionwhenprimedwith(1b),andwithaprepositionalobjectconstructionwhenprimedwith(1a).Inapreviouscorpus-study,usingtranscriptionsofspontaneous,task-orientedandnon-task-orienteddialogue,utteranceswereannotatedwithsyntactictrees,whichwethenusedtodeterminethephrase-structurerulesthatlicensedproduction(andcom-prehension)oftheutterances(Reitteretal.,2006b).Foreachrule,thetimeofitsoccurrencewasnoted,e.g.wenoted3.117.9sNP→ATAPNNafencedmeadow4.125.5sNP→ATAPNNtheabandonedcottageInthisstudy,wethenfoundthatthere-occurrenceofarule(asin4)wascorrelatedwiththetemporaldistancetotheﬁrstoccurrence(3),e.g.,7.6seconds.Theshorterthedistancebetweenprime(3)andtar-get(4),themorelikelywererulestore-occur.Inaconversation,primingmayleadaspeakertochooseaverboverasynonymbecausetheirin-terlocutorhasuseditafewsecondsbefore.This,inturn,willincreasethelikelihoodofthestruc-turalformoftheargumentsinthegovernedverbalphrase–simplybecauselexicalitemshavetheirpref-erencesforparticularsyntacticstructures,butalsobecausestructuralprimingmaybestrongeriflexi-calitemsarerepeated(lexicalboost,PickeringandBranigan(1998)).Additionally,thestructuralprim-ingeffectsintroducedabovewillmakeapreviouslyobservedorproducedsyntacticstructuremorelikelytobere-used.Thischainreactionleadsinterlocu-torsindialoguetoreachacommonsituationmodel.NotethattheIAM,inwhichinterlocutorsautomati-callyandcheaplybuildacommonrepresentationofcommonknowledge,isatoddswithviewsthataf-fordeachdialogueparticipantanexplicitandsepa-raterepresentationoftheirinterlocutor’sknowledge.Theconnectionbetweenlinguisticpersistenceorprimingeffectsandthesuccessofdialogueiscru-cialfortheIAM.Thepredictionsarisingfromthis,however,haveeludedtestingsofar.Inourpreviousstudy(Reitteretal.,2006b),wefoundmoresyn-tacticpriminginthetask-orienteddialoguesoftheMapTaskcorpusthaninthespontaneousconversa-tioncollectedintheSwitchboardcorpus.However,wecomparedprimingeffectsacrosstwodatasets,whereparticipantsandconversationtopicsdifferedgreatly.Switchboardcontainsspontaneousconver-sationoverthetelephone,whilethetask-orientedMapTaskcorpuswasrecordedwithinterlocutorsco-present.Whiletheresult(moreprimingintask-orienteddialogue)supportedthepredictionsofIAM,cognitiveloadeffectscouldnotbedistin-guishedfrompriming.Inthecurrentstudy,weex-aminestructuralrepetitionintask-orienteddialogueonlyandfocusonanextrinsicmeasure,namelytasksuccess.2RelatedWorkPriorworkonpredictingtasksuccesshasbeendoneinthecontextofhuman-computerspokendi-aloguesystems.Featuressuchasrecognitioner-rorrates,naturallanguageunderstandingconﬁdenceandcontextshifts,conﬁrmationsandre-prompts(di-aloguemanagement)havebeenusedclassifydia-810

loguesintosuccessfulandproblematicones(Walkeretal.,2000).Withtheseautomaticallyobtainablefeatures,anaccuracyof79%canbeachievedgiventheﬁrsttwoturnsof“HowmayIhelpyou?”di-alogues,wherecallersaresupposedtoberoutedgivenashortstatementfromthemaboutwhattheywouldliketodo.Fromthewholeinteraction(veryrarelymorethanﬁveturns),87%accuracycanbeachieved(36%ofdialogueshadbeenhand-labeled“problematic”).However,themostpredictivefea-tures,whichrelatedtoautomaticspeechrecognitionerrors,areneitheravailableinthehuman-humandi-alogueweareconcernedwith,noraretheylikelytobethecauseofcommunicationproblemsthere.Moreover,failuresintheMapTaskdialoguesareduetotheactualgoings-onwhentwointerlocutorsengageincollaborativeproblem-solvingtojointlyreachanunderstanding.Insuchdialogues,inter-locutorsworkoveraperiodofabouthalfanhour.Topredicttheirdegreeofsuccess,wewillleveragethephenomenonofpersistence,orpriming.Inpreviouswork,twoparadigmshaveseenexten-siveusetomeasurerepetitionandprimingeffects.Experimentalstudiesexposesubjectstoaparticularsyntacticconstruction,eitherbyhavingthempro-ducetheconstructionbycompletingasamplesen-tence,orbyhavinganexperimenterorconfederateinterlocutorusetheconstruction.Then,subjectsareaskedtodescribeapictureorcontinuewithagiventask,elicitingthetargetconstructionoracompet-ing,semanticallyequivalentalternative.Theanaly-sisthenshowsaneffectofthecontrolledconditiononthesubject’suseofthetargetconstruction.Observationalstudiesusenaturalisticdata,suchastextanddialoguefoundincorpora.Here,theprimeconstructionisnotcontrolled–butagain,acorrelationbetweenprimesandtargetsissought.Speciﬁccompetingconstructionssuchasac-tive/passive,verbalparticleplacementorthat-deletioninEnglishareoftentheobjectofstudy(Szmrecsanyi,2005;Gries,2005;Dubeyetal.,2005;J¨ager,2006),buttheeffectcanalsobegen-eralizedtosyntacticphrase-structurerulesorcom-binatorialcategories(Reitteretal.,2006a).Church(2000)proposesadaptivelanguagemod-elstoaccountforlexicaladaptation.Eachdocumentissplitintoprimeandtargethalves.Then,forse-lectedwordsw,themodelestimatesP(+adapt)=P(w∈target|w∈prime)P(+adapt)ishigherthanPprior=P(w∈target),whichisnotsurprising,sincetextsareusu-allyaboutalimitednumberoftopics.Thismethodlooksatrepetitionoverwholedoc-umenthalves,independentlyofdecay.Inthispa-per,weapplythesametechniquetosyntacticrules,whereweexpecttoestimatesyntacticprimingef-fectsofthelong-termvariety.3Repetition-basedSuccessPrediction3.1TheSuccessPredictionTaskInthefollowing,wedeﬁnetwovariantsofthetaskandthendescribeamodelthatusesrepetitioneffectstopredictsuccess.Task1:Successisestimatedwhenanentiredi-alogueisgiven.Alllinguisticandnon-linguisticinformationavailablemaybeused.Thistaskre-ﬂectspost-hocanalysisapplications,wheredia-loguesneedtobeevaluatedwithouttheactualsuc-cessmeasurebeingavailableforeachdialogue.Thiscoverscaseswhere,e.g.,itisunclearwhetheracallcenteragentoranautomatedsystemactuallyre-spondedtothecallsatisfactorily.Task2:Successispredictedwhenjusttheinitial5-minuteportionofthedialogueisavailable.Adia-loguesystem’soracallcenteragent’sstrategymaybeinﬂuenceddependingonsuchaprediction.3.2MethodToaddressthetasksdescribedinthepreviousSec-tion,wetrainsupportvectormachines(SVM)topredictthetasksuccessscoreofadialoguefromlexicalandsyntacticrepetitioninformationaccumu-lateduptoaspeciﬁedpointintimeinthedialogue.DataTheHCRCMapTaskcorpus(Andersonetal.,1991)contains128dialoguesbetweensubjects,whoweregiventwoslightlydifferentmapsdepictingthesame(imaginary)landscape.Onesubjectgivesdi-rectionsforapredeﬁnedroutetoanothersubject,whofollowsthemanddrawsarouteontheirmap.Thespokeninteractionswererecorded,tran-scribedandsyntacticallyannotatedwithphrase-structuregrammar.811

TheMapTaskprovidesuswithaprecisemeasureofsuccess,namelythedeviationofthepredeﬁnedandfollowedroute.Successcanbequantiﬁedbycomputingtheinversedeviationbetweensubjects’paths.Bothsubjectsineachtrialwereaskedtodraw”their”respectiverouteonthemapthattheyweregiven.Thedeviationbetweentherespectivepathsdrawnbyinterlocutorswasthendeterminedastheareacoveredinbetweenthepaths(PATHDEV).FeaturesRepetitionismeasuredonalexicalandasyntacticlevel.Todoso,weidentifyallconstituentsintheutterancesasperphrase-structureanalysis.[Go[to[the[[whitehouse][on[theright]]]]]]wouldyield11constituents.Eachconstituentislicensedbyasyntacticrule,forinstanceVP→VPPforthetop-mostconstituentintheaboveexample.Foreachconstituent,wecheckwhetheritisalex-icalorsyntacticrepetition,i.e.ifthesamewordsoccurredbefore,orifthelicensingrulehasoccurredbeforeinthesamedialogue.Ifso,weincrementcountersforlexicaland/orsyntacticrepetitions,andincreaseafurthercounterforstringrepetitionbythelengthofthephrase(incharacters).Thelattervari-ableaccountsfortherepetitionoflongphrases.Weincludeadatapointforeach10-secondinter-valofthedialogue,withfeaturesreportingthelexi-cal(LEXREP),syntactic(SYNREP)andcharacter-based(CHARREP)repetitionsuptothatpointintime.Atimestampandthetotalnumbersofcon-stituentsandcharactersarealsoincluded(LENGTH).Thisway,themodelmayworkwithrepetitionpro-portionsratherthantheabsolutecounts.Wetrainasupportvectormachineforregressionwitharadialbasisfunctionkernel(γ=5),usingthefeaturesasdescribedaboveandthePATHDEVscoreasoutput.3.3EvaluationWecastthetaskasaregressionproblem.Topre-dictadialogue’sscore,weapplytheSVMtoitsdatapoints.Themeanoutcomeistheestimatedscore.Asuitableevaluationmeasure,theclassicalr2,indicatestheproportionofthevarianceintheac-tualtasksuccessscorethatcanbepredictedbythemodel.Allresultsreportedhereareproducedfrom10-foldcross-validated90%training/10%testTask1Task2ALLFeatures0.170.14ALLw/oSYNREP0.150.06ALLw/oLEX/CHARREP0.090.07LENGTHONLY0.09n/aBaseline0.010.01Table1:Portionofvarianceexplained(r2)splitsofthedialogues.Nofulldialoguewasin-cludedinbothtestandtrainingsets.Task1wasevaluatedwithalldata,theTask2modelwastrainedandtestedondatapointssampledfromtheﬁrst5minutesofthedialogue.ForTask1(fulldialogues),theresults(Table1)indicatethatALLrepetitionfeaturestogetherwiththeLENGTHoftheconversation,accountforabout17%ofthetotalscorevariance.Therepetitionfea-turesimproveontheperformanceachievedfromdi-aloguelengthalone(about9%).ForthemoredifﬁcultTask2,ALLfeaturesto-getherachieve14%ofthevariance.(NotethatLENGTHisnotavailable.)Whenthesyntacticrepe-titionfeatureistakenoutandonlylexical(LEXREP)andcharacterrepetition(CHARREP)areused,weachieve6%inexplainedvariance.Thebaselineisimplementedasamodelthatal-waysestimatesthemeanscore.Itshould,theoreti-cally,becloseto0.3.4DiscussionObviously,linguisticinformationalonewillnotex-plainthemajorityofthetask-solvingabilities.Apartfromsubject-relatedfactors,communicativestrate-gieswillplayarole.However,linguisticrepetitionservesasagoodpredictorofhowwellinterlocutorswillcompletetheirjointtask.Thefeaturesusedarerelativelysim-ple:providedthereissomesyntacticannotation,rulerepetitioncaneasilybedetected.Evenwith-outsyntacticinformation,lexicalrepetitionalreadygoesalongway.Butwhatkindofrepetitionisitthatplaysaroleintask-orienteddialogue?Leavingoutfeaturesisnotanidealmethodtoquantifytheirinﬂuence–inpar-ticular,wherefeaturesinter-correlate.Thecontribu-tionofsyntacticrepetitionisstillunclearfromthe812

presentresults:itactsasausefulpredictoronlyoverthecourseofthewholedialogues,butnotwithina5-minutetimespan,wheretheSVMcannotincor-porateitsinformationalcontent.Wewillthereforeturntoamoredetailedanalysisofstructuralrepetition,whichshouldhelpusdrawconclusionsrelatingtothepsycholinguisticsofdia-logue.4LongtermandshorttermprimingInthefollowing,wewillexaminesyntactic(struc-tural)primingasoneofthedrivingforcesbehindalignment.Wechoosesyntacticoverlexicalprimingfortworeasons.Lexicalrepetitionduetoprimingisdifﬁculttodistinguishfromrepetitionthatisduetointerlocutorsattendingtoaparticulartopicofcon-versation,which,incoherentdialogue,meansthattopicsareclustered.Lexicalchoicereﬂectsthosetopics,henceweexpectclustersofparticulartermi-nology.Secondly:themapsusedtocollectthedia-loguesintheMapTaskcorpuscontainedlandmarkswithlabels.Itisonlynatural(evenifbymeanstocross-modalpriming)thatspeakerswillidentifylandmarksusingthelabelsandshowlittlevariabilityinlexicalchoice.Wewillmeasurerepetitionofsyn-tacticrules,wherebyword-by-wordrepetition(topi-calityeffects,parroting)isexplicitlyexcluded.Forsyntacticpriming1,tworepetitioneffectshavebeenidentiﬁed.Classicalprimingeffectsarestrong:around10%forsyntacticrules(Reitteretal.,2006b).However,theydecayquickly(Braniganetal.,1999)andreachalowplateauafterafewsec-onds,whichlikenstotheeffecttosemantic(similar-ity)priming.Whatcomplicatesmattersisthatthereisalsoadifferent,long-termadaptationeffectthatisalsocommonlycalled(repetition)priming.Adaptationhasbeenshowntolastlonger,fromminutes(BockandGrifﬁn,2000)toseveraldays.Lexicalboostinteractions,wherethelexicalrep-etitionofmaterialwithintherepeatedstructurestrengthensstructuralpriming,havebeenobservedforshort-termpriming,butnotforlong-termprim-ingtrialswherematerialintervenedbetweenprimeandtargetutterances(KonopkaandBock,2005).Thus,short-andlong-termadaptationeffectsmay1inproductionandcomprehension,whichwewillnotdis-tinguishfurtherforspacereasons.Ourdataare(off-line)pro-ductiondata.wellbeduetoseparatecognitiveprocesses,asre-centlyarguedby(FerreiraandBock,2006).Section5dealswithdecay-basedshort-termpriming,Sec-tion6withlong-termadaptation.PickeringandGarrod(2004)donotmakethetypeofprimingsupportingalignmentexplicit.Shouldweﬁnddifferencesinthewaytasksuccessinteractswithdifferentkindsofrepetitioneffects,thenthiswouldbeagoodindicationaboutwhateffectsup-portsIAM.Moreconcretely,wecouldsaywhetheralignmentisduetotheautomatic,classicalprimingeffect,orwhetheritisbasedonalong-termeffectthatispossiblyclosertoimplicitlearning(Changetal.,2006).5Short-termprimingInthissection,weattempttodetectdifferencesinthestrengthofshort-termpriminginsuccessfulandlesssuccessfuldialogues.Todoso,weusethemea-sureofprimingstrengthestablishedbyReitteretal.(2006b),whichthenallowsustotestwhetherprim-inginteractswithtasksuccess.Undertheassump-tionsofIAMwewouldexpectsuccessfuldialoguestoshowmoreprimingthanunsuccessfulones.Obviously,difﬁcultieswiththetaskathandmaybeduetoarangeofproblemsthatthesubjectsmayhave,linguisticandotherwise.Butgiventhatthedi-aloguescontainvariablelevelsofsyntacticpriming,onewouldexpectthatthishasatleastsomeinﬂu-enceontheoutcomeofthetask.5.1Method:LogisticRegressionWeusedmixed-effectsregressionmodelsthatpre-dictabinaryoutcome(repetition)usinganumberofdiscreteandcontinuousfactors.2Asaﬁrststep,ourmodelingefforttriestoestab-lishaprimingeffect.Todoso,wecanmakeuseofthefactthattheprimingeffectdecaysovertime.Howstrongthatdecayisgivesusanindicationofhowmuchrepetitionprobabilityweseeshortlyafterthestimulus(prime)comparedtotheprobabilityofchancerepetition–withouteverexplicitlycalculatingsuchaprior.Thuswedeﬁnethestrengthofprimingasthede-cayrateofrepetitionprobability,fromshortlyafter2WeuseGeneralizedLinearMixedEffectsmodelsﬁttedus-ingGlmmPQLintheMASSRlibrary.813

theprimeto15secondsafterward(predictor:DIST).Thus,wetakeseveralsamplesatvaryingdistances(d),lookingatcasesofstructuralrepetition,andcaseswherestructurehasnotbeenrepeated.Inthesyntacticcontext,syntacticrulessuchasVP→VPPPreﬂectsyntacticdecisions.Primingofasyntacticconstructionshowsupinthetendencytorepeatsuchrulesindifferentlexicalcontexts.Thus,weexaminewhethersyntacticruleshavebeenre-peatedatadistanced.Foreachsyntacticrulethatoccursattimet1,wecheckaone-secondtimepe-riod[t1−d−0.5,t1−d+0.5]foranoccurrenceofthesamerule,whichwouldconstituteaprime.Thus,themodelwillbeabletoimplicitlyestimatetheprobabilityofrepetition.GeneralizedLinearRegressionModels(GLMs)canthenmodelthedecaybyestimatingtherela-tionshipbetweendandtheprobabilityofrulerepe-tition.Themodelisdesignedtopredictwhetherrep-etitionwilloccur,or,moreprecisely,whetherthereisaprimeforagiventarget(priming).Underano-primingnull-hypothesis,wewouldassumethattheprimingprobabilityisindependentofd.Ifthereispriming,however,increasingdwillnegativelyinﬂu-encetheprimingprobability(decay).So,weexpectamodelparameter(DIST)fordthatisreliablyneg-ative,andlower,ifthereismorepriming.Withthismethod,wedrawmultiplesamplesfromthesameutterance–fordifferentd,butalsofordif-ferentsyntacticrulesoccurringinthoseutterances.Becausethesesamplesareinter-dependent,weuseagroupingvariableindicatingthesourceutterance.BecausethedatasetissparsewithrespecttoPRIME,balancedsamplingisneededtoensureanequalnumberofdatapointsofprimingandnon-primingcases(PRIME)isincluded.ThismethodhasbeenpreviouslyusedtoconﬁrmprimingeffectsforthegeneralcaseofsyntacticrulesbyReitteretal.(2006b).Additionally,theGLMcantakeintoaccountcategoricalandcontinuouscovari-atesthatmayinteractwiththeprimingeffect.Inthepresentexperiment,weuseaninteractiontermtomodeltheeffectoftasksuccess.3Thecrucialin-teraction,inourcase,istasksuccess:PATHDEVisthedeviationofthepathsthattheinterlocutorsdrew,3WeusetheA∗BoperatorinthemodelformulastoindicatetheinclusionofmaineffectsofthefeaturesAandBandtheirinteractionsA:B.normalizedtotherange[0,1].ThecoremodelisthusPRIME∼log(DIST)∗PATHDEV.IfIAMiscorrect,wewouldexpectthatthedevia-tionofpaths,whichindicatesnegativetasksuccess,willnegativelycorrelatewiththeprimingeffect.5.2ResultsShort-termprimingreliablycorrelated(negatively)withthedistance,henceweseeadecayandprimingeffect(DIST,b=−0.151,p<0.0001,asshowninpreviouswork).Notably,pathdeviationandshort-termprimingdidnotcorrelate.Themodelshowedwasnosuchinteraction(DIST:PATHDEV,p=0.91).Wealsotestedforaninteractionwithanad-ditionalfactorindicatingwhetherprimeandtar-getwereutteredbythesameoradifferentspeaker(comprehension-productionvs.production-productionpriming).Nosuchinteractionap-proachedreliability(log(DIST):PATHDEV:ROLE,p=0.60).Wealsotestedwhetherprimingchangesovertimeoverthecourseofeachdialogue.Itdoesnot.Therewerenoreliableinteractioneffectsofcenteredprime/targettimes(log(DIST):log(STARTTIME),p=0.75,log(DIST):PATHDEV:log(STARTTIME),p=0.63).Reducingthemodelbyremovingunreliableinteractionsdidnotyieldanyreliableeffects.5.3DiscussionWehaveshownthatwhilethereisaclearprimingeffectintheshortterm,thesizeofthisprimingeffectdoesnotcorrelatewithtasksuccess.Thereisnoreliableinteractionwithsuccess.Doesthisindicatethatthereisnostrongfunc-tionalcomponenttopriminginthedialoguecon-text?Theremaystillbeaninﬂuenceofcognitiveloadduetospeakersworkingonthetask,oranover-alldispositionforhigherprimingintask-orienteddi-alogue:Reitteretal.(2006b)pointatstrongerprim-inginsuchsituations.Butourresultsherearedifﬁ-culttoreconcilewiththemodelsuggestedbyPicker-ingandGarrod(2004),ifwetakeshort-termprimingasthedrivingforcebehindIAM.Short-termprimingdecayswithinafewseconds.Thus,towhatextentcouldsyntacticpriminghelpin-terlocutorsaligntheirsituationmodels?IntheMap814

Taskexperiments,interlocutorsneedtorefertoland-marksregularly–butnoteveryfewseconds.Itwouldbesensibletoexpectlonger-termadaptation(withinminutes)todrivedialoguesuccess.6Long-termadapationLong-termadaptationisaformofprimingthatoccursoverminutesandcould,therefore,supportlinguisticandsituationmodelalignmentintask-orienteddialogue.IAMandthesuccessoftheSVMbasedmethodcouldbebasedonsuchanef-fectinsteadofshort-termpriming.Analogoustothethepreviousexperiment,wehypothesizethatmoreadaptationrelatestomoretasksuccess.6.1MethodAftertheinitialfewseconds,structuralrepetitionshowslittledecay,butcanbedemonstratedevenminutesorlongerafterthestimulus.Tomeasurethistypeofadapation,weneedadifferentstrategytoes-timatethesizeofthiseffect.Whileshort-termprimingcanbepin-pointedus-ingthecharacteristicdecay,forlong-termprimingweneedtoinspectwholedialoguesandconstructandcontrastdialogueswhereprimingispossibleandoneswhereitisnot.FactorSAMEDOCdistinguishesthetwosituations:1)Primingcanhappenincon-tiguousdialogues.Wetreattheﬁrsthalfofthedia-logueasprimingperiod,andtheruleinstancesinthesecondhalfastargets.2)Thecontrolcaseiswhenprimingcannothavetakenplace,i.e.,betweenunre-lateddialogues.Primeperiodandtargetsstemfromseparaterandomlysampleddialoguehalvesthatal-wayscomefromdifferentdialogues.Thus,ourmodel(PRIME∼SAMEDOC∗PATHDEV)estimatestheinﬂuenceofprimingonruleus.FromaBayesianperspective,wewouldsaythatthesecondkindofdata(non-priming)al-lowthemodeltoestimateapriorforrulerepetitions.ThegoalisnowtoestablishacorrelationbetweenSAMEDOCandtheexistenceofrepetition.Ifandonlyifthereislong-termadapationwouldweex-pectsuchacorrelation.Analogoustotheshort-termprimingmodel,wedeﬁnerepetitionastheoccurrenceofaprimewithintheﬁrstdocumenthalf(PRIME),andsamplerulein-stancesfromtheseconddocumenthalf.Toexclude●●●●●●●●●●●●0.00.51.00.820.840.860.880.900.92log path deviation (inverse success)relative repetition (log−odds)morelesssuccesslessmoresyn. adaptationFigure1:Relativerulerepetitionprobability(chancerepetitionexluded)over(neg.)tasksuccess.short-termprimingeffects,wedropa10-secondpor-tioninthemiddleofthedialogues.TasksuccessisinversepathdeviationPATHDEVasbefore,whichshould,underIAMassumptions,interactwiththeeffectestimatedforSAMEDOC.6.2ResultsLong-termrepetitionshowedapositiveprimingef-fect(SAMEDOC,b=3.303,p<0.0001).Thisgeneralizespreviousexperimentalprimingresultsinlong-termpriming.Long-term-repetitiondidnotinter-actwith(normalized)rulefrequency(SAMEDOC:log(RULEFREQ,b=−0.044,p=0.35).Theinteractionwasremovedforallotherparametersreported.4Theeffectinteractedreliablywiththepathdeviationscores(SAMEDOC:PATHDEV,b=−0.624,p<0.05).Weﬁndareliablecorrelationoftasksuccessandsyntacticpriming.Strongerpathdeviationsrelatetoweakerpriming.6.3DiscussionThemoreprimingwesee,thebettersubjectsper-formatsynchronizingtheirroutesonthemaps.Thisisexactlywhatonewouldexpectundertheassump-4SuchaninteractionalsocouldnotbefoundinareducedmodelwithonlySAMEDOCandRULEFREQ.815

tionofIAM.Also,thereisnoevidenceforstrongerlong-termadaptationofrarerules,whichmaypointoutaqualitativedifferencetoshort-termpriming.Ofcourse,thiscorrelationdoesnotnecessarilyin-dicateacausalrelationship.However,participantsinMapTaskdidnotreceiveanexplicitindicationaboutwhethertheywereonthe“righttrack”.Mis-takes,suchaspassingalandmarkonitsEastandnotontheWestside,weremadeandwentunno-ticed.Thus,itisnotverylikelythattasksuccesscausedalignmenttoimproveatlarge.Wesuspectsuchapossibility,however,forveryunsuccessfuldialogues.Acloserlookatthecorrelation(Figure1)revealsthatwhileadaptationindeeddecreasesastasksuccessdecreases,adaptationincreasedagainforsomeoftheleastsuccessfuldialogues.Itispos-siblethathere,miscoordinationbecameapparenttotheparticipants,whothentriedtoswitchstrategies.Or,simplyput:toomuchalignment(andtoolittlerisk-taking)isunhelpful.Further,qualitative,workneedstobedonetoinvestigatethishypothesis.Fromanappliedperspective,thecorrelationshowsthatoftherepetitioneffectsincludedinourtask-successpredictionmodel,itislong-termsyn-tacticadaptationasopposedtothemoreautomaticshort-termprimingeffectthatcontributestopredic-tionaccuracy.Wetakethisasanindicationtoin-cludeadaptationratherthanjustpriminginamodelofalignmentindialogue.7ConclusionTasksuccessinhuman-humandialogueispredictable–themoresuccessfullyspeakerscollab-orate,themoretheyshowlinguisticadaptation.ThisconﬁrmsourinitialhypothesisofIAM.Intheappliedmodel,knowledgeoflexicalandsyntacticrepetitionhelpstodeterminetasksuccessevenafterjustafewminutesoftheconversation.Wesuggestedtwoapplication-orientedtasks(es-timatingandpredictingtasksuccess)andanap-proachtoaddressthem.Theynowprovideanop-portunitytoexploreandexploitotherlinguisticandextra-linguisticparameters.Thesecondcontributionisacloserinspectionofstructuralrepetition,whichshowedthatitislong-termadaptationthatvarieswithtasksuccess,whileshort-termprimingappearslargelyautonomous.Long-termadaptationmaythusbeastrategythataidsdialoguepartnersinaligningtheirlanguageandtheirsituationmodels.AcknowledgmentsTheauthorswouldliketothankFrankKellerandthereviewers.TheﬁrstauthorissupportedbytheEdinburgh-StanfordLink.ReferencesA.Anderson,M.Bader,E.Bard,E.Boyle,G.M.Doherty,S.Garrod,S.Isard,J.Kowtko,J.McAllister,J.Miller,C.Sotillo,H.Thompson,andR.Weinert.1991.TheHCRCMapTaskcorpus.LanguageandSpeech,34(4):351–366.J.KathrynBock.1986.Syntacticpersistenceinlanguagepro-duction.CognitivePsychology,18:355–387.J.KathrynBockandZenziGrifﬁn.2000.Thepersistenceofstructuralpriming:transientactivationorimplicitlearning?JofExperimentalPsychology:General,129:177–192.H.P.Branigan,M.J.Pickering,andA.A.Cleland.1999.Syn-tacticpriminginlanguageproduction:evidenceforrapiddecay.PsychonomicBulletinandReview,6(4):635–640.F.Chang,G.Dell,andK.Bock.2006.Becomingsyntactic.PsychologicalReview,113(2):234–272.KennethW.Church.2000.Empirialestimatesofadaptation:Thechanceoftwonoriegasisclosertop/2thanp2.InColing-2000,Saarbr¨ucken,Germany.A.Dubey,F.Keller,andP.Sturt.2005.Parallelismincoordi-nationasaninstanceofsyntacticpriming:Evidencefromcorpus-basedmodeling.InProc.HLT/EMNLP-2005,pp.827–834.Vancouver,Canada.VicFerreiraandKathrynBock.2006.Thefunctionsofstruc-turalpriming.LanguageandCognitiveProcesses,21(7-8).StefanTh.Gries.2005.Syntacticpriming:Acorpus-basedap-proach.JofPsycholinguisticResearch,34(4):365–399.T.FlorianJ¨ager.2006.RedundancyandSyntacticReductioninSpontaneousSpeech.Ph.D.thesis,StanfordUniversity.AgnieszkaKonopkaandJ.KathrynBock.2005.Helpingsyntaxout:Whatdowordsdo?InProc.18thCUNY.Tucson,AZ.MartinJ.PickeringandHollyP.Branigan.1998.Therepresen-tationofverbs:Evidencefromsyntacticpriminginlanguageproduction.JournalofMemoryandLanguage,39:633–651.MartinJ.PickeringandSimonGarrod.2004.Towardamech-anisticpsychologyofdialogue.BehavioralandBrainSci-ences,27:169–225.D.Reitter,J.Hockenmaier,andF.Keller.2006a.Prim-ingeffectsinCombinatoryCategorialGrammar.InProc.EMNLP-2006,pp.308–316.Sydney,Australia.D.Reitter,J.D.Moore,andF.Keller.2006b.Primingofsyn-tacticrulesintask-orienteddialogueandspontaneouscon-versation.InProc.CogSci-2006,pp.685–690.Vancouver,Canada.BenediktSzmrecsanyi.2005.Creaturesofhabit:Acorpus-linguisticanalysisofpersistenceinspokenenglish.CorpusLinguisticsandLinguisticTheory,1(1):113–149.M.Walker,I.Langkilde,J.Wright,A.Gorin,andD.Litman.2000.Learningtopredictproblematicsituationsinaspokendialoguesystem:experimentswithHowmayIhelpyou?InProc.NAACL-2000,pp.210–217.SanFrancisco,CA.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 816–823,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

816

ResolvingIt,This,andThatinUnrestrictedMulti-PartyDialogChristophM¨ullerEMLResearchgGmbHVillaBoschSchloß-Wolfsbrunnenweg3369118Heidelberg,Germanychristoph.mueller@eml-research.deAbstractWepresentanimplementedsystemfortheresolutionofit,this,andthatintran-scribedmulti-partydialog.Thesystemhan-dlesNP-anaphoricaswellasdiscourse-deicticanaphors,i.e.pronounswithVPan-tecedents.SelectionalpreferencesforNPorVPantecedentsaredeterminedonthebasisofcorpuscounts.Ourresultsshowthatthesystemperformssigniﬁcantlybetterthanarecency-basedbaseline.1IntroductionThispaperdescribesafullyautomaticsystemforresolvingthepronounsit,this,andthatinunre-strictedmulti-partydialog.ThesystemprocessesmanualtranscriptionsfromtheICSIMeetingCor-pus(Janinetal.,2003).Thefollowingisashortfragmentfromoneofthesetranscripts.ThelettersFNinthespeakertagmeanthatthespeakerisafe-malenon-nativespeakerofEnglish.Thebracketsandsubscriptnumbersarenotpartoftheoriginaltranscript.FN083:Maybeyoucanalsoreadthroughthe-allthetextwhichisonthewebpagescuzI’dliketochangethetextabitcuzsometimes[it]1’stoolong,sometimes[it]2’stooshort,inbreathmaybetheEnglishisnotthatgood,soin-breathum,butanyways-SoItriedtodo[this]3todayandifyoucoulddo[it]4afterwards[it]5wouldbereallynicecuzI’mquitesurethatIcan’tﬁndevery,like,ortho-graphicmistakein[it]6orsomething.(Bns003)Foreachofthesix3rd-personpronounsintheexam-ple,thetaskistoautomaticallyidentifyitsreferent,i.e.theentity(ifany)towhichthespeakermakesreference.Onceareferenthasbeenidentiﬁed,thepronounisresolvedbylinkingittooneofitsan-tecedents,i.e.oneofthereferent’searliermentions.Forhumans,identiﬁcationofapronoun’sreferentisofteneasy:it1,it2,andit6areprobablyusedtorefertothetextonthewebpages,whileit4isprob-ablyusedtorefertoreadingthistext.Humansalsohavenoproblemdeterminingthatit5isnotanormalpronounatall.Inothercases,resolvingapronounisdifﬁcultevenforhumans:this3couldbeusedtorefertoeitherreadingorchangingthetextonthewebpages.Thepronounisambiguousbecauseevi-denceformorethanoneinterpretationcanbefound.Ambiguouspronounsarecommoninspokendialog(Poesio&Artstein,2005),afactthathastobetakenintoaccountwhenbuildingaspokendialogpronounresolutionsystem.Oursystemisintendedasacom-ponentinanextractivedialogsummarizationsys-tem.Thereareseveralwaysinwhichcoreferencein-formationcanbeintegratedintoextractivesumma-rization.Kabadjovetal.(2005)e.g.obtainedtheirbestextractionresultsbyspecifyingforeachsen-tencewhetheritcontainedamentionofaparticularanaphoricchain.Apartfromimprovingtheextrac-tionitself,coreferenceinformationcanalsobeusedtosubstituteanaphorswiththeirantecedents,thusimprovingthereadabilityofasummarybyminimiz-ingthenumberofdanglinganaphors,i.e.anaphorswhoseantecedentsoccurinutterancesthatarenotpartofthesummary.Thepaperisstructuredasfol-lows:Section2outlinesthemostimportantchal-lengesandthestateoftheartinspokendialogpro-nounresolution.Section3describesourannotationexperiments,andSection4describestheautomatic817

dialogpreprocessing.ResolutionexperimentsandresultscanbefoundinSection5.2PronounResolutioninSpokenDialogSpokenlanguageposessomechallengesforpro-nounresolution.Someofthesearisefromnonrefer-entialresp.nonresolvablepronouns,whichareim-portanttoidentifybecausefailuretodosocanharmpronounresolutionprecision.Onecommontypeofnonreferentialpronounispleonasticit.Anothercauseofnonreferentialitythatonlyappliestospokenlanguageisthatthepronounisdiscarded,i.e.itispartofanincompleteorabandonedutterance.Dis-cardedpronounsoccurinutterancesthatareaban-donedaltogether.ME010:Yeah.Yeah.No,no.Therewasawholeco-Therewasalittlecontractsigned.Itwas-Yeah.(Bed017)Iftheutterancecontainsaspeechrepair(Heeman&Allen,1999),apronouninthereparandumpartisalsotreatedasdiscardedbecauseitisnotpartoftheﬁnalutterance.ME10:That’s-that’s-sothat’sa-that’saverygoodquestion,then-nowthatit-Iunderstandit.(Bro004)Inthecorpusoftask-orientedTRAINSdialogsde-scribedinByron(2004),therateofdiscardedpro-nounsis7outof57(12.3%)foritand7outof100(7.0%)forthat.Schiffman(1985)reportsthatinhercorpusofcareer-counselinginterviews,164outof838(19.57%)instancesofitand80outof582(13.75%)instancesofthatoccurinabandonedutterances.Thereisathirdclassofpronounswhichisreferen-tialbutnonethelessunresolvable:vaguepronouns(Eckert&Strube,2000)arecharacterizedbyhavingnoclearlydeﬁnedtextualantecedent.Rather,vaguepronounsareoftenusedtorefertothetopicofthecurrent(sub-)dialogasawhole.Finally,inspokenlanguagethepronounsit,this,andthatareoftendiscoursedeictic(Webber,1991),i.e.theyareusedtorefertoanabstractobject(Asher,1993).WetreatasabstractobjectsallreferentsofVPantecedents,anddonotdistinguishbetweenVPandSantecedents.ME013:Well,Imeanthere’sthisCyberTranscriberservice,right?ME025:Yeah,that’strue,that’strue.(Bmr001)Discoursedeixisisveryfrequentinspokendialog:TherateofdiscoursedeicticexpressionsreportedinEckert&Strube(2000)is11.8%forpronounsandasmuchas70.9%fordemonstratives.2.1StateoftheArtPronounresolutioninspokendialoghasnotreceivedmuchattentionyet,andamajorlimitationofthefewimplementedsystemsisthattheyarenotfullyau-tomatic.Instead,theydependonmanualremovalofunresolvablepronounslikepleonasticitanddis-cardedandvaguepronouns,whicharethuspre-ventedfromtriggeringaresolutionattempt.Thiseliminatesamajorsourceoferror,butitrendersthesystemsinapplicableinareal-worldsettingwherenosuchmanualpreprocessingisfeasible.Oneoftheearliestempiricallybasedworksadress-ing(discoursedeictic)pronounresolutioninspo-kendialogisEckert&Strube(2000).Theau-thorsoutlinetwoalgorithmsforidentifyingthean-tecedentsofpersonalanddemonstrativepronounsintwo-partytelephoneconversationsfromtheSwitch-boardcorpus.Thealgorithmsdependontwonon-trivialtypesofinformation:theincompatibilityofagivenpronounwitheitherconcreteorabstractan-tecedents,andthestructureofthedialogintermsofdialogacts.Thealgorithmsarenotimplemented,andEckert&Strube(2000)reportresultsofthemanualapplicationtoasetofthreedialogs(199ex-pressions,includingotherpronounsthanit,this,andthat).Precisionandrecallare66.2resp.68.2forpronounsand63.6resp.70.0fordemonstratives.Animplementedsystemforresolvingpersonalanddemonstrativepronounsintask-orientedTRAINSdialogsisdescribedinByron(2004).Thesystemusesanexplicitrepresentationofdomain-dependentsemanticcategoryrestrictionsforpredicateargu-mentpositions,andachievesaprecisionof75.0andarecallof65.0forit(50instances)andaprecisionof67.0andarecallof62.0forthat(93instances)ifallavailablerestrictionsareused.Precisiondropsto52.0foritand43.0forthatwhenonlydomain-independentrestrictionsareused.Toourknowledge,thereisonlyoneimplementedsystemsofarthatresolvesnormalanddiscoursede-icticpronounsinunrestrictedspokendialog(Strube&M¨uller,2003).ThesystemrunsondialogsfromtheSwitchboardportionofthePennTreebank.For818

it,thisandthat,theauthorsreport40.41precisionand12.64recall.Therecalldoesnotreﬂecttheac-tualpronounresolutionperformanceasitiscalcu-latedagainstallcoreferentiallinksinthecorpus,notjustthosewithpronominalanaphors.Thesystemdrawssomenon-trivialinformationfromthePennTreebank,includingcorrectNPchunks,grammati-calfunctiontags(subject,object,etc.)anddiscardedpronouns(basedonthe-UNF-tag).Thetreebankinformationisalsousedfordeterminingtheacces-sibilityofpotentialcandidatesfordiscoursedeicticpronouns.Incontrasttotheseapproaches,theworkdescribedinthefollowingisfullyautomatic,usingonlyinfor-mationfromtheraw,transcribedcorpus.Nomanualpreprocessingisperformed,sothatduringtesting,thesystemisexposedtothefullrangeofdiscarded,pleonastic,andotherunresolvablepronouns.3DataCollectionTheICSIMeetingCorpus(Janinetal.,2003)isacollectionof75manuallytranscribedgroupdis-cussionsofaboutonehoureach,involvingthreetotenspeakers.Aconsiderablenumberofpartic-ipantsarenon-nativespeakersofEnglish,whoseproﬁciencyissometimespoor,resultingindisﬂu-entorincomprehensiblespeech.Thediscussionsarereal,unstagedmeetingsonvarious,technicaltopics.Mostofthediscussionsareregularweeklymeet-ingsofaquiteinformalconversationalstyle,con-tainingmanyinterrupts,asides,andjokes(Janin,2002).Thecorpusfeaturesasemi-automaticallygeneratedsegmentationinwhicheachsegmentisas-sociatedwithaspeakertagandastartandendtimestamp.Timestampsonthewordlevelarenotavail-able.Thetranscriptioncontainscapitalizationandpunctuation,anditalsoexplicitlyrecordsinterrup-tionpointsandwordfragments(Heeman&Allen,1999),butnottheextentoftherelateddisﬂuencies.3.1AnnotationTheannotationwasdonebynaiveproject-externalannotators,twonon-nativeandtwonativespeak-ersofEnglish,withtheannotationtoolMMAX21onﬁverandomlyselecteddialogs2.Theannotation1http://mmax.eml-research.de2Bed017,Bmr001,Bns003,Bro004,andBro005.instructionsweredeliberatelykeptsimple,explain-ingandillustratingthebasicnotionsofanaphoraanddiscoursedeixis,anddescribinghowmarkablesweretobecreatedandlinkedintheannotationtool.Thispracticeofusingahighernumberofnaive–ratherthanfewer,highlytrained–annotatorswasmotivatedbyourintentiontoelicitasmanyplau-sibleinterpretationsaspossibleinthepresenceofambiguity.Itwasinspiredbytheannotationex-perimentsofPoesio&Artstein(2005)andArtstein&Poesio(2006).Theirexperimentsemployedupto20annotators,andtheyallowedfortheexplicitannotationofambiguity.Incontrast,ourannota-torswereinstructedtochoosethesinglemostplau-sibleinterpretationincaseofperceivedambigu-ity.Theannotationcoveredthepronounsit,this,andthatonly.Markablesforthesetokenswerecreatedautomatically.Fromamongthepronomi-nal3instances,theannotatorsthenidentiﬁednormal,vague,andnonreferentialpronouns.Fornormalpro-nouns,theyalsomarkedthemostrecentantecedentusingtheannotationtool’scoreferenceannotationfunction.Markablesforantecedentsotherthanit,this,andthathadtobecreatedbytheannotatorsbydraggingthemouseovertherespectivewordsinthetool’sGUI.Nominalantecedentscouldbeei-thernounphrases(NP)orpronouns(PRO).VPan-tecedents(fordiscoursedeicticpronouns)spannedonlytheverbphrasehead,i.e.theverb,nottheen-tirephrase.Bythis,wetriedtoreducethenumberofdisagreementscausedbydifferingmarkablede-marcations.Theannotationofdiscoursedeixiswaslimitedtocaseswheretheantecedentwasaﬁniteorinﬁniteverbphraseexpressingaproposition,eventtype,etc.43.2ReliabilityInter-annotatoragreementwascheckedbycomput-ingthevariantofKrippendorff’sαdescribedinPas-sonneau(2004).Thismetricrequiresallannotationstocontainthesamesetofmarkables,aconditionthatisnotmetinourcase.Therefore,wereportαvaluescomputedontheintersectionofthecom-3Theautomaticallycreatedmarkablesincludedallinstancesofthisandthat,i.e.alsorelativepronouns,determiners,com-plementizers,etc.4Arbitraryspansoftextcouldnotserveasantecedentsfordiscoursedeicticpronouns.Therespectivepronounsweretobetreatedasvague,duetolackofawell-deﬁnedantecedent.819

paredannotations,i.e.onthosemarkablesthatcanbefoundinallfourannotations.Onlyasubsetofthemarkablesineachannotationisrelevantforthedeterminationofinter-annotatoragreement:allnon-pronominalmarkables,i.e.allantecedentmarkablesmanuallycreatedbytheannotators,andallreferen-tialinstancesofit,this,andthat.ThesecondcolumninTable1containsthecardinalityoftheunionofallfourannotators’markables,i.e.thenumberofalldistinctrelevantmarkablesinallfourannotations.Thethirdandfourthcolumncontainthecardinalityandtherelativesizeoftheintersectionofthesefourmarkablesets.Theﬁfthcolumncontainsαcalcu-latedonthemarkablesintheintersectiononly.Thefourannotatorsonlyagreedintheidentiﬁcationofmarkablesinapprox.28%ofcases.αintheﬁvedialogsrangesfrom.43to.52.|1∪2∪3∪4||1∩2∩3∩4|αBed01739710927.46%.47Bmr00161919531.50%.43Bns00352913124.76%.45Bro00470314220.20%.45Bro00553013224.91%.52Table1:Krippendorff’sαforfourannotators.3.3DataSubsetsInviewofthesubjectivityoftheannotationtask,whichispartlyreﬂectedinthelowagreementevenonmarkableidentiﬁcation,themanualcreationofaconsensus-basedgoldstandarddatasetdidnotseemfeasible.Instead,wecreatedcoredatasetsfromallfourannotationsbymeansofmajoritydecisions.Thecoredatasetsweregeneratedbyautomaticallycollectingineachdialogthoseanaphor-antecedentpairsthatatleastthreeannotatorsidentiﬁedindepen-dentlyofeachother.Therationaleforthisapproachwasthatananaphoriclinkisthemoreplausiblethemoreannotatorsidentifyit.Suchadatasetcertainlycontainssomespuriousordubiouslinks,whilelack-ingsomecorrectbutmoredifﬁcultones.However,wearguethatitconstitutesaplausiblesubsetofanaphoriclinksthatareusefultoresolve.Table2showsthenumberandlengthsofanaphoricchainsinthecoredataset,brokendownaccord-ingtothetypeofthechain-initialantecedent.TheraretypeOTHERmainlycontainsadjectivalan-tecedents.Morethan75%ofallchainsconsistoftwoelementsonly.Morethan33%beginwithapronoun.Fromtheperspectiveofextractivesum-marization,theresolutionoftheselatterchainsisnothelpfulsincethereisnonon-pronominalantecedentthatitcanbelinkedtoorsubstitutedwith.length23456>6totalBed017NP1732-1-23PRO14-2---16VP61----7OTHER-------all3744-1-4680.44%Bmr001NP144111223PRO199221134VP95----14OTHER-------all421833237159.16%Bns003NP18331--25PRO1811---20VP144----18OTHER-------all50841--6379.37%Bro004NP38531--47PRO214-1--26VP811---10OTHER21----3all691142--8680.23%Bro005NP3771---45PRO1531---19VP81-1--10OTHER3-----3all631121--7781.82%ΣNP1242210322163PRO87176311115VP451211--59OTHER51----6all261521773334376.01%Table2:Anaphoricchainsincoredataset.4AutomaticPreprocessingDatapreprocessingwasdonefullyautomatically,usingonlyinformationfromthemanualtran-scription.Punctuationsignsandsomeheuristicswereusedtospliteachdialogintoasequenceofgraphemicsentences.Then,ashallowdisﬂu-encydetectionandremovalmethodwasapplied,whichremoveddirectrepetitions,nonlexicalizedﬁlledpauseslikeuh,um,interruptionpoints,andwordfragments.Eachsentencewasthenmatchedagainstalistofpotentialdiscoursemarkers(actu-ally,like,youknow,Imean,etc.)Ifasentencecontainedoneormorematches,stringvariantswerecreatedinwhichtherespectivewordsweredeleted.Eachofthesevariantswasthensubmittedtoaparsertrainedonwrittentext(Charniak,2000).Thevari-antwiththehighestprobability(asdeterminedbytheparser)waschosen.NPchunkmarkableswerecreatedforallnon-recursiveNPconstituentsidenti-820

ﬁedbytheparser.Then,VPchunkmarkableswerecreated.ComplexverbalconstructionslikeMD+INFINITIVEweremodelledbycreatingmarkablesfortheindividualexpressions,andattachingthemtoeachotherwithlabelledrelationslikeINFINI-TIVECOMP.NPchunkswerealsoattached,usingrelationslikeSUBJECT,OBJECT,etc.5AutomaticPronounResolutionWemodelpronounresolutionasbinaryclassiﬁca-tion,i.e.asthemappingofanaphoricmentionstopreviousmentionsofthesamereferent.Thismethodisnotincremental,i.e.itcannottakeintoaccountearlierresolutiondecisionsoranyotherinformationbeyondthatwhichisconveyedbythetwomentions.Sincemorethan75%oftheanaphoricchainsinourdatasetwouldnotbeneﬁtfromincrementalprocess-ingbecausetheycontainoneanaphoronly,weseethislimitationasacceptable.Inaddition,incremen-talprocessingbearstheriskofsystemdegradationduetoerrorpropagation.5.1FeaturesInthebinaryclassiﬁcationmodel,apronounisre-solvedbycreatingasetofcandidateantecedentsandsearchingthissetforamatchingone.Thissearchprocessismainlyinﬂuencedbytwofactors:ex-clusionofcandidatesduetoconstraints,andselec-tionofcandidatesduetopreferences(Mitkov,2002).Ourfeaturesencodeinformationrelevanttothesetwofactors,plusmoregenerallydescriptivefactorslikedistanceetc.Computationofallfeatureswasfullyautomatic.Shallowconstraintsfornominalantecedentsincludenumber,genderandpersonincompatibility,embed-dingoftheanaphorintotheantecedent,andcoar-gumenthood(i.e.theantecedentandanaphormustnotbegovernedbythesameverb).ForVPan-tecedents,acommonshallowconstraintisthattheanaphormustnotbegovernedbytheVPantecedent(so-calledargumenthood).Preferences,ontheotherhand,deﬁneconditionsunderwhichacandidateprobablyisthecorrectantecedentforagivenpro-noun.Acommonshallowpreferencefornomi-nalantecedentsistheparallelfunctionpreference,whichstatesthatapronounwithaparticulargram-maticalfunction(i.e.subjectorobject)preferablyhasanantecedentwithasimilarfunction.Thesub-jectpreference,incontrast,statesthatsubjectan-tecedentsaregenerallypreferredoverthosewithlesssalientfunctions,independentofthegrammat-icalfunctionoftheanaphor.Someofourfeaturesencodethisfunctionalandstructuralparallelism,in-cludingidentityofform(forPROantecedents)andidentityofgrammaticalfunctionorgoverningverb.AmoresophisticatedconstraintonNPan-tecedentsiswhatEckert&Strube(2000)callI-Incompatibility,i.e.thesemanticincompatibilityofapronounwithanindividual(i.e.NP)antecedent.AsEckert&Strube(2000)note,subjectpronounsincopulaconstructionswithadjectivesthatcanonlymodifyabstractentities(likee.g.true,correct,right)areincompatiblewithconcreteantecedentslikecar.Wepostulatethatthepreferenceofanadjectivetomodifyanabstractentity(inthesenseofEckert&Strube(2000))canbeoperationalizedasthecondi-tionalprobabilityoftheadjectivetoappearwithato-inﬁnitiveresp.athat-sentencecomplement,andintroducetwofeatureswhichcalculatetherespec-tivepreferenceonthebasisofcorpus5counts.Fortheﬁrstfeature,thefollowingqueryisused:#it(’s|is|was|were)ADJto#it(’s|is|was|were)ADJAccordingtoEckert&Strube(2000),pronounsthatareobjectsofverbswhichmainlytakesentencecomplements(likeassume,say)exhibitasimilarincompatibilitywithNPantecedents,andwecap-turethiswithasimilarfeature.ConstraintsforVPsincludethefollowing:VPsareinaccessiblefordiscoursedeicticreferenceiftheyfailtomeettherightfrontiercondition(Webber,1991).WeuseafeaturewhichissimilartothatusedbyStrube&M¨uller(2003)inthatitapproximatestherightfrontieronthebasisofsyntactic(ratherthandis-coursestructural)relations.AnotherconstraintisA-Incompatibility,i.e.theincompatibilityofapro-nounwithanabstract(i.e.VP)antecedent.Accord-ingtoEckert&Strube(2000),subjectpronounsincopulaconstructionswithadjectivesthatcanonlymodifyconcreteentities(likee.g.expensive,tasty)areincompatiblewithabstractantecedents,i.e.they5Basedontheapprox.250,000,000wordTIPSTERcorpus(Harman&Liberman,1994).821

cannotbediscoursedeictic.Thefunctionofthisconstraintisalreadycoveredbythetwocorpus-basedfeaturesdescribedaboveinthecontextofI-Incompatibility.Anotherfeature,basedonYangetal.(2005),encodesthesemanticcompatibilityofanaphorandNPantecedent.Weoperationalizetheconceptofsemanticcompatibilitybysubstitut-ingtheanaphorwiththeantecedentheadandper-formingcorpusqueries.E.g.,iftheanaphorisob-ject,thefollowingquery6isused:#(V|Vs|Ved|Ving)(∅|a|an|the|this|that)ANTE+#(V|Vs|Ved|Ving)(∅|the|these|those)ANTES#(ANTE|ANTES)Iftheanaphoristhesubjectinanadjectivecop-ulaconstruction,weusethefollowingcorpuscounttoquantifythecompatibilitybetweenthepredi-catedadjectiveandtheNPantecedent(Lapataetal.,1999):#ADJ(ANTE|ANTES)+#ANTE(is|was)ADJ+#ANTES(are|were)ADJ#ADJAthirdclassofmoregeneralpropertiesofthepo-tentialanaphor-antecedentpairincludesthetypeofanaphor(personalvs.demonstrative),typeofan-tecedent(deﬁnitevs.indeﬁnitenounphrase,pro-noun,ﬁnitevs.inﬁniteverbphrase,etc.).Specialfeaturesfortheidentiﬁcationofdiscardedexpres-sionsincludethedistance(inwords)totheclosestpreceedingresp.followingdisﬂuency(indicatedinthetranscriptionasaninterruptionpoint,wordfrag-ment,oruhresp.um).Therelationbetweenpo-tentialanaphorand(anytypeof)antecedentisde-scribedintermsofdistanceinseconds7andwords.ForVPantecedents,thedistanceiscalculatedfromthelastwordintheentirephrase,notfromthephrasehead.Anotherfeaturewhichisrelevantfordialogencodeswhetherbothexpressionsareutteredbythesamespeaker.6Vistheverbgoverningtheanaphor.Correctinﬂectedformswerealsogeneratedforirregularverbs.ANTEresp.ANTESisthesingularresp.pluralheadoftheantecedent.7Sincethedatadoesnotcontainword-leveltimestamps,thisdistanceisdeterminedonthebasisofasimpleforcedalign-ment.Forthis,weestimatedthenumberofsyllablesineachwordonthebasisofitsvowelclusters,andsimplydistributedtheknowndurationofthesegmentevenlyonallwordsitcon-tains.5.2DataRepresentationandGenerationMachinelearningdatafortrainingandtestingwascreatedbypairingeachanaphorwitheachofitscompatiblepotentialantecedentswithinacertaintemporaldistance(9secondsforNPand7secondsforVPantecedents),andlabellingtheresultingdatainstanceaspositiveresp.negative.VPantecedentcandidateswerecreatedonlyiftheanaphorwasei-therthat8ortheobjectofaformofdo.Ourcoredatasetdoesnotcontainanynonreferen-tialpronouns,thoughtheclassiﬁerisexposedtothefullrangeofpronouns,includingdiscardedandoth-erwisenonreferentialones,duringtesting.Wetrytomaketheclassiﬁerrobustagainstnonreferentialpronounsinthefollowingway:Fromthemanualannotations,weselectinstancesofit,this,andthatthatatleastthreeannotatorsidentiﬁedasnonrefer-ential.Foreachofthese,weaddthefullrangeofall-negativeinstancestothetrainingdata,applyingtheconstraintsmentionedabove.5.3EvaluationMeasureAsBagga&Baldwin(1998)pointout,inanapplication-orientedsetting,notallanaphoriclinksareequallyimportant:Ifapronounisresolvedtoananaphoricchainthatcontainsonlypronouns,thisresolutioncanbetreatedasneutralbecauseithasnoapplication-leveleffect.Thecommoncorefer-enceevaluationmeasuredescribedinVilainetal.(1995)isinappropriateinthissetting.Wecalculateprecision,recallandF-measureonthebasisofthefollowingdeﬁnitions:Apronounisresolvedcor-rectlyresp.incorrectlyonlyifitislinked(directlyortransitively)tothecorrectresp.incorrectnon-pronominalantecedent.Likewise,thenumberofmaximallyresolvablepronounsinthecoredataset(i.e.theevaluationkey)isdeterminedbyconsider-ingonlypronounsinthosechainsthatdonotbeginwithapronoun.Notethatourdeﬁnitionofprecisionisstricter(andyieldslowerﬁgures)thanthatap-pliedintheACEcontext,asthelatterignoresincor-rectlinksbetweentwoexpressionsintheresponse8Itisacommonobservationthatdemonstratives(inpartic-ularthat)arepreferredoveritfordiscoursedeicticreference(Schiffman,1985;Webber,1991;Asher,1993;Eckert&Strube,2000;Byron,2004;Poesio&Artstein,2005).Thispreferencecanalsobeobservedinourcoredataset:44outof59VPan-tecedents(69.49%)areanaphoricallyreferredtobythat.822

iftheseexpressionshappentobeunannotatedinthekey,whilewetreatthemasprecisionerrorsunlesstheantecedentisapronoun.Thesameistrueforlinksintheresponsethatwereidentiﬁedbylessthanthreeannotatorsinthekey.Whileitispracticaltotreatthoselinksaswrong,itisalsosimplisticbe-causeitdoesnotdojusticetoambiguouspronouns(cf.Section6).5.4ExperimentsandResultsOurbestmachinelearningresultswereobtainedwiththeWeka9LogisticRegressionclassiﬁer.10Allexperimentswereperformedwithdialog-wisecross-validation.Foreachrun,trainingdatawascreatedfromthemanuallyannotatedmarkablesinfourdi-alogsfromthecoredataset,whiletestingwasper-formedontheautomaticallydetectedchunksintheremainingﬁfthdialog.Fortrainingandtesting,theperson,number11,gender,and(co-)argumentcon-straintswereused.Ifananaphorgaverisetoapos-itiveinstance,nonegativetraininginstanceswerecreatedbeyondthatinstance.Ifareferentialanaphordidnotgiverisetoapositivetraininginstance(be-causeitsantecedentfelloutsidethesearchscopeorbecauseitwasremovedbyaconstraint),noin-stanceswerecreatedforthatanaphor.InstancesfornonreferentialpronounswereaddedtothetrainingdataasdescribedinSection5.2.Duringtesting,weselectforeachpotentialanaphorthepositiveantecedentwiththehighestoverallcon-ﬁdence.Testingparametersincludeit-filter,whichswitchesonandoffthemoduleforthedetec-tionofnonreferentialitdescribedinM¨uller(2006).Whenevaluatedalone,thismoduleyieldsapreci-sionof80.0andarecallof60.9forthedetectionofpleonasticanddiscardeditintheﬁveICSIdi-alogs.Fortraining,thismodulewasalwayson.Wealsovarytheparametertipster,whichcon-trolswhetherornotthecorpusfrequencyfeaturesareused.Iftipsterisoff,weignorethecorpusfrequencyfeaturesbothduringtrainingandtesting.Weﬁrstranasimplebaselinesystemwhichre-solvedpronounstotheirmostrecentcompatiblean-tecedent,applyingthesamesettingsandconstraints9http://www.cs.waikato.ac.nz/ml/weka/10ThefullsetofexperimentsisdescribedinM¨uller(2007).11Thenumberconstraintappliestoitonly,asthisandthatcanhavebothsingularandpluralantecedents(Byron,2004).asfortesting(cf.above).TheresultscanbefoundintheﬁrstpartofTable3.Precision,recallandF-measureareprovidedforALLandforNPandVPantecedentsindividually.Theparametertipsterisnotavailableforthebaselinesystem.Thebestbaselineperformanceisprecision4.88,recall20.06andF-measure7.85inthesettingwithit-filteron.Asexpected,thisﬁlteryieldsanincreaseinpre-cisionandadecreaseinrecall.Thenegativeeffectisoutweighedbythepositiveeffect,leadingtoasmallbutinsigniﬁcant12increaseinF-measureforalltypesofantecedents.BaselineLogisticRegressionSettingAntePRFPRF-it-ﬁlter-tipsterNP4.6227.127.9018.5320.3419.39∗VP1.722.632.0813.7910.5311.94ALL4.4020.697.2517.6717.5617.61∗+tipsterNP---19.3322.0320.59∗∗∗VP---13.4311.8412.59ALL---18.1619.1218.63∗∗+it-ﬁlter-tipsterNP5.1826.278.6517.8717.8017.83∗VP1.772.632.1213.1210.5311.68ALL4.8820.067.8516.8915.6716.26∗+tipsterNP---20.8221.6121.21∗∗VP---11.2710.5310.88ALL---18.6718.5018.58∗∗Table3:Resolutionresults.ThesecondpartofTable3showstheresultsoftheLogisticRegressionclassiﬁer.Whencomparedtothebestbaseline,theF-measuresareconsistentlybetterforNP,VP,andALL.Theimprovementis(sometimeshighly)signiﬁcantforNPandALL,butneverforVP.ThebestF-measureforALLis18.63,yieldedbythesettingwithit-filteroffandtipsteron.ThissettingalsoyieldsthebestF-measureforVPandthesecondbestforNP.Thecontributionoftheit-ﬁlterisdisappointing:Inbothtipstersettings,theit-ﬁltercausesF-measureforALLtogodown.Thecontributionofthecorpusfeatures,ontheotherhand,issomewhatinconclu-sive:Inbothit-filtersettings,theycauseanin-creaseinF-measureforALL.Intheﬁrstsetting,thisincreaseisaccompaniedbyanincreaseinF-measureforVP,whileinthesecondsetting,F-measureforVPgoesdown.Ithastobenoted,however,thatnoneoftheimprovementsbroughtaboutbytheit-ﬁlterorthetipstercorpusfeaturesisstatisticallysig-niﬁcant.ThisalsoconﬁrmssomeoftheﬁndingsofKehleretal.(2004),whofoundfeaturessimilarto12SigniﬁcanceofimprovementinF-measureistestedusingapairedone-tailedt-testandp<=0.05(∗),p<=0.01(∗∗),andp<=0.005(∗∗∗).823

ourtipstercorpusfeaturesnottobesigniﬁcantforNP-anaphoricpronounresolutioninwrittentext.6ConclusionsandFutureWorkThesystemdescribedinthispaperis–toourknowl-edge–theﬁrstattempttowardsfullyautomaticres-olutionofNP-anaphoricanddiscoursedeicticpro-nouns(it,this,andthat)inmulti-partydialog.Un-likeotherimplementedsystems,itisusableinare-alisticsettingbecauseitdoesnotdependonmanualpronounpreselectionornon-trivialdiscoursestruc-tureordomainknowledge.Thedownsideisthat,atleastinourstrictevaluationscheme,theperfor-manceisratherlow,especiallywhencomparedtothatofstate-of-the-artsystemsforpronounresolu-tioninwrittentext.Infuturework,itmightbeworthwhiletoconsiderlessrigorousandthusmoreappropriateevaluationschemesinwhichlinksareweightedaccordingtohowmanyannotatorsidenti-ﬁedthem.Initscurrentstate,thesystemonlyprocessesman-ualdialogtranscripts,butitalsoneedstobeeval-uatedontheoutputofanautomaticspeechrecog-nizer.Whilethiswilladdmorenoise,itwillalsogiveaccesstousefulprosodicfeatureslikestress.Finally,thesystemalsoneedstobeevaluatedextrin-sically,i.e.withrespecttoitscontributiontodialogsummarization.Itmightturnoutthatoursystemal-readyhasapositiveeffectonextractivesummariza-tion,eventhoughitsperformanceislowinabsoluteterms.Acknowledgments.ThisworkhasbeenfundedbytheDeutscheForschungsgemeinschaftaspartoftheDIANA-Summproject(STR-545/2-1,2)andbytheKlausTschiraFoundation.WearegratefultotheanonymousACLreviewersforhelpfulcommentsandsuggestions.WealsothankRonArtsteinforhelpwithsigniﬁcancetesting.ReferencesArtstein,R.&M.Poesio(2006).Identifyingreferencetoab-stractobjectsindialogue.InProc.ofBranDial-06,pp.56–63.Asher,N.(1993).ReferencetoAbstractObjectsinDiscourse.Dordrecht,TheNetherlands:Kluwer.Bagga,A.&B.Baldwin(1998).Algorithmsforscoringcoref-erencechains.InProc.ofLREC-98,pp.79–85.Byron,D.K.(2004).Resolvingpronominalreferencetoab-stractentities.,(Ph.D.thesis).UniversityofRochester.Charniak,E.(2000).Amaximum-entropy-inspiredparser.InProc.ofNAACL-00,pp.132–139.Eckert,M.&M.Strube(2000).Dialogueacts,synchronis-ingunitsandanaphoraresolution.JournalofSemantics,17(1):51–89.Harman,D.&M.Liberman(1994).TIPSTERCompleteLDC93T3A.3CD-ROMS.LinguisticDataConsortium,Philadelphia,Penn.,USA.Heeman,P.&J.Allen(1999).Speechrepairs,intonationalphrases,anddiscoursemarkers:Modelingspeakers’ut-terancesinspokendialogue.ComputationalLinguistics,25(4):527–571.Janin,A.(2002).Meetingrecorder.InProceedingsoftheAppliedVoiceInput/OutputSocietyConference(AVIOS),SanJose,California,USA,May2002.Janin,A.,D.Baron,J.Edwards,D.Ellis,D.Gelbart,N.Mor-gan,B.Peskin,T.Pfau,E.Shriberg,A.Stolcke&C.Wooters(2003).TheICSIMeetingCorpus.InPro-ceedingsoftheIEEEInternationalConferenceonAcous-tics,SpeechandSignalProcessing,HongKong,pp.364–367.Kabadjov,M.A.,M.Poesio&J.Steinberger(2005).Task-basedevaluationofanaphoraresolution:Thecaseofsummarization.InProceedingsoftheRANLPWorkshoponCrossingBarriersinTextSummarizationResearch,Borovets,Bulgaria.Kehler,A.,D.Appelt,L.Taylor&A.Simma(2004).The(non)utilityofpredicate-argumentfrequenciesforpro-nouninterpretation.InProc.ofHLT-NAACL-04,pp.289–296.Lapata,M.,S.McDonald&F.Keller(1999).Determinantsofadjective-nounplausibility.InProc.ofEACL-99,pp.30–36.Mitkov,R.(2002).AnaphoraResolution.London,UK:Long-man.M¨uller,C.(2006).Automaticdetectionofnonreferentialitinspokenmulti-partydialog.InProc.ofEACL-06,pp.49–56.M¨uller,C.(2007).Fullyautomaticresolutionofit,this,andthatinunrestrictedmulti-partydialog.,(Ph.D.thesis).EberhardKarlsUniversit¨atT¨ubingen,Germany.Toap-pear.Passonneau,R.J.(2004).Computingreliabilityforco-referenceannotation.InProc.ofLREC-04.Poesio,M.&R.Artstein(2005).Thereliabilityofanaphoricannotation,reconsidered:Takingambiguityintoaccount.InProceedingsoftheACLWorkshoponFrontiersinCor-pusAnnotationII:PieintheSky,pp.76–83.Schiffman,R.J.(1985).Discourseconstraintson’it’and’that’:AStudyofLanguageUseinCareerCounselingInterviews.,(Ph.D.thesis).UniversityofChicago.Strube,M.&C.M¨uller(2003).Amachinelearningapproachtopronounresolutioninspokendialogue.InProc.ofACL-03,pp.168–175.Vilain,M.,J.Burger,J.Aberdeen,D.Connolly&L.Hirschman(1995).Amodel-theoreticcoreferencescoringscheme.InProc.ofMUC-6,pp.45–52.Webber,B.L.(1991).Structureandostensionintheinterpre-tationofdiscoursedeixis.LanguageandCognitivePro-cesses,6(2):107–135.Yang,X.,J.Su&C.L.Tan(2005).Improvingpronounreso-lutionusingstatistics-basedsemanticcompatibilityinfor-mation.InProc.ofACL-05,pp.165–172.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 824–831,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

824

A Comparative Study of Parameter Estimation Methods for Statistical Natural Language Processing Jianfeng Gao*, Galen Andrew*, Mark Johnson*&, Kristina Toutanova* *Microsoft Research, Redmond WA 98052, {jfgao,galena,kristout}@microsoft.com &Brown University, Providence, RI 02912,  mj@cs.brown.edu  Abstract This paper presents a comparative study of five parameter estimation algorithms on four NLP tasks. Three of the five algorithms are well-known in the computational linguistics community: Maximum Entropy (ME) estima-tion with L2 regularization, the Averaged Perceptron (AP), and Boosting.  We also in-vestigate ME estimation with L1 regularization using a novel optimization algorithm, and BLasso, which is a version of Boosting with Lasso (L1) regularization.  We first investigate all of our estimators on two re-ranking tasks: a parse selection task and a language model (LM) adaptation task.  Then we apply the best of these estimators to two additional tasks involving conditional sequence models: a Conditional Markov Model (CMM) for part of speech tagging and a Conditional Random Field (CRF) for Chinese word segmentation. Our experiments show that across tasks, three of the estimators — ME estimation with L1 or L2 regularization, and AP — are in a near sta-tistical tie for first place. 1 Introduction Parameter estimation is fundamental to many sta-tistical approaches to NLP. Because of the high-dimensional nature of natural language, it is often easy to generate an extremely large number of features.  The challenge of parameter estimation is to find a combination of the typically noisy, re-dundant features that accurately predicts the target output variable and avoids overfitting. Intuitively, this can be achieved either by selecting a small number of highly-effective features and ignoring the others, or by averaging over a large number of weakly informative features.  The first intuition motivates feature selection methods such as Boosting and BLasso (e.g., Collins 2000; Zhao and Yu, 2004), which usually work best when many features are completely irrelevant. L1 or Lasso regularization of linear models, introduced by Tibshirani (1996), embeds feature selection into regularization so that both an assessment of the reliability of a feature and the decision about whether to remove it are done in the same frame-work, and has generated a large amount of interest in the NLP community recently (e.g., Goodman 2003; Riezler and Vasserman 2004).  If on the other hand most features are noisy but at least weakly correlated with the target, it may be reasonable to attempt to reduce noise by averaging over all of the features.  ME estimators with L2 regularization, which have been widely used in NLP tasks (e.g., Chen and Rosenfeld 2000; Charniak and Johnson 2005; Johnson et al. 1999), tend to produce models that have this property.  In addition, the perceptron algorithm and its variants, e.g., the voted or aver-aged perceptron, is becoming increasingly popular due to their competitive performance, simplicity in implementation and low computational cost in training (e.g., Collins 2002). While recent studies claim advantages for L1 regularization, this study is the first of which we are aware to systematically compare it to a range of estimators on a diverse set of NLP tasks.  Gao et al. (2006) showed that BLasso, due to its explicit use of L1 regularization, outperformed Boosting in the LM adaptation task.  Ng (2004) showed that for logistic regression, L1 regularization outperforms L2 regu-larization on artificial datasets which contain many completely irrelevant features.  Goodman (2003) showed that in two out of three tasks, an ME esti-mator with a one-sided Laplacian prior (i.e., L1 regularization with the constraint that all feature weights are positive) outperformed a comparable estimator using a Gaussian prior (i.e., L2 regulari-zation).  Riezler and Vasserman (2004) showed that an L1-regularized ME estimator outperformed an L2-regularized estimator for ranking the parses of a stochastic unification-based grammar. 825

While these individual estimators are well de-scribed in the literature, little is known about the relative performance of these methods because the published results are generally not directly compa-rable.  For example, in the parse re-ranking task, one cannot tell whether the L2- regularized ME approach used by Charniak and Johnson (2005) significantly outperforms the Boosting method by Collins (2000) because different feature sets and n-best parses were used in the evaluations of these methods.  This paper conducts a much-needed comparative study of these five parameter estimation algorithms on four NLP tasks: ME estimation with L1 and L2 regularization, the Averaged Perceptron (AP), Boosting, and BLasso, a version of Boosting with Lasso (L1) regularization.  We first investigate all of our estimators on two re-ranking tasks: a parse selection task and a language model adaptation task. Then we apply the best of these estimators to two additional tasks involving conditional sequence models: a CMM for POS tagging and a CRF for Chinese word segmentation.  Our results show that ME estimation with L2 regularization achieves the best performing estimators in all of the tasks, and AP achieves almost as well and requires much less training time. L1 (Lasso) regularization also per-forms well and leads to sparser models. 2 Estimators All the four NLP tasks studied in this paper are based on linear models (Collins 2000) which re-quire learning a mapping from inputs 𝑥∈𝑋to outputs 𝑦∈𝑌.  We are given:  Training samples (𝑥𝑖,𝑦𝑖) for 𝑖=1…𝑁,  A procedure 𝑮𝑬𝑵 to generate a set of candi-dates 𝑮𝑬𝑵(𝑥) for an input x,   A feature mapping Φ:𝑋×𝑌↦ℝ𝐷 to map each (𝑥,𝑦) to a vector of feature values, and  A parameter vector 𝒘∈ℝ𝐷, which assigns a real-valued weight to each feature. For all models except the CMM sequence model for POS tagging, the components 𝑮𝑬𝑵, Φ and 𝒘 di-rectly define a mapping from an input 𝑥 to an output 𝐹(𝑥) as follows: 𝐹 𝑥 =arg max𝑦∈𝑮𝑬𝑵 𝑋 Φ 𝑥,𝑦 ⋅𝒘. (1) In the CMM sequence classifier, locally normalized linear models to predict the tag of each word token are chained together to arrive at a probability esti-mate for the entire tag sequence, resulting in a slightly different decision rule. Linear models, though simple, can capture very complex dependencies because the features can be arbitrary functions of the input/output pair.  For example, we can define a feature to be the log con-ditional probability of the output as estimated by some other model, which may in turn depend on arbitrarily complex interactions of „basic‟ features.  In practice, with an appropriate feature set, linear models achieve very good empirical results on various NLP tasks.  The focus of this paper however is not on feature definition (which requires domain knowledge and varies from task to task), but on parameter estimation (which is generic across tasks).  We assume we are given fixed feature templates from which a large number of features are generated.  The task of the estimator is to use the training samples to choose a parameter vector 𝒘, such that the mapping 𝐹(𝑥) is capable of correctly classifying unseen examples. We will describe the five estimators in our study individually. 2.1 ME estimation with L2 regularization Like many linear models, the ME estimator chooses 𝒘 to minimize the sum of the empirical loss on the training set and a regularization term: 𝒘 =argmin𝒘  𝐿 𝒘 +𝑅 𝒘   . (2) In this case, the loss term L(w) is the negative con-ditional log-likelihood of the training data,  𝐿 𝒘 =− log𝑃 𝑦𝑖 𝑥𝑖)𝑛𝑖=1,  where 𝑃 𝑦 𝑥)=exp Φ 𝑥,𝑦 ⋅𝒘  exp(Φ 𝑥,𝑦′ ⋅𝒘)𝑦′∈𝐺𝐸𝑁 𝑥  and the regularizer term 𝑅 𝒘 =𝛼 𝑤𝑗2𝑗 is the weighted squared L2 norm of the parameters. Here,  is a parameter that controls the amount of regu-larization, optimized on held-out data.  This is one of the most popular estimators,  largely due to its appealing computational proper-ties: both 𝐿 𝒘  and 𝑅(𝒘) are convex and differen-tiable, so gradient-based numerical algorithms can be used to find the global minimum efficiently.  In our experiments, we used the limited memory quasi-Newton algorithm (or L-BFGS, Nocedal and Wright 1999) to find the optimal 𝒘 because this method has been shown to be substantially faster than other methods such as Generalized Iterative Scaling (Malouf 2002).  826

Because for some sentences there are multiple best parses (i.e., parses with the same F-Score), we used the variant of ME estimator described in Riezler et al. (2002), where 𝐿 𝒘  is defined as the likelihood of the best parses 𝑦∈𝑌(𝑥) relative to the n-best parser output 𝑮𝑬𝑵 𝑥 , (i.e., 𝑌 𝑥 ⊑𝑮𝑬𝑵(𝑥)): 𝐿 𝒘 =− log 𝑃(𝑦𝑖|𝑥𝑖)𝑦𝑖∈𝑌(𝑥𝑖)𝑛𝑖=1. We applied this variant in our experiments of parse re-ranking and LM adaptation, and found that on both tasks it leads to a significant improvement in performance for the L2-regularied ME estimator but not for the L1-regularied ME estimator. 2.2 ME estimation with L1 regularization This estimator also minimizes the negative condi-tional log-likelihood, but uses an L1 (or Lasso) penalty. That is, 𝑅(𝒘) in Equation (2) is defined according to 𝑅 𝒘 =𝛼  𝑤𝑗 𝑗. L1 regularization typically leads to sparse solutions in which many feature weights are exactly zero, so it is a natural candidate when feature selection is desirable. By contrast, L2 regularization produces solutions in which most weights are small but non-zero. Optimizing the L1-regularized objective function is challenging because its gradient is discontinuous whenever some parameter equals zero. Kazama and Tsujii (2003) described an estimation method that constructs an equivalent constrained optimization problem with twice the number of variables.  However, we found that this method is impracti-cally slow for large-scale NLP tasks. In this work we use the orthant-wise limited-memory qua-si-Newton algorithm (OWL-QN), which is a mod-ification of L-BFGS that allows it to effectively handle the discontinuity of the gradient (Andrew and Gao 2007). We provide here a high-level de-scription of the algorithm. A quasi-Newton method such as L-BFGS uses first order information at each iterate to build an approximation to the Hessian matrix, 𝑯, thus mod-eling the local curvature of the function. At each step, a search direction is chosen by minimizing a quadratic approximation to the function: 𝑄 𝑥 =12 𝑥−𝑥0 ′𝑯 𝑥−𝑥0 +𝑔0′(𝑥−𝑥0) where 𝑥0 is the current iterate, and 𝑔0 is the func-tion gradient at 𝑥0.  If 𝑯 is positive definite, the minimizing value of 𝑥 can be computed analytically according to: 𝑥∗=𝑥0−𝑯−1𝑔0. L-BFGS maintains vectors of the change in gradient 𝑔𝑘−𝑔𝑘−1 from the most recent iterations, and uses them to construct an estimate of the inverse Hessian 𝑯−𝟏. Furthermore, it does so in such a way that 𝑯−1𝑔0 can be computed without expanding out the full matrix, which is typically unmanageably large. The computation requires a number of operations linear in the number of variables. OWL-QN is based on the observation that when restricted to a single orthant, the L1 regularizer is differentiable, and is in fact a linear function of 𝒘.  Thus, so long as each coordinate of any two con-secutive search points does not pass through zero, 𝑅(𝒘) does not contribute at all to the curvature of the function on the segment joining them.  There-fore, we can use L-BFGS to approximate the Hes-sian of 𝐿 𝒘  alone, and use it to build an approxi-mation to the full regularized objective that is valid on a given orthant. To ensure that the next point is in the valid region, we project each point during the line search back onto the chosen orthant.1 At each iteration, we choose the orthant containing the current point and into which the direction giving the greatest local rate of function decrease points. This algorithm, although only a simple modifi-cation of L-BFGS, works quite well in practice. It typically reaches convergence in even fewer itera-tions than standard L-BFGS takes on the analogous L2-regularized objective (which translates to less training time, since the time per iteration is only negligibly higher, and total time is dominated by function evaluations). We describe OWL-QN more fully in (Andrew and Gao 2007). We also show that it is significantly faster than Kazama and Tsujii‟s algorithm for L1 regularization and prove that it is guaranteed converge to a parameter vector that globally optimizes the L1-regularized objective. 2.3 Boosting The Boosting algorithm we used is based on Collins (2000).  It optimizes the pairwise exponential loss (ExpLoss) function (rather than the logarithmic loss optimized by ME).  Given a training sample (𝑥𝑖,𝑦𝑖), for each possible output 𝑦𝑗∈𝑮𝑬𝑵(𝑥𝑖), we                                                       1 This projection just entails zeroing-out any coordinates that change sign. Note that it is possible for a variable to change sign in two iterations, by moving from a negative value to zero, and on a the next iteration moving from zero to a positive value. 827

define the margin of the pair (𝑦𝑖,𝑦𝑗) with respect to 𝒘 as 𝑀 𝑦𝑖,𝑦𝑗 =Φ 𝑥𝑖,𝑦𝑖 ⋅𝒘− Φ 𝑥𝑖,𝑦𝑗 ⋅𝒘. Then ExpLoss is defined as ExpLoss 𝒘 =  exp −M yi,yj  𝑦𝑗∈𝑮𝑬𝑵 𝑥𝑖 𝑖 (3) Figure 1 summarizes the Boosting algorithm we used. It is an incremental feature selection proce-dure. After initialization, Steps 2 and 3 are repeated T times; at each iteration, a feature is chosen and its weight is updated as follows.  First, we define Upd(𝒘,𝑘,𝛿) as an updated model, with the same parameter values as 𝑤 with the exception of 𝑤𝑘, which is incremented by 𝛿: Upd 𝒘,𝑘,𝛿 =(𝑤1,…,𝑤𝑘+𝛿,…,𝑤𝐷)  Then, Steps 2 and 3 in Figure 1 can be rewritten as Equations (4) and (5), respectively.  𝑘∗,𝛿∗ =argmin𝑘,𝛿ExpLoss(Upd 𝒘,𝑘,𝛿 ) (4) 𝒘𝑡=Upd(𝒘𝑡−1,𝑘∗,𝛿∗) (5) Because Boosting can overfit we update the weight of 𝑓𝑘∗ by a small fixed step size , as in Equation (6), following the FSLR algorithm (Hastie et al. 2001).  𝒘𝑡=Upd(𝒘𝑡−1,𝑘∗,𝜖×sign 𝛿∗ ) (6) By taking such small steps, Boosting imposes a kind of implicit regularization, and can closely approximate the effect of L1 regularization in a local sense (Hastie et al. 2001).  Empirically, smaller values of 𝜖 lead to smaller numbers of test errors. 2.4 Boosted Lasso The Boosted Lasso (BLasso) algorithm was origi-nally proposed in Zhao and Yu (2004), and was adapted for language modeling by Gao et al. (2006). BLasso can be viewed as a version of Boosting with L1 regularization. It optimizes an L1-regularized ExpLoss function: LassoLoss 𝒘 =ExpLoss(𝒘)+𝑅(𝒘) (7) where 𝑅 𝒘 =𝛼  𝑤𝑗 𝑗 . BLasso also uses an incremental feature selec-tion procedure to learn parameter vector 𝒘, just as Boosting does.  Due to the explicit use of the regu-larization term 𝑅(𝒘), however, there are two major differences from Boosting.  At each iteration, BLasso takes either a forward step or a backward step.  Similar to Boosting, at each forward step, a feature is selected and its weight is updated according to Eq. (8) and (9).  𝑘∗,𝛿∗ =𝑎𝑟𝑔𝑚𝑖𝑛𝑘,𝛿=±𝜖ExpLoss(Upd 𝒘,𝑘,𝛿 ) (8) 𝒘𝑡=Upd(𝒘𝑡−1,𝑘∗,𝜖×sign 𝛿∗ ) (9) There is a small but important difference between Equations (8) and (4). In Boosting, as shown in Equation (4), a feature is selected by its impact on reducing the loss with its optimal update 𝛿∗. By contrast, in BLasso, as shown in Equation (8), rather than optimizing over 𝛿 for each feature, the loss is calculated with an update of either +𝜖 or −𝜖, i.e., grid search is used for feature weight estima-tion.  We found in our experiments that this mod-ification brings a consistent improvement. The backward step is unique to BLasso.  At each iteration, a feature is selected and the absolute value of its weight is reduced by 𝜖 if and only if it leads to a decrease of the LassoLoss, as shown in Equations (10) and (11), where   is a tolerance parameter. 𝑘∗=argmin𝑘:𝑤𝑘≠0ExpLoss(Upd(𝒘,𝑘,−𝜖sign 𝑤𝑘 ) (10) 𝒘𝑡=Upd(𝒘𝑡−1,𝑘∗,sign(𝑤𝑘∗)×𝜖)  (11) if LassoLoss 𝒘𝑡−1,𝛼𝑡−1 −LassoLoss 𝒘𝑡,𝛼𝑡 >𝜃 Figure 2 summarizes the BLasso algorithm we used. After initialization, Steps 4 and 5 are repeated T times; at each iteration, a feature is chosen and its weight is updated either backward or forward by a fixed amount 𝜖.  Notice that the value of 𝛼 is adap-tively chosen according to the reduction of ExpLoss during training.  The algorithm starts with a large initial 𝛼, and then at each forward step the value of 𝛼 decreases until ExpLoss stops decreasing.  This is intuitively desirable: it is expected that most highly effective features are selected in early stages of training, so the reduction of ExpLoss at each step in early stages are more substantial than in later stages.  These early steps coincide with the Boosting steps most of the time.  In other words, the effect of backward steps is more visible at later stages.  It can be proved that for a finite number of features and 𝜃=0, the BLasso algorithm shown in Figure 2 converges to the Lasso solution when 𝜖→0. See Gao et al. (2006) for implementation details, and Zhao and Yu (2004) for a theoretical justification for BLasso. 1 Set w0 = argminw0ExpLoss(w); and wd = 0 for d=1…D 2 Select a feature fk* which has largest estimated impact on reducing ExpLoss of Equation (3) 3 Update λk*   λk* + δ*, and return to Step 2 Figure 1: The boosting algorithm 828

2.5 Averaged Perceptron The perceptron algorithm can be viewed as a form of incremental training procedure (e.g., using sto-chastic approximation) that optimizes a minimum square error (MSE) loss function (Mitchell, 1997).  As shown in Figure 3, it starts with an initial pa-rameter setting and updates it for each training example. In our experiments, we used the Averaged Perceptron algorithm of Freund and Schapire (1999), a variation that has been shown to be more effective than the standard algorithm (Collins 2002).  Let 𝒘𝑡,𝑖 be the parameter vector after the 𝑖th training sample has been processed in pass 𝑡 over the training data. The average parameters are de-fined as𝒘  =𝟏𝑻𝑵  𝒘𝒕,𝒊𝒊𝒕 where T is the number of epochs, and N is the number of training samples. 3 Evaluations From the four tasks we consider, parsing and lan-guage model adaptation are both examples of re-ranking.  In these tasks, we assume that we have been given a list of candidates 𝑮𝑬𝑵(𝑥) for each training or test sample  𝑥,𝑦 , generated using a baseline model.  Then, a linear model of the form in Equation (1) is used to discriminatively re-rank the candidate list using additional features which may or may not be included in the baseline model.  Since the mapping from 𝑥 to 𝑦 by the linear model may make use of arbitrary global features of the output and is performed “all at once”, we call such a linear model a global model.  In the other two tasks (i.e., Chinese word seg-mentation and POS tagging), there is no explicit enumeration of 𝑮𝑬𝑵(𝑥).  The mapping from 𝑥 to 𝑦 is determined by a sequence model which aggre-gates the decisions of local linear models via a dynamic program.  In the CMM, the local linear models are trained independently, while in the CRF model, the local models are trained jointly.  We call these two linear models local models because they dynamically combine the output of models that use only local features. While it is straightforward to apply the five es-timators to global models in the re-ranking framework, the application of some estimators to the local models is problematic. Boosting and BLasso are too computationally expensive to be applied to CRF training and we compared the other three better performing estimation methods for this model. The CMM is a probabilistic sequence model and the log-loss used by ME estimation is most natural for it; thus we limit the comparison to the two kinds of ME models for CMMs. Note that our goal is not to compare locally trained models to globally trained ones; for a study which focuses on this issue, see (Punyakanok et al. 2005). In each task we compared the performance of different estimators using task-specific measures. We used the Wilcoxon signed rank test to test the statistical significance of the difference among the competing estimators. We also report other results such as number of non-zero features after estima-tion, number of training iterations, and computation time (in minutes of elapsed time on an XEONTM MP 3.6GHz machine). 3.1 Parse re-ranking We follow the experimental paradigm of parse re-ranking outlined in Charniak and Johnson (2005), and fed the features extracted by their pro-gram to the five rerankers we developed.  Each uses a linear model trained using one of the five esti-mators. These rerankers attempt to select the best parse 𝑦 for a sentence 𝑥 from the 50-best list of possible parses 𝑮𝑬𝑵 𝑥  for the sentence. The li-near model combines the log probability calculated by the Charniak (2000) parser as a feature with 1,219,272 additional features.  We trained the fea-1 Initialize w0: set w0 = argminw0ExpLoss(w), and wd = 0 for d=1…D. 2 Take a forward step according to Eq. (8) and (9), and the updated model is denoted by w1 3 Initialize  = (ExpLoss(w0)-ExpLoss(w1))/ 4 Take a backward step if and only if it leads to a de-crease of LassoLoss according to Eq. (10) and (11), where   = 0; otherwise 5 Take a forward step according to Eq. (8) and (9); update  = min(, (ExpLoss(wt-1)-ExpLoss(wt))/ ); and return to Step 4. Figure 2: The BLasso algorithm 1 Set w0 = 1 and wd = 0 for d=1…D 2 For t = 1…T (T = the total number of iterations) 3    For each training sample (xi, yi), i = 1…N 4 𝑧𝑖=arg max𝑧∈𝐺𝐸𝑁 𝑥_𝑖 Φ 𝑥𝑖,𝑧 ⋅𝑤 Choose the best candidate zi from GEN(xi) using the current model w, 5       w = w +  η((xi, yi) – (xi, zi)), where η is the size of learning step, optimized on held-out data. Figure 3: The perceptron algorithm  829

ture weights w on Sections 2-19 of the Penn Tree-bank, adjusted the regularizer constant 𝛼 to max-imize the F-Score on Sections 20-21 of the Tree-bank, and evaluated the rerankers on Section 22.  The results are presented in Tables 12 and 2, where Baseline results were obtained using the parser by Charniak (2000).  The ME estimation with L2 regularization out-performs all of the other estimators significantly except for the AP, which performs almost as well and requires an order of magnitude less time in training.  Boosting and BLasso are feature selection methods in nature, so they achieve the sparsest models, but at the cost of slightly lower perfor-mance and much longer training time. The L1-regularized ME estimator also produces a rela-tively sparse solution whereas the Averaged Per-ceptron and the L2-regularized ME estimator assign almost all features a non-zero weight.  3.2 Language model adaptation Our experiments with LM adaptation are based on the work described in Gao et al. (2006). The va-riously trained language models were evaluated according to their impact on Japanese text input accuracy, where input phonetic symbols 𝑥 are mapped into a word string 𝑦. Performance of the application is measured in terms of character error                                                       2 The result of ME/L2 is better than that reported in Andrew and Gao (2007) due to the use of the variant of L2-regularized ME estimator, as described in Section 2.1.  CER # features time (min) #train iter Baseline 10.24%    MAP 7.98%    ME/L2 6.99% 295,337 27 665 ME/L1 7.01% 53,342 25 864 AP 7.23% 167,591 6 56 Boost 7.54% 32,994 175 71,000 BLasso 7.20% 33,126 238 250,000 Table 3. Performance summary of estimators (lower is better) on language model adaptation  ME/L2 ME/L1 AP Boost BLasso ME/L2  ~ >> >> >> ME/L1 ~  >> >> >> AP << <<  >> ~ Boost << << <<  << BLasso << << ~ >>  Table 4. Statistical significance test results. rate (CER), which is the number of characters wrongly converted from 𝑥 divided by the number of characters in the correct transcript. Again we evaluated five linear rerankers, one for each estimator. These rerankers attempt to select the best conversions 𝑦 for an input phonetic string 𝑥 from a 100-best list 𝑮𝑬𝑵(𝑥)of possible conver-sions proposed by a baseline system. The linear model combines the log probability under a trigram language model as base feature and additional 865,190 word uni/bi-gram features.  These uni/bi-gram features were already included in the trigram model which was trained on a background domain corpus (Nikkei Newspaper). But in the linear model their feature weights were trained discriminatively on an adaptation domain corpus (Encarta Encyclopedia). Thus, this forms a cross domain adaptation paradigm.  This also implies that the portion of redundant features in this task could be much larger than that in the parse re-ranking task, especially because the background domain is reasonably similar to the adaptation domain.  We divided the Encarta corpus into three sets that do not overlap.  A 72K-sentences set was used as training data, a 5K-sentence set as development data, and another 5K-sentence set as testing data. The results are presented in Tables 3 and 4, where Baseline is the word-based trigram model trained on background domain corpus, and MAP (maxi-mum a posteriori) is a traditional model adaptation method, where the parameters of the background model are adjusted so as to maximize the likelihood of the adaptation data.   F-Score # features time (min) # train iter Baseline 0.8986     ME/L2 0.9176 1,211,026 62     129  ME/L1 0.9165 19,121 37 174  AP 0.9164 939,248 2 8  Boosting 0.9131 6,714 495 92,600  BLasso 0.9133 8,085 239 56,500  Table 1: Performance summary of estimators on parsing re-ranking (ME/L2: ME with L2 regulari-zation; ME/L1:  ME with L1 regularization)  ME/L2 ME/L1 AP Boost BLasso ME/L2  >> ~ >> >> ME/L1 <<  ~ > ~ AP ~ ~  >> > Boost << < <<  ~ Blasso << ~ < ~  Table 2: Statistical significance test results (“>>” or “<<” means P-value < 0.01; > or < means 0.01 < P-value  0.05; “~” means P-value > 0.05)  830

The results are more or less similar to those in the parsing task with one visible difference: L1 regularization achieved relatively better perfor-mance in this task.  For example, while in the parsing task ME with L2 regularization significantly outperforms ME with L1 regularization, their per-formance difference is not significant in this task. While in the parsing task the performance differ-ence between BLasso and Boosting is not signifi-cant, BLasso outperforms Boosting significantly in this task.  Considering that a much higher propor-tion of the features are redundant in this task than the parsing task, the results seem to corroborate the observation that L1 regularization is robust to the presence of many redundant features. 3.3 Chinese word segmentation Our third task is Chinese word segmentation (CWS). The goal of CWS is to determine the boundaries between words in a section of Chinese text.  The model we used is the hybrid Mar-kov/semi- Markov CRF described by Andrew (2006), which was shown to have state-of-the-art accuracy. We tested models trained with the various estimation methods on the Microsoft Research Asia corpus from the Second International Chinese Word Segmentation, and we used the same train/test split used in the competition.  The model and experi-mental setup is identical with that of Andrew (2006) except for two differences.  First, we extracted features from both positive and negative training examples, while Andrew (2006) uses only features that occur in some positive training example. Second, we used the last 4K sentences of the training data to select the weight of the regularizers and to determine when to stop perceptron training. We compared three of the best performing es-timation procedures on this task: ME with L2 regu-larization, ME with L1 regularization, and the Av-eraged Perceptron.  In this case, ME refers to mi-nimizing the negative log-probability of the correct segmentation, which is globally normalized, while the perceptron is trained using at each iteration the exact maximum-scoring segmentation with the current weights. We observed the same pattern as in the other tasks: the three algorithms have nearly identical performance, while L1 uses only 6% of the features, and the Averaged Perceptron requires significantly fewer training iterations.  In this case, L1 was also several times faster than L2. The results are summarized in Table 5.3 We note that all three algorithms performed slightly better than the model used by Andrew (2006), which also used L2 regularization (96.84 F1).  We believe the difference is due to the use of features derived from negative training examples. 3.4 POS tagging Finally we studied the impact of the regularization methods on a Maximum Entropy conditional Markov Model (MEMM, McCallum et al. 2000) for POS tagging. MEMMs decompose the conditional probability of a tag sequence given a word sequence as follows: 𝑃 𝑡1…𝑡𝑛 𝑤1…𝑤𝑛 = 𝑃(𝑡𝑖|𝑡𝑖−1…𝑡𝑖−𝑘,𝑤1…𝑤𝑛)𝑛𝑖=1 where the probability distributions for each tag given its context are ME models.  Following pre-vious work (Ratnaparkhi, 1996), we assume that the tag of a word is independent of the tags of all pre-ceding words given the tags of the previous two words (i.e., 𝑘=2 in the equation above). The local models at each position include features of the current word, the previous word, the next word, and features of the previous two tags.  In addition to lexical identity of the words, we used features of word suffixes, capitalization, and number/special character signatures of the words. We used the standard splits of the Penn Treebank from the tagging literature (Toutanova et al. 2003) for training, development and test sets.  The training set comprises Sections 0-18, the development set — Sections 19-21, and the test set — Sections 22-24.  We compared training the ME models using L1 and L2 regularization.  For each of the two types of regularization we selected the best value of the regularization constant using grid search to optim-ize the accuracy on the development set.  We report final accuracy measures on the test set in Table 6.  The results on this task confirm the trends we have seen so far.  There is almost no difference in                                                       3 Only the L2 vs. AP comparison is significant at a 0.05 level according to the Wilcoxon signed rank test.  Test F1 # features # train iter ME/L2 0.9719 8,084,086 713 ME/L1 0.9713 317,146 201 AP 0.9703 1,965,719 162 Table 5. Performance summary of estimators on CWS  831

accuracy of the two kinds of regularizations, and indeed the differences were not statistically signif-icant.  Estimation with L1 regularization required considerably less time than estimation with L2, and resulted in a model which is more than ten times smaller.  4 Conclusions We compared five of the most competitive para-meter estimation methods on four NLP tasks em-ploying a variety of models, and the results were remarkably consistent across tasks.  Three of the methods — ME estimation with L2 regularization, ME estimation with L1 regularization, and the Av-eraged Perceptron — were nearly indistinguishable in terms of test set accuracy, with ME estimation with L2 regularization perhaps enjoying a slight lead.  Meanwhile, ME estimation with L1 regulari-zation achieves the same level of performance while at the same time producing sparse models, and the Averaged Perceptron provides an excellent com-promise of high performance and fast training. These results suggest that when deciding which type of parameter estimation to use on these or similar NLP tasks, one may choose any of these three popular methods and expect to achieve com-parable performance.  The choice of which to im-plement should come down to other considerations: if model sparsity is desired, choose ME estimation with L1 regularization (or feature selection methods such as BLasso); if quick implementation and training is necessary, use the Averaged Perceptron; and ME estimation with L2 regularization may be used if it is important to achieve the highest ob-tainable level of performance. References Andrew, G. 2006. A hybrid Markov/semi-Markov condi-tional random field for sequence segmentation. In EMNLP, 465-472. Andrew, G. and Gao, J. 2007. Scalable training of L1-regularized log-linear models. In ICML. Charniak, E. 2000. A maximum-entropy-inspired parser. In NAACL, 132-139. Charniak, E. and Johnson, M. 2005. Coarse-to-fine n-best parsing and MaxEnt discriminative re-ranking. In ACL. 173-180. Chen, S.F., and Rosenfeld, R. 2000. A survey of smoothing techniques for ME models. IEEE Trans. On Speech and Audio Processing, 8(2): 37-50. Collins, M. 2000. Discriminative re-ranking for natural language parsing. In ICML, 175-182. Collins, M. 2002. Discriminative training methods for hid-den Markov models: Theory and experiments with per-ceptron algorithms. In EMNLP, 1-8. Freund, Y, R. Iyer, R. E. Schapire, and Y. Singer. 1998. An efficient boosting algorithm for combining preferences. In ICML’98.  Freund, Y. and Schapire, R. E. 1999. Large margin classifica-tion using the perceptron algorithm. In Machine Learning, 37(3): 277-296. Hastie, T., R. Tibshirani and J. Friedman. 2001. The elements of statistical learning. Springer-Verlag, New York. Gao, J., Suzuki, H., and Yu, B. 2006. Approximation lasso methods for language modeling. In ACL. Goodman, J. 2004. Exponential priors for maximum entropy models. In NAACL. Johnson, M., Geman, S., Canon, S., Chi, Z., and Riezler, S. 1999. Estimators for stochastic “Unification-based” grammars. In ACL. Kazama, J. and Tsujii, J. 2003. Evaluation and extension of maximum entropy models with inequality constraints. In EMNLP. Malouf, R. 2002. A comparison of algorithms for maximum entropy parameter estimation. In HLT. McCallum A, D. Freitag and F. Pereira. 2000. Maximum entropy markov models for information extraction and segmentation. In ICML. Mitchell, T. M. 1997. Machine learning. The McGraw-Hill Companies, Inc. Ng, A. Y. 2004. Feature selection, L1 vs. L2 regularization, and rotational invariance. In ICML. Nocedal, J., and Wright, S. J. 1999. Numerical Optimization. Springer, New York. Punyakanok, V., D. Roth, W. Yih, and D. Zimak. 2005. Learning and inference over constrained output. In IJCAI. Ratnaparkhi, A. 1996. A maximum entropy part-of-speech tagger. In EMNLP. Riezler, S., and Vasserman, A. 2004. Incremental feature selection and L1 regularization for relax maximum entro-py modeling. In EMNLP.  Riezler, S., King, T. H., Kaplan, R. M., Crouch, R., Maxwell, J., and Johnson, M. 2002. Parsing the wall street journal using a lexical-functional grammar and discriminative estima-tion techniques. In ACL. 271-278.  Tibshirani, R. 1996. Regression shrinkage and selection via the lasso. J. R. Statist. Soc. B, 58(1): 267-288. Toutanova, K., Klein, D., Manning, C. D., and Singer, Y. 2003. Feature-rich Part-of-Speech tagging with a cyclic dependency network. In HLT-NAACL, 252-259. Zhao, P. and B. Yu. 2004. Boosted lasso. Tech Report, Statistics Department, U. C. Berkeley.  Accuracy (%) # features # train iter MEMM/L2 96.39 926,350 467 MEMM/L1 96.41 84,070 85 Table 6. Performance summary of estimators on POS tagging Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 832–839,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

832

GrammarApproximationbyRepresentativeSublanguage:ANewModelforLanguageLearningSmarandaMuresanInstituteforAdvancedComputerStudiesUniversityofMarylandCollegePark,MD20742,USAsmara@umiacs.umd.eduOwenRambowCenterforComputationalLearningSystemsColumbiaUniversityNewYork,NY10027,USArambow@cs.columbia.eduAbstractWeproposeanewlanguagelearningmodelthatlearnsasyntactic-semanticgrammarfromasmallnumberofnaturallanguagestringsannotatedwiththeirsemantics,alongwithbasicassumptionsaboutnaturallan-guagesyntax.Weshowthatthesearchspaceforgrammarinductionisacompletegram-marlattice,whichguaranteestheuniquenessofthelearnedgrammar.1IntroductionThereisconsiderableinterestinlearningcomputa-tionalgrammars.1Whilemuchattentionhasfocusedonlearningsyntacticgrammarseitherinasuper-visedorunsupervisedmanner,recentlythereisagrowinginteresttowardlearninggrammars/parsersthatcapturesemanticsaswell(Bosetal.,2004;ZettlemoyerandCollins,2005;GeandMooney,2005).Learningbothsyntaxandsemanticsisarguablymoredifﬁcultthanlearningsyntaxalone.Infor-malgrammarlearningtheoryithasbeenshownthatlearningfrom“goodexamples,”orrepresentativeexamples,ismorepowerfulthanlearningfromalltheexamples(Freivaldsetal.,1993).HaghighiandKlein(2006)showthatusingahandfulof“proto-1ThisresearchwassupportedbytheNationalScienceFoun-dationunderDigitalLibraryInitiativePhaseIIGrantNumberIIS-98-17434(JudithKlavansandKathleenMcKeown,PIs).WewouldliketothankJudithKlavansforhercontributionsoverthecourseofthisresearch,KathyMcKeownforherin-put,andseveralanonymousreviewersforveryusefulfeedbackonearlierdraftsofthispaper.types”signiﬁcantlyimprovesoverafullyunsuper-visedPCFGinductionmodel(theirprototypeswereformedbysequencesofPOStags;forexample,pro-totypicalNPswereDTNN,JJNN).Inthispaper,wepresentanewgrammarformal-ismandanewlearningmethodwhichtogetherad-dresstheproblemoflearningasyntactic-semanticgrammarinthepresenceofarepresentativesampleofstringsannotatedwiththeirsemantics,alongwithminimalassumptionsaboutsyntax(suchassyntac-ticcategories).Thesemanticrepresentationisanontology-basedsemanticrepresentation.Theanno-tationoftherepresentativeexamplesdoesnotin-cludetheentirederivation,unlikemostoftheex-istingsyntactictreebanks.Theaimofthepaperistopresenttheformalaspectsofourgrammarinductionmodel.InSection2,wepresentanewgrammarformal-ism,calledLexicalizedWell-FoundedGrammars,atypeofconstraint-basedgrammarsthatcombinesyntaxandsemantics.Wethenturntothetwomainresultsofthispaper.InSection3weshowthatourgrammarscanalwaysbelearnedfromasetofpositiverepresentativeexamples(withnonegativeexamples),andthesearchspaceforgrammarin-ductionisacompletegrammarlattice,whichguar-anteestheuniquenessofthelearnedgrammar.InSection4,weproposeanewcomputationallyefﬁ-cientmodelforgrammarinductionfrompairsofut-terancesandtheirsemanticrepresentations,calledGrammarApproximationbyRepresentativeSublan-guage(GARS).Section5discussesthepracticaluseofourmodelandSection6statesourconclusionsandfuturework.833

2LexicalizedWell-FoundedGrammarsLexicalizedWell-FoundedGrammars(LWFGs)areatypeofDeﬁniteClauseGrammars(PereiraandWarren,1980)where:(1)theContext-FreeGram-marbackboneisextendedbyintroducingapar-tialorderingrelationamongnonterminals(well-founded)2)eachstringisassociatedwithasyntactic-semanticrepresentationcalledsemanticmolecule;3)grammarruleshavetwotypesofcon-straints:oneforsemanticcompositionandoneforontology-basedsemanticinterpretation.Thepartialorderingamongnonterminalsallowstheorderingofthegrammarrules,andthusfacili-tatesthebottom-upinductionofthesegrammars.Thesemanticmoleculeisasyntactic-semanticrepresentationofnaturallanguagestrings 	
where(head)encodestheinformationrequiredforsemanticcomposition,and(body)istheac-tualsemanticrepresentationofthestring.Figure1showsexamplesofsemanticmoleculesforanad-jective,anounandanounphrase.Therepresen-tationsassociatedwiththelexicalitemsarecalledelementarysemanticmolecules(I),whiletherep-resentationsbuiltbythecombinationofothersarecalledderivedsemanticmolecules(II).Theheadofthesemanticmoleculeisaﬂatfeaturestructure,havingatleasttwoattributesencodingthesyntac-ticcategoryoftheassociatedstring,cat,andtheheadofthestring,head.Thesetofattributesisﬁniteandknownaprioriforeachsyntacticcate-gory.Thebodyofthesemanticmoleculeisaﬂat,ontology-basedsemanticrepresentation.Itisalog-icalform,builtasaconjunctionofatomicpredi-cates !"#%$&'(,wherevari-ablesareeitherconceptorslotidentiﬁersinanon-tology.Forexample,theadjectivemajorisrepre-sentedas*),+-/.%01!32!546$7)98:<;),+,whichsaysthatthemeaningofanadjectiveisaconcept(),+-/.#0=!>2!546$),whichisavalueofapropertyofanotherconcept()?85<;),+)intheontology.Thegrammarnonterminalsareaugmentedwithpairsofstringsandtheirsemanticmolecules.Thesepairsarecalledsyntagmas,andaredenotedby@A 7 CBA 7	
B.Therearetwotypesofcon-straintsatthegrammarrulelevel—oneforsemanticcomposition(deﬁneshowthemeaningofanaturallanguageexpressioniscomposedfromthemeaningI.ElementarySemanticMolecules(major/adj)D=EFFFFFFGHIJKLcatadjheadMImodMONPQRSIUTMI.isa=major,MON.Y=MIVW*XXXXXXY(damage/noun)D=EFFFFFFGHZJKLcatnounnrsgheadMZPQRSZTMZ.isa=damageVWXXXXXXYII.DerivedSemanticMolecule(majordamage)D=EFFFFFFGHJKLcatnnrsgheadXPQRSTMI.isa=major,X.Y=MI,X.isa=damageVW*XXXXXXYIII.ConstraintGrammarRule[]\_^a`bUcdef6gihkjUl\_^I`bcIdIef`C[]\_^N`bcNdNemfn'oqp r*sut-`*o"r*vxwyrx\zS*foqp r*sut-\H`H'I`HNf:{}|H1~{5`H1~H'mj{H'I~j`H1~H'mj{HN~H'mj`H1~-({HN~-#`HI~{jUl#`HN~{=or*vxwyr\ySCfreturnsMI=MAJOR,M=DAMAGE,=DEGREEfromontologyFigure1:Examplesoftwoelementarysemanticmolecules(I),aderivedsemanticmolecule(II)ob-tainedbycombiningthem,andaconstraintgrammarruletogetherwiththeconstraintsm%,%=(III).ofitsparts)andoneforontology-basedsemanticin-terpretation.AnexampleofaLWFGruleisgiveninFigure1(III).Thecompositionconstraintsm%appliedtotheheadsofthesemanticmolecules,formasystemofequationsthatisasimpliﬁedversionof“pathequations”(Shieberetal.,1983),becausetheheadsareﬂatfeaturestructures.Theseconstraintsarelearnedtogetherwiththegrammarrules.Theontology-basedconstraintsrepresentthevalidationontheontology,andareappliedtothebodyofthesemanticmoleculeassociatedwiththeleft-handsidenonterminal.Theyarenotlearned.Currently,%=isapredicatewhichcansucceedorfail.Whenitsucceeds,itinstantiatesthevariablesofthesemanticrepresentationwithconcepts/slotsintheontology.Forexample,giventhephrasemajordamage,%=succeedsandreturns()+=MAJOR,)=DAMAGE,;=DEGREE),whilegiventhephrasemajorbirthitfails.Weleavethediscussionoftheontologycon-straintsforafuturepaper,sinceitisnotneededforthemainresultofthispaper.WegivebelowtheformaldeﬁnitionofLexical-834

izedWell-FoundedGrammars,exceptthatwedonotdeﬁneformallytheconstraintsduetolackofspace(see(Muresan,2006)fordetails).Deﬁnition1.ALexicalizedWell-FoundedGram-mar(LWFG)isa6-tuple, 777	7
7,where:1.isaﬁnitesetofterminalsymbols.2.isaﬁnitesetofelementarysemanticmoleculescorrespondingtothesetofterminalsymbols.3.isaﬁnitesetofnonterminalsymbols.4.isapartialorderingrelationamongthenon-terminals.5.
isasetofconstraintrules.AconstraintruleiswrittenA@B+A@+B7=7A@BA@B,where@A@O7@+17UUU7@(Bsuchthat@A 7 CB7@A %7 B7	.7  + 7 9 + ,andisthesemanticcompo-sitionoperator.Forbrevity,wedenotearuleby,where !7 !#".Fortheruleswhoseleft-handsidearepreterminals,A@B$,weusethenotation@.Therearethreetypesofrules:orderednon-recursive,orderedrecursive,andnon-orderedrules.AgrammarruleA@B%+A@+B7=7A@kBA@B,isanorderedrule,ifforall,wehave&.InLWFGs,eachnonterminalsymbolisaleft-handsideinatleastoneorderednon-recursiveruleandtheemptystringcannotbederivedfromanynonterminalsymbol.6.' isthestartnonterminalsymbol,and() *+]7%'(weusethesamenotationforthereﬂexive,transitiveclosureof).Therelationisapartialorderingonlyamongnonterminals,anditshouldnotbeconfusedwithinformationorderingderivedfromtheﬂatfeaturestructures.Thisrelationmakesthesetofnontermi-nalswell-founded,whichallowstheorderingofthegrammarrules,aswellastheorderingofthesyntag-masgeneratedbyLWFGs.Deﬁnition2.GivenaLWFG, ,thegroundsyntagmaderivationrelation,,-,2isde-ﬁnedas:.0/21.43561(if@A 7 B7  2Thegroundderivation(“reduction”in(Wintner,1999))canbeviewedasthebottom-upcounterpartoftheusualderivation.7  ,i.e.,isapreterminal),and79835618;:=<+:?>?>?>::.A@B1DCE/7F@B1FC:?>?>?>:79G@B1GCHIJ@;K1LC.3561.InLWFGsallsyntagmas@A 7 B,derivedfromanonterminalhavethesamecategoryoftheirsemanticmolecules .3Thelanguageofagrammar isthesetofallsyntagmasgeneratedfromthestartsymbol,i.e.,MA BN@PO@A 7 B7  Q"7,-@SR.Thesetofallsyntagmasgeneratedbyagrammar isM1A BTN@PO@A 7 CB7  U"7VWT 7X,-@SR.GivenaLWFG wecallasetY1[ZM1A Basublanguageof .Extendingthenotation,givenaLWFG ,thesetofsyntagmasgeneratedbyaruleAB \
isM1ABN@PO@A 7 B7  ]"7AB,-@SR,whereA^B,-@denotesthegroundderiva-tion_,-@obtainedusingtheruleinthelastderivationstep(wehavebottom-upderiva-tion).WewillusetheshortnotationM1A$B,where$isagrammarrule.GivenaLWFG andasublanguageY1(notnec-essarilyof )wedenoteby`A BM1A BbaY1,thesetofsyntagmasgeneratedby reducedtothesublanguageY1.Givenagrammarrule$ c
d,wecall`A$BM1A$BeaY1thesetofsyntagmasgeneratedby$reducedtothesublanguageY1.Aswehavepreviouslymentioned,thepartialor-deringamonggrammarnonterminalsallowstheor-deringofthesyntagmasgeneratedbythegrammar,whichallowsustodeﬁnetherepresentativeexam-plesofaLWFG.RepresentativeExamples.Informally,therepre-sentativeexamplesYgfofaLWFG, ,arethesim-plestsyntagmasground-derivedbythegrammar ,i.e.,foreachgrammarrulethereexistasyntagmawhichisground-derivedfromitintheminimumnumberofsteps.Thus,thesizeoftherepresenta-tiveexamplesetisequalwiththesizeofthesetofgrammarrules,OYgfhOO
hO.Thissetofrepresentativeexamplesisusedbythegrammarlearningmodeltogeneratethecandi-datehypotheses.Forgeneralization,alargersublan-guageY1#iYjfisused,whichwecallrepresenta-tivesublanguage.3Thispropertyisusedfordeterminingthelhsnonterminalofthelearnedrule.835

PSfragreplacements 
	 
	 
	 
	   !"$#&%' (! ()* +"$#&%,
	 ()-*
	 (.
!"$#&%/ )-*
	 )
+-"0#1%,2	354635463546354678949*:;=<the,noise,loud,clear=>@?=<noise,loudnoise,thenoise=>BA=>?DC<clearloudnoise,theloudnoise=EFHGJI=>KAEFHLNMOI=>@?C<clearloudnoise=EFHLJPQI=>?C<theloudnoise=EFHRJI=>S?RulespecializationstepsTVUXWIYTVZUT\[]WIYTVZ[RulegeneralizationstepsTZUWI^TVUTQZ[WI^T[Figure2:Exampleofasimplegrammarlattice.AllgrammarsgenerateY+f,andonly_generatesY1(isacommonlexiconforallthegrammars)3AGrammarLatticeasaSearchSpaceforGrammarInductionInthissectionwepresentaclassofLexicalizedWell-FoundedGrammarsthatformacompletelat-tice.Thisgrammarlatticeisthesearchspaceforourgrammarinductionmodel,whichwepresentinSection4.AnexampleofagrammarlatticeisgiveninFigure2,whereforsimplicity,weonlyshowthecontext-freebackboneofthegrammarrules,andonlystrings,notsyntagmas.Intuitively,thegram-marsfoundlowerinthelatticearemorespecializedthantheoneshigherinthelattice.Forlearning,Yfisusedtogeneratethemostspeciﬁchypotheses(grammarrules),andthusallthegrammarsshouldbeabletogeneratethoseexamples.Thesublan-guageY1isusedduringgeneralization,thusonlythemostgeneralgrammar,_,isabletogeneratetheentiresublanguage.Inotherwords,thegener-alizationprocessisboundedbyY1,thatiswhyourmodeliscalledGrammarApproximationbyRepre-sentativeSublanguage.TherearetwopropertiesthatLWFGsshouldhaveinordertoformacompletelattice:1)theyshouldbeunambiguous,and2)theyshouldpreservethepars-ingoftherepresentativeexampleset,Yf.Wedeﬁnethesetwopropertiesinturn.Deﬁnition3.ALWFG, ,isunambiguousw.r.t.asublanguageY1ZM1A Bif(@ Y1thereisoneandonlyonerulethatderives@.Sincetheunambiguityisrelativetoasetofsyntagmas(pairsofstringsandtheirsemanticmolecules)andnottoasetofnaturallanguagestrings,therequirementiscompatiblewithmodel-ingnaturallanguage.Forexample,anambiguousstringsuchasJohnsawthemanwiththetelescopecorrespondstotwounambiguoussyntagmas.Inordertodeﬁnethesecondproperty,weneedtodeﬁnetherulespecializationstepandtherulegeneralizationstepofunambiguousLWFGs,suchthattheyareYgf-parsing-preservingandarethein-verseofeachother.ThepropertyofYf-parsing-preservingmeansthatboththeinitialandthespe-cialized/generalizedrulesground-derivethesamesyntagma,@. Yf.Deﬁnition4.Therulespecializationstep:.A@B1&`CE/a7@13bCdc0HI`7@1bC/*e4H	Ib.@1&`C/afecJHIZ`isYf-parsing-preserving,ifthereexists@. Yfand$hgVi,-@.and$j 1i%,Z-@.,where$1gQi=kFdl`ImonpFdl3bIrqSsut`,$7=pvFdlbImxwJsytb,and$jC1i%=kFdl`Izm{nyw|q@sftZ`.Wewrite$hgVi#}b~$jC1i%.Therulegeneralizationstep:.A@B1&`9C/afec9H	IZ`7@1bC/*e0HIb.A@B1&`9C/a7@13bCc0HIz`isYf-parsing-preserving,ifthereexists@. Yfand$jC1i%,Z-@.and$gVi#,-@..Wewrite$j 1i%}b$1gVi#.Since@.isarepresentativeexample,itisderivedintheminimumnumberofderivationsteps,andthustherule$7isalwaysanordered,non-recursiverule.836

Thegoaloftherulespecializationstepistoob-tainanewtargetgrammar from bymodify-ingaruleof .Similarly,thegoaloftherulegen-eralizationstepistoobtainanewtargetgrammar from bymodifyingaruleof .Theyarenottobetakenasthederivation/reductionconceptsinparsing.Thespecialization/generalizationstepsaretheinverseofeachother.Fromboththespe-cializationandthegeneralizationstepwehavethat:M1A$gVi#BiM1A$jC1i%B.InFigure2,thespecializationstep$:8}F~$8isYf-parsing-preserving,becausetherule$8ground-derivesthesyntagmaloudnoise.Ifinsteadwewouldhaveaspecializationstep$:8}U~$8($8 #4 %4),itwouldnotbeYf-parsing-preservingsincethesyntagmaloudnoisecouldnolongerbeground-derivedfromtherule$8(whichrequirestwoadjectives).Deﬁnition5.Agrammar isone-stepspecial-izedfromagrammar , }F~ ,ifVq$=7$6+ 
andVq$7$6+ \
Z,s.t.$}F~$,and($7 
iff 
Z.Agrammar isspecializedfromagrammar , ,~ ,ifitisobtainedfrom in-specializationsteps: }F~}G~ ,whereisﬁ-nite.Weextendthenotationsothatwehave ,~ .Similarly,wedeﬁnetheconceptofagrammar generalizedfromagrammar , , usingtherulegeneralizationstep.InFigure2,thegrammarisone-stepspecial-izedfromthegrammar +,i.e., +}F~,sincepreservetheparsingoftherepresentativeexam-plesYjf.Agrammarwhichcontainstherule$8 %4 %4insteadof$8isnotspecializedfromthegrammar +sinceitdoesnotpreservetheparsingoftherepresentativeexampleset,Yf.Suchgrammarswillnotbeinthelattice.Inordertodeﬁnethegrammarlatticeweneedtointroduceonemoreconcept:anormalizedgrammarw.r.t.asublanguage.Deﬁnition6.ALWFG iscallednormalizedw.r.t.asublanguageY1(notnecessarilyofG),ifnoneofthegrammarrules$fj hi#of canbefurthergener-alizedtoarule$hgVi#bytherulegeneralizationstepsuchthat`A$jC&i#B	`A$1gVi#B.InFigure2,grammar_isnormalizedw.r.t.Y1,while, +and 8arenot.Wenowdeﬁneagrammarlattice
whichwillbethesearchspaceforourgrammarlearningmodel.Weﬁrstdeﬁnethesetoflatticeelements.Let_beaLWFG,normalizedandunambiguousw.r.t.asublanguageY1ZM1A_BwhichincludestherepresentativeexamplesetY2fofthegrammar_(Y1iYjf).Let_N O_,~ Rbethesetofgrammarsspecializedfrom_.Wecall_thetopelementof,andthebottomelementof,if(  7Q_,~  ,~.Thebottomelement,,isthegrammarspecializedfrom_,suchthattheright-handsideofallgrammarrulescontainsonlypreterminals.Wehave`A_BY1and`ABiYf.Thegrammarsinhavethefollowingtwoprop-erties(Muresan,2006):Fortwogrammars and ,wehavethat isspecializedfrom ifandonlyif isgener-alizedfrom ,withM1A BiM1A B.AllgrammarsinpreservetheparsingoftherepresentativeexamplesetY2f.Notethatwehavethatfor 7  ,if ,~ then`A Bi`A B.Thesystem
7,~isacompletegram-marlattice(see(Muresan,2006)forthefullformalproof).InFigure2thegrammars +, 8,_,pre-servetheparsingoftherepresentativeexamplesYf.Wehavethat_}F~ +,_}F~ 8, 8}F~, +}F~and_,~.Duetospacelimitationwedonotdeﬁneheretheleastupperbound(a),andthegreatestlowerbound(),operators,butinthisexample_= + 8,= + 8.Inodertogivealearnabilitytheoremweneedtoshowthatand_elementsofthelatticecanbebuilt.First,anassumptioninourlearningmodelisthattherulescorrespondingtothegrammarpreter-minalsaregiven.Thus,foragivensetofrepresenta-tiveexamples,Yf,wecanbuildthegrammarus-ingabottom-uprobustparser,whichreturnspartialanalyses(chunks)ifitcannotreturnafullparse.Inordertosoundlybuildthe_elementofthegrammarlatticefromthegrammarthroughgeneralization,wemustgivethedeﬁnitionofagrammar confor-malw.r.t.Y1.837

Deﬁnition7.ALWFG isconformalw.r.t.asub-languageY1ZM1A Biff isnormalizedandun-ambiguousw.r.t.Y1andtherulespecializationstepguaranteesthat`A$gVi#B `A$jC&i#Bforallgrammarsspecializedfrom .Theonlyrulegeneralizationstepsallowedinthegrammarinductionprocessarethosewhichguaran-teethesamerelation`A$fjC&i#B`A$1gVi#B,whichen-suresthatallthegeneralizedgrammarsbelongtothegrammarlattice.InFigure2,_isconformaltothegivensub-languageY1.IfthesublanguagewereY,1YfNclearloudnoiseRthen_wouldnotbecon-formaltoY,1since`A_B`A +BY,1andthusthespecializationstepwouldnotsatisfytherelation`A-B `AB.Dur-inglearning,thegeneralizationstepcannotgeneral-izefromgrammar +to_.Theorem1(LearnabilityTheorem).IfY2fisthesetofrepresentativeexamplesassociatedwithaLWFG conformalw.r.t.asublanguageY1iYjf,then canalwaysbelearnedfromY2fandY1asthegrammarlatticetopelement(_ ).Theproofisgivenin(Muresan,2006).IfthehypothesisofTheorem1holds,thenanygrammarinductionalgorithmthatusesthecompletelatticesearchspacecanconvergetothelatticetopel-ement,usingdifferentsearchstrategies.Inthenextsectionwepresentournewmodelofgrammarlearn-ingwhichreliesonthepropertyofthesearchspaceasgrammarlattice.4GrammarInductionModelBasedonthetheoreticalfoundationofthehypoth-esissearchspaceforLWFGlearninggivenintheprevioussection,wedeﬁneourgrammarinductionmodel.First,wepresenttheLWFGinductionasanInductiveLogicProgrammingproblem.Second,wepresentournewrelationallearningmodelforLWFGinduction,calledGrammarApproximationbyRep-resentativeSublanguage(GARS).4.1GrammarInductionProbleminILP-settingInductiveLogicProgramming(ILP)isaclassofre-lationallearningmethodsconcernedwithinducingﬁrst-orderHornclausesfromexamplesandback-groundknowledge.KietzandDˇzeroski(1994)haveformallydeﬁnedtheILP-learningproblemasthetu-ple~7Me7MY97M
	,where~istheprovabilityre-lation(alsocalledthegeneralizationmodel),MPisthelanguageofthebackgroundknowledge,MYisthelanguageofthe(positiveandnegative)exam-ples,andM
	isthehypothesislanguage.Thegen-eralILP-learningproblemisundecidable.PossiblechoicestorestricttheILP-problemare:theprovabil-ityrelation,~,thebackgroundknowledgeandthehypothesislanguage.ResearchinILPhaspresentedpositiveresultsonlyforverylimitedsubclassesofﬁrst-orderlogic(KietzandDˇzeroski,1994;Cohen,1995),whicharenotappropriatetomodelnaturallanguagegrammars.Ourgrammarinductionproblemcanbeformu-latedasanILP-learningproblem~7MP7MY7M	asfollows:Theprovabilityrelation,~,isgivenbyrobustparsing,andwedenoteitby~}.Weusethe“parsingasdeduction”technique(Shieberetal.,1995).Forallsyntagmaswecansayinpolynomialtimewhethertheybelongornottothegrammarlanguage.Thus,usingthe~}asgeneralizationmodel,ourgrammarinductionproblemisdecidable.Thelanguageofbackgroundknowledge,MP,isthesetofLWFGrulesthatarealreadylearnedtogetherwithelementarysyntagmas(i.e.,correspondingtothelexicon),whicharegroundatoms(thevariablesaremadecon-stants).Thelanguageofexamples,MYaresyntagmasoftherepresentativesublanguage,whicharegroundatoms.Weonlyhavepositiveexamples.Thehypothesislanguage,M
	,isaLWFGlat-ticewhosetopelementisaconformalgram-mar,andwhichpreservetheparsingofrepre-sentativeexamples.4.2GrammarApproximationbyRepresentativeSublanguageModelWehaveformulatedthegrammarinductionproblemintheILP-setting.Thetheoreticallearningmodel,838

calledGrammarApproximationbyRepresentativeSublanguage(GARS),canbeformulatedasfollows:Given:arepresentativeexamplesetY2f,lexicallycon-sistent(i.e.,itallowstheconstructionofthegrammarlatticeelement)aﬁnitesublanguageY1,conformalandthusunambiguous,whichincludestherepresenta-tiveexampleset,Y1iYjf.Wecalledthissublanguage,therepresentativesublanguageLearnagrammar ,usingtheaboveILP-learningsetting,suchthat isuniqueandY1ZM1A B.Thehypothesisspaceisacompletegrammarlat-tice,andthustheuniquenesspropertyofthelearnedgrammarisguaranteedbythelearnabilitytheorem(i.e.,thelearnedgrammaristhelatticetopele-ment).ThislearnabilityresultextendssigniﬁcantlytheclassofproblemslearnablebyILPmethods.TheGARSmodelusestwopolynomialalgo-rithmsforLWFGlearning.Intheﬁrstalgorithm,thelearnerispresentedwithanorderedsetofrep-resentativeexamples(syntagmas),i.e.,theexamplesareorderedfromthesimplesttothemostcomplex.ThereadershouldrememberthatforaLWFG ,thereexistsapartialorderingamongthegrammarnonterminals,whichallowsatotalorderingoftherepresentativeexamplesofthegrammar .Thus,inthisalgorithm,thelearnerhasaccesstotheorderedrepresentativesyntagmaswhenlearningthegram-mar.However,inpracticeitmightbedifﬁculttoprovidethelearnerwiththe“true”orderofexam-ples,especiallywhenmodelingcomplexlanguagephenomena.Thesecondalgorithmisaniterativeal-gorithmthatlearnsstartingfromarandomorderoftherepresentativeexampleset.Duetothepropertyofthesearchspace,bothalgorithmsconvergetothesametargetgrammar.UsingILPandtheoryrevisionterminology(Greiner,1999),wecanestablishthefollowinganal-ogy:syntagmas(examples)are“labeledqueries”,theLWFGlatticeisthe“spaceoftheories”,andaLWFGinthelatticeis“atheory.”Theﬁrstalgorithmlearnsfroman“emptytheory”,whilethesecondal-gorithmisaninstanceof“theoryrevision”,sincethegrammar(“theory”)learnedduringtheﬁrstiteration,isthenrevised,bydeletingandaddingrules.Bothofthesealgorithmsarecoversetalgorithms.Intheﬁrststepthemostspeciﬁcgrammarruleisgeneratedfromthecurrentrepresentativeexam-ple.Thecategorynameannotatedintherepresen-tativeexamplegivesthenameofthelhsnontermi-nal(predicateinventioninILPterminology),whiletherobustparserreturnstheminimumnumberofchunksthatcovertherepresentativeexample.Inthesecondstepthismostspeciﬁcruleisgeneralizedus-ingasperformancecriterionthenumberoftheex-amplesinY1thatcanbeparsedusingthecandidategrammarrule(hypothesis)togetherwiththeprevi-ouslearnedrules.Forthefulldetailsforthesetwoalgorithms,andtheproofoftheirpolynomialefﬁ-ciency,wereferthereaderto(Muresan,2006).5DiscussionApracticaladvantageofourGARSmodelisthatinsteadofwritingsyntactic-semanticgrammarsbyhand(bothrulesandconstraints),weconstructjustasmallannotatedtreebank-utterancesandtheirse-manticmolecules.Ifthegrammarneedstobere-ﬁned,orenhanced,weonlyreﬁne,orenhancetherepresentativeexamples/sublanguage,andnotthegrammarrulesandconstraints,whichwouldbeamoredifﬁculttask.WehavebuiltaframeworktotestwhetherourGARSmodelcanlearndiverseandcomplexlin-guisticphenomena.Wehaveprimarilyanalyzedasetofdeﬁnitional-typesentencesinthemedicaldo-main.Thephenomenacoveredbyourlearnedgram-marincludescomplexnounphrases(includingnouncompounds,nominalization),prepositionalphrases,relativeclausesandreducedrelativeclauses,ﬁniteandnon-ﬁniteverbalconstructions(including,tense,aspect,negation,andsubject-verbagreement),cop-ulatobe,andraisingandcontrolconstructions.Wealsolearnedrulesforwh-questions(includinglong-distancedependencies).InFigure3weshowtheontology-levelrepresentationofadeﬁnition-typesentenceobtainedusingourlearnedgrammar.Itincludesthetreatmentofreducedrelativeclauses,raisingconstruction(tendstopersist,wherevirusisnottheargumentoftendsbuttheargumentofpersist),andnouncompounds.Thelearnedgram-martogetherwithasemanticinterpretertargetedtoterminologicalknowledgehasbeenusedinanacquisition-queryexperiment,wheretheanswersareattheconceptlevel(thequeryingisagraph839

HepatitisBisanacuteviralhepatitiscausedbyavirusthattendstopersistinthebloodserum.#hepatitis#acute#viral#cause#blood#virussubkind_ofthofdurationagproplocationth#tend#persist#serum#’HepatitisB’Figure3:Adeﬁnition-typesentenceanditsontology-basedrepresentationobtainedusingourlearnedLWFGmatchingproblemwherethe“wh-word”matchestheanswerconcept).Adetaileddiscussionofthelinguisticphenomenacoveredbyourlearnedgram-marusingtheGARSmodel,aswellastheuseofthisgrammarforterminologicalknowledgeacquisition,isgivenin(Muresan,2006).Tolearnthegrammarusedintheseexperimentsweannotated151representativeexamplesand448examplesusedasarepresentativesublanguageforgeneralization.Annotatingtheseexamplesrequiresknowledgeaboutcategoriesandtheirattributes.Weused31categories(nonterminals)and37attributes(e.g.,category,head,number,person).Inthisexperiment,wechosetherepresentativeexamplesguidedbythetypeofphenomenawewantedtomod-eledandwhichoccurredinourcorpus.Wealsoused13lexicalcategories(i.e.,partsofspeech).Thelearnedgrammarcontains151rulesand151con-straints.6ConclusionWehavepresentedLexicalizedWell-FoundedGrammars,atypeofconstraint-basedgrammarsfornaturallanguagespeciﬁcallydesignedtoen-ablelearningfromrepresentativeexamplesanno-tatedwithsemantics.Wehavepresentedanewgrammarlearningmodelandshowedthatthesearchspaceisacompletegrammarlatticethatguaranteestheuniquenessofthelearnedgrammar.Startingfromthesefundamentaltheoreticalresults,thereareseveraldirectionsintowhichtotakethisresearch.Aﬁrstobviousextensionistohaveprobabilistic-LWFGs.Forexample,theontologyconstraintsmightnotbe“hard”constraints,but“soft”ones(be-causelanguageexpressionsaremoreorlesslikelytobeusedinacertaincontext).Investigatingwheretoaddprobabilities(ontology,grammarrules,orboth)ispartofourplannedfuturework.Anotherfutureextensionofthisworkistoinvestigatehowtoauto-maticallyselecttherepresentativeexamplesfromanexistingtreebank.ReferencesJohanBos,StephenClark,MarkSteedman,JamesR.Curran,andJuliaHockenmaier.2004.Wide-coveragesemanticrepresentationsfromaCCGparser.InPro-ceedingsofCOLING-04.WilliamCohen.1995.Pac-learningrecursivelogicpro-grams:Negativeresults.JournalofArtiﬁcialIntelli-genceResearch,2:541–573.RusinsFreivalds,EﬁmB.Kinber,andRolfWieha-gen.1993.Onthepowerofinductiveinferencefromgoodexamples.TheoreticalComputerScience,110(1):131–144.R.GeandR.J.Mooney.2005.Astatisticalsemanticparserthatintegratessyntaxandsemantics.InPro-ceedingsofCoNLL-2005.RussellGreiner.1999.Thecomplexityoftheoryrevi-sion.ArtiﬁcialIntelligenceJournal,107(2):175–217.AriaHaghighiandDanKlein.2006.Prototype-drivengrammarinduction.InProceedingsofACL’06.J¨org-UweKietzandSaˇsoDˇzeroski.1994.Inductivelogicprogrammingandlearnability.ACMSIGARTBulletin.,5(1):22–32.SmarandaMuresan.2006.LearningConstraint-basedGrammarsfromRepresentativeExamples:TheoryandApplications.Ph.D.thesis,ColumbiaUniversity.http://www1.cs.columbia.edu/ smara/muresanthesis.pdf.FernandoC.PereiraandDavidH.DWarren.1980.Deﬁ-niteClauseGrammarsforlanguageanalysis.ArtiﬁcialIntelligence,13:231–278.StuartShieber,HansUszkoreit,FernandoPereira,JaneRobinson,andMabryTyson.1983.TheformalismandimplementationofPATR-II.InBarbaraJ.GroszandMarkStickel,editors,ResearchonInteractiveAc-quisitionandUseofKnowledge,pages39–79.SRIIn-ternational,MenloPark,CA,November.StuartShieber,YvesSchabes,andFernandoPereira.1995.Principlesandimplementationofdeductiveparsing.JournalofLogicProgramming,24(1-2):3–36.ShulyWintner.1999.Compositionalsemanticsforlin-guisticformalisms.InProceedingsoftheACL’99.LukeS.ZettlemoyerandMichaelCollins.2005.Learn-ingtomapsentencestologicalform:Structuredclas-siﬁcationwithprobabilisticcategorialgrammars.InProceedingsofUAI-05.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 840–847,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

840

ChineseSegmentationwithaWord-BasedPerceptronAlgorithmYueZhangandStephenClarkOxfordUniversityComputingLaboratoryWolfsonBuilding,ParksRoadOxfordOX13QD,UK{yue.zhang,stephen.clark}@comlab.ox.ac.ukAbstractStandardapproachestoChinesewordseg-mentationtreattheproblemasataggingtask,assigninglabelstothecharactersinthesequenceindicatingwhetherthechar-actermarksawordboundary.Discrimina-tivelytrainedmodelsbasedonlocalchar-acterfeaturesareusedtomakethetaggingdecisions,withViterbidecodingﬁndingthehighestscoringsegmentation.Inthispaperweproposeanalternative,word-basedseg-mentor,whichusesfeaturesbasedoncom-pletewordsandwordsequences.Thegener-alizedperceptronalgorithmisusedfordis-criminativetraining,andweuseabeam-searchdecoder.ClosedtestsontheﬁrstandsecondSIGHANbakeoffsshowthatoursys-temiscompetitivewiththebestinthelitera-ture,achievingthehighestreportedF-scoresforanumberofcorpora.1IntroductionWordsarethebasicunitstoprocessformostNLPtasks.TheproblemofChinesewordsegmentation(CWS)istoﬁndthesebasicunitsforagivensen-tence,whichiswrittenasacontinuoussequenceofcharacters.ItistheinitialstepformostChinesepro-cessingapplications.Chinesecharactersequencesareambiguous,of-tenrequiringknowledgefromavarietyofsourcesfordisambiguation.Out-of-vocabulary(OOV)wordsareamajorsourceofambiguity.Forexample,adifﬁcultcaseoccurswhenanOOVwordconsistsofcharacterswhichhavethemselvesbeenseenaswords;hereanautomaticsegmentormaysplittheOOVwordintoindividualsingle-characterwords.TypicalexamplesofunseenwordsincludeChinesenames,translatedforeignnamesandidioms.Thesegmentationofknownwordscanalsobeambiguous.Forexample,“Ìb”shouldbe“Ì(here)b(ﬂour)”inthesentence“ÌbŒˆ5”(ﬂourandriceareexpensivehere)or“(here)Ìb(inside)”inthesentence“Ìbˆ”(it’scoldinsidehere).Theambiguitycanberesolvedwithinformationabouttheneighboringwords.Incomparison,forthesentences“=(cid:26)ˆ(cid:16)Ÿ”,possiblesegmentationsinclude“=(thediscus-sion)(cid:26)(will)ˆ(very)(cid:16)Ÿ(besuccessful)”and“=(cid:26)(thediscussionmeeting)ˆ(very)(cid:16)Ÿ(besuccessful)”.Theambiguitycanonlyberesolvedwithcontextualinformationoutsidethesentence.Humanreadersoftenusesemantics,contextualin-formationaboutthedocumentandworldknowledgetoresolvesegmentationambiguities.ThereisnoﬁxedstandardforChinesewordseg-mentation.Experimentshaveshownthatthereisonlyabout75%agreementamongnativespeakersregardingthecorrectwordsegmentation(Sproatetal.,1996).Also,speciﬁcNLPtasksmayrequiredif-ferentsegmentationcriteria.Forexample,“(cid:23)¬ö”couldbetreatedasasingleword(BankofBei-jing)formachinetranslation,whileitismorenatu-rallysegmentedinto“(cid:23)¬(Beijing)ö(bank)”fortaskssuchastext-to-speechsynthesis.There-fore,supervisedlearningwithspeciﬁcallydeﬁnedtrainingdatahasbecomethedominantapproach.FollowingXue(2003),thestandardapproachto841

supervisedlearningforCWSistotreatitasataggingtask.Tagsareassignedtoeachcharacterinthesen-tence,indicatingwhetherthecharacterisasingle-characterwordorthestart,middleorendofamulti-characterword.Thefeaturesareusuallyconﬁnedtoaﬁve-characterwindowwiththecurrentcharacterinthemiddle.Inthisway,dynamicprogrammingalgorithmssuchastheViterbialgorithmcanbeusedfordecoding.Severaldiscriminativelytrainedmodelshavere-centlybeenappliedtotheCWSproblem.Exam-plesincludeXue(2003),Pengetal.(2004)andShiandWang(2007);theseusemaximumentropy(ME)andconditionalrandomﬁeld(CRF)models(Ratna-parkhi,1998;Laffertyetal.,2001).Anadvantageofthesemodelsistheirﬂexibilityinallowingknowl-edgefromvarioussourcestobeencodedasfeatures.Contextualinformationplaysanimportantroleinwordsegmentationdecisions;especiallyusefulisin-formationaboutsurroundingwords.Considerthesentence“	(cid:22)(cid:26)”,whichcanbefrom“v	(amongwhich)(cid:22)(foreign)(cid:26)(companies)”,or“	(inChina)(cid:22)(foreigncompanies)(cid:26)¡(business)”.Notethattheﬁve-characterwindowsurrounding“(cid:22)”isthesameinbothcases,makingthetaggingdecisionforthatcharacterdifﬁcultgiventhelocalwindow.However,thecorrectdecisioncanbemadebycomparisonofthetwothree-wordwin-dowscontainingthischaracter.Inordertoexplorethepotentialofword-basedmodels,weadapttheperceptrondiscriminativelearningalgorithmtotheCWSproblem.Collins(2002)proposedtheperceptronasanalternativetotheCRFmethodforHMM-styletaggers.However,ourmodeldoesnotmapthesegmentationproblemtoatagsequencelearningproblem,butdeﬁnesfea-turesonsegmentedsentencesdirectly.Henceweuseabeam-searchdecoderduringtrainingandtest-ing;ourideaissimilartothatofCollinsandRoark(2004)whousedabeam-searchdecoderaspartofaperceptronparsingmodel.Ourworkcanalsobeseenaspartoftherecentmovetowardssearch-basedlearningmethodswhichdonotrelyondynamicpro-grammingandarethusabletoexploitlargerpartsofthecontextformakingdecisions(DaumeIII,2006).Westudyseveralfactorsthatinﬂuencetheper-formanceoftheperceptronwordsegmentor,includ-ingtheaveragedperceptronmethod,thesizeofthebeamandtheimportanceofword-basedfeatures.Wecomparetheaccuracyofourﬁnalsystemtothestate-of-the-artCWSsystemsintheliteratureusingtheﬁrstandsecondSIGHANbakeoffdata.Oursys-temiscompetitivewiththebestsystems,obtainingthehighestreportedF-scoresonanumberofthebakeoffcorpora.Theseresultsdemonstratetheim-portanceofword-basedfeaturesforCWS.Further-more,ourapproachprovidesanexampleofthepo-tentialofsearch-baseddiscriminativetrainingmeth-odsforNLPtasks.2ThePerceptronTrainingAlgorithmWeformulatetheCWSproblemasﬁndingamappingfromaninputsentencex∈Xtoanoutputsentencey∈Y,whereXisthesetofpossiblerawsentencesandYisthesetofpossiblesegmentedsentences.Givenaninputsentencex,thecorrectoutputseg-mentationF(x)satisﬁes:F(x)=argmaxy∈GEN(x)Score(y)whereGEN(x)denotesthesetofpossiblesegmen-tationsforaninputsentencex,consistentwithnota-tionfromCollins(2002).Thescoreforasegmentedsentenceiscomputedbyﬁrstmappingitintoasetoffeatures.Afeatureisanindicatoroftheoccurrenceofacertainpatterninasegmentedsentence.Forexample,itcanbetheoccurrenceof“Ìb”asasingleword,ortheoccur-renceof“Ì”separatedfrom“b”intwoadjacentwords.Bydeﬁningfeatures,asegmentedsentenceismappedintoaglobalfeaturevector,inwhicheachdimensionrepresentsthecountofaparticularfea-tureinthesentence.Theterm“global”featurevec-torisusedbyCollins(2002)todistinguishbetweenfeaturecountvectorsforwholesequencesandthe“local”featurevectorsinMEtaggingmodels,whichareBooleanvaluedvectorscontainingtheindicatorfeaturesforoneelementinthesequence.DenotetheglobalfeaturevectorforsegmentedsentenceywithΦ(y)∈Rd,wheredisthetotalnumberoffeaturesinthemodel;thenScore(y)iscomputedbythedotproductofvectorΦ(y)andaparametervectorα∈Rd,whereαiistheweightfortheithfeature:Score(y)=Φ(y)·α842

Inputs:trainingexamples(xi,yi)Initialization:setα=0Algorithm:fort=1..T,i=1..Ncalculatezi=argmaxy∈GEN(xi)Φ(y)·αifzi6=yiα=α+Φ(yi)−Φ(zi)Outputs:αFigure1:theperceptronlearningalgorithm,adaptedfromCollins(2002)Theperceptrontrainingalgorithmisusedtodeter-minetheweightvaluesα.Thetrainingalgorithminitializestheparametervectorasallzeros,andupdatesthevectorbydecod-ingthetrainingexamples.Eachtrainingsentenceisturnedintotherawinputform,andthendecodedwiththecurrentparametervector.Theoutputseg-mentedsentenceiscomparedwiththeoriginaltrain-ingexample.Iftheoutputisincorrect,theparametervectorisupdatedbyaddingtheglobalfeaturevectorofthetrainingexampleandsubtractingtheglobalfeaturevectorofthedecoderoutput.Thealgorithmcanperformmultiplepassesoverthesametrainingsentences.Figure1givesthealgorithm,whereNisthenumberoftrainingsentencesandTisthenum-berofpassesoverthedata.NotethatthealgorithmfromCollins(2002)wasdesignedfordiscriminativelytraininganHMM-styletagger.Featuresareextractedfromaninputse-quencexanditscorrespondingtagsequencey:Score(x,y)=Φ(x,y)·αOuralgorithmisnotbasedonanHMM.Foragiveninputsequencex,eventhelengthofdifferentcandi-datesy(thenumberofwords)isnotﬁxed.Becausetheoutputsequencey(thesegmentedsentence)con-tainsalltheinformationfromtheinputsequencex(therawsentence),theglobalfeaturevectorΦ(x,y)isreplacedwithΦ(y),whichisextractedfromthecandidatesegmentedsentencesdirectly.Despitetheabovedifferences,sincethetheoremsofconvergenceandtheirproof(Collins,2002)areonlydependentonthefeaturevectors,andnotonthesourceofthefeaturedeﬁnitions,theperceptronalgorithmisapplicabletothetrainingofourCWSmodel.2.1TheaveragedperceptronTheaveragedperceptronalgorithm(Collins,2002)wasproposedasawayofreducingoverﬁttingonthetrainingdata.Itwasmotivatedbythevoted-perceptronalgorithm(FreundandSchapire,1999)andhasbeenshowntogiveimprovedaccuracyoverthenon-averagedperceptrononanumberoftasks.LetNbethenumberoftrainingsentences,Tthenumberoftrainingiterations,andαn,ttheparame-tervectorimmediatelyafterthenthsentenceinthetthiteration.Theaveragedparametervectorγ∈Rdisdeﬁnedas:γ=1NTXn=1..N,t=1..Tαn,tTocomputetheaveragedparametersγ,thetrain-ingalgorithminFigure1canbemodiﬁedbykeep-ingatotalparametervectorσn,t=Pαn,t,whichisupdatedusingαaftereachtrainingexample.Aftertheﬁnaliteration,γiscomputedasσn,t/NT.Intheaveragedperceptronalgorithm,γisusedinsteadofαastheﬁnalparametervector.Withalargenumberoffeatures,calculatingthetotalparametervectorσn,taftereachtrainingexam-pleisexpensive.Sincethenumberofchangeddi-mensionsintheparametervectorαaftereachtrain-ingexampleisasmallproportionofthetotalvec-tor,weusealazyupdateoptimizationforthetrain-ingprocess.1Deﬁneanupdatevectorτtorecordthenumberofthetrainingsentencenanditerationtwheneachdimensionoftheaveragedparametervectorwaslastupdated.Thenaftereachtrainingsentenceisprocessed,onlyupdatethedimensionsofthetotalparametervectorcorrespondingtothefeaturesinthesentence.(Exceptforthelastexam-pleinthelastiteration,wheneachdimensionofτisupdated,nomatterwhetherthedecoderoutputiscorrectornot).Denotethesthdimensionineachvectorbeforeprocessingthenthexampleinthetthiterationasαn−1,ts,σn−1,tsandτn−1,ts=(nτ,s,tτ,s).Supposethatthedecoderoutputzn,tisdifferentfromthetrainingexampleyn.Nowαn,ts,σn,tsandτn,tscan1DaumeIII(2006)describesasimilaralgorithm.843

beupdatedinthefollowingway:σn,ts=σn−1,ts+αn−1,ts×(tN+n−tτ,sN−nτ,s)αn,ts=αn−1,ts+Φ(yn)−Φ(zn,t)σn,ts=σn,ts+Φ(yn)−Φ(zn,t)τn,ts=(n,t)Wefoundthatthislazyupdatemethodwassignif-icantlyfasterthanthenaivemethod.3TheBeam-SearchDecoderThedecoderreadscharactersfromtheinputsen-tenceoneatatime,andgeneratescandidateseg-mentationsincrementally.Ateachstage,thenextin-comingcharacteriscombinedwithanexistingcan-didateintwodifferentwaystogeneratenewcandi-dates:itiseitherappendedtothelastwordinthecandidate,ortakenasthestartofanewword.Thismethodguaranteesexhaustivegenerationofpossiblesegmentationsforanyinputsentence.Twoagendasareused:thesourceagendaandthetargetagenda.Initiallythesourceagendacontainsanemptysentenceandthetargetagendaisempty.Ateachprocessingstage,thedecoderreadsinacharacterfromtheinputsentence,combinesitwitheachcandidateinthesourceagendaandputsthegeneratedcandidatesontothetargetagenda.Aftereachcharacterisprocessed,theitemsinthetargetagendaarecopiedtothesourceagenda,andthenthetargetagendaiscleaned,sothatthenewlygeneratedcandidatescanbecombinedwiththenextincom-ingcharactertogeneratenewcandidates.Afterthelastcharacterisprocessed,thedecoderreturnsthecandidatewiththebestscoreinthesourceagenda.Figure2givesthedecodingalgorithm.Forasentencewithlengthl,thereare2l−1differ-entpossiblesegmentations.Toguaranteereasonablerunningspeed,thesizeofthetargetagendaislim-ited,keepingonlytheBbestcandidates.4FeaturetemplatesThefeaturetemplatesareshowninTable1.Features1and2containonlywordinformation,3to5con-taincharacterandlengthinformation,6and7con-tainonlycharacterinformation,8to12containwordandcharacterinformation,while13and14containInput:rawsentencesent–alistofcharactersInitialization:setagendassrc=[[]],tgt=[]Variables:candidatesentenceitem–alistofwordsAlgorithm:forindex=0..sent.length−1:varchar=sent[index]foreachiteminsrc://appendasanewwordtothecandidatevaritem1=itemitem1.append(char.toWord())tgt.insert(item1)//appendthecharactertothelastwordifitem.length>1:varitem2=itemitem2[item2.length−1].append(char)tgt.insert(item2)src=tgttgt=[]Outputs:src.bestitemFigure2:Thedecodingalgorithmwordandlengthinformation.Anysegmentedsen-tenceismappedtoaglobalfeaturevectoraccordingtothesetemplates.Thereare356,337featureswithnon-zerovaluesafter6trainingiterationsusingthedevelopmentdata.Forthisparticularfeatureset,thelongestrangefeaturesarewordbigrams.Therefore,amongpartialcandidatesendingwiththesamebigram,thebestonewillalsobeinthebestﬁnalcandidate.Thedecodercanbeoptimizedaccordingly:whenanin-comingcharacteriscombinedwithcandidateitemsasanewword,onlythebestcandidateiskeptamongthosehavingthesamelastword.5ComparisonwithPreviousWorkAmongthecharacter-taggingCWSmodels,Lietal.(2005)usesanunevenmarginalterationofthetradi-tionalperceptronclassiﬁer(Lietal.,2002).Eachcharacterisclassiﬁedindependently,usinginfor-mationintheneighboringﬁve-characterwindow.Liang(2005)usesthediscriminativeperceptronal-gorithm(Collins,2002)toscorewholecharactertagsequences,ﬁndingthebestcandidatebytheglobalscore.ItcanbeseenasanalternativetotheMEandCRFmodels(Xue,2003;Pengetal.,2004),which844

1wordw2wordbigramw1w23single-characterwordw4awordstartingwithcharactercandhavinglengthl5awordendingwithcharactercandhavinglengthl6space-separatedcharactersc1andc27characterbigramc1c2inanyword8theﬁrstandlastcharactersc1andc2ofanyword9wordwimmediatelybeforecharacterc10charactercimmediatelybeforewordw11thestartingcharactersc1andc2oftwocon-secutivewords12theendingcharactersc1andc2oftwocon-secutivewords13awordoflengthlandthepreviouswordw14awordoflengthlandthenextwordwTable1:featuretemplatesdonotinvolvewordinformation.Wangetal.(2006)incorporatesanN-gramlanguagemodelinMEtag-ging,makinguseofwordinformationtoimprovethecharactertaggingmodel.Thekeydifferencebe-tweenourmodelandtheabovemodelsistheword-basednatureofoursystem.Oneexistingmethodthatisbasedonsub-wordin-formation,Zhangetal.(2006),combinesaCRFandarule-basedmodel.Unlikethecharacter-taggingmodels,theCRFsubmodelassignstagstosub-words,whichincludesingle-characterwordsandthemostfrequentmultiple-characterwordsfromthetrainingcorpus.Thusitcanbeseenasasteptowardsaword-basedmodel.However,sub-wordsdonotnecessarilycontainfullwordinformation.More-over,sub-wordextractionisperformedseparatelyfromfeatureextraction.Anotherdifferencefromourmodelistherule-basedsubmodel,whichusesadictionary-basedforwardmaximummatchmethoddescribedbySproatetal.(1996).6ExperimentsTwosetsofexperimentswereconducted.Theﬁrst,usedfordevelopment,wasbasedonthepartofChi-neseTreebank4thatisnotinChineseTreebank3(sinceCTB3wasusedaspartoftheﬁrstbake-off).Thiscorpuscontains240Kcharacters(150Kwordsand4798sentences).80%ofthesentences(3813)wererandomlychosenfortrainingandtherest(985sentences)wereusedasdevelopmenttest-ingdata.Theaccuraciesandlearningcurvesforthenon-averagedandaveragedperceptronwerecom-pared.Theinﬂuenceofparticularfeaturesandtheagendasizewerealsostudied.ThesecondsetofexperimentsusedtrainingandtestingsetsfromtheﬁrstandsecondinternationalChinesewordsegmentationbakeoffs(SproatandEmerson,2003;Emerson,2005).Theaccuraciesarecomparedtoothermodelsintheliterature.F-measureisusedastheaccuracymeasure.De-ﬁneprecisionpasthepercentageofwordsinthede-coderoutputthataresegmentedcorrectly,andrecallrasthepercentageofgoldstandardoutputwordsthatarecorrectlysegmentedbythedecoder.The(balanced)F-measureis2pr/(p+r).CWSsystemsareevaluatedbytwotypesoftests.Theclosedtestsrequirethatthesystemistrainedonlywithadesignatedtrainingcorpus.Anyextraknowledgeisnotallowed,includingcommonsur-names,ChineseandArabicnumbers,Europeanlet-ters,lexicons,part-of-speech,semanticsandsoon.Theopentestsdonotimposesuchrestrictions.Opentestsmeasureamodel’scapabilitytoutilizeextrainformationanddomainknowledge,whichcanleadtoimprovedperformance,butsincethisextrainformationisnotstandardized,directcomparisonbetweenopentestresultsislessinformative.Inthispaper,wefocusonlyontheclosedtest.However,theperceptronmodelallowsawiderangeoffeatures,andsofutureworkwillconsiderhowtointegrateopenresourcesintooursystem.6.1LearningcurveInthisexperiment,theagendasizewassetto16,forbothtrainingandtesting.Table2showsthepreci-sion,recallandF-measureforthedevelopmentsetafter1to10trainingiterations,aswellasthenum-berofmistakesmadeineachiteration.Thecorre-spondinglearningcurvesforboththenon-averagedandaveragedperceptronaregiveninFigure3.Thetableshowsthatthenumberofmistakesmadeineachiterationdecreases,reﬂectingtheconver-genceofthelearningalgorithm.Theaveragedper-845

Iteration12345678910P(non-avg)89.091.692.092.392.592.592.592.792.692.6R(non-avg)88.391.492.292.692.792.893.093.093.193.2F(non-avg)88.691.592.192.592.692.692.792.892.892.9P(avg)91.792.893.192.293.193.293.293.293.293.2R(avg)91.692.993.393.493.493.593.593.593.693.6F(avg)91.692.993.293.393.393.493.393.393.493.4#Wrongsentences34011652945621463288217176151139Table2:accuracyusingnon-averagedandaveragedperceptron.P-precision(%),R-recall(%),F-F-measure.B2481632641282565121024Tr6606106838301111164525454922910415598Seg18.6518.1828.8526.5236.5856.4595.45173.38325.99559.87F86.9092.9593.3393.3893.2593.2993.1993.0793.2493.34Table3:theinﬂuenceofagendasize.B-agendasize,Tr-trainingtime(seconds),Seg-testingtime(seconds),F-F-measure.0.860.870.880.890.90.910.920.930.9412345678910number of training iterationsF-measurenon-averagedaveragedFigure3:learningcurvesoftheaveragedandnon-averagedperceptronalgorithmsceptronalgorithmimprovesthesegmentationac-curacyateachiteration,comparedwiththenon-averagedperceptron.Thelearningcurvewasusedtoﬁxthenumberoftrainingiterationsat6fortheremainingexperiments.6.2TheinﬂuenceofagendasizeReducingtheagendasizeincreasesthedecodingspeed,butitcouldcauselossofaccuracybyelimi-natingpotentiallygoodcandidates.Theagendasizealsoaffectsthetrainingtime,andresultingmodel,sincetheperceptrontrainingalgorithmusesthede-coderoutputtoadjustthemodelparameters.Table3showstheaccuracieswithtendifferentagendasizes,eachusedforbothtrainingandtesting.AccuracydoesnotincreasebeyondB=16.Moreover,theaccuracyisquitecompetitiveevenwithBaslowas4.Thisreﬂectsthefactthatthebestsegmentationisoftenwithinthecurrenttopfewcan-didatesintheagenda.2SincethetrainingandtestingtimegenerallyincreasesasNincreases,theagendasizeisﬁxedto16fortheremainingexperiments.6.3TheinﬂuenceofparticularfeaturesOurCWSmodelishighlydependentuponwordin-formation.MostofthefeaturesinTable1arerelatedtowords.Table4showstheaccuracywithvariousfeaturesfromthemodelremoved.Amongthefeatures,vocabularywords(feature1)andlengthpredictionbycharacters(features3to5)showedstronginﬂuenceontheaccuracy,whilewordbigrams(feature2)andspecialcharactersinthem(features11and12)showedcomparativelyweakin-ﬂuence.2TheoptimizationinSection4,whichhasapruningeffect,wasappliedtothisexperiment.Similarobservationsweremadeinseparateexperimentswithoutsuchoptimization.846

FeaturesFFeaturesFAll93.38w/o192.88w/o293.36w/o3,4,592.72w/o693.13w/o793.13w/o893.14w/o9,1093.31w/o11,1293.38w/o13,1493.23Table4:theinﬂuenceoffeatures.(F:F-measure.FeaturenumbersarefromTable1)6.4ClosedtestontheSIGHANbakeoffsFourtrainingandtestingcorporawereusedintheﬁrstbakeoff(SproatandEmerson,2003),includingtheAcademiaSinicaCorpus(AS),thePennChineseTreebankCorpus(CTB),theHongKongCityUni-versityCorpus(CU)andthePekingUniversityCor-pus(PU).However,becausethetestingdatafromthePennChineseTreebankCorpusiscurrentlyun-available,weexcludedthiscorpus.ThecorporaareencodedinGB(PU,CTB)andBIG5(AS,CU).Inordertotestthemconsistentlyinoursystem,theyareallconvertedtoUTF8withoutlossofinforma-tion.TheresultsareshowninTable5.WefollowtheformatfromPengetal.(2004).Eachrowrepre-sentsaCWSmodel.TheﬁrsteightrowsrepresentmodelsfromSproatandEmerson(2003)thatpartic-ipatedinatleastoneclosedtestfromthetable,row“Peng”representstheCRFmodelfromPengetal.(2004),andthelastrowrepresentsourmodel.TheﬁrstthreecolumnsrepresenttestswiththeAS,CUandPUcorpora,respectively.Thebestscoreineachcolumnisshowninbold.Thelasttwocolumnsrep-resenttheaverageaccuracyofeachmodeloverthetestsitparticipatedin(SAV),andouraverageoverthesametests(OAV),respectively.Foreachrowthebestaverageisshowninbold.Weachievedthebestaccuracyintwoofthethreecorpora,andbetteroverallaccuracythanthemajor-ityoftheothermodels.TheaveragescoreofS10is0.7%higherthanourmodel,butS10onlypartici-patedintheHKtest.Fourtrainingandtestingcorporawereusedinthesecondbakeoff(Emerson,2005),includingtheAcademiaSinicacorpus(AS),theHongKongCityUniversityCorpus(CU),thePekingUniversityCor-pus(PK)andtheMicrosoftResearchCorpus(MR).ASCUPUSAVOAVS0193.890.195.193.095.0S0493.993.994.0S0594.289.491.895.3S0694.592.492.493.195.0S0890.493.692.094.3S0996.194.695.495.3S1094.794.794.0S1295.991.693.895.6Peng95.692.894.194.295.096.594.694.0Table5:theaccuraciesovertheﬁrstSIGHANbake-offdata.ASCUPKMRSAVOAVS1494.794.395.096.495.195.4S15b95.294.194.195.894.895.4S2794.594.095.096.094.995.4Zh-a94.794.694.596.495.195.4Zh-b95.195.195.197.195.695.494.695.194.597.2Table6:theaccuraciesoverthesecondSIGHANbakeoffdata.Differentencodingswereprovided,andtheUTF8dataforallfourcorporawereusedinthisexperi-ment.FollowingtheformatofTable5,theresultsforthisbakeoffareshowninTable6.WechosethethreemodelsthatachievedatleastonebestscoreintheclosedtestsfromEmerson(2005),aswellasthesub-word-basedmodelofZhangetal.(2006)forcomparison.Row“Zh-a”and“Zh-b”representthepuresub-wordCRFmodelandtheconﬁdence-basedcombinationoftheCRFandrule-basedmodels,re-spectively.Again,ourmodelachievedbetteroverallaccu-racythanthemajorityoftheothermodels.Onesys-temtoachievecomparableaccuracywithoursys-temisZh-b,whichimprovesuponthesub-wordCRFmodel(Zh-a)bycombiningitwithanindependentdictionary-basedsubmodelandimprovingtheaccu-racyofknownwords.Incomparison,oursystemisbasedonasingleperceptronmodel.Insummary,closedtestsforboththeﬁrstandthesecondbakeoffshowedcompetitiveresultsforour847

systemcomparedwiththebestresultsinthelitera-ture.Ourword-basedsystemachievedthebestF-measuresovertheAS(96.5%)andCU(94.6%)cor-poraintheﬁrstbakeoff,andtheCU(95.1%)andMR(97.2%)corporainthesecondbakeoff.7ConclusionsandFutureWorkWeproposedaword-basedCWSmodelusingthediscriminativeperceptronlearningalgorithm.Thismodelisanalternativetotheexistingcharacter-basedtaggingmodels,andallowswordinformationtobeusedasfeatures.Oneattractivefeatureoftheperceptrontrainingalgorithmisitssimplicity,con-sistingofonlyadecoderandatrivialupdateprocess.Weuseabeam-searchdecoder,whichplacesourworkinthecontextofrecentproposalsforsearch-baseddiscriminativelearningalgorithms.ClosedtestsusingtheﬁrstandsecondSIGHANCWSbake-offdatademonstratedoursystemtobecompetitivewiththebestintheliterature.Openfeatures,suchasknowledgeofnumbersandEuropeanletters,andrelationshipsfromsemanticnetworks(ShiandWang,2007),havebeenreportedtoimproveaccuracy.Therefore,giventheﬂexibilityofthefeature-basedperceptronmodel,anobviousnextstepisthestudyofopenfeaturesintheseg-mentor.Also,wewishtoexplorethepossibilityofin-corporatingPOStaggingandparsingfeaturesintothediscriminativemodel,leadingtojointdecod-ing.Theadvantageistwo-fold:higherlevelsyn-tacticinformationcanbeusedinwordsegmenta-tion,whilejointdecodinghelpstopreventbottom-uperrorpropagationamongthedifferentprocessingsteps.AcknowledgementsThisworkissupportedbytheORSandClarendonFund.Wethanktheanonymousreviewersfortheirinsightfulcomments.ReferencesMichaelCollinsandBrianRoark.2004.Incrementalparsingwiththeperceptronalgorithm.InProceedingsofACL’04,pages111–118,Barcelona,Spain,July.MichaelCollins.2002.Discriminativetrainingmethodsforhiddenmarkovmodels:Theoryandexperimentswithper-ceptronalgorithms.InProceedingsofEMNLP,pages1–8,Philadelphia,USA,July.HalDaumeIII.2006.PracticalStructuredLearningforNatu-ralLanguageProcessing.Ph.D.thesis,USC.ThomasEmerson.2005.ThesecondinternationalChinesewordsegmentationbakeoff.InProceedingsofTheFourthSIGHANWorkshop,Jeju,Korea.Y.FreundandR.Schapire.1999.Largemarginclassiﬁcationusingtheperceptronalgorithm.InMachineLearning,pages277–296.J.Lafferty,A.McCallum,andF.Pereira.2001.Conditionalrandomﬁelds:Probabilisticmodelsforsegmentingandla-belingsequencedata.InProceedingsofthe18thICML,pages282–289,Massachusetts,USA.Y.Li,Zaragoza,R.H.,Herbrich,J.Shawe-Taylor,andJ.Kan-dola.2002.Theperceptronalgorithmwithunevenmargins.InProceedingsofthe9thICML,pages379–386,Sydney,Australia.YaoyongLi,ChuanjiangMiao,KalinaBontcheva,andHamishCunningham.2005.PerceptronlearningforChinesewordsegmentation.InProceedingsoftheFourthSIGHANWork-shop,Jeju,Korea.PercyLiang.2005.Semi-supervisedlearningfornaturallan-guage.Master’sthesis,MIT.F.Peng,F.Feng,,andA.McCallum.2004.Chinesesegmenta-tionandnewworddetectionusingconditionalrandomﬁelds.InProceedingsofCOLING,Geneva,Switzerland.AdwaitRatnaparkhi.1998.MaximumEntropyModelsforNat-uralLanguageAmbiguityResolution.Ph.D.thesis,UPenn.YanxinShiandMengqiuWang.2007.Adual-layerCRFbasedjointdecodingmethodforcascadesegmentationandlabellingtasks.InProceedingsofIJCAI,Hyderabad,India.RichardSproatandThomasEmerson.2003.Theﬁrstinterna-tionalChinesewordsegmentationbakeoff.InProceedingsofTheSecondSIGHANWorkshop,pages282–289,Sapporo,Japan,July.R.Sproat,C.Shih,W.Gail,andN.Chang.1996.Astochas-ticﬁnite-stateword-segmentationalgorithmforChinese.InComputationalLinguistics,volume22(3),pages377–404.XinhaoWang,XiaojunLin,DianhaiYu,HaoTian,andXihongWu.2006.Chinesewordsegmentationwithmaximumen-tropyandn-gramlanguagemodel.InProceedingsoftheFifthSIGHANWorkshop,pages138–141,Sydney,Australia,July.N.Xue.2003.Chinesewordsegmentationascharactertag-ging.InInternationalJournalofComputationalLinguisticsandChineseLanguageProcessing,volume8(1).RuiqiangZhang,GenichiroKikui,andEiichiroSumita.2006.Subword-basedtaggingbyconditionalrandomﬁeldsforChinesewordsegmentation.InProceedingsoftheHumanLanguageTechnologyConferenceoftheNAACL,Compan-ion,volumeShortPapers,pages193–196,NewYorkCity,USA,June.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 848–855,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

848

UnsupervisedCoreferenceResolutioninaNonparametricBayesianModelAriaHaghighiandDanKleinComputerScienceDivisionUCBerkeley{aria42,klein}@cs.berkeley.eduAbstractWepresentanunsupervised,nonparamet-ricBayesianapproachtocoreferencereso-lutionwhichmodelsbothglobalentityiden-tityacrossacorpusaswellasthesequen-tialanaphoricstructurewithineachdocu-ment.Whilemostexistingcoreferenceworkisdrivenbypairwisedecisions,ourmodelisfullygenerative,producingeachmentionfromacombinationofglobalentityproper-tiesandlocalattentionalstate.Despitebe-ingunsupervised,oursystemachievesa70.3MUCF1measureontheMUC-6testset,broadlyintherangeofsomerecentsuper-visedresults.1IntroductionReferringtoanentityinnaturallanguagecanbroadlybedecomposedintotwoprocesses.First,speakersdirectlyintroducenewentitiesintodis-course,entitieswhichmaybesharedacrossdis-courses.Thisinitialreferenceistypicallyaccom-plishedwithproperornominalexpressions.Second,speakersreferbacktoentitiesalreadyintroduced.Thisanaphoricreferenceiscanonically,thoughofcoursenotalways,accomplishedwithpronouns,andisgovernedbylinguisticandcognitiveconstraints.Inthispaper,wepresentanonparametricgenerativemodelofadocumentcorpuswhichnaturallycon-nectsthesetwoprocesses.Mostrecentcoreferenceresolutionworkhasfo-cusedonthetaskofdecidingwhichmentions(nounphrases)inadocumentarecoreferent.Thedomi-nantapproachistodecomposethetaskintoacol-lectionofpairwisecoreferencedecisions.Onethenappliesdiscriminativelearningmethodstopairsofmentions,usingfeatureswhichencodepropertiessuchasdistance,syntacticenvironment,andsoon(Soonetal.,2001;NgandCardie,2002).Althoughsuchapproacheshavebeensuccessful,theyhaveseveralliabilities.First,richfeaturesrequireplen-tifullabeleddata,whichwedonothaveforcorefer-encetasksinmostdomainsandlanguages.Second,coreferenceisinherentlyaclusteringorpartitioningtask.Naivepairwisemethodscananddofailtopro-ducecoherentpartitions.Oneclassicsolutionistomakegreedyleft-to-rightlinkagedecisions.Recentworkhasaddressedthisissueinmoreglobalways.McCallumandWellner(2004)usegraphpartion-inginordertoreconcilepairwisescoresintoaﬁnalcoherentclustering.Nonetheless,allthesesystemscruciallyrelyonpairwisemodelsbecausecluster-levelmodelsaremuchhardertoworkwith,combi-natorially,indiscriminativeapproaches.Anotherthreadofcoreferenceworkhasfocusedontheproblemofidentifyingmatchesbetweendocuments(Milchetal.,2005;BhattacharyaandGetoor,2006;DaumeandMarcu,2005).Thesemethodsignorethesequentialanaphoricstructureinsidedocuments,butconstructmodelsofhowandwhenentitiesaresharedbetweenthem.1Thesemodels,asours,aregenerativeones,sincethefo-cusisonclusterdiscoveryandthedataisgenerallyunlabeled.Inthispaper,wepresentanovel,fullygenera-tive,nonparametricBayesianmodelofmentionsinadocumentcorpus.Ourmodelcapturesbothwithin-andcross-documentcoreference.Atthetop,ahi-erarchicalDirichletprocess(Tehetal.,2006)cap-1Milchetal.(2005)workswithcitationsratherthandis-coursesanddoesmodelthelinearstructureofthecitations.849

turescross-documententity(andparameter)shar-ing,while,atthebottom,asequentialmodelofsaliencecaptureswithin-documentsequentialstruc-ture.Asajointmodelofseveralkindsofdiscoursevariables,itcanbeusedtomakepredictionsabouteitherkindofcoreference,thoughwefocusexperi-mentallyonwithin-documentmeasures.Tothebestofourabilitytocompare,ourmodelachievesthebestunsupervisedcoreferenceperformance.2ExperimentalSetupWeadopttheterminologyoftheAutomaticContextExtraction(ACE)task(NIST,2004).Forthispaper,weassumethateachdocumentinacorpusconsistsofasetofmentions,typicallynounphrases.Eachmentionisareferencetosomeentityinthedomainofdiscourse.Thecoreferenceresolutiontaskistopartitionthementionsaccordingtoreferent.Men-tionscanbedividedintothreecategories,propermentions(names),nominalmentions(descriptions),andpronominalmentions(pronouns).Insection3,wepresentasequenceofincreas-inglyenrichedmodels,motivatingeachfromshort-comingsoftheprevious.Aswego,wewillindicatetheperformanceofeachmodelondatafromACE2004(NIST,2004).Inparticular,weusedasourdevelopmentcorpustheEnglishtranslationsoftheArabicandChinesetreebanks,comprising95docu-mentsandabout3,905mentions.Thisdatawasusedheavilyformodeldesignandhyperparameterselec-tion.Insection5,wepresentﬁnalresultsfornewtestdatafromMUC-6onwhichnotuningordevel-opmentwasperformed.Thistestdatawillformourbasisforcomparisontopreviouswork.Inallexperiments,asiscommon,wewillassumethatwehavebeengivenaspartofourinputthetruementionboundaries,theheadwordofeachmentionandthementiontype(proper,nominal,orpronom-inal).FortheACEdatasets,theheadandmentiontypearegivenaspartofthementionannotation.FortheMUCdata,theheadwascrudelychosentobetherightmostmentiontoken,andthementiontypewasautomaticallydetected.Wewillnotassumeanyotherinformationtobepresentinthedatabe-yondthetextitself.Inparticular,unlikemuchre-latedwork,wedonotassumegoldnamedentityrecognition(NER)labels;indeedwedonotassumeobservedNERlabelsorPOStagsatall.Ourpri-αβKφKZiHiJIαβ∞φ∞ZiHiIJ(a)(b)Figure1:Graphicalmodeldepictionofdocumentlevelen-titymodelsdescribedinsections3.1and3.2respectively.Theshadednodesindicateobservedvariables.maryperformancemetricwillbetheMUCF1mea-sure(Vilainetal.,1995),commonlyusedtoevalu-atecoreferencesystemsonawithin-documentbasis.Sinceoursystemreliesonsampling,allresultsareaveragedoverﬁverandomruns.3CoreferenceResolutionModelsInthissection,wepresentasequenceofgener-ativecoreferenceresolutionmodelsfordocumentcorpora.Allareessentiallymixturemodels,wherethemixturecomponentscorrespondtoentities.Asfarasnotation,weassumeacollectionofIdocu-ments,eachwithJimentions.Weuserandomvari-ablesZtoreferto(indicesof)entities.Wewilluseφztodenotetheparametersforanentityz,andφtorefertotheconcatenationofallsuchφz.Xwillrefersomewhatlooselytothecollectionofvariablesassociatedwithamentioninourmodel(suchastheheadorgender).WewillbeexplicitaboutXandφzshortly.Ourgoalwillbetoﬁndthesettingoftheentityindiceswhichmaximizetheposteriorprobability:Z∗=argmaxZP(Z|X)=argmaxZP(Z,X)=argmaxZZP(Z,X,φ)dP(φ)whereZ,X,andφdenotealltheentityindices,ob-servedvalues,andparametersofthemodel.NotethatwetakeaBayesianapproachinwhichallpa-rametersareintegratedout(orsampled).Theinfer-encetaskisthusprimarilyasearchproblemovertheindexlabelsZ.850

(a)(b)(c)The Weir Group1, whose2  headquarters3 is in the US4, is a large, specialized corporation5 investing in the area of electricity generation. This  power plant6, which7  will be situated in Rudong8, Jiangsu9, has an annual generation capacity of 2.4 million kilowatts.  The Weir Group1, whose1  headquarters2 is in the US3, is a large, specialized corporation4 investing in the area of electricity generation. This  power plant5, which1  will be situated in Rudong6, Jiangsu7, has an annual generation capacity of 2.4 million kilowatts.  The Weir Group1, whose1  headquarters2 is in the US3, is a large, specialized corporation4 investing in the area of electricity generation. This  power plant5, which5  will be situated in Rudong6, Jiangsu7, has an annual generation capacity of 2.4 million kilowatts.  Figure2:Exampleoutputfromvariousmodels.Theoutputfrom(a)isfromtheinﬁnitemixturemodelofsection3.2.Itincorrectlylabelsbothboxedcasesofanaphora.Theoutputfrom(b)usesthepronounheadmodelofsection3.3.ItcorrectlylabelstheﬁrstcaseofanaphorabutincorrectlylabelsthesecondpronominalasbeingcoreferentwiththedominantdocumententityTheWeirGroup.Thiserrorisﬁxedbyaddingthesaliencefeaturecomponentfromsection3.4ascanbeseenin(c).3.1AFiniteMixtureModelOurﬁrst,overlysimplistic,corpusmodelisthestan-dardﬁnitemixtureofmultinomialsshowninﬁg-ure1(a).Inthismodel,eachdocumentisindepen-dentsaveforsomeglobalhyperparameters.Insideeachdocument,thereisaﬁnitemixturemodelwithaﬁxednumberKofcomponents.Thedistributionβovercomponents(entities)isadrawfromasymmet-ricDirichletdistributionwithconcentrationα.Foreachmentioninthedocument,wechooseacompo-nent(anentityindex)zfromβ.Entityzisthenasso-ciatedwithamultinomialemissiondistributionoverheadwordswithparametersφhZ,whicharedrawnfromasymmetricDirichletoverpossiblementionheadswithconcentrationλH.2NotethatheretheXforamentionconsistsonlyofthementionheadH.Asweenrichourmodels,wesimultaneouslyde-velopanaccompanyingGibbssamplingproceduretoobtainsamplesfromP(Z|X).3Fornow,allheadsHareobservedandallparameters(βandφ)canbeintegratedoutanalytically:fordetailsseeTehetal.(2006).TheonlysamplingisforthevaluesofZi,j,theentityindexofmentionjindocumenti.Therelevantconditionaldistributionis:4P(Zi,j|Z−i,j,H)∝P(Zi,j|Z−i,j)P(Hi,j|Z,H−i,j)whereHi,jistheheadofmentionjindocumenti.Expandingeachterm,wehavethecontributionoftheprior:P(Zi,j=z|Z−i,j)∝nz+α2Ingeneral,wewilluseasubscriptedλtoindicateconcen-trationforﬁniteDirichletdistributions.Unlessotherwisespec-iﬁed,λconcentrationparameterswillbesettoe−4andomittedfromdiagrams.3OnecouldusetheEMalgorithmwiththismodel,butEMwillnotextendeffectivelytothesubsequentmodels.4Here,Z−i,jdenotesZ−{Zi,j}wherenzisthenumberofelementsofZ−i,jwithentityindexz.Similarlywehaveforthecontribu-tionoftheemissions:P(Hi,j=h|Z,H−i,j)∝nh,z+λHwherenh,zisthenumberoftimeswehaveseenheadhassociatedwithentityindexzin(Z,H−i,j).3.2AnInﬁniteMixtureModelAcleardrawbackoftheﬁnitemixturemodelistherequirementthatwespecifyapriorianumberofen-titiesKforadocument.WewouldlikeourmodeltoselectKinaneffective,principledway.Amech-anismfordoingsoistoreplacetheﬁniteDirichletprioronβwiththenon-parametricDirichletprocess(DP)prior(Ferguson,1973).5Doingsogivesthemodelinﬁgure1(b).Notethatwenowlistanin-ﬁnitenumberofmixturecomponentsinthismodelsincetherecanbeanunboundednumberofentities.RatherthanaﬁniteβwithasymmetricDirichletdistribution,inwhichdrawstendtohavebalancedclusters,wenowhaveaninﬁniteβ.However,mostdrawswillhaveweightswhichdecayexponentiallyquicklyintheprior(thoughnotnecessarilyintheposterior).Therefore,thereisanaturalpenaltyforeachclusterwhichisactuallyused.WithZobservedduringsampling,wecaninte-grateoutβandcalculateP(Zi,j|Z−i,j)analytically,usingtheChineserestaurantprocessrepresentation:P(Zi,j=z|Z−i,j)∝(α,ifz=znewnz,otherwise(1)whereznewisanewentityindexnotusedinZ−i,jandnzisthenumberofmentionsthathaveentityin-dexz.Asidefromthischange,samplingisidentical5WedonotgiveadetailedpresentationoftheDirichletpro-cesshere,butseeTehetal.(2006)forapresentation.851

PERS : 0.97,   LOC : 0.01,  ORG: 0.01,  MISC: 0.01 Entity TypeSING: 0.99, PLURAL: 0.01Number     MALE: 0.98, FEM: 0.01, NEUTER: 0.01GenderBush : 0.90,   President : 0.06,  .....HeadφtφhφnφgX=ZZMTNGHφθ∞(a)(b)Figure3:(a)Anentityanditsparameters.(b)Theheadmodeldescribedinsection3.3.Theshadednodesindicateobservedvariables.Thementiontypedetermineswhichsetofparentsareused.Thedependenceofmentionvariableonentityparametersφandpronounheadmodelθisomitted.totheﬁnitemixturecase,thoughwiththenumberofclustersactuallyoccupiedineachsampledriftingupwardsordownwards.Thismodelyieldeda54.5F1onourdevelop-mentdata.6Thismodelis,however,hopelesslycrude,capturingnothingofthestructureofcoref-erence.Itslargestempiricalproblemisthat,un-surprisingly,pronounmentionssuchashearegiventheirownclusters,notlabeledascoreferentwithanynon-pronominalmention(seeﬁgure2(a)).3.3PronounHeadModelWhileanentity-speciﬁcmultinomialdistributionoverheadsmakessenseforproper,andsomenom-inal,mentionheads,itdoesnotmakesensetogen-eratepronominalmentionsthissameway.I.e.,allentitiescanbereferredtobygenericpronouns,thechoiceofwhichdependsonentitypropertiessuchasgender,notthespeciﬁcentity.Wethereforeenrichanentity’sparametersφtocontainnotonlyadistributionoverlexicalheadsφh,butalsodistributions(φt,φg,φn)overproper-ties,whereφtparametrizesadistributionoveren-titytypes(PER,LOC,ORG,MISC),andφgforgen-der(MALE,FEMALE,NEUTER),andφnfornumber(SG,PL).7Weassumeeachofthesepropertydistri-butionsisdrawnfromasymmetricDirichletdistri-butionwithsmallconcentrationparameterinordertoencourageapeakedposteriordistribution.6Seesection4forinferencedetails.7Itmightseemthatentitiesshouldsimplyhave,forexam-ple,agendergratherthanadistributionovergendersφg.Therearetworeasonstoadoptthesofterapproach.First,onecanrationalizeitinprinciple,forentitieslikecarsorshipswhosegrammaticalgenderisnotdeterministic.However,therealrea-sonisthatinferenceissimpliﬁed.Inanyevent,wefoundthesepropertydistributionstobehighlydeterminizedintheposterior.αβ∞φ∞Z1Z3L1S1T1 N1G1M1 =NAMZ2L2S2N2G2M2 =NOMT2H2 ="president"H1 ="Bush"H3 ="he"N2 =SGG2 =MALEM3 =PROT2L3S3θIFigure4:Coreferencemodelatthedocumentlevelwithentitypropertiesaswellsaliencelistsusedformentiontypedistri-butions.Thediamondnodesindicatedeterministicfunctions.Shadednodesindicateobservedvariables.Althoughitappearsthateachmentionheadnodehasmanyparents,foragivenmen-tiontype,thementionheaddependsononlyasmallsubset.De-pendenciesinvolvingparametersφandθareomitted.Previously,whenanentityzgeneratedamention,itdrewaheadwordfromφhz.Itnowundergoesamorecomplexandstructuredprocess.ItﬁrstdrawsanentitytypeT,agenderG,anumberNfromthedistributionsφt,φg,andφn,respectively.Oncethepropertiesarefetched,amentiontypeMischosen(proper,nominal,pronoun),accordingtoaglobalmultinomial(againwithasymmetricDirichletpriorandparameterλM).Thiscorrespondstothe(tem-porary)assumptionthatthespeakermakesarandomi.i.d.choiceforthetypeofeachmention.Ourheadmodelwillthengenerateahead,con-ditioningontheentity,itsproperties,andthemen-tiontype,asshowninﬁgure3(b).IfMisnotapronoun,theheadisdrawndirectlyfromtheen-tityheadmultinomialwithparametersφhz.Other-wise,itisdrawnbasedonaglobalpronounheaddis-tribution,conditioningontheentitypropertiesandparametrizedbyθ.Formally,itisgivenby:P(H|Z,T,G,N,M,φ,θ)=(P(H|T,G,N,θ),ifM=PROP(H|φhZ),otherwiseAlthoughwecanobservethenumberandgen-derdrawsforsomementions,likepersonalpro-nouns,therearesomeforwhichpropertiesaren’tobserved(e.g.,it).Becausetheentityprop-ertydrawsarenot(all)observed,wemustnowsampletheunobservedonesaswellastheen-tityindicesZ.Forinstance,wecouldsample852

SalienceFeaturePronounProperNominalTOP0.750.170.08HIGH0.550.280.17MID0.390.400.21LOW0.200.450.35NONE0.000.880.12Table1:Posteriordistributionofmentiontypegivensaliencebybucketingentityactivationrank.Pronounsarepreferredforentitieswhichhavehighsalienceandnon-pronominalmentionsarepreferredforinactiveentities.Ti,j,theentitytypeofpronominalmentionjindocumenti,using,P(Ti,j|Z,N,G,H,T−i,j)∝P(Ti,j|Z)P(Hi,j|T,N,G,H),wheretheposteriordistributionsontherighthandsidearestraight-forwardbecausetheparameterpriorsareallﬁniteDirichlet.SamplingGandNareidentical.Ofcoursewehavepriorknowledgeaboutthere-lationshipbetweenentitytypeandpronounheadchoice.Forexample,weexpectthatheisusedformentionswithT=PERSON.Ingeneral,weassumethatforeachpronominalheadwehavealistofcom-patibleentitytypes,whichweencodeviatheprioronθ.WeassumeθisdrawnfromaDirichletdistri-butionwhereeachpronounheadisgivenasyntheticcountof(1+λP)foreach(t,g,n)wheretiscom-patiblewiththepronounandgivenλPotherwise.So,whileitwillbepossibleintheposteriortousehetorefertoanon-person,itwillbebiasedtowardsbeingusedwithpersons.Thismodelgivessubstantiallyimprovedpredic-tions:64.1F1onourdevelopmentdata.Ascanbeseeninﬁgure2(b),thismodeldoescorrectthesys-tematicproblemofpronounsbeingconsideredtheirownentities.However,itstilldoesnothaveapref-erenceforassociatingpronominalreferencestoen-titieswhichareinanywaylocal.3.4AddingSalienceWewouldlikeourmodeltocapturehowmentiontypesaregeneratedforagivenentityinarobustandsomewhatlanguageindependentway.Thechoiceofentitiesmayreasonablybeconsideredtobeindepen-dentgiventhemixingweightsβ,buthowwerealizeanentityisstronglydependentoncontext(Geetal.,1998).Inordertocapturethisinourmodel,weenrichitasshowninﬁgure4.Asweproceedthroughadocument,generatingentitiesandtheirmentions,wemaintainalistoftheactiveentitiesandtheirsaliences,oractivityscores.Everytimeanentityismentioned,weincrementitsactivityscoreby1,andeverytimewemovetogeneratethenextmention,allactivityscoresdecaybyaconstantfactorof0.5.Thisgivesrisetoanorderedlistofentityactivations,L,wheretherankofanentitydecaysexponentiallyasnewmentionsaregenerated.Wecallthislistasaliencelist.Givenasaliencelist,L,eachpossibleentityzhassomerankonthislist.WediscretizetheseranksintoﬁvebucketsS:TOP(1),HIGH(2-3),MID(4-6),LOW(7+),andNONE.GiventheentitychoicesZ,boththelistLandbucketsSaredeter-ministic(seeﬁgure4).WeassumethatthementiontypeMisconditionedonSasshowninﬁgure4.Wenotethatcorrectlysamplinganentitynowre-quiresthatweincorporatetermsforhowachangewillaffectallfuturesaliencevalues.Thischangesoursamplingequationforexistingentities:P(Zi,j=z|Z−i,j)∝nzYj0≥jP(Mi,j0|Si,j0,Z)(2)wheretheproductrangesoverfuturementionsinthedocumentandSi,j0isthevalueoffuturesaliencefeaturegiventhesettingofallentities,includingset-tingthecurrententityZi,jtoz.Asimilarequationholdsforsamplinganewentity.Notethat,asdis-cussedbelow,thisfullproductcanbetruncatedasanapproximation.Thismodelgivesa71.5F1onourdevelopmentdata.Table1showstheposteriordistributionofthementiontypegiventhesaliencefeature.Thismodelﬁxesmanyanaphoraerrorsandinparticularﬁxesthesecondanaphoraerrorinﬁgure2(c).3.5CrossDocumentCoreferenceOneadvantageofafullygenerativeapproachisthatwecanallowentitiestobesharedbetweendocu-mentsinaprincipledway,givingusthecapacitytodocross-documentcoreference.Moreover,sharingacrossdocumentspoolsinformationabouttheprop-ertiesofanentityacrossdocuments.Wecaneasilylinkentitiesacrossacorpusbyas-sumingthatthepoolofentitiesisglobal,withglobalmixingweightsβ0drawnfromaDPpriorwithconcentrationparameterγ.Eachdocumentuses853

αβ∞φ∞Z1Z3L1S1T1 N1G1M1 =NAMZ2L2S2N2G2M2 =NOMT2H2 ="president"H1 ="Bush"H3 ="he"N2 =SGG2 =MALEM3 =PROT2L3S3β0∞γθIFigure5:GraphicaldepictionoftheHDPcoreferencemodeldescribedinsection3.5.Thedependenciesbetweentheglobalentityparametersφandpronounheadparametersθonthemen-tionobservationsarenotdepicted.thesameglobalentities,buteachhasadocument-speciﬁcdistributionβidrawnfromaDPcenteredonβ0withconcentrationparameterα.Uptothepointwhereentitiesarechosen,thisformulationfollowsthebasichierarchicalDirichletprocesspriorofTehetal.(2006).Oncetheentitiesarechosen,ourmodelfortherealizationofthementionsisasbefore.Thismodelisdepictedgraphicallyinﬁgure5.Althoughitispossibletointegrateoutβ0aswedidtheindividualβi,weinsteadchooseforef-ﬁciencyandsimplicitytosampletheglobalmix-turedistributionβ0fromtheposteriordistributionP(β0|Z).8Thementiongenerationtermsinthemodelandsamplerareunchanged.Inthefullhierarchicalmodel,ourequation(1)forsamplingentities,ignoringthesaliencecomponentofsection3.4,becomes:P(Zi,j=z|Z−i,j,β0)∝(αβu0,ifz=znewnz+αβz0,otherwisewhereβz0istheprobabilityoftheentityzunderthesampledglobalentitydistributionandβu0istheun-knowncomponentmassofthisdistribution.TheHDPlayerofsharingimprovesthemodel’spredictionsto72.5F1onourdevelopmentdata.Weshouldemphasizethatourevaluationisofcourseper-documentanddoesnotreﬂectcross-documentcoreferencedecisions,onlythegainsthroughcross-documentsharing(seesection6.2).8Wedonotgivethedetailshere;seeTehetal.(2006)forde-tailsonhowtoimplementthiscomponentofthesampler(called“directassignment”inthatreference).4InferenceDetailsUpuntilnow,we’vediscussedGibbssampling,butwearenotinterestedinsamplingfromtheposte-riorP(Z|X),butinﬁndingitsmode.Insteadofsamplingdirectlyfromtheposteriordistribution,weinsteadsampleentitiesproportionallytoexponen-tiatedentityposteriors.Theexponentisgivenbyexpcik−1,whereiisthecurrentroundnumber(start-ingati=0),c=1.5andk=20isthetotalnum-berofsamplingepochs.Thisslowlyraisesthepos-teriorexponentfrom1.0toec.Inourexperiments,wefoundthisproceduretooutperformsimulatedan-nealing.WealsofoundsamplingtheT,G,andNvariablestobeparticularlyinefﬁcient,soinsteadwemaintainsoftcountsovereachofthesevariablesandusetheseinplaceofahardsamplingscheme.Wealsofoundthatcorrectlyaccountingforthefutureimpactofsaliencechangestobeparticularlyinefﬁ-cient.However,ignoringthosetermsentirelymadenegligibledifferenceinﬁnalaccuracy.95FinalExperimentsWepresentourﬁnalexperimentsusingthefullmodeldevelopedinsection3.Asinsection3,weusetruementionboundariesandevaluateusingtheMUCF1measure(Vilainetal.,1995).Allhyper-parametersweretunedonthedevelopmentsetonly.Thedocumentconcentrationparameterαwassetbytakingaconstantproportionoftheaveragenumberofmentionsinadocumentacrossthecorpus.Thisnumberwaschosentominimizethesquarederrorbetweenthenumberofproposedentitiesandtrueentitiesinadocument.ItwasnottunedtomaximizetheF1measure.Acoefﬁcientof0.4waschosen.TheglobalconcentrationcoefﬁcientγwaschosentobeaconstantproportionofαM,whereMisthenumberofdocumentsinthecorpus.Wefound0.15tobeagoodvalueusingthesameleast-squarepro-cedure.Thevaluesforthesecoefﬁcientswerenotchangedfortheexperimentsonthetestsets.5.1MUC-6OurmainevaluationisonthestandardMUC-6for-maltestset.10Thestandardexperimentalsetupfor9Thiscorrespondstotruncatingequation(2)atj0=j.10SincetheMUCdataisnotannotatedwithmentiontypes,weautomaticallydetectthisinformationinthesamewayasLuo854

DatasetNumDocs.Prec.RecallF1MUC-66080.852.863.9+DRYRUN-TRAIN25179.159.768.0+ENGLISH-NWIRE38180.462.470.3DatasetPrec.RecallF1ENGLISH-NWIRE66.762.364.2ENGLISH-BNEWS63.261.362.3CHINESE-NWIRE71.663.367.2CHINESE-BNEWS71.261.866.2(a)(b)Table2:FormalResults:OursystemevaluatedusingtheMUCmodeltheoreticmeasureVilainetal.(1995).Thetablein(a)isourperformanceonthethirtydocumentMUC-6formaltestsetwithincreasingamountsoftrainingdata.Inallcasesforthetable,weareevaluatingonthesamethirtydocumenttestsetwhichisincludedinourtrainingset,sinceoursysteminunsupervised.Thetablein(b)isourperformanceontheACE2004trainingsets.thisdataisa30/30documenttrain/testsplit.Train-ingoursystemonall60documentsofthetrainingandtestset(asthisisinanunsupervisedsystem,theunlabeledtestdocumentsarepresentattrain-ingtime),butevaluatingonlyonthetestdocuments,gave63.9F1andislabeledMUC-6intable2(a).Oneadvantageofanunsupervisedapproachisthatwecaneasilyutilizemoredatawhenlearningamodel.WedemonstratetheeffectivenessofthisfactbyevaluatingontheMUC-6testdocumentswithin-creasingamountsofunannotatedtrainingdata.Weﬁrstaddedthe191documentsfromtheMUC-6dryruntrainingset(whichwerenotpartofthetrain-ingdataforofﬁcialMUC-6evaluation).Thismodelgave68.0F1andislabeled+DRYRUN-TRAINinta-ble2(a).WethenaddedtheACEENGLISH-NWIREtrainingdata,whichisfromadifferentcorporathantheMUC-6testsetandfromadifferenttimeperiod.Thismodelgave70.3F1andislabeled+ENGLISH-NWIREintable2(a).Ourresultsonthistestsetaresurprisinglycom-parableto,thoughslightlylowerthan,somerecentsupervisedsystems.McCallumandWellner(2004)report73.4F1ontheformalMUC-6testset,whichisreasonablyclosetoourbestMUC-6numberof70.3F1.McCallumandWellner(2004)alsoreportamuchlower91.6F1ononlypropernounsmen-tions.Oursystemachievesa89.8F1whenevalu-ationisrestrictedtoonlypropermentions.11Theetal.(2004).AmentionisproperifitisannotatedwithNERinformation.ItisapronouniftheheadisonthelistofEn-glishpronouns.Otherwise,itisanominalmention.NotewedonotusetheNERinformationforanypurposebutdeterminingwhetherthementionisproper.11ThebestresultsweknowontheMUC-6testsetusingthestandardsettingareduetoLuoetal.(2004)whoreporta81.3F1(muchhigherthanothers).However,itisnotclearthisisacomparablenumber,duetotheapparentuseofgoldNERfea-tures,whichprovideastrongcluetocoreference.Regardless,itisunsurprisingthattheirsystem,whichhasmanyrichfeatures,wouldoutperformours.HEADENTTYPEGENDERNUMBERBush:1.0PERSMALESGAP:1.0ORGNEUTERPLviacom:0.64,company:0.36ORGNEUTERSGteamsters:0.22,union:0.78,MISCNEUTERPLTable3:Frequententitiesoccurringacrossdocumentsalongwithheaddistributionandmodeofpropertydistributions.closestcomparableunsupervisedsystemisCardieandWagstaff(1999)whousepairwiseNPdistancestoclusterdocumentmentions.Theyreporta53.6F1onMUC6whentuningdistancemetricweightstomaximizeF1onthedevelopmentset.5.2ACE2004WealsoperformedexperimentsonACE2004data.Duetolicensingrestrictions,wedidnothaveaccesstotheACE2004formaldevelopmentandtestsets,andsotheresultspresentedareonthetrainingsets.Wereportresultsonthenewswiresection(NWIREintable2b)andthebroadcastnewssection(BNEWSintable2b).Thesedatasetsincludetheprenomi-nalmentiontype,whichisnotpresentintheMUC-6data.Wetreatedprenominalsanalogouslytothetreatmentofproperandnominalmentions.WealsotestedoursystemontheChinesenewswireandbroadcastnewssectionsoftheACE2004trainingsets.Ourrelativelyhigherperfor-manceonChinesecomparedtoEnglishisperhapsduetothelackofprenominalmentionsintheChi-nesedata,aswellasthepresenceoffewerpronounscomparedtoEnglish.OurACEresultsaredifﬁculttocompareexactlytopreviousworkbecausewedidnothaveaccesstotherestrictedformaltestset.However,wecanperformaroughcomparisonbetweenourresultsonthetrainingdata(withoutcoreferenceannotation)tosupervisedworkwhichhasusedthesametrainingdata(withcoreferenceannotation)andevaluatedontheformaltestset.DenisandBaldridge(2007)re-855

port67.1F1and69.2F1ontheEnglishNWIREandBNEWSrespectivelyusingtruementionboundaries.Whileoursystemunderperformsthesupervisedsys-tems,itsaccuracyisnonethelesspromising.6Discussion6.1ErrorAnalysisThelargestsourceoferrorinoursystemisbetweencoreferentproperandnominalmentions.Themostcommonexamplesofthiskindoferrorareappos-itiveusagese.g.GeorgeW.Bush,presidentoftheUS,visitedIdaho.Anothererrorofthissortcanbeseeninﬁgure2,wherethecorporationmentionisnotlabeledcoreferentwiththeTheWeirGroupmen-tion.Examplessuchastheseillustratetheregular(atleastinnewswire)phenomenonthatnominalmen-tionsareusedwithinformativeintent,evenwhentheentityissalientandapronouncouldhavebeenusedunambiguously.Thisaspectofnominalmentionsisentirelyunmodeledinoursystem.6.2GlobalCoreferenceSincewedonothavelabeledcross-documentcoref-erencedata,wecannotevaluateoursystem’scross-documentperformancequantitatively.However,inadditiontoobservingthewithin-documentgainsfromsharingshowninsection3,wecanmanuallyinspectthemostfrequentlyoccurringentitiesinourcorpora.Table3showssomeofthemostfrequentlyoccurringentitiesacrosstheEnglishACENWIREcorpus.NotethatBushisthemostfrequententity,thoughhis(andothers’)nominalclusterpresidentismistakenlyitsownentity.Mergingofproperandnominalclustersdoesoccurascanbeseenintable3.6.3UnsupervisedNERWecanuseourmodeltoforunsupervisedNERtagging:foreachpropermention,assignthemodeofthegeneratingentity’sdistributionoverentitytypes.Notethatinourmodeltheonlywayanen-titybecomesassociatedwithanentitytypeisbythepronounsusedtorefertoit.12IfweevaluateoursystemasanunsupervisedNERtaggerforthepropermentionsintheMUC-6testset,ityieldsa12Geetal.(1998)exploitasimilarideatoassigngendertopropermentions.per-labelaccuracyof61.2%(onMUClabels).Al-thoughnowhereneartheperformanceofstate-of-the-artsystems,thisresultbeatsasimplebaselineofalwaysguessingPERSON(themostcommonentitytype),whichyields46.4%.Thisresultisinterest-inggiventhatthemodelwasnotdevelopedforthepurposeofinferringentitytypeswhatsoever.7ConclusionWehavepresentedanovel,unsupervisedapproachtocoreferenceresolution:globalentitiesaresharedacrossdocuments,thenumberofentitiesisdeter-minedbythemodel,andmentionsaregeneratedbyasequentialsaliencemodelandamodelofpronoun-entityassociation.Althoughoursystemdoesnotperformquiteaswellasstate-of-the-artsupervisedsystems,itsperformanceisinthesamegeneralrange,despitethesystembeingunsupervised.ReferencesI.BhattacharyaandL.Getoor.2006.Alatentdirichletmodelforunsupervisedentityresolution.SIAMconferenceondatamining.ClaireCardieandKiriWagstaff.1999.Nounphrasecorefer-enceasclustering.EMNLP.HalDaumeandDanielMarcu.2005.ABayesianmodelforsu-pervisedclusteringwiththeDirichletprocessprior.JMLR.PascalDenisandJasonBaldridge.2007.Global,jointdetermi-nationofanaphoricityandcoreferenceresolutionusinginte-gerprogramming.HLT-NAACL.ThomasFerguson.1973.Abayesiananalysisofsomenon-parametricproblems.AnnalsofStatistics.NiyuGe,JohnHale,andEugeneCharniak.1998.Astatisticalapproachtoanaphoraresolution.SixthWorkshoponVeryLargeCorpora.XiaoqiangLuo,AbeIttycheriah,HongyanJing,NandaKamb-hatla,andSalimRoukos.2004.Amention-synchronouscoreferenceresolutionalgorithmbasedonthebelltree.ACL.AndrewMcCallumandBenWellner.2004.Conditionalmod-elsofidentityuncertaintywithapplicationtonouncorefer-ence.NIPS.BrianMilch,BhaskaraMarthi,StuartRussell,DavidSontag,DanielL.Ong,andAndreyKolobov.2005.Blog:Proba-bilisticmodelswithunknownobjects.IJCAI.VincentNgandClaireCardie.2002.Improvingmachinelearn-ingapproachestocoreferenceresolution.ACL.NIST.2004.TheACEevaluationplan.W.Soon,H.Ng,andD.Lim.2001.Amachinelearningap-proachtocoreferenceresolutionofnounphrases.Computa-tionalLinguistics.YeeWhyeTeh,MichaelJordan,MatthewBeal,andDavidBlei.2006.Hierarchicaldirichletprocesses.JournaloftheAmer-icanStatisticalAssociation,101.MarcVilain,JohnBurger,JohnAberdeen,DennisConnolly,andLynetteHirschman.1995.Amodel-theoreticcorefer-encescoringscheme.MUC-6.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 856–863,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

856

Pivot Language Approach for Phrase-Based Statistical Machine Translation Hua Wu and Haifeng Wang Toshiba (China) Research and Development Center 5/F., Tower W2, Oriental Plaza, No.1, East Chang An Ave., Dong Cheng District Beijing, 100738, China {wuhua,wanghaifeng}@rdc.toshiba.com.cn   Abstract This paper proposes a novel method for phrase-based statistical machine translation by using pivot language. To conduct trans-lation between languages Lf and Le with a small bilingual corpus, we bring in a third language Lp, which is named the pivot lan-guage. For Lf-Lp and Lp-Le, there exist large bilingual corpora. Using only Lf-Lp and Lp-Le bilingual corpora, we can build a translation model for Lf-Le. The advantage of this method lies in that we can perform translation between Lf and Le even if there is no bilingual corpus available for this language pair. Using BLEU as a metric, our pivot language method achieves an ab-solute improvement of 0.06 (22.13% rela-tive) as compared with the model directly trained with 5,000 Lf-Le sentence pairs for French-Spanish translation. Moreover, with a small Lf-Le bilingual corpus available, our method can further improve the transla-tion quality by using the additional Lf-Lp and Lp-Le bilingual corpora. 1 Introduction For statistical machine translation (SMT), phrase-based methods (Koehn et al., 2003; Och and Ney, 2004) and syntax-based methods (Wu, 1997; Al-shawi et al. 2000; Yamada and Knignt, 2001; Melamed, 2004; Chiang, 2005; Quick et al., 2005; Mellebeek et al., 2006) outperform word-based methods (Brown et al., 1993). These methods need large bilingual corpora. However, for some lan-guages pairs, only a small bilingual corpus is available, which will degrade the performance of statistical translation systems. To solve this problem, this paper proposes a novel method for phrase-based SMT by using a pivot language. To perform translation between languages Lf and Le, we bring in a pivot language Lp, for which there exist large bilingual corpora for language pairs Lf-Lp and Lp-Le. With the Lf-Lp and Lp-Le bilingual corpora, we can build a translation model for Lf-Le by using Lp as the pivot language. We name the translation model pivot model. The advantage of this method lies in that we can con-duct translation between Lf and Le even if there is no bilingual corpus available for this language pair. Moreover, if a small corpus is available for Lf-Le, we build another translation model, which is named standard model. Then, we build an interpo-lated model by performing linear interpolation on the standard model and the pivot model. Thus, the interpolated model can employ both the small Lf-Le corpus and the large Lf-Lp and Lp-Le corpora. We perform experiments on the Europarl corpus (Koehn, 2005). Using BLEU (Papineni et al., 2002) as a metric, our method achieves an absolute im-provement of 0.06 (22.13% relative) as compared with the standard model trained with 5,000 Lf-Le sentence pairs for French-Spanish translation. The translation quality is comparable with that of the model trained with a bilingual corpus of 30,000 Lf-Le sentence pairs. Moreover, translation quality is further boosted by using both the small Lf-Le bilin-gual corpus and the large Lf-Lp and Lp-Le corpora. Experimental results on Chinese-Japanese trans-lation also indicate that our method achieves satis-factory results using English as the pivot language.  857

The remainder of this paper is organized as fol-lows. In section 2, we describe the related work. Section 3 briefly introduces phrase-based SMT. Section 4 and Section 5 describes our method for phrase-based SMT using pivot language. We de-scribe the experimental results in sections 6 and 7. Lastly, we conclude in section 8. 2 Related Work Our method is mainly related to two kinds of methods: those using pivot language and those using a small bilingual corpus or scarce resources.  For the first kind, pivot languages are employed to translate queries in cross-language information retrieval (CLIR) (Gollins and Sanderson, 2001; Kishida and Kando, 2003). These methods only used the available dictionaries to perform word by word translation. In addition, NTCIR 4 workshop organized a shared task for CLIR using pivot lan-guage. Machine translation systems are used to translate queries into pivot language sentences, and then into target sentences (Sakai et al., 2004). Callison-Burch et al. (2006) used pivot lan-guages for paraphrase extraction to handle the un-seen phrases for phrase-based SMT. Borin (2000) and Wang et al. (2006) used pivot languages to improve word alignment. Borin (2000) used multi-lingual corpora to increase alignment coverage. Wang et al. (2006) induced alignment models by using two additional bilingual corpora to improve word alignment quality. Pivot Language methods were also used for translation dictionary induction (Schafer and Yarowsky, 2002), word sense disam-biguation (Diab and Resnik, 2002), and so on. For the second kind, Niessen and Ney (2004) used morpho-syntactic information for translation between language pairs with scarce resources. Vandeghinste et al. (2006) used translation dic-tionaries and shallow analysis tools for translation between the language pair with low resources. A shared task on word alignment was organized as part of the ACL 2005 Workshop on Building and Using Parallel Texts (Martin et al., 2005). This task focused on languages with scarce resources. For the subtask of unlimited resources, some re-searchers (Aswani and Gaizauskas, 2005; Lopez and Resnik, 2005; Tufis et al., 2005) used lan-guage-dependent resources such as dictionary, the-saurus, and dependency parser to improve word alignment results. In this paper, we address the translation problem for language pairs with scarce resources by bring-ing in a pivot language, via which we can make use of large bilingual corpora. Our method does not need language-dependent resources or deep linguistic processing. Thus, the method is easy to be adapted to any language pair where a pivot lan-guage and corresponding large bilingual corpora are available. 3 Phrase-Based SMT According to the translation model presented in (Koehn et al., 2003), given a source sentence f, the best target translationbeste can be obtained according to the following model )()()|(maxarg)|(maxargeeeeeffeelengthLMbestωppp== (1)Where the translation model )|(efpcan be decomposed into  ∏=−−=IiiiiiiiIIaefpbadefefp1111),|()()|()|(λφw(2)Where )|(iiefφ and )(1−−iibad denote phrase translation probability and distortion probability, respectively. ),|(aefpiiw is the lexical weight, and λ is the strength of the lexical weight. 4 Phrase-Based SMT Via Pivot Language This section will introduce the method that per-forms phrase-based SMT for the language pair Lf-Le by using the two bilingual corpora of Lf-Lp and Lp-Le. With the two additional bilingual corpora, we train two translation models for Lf-Lp and Lp-Le, respectively. Based on these two models, we build a pivot translation model for Lf-Le, with Lp as a pivot language. According to equation (2), the phrase translation probability and the lexical weight are language dependent. We will introduce them in sections 4.1 and 4.2, respectively. 4.1 Phrase Translation Probability Using the Lf-Lp and Lp-Le bilingual corpora, we train two phrase translation probabilities 858

)|(iipfφ and )|(iiepφ, where ip is the phrase in the pivot language Lp. Given the phrase translation probabilities )|(iipfφ and )|(iiepφ, we obtain the phrase translation probability )|(iiefφ according to the following model. ∑=ipiiiiiiiepepfef)|(),|()|(φφφ (3)The phrase translation probability ),|(iiiepfφ does not depend on the phase ie in the language Le, since it is estimated from the Lf-Lp bilingual corpus. Thus, equation (3) can be rewritten as  ∑=ipiiiiiieppfef)|()|()|(φφφ (4)4.2 Lexical Weight Given a phrase pair ),(ef and a word alignment a between the source word positions ni,...,1= and the target word positions mj,...,1=, the lexical weight can be estimated according to the following method (Koehn et al., 2003). ∏∑=∈∀∈=niajijiefwajijaefp1),()|(),(|1),|(w (5)In order to estimate the lexical weight, we first need to obtain the alignment information a be-tween the two phrases f and e, and then estimate the lexical translation probability )|(efw accord-ing to the alignment information. The alignment information of the phrase pair ),(ef can be in-duced from the two phrase pairs ),(pf and ),(ep.  Figure 1. Alignment Information Induction Let 1a and 2a represent the word alignment in-formation inside the phrase pairs ),(pf and ),(ep respectively, then the alignment information a inside ),(ef can be obtained as shown in (6). An example is shown in Figure 1. }),(&),(:|),{(21aepapfpefa∈∈∃= (6)With the induced alignment information, this paper proposes a method to estimate the probabil-ity directly from the induced phrase pairs. We name this method phrase method. If we use K to denote the number of the induced phrase pairs, we estimate the co-occurring frequency of the word pair ),(ef according to the following model. ∑∑===niaiKkkieeffefefcount11),(),()|(),(δδφ (7)Where )|(efkφ is the phrase translation probabil-ity for phrase pair k. 1),(=yxδ if yx=; other-wise, 0),(=yxδ. Thus, lexical translation prob-ability can be estimated as in (8). ∑='),'(),()|(fefcountefcountefw (8)We also estimate the lexical translation prob-ability )|(efw using the method described in (Wang et al., 2006), which is shown in (9). We named it word method in this paper. );,()|()|()|(pefsimepwpfwefwp∑=(9)Where )|(pfw and )|(epw are two lexical probabilities, and );,(pefsim is the cross-language word similarity. 5 Interpolated Model If we have a small Lf-Le bilingual corpus, we can employ this corpus to estimate a translation model as described in section 3. However, this model may perform poorly due to the sparseness of the data. In order to improve its performance, we can employ the additional Lf-Lp and Lp-Le bilingual corpora. Moreover, we can use more than one pivot lan-guage to improve the translation performance if the corresponding bilingual corpora exist. Different pivot languages may catch different linguistic phe-859

nomena, and improve translation quality for the desired language pair Lf-Le in different ways. If we include n pivot languages, n pivot mod-els can be estimated using the method as described in section 4. In order to combine these n pivot models with the standard model trained with the Lf-Le corpus, we use the linear interpolation method. The phrase translation probability and the lexical weight are estimated as shown in (10) and (11), respectively. ∑==niiiefef0)|()|(φαφ (10)∑==niiiaefpaefp0),|(),|(w,wβ (11)Where )|(0efφ and ),|(aefpw,0 denote the phrase translation probability and lexical weight trained with the Lf-Le bilingual corpus, respec-tively. )|(efiφ and ),|(aefpiw, (ni,...,1=) are the phrase translation probability and lexical weight estimated by using the pivot languages. iα and iβ are the interpolation coefficients. 6 Experiments on the Europarl Corpus 6.1 Data A shared task to evaluate machine translation per-formance was organized as part of the NAACL/HLT 2006 Workshop on Statistical Ma-chine Translation (Koehn and Monz, 2006). The shared task used the Europarl corpus (Koehn, 2005), in which four languages are involved: Eng-lish, French, Spanish, and German. The shared task performed translation between English and the other three languages. In our work, we perform translation from French to the other three lan-guages. We select French to Spanish and French to German translation that are not in the shared task because we want to use English as the pivot lan-guage. In general, for most of the languages, there exist bilingual corpora between these languages and English since English is an internationally used language. Table 1 shows the information about the bilin-gual training data. In the table, "Fr", "En", "Es", and "De" denotes "French", "English", "Spanish", and "German", respectively. For the language pairs Lf-Le not including English, the bilingual corpus is Language Pairs Sentence Pairs Source Words Target Words Fr-En 688,03115,323,737 13,808,104Fr-Es 640,66114,148,926 13,134,411Fr-De 639,69314,215,058 12,155,876Es-En 730,74015,676,710 15,222,105De-En 751,08815,256,793 16,052,269De-Es 672,81313,246,255 14,362,615Table 1. Training Corpus for European Languages extracted from Lf-English and English-Le since Europarl corpus is a multilingual corpus.  For the language models, we use the same data provided in the shared task. We also use the same development set and test set provided by the shared task. The in-domain test set includes 2,000 sen-tences and the out-of-domain test set includes 1,064 sentences for each language. 6.2 Translation System and Evaluation Method To perform phrase-based SMT, we use Koehn's training scripts1 and the Pharaoh decoder (Koehn, 2004). We run the decoder with its default settings and then use Koehn's implementation of minimum error rate training (Och, 2003) to tune the feature weights on the development set. The translation quality was evaluated using a well-established automatic measure: BLEU score (Papineni et al., 2002). And we also use the tool provided in the NAACL/HLT 2006 shared task on SMT to calculate the BLEU scores. 6.3 Comparison of Different Lexical Weights As described in section 4, we employ two methods to estimate the lexical weight in the translation model. In order to compare the two methods, we translate from French to Spanish, using English as the pivot language. We use the French-English and English-Spanish corpora described in Table 1 as training data.  During training, before estimating the Spanish to French phrase translation probabil-ity, we filter those French-English and English-Spanish phrase pairs whose translation probabili-ties are below a fixed threshold 0.001.2 The trans-lation results are shown in Table 2.                                                  1 It is located at http://www.statmt.org/wmt06/shared-task/baseline.htm  2 In the following experiments using pivot languages, we use the same filtering threshold for all of the language pairs. 860

The phrase method proposed in this paper per-forms better than the word method proposed in (Wang et al., 2006). This is because our method uses phrase translation probability as a confidence weight to estimate the lexical translation probabil-ity. It strengthens the frequently aligned pairs and weakens the infrequently aligned pairs. Thus, the following sections will use the phrase method to estimate the lexical weight. Method In-Domain Out-of-DomainPhrase  0.3212 0.2098 Word 0.2583 0.1672 Table 2. Results with Different Lexical Weights 6.4 Results of Using One Pivot Language This section describes the translation results by using only one pivot language. For the language pair French and Spanish, we use English as the pivot language. The entire French-English and English-Spanish corpora as described in section 4 are used to train a pivot model for French-Spanish. As described in section 5, if we have a small Lf-Le bilingual corpus and large Lf-Lp and Lp-Le bilin-gual corpora, we can obtain interpolated models. In order to conduct the experiments, we ran-domly select 5K, 10K, 20K, 30K, 40K, 50K, and 100K sentence pairs from the French-Spanish cor-pus. Using each of these corpora, we train a stan-dard translation model.  For each standard model, we interpolate it with the pivot model to get an interpolated model. The interpolation weights are tuned using the develop-ment set. For all the interpolated models, we set 9.00=α, 1.01=α, 9.00=β, and 1.01=β. We test the three kinds of models on both the in-domain and out-of-domain test sets. The results are shown in Figures 2 and 3.  The pivot model achieves BLEU scores of 0.3212 and 0.2098 on the in-domain and out-of-domain test set, respectively. It achieves an abso-lute improvement of 0.05 on both test sets (16.92% and 35.35% relative) over the standard model trained with 5,000 French-Spanish sentence pairs. And the performance of the pivot models are com-parable with that of the standard models trained with 20,000 and 30,000 sentence pairs on the in-domain and out-of-domain test set, respectively. When the French-Spanish training corpus is in-creased, the standard models quickly outperform the pivot model. 2527293133353751020304050100Fr-Es Data (k pairs)BLEU (%)InterpolatedStandardPivot   Figure 2. In-Domain French-Spanish Results 1416182022242651020304050100Fr-Es Data (K pairs)BLEU (%)InterpolatedStandardPivot  Figure 3. Out-of-Domain French-Spanish Results 1820222426283051020304050100Fr-En Data (k Pairs)BLEU (%)InterpolatedStandardPivot   Figure 4. In-Domain French-English Results 9101112131415161751020304050100Fr-De Data (k Pairs)BLEU (%)InterpolatedStandardPivot  Figure 5. In-Domain French-German Results When only a very small French-Spanish bilin-gual corpus is available, the interpolated method can greatly improve the translation quality. For example, when only 5,000 French-Spanish sen-tence pairs are available, the interpolated model outperforms the standard model by achieving a relative improvement of 17.55%, with the BLEU score improved from 0.2747 to 0.3229. With 50,000 French-Spanish sentence pairs available, the interpolated model significantly3 improves the translation quality by achieving an absolute im-                                                 3 We conduct the significance test using the same method as described in (Koehn and Monz, 2006). 861

provement of 0.01 BLEU. When the French-Spanish training corpus increases to 100,000 sen-tence pairs, the interpolated model achieves almost the same result as the standard model. This indi-cates that our pivot language method is suitable for the language pairs with small quantities of training data available. Besides experiments on French-Spanish transla-tion, we also conduct translation from French to English and French to German, using German and English as the pivot language, respectively. The results on the in-domain test set4 are shown in Fig-ures 4 and 5. The tendency of the results is similar to that in Figure 2. 6.5 Results of Using More Than One Pivot Language For French to Spanish translation, we also intro-duce German as a pivot language besides English. Using these two pivot languages, we build two dif-ferent pivot models, and then perform linear inter-polation on them. The interpolation weights for the English pivot model and the German pivot model are set to 0.6 and 0.4 respectively5. The translation results on the in-domain test set are 0.3212, 0.3077, and 0.3355 for the pivot models using English, German, and both German and English as pivot languages, respectively. With the pivot model using both English and German as pivot languages, we interpolate it with the standard models trained with French-Spanish corpora of different sizes as described in the above section. The comparison of the translation results among the interpolated models, standard models, and the pivot model are shown in Figure 6. It can be seen that the translation results can be further improved by using more than one pivot language. The pivot model "Pivot-En+De" using two pivot languages achieves an absolute im-provement of 0.06 (22.13% relative) as compared with the standard model trained with 5,000 sen-tence pairs. And it achieves comparable translation result as compared with the standard model trained with 30,000 French-Spanish sentence pairs. The results in Figure 6 also indicate the interpo-lated models using two pivot languages achieve the                                                  4 The results on the out-of-domain test set are similar to that in Figure 3. We only show the in-domain translation results in all of the following experiments because of space limit. 5 The weights are tuned on the development set. best results of all. Significance test shows that the interpolated models using two pivot languages sig-nificantly outperform those using one pivot lan-guage when less than 50,000 French-Spanish sen-tence pairs are available. 272829303132333435363751020304050100Fr-Es Data (k Pairs)BLEU (%)Interpolated-En+DeInterpolated-EnInterpolated-DeStandardPivot-En+De Figure 6. In-Domain French-Spanish Translation Results by Using Two Pivot Languages 6.6 Results by Using Pivot Language Related Corpora of Different Sizes In all of the above results, the corpora used to train the pivot models are not changed. In order to ex-amine the effect of the size of the pivot corpora, we decrease the French-English and English-French corpora. We randomly select 200,000 and 400,000 sentence pairs from both of them to train two pivot models, respectively. The translation results on the in-domain test set are 0.2376, 0.2954, and 0.3212 for the pivot models trained with 200,000, 400,000, and the entire French-English and English-Spanish corpora, respectively. The results of the interpolated models and the standard models are shown in Figure 7. The results indicate that the larger the training corpora used to train the pivot model are, the better the translation quality is. 272829303132333435363751020304050100Fr-Es Data (k pairs)BLEU (%)Interpolated-Allinterpolated-400kInterpolated-200kStandard Figure 7. In-Domain French-Spanish Results by Using Lf-Lp and Lp-Le Corpora of Different Sizes 862

7 Experiments on Chinese to Japanese Translation In section 6, translation results on the Europarl multilingual corpus indicate the effectiveness of our method. To investigate the effectiveness of our method by using independently sourced parallel corpora, we conduct Chinese-Japanese translation using English as a pivot language in this section, where the training data are not limited to a specific domain. The data used for this experiment is the same as those used in (Wang et al., 2006). There are 21,977, 329,350, and 160,535 sentence pairs for the lan-guage pairs Chinese-Japanese, Chinese-English, and English-Japanese, respectively. The develop-ment data and testing data include 500 and 1,000 Chinese sentences respectively, with one reference for each sentence. For Japanese language model training, we use about 100M bytes Japanese corpus. The translation result is shown in Figure 8. The pivot model only outperforms the standard model trained with 2,500 sentence pairs. This is because (1) the corpora used to train the pivot model are smaller as compared with the Europarl corpus; (2) the training data and the testing data are not limited to a specific domain; (3) The languages are not closely related. 6810121416182.551021.9Chinese-Japanese Data (k pairs)BLEU (%)InterpolatedStandardPivot   Figure 8. Chinese-Japanese Translation Results The interpolated models significantly outper-form the other models. When only 5,000 sentence pairs are available, the BLEU score increases rela-tively by 20.53%. With the entire (21,977 pairs) Chinese-Japanese available, the interpolated model relatively increases the BLEU score by 5.62%, from 0.1708 to 0.1804. 8 Conclusion This paper proposed a novel method for phrase-based SMT on language pairs with a small bilin-gual corpus by bringing in pivot languages. To per-form translation between Lf and Le, we bring in a pivot language Lp, via which the large corpora of Lf-Lp and Lp-Le can be used to induce a translation model for Lf-Le. The advantage of this method is that it can perform translation between the lan-guage pair Lf-Le even if no bilingual corpus for this pair is available. Using BLEU as a metric, our method achieves an absolute improvement of 0.06 (22.13% relative) as compared with the model di-rectly trained with 5,000 sentence pairs for French-Spanish translation. And the translation quality is comparable with that of the model directly trained with 30,000 French-Spanish sentence pairs. The results also indicate that using more pivot lan-guages leads to better translation quality. With a small bilingual corpus available for Lf-Le, we built a translation model, and interpolated it with the pivot model trained with the large Lf-Lp and Lp-Le bilingual corpora. The results on both the Europarl corpus and Chinese-Japanese transla-tion indicate that the interpolated models achieve the best results. Results also indicate that our pivot language approach is suitable for translation on language pairs with a small bilingual corpus. The less the Lf-Le bilingual corpus is, the bigger the improvement is. We also performed experiments using Lf-Lp and Lp-Le corpora of different sizes. The results indi-cate that using larger training corpora to train the pivot model leads to better translation quality. References Hiyan Alshawi, Srinivas Bangalore, and Shona Douglas. 2000. Learning Dependency Translation Models as Collections of Finite-State Head Transducers. Com-putational Linguistics, 26(1):45-60. Niraj Aswani and Robert Gaizauskas. 2005. Aligning Words in English-Hindi Parallel Corpora. In Proc. of the ACL 2005 Workshop on Building and Using Par-allel Texts: Data-driven Machine Translation and Beyond, pages 115-118. Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The Mathematics of Statistical Machine Translation: Pa-rameter Estimation. Computational Linguistics, 19(2): 263-311. Chris Callison-Burch, Philipp Koehn, and Miles Os-borne. 2006. Improved Statistical Machine Transla-863

tion Using Paraphrases. In Proc. of NAACL-2006, pages 17-24. Lars Borin. 2000. You'll Take the High Road and I'll Take the Low Road: Using a Third Language to Im-prove Bilingual Word Alignment. In Proc. of COL-ING-2000, pages 97-103. David Chiang. 2005. A Hierarchical Phrase-Based Model for Statistical Machine Translation. In Proc. of ACL-2005, pages 263-270. Mona Diab  and  Philip Resnik. 2002. An Unsupervised Method for Word Sense Tagging using Parallel Cor-pora. In Proc. of ACL-2002, pages 255-262. Tim Gollins and Mark Sanderson. 2001. Improving Cross Language Information Retrieval with Triangu-lated Translation. In Proc. of ACM SIGIR-2001, pages 90-95. Kazuaki Kishida and Noriko Kando.  2003. Two-Stage Refinement of Query Translation in a Pivot Lan-guage Approach to Cross-Lingual Information Re-trieval: An Experiment at CLEF 2003. In Proc. of CLEF-2003. pages 253-262. Philipp Koehn. 2004. Pharaoh: A Beam Search Decoder for Phrase-Based Statistical Machine Translation Models. In Proc. of AMTA-2004, pages 115-124. Philipp Koehn. 2005. Europarl: A Parallel Corpus for Statistical Machine Translation. In Proc. of MT Summit X, pages 79-86. Philipp Koehn and Christof Monz. 2006. Manual and Automatic Evaluation of Machine Translation be-tween European Languages. In Proc. of the 2006 HLT-NAACL Workshop on Statistical Machine Translation, pages 102-121. Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical Phrase-Based Translation. In Proc. of HLT-NAAC- 2003, pages 127-133. Adam Lopez and Philip Resnik. 2005. Improved HMM Alignment Models for Languages with Scarce Re-sources. In Proc. of the ACL-2005 Work-shop on Building and Using Parallel Texts: Data-driven Ma-chine Translation and Beyond, pages 83-86. Joel Martin, Rada Mihalcea, and Ted Pedersen. 2005. Word Alignment for Languages with Scarce Re-sources. In Proc. of the ACL-2005 Workshop on Building and Using Parallel Texts: Data-driven Ma-chine Translation and Beyond, pages 65-74. Dan Melamed. 2004. Statistical Machine Translation by Parsing. In Proc. of ACL-2004, pages 653-660. Bart Mellebeek, Karolina Owczarzak, Declan Groves, Josef Van Genabith, and Andy Way. 2006. A Syntac-tic Skeleton for Statistical Machine Translation. In Proc. of EAMT-2006, pages 195-202. Sonja Niessen and Hermann Ney. 2004. Statistical Machine Translation with Scarce Resources Using Morpho-Syntactic Information. Computational linguistics, 30(2): 181-204. Franz Josef Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In Proc. of ACL-2003, pages 160-167. Franz Josef Och and Hermann Ney. 2004. The Align-ment Template Approach to Statistical Machine Translation. Computational Linguistics, 30(4):417-449. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a Method for Automatic Evaluation of Machine Translation. In Proc. of ACL-2002, pages 311-318. Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency Treelet Translation: Syntactically In-formed Phrasal SMT. In Proc. of ACL-2005, pages 271-279. Tetsuya Sakai, Makoto Koyama, Akira Kumano, and Toshihiko Manabe. 2004. Toshiba BRIDJE at NTCIR-4 CLIR: Monolingual/Bilingual IR and Flexible Feedback. In Proc. of NTCIR 4. Charles Schafer and David Yarowsky. 2002. Inducing Translation Lexicons via Diverse Similarity Meas-ures and Bridge Languages. In Proc. of CoNLL-2002, pages 1-7. Haifeng Wang, Hua Wu, and Zhanyi Liu. 2006. Word Alignment for Languages with Scarce Resources Us-ing Bilingual Corpora of Other Language Pairs. In Proc. of COLING/ACL-2006 Main Conference Poster Sessions, pages 874-881. Dan Tufis, Radu Ion, Alexandru Ceausu, and Dan Ste-fanescu. 2005. Combined Word Alignments. In Proc. of the ACL-2005 Workshop on Building and Using Parallel Texts: Data-driven Machine Translation and Beyond, pages 107-110. Vincent Vandeghinste, Ineka Schuurman, Michael Carl, Stella Markantonatou, and Toni Badia. 2006. METIS-II: Machine Translation for Low-Resource Languages. In Proc. of LREC-2006, pages 1284-1289. Dekai Wu. 1997. Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora. Computational Linguistics, 23(3):377-403. Kenji Yamada and Kevin Knight. 2001. A Syntax Based Statistical Translation Model. In Proc. of ACL-2001, pages 523-530. Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 864–871,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

864

BootstrappingaStochasticTransducerforArabic-EnglishTransliterationExtractionTarekSherifandGrzegorzKondrakDepartmentofComputingScienceUniversityofAlbertaEdmonton,Alberta,CanadaT6G2E8{tarek,kondrak}@cs.ualberta.caAbstractWeproposeabootstrappingapproachtotrainingamemorilessstochastictransducerforthetaskofextractingtransliterationsfromanEnglish-Arabicbitext.Thetrans-ducerlearnsitssimilaritymetricfromthedatainthebitext,andthuscanfunc-tiondirectlyonstringswrittenindifferentwritingscriptswithoutanyadditionallan-guageknowledge.Weshowthatthisboot-strappedtransducerperformsaswellorbet-terthanamodeldesignedspeciﬁcallytode-tectArabic-Englishtransliterations.1IntroductionTransliterationsarewordsthatareconvertedfromonewritingscripttoanotheronthebasisoftheirpro-nunciation,ratherthanbeingtranslatedonthebasisoftheirmeaning.Transliterationsincludenameden-tities(e.g.		(cid:16)(cid:30)ƒð		(cid:30)(cid:10)g./JaneAusten)andlexicalloans(e.g.	ñ(cid:10)		(cid:30)(cid:10)	®Ê(cid:16)/television).Analgorithmtodetecttransliterationsautomati-callyinabitextcanbeaneffectivetoolformanytasks.Modelsofmachinetransliterationsuchasthosepresentedin(Al-OnaizanandKnight,2002)or(AbdulJaleelandLarkey,2003)requirealargesetofsampletransliterationstousefortraining.Ifsuchatrainingsetisunavailableforaparticularlanguagepair,adetectionalgorithmwouldleadtoasignif-icantgainintimeoverattemptingtobuildthesetmanually.Algorithmsforcross-languageinforma-tionretrievaloftenencountertheproblemofout-of-vocabularywords,orwordsnotpresentinthealgo-rithm’slexicon.Often,asigniﬁcantproportionofthesewordsarenamedentitiesandthusarecandi-datesfortransliteration.Atransliterationdetectionalgorithmcouldbeusedtomapnamedentitiesinaquerytopotentialtransliterationsinthetargetlan-guagetext.Themainchallengeintransliterationdetectionliesinthefactthattransliterationisalossyprocess.Inotherwords,informationcanbelostabouttheoriginalwordwhenitistransliterated.Thiscanoc-curbecauseofphoneticgapsinonelanguageortheother.Forexample,theEnglish[p]sounddoesnotexistinArabic,andtheArabic[	]sound(madebytheletter¨)doesnotexistinEnglish.Thus,PaulistransliteratedasÈñ.[bul],andú(cid:10)Î«[	ali]istranslit-eratedasAli.Anotherformoflossoccurswhentherelationshipbetweentheorthographicandphoneticrepresentationsofawordareunclear.Forexample,the[k]soundwillalwaysbewrittenwiththeletterinArabic,butinEnglishitcanbewrittenasc,kch,ck,ccorkk(nottomentionbeingoneofthesoundsproducedbyx).Finally,lettersmaybedeletedinonelanguageortheother.InArabic,shortvowelswilloftenbeomitted(e.g.	(cid:173)ƒñ(cid:10)/Yousef),whileinEnglishtheArabicZand¨areoftendeleted(e.g.É(cid:10)«AÖ…/Ismael).WeexploretheuseofwordsimilaritymetricsonthetaskofArabic-Englishtransliterationdetectionandextraction.Oneofourprimarygoalsinexplor-ingthesemetricsistoassesswhetheritispossiblemaintainhighperformancewithoutmakingtheal-gorithmslanguage-speciﬁc.Manyword-similaritymetricsrequirethatthestringsbeingcomparedbe865

writteninthesamescript.Levenshteineditdistance,forexample,doesnotproduceameaningfulscoreintheabsenceofcharacteridentities.Thus,ifthesemetricsaretobeusedfortransliterationextraction,modiﬁcationsmustbemadetoallowthemtocom-paredifferentscripts.Freemanetal.(2006)taketheapproachofman-uallyencodingagreatdealoflanguageknowl-edgedirectlyintotheirArabic-Englishfuzzymatch-ingalgorithm.Theydeﬁneequivalenceclassesbe-tweenlettersinthetwoscriptsandperformseveralrule-basedtransformationstomakewordpairsmorecomparable.Thisapproachisunattractivefortworeasons.Firstly,predictingallpossiblerelationshipsbetweenlettersinEnglishandArabicisdifﬁcult.Forexample,allowanceshavetobemadeforun-usualpronunciationsinforeignwordssuchasthechinclich´eorthecinMilosevic.Secondly,thealgo-rithmbecomescompletelylanguage-speciﬁc,whichmeansthatitcannotbeusedforanyotherlanguagepair.Weproposeamethodtolearnletterrelation-shipsdirectlyfromthebitextcontainingthetranslit-erations.OurmodelisbasedonthememorilessstochastictransducerproposedbyRistadandYian-ilos(1998),whichderivesaprobabilisticword-similarityfunctionfromasetofexamples.Thetransducerisabletolearneditdistancecostsbe-tweendisjointsetsofcharactersrepresentingdif-ferentwritingscriptswithoutanylanguage-speciﬁcknowledge.Thetransducerapproach,however,re-quiresalargesetoftrainingexamples,whichisalimitationnotpresentinthefuzzymatchingalgo-rithm.Thus,weproposeabootstrappingapproach(Yarowsky,1995)totrainthestochastictransduceriterativelyasitextractstransliterationsfromabi-text.Thebootstrappedstochastictransduceriscom-pletelylanguage-independent,andweshowthatitisabletoperformatleastaswellastheArabic-Englishspeciﬁcfuzzymatchingalgorithm.Theremainderofthispaperisorganizedasfol-lows.Section2presentsourbootstrappingmethodtotrainastochastictransducer.Section3outlinestheArabic-Englishfuzzymatchingalgorithm.Sec-tion4discussesotherword-similaritymodelsusedforcomparison.Section5describestheresultsoftwoexperimentsperformedtotestthemodels.Sec-tion6brieﬂydiscussespreviousapproachestode-tectingtransliterations.Section7presentsourcon-clusionsandpossibilitiesforfuturework.2BootstrappingwithaStochasticTransducerRistadandYianilos(1998)proposeaprobabilisticframeworkforwordsimilarity,inwhichthesimi-larityofapairofwordsisdeﬁnedasthesumoftheprobabilitiesofallpathsthroughamemorilessstochastictransducerthatgeneratethepairofwords.Thisisreferredtoastheforwardscoreofthepairofwords.Theyoutlineaforward-backwardalgorithmtotrainthemodelandshowthatitoutperformsLev-enshteineditdistanceonthetaskofpronunciationclassiﬁcation.Thetrainingalgorithmbeginsbycallingthefor-ward(Equation1)andbackward(Equation2)func-tionstoﬁllintheFandBtablesfortrainingpairsandtwithrespectivelengthsIandJ.F(0,0)=1F(i,j)=P(si,ǫ)F(i−1,j)+P(ǫ,tj)F(i,j−1)+P(si,tj)F(i−1,j−1)(1)B(I,J)=1B(i,j)=P(si+1,ǫ)B(i+1,j)+P(ǫ,tj+1)B(i,j+1)+P(si+1,tj+1)B(i+1,j+1)(2)Theforwardvalueateachposition(i,j)intheFmatrixsigniﬁesthesumoftheprobabilitiesofallpathsthroughthetransducerthatproducethepreﬁxpair(si1,tj1),whileB(i,j)containsthesumoftheprobabilitiesofallpathsthroughthetransducerthatgeneratethesufﬁxpair(sIi+1,tJj+1).Thesetablescanthenbeusedtocollectpartialcountstoupdatetheprobabilities.Forexample,themapping(si,tj)wouldcontributeacountaccordingtoEquation3.Thesecountsarethennormalizedtoproducetheup-datedprobabilitydistribution.C(si,tj)+=F(i−1,j−1)P(si,tj)B(i,j)F(I,J)(3)Themajorissueinportingthememorilesstrans-ducerovertoourtaskoftransliterationextraction866

isthatitstrainingissupervised.Inotherwords,itwouldrequirearelativelylargesetofknowntranslit-erationsfortraining,andthisisexactlywhatwewouldlikethemodeltoacquire.Inordertoover-comethisproblem,welooktothebootstrappingmethodoutlinedin(Yarowsky,1995).Yarowskytrainsarule-basedclassiﬁerforwordsensedisam-biguationbystartingwithasmallsetofseedex-amplesforwhichthesenseisknown.Thetrainedclassiﬁeristhenusedtolabelexamplesforwhichthesenseisunknown,andthesenewlylabeledex-amplesarethenusedtoretraintheclassiﬁer.Theprocessisrepeateduntilconvergence.Ourmethodusesasimilarapproachtotrainthestochastictransducer.Thealgorithmproceedsasfollows:1.Initializethetrainingsetwiththeseedpairs.2.Trainthetransducerusingtheforward-backwardalgorithmonthecurrenttrainingset.3.Calculatetheforwardscoreforallwordpairsunderconsideration.4.Iftheforwardscoreforapairofwordsisaboveapredeterminedacceptancethreshold,addthepairtothetrainingset.5.Repeatsteps2-4untilthetrainingsetceasestogrow.Oncetrainingstops,thetransducercanbeusedtoscorepairsofwordsnotinthetrainingset.Forourexperiments,theacceptancethresholdwasop-timizedonaseparatedevelopmentset.Forwardscoreswerenormalizedbytheaverageofthelengthsofthetwowords.3Arabic-EnglishFuzzyStringMatchingInthissection,weoutlinethefuzzystringmatchingalgorithmproposedbyFreemanetal.(2006).ThealgorithmisbasedonthestandardLevenshteindis-tanceapproach,butencodesagreatdealofknowl-edgeabouttherelationshipsbetweenEnglishandArabicletters.Initially,thecandidatewordpairismodiﬁedintwoways.Theﬁrsttransformationisarule-basedletternormalizationofbothwords.Someexamplesofnormalizationinclude:•Englishdoublelettercollapse:e.g.Miller→Miler.,(cid:13),Æ,ø↔a,e,i,o,u .↔b,p,v(cid:16) , ,(cid:17) ↔th.↔j,g	 ↔d,z¨,Z↔’,c,a,e,i,o,u(cid:16)†↔q,g,k↔k,c,sø(cid:10)↔y,i,e,j(cid:16)è↔a,eTable1:Asampleoftheletterequivalenceclassesforfuzzystringmatching.AlgorithmVowelNorm(Estring,Astring)foreachi:=0tomin(|Estring|,|Astring|)foreachj:=0tomin(|Estring|,|Astring|)ifAstringi=EstringjOutstring.=Estringj;i++;j++;ifvowel(Astringi)∧vowel(Estringj)Outstring.=Estringj;i++;j++;if¬vowel(Astringi)∧vowel(Estringj)j++;ifj<|Estringj|Outstring.=Estringj;i++;j++;elseOutstring.=Estringj;i++;j++;whilej<|Estring|if¬vowel(Estringj)Outstring.=Estringj;j++;returnOutstring;Figure1:Pseudocodeforthevoweltransformationprocedure.•Arabichamzacollapse:e.g.	¬	å(cid:17)…(cid:13)→	¬	å(cid:17)….•Individualletternormalizations:e.g.Hen-drix→Hendriksor	(cid:173)(cid:10)	å(cid:17)…→	(cid:173)(cid:10)	îD….ThesecondtransformationisaniterationthroughbothwordstoremoveanyvowelsintheEnglishwordforwhichthereisnosimilarlypositionedvowelintheArabicword.Thepseudocodeforourimplementationofthisvoweltransformationispre-sentedinFigure1.Afterletterandvoweltransformations,theLeven-shteindistanceiscomputedusingtheletterequiva-lencesasmatchesinsteadofidentities.Someequiv-alenceclassesbetweenEnglishandArabiclettersareshowninTable1.TheArabicandEnglishletterswithinaclassaretreatedasidentities.Forexample,theArabic	¬canmatchbothfandvinEnglishwithnocost.TheresultingLevenshteindistanceisnor-malizedbythesumofthelengthsofbothwords.867

LevenshteinALINEFuzzyMatchBootstrapLang.-speciﬁcNoNoYesNoPreprocessingRomanizationPhon.ConversionNoneNoneData-drivenNoNoNoYesTable2:Comparisonoftheword-similaritymodels.Severalothermodiﬁcations,suchaslightstem-mingandmultiplepassestodiscovermoredifﬁ-cultmappings,werealsoproposed,buttheywerefoundtoinﬂuenceperformanceminimally.Thus,theequivalenceclassesandtransformationsaretheonlymodiﬁcationswereproduceforourexperi-mentshere.4OtherModelsofWordSimilarityInthissection,wepresenttwomodelsofwordsimi-larityusedforpurposesofcomparison.LevenshteindistanceandALINEarenotlanguage-speciﬁcperse,butrequirethatthewordsbeingcomparedbewritteninacommonscript.Thus,theyrequiresomelanguageknowledgeinordertoconvertoneorbothofthewordsintothecommonscript.AcomparisonofallthemodelspresentedisgiveninTable2.4.1LevenshteinEditDistanceAsabaselineforourexperiments,weusedLeven-shteineditdistance.Thealgorithmsimplycountstheminimumnumberofinsertions,deletionsandsubstitutionsrequiredtoconvertonestringintoan-other.Levenshteindistancedependsonﬁndingiden-ticalletters,sobothwordsmustusethesameal-phabet.Priortocomparison,weconverttheAra-bicwordsintotheLatinalphabetusingtheintuitivemappingsforeachlettershowninTable3.Thedistancesarealsonormalizedbythelengthofthelongerofthetwowordstoavoidexcessivelypenal-izinglongerwords.4.2ALINEUnlikeotheralgorithmspresentedhere,theALINEalgorithm(Kondrak,2000)operatesinthephonetic,ratherthantheorthographic,domain.Itwasorig-inallydesignedtoidentifycognatesinrelatedlan-guages,butitcanbeusedtocomputesimilaritybe-tweenanypairofwords,providedthattheyareex-pressedinastandardphoneticnotation.Individual,(cid:13),Æ,Z→a .→b(cid:16) , →t(cid:16)è→a(cid:17) →thh.→jh,è→h→khX,	 →d	X,	 →th→r	→z€, →s(cid:17)€→sh¨→’	¨→g	¬→f(cid:16)†→q→kÈ→l→m	→nð→wø(cid:10)→yTable3:ArabicRomanizationforLevenshteindis-tance.phonemesinputtothealgorithmaredecomposedintoadozenphoneticfeatures,suchasPlace,Man-nerandVoice.Asubstitutionscorebetweenapairofphonemesisbasedonthesimilarityasassessedbyacomparisonoftheindividualfeatures.Afteranoptimalalignmentofthetwowordsiscomputedwithadynamicprogrammingalgorithm,theoverallsimilarityscoreissettothesumofthescoresofalllinksinthealignmentnormalizedbythelengthofthelongerofthetwowords.Inourexperiments,theArabicandEnglishwordswereconvertedintophonetictranscriptionsusingadeterministicrule-basedtransformation.Thetran-scriptionswereonlyapproximate,especiallyforEn-glishvowels.Arabicemphaticconsonantswerede-pharyngealized.5EvaluationTheword-similaritymetricswereevaluatedontwoseparatetasks.Inexperiment1(Section5.1)thetaskwastoextracttransliterationsfromasentencealignedbitext.Experiment2(Section5.2)providesthealgorithmswithnamedentitiesfromanEnglishdocumentandrequiresthemtoextractthetransliter-ationsfromthedocument’sArabictranslation.Thetwobitextsusedintheexperimentswerethe868

Figure2:Precisionpernumberofwordsextractedforthevariousalgorithmsfromasentence-alignedbitext.ArabicTreebankPart1-10kwordEnglishTransla-tioncorpusandtheArabicEnglishParallelNewsPart1corpus(approx.2.5Mwords).Bothbi-textscontainArabicnewsarticlesandtheirEnglishtranslationsalignedatthesentencelevel,andbothareavailablefromtheLinguisticDateConsortium.TheTreebankdatawasusedasadevelopmentsettooptimizetheacceptancethresholdusedbythebootstrappedtransducer.Testingforthesentence-alignedextractiontaskwasdoneontheﬁrst20ksentences(approx.50kwords)oftheparallelnewsdata,whilethenamedentityextractiontaskwasper-formedontheﬁrst1000documentsoftheparal-lelnewsdata.Theseedsetforbootstrappingthestochastictransducerwasmanuallyconstructedandconsistedof14namesandtheirtransliterations.5.1Experiment1:Sentence-AlignedDataTheﬁrsttaskusedtotestthemodelswastocompareandscorethewordsremainingineachbitextsen-tencepairafterpreprocessingthebitextinthefol-lowingway:•TheEnglishcorpusistokenizedusingamodi-ﬁed1versionofWordSplitter2.•AlluncapitalizedEnglishwordsareremoved.•Stopwords(mainlyprepositionsandauxiliary1Thewaytheprogramhandlesapostrophes(’)hadtobemodiﬁedsincetheyaresometimesusedtorepresentglottalstopsintransliterationsofArabicwords,e.g.qala’a.2Availableathttp://l2r.cs.uiuc.edu/˜cogcomp/tools.php.verbs)areremovedfrombothsidesofthebi-text.•AnyEnglishwordsoflengthlessthan4andArabicwordsoflengthlessthan3areremoved.EachalgorithmﬁndsthetopmatchforeachEn-glishwordandthetopmatchforeachArabicword.Iftwowordsmarkeachotherastheirtopscorers,thenthepairismarkedasatransliterationpair.Thisone-to-oneconstraintismeanttoboostprecision,thoughitwillalsolowerrecall.Thisisbecauseformanyofthetasksinwhichtransliterationextractionwouldbeuseful(suchasbuildingalexicon),preci-sionisdeemedmoreimportant.Transliterationpairsaresortedaccordingtotheirscores,andthetop500hundredscoringpairsarereturned.Theresultsforthesentence-alignedextractiontaskarepresentedinFigure2.Sincethenumberofactualtransliterationsinthedatawasunknown,therewasnowaytocomputerecall.Themeasureusedhereistheprecisionforeach100wordsex-tractedupto500.Thebootstrappingmethodisequaltooroutperformstheothermethodsatalllevels,in-cludingtheArabic-Englishspeciﬁcfuzzymatchal-gorithm.Fuzzymatchingdoeswellfortheﬁrstfewhundredwordsextracted,buteventuallyfallsbelowthelevelofthebaselineLevenshtein.Interestingly,thebootstrappedtransducerdoesnotseemtohavetroublewithdigraphs,despitetheone-to-onenatureofthecharacteroperations.Wordpairswithtwo-to-onemappingssuchassh/(cid:17)€or869

MetricArabicRomanizedEnglish1Bootstrap		(cid:10)	(cid:30)(cid:10)	gBalakhyrynAlgerian2BootstrapÕÎƒðwslmIslam3FuzzyM.É¾ËlklAlkella4FuzzyM.	AÔ«’mAncommon5ALINE	ƒskrsugar6Leven. .A(cid:13)asabArab7AllAÓmarkMarks8All	ñ(cid:10)ƒðrwsywnRussian9All(cid:16)é(cid:10)j.(cid:28)(cid:10)(cid:16)	(cid:16)(cid:30)ƒistratyjyastrategic10All				¯frnkFrenchTable4:Asampleoftheerrorsmadebytheword-similaritymetrics.x/(cid:129)tendtoscorelowerthantheircounterpartscomposedofonlyone-to-onemappings,butnever-thelessscorehighly.Asampleoftheerrorsmadebyeachword-similaritymetricispresentedinTable4.Errors1-6areindicativeoftheweaknessesofeachindivid-ualalgorithm.Thebootstrappingmethodencoun-tersproblemswhenerroneouspairsbecomepartofthetrainingdata,therebyreinforcingtheerrors.TheonlyproblematicmappinginError1isthe/gmap-ping,andthusthepairhaslittletroublegettingintothetrainingdata.Oncethepairispartoftrainingdata,thealgorithmlearnsthatthemappingisac-ceptableandusesittoacquireothertrainingpairsthatcontainthesameerroneousmapping.Theprob-lemwiththefuzzymatchingalgorithmseemstobethatitcreatestoolargeaclassofequivalentwords.Thepairsinerrors3and4aregivenatotaleditcostof0.Thisispossiblebecauseoftheoverlygen-eralletterandvoweltransformations,aswellasun-usualchoicesmadeforletterequivalences(e.g.¨/cinerror4).ALINE’serrorstendtooccurwhenitlinkstwoletters,basedonphoneticsimilarity,thatarenevermappedtoeachotherintransliterationbe-causetheyeachhaveamoredirectequivalentintheotherlanguage(error5).AlthoughtheArabic[k]isphoneticallysimilartotheEnglishg,theywouldneverbemappedtoeachothersinceEnglishhassev-eralwaysofrepresentinganactual[k]sound.ErrorsmadebyLevenshteindistance(error6)aresimplyduetothefactthatitconsidersallnon-identitymap-pingstobeequivalent.Errors7-10areexamplesofgeneralerrorsmadebyallthealgorithms.Themostcommonerrorwasrelatedtoinﬂection(error7).Thewordsareessen-tiallytransliterationsofeachother,butoneortheotherofthetwowordstakesapluralorsomeotherinﬂectionalendingthatcorruptsthephoneticmatch.Error8representsthecommonproblemofinciden-tallettersimilarity.TheEnglish-ianendingusedfornationalitiesisverysimilartotheArabic	ñ(cid:10)[ijun]and		(cid:30)(cid:10)(cid:10)[ijin]endingswhichareusedforthesamepurpose.Theyaresimilarphoneticallyand,sincetheyarefunctionallysimilar,willtendtoco-occur.Sinceneithercanbesaidtobederivedfromtheother,however,theycannotbeconsideredtranslit-erations.Error9isacaseoftwowordsofcommonorigintakingonlanguage-speciﬁcderivationalend-ingsthatcorruptthephoneticmatch.Finally,error10showsamapping(/c)thatisoftencorrectintransliteration,butisinappropriateinthisparticularcase.5.2Experiment2:Document-AlignedNamedEntityRecognitionThesecondexperimentprovidesamorechallengingtaskfortheevaluationofthemodels.Itisstruc-turedasacross-languagenamedentityrecognitiontasksimilartothoseoutlinedin(LeeandChang,2003)and(KlementievandRoth,2006).Essen-tially,thegoalistousealanguageforwhichnamedentityrecognitionsoftwareisreadilyavailableasareferencefortaggingnamedentitiesinalanguageforwhichsuchsoftwareisnotavailable.Forthistask,thesentencealignmentofthebitextisignored.ForeachnamedentityinanEnglishdocument,themodelsmustselectatransliterationfromwithinthedocument’sentireArabictranslation.Thisismeanttobealooseapproximationofthe“comparable”corporausedin(KlementievandRoth,2006).Thecomparablecorporaarerelateddocumentsindiffer-entlanguagesthatarenottranslations(e.g.newsar-ticlesdescribingthesameevent),andthussentencealignmentisnotpossible.Theﬁrst1000documentsintheparallelnewsdatawereusedfortesting.TheEnglishsideofthebi-textwastaggedwithNamedEntityTagger3,whichlabelsnamedentitiesasperson,location,organiza-3Availableathttp://l2r.cs.uiuc.edu/˜cogcomp/tools.php.870

MethodAccuracyLevenshtein69.3ALINE71.9FuzzyMatch74.6Bootstrapping74.6Table5:PrecisionofthevariousalgorithmsontheNERdetectiontask.MetricArabicRomanizedEnglish1BothY.«’bdAbdallah2BootstrapY(cid:10)YªËal’dydAlhadidi3FuzzyMatch		Ö(cid:17)thmnOthmanTable6:AsampleoferrorsmadeontheNERdetec-tiontask.tionormiscellaneous.Thewordslabeledasper-sonwereextracted.Personnamesarealmostalwaystransliterated,whilefortheothercategoriesthisisfarlesscertain.Thelistwasthenhand-checkedtoensurethatallnameswerecandidatesfortransliter-ation,leaving822names.Therestrictionsonwordlengthandstopwordswerethesameasbefore,butinthistaskeachoftheEnglishpersonnamesfromagivendocumentwerecomparedtoallvalidwordsinthecorrespondingArabicdocument,andthetopscorerforeachEnglishnamewasreturned.TheresultsfortheNERdetectiontaskarepre-sentedinTable5.Itseemsthebootstrappedtrans-ducer’sadvantageisrelativetotheproportionofcorrecttransliterationpairstothetotalnumberofcandidates.Asthisproportionbecomessmallerthetransducerisgivenmoreopportunitiestocorruptitstrainingdataandperformanceisaffectedaccord-ingly.Nevertheless,thetransducerisabletoper-formaswellasthelanguage-speciﬁcfuzzymatch-ingalgorithmonthistask,despitethegreaterchal-lengeposedbyselectingcandidatesfromentiredoc-uments.AsampleoferrorsmadebythebootstrappedtransducerandfuzzymatchingalgorithmsisshowninTable6.Error1wasduetothefactthatnamesaresometimessplitdifferentlyinArabicandEnglish.TheArabicéÊËY.«(2words)isgenerallywrittenasAbdallahinEnglish,leadingtopartialmatcheswithpartoftheArabicname.Error2showsanissuewiththeone-to-onenatureofthetransducer.Thedeletedhcanbelearnedinmappingssuchassh/(cid:17)€orph/	¬,butitisgenerallyinappropriatetodeleteanhonitsown.Error3againshowsthatthefuzzymatchingalgorithm’slettertransformationsaretoogeneral.Thevowelremovalsleadtoa0costmatchinthiscase.6RelatedWorkSeveralothermethodsfordetectingtransliterationsbetweenvariouslanguagepairshavebeenproposed.Thesemethodsdifferintheircomplexityaswellasintheirapplicabilitytolanguagepairsotherthanthepairforwhichtheywereoriginallydesigned.Collieretal.(1997)presentamethodforidenti-fyingtransliterationsinanEnglish-Japanesebitext.TheirmodelﬁrsttranscribestheJapanesewordex-pressedinthekatakanasyllabicscriptasthecon-catenationofallpossibletransliterationsofthein-dividualsymbols.Adepth-ﬁrstsearchisthenap-pliedtocomputethenumberofmatchesbetweenthistranscriptionandacandidateEnglishtransliter-ation.Themethodrequiresamanualenumerationofthepossibletransliterationsforeachkatakanasym-bol,whichisunfeasibleformanylanguagepairs.InthemethoddevelopedbyTsuji(2002),katakanastringsareﬁrstsplitintotheirmoraunits,andthenthetransliterationsoftheunitsareassessedmanuallyfromasetoftrainingpairs.Foreachkatakanastringinabitext,allpossibletranslitera-tionsareproducedbasedonthetransliterationunitsdeterminedfromthetrainingset.Thetranslitera-tioncandidatesarethencomparedtotheEnglishwordsaccordingtotheDicescore.Themanualenu-merationofpossiblemappingsmakesthisapproachunattractiveformanylanguagepairs,andthegen-erationofallpossibletransliterationcandidatesisproblematicintermsofcomputationalcomplexity.LeeandChang(2003)detecttransliterationswithagenerativenoisychanneltransliterationmodelsimilartothetransducerpresentedin(KnightandGraehl,1998).TheEnglishsideofthecorpusistaggedwithanamedentitytagger,andthemodelisusedtoisolatethetransliterationsintheChinesetranslation.Thismodel,likethetransducerpro-posedbyRistadandYianilos(1998),mustbetrainedonalargenumberofsampletransliterations,mean-ingitcannotbeusedifsucharesourceisnotavail-871

able.KlementievandRoth(2006)bootstrapwithaper-ceptronandusetemporalanalysistodetecttranslit-erationsincomparableRussian-Englishnewscor-pora.TheEnglishsideisﬁrsttaggedbyanamedentitytagger,andtheperceptronproposestransliter-ationsforthenamedentities.Thecandidatetranslit-erationpairsarethenrerankedaccordingthesimilar-ityoftheirdistributionsacrossdates,ascalculatedbyadiscreteFouriertransform.7ConclusionandFutureWorkWepresentedabootstrappingapproachtotrainingastochastictransducer,whichlearnsscoringparam-etersautomaticallyfromabitext.Theapproachiscompletelylanguage-independent,andwasshowntoperformaswellorbetterthananArabic-EnglishspeciﬁcsimilaritymetriconthetaskofArabic-Englishtransliterationextraction.Althoughthebootstrappedtransducerislanguage-independent,itlearnsonlyone-to-oneletterrelationships,whichisapotentialdrawbackintermsofportingittootherlanguages.OurmodelisabletocaptureEnglishdigraphsandtrigraphs,but,asofyet,wecannotguaranteethemodel’ssuccessonlanguageswithmorecomplexletterrelationships(e.g.alogographicwritingsystemsuchasChinese).Moreresearchisnecessarytoevaluatethemodel’sperformanceonotherlanguages.Anotherareaopentofutureresearchistheuseofmorecomplextransducersforwordcomparison.Forexample,Linden(2006)presentsamodelwhichlearnsprobabilitiesforeditoperationsbytakingintoaccountthecontextinwhichthecharactersappear.Itremainstobeseenhowsuchamodelcouldbeadaptedtoabootstrappingsetting.AcknowledgmentsWewouldliketothankthemembersoftheNLPre-searchgroupattheUniversityofAlbertafortheirhelpfulcommentsandsuggestions.ThisresearchwassupportedbytheNaturalSciencesandEngi-neeringResearchCouncilofCanada.ReferencesN.AbdulJaleelandL.S.Larkey.2003.StatisticaltransliterationforEnglish-Arabiccrosslanguagein-formationretrieval.InCIKM,pages139–146.Y.Al-OnaizanandK.Knight.2002.Machinetranslit-erationofnamesinArabictext.InACLWorkshoponComp.ApproachestoSemiticLanguages.N.Collier,A.Kumano,andH.Hirakawa.1997.Acqui-sitionofEnglish-Japanesepropernounsfromnoisy-parallelnewswirearticlesusingKatakanamatching.InNaturalLanguagePaciﬁcRimSymposium(NL-PRS’97),Phuket,Thailand,pages309–314,Decem-ber.A.Freeman,S.Condon,andC.Ackerman.2006.CrosslinguisticnamematchinginEnglishandAra-bic.InHumanLanguageTechnologyConferenceoftheNAACL,pages471–478,NewYorkCity,USA,June.AssociationforComputationalLinguistics.A.KlementievandD.Roth.2006.Namedentitytranslit-erationanddiscoveryfrommultilingualcomparablecorpora.InHumanLanguageTechnologyConferenceoftheNAACL,pages82–88,NewYorkCity,USA,June.AssociationforComputationalLinguistics.K.KnightandJ.Graehl.1998.Machinetransliteration.ComputationalLinguistics,24(4):599–612.G.Kondrak.2000.Anewalgorithmforthealignmentofphoneticsequences.InNAACL2000,pages288–295.C.LeeandJ.S.Chang.2003.AcquisitionofEnglish-Chinesetransliteratedwordpairsfromparallel-alignedtextsusingastatisticalmachinetransliterationmodel.InHLT-NAACL2003WorkshoponBuildingandusingparalleltexts,pages96–103,Morristown,NJ,USA.AssociationforComputationalLinguistics.K.Linden.2006.Multilingualmodelingofcross-lingualspellingvariants.InformationRetrieval,9(3):295–310,June.E.S.RistadandP.N.Yianilos.1998.Learningstring-editdistance.IEEETransactionsonPatternAnalysisandMachineIntelligence,20(5):522–532.K.Tsuji.2002.AutomaticextractionoftranslationalJapanese-katakanaandEnglishwordpairs.Interna-tionalJournalofComputerProcessingofOrientalLanguages,15(3):261–279.D.Yarowsky.1995.Unsupervisedwordsensedisam-biguationrivalingsupervisedmethods.InMeetingoftheAssociationforComputationalLinguistics,pages189–196.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 872–879,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

872

Benefits of the ‘Massively Parallel Rosetta Stone’:  Cross-Language Information Retrieval with over 30 Languages Peter A. Chew Sandia National Laboratories P. O. Box 5800, MS 1012 Albuquerque, NM 87185-1012, USA pchew@sandia.govAhmed Abdelali New Mexico State University P.O. Box 30002, Mail Stop 3CRL Las Cruces, NM 88003-8001, USA ahmed@crl.nmsu.eduAbstract In this paper, we describe our experiences in extending a standard cross-language in-formation retrieval (CLIR) approach which uses parallel aligned corpora and Latent Semantic Indexing. Most, if not all, previous work which follows this ap-proach has focused on bilingual retrieval; two examples involve the use of French-English or English-Greek parallel cor-pora. Our extension to the approach is ‘massively parallel’ in two senses, one linguistic and the other computational. First, we make use of a parallel aligned corpus consisting of almost 50 parallel translations in over 30 distinct languages, each in over 30,000 documents. Given the size of this dataset, a ‘massively parallel’ approach was also necessitated in the more usual computational sense. Our re-sults indicate that, far from adding more noise, more linguistic parallelism is better when it comes to cross-language retrieval precision, in addition to the self-evident benefit that CLIR can be performed on more languages. 1Introduction Approaches to cross-language information retrieval (CLIR) fall generally into one of two types, or some combination thereof: the ‘query translation’ approach or the ‘parallel corpus’ approach. The first of these, which is perhaps more common, in-volves translation of the query into the target lan-guage, for example using machine translation or on-line dictionaries.  The second makes use of par-allel aligned corpora as training sets. One approach which uses parallel corpora does this in conjunc-tion with Latent Semantic Indexing (LSI) (Lan-dauer and Littman 1990, Young 1994). According to Berry et al. (1994:21), the use of LSI with paral-lel corpora can be just as effective as the query translation approach, and avoids some of the draw-backs of the latter, discussed in Nie et al. (1999). Generally, research in CLIR has not attempted to use very many languages at a time (see for ex-ample Nie and Jin 2002). With query translation (although that is not the approach that Nie and Jin take), this is perhaps understandable, as for each new language, a new translation algorithm must be included. The effort involved in extending query translation to multiple languages, therefore, is likely to be in proportion to the number of lan-guages. With parallel corpora, the reason that research has been limited to only a few languages at a time –and usually just two at a time, as in the LSI work cited above – is more likely to be rooted in the widespread perception that good parallel corpora are difficult to obtain (see for example Asker 2004). However, recent work (Resnik et al. 1999, Chew et al. 2006) has challenged this idea. One advantage of a ‘massively parallel’ multi-lingual corpus is perhaps self-evident: within the LSI framework, the more languages are mapped into the single conceptual space, the fewer restric-tions there are on which languages documents can be selected from for cross-language retrieval. However, several questions were raised for us as 873

we contemplated the use of a massively parallel corpus. Would the addition of languages not used in testing create ‘noise’ for a given language pair, reducing the precision of CLIR? Could partially parallel corpora be used? Our work appears to show both that more languages are generally bene-ficial, and even incomplete parallel corpora can be used. In the remainder of this paper, we provide evidence for this claim. The paper is organized as follows: section 2 describes the work we undertook to build the parallel corpus and its characteristics. In section 3, we outline the mechanics behind the 'Rosetta-Stone' type method we use for cross-language comparison. In section 4, we present and discuss the results of the various tests we per-formed. Finally, we conclude on our findings in section 5. 2The massively parallel corpus  Following Chew et al. (2006), our parallel corpus was built up from translations of the Bible which are freely available on the World Wide Web. Al-though reliable comparable statistics are hard to find, it appears to be generally agreed that the Bi-ble is the world’s most widely translated book, with complete translations in 426 languages and partial translations in 2,403 as of December 31, 2005 (Bible Society, 2006). Great care is taken over the translations, and they are alignable by chapter and verse. According to Resnik et al. (1999), the Bible’s coverage of modern vocabulary may be as high as 85%. The vast majority of the translations we used came from the ‘Unbound Bi-ble’ website (Biola University, 2005-2006); from this website, the text of a large number of different translations of the Bible can – most importantly for our purposes – be downloaded in a tab-delimited format convenient for loading into a database and then indexing by chapter and verse in order to en-sure ‘parallelism’ in the corpus. The number of translations available at the website is apparently being added to, based on our observations access-ing the website on a number of different occasions. The languages we have included in our multilin-gual parallel corpus include those both ancient and modern, and are as follows: Language No. of translations Used in tests Afrikaans 1 12+ Albanian 1 27+ Arabic 1 All Chinese (Simplified) 1 44+ Chinese (Traditional) 1 44+ Croatian 1 27+ Czech 2 12+ Danish 1 12+ Dutch 1 12+ English 7 All Finnish 3 27+ French 2 All German 4 8,27+ Greek (New Testament) 2 46+ Hebrew (Old Testament) 1 46+ Hebrew (Modern) 1 6,12+ Hungarian 1 6+ Italian 2 8,27+ Japanese* 1 9+ Korean 1 27+ Latin 1 8,9,28+ Maori 1 7,8,9,27+ Norwegian 1 27+ Polish* 1 27+ Portuguese 1 27+ Russian 1 All Spanish 2 All Swedish 1 27+ Tagalog 1 27+ Thai 1 27+ Vietnamese 1 27,44+ Table 1. Languages1The languages above represent many of the ma-jor language groups: Austronesian (Maori and Tagalog); Altaic (Japanese and Korean); Sino-Tibetan (Chinese); Semitic (Arabic and Hebrew);  Finno-Ugric (Finnish and Hungarian); Austro-Asiatic (Vietnamese); Tai-Kadai (Thai); and Indo-European (the remaining languages). The two New Testament Greek versions are the Byzan-tine/Majority Text (2000), and the parsed version of the same text, in which we treated distinct mor-phological elements (such as roots or inflectional endings) as distinct terms. Overall, the list includes  1Translations in languages marked with an asterisk above were obtained from websites other than the ‘Unbound Bible’ website. ‘Used in tests’ indicates in which tests in Table 2 below the language was used as training data, and hence the order of addition of languages to the training data. 874

47 versions in 31 distinct languages (assuming without further discussion here that each entry in the list represents a distinct language). We aligned the translations by verse, and, since there are some differences in versification between translations (for example, the Hebrew Old Testa-ment includes the headings for the Psalms as sepa-rate verses, unlike most translations), we spent some time cleaning the data to ensure the align-ment was as good as possible, given available re-sources and our knowledge of the languages. (Even after this process, the alignment was not perfect, and differences in how well the various transla-tions were aligned may account for some of the variability in the outcome of our experiments, de-pending on which translations were used.) The end result was that our parallel corpus consisted of 31,226 ‘mini-documents’ – the total number of text chunks2after the cleaning process, aligned across all 47 versions. The two New Testament Greek versions, and the one Old Testament Hebrew ver-sion, were exceptions because these are only par-tially complete; the former have text in only 7,953 of the verses, and the latter has text in 23,266 of the verses. For some versions, a few of the verse translations are incomplete where a particular verse has been skipped in translation; this also explains the fact that the number of Hebrew and Greek text chunks together do not add up to 31,226. However, the number of such verses is negligible in compari-son to the total. 3Framework The framework we used was the standard LSI framework described in Berry et al. (1994). Each aligned mini-document from the parallel corpus consists of the combination of text from all the 31 languages. A document-by-term matrix is formed in which each cell represents a weighted frequency of a particular term tin a particular document k.We used a standard log-entropy weighting scheme, where the weighted frequency W is given by:  W=log2(F) ×(1 + Ht/log2(N))  where F is the raw frequency of tin k,Htis the standard ‘plog p’measure of the entropy of the term across all documents, and N is the number of  2The text chunks generally had the same boundaries as the verses in the original text. documents in the corpus. The last term in the ex-pression above, log2(N), is the maximum entropy that any term can have in the corpus, and therefore (1 + Ht/log2(N)) is 1 for the most distinctive terms in the corpus, 0 for those which are least dis-tinctive. The sparse document-by-term matrix is sub-jected to singular value decomposition (SVD), and areduced non-sparse matrix is output. Generally, we used the output corresponding to the top 300 singular values in our experiments. When we had a smaller number of languages in the mix, it was possible to use SVDPACK (Berry et al. 1996), which is an open-source non-parallel algorithm for computing the SVD, but for larger problems (in-volving more than a couple of dozen parallel ver-sions), use of a parallel algorithm (in a library called Trilinos) was necessitated. (This was run on aLinux cluster consisting of 4,096 dual CPU com-pute nodes, running on Dell PowerEdge 1850 1U Servers with 6GB of RAM.) In order to test the precision versus recall of our framework, we used translations of the 114 suras of the Qu’ran into five languages, Arabic, English, French, Russian and Spanish. The number of documents used for testing is fairly small, but large enough to give comparative results for our pur-poses which are still highly statistically significant. The test set was split into each of the 10 possible language-pair combinations: Arabic-English, Ara-bic-French, English-French, and so on. For each language pair and test, 228 distinct ‘queries’ were submitted – each query consisting of one of the 228 sura ‘documents’. If the highest-ranking document in the other language of the pair was in fact the query’s translation, then the result was deemed ‘correct’. To assess the aggregate per-formance of the framework, we used two meas-ures: average precision at 0 (the maximum precision at any level of recall), and average preci-sion at 1 document (1 if the ‘correct’ document ranked highest, zero otherwise). The second meas-ure is a stricter one, but we generally found that there is a high rate of correlation between the two measures anyway. 4Results and Discussion The following tables show the results of our tests. First, we present in Table 2 the overall summary, 875

with averages across all language pairs used in testing.  Average precision No. of parallel versionsAt 0 at 1 doc. 20.706064 0.571491 30.747620 0.649269 40.617615 0.531873 50.744951 0.656140 60.811666 0.732602 70.827246 0.753070 80.824501 0.750000 90.823430 0.746053 12 0.827761 0.752632 27 0.825577 0.751316 28 0.823137 0.747807 44 0.839346 0.765789 46 0.839319 0.766667 47 0.842936 0.774561 Table 2. Summary results for all language pairs  From the above, the following should be clear: as more parallel translations are added to the index, the average precision rises considerably at first, and then begins to level off after about the seventh parallel translation. The results will of course vary according to which combination of translations is selected for the index. The number of such combi-nations is generally very large: for example, with 47 translations available, there are 47! / (40! 7!), or 62,891,499, possible ways of selecting 7 transla-tions. Thus, for any particular number of parallel versions, we had to use some judgement in which parallel versions to select, since there was no way to achieve anything like exhaustive coverage of the possibilities. Further, with more than 7 parallel translations, there is certainly no justification for saying that adding more translations or languages increases the ‘noise’ for languages in the test set, since beyond 7 the average precision remains fairly level. If any-thing, in fact, the precision still appears to rise slightly. For example, the average precision at 1 document rises by more than 0.75 percentage points between 46 and 47 versions. Given that in each of these experiments, we are measuring preci-sion 228 times per language pair, and therefore 2,280 times in total, this small rise in precision is significant (p ≈0.034). Interestingly, the 47th ver-sion to be added was parsed New Testament Greek. It appears, therefore, that the parsing helped in particular; we also have evidence from other experiments (not presented here) that overall preci-sion is generally improved for all languages when Arabic wordforms are replaced by their respective citation forms (the bare root, or stem) – also a form of morphological parsing. Ancient Greek, like Arabic, is morphologically highly complex, so it would be understandable that parsing (or stem-ming) would help when parsing of either language is used in training. One other point needs to be made here: the three versions added after the 44th version were the three incomplete versions (the two Greek versions cover just the New Testament, while Ancient Hebrew covers just the Old Testament). The above-mentioned increase in precision which resulted from the addition of these three versions is clear evidence that even in the case where a parallel cor-pus is defective for some language(s), including those languages can still result in the twofold bene-fit that (1) those languages are now available for analysis, and (2) precision is maintained or in-creased for the remaining languages. Finally, precision at 1 document, the stricter of the two measures, is by definition less than or equal to precision at 0. This taken into account, it is also interesting that the gap between the two measures seems to narrow as more parallel transla-tions and parsing are added, as Figure 1 shows. For certain applications where it is important that the translation is ranked first, not just highly, among all retrieved documents, there is thus a par-ticular benefit in using a ‘massively parallel’ aligned corpus. 0%2%4%6%8%10%12%14%16%23456789122728444647Number of parallel translationsDifferentialinpercentagepointsFigure 1. Differential between precision at 0 and precision at 1 document, by number of languages 876

Now we move on to look at more detailed re-sults by language pair. Figure 2 below breaks down the results for precision at 1 document by language pair. In all tests, the two languages in each pair were (naturally) always included in the languages used for training. There is more volatil-ity in the results by language pair than there is in the overall results, shown again at the right of the graph, which should come as no surprise since the averages are based on samples a tenth of the size. Generally, however, the pattern is the same for particular language pairs as it is overall; the more parallel versions are used in training, the better the average precision. There are some more detailed observations which should also be made from Figure 2. First, the average precision clearly varies quite widely between language pairs. The language pairs with the best average precision are those in which two of English, French and Spanish are present. Of the  five languages used for testing, these three cluster together genetically, since all three are Western (Germanic or Romance) Indo-European languages. Moreover, these are the three languages of the five which are written in the Roman alphabet. 00.10.20.30.40.50.60.70.80.91Spanish-ArabicArabic-FrenchRussian-ArabicEnglish-ArabicSpanish-RussianRussian-FrenchEnglish-RussianEnglish-SpanishSpanish-FrenchEnglish-FrenchOVERALLLanguage PairPrecisionat1document23456789122728444647Number of languages Figure 2. Chart of precision at 1 doc. by language pair and number of parallel training versions  However, we believe the explanation for the poorer results for language pairs involving either Arabic, Russian, or both, can be pinned down to something more specific. We have already par-tially alluded to the obvious difference between Arabic and Russian on the one hand, and English, French and Spanish on the other: that Arabic and Russian are highly morphologically rich, while English, French and Spanish are generally analytic languages. This has a clear effect on the statistics for the languages in question, as can be seen in Table 3, which is based on selected translations of the Bible for each of the languages in question.  Translation Types Tokens English (King James) 12,335 789,744 Spanish (Reina Valera 1909) 28,456 704,004 Russian (Synodal 1876) 47,226 560,524 Arabic (Smith Van Dyke) 55,300 440,435 French (Darby) 20,428 812,947 Table 3. Statistics for Bible translations in 5 lan-guages used in test data 877

Assuming that the respective translations are faithful (and we have no reason to believe other-wise), and based on the statistics in Table 3, it should be the case that Arabic contains the most ‘information’ per term (in the information theoretic sense), followed by Russian, Spanish, English and French.3Again, this corresponds to our intuition that much information is contained in Arabic pat-terns and Russian inflectional morphemes, which in English, French and Spanish would be contained in separate terms (for example, prepositions). Without additional pre-processing, however, LSI cannot deal adequately with root-pattern or inflectional morphology. Moreover, it is clearly a weakness of LSI, or at least the standard log-entropy weighting scheme as applied within this framework, that it makes no adjustment for differ-ences in information content per word between languages. Even though we can assume near-equivalency of information content between the different translations above, according to the stan-dard log-entropy weighting scheme there are large differences between the total entropy of particular parallel documents; in general, languages such as English are overweighted while those such as Ara-bic are underweighted. Now that this issue is in perspective, we should draw attention to another detail in Figure 2. Note that the language pairs which benefited most from the addition of Ancient Greek and Hebrew into the training data were those which included Russian, and Russian-Arabic saw the greatest increase in precision. Recall also that the 47th version to be added was the parsed Greek, so that essentially each Greek morpheme is represented by a distinct term. From Figure 2, it seems clear that the inclu-sion of parsed Greek in particular boosted the pre-cision for Russian (this is most visible at the right-hand side of the set of columns for Russian-Arabic and English-Russian). There are, after all, notable similarities between modern Russian and Ancient Greek morphology (for example, the nominal case system). Essentially, the parsed Greek acts as a ‘clue’ to LSI in associating inflected forms in Rus- 3To clarify the meaning of ‘term’ here: for all languages ex-cept Chinese, text is tokenized in our framework into terms using regular expressions; each non-word character (such as punctuation or white space) is assumed to mark the boundary of a word. For Chinese, we made the simplifying assumption that each character represented a separate term. sian with preposition/non-inflected combinations in other languages. These results seem to be further confirmation of the notion that parsing just one of the languages in the mix helps overall; the greatest boost is for those languages with morphology re-lated to that of the parsed language, but there is at least a maintenance, and perhaps a small boost, in the precision for unrelated languages too. Finally, we turn to look at some effects of the particular languages selected for training. Included in the results above, there were three separate tests run in which there were 6 training versions. In all three, Arabic, English, French, Russian and Span-ish were included. The only factor we varied in the three tests was the sixth version. In the three tests, we used Modern Hebrew (a Semitic language, along with Arabic), Hungarian (a Uralic language, not closely related to any of the other five lan-guages), and a second English version respectively. The results of these tests are shown in Figure 3, with figures for the test in which only 5 versions were included for comparative purposes. From these results, it is apparent first of all that it was generally beneficial to add a sixth version, regardless of whether the version added was Eng-lish, Hebrew or Hungarian. This is consistent with the results reported elsewhere in this paper. Sec-ond, it is also apparent that the greatest benefit overall was had by using an additional English ver-sion, rather than using Hebrew or Hungarian. Moreover, perhaps surprisingly, the use of Hebrew in training – even though Hebrew is related to Arabic – was of less benefit to Arabic than either Hungarian or an additional English version. It ap-pears that the use of multiple versions in the same language is beneficial because it enables LSI to make use of the many different instantiations in the expression of a concept in a single language, and that this effect can be greater than the effect which obtains from using heterogeneous languages, even if there is a genetic relationship to existing lan-guages. 878

00.10.20.30.40.50.60.70.80.91Russian-ArabicSpanish-ArabicArabic-FrenchEnglish-ArabicSpanish-RussianRussian-FrenchEnglish-RussianEnglish-SpanishSpanish-FrenchEnglish-FrenchOVERALLLanguage pairPrecisionat1doc.Arabic, English, French, Russian, SpanishArabic, English, French, Hebrew, Russian, SpanishArabic, English, French, Hungarian, Russian, SpanishArabic, English x 2, French, Russian, Spanish Figure 3. Precision at 1 document for 6 training versions, with results of using different mixes of lan-guages for training  Figure 3 may also shed some additional light on one other detail from Figure 2: a perceptible jump in precision between 28 and 44 training versions for Arabic-English and Arabic-French. It should be mentioned that among the 16 additional versions were five English versions (American Standard Version, Basic English Bible, Darby, Webster’s Bible, and Young’s Literal Translation), and one French version (Louis Segond 1910). It seems that Figure 2 and Figure 3 both point to the same thing: that the use of parallel versions or translations in a single language can be particularly beneficial to overall precision within the LSI framework – even to a greater extent than the use of parallel transla-tions in different languages. 5Conclusion In this paper, we have shown how ‘massive paral-lelism’ in an aligned corpus can be used to im-prove the results of cross-language information retrieval. Apart from the obvious advantage (the ability to automate the processing of a greater vari-ety of linguistic data within a single framework), we have shown that including more parallel trans-lations in training improves the precision of CLIR across the board. This is true whether the addi-tional translations are in the language of another translation already within the training set, whether they are in a related language, or whether they are in an unrelated language; although this is not to say that these choices do not lead to (generally minor) variations in the results. The improvement in preci-sion also appears to hold whether the additional translations are complete or incomplete, and it ap-pears that morphological pre-processing helps, not just for the languages pre-processed, but again across the board. Our work also offers further evidence that the supply of useful pre-existing parallel corpora is not perhaps as scarce as it is sometimes claimed to be. Compilation of the 47-version parallel corpus we used was not very time-consuming, especially if the time taken to clean the data is not taken into 879

account, and all the textual material we used is publicly available on the World Wide Web. While the experiments we performed were on non-standard test collections (primarily because the Qu’ran was easy to obtain in multiple lan-guages), it seems that there is no reason to believe our general observation – that more parallelism in the training data is beneficial for cross-language retrieval – would not hold for text from other do-mains. Whether the genre of text used as training data affects the absolute rate of retrieval precision for text of a different genre (e.g. news articles, shopping websites) is a separate question, and one we intend to address more fully in future work. In summary, it appears that we are able to achieve the results we do partly because of the in-herent properties of LSI. In essence, when the data from more and more parallel translations are sub-jected to SVD, the LSI ‘concepts’ become more and more reinforced. The resulting trend for preci-sion to increase, despite ‘blips’ for individual lan-guages, can be seen for all languages. To put it in more prosaic terms, the more different ways the same things are said in, the more understandable they become – including in cross-language infor-mation retrieval. Acknowledgement Sandia is a multiprogram laboratory operated by Sandia Corporation, a Lockheed Martin Company, for the United States Department of Energy’s Na-tional Nuclear Security Administration under con-tract DE-AC04-94AL85000. References  Lars Asker. 2004. Building Resources: Experiences from Amharic Cross Language Information Re-trieval. Paper presented at Cross-Language Informa-tion Retrieval and Evaluation: Workshop of the Cross-Language Evaluation Forum, CLEF 2004.Ricardo Baeza-Yates and Berthier Ribeiro-Neto. 1999. Modern Information Retrieval.New York: ACM Press. Michael Berry, Theresa Do, Gavin O’Brien, Vijay Krishna, and Sowmimi Varadhan. 1996. SVDPACKC (Version 1.0) User’s Guide. Knoxville, TN: University of Tennessee. Bible Society. 2006. AStatistical Summary of Lan-guages with the Scriptures.Accessed at http://www.biblesociety.org/latestnews/latest341-slr2005stats.html on Jan. 5, 2007. Biola University. 2005-2006. The Unbound Bible.Ac-cessed at http://www.unboundbible.com/ on Jan. 5, 2007. Peter Chew, Stephen Verzi, Travis Bauer and Jonathan McClain. 2006. Evaluation of the Bible as a Re-source for Cross-Language Information Retrieval. In Proceedings of the Workshop on Multilingual Lan-guage Resources and Interoperability,68-74. Syd-ney: Association for Computational Linguistics. Susan Dumais. 1991. Improving the Retrieval of Infor-mation from External Sources. Behavior Research Methods, Instruments, and Computers 23(2):229-236. Julio Gonzalo. 2001. Language Resources in Cross-Language Text Retrieval: a CLEF Perspective. In Carol Peters (ed.). Cross-Language Information Re-trieval and Evaluation: Workshop of the Cross-Language Evaluation Forum, CLEF 2000:36-47. Berlin: Springer-Verlag.  Dragos Munteanu and Daniel Marcu. 2006. Improving Machine Translation Performance by Exploiting Non-Parallel Corpora. Computational Linguistics 31(4):477-504. Jian-Yun Nie and Fuman Jin. 2002. A Multilingual Ap-proach to Multilingual Information Retrieval. Pro-ceedings of the Cross-Language Evaluation Forum,101-110. Berlin: Springer-Verlag. Jian-Yun Nie, Michel Simard, Pierre Isabelle, and Rich-ard Durand. 1999. Cross-Language Retrieval based on Parallel Texts and Automatic Mining of Parallel Texts from the Web. Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,74-81, August 15-19, 1999, Berkeley, CA.  Carol Peters (ed.). 2001. Cross-Language Information Retrieval and Evaluation: Workshop of the Cross-Language Evaluation Forum, CLEF 2000.Berlin: Springer-Verlag. Recherche appliquée en linguistique informatique (RALI). 2006. Corpus aligné bilingue anglais-français.Accessed at http://rali.iro.umontreal.ca/ on February 22, 2006. Philip Resnik, Mari Broman Olsen, and Mona Diab. 1999. The Bible as a Parallel Corpus: Annotating the "Book of 2000 Tongues". Computers and the Hu-manities,33: 129-153.  Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 880–887,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

880

ARe-examinationofMachineLearningApproachesforSentence-LevelMTEvaluationJoshuaS.AlbrechtandRebeccaHwaDepartmentofComputerScienceUniversityofPittsburgh{jsa8,hwa}@cs.pitt.eduAbstractRecentstudiessuggestthatmachinelearn-ingcanbeappliedtodevelopgoodauto-maticevaluationmetricsformachinetrans-latedsentences.Thispaperfurtherana-lyzesaspectsoflearningthatimpactper-formance.Wearguethatpreviouslypro-posedapproachesoftrainingaHuman-Likenessclassiﬁerisnotaswellcorrelatedwithhumanjudgmentsoftranslationqual-ity,butthatregression-basedlearningpro-ducesmorereliablemetrics.Wedemon-stratethefeasibilityofregression-basedmetricsthroughempiricalanalysisoflearn-ingcurvesandgeneralizationstudiesandshowthattheycanachievehighercorrela-tionswithhumanjudgmentsthanstandardautomaticmetrics.1IntroductionAsmachinetranslation(MT)researchadvances,theimportanceofitsevaluationalsogrows.Efﬁcientevaluationmethodologiesareneededbothforfacili-tatingthesystemdevelopmentcycleandforprovid-inganunbiasedcomparisonbetweensystems.Tothisend,anumberofautomaticevaluationmetricshavebeenproposedtoapproximatehumanjudg-mentsofMToutputquality.Althoughstudieshaveshownthemtocorrelatewithhumanjudgmentsatthedocumentlevel,theyarenotsensitiveenoughtoprovidereliableevaluationsatthesentencelevel(Blatzetal.,2003).Thissuggeststhatcurrentmet-ricsdonotfullyreﬂectthesetofcriteriathatpeopleuseinjudgingsententialtranslationquality.Arecentdirectioninthedevelopmentofmet-ricsforsentence-levelevaluationistoapplyma-chinelearningtocreateanimprovedcompositemet-ricoutoflessindicativeones(Corston-Oliveretal.,2001;KuleszaandShieber,2004).Undertheas-sumptionthatgoodmachinetranslationwillpro-duce“human-like”sentences,classiﬁersaretrainedtopredictwhetherasentenceisauthoredbyahumanorbyamachinebasedonfeaturesofthatsentence,whichmaybethesentence’sscoresfromindivid-ualautomaticevaluationmetrics.Theconﬁdenceoftheclassiﬁer’spredictioncanthenbeinterpretedasajudgmentonthetranslationqualityofthesentence.Thus,thecompositemetricisencodedintheconﬁ-dencescoresoftheclassiﬁcationlabels.Whilethelearningapproachtometricdesignof-fersthepromiseofeaseofcombiningmultiplemet-ricsandthepotentialforimprovedperformance,severalsalientquestionsshouldbeaddressedmorefully.First,islearninga“HumanLikeness”classi-ﬁerthemostsuitableapproachforframingtheMT-evaluationquestion?Analternativeisregression,inwhichthecompositemetricisexplicitlylearnedasafunctionthatapproximateshumans’quantitativejudgments,basedonasetofhumanevaluatedtrain-ingsentences.Althoughregressionhasbeencon-sideredonasmallscaleforasinglesystemascon-ﬁdenceestimation(Quirk,2004),thisapproachhasnotbeenstudiedasextensivelyduetoscalabilityandgeneralizationconcerns.Second,howdoesthedi-versityofthemodelfeaturesimpactthelearnedmet-ric?Third,howwelldolearning-basedmetricsgen-eralizebeyondtheirtrainingexamples?Inparticu-lar,howwellcanametricthatwasdevelopedbased881

ononegroupofMTsystemsevaluatethetranslationqualitiesofnewsystems?Inthispaper,wearguefortheviabilityofaregression-basedframeworkforsentence-levelMT-evaluation.Throughempiricalstudies,weﬁrstshowthathavinganaccurateHuman-Likenessclas-siﬁerdoesnotnecessarilyimplyhavingagoodMT-evaluationmetric.Second,weanalyzetheresourcerequirementforregressionmodelsfordifferentsizesoffeaturesetsthroughlearningcurves.Finally,weshowthatSVM-regressionmetricsgeneralizebetterthanSVM-classiﬁcationmetricsintheirevaluationofsystemsthataredifferentfromthoseinthetrain-ingset(bylanguagesandbyyears),andtheircorre-lationswithhumanassessmentarehigherthanstan-dardautomaticevaluationmetrics.2MTEvaluationRecentautomaticevaluationmetricstypicallyframetheevaluationproblemasacomparisontask:howsimilaristhemachine-producedoutputtoasetofhuman-producedreferencetranslationsforthesamesourcetext?However,asthenotionofsimilar-ityisitselfunderspeciﬁed,severaldifferentfami-liesofmetricshavebeendeveloped.First,simi-laritycanbeexpressedintermsofstringeditdis-tances.Inadditiontothewell-knownworderrorrate(WER),moresophisticatedmodiﬁcationshavebeenproposed(Tillmannetal.,1997;Snoveretal.,2006;Leuschetal.,2006).Second,similar-itycanbeexpressedintermsofcommonwordse-quences.SincetheintroductionofBLEU(Papinenietal.,2002)thebasicn-gramprecisionideahasbeenaugmentedinanumberofways.MetricsintheRougefamilyallowforskipn-grams(LinandOch,2004a);KauchakandBarzilay(2006)takepara-phrasingintoaccount;metricssuchasMETEOR(BanerjeeandLavie,2005)andGTM(Melamedetal.,2003)calculatebothrecallandprecision;ME-TEORisalsosimilartoSIA(LiuandGildea,2006)inthatwordclassinformationisused.Finally,re-searchershavebeguntolookforsimilaritiesatadeeperstructurallevel.Forexample,LiuandGildea(2005)developedtheSub-TreeMetric(STM)overconstituentparsetreesandtheHead-WordChainMetric(HWCM)overdependencyparsetrees.Withthiswidearrayofmetricstochoosefrom,MTdevelopersneedawaytoevaluatethem.Onepossibilityistoexaminewhethertheautomaticmet-ricranksthehumanreferencetranslationshighlywithrespecttomachinetranslations(LinandOch,2004b;Amig´oetal.,2006).Thereliabilityofametriccanalsobemoredirectlyassessedbyde-termininghowwellitcorrelateswithhumanjudg-mentsofthesamedata.Forinstance,asapartoftherecentNISTsponsoredMTEvaluation,eachtrans-latedsentencebyparticipatingsystemsisevaluatedbytwo(non-reference)humanjudgesonaﬁvepointscaleforitsadequacy(doesthetranslationretainthemeaningoftheoriginalsourcetext?)andﬂuency(doesthetranslationsoundnaturalinthetargetlan-guage?).Thesehumanassessmentdataareanin-valuableresourceformeasuringthereliabilityofau-tomaticevaluationmetrics.Inthispaper,weshowthattheyarealsoinformativeindevelopingbettermetrics.3MTEvaluationwithMachineLearningAgoodautomaticevaluationmetriccanbeseenasacomputationalmodelthatcapturesahuman’sde-cisionprocessinmakingjudgmentsabouttheade-quacyandﬂuencyoftranslationoutputs.Inferringacognitivemodelofhumanjudgmentsisachalleng-ingproblembecausetheultimatejudgmentencom-passesamultitudeofﬁne-graineddecisions,andthedecisionprocessmaydifferslightlyfrompersontoperson.Themetricscitedintheprevioussectionaimtocapturecertainaspectsofhumanjudgments.Onewaytocombinethesemetricsinauniformandprincipledmanneristhroughalearningframework.Theindividualmetricsparticipateasinputfeatures,fromwhichthelearningalgorithminfersacompos-itemetricthatisoptimizedontrainingexamples.Reframingsentence-leveltranslationevaluationasaclassiﬁcationtaskwasﬁrstproposedbyCorston-Oliveretal.(2001).Interestingly,insteadofrecastingtheclassiﬁcationproblemasa“Hu-manAcceptability”test(distinguishinggoodtrans-lationsoutputsfrombadone),theychosetodevelopaHuman-Likenessclassiﬁer(distinguishingout-putsseemhuman-producedfrommachine-producedones)toavoidthenecessityofobtainingmanu-allylabeledtrainingexamples.Later,KuleszaandShieber(2004)notedthatifaclassiﬁerprovidesa882

conﬁdencescoreforitsoutput,thatvaluecanbeinterpretedasaquantitativeestimateoftheinputinstance’stranslationquality.Inparticular,theytrainedanSVMclassiﬁerthatmakesitsdecisionsbasedonasetofinputfeaturescomputedfromthesentencetobeevaluated;thedistancebetweeninputfeaturevectorandtheseparatinghyperplanethenservesastheevaluationscore.Theunderlyingas-sumptionforbothisthatimprovingtheaccuracyoftheclassiﬁerontheHuman-LikenesstestwillalsoimprovetheimplicitMTevaluationmetric.Amoredirectalternativetotheclassiﬁcationap-proachistolearnviaregressionandexplicitlyop-timizeforafunction(i.e.MTevaluationmetric)thatapproximateshumanjudgmentsintrainingex-amples.KuleszaandShieber(2004)raisedtwomainobjectionsagainstregressionforMTevalua-tions.Oneisthatregressionrequiresalargesetoflabeledtrainingexamples.Anotheristhatregressionmaynotgeneralizewellovertime,andre-trainingmaybecomenecessary,whichwouldrequirecol-lectingadditionalhumanassessmentdata.Whilethesearelegitimateconcerns,weshowthroughem-piricalstudies(inSection4.2)thattheadditionalre-sourcerequirementisnotimpracticallyhigh,andthataregression-basedmetrichashighercorrela-tionswithhumanjudgmentsandgeneralizesbetterthanametricderivedfromaHuman-Likenessclas-siﬁer.3.1RelationshipbetweenClassiﬁcationandRegressionClassiﬁcationandregressionarebothprocessesoffunctionapproximation;theyusetrainingexamplesassampleinstancestolearnthemappingfromin-putstothedesiredoutputs.Themajordifferencebe-tweenclassiﬁcationandregressionisthatthefunc-tionlearnedbyaclassiﬁerisasetofdecisionbound-ariesbywhichtoclassifyitsinputs;thusitsoutputsarediscrete.Incontrast,aregressionmodellearnsacontinuousfunctionthatdirectlymapsaninputtoacontinuousvalue.AnMTevaluationmetricisinherentlyacontinuousfunction.Castingthetaskasa2-wayclassiﬁcationmaybetoocoarse-grained.TheHuman-Likenessformulationoftheproblemin-troducesanotherlayerofapproximationbyassum-ingequivalencebetween“LikeHuman-Produced”and“Well-formed”sentences.InSection4.1,weshowempiricallythathighaccuracyintheHuman-LikenesstestdoesnotnecessarilyentailgoodMTevaluationjudgments.3.2FeatureRepresentationToascertaintheresourcerequirementsfordifferentmodelsizes,weconsideredtwofeaturemodels.ThesmalleroneusesthesameninefeaturesasKuleszaandShieber,whichwerederivedfromBLEUandWER.Thefullmodelconsistsof53features:someareadaptedfromrecentlydevelopedmetrics;othersarenewfeaturesofourown.Theyfallintothefol-lowingmajorcategories1:String-basedmetricsoverreferencesThesein-cludethenineKuleszaandShieberfeaturesaswellasprecision,recall,andfragmentation,ascalcu-latedinMETEOR;ROUGE-inspiredfeaturesthatarenon-consecutivebigramswithagapsizeofm,where1≤m≤5(skip-m-bigram),andROUGE-L(longestcommonsubsequence).Syntax-basedmetricsoverreferencesWeun-rolledHWCMintotheirindividualchainsoflengthc(where2≤c≤4);wemodiﬁedSTMsothatitiscomputedoverunlexicalizedconstituentparsetreesaswellasoverdependencyparsetrees.String-basedmetricsovercorpusFeaturesinthiscategoryaresimilartothoseinString-basedmetricoverreferenceexceptthatalargeEnglishcor-pusisusedas“reference”instead.Syntax-basedmetricsovercorpusAlargede-pendencytreebankisusedasthe“reference”insteadofparsedhumantranslations.Inadditiontoadap-tationsoftheSyntax-basedmetricsoverreferences,wehavealsocreatedfeaturestoverifytheargumentstructuresforcertainsyntacticcategories.4EmpiricalStudiesInthesestudies,thelearningmodelsusedforbothclassiﬁcationandregressionaresupportvectorma-chines(SVM)withGaussiankernels.AllmodelsaretrainedwithSVM-Light(Joachims,1999).OurprimaryexperimentaldatasetisfromNIST’s20031Asfeatureengineeringisnottheprimaryfocusofthispa-per,thefeaturesarebrieﬂydescribedhere,butimplementa-tionaldetailswillbemadeavailableinatechnicalreport.883

ChineseMTEvaluations,inwhichtheﬂuencyandadequacyof919sentencesproducedbysixMTsys-temsarescoredbytwohumanjudgesona5-pointscale2.Becausethejudgesevaluatesentencesac-cordingtotheirindividualstandards,theresultingscoresmayexhibitabiaseddistribution.Wenormal-izehumanjudges’scoresfollowingtheprocessde-scribedbyBlatzetal.(2003).Theoverallhumanas-sessmentscoreforatranslationoutputistheaverageofthesumoftwojudges’normalizedﬂuencyandadequacyscores.Thefulldataset(6×919=5514instances)issplitintosetsoftraining,heldoutandtestdata.Heldoutdataisusedforparametertuning(i.e.,theslackvariableandthewidthoftheGaus-sian).Whentrainingclassiﬁers,assessmentscoresarenotused,andthetrainingsetisaugmentedwithallavailablehumanreferencetranslationsentences(4×919=3676instances)toserveaspositiveex-amples.Tojudgethequalityofametric,wecomputeSpearmanrank-correlationcoefﬁcient,whichisarealnumberrangingfrom-1(indicatingperfectneg-ativecorrelations)to+1(indicatingperfectposi-tivecorrelations),betweenthemetric’sscoresandtheaveragedhumanassessmentsontestsentences.WeuseSpearmaninsteadofPearsonbecauseitisadistribution-freetest.Toevaluatetherela-tivereliabilityofdifferentmetrics,weuseboot-strappingre-samplingandpairedt-testtodeterminewhetherthedifferencebetweenthemetrics’correla-tionscoreshasstatisticalsigniﬁcance(at99.8%con-ﬁdencelevel)(Koehn,2004).Eachreportedcorrela-tionrateistheaverageof1000trials;eachtrialcon-sistsofnsampledpoints,wherenisthesizeofthetestset.Unlessexplicitlynoted,thequalitativedif-ferencesbetweenmetricswereportarestatisticallysigniﬁcant.Asabaselinecomparison,wereportthecorrelationratesofthreestandardautomaticmetrics:BLEU,METEOR,whichincorporatesrecallandstemming,andHWCM,whichusessyntax.BLEUissmoothedtobemoreappropriateforsentence-levelevaluation(LinandOch,2004b),andthebi-gramversionsofBLEUandHWCMarereportedbecausetheyhavehighercorrelationsthanwhenlongern-gramsareincluded.Thisphenomenonhas2ThiscorpusisavailablefromtheLinguisticDataConsor-tiumasMultipleTranslationChinesePart4. 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 45 50 55 60 65 70 75 80 85Correlation Coefficient with Human Judgement (R)Human-Likeness Classifier Accuracy (%)Figure1:Thisscatterplotcomparesclassiﬁers’ac-curacywiththeircorrespondingmetrics’correla-tionswithhumanassessmentsbeenpreviouslyobservedbyLiuandGildea(2005).4.1RelationshipbetweenClassiﬁcationAccuracyandQualityofEvaluationMetricAconcerninusingametricderivedfromaHuman-Likenessclassiﬁeriswhetheritwouldbepredic-tiveforMTevaluation.KuleszaandShieber(2004)triedtodemonstrateapositivecorrelationbetweentheHuman-LikenessclassiﬁcationtaskandtheMTevaluationtaskempirically.Theyplottedtheclas-siﬁcationaccuracyandevaluationreliabilityforanumberofclassiﬁers,whichweregeneratedasapartofagreedysearchforkernelparametersandfoundsomelinearcorrelationbetweenthetwo.Thisproofofconceptisalittlemisleading,however,be-causethepopulationofthesampledclassiﬁerswasbiasedtowardthosefromthesameneighborhoodasthelocaloptimalclassiﬁer(soaccuracyandcorre-lationmayonlyexhibitlinearrelationshiplocally).Here,weperformasimilarstudyexceptthatwesampledthekernelparametermoreuniformly(onalogscale).AsFigure1conﬁrms,havinganac-curateHuman-Likenessclassiﬁerdoesnotnecessar-ilyentailhavingagoodMTevaluationmetric.Al-thoughthetwotasksdoseemtobepositivelyre-lated,andinthelimittheremaybeasystemthatisgoodatbothtasks,onemayimproveclassiﬁcationwithoutimprovingMTevaluation.Forthissetofheldoutdata,atthenear80%accuracyrange,ade-rivedmetricmighthaveanMTevaluationcorrela-tioncoefﬁcientanywherebetween0.25(onparwith884

unsmoothedBLEU,whichisknowntobeunsuitableforsentence-levelevaluation)and0.35(competitivewithstandardmetrics).4.2LearningCurvesToinvestigatethefeasibilityoftrainingregressionmodelsfromassessmentdatathatarecurrentlyavailable,weconsiderbothasmallandalargeregressionmodel.Thesmallermodelconsistsofninefeatures(sameasthesetusedbyKuleszaandShieber);theotherusesthefullsetof53featuresasdescribedinSection3.2.ThereliabilityofthetrainedmetricsarecomparedwiththosedevelopedfromHuman-Likenessclassiﬁers.Wefollowasim-ilartrainingandtestingmethodologyaspreviousstudies:weheldout1/6oftheassessmentdatasetforSVMparametertuning;ﬁve-foldcrossvalidationisperformedwiththeremainingsentences.Althoughthemetricsareevaluatedonunseentestsentences,thesentencesareproducedbythesameMTsystemsthatproducedthetrainingsentences.Inlaterexper-iments,weinvestigategeneralizingtomoredistantMTsystems.Figure2(a)showsthelearningcurvesforthetworegressionmodels.Asthegraphindicates,evenwithalimitedamountofhumanassessmentdata,regressionmodelscanbetrainedtobecomparabletostandardmetrics(representedbyMETEORinthegraph).Thesmallfeaturemodelisclosetoconver-genceafter1000trainingexamples3.Themodelwithamorecomplexfeaturesetdoesrequiremoretrainingdata,butitscorrelationbegantoovertakeMETEORafter2000trainingexamples.Thisstudysuggeststhatthestart-upcostofbuildingevenamoderatelycomplexregressionmodelisnotimpos-siblyhigh.AlthoughwecannotdirectlycomparethelearningcurvesoftheHuman-Likenessclassiﬁerstothoseoftheregressionmodels(sincetheclassiﬁer’strainingexamplesareautomaticallylabeled),trainingexam-plesforclassiﬁersarenotentirelyfree:humanref-erencetranslationsstillmustbedevelopedforthesourcesentences.Figure2(c)showsthelearningcurvesfortrainingHuman-Likenessclassiﬁers(intermsofimprovingaclassiﬁer’saccuracy)usingthesametwofeaturesets,andFigure2(b)showsthe3Thetotalnumberoflabeledexamplesrequirediscloserto2000,sincetheheldoutsetuses919labeledexamples.correlationsofthemetricsderivedfromthecorre-spondingclassiﬁers.Thepairofgraphsshow,es-peciallyinthecaseofthelargerfeatureset,thatalargeimprovementinclassiﬁcationaccuracydoesnotbringproportionalimprovementinitscorre-spondingmetrics’scorrelation;withanaccuracyofnear90%,itscorrelationcoefﬁcientis0.362,wellbelowMETEOR.ThisexperimentfurtherconﬁrmsthatjudgingHuman-LikenessandjudgingHuman-Acceptabilityarenottightlycoupled.Earlier,wehaveshowninFigure1thatdifferentSVMparameterizationsmayresultinclassiﬁerswiththesameaccuracyratebutdifferentcorrelationsrates.Asawaytoincorpo-ratesomeassessmentinformationintoclassiﬁcationtraining,wemodifytheparametertuningprocesssothatSVMparametersarechosentooptimizeforas-sessmentcorrelationsintheheldoutdata.Byincur-ringthissmallamountofhumanassesseddata,thisparametersearchimprovestheclassiﬁer’scorrela-tions:themetricusingthesmallerfeaturesetin-creasedfrom0.423to0.431,andthatofthelargersetincreasedfrom0.361to0.422.4.3GeneralizationWeconductedtwogeneralizationstudies.Theﬁrstinvestigateshowwellthetrainedmetricsevaluatesystemsfromotheryearsandsystemsdevelopedforadifferentsourcelanguage.Thesecondstudydelvesmoredeeplyintohowvariationsinthetrain-ingexamplesaffectalearnedmetric’sabilitytogen-eralizetodistantsystems.Thelearningmodelsforbothexperimentsusethefullfeatureset.Cross-YearGeneralizationTotesthowwellthelearning-basedmetricsgeneralizetosystemsfromdifferentyears,wetrainedbotharegression-basedmetric(R03)andaclassiﬁer-basedmetric(C03)withtheentireNIST2003Chinesedataset(using20%ofthedataasheldout4).Allmetricsarethenappliedtothreenewdatasets:NIST2002ChineseMTEvaluation(3systems,2634sentencestotal),NIST2003ArabicMTEvaluation(2systems,1326sentencestotal),andNIST2004ChineseMTEvalu-ation(10systems,4470sentencestotal).Theresults4Here,too,weallowedtheclassiﬁer’sparameterstobetunedforcorrelationwithhumanassessmentontheheldoutdataratherthanaccuracy.885

(a)(b)(c)Figure2:Learningcurves:(a)correlationswithhumanassessmentusingregressionmodels;(b)correlationswithhumanassessmentusingclassiﬁers;(c)classiﬁeraccuracyondeterminingHuman-Likeness.DatasetR03C03BLEUMET.HWCM2002Ara0.4660.3840.4230.4310.4242002Chn0.3090.2500.2690.2900.2602004Chn0.6020.5660.5880.5630.546Table1:Correlationsforcross-yeargeneralization.Learning-basedmetricsaredevelopedfromNIST2003Chinesedata.Allmetricsaretestedondatasetsfrom2003Arabic,2002Chineseand2004Chinese.aresummarizedinTable1.WeseethatR03con-sistentlyhasabettercorrelationratethantheothermetrics.Atﬁrst,itmayseemasifthedifferencebetweenR03andBLEUisnotaspronouncedforthe2004dataset,callingtoquestionwhetheralearnedmet-ricmightbecomequicklyout-dated,wearguethatthisisnotthecase.The2004datasethasmanymoreparticipatingsystems,andtheyspanawiderrangeofqualities.Thus,itiseasiertoachieveahighrankcorrelationonthisdatasetthanpreviousyearsbecausemostmetricscanqualitativelydiscernthatsentencesfromoneMTsystemarebetterthanthosefromanother.Inthenextexperiment,weex-aminetheperformanceofR03withrespecttoeachMTsysteminthe2004datasetandshowthatitscor-relationrateishigherforbetterMTsystems.RelationshipbetweenTrainingExamplesandGeneralizationTable2showstheresultofagen-eralizationstudysimilartobefore,exceptthatcor-relationsareperformedoneachsystem.Therowsorderthetestsystemsbytheirtranslationquali-tiesfromthebestperformingsystem(2004-Chn1,whoseaveragehumanassessmentscoreis0.655outof1.0)totheworst(2004-Chn10,whosescoreis0.255).Inadditiontotheregressionmetricfromthepreviousexperiment(R03-all),weconsidertwomoreregressionmetricstrainedfromsubsetsofthe2003dataset:R03-Bottom5istrainedfromthesub-setthatexcludesthebest2003MTsystem,andR03-Top5istrainedfromthesubsetthatexcludestheworst2003MTsystem.Weﬁrstobservethatonapertest-systembasis,theregression-basedmetricsgenerallyhavebettercorrelationratesthanBLEU,andthatthegapisaswideaswhatwehaveobservedintheearliercross-yearsstudies.Theoneexceptioniswhenevaluating2004-Chn8.Noneofthemetricsseemstocorrelateverywellwithhumanjudgesonthissystem.Be-causetheregression-basedmetricusestheseindivid-ualmetricsasfeatures,itscorrelationalsosuffers.Duringregressiontraining,themetricisopti-mizedtominimizethedifferencebetweenitspre-dictionandthehumanassessmentsofthetrainingdata.Iftheinputfeaturevectorofatestinstanceisinaverydistantspacefromtrainingexamples,thechanceforerrorishigher.Asseenfromtheresults,thelearnedmetricstypicallyperformbetterwhenthetrainingexamplesincludesentencesfromhigher-qualitysystems.Consider,forexample,thedifferencesbetweenR03-allandR03-Top5versusthedifferencesbetweenR03-allandR03-Bottom5.BothR03-Top5andR03-Bottom5differfromR03-allbyonesubsetoftrainingexamples.SinceR03-all’scorrelationratesaregenerallyclosertoR03-Top5thantoR03-Bottom5,weseethathavingseenextratrainingexamplesfromabadsystemisnotasharmfulashavingnotseentrainingexamplesfromagoodsystem.Thisisexpected,sincetherearemanywaystocreatebadtranslations,soseeingapartic-886

R03-allR03-Bottom5R03-Top5BLEUMETEORHWCM2004-Chn10.4950.4600.5180.4560.4570.4442004-Chn20.3980.3300.4400.3520.3470.3442004-Chn30.4250.3890.4590.3690.4020.3692004-Chn40.4320.3920.4340.4000.4000.3622004-Chn50.4520.4410.4430.3700.4260.3262004-Chn60.4050.3920.4060.3900.3570.3802004-Chn70.4430.4320.4480.3900.4080.3922004-Chn80.2370.2560.2560.2650.2590.1792004-Chn90.5810.5690.5910.5270.5370.5352004-Chn100.3140.3130.3540.3210.3030.3582004-all0.6020.5670.6170.5880.5630.546Table2:Metriccorrelationswithineachsystem.Thecolumnsspecifywhichmetricisused.TherowsspecifywhichMTsystemisunderevaluation;theyareorderedbyhuman-judgedsystemquality,frombesttoworst.ForeachevaluatedMTsystem(row),thehighestcoefﬁcientinboldfont,andthosethatarestatisticallycomparabletothehighestareshowninitalics.ulartypeofbadtranslationsfromonesystemmaynotbeveryinformative.Incontrast,theneighbor-hoodofgoodtranslationsismuchsmaller,andiswhereallthesystemsareaimingfor;thus,assess-mentsofsentencesfromagoodsystemcanbemuchmoreinformative.4.4DiscussionExperimentalresultsconﬁrmthatlearningfromtrainingexamplesthathavebeendoublyapprox-imated(classlabelsinsteadofordinals,human-likenessinsteadofhuman-acceptability)doesnega-tivelyimpacttheperformanceofthederivedmetrics.Inparticular,weshowedthattheydonotgeneralizeaswelltonewdataasmetricstrainedfromdirectregression.Weseetwolingeringpotentialobjectionstowarddevelopingmetricswithregression-learning.Oneistheconcernthatasystemunderevaluationmighttrytoexplicitly“gamethemetric5.”Thisisacon-cernsharedbyallautomaticevaluationmetrics,andpotentialproblemsinstand-alonemetricshavebeenanalyzed(Callison-Burchetal.,2006).Inalearningframework,potentialpitfallsforindividualmetricsareamelioratedthroughacombinationofevidences.Thatsaid,itisstillprudenttodefendagainstthepo-tentialofasystemgamingasubsetofthefeatures.Forexample,ourﬂuency-predictorfeaturesarenotstrongindicatorsoftranslationqualitiesbythem-selves.Wewanttoavoidtrainingametricthatas-5Or,inalessadversarialsetting,asystemmaybeperform-ingminimumerror-ratetraining(Och,2003)signsahigherthandeservingscoretoasentencethatjusthappenstohavemanyn-grammatchesagainstthetarget-languagereferencecorpus.Thiscanbeachievedbysupplementingthecurrentsetofhu-manassessedtrainingexampleswithautomaticallyassessedtrainingexamples,similartothelabelingprocessusedintheHuman-Likenessclassiﬁcationframework.Forinstance,asnegativetrainingex-amples,wecanincorporateﬂuentsentencesthatarenotadequatetranslationsandassignthemlowover-allassessmentscores.Asecond,relatedconcernisthatbecausethemet-ricistrainedonexamplesfromcurrentsystemsus-ingcurrentlyrelevantfeatures,eventhoughitgener-alizeswellinthenearterm,itmaynotcontinuetobeagoodpredictorinthedistantfuture.Whilepe-riodicretrainingmaybenecessary,weseevalueintheﬂexibilityofthelearningframework,whichal-lowsfornewfeaturestobeadded.Moreover,adap-tivelearningmethodsmaybeapplicableifasmallsampleofoutputsofsomerepresentativetranslationsystemsismanuallyassessedperiodically.5ConclusionHumanjudgmentofsentence-leveltranslationqual-itydependsonmanycriteria.Machinelearningaf-fordsauniﬁedframeworktocomposethesecrite-riaintoasinglemetric.Inthispaper,wehavedemonstratedtheviabilityofaregressionapproachtolearningthecompositemetric.Ourexperimentalresultsshowthatbytrainingfromsomehumanas-887

sessments,regressionmethodsresultinmetricsthathavebettercorrelationswithhumanjudgmentsevenasthedistributionofthetestedpopulationchanges.AcknowledgmentsThisworkhasbeensupportedbyNSFGrantsIIS-0612791andIIS-0710695.WewouldliketothankReginaBarzilay,RicCrabbe,DanGildea,AlexKulesza,AlonLavie,andMatthewStoneaswellastheanonymousreviewersforhelpfulcommentsandsuggestions.WearealsogratefultoNISTformakingtheirassessmentdataavailabletous.ReferencesEnriqueAmig´o,Jes´usGim´enez,JulioGonzalo,andLlu´ısM`arquez.2006.MTevaluation:Human-likevs.humanac-ceptable.InProceedingsoftheCOLING/ACL2006MainConferencePosterSessions,Sydney,Australia,July.SatanjeevBanerjeeandAlonLavie.2005.Meteor:Anauto-maticmetricforMTevaluationwithimprovedcorrelationwithhumanjudgments.InACL2005WorkshoponIntrinsicandExtrinsicEvaluationMeasuresforMachineTranslationand/orSummarization,June.JohnBlatz,ErinFitzgerald,GeorgeFoster,SimonaGandrabur,CyrilGoutte,AlexKulesza,AlbertoSanchis,andNicolaUefﬁng.2003.Conﬁdenceestimationformachinetrans-lation.TechnicalReportNaturalLanguageEngineeringWorkshopFinalReport,JohnsHopkinsUniversity.ChristopherCallison-Burch,MilesOsborne,andPhilippKoehn.2006.Re-evaluatingtheroleofBLEUinmachinetranslationresearch.InTheProceedingsoftheThirteenthConferenceoftheEuropeanChapteroftheAssociationforComputationalLinguistics.SimonCorston-Oliver,MichaelGamon,andChrisBrockett.2001.Amachinelearningapproachtotheautomaticeval-uationofmachinetranslation.InProceedingsofthe39thAnnualMeetingoftheAssociationforComputationalLin-guistics,July.ThorstenJoachims.1999.Makinglarge-scaleSVMlearningpractical.InBernhardSch¨oelkopf,ChristopherBurges,andAlexanderSmola,editors,AdvancesinKernelMethods-SupportVectorLearning.MITPress.DavidKauchakandReginaBarzilay.2006.Paraphrasingforautomaticevaluation.InProceedingsoftheHumanLan-guageTechnologyConferenceoftheNAACL,MainConfer-ence,NewYorkCity,USA,June.PhilippKoehn.2004.Statisticalsigniﬁcancetestsformachinetranslationevaluation.InProceedingsofthe2004Confer-enceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP-04).AlexKuleszaandStuartM.Shieber.2004.Alearningap-proachtoimprovingsentence-levelMTevaluation.InPro-ceedingsofthe10thInternationalConferenceonTheoreticalandMethodologicalIssuesinMachineTranslation(TMI),Baltimore,MD,October.GregorLeusch,NicolaUefﬁng,andHermannNey.2006.CDER:EfﬁcientMTevaluationusingblockmovements.InTheProceedingsoftheThirteenthConferenceoftheEuro-peanChapteroftheAssociationforComputationalLinguis-tics.Chin-YewLinandFranzJosefOch.2004a.Automaticevalu-ationofmachinetranslationqualityusinglongestcommonsubsequenceandskip-bigramstatistics.InProceedingsofthe42ndAnnualMeetingoftheAssociationforComputa-tionalLinguistics,July.Chin-YewLinandFranzJosefOch.2004b.Orange:amethodforevaluatingautomaticevaluationmetricsforma-chinetranslation.InProceedingsofthe20thInternationalConferenceonComputationalLinguistics(COLING2004),August.DingLiuandDanielGildea.2005.Syntacticfeaturesforevaluationofmachinetranslation.InACL2005WorkshoponIntrinsicandExtrinsicEvaluationMeasuresforMachineTranslationand/orSummarization,June.DingLiuandDanielGildea.2006.Stochasticiterativealign-mentformachinetranslationevaluation.InProceedingsoftheJointConferenceoftheInternationalConferenceonComputationalLinguisticsandtheAssociationforCom-putationalLinguistics(COLING-ACL’2006)PosterSession,July.I.DanMelamed,RyanGreen,andJosephTurian.2003.Preci-sionandrecallofmachinetranslation.InInProceedingsoftheHLT-NAACL2003:ShortPapers,pages61–63,Edmon-ton,Alberta.FranzJosefOch.2003.Minimumerrorratetrainingforstatis-ticalmachinetranslation.InProceedingsofthe41stAnnualMeetingoftheAssociationforComputationalLinguistics.KishorePapineni,SalimRoukos,ToddWard,andWei-JingZhu.2002.Bleu:amethodforautomaticevaluationofma-chinetranslation.InProceedingsofthe40thAnnualMeetingoftheAssociationforComputationalLinguistics,Philadel-phia,PA.ChristopherQuirk.2004.Trainingasentence-levelmachinetranslationconﬁdencemeasure.InProceedingsofLREC2004.MatthewSnover,BonnieDorr,RichardSchwartz,LinneaMic-ciulla,andJohnMakhoul.2006.Astudyoftranslationeditratewithtargetedhumanannotation.InProceedingsofthe8thConferenceoftheAssociationforMachineTranslationintheAmericas(AMTA-2006).ChristophTillmann,StephanVogel,HermannNey,HassanSawaf,andAlexZubiaga.1997.AcceleratedDP-basedsearchforstatisticaltranslation.InProceedingsofthe5thEuropeanConferenceonSpeechCommunicationandTech-nology(EuroSpeech’97).Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 888–895,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

888

AutomaticAcquisitionofRankedQualiaStructuresfromtheWeb1PhilippCimianoInst.AIFB,UniversityofKarlsruheEnglerstr.11,D-76131Karlsruhecimiano@aifb.uni-karlsruhe.deJohannaWenderothInst.AIFB,UniversityofKarlsruheEnglerstr.11,D-76131Karlsruhejowenderoth@googlemail.comAbstractThispaperpresentsanapproachfortheau-tomaticacquisitionofqualiastructuresfornounsfromtheWebandthusopensthepos-sibilitytoexploretheimpactofqualiastruc-turesfornaturallanguageprocessingatalargerscale.Theapproachbuildsonear-lierworkbasedontheideaofmatchingspe-ciﬁclexico-syntacticpatternsconveyingacertainsemanticrelationontheWorldWideWebusingstandardsearchengines.Inourapproach,thequaliaelementsareactuallyrankedforeachqualiarolewithrespecttosomemeasure.Thespeciﬁccontributionofthepaperliesintheextensiveanalysisandquantitativecomparisonofdifferentmea-suresforrankingthequaliaelements.Fur-ther,fortheﬁrsttime,wepresentaquan-titativeevaluationofsuchanapproachforlearningqualiastructureswithrespecttoahandcraftedgoldstandard.1IntroductionQualiastructureshavebeenoriginallyintroducedby(Pustejovsky,1991)andareusedforavarietyofpurposesinnaturallanguageprocessing(NLP),suchasfortheanalysisofcompounds(JohnstonandBusa,1996)aswellasco-compositionandcoercion(Pustejovsky,1991),butalsoforbridgingreferenceresolution(Bosetal.,1995).Further,ithasalso1TheworkreportedinthispaperhasbeensupportedbytheX-Mediaproject,fundedbytheEuropeanCommissionunderECgrantnumberIST-FP6-026978aswellbytheSmartWebproject,fundedbytheGermanMinistryofResearch.Thankstoallourcolleaguesforhelpingtoevaluatetheapproach.beenarguedthatqualiastructuresandlexicalseman-ticrelationsingeneralhaveapplicationsininforma-tionretrieval(Voorhees,1994;Pustejovskyetal.,1993).Onemajorbottleneckhoweveristhatcur-rentlyqualiastructuresneedtobecreatedbyhand,whichisprobablyalsothereasonwhythereareal-mostnopracticalNLPsystemsusingqualiastruc-tures,butalotofsystemsrelyingonpubliclyavail-ableresourcessuchasWordNet(Fellbaum,1998)orFrameNet(Bakeretal.,1998)assourceoflex-ical/worldknowledge.TheworkdescribedinthispaperaddressesthisissueandpresentsanapproachtoautomaticallylearningqualiastructuresfornounsfromtheWeb.TheapproachisinspiredinrecentworkonusingtheWebtoidentifyinstancesofare-lationofinterestsuchasin(Markertetal.,2003)and(Etzionietal.,2005).Theseapproachesrelyonacombinationoftheusageoflexico-syntacticpattensconveyingacertainrelationofinterestasdescribedin(Hearst,1992)withtheideaofusingthewebasabigcorpus(cf.(KilgariffandGrefenstette,2003)).Ourapproachdirectlybuildsonourpreviouswork(CimianoandWenderoth,2005)anadherestotheprincipledideaoflearningrankedqualiastructures.Infact,arankingofqualiaelementsisusefulasithelpstodetermineacut-offpointandasareliabil-ityindicatorforlexicographersinspectingthequaliastructures.Incontrasttoourpreviouswork,thefo-cusofthispaperliesinanalyzingdifferentmeasuresforrankingthequaliaelementsintheautomaticallyacquiredqualiastructures.Wealsointroducead-ditionalpatternsfortheagentiverolewhichmakeuseofwildcardoperators.Further,wepresentagoldstandardforqualiastructurescreatedforthe30wordsusedintheevaluationofYamadaandBald-win(YamadaandBaldwin,2004).Theevaluation889

presentedhereisthusmuchmoreextensivethanourpreviousone(CimianoandWenderoth,2005),inwhichonly7wordswereused.Wepresentaquanti-tativeevaluationofourapproachandacomparisonofthedifferentrankingmeasureswithrespecttothisgoldstandard.Finally,wealsoprovideanevaluationinwhichtestpersonswereaskedtoinspectandratethelearnedqualiastructuresaposteriori.Thepaperisstructuredasfollows:Section2introducesqualiastructuresforthesakeofcompletenessanddescribesthespeciﬁcstructuresweaimtoacquire.Section3describesourapproachindetail,whileSection4discussestherankingmeasuresused.Section5thenpresentsthegoldstandardaswellasthequalitativeevaluationofourapproach.Beforeconcluding,wediscussrelatedworkinSection6.2QualiaStructuresIntheGenerativeLexicon(GL)framework(Puste-jovsky,1991),PustejovskyreusedAristotle’sbasicfactors(i.e.thematerial,agentive,formalandﬁnalcauses)forthedescriptionofthemeaningoflexi-calelements.Infact,heintroducedsocalledqualiastructuresbywhichthemeaningofalexicalele-mentisdescribedintermsoffourroles:Constitutive(describingphysicalpropertiesofanobject,i.e.itsweight,materialaswellaspartsandcomponents),Agentive(describingfactorsinvolvedinthebringingaboutofanobject,i.e.itscreatororthecausalchainleadingtoitscreation),Formal(describingproper-tieswhichdistinguishanobjectwithinalargerdo-main,i.e.orientation,magnitude,shapeanddimen-sionality),andTelic(describingthepurposeorfunc-tionofanobject).Mostofthequaliastructuresusedin(Pustejovsky,1991)howeverseemtohaveamorerestrictedinter-pretation.Infact,inmostexamplestheConstitutiveroleseemstodescribethepartsorcomponentsofanobject,whiletheAgentiveroleistypicallydescribedbyaverbdenotinganactionwhichtypicallybringstheobjectinquestionintoexistence.TheFormalrolenormallyconsistsintypinginformationabouttheobject,i.e.itshypernym.Inourapproach,weaimtoacquirequaliastructuresaccordingtothisre-strictedinterpretation.3AutomaticallyAcquiringQualiaStructuresOurapproachtolearningqualiastructuresfromtheWebisontheonehandbasedontheassumptionthatinstancesofacertainsemanticrelationcanbeacquiredbymatchingcertainlexico-syntacticpat-ternsmoreorlessreliablyconveyingtherelationofinterestinlinewiththeseminalworkofHearst(Hearst,1992),whodeﬁnedpatternsconveyinghy-ponym/hypernymrelations.However,itiswellknownthatHearst-stylepatternsoccurrarely,suchthatmatchingthesepatternsontheWebinordertoalleviatetheproblemofdatasparsenessseemsapromisingsolution.Infact,inourcasewearenotonlylookingforthehypernymrelation(comparabletotheFormal-role)butforsimilarpatternsconvey-ingaConstitutive,TelicorAgentiverelation.Ourapproachconsistsof5phases;foreachqualiaterm(thewordwewanttoﬁndthequaliastructurefor)we:1.generateforeachqualiaroleasetofsocalledclues,i.e.searchenginequeriesindicatingtherelationofinterest,2.downloadthesnippets(abstracts)ofthe50ﬁrstwebsearchengineresultsmatchingthegeneratedclues,3.part-of-speech-tagthedownloadedsnippets,4.matchpatternsintheformofregularexpressionsconveyingthequaliaroleofinterest,and5.weightandrankthereturnedqualiaelementsac-cordingtosomemeasure.Thepatternsinourpatternlibraryareactuallytuples(p,c)wherepisaregularexpressionde-ﬁnedoverpart-of-speechtagsandcafunctionc:string→stringcalledtheclue.Givenanomi-nalnandacluec,thequeryc(n)issenttothewebsearchengineandtheabstractsoftheﬁrstmdocu-mentsmatchingthisqueryaredownloaded.Thenthesnippetsareprocessedtoﬁndmatchesofthepatternp.Forexample,giventhecluef(x)=“suchasp(x)00andthequaliatermcomputerwewoulddownloadmabstractsmatchingthequeryf(computer),i.e.”suchascomputers”.Herebyp(x)isafunctionreturningthepluralformofx.Weim-plementedthisfunctionasalookupinalexiconinwhichpluralnounsaremappedtotheirbaseform.Withtheuseofsuchclues,wethusdownloadanum-890

berofsnippetsreturnedbythewebsearchengineinwhichacorrespondingregularexpressionwillprob-ablybematched,thusrestrictingthelinguisticanal-ysistoafewpromisingpages.Thedownloadedab-stractsarethenpart-of-speechtaggedusingQTag(TuﬁsandMason,1998).Thenwematchthecorre-spondingpatternpinthedownloadedsnippetsthusyieldingcandidatequaliaelementsasoutput.Thequaliaelementsarethenrankedaccordingtosomemeasure(compareSection4),resultinginwhatwecallRankedQualiaStructures(RQSs).ThecluesandpatternsusedforthedifferentrolescanbefoundinTables1-4.Inthespeciﬁcationoftheclues,thefunctiona(x)returnstheappropriateindeﬁnitearti-cle–‘a’or‘an’–ornoarticleatallforthenounx.Theuseofanindeﬁnitearticleornoarticleatallac-countsforthedistinctionbetweencountablenouns(e.g.suchasknife)andmassnouns(e.g.water).Thechoicebetweenusingthearticles’a’,’an’ornoarticleatallisdeterminedbyissuingappropriatequeriestothewebsearchengineandchoosingthearticleleadingtothehighestnumberofresults.Thecorrespondingpatternsarethenmatchedinthe50snippetsreturnedbythesearchengineforeachclue,thusleadingtoupto50potentialqualiaelementsperclueandpattern2.Thepatternsareactuallydeﬁnedoverpart-of-speechtags.WeindicatePOS-tagsinsquarebrackets.However,forthesakeofsimplic-ity,welargelyomitthePOS-tagsforthelexicalele-mentsinthepatternsdescribedinTables1-4.Notethatweusetraditionalregularexpressionoperatorssuchas∗(sequence),+(sequencewithatleastoneelement)|(alternative)and?(option).Ingeneral,wedeﬁneanounphrase(NP)bythefollowingreg-ularexpression:NP:=[DT]?([JJ])+?[NN(S?)])+3,wheretheheadistheunderlinedexpression,whichislemmatizedandconsideredasacandidatequaliaelement.Forallthepatternsdescribedinthissec-tion,theunderlinedpartcorrespondstotheextractedqualiaelement.Inthepatternsfortheformalrole(compareTable1),NPQTisanounphrasewiththequaliatermashead,whereasNPFisanounphrasewiththepotentialqualiaelementashead.Fortheconstitutiverolepatterns,weuseanounphrasevari-2Fortheconstitutiverolethesecanbeevenmoreduetothefactthatweconsiderenumerations.3ThoughQtagusesanotherpart-of-speechtagset,werelyonthewell-knownPennTreebanktagsetforpresentationpurposes.CluePatternSingular“a(x)xisakindof”NPQTisakindofNPF“a(x)xis”NPQTisakindofNPF“a(x)xandother”NPQT(,)?andotherNPF“a(x)xorother”NPQT(,)?orotherNPFPlural“suchasp(x)”NPFsuchasNPQT“p(x)andother”NPQT(,)?andotherNPF“p(x)orother”NPQT(,)?orotherNPF“especiallyp(x)”NPF(,)?especiallyNPQT“includingp(x)”NPF(,)?includingNPQTTable1:CluesandPatternsfortheFormalroleantNP’deﬁnedbytheregularexpressionNP’:=(NPof[IN])?NP(,NP)*((,)?(and|or)NP)?,whichallowstoextractenumerationsofconstituents(com-pareTable2).Itisimportanttomentionthatinthecaseofexpressionssuchas”acarcomprisesaﬁxednumberofbasiccomponents”,”dataminingcom-prisesarangeofdataanalysistechniques”,”booksconsistofaseriesofdots”,or”aconversationismadeupofaseriesofobservableinterpersonalex-changes”,onlytheNPafterthepreposition’of’istakenintoaccountasqualiaelement.TheTelicRoleisinprincipleacquiredinthesamewayastheFor-malandConstitutiveroleswiththeexceptionthatthequaliaelementisnotonlytheheadofanounphrase,butalsoaverboraverbfollowedbyanounphrase.Table3givesthecorrespondingcluesandpatterns.Inparticular,thereturnedcandidatequaliaelementsarethelemmatizedunderlinedexpressionsinPURP:=[VB]NP|NP|be[VBD].Finally,con-cerningthecluesandpatternsfortheagentiveroleshowninTable4,itisinterestingtoemphasizetheusageoftheadjectives’new’and’complete’.Theseadjectivesareusedinthepatternstoincreasetheex-pectationfortheoccurrenceofacreationverb.Ac-cordingtoourexperiments,thesepatternsarein-deedmorereliableinﬁndingappropriatequaliaele-mentsthanthealternativeversionwithouttheadjec-tives‘new’and‘complete’.Notethatinallpatterns,theparticiple(VBD)isalwaysreducedtobaseform(VB)viaalexiconlookup.Ingeneral,thepatternshavebeencraftedbyhand,testingandreﬁningtheminaniterativeprocess,payingattentiontomaximizetheircoveragebutalsoaccuracy.Inthefuture,weplantoexploitanapproachtoautomaticallylearnthepatterns.891

CluePatternSingular“a(x)xismadeupof”NPQTismadeupofNP’C“a(x)xismadeof”NPQTismadeofNP’C“a(x)xcomprises”NPQTcomprises(of)?NP’C“a(x)xconsistsof”NPQTconsistsofNP’CPlural“p(x)aremadeupof”NPQTismadeupofNP’C“p(x)aremadeof”NPQTaremadeofNP’C“p(x)comprise”NPQTcomprise(of)?NP’C“p(x)consistof”NPQTconsistofNP’CTable2:CluesandPatternsfortheConstitutiveRoleCluePatternSingular“purposeofa(x)xis”purposeof(a|an)xis(to)?PURP“a(x)isusedto”(a|an)xisusedtoPURPPlural“purposeofp(x)is”purposeofp(x)is(to)?PURP“p(x)areusedto”p(x)areusedtoPURPTable3:CluesandPatternsfortheTelicRole4RankingMeasuresInordertorankthedifferentqualiaelementsofagivenqualiastructure,werelyonacertainrankingmeasure.Inourexperiments,weanalyzefourdiffer-entrankingmeasures.Ontheonehand,weexploremeasureswhichusetheWebtocalculatethecorre-lationstrengthbetweenaqualiatermanditsqualiaelements.ThesemeasuresareWeb-basedversionsoftheJaccardcoefﬁcient(Web-Jac),thePointwiseMutualInformation(Web-PMI)andtheconditionalprobability(Web-P).WealsopresentaversionoftheconditionalprobabilitywhichdoesnotusetheWebbutmerelyreliesonthecountsofeachqualiaelementasproducedbythelexico-syntacticpatterns(P-measure).Wedescribethesemeasuresinthefol-lowing.4.1Web-basedJaccardMeasure(Web-Jac)Ourweb-basedJaccard(Web-Jac)measurereliesonthewebsearchenginetocalculatethenumberofdocumentsinwhichxandyco-occurclosetoeachother,dividedbythenumberofdocumentseachoneoccurs,i.e.Web-Jac(x,y):=Hits(x∗y)Hits(x)+Hits(y)−Hits(xANDy)Soherewearerelyingonthewildcardoperator’*’providedbytheGooglesearchengineAPI4.Though4Infact,fortheexperimentsdescribedinthispaperwerelyontheGoogleAPI.CluePatternSingular“to*a(x)newx”to[RB]?[VB]a?newx“to*a(x)completex”to[RB]?[VB]a?completex“a(x)newhasbeen*”a?newxhasbeen[VBD]“a(x)completexhasbeen*”a?completehasbeen[VBD]Plural“to*newp(x)”to[RB]?[VB]newp(x)“to*completep(x)”to[RB]?[VB]completep(x)Table4:CluesandPatternsfortheAgentiveRolethespeciﬁcfunctionofthe’*’operatorasimple-mentedbyGoogleisactuallyunknown,thebehaviorissimilartotheformerlyavailableAltavistaNEARoperator5.4.2Web-basedPointwiseMutualInformation(Web-PMI)InlinewithMagninietal.(Magninietal.,2001),wedeﬁneaPMI-basedmeasureasfollows:Web−PMI(x,y):=log2Hits(xANDy)MaxPagesHits(y)Hits(y)wheremaxPagesisanapproximationforthemaxi-mumnumberofEnglishwebpages6.4.3Web-basedConditionalProbability(Web-P)TheconditionalprobabilityP(x|y)isessentiallytheprobabilitythatxistruegiventhatyistrue,i.e.Web-P(x,y):=P(x|y)=P(x,y)P(y)=Hits(xNEARy)Hits(y)wherebyHits(xNEARy)iscalculatedasmentionedaboveusingthe‘*’operator.Incontrasttothemeasuresdescribedabove,thisoneisasym-metricsothatorderindeedmatters.GivenaqualiatermqtaswellasaqualiaelementqeweactuallycalculateWeb-P(qe,qt)foraspeciﬁcqualiarole.4.4ConditionalProbability(P)Thenonweb-basedconditionalprobabilityessen-tiallydiffersfromtheWeb-basedconditionalprob-abilityinthatweonlyrelyonthequaliaelements5Initialexperimentsindeedshowedthatcountingpagesinwhichthetwotermsoccurneareachotherincontrasttocount-ingpagesinwhichtheymerelyco-occurimprovedtheresultsoftheJaccardmeasurebyabout15%.6Wedeterminethisnumberexperimentallyasthenumberofwebpagescontainingthewords’the’and’and’.892

matched.Onthebasisofthese,wethencalculatetheprobabilityofacertainqualiaelementgivenacertainroleonthebasisofitsfrequencyofappear-ancewithrespecttothetotalnumberofqualiaele-mentsderivedforthisrole,i.e.wesimplycalculateP(qe|qr,qt)onthebasisofthederivedoccurrences,whereqtisagivenqualiaterm,qristhespeciﬁcqualiaroleandqeisaqualiaelement.5EvaluationInthissection,weﬁrstofalldescribeourevaluationmeasures.Thenwedescribethecreationofthegoldstandard.Further,wepresenttheresultsofthecom-parisonofthedifferentrankingmeasureswithre-specttothegoldstandard.Finally,wepresentan‘aposteriori’evaluationshowingthatthequaliastruc-tureslearnedareindeedreasonable.5.1EvaluationMeasuresAsourfocusistocomparethedifferentmeasuresdescribedabove,weneedtoevaluatetheircorre-spondingrankingsofthequaliaelementsforeachqualiastructure.Thisisasimilarcasetoevaluat-ingtherankingofdocumentswithininformationre-trievalsystems.Infact,asdoneinstandardinfor-mationretrievalresearch,ouraimistodetermineforeachrankingtheprecision/recalltrade-offwhenconsideringmoreorlessoftheitemsstartingfromthetopoftherankedlist.Thus,weevaluateourap-proachcalculatingprecisionatstandardrecalllevelsastypicallydoneininformationretrievalresearch(compare(Baeza-YatesandRibeiro-Neto,1999)).Herebythe11standardrecalllevelsare0%,10%,20%,30%,40%,50%,60%,70%,80%,90%and100%.Further,precisionatthesestandardrecalllevelsiscalculatedbyinterpolatingrecallasfol-lows:P(rj)=maxrj≤r≤rj+1P(r),where,j∈{0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1}.Thiswaywecancomparetheprecisionoverstandardre-callﬁguresforthedifferentrankings,thusobservingwhichmeasureleadstothebetterprecision/recalltrade-off.Inaddition,inordertoprovideonesinglevaluetocompare,wealsocalculatetheF-Measurecor-respondingtothebestprecision/recalltrade-offforeachrankingmeasure.ThisF-Measurethuscorre-spondstothebestcut-offpointwecanﬁndfortheitemsintherankedlist.Infact,weusethewell-knownF1measurecorrespondingtotheharmonicmeanbetweenrecallandprecision:F1:=maxj2P(rj)rjP(rj)+rjAsabaseline,wecompareourresultstoanaivestrategywithoutanyranking,i.e.wecalculatetheF-Measureforalltheitemsinthe(unranked)listofqualiaelements.Consequently,fortherankingstobeuseful,theyneedtoyieldhigherF-Measuresthanthisnaivebaseline.5.2GoldStandardThegoldstandardwascreatedforthe30wordsusedalreadyintheexperimentsdescribedin(YamadaandBaldwin,2004):accounting,beef,book,car,cash,clinic,complexity,counter,county,delegation,door,estimate,executive,food,gaze,imagination,inves-tigation,juice,knife,letter,maturity,novel,phone,prisoner,profession,review,register,speech,sun-shine,table.Thesewordsweredistributedmoreorlessuniformlybetween30participantsofourexper-iment,makingsurethatthreequaliastructuresforeachwordwerecreatedbythreedifferentsubjects.Theparticipants,whowereallnon-linguistics,re-ceivedashortinstructionintheformofashortpre-sentationexplainingwhatqualiastructuresare,theaimsoftheexperimentaswellastheirspeciﬁctask.Theywerealsoshownsomeexamplesforqualiastructuresforwordsnotconsideredinourexperi-ments.Further,theywereaskedtoprovidebetween5and10qualiaelementsforeachqualiarole.Theparticipantscompletedthetestviae-mail.Asaﬁrstinterestingobservation,itisworthmentioningthattheparticipantsonlydelivered3-5qualiaelementsonaveragedependingontheroleinquestion.Thisshowsalreadythatparticipantshadtroubleinﬁnd-ingdifferentqualiaelementsforagivenqualiarole.Wecalculatetheagreementforthetaskofspecify-ingqualiastructuresforaparticulartermandroleastheaveragedpairwiseagreementbetweenthequaliaelementsdeliveredbythethreesubjects,henceforthS1,S2andS3as:Agr:=|S1∩S2||S1∪S2|+|S1∩S3||S1∪S3|+|S2∪S3||S2∩S3|3Averagingoveralltherolesandwords,wegetanaverageagreementof11.8%,i.e.ourhumantest893

subjectscoincideinslightlymorethanevery10thqualiaelement.Thisiscertainlyaverylowagree-mentandcertainlyhintsatthefactthatthetaskcon-siderediscertainlydifﬁcult.Theagreementwaslowest(7.29%)forthetelicrole.Afurtherinterestingobservationisthatthelowestagreementisyieldedformoreabstractwords,whiletheagreementforveryconcretewordsisreasonable.Forexample,theﬁvewordswiththehighestagree-mentareindeedconcretethings:knife(31%),cash(29%),juice(21%),car(20%)anddoor(19%).Thewordswithanagreementbelow5%aregaze,pris-oner,accounting,maturity,complexityanddelega-tion.Inparticular,ourtestsubjectshadsubstantialdifﬁcultiesinﬁndingthepurposeofsuchabstractwords.Infact,theagreementonthetelicroleisbe-low5%formorethanhalfofthewords.Ingeneral,thisshowsthatanyautomaticap-proachtowardslearningqualiastructuresfacesse-verelimits.Forsure,wecannotexpecttheresultsofanautomaticevaluationtobeveryhigh.Forex-ample,forthetelicroleof‘clinic’,onetestsubjectspeciﬁedthequaliaelement‘cure’,whileanotheronespeciﬁed‘curedisease’,thusleadingtoadis-agreementinspiteoftheobviousagreementatthesemanticlevel.Inthisline,theaverageagreementreportedabovehasinfacttoberegardedasalowerboundfortheactualagreement.Ofcourse,ourap-proachtocalculatingagreementistoostrict,butinabsenceofaclearandcomputabledeﬁnitionofse-manticagreement,itwillsufﬁceforthepurposesofthispaper.5.3GoldStandardEvaluationWeranexperimentscalculatingthequaliastructureforeachofthe30words,rankingtheresultingqualiaelementsforeachqualiastructureusingthedifferentmeasuresdescribedinSection4.Figure1showsthebestF-Measurecorrespond-ingtoacut-offleadingtoanoptimalprecision/recalltrade-off.WeseethattheP-measureperformsbest,whiletheWeb-PmeasureandtheWeb-Jacmeasurefollowatabout0.05and0.2pointsdistance.ThePMI-basedmeasureindeedleadstotheworstF-Measurevalues.Indeed,theP-measuredeliveredthebestresultsfortheformalandagentiveroles,whileforthecon-stitutiveandtelicrolestheWeb-Jacmeasureper-Figure1:AverageF1measureforthedifferentrank-ingmeasuresformedbest.ThereasonwhyPMIperformssobadlyisthefactthatitfavorstoospeciﬁcresultswhichareunlikelytooccurassuchinthegoldstandard.Forexample,whiletheconditionalprobabilityrankshighest:explore,helpillustrate,illustrateanden-richforthetelicroleofnovel,thePMI-basedmea-surerankshighest:exploregreatthemes,illustratetheologicalpoints,conveytruth,teachreadingskillsandillustrateconcepts.Aseriesofsigniﬁcancetests(pairedStudent’st-testatanα-levelof0.05)showedthatthethreebestperformingmeasures(P,Web-PandWeb-Jaccard)shownorealdifferenceamongthem,whileallthreeshowsigniﬁcantdifferencetotheWeb-PMImeasure.Asecondseriesofsignif-icancetests(againpairedStudent’st-testatanα-levelof0.05)showedthatallrankingmeasuresin-deedsigniﬁcantlyoutperformthebaseline,whichshowsthatourrankingsareindeedreasonable.In-terestingly,thereseemstobeaninterestingpositivecorrelationbetweentheF-Measureandthehumanagreement.Forexample,forthebestperformingrankingmeasure,i.e.theP-measure,wegetanav-erageF-Measureof21%forwordswithanagree-mentover5%,whilewegetanF-Measureof9%forwordswithanagreementbelow5%.Therea-sonhereprobablyisthatthosewordsandqualiaele-mentsforwhichpeoplearemoreconﬁdentalsohaveahigherfrequencyofappearanceontheWeb.5.4AposterioriEvaluationInordertocheckwhethertheautomaticallylearnedqualiastructuresarereasonablefromanintuitivepointofview,wealsoperformedanaposteriori894

evaluationinthelinesof(CimianoandWenderoth,2005).Inthisexperiment,wepresentedthetop10rankedqualiaelementsforeachqualiarolefor10randomlyselectedwordstothedifferenttestper-sons.HereweonlyusedtheP-measureforrank-ingasitperformedbestinourpreviousevaluationwithregardtothegoldstandard.Inordertover-ifythatoursampleisnotbiased,wecheckedthattheF-Measureyieldedbyour10randomlyselectedwords(17.7%)doesnotdiffersubstantiallyfromtheoverallaverageF-Measure(17.1%)tobesurethatwehavechosenwordsfromallF-Measureranges.Inparticular,weaskeddifferenttestsubjectswhichalsoparticipatedinthecreationofthegoldstandardtoratethequaliaelementswithrespecttotheirap-propriatenessforthequaliatermusingascalefrom0to3,whereby0means’wrong’,1’nottotallywrong’,2’acceptable’and3’totallycorrect’.Theparticipantsconﬁrmedthatitwaseasiertovalidateexistingqualiastructuresthantocreatethemfromscratch,whichalreadycorroboratestheusefulnessofourautomaticapproach.Thequaliastructureforeachofthe10randomlyselectedwordswasvali-datedindependentlybythreetestpersons.Infact,inwhatfollowswealwaysreportresultsaveragedforthreetestsubjects.Figure2showstheaveragevaluesfordifferentroles.Weobservethatthecon-stitutiveroleyieldsthebestresults,followedbytheformal,telicandagentiveroles(inthisorder).Ingeneral,allresultsareabove2,whichshowsthatthequaliastructuresproducedareindeedacceptable.Thoughwedonotpresenttheseresultsinmorede-tailduetospacelimitations,itisalsointerestingtomentionthattheF-Measurecalculatedwithrespecttothegoldstandardwasingeneralhighlycorrelatedwiththevaluesassignedbythehumantestsubjectsinthisaposteriorivalidation.6RelatedWorkInsteadofmatchingHearst-stylepatterns(Hearst,1992)inalargetextcollection,someresearchershaverecentlyturnedtotheWebtomatchthesepat-ternssuchasin(Markertetal.,2003)or(Etzionietal.,2005).Ourapproachgoesfurtherinthatitnotonlylearnstyping,superconceptorinstance-ofrela-tions,butalsoConstitutive,TelicandAgentiverela-tions.Figure2:AverageratingsforeachqualiaroleTherealsoexistapproachesspeciﬁcallyaimingatlearningqualiaelementsfromcorporabasedonma-chinelearningtechniques.Claveauetal.(Claveauetal.,2003)forexampleuseInductiveLogicPro-grammingtolearnifagivenverbisaqualiaele-mentornot.However,theirapproachdoesnogoasfaraslearningthecompletequaliastructureforalexicalelementasinourapproach.Further,intheirapproachtheydonotdistinguishbetweendifferentqualiarolesandrestrictthemselvestoverbsaspo-tentialﬁllersofqualiaroles.YamadaandBaldwin(YamadaandBaldwin,2004)presentanapproachtolearningTelicandAgentiverelationsfromcorporaanalyzingtwodifferentap-proaches:onerelyingonmatchingcertainlexico-syntacticpatternsasintheworkpresentedhere,butalsoasecondapproachconsistingintrainingamax-imumentropymodelclassiﬁer.Thepatternsusedby(YamadaandBaldwin,2004)differsubstantiallyfromtheonesusedinthispaper,whichismainlyduetothefactthatsearchenginesdonotprovidesupportforregularexpressionsandthusinstantiat-ingapatternas’V[+ing]Noun’isimpossibleinourapproachastheverbsareunknownapriori.PoesioandAlmuhareb(PoesioandAlmuhareb,2005)presentamachinelearningbasedapproachtoclassifyingattributesintothesixcategories:qual-ity,part,related-object,activity,related-agentandnon-attribute.7ConclusionWehavepresentedanapproachtoautomaticallylearningqualiastructuresfromtheWeb.Suchanapproachisespeciallyinterestingeitherforlexicog-895

raphersaimingatconstructinglexicons,butevenmorefornaturallanguageprocessingsystemsre-lyingondeeplexicalknowledgeasrepresentedbyqualiastructures.Inparticular,wehavefocusedonlearningrankedqualiastructureswhichallowtoﬁndanidealcut-offpointtoincreasethepreci-sion/recalltrade-offofthelearnedstructures.Wehaveabstractedfromtheissueofﬁndingtheappro-priatecut-off,leavingthisforfuturework.Inpartic-ular,wehaveevaluateddifferentrankingmeasuresforthispurpose,showingthatalloftheanalyzedmeasures(Web-P,Web-Jaccard,Web-PMIandtheconditionalprobability)signiﬁcantlyoutperformedabaselineusingnorankingmeasure.Overall,theplainconditionalprobabilityP(notcalculatedovertheWeb)aswellastheconditionalprobabilitycal-culatedovertheWeb(Web-P)deliveredthebestre-sults,whilethePMI-basedrankingmeasureyieldedtheworstresults.Ingeneral,ourmainaimhasbeentoshowthat,thoughthetaskofautomaticallylearn-ingqualiastructuresisindeedverydifﬁcultasshownbyourlowhumanagreement,reasonablestructurescanindeedbelearnedwithapattern-basedapproachaspresentedinthispaper.Furtherworkwillaimatinducingthepatternsautomaticallygivensomeseedexamples,butalsoatusingtheautomaticallylearnedstructureswithinNLPapplications.Thecre-atedqualiastructuregoldstandardisavailableforthecommunity7.ReferencesR.Baeza-YatesandB.Ribeiro-Neto.1999.ModernIn-formationRetrieval.Addison-Wesley.C.F.Baker,C.J.Fillmore,andJ.B.Lowe.1998.TheBerkeleyFrameNetProject.InProceedingsofCOL-ING/ACL’98,pages86–90.J.Bos,P.Buitelaar,andM.Mineur.1995.Bridgingascoerciveaccomodation.InWorkingNotesoftheEdin-burghConferenceonComputationalLogicandNatu-ralLanguageProcessing(CLNLP-95).P.CimianoandJ.Wenderoth.2005.Learningqualiastructuresfromtheweb.InProceedingsoftheACLWorkshoponDeepLexicalAcquisition,pages28–37.V.Claveau,P.Sebillot,C.Fabre,andP.Bouillon.2003.Learningsemanticlexiconsfromapart-of-speechandsemanticallytaggedcorpususinginductivelogicpro-gramming.JournalofMachineLearningResearch,(4):493–525.7Seehttp://www.cimiano.de/qualia.O.Etzioni,M.Cafarella,D.Downey,A-M.Popescu,T.Shaked,S.Soderland,D.S.Weld,andA.Yates.2005.Unsupervisednamed-entityextractionfromtheweb:Anexperimentalstudy.ArtiﬁcialIntelligence,165(1):91–134.C.Fellbaum.1998.WordNet,anelectroniclexicaldatabase.MITPress.M.A.Hearst.1992.Automaticacquisitionofhyponymsfromlargetextcorpora.InProceedingsofCOL-ING‘92,pages539–545.M.JohnstonandF.Busa.1996.Qualiastructureandthecompositionalinterpretationofcompounds.InPro-ceedingsoftheACLSIGLEXworkshoponbreadthanddepthofsemanticlexicons.A.KilgariffandG.Grefenstette,editors.2003.SpecialIssueontheWebasCorpusoftheJournalofCompu-tationalLinguistics,volume29(3).MITPress.B.Magnini,M.Negri,R.Prevete,andH.Tanev.2001.Isittherightanswer?:exploitingwebredundancyforanswervalidation.InProceedingsofthe40thAnnualMeetingoftheACL,pages425–432.K.Markert,N.Modjeska,andM.Nissim.2003.Us-ingthewebfornominalanaphoraresolution.InPro-ceedingsoftheEACLWorkshopontheComputationalTreatmentofAnaphora.M.PoesioandA.Almuhareb.2005.Identifyingconceptattributesusingaclassiﬁer.InProceedingsoftheACLWorkshoponDeepLexicalAcquisition,pages18–27.J.Pustejovsky,P.Anick,andS.Bergler.1993.Lexi-calsemantictechniquesforcorpusanalysis.Compu-tationalLingustics,SpecialIssueonUsingLargeCor-poraII,19(2):331–358.J.Pustejovsky.1991.Thegenerativelexicon.Computa-tionalLinguistics,17(4):209–441.D.TuﬁsandO.Mason.1998.TaggingRomanianTexts:aCaseStudyforQTAG,aLanguageIndepen-dentProbabilisticTagger.InProceedingsofLREC,pages589–96.E.M.Voorhees.1994.Queryexpansionusinglexical-semanticrelations.InProceedingsofthe17thannualinternationalACMSIGIRconferenceonResearchanddevelopmentininformationretrieval,pages61–69.I.YamadaandT.Baldwin.2004.Automaticdiscoveryoftelicandagentiverolesfromcorpusdata.InPro-ceedingsofthethe18thPaciﬁcAsiaConferenceonLanguage,InformationandComputation(PACLIC).Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 896–903,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

896

ASequencingModelforSituationEntityClassiﬁcationAlexisPalmer,EliasPonvert,JasonBaldridge,andCarlotaSmithDepartmentofLinguisticsUniversityofTexasatAustin{alexispalmer,ponvert,jbaldrid,carlotasmith}@mail.utexas.eduAbstractSituationentities(SEs)aretheevents,states,genericstatements,andembeddedfactsandpropositionsintroducedtoadiscoursebyclausesoftext.Wereportontheﬁrstdata-drivenmodelsforlabelingclausesaccordingtothetypeofSEtheyintroduce.SEclassiﬁ-cationisimportantfordiscoursemodeiden-tiﬁcationandfortrackingthetemporalpro-gressionofadiscourse.Weshowthat(a)linguistically-motivatedcooccurrencefea-turesandgrammaticalrelationinformationfromdeepsyntacticanalysisimproveclas-siﬁcationaccuracyand(b)usingasequenc-ingmodelprovidesimprovementsoveras-signinglabelsbasedontheutterancealone.Wereportongenreeffectswhichsupporttheanalysisofdiscoursemodeshavingcharac-teristicdistributionsandsequencesofSEs.1IntroductionUnderstandingdiscourserequiresidentifyingtheparticipantsinthediscourse,thesituationstheypar-ticipatein,andthevariousrelationshipsbetweenandamongbothparticipantsandsituations.Coreferenceresolution,forexample,isconcernedwithunder-standingtherelationshipsbetweenreferencestodis-courseparticipants.Thispaperaddressestheprob-lemofidentifyingandclassifyingreferencestositu-ationsexpressedinwrittenEnglishtexts.Situationentities(SEs)aretheevents,states,genericstatements,andembeddedfactsandpropo-sitionswhichclausesintroduce(Vendler,1967;Verkuyl,1972;Dowty,1979;Smith,1991;Asher,1993;CarlsonandPelletier,1995).Considerthetextpassagebelow,whichintroducesanevent-typeentityin(1),areport-typeentityin(2),andastate-typeentityin(3).(1)SonyCorp.hasheavilypromotedtheVideoWalkmansincetheproduct’sintroductionlastsummer,(2)butBobGerson,videoeditorofThisWeekinCon-sumerElectronics,says(3)Sonyconceivesof8mmasa“familyofproducts,camcordersandVCRdecks,”SEclassiﬁcationisafundamentalcomponentinde-terminingthediscoursemodeoftexts(Smith,2003)and,alongwithaspectualclassiﬁcation,fortempo-ralinterpretation(MoensandSteedman,1988).Itmaybeusefulfordiscourserelationprojectionanddiscourseparsing.Thoughsituationentitiesarewell-studiedinlin-guistics,theyhavereceivedverylittlecomputationaltreatment.Thispaperpresentstheﬁrstdata-drivenmodelsforSEclassiﬁcation.Ourtwomainstrate-giesare(a)theuseoflinguistically-motivatedfea-turesand(b)theimplementationofSEclassiﬁcationasasequencingtask.Ourresultsalsoprovideempir-icalsupportfortheverynotionofdiscoursemodes,asweseecleargenreeffectsinSEclassiﬁcation.WebeginbydiscussingSEsinmoredetail.Sec-tion3describesourtwoannotateddatasetsandpro-videsexamplesofeachSEtype.Section4discussesfeaturesets,andsections5and6presentmodels,experiments,andresults.897

2DiscoursemodesandsituationentitiesInthissection,wediscusssomeofthelinguisticmo-tivationforSEclassiﬁcationandtherelationofSEclassiﬁcationtodiscoursemodeidentiﬁcation.2.1SituationentitiesThecategorizationofSEsintoaspectualclassesismotivatedbypatternsintheirlinguisticbehavior.Weadoptanexpandedversionofaparadigmrelat-ingSEstodiscoursemode(Smith,2003)andchar-acterizeSEswithfourbroadcategories:1.Eventualities.Events(E),particularstates(S),andreports(R).Risasub-typeofEforSEsintroducedbyverbsofspeech(e.g.,say).2.Generalstatives.Generics(G)andgeneraliz-ingsentences(GS).Theformerareutterancespredicatedofageneralclassorkindratherthanofanyspeciﬁcindividual.Thelatterarehabit-ualutterancesthatrefertoongoingactionsorpropertiespredicatedofspeciﬁcindividuals.3.Abstractentities.Facts(F)andproposi-tions(P).14.Speech-acttypes.Questions(Q)andimpera-tives(IMP).ExamplesofeachSEtypearegiveninsection3.2.Thereareanumberoflinguistictestsforiden-tifyingsituationentities(Smith,2003).ThetermlinguistictestreferstoarulewhichcorrelatesanSEtypetoparticularlinguisticforms.Forexam-ple,event-typeverbsinsimplepresenttensearealinguisticcorrelateofGS-typeSEs.TheselinguistictestsvaryintheirprecisionanddifferenttestsmaypredictdifferentSEtypesforthesameclause.Arule-basedimplementationus-ingthemtoclassifySEswouldrequirecarefulruleorderingormediationofruleconﬂicts.However,sincetheserulesareexactlythesortofinformationextractedasfeaturesindata-drivenclassiﬁers,they1InoursystemthesetwoSEtypesareidentiﬁedlargelyascomplementsoffactiveandpropositionalverbsasdiscussedinPeterson(1997).Factandpropositionalcomplementshavesomelinguisticaswellassomenotionaldifferences.Factsmayhavecausaleffects,andfactsareintheworld.Neitheroftheseistrueforpropositions.Inaddition,thetwohavesomewhatdifferentsemanticconsequencesofapresuppositionalnature.canbecleanlyintegratedbyassigningthemempiri-callydeterminedweights.Weusemaximumentropymodels(Bergeretal.,1996),whichareparticularlywell-suitedfortasks(likeours)withmanyoverlap-pingfeatures,toharnesstheselinguisticinsightsbyusingfeaturesinourmodelswhichencode,directlyorindirectly,thelinguisticcorrelatestoSEtypes.Thefeaturesaredescribedindetailinsection4.2.2BasicandderivedsituationtypesSituationentitieseachhaveabasicsituationtype,determinedbytheverbplusitsarguments,theverbconstellation.Theverbitselfplaysakeyroleinde-terminingbasicsituationtypebutitisnottheonlyfactor.Changesintheargumentsortenseoftheverbsometimeschangethebasicsituationtypes:(4)Mickeypaintedthehouse.(E)(5)Mickeypaintshouses.(GS)IfSEtypecouldbedeterminedsolelybytheverbconstellation,automaticclassiﬁcationofSEswouldbearelativelystraightforwardtask.However,otherpartsoftheclauseoftenoverridethebasicsituationtype,resultinginaspectualcoercionandaderivedsituationtype.Forexample,amodaladverbcantriggeraspectualcoercion:(6)Mickeyprobablypaintshouses.(P)SeriouschallengesforSEclassiﬁcationarisefromtheaspectualambiguityandﬂexibilityofmanypredicatesaswellasfromaspectualcoercion.2.3DiscoursemodesMuchofthemotivationofSEclassiﬁcationistowardthebroadergoalofidentifyingdiscoursemodes,whichprovidealinguisticcharacterizationoftextualpassagesaccordingtothesituationenti-tiesintroduced.Theycorrespondtointuitionsastotherhetoricalorsemanticcharacterofatext.Pas-sagesofwrittentextcanbeclassiﬁedintomodesofdiscourse–Narrative,Description,Argument,In-formation,andReport–byexaminingconcretelin-guisticcuesinthetext(Smith,2003).Thesecuesareoftwoforms:thedistributionofsituationentitytypesandthemodeofprogression(eithertemporalormetaphorical)throughthetext.898

Forexample,theNarrationandReportmodesbothcontainmainlyeventsandtemporallyboundedstates;theydifferintheirprinciplesoftemporalpro-gression.Reportpassagesprogresswithrespectto(deictic)speechtime,whereasNarrativepassagesprogresswithrespectto(anaphoric)referencetime.PassagesintheDescriptionmodearepredominantlystative,andArgumentmodepassagestendtobecharacterizedbypropositionsandInformationmodepassagesbyfactsandstates.3DataThissectiondescribesthedatasetsusedintheex-periments,theprocessforcreatingannotatedtrain-ingdata,andpreprocessingsteps.Also,wegiveex-amplesofthetenSEtypes.TherearenoestablisheddatasetsforSEclassiﬁ-cation,sowecreatedannotatedtrainingdatatotestourmodels.Wehaveannotatedtwodatasets,onefromtheBrowncorpusandonebasedondatafromtheMessageUnderstandingConference6(MUC6).3.1SegmentationTheBrowntextsweresegmentedaccordingtoSE-containingclausalboundaries,andeachclausewaslabeledwithanSElabel.Segmentationisitselfadifﬁculttask,andwemadesomesimpliﬁcations.Ingeneral,clausalcomplementsofverbslikesaywhichhaveclausaldirectobjectsweretreatedasseparateclausesandgivenanSElabel.Clausalcom-plementsofverbswhichhaveanentityasadirectobjectandsecondclausalcomplement(suchasno-tify)werenottreatedasseparateclauses.Inaddi-tion,somemodifyingandadjunctclauseswerenotassignedseparateSElabels.TheMUCtextscametoussegmentedintoele-mentarydiscourseunits(EDUs),andeachEDUwaslabeledbytheannotators.Thetwodatasetsweresegmentedaccordingtoslightlydifferentconven-tions,andwedidnotnormalizethesegmentation.Theinconsistenciesinsegmentationintroducesomeerrortotheotherwisegold-standardsegmentations.3.2AnnotationEachtextwasindependentlyannotatedbytwoex-pertsandreviewedbyathird.Eachclausewasas-signedpreciselyoneSElabelfromthesetoftenpossiblelabels.ForclauseswhichintroducemoreSETextSThatcompareswithroughlypaperback-bookdimensionsforVHS.GAccordingly,mostVHScamcordersareusuallybulkyandweigharoundeightpoundsormore.S“Carlisatenaciousfellow,”RsaidasourceclosetoUSAir.GS“Hedoesn’tgiveupeasilyGSandoneshouldneverunderestimatewhathecanorwilldo.”SForJenksknewFthatBari’sdefensesweremadeofpaper.EMr.IcahnthenproposedPthatUSAirbuyTWA,IMP“Fermate”!RMusmannobellowedtohisItaliancrewmen.QWhat’shername?SQuiteseriously,thenamesmentionedaspossibilitieswerethreemaleapparatchiksfromtheBeltway’sDemocraticpoliticalmachineNByAndrewB.CohenStaffReporterofTheWSJTable1:ExampleclausesandtheirSEannota-tion.Horizontallinesseparateextractsfromdiffer-enttexts.thanoneSE,theannotatorsselectedthemostsalientone.Thissituationaroseprimarilywhencomple-mentclauseswerenottreatedasdistinctclauses,inwhichcasetheSEselectedwastheoneintroducedbythemainverb.ThelabelNwasusedforclauseswhichdonotintroduceanysituationentity.TheBrowndatasetconsistsof20“popularlore”textsfromsectioncfoftheBrowncorpus.Seg-mentationofthesetextsresultedinatotalof4390clauses.Ofthese,3604wereusedfortraininganddevelopment,and786wereheldoutasﬁnaltest-ingdata.TheMUCdatasetconsistsof50WallStreetJournalnewspaperarticlessegmentedtoato-talof1675clauses.137MUCclauseswereheldoutfortesting.TheBrowntextsarelongerthantheMUCtexts,withanaverageof219.5clausesperdocumentascomparedtoMUC’saverageof33.5clauses.TheaverageclauseintheBrowndatacontains12.6words,slightlylongerthantheMUCtexts’averageof10.9words.Table1providesexamplesofthetenSEtypesaswellasshowinghowclausesweresegmented.EachSE-containingexampleisasequenceofEDUsfromthedatasetsusedinthisstudy.899

WWORDSwords&punctuationWTW(seeabove)POSONLYPOStagforeachwordWORD/POSword/POSpairforeachwordWTLWT(seeabove)FORCEPREDTifclause(orprecedingclause)containsforcepredicatePROPPREDTifclause(orprecedingclause)containspropositionalverbFACTPREDTifclause(orprecedingclause)containsfactiveverbGENPREDTifclausecontainsgenericpredicateHASFINTifclausecontainsﬁniteverbHASMODALTifclausecontainsmodalverbFREQADVTifclausecontainsfrequencyadverbMODALADVTifclausecontainsmodaladverbVOLADVTifclausecontainsvolitionaladverbFIRSTVBlexicalitemandPOStagforﬁrstverbWTLGWTL(seeabove)VERBSallverbsinclauseVERBTAGSPOStagsforallverbsMAINVBmainverbofclauseSUBJsubjectofclause(lexicalitem)SUPERCCGsupertagTable2:FeaturesetsforSEclassiﬁcation3.3PreprocessingThelinguistictestsforSEclassiﬁcationappealtomultiplelevelsoflinguisticinformation;therearelexical,morphological,syntactic,categorial,andstructuraltests.Inordertoaccesscategorialandstructuralinformation,weusedtheC&C2toolkit(ClarkandCurran,2004).Itprovidespart-of-speechtagsandCombinatoryCategorialGrammar(CCG)(Steedman,2000)categoriesforwordsandsyntac-ticdependenciesacrosswords.4FeaturesOneofourgoalsinundertakingthisstudywastoexploretheuseoflinguistically-motivatedfeaturesanddeepsyntacticfeaturesinprobabilisticmodelsforSEclassiﬁcation.Thenatureofthetaskrequiresfeaturescharacterizingtheentireclause.Here,wedescribeourfourfeaturesets,summarizedintable2.Thefeaturesetsareadditive,extendingverybasicfeaturesetsﬁrstwithlinguistically-motivatedfea-turesandthenwithdeepsyntacticfeatures.2svn.ask.it.usyd.edu.ap/trac/candc/wiki4.1Basicfeaturesets:WandWTTheWORDS(W)featuresetlooksonlyatthewordsandpunctuationintheclause.Thesefeaturesareobtainedwithnolinguisticprocessing.WORDS/TAGS(WT)incorporatespart-of-speech(POS)tagsforeachword,number,andpunctuationmarkintheclauseandtheword/tagpairsforeachelementoftheclause.POStagsprovidevaluablein-formationaboutsyntacticcategoryaswellascertainkindsofshallowsemanticinformation(suchasverbtense).Thetagsareusefulforidentifyingverbs,nouns,andadverbs,andthewordsthemselvesrepre-sentlexico-semanticinformationinthefeaturesets.4.2Linguistically-motivatedfeatureset:WTLTheWORDS/TAGS/LINGUISTICCORRELATES(WTL)featuresetintroduceslinguistically-motivatedfeaturesgleanedfromtheliteratureonSEs;eachfeatureencodesalinguisticcuethatmaycorrelatetooneormoreSEtypes.Thesefeaturesarenotdirectlyannotated;insteadtheyareextractedbycomparingwordsandtheirtagsforthecurrentandimmediatelyprecedingclausestolistscontainingappropriatetriggers.ThelistsarecompiledfromtheliteratureonSEs.Forexample,clausesembeddedunderpredicateslikeforcegenerallyintroduceE-typeSEs:(7)Iforced[Johntoruntheracewithme].(8)*Iforced[JohntoknowFrench].Thefeatureforce-PREVisextractedifamemberoftheforce-typepredicatewordlistoccursinthepreviousclause.Someofthecorrelationsdiscussedinthelitera-turerelyonalevelofsyntacticanalysisnotavailableintheWTLfeatureset.Forexample,stativityofthemainverbisonefeatureusedtodistinguishbetweeneventandstateSEs,andparticularverbsandverbtenseshavetendencieswithrespecttostativity.Toapproximatethemainverbwithoutsyntacticanaly-sis,WTLusesthelexicalitemoftheﬁrstverbintheclauseandthePOStagsofallverbsintheclause.Theselinguistictestsarenon-absolute,makingtheminappropriateforarule-basedmodel.Ourmodelshandlethedefeasibilityofthesecorrelationsprobabilistically,asisstandardformachinelearningfornaturallanguageprocessing.900

4.3Additionofdeepfeatures:WTLGTheWORDS/TAGS/LINGUISTICCORRE-LATES/GRAMMATICALRELATIONS(WTLG)featuresetusesadeeperlevelofsyntacticanalysisviafeaturesextractedfromCCGparserepresenta-tionsforeachclause.Thisfeaturesetrequiresanadditionalstepoflinguisticprocessingbutprovidesabasisformoreaccurateclassiﬁcation.WTLapproximatedthemainverbbysloppilytak-ingtheﬁrstverbintheclause;incontrast,WTLGusesthemainverbidentiﬁedbytheparser.Theparseralsoreliablyidentiﬁesthesubject,whichisusedasafeature.Supertags–CCGcategoriesassignedtowords–provideaninterestingclassoffeaturesinWTLG.Theysuccinctlyencoderichergrammaticalinforma-tionthansimplePOStags,especiallysubcategoriza-tionandargumenttypes.Forexample,thetagS\NPdenotesanintransitiveverb,whereas(S\NP)/NPdenotesatransitiveverb.Assuch,theycanbeseenasawayofencodingtheverbalconstellationanditseffectonaspectualclassiﬁcation.5ModelsWeconsidertwotypesofmodelsfortheautomaticclassiﬁcationofsituationentities.Theﬁrst,ala-belingmodel,utilizesamaximumentropymodeltopredictSElabelsbasedonclause-levellinguisticfeaturesasdiscussedabove.Thismodelignoresthediscoursepatternsthatlinkmultipleutterances.Be-causethesepatternsrecur,asequencingmodelmaybebettersuitedtotheSEclassiﬁcationtask.Oursecondmodelthusextendstheﬁrstbyincorporatingthepreviousn(0≤n≤6)labelsasfeatures.Sequencingisstandardlyusedfortaskslikepart-of-speechtagging,whichgenerallyassumesmallerunitstobebothtaggedandconsideredascontextfortagging.Wearetaggingattheclauselevelratherthanatthewordlevel,butthestructureoftheprob-lemisessentiallythesame.WethusadaptedtheOpenNLPmaximumentropypart-of-speechtagger3(Hockenmaieretal.,2004)toextractfeaturesfromutterancesandtotagsequencesofutterancesinsteadofwords.Thisallowstheuseoffeaturesofadjacentclausesaswellaspreviously-predictedlabelswhenmakingclassiﬁcationdecisions.3http://opennlp.sourceforge.net.6ExperimentsInthissectionwegiveresultsfortestingonBrowndata.Allresultsarereportedintermsofaccu-racy,deﬁnedasthepercentageofcorrectly-labeledclauses.Standard10-foldcross-validationonthetrainingdatawasusedtodevelopmodelsandfea-turesets.Theoptimizedmodelswerethentestedontheheld-outBrownandMUCdata.ThebaselinewasdeterminedbyassigningS(state),themostfrequentlabelinbothtrainingsets,toeachclause.Baselineaccuracywas38.5%and36.2%forBrownandMUC,respectively.Ingeneral,accuracyﬁguresforMUCaremuchhigherthanforBrown.ThisislikelyduetothefactthattheMUCtextsaremoreconsistent:theyareallnewswiretextsofafairlyconsistenttoneandgenre.TheBrowntexts,incontrast,arefromthe‘popularlore’sectionofthecorpusandspanawiderangeoftopicsandtexttypes.Nonetheless,thepatternsbetweenthefeaturesetsanduseofsequencepredic-tionholdacrossbothdatasets;here,wefocusourdiscussionontheresultsfortheBrowndata.6.1LabelingresultsTheresultsforthelabelingmodelappearinthetwocolumnslabeled‘n=0’intable3.OnBrown,thesimpleWfeaturesetbeatsthebaselineby6.9%withanaccuracyof45.4%.AddingPOSinformation(WT)boostsaccuracy4.5%to49.9%.WedidnotseetheexpectedincreaseinperformancefromthelinguisticallymotivatedWTLfeatures,butratheraslightdecreaseinaccuracyto48.9%.Thesefeaturesmayrequireagreateramountoftrainingmaterialtobeeffective.AdditionofdeeplinguisticinformationwithWTLGimprovedperformanceto50.6%,againof5.2%overwordsalone.6.2OracleresultsTodeterminethepotentialeffectivenessofsequenceprediction,weperformedoracleexperimentsonBrownbyincludingpreviousgold-standardlabelsasfeatures.Figure1illustratestheresultsfromora-cleexperimentsincorporatingfromzerotosixpre-viousgold-standardSElabels(thelookback).TheincreaseinperformanceillustratestheimportanceofcontextintheidentiﬁcationofSEsandmotivatestheuseofsequenceprediction.901

424446485052545658600123456AccLookbackWWTWTLWTLGFigure1:OracleresultsonBrowndata.6.3SequencingresultsTable3givestheresultsofclassiﬁcationwiththese-quencingmodelontheBrowndata.Aswiththela-belingmodel,accuracyisboostedbyWTandWTLGfeaturesets.WeseeanunexpecteddegradationinperformanceinthetransitionfromWTtoWTL.Themostinterestingresultshere,though,arethegainsinaccuracyfromuseofpreviously-predictedlabelsasfeaturesforclassiﬁcation.Whenlabelingperformanceisrelativelypoor,aswithfeaturesetW,previouslabelshelpverylittle,butaslabelingaccu-racyincreases,previouslabelsbegintoeffectnotice-ableincreasesinaccuracy.Forthebesttwofeaturesets,consideringtheprevioustwolabelsraisestheaccuracy2.0%and2.5%,respectively.Inmostcases,though,performancestartstode-gradeasthemodelincorporatesmorethantwopre-viouslabels.ThisdegradationisillustratedinFig-ure2.Theexplanationforthisisthatthemodelisstillveryweak,withanaccuracyoflessthan54%fortheBrowndata.Themorepreviouspredictedla-belsthemodelconditionson,thegreaterthelikeli-hoodthatoneormoreofthelabelsisincorrect.Withgold-standardlabels,weseeasteadyincreaseinac-curacyaswelookfurtherback,andwewouldneedabetterperformingmodeltofullytakeadvantageofknowledgeofSEpatternsindiscourse.Thesequencingmodelplaysacrucialrole,partic-ularlywithsuchasmallamountoftrainingmaterial,andourresultsindicatetheimportanceoflocalcon-textindiscourseanalysis.424446485052540123456WWTWTLWTLGFigure2:SequencingresultsonBrowndata.BROWNLookback(n)0123456W45.445.246.146.642.843.042.4WT49.952.451.949.247.246.244.8WTL48.950.550.148.946.744.945.0WTLG50.652.953.148.146.445.945.7Baseline38.5Table3:SEclassiﬁcationresultswithsequencingonBrowntestset.Boldcellindicatesaccuracyat-tainedbymodelparametersthatperformedbestondevelopmentdata.6.4ErroranalysisGiventhatasingleoneofthetenpossiblelabelsoccursformorethan35%ofclausesinbothdatasets,itisusefultolookatthedistributionofer-rorsoverthelabels.Table4isaconfusionmatrixfortheheld-outBrowndatausingthebestfeatureset.4Theﬁrstcolumngivesthelabelandnumberofoccurrencesofthatlabel,andthesecondcolumnistheaccuracyachievedforthatlabel.Thenexttwocolumnsshowthepercentageoferroneousla-belstakenbythelabelsSandGS.Thesetwolabelsarethemostcommonlabelsinthedevelopmentset(38.5%and32.5%).Theﬁnalcolumnsumstheper-centagesoferrorsassignedtotheremainingsevenlabels.Asonewouldexpect,themodellearnsthepredominanceofthesetwolabels.Thereareafewinterestingpointstomakeaboutthisdata.First,66%ofG-typeclausesaremistakenlyas-signedthelabelGS.ThisisinterestingbecausethesetwoSE-typesconstitutethebroaderSEcat-4Thankstotheanonymousreviewerwhosuggestedthisuse-fulwayoflookingatthedata.902

%Correct%IncorrectLabelLabelSGSOtherS(278)72.7n/a14.013.3E(203)50.737.011.80.5GS(203)44.846.3n/a8.9R(26)38.530.811.519.2N(47)23.431.923.421.3G(12)0.025.066.78.3IMP(8)0.075.025.00.0P(7)0.071.428.60.0F(2)0.0100.00.00.0Table4:ConfusionmatrixforBrownheld-outtestdata,WTLGfeatureset,lookbackn=2.Numbersinparenthesesindicatehowmanyclauseshavetheassociatedgoldstandardlabel.egoryofgeneralizingstatives.ThedistributionoferrorsforR-typeclausespointsoutanotherinterest-ingclassiﬁcationdifﬁculty.5Unliketheothercat-egories,thepercentageoffalse-otherlabelsforR-typeclausesishigherthanthatoffalse-GSlabels.80%ofthesefalse-otherlabelsareoftypeE.TheexplanationforthisisthatR-typeclausesareasub-typeoftheeventclass.6.5GenreeffectsinclassiﬁcationDifferenttextdomainsfrequentlyhavedifferentcharacteristicproperties.Discoursemodesareonewayofanalyzingthesedifferences.Itisthusin-terestingtocompareSEclassiﬁcationwhentrainingandtestingmaterialcomefromdifferentdomains.Table5showstheperformanceonBrownwhentrainingonBrownand/orMUCusingtheWTLGfeaturesetwithsimplelabelingandwithsequencepredictionwithalookbackoftwo.Anumberofthingsaresuggestedbytheseﬁgures.First,thela-belingmodel(lookbackofzero),beatsthebaselineevenwhentrainingonout-of-domaintexts(43.1%vs.38.5%),butthisisunsurprisinglyfarbelowtrainingonin-domaintexts(43.1%vs.50.6%).Second,whilesequencepredictionhelpswithin-domaintraining(53.1%vs50.6%),itmakesnodifferencewithout-of-domaintraining(42.9%vs43.1%).ThisindicatesthatthepatternsofSEsinatextdoindeedcorrelatewithdomainsandtheirdis-coursemodes,inlinewithcase-studiesinthedis-coursemodestheory(Smith,2003).Finally,mix-5Thankstoananonymousreviewerforbringingthistoourattention.lookbackBrowntestsetWTLGtrain:Brown050.6253.1train:MUC043.1242.9train:all050.4249.5Table5:Cross-domainSEclassiﬁcationingout-of-domaintrainingmaterialwithin-domainmaterialdoesnothurtlabellingaccuracy(50.4%vs50.6%),butitdoestakeawaythegainsfromse-quencing(49.5%vs53.1%).Thesegenreeffectsaresuggestive,butinconclu-sive.Asimilarsetupwithmuchlargertrainingandtestingsetswouldbenecessarytoprovideaclearerpictureoftheeffectofmixeddomaintraining.7RelatedworkThoughweareawareofnopreviousworkinSEclassiﬁcation,othershavefocusedonautomaticde-tectionofaspectualandtemporaldata.KlavansandChodorow(1992)laidthefounda-tionforprobabilisticverbclassiﬁcationwiththeirinterpretationofaspectualpropertiesasgradientandtheiruseofstatisticstomodelthegradience.Theyimplementasinglelinguistictestforstativity,treat-inglexicalpropertiesofverbsastendenciesratherthanabsolutecharacteristics.LinguisticindicatorsforaspectualclassiﬁcationarealsousedbySiegel(1999),whoevaluates14in-dicatorstotestverbsforstativityandtelicity.Manyofhisindicatorsoverlapwithourfeatures.SiegelandMcKeown(2001)addressclassiﬁca-tionofverbsforstativity(eventvs.state)andforcompletedness(culminatedvs.non-culminatedevents).Theycomparethreesupervisedandoneun-supervisedmachinelearningsystems.Thesystemsobtainrelativelyhighaccuracyﬁgures,buttheyaredomain-speciﬁc,requireextensivehumansupervi-sion,anddonotaddressaspectualcoercion.MerloandStevenson(2001)usecorpus-basedthematicroleinformationtoidentifyandclassifyunergative,unaccusative,andobject-dropverbs.StevensonandMerlonotethatstatisticalanalysiscannotandshouldnotbeseparatedfromdeeperlin-guisticanalysis,andourresultssupportthatclaim.903

Theadvantagesofourapproacharethebroadenedconceptionoftheclassiﬁcationtaskandtheuseofsequencepredictiontocaptureawidercontext.8ConclusionsSituationentityclassiﬁcationisalittle-studiedbutimportantclassiﬁcationtaskfortheanalysisofdis-course.Wehavepresentedtheﬁrstdata-drivenmod-elsforSEclassiﬁcation,motivatingthetreatmentofSEclassiﬁcationasasequencingtask.Wehaveshownthatlinguisticcorrelationstosit-uationentitytypeareusefulfeaturesforproba-bilisticmodels,asaregrammaticalrelationsandCCGsupertagsderivedfromsyntacticanalysisofclauses.Modelsforthetaskperformpoorlygivenverybasicfeaturesets,butminimallinguisticpro-cessingintheformofpart-of-speechtaggingim-provesperformanceevenonsmalldatasetsusedforthisstudy.Performanceimprovesevenmorewhenwemovebeyondsimplefeaturesetsandincorpo-ratelinguistically-motivatedfeaturesandgrammat-icalrelationsfromdeepsyntacticanalysis.Finally,usingsequencepredictionbyadaptingaPOS-taggerfurtherimprovesresults.Thetaggerweadaptedusesbeamsearch;thisal-lowstractableuseofmaximumentropyforeachla-belingdecisionbutforgoestheabilitytoﬁndtheoptimallabelsequenceusingdynamicprogrammingtechniques.Incontrast,ConditionalRandomFields(CRFs)(Laffertyetal.,2001)allowtheuseofmax-imumentropytosetfeatureweightswithefﬁcientrecoveryoftheoptimalsequence.ThoughCRFsaremorecomputationallyintensive,thesmallsetofSElabelsshouldmakethetasktractableforCRFs.Infuture,weintendtotesttheutilityofSEsindis-courseparsing,discoursemodeidentiﬁcation,anddiscourserelationprojection.AcknowledgmentsThisworkwassupportedbytheMorrisMemorialTrustGrantfromtheNewYorkCommunityTrust.TheauthorswouldliketothankNicholasAsher,PascalDenis,KatrinErk,GarrettHeifrin,JulieHunter,JonasKuhn,RayMooney,BrianReese,andtheanonymousreviewers.ReferencesN.Asher.1993.ReferencetoAbstractobjectsinDis-course.KluwerAcademicPublishers.A.Berger,S.DellaPietra,andV.DellaPietra.1996.Amaximumentropyapproachtonaturallanguagepro-cessing.ComputationalLinguistics,22(1):39–71.G.CarlsonandF.J.Pelletier,editors.1995.TheGenericBook.UniversityofChicagoPress,Chicago.S.ClarkandJ.R.Curran.2004.ParsingtheWSJusingCCGandlog–linearmodels.InProceedingsofACL–04,pages104–111,Barcelona,Spain.D.Dowty.1979.WordMeaningandMontagueGram-mar.Reidel,Dordrecht.J.Hockenmaier,G.Bierner,andJ.Baldridge.2004.Ex-tendingthecoverageofaCCGsystem.ResearchonLanguageandComputation,2:165–208.J.L.KlavansandM.S.Chodorow.1992.Degreesofstativity:Thelexicalrepresentationofverbaspect.InProceedingsofCOLING14,Nantes,France.J.Lafferty,A.McCallum,andF.Pereira.2001.Con-ditionalrandomﬁelds:Probabilisticmodelsforseg-mentingandlabellingsequencedata.InProceedingsofICML,pages282–289,Williamstown,USA.P.MerloandS.Stevenson.2001.Automaticverbclas-siﬁcationbasedonstatisticaldistributionsofargumentstructure.ComputationalLinguistics.M.MoensandM.Steedman.1988.Temporalontol-ogyandtemporalreference.ComputationalLinguis-tics,14(2):15–28.P.Peterson.1997.FactPropositionEvent.Kluwer.E.V.SiegelandK.R.McKeown.2001.Learningmeth-odstocombinelinguisticindicators:Improvingas-pectualclassiﬁcationandrevealinglinguisticinsights.ComputationalLinguistics,26(4):595–628.E.V.Siegel.1999.Corpus-basedlinguisticindicatorsforaspectualclassiﬁcation.InProceedingsofACL37,UniversityofMaryland,CollegePark.C.S.Smith.1991.TheParameterofAspect.Kluwer.C.S.Smith.2003.ModesofDiscourse.CambridgeUniversityPress.M.Steedman.2000.TheSyntacticProcess.MITPress/BradfordBooks.Z.Vendler,1967.LinguisticsinPhilosophy,chapterVerbsandTimes,pages97–121.CornellUniversityPress,Ithaca,NewYork.H.Verkuyl.1972.OntheCompositionalNatureoftheAspects.Reidel,Dordrecht.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 904–911,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

904

WordsandEchoes:AssessingandMitigatingtheNon-RandomnessProbleminWordFrequencyDistributionModelingMarcoBaroniCIMeC(UniversityofTrento)C.soBettini3138068Rovereto,Italymarco.baroni@unitn.itStefanEvertIKW(UniversityofOsnabr¨uck)Albrechtstr.2849069Osnabr¨uck,Germanystefan.evert@uos.deAbstractFrequencydistributionmodelstunedtowordsandotherlinguisticeventscanpre-dictthenumberofdistincttypesandtheirfrequencydistributioninsamplesofarbi-trarysizes.Weconduct,fortheﬁrsttime,arigorousevaluationofthesemodelsbasedoncross-validationandseparationoftrain-ingandtestdata.Ourexperimentsrevealthatthepredictionaccuracyofthemodelsismarredbyseriousoverﬁttingproblems,duetoviolationsoftherandomsamplingas-sumptionincorpusdata.Wethenproposeasimplepre-processingmethodtoallevi-atesuchnon-randomnessproblems.Furtherevaluationconﬁrmstheeffectivenessofthemethod,whichcomparesfavourablytomorecomplexcorrectiontechniques.1IntroductionLarge-Number-of-Rare-Events(LNRE)models(Baayen,2001)areaclassofspecializedstatisticalmodelsthatallowustoestimatethecharacteristicsofthedistributionoftypeprobabilitiesintype-richlinguisticpopulations(suchaswords)fromlimitedsamples(ourcorpora).Theyalsoallowustoextrapolatequantitiessuchasvocabularysize(thenumberofdistincttypes)andthenumberofhapaxes(typesoccurringjustonce)beyondagivencorpusormakepredictionsforcompletelyunseendatafromthesameunderlyingpopulation.LNREmodelshaveapplicationsintheoreticallin-guistics,e.g.forcomparingthetyperichnessofmor-phologicalorsyntacticprocessesthatareattestedtodifferentdegreesinthedata(Baayen,1992).Con-siderforexampleaverycommonpreﬁxsuchasre-andaratherrarepreﬁxsuchasmeta-.WithLNREmodelswecananswerquestionssuchas:Ifwecouldobtainasmanytokensofmeta-aswehaveofre-,wouldwealsoseeasmanydistincttypes?Inotherwords,isthepreﬁxmeta-asproductiveasthepreﬁxre-?PracticalNLPapplications,ontheotherhand,includeestimatinghowmanyout-of-vocabularywordswewillencountergivenalexiconofacertainsize,ormakinginformedguessesabouttypecountsinverylargedatasets(e.g.,howmanytyposarethereontheInternet?)Inthispaper,afterintroducingLNREmodels(Section2),wepresentanevaluationoftheirper-formancebasedonseparatetrainingandtestdataaswellascross-validation(Section3).Asfarasweknow,thisistheﬁrsttimethatsucharigorousevaluationhasbeenconducted.Theresultsshowhowevaluatingonthetrainingset,acommonstrat-egyinLNREresearch,favoursmodelsthatoverﬁtthetrainingdataandperformpoorlyonunseendata.TheyalsoconﬁrmtheobservationbyEvertandBa-roni(2006)thatcurrentLNREmodelsachieveonlyunsatisfactorypredictionaccuracy,andthisistheis-sueweturntointhesecondpartofthepaper(Sec-tion4).Havingidentiﬁedtheviolationoftheran-domsamplingassumptionbyreal-worlddataasoneofthemainfactorsaffectingthequalityofthemod-els,wepresentanewapproachtoalleviatingnon-randomnessproblems.FurtherevaluationshowsoursolutiontooutperformBaayen’s(2001)partition-adjustmentmethod,theformerstate-of-the-artinnon-randomnesscorrection.Section5concludesby905

pointingoutdirectionsforfuturework.2LNREmodelsBaayen(2001)introducesafamilyofmodelsforZipf-likefrequencydistributionsoflinguisticpop-ulations,referredtoasLNREmodels.Suchalin-guisticpopulationisformallydescribedbyaﬁniteorcountablyinﬁnitesetoftypesωiandtheiroccur-renceprobabilitiesπi.Wordfrequencymodelsarenotconcernedwiththeprobabilities(i.e.,relativefrequencies)ofspeciﬁcindividualtypes,butrathertheoveralldistributionoftheseprobabilities.Numberingthetypesinorderofdecreasingprob-ability(π1≥π2≥π3≥...,calledapopula-tionZipfranking),wecanspecifyaLNREmodelfortheirdistributionasafunctionthatcomputesπifromtheZipfrankiofωi.Forinstance,theZipf-Mandelbrotlaw1isdeﬁnedbytheequationπi=C(i+b)a(1)withparametersa>1andb>0.Itismathemati-callymoreconvenienttoformulateLNREmodelsintermsofatypedensityfunctiong(π)ontheintervalπ∈[0,1],suchthatZBAg(π)dπ(2)isthe(approximate)numberoftypesωiwithA≤πi≤B.Evert(2004)showsthatZipf-Mandelbrotcorrespondstoatypedensityoftheformg(π):=(C·π−α−1A≤π≤B0otherwise(3)withparameters0<α<1and0≤A<B.2Modelsthatareformulatedintermsofsuchatypedensityghavemanydirectapplications(e.g.usinggasaBayesianprior),andwerefertothemasproperLNREmodels.AssumingthatacorpusofNtokensisarandomsamplefromsuchapopulation,wecanmakepre-dictionsaboutlexicalstatisticssuchasthenumber1TheZipf-MandelbrotlawisanextensionofZipf’slaw(whichhasa=1andb=0).Whilethelatteroriginallyreferstotypefrequenciesinagivensample,theZipf-Mandelbrotlawisformulatedfortypeprobabilitiesinapopulation.2Inthisequation,CisanormalizingconstantrequiredinordertoensureR10πg(π)dπ=1,theequivalentofPiπi=1.V(N)ofdifferenttypesinthecorpus(thevocab-ularysize),thenumberV1(N)ofhapaxlegomena(typesoccurringjustonce),aswellasthefurtherdis-tributionoftypefrequenciesVm(N).Sincethepre-cisevalueswouldbedifferentfromsampletosam-ple,themodelpredictionsaregivenbyexpectationsE[V(N)]andE[Vm(N)],whichcanbecomputedwithrelativeeasefromthetypedensityfunctiong.BycomparingexpectedandobservedvaluesofVandVm(forthelowestfrequencyranks,usuallyuptom=15),theparametersofaLNREmodelcanbeestimated(werefertothisastrainingthemodel),allowinginferencesaboutthepopulation(suchasthetotalnumberoftypesinthepopulation)aswellasfurtherapplicationsoftheestimatedtypedensity(e.g.forGood-Turingsmoothing).Sincewecancal-culateexpectedvaluesforsamplesofarbitrarysizeN,wecanusethetrainedmodeltopredicthowmanynewtypeswouldbeseeninalargercorpus,howmanyhapaxestherewouldbe,etc.ThiskindofvocabularygrowthextrapolationhasbecomeoneofthemostimportantapplicationsofLNREmodelsinlinguisticsandNLP.AdetailedaccountofthemathematicsofLNREmodelscanbefoundinBaayen(2001,Ch.2).BaayendescribestwoLNREmodels,lognormalandGIGP,aswellasseveralotherapproaches(in-cludingaversionofZipf’slawandtheYule-Simonmodel)thatarenotbasedonatypedensityandhencedonotqualifyasproperLNREmodels.TwoLNREmodelsbasedonZipf’slaw,ZMandfZM,areintroducedbyEvert(2004).Inthefollowing,wewillonlyconsiderproperLNREmodelsbecauseoftheirconsiderablygreaterutility,andbecausetheirperformanceinextrapo-lationtasksappearstobebetterthan,oratleastcomparableto,theothermodels(EvertandBaroni,2006).Inaddition,weexcludethelognormalmodelbecauseofitscomputationalcomplexityandnumer-icalinstability.3Ininitialevaluationexperiments,theperformanceoflognormalwasalsoinferiortotheremainingthreemodels(ZM,fZMandGIGP).NotethatZMisthemostsimplisticmodel,withonly2parametersandassuminganinﬁnitepopulationvocabulary,whilefZMandGIGPhave3parameters3Therearenoclosedformequationsfortheexpectationsofthelognormalmodel,whichhavetobecalculatedbynumericalintegration.906

andcanmodelpopulationsofdifferentsizes.3EvaluationofLNREmodelsLNREmodelsaretraditionallyevaluatedbylook-ingathowwellexpectedvaluesgeneratedbythemﬁtempiricalcountsextractedfromthesamedata-setusedforparameterestimation,oftenbyvisualinspectionofdifferencesbetweenobservedandpre-dicteddatainplots.Morerigorously,Baayen(2001)andEvert(2004)comparethefrequencydistribu-tionobservedinthetrainingsettotheonepredictedbythemodelwithamultivariatechi-squaredtest.Aswewillshowbelow,evaluatingstandardLNREmodelsonthesamedatathatwereusedtoestimatetheirparametersfavoursoverﬁtting,whichresultsinpoorperformanceonunseendata.EvertandBaroni(2006)attempt,fortheﬁrsttime,toevaluateLNREmodelsonunseendata.However,ratherthansplittingthedataintoseparatetrainingandtestsets,theyevaluatethemodelsinanextra-polationsetting,wheretheparametersofthemodelareestimatedonasubsetofthedatausedfortesting.EvertandBaronidonotattempttocross-validatetheresults,andtheydonotprovideaquantitativeevalu-ation,relyinginsteadonvisualinspectionofempir-icalandobservedvocabularygrowthcurves.3.1DataandprocedureWeranourexperimentswiththreecorporaindiffer-entlanguagesandrepresentingdifferenttextualty-pologies:theBritishNationalCorpus(BNC),a“bal-anced”corpusofBritishEnglishofabout100mil-liontokensillustratingdifferentcommunicativeset-tings,genresandtopics;thedeWaCcorpus,aWeb-crawledcorpusofabout1.5billionGermanwords;andthelaRepubblicacorpus,anItaliannewspapercorpusofabout380millionwords.4Fromeachcorpus,weextracted20non-overlappingsamplesofrandomlyselecteddocu-ments,amountingtoatotalof4milliontokenseach(punctuationmarksandentirelynon-alphabeticalto-kenswereremovedbeforesampling,andallwordswereconvertedtolowercase).Eachofthesesam-pleswasthensplitintoatrainingsetof1millionto-kens(thetrainingsizeN0)andatestsetof3million4Seewww.natcorp.ox.ac.uk,http://wacky.sslmit.unibo.itandhttp://sslmit.unibo.it/repubblicatokens.ThedocumentsinthelaRepubblicasam-pleswereorderedchronologicallybeforesplitting,tosimulateatypicalscenarioarisingwhenworkingwithnewspaperdata,wherethedataavailablefortrainingprecede,chronologically,thedataonewantstogeneralizeto.WeestimateparametersoftheZM,fZMandGIGPmodelsoneachtrainingset,usingthezipfRtoolkit.5Themodelsarethenusedtopredicttheexpectednumberofdistincttypes,i.e.,vocabularysizeV,atsamplesizesof1,2and3milliontokens,equivalentto1,2and3timesthesizeofthetrainingset(werefertotheseasthepredictionsizesN0,2N0and3N0,respectively).Finally,theexpectedvo-cabularysizeE[V(N)]iscomparedtotheobservedvalueV(N)inthetestsetforN=N0,N=2N0andN=3N0.WealsolookatV1(N),thenumberofhapaxlegomena,inthesameway.OurmainfocusisVprediction,sincethisisbyfarthemostusefulmeasureinpracticalapplica-tions,wherewearetypicallyinterestedinknowinghowmanytypes(orhowmanytypesbelongingtoacertaincategory)wewillseeasoursamplesizeincreases(HowmanytyposarethereontheWeb?Howmanytypeswithpreﬁxmeta-wouldweseeifwehadasmanytypesofmeta-aswehaveofre-?)Hapaxlegomenacounts,ontheotherhand,playacentralroleinquantifyingmorphologicalpro-ductivity(Baayen,1992)andtheygiveusaﬁrstin-sightintohowgoodthemodelsareatpredictingfre-quencydistributions,besidesvocabularysize(aswewillsee,amodel’ssuccessinpredictingVdoesnotnecessaryimplythatthemodelisalsocapturingtherightfrequencydistribution).Forallmodels,corporaandpredictionsizes,goodness-of-ﬁtofthemodelonthetrainingsetismeasuredwithamultivariatechi-squaredtest(Baayen,2001,118-122).Performanceofthemod-elsinpredictionofVisassessedviarelativeerror,computedforeachofthe20samplesfromacorpusandthe3predictionsizesasfollows:e=E[V(N)]−V(N)V(N)whereN=k·N0isthepredictionsize(fork=1,2,3),V(N)istheobservedVintherelevanttest5http://purl.org/stefan.evert/zipfR907

setatsizeN,andE[V(N)]isthecorrespondingex-pectedVpredictedbyamodel.6Foreachcorpusandpredictionsizeweobtain20valuesei(viz.,e1,...,e20).Asasummarymeasure,wereportthesquarerootofthemeansquarerelativeerror(rMSE)calculatedaccordingto√rMSE=vuut120·20Xi=1(ei)2Thisgivesusanoverallassessmentofpredictionac-curacy(wetakethesquareroottoobtainvaluesonthesamescaleasrelativeerrors,andthuseasiertointerpret).WecomplementrMSEswithreportsontheaveragerelativeerror(indicatingwhetherthereisasystematicunder-oroverestimationbias)anditsasymptotic95%conﬁdenceintervals,basedontheempiricalstandarddeviationoftheeiacrossthe20trials(theconﬁdenceintervalsareusuallysomewhatlargerthantheactualrangeofvaluesfoundintheexperiments,sotheyshouldbeseenas“pessimisticestimates”oftheactualvariance).3.2ResultsThepanelsofFigure1reportrMSEvaluesforthe3corporaandforeachpredictionsize.Fornow,wefocusontheﬁrst3histogramsofeachpanel,thatpresentrMSEsforthe3LNREmodelsintro-ducedabove:ZM,fZMandGIGP(theremaininghistogramswillbediscussedlater).7ForallcorporaandallextrapolationsizesbeyondN0,thesimpleZMmodeloutperformsthemoreso-phisticatedfZMandGIGPmodels(whichseemtobeverysimilartoeachother).Evenatthelargestpredictionsizeof3N0,ZM’srMSEiswellbelow10%,whereastheothermodelshave,intheworstcase(BNC3N0),arMSEabove15%.Figure2presentsplotsofaveragerelativeerroranditsem-piricalconﬁdenceintervals(again,focusfornowontheZM,fZMandGIGPresults;therestoftheﬁgureisdiscussedlater).Weseethatthepoorperformance6WenormalizebyV(N)ratherthan(afunctionof)E[V(N)]becauseinthelattercasewewouldfavourmodelsthatoverestimateV,comparedtoonesthatareequally“close”tothecorrectvaluebutunderestimateV.7Atablewiththefullnumericalresultsisavailableuponrequest;weﬁnd,however,thatgraphicalsummariessuchasthosepresentedinthispapermaketheresultseasiertointerpret.offZMandGIGPisduetotheirtendencytounder-estimatethetruevocabularysizeV,whilevarianceiscomparableacrossmodels.TherMSEsofV1predictionarereportedinFig-ure3.V1predictionperformanceispooreracrosstheboard,andZMisnolongeroutperformingtheothermodels.Forspacereasons,wedonotpresentrelativeerrorandvarianceplotsforV1,butthegen-eraltrendsarethesameobservedforV,exceptthatthebiasofZMtowardsV1overestimationismuchclearerthanforV.Interestingly,goodness-of-ﬁtonthetrainingdataisnotagoodpredictorofVandV1predictionper-formanceonunseendata.ThisisshowninFigure4,whichplotsrMSEforpredictionofVagainstgoodness-of-ﬁt(quantiﬁedbymultivariateX2onthetrainingset,asdiscussedabove)forallcorporaandLNREmodelsatthe3N0predictionsize(butthesamepatternsemergeatotherpredictionsizesandwithV1).ThelargerX2,thepoorerthetrainingsetﬁt;thelargerrMSE,theworsetheprediction.Thus,ideally,weshouldseeapositivecorrelationbetweenX2andrMSE.Focusingfornowonthecircles(pin-pointingtheZM,fZMandGIGPmodels),weseethatthereisinsteadanegativecorrelationbetweengoodnessofﬁtonthetrainingsetandqualityofpre-dictiononunseendata.8First,theseresultsindicatethat,ifwetakegood-nessofﬁtonthetrainingsetasacriterionforchoos-ingthebestmodel(asdonebyBaayenandEvert),weendupselectingtheworstmodelforactualpre-dictiontasks.Thisis,webelieve,averystrongcaseforapplyingthesplittrain-testcross-validationmethodusedinotherareasofstatisticalNLPtofre-quencydistributionmodeling.Second,thedatasug-gestthatthemoresophisticatedmodelsareoverﬁt-tingthetrainingset,leadingtopoorerperformancethanthesimplerZMonunseendata.Weturnnowtowhatwethinkisthemaincauseforthisoverﬁtting.4Non-randomnessandechoesTheresultsintheprevioussectionindicatethattheVspredictedbyLNREmodelsareatbest“ballparkestimates”(andV1predictions,witharelativeerrorthatisoftenabove20%,donotevenqualifyasplau-8Withcorrelationcoefﬁcientsofr<−.8,signiﬁcantatthe0.01leveldespitethesmallsamplesize.908

ZMfZMGIGPfZMechoGIGPechoGIGPpartitionN02N03N0rMSE for E[V] vs. V on test set (BNC)rMSE    (%)05101520ZMfZMGIGPfZMechoGIGPechoGIGPpartitionN02N03N0rMSE for E[V] vs. V on test set (DEWAC)rMSE    (%)05101520ZMfZMGIGPfZMechoGIGPechoGIGPpartitionN02N03N0rMSE for E[V] vs. V on test set (REPUBBLICA)rMSE    (%)05101520Figure1:rMSEsofpredictedVontheBNC,deWaCandlaRepubblicadata-setsZMfZMGIGPfZMechoGIGPechoGIGPpartitionRelative error: E[V] vs. V on test set (BNC)relative error (%)−40−2002040lllllllN02N03N0ZMfZMGIGPfZMechoGIGPechoGIGPpartitionRelative error: E[V] vs. V on test set (DEWAC)relative error (%)−20−1001020lllllllN02N03N0ZMfZMGIGPfZMechoGIGPechoGIGPpartitionRelative error: E[V] vs. V on test set (REPUBBLICA)relative error (%)−20−1001020lllllllN02N03N0Figure2:Averagerelativeerrorsandasymptotic95%conﬁdenceintervalsofVpredictiononBNC,deWaCandlaRepubblicadata-setsZMfZMGIGPfZMechoGIGPechoGIGPpartitionN02N03N0rMSE for E[V1] vs. V1 on test set (BNC)rMSE    (%)01020304050ZMfZMGIGPfZMechoGIGPechoGIGPpartitionN02N03N0rMSE for E[V1] vs. V1 on test set (DEWAC)rMSE    (%)01020304050ZMfZMGIGPfZMechoGIGPechoGIGPpartitionN02N03N0rMSE for E[V1] vs. V1 on test set (REPUBBLICA)rMSE    (%)01020304050Figure3:rMSEsofpredictedV1ontheBNC,deWaCandlaRepubblicadata-sets909

050001000015000051015Accuracy for V on test set (3N0)X2rMSE    (%)llllllllllstandardechomodelpartition−adjustedFigure4:CorrelationbetweenX2andVpredictionrMSEacrosscorporaandmodelssibleballparkestimates).Althoughsuchroughesti-matesmightbemorethanadequateformanypracti-calapplications,isitpossibletofurtherimprovethequalityofLNREpredictions?Amajorfactorhamperingpredictionqualityisthatrealtextsmassivelyviolatetherandomnessas-sumptionmadeinLNREmodeling:words,ratherobviously,arenotpickedatrandomonthebasisoftheirpopulationprobability(EvertandBaroni,2006;Baayen,2001).Thetopic-driven“clumpi-ness”oflowfrequencycontentwordsreducesthenumberofhapaxlegomenaandotherrareeventsusedtoestimatetheparametersofLNREmodels,leadingthemodelstounderestimatethetyperich-nessofthepopulation.Interestingly(butunsurpris-ingly),ZMwithitsassumptionofaninﬁnitepop-ulation,islesspronetothiseffect,andthusithasabetterpredictionperformancethanthemoreso-phisticatedfZMandGIGPmodels,despiteitspoorgoodness-of-ﬁt.Theeffectofnon-randomnessisillustratedveryclearlyfortheBNC(butthesamecouldbeshownfortheothercorpora)byFigure5,acomparisonofrMSEforpredictionofVfromourexperimentsabovetoresultsobtainedonversionsoftheBNCsampleswithwordsscrambledinrandomorder,thusforciblyremovingnon-randomnesseffects.WeseefromthisﬁgurethattheperformanceofbothfZMandGIGPimprovesdramaticallywhentheyaretrainedandtestedonrandomizedsequencesofwords.Interestingly,randomizationhasinsteadanegativeeffectonZMperformance.ZMfZMGIGPZMrandomfZMrandomGIGPrandomN02N03N0rMSE for E[V] vs. V on test set (BNC)rMSE    (%)05101520Figure5:rMSEsofpredictedVonunmodiﬁedvs.randomizedversionsoftheBNCsets4.1Previousapproachestonon-randomnessWhilenon-randomnessiswidelyacknowledgedasaseriousproblemforthestatisticalanalysisofcor-pusdata,veryfewauthorshavesuggestedcorrec-tionstrategies.Thekeyproblemofnon-randomdataseemstobethattheoccurrencefrequenciesofatypeindifferentdocumentsdonotfollowthebinomialdistributionassumedbyrandomsamplingmodels.Oneapproachisthereforetomodelthisdistribu-tionexplicitly,replacingthebinomialwithitssin-gleparameterπbyamorecomplexdistributionthathasadditionalparameters(ChurchandGale,1995;Katz,1996).However,thesedistributionsarecur-rentlynotapplicabletoLNREmodeling,whichisbasedontheoverallfrequenciesoftypesinacor-pusratherthantheirfrequenciesinindividualdoc-uments.Theoverallfrequenciescanonlybecalcu-latedbysummationoveralldocumentsinthecor-pus,resultinginamathematicallyandnumericallyintractablemodel.Inaddition,thetypedensityg(π)wouldhavetobeextendedtoamulti-dimensionalfunction,requiringalargenumberofparameterstobeestimatedfromthedata.Baayen(2001)suggestsadifferentapproach,whichpartitionsthepopulationinto“normal”typesthatsatisfytherandomsamplingassumption,and“totallyunderdispersed”types,whichareassumedtoconcentratealloccurrencesinthecorpusintoa910

single“burst”.UsingastandardLNREmodelforthenormalpartofthepopulationandasimplelin-eargrowthmodelfortheunderdispersedpart,ad-justedvaluesforE[V]andE[Vm]caneasilybecal-culated.Theseso-calledpartition-adjustedmodels(whichintroduceoneadditionalparameter)arethustheonlyviablemodelsfornon-randomnesscorrec-tioninLNREmodelingandhavetobeconsideredthestateoftheart.4.2EchoadjustmentRatherthanmakingmorecomplexassumptionsaboutthepopulationdistributionorthesamplingmodel,weproposethatnon-randomnessshouldbetackledasapre-processingproblem.Theissue,weargue,isreallywiththewaywecountoccurrencesoftypes.Thefactthatararetopic-speciﬁcwordoc-curs,say,fourtimesinasingledocumentdoesnotmakeitanylessahapaxlegomenonforourpurposesthanifthewordoccurredonce(thisisthecase,forexample,ofthewordchondriticintheBNC,whichoccurs4times,allinthesamescientiﬁcdocument).Weoperationalizeourintuitionbyproposingthat,forourpurposes,eachcontentword(atleasteachrare,topic-speciﬁccontentword)occursmaximallyonceinadocument,andallotherinstancesofthatwordinthedocumentarereallyinstancesofaspe-cial“anaphoric”type,whosefunctionisthatof“echoing”thecontentwordsinthedocument.Thus,intheBNCdocumentmentionedabove,thewordchondriticiscountedonlyonce,whereastheotherthreeoccurrencesareconsideredastokensoftheechotype.Thus,wearecountingwhatinthein-formationretrievalliteratureisknownasdocumentfrequencies.Intuitively,thesearelesssusceptibletotopicalclumpinesseffectsthanplaintokenfrequen-cies.However,byreplacingrepeatedwordswithechotokens,wecansticktoasamplingmodelbasedonrandomwordtokensampling(ratherthandocu-mentsampling),sothattheLNREmodelscanbeapplied“asis”toecho-adjustedcorpora.Echo-adjustmentdoesnotaffectthesamplesizeNnorthevocabularysizeV,makingtheinterpre-tationofresultsobtainedwithecho-adjustedmod-elsentirelystraightforward.Ndoesnotchangebe-causerepeatedtypesarereplacedwithechotokens,notdeleted.Vdoesnotchangebecauseonlyre-peatedtypesarereplaced.Thus,notypepresentintheoriginalcorpusdisappears(moreprecisely,Vin-creasesby1becauseoftheadditionoftheechotype,butgiventhelargesizeofVthiscanbeignoredforallpracticalpurposes).Thus,theexpectedVcom-putedforaspeciﬁedsamplesizeNwithamodeltrainedonanecho-adjustedcorpuscanbedirectlycomparedtoobservedvaluesatN,andtopredic-tionsmadeforthesameNbymodelstrainedonanunprocessedcorpus.Thesameisnottrueforthepre-dictionofthefrequencydistribution,where,forthesameN,echo-basedmodelspredictthedistributionofdocumentfrequencies.Weareproposingechoesasamodelfortheus-ageof(rare)contentwords.Itwouldbedifﬁ-culttodecidewheretheboundaryisbetweentop-icalwordsthatareinsertedonceinadiscourseandthenanaphoricallymodulatedand“general-purpose”wordsthatconstitutetheframeofthedis-courseandcanoccurmultipletimes.Luckily,wedonothavetomakethisdecisionwhenestimatingaLNREmodel,sincemodelﬁttingisbasedonthedistributionofthelowestfrequencies.Forexample,withthedefaultzipfRmodelﬁttingsetting,onlythelowest15spectrumelementsareusedtoﬁtthemod-els.Foranyreasonablysizedcorpus,itisunlikelythatfunctionwordsandcommoncontentwordswilloccurinlessthan16documents,andthustheirdis-tributionwillbeirrelevantformodelﬁtting.Thus,wecanignoretheissueofwhatistheboundarybe-tweentopicalwordstobeecho-adjustedandgeneralwords,aslongaswecanbeconﬁdentthatthesetoflowestfrequencywordsusedformodelﬁttingbe-longtothetopicalset.9Thismakespracticalecho-adjustmentextremelysimple,sinceallwehavetodoistoreplaceallrepetitionsofawordinthesamedocumentwithechotokens,andestimatetheparam-etersofaplainLNREmodelwiththeresultingver-sionofthetrainingcorpus.4.3ExperimentswithechoadjustmentUsingthesametrainingandtestsetsasinSec-tion3.1,wetrainthepartition-adjustedGIGPmodel9TheissuebecomesmoredelicateifwewanttopredictthefrequencyspectrumratherthanV,sinceamodeltrainedonecho-adjusteddatawillpredictecho-adjustedfrequenciesacrosstheboard.However,inmanytheoreticalandpracticalsettingsonlythelowestfrequencyspectrumelementsareofin-terest,where,again,itissafetoassumethatwordsarehighlytopic-dependent,andecho-adjustmentisappropriate.911

implementedintheLEXSTATStoolkit(Baayen,2001).Weestimatetheparametersofecho-adjustedZM,fZMandGIGPmodelsonversionsofthetrain-ingcorporathathavebeenpre-processedasde-scribedabove.TheperformanceofthemodelsisevaluatedwiththesamemeasuresasinSection3.1(forpredictionofV1,echo-adjustedversionsofthetestdataareused).Figure1reportstheperformanceoftheecho-adjustedfZMandGIGPmodelsandofpartition-adjustedGIGP(echo-adjustedZMperformedsys-tematicallymuchworsethantheotherecho-adjustedmodelsandtypicallyworsethanuncorrectedZM,anditisnotreportedintheﬁgure).Bothcorrectionmethodsleadtoadramaticimprovement,bringingthepredictionperformanceoffZMandGIGPtolev-elscomparabletoZM(withthelatteroutperformingthecorrectedmodelsontheBNC,butbeingoutper-formedonlaRepubblica).Moreover,echo-adjustedGIGPisasgoodaspartitionedGIGPonlaRepub-blica,andbetteronbothBNCanddeWaC,suggest-ingthatthemuchsimplerecho-adjustmentmethodisatleastasgoodandprobablybetterthanBaayen’spartitioning.ThemeanerrorandconﬁdenceintervalplotsinFigure2showthattheecho-adjustedmodelshaveamuchweakerunderestimationbiasthanthecorrespondingunadjustedmodels,andarecompara-bleto,ifnotbetterthan,ZM(althoughtheymighthaveatendencytodisplaymorevariance,asclearlyillustratedbytheperformanceofecho-adjustedfZMonlaRepubblicaat3N0predictionsize).Finally,theecho-adjustedmodelsclearlystandoutwithre-specttoZMwhenitcomestoV1prediction(Fig-ure3),indicatingthatecho-adjustedversionsofthemoresophisticatedfZMandGIGPmodelsshouldbethefocusoffutureworkonimprovingpredic-tionofthefullfrequencydistribution,ratherthanplainZM.Moreover,echo-adjustedGIGPisoutper-formingpartitionedGIGP,andemergingasthebestmodeloverall.10Reassuringly,fortheechoedmod-elsthereisaverystrongpositivecorrelationbetweengoodness-of-ﬁtonthetrainingsetandqualityofpre-diction,asillustratedforVpredictionat3N0bythetrianglesinFigure4(again,thepatternsinthis10InlookingattheV1data,itmustbekeptinmind,how-ever,thatV1hasadifferentinterpretationwhenpredictedbyecho-adjustedmodels,i.e.,itisthenumberofdocument-basedhapaxes,thenumberoftypesthatoccurinonedocumentonly.ﬁgurerepresentthegeneraltrendforecho-adjustedmodelsfoundinallsettings).11Thisindicatesthattheover-ﬁttingproblemhasbeenresolved,andforecho-adjustedmodelsgoodness-of-ﬁtonthetrain-ingsetisareliableindicatorofpredictionaccuracy.5ConclusionDespitetheencouragingresultswereported,muchwork,ofcourse,remainstobedone.Evenwiththeecho-adjustedmodels,predictionofV1suffersfromlargeerrorsandpredictionofVquicklydeteri-orateswithincreasingpredictionsizeN.Ifthemod-els’estimatesfor3timesthesizeofthetrainingsethaveacceptableerrorsofaround5%,formanyap-plicationswemightwanttoextrapolateto100N0ormore(recalltheexampleofestimatingtypecountsfortheentireWeb).Moreover,echo-adjustedmod-elsmakepredictionspertainingtothedistributionofdocumentfrequencies,ratherthanplaintokenfre-quencies.Thefullimplicationsofthisremaintobeinvestigated.Finally,futureworkshouldsystem-aticallyexploretowhatextentdifferenttextualty-pologiesareaffectedbythenon-randomnessprob-lem(notice,e.g.,thatnon-randomnessseemstobeagreaterproblemfortheBNCthanforthemoreuni-formlaRepubblicacorpus).ReferencesBaayen,Harald.1992.Quantitativeaspectsofmorpho-logicalproductivity.YearbookofMorphology1991,109-150.Baayen,Harald.2001.Wordfrequencydistributions.Dordrecht:Kluwer.Church,KennethW.andWilliamA.Gale.1995.Poissonmixtures.JournalofNaturalLanguageEngineering1,163-190.Evert,Stefan.2004.AsimpleLNREmodelforrandomcharactersequences.ProceedingsofJADT2004,411-422.Evert,StefanandMarcoBaroni.2006.Testingtheex-trapolationqualityofwordfrequencymodels.Pro-ceedingsofCorpusLinguistics2005.Katz,SlavaM.1996.Distributionofcontentwordsandphrasesintextandlanguagemodeling.NaturalLan-guageEngineering,2(2)15-59.11Withsigniﬁcantcorrelationcoefﬁcientsofr=.76for2N0(p<0.05)andr=.94for3N0(p(cid:28)0.01).Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 912–919,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

912

ASystemforLarge-ScaleAcquisitionofVerbal,NominalandAdjectivalSubcategorizationFramesfromCorporaJuditaPreiss,TedBriscoe,andAnnaKorhonenComputerLaboratoryUniversityofCambridge15JJThomsonAvenueCambridgeCB30FD,UKJudita.Preiss,Ted.Briscoe,Anna.Korhonen@cl.cam.ac.ukAbstractThispaperdescribestheﬁrstsystemforlarge-scaleacquisitionofsubcategorizationframes(SCFs)fromEnglishcorpusdatawhichcanbeusedtoacquirecomprehen-sivelexiconsforverbs,nounsandadjectives.Thesystemincorporatesanextensiverule-basedclassiﬁerwhichidentiﬁes168verbal,37adjectivaland31nominalframesfromgrammaticalrelations(GRs)outputbyaro-bustparser.Thesystemachievesstate-of-the-artperformanceonallthreesets.1IntroductionResearchintoautomaticacquisitionoflexicalin-formationfromlargerepositoriesofunannotatedtext(suchastheweb,corporaofpublishedtext,etc.)isstartingtoproducelargescalelexicalre-sourceswhichincludefrequencyandusageinfor-mationtunedtogenresandsublanguages.Suchresourcesarecriticalfornaturallanguageprocess-ing(NLP),bothforenhancingtheperformanceofstate-of-artstatisticalsystemsandforimprovingtheportabilityofthesesystemsbetweendomains.OnetypeoflexicalinformationwithparticularimportanceforNLPissubcategorization.Accesstoanaccurateandcomprehensivesubcategoriza-tionlexiconisvitalforthedevelopmentofsuccess-fulparsingtechnology(e.g.(Carrolletal.,1998),importantformanyNLPtasks(e.g.automaticverbclassiﬁcation(SchulteimWaldeandBrew,2002))andusefulforanyapplicationwhichcanbeneﬁtfrominformationaboutpredicate-argumentstruc-ture(e.g.InformationExtraction(IE)((Surdeanuetal.,2003)).Theﬁrstsystemscapableofautomaticallylearn-ingasmallnumberofverbalsubcategorizationframes(SCFs)fromunannotatedEnglishcorporaemergedoveradecadeago(Brent,1991;Manning,1993).SubsequentresearchhasyieldedsystemsforEnglish(CarrollandRooth,1998;BriscoeandCar-roll,1997;Korhonen,2002)capableofdetectingcomprehensivesetsofSCFswithpromisingaccu-racyanddemonstratedsuccessinapplicationtasks(e.g.(Carrolletal.,1998;Korhonenetal.,2003)).Recently,alargepubliclyavailablesubcategoriza-tionlexiconwasproducedusingsuchtechnologywhichcontainsframeandfrequencyinformationforover6,300Englishverbs–theVALEXlexicon(Ko-rhonenetal.,2006).Whiletherehasbeenconsiderableworkinthearea,mostofithasfocussedonverbs.Althoughverbsaretherichestwordsintermsofsubcatego-rizationandalthoughverbSCFdistributiondataislikelytoofferthegreatestboostinparserperfor-mance,accurateandcomprehensiveknowledgeofthemanynounandadjectiveSCFsinEnglishcouldimprovetheaccuracyofparsingatseverallevels(fromtaggingtosyntacticandsemanticanalysis).Furthermoretheselectionofthecorrectanalysisfromthesetreturnedbyaparserwhichdoesnotini-tiallyutilizeﬁne-grainedlexico-syntacticinforma-tioncandependontheinteractionofconditionalprobabilitiesoflemmasofdifferentclassesoccur-913

ringwithspeciﬁcSCFs.Forexample,a)andb)be-lowindicatethemostplausibleanalysesinwhichthesententialcomplementattachestothenounandverbrespectivelya)Kim(VPbelieves(NPtheevidence(ScompthatSandywaspresent)))b)Kim(VPpersuaded(NPthejudge)(ScompthatSandywaspresent))However,botha)andb)consistofanidenticalsequenceofcoarse-grainedlexicalsyntacticcate-gories,socorrectlyrankingthemrequireslearn-ingthatP(NP|believe).P(Scomp|evidence)>P(NP&Scomp|believe).P(None|evidence)andP(NP|persuade).P(Scomp|judge)<P(NP&Scomp|persuade).P(None|judge).Ifweacquiredframesandframefrequenciesforallopen-classpredicatestakingSCFsusingasinglesys-temappliedtosimilardata,wewouldhaveabetterchanceofmodelingsuchinteractionsaccurately.Inthispaperwepresenttheﬁrstsystemforlarge-scaleacquisitionofSCFsfromEnglishcorpusdatawhichcanbeusedtoacquirecomprehensivelexi-consforverbs,nounsandadjectives.Theclassiﬁerincorporates168verbal,37adjectivaland31nomi-nalSCFdistinctions.Animprovedacquisitiontech-niqueisusedwhichexpandsontheideasYallopetal.(2005)recentlyexploredforasmallexperimentonadjectivalSCFacquisition.ItinvolvesidentifyingSCFsonthebasisofgrammaticalrelations(GRs)intheoutputoftheRASP(RobustAccurateStatisticalParsing)system(Briscoeetal.,2006).Asdetailedlater,thesystemperformsbetterwithverbsthanpreviouscomparablestate-of-artsystems,achieving68.9F-measureindetectingSCFtypes.Itachievessimilarlygoodperformancewithnounsandadjectives(62.2and71.9F-measure,respectively).Additionally,wehavedevelopedatoolforlin-guisticannotationofSCFsincorpusdataaimedatalleviatingtheprocessofobtainingtrainingandtestdataforsubcategorizationacquisition.Thetoolin-corporatesanintuitiveinterfacewiththeabilitytosigniﬁcantlyreducethenumberofframespresentedtotheuserforeachsentence.WeintroducethenewsystemforSCFacquisitioninsection2.Detailsoftheexperimentalevaluationaresuppliedinsection3.Section4providesdiscus-sionofourresultsandfuturework,andsection5concludes.2DescriptionoftheSystemAcommonstrategyinexistinglarge-scaleSCFac-quisitionsystems(e.g.(BriscoeandCarroll,1997))istoextractSCFsfromparsetrees,introducinganunnecessarydependenceonthedetailsofaparticu-larparser.InourapproachSCFsareextractedfromGRs—representationsofhead-dependentrelationswhicharemoreparser/grammarindependentbutattheappropriatelevelofabstractionforextractionofSCFs.AsimilarapproachwasrecentlymotivatedandexploredbyYallopetal.(2005).Adecision-treeclassiﬁerwasdevelopedfor30adjectivalSCFtypeswhichtestsforthepresenceofGRsintheGRout-putoftheRASP(RobustAccurateStatisticalPars-ing)system(BriscoeandCarroll,2002).Theresultsreportedwith9testadjectiveswerepromising(68.9F-measureindetectingSCFtypes).Ouracquisitionprocessconsistsoffourmainsteps:1)extractingGRsfromcorpusdata,2)feedingtheGRsetsasinputtoarule-basedclassiﬁerwhichincrementallymatchesthemwiththecorrespondingSCFs,3)buildinglexicalentriesfromtheclassiﬁeddata,and4)ﬁlteringthoseentriestoobtainamoreaccuratelexicon.Thedetailsofthesestepsarepro-videdinthesubsequentsections.2.1ObtainingGrammaticalRelationsWeobtaintheGRsusingtherecent,secondreleaseoftheRASPtoolkit(Briscoeetal.,2006).RASPisamodularstatisticalparsingsystemwhichincludesatokenizer,tagger,lemmatizer,andawide-coverageuniﬁcation-basedtag-sequenceparser.WeusethestandardscriptssuppliedwithRASPtooutputthesetofGRsforthemostprobableanalysisreturnedbytheparseror,inthecaseofparsefailures,theGRsforthemostlikelysequenceofsubanalyses.TheGRsareorganizedasasubsumptionhierarchyasshowninFigure1.ThedependencyrelationshipswhichtheGRsem-bodycorrespondcloselytothehead-complementstructurewhichsubcategorizationacquisitionat-temptstorecover,whichmakesGRsidealinputtotheSCFclassiﬁer.Considertheargumentsofeasy914

dependenttaargmoddetauxconjmodargncmodxmodcmodpmodsubjdobjsubjcompncsubjxsubjcsubjobjpcompclausaldobjobj2iobjxcompccompFigure1:TheGRhierarchyusedbyRASPSUBJECTNP1,ADJ-COMPS*PP"PVALforNP3#,VPMOODto-inﬁnitiveSUBJECT3OMISSION1+Figure2:FeaturestructureforSCFadj-obj-for-to-inf(|These:1_DD2||example+s:2_NN2||of:3_IO||animal:4_JJ||senses:5_NN2||be+:6_VBR||relatively:7_RR||easy:8_JJ||for:9_IF||we+:10_PPIO2||to:11_TO||comprehend:12_VV0|)...xcomp(_be+[6]easy:[8])xcomp(to[11]be+[6]comprehend:[12])ncsubj(be+[6]example+s[2]_)ncmod(for[9]easy[8]we+[10])ncsubj(comprehend[12]we+[10],_)...Figure3:GRsfromRASPforadj-obj-for-to-infinthesentence:Theseexamplesofanimalsensesarerelativelyeasyforustocomprehendastheyarenottoofarremovedfromourownexperience.Ac-cordingtotheCOMLEXclassiﬁcation,thisisanex-ampleoftheframeadj-obj-for-to-inf,showninFigure2,(usingAVMnotationinplaceofCOMLEXs-expressions).PartoftheoutputofRASPforthissentenceisshowninFigure3.EachinstantiatedGRinFigure3correspondstooneormorepartsofthefeaturestructureinFig-ure2.xcomp(be[6]easy[8])establishesbe[6]astheheadoftheVPinwhicheasy[8]occursasacomplement.Theﬁrst(PP)-complementisforus,asindicatedbyncmod(for[9]easy[8]we+[10]),withforasPFORMandwe+(us)asNP.Thesec-ondcomplementisrepresentedbyxcomp(to[11]be+[6]comprehend[12]):ato-inﬁnitiveVP.Thexcomp?Y:pos=vb,val=be?X:pos=adjxcomp?S:val=to?Y:pos=vb,val=be?W:pos=VV0ncsubj?Y:pos=vb,val=be?Z:pos=nounncmod?T:val=for?X:pos=adj?Y:pos=pronncsubj?W:pos=VV0?V:pos=pronFigure4:Patternforframeadj-obj-for-to-infNPheadedbyexamplesismarkedasthesubjectoftheframebyncsubj(be[6]examples[2]),andncsubj(comprehend[12]we+[10])correspondstothecoindexationmarkedby3:thesubjectoftheVPistheNPofthePP.TheonlypartofthefeaturestructurewhichisnotrepresentedbytheGRsiscoin-dexationbetweentheomitteddirectobject1oftheVP-complementandthesubjectofthewholeclause.2.2SCFClassiﬁerSCFFramesTheSCFsrecognizedbytheclassiﬁerwereob-tainedbymanuallymergingtheframesexempli-ﬁedintheCOMLEXSyntax(Grishmanetal.,1994),ANLT(Boguraevetal.,1987)and/orNOMLEX(Macleodetal.,1997)dictionariesandincludingadditionalframesfoundbymanualinspectionofunclassiﬁableexamplesduringdevelopmentoftheclassiﬁer.Theseconsistedofe.g.someoccurrencesofphrasalverbswithcomplexcomplementationandwithﬂexibleorderingofthepreposition/particle,somenon-passivizablewordswithasurfacedirectobject,andsomerarercombinationsofgovernedprepositionandcomplementizercombinations.Theframeswerecreatedsothattheyabstractoverspeciﬁclexically-governedparticlesandprepo-sitionsandspeciﬁcpredicateselectionalpreferences915

butincludesomederivedsemi-predictableboundeddependencyconstructions.ClassiﬁerTheclassiﬁeroperatesbyattemptingtomatchthesetofGRsassociatedwitheachsentenceagainstoneormoreruleswhichexpressthepossiblemappingsfromGRstoSCFs.Therulesweremanuallydevel-opedbyexaminingasetofdevelopmentsentencestodeterminewhichrelationswereactuallyemittedbytheparserforeachSCF.Inourrulerepresentation,aGRpatternisasetofpartiallyinstantiatedGRswithvariablesinplaceofheadsanddependents,augmentedwithconstraintsthatrestrictthepossibleinstantiationsofthevari-ables.AmatchissuccessfulifthesetofGRsforasentencecanbeuniﬁedwithanyrule.Uniﬁca-tionofsentenceGRsandaruleGRpatternoccurswhenthereisaone-to-onecorrespondencebetweensentenceelementsandruleelementsthatincludesaconsistentmappingfromvariablestovalues.Asamplepatternformatchingadj-obj-for-to-infcanbeseeninFig-ure4.EachelementmatcheseitheranemptyGRslot(),avariablewithpossibleconstraintsonpartofspeech(pos)andwordvalue(val),oranalreadyinstantiatedvariable.UnlikeinYallop’swork(Yal-lopetal.,2005),ourrulesaredeclarativeratherthanproceduralandtheserules,writtenindependentlyoftheacquisitionsystem,areexpandedbythesysteminanumberofwayspriortoexecution.Forexample,theverbruleswhichcontainanncsubjrelationwillnotcontainoneinsideanembeddedclause.Forverbs,thebasicrulesetcontains248rulesbutautomaticexpansiongivesriseto1088classiﬁerrulesforverbs.Numerousapproacheswereinvestigatedtoallowanefﬁcientexecutionofthesystem:forexample,foreachtargetwordinasentence,weinitiallyﬁndthenumberofARGumentGRs(seeFigure1)containingitinheadposition,asthewordmustappearinex-actlythesamesetinamatchingrule.ThisallowsustodiscardallpatternswhichspecifyadifferentnumberofGRs:forexample,forverbseachgrouponlycontainsanaverageof109patterns.Forafurtherincreaseinspeed,boththesentenceGRsandtheGRswithinthepatternsareordered(ac-cordingtofrequency)andmatchingisperformedus-ingabackingoffstrategyallowingustoexploittherelativelylownumberofpossibleGRs(comparedtothenumberofpossiblerules).Thesystemexe-cuteson3500sentencesinapprox.1.5secondsofrealtimeonamachinewitha3.2GHzIntelXenonprocessorand4GBofRAM.LexiconCreationandFilteringLexicalentriesareconstructedforeachwordandSCFcombinationfoundinthecorpusdata.Eachlex-icalentryincludestherawandrelativefrequencyoftheSCFwiththewordinquestion,andincludesvar-iousadditionalinformatione.g.aboutthesyntaxofdetectedargumentsandtheargumentheadsindif-ferentargumentpositions1.Finallytheentriesareﬁlteredtoobtainamoreaccuratelexicon.Awaytomaximisetheaccu-racyofthelexiconwouldbetosmooth(correct)theacquiredSCFdistributionswithback-offestimatesbasedonlexical-semanticclassesofverbs(Korho-nen,2002)(seesection4)beforeﬁlteringthem.However,inthisﬁrstexperimentwiththenewsys-temweﬁlteredtheentriesdirectlysothatwecouldevaluatetheperformanceofthenewclassiﬁerwith-outanyadditionalmodules.Forthesamereason,theﬁlteringwasdonebyusingaverysimplemethod:bysettingempiricallydeterminedthresholdsontherelativefrequenciesofSCFs.3ExperimentalEvaluation3.1DataInordertotesttheaccuracyofoursystem,wese-lectedasetof183verbs,30nounsand30adjec-tivesforexperimentation.Thewordswereselectedatrandom,subjecttotheconstraintthattheyexhib-itedmultiplecomplementationpatternsandhadasufﬁcientnumberofcorpusoccurrences(>150)forexperimentation.Wetookthe100M-wordBritishNationalCorpus(BNC)(Burnard,1995),andex-tractedallsentencescontaininganoccurrenceofoneofthetestwords.Thesentenceswereprocessedus-ingtheSCFacquisitionsystemdescribedinthepre-vioussection.Thecitationsfromwhichentrieswerederivedtotaledapproximately744Kforverbsand219Kfornounsandadjectives,respectively.1ThelexicalentriesaresimilartothoseintheVALEXlexi-con.See(Korhonenetal.,2006)forasampleentry.916

3.2GoldStandardOurgoldstandardwasbasedonamanualanalysisofsomeofthetestcorpusdata,supplementedwithadditionalframesfromtheANLT,COMLEX,and/orNOMLEXdictionaries.Thegoldstandardforverbswasavailable,butitwasextendedtoincludeaddi-tionalSCFsmissingfromtheoldsystem.Fornounsandadjectivesthegoldstandardwascreated.Foreachnounandadjective,100-300sentencesfromtheBNC(anaverageof267perword)wererandomlyextracted.Theresultingc.16KsentenceswerethenmanuallyassociatedwithappropriateSCFs,andtheSCFfrequencycountswererecorded.ToalleviatethemanualanalysiswedevelopedatoolwhichﬁrstusestheRASPparserwithsomeheuristicstoreducethenumberofSCFpresented,andthenallowsanannotatortoselectthepreferredchoiceinawindow.Theheuristicsreducedtheav-eragenumberofSCFspresentedalongsideeachsen-tencefrom52to7.TheannotatorwasalsopresentedwithanexamplesentenceofeachSCFandanintu-itivenamefortheframe,suchasPRED(e.g.Kimissilly).Theprogramincludesanoptiontorecordthatparticularsentencescouldnot(initially)beclas-siﬁed.AscreenshotofthetoolisshowninFigure5.Themanualanalysiswasdonebytwolinguists;onewhodidtheﬁrstannotationforthewholedata,andanotherwhore-evaluatedandcorrectedsomeoftheinitialframeassignments,andclassiﬁedmostofthedataleftunclassiﬁedbytheﬁrstannotator2).Atotalof27SCFtypeswerefoundforthenounsand30fortheadjectivesintheannotateddata.Theav-eragenumberofSCFstakenbynounswas9(withtheaverageof2addedfromdictionariestosupple-mentthemanualannotation)andbyadjectives11(3ofwhichwerefromdictionaries).Thelatterarerareandmaynotbeexempliﬁedinthedatagiventheextractionsystem.3.3EvaluationMeasuresWeusedthestandardevaluationmetricstoevaluatetheaccuracyoftheSCFlexicons:typeprecision(thepercentageofSCFtypesthatthesystemproposes2Theprocessprecludedmeasurementsofinter-annotatoragreement,butthiswasjudgedlessimportantthantheenhancedaccuracyofthegoldstandarddata.Figure5:Samplescreenoftheannotationtoolwhicharecorrect),typerecall(thepercentageofSCFtypesinthegoldstandardthatthesystemproposes)andtheF-measurewhichistheharmonicmeanoftypeprecisionandrecall.Wealsocomparedthesimilaritybetweentheac-quiredunﬁltered3SCFdistributionsandgoldstan-dardSCFdistributionsusingvariousmeasuresofdistributionalsimilarity:theSpearmanrankcorre-lation(RC),Kullback-Leiblerdistance(KL),Jensen-Shannondivergence(JS),crossentropy(CE),skewdivergence(SD)andintersection(IS).Thedetailsofthesemeasuresandtheirapplicationtosubcatego-rizationacquisitioncanbefoundin(KorhonenandKrymolowski,2002).Finally,werecordedthetotalnumberofgoldstandardSCFsunseeninthesystemoutput,i.e.thetypeoffalsenegativeswhichwereneverdetectedbytheclassiﬁer.3.4ResultsTable1includestheaverageresultsforthe183verbs.TheﬁrstcolumnshowstheresultsforBriscoeandCarroll’s(1997)(B&C)systemwhenthissys-temisrunwiththeoriginalclassiﬁerbutamorerecentversionoftheparser(BriscoeandCarroll,2002)andthesameﬁlteringtechniqueasournewsystem(thresholdingbasedontherelativefrequen-ciesofSCFs).TheclassiﬁerofB&Csystemiscom-parabletoourclassiﬁerinthesensethatittargetsal-mostthesamesetofverbalSCFs(165outofthe168;the3additionalonesareinfrequentinlanguageandthusunlikelytoaffectthecomparison).Thesecondcolumnshowstheresultsforournewsystem(New).3NothresholdwasappliedtoremovethenoisySCFsfromthedistributions.917

Verbs-MethodMeasuresB&CNewPrecision(%)47.381.8Recall(%)40.459.5F-measure43.668.9KL3.241.57JS0.200.11CE4.853.10SD1.390.74RC0.330.66IS0.490.76UnseenSCFs2817Table1:AverageresultsforverbsTheﬁguresshowthatthenewsystemclearlyper-formsbetterthantheB&Csystem.Ityields68.9F-measurewhichisa25.3absoluteimprovementovertheB&Csystem.Thebetterperformancecanbeob-servedonallmeasures,butparticularlyonSCFtypeprecision(81.8%withoursystemvs.47.3%withtheB&Csystem)andonmeasuresofdistributionalsim-ilarity.TheclearlyhigherIS(0.76vs.0.49)andthefewergoldstandardSCFsunseenintheoutputoftheclassiﬁer(17vs.28)indicatethatthenewsystemiscapableofdetectingahighernumberofSCFs.Themainreasonforbetterperformanceistheabilityofthenewsystemtodetectanumberofchal-lengingorcomplexSCFswhichtheB&Csystemcouldnotdetect4.Theimprovementispartlyat-tributabletomoreaccurateparsesproducedbythesecondreleaseofRASPandpartlytotheimprovedSCFclassiﬁerdevelopedhere.Forexample,thenewsystemisnowabletodistinguishpredicativePPar-guments,suchasIsenthimasamessengerfromthewiderclassofreferentialPParguments,supportingdiscriminationofseveralsyntacticallysimilarSCFswithdistinctsemantics.RunningoursystemontheadjectiveandnountestdatayieldedtheresultssummarizedinTable2.TheF-measureislowerfornouns(62.2)thanforverbs(68.9);foradjectivesitisslightlybetter(71.9).54TheresultsreportedherefortheB&Csystemarelowerthanthoserecentlyreportedin(Korhonenetal.,2006)forthesamesetof183testverbs.Thisisbecauseweuseanimprovedgoldstandard.However,theresultsfortheB&Csystemre-portedusingthelessambitiousgoldstandardarestilllessac-curate(58.6F-measure)thantheonesreportedhereforthenewsystem.5Theresultsfordifferentwordclassesarenotdirectlycom-parablebecausetheyareaffectedbythetotalnumberofSCFsevaluatedforeachwordclass,whichishigherforverbsandMeasuresNounsAdjectivesPrecision(%)91.295.5Recall(%)47.257.6F-measure62.271.9KL0.910.69JS0.090.05CE2.032.01SD0.480.36RC0.700.77IS0.620.72UnseenSCFs157Table2:AverageresultsfornounsandadjectivesThenounandadjectiveclassiﬁersyieldveryhighprecisioncomparedtorecall.Thelowerrecallﬁg-uresaremostlyduetothehighernumberofgoldstandardSCFsunseenintheclassiﬁeroutput(ratherthan,forexample,theﬁlteringstep).Thisispar-ticularlyevidentfornounsforwhich15ofthe27framesexempliﬁedinthegoldstandardaremissingintheclassiﬁeroutput.Foradjectivesonly7ofthe30goldstandardSCFsareunseen,resultinginbetterrecall(57.6%vs.47.2%fornouns).Forverbs,subcategorizationacquisitionperfor-manceoftencorrelateswiththesizeoftheinputdatatoacquisition(themoredata,thebetterperfor-mance).WhenconsideringtheF-measureresultsfortheindividualwordsshowninTable3thereappearstobelittlesuchcorrelationfornounsandadjectives.Forexample,althoughthereareindividualhighfre-quencynounswithhighperformance(e.g.plan,freq.5046,F90.9)andlowfrequencynounswithlowperformance(e.g.characterisation,freq.91,F40.0),therearealsomanynounswhichcontradictthetrend(comparee.g.answer,freq.2510,F50.0withfondness,freq.71,F85.7).6AlthoughtheSCFdistributionsfornounsandad-jectivesappearZipﬁan(i.e.themostfrequentframesarehighlyprobable,butmostframesareinfre-quent),thetotalnumberofSCFsperwordistypi-callysmallerthanforverbs,resultinginbetterresis-tancetosparsedataproblems.Thereis,however,aclearcorrelationbetweentheperformanceandthetypeofgoldstandardSCFstakenbyindividualwords.Manyofthegoldstan-lowerfornounsandadjectives.Thisparticularlyappliestothesensitivemeasuresofdistributionalsimilarity.6Thefrequencieshererefertothenumberofcitationssuc-cessfullyprocessedbytheparserandtheclassiﬁer.918

NounFAdjectiveFabundance75.0able66.7acknowledgement47.1angry62.5answer50.0anxious82.4anxiety53.3aware87.5apology50.0certain73.7appearance46.2clear77.8appointment66.7curious57.1belief76.9desperate83.3call58.8difﬁcult77.8characterisation40.0doubtful63.6communication40.0eager83.3condition66.7easy66.7danger76.9generous57.1decision70.6imperative81.8deﬁnition42.8important60.9demand66.7impractical71.4desire71.4improbable54.6doubt66.7insistent80.0evidence66.7kind66.7examination54.6likely66.7experimentation60.0practical88.9fondness85.7probable80.0message66.7sure84.2obsession54.6unaware85.7plan90.9uncertain60.0provision70.6unclear63.2reminder63.2unimportant61.5rumour61.5unlikely69.6temptation71.4unspeciﬁed50.0use60.0unsure90.0Table3:SystemperformanceforeachtestnounandadjectivedardnominalandadjectivalSCFsunseenbytheclassiﬁerinvolvecomplexcomplementationpatternswhicharechallengingtoextract,e.g.thoseexem-pliﬁedinTheargumentofJowithKimaboutFidosurfaced,Jo’spreferencethatKimbesackedsur-faced,andthatSandycameiscertain.Inaddition,manyoftheseSCFsunseeninthedataarealsoverylowinfrequency,andsomemayevenbetruenega-tives(recallthatthegoldstandardwassupplementedwithadditionalSCFsfromdictionaries,whichmaynotnecessarilyappearinthetestdata).ThemainproblemisthattheRASPparsersystem-aticallyfailstoselectthecorrectanalysisforsomeSCFswithnounsandadjectivesregardlessoftheircontextofoccurrence.Infuturework,wehopetoal-leviatethisproblembyusingtheweightedGRoutputfromthetopn-rankedparsesreturnedbytheparserasinputtotheSCFclassiﬁer.4DiscussionThecurrentsystemneedsreﬁnementtoalleviatethebiasagainstsomeSCFsintroducedbytheparser’sunlexicalizedparseselectionmodel.Weplantoin-vestigateusingweightedGRoutputwiththeclas-siﬁerratherthanjusttheGRsetfromthehighestrankedparse.SomeSCFclassesalsoneedtobefur-therresolvedmainlytodifferentiatecontroloptionswithpredicativecomplementation.Thisrequiresalexico-semanticclassiﬁcationofpredicateclasses.ExperimentswithBriscoeandCarroll’ssystemhaveshownthatitispossibletoincorporatesomesemanticinformationintheacquisitionprocessus-ingatechniquethatsmoothstheacquiredSCFdis-tributionsusingback-off(i.e.probability)estimatesbasedonlexical-semanticclassesofverbs(Korho-nen,2002).Theestimateshelptocorrecttheac-quiredSCFdistributionsandpredictSCFswhicharerareorunseene.g.duetosparsedata.Theycouldalsoformthebasisforpredictingcontrolofpredica-tivecomplements.Weplantomodifyandextendthistechniqueforthenewsystemanduseittoimprovetheperfor-mancefurther.Thetechniquehassofarbeenappliedtoverbsonly,butitcanalsobeappliedtonounsandadjectivesbecausetheycanalsobeclassiﬁedonlexical-semanticgrounds.Forexample,theadjec-tivesimplebelongstotheclassofEASYadjectives,andthisknowledgecanhelptopredictthatittakessimilarSCFstotheotherclassmembersandthatcontrolof‘understood’argumentswillpatternwitheasy(e.g.easy,difﬁcult,convenient):TheproblemwillbesimpleforJohntosolve,ForJohntosolvetheproblemwillbesimple,Theproblemwillbesim-pletosolve,etc.Furtherresearchisneededbeforehighlyaccuratelexiconsencodinginformationalsoaboutsemanticaspectsofsubcategorization(e.g.differentpredicatesenses,themappingfromsyntacticargumentstosemanticrepresentationofargumentstructure,se-lectionalpreferencesonargumentheads,diathesisalternations,etc.)canbeobtainedautomatically.However,withtheextensionssuggestedabove,thesystempresentedhereissufﬁcientlyaccurateforbuildinganextensiveSCFlexiconcapableofsup-portingvariousNLPapplicationtasks.Suchalex-iconwillbebuiltanddistributedforresearchpur-919

posesalongwiththegoldstandarddescribedhere.5ConclusionWehavedescribedtheﬁrstsystemforautomaticallyacquiringverbal,nominalandadjectivalsubcat-egorizationandassociatedfrequencyinformationfromEnglishcorpora,whichcanbeusedtobuildlarge-scalelexiconsforNLPpurposes.Wehavealsodescribedanewannotationtoolforproducingtrainingandtestdataforthetask.Theacquisitionsystem,whichiscapableofdistinguishing168verbal,37adjectivaland31nominalframes,clas-siﬁescorpusoccurrencestoSCFsonthebasisofGRsproducedbyarobuststatisticalparser.TheinformationprovidedbyGRscloselymatchesthestructurethatsubcategorizationacquisitionseekstorecover.Ourexperimentshowsthatthesystemachievesstate-of-the-artperformancewitheachwordclass.ThediscussionsuggestswaysinwhichwecouldimprovethesystemfurtherbeforeusingittobuildalargesubcategorizationlexiconcapableofsupportingvariousNLPapplicationtasks.AcknowledgementsThisworkwassupportedbytheRoyalSocietyandUKEPSRCproject‘AccurateandComprehensiveLexicalClassiﬁcationforNaturalLanguagePro-cessingApplications’(ACLEX).WewouldliketothankDianeNichollsforherhelpduringthiswork.ReferencesB.Boguraev,J.Carroll,E.J.Briscoe,D.Carter,andC.Grover.1987.Thederivationofagrammatically-indexedlexiconfromtheLongmanDictionaryofContemporaryEnglish.InProc.ofthe25thAnnualMeetingofACL,pages193–200,Stanford,CA.M.Brent.1991.Automaticacquisitionofsubcategorizationframesfromuntaggedtext.InProc.ofthe29thMeetingofACL,pages209–214.E.J.BriscoeandJ.Carroll.1997.AutomaticExtractionofSubcategorizationfromCorpora.InProc.ofthe5thANLP,WashingtonDC,USA.E.J.BriscoeandJ.Carroll.2002.Robustaccuratestatisticalannotationofgeneraltext.InProc.ofthe3rdLREC,pages1499–1504,LasPalmas,CanaryIslands,May.E.J.Briscoe,J.Carroll,andR.Watson.2006.Thesecondreleaseoftheraspsystem.InProc.oftheCOLING/ACL2006InteractivePresentationSessions,Sydney,Australia.L.Burnard,1995.TheBNCUsersReferenceGuide.BritishNationalCorpusConsortium,Oxford,May.G.CarrollandM.Rooth.1998.Valenceinductionwithahead-lexicalizedpcfg.InProc.ofthe3rdConferenceonEMNLP,Granada,Spain.J.Carroll,G.Minnen,andE.J.Briscoe.1998.CanSubcat-egorisationProbabilitiesHelpaStatisticalParser?InPro-ceedingsofthe6thACL/SIGDATWorkshoponVeryLargeCorpora,pages118–126,Montreal,Canada.R.Grishman,C.Macleod,andA.Meyers.1994.COMLEXSyntax:BuildingaComputationalLexicon.InCOLING,Kyoto.A.KorhonenandY.Krymolowski.2002.OntheRobustnessofEntropy-BasedSimilarityMeasuresinEvaluationofSub-categorizationAcquisitionSystems.InProc.oftheSixthCoNLL,pages91–97,Taipei,Taiwan.A.Korhonen,Y.Krymolowski,andZ.Marx.2003.ClusteringPolysemicSubcategorizationFrameDistributionsSemanti-cally.InProc.ofthe41stAnnualMeetingofACL,pages64–71,Sapporo,Japan.A.Korhonen,Y.Krymolowski,andE.J.Briscoe.2006.Alargesubcategorizationlexiconfornaturallanguageprocess-ingapplications.InProc.ofthe5thLREC,Genova,Italy.A.Korhonen.2002.Subcategorizationacquisition.Ph.D.the-sis,UniversityofCambridgeComputerLaboratory.C.Macleod,A.Meyers,R.Grishman,L.Barrett,andR.Reeves.1997.Designingadictionaryofderivednominals.InProc.ofRANLP,TzigovChark,Bulgaria.C.Manning.1993.AutomaticAcquisitionofaLargeSubcat-egorizationDictionaryfromCorpora.InProc.ofthe31stMeetingofACL,pages235–242.S.SchulteimWaldeandC.Brew.2002.Inducinggermanse-manticverbclassesfrompurelysyntacticsubcategorisationinformation.InProc.ofthe40thAnnualMeetingofACL,Philadephia,USA.M.Surdeanu,S.Harabagiu,J.Williams,andP.Aarseth.2003.Usingpredicate-argumentstructuresforinformationextrac-tion.InProc.ofthe41stAnnualMeetingofACL,Sapporo.J.Yallop,A.Korhonen,andE.J.Briscoe.2005.Auto-maticacquisitionofadjectivalsubcategorizationfromcor-pora.InProc.ofthe43rdAnnualMeetingoftheAssociationforComputationalLinguistics,pages614–621,AnnArbor,Michigan.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 920–927,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

920

ALanguage-IndependentUnsupervisedModelforMorphologicalSegmentationVeraDembergSchoolofInformaticsUniversityofEdinburghEdinburgh,EH89LW,GBv.demberg@sms.ed.ac.ukAbstractMorphologicalsegmentationhasbeenshowntobebeneﬁcialtoarangeofNLPtaskssuchasmachinetranslation,speechrecognition,speechsynthesisandinfor-mationretrieval.Recently,anumberofapproachestounsupervisedmorphologicalsegmentationhavebeenproposed.Thispaperdescribesanalgorithmthatdrawsfrompreviousapproachesandcombinesthemintoasimplemodelformorpholog-icalsegmentationthatoutperformsotherapproachesonEnglishandGerman,andalsoyieldsgoodresultsonagglutinativelanguagessuchasFinnishandTurkish.Wealsoproposeamethodfordetectingvariationwithinstemsinanunsupervisedfashion.Thesegmentationqualityreachedwiththenewalgorithmisgoodenoughtoimprovegrapheme-to-phonemeconversion.1IntroductionMorphologicalsegmentationhasbeenshowntobebeneﬁcialtoanumberofNLPtaskssuchasma-chinetranslation(GoldwaterandMcClosky,2005),speechrecognition(Kurimoetal.,2006),informa-tionretrieval(MonzanddeRijke,2002)andques-tionanswering.Segmentingawordintomeaning-bearingunitsisparticularlyinterestingformorpho-logicallycomplexlanguageswherewordscanbecomposedofseveralmorphemesthroughinﬂection,derivationandcomposition.Datasparsenessforsuchlanguagescanbesigniﬁcantlydecreasedwhenwordsaredecomposedmorphologically.Thereex-istanumberofrule-basedmorphologicalsegmen-tationsystemsforarangeoflanguages.However,expertknowledgeandlabourareexpensive,andtheanalyzersmustbeupdatedonaregularbasisinor-dertocopewithlanguagechange(theemergenceofnewwordsandtheirinﬂections).Onemightarguethatunsupervisedalgorithmsarenotaninterestingoptionfromtheengineeringpointofview,becauserule-basedsystemsusuallyleadtobetterresults.However,segmentationsfromanunsupervisedalgo-rithmthatislanguage-independentare“cheap”,be-causetheonlyresourceneededisunannotatedtext.Ifsuchanunsupervisedsystemreachesaperfor-mancelevelthatisgoodenoughtohelpanothertask,itcanconstituteanattractiveadditionalcomponent.Recently,anumberofapproachestounsupervisedmorphologicalsegmentationhavebeenproposed.Thesealgorithmsautonomouslydiscovermorphemesegmentationsinunannotatedtextcorpora.Herewedescribeamodiﬁcationofonesuchunsupervisedal-gorithm,RePortS(KeshavaandPitler,2006).TheRePortSalgorithmperformedbestonEnglishinarecentcompetitiononunsupervisedmorphologicalsegmentation(Kurimoetal.,2006),buthadverylowrecallonmorphologicallymorecomplexlanguageslikeGerman,FinnishorTurkish.Weaddanewstepdesignedtoachievehigherrecallonmorpho-logicallycomplexlanguagesandproposeamethodforidentifyingrelatedstemsthatunderwentregularnon-concatenativemorphologicalprocessessuchasumlautingorablauting,aswellasmorphologicalal-ternationsalongmorphemeboundaries.Thepaperisstructuredasfollows:Section921

2discussestherelationshipbetweenlanguage-dependencyandthelevelofsupervisionofalearn-ingalgorithm.WethengiveanoutlineofthemainstepsoftheRePortSalgorithminsection3andex-plainthemodiﬁcationstotheoriginalalgorithminsection4.Section5comparesresultsfordifferentlanguages,quantiﬁesthegainsfromthemodiﬁca-tionsonthealgorithmandevaluatesthealgorithmonagrapheme-to-phonemeconversiontask.Weﬁ-nallysummarizeourresultsinsection6.2PreviousWorkTheworld’slanguagescanbeclassiﬁedaccordingtotheirmorphologyintoisolatinglanguages(littleornomorphology,e.g.Chinese),agglutinativelan-guages(whereawordcanbedecomposedintoalargenumberofmorphemes,e.g.Turkish)andin-ﬂectionallanguages(morphemesarefusedtogether,e.g.Latin).Phenomenathataredifﬁculttocopewithformanyoftheunsupervisedalgorithmsarenon-concatenativeprocessessuchasvowelharmoniza-tion,ablautingandumlauting,ormodiﬁcationsattheboundariesofmorphemes,aswellasinﬁxation(e.g.inTagalog:sulat‘write’,s-um-ulat‘wrote’,s-in-ulat‘waswritten’),circumﬁxation(e.g.inGer-man:mach-en‘do’,ge-mach-t‘done’),theAra-bicbrokenpluralorreduplications(e.g.inPinge-lapese:mejr‘tosleep’,mejmejr‘sleeping’,mejme-jmejr‘stillsleeping’).Forwordsthataresubjecttooneoftheaboveprocessesitisnottrivialtoautomat-icallygrouprelatedwordsanddetectregulartrans-formationalpatterns.Arangeofautomatedalgorithmsformorpholog-icalanalysiscopewithconcatenativephenomena,andbasetheirmechanicsonstatisticsabouthypoth-esizedstemsandafﬁxes.Theseapproachescanbefurthercategorizedintoonesthatuseconditionalentropybetweenletterstodetectsegmentbound-aries(Harris,1955;HaferandWeiss,1974;D´ejean,1998;Monsonetal.,2004;Bernhard,2006;Ke-shavaandPitler,2006;Bordag,2006),approachesthatuseminimaldescriptionlengthandtherebymin-imizethesizeofthelexiconasmeasuredinen-triesandlinksbetweentheentriestoconstituteawordform(Goldsmith,2001;CreutzandLagus,2006).Thesetwotypesofapproachesverycloselytietheorthographicformofthewordtothemor-phemes.Theyarethusnotwell-suitedforcopingwithstemchangesormodiﬁcationsattheedgesofmorphemes.Onlyveryfewapproacheshavead-dressedwordinternalvariations(YarowskiandWi-centowski,2000;NeuvelandFulop,2002).Apopularandeffectiveapproachfordetectingin-ﬂectionalparadigmsandﬁlterafﬁxlistsistoclustertogetherafﬁxesorregulartransformationalpatternsthatoccurwiththesamestem(Monsonetal.,2004;Goldsmith,2001;Gaussier,1999;SchoneandJuraf-sky,2000;YarowskiandWicentowski,2000;Neu-velandFulop,2002;Jacquemin,1997).Wedrawfromthisideaofclusteringinordertodetectortho-graphicvariantsofstems;seeSection4.3.Afewapproachesalsotakeintoaccountsyntac-ticandsemanticinformationfromthecontextthewordoccurs(SchoneandJurafsky,2000;Bordag,2006;YarowskiandWicentowski,2000;Jacquemin,1997).Exploitingsemanticandsyntacticinforma-tionisveryattractivebecauseitaddsanadditionaldimension,buttheseapproacheshavetocopewithmoreseveredatasparsenessissuesthanapproachesthatemphasizeword-internalcues,andtheycanbecomputationallyexpensive,especiallywhentheyuseLSA.TheoriginalRePortSalgorithmassumesmor-phologytobeconcatenative,andspecializesonpre-ﬁxationandsufﬁxation,likemostoftheaboveap-proaches,whichweredevelopedandimplementedforEnglish(Goldsmith,2001;SchoneandJurafsky,2000;NeuvelandFulop,2002;YarowskiandWi-centowski,2000;Gaussier,1999).However,manylanguagesaremorphologicallymorecomplex.ForexampleinGerman,analgorithmalsoneedstocopewithcompounding,andinTurkishwordscanbeverylongandcomplex.WethereforeextendedtheoriginalRePortSalgorithmtobebetteradaptedtocomplexmorphologyandsuggestamethodforcop-ingwithstemvariation.Thesemodiﬁcationsren-derthealgorithmmorelanguage-independentandtherebymakeitattractiveforapplyingtootherlan-guagesaswell.3TheRePortSAlgorithmOnEnglish,theRePortSalgorithmclearlyout-performedallothersystemsinMorphoChallenge922

20051(Kurimoetal.,2006),obtaininganF-measureof76.8%(76.2%prec.,77.4%recall).ThenextbestsystemobtainedanF-scoreof69%.However,thealgorithmdoesnotperformaswellonotherlan-guages(Turkish,Finnish,German)duetolowre-call(see(KeshavaandPitler,2006)and(Demberg,2006),p.47).Therearethreemainstepsinthealgorithm.First,thedataisstructuredintwotrees,whichprovidethebasisforefﬁcientcalculationoftransitionalproba-bilitiesofalettergivenitscontext.Thesecondstepistheafﬁxacquisitionstep,duringwhichasetofmorphemesisidentiﬁedfromthecorpusdata.Thethirdstepusesthesemorphemestosegmentwords.3.1DataStructureThedataisstoredintwotrees,theforwardtreeandthebackwardtree.Branchescorrespondtoletters,andnodesareannotatedwiththetotalcorpusfre-quencyofthelettersequencefromtherootofthetreeuptothenode.Duringtheafﬁxidentiﬁcationprocess,theforwardtreeisusedfordiscoveringsuf-ﬁxesbycalculatingtheprobabilityofseeingacer-tainlettergiventhepreviouslettersoftheword.Thebackwardtreeisusedtodeterminetheprobabilityofalettergiventhefollowinglettersofawordinordertoﬁndpreﬁxes.Ifthetransitionalprobabil-ityishigh,thewordshouldnotbesplit,whereaslowprobabilityisagoodindicatorofamorphemeboundary.Insuchatree,stemstendtostaytogetherinlongunarybranches,whilethebranchingfactorishighinplaceswheremorphemeboundariesoccur.Theunderlyingideaofexploiting“LetterSucces-sorVariety”wasﬁrstproposedin(Harris,1955),andhassincebeenusedinanumberofmorphemicseg-mentationalgorithms(HaferandWeiss,1974;Bern-hard,2006;Bordag,2006).3.2FindingAfﬁxesThesecondstepisconcernedwithﬁndinggoodaf-ﬁxes.Theprocedureisquitesimpleandcanbedi-videdintotwosubtasks.(1)generatingallpossibleafﬁxesand(2)validatingthem.Thevalidationstepisnecessarytoexcludebadafﬁxcandidates(e.g.let-tersequencesthatoccurtogetherfrequentlysuchassch,sprorchinGermanorsh,th,quinEnglish).1www.cis.hut.fi/morphochallenge2005/Anafﬁxisvalidatedifallthreecriteriaaresatisﬁedforatleast5%ofitsoccurrences:1.Thesubstringthatremainsafterpeelingoffanafﬁxisalsoawordinthelexicon.2.Thetransitionalprobabilitybetweenthesecond-lastandthelaststemletteris≈1.3.Thetransitionalprobabilityoftheafﬁxletternexttothestemis<1(tolerance0.02).Finally,allafﬁxesthatareconcatenationsoftwoormoreothersufﬁxes(e.g.,-ungencanbesplitupin-ungand-eninGerman)areremoved.Thisstepre-turnstwolistsofmorphologicalsegments.Thepre-ﬁxlistcontainspreﬁxesaswellasstemsthatusuallyoccuratthebeginningofwords,whilethesufﬁxlistcontainssufﬁxesandstemsthatoccurattheendofwords.Intheremainderofthepaper,wewillrefertothecontentoftheselistsas“preﬁxes”and“suf-ﬁxes”,althoughtheyalsoincludestems.ThereareseveralassumptionsencodedinthisprocedurethatarespeciﬁctoEnglish,andcauserecalltobelowforotherlanguages:1)allstemsarevalidwordsinthelexicon;2)afﬁxesoccuratthebeginningorendofwordsonly;and3)afﬁxationdoesnotchangestems.Insection4,weproposewaysofrelaxingtheseas-sumptionstomakethissteplesslanguage-speciﬁc.3.3SegmentingWordsTheﬁnalstepisthecompletesegmentationofwordsgiventhelistofafﬁxesacquiredinthepreviousstep.TheoriginalRePortSalgorithmusesaverysimplemethodthatpeelsoffthemostprobablesufﬁxthathasatransitionalprobabilitysmallerthan1,untilnomoreafﬁxesmatchoruntillessthanhalfofthewordremains.Thislastconditionisproblematicsinceitdoesnotscaleupwelltolanguageswithcomplexmorphology.Thesamepeeling-offprocessisexe-cutedforpreﬁxes.Althoughthismethodisbetterthanusingaheuristicsuchas‘alwayspeeloffthelongestpos-sibleafﬁx’,becauseittakesintoaccountprobablesitesoffracturesinwords,itisnotsensitivetotheafﬁxcontextorthemorphotacticsofthelan-guage.Typicalmistakesthatarisefromthiscon-ditionarethatinﬂectionalsufﬁxes,whichcanonlyoccurword-ﬁnally,mightbesplitoffinthemiddleofawordafterpreviouslyhavingpeeledoffanum-berofothersufﬁxes.923

4ModiﬁcationsandExtensions4.1MorphemeAcquisitionWhenwerantheoriginalalgorithmonaGermandataset,nosufﬁxeswerevalidatedbutreasonablepreﬁxlistswerefound.ThealgorithmworksﬁneforEnglishsufﬁxes–whydoesitfailonGerman?Thealgorithm’sfailuretodetectGermansufﬁxesiscausedbytheinvalidassumptionthatastemmustbeawordinthecorpus.Germanverbstemsdonotoccurontheirown(exceptforcertainimpera-tiveforms).Afterstrippingoffthesufﬁxoftheverbabholst‘fetch’,theremainingstringabholcannotbefoundinthelexicon.However,wordslikeabholen,abholt,abholeorAbholungarepartofthecorpus.ThesameproblemalsooccursforGermannouns.Therefore,thisﬁrstconditionoftheafﬁxacqui-sitionstepneedstobereplaced.Wethereforeintro-ducedanadditionalstepforbuildinganintermediatestemcandidatelistintotheafﬁxacquisitionprocess.Theﬁrstconditionisreplacedbyaconditionthatcheckswhetherastemisinthestemcandidatelist.Thisnewstemcandidateacquisitionprocedurecom-prisesthreesteps:Step1:CreationofstemcandidatelistAllsubstringsthatsatisfyconditions2and3butnotcondition1,arestoredtogetherwiththesetofafﬁxestheyoccurwith.Thisprocessissimilartotheideaofregisteringsignatures(Goldsmith,2001;NeuvelandFulop,2002).Forexample,letusas-sumeourcorpuscontainsthewordsAuff¨uhrender,Auff¨uhrung,auff¨uhrtandAuff¨uhrlaunebutnotthestemitself,sinceauff¨uhr‘act’isnotavalidGer-manword.Conditions2and3aremet,becausethetransitionalprobabilitybetweenauff¨uhrandthenextletterislow(therearealotofdifferentpos-siblecontinuations)andthetransitionalprobabilityP(r|auff¨uh)≈1.Thestemcandidateauff¨uhristhenstoredtogetherwiththesufﬁxcandidates{ender,ung,en,t,laune}.Step2:RankingcandidatestemsTherearetwotypesofafﬁxcandidates:type-1afﬁxcandidatesarewordsthatarecontainedinthedatabaseasfullwords(thoseareduetocompounding);type-2afﬁxcandidatesareinﬂectionalandderiva-tionalsufﬁxes.Whenrankingthestemcandidates,wetakeintoaccountthenumberoftype-1afﬁxcan-didatesandtheaveragefrequencyoftpye-2afﬁxFigure1:Determiningthethresholdforvalidatingthebestcandidatesfromthestemcandidatelist.candidates.Theﬁrstconditionhasverygoodprecision,sim-ilartotheoriginalmethod.Themorphemesfoundwiththismethodarepredominantlystemformsthatoccurincompoundingorderivation(Komposition-sst¨ammeandDerivationsst¨amme).Thesecondcon-ditionenablesustodifferentiatebetweenstemsthatoccurwithcommonsufﬁxes(andthereforehavehighaveragefrequencies),andpseudostemssuchasrunterschwhoseafﬁxlistcontainsmanynon-morphemes(e.g.lucken,iebt,aute).Thesenon-morphemesareveryraresincetheyarenotgener-atedbyaregularprocess.Step3:PruningAllstemcandidatesthatoccurlessthanthreetimesareremovedfromthelist.Theremainingstemcan-didatesareorderedaccordingtotheaveragefre-quencyoftheirnon-wordsufﬁxes.Thiscriterionputsthehighqualitystemcandidates(thatoccurwithverycommonsufﬁxes)tothetopofthelist.Inordertoobtainahigh-precisionstemlist,itisnecessarytocutthelistofcandidatesatsomepoint.Thethresholdforthisisdeterminedbythedata:wechoosethepointatwhichthefunctionoflist-rankvs.scorechangessteepness(seeFigure1).Thisvisualchangeofsteepnesscorrespondstothepointwherepotentialstemsfoundgetmorenoisybecausethestringswithwhichtheyoccurarenotcommonafﬁxes.Wefoundtheperformanceoftheresult-ingmorphologicalsystemtobequitestable(±1%f-score)foranycuttingpointontheslopebetween20%and50%ofthelist(fortheGermandatasetranks4000and12000),butimportantlybeforethefunctiontailsoff.Thethresholdwasalsorobustacrosstheotherlanguagesanddatasets.924

4.2MorphologicalSegmentationAsdiscussedinsection3.3,theoriginalimplemen-tationofthealgorithmiterativelychopsoffthemostprobableafﬁxesatbothedgesofthewordwithouttakingintoaccountthecontextoftheafﬁx.Inmor-phologicallycomplexlanguages,thiscontext-blindapproachoftenleadstosuboptimalresults,andalsoallowssegmentationsthataremorphotacticallyim-possible,suchasinﬂectionalsufﬁxesinthemiddleofwords.Anotherriskisthatthelettersequencethatisleftafterremovingpotentialpreﬁxesandsufﬁxesfrombothendsisnotaproperstemitselfbutjustasingleletterorvowel-lessletter-sequence.Theseproblemscanbesolvedbyusingabi-gramlanguagemodeltocapturethemorphotacticproper-tiesofaparticularlanguage.Insteadofsimplypeel-ingoffthemostprobableafﬁxesfrombothendsoftheword,allpossiblesegmentationsofthewordaregeneratedandrankedusingthelanguagemodel.Theprobabilitiesforthelanguagemodelarelearntfromasetofwordsthatweresegmentedwiththeorigi-nalsimpleapproach.Thisbootstrappingallowsustoensurethattheapproachremainsfullyunsuper-vised.Atthebeginningandendofeachword,anedgemarker‘#’isattachedtotheword.Themodelcanthenalsoacquireprobabilitiesaboutwhichaf-ﬁxesoccurmostoftenattheedgesofwords.Table2showsthatﬁlteringthesegmentationre-sultswiththen-gramlanguagemodelcausedasig-niﬁcantimprovementontheoverallF-scoreformostlanguages,andledtosigniﬁcantchangesinpre-cisionandrecall.Whereastheoriginalsegmen-tationyieldedbalancedprecisionandrecall(both68%),thenewﬁlteringboostsprecisiontoover73%,with64%recall.Whichmethodispreferable(i.e.whetherprecisionorrecallismoreimportant)istask-dependent.Infuturework,weplantodrawon(CreutzandLagus,2006),whouseaHMMwithmorphemiccat-egoriestoimposemorphotacticconstraints.Insuchanapproach,eachelementfromtheafﬁxlistisas-signedwithacertainprobabilitytotheunderlyingcategoriesof“stem”,“preﬁx”or“sufﬁx”,depend-ingontheleftandrightperplexityofmorphemes,aswellasmorphemelengthandfrequency.Thetran-sitionalprobabilitiesfromonecategorytothenextmodelthemorphotacticrulesofalanguage,whichcanthusbelearntautomatically.4.3LearningStemVariationStemvariationthroughablautingandumlauting(anEnglishexampleisrun–ran)isaninterest-ingproblemthatcannotbecapturedbythealgo-rithmoutlinedabove,asvariationstakeplacewithinthemorphemes.Stemvariationscanbecontext-dependentanddonotconstituteamorphemeinthemselves.Germanumlautingandablautingleadstodatasparsenessproblemsinmorphologicalseg-mentationandafﬁxacquisition.Oneproblemisthatafﬁxeswhichusuallycauseablautingorumlautingareverydifﬁculttoﬁnd.Typically,ablautedorum-lautedstemsareonlyseenwithaverysmallnumberofdifferentafﬁxes,whichmeansthattheafﬁxsetsofsuchstemsaredividedintoseveralunrelatedsub-sets,causingthestemtobeprunedfromthestemcandidatelist.Secondly,ablautingandumlautingleadtolowtransitionalprobabilitiesatthepositionsinstemswherethesephenomenaoccur.Considerforexampletheafﬁxsetforthestemcandidatebock-spr,whichcontainsthepseudoafﬁxesung,¨ungeandingen.Themorphemessprung,spr¨ungandsprin-genarederivedfromtherootspring‘tojump’.Inthesegmentationstepthislowtransitionalprobabil-itythusleadstooversegmentation.Wethereforeinvestigatedwhetherwecanlearntheseregularstemvariationsautomatically.Asim-plewaytoacquirethestemvariationsistolookatthesufﬁxclusterswhicharecalculatedduringthestem-acquisitionstep.Whenlookingatthesetsofsubstringsthatareclusteredtogetherbyhavingthesamepreﬁx,wefoundthattheyareofteninﬂectionsofoneanother,becauselexicalizedcompoundsareusedfrequentlyindifferentinﬂectionalvariants.Forexample,weﬁndTrainingssprungaswellasTrain-ingsspr¨ungeinthecorpus.Theafﬁxlistofthestemcandidatetrainingsthuscontainsthewordssprungandspr¨unge.Editdistancecanthenbeusedtoﬁnddifferencesbetweenallwordsinacertainafﬁxlist.Pairswithsmalleditdistancesarestoredandrankedbyfrequency.Regulartransformationrules(e.g.ablautingandumlauting,u→¨u..e)occuratthetopofthelistandareautomaticallyacceptedasrules(seeTable1).Thismethodallowsustonotonlyﬁndtherelationbetweentwowordsinthelex-icon(SprungandSpr¨unge)butalsotoautomaticallylearnrulesthatcanbeappliedtounknownwordstocheckwhethertheirvariantisawordinthelexicon.925

freq.diff.examples1682a¨a..esack-s¨acke,brach-br¨ache,stark-st¨arke344a¨asahen-s¨ahen,garten-g¨arten321u¨u..eﬂug-ﬂ¨uge,bund-b¨unde289¨aa..svertr¨age-vertrages,p¨asse-passes189o¨o..echor-ch¨ore,strom-str¨ome,?r¨ohre-rohr175tensetzt-setzen,bringt-bringen168auladen-luden,*damm-dumm160ßssl¨aßt-l¨asst,mißbrauch-missbrauch[...]136aenﬁrma-ﬁrmen,thema-themen[...]2ßg*ﬂießen-ﬂiegen,*laßt-lagt2umo*studiums-studiosTable1:Excerptsfromthestemvariationdetectionalgorithmresults.Morphologicallyunrelatedwordpairsaremarkedwithanasterisk.Weintegratedinformationaboutstemvariationfromtheregularstemtransformationrules(thosewiththehighestfrequencies)intothesegmentationstepbycreatingequivalencesetsofletters.Forex-ample,theruleu→¨u..egeneratesanequivalenceset{¨u,u}.Thesetwolettersthencountasthesameletterwhencalculatingtransitionalprobabilities.WeevaluatedthebeneﬁtofintegratingstemvariationinformationforGermanontheGermanCELEXdataset,andachievedanimprovementof2%inrecall,withoutanylossinprecision(F-measure:69.4%,Precision:68.1%,Recall:70.8%;valuesforRePortS-stems).Forbettercomparabilitytoothersystemsandlanguages,resultsreportedinthenextsectionrefertothesystemversionthatdoesnotin-corporatestemvariation.5EvaluationForevaluatingthedifferentversionsofthealgorithmonEnglish,TurkishandFinnish,weusedthetrain-ingandtestsetsfromMorphoChallengetoenablecomparisonwithothersystems.PerformanceofthealgorithmonGermanwasevaluatedon244kmanu-allyannotatedwordsfromCELEXbecauseGermanwasnotincludedintheMorphoChallengedata.Table2showsthattheintroductionofthestemcandidateacquisitionstepledtomuchhigherrecallonGerman,FinnishandTurkish,butcausedsomelossesinprecision.ForEnglish,addingbothcom-ponentsdidnothavealargeeffectoneitherpreci-sionorrecall.Thismeansthatthiscomponentiswellbehaved,i.e.itimprovesperformanceonlan-guageswheretheintermediatestem-acquisitionstepLang.alg.versionF-Meas.Prec.RecallEng1original76.8%76.2%77.4%stems67.6%62.9%73.1%n-gramseg.75.1%74.4%75.9%Ger2original59.2%71.1%50.7%stems68.4%68.1%68.6%n-gramseg.68.9%73.7%64.6%Tur1original54.2%72.9%43.1%stems61.8%65.9%58.2%n-gramseg.64.2%65.2%63.3%Fin1original47.1%84.5%32.6%stems56.6%74.1%45.8%n-gramseg.58.9%76.1%48.1%max-split*61.3%66.3%56.9%Table2:Performanceofthealgorithmwiththemod-iﬁcationsondifferentlanguages.1MorphoChallengeData,2GermanCELEXisneeded,butdoesnotimpairresultsonotherlan-guages.RecallforFinnishisstillverylow.Itcanbeimproved(attheexpenseofprecision)byselectingtheanalysiswiththelargestnumberofsegmentsinthesegmentationstep.Theresultsforthisheuris-ticwasonlyevaluatedonasmallertestset(ca.700wds),hencemarkedwithanasteriskinTable2.Thealgorithmisveryefﬁcient:Whentrainedonthe240mtokensoftheGermanTAZcorpus,ittakesuplessthan1GBofmemory.Thetrainingphasetakesapprox.5minona2.4GHzmachine,andthesegmentationofthe250ktestwordstakes3minfortheversionthatdoesthesimplesegmentationandabout8minfortheversionthatgeneratesallpossi-blesegmentationsandusesthelanguagemodel.5.1ComparisontoothersystemsThismodiﬁedversionofthealgorithmperformssec-ondbestforEnglish(afteroriginalRePortS)andranksthirdforTurkish(afterBernhardsalgorithmwith65.3%F-measureandMorfessor-Categories-MAPwith70.7%).OnGerman,ourmethodsig-niﬁcantlyoutperformedtheotherunsupervisedal-gorithms,seeTable3.WhilemostofthesystemscomparedhereweredevelopedforlanguagesotherthanGerman,(Bordag,2006)describesasystemini-tiallybuiltforGerman.Whentrainedonthe“Pro-jektDeutscherWortschatz”corpuswhichcomprises24millionsentences,itachievesanF-scoreof61%(precision60%,recall62%2)whenevaluatedonthefullCELEXcorpus.2Datafrompersonalcommunication.926

morphologyF-Meas.Prec.RecallSMOR-disamb283.6%87.1%80.4%ETI79.5%75.4%84.1%SMOR-disamb171.8%95.4%57.6%RePortS-lm68.8%73.7%64.6%RePortS-stems68.4%68.1%68.6%bestBernhard63.5%64.9%62.1%Bordag61.4%60.6%62.3%orig.RePortS59.2%71.1%50.7%bestMorfessor1.052.6%70.9%41.8%Table3:Evaluatingrule-basedanddata-basedsys-temsformorphologicalsegmentationwithrespecttoCELEXmanualmorphologicalannotation.Rule-basedsystemsarecurrentlythemostcom-monapproachtomorphologicaldecompositionandperformbetteratsegmentingwordsthanstate-of-the-artunsupervisedalgorithms(seeTable3forper-formanceofstate-of-the-artrule-basedsystemseval-uatedonthesamedata).BoththeETI3andtheSMOR(Schmidetal.,2004)systemsrelyonalargelexiconandasetofrules.TheSMORsystemre-turnsasetofanalysesthatcanbedisambiguatedindifferentways.Fordetailsrefertopp.29–33in(Demberg,2006).5.2EvaluationonGrapheme-to-PhonemeConversionMorphologicalsegmentationisnotofvalueinitself–thequestioniswhetheritcanhelpimproveresultsonanapplication.PerformanceimprovementsduetomorphologicalinformationhavebeenreportedforexampleinMT,informationretrieval,andspeechrecognition.Forthelattertask,morphologicalseg-mentationsfromtheunsupervisedsystemspresentedherehavebeenshowntoimproveaccuracy(Kurimoetal.,2006).Anothermotivationforevaluatingthesystemonataskratherthanonmanuallyannotateddataisthatlinguisticallymotivatedmorphologicalsegmen-tationisnotnecessarilythebestpossiblesegmenta-tionforacertaintask.Evaluationagainstamanu-allyannotatedcorpuspreferssegmentationsthatareclosesttolinguisticallymotivatedanalyses.Further-more,itmightbeimportantforacertaintasktoﬁndaparticulartypeofmorphemeboundaries(e.g.boundariesbetweenstems),butforanothertaskit3EloquentTechnology,Inc.(ETI)TTSsystem.www.mindspring.com/˜ssshp/ssshp_cd/ss_eloq.htmmorphologyF-Meas.(CELEX)PER(dt)CELEX100%2.64%ETI79.5%2.78%SMOR-disamb283.0%3.00%SMOR-disamb171.8%3.28%RePortS-lm68.8%3.45%nomorphology3.63%orig.RePortS59.2%3.83%Bernhard63.5%3.88%RePortS-stem68.4%3.98%Morfessor1.052.6%4.10%Bordag64.1%4.38%Table4:F-measureforevaluationonmanuallyan-notatedCELEXandphonemeerrorrate(PER)fromg2pconversionusingadecisiontree(dt).mightbeveryimportanttoﬁndboundariesbetweenstemsandsufﬁxes.Thestandardevaluationproce-duredoesnotdifferentiatebetweenthetypesofmis-takesmade.Finally,onlyevaluationonataskcanprovideinformationastowhetherhighprecisionorhighrecallismoreimportant,therefore,thedecisionastowhichversionofthealgorithmshouldbecho-sencanonlybetakengivenaspeciﬁctask.Forthesereasonswedecidedtoevaluatetheseg-mentationfromthenewversionsoftheRePortSal-gorithmonaGermangrapheme-to-phoneme(g2p)conversiontask.Theevaluationonthistaskismoti-vatedbythefactthat(Demberg,2007)showedthatgood-qualitymorphologicalpreprocessingcanim-proveg2pconversionresults.Weherecomparetheeffectofusingoursystem’ssegmentationstoarangeofdifferentmorphologicalsegmentationsfromothersystems.Weraneachoftherule-basedsystems(ETI,SMOR-disamb1,SMOR-disamb2)andtheunsupervisedalgorithms(originalRePortS,Bern-hard,Morfessor1.0,Bordag)ontheCELEXdatasetandretrainedourdecisiontree(animplementa-tionbasedon(LucassenandMercer,1984))onthedifferentmorphologicalsegmentations.Table4showstheF-scoreofthedifferentsystemswhenevaluatedonthemanuallyannotatedCELEXdata(fulldataset)andthephonemeerrorrate(PER)fortheg2pconversionalgorithmwhenannotatedwithmorphologicalboundaries(smallertestset,sincethedecisiontreeisasupervisedmethodandneedstrainingdata).Aswecanseefromtheresults,thedistributionofprecisionandrecall(seeTable3)hasanimportantimpactontheconversionquality:theRePortSversionwithhigherprecisionsigniﬁ-927

cantlyoutperformstheotherversiononthetask,al-thoughtheirF-measuresarealmostidentical.Re-markably,theRePortSversionthatusestheﬁlter-ingstepistheonlyunsupervisedsystemthatbeatstheno-morphologybaseline(p<0.0001).Whileallotherunsupervisedsystemstestedheremakethesystemperformworsethanitwouldwithoutmor-phologicalinformation,thisnewversionimprovesaccuracyong2pconversion.6ConclusionsAsigniﬁcantimprovementinF-scorewasachievedbythreesimplemodiﬁcationstotheRePortSal-gorithm:generatinganintermediaryhigh-precisionstemcandidatelist,usingalanguagemodeltodis-ambiguatebetweenalternativesegmentations,andlearningpatternsforregularstemvariation,whichcanthenalsobeexploitedforsegmentation.Thesemodiﬁcationsimprovedresultsonfourdifferentlan-guagesconsidered:English,German,TurkishandFinnish,andachievedthebestresultsreportedsofarforanunsupervisedsystemformorphologicalseg-mentationonGerman.Weshowedthatthenewver-sionofthealgorithmistheonlyunsupervisedsys-temamongthesystemsevaluatedherethatachievessufﬁcientqualitytoimprovetranscriptionperfor-manceonagrapheme-to-phonemeconversiontask.AcknowledgmentsIwouldliketothankEmilyPitlerandSamarthKe-shavaformakingavailablethecodeoftheRePortSalgorithm,andStefanBordagandDelphineBern-hardforrunningtheiralgorithmsontheGermandata.ManythanksalsotoMattiVarjokallioforeval-uatingthedataontheMorphoChallengetestsetsforFinnish,TurkishandEnglish.Furthermore,IamverygratefultoChristophZwirelloandGregorM¨ohlerfortrainingthedecisiontreeonthenewmor-phologicalsegmentation.IalsowanttothankFrankKellerandtheACLreviewersforvaluableandin-sightfulcomments.ReferencesDelphineBernhard.2006.Unsupervisedmorphologicalseg-mentationbasedonsegmentpredictabilityandwordseg-mentsalignment.InProceedingsof2ndPascalChallengesWorkshop,pages19–24,Venice,Italy.StefanBordag.2006.Two-stepapproachtounsupervisedmor-phemesegmentation.InProceedingsof2ndPascalChal-lengesWorkshop,pages25–29,Venice,Italy.MathiasCreutzandKristaLagus.2006.Unsupervisedmodelsformorphemesegmentationandmorphologylearning.InACMTransactiononSpeechandLanguageProcessing.H.D´ejean.1998.Morphemesasnecessaryconceptsforstruc-tures:Discoveryfromuntaggedcorpora.InWorkshoponparadigmsandGroundinginNaturalLanguageLearning,pages295–299,Adelaide,Australia.VeraDemberg.2006.Letter-to-phonemeconversionforaGer-manTTS-System.Master’sthesis.IMS,Univ.ofStuttgart.VeraDemberg.2007.Phonologicalconstraintsandmorpho-logicalpreprocessingforgrapheme-to-phonemeconversion.InProc.ofACL-2007.EricGaussier.1999.Unsupervisedlearningofderivationalmorphologyfrominﬂectionallexicons.InACL’99Work-shopProceedings,UniversityofMaryland.CELEXGermanLinguisticUserGuide,1995.CenterforLex-icalInformation.Max-Planck-InstitutforPsycholinguistics,Nijmegen.JohnGoldsmith.2001.Unsupervisedlearningofthemor-phologyofanaturallanguage.computationalLinguistics,27(2):153–198,June.S.GoldwaterandD.McClosky.2005.Improvingstatisticalmtthroughmorphologicalanalysis.InProc.ofEMNLP.MargaretA.HaferandStephenF.Weiss.1974.Wordsegmen-tationbylettersuccessorvarieties.InformationStorageandRetrieval10,pages371–385.ZelligHarris.1955.Fromphonemetomorpheme.Language31,pages190–222.ChristianJacquemin.1997.Guessingmorphologyfromtermsandcorpora.InResearchandDevelopmentinInformationRetrieval,pages156–165.S.KeshavaandE.Pitler.2006.Asimpler,intuitiveapproachtomorphemeinduction.InProceedingsof2ndPascalChal-lengesWorkshop,pages31–35,Venice,Italy.M.Kurimo,M.Creutz,M.Varjokallio,E.Arisoy,andM.Sar-aclar.2006.Unsupervsiedsegmentationofwordsintomor-phemes–Challenge2005:Anintroductionandevaluationreport.InProc.of2ndPascalChallengesWorkshop,Italy.J.LucassenandR.Mercer.1984.Aninformationtheoreticapproachtotheautomaticdeterminationofphonemicbase-forms.InICASSP9.C.Monson,A.Lavie,J.Carbonell,andL.Levin.2004.Un-supervisedinductionofnaturallanguagemorphologyinﬂec-tionclasses.InProceedingsoftheSeventhMeetingofACL-SIGPHON,pages52–61,Barcelona,Spain.C.MonzandM.deRijke.2002.Shallowmorphologicalanaly-sisinmonolingualinformationretrievalforDutch,German,andItalian.InProceedingsCLEF2001,LNCS2406.SylvainNeuvelandSeanFulop.2002.Unsupervisedlearningofmorphologywithoutmorphemes.InProc.oftheWshponMorphologicalandPhonologicalLearning,ACLPub.HelmutSchmid,ArneFitschen,andUlrichHeid.2004.SMOR:AGermancomputationalmorphologycoveringderivation,compositionandinﬂection.InProc.ofLREC.PatrickSchoneandDanielJurafsky.2000.Knowledge-freeinductionofmorphologyusinglatentsemanticanalysis.InProc.ofCoNLL-2000andLLL-2000,Lisbon,Portugal.Tageszeitung(TAZ)Corpus.ContrapressMediaGmbH.https://www.taz.de/pt/.etc/nf/dvd.DavidYarowskiandRichardWicentowski.2000.Minimallysupervisedmorphologicalanalysisbymultimodalalign-ment.InProceedingsofACL2000,HongKong.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 928–935,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

928

UsingMazurkiewiczTraceLanguagesforPartition-BasedMorphologyFranc‚oisBarth·elemyCNAMCedric,292rueSaint-Martin,75003Paris(France)INRIAAtoll,domainedeVoluceau,78153LeChesnaycedex(France)barthe@cnam.frAbstractPartition-basedmorphologyisanapproachof(cid:2)nite-statemorphologywhereagrammardescribesaspecialkindofregularrelations,whichsplitallthestringsofagiventupleintothesamenumberofsubstrings.Theyarecompiledin(cid:2)nite-statemachines.Inthispaper,weaddressthequestionofmerginggrammarsusingdifferentpartitioningsintoasingle(cid:2)nite-statemachine.Amorphologi-caldescriptionmaythenbeobtainedbypar-allelorsequentialapplicationofconstraintsexpressedondifferentpartitionnotions(e.g.morpheme,phoneme,grapheme).Thethe-oryofMazurkiewiczTraceLanguages,awellknownsemanticsofparallelsystems,providesawayofrepresentingandcompil-ingsuchadescription.1Partition-BasedMorphologyFinite-StateMorphologyisbasedontheideathatregularrelationsareanappropriateformalismtode-scribethemorphologyofanaturallanguage.Sucharelationisasetofpairs,the(cid:2)rstcomponentbeinganactualformcalledsurfaceform,thesecondcompo-nentbeinganabstractdescriptionofthisformcalledlexicalform.Itisusuallyimplementedbya(cid:2)nite-statetransducer.Relationsarenotoriented,sothesametransducermaybeusedbothforanalysisandgeneration.Theymaybenon-deterministic,whenthesameformbelongstoseveralpairs.Further-more,(cid:2)nitestatemachineshaveinterestingproper-ties,theyarecomposableandef(cid:2)cient.TherearetwomaintrendsinFinite-StateMor-phology:rewrite-rulesystemsandtwo-levelrulesystems.Rewrite-rulesystemsdescribethemor-phologyoflanguagesusingcontextualrewriteruleswhichareeasilyappliedincascade.Rulesarecom-piledinto(cid:2)nite-statetransducersandmergedusingtransducercomposition(KaplanandKay,1994).TheotherimportanttrendofFinite-StateMor-phologyisTwo-LevelMorphology(Koskenniemi,1983).Inthisapproach,notonlypairsoflexicalandsurfacestringsarerelated,butthereisaone-to-onecorrespondencebetweentheirsymbols.Itmeansthatthetwostringsofagivenpairmusthavethesamelength.Wheneverasymbolofonesidedoesnothaveanactualcounterpartintheotherstring,aspecialsymbol0isinsertedattherelevantpo-sitioninordertoful(cid:2)llthesame-lengthconstraint.Forexample,thecorrespondencebetweenthesur-faceformspiesandthemorphemeconcatenationspy+sisgivenasfollows:spy0+sspie0sSame-lengthrelationsareclosedunderintersection,sotwo-levelgrammarsdescribeasystemasthesi-multaneousapplicationoflocalconstraints.Athirdapproach,Partition-BasedMorphology,consistsinsplittingthestringsofapairintothesamenumberofsubstrings.Thesame-lengthconstraintdoesnotholdonsymbolsbutonsubstrings.Forex-ample,spiesandspy+smaybepartitionedasfollows:spy+sspiesThepartition-basedapproachwas(cid:2)rstproposedby(Blacketal.,1987)andfurtherimprovedby(Pul-manandHepple,1993)and(Grimley-Evansetal.,929

1996).Ithasbeenusedtodescribethemorphol-ogyofSyriac(Kiraz,2000),Akkadian(Barth·elemy,2006)andArabicDialects(Habashetal.,2005).Theseworksusemulti-tapetransducersinsteadofusualtwotapetransducers,describingaspecialcaseofn-aryrelationsinsteadofbinaryrelations.De(cid:2)nition1Partitionedn-relationApartitionedn-relationisasetof(cid:2)nitesequencesofstringn-tuples.Forinstance,then-tuplesequenceoftheexample(spy,spies)givenaboveis(s,s)(p,p)(y,ie)(+,)(s,s).Ofcourse,allthepartitionedn-relationsarenotrecognizableusinga(cid:2)nite-statemachine.Grimley-Evansandal.proposeapartition-basedformalismwithastrongrestriction:thestringn-tuplesusedinthesequencesbelongtoa(cid:2)nitesetofsuchn-tuples(thecentersofcontext-restrictionrules).Theydescribeanalgorithmwhichcompilesasetofcontextualrulesdescribingapartitionedn-relationintoanepsilon-freelettertransducer.(Barth·elemy,2005)proposedamorepowerfulframework,wheretherelationsarede(cid:2)nedbyconcatenatingtuplesofindependentregularexpressionsandoperationsonpartitionedn-relationssuchasintersectionandcomplementationareconsidered.Inthispaper,weproposetouseMazurkiewiczTraceLanguagesinsteadofpartitionedrelationasthesemanticsofpartition-basedmorphologicalfor-malisms.Thebene(cid:2)tsaretwofold:(cid:2)rstly,thereisanextensionoftheformalpowerwhichallowsthecombinationofmorphologicaldescriptionusingdif-ferentpartitioningsofforms.Secondly,thecompi-lationofsuchlanguagesinto(cid:2)nite-statemachineshasbeenexhaustivelystudied.Theirclosureprop-ertiesprovideoperationsusefulformorphologicalpurposes.Theyincludetheconcatenation(forinstanceforcompoundwords),theintersectionusedtomergelocalconstraints,theunion(modularlexicon),thecomposition(cascadingdescriptions,formrecogni-tionandgeneration),theprojection(toextractoneleveloftherelation),thecomplementationandsetdifference,usedtocompilecontextualrulesfol-lowingthealgorithmsin(KaplanandKay,1994),(Grimley-Evansetal.,1996)and(Yli-Jyr¤aandKoskenniemi,2004).Theuseofthenewsemanticsdoesnotimplyanychangeoftheuser-levelformalisms,thankstoastraightforwardhomomorphismfrompartitionedn-relationstoMazurkiewiczTraceLanguages.2MazurkiewiczTraceLanguagesWithinagivenn-tuple,thereisnomeaningfulorderbetweensymbolsofthedifferentlevels.Mazurkiewicztracelanguagesisatheorywhichex-pressespartialorderingbetweensymbols.Theyhavebeende(cid:2)nedandstudiedintherealmofpar-allelcomputing.Inthissection,werecalltheirde(cid:2)nitionandsomeclassicalresults.(DiekertandM·etivier,1997)givesanexhaustivepresentationonthesubjectwithadetailedbibliography.Itcontainsalltheresultsmentionedhereandreferstotheirorig-inalpublication.2.1De(cid:2)nitionsAPartiallyCommutativeMonoidisde(cid:2)nedonanalphabetΣwithanindependencebinaryrelationIoverΣ×Σwhichissymmetricandirre(cid:3)exive.Twoindependentsymbolscommutefreelywhereasnon-independentsymbolsdonot.Ide(cid:2)nesanequiva-lencerelation∼IonΣ∗:twowordsareequivalentifoneistheresultofaseriesofcommutationofpairsofsuccessivesymbolswhichbelongtoI.Thenota-tion[x]isusedtodenotetheequivalenceclassofastringxwithrespectto∼I.ThePartiallyCommutativeMonoidM(Σ,I)isthequotientofthefreemonoidΣ∗bytheequiva-lencerelation∼I.ThebinaryrelationD=(Σ×Σ)−Iiscalledthedependencerelation.Itisre(cid:3)exiveandsymmetric.ϕisthecanonicalhomomorphismde(cid:2)nedby:ϕ:Σ∗→M(Σ,I)x7→[x]AMazurkiewicztracelanguage(abbreviation:tracelanguage)isasubsetofapartiallycommuta-tivemonoidM(Σ,I).2.2RecognizableTraceLanguagesAtracelanguageTissaidrecognizableifthereexistsanhomomorphismνfromM(Σ,I)toa(cid:2)-nitemonoidSsuchthatT=ν−1(F)forsomeF⊆S.ArecognizableTraceLanguagemaybeimplementedbyaFinite-StateAutomaton.930

Atrace[x]issaidtobeconnectedifthedepen-dencerelationrestrictedtothealphabetof[x]isaconnectedgraph.Atracelanguageisconnectedifallitstracesareconnected.Astringxissaidtobeinlexicographicnormalformifxisthesmalleststringofitsequivalenceclass[x]withrespecttothelexicographicorderinginducedbyanorderingonΣ.ThesetofstringsinlexicographicnormalformiswrittenLexNF.Thissetisaregularlanguagewhichisdescribedbythefollowingregularexpression:LexNF=Σ∗−S(a,b)∈I,a<bΣ∗b(I(a))∗aΣ∗whereI(a)denotesthesetofsymbolsindependentfroma.Property1LetT⊆M(Σ,I)beatracelanguage.Thefollowingassertionsareequivalent:•Tisrecognizable•TisexpressibleasarationalexpressionwheretheKleenestarisusedonlyonconnectedlan-guages.•ThesetMin(T)={x∈LexNF|[x]∈T}isaregularlanguageoverΣ∗.Recognizabilityiscloselyrelatedtothenotionofiterativefactor,whichisthelanguage-levelequiva-lentofaloopina(cid:2)nite-statemachine.Iftwosym-bolsaandbsuchthata<bbelongtoaloop,andiftheloopistraversedseveraltimes,thenoccurrencesofaandbareinterlaced.Forsuchastringtobeinlexicographicnormalform,adependentsymbolmustappearintheloopbetweenbanda.2.3OperationsandclosurepropertiesRecognizabletracelanguagesareclosedunderin-tersectionandunion.Furthermore,Min(T1)∪Min(T2)=Min(T1∪T2)andMin(T1)∩Min(T2)=Min(T1∩T2).Itcomesfromthefactthatintersec-tionanduniondonotcreatenewiterativefactor.Thepropertyonlexicographicnormalformcomesfromthefactthatallthetracesintheresultoftheopera-tionbelongtoatleastoneoftheoperandswhichareinnormalform.Recognizabletracelanguageareclosedunderconcatenation.Concatenationdonotcreatenewit-erativefactors.TheconcatenationMin(T1)Min(T2)isnotnecessarilyinlexicographicnormalform.Forinstance,supposethata>b.Then{[a]}.{[b]}={[ab]},butMin({[a]})=a,Min({[b]})=b,andMin({[ab]})=ba.Recognizabletracelanguagesareclosedundercomplementation.RecognizableTraceLanguagesarenotclosedun-derKleenestar.Forinstance,a<b,Min([ab]∗)=anbnwhichisknownnottoberegular.TheprojectiononasubsetSofΣistheopera-tionwrittenπS,whichdeletesalltheoccurrencesofsymbolsinΣ−Sfromthetraces.Recogniz-abletracelanguagesarenotclosedunderprojection.Thereasonisthattheprojectionmaydeletesymbolswhichmakesthelanguagesofloopsconnected.3PartitionedrelationsandtracelanguagesItispossibletoconvertapartitionedrelationintoatracelanguageasfollows:•representthepartitionboundariesusingasym-bolωnotinΣ.•distinguishthesymbolsaccordingtothecom-ponent(tape)ofthen-tupletheybelongto.Forthispurpose,wewilluseasubscript.•de(cid:2)nethedependencerelationDby:(cid:150)ωisdependentfromalltheothersymbols(cid:150)symbolsinΣsharingthesamesubscriptaremutuallydependentwhereassymbolshavingdifferentsubscriptaremutuallyin-dependent.Forinstance,thespyn-tuplesequence(s,s)(p,p)(y,ie)(+,)(s,s)istranslatedintothetraceωs1s2ωp1p2ωy1i2e2ω+1ωs1s2ω.The(cid:2)gure1givesthepartialorderbetweensymbolsofthistrace.Thedependencerelationisintuitivelysound.Forinstance,inthethirdn-tuple,thereisadependencybetweeniandewhichcannotbepermuted,butthereisnodependencybetweeni(resp.e)andy:iisnei-therbeforenoraftery.Therearethreeequivalentpermutations:y1i2e2,i2y1e2andi2e2y1.Inanim-plementation,onecanonicalrepresentationmustbechosen,inordertoensurethatsetoperations,suchasintersection,arecorrect.Thenotionoflexicographicnormalform,basedonanyarbitrarybut(cid:2)xedorderonsymbols,givessuchacanonicalform.931

tape 1tape 2ws1s2wwp1p2i2e2y1w+1ws1s2wFigure1:PartiallyorderedsymbolsThecompilationofthetracelanguageintoa(cid:2)nite-stateautomatonhasbeenstudiedthroughthenotionofrecognizability.Thisautomatonisverysimilartoann-tapetransducer.TheTraceLan-guagetheorygivespropertiessuchasclosureunderintersectionandsoundnessofthelexicographicnor-malform,whichdonotholdforusualtransducersclasses.Italsoprovidesacriteriontorestrictthede-scriptionoflanguagesthroughregularexpressions.Thisrestrictionisthattheclosureoperator(Kleenestar)mustoccuronconnectedlanguagesonly.Inthetranslationofapartition-basedregularexpression,astarmayappeareitheronastringofsymbolsofagiventapeoronastringwithatleastoneoccurrenceofω.Anotherbene(cid:2)tofMazurkiewicztracelanguageswithrespecttopartitionedrelationsistheirabilitytorepresentthesegmentationofthesameformus-ingtwodifferentpartitionings.Theexampleof(cid:2)g-ure2usestwopartitioningsoftheformspy+s,onebasedonthenotionofmorpheme,theotheronthenotionofphoneme.Thenotation<pos=noun>and<number=pl>standsfortwosinglesymbols.Flatfeaturestructuresover(small)(cid:2)nitedomainsareeasilyrepresentedbyastringofsuchsymbols.N-tuplesarenotveryconvenienttorepresentsuchasystem.Partition-basedformalismareespeciallyadaptedtoexpressrelationsbetweendifferentrepresentationsuchasfeaturestructuresandaf(cid:2)xes,withrespecttotwo-levelmorphologywhichimposesanarti(cid:2)cialsymbol-to-symbolmapping.Amulti-partitionedrelationmaybeobtainedbymergingthetranslationoftwopartition-basedgram-marswhichshareoneormorecommontapes.Suchamergingisperformedbythejoinoperatoroftherelationalalgebra.Usingapartition-basedgrammarforrecognitionorgenerationimpliessuchanoper-ation:thegrammarisjoinedwitha1-tapemachinewithoutpartitioningrepresentingtheformtoberec-ognized(surfacelevel)orgenerated(lexicallevel).4Multi-TapeTraceLanguagesInthissection,wede(cid:2)neasubclassofMazurkiewiczTraceLanguagesespeciallyadaptedtopartition-basedmorphology,thankstoanexplicitnotionoftapepartiallysynchronizedbypartitionboundaries.De(cid:2)nition2Amulti-tapepartiallycommutativemonoidisde(cid:2)nedbyatuple(Σ,Θ,Ω,µ)where•Σisa(cid:2)nitesetofsymbolscalledthealphabet.•Θisa(cid:2)nitesetofsymbolscalledthetapes.•Ωisa(cid:2)nitesetofsymbolswhichdonotbelongtoΣ,calledthepartitionboundaries.•µisamappingfromΣ∪Ωto2θsuchthatµ(x)isasingletonforanyx∈Σ.ItisthePartiallyCommutativeMonoidM(Σ∪Ω,Iµ)wheretheindependencerelationisde(cid:2)nedbyIµ={(x,y)∈Σ∪Ω×Σ∪Ω|µ(x)∩µ(y)=∅}.Notation:MPM(Σ,Θ,Ω,µ).AMulti-TapeTraceLanguageisasubsetofaMulti-Tapepartiallycommutativemonoid.Wenowaddresstheproblemofrelationalop-erationsoverRecognizableMulti-TapeTraceLan-guages.Recognizablelanguagesmaybeimple-mentedby(cid:2)nite-stateautomatainlexicographicnormalform,usingthemorphismϕ−1.Operationsontracelanguagesareimplementedbyoperationson(cid:2)nite-stateautomata.Wearelookingforimple-mentationspreservingthenormalformproperty,be-causechangingtheorderinregularlanguagesisnotastandardoperation.Somesetoperationsareverysimpletoimple-ment,namelyunion,intersectionanddifference.932

tape 1tape 3tape 2w1w2<pos=noun>s2s3w2w2p3p2i3e3w2y2w1<number=pl>w1w2s2s3Figure2:TwopartitionsofthesametapeTheelementsoftheresultofsuchanoperationbe-longstooneorbothoperands,andarethereforeinlexicographicnormalform.IfwewriteMin(T)thesetMin(T)={x∈LexNF|[x]∈T},whereTisaMulti-TapeTraceLanguage,wehavetriviallytheproperties:•Min(T1∪T2)=Min(T1)∪Min(T2)•Min(T1∩T2)=Min(T1)∩Min(T2)•Min(T1−T2)=Min(T1)−Min(T2)ImplementingthecomplementationisnotsostraightforwardbecauseMin(T)isusuallynotequaltoMin(T).ThelatersetcontainsstringsnotinlexicalnormalformswhichmaybelongtotheequivalenceclassofamemberofTwithrespectto∼I.ThecomplementationmustnotbecomputedwithrespecttoregularlanguagesbuttoLexNF.Min(T)=LexNF−Min(T)Asalreadymentioned,theconcatenationoftworegularlanguagesinlexicographicnormalformisnotnecessarilyinnormalform.Wedonothaveageneralsolutiontotheproblembuttwopartialso-lutions.Firstly,itiseasytotestwhetherthere-sultisactuallyinnormalformornot.Secondly,theresultisinnormalformwheneverasynchro-nizationpointbelongingtoallthelevelsisinsertedbetweenthestringsofthetwolanguages.Letωu∈Ω,µ(ωu)=Θ.Then,Min(T1.{ωu}.T2)=Min(T1).Min(ωu).Min(T2).Theclosure(Kleenestar)operationcreatesanewiterativefactorandtherefore,theresultmaybeanonrecognizabletracelanguage.Hereagain,con-catenatingaglobalsynchronizationpointattheendofthelanguagegivesatracelanguageclosedunderKleenestar.Byde(cid:2)nition,suchalanguageiscon-nected.Furthermore,theresultisinnormalform.Sofar,operationshaveoperandsandtheresultbe-longingtothesameMulti-tapeMonoid.Itisnotthecaseofthelasttwooperations:projectionandjoin.WeusethetheoperatorsDom,Range,andtherelationsIdandInsertasde(cid:2)nedin(KaplanandKay,1994):•Dom(R)={x|∃y,(x,y)∈R}•Range(R)={y|∃x,(x,y)∈R}•Id(L)={(x,x)|x∈L}•Insert(S)=(Id(Σ)∪({}×S))∗.ItisusedtoinsertfreelysymbolsfromSinastringfromΣ∗.Conversely,Insert(S)−1removesalltheoccurrencesofsymbolsfromS,ifS∩Σ=∅.Theresultofaprojectionoperationmaynotberecognizableifitdeletessymbolsmakingiterativefactorsconnected.Furthermore,whentheresultisrecognizable,theprojectiononMin(T)isnotnec-essarilyinnormalform.Bothphenomenacomefromthedeletionofsynchronizationpoints.There-fore,aprojectionwhichdeletesonlysymbolsfromΣissafe.Thedeletionofsynchronizationpointsisalsopossiblewhenevertheydonotsynchronizeany-thingmoreintheresultoftheprojectionbecauseallbutpossiblyoneofitstapeshavebeendeleted.Inthetape-orientedcomputationsystem,wearemainlyinterestedintheprojectionwhichdeletessometapesandpossiblysomerelatedsynchroniza-tionpoints.Property2ProjectionLetTbeatracelanguageovertheMTMM=(Σ,Θ,w,µ).LetΩ1⊂ΩandΘ1⊂Θ.If933

∀ω∈Ω−Ω1,|µ(ω)∩Θ1|≤1,thenMin(πΘ1,Ω1(T))=Range(Insert({x∈Σ|µ(x)/∈Θ1}∪Ω−Ω1)−1◦Min(T))Thejoinoperationisnamedbyanalogywiththeoperatoroftherelationalalgebra.Ithasbeende(cid:2)nedon(cid:2)nite-statetransducers(Kempeetal.,2004).De(cid:2)nition3Multi-tapejoinLetT1⊂MTM(Σ1,Θ1,Ω1,µ1)andT2⊂TM(Σ2,Θ2,Ω2,µ2)betwomulti-tapetracelan-guages.T11T2isde(cid:2)nedifandonlyif•∀σ∈Σ1∩Σ2,µ1(σ)∩Θ2=µ2(σ)∩Θ1•∀ω∈Ω1∩Ω2,µ1(ω)∩Θ2=µ2(ω)∩Θ1TheMulti-tapeTraceLanguageT11T2isde(cid:2)nedontheMulti-tapePartiallyCommutativeMonoidMTM(Σ1∪Σ2,Θ1∪Θ2,Ω1∪Ω2,µ)whereµ(x)=µ1(x)∪µ2(x).Itisde(cid:2)nedbyπΣ1∪Θ1∪Ω1(T11T2)=T1andπΣ2∪Θ2∪Ω2(T11T2)=T2.IfthetwooperandsT1andT2belongtothesameMTM,thenT11T2=T1∩T2.Iftheoperandsbelongtodisjointmonoids(whichdonotshareanysymbol),thenthejoinisaCartesianproduct.Theimplementationofthejoinreliesonthe(cid:2)nite-stateintersectionalgorithm.Thisalgorithmworkswheneverthecommonsymbolsofthetwolanguagesappearinthesameorderinthetwooperands.Thenormalformdoesnotensurethisproperty,becausesymbolsinthecommonpartofthejoinmaybesyn-chronizedbytapesnotinthecommonpart,bytran-sitivity,likeintheexampleofthe(cid:2)gure3.Inthisexample,contape3andfontape1areorderedc<fbytransitivityusingtape2.bcw1aw2fgtape 1tape 2tape 3w0w0deFigure3:indirecttapesynchronizationLetT⊆MPM(Σ,Θ,Ω,µ)amulti-partitiontracelanguage.LetGTbethelabeledgraphwherethenodesarethetapesymbolsfromΘandtheedgesaretheset{(x,ω,y)∈Θ×Ω×Θ|x∈µ(ω)andy∈µ(ω)}.LetSync(Θ)bethesetde-(cid:2)nedbySync(Θ)={ω∈Ω|ωappearsinGTonapathbetweentwotapesofΘ}.TheGTgraphforexampleofthe(cid:2)gure3isgivenin(cid:2)gure4andSync({1,3})={ω0,ω1,ω2}.tape 2w0w0w1tape 1w2w0tape 3Figure4:theGTgraphSync(Θ)isdifferentfromµ−1(Θ)∩Ωbecausesomesynchronizationpointsmayinduceanorderbetweentwotapesbytransitivity,usingothertapes.Property3LetT1⊆MPM(Σ1,Θ1,Ω1,µ1)andT2⊆MPM(Σ2,Θ2,Ω2,µ2)betwomulti-partitiontracelanguages.LetΣ=Σ1∩Σ2andΩ=Ω1∩Ω2.IfSync(Θ1∩Θ2)⊆Ω,thenπΣ∪Ω(Min(T1))∩πΣ∪Ω(Min(T2))=Min(πΣ∪Ω(T1)∩πΣ∪Ω(T2)Thispropertyexpressesthefactthatsymbolsbe-longingtobothlanguagesappearinthesameorderinlexicographicnormalformswheneverallthedi-rectandindirectsynchronizationsymbolsbelongtothetwolanguagestoo.Property4LetT1⊆MPM(Σ1,Θ1,Ω1,µ1)andT2⊆MPM(Σ2,Θ2,Ω2,µ2)betwomulti-partitiontracelanguages.IfΘ1∩Θ2isasingleton{θ}andif∀ω∈Ω1∩Ω2,θ∈µ(ω),thenπΣ∪Ω(Min(T1))∩πΣ∪Ω(Min(T2))=Min(πΣ∪Ω(T1)∩πΣ∪Ω(T2)Thissecondpropertyexpressesthefactthatsym-bolsappearnecessarilyinthesameorderinthetwooperandsiftheintersectionofthetwolanguagesisrestrictedtosymbolsofasingletape.Thispropertyisstraightforwardsincesymbolsofagiventapearemutuallydependent.Wenowde(cid:2)neacomputationover(Σ∪Ω)∗whichcomputesMin(T11T2).LetT1⊂MTM(Σ1,Θ1,ω1,µ1)andT2⊂MTM(Σ2,Θ2,Ω2,µ2)betworecognizablemulti-tapetracelanguages.IfSync(Θ1∩Θ2)⊆Ω,thenMin(T11T2)=Range(Min(T1◦Insert(Σ2−Σ1)◦Id(LexNF))∩Range(Min(T2)◦Insert(Σ1−Σ2)◦Id(LexNF)).934

5AshortexampleWehavewrittenamorphologicaldescriptionofTurkishverbalmorphologyusingtwodifferentpar-titionings.The(cid:2)rstonecorrespondstothenotionofaf(cid:2)x(morpheme).Itisusedtodescribethemor-photacticsofthelanguageusingrulessuchasthefollowingcontext-restrictionrule:(y?I4m,1sing)⇒(I?yor,prog)|(y?E2cE2k,future)Inthisrule,y?standsforanoptionaly,I4andE2forabstractvowelswhichrealizationsaresubjecttovowelharmonyandI?isanoptionaloccurrenceofthe(cid:2)rstvowel.Therulemayberead:thesuf(cid:2)xy?I4mdenotinga(cid:2)rstpersonsingularmayappearonlyafterthesuf(cid:2)xofprogressiveorthesuf(cid:2)xoffuture1.Suchrulesdescribesimplyaf(cid:2)xorderinverbalforms.Thesecondpartitioningisasymbol-to-symbolcorrespondencesimilartotheoneusedinstandardtwo-levelmorphology.Thispartitioningismoreconvenienttoexpresstheconstraintsofvowelhar-monywhichoccursanywhereintheaf(cid:2)xesanddoesnotdependonaf(cid:2)xboundaries.Herearetwooftherulesimplementingvowelhar-mony:(I4,i)⇒(Vow,e|i)(Cons,Cons)*(I4,u)⇒(Vow,o|u)(Cons,Cons)*VowandConsdenoterespectivelythesetsofvowelsandconsonants.Theserulesmayberead:asymbolI4isrealizedasi(resp.u)whenevertheclosestpre-cedingvowelisrealizedaseori(resp.ooru).Therealizationornotofanoptionallettermaybeexpressedusingoneortheotherpartitioning.Theseoptionallettersalwaysappearinthe(cid:2)rstpositionofanaf(cid:2)xanddependsonlyonthelastletteroftheprecedingaf(cid:2)x.(y?,y)⇒(Vow,Vow)Hereisanexampleofaverbalformgivenasa3-taperelationpartitionedusingthetwopartitionings.verbalrootprog1singgelI?yorY?I4mgeliyorumThetranslationofeachruleintoaMulti-tapeTraceLanguageinvolvestwotasks:introducingpar-1Theactualrulehas5otheralternativetenses.Ithasbeenshortenedforclarity.titionboundarysymbolsateachfrontierbetweenpartitions.Adifferentsymbolisusedforeachkindofpartitioning.Distinguishingsymbolsfromdiffer-enttapesinordertoensurethatµ(x)isasingletonforeachx∈Σ.SymbolsofΣarethereforepairswiththesymbolappearingintheruleas(cid:2)rstcom-ponentandthetapeidenti(cid:2)er,anumber,assecondcomponent.Anycompleteorderbetweensymbolswouldde(cid:2)nealexicographicnormalform.Theorderusedbyoursystemorderssymbolwithrespecttotapes:symbolsofthe(cid:2)rsttapearesmallerthanthesymbolsoftape2,andsoon.Theor-derbetweensymbolsofasametapeisnotimpor-tantbecausethesesymbolsaremutuallydependent.Thetranslationofatuple(a1...an,b1...bm)is(a1,1)...(an,1)(b1,2)...(bm,2)ω1.Suchastringisinlexicographicnormalform.Furthermore,thisexpressionisconnected,thankstothepartitionboundarywhichsynchronizesallthetapes,soitsclosureisrecognizable.Theconcatenationtooissafe.Allcontextualrulesarecompiledfollowingthealgorithmin(Yli-Jyr¤aandKoskenniemi,2004)2.Thenalltherulesdescribingaf(cid:2)xesareintersectedinanautomaton,andalltherulesdescribingsurfacetransformationareintersectedinanotherautomaton.Thenajoinisperformedtoobtainthe(cid:2)nalmachine.Thisjoinispossiblebecausetheintersectionofthetwolanguagesconsistsinonetape(cf.property4).Usingiteitherforrecognitionorgenerationisalsodonebyajoin,possiblyfollowedbyaprojection.Forinstance,torecognizeasurfaceformgeliyorum,(cid:2)rstcompileitinthemulti-tapetracelanguage(g,3)(e,3)(l,3)...(m,3),joinitwiththemorphologicaldescription,andthenprojectthere-sultontape1toobtainanabstractform(verbalroot,1)(prog,1)(1sing,1).Finallyex-tractthe(cid:2)rstcomponentofeachpair.6ConclusionPartition-orientedrulesareaconvenientwaytode-scribesomeoftheconstraintsinvolvedinthemor-phologyofthelanguage,butnotalltheconstraintsrefertothesamepartitionnotion.Describingarule2Twoothercompilationalgorithmalsoworkontherulesofthisexample(KaplanandKay,1994),(Grimley-Evansetal.,1996).(Yli-Jyr¤aandKoskenniemi,2004)ismoregeneral.935

withanirrelevantoneissometimesdif(cid:2)cultandin-elegant.Forinstance,describingvowelharmonyus-ingapartitioningbasedonmorphemestakesneces-sarilyseveralrulescorrespondingtothecaseswheretheharmonyiswithinamorphemeoracrossseveralmorphemes.Previouspartition-basedformalismsuseauniquepartitioningwhichisusedinallthecontextualrules.Ourpropositionistouseseveralpartitioningsinor-dertoexpressconstraintswiththepropergranular-ity.Typically,thesepartitioningscorrespondtothenotionsofmorphemes,phonemesandgraphemes.Partition-basedgrammarshavethesametheoret-icalpowerastwo-levelmorphology,whichisthepowerofregularlanguages.Itwasdesignedtore-main(cid:2)nite-stateandclosedunderintersection.Itiscompiledin(cid:2)nite-stateautomatawhichareformallyequivalenttotheepsilon-freelettertransducersusedbytwo-levelmorphology.Itissimplymoreeasytouseinsomecases,justliketwo-levelrulesaremoreconvenientthansimpleregularexpressionsforsomeapplications.Partition-Basedmorphologyisconvenientwhen-everthedifferentlevelsuseverydifferentrepresen-tations,likefeaturestructuresandstrings,ordif-ferentwritingsystems(e.g.Japanesehiraganaandtranscription).Two-levelrulesontheotherhandareconvenientwhenevertherelatedstringsarevari-antsofthesamerepresentationlikeintheexample(spy+s,spies).Notethatmulti-partitionmorphologymayuseaone-to-onecorrespondenceasoneofitspartitionings,andthereforeiscompatiblewithusualtwo-levelmorphology.Withrespecttorewriterulesystems,partition-basedmorphologygivesbettersupporttoparallelruleapplicationandcontextde(cid:2)nitionmayinvolveseverallevels.Thecounterpartisariskofcon(cid:3)ictsbetweencontextualrules.AcknowledgementWewouldliketothankananonymousrefereeofthispaperforhis/herhelpfulcomments.ReferencesFranc‚oisBarth·elemy.2005.Partitioningmultitapetrans-ducers.InInternationalWorkshoponFiniteStateMethodsinNaturalLanguageProcessing(FSMNLP),Helsinki,Finland.Franc‚oisBarth·elemy.2006.Unanalyseurmor-phologiqueutilisantlajointure.InTraitementAu-tomatiquedelaLangueNaturelle(TALN),Leuven,Belgium.AlanBlack,GraemeRitchie,StevePulman,andGrahamRussell.1987.Formalismsformorphographemicdescription.InProceedingsofthethirdconferenceonEuropeanchapteroftheAssociationforCompu-tationalLinguistics(EACL),pages11(cid:150)18.VolkerDiekertandYvesM·etivier.1997.Partialcommu-tationandtraces.InG.RozenbergandA.Salomaa,editors,HandbookofFormalLanguages,Vol.3,pages457(cid:150)534.Springer-Verlag,Berlin.EdmundGrimley-Evans,GeorgeKiraz,andStephenPul-man.1996.Compilingapartition-basedtwo-levelformalism.InCOLING,pages454(cid:150)459,Copenhagen,Denmark.NizarHabash,OwenRambow,andGeorgeKiraz.2005.Morphologicalanalysisandgenerationforarabicdi-alects.InProceedingsoftheACLWorkshoponSemiticLanguages,AnnHarbour,Michigan.RonaldM.KaplanandMartinKay.1994.Regularmod-elsofphonologicalrulesystems.ComputationalLin-guistics,20:3:331(cid:150)378.Andr·eKempe,Jean-MarcChamparnaud,andJasonEis-ner.2004.Anoteonjoinandauto-intersectionofn-aryrationalrelations.InB.WatsonandL.Cleophas,editors,Proc.EindhovenFASTARDays,Eindhoven,Netherlands.GeorgeAntonKiraz.2000.Multitierednonlinearmor-phologyusingmultitape(cid:2)niteautomata:acasestudyonsyriacandarabic.Comput.Linguist.,26(1):77(cid:150)105.KimmoKoskenniemi.1983.Two-levelmodelformor-phologicalanalysis.InIJCAI-83,pages683(cid:150)685,Karlsruhe,Germany.StephenG.PulmanandMarkR.Hepple.1993.Afeature-basedformalismfortwo-levelphonology.ComputerSpeechandLanguage,7:333(cid:150)358.AnssiYli-Jyr¤aandKimmoKoskenniemi.2004.Compil-ingcontextualrestrictionsonstringsinto(cid:2)nite-stateautomata.InB.WatsonandL.Cleophas,editors,Proc.EindhovenFASTARDays,Eindhoven,Nether-lands.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 936–943,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

936

Much ado about nothing:                                         A social network model of Russian paradigmatic gaps Robert Daland Andrea D. Sims Janet Pierrehumbert Department of Linguistics Northwestern University 2016 Sheridan Road Evanston, IL 60208 USA r-daland, andrea-sims, jbp@northwestern.edu      Abstract A number of Russian verbs lack 1sg non-past forms. These paradigmatic gaps are puzzling because they seemingly contradict the highly productive nature of inflectional systems. We model the persistence and spread of Russian gaps via a multi-agent model with Bayesian learning. We ran three simulations: no grammar learning, learning with arbitrary analogical pressure, and morphophonologically conditioned learning. We compare the results to the attested historical development of the gaps. Contradicting previous accounts, we propose that the persistence of gaps can be explained in the absence of synchronic competition between forms. 1 Introduction Paradigmatic gaps present an interesting challenge for theories of inflectional structure and language learning. Wug tests, analogical change and children’s overextensions of regular patterns demonstrate that inflectional morphology is highly productive. Yet lemmas sometimes have “missing” inflected forms. For example, in Russian the majority of verbs have first person singular (1sg) non-past forms (e.g., posadit’ ‘to plant’, posažu ‘I will plant’), but no 1sg form for a number of similar verbs (e.g., pobedit’ ‘to win’, *pobežu ‘I will win’). The challenge lies in explaining this apparent contradiction. Given the highly produc-tive nature of inflection, why do paradigmatic gaps arise? Why do they persist?     One approach explains paradigmatic gaps as a problem in generating an acceptable form.  Under this hypothesis, gaps result from irreconcilable conflict between two or more inflectional patterns.  For example, Albright (2003) presents an analysis of Spanish verbal gaps based on the Minimal Generalization Learner (Albright and Hayes 2002). In his account, competition between mid-vowel diphthongization (e.g., s[e]ntir ‘to feel’, s[je]nto ‘I feel’) and non-diphthongization (e.g., p[e]dir ‘to ask’, p[i]do ‘I ask’) leads to paradigmatic gaps in lexemes for which the applicability of diphthon-gization has low reliability (e.g., abolir ‘to abolish, *ab[we]lo, *ab[o]lo ‘I abolish’).   However, this approach both overpredicts and underpredicts the existence of gaps cross-linguistically.  First, it predicts that gaps should occur whenever the analogical forces determining word forms are contradictory and evenly weighted. However, variation between two inflectional patterns seems to more commonly result from such a scenario.  Second, the model predicts that if the form-based conflict disappears, the gaps should also disappear. However, in Russian and probably in other languages, gaps persist even after the loss of competing inflectional patterns or other synchronic form-based motivation (Sims 2006).   By contrast, our approach operates at the level of inflectional property sets (IPS), or more properly, at the level of inflectional paradigms.  We propose that once gaps are established in a language for whatever reason, they persist because learners infer the relative non-use of a given  1937

combination of stem and IPS.1  Put differently, we hypothesize that speakers possess at least two kinds of knowledge about inflectional structure: (1) knowledge of how to generate the appropriate form for a given lemma and IPS, and (2) knowledge of the probability with which that combination of lemma and property set is expressed, regardless of the form. Our approach differs from previous accounts in that persistence of gaps is attributed to the latter kind of knowledge, and does not depend on synchronic morphological competition. We present a case study of the Russian verbal gaps, which are notable for their persistence.  They arose between the mid 19th and early 20th century (Baerman 2007), and are still strongly attested in the modern language, but have no apparent synchronic morphological cause.   We model the persistence and spread of the Russian verbal gaps with a multi-agent model with Bayesian learning.  Our model has two kinds of agents, adults and children. A model cycle consists of two phases: a production-perception phase, and a learning-maturation phase. In the production-perception phase, adults produce a batch of linguistic data (verb forms), and children listen to the productions from the adults they know. In the learning-maturation phase, children build a grammar based on the input they have received, then mature into adults.  The existing adults die off, and the next generation of children is born. Our model exhibits similar behavior to what is known about the development of Russian gaps. 2 The historical and distributional facts of Russian verbal gaps 2.1 Traditional descriptions Grammars and dictionaries of Russian frequently cite paradigmatic gaps in the 1sg non-past.  Nine major dictionaries and grammars, including Švedova (1982) and Zaliznjak (1977), yielded a combined list of 96 gaps representing 68 distinct stems.  These verbal gaps fall almost entirely into the second conjugation class, and they overwhelmingly affect the subgroup of dental stems.  Commonly cited gaps include: *galžu ‘I make a hubbub’; *očučus’ ‘I come to be (REFL)’; 1SG *oščušču ‘I feel’; *pobežu ‘I will win’; and *ubežu ‘I will convince’.2                                                                                                  1Paradigmatic gaps also probably serve a sociolinguistic purpose, for example as markers of education, but socio-linguistic issues are beyond the scope of this paper. There is no satisfactory synchronic reason for the existence of the gaps.  The grouping of gaps among 2nd conjugation dental stems is seemingly non-arbitrary because these are exactly the forms that would be subject to a palatalizing morphopho-nological alternation (tj → tS or Sj, dj → Z, sj → S, zj → Z). Yet the Russian gaps do not meet the criteria for morphophonological competition as intended by Albright’s (2003) model, because the alternations apply automatically in Contemporary Standard Russian. Analogical forces should thus heavily favor a single form, for example, pobežu. Traditional explanations for the gaps, such as homophony avoidance (Švedova 1982) are also unsatisfactory since they can, at best, explain only a small percentage of the gaps. Thus, the data suggest that gaps persist in Russian primarily because they are not uttered, and this non-use is learned by succeeding generations of Russian speakers.3  The clustering of the gaps among 2nd conjugation dental stems most likely is partially a remnant of their original causes, and partially represents analogic extension of gaps along morphophonological lines (see 2.3 below). 2.2 Empirical evidence for and operational definition of gaps When dealing with descriptions in semi- prescriptive sources such as dictionaries, we must always ask whether they accurately represent language use. In other words, is there empirical evidence that speakers fail to use these words? We sought evidence of gaps from the Russian National Corpus (RNC). 4  The RNC is a balanced textual corpus with 77.6 million words consisting primarily of the contemporary Russian literary language.  The content is prose, plays, memoirs and biographies, literary criticism, newspaper and magazine articles, school texts, religious and  2 We use here the standard Cyrillic transliteration used by linguists.  It should not be considered an accurate phonological representation.  Elsewhere, when phonological issues are relevant, we use IPA. 3 See Manning (2003) and Zuraw (2003) on learning from implicit negative evidence. 4 Documentation: http://ruscorpora.ru/corpora-structure.html Mirror site used for searching: http://corpus.leeds.ac.uk/ruscorpora.html.      2938

philosophical materials, technical and scientific texts, judicial and governmental publications, etc. We gathered token frequencies for the six non-past forms of 3,265 randomly selected second conjugation verb lemmas.  This produced 11,729 inflected forms with non-zero frequency.5  As described in Section 3 below, these 11,729 form frequencies became our model’s seed data. To test the claim that Russian has verbal gaps, we examined a subsample of 557 2nd conjugation lemmas meeting the following criteria: (a) total non-past frequency greater than 36 raw tokens, and (b) 3sg and 3pl constituting less than 85% of total non-past frequency. 6  These constraints were designed to select verbs for which all six person-number combinations should be robustly attested, and to minimize sampling errors by removing lemmas with low attestation. We calculated the probability of the 1sg inflection by dividing the number of 1sg forms by the total number of non-past forms. The subset was bimodally distributed with one peak near 0%, a trough at around 2%, and the other peak at 13.3%.  The first peak represents lemmas in which the 1sg form is basically not used – gaps. Accordingly, we define gaps as second conjugation verbs which meet criteria (a) and (b) above, and for which the 1sg non-past form constitutes less than 2% of total non-past frequency for that lemma (N=56). In accordance with the grammatical descrip-tions, our criteria are disproportionately likely to identify dental stems as gaps. Still, only 43 of 412 dental stems (10.4%) have gaps, compared with 13 gaps among 397 examples of other stems (3.3%).   Second, not all dental stems are equally affected.  There seems to be a weak prototypicality effect centered around stems ending in /dj/, from which /tj/ and /zj/ each differ by one phonological feature.  There may also be some weak semantic factors that we do not consider here.  /dj/ /tj/ /zj/ /sj/ /stj/ 13.3% (19/143) 12.4% (14/118) 11.9% (5/42) 4.8% (3/62) 4.3% (2/47) Table 1. Distribution of Russian verbal gaps among dental stems                                                  5 We excluded 29 high-frequency lemmas for which the corpus did not provide accurate counts. 6 Russian has a number of verbs for which only the 3sg and 3pl are regularly used. 2.3 Some relevant historical facts A significant difference between the morpho-logical competition approach and our statistical learning approach is that the former attempts to provide a single account for both the rise and the perpetuation of paradigmatic gaps.  By contrast, our statistical learning model does not require that the morphological system provide synchronic motivation. The following question thus arises: Were the Russian gaps originally caused by forces which are no longer in play in the language? Baerman and Corbett (2006) find evidence that the gaps began with a single root, -bed- (e.g., pobedit’ ‘to win’), and subsequently spread analogically within dental stems.  Baerman (2007) expands on the historical evidence, finding that a conspiracy of several factors provided the initial push towards defective 1sg forms. Most important among these, many of the verbs with 1sg gaps in modern Russian are historically associated with aberrant morphophonological alternations. He argues that when these unusual alternations were eliminated in the language, some of the words failed to be integrated into the new morphological patterns, which resulted in lexically specified gaps. Important to the point here is that the elimination of marginal alternations removed an earlier synchronic motivation for the gaps.  Yet gaps have persisted and new gaps have arisen (e.g., pylesosit’ ‘to vacuum’). This persistence is the behavior that we seek to model. 3 Formal aspects of the model We take up two questions: How much machinery do we need for gaps to persist? How much machinery do we need for gaps to spread to phono-logically similar words?  We model three scenarios.  In the first scenario there is no grammar learning.   Adult agents produce forms by random sampling from the forms that heard as children, and child agents hear those forms. In the subsequent generation children become adults. In this scenario there is thus no analogical pressure. Any perse-verance of gaps results from word-specific learning. The second scenario is similar to the first, except that the learning process includes analogical pressure from a random set of words.  Specifically, for a target concept, the estimated distribution of its IPS is influenced by the distribution of known words. This enables the learner to express a known  3939

concept with a novel IPS. For example, imagine that a learner hears the present tense verb form googles, but not the past tense googled. By analogy with other verbs, learners can expect the past tense to occur with a certain frequency, even if they have not encountered it.   The third scenario builds upon the second.  In this version, the analogical pressure is not completely random.  Instead, it is weighted by morphophonological similarity – similar word forms contribute more to the analogical force on a target concept than do dissimilar forms.  This addition to the model is motivated by the pervasive importance of stem shape in the Russian morphological system generally, and potentially provides an account for the phonological prototypicality effect among Russian gaps. The three scenarios thus represent increasing machinery for the model, and we use them to explore the conditions necessary for gaps to persist and spread.  We created a multi-agent network model with Bayesian learning component.  In the following sections we describe the model’s structure, and outline the criteria by which we evaluate its output under the various conditions. 3.1 Social structure Our model includes two generations of agents.  Adult agents output linguistic forms, which provide linguistic input for child agents.  Output/input occurs in batches.7  After each batch all adults die, all children mature into adults, and a new generation of children is born. Each run of the model included 10 generations of agents.   We model the social structure with a random network.  Each adult produces 100,000 verb forms, and each child is exposed to every production from every adult to whom they are connected. Each generation consisted of 50 adult agents, and child agents are connected to adults with some probability p.  On average, each child agent is connected to 10 adult agents, meaning that each child hears, on average, 1,000,000 tokens. 3.2 Linguistic events Russian gaps are localized to second conjugation non-past verb forms, so productions of these forms are the focus of interest.  Formally, we define a linguistic event as a concept-inflection-form (C,I,F) triple. The concept serves to connect the different forms and inflections of the same lemma.                                                  7 See Niyogi (2006) for why batch learning is a reasonable approximation in this context. 3.3 Definition of grammar  A grammar is defined as a probability distribution over linguistic events. This gives rise to natural formulations of learning and production as statistical processes: learning is estimating a probability distribution from existing data, and production is sampling from a probability distribution.  The grammar can be factored into modular components:  p(C, I, F) = p(C) · p(I | C) · p(F | C, I)  In this paper we focus on the probability distribution of concept-inflection pairs.  In other words, we focus on the relative frequency of inflectional property sets (IPS) on a lemma-by-lemma basis, represented by the middle term above. Accordingly, we made the simplest possible assumptions for the first and last terms. To calculate the probability of a concept, children use the sample frequency (e.g., if they hear 10 tokens of the concept ‘eat’, and 1,000 tokens total, then p(‘eat’) = 10/1000 = .01). Learning of forms is perfect. That is, learners always produce the correct form for every concept-inflection pair. 3.4 Learning model Although production in the real world is governed by semantics, we treat it here as a statistical process, much like rolling a six-sided die which may or may not be fair. When producing a Russian non-past verb, there are six possible combinations of inflectional properties (3 persons * 2 numbers).  In our model, word learning involves estimating the probability distribution over the frequencies of the six forms on a lemma-by-lemma basis. A hypothetical example that introduces our variables:   jest’ 1sg 2sg 3sg 1pl 2pl 3pl SUM D 15 5 45 5 5 25 100 d 0.15 0.05 0.45 0.05 0.05 0.25 1 Table 2. Hypothetical probability distribution  The first row indicates the concept and the inflections. The second row (D) indicates the  4940

hypothetical number of tokens of jest’ ‘eat’ that the learner heard for each inflection (bolding indicates a six-vector).  We use |D| to indicate the sum of this row (=100), which is the concept frequency.  The third row (d) indicates the sample probability of that inflection, which is simply the second row divided by |D|.   The learner’s goal is to estimate the distribution that generated this data. We assume the multinomial distribution, whose parameter is simply the vector of probabilities of each IPS. For each concept, the learner’s task is to estimate the probability of each IPS, represented by h in the equations below.  We begin with Bayes’ rule:  p(h | D) ∝ p(h) · multinom(D | h)  The prior distribution constitutes the analogical pressure on the lemma. It is generated from the “expected” behavior, h0, which is an average of the known behavior from a random sample of other lemmas. The parameter κ determines the number of lemmas that are sampled for this purpose – it represents how many existing words affect a new word. To model the effect of morphophonological similarity (mpSim), in one variant of the model we weight this average by the similarity of the stem-final consonant.8  For example, this has the effect that existing dental stems have more of an effect on dental stems.  In this case, we define  h0 = Σc’ in sample d c’ · mpSim(c, c’)/Σ mpSim(c, c’)  We use a featural definition of similarity, so that if the stem-final consonants differ by 0, 1, 2, or 3 or more phonological features, the resulting similarity is 1, 2/3, 1/3, or 0, respectively. The prior distribution should assign higher probability to hypotheses that are “closer” to this expected behavior h0. Since the hypothesis is itself a probability distribution, the natural measure to use is the KL divergence. We used an exponentially distributed prior with parameter β:  p(h) ∝ exp(-β· h0 || h)                                                   8 In Russian, the stem-final consonant is important for morphological behavior generally. Any successful Russian learner would have to extract the generalization, completely apart from the issues posed by gaps. As will be shown shortly, β has a natural interpretation as the relative strength of the prior with respect to the observed data. The learner calculates their final grammar by taking the mode of the posterior distribution (MAP). It can be shown that this value is given by  arg max p(h | D) = (β· h0 + |D|· d)/(β+|D|)  Thus, the output of this learning rule is a probability vector h that represents the estimated probability of each of the six possible IPS’s for that concept. As can be seen from the equation above, this probability vector is an average of the expected behavior h0 and the observed data d, weighted by β and the amount of observed data |D|, respectively. Our approach entails that from the perspective of a language learner, gaps are not qualitatively distinct from productive forms.  Instead, 1sg non-past gaps represent one extreme of a range of probabilities that the first person singular will be produced.  In this sense, “gaps” represent an artificial boundary which we place on a gradient structure for the purpose of evaluating our model.  The contrast between our learning model and the account of gaps presented in Albright (2003) merits emphasis at this point.  Generally speaking, learning a word involves at least two tasks:  learning how to generate the appropriate phonological form for a given concept and inflectional property set, and learning the probability that a concept and inflectional property set will be produced at all.  Albright’s model focuses on the former aspect; our model focuses on the latter. In short, our account of gaps lies in the likelihood of a concept-IPS pair being expressed, not in the likelihood of a form being expressed. 3.5 Production model We model language production as sampling from the probability distribution that is the output of the learning rule. 3.6 Seeding the model The input to the first generation was sampled from the verbs identified in the corpus search (see 2.2). Each input set contained 1,000,000 tokens, which was the average amount of input for agents in all succeeding generations.  This made the first  5941

generation’s input as similar as possible to the input of all succeeding generations. 3.7 Parameter space in the three scenarios In our model we manipulate two parameters – the strength of the analogical force on a target concept during the learning process (β), and the number of concepts which create the analogical force (κ), taken randomly from known concepts.   As discussed above, we model three scenarios.  In the first scenario, there is no grammar learning, so there is only one condition (β = 0).  For the second and third scenarios, we run the model with four values for β, ranging from weak to strong analogical force (0.05, 0.25, 1.25, 6.25), and two values for κ, representing influence from a small or large set of other words (30, 300). 4 Evaluating the output of the model We evaluate the output of our model against the following question: How well do gaps persist?   We count as gaps any forms meeting the criteria outlined in 2.2 above, tabulating the number of gaps which exist for only one generation, for two total generations, etc.  We define τ as the expected number of generations (out of 10) that a given concept meets the gap criteria.  Thus, τ represents a gap’s “life expectancy” (see Figure 1). We found that this distribution is exponential – there are few gaps that exist for all ten generations, and lots of gaps that exist for only one, so we calculated τ with a log linear regression.  Each value reported is an average over 10 runs.   As discussed above, our goal was to discover whether the model can exhibit the same qualitative behavior as the historical development of Russian gaps.  Persistence across a handful of generations (so far) and spread to a limited number of similar forms should be reflected by a non-negligible τ.  5 Results In this section we present the results of our model under the scenarios and parameter settings above. Remember that in the first scenario there is no grammar learning. This run of the model represents the baseline condition – completely word-specific knowledge.  Sampling results in random walks on form frequencies, so once a word form disappears it never returns to the sample.  Word-specific learning is thus sufficient for the perseverance of existing paradigmatic gaps and the creation of new ones.  With no analogical pressure, gaps are robustly attested (τ = 6.32).  However, the new gaps are not restricted to the 1sg, and under this scenario, learners are unable to generalize to a novel pairing of lexeme + IPS.   The second scenario presents a more complicated picture.  As shown in Table 3, as analogical pressure (β) increases, gap life expectancy (τ) decreases.  In other words, high analogical pressure quickly eliminates atypical frequency distributions, such as those exhibited by gaps. The runs with low values of β are particularly interesting because they represent an approximate balance between elimination of gaps as a general behavior, and the short-term persistence and even spread of gaps due to sampling artifacts and the influence of existing gaps. Thus, although the limit behavior is for gaps to disappear, this scenario retains the ability to explain persistence of gaps due to word-specific learning when there is weak analogical force. At the same time, the facts of Russian differ from the behavior of the model in that the Russian gaps spread to morphophonologically similar forms, not random ones.  The third version of our model weights the analogical strength of different concepts based upon morphophonological similarity to the target.    κ β τ (random) τ  (phono.) -- 0 6.32  30 0.05 4.95 5.77 30 0.25 3.46 5.28 30 1.25 1.91 3.07 30 6.25 2.59 1.87  300 0.05 4.97 5.99 300 0.25 3.72 5.14 300 1.25 1.90 3.10 300 6.25 2.62 1.84 Table 3. Life expectancy of gaps, as a function of the strength of random analogical forces  Under these conditions we get two interesting results, presented in Table 3 above.  First, gaps persist slightly better overall in scenario 3 than in  6942

scenario 2 for all levels of κ and β. 9  Compare the τ values for random analogical force (scenario 2) with the τ values for morphophonologically weighted analogical force (scenario 3). Second, strength of analogical force matters. When there is weak analogical pressure, weighting for morphophonological similarity has little effect on the persistence and spread of gaps.  However, when there is relatively strong analogical pressure, morphophonological similarity helps atypical frequency distributions to persist, as shown in Figure 1.  This results from the fact that there is a prototypicality effect for gaps.  Since dental stems are more likely to be gaps, incorporating sensitivity to stem shape causes the analogical pressure on target dental stems to be relatively stronger from words that are gaps. Correspondingly, the analogical pressure on non-dental stems is relatively stronger from words that are not gaps.  The prototypical stem shape for a gap is thereby perpetuated and gaps spread to new dental stems.  012345612345678910# of generationslog(# of gaps)random, β = 0.05random, β = 1.25phonological, β = 0.05phonological, β = 1.25Figure 1. Gap life expectancy (β=0.05, κ=30)                                                    9 The apparent increase in gap half-life when β=6.25 is an artifact of the regression model. There were a few well-entrenched gaps whose high lemma frequency enables them to resist even high levels of analogical pressure over 10 generations.  These data points skewed the regression, as shown by a much lower R2 (0.5 vs. 0.85 or higher for all the other conditions).  6 Discussion In conclusion, our model has in many respects succeeded in getting gaps to perpetuate and spread.  With word-specific learning alone, well-entrenched gaps can be maintained across multiple generations.  More significantly, weak analogical pressure, especially if weighted for morpho-phonological similarity, results in the perseverance and short-term growth of gaps.   This is essentially the historical pattern of the Russian verbal gaps.  These results highlight several issues regarding both the nature of paradigmatic gaps and the structure of inflectional systems generally. We claim that it is not necessary to posit an irreconcilable conflict in the generation of inflected forms in order to account for gaps.  Remember that in our model, agents face no conflict in terms of which form to produce – there is only one possibility.  Yet the gaps persist in part because of analogical pressure from existing gaps.  Albright (2003) himself is agnostic on the issue of whether form-based competition is necessary for the existence and persistence of gaps, but Hudson (2000), among others, claims that gaps could not exist in the absence of it.  We have presented evidence that this claim is unfounded. But why would someone assume that grammar competition is necessary?  Hudson’s claim arises from a confusion of two issues.  Discussing the English paradigmatic gap amn’t, Hudson states that “a simple application of [the usage-based learning] principle would be to say that the gap exists simply because nobody says amn’t...  But this explanation is too simple... There are many inflected words that may never have been uttered, but which we can nevertheless imagine ourselves using, given the need; we generate them by generalization” (Hudson 2000:300).  By his logic, there must therefore be some source of grammar conflict which prevents speakers from generalizing.   However, there is a substantial difference between having no information about a word, and having information about the non-usage of a word.  We do not dispute learners’ ability to generalize.  We only claim that information of non-usage is sufficient to block such generalizations.  When confronted with a new word, speakers will happily generalize a word form, but this is not the same task that they perform when faced with gaps.  7943

The perseverance of gaps in the absence of form-based competition shows that a different, non-form level of representation is at issue.  Generating inflectional morphology involves at least two different types of knowledge: knowledge about the appropriate word form to express a given concept and IPS on the one hand, and knowledge of how often that concept and IPS is expressed on the other. The emergence of paradigmatic gaps may be closely tied to the first type of knowledge, but the Russian gaps, at least, persist because of the second type of knowledge.  We therefore propose that morphology may be defective at the morphosyntactic level. This returns us to the question that we began this paper with –  how paradigmatic gaps can persist in light of the overwhelming productivity of inflectional morphology.  Our model suggests that the apparent contradiction is, at least in some cases, illusory.  Productivity refers to the likelihood of a given inflectional pattern applying to a given combination of stem and IPS.  Our account is based in the likelihood of the stem and inflectional property set being expressed at all, regardless of the form.  In short, the Russian paradigmatic gaps represent an issue which is orthogonal to productivity.  The two issues are easily confused, however.  An unusual frequency distribution can make it appear that there is in fact a problem at the level of form, even when there may not be. Finally, our simulations raise the question of whether the 1sg non-past gaps in Russian will persist in the language in the long term. In our model, analogical forces delay convergence to the mean, but the limit behavior is that all gaps disappear.  Although there is evidence in Russian that words can develop new gaps, we do not know with any great accuracy whether the set of gaps is currently expanding, contracting, or approximately stable.  Our model predicts that in the long run, the gaps will disappear under general analogical pressure.  However, another possibility is that our model includes only enough factors (e.g., morphophonological similarity) to approximate the short-term influences on the Russian gaps and that we would need more factors, such as semantics, to successfully model their long-term development.  This remains an open question.  References Albright, Adam. 2003. A quantitative study of Spanish paradigm gaps. In West Coast Conference on Formal Linguistics 22 proceedings, eds. Gina Garding and Mimu Tsujimura. Somerville, MA: Cascadilla Press, 1-14. Albright, Adam, and Bruce Hayes. 2002. Modeling English past tense intuitions with minimal generalization. In Proceedings of the Sixth Meeting of the Association for Computational Linguistics Special Interest Group in Computational Phonology in Philadelphia, July 2002, ed. Michael Maxwell. Cambridge, MA: Association for Computational Linguistics, 58-69. Baerman, Matthew. 2007. The diachrony of defectiveness. Paper presented at 43rd Annual Meeting of the Chicago Linguistic Society in Chicago, IL, May 3-5, 2007. Baerman, Matthew, and Greville Corbett. 2006. Three types of defective paradigms. Paper presented at The Annual Meeting of the Linguistic Society of America in Albuquerque, NM, January 5-8, 2006. Hudson, Richard. 2000. *I amn’t. Language 76 (2):297-323. Manning, Christopher. 2003. Probabilistic syntax. In Probabilistic linguistics, eds. Rens Bod, Jennifer Hay and Stephanie Jannedy. Cambridge, MA: MIT Press, 289-341. Niyogi, Partha. 2006. The computational nature of language learning and evolution. Cambridge, MA: MIT Press. Sims, Andrea. 2006. Minding the gaps: Inflectional defectiveness in paradigmatic morphology. Ph.D. thesis: Linguistics Department, The Ohio State University. Švedova, Julja. 1982. Grammatika sovremennogo russkogo literaturnogo jayzka. Moscow: Nauka. Zaliznjak, A.A., ed. 1977. Grammatičeskij slovar' russkogo jazyka: Slovoizmenenie. Moskva: Russkij jazyk. Zuraw, Kie. 2003. Probability in language change. In Probabilistic linguistics, eds. Rens Bod, Jennifer Hay and Stephanie Jannedy. Cambridge, MA: MIT Press, 139-176.   8Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 944–951,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

944

Substring-BasedTransliterationTarekSherifandGrzegorzKondrakDepartmentofComputingScienceUniversityofAlbertaEdmonton,Alberta,CanadaT6G2E8{tarek,kondrak}@cs.ualberta.caAbstractTransliterationisthetaskofconvertingawordfromonealphabeticscripttoanother.Wepresentanovel,substring-basedap-proachtotransliteration,inspiredbyphrase-basedmodelsofmachinetranslation.Wein-vestigatetwoimplementationsofsubstring-basedtransliteration:adynamicprogram-mingalgorithm,andaﬁnite-statetransducer.Weshowthatoursubstring-basedtransducernotonlyoutperformsastate-of-the-artletter-basedapproachbyasigniﬁcantmargin,butisalsoordersofmagnitudefaster.1IntroductionAsigniﬁcantproportionofout-of-vocabularywordsinmachinetranslationmodelsorcrosslanguagein-formationretrievalsystemsarenamedentities.Ifthelanguagesarewrittenindifferentscripts,thesenamesmustbetransliterated.Transliterationisthetaskofconvertingawordfromonewritingscripttoanother,usuallybasedonthephoneticsoftheorig-inalword.Ifthetargetlanguagecontainsallthephonemesusedinthesourcelanguage,thetranslit-erationisstraightforward.Forexample,theArabictransliterationofAmandaisY	AÓ(cid:13),whichisessen-tiallypronouncedinthesameway.However,ifsomeofthesoundsaremissinginthetargetlan-guage,theyaregenerallymappedtothemostpho-neticallysimilarletter.Forexample,thesound[p]inthenamePaul,doesnotexistinArabic,andthephonotacticconstraintsofArabicdisallowthesound[A]inthiscontext,sothewordistransliteratedasÈñ.,pronounced[bul].Theinformationlossinherentintheprocessoftransliterationmakesback-transliteration,whichistherestorationofapreviouslytransliteratedword,aparticularlydifﬁculttask.Anyphoneticallyrea-sonableforwardtransliterationisessentiallycorrect,althoughoccasionallythereisastandardtranslitera-tion(e.g.OmarSharif).Intheoriginalscript,how-ever,thereisusuallyonlyasinglecorrectform.Forexample,bothNaguibMahfouzandNajibMahfuzarereasonabletransliterationsof	 ñ	®×.(cid:10).(cid:26)	,butTsharlzDykensiscertainlynotacceptableifoneisreferringtotheauthorofOliverTwist.Inastatisticalapproachtomachinetranslitera-tion,givenaforeignwordF,weareinterestedinﬁndingtheEnglishwordˆEthatmaximizesP(E|F).UsingBayes’rule,andkeepinginmindthatFisconstant,wecanformulatethetaskasfollows:ˆE=argmaxEP(F|E)P(E)P(F)=argmaxEP(F|E)P(E)Thisisknownasthenoisychannelapproachtomachinetransliteration,whichsplitsthetaskintotwoparts.Thelanguagemodelprovidesanesti-mateoftheprobabilityP(E)ofanEnglishword,whilethetransliterationmodelprovidesanestimateoftheprobabilityP(F|E)ofaforeignwordbeingatransliterationofanEnglishword.Theprobabilitiesassignedbythetransliterationandlanguagemod-elscounterbalanceeachother.Forexample,sim-plyconcatenatingthemostcommonmappingforeachletterintheArabicstringÉ¾(cid:10)AÓ,producesthestringmaykl,whichisbarelypronounceable.Inor-dertogeneratethecorrectMichael,amodelneeds945

toknowtherelativelyrareletterrelationshipsch/andae/ǫ,andtobalancetheirunlikelihoodagainsttheprobabilityofthecorrecttransliterationbeinganactualEnglishname.ThesearchfortheoptimalEnglishtransliterationˆEforagivenforeignnameFisreferredtoasde-coding.Anefﬁcientapproachtodecodingisdy-namicprogramming,inwhichsolutionstosubprob-lemsaremaintainedinatableandusedtobuilduptheglobalsolutioninabottom-upapproach.Dy-namicprogrammingapproachesareoptimalaslongasthedynamicprogramminginvariantassumptionholds.Thisassumptionstatesthatiftheoptimalpaththroughagraphhappenstogothroughstateq,thenthisoptimalpathmustincludethebestpathuptoandincludingq.Thus,onceanoptimalpathtostateqisfound,allotherpathstoqcanbeeliminatedfromthesearch.Thevalidityofthisassumptiondependsonthestatespaceusedtodeﬁnethemodel.Typ-ically,forproblemsrelatedtowordcomparison,adynamicprogrammingapproachwilldeﬁnestatesaspositionsinthesourceandtargetwords.Aswillbeshownlater,however,notallmodelscanberepre-sentedwithsuchastatespace.Thephrase-basedapproachdevelopedforstatis-ticalmachinetranslation(Koehnetal.,2003)isdesignedtoovercometherestrictionsonmany-to-manymappingsinword-basedtranslationmodels.Thisapproachisbasedonlearningcorrespondencesbetweenphrases,ratherthanwords.Phrasesaregeneratedonthebasisofaword-to-wordalignment,withtheconstraintthatnowordswithinthephrasepairarelinkedtowordsoutsidethephrasepair.Inthispaper,weproposetoapplyphrase-basedtranslationmethodstothetaskofmachinetranslit-eration,inanapproachwerefertoassubstring-basedtransliteration.Weconsidertwoimplemen-tationsofthesemodels.Theﬁrstisanadaptationofthemonotonesearchalgorithmoutlinedin(ZensandNey,2004).Thesecondencodesthesubstring-basedtransliterationmodelasatransducer.There-sultsofexperimentsonArabic-to-Englishtransliter-ationshowthatthesubstring-basedtransducerout-performsastate-of-the-artletter-basedtransducer,whileatthesametimebeingordersofmagnitudesmallerandfaster.Theremainderofthepaperisorganizedasfol-lows.Section2discussespreviousapproachestomachinetransliteration.Section3presentstheletter-basedtransducerapproachtoArabic-Englishtransliterationproposedin(Al-OnaizanandKnight,2002),whichweuseasthemainpointofcom-parisonforoursubstring-basedmodels.Section4presentsoursubstring-basedapproachestotranslit-eration.InSection5,weoutlinetheexperimentsusedtoevaluatethemodelsandpresenttheirresults.Finally,Section6containsouroverallimpressionsandconclusions.2PreviousWorkArababietal.(1994)proposetomodelforwardtransliterationthroughacombinationofneuralnetandexpertsystems.Theirmaintaskwastovow-elizetheArabicnamesasapreprocessingstepfortransliteration.TheirmethodisArabic-speciﬁcandrequiresthattheArabicnameshavearegularpatternofvowelization.KnightandGraehl(1998)modelthetranslitera-tionofJapanesesyllabickatakanascriptintoEn-glishwithasequenceofﬁnite-statetransducers.AfterperformingaconversionoftheEnglishandkatakanasequencestotheirphoneticrepresenta-tions,thecorrespondencesbetweentheEnglishandJapanesephonemesarelearnedwiththeexpectationmaximization(EM)algorithm.StallsandKnight(1998)adaptthisapproachtoArabic,withthemod-iﬁcationthattheEnglishphonemesaremappeddi-rectlytoArabicletters.Al-OnaizanandKnight(2002)ﬁndthatamodelmappingdirectlyfromEn-glishtoArabiclettersoutperformsthephoneme-to-lettermodel.AbdulJaleelandLarkey(2003)modelforwardtransliterationfromArabictoEnglishbytreatingthewordsassentencesandusingastatisticalwordalignmentmodeltoaligntheletters.TheyselectcommonEnglishn-gramsbasedoncaseswhenthealignmentlinksanArabiclettertoseveralEnglishletters,andconsiderthesen-gramsassinglelettersforthepurposeoftraining.TheEnglishtranslitera-tionsareproducedusingprobabilities,learnedfromthetrainingdata,forthemappingsbetweenArabiclettersandEnglishletters/n-grams.Lietal.(2004)proposealetter-to-lettern-gramtransliterationmodelforChinese-Englishtransliter-ationinanattempttoallowfortheencodingofmore946

contextualinformation.Themodelisolatesindivid-ualmappingoperationsbetweentrainingpairs,andthenlearnsn-gramprobabilitiesforsequencesofthesemappingoperations.Ekbaletal.(2006)adaptthismodeltothetransliterationofnamesfromBen-galitoEnglish.3Letter-basedTransliterationThemainpointofcomparisonfortheevaluationofoursubstring-basedmodelsoftransliterationistheletter-basedtransducerproposedby(Al-OnaizanandKnight,2002).Theirmodelisacompositionofatransliterationtransducerandalanguagetrans-ducer.Mappingsinthetransliterationtransduceraredeﬁnedbetween1-3Englishlettersand0-2Arabicletters,andtheirprobabilitiesarelearnedbyEM.Thetransliterationtransducerissplitintothreestatestoallowmappingprobabilitiestobelearnedsepa-ratelyforlettersatthebeginning,middleandendofaword.Unlikethetransducersproposedin(StallsandKnight,1998)and(KnightandGraehl,1998)noattemptismadetomodelthepronunciationofwords.Althoughnamesaregenerallytransliteratedbasedonhowtheysound,nothowtheylook,theletter-phonemeconversionitselfisproblematicasitisnotatrivialtask.Manytransliteratedwordsarepropernames,whosepronunciationrulesmayvarydependingonthelanguageoforigin(Lietal.,2004).Forexample,chisgenerallypronouncedaseither[]or[k]inEnglishnames,butas[S]inFrenchnames.Thelanguagemodelisimplementedasaﬁnitestateacceptorusingacombinationofwordunigramandlettertrigramprobabilities.Essentially,thewordunigrammodelactsasaprobabilisticlookuptable,allowingforwordsseeninthetrainingdatatobeproducedwithhighaccuracy,whilethelettertrigramprobabilitiesareusedmodelwordsnotseeninthetrainingdata.4Substring-basedTransliterationOursubstring-basedtransliterationapproachisanadaptationofphrase-basedmodelsofmachinetrans-lationtothedomainoftransliteration.Inparticular,ourmethodsareinspiredbythemonotonesearchalgorithmproposedin(ZensandNey,2004).Weintroducetwomodelsofsubstring-basedtranslitera-tion:theViterbisubstringdecoderandthesubstring-basedtransducer.Table1presentsacomparisonofthesubstring-basedmodelstotheletter-basedmodeldiscussedinSection3.4.1TheMonotoneSearchAlgorithmZensandNey(2004)proposealinear-timedecodingalgorithmforphrase-basedmachinetranslation.Thealgorithmrequiresthatthetranslationofphrasesbesequential,disallowinganyphrasereorderinginthetranslation.Startingfromaword-basedalignmentforeachpairofsentences,thetrainingforthealgorithmac-ceptsallcontiguousbilingualphrasepairs(uptoapredeterminedmaximumlength)whosewordsareonlyalignedwitheachother(Koehnetal.,2003).TheprobabilitiesP(˜f|˜e)foreachforeignphrase˜fandEnglishphrase˜earecalculatedonthebasisofcountsgleanedfromabitext.Sincethecount-ingprocessismuchsimplerthantryingtolearnthephraseswithEM,themaximumphraselengthcanbemadearbitrarilylongwithminimaljumpsincom-plexity.Thisallowsthemodeltoactuallyencodecontextualinformationintothetranslationmodelin-steadofleavingitcompletelytothelanguagemodel.Therearenonull(ǫ)phrasessothemodeldoesnothandleinsertionsordeletionsexplicitly.Theycanbehandledimplicitly,however,byincludinginsertedordeletedwordsasmembersofalargerphrase.DecodinginthemonotonesearchalgorithmisperformedwithaViterbidynamicprogrammingap-proach.ForaforeignsentenceoflengthJandaphraselengthmaximumofM,atableisﬁlledwitharowjforeachpositionintheinputforeignsentence,representingatranslationsequenceendingatthatforeignword,andeachcolumnerepresentspossi-bleﬁnalEnglishwordsforthattranslationsequence.EachentryinthetableQisﬁlledaccordingtothefollowingrecursion:Q(0,$)=1Q(j,e)=maxe′,˜e,˜fP(˜f|˜e)P(˜e|e′)Q(j′,e′)Q(J+1,$)=maxe′Q(J,e′)P($|e′)where˜fisaforeignphrasebeginningatj′+1,end-ingatjandconsistingofuptoMwords.The‘$’symbolisthesentenceboundarymarker.947

LetterTransducerViterbiSubstringSubstringTransducerModelTypeTransducerDynamicProgrammingTransducerTransliterationModelLetterSubstringSubstringLanguageModelWord/LetterSubstring/LetterWord/LetterNullSymbolsYesNoNoAlignmentsAllMostProbableMostProbableTable1:Comparisonofstatisticaltransliterationmodels.Intheaboverecursion,thelanguagemodelisrepresentedasP(˜e|e′),theprobabilityoftheEn-glishphrasegiventhepreviousEnglishword.Be-causeofdatasparsenessissuesinthecontextofwordphrases,theactualimplementationapproxi-matesthisprobabilityusingwordn-grams.4.2ViterbiSubstringDecoderWeproposetoadaptthemonotonesearchalgorithmtothedomainoftransliterationbysubstitutinglet-tersandsubstringsforthewordsandphrasesoftheoriginalmodel.Thereare,infact,strongindica-tionsthatthemonotonesearchalgorithmisbettersuitedtotransliterationthanitistotranslation.Un-likemachinetranslation,wheretheconstraintonre-orderingrequiredbymonotonesearchisfrequentlyviolated,transliterationisaninherentlysequentialprocess.Also,thesparsityissueintrainingthelan-guagemodelismuchlesspronounced,allowingustomodelP(˜e|e′)directly.Inordertotrainthemodel,weextracttheone-to-oneViterbialignmentofatrainingpairfromastochastictransducerbasedonthemodeloutlinedin(RistadandYianilos,1998).Substringsarethengeneratedbyiterativelyappendingadjacentlinksorunlinkedletterstotheone-to-onelinksofthealign-ment.Forexample,assumingamaximumsubstringlengthof2,the<r,>linkinthealignmentpre-sentedinFigure1wouldparticipateinthefollowingsubstringpairs:<r,>,<ur,>,and<ra,>.ThefactthattheViterbisubstringdecoderem-ploysadynamicprogrammingsearchthroughthesource/targetletterstatespacedescribedinSection1renderstheuseofawordunigramlanguagemodelimpossible.Thisisduetothefactthatalternatepathstoagivensource/targetletterpairarebeingeliminatedasthesearchproceeds.Forexample,supposetheViterbisubstringdecoderweregiventheFigure1:Aone-to-onealignmentofMouradandX	Ó.ForclaritytheArabicnameiswrittenlefttoright.ArabicstringÕç(cid:10)	,andtherearetwovalidEnglishnamesinthelanguagemodel,Karim(thecorrecttransliterationoftheinput)andKristine(theArabictransliterationofwhichwouldbe		(cid:30)(cid:10)(cid:16)ƒ	).Theop-timalpathuptothesecondlettermightgothrough<,k>,<,r>.Atthispoint,itistransliteratingintothenameKristine,butassoonasithitsthethirdlet-ter(ø(cid:10)),itisclearthatthisistheincorrectchoice.Inordertorecoverfromtheerror,thesearchwouldhavetobacktracktothebeginningandreturntostate<,r>fromadifferentpath,butthisisanimpos-sibilitysinceallotherpathstothatstatehavebeeneliminatedfromthesearch.4.3Substring-basedTransducerThemajoradvantagetheletter-basedtransducerpre-sentedinSection3hasovertheViterbisubstringde-coderisitswordunigramlanguagemodel,whichallowsittoreproducewordsseeninthetrainingdatawithhighaccuracy.Ontheotherhand,theViterbisubstringdecoderisabletoencodecon-textualinformationinthetransliterationmodelbe-causeofitsabilitytoconsiderlargermany-to-manymappings.Inanovelapproachpresentedhere,weproposeasubstring-basedtransducerthatdrawsonbothadvantages.ThesubstringtransliterationmodellearnedfortheViterbisubstringdecoderisencodedasatransducer,thusallowingittouseaworduni-948

gramlanguagemodel.Ourmodel,whichwerefertoasthesubstring-basedtransducer,hasseveralad-vantagesoverthepreviouslypresentedmodels.•Thesubstring-basedtransducercanbecom-posedwithawordunigramlanguagemodel,al-lowingittotransliteratenamesseenintrainingforthelanguagemodelwithgreateraccuracy.•Longermany-to-manymappingsenablethetransducertoencodecontextualinformationintothetransliterationmodel.Comparedtotheletter-basedtransducer,itallowsforthegener-ationoflongerwell-formedsubstrings(orpo-tentiallyevenentirewords).•Theletter-basedtransducerconsidersallpossi-blealignmentsofthetrainingexamples,mean-ingthatmanylow-probabilitymappingsareen-codedintothemodel.Thisissueisevenmorepronouncedincaseswherethedesiredtranslit-erationisnotinthewordunigrammodel,anditisguidedbytheweakerlettertrigrammodel.Thesubstring-basedtransducercaneliminatemanyoftheselow-probabilitymappingsbe-causeofitscommitmenttoasinglehigh-probabilityone-to-onealignmentduringtrain-ing.•Amajorcomputationaladvantagethismodelhasovertheletter-basedtransduceristhefactthatnullcharacters(ǫ)arenotencodedexplic-itly.SincetheArabicinputtotheletter-basedtransducercouldcontainanarbitrarynumberofnulls,thepotentialnumberofoutputstringsfromthetransliterationtransducerisinﬁnite.Thus,thecompositionwiththelanguagetrans-ducermustbedoneinsuchawaythatthereisavalidpathforallofthestringsoutputbythetransliterationtransducerthathaveapos-itiveprobabilityinthelanguagemodel.Thisleadstoprohibitivelylargetransducers.Ontheotherhand,thesubstring-basedtransducerhan-dlesnullsimplicitly(e.g.themappingke:im-plicitlyrepresentse:ǫafterak),sothetrans-duceritselfisnotrequiredtodealwiththem.5ExperimentsInthissection,wedescribetheevaluationofourmodelsonthetaskofArabic-to-Englishtransliter-ation.5.1DataForourexperiments,werequiredbilingualnamepairsfortestinganddevelopmentdata,aswellasforthetrainingofthetransliterationmodels.Totrainthelanguagemodels,wesimplyneededalistofEn-glishnames.BilingualdatawasextractedfromtheArabic-EnglishParallelNewspart1(approx.2.5Mwords)andtheArabicTreebankPart1-10kwordEnglishTranslation.BothbitextscontainArabicnewsarticlesandtheirEnglishtranslations.TheEn-glishnamelistforthelanguagemodeltrainingwasextractedfromtheEnglish-ArabicTreebankv1.0(approx.52kwords)1.ThelanguagemodeltrainingsetconsistedofallwordslabeledaspropernamesinthiscorpusalongwithalltheEnglishnamesinthetransliterationtrainingset.Anynamesinanyofthedatasetsthatconsistedofmultiplewords(e.g.ﬁrstname/lastnamepairs)weresplitandconsid-eredindividually.Trainingdataforthetranslitera-tionmodelconsistedof2844English-Arabicpairs.Thelanguagemodelwastrainedonaseparatesetof10991(4494unique)Englishnames.Theﬁnaltestsetof300English-Arabictransliterationpairscontainednooverlapwiththesetthatwasusedtoinducethetransliterationmodels.5.2EvaluationMethodologyForeachofthe300transliterationpairsinthetestset,thenamewritteninArabicservedasinputtothemodels,whileitsEnglishcounterpartwasconsid-eredagoldstandardtransliterationforthepurposeofevaluation.Twoseparatetestswereperformedonthetestset.Intheﬁrst,the300Englishwordsinthetestsetwereaddedtothetrainingdataforthelanguagemodels(theseentest),whileinthesec-ond,allEnglishwordsinthetestsetwereremovedfromthelanguagemodel’strainingdata(theunseentest).Bothtestswererunonthesamesetofwordstoensurethatvariationsinperformanceforseenandunseenwordsweresolelyduetowhetherornottheyappearinthelanguagemodel(andnot,forexam-ple,theirlanguageoforigin).Theseentestissim-ilartotestsrunin(KnightandGraehl,1998)and(StallsandKnight,1998)wherethemodelscouldnotproduceanywordsnotincludedinthelanguage1AllcorporaaredistributedbytheLinguisticDataConsor-tium.Despitethename,theEnglish-ArabicTreebankv1.0con-tainsonlyEnglishdata.949

modeltrainingdata.Themodelswereevaluatedontheseentestsetintermsofexactmatchestothegoldstandard.Becausethetaskofgeneratingtransliter-ationsfortheunseentestsetismuchmoredifﬁcult,exactmatchaccuracywillnotprovideameaningfulmetricforcomparison.Thus,asoftermeasureofperformancewasrequiredtoindicatehowclosethegeneratedtransliterationsaretothegoldstandard.WeusedLevenshteindistance:thenumberofinser-tions,deletionsandsubstitutionsrequiredtoconvertonestringintoanother.Wepresenttheresultssep-aratelyfornamesofArabicoriginandforthoseofnon-Arabicorigin.Wealsoperformedathirdtestonwordsthatap-pearinboththetransliterationandlanguagemodeltrainingdata.Thistestwasnotindicativeoftheoverallstrengthofthemodelsbutwasmeanttogiveasenseofhowmucheachmodeldependsonitslan-guagemodelversusitstransliterationmodel.5.3SetupFiveapproacheswereevaluatedontheArabic-Englishtransliterationtask.•Baseline:Asabaselineforourexperiments,weusedasimpledeterministicmappingalgo-rithmwhichmapsArabicletterstothemostlikelyletterorsequenceoflettersinEnglish.•Letter-basedTransducer:Mappingproba-bilitieswerelearnedbyrunningtheforward-backwardalgorithmuntilconvergence.Thelanguagemodelisacombinationofwordun-igramandlettertrigrammodelsandselectsawordunigramorlettertrigrammodelingoftheEnglishworddependingonwhicheveroneas-signsthehighestprobability.Theletter-basedtransducerwasimplementedinCarmel2.•ViterbiSubstringDecoder:Weexperimentedwithmaximumsubstringlengthsbetween3and10onthedevelopmentset,andfoundthatamaximumlengthof6wasoptimal.•Substring-basedTransducer:Thesubstring-basedtransducerwasalsoimplementedinCarmel.Wefoundthatthismodelworkedbestwithamaximumsubstringlengthof4.2Carmelisaﬁnite-statetransducerpackagewrittenbyJonathanGraehl.Itisavailableathttp://www.isi.edu/licensed-sw/carmel/.MethodArabicNon-ArabicAllBaseline1.92.12.0Lettertrans.45.964.354.7Viterbisubstring15.930.122.7Substringtrans.59.981.170.0Human33.140.636.7Table2:Exactmatchaccuracypercentageontheseentestsetforvariousmethods.MethodArabicNon-ArabicAllBaseline2.322.802.55Lettertrans.2.462.632.54Viterbisubstring1.902.132.01Substringtrans.1.922.412.16Human1.241.421.33Table3:AverageLevenshteindistanceontheun-seentestsetforvariousmethods.•Human:Forthepurposeofcomparison,weallowedanindependenthumansubject(ﬂuentinArabic,butanativespeakerofEnglish)toperformthesametask.ThesubjectwasaskedtotransliteratetheArabicwordsinthetestsetwithoutanyadditionalcontext.Noadditionalresourcesorcollaborationwereallowed.5.4ResultsontheTestSetTable2presentsthewordaccuracyperformanceofeachtransliteratorwhenthetestsetisavailabletothelanguagemodels.Table3showstheaverageLeven-shteindistanceresultswhenthetestsetisunavail-abletothelanguagemodels.Exactmatchperfor-mancebytheautomatedapproachesontheunseensetdidnotexceed10.3%(achievedbytheViterbisubstringdecoder).Resultsontheseentestsug-gestthatnon-Arabicwords(backtransliterations)areeasiertotransliterateexactly,whileresultsfortheunseentestsuggestthaterrorsonArabicwords(forwardtransliterations)tendtobeclosertothegoldstandard.Overall,oursubstring-basedtransducerclearlyoutperformstheletter-basedtransducer.Itsper-formanceisbetterinbothtests,butitsadvantageisparticularlypronouncedonwordsithasseeninthetrainingdataforthelanguagemodel(thetask950

ArabicLBTSBTCorrect1	AÒ(cid:17)«UthmanUthmanOthman2	¬	å(cid:17)…AsharfAsharfAshraf3(cid:16)ª	¯RafeetArafatRefaat4(cid:16)éÓAƒIstamadayAsumaUsama5	AÖ(cid:10)ErdmanAlimanIman6(cid:17)(cid:129)(cid:16)(cid:29)ððWortchWatchWatch7		Ê(cid:10)ÓMellisMillsMills8ø(cid:10)	(cid:30)(cid:10)	¯FebruaryFirariFerrariTable4:Asampleoftheerrorsmadebytheletter-based(LBT)andsegment-based(SBT)transducers.forwhichtheletter-basedtransducerwasoriginallydesigned).Sincebothtransducersuseexactlythesamelanguagemodel,thefactthatthesubstring-basedtransduceroutperformstheletter-basedtrans-ducerindicatesthatitlearnsastrongertranslitera-tionmodel.TheViterbisubstringdecoderseemstostrugglewhenitcomestorecreatingwordsseenthelanguagetrainingdata,asevidencedbyitsweakperformanceontheseentest.Obviously,itssubstring/letterbi-gramlanguagemodelisnomatchforthewordun-igrammodelusedbythetransducersonthistask.Ontheotherhand,itsstrongerperformanceontheunseentestsetsuggeststhatitslanguagemodelisstrongerthanthelettertrigramusedbythetransduc-erswhenitcomestogeneratingcompletelynovelwords.Asampleoftheerrorsmadebytheletter-andsubstring-basedtransducersispresentedinTable4.Ingeneral,whenbothmodelserr,thesubstring-basedtransducertendstowardmorephoneticallyreasonablechoices.Themostcommontypeofer-rorissimplycorrectalternateEnglishspellingsofanArabicname(error1).Error2isanexampleofalearnedmappingbeingmisplaced(thedeleteda).Error3indicatesthattheletter-basedtransducerisabletoavoidthesemisplacedmappingsatthebe-ginningorendofawordbecauseofitsthree-statetransliterationtransducer(i.e.itlearnsnottoallowvoweldeletionsatthebeginningofaword).Errors4and5arecaseswheretheletter-basedtransducerproducedparticularlyawkwardtransliterations.Er-rors6and7arenamesthatactuallyappearinthewordunigrammodelbutweremissedbytheletter-basedtransducer,whileerror8isanexampleoftheMethodExactmatchAvgLev.Lettertransducer81.20.46Viterbisubstring83.20.24Substringtransducer94.40.09Table5:Resultsfortestingonthetransliterationtrainingset.letter-basedtransducerincorrectlychoosinganamefromthewordunigrammodel.AsdiscussedinSec-tion4.3,thisislikelyduetomappingslearnedfromlow-probabilityalignments.5.5ResultsontheTrainingSetThesubstring-basedapproachesencodeagreatdealofcontextualinformationintothetransliterationmodel.Inordertoassesshowmuchtheperfor-manceofeachapproachdependsonitslanguagemodelversusitstransliterationmodel,wetestedthethreestatisticalmodelsonthesetof2844namesseeninboththetransliterationandlanguagemodeltraining.Theresultsofthisexperimentarepre-sentedinTable5.TheViterbisubstringdecoderre-ceivesthebiggestboost,outperformingtheletter-basedtransducer,whichindicatesthatitsstrengthliesmainlyinitstransliterationmodelingasopposedtoitslanguagemodeling.Thesubstring-basedtrans-ducer,however,stilloutperformsitbyalargemar-gin,achievingnear-perfectresults.Mostofthere-mainingerrorscanbeattributedtonameswithalter-natecorrectspellingsinEnglish.Theresultsalsosuggestthatthesubstring-basedtransducerpracticallysubsumesanaive“lookupta-ble”approach.Althoughtheaccuracyachievedislessthan100%,thesubstring-basedtransducerhasthegreatadvantageofbeingabletohandlenoiseintheinput.Inotherwords,ifthespellingofaninputworddoesnotmatchanArabicwordfromthetrain-ingdata,alookuptablewillgeneratenothing,whilethesubstring-basedtransducercouldstillsearchforthecorrecttransliteration.5.6ComputationalConsiderationsAnotherpointofcomparisonbetweenthemodelsiscomplexity.Theletter-basedtransducerencodes56144mappingswhilethesubstring-basedtrans-ducerencodes13948,butasshowninTable6,once951

MethodSize(states/arcs)Lettertransducer86309/547184Substringtransducer759/2131Table6:Transducersizesforcompositionwiththewordù(cid:10)ÒÊg(Helmy).MethodTimeLettertransducer5h52minViterbisubstring3secSubstringtransducer11secTable7:Runningtimesforthe300wordtestset.thetransducersarefullycomposed,thedifferencebecomesevenmorepronounced.AsdiscussedinSection4.3,thereasonforthesizeexplosionfac-torintheletter-basedtransduceristhepossibilityofnullcharactersintheinputword.Therunningtimesforthestatisticalapproachesonthe300wordtestsetarepresentedinTable7.Thehugecomputationaladvantageofthesubstring-basedapproachmakesitamuchmoreattractiveop-tionforanyreal-worldapplication.Testswereper-formedonanAMDAthlon643500+machinewith2GBofmemoryrunningRedHatEnterpriseLinuxrelease4.6ConclusionInthispaper,wepresentedanewsubstring-basedapproachtomodelingtransliterationinspiredbyphrase-basedmodelsofmachinetranslation.Wetestedbothdynamicprogrammingandﬁnite-statetransducerimplementations,thelatterofwhichen-abledustouseawordunigramlanguagemodeltoimprovetheaccuracyofgeneratedtransliterations.TheresultsofevaluationonthetaskofArabic-Englishtransliterationindicatethatthesubstring-basedapproachnotonlyimprovesperformanceoverastate-of-the-artletter-basedmodel,butalsoleadstomajorgainsinefﬁciency.Sincenolanguage-speciﬁcinformationwasencodeddirectlyintothemodels,theycanalsobeusedfortransliterationbe-tweenotherlanguagepairs.Inthefuture,weplantoconsidermorecom-plexlanguagemodelsinordertoimprovethere-sultsonunseenwords,whichshouldcertainlybefeasibleforthesubstring-basedtransducerbecauseofitsefﬁcientmemoryusage.Anotherfeatureofthesubstring-basedtransducerthatwehavenotyetex-ploredisitsabilitytoeasilyproduceann-bestlistoftransliterations.Weplantoinvestigatewhetherus-ingmethodslikediscriminativereranking(OchandNey,2002)onsuchann-bestlistcouldimproveper-formance.AcknowledgmentsWewouldliketothankColinCherryandtheothermembersoftheNLPresearchgroupattheUniver-sityofAlbertafortheirhelpfulcomments.Thisre-searchwassupportedbytheNaturalSciencesandEngineeringResearchCouncilofCanada.ReferencesN.AbdulJaleelandL.S.Larkey.2003.StatisticaltransliterationforEnglish-Arabiccrosslanguagein-formationretrieval.InCIKM,pages139–146.Y.Al-OnaizanandK.Knight.2002.Machinetranslit-erationofnamesinArabictext.InACLWorkshoponComp.ApproachestoSemiticLanguages.M.Arababi,S.M.Fischthal,V.C.Cheng,andE.Bart.1994.AlgorithmnsforArabicnametransliteration.IBMJournalofResearchandDevelopment,38(2).A.Ekbal,S.K.Naskar,andS.Bandyopadhyay.2006.Amodiﬁedjointsource-channelmodelfortransliter-ation.InCOLING/ACLPosterSessions,pages191–198.K.KnightandJ.Graehl.1998.Machinetransliteration.ComputationalLinguistics,24(4):599–612.P.Koehn,F.J.Och,andD.Marcu.2003.Statisticalphrase-basedtranslation.InNAACL-HLT,pages48–54.H.Li,M.Zhang,andJ.Su.2004.Ajointsource-channelmodelformachinetransliteration.InACL,pages159–166.F.J.OchandH.Ney.2002.Discriminativetrainingandmaximumentropymodelsforstatisticalmachinetranslation.InACL,pages295–302.E.S.RistadandP.N.Yianilos.1998.Learningstring-editdistance.IEEETransactionsonPatternAnalysisandMachineIntelligence,20(5):522–532.B.StallsandK.Knight.1998.TranslatingnamesandtechnicaltermsinArabictext.InCOLING/ACLWork-shoponComp.ApproachestoSemiticLanguages.R.ZensandH.Ney.2004.Improvementsinphrase-basedstatisticalmachinetranslation.InHLT-NAACL,pages257–264.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 952–959,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

952

PipelineIterationKristyHollingsheadandBrianRoarkCenterforSpokenLanguageUnderstanding,OGISchoolofScience&EngineeringOregonHealth&ScienceUniversity,Beaverton,Oregon,97006USA{hollingk,roark}@cslu.ogi.eduAbstractThispaperpresentspipelineiteration,anap-proachthatusesoutputfromlaterstagesofapipelinetoconstrainearlierstagesofthesamepipeline.Wedemonstratesig-niﬁcantimprovementsinastate-of-the-artPCFGparsingpipelineusingbase-phraseconstraints,derivedeitherfromlaterstagesoftheparsingpipelineorfromaﬁnite-stateshallowparser.Thebestperformanceisachievedbyrerankingtheunionofun-constrainedparsesandrelativelyheavily-constrainedparses.1IntroductionA“pipeline”systemconsistsofasequenceofpro-cessingstagessuchthattheoutputfromonestageprovidestheinputtothenext.Eachstageinsuchapipelineidentiﬁesasubsetofthepossiblesolutions,andlaterstagesareconstrainedtoﬁndsolutionswithinthatsubset.Forexample,apart-of-speechtaggercouldconstraina“basephrase”chunker(Rat-naparkhi,1999),orthen-bestoutputofaparsercouldconstrainareranker(CharniakandJohnson,2005).Apipelineistypicallyusedtoreducesearchcomplexityforrichmodelsusedinlaterstages,usu-allyattheriskthatthebestsolutionsmaybeprunedinearlystages.Pipelinesystemsareubiquitousinnaturallan-guageprocessing,usednotonlyinparsing(Rat-naparkhi,1999;Charniak,2000),butalsomachinetranslation(OchandNey,2003)andspeechrecogni-tion(Fiscus,1997;Goeletal.,2000),amongothers.Despitethewidespreaduseofpipelines,theyhavebeenunderstudied,withverylittleworkongen-eraltechniquesfordesigningandimprovingpipelinesystems(althoughcf.Finkeletal.(2006)).Thispa-perpresentsonesuchgeneraltechnique,hereap-pliedtostochasticparsing,wherebyoutputfromlaterstagesofapipelineisusedtoconstrainearlierstagesofthesamepipeline.Toourknowledge,thisistheﬁrsttimesuchapipelinearchitecturehasbeenproposed.Itmayseemsurprisingthatlaterstagesofapipeline,alreadyconstrainedtobeconsistentwiththeoutputofearlierstages,canproﬁtablyinformtheearlierstagesinasecondpass.However,therichermodelsusedinlaterstagesofapipelinepro-videabetterdistributionoverthesubsetofpossiblesolutionsproducedbytheearlystages,effectivelyresolvingsomeoftheambiguitiesthataccountformuchoftheoriginalvariation.Ifanearlierstageisthenconstrainedinasecondpassnottovarywithre-specttotheseresolvedambiguities,itwillbeforcedtoﬁndothervariations,whichmayincludebetterso-lutionsthanwereoriginallyprovided.Togivearoughillustration,considertheVenndi-agraminFig.1(i).SetArepresentstheoriginalsub-setofpossiblesolutionspassedalongbytheearlierstage,andthedarkshadedregionrepresentshigh-probabilitysolutionsaccordingtolaterstages.Ifsomeconstraintsarethenextractedfromthesehigh-probabilitysolutions,deﬁningasubsetofsolutions(S)thatruleoutsomeofA,theearlystagewillbeforcedtoproduceadifferentset(B).Constraintsderivedfromlaterstagesofthepipelinefocusthesearchinanareabelievedtocontainhigh-qualitycandidates.Anotherscenarioistouseadifferentmodelal-togethertoconstrainthepipeline.Inthisscenario,(i)(ii)A(cid:13)B(cid:13)S(cid:13)A(cid:13)B(cid:13)S(cid:13)Figure1:TwoVenndiagrams,representing(i)constraintsderivedfromlaterstagesofaniteratedpipelinedsystem;and(ii)constraintsderivedfromadifferentmodel.953

representedinFig.1(ii),theothermodelconstrainstheearlystagetobeconsistentwithsomesubsetofsolutions(S),whichmaybelargelyorcompletelydisjointfromtheoriginalsetA.Again,adifferentset(B)results,whichmayincludebetterresultsthanA.WhereaswheniteratingweareguaranteedthatthenewsubsetSwilloverlapatleastpartiallywiththeoriginalsubsetA,thatisnotthecasewhenmakinguseofconstraintsfromaseparatelytrainedmodel.Inthispaper,weinvestigatepipelineiterationwithinthecontextoftheCharniakandJohnson(2005)parsingpipeline,byconstrainingparsestobeconsistentwithabase-phrasetree.Wederivethesebase-phraseconstraintsfromthreesources:thererankingstageoftheparsingpipeline;aﬁnite-stateshallowparser(Hollingsheadetal.,2005);andacombinationoftheoutputfromthesetwosources.Wecomparetherelativeperformanceofthesethreesourcesandﬁndthebestperformanceimprovementsusingconstraintsderivedfromaweightedcombina-tionofshallowparseroutputandrerankeroutput.TheCharniakparsingpipelinehasbeenexten-sivelystudiedoverthepastdecade,withanum-berofpapersfocusedonimprovingearlystagesofthepipeline(Charniaketal.,1998;CaraballoandCharniak,1998;BlahetaandCharniak,1999;HallandJohnson,2004;Charniaketal.,2006)aswellasmanyfocusedonoptimizingﬁnalparseaccuracy(Charniak,2000;CharniakandJohnson,2005;Mc-Closkyetal.,2006).Thisfocusonoptimizationhasmadesystemimprovementsverydifﬁculttoachieve;yetourrelativelysimplearchitectureyieldsstatisti-callysigniﬁcantimprovements,makingpipelineit-erationapromisingapproachforothertasks.2ApproachOurapproachusestheCharniakstate-of-the-artparsingpipeline.Thewell-knownCharniak(2000)coarse-to-ﬁneparserisatwo-stageparsingpipeline,inwhichtheﬁrststageusesavanillaPCFGtopop-ulateachartofparseconstituents.Thesecondstage,constrainedtoonlythoseitemsintheﬁrst-stagechart,usesareﬁnedgrammartogenerateann-bestlistofparsecandidates.CharniakandJohn-son(2005)extendedthispipelinewithadiscrimina-tivemaximumentropymodeltorerankthen-bestparsecandidates,derivingasigniﬁcantbeneﬁtfromtherichermodelemployedbythereranker.Forourexperiments,wemodiﬁedtheparser1to1ftp://ftp.cs.brown.edu/pub/nlparser/BaseShallowParserPhrasesPhrasesCharniakparser-best91.994.4reranker-best92.894.8Finite-stateshallowparser91.794.3Table1:F-scoresonWSJsection24ofoutputfromtwoparsersonthesimilartasksofbase-phraseparsingandshallow-phraseparsing.Forevaluation,baseandshallowphrasesareextractedfromtheCharniak/Johnsonfull-parseoutput.allowustooptionallyprovidebase-phrasetreestoconstraintheﬁrststageofparsing.2.1BasePhrasesFollowingRatnaparkhi(1999),wedeﬁneabasephraseasanyparsenodewithonlypreterminalchil-dren.UnliketheshallowphrasesdeﬁnedfortheCoNLL-2000SharedTask(TjongKimSangandBuchholz,2000),basephrasescorresponddirectlytoconstituentsthatappearinfullparses,andhencecanprovideastraightforwardconstraintonedgeswithinachartparser.Incontrast,shallowphrasescollapsecertainnon-constituents—suchasauxiliarychains—intoasinglephrase,andhencearenotdi-rectlyapplicableasconstraintsonachartparser.Wehavetwomethodsforderivingbase-phraseannotationsforastring.First,wetrainedaﬁnite-stateshallowparseronbasephrasesextractedfromthePennWallSt.Journal(WSJ)Treebank(Marcusetal.,1993).Thetreebanktreesarepre-processedidenticallytotheprocedurefortrainingtheCharniakparser,e.g.,emptynodesandfunctiontagsarere-moved.Theshallowparseristrainedusingtheper-ceptronalgorithm,withafeaturesetnearlyidenticaltothatfromShaandPereira(2003),andachievescomparableperformancetothatpaper.SeeHolling-sheadetal.(2005)formoredetails.Second,basephrasescanbeextractedfromthefull-parseoutputoftheCharniakandJohnson(2005)reranker,viaasimplescripttoextractnodeswithonlypreterminalchildren.Table1showsthesesystems’bracketingaccu-racyonboththebase-phraseandshallowparsingtasksforWSJsection24;eachsystemwastrainedonWSJsections02-21.Fromthistablewecanseethatbasephrasesaresubstantiallymoredifﬁcultthanshallowphrasestoannotate.Outputfromtheﬁnite-stateshallowparserisroughlyasaccurateasoutputextractedfromtheCharniakparser-besttrees,thoughafairamountbelowoutputextractedfromthereranker-besttrees.Inadditiontousingbasephraseconstraintsfromthesetwosourcesindependently,wealsolookedat954

combiningthepredictionsofbothtoobtainmorere-liableconstraints.Wenextpresentamethodofcom-biningoutputfrommultipleparsersbasedoncom-binedprecisionandrecalloptimization.2.2CombiningParsern-bestListsInordertoselecthigh-likelihoodconstraintsforthepipeline,wemaywanttoextractannotationswithhighlevelsofagreement(“consensushypotheses”)betweencandidates.Inaddition,wemaywanttofavorprecisionoverrecalltoavoiderroneouscon-straintswithinthepipelineasmuchaspossible.HerewediscusshowatechniquepresentedinGood-man’sthesis(1998)canbeappliedtodothis.Wewillﬁrstpresentthiswithinageneralchartparsingapproach,thenmovetohowweuseitforn-bestlists.LetTbethesetoftreesforaparticularinput,andletaparseT∈Tbeconsideredasasetoflabeledspans.Then,foralllabeledspansX∈T,wecancalculatetheposteriorprobabilityγ(X)asfollows:γ(X)=XT∈TP(T)JX∈TKPT0∈TP(T0)(1)whereJX∈TK=(cid:26)1ifX∈T0otherwise.Goodman(1996;1998)presentsamethodforus-ingtheposteriorprobabilityofconstituentstomaxi-mizetheexpectedlabeledrecallofbinarybranchingtrees,asfollows:bT=argmaxT∈TXX∈Tγ(X)(2)Essentially,ﬁndthetreewiththemaximumsumoftheposteriorprobabilitiesofitsconstituents.Thisisdonebycomputingtheposteriorprobabilitiesofconstituentsinachart,typicallyviatheInside-Outsidealgorithm(Baker,1979;LariandYoung,1990),followedbyaﬁnalCYK-likepasstoﬁndthetreemaximizingthesum.Fornon-binarybranchingtrees,whereprecisionandrecallmaydiffer,Goodman(1998,Ch.3)pro-posesthefollowingcombinedmetricforbalancingprecisionandrecall:bT=argmaxT∈TXX∈T(γ(X)−λ)(3)whereλrangesfrom0to1.Settingλ=0isequiv-alenttoEq.2andthusoptimizesrecall,andsettingλ=1optimizesprecision;Appendix5attheendofthispaperpresentsbriefderivationsofthesemet-rics.2Thus,λfunctionsasamixingfactortobalancerecallandprecision.Thisapproachalsogivesusastraightforwardwaytocombinen-bestoutputsofmultiplesystems.Todothis,weconstructachartoftheconstituentsinthetreesfromthen-bestlists,andallowanycombina-tionofconstituentsthatresultsinatree–evenonewithnointernalstructure.Insuchaway,wecanproducetreesthatonlyincludeasmallnumberofhigh-certaintyconstituents,andleavetheremainderofthestringunconstrained,evenifsuchtreeswerenotcandidatesintheoriginaln-bestlists.Forsimplicity,wewillherediscussthecombina-tionoftwon-bestlists,thoughitgeneralizesintheobviouswaytoanarbitrarynumberoflists.LetTbetheunionofthetwon-bestlists.ForalltreesT∈T,letP1(T)betheprobabilityofTintheﬁrstn-bestlist,andP2(T)theprobabilityofTinthesec-ondn-bestlist.Then,wedeﬁneP(T)asfollows:P(T)=αP1(T)PT0∈TP1(T0)+P2(T)PT0∈TP2(T0)(4)wheretheparameterαdictatestherelativeweightofP1versusP2inthecombination.3Forthispaper,wecombinedtwon-bestlistsofbase-phrasetrees.Althoughthereisnohierarchi-calstructureinbase-phraseannotations,theycanberepresentedasﬂattrees,asshowninFig.2(a).Weconstructedachartfromthetwolistsbeingcom-bined,usingEq.4todeﬁneP(T)inEq.1.Wewishtoconsidereverypossiblecombinationofthebasephrases,sofortheﬁnalCYK-likepasstoﬁndtheargmaxtree,weincludedrulesforattachingeachpreterminaldirectlytotherootofthetree,inaddi-tiontorulespermittinganycombinationofhypoth-esizedbasephrases.ConsiderthetreesinFig.2.Figure2(a)isashallowparsewiththreeNPbasephrases;Figure2(b)isthesameparsewheretheROOTproduc-tionhasbeenbinarizedfortheﬁnalCYK-likepass,whichrequiresbinaryproductions.Ifweincludeproductionsoftheform‘ROOT→XROOT’and‘ROOT→XY’forallnon-terminalsXandY(in-cludingPOStags),thenanytree-structuredcom-binationofbasephraseshypothesizedineithern-2OurnotationdiffersslightlyfromthatinGoodman(1998),thoughtheapproachesareformallyequivalent.3NotethatP1andP2arenormalizedineq.4,andthusarenotrequiredtobetrueprobabilities.Inturn,Pisnormalizedwhenusedineq.1,suchthattheposteriorprobabilityγisatrueprobability.HencePneednotbenormalizedineq.4.955

(a)ROOT   @@@PPPPPPPNPHHDTtheNNbrokerVBDsoldNPHHDTtheNNSstocksNPNNyesterday(b)ROOTHHHNPHHDTtheNNbrokerROOTHHVBDsoldROOTHHHNPHHDTtheNNSstocksNPNNyesterday(c)SHHHHNPHHDTtheNNbrokerVPHHHHHVBDsoldNPHHDTtheNNSstocksNPNNyesterdayFigure2:Base-phrasetrees(a)asproducedforann-bestlistand(b)afterroot-binarizationforn-bestlistcombination.Full-parsetree(c)consistentwithconstrainingbase-phrasetree(a).87888990919293949596978687888990919293949596precisionrecallCharniak − reranked (solid viterbi)Finite−state shallow parser (solid viterbi)Charniak reranked + Finite−stateFigure3:Thetradeoffbetweenrecallandprecisionusingarangeofλvalues(Eq.3)toselecthigh-probabilityannotationsfromann-bestlist.Resultsareshownon50-bestlistsofbase-phraseparsesfromtwoparsers,andonthecombinationofthetwolists.bestlistisallowed,includingtheonewithnobasephrasesatall.Notethat,forthepurposeofﬁndingtheargmaxtreeinEq.3,weonlysumtheposteriorprobabilitiesofbase-phraseconstituents,andnottheROOTsymbolorPOStags.Figure3showstheresultsofperformingthiscom-binedprecision/recalloptimizationonthreeseparaten-bestlists:the50-bestlistofbase-phrasetreesex-tractedfromthefull-parseoutputoftheCharniakandJohnson(2005)reranker;the50-bestlistoutputbytheHollingsheadetal.(2005)ﬁnite-stateshallowparser;andtheweightedcombinationofthetwolistsatvariousvaluesofλinEq.3.Forthecombination,wesetα=2inEq.4,withtheCharniakandJohnson(2005)rerankerprovidingP1,effectivelygivingthererankertwicetheweightoftheshallowparserindeterminingtheposteriors.Theshallowparserhasperceptronscoresasweights,andthedistributionofthesescoresafterasoftmaxnormalizationwastoopeakedtobeofutility,soweusedthenormalizedreciprocalrankofeachcandidateasP2inEq.4.Wepointoutseveraldetailsintheseresults.First,usingthismethoddoesnotresultinanF-measureimprovementovertheViterbi-bestbase-phraseparses(shownassolidsymbolsinthegraph)foreitherthererankerorshallowparser.Also,us-ingthismodeleffectsagreaterimprovementinpre-cisionthaninrecall,whichisunsurprisingwiththesenon-hierarchicalannotations;unlikefullpars-ing(wherelongsequencesofunaryproductionscanimproverecallarbitrarily),inbase-phraseparsing,anygivenspancanhaveonlyonenon-terminal.Fi-nally,weseethatthecombinationofthetwon-bestlistsimprovesovereitherlistinisolation.3ExperimentalSetupForourexperimentsweconstructedasimpleparsingpipeline,showninFig.4.AtthecoreofthepipelineistheCharniakandJohnson(2005)coarse-to-ﬁneparserandMaxEntreranker,describedinSec.2.Theparserconstitutestheﬁrstandsecondstagesofourpipeline,andthererankertheﬁnalstage.Fol-lowingCharniakandJohnson(2005),wesettheparsertooutput50-bestparsesforallexperimentsdescribedhere.Weconstrainonlytheﬁrststageoftheparser:duringchartconstruction,wedisallowanyconstituentsthatconﬂictwiththeconstraints,asdescribedindetailinthenextsection.3.1ParserConstraintsWeusebasephrases,asdeﬁnedinSec.2.1,tocon-straintheﬁrststageofourparsingpipeline.Undertheseconstraints,fullparsesmustbeconsistentwiththebase-phrasetreeprovidedasinputtotheparser,i.e.,anyvalidparsemustcontainallofthebase-phraseconstituentsintheconstrainingtree.Thefull-parsetreeinFig.2(c),forexample,isconsis-tentwiththebase-phrasetreeinFig.2(a).Implementingtheseconstraintsinaparserisstraightforward,oneoftheadvantagesofusingbasephrasesasconstraints.Sincetheinternalstructureofbasephrasesis,bydeﬁnition,limitedtopreter-minalchildren,wecanconstraintheentireparsebyconstrainingtheparentsoftheappropriatepretermi-nalnodes.Foranypreterminalthatoccurswithinthespanofaconstrainingbasephrase,theonlyvalidparentisanodematchingboththespan(startandendpoints)andthelabeloftheprovidedbase956

A3(cid:13)Shallow(cid:13)Parser(cid:13)Coarse(cid:13)Parser(cid:13)Fine(cid:13)Parser(cid:13)Reranker(cid:13)D(cid:13)C(cid:13)B(cid:13)extracted(cid:13)base phrases(cid:13)A1(cid:13)A2(cid:13)+(cid:13)Figure4:Theiteratedparsingpipeline.Intheﬁrstiteration,thecoarseparsermaybeeitherunconstrained,orconstrainedbybasephrasesfromtheshallowparser(A1).Intheseconditeration,basephraseconstraintsmaybeextractedeitherfromrerankeroutput(A2)orfromaweightedcombinationofshal-lowparseroutputandrerankeroutput(A3).Multiplesetsofn-bestparses,asoutputbythecoarse-to-ﬁneparserunderdif-ferentconstraintconditions,maybejoinedinasetunion(C).phrase.Allotherproposedparent-nodesarere-jected.Insuchaway,foranyparsetocovertheentirestring,itwouldhavetobeconsistentwiththeconstrainingbase-phrasetree.Wordsthatfalloutsideofanybase-phrasecon-straintareunconstrainedinhowtheyattachwithintheparse;hence,abase-phrasetreewithfewwordscoveredbybase-phraseconstraintswillresultinalargersearchspacethanonewithmanywordscov-eredbybasephrases.Wealsoputnorestrictionsonthepreterminallabels,evenwithinthebasephrases.Wenormalizedforpunctuation.Iftheparserfailstoﬁndavalidparsewiththeconstraints,thenwelifttheconstraintsandallowanyparseconstituentorig-inallyproposedbytheﬁrst-stageparser.3.2ExperimentalConditionsOurexperimentswilldemonstratetheeffectsofcon-strainingtheCharniakparserunderseveraldiffer-entconditions.Thebaselinesystemplacesnocon-straintsontheparser.Theremainingexperimen-talconditionseachconsideroneofthreepossiblesourcesofthebasephraseconstraints:(1)thebasephrasesoutputbytheﬁnite-stateshallowparser;(2)thebasephrasesextractedfromoutputofthereranker;and(3)acombinationoftheoutputfromtheshallowparserandthereranker,whichispro-ducedusingthetechniquesoutlinedinSec.2.2.ConstraintsareenforcedasdescribedinSec.3.1.UnconstrainedForourbaselinesystem,weruntheCharniakandJohnson(2005)parserandrerankerwithdefaultparameters.Theparserispro-videdwithtreebank-tokenizedtextand,asmen-tionedpreviously,outputs50-bestparsecandidatestothereranker.FS-constrainedTheFS-constrainedconditionprovidesacomparisonpointofnon-iteratedcon-straints.Underthiscondition,theone-bestbase-SystemLRLPFFinite-stateshallowparser91.392.091.7Charniakreranker-best92.293.392.8Combination(λ=0.5)92.294.193.2Combination(λ=0.9)81.097.488.4Table2:Labeledrecall(LR),precision(LP),andF-scoresonWSJsection24ofbase-phrasetreesproducedbythethreepossiblesourcesofconstraints.phrasetreeoutputbytheﬁnite-stateshallowparserisinputasaconstrainttotheCharniakparser.Weruntheparserandrerankerasbefore,undercon-straintsfromtheshallowparser.TheaccuracyoftheconstraintsusedunderthisconditionisshownintheﬁrstrowofTable2.Notethatthisconditionisnotaninstanceofpipelineiteration,butisincludedtoshowtheperformancelevelsthatcanbeachievedwithoutiteration.Reranker-constrainedWewillusethereranker-constrainedconditiontoexaminetheef-fectsofpipelineiteration,withnoinputfromothermodelsoutsidethepipeline.Wetakethereranker-bestfull-parseoutputundertheconditionofuncon-strainedsearch,andextractthecorrespondingbase-phrasetree.Weruntheparserandrerankerasbe-fore,nowwithconstraintsfromthereranker.TheaccuracyoftheconstraintsusedunderthisconditionisshowninthesecondrowofTable2.Combo-constrainedThecombo-constrainedconditionsaredesignedtocomparetheeffectsofgeneratingconstraintswithdifferentcombinationparameterizations,i.e.,differentλparametersinEq.3.Forthisexperimentalcondition,weextractbase-phrasetreesfromthen-bestfull-parsetreesoutputbythereranker.Wecombinethislistwiththen-bestlistoutputbytheﬁnite-stateshallowparser,exactlyasdescribedinSec.2.2,againwiththererankerpro-vidingP1andα=2inEq.4.Weexaminedarangeofoperatingpointsfromλ=0.4toλ=0.9,andre-porttwopointshere(λ=0.5andλ=0.9),whichrep-resentthehighestoverallaccuracyandthehighestprecision,respectively,asshowninTable2.ConstrainedandUnconstrainedUnionWheniteratingthispipeline,theoriginaln-bestlistoffullparsesoutputfromtheunconstrainedparserisavail-ableatnoadditionalcost,andourﬁnalsetofex-perimentalconditionsinvestigatetakingtheunionofconstrainedandunconstrainedn-bestlists.Theimposedconstraintscanresultincandidatesetsthatarelargely(orcompletely)disjointfromtheuncon-strainedsets,anditmaybethattheunconstrainedsetisinmanycasessuperiortotheconstrainedset.957

ConstraintsParser-bestReranker-bestOracle-best#CandidatesBaseline(Unconstrained,50-best)88.9290.2495.9547.9FS-constrained88.4489.5094.1046.2Reranker-constrained89.6090.4695.0746.9Combo-constrained(λ=0.5)89.8190.7495.4146.3Combo-constrained(λ=0.9)89.3490.4395.9147.5Table3:Full-parseF-scoresonWSJsection24.Theunconstrainedsearch(ﬁrstrow)providesabaselinecomparisonfortheeffectsofconstrainingthesearchspace.Thelastfourrowsdemonstratetheeffectofvariousconstraintconditions.Evenourhigh-precisionconstraintsdidnotreach100%precision,attestingtothefactthattherewassomeerrorinallconstrainedconditions.Bycon-structingtheunionofthetwon-bestlists,wecantakeadvantageofthenewconstrainedcandidatesetwithoutrunningtheriskthattheconstraintshavere-sultedinaworsen-bestlist.Notethattheparserprobabilitiesareproducedfromthesamemodelinbothpasses,andarehencedirectlycomparable.Theoutputofthesecondpassofthepipelinecouldbeusedtoconstrainathirdpass,formultiplepipelineiterations.However,wefoundthatfurtheriterationsprovidednoadditionalimprovements.3.3DataUnlessstatedotherwise,allreportedresultswillbeF-scoresonWSJsection24ofthePennWSJTree-bank,whichwasourdevelopmentset.TrainingdatawasWSJsections02-21,withsection00asheld-outdata.Crossfoldvalidation(20-foldwith2,000sentencesperfold)wasusedtotrainthererankerforeverycondition.Evaluationwasperformedus-ingevalbunderstandardparameterizations.WSJsection23wasusedonlyforﬁnaltesting.4Results&DiscussionWeevaluatetheone-bestparsecandidatesbeforeandafterreranking(parser-bestandreranker-best,respectively).Weadditionallyprovidethebest-possibleF-scoreinthen-bestlist(oracle-best)andthenumberofuniquecandidatesinthelist.Table3presentstrialsshowingtheeffectofcon-strainingtheparserundervariousconditions.Con-strainingtheparsertothebasephrasesproducedbytheﬁnite-stateshallowparser(FS-constrained)hurtsperformancebyhalfapoint.Constrainingtheparsertothebasephrasesproducedbythereranker,however,providesa0.7percentimprovementintheparser-bestaccuracy,anda0.2percentimprovementafterreranking.Combiningthetwobase-phrasen-bestliststoderivetheconstraintsprovidesfurtherimprovementswhenλ=0.5,toatotalimprovementof0.9and0.5percentoverparser-bestandreranker-bestaccuracy,respectively.Performancedegradesatλ=0.9relativetoλ=0.5,indicatingthat,evenatalowerprecision,moreconstraintsarebeneﬁcial.Theoracleratedecreasesunderallofthecon-strainedconditionsascomparedtothebaseline,demonstratingthattheparserwaspreventedfromﬁndingsomeofthebestsolutionsthatwereorig-inallyfound.However,theimprovementinF-scoreshowsthattheconstraintsassistedtheparserinachievinghigh-qualitysolutionsdespitethisde-gradedoracleaccuracyofthelists.Table4showstheresultswhentakingtheunionoftheconstrainedandunconstrainedlistspriortoreranking.Severalinterestingpointscanbenotedinthistable.First,despitethefactthattheFS-constrainedconditionhurtsperformanceinTable3,theunionprovidesa0.5percentimprovementoverthebaselineintheparser-bestperformance.Thisindicatesthat,insomecases,theCharniakparserisscoringparsesintheconstrainedsethigherthanintheunconstrainedset,whichisevidenceofsearcherrorsintheunconstrainedcondition.OnecanseefromthenumberofcandidatesthattheFS-constrainedconditionprovidesthesetofcandidatesmostdisjointfromtheoriginalunconstrainedparser,leadingtothelargestnumberofcandidatesintheunion.Surprisingly,eventhoughthissetprovidedthehighestparser-bestF-scoreofalloftheunionsets,itdidnotleadtosigniﬁcantoverallimprove-mentsafterreranking.Inallotherconditions,takingtheunionde-creasestheparser-bestaccuracywhencomparedtothecorrespondingconstrainedoutput,butimprovesthereranker-bestaccuracyinallbutthecombo-constrainedλ=0.9condition.Oneexplanationforthelowerperformanceatλ=0.9versusλ=0.5isseeninthenumberofcandidates,about7.5fewerthanintheλ=0.5condition.Therearefewercon-straintsinthehigh-precisioncondition,sothere-sultingn-bestlistsdonotdivergeasmuchfromtheoriginallists,leadingtolessdiversityintheirunion.Thegainsinperformanceshouldnotbeattributedtoincreasingthenumberofcandidatesnortoallow-958

ConstraintsParser-bestReranker-bestOracle-best#CandidatesBaseline(Unconstrained,50-best)88.9290.2495.9547.9Unconstrained∪FS-constrained89.3990.2796.6174.9Unconstrained∪Reranker-constrained89.2390.5996.4870.3Unconstrained∪Combo(λ=0.5)89.2890.7896.5369.7Unconstrained∪Combo(λ=0.9)89.0390.4496.4062.1Unconstrained(100-best)88.8290.1396.3895.2Unconstrained(50-best,beam×2)89.0190.4596.1348.1Table4:Full-parseF-scoresonWSJsection24aftertakingthesetunionofunconstrainedandconstrainedparseroutputunderthe4differentconstraintconditions.Also,F-scorefor100-bestparses,and50-bestparseswithanincreasedbeamthreshold,outputbytheCharniakparserundertheunconstrainedcondition.ConstraintsF-scoreBaseline(Unconstrained,50-best)91.06Unconstrained∪Combo(λ=0.5)91.48Table5:Full-parseF-scoresonWSJsection23forourbest-performingsystemonWSJsection24.The0.4percentF-scoreimprovementissigniﬁcantatp<0.001.ingtheparsermoretimetogeneratetheparses.ThepenultimaterowinTable4showstheresultswith100-bestlistsoutputintheunconstrainedcondition,whichdoesnotimproveuponthe50-bestperfor-mance,despiteanimprovedoracleF-score.Sincetheseconditerationthroughtheparsingpipelineclearlyincreasestheoverallprocessingtimebyafactoroftwo,wealsocompareagainstoutputob-tainedbydoublingthecoarse-parser’sbeamthresh-old.ThelastrowinTable4showsthattheincreasedthresholdyieldsaninsigniﬁcantimprovementoverthebaseline,despiteaverylargeprocessingburden.Weappliedourbest-performingmodel(Uncon-strained∪Combo,λ=0.5)tothetestset,WSJsec-tion23,forcomparisonagainstthebaselinesystem.Table5showsa0.4percentF-scoreimprovementoverthebaselineforthatsection,whichisstatisti-callysigniﬁcantatp<0.001,usingthestratiﬁedshufﬂingtest(Yeh,2000).5Conclusion&FutureWorkInsummary,wehavedemonstratedthatpipelineit-erationcanbeusefulinimprovingsystemperfor-mance,byconstrainingearlystagesofthepipelinewithoutputderivedfromlaterstages.Whilethecurrentworkmadeuseofaparticularkindofconstraint—basephrases—manyotherscouldbeex-tractedaswell.Preliminaryresultsextendingtheworkpresentedinthispapershowparseraccuracyimprovementsfrompipelineiterationwhenusingconstraintsbasedonanunlabeledpartialbracketingofthestring.Higher-levelphrasesegmentationsorfullyspeciﬁedtreesoverportionsofthestringmightalsoprovetobeeffectiveconstraints.Thetech-niquesshownherearebynomeanslimitedtopars-ingpipelines,andcouldeasilybeappliedtoothertasksmakinguseofpipelinearchitectures.AcknowledgmentsThankstoMartinJanscheforusefuldiscussionsontopicsrelatedtothispaper.TheﬁrstauthorofthispaperwassupportedunderanNSFGraduateRe-searchFellowship.Inaddition,thisresearchwassupportedinpartbyNSFGrant#IIS-0447214.Anyopinions,ﬁndings,conclusionsorrecommendationsexpressedinthispublicationarethoseoftheauthorsanddonotnecessarilyreﬂecttheviewsoftheNSF.ReferencesJ.K.Baker.1979.Trainablegrammarsforspeechrecognition.InSpeechCommunicationpapersforthe97thMeetingoftheAcousticalSocietyofAmerica.D.BlahetaandE.Charniak.1999.Automaticcompensationforparserﬁgure-of-meritﬂaws.InProceedingsofthe37thAnnualMeetingofACL,pages513–518.S.CaraballoandE.Charniak.1998.Newﬁguresofmeritforbest-ﬁrstprobabilisticchartparsing.ComputationalLin-guistics,24(2):275–298.E.CharniakandM.Johnson.2005.Coarse-to-ﬁnen-bestpars-ingandMaxEntdiscriminativereranking.InProceedingsofthe43rdAnnualMeetingofACL,pages173–180.E.Charniak,S.Goldwater,andM.Johnson.1998.Edge-basedbest-ﬁrstchartparsing.InProceedingsofthe6thWorkshopforVeryLargeCorpora,pages127–133.E.Charniak,M.Johnson,M.Elsner,J.L.Austerweil,D.Ellis,S.R.Iyangar,J.Moore,M.T.Pozar,C.Hill,T.Q.Vu,andI.Haxton.2006.Multi-levelcourse-to-ﬁnePCFGparsing.InProceedingsoftheHLT-NAACLAnnualMeeting,pages168–175.E.Charniak.2000.AMaximum-Entropy-inspiredparser.InProceedingsofthe1stAnnualMeetingofNAACLand6thConferenceonANLP,pages132–139.J.R.Finkel,C.D.Manning,andA.Y.Ng.2006.Solvingtheproblemofcascadingerrors:ApproximateBayesianinfer-enceforlinguisticannotationpipelines.InProceedingsofEMNLP,pages618–626.J.Fiscus.1997.Apost-processingsystemtoyieldreducedworderrorrates:Recognizeroutputvotingerrorreduction(ROVER).InProceedingsoftheIEEEWorkshoponAuto-maticSpeechRecognitionandUnderstanding.V.Goel,S.Kumar,andW.Byrne.2000.SegmentalminimumBayes-riskASRvotingstrategies.InProceedingsofICSLP,pages139–142.959

J.Goodman.1996.Parsingalgorithmsandmetrics.InPro-ceedingsofthe34thAnnualMeetingofACL,pages177–183.J.Goodman.1998.Parsinginside-out.Ph.D.thesis,HarvardUniversity.K.HallandM.Johnson.2004.Attentionshiftingforparsingspeech.InProceedingsofthe42ndAnnualMeetingofACL,pages40–46.K.Hollingshead,S.Fisher,andB.Roark.2005.Comparingandcombiningﬁnite-stateandcontext-freeparsers.InPro-ceedingsofHLT-EMNLP,pages787–794.K.LariandS.J.Young.1990.Theestimationofstochasticcontext-freegrammarsusingtheinside-outsidealgorithm.ComputerSpeechandLanguage,4(1):35–56.M.P.Marcus,M.A.Marcinkiewicz,andB.Santorini.1993.BuildingalargeannotatedcorpusofEnglish:ThePenntree-bank.ComputationalLinguistics,19:314–330.D.McClosky,E.Charniak,andM.Johnson.2006.Rerankingandself-trainingforparseradaptation.InProceedingsofCOLING-ACL,pages337–344.F.J.OchandH.Ney.2003.Asystematiccomparisonofvariousstatisticalalignmentmodels.ComputationalLinguistics,29.A.Ratnaparkhi.1999.Learningtoparsenaturallanguagewithmaximumentropymodels.MachineLearning,34(1-3):151–175.F.ShaandF.Pereira.2003.Shallowparsingwithconditionalrandomﬁelds.InProceedingsoftheHLT-NAACLAnnualMeeting,pages134–141.E.F.TjongKimSangandS.Buchholz.2000.IntroductiontotheCoNLL-2000sharedtask:Chunking.InProceedingsofCoNLL,pages127–132.A.Yeh.2000.Moreaccuratetestsforthestatisticalsigniﬁ-canceofresultdifferences.InProceedingsofthe18thInter-nationalCOLING,pages947–953.AppendixACombinedPrecision/RecallDecodingRecallthatTisthesetoftreesforaparticularinput,andeachT∈Tisconsideredasasetoflabeledspans.ForalllabeledspansX∈T,wecancalcu-latetheposteriorprobabilityγ(X)asfollows:γ(X)=XT∈TP(T)JX∈TKPT0∈TP(T0)whereJX∈TK=(cid:26)1ifX∈T0otherwise.Ifτisthereferencetree,thelabeledprecision(LP)andlabeledrecall(LR)ofaTrelativetoτaredeﬁnedasLP=|T∩τ||T|LR=|T∩τ||τ|where|T|denotesthesizeofthesetT.AmetricveryclosetoLRis|T∩τ|,thenumberofnodesincommonbetweenthetreeandtheref-erencetree.Tomaximizetheexpectedvalue(E)ofthismetric,wewanttoﬁndthetreebTasfollows:bT=argmaxT∈TEh|T\τ|i=argmaxT∈TXT0∈TP(T0)(cid:2)|TTT0|(cid:3)PT00∈TP(T00)=argmaxT∈TXT0∈TP(T0)PX∈TJX∈T0KPT00∈TP(T00)=argmaxT∈TXX∈TXT0∈TP(T0)JX∈T0KPT00∈TP(T00)=argmaxT∈TXX∈Tγ(X)(5)ThisexactlymaximizestheexpectedLRinthecaseofbinarybranchingtrees,andiscloselyre-latedtoLRfornon-binarybranchingtrees.Simi-lartomaximizingtheexpectednumberofmatch-ingnodes,wecanminimizetheexpectednumberofnon-matchingnodes,forametricrelatedtoLP:bT=argminT∈TEh|T|−|T\τ|i=argmaxT∈TEh|T\τ|−|T|i=argmaxT∈TXT0∈TP(T0)(cid:2)|TTT0|−|T|(cid:3)PT00∈TP(T00)=argmaxT∈TXT0∈TP(T0)PX∈T(JX∈T0K−1)PT00∈TP(T00)=argmaxT∈TXX∈TXT0∈TP(T0)(JX∈T0K−1)PT00∈TP(T00)=argmaxT∈TXX∈T(γ(X)−1)(6)Finally,wecancombinethesetwometricsinalinearcombination.Letλbeamixingfactorfrom0to1.Thenwecanoptimizetheweightedsum:bT=argmaxT∈TEh(1−λ)|T\τ|+λ(|T\τ|−|T|)i=argmaxT∈T(1−λ)Eh|T\τ|i+λEh|T\τ|−|T|i=argmaxT∈T"(1−λ)XX∈Tγ(X)#+"λXX∈T(γ(X)−1)#=argmaxT∈TXX∈T(γ(X)−λ)(7)Theresultisacombinedmetricforbalancingpreci-sionandrecall.Notethat,ifλ=0,Eq.7isthesameasEq.5andthusmaximizestheLRmetric;andifλ=1,Eq.7isthesameasEq.6andthusmaximizestheLPmetric.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 960–967,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

960

LearningSynchronousGrammarsforSemanticParsingwithLambdaCalculusYukWahWongandRaymondJ.MooneyDepartmentofComputerSciencesTheUniversityofTexasatAustin{ywwong,mooney}@cs.utexas.eduAbstractThispaperpresentstheﬁrstempiricalresultstoourknowledgeonlearningsynchronousgrammarsthatgeneratelogicalforms.Usingstatisticalmachinetranslationtechniques,asemanticparserbasedonasynchronouscontext-freegrammaraugmentedwithλ-operatorsislearnedgivenasetoftrainingsentencesandtheircorrectlogicalforms.Theresultingparserisshowntobethebest-performingsystemsofarinadatabasequerydomain.1IntroductionOriginallydevelopedasatheoryofcompilingpro-gramminglanguages(AhoandUllman,1972),syn-chronousgrammarshaveseenasurgeofinterestre-centlyinthestatisticalmachinetranslation(SMT)communityasawayofformalizingsyntax-basedtranslationmodelsbetweennaturallanguages(NL).Ingeneratingmultipleparsetreesinasinglederiva-tion,synchronousgrammarsareidealformodel-ingsyntax-basedtranslationbecausetheydescribenotonlythehierarchicalstructuresofasentenceanditstranslation,butalsotheexactcorrespon-dencebetweentheirsub-parts.Amongthegram-marformalismssuccessfullyputintouseinsyntax-basedSMTaresynchronouscontext-freegram-mars(SCFG)(Wu,1997)andsynchronoustree-substitutiongrammars(STSG)(YamadaandKnight,2001).BothformalismshaveledtoSMTsys-temswhoseperformanceisstate-of-the-art(Chiang,2005;Galleyetal.,2006).SynchronousgrammarshavealsobeenusedinotherNLPtasks,mostnotablysemanticparsing,whichistheconstructionofacomplete,formalmeaningrepresentation(MR)ofanNLsentence.Inourpreviouswork(WongandMooney,2006),se-manticparsingiscastasamachinetranslationtask,whereanSCFGisusedtomodelthetranslationofanNLintoaformalmeaning-representationlan-guage(MRL).Ouralgorithm,WASP,usesstatisticalmodelsdevelopedforsyntax-basedSMTforlexicallearningandparsedisambiguation.Theresultisarobustsemanticparserthatgivesgoodperformanceinvariousdomains.Morerecently,weshowthatourSCFG-basedparsercanbeinvertedtoproduceastate-of-the-artNLgenerator,whereaformalMRListranslatedintoanNL(WongandMooney,2007).Currently,theuseoflearnedsynchronousgram-marsinsemanticparsingandNLgenerationislim-itedtosimpleMRLsthatarefreeoflogicalvari-ables.ThisisbecausegrammarformalismssuchasSCFGdonothaveaprincipledmechanismforhan-dlinglogicalvariables.Thisisunfortunatebecausemostexistingworkoncomputationalsemanticsisbasedonpredicatelogic,wherelogicalvariablesplayanimportantrole(BlackburnandBos,2005).Forsomedomains,thisproblemcanbeavoidedbytransformingalogicallanguageintoavariable-free,functionallanguage(e.g.theGEOQUERYfunctionalquerylanguageinWongandMooney(2006)).How-ever,developmentofsuchafunctionallanguageisnon-trivial,andaswewillsee,logicallanguagescanbemoreappropriateforcertaindomains.Ontheotherhand,mostexistingmethodsformappingNLsentencestologicalformsinvolvesub-stantialhand-writtencomponentsthataredifﬁculttomaintain(JoshiandVijay-Shanker,2001;Bayeretal.,2004;Bos,2005).ZettlemoyerandCollins(2005)presentastatisticalmethodthatisconsider-961

ablymorerobust,butitstillreliesonhand-writtenrulesforlexicalacquisition,whichcancreateaper-formancebottleneck.Inthiswork,weshowthatmethodsdevelopedforSMTcanbebroughttobearontaskswherelogicalformsareinvolved,suchassemanticparsing.Inpar-ticular,weextendtheWASPsemanticparsingalgo-rithmbyaddingvariable-bindingλ-operatorstotheunderlyingSCFG.Theresultingsynchronousgram-margenerateslogicalformsusingλ-calculus(Mon-tague,1970).Asemanticparserislearnedgivenasetofsentencesandtheircorrectlogicalformsus-ingSMTmethods.Thenewalgorithmiscalledλ-WASP,andisshowntobethebest-performingsys-temsofarintheGEOQUERYdomain.2TestDomainInthiswork,wemainlyconsidertheGEOQUERYdomain,whereaquerylanguagebasedonPrologisusedtoqueryadatabaseonU.S.geography(ZelleandMooney,1996).Thequerylanguageconsistsoflogicalformsaugmentedwithmeta-predicatesforconceptssuchassmallestandcount.Figure1showstwosamplelogicalformsandtheirEnglishglosses.Throughoutthispaper,weusethenotationx1,x2,...forlogicalvariables.AlthoughProloglogicalformsarethemainfocusofthispaper,ouralgorithmmakesminimalassump-tionsaboutthetargetMRL.TheonlyrestrictionontheMRListhatitbedeﬁnedbyanunambiguouscontext-freegrammar(CFG)thatdividesalogicalformintosubformulas(andtermsintosubterms).Figure2(a)showsasampleparsetreeofalogicalform,whereeachCFGproductioncorrespondstoasubformula.3TheSemanticParsingAlgorithmOurworkisbasedontheWASPsemanticparsingal-gorithm(WongandMooney,2006),whichtranslatesNLsentencesintoMRsusinganSCFG.InWASP,eachSCFGproductionhasthefollowingform:A→hα,βi(1)whereαisanNLphraseandβistheMRtranslationofα.Bothαandβarestringsofterminalandnon-terminalsymbols.Eachnon-terminalinαappearsinβexactlyonce.Weuseindicestoshowthecor-respondencebetweennon-terminalsinαandβ.Allderivationsstartwithapairofco-indexedstartsym-bols,hS1,S1i.Eachstepofaderivationinvolvestherewritingofapairofco-indexednon-terminalsbythesameSCFGproduction.Theyieldofaderiva-tionisapairofterminalstrings,he,fi,whereeisanNLsentenceandfistheMRtranslationofe.Forconvenience,wecallanSCFGproductionarulethroughoutthispaper.WhileWASPworkswellfortargetMRLsthatarefreeoflogicalvariablessuchasCLANG(WongandMooney,2006),itcannoteasilyhandlevariouskindsoflogicalformsusedincomputationalseman-tics,suchaspredicatelogic.TheproblemisthatWASPlacksaprincipledmechanismforhandlinglogicalvariables.Inthiswork,weextendtheWASPalgorithmbyaddingavariable-bindingmechanismbasedonλ-calculus,whichallowsforcompositionalsemanticsforlogicalforms.ThisworkisbasedonanextendedversionofSCFG,whichwecallλ-SCFG,whereeachrulehasthefollowingform:A→hα,λx1...λxk.βi(2)whereαisanNLphraseandβistheMRtrans-lationofα.Unlike(1),βisastringoftermi-nals,non-terminals,andlogicalvariables.Thevariable-bindingoperatorλbindsoccurrencesofthelogicalvariablesx1,...,xkinβ,whichmakesλx1...λxk.βaλ-functionofarityk.Whenap-pliedtoalistofarguments,(xi1,...,xik),theλ-functiongivesβσ,whereσisasubstitutionoper-ator,{x1/xi1,...,xk/xik},thatreplacesallboundoccurrencesofxjinβwithxij.Ifanyofthear-gumentsxijappearinβasafreevariable(i.e.notboundbyanyλ),thenthosefreevariablesinβmustberenamedbeforefunctionapplicationtakesplace.Eachnon-terminalAjinβisfollowedbyalistofarguments,xj=(xj1,...,xjkj).Duringpars-ing,Ajmustberewrittenbyaλ-functionfjofar-itykj.LikeSCFG,aderivationstartswithapairofco-indexedstartsymbolsandendswhenallnon-terminalshavebeenrewritten.Tocomputetheyieldofaderivation,eachfjisappliedtoitscorrespond-ingargumentsxjtoobtainanMRstringfreeofλ-operatorswithlogicalvariablesproperlynamed.962

(a)answer(x1,smallest(x2,(state(x1),area(x1,x2))))Whatisthesmalleststatebyarea?(b)answer(x1,count(x2,(city(x2),major(x2),loc(x2,x3),nextto(x3,x4),state(x3),equal(x4,stateid(texas)))))HowmanymajorcitiesareinstatesborderingTexas?Figure1:SamplelogicalformsintheGEOQUERYdomainandtheirEnglishglosses.(a)smallest(x2,(FORM,FORM))QUERYanswer(x1,FORM)area(x1,x2)state(x1)(b)λx1.smallest(x2,(FORM(x1),FORM(x1,x2)))QUERYanswer(x1,FORM(x1))λx1.state(x1)λx1.λx2.area(x1,x2)Figure2:ParsetreesofthelogicalforminFigure1(a).Asaconcreteexample,Figure2(b)showsanMRparsetreethatcorrespondstotheEnglishparse,[Whatisthe[smallest[state][byarea]]],basedontheλ-SCFGrulesinFigure3.TocomputetheyieldofthisMRparsetree,westartfromtheleafnodes:applyλx1.state(x1)totheargument(x1),andλx1.λx2.area(x1,x2)tothearguments(x1,x2).ThisresultsintwoMRstrings:state(x1)andarea(x1,x2).SubstitutingtheseMRstringsfortheFORMnon-terminalsintheparentnodegivestheλ-functionλx1.smallest(x2,(state(x1),area(x1,x2))).Applyingthisλ-functionto(x1)givestheMRstringsmallest(x2,(state(x1),area(x1,x2))).SubstitutingthisMRstringfortheFORMnon-terminalinthegrandparentnodeinturngivesthelogicalforminFigure1(a).ThisistheyieldoftheMRparsetree,sincetherootnodeoftheparsetreeisreached.3.1LexicalAcquisitionGivenasetoftrainingsentencespairedwiththeircorrectlogicalforms,{hei,fii},themainlearningtaskistoﬁndaλ-SCFG,G,thatcoversthetrain-ingdata.Likemostexistingworkonsyntax-basedSMT(Chiang,2005;Galleyetal.,2006),wecon-structGusingrulesextractedfromwordalignments.WeusetheK=5mostprobablewordalignmentsforthetrainingsetgivenbyGIZA++(OchandNey,2003),withvariablenamesignoredtoreducespar-sity.Rulesarethenextractedfromeachwordalign-mentasfollows.Togroundourdiscussion,weusethewordalign-mentinFigure4asanexample.TorepresentthelogicalforminFigure4,weuseitslinearizedparse—alistofMRLproductionsthatgeneratethelogicalform,intop-down,left-mostorder(cf.Fig-ure2(a)).SincetheMRLgrammarisunambiguous,everylogicalformhasauniquelinearizedparse.Weassumethealignmenttoben-to-1,whereeachwordislinkedtoatmostoneMRLproduction.Rulesareextractedinabottom-upmanner,start-ingwithMRLproductionsattheleavesoftheMRparsetree,e.g.FORM→state(x1)inFig-ure2(a).GivenanMRLproduction,A→β,aruleA→hα,λxi1...λxik.βiisextractedsuchthat:(1)αistheNLphraselinkedtotheMRLproduc-tion;(2)xi1,...,xikarethelogicalvariablesthatappearinβandoutsidethecurrentleafnodeintheMRparsetree.Ifxi1,...,xikwerenotboundbyλ,theywouldbecomefreevariablesinβ,subjecttorenamingduringfunctionapplication(andtherefore,invisibletotherestofthelogicalform).Forexam-ple,sincex1isanargumentofthestatepredicateaswellasanswerandarea,x1mustbebound(cf.thecorrespondingtreenodeinFigure2(b)).TheruleextractedforthestatepredicateisshowninFigure3.ThecasefortheinternalnodesoftheMRparsetreeissimilar.GivenanMRLproduction,A→β,whereβcontainsnon-terminalsA1,...,An,aruleA→hα,λxi1...λxik.β′iisextractedsuchthat:(1)αistheNLphraselinkedtotheMRLproduction,withnon-terminalsA1,...,Anshowingtheposi-tionsoftheargumentstrings;(2)β′isβwitheachnon-terminalAjreplacedwithAj(xj1,...,xjkj),wherexj1,...,xjkjaretheboundvariablesintheλ-functionusedtorewriteAj;(3)xi1,...,xikarethelogicalvariablesthatappearinβ′andoutsidethecurrentMRsub-parse.Forexample,seetherule963

FORM→hstate,λx1.state(x1)iFORM→hbyarea,λx1.λx2.area(x1,x2)iFORM→hsmallestFORM1FORM2,λx1.smallest(x2,(FORM1(x1),FORM2(x1,x2)))iQUERY→hwhatis(1)FORM1,answer(x1,FORM1(x1))iFigure3:λ-SCFGrulesforparsingtheEnglishsentenceinFigure1(a).smallesttheiswhatstatebyareaQUERY→answer(x1,FORM)FORM→smallest(x2,(FORM,FORM))FORM→state(x1)FORM→area(x1,x2)Figure4:WordalignmentforthesentencepairinFigure1(a).extractedforthesmallestpredicateinFigure3,wherex2isanargumentofsmallest,butitdoesnotappearoutsidetheformulasmallest(...),sox2neednotbeboundbyλ.Ontheotherhand,x1appearsinβ′,anditappearsoutsidesmallest(...)(asanargumentofanswer),sox1mustbebound.RuleextractioncontinuesinthismanneruntiltherootoftheMRparsetreeisreached.Figure3showsalltherulesextractedfromFigure4.13.2ProbabilisticSemanticParsingModelSincethelearnedλ-SCFGcanbeambiguous,aprobabilisticmodelisneededforparsedisambigua-tion.Weusethemaximum-entropymodelproposedinWongandMooney(2006),whichdeﬁnesacondi-tionalprobabilitydistributionoverderivationsgivenanobservedNLsentence.TheoutputMRistheyieldofthemostprobablederivationaccordingtothismodel.Parameterestimationinvolvesmaximizingtheconditionallog-likelihoodofthetrainingset.Foreachrule,r,thereisafeaturethatreturnsthenum-beroftimesrisusedinaderivation.MorefeatureswillbeintroducedinSection5.4PromotingNL/MRLIsomorphismWehavedescribedtheλ-WASPalgorithmwhichgenerateslogicalformsbasedonλ-calculus.Whilereasonablyeffective,itcanbeimprovedinseveralways.Inthissection,wefocusonimprovinglexicalacquisition.1Fordetailsregardingnon-isomorphicNL/MRparsetrees,removalofbadlinksfromalignments,andextractionofwordgaps(e.g.thetoken(1)inthelastruleofFigure3),seeWongandMooney(2006).Toseewhythecurrentlexicalacquisitionalgo-rithmcanbeproblematic,considerthewordalign-mentinFigure5(forthesentencepairinFig-ure1(b)).Norulescanbeextractedforthestatepredicate,becausetheshortestNLsubstringthatcoversthewordstatesandtheargumentstringTexas,i.e.statesborderingTexas,containsthewordbordering,whichislinkedtoanMRLproductionoutsidetheMRsub-parserootedatstate.Ruleextractionisforbiddeninthiscasebecauseitwoulddestroythelinkbetweenborderingandnextto.Inotherwords,theNLandMRparsetreesarenotisomorphic.ThisproblemcanbeamelioratedbytransformingthelogicalformofeachtrainingsentencesothattheNLandMRparsetreesaremaximallyisomor-phic.Thisispossiblebecausesomeoftheopera-torsusedinthelogicalforms,notablytheconjunc-tionoperator(,),arebothassociative(a,(b,c)=(a,b),c=a,b,c)andcommutative(a,b=b,a).Hence,conjunctscanbereorderedandre-groupedwithoutchangingthemeaningofaconjunc-tion.Forexample,ruleextractionwouldbepos-sibleifthepositionsofthenexttoandstateconjunctswereswitched.Wepresentamethodforregroupingconjunctstopromoteisomorphismbe-tweenNLandMRparsetrees.2Givenaconjunc-tion,itdoesthefollowing:(SeeFigure6forthepseudocode,andFigure5foranillustration.)Step1.IdentifytheMRLproductionsthatcorre-spondtotheconjunctsandthemeta-predicatethattakestheconjunctionasanargument(countinFigure5),andﬁgurethemasverticesinanundi-2Thismethodalsoappliestoanyoperatorsthatareassocia-tiveandcommutative,e.g.disjunction.Forconcreteness,how-ever,weuseconjunctionasanexample.964

QUERY→answer(x1,FORM)howmanymajorcitiesareinstatesborderingtexasFORM→count(x2,(CONJ),x1)CONJ→city(x2),CONJCONJ→major(x2),CONJCONJ→loc(x2,x3),CONJCONJ→nextto(x3,x4),CONJCONJ→state(x3),FORMFORM→equal(x4,stateid(texas))OriginalMRparsex2x3x4howmanycitiesinstatesborderingtexasmajorQUERYanswer(x1,FORM)count(x2,(CONJ),x1)major(x2),CONJcity(x2),CONJloc(x2,x3),CONJstate(x3),CONJnextto(x3,x4),FORMequal(x4,stateid(texas))QUERYanswer(x1,FORM)count(x2,(CONJ),x1)major(x2),CONJcity(x2),CONJloc(x2,x3),CONJequal(x4,stateid(texas))nextto(x3,x4),CONJstate(x3),FORM(shownaboveasthickedges)Step5.FindMSTStep4.AssignedgeweightsStep6.ConstructMRparseFormgraphSteps1–3.Figure5:TransformingthelogicalforminFigure1(b).ThestepnumberscorrespondtothoseinFigure6.Input:Aconjunction,c,ofnconjuncts;MRLproductions,p1,...,pn,thatcorrespondtoeachconjunct;anMRLproduction,p0,thatcorrespondstothemeta-predicatetakingcasanargument;anNLsentence,e;awordalignment,a.Letv(p)bethesetoflogicalvariablesthatappearinp.Createanundirectedgraph,Γ,withverticesV={pi|i=0,...,n}1andedgesE={(pi,pj)|i<j,v(pi)∩v(pj)6=∅}.Lete(p)bethesetofwordsinetowhichpislinkedaccordingtoa.Letspan(pi,pj)betheshortestsubstringofethat2includese(pi)∪e(pj).Subtract{(pi,pj)|i6=0,span(pi,pj)∩e(p0)6=∅}fromE.Addedges(p0,pi)toEifpiisnotalreadyconnectedtop0.3Foreachedge(pi,pj)inE,setedgeweighttotheminimumworddistancebetweene(pi)ande(pj).4Findaminimumspanningtree,T,forΓusingKruskal’salgorithm.5Usingp0astheroot,constructaconjunctionc′basedonT,andthenreplacecwithc′.6Figure6:AlgorithmforregroupingconjunctstopromoteisomorphismbetweenNLandMRparsetrees.rectedgraph,Γ.Anedge(pi,pj)isinΓifandonlyifpiandpjcontainoccurrencesofthesamelogicalvariables.EachedgeinΓindicatesapossibleedgeinthetransformedMRparsetree.Intuitively,twoconceptsarecloselyrelatediftheyinvolvethesamelogicalvariables,andtherefore,shouldbeplacedclosetogetherintheMRparsetree.Bykeepingoc-currencesofalogicalvariableincloseproximityintheMRparsetree,wealsoavoidunnecessaryvari-ablebindingsintheextractedrules.Step2.RemoveedgesfromΓwhoseinclusionintheMRparsetreewouldpreventtheNLandMRparsetreesfrombeingisomorphic.Step3.AddedgestoΓtomakesurethataspanningtreeforΓexists.Steps4–6.Assignedgeweightsbasedonworddis-tance,ﬁndaminimumspanningtree,T,forΓ,thenregrouptheconjunctsbasedonT.ThechoiceofTreﬂectstheintuitionthatwordsthatoccurcloseto-getherinasentencetendtobesemanticallyrelated.Thisprocedureisrepeatedforallconjunctionsthatappearinalogicalform.Rulesarethenex-tractedfromthesameinputalignmentusedtore-groupconjuncts.Ofcourse,theregroupingofcon-junctsrequiresagoodalignmenttobeginwith,andthatrequiresareasonableorderingofconjunctsinthetrainingdata,sincethealignmentmodelissen-sitivetowordorder.Thissuggestsaniterativealgo-rithminwhichabettergroupingofconjunctsleadstoabetteralignmentmodel,whichguidesfurtherre-groupinguntilconvergence.Wedidnotpursuethis,asitisnotneededinourexperimentssofar.965

(a)answer(x1,largest(x2,(state(x1),major(x1),river(x1),traverse(x1,x2))))Whatistheentitythatisastateandalsoamajorriver,thattraversessomethingthatisthelargest?(b)answer(x1,smallest(x2,(highest(x1,(point(x1),loc(x1,x3),state(x3))),density(x1,x2))))Amongthehighestpointsofallstates,whichonehasthelowestpopulationdensity?(c)answer(x1,equal(x1,stateid(alaska)))Alaska?(d)answer(x1,largest(x2,(largest(x1,(state(x1),nextto(x1,x3),state(x3))),population(x1,x2))))Amongthelargeststatethatborderssomeotherstate,whichistheonewiththelargestpopulation?Figure7:Typicalerrorsmadebytheλ-WASPparser,alongwiththeirEnglishinterpretations,beforeanylanguagemodelingforthetargetMRLwasdone.5ModelingtheTargetMRLInthissection,weproposetwomethodsformodel-ingthetargetMRL.Thisismotivatedbythefactthatmanyoftheerrorsmadebytheλ-WASPparsercanbedetectedbyinspectingtheMRtranslationsalone.Figure7showssometypicalerrors,whichcanbeclassiﬁedintotwobroadcategories:1.Typemismatcherrors.Forexample,astatecan-notpossiblybeariver(Figure7(a)).Alsoitisawkwardtotalkaboutthepopulationdensityofastate’shighestpoint(Figure7(b)).2.Errorsthatdonotinvolvetypemismatch.Forex-ample,aquerycanbeoverlytrivial(Figure7(c)),orinvolveaggregatefunctionsonaknownsingle-ton(Figure7(d)).Theﬁrsttypeoferrorscanbeﬁxedbytypecheck-ing.Eachm-placepredicateisassociatedwithalistofm-tuplesshowingallvalidcombinationsofentitytypesthatthemargumentscanreferto:point():{(POINT)}density(,):{(COUNTRY,NUM),(STATE,NUM),(CITY,NUM)}Thesem-tuplesofentitytypesaregivenasdo-mainknowledge.Theparsermaintainsasetofpossibleentitytypesforeachlogicalvariablein-troducedinapartialderivation(exceptthosethatarenolongervisible).Ifthereisalogicalvari-ablethatcannotrefertoanytypesofentities(i.e.thesetofentitytypesisempty),thenthepar-tialderivationisconsideredinvalid.Forexam-ple,basedonthetuplesshownabove,point(x1)anddensity(x1,)cannotbebothtrue,because{POINT}∩{COUNTRY,STATE,CITY}=∅.Theuseoftypecheckingistoexploitthefactthatpeo-pletendnottoaskquestionsthatobviouslyhavenovalidanswers(Grice,1975).ItisalsosimilartoSchuler’s(2003)useofmodel-theoreticinterpreta-tionstoguidesyntacticparsing.Errorsthatdonotinvolvetypemismatcharehandledbyaddingnewfeaturestothemaximum-entropymodel(Section3.2).Weonlyconsiderfea-turesthatarebasedontheMRtranslations,andtherefore,thesefeaturescanbeseenasanimplicitlanguagemodelofthetargetMRL(Papinenietal.,1997).Ofthemanyfeaturesthatwehavetried,onefeaturesetstandsoutasbeingthemosteffec-tive,thetwo-levelrulesinCollinsandKoo(2005),whichgivethenumberoftimesagivenruleisusedtoexpandanon-terminalinagivenparentrule.WeuseonlytheMRLpartoftherules.Forex-ample,anegativeweightforthecombinationofQUERY→answer(x1,FORM(x1))andFORM→λx1.equal(x1,)woulddiscourageanyparsethatyieldsFigure7(c).Thetwo-levelrulesfeatures,alongwiththefeaturesdescribedinSection3.2,areusedintheﬁnalversionofλ-WASP.6ExperimentsWeevaluatedtheλ-WASPalgorithmintheGEO-QUERYdomain.ThelargerGEOQUERYcorpuscon-sistsof880Englishquestionsgatheredfromvarioussources(WongandMooney,2006).ThequestionsweremanuallytranslatedintoProloglogicalforms.Theaveragelengthofasentenceis7.57words.Weperformedasinglerunof10-foldcrossvalidation,andmeasuredtheperformanceofthelearnedparsersusingprecision(percentageoftrans-lationsthatwerecorrect),recall(percentageoftestsentencesthatwerecorrectlytranslated),andF-measure(harmonicmeanofprecisionandrecall).Atranslationisconsideredcorrectifitretrievesthesameanswerasthecorrectlogicalform.Figure8showsthelearningcurvesfortheλ-966

 0 10 20 30 40 50 60 70 80 90 100 0 100 200 300 400 500 600 700 800 900Precision (%)Number of training exampleslambda-WASPWASPSCISSORZ&C(a)Precision 0 10 20 30 40 50 60 70 80 90 100 0 100 200 300 400 500 600 700 800 900Recall (%)Number of training exampleslambda-WASPWASPSCISSORZ&C(b)RecallFigure8:LearningcurvesforvariousparsingalgorithmsonthelargerGEOQUERYcorpus.(%)λ-WASPWASPSCISSORZ&CPrecision91.9587.1992.0896.25Recall86.5974.7772.2779.29F-measure89.1980.5080.9886.95Table1:PerformanceofvariousparsingalgorithmsonthelargerGEOQUERYcorpus.WASPalgorithmcomparedto:(1)theoriginalWASPalgorithmwhichusesafunctionalquerylan-guage(FunQL);(2)SCISSOR(GeandMooney,2005),afully-supervised,combinedsyntactic-semanticparsingalgorithmwhichalsousesFunQL;and(3)ZettlemoyerandCollins(2005)(Z&C),aCCG-basedalgorithmwhichusesProloglogicalforms.Table1summarizestheresultsattheendofthelearningcurves(792trainingexamplesforλ-WASP,WASPandSCISSOR,600forZ&C).Afewobservationscanbemade.First,algorithmsthatuseProloglogicalformsasthetargetMRLgen-erallyshowbetterrecallthanthoseusingFunQL.Inparticular,λ-WASPhasthebestrecallbyfar.OnereasonisthatitallowslexicalitemstobecombinedinwaysnotallowedbyFunQLorthehand-writtentemplatesinZ&C,e.g.[smallest[state][byarea]]inFigure3.Second,Z&Chasthebestprecision,al-thoughtheirresultsarebasedon280testexamplesonly,whereasourresultsarebasedon10-foldcrossvalidation.Third,λ-WASPhasthebestF-measure.Toseetherelativeimportanceofeachcomponentoftheλ-WASPalgorithm,weperformedtwoabla-tionstudies.First,wecomparedtheperformanceofλ-WASPwithandwithoutconjunctregrouping(Section4).Second,wecomparedtheperformanceofλ-WASPwithandwithoutlanguagemodelingfortheMRL(Section5).Table2showstheresults.Itisfoundthatconjunctregroupingimprovesrecall(p<0.01basedonthepairedt-test),andtheuseoftwo-levelrulesinthemaximum-entropymodelim-provesprecisionandrecall(p<0.05).Typecheck-ingalsosigniﬁcantlyimprovesprecisionandrecall.Amajoradvantageofλ-WASPoverSCISSORandZ&Cisthatitdoesnotrequireanypriorknowl-edgeoftheNLsyntax.Figure9showstheperfor-manceofλ-WASPonthemultilingualGEOQUERYdataset.The250-exampledatasetisasubsetofthelargerGEOQUERYcorpus.AllEnglishquestionsinthisdatasetweremanuallytranslatedintoSpanish,JapaneseandTurkish,whilethecorrespondingPro-logqueriesremainunchanged.Figure9showsthatλ-WASPperformedcomparablyforallNLs.Incon-trast,SCISSORcannotbeuseddirectlyonthenon-Englishdata,becausesyntacticannotationsareonlyavailableinEnglish.Z&Ccannotbeuseddirectlyeither,becauseitrequiresNL-speciﬁctemplatesforbuildingCCGgrammars.7ConclusionsWehavepresentedλ-WASP,asemanticparsingal-gorithmbasedonaλ-SCFGthatgenerateslogicalformsusingλ-calculus.AsemanticparserislearnedgivenasetoftrainingsentencesandtheircorrectlogicalformsusingstandardSMTtechniques.Theresultisarobustsemanticparserforpredicatelogic,anditisthebest-performingsystemsofarintheGEOQUERYdomain.ThisworkshowsthatitispossibletousestandardSMTmethodsintaskswherelogicalformsarein-volved.Forexample,itshouldbestraightforwardtoadaptλ-WASPtotheNLgenerationtask—alloneneedsisadecoderthatcanhandleinputlogicalforms.Othertasksthatcanpotentiallybeneﬁtfrom967

(%)PrecisionRecallλ-WASP91.9586.59w/oconj.regrouping90.7383.07(%)PrecisionRecallλ-WASP91.9586.59w/otwo-levelrules88.4684.32andw/otypechecking65.4563.18Table2:Performanceofλ-WASPwithcertaincomponentsofthealgorithmremoved. 0 20 40 60 80 100 0 50 100 150 200 250Precision (%)Number of training examplesEnglishSpanishJapaneseTurkish(a)Precision 0 20 40 60 80 100 0 50 100 150 200 250Recall (%)Number of training examplesEnglishSpanishJapaneseTurkish(b)RecallFigure9:Learningcurvesforλ-WASPonthemultilingualGEOQUERYdataset.thisincludequestionansweringandinterlingualMT.Infuturework,weplantofurthergeneralizethesynchronousparsingframeworktoallowdifferentcombinationsofgrammarformalisms.Forexam-ple,tohandlelong-distancedependenciesthatoccurinopen-domaintext,CCGandTAGwouldbemoreappropriatethanCFG.Certainapplicationsmayre-quiredifferentmeaningrepresentations,e.g.framesemantics.Acknowledgments:WethankRohitKate,Raz-vanBunescuandtheanonymousreviewersfortheirvaluablecomments.ThisworkwassupportedbyagiftfromGoogleInc.ReferencesA.V.AhoandJ.D.Ullman.1972.TheTheoryofPars-ing,Translation,andCompiling.PrenticeHall,EnglewoodCliffs,NJ.S.Bayer,J.Burger,W.Greiff,andB.Wellner.2004.TheMITRElogicalformgenerationsystem.InProc.ofSenseval-3,Barcelona,Spain,July.P.BlackburnandJ.Bos.2005.RepresentationandInferenceforNaturalLanguage:AFirstCourseinComputationalSe-mantics.CSLIPublications,Stanford,CA.J.Bos.2005.Towardswide-coveragesemanticinterpretation.InProc.ofIWCS-05,Tilburg,TheNetherlands,January.D.Chiang.2005.Ahierarchicalphrase-basedmodelforsta-tisticalmachinetranslation.InProc.ofACL-05,pages263–270,AnnArbor,MI,June.M.CollinsandT.Koo.2005.Discriminativererankingfornaturallanguageparsing.ComputationalLinguistics,31(1):25–69.M.Galley,J.Graehl,K.Knight,D.Marcu,S.DeNeefe,W.Wang,andI.Thayer.2006.Scalableinferenceandtrain-ingofcontext-richsyntactictranslationmodels.InProc.ofCOLING/ACL-06,pages961–968,Sydney,Australia,July.R.GeandR.J.Mooney.2005.Astatisticalsemanticparserthatintegratessyntaxandsemantics.InProc.ofCoNLL-05,pages9–16,AnnArbor,MI,July.H.P.Grice.1975.Logicandconversation.InP.ColeandJ.Morgan,eds.,SyntaxandSemantics3:SpeechActs,pages41–58.AcademicPress,NewYork.A.K.JoshiandK.Vijay-Shanker.2001.Compositionalse-manticswithlexicalizedtree-adjoininggrammar(LTAG):Howmuchunderspeciﬁcationisnecessary?InH.Buntetal.,eds.,ComputingMeaning,volume2,pages147–163.KluwerAcademicPublishers,Dordrecht,TheNetherlands.R.Montague.1970.Universalgrammar.Theoria,36:373–398.F.J.OchandH.Ney.2003.Asystematiccomparisonofvari-ousstatisticalalignmentmodels.ComputationalLinguistics,29(1):19–51.K.A.Papineni,S.Roukos,andR.T.Ward.1997.Feature-basedlanguageunderstanding.InProc.ofEuroSpeech-97,pages1435–1438,Rhodes,Greece.W.Schuler.2003.Usingmodel-theoreticsemanticinterpre-tationtoguidestatisticalparsingandwordrecognitioninaspokenlanguageinterface.InProc.ofACL-03,pages529–536.Y.W.WongandR.J.Mooney.2006.Learningforseman-ticparsingwithstatisticalmachinetranslation.InProc.ofHLT/NAACL-06,pages439–446,NewYorkCity,NY.Y.W.WongandR.J.Mooney.2007.Generationbyinvertingasemanticparserthatusesstatisticalmachinetranslation.InProc.ofNAACL/HLT-07,Rochester,NY,toappear.D.Wu.1997.Stochasticinversiontransductiongrammarsandbilingualparsingofparallelcorpora.ComputationalLin-guistics,23(3):377–403.K.YamadaandK.Knight.2001.Asyntax-basedstatisti-caltranslationmodel.InProc.ofACL-01,pages523–530,Toulouse,France.J.M.ZelleandR.J.Mooney.1996.Learningtoparsedatabasequeriesusinginductivelogicprogramming.InProc.ofAAAI-96,pages1050–1055,Portland,OR,August.L.S.ZettlemoyerandM.Collins.2005.Learningtomapsen-tencestologicalform:Structuredclassiﬁcationwithproba-bilisticcategorialgrammars.InProc.ofUAI-05,Edinburgh,Scotland,July.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 968–975,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

968

GeneralizingTreeTransformationsforInductiveDependencyParsingJensNilsson∗JoakimNivre∗†∗V¨axj¨oUniversity,SchoolofMathematicsandSystemsEngineering,Sweden†UppsalaUniversity,Dept.ofLinguisticsandPhilology,Sweden{jni,nivre,jha}@msi.vxu.seJohanHall∗AbstractPreviousstudiesindata-drivendependencyparsinghaveshownthattreetransformationscanimproveparsingaccuracyforspeciﬁcparsersanddatasets.Weinvestigatetowhatextentthiscanbegeneralizedacrosslanguages/treebanksandparsers,focusingonpseudo-projectiveparsing,asawayofcapturingnon-projectivedependencies,andtransformationsusedtofacilitateparsingofcoordinatestructuresandverbgroups.Theresultsindicatethatthebeneﬁcialeffectofpseudo-projectiveparsingisindependentofparsingstrategybutsensitivetolanguageortreebankspeciﬁcproperties.Bycontrast,theconstructionspeciﬁctransformationsappeartobemoresensitivetoparsingstrategybuthaveaconstantpositiveeffectoverseverallanguages.1IntroductionTreebankparsersaretrainedonsyntacticallyanno-tatedsentencesandamajorpartoftheirsuccesscanbeattributedtoextensivemanipulationsofthetrain-ingdataaswellastheoutputoftheparser,usuallyintheformofvarioustreetransformations.Thiscanbeseeninstate-of-the-artconstituency-basedparserssuchasCollins(1999),Charniak(2000),andPetrovetal.(2006),andtheeffectsofdifferenttrans-formationshavebeenstudiedbyJohnson(1998),KleinandManning(2003),andBikel(2004).Corre-spondingmanipulationsintheformoftreetransfor-mationsfordependency-basedparsershaverecentlygainedmoreinterest(NivreandNilsson,2005;HallandNov´ak,2005;McDonaldandPereira,2006;Nilssonetal.,2006)butarestilllessstudied,partlybecauseconstituency-basedparsinghasdominatedtheﬁeldforalongtime,andpartlybecausedepen-dencystructureshavelessstructuretomanipulatethanconstituentstructures.Mostofthestudiesinthistraditionfocusonapar-ticularparsingmodelandaparticulardataset,whichmeansthatitisdifﬁculttosaywhethertheeffectofagiventransformationisdependentonapartic-ularparsingstrategyoronpropertiesofaparticu-larlanguageortreebank,orboth.Theaimofthisstudyistofurtherinvestigatesometreetransforma-tiontechniquespreviouslyproposedfordata-drivendependencyparsing,withthespeciﬁcaimoftryingtogeneralizeresultsacrosslanguages/treebanksandparsers.Moreprecisely,wewanttoestablish,ﬁrstofall,whetherthetransformationassuchmakesspeciﬁcassumptionsaboutthelanguage,treebankorparserand,secondly,whethertheimprovedpars-ingaccuracythatisduetoagiventransformationisconstantacrossdifferentlanguages,treebanks,andparsers.Thethreetypesofsyntacticphenomenathatwillbestudiedherearenon-projectivity,coordinationandverbgroups,whichindifferentwaysposeprob-lemsfordependencyparsers.Wewillfocusontreetransformationsthatcombinepreprocessingwithpost-processing,andwheretheparseristreatedasablackbox,suchasthepseudo-projectiveparsingtechniqueproposedbyNivreandNilsson(2005)andthetreetransformationsinvestigatedinNils-sonetal.(2006).Tostudytheinﬂuenceoflan-969

guageandtreebankspeciﬁcpropertieswewillusedatafromArabic,Czech,Dutch,andSlovene,takenfromtheCoNLL-Xsharedtaskonmultilingualde-pendencyparsing(BuchholzandMarsi,2006).Tostudytheinﬂuenceofparsingmethodology,wewillcomparetwodifferentparsers:MaltParser(Nivreetal.,2004)andMSTParser(McDonaldetal.,2005).Notethat,whileitispossibleinprincipletodistin-guishbetweensyntacticpropertiesofalanguageassuchandpropertiesofaparticularsyntacticannota-tionofthelanguageinquestion,itwillbeimpossi-bletoteasetheseapartintheexperimentsreportedhere,sincethiswouldrequirehavingnotonlymul-tiplelanguagesbutalsomultipletreebanksforeachlanguage.Inthefollowing,wewillthereforespeakaboutthepropertiesoftreebanks(ratherthanlan-guages),butitshouldbeunderstoodthattheseprop-ertiesingeneraldependbothonpropertiesofthelanguageandoftheparticularsyntacticannotationadoptedinthetreebank.Therestofthepaperisstructuredasfollows.Sec-tion2surveystreetransformationsusedindepen-dencyparsinganddiscussesdependenciesbetweentransformations,ontheonehand,andtreebanksandparsers,ontheother.Section3introducesthefourtreebanksusedinthisstudy,andsection4brieﬂydescribesthetwoparsers.Experimentalresultsarepresentedinsection5andconclusionsinsection6.2Background2.1Non-projectivityThetreetransformationsthathaveattractedmostin-terestintheliteratureondependencyparsingarethoseconcernedwithrecoveringnon-projectivity.Thedeﬁnitionofnon-projectivitycanbefoundinKahaneetal.(1998).Informally,anarcisprojec-tiveifalltokensitcoversaredescendantsofthearc’sheadtoken,andadependencytreeisprojectiveifallitsarcsareprojective.1Thefullpotentialofdependencyparsingcanonlyberealizedifnon-projectivityisallowed,whichposeaproblemforprojectivedependencyparsers.Directnon-projectiveparsingcanbeperformedwithgoodaccuracy,e.g.,usingtheChu-Liu-Edmondsal-1Ifdependencyarcsaredrawnabovethelinearlyorderedsequenceoftokens,precededbyaspecialrootnode,thenanon-projectivedependencytreealwayshascrossingarcs.gorithm,asproposedbyMcDonaldetal.(2005).Ontheotherhand,non-projectiveparserstend,amongotherthings,tobeslower.Inordertomaintainthebeneﬁtsofprojectiveparsing,treetransformationstechniquestorecovernon-projectivitywhileusingaprojectiveparserhavebeenproposedinseveralstud-ies,somedescribedbelow.Indiscussingtherecoveryofemptycategoriesindata-drivenconstituencyparsing,Campbell(2004)distinguishesbetweenapproachesbasedonpurepost-processingandapproachesbasedonacombi-nationofpreprocessingandpost-processing.Thesamedivisioncanbemadefortherecoveryofnon-projectivedependenciesindata-drivendependencyparsing.PurePost-processingHallandNov´ak(2005)proposeacorrectivemodel-ingapproach.ThemotivationisthattheparsersofCollinsetal.(1999)andCharniak(2000)adaptedtoCzecharenotabletocreatethenon-projectivearcspresentinthetreebank,whichisunsatisfac-tory.Theythereforeaimtocorrecterroneousarcsintheparser’soutput(speciﬁcallyallthosearcswhichshouldbenon-projective)bytrainingaclassiﬁerthatpredictsthemostprobableheadofatokenintheneighborhoodoftheheadassignedbytheparser.Anotherexampleisthesecond-orderapproximatespanningtreeparserdevelopedbyMcDonaldandPereira(2006).ItstartsbyproducingthehighestscoringprojectivedependencytreeusingEisner’sal-gorithm.Inthesecondphase,treetransformationsareperformed,replacinglowerscoringprojectivearcswithhigherscoringnon-projectiveones.PreprocessingwithPost-processingThetrainingdatacanalsobepreprocessedtofacili-tatetherecoveryofnon-projectivearcsintheoutputofaprojectiveparser.Thepseudo-projectivetrans-formationproposedbyNivreandNilsson(2005)issuchanapproach,whichiscompatiblewithdiffer-entparserengines.First,thetrainingdataisprojectivizedbymakingnon-projectivearcsprojectiveusingaliftingoper-ation.Thisiscombinedwithanaugmentationofthedependencylabelsofprojectivizedarcs(and/orsurroundingarcs)withinformationthatprobablyre-vealstheircorrectnon-projectivepositions.Theout-970

(PS)C1?S1?C2?(MS)C1?S1?C2?(CS)C1?S1?C2?Figure1:Dependencystructureforcoordinationputoftheparser,trainedontheprojectivizeddata,isthendeprojectivizedbyaheuristicsearchusingtheaddedinformationinthedependencylabels.Theonlyassumptionmadeabouttheparseristhereforethatitcanlearntoderivelabeleddependencystruc-tureswithaugmenteddependencylabels.2.2CoordinationandVerbGroupsThesecondtypeoftransformationconcernslinguis-ticphenomenathatarenotimpossibleforaprojec-tiveparsertoprocessbutwhichmaybedifﬁculttolearn,givenacertainchoiceofdependencyanaly-sis.Thisstudyisconcernedwithtwosuchphe-nomena,coordinationandverbgroups,forwhichtreetransformationshavebeenshowntoimproveparsingaccuracyforMaltParseronCzech(Nils-sonetal.,2006).ThegeneralconclusionofthisstudyisthatcoordinationandverbgroupsinthePragueDependencyTreebank(PDT),basedonthe-oriesofthePragueschool(PS),areannotatedinawaythatisdifﬁcultfortheparsertolearn.Bytrans-formingcoordinationandverbgroupsinthetrain-ingdatatoanannotationsimilartothatadvocatedbyMel’ˇcuk(1988)andthenperforminganinversetransformationontheparseroutput,parsingaccu-racycanthereforebeimproved.Thisisagainaninstanceoftheblack-boxidea.Schematically,coordinationisannotatedinthePragueschoolasdepictedinPSinﬁgure1,wheretheconjunctsaredependentsoftheconjunction.InMel’ˇcukstyle(MS),ontheotherhand,conjunctsandconjunction(s)formachaingoingfromlefttoright.Athirdwayoftreatingcoordination,notdis-cussedbyNilssonetal.(2006),isusedbytheparserofCollins(1999),whichinternallyrepresentscoor-dinationasadirectrelationbetweentheconjuncts.ThisisillustratedinCSinﬁgure1,wherethecon-junctiondependsononeoftheconjuncts,inthiscaseontherightmostone.Nilssonetal.(2006)alsoshowthattheannotationofverbgroupsisnotwell-suitedforparsingPDTusingMaltParser,andthattransformingthedepen-dencystructureforverbgroupshasapositiveimpactonparsingaccuracy.InPDT,auxiliaryverbsarede-pendentsofthemainverb,whereasitaccordingtoMel’ˇcukisthe(ﬁnite)auxiliaryverbthatistheheadofthemainverb.Again,theparsingexperimentsinthisstudyshowthatverbgroupsaremoredifﬁculttoparseinPSthaninMS.2.3Transformations,Parsers,andTreebanksPseudo-projectiveparsingandtransformationsforcoordinationandverbgroupsareinstancesofthesamegeneralmethodology:1.Applyatreetransformationtothetrainingdata.2.Trainaparseronthetransformeddata.3.Parsenewsentences.4.Applyaninversetransformationtotheoutputoftheparser.Inthisscheme,theparseristreatedasablackbox.Allthatisassumedisthatitisadata-drivenparserdesignedfor(projective)labeleddependencystructures.Inthissense,thetreetransformationsareindependentofparsingmethodology.Whetherthebeneﬁcialeffectofatransformation,ifany,isalsoindependentofparsingmethodologyisanotherquestion,whichwillbeaddressedintheexperimen-talpartofthispaper.Thepseudo-projectivetransformationisindepen-dentnotonlyofparsingmethodologybutalsooftreebank(andlanguage)speciﬁcproperties,aslongasthetargetrepresentationisa(potentiallynon-projective)labeleddependencystructure.Bycon-trast,thecoordinationandverbgrouptransforma-tionspresupposenotonlythatthelanguageinques-tioncontainstheseconstructionsbutalsothatthetreebankadoptsaPSannotation.Inthissense,theyaremorelimitedintheirapplicabilitythanpseudo-projectiveparsing.Again,itisadifferentquestionwhetherthetransformationshaveapositiveeffectforalltreebanks(languages)towhichtheycanbeapplied.3TreebanksTheexperimentsaremostlyconductedusingtree-bankdatafromtheCoNLLsharedtask2006.This971

SloveneArabicDutchCzechSDTPADTAlpinoPDT#T29541951249#S1.51.513.372.7%-NPS22.211.236.423.2%-NPA1.80.45.41.9%-C9.38.54.08.5%-A8.8--1.3Table1:Overviewofthedatasets(orderedbysize),where#S*1000=numberofsentences,#T*1000=numberoftokens,%-NPS=percentageofnon-projectivesentences,%-NPA=percentageofnon-projectivearcs,%-C=percentageofconjuncts,%-A=percentageofauxiliaryverbs.subsectionsummarizessomeoftheimportantchar-acteristicsofthesedatasets,withanoverviewinta-ble1.AnydetailsconcerningtheconversionfromtheoriginalformatsofthevarioustreebankstotheCoNLLformat,apuredependencybasedformat,arefoundindocumentationreferredtoinBuchholzandMarsi(2006).PDT(Hajiˇcetal.,2001)isthelargestmanuallyannotatedtreebank,andasalreadymentioned,itadoptsPSforcoordinationandverbgroups.Asthelastfourrowsreveal,PDTcontainsaquitehighproportionofnon-projectivity,sincealmosteveryfourthdependencygraphcontainsatleastonenon-projectivearc.Thetablealsoshowsthatcoordina-tionismorecommonthanverbgroupsinPDT.Only1.3%ofthetokensinthetrainingdataareidentiﬁedasauxiliaryverbs,whereas8.5%ofthetokensareidentiﬁedasconjuncts.BothSloveneDependencyTreebank(Dˇzeroskietal.,2006)(SDT)andPragueArabicDependencyTreebank(Hajiˇcetal.,2004)(PADT)annotateco-ordinationandverbgroupsasinPDT,sincetheytooareinﬂuencedbythetheoriesofthePragueschool.Theproportionsofnon-projectivityandconjunctsinSDTareinfactquitesimilartotheproportionsinPDT.Thebigdifferenceistheproportionofauxil-iaryverbs,withmanymoreauxiliaryverbsinSDTthaninPDT.Itisthereforeplausiblethatthetrans-formationsforverbgroupswillhavealargerimpactonparseraccuracyinSDT.ArabicisnotaSlaviclanguagessuchasCzechandSlovene,andtheannotationinPADTisthere-foremoredissimilartoPDTthanSDTis.OnesuchexampleisthatArabicdoesnothaveauxiliaryverbs.Table1thusdoesnotgiveﬁguresverbgroups.Theamountofcoordinationisontheotherhandcompa-rabletobothPDTandSDT.Thetablealsorevealsthattheamountofnon-projectivearcsisabout25%ofthatinPDTandSDT,althoughtheamountofnon-projectivesentencesisstillaslargeas50%ofthatinPDTandSDT.Alpino(vanderBeeketal.,2002)intheCoNLLformat,thesecondlargesttreebankinthisstudy,isnotascloselytiedtothetheoriesofthePragueschoolastheothers,butstilltreatscoordinationinawaysimilartoPS.Thetableshowsthatcoor-dinationislessfrequentintheCoNLLversionofAlpinothaninthethreeothertreebanks.TheothercharacteristicofAlpinoisthehighshareofnon-projectivity,wheremorethaneverythirdsentenceisnon-projective.Finally,thelackofinformationabouttheshareofauxiliaryverbsisnotduetothenon-existenceofsuchverbsinDutchbuttothefactthatAlpinoadoptsanMSannotationofverbgroups(i.e.,treatingmainverbsasdependentsofauxiliaryverbs),whichmeansthattheverbgrouptransforma-tionofNilssonetal.(2006)isnotapplicable.4ParsersTheparsersusedintheexperimentsareMalt-Parser(Nivreetal.,2004)andMSTParser(Mc-Donaldetal.,2005).Theseparsersarebasedonverydifferentparsingstrategies,whichmakesthemsuitableinordertotesttheparserindependenceofdifferenttransformations.MaltParseradoptsagreedy,deterministicparsingstrategy,derivingala-beleddependencystructureinasingleleft-to-rightpassovertheinputandusessupportvectorma-chinestopredictthenextparsingaction.MST-Parserinsteadextractsamaximumspanningtreefromadenseweightedgraphcontainingallpossi-bledependencyarcsbetweentokens(withEisner’salgorithmforprojectivedependencystructuresortheChu-Liu-Edmondsalgorithmfornon-projectivestructures),usingaglobaldiscriminativemodelandonlinelearningtoassignweightstoindividualarcs.22Theexperimentsinthispaperarebasedontheﬁrst-orderfactorizationdescribedinMcDonaldetal.(2005)972

5ExperimentsTheexperimentsreportedinsection5.1–5.2belowarebasedonthetrainingsetsfromtheCoNLL-Xsharedtask,exceptwherenoted.Theresultsre-portedareobtainedbyaten-foldcross-validation(withapseudo-randomizedsplit)foralltreebanksexceptPDT,where80%ofthedatawasusedfortrainingand20%fordevelopmenttesting(againwithapseudo-randomizedsplit).Insection5.3,wegiveresultsfortheﬁnalevaluationontheCoNLL-XtestsetsusingallthreetransformationstogetherwithMaltParser.Parsingaccuracyisprimarilymeasuredbytheun-labeledattachmentscore(ASU),i.e.,thepropor-tionoftokensthatareassignedthecorrecthead,ascomputedbytheofﬁcialCoNLL-Xevaluationscriptwithdefaultsettings(thusexcludingallpunctuationtokens).Insection5.3wealsoincludethelabeledattachmentscore(ASL)(whereatokenmusthaveboththecorrectheadandthecorrectdependencyla-beltobecountedascorrect),whichwastheofﬁcialevaluationmetricintheCoNLL-Xsharedtask.5.1ComparingTreebanksWestartbyexaminingtheeffectoftransformationsondatafromdifferenttreebanks(languages),usingasingleparser:MaltParser.Non-projectivityThequestioninfocushereiswhethertheeffectofthepseudo-projectivetransformationforMaltParservarieswiththetreebank.Table2presentstheun-labeledattachmentscoreresults(ASU),compar-ingthepseudo-projectiveparsingtechnique(P-Proj)withtwobaselines,obtainedbytrainingthestrictlyprojectiveparserontheoriginal(non-projective)trainingdata(N-Proj)andonprojectivizedtrain-ingdatawithnoaugmentationofdependencylabels(Proj).Theﬁrstthingtonoteisthatpseudo-projectiveparsinggivesasigniﬁcantimprovementforPDT,aspreviouslyreportedbyNivreandNilsson(2005),butalsoforAlpino,wheretheimprovementisevenlarger,presumablybecauseofthehigherproportionofnon-projectivedependenciesintheDutchtree-bank.Bycontrast,thereisnosigniﬁcantimprove-mentforeitherSDTorPADT,andevenasmalldropN-ProjProjP-ProjSDT77.2776.63∗∗77.11PADT76.9677.07∗77.07∗Alpino82.7583.28∗∗87.08∗∗PDT83.4183.32∗∗84.42∗∗Table2:ASUforpseudo-projectiveparsingwithMaltParser.McNemar’stest:∗=p<.05and∗∗=p<0.01comparedtoN-Proj.123>3SDT88.49.11.70.84PADT66.514.45.213.9Alpino84.613.81.50.07PDT93.85.60.50.1Table3:Thenumberofliftsfornon-projectivearcs.intheaccuracyﬁguresforSDT.Finally,incontrasttotheresultsreportedbyNivreandNilsson(2005),simplyprojectivizingthetrainingdata(withoutus-inganinversetransformation)isnotbeneﬁcialatall,exceptpossiblyforAlpino.Butwhydoesnotpseudo-projectiveparsingim-proveaccuracyforSDTandPADT?Onepossi-blefactoristhecomplexityofthenon-projectiveconstructions,whichcanbemeasuredbycountingthenumberofliftsthatarerequiredtomakenon-projectivearcsprojective.Themoredeeplynestedanon-projectivearcis,themoredifﬁcultitistore-coverbecauseofparsingerrorsaswellassearcher-rorsintheinversetransformation.Theﬁguresinta-ble3shedsomeinterestinglightonthisfactor.Forexample,whereas93.8%ofallarcsinPDTrequireonlyoneliftbeforetheybecomeprojec-tive(88.4%and84.6%forSDTandAlpino,respec-tively),thecorrespondingﬁgureforPADTisaslowas66.5%.PADTalsohasahighproportionofverydeeplynestednon-projectivearcs(>3)incompari-sontotheothertreebanks,makingtheinversetrans-formationforPADTmoreproblematicthanfortheothertreebanks.TheabsenceofapositiveeffectforPADTisthereforeunderstandablegiventhedeeplynestednon-projectiveconstructionsinPADT.However,onequestionthatstillremainsiswhySDTandPDT,whicharesosimilarintermsofbothnestingdepthandamountofnon-projectivity,be-973

Figure2:LearningcurvesforAlpinomeasuredaserrorreductionforASU.havedifferentlywithrespecttopseudo-projectiveparsing.Anotherfactorthatmaybeimportanthereistheamountoftrainingdataavailable.Asshownintable1,PDTismorethan40timeslargerthanSDT.Toinvestigatetheinﬂuenceoftrainingsetsize,alearningcurveexperimenthasbeenper-formed.Alpinoisasuitabledatasetforthisduetoitsrelativelylargeamountofbothdataandnon-projectivity.Figure2showsthelearningcurveforpseudo-projectiveparsing(P-Proj),comparedtousingonlyprojectivizedtrainingdata(Proj),measuredaserrorreductioninrelationtotheoriginalnon-projectivetrainingdata(N-Proj).Theexperimentwasper-formedbyincrementallyaddingcross-validationfolds1–8tothetrainingset,usingfolds9–0asstatictestdata.OnecannotethattheerrorreductionforProjisunaffectedbytheamountofdata.Whiletheerrorreductionvariesslightly,itturnsoutthattheerrorreductionisvirtuallythesamefor10%ofthetrain-ingdataasfor80%.Thatis,thereisnocorrela-tionifinformationconcerningtheliftsarenotaddedtothelabels.However,withapseudo-projectivetransformation,whichactivelytriestorecovernon-projectivity,thelearningcurveclearlyindicatesthattheamountofdatamatters.Alpino,with36%non-projectivesentences,startsatabout17%andhasaclimbingcurveuptoalmost25%.Althoughthisexperimentshowsthatthereisacorrelationbetweentheamountofdataandtheaccu-racyforpseudo-projectiveparsing,itdoesprobablynottellthewholestory.Ifitdid,onewouldexpectthattheerrorreductionforthepseudo-projectivetransformationwouldbemuchclosertoProjwhenNoneCoordVGSDT77.2779.33∗∗77.92∗∗PADT76.9679.05∗∗-Alpino82.7583.38∗∗-PDT83.4185.51∗∗83.58∗∗Table4:ASUforcoordinationandverbgrouptrans-formationswithMaltParser(None=N-Proj).Mc-Nemar’stest:∗∗=p<.01comparedtoNone.theamountofdataislow(totheleftintheﬁg-ure)thantheyapparentlyare.Ofcourse,thedif-ferenceislikelytodiminishwithevenlessdata,butitshouldbenotedthat10%ofAlpinohasabouthalfthesizeofPADT,forwhichthepositiveimpactofpseudo-projectiveparsingisabsent.TheabsenceofincreasedaccuracyforSDTcanpartiallybeex-plainedbythehighershareofnon-projectivearcsinAlpino(3timesmore).CoordinationandVerbGroupsThecorrespondingparsingresultsusingMaltParserwithtransformationsforcoordinationandverbgroupsareshownintable4.ForSDT,PADTandPDT,theannotationofcoordinationhasbeentrans-formedfromPStoMS,asdescribedinNilssonetal.(2006).ForAlpino,thetransformationisfromPStoCS(cf.section2.2),whichwasfoundtogiveslightlybetterperformanceinpreliminaryexperi-ments.Thebaselinewithnotransformation(None)isthesameasN-Projintable2.Astheﬁguresindicate,transformingcoordinationisbeneﬁcialnotonlyforPDT,asreportedbyNilssonetal.(2006),butalsoforSDT,PADT,andAlpino.ItisinterestingtonotethatSDT,PADTandPDT,withcomparableamountsofconjuncts,havecompara-bleincreasesinaccuracy(about2percentagepointseach),despitethelargedifferencesintrainingsetsize.ItisthereforenotsurprisingthatAlpino,withamuchsmalleramountofconjuncts,hasalowerin-creaseinaccuracy.Takentogether,theseresultsin-dicatethatthefrequencyoftheconstructionismoreimportantthanthesizeofthetrainingsetforthistypeoftransformation.Thesamegeneralizationovertreebanksholdsforverbgroupstoo.Thelastcolumnintable4showsthattheexpectedincreaseinaccuracyforPDTisac-974

AlgorithmN-ProjProjP-ProjEisner81.7983.2386.45CLE86.39Table5:Pseudo-projectiveparsingresults(ASU)forAlpinowithMSTParser.companiedbyaevenhigherincreaseforSDT.ThiscanprobablybeattributedtothehigherfrequencyofauxiliaryverbsinSDT.5.2ComparingParsersThemainquestioninthissectionistowhatextentthepositiveeffectofdifferenttreetransformationsisdependentonparsingstrategy,sinceallprevi-ousexperimentshavebeenperformedwithasingleparser(MaltParser).Forcomparisonwehaveper-formedtwoexperimentswithMSTParser,version0.1,whichisbasedonaverydifferentparsingmeth-dology(cf.section4).Duetosometechnicaldif-ﬁculties(notablytheveryhighmemoryconsump-tionwhenusingMSTParserforlabeleddependencyparsing),wehavenotbeenabletoreplicatetheex-perimentsfromtheprecedingsectionexactly.Theresultspresentedbelowmustthereforeberegardedasapreliminaryexplorationofthedependenciesbe-tweentreetransformationsandparsingstrategy.Table5presentsASUresultsforMSTParserincombinationwithpseudo-projectiveparsingappliedtotheAlpinotreebankofDutch.3TheﬁrstrowcontainstheresultforEisner’salgorithmusingnotransformation(N-Proj),projectivizedtrainingdata(Proj),andpseudo-projectiveparsing(P-Proj).TheﬁguresshowapatternverysimilartothatforMalt-Parser,withaboostinaccuracyforProjcomparedtoN-Proj,andwithasigniﬁcantlyhigheraccuracyforP-ProjoverProj.ItisalsoworthnotingthattheerrorreductionbetweenN-ProjandP-Projisactu-allyhigherforMSTParserherethanforMaltParserintable2.ThesecondrowcontainstheresultfortheChu-Liu-Edmondsalgorithm(CLE),whichconstructsnon-projectivestructuresdirectlyandthereforedoes3Theﬁguresarenotcompletelycomparabletotheprevi-ouslypresentedDutchresultsforMaltParser,sinceMaltParser’sfeaturemodelhasaccesstoalltheinformationintheCoNLLdataformat,whereasMSTParserinthisexperimentonlycouldhandlewordformsandpart-of-speechtags.Trans.NoneCoordVGASU84.583.584.5Table6:Coordinationandverbgrouptransforma-tionsforPDTwiththeCLEalgorithm.DevEvalNivMcDSDTASU80.4082.0178.7283.17ASL71.0672.4470.3073.44PADTASU78.9778.5677.5279.34ASL67.6367.5866.7166.91AlpinoASU87.6382.8581.3583.57ASL84.0279.7378.5979.19PDTASU85.7285.9884.8087.30ASL78.5678.8078.4280.18Table7:EvaluationonCoNLL-Xtestdata;Malt-Parserwithalltransformations(Dev=development,Eval=CoNLLtestset,Niv=Nivreetal.(2006),McD=McDonaldetal.(2006))notrequirethepseudo-projectivetransformation.AcomparisonbetweenEisner’salgorithmwithpseudo-projectivetransformationandCLErevealsthatpseudo-projectiveparsingisatleastasaccurateasnon-projectiveparsingforASU.(Thesmalldif-ferenceisnotstatisticallysigniﬁcant.)Bycontrast,nopositiveeffectcouldbedetectedforthecoordinationandverbgrouptransformationstogtherwithMSTParser.Theﬁguresintable6arenotbasedonCoNLLdata,butinsteadontheevalu-ationtestsetoftheoriginalPDT1.0,whichenablesadirectcomparisontoMcDonaldet.al.(2005)(theNonecolumn).Weseethatthereisevenanegativeeffectforthecoordinationtransformation.Thesere-sultsclearlyindicatethattheeffectofthesetransfor-mationsisatleastpartlydependentonparsingstrat-egy,incontrasttowhatwasfoundforthepseudo-projectiveparsingtechnique.5.3CombiningTransformationsInordertoassessthecombinedeffectofallthreetransformationsinrelationtothestateoftheart,weperformedaﬁnalevaluationusingMaltParseronthededicatedtestsetsfromtheCoNLL-Xsharedtask.Table7givestheresultsforbothdevelop-ment(cross-validationforSDT,PADT,andAlpino;975

developmentsetforPDT)andﬁnaltest,comparedtothetwotopperformingsystemsinthesharedtask,MSTParserwithapproximatesecond-ordernon-projectiveparsing(McDonaldetal.,2006)andMaltParserwithpseudo-projectiveparsing(butnocoordinationorverbgrouptransformations)(Nivreetal.,2006).Lookingatthelabeledattachmentscore(ASL),theofﬁcialscoringmetricoftheCoNLL-Xsharedtask,weseethatthecombinedef-fectofthethreetransformationsbooststheperfor-manceofMaltParserforalltreebanksandintwocasesoutoffouroutperformsMSTParser(whichwasthetopscoringsystemforallfourtreebanks).6ConclusionInthispaper,wehaveexaminedthegeneralityoftreetransformationsfordata-drivendependencyparsing.Theresultsindicatethatthepseudo-projectiveparsingtechniquehasapositiveeffectonparsingaccuracythatisindependentofparsingmethodologybutsensitivetotheamountoftrainingdataaswellastothecomplexityofnon-projectiveconstructions.Bycontrast,theconstruction-speciﬁctransformationstargetingcoordinationandverbgroupsappeartohaveamorelanguage-independenteffect(forlanguagestowhichtheyareapplicable)butdonothelpforallparsers.Moreresearchisneededinordertoknowexactlywhatthedependen-ciesarebetweenparsingstrategyandtreetransfor-mations.Regardlessofthis,however,itissafetoconcludethatpre-processingandpost-processingisimportantnotonlyinconstituency-basedparsing,aspreviouslyshowninanumberofstudies,butalsoforinductivedependencyparsing.ReferencesD.Bikel.2004.IntricaciesofCollins’parsingmodel.Compu-tationalLinguistics,30:479–511.S.BuchholzandE.Marsi.2006.CoNLL-XSharedTaskonMultilingualDependencyParsing.InProceedingsofCoNLL,pages1–17.R.Campbell.2004.UsingLinguisticPrinciplestoRecoverEmptyCategories.InProceedingsofACL,pages645–652.E.Charniak.2000.AMaximum-Entropy-InspiredParser.InProceedingsofNAACL,pages132–139.M.Collins,J.Hajiˇc,L.Ramshaw,andC.Tillmann.1999.AstatisticalparserforCzech.InProceedingsofACL,pages100–110.M.Collins.1999.Head-DrivenStatisticalModelsforNaturalLanguageParsing.Ph.D.thesis,UniversityofPennsylvania.S.Dˇzeroski,T.Erjavec,N.Ledinek,P.Pajas,Z.ˇZabokrtsky,andA.ˇZele.2006.TowardsaSloveneDependencyTreebank.InLREC.J.Hajiˇc,B.V.Hladka,J.Panevov´a,EvaHajiˇcov´a,PetrSgall,andPetrPajas.2001.PragueDependencyTreebank1.0.LDC,2001T10.J.Hajiˇc,O.Smrˇz,P.Zem´anek,J.ˇSnaidauf,andE.Beˇska.2004.PragueArabicDependencyTreebank:DevelopmentinDataandTools.InNEMLAR,pages110–117.K.HallandV.Nov´ak.2005.Correctivemodelingfornon-projectivedependencyparsing.InProceedingsofIWPT,pages42–52.M.Johnson.1998.PCFGModelsofLinguisticTreeRepresen-tations.ComputationalLinguistics,24:613–632.S.Kahane,A.Nasr,andO.Rambow.1998.Pseudo-Projectivity:APolynomiallyParsableNon-ProjectiveDe-pendencyGrammar.InProceedingsofCOLING/ACL,pages646–652.D.KleinandC.Manning.2003.Accurateunlexicalizedpars-ing.InProceedingsofACL,pages423–430.R.McDonaldandF.Pereira.2006.OnlineLearningofAp-proximateDependencyParsingAlgorithms.InProceedingsofEACL,pages81–88.R.McDonald,F.Pereira,K.Ribarov,andJ.Hajiˇc.2005.Non-projectivedependencyparsingusingspanningtreeal-gorithms.InProceedingsofHLT/EMNLP,pages523–530.R.McDonald,K.Lerman,andF.Pereira.2006.Multilingualdependencyanalysiswithatwo-stagediscriminativeparser.InProceedingsofCoNLL,pages216–220.I.Mel’ˇcuk.1988.DependencySyntax:TheoryandPractice.StateUniversityofNewYorkPress.J.Nilsson,J.Nivre,andJ.Hall.2006.GraphTransforma-tionsinData-DrivenDependencyParsing.InProceedingsofCOLING/ACL,pages257–264.J.NivreandJ.Nilsson.2005.Pseudo-ProjectiveDependencyParsing.InProceedingsofACL,pages99–106.J.Nivre,J.Hall,andJ.Nilsson.2004.Memory-basedDepen-dencyParsing.InH.T.NgandE.Riloff,editors,Proceed-ingsofCoNLL,pages49–56.J.Nivre,J.Hall,J.Nilsson,G.Eryiˇgit,andS.Marinov.2006.LabeledPseudo-ProjectiveDependencyParsingwithSup-portVectorMachines.InProceedingsofCoNLL,pages221–225.S.Petrov,L.Barrett,R.Thibaux,andD.Klein.2006.LearningAccurate,Compact,andInterpretableTreeAnnotation.InProceedingsofCOLING/ACL,pages433–440.L.vanderBeek,G.Bouma,R.Malouf,andG.vanNoord.2002.TheAlpinodependencytreebank.InComputationalLinguisticsintheNetherlands(CLIN).Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 976–983,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

976

LearningMultilingualSubjectiveLanguageviaCross-LingualProjectionsRadaMihalceaandCarmenBaneaDepartmentofComputerScienceUniversityofNorthTexasrada@cs.unt.edu,carmenb@unt.eduJanyceWiebeDepartmentofComputerScienceUniversityofPittsburghwiebe@cs.pitt.eduAbstractThispaperexploresmethodsforgeneratingsubjectivityanalysisresourcesinanewlan-guagebyleveragingonthetoolsandre-sourcesavailableinEnglish.GivenabridgebetweenEnglishandtheselectedtargetlan-guage(e.g.,abilingualdictionaryorapar-allelcorpus),themethodscanbeusedtorapidlycreatetoolsforsubjectivityanalysisinthenewlanguage.1IntroductionThereisgrowinginterestintheautomaticextractionofopinions,emotions,andsentimentsintext(sub-jectivity),toprovidetoolsandsupportforvariousnaturallanguageprocessingapplications.MostoftheresearchtodatehasfocusedonEnglish,whichismainlyexplainedbytheavailabilityofresourcesforsubjectivityanalysis,suchaslexiconsandman-uallylabeledcorpora.Inthispaper,weinvestigatemethodstoauto-maticallygenerateresourcesforsubjectivityanaly-sisforanewtargetlanguagebyleveragingontheresourcesandtoolsavailableforEnglish,whichinmanycasestookyearsofworktocomplete.Specif-ically,throughexperimentswithcross-lingualpro-jectionofsubjectivity,weseekanswerstothefol-lowingquestions.First,canwederiveasubjectivitylexiconforanewlanguageusinganexistingEnglishsubjectivitylexiconandabilingualdictionary?Second,canwederivesubjectivity-annotatedcorporainanewlan-guageusingexistingsubjectivityanalysistoolsforEnglishandaparallelcorpus?Finally,third,canwebuildtoolsforsubjectivityanalysisforanewtargetlanguagebyrelyingontheseautomaticallygener-atedresources?WefocusourexperimentsonRomanian,selectedasarepresentativeofthelargenumberoflanguagesthathaveonlylimitedtextprocessingresourcesde-velopedtodate.Notethat,althoughweworkwithRomanian,themethodsdescribedareapplicabletoanyotherlanguage,asintheseexperimentswe(pur-posely)donotuseanylanguage-speciﬁcknowledgeofthetargetlanguage.GivenabridgebetweenEn-glishandtheselectedtargetlanguage(e.g.,abilin-gualdictionaryoraparallelcorpus),themethodscanbeappliedtootherlanguagesaswell.Afterprovidingmotivations,wepresenttwoap-proachestodevelopingsentence-levelsubjectivityclassiﬁersforanewtargetlanguage.TheﬁrstusesasubjectivitylexicontranslatedfromanEnglishone.ThesecondusesanEnglishsubjectivityclassiﬁerandaparallelcorpustocreatetarget-languagetrain-ingdatafordevelopingastatisticalclassiﬁer.2MotivationAutomaticsubjectivityanalysismethodshavebeenusedinawidevarietyoftextprocessingapplica-tions,suchastrackingsentimenttimelinesinon-lineforumsandnews(Lloydetal.,2005;Balogetal.,2006),reviewclassiﬁcation(Turney,2002;Pangetal.,2002),miningopinionsfromproductreviews(HuandLiu,2004),automaticexpressivetext-to-speechsynthesis(Almetal.,2005),textse-manticanalysis(WiebeandMihalcea,2006;EsuliandSebastiani,2006),andquestionanswering(YuandHatzivassiloglou,2003).977

Whilemuchrecentworkinsubjectivityanalysisfocusesonsentiment(atypeofsubjectivity,namelypositiveandnegativeemotions,evaluations,andjudgments),weopttofocusonrecognizingsubjec-tivityingeneral,fortworeasons.First,evenwhensentimentisthedesiredfocus,researchersinsentimentanalysishaveshownthatatwo-stageapproachisoftenbeneﬁcial,inwhichsubjectiveinstancesaredistinguishedfromobjec-tiveones,andthenthesubjectiveinstancesarefur-therclassiﬁedaccordingtopolarity(YuandHatzi-vassiloglou,2003;PangandLee,2004;Wilsonetal.,2005;KimandHovy,2006).Infact,theprob-lemofdistinguishingsubjectiveversusobjectivein-stanceshasoftenprovedtobemoredifﬁcultthansubsequentpolarityclassiﬁcation,soimprovementsinsubjectivityclassiﬁcationpromisetopositivelyimpactsentimentclassiﬁcation.Thisisreportedinstudiesofmanualannotationofphrases(Takamuraetal.,2006),recognizingcontextualpolarityofex-pressions(Wilsonetal.,2005),andsentimenttag-gingofwordsandwordsenses(AndreevskaiaandBergler,2006;EsuliandSebastiani,2006).Second,anNLPapplicationmayseekawiderangeoftypesofsubjectivityattributedtoaper-son,suchastheirmotivations,thoughts,andspecu-lations,inadditiontotheirpositiveandnegativesen-timents.Forinstance,theopiniontrackingsystemLydia(Lloydetal.,2005)givesseparateratingsforsubjectivityandsentiment.Thesecanbedetectedwithsubjectivityanalysisbutnotbyamethodfo-cusedonlyonsentiment.Thereisworld-wideinterestintextanalysisappli-cations.Whileworkonsubjectivityanalysisinotherlanguagesisgrowing(e.g.,Japanesedataareusedin(Takamuraetal.,2006;KanayamaandNasukawa,2006),Chinesedataareusedin(Huetal.,2005),andGermandataareusedin(KimandHovy,2006)),muchoftheworkinsubjectivityanalysishasbeenappliedtoEnglishdata.Creatingcorporaandlexicalresourcesforanewlanguageisverytimeconsum-ing.Ingeneral,wewouldliketoleverageresourcesalreadydevelopedforonelanguagetomorerapidlycreatesubjectivityanalysistoolsforanewone.Thismotivatesourexplorationanduseofcross-linguallexicontranslationsandannotationprojections.Mostifnotallworkonsubjectivityanalysishasbeencarriedoutinamonolingualframework.Wearenotawareofmulti-lingualworkinsubjectivityanalysissuchasthatproposedhere,inwhichsubjec-tivityanalysisresourcesdevelopedforonelanguageareusedtosupportdevelopingresourcesinanother.3ALexicon-BasedApproachManysubjectivityandsentimentanalysistoolsrelyonmanuallyorsemi-automaticallyconstructedlex-icons(YuandHatzivassiloglou,2003;RiloffandWiebe,2003;KimandHovy,2006).Giventhesuc-cessofsuchtechniques,theﬁrstapproachwetaketogeneratingatarget-languagesubjectivityclassi-ﬁeristocreateasubjectivitylexiconbytranslatinganexistingsourcelanguagelexicon,andthenbuildaclassiﬁerthatreliesontheresultinglexicon.Below,wedescribethetranslationprocessanddiscusstheresultsofanannotationstudytoassessthequalityofthetranslatedlexicon.Wethende-scribeandevaluatealexicon-basedtarget-languageclassiﬁer.3.1TranslatingaSubjectivityLexiconThesubjectivitylexiconweuseisfromOpinion-Finder(WiebeandRiloff,2005),anEnglishsub-jectivityanalysissystemwhich,amongotherthings,classiﬁessentencesassubjectiveorobjective.Thelexiconwascompiledfrommanuallydevelopedre-sourcesaugmentedwithentrieslearnedfromcor-pora.Itcontains6,856uniqueentries,outofwhich990aremulti-wordexpressions.Theentriesinthelexiconhavebeenlabeledforpartofspeech,andforreliability–thosethatappearmostofteninsubjec-tivecontextsarestrongcluesofsubjectivity,whilethosethatappearlessoften,butstillmoreoftenthanexpectedbychance,arelabeledweak.Toperformthetranslation,weusetwobilingualdictionaries.TheﬁrstisanauthoritativeEnglish-Romaniandictionary,consistingof41,500entries,1whichweuseasthemaintranslationresourceforthelexicontranslation.Theseconddictionary,drawnfromtheUniversalDictionarydownloadsite(UDP,2007)consistsof4,500entrieswrittenlargelybyWebvolunteercontributors,andthusisnoterrorfree.Weusethisdictionaryonlyforthoseentriesthatdonotappearinthemaindictionary.1UniqueEnglishentries,eachwithmultipleRomaniantranslations.978

Therewereseveralchallengesencounteredinthetranslationprocess.First,althoughtheEnglishsub-jectivitylexiconcontainsinﬂectedwords,wemustusethelemmatizedforminordertobeabletotrans-latetheentriesusingthebilingualdictionary.How-ever,wordsmaylosetheirsubjectivemeaningoncelemmatized.Forinstance,theinﬂectedformofmemoriesbecomesmemory.OncetranslatedintoRomanian(asmemorie),itsmainmeaningisob-jective,referringtothepowerofretaininginforma-tionasinIronsupplementsmayimproveawoman’smemory.Second,neitherthelexiconnorthebilingualdic-tionaryprovidesinformationonthesenseofthein-dividualentries,andthereforethetranslationhastorelyonthemostprobablesenseinthetargetlan-guage.Fortunately,thebilingualdictionaryliststhetranslationsinreverseorderoftheirusagefrequen-cies.Nonetheless,theambiguityofthewordsandthetranslationsstillseemstorepresentanimpor-tantsourceoferror.Moreover,thelexiconsome-timesincludesidenticalentriesexpressedthroughdifferentpartsofspeech,e.g.,grudgehastwosepa-rateentries,foritsnounandverbroles,respectively.Ontheotherhand,thebilingualdictionarydoesnotmakethisdistinction,andthereforewehaveagaintorelyonthe“mostfrequent”heuristiccapturedbythetranslationorderinthebilingualdictionary.Finally,thelexiconincludesasigniﬁcantnumber(990)ofmulti-wordexpressionsthatposetransla-tiondifﬁculties,sometimesbecausetheirmeaningisidiomatic,andsometimesbecausethemulti-wordexpressionisnotlistedinthebilingualdictionaryandthetranslationoftheentirephraseisdifﬁculttoreconstructfromthetranslationsoftheindividualwords.Toaddressthisproblem,whenatranslationisnotfoundinthedictionary,wecreateoneusingaword-by-wordapproach.ThesetranslationsarethenvalidatedbyenforcingthattheyoccuratleastthreetimesontheWeb,usingcountscollectedfromtheAltaVistasearchengine.Themulti-wordexpres-sionsthatarenotvalidatedinthisprocessaredis-carded,reducingthenumberofexpressionsfromaninitialsetof990toaﬁnalsetof264.TheﬁnalsubjectivitylexiconinRomaniancon-tains4,983entries.Table1showsexamplesofen-triesintheRomanianlexicon,togetherwiththeircorrespondingoriginalEnglishform.ThetableRomanianEnglishattributesˆınfrumuset¸abeautifyingstrong,verbnotabilnotableweak,adjplinderegretfullofregretsstrong,adjsclavslavesweak,nounTable1:ExamplesofentriesintheRomaniansub-jectivitylexiconalsoshowsthereliabilityoftheexpression(weakorstrong)andthepartofspeech–attributesthatareprovidedintheEnglishsubjectivitylexicon.ManualEvaluation.Wewanttoassessthequalityofthetranslatedlexi-con,andcompareittothequalityoftheoriginalEn-glishlexicon.TheEnglishsubjectivitylexiconwasevaluatedin(WiebeandRiloff,2005)againstacor-pusofEnglish-languagenewsarticlesmanuallyan-notatedforsubjectivity(theMPQAcorpus(Wiebeetal.,2005)).Accordingtothisevaluation,85%oftheinstancesofthecluesmarkedasstrongand71.5%ofthecluesmarkedasweakareinsubjectivesentencesintheMPQAcorpus.SincethereisnocomparableRomaniancorpus,analternatewaytojudgethesubjectivityofaRo-manianlexiconentryisneeded.TwonativespeakersofRomanianannotatedthesubjectivityof150randomlyselectedentries.Eachannotatorindependentlyreadapproximately100ex-amplesofeachdrawnfromtheWeb,includingalargenumberfromnewssources.Thesubjectivityofawordwasconsequentlyjudgedinthecontextswhereitmostfrequentlyappears,accountingforitsmostfrequentmeaningsontheWeb.ThetagsetusedfortheannotationsconsistsofS(ubjective),O(bjective),andB(oth).AW(rong)la-belisalsousedtoindicateawrongtranslation.Table2showsthecontingencytableforthetwoannota-tors’judgmentsonthisdata.SOBWTotalS5369068O1271029B5318026W0002727Total59362827150Table2:Agreementon150entriesintheRomanianlexiconWithoutcountingthewrongtranslations,theagreementismeasuredat0.80,withaKappaκ=979

0.70,whichindicatesconsistentagreement.Afterthedisagreementswerereconciledthroughdiscus-sions,theﬁnalsetof123correctlytranslatedentriesdoesinclude49.6%(61)subjectiveentries,butfully23.6%(29)werefoundinthestudytohaveprimar-ilyobjectiveuses(theother26.8%aremixed).Thus,thisstudysuggeststhattheRomaniansub-jectivitycluesderivedthroughtranslationarelessre-liablethantheoriginalsetofEnglishclues.Insev-eralcases,thesubjectivityislostinthetranslation,mainlyduetowordambiguityineitherthesourceortargetlanguage,orboth.Forinstance,thewordfragilecorrectlytranslatesintoRomanianasfragil,yetthiswordisfrequentlyusedtorefertobreakableobjects,anditlosesitssubjectivemeaningofdel-icate.Otherwords,suchasone-sided,completelylosesubjectivityoncetranslated,asitbecomesinRomaniancuosinguralatur˘a,meaningwithonlyoneside(asofobjects).Interestingly,thereliabilityofcluesintheEnglishlexiconseemstohelppreservesubjectivity.Outofthe77entriesmarkedasstrong,11werejudgedtobeobjectiveinRomanian(14.3%),comparedto14ob-jectiveRomanianentriesobtainedfromthe36weakEnglishclues(39.0%).3.2Rule-basedSubjectivityClassiﬁerUsingaSubjectivityLexiconStartingwiththeRomanianlexicon,wedevelopedalexicalclassiﬁersimilartotheoneintroducedby(RiloffandWiebe,2003).Atthecoreofthismethodisahigh-precisionsubjectivityandobjectivityclas-siﬁerthatcanlabellargeamountsofrawtextusingonlyasubjectivitylexicon.Theirmethodisfurtherimprovedwithabootstrappingprocessthatlearnsextractionpatterns.Inourexperiments,however,weapplyonlytherule-basedclassiﬁcationstep,sincetheextractionstepcannotbeimplementedwithouttoolsforsyntacticparsingandinformationextrac-tionnotavailableinRomanian.Theclassiﬁerreliesonthreemainheuristicstola-belsubjectiveandobjectivesentences:(1)iftwoormorestrongsubjectiveexpressionsoccurinthesamesentence,thesentenceislabeledSubjective;(2)ifnostrongsubjectiveexpressionsoccurinasentence,andatmosttwoweaksubjectiveexpres-sionsoccurintheprevious,current,andnextsen-tencecombined,thenthesentenceislabeledObjec-tive;(3)otherwise,ifnoneofthepreviousrulesap-ply,thesentenceislabeledUnknown.ThequalityoftheclassiﬁerwasevaluatedonaRomaniangold-standardcorpusannotatedforsub-jectivity.TwonativeRomanianspeakers(Ro1andRo2)manuallyannotatedthesubjectivityofthesen-tencesofﬁverandomlyselecteddocuments(504sentences)fromtheRomaniansideofanEnglish-Romanianparallelcorpus,accordingtotheanno-tationschemein(Wiebeetal.,2005).Agreementbetweenannotatorswasmeasured,andthentheirdifferenceswereadjudicated.Thebaselineonthisdatasetis54.16%,whichcanbeobtainedbyas-signingadefaultSubjectivelabeltoallsentences.(MoreinformationaboutthecorpusandannotationsaregiveninSection4below,whereagreementbe-tweenEnglishandRomanianalignedsentencesisalsoassessed.)Asmentionedearlier,duetothelexiconprojec-tionprocessthatisperformedviaabilingualdictio-nary,theentriesinourRomaniansubjectivitylex-iconareinalemmatizedform.Consequently,wealsolemmatizethegold-standardcorpus,toallowfortheidentiﬁcationofmatcheswiththelexicon.Forthispurpose,weusetheRomanianlemmatizerdevelopedbyIonandTuﬁs¸(Ion,2007),whichhasanestimatedaccuracyof98%.2Table3showstheresultsoftherule-basedclassi-ﬁer.Weshowtheprecision,recall,andF-measureindependentlymeasuredforthesubjective,objec-tive,andallsentences.Wealsoevaluatedavari-ationoftherule-basedclassiﬁerthatlabelsasen-tenceasobjectiveifthereareatmostthreeweakex-pressionsintheprevious,current,andnextsentencecombined,whichraisestherecalloftheobjectiveclassiﬁer.Ourattemptstoincreasetherecallofthesubjectiveclassiﬁerallresultedinsigniﬁcantlossinprecision,andthuswekepttheoriginalheuristic.InitsoriginalEnglishimplementation,thissys-temwasproposedasbeinghigh-precisionbutlowcoverage.EvaluatedontheMPQAcorpus,ithassubjectiveprecisionof90.4,subjectiverecallof34.2,objectiveprecisionof82.4,andobjectivere-callof30.7;overall,precisionis86.7andrecallis32.6(WiebeandRiloff,2005).Weseeasimilarbe-havioronRomanianforsubjectivesentences.Thesubjectiveprecisionisgood,albeitatthecostoflow2DanTuﬁs¸,personalcommunication.980

MeasureSubjectiveObjectiveAllsubj=atleasttwostrong;obj=atmosttwoweakPrecision80.0056.5062.59Recall20.5148.9133.53F-measure32.6452.5243.66subj=atleasttwostrong;obj=atmostthreeweakPrecision80.0056.8561.94Recall20.5161.0339.08F-measure32.6458.8647.93Table3:Evaluationoftherule-basedclassiﬁerrecall,andthustheclassiﬁercouldbeusedtohar-vestsubjectivesentencesfromunlabeledRomaniandata(e.g.,forasubsequentbootstrappingprocess).Thesystemisnotveryeffectiveforobjectiveclassi-ﬁcation,however.Recallthattheobjectiveclassiﬁerreliesontheweaksubjectivityclues,forwhichthetransferofsubjectivityinthetranslationprocesswasparticularlylow.4ACorpus-BasedApproachGiventhelownumberofsubjectiveentriesfoundintheautomaticallygeneratedlexiconandthesubse-quentlowrecallofthelexicalclassiﬁer,wedecidedtoalsoexploreasecond,corpus-basedapproach.Thisapproachbuildsasubjectivity-annotatedcor-pusforthetargetlanguagethroughprojection,andthentrainsastatisticalclassiﬁerontheresultingcorpus(numerousstatisticalclassiﬁershavebeentrainedforsubjectivityorsentimentclassiﬁcation,e.g.,(Pangetal.,2002;YuandHatzivassiloglou,2003)).Thehypothesisisthatwecaneliminatesomeoftheambiguities(andconsequentlossofsub-jectivity)observedduringthelexicontranslationbyaccountingforthecontextoftheambiguouswords,whichispossibleinacorpus-basedapproach.Ad-ditionally,wealsohopetoimprovetherecalloftheclassiﬁer,byaddressingthosecasesnotcoveredbythelexicon-basedapproach.Intheexperimentsreportedinthissection,weuseaparallelcorpusconsistingof107documentsfromtheSemCorcorpus(Milleretal.,1993)andtheirmanualtranslationsintoRomanian.3Thecor-pusconsistsofroughly11,000sentences,withap-proximately250,000tokensoneachside.Itisabal-ancedcorpuscoveringanumberoftopicsinsports,politics,fashion,education,andothers.3ThetranslationwascarriedoutbyaRomaniannativespeaker,studentinadepartmentof“ForeignLanguagesandTranslations”inRomania.Below,webeginwithamanualannotationstudytoassessthequalityofannotationandpreservationofsubjectivityintranslation.Wethendescribetheautomaticconstructionofatarget-languagetrainingset,andevaluateaclassiﬁertrainedonthatdata.AnnotationStudy.Westartbyperforminganagreementstudymeanttodeterminetheextenttowhichsubjectivityispre-servedbythecross-lingualprojections.Inthestudy,threeannotators–onenativeEnglishspeaker(En)andtwonativeRomanianspeakers(Ro1andRo2)–ﬁrsttrainedon3randomlyselecteddocuments(331sentences).Theythenindependentlyannotatedthesubjectivityofthesentencesoftworandomlyse-lecteddocumentsfromtheparallelcorpus,account-ingfor173alignedsentencepairs.Theannotatorshadaccessexclusivelytotheversionofthesen-tencesintheirlanguage,toavoidanybiasthatcouldbeintroducedbyseeingthetranslationintheotherlanguage.NotethattheRomanianannotations(afteralldif-ferencesbetweentheRomanianannotatorsweread-judicated)ofall331+173sentencesmakeupthegoldstandardcorpususedintheexperimentsre-portedinSections3.2and4.1.Beforepresentingtheresultsoftheannotationstudy,wegivesomeexamples.ThefollowingareEnglishsubjectivesentencesandtheirRomaniantranslations(thesubjectiveelementsareshowninbold).[en]ThedesiretogiveBroglioasmanystartsaspossible.[ro]Dorint¸adea-idaluiBrogliocˆatmaimultestarturiposibile.[en]SupposehedidliebesideLenin,woulditbepermanent?[ro]S˘apresupunemc˘aarﬁas¸ezatal˘aturideLenin,oarevaﬁpentrutotdeauna?Thefollowingareexamplesofobjectiveparallelsentences.[en]ThePirateshavea9-6recordthisyearandtheRedbirdsare7-9.[ro]Pirat¸iiauunpalmaresde9la6anulacestasiP˘as˘arileRos¸iiau7la9.[en]Oneoftheobstaclestotheeasycontrolofa2-yearoldchildisalackofverbalcommunication.[ro]Unuldintreobstacoleleˆıncontrolareaunuicopilde2aniestelipsacomunic˘ariiverbale.981

TheannotatorsweretrainedusingtheMPQAannotationguidelines(Wiebeetal.,2005).ThetagsetconsistsofS(ubjective),O(bjective)andU(ncertain).FortheUtags,aclasswasalsogiven;OUmeans,forinstance,thattheannotatorisuncer-tainbutsheisleaningtowardO.Table4showsthepairwiseagreementﬁguresandtheKappa(κ)calcu-latedforthethreeannotators.Thetablealsoshowstheagreementwhentheborderlineuncertaincasesareremoved.allsentencesUncertainremovedpairagreeκagreeκ(%)removedRo1&Ro20.830.670.890.7723En&Ro10.770.540.860.7326En&Ro20.780.550.910.8220Table4:Agreementonthedatasetof173sentences.Annotationsperformedbythreeannotators:onena-tiveEnglishspeaker(En)andtwonativeRomanianspeakers(Ro1andRo2)Whenallthesentencesareincluded,theagree-mentbetweenthetwoRomanianannotatorsismea-suredat0.83(κ=0.67).Ifweremovetheborder-linecaseswhereatleastoneannotator’stagisUn-certain,theagreementrisesto0.89withκ=0.77.Theseﬁguresaresomewhatlowerthantheagree-mentobservedduringprevioussubjectivityanno-tationstudiesconductedonEnglish(Wiebeetal.,2005)(theannotatorsweremoreextensivelytrainedinthosestudies),buttheynonethelessindicatecon-sistentagreement.Interestingly,whentheagreementisconductedcross-linguallybetweenanEnglishandaRomanianannotator,theagreementﬁgures,althoughsome-whatlower,arecomparable.Infact,oncetheUncertaintagsareremoved,themonolingualandcross-lingualagreementandκvaluesbecomeal-mostequal,whichsuggeststhatinmostcasesthesentence-levelsubjectivityispreserved.ThedisagreementswerereconciledﬁrstbetweenthelabelsassignedbythetwoRomanianannotators,followedbyareconciliationbetweentheresultingRomanian“gold-standard”labelsandthelabelsas-signedbytheEnglishannotator.Inmostcases,thedisagreementacrossthetwolanguageswasfoundtobeduetoadifferenceofopinionaboutthesen-tencesubjectivity,similartothedifferencesencoun-teredinmonolingualannotations.However,therearecaseswherethedifferencesareduetothesub-jectivitybeinglostinthetranslation.Sometimes,thisisduetoseveralpossibleinterpretationsforthetranslatedsentence.Forinstance,thefollowingsen-tence:[en]TheyhonoredthebattlingBillikenslastnight.[ro]Eii-aucelebratpeBillikenssearatrecut˘a.ismarkedasSubjectiveinEnglish(incontext,theEnglishannotatorinterpretedhonoredasreferringtopraisesoftheBillikens).However,theRomaniantranslationofhonorediscelebratwhich,whilecor-rectasatranslation,hasthemorefrequentinterpre-tationofhavingaparty.ThetwoRomanianannota-torschosethisinterpretation,whichcorrespondinglyleadthemtomarkthesentenceasObjective.Inothercases,inparticularwhenthesubjectivityisduetoﬁguresofspeechsuchasirony,thetrans-lationsometimesmissestheironicaspects.Forin-stance,thetranslationofeggheadwasnotperceivedasironicbytheRomanianannotators,andconse-quentlythefollowingsentencelabeledSubjectiveinEnglishisannotatedasObjectiveinRomanian.[en]IhavelivedformanyyearsinaConnecti-cutcommutingtownwithahighpercentageof[...]businessexecutivesofeggheadtastes.[ro]Amtr˘aitmult¸ianiˆıntr-unoras¸dinapropieredeConnecticutceaveaomareproport¸iede[...]oa-menideafacericugusturiintelectuale.4.1TranslatingaSubjectivity-AnnotatedCorpusandCreatingaMachineLearningSubjectivityClassiﬁerTofurthervalidatethecorpus-basedprojectionofsubjectivity,wedevelopedasubjectivityclassiﬁertrainedonRomaniansubjectivity-annotatedcorporaobtainedviacross-lingualprojections.Ideally,onewouldgenerateanannotatedRoma-niancorpusbytranslatingEnglishdocumentsman-uallyannotatedforsubjectivitysuchastheMPQAcorpus.Unfortunately,themanualtranslationofthiscorpuswouldbeprohibitivelyexpensive,bothtime-wiseandﬁnancially.Theotheralternative–auto-maticmachinetranslation–hasnotyetreachedalevelthatwouldenablethegenerationofahigh-qualitytranslatedcorpus.WethereforedecidedtouseadifferentapproachwhereweautomaticallyannotatetheEnglishsideofanexistingEnglish-Romaniancorpus,andsubsequentlyprojectthean-notationsontotheRomaniansideoftheparallelcor-982

PrecisionRecallF-measurehigh-precision86.732.647.4high-coverage79.470.674.7Table5:Precision,recall,andF-measureforthetwoOpinionFinderclassiﬁers,asmeasuredontheMPQAcorpus.pusacrossthesentence-levelalignmentsavailableinthecorpus.Fortheautomaticsubjectivityannotations,wegeneratedtwosetsoftheEnglish-sideannotations,oneusingthehigh-precisionclassiﬁerandoneusingthehigh-coverageclassiﬁeravailableintheOpinion-Findertool.Thehigh-precisionclassiﬁerinOpin-ionFinderusesthecluesofthesubjectivitylexicontoharvestsubjectiveandobjectivesentencesfromalargeamountofunannotatedtext;thisdataisthenusedtoautomaticallyidentifyasetofextractionpat-terns,whicharethenusediterativelytoidentifyalargersetofsubjectiveandobjectivesentences.Inaddition,inOpinionFinder,thehigh-precisionclassiﬁerisusedtoproduceanEnglishlabeleddatasetfortraining,whichisusedtogenerateitsNaiveBayeshigh-coveragesubjectivityclassiﬁer.Table5showstheperformanceofthetwoclassiﬁersontheMPQAcorpusasreportedin(WiebeandRiloff,2005).Notethat55%ofthesentencesintheMPQAcorpusaresubjective–whichrepresentsthebaselineforthisdataset.ThetwoOpinionFinderclassiﬁersareusedtola-belthetrainingcorpus.Afterremovingthe504testsentences,weareleftwith10,628sentencesthatareautomaticallyannotatedforsubjectivity.Table6showsthenumberofsubjectiveandobjectivesen-tencesobtainedwitheachclassiﬁer.ClassiﬁerSubjectiveObjectiveAllhigh-precision1,6292,3343,963high-coverage5,0505,57810,628Table6:SubjectiveandobjectivetrainingsentencesautomaticallyannotatedwithOpinionFinder.Next,theOpinionFinderannotationsarepro-jectedontotheRomaniantrainingsentences,whicharethenusedtodevelopaprobabilisticclassiﬁerfortheautomaticlabelingofsubjectivityinRomaniansentences.Similarto,e.g.,(Pangetal.,2002),weuseaNaiveBayesalgorithmtrainedonwordfeaturesco-occurringwiththesubjectiveandtheobjectiveclas-siﬁcations.Weassumewordindependence,andweusea0.3cut-offforfeatureselection.Whilere-centworkhasalsoconsideredmorecomplexsyn-tacticfeatures,wearenotabletogeneratesuchfea-turesforRomanianastheyrequiretoolscurrentlynotavailableforthislanguage.Wecreatetwoclassiﬁers,onetrainedoneachdataset.Thequalityoftheclassiﬁersisevaluatedonthe504-sentenceRomaniangold-standardcorpusdescribedabove.Recallthatthebaselineonthisdatasetis54.16%,thepercentageofsentencesinthecor-pusthataresubjective.Table7showstheresults.SubjectiveObjectiveAllprojectionsource:OFhigh-precisionclassiﬁerPrecision65.0269.6264.48Recall82.4147.6164.48F-measure72.6856.5464.68projectionsource:OFhigh-coverageclassiﬁerPrecision66.6670.1767.85Recall81.3152.1767.85F-measure72.6856.5467.85Table7:Evaluationofthemachinelearningclassi-ﬁerusingtrainingdataobtainedviaprojectionsfromdataautomaticallylabeledbyOpinionFinder(OF).OurbestclassiﬁerhasanF-measureof67.85,andisobtainedbytrainingonprojectionsfromthehigh-coverageOpinionFinderannotations.Al-thoughsmallerthanthe74.70F-measureobtainedbytheEnglishhigh-coverageclassiﬁer(seeTa-ble5),theresultappearsremarkablegiventhatnolanguage-speciﬁcRomanianinformationwasused.Theoverallresultsobtainedwiththemachinelearningapproachareconsiderablyhigherthanthoseobtainedfromtherule-basedclassiﬁer(exceptfortheprecisionofthesubjectivesentences).Thisismostlikelyduetothelexicontranslationprocess,whichasmentionedintheagreementstudyinSec-tion3.1,leadstoambiguityandlossofsubjectivity.Instead,thecorpus-basedtranslationsseemtobetteraccountfortheambiguityofthewords,andthesub-jectivityisgenerallypreservedinthesentencetrans-lations.5ConclusionsInthispaper,wedescribedtwoapproachestogener-atingresourcesforsubjectivityannotationsforanew983

language,byleveragingonresourcesandtoolsavail-ableforEnglish.Theﬁrstapproachbuildsatargetlanguagesubjectivitylexiconbytranslatinganexist-ingEnglishlexiconusingabilingualdictionary.Thesecondgeneratesasubjectivity-annotatedcorpusinatargetlanguagebyprojectingannotationsfromanautomaticallyannotatedEnglishcorpus.Theseresourceswerevalidatedintwoways.First,wecarriedoutannotationstudiesmeasuringtheextenttowhichsubjectivityispreservedacrosslanguagesineachofthetworesources.Thesestud-iesshowthatonlyarelativelysmallfractionoftheentriesinthelexiconpreservetheirsubjectivityinthetranslation,mainlyduetotheambiguityinboththesourceandthetargetlanguages.Thisiscon-sistentwithobservationsmadeinpreviousworkthatsubjectivityisapropertyassociatednotwithwords,butwithwordmeanings(WiebeandMihal-cea,2006).Incontrast,thesentence-levelsubjectiv-itywasfoundtobemorereliablypreservedacrosslanguages,withcross-lingualinter-annotatoragree-mentscomparabletothemonolingualones.Second,wevalidatedthetwoautomaticallygen-eratedsubjectivityresourcesbyusingthemtobuildatoolforsubjectivityanalysisinthetargetlanguage.Speciﬁcally,wedevelopedtwoclassiﬁers:arule-basedclassiﬁerthatreliesonthesubjectivitylexi-condescribedinSection3.1,andamachinelearn-ingclassiﬁertrainedonthesubjectivity-annotatedcorpusdescribedinSection4.1.Whilethehighestprecisionforthesubjectiveclassiﬁcationisobtainedwiththerule-basedclassiﬁer,theoverallbestresultof67.85F-measureisduetothemachinelearningapproach.Thisresultisconsistentwiththeanno-tationstudies,showingthatthecorpusprojectionspreservesubjectivitymorereliablythanthelexicontranslations.Finally,neitheroneoftheclassiﬁersreliesonlanguage-speciﬁcinformation,butratheronknowl-edgeobtainedthroughprojectionsfromEnglish.Asimilarmethodcanthereforebeusedtoderivetoolsforsubjectivityanalysisinotherlanguages.ReferencesAlinaAndreevskaiaandSabineBergler.Miningwordnetforfuzzysentiment:SentimenttagextractionfromWordNetglosses.InProceedingsofEACL2006.CeciliaOvesdotterAlm,DanRoth,andRichardSproat.2005.Emotionsfromtext:Machinelearningfortext-basedemo-tionprediction.InProceedingsofHLT/EMNLP2005.KrisztianBalog,GiladMishne,andMaartendeRijke.2006.Whyaretheyexcited?identifyingandexplainingspikesinblogmoodlevels.InEACL-2006.AndreaEsuliandFabrizioSebastiani.2006.Determiningtermsubjectivityandtermorientationforopinionmining.InPro-ceedingstheEACL2006.MinqingHuandBingLiu.2004.Miningandsummarizingcustomerreviews.InProceedingsofACMSIGKDD.YiHu,JianyongDuan,XiaomingChen,BingzhenPei,andRuzhanLu.2005.Anewmethodforsentimentclassiﬁ-cationintextretrieval.InProceedingsofIJCNLP.RaduIon.2007.Methodsforautomaticsemanticdisambigua-tion.ApplicationstoEnglishandRomanian.Ph.D.thesis,TheRomanianAcademy,RACAI.HiroshiKanayamaandTetsuyaNasukawa.2006.Fullyauto-maticlexiconexpansionfordomain-orientedsentimentanal-ysis.InProceedingsofEMNLP2006.Soo-MinKimandEduardHovy.2006.Identifyingandana-lyzingjudgmentopinions.InProceedingsofHLT/NAACL2006.LevonLloyd,DimitriosKechagias,andStevenSkiena.2005.Lydia:Asystemforlarge-scalenewsanalysis.InProceed-ingsofSPIRE2005.GeorgeMiller,ClaudiaLeacock,TangeeRandee,andRossBunker.1993.Asemanticconcordance.InProceedingsoftheDARPAWorkshoponHumanLanguageTechnology.BoPangandLillianLee.2004.Asentimentaleducation:Sen-timentanalysisusingsubjectivitysummarizationbasedonminimumcuts.InProceedingsofACL2004.BoPang,LillianLee,andShivakumarVaithyanathan.2002.Thumbsup?Sentimentclassiﬁcationusingmachinelearningtechniques.InProceedingsofEMNLP2002.EllenRiloffandJanyceWiebe.2003.Learningextractionpat-ternsforsubjectiveexpressions.InProceedingsofEMNLP2003.HiroyaTakamura,TakashiInui,andManabuOkumura.2006.Latentvariablemodelsforsemanticorientationsofphrases.InProceedingsofEACL2006.PeterTurney.2002.Thumbsuporthumbsdown?Semanticorientationappliedtounsupervisedclassiﬁcationofreviews.InProceedingsofACL2002.UniversalDictionary.2007.Availableatwww.dicts.info/uddl.php.JanyceWiebeandRadaMihalcea.2006.Wordsenseandsub-jectivity.InProceedingsofCOLING-ACL2006.JanyceWiebeandEllenRiloff.2005.Creatingsubjectiveandobjectivesentenceclassiﬁersfromunannotatedtexts.InProceedingsofCICLing2005(invitedpaper).Availableatwww.cs.pitt.edu/mpqarequest.JanyceWiebe,TheresaWilson,andClaireCardie.2005.Annotatingexpressionsofopinionsandemotionsinlan-guage.LanguageResourcesandEvaluation,39(2/3):164–210.Availableatwww.cs.pitt.edu/mpqa.TheresaWilson,JanyceWiebe,andPaulHoffmann.2005.Recognizingcontextualpolarityinphrase-levelsentimentanalysis.InProceedingsofHLT/EMNLP2005.HongYuandVasileiosHatzivassiloglou.2003.Towardsan-sweringopinionquestions:Separatingfactsfromopinionsandidentifyingthepolarityofopinionsentences.InPro-ceedingsofEMNLP2003.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 984–991,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

984

SentimentPolarityIdentiﬁcationinFinancialNews:ACohesion-basedApproachAnnDevittSchoolofComputerScience&Statistics,TrinityCollegeDublin,IrelandAnn.Devitt@cs.tcd.ieKhurshidAhmadSchoolofComputerScience&Statistics,TrinityCollegeDublin,IrelandKhurshid.Ahmad@cs.tcd.ieAbstractTextisnotunadulteratedfact.AtextcanmakeyoulaughorcrybutcanitalsomakeyoushortsellyourstocksincompanyAandbuyupoptionsincompanyB?Researchinthedomainofﬁnancestronglysuggeststhatitcan.Studieshaveshownthatboththeinformationalandaffectiveaspectsofnewstextaffectthemarketsinprofoundways,im-pactingonvolumesoftrades,stockprices,volatilityandevenfutureﬁrmearnings.Thispaperaimstoexploreacomputablemetricofpositiveornegativepolarityinﬁnancialnewstextwhichisconsistentwithhumanjudgmentsandcanbeusedinaquantita-tiveanalysisofnewssentimentimpactonﬁ-nancialmarkets.Resultsfromapreliminaryevaluationarepresentedanddiscussed.1IntroductionResearchinsentimentanalysishasemergedtoad-dresstheresearchquestions:whatisaffectintext?whatfeaturesoftextservetoconveyit?howcanthesefeaturesbedetectedandmeasuredautomati-cally.Sentenceandphraselevelsentimentanaly-sisinvolvesasystematicexaminationoftexts,suchasblogs,reviewsandnewsreports,forpositive,negativeorneutralemotions(Wilsonetal.,2005;Grefenstetteetal.,2004).Theterm“sentimentanalysis”isusedratherdifferentlyinﬁnancialeco-nomicswhereitreferstothederivationofmarketconﬁdenceindicatorsfromproxiessuchasstockpricesandtradingvolumes.ThereisatraditiongoingbacktotheNobelSveriges–RiksbankLaure-atesHerbertSimon(1978Prize)andDanielKah-neman(2002Prize),thatshowsthatinvestorsandtradersinsuchmarketscanbehaveirrationallyandthatthisboundedrationalityisinspiredbywhatthetradersandinvestorshearfromothersaboutthecon-ditionsthatmayormaynotprevailinthemarkets.RobertEngle(2003Prize)hasgivenamathematicaldescriptionoftheasymmetricandaffectiveimpactofnewsonprices:positivenewsistypicallyrelatedtolargechangesinpricesbutonlyforashorttime;converselytheeffectofnegativenewsonpricesandvolumesoftradingislongerlasting.Theemergentdomainofsociologyofﬁnanceexaminesﬁnancialmarketsassocialconstructsandhowcommunica-tions,suchase-mailsandnewsreports,maybeloadedwithsentimentwhichcoulddistortmarkettrading(MacKenzie,2003).Itwouldappearthatnewsaffectsthemarketsinprofoundways,impactingonvolumesoftrade,stockreturns,volatilityofpricesandevenfutureﬁrmearnings.Inthedomainofnewsimpactanaly-sisinﬁnance,inrecentyearsthefocushasexpandedfrominformationaltoaffectivecontentoftextinanefforttoexplaintherelationshipbetweentextandthemarkets.Alltext,beitnews,blogs,accountingreportsorpoetry,hasanon-factualdimensioncon-veyingopinion,invokingemotion,providinganu-ancedperspectiveofthefactualcontentofthetext.Withtheincreaseofcomputationalpowerandlex-icalandcorpusresourcesitseemscomputationallyfeasibletodetectsomeoftheaffectivecontentoftextautomatically.Themotivationfortheworkre-portedhereistoidentifyametricforsentimentpo-985

laritywhichreliablyreplicateshumanevaluationsandwhichisreadilyderivablefromfreetext.Thisresearchisbeingcarriedoutinthecontextofastudyoftheimpactofnewsanditsattendantbiasesonﬁnancialmarkets,formalizingearliermulti-lingual,corpus-basedempiricalworkthatanalysedchangeinsentimentandvolumeofnewsinlargeﬁnancialnewscorpora(Ahmadetal.,2006).Asystematicanalysisoftheimpactofnewsbiasorpolarityonmarketvariablesrequiresanumericvalueforsenti-mentintensity,aswellasabinarytagforsentimentpolarity,toidentifytrendsinthesentimentindica-toraswellasturningpoints.Inthisapproach,thecontributiontoanoverallsentimentpolarityandin-tensitymetricofindividuallexicalitemswhichare“affective”bydeﬁnitionisdeterminedbytheircon-nectivityandpositionwithinarepresentationofthetextasawholebasedontheprinciplesoflexicalco-hesion.Thecontributionofeachelementisthere-forenotpurelyadditivebutratherismitigatedbyitsrelevanceandpositionrelativetootherelements.Section2setsoutrelatedworkinthesentimentanalysisdomainbothincomputationallinguisticsandinﬁnancewherethesetechniqueshavebeenappliedwithsomesuccess.Section3outlinesthecohesion-basedalgorithmforsentimentpolarityde-tection,theresourcesusedandthebeneﬁtsofusingthegraph-basedtextrepresentationapproach.Thisapproachwasevaluatedrelativetoasmallcorpusofgoldstandardsentimentjudgments.Thederivationofthegoldstandardanddetailsoftheevaluationareoutlinedinsection4.Theresultsarepresentedanddiscussedinsection5andsection6concludeswithalookatfuturechallengesforthisresearch.2RelatedWork2.1CognitiveTheoriesofEmotionInordertounderstandhowemotioncanberealisedintext,wemustﬁrsthaveanotionofwhatemo-tionisandhowpeopleexperienceit.Currentcogni-tivetheoriesofwhatconstitutesemotionaredividedbetweentwoprimaryapproaches:categoricalanddimensional.TheDarwiniancategoricalapproachpositsaﬁnitesetofbasicemotionswhichareexpe-rienceduniversallyacrosscultures,(e.g.anger,fear,sadness,surprise(EkmanandFriesen,1971)).Thesecondapproachdelineatesemotionsaccordingtomultipledimensionsratherthanintodiscretecate-gories.Thetwoprimarydimensionsinthisaccountareagood–badaxis,thedimensionofvalenceorevaluation,andastrong-weakaxis,thedimensionofactivationorintensity(Osgoodetal.,1957).Theworkreportedhereaimstoconﬂatetheevaluationandactivationdimensionsintoonemetricwiththesizeofthevalueindicatingstrengthofactivationanditssign,polarityontheevaluationaxis.2.2SentimentAnalysisSentimentanalysisincomputationallinguisticshasfocusedonexaminingwhattextualfeatures(lexi-cal,syntactic,punctuation,etc)contributetoaffec-tivecontentoftextandhowthesefeaturescanbedetectedautomaticallytoderiveasentimentmetricforaword,sentenceorwholetext.Wiebeandcol-leagueshavelargelyfocusedonidentifyingsubjec-tivityintexts,i.e.identifyingthosetextswhichareaffectivelyneutralandthosewhicharenot.Thisworkhasbeengroundedinastronghumanevalu-ativecomponent.Thesubjectivityidentiﬁcationre-searchhasmovedfrominitialworkusingsyntacticclass,punctuationandsentencepositionfeaturesforsubjectivityclassiﬁerstolaterworkusingmorelex-icalfeatureslikegradationofadjectivesorwordfre-quency(Wiebeetal.,1999;Wiebeetal.,2005).Oth-ers,suchasTurney(2002),PangandVaithyanathan(2002),haveexaminedthepositiveornegativepo-larity,ratherthanpresenceorabsence,ofaffectivecontentintext.KimandHovy(2004),amongoth-ers,havecombinedthetwotasks,identifyingsub-jectivetextanddetectingitssentimentpolarity.Theindicatorsofaffectivecontenthavebeendrawnfromlexicalsources,corporaandtheworldwidewebandcombinedinavarietyofways,includingfactoranal-ysisandmachinelearningtechniques,todeterminewhenatextcontainsaffectivecontentandwhatisthepolarityofthatcontent.2.3SentimentandNewsImpactAnalysisNiederhoffer(1971),academicandhedgefundman-ager,analysed20yearsofNewYorkTimeshead-linesclassiﬁedinto19semanticcategoriesandonagood-badratingscaletoevaluatehowthemarketsreactedtogoodandbednews:hefoundthatmar-ketsdoreacttonewswithatendencytooverreacttobadnews.Somewhatprophetically,hesuggests986

thatnewsshouldbeanalysedbycomputerstointro-ducemoreobjectivityintheanalysis.EngleandNg(1993)proposedthenewsimpactcurveasamodelforhownewsimpactsonvolatilityinthemarketwithbadnewsintroducingmorevolatilitythangoodnews.Theyusedthemarketvariable,stockreturns,asaproxyfornews,anunexpecteddropinreturnsforbadnewsandanunexpectedriseforgoodnews.Indeed,muchearlyworkusedsuchmarketvariablesorreadilyquantiﬁableaspectsofnewsasaproxyforthenewsitself:e.g.newsarrival,type,provenanceandvolumes(Cutleretal.,1989;MitchellandMul-herin,1994).Morerecentstudieshaveproceededinaspiritofcomputer-aidedobjectivitywhichen-tailsdetermininglinguisticfeaturestobeusedtoautomaticallycategorisetextintopositiveornega-tivenews.Davisetal(2006)investigatetheeffectsofoptimisticorpessimisticlanguageusedinﬁnan-cialpressreleasesonfutureﬁrmperformance.Theyconcludethata)readersformexpectationsregard-ingthehabitualbiasofwritersandb)reactmorestronglytoreportswhichviolatetheseexpectations,stronglysuggestingthatreaders,andbyextensionthemarkets,formexpectationsaboutandreacttonotonlycontentbutalsoaffectiveaspectsoftext.Tet-lock(2007)alsoinvestigateshowapessimismfac-tor,automaticallygeneratedfromnewstextthroughtermclassiﬁcationandprincipalcomponentsanaly-sis,mayforecastmarketactivity,inparticularstockreturns.Heﬁndsthathighnegativityinnewspre-dictslowerreturnsupto4weeksaroundstoryre-lease.Thestudiesestablisharelationshipbetweenaffectivebiasintextandmarketactivitythatmarketplayersandregulatorsmayhavetoaddress.3Approach3.1Cohesion-basedTextRepresentationTheapproachemployedherebuildsonacohesion-basedtextrepresentationalgorithmusedinanewsstorycomparisonapplicationdescribedin(Devitt,2004).Thealgorithmbuildsagraphrepresenta-tionoftextfrompart-of-speechtaggedtextwithoutdisambiguationusingWordNet(Fellbaum,1998)asarealworldknowledgesourcetoreduceinforma-tionlossinthetransitionfromtexttotext-basedstructure.Therepresentationisdesignedwithinthetheoreticalframeworkoflexicalcohesion(HallidayandHasan,1976).Aspectsofthecohesivestruc-tureofatextarecapturedinagraphrepresentationwhichcombinesinformationderivedfromthetextandWordNetsemanticcontent.Thegraphstructureiscomposedofnodesrepresentingconceptsinorde-rivedfromthetextconnectedbyrelationsbetweentheseconceptsinWordNet,suchasantonymyorhy-pernymy,orderivedfromthetext,suchasadjacencyinthetext.Inaddition,theapproachprovidesthefacilitytomanipulateorcontrolhowtheWordNetsemanticcontentinformationisinterpretedthroughtheuseoftopologicalfeaturesoftheknowledgebase.InordertoevaluatetherelativecontributionofWordNetconceptstotheinformationcontentofatextasawhole,anodespeciﬁcitymetricwasderivedbasedonanempiricalanalysisofthedistributionoftopologicalfeaturesofWordNetsuchasinheritance,hierarchydepth,clusteringcoefﬁcientsandnodede-greeandhowthesefeaturesmapontohumanjudg-mentsofconceptspeciﬁcityorinformativity.Thismetricaddressestheissueoftheunevenpopulationofmostknowledgebasessothatthelocalidiosyn-craticcharacteristicsofWordNetcanbemitigatedbysomeofitsglobalfeatures.3.2SentimentPolarityOverlayByexploitingexistinglexicalresourcesforsenti-mentanalysis,anexplicitaffectivedimensioncanbeoverlaidonthisbasictextmodel.Ourapproachtopolaritymeasurement,likeothers,reliesonalex-iconoftaggedpositiveandnegativesentimenttermswhichareusedtoquantifypositive/negativesenti-ment.Inthisﬁrstiterationofthework,SentiWN(EsuliandSebastiani,2006)wasusedasitprovidesareadilyinterpretablepositiveandnegativepolarityvalueforasetof“affective”termswhichconﬂatesOsgood’s(1957)evaluativeandactivationdimen-sions.Furthermore,itisbasedonWordNet2.0andcanthereforebeintegratedintotheexistingtextrep-resentationalgorithm,wheresomenodesintheco-hesiongraphcarryaSentiWNsentimentvalueandothersdonot.Thecontributionofindividualpolar-itynodestothepolaritymetricofthetextasawholeisthendeterminedwithrespecttothetextualinfor-mationandWNsemanticandtopologicalfeaturesencodedintheunderlyinggraphrepresentationofthetext.Threepolaritymetricswereimplementedtoevaluatetheeffectivenessofexploitingdifferent987

aspectsofthecohesion-basedgraphstructure.BasicCohesionMetricisbasedsolelyonfrequencyofsentiment-bearingnodesinorderivedfromthesourcetext,i.e.thesumofpolarityvaluesforallnodesinthegraph.RelationTypeMetricmodiﬁesthebasicmetricwithrespecttothetypesofWordNetrelationsinthetext-derivedgraph.Foreachnodeinthegraph,itssentimentvalueistheproductofitspolarityvalueandarelationweightforeachrelationthisnodeen-tersintointhegraphstructure.Unlikemostlexicalchainingalgorithms,notallWordNetrelationsaretreatedasequal.Inthissentimentoverlay,therela-tionswhicharedeemedmostrelevantarethosethatpotentiallydenotearelationoftheaffectivedimen-sion,likeantonymy,andthosewhichconstitutekeyorganisingprinciplesofthedatabase,suchashy-pernymy.Potentiallyaffect-effectingrelationshavethestrongestweightingwhilemoreamorphousrela-tions,suchas“alsosee”,havethelowest.NodeSpeciﬁcityMetricmodiﬁesthebasicmetricwithrespecttoameasureofnodespeciﬁcitycalcu-latedonthebasisoftopographicalfeaturesofWord-Net.Theintuitionbehindthismeasureisthathighlyspeciﬁcnodesorconceptsmaycarrymoreinforma-tionaland,byextension,affectivecontentthanlessspeciﬁcones.Wehavenotedthedifﬁcultyofusingaknowledgebasewhoseinternalstructureisnotho-mogeneousandwhoseidiosyncrasiesarenotquanti-ﬁed.Thespeciﬁcitymeasureaimstofactoroutpop-ulationsparsenessordensityinWordNetbyevaluat-ingthecontributionofeachnoderelativetoitsdepthinthehierarchy,itsconnectivity(branchingFactor)anditssiblings:Spc=(depth+ln(siblings)−ln(branchingFactor))NormalizingFactor(1)Thethreemetricsarefurtherspecialisedaccordingtothefollowingtwobooleanﬂags:InText:themetriciscalculatedbasedon1)onlythosenodesrepresentingtermsinthesourcetext,or2)allnodesinthegraphrepresentationderivedfromthetext.Inthisway,themetricscanbecalculatedusinginformationderivedfromthegraphrepresen-tation,suchasnodespeciﬁcity,withoutpotentiallynoisycontributionsfromnodesnotinthesourcetextbutrelatedtothem,viarelationssuchashypernymy.Modiﬁers:themetriciscalculatedusingallopenclasspartsofspeechormodiﬁersalone.Onacur-soryinspectionofSentiWN,itseemsthatmodiﬁershavemorereliablevaluesthannounsorverbs.Thisoptionwasincludedtotestforpossibleadverseef-fectsofthelexicon.Intotalforeachmetrictherearefouroutcomescom-bininginTexttrue/falseandmodiﬁerstrue/false.4EvaluationThegoalofthisresearchistoexaminetherelation-shipbetweenﬁnancialmarketsandﬁnancialnews,inparticularthepolarityofﬁnancialnews.Thedo-mainofﬁnanceprovidesdataandmethodsforsolidquantitativeanalysisoftheimpactofsentimentpo-larityinnews.However,inordertoengagewiththislongtraditionofanalysisoftheinstrumentsandrelatedvariablesoftheﬁnancialmarkets,thequan-titativemeasureofpolaritymustbenotonlyeasytocompute,itmustbeconsistentwithhumanjudg-mentsofpolarityinthisdomain.Thisevaluationisaﬁrststeponthepathtoestablishingreliabilityforasentimentmeasureofnews.Unfortunately,thefo-cusonnews,asopposedtoothertexttypes,hasitsdifﬁculties.Muchoftheworkinsentimentanaly-sisinthecomputationallinguisticsdomainhasfo-cusedeitheronshortsegments,suchassentences(Wilsonetal.,2005),oronlongerdocumentswithanexplicitpolarityorientationlikemovieorprod-uctreviews(Turney,2002).Notallnewsitemsmayexpressovertsentiment.Therefore,inordertotestourhypothesis,weselectedanewstopicwhichwasconsideredaprioritohaveemotivecontent.4.1CorpusMarketsreactstrongesttoinformationaboutﬁrmstowhichtheyhaveanemotionalattachment(Mac-Gregoretal.,2000).Furthermore,takeoversandmergersareusuallyseenashighlyemotivecontexts.Tocombinethesetwoemotion-enhancingfactors,acorpusofnewstextswascompiledonthetopicoftheaggressivetakeoverbidofalow-costairline(Ryanair)fortheIrishﬂag-carrierairline(AerLin-gus).Bothairlineshaveastrong(positiveandnega-tive)emotionalattachmentformanyinIreland.Fur-thermore,bothairlinesarehighlyvisiblewithinthecountryandhavevocalsupportersanddetractorsinthepublicarena.Thecorpusisdrawnfromthe988

nationalmediaandinternationalnewswiresourcesandspans4monthsin2006fromtheﬂotationoftheﬂagcarrieronthestockexchangeinSeptem-ber2006,throughthe“surprise”take-overbidan-nouncementbyRyanair,tothewithdrawalofthebidbyRyanairinDecember2006.14.2GoldStandardAsetof30textsselectedfromthecorpuswasanno-tatedby3peopleona7-pointscalefromverypos-itivetoverynegative.Giventhatatakeoverbidhastwoplayers,therespondentswereaskedalsotoratethesemanticorientationofthetextswithrespecttothetwoplayers,RyanairandAerLingus.Respon-dentswereallnativeEnglishspeakers,2femaleand1male.Toensureemotionalengagementinthetask,theywereﬁrstaskedtoratetheirpersonalattitudetothetwoairlines.Theratingsinallthreecaseswereontheextremeendsofthe7pointscale,withverypositiveattitudestowardstheﬂagcarrierandverynegativeattitudestowardsthelow-costairline.Re-spondentattitudesmayimpactontheirtextevalu-ationsbut,giventhehighagreementofattitudesinthisstudy,thisimpactshouldatleastbeconsistentacrosstheindividualsinthestudy.Alargerstudyshouldcontrolexplicitlyforthisvariable.Astherespondentsgaveratingsonarankedscale,inter-respondentreliabilitywasdeterminedusingKrippendorf’salpha,amodiﬁcationoftheKappacoefﬁcientforordinaldata(Krippendorff,1980).Onthegeneralrankingscale,therewaslittleagreement(kappa=0.1685),corroboratingfeedbackfromre-spondentsonthedifﬁcultyofprovidingageneralratingfortextpolaritydistinctfromaratingwithre-specttooneofthetwocompanies.However,therewasanacceptabledegreeofagreement(Groveetal.,1981)ontheRyanairandAerLinguspolarityrat-ings,kappa=0.5795andkappa=0.5589respec-tively.Resultsreportcorrelationswiththeseratingswhichareconsistentand,fromtheﬁnancialmarketperspective,potentiallymoreinteresting.21AcorrelationanalysisofhumansentimentratingswithRyanairandAerLingusstockpricesforthelastquarterof2006wasconducted.Theﬁndingssuggestthatstockpriceswerecor-relatedwithratingswithrespecttoAerLingus,suggestingthat,duringthistakeoverperiod,investorsmayhavebeeninﬂuencedbysentimentexpressedinnewstowardsAerLingus.However,thetimeseriesistooshorttoensurestatisticalsigniﬁcance.2Resultsinthispaperarereportedwithrespecttothe4.3PerformanceMetricsTheperformanceofthepolarityalgorithmwaseval-uatedrelativetoacorpusofhuman-annotatednewstexts,focusingontwoseparatedimensionsofpolar-ity:1.Polaritydirection:thetaskofassigningabi-narypositive/negativevaluetoatext2.Polarityintensity:thetaskofassigningavaluetoindicatethestrengthofthenegative/positivepolarityinatext.Performanceontheformerisreportedusingstan-dardrecallandprecisionmetrics.Thelatterisre-portedasacorrelationwithaveragehumanratings.4.4BaselineForthemetricsinsection3,thebaselineforcompar-isonsumstheSentiWNpolarityratingforonlythoselexicalitemspresentinthetext,notexploitinganyaspectofthegraphrepresentationofthetext.ThisbaselinecorrespondstotheBasicCohesionMetric,withinText=true(onlylexicalitemsinthetext)andmodifiers=false(allpartsofspeech).5ResultsandDiscussion5.1BinaryPolarityAssignmentThebaselineresultsforpositiveratings,negativerat-ingsandoverallaccuracyforthetaskofassigningapolaritytagarereportedintable1.TheresultsshowTypePrecisionRecallFScorePositive0.3810.72730.5Negative0.6670.31580.4286Overall0.46670.46670.4667Table1:Baselineresultsthatthebaselinetendstowardsthepositiveendoftheratingspectrum,withhighrecallforpositiverat-ingsbutlowprecision.Conversely,negativeratingshavehighprecisionbutlowrecall.Figures1to3illustratetheperformanceforpositive,negativeandoverallratingsofallmetric–inText–Modiﬁercombi-nations,enumeratedintable2,relativetothisbase-line,thehorizontal.Thosemetricswhichsurpassthislinearedeemedtooutperformthebaseline.Ryanairratingsastheyhadthehighestinter-rateragreement.989

1Cohesion5Relation9NodeSpec2CohesionTxt6RelationTxt10NodeSpecTxt3CohesionMod7RelationMod11NodeSpecMod4CohesionTxtMod8RelationTxtMod12NodeSpecTxtModTable2:MetrictypesinFigures1-3Figure1:FScoreforPositiveRatingsAllmetricshaveabiastowardspositiveratingswithattendanthighpositiverecallvaluesandim-provedf-scoreforpositivepolarityassignments.TheBasicCohesionMetricmarginallyoutperformsthebaselineoverallindicatingthatexploitingthegraphstructuregivessomeaddedbeneﬁt.FortheRelationsandSpeciﬁcitymetrics,systemperfor-mancegreatlyimprovesonthebaselineforthemodifiers=trueoptions,whereas,whenallpartsofspeechareincluded(modifier=false),perfor-mancedropssigniﬁcantly.Thissensitivitytoinclu-sionofallwordclassescouldsuggestthatmodiﬁersarebetterindicatorsoftextpolaritythanotherwordclassesorthatthemetricsusedarenotappropriatetonon-modiﬁerpartsofspeech.Theformerhypoth-esisisnotsupportedbytheliteraturewhilethelatterisnotsupportedbypriorsuccessfulapplicationofthesemetricsinatextcomparisontask.Inordertoinvestigatethesourceofthissensitivity,weintendtoexaminethedistributionofrelationtypesandnodespeciﬁcityvaluesforsentiment-bearingtermstode-terminehowbesttotailorthesemetricstothesenti-mentidentiﬁcationtask.Afurtherhypothesisisthatthebasicpolarityval-uesfornon-modiﬁersarelessreliablethanforad-jectivesandadverbs.Onacursoryinspectionofpo-larityvaluesofnounsandadjectivesinSentiWN,itwouldappearthatadjectivesaresomewhatmorere-liablylabelledthannouns.Forexample,crimeandFigure2:FScoreforNegativeRatingssomeofitshyponymsarelabelledasneutral(e.g.forgery)orevenpositive(e.g.assault)whereascrim-inalislabelledasnegative.Thisillustratesakeyweaknessinalexicalapproachsuchasthis:over-relianceonlexicalresources.Nolexicalresourceisinfallible.Itisthereforevitaltospreadtheassoci-atedriskbyusingmorethanoneknowledgesource,e.g.multiplesentimentlexicaorusingcorpusdata.Figure3:FScoreforAllRatings5.2PolarityIntensityValuesTheresultsonthepolarityintensitytaskparalleltheresultsonpolaritytagassignment.Table3setsoutthecorrelationcoefﬁcientsforthemetricswithre-specttotheaveragehumanrating.Again,thebestperformersaretherelationtypeandnodespeciﬁcitymetricsusingonlymodiﬁers,signiﬁcanttothe0.05level.Yetthecorrelationcoefﬁcientsoverallarenotveryhigh.Thiswouldsuggestthatperhapsthere-lationshipbetweenthehumanrankingscaleandtheautomaticoneisnotstrictlylinear.Althoughthehu-manratingsmapapproximatelyontotheautomati-990

callyderivedscale,theredoesnotseemtobeaclearonetoonemapping.Thesectionthatfollowsdiscussthisandsomeoftheotherissueswhichthisevalua-tionprocesshasbroughttolight.MetricinTextModiﬁerCorrelationBasicCohesionNoNo0.47**YesNo0.42*NoYes0.47**YesYes0.47**RelationTypeNoNo-0.1**YesNo-0.13*NoYes0.5**YesYes0.38*NodeSpeciﬁcityNoNo0.00YesNo-0.03NoYes0.48**YesYes0.38*Table3:CorrelationCoefﬁcientsforhumanratings.**.Signiﬁcantatthe0.01level.*.Signiﬁcantatthe0.05level.5.3IssuesTheRatingScaleandThresholdingOverallthealgorithmtendstowardsthepositiveendofthespectrumindirectcontrasttohumanraterswith55-70%ofallratingsbeingnegative.Further-more,thecorrelationofhumantoalgorithmratingsissigniﬁcantbutnotstronglydirectional.Itwouldappearthattherearemorepositivelexicalitemsintext,hencethealgorithm’spositivebias.Yetmuchofthispositivityisnothavingastrongimpactonreaders,hencethenegativebiasobservedintheseevaluators.Thisraisesquestionsaboutthescaleofhumanpolarityjudgments:arepeoplemoresensi-tivetonegativityintext?isthereapositivebaselineintextthatpeopleﬁndunremarkableandignore?Toinvestigatethisissue,wewillconductacompar-ativecorpusanalysisofthedistributionofpositiveandnegativelexicalitemsintextandtheirperceivedstrengthsintext.Theresultsofthisanalysisshouldhelptolocatesentimentturningpointsorthresholdsandestablishanelasticsentimentscalewhichallowsforbaselinebutdisregardedpositivityintext.TheImpactoftheLexiconThealgorithmdescribedhereislexicon-based,fullyreliantonavailablelexicalresources.However,wehavenotedthatanover-relianceonlexicahasitsdisadvantages,asanyhand-codedorcorpus-derivedlexiconwillhavesomedegreeoferrororinconsis-tency.Inordertoaddressthisissue,itisneces-sarytospreadtheriskassociatedwithasinglelex-icalresourcebydrawingonmultiplesources,asin(KimandHovy,2005).TheSentiWNlexiconusedinthisimplementationisderivedfromaseedwordsetsupplementedWordNetrelationsandassuchithasnotbeenpsychologicallyvalidated.Forthisrea-son,ithasgoodcoveragebutsomeinconsistency.Whissel’sDictionaryofAffect(1989)ontheotherhandisbasedentirelyonhumanratingsofterms.It’scoveragemaybenarrowerbutaccuracymightbemorereliable.Thisdictionaryalsohastheadvan-tageofseparatingoutOsgood’s(1957)evaluativeandactivationdimensionsaswellasan“imaging”ratingforeachtermtoallowamulti-dimensionalanalysisofaffectivecontent.TheWNAffectlexi-con(Valituttietal.,2004)againprovidessomewhatdifferentratingtypeswheretermsareclassiﬁedintermsofdenotingorevokingdifferentphysicalormentalaffectivereactions.Together,theseresourcescouldoffernotonlymoreaccuratebasepolarityval-uesbutalsomorenuancedmetricsthatmaybettercorrespondtohumannotionsofaffectintext.TheGoldStandardSentimentratingevaluationisnotastraight-forwardtask.Wiebeetal(2005)notemanyofthedifﬁcul-tiesassociatedhumansentimentratingsoftext.Asnotedabove,itcanbeevenmoredifﬁcultwheneval-uatingnewswherethetextisintendedtoappearim-partial.Theattitudeoftheevaluatorcanbeallim-portant:theirattitudetotheindividualsororgani-sationsinthetext,theirprofessionalviewpointasamarketplayeroranordinarypunter,theirattitudetouncertaintyandriskwhichcanbeakeyfactorintheworldofﬁnance.Inordertoaddresstheseissuesforthedomainofnewsimpactinﬁnancialmarkets,theexpertiseofmarketprofessionalsmustbeelicitedtodeterminewhattheylookforintextandwhatview-pointtheyadoptwhenreadingﬁnancialnews.Ineconometricanalysis,stockpriceortradingvolumedataconstituteanalternativegoldstandard,repre-sentingaproxyforhumanreactiontonews.Foreco-nomicsigniﬁcance,thedatamustspanatimeperiodofseveralyearsandcompilationofatextandstock991

pricecorpusforalargescaleanalysisisunderway.6ConclusionsandFutureWorkThispaperpresentsalexicalcohesionbasedmet-ricofsentimentintensityandpolarityintextandanevaluationofthismetricrelativetohumanjudg-mentsofpolarityinﬁnancialnews.Wearecon-ductingfurtherresearchonhowbesttocaptureapsychologicallyplausiblemeasureofaffectivecon-tentoftextbyexploitingavailableresourcesandabroaderevaluationofthemeasurerelativetohumanjudgmentsandexistingmetrics.Thisresearchisex-pectedtocontributetosentimentanalysisinﬁnance.Givenareliablemetricofsentimentintext,whatistheimpactofchangesinthisvalueonmarketvariables?Thisinvolvesasociolinguisticdimensiontodeterminewhatpublicationsortextsbestcharac-teriseoraremostreadandhavethegreatestinﬂu-enceinthisdomainandtheeconomicdimensionofcorrelationwitheconomicindicators.ReferencesKhurshidAhmad,DavidCheng,andYousifAlmas.2006.Multi–lingualsentimentanalysisinﬁnancialnewsstreams.InProc.ofthe1stIntl.Conf.onGridinFinance,Italy.DavidM.Cutler,JamesM.Poterba,andLawrenceH.Sum-mers.1989.Whatmovesstockprices.JournalofPortfolioManagement,79:223–260.AngelaK.Davis,JeremyM.Piger,andLisaM.Sedor.2006.Beyondthenumbers:Ananalysisofoptimisticandpes-simisticlanguageinearningspressreleases.Technicalre-port,FederalReserveBankofStLouis.AnnDevitt.2004.MethodsforMeaningfulTextRepresentationandComparison.Ph.D.thesis,TrinityCollegeDublin.PaulEkmanandW.V.Friesen.1971.Constantsacrossculturesinthefaceandemotion.JournalofPersonalityandSocialPsychology,17:124–129.RobertF.EngleandVictorK.Ng.1993.Measuringandtest-ingtheimpactofnewsonvolatility.JournalofFinance,48(5):1749–1778.AndreaEsuliandFabrizioSebastiani.2006.Sentiwordnet:Apubliclyavailablelexicalresourceforopinionmining.InProceedingsofLREC2006.ChristianeFellbaum.1998.WordNet,anelectroniclexicaldatabase.MITPress.GregoryGrefenstette,YanQu,JamesG.Shanahan,andDavidA.Evans.2004.Couplingnichebrowsersandaffectanalysisforanopinionminingapplication.InProceedingsofRIAO-04,pages186–194.WilliamN.Grove,NancyC.Andreasen,PatriciaMcDonald-Scott,MartinB.Keller,andRobertW.Shapiro.1981.Reli-abilitystudiesofpsychiatricdiagnosis.theoryandpractice.ArchivesofGeneralPsychiatry,38:408–413.MichaelA.K.HallidayandRuqaiyaHasan.1976.CohesioninEnglish.Longman.Soo-MinKimandEduardHovy.2004.Determiningthesenti-mentofopinions.InProceedingsofCOLING2004.Soo-MinKimandEduardHovy.2005.Automaticdetectionofopinionbearingwordsandsentences.InProc.ofIJCNLP-05,JejuIsland,Korea.KlausKrippendorff.1980.ContentAnalysis:anIntroductiontoitsMethodology.SagePublications,BeverlyHills,CA.DonaldG.MacGregor,PaulSlovic,DavidDreman,andMichaelBerry.2000.Imagery,affect,andﬁnancialjudg-ment.TheJournalofPsychologyandFinancialMarkets,1(2):104–110.DonaldMacKenzie.2003.Long-termcapitalmanagementandthesociologyofarbitrage.EconomyandSociety,32:349–380.MarkL.MitchellandJ.HaroldMulherin.1994.Theimpactofpublicinformationonthestockmarket.JournalofFinance,49(3):923–950.VictorNiederhoffer.1971.Theanalysisofworldeventsandstockprices.JournalofBusiness,44(2):193–219.CharlesE.Osgood,GeorgeJ.Suci,andPercyH.Tannenbaum.1957.TheMeasurementofmeaning.UniversityofIllinoisPress,Chicago,Ill.BoPang,LillianLee,andShivakumarVaithyanathan.2002.Thumbsup?Sentimentclassiﬁcationusingmachinelearningtechniques.InProc.ofEMNLP-02,pages79–86.PaulC.Tetlock.2007.Givingcontenttoinvestorsentiment:Theroleofmediainthestockmarket.JournalofFinance.forthcoming.PeterD.Turney.2002.Thumbsuporthumbsdown?semanticorientationappliedtounsupervisedclassiﬁcationofreviews.InProceedingsofACL’02,pages417–424.AlessandroValitutti,CarloStrapparava,andOlivieroStock.2004.Developingaffectivelexicalresources.PsychNologyJournal,2(1):61–83.CynthiaWhissell.1989.Thedictionaryofaffectinlanguage.InR.PlutchikandH.Kellerman,editors,Emotion:theoryresearchandexperience,volume4.Acad.Press,London.JanyceM.Wiebe,RebeccaF.Bruce,andThomasP.O’Hara.1999.Developmentanduseofagold-standarddatasetforsubjectivityclassiﬁcations.InProceedingsofACL-99.JanyceWiebe,TheresaWilson,andClaireCardie.2005.An-notatingexpressionsofopinionsandemotionsinlanguage.LanguageResourcesandEvaluation,39:165–210.TheresaWilson,JanyceWiebe,andPaulHoffmann.2005.Recognizingcontextualpolarityinphrase-levelsentimentanalysis.InProc.ofHLT/EMNLP-2005,pages347–354.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 992–999,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

992

WeaklySupervisedLearningforHedgeClassiﬁcationinScientiﬁcLiteratureBenMedlockComputerLaboratoryUniversityofCambridgeCambridge,CB3OFDbenmedlock@cantab.netTedBriscoeComputerLaboratoryUniversityofCambridgeCambridge,CB3OFDejb@cl.cam.ac.ukAbstractWeinvestigateautomaticclassiﬁcationofspeculativelanguage(‘hedging’),inbiomedicaltextusingweaklysupervisedmachinelearning.Ourcontributionsincludeaprecisedescriptionofthetaskwithanno-tationguidelines,analysisanddiscussion,aprobabilisticweaklysupervisedlearningmodel,andexperimentalevaluationofthemethodspresented.WeshowthathedgeclassiﬁcationisfeasibleusingweaklysupervisedML,andpointtowardavenuesforfutureresearch.1IntroductionTheautomaticprocessingofscientiﬁcpapersusingNLPandmachinelearning(ML)techniquesisanincreasinglyimportantaspectoftechnicalinformat-ics.Inthequestforadeepermachine-driven‘under-standing’ofthemassofscientiﬁcliterature,afre-quentlyoccuringlinguisticphenomenonthatmustbeaccountedforistheuseofhedgingtodenotepropositionsofaspeculativenature.Considerthefollowing:1.OurresultsprovethatXfK89inhibitsFelin-9.2.OurresultssuggestthatXfK89mightinhibitFelin-9.Thesecondexamplecontainsahedge,signaledbytheuseofsuggestandmight,whichrendersthepropositioninhibit(XfK89→Felin-9)speculative.Suchanalysiswouldbeusefulinvariousapplica-tions;forinstance,considerasystemdesignedtoidentifyandextractinteractionsbetweengeneticen-titiesinthebiomedicaldomain.Case1abovepro-videscleartextualevidenceofsuchaninteractionandjustiﬁesextractionofinhibit(XfK89→Felin-9),whereascase2providesonlyweakevidenceforsuchaninteraction.Hedgingoccursacrosstheentirespectrumofsci-entiﬁcliterature,thoughitisparticularlycommonintheexperimentalnaturalsciences.Inthisstudyweconsidertheproblemoflearningtoautomaticallyclassifysentencescontaininginstancesofhedging,givenonlyaverylimitedamountofannotator-labelled‘seed’data.Thisfallswithintheweaklysu-pervisedMLframework,forwhicharangeoftech-niqueshavebeenpreviouslyexplored.Thecontri-butionsofourworkareasfollows:1.Weprovideacleardescriptionoftheprob-lemofhedgeclassiﬁcationandofferanim-provedandexpandedsetofannotationguide-lines,whichaswedemonstrateexperimentallyaresufﬁcienttoinduceahighlevelofagree-mentbetweenindependentannotators.2.Wediscussthespeciﬁcitiesofhedgeclassiﬁca-tionasaweaklysupervisedMLtask.3.Wederiveaprobabilisticweaklysupervisedlearningmodelanduseittomotivateourap-proach.4.Weanalyzeourlearningmodelexperimentallyandreportpromisingresultsforthetaskonanewpublicly-availabledataset.12RelatedWork2.1HedgeClassiﬁcationWhilethereisacertainamountofliteraturewithinthelinguisticscommunityontheuseofhedgingin1availablefromwww.cl.cam.ac.uk/∼bwm23/993

scientiﬁctext,eg.(Hyland,1994),thereislittleofdirectrelevancetothetaskofclassifyingspeculativelanguagefromanNLP/MLperspective.ThemostclearlyrelevantstudyisLightetal.(2004)wherethefocusisonintroducingtheprob-lem,exploringannotationissuesandoutliningpo-tentialapplicationsratherthanonthespeciﬁcitiesoftheMLapproach,thoughtheydopresentsomeresultsusingamanuallycraftedsubstringmatch-ingclassiﬁerandasupervisedSVMonacollectionofMedlineabstracts.Wewilldrawonthisworkthroughoutourpresentationofthetask.Hedgingissometimesclassedundertheumbrellaconceptofsubjectivity,whichcoversavarietyoflin-guisticphenomenausedtoexpressdifferingformsofauthorialopinion(Wiebeetal.,2004).Riloffetal.(2003)explorebootstrappingtechniquestoidentifysubjectivenounsandsubsequentlyclassifysubjec-tivevs.objectivesentencesinnewswiretext.Theirworkbearssomerelationtoours;however,ourdo-mainsofinterestdiffer(newswirevs.scientiﬁctext)andtheydonotaddresstheproblemofhedgeclas-siﬁcationdirectly.2.2WeaklySupervisedLearningRecentyearshavewitnessedasigniﬁcantgrowthofresearchintoweaklysupervisedMLtechniquesforNLPapplications.Differentapproachesareof-tencharacterisedaseithermulti-orsingle-view,wheretheformergeneratemultipleredundant(orsemi-redundant)‘views’ofadatasampleandper-formmutualbootstrapping.Thisideawasfor-malisedbyBlumandMitchell(1998)intheirpresentationofco-training.Co-traininghasalsobeenusedfornamedentityrecognition(NER)(CollinsandSinger,1999),coreferenceresolution(NgandCardie,2003),textcategorization(NigamandGhani,2000)andimprovinggenenamedata(Wellner,2005).Conversely,single-viewlearningmodelsoperatewithoutanexplicitpartitionofthefeaturespace.Perhapsthemostwellknownofsuchapproachesisexpectationmaximization(EM),usedbyNigametal.(2000)fortextcategorizationandbyNgandCardie(2003)incombinationwithameta-levelfea-tureselectionprocedure.Self-trainingisanalterna-tivesingle-viewalgorithminwhichalabelledpoolisincrementallyenlargedwithunlabelledsamplesforwhichthelearnerismostconﬁdent.EarlyworkbyYarowsky(1995)fallswithinthisframework.BankoandBrill(2001)use‘bagging’andagree-menttomeasureconﬁdenceonunlabelledsamples,andmorerecentlyMcCloskyetal.(2006)useself-trainingforimprovingparsereranking.Otherrelevantrecentworkincludes(Zhang,2004),inwhichrandomfeatureprojectionandacommitteeofSVMclassiﬁersisusedinahybridco/self-trainingstrategyforweaklysupervisedre-lationclassiﬁcationand(Chenetal.,2006)whereagraphbasedalgorithmcalledlabelpropagationisemployedtoperformweaklysupervisedrelationex-traction.3TheHedgeClassiﬁcationTaskGivenacollectionofsentences,S,thetaskistolabeleachsentenceaseitherspeculativeornon-speculative(specornspechenceforth).Speciﬁcally,Sistobepartitionedintotwodisjointsets,onerep-resentingsentencesthatcontainsomeformofhedg-ing,andtheotherrepresentingthosethatdonot.Tofurtherelucidatethenatureofthetaskandim-proveannotationconsistency,wehavedevelopedanewsetofguidelines,buildingontheworkofLightetal.(2004).AsnotedbyLightetal.,speculativeassertionsaretobeidentiﬁedonthebasisofjudge-mentsabouttheauthor’sintendedmeaning,ratherthanonthepresenceofcertaindesignatedhedgeterms.WebeginwiththehedgedeﬁnitiongivenbyLightetal.(item1)andintroduceasetoffurtherguidelinestohelpelucidatevarious‘greyareas’andtightenthetaskspeciﬁcation.Theseweredevelopedafterinitialannotationbytheauthors,andthroughdiscussionwithcolleagues.FurtherexamplesaregiveninonlineAppendixA2.Thefollowingareconsideredhedgeinstances:1.Anassertionrelatingtoaresultthatdoesnotnecessarilyfollowfromworkpresented,butcouldbeextrapolatedfromit(Lightetal.).2.Relayofhedgemadeinpreviouswork.DlandSerhavebeenproposedtoactredundantlyinthesensorybristlelineage.3.Statementofknowledgepaucity.2availablefromwww.cl.cam.ac.uk/∼bwm23/994

HowendocytosisofDlleadstotheactivationofNre-mainstobeelucidated.4.Speculativequestion.AsecondimportantquestioniswhethertheroXgeneshavethesame,overlappingorcomplementingfunctions.5.Statementofspeculativehypothesis.Totestwhetherthereportedseaurchinsequencesrepre-sentatrueRAG1-likematch,werepeatedtheBLASTPsearchagainstallGenBankproteins.6.Anaphorichedgereference.Thishypothesisissupportedbyourﬁndingthatbothpu-pariationrateandsurvivalareaffectedbyEL9.Thefollowingarenotconsideredhedgeinstances:1.Indicationofexperimentallyobservednon-universalbehaviour.proteinswithsingleBIRdomainscanalsohavefunctionsincellcycleregulationandcytokinesis.2.Conﬁdentassertionbasedonexternalwork.TwodistinctE3ubiquitinligaseshavebeenshowntoreg-ulateDlsignalinginDrosophilamelanogaster.3.Statementofexistenceofproposedalterna-tives.Differentmodelshavebeenproposedtoexplainhowen-docytosisoftheligand,whichremovestheligandfromthecellsurface,resultsinNreceptoractivation.4.Experimentally-supportedconﬁrmationofpre-viousspeculation.HereweshowthatthehemocytesarethemainregulatorofadenosineintheDrosophilalarva,aswasspeculatedpreviouslyformammals.5.Negationofprevioushedge.Althoughtheadgf-amutationleadstolarvalorpupaldeath,wehaveshownthatthisisnotduetotheadenosineordeoxyadenosinesimplyblockingcellularproliferationorsurvival,astheexperimentsinvitrowouldsuggest.4DataWeusedanarchiveof5579full-textpapersfromthefunctionalgenomicsliteraturerelatingtoDrosophilamelanogaster(thefruitﬂy).Thepaperswerecon-vertedtoXMLandlinguisticallyprocessedusingtheRASPtoolkit3.Weannotatedsixofthepa-perstoformatestsetwithatotalof380specsen-tencesand1157nspecsentences,andrandomlyse-lected300,000sentencesfromtheremainingpapersastrainingdatafortheweaklysupervisedlearner.Toensureselectionofcompletesentencesratherthan3www.informatics.susx.ac.uk/research/nlp/raspFrel1κOriginal0.82930.9336Corrected0.96520.9848Table1:AgreementScoresheadings,captionsetc.,unlabelledsampleswerechosenundertheconstraintsthattheymustbeatleast10wordsinlengthandcontainamainverb.5AnnotationandAgreementTwoseparateannotatorswerecommissionedtola-belthesentencesinthetestset,ﬁrstlyoneoftheauthorsandsecondlyadomainexpertwithnopriorinputintotheguidelinedevelopmentprocess.Thetwoannotatorslabelledthedataindependentlyus-ingtheguidelinesoutlinedinsection3.RelativeF1(Frel1)andCohen’sKappa(κ)werethenusedtoquantifythelevelofagreement.Forbrevitywereferthereaderto(ArtsteinandPoesio,2005)and(Hripc-sakandRothschild,2004)forformulationanddis-cussionofκandFrel1respectively.Thetwometricsarebasedondifferentassump-tionsaboutthenatureoftheannotationtask.Frel1isfoundedonthepremisethatthetaskistorecog-niseandlabelspecsentencesfromwithinaback-groundpopulation,anddoesnotexplicitlymodelagreementonnspecinstances.Itrangesfrom0(noagreement)to1(nodisagreement).Conversely,κgivesexplicitcreditforagreementonbothspecandnspecinstances.Theobservedagreementisthencorrectedfor‘chanceagreement’,yieldingametricthatrangesbetween−1and1.Givenourdeﬁni-tionofhedgeclassiﬁcationandassessingthemannerinwhichtheannotationwascarriedout,wesuggestthatthefoundingassumptionofFrel1ﬁtsthenatureofthetaskbetterthanthatofκ.Followinginitialagreementcalculation,thein-stancesofdisagreementwereexamined.Itturnedoutthatthelargemajorityofcasesofdisagreementwereduetonegligenceonbehalfofoneorotheroftheannotators(i.e.casesofclearhedgingthatweremissed),andthatthecasesofgenuinedisagreementwereactuallyquiterare.Newlabelingswerethencreatedwiththenegligentdisagreementscorrected,resultinginsigniﬁcantlyhigheragreementscores.Valuesfortheoriginalandnegligence-correctedla-995

belingsarereportedinTable1.Annotatorconferralviolatesthefundamentalas-sumptionofannotatorindependence,andsothelat-teragreementscoresdonotrepresentthetruelevelofagreement;however,itisreasonabletoconcludethattheactualagreementisapproximatelylowerboundedbytheinitialvaluesandupperboundedbythelattervalues.Infacteventhelowerboundiswellwithintherangeusuallyacceptedasrepresent-ing‘good’agreement,andthusweareconﬁdentinacceptinghumanlabelingasagold-standardforthehedgeclassiﬁcationtask.Forourexperiments,weusethelabelingofthegeneticsexpert,correctedfornegligentinstances.6DiscussionInthisstudyweusesingletermsasfeatures,basedontheintuitionthatmanyhedgecuesaresingleterms(suggest,likelyetc.)andduetothesuccessof‘bagofwords’representationsinmanyclassiﬁca-tiontaskstodate.Investigatingmorecomplexsam-plerepresentationstrategiesisanavenueforfutureresearch.Thereareanumberoffactorsthatmakeourfor-mulationofhedgeclassiﬁcationbothinterestingandchallengingfromaweaklysupervisedlearningper-spective.Firstly,duetotherelativesparsityofhedgecues,mostsamplescontainlargenumbersofirrele-vantfeatures.Thisisincontrasttomuchpreviousworkonweaklysupervisedlearning,whereforin-stanceinthecaseoftextcategorization(BlumandMitchell,1998;Nigametal.,2000)almostallcon-tenttermsaretosomedegreerelevant,andirrel-evanttermscanoftenbeﬁlteredout(e.g.stop-wordremoval).Inthesamevein,forthecaseofentity/relationextractionandclassiﬁcation(CollinsandSinger,1999;Zhang,2004;Chenetal.,2006)thecontextoftheentityorentitiesinconsiderationprovidesahighlyrelevantfeaturespace.Anotherinterestingfactorinourformulationofhedgeclassiﬁcationisthatthenspecclassisdeﬁnedonthebasisoftheabsenceofhedgecues,render-ingithardtomodeldirectly.Thischaracteristicisalsoproblematicintermsofselectingareliablesetofnspecseedsentences,asbydeﬁnitionatthebeginningofthelearningcyclethelearnerhaslit-tleknowledgeaboutwhatahedgelookslike.Thisproblemisaddressedinsection10.3.Inthisstudywedevelopalearningmodelbasedaroundtheconceptofiterativelypredictinglabelsforunlabelledtrainingsamples,thebasicparadigmforbothco-trainingandself-training.Howeverwegeneralisebyframingthetaskintermsoftheacqui-sitionoflabelledtrainingdata,fromwhichasuper-visedclassiﬁercansubsequentlybelearned.7AProbabilisticModelforTrainingDataAcquisitionInthissection,wederiveasimpleprobabilisticmodelforacquiringtrainingdataforagivenlearn-ingtask,anduseittomotivateourapproachtoweaklysupervisedhedgeclassiﬁcation.Given:•samplespaceX•setoftargetconceptclassesY={y1...yN}•targetfunctionY:X→Y•setofseedsamplesforeachclassS1...SNwhereSi⊂Xand∀x∈Si[Y(x)=yi]•setofunlabelledsamplesU={x1...xK}Aim:InferasetoftrainingsamplesTiforeachcon-ceptclassyisuchthat∀x∈Ti[Y(x)=yi]Now,itfollowsthat∀x∈Ti[Y(x)=yi]issatisﬁedinthecasethat∀x∈Ti[P(yi|x)=1],whichleadstoamodelinwhichTiisinitialisedtoSiandtheniter-ativelyaugmentedwiththeunlabelledsample(s)forwhichtheposteriorprobabilityofclassmembershipismaximal.Formally:Ateachiteration:Ti←xj(∈U)wherej=argmaxj[P(yi|xj)](1)ExpansionwithBayes’Ruleyields:argmaxj[P(yi|xj)]=argmaxj(cid:20)P(xj|yi)·P(yi)P(xj)(cid:21)(2)AninterestingobservationistheimportanceofthesamplepriorP(xj)inthedenominator,of-tenignoredforclassiﬁcationpurposesbecauseofitsinvariancetoclass.Wecanexpandfurtherby996

marginalisingovertheclassesinthedenominatorinexpression2,yielding:argmaxj"P(xj|yi)·P(yi)PNn=1P(yn)P(xj|yn)#(3)soweareleftwiththeclasspriorsandclass-conditionallikelihoods,whichcanusuallybeesti-mateddirectlyfromthedata,atleastunderlimiteddependenceassumptions.Theclasspriorscanbeestimatedbasedontherelativedistributionsizesde-rivedfromthecurrenttrainingsets:P(yi)=|Ti|Pk|Tk|(4)where|S|isthenumberofsamplesintrainingsetS.Ifweassumefeatureindependence,whichaswewillseeforourtaskisnotasgrossanapproximationasitmayatﬁrstseem,wecansimplifytheclass-conditionallikelihoodinthewellknownmanner:P(xj|yi)=YkP(xjk|yi)(5)andthenestimatethelikelihoodforeachfeature:P(xk|yi)=αP(yi)+f(xk,Ti)αP(yi)+|Ti|(6)wheref(x,S)isthenumberofsamplesintrainingsetSinwhichfeaturexispresent,andαisauni-versalsmoothingconstant,scaledbytheclassprior.Thisscalingismotivatedbytheprinciplethatwith-outknowledgeofthetruedistributionofapartic-ularfeatureitmakessensetoincludeknowledgeoftheclassdistributioninthesmoothingmecha-nism.Smoothingisparticularlyimportantintheearlystagesofthelearningprocesswhentheamountoftrainingdataisseverelylimitedresultinginunre-liablefrequencyestimates.8HedgeClassiﬁcationWewillnowconsiderhowtoapplythislearningmodeltothehedgeclassiﬁcationtask.Asdiscussedearlier,thespeculative/non-speculativedistinctionhingesonthepresenceorabsenceofafewhedgecueswithinthesentence.Workingonthispremise,allfeaturesarerankedaccordingtotheirprobabilityof‘hedgecue-ness’:P(spec|xk)=P(xk|spec)·P(spec)PNn=1P(yn)P(xk|yn)(7)whichcanbecomputeddirectlyusing(4)and(6).Themmostprobablefeaturesarethenselectedfromeachsentencetocompute(5)andtherestareig-nored.Thishasthedualbeneﬁtofremovingirrele-vantfeaturesandalsoreducingdependencebetweenfeatures,astheselectedfeatureswilloftenbenon-localandthusnottootightlycorrelated.Notethatthisideadiffersfromtraditionalfeatureselectionintwoimportantways:1.Onlyfeaturesindicativeofthespecclassareretained,ortoputitanotherway,nspecclassmembershipisinferredfromtheabsenceofstrongspecfeatures.2.Featureselectioninthiscontextisnotaprepro-cessingstep;i.e.thereisnore-estimationafterselection.Thishasthepotentiallydetrimentalsideeffectofskewingtheposteriorestimatesinfavourofthespecclass,butisadmissibleforthepurposesofrankingandclassiﬁcationbyposteriorthresholding(seenextsection).9ClassiﬁcationTheweaklysupervisedlearnerreturnsalabelleddatasetforeachclass,fromwhichaclassiﬁercanbetrained.Wecaneasilyderiveaclassiﬁerusingtheestimatesfromourlearningmodelby:xj→specifP(spec|xj)>σ(8)whereσisanarbitrarythresholdusedtocontroltheprecision/recallbalance.Forcomparisonpurposes,wealsouseJoachims’SVMlight(Joachims,1999).10ExperimentalEvaluation10.1MethodToexaminethepracticalefﬁcacyofthelearningandclassiﬁcationmodelswehavepresented,weusethefollowingexperimentalmethod:1.Generateseedtrainingdata:SspecandSnspec2.Initialise:Tspec←SspecandTnspec←Snspec3.Iterate:•OrderUbyP(spec|xj)(expression3)•Tspec←mostprobablebatch•Tnspec←leastprobablebatch•TrainclassiﬁerusingTspecandTnspec997

Rankα=0α=1α=5α=100α=5001interactswithsuggestsuggestsuggestsuggest2TAFblikelylikelylikelylikely3sextamaymaymaymay4CRYsmightmightTheseThese5DsRedseemsseemsresultsresults6Cell-NonautonomoussuggestsTakenmightthat7arvaprobablysuggestsobservationsbe8inter-homologuesuggestingprobablyTakendata9MohantypossiblyTogetherﬁndingsit10meldsuggestedsuggestingOurOur11aDNATakenpossiblyseemsobservations12Deerunlikelysuggestedtogetherrole13BorelTogetherﬁndingsTogethermost14substripephysiologyobservationsrolethese15FailingmodulatedGiventhattogetherTable2:FeaturesrankedbyP(spec|xk)forvaryingα•Computespecrecall/precisionBEP(break-evenpoint)onthetestdataThebatchsizeforeachiterationissetto0.001∗|U|.Aftereachlearningiteration,wecomputethepreci-sion/recallBEPforthespecclassusingbothclas-siﬁerstrainedonthecurrentlabelleddata.WeuseBEPbecauseithelpstomitigateagainstmisleadingresultsduetodiscrepanciesinclassiﬁcationthresh-oldplacement.Disadvantageously,BEPdoesnotmeasureaclassiﬁer’sperformanceacrossthewholeoftherecall/precisionspectrum(ascanbeobtained,forinstance,fromreceiver-operatingcharacteristic(ROC)curves),butforourpurposesitprovidesaclear,abstractedoverviewofaclassiﬁer’saccuracygivenaparticulartrainingset.10.2ParameterSettingThetrainingandclassiﬁcationmodelswehavepre-sentedrequirethesettingoftwoparameters:thesmoothingparameterαandthenumberoffeaturespersamplem.Analysisoftheeffectofvaryingαonfeaturerankingrevealsthatwhenα=0,lowfre-quencytermswithspuriousclasscorrelationdom-inateandasαincreases,highfrequencytermsbe-comeincreasinglydominant,eventuallysmoothingawaygenuinelow-to-midfrequencycorrelations.ThiseffectisillustratedinTable2,andfromthisanalysiswechoseα=5asanappropriatelevelofsmoothing.Weusem=5basedontheintuitionthatﬁveisaroughupperboundonthenumberofhedgecuefeatureslikelytooccurinanyonesentence.WeusethelinearkernelforSVMlightwiththedefaultsettingfortheregularizationparameterC.Weconstructbinaryvalued,L2-normalised(unitlength)inputvectorstorepresenteachsentence,asthisresultedinbetterperformancethanusingfrequency-basedweightsandconcordswithourpresence/absencefeatureestimates.10.3SeedGenerationThelearningmodelwehavepresentedrequiresasetofseedsforeachclass.Togenerateseedsforthespecclass,weextractedallsentencesfromUcontainingeither(orboth)ofthetermssuggestorlikely,astheseareverygood(thoughnotperfect)hedgecues,yielding6423specseeds.Generatingseedsfornspecismuchmoredifﬁcult,asintegrityrequirestheabsenceofhedgecues,andthiscannotbedoneautomatically.Thus,weusedthefollowingproceduretoobtainasetofnspecseeds:1.CreateinitialSnspecbysamplingrandomlyfromU.2.Manuallyremovemore‘obvious’speculativesentencesusingpatternmatching3.Iterate:•OrderSnspecbyP(spec|xj)usingesti-matesfromSspecandcurrentSnspec•Examinemostprobablesentencesandre-movespeculativeinstancesWestartedwith8830sentencesandafteracoupleofhoursworkreducedthisdowntoa(stillpotentiallynoisy)nspecseedsetof7541sentences.998

 0.58 0.6 0.62 0.64 0.66 0.68 0.7 0.72 0.74 0.76 0.78 0.8 0 20 40 60 80 100 120 140BEPIterationProb (Prob)Prob (SVM)SVM (Prob)SVM (SVM)BaselineProb(Prob)denotesourprobabilisticlearningmodelandclassiﬁer(§9)Prob(SVM)denotesprobabilisticlearningmodelwithSVMclassiﬁerSVM(Prob)denotescommittee-basedmodel(§10.4)withprobabilisticclassiﬁerSVM(SVM)denotescommittee-basedmodelwithSVMclassiﬁerBaselinedenotessubstringmatchingclassiﬁerof(Lightetal.,2004)Figure1:Learningcurves10.4BaselinesAsabaselineclassiﬁerweusethesubstringmatch-ingtechniqueof(Lightetal.,2004),whichlabelsasentenceasspecifitcontainsoneormoreofthefollowing:suggest,potential,likely,may,atleast,inpart,possibl,furtherinvestigation,unlikely,pu-tative,insights,pointtoward,promiseandpropose.Toprovideacomparisonforourlearningmodel,weimplementamoretraditionalself-trainingpro-cedureinwhichateachiterationacommitteeofﬁveSVMsistrainedonrandomlygeneratedoverlappingsubsetsofthetrainingdataandtheircumulativecon-ﬁdenceisusedtoselectitemsforaugmentingthelabelledtrainingdata.Forsimilarworksee(BankoandBrill,2001;Zhang,2004).10.5ResultsFigure1plotsaccuracyasafunctionofthetrain-ingiteration.After150iterations,alloftheweaklysupervisedlearningmodelsaresigniﬁcantlymoreaccuratethanthebaselineaccordingtoabinomialsigntest(p<0.01),thoughthereisclearlystillmuchroomforimprovement.Thebaselineclassi-ﬁerachievesaBEPof0.60whilebothclassiﬁersusingourlearningmodelreachapproximately0.76BEPwithlittletotellbetweenthem.Interestingly,thecombinationoftheSVMcommittee-basedlearn-ingmodelwithourclassiﬁer(denotedby‘SVM(Prob)’),performscompetitivelywithbothoftheap-proachesthatuseourprobabilisticlearningmodelandsigniﬁcantlybetterthantheSVMcommittee-basedlearningmodelwithanSVMclassiﬁer,‘SVM(SVM)’,accordingtoabinomialsigntest(p<0.01)after150iterations.Theseresultssuggestthatper-formancemaybeenhancedwhenthelearningandclassiﬁcationtasksarecarriedoutbydifferentmod-els.Thisisaninterestingpossibility,whichwein-tendtoexplorefurther.Animportantissueinincrementallearningsce-nariosisidentiﬁcationoftheoptimumstoppingpoint.Variousmethodshavebeeninvestigatedtoad-dressthisproblem,suchas‘counter-training’(Yan-garber,2003)andcommitteeagreement(Zhang,2004);howsuchideascanbeadaptedforthistaskisoneofmanyavenuesforfutureresearch.10.6ErrorAnalysisSomeerrorsareduetothevarietyofhedgeforms.Forexample,thelearningmodelswereunsuccess-fulinidentifyingassertivestatementsofknowledgepaucity,eg:Thereisnoclearevidenceforcy-tochromecreleaseduringapoptosisinCelegansorDrosophila.Whetheritispossibletolearnsuchexampleswithoutadditionalseedinformationisanopenquestion.Thisexamplealsohighlightsthepo-tentialbeneﬁtofanenrichedsamplerepresentation,inthiscaseonewhichaccountsforthenegationofthephrase‘clearevidence’whichotherwisemightsuggestastronglynon-speculativeassertion.Inmanycaseshedgeclassiﬁcationischallengingevenforahumanannotator.Forinstance,distin-guishingbetweenaspeculativeassertionandonerelatingtoapatternofobservednon-universalbe-haviourisoftendifﬁcult.Thefollowingexamplewaschosenbythelearnerasaspecsentenceonthe150thtrainingiteration:Eachcomponentconsistsofasetofsubcomponentsthatcanbelocalizedwithinalargerdistributedneuralsystem.Thesentencedoesnot,infact,containahedgebutratherastate-mentofobservednon-universalbehaviour.How-ever,analmostidenticalvariantwith‘could’insteadof‘can’wouldbeastrongspeculativecandidate.Thishighlightsthesimilaritybetweenmanyhedgeandnon-hedgeinstances,whichmakessuchcaseshardtolearninaweaklysupervisedmanner.999

11ConclusionsandFutureWorkWehaveshownthatweaklysupervisedMLisap-plicabletotheproblemofhedgeclassiﬁcationandthatareasonablelevelofaccuracycanbeachieved.Theworkpresentedherehasapplicationinthewideracademiccommunity;infactakeymotivationinthisstudyistoincorporatehedgeclassiﬁcationintoaninteractivesystemforaidingcuratorsinthecon-structionandpopulationofgenedatabases.Wehavepresentedourinitialresultsonthetaskusingasim-pleprobabilisticmodelinthehopethatthiswillencourageotherstoinvestigatealternativelearningmodelsandpursuenewtechniquesforimprovingac-curacy.Ournextaimistoexplorepossibilitiesofintroducinglinguistically-motivatedknowledgeintothesamplerepresentationtohelpthelearneridentifykeyhedge-relatedsententialcomponents,andalsotoconsiderhedgeclassiﬁcationatthegranularityofas-sertionsratherthantextsentences.AcknowledgementsThisworkwaspartiallysupportedbytheFlySlipproject,BBSRCGrantBBS/B/16291,andwethankNikiforosKaramanisandRuthSealforthoroughan-notationandhelpfuldiscussion.TheﬁrstauthorissupportedbyanUniversityofCambridgeMillen-niumScholarship.ReferencesRonArtsteinandMassimoPoesio.2005.Kappa3=al-pha(orbeta).Technicalreport,UniversityofEssexDepartmentofComputerScience.MicheleBankoandEricBrill.2001.Scalingtoveryverylargecorporafornaturallanguagedisambiguation.InMeetingoftheAssociationforComputationalLinguis-tics,pages26–33.AvrimBlumandTomMitchell.1998.Combiningla-belledandunlabelleddatawithco-training.InPro-ceedingsofCOLT’98,pages92–100,NewYork,NY,USA.ACMPress.JinxiuChen,DonghongJi,ChewL.Tan,andZhengyuNiu.2006.Relationextractionusinglabelpropaga-tionbasedsemi-supervisedlearning.InProceedingsofACL’06,pages129–136.M.CollinsandY.Singer.1999.Unsupervisedmod-elsfornamedentityclassiﬁcation.InProceedingsoftheJointSIGDATConferenceonEmpiricalMethodsinNLPandVeryLargeCorpora.GeorgeHripcsakandAdamRothschild.2004.Agree-ment,thef-measure,andreliabilityininformationre-trieval.JAmMedInformAssoc.,12(3):296–298.K.Hyland.1994.Hedginginacademicwritingandeaptextbooks.EnglishforSpeciﬁcPurposes,13:239–256.ThorstenJoachims.1999.Makinglarge-scalesup-portvectormachinelearningpractical.InA.SmolaB.Sch¨olkopf,C.Burges,editor,AdvancesinKernelMethods:SupportVectorMachines.MITPress,Cam-bridge,MA.M.Light,X.Y.Qiu,andP.Srinivasan.2004.Thelan-guageofbioscience:Facts,speculations,andstate-mentsinbetween.InProceedingsofBioLink2004WorkshoponLinkingBiologicalLiterature,Ontolo-giesandDatabases:ToolsforUsers,Boston,May2004.DavidMcClosky,EugeneCharniak,andMarkJohnson.2006.Effectiveself-trainingforparsing.InHLT-NAACL.VincentNgandClaireCardie.2003.Weaklysupervisednaturallanguagelearningwithoutredundantviews.InProceedingsofNAACL’03,pages94–101,Morris-town,NJ,USA.K.NigamandR.Ghani.2000.Understandingthebe-haviorofco-training.InProceedingsofKDD-2000WorkshoponTextMining.KamalNigam,AndrewK.McCallum,SebastianThrun,andTomM.Mitchell.2000.TextclassiﬁcationfromlabeledandunlabeleddocumentsusingEM.MachineLearning,39(2/3):103–134.EllenRiloff,JanyceWiebe,andTheresaWilson.2003.Learningsubjectivenounsusingextractionpatternbootstrapping.InSeventhConferenceonNaturalLan-guageLearning(CoNLL-03).ACLSIGNLL.,pages25–32.BenWellner.2005.Weaklysupervisedlearningmeth-odsforimprovingthequalityofgenenamenormal-izationdata.InProceedingsoftheACL-ISMBWork-shoponLinkingBiologicalLiterature,OntologiesandDatabases,pages1–8,Detroit,June.AssociationforComputationalLinguistics.JanyceWiebe,TheresaWilson,RebeccaBruce,MatthewBell,andMelanieMartin.2004.Learningsubjectivelanguage.Comput.Linguist.,30(3):277–308.RomanYangarber.2003.Counter-trainingindiscoveryofsemanticpatterns.InProceedingsofACL’03,pages343–350,Morristown,NJ,USA.DavidYarowsky.1995.Unsupervisedwordsensedis-ambiguationrivalingsupervisedmethods.InPro-ceedingsofACL’95,pages189–196,Morristown,NJ,USA.ACL.ZhuZhang.2004.Weakly-supervisedrelationclas-siﬁcationforinformationextraction.InCIKM’04:ProceedingsofthethirteenthACMinternationalcon-ferenceonInformationandknowledgemanagement,pages581–588,NewYork,NY,USA.ACMPress.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 1000–1007,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

1000

TextAnalysisforAutomaticImageAnnotationKoenDeschachtandMarie-FrancineMoensInterdisciplinaryCentreforLaw&ITDepartmentofComputerScienceKatholiekeUniversiteitLeuvenTiensestraat41,3000Leuven,Belgium{koen.deschacht,marie-france.moens}@law.kuleuven.ac.beAbstractWepresentanovelapproachtoautomati-callyannotateimagesusingassociatedtext.Wedetectandclassifyallentities(personsandobjects)inthetextafterwhichwede-terminethesalience(theimportanceofanentityinatext)andvisualness(theextenttowhichanentitycanbeperceivedvisually)oftheseentities.Wecombinethesemea-surestocomputetheprobabilitythatanen-tityispresentintheimage.Thesuitabilityofourapproachwassuccessfullytestedon100image-textpairsofYahoo!News.1IntroductionOursocietydealswithagrowingbulkofun-structuredinformationsuchastext,imagesandvideo,asituationwitnessedinmanydomains(news,biomedicalinformation,intelligenceinformation,businessdocuments,etc.).Thisgrowthcomesalongwiththedemandformoreeffectivetoolstosearchandsummarizethisinformation.Moreover,thereistheneedtomineinformationfromtextsandimageswhentheycontributetodecisionmakingbygov-ernments,businessesandotherinstitutions.Thecapabilitytoaccuratelyrecognizecontentinthesesourceswouldlargelycontributetoimprovedindex-ing,classiﬁcation,ﬁltering,miningandinterroga-tion.Algorithmsandtechniquesforthedisclosureofinformationfromthedifferentmediahavebeende-velopedforeverymediumindependentlyduringthelastdecennium,butonlyrecentlytheinterplaybe-tweenthesedifferentmediahasbecomeatopicofinterest.Oneofthepossibleapplicationsistohelpanalysisinonemediumbyemployinginformationfromanothermedium.Inthispaperwestudytextthatisassociatedwithanimage,suchasforinstanceimagecaptions,videotranscriptsorsurroundingtextinawebpage.Wedeveloptechniquesthatextractinformationfromthesetextstohelpwiththedifﬁ-culttaskofaccurateobjectrecognitioninimages.Althoughimagesandassociatedtextsnevercontainpreciselythesameinformation,inmanysituationstheassociatedtextoffersvaluableinformationthathelpstointerprettheimage.ThecentralobjectiveoftheCLASSproject1istodevelopadvancedlearningmethodsthatallowima-ges,videoandassociatedtexttobeautomaticallyanalyzedandstructured.Inthispaperwetestthefeasibilityofautomaticallyannotatingimagesbyus-ingtextualinformationinnear-parallelimage-textpairs,inwhichmostofthecontentoftheimagecorrespondstocontentofthetextandviceversa.Wewillfocusonentitiessuchaspersonsandob-jects.Wewillherebytakeintoaccountthetext’sdis-coursestructureandsemantics,whichallowamoreﬁne-grainedidentiﬁcationofwhatcontentmightbepresentintheimage,andwillenrichourmodelwithworldknowledgethatisnotpresentinthetext.Wewillﬁrstdiscussthecorpusonwhichweap-plyandtestourtechniquesinsection2,afterwhichweoutlinewhattechniqueswehavedeveloped:westartwithabaselinesystemtoannotateimageswithpersonnames(section3)andimprovethisbycom-putingtheimportanceofthepersonsinthetext(sec-tion4).Wewillthenextendthemodeltoincludeall1http://class.inrialpes.fr/1001

HiramMyers,ofEdmond,Okla.,walksacrossthefence,attemptingtodeliverwhathecalleda’people’sindictment’ofHalliburtonCEODavidLesar,outsidethesiteoftheannualHalliburtonshareholdersmeetinginDuncan,Okla.,leadingtohisarrest,Wednesday,May17,2006.Figure1:Image-textpairwithentity“HiramMyers”appearingbothinthetextandintheimage.typesofobjects(section5)andimproveitbydeﬁn-ingandcomputingthevisualnessmeasure(section6).Finallywewillcombinethesedifferenttech-niquesinoneprobabilisticmodelinsection7.2TheparallelcorpusWehavecreatedaparallelcorpusconsistingof1700image-textpairs,retrievedfromtheYahoo!Newswebsite2.Everyimagehasanaccompanyingtextwhichdescribesthecontentoftheimage.Thistextwillingeneraldiscussoneormorepersonsintheimage,possiblyoneormoreotherobjects,theloca-tionandtheeventforwhichthepicturewastaken.Anexampleofanimage-textpairisgiveninﬁg.1.Notallpersonsorobjectswhoarepicturedintheimagesarenecessarilydescribedinthetexts.Theinverseisalsotrue,i.e.contentmentionedinthetextmaynotbepresentintheimage.Wehaverandomlyselected100text-pairsfromthecorpus,andoneannotatorhaslabeledeveryimage-textpairwiththeentities(i.e.personsand2http://news.yahoo.com/otherobjects)thatappearbothintheimageandinthetext.Forexample,theimage-textpairshowninﬁg.1isannotatedwithoneentity,”HiramMyers”,sincethisistheonlyentitythatappearsbothinthetextandintheimage.Onaveragethesetextscontain15.04entities,ofwhich2.58appearintheimage.Tobuildtheappearancemodelofthetext,wehavecombineddifferenttools.Wewillevaluateeverytoolseparatelyon100image-textpairs.Thiswaywehaveadetailedviewonthenatureoftheerrorsintheﬁnalmodel.3AutomaticallyannotatingpersonnamesGivenatextthatisassociatedwithanimage,wewanttocomputeaprobabilisticappearancemodel,i.e.acollectionofentitiesthatarevisibleintheimage.Wewillstartwithamodelthatholdsthenamesofthepersonsthatappearintheimage,suchaswasdoneby(Satohetal.,1999;Bergetal.,2004),andextendthismodelinsection5toincludeallotherobjects.3.1NamedEntityRecognitionAlogicalﬁrststeptodetectpersonnamesisNamedEntityRecognition(NER).WeusetheOpenNLPpackage3,whichdetectsnounphrasechunksinthesentencesthatrepresentpersons,locations,organi-zationsanddates.Toimprovetherecognitionofpersonnames,weuseadictionaryofnames,whichwehaveextractedfromtheWikipedia4website.WehavemanuallyevaluatedperformanceofNERonourtestcorpusandfoundthatperformancewassa-tisfying:weobtainedaprecisionof93.37%andare-callof97.69%.Precisionisthepercentageofiden-tiﬁedpersonnamesbythesystemthatcorrespondstocorrectpersonnames,andrecallisthepercentageofpersonnamesinthetextthathavebeencorrectlyidentiﬁedbythesystem.Thetextscontainasmallnumberofnounphrasecoreferentsthatareintheformofpronouns,wehaveresolvedtheseusingtheLingPipe5package.3.2BaselinesystemWewanttoannotateanimageusingtheassociatedtext.Wetrytoﬁndthenamesofpersonswhichare3http://opennlp.sourceforge.net/4http://en.wikipedia.org/5http://www.alias-i.com/lingpipe/1002

bothdescribedinthetextandvisibleintheimage,andwewanttodosobyrelyingonlyonananalysisofthetext.Insomecases,suchasthefollowingexample,thetextstatesexplicitlywhetherapersonis(not)visibleintheimage:PresidentBush[...]withDanishPrimeMinisterAndersFoghRasmussen,notpictured,atCampDavid[...].Developingasystemthatcouldextractthisinforma-tionisnottrivial,andevenifwecoulddoso,onlyaverysmallpercentageofthetextsinourcorpuscon-tainthiskindofinformation.Inthenextsectionwewilllookintoamethodthatisapplicabletoawiderangeof(descriptive)textsandthatdoesnotrelyonspeciﬁcinformationwithinthetext.Toevaluatetheperformanceofthissystem,wewillcompareitwithasimplebaselinesystem.Thebaselinesystemassumesthatallpersonsinthetextarevisibleintheimage,whichresultsinaprecisionof71.27%andarecallof95.56%.The(low)preci-sioncanbeexplainedbythefactthatthetextsoftendiscusspeoplewhicharenotpresentintheimage.4DetectionofthesalienceofapersonNotallpersonsdiscussedinatextareequallyim-portant.Wewouldliketodiscoverwhatpersonsareinthefocusofatextandwhatpersonsareonlymentionedbrieﬂy,becausewepresumethatmoreimportantpersonsinthetexthavealargerproba-bilityofappearingintheimagethanlessimportantpersons.Becauseoftheshortlengthsofthedocu-mentsinourcorpus,ananalysisoflexicalcohesionbetweentermsinthetextwillnotbesufﬁcientfordistinguishingbetweenimportantandlessimportantentities.Wedeﬁneameasure,salience,whichisanumberbetween0and1thatrepresentstheimpor-tanceofanentityinatext.Wepresenthereamethodforcomputingthisscorebasedonanindepthana-lysisofthediscourseofthetextandofthesyntacticstructureoftheindividualsentences.4.1DiscoursesegmentationThediscoursesegmentationmodule,whichwede-velopedinearlierresearch,hierarchicallyandse-quentiallysegmentsthediscourseindifferenttopicsandsubtopicsresultinginatableofcontentsofatext(Moens,2006).Thetableshowsthemainen-titiesandtherelatedsubtopicentitiesinatree-likestructurethatalsoindicatesthesegments(bymeansofcharacterpointers)towhichanentityapplies.Thealgorithmdetectspatternsofthematicprogressionintextsandcanthusrecognizethemaintopicofasen-tence(i.e.,aboutwhomorwhatthesentencespeaks)andthehierarchicalandsequentialrelationshipsbe-tweenindividualtopics.Amixturemodel,takingintoaccountdifferentdiscoursefeatures,istrainedwiththeExpectationMaximizationalgorithmonanannotatedDUC-2003corpus.Weusetheresultingdiscoursesegmentationtodeﬁnethesalienceofin-dividualentitiesthatarerecognizedastopicsofasentence.Wecomputeforeachnounentityerinthediscourseitssalience(Sal1)inthediscoursetree,whichisproportionalwiththedepthoftheentityinthediscoursetree-herebyassumingthatdeeperinthistreemoredetailedtopicsofatextaredescribed-andnormalizethisvaluetobebetweenzeroandone.Whenanentityoccursindifferentsubtrees,itsmax-imumscoreischosen.4.2ReﬁnementwithsentenceparseinformationBecausenotallentitiesofthetextarecapturedinthediscoursetree,weimplementanadditionalreﬁne-mentofthecomputationofthesalienceofanentitywhichisinspiredby(Moensetal.,2006).Theseg-mentationmodulealreadydeterminesthemaintopicofasentence.Sincethesyntacticstructureisoftenindicativeoftheinformationdistributioninasen-tence,wecandeterminetherelativeimportanceoftheotherentitiesinasentencebyrelyingonthere-lationshipsbetweenentitiesassignaledbytheparsetree.Whendeterminingthesalienceofanentity,wetakeintoaccounttheleveloftheentitymentionintheparsetree(Sal2),andthenumberofchildrenfortheentityinthisstructure(Sal3),wherethenormal-izedscoreisrespectivelyinverselyproportionalwiththedepthoftheparsetreewheretheentityoccurs,andproportionalwiththenumberofchildren.Wecombinethethreesaliencevalues(Sal1,Sal2andSal3)byusingalinearweighting.Wehaveexperimentallydeterminedreasonablecoefﬁ-cientsforthesethreevalues,whicharerespectively0.8,0.1and0.1.Eventually,wecouldlearnthesecoefﬁcientsfromatrainingcorpus(e.g.,withthe1003

PrecisionRecallF-measureNER71.27%95.56%81.65%NER+DYN97.66%92.59%95.06%Table1:Comparisonofmethodstopredictwhatper-sonsdescribedinthetextwillappearintheimage,usingNamedEntityRecognition(NER),andthesaliencemeasurewithdynamiccut-off(DYN).ExpectationMaximizationalgorithm).Wedonotseparatelyevaluateourtechnologyforsaliencedetectionasthistechnologywasalreadyextensivelyevaluatedinthepast(Moens,2006).4.3EvaluatingtheimprovedsystemThesaliencemeasuredeﬁnesarankingofallthepersonsinatext.Wewillusethisrankingtoimproveourbaselinesystem.Weassumethatitispossibletoautomaticallydeterminethenumberoffacesthatarerecognizedintheimage,whichgivesusanindi-cationofasuitablecut-offvalue.Thisapproachisreasonablesincefacedetection(determinewhetherafaceispresentintheimage)issigniﬁcanteasierthanfacerecognition(determinewhichpersonispresentintheimage).Intheimprovedmodelweassumethatpersonswhicharerankedhigherthan,orequalto,thecut-offvalueappearintheimage.Forex-ample,if4facesappearintheimage,weassumethatonlythe4personsofwhichthenamesinthetexthavebeenassignedthehighestsalienceappearintheimage.Weseefromtable1thattheprecision(97.66%)hasimproveddrastically,whiletherecallremainedhigh(92.59%).Thisconﬁrmsthehypoth-esisthatdeterminingthefocusofatexthelpsinde-terminingthepersonsthatappearintheimage.5AutomaticallyannotatingpersonsandobjectsAfterhavingdevelopedareasonablesuccessfulsys-temtodetectwhatpersonswillappearintheimage,weturntoamoredifﬁcultcase:Detectingpersonsandallotherobjectsthataredescribedinthetext.5.1EntitydetectionWewillﬁrstdetectwhatwordsinthetextrefertoanentity.Forthis,weperformpart-of-speechtagging(i.e.,detectingthesyntacticwordclasssuchasnoun,verb,etc.).Wetakethateverynouninthetextrep-resentsanentity.WehaveusedLTPOS(Mikheev,1997),whichperformedthetaskalmosterrorless(precisionof98.144%andrecallof97.36%onthenounsinthetestcorpus).PersonnameswhichweresegmentedusingtheNERpackagearealsomarkedasentities.5.2BaselinesystemWewanttodetecttheobjectsandthenamesofper-sonswhicharebothvisibleintheimageandde-scribedinthetext.Westartwithasimplebaselinesystem,inwhichweassumethateveryentityinthetextappearsintheimage.Ascanbeexpected,thisresultsinahighrecall(91.08%),andaverylowpre-cision(15.62%).Weseethattheproblemhereisfarmoredifﬁcultcomparedtodetectingonlyper-sonnames.Thiscanbeexplainedbythefactthatmanyentities(suchasforexampleAugust,ideaandhistory)willnever(oronlyindirectly)appearinanimage.Inthenextsectionwewilltrytodeterminewhattypesofentitiesaremorelikelytoappearintheimage.6DetectionofthevisualnessofanentityTheassumptionthateveryentityinthetextappearsintheimageisrathercrude.Wewillenrichourmodelwithexternalworldknowledgetoﬁndenti-tieswhicharenotlikelytoappearinanimage.Wedeﬁneameasurecalledvisualness,whichisdeﬁnedastheextenttowhichanentitycanbeperceivedvi-sually.6.1EntityclassiﬁcationAfterwehaveperformedentitydetection,wewanttoclassifyeveryentityaccordingtoacertainseman-ticdatabase.WeusetheWordNet(Fellbaum,1998)database,whichorganizesEnglishnouns,verbs,ad-jectivesandadverbsinsynsets.Asynsetisacol-lectionofwordsthathaveaclosemeaningandthatrepresentanunderlyingconcept.Anexampleofsuchasynsetis“person,individual,someone,some-body,mortal,soul”.Allthesewordsrefertoahu-1004

manbeing.Inordertocorrectlyassignanouninatexttoitssynset,i.e.,todisambiguatethesenseofthisword,weuseanefﬁcientWordSenseDis-ambiguation(WSD)systemthatwasdevelopedbytheauthorsandwhichisdescribedin(DeschachtandMoens,2006).PropernamesarelabeledbytheNamedEntityRecognizer,whichrecognizesper-sons,locationsandorganizations.TheselabelsinturnallowustoassignthecorrespondingWordNetsynset.ThecombinationoftheWSDsystemandtheNERpackageachieveda75.97%accuracyinclassi-fyingtheentities.Apartfromerrorsthatresultedfromerroneousentitydetection(32.32%),errorsweremainlyduetotheWSDsystem(60.56%)andinasmalleramounttotheNERpackage(8.12%).6.2WordNetsimilarityWedeterminethevisualnessforeverysynsetus-ingamethodthatwasinspiredbyKampsandMarx(2002).KampsandMarxuseadistancemeasuredeﬁnedontheadjectivesoftheWordNetdatabasetogetherwithtwoseedadjectivestodeterminetheemotiveoraffectivemeaningofanygivenadjective.Theycomputetherelativedistanceoftheadjectivetotheseedsynsets“good”and“bad”andusethisdistancetodeﬁneameasureofaffectivemeaning.Wetakeasimilarapproachtodeterminethevisu-alnessofagivensynset.WeﬁrstdeﬁneasimilaritymeasurebetweensynsetsintheWordNetdatabase.Thenweselectasetofseedsynsets,i.e.synsetswithapredeﬁnedvisualness,andusethesimilarityofagivensynsettotheseedsynsetstodeterminethevisualness.6.3DistancemeasureTheWordNetdatabasedeﬁnesdifferentrelationsbe-tweenitssynsets.Animportantrelationfornounsisthehypernym/hyponymrelation.AnounXisahy-pernymofanounYifYisasubtypeorinstanceofX.Forexample,“bird”isahypernymof“penguin”(and“penguin”isahyponymof“bird”).AsynsetinWordNetcanhaveoneormorehypernyms.Thisrelationorganizesthesynsetsinahierarchicaltree(Hayes,1999).ThesimilaritymeasuredeﬁnedbyLin(1998)usesthehypernym/hyponymrelationtocomputease-manticsimilaritybetweentwoWordNetsynsetsS1andS2.Firstitﬁndsthemostspeciﬁc(lowestinthetree)synsetSpthatisaparentofbothS1andS2.ThenitcomputesthesimilarityofS1andS2assim(S1,S2)=2logP(Sp)logP(S1)+logP(S2)HeretheprobabilityP(Si)istheprobabilityoflabelinganywordinatextwithsynsetSiorwithoneofthedescendantsofSiintheWordNethier-archy.WeestimatetheseprobabilitiesbycountingthenumberofoccurrencesofasynsetintheSem-corcorpus(Fellbaum,1998;Landesetal.,1998),whereallnounchunksarelabeledwiththeirWord-Netsynset.TheprobabilityP(Si)iscomputedasP(Si)=C(Si)PNn=1C(Sn)+PKk=1P(Sk)whereC(Si)isthenumberofoccurrencesofSi,NisthetotalnumberofsynsetsinWordNetandKisthenumberofchildrenofSi.TheWord-Net::Similaritypackage(Pedersenetal.,2004)im-plementsthisdistancemeasureandwasusedbytheauthors.6.4SeedsynsetsWehavemanuallyselected25seedsynsetsinWord-Net,wherewetriedtocoverthewiderangeoftopicswewerelikelytoencounterinthetestcorpus.Wehavesetthevisualnessoftheseseedsynsetstoeither1(visual)or0(notvisual).Wedeterminethevisu-alnessofallothersynsetsusingtheseseedsynsets.Asynsetthatisclosetoavisualseedsynsetgetsahighvisualnessandviceversa.Wechoosealinearweighting:vis(s)=Xivis(si)sim(s,si)C(s)wherevis(s)returnsanumberbetween0and1de-notingthevisualnessofasynsets,siaretheseedsynsets,sim(s,t)returnsanumberbetween0and1denotingthesimilaritybetweensynsetssandtandC(s)isconstantgivenasynsets:C(s)=Xisim(s,si)1005

6.5EvaluationofthevisualnesscomputationTodeterminethevisualness,weﬁrstassignthecor-rectWordNetsynsettoeveryentity,afterwhichwecomputeavisualnessscoreforthesesynsets.Sincethesescoresareﬂoatingpointnumbers,theyarehardtoevaluatemanually.Duringevaluation,wemakethesimplifyingassumptionthatallentitieswithavisualnessbelowacertainthresholdarenotvisual,andallentitiesabovethisthresholdarevi-sual.Wechoosethisthresholdtobe0.5.Thisre-sultsinanaccuracyof79.56%.Errorsaremainlycausedbyerroneousentitydetectionandclassiﬁca-tion(63.10%)butalsobecauseofanincorrectas-signmentofthevisualness(36.90%)bythemethoddescribedabove.7CreatinganappearancemodelusingsalienceandvisualnessIntheprevioussectionwehavecreatedamethodtocalculateavisualnessscoreforeveryentity,becausewestatedthatremovingtheentitieswhichcanneverbeperceivedvisuallywillimprovetheperformanceofourbaselinesystem.Anexperimentprovesthatthisisexactlythecase.Ifweassumethatonlytheentitiesthathaveavisualnessabovea0.5thresh-oldarevisibleandwillappearintheimage,wegetaprecisionof48.81%andarecallof87.98%.Weseefromtable2thatthisisalreadyasigniﬁcantim-provementoverthebaselinesystem.Insection4wehaveseenthatthesaliencemea-surehelpsindeterminingwhatpersonsarevisibleintheimage.Wehaveusedthefactthatfacedetectioninimagesisrelativelyeasilyandcanthussupplyacut-offvaluefortherankedpersonnames.Inthepresentstate-of-the-art,wearenotabletoexploitasimilarfactwhendetectingalltypesofentities.Wewillthususethesaliencemeasureinadifferentway.Wecomputethesalienceofeveryentity,andweassumethatonlytheentitieswithasaliencescoreaboveathresholdof0.5willappearintheimage.Weseethatthismethoddrasticallyimprovespreci-sionto66.03%,butalsolowersrecalluntil54.26%.Wenowcreatealastmodelwherewecombineboththevisualnessandthesaliencemeasures.Wewanttocalculatetheprobabilityoftheoccurrenceofanentityeimintheimage,givenatextt,P(eim|t).WeassumethatthisprobabilityisproportionalwithPrecisionRecallF-measureEnt15.62%91.08%26.66%Ent+Vis48.81%87.98%62.78%Ent+Sal66.03%54.26%59.56%Ent+Vis+Sal70.56%67.82%69.39%Table2:Comparisonofmethodstopredicttheen-titiesthatappearintheimage,usingentitydetec-tion(Ent),andthevisualness(Vis)andsalience(Sal)measures.thedegreeofvisualnessandsalienceofeimint.Inourframework,P(eim|t)iscomputedastheproductofthesalienceoftheentityeimanditsvisualnessscore,asweassumebothscorestobeindependent.Again,forevaluationsake,wechooseathresholdof0.4totransformthiscontinuousrankingintoabinaryclassiﬁcation.Thisresultsinaprecisionof70.56%andarecallof67.82%.Thismodelisthebestofthe4modelsforentityannotationwhichhavebeenevaluated.8RelatedResearchUsingtextthataccompaniestheimageforannotat-ingimagesandfortrainingimagerecognitionisnotnew.Theearliestwork(onlyonpersonnames)isbySatoh(1999)andthisresearchcanbeconsideredastheclosesttoourwork.Theauthorsmakeadis-tinctionbetweenpropernames,commonnounsandotherwords,anddetectentitiesbasedonathesauruslistofpersons,socialgroupsandotherwords,thusexploitingalreadysimplesemantics.Alsoarudi-mentaryapproachtodiscourseanalysisisfollowedbytakingintoaccountthepositionofwordsinatext.Theresultswerenotsatisfactory:752wordswereextractedfromvideoascandidatesforbeingintheaccompanyingimages,butonly94werecorrectwhere658werefalsepositives.Morietal.(2000)learntextualdescriptionsofimagesfromsurround-ingtexts.Theseauthorsﬁlternounsandadjectivesfromthesurroundingtextswhentheyoccuraboveacertainfrequencyandobtainamaximumhitrateoftop3wordsthatissituatedbetween30%and40%.Otherapproachesconsiderboththetextualandimagefeatureswhenbuildingacontentmodeloftheimage.Forinstance,somecontentisselectedfromthetext(suchaspersonnames)andfromthe1006

image(suchasfaces)andbothcontributeindescrib-ingthecontentofadocument.ThisapproachwasfollowedbyBarnard(2003).Westerveld(2000)combinesimagefeaturesandwordsfromcollateraltextintoonesemanticspace.ThisauthorusesLatentSemanticIndexingforrep-resentingtheimage/textpaircontent.Ayacheetal.(2005)classifyvideodataintodifferenttopicalcon-cepts.Theresultsoftheseapproachesareoftendis-appointing.Themethodshererepresentthetextasabagofwordspossiblyaugmentedwithatf(termfre-quency)xidf(inversedocumentfrequency)weightofthewords(Amiretal.,2005).Inexceptionalcases,thehierarchicalXMLstructureofatextdoc-ument(whichwasmanuallyannotated)istakenintoaccount(Westerveldetal.,2005).Themostinter-estingworkheretomentionistheworkofBergetal.(2004)whoalsoprocessthenearlyparallelimage-textpairsfoundintheYahoo!newscorpus.Theylinkfacesintheimagewithnamesinthetext(recognizedwithnamedentityrecognition),butdonotconsiderotherobjects.Theyconsiderpairsofpersonnames(text)andfaces(image)anduseclus-teringwiththeExpectationMaximizationalgorithmtoﬁndallfacesbelongingtoacertainperson.Intheirmodeltheyconsidertheprobabilitythatanen-tityispicturedgiventhetextualcontext(i.e.,thepart-of-speechtagsimmediatelypriorandafterthename,thelocationofthenameinthetextandthedistancetoparticularsymbolssuchas“(R)”),whichislearnedwithaprobabilisticclassiﬁerineachstepoftheEMiteration.Theyobtainedanaccuracyof84%onpersonfacerecognition.IntheCLASSprojectweworktogetherwithgroupsspecializedinimagerecognition.Infutureworkwewillcombinefaceandobjectrecognitionwithtextanalysistechniques.Weexpecttherecog-nitionanddisambiguationoffacestoimproveifmanyimage-textpairsthattreatthesamepersonareused.Ontheotherhandourapproachisalsovalu-ablewhentherearefewimage-textpairsthatpictureacertainpersonorobject.TheapproachofBergetal.couldbeaugmentedwiththetypicalfeaturesthatweuse,namelysalienceandvisualness.InDe-schachtetal.(2007)wehaveevaluatedtherankingofpersonsandobjectsbythemethodwehavede-scribedhereandwehaveshownthatthisrankingcorrelateswiththeimportanceofpersonsandob-jectsinthepicture.Noneoftheabovestate-of-the-artapproachesconsidersalienceandvisualnessasdiscriminatingfactorsintheentityrecognition,althoughtheseas-pectscouldadvancethestate-of-the-art.9ConclusionOursocietyinthe21stcenturyproducesgiganticamountsofdata,whichareamixtureofdifferentmedia.Ourrepositoriescontaintextsinterwovenwithimages,audioandvideoandweneedauto-matedwaystoautomaticallyindexthesedataandtoautomaticallyﬁndinterrelationshipsbetweenthevariousmediacontents.Thisisnotaneasytask.However,ifwesucceedinrecognizingandaligningcontentinnear-parallelimage-textpairs,wemightbeabletousethisacquiredknowledgeinindex-ingcomparableimage-textpairs(e.g.,invideo)byaligningcontentinthesemedia.Intheexperimentdescribedabove,weanalyzethediscourseandsemanticsoftextsofnear-parallelimage-textpairsinordertocomputetheprobabilitythatanentitymentionedinthetextisalsopresentintheaccompanyingimage.First,wehavedevelopedanapproachforcomputingthesalienceofeachen-titymentionedinthetext.Secondly,wehaveusedtheWordNetclassiﬁcationinordertodetectthevi-sualnessofanentity,whichistranslatedintoavi-sualnessprobability.Thecombinedsalienceandvi-sualnessprovideascorethatsignalstheprobabilitythattheentityispresentintheaccompanyingimage.Weextensivelyevaluatedallthedifferentmodulesofoursystem,pinpointingweakpointsthatcouldbeimprovedandexposingthepotentialofourworkincross-mediaexploitationofcontent.Wewereabletodetectthepersonsinthetextthatarealsopresentintheimagewitha(evenlyweighted)F-measureofmorethan95%,andinaddi-tionwereabletodetecttheentitiesthatarepresentintheimagewithaF-measureofmorethan69%.Theseresultshavebeenobtainedbyrelyingonlyonananalysisofthetextandweresubstantiallybetterthanthebaselineapproach.Evenifwecannotre-solveallambiguity,keepingthemostconﬁdenthy-pothesesgeneratedbyourtextualhypotheseswillgreatlyassistinanalyzingimages.Inthefuturewehopetoextrinsicallyevaluate1007

theproposedtechnologies,e.g.,bytestingwhethertherecognizedcontentinthetext,improvesimagerecognition,retrievalofmultimediasources,miningofthesesources,andcross-mediaretrieval.Inaddi-tion,wewillinvestigatehowwecanbuildmorere-ﬁnedappearancemodelsthatincorporateattributesandactionsofentities.AcknowledgmentsTheworkreportedinthispaperwassupportedbytheEU-ISTprojectCLASS(Cognitive-LevelAnnotationusingLatentStatisticalStructure,IST-027978).WeacknowledgetheCLASSconsortiumpartnersfortheirvaluablecommentsandwearees-peciallygratefultoYvesGufﬂetfromtheINRIAresearchteam(Grenoble,France)forcollectingtheYahoo!Newsdataset.ReferencesArnonAmir,JanneArgillander,MurrayCampbell,AlexanderHaubold,GiridharanIyengar,ShahramEbadollahi,FengKang,MilindR.Naphade,ApostolNatsev,JohnR.Smith,JelenaTeˇsi´o,andTimoVolk-mer.2005.IBMResearchTRECVID-2005VideoRetrievalSystem.InProceedingsofTRECVID2005,Gaithersburg,MD.St´ephaneAyache,GeargesM.Qunot,JrmeGensel,andShin’IchiSatoh.2005.CLIPS-LRS-NIIExperimentsatTRECVID2005.InProceedingsofTRECVID2005,Gaithersburg,MD.KobusBarnard,PinarDuygulu,NandodeFreitas,DavidForsyth,DavidBlei,andMichaelI.Jordan.2003.MatchingWordsandPictures.JournalofMachineLearningResearch,3(6):1107–1135.TamaraL.Berg,AlexanderC.Berg,JaetyEdwards,andD.A.Forsyth.2004.Who’sinthePicture?InNeuralInformationProcessingSystems,pages137–144.KoenDeschachtandMarie-FrancineMoens.2006.Ef-ﬁcientHierarchicalEntityClassiﬁcationUsingCon-ditionalRandomFields.InProceedingsofthe2ndWorkshoponOntologyLearningandPopulation,pages33–40,Sydney,July.KoenDeschacht,Marie-FrancineMoens,andWRobeyns.2007.Cross-mediaentityrecogni-tioninnearlyparallelvisualandtextualdocuments.InProceedingsofthe8thRIAOConferenceonLarge-ScaleSemanticAccesstoContent(Text,Image,VideoandSound).Cmu.(inpress).ChristianeFellbaum.1998.WordNet:AnElectronicLexicalDatabase.TheMITPress.BrianHayes.1999.TheWebofWords.AmericanSci-entist,87(2):108–112,March-April.JaapKampsandMaartenMarx.2002.WordswithAtti-tude.InProceedingsofthe1stInternationalConfer-enceonGlobalWordNet,pages332–341,India.ShariLandes,ClaudiaLeacock,andRandeeI.Tengi.1998.BuildingSemanticConcordances.InChris-tianeFellbaum,editor,WordNet:AnElectronicLex-icalDatabase.TheMITPress.DekangLin.1998.AnInformation-TheoreticDeﬁnitionofSimilarity.InProc.15thInternationalConf.onMa-chineLearning.AndreiMikheev.1997.AutomaticRuleInductionforUnknown-WordGuessing.ComputationalLinguis-tics,23(3):405–423.Marie-FrancineMoens,PatrickJeuniaux,RoxanaAngheluta,andRudradebMitra.2006.Measur-ingAboutnessofanEntityinaText.InProceed-ingsofHLT-NAACL2006TextGraphs:Graph-basedAlgorithmsforNaturalLanguageProcessing,EastStroudsburg.ACL.Marie-FrancineMoens.2006.UsingPatternsofThe-maticProgressionforBuildingaTableofContentofaText.JournalofNaturalLanguageEngineering,12(3):1–28.YasuhideMori,HironobuTakahashi,andRyuichiOka.2000.AutomaticWordAssignmenttoImagesBasedonImageDivisionandVectorQuantization.InRIAO-2000Content-BasedMultimediaInformationAccess,Paris,April12-14.TedPedersen,SiddharthPatwardhan,andJasonMiche-lizzi.2004.WordNet::Similarity-MeasuringtheRe-latednessofConcepts.InTheProceedingsofFifthAn-nualMeetingoftheNorthAmericanChapteroftheAs-sociationforComputationalLinguistics(NAACL-04),Boston,May.Shin’ichiSatoh,YuichiNakamura,andTakeoKanade.1999.Name-It:NamingandDetectingFacesinNewsVideos.IEEEMultiMedia,6(1):22–35,January-March.ThijsWesterveld,JanC.vanGemert,RobertoCornac-chia,DjoerdHiemstra,andArjendeVries.2005.AnIntegratedApproachtoTextandImageRetrieval.InProceedingsofTRECVID2005,Gaithersburg,MD.ThijsWesterveld.2000.ImageRetrieval:ContentversusContext.InContent-BasedMultimediaInformationAccess,RIAO2000ConferenceProceedings,pages276–284,April.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 1008–1015,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

1008

User Requirements Analysis for Meeting Information Retrieval  Based on Query Elicitation Vincenzo Pallotta Department of Computer Science University of Fribourg Switzerland Vincenzo.Pallotta@unifr.ch Violeta Seretan Language Technology Laboratory  University of Geneva Switzerland seretan@lettres.unige.ch Marita Ailomaa Artificial Intelligence Laboratory  Ecole Polytechnique Fédérale  de Lausanne (EPFL), Switzerland Marita.Ailomaa@epfl.ch    Abstract We present a user requirements study for Question Answering on meeting records that assesses the difficulty of users ques-tions in terms of what type of knowledge is required in order to provide the correct an-swer. We grounded our work on the em-pirical analysis of elicited user queries. We found that the majority of elicited queries (around 60%) pertain to argumentative processes and outcomes. Our analysis also suggests that standard keyword-based In-formation Retrieval can only deal success-fully with less than 20% of the queries, and that it must be complemented with other types of metadata and inference. 1 Introduction Meeting records constitute a particularly important and rich source of information. Meetings are a frequent and sustained activity, in which multi-party dialogues take place that are goal-oriented and where participants perform a series of actions, usually aimed at reaching a common goal: they exchange information, raise issues, express opinions, make suggestions, propose solutions, provide arguments (pro or con), negotiate alternatives, and make decisions. As outcomes of the meeting, agreements on future action items are reached, tasks are assigned, conflicts are solved, etc. Meeting outcomes have a direct impact on the efficiency of organization and team performance, and the stored and indexed meeting records serve as reference for further processing (Post et al., 2004). They can also be used in future meetings in order to facilitate the decision-making process by accessing relevant information from previous meetings (Cremers et al., 2005), or in order to make the discussion more focused (Conklin, 2006).  Meetings constitute a substantial and important source of information that improves corporate or-ganization and performance (Corrall, 1998; Ro-mano and Nunamaker, 2001). Novel multimedia techniques have been dedicated to meeting record-ing, structuring and content analysis according to the metadata schema, and finally, to accessing the analyzed content via browsing, querying or filter-ing (Cremers et al., 2005; Tucker and Whittaker, 2004). This paper focuses on debate meetings (Cugini et al., 1997) because of their particular richness in information concerning the decision-making proc-ess. We consider that the meeting content can be organized on three levels: (i) factual level (what happens: events, timeline, actions, dynamics); (ii) thematic level (what is said: topics discussed and details); (iii) argumentative level (which/how com-mon goals are reached).  The information on the first two levels is ex-plicit information that can be usually retrieved di-rectly by searching the meeting records with ap-propriate IR techniques (i.e., TF-IDF). The third level, on the contrary, contains more abstract and tacit information pertaining to how the explicit in-formation contributes to the rationale of the meet-ing, and it is not present as such in raw meeting data: whether or not the meeting goal was reached, what issues were debated, what proposals were made, what alternatives were discussed, what ar-guments were brought, what decisions were made, what task were assigned, etc.  The motivating scenario is the following: A user 1009

needs information about a past meeting, either in quality of a participant who wants to recollect a discussion (since the memories of co-participants are often inconsistent, cf. Banerjee et al., 2005), or as a non-participant who missed that meeting. Instead of consulting the entire meeting-related information, which is usually heterogeneous and scaterred (audio-video recordings, notes, minutes, e-mails, handouts, etc.), the user asks natural language questions to a query engine which retrieves relevant information from the meeting records. In this paper we assess the users' interest in retrieving argumentative information from meetings and what kind of knowledge is required for answering users' queries. Section 2 reviews previous user requirements studies for the meeting domain. Section 3 describes our user requirements study based on the analysis of elicited user queries, presents its main findings, and discusses the implications of these findings for the design of meeting retrieval systems. Section 4 concludes the paper and outlines some directions for future work. 2 Argumentative Information in Meeting Information Retrieval Depending on the meeting browser type1, different levels of meeting content become accessible for information retrieval. Audio and video browsers deal with factual and thematic information, while artifact browsers might also touch on deliberative information, as long as it is present, for instance, in the meeting minutes. In contrast, derived-data browsers aim to account for the argumentative in-formation which is not explicitly present in the meeting content, but can be inferred from it. If minutes are likely to contain only the most salient deliberative facts, the derived-data browsers are much more useful, in that they offer access to the full meeting record, and thus to relevant details about the deliberative information sought. 2.1 Importance of Argumentative Structure  As shown by Rosemberg and Silince (1999), track-ing argumentative information from meeting dis-                                                1 (Tucker and Whittaker, 2004) identifies 4 types of meeting browsers: audio browsers, video browsers, artifacts browsers (that exploit meeting minutes or other meeting-related docu-ments), and browsers that work with derived data (such as discourse and temporal structure information). cussions is of central importance for building pro-ject memories since, in addition to the "strictly fac-tual, technical information", these memories must also store relevant information about deci-sion-making processes. In a business context, the information derived from meetings is useful for future business processes, as it can explain phe-nomena and past decisions and can support future actions by mining and assessment (Pallotta et al., 2004). The argumentative structure of meeting dis-cussions, possibly visualized in form of argumen-tation diagrams or maps, can be helpful in meeting browsing. To our knowledge, there are at least three meeting browsers that have adopted argu-mentative structure: ARCHIVUS (Lisowska et al., 2004b), ViCoDe (Marchand-Maillet and Bruno, 2005), and the Twente-AMI JFerret browser (Rienks and Verbree, 2006).  2.2 Query Elicitation Studies  The users' interest in argumentation dimension of meetings has been highlighted by a series of recent studies that attempted to elicit the potential user questions about meetings (Lisowska et al., 2004a; Benerjee at al., 2005; Cremers et al., 2005). The study of Lisowska et al. (2004a), part of the IM2 research project2, was performed in a simu-lated environment in which users were asked to imagine themselves in a particular role from a se-ries of scenarios. The participants were both IM2 members and non-IM2 members and produced about 300 retrospective queries on recorded meet-ings. Although this study has been criticized by Post et al. (2004), Cremers et al. (2005), and Ban-erjee et al. (2005) for being biased, artificial, ob-trusive, and not conforming to strong HCI method-ologies for survey research, it shed light on poten-tial queries and classified them in two broad cate-gories, that seem to correspond to our argumenta-tive/non-argumentative distinction (Lisowska et al., 2004a: 994): • “elements related to the interaction among par-ticipants: acceptance/rejection, agree-ment/disagreement; proposal, argumentation (for and against); assertions, statements; deci-sions; discussions, debates; reactions; ques-tions; solutions”;                                                 2 http://www.im2.ch 1010

•  “concepts from the meeting domains: dates, times; documents; meeting index: current, pre-vious, sets; participants; presentations, talks; projects; tasks, responsibilities; topics”.  Unfortunately, the study does not provide precise information on the relative proportions of queries for the classification proposed, but simply suggests that overall more queries belong to the second category, while queries requiring understanding of the dialogue structure still comprise a sizeable proportion. The survey conducted by Banerjee et al. (2005) concerned instead real, non-simulated interviews of busy professionals about actual situations, re-lated either to meetings in which they previously participated, or to meetings they missed. More than half of the information sought by interviewees concerned, in both cases, the argumentative dimen-sion of meetings. For non-missed meetings, 15 out of the 26 in-stances (i.e., 57.7%) concerned argumentative as-pects: what the decision was regarding a topic (7); what task someone was assigned (4); who made a particular decision (2); what was the participants' reaction to a particular topic (1); what the future plan is (1). The other instances (42.3%) relate to the thematic dimension, i.e., specifics of the dis-cussion on a topic (11).  As for missed meetings, the argumentative in-stances were equally represented (18/36): decisions on a topic (7); what task was assigned to inter-viewee (4); whether a particular decision was made (3); what decisions were made (2); reasons for a decision (1); reactions to a topic (1). The thematic questions concern topics discussed, announce-ments made, and background of participants.  The study also showed that the recovery of in-formation from meeting recordings is significantly faster when discourse annotations are available, such as the distinction between discussion, presen-tation, and briefing. Another unobtrusive user requirements study was performed by Cremers et al. (2005) in a "semi-natural setting" related to the design of a meeting browser. The top 5 search interests highlighted by the 60 survey participants were: decisions made, participants/speakers, topics, agenda items, and arguments for decision. Of these, the ones shown in italics are argumentative. In fact, the authors acknowledge the necessity to include some "func-tional" categories as innovative search options. Interestingly, from the user interface evaluation presented in their paper, one can indirectly infer how salient the argumentative information is per-ceived by users: the icons that the authors intended for emotions, i.e., for a emotion-based search facil-ity, were actually interpreted by users as referring to people’s opinion: What is person X's opinion? – positive, negative, neutral. 3 User Requirements Analysis The existing query elicitation experiments reported in Section 2 highlighted a series of question types that users typically would like to ask about meet-ings. It also revealed that the information sought can be classified into two broad categories: argu-mentative information (about the argumentative process and the outcome of debate meetings), and non-argumentative information (factual, i.e., about the meeting as a physical event, or thematic, i.e., about what has been said in terms of topics). The study we present in this section is aimed at assessing how difficult it is to answer the questions that users typically ask about a meeting. Our goal is to provide insights into:  • how many queries can be answered using stan-dard IR techniques on meeting artefacts only (e.g., minutes, written agenda, invitations); • how many queries can be answered with IR on meeting recordings; • what kind of additional information and infer-ence is needed when IR does not apply or it is insufficient (e.g., information about the par-ticipants and the meeting dynamics, external information about the meeting’s context such as the relation to a project, semantic interpreta-tion of question terms and references, compu-tation of durations, aggregation of results, etc). Assessing the level of difficulty of a query based on the two above-mentioned categories might not provide insightful results, because these would be too general, thus less interpretable. Also, the com-plex queries requiring mixed information would escape observation because assigned to a too gen-eral class. We therefore considered it necessary to perform a separate analysis of each query instance, as this provides not only detailed, but also trace-able information. 1011

3.1 Data: Collecting User Queries Our analysis is based on a heterogeneous collec-tion of queries for meeting data. In general, an un-biased queries dataset is difficult to obtain, and the quality of a dataset can vary if the sample is made of too homogenous subjects (e.g., people belong-ing to the same group as members of the same pro-ject). In order to cope with this problem, our strat-egy was to use three different datasets collected in different settings:  • First, we considered the IM2 dataset collected by Lisowska et al. (2004a), the only set of user queries on meetings available to date. It com-prises 270 questions (shortly described in Sec-tion 2) annotated with a label showing whether or not the query was produced by an IM2-member. These queries are introspective and not related to any particular recorded meeting. • Second, we cross-validated this dataset with a large corpus of 294 natural language state-ments about existing meetings records. This dataset, called the BET observations (Wellner et al., 2005), was collected by subjects who were asked to watch several meeting record-ings and to report what the meeting partici-pants appeared to consider interesting. We use it as a ‘validation’ set for the IM2 queries: an IM2 query is considered as ‘realistic’ or ‘em-pirically grounded’ if there is a BET observa-tion that represents a possible answer to the query. For instance, the query Why was the proposal made by X not accepted? matches the BET observation Denis eliminated Silence of the Lambs as it was too violent. • Finally, we collected a new set of ‘real’ queries by conducting a survey of user requirements on meeting querying in a natural business set-ting. The survey involved 3 top managers from a company and produced 35 queries. We called this dataset Manager Survey Set (MS-Set). The queries from the IM2-set (270 queries) and the MS-Set (35 queries) were analyzed by two differ-ent teams of two judges. Each team discussed each query, and classified it along the two main dimen-sions we are interested in: • query type: the type of meeting content to which the query pertains; • query difficulty: the type of information re-quired to provide the answer. 3.2 Query Type Analysis Each query was assigned exactly one of the follow-ing four possible categories (the one perceived as the most salient): 1. factual: the query pertains to the factual meet-ing content; 2. thematic: the query pertains to the thematic meeting content; 3. process: the query pertains to the argumenta-tive meeting content, more precisely to the ar-gumentative process; 4. outcome: the query pertains to the argumenta-tive meeting content, more precisely to the outcome of the argumentative process. IM2-set (size:270) MS-Set  (size: 35) Category Team1 Team2 Team1 Team2 Factual 24.8% 20.0% 20.0% Thematic 18.5% 45.6% 20.0% 11.4% Process 30.0% 32.6% 22.9% 28.6% Outcome 26.7% 21.8% 37.1% 40.0% Process+ Outcome 56.7% 54.4% 60.0% 68.6% Table 1. Query classification according to the meeting content type. Results from this classification task for both query sets are reported in Table 1. In both sets, the information most sought was argumentative: about 55% of the IM2-set queries are argumentative (process or outcome). This invalidates the initial estimation of Lisowska et al. (2004a:994) that the non-argumentative queries prevail, and confirms the figures obtained in (Banerjee et al., 2005), ac-cording to which 57.7% of the queries are argu-mentative. In our real managers survey, we ob-tained even higher percentages for the argumenta-tive queries (60% or 68.6%, depending on the an-notation team). The argumentative queries are fol-lowed by factual and thematic ones in both query sets, with a slight advantage for factual queries. The inter-annotator agreement for this first clas-sification is reported in Table 2. The proportion of queries on which annotators agree in classifying them as argumentative is significantly high. We only report here the agreement results for the indi-vidual argumentative categories (Process, Out-come) and both (Process & Outcome). There were 213 queries (in IM2-set) and 30 queries (in MS-1012

set) that were consistently annotated by the two teams on both categories. Within this set, a high percentage of queries were argumentative, that is, they were annotated as either Process or Outcome (label AA in the table). IM2-set (size: 270) MS-set (size: 35) Category ratio kappa ratio kappa Process 84.8% 82.9% 88.6% 87.8% Outcome 90.7% 89.6% 91.4% 90.9% Process & Outcome 78.9% 76.2% 85.7% 84.8% AA 117/213 = 54.9%  19/30 = 63.3%  Table 2. Inter-annotator agreement for query-type classification. Furthermore, we provided a re-assessment of the proportion of argumentative queries with respect to query origin for the IM2-set (IM2 members vs. non-IM2 members): non-IM2 members issued 30.8% of agreed argumentative queries, a propor-tion that, while smaller compared to that of IM2 members (69.2%), is still non-negligible. This con-trasts with the opinion expressed in (Lisowska et al., 2004a) that argumentative queries are almost exclusively produced by IM2 members.  Among the 90 agreed IM2 queries that were cross-validated with the BET-observation set, 28.9% were argumentative. We also noted that the ratio of BET statements that contain argumentative information is quite high (66.9%). 3.3 Query Difficulty Analysis In order to assess the difficulty in answering a query, we used the following categories that the annotators could assign to each query, according to the type of information and techniques they judged necessary for answering it: 1. Role of IR: states the role of standard3 Informa-tion Retrieval (in combination with Topic Ex-traction4) techniques in answering the query. Possible values:  a. Irrelevant (IR techniques are not appli-cable). Example: What decisions have been made?                                                 3 By standard IR we mean techniques based on bag-of-word search and TF-IDF indexing. 4 Topic extraction techniques are based on topic shift detec-tion (Galley et al., 2003) and keyword extraction (van der Plas et al., 2004). b. successful (IR techniques are sufficient). Example: Was the budget approved? c. insufficient (IR techniques are necessary, but not sufficient alone since they re-quire additional inference and informa-tion, such as argumentative, cross-meeting, external corporate/project knowledge). Example: Who rejected the proposal made by X on issue Y? 2. Artefacts: information such as agenda, min-utes of previous meetings, e-mails, invita-tions and other documents related and avail-able before the meeting. Example: Who was invited to the meeting? 3. Recordings: the meeting recordings (audio, visual, transcription). This is almost always true, except for queries where Artefacts or Metadata are sufficient, such as What was the agenda?, Who was invited to the meet-ing?). 4. Metadata: context knowledge kept in static metadata (e.g., speakers, place, time). Ex-ample: Who were the participants at the meeting? 5. Dialogue Acts & Adjacency Pairs: Example: What was John’s response to my comment on the last meeting? 6. Argumentation: metadata (annotations) about the argumentative structure of the meeting content. Example: Did everybody agree on the decisions, or were there differ-ences of opinion? 7.  Semantics: semantic interpretation of terms in the query and reference resolution, in-cluding deictics (e.g., for how long, usually, systematically, criticisms; this, about me, I). Example: What decisions got made easily? The term requiring semantic interpretation is underlined.  8. Inference: inference (deriving information that is implicit), calculation, and aggregation (e.g., for ‘command’ queries asking for lists of things – participants, issues, proposals). Example: What would be required from me? 1013

9. Multiple meetings: availability of multiple meeting records. Example: Who usually at-tends the project meetings?  10. External: related knowledge, not explicitly present in the meeting records (e.g., infor-mation about the corporation or the projects related to the meeting). Example: Did some-body talk about me or about my work? Results of annotation reported on the two query sets are synthesized in Table 3: IR is sufficient for answering 14.4% of the IM2 queries, and 20% of the MS-set queries. In 50% and 25.7% of the cases, respectively, it simply cannot be applied (irrele-vant). Finally, IR alone is not enough in 35.6% of the queries from the IM2-set, and in 54.3% of the MS-set; it has to be complemented with other techniques.  IM2-set MS-set IR is: all  queries AA all  queries AA Sufficient 39/270 = 14.4% 1/117 = 0.8% 7/35 = 20.0% 1/19 = 5.3% Irrelevant 135/270 = 50.0%  55/117 = 47.0% 9/35 = 25.7% 3/19 = 15.8% Insufficient 96/270 = 35.6% 61/117 = 52.1% 19/35 = 54.3% 15/19 = 78.9% Table 3. The role of IR (and topic extraction) in answering users’ queries. If we consider agreed argumentative queries (Section 3.2), IR is effective in an extremely low percentage of cases (0.8% for IM2-set and 5.3% for MS-Set). IR is insufficient in most of the cases (52.1% and 78.9%) and inapplicable in the rest of the cases (47% and 15.8%). Only one argumenta-tive query from each set was judged as being an-swerable with IR alone: What were the decisions to be made (open questions) regarding the topic t1? When is the NEXT MEETING planned? (e.g. to follow up on action items). Table 4 shows the number of queries in each set that require argumentative information in order to be answered, distributed according to the query types. As expected, no argumentation information is necessary for answering factual queries, but some thematic queries do need it, such as What was decided about topic T? (24% in the IM2-set and 42.9% in the M.S.-set).  Overall, the majority of queries in both sets re-quire argumentation information in order to be an-swered (56.3% from IM2 queries, and 65.7% from MS queries). IM2-set, Annotation 1 MS-set, Annotation 1 Category total Req. arg. Ratio Total Req. arg. Ratio Factual 67 0 0% 7 0 0% Thematic 50 12 24.0% 7 3 42.9% Process 81 73 90.1% 8 7 87.5% Outcome 72 67 93.1% 13 13 100% All 270 152 56.3% 35 23 65.7% Table 4. Queries requiring argumentative informa-tion. We finally looked at what kind of information is needed in those cases where IR is perceived as in-sufficient or irrelevant. Table 5 lists the most fre-quent combinations of information types required for the IM2-set and the MS-set. 3.4 Summary of Findings The analysis of the annotations obtained for the 305 queries (35 from the Manager Survey set, and 270 from the IM2-set) revealed that: • The information most sought by users from meetings is argumentative (i.e., pertains to the argumentative process and its outcome). It constitutes more than half of the total queries, while factual and thematic information are similar in proportions (Table 1); • There was no significant difference in this re-spect between the IM2-set and the MS-set (Table 1); • The decision as to whether a query is argumen-tative or not is easy to draw, as suggested by the high inter-annotator agreement shown in Table 2; • Standard IR and topic extraction techniques are perceived as insufficient in answering most of the queries. Only less than 20% of the whole query set can be answered with IR, and almost no argumentative question (Table 3). • Argumentative information is needed in an-swering the majority of the queries (Table 4); • When IR alone fails, the information types that are needed most are (in addition to recordings): Argumentation, Semantics, Inference, and Metadata (Table 5); see Section 3.3 for their description.  1014

 IR alone fails IM2-set Information types IR insufficient             96 cases   35.6% IR irrelevant         135 cases    50% Artefacts         x     Recordings x x x x x x x x x x x   Meta-data   x  x   x  x  x x Dlg acts & Adj. pairs              Argumentation x x x x x x x x x  x   Semantics x x x x x   x x x x x  Inference x  x x   x x x x x x  Multiple meetings    x        x  External                                Cases 15 11 9 8 7 5 4 14 9 8 8 7 5                   Ratio (%) 15.6 11.5 9.4 8.3 7.3 5.2 4.2 10.4 6.7 5.9 5.9 5.2 3.7  IR alone fails MS-set Information types IR insufficient     19 cases   54.3% IR irrelevant   9 cases   54.3% Artefacts     x x Recordings x x x x   Meta-data     x x Dlg acts & Adj. pairs       Argumentation x x x x   Semantics x  x x x  Inference x x  x x  Multiple meetings       External    x                     Cases 6 4 2 2 2 2                   Ratio (%) 31.6 21 10.5 10.5 22.2 22.2 Table 5. Some of the most frequent combinations of information required for answering the queries in the IM2-Set and in the MS-set when IR alone fails. 3.5 Discussion Searching relevant information through the re-corded meeting dialogues poses important prob-lems when using standard IR indexing techniques (Baeza-Yates and Ribeiro-Nieto, 2000), because users ask different types of queries for which a single retrieval strategy (e.g., keywords-based) is insufficient. This is the case when looking at an-swers that require some sort of entailment, such as inferring that a proposal has been rejected when a meeting participant says Are you kidding?.  Spoken-language information retrieval (Vinci-arelli, 2004) and automatic dialogue-act extraction techniques (Stolke et al., 2000; Clark and Popescu-Belis, 2004; Ang et al., 2005) have been applied to meeting recordings and produced good results un-der the assumption that the user is interested in retrieving either topic-based or dialog act-based information. But this assumption is partially in-validated by our user query elicitation analysis, which showed that such information is only sought in a relatively small fraction of the users’ queries. A particular problem for these approaches is that the topic looked for is usually not a query itself (Was topic T mentioned?), but just a parameter in more structured questions (What was decided about T?). Moreover, the relevant participants’ contributions (dialog acts) need to be retrieved in combination, not in isolation (The reactions to the proposal made by X). 4 Conclusion and Future Work While most of the research community has ne-glected the importance of argumentative queries in meeting information retrieval, we provided evi-dence that this type of queries is actually very common. We quantified the proportion of queries involving the argumentative dimension of the meeting content by performing an in-depth analy-sis of queries collected in two different elicitation surveys. The analysis of the annotations obtained for the 305 queries (270 from the IM2-set, 35 from MS-set) was aimed at providing insights into dif-ferent matters: what type of information is typi-cally sought by users from meetings; how difficult it is, and what kind of information and techniques are needed in order to answer user queries.  This work represents an initial step towards a better understanding of user queries on the meeting domain. It could provide useful intuitions about 1015

how to perform the automatic classification of an-swer types and, more importantly, the automatic extraction of argumentative features and their rela-tions with other components of the query (e.g., topic, named entities, events). In the future, we intend to better ground our first empirical findings by i) running the queries against a real IR system with indexed meeting transcripts and evaluate the quality of the obtained answers; ii) ask judges to manually rank the difficulty of each query, and iii) compare the two rankings. We would also like to see how frequent argumentative queries are in other domains (such as TV talk shows or political debates) in order to generalize our results. Acknowledgements We wish to thank Martin Rajman and Hatem Ghorbel for their constant and valuable feedback. This work has been partially supported by the Swiss National Science Foundation NCCR IM2 and by the SNSF grant no. 200021-116235. References Jeremy Ang, Yang Liu and Elizabeth Shriberg. 2005. Automatic Dialog Act Segmentation and Classification in Multiparty Meetings. In Proceedings of IEEE ICASSP 2005, Philadelphia, PA, USA. Ricardo Baeza-Yates and Berthier Ribeiro-Nieto. 2000. Modern Information Retrieval. Addison Wesley. Satanjeev Banerjee, Carolyn Rose and Alexander I. Rudnicky. 2005. The Necessity of a Meeting Recording and Playback System, and the Benefit of Topic-Level Annotations to Meeting Browsing. In Proceedings of INTERACT 2005, Rome, Italy. Alexander Clark and Andrei Popescu-Belis. 2004. Multi-level Dialogue Act Tags. In Proceedings of SIGDIAL'04, pages 163–170. Cambridge, MA, USA. Jeff Conklin. 2006. Dialogue Mapping: Building Shared Understanding of Wicked Problems. John Wiley & Sons. Sheila Corrall. 1998. Knowledge management. Are we in the knowledge management business? ARIADNE: the Web version, 18. Anita H.M Cremers, Bart Hilhorst and Arnold P.O.S Vermeeren. 2005. “What was discussed by whom, how, when and where?" personalized browsing of annotated multimedia meeting recordings. In Proceedings of HCI 2005, pages 1–10, Edinburgh, UK.  John Cugini, Laurie Damianos, Lynette Hirschman, Robyn Kozierok, Jeff Kurtz, Sharon Laskowski and Jean Scholtz. 1997. Methodology for evaluation of collaborative systems. Technical Report Rev. 3.0, The Evaluation Working Group of the DARPA Intelligent Collaboration and Visualization Program. Michel Galley, Kathleen McKeown, Eric Fosler-Lussier and Hongyan Jing. 2003. Discourse Segmentation of Multi-Party Conversation. In Proceedings of ACL 2003, pages 562–569, Sapporo, Japan. Agnes Lisowska, Andrei Popescu-Belis and Susan Armstrong. 2004a. User Query Analysis for the Specification and Evaluation of a Dialogue Processing and Retrieval System. In Proceedings LREC 2004, pages 993–996, Lisbon, Portugal. Agnes Lisowska, Martin Rajman and Trung H. Bui. 2004b. ARCHIVUS: A System for Accesssing the Content of Recorded Multimodal Meetings. In Proceedings of MLMI 2004, Martigny, Switzerland. Stéphane Marchand-Maillet and Eric Bruno. 2005. Collection Guiding: A new framework for handling large multimedia collections. In Proceeding of AVIVDiLib05, Cortona, Italy. Vincenzo Pallotta, Hatem Ghorbel, Afzal Ballim, Agnes Lisowska and Stéphane Marchand-Maillet. 2004. Towards meeting information systems: Meeting knowledge management. In Proceedings of ICEIS 2005, pages 464–469, Porto, Portugal. Lonneke van der Plaas, Vincenzo Pallotta, Martin Rajman and Hatem Ghorbel. 2004. Automatic keyword extraction from spoken text: A comparison between two lexical resources: the EDR and WordNet. In Proceedings of the LREC 2004, pages 2205–2208, Lisbon, Portugal. Wilfried M. Post, Anita H.M. Cremers and Olivier Blanson Henkemans. 2004. A Research Environment for Meeting Behavior. In Proceedings of the 3rd Workshop on Social Intelligence Design, pages 159–165, University of Twente, Enschede, The Netherlands. Rutger Rienks and Daan Verbree. 2006. About the Usefulness and Learnability of Argument–Diagrams from Real Discussions. In Proceedings of MLMI 2006, Washington DC, USA. Nicholas C. Romano Jr. and Jay F. Nunamaker Jr. 2001. Meeting Analysis: Findings from Research and Practice. In Proceedings of HICSS-34, Maui, HI, IEEE Computer Society. Duska Rosemberg and John A.A. Silince. 1999. Common ground in computer-supported collaborative argumentation. In Proceedings of the CLSCL99, Stanford, CA, USA. Andreas Stolcke, Klaus Ries, Noah Coccaro, Elizabeth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul Taylor, Rachel Martin, Carol Van Ess-Dykema and Marie Meteer. 2000. Dialog Act Modeling for Automatic Tagging and Recognition of Conversational Speech. Computational Linguistics, 26(3):339–373.  Simon Tucker and Steve Whittaker. 2004. Accessing multimodal meeting data: systems, problems and possibilities. In Proceedings of MLMI 2004, Martigny, Switzerland. Alessandro Vinciarelli. 2004. Noisy text categorization. In Proceedings of ICPR 2004, Cambridge, UK. Pierre Wellner, Mike Flynn, Simon Tucker, Steve Whittaker. 2005. A Meeting Browser Evaluation Test. In Proceedings of CHI 2005, Portand, Oregon, USA. Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 1016–1023,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

1016

CombiningMultipleKnowledgeSourcesforDialogueSegmentationinMultimediaArchivesPei-YunHsuehSchoolofInformaticsUniversityofEdinburghEdinburgh,UKEH89WLp.hsueh@ed.ac.ukJohannaD.MooreSchoolofInformaticsUniversityofEdinburghEdinburgh,UKEH89WLJ.Moore@ed.ac.ukAbstractAutomaticsegmentationisimportantformakingmultimediaarchivescomprehensi-ble,andfordevelopingdownstreaminfor-mationretrievalandextractionmodules.Inthisstudy,weexploreapproachesthatcansegmentmultipartyconversationalspeechbyintegratingvariousknowledgesources(e.g.,words,audioandvideorecordings,speakerintentionandcontext).Inparticu-lar,weevaluatetheperformanceofaMax-imumEntropyapproach,andexaminetheeffectivenessofmultimodalfeaturesonthetaskofdialoguesegmentation.Wealsopro-videaquantitativeaccountoftheeffectofusingASRtranscriptionasopposedtohu-mantranscripts.1IntroductionRecentadvancesinmultimediatechnologieshaveledtohugearchivesofaudio-videorecordingsofmultipartyconversationsinawiderangeofareasincludingclinicaluse,onlinevideosharingser-vices,andmeetingcaptureandanalysis.Whileitisstraightforwardtoreplaysuchrecordings,ﬁnd-inginformationfromtheoftenlengthyarchivesisamorechallengingtask.Annotatingimplicitseman-ticstoenhancebrowsingandsearchingofrecordedconversationalspeechhasthereforeposednewchal-lengestotheﬁeldofmultimediainformationre-trieval.Onecriticalproblemishowtodivideunstructuredconversationalspeechintoanumberoflocallyco-herentsegments.Theproblemisimportantfortworeasons:First,empiricalanalysishasshownthatan-notatingtranscriptswithsemanticinformation(e.g.,topics)enablesuserstobrowseandﬁndinformationfrommultimediaarchivesmoreefﬁciently(Baner-jeeetal.,2005).Second,becausetheautomaticallygeneratedsegmentsmakeupforthelackofexplicitorthographiccues(e.g.,storyandparagraphbreaks)inconversationalspeech,dialoguesegmentationisusefulinmanyspokenlanguageunderstandingtasks,includinganaphoraresolution(GroszandSid-ner,1986),informationretrieval(e.g.,asinputfortheTRECSpokenDocumentRetrieval(SDR)task),andsummarization(ZechnerandWaibel,2000).ThisstudythereforeaimstoexplorewhetheraMaximumEntropy(MaxEnt)classiﬁercaninte-gratemultipleknowledgesourcesforsegmentingrecordedspeech.Inthispaper,weﬁrstevaluatetheeffectivenessoffeaturesthathavebeenproposedinpreviouswork,withafocusonfeaturesthatcanbeextractedautomatically.Second,weexamineotherknowledgesourcesthathavenotbeenstudiedsys-tematicallyinpreviouswork,butwhichweexpecttobegoodpredictorsofdialoguesegments.Inad-dition,asourultimategoalistodevelopaninfor-mationretrievalmodulethatcanbeoperatedinafullyautomaticfashion,wealsoinvestigatetheim-pactofautomaticspeechrecognition(ASR)errorsonthetaskofdialoguesegmentation.2PreviousWorkInpreviouswork,theproblemofautomaticdia-loguesegmentationisoftenconsideredassimilartotheproblemoftopicsegmentation.Therefore,re-searchhasadoptedtechniquespreviouslydeveloped1017

tosegmenttopicsintext(Kozima,1993;Hearst,1997;Reynar,1998)andinreadspeech(e.g.,broad-castnews)(PonteandCroft,1997;Allanetal.,1998).Forexample,lexicalcohesion-basedalgo-rithms,suchasLCSEG(Galleyetal.,2003),oritswordfrequency-basedpredecessorTextTile(Hearst,1997)capturetopicshiftsbymodelingthesimilarityofwordrepetitioninadjacentwindows.However,recentworkhasshownthatLCSEGislesssuccessfulinidentifying“agenda-basedconver-sationsegments”(e.g.,presentation,groupdiscus-sion)thataretypicallysignalledbydifferencesingroupactivity(HsuehandMoore,2006).ThisisnotsurprisingsinceLCSEGconsidersonlylexicalcohesion.Previousworkhasshownthattrainingasegmentationmodelwithfeaturesthatareextractedfromknowledgesourcesotherthanwords,suchasspeakerinteraction(e.g.,overlaprate,pause,andspeakerchange)(Galleyetal.,2003),orpartici-pantbehaviors,e.g.,notetakingcues(BanerjeeandRudnicky,2006),canoutperformLCSEGonsimilartasks.Inmanyotherﬁeldsofresearch,avarietyoffea-tureshavebeenidentiﬁedasindicativeofsegmentboundariesindifferenttypesofrecordedspeech.Forexample,Brownetal.(1980)haveshownthatadiscoursesegmentoftenstartswithrelativelyhighpitchedsoundsandendswithsoundsofpitchwithinamorecompressedrange.PassonneauandLit-man(1993)identiﬁedthattopicshiftsoftenoccurafterapauseofrelativelylongduration.Otherprosodiccues(e.g.,pitchcontour,energy)havebeenstudiedfortheircorrelationwithstorysegmentsinreadspeech(Turetal.,2001;Levow,2004;Chris-tensenetal.,2005)andwiththeory-baseddiscoursesegmentsinspontaneousspeech(e.g.,direction-givenmonologue)(HirschbergandNakatani,1996).Inaddition,headandhand/forearmmovementsareusedtodetectgroup-actionbasedsegments(Mc-Cowanetal.,2005;Al-Hamesetal.,2005).However,manyotherfeaturesthatweexpecttosignalsegmentboundarieshavenotbeenstudiedsystematically.Forinstance,speakerintention(i.e.,dialogueacttypes)andconversationalcontext(e.g.,speakerrole).Inaddition,althoughthesefeaturesareexpectedtobecomplementarytooneanother,fewofthepreviousstudieshavelookedattheques-tionhowtouseconditionalapproachestomodelthecorrelationamongfeatures.3Methodology3.1MeetingCorpusThisstudyaimstoexploreapproachesthatcanin-tegratemultimodalinformationtodiscoverimplicitsemanticsfromconversationarchives.Asourgoalistoidentifymultimodalcuesofsegmentationinface-to-faceconversation,weusetheAMImeetingcorpus(Carlettaetal.,2006),whichincludesaudio-videorecordings,totestourapproach.Inparticu-lar,weareusing50scenario-basedmeetingsfromtheAMIcorpus,inwhichparticipantsareassignedtodifferentrolesandgivenspeciﬁctasksrelatedtodesigningaremotecontrol.Onaverage,AMImeet-ingslast26minutes,withover4,700wordstran-spired.Thiscorpusincludesannotationfordialoguesegmentationandtopiclabels.Intheannotationpro-cess,annotatorsweregiventhefreedomtosubdi-videasegmentintosubsegmentstoindicatewhenthegroupwasdiscussingasubtopic.Annotatorswerealsogivenasetofsegmentdescriptionstobeusedaslabels.Annotatorswereinstructedtoaddanewlabelonlyiftheycouldnotﬁndamatchinthestandardset.Thesetofsegmentdescriptionscanbedividedtothreecategories:activity-based(e.g.,presentation,discussion),issue-based(e.g.,budget,usability),andfunctionalsegments(e.g.,chitchat,opening,closing).3.2PreprocessingTheﬁrststepistobreakarecordedmeetingintominimalunits,whichcanvaryfromsentencechunkstoblocksofsentences.Inthisstudy,weusespurts,thatis,consecutivespeechwithnopauselongerthan0.5seconds,asminimalunits.Then,toexaminethedifferencebetweenthesetoffeaturesthatarecharacteristicofsegmentationatbothcoarseandﬁnelevelsofgranularity,thisstudycharacterizesadialogueasasequenceofsegmentsthatmaybefurtherdividedintosub-segments.Wetakethetheory-freedialoguesegmentationannota-tionsinthecorpusandﬂattenthesub-segmentstruc-tureandconsideronlytwolevelsofsegmentation:top-levelsegmentsandallsub-levelsegments.1We1Wetakethespurtswhichtheannotatorschooseasthebe-ginningofasegmentasthetopicboundaries.Onaverage,1018

observedthatannotatorstendedtoannotateactivity-basedsegmentsonlyatthetoplevel,whereastheyoftenincludedsub-topicswhensegmentingissue-basedsegments.Forexample,atop-levelinterfacespecialistpresentationsegmentcanbedividedintoagenda/equipmentissues,userrequirements,exist-ingproducts,andlookandusabilitysub-levelseg-ments.3.3IntercoderAgreementTomeasureintercoderagreement,weemploythreedifferentmetrics:thekappacoefﬁcient,PK,andWD.Kappavaluesmeasurehowwellapairofan-notatorsagreeonwherethesegmentsbreak.PKistheprobabilitythattwospurtsdrawnrandomlyfromadocumentareincorrectlyidentiﬁedasbelongingtothesamesegment.WindowDiff(WD)calculatestheerrorratebymovingaslidingwindowacrossthetranscriptcountingthenumberoftimesthehypoth-esizedandreferencesegmentboundariesarediffer-ent.Whilenotuncontroversial,theuseofthesemet-ricsiswidespread.Table1showstheintercoderagreementofthetop-levelandsub-levelsegmenta-tionrespectively.Itisunclearwhetherthekappavaluesshownhereindicatereliableintercoderagreement.2ButgiventhelowdisagreementrateamongcodingsintermsofthePKandWDscores,wewillargueforthereli-abilityoftheannotationprocedureusedinthisstudy.Also,toourknowledgethereporteddegreeofagree-mentisthebestintheﬁeldofmeetingdialogueseg-mentation.3IntercoderKappaPKWDTOP0.660.110.17SUB0.590.230.28Table1:Intercoderagreementofannotationsatthetop-level(TOP)andsub-level(SUB)segments.theannotatorsmarked8.7top-levelsegmentsand14.6sub-segmentspermeeting.2Incomputationallinguistics,kappavaluesover0.67pointtoreliableintercoderagreement.ButDiEugenioandGlass(2004)havefoundthatthisinterpretationdoesnotholdtrueforalltasks.3Forexample,Gruensteinetal.(2005)reportkappa(PK/WD)of0.41(0.28/0.34)fordeterminingthetop-leveland0.45(0.27/0.35)forthesub-levelsegmentsintheICSImeetingcorpus.3.4FeatureExtractionAsreportedinSection2,thereisawiderangeoffeaturesthatarepotentiallycharacteristicofsegmentboundaries,andweexpecttoﬁndsomeofthemuse-fulforautomaticrecognitionofsegmentboundaries.Thefeaturesweexplorecanbedividedintothefol-lowingﬁveclasses:ConversationalFeatures:WefollowGalleyetal.(2003)andextractedasetofconversationalfea-tures,includingtheamountofoverlappingspeech,theamountofsilencebetweenspeakersegments,speakeractivitychange,thenumberofcuewords,andthepredictionsofLCSEG(i.e.,thelexicalco-hesionstatistics,theestimatedposteriorprobability,thepredictedclass).LexicalFeatures:Wecompilethelistofwordsthatoccurmorethanonceinthespurtsthathavebeenmarkedasatop-levelorsub-segmentboundaryinthetrainingset.Eachspurtisthenrepresentedasavectorspaceofunigramsfromthislist.ProsodicFeatures:WeusethedirectmodellingapproachproposedinShribergandStolcke(2001)andincludemaximumF0andenergyofthespurt,meanF0andenergyofthespurt,pitchcontour(i.e.,slope)andenergyatmultiplepoints(e.g.,theﬁrstandlast100and200ms,theﬁrstandlastquarter,theﬁrstandsecondhalf)ofaspurt.Wealsoincluderateofspeech,in-spurtsilence,precedingandsub-sequentpauses,andduration.Therateofspeechiscalculatedasboththenumberofwordsandthenum-berofsyllablesspokenpersecond.MotionFeatures:Wemeasurethemagnitudeofrelevantmovementsinthemeetingroomusingmethodsthatdetectmovementsdirectlyfromvideorecordingsinframesof40ms.Ofspecialinterestarethefrontalshotsasrecordedbythecloseupcameras,thehandmovementsasrecordedbytheoverviewcameras,andshotsoftheareasoftheroomwherepresentationsaremade.Wethenaveragethemagni-tudeofmovementsovertheframeswithinaspurtasitsfeaturevalue.ContextualFeatures:Theseincludedialogueacttype4andspeakerrole(e.g.,projectmanager,mar-4Intheannotations,eachdialogueactisclassiﬁedasoneof15types,includingactsaboutinformationexchange(e.g.,Inform),actsaboutpossibleactions(e.g.,Suggest),actswhoseprimarypurposeistosmooththesocialfunctioning(e.g.,Be-positive),actsthatarecommentingonpreviousdiscussion(e.g.,1019

ketingexpert).Aseachspurtmayconsistofmultipledialogueacts,werepresenteachspurtasavectorofdialogueacttypes,whereinacomponentis1or0dependingonwhetherthetypeoccursinthespurt.3.5MultimodalIntegrationUsingMaximumEntropyModelsPreviousworkhasusedMaxEntmodelsforsentenceandtopicsegmentationandshownthatconditionalapproachescanyieldcompetitiveresultsonthesetasks(Christensenetal.,2005;HsuehandMoore,2006).Inthisstudy,wealsouseaMaxEntclas-siﬁer5fordialoguesegmentationunderthetypicalsupervisedlearningscheme,thatis,totraintheclas-siﬁertomaximizetheconditionallikelihoodoverthetrainingdataandthentousethetrainedmodeltopredictwhetheranunseenspurtinthetestsetisasegmentboundaryornot.BecausecontinuousfeatureshavetobediscretizedforMaxEnt,weap-pliedahistogrambinningapproach,whichdividesthevaluerangeintoNintervalsthatcontainanequalnumberofcountsasspeciﬁedinthehistogram,todiscretizethedata.4ExperimentalResults4.1ProbabilisticModelsTheﬁrstquestionwewanttoaddressiswhetherthedifferenttypesofcharacteristicmultimodalfea-turescanbeintegrated,usingtheconditionalMax-Entmodel,toautomaticallydetectsegmentbound-aries.Inthisstudy,weuseasetof50meet-ings,whichconsistsof17,977spurts.Amongthesespurts,only1.7%and3.3%aretop-levelandsub-segmentboundaries.Forourexperimentsweuse10-foldcrossvalidation.Thebaselineisthere-sultobtainedbyusingLCSEG,anunsupervisedap-proachexploitingonlylexicalcohesionstatistics.Table2showstheresultsobtainedbyusingthesamesetofconversational(CONV)featuresusedinpreviouswork(Galleyetal.,2003;HsuehandMoore,2006),andresultsobtainedbyusingalltheavailablefeatures(ALL).TheevaluationmetricsPKandWDareconventionalmeasuresoferrorratesinsegmentation(seeSection3.3).InRow2,weseeElicit-Assessment),andactsthatallowcompletesegmentation(e.g.,Stall).5TheparametersoftheMaxEntclassiﬁerareoptimizedus-ingLimited-MemoryVariableMetrics.TOPSUBErrorRatePKWDPKWDBASELINE(LCSEG)0.400.490.400.47MAXENT(CONV)0.340.340.370.37MAXENT(ALL)0.300.330.340.36Table2:ComparetheresultofMaxEntmodelstrainedwithonlyconversationalfeatures(CONV)andwithallavailablefeatures(ALL).thatusingaMaxEntclassiﬁertrainedontheconver-sationalfeatures(CONV)aloneimprovesovertheLCSEGbaselineby15.3%fortop-levelsegmentsand6.8%forsub-levelsegements.Row3showsthatcombiningadditionalknowledgesources,in-cludinglexicalfeatures(LX1)andthenon-verbalfeatures,prosody(PROS),motion(MOT),andcon-text(CTXT),yieldsafurtherimprovement(of8.8%fortop-levelsegmentationand5.4%forsub-levelsegmentation)overthemodeltrainedonconversa-tionalfeatures.4.2FeatureEffectsThesecondquestionwewanttoaddressiswhichknowledgesources(andcombinations)aregoodpredictorsforsegmentboundaries.Inthisroundofexperiments,weevaluatetheperformanceofdiffer-entfeaturecombinations.Table3furtherillustratestheimpactofeachfeatureclassontheerrorratemetrics(PK/WD).Inaddition,asthePKandWDscoredonotreﬂectthemagnitudeofover-orunder-prediction,wealsoreportontheaveragenumberofhypothesizedsegmentboundaries(Hyp).Thenum-berofreferencesegmentsintheannotationsis8.7atthetop-leveland14.6atthesub-level.Rows2-6inTable3showtheresultsofmodelstrainedwitheachindividualfeatureclass.Weper-formedaone-wayANOVAtoexaminetheeffectofdifferentfeatureclasses.TheANOVAsuggestsareliableeffectoffeatureclass(F(5,54)=36.1;p<.001).Weperformedpost-hoctests(TukeyHSD)totestforsigniﬁcantdifferences.Analysisshowsthatthemodelthatistrainedwithlexicalfeaturesalone(LX1)performssigniﬁcantlyworsethantheLCSEGbaseline(p<.001).Thisisduetothefactthatcuewords,suchasokayandnow,learnedfromthetrainingdatatosignalseg-1020

TOPSUBHypPKWDHypPKWDBASELINE17.60.400.4917.60.400.47(LCSEG)LX161.20.530.7265.10.490.66CONV3.10.340.342.90.370.37PROS2.30.350.352.50.370.37MOT96.20.360.4096.20.380.41CTXT2.60.340.342.20.370.37ALL7.70.290.337.60.350.38Table3:Effectsofindividualfeatureclassesandtheircombinationondetectingsegmentboundaries.mentboundaries,areoftenusedfornon-discoursepurposes,suchasmakingasemanticcontributiontoanutterance.6Thus,wehypothesizethattheseam-biguouscuewordshaveledtheLX1modeltoover-predict.Row7furthershowsthatwhenallavail-ablefeatures(includingLX1)areused,thecom-binedmodel(ALL)yieldsperformancethatissig-niﬁcantlybetterthanthatobtainedwithindividualfeatureclasses(F(5,54)=32.2;p<.001).TOPSUBHypPKWDHypPKWDALL7.70.290.337.60.350.38ALL-LX13.90.350.353.50.370.38ALL-CONV6.60.300.346.80.350.37ALL-PROS5.60.290.317.40.330.35ALL-MOTION7.50.300.357.30.350.37ALL-CTXT7.20.290.336.70.360.38Table4:PerformancechangeoftakingouteachindividualfeatureclassfromtheALLmodel.Table4illustratestheerrorratechange(i.e.,in-creasedordecreasedPKandWDscore)7thatisincurredbyleavingoutonefeatureclassfromtheALLmodel.ResultsshowthatCONV,PROS,MO-TIONandCTXTcanbetakenoutfromtheALLmodelindividuallywithoutincreasingtheerrorratesigniﬁcantly.8Morevoer,thecombinedmodelsal-6HirschbergandLitman(1987)haveproposedtodiscrimi-natethedifferentusesintonationally.7Notethattheincreaseinerrorrateindicatesperformancedegradation,andviceversa.8Signtestswereusedtotestforsigniﬁcantdifferencesbe-tweenmeansineachfoldofcrossvalidation.waysperformbetterthantheLX1model(p<.01),cf.Table3.Thissuggeststhatthenon-lexicalfeatureclassesarecomplementarytoLX1,andthusitisessentialtoincorporatesome,butnotnecessarilyall,ofthenon-lexicalclassesintothemodel.TOPSUBHypPKWDHypPKWDLX161.20.530.7265.10.490.66MOT96.20.360.4096.20.380.41LX1+CONV5.30.270.306.90.320.35LX1+PROS6.20.300.337.30.360.38LX1+MOT20.20.390.4924.80.390.47LX1+CTXT6.30.280.317.20.330.35MOT+PROS62.00.340.3462.10.370.37MOT+CTXT2.70.330.332.30.370.37Table5:Effectsofcombiningcomplementaryfea-turesondetectingsegmentboundaries.Table5furtherillustratestheperformanceofdif-ferentfeaturecombinationsondetectingsegmentboundaries.BysubtractingthePKorWDscoreinRow1,theLX1model,fromthatinRows3-6,wecantellhowessentialeachofthenon-lexicalclassesistobecombinedwithLX1intoonemodel.ResultsshowthatCONVisthemostessential,followedbyCTXT,PROSandMOT.Theadvantageofincorpo-ratingthenon-lexicalfeatureclassesisalsoshowninthenoticeablyreducednumberofoverpredictionsascomparedtothatoftheLX1model.Toanalyzewhetherthereisasigniﬁcantinterac-tionbetweenfeatureclasses,weperformedanotherroundofANOVAteststoexaminetheeffectofLX1andeachofthenon-lexicalfeatureclassesonde-tectingsegmentboundaries.Thisanalysisshowsthatthereisasigniﬁcantinteractioneffectonde-tectingbothtop-levelandsub-levelsegmentbound-aries(p<.01),suggestingthattheperformanceofLX1issigniﬁcantlyimprovedwhencombinedwithanynon-lexicalfeatureclass.Also,amongthenon-lexicalfeatureclasses,combiningprosodicfeaturessigniﬁcantlyimprovestheperformanceofthemodelinwhichthemotionfeaturesarecombinedtodetecttop-levelsegmentboundaries(p<.05).1021

4.3DegradationUsingASRThethirdquestionwewanttoaddresshereiswhetherusingtheoutputofASRwillcausesig-niﬁcantdegradationtotheperformanceoftheseg-mentationapproaches.TheASRtranscriptsusedinthisexperimentareobtainedusingstandardtechnol-ogyincludingHMMbasedacousticmodelingandN-grambasedlanguagemodels(Hainetal.,2005).Theaverageworderrorrates(WER)are39.1%.WealsoappliedawordalignmentalgorithmtoensurethatthenumberofwordsintheASRtranscriptsisthesameasthatinthehuman-producedtranscripts.InthiswaywecancomparethePKandWDmetricsobtainedontheASRoutputsdirectlywiththatonthehumantranscripts.Inthisstudy,weagainuseasetof50meetingsand10-foldcrossvalidation.Wecomparetheper-formanceofthereferencemodels,whicharetrainedonhumantranscriptsandtestedonhumantran-scripts,withthatoftheASRmodels,whicharetrainedonASRtranscriptsandtestedonASRtran-scripts.Table6showsthatdespitethewordrecogni-tionerrors,noneoftheLCSEG,theMaxEntmodelstrainedwithconversationalfeatures,andtheMax-Entmodelstrainedwithallavailablefeaturesper-formsigniﬁcantlyworseonASRtranscriptsthanonreferencetranscripts.Onepossibleexplanationforthis,whichwehaveobservedinourcorpus,isthattheASRsystemislikelytomis-recognizedifferentoccurencesofwordsinthesameway,andthusthelexicalcohesionstatistic,whichcapturesthesimilar-ityofwordrepetitionbetweentwoadjacencywin-dows,isalsolikelytoremainunchanged.Inaddi-tion,whenthemodelsaretrainedwithotherfeaturesthatarenotaffectedbytherecognitionerrors,suchaspauseandoverlap,thenegativeimpactsofrecog-nitionerrorsarefurtherreducedtoaninsigniﬁcantlevel.5DiscussionTheresultsinSection4showthebeneﬁtsofinclud-ingadditionalknowledgesourcesforrecognizingsegmentboundaries.Thenextquestiontobead-dressediswhatfeaturesinthesesourcesaremostusefulforrecognition.Toprovideaqualitativeac-countofthesegmentationcues,weperformedananalysistodeterminewhethereachproposedfeatureTOPSUBErrorRatePKWDPKWDLCSEG(REF)0.450.570.420.47LCSEG(ASR)0.450.580.400.47MAXENT-CONV(REF)0.340.340.370.37MAXENT-CONV(ASR)0.340.330.380.38MAXENT-ALL(REF)0.300.330.340.36MAXENT-ALL(ASR)0.310.340.340.37Table6:Effectsofwordrecognitionerrorsonde-tectingsegmentsboundaries.discriminatestheclassofsegmentboundaries.Pre-viousworkhasidentiﬁedstatisticalmeasures(e.g.,LogLikelihoodratio)thatareusefulfordetermin-ingthestatisticalassociationstrength(relevance)oftheoccurrenceofann-gramfeaturetotargetclass(HsuehandMoore,2006).HereweextendthatstudytocalculatetheLogLikelihoodrelevanceofallofthefeaturesusedintheexperiments,andusethestatisticstorankthefeatures.Ouranalysisshowsthatpeopledospeakandbe-havedifferentlynearsegmentboundaries.Someoftheidentiﬁedsegmentationcuesmatchpreviousﬁndings.Forexample,asegmentislikelytostartwithhigherpitchedsounds(Brownetal.,1980;Ay-ers,1994)andalowerrateofspeech(Lehiste,1980).Also,interlocutorspauselongerthanusualtomakesurethateveryoneisreadytomoveontoanewdis-cussion(Brownetal.,1980;PassonneauandLit-man,1993)andusesomeconventionalexpressions(e.g.,now,okay,let’s,um,so).Ouranalysisalsoidentiﬁedsegmentationcuesthathavenotbeenmentionedinpreviousresearch.Forexample,interlocutorsdonotmovearoundalotwhenanewdiscussionisbroughtup;interlocutorsmentionagendaitems(e.g.,presentation,meeting)orcontentwordsmoreoftenwheninitiatinganewdiscussion.Also,fromtheanalysisofcurrentdi-alogueacttypesandtheirimmediatecontexts,wealsoobservethatatsegmentboundariesinterlocu-torsdothefollowingmoreoftenthanusual:startspeakingbeforetheyareready(Stall),giveinfor-mation(Inform),elicitanassessmentofwhathasbeensaidsofar(Elicit-assessment),oracttosmoothsocialfunctioningandmakethegrouphappier(Be-positive).1022

6ConclusionsandFutureWorkThisstudyexplorestheuseoffeaturesfrommul-tipleknowledgesources(i.e.,words,prosody,mo-tion,interactioncues,speakerintentionandrole)fordevelopinganautomaticsegmentationcomponentinspontaneous,multipartyconversationalspeech.Inparticular,weaddressedthefollowingquestions:(1)CanaMaxEntclassiﬁerintegratethepotentiallycharacteristicmultimodalfeaturesforautomaticdi-aloguesegmentation?(2)Whatarethemostdis-criminativeknowledgesourcesfordetectingseg-mentboundaries?(3)DoestheuseofASRtran-scriptionsigniﬁcantlydegradetheperformanceofasegmentationmodel?Firstofall,ourresultsshowthatawellperform-ingMaxEntmodelcanbetrainedwithavailableknowledgesources.Ourresultsimproveonpreviouswork,whichusesonlyconversationalfeatures,by8.8%fortop-levelsegmentationand5.4%forsub-levelsegmentation.Analysisoftheeffectivenessofthevariousfeaturesshowsthatlexicalfeatures(i.e.,cuewords)arethemostessentialfeatureclasstobecombinedintothesegmentationmodel.How-ever,lexicalfeaturesmustbecombinedwithotherfeatures,inparticular,conversationalfeatures(i.e.,lexicalcohesion,overlap,pause,speakerchange),totrainwellperformingmodels.Inaddition,manyofthenon-lexicalfeatureclasses,includingthosethathavebeenidentiﬁedasindicativeofsegmentboundariesinpreviouswork(e.g.,prosody)andthosethatwehypothesizedasgoodpredictorsofsegmentboundaries(e.g.,mo-tion,context),arenotbeneﬁcialforrecognizingboundarieswhenusedinisolation.However,thesenon-lexicalfeaturesareusefulwhencombinedwithlexicalfeatures,asthepresenceofthenon-lexicalfeaturescanbalancethetendencyofmodelstrainedwithlexicalcuesalonetooverpredict.Experimentsalsoshowthatitispossibletoseg-mentconversationalspeechdirectlyontheASRout-puts.Theseresultsencouraginglyshowthatwecansegmentconversationalspeechusingfeaturesextractedfromdifferentknowledgesources,andinturn,facilitatethedevelopmentofafullyautomaticsegmentationcomponentformultimediaarchives.Withthesegmentationmodelsdevelopedanddis-criminativeknowledgesourcesidentiﬁed,aremain-ingquestioniswhetheritispossibletoautomat-icallyselectthediscriminativefeaturesforrecog-nition.Thisisparticularlyimportantforprosodicfeatures,becausethedirectmodellingapproachweadoptedresultedinalargenumberoffeatures.Weexpectthatbyapplyingfeatureselectionmethodswecanfurtherimprovetheperformanceofauto-maticsegmentationmodels.Intheﬁeldofmachinelearningandpatternanalysis,manymethodsandse-lectioncriteriahavebeenproposed.Ournextstepwillbetoexaminetheeffectivenessofthesemeth-odsforthetaskofautomaticsegmentation.Also,wewillfurtherexplorehowtochoosethebestperform-ingensembleofknowledgesourcessoastofacili-tateautomaticselectionofknowledgesourcestobeincluded.AcknowledgementThisworkwassupportedbytheEU6thFWPISTIn-tegratedProjectAMI(AugmentedMulti-partyInter-action,FP6-506811).OurspecialthankstoWesselKraaij,StephanRaaijmakers,SteveRenals,GabrielMurray,JeanCarletta,andtheanonymousreview-ersforvaluablecomments.ThanksalsototheAMIASRgroupforproducingtheASRtranscriptions,andtoourresearchpartnersinTNOforgeneratingmotionfeatures.ReferencesM.Al-Hames,A.Dielmann,D.GaticaPerez,S.Reiter,S.Renals,andD.Zhang.2005.Multimodalintegra-tionformeetinggroupactionsegmentationandrecog-nition.InProc.ofMLMI2005.J.Allan,J.Carbonell,G.Doddington,J.Yamron,andY.Yang.1998.Topicdetectionandtrackingpilotstudy:Finalreport.InProc.oftheDARPABroadcastNewsTranscriptionandUnderstandingWorkshop.G.M.Ayers.1994.Discoursefunctionsofpitchrangeinspontaneousandreadspeech.InJenniferJ.Venditti,editor,OSUWorkingPapersinLinguistics,volume44,pages1–49.S.BanerjeeandA.Rudnicky.2006.Segmentingmeet-ingsintoagendaitemsbyextractingimplicitsupervi-sionfromhumannote-taking.InProc.ofIUI2006.S.Banerjee,C.Rose,andA.I.Rudnicky.2005.Thenecessityofameetingrecordingandplaybacksystem,andthebeneﬁtoftopic-levelannotationstomeeting1023

browsing.InProc.oftheTenthInternationalConfer-enceonHuman-ComputerInteraction.G.Brown,K.L.Currie,andJ.Kenworthe.1980.Ques-tionsofIntonation.UniversityParkPress.J.Carlettaetal.2006.TheAMImeetingcorpus:Apre-announcement.InSteveRenalsandSamyBengio,ed-itors,Springer-VerlagLectureNotesinComputerSci-ence,volume3869.Springer-Verlag.H.Christensen,B.Kolluru,Y.Gotoh,andS.Renals.2005.Maximumentropysegmentationofbroadcastnews.InProc.ofICASP,PhiladelphiaUSA.B.DiEugenioandM.G.Glass.2004.Thekappastatistic:Asecondlook.ComputationalLinguistics,30(1):95–101.M.Galley,K.McKeown,E.Fosler-Lussier,andH.Jing.2003.Discoursesegmentationofmulti-partyconver-sation.InProc.ofACL2003.B.GroszandC.Sidner.1986.Attention,intentions,andthestructureofdiscourse.ComputationalLinguistics,12(3).A.Gruenstein,J.Niekrasz,andM.Purver.2005.Meet-ingstructureannotation:Dataandtools.InProc.oftheSIGdialWorkshoponDiscourseandDialogue.T.Hain,J.Dines,G.Garau,M.Karaﬁat,D.Moore,V.Wan,R.Ordelman,andS.Renals.2005.Tran-scriptionofconferenceroommeetings:Aninvestiga-tion.InProc.ofInterspeech2005.M.Hearst.1997.TextTiling:Segmentingtextintomulti-paragraphsubtopicpassages.ComputationalLinguis-tics,25(3):527–571.J.HirschbergandD.Litman.1987.Nowlet’stalkaboutnow:identifyingcuephrasesintonationally.InProc.ofACL1987.J.HirschbergandC.H.Nakatani.1996.Aprosodicanal-ysisofdiscoursesegmentsindirection-givingmono-logues.InProc.ofACL1996.P.HsuehandJ.D.Moore.2006.Automatictopicseg-mentationandlablellinginmultipartydialogue.IntheﬁrstIEEE/ACMworkshoponSpokenLanguageTech-nology(SLT)2006.H.Kozima.1993.Textsegmentationbasedonsimilaritybetweenwords.InProc.ofACL1993.I.Lehiste.1980.Phoneticcharacteristicsofdiscourse.IntheMeetingoftheCommitteeonSpeechResearch,AcousticalSocietyofJapan.G.Levow.2004.Prosody-basedtopicsegmentationformandarinbroadcastnews.InProc.ofHLT2004.I.McCowan,D.Gatica-Perez,S.Bengio,G.Lathoud,M.Barnard,andD.Zhang.2005.Automaticanalysisofmultimodalgroupactionsinmeetings.IEEETrans-actionsonPatternAnalysisandMachineIntelligence(PAMI),27(3):305–317.R.PassonneauandD.Litman.1993.Intention-basedsegmentation:Humanreliabilityandcorrelationwithlinguisticcues.InProc.ofACL1993.J.PonteandW.Croft.1997.Textsegmentationbytopic.InProc.oftheConferenceonResearchandAdvancedTechnologyforDigitalLibraries1997.J.Reynar.1998.TopicSegmentation:AlgorithmsandApplications.Ph.D.thesis,UPenn,PAUSA.E.ShribergandA.Stolcke.2001.Directmodelingofprosody:Anoverviewofapplicationsinautomaticspeechprocessing.InProc.InternationalConferenceonSpeechProsody2004.G.Tur,D.Hakkani-Tur,A.Stolcke,andE.Shriberg.2001.Integratingprosodicandlexicalcuesforauto-matictopicsegmentation.ComputationalLinguistics,27(1):31–57.K.ZechnerandA.Waibel.2000.DIASUMM:Flexi-blesummarizationofspontaneousdialoguesinunre-stricteddomains.InProc.ofCOLING-2000.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 1024–1031,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

1024

Topic Analysis for Psychiatric Document Retrieval Liang-Chih Yu*‡, Chung-Hsien Wu*, Chin-Yew Lin†, Eduard Hovy‡ and Chia-Ling Lin**Department of CSIE, National Cheng Kung University, Tainan, Taiwan, R.O.C. †Microsoft Research Asia, Beijing, China ‡Information Sciences Institute, University of Southern California, Marina del Rey, CA, USA liangchi@isi.edu,{chwu,totalwhite}@csie.ncku.edu.tw,cyl@microsoft.com,hovy@isi.edu   Abstract Psychiatric document retrieval attempts to help people to efficiently and effectively locate the consultation documents relevant to their depressive problems. Individuals can understand how to alleviate their symp-toms according to recommendations in the relevant documents. This work proposes the use of high-level topic information ex-tracted from consultation documents to im-prove the precision of retrieval results. The topic information adopted herein includes negative life events, depressive symptoms and semantic relations between symptoms, which are beneficial for better understand-ing of users' queries. Experimental results show that the proposed approach achieves higher precision than the word-based re-trieval models, namely the vector space model (VSM) and Okapi model, adopting word-level information alone. 1 Introduction Individuals may suffer from negative or stressful life events, such as death of a family member, ar-gument with a spouse and loss of a job. Such events play an important role in triggering depres-sive symptoms, such as depressed moods, suicide attempts and anxiety. Individuals under these cir-cumstances can consult health professionals using message boards and other services. Health profes-sionals respond with suggestions as soon as possi-ble. However, the response time is generally sev-eral days, depending on both the processing time required by health professionals and the number of problems to be processed. Such a long response time is unacceptable, especially for patients suffer-ing from psychiatric emergencies such as suicide attempts. A potential solution considers the prob-lems that have been processed and the correspond-ing suggestions, called consultation documents, as the psychiatry web resources. These resources gen-erally contain thousands of consultation documents (problem-response pairs), making them a useful information resource for mental health care and prevention. By referring to the relevant documents, individuals can become aware that they are not alone because many people have suffered from the same or similar problems. Additionally, they can understand how to alleviate their symptoms ac-cording to recommendations. However, browsing and searching all consultation documents to iden-tify the relevant documents is time consuming and tends to become overwhelming. Individuals need to be able to retrieve the relevant consultation documents efficiently and effectively. Therefore, this work presents a novel mechanism to automati-cally retrieve the relevant consultation documents with respect to users' problems. Traditional information retrieval systems repre-sent queries and documents using a bag-of-words approach. Retrieval models, such as the vector space model (VSM) (Baeza-Yates and Ribeiro-Neto, 1999) and Okapi model (Robertson et al., 1995; Robertson et al., 1996; Okabe et al., 2005), are then adopted to estimate the relevance between queries and documents. The VSM represents each query and document as a vector of words, and adopts the cosine measure to estimate their rele-vance. The Okapi model, which has been used on the Text REtrieval Conference (TREC) collections, developed a family of word-weighting functions 1025

for relevance estimation. These functions consider word frequencies and document lengths for word weighting. Both the VSM and Okapi models esti-mate the relevance by matching the words in a query with the words in a document. Additionally, query words can further be expanded by the con-cept hierarchy within general-purpose ontologies such as WordNet (Fellbaum, 1998), or automati-cally constructed ontologies (Yeh et al., 2004). However, such word-based approaches only consider the word-level information in queries and documents, ignoring the high-level topic informa-tion that can help improve understanding of users' queries. Consider the example consultation docu-ment in Figure 1. A consultation document com-prises two parts: the query part and recommenda-tion part. The query part is a natural language text, containing rich topic information related to users' depressive problems. The topic information in-cludes negative life events, depressive symptoms, and semantic relations between symptoms. As in-dicated in Figure 1, the subject suffered from a love-related event, and several depressive symp-toms, such as <Depressed>, <Suicide>, <Insom-nia> and <Anxiety>. Moreover, there is a cause-effect relation holding between <Depressed> and <Suicide>, and a temporal relation holding be-tween <Depressed> and <Insomnia>. Different topics may lead to different suggestions decided by experts. Therefore, an ideal retrieval system for consultation documents should consider such topic information so as to improve the retrieval precision. Natural language processing (NLP) techniques can be used to extract more precise information from natural language texts (Wu et al., 2005a; Wu et al., 2005b; Wu et al., 2006; Yu et al., 2007). This work adopts the methodology presented in (Wu et al. 2005a) to extract depressive symptoms and their relations, and adopts the pattern-based method presented in (Yu et al., 2007) to extract negative life events from both queries and consul-tation documents. This work also proposes a re-trieval model to calculate the similarity between a query and a document by combining the similari-ties of the extracted topic information. The rest of this work is organized as follows. Section 2 briefly describes the extraction of topic information. Section 3 presents the retrieval model. Section 4 summarizes the experimental results. Conclusions are finally drawn in Section 5. 2 Framework of Consultation Document Retrieval Figure 2 shows the framework of consultation document retrieval. The retrieval process begins with receiving a user’s query about his depressive problems in natural language. The example query is shown in Figure 1. The topic information is then extracted from the query, as shown in the center of Figure 2. The extracted topic information is repre-Consultation DocumentQuery:It's normal to feel this way when going through these kinds of struggles, but over time your emotions should level out. Suicide doesn't solve anything; think about how it would affect your family........ There are a few things you can try to help you get to sleep at night, like drinking warm milk, listening to relaxing music....... Recommendation:After that, it took me a long time to fall asleep at night.  <Depressed><Suicide><Insomnia><Anxiety>cause-effecttemporalI broke up with my boyfriend.I often felt like crying and felt pain every day. So, I tried to kill myself several times. In recent months, I often lose my temper for no reason. Figure 1.  Example of a consultation document. The bold arrowed lines denote cause-effect relations; ar-rowed lines denote temporal relations; dashed lines denote temporal boundaries, and angle brackets de-note depressive symptoms 1026

sented by the sets of negative life events, depres-sive symptoms, and semantic relations. Each ele-ment in the event set and symptom set denotes an individual event and symptom, respectively, while each element in the relation set denotes a symptom chain to retain the order of symptoms. Similarly, the query parts of consultation documents are rep-resented in the same manner. The relevance esti-mation then calculates the similarity between the input query and the query part of each consultation document by combining the similarities of the sets of events, symptoms, and relations within them. Finally, a list of consultation documents ranked in the descending order of similarities is returned to the user. In the following, the extraction of topic informa-tion is described briefly. The detailed process is described in (Wu et al. 2005a) for symptom and relation identification, and in (Yu et al., 2007) for event identification. 1) Symptom identification: A total of 17 symp-toms are defined based on the Hamilton De-pression Rating Scale (HDRS) (Hamilton, 1960). The identification of symptoms is sen-tence-based. For each sentence, its structure is first analyzed by a probabilistic context free grammar (PCFG), built from the Sinica Tree-bank corpus developed by Academia Sinica, Taiwan (http://treebank.sinica.edu.tw), to gen-erate a set of dependencies between word to-kens. Each dependency has the format (modi-fier, head, relmodifier,head). For instance, the de-pendency (matters, worry about, goal) means that "matters" is the goal to the head of the sen-tence "worry about". Each sentence can then be associated with a symptom based on the probabilities that dependencies occur in all symptoms, which are obtained from a set of training sentences. 2) Relation Identification: The semantic rela-tions of interest include cause-effect and tem-poral relations. After the symptoms are ob-tained, the relations holding between symp-toms (sentences) are identified by a set of dis-course markers. For instance, the discourse markers "because" and "therefore" may signal cause-effect relations, and "before" and "after" may signal temporal relations. 3) Negative life event identification: A total of 5 types of events, namely <Family>, <Love>, <School>, <Work> and <Social> are defined based on Pagano et al’s (2004) research. The identification of events is a pattern-based ap-proach. A pattern denotes a semantically plau-sible combination of words, such as <parents, divorce> and <exam, fail>. First, a set of pat-terns is acquired from psychiatry web corpora by using an evolutionary inference algorithm. The event of each sentence is then identified by using an SVM classifier with the acquired patterns as features. 3 Retrieval Model The similarity between a query and a document, (,)Simqd, is calculated by combining the similari-ties of the sets of events, symptoms and relations within them, as shown in (1). ConsultationDocumentsRankingRelevanceEstimationQuery(Figure 1)Topic InformationSymptom IdentificationNegative Life EventIdentificationRelationIdentificationDSADSCause-EffectDIATemporalISIA<Love>Topic Analysis Figure 2.  Framework of consultation document retrieval. The rectangle denotes a negative life event re-lated to love relation. Each circle denotes a symptom. D: Depressed, S: Suicide, I: Insomnia, A: Anxiety. 1027

(,)(,)(,)(1)(,),EvnSymRelSimqdSimqdSimqdSimqdαβαβ=++−−  (1) where (,)EvnSimqd, (,)SymSimqd and (,)RelSimqd, denote the similarities of the sets of events, symp-toms and relations, respectively, between a query and a document, and α and βdenote the combi-nation factors. 3.1 Similarity of events and symptoms The similarities of the sets of events and symptoms are calculated in the same method. The similarity of the event set (or symptom set) is calculated by comparing the events (or symptoms) in a query with those in a document. Additionally, only the events (or symptoms) with the same type are considered. The events (or symptoms) with different types are considered as irrelevant, i.e., no similarity. For instance, the event <Love> is considered as irrelevant to <Work>. The similarity of the event set is calculated by (,)1(,)cos(,).,()EvnqdqdqdeqdSimqdTypeeeeeconstNEvnEvn∈∩=+∪∑ (2) where qEvn and dEvn denote the event set in a query and a document, respectively; qe and de denote the events; ()qdNEvnEvn∪ denotes the cardinality of the union of qEvn and dEvn as a normalization factor, and (,)qdTypeee denotes an identity function to check whether two events have the same type, defined as 1     ()()(,).0    otherwiseqdqdTypeeTypeeTypeee=⎧⎪=⎨⎪⎩  (3) The cos(,)qdee denotes the cosine angle between two vectors of words representing qe and de, as shown below. ()()12211cos(,),qdqdTiieeiqdTTiieeiiwweeww====∑∑∑  (4) where w denotes a word in a vector, and T denotes the dimensionality of vectors. Accordingly, when two events have the same type, their similarity is given as cos(,)qdee plus a constant, const.. Addi-tionally, cos(,)qdee and const. can be considered as the word-level and topic-level similarities, re-spectively. The optimal setting of const. is deter-mined empirically. 3.2 Similarity of relations When calculating the similarity of relations, only the relations with the same type are considered. That is, the cause-effect (or temporal) relations in a query are only compared with the cause-effect (or temporal) relations in a document. Therefore, the similarity of relation sets can be calculated as ,1(,)(,)(,),qdRelqdqdrrSimqdTyperrSimrrZ=∑ (5) ()()()(),CqCdTqTdZNrNrNrNr=+  (6) where qrand drdenote the relations in a query and a document, respectively; Z denotes the normaliza-tion factor for the number of relations; (,)qdTypeee denotes an identity function similar to (3), and ()CNi  and ()TNi denote the numbers of cause-effect and temporal relations. Both cause-effect and temporal relations are rep-resented by symptom chains. Hence, the similarity of relations is measured by the similarity of symp-tom chains. The main characteristic of a symptom chain is that it retains the cause-effect or temporal order of the symptoms within it. Therefore, the order of the symptoms must be considered when calculating the similarity of two symptom chains. Accordingly, a sequence kernel function (Lodhi et al., 2002; Cancedda et al., 2003) is adopted to cal-culate the similarity of two symptom chains. A sequence kernel compares two sequences of sym-bols (e.g., characters, words) based on the subse-quences within them, but not individual symbols. Thereby, the order of the symptoms can be incor-porated into the comparison process. The sequence kernel calculates the similarity of two symptom chains by comparing their sub-symptom chains at different lengths. An increasing number of common sub-symptom chains indicates a greater similarity between two symptom chains. For instance, both the two symptom chains 1234ssss and 321sss contain the same symptoms 1s, 2s and 3s, but in different orders. To calculate the similarity between these two symptom chains, the sequence kernel first calculates their similarities at length 2 and 3, and then averages the similarities at the two lengths. To calculate the similarity at 1028

length 2, the sequence kernel compares their sub-symptom chains of length 2, i.e., 121314232434{,,,,,}ssssssssssss and 323121{,,}ssssss. Similarly, their similarity at length 3 is calculated by comparing their sub-symptom chains of length 3, i.e., 123124134234{, , , }ssssssssssss and 321{}sss. Obviously, no similarity exists between 1234ssss and 321sss, since no sub-symptom chains are matched at both lengths. In this example, the sub-symptom chains of length 1, i.e., individual symp-toms, do not have to be compared because they contain no information about the order of symp-toms. Additionally, the sub-symptom chains of length 4 do not have to be compared, because the two symptom chains share no sub-symptom chains at this length. Hence, for any two symptom chains, the length of the sub-symptom chains to be com-pared ranges from two to the minimum length of the two symptom chains. The similarity of two symptom chains can be formally denoted as 1212122(,)(,)                (,)1                (,),1NNqdqdNNqdNNNnqdnSimrrSimscscKscscKscscN=≡==−∑  (7) where 1Nqsc and 2Ndsc denote the symptom chains corresponding to qr and dr, respectively;1N and 2N denote the length of 1Nqsc and 2Ndsc, respec-tively; (  ,  )Kii denotes the sequence kernel for calculating the similarity between two symptom chains; (  ,  )nKii denotes the sequence kernel for calculating the similarity between two symptom chains at length n, and N is the minimum length of the two symptom chains, i.e., 12min(,)NNN=. The sequence kernel 12(,)NNnijKscsc is defined as 211212121122()()(,)  ()()()()         ,()()()()nnnNNnjNNninijNNninjNNuiujuSCNNNNuiujuiujuSCuSCscscKscscscscscscscscscscφφφφφφ∈∈∈ΦΦ=ΦΦ=∑∑∑i(8) where 12(,)NNnijKscsc is the normalized inner product of vectors 1()NniscΦ and 2()NnjscΦ; ()nΦi denotes a mapping that transforms a given symp-tom chain into a vector of the sub-symptom chains of length n; ()uφi denotes an element of the vector, representing the weight of a sub-symptom chain u, and nSC denotes the set of all possible sub-symptom chains of length n. The weight of a sub-symptom chain, i.e., ()uφi, is defined as 1111       is a contiguous sub-symptom chain of     is a non-contiguous sub-symptom chain()         with  skipped symptoms0       does not appear in ,NiNuiNiuscuscuscθλφθ⎧⎪⎪=⎨⎪⎪⎩(9) where [0,1]λ∈ denotes a decay factor that is adopted to penalize the non-contiguous sub-symptom chains occurred in a symptom chain based on the skipped symptoms. For instance, 1223123123()()1ssssssssssφφ== since 12ss and 23ss are considered as contiguous in 123sss, and 131123()sssssφλ= since 13ss is a non-contiguous sub-symptom chain with one skipped symptom. The decay factor is adopted because a contiguous sub-symptom chain is preferable to a non-contiguous chain when comparing two symptom chains. The setting of the decay factor is domain dependent. If 1λ=, then no penalty is applied for skipping symptoms, and the cause-effect and tem-poral relations are transitive. The optimal setting of Figure 3.  Illustrative example of relevance com-putation using the sequence kernel function. 1029

λ is determined empirically. Figure 3 presents an example to summarize the computation of the similarity between two symptom chains. 4 Experimental Results 4.1 Experiment setup 1) Corpus: The consultation documents were collected from the mental health website of the John Tung Foundation (http://www.jtf.org.tw) and the PsychPark (http://www.psychpark.org), a virtual psychiatric clinic, maintained by a group of volunteer professionals of Taiwan Association of Mental Health Informatics (Bai et al. 2001). Both of the web sites provide various kinds of free psychiatric services and update the consultation documents periodically. For privacy consideration, all personal infor-mation has been removed. A total of 3,650 consultation documents were collected for evaluating the retrieval model, of which 20 documents were randomly selected as the test query set, 100 documents were randomly se-lected as the tuning set to obtain the optimal parameter settings of involved retrieval models, and the remaining 3,530 documents were the reference set to be retrieved. Table 1 shows the average number of events, symptoms and rela-tions in the test query set. 2) Baselines: The proposed method, denoted as Topic, was compared to two word-based re-trieval models: the VSM and Okapi BM25 models. The VSM was implemented in terms of the standard TF-IDF weight. The Okapi BM25 model is defined as (1)3123(1)(1)||,tQkqtfktfavdldlwkQKtfkqtfavdldl∈++−++++∑ (10) where t denotes a word in a query Q; qtf and tf denote the word frequencies occurring in a query and a document, respectively, and  (1)w denotes the Robertson-Sparck Jones weight of t (without relevance feedback), defined as (1)0.5log,0.5Nnwn−+=+            (11) where N denotes the total number of docu-ments, and n denotes the number of documents containing t. In (10), K is defined as 1((1)/),Kkbbdlavdl=−+⋅            (12) where dl and avdl denote the length and aver-age length of a document, respectively. The default values of 1k, 2k, 3k and b are describe in (Robertson et al., 1996), where 1k ranges from 1.0 to 2.0; 2k is set to 0; 3k is set to 8, and b ranges from 0.6 to 0.75. Additionally, BM25 can be considered as BM15 and BM11 when b is set to 1 and 0, respectively. 3) Evaluation metric: To evaluate the retrieval models, a multi-level relevance criterion was adopted. The relevance criterion was divided into four levels, as described below. (cid:122) Level 0: No topics are matched between a query and a document. (cid:122) Level 1: At least one topic is partially matched between a query and a document. (cid:122) Level 2: All of the three topics are partially matched between a query and a document. (cid:122) Level 3: All of the three topics are partially matched, and at least one topic is exactly matched between a query and a document. To deal with the multi-level relevance, the dis-counted cumulative gain (DCG) (Jarvelin and Kekalainen, 2000) was adopted as the evalua-tion metric, defined as [1],                                  1     [][1][]/log,otherwisecGifiDCGiDCGiGii=⎧⎪=⎨−+⎪⎩(13) where i denotes the i-th document in the re-trieved list; G[i] denotes the gain value, i.e., relevance levels, of the i-th document, and c denotes the parameter to penalize a retrieved document in a lower rank. That is, the DCG simultaneously considers the relevance levels, and the ranks in the retrieved list to measure the retrieval precision. For instance, let <3,2,3,0,0> denotes the retrieved list of five documents with their relevance levels. If no penalization is used, then the DCG values for Topic Avg. NumberNegative Life Event 1.45 Depressive Symptom 4.40 Semantic Relation 3.35 Table 1. Characteristics of the test query set. 1030

the retrieved list are <3,5,8,8,8>, and thus DCG[5]=8. Conversely, if c=2, then the docu-ments retrieved at ranks lower than two are pe-nalized. Hence, the DCG values for the re-trieved list are <3,5,6.89,6.89,6.89>, and DCG[5]=6.89. The relevance judgment was performed by three experienced physicians. First, the pooling method (Voorhees, 2000) was adopted to gen-erate the candidate relevant documents for each test query by taking the top 50 ranked documents retrieved by each of the involved retrieval models, namely the VSM, BM25 and Topic. Two physicians then judged each can-didate document based on the multilevel rele-vance criterion. Finally, the documents with disagreements between the two physicians were judged by the third physician. Table 2 shows the average number of relevant docu-ments for the test query set. 4) Optimal parameter setting: The parameter settings of BM25 and Topic were evaluated us-ing the tuning set. The optimal setting of BM25 were k1 =1 and b=0.6. The other two pa-rameters were set to the default values, i.e., 20k= and 38k=. For the Topic model, the parameters required to be evaluated include the combination factors, α and β, described in (1); the constant const. described in (2), and the decay factor, λ, described in (9). The op-timal settings were 0.3α=; 0.5β=; const.=0.6 and 0.8λ=. 4.2 Retrieval results The results are divided into two groups: the preci-sion and efficiency. The retrieval precision was measured by DCG values. Additionally, a paired, two-tailed t-test was used to determine whether the performance difference was statistically significant. The retrieval efficiency was measure by the query processing time, i.e., the time for processing all the queries in the test query set. Table 3 shows the comparative results of re-trieval precision. The two variants of BM25, namely BM11 and BM15, are also considered in comparison. For the word-based retrieval models, both BM25 and BM11 outperformed the VSM, and BM15 performed worst. The Topic model achieved higher DCG values than both the BM-series models and VSM. The reasons are three-fold. First, a negative life event and a symptom can each be expressed by different words with the same or similar meaning. Therefore, the word-based mod-els often failed to retrieve the relevant documents when different words were used in the input query. Second, a word may relate to different events and symptoms. For instance, the term "worry about" is Relevance Level Avg. NumberLevel 1 18.50 Level 2 9.15 Level 3 2.20 Table 2. Average number of relevant documents for the test query set.  DCG(5) DCG(10)DCG(20)DCG(50)DCG(100) Topic 4.7516* 6.9298 7.6040* 8.3606* 9.3974* BM25 4.4624 6.7023 7.1156 7.8129 8.6597 BM11 3.8877 4.9328 5.9589 6.9703 7.7057 VSM 2.3454 3.3195 4.4609 5.8179 6.6945 BM15 2.1362 2.6120 3.4487 4.5452 5.7020 Table 3. DCG values of different retrieval models.  * Topic vs BM25 significantly different (p<0.05) Retrieval ModelAvg. Time (seconds)Topic 17.13 VSM 0.68 BM25 0.48 Table 4. Average query processing time of differ-ent retrieval models. 1031

a good indicator for both the symptoms <Anxiety> and <Hypochondriasis>. This may result in ambi-guity for the word-based models. Third, the word-based models cannot capture semantic relations between symptoms. The Topic model incorporates not only the word-level information, but also more useful topic information about depressive problems, thus improving the retrieval results. The query processing time was measured using a personal computer with Windows XP operating system, a 2.4GHz Pentium IV processor and 512MB RAM. Table 4 shows the results. The topic model required more processing time than both VSM and BM25, since identification of topics in-volves more detailed analysis, such as semantic parsing of sentences and symptom chain construc-tion. This finding indicates that although the topic information can improve the retrieval precision, incorporating such high-precision features reduces the retrieval efficiency. 5 Conclusion This work has presented the use of topic informa-tion for retrieving psychiatric consultation docu-ments. The topic information can provide more precise information about users' depressive prob-lems, thus improving the retrieval precision. The proposed framework can also be applied to differ-ent domains as long as the domain-specific topic information is identified. Future work will focus on more detailed experiments, including the contribu-tion of each topic to retrieval precision, the effect of using different methods to combine topic infor-mation, and the evaluation on real users. References Baeza-Yates, R. and B. Ribeiro-Neto. 1999. Modern Information Retrieval. Addison-Wesley, Reading, MA. Cancedda, N., E. Gaussier, C. Goutte, and J. M. Renders. 2003. Word-Sequence Kernels. Journal of Machine Learning Research, 3(6):1059-1082. Fellbaum, C. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge, MA. Hamilton, M. 1960. A Rating Scale for Depression. Journal of Neurology, Neurosurgery and Psychiatry, 23:56-62 Jarvelin, K. and J. Kekalainen. 2000. IR Evaluation Methods for Retrieving Highly Relevant Documents. In Proc. of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 41-48. Lodhi, H., C. Saunders, J. Shawe-Taylor, N. Cristianini, and C. Watkins. 2002. Text Classification Using String Kernels. Journal of Machine Learning Re-search, 2(3):419-444. Okabe, M., K. Umemura and S. Yamada. 2005. Query Expansion with the Minimum User Feedback by Transductive Learning. In Proc. of HLT/EMNLP, Vancouver, Canada, pages 963-970. Pagano, M.E., A.E. Skodol, R.L. Stout, M.T. Shea, S. Yen, C.M. Grilo, C.A. Sanislow, D.S. Bender, T.H. McGlashan, M.C. Zanarini, and J.G. Gunderson. 2004. Stressful Life Events as Predictors of Function-ing: Findings from the Collaborative Longitudinal Personality Disorders Study. Acta Psychiatrica Scan-dinavica, 110: 421-429. Robertson, S. E., S. Walker, S. Jones, M. M. Hancock-Beaulieu, and M.Gatford. 1995. Okapi at TREC-3. In Proc. of the Third Text REtrieval Conference (TREC-3), NIST. Robertson, S. E., S. Walker, M. M. Beaulieu, and M.Gatford. 1996. Okapi at TREC-4. In Proc. of the fourth Text REtrieval Conference (TREC-4), NIST. Voorhees, E. M. and D. K. Harman. 2000. Overview of the Sixth Text REtrieval Conference (TREC-6). In-formation Processing and Management, 36(1):3-35. Wu, C. H., L. C. Yu, and F. L. Jang. 2005a. Using Se-mantic Dependencies to Mine Depressive Symptoms from Consultation Records. IEEE Intelligent System, 20(6):50-58. Wu, C. H., J. F. Yeh, and M. J. Chen. 2005b. Domain-Specific FAQ Retrieval Using Independent Aspects. ACM Trans. Asian Language Information Processing, 4(1):1-17. Wu, C. H., J. F. Yeh, and Y. S. Lai. 2006. Semantic Segment Extraction and Matching for Internet FAQ Retrieval. IEEE Trans. Knowledge and Data Engi-neering, 18(7):930-940. Yeh, J. F., C. H. Wu, M. J. Chen, and L. C. Yu. 2004. Automated Alignment and Extraction of Bilingual Domain Ontology for Cross-Language Domain-Specific Applications. In Proc. of the 20th COLING, Geneva, Switzerland, pages 1140-1146. Yu, L. C., C. H. Wu, Yeh, J. F., and F. L. Jang. 2007. HAL-based Evolutionary Inference for Pattern Induc-tion from Psychiatry Web Resources. Accepted by IEEE Trans. Evolutionary Computation. Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 1032–1039,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

1032

Whattobe?-ElectronicCareerGuidanceBasedonSemanticRelatednessIrynaGurevych,ChristofM¨ullerandTorstenZeschUbiquitousKnowledgeProcessingGroupTelecooperation,DarmstadtUniversityofTechnologyHochschulstr.10,64289Darmstadt,Germanyhttp://www.ukp.tu-darmstadt.de{gurevych,mueller,zesch}@tk.informatik.tu-darmstadt.deAbstractWepresentastudyaimedatinvestigatingtheuseofsemanticinformationinanovelNLPapplication,ElectronicCareerGuid-ance(ECG),inGerman.ECGisformu-latedasaninformationretrieval(IR)task,wherebytextualdescriptionsofprofessions(documents)arerankedfortheirrelevancetonaturallanguagedescriptionsofaper-son’sprofessionalinterests(thetopic).WecomparetheperformanceoftwosemanticIRmodels:(IR-1)utilizingsemanticrelat-edness(SR)measuresbasedoneitherword-netorWikipediaandasetofheuristics,and(IR-2)measuringthesimilaritybetweenthetopicanddocumentsbasedonExplicitSemanticAnalysis(ESA)(GabrilovichandMarkovitch,2007).Weevaluatetheperfor-manceofSRmeasuresintrinsicallyonthetasksof(T-1)computingSR,and(T-2)solv-ingReader’sDigestWordPower(RDWP)questions.1ElectronicCareerGuidanceCareerguidanceisimportantbothforthepersonin-volvedandforthestate.Notwellinformeddeci-sionsmaycausepeopletodropthetrainingprogramtheyareenrolledin,yieldinglossoftimeandﬁnan-cialinvestments.However,thereisamismatchbet-weenwhatpeopleknowaboutexistingprofessionsandthevarietyofprofessions,whichexistinreal-ity.Somestudiesreportthatschoolleaverstypi-callychoosetheprofessionsknowntothem,suchaspoliceman,nurse,etc.Manyotherprofessions,whichcanpossiblymatchtheinterestsofthepersonverywell,arenotchosen,astheirtitlesareunknownandpeopleseekingcareeradvicedonotknowabouttheirexistence,e.g.electronicsinstaller,orchem-icallaboratoryworker.However,peopleareverygoodatdescribingtheirprofessionalinterestsinnat-urallanguage.Thatiswhytheyareevenaskedtowriteashortessaypriortoanappointmentwithacareerguidanceexpert.Electroniccareerguidanceis,thus,asupplementtocareerguidancebyhumanexperts,helpingyoungpeopletodecidewhichprofessiontochoose.Thegoalistoautomaticallycomputearankedlistofpro-fessionsaccordingtotheuser’sinterests.AcurrentsystememployedbytheGermanFederalLabourOfﬁce(GFLO)intheirautomaticcareerguidancefront-end1isbasedonvocationaltrainings,manu-allyannotatedusingatagsetof41keywords.Theusermustselectappropriatekeywordsaccordingtoherinterests.Inreply,thesystemconsultsaknowl-edgebasewithprofessionsmanuallyannotatedwiththekeywordsbycareerguidanceexperts.There-after,itoutputsalistofthebestmatchingprofes-sionstotheuser.Thisapproachhastwosigniﬁcantdisadvantages.Firstly,theknowledgebasehastobemaintainedandsteadilyupdated,asthenumberofprofessionsandkeywordsassociatedwiththemiscontinuouslychanging.Secondly,theuserhastodescribeherinterestsinaveryrestrictedway.Atthesametime,GFLOmaintainsanextensivedatabasewithtextualdescriptionsofprofessions,1http://www.interesse-beruf.de/1033

calledBERUFEnet.2Therefore,wecasttheprob-lemofECGasanIRtask,tryingtoremovethedisadvantagesofconventionalECGoutlinedabovebylettingtheuserdescribeherinterestsinashortnaturallanguageessay,calledaprofessionalproﬁle.ExampleessaytranslatedtoEnglishIwouldliketoworkwithanimals,totreatandlookafterthem,butIcannotstandthesightofbloodandtaketoomuchpityonthem.Ontheotherhand,Iliketoworkonthecomputer,canprograminC,PythonandVBandsoIcouldconsidersoftwaredevelopmentasanappropriateprofession.Icannotimagineworkinginakindergarden,asasocialworkerorasateacher,asIamnotverygoodatassertingmyself.TextualdescriptionsofprofessionsarerankedgivensuchanessaybyusingNLPandIRtech-niques.Asessaysanddescriptionsofprofessionsdisplayamismatchbetweenthevocabulariesoftop-icsanddocumentsandthereislackofcontextualin-formation,duetothedocumentsbeingfairlyshortascomparedtostandardIRscenarios,lexicalse-manticinformationshouldbeespeciallybeneﬁcialtoanIRsystem.Forexample,theproﬁlecancon-tainwordsaboutsomeobjectsoractivitiesrelatedtotheprofession,butnotdirectlymentionedinthede-scription,e.g.oven,cakesintheproﬁleandpastries,baker,orconfectionerinthedocument.Therefore,weproposetoutilizesemanticrelatednessasarank-ingfunctioninsteadofconventionalIRtechniques,aswillbesubstantiatedbelow.2SystemArchitectureIntegratinglexicalsemanticknowledgeinECGre-quirestheexistenceofknowledgebasesencodingdomainandlexicalknowledge.Inthispaper,wein-vestigatetheutilityoftwoknowledgebases:(i)aGermanwordnet,GermaNet(Kunze,2004),and(ii)theGermanportionofWikipedia.3AlargebodyofresearchexistsonusingwordnetsinNLPapplica-tionsandinparticularinIR(MoldovanandMihal-cea,2000).Theknowledgeinwordnetshasbeentypicallyutilizedbyexpandingquerieswithrelatedterms(Vorhees,1994;Smeatonetal.,1994),con-ceptindexing(Gonzaloetal.,1998),orsimilaritymeasuresasrankingfunctions(Smeatonetal.,1994;M¨ullerandGurevych,2006).Recently,Wikipedia2http://infobub.arbeitsagentur.de/berufe/3http://de.wikipedia.org/hasbeendiscoveredasapromisinglexicalseman-ticresourceandsuccessfullyusedinsuchdifferentNLPtasksasquestionanswering(Ahnetal.,2004),namedentitydisambiguation(BunescuandPasca,2006),andinformationretrieval(Katzetal.,2005).Furtherresearch(Zeschetal.,2007b)indicatesthatGermanwordnetandWikipediashowdifferentper-formancedependingonthetaskathand.Departingfromthis,weﬁrstcomparetwoseman-ticrelatedness(SR)measuresbasedontheinforma-tioneitherintheGermanwordnet(Lin,1998)calledLIN,orinWikipedia(GabrilovichandMarkovitch,2007)calledExplicitSemanticAnalysis,orESA.Weevaluatetheirperformanceintrinsicallyonthetasksof(T-1)computingsemanticrelatedness,and(T-2)solvingReader’sDigestWordPower(RDWP)questionsandmakeconclusionsabouttheabilityofthemeasurestomodelcertainaspectsofsemanticrelatednessandtheircoverage.Furthermore,wefol-lowtheapproachbyM¨ullerandGurevych(2006),whoproposedtoutilizetheLINmeasureandasetofheuristicsasanIRmodel(IR-1).Additionally,weutilizetheESAmeasureinasemanticinformationretrievalmodel,asthismea-sureissigniﬁcantlybetteratvocabularycover-ageandatmodellingcrosspart-of-speechrelations(GabrilovichandMarkovitch,2007).WecomparetheperformanceofESAandLINmeasuresinatask-basedIRevaluationandanalyzetheirstrengthsandlimitations.Finally,weapplyESAtodirectlycom-putetextsimilaritiesbetweentopicsanddocuments(IR-2)andcomparetheperformanceoftwoseman-ticIRmodelsandabaselineExtendedBoolean(EB)model(Saltonetal.,1983)withqueryexpansion.4Tosummarize,thecontributionsofthispaperarethree-fold:(i)wepresentanovelsystem,utilizingNLPandIRtechniquestoperformElectronicCareerGuidance,(ii)westudythepropertiesandintrinsi-callyevaluatetwoSRmeasuresbasedonGermaNetandWikipediaforthetasksofcomputingseman-ticrelatednessandsolvingReader’sDigestWordPowerGamequestions,and(iii)weinvestigatetheperformanceoftwosemanticIRmodelsinataskbasedevaluation.4WealsoranexperimentswithOkapiBM25modelasim-plementedintheTerrierframework,buttheresultswereworsethanthosewiththeEBmodel.Therefore,welimitourdiscus-siontothelatter.1034

3ComputingSemanticRelatedness3.1SRMeasuresGermaNetbasedmeasuresGermaNetisaGer-manwordnet,whichadoptedthemajorpropertiesanddatabasetechnologyfromPrinceton’sWord-Net(Fellbaum,1998).However,GermaNetdis-playssomestructuraldifferencesandcontentori-entedmodiﬁcations.Itsdesignersreliedmainlyonlinguisticevidence,suchascorpusfrequency,ratherthanpsycholinguisticmotivations.Also,GermaNetemploysartiﬁcial,i.e.non-lexicalizedconcepts,andadjectivesarestructuredhierarchicallyasopposedtoWordNet.Currently,GermaNetincludesabout40000synsetswithmorethan60000wordsensesmodellingnouns,verbsandadjectives.WeusethesemanticrelatednessmeasurebyLin(1998)(referredtoasLIN),asitconsistentlyisamongthebestperformingwordnetbasedmeasures(GurevychandNiederlich,2005;BudanitskyandHirst,2006).Lindeﬁnedsemanticsimilarityusingaformuladerivedfrominformationtheory.Thismea-sureissometimescalledauniversalsemanticsim-ilaritymeasureasitissupposedtobeapplication,domain,andresourceindependent.Liniscomputedas:simc1,c2=2×logp(LCS(c1,c2))logp(c1)+logp(c2)wherec1andc2areconcepts(wordsenses)corre-spondingtow1andw2,logp(c)istheinformationcontent,andLCS(c1,c2)isthelowestcommonsub-sumerofthetwoconcepts.Theprobabilitypiscom-putedastherelativefrequencyofwords(represent-ingthatconcept)inthetaz5corpus.WikipediabasedmeasuresWikipediaisafreeonlineencyclopediathatisconstructedinacol-laborativeeffortofvoluntarycontributorsandstillgrowsexponentially.Duringthisprocess,Wikipediahasprobablybecomethelargestcollectionoffreelyavailableknowledge.Wikipediasharesmanyofitspropertieswithotherwellknownlexicalseman-ticresources(likedictionaries,thesauri,semanticwordnetsorconventionalencyclopedias)(Zeschetal.,2007a).AsWikipediaalsomodelsrelatednessbetweenconcepts,itisbettersuitedforcomputing5http://www.taz.desemanticrelatednessthanGermaNet(Zeschetal.,2007b).Inveryrecentwork,GabrilovichandMarkovitch(2007)introduceaSRmeasurecalledExplicitSe-manticAnalysis(ESA).TheESAmeasurerepre-sentsthemeaningofatermasahigh-dimensionalconceptvector.TheconceptvectorisderivedfromWikipediaarticles,aseacharticlefocusesonacer-taintopic,andcanthusbeviewedasexpressingaconcept.ThedimensionoftheconceptvectoristhenumberofWikipediaarticles.EachelementofthevectorisassociatedwithacertainWikipediaarticle(orconcept).Ifthetermcanbefoundinthisarticle,theterm’stﬁdfscore(SaltonandMcGill,1983)inthisarticleisassignedtothevectorelement.Oth-erwise,0isassigned.Asaresult,aterm’scon-ceptvectorrepresentstheimportanceofthetermforeachconcept.Semanticrelatednessoftwotermscanthenbeeasilycomputedasthecosineoftheircorre-spondingconceptvectors.Ifwewanttomeasurethesemanticrelatednessoftextsinsteadofterms,wecanalsouseESAconceptvectors.Atextisrep-resentedastheaverageconceptvectorofitsterms’conceptvectors.Then,therelatednessoftwotextsiscomputedasthecosineoftheiraverageconceptvectors.AsESAusesalltextualinformationinWikipedia,themeasureshowsexcellentcoverage.Therefore,weselectitasthesecondmeasureforintegrationintoourIRsystem.3.2DatasetsSemanticrelatednessdatasetsforGermanem-ployedinourstudyarepresentedinTable1.Gurevych(2005)conductedexperimentswithtwodatasets:i)aGermantranslationoftheEnglishdatasetbyRubensteinandGoodenough(1965)(Gur65),andii)alargerdatasetcontaining350wordpairs(Gur350).ZeschandGurevych(2006)createdathirddatasetfromdomain-speciﬁccorporausingasemi-automaticprocess(ZG222).Gur65israthersmallandcontainsonlynoun-nounpairscon-nectedbyeithersynonymyorhypernymy.Gur350containsnouns,verbsandadjectivesthatarecon-nectedbyclassicalandnon-classicalrelations(Mor-risandHirst,2004).However,wordpairsforthisdatasetarebiasedtowardsstrongclassicalrela-tions,astheyweremanuallyselectedfromacorpus.1035

CORRELATIONrDATASETYEARLANGUAGE#PAIRSPOSSCORES#SUBJECTSINTERINTRAGur652005German65Ndiscrete{0,1,2,3,4}24.810-Gur3502006German350N,V,Adiscrete{0,1,2,3,4}8.690-ZG2222006German222N,V,Adiscrete{0,1,2,3,4}21.490.647Table1:ComparisonofdatasetsusedforevaluatingsemanticrelatednessinGerman.ZG222doesnothavethisbias.FollowingtheworkbyJarmaszandSzpakow-icz(2003)andTurney(2006),wecreatedasec-onddatasetcontainingmultiplechoicequestions.Wecollected1072multiple-choicewordanalogyquestionsfromtheGermanReader’sDigestWordPowerGame(RDWP)fromJanuary2001toDe-cember2005(WallaceandWallace,2005).Wedis-carded44questionsthathadmorethanonecorrectanswer,and20questionsthatusedaphraseinsteadofasingletermasquery.Theresulting1008ques-tionsformourevaluationdataset.Anexampleques-tionisgivenbelow:Mufﬁn(mufﬁn)a)Kleingeb¨ack(smallcake)b)Spenglerwerkzeug(plumbingtool)c)Miesepeter(killjoy)d)Wildschaf(moufﬂon)Thetaskistoﬁndthecorrectchoice-‘a)’inthiscase.Thisdatasetissigniﬁcantlylargerthananyofthepreviousdatasetsemployedinthistypeofevalua-tion.Also,itisnotrestrictedtosynonymquestions,asintheworkbyJarmaszandSzpakowicz(2003),butalsoincludeshypernymy/hyponymy,andfewnon-classicalrelations.3.3AnalysisofResultsTable2givestheresultsofevaluationonthetaskofcorrelatingtheresultsofanSRmeasurewithhu-manjudgmentsusingPearsoncorrelation.TheGer-maNetbasedLINmeasureoutperformsESAontheGur65dataset.Ontheotherdatasets,ESAisbetterthanLIN.Thisisclearlyduetothefact,thatGur65containsonlynoun-nounwordpairsconnectedbyclassicalsemanticrelations,whiletheotherdatasetsalsocontaincrosspart-of-speechpairsconnectedbynon-classicalrelations.TheWikipediabasedESAmeasurecanbettercapturesuchrelations.Addition-ally,Table3showsthatESAalsocoversalmostallGUR65GUR350ZG222#coveredwordpairs5311655Upperbound0.800.640.44GermaNetLin0.730.500.08WikipediaESA0.560.520.32Table2:PearsoncorrelationrofhumanjudgmentswithSRmeasuresonwordpairscoveredbyGer-maNetandWikipedia.COVEREDPAIRSDATASET#PAIRSLINESAGur65656065Gur350350208333ZG22222288205Table3:NumberofcoveredwordpairsbasedonLinorESAmeasureondifferentdatasets.wordpairsineachdataset,whileGermaNetismuchlowerforGur350andZG222.ESAperformsevenbetterontheReader’sDigesttask(seeTable4).Itshowshighcoverageandnearhumanperformanceregardingtherelativenumberofcorrectlysolvedquestions.6Giventhehighperformanceandcover-ageoftheWikipediabasedESAmeasure,weexpectittoyieldbetterIRresultsthanLIN.4InformationRetrieval4.1IRModelsPreprocessingForcreatingthesearchindexforIRmodels,weapplyﬁrsttokenizationandthenre-movestopwords.WeuseageneralGermanstop6Valuesforhumanperformanceareforonesubject.Thus,theyonlyindicatetheapproximatedifﬁcultyofthetask.Weplantousethisdatasetwithamuchlargergroupofsubjects.#ANSWERED#CORRECTRATIOHuman10088740.87GermaNetLin2981530.51WikipediaESA7895720.72Table4:Evaluationresultsonmultiple-choicewordanalogyquestions.1036

wordlistextendedwithhighlyfrequentdomainspe-ciﬁcterms.Beforeaddingtheremainingwordstotheindex,theyarelemmatized.Weﬁnallysplitcompoundsintotheirconstituents,andaddboth,constituentsandcompounds,totheindex.EBmodelLucene7isanopensourcetextsearchlibrarybasedonanEBmodel.Aftermatchingthepreprocessedqueriesagainsttheindex,thedocu-mentcollectionisdividedintoasetofrelevantandirrelevantdocuments.Thesetofrelevantdocumentsis,then,rankedaccordingtotheformulagiveninthefollowingequation:rEB(d,q)=nqXi=1tf(tq,d)·idf(tq)·lengthNorm(d)wherenqisthenumberoftermsinthequery,tf(tq,d)isthetermfrequencyfactorfortermtqindocumentd,idf(tq)istheinversedocumentfre-quencyoftheterm,andlengthNorm(d)isanor-malizationvalueofdocumentd,giventhenumberoftermswithinthedocument.Weaddedasimplequeryexpansionalgorithmusing(i)synonyms,and(ii)hyponyms,extractedfromGermaNet.IRbasedonSRForthe(IR-1)model,weuti-lizetwoSRmeasuresandasetofheuristics:(i)theLinmeasurebasedonGermaNet(LIN),and(ii)theESAmeasurebasedonWikipedia(ESA-Word).ThisalgorithmwasappliedtotheGermanIRbench-markwithpositiveresultsbyM¨ullerandGurevych(2006).ThealgorithmcomputesaSRscoreforeachqueryanddocumenttermpair.Scoresaboveapre-deﬁnedthresholdaresummedupandweightedbydifferentfactors,whichboostorlowerthescoresfordocuments,dependingonhowmanyquerytermsarecontainedexactlyorcontributeahighenoughSRscore.Inordertointegratethestrengthsoftradi-tionalIRmodels,theinversedocumentfrequencyidfisconsidered,whichmeasuresthegeneralim-portanceofatermforpredictingthecontentofadocument.Theﬁnalformulaofthemodelisasfol-lows:rSR(d,q)=Pndi=1Pnqj=1idf(tq,j)·s(td,i,tq,j)(1+nnsm)·(1+nnr)7http://lucene.apache.orgwherendisthenumberoftokensinthedocument,nqthenumberoftokensinthequery,td,ithei-thdocumenttoken,tq,jthej-thquerytoken,s(td,i,tq,j)theSRscorefortherespectivedocumentandqueryterm,nnsmthenumberofquerytermsnotexactlycontainedinthedocument,nnrthenumberofquerytokens,whichdonotcontributeaSRscoreabovethethreshold.Forthe(IR-2)model,weapplytheESAmethodfordirectlycomparingthequerywithdocuments,asdescribedinSection3.1.4.2DataThecorpusemployedinourexperimentswasbuiltbasedonareal-lifeIRscenariointhedomainofECG,asdescribedinSection1.Thedocumentcol-lectionisextractedfromBERUFEnet,8adatabasecreatedbytheGFLO.Itcontainstextualdescrip-tionsofabout1,800vocationaltrainings,and4,000descriptionsofprofessions.Werestrictthecollec-tiontoasubsetofBERUFEnetdocuments,consist-ingof529descriptionsofvocationaltrainings,duetotheprocessnecessarytoobtainrelevancejudg-ments,asdescribedbelow.Thedocumentscontainnotonlydetailsofprofessions,butalsoalotofinfor-mationconcerningthetrainingandadministrativeissues.Weonlyusethoseportionsofthedescrip-tions,whichcharacterizetheprofessionitself.Wecollectedrealnaturallanguagetopicsbyask-ing30humansubjectstowriteanessayabouttheirprofessionalinterests.Thetopicscontain130words,onaverage.MakingrelevancejudgmentsforECGrequiresdomainexpertise.Therefore,weappliedanautomaticmethod,whichusestheknowledgebaseemployedbytheGFLO,describedinSection1.Toobtainrelevancejudgments,weﬁrstannotateeachessaywithrelevantkeywordsfromthetagsetof41andretrievearankedlistofprofessions,whichwereassignedoneormorekeywordsbydomainexperts.Tomaptherankedlisttoasetofrelevantandir-relevantprofessions,weuseathresholdof3,assuggestedbycareerguidanceexperts.Thissettingyieldsonaverage93relevantdocumentspertopic.Thequalityoftheautomaticallycreatedgoldstan-darddependsonthequalityoftheappliedknowl-edgebase.Astheknowledgebasewascreatedby8http://berufenet.arbeitsamt.de/1037

domainexpertsandisatthecoreoftheelectronicca-reerguidancesystemoftheGFLO,weassumethatthequalityisadequatetoensureareliableevalua-tion.4.3AnalysisofResultsInTable5,wesummarizetheresultsoftheex-perimentsapplyingdifferentIRmodelsontheBERUFEnetdata.Webuildqueriesfromnaturallanguageessaysby(QT-1)extractingnouns,verbs,andadjectives,(QT-2)usingonlynouns,and(QT-3)manuallyassigningsuitablekeywordsfromthetagsetwith41keywordstoeachtopic.Wereporttheresultswithtwodifferentthresholds(.85and.98)fortheLinmodel,andwiththreedifferentthresholds(.11,.13and.24)fortheESA-Wordmodels.Theevaluationmetricsusedaremeanaverageprecision(MAP),precisionaftertendocuments(P10),thenumberofrelevantreturneddocuments(#RRD).WecomputetheabsolutevalueofSpearman’srankcor-relationcoefﬁcient(SRCC)bycomparingtherele-vancerankingofoursystemwiththerelevancerank-ingoftheknowledgebaseemployedbytheGFLO.UsingqueryexpansionfortheEBmodelde-creasestheretrievalperformanceformostconﬁgu-rations.TheSRbasedmodelsoutperformtheEBmodelinallconﬁgurationsandevaluationmetrics,exceptforP10onthekeywordbasedqueries.TheLinmodelisalwaysoutperformedbyatleastoneoftheESAmodels,exceptfor(QT-3).(IR-2)performsbestonlongerqueriesusingnouns,verbs,adjectivesorjustnouns.Comparingthenumberofrelevantretrieveddoc-uments,weobservethattheIRmodelsbasedonSRareabletoreturnmorerelevantdocumentsthantheEBmodel.Thissupportstheclaimthatsemanticknowledgeisespeciallyhelpfulforthevocabularymismatchproblem,whichcannotbeaddressedbyconventionalIRmodels.E.g.,onlySR-basedmod-elscanﬁndthejobinformationtechnicianforapro-ﬁlewhichcontainsthesentenceMyinterestsandskillsareintheﬁeldoflanguagesandIT.Thejobcouldonlybejudgedasrelevant,asthesemanticrelationbetweenITintheproﬁleandinformationtechnologyintheprofessionaldescriptioncouldbefound.InouranalysisoftheBERUFEnetresultsob-tainedon(QT-1),wenoticedthatmanyerrorswereduetothetopicsexpressedinfreenaturallanguageessays.SomesubjectsdeviatedfromthegiventasktodescribetheirprofessionalinterestsanddescribedfactsthatareratherirrelevanttothetaskofECG,e.g.ItisimportanttospeakdifferentlanguagesinthegrowingEuropeanUnion.Ifallcontentwordsareextractedtobuildaquery,alotofnoiseisintro-duced.Therefore,weconductedfurtherexperimentswith(QT-2)and(QT-3):buildingthequeryusingonlynouns,andusingmanuallyassignedkeywordsbasedonthetagsetof41keywords.Forexample,thefollowingqueryisbuiltfortheprofessionalpro-ﬁlegiveninSection1.Keywordsassigned:carefor/nurse/educate/teach;use/programcomputer;office;outside:outsidefacilities/naturalenvironment;animals/plantsIRresultsobtainedon(QT-2)and(QT-3)showthattheperformanceisbetterfornouns,andsig-niﬁcantlybetterforthequeriesbuiltofkeywords.ThissuggeststhatinordertoachievehighIRperfor-manceforthetaskofElectronicCareerGuidance,itisnecessarytopreprocessthetopicsbyperform-inginformationextractiontoremovethenoisefromfreetextessays.Asaresultofthepreprocessing,naturallanguageessaysshouldbemappedtoasetofkeywordsrelevantfordescribingaperson’sin-terests.Ourresultssuggestthattheword-basedse-manticrelatednessIRmodel(IR-1)performssignif-icantlybetterinthissetting.5ConclusionsWepresentedasystemforElectronicCareerGuid-anceutilizingNLPandIRtechniques.Givenanat-urallanguageprofessionalproﬁle,relevantprofes-sionsarecomputedbasedontheinformationaboutsemanticrelatedness.WeintrinsicallyevaluatedandanalyzedthepropertiesoftwosemanticrelatednessmeasuresutilizingthelexicalsemanticinformationinaGermanwordnetandWikipediaonthetasksofestimatingsemanticrelatednessscoresandanswer-ingmultiple-choicequestions.Furthermore,weap-pliedthesemeasurestoanIRtask,wherebytheywereusedeitherincombinationwithasetofheuris-ticsortheWikipediabasedmeasurewasusedtodi-rectlycomputesemanticrelatednessoftopicsand1038

MODEL(QT-1)NOUNS,VERBS,ADJ.(QT-2)NOUNS(QT-3)KEYWORDSMAPP10#RRDSRCCMAPP10#RRDSRCCMAPP10#RRDSRCCEB.39.582581.306.38.582297.335.54.762755.497EB+SYN.37.562589.288.38.572310.331.54.732768.530EB+HYPO.34.472702.275.38.562328.327.47.652782.399Lin.85.41.562787.338.40.592770.320.59.732787.578Lin.98.41.612753.326.42.592677.341.58.742783.563ESA-Word.11.39.562787.309.44.632787.355.60.772787.535ESA-Word.13.38.592787.282.43.622787.338.62.762787.550ESA-Word.24.40.602787.259.43.602699.306.54.732772.482ESA-Text.47.622787.368.55.712787.462.56.742787.489Table5:InformationRetrievalperformanceontheBERUFEnetdataset.documents.Weexperimentedwiththreedifferentquerytypes,whichwerebuiltfromthetopicsby:(QT-1)extractingnouns,verbs,adjectives,(QT-2)extractingonlynouns,or(QT-3)manuallyassign-ingseveralkeywordstoeachtopicfromatagsetof41keywords.InanintrinsicevaluationofLINandESAmea-suresonthetaskofcomputingsemanticrelatedness,wefoundthatESAcapturestheinformationaboutsemanticrelatednessandnon-classicalsemanticre-lationsconsiderablybetterthanLIN,whichoperatesonanis-ahierarchyand,thus,bettercapturesthein-formationaboutsemanticsimilarity.OnthetaskofsolvingRDWPquestions,theESAmeasuresignif-icantlyoutperformedtheLINmeasureintermsofcorrectness.Onbothtasks,thecoverageofESAismuchbetter.Despitethis,theperformanceofLINandESAaspartofanIRmodelisonlyslightlydifferent.ESAperformsbetterforalllengthsofqueries,butthedifferencesarenotassigniﬁcantasintheintrinsicevaluation.Thisindicatesthattheinformationprovidedbybothmeasures,basedondifferentknowledgebases,mightbecomplementaryfortheIRtask.WhenESAisappliedtodirectlycomputeseman-ticrelatednessbetweentopicsanddocuments,itout-performsIR-1andthebaselineEBmodelbyalargemarginforQT-1andQT-2queries.ForQT-3,i.e.,theshortesttypeofquery,itperformsworsethanIR-1utilizingESAandasetofheuristics.Also,theperformanceofthebaselineEBmodelisverystronginthisexperimentalsetting.Thisresultin-dicatesthatIR-2utilizingconventionalinformationretrievaltechniquesandsemanticinformationfromWikipediaisbettersuitedforlongerqueriesprovid-ingenoughcontext.Forshorterqueries,softmatch-ingtechniquesutilizingsemanticrelatednesstendtobebeneﬁcial.Itshouldbeborninmind,thattheconstructionofQT-3queriesinvolvedamanualstepofassigningthekeywordstoagivenessay.Inthisexperimen-talsetting,allmodelsshowthebestperformance.Thisindicatesthatprofessionalproﬁlescontainalotofnoise,sothatmoresophisticatedNLPanalysisoftopicsisrequired.Thiswillbeimprovedinourfuturework,wherebythesystemwillincorporateaninformationextractioncomponentforautomat-icallymappingtheprofessionalproﬁletoasetofkeywords.Wewillalsointegrateacomponentforanalyzingthesentimentstructureoftheproﬁles.Webelievethattheﬁndingsfromourworkonapply-ingIRtechniquestothetaskofElectronicCareerGuidancegeneralizetosimilarapplicationdomains,wheretopicsanddocumentsdisplaysimilarproper-ties(withrespecttotheirlength,free-textstructureandmismatchofvocabularies)anddomainandlex-icalknowledgeisrequiredtoachievehighlevelsofperformance.AcknowledgmentsThisworkwassupportedbytheGermanResearchFoundationundergrant”SemanticInformationRe-trievalfromTextsintheExampleDomainElec-tronicCareerGuidance”,GU798/1-2.Wearegrate-fultotheBundesagenturf¨urArbeitforprovidingtheBERUFEnetcorpus.Wewouldliketothanktheanonymousreviewersforvaluablefeedbackonthispaper.WewouldalsoliketothankPikluGuptaforhelpfulcomments.1039

ReferencesDavidAhn,ValentinJijkoun,GiladMishne,KarinM¨uller,MaartendeRijke,andStefanSchlobach.2004.UsingWikipediaattheTRECQATrack.InProceedingsofTREC2004.AlexanderBudanitskyandGraemeHirst.2006.Evalu-atingWordNet-basedMeasuresofSemanticDistance.ComputationalLinguistics,32(1).RazvanBunescuandMariusPasca.2006.UsingEn-cyclopedicKnowledgeforNamedEntityDisambigua-tion.InProceedingsofACL,pages9–16,Trento,Italy.ChristianeFellbaum.1998.WordNetAnElectronicLex-icalDatabase.MITPress,Cambridge,MA.EvgeniyGabrilovichandShaulMarkovitch.2007.Com-putingSemanticRelatednessusingWikipedia-basedExplicitSemanticAnalysis.InProceedingsofThe20thInternationalJointConferenceonArtiﬁcialIn-telligence(IJCAI),Hyderabad,India,January.JulioGonzalo,FelisaVerdejo,IrinaChugur,andJuanCigarran.1998.IndexingwithWordNetsynsetscanimprovetextretrieval.InProceedingsoftheColing-ACL’98WorkshopUsageofWordNetinNaturalLan-guageProcessingSystems,Montreal,Canada,August.IrynaGurevychandHendrikNiederlich.2005.Comput-ingsemanticrelatednessingermanwithrevisedinfor-mationcontentmetrics.InProceedingsof”OntoLex2005-OntologiesandLexicalResources”IJCNLP’05Workshop,pages28–33,October11–13.IrynaGurevych.2005.UsingtheStructureofaConcep-tualNetworkinComputingSemanticRelatedness.InProceedingsofthe2ndInternationalJointConferenceonNaturalLanguageProcessing,pages767–778,JejuIsland,RepublicofKorea.MarioJarmaszandStanSzpakowicz.2003.Roget’sthe-saurusandsemanticsimilarity.InRANLP,pages111–120.BorisKatz,GregoryMarton,GaryBorchardt,AlexisBrownell,SueFelshin,DanielLoreto,JesseLouis-Rosenberg,BenLu,FedericoMora,StephanStiller,OzlemUzuner,andAngelaWilcox.2005.Externalknowledgesourcesforquestionanswering.InPro-ceedingsofthe14thAnnualTextREtrievalConference(TREC’2005),November.ClaudiaKunze,2004.Lexikalisch-semantischeWort-netze,chapterComputerlinguistikundSprachtech-nologie,pages423–431.SpektrumAkademischerVerlag.DekangLin.1998.Aninformation-theoreticdeﬁni-tionofsimilarity.InProceedingsofthe15thInterna-tionalConferenceonMachineLearning,pages296–304.MorganKaufmann,SanFrancisco,CA.DanMoldovanandRadaMihalcea.2000.UsingWord-NetandlexicaloperatorstoimproveInternetsearches.IEEEInternetComputing,4(1):34–43.JaneMorrisandGraemeHirst.2004.Non-ClassicalLexicalSemanticRelations.InWorkshoponCom-putationalLexicalSemantics,HumanLanguageTech-nologyConferenceoftheNorthAmericanChapteroftheACL,Boston.ChristofM¨ullerandIrynaGurevych.2006.ExploringthePotentialofSemanticRelatednessinInformationRetrieval.InProceedingsofLWA2006Lernen-Wis-sensentdeckung-Adaptivit¨at:InformationRetrieval,pages126–131,Hildesheim,Germany.GI-FachgruppeInformationRetrieval.HerbertRubensteinandJohnB.Goodenough.1965.ContextualCorrelatesofSynonymy.CommunicationsoftheACM,8(10):627–633.GerardSaltonandMichaelJ.McGill.1983.IntroductiontoModernInformationRetrieval.McGraw-Hill,NewYork.GerardSalton,EdwardFox,andHarryWu.1983.Ex-tendedbooleaninformationretrieval.CommunicationoftheACM,26(11):1022–1036.AlanF.Smeaton,FergusKelledy,andRuariO’Donell.1994.TREC-4ExperimentsatDublinCityUniver-sity:Thresholdingpostinglists,queryexpansionwithWordNetandPOStaggingofSpanish.InProceedingsofTREC-4,pages373–390.PeterD.Turney.2006.Similarityofsemanticrelations.ComputationalLinguistics,32(3):379–416.EllenVorhees.1994.Queryexpansionusinglexical-semanticrelations.InProceedingsofthe17thAn-nualACMSIGIRConferenceonResearchandDevel-opmentinInformationRetrieval,pages61–69.DeWittWallaceandLilaAchesonWallace.2005.Reader’sDigest,dasBestef¨urDeutschland.Jan2001–Dec2005.VerlagDasBeste,Stuttgart.TorstenZeschandIrynaGurevych.2006.AutomaticallyCreatingDatasetsforMeasuresofSemanticRelated-ness.InProceedingsoftheWorkshoponLinguisticDistances,pages16–24,Sydney,Australia,July.As-sociationforComputationalLinguistics.TorstenZesch,IrynaGurevych,andMaxM¨uhlh¨auser.2007a.AnalyzingandAccessingWikipediaasaLexi-calSemanticResource.InBiannualConferenceoftheSocietyforComputationalLinguisticsandLanguageTechnology,pages213–221,Tuebingen,Germany.TorstenZesch,IrynaGurevych,andMaxM¨uhlh¨auser.2007b.ComparingWikipediaandGermanWord-netbyEvaluatingSemanticRelatednessonMultipleDatasets.InProceedingsofNAACL-HLT.Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 1040–1047,

Prague, Czech Republic, June 2007. c(cid:13)2007 Association for Computational Linguistics

1040

ExtractingSocialNetworksandBiographicalFactsFromConversationalSpeechTranscriptsHongyanJingIBMT.J.WatsonResearchCenter1101KitchawanRoadYorktownHeights,NY10598hjing@us.ibm.comNandaKambhatlaIBMIndiaResearchLabEGL,DomlurRingRoadBangalore-560071,Indiakambhatla@in.ibm.comSalimRoukosIBMT.J.WatsonResearchCenter1101KitchawanRoadYorktownHeights,NY10598roukos@us.ibm.comAbstractWepresentageneralframeworkforautomaticallyextractingsocialnetworksandbiographicalfactsfromconversationalspeech.Ourapproachreliesonfusingtheoutputproducedbymultipleinforma-tionextractionmodules,includingentityrecognitionanddetection,relationdetec-tion,andeventdetectionmodules.Wedescribethespeciﬁcfeaturesandalgo-rithmicreﬁnementseffectiveforconver-sationalspeech.Thesecumulativelyin-creasetheperformanceofsocialnetworkextractionfrom0.06to0.30forthedevel-opmentset,andfrom0.06to0.28forthetestset,asmeasuredbyf-measureonthetieswithinanetwork.Thesameframe-workcanbeappliedtoothergenresoftext—wehavebuiltanautomaticbiographygenerationsystemforgeneraldomaintextusingthesameapproach.1IntroductionAsocialnetworkrepresentssocialrelationshipsbetweenindividualsororganizations.Itconsistsofnodesandties.Nodesareindividualactorswithinthenetworks,generallyapersonoranor-ganization.Tiesaretherelationshipsbetweenthenodes.Socialnetworkanalysishasbecomeakeytechniqueinmanydisciplines,includingmodernsociologyandinformationscience.Inthispaper,wepresentoursystemforau-tomaticallyextractingsocialnetworksandbio-graphicalfactsfromconversationalspeechtran-scriptsbyintegratingtheoutputofdifferentIEmodules.TheIEmodulesarethebuildingblocks;thefusingmoduledepictsthewaysofassemblingthesebuildingblocks.TheﬁnaloutputdependsonwhichfundamentalIEmodulesareusedandhowtheirresultsareintegrated.Thecontributionsofthisworkaretwofold.Weproposeageneralframeworkforextractingsocialnetworksandbiographiesfromtextthatappliestoconversationalspeechaswellasothergenres,in-cludinggeneralnewswirestories.Secondly,wepresentspeciﬁcmethodsthatprovedeffectiveforusforimprovingtheperformanceofIEsystemsonconversationalspeechtranscripts.Theseimprove-mentsincludefeatureengineeringandalgorithmicrevisionsthatledtoanearlyﬁve-foldperformanceincreaseforbothdevelopmentandtestsets.Inthenextsection,wepresentourframeworkforextractingsocialnetworksandotherbiograph-icalfactsfromtext.InSection3,wediscussthereﬁnementswemadetoourIEmodulesinordertoreliablyextractinformationfromconversationalspeechtranscripts.InSection4,wedescribetheexperiments,evaluationmetrics,andtheresultsofsocialnetworkandbiographyextraction.InSec-tion5,weshowtheresultsofapplyingtheframe-worktoothergenresoftext.Finally,wediscussrelatedworkandconcludewithlessonslearnedandfuturework.2TheGeneralFrameworkForextractionofsocialnetworksandbiographi-calfacts,ourapproachreliesonthreestandardIEmodules—entitydetectionandrecognition,rela-tiondetection,andeventdetection—andafusionmodulethatintegratestheoutputfromthethreeIEsystems.2.1Entity,Relation,andEventDetectionWeusethetermentitytorefertoaperson,anor-ganization,orotherrealworldentities,asadopted1041

intheAutomaticContentExtraction(ACE)Work-shops(ACE,2005).Amentionisareferencetoarealworldentity.Itcanbenamed(e.g.“JohnLennon”),nominal(e.g.“mother”),orpronomi-nal(e.g.“she”).Entitydetectionisgenerallyaccomplishedintwosteps:ﬁrst,amentiondetectionmoduleiden-tiﬁesallthementionsofinterest;second,aco-referencemodulemergesmentionsthatrefertothesameentityintoasingleco-referencechain.Arelationdetectionsystemidentiﬁes(typi-cally)binaryrelationshipsbetweenpairsofmen-tions.Forinstance,forthesentence“I’minNewYork”,thefollowingrelationexists:locatedAt(I,NewYork).Aneventdetectionsystemidentiﬁeseventsofinterestandtheargumentsoftheevent.Forex-ample,fromthesentence“JohnmarriedEvain1940”,thesystemshouldidentifythemarriageevent,thepeoplewhogotmarriedandthetimeoftheevent.ThelatestACEevaluationsinvolvealloftheabovetasks.However,asshowninthenextsec-tion,ourfocusisquitedifferentfromACE—weareparticularlyinterestedinimprovingperfor-manceforconversationalspeechandbuildingontopofACEtaskstoproducesocialnetworksandbiographies.2.2FusionModuleThefusionmodulemergestheoutputfromIEmodulestoextractsocialnetworksandbiographi-calfacts.Forexample,ifarelationdetectionsys-temhasidentiﬁedtherelationmotherOf(mother,my)fromtheinputsentence“mymotherisacook”,andifanentityrecognitionmodulehasgeneratedentitiesreferencedbythementions{my,Josh,me,I,I,......}and{mother,she,her,her,Rosa......},thenbyreplacingmyandmotherwiththenamedmentionswithinthesameco-referencechains,thefusionmoduleproducesthefollow-ingnodesandtiesinasocialnetwork:motherOf(Rosa,Josh).Wegeneratethenodesofsocialnetworksbyse-lectingallthePERSONentitiesproducedbytheentityrecognitionsystem.Typically,weonlyin-cludeentitiesthatcontainatleastonenamedmen-tion.Toidentifytiesbetweennodes,weretrieveallrelationsthatindicatesocialrelationshipsbe-tweenapairofnodesinthenetwork.Weextractbiographicalproﬁlesbyselectingtheevents(extractedbytheeventextractionmodule)andcorrespondingrelations(extractedbytherela-tionextractionmodule)thatinvolveagivenindi-vidualasanargument.Whenmultipledocumentsareused,thenweemployacross-documentco-referencesystem.3ImprovingPerformanceforConversationalSpeechTranscriptsExtractinginformationfromconversationalspeechtranscriptsisuniquelychallenging.Inthissection,wedescribethedatacollectionusedinourexperiments,andexplainspeciﬁctechniquesweusedtoimproveIEperformanceonthisdata.3.1ConversationalSpeechCollectionWeuseacorpusofvideotaped,digitizedoralin-terviewswithHolocaustsurvivorsinourexperi-ments.ThisdatawascollectedbytheUSCShoahFoundationInstitute(formerlyknownastheVi-sualHistoryFoundation),andhasbeenusedinmanyresearchactivitiesundertheMultilingualAccesstoLargeSpokenArchives(MALACH)project(Gustmanetal.,2002;Oardetal.,2004).Thecollectioncontainsoralinterviewsin32lan-guagesfrom52,000survivors,liberators,rescuersandwitnessesoftheHolocaust.Thisdataisverychallenging.Besidestheusualcharacteristicsofconversationalspeech,suchasspeakerturnsandspeechrepairs,theinterviewtranscriptscontainalargepercentageofungram-matical,incoherent,orevenincomprehensibleclauses(asampleinterviewsegmentisshowninFigure1).Inaddition,eachinterviewcoversmanypeopleandplacesoveralongtimeperiod,whichmakesitevenmoredifﬁculttoextractsocialnet-worksandbiographicalfacts.speaker2inonthatninthofNovem-bernineteenhundredthirtyeightIwaswithmyparentsathomeweheardnotthroughtheweheardeventhroughthewindowsthecrashingofglassthecrashingofandandtheyareourcan’tFigure1:Sampleinterviewsegment.3.2TheImportanceofCo-referenceResolutionOurinitialattemptsatsocialnetworkextractionfortheabovedatasetresultedinaverypoorscore1042

of0.06f-measureforﬁndingtherelationswithinanetwork(asshowninTable3asbaselineperfor-mance).Anerroranalysisindicatedpoorco-referenceresolutiontobethechiefculpritforthelowper-formance.Forinstance,supposewehavetwoclauses:“hismother’snameisMary”and“hisbrotherMarkwenttothearmy”.Furthersup-posethat“his”intheﬁrstclausereferstoapersonnamed“John”and“his”inthesecondclausereferstoapersonnamed“Tim”.Iftheco-referencesystemworksperfectly,thesystemshouldﬁndasocialnetworkinvolvingfourpeo-ple:{John,Tim,Mary,Mark},andtheties:moth-erOf(Mary,John),andbrotherOf(Mark,Tim).However,iftheco-referencesystemmistakenlylinks“John”to“his”inthesecondclauseandlinks“Tim”to“his”intheﬁrstclause,thenwewillstillhaveanetworkwithfourpeople,butthetieswillbe:motherOf(Mary,Tim),andbrotherOf(Mark,John),whicharecompletelywrong.Thisexampleshowsthatco-referenceerrorsinvolvingmentionsthatarerelationargumentscanleadtoverybadperformanceinsocialnetworkextraction.Ourexistingco-referencemoduleisastate-of-the-artsystemthatproducesverycompetitivere-sultscomparedtootherexistingsystems(Luoetal.,2004).Ittraversesthedocumentfromlefttorightandusesamention-synchronousapproachtodecidewhetheramentionshouldbemergedwithanexistingentityorstartanewentity.However,ourexistingsystemhasshortcomingsforthisdata:thesystemlacksfeaturesforhan-dlingconversationalspeech,andthesystemof-tenmakesmistakesinpronounresolution.Re-solvingpronominalreferencesisveryimportantforextractingsocialnetworksfromconversationalspeech,asillustratedinthepreviousexample.3.3ImprovingCo-referenceforConversationalSpeechWedevelopedanewco-referenceresolutionsys-temforconversationalspeechtranscripts.Simi-lartomanypreviousworksonco-reference(Ng,2005),wecasttheproblemasaclassiﬁcationtaskandsolveitintwosteps:(1)trainaclassiﬁertodeterminewhethertwomentionsareco-referentornot,and(2)useaclusteringalgorithmtopartitionthementionsintoclusters,basedonthepairwisepredictions.Weaddedmanyfeaturestoourmodelspeciﬁ-callydesignedforconversationalspeech,andsig-niﬁcantlyimprovedtheagglomerativeclusteringusedforco-reference,includingintegratingrela-tionsasconstraints,anddesigningbetterclusterlinkagemethodsandclusteringstoppingcriteria.3.3.1AddingFeaturesforConversationalSpeechWeaddedmanyfeaturestoourmodelspeciﬁ-callydesignedforconversationalspeech:Speakerroleidentiﬁcation.Inmanualtran-scripts,thespeakerturnsaregivenandeachspeakerislabeleddifferently(e.g.“speaker1”,“speaker2”),buttheidentityofthespeakerisnotgiven.Aninterviewtypicallyinvolves2ormorespeakersanditisusefultoidentifytherolesofeachspeaker(e.g.interviewer,interviewee,etc.).Forinstance,”you”spokenbytheinterviewerislikelytobelinkedwith”I”spokenbytheinter-viewee,but”you”spokenbythethirdpersonintheinterviewismorelikelytobereferringtotheinterviewerthantotheinterviewee.Wedevelopedaprogramtoidentifythespeakerroles.Theprogramclassiﬁesthespeakersintothreecategories:interviewer,interviewee,andothers.Thealgorithmreliesonthreeindicators—numberofturnsbyeachspeaker,differenceinnumberofwordsspokenbyeachspeaker,andtheratioofﬁrst-personpronounssuchas“I”,“me”,and“we”vs.second-personpronounssuchas“you”and“your”.Thisspeakerroleidentiﬁca-tionprogramworksverywellwhenwecheckedtheresultsonthedevelopmentandtestset—theinterviewersandsurvivorsinallthedocumentsinthedevelopmentsetwerecorrectlyidentiﬁed.Speakerturns.Usingtheresultsfromthespeakerroleidentiﬁcationprogram,weenrichcer-tainfeatureswithspeakerturninformation.Forexample,withoutthisinformation,thesystemcan-notdistinguish“I”spokenbyaninterviewerfrom“I”spokenbyaninterviewee.Spellingfeaturesforspeechtranscripts.Weaddadditionalspellingfeaturessothatmentionssuchas“CylaCYLALewin”and“CylaLewin”areconsideredasexactmatches.Nameswithspelled-outlettersoccurfrequentlyinourdatacol-lection.NamePatterns.Weaddsomefeaturesthatcapturefrequentsyntacticstructuresthatspeakersusetoexpressnames,suchas“hernameisIrene”,“mycousinMark”,and“interviewerEllen”.Pronounfeatures.Toimprovetheperfor-1043

manceonpronouns,weaddfeaturessuchasthespeakerturnsofthepronouns,whetherthetwopronounsagreeinpersonandnumber,whetherthereexistothermentionsbetweenthem,etc.Othermiscellaneousfeatures.Wealsoin-cludeotherfeaturessuchasgender,tokendis-tance,sentencedistance,andmentiondistance.Wetrainedamaximum-entropyclassiﬁerusingthesefeatures.Foreachpairofmentions,theclas-siﬁeroutputstheprobabilitythatthetwomentionsareco-referent.Wealsomodiﬁedexistingfeaturestomakethemmoreapplicabletoconversationalspeech.Forinstance,weaddedpronoun-distancefeaturestakingintoaccountthepresenceofotherpronom-inalreferencesinbetween(ifso,thetypesofthepronouns),othermentionsinbetween,etc.3.3.2ImprovingAgglomerativeClusteringWeuseanagglomerativeclusteringapproachforpartitioningmentionsintoentities.Thisisabottom-upapproachwhichjoinstheclosestpairofclusters(i.e.,entities)ﬁrst.Initially,eachmen-tionisplacedintoitsowncluster.IfwehaveNmentionstocluster,westartwithNclusters.Theintuitionbehindchoosingtheagglomera-tivemethodistomergethemostconﬁdentpairsﬁrst,andusethepropertiesofexistingclusterstoconstrainfutureclustering.Thisseemstobeespe-ciallyimportantforourdatacollection,sincecon-versationalspeechtendstohavealotofrepetitionsorlocalstructuresthatindicateco-reference.Insuchcases,itisbeneﬁcialtomergethesecloselyrelatedmentionsﬁrst.Clusterlinkagemethod.Inagglomerativeclustering,eachcyclemergestwoclustersintoasinglecluster,thusreducingthenumberofclus-tersbyone.Weneedtodecideuponamethodofmeasuringthedistancebetweentwoclusters.Ateachcycle,thetwomentionswiththehigh-estco-referentprobabilityarelinkedﬁrst.Thisre-sultsinthemergingofthetwoclustersthatcontainthesetwomentions.Weimproveuponthismethodbyimposingmin-imaldistancecriteriabetweenclusters.Twoclus-tersC1andC2canbecombinedonlyifthedis-tancebetweenallthementionsfromC1andallthementionsfromC2isabovetheminimaldis-tancethreshold.Forinstance,supposeC1={he,father},andC2={he,brother},and“he”fromC1and“he”fromC2hasthehighestlinkageprobability.Thestandardsinglelinkagemethodwillcombinethesetwoclusters,despitethefactthat“father”and“brother”areveryunlikelytobelinked.Imposingminimaldistancecriteriacansolvethisproblemandpreventthelinkageofclusterswhichcontainverydissimilarmentions.Inpractice,weusedmultipleminimaldistancethresholds,suchasminimaldistancebetweentwonamedmentionsandminimaldistancebetweentwonominalmentions.Wechosenottousecompleteoraveragelink-agemethods.Inourdatacollection,thenarrationscontainalotofpronounsandthefocustendstobeverylocal.Whereasthesimilaritymodelmaybereasonablygoodatpredictingthedistancebe-tweentwopronounsthatareclosetoeachother,itisnotgoodatpredictingthedistancebetweenpro-nounsthatarefurthurapart.Therefore,itseemsmorereasonabletousesinglelinkagemethodwithmodiﬁcationsthancompleteoraveragelinkagemethods.Usingrelationstoconstrainclustering.An-othernoveltyofourco-referencesystemistheuseofrelationsforconstrainingco-reference.Theideaisthattwoclustersshouldnotbemergedifsuchmergingwillintroducecontradictoryrela-tions.Forinstance,ifweknowthatpersonentityAisthemotherofpersonentityB,andpersonen-tityCisthesisterofB,thenAandCshouldnotbelinkedsincetheresultingentitywillbeboththemotherandthesisterofB.Weconstructco-existentrelationsetsfromthetrainingdata.Foranytwopairsofentities,wecol-lectallthetypesofrelationsthatexistbetweenthem.Thesetypesofrelationsarelabeledasco-existent.Forinstance,“motherOf”and“par-entOf”canco-exist,but“motherOf”and“sis-terOf”cannot.Byusingtheserelationconstraints,thesystemrefrainsfromgeneratingcontradictoryrelationsinsocialnetworks.Speedimprovement.SupposethenumberofmentionsisN,thetimecomplexityofsimplelink-agemethodisO(N2).Withtheminimaldis-tancecriteria,thecomplexityisO(N3).However,Ncanbedramaticallyreducedforconversationaltranscriptsbyﬁrstlinkingalltheﬁrst-personpro-nounsbyeachspeaker.4ExperimentsInthissection,wedescribetheexperimentalsetupandpresentsampleoutputsandevaluationresults.1044

TrainDevTestWords198k73k255kMentions43k16k56kRelations7K3k8kTable2:ExperimentalDataSets.4.1DataAnnotationThedatausedinourexperimentsconsistofpartialorcompleteEnglishinterviewsofHolocaustsur-vivors.Theinputtooursystemistranscriptsofinterviews.Wemanuallyannotatedmanualtranscriptswithentities,relations,andeventcategories,speciﬁ-callydesignedforthistaskandtheresultsofcare-fuldataanalysis.Theannotationwasperformedbyasingleannotatoroverafewmonths.Thean-notationcategoriesforentities,events,andrela-tionsareshowninTable1.PleasenotethattheeventandrelationdeﬁnitionsareslightlydifferentthanthedeﬁnitionsinACE.4.2TrainingandTestSetsWedividedthedataintotraining,development,andtestdatasets.Table2showsthesizeofeachdataset.Thetrainingsetincludestranscriptsofpartialinterviews.Thedevelopmentsetconsistsof5completeinterviews,andthetestsetcon-sistsof15completeinterviews.Thereasonthatthetrainingsetcontainsonlypartialinterviewsisduetothehighcostoftranscriptionandannota-tion.Sincethosepartialinterviewshadalreadybeentranscribedforspeechrecognitionpurpose,wedecidedtoreusetheminourannotation.Inad-dition,wetranscribedandannotated20completeinterviews(eachinterviewisabout2hours)forbuildingthedevelopmentandtestsets,inordertogiveamoreaccurateassessmentofextractionperformance.4.3ImplementationWedevelopedtheinitialentitydetection,rela-tiondetection,andeventdetectionsystemsusingthesametechniquesasoursubmissionsystemstoACE(Florianetal.,2004).Oursubmissionsys-temsusestatisticalapproaches,andhaverankedinthetoptierinACEevaluations.Weeasilybuiltthemodelsforourapplicationbyretrainingexist-ingsystemswithourtrainingset.Theentitydetectiontaskisaccomplishedintwosteps:mentiondetectionandco-referenceresolu-tion.Thementiondetectionisformulatedasala-Figure2:Socialnetworkextractedbythesystem.belingproblem,andamaximum-entropyclassiﬁeristrainedtoidentifyallthementions.Similarly,relationdetectionisalsocastasaclassiﬁcationproblem—foreachpairofmen-tions,thesystemdecideswhichtypeofrelationexistsbetweenthem.Itusesamaximum-entropyclassiﬁerandvariouslexical,contextual,andsyn-tacticfeaturesforsuchpredications.Eventdetectionisaccomplishedintwosteps:ﬁrst,identifyingtheeventanchorwordsusinganapproachsimilartomentiondetection;then,iden-tifyingeventargumentsusinganapproachsimilartorelationdetection.Theco-referenceresolutionsystemforconver-sationalspeechandthefusionmoduleweredevel-opedanew.4.4TheOutputThesystemaimstoextractthefollowingtypesofinformation:•Thesocialnetworkofthesurvivor.•Importantbiographicalfactsabouteachper-soninthesocialnetwork.•Trackthemovementsofthesurvivorandotherindividualsinthesocialnetwork.Figure2showsasamplesocialnetworkex-tractedbythesystem(onlypartialofthenetworkisshown).Figure3showssamplebiographicalfactsandmovementsummariesextractedbythesystem.Ingeneral,wefocusmoreonhigherpre-cisionthanrecall.4.5EvaluationInthispaper,wefocusonlyontheevaluationofsocialnetworkextraction.Weﬁrstdescribethemetricsforsocialnetworkevaluationandthenpresenttheresultsofthesystem.1045

Entity(12)Event(8)Relation(34)SocialRels(12)EventArgs(8)BioFacts(14)AGECUSTODYaidgiverOfaffectedBybornAtCOUNTRYDEATHauntOfagentOfbornOnDATEHIDINGcousinOfparticipantIncitizenOfDATEREFLIBERATIONfatherOftimeOfdiedAtDURATIONMARRIAGEfriendOftravelArrangerdiedOnGHETTOORCAMPMIGRATIONgrandparentOftravelFromemployeeOfOCCUPATIONSURVIVALmotherOftravelPersonhasPropertyORGANIZATIONVIOLENCEotherRelativeOftravelTolocatedAtOTHERLOCparentOfmanagerOfPEOPLEsiblingOfmemberOfPERSONspouseOfnearSALUTATIONuncleOfpartOfpartOfManyresideInTable1:AnnotationCategoriesforEntities,Events,andRelations.SidoniaLax:dateofbirth:JunetheeighthnineteentwentysevenMovements:MovedTo:AuschwitzMovedTo:UnitedStates......Figure3:Biographicalfactsandmovementsum-mariesextractedbythesystem.Tocomparetwosocialnetworks,weﬁrstneedtomatchthenodesandtiesbetweenthenetworks.Twonodes(i.e.,entities)arematchediftheyhavethesamecanonicalname.Twoties(i.e.,edgesorrelations)arematchedifthesethreecriteriaaremet:theycontainthesametypeofrelations,theargumentsoftherelationarethesame,andtheor-deroftheargumentsarethesameiftherelationisunsymmetrical.Wedeﬁnethethefollowingmeasurementsforsocialnetworkevaluation:theprecisionfornodes(orties)istheratioofcommonnodes(orties)inthetwonetworkstothetotalnumberofnodes(orties)inthesystemoutput,therecallfornodes(orties)istheratioofcommonnodes(orties)inthetwonetworkstothetotalnumberofnodes/tiesinthereferenceoutput,andthef-measurefornodes(orties)istheharmonicmeanofprecisionandre-callfornodes(orties).Thef-measurefortiesin-dicatestheoverallperformanceofsocialnetworkextraction.F-meaDevTestBaselineNewBaselineNewNodes0.590.640.620.66Ties0.060.300.060.28Table3:Performanceofsocialnetworkextraction.Table3showstheresultsofsocialnetworkex-traction.Thenewco-referenceapproachimprovestheperformanceforf-measureontiesbyﬁve-foldondevelopmentsetandbynearlyﬁve-foldfortestset.Wealsotestedthesystemusingautomatictran-scriptsbyourspeechrecognitionsystem.Notsur-prisingly,theresultismuchworse:thenodesf-measureis0.11forthetestset,andthesystemdidnotﬁndanyrelations.Afewfactorsareac-countableforthislowperformance:(1)Speechrecognitionisverychallengingforthisdataset,sincethetestimoniescontainedelderly,emotional,accentedspeech.Giventhatthespeechrecogni-tionsystemfailstorecognizemostofthepersonnames,extractionofsocialnetworksisdifﬁcult.(2)Theextractionsystemsperformworseonau-tomatictranscripts,duetothequalityoftheauto-matictranscript,andthediscrepancybetweenthetrainingandtestdata.(3)Ourmeasurementsareverystrict,andnopartialcreditisgiventopartiallycorrectentitiesorrelations.Wedecidednottopresenttheevaluationresultsoftheindividualcomponentssincetheperfor-manceofindividualcomponentsarenotatallin-dicativeoftheoverallperformance.Forinstance,asinglepronounco-referenceerrormightslighlty1046

changetheco-referencescore,butcanintroduceaseriouserrorinthesocialnetwork,asshownintheexampleinSection3.2.5BiographyGenerationfromGeneralDomainTextWehaveappliedthesameframeworktobiogra-phygenerationfromgeneralnewsarticles.ThisgeneralsystemalsocontainsthreefundamentalIEsystemsandafusionmodule,similartotheworkpresentedinthepaper.ThedifferenceisthattheIEsystemsaretrainedongeneralnewstextusingdif-ferentcategoriesofentities,relations,andevents.AsamplebiographyoutputextractedfromTDT5EnglishdocumentsisshowninFigure4.Thenumbersinbracketsindicatethecorpuscountofthefacts.SaddamHussein:BasicInformation:citizenship:Iraq[203]occupation:president[4412],leader[1792],dictator[664],...relative:odai[89],qusay[65],uday[65],...LifeEvents:placesbeento:bagdad[403],iraq[270],palaces[149]...Organizationsassociatedwith:managerofbaathparty[1000],...CustodyEvents:Saddamwasarrested[52],CommunicationEvents:Saddamsaid[3587]......Figure4:Samplebiographyoutput.6RelatedWorkWhiletherehasbeenpreviousworkonextractingsocialnetworksfromemailsandtheweb(Culottaetal.,2004),webelievethisistheﬁrstpapertopresentafull-ﬂedgedsystemforextractingsocialnetworksfromconversationalspeechtranscripts.Similarly,mostoftheworkonco-referenceres-olutionhasnotfocusedonconversationalspeech.(Jietal.,2005)usessemanticrelationstoreﬁneco-referencedecisions,butinaapproachdifferentfromours.7ConclusionsandFutureWorkWehavedescribedanovelapproachforextractingsocialnetworks,biographicalfacts,andmovementsummariesfromtranscriptsoforalinterviewswithHolocaustsurvivors.Wehaveimprovedtheper-formanceofsocialnetworkextractionﬁve-fold,comparedtoabaselinesystemthatalreadyusesstate-of-the-arttechnology.Inparticular,weim-provedtheperformanceofco-referenceresolutionforconversationalspeech,byfeatureengineeringandimprovingtheclusteringalgorithm.Althoughourapplicationdataconsistsofcon-versationalspeechtranscriptsinthispaper,thesameextractionapproachcanbeappliedtogeneral-domaintextaswell.Extractinggeneral,richsocialnetworksisveryimportantinmanyap-plications,sinceitprovidestheknowledgeofwhoisconnectedtowhomandhowtheyareconnected.Therearemanyinterestingissuesinvolvedinbiographygenerationfromalargedatacollection,suchashowtoresolvecontradictions.Thecountsfromthecorpuscertainlyhelptoﬁlteroutfalseinformationwhichwouldotherwisebedifﬁculttoﬁlter.Butbettertechnologyatdetectingandre-solvingcontradictionswilldeﬁnitelybebeneﬁcial.AcknowledgmentWewouldliketothankMartinFranzandBhuvanaRamabhadranfortheirhelpduringthisproject.ThisprojectisfundedbyNSFundertheInfor-mationTechnologyResearch(ITR)program,NSFIISAwardNo.0122466.Anyopinions,ﬁndingsandconclusionsorrecommendationsexpressedinthismaterialarethoseoftheauthorsanddonotnecessarilyreﬂecttheviewsoftheNSF.References2005.Automaticcontentextraction.http://www.nist.gov/speech/tests/ace/.AronCulotta,RonBekkerman,andAndrewMcCal-lum.2004.Extractingsocialnetworksandcon-tactinformationfromemailandtheweb.InCEAS,MountainView,CA.RaduFlorian,HanyHassan,AbrahamIttycheriah,HongyanJing,NandaKambhatla,XiaoqiangLuo,NicolasNicolov,andSalimRoukos.2004.Asta-tisticalmodelformultilingualentitydetectionandtracking.InProceedingsof.HLT-NAACL2004.SamuelGustman,DagobertSoergelandDouglasOard,WilliamByrne,MichaelPicheny,BhuvanaRamab-hadran,andDouglasGreenberg.2002.Support-ingaccesstolargedigitaloralhistoryarchives.InProceedingsoftheJointConferenceonDigitalLi-braries,pages18–27.1047

HengJi,DavidWestbrook,andRalphGrishman.2005.Usingsemanticrelationstoreﬁnecoreferencedeci-sions.InProceedingsofHLT/EMNLP’05,Vancou-ver,B.C.,Canada.XiaoqiangLuo,AbeIttycheriah,HongyanJing,NandaKambhatla,andSalimRoukos.2004.Amention-synchronouscoreferenceresolutionalgorithmbasedonthebelltree.InProceedingsofthe42ndAn-nualMeetingoftheAssociationforComputationalLinguistics(ACL2004),pages135–142,Barcelona,Spain.VincentNg.2005.Machinelearningforcoreferenceresolution:Fromlocalclassiﬁcationtoglobalrank-ing.InProceedingsofACL’04.D.Oard,D.Soergel,D.Doermann,X.Huang,G.C.Murray,J.Wang,B.Ramabhadran,M.Franz,S.Gustman,J.Mayﬁeld,L.Kharevych,andS.Strassel.2004.Buildinganinformationre-trievaltestcollectionforspontaneousconversationalspeech.InProceedingsofSIGIR’04,Shefﬁeld,U.K.Author Index

Abdelali, Ahmed, 872
Ahmad, Khurshid, 984
Ailomaa, Marita, 1008
Albrecht, Joshua, 296, 880
Andrew, Galen, 824
Aw, Aiti, 200

Babych, Bogdan, 136
Baldridge, Jason, 896
Banea, Carmen, 976
Bangalore, Srinivas, 152
Barthelemy, Francois, 928
Barzilay, Regina, 504, 544
Basili, Roberto, 776
Basu, Anupam, 104
Benus, Stefan, 800
Bergsma, Shane, 656
Blitzer, John, 440
Bod, Rens, 400
Bonnema, Remko, 792
Branavan, S. R. K., 544
Briscoe, Ted, 912, 992
Brody, Samuel, 448
Bunescu, Razvan, 576

Campbell, Lyle, 65
Chai, Joyce, 368
Chan, Yee Seng, 33, 49
Chang, Ming-Wei, 280
Chang, Pi-Chuan, 9
Chavez, Hector, 800
Che, Wanxiang, 200
Chew, Peter, 872
Chiang, David, 33, 144
Choudhury, Monojit, 104
Chua, Tat-Seng, 592
Cimiano, Philipp, 888
Clark, Stephen, 248, 840
Cohn, Trevor, 728

Collobert, Ronan, 560
Cong, Gao, 81
Curran, James, 240, 248

D’Haro, Luis Fernando, 376
Dagan, Ido, 456
Daille, B´eatrice, 664
Daland, Robert, 936
Dale, Robert, 344
Dang, Hoa Trang, 768
Daume III, Hal, 65, 256
Davidov, Dmitry, 232
Davis, Randall, 352
De Boni, Marco, 792
Demberg, Vera, 96, 920
DeNero, John, 17
Deng, Yonggang, 1
Deschacht, Koen, 1000
Deshpande, Pawan, 544
Devitt, Ann, 984
Dong, Minghui, 120
Downey, Doug, 696
Dras, Mark, 344
Dredze, Mark, 440

Eisenstein, Jacob, 352
Elhadad, Michael, 224
Erk, Katrin, 216
Esuli, Andrea, 424
Etzioni, Oren, 696

Feldman, Ronen, 600
Ferret, Olivier, 480
Filippova, Katja, 320
Finkel, Jenny Rose, 272
Fisher, Seeger, 488

Ganguly, Niloy, 104
Gao, Jianfeng, 824

1049

Gao, Yuqing, 1
Gardent, Claire, 328
Ghose, Anindya, 416
Gildea, Daniel, 184
Girju, Roxana, 568
Glass, James, 504
Goldberg, Yoav, 224
Goldwater, Sharon, 744
Gordon, Andrew, 192
Gravano, Agustin, 800
Grenager, Trond, 272
Grifﬁths, Tom, 744
Guan, Yi, 720
Gurevych, Iryna, 1032

Haffari, Gholamreza, 25
Haffner, Patrick, 152
Haghighi, Aria, 848
Hall, Johan, 968
Hall, Keith, 392
Hannan, Kerry, 432
Hao, Yanfen, 57
Hartley, Anthony, 136
Hassan, Hany, 288
Havelka, Jiri, 608
Henderson, James, 632
Hirschberg, Julia, 800
Hogan, Deirdre, 680
Hollingshead, Kristy, 952
Hovy, Eduard, 1024
Hsueh, Pei-Yun, 1016
Huang, Liang, 144
Huang, Yun, 704
Hwa, Rebecca, 296, 880

Ipeirotis, Panagiotis, 416

Jansche, Martin, 736
Jiang, Jing, 264
Jin, Rong, 368
Jing, Hongyan, 1040
Johnson, Mark, 168, 824
Johnston, Michael, 376
Joshi, Aravind, 760

Kageura, Kyo, 664
Kambhatla, Nanda, 1040

Kan, Min-Yen, 712
Kanazawa, Makoto, 176
Kanthak, Stephan, 152
Karimi, Sarvnaz, 640, 648
Kim, Kyoung-Young, 112
Klein, Dan, 17, 848
Ko, Jeongwoo, 784
Koller, Alexander, 336
Kondrak, Grzegorz, 656, 864, 944
Koppel, Moshe, 232
Korhonen, Anna, 912
Kow, Eric, 328
Kuhlmann, Marco, 160
Kuo, Jin-Shea, 120
Kurimo, Mikko, 89

Lafferty, John D., 752
Lane, Ian, 520
Lapata, Mirella, 728
Lee, John, 81, 472
Levine, Michelle, 376
Li, Chi-Ho, 720
Li, Haizhou, 120, 712
Li, Hang, 688
Li, Hong, 584
Li, Minghui, 720
Li, Mu, 720
Li, Sheng, 200
Lin, Chia-Ling, 1024
Lin, Chin-Yew, 81, 1024
Lin, Jimmy, 768
Lin, Shouxun, 704
Litman, Diane, 360
Liu, Chang, 208
Liu, Feifan, 672
Liu, Qun, 704
Liu, Ting, 200
Liu, Xiaohua, 81
Liu, Yang, 672, 704
Liu, Yi, 368, 464

Ma, Yanjun, 304
Mairesse, Francois, 496
Malioutov, Igor, 504
Manandhar, Suresh, 776
Manning, Christopher D., 272

1050

Marco, Baroni, 904
Maslennikov, Mstislav, 592
Matsoukas, Spyros, 312
McDonald, Ryan, 432
Medlock, Ben, 992
Mihalcea, Rada, 976
Minkov, Einat, 128
Mitamura, Teruko, 784
Mittal, Vibhu, 464
Miyao, Yusuke, 624
Moens, Marie-Francine, 1000
M¨ohl, Mathias, 160
M¨ohler, Gregor, 96
Mooney, Raymond, 576, 960
Moore, Johanna D., 808, 1016
Morin, Emmanuel, 664
Moschitti, Alessandro, 776
Mudraya, Olga, 136
Mukherjee, Animesh, 104
M¨uller, Christoph, 816
Muresan, Smaranda, 832
Mutton, Andrew, 344
Mller, Christof, 1032

Neylon, Tyler, 432
Ng, Hwee Tou, 33, 49, 208, 688
Ng, Vincent, 536
Nilsson, Jens, 968
Nivre, Joakim, 968
Nyberg, Eric, 784

Okanohara, Daisuke, 73
Osborne, Miles, 512

Pallotta, Vincenzo, 1008
Palmer, Alexis, 896
Park, Alex, 504
Pereira, Fernando, 440
Pierrehumbert, Janet, 936
Ponvert, Elias, 896
Preiss, Judita, 912
Puurula, Antti, 89

Quarteroni, Silvia, 776

Rambow, Owen, 832
Rappoport, Ari, 232, 408, 616

Ratinov, Lev, 280
Reichart, Roi, 408, 616
Reitter, David, 808
Renger, Bernard, 376
Reynar, Jeff, 432
Riezler, Stefan, 464
Roark, Brian, 488, 952
Rosenfeld, Benjamin, 600
Rosti, Antti-Veikko, 312
Rotaru, Mihai, 360
Roth, Dan, 280
Roukos, Salim, 1040

Sagae, Kenji, 624
Sarkar, Anoop, 25
Satta, Giorgio, 760
Schmid, Helmut, 96
Schoenmackers, Stefan, 696
Scholer, Falk, 640, 648
Schultz, Tanja, 520
Schwartz, Richard, 312
Sebastiani, Fabrizio, 424
Seginer, Yoav, 384
Seretan, Violeta, 1008
Setiawan, Hendra, 712
Sharoff, Serge, 136
Shen, Libin, 760
Sherif, Tarek, 864, 944
Shnarch, Eyal, 456
Sim, Khe Chai, 120
Sima’an, Khalil, 288
Sims, Andrea D., 936
Smith, Carlota, 896
Smith, Noah A., 752
Specia, Lucia, 41
Spitters, Martijn, 792
Sproat, Richard, 112
Stefan, Evert, 904
Stevenson, Mark, 41
Stone, Matthew, 336
Stroppa, Nicolas, 304
Strube, Michael, 320
Su, Jian, 528
Sun, Guihua, 81
Sundararajan, Arun, 416
Suzuki, Hisami, 128

1051

Swanson, Reid, 192
Szpektor, Idan, 456

Takeuchi, Koichi, 664
Talbot, David, 512
Tam, Yik-Cheung, 520
Tan, Chew Lim, 200
Tang, Jie, 688
Temperley, David, 184
Titov, Ivan, 632
Toutanova, Kristina, 9, 128, 824
Tsochantaridis, Ioannis, 464
Tsujii, Jun’ichi, 73, 624
Turpin, Andrew, 640, 648

Uefﬁng, Nicola, 25
Uszkoreit, Hans, 584

Vadas, David, 240
Vail, Douglas L., 752
Vasserman, Alexander, 464
Veale, Tony, 57
Volpe Nunes, Maria das Grac¸as, 41

Walker, Marilyn, 496
Wan, Stephen, 344
Wan, Xiaojun, 552
Wang, Haifeng, 856
Way, Andy, 288, 304
Wells, Mike, 432
Wenderoth, Johanna, 888
Weston, Jason, 560
Wiebe, Janyce, 976
Wilcox, Lauren, 800
Wong, Yuk Wah, 960
Wu, Chung-Hsien, 1024
Wu, Hua, 856

Xiao, Jianguo, 552
Xiong, Zhongyang, 81
Xu, Feiyu, 584

Yang, Jianwu, 552
Yang, Xiaofeng, 528
Yoon, Su-Youn, 112
Yu, Liang-Chih, 1024

Zavrel, Jakub, 792

Zesch, Torsten, 1032
Zhai, ChengXiang, 264
Zhang, Dongdong, 720
Zhang, Min, 200
Zhang, Yue, 840
Zhao, Tiejun, 688
Zhou, Guodong, 200
Zhou, Ming, 81, 720
Zhu, Conghui, 688

1052

 ACL  2007PRAGUEISBN 978-1-932432-86-2®ČSKI