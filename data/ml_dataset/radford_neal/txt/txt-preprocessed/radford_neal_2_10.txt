Abstract.Empiricallyassessingthepredictiveperformanceoflearningmethodsis
anessentialcomponentofresearchinmachinelearning.TheDELVEenvironment
wasdevelopedtosupportsuchassessments.Itprovidesacollectionofdatasets,a
standardapproachtoconductingexperimentswiththesedatasets,andsoftwarefor
thestatisticalanalysisofexperimentalresults.Inthispaper,DELVEisusedto
assesstheperformanceofneuralnetworkmethodswhentheinputsavailabletothe
networkhavevaryingdegreesofrelevance.Theresultscon(cid:12)rmthattheBayesian
methodof\AutomaticRelevanceDetermination"(ARD)isoften(butnotalways)
helpful,andshowthatavariationon\earlystopping"inspiredbyARDisalso
bene(cid:12)cial.Theexperimentsalsorevealsomeotherinterestingcharacteristicsofthe
methodstested.Thisexampleillustratestheessentialroleofempiricaltesting,and
showsthestrengthsandweaknessesoftheDELVEenvironment.
Introduction
Thispaperhastwopurposes:Toshowhowtheperformanceofmachine
learningmethodscanbeassessedusingtheDELVEenvironment,andto
investigatesomemethodsfordealingwithlearningproblemsinwhichthe
availableinputsareofvaryingrelevance.Thesetopicsarelinkedbytheuse
oftheDELVEenvironmenttoassesstheperformanceoftherelevancedeter-
minationmethods.
Below,Idiscusstheneedforempiricalassessmentsoflearningmethods,
andthede(cid:12)cienciesofcurrentpracticeinthisarea.Ithendiscusstheproblem
ofdeterminingtherelevanceofthevariousinputsthatmaybeavailablefor
useinasupervisedlearningproblem(eitherregressionorclassi(cid:12)cation).Fol-
lowingthisintroduction,IdescribetheDELVEenvironmentforconducting
empiricalassessmentsinmoredetail,anddescribetwotypesofneuralnet-
workmethods,basedonBayesianlearningimplementedusingMarkovchain
MonteCarlo,andonensemblesofnetworkstrainedusingearlystopping.
Bayesianlearningcanbedoneinconjunctionwiththetechniqueof\Auto-
maticRelevanceDetermination"(ARD),whichpastexperimentssuggestis
bene(cid:12)cial.ThemechanicsofARDaretheinspirationforavariationofearly
stoppingthatwemighthopewillalsobebene(cid:12)cialwhensomeinputsare
morerelevantthanothers.
TheDELVEenvironmentisthenusedtoassesstheperformanceofthe
Bayesianandearlystoppingmethods,withandwithouttheuseofrelevance


RadfordM.Neal
determinationtechniques.Theresultsallowconclusionstobedrawnabout
theworthofthesemethods,andaboutthestrengthsandweaknessesofthe
currentDELVEenvironment.
.Theneedforgoodempiricalassessmentsoflearningmethods
Onemighthopethattheperformanceofmachinelearningmethodscouldbe
fullyanalysedtheoretically,eliminatinganyneedforempiricalassessmentsof
performanceonactualproblems.Theoreticalanalysiscanbedi(cid:14)cult,how-
ever.Morefundamentally,withoutfurtherknowledge,acompleteanalysisis
generallyimpossibleinprinciple,sincetheperformanceofalearningmethod
usuallydependsontherealsituationinwhichitisapplied.
Bayesianmethodsdoincorporatefurtherknowledgeoftheproblem,in
theformofapriordistributionthatismeanttocapturetheuser'sbeliefs
abouttherealsituation.Ifthepriordistributionaccuratelyre(cid:13)ectsthese
beliefs,aBayesianmightregardempiricaltestsofthemethodbasedonthis
priorasirrelevant.Inpractice,however,onecanseldombesurethattheprior
distributionusedforacomplexmodelactuallycapturestherealpriorbeliefs.
Partly,thisisbecausethesubstantiale(cid:11)ortrequiredtoensurethiscannot
bejusti(cid:12)edwhenitisthoughtlikelythatacruderapproximationwillsu(cid:14)ce
fortheproblemathand.Accurateexpressionofpriorbeliefsisalsolimited
bythedi(cid:14)cultyofvisualizingdistributionsinhighdimensions.Empirical
testsmayreveal(cid:13)awsinthespeci(cid:12)cationofthemodelorprior,andmay
alsobeneededtoassesstheadequacyofthecomputationalmethodsusedto
implementaBayesianmodel,whichofteninvolveapproximations,atleast
whenallowedonlyalimitedamountofcomputationtime.
Methodsjusti(cid:12)edwithinotherframeworkswillalsooftenrequireempiri-
caltestinginordertoassesstheirpracticalworth.Furthermore,performance
onactualproblemsistheonlycriterionbywhichonecancomparemethods
thathavebeenjusti(cid:12)edwithindi(cid:11)erentphilosophicalframeworks,orthatare
basedonvagueintuitionsratherthancoherentprinciples.Empiricaltestingis
howwewillultimatelybeabletoseetherelativemeritsoftheBayesian,min-
imumdescriptionlength,structuralriskminimization,andotherapproaches
|thoughofcourse,nosingletestwillbedecisiveinthisregard.
Despitetheirimportance,goodempiricalassessmentsarerare.Inthe
statisticalliterature,itisquitecommonfornewly-proposedmethodstobe
illustratedonaproblemthatisreal(thoughoftenstale),withtheapparent
intentofdemonstratingthatthemethodcanproduceinterestingresultsin
actualapplications.Inworkingthroughsuchproblems,itistypicalforall
thedatatobeusedintrainingthemethod,withtheresultthatnoneisleft
forcheckingtheaccuracyoftheresults.Thispracticeseemstoresultfroma
confusionbetweensolvingarealproblemanddoingmethodologicalresearch.
Whenthegoalistosolvetheproblem,oneshouldofcourseuseallthedata
available,inonewayoranother.Butwhenassessingtheworthofanewly-

AssessingRelevanceDeterminationMethodsUsingDELVE

proposedmethod,weneedanindependentcheckofitsaccuracy,whichis
obtainableonlybytestingondatanotusedduringtraining.
Testsondataseparatefromthatusedintrainingaremorecommoninthe
neuralnetworkandmachinelearningliteratures.Oneapproachistodivide
adatasetintoa\training"setanda\test"set.Severalmethodsarethen
appliedtothetrainingset,onthebasisofwhicheachmakespredictionsfor
casesinthetestset.Thepredictiveperformancesofthemethodsarethen
compared.Thismethodologysu(cid:11)ersfromtwoseriousproblems.First,the
datasetsusedareoftenquitesmall(lessthancases),thetestsetsare
evensmaller,andconsequentlyonlyquitelargedi(cid:11)erencesinperformance
canbereliablydiscerned.Second,thismethodologyignoresthepossibility
thattherelativeperformanceofthemethodsmayvarywiththerandom
choiceoftrainingset.
Inanattempttoovercometheseproblems,n-foldcross-validationassess-
mentsaresometimesperformed,inwhichthedataisdividedintonportions,
andntrainingrunsaredone,eachonallbutoneportion,withtheheld-out
portionbeingusedasatestset.Unfortunately,theoverlapofthetraining
setsinthisschemeintroducesunknowabledependenciesbetweenruns,mak-
ingformalstatisticaltestsimpossible.InvestigationsbyDietterich(		)
showthatapplyingat-testdespitethesedependenciescanresultininvalid
results,thoughthesituationisnotasbadasforthewidely-usedresampled
t-testprocedure,basedonmultiplerandomsplitsintotrainingandtestsets,
whichisguaranteedtoproduceanapparently\signi(cid:12)cant"resultifenough
randomsplitsareused.Dietterichintroducesanew\xcv"testintendedto
replacethese(cid:13)awedtests,butbyhisownevidence,thistestisitselfdeeply
(cid:13)awed|itisbasedonnumerousincorrectindependenceassumptions,and
inoneexamplehasaTypeIerrorprobabilitythatiso(cid:11)byafactoroftwo.
Therootcauseofthissadstateofa(cid:11)airsisaninsistenceontryingto
answerthefollowingquestion:\GiventwolearningalgorithmsAandBand
asmalldatasetS,whichalgorithmwillproducemoreaccurateclassi(cid:12)ers
whentrainedondatasetsofthesamesizeasS?"(Dietterich		).This
questionisfundamentallyunanswerable,giventhatonlyasingleinstanceof
adatasetofsizeSisavailable.Smalldatasetsaresimplynotsuitablefortests
oflearningmethods.Oneshoulduselargedatasetsinstead,fromwhichone
canselectseveralnon-overlappingtrainingsetsofwhateversizeisofinterest,
alongwithaseparatetestsetthatisreasonablylarge.
Invalidresultscanbeobtainedwiththisapproachtoo,however,asil-
lustratedbyFinno(cid:11),Hergert,andZimmerman(		).Intheirexperiments,
theyusesixseparatetrainingsets,eachaccompaniedbyatestsetwith
cases.Goodcomparisonsshouldbepossiblewithsuchasetup,butunfortu-
natelytheydestroythevalidityoftheresultsbysettingparametersofthe
methodsbasedoninformationfromallsixtrainingsets.Furthermore,they
deriveastatisticaltestbasedonthedubiousassumptionthattherelative
performanceoftwomethodsisthesameforalltrainingsets.Notwishing


RadfordM.Neal
togotothee(cid:11)ortofcomputingthistest,theyuseanapproximationwhich
discardsthepairinginformationintheexperiment,whichusuallygreatly
reducesthepowerofatesttodetectdi(cid:11)erences.Theythenuseextremely
generoussigni(cid:12)cancelevelsof.and.(orpossibly.and.,de-
pendingoncontext).Theirresultswouldthereforeappeartobeessentially
meaningless.Otherresearchershaveconductedequally(cid:13)awedexperiments.
TheDELVEproject(Rasmussen,etal		)aimstoimprovethequality
ofempiricalassessmentsbybuildingacollectionoflargedatasetssuitablefor
empiricalassessments,alongwithsoftwarethatsupportsavalidexperimen-
talmethodologyandvalidanalysisoftheresults.Collecting,producing,or
adaptingsuitabledatasetsisnotasimpletask,whichgoessomewaystoward
explainingwhyunsuitabledatasetsareoftenused.Wedonotyethavethe
varietyofdatasetsthatwewouldliketohaveinDELVE.Wedohavea(cid:12)rst
versionofthesoftware,andamodestcollectionofresults.
DELVEisdescribedmorefullyinSection.First,however,Iwilldiscuss
aparticularresearchproblemthatwillbeusedinthispapertoillustratehow
empiricalassessmentsusingDELVEcanhelpanswerinterestingquestions.
.Theproblemofdealingwithvaryinginputrelevance
Forlearningtobesuccessful,wemustalwaysdiscardmuchinformationthat
isknownaprioritobeirrelevant,asotherwisewewillbefooledbythechance
associationsthatwearelikelytoencounterifwesearchforrelationshipswith
ahugenumberofinputvariables.Butwhatshouldwedowhenwethinkthat
someinputsmightbeimportant,butmightalsobeoflittlerelevance?Can
weusethedatatodeterminethedegreetowhicheachinputisrelevant,and
therebyavoidboththecostofnotusinginputsthatareuseful,andthecost
ofbeingmisledbychanceassociationswithinputsthathavelittlerelevance?
Oneapproachistoselectasubsetofinputvariablesusingsomecriterion
thatbalancesdata(cid:12)tandmodelcomplexity(eg,AIC).Whennumberof
inputsislarge,consideringallpossiblesubsetsisinfeasible,butonecantry
addingvariablesoneatatime(forwardselection),orremovingthemoneata
time(backwardelimination).Thisapproachhaslongbeenusedinthecontext
oflinearregression.Bonnlander(		)reviewsvariableselectionmethodsfor
neuralnetworks,anddevelopsonebasedonmutualinformation.
Variableselectionseemstometobeadubiousprocedure,sinceifwe
thinkaninputmightberelevant,wewillusuallyalsothinkthatitisprob-
ablyatleastalittlebitrelevant.Situationswherewethinkallvariablesare
eitherhighlyrelevantornotrelevantatallseemuncommon(thoughnot,
ofcourse,impossible).Accordingly,weshouldusuallynotseektoeliminate
somevariables,butrathertoadjustthedegreetowhicheachvariableis
consideredrelevant.Wemayhopethatamethodofthissortfordetermin-
ingtherelevanceofinputswillbeabletoavoidpayingundueattentionto
thelessrelevantinputs,whilestillmakinguseofthesmallamountofextra
informationthattheyprovide.

AssessingRelevanceDeterminationMethodsUsingDELVE

ThisisthephilosophybehindtheBayesianmethodof\AutomaticRele-
vanceDetermination"(ARD)(MacKay		;Neal		)formultilayerper-
ceptronnetworks.Inthismethod,ahyperparameterisassociatedwitheach
input,whichcontrolsthesizeoftheweightsassociatedwithconnectionsout
ofthatinput.Ifthehyperparameterforaninputissmall,weightsforthat
inputwilllikelybesmall,andtheinputwillthereforehaveonlyasmalle(cid:11)ect
onthenetwork'spredictions.Theserelevancehyperparametersaresetina
Bayesianfashion,accordingtotheresultingprobabilityoftheobserveddata.
TheARDprocedurehasahigh-leveljusti(cid:12)cation,asthecorrectthingto
dogivenpriorknowledgethatsomeinputsmaybelessrelevantthanothers.
Onecanalsolookatthelow-levelmechanicsofhowARDoperates,however,
whichonecantrytoadaptforuseinanon-Bayesiancontext.Inthispaper,
Iwillinvestigateavariationoftrainingwithearlystoppingthatusesan
ARD-likemethodfordealingwithinputsofvaryingrelevance.Theaimisto
determinewhethersuchamethodisactuallyanimprovementoveraplain
earlystoppingmethod,andhowitsperformancecomparestotheBayesian
ARDmethod.Someotheraspectsofthesemethodswillbeexaminedalong
theway.
TheDELVEenvironment
TheDELVEenvironment,developedattheUniversityofToronto,aimsto
supportvalidempiricalassessmentsoflearningmethodsforregressionand
classi(cid:12)cationapplications.Weareespeciallyinterestedinmethodsfor(cid:12)nding
non-linearrelationshipsinhigh-dimensionaldata.Neuralnetworks,classi(cid:12)-
cationtrees,andnearestneighbormethodsareamongthemanyapproaches
thathavebeentakentosuchproblems.Eachsuchapproachhasmanyvari-
ations.WehopethatDELVEwillhelpindeterminingwhichvariationsare
actuallyuseful,andwillallownewvariationstobeeasilyassessed.
DELVEconsistsofanarchiveofdatasetsforuseinassessinglearning
methods,asetofconventionsandsoftwarethatsupportexperimentswith
thisdata,andanarchiveoftheresultsofsuchassessments.Iwillhere
giveabriefandsomewhatsimpli(cid:12)edsketchofthemainaspectsofDELVE.
Forcompletedetails,seetheDELVEmanual(Rasmussen,etal		)and
thePhDthesisbyRasmussen(		).TheDELVEarchiveislocatedat
http://www.cs.utoronto.ca/(cid:24)delve/.
.Fromdatasetstotaskinstances
ADELVEdatasetconsistsofasetofcases.Foreachcase,thevaluesof
certainattributesarerecorded.Atpresent,thecasesinaDELVEdatasetare
consideredtobeindependent,thoughinfuturewemaysupportexperiments
withtimeseriesdataorinwhichothersortsofdependenciesarepresent.


RadfordM.Neal
DELVEdatasetscanhavevariousorigins.Naturaldatasetsfromreal-
worldlearningproblemsareusefulbecausetheyarerepresentativeofthe
problemswewouldactuallyliketosolve.Arti(cid:12)cialdatasetsbasedonsimple
formulasareeasytoconstruct,andcanallowteststhatarenarrowlyfocused
onparticularissues.Atpresent,mostDELVEdatasetsareofanintermedi-
atetype,producedbyrealisticsimulationprogramsthatmimicasituation
whereonemightwellwishtousealearningmethod.Thisallowsustogener-
ateanunlimitednumberofcases,andalsoletsusproducefamiliesofrelated
datasetsthatcanprovideinformationaboutwhichfactorsin(cid:13)uencetheper-
formanceofthelearningmethods.Wehopethatthesimulationsusedare
realisticenoughthattheyarerepresentativeofrealapplications.
The(cid:12)rststepinusingadatasettoassesslearningmethodsistode(cid:12)ne
whatwecallaprototask,inwhichoneattributeisidenti(cid:12)edasthetargetto
bepredicted,andsomeotherattributesareidenti(cid:12)edasinputs,whichare
availableforuseinpredictingthetarget.Whenthetargetisarealnumber,
wehavearegressionprototask;whenthetargetisfroma(cid:12)niteset,wehave
aclassi(cid:12)cationprototask.Inthispaper,onlyregressionprototasksareused,
butDELVEsupportsclassi(cid:12)cationaswell.
Foreachprototask,wecande(cid:12)neoneormoretasks,inwhichthenumber
oftrainingcasesisspeci(cid:12)ed.Itismeaningfultoaskhowwellaparticular
learningmethoddoesonatask,asjudgedbytheexpectedlosswhenitis
usedtopredictthetargetinatestcaseusingtheinformationfromatraining
setofthespeci(cid:12)edsize.SeverallossfunctionsaresupportedbyDELVE,but
inthispaperonlysquarederrorlosswillbeused.
Theexpectedlossofamethodonataskistheaveragelosswithrespectto
therandomselectionofatrainingsetofthesizespeci(cid:12)edforthetaskandthe
randomselectionofatestcase.Boththeserandomselectionsarefromthereal
distributionfromwhichthecasesweredrawn,notfromtheparticularsetof
casesthatwehaveavailablefortesting.Theexpectedlosscanthereforeonly
beestimated,withsomeuncertainty,onthebasisofperformanceonseveral
taskinstances,eachofwhichconsistsofatrainingsetandasetoftestcases.
Thetrainingsetsfordi(cid:11)erenttaskinstancesarealwaysdisjoint,becausethere
isnowayofaccountingforthedependenciesthatwouldoccuriftrainingsets
overlapped.Inthispaper,thetestsetsusedarealsonon-overlapping.This
simpli(cid:12)esthestatisticalanalysis,andismoree(cid:14)cientwhenthereisnolimit
ontheamountofsimulateddatathatcanbegenerated.
MostDELVEprototasksareassociatedwithasetof(cid:12)vetaskswithtrain-
ingsetsizesof,,,,and.Eighttaskinstancesareusually
usedforeachofthesetasks,exceptthatoftenonlyfourinstancesareusedfor
thetaskwithtrainingcases.Itisimportantthatsuchconventionsbe
establishedforeachprototask,sothattheresultsofexperimentsbydi(cid:11)erent
researcherscanbecompared.However,di(cid:11)erentconventionsmightbeused
forsomenewprototaskifthiswasthoughtdesirable.

AssessingRelevanceDeterminationMethodsUsingDELVE

.De(cid:12)ningandrunningalearningmethod
Beforealearningmethodcanbeassessed,itmust(cid:12)rstbecarefullyde(cid:12)ned.
Afullyautomaticmethodcanbede(cid:12)nedbyaprogramthatimplements
it,perhapssupplementedbysomeexplicitinstructionsonwhatthehuman
operatorshoulddoincertainsituations(eg,reducethegradientdescent
learningrateforaneuralnetworkbyafactoroftwoifinstabilityisobserved).
Methodsthatrequiresubstantialhumanjudgementcouldintheorybede(cid:12)ned
byadescriptionofthemethodthatcouldbeunderstoodbyanydataanalyst
withasuitablebackground.Properlyassessingsuchamanualmethodwould
bequitecostly,however,asmanyanalystswouldneedtolearnhowtousethe
method,andthenanalyseinstancesofvarioustasks.Alltheresultscurrently
archivedinDELVEareforautomaticmethods.
Aprogramimplementingamethodreadstheinputsandtargetsforthe
trainingcases,plustheinputsforthetestcases,andproducesguessesatthe
targetsinthetestcases.Theguessforonetestcaseismeanttobemade
withoutreferencetotheinputsforothertestcases.Somemethodswillmake
useofrandomnumbers|forexample,torandomlyinitializetheweights
inaneuralnetwork,oraspartofaMonteCarlomethodforevaluating
integrals.Suchmethodsshoulduseadi(cid:11)erentrandomnumberseedforeach
taskinstance.Otherwise,theresultsmightdependontheparticularseed
chosen,andifsowouldbeveryfragile,subjecttochangewheneveraminor
modi(cid:12)cationtotheprogramaltersthesequenceofcallstotherandomnumber
generator,orwhenthetaskisslightlyaltered(eg,theinputsarere-ordered).
Itisuptothewriterofalearningmethodhowthetrainingdataandtest
inputsareusedtomakepredictionsforthetesttargets.However,DELVE
providessupportforsomecommonencodingandnormalizationprocedures,
whichmanyresearchersmaychoosetouseiftheyarenotspeci(cid:12)callyinvesti-
gatingthisaspectoflearning.Allthemethodsdiscussedinthispaperusethe
defaultDELVEnormalizationforinputsandtargets,whichistoshiftthem
sothattheyhavemedianzerooverthetrainingset,andtoscalethemso
thatoverthetrainingsettheiraverageabsolutedeviationfromthemedian
isone.TheDELVEsoftwarecreatesdata(cid:12)lesforeachtaskinstanceusing
thisnormalizingtransformation(whichisusuallydi(cid:11)erentforeachinstance).
Theprogramimplementingthemethodcanthenberun,afterwhichDELVE
willtransformthemethod'sguessesforthenormalizedtesttargetsbackto
theoriginalform.Foreachtestcase,thesquarederrorlosswithrespectto
theactualtargetvalueisthencomputed.
.Analysingtheresults
Theresultofrunningalearningmethodonataskisasetof(cid:12)lesthatrecord
theguessesthemethodmadeforeachtestcaseineachtaskinstance.These
guessesareretainedforlateranalysis,whichmightpotentiallyincludeade-
tailedcomparisonoftheguessesmadebydi(cid:11)erentmethods.Atpresent,how-
ever,theonlyanalysessupportedbyDELVEareestimationoftheexpected


RadfordM.Neal
loss(eg,squarederror)onataskforasinglemethod,andthecomparisonof
theexpectedlossesonataskfortwomethods.
Theseanalysesarefairlysimplewhenboththetrainingsetsandthetest
setsindi(cid:11)erenttaskinstancesaredisjoint.SupposethatwehaveItask
instances,foreachofwhichwehavebothatrainingsetandasetofJtest
cases.SupposethatwhenmethodAistrainedonthetrainingsetforinstance
i,itslosswhenpredictingthetargetfortestcasejofinstanceiisyij.We
cancomputetheaveragelossfortestcasesininstancei,written(cid:22)yi,andthe
averagelossoverallinstances,written(cid:22)y,asfollows:
(cid:22)yi=JJXj=yij;
IXi=(cid:22)yi
(cid:22)y=I
()
Notethat(cid:22)yisanunbiasedestimateofthetrueexpectedlossforthismethod
withrespecttotherandomselectionofatrainingsetandatestcase.Because
thetaskinstancesareindependent,itispossibletoestimatethestandard
errorof(cid:22)y(seeRasmussen,etal(		)fordetails).Acon(cid:12)denceintervalfor
thetrueexpectedlosscouldbeobtainedifonewerewillingtoassumethat
thedistributionof(cid:22)yisclosetoGaussian,whichisreasonableifIislarge,
thoughperhapsnotwhenI=.Suchcon(cid:12)denceintervalsarenotcomputed
byDELVEatpresent,however.
Ourmaininterestisusuallyincomparingtwomethods,AandB,with
respecttotheirexpectedlossonatask.Ifthesetwomethodshavebeen
runinthestandardwaywithintheDELVEenvironment,wewillhavethe
resultsforbothmethodsonthesametrainingsetsandtestcases.Wecan
thereforedoapairedcomparisonoftheseresults,whichismorepowerful
thanacomparisonusingresultsondi(cid:11)erenttrainingandtestsets.Letxij
bethedi(cid:11)erenceinthelossformethodAandthelossformethodBwhen
eachistrainedonthetrainingsetforinstanceiandthenusedtopredict
thetargetintestcasejofinstancei.Wecancomputeaveragesforthese
di(cid:11)erencesinthesamewayaswedidforthelossesofasinglemethod:
IXi=(cid:22)xi
(cid:22)xi=JJXj=xij;
(cid:22)x=I
()
Here,(cid:22)xisanunbiasedestimateforhowmuchgreatertheexpectedlossof
methodAisthantheexpectedlossofmethodB.If(cid:22)xispositive,thenwehave
evidencethatmethodBisbetterthanA,andconversely,if(cid:22)xisnegative,we
haveevidencethatAisbetter.
However,evenifthereisnotruedi(cid:11)erenceinexpectedperformancebe-
tweenAandB,wewillgenerallyobtainsomenon-zerovaluefor(cid:22)x,andhence
someindicationthatoneortheothermethodisbetterthantheother.In-
deed,evenifBisbetterthanA,thereissomechancethat(cid:22)xwillbenegative,
indicatingthereverse.Weneedsomewayoftellingwhetherthesignof(cid:22)xis
areliableguidetowhichmethodisbetter.Becausethe(cid:22)xiareindependent,

AssessingRelevanceDeterminationMethodsUsingDELVE

wecanuseat-testforthis,basedonthefollowingt-statistic:
IXi=((cid:22)xi(cid:0)(cid:22)x)#(cid:0)=
t=(cid:22)x"

()
I(I(cid:0))
Ifthe(cid:22)xihaveaGaussiandistribution,thist-statisticwillhaveat-distribution
withI(cid:0)degreesoffreedom.Onthisbasis,wecan(cid:12)ndthep-valueforthe
comparison,whichistheprobabilityofobtainingat-statisticatleastaslarge
(inabsolutevalue)asthatobserved,ifinfactthetwomethodshavethesame
expectedloss.Ifthep-valueissmall(traditionally,lessthan.),wehave
goodevidencethattheapparentlybettermethodisactuallybetter.
Thereisnostrongreasontobelievethatthedistributionofthe(cid:22)xiis
actuallyGaussian,souseofthist-testisnotbeyondquestion.Fortunately,
however,thet-testisfairlyrobustinthisrespect,sotheresultsareusable,if
treatedwithappropriatecaution.Theimpactofthe(cid:22)xibeingnon-Gaussian
wouldbelessifIwerelarger,butwehaveneverthelesschosenI=formost
DELVEtasksbecausemanyinterestingmethodstakequitealongtimeto
run.DELVEusesamorecomplexanalysisofvarianceprocedurewhenthe
sametestsetisusedforeachtaskinstance,asmaybenecessaryforanatural
datasetwithafairlysmallnumberofcases.DELVEdoesnotuseoverlapping
trainingsets,sincethereisnowayofobtainingvalidstandarderrorsor
p-valuesinthiscontext,atleastnotwithoutdetailedpriorknowledgeofthe
behaviourofthemethods,whichispresumablynotavailableformethods
whosebehaviourisbeinginvestigatedempirically.
Themethodstobeassessed
Themethodsassessedinthispaperarebuiltaroundmultilayerperceptron
networks.Methodsinthemlp-mcfamilyuseBayesianlearningimplemented
usingMarkovchainMonteCarlotechniques.Methodsinthemlp-bgdfamily
(cid:12)ndanensembleofnetworkstrainedusingbatchgradientdescentwithearly
stopping.
Iwillstartbydescribingsomecharacteristicscommontoallthesemeth-
ods,afterwhichIdescribethetwobasicmethods,whichdonottrytoadapt
tovaryingrelevanceoftheinputs.IthendescribetheBayesianAutomatic
RelevanceDetermination(ARD)method,andthevariationonearlystopping
inspiredbyit.Asimplervariationonearlystoppingwhichmightalsohelp
wheninputsvaryinrelevanceisalsodescribed.
.Commoncharacteristicsofthemethods
Allthemethodsde(cid:12)nedinthispaperarebasedonmultilayerperceptron
(\backprop")networkswithonelayerofhiddenunits,withtanhactivation


.
.
.
.


RadfordM.Neal
Trainingsetsize



mlp-bgd-*
inputs
.



inputs
.



mlp-mc-*
inputs
.



inputs
.



Fig..Computationtimeinminutes,ona	MHzMIPSRprocessor.
function.Thisnumberofhiddenunitswaschoseninthehopesthatitislarge
enoughformosttaskswithuptotrainingcases|ie,thatlittlewould
begainedbyusingmorehiddenunitswiththatamountofdata.Boththe
Bayesianandtheearlystoppingmethodsaresupposedtoavoidover(cid:12)tting,
sothenumberofhiddenunitsisnotreducedforsmallertrainingsets.The
hiddenunitshaveconnectionstoallinputs,andalsohaveabiasinput.The
singleoutput(whoseactivationfunctionistheidentity)isconnectedtoall
thehiddenunits,andagainhasabiasinput.OneoftheBayesianMonte
Carlomethods(mlp-mc-)haddirectconnectionsfromtheinputstothe
outputaswell.
BayesianMonteCarloandearlystoppingensemblemethodshavepre-
viouslybeenusedinDELVEassessmentsbyRasmussen(		).Boththese
approachescanbeautomatedrelativelysimply.Incontrast,manycommonly-
usedapproachestonetworktrainingrequirethathumanjudgementsbemade
regardinghowtosetweightdecayparameters,exactlyhowmanyhidden
unitstouse,orwhentostoponetrainingrunandstartanother.Often,itis
notclearonwhatbasisthesedecisionsaretobemade.Assessmentsofsuch
methodswillonlybepossibleifandwhentheyhavebeenproperlyde(cid:12)ned.
FollowingRasmussen(		),themethodsusedinthispaperarede(cid:12)ned
suchthattheytakeaspeci(cid:12)edamountofcomputationtime.However,Ihave
de(cid:12)nedtimeintermsofthenumberofnetworkforwardandbackwardpasses,
ratherthanactualcomputationtimeonaparticularcomputer.Inparticular,
allthemethods(exceptthoseinSection.)wereallowedtheequivalentof
,passesoverthecompletetrainingset.Withthisde(cid:12)nition,twometh-
odsdesignedtotakethesameamountoftimemayactuallydi(cid:11)ersomewhat
intimetaken,duetodi(cid:11)eringoverheads,andthisrelationshipmayvary
somewhatdependingonthecomputerused.Theadvantageofthisapproach
isthattheresultsofthemethodswillbethesameonanycomputer,aside
frompossibledi(cid:11)erencesduetorandomnumbergeneratorsand(cid:13)oating-point
roundo(cid:11)error.
TheactualtimestakenbythemethodsareshowninFigure,fordi(cid:11)erent
sizesoftrainingset,anddi(cid:11)erentnumbersofinputs.Timevariedlittlefor

AssessingRelevanceDeterminationMethodsUsingDELVE

di(cid:11)erentmethodswithinthesamefamily.Rasmussen'smlp-ese-procedure
usedsomewhatlesstime,andhismlp-mc-procedureusedconsiderablymore
time.(Duetodi(cid:11)erencesinmachinesusedandthetimescalingemployed,
thesetimedi(cid:11)erencescannotbesummarizedbyasimplefactor.)
Allthemethodsinthispaperareimplementedusingthesoftwarefor(cid:13)ex-
ibleBayesianmodelingavailablefrommywebpage(speci(cid:12)cally,therelease
of		--).TheexactdescriptionsofthemethodsareintheDELVE
archive,alongwiththeresultsoftheexperiments.
.BayesianlearningusingMarkovchainMonteCarlo
InaMonteCarloimplementationofBayesianneuralnetworklearning(Neal
		),wedonottryto(cid:12)ndasingle\optimal"setofnetworkweights,but
insteadtrytosamplefromtheposteriordistributionfornetworkweights,
basedonapriordistributionandonthelikelihoodgiventhetrainingcases.
Wethenmakepredictionsfortestcasesbyaveragingtheoutputsproduced
bythenetworksinthissamplefromtheposteriordistribution.
Wecantrytoobtainasampleofnetworksfromtheposteriordistribution
bysimulatingaMarkovchainthathastheposteriorasitsequilibriumdis-
tribution.Forthemethodsinthemlp-mcfamily,thisMarkovchainisbuilt
usingtwotypesofupdates.Onetypechangestheweightsandbiasesinthe
networkusingadynamicaltechniquethatresemblesgradientdescentwith
\momentum".Theothertypeofupdatechangesthehyperparametersthat
controlthedistributionoftheweightsandbiases,alongwiththestandard
deviationforthe\noise"|thedi(cid:11)erencebetweenthenetworkoutputand
theactualtarget.Formoredetailsontheseupdates,see(Neal		)andthe
softwaredocumentation.
TheMarkovchainconvergestothedesireddistributionasymptotically,
butwillnotsamplefromtheexactlycorrectdistributioninarunof(cid:12)nite
length.Inanapplicationwithoutstricttimeconstraints,oneshouldexamine
theprogressofthechain,inordertodecidehowlongtoletitrunsoasto
produceanearly-correctresult.Theinitialpartoftherun,beforeequilibrium
appearstohavebeen(nearly)reached,shouldbediscarded.Predictionscan
thenbemadebyaveragingtheoutputsofnetworksfromtheremainingpart
oftherun.
Inatime-criticalapplication,onemightnothavetheoptionofrunning
thechainforlonger.Inthiscaseonecandonothingexceptusethelatter
partofthechainforpredictions,andhopeforthebest.Thereisnotmuchto
begainedbyjudgingexactlyhowmuchtodiscardofthebeginningofsucha
(cid:12)x-lengthrun|justdiscardingthe(cid:12)rstthirdandusingthelasttwo-thirds
forpredictionsseemstobegenerallysuitable.Thisapproachisconvenient
forautomatedassessments,sinceitinvolvesnohumanjudgement.
Themlp-mc-methodde(cid:12)nedbyRasmussen(		)usesthisapproach
ofrunningtheMarkovchainfora(cid:12)xedamountoftime,anddiscardinga
(cid:12)xedfraction.Fortheassessmentsdoneinthispaper,Ide(cid:12)nedthreerelated


RadfordM.Neal
methods,mlp-mc-,mlp-mc-,andmlp-mc-,whichalsousethisapproach,
butwhichusesomewhatdi(cid:11)erentMarkovchainupdates,involving\partial
gradients"(seeNeal		,Section..).Thesemethodsarealsoallowed
muchlesstimethanmlp-mc-.
Thethreenewmlp-mcmethodsdi(cid:11)erintheirnetworkarchitectureand
inwhethertheyuseAutomaticRelevanceDetermination.Themlp-mc-
methodhasnodirectinput-outputconnections,anddoesnotuseARD.It
doesuseahierarchicalpriorthataccountsforthedi(cid:11)eringrolesofconnec-
tionsofdi(cid:11)erenttypes(seeMacKay		).Theinput-to-hiddenweightsare
placedinonegroup,whosestandarddeviationisahyperparameter,thehid-
denunitbiasesformanothergroup,controlledbyanotherhyperparameter,
andthehidden-to-outputweightsformathirdgroup,controlledbyathird
hyperparameter.Thenoisestandarddeviationisalsoavariablehyperparam-
eter.Theoutputbiashasa(cid:12)xedstandarddeviationofone.
Thehopeisthattheposteriordistributionofthesehyperparameters,
whichdeterminecharacteristicssuchasthesmoothnessoftherelationship,
willbeconcentratedonvaluesthataresuitablefortheactualproblem.If
thisworksasintended,\over(cid:12)tting"willnotoccur.
.Ensemblesofnetworkstrainedwithearlystopping
Thetechniqueofearlystopping(MorganandBourland		)isasimple
andcomputationallycheapwayoftryingtoavoidover(cid:12)ttingwhentraininga
neuralnetwork.Inthesimplestformofthistechnique,theavailabletraining
dataisdividedintotwoportions.Oneportionisoftencalledthe\training
set",buttoavoidconfusionwiththefulltrainingset,Iwillherecallitthe
estimationset.Thenetworkistrainedbysomeoptimizationmethodsoas
tominimizesquarederror(orsomeothererrorfunction)onthecasesinthis
estimationset,startingfromaninitialstateinwhichthenetworkweightsare
small.Theotherportionofthetrainingdataiscalledthevalidationset.The
erroroncasesinthevalidationsetisusedtoselectanetworkfromamong
thosefoundinthecourseofminimizingtheerrorontheestimationset.Due
toover(cid:12)tting,thebestnetworkasjudgedbyerroronthevalidationsetis
oftennottheonewithsmallesterrorontheestimationset(whichwillbethe
lastonefound,iftheoptimizationmethodisstable).Thenetworkwiththe
smallestvalidationerrorisusedtomakepredictionsfortestcases.
Thisprocedureseemsratheradhoc.Theresultswillclearlydependon
theexactoptimizationprocedureemployed.Onlypartofthedataisused
forestimatingtheweights,whichseemswasteful.It'shardtoseehowthe
methodcanbejusti(cid:12)edwithinanygeneralframeworkforlearning.Onthe
otherhand,theprocedurerequiresonlyasingletrainingrun,andhencemay
befeasiblewhenothermethodsarenot.Earlystoppingisalsorelativelyeasy
toautomate.
Rasmussen(		)hasde(cid:12)nedanearlystoppingmethod(mlp-ese-)
thatusesanensembleofseveralnetworks.Theensembleisfoundbytraining

AssessingRelevanceDeterminationMethodsUsingDELVE

networkswithearlystoppingusingseveralrandomsplitsofthetrainingdata
intoestimationandvalidationsets.Predictionsfortestcasesarethenfound
byaveragingtheoutputsofallthenetworksintheensemble.Trainingan
ensembleobviouslyrequiresmoretimethantrainingasinglenetwork,but
ithastheadvantagethatallthedataislikelytobeusedforestimatingthe
weights,inatleastsomeofthenetworks.Predictionsusinganensembleare
infactguaranteedtobebetterthanpredictionsfromarandomlyselected
networkintheensemble(Perrone		),thoughthereisnoguaranteethat
usingalltheavailabletimetocontinuetrainingasinglenetworkwouldnot
havebeenbetter.
Ihavede(cid:12)nedasimilarbutsomewhatsimplermethod,mlp-bgd-.This
methodrandomlypartitionsthetrainingdataintofourequalportionsand
thentrainsanensembleoffournetworksbyearlystopping.Eachtraining
runusesthreeofthefourportionsastheestimationsetandtheremaining
portionasthevalidationset.Incontrast,mlp-ese-usesanensembleof
variablesize,withthedatabeingsplitrandomlyforeachmemberofthe
ensemble.Themlp-bgd-methodminimizestheerrorontheestimationset
using,epochsofsimplebatchgradientdescent.Itisthereforenotan
\earlystopping"methodintheliteralsense,sinceitcontinuestrainingfor
thisnumberofiterationsregardlessofwhatishappeningtotheerroronthe
validationset.Incontrast,mlp-ese-usesavariablenumberofiterations
oftheconjugategradientmethod,thoughittoogenerallycontinuestraining
forawhilepastthepointwherevalidationerrorstartsincreasing,incaseit
shouldgodownagainlater.
Indetail,eachbatchgradientdescentiterationofmlp-bgd-changesthe
weightsinfollowingway(withbiasesbeinghandledsimilarly):
(cid:15)N+NXc=@Ec@wij
wij=wij(cid:0)
()
Here,wijistheweightontheconnectionfromunititounitj,Nisthe
numberofcasesintheestimationset,andEcishalfthesquarederroron
casec.Thestepsize,(cid:15),wassetto.,whichwasadequateforallthetests
done,butifinstabilityisobserved,itshouldbereducedasnecessary.(Note
thattheinputsandtargetsarenormalized,sothereisnogeneralneedto
change(cid:15)tomatchchangesinthescalingoftheinputsortargets.)
Networksaresavedafterofthe,batchgradientdescentiter-
ations.Theseiterationsarespacedapproximatelylogarithmically.Once
trainingis(cid:12)nished,thebestofthesavednetworksisselected,asjudged
bysquarederroronthevalidationset.
Unlikemlp-ese-,thenetworkstrainedbymlp-bgd-havenodirect
input-to-outputconnections.Intestsontoyproblems,includingsuchconnec-
tionschangedthebehaviourofearlystoppingsubstantially,andapparently
fortheworse.


RadfordM.Neal
.BayesianAutomaticRelevanceDetermination(ARD)
TheAutomaticRelevanceDetermination(ARD)method(MacKay		;
Neal		)usesanelaborationofthehierarchicalpriorusedinmlp-mc-(see
Section.).Ratherthanplacingtheweightsonallinput-to-hiddenconnec-
tionsinonegroup,controlledbyasinglehyperparameter,input-to-hidden
weightsaregroupedaccordingtowhichinputtheyareassociatedwith,and
aseparatehyperparameterisusedforeachsuchgroup.Thehyperparameter
foraninputdeterminesthestandarddeviationoftheweightsonconnections
fromthatinput,andrepresentstherelevanceofthatinputtothetaskof
predictingthetarget.Wehopethattherelevancehyperparametersforthe
lessrelevantinputswilltakeonsmallvalues,preventingtheseinputsfrom
havinganunduee(cid:11)ectonthepredictions.
Ihavede(cid:12)nedtwoARDmethods,mlp-mc-andmlp-mc-.Themlp-mc-
methodde(cid:12)nedbyRasmussen(		)alsousesARD.Theonlydi(cid:11)erence
betweenmlp-mc-andmlp-mc-isthatmlp-mc-hasdirectinput-output
connections(asdoesmlp-mc-),whereasmlp-mc-doesnot(likemlp-mc-).
Whenpresent,directinput-outputconnectionsareassociatedwithrelevance
hyperparametersthatareseparatefromthoseassociatedwithinput-to-hidden
connections.Ifoundinpreliminaryteststhatthepresenceofdirectinput-
outputconnectionssometimesslowedtheconvergenceoftheMarkovchain.
Whichmodelisbetterwithunlimitedcomputationtimeshoulddependon
whetherinputsthatarerelevanttothelinearcomponentoftherelationship
ofinputstotargetaregenerallyalsorelevanttothenon-linearcomponentof
thisrelationship.
ARDcanbeunderstoodandjusti(cid:12)edinhigh-levelterms,asthecorrect
Bayesianprocedurewhenonehaspriorinformationthatsomeinputsare
likelytobemorerelevantthanothers.However,onecanalsolookatthe
mechanicsofhowaBayesianprocedureusingARDoperates,asawayof
gaininginsightsthatmightbeappliedinothercontexts.ARDcanbeimple-
mentedeitherbyMonteCarlomethods(Neal		),orbyusingGaussian
approximations(MacKay		,		).Althoughtheseimplementationsdi(cid:11)er
substantially,theycanbothbeviewedasoperatinginroughlythefollowing
fashion,ifweassumethatboththeweightsandtherelevancehyperparame-
tersforallinputsstartoutsmall:
)Thedataforcessomeinput-outputweightstotakeonlargervalues,in
ordertobetterpredictthetargets,overcomingthetendencyofthesmall
relevancehyperparameterstokeeptheseweightssmall.
)Therelevancehyperparametersassociatedwithinputsforwhichsome
input-outputweightsarenowlargeralsobecomelarger,sincetheymust
re(cid:13)ectthedistributionoftheweightstheycontrol.
)Otherweightsfromtheinputswithlargerrelevancehyperparametersare
nowallowedtogetbigger.

AssessingRelevanceDeterminationMethodsUsingDELVE

Thisisapositivefeedbackmechanism,inwhichsomelargeweightsoutofan
inputencourageallweightsfromthatinputtobelarger,includingweights
thatwerenotinitiallyforcedtobelargebythedata.Thispositivefeedbackis
themechanisticexpressionofapriorbeliefthatifaninputisrelevantinone
way(ascapturedbyoneweightfromit),itislikelytoberelevantinother
waysaswell(ascapturedbyotherweightsfromit).Wecantrytoadapt
ARDtoothercontextsbybuildingsimilarpositivefeedbackmechanisms.
.AnearlystoppingmethodinspiredbyARD
Themlp-bgd-methodisanattempttobuildanARD-likefeedbackmecha-
nismintothebatchgradientdescentprocedurewithearlystopping.Itoper-
atesinthesamewayasmlp-bgd-(seeSection.),exceptthatthestepsizes
fortheweightsoninput-outputconnectionsvaryaccordingtothetotalgradi-
entonconnectionsfromthatinput,mimickingtosomeextentthemechanics
ofARD.
Indetail,abatchgradientdescentiterationformlp-bgd-changesthe
weightsasfollows:
wij=wij(cid:0)(cid:15)riN+NXc=@Ec@wij
()
Thisisthesameasequation()exceptthatthestepsizeforweightwijis
potentiallyalteredbyafactorri.Forthebiasesandthehidden-to-output
weights,riisalwaysone.Stepsizesforinput-to-outputweightsareadjusted
byfactorscomputedasfollows:
ri=Gi(cid:14)maxiGi
()
Here,Giisthemagnitudeoftheerrorgradientwithrespecttoweightsout
ofinputi:Gi=Xj(cid:18)@Ec@wij(cid:19)
()
Thee(cid:11)ectofthismodi(cid:12)cationisthattheweightsoutoftheinputthatis
mostimportant(atthepresentstageoflearning)areupdatedwiththesame
stepsizeasinmlp-bgd-,butweightsoutofinputsthatarelessimportantare
changedbysmalleramounts.Wehopethatthiswillallowtherelationshipof
thetargettothemorerelevantinputstobediscoveredbeforemuchover(cid:12)tting
tothelessrelevantinputshasoccurred.Thedetailsofthismethod,such
astheuseofthefourthpowerofthegradientmagnitude,werearrivedat
empirically,basedontestsontoydatasets,andasmallamountoftesting
ondatasetsinthekinfamily,whichDELVEdesignatesas\development"
datasets,onwhichsuchpreliminarytestsareallowed.


RadfordM.Neal
.Asimplervariationonearlystopping
Themlp-bgd-methodde(cid:12)nedabovecontinuallychangesthestepsizesfor
weightsoutofdi(cid:11)erentinputsduringthecourseoflearning.Onecouldinstead
tryto(cid:12)ndasimplerstaticscheme,inwhichthestepsizesare(cid:12)xedatthe
beginning,basedonsimplestatisticsthatcanbecomputedfromthedata.
Onewouldcertainlywanttoknowifsuchaschemedoesaswellasorbetter
thanthemoreelaborateschemeinspiredbyARD.
Themlp-bgd-methodusessuchastaticscheme.Itisthesameas
mlp-bgd-exceptthatthestepsizeforupdatesofweightsoutofinputi
isadjustedbythefollowingfactor:
ri=maxh(cid:0);Ci(cid:14)maxiCii
()
Ciiscomputedfromtheinputs,x(c)i
,andtargets,t(c),inthecasesinthe
estimationset,asfollows:
Ci=NXc=x(c)i
t(c)
(	)
Sincetheinputsandtargetsarenormalizedtohavemediansofzeroand
averageabsolutedeviationsfromthemedianofone,Ciwillbeapproximately
Ntimesthecorrelationofinputiwiththetarget.Whentheweightsareall
zero,Ciisalsotheerrorgradientwithrespecttounitiofalinearregression
modelforthetarget.Atthebeginningoftraining,whenalltheweightsare
small,itwillthusbeanalogoustoGiofSection..
Theactualimplementationofthismethodusesthesamestepsizeforall
weights,butscalesinputi(aftertheusualnormalization)bythefactorpri.
Thishasthesamee(cid:11)ectasusingastepsizeproportionaltori,sincethe
weightsoutofinputimustnowbelargerbyafactorof=pritoachieve
thesamee(cid:11)ectaswithoutscaling,whilethegradientissmallerbythesame
factor.
Themainexperimentsandtheirresults
Theprevioussectionde(cid:12)nedBayesianMonteCarloandearlystoppingensem-
blemethods,withandwithouttechniquesforadaptingtovaryingrelevance
ofinputs.Thisgivesfourbasiccombinations,butwithfurthervariations,a
totalofsixnewmethodswerede(cid:12)ned.ThesearesummarizedinFigure,
alongwithtworelatedmethodsassessedbyRasmussen(		).Inthissec-
tion,Idescribehowtheperformanceofthesemethodswasassessedina
systematicmannerusingDELVE.

AssessingRelevanceDeterminationMethodsUsingDELVE

Earlystoppingensemblemethods
mlp-ese-Rasmussen'smethod:directI-Oconnections,noARD
mlp-bgd-Newmethod:nodirectI-Oconnections,noARD
mlp-bgd-Newmethod:nodirectI-Oconnections,ARD
mlp-bgd-Newmethod:nodirectI-Oconnections,staticARDscheme
BayesianMonteCarlomethods
Rasmussen'smethod:directI-Oconnections,ARD,longtime
mlp-mc-
Newmethod:directI-Oconnections,ARD
mlp-mc-
mlp-mc-
Newmethod:nodirectI-Oconnections,ARD
mlp-mc-
Newmethod:nodirectI-Oconnections,noARD
Fig..Summaryofthesixnewmethods,alongwithRasmussen'stwomethods.
.Questionstobeaddressed
Theprimarymotivationfortheassessmentsdescribedhereistoanswerthe
followingquestions:
)DoesARDactuallyimprovetheperformanceoftheBayesianMonteCarlo
method?Ifso,byhowmuch?
)DoesthevariationonearlystoppinginspiredbyARDimproveperfor-
mance?Ifso,byhowmuch?
)IstheperformanceofearlystoppingwithanARD-likeprocedurebetter
orworsethantheBayesianARDmethod?
Experimentsrelevanttoquestion()havepreviouslybeendonebyVivarelli
andWilliams(		).TheirresultssuggestthatARDwasbene(cid:12)cial,butthe
di(cid:11)erencesobservedlackedstatisticalsigni(cid:12)cance,andwereforasingletask.
Whileinvestigatingtheperformanceoftherelevancedeterminationmeth-
ods,wewillalsoinevitablyobtainfurtherinformationaboutwhetherthe
BayesianMonteCarlomethodisbetterthanearlystoppingensembles,a
questionpreviouslyaddressedbyRasmussen(		).Oneshouldnotethat
therelativeperformanceofthesetechniquesmaywelldependontheamount
ofcomputationtimetheyareallowed.Thesixnewmethodsarede(cid:12)nedtoall
takeroughlythesameamountoftime,whichismuchlessthanthatallowed
fortheBayesianMonteCarlomethodde(cid:12)nedbyRasmussen(mlp-mc-).
Rasmussenfoundthatmlp-mc-wasbetteroverallthanhisearlystopping
ensemblemethod(mlp-ese-),evenwhenitwasrestrictedtousingthesame
amountoftime.However,sincetheearlystoppingmethodsde(cid:12)nedinthis
paperdi(cid:11)erinimportantwaysfrommlp-ese-,itispossiblethatthecom-
parisonwiththeBayesianMonteCarlomethodsmightturnoutdi(cid:11)erently.


RadfordM.Neal
.Tasksonwhichthemethodsweretried
DELVEcontainsseveralfamiliesofdatasetsintendedforuseinsystematic
experiments.Dataineachfamilyisgeneratedusingarealisticsimulation
program.Inthepresentfamilies,asingleprototaskisde(cid:12)nedforeachdataset,
inwhichatargetattributeistobepredictedfromtheotherinputattributes.
Thetargetattributewasalwaysreal-valuedfortheseexperiments,producing
aregressiontask.Datasetswithinafamilyaredistinguishedbythenumber
ofinputs,bytheamountofnoiseintherelationshipofthetargettothe
inputs,andbythedegreetowhichthisrelationshipisnon-linear.Foreach
prototask,(cid:12)vetasksarede(cid:12)ned,inwhichthenumberofcasesinthetraining
is,,,,or.
Thesefamiliesallowassessmentsofhowtheperformanceofamethod
varieswiththecharacteristicsofthedataset.Runningmethodsonallmem-
bersofafamilycantakealongtime,however.Fortheassessmentsofthe
relevancedeterminationmethods,Iusedonlythedatasetsforwhichthere-
lationshipwasnon-linear,withamoderateamountofnoise.However,Idid
runthemethodsonboththedatasetswithinputsandthosewithinputs,
sincethisdimensionofvariationisclearlyimportantinassessingmethodsfor
determininginputrelevance.Thesetwomembersofafamilyareidenti(cid:12)ed
bythesu(cid:14)xes\nm"and\nm".
Threefamiliesofdatasetswereusedintheexperimentsreportedhere:
kin
Thisdatacomesfromasimulationofthekinematicsofarobot
arm.Thetargetattributeisthedistanceoftheendofthearm
fromatarget;theinputsarevariousjointpositions,angles,etc.
ThisfamilywascreatedbyZ.Ghahramani.
pumadynThisdatacomesfromasimulationofthedynamicsofthePuma
robotarm.Thetargetattributeistheangularaccelerationof
oneofthelinks;theinputsarevariousangularpositions,velocities,
torques,etc.ThisfamilywascreatedbyZ.Ghahramani.
Thisdatacomesfromasimulationofcustomersbeingservedby
bank
banks.Thetargetattributeistheproportionofcustomerswho
goawaybecauseallthequeuesinthebankarefull;theinputs
arevariousparametersofthesimulation,suchasthemaximum
lengthsofthequeues,andthesizesofthepopulationindi(cid:11)erent
areas.ThisfamilywascreatedbyC.E.Rasmussen.
ThekinandpumadynfamilieswerealsousedbyRasmussen(		)toassess
mlp-mc-andmlp-ese-.TheresultsoftheseassessmentsareintheDELVE
archive,allowingcomparisonofthesemethodswiththenewmethods.
.Runningthemethodsonthetaskinstances
Inthemainseriesofexperiments,thesixmethodswereeachrunoninstances
oftasks:families,datasetsperfamily(withandinputs),and

AssessingRelevanceDeterminationMethodsUsingDELVE

tasksperprototask(withvaryingtrainingsetsizes).Performanceoneach
taskwasassessedusinginstances,eachofwhichconsistedofatrainingset
andasetoftestcases,exceptthatonlyinstanceswereusedforthetasks
withtrainingcases.Thetotalcomputationtimeusedinrunningoneof
themethodsonallthesetaskinstanceswasapproximatelyhours.
Beforeamethodisrun,theDELVEsoftwareisusedtoproducedata
(cid:12)lesforeachinstance,inaccordancewiththeencodingandnormalization
optionsspeci(cid:12)edforthemethod.Whenthetargetsarenormalized(asthey
werehere),DELVEalsotranslatestheguessesthemethodmakesbacktothe
originalformat.DELVEconventionsfordirectoryhierarchiesand(cid:12)lenames
makeiteasiertokeepstraightwhatishappeningduringthisprocess.
Theserunsproducequiteabitofdata.Ataminimum,aguessforthetar-
getineachtestcaseofeachinstancemustbesaved.Manymethodsproduce
morethanoneguess,intendedforusewhendi(cid:11)erentlossfunctionsareused
(thisisthecaseforthemlp-mcfamily,thoughnotformlp-bgdfamily).The
lossesthatresultfromusingtheseguessesareusuallysavedaswell,sothat
theywillnothavetoberecomputedwheneverthemethodiscomparedwith
another.Somemethodsalsoproduce(cid:12)lesrecordingwhathappenedduring
learningforeachinstance|eg,themlp-bgdmethodsrecordwhichitera-
tionwasselectedforusebasedonvalidationerror.These(cid:12)nalresultscanbe
submittedtotheDELVEarchive.
.Presentationandinterpretationoftheresults
Onceamethodhasbeenrunontheinstancesofatask,theDELVEsoftware
canbeusedtoestimatetheexpectedsquared-errorlossofthemethodon
thattask,asdescribedinSection..Anestimateofthestandarderrorof
thisestimateisobtainedaswell.
Theestimatedexpectedlossisprintedbothin\raw"form,corresponding
totheoriginalscalingofthetargets,andin\standardized"form.Forsquared-
errorloss,lossesarestandardizedbydividingbythevarianceofthetargets
inthecompletesetoftestcasesusedforallinstancesofthetask.Amethod
thatjustguessesthemeanofthetargetsinthetrainingsetwouldtherefore
beexpectedtohaveastandardizedexpectedlossofclosetoone.Notethat
standardizationoflossesispurelyaconventionforpresentingresultsinamore
meaningfulform.Ithasnothingtodowiththenormalizationofattributes
thatmay(ormaynot)bepartofthede(cid:12)nitionofamethod.
Figures,,andshowthestandardizedsquared-errorlossesthatthe
methodsachievedontasksfromthethreefamilies.Thenameofthedataset,
followedbythenameofthe(only)prototask,areshownatthetopofeach
plot.Eachplotshowsresultsforthesixnewmethods,alongwithmlp-ese-
andmlp-ese-forthekinandpumadynfamilies.Forthebankfamily,these
oldermethodshavenotbeenrun,sotheresultsofasimplelinearregression
method,lin-,areincludedforcomparisoninstead.


RadfordM.Neal
These(cid:12)gureshaveaformatduetoRasmussen(		).Theresultsforone
taskarecontainedinaverticalrectanglelabeledatthetopwiththesize
ofthetrainingsetforthattask.Withinthisrectangle,eachmethodhasa
column,withtheleft-to-rightorderofthemethodsbeingthesameasthetop-
to-bottomorderofthelistofmethodslowerinthe(cid:12)gure.Thehorizontalline
inthecolumnforamethodmarkstheestimatedexpectedlossofthatmethod
onthetask.Theverticallineextendsupanddownadistancecorresponding
toplusorminustheestimatedstandarderror.
Belowtherectangleforeachtaskisanarrayofp-valuesforpairwise
comparisonsoftheperformanceoftwomethodsonthistask,usingthepaired
t-testdescribedinSection..IftheestimatedexpectedlossofmethodAis
signi(cid:12)cantlylessthanthatformethodB,adigitgivingthesigni(cid:12)cancelevelis
placedinthecolumnformethodAandtherowformethodB.Thisdigitis
timesthep-valueforthiscomparison,roundedup(eg,thedigitindicatesa
p-valuebetween.and.).Adotisshowninarowandcolumnposition
iftheobserveddi(cid:11)erencebetweenthemethodsisnotsigni(cid:12)cant(p-value
greaterthan.	),orifthebettermethodistheoneassociatedwiththe
row,ratherthanthecolumn.
Whenexaminingthese(cid:12)gures,theplotsoftheestimatedexpectedlosses
forthevariousmethodscanbeviewed(cid:12)rst,inordertogetageneralimpres-
sionoftheresults.Onecanthencheckwhetherthoseobserveddi(cid:11)erences
thatseemofinterestarestatisticallysigni(cid:12)cantbylookingatthep-value
fortherelevantcomparison.Ifthisissmall,onecanbereasonablycon(cid:12)dent
thattheapparentdi(cid:11)erenceinperformanceofthetwomethodsre(cid:13)ectsat
leastthesignoftheactualdi(cid:11)erence.Ontheotherhand,ifthep-valuefor
acomparisonisnotsmall,theapparentlybettermethodmightactuallybe
worse.Anotherwayofviewingtheresultsistolookacrosstherowofp-values
foramethod.Adigitwillappearinthisrowifsomeothermethodwas
observedtobebetter,withthedi(cid:11)erencebeingsigni(cid:12)cantattheindicated
level.Similarly,lookingdownthecolumnofp-valuesforamethodwillreveal
whichmethodsithasbeenshowntobebetterthan.
Preliminaryconclusionsandfurtherexperiments
Theresultsofthemainexperimentscanbeinterpretedtogivesomeprelim-
inaryanswerstothequestionsaskedinSection..Asistypical,however,
reallyunderstandingtheresultsrequiresfurtherinvestigation,includingsome
furtherexperiments.
.Summaryofresultsofthemainexperiments
Iwillstartbysummarizingwhatonecanconcludefromtheresultsofthe
mainexperimentsonthethreefamiliesoftasks.

AssessingRelevanceDeterminationMethodsUsingDELVE



kin-8nm/dist

64

128

256

512

1024




1

1
1


2


1
4
1
1





5

7















2

3
3














8








6

6






1

7






1

5






1

4
7














1

7
5




















2
2
9
1

1
3





1








1








1

















1








2





1
1
1
1





1
1
2
1






4
2
1








1








1














1
1
1
1

5



1
1
1
1





1
1
1
1





1
1
1
1







3
1








1








1















3

1






1
6
1






3
4
1






3

1







64







6



































2








9











128

256

512

kin-32nm/dist

Fig..Resultsonthekinfamilyoftasks.
















2








6

















3




9
2












































4















7
2
2

7
6




















9


3





























6








9











1
1
1
1



6

1024

1
1
2
1





1
1
1
1



5

2
3
1
1








5















5

8














1
1
2
1



3

1
1
2
1



5

1
1
2
1



5

1
1
3
1





0.70

0.60

0.50

0.40

0.30

0.20

0.10

0.00
mlp-ese-1
mlp-bgd-1
mlp-bgd-2
mlp-bgd-3
mlp-mc-1
mlp-mc-2
mlp-mc-3
mlp-mc-4

1.00
0.90
0.80
0.70
0.60
0.50
0.40
0.30
0.20
0.10
0.00
mlp-ese-1
mlp-bgd-1
mlp-bgd-2
mlp-bgd-3
mlp-mc-1
mlp-mc-2
mlp-mc-3
mlp-mc-4



RadfordM.Neal

0.40

0.30

0.20

0.10

0.00
mlp-ese-1
mlp-bgd-1
mlp-bgd-2
mlp-bgd-3
mlp-mc-1
mlp-mc-2
mlp-mc-3
mlp-mc-4

1.00
0.90
0.80
0.70
0.60
0.50
0.40
0.30
0.20
0.10
0.00
mlp-ese-1
mlp-bgd-1
mlp-bgd-2
mlp-bgd-3
mlp-mc-1
mlp-mc-2
mlp-mc-3
mlp-mc-4




















2





1










7
6





6







2


pumadyn-8nm/accel

64

128

256

512

1024










1








1
2







1
1
1


4

2

1
1
1
6

4

2










1
1
1
7

3

2



















3








1
1







1
1
1




1

1
1
1




3

1
1
1




1

1
1
1


9

1

1
1
4
























2
1







1
1
1




2

1
1
1




1










1
1
1
8



1

1
1
1







4

















1







1
1
1






1
1
1
9



2










1
1
1
8



4

1
1
1

























3







4
8







1
2
2
3



1

2
1
1
9





2
7







2
4
4






64

128

256

512

pumadyn-32nm/accel

Fig..Resultsonthepumadynfamilyoftasks.










1
1
5
1

2

1

6
5





4

1
1
2
1

1

1



















1


1





1
1

1

6

1

1








1
4

1

8

1

4


7





1
1

1



1













1





1


1





1
1

1

7












1


1








4





1
1

1



1




























1
1





1

1


1





1024

1


1








1





3


1





2
1

1














1
3

1








6





1
1
1
1



1

1
1
1
1





AssessingRelevanceDeterminationMethodsUsingDELVE



bank-8nm/rej

64

128

256

512

1024

0.40

0.30

0.20

0.10

0.00
lin-1
mlp-bgd-1
mlp-bgd-2
mlp-bgd-3
mlp-mc-2
mlp-mc-3
mlp-mc-4


1














8
1


7




1






































3







1
4

4


9

5























5















1


4

5


1
2

3

3
5

7







3




3
9

























1


1


1

1
9

1

4
1

1







1


6




1


3


6

1















1


3




1


3




1







1


3




1







1


7




64

0.50

0.40

0.30

0.20

0.10

0.00
lin-1
mlp-bgd-1
mlp-bgd-2
mlp-bgd-3
mlp-mc-2
mlp-mc-3
mlp-mc-4









1







1
1






1
1




1

1
1




1

128

256

512

bank-32nm/rej

Fig..Resultsonthebankfamilyoftasks.









1







1
2






1







1
1



1
6

2




8


1
6



3


























3







1
2



































6
4

1



3
2
2









1
1


1
1
1

1



1
1
1






8
3

1024

























1





6

1
2

3


2

2





2







2


















RadfordM.Neal
Resultsforthekinfamily(Figure)aregenerallyfavourableforthe
BayesianMonteCarlomethods,particularlyonthetaskswithlargertrain-
ingsets.Thisisveryclearforthetaskwith-inputsandtrainingcases
andthetaskswith-inputsandortrainingcases,onwhichall
theBayesianMonteCarlomethodsarebetterthanalltheearlystopping
methods,withthep-valuesforthesecomparisonsbeing.orlower.The
magnitudeoftheadvantageisnothuge,butislargeenoughthatitwould
oftenbepracticallyimportant.Forsmallertrainingsetsizes,theBayesian
MonteCarlomethodsarenotconsistentlybetter.Forsometasks,itseems
thattheMarkovchainusedforsamplingmaynothavecomeclosetocon-
verging.Thiswouldexplainthefairlylargedi(cid:11)erencesamongtheBayesian
MonteCarlomethodsforthe-inputtaskswithortrainingcases|
inparticular,sincethemodelsformlp-mc-andmlp-mc-arequitesimilar,
itismostplausiblethatthedi(cid:11)erencesbetweenthemareduetothelonger
computationtimeallowedformlp-mc-.
Amongtheearlystoppingensemblemethods,themoststrikingresultis
thatmlp-bgd-,whichusesthesimplestaticapproachtorelevancedetermi-
nation,doesverypoorlyonthe-inputtasks.Italsoappearsworsethanthe
otherearlystoppingensemblemethodsonthe-inputtask,thoughthedif-
ferencethereislessdramatic.Nolargedi(cid:11)erencesareseenamongmlp-ese-,
mlp-bgd-,andmlp-bgd-.Thereissomeindicationthatmlp-bgd-,which
usestherelevancedeterminationtechnique,isabitbetterthanmlp-bgd-,
whichdoesnot,butissimilarotherwise.However,whenjudgedonatask-by-
taskbasis,theobservedadvantageformlp-bgd-isstatisticallysigni(cid:12)cant
onlyforthe-inputtaskwithtrainingcases.
Di(cid:11)erencesamongtheBayesianMonteCarlomethodsaresmall,except
forthetwotaskswhereitseemsthatconvergencewasaproblem.Forthe
taskswith-inputsandortrainingcases,asmallbutstatistically
signi(cid:12)cantdisadvantageisseenforthemethodwithoutARD(mlp-mc-).
Thisnon-ARDmethodperformedbetteronsomeofthetaskswithsmaller
trainingsets,butonemightsuspectthatthisisduetofasterconvergenceof
theMarkovchain,ratherthantheARDmodelbeingbad.
Somelargedi(cid:11)erencesinperformanceareseenonthepumadyndatasets
(Figure).Looking(cid:12)rstattherelativeperformanceofthefourearlystopping
ensemblemethods,weseethatmlp-bgd-doesverywellonthe-inputtasks,
particularlywhenthetrainingsetissmall,butitdoesverybadlyonthe-
inputtasks.Themlp-ese-methodalsodoesmuchworsethanmlp-bgd-
andmlp-bgd-onsometasks,anditneverdoesmuchbetter.Thereisalso
aconsistentadvantageofmlp-bgd-(withrelevancedetermination)over
mlp-bgd-.Forthetaskswith-inputsandorinputs,thisadvantage
isquitelarge.
TheresultsoftheBayesianMonteCarlomethodsonthepumadyntasks
areconfusing.Somelargedi(cid:11)erencesareseenamongthesemethods,which
areverylikelytheresultoftheMarkovchainbeingfarfromconvergence

AssessingRelevanceDeterminationMethodsUsingDELVE

forsomemethodsonsometaskinstances.Largestandarderrorsarealso
seen,whichareprobablytheresultoflargevariationinhowwelltheMarkov
chainconvergedondi(cid:11)erenttrainingsets(orwithdi(cid:11)erentrandomnumbe
seeds).Becauseoftheconvergenceproblems,whichareespeciallypronounced
forthe-inputtasks,detailedcomparisonsoftheBayesianMonteCarlo
methodsareprobablynotveryfruitful.Usefulcomparisonswiththeearly
stoppingensemblemethodsarealsodi(cid:14)cult.Whenconvergenceseemsto
beaproblem,theearlystoppingensemblemethods(especiallymlp-bgd-)
mayperformbetter,buttheBayesianMonteCarlomethodsseemtodo
betterwhenconvergenceispresumedtonotbeaproblem(eg,withmlp-mc-,
mlp-mc-,andmlp-mc-onthe-inputtaskwithtrainingcases).
Forthebanktasks(Figure),themlp-bgd-isconsistentlyabitbetter
thanmlp-bgd-,andbothareusuallymuchbetterthanbothmlp-ese-
andmlp-bgd-.TheBayesianMonteCarlomethodsdosurprisinglypoorly
here.Noneofthemareeversigni(cid:12)cantlybetterthanmlp-bgd-,andtheyare
sometimessubstantiallyworse.Onthe-inputtaskwithtrainingcases,
mlp-mc-andmlp-mc-areevenworsethanthesimplelinearregression
model(lin-).
Thispreliminarylookattheresultsneedstobesupplementedbyamore
detailedexaminationofthebehaviourofthemethods,sothatthesourceof
thedi(cid:11)erencesseencanbeidenti(cid:12)ed.
.Moreontheearlystoppingensemblemethods
Tobetterunderstandtheresultsfortheearlystoppingmethods,the(cid:12)rst
thingtolookatiswhatiterationwasselectedforuseonthebasisofthe
erroronthevalidationset.
Formlp-bgd-andmlp-bgd-,quiteearlyiterationsareoftenchosen.
Indeed,forseveraltaskinstances,thenetworkafteronlyoneiterationofbatch
gradientdescentwasused!Onlyrarelyisthenetworkchosenthelastone
found,afterthefull,iterations.Itthereforeseemsthatthesemethods
wouldnotbene(cid:12)tmuchfrombeingallowedmorecomputationtime.Thefact
thatveryearlynetworksaresometimeschosenmayexplainwhymlp-ese-
sometimesperformsworsethanmlp-bgd-,asitusesthemoresophisticated
conjugategradientoptimizationmethod,whichmaybe\toogood",moving
pasttheoptimalpointinthe(cid:12)rstiteration.However,itisalsopossiblethat
thepresenceofdirectinput-outputconnectionsinmlp-ese-isthesource
oftheperformancedi(cid:11)erence.
Thelastiterationwaschosenmuchmoreoftenbymlp-bgd-.Thismight
beexpectedwheneveritsstaticselectionofstepsizesfortheweightsoutof
di(cid:11)erentinputsturnsouttobeinappropriate.Althoughthemethodmight
thenbene(cid:12)tfrombeingallowedmoretime,theobjectiveofslowingdown
learningforthelessrelevantinputswillnothavebeenachievedinanyevent.
Finally,wemightwonderhowmuchtheuseofanensemblecontributes
totheperformanceoftheearlystoppingmethods.Thisisnotdirectlyrele-



0.90

0.80

0.70

0.60

0.50

0.40

0.30

RadfordM.Neal

kin-8nm/dist

64

128

256

512 1024

pumadyn-8nm/accel

64

128

256

512 1024

0.30

0.20

0.10


1




128

256


3





1





1





1





1





4





1





2





2




0.10

0.20

64


512 1024

512 1024

kin-32nm/dist

0.00
mlp-bgd-2
mlp-bgd-2b

pumadyn-32nm/accel
64

128

256

0.00
mlp-bgd-2
mlp-bgd-2b

1.00
0.90
0.80
0.70
0.60
0.50
0.40
0.30
0.20
0.10
0.00
mlp-bgd-2
mlp-bgd-2b

1.00
0.90
0.80
0.70
0.60
0.50
0.40
0.30
0.20
0.10
0.00
mlp-bgd-2
mlp-bgd-2b

Fig..Resultsusingearlystoppingwithandwithoutanensemble.
vanttothemainobjectiveofthisinvestigation,butitiseasilytodetermine,
byjustmakingpredictionsbasedonthe(cid:12)rstofthefournetworksalready
trainedinthemainexperiments.Figureshowstheresultsofacomparison
oftheperformanceofmlp-bgd-andamodi(cid:12)cationofitwithouttheensem-
ble(mlp-bgd-b).Ascanbeseen,theensembleoftenreducestheexpected
lossbyasubstantialamount,especiallywhenthetrainingsetissmall.Note
thatforthisquestion,thep-valuesforthecomparisonsareirrelevant.Theory
guaranteesthatusinganensembleisbetterthanusingjustonearbitrarily
selectednetworkfromtheensemble.Theonlyquestioniswhetherthead-
vantageoftheensembleisbigenoughtojustifythecomputationalcostof
trainingfournetworksratherthanone.


4





1





5





3

















1











1










AssessingRelevanceDeterminationMethodsUsingDELVE

.MoreontheBayesianMonteCarlomethods
TwoissuesregardingtheBayesianMonteCarlomethodsneedtoberesolved:
Arethevariableresultsseenonthepumadyntasksinfactduetothemethods
notalwayshavingsu(cid:14)cienttimetoapproachconvergence?Andwhatisthe
reasonforthepoorperformanceonthebanktasks?Onepossibilityisthat
thispoorperformancealsoresultsfromthemethodsnothavingenoughtime
toconverge.
Toinvestigatethesequestions,Ididfurtherrunsonthepumadyn-nm
andbank-nmtasks,usingvariationsontheBayesianMonteCarlomethods
calledmlp-mc-b,mlp-mc-b,andmlp-mc-b.Thesemethodswereidentical
tomlp-mc-,mlp-mc-,andmlp-mc-exceptthattheywereallowedthree
timesasmuchcomputationtime.TheresultsareshowninFigure.
MoretimedidindeedimprovetheperformanceoftheBayesianMonte
Carlomethodsonsomeofthepumadyn-nmtasks.Withtheextratime,
mlp-mc-bisverymuchbetterthanthebestearlystoppingensemblemethod
(mlp-bgd-)foralltrainingsetsizes,exceptperhapsfortrainingsetsofsize
,forwhichthep-valueofthecomparisonis..(SeeFiguresand.)
Theresultsformlp-mc-barestillanomalous,asthemodelusedbythis
methodisverysimilartothatofmlp-mc-,butitsperformanceissometimes
muchworse.Onthetaskwithtrainingcases,itspoorerperformance
isduetobadresultsononeofthefourinstances.Whentherunonthis
instanceisallowedevenmoretime,theMarkovchainmovestoastatewith
muchdi(cid:11)erenthyperparametervalues,similartothosefoundwiththeother
instances.Itthereforeseemsthattheperformanceofmlp-mc-bisstilllim-
itedbycomputationtime.
Withorwithoutextratime,theuseofARDappearsbene(cid:12)cialonthe
pumadyn-nmtasks,asseenbycomparingmlp-mc-ormlp-mc-bwith
mlp-mc-ormlp-mc-b.However,onemightstillwonderwhetheratleast
partoftheverylargebene(cid:12)tofARDseenforthetaskswithandtrain-
ingcasesmightbeduetopoorerconvergencewhenARDisnotused,rather
thantotheARDmodelbeingbetter.Examinationofthehyperparameter
valuesduringindividualrunsofmlp-mc-bsuggeststhatpoorconvergence
maystillbeaproblemforthismethod.
TheresultsofallowingtheBayesianMonteCarlomethodsmoretimefor
thebank-nmtasksarerathersurprising|withmoretime,theperformance
isworse,especiallyonthetaskswithlargertrainingsets.Examiningthese
runsindetailshowsthattheMarkovchainseventuallyconvergetostatesin
whichthenoiselevelisquitesmall.
Recallthatthetargetforthebanktasksisaproportion,whichwillbebe-
tweenand.Thedistributionofthistargetforbank-nmisquiteskewed,
withmanyvaluesatornearzero,butafewthataremuchlarger(upto
about.).Itseemsquitelikelythattheconditionaldistributionsforthe
targetgivenparticularvaluesfortheinputsarealsoskewed,andfurther-
morehaveavariancethatvariesdependingontheinputs(ie,thedatais



RadfordM.Neal

1.00
0.90
0.80
0.70
0.60
0.50
0.40
0.30
0.20
0.10
0.00

pumadyn-32nm/accel

128

256

512

1024

64



mlp-mc-1
mlp-mc-2
mlp-mc-2b
mlp-mc-3
mlp-mc-3b
mlp-mc-4
mlp-mc-4b






6
5




2









6
6














9
8


















2
2


1
1






4
6






4
5


1
1


1
1


1
1


1
1














































1
1




2

1
1














5



8
8


1
1






















1
1


2
2


1
1


















































4






















256

512

64

128

0.20

0.30

0.40

1024

bank-8nm/rej

Fig..ResultswhentheBayesianMonteCarlomethodsareallowedmoretime.

0.00
mlp-mc-2
mlp-mc-2b
mlp-mc-3
mlp-mc-3b
mlp-mc-4
mlp-mc-4b

0.10




































2
3
9
6














































1
3
8
9
6





































1

5

7









9

2
6
3

























5






5

3


















2

3








AssessingRelevanceDeterminationMethodsUsingDELVE

\heteroskedastic").Incontrast,themodelsusedbyalltheBayesianMonte
CarlomethodstestedassumethatthenoiseisGaussian,andthatitsvariance
doesnotvarywiththeinputs.Thismismatchbetweenmodelandrealityis
likelytheproblem.Themodelthinksthenoisevarianceissmallbecausein
someregionsoftheinputspacethetargetscanindeedbepredictedquite
accurately.Thissmallnoisevariancethenproducespoorresultsintheparts
oftheinputspacewheretherealnoisevarianceislarge.
Whatwaslearnedaboutthemethods
Inlightoftheseexperimentalresults,whatcanwesayconcerningtheques-
tionsposedinSection.?
TheBayesianMonteCarlomethodusingARD(mlp-mc-)generallyper-
formedabitbetterthanthecorrespondingmethodwithoutARD(mlp-mc-).
Forafewtasks,theadvantageofARDwasdramatic(pumadyn-nmwith
trainingcases,pumadyn-nmwithandtrainingcases).Moreoften,
however,ARDperformedonlyslightlybetter,andinsomecasesnosignif-
icantdi(cid:11)erencewasobserved.Asigni(cid:12)cantdisadvantageformlp-mc-was
seenforthekin-nmandkin-nmtaskswithtrainingcases.Sincethis
disadvantagewasnotsharedwiththeothertwoARDprocedures(mlp-mc-
andmlp-mc-),itwasprobablyduetolackofconvergencewithintheallowed
time.Althoughthebene(cid:12)tofARDintermsofpredictiveperformanceisoften
modest,identifyingthelessrelevantinputsmayalsohelpininterpretingthe
resultsoftraining.Interpretabilityisimportantinmanyapplications,butis
notaddressedbyDELVEassessments.
Anotherconclusionfromtheseexperimentsisthatobtainingreliablere-
sultsusingBayesianMonteCarlomethodsrequiressomewhatmorecompu-
tationtimethanwasallowedforthemethodsassessedhere.Evenincreasing
thetimeallowedbyafactorofthreewasnotcompletelysatisfactory.Never-
theless,evenwiththetimerestrictionusedhere,theBayesianMonteCarlo
methodssometimesperformedbetterthantheearlystoppingmethods.
ThevariationonearlystoppinginspiredbyARD(mlp-bgd-)appears
tobeverysuccessful.Itwassometimessubstantiallybetterthanthecorre-
spondingprocedurewithoutrelevancedetermination(mlp-bgd-),andwas
neverseentobesigni(cid:12)cantlyworse.Thesimplerstaticmethod(mlp-bgd-)
occasionallyperformedwell,butmoreoftenitperformedverybadly.
IncontrastwiththeBayesianMonteCarlomethods,theearlystopping
methodsmlp-bgd-andmlp-bgd-appeartohavereachedmostoftheir
potentialwithinthecomputationtimeallowedhere.Theadvantageofus-
inganensembleoffournetworksinthesemethodswasfairlysubstantial,
andprobablyworthwhileinmostapplications,giventhattrainingtimewas
nottoolarge.Thesemethodsgenerallyperformedaswellasorbetterthan
Rasmussen's(		)mlp-ese-method,eventhoughitusesamoresophis-


RadfordM.Neal
ticatedoptimizationmethodandamorecomplexheuristicforbuildingan
ensemble.Theconjugategradientoptimizationmethodusedbymlp-ese-
mayactuallybetoogood,sometimesmovingpasttheoptimumpointinthe
(cid:12)rstiteration.
AnuglyaspectoftheARD-likemodi(cid:12)cationofearlystoppingisthat,
apartfromtheinspirationfromBayesianARD,itisratherarbitraryand
adhoc.Thismaybeageneralcharacteristicofearlystoppingmethods.Cer-
tainlythesuccessoftheARD-likemodi(cid:12)cationisafurtherdemonstrationof
howsensitiveearlystoppingistotheexactmethodusedforoptimization.
ThecomparisonofBayesianARDwithearlystoppingusingARD-like
relevancedeterminationiscomplicatedbytheapparentlackofconvergence
ofsomerunsoftheBayesianARDmethodsandbytheunusualbehaviour
oftheBayesianmethodsonthebanktasks.ReliableresultsusingBayesian
MonteCarlomethodscanbeexpectedonlyiftwoconditionsaresatis(cid:12)ed:
)Thereissu(cid:14)cienttimefortheMarkovchainsamplertoconvergetoa
goodapproximationtotheposteriordistribution.
)Theprobabilisticmodelusedisafairlycloseapproximationtoreality.
Sometimes,aBayesianMonteCarlomethodmayproducebetterresultsthan
othermethodseventhoughithasnotcomeclosetoconverging,butrelyingon
thisisnotadvisableifallowingmorecomputationtimeisanoption.Models
seldomcorrespondexactlytotherealsituation,andwemayhopethatminor
(cid:13)awswillnothavedrastice(cid:11)ects,butthereisnoreasontoexpectgood
resultsfromaBayesianmodelwithserious(cid:13)awsinthespeci(cid:12)cationofeither
thelikelihoodortheprior.
Whentheabovetwoconditionsaremet,BayesianMonteCarlousing
ARDappearstoperformbetterthanearlystoppingensembles,evenwith
theARD-likemodi(cid:12)cation.However,theresultsonthebanktasksshowthat
earlystoppingensemblescanperformbetterwhentheBayesianmodelisnot
appropriateforthesituation.
Whenmanualinterventionispossible,oneshouldofcoursecheckforma-
jordeparturesfrommodelassumptions,andmodifythemodelifnecessary.
Forthebanktasks,itmightbeappropriatetoapplysomenon-lineartrans-
formationtothetargetinordertobettermatchthemodel'sassumptionof
Gaussiannoisewithconstantvariance.Itwouldbeinterestingtocompare
thevariousmethodsonavariationofthebanktaskswithsuchatransformed
target,orinwhichthemethodswereprovidedwithpriorinformationthat
suchatransformationmightbeuseful(butstillmadepredictionsforthe
originaltarget).
Finally,oneshouldnotethatalltheconclusionsinthisstudyarebasedon
onlythreefamiliesofdatasets,forallofwhichtheinputattributesrepresent
diversequantities.Itistobeexpectedthatsomeoftheseheterogeneous
inputswillturnouttobemorerelevantthanothers.Ontheotherhand,
taskscertainlyexistforwhichanysortofrelevancedeterminationmethod

AssessingRelevanceDeterminationMethodsUsingDELVE

willbedisadvantageous.Forexample,iftheinputsarepixelsofanimage,
andthetaskisknowntobeinvariantwithrespecttotranslationoftheimage,
allpixelsmustbeequallyrelevant.
WhatwaslearnedaboutDELVE
IhopetheexperimentsreportedherehaveservedtoillustratehowDELVE
canbeusedtoassesstheperformanceoflearningmethods.Majorassessments
usingDELVEarealsodescribedbyRasmussen(		)andbyWaterhouse
(		).Thegoalofsuchassessmentsisnotsimplytoobtainatableofnum-
bers,buttogaininsightintothemethodsassessed.IntheStatLogproject
(Mitchie,etal		)manymethodswereassessedonmanydatasets,andan
attemptwasthenmadetoobtaininsightbyexploratorymethodssuchas
multidimensionalscalingandhierarchicalclustering,withoutanyattemptto
assessstatisticalsigni(cid:12)cance.Inmyopinion,itismoreusefultoaddressex-
plicitquestions,andindoingso,itisimportanttoobtainvalidindicationsof
statisticalsigni(cid:12)cance,asotherwiseonemaybeledto\explain"phenomena
thatweresimplychanceoccurrences.
Thisapproachhashereproducedanimportantresult|thattheARD-
likevariationonearlystoppingisbene(cid:12)cialfortasksofthesortusedinthis
assessment|aswellasseveralotherinterestingresults.Thisdemonstrates
thatthepresentDELVEconventionsfornumbersofinstancesandnumbers
oftestcasescanproducestatisticallysigni(cid:12)cantresultswheninterestingdif-
ferencesarepresent.Experimentsonsmallerdatasetsmayinsteadproduce
inconclusiveresultsexceptwhenextremedi(cid:11)erencesexist(Rasmussen		).
Theresultsreportedherealsoillustratetheimportanceofrunningthe
methodsontaskswithvaryingnumbersoftrainingcases.Manyoftheinter-
estingphenomenaseenmighthavebeenmissedifasinglesizeoftrainingset
hadbeenarbitrarilychosen.
OneofthestrengthsoftheDELVEenvironmentseenhereistheease
withwhichcomparisonscanbemadetotheresultsofothermethodsheldin
theDELVEarchive.Notethatforpairedsigni(cid:12)cantteststobepossible,it
isnecessaryforthelossesoneachtestcasetobearchivedforeachmethod.
Whenonlytheaveragelossforamethodisknown(eg,fromareportin
theliterature),nosigni(cid:12)cancetestingispossible.Whentheaveragelossis
accompaniedbyanestimatedstandarderror,signi(cid:12)cancetestsarepossible,
butthesetestswillbeoflowpower,sincethepairinginformationisignored.
OneweaknessofDELVEatpresentisthatwehaveonlyafewfamilies
ofdatasets,alongwithsomeisolateddatasets.Thenumberoffamiliescould
certainlybeincreased,buthowbesttodecideonthetypesofdatasetsto
createisanopenquestion.Interpretationofresultscanalsobedi(cid:14)cultifthe
natureofadatasetisnotclear.TheprogramsthatgeneratedtheDELVE


RadfordM.Neal
datasetsareavailablefromtheDELVEarchive,butsomemoreaccessible
characterizationofeachdatasetwouldbedesirable.
Amorefundamentalproblemisthatrunningamethodonallthetasks
inmanyfamiliescantakealargeamountofcomputationtime.Thispracti-
caldi(cid:14)cultyiswhytheexperimentsinthispaperusedonlythe\nm"tasks
(non-linear,mediumnoise).TheDELVEsoftwareisitselfabitslow,due
toitsbeingwritteninthetcllanguage.Fixingthiswouldbehelpful,espe-
ciallywhenthemethodsbeingassessedarefast.Assessingmethodsthatare
slower,suchasneuralnetworks,willinevitablyrequireasubstantialamount
ofcomputationtime,butonemayhopethatthebene(cid:12)tobtainedfromthis
e(cid:11)ortwillincreaseastheresultsofmoremethodsareplacedintheDELVE
archive.
Acknowledgements
TheDELVEenvironmentwasdevelopedattheUniversityofTorontobyCarl
Rasmussen,Geo(cid:11)Hinton,DrewvanCamp,MikeRevow,ZoubinGhahra-
mani,RafalKustra,RobTibshirani,andmyself.IthankZoubinGhahra-
mani,DavidMacKay,andRobTibshiraniforhelpfulcomments.Thiswork
wassupportedbytheInstituteforRoboticsandIntelligenceSystemsandby
theNaturalSciencesandEngineeringResearchCouncilofCanada.
