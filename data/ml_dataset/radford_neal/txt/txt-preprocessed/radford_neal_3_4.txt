Abstract Gaussian Process (GP) regression models typically assume that residuals are Gaussian
and have the same variance for all observations. However, applications with input-dependent noise
(heteroscedastic residuals) frequently arise in practice, as do applications in which the residuals
do not have a Gaussian distribution. In this paper, we propose a GP Regression model with a
latent variable that serves as an additional unobserved covariate for the regression. This model
(which we call GPLC) allows for heteroscedasticity since it allows the function to have a changing
partial derivative with respect to this unobserved covariate. With a suitable covariance function,
our GPLC model can handle (a) Gaussian residuals with input-dependent variance, or (b) non-
Gaussian residuals with input-dependent variance, or (c) Gaussian residuals with constant variance.
We compare our model, using synthetic datasets, with a model proposed by Goldberg, Williams
and Bishop (1998), which we refer to as GPLV, which only deals with case (a), as well as a standard
GP model which can handle only case (c). Markov Chain Monte Carlo methods are developed for
both modelsl. Experiments show that when the data is heteroscedastic, both GPLC and GPLV
give better results (smaller mean squared error and negative log-probability density) than standard
GP regression. In addition, when the residual are Gaussian, our GPLC model is generally nearly as
good as GPLV, while when the residuals are non-Gaussian, our GPLC model is better than GPLV.

1

Introduction

Gaussian Process (GP) regression models are popular in the machine learning community (see,
for example, the text by Rasmussen and Williams (2006)), perhaps mainly because these models
are very exible  one can choose from many covariance functions to achieve dierent degrees of
smoothness or dierent degrees of additive structure, the parameters of such a covariance function
can be automatically determined from the data. However, standard GP regression models typi-
cally assume that the residuals have i.i.d. Gaussian distributions that do not depend on the input
covariates, though in many applications, the variances of the residuals do depend on the inputs,
and the distributions of the residuals are not necessarily Gaussian. In this paper, we present a
GP regression model which can deal with input-dependent residuals. This model includes a latent
variable with a xed distribution as an unobserved input covariate. When the partial derivative of

1

the response with respect to this unobserved covariate changes across observations, the variance of
the residuals will change. When the latent variable is transformed non-linearly, the residuals will
be non-Gaussian. We call this the Gaussian Process with a Latent Covariate (GPLC) regression
model.

In Section 2 below, we give the details of this model as well as the standard GP model (STD)
and a model due to Goldberg, Williams and Bishop (1998), which we call the Gaussian Process
regression with a Latent Variance (GPLV) model, and we discuss the relationships/equivalencies
between these models. We describe computational methods in Section 3, and present the results of
these models on various synthetic datasets in Section 4.

2 The models

We look at non-linear regression problems, where the aim is to nd the association between a
vector of covariates x and a response y using n observed pairs (x1, y1), ..., (xn, yn), and then make
predictions for yn+1, yn+2, ... corresponding to xn+1, xn+2...:

yi = f (xi) + i

(1)

The covariate xi is a vector of length p, and the correspoding response yi is a scalar.

2.1 The standard GP regression model

In the standard Gaussian process regression model The random residuals, is, are assumed to have
i.i.d. Gaussian distributions with mean 0 and constant variance 2.

Bayesian GP models assume that the noise-free function f comes from a Gaussian Process which
has prior mean function zero and some specied covariance function. Note that a zero mean prior
is not a requirement  we could specify a non-zero prior mean function m(x) if we have a priori
knowledge of the mean structure. Using a zero mean prior just reects our prior knowledge that the
function is equally likely to be positive or negative. It doesnt mean we believe the actual function
will have an average over its domain of zero.

The covariance functions can be fully-specied functions, but common practice is to specify a
covariance function with unknown hyperparameters, and then estimate the hyperparameters from
the data. Given the values of the hyperparameters, the responses, y, in a set of cases have a
multivariate Gaussian distribution with zero mean and a covariance matrix given by

(2)
where ij = 0 when i (cid:54)= j and ii = 1, and K(xi, xj) is the covariance function of f . Any function
that always leads to a positive semi-denite covariance matrix can be used as a covariance function.
One example is the squared exponential covariance function with isotropic length-scale:

Cov(yi, yj) = K(xi, xj) + ij2

(cid:18)

(cid:107)xi  xj(cid:107)2

2

(cid:19)

K(xi, xj) = c2 + 2 exp

(3)

Where c is a suitably chosen constant, and ,  and  are hyperparameters. 2 is sometimes
referred to as the signal variance, which controls the magnitude of variation of f ; 2 is the

2

residual variance;  is the length scale parameter for the input covariates. We can also assign a
dierent length scale parameter to each covariate, which leads to the squared exponential covariance
function with automatic relevance determination (ARD):

K(xi, xj) = c2 + 2 exp

(cid:32)
 p(cid:88)

k=1

(xik  xjk)2

2
k

(cid:33)

We will use squared exponential forms of covariance function from (3) or (4) in most of this paper.
If the values of the hyperparameters are known, then the predictive distribution of y for a test
case x based on observed values x = (x1, ..., xn) and y = (y1, ..., yn) is Gaussian with the following
mean and variance:

E(y|x, y, x) = kT C1y

Var(y|x, y, x) = v  kT C1k

(4)

(5)

(6)

(7)

In the equations above, k is the vector of covariances between y and each of y1, . . . , yn, C is the
covariance matrix of the observed y, and v is the prior variance of y, which is Cov(y, y) from (2).

When the values of the hyperparameters (denoted as ) are unknown and therefore have to be
estimated from the data, we put priors on them (typically independent Gaussian priors on the loga-
rithm of each hyperparameter), and obtain the posterior distribution p(|x, y)  N (y|0, C())p().
The predictive mean of y can then be computed by integrating over the posterior distribution of
the hyperparameters:

E(y|x, y, x) =

k()T C()1y  p(|x, y)d

Letting E = E(y|x, y, x), the predictive variance is given by
Var(y|x, y, x) = E[Var(y|x, y, x, )] + Var[E(y|x, y, x, )]

(cid:2)v()  k()T C()1k()(cid:3) p(|x, y)d +

(cid:90)

(cid:90)

=

(cid:2)k()T C()1y  E(cid:3)2

(8)
p(|x, y)d





2.2 A GP regression model with a latent covariate

In this paper, we consider adding a latent variable w into the model as an unobserved input. The
regression equation then becomes

yi = g(xi, wi) + i.

(9)

In this setting, the latent value wi has some known random distribution, the same for all i. If we
view g(xi, wi) as a function of xi only, its value is random, due to the randomness of wi. So g is
not the regression function giving the expected value of y for a given value of x  that is given by
the averge value of g over all w, which we write as f (x) = E(y|x):

(cid:90)



(cid:90)

f (x) =

g(x, w)p(w)dw

(10)

where p(w) is the probability density of w. Note that (10) implies that the term i, which we assume
has i.i.d. Gaussian distribution with constant variance, is not the real residual of the regression,
since

i = yi  g(xi, wi) (cid:54)= yi  f (xi) = i

3

where i is the true residual. We put i in the regression for two reasons. First, the covariance
function for g can sometimes produce nearly singular covariance matrices, that are computationally
non-invertible because of round-o error. Adding a small diagonal term can avoid the computational
issue without signicantly changing the properties of the covariance matrix. Secondly, the function g
will produce a probability density function for  that has singularities at points where the derivative
of g with respect to w is zero, which is probably not desired in most applications. Adding a jitter
term i smooths away such singularities.

We will model g(x, w) using a GP with a squared exponential covariance function with ARD,

for which the covariance between training cases i and j, with latent values wi and wj, is

Cov(yi, yj) = K((xi, wi), (xj, wj)) + 2ij

= c2 + 2 exp

(xik  xjk)2

2
k

 (wi  wj)2

2
p+1

(cid:32)
 p(cid:88)

k=1

(cid:33)

(11)

+ 2ij

We choose independent standard normals as the distributions for w1, ..., wn. The mean for the
wi is chosen to be zero because the squared exponential function is stationary, and hence only the
dierence between wi and wj matters. The variance of the wi is xed at 1 because the eect of a
change of scale of wi can be achieved instead by a change in the length scale parameter lp+1.

We write p(w) for the density for the vector of latent variables w, and p() for the prior density of
all the hyperparameters (denoted as a vector ). The posterior joint density for the latent variables
and the hyperparameters is

p(w, |x, y) = N (y|0, C(, w))p(w)p()

(12)
where N (a|, ) denotes the probability density of a multivariate Gaussian distribution with mean
 and covariance matrix , evaluated at a. C(, w) is the covariance matrix of y, which depends
on  and w.

The prediction formulas for GPLC models are similar to (7) and (8). In addition to averaging
over the hyperparameters, we also have to average over the posterior distribution of the latent
variables w = (w1, ..., wn):

E(y|x, y, x) =

k(, w, w)T C(, w)1y p(, w|x, y)ddw

(cid:90)

(cid:90)

W



(13)

(14)

Var(y|x, y, x) = E,w[Var(y|x, y, x, , w)] + Var,w[E(y|x, y, x, , w)]

(cid:2)v(, w)  k(, w, w)T C(, w)1k(, w, w)(cid:3) p(w, |x, y)ddwdw
(cid:90)
(cid:2)k(, w, w)T C(, w)1y  E(cid:3)2

p(w, |x, y)ddwdw

(cid:90)

=

(cid:90)
(cid:90)



W

+

W



where E = E(y|x, y, x)

Note that the vector of covariances of the response in a test case with the responses in training
cases, written as k(, w) in (13) and (14), depends on, w, the latent value for the test case. Since
we do not observe w, we randomly draw values from the prior distribution of w, compute the

4

Figure 1: How GPLC can produce a non-Gaussian distribution of residuals.

corresponding expectation or variance and take the average. Similarly, the prior variance for the
response in a test case, written v(, w) above, depends in general on w (though not for the squared
exponential covariance function that we use in this paper).

To see that this model allows residual variances to depend on x, and that the residuals can have

non-Gaussian distributions, we compute the Taylor-expansion of g(x, w) at w = 0:

g(x, w) = g(x, 0) + g(cid:48)

2(x, 0)w +

w2
2

g(cid:48)(cid:48)
2 (x, 0) + ...

(15)

2 and g(cid:48)(cid:48)

where g(cid:48)
2 denotes the rst and second order partial derivatives of g with respect to its second
argument (w). If we can ignore the second and higher order terms, i.e. the linear approximation is
good enough, then the response given x is Gaussian, and

Var[g(x, w)]  0 + [g(cid:48)

2(x, 0)]2Var(w) = [g(cid:48)

2(x, 0)]2

(16)

which depends on x when g(cid:48)
2(x, 0) depends on x (which usually is the case when g is drawn from
a GP prior). Thus in this case, the model produces Gaussian residuals with input-dependent
variances.

If the high-order terms in (15) cannot be ignored, then the model will have non-Gaussian, input-
dependent residuals. For example, consider g(x, w) = (x + w)2, where the second order term in w
clearly cannot be ignored. Conditional on x, g(x, w) follows a non-central Chi-Squared distribution.
Figure 1 illustrates that at x = 2, an unobserved normally distributed input w translates into a
non-Gaussian output y. Note that for demonstration purposes the density curves of w and y are
not to scale (since the scales on the x-axis and y-axis are dierent).

Figure 2 illustrates how an unobserved covariate can produce heteroscedasticity. The data in the
left plot are generated from a GP, with xi drawn uniformly from [0,5] and wi drawn from N (0, 1).
The hyperparameters of the squared exponential covariance function were set to  = 3, x = 0.8,
and w = 3. Supposing we only observe (x, y), the data is clearly heteroscedastistic, since the
spread of y against x changes when x changes. For instance, the spread of y looks much bigger

5

012345051015202530xyy=(x+w)2  E(Y|X=x)Density of w+2Density of yFigure 2: Heteroscedasticity produced by an unobserved covariate. The left plot shows a sample of
x and y from the GP prior, with w not shown. The right plot shows 19 dashed curves of g(xi, wi)
(for the same g as on the left) where the wi are xed to the same value, equal to the 5ith percentile
of the standard normal for the ith curve (i.e. 1 to 19).

when x is around 1.8 than it is when x is near 3.5. We also notice that the distribution of the
residuals cant be Gaussian, as, for instance, we see strong skewness near x = 5. These plots show
that if an important input quantity is not observed, the function values based only on the observed
inputs will in general be heteroscedastic, and non-Gaussian (even if the noise term i is Gaussian).

Note that although an unobserved input quantity will create heteroscedasticity, our model can
work well even if no such quantity really exists. The model can be seen as just using the latent
variable as a mathematical trick, to produce changing residual variances. Whether or not there
really exists an unobserved input quantity doesnt matter (though in practice, unobserved quantities
often do exist).

2.3 A GP regression model with a latent variance

Goldberg, Williams and Bishop (1998) proposed a GP treatment of regression with input-dependent
residuals. In their scheme, a main GP models the mean of the response just like a standard GP
regression model, except the residuals are not assumed to have constant variance  a secondary
GP is used to model the logarithm of the standard deviation of the noise, which depends on the
input. The regression equation looks the same as in (1):

yi = f (xi) + i

(17)

but the residuals 1, ...n are do not have the same variance  instead, the logarithm of the standard
deviation zi = log SD[(xi)] depends on xi through:

zi = r(xi) + Ji

(18)

6

01234543210123456xyy=f(x,w)  random ww=00123453210123456xyy=f(x,w)  w=0f (x) and r(x) are both given (independent) GP priors, with zero mean and covariance functions
Cy and Cz, which have dierent hyperparameters (e.g. (y, y) and (z, z)). Ji is a Gaussian
jitter term (see Neal, 1997) which has i.i.d. Gaussian distribution with zero mean and standard
deviation J (a preset constant, usually a very small number, e.g. 103). Writing x = (x1, ..., xn),
y = (y1, ..., yn), y = (y, y), z = (z, z), and z = (z1, ..., zn), the posterior density function of
the latent values and the hyperparameters is
p(y, z, z|x, y)  p(y|x, z, y)p(z|x, z)p(y, z)  N (y|0, Cy(y, z))N (z|0, Cz(z))p(y, z)
where Cy is the covariance matrix for y (for the main GP), Cz is the covariance matrix for
z (for the secondary GP) and p(y, z) represents the prior density for the hyperparameters
(typically independent Gaussian priors for their logarithms). The predictive mean can be com-
puted in a similar fashion as the prediction of GPLC, but instead of averaging over w, we average
over z. To compute the covariance vector k, we need the value of zn+1, which we can sample from
p(zn+1|z1, ..., zn).

(19)

Alternatively, instead of using a GP to model the logarithm of the residuals standard deviations,
we can set the standard deviations to the absolute values of a function modeled by a GP. That is,
we let SD(i) = |zi|, with zi = r(xi). So the regression model can be written as

yi = f (xi) + r(xi)ui

(20)

where ui

iid N (0, 1).

This is similar to modeling the log of the standard deviation with a GP, but it does allow the
standard devaition, |zi|, to be zero, whereas exp(zi) is always positive, and it is less likely to produce
extremely large values for the standard deviation of a residual. A more general approach is taken
by Wilson and Ghahramani (2010), who use a parameterized function to map values modeled by a
GP to residual variances, estimating the parameters from the data.

In the original paper by Goldberg et. al., a toy example was given where the hyperparameters
are all xed, with only the latent values sampled using MCMC. In this paper, we will take a full
Bayesian approach, where both the hyperparameters and the z values are sampled from (19). In
addition, we will discuss fast computation methods for this model in Section 3.

2.4 Relationships between GPLC and other models

We will show in this section that GPLC can be equivalent to the a standard GP regression model
or a GPLV model, when the covariance function is suitably specied.

Suppose the function g(x, w) in (9) has the form

g(xi, wi) = h(xi) + wi

(21)

where wi  N (0, 1).
If we only observe xi but not wi, then (21) is a regression problem with
unknown i.i.d. Gaussian residuals with mean 0 and variance 2, which is equivalent to the standard
GP regression model (1), if we give a GP prior to h. If we specify a covariance function that produces
such a g(x, w), then if we set i = 0 (or equivalently, the hyperparameter 2 = Var(i) = 0), our
GPLC model will be equivalent to the standard GP model. Below, we compute the covariance
between training cases i and j (with latent values wi and wj) to nd out the form of the appropriate
covariance function.

7

Lets put a GP prior with zero mean and covariance function K1(xi, xj) on h(x). As usual, the wi
have independent N (0, 1) priors. Since the values of g(x, w) are a linear combination of independent
Gaussians, they will have a Gaussian process distribution, conditional on the hyperparameters. Now
the covariance between cases i and j is

Cov[g(xi, wi), g(xj, wj)] = E[(h(xi) + wi)(h(xj) + wj)]

= E[h(xi)h(xj)] + 2wiwj
= K1(xi, xj) + 2wiwj

Therefore, if we put a GP prior on g(x, w) with zero mean and covariance function

K[(xi, wi), (xj, wj)] = K1(xi, xj) + 2wiwj

(22)

(23)

the results given by GPLC will be equivalent to standard GP regression with covariance function
K1. In practice, if we are willing to make the assumption that the residuals have equal variances (or
know this as a fact), this modied GPLC model is not useful, since the complexity of handling latent
variables computationally is unnecessary. However, consider a more general covariance function

where K1[(xi, wi), (xj, wj)] = exp(cid:0)(cid:80)p
nential covariance function with ARD, and K2[(xi, wi), (xj, wj)] = (cid:80)p

k=1(xik  xjk)2/2

k  (wi  wj)2/2
p+1
k=1 2

K[(xi, wi), (xj, wj)] = K1[(xi, wi), (xj, wj)] + K2[(xi, wi), (xj, wj)]

(cid:1) is a squared expo-

(24)

p+1wiwj is
a linear covariance function with ARD. Then the covariance function (23) can be obtained as a
limiting case of (24), when p+1 goes to innity in K1 and 1, ..., p all go to zero. Therefore,
we could use this more general model, and let the data choose whether to (nearly) t the simpler
standard GP model.

kxikxjk + 2

Similarly, if we believe that the function g(x, w) is of the form

g(x, w) = h1(x) + wh2(x)

(25)

with h1 and h2 independently having Gaussian Process priors with covariance functions K1 and
K2, we can use a GPLC model with a covariance function of the form

K[(xi, wi), (xj, wj)] = K1(xi, xj) + wiwjK2(xi, xj)

(26)

Now consider the alternative GPLV model (20): if we put independent GP priors on f (xi) and
r(xi), each with zero mean, and covariance functions K1 and K2, respectively, then model (20) is
equivalent to the modied GPLC model above with covariance function (26). The hyperparameters
of K1 of both models should have the same posterior distribution, as would the hyperparameter of
K2. Notice that the two models have dierent latent variables: the latent variable in GPLC, wi, is
the value of the ith (normalized) residual; the latent variable in GPLV is zi = r(xi), which is plus
or minus the standard deviation of the ith residual.

3 Computation

Bayesian inference for GP models is based on the posterior distribution of the hyperparameters and
the latent variables. Unfortunately this distribution is seldom analytically tractable. We usually
use Markov Chain Monte Carlo to sample the hyperparameters and the latent values from their
posterior distribution.

8

3.1 Overview of methods

Common choices of MCMC method include the classic Metropolis algorithm (Metropolis et. al,
1953) and slice sampling (Neal, 2003). The Metropolis sampler is easy to implement, but for
high-dimensional distributions, it is generally hard to tune. We can update all the parameters at
each iteration using a multivariate proposal distribution (e.g. N (0, D) where D is diagonal), or
we can update one parameter at a time based on a univariate proposal distribution. Either way,
to contruct an ecient MCMC method we have to assign an appropriate value for the proposal
standard deviation (a tuning parameter) for each hyperparameter or latent variable so that the
acceptance rate on each variable is neither too big nor too small (generally, between 20% and 80%
is acceptable, though the optimal value is typically unknown). There is generally no good way to
nd out what tuning parameter value is the best for each variable other than trial and error. For
high-dimensional problems, tuning the chain is very dicult. Using squared exponential covariance
function, our model GPLC has D = n + p + 3 variables (including hyperparameters and latent
variables), and the GPLV model has D = n + 2p + 2 variables.

Slice sampling, on the other hand, although slightly more dicult to implement, is relatively
easier to tune. It does also have tuning parameters (one can control the step-out size and the number
of step-outs), but the performance of the chain is not very sensitive to the tuning parameters.
Figure 2.7 of Thompson (2011) demonstrates that step-out sizes from 1 to 1000 all lead to similar
computation time, while a change in proposal standard deviation from 1 to 1000 for a Metropolis
sampler can result in a MCMC which is 10 to 100 times slower. In this paper, we use univariate
step-out slice sampling for regular GP regression models and GPLC models. For GPLV, since the
latent values are highly correlated, regular Metropolis and slice samplers do not work well. We will
give a modied Metropolis sampler than works better than both of these simpler samplers.

3.2 Major computations for GP models

Evaluating the log of the posterior probability density of a GP model is typically dominated by com-
putating the covariance matrix, C, and nding the Cholesky decomposition of C, with complexities
pn2 and n3, respectively.

For both standard GP models and GPLV models, updates of most of the hyperparameters require
that the covariance matrix C be recomputed, and hence also the Cholesky decomposition (denoted
as chol(C)). For GPLC, when the ith latent variable is updated, most of C is unchanged, except for
the ith row and ith column. This change requires a rank-n update on the Cholesky decomposition,
which is almost as costly as as nding the Cholesky decomposition for a new C.

Things are slightly more complicated for GPLV, since the model consists of two GPs, with two
covariance matrices. When one of the hyperparameters for the main GP (denoted as y) is changed,
the covariance matrix for the main GP, Cy, is changed, and thus chol(Cy) has to be recomputed.
However, Cz, the covariance matrix for the secondary GP, remain unchanged. When one of z, the
hyperparameters of the secondary GP is changed, Cy (and chol(Cy)) remain unchanged, but Cz and
chol(Cz) must be recomputed. When one of the latent values, say the ith, is changed, Cz remains
unchanged as it only depends on x and z, but the ith entry on the diagonal of Cy is changed. This
minor change to Cy requires only a rank-1 update (Sherman et. al, 1950), with complexity n2.

We list the major operations for the GP models discussed in this paper in Table 1.

9

one hyperparameter

one latent variable

STD

GPLC

GPLV

Operation
Complexity

# of such operations

Operation
Complexity

# of such operations

Operation
Complexity

# of such operations

C, chol(C)

pn2, n3
p + 2

C, chol(C)

pn2, n3
p + 3

Cy, chol(Cy) or Cz, chol(Cz)

pn2, n3
2p + 2

-
-
-

1/n of C, all of chol(C)

pn, n3

n

rank-1 update Cy

n2
n

Table 1: Major operations needed when hyperparameters and latent variables change in GP models.

3.3 A modied Metropolis sampler for GPLV

Neal (1998) describes a method for updating latent variables in a GP model that uses a proposal
distribution that takes into account the correlation information. This method proposes to change
the current latent values, z, to a z(cid:48) obtained by

z(cid:48) = (1  a2)1/2z + aLu

(27)

where a is a small constant (a tuning parameter, typically slightly greater than zero), L is the
lower triangular Cholesky decomposition for Cz, the covariance matrix for the N (0, Cz(z)) prior
for z, and u is a random vector of i.i.d. standard normal values. The transition from z to z(cid:48) is
reversible, and leaves the prior for z invariant. Because of this, the Metropolis-Hastings acceptance
probability for these proposals depends only on the ratio of likelihoods for z(cid:48) and z.

We will use this method to develop a sampling strategy for GPLV. Recall the unnormalized

posterior distribution for the hyperparameters and latent values is given by
p(y, z, z|x, y) = N (y|0, Cy(y, z))N (z|0, Cz(z))p(y, z)

y, (cid:48)

To obtain new values (cid:48)

z and z(cid:48) based on current values y, z and z, we can do the following:
1. For each of the hyperparameters in y (i.e. those associated with the main GP), do an
update of this hyperparameter (for instance a Metropolis or slice sampling update). Notice
that for each of these updates we need to recompute chol(Cy), but not chol(Cz), since Cz
does not depend on y.

2. For each of the hyperparameters in z (i.e. those for the secondary GP):

(a) Do an update of this hyperparameter (e.g. with Metropolis or slice sampling). We need

to recompute chol(Cz) for this, but not chol(Cy), since Cy does not depend on z.

(b) Update all of z with the proposal described in (27). We need to recompute chol(Cy) to
do this, but not chol(Cz), since Cz depends only on z but not z. We repeat this step
for m times (a tuning parameter) before moving to the next hyperparameter in z.

In this scheme, the hyperparameters y and z are not highly correlated and hence are relatively
easy to sample using the Metropolis algorithm. The latent variables z are highly correlated. Because
updating the z-values is hard, we try to update them as much as possible. Notice that Cz depends
only on x and z, so a change of z will not result in a change of Cz. Hence once we update a
component of z (and obtain a new Cz), it makes sense to do m > 1 updates on z before updating
another z-hyperparameter.

10

4 Experiments

We will compare our GPLC model with Goldberg et. al.s GPLV model, and with a standard GP
regression model having Gaussian residuals of constant variance.

4.1 Datasets

We use four synthetic datasets, with one or three covariates, and Gaussian or non-Gaussian resid-
uals, as summarized below:

Dataset

U1
U2
M1
M2

p True function Residual SD Residual distribution
1
1
3
3

f (x)
f (x)
g(x)
g(x)

r(x)
r(x)
s(x)
s(x)

non-Gaussian

Gaussian

non-Gaussian

Gaussian

Datasets U1 and U2 both have one covariate, which is uniformly drawn from [0,1], and the true

function is

f (xi) = [1 + sin(4xi)]1.1

For U1, the response yi = f (xi) + i is contaminated with a Gaussian noise, i, with input-

dependent standard deviation

SD(i) = r(xi) = 0.2 + 0.3 exp[30(xi  0.5)2]

tion, EV (, ) (see Leadbetter et. al., 2011), with probability density (1/)e()/ exp(cid:0)e()/(cid:1).

For U2, the response has a non-Gaussian residual, , with a location-scale extreme value distribu-

The mean of  is E() =  + , where  = 0.5772 . . . is the Eulers constant. The variance of 
is Var() = 22/6. We translate and scale the EV residuals so that their mean is zero and their
standard deviation is r(x) (same as those of  in U1). The density curve of a EV residual with
mean 0 and variance 1 is shown in Figure 3.

Figure 3: Density curve of extreme value residual with mean 0 and variance 1.

11

864202400.050.10.150.20.250.30.350.40.450.5xf(x)Extreme Value DistributionDatasets M1 and M2 both have three independent, standard normal covariates, denoted as

x = (x1, x2, x3). The true function is

g(x) = [1 + sin(x1/1.5 + 2)]0.9  [1 + sin(x2/2 + x3/3  2)]1.5

As we did for U1 and U2, we add Gaussian residuals to M1 and extreme value residuals to M2.
For both M1 and M2, the standard deviations of these residuals depend on the input covariates as
follows:

s(x) = 0.1 + 0.4 exp[0.2(x1  1)2  0.3(x2  2)2] + 0.3 exp[0.3(x3 + 2)2]

4.2 Predictive performance of the methods

For each dataset (U1, U2, M1, and M2), we randomly generated 10 dierent training sets (using
the same program but dierent random seeds), each with n = 100 observations, and a test dataset
with N = 5000 observations. We obtained MCMC samples using the methods described in the
previous section, dropping the initial 1/4 samples as burn-in, and used them to make predictions
for the test cases.

In order to evaluate how well each model does in terms of the mean of its predictive distribution,
we computed the mean squared error (MSE) with respect to the true function values f (xi) as
follows

where y1, ..., yN ) are the predicted responses for test cases. We also computed the average negative
log-probability density (NLPD) of the responses in the test cases, as follows

(28)

(29)

MSE(y) =

1
N

(yi  f (xi))2

i=1

N(cid:88)
 1

M

N(cid:88)

i=1

log

M(cid:88)

j=1

NLPD(y) =  1
N

(y(i)|ij, 2
ij)



where (|, 2) denotes the probability density for N (, 2), ij, 2
ij is the predictive mean and
variance for test case y(i) using the hyperparameters and latent variables from the jth MCMC
iteration, and M is the number of MCMC samples used for prediction.

We give pairwise comparison of the MSE and the NLPD in Figures 4, 5, 6, and 7. These plots
show that both GPLC and GPLV give smaller NLPD values than the standard GP model for all
datasets. At least for the multivariate datasets, GPLC and GPLV also give smaller MSEs than the
standard GP model. This shows that both GPLC and GPLV can be eective for heteroscedastic
regression problems.

Comparing GPLC and GPLV, we notice that for datasets with Gaussian residuals, GPLC is
almost as good as GPLV (except for the NLPDs for U1, where GPLV gives smaller values 8 out of
10 times), while for non-Gaussian residuals, GPLC is the clear winner, giving MSEs and NLPDs
that are smaller than for GPLV most of the time. The numerical MSE and NLPD values are listed
in the Appendix.

12

Negative Log Probability Density

Mean Squared Error

Figure 4: Dataset U1: Pairwise comparison of methods using NLPD(Left) and MSE(right)

Negative Log Probability Density

Mean Squared Error

Figure 5: Dataset U2: Pairwise comparison of methods using NLPD(Left) and MSE(right)

13

STD0.20.30.20.250.30.35STDGPLC0.20.30.20.250.30.35GPLCSTD0.20.30.20.250.30.35STDGPLV0.20.30.20.250.30.35GPLVSTDGPLC0.20.250.30.20.250.3GPLCGPLV0.20.250.30.20.250.3GPLVGPLCGPLVSTD51015x 10351015x 103STDGPLC51015x 10351015x 103GPLCSTD2468101214x 1032468101214x 103STDGPLV2468101214x 1032468101214x 103GPLVSTDGPLC51015x 10351015x 103GPLCGPLV51015x 10351015x 103GPLVGPLCGPLVREG0.10.20.30.10.20.3REGGPLC0.10.20.30.10.20.3GPLCREG0.20.30.150.20.250.3REGGPLV0.20.30.150.20.250.3GPLVREGGPLC0.10.20.10.150.20.25GPLCGPLV0.10.20.10.150.20.25GPLVGPLCGPLVREG246x 103246x 103REGGPLC246x 103246x 103GPLCREG246x 103246x 103REGGPLV246x 103246x 103GPLVREGGPLC24x 10312345x 103GPLCGPLV24x 10312345x 103GPLVGPLCGPLVNegative Log Probability Density

Mean Squared Error

Figure 6: Dataset M1: Pairwise comparison of methods using NLPD(Left) and MSE(right)

Negative Log Probability Density

Mean Squared Error

Figure 7: Dataset M2: Pairwise comparison of methods using NLPD(Left) and MSE(right)

14

STD0.30.40.50.30.40.5STDGPLC0.30.40.50.30.40.5GPLCSTD0.30.40.50.30.40.5STDGPLV0.30.40.50.30.40.5GPLVSTDGPLC0.30.40.30.350.40.45GPLCGPLV0.30.40.30.350.40.45GPLVGPLCGPLVSTD0.020.030.040.020.030.04STDGPLC0.020.030.040.020.030.04GPLCSTD0.020.040.010.020.030.04STDGPLV0.020.040.010.020.030.04GPLVSTDGPLC0.020.040.010.020.030.04GPLCGPLV0.020.040.010.020.030.04GPLVGPLCGPLVSTD0.30.40.50.30.40.5STDGPLC0.30.40.50.30.40.5GPLCSTD0.30.40.50.30.40.5STDGPLV0.30.40.50.30.40.5GPLVSTDGPLC0.250.30.350.40.250.30.350.4GPLCGPLV0.250.30.350.40.250.30.350.4GPLVGPLCGPLVSTD0.020.040.010.020.030.04STDGPLC0.020.040.010.020.030.04GPLCSTD0.020.040.010.020.030.040.05STDGPLV0.020.040.010.020.030.040.05GPLVSTDGPLC0.020.040.010.020.030.040.05GPLCGPLV0.020.040.010.020.030.040.05GPLVGPLCGPLV4.3 Comparison of MCMC methods for GPLV models

To test whether or not the modied Metropolis sampler described in Section 3.3 is eective, we
compare it to two standard samplers, which update the latent values one by one using either the
Metropolis algorithm or the univariate step-out slice sampler. The Metropolis algorithm is used to
update the hyperparameters in all of the three samplers. The signicant computations are listed
in Table 2.

We adjust the tuning parameters so that the above samplers are reasonably ecient. For the slice
sampler, we use a slice width of 1, and allow indeite stepping-out. For the univariate Metropolis
sampler, we adjust the standard deviations of the Gaussian proposals so that the acceptance rate
of each parameter variable is around 50%. For the modied Metropolis sampler, we set a = 0.3
and m = 40. The acceptance rate for zi is around 1.4%, but since we sample zi 40 times for each
iteration, we have about 60% of chance to get a new value of zi while all hyperparameters are
updated once.

The eciency of an MCMC method is usually measured by the autocorrelation time,  , of values

from the chain it produces (see Neal, 1993):

 = 1 + 2

(cid:88)

k(cid:88)

i

(30)

where i is the lag-i autocorrelation. Roughly speaking, the autocorrelation time is the number of
iterations a sampler needs to obtain another nearly uncorrelated sample.

i=1

With an MCMC sample of size M , we can only estimate sample autocorrelations i up to
i = M  1, and since these estimates are all noisy, we usually estimate  from a more limited set
of autocorrelations, as follows:

 = 1 + 2

i

(31)

where k is a cut-o point where i seems to be nearly 0 for all i > k,

i=1

In our experiment, we will consider the autocorrelation time of both the hyperparameters and
the latent variables. The model has four hyperparameters (y, y and z, z), so it is not too dicult
to look at all of them. But there are n = 100 latent variables, each with its own autocorrelation
time. Instead of comparing all of them one by one, we will compare the autocorrelation time of the
sum of the latent variables as well as the sum of the squares of the latent variables.

Another measure of sampler eciency is the time it takes for a sampler to reach equilibrium,
i.e. the stationary distribution of the Markov chain. This is often referred to as the Markov chain
mixing time (denoted as TM ). Theoretical bound of mixing rate can be found for some classical

Modied Metropolis

Operation

Cy, chol(Cy) Cz, chol(Cz)

# of such operations

p + 1

p + 1

y

z

z

Cy, chol(Cy)

m(p + 1)

Metropolis/Slice

Operation

Cy, chol(Cy) Cz, chol(Cz)

rank-1 up chol(Cy)

# of such operations

p + 1

p + 1

n

Table 2: Major operations of the MCMC methods for GPLV

15

Modied Metropolis
Metropolis
Slice



y
3.06
2.81
13.37

y
4.92
11.21
16.71

z
3.09
2.73
11.85

z

10.63
27.26
23.65

(cid:80)

i zi
0.32
13.78
37.81

(cid:80)

i z2
i
0.46
13.92
53.81

Table 3: Autocorrelation times (adjusted for computation time) of MCMC methods for GPLV.

algorithms, however, in practice, the mixing time of a sophisticated sampler is usually very dicult
to determine.
It is common practice to look at trace plots of log-probability density values to
decide whether or not a chain has reached equilibrium (this is usually how the number of burn-in
iterations is decided).

The mixing time and the autocorrelation time usually agree in the sense that a sampler that
takes less time to get a new uncorrelated sample can usually achieve equilibrium faster, and vice
versa, though this is not always the case.

Note both autocorrelation time  and mixing time TM take the number of iterations of the a
Markov chain as their unit of measurement. However, the CPU time required for an iteration
diers between samplers. For a fair comparison, we adjust  by multiplying it with the average
CPU time per iteration. The result, which we denote as  , measures the CPU time a sampler needs
to obtain an uncorrelated sample. To fairly compare mixing times using trace plots, we will adjust
the number of iterations in the plots so that each entire trace takes the same amount of time.

We run the three samplers ve times, starting from the same point (the prior mean) but with
dierent random seeds. The average adjusted autocorrelation times are listed in Table 3. The
modied Metropolis sampler signicantly outperforms the others at sampling the latent variables:
it is about 50 to 100 times faster than the regular Metropolis sampler and slice sampler. For the
hyperparameters, however, the modied Metropolis sampler gives roughly the same autocorrelation
times as the regular Metropolis sampler does. Both of them seems to work better than slice sampler,
but the dierence is much smaller than the dierence in sampling latent variables. Figure 8 shows
selected autocorrelation plots from one of the ve runs (adjusted for computation time).

Figure 9 gives the trace plots of the three methods for one of the ve runs (other runs are similar).
The bottom plot is the trace of log-probability density values of the initial 400 iterations of the
slice sampler. The middle plot shows the initial iterations of the regular Metropolis sampler, with
the number adjusted to take the same time as the 400 slice sampler iterations. The top plot shows
the initial iterations of the modied Metropolis sampler, again taking the same computation time.

It is clear that the modied Metropolis takes the least time to mix. Starting from the prior
mean (which seem to be a reasonable initial point), with log-probability density (LPD) value of
approximately 13, the modied Metropolis method immediately pushes the LPD to 70 at the
second step, and then soon declines slightly to what appears to be the equilibrium distribution.
The other two methods move take much more time to reach this equilibrium distribution, with the
simple Metropolis and slice sampling methods taking roughly the same amount of time.

We conclude that the modied Metropolis is the best of these MCMC method  the fastest to
reach equilibrium, the best at sampling latent values thereafter, and at least as good at sampling
hyperparameters.

16

Figure 8: Selected autocorrelation plots for MCMC methods for GPLV (with horizontal scales that
adjust for computation time).

Figure 9: Trace plots of log posterior density for MCMC methods for GPLV.

17

020400.500.51LagSample ACF for  zModified Metropolis01000.500.51LagSample ACF for  zMetropolis01020300.500.51LagSample ACF for zSlice01002000.500.51LagSample ACF for zModified Metropolis05000.500.51LagSample ACF for zMetropolis0501001500.500.51LagSample ACF for zSlice05010015020025030035040020020406080logdensitySlice020040060080010001200140016001800200020020406080logdensityMetropolis020040060080010001200140020020406080logdensityModified MetropolisIn a simpler context  nancial time series where no main GP is needed, since the mean
response can be taken to be always zero  Wilson and Ghahramani (2010) use the elliptical slice
sampling method of Murray, et. al. (2010) to sample latent values that determine the variances
of observations. Elliptical slice sampling is related to the modied Metropolis method above. It
would be interesting to see how they compare in a general regression context.

