4
1
0
2

 
c
e
D
9

 

 
 
]

O
C

.
t
a
t
s
[
 
 

1
v
3
1
0
3

.

2
1
4
1
:
v
i
X
r
a

Eﬃcient Bayesian inference for stochastic volatility models

with ensemble MCMC methods

Alexander Y. Shestopaloﬀ

Department of Statistical Sciences

University of Toronto

alexander@utstat.utoronto.ca

Radford M. Neal

Department of Statistical Sciences
& Department of Computer Science

University of Toronto

radford@utstat.utoronto.ca

9 December 2014

Abstract

In this paper, we introduce eﬃcient ensemble Markov Chain Monte Carlo (MCMC)
sampling methods for Bayesian computations in the univariate stochastic volatility model.
We compare the performance of our ensemble MCMC methods with an improved version
of a recent sampler of Kastner and Fruwirth-Schnatter (2014). We show that ensemble
samplers are more eﬃcient than this state of the art sampler by a factor of about 3.1, on a
data set simulated from the stochastic volatility model. This performance gain is achieved
without the ensemble MCMC sampler relying on the assumption that the latent process is
linear and Gaussian, unlike the sampler of Kastner and Fruwirth-Schnatter.

The stochastic volatility model is a widely-used example of a state space model with non-linear
or non-Gaussian transition or observation distributions.
It models observed log-returns y =
(y1, . . . , yN ) of a ﬁnancial time series with time-varying volatility, as follows:

Yi|xi ∼ N (0, exp(c + σxi)),
X1 ∼ N (0, 1/(1 − φ2))

Xi|xi−1 ∼ N (φxi−1, 1)

i = 1, . . . , N

(1)
(2)
(3)

Here, the latent process xi determines the unobserved log-volatility of yi. Because the relation
of the observations to the latent state is not linear and Gaussian, this model cannot be directly
handled by eﬃcient methods based on the Kalman ﬁlter.

In a Bayesian approach to this problem, we estimate the unknown parameters θ = (c, φ, σ) by
sampling from their marginal posterior distribution p(θ|y). This distribution cannot be written
down in closed form. We can, however, write down the joint posterior of θ and the log-volatilities
x = (x1, . . . , xN ), p(θ, x|y) and draw samples of (θ, x) from it. Discarding the x coordinates in
each draw will give us a sample from the marginal posterior distribution of θ.

1

To sample from the posterior distribution of the stochastic volatility model, we develop two
new MCMC samplers within the framework of ensemble MCMC, introduced by Neal (2010).
The key idea underlying ensemble MCMC is to simultaneously look at a collection of points (an
“ensemble”) in the space we are sampling from, with the K ensemble elements chosen in such a
way that the density of interest can be simultaneously evaluated at all of the ensemble elements
in less time than it would take to evaluate the density at all K points separately.

Previously, Shestopaloﬀ and Neal (2013) developed an ensemble MCMC sampler for non-
linear, non-Gaussian state space models, with ensembles over latent state sequences, using the
embedded HMM (Hidden Markov Model) technique of Neal (2003), Neal et al. (2004). This
ensemble MCMC sampler was used for Bayesian inference in a population dynamics model and
shown to be more eﬃcient than methods which only look at a single sequence at a time. In this
paper we consider ensemble MCMC samplers that look not only at ensembles over latent state
sequences as in Shestopaloﬀ and Neal (2013) but also over a subset of the parameters. We see
how well both of these methods work for the widely-used stochastic volatility model.

1 Bayesian inference for the stochastic volatility model

Bayesian inference for the stochastic volatility model has been extensively studied. In this paper,
we focus on comparisons with the method of Kastner and Fruwirth-Schnatter (2014). This state-
of-the-art method combines the method of Kim et al. (1998) with the ASIS (Ancillary Suﬃciency
Interweaving Strategy) technique of Yu and Meng (2011). Kastner and Fruwirth-Schnatter’s
method consists of two parts. The ﬁrst is an update of the latent variables x and the second is a
joint update of θ and the latent variables x. We improve this method here by saving and re-using
suﬃcient statistics to do multiple parameter updates at little additional computational cost.

1.1 A linear Gaussian approximation for sampling latent sequences

The non-linear relationship between the latent and the observation process prohibits the direct
use of Kalman ﬁlters for sampling the latent variables xi. Kim et al.
(1998) introduced an
approximation to the stochastic volatility model that allows using Kalman ﬁlters to draw samples
of xi which can be later reweighed to give samples from their exact posterior distribution. This
approximation proceeds as follows. First, the observation process for the stochastic volatility
model is written in the form

log(y2

i ) = c + σxi + ζi

(4)

where ζi has a log(χ2

1) distribution.

Next, the distribution of ζi is approximated by a ten-component mixture of Gaussians with
mixture weights πk, means mk and variances τ 2
k , k = 1, . . . , 10. The values of these mixture
weights, means and variances can be found in Omori (2007). At each step of the sampler, at
each time i, a single component of the mixture is chosen to approximate the distribution of ζi by

2

drawing a mixture component indicator ri ∈ {1, . . . , 10} with probabilities proportional to

P (ri = k|yi, xi, c, σ) ∝ πk

1
τk

exp((log(y2

i ) − c − σxi)2/τ 2
k )

(5)

Conditional on ri, the observation process is now linear and Gaussian, as is the latent process:

log(Y 2

i )|xi, ri, c, σ ∼ N (mri + c + σxi, τ 2

X1|φ ∼ N (0, 1/(1 − φ2))

Xi|xi−1 ∼ N (φxi−1, 1)

ri

),

i = 1, . . . , N

(6)
(7)
(8)

Kalman ﬁltering followed by a backward sampling pass can now be used to sample a latent

sequence x. For a description of this sampling procedure, see Petris et al. (2009).

The mis-speciﬁcation of the model due to the approximation can be corrected using importance

weights

w(l) =

(cid:81)N
i=1 f (yi|x(l)
k=1 πkg(log(y2

i

i )|x(l)

i

i=1((cid:80)10
(cid:81)N

, c(l), σ(l))

(9)

refers to a draw. Posterior expectations of functions of θ can then be computed as(cid:80) w(l)f (θ(l)),

where f is the N (0, exp(c + σxi)) density, g is the N (mk + c + σxi, τk) density and the index l

, c(l), σ(l), mk, τk))

with θ(l) the draws.

We note that, when doing any other updates aﬀecting x in combination with this approxi-
mate scheme, we need to continue to use the same mixture of Gaussians approximation to the
observation process. If an update drawing from an approximate distribution is combined with an
update drawing from an exact distribution, neither update will draw samples from their target
distribution, since neither update has a chance to reach equilibrium before the other update dis-
turbs things. We would then be unable to compute the correct importance weights to estimate
posterior expectations of functions of θ.

1.2 ASIS updates

ASIS methods (Yu and Meng (2011)) are based on the idea of interweaving two parametrizations.
For the stochastic volatility model, these are the so-called non-centered (NC) and centered (C)
parametrizations. The NC parametrization is the one in which the stochastic volatility model
was originally presented above. The C parametrization for the stochastic volatility model is

Yi|˜x1 ∼ N (0, exp(˜xi)),
˜X1 ∼ N (c, σ2/(1 − φ2))

˜Xi|˜xi−1 ∼ N (c + φ(˜xi−1 − c), σ2)

i = 1, . . . , N

(10)
(11)
(12)

The mixture of Gaussians approximation for C is the same as for NC.

3

Kastner and Fruwirth-Schnatter (2014) propose two new sampling schemes, GIS-C and GIS-
NC, in which they interweave these two parametrizations, using either the NC or C parameteri-
zation as the baseline. The authors report a negiligible performance diﬀerence between using NC
or C as the baseline. For the purposes of our comparisons, we use the method with NC as the
baseline, GIS-NC, which proceeds as follows.

1. Draw x given θ, r, y using the linear Gaussian approximation update (NC)

2. Draw θ given x, r, y using a Metropolis update (NC)

3. Move to C by setting ˜x = c + σx

4. Draw θ given ˜x, r, y using a Metropolis update (C)
5. Move back to NC by setting x = ˜x−c

σ

6. Redraw the mixture component indicators r given θ, x, y.

Theorem 4 of Yu and Meng (2011) establishes a link between ASIS and the PX-DA (Param-
eter Expansion-Data Augmentation) method of Liu and Wu (1999). In the case of the stochastic
volatility model, this means that we can view the ASIS scheme for updating x and θ as a combi-
nation of two updates, both done in the NC parametrization. The ﬁrst of these draws new values
for θ conditional on x. The second draws new values for both x and θ, such that when we propose
to update c to c∗ and σ to σ∗, we also propose to update the sequence x to x∗ = ((c+σx)−c∗)/σ∗.
For this second update, the Metropolis acceptance probability needs to be multiplied by a Jaco-
bian factor (σ/σ∗)N to account for scaling σ. A joint translation update for c and x has been
previously considered by Liu and Sabatti (2000) and successfully applied to to stochastic volatility
model. Scale updates are considered by Liu and Sabatti (2000) as well, though they do not apply
them to the stochastic volatility model.

The view of ASIS updates as joint updates to θ and x makes it easier to see why ASIS
updates improve eﬃciency. At ﬁrst glance, they look like they only update the parameters, but
they actually end up proposing to change both θ and x in a way that preserves dependence
between them. This means that moves proposed in ASIS updates are more likely to end up in a
region of high posterior density, and so be accepted.

Kastner and Fruwirth-Schnatter (2014) do a single Metropolis update of the parameters for
every update of the latent sequence. However, we note that given values for the mixture indices r,
y and x, low-dimensional suﬃcient statistics exist for all parameters in the centered parametriza-
tion. In the non-centered parametrization, given r, y and x, low-dimensional suﬃcient statistics
exist for φ. We propose doing multiple Metropolis updates given saved values of these suﬃcient
statistics (for all parameters in the case of C and for φ in the case of NC). This allows us to
reach equilibrium given a ﬁxed latent sequence at little computational cost since additional up-
dates have small cost, not dependent on N . Also, this eliminates the need to construct complex
proposal schemes, since with these repeated samples the algorithm becomes less sensitive to the
particular choice of proposal density.

4

The suﬃcient statistics in the case of NC are

N(cid:88)

N(cid:88)

t1 =

x2
i ,

t2 =

xi−1xi,

t3 = x2

1 + x2
N

(13)

i=1

i=2

with the log-likelihood of φ as a function of the suﬃcient statistics being
log(L(φ|t)) = (1/2) log(1 − φ2) − (1/2)(φ2(t1 − t3) − 2φt2 + t1)

In the case of C the suﬃcient statistics are

N(cid:88)

N−1(cid:88)

˜t1 =

˜x2
i ,

˜t2 =

˜x2
i

˜t3 =

i=1

i=2

i=2

N(cid:88)

N−1(cid:88)

i=2

˜xi−1 ˜xi

˜t4 =

˜xi

˜t5 = ˜x1 + ˜xN

(14)

with the log-likelihood as a function of the suﬃcient statistics being

log(L(c, φ, σ2|˜t)) = −(N/2) log(σ2) + (1/2) log(1 − φ2)

−(1/2)(˜t1 + φ2˜t2 − 2φ˜t3 − 2cφ2˜t4 − 2c(˜t4 + ˜t5)
+4cφ˜t4 + 2cφ˜t5 + (N − 1)(c(φ − 1))2 + c2(1 − φ2))/σ2

(15)

The details of the derivations are given in the Appendix.

2 Ensemble MCMC methods for

stochastic volatility models

The general framework underlying ensemble MCMC methods was introduced by Neal (2010).
An ensemble MCMC method using embedded HMMs for parameter inference in non-linear, non-
Gaussian state space models was introduced by Shestopaloﬀ and Neal (2013). We brieﬂy review
ensemble methods for non-linear, non-Gaussian state space models here.

Ensemble MCMC builds on the idea of MCMC using a temporary mapping. Suppose we
are interested in sampling from a distribution with density π(z) on Z. We can do this by
constructing a Markov chain with transition kernel T (z(cid:48)|z) with invariant distribution π. The
temporary mapping strategy takes T to be a composition of three stochastic mappings. The ﬁrst
mapping, ˆT , takes z to an element w of some other space W. The second, ¯T , updates w to w(cid:48).
The last, ˇT , takes w(cid:48) back to some z(cid:48) ∈ Z. The idea behind this strategy is that doing updates in
an intermediate space W may allow us to make larger changes to z, as opposed to doing updates
directly in Z.

In the ensemble method, the space W is taken to be the K-fold Cartesian product of Z. First,
z mapped to an ensemble w = (z(1), . . . , z(K)), with the current value z assigned to z(k), with
k ∈ {1, . . . , K} chosen uniformly at random. The remaining elements z(j) for j (cid:54)= k are chosen

5

from their conditional distribution under an ensemble base measure ζ, given that z(k) = z. The
marginal density of an ensemble element z(k) in the ensemble base measure ζ is denoted by ζ(z(k)).
Next, w is updated to w(cid:48) using any update that leaves invariant the ensemble density

K(cid:88)

i=1

ρ(w) = ρ((z(1), . . . , z(K))) = ζ((z(1), . . . , z(K)))

1
K

π(z(k))
ζk(z(k))

(16)

Finally, a new value z(cid:48) is chosen by selecting an element z(k) from the ensemble with probabilities
proportional to π(z(k))/ζk(z(k)). The beneﬁt of doing, say Metropolis, updates in the space of
ensembles is that a proposed move is more likely to be accepted, since for the ensemble density
to be large it is enough that the proposed ensemble contains at least some elements with high
density under π.

In Shestopaloﬀ and Neal (2013), we consider an ensemble over latent state sequences x. Specif-
ically, the current state, (x, θ), consisting of the latent states x and the parameters θ is mapped
to an ensemble y = ((x(1), θ), . . . , (x(K), θ)) where the ensemble contains all distinct sequences x(k)
passing through a collection of pool states chosen at each time i. The ensemble is then updated
to y(cid:48) = ((x(1), θ(cid:48)), . . . , (x(K), θ(cid:48)) using a Metropolis update that leaves ρ invariant. At this step,
only θ is changed. We then map back to a new x(cid:48) = (x(cid:48), θ(cid:48)), where x(cid:48) is now potentially diﬀerent
from the original x. We show that this method considerably improves sampling eﬃciency in the
Ricker model of population dynamics.

As in the original Neal (2010) paper, we emphasize here that applications of ensemble methods
are worth investigating when the density at each of the K elements of an ensemble can be
computed in less time than it takes to do K separate density evaluations. For the stochastic
volatility model, this is possible for ensembles over latent state sequences, and over the parameters
c and σ2. In this paper, we will only consider joint ensembles over x and over σ. Since we will
use η = log(σ2) in the MCMC state, we will refer to ensembles over η below.

We propose two ensemble MCMC sampling schemes for the stochastic volatility model. The
ﬁrst, ENS1, updates the latent sequence, x, and η by mapping to an ensemble composed of latent
sequences x and values of η, then immediately mapping back to new values of x and η. The
second, ENS2, maps to an ensemble of latent state sequences x and values of η, like ENS1, then
updates φ using an ensemble density summing over x and η, and ﬁnally maps back to new values
of x and η.

For both ENS1 and ENS2, we ﬁrst create a pool of η values with Lη elements, and at each
time, i, a pool of values for the latent state xi, with Lx elements. The current value of η is
assigned to the pool element η[1] and for each time i, the current xi is assigned to the pool
element x[1]
. (Since the pool states are drawn independently, we don’t need to randomly assign
i
an index to the current η and the current xi’s in their pools.) The remaining pool elements are
drawn independently from some distribution having positive probability for all possible values of
xi and η, say κi for xi and λ for η.

The total number of ensemble elements that we can construct using the pools over xi and over
x . Naively evaluating the ensemble density presents an enormous computational burden
x . By using the forward algorithm, together with

η is LηLN
for Lx > 1, taking time on the order of LηLN

6

a “caching” technique, we can evaluate the ensemble density much more eﬃciently, in time on
the order of LηL2
xN . The forward algorithm is used to eﬃciently evaluate the densities for the
ensemble over the xi. The caching technique is used to eﬃciently evaluate the densities for the
ensemble over η, which gives us a substantial constant factor speed-up in terms of computation
time.

In detail, we do the following. Let p(x1) be the initial state distribution, p(xi|xi−1) the
transition density for the latent process and p(yi|xi, η) the observation probabilities. We begin
by computing and storing the initial latent state probabilities — which do not depend on η —
for each pool state x[k]

1 at time 1.

P1 = (p(x[1]

1 ), . . . , p(x[Lx]

1

))

(17)

For each η[l] in the pool and each pool state x[k]

1 we then compute and store the initial forward

probabilities

α[l]
1 (x[k]

1 |η[l]) = p(x[k]
1 )

p(y1|x[k]
1 , η[l])
κ1(x[k]
1 )

(18)

Then, for i > 1, we similarly compute and store the matrix of transition probabilities

p(x[1]

p(x[1]

i |x[1]
i−1)
...
i |x[Lx]
i−1 )



i

|x[1]
i−1)

. . . p(x[Lx]
...
. . .
|x[Lx]
. . . p(x[Lx]
i−1 )

i

Pi =

where

p(x[k1]

|x[k2]
i−1) ∝ exp(−(x[k1]

i − φx[k2]

i−1)2/2)

(19)
for k1, k2 ∈ {1, . . . , Lx}. We then
are transition probabilities between pool states x[k2]
use the stored values of the transition probabilities Pi to eﬃciently compute the vector of forward
probabilities for all values of η[l] in the pool

i−1 and x[k1]

i

i

i (xi|η[l]) =
α[l]

p(yi|xi, η[l])

κi(xi)

with xi ∈ {x[1]

i

, . . . , x[Lx]

i

}.

Lx(cid:88)

k=1

At each time i, we divide the forward probabilities α[l]
i values and using the normalized α[l]

i (xi|η[l]), storing
the c[l]
i ’s in the next step of the recursion. This is needed
to prevent underﬂow and for ensemble density computations.
In the case of all the forward
probabilities summing to 0, we set the forward probabilities at all subsequent times to 0. Note
that we won’t get underﬂows for all values of η[l], since we are guaranteed to have a log-likelihood
that is not −∞ for the current value of η in the MCMC state.

i (xi) by c[l]

k=1 α[l]

i =(cid:80)Lx

p(xi|x[k]

i−1)α[l]

i−1(x[k]

i−1|η[l]),

i = 1, . . . , N

(20)

7

For each η[l], the ensemble density can then be computed as

N(cid:89)

ρ[l] =

c[l]
i

(21)

To avoid overﬂow or underﬂow, we work with the logarithm of ρ[l].

i=1

Even with caching, computing the forward probabilities for each η[l] in the pool is still an
order L2
x operation since we multiply the vector of forward probabilities from the previous step
by the transition matrix. However, if we do not cache and re-use the transition probabilities Pi
when computing the forward probabilities for each value of η[l] in the pool, the computation of
the ensemble densities ρ[l], for all l = 1, . . . , Lη, would be about 10 times slower. This is because
computing forward probabilities for a value of η given saved transition probabilities only involves
multiplications and additions, and not exponentiations, which are comparatively more expensive.

In ENS1, after mapping to the ensemble, we immediately sample new values of η and x from
the ensemble. We ﬁrst sample a η[l] from the marginal ensemble distribution, with probabilities
proportional to ρ[l]. After we have sampled an η[l], we sample a latent sequence x conditional on
η[l], using a stochastic backwards recursion. The stochastic backwards recursion ﬁrst samples a
N (xN|η[l]). Then, given the
state xN from the pool at time N with probabilities proportional to α[l]
sampled value of xi, we sample xi−1 from the pool at time i − 1 with probabilities proportional
to p(xi|xi−1)α[l]

i−1(xi−1|η[l]), going back to time 1.

In the terminology of Shestopaloﬀ and Neal (2013) this is a “single sequence” update combined
with an ensemble update for η (which is a “fast” variable in the terminology of Neal (2010) since
recomputation of the likelihood function after changes to this variable is fast given the saved
transition probabilities).

ensemble, (cid:80)Lη

In ENS2, before mapping back to a new η and a new x as in ENS1, we perform a Metropolis
update for φ using the ensemble density summing over all η[l] and all latent sequences in the
l=1 ρ[l]. This approximates updating φ using the posterior density of θ with x and
η integrated out, when the number of pool states is large. The update nevertheless leaves the
correct distribution exactly invariant, even if the number of pool states is not large.

2.1 Choosing the pool distribution

For a pool distribution for xi, a good candidate is the stationary distribution of xi in the

A good choice for the pool distribution is crucial for the eﬃcient performance of the ensemble
MCMC method.

AR(1) latent process, which is N (0, 1/(cid:112)1 − φ2). The question here is how to choose φ. For
call it φcur and draw pool states from N ∼ (0, c/(cid:112)1 − φ2

ENS1, which does not change φ, we can simply use the current value of φ from the MCMC state,
cur) for some scaling factor c. Typically,
we would choose c > 1 in order to ensure that for diﬀerent values of φ, we produce pool states
that cover the region where xi has high probability density.

We cannot use this pool selection scheme for ENS2 because the reverse transition after a change

8

For example, we can propose a value φ∗, and draw the pool states from N ∼ (0, c/(cid:112)1 − φ2

in φ would use diﬀerent pool states, undermining the proof via reversibility that the ensemble
transitions leave the posterior distribution invariant. However, we can choose pool states that
depend on both the current and the proposed values of φ, say φ and φ∗, in a symmetric fashion.
avg)
where φavg is the average of φ and φ∗. The validity of this scheme can be seen by considering φ∗
to be an additional variable in the model; proposing to update φ to φ∗ can then be viewed as
proposing to swap φ and φ∗ within the MCMC state.

We choose pool states for η by sampling them from the model prior. Alternative schemes are
possible, but we do not consider them here. For example, it is possible to draw local pool states
for η which stay close to the current value of η by running a Markov chain with some desired
stationary distribution J steps forwards and Lη − J − 1 steps backwards, starting at the current
value of η. For details, see Neal (2003).

In our earlier work (Shestopaloﬀ and Neal (2013)), one recommendation we made was to
consider pool states that depend on the observed data yi at a given point, constructing a “pseudo-
posterior” for xi using data observed at time i or in a small neighbourhood around i. For the
ensemble updates ENS1 and ENS2 presented here, we cannot use this approach, as we would then
need to make the pool states also depend on the current values of c and η, the latter of which is
aﬀected by the update. We could switch to the centered parametrization to avoid this problem,
but that would prevent us from making η a fast variable.

3 Comparisons

The goal of our computational experiments is to determine how well the introduced variants of
the ensemble method compare to our improved version of the Kastner and Fruwirth-Schnatter
(2014) method. We are also interested in understanding when using a full ensemble update is
helpful or not.

3.1 Data

We use a series simulated from the stochastic volatility model with parameters c = 0.5, φ =
0.98, σ2 = 0.15 with N = 1000. A plot of the data is presented in Figure 1.

We use the following priors for the model parameters.

c ∼ N (0, 1)
φ ∼ Unif[0, 1]
σ2 ∼ Inverse-Gamma(2.5, 0.075)

We use the parametrization in which the Inverse-Gamma(α, β) has probability density

f (x) =

βα
Γ(α)

xα−1e−β/x,

x > 0

9

(22)
(23)
(24)

(25)

(a) y

(b) x

Figure 1: Data set used for testing.

For α = 2.5, β = 0.075 the 2.5% and 97.5% quantiles of this distribution are approximately
(0.0117, 0.180).

In the MCMC state, we transform φ and σ2 to

η = log(σ2)
γ = log((1 + φ)/(1 − φ))

(26)
(27)

with the priors transformed correspondingly.

3.2 Sampling schemes and tuning

We compare three sampling schemes — the Kastner and Fruwirth-Schnatter (KF) method, and
our two ensemble schemes, ENS1, in which we map to an ensemble of η and x values and imme-
diately map back, and ENS2, in which we additionally update γ with an ensemble update before
mapping back.

We combine the ensemble scheme with the computationally cheap ASIS Metropolis updates.
It is sensible to add cheap updates to a sampling scheme if they are available. Note that the ASIS
(or translation and scale) updates we use in this paper are generally applicable to location-scale
models and are not restricted by the linear and Gaussian assumption.

Pilot runs showed that 80 updates appears to be the point at which we start to get diminishing
returns from using more Metropolis updates (given the suﬃcient statistics) in the KF scheme.
This is the number of Metropolis updates we use with the ensemble schemes as well.

The KF scheme updates the state as follows:

1. Update x, using the Kalman ﬁlter-based update, using the current mixture indicators r.

10

01002003004005006007008009001000−4−3−2−1012345601002003004005006007008009001000−15−10−505102. Update the parameters using the mixture approximation to the observation density. This
step consists of 80 Metropolis updates to φ given the suﬃcient statistics for NC, followed
by one joint update of c and η.

3. Change to the C parametrization.

4. Update all three parameters simultaneously using 80 Metropolis updates, given the suﬃcient
statistics for C. Note that this update does not depend on the observation density and is
therefore exact.

5. Update the mixture indicators r.

The ENS1 scheme proceeds as follows:

1. Map to an ensemble of η and x.

2. Map back to a new value of η and x.

3. Do steps 2) - 4) as for KF, but with the exact observation density.

The ENS2 scheme proceeds as follows:

1. Map to an ensemble of η and x.

2. Update γ using an ensemble Metropolis update.

3. Map back to a new value of η and x.

4. Do steps 2) - 4) as for KF, but with the exact observation density.

The Metropolis updates use a normal proposal density centered at the current parameter
values. Proposal standard deviations for the Metropolis updates in NC were set to estimated
marginal posterior standard deviations, and to half of that in C. This is because in C, we update
all three parameters at once, whereas in NC we update c and η jointly and φ separately. The
marginal posterior standard deviations were estimated using a pilot run of the ENS2 method.
The tuning settings for the Metropolis updates are presented in Table 1.

For ensemble updates of γ, we also use a normal proposal density centered at the current value
of γ, with a proposal standard deviation of 1, which is double the estimated marginal posterior
standard deviation of γ. The pool states over xi are selected from the stationary distribution
cur for the ENS1 scheme and

of the AR(1) latent process, with standard deviation 2/(cid:112)1 − φ2
2/(cid:112)1 − φ2

avg for the ENS2 scheme. We used the prior density of η to select pool states for η.

For each method, we started the samplers from 5 randomly chosen points. Parameters were
initalized to their prior means (which were 0 for c, 1.39 for γ and −3.29 for η), and each
xi, i = 1, . . . , N , was initialized independently to a value randomly drawn from the stationary
distribution of the AR(1) latent process, given γ set to the prior mean. For the KF updates, the
mixture indicators r where all initialized to 5’s, this corresponds to the mixture component whose
median matches the median of the log(χ2
1) distribution most closely. All methods were run for
approximately the same amount of computational time.

11

3.3 Results

Before comparing the performance of the methods, we veriﬁed that the methods give the same
answer up to expected variation by looking at the 95% conﬁdence intervals each produced for the
posterior means of the parameters. These conﬁdence intervals were obtained from the standard
error of the average posterior mean estimate over the ﬁve runs. The KF estimates were ad-
justed using the importance weights that compensate for the use of the approximate observation
distribution. No signiﬁcant disagreement between the answers from the diﬀerent methods was
apparent. We then evaluated the performance of each method using estimates of autocorrela-
tion time, which measures how many MCMC draws are needed to obtain the equivalent of one
independent draw.

To estimate autocorrelation time, we ﬁrst estimated autocovariances for each of the ﬁve runs,
discarding the ﬁrst 10% of the run as burn-in, and plugging in the overall mean of the ﬁve runs
into the autocovariance estimates. (This allows us to detect if the diﬀerent runs for each method
are exploring diﬀerent regions of the parameter/latent variable space). We then averaged the
resulting autocovariance estimates and used this average to get autocorrelation estimates ˆρk.
k=1 ˆρi, with K chosen to be the point
beyond which the ρk become approximately 0. All autocovariances were estimated using the Fast
Fourier Transform for computational eﬃciency.

Finally, autocorrelation time was estimated as 1 + 2(cid:80)K

The results are presented in Tables 2 and 3. The timings for each sampler represent an average
over 100 iteratons (each iteration consisting of the entire sequence of updates), with the samplers
started from a point taken after the sampler converged to equilibrium. The program was written
in MATLAB and run on a Linux system with an Intel Xeon X5680 3.33 GHz CPU. For a fair
comparison, we multiply estimated autocorrelation times by the time it takes to do one iteration
and compare these estimates.

We ran the KF method for 140, 000 iterations, with estimated autocorrelation times using the
original (unweighed) sequence for (c, γ, η) of (2.1, 37, 73), which after adjusting by computation
time of 0.16 seconds per iteration are (0.34, 5.9, 12). It follows that the ENS1 method with Lx set
to 50 and Lη set to 10 is better than the KF method by a factor of about 3.1 for the parameter
η. For ENS2, the same settings Lx = 50 and Lη = 10 appears to give the best results, with ENS2
worse by a factor of about 1.7 than ENS1 for sampling η. We also see that the ENS1 and ENS2
methods aren’t too sensitive to the particular tuning parameters, so long at there is a suﬃcient
number of ensemble elements both for xi and for η.

Method

KF

ENS1
ENS2

Prop. Std. (NC) Acc. Rate
for γ (NC)

γ

c

η

0.21

0.5

0.36

0.52

Acc. Rate

for (c, η) (NC)

Prop. Std. (C)
c
η

γ

Acc. Rate
for (c, γ, η)

0.12

0.12

0.105

0.25

0.18

0.22

Table 1: Metropolis proposal standard deviations with associated acceptance rates.

12

Lx Lη

Iterations Time/iter (s)

10

30

50

70

1
10
30
50

1
10
30
50

1
10
30
50

1
10
30
50

195000
180000
155000
140000

155000
135000
110000
65000

115000
95000
55000
55000

85000
60000
50000
45000

0.11
0.12
0.14
0.16

0.14
0.16
0.20
0.33

0.19
0.23
0.38
0.39

0.25
0.38
0.42
0.48

ACT

γ

99
95
81
91

35
18
19
16

34
11
8.9
11

33
8.3
8.4
9.1

η

160
150
130
140

71
26
26
24

68
17
12
14

67
11
11
12

c

2.6
2.7
2.6
2.3

2.4
2.2
2.3
1.9

1.9
1.9
2.2
1.9

2.2
1.9
1.8
1.9

ACT × time
η
c

γ

0.29
0.32
0.36
0.37

0.34
0.35
0.46
0.63

0.36
0.44
0.84
0.74

0.55
0.72
0.76
0.91

11
11
11
15

4.9
2.9
3.8
5.3

6.5
2.5
3.4
4.3

8.3
3.2
3.5
4.4

18
18
18
22

9.9
4.2
5.2
7.9

13
3.9
4.6
5.5

17
4.2
4.6
5.8

Table 2: Performance of method ENS1.

Lx Lη Acc. Rate for γ

Iterations Time/iter (s)

10

30

50

70

1
10
30
50

1
10
30
50

1
10
30
50

1
10
30
50

0.32
0.32
0.32
0.32

0.33
0.33
0.33
0.34

0.34
0.35
0.34
0.34

0.34
0.35
0.36
0.36

110000
95000
80000
70000

80000
70000
55000
35000

60000
50000
30000
25000

45000
30000
25000
25000

0.20
0.23
0.27
0.30

0.26
0.31
0.39
0.61

0.36
0.44
0.71
0.81

0.49
0.72
0.86
0.96

ACT

φ

100
91
97
90

34
18
18
12

33
10
10
12

29
7.3
7.3
5.9

η

170
140
150
140

68
26
27
19

69
15
15
17

61
11
9.3
7.8

c

2.5
2.4
2.5
2.7

2.3
2.3
2
2.4

1.7
2.1
1.8
1.8

2.2
1.6
1.6
1.8

ACT × time
η
c

γ

0.5
0.55
0.68
0.81

0.6
0.71
0.78
1.5

0.61
0.92
1.3
1.5

1.1
1.2
1.4
1.7

20
21
26
27

8.8
5.6
7
7.3

12
4.4
7.1
9.7

14
5.3
6.3
5.7

34
32
41
42

18
8.1
11
12

25
6.6
11
14

30
7.9
8
7.5

Table 3: Performance of method ENS2.

13

Method

KF

ENS1
ENS2

c

0.2300 (± 0.0004)
0.2311 (± 0.0006)
0.2306 (± 0.0008)

γ

4.3265 (± 0.0053)
4.3228 (± 0.0017)
4.3303 (± 0.0025)

η

-3.7015 (± 0.0054)
-3.6986 (± 0.0015)
-3.7034 (± 0.0021)

Table 4: Estimates of posterior means, with standard errors of posterior means shown in brackets.

The results show that using a small ensemble (10 or so pool states) over η is particularly
helpful. One reason for this improvement is the ability to use the caching technique to make
these updates computationally cheap. A more basic reason is that updates of η consider the
entire collection of latent sequences, which allows us to make large changes to η, compared to the
Metropolis updates.

Even though the ENS2 method in this case is outperformed by the ENS1 method, we have
only applied it to one data set and there is much room for further tuning and improvement
of the methods. A possible explanation for the lack of substantial performance gain with the
ensemble method is that conditional on a single sequence, the distribution of φ has standard
deviation comparable to its marginal standard deviation, which means that we can’t move too
much further with an ensemble update than we do with our Metropolis updates. An indication
of this comes from the acceptance rate for ensemble updates of γ in ENS2, which we can see isn’t
improved by much as more pool states are added.

Parameter estimates for the best performing KF, ENS1 and ENS2 settings are presented in
Table 4. These estimates were obtained by averaging samples from all 5 runs with 10% of the
sample discarded as burn-in. We see that the diﬀerences between the standard errors are in
approximate agreement with the diﬀerences in autocorrelation times for the diﬀerent methods.

4 Conclusion

We found that noticeable performance gains can be obtained by using ensemble MCMC based
sampling methods for the stochastic volatility model. It may be possible to obtain even larger
gains on diﬀerent data sets, and with even better tuning. In particular, it is possible that the
method of updating φ with an ensemble, or some variation of it, actually performs better than a
single sequence method in some other instance.

The method of Kastner and Fruwirth-Schnatter (2014) relies on the assumption that the state
process is linear and Gaussian, which enables eﬃcient state sequence sampling using Kalman
ﬁlters. The method would not be applicable if this was not the case. However, the ensemble
method could still be applied to this case as well.
It would be of interest to investigate the
performance of ensemble methods for stochastic volatility models with diﬀerent noise structures
for the latent process. It would also be interesting to compare the performance of the ensemble
MCMC method with the PMCMC-based methods of Andrieu et. al (2010) and also to see whether
techniques used to improve PMCMC methods can be used to improve ensemble methods and vice
versa.

14

Multivariate versions of stochastic volatility models, for example those considered in Scharth
and Kohn (2013) are another class of models for which inference is diﬃcult, and that it would
be interesting to apply the ensemble MCMC method to. We have done preliminary experiments
applying ensemble methods to multivariate stochastic volatility models, with promising results.
For these models, even though the latent process is linear and Gaussian, due to a non-constant
covariance matrix the observation process does not have a simple and precise mixture of Gaussians
approximation.

Acknowledgements

This research was supported by the Natural Sciences and Engineering Research Council of
Canada. A. S. is in part funded by an NSERC Postgraduate Scholarship. R. N. holds a Canada
Research Chair in Statistics and Machine Learning.

References

Andrieu, C., Doucet, A. and Holenstein, R. (2010). “Particle Markov chain Monte Carlo meth-

ods”, Journal of the Royal Statistical Society B, vol. 72, pp. 269-342.

Kastner, G. and Fruhwirth-Schnatter, S. (2014). “Ancillarity-suﬃciency interweaving strategy
(ASIS) for boosting MCMC estimation of stochastic volatility models”, Computational Statis-
tics & Data Analysis, vol. 76, pp. 408-423.

Kim, S., Shephard, N. and Chib, S. (1998). “Stochastic volatility:

likelihood inference and

comparison with ARCH models”, Review of Economic Studies. vol. 65, pp. 361-393.

Lindsten, F. and Schon, T. B. (2013). “Backward simulation methods for Monte Carlo statistical

inference”, Foundations and Trends in Machine Learning. vol. 6(1), pp. 1-143.

Liu, J.S. and Sabatti, C. (2000). “Generalized Gibbs sampler and multigrid Monte Carlo for

Bayesian computation”, Biometrika, vol. 87, pp. 353-369.

Liu, J.S. and Wu, Y.N. (1999). “Parameter expansion for data augmentation”, Journal of the

American Statistical Association vol. 94, pp. 1264-1274.

Neal, R. M. (2003). “Markov Chain Sampling for Non-linear State Space Models using Embedded
Hidden Markov Models”, Technical Report No. 0304, Department of Statistics, University of
Toronto, http://arxiv.org/abs/math/0305039.

Neal, R. M., Beal, M. J., and Roweis, S. T. (2004). “Inferring state sequences for non-linear
systems with embedded hidden Markov models”, in S. Thrun, et al (editors), Advances in
Neural Information Processing Systems 16, MIT Press.

15

Neal, R. M. (2010). “MCMC Using Ensembles of States for Problems with Fast and Slow Variables
such as Gaussian Process Regression”, Technical Report No. 1011, Department of Statistics,
University of Toronto, http://arxiv.org/abs/1101.0387.

Omori, Y., Chib, S., Shephard, N. and Nakajima, J. (2007). ”Stochastic volatility model with
leverage: fast and eﬃcient likelihood inference”, Journal of Econometrics, vol. 140-2, pp. 425-
449.

Petris, G., Petrone, S. and Campagnoli, P. (2009). Dynamic Linear Models with R, Springer:

New York.

Scharth, M. and Kohn, R. (2013). “Particle Eﬃcient Importance Sampling”, arXiv preprint

1309.6745v1.

Shestopaloﬀ, A. Y. and Neal, R. M. (2013). “MCMC for non-linear state space models using

ensembles of latent sequences”, Technical Report, http://arxiv.org/abs/1305.0320.

Yu, Y. and Meng, X. (2011). “To Center or Not to Center, That is Not the Question: An
Ancillarity-Suﬃciency Interweaving Strategy (ASIS) for Boosting MCMC Eﬃciency”, Journal
of Computational and Graphical Statistics, vol. 20 (2011), pp. 531-570.

Appendix

Here, we derive the suﬃcient statistics for the stochastic volatility model in the two parametriza-
tions and the likelihoods in terms of suﬃcient statistics.

For NC, we derive low-dimensional suﬃcient statistics for φ as follows

p(x|φ) ∝ (cid:112)
∝ (cid:112)
∝ (cid:112)

1 − φ2 exp

1 − φ2 exp

1 − φ2 exp

(cid:16) − (1 − φ2)x2
(cid:16) − (x2
(cid:16) − (φ2

1 − φ2x2
N−1(cid:88)

1/2) exp(− N(cid:88)
N(cid:88)
i − 2φ
x2
N(cid:88)

1 +

i=2

i=2

N(cid:88)
N(cid:88)

i=2

i=2

i=2

i=1

xixi−1 +

x2
i )/2

(xi − φxi−1)2/2

xixi−1 + φ2

(cid:17)

x2
i−1)/2

(cid:17)
N(cid:88)

i=2

(cid:17)

i − 2φ
x2
N(cid:88)

i=2

16

Letting

we can write

x2
i ,

t1 =

N(cid:88)
p(x|φ) ∝ (cid:112)

i=1

t2 =

xi−1xi,

t3 = x2

1 + x2
N

1 − φ2 exp(−(φ2(t1 − t3) − 2φt2 + t1)/2)

(cid:16) − N(cid:88)

((˜xi − (c + φ(˜xi−1 − c))2/2σ2(cid:17)

(cid:16) − (1 − φ2)(˜x1 − c)2/2σ2) exp
(cid:16) − (˜x2

1 − φ2 ˜x2

i=2

1 − 2˜x1c + c2 + 2cφ2 ˜x1 − c2φ2 +

N(cid:88)
(c + φ(˜xi−1 − c))2)/2σ2(cid:17)
N(cid:88)

i=2

˜x2
i

1 − 2˜x1c + c2 + 2cφ2 ˜x1 − c2φ2 − 2c

˜xi + (N − 1)c2 + 2cφ

(˜xi−1 − c) + φ2

N(cid:88)

i=2

˜xi

N(cid:88)
(˜xi−1 − c)2)/2σ2(cid:17)
N(cid:88)
N(cid:88)

i=2

For C, we have

p(x|c, φ, σ2) ∝

∝

∝

∝

∝

exp

exp

exp

i=2

σn/2

σn/2

σn/2

−2

(cid:112)1 − φ2
(cid:112)1 − φ2
N(cid:88)
(cid:112)1 − φ2
N(cid:88)
(cid:112)1 − φ2
N(cid:88)
N(cid:88)
(cid:112)1 − φ2
N−1(cid:88)

−2φ

−2φ

+φ2

σn/2

σn/2

i=2

i=2

i=2

+4cφ

Letting

N(cid:88)

i=1

˜x2
i ,

˜t1 =

we can write

p(x|c, φ, σ2) =

˜xi(c + φ(˜xi−1 − c)) +

(cid:16) − (
N(cid:88)

i=1

i=2

i − φ2 ˜x2
˜x2
N(cid:88)

˜xi ˜xi−1 + 2cφ

(cid:16) − (
N(cid:88)

exp

i=1

˜xi ˜xi−1 + 2cφ

1 − 2˜x1c + c2 + 2cφ2 ˜x1 − c2φ2 − 2c

˜xi

i=2

˜xi−1 − 2(N − 1)c2φ)

i=2

i=2

˜xi + (N − 1)c2 + 2cφ

i − φ2 ˜x2
˜x2
N(cid:88)
N(cid:88)
˜xi−1 + (N − 1)c2φ2)/2σ2(cid:17)
N(cid:88)
N−1(cid:88)

N(cid:88)

i=2

i=2

˜xi

i=2

exp

i=1

i=2

i=2

i=2

i=2

i=1

˜x2
i + φ2

˜xi − 2c

N−1(cid:88)

i − 2φ
˜x2

˜xi−1 ˜xi + 2cφ2

i−1 − 2cφ2
˜x2
(cid:16) − (
N(cid:88)
N(cid:88)
˜xi + 2cφ(˜x1 + ˜xN ) + (N − 1)(c(φ − 1))2 + c2(1 − φ2))/2σ2(cid:17)
N(cid:88)
N−1(cid:88)
(cid:112)1 − φ2
(cid:16) − (˜t1 + φ2˜t2 − 2φ˜t3 − 2cφ2˜t4 − 2c(˜t4 + ˜t5)
+4cφ˜t4 + 2cφ˜t5 + (N − 1)(c(φ − 1))2 + c2(1 − φ2))/2σ2(cid:17)

˜t5 = ˜x1 + ˜xN

N−1(cid:88)

˜xi−1 ˜xi

˜t4 =

˜t3 =

σn/2

exp

˜x2
i

˜xi

i=2

i=2

i=2

˜t2 =

17

