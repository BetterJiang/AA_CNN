Volume 2

Number 3

2007

Bayesian Analysis

Splitting and Merging Components of a Nonconjugate Dirichlet Process Mixture
Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . S. Jain and R. M. Neal

Comment on Article by Jain and Neal . . . . . . . . . . . . . . . . . . . . . . . . . David B. Dahl

Comment on Article by Jain and Neal . . . . . . . . . . . . . . . . . . . . . . . . . . C. P. Robert

445

473

479

Comment on Article by Jain and Neal . . . . . . . . . . . . . . . . . . . . . S. N. MacEachern 483

Rejoinder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . S. Jain and R. M. Neal

495

Hidden Markov Dirichlet Process: Modeling Genetic Inference in Open Ancestral

Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E. P. Xing and K. Sohn 501

Re-considering the variance parameterization in multiple precision models . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Y. He, J. S. Hodges, and B. P. Carlin 529

Cluster Allocation Design Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A. M. Madrigal

557

Bayesian Hierarchical Multiresolution Hazard Model for the Study of Time-Dependent

Failure Patterns in Early Stage Breast Cancer . . . . . . V. Duki(cid:19)c and J. Dignam 591

A Spatially-adjusted Bayesian Additive Regression Tree Model to Merge Two
Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . S. Zhang, Y. Shih, and P. M(cid:127)uller

611

Bayesian Analysis (2007)

2, Number 3, pp. 445{472

Splitting and Merging Components of a

Nonconjugate Dirichlet Process Mixture Model

Sonia Jain(cid:3) and Radford M. Nealy

Abstract. The inferential problem of associating data to mixture components is dif-
(cid:12)cult when components are nearby or overlapping. We introduce a new split-merge
Markov chain Monte Carlo technique that e(cid:14)ciently classi(cid:12)es observations by splitting
and merging mixture components of a nonconjugate Dirichlet process mixture model.
Our method, which is a Metropolis-Hastings procedure with split-merge proposals, sam-
ples clusters of observations simultaneously rather than incrementally assigning observa-
tions to mixture components. Split-merge moves are produced by exploiting properties
of a restricted Gibbs sampling scan. A simulation study compares the new split-merge
technique to a nonconjugate version of Gibbs sampling and an incremental Metropolis-
Hastings technique. The results demonstrate the improved performance of the new
sampler.

Keywords: Bayesian model, Markov chain Monte Carlo, split-merge moves, nonconjugate prior

1 Introduction

Bayesian mixture models have gained in popularity as an alternative to traditional
density estimation and clustering techniques. In particular, Bayesian mixture models in
which a Dirichlet process prior de(cid:12)nes the mixing distribution are of interest due to their
(cid:13)exibility in (cid:12)tting a countably in(cid:12)nite number of components (Ferguson (1983)). Much
of the recent research related to the Dirichlet process mixture model has been devoted
to developing computational techniques, usually Markov chain Monte Carlo methods,
to sample from its posterior distribution (Neal (2000), MacEachern and M(cid:127)uller (1998)).
Other techniques to estimate the Dirichlet process model include sequential importance
sampling (MacEachern et al. (1999)) and variational methods (Blei and Jordan (2004)).
The practical utility of these methods is illustrated by their recent use for complex bi-
ological and genetics problems, such as haplotype reconstruction (Xing et al. (2004)),
estimation of rates of non-synonymous and synonymous nucleotide substitutions as evi-
dence for natural selection in evolutionary biology problems (Huelsenbeck et al. (2006)),
and determination of di(cid:11)erential gene expression (Do et al. (2005)).

The focus of this article is on Markov chain sampling for nonconjugate Dirichlet pro-
cess mixture models, building on our previous work for conjugate models (Jain and Neal
(2004)). Conjugate models are appropriate for some problems, which is convenient due

(cid:3)Division of Biostatistics and Bioinformatics, Department of Family and Preventive Medicine, Uni-

versity of California at San Diego, La Jolla, CA, mailto:sojain@ucsd.edu

yDepartment of Statistics and Department of Computer Science, University of Toronto, Toronto,

Ontario, Canada, http://www.cs.toronto.edu/~radford/

c(cid:13) 2007 International Society for Bayesian Analysis

ba0007

446

Splitting and Merging Components of a Nonconjugate DPMM

to the analytical tractability of these priors. However, in many situations, conjugate
priors can be too restrictive. Forcing conjugacy on the model can lead to undesirable
or even nonsensical priors. A classic example is a simple model for normally distributed
data, where conjugacy requires an assumption that the mean and variance are a priori
dependent, which is often unrealistic in actual problems.

Computationally, Markov chain sampling procedures can operate di(cid:11)erently depend-
ing on whether conjugacy is assumed. In the conjugate case, we can analytically in-
tegrate away the mixing proportions for the components and the parameters for each
component. This leads to Markov chain Monte Carlo procedures that update only
the latent indicator variable associating mixture components with data observations
(MacEachern (1994), Neal (1992)). However, in the nonconjugate case, the parameters
of the model cannot be integrated away and must be included in the Markov chain
update. Further, since we lose the advantage of analytic tractability, computational
di(cid:14)culties arise, which makes it more di(cid:14)cult, but not impossible, to construct valid
Markov chain Monte Carlo procedures.

Nonconjugate Markov chain sampling methods based on the Gibbs sampler have
been proposed previously; see, for instance, MacEachern and M(cid:127)uller (1998) and Neal
(2000). When the mixture components are nearby or overlapping, these incremental
samplers (as well as those for conjugate models) su(cid:11)er from computational di(cid:14)culties,
such as remaining stuck in isolated modes and poor mixing between components.

Alternative nonincremental Markov chain samplers for the Dirichlet process mixture
model based on split-merge moves have been proposed by Green and Richardson (2001)
and by ourselves (Jain and Neal (2004)). In a single iteration, these methods can split a
mixture component moving all observations to an appropriate new component, or merge
two distinct components together. The Green and Richardson (2001) method is based
on the reversible-jump procedure, in which numerous ways to propose a split move are
possible. Since speci(cid:12)c moment conditions must be preserved, the split-merge proposals
are model-dependent. Jain and Neal (2004) introduce a Metropolis-Hastings technique
with split-merge proposals for conjugate Dirichlet process mixture models. The inno-
vation in this work is exploiting properties of a Gibbs sampling scan to construct split-
merge moves, such that their Metropolis-Hastings proposals are model-independent. In
this article, we extend the conjugate split-merge technique to a class of nonconjugate
Dirichlet process mixture models by developing a novel scheme to incorporate the model
parameters into the sampling procedure.

This article is organized as follows. Section 2 de(cid:12)nes the nonconjugate Dirichlet
process mixture model. Section 3 brie(cid:13)y describes the Metropolis-Hastings split-merge
technique based on Gibbs sampling proposals. The new split-merge technique for a class
of nonconjugate models is proposed in Section 4. Next, in Section 5, we illustrate the
utility of our method in by comparing it to an auxiliary Gibbs sampling method (Neal
(2000), Algorithm 8). Section 6 is a general discussion and concluding remarks. Details
of a simulation study are provided in the Appendix in Section 7.

S. Jain and R. M. Neal

2 The model

447

The Dirichlet process mixture model takes the following hierarchical model form for
observed data y = (y1; : : : ; yn) that is considered exchangeable:

yi j (cid:18)i (cid:24) F ((cid:18)i)
(cid:18)i j G (cid:24) G

G (cid:24) DP (G0; (cid:11))

(1)

Here, F ((cid:18)i) is a component parameterized by (cid:18)i from a parametric distribution whose
density will be written as f (y; (cid:18)). G is the mixing distribution. G0 de(cid:12)nes a base
distribution for the Dirichlet process (DP ) prior, while (cid:11) is a concentration parameter
that takes values greater than zero. The usual conditional independence assumptions
for a hierarchical model apply, so that the only dependencies are those that are explicitly
shown.

Realizations of the Dirichlet process are discrete with probability one. A conse-
quence of this is that the mixture model in equation (1) can be viewed as a countably
in(cid:12)nite mixture model (Ferguson (1983)). This is evident when we simplify the model
in equation (1) by integrating G over its prior distribution. The (cid:18)i follow a generalized
Polya urn scheme (Blackwell and MacQueen (1973)) and the prior distribution for the
(cid:18)i may be represented by the following conditional distributions:

(cid:18)1 (cid:24) G0

(cid:18)i j (cid:18)1; : : : ; (cid:18)i(cid:0)1 (cid:24)

1

i(cid:0)1+(cid:11)

i(cid:0)1

Xj=1

(cid:14)((cid:18)j ) +

(cid:11)

i(cid:0)1+(cid:11)

G0

(2)

where (cid:14)((cid:18)j ) is the distribution which is a point mass at (cid:18)j .

We can represent the fact that (2) results in some of the (cid:18)i being identical by setting
(cid:18)i = (cid:30)ci , where ci represents the latent class associated with observation i, and all (cid:30)c are
independently drawn from G0. The Polya urn scheme for sampling the (cid:18)i is equivalent
to the following scheme for sampling the latent variables, ci, and associated (cid:30)c:

P (ci = c j c1; : : : ; ci(cid:0)1) =

P (ci 6= cj for all j < i j c1; : : : ; ci(cid:0)1) =

ni;c

i (cid:0) 1 + (cid:11)

(cid:11)

i (cid:0) 1 + (cid:11)

;

for c 2 fcjgj<i

(3)

where ni;c is the number of ck for k < i that are equal to c. The probabilities shown
in (3) de(cid:12)ne the Dirichlet process model. This notation will be employed in subsequent
sections.

3 Jain and Neal’s conjugate split-merge procedure

We have previously introduced a split-merge Metropolis-Hastings procedure for conju-
gate Dirichlet process mixture models (Jain and Neal (2004); Jain (2002)). In the con-
jugate version of the algorithm, we assume that F is conjugate to G0 in equation (1), so

448

Splitting and Merging Components of a Nonconjugate DPMM

the model parameters, (cid:30)c, in addition to the mixing distribution, G, can be integrated
away. The state of the Markov chain consists only of the mixture component indicators,
ci.

This sampler proposes nonincremental moves that can produce major changes to
the con(cid:12)guration of observations to mixture components in a single iteration. The
split-merge proposals are evaluated by a Metropolis-Hastings procedure, in which split
proposals are constructed by exploiting properties of a restricted Gibbs sampling scan
on the component indicators, ci. The Gibbs sampling scan is restricted in that it is
only performed on a subset of the data (the observations associated with the merged
component that is proposed to be split) and will only allocate observations between two
mixture components.

To achieve more reasonable split proposals, several intermediate restricted Gibbs
sampling scans are conducted prior to the (cid:12)nal restricted Gibbs sampling scan, which is
used to calculate the Metropolis-Hastings acceptance probability. The result of the last
intermediate Gibbs sampling scan is denoted as the random launch state, from which the
restricted Gibbs sampling transition probability is explicitly calculated. The number of
intermediate restricted Gibbs sampling scans is considered a tuning parameter of this
algorithm.

Note that for a merge proposal, there is only one way to combine items in two
components to one component. However, deciding whether to accept or reject a merge
proposal requires hypothetical consideration of the reverse split, which requires compu-
tations similar to those done for an actual split. A description of the steps involved in
this algorithm, details to compute the Metropolis-Hastings acceptance probability, and a
discussion of the validity of the conjugate version of the split-merge Metropolis-Hastings
algorithm are provided in Jain and Neal (2004).

4 The nonconjugate split-merge procedure

We adapt Jain and Neal’s conjugate split-merge Markov chain procedure described in
Section 3 to accommodate models with nonconjugate priors. As mentioned earlier,
because conjugate priors are not appropriate for all modeling situations, much of the
recent Bayesian mixture modeling literature has been dedicated to nonconjugate al-
gorithms (for instance, MacEachern and M(cid:127)uller (1998), Green and Richardson (2001),
and Neal (2000)). A major impediment in designing nonconjugate procedures is the
computational di(cid:14)culty that arises when the model is no longer analytically tractable.

We say the model is nonconjugate when G0 is not conjugate to F in the mixture
model (equation 1). Aside from being unable to simplify the state of the Markov chain
by integrating away the model parameters, (cid:30), the main obstacle occurs when trying to
sample for a new mixture component. When a ci is updated, it can be set either to
one of the other components currently associated with some observation or to a new
mixture component. The probability of setting ci to a new component involves the

integral, R F (yi; (cid:30)) dG0((cid:30)), which is analytically intractable in most nonconjugate situ-

S. Jain and R. M. Neal

449

ations. Allowances that some previous nonconjugate methods have made when dealing
with this integral include approximating the true posterior distribution by another sta-
tionary distribution (which can be extremely detrimental) or creating model-speci(cid:12)c ad
hoc algorithms (which fail to generalize well).

Neal (2000) proposed two incremental Markov chain sampling procedures: Gibbs
sampling with auxiliary parameters (Algorithm 8), and an incremental Metropolis-
Hastings technique (Algorithm 5). These are exact Markov chain Monte Carlo methods
that sample the correct posterior distribution and are straightforward to implement.
However, in situations where the mixture components are nearby or similar in struc-
ture, these incremental methods’ performance is analogous to the incremental methods
for conjugate models (see Jain and Neal (2004)). To overcome their problems, such
as remaining stuck in isolated modes and poor mixing between mixture components,
we have developed a nonincremental split-merge alternative. In the next section, we
compare empirically the performance of the new sampler to Neal’s two incremental
algorithms.

In this article, we show how such a nonincremental split-merge procedure can be
applied when the model uses a particular type of nonconjugate prior, the conditionally
conjugate family of priors.
In conditionally conjugate models, it is still impossible

to e(cid:14)ciently compute the integral, R F (yi; (cid:30)) dG0((cid:30)). However, the pair F and G0

are conditionally conjugate in one model parameter if the remaining parameters are
held (cid:12)xed. A well-known instance of this is the following Normal model. Suppose
the observations, y1; : : : ; yn, are distributed as F (yi; (cid:22); (cid:27)2) = Normal(yi; (cid:22); (cid:27)2), and
the prior is G0((cid:22); (cid:27)(cid:0)2) = Normal((cid:22); w; B(cid:0)1) (cid:1) Gamma((cid:27)(cid:0)2; r; R). The distributions,
F (yi; (cid:22); (cid:27)2) and G0((cid:22); (cid:27)(cid:0)2), are conjugate in (cid:22) when (cid:27)2 is (cid:12)xed, and conjugate in (cid:27)2
if (cid:22) is (cid:12)xed. But, the joint posterior distribution is not analytically tractable. For the
sake of brevity, when this nonconjugate Normal-Gamma prior is applied to a Normal
mixture model, we will refer to it as the Normal-Gamma mixture model. Note, however,
that this model using a conjugate prior, in which the mean and variance are a priori
dependent, is sometime referred to similarly.

4.1 Restricted Gibbs sampling split-merge proposals

The conjugate split-merge algorithm of Section 3 cannot be applied directly to the con-
ditionally conjugate case, but the basic mechanism of creating restricted Gibbs sampling
split-merge proposals can still be applied. Since the model parameters, (cid:30)c, cannot be
integrated away, the state of the Markov chain for the split-merge sampler consists of
both the component indicators and model parameters, denoted by (cid:13) = (c; (cid:30)), where
c = (c1; : : : ; cn) and (cid:30) = ((cid:30)c : c 2 fc1; : : : ; cng).

Conditional conjugacy in the model is required so that restricted Gibbs sampling
scans can be performed to allocate observations reasonably between two mixture com-

ponents. During these scans, we do not need to compute the integral, R F (yi; (cid:30)) dG0((cid:30)),

since we are only allocating observations between two known components that have at
least one observation already assigned to them. For a nonconjugate model, a restricted

450

Splitting and Merging Components of a Nonconjugate DPMM

Gibbs sampling scan also updates the parameters for the a(cid:11)ected mixture components,
while holding the parameters of the other components (cid:12)xed. Note that use of a re-
stricted Gibbs sampling scan (and consequently, conditional conjugacy) is only crucial
for the (cid:12)nal Gibbs sampling scan from the launch state, since it allows the Metropolis-
Hastings proposal density can be calculated. The intermediate scans could be replaced
by some other type of Markov chain update.

Due to the inclusion of the model parameters, when two separate components are
being merged to a single component, there is no longer only one possible component
to merge into. The merged component is now de(cid:12)ned by component parameters,
which must be accounted for in the Metropolis-Hastings acceptance probability (in Sec-
tion 4.3). The algorithm addresses this problem by conducting intermediate restricted
Gibbs sampling for the merged component’s parameters to arrive at a launch state (in
a similar fashion as the \split" intermediate Gibbs sampling). From this launch state,
one (cid:12)nal restricted Gibbs sampling scan is performed to obtain the model parameters
of the proposed merged component. The number of intermediate Gibbs sampling scans
for the merged component’s parameters is an additional tuning parameter in this al-
gorithm. In this generalized version of the split-merge algorithm, there are therefore
two launch states, (cid:13) Lsplit and (cid:13)Lmerge , that are necessary in order to calculate Gibbs
sampling transition kernels for the split and merge proposal distributions.

4.2 Restricted Gibbs sampling split-merge procedure for the noncon-

jugate case

Let the state of the Markov chain consist of (cid:13) = (c; (cid:30)) where c = (c1; : : : ; cn) and (cid:30) = ((cid:30)c :
c 2 fc1; : : : ; cng).

1. Select two distinct observations, i and j, at random uniformly.

2. Let S denote the set of observations, k 2 f1; : : : ; ng, for which k 6= i and k 6= j, and

ck = ci or ck = cj .

3. De(cid:12)ne launch states, (cid:13) Lsplit and (cid:13) Lmerge , that will be used to de(cid:12)ne Gibbs sampling

distributions required for the split and merge proposals.

(cid:15) Obtain launch state (cid:13) Lsplit = (cLsplit ; (cid:30)Lsplit ) as follows:

{ If ci = cj , then let c

be set to a new component such that

Lsplit
i

c

c

= ci and c

=2 fc1; : : : ; cng and let c

Lsplit
i
Lsplit
i
pendently with equal probability, to either of the distinct components, c
or c

= cj . For every k 2 S, randomly set c

. Initialize model parameters, (cid:30)

= cj . Otherwise, when ci

Lsplit
k

Lsplit
j

Lsplit
j

6= cj,

let

, inde-
Lsplit
i

Lsplit
Lsplit
i

c

Lsplit
Lsplit
j

c

and (cid:30)

, associated with

Lsplit
j

the two distinct components by drawing new values from their prior distribu-
tion.

{ Modify (cid:13) Lsplit by performing t intermediate restricted Gibbs sampling scans

to update cLsplit , (cid:30)

Lsplit
Lsplit
i

c

, and (cid:30)

Lsplit
Lsplit
j

c

.

(cid:15) Obtain launch state (cid:13) Lmerge = (cLmerge ; (cid:30)Lmerge ) as follows:

S. Jain and R. M. Neal

451

= cLmerge
{ If ci = cj, then let cLmerge
if ci 6= cj , then set cLmerge
= cLmerge
Initialize model parameter, (cid:30)Lmerge

j

j

i

i

c

Lmerge
j

= cj (which is the same as ci). Similarly,
= cj . For every k 2 S, set cLmerge
, associated with the merged component

= cj .

k

by drawing a new value from its prior distribution.

{ Modify (cid:13) Lmerge by performing r intermediate restricted Gibbs sampling scans

to update (cid:30)Lmerge

.

c

Lmerge
j

4. If items i and j are in the same mixture component, i.e. ci = cj , then:

(a) Propose a new assignment of data items to mixture components, denoted as csplit,
and csplit
,
and

in which component ci = cj is split into two separate components, csplit
and propose new values for the corresponding components’ parameters, (cid:30)split

j

i

c

split
i

. De(cid:12)ne each element of the candidate state, (cid:13) split = (csplit; (cid:30)split), as

(cid:30)split

c

split
j

follows:

i

j

= c

= c

Lsplit
i

Lsplit
i
Lsplit
j

(note that c

(cid:15) Let csplit
(cid:15) Let csplit
(cid:15) By conducting one (cid:12)nal Gibbs sampling scan from the launch state, (cid:13) Lsplit ,
or csplit

be set to either component csplit

(which is the same as cj )

=2 fc1; : : : ; cng)

for every observation k 2 S, let csplit
and draw values for the model parameters, (cid:30)split

k

and (cid:30)split

.

j

i

c

split
i

c

split
j

(cid:15) For observations k =2 S [ fi; jg, let csplit

k

= ck, and for c =2 fcsplit

i

; csplit

j

g, let

(cid:30)split
csplit = (cid:30)c.

(b) Compute the proposal densities, q((cid:13) splitj(cid:13)) and q((cid:13)j(cid:13) split), that will be used to

calculate the Metropolis-Hastings acceptance probability.

(cid:15) Calculate the split proposal density, q((cid:13) splitj(cid:13)), by computing the Gibbs sam-
pling transition kernel from the split launch state, (cid:13) Lsplit , to the (cid:12)nal proposed
state, (cid:13) split. The Gibbs sampling transition kernel is the product of the in-
dividual probabilities of setting each element in the launch state to its (cid:12)nal
proposed value during the (cid:12)nal Gibbs sampling scan.

(cid:15) Calculate the corresponding proposal density, q((cid:13)j(cid:13) split), by computing the
Gibbs sampling transition kernel from the merge launch state, (cid:13) Lmerge , to the
original merged con(cid:12)guration, (cid:13). The Gibbs sampling transition kernel is the
product of the probability of setting each element in the original merge state
(in this case, elements of (cid:30)cj ) to its original value in a (hypothetical) Gibbs
sampling scan from the merge launch state.

(c) Evaluate the proposal by the Metropolis-Hastings acceptance probability a((cid:13) split; (cid:13)).
If the proposal is accepted, (cid:13) split becomes the next state in the Markov chain. If
the proposal is rejected, the original con(cid:12)guration and model parameter, (cid:13), remain
as the next state.

5. Otherwise, if i and j are in di(cid:11)erent mixture components, i.e. ci 6= cj, then:

(a) Propose a new assignment of data items to mixture components, denoted as cmerge,
in which distinct components, ci and cj , are combined into a single component, and
propose a new value for the corresponding merged component’s model parameter,
(cid:30)merge
. De(cid:12)ne each element of the candidate state, (cid:13) merge = (cmerge; (cid:30)merge), as
follows:

merge
j

c

452

Splitting and Merging Components of a Nonconjugate DPMM

i

i

= cLmerge
= cLmerge

(cid:15) Let cmerge
(cid:15) Let cmerge
(cid:15) For every observation k 2 S, let cmerge
(cid:15) For observations k =2 S [ fi; jg,

k

j

j

(which is the same as cj )

(which is the same as cj )

(cid:30)merge

cmerge = (cid:30)c.

= cLmerge

(which is the same as cj )

= ck, and for c 6= cmerge, let

j
let cmerge

k

(cid:15) Conduct one (cid:12)nal restricted Gibbs sampling scan from the launch state,

(cid:13) Lmerge , in order to draw a new value for the model parameter, (cid:30)merge

c

merge
j

.

(b) Compute the proposal densities, q((cid:13) mergej(cid:13)) and q((cid:13)j(cid:13) merge), that will be used to

calculate the Metropolis-Hastings acceptance probability.

(cid:15) Calculate the merge proposal density, q((cid:13) mergej(cid:13)), by computing the Gibbs
sampling transition kernel from the merge launch state, (cid:13) Lmerge , to the (cid:12)nal
proposed state, (cid:13) merge. The Gibbs sampling transition kernel is the probability
of setting (cid:30)Lmerge
, via one Gibbs sampling
scan.

to its (cid:12)nal proposed value, (cid:30)merge

Lmerge
j

merge
j

c

c

(cid:15) Calculate the corresponding proposal density, q((cid:13)j(cid:13) merge), by computing the
Gibbs sampling transition kernel from the split launch state, (cid:13) Lsplit , to the
original split con(cid:12)guration, (cid:13). The Gibbs sampling transition kernel is the
product of the probabilities of setting each element in the original split state
to its original value in a (hypothetical) Gibbs sampling scan from the split
launch state.

(c) Evaluate the proposal by the Metropolis-Hastings acceptance probability a((cid:13) merge; (cid:13)).

If the proposal is accepted, (cid:13) merge becomes the next state. If the merge proposal
is rejected, the original con(cid:12)guration and model parameters, (cid:13), remain as the next
state.

4.3 The Metropolis-Hastings acceptance probability

The Metropolis-Hastings acceptance probability (Metropolis et al. (1953), Hastings (1970))
takes the following form when updating (cid:13) = (c; (cid:30)):

a((cid:13) (cid:3); (cid:13)) = min(cid:20)1;

q((cid:13)j(cid:13)(cid:3))
q((cid:13) (cid:3)j(cid:13))

P ((cid:13) (cid:3))
P ((cid:13))

L((cid:13)(cid:3)jy)

L((cid:13)jy) (cid:21)

(4)

where (cid:13)(cid:3) is either (cid:13)split or (cid:13) merge depending on the type of proposal.

The prior distribution, P ((cid:13)), will be a product of the individual prior distributions
for c and (cid:30), since they are a priori independent. As before, the prior distribution
for P (c) will be a product of factors in equation (3). The (cid:30)c for di(cid:11)erent mixture
components are independent. Therefore, the prior distribution for P ((cid:13)) is:

P ((cid:30)c)

P ((cid:13)) = P (c) Yc 2 c
k=1((cid:11)+k(cid:0)1) Yc 2 c
Qn

= (cid:11)D Qc 2 c(nc(cid:0)1)!

g((cid:30)c)

(5)

(6)

S. Jain and R. M. Neal

453

where D is the number of distinct mixture components, nc is the count of items belonging
to mixture component c 2 c, and g((cid:30)c) is the prior probability density function for (cid:30)c
for mixture component c 2 c.

For the split proposal, the appropriate ratio of prior distributions is:

P ((cid:13) split)

P ((cid:13))

= (cid:11)

(nsplit
csplit
i

(cid:0)1)! (nsplit
csplit
j
(nci (cid:0)1)! g((cid:30)ci)

(cid:0)1)! g((cid:30)split
csplit
i

) g((cid:30)split
csplit
j

)

(7)

where (cid:13) is the original state in which i and j belong to the same mixture component,
nsplit
are the number of observations associated with each split component.
csplit
i

and nsplit
csplit
j

The ratio of the prior distributions simpli(cid:12)es because the denominator in equation (6)
and factors not associated with components that are directly involved in the Metropolis-
Hastings update cancel.

For the merge proposal, the prior ratio simpli(cid:12)es to:

P ((cid:13)merge)

P ((cid:13))

=

1
(cid:11)

(nmerge
cmerge
i

(cid:0)1)! g((cid:30)merge
cmerge
i

)

(nci (cid:0)1)! (ncj (cid:0)1)! g((cid:30)ci ) g((cid:30)cj )

(8)

where nmerge
denotes the number of observations associated with the single merged
cmerge
i
component. (cid:13) represents the original state in which items i and j belong to separate
components.

The likelihood, L((cid:13)jy), will be a product over n observations:

L((cid:13)jy) =

f (yk; (cid:30)ck )

n

Yk=1

(9)

L((cid:13)jy) can be expressed as a double product over components, c, and items, k 2 f1; : : : ; ng,
associated with each component:

L((cid:13)jy) =

D

Yc=1 Yk : ck=c

f (yk; (cid:30)c)

(10)

where D is the number of distinct components. This expression to calculate the likeli-
hood is often easier to use in real examples.

Likelihood factors involving items associated with components not directly involved
in the split proposal cancel. The ratio of likelihoods in equation (4) reduces to the
following:

L((cid:13)splitjy)

L((cid:13)jy)

=

Yk : csplit

k =csplit

i

f (yk; (cid:30)split
csplit
i

) Yk : csplit

k =csplit

j

f (yk; (cid:30)ci )

Yk : ck=ci

f (yk; (cid:30)split
csplit
j

)

(11)

454

Splitting and Merging Components of a Nonconjugate DPMM

Likewise, for the merge proposal, the ratio of likelihoods is:

L((cid:13)mergejy)

L((cid:13)jy)

=

k

Yk : cmerge
Yk : ck=ci

f (yk; (cid:30)ci ) Yk : ck=cj

f (yk; (cid:30)merge
cmerge
i

)

=cmerge

i

f (yk; (cid:30)cj )

(12)

The Metropolis-Hastings proposal density, q((cid:13) (cid:3)j(cid:13)), is the restricted Gibbs sampling
transition kernel from launch state (cid:13) L to (cid:12)nal state (cid:13) (cid:3). This is a product of the
conditional probabilities of each individual update of the vector c(cid:3) from cL and the
conditional densities of assigning successive components of (cid:30)L to their (cid:12)nal values, (cid:30)(cid:3).

Typically, for each mixture component, (cid:30) is composed of more than one model
parameter, i.e. each (cid:30)c can be a vector of parameters. For example, in the normal
model, there are two parameters per component, (cid:30)c = ((cid:22)c; (cid:27)2
c ). In a Gibbs sampling
scan, each element of parameter (cid:30)c is updated individually, while holding the other
elements of (cid:30)c (cid:12)xed. A single element of (cid:30)c is updated in a restricted Gibbs sampling
scan by drawing a new value from its full conditional distribution.

We will denote the product of conditional probabilities obtained from one full scan
of restricted Gibbs sampling as PGS . Since (cid:13) is comprised of both c and (cid:30), for clarity,
we can split the Gibbs sampling transition kernel into its factors. The order of updating
the variables does not a(cid:11)ect the validity of the method, but for presentation purposes,
we assume that Gibbs sampling updates (cid:30) (cid:12)rst (as is done in the later examples):

q((cid:13) (cid:3)j(cid:13)) = PGS((cid:30)(cid:3) j (cid:30)L; cL; y) (cid:1) PGS(c(cid:3) j cL; (cid:30)(cid:3); y)

(13)

An individual update of a particular ck is as follows:

P (ck j c(cid:0)k; (cid:30)ck ; yk) =

n(cid:0)k;ck f (yk; (cid:30)ck )

n(cid:0)k;ci f (yk; (cid:30)ci ) + n(cid:0)k;cj f (yk; (cid:30)cj )

(14)

where c(cid:0)k represents the cl for l 6= k in S [ fi; jg, n(cid:0)k;c is the number of cl for l 6= k in
S[fi; jg that are equal to c, and f (yk; (cid:30)c) is the likelihood. Here, ck is restricted to being
either ci or cj . Each time a ck or (cid:30)ck is incrementally modi(cid:12)ed during a restricted Gibbs
sampling scan, it is immediately used in the subsequent Gibbs sampling computation.

The required ratios for the split and merge proposals are shown below in equa-
tions (15) and (16), respectively. For the merge proposal, there is still only one way
to combine items in two components into one component, so PGS(cjcLmerge ; (cid:30); y) = 1
in equation (15). The same is true for P (cmergejcLmerge ; (cid:30)merge; y) in equation (16).
However, since speci(cid:12)c parameters now de(cid:12)ne the mixture components, there are nu-
merous possibilities for choosing a particular mixture component. We address this, in a
similar method as the split scenario, by conducting intermediate Gibbs sampling scans
to decide the value of the merged component’s parameters. One (cid:12)nal Gibbs sampling
scan is conducted from the launch state to calculate the Gibbs sampling transition
kernel.

S. Jain and R. M. Neal

455

The ratio of transition densities for the split proposal is:

q((cid:13)j(cid:13) split)
q((cid:13) splitj(cid:13))

=

=

PGS ((cid:30)split
split
i

c

j(cid:30)

Lsplit
split
i

c

ci

PGS ((cid:30)ci j(cid:30)Lmerge
; cLsplit ; y) PGS ((cid:30)split
split
j

c

; cLmerge ; y) PGS (cjcLmerge ; (cid:30); y)

j (cid:30)

Lsplit
split
j

c

; cLsplit ; y) PGS (csplitjcLsplit ; (cid:30)split; y)

PGS ((cid:30)split

c

split
i

j(cid:30)

Lsplit
split
i

c

; c

PGS ((cid:30)ci j(cid:30)Lmerge

; cLmerge ; y)

ci
Lsplit ; y) PGS ((cid:30)split

j(cid:30)

Lsplit
split
j

c

; c

Lsplit ; y) PGS (c

splitjc

Lsplit ; (cid:30)

split; y)

c

split
j

(15)

To calculate q((cid:13)j(cid:13) split), the same intermediate Gibbs sampling operations that are
performed when proposing a merge must be conducted here to arrive at a suitable
merge launch state, even though no actual merge is performed. The Gibbs sampling
transition probability is calculated from the launch state (which is the last intermediate
Gibbs sampling state) to the original merged state. These operations are necessary to
produce the correct proposal ratios.

For the merge proposal, the ratio of transition densities is:

q((cid:13)j(cid:13)merge)
q((cid:13)mergej(cid:13))

=

=

PGS ((cid:30)ci j(cid:30)

Lsplit
ci

PGS ((cid:30)merge
merge
i

c

; cLsplit ; y) PGS ((cid:30)cj j(cid:30)
j(cid:30)

Lmerge
merge
c
i

Lsplit
cj

; cLsplit ; y) PGS (cjcLsplit ; (cid:30); y)

; cLmerge ; y) PGS (cmergejcLmerge ; (cid:30)merge; y)

PGS ((cid:30)ci j(cid:30)

Lsplit
ci

; cLsplit ; y) PGS ((cid:30)cj j(cid:30)

Lsplit
cj

; cLsplit ; y) PGS (cjcLsplit ; (cid:30); y)

PGS ((cid:30)merge
merge
i

c

j(cid:30)

Lmerge
merge
c
i

; cLmerge ; y)

(16)

To obtain q((cid:13)j(cid:13)merge), we similarly perform the same intermediate Gibbs sampling
moves when proposing a split, even though no actual split is proposed (since it is
already known). This time the Gibbs sampling transition probability is calculated from
the launch state to the original split state. This ensures correct proposal ratios.

The number of intermediate Gibbs sampling scans used to arrive at suitable launch
states for both split and merge proposals are tuning parameters of this algorithm. There
is an additional tuning parameter for the nonconjugate split-merge procedure that is
not present in the conjugate version, which did not require a merge launch state.

4.4 Validity of the algorithm

The nonconjugate split-merge procedure described here is justi(cid:12)ed as a valid two-stage
random Metropolis-Hastings procedure. In the (cid:12)rst stage, we randomly select of obser-
vations i and j to decide which subset of Metropolis-Hastings proposals will be consid-
ered. In the second stage, we randomly select a launch state from among all possible
launch states (given the selection of observations i and j), by means of intermediate
Gibbs sampling scans. We then perform a standard Metropolis-Hastings update with a
proposal distribution that depends on the selection of i and j and on the launch state.

456

Splitting and Merging Components of a Nonconjugate DPMM

As discussed by Tierney (1994), a random selection among transitions (in this case, via
random selection of a proposal distribution) is a valid way of constructing Markov chain
Monte Carlo algorithms, as long as all the transitions that might be selected are valid
on their own.

A subtle clari(cid:12)cation should be pointed out regarding the construction of the Metropolis-

Hastings acceptance probability for the nonconjugate procedure. When a split is pro-
posed from a merged state, only one (cid:30)c is included in the equations, since the merged
component has only one set of parameters associated with it now. We happen to ini-
tially pick (cid:30)cj to be associated with the observations in the merged component, but this
is equivalent to initially selecting (cid:30)ci since the labels are irrelevant. To avoid changing
dimensions when we compute the Metropolis-Hastings acceptance probability, we could
include the appropriate (cid:30)ci terms in the computations. Since (cid:30)ci is an extra parameter
for the merged component that is no longer associated with the data, we choose to
propose a new value for it during the restricted Gibbs sampling scan by drawing from
its prior distribution. This choice conveniently allows the prior density for this term to
implicitly cancel with the corresponding term in the proposal density of the acceptance
probability, showing that the change in dimensionality is not a problem. Consider the
following set-up for the prior and proposal ratios for a split proposal which include the
(cid:30)ci terms. We intentionally omit the likelihoods and indicator terms for simplicity and
space considerations:

P ((cid:30)split
csplit
i

) P ((cid:30)split
csplit
j

)

P ((cid:30)ci ) P ((cid:30)cj )

PGS((cid:30)ci j(cid:30)Lmerge
ci
j(cid:30)Lsplit
csplit
i

PGS((cid:30)split
csplit
i

; cLmerge ) PGS((cid:30)cj j(cid:30)Lmerge
cj
; cLsplit; y)PGS ((cid:30)split
csplit
j

j(cid:30)Lsplit
csplit
j

; cLmerge ; y)

; cLsplit; y)

ci

The proposal factor, PGS ((cid:30)cij(cid:30)Lmerge

; cLmerge ) does not depend on the data, since
the (cid:30)cj factor has been selected earlier to be the merged component’s parameter. There-
fore, a new draw from (cid:30)ci ’s conditional distribution will be equivalent to drawing a new
value from its prior distribution, and this will cancel with the prior term, P ((cid:30)ci ). As
a result, the ratios described earlier do not need to include these terms. The identical
situation occurs in the case when a merge is proposed from an original split state and
is handled similarly.

Note that it is possible to propose any con(cid:12)guration of observations from any ini-
tial state via a sequence of split and then merge proposals. However, to ensure (cid:30)-
irreducibility on a continuous state space, it must be possible to propose any set of
parameter values for each component. This will be true if each individual restricted
Gibbs sampling conditional distribution for parameters of components that are involved
in a particular split or merge update has a positive probability density of proposing any
value. To ensure that the split-merge algorithm is well-de(cid:12)ned, the model should satisfy
the condition that the distributions F (yi; (cid:18)i) be mutually absolutely continuous for all
(cid:18) in the support of G0.

S. Jain and R. M. Neal

457

5 Performance of the nonconjugate split-merge proce-

dure

Suppose we consider a Normal mixture model, in which the data, y = (y1; : : : ; yn), are
independent and identically distributed, such that each observation, yi, given the class,
ci, has m Normally distributed attributes, (yi1; : : : ; yim). An observation’s attributes
are independent given the class, ci. The Normal mixture model is commonly used in
Bayesian mixture analysis because of its simplicity in constructing conditional distribu-
tions and (cid:13)exibility in modeling a number of heterogeneous populations simultaneously.

5.1 The Normal mixture model with Normal-Gamma prior

We model data from a mixture of Normal distributions using a Dirichlet process mixture
model with Normal-Gamma prior, as follows:

yi j (cid:22)i; (cid:28)i (cid:24) F (yi; (cid:22)i; (cid:28)i) = N (yi; (cid:22)i; (cid:28) (cid:0)1
((cid:22)i; (cid:28)i) j G (cid:24) G

i

I m)

G (cid:24) DP (G0; (cid:11))

G0((cid:22); (cid:28) ) = N ((cid:22); w; B(cid:0)1) (cid:1) Gamma ((cid:28) ; r; R)

(17)

where (cid:28) , the precision parameter, is (cid:27)(cid:0)2. Hyperpriors could be placed on w; B; r, and
R to add another stage to this hierarchy if desired. Here, we consider these parameters
to be known.

The probability density function for the prior distribution of (cid:22) given in (17) is:

g((cid:22) j w; B) =(cid:18) B
2(cid:25)(cid:19)

1

2

exp(cid:18) (cid:0)B

2

((cid:22) (cid:0) w)2(cid:19)

where B is a precision parameter.

The probability density function for the prior for (cid:28) is:

g((cid:28) j r; R) =

1

Rr (cid:0)(r)

R (cid:19)
(cid:28) r(cid:0)1exp(cid:18) (cid:0)(cid:28)

(18)

(19)

This parameterization of the Gamma density is adopted throughout this section.

These priors, equations (18) and (19), are necessary to compute the priors for the

parameters in the Metropolis-Hastings acceptance probability of equation (4).

It is straightforward to set up the conditional distributions required for the restricted
Gibbs sampling in the split-merge procedure used in the Metropolis-Hastings proposal
densities. For the model parameters, this amounts to sampling from the marginal
posterior distributions for a particular parameter of component c. The conditional
posterior distribution for (cid:22)ch (when (cid:28)ch is known) for a speci(cid:12)c attribute h is:

(cid:22)ch j c; y; (cid:28)ch; w; B (cid:24) N(cid:18) w B + (cid:22)ych nc (cid:28)ch

B + nc (cid:28)ch

;

1

B + nc (cid:28)ch(cid:19)

(20)

458

Splitting and Merging Components of a Nonconjugate DPMM

where nc is the number of observations belonging to component c and (cid:22)ych is the mean
of these observations for attribute h.

Similarly, if (cid:22)ch is (cid:12)xed, the conditional posterior distribution for (cid:28)ch for a particular

attribute h is:

(cid:28)ch j c; y; (cid:22)ch; r; R (cid:24) Gamma0
BBB@

r +

nc
2

;

R(cid:0)1 +

1

1

2 Xk:ck=c

(ykh (cid:0) (cid:22)ch)2

1
CCCA

(21)

The conditional posterior distribution for an indicator variable, ci, is obtained by
combining the probability of the data (given in equation 17) given a value for ci with
the prior for indicators, P (c). This yields for c 2 fcjgj6=i:

P (ci = c j c(cid:0)i; (cid:22)c; (cid:28)c; yi) / P (ci = c j c(cid:0)i) (cid:1) P (yi j (cid:22)c; (cid:28)c; c(cid:0)i)

(22)

/ n(cid:0)i;c

(cid:28)

m

Yh=1

1

2

ch exp(cid:18) (cid:0)(cid:28)ch

2

(yih (cid:0) (cid:22)ch)

2(cid:19)

These conditional distributions are also employed in computations required for Gibbs
sampling with auxiliary parameters and incremental Metropolis-Hastings updates that
will be used as comparisons to the nonconjugate split-merge technique later in this
article.

The likelihood used in computing acceptance probabilities for split-merge updates
is much simpler to obtain than in the conjugate case, since the parameters are not inte-
grated away. For the mixture of Normals, the likelihood (given component indicators)
is

L((cid:13)jy) =

D

Yc=1 Yk : ck=c

1

2

m

Yh=1 (cid:16) (cid:28)ch
2(cid:25)(cid:17)

exp(cid:18) (cid:0)(cid:28)ch

2

(ykh (cid:0) (cid:22)ch)2(cid:19)

Interchanging the products over k and h of equation (23) yields the following:

L((cid:13)jy) =

nc
2

D

Yc=1

m

Yh=1 (cid:16) (cid:28)ch
2(cid:25)(cid:17)

exp  (cid:0)(cid:28)ch

2 Xk:ck =c

(ykh (cid:0) (cid:22)ch)2!

(23)

(24)

5.2

Illustration: Beetle Data

The Dirichlet process mixture model is a useful tool in model-based, unsupervised cluster
analysis. We illustrate the practical utility of our split-merge algorithm with a six-
dimensional data set from Lubischew (1962) that has been previously used by West et al.
(1994). The data consists of six measurements of physical characteristics of three species

S. Jain and R. M. Neal

459

of male beetles for a total of n = 74 beetles. The three species are chactocnema
concina, chactocnema heikertinger, and chactocnema heptapotamica, in which nconc =
21, nheik = 31, and nhept = 22.

The measurements for the ith beetle are denoted as: yij = (yi1; : : : ; yi6) for i =

(1; : : : ; 74). The six measurements are:

y:1 = width of the (cid:12)rst joint
y:2 = width of the second joint
y:3 = maximal width of the aedeagus
y:4 = front angle of the aedeagus
y:5 = maximal width of the head
y:6 = aedeagus side-width

^(cid:22)1 = 177:3
^(cid:22)2 = 124:0
^(cid:22)3 = 50:4
^(cid:22)4 = 134:8
^(cid:22)5 = 13:0
^(cid:22)6 = 95:4

^(cid:27)2
1 = 865:1
^(cid:27)2
2 = 71:9
^(cid:27)2
3 = 7:6
^(cid:27)2
4 = 107:1
^(cid:27)2
5 = 4:6
^(cid:27)2
6 = 204:6

The objective of our analysis is to recover the three latent classes corresponding to the
three di(cid:11)erent species of beetles without using the species information in the analysis.
We apply the Normal-Gamma Dirichlet process mixture model to this data, identical to
equation 17. The Dirichlet process parameter, (cid:11), is set to one. The values for the priors
of the parameters have been set for each dimension as follows: wj = (w1; : : : ; w6) =
(100; 100; 50; 100; 25; 100), B(cid:0)1
6 ) = (500; 100; 25; 100; 25; 150) where B
is a precision parameter, r = 1 across all six dimensions, and R = 5 across all six
dimensions.

1 ; : : : ; B(cid:0)1

j = (B(cid:0)1

We applied the nonconjugate split-merge algorithm (5,1,1,5), in which (cid:12)ve interme-
diate Gibbs sampling scans were each used to reach the launch states for the split and
merge proposals. One split-merge update was used in a single iteration and one (cid:12)nal
incremental Gibbs sampling scan was conducted after the (cid:12)nal split-merge update. For
comparison purposes, we considered the Gibbs sampling technique of Neal (2000) with
v = 3 auxiliary components to this data. Computation time per iteration is similar
for both algorithms. For each algorithm, results are provided for the case in which all
observations are initially assigned to the same mixture component, and each algorithm
is run for 5000 iterations.

From the two top trace plots given in Figure 1, it is evident that Gibbs sampling is
unable to separate the data and leaves all observations in the same mixture component.
It is clear that Gibbs sampling will take longer to reach equilibrium. On the other
hand, split-merge splits the data into three major clusters (corresponding to the correct
proportion of observations to species, i.e. 42%, 30% and 28%.) within the (cid:12)rst twenty
iterations.

To generate the two bottom trace plots in Figure 1, we set the prior values of wj
and B(cid:0)1 to be more re(cid:13)ective of the data. The values used are: wj = (w1; : : : ; w6) =
(100; 100; 50; 100; 10; 100) and B(cid:0)1
Gibbs sampling does recover the three di(cid:11)erent species groups almost immediately, it is
important to note that it becomes stuck in a low probability two-component con(cid:12)gura-
tion and mixes poorly. However, split-merge continues to mix well in a three-component
con(cid:12)guration.

6 ) = (800; 100; 10; 100; 10; 200). While

1 ; : : : ; B(cid:0)1

j = (B(cid:0)1

As a (cid:12)nal check, the simulations were repeated by starting the simulation from

460

Splitting and Merging Components of a Nonconjugate DPMM

a typical state of the competing method’s apparent equilibrium distribution. Gibbs
sampling stayed in the three-component state that it was started from, con(cid:12)rming that
the three-component state has high posterior probability, and that the di(cid:11)erence seen
is not the result of some bug in the split-merge procedure. When the simulations were
repeated using an initial state in which each observation is in a di(cid:11)erent component,
the Gibbs sampler is able to reach equilibrium sooner and performs better.

The results from the beetle data illustration show that Gibbs sampling experiences
a long burn-in time compared to the nonconjugate split-merge technique and is not
always suitable for high-dimensional analysis. While it is true that the values of the
priors for the parameters may not be ideal and that more realistic values may yield
better sampling, often in real data analysis, there is no a priori information to suggest
reasonable priors. A Markov chain Monte Carlo technique that can overcome poor
choices in priors is preferred, as illustrated here, since this leads to shorter burn-in
times and full exploration of the posterior distribution.

6 Discussion

The nonincremental split-merge procedure for nonconjugate models introduced in this
article avoids the problem of being trapped in local modes, allowing the posterior dis-
tribution to be fully explored. In general, the nonconjugate split-merge procedure can
become computationally expensive, but when Gibbs sampling or some other incremen-
tal procedure fails to reach equilibrium in a sensible amount of time, this procedure
becomes necessary. Another related issue is burn-in time. Even if an incremental pro-
cedure reaches stationarity within a desired time limit, one must often discard a large
number of early iterations, which can lead to poor estimates. In split-merge type sit-
uations, the computational burden of using a nonincremental procedure is o(cid:11)set by its
quick burn-in and dramatic improvement in performance. To further improve sampling
performance in which both large changes to the clustering con(cid:12)guration and small re(cid:12)ne-
ments are required, we recommend combining split-merge and Gibbs sampling updates
as a way to reap the bene(cid:12)ts of both samplers.

In higher dimensions, split-merge procedures continue to work well as the compo-
nents are moved closer together. Convergence to the equilibrium distribution is rela-
tively quick. It is possible that the split-merge procedure may break down for very high
dimensional problems, because appropriate splits will be rejected, since it will become
unlikely that a merge operation from the split state would produce the same merged
parameter values as the current state. However, we have not encountered an example
of this. Perhaps this issue arises only in situations where the dimensionality is in the
hundreds.

A possible extension of the split-merge technique is to employ the Dahl (2003) se-
quentially allocated split-merge sampler as a method to initialize the intermediate Gibbs
sampling step. This method could potentially provide a better starting state than our
method of performing a random split of items and selecting values for the parameters
from the prior.

S. Jain and R. M. Neal

7 Appendix

461

The purpose of the following simulation study is to classify observations into appropriate
latent classes using the Normal-Gamma Dirichlet process mixture model. We can make
this problem computationally more di(cid:14)cult by increasing the dimensionality of the data
and by moving the components closer together. Various combinations of these factors
were tested on all procedures. We found that the split-merge procedures outperformed
the incremental procedures even in very low-dimensional problems, in which distinct
components were visible by eye, showing the di(cid:14)culty that incremental samplers have
in reaching equilibrium even in simple problems when the components are similar.

We will consider two simulated data sets with a (cid:12)nite number of components. We
expect that the Dirichlet process mixture model will model the (cid:12)nite situation perfectly
well without problems such as over(cid:12)tting, even though the model allows an in(cid:12)nite
number of components. For each of the two examples, the data are composed of (cid:12)ve
equally-probable mixture components, in which each component is a distribution over
m dimensions. To maintain uniformity amongst the examples, we generated n = 100
observations, strati(cid:12)ed so that 20 observations came from each of the (cid:12)ve mixture
components.

Data for the two examples were randomly generated from the mixture distributions
shown in Tables 1 and 2. Scatterplots of the data are shown in Figures 2 and 3. A
standard deviation of 0.2 was selected for all Normal distributions, so that only the
means would vary. The (cid:12)rst example holds the dimensionality at two. The second
example di(cid:11)ers from the (cid:12)rst in that the dimensionality is increased to three, and the
components are closer together.
Intentional asymmetry is introduced so that three
components are more similar than the other two. This is intended to test whether the
nonconjugate split-merge techniques can split in three ways.

The Dirichlet process parameter, (cid:11), is set to one for all demonstrations. Recall that
a small value of (cid:11) places stronger belief that the number of mixture components in
the data is likely to be small. The parameters of the priors for the parameters on the
component distributions have been set to the same values over all dimensions as follows:
w = 5, B = 1=12, r = 1, and R = 5. Here, B is a precision parameter. For consistency,
these parameters are (cid:12)xed at these values for all simulations. In actual problems, these
parameters could be set either by prior knowledge or given higher-level priors.

7.1 Performance

For the two examples, two incremental procedures, Gibbs sampling with v = 3 auxil-
iary variables, and an incremental Metropolis-Hastings method, are compared to four
versions of the nonconjugate split-merge procedure. We use four parameters to describe
the various split-merge procedures:

1. Number of intermediate Gibbs sampling scans to reach the launch state for a split

proposal

462

Splitting and Merging Components of a Nonconjugate DPMM

Table 1: True mixture distribution for Example 1.

c
1
2
3
4
5

P (ci = c)

P (yihjci = c); h = 1; 2

0.2
0.2
0.2
0.2
0.2

N(2.0, 0.04) N(3.0, 0.04)
N(3.0, 0.04) N(2.0, 0.04)
N(3.3, 0.04) N(3.3, 0.04)
N(8.0, 0.04) N(9.0, 0.04)
N(9.0, 0.04) N(8.5, 0.04)

Table 2: True mixture distribution for Example 2.

c
1
2
3
4
5

P (ci = c)

P (yihjci = c); h = 1; 2; 3

0.2
0.2
0.2
0.2
0.2

N(2.0, 0.04) N(2.0, 0.04) N(3.0, 0.04)
N(2.0, 0.04) N(3.0, 0.04) N(2.0, 0.04)
N(2.0, 0.04) N(2.5, 0.04) N(2.5, 0.04)
N(8.0, 0.04) N(8.0, 0.04) N(8.0, 0.04)
N(8.0, 0.04) N(9.0, 0.04) N(9.0, 0.04)

S. Jain and R. M. Neal

463

2. Number of split-merge updates done in a single overall iteration

3. Number of complete incremental Gibbs sampling scans after the (cid:12)nal split-merge

update

4. Number of intermediate Gibbs sampling scans to reach the launch state for a

merge proposal

The four split-merge procedures we tested are described using these numbers as Split-
Merge (0,1,0,0), Split-Merge (5,1,0,5), Split-Merge (0,1,1,0), and Split-Merge (5,1,1,5).

We compared the split-merge procedures with both the auxiliary variable and

Metropolis-Hastings incremental samplers because we did not know beforehand which
incremental method would perform better in situations where splits and merges might
be necessary. Performance of the auxiliary variable Gibbs sampling is expected to
improve as we increase the number of auxiliary components, except that it also takes
longer per iteration (Neal (2000)). We did vary this parameter, but will report (cid:12)ndings
for v = 3 for all examples, since this version is comparable to the best version of split-
merge in terms of computation time per iteration. As the incremental (cid:12)nal scan for
the split-merge procedure, Gibbs sampling with one auxiliary variable is used for all
examples.

Performance measures that were considered include trace plots over time (Figures 4
and 5) and computation time per iteration (Table 3). The trace plots show (cid:12)ve values
which represent the fractions of observations associated with the most common, two
most common, three most common, four most common, and (cid:12)ve most common mixture
components. Since each of the (cid:12)ve components appear equally in the samples, if the
true situation were captured exactly, the (cid:12)ve traces would occur at values of 0.2, 0.4,
0.6, 0.8, and 1.0.

For each algorithm, all observations were assigned to the same mixture component
for the initial state, and each algorithm was run for 5000 iterations. All simulations
were performed on Matlab, Version 6.1, on a Dell Precision 530 workstation (which has
a 1.7 GHz Pentium 4 processor). Note that the computation times reported include the
extra time spent due to Matlab’s ine(cid:14)ciencies when copying and incrementally updating
arrays, which are not inherent in the algorithm.

7.1.1 Example 1

The three types of procedures, incremental Metropolis-Hastings, incremental Gibbs sam-
pling with auxiliary variables, and split-merge, correctly classify the data in Figure 2
into (cid:12)ve distinct clusters. The main di(cid:11)erence in performance is the number of burn-in
iterations that must be discarded.

The trace plots in Figure 4 show that Gibbs sampling with three auxiliary param-
eters has fewer burn-in iterations than the incremental Metropolis-Hastings method
(compare 1000 to 3200 burn-in iterations). However, since the incremental Metropolis-
Hastings method is approximately 5.5 times faster per iteration than the auxiliary Gibbs

464

Splitting and Merging Components of a Nonconjugate DPMM

Table 3: Time per iteration (in seconds) for the algorithms tested.

Algorithm

Example 1 Example 2

Incremental M-H
Gibbs Sampling
Split-Merge (0,1,0,0)
Split-Merge (0,1,1,0)
Split-Merge (5,1,0,5)
Split-Merge (5,1,1,5)

0.08
0.45
0.05
0.27
0.16
0.40

0.09
0.60
0.10
0.35
0.24
0.53

sampling method, it actually converges sooner with respect to computation time. Split-
Merge (5,1,0,5) almost immediately splits the data into (cid:12)ve components, but notice
that the proportions do not occur at exactly 0.2 intervals until after the (cid:12)rst thousand
iterations. It takes this procedure longer to move a few singleton observations between
components, since there is no (cid:12)nal incremental update to make these minor adjust-
ments. In (cid:12)ve thousand iterations, it is not clear if Split-Merge (5,1,0,5) has actually
reached the equilibrium distribution. Split-Merge (0,1,0,0) does not reach the equilib-
rium distribution in the (cid:12)ve thousand iterations shown. Because the split and merge
proposals have no intermediate Gibbs sampling scans, the proposals are not expected to
be realistic. Split-Merge (0,1,0,0) is essentially a simple random split procedure, except
that one restricted Gibbs sampling scan is conducted to reach the (cid:12)nal state, which of
course will not lead to reasonable split and merge proposals.

However, either by adding intermediate Gibbs sampling scans (as in the case of Split-
Merge (5,1,0,5)) or adding a (cid:12)nal full incremental scan (as in Split-Merge (0,1,1,0)),
the correct proportion of items in each cluster is established. Split-Merge (0,1,1,0)
eventually reaches the (cid:12)ve component con(cid:12)guration after 500 burn-in iterations. The
(cid:12)nal procedure of Figure 4, Split-Merge (5,1,1,5), (cid:12)nds the (cid:12)ve components immediately,
and it appears that there is negligible burn-in (four iterations). The computation time
per iteration is higher for Split-Merge (5,1,1,5) versus Split-Merge (0,1,1,0) and (5,1,0,5),
but the computation time to equilibrium is much lower.

7.1.2 Example 2

Example 2 has three dimensions and the mixture components are close together. A
perspective scatterplot of the data is given in Figure 3, and it shows that the compo-
nents are di(cid:14)cult to distinguish. Given the priors selected, there is signi(cid:12)cant posterior
probability for both the four and (cid:12)ve mixture component con(cid:12)gurations. Only Split-
Merge (5,1,0,5) and Split-Merge (5,1,1,5) mix between these con(cid:12)gurations, as observed
in Figure 5. The incremental samplers and the split-merge procedures with zero in-
termediate restricted Gibbs sampling scans do not (cid:12)nd the (cid:12)ve components over the
5000 iterations, but are stuck in either two or four components. If each item is initially
assigned to a di(cid:11)erent mixture component (plots not included), these samplers do split
the data into (cid:12)ve components, but take a long time to move to four components, indi-

S. Jain and R. M. Neal

465

cating poor mixing. Here, the problem is that the deletion of a component is rare under
both incremental updates and poor split-merge proposals.

Comparing further the two procedures that appear to converge, the autocorrelation
time for trace 1 is much lower for Split-Merge (5,1,1,5) than Split-Merge (5,1,0,5) (126 vs.
718). For the autocorrelation time of an indicator variable, I26;57, coding if observations
26 and 57 are in the same component, the time is much lower for Split-Merge (5,1,1,5)
(38 vs. 417). Even though both algorithms do mix between the two con(cid:12)gurations and
Split-Merge (5,1,0,5) is faster per iteration, the improvement in autocorrelation time for
Split-Merge (5,1,1,5) cannot be ignored. The extra full scan of incremental sampling
for minor adjustments is worth the computational e(cid:11)ort.

7.1.3 Summary of (cid:12)ndings

It appears that split-merge moves are necessary in nonconjugate problems of this sort.
Incremental samplers perform adequately when the components are distinct clusters in
low dimensions, but as the components become more di(cid:14)cult to distinguish, these sam-
plers take much longer to reach equilibrium. It is important to note that the incremental
samplers begin to break down even in low dimensions. The split-merge procedures are
able to handle three-way splits without any problems, although this is done by two
two-way splits.

The split-merge procedure with several intermediate Gibbs sampling scans followed
by an incremental full scan is the best version of the split-merge procedure. The split-
merge method relies on proposing appropriate new clusters, which is accomplished by
conducting several intermediate scans to reach the split and merge launch states. The
split-merge methods generally have a longer computation time per iteration. However,
in the case of the Gibbs sampling procedure with v = 3 auxiliary parameters, the
best version of the split-merge procedure, Split-Merge (5,1,1,5), is slightly faster in our
implementation (see Table 3). Therefore, there does not appear to be any advantage in
using only incremental procedures for these types of problems.

References
Blackwell, D. and MacQueen, J. B. (1973). \Ferguson distributions via P(cid:19)olya urn

schemes." Annals of Statistics, 1: 353{355. 447

Blei, D. M. and Jordan, M. I. (2004). \Variational methods for the Dirichlet process."
ACM International Conference Proceeding Series: Proceedings of the twenty-(cid:12)rst
international conference on machine learning. Vol. 69, article no. 12. 445

Dahl, D. B. (2003). \An improved merge-split sampler for conjugate Dirichlet process
mixture models." Technical Report 1086, Department of Statistics, University of
Wisconsin. 460

Do, K.-A., M(cid:127)uller, P., and Tang, F. (2005). \A Bayesian mixture model for di(cid:11)eren-

466

Splitting and Merging Components of a Nonconjugate DPMM

tial gene expression." Journal of the Royal Statistical Society: Series C (Applied
Statistics), 54: 627{644. 445

Ferguson, T. S. (1983). \Bayesian density estimation by mixtures of normal distribu-
tions." In Rizvi, H. and Rustagi, J. (eds.), Recent Advances in Statistics, 287{303.
New York: Academic Press. 445, 447

Green, P. J. and Richardson, S. (2001). \Modelling heterogeneity with and without the

Dirichlet process." Scandinavian Journal of Statistics, 28: 355{375. 446, 448

Hastings, W. K. (1970). \Monte Carlo sampling methods using Markov chains and their

applications." Biometrika, 57: 97{109. 452

Huelsenbeck, J. P., Jain, S., Frost, S. W. D., and Pond, S. L. K. (2006). \A Dirichlet
process model for detecting positive selection in protein-coding DNA sequences."
PNAS, 103: 6263{6268. 445

Jain, S. (2002). \Split-Merge Techniques for Bayesian Mixture Models." Unpublished

Ph.D. dissertation, University of Toronto, Department of Statistics. 447

Jain, S. and Neal, R. M. (2004). \A split-merge Markov chain Monte Carlo procedure
for the Dirichlet process mixture model." Journal of Computational and Graphical
Statistics, 13: 158{182. 445, 446, 447, 448, 449

Lubischew, A. (1962). \On the use of discriminant functions in taxonomy." Biometrics,

18: 455{477. 458

MacEachern, S. N. (1994). \Estimating normal means with a conjugate style Dirichlet
process prior." Communications in Statistics: Simulation and Computation, 23: 727{
741. 446

MacEachern, S. N., Clyde, M., and Liu, J. (1999). \Sequential importance sampling
for nonparametric Bayes models: the next generation." The Canadian Journal of
Statistics, 27: 251{267. 445

MacEachern, S. N. and M(cid:127)uller, P. (1998). \Estimating mixture of Dirichlet process
models." Journal of Computational and Graphical Statistics, 7: 223{238. 445, 446,
448

Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H., and Teller, E.
(1953). \Equation of state calculations by fast computing machines." Journal of
Chemical Physics, 21: 1087{1092. 452

Neal, R. M. (1992). \Bayesian mixture modeling." In Smith, C. R., Erickson, G. J.,
and Neudorfer, P. O. (eds.), Maximum Entropy and Bayesian Methods: Proceedings
of the 11th International Workshop on Maximum Entropy and Bayesian Methods of
Statistical Analysis, Seattle 1991, 197{211. Dordrecht: Kluwer Academic Publishers.
446

S. Jain and R. M. Neal

467

| (2000). \Markov chain sampling methods for Dirichlet process mixture models."
Journal of Computational and Graphical Statistics, 9: 249{265. 445, 446, 448, 449,
459, 463

Tierney, L. (1994). \Markov chains for exploring posterior distributions (with discus-

sion)." Annals of Statistics, 22: 1701{1762. 456

West, M., M(cid:127)uller, P., and Escobar, M. D. (1994). \Hierarchical priors and mixture
models, with application in regression and density estimation." In Freeman, P. R.
and Smith, A. F. M. (eds.), Aspects of Uncertainty, 363{386. New York: Wiley. 458

Xing, E., Sharan, R., and Jordan, M. I. (2004). \Bayesian Haplotype Inference via the
Dirichlet Process." ACM International Conference Proceeding Series: Proceedings of
the twenty-(cid:12)rst international conference on machine learning. Vol. 69, article no. 111.
445

Acknowledgments

Sonia Jain’s research is supported by a UCSD Academic Senate Research Grant (Grant Num-
ber RG012H). Radford Neal’s research is supported by the Natural Sciences and Engineering
Research Council of Canada. He holds a Canada Research Chair in statistics and machine
learning. The authors would like to thank the Associate Editor and referees for suggestions
that improved this manuscript.

468

Splitting and Merging Components of a Nonconjugate DPMM

Auxiliary Gibbs Sampling

Split−Merge (5,1,1,5)

1

0.8

0.6

0.4

0.2

0

1

0.8

0.6

0.4

0.2

0

1000

2000
3000
iteration (T)

4000

5000

Auxiliary Gibbs Sampling

1000

2000
3000
iteration (T)

4000

5000

1

0.8

0.6

0.4

0.2

0

1

0.8

0.6

0.4

0.2

0

1000

2000
3000
iteration (T)

4000

5000

Split−Merge (5,1,1,5)

1000

2000
3000
iteration (T)

4000

5000

Figure 1: Trace plots comparing Auxiliary Gibbs Sampling to Split-Merge (5,1,1,5)
for the beetle data using vague priors (top) and realistic priors (bottom). Trace plots
show three traces which represent the fractions of observations associated with the most
common, second most common, and third most common mixture components.

S. Jain and R. M. Neal

469

10

9

8

7

6

5

4

3

2

1

0

0

1

2

3

4

5

6

7

8

9

10

Figure 2: Scatterplot of the data in Example 1

470

Splitting and Merging Components of a Nonconjugate DPMM

10

9

8

7

6

5

4

3

2

1
10

8

6

4

2

4

2

0

0

10

8

6

Figure 3: Scatterplot of the data in Example 2. The two x’s represent observations 26
and 57 used in autocorrelation calculations for an indicator variable.

S. Jain and R. M. Neal

471

Incremental M−H

Gibbs Sampling v=3

1

0.8

0.6

0.4

0.2

0

1

0.8

0.6

0.4

0.2

0

1

0.8

0.6

0.4

0.2

0

1000 2000 3000 4000 5000

Split−Merge (0,1,0,0)

1000 2000 3000 4000 5000

Split−Merge (0,1,1,0)

1000 2000 3000 4000 5000

1

0.8

0.6

0.4

0.2

0

1

0.8

0.6

0.4

0.2

0

1

0.8

0.6

0.4

0.2

0

1000 2000 3000 4000 5000

Split−Merge (5,1,0,5)

1000 2000 3000 4000 5000

Split−Merge (5,1,1,5)

1000 2000 3000 4000 5000

Figure 4: Trace plots of the six algorithms in Example 1.

472

Splitting and Merging Components of a Nonconjugate DPMM

Incremental M−H

Gibbs Sampling v=3

1

0.8

0.6

0.4

0.2

0

1

0.8

0.6

0.4

0.2

0

1

0.8

0.6

0.4

0.2

0

1000 2000 3000 4000 5000

Split−Merge (0,1,0,0)

1000 2000 3000 4000 5000

Split−Merge (0,1,1,0)

1000 2000 3000 4000 5000

1

0.8

0.6

0.4

0.2

0

1

0.8

0.6

0.4

0.2

0

1

0.8

0.6

0.4

0.2

0

1000 2000 3000 4000 5000

Split−Merge (5,1,0,5)

1000 2000 3000 4000 5000

Split−Merge (5,1,1,5)

1000 2000 3000 4000 5000

Figure 5: Trace plots of the six algorithms in Example 2.

Bayesian Analysis (2007)

2, Number 3, pp. 473{478

Comment on Article by Jain and Neal

David B. Dahl(cid:3)

1 Introduction

Sonia Jain and Radford Neal (JN) make a signi(cid:12)cant contribution to the literature on
Markov chain Monte Carlo (MCMC) sampling techniques for Dirichlet process mixture
(DPM) models. The paper presents some very nice ideas and will be on my required
reading list for students working with me. DPM models are widely used for Bayesian
nonparametric analyses and e(cid:14)cient sampling techniques are essential for their routine
application. Incremental samplers for nonconjugate DPM models, such as the Auxiliary
Gibbs sampler in Neal (2000), are easily implemented and potentially very e(cid:14)cient.
Unfortunately, these samplers can also have di(cid:14)culty mixing over the entire sample
space and standard MCMC diagnostics may fail to indicate the problem. JN’s paper
represents a signi(cid:12)cant advance by providing a non-incremental sampler for conditionally
conjugate DPM models.

The authors have a history of in(cid:13)uential papers in this area, including Neal (2000)
and Jain and Neal (2004). Their 2004 paper provided a split-merge sampler for con-
jugate DPM models, where the base distribution G0 in the Dirichlet process prior is
conjugate to the likelihood F . By exploiting this conjugacy, the model parameters of a
cluster can be integrated away. The state of the Markov chain is merely the clustering of
observations. Thus, sampling algorithms for conjugate DPM model attempt to sample
from the posterior clustering distribution.

In nonconjugate DPM models, the model parameters of a cluster cannot be inte-
grated away. Sampling algorithms must simultaneously address the clustering and the
model parameters associated with each cluster. Green and Richardson (2001) were the
(cid:12)rst to propose a split-merge sampler for nonconjugate DPM models. Their procedure is
based on reversible jump MCMC (Green 1995; Richardson and Green 1997) where the
Metropolis-Hastings proposals are model-speci(cid:12)c. In this paper, JN provide an MCMC
sampler that can be generically applied to any conditionally conjugate DPM model.

2 Conditional Conjugate vs. Nonconjugate

It is important to note that conditional conjugacy is a necessary prerequisite for the
application of JN’s sampler. Suppose the model parameters for the cluster containing
observation i are (cid:30)1; : : : ; (cid:30)H with likelihood F (yij(cid:30)1; : : : ; (cid:30)H ) and prior G0((cid:30)1; : : : ; (cid:30)H ).
A DPM model is conditionally conjugate if, for each (cid:30)h 2 f(cid:30)1; : : : ; (cid:30)H g, G0((cid:30)1; : : : ; (cid:30)H )
is conjugate to F (yij(cid:30)1; : : : ; (cid:30)H ) in (cid:30)h. JN’s procedure relies on conditional conjugacy

(cid:3)Department

of

Statistics,

Texas

A&M University,

College

Station,

TX,

http://www.stat.tamu.edu/~dahl

c(cid:13) 2007 International Society for Bayesian Analysis

ba0007

474

Comment on Article by Jain and Neal

and hence their procedure is not applicable to all nonconjugate DPM model. Whether
the conditional conjugacy constraint imposes a practical limitation is perhaps problem-
speci(cid:12)c.

3 Cluster Labels, Set Partition, and Implementation

JN describe their algorithm using notation involving cluster labels c1; : : : ; cn. An al-
ternative way of describing sampling algorithms for DPM models uses set partition
notation. In my experience, the set partition notation provides a straightforward presen-
tation with simple notation. A set partition (cid:25) = fS1; : : : ; Sqg of S0 = f1; : : : ; ng divides
the n integers into mutually-exclusive, non-empty, and exhaustive clusters S1; : : : ; Sq. I
especially (cid:12)nd the set partition notation helpful when translating sampling algorithms
for DPM models to computer code. I use an array of length n whose elements point to
C++ classes representing clusters containing the model parameters and a set of integers
(for the cluster membership).

Regardless of notational preference, readers should be assured that the actual imple-
mentation of JN’s sampler need not be complex. The core of my implementation of JN’s
split-merge procedure is 158 lines of C++ code, whereas the core of my implementation
of the Auxiliary Gibbs sampler is 53 lines of C++ code. The extra time and mental
e(cid:11)ort needed to implement their split-merge sampler can pay large dividends for models
and datasets where the Auxiliary Gibbs sampler is likely to have problems.

4 Initial States & Bene(cid:12)ts of Split/Merge Samplers

I applied JN’s split-merge algorithm to their Normal-Gamma mixture model and the
beetle data used in their example. In the top two plots of JN’s Figure 1, they use their
vague priors with hyperparameters wj = (100; 100; 50; 100; 25; 100),
B(cid:0)1
j = (500; 100; 25; 100; 25; 150), and r = R = 1 across all six dimensions. The top left
plot corresponds to Auxiliary Gibbs sampling and shows that this sampler never moves
away from the con(cid:12)guration with all observations in one cluster. JN contrast that with
their Split-Merge (5,1,1,5) sampler (shown in the top right plot of JN’s Figure 1) which
is able to readily (cid:12)nd the true three-component structure in the data.

In my implementation with 100 di(cid:11)erent random number seeds, the Auxiliary Gibbs
sampler was able to (cid:12)nd the three-component structure in 98 instances and a two-
component structure (hinted at in the bottom left plot of JN’s Figure 1) in the remaining
two instances. Why could my implementation of the algorithm (cid:12)nd the true structure,
but their implementation of the same algorithm could not? The issue was the initial
values of the model parameters. I sampled the initial value of the model parameters from
the prior G0. If, instead, I set the initial values of the model parameters to the sample
means and precisions, I am able to replicate the results of JN in 100 of 100 instances.
Also, if each observation is initially placed in its own cluster, the Auxiliary Gibbs sampler
performed well (regardless of the method used to set the model parameters). My Figure
1 summarizes the results, showing that the problem with the Auxiliary Gibbs sampler

David B. Dahl

475

Initially One Cluster

Initially n Clusters

Model

Parameter

Set to
Sample
Means &
Precisions

Model

Parameter
Sampled
from the

Prior

Figure 1: Trace plots from the Auxiliary Gibbs Sampler for the beetle data using JN’s
vague priors and four typical initial states for the Markov chain. The poor performance
of the Auxiliary Gibbs sampler (shown in top left plot of JN’s Figure 1) is only present
when every observation is initially clustered together and the model parameters are
initially set to the sample means and precisions.

is only present when every observation is initially clustered together and the model
parameters are initially set to the sample means and precisions.

In my experience replicating the JN’s Figure 1, their split-merge sampler is not
sensitive to the initial values. For the beetle data and their Normal-Gamma mixture
model, their sampler immediately (cid:12)nds the true three-component structure as shown in
the plots on the right in JN’s Figure 1.

It is interesting to observe that the posterior distribution apparently has virtually
no support for anything other than three components. Notice that the Split-Merge
(5,1,1,5) sampler never moves away from three clusters (to a con(cid:12)guration with two or
four clusters, for example). The jitter present in the right hand size of JN’s Figure 1 is
due purely to the fact that their split-merge sampler embeds the Auxiliary Gibbs sampler
(whose number of scans per split-merge attempt is given as the the third argument in

476

Comment on Article by Jain and Neal

Algorithm
Gibbs Sampling v = 3
Split-Merge (0,1,0,0)
Split-Merge (0,1,1,0)
Split-Merge (5,1,0,5)
Split-Merge (5,1,1,5)
Seconds per Iteration

Jain & Neal

Dahl

Example 1

Example 2

Example 1

Example 2

1.00
0.11
0.60
0.36
0.89
0.45

1.00
0.17
0.58
0.40
0.88
0.60

1.00
0.17
0.61
0.84
1.32

(0.80, 1.13)
(0.17, 0.18)
(0.60, 0.62)
(0.79, 0.88)
(1.30, 1.35)

5:50 (cid:2) 10(cid:0)4

1.00
0.18
0.61
0.86
1.34

(0.84, 1.12)
(0.17, 0.19)
(0.60, 0.63)
(0.82, 0.89)
(1.32, 1.37)

6:75 (cid:2) 10(cid:0)4

Table 1: Comparison of relative CPU time of the various samplers depending on the
dataset and the implementation. Jain & Neal columns are derived from Table 3. The
Dahl columns show averages from 100 replications and the 2.5th and 97.5th quantiles.
The data have been standardized by the \Seconds per Iteration" row to make them
comparable across computers and programming languages.

the quad specifying the details of their sampler). Thus, the CPU time spent on trying
to merge and split is wasted and time would be better spent on just the Auxiliary Gibbs
sampler. The same can be said concerning the (cid:12)rst simulated dataset in JN’s Figure
4. In contrast, Example 2 (shown in JN’s Figure 5) does provide a compelling case for
the split-merge sampler. It freely moves between four and (cid:12)ve components, whereas the
Auxiliary Gibbs sampler is unlikely to easily switch between four and (cid:12)ve components.

5 Timing

My (cid:12)nal point concerns inherent variability in the implementation of algorithms due to
the chosen programming language and data structures. JN have two simulated datasets
(labeled Example 1 and Example 2) which they use to compare the various samplers. My
Table 1 compares the CPU time of my C++ implementation of the various algorithms
with that of JN’s Matlab implementation. The (cid:12)rst two columns are taken from JN’s
Table 3. The Dahl columns show averages from 100 replications and the 2.5th and 97.5th
quantiles. The important point is the relative performance of the various sampling
algorithms (not the speeds of di(cid:11)erent computers or programming languages), so the
data has been scaled by the \Seconds per Iteration" row. Speci(cid:12)cally, the Auxiliary
Gibbs sampler with three auxiliary parameters (labeled as \Gibbs sampling v = 3") is
set at 1.0 within each column.

Notice that relative CPU time taken by each of the samplers, within an implemen-
tation, is relatively constant across the two example datasets. There are, however, very
di(cid:11)erent relative CPU times across implementations within a dataset. Recall that the
Split-Merge(5,1,1,5) sampler embeds one Auxiliary Gibbs update with one auxiliary
parameter per split-merge attempt. The Split-Merge(5,1,0,5) does not have any embed-
ded Auxiliary Gibbs updates, leading to a 1 (cid:0) 0:36=0:89 = 60% reduction in the CPU
time per iteration for JN’s implementation of Example 1. In contrast, my implemen-
tation of Split-Merge(5,1,0,5) provides only 1 (cid:0) 0:84=1:32 = 36% reduction from my
Split-Merge(5,1,1,5).

David B. Dahl

477

JN (2007) compare an Auxiliary Gibbs sampler with three auxiliary parameters with
their Split-Merge(5,1,1,5) sampler which embeds an Auxiliary Gibbs sampler with one
auxiliary parameter. They chose three and one auxiliary parameters respectively to
make the CPU times comparable per iteration and then run each sampler for a (cid:12)xed
number of iterations. In my experience, additional auxiliary parameters are often not
worth the extra CPU e(cid:11)ort. For the sake of comparison, it might be more useful to
have the number of auxiliary parameters be the same for both samplers. Comparisons
would then be based on a (cid:12)xed CPU time rather than a (cid:12)xed number of iterations.

6 Conclusion

JN have made a signi(cid:12)cant contribution to the literature on sampling algorithms for
DPM models. In implementing their algorithm and model and in using their example
datasets, I found their method can have substantial bene(cid:12)ts over the Auxiliary Gibbs
sampler when used to sample from the posterior distribution of conditionally conjugate
DPM models. Their algorithm is certainly more complicated than the Auxiliary Gibbs
sampler, but perhaps not as di(cid:14)cult as one might initially expect. My experience with
JN’s Figure 1 reinforced the importance of using a variety of starting states, particularly
when using the Auxiliary Gibbs sampler. It was nice to see that initial starting values
were not an issue for JN’s split-merge sampler. Although the relative CPU timings of
JN’s implementation and mine can be quite di(cid:11)erent, the salient point is that split-
merge samplers can (cid:12)nding high-probability regions in posterior distributions that may
be missed by incremental samplers.

References
Green, P. J. (1995). \Reversible jump Markov chain Monte Carlo computation and

Bayesian model determination." Biometrika, 82: 711{732. 473

Green, P. J. and Richardson, S. (2001). \Modelling heterogeneity with and without the

Dirichlet process." Scandinavian Journal of Statistics, 28: 355{375. 473

Jain, S. and Neal, R. M. (2004). \A Split-Merge Markov Chain Monte Carlo Procedure
for the Dirichlet Process Mixture Model." Journal of Computational and Graphical
Statistics, 13(1): 158{182. 473

Neal, R. M. (2000). \Markov Chain Sampling Methods for Dirichlet Process Mixture

Models." Journal of Computational and Graphical Statistics, 9: 249{265. 473

Richardson, S. and Green, P. J. (1997). \On Bayesian Analysis of Mixtures With An
Unknown Number of Components (Disc: P758-792) (Corr: 1998V60 P661)." Journal
of the Royal Statistical Society, Series B, Methodological, 59: 731{758. 473

478

Comment on Article by Jain and Neal

Bayesian Analysis (2007)

2, Number 3, pp. 479{482

Comment on Article by Jain and Neal

C.P. Robert(cid:3)

From a stylistic point of view, I think this paper reads very much like a sequel to
the important paper Jain and Neal (2004) and therefore it is not exactly self-contained
since the main bulk of the paper is a commentary of the program provided in Section
4.2. Instead of the current version, I would thus have preferred a truly self-contained
version with a more user-friendly introduction, for instance when reading and re-reading
Sections 3 and 4.1...1

The central point of the paper is to extend Jain and Neal (2004) so that the lack
of complete conjugacy of the prior does not prevent the algorithm from being run.
Indeed, in Jain and Neal (2004), the model parameters are completely hidden in that the
likelihood and the prior only depend on the cluster index vector c, which means working
in a (cid:12)nite set. The di(cid:14)culty with priors G0 that do not lead to closed form marginals
is that the parameters must take part in the simulation process. The idea at the core
of the current paper is to take advantage of the conditional conjugacy, i.e. the fact that
the prior on a given parameter is still conjugate and thus manageable, conditional on
all the other parameters, so that a Gibbs sampling version can be implemented.

At this stage, I understand the rationale of the partial conjugacy for the Metropolis-
Hastings ratio to be computed (Section 4.1) but I wonder how di(cid:14)cult it would be
to extend the idea to any type of prior distribution.
I also note that at both split
and merge stages the algorithm simulates new values of the parameter from the prior
distribution, rather than from a more adapted distribution. This is as generic as it
can be, but simulating from vague priors usually slows down algorithms and it is of
course impossible for improper priors. It thus seems to me that the factor t directing
the number of intermediate Gibbs (or Metropolis-Hastings) iterations in Step 3 must
be in(cid:13)uential in the overall behaviour of the algorithm and that large values of t may
be necessary to overcome the dependence on the starting value.

More generally, I also wonder why a more global tempering strategy would not fare
better than the local split-merge proposals used in the paper. For illustration purposes,
I implemented below the regular Gibbs sampler in the [BetaBinomial] Example 1 of
Jain and Neal (2004) and compared it with a na(cid:127)(cid:16)ve tempered version where the tem-
pered likelihood L(cid:28) is made of a product of (cid:28) (cid:21) 1 (sub)likelihoods based on a partition
of the observations in (cid:28) random clusters, (cid:28) being itself uniform on f1; : : : ; n=2g. (The
advantages of using this form of tempering are (a) that the same Gibbs sampler can
be used for the sublikelihoods and (b) that the normalising constant of the tempered
version is still available, as opposed to the choice of a power of the likelihood. The ac-
ceptance probability at the end of the tempered moves is then function of the likelihood
ratio L((cid:18)jx)=L(cid:28) ((cid:18)jx) and can be directly computed.) As shown on Figure 1 (bottom),

(cid:3)CREST and CEREMADE, Uni. Paris Dauphine, France, mailto:xian@ceremade.dauphine.fr
1This may explain why the following reads more like an eloped referee’s report than like a true

discussion!

c(cid:13) 2007 International Society for Bayesian Analysis

ba0007

480

Comment on Article by Jain and Neal

explained below, the mixing and the exploration of various likelihood values is quite
improved with this tempered scheme, since no column sticks to a single colour theme.

Since Dirichlet mixtures are closely related to mixtures, I would have liked to
read some discussion on the label switching phenomenon (see, e.g., Stephens 2000;
Marin et al. 2005; Jasra et al. 2005). Indeed, while the original model of Jain and Neal
(2004) is somehow impervious to the issue of label switching, since the clustering pa-
rameterisation only focus on class allocations, the introduction of the parameter in the
game means that a proper exploration of the posterior requires the reproduction of the
symmetry in the various components of the mixture. Using a split-merge basis for this
exploration may then prove to be insu(cid:14)ciently powerful for this task.

In fact, it is close to impossible to judge of the overall convergence performances
from the simulation output, which solely concentrates on the cluster sizes. Addi-
tional graphical summaries would be welcome, like the \allocation map" advertised
in Robert and Casella (2004) and represented on both Figures 1 and 2. The pixelised
lines on the pictures represent the cluster index via di(cid:11)erent colours for all observa-
tions, the index on the (cid:12)rst axis being the index of the observation. The second axis
corresponds to the iteration index. Long vertical stripes of similar colours indicate poor
mixing of the algorithm.

In this illustration, we see clearly that the 5 equal groups of Example 1 of Jain and Neal

(2004) are identi(cid:12)ed by the Gibbs sampler{as signalled by the homogeneous columns
1 (cid:0) 20, 21 (cid:0) 40, 41 (cid:0) 60, 61 (cid:0) 80 and 81 (cid:0) 100|and, furthermore, that label switching
does occur, even if at a very slow pace|as shown by columns 61 (cid:0) 80 for instance.

A point of detail (?) is that the algorithm must be (is) validated as a Gibbs procedure
rather than as a Metropolis-Hastings algorithm, given that at any stage only a subset
of the parameters and of the clustering indicators is updated. In addition, this is quite
an interesting example of algorithmic bypassing the varying dimension pitfalls, since
it avoids dealing with the measure theoretic subtleties encountered by reversible jump
for instance (Green 1995) while being in a continuous varying dimension state space,
contrary to the setup of Jain and Neal (2004).

References

Green, P. (1995). \Reversible jump MCMC computation and Bayesian model determi-

nation." Biometrika, 82(4): 711{732. 480

Jain, S. and Neal, R. (2004). \A Split-Merge Markov Chain Monte Carlo Procedure
for the Dirichlet Process Mixture Model." J. Computat. Graphical Statist., 13(1):
158{182. 479, 480, 481

Jasra, A., Holmes, C., and Stephens, D. (2005). \Markov Chain Monte Carlo Methods
and the Label Switching Problem in Bayesian Mixture Modeling." Statistical Science,
20(1): 50{67. 480

Marin, J., Mengersen, K., and Robert, C. (2005). \Bayesian Modelling and Inference

Robert, C. P.

481

0
0
0
1
 
x
 
s
n
o

i
t

a
r
e

t
i

0
0
0
1

 
x
 
s
n
o

i
t

a
r
e

t
i

0
8

0
6

0
4

0
2

0

0
1

8

6

4

2

0

20

40

60

80

100

observation index

0
8
2
−

0
2
3
−

0
6
3
−

0
8
2
−

0
2
3
−

0
6
3
−

20

40

60

80

100

observation index

Figure 1: (top) Allocation map of the simulated cluster index vector c(t) for m = 6,
n = 100 observations and T = 105 Gibbs iterations (subsampled every 1000 iteration),
in the setup of Example 1 of Jain and Neal (2004). The colours used in the graphs range
from red (1) to white (6) and identify the labels of the cluster indicators ci along the
iterations. The superimposed graph is the corresponding sequence of likelihood values
over the T = 105 Gibbs iterations, associated with the scale on the right hand side.
(bottom) Same representation for a tempered version with T = 103 iterations made of
To = 102 tempered moves.

482

Comment on Article by Jain and Neal

s
n
o
i
t
a
r
e
t
i

5
0
+
e
1

4
0
+
e
8

4
0
+
e
6

4
0
+
e
4

4
0
+
e
2

0
0
+
e
0

20

40

60

80

100

observation index

Figure 2: Same representation as Figure 1 for another run of the Gibbs sampler,

on Mixtures of Distributions." In Rao, C. and Dey, D. (eds.), Handbook of Statistics,
volume 25. Springer-Verlag, New York. 480

Robert, C. and Casella, G. (2004). Monte Carlo Statistical Methods. Springer-Verlag,

New York, second edition. 480

Stephens, M. (2000). \Dealing with label switching in mixture models." J. Royal Statist.

Soc. Series B, 62(4): 795{809. 480

Bayesian Analysis (2007)

2, Number 3, pp. 483{494

Comment on Article by Jain and Neal

Steven N. MacEachern(cid:3)

1 Introduction

It was with great interest that I read Jain and Neal’s paper. In the paper, they address a
tough problem, namely how to improve the mixing/convergence of Markov chain Monte
Carlo (MCMC) algorithms for an important class of models. The models are those
involving mixtures of Dirichlet processes, ranging from a fairly straightforward mixture
of Dirichlet processes model to the more complex models that are springing up in a wide
variety of applications. The algorithms are in the split-merge vein, allowing a di(cid:11)erent
kind of step than incremental Gibbs samplers. The extension of the split-merge tech-
nology with targeted proposals to conditionally conjugate models is a welcome addition
to the collection of transitions available for (cid:12)tting models that include the Dirichlet
process as a component.

Jain and Neal’s algorithms (see also Dahl, 2005) have re(cid:12)ned the technology of split-
merge samplers so that proposals are no longer \blind", but, through intermediate Gibbs
scans, move toward a region of higher posterior probability. The ability to target better
proposals results in algorithms that naturally make better proposals, and this improves
mixing of the Markov chain. An important element of these intermediate Gibbs scans
is their ability to move toward a more appropriate launch state.

This discussion focuses on two features that are hidden in the innards of the algo-
rithm. The (cid:12)rst is the notion of identi(cid:12)ability and the second is that of a random scan.
Jain and Neal’s algorithms make nice use of a non-identi(cid:12)able model for the interme-
diate Gibbs scans (section 4.2, step 3 and following) to produce what are presumably
better proposals. They also implicitly use a random scan for split and merge proposals
in the sense that cases i and j are selected at random (section 4.2, step 1). The remain-
der of this discussion looks at these issues in the context of a simple, arti(cid:12)cial example
where one can explicitly calculate rates of convergence for a variety of incremental Gibbs
algorithms. The hope is that the example, in spite of its simplicity, provides insight into
the e(cid:11)ectiveness of the algorithms and suggests potential directions for their further
re(cid:12)nement.

2 Identi(cid:12)ability

While details of various algorithms are left for the next section, one recurring issue
in proposals for novel algorithms for Dirichlet based models is identi(cid:12)ability. This is-
sue is not limited to mixture models, but arises in many other contexts. There is

(cid:3)Department

of

Statistics,

The

Ohio

State

University,

Columbus,

OH,

mailto:snm@stat.ohio-state.edu

c(cid:13) 2007 International Society for Bayesian Analysis

ba0007

484

Comment on Article by Jain and Neal

often a connection between identi(cid:12)ability and the convergence rate of a Markov chain:
Identi(cid:12)able models may show quicker convergence to the limiting distribution than do
non-identi(cid:12)able models. This has led some to suggest a general principle that non-
identi(cid:12)able models be avoided when MCMC methods are to be used to (cid:12)t the model.
This section reviews the arguments raised against non-identi(cid:12)able models, and the fol-
lowing section develops the arguments in more detail through consideration of a simple
example.

Consider a model where there is a parameter space, say (cid:2). The distribution of the
data depends on the value of the parameter, so that X (cid:24) F(cid:18) for some (cid:18) (cid:24) (cid:2). A model
is non-identi(cid:12)able if there exist (cid:18)1; (cid:18)2 2 (cid:2), with (cid:18)1 6= (cid:18)2; for which F(cid:18)1 = F(cid:18)2 . Models
that are not non-identi(cid:12)able are called identi(cid:12)able models. Typically, when the model
is non-identi(cid:12)able, it will be the case that for every (cid:18)1 2 (cid:2) there exists a (cid:18)2 2 (cid:2), with
(cid:18)2 6= (cid:18)1 for which F(cid:18)1 = F(cid:18)2 .

Several reasons have been given for avoiding the use of non-identi(cid:12)able models.
First, while a Bayesian approach places a prior over the parameter space, and so, in
principle, there is no di(cid:14)culty in creating estimates with this methodology, there is the
question of consistency. Identi(cid:12)ability is closely connected with parameter estimation.
Methods such as maximum likelihood cannot distinguish between parameter values that
imply the same distribution for the data, and so may not produce unique estimates.
Bayes estimates, heavily based on the likelihood, are typically also inconsistent for non-
identi(cid:12)able models. However, if consideration is restricted to identi(cid:12)able functionals,
the Bayes estimates will typically be consistent, as they are under identi(cid:12)able models. A
desire to interpret parameter values directly is closely related to a desire for consistency.
Restricting interpretation to identi(cid:12)able quantities g((cid:18)), such that if g((cid:18)1) 6= g((cid:18)2) then
F(cid:18)1 6= F(cid:18)2 , the worry about non-identi(cid:12)ability disappears. A complete, identical Bayes
analysis could be done on an identi(cid:12)able model. This (cid:12)rst objection has no connection
to the use of MCMC methods.

Second, there are examples where the convergence rate of a Markov chain is improved
by the choice of an identi(cid:12)able model. The convergence here is convergence of (cid:25)n, the
distribution of (cid:18)nj(cid:18)0, to the limiting distribution of the Markov chain. The limiting
distribution is, by construction, also the posterior distribution of (cid:18). The main purpose
of the simulation is to provide estimates of posterior summaries, and, although there is
a di(cid:11)erence between the accuracy of these estimates and the convergence rate, in most
circumstances the two produce qualitative agreement: A better convergence rate means
more accurate estimators. This issue is examined in the next section.

Third, there is the practical issue of how well the MCMC algorithm works when ac-
tually implemented. The main concerns are the numerical accuracy and stability of the
computations. In some instances, particularly with very di(cid:11)use posterior distributions,
some of the parameter values generated during the course of the simulation may be
enormous. This can lead to unstable computations and hence to inaccurate estimates.

Fourth, there is the issue of prior elicitation. The choice of a model has an impact on
the particular prior that is chosen. This choice is not directly tied to the use of MCMC
methods, but is an issue of increasing importance now that more complex models are

S. N. MacEachern

485

being (cid:12)t. Examples include collinearity and the variable selection problem where priors
are chosen according to prescription, problems based on the hierarchical model, and
nonparametric Bayes problems.

Consider (cid:12)tting models with MCMC methods. The Markov chain upon which the
simulation is based is realized through successive generations of a parameter vector, (cid:18).
The chain, assumed to be irreducible and aperiodic, is also assumed to have a (cid:12)xed
transition matrix, say P . Consequently, it has a limiting distribution, (cid:25). The transition
matrix is chosen in such fashion that (cid:25) is the posterior distribution for (cid:18)jX. A realization
of the chain consists of a sequence (cid:18)1; (cid:18)2; : : : ; (cid:18)N . Convergence is often described in
terms of the total variation norm: We wish jj(cid:25)n (cid:0) (cid:25)jj to approach 0 quickly. For (cid:12)nite
state chains, the rate of convergence is governed by the second largest eigenvalue of the
transition matrix. The convergence rate of more complex chains is determined by a
similar quantity.

The MCMC method constructs P by creating a set of transition kernels. For a (cid:12)xed
scan algorithm, the overall transition kernel is the product of, say, p transition kernels,
P = P1 : : : Pp. A random scan sampler selects one of the Pi at random. A popular

choice is to select the Pi with equal probabilities, so that P = p(cid:0)1P Pi.

Two useful techniques for improving convergence of a sampler are (i) to gener-
ate a block of parameters at a time (say (cid:18)1; : : : ; (cid:18)c is generated from [(cid:18)1; : : : ; (cid:18)cj(cid:18) (cid:0)
f(cid:18)1; : : : ; (cid:18)cg; X]), and (ii) to collapse or coarsen the state space of the Markov chain
by reducing the dimension of (cid:18). The dimension of (cid:18) is reduced through integration.
For example, (cid:18)p may be marginalized, leaving only (cid:18)1; : : : ; (cid:18)p(cid:0)1. For discussion, theory
and examples of (i) and (ii) see Liu (1995); of (ii) see also MacEachern (1994). The
impact of non-identi(cid:12)ability on MCMC algorithms is closely connected to blocking and
coarsening.

3 Illustration

Nonparametric Bayesian models have been considered for several decades. Early mod-
els, such as those of Kraft and van Eeden (1964) and Ramsey (1972) for the bioassay
problem, provided a start in the area. These models were based on the notion of a
Dirichlet distribution being the conjugate prior for multinomial data. The models were
nonparametric in the sense that the prior had full support on the set of multinomial
probability vectors. This work was followed by the well-known work of Ferguson (1973)
and Antoniak (1974). Early work exploiting mixtures of Dirichlet processes includes
Berry and Christensen (1979) and Lo (1984).

The mixture of Dirichlet process model has many applications beyond bioassay. The

basic mixture of Dirichlet processes model may be written as follows:

F (cid:24) Dir((cid:11))

(cid:18)1; : : : ; (cid:18)pjF (cid:24) F

Xij(cid:18)i (cid:24) G(cid:18)i; for i = 1; : : : ; p:

486

Comment on Article by Jain and Neal

Here, following Ferguson’s notation, (cid:11), the positive, (cid:12)nite measure that parameterizes
the Dirichlet process, is often split into its total mass, M , and its shape, say F0. Thus if
(cid:11) is a measure on the real line, F0 is a distribution function, M > 0, and (cid:11)(((cid:0)1; x]) =
M F0(x).

G(cid:18) is a distribution indexed by the parameter (cid:18). The models are easily generalized
to include hyperparameters that index (cid:11), groups of observations associated with each
(cid:18)i, observation speci(cid:12)c covariates, and additional parameters common to some or all
observations. The bioassay problem is one which (cid:12)ts into this framework.

There are three main types of MCMC methods that have been widely used for the
mixture of Dirichlet process models. The (cid:12)rst is based directly on the hierarchical model
written above. It makes use of the sequence of conditional generations [F j(cid:18)]; [(cid:18)jF ]. See
Kuo and Smith (1992), Gelfand and Kuo (1991) and, in a general setting, Ishwaran
and Zarepour (2000) for details. See also Diebolt and Robert (1994) in the context of
a related (cid:12)nite mixture model.

The second type of Markov chain method makes use of an alternative representation
of the Dirichlet process known as the Polya urn scheme (Blackwell and MacQueen,
1973). Under the Polya urn scheme, the random distribution function F is marginalized,
resulting in the model

(cid:18)1; : : : ; (cid:18)p (cid:24) F(cid:18)1;:::;(cid:18)p

Xij(cid:18)i (cid:24) G(cid:18)i; for i = 1; : : : ; p:

To simplify description, take F0 to be continuous. With this model, the components
(cid:18)i are no longer conditionally independent. Instead, they have a distribution that is
built up sequentially: (cid:18)1 (cid:24) F0. For i > 1, (cid:18)i
is set equal to (cid:18)j with probability
1=(M + i (cid:0) 1) and is drawn from F0, independent of previous draws from F0, with
probability M=(M + i (cid:0) 1). The induced distribution on the vector (cid:18) is often thought
of in two parts. The (cid:12)rst is the partition of (cid:18) into distinct values, and the second is
the location of the, say k, elements of the partition. Each partition receives positive
probability under the prior. Given a partition, the k locations of the elements, denoted
(cid:18)(cid:3)
1; : : : ; (cid:18)(cid:3)
k, are i.i.d. draws from F0. A Markov chain based on this representation of
the model involves sequential generation of [(cid:18)ij(cid:18)(cid:0)i], for i = 1; : : : ; p, with the updating
performed immediately in each case. See Escobar (1994) and Escobar and West (1995)
for algorithms of this sort. These algorithms may be re(cid:12)ned by discarding the locations
of the clusters and running a Markov chain on only the space of partitions of (cid:18) (Neal,
1992; MacEachern, 1994). Such chains tend to produce quicker convergence to the
posterior and naturally suggest better estimators. The calculations below refer to this
last re(cid:12)nement of the algorithm, though they can be replicated when the locations are
present.

The third type of algorithm is the split-merge algorithm with its ability to make
large moves in directions not easily traveled in with algorithms of the (cid:12)rst two types.
The simple example can be (cid:12)t with the simple split-merge algorithm of Jain and Neal
(2000). In this case, the improvements in the algorithm do not change its performance.

S. N. MacEachern

487

The Markov chain runs on a state space which consists of all partitions of (cid:18) into
clusters. This is a (cid:12)nite state space, which is denoted by S. An element in the state space
is a p-dimensional vector, s = (s1; : : : ; sp), with component si indicating to which cluster
(cid:18)i belongs. If there are k clusters of (cid:18)i, there will be k distinct integers in the partition
vector. If (cid:18)i and (cid:18)j are in the same cluster, si = sj ; if in di(cid:11)erent clusters, si 6= sj .
The fact that the state space is (cid:12)nite allows us to perform exact calculations on the
transition matrix of the Markov chain in small examples. Several chains are compared
for the case of p = 3. A major issue is the labelling of the state space. Two identi(cid:12)able
labellings and one non-identi(cid:12)able labelling are considered. The labelling/identi(cid:12)ability
issue is cleanest for Type II algorithms. The labellings are presented in Table 1.

The (cid:12)rst Type II scheme numbers the clusters consecutively from 1 to k as they are
built up from the Polya urn scheme. Thus s1 = 1, and for all i for which (cid:18)i = (cid:18)1, si = 1.
The second cluster is begun by the (cid:12)rst (cid:18)i 6= (cid:18)1, and so si = 2 for i = inf [jj(cid:18)j 6= (cid:18)1]. All
other (cid:18)j equal to this (cid:18)i are in this cluster and so are assigned sj = 2. The numbering of
the later clusters proceeds in a similar fashion, so that for a legitimate partition vector
(i.e., one which receives positive probability under the prior) representing k clusters,
the numbers 1 through k will appear and their (cid:12)rst appearances will occur in increasing
order. The (cid:12)nal legitimate values of s for the case p = 3 appear in Table 1 under
the heading scheme 1. With this parameter space, the model is identi(cid:12)able. Each
legitimate con(cid:12)guration vector produces a distinct partition of the (cid:18) and hence (under
the mild regularity condition that there is a set of (cid:18)i with positive F0 probability such
that G(cid:18)1 = G(cid:18)2 i(cid:11) (cid:18)1 = (cid:18)2) produces a distinct distribution for X.

The second Type II scheme is similar to the (cid:12)rst in that there is a 1-1 mapping
between partitions and legitimate con(cid:12)guration vectors. The di(cid:11)erence is in how the
clusters are labelled. Again, all (cid:18)i in a cluster will have the same index in the con(cid:12)g-
uration vector. Those (cid:18)i in the cluster with (cid:18)1 have si = 1. Further clusters have an
index equal to inf [jj(cid:18)j in cluster]. For example, de(cid:12)ne i = inf [jj(cid:18)j 6= (cid:18)1]. Then sj = i
for all j such that (cid:18)j = (cid:18)i. The legitimate values for s under this labelling scheme when
p = 3 appear in Table 1 under the heading scheme 2. Since there is a 1 (cid:0) 1 mapping
between this labelling and the previous one, identi(cid:12)ability for this model follows from
identi(cid:12)ability of scheme 1.

The third Type II scheme produces a non-identi(cid:12)able model. With this scheme,
the clusters will each receive a distinct integer from 1 to n, and each (cid:18)i in a particular
cluster will receive the same index. There is, however, no other restriction on the index
values assigned to the clusters. To create this scheme formally, begin with the (cid:12)rst
labelling scheme. Probabilities of the legitimate states are determined by the Polya
urn scheme. Then the probability for a particular con(cid:12)guration is distributed among
the possible labellings for the con(cid:12)guration. For a con(cid:12)guration with k clusters, there
are n!=(n (cid:0) k)! distinct labellings. The probability for this con(cid:12)guration is distributed
uniformly among these labellings. This model is clearly non-identi(cid:12)able, since there are
several parameter values (here several di(cid:11)erent con(cid:12)guration vectors) which produce the
same distribution for the data. Interestingly, [F j(cid:18); X; s] depends on s only through the
con(cid:12)guration. Hence, any inference depends only on the equivalence class on s de(cid:12)ned
by the con(cid:12)guration itself.

488

Comment on Article by Jain and Neal

State Con(cid:12)guration
a
b
c
d
e

(cid:18)1; (cid:18)2; (cid:18)3
(cid:18)1 = (cid:18)2; (cid:18)3
(cid:18)1 = (cid:18)3; (cid:18)2
(cid:18)1; (cid:18)2 = (cid:18)3
(cid:18)1 = (cid:18)2 = (cid:18)3

scheme 1
1,2,3
1,1,2
1,2,1
1,2,2
1,1,1

scheme 2
1,2,3
1,1,3
1,2,1
1,2,2
1,1,1

Table 1: Labellings of con(cid:12)gurations under schemes 1 and 2.

Gibbs samplers were developed for each of the labelling schemes above for the no-
data problem. The transition matrix for a (cid:12)xed scan, in the order [s1js2; s3]; [s2js1; s3],
and then [s3js1; s2] was calculated analytically. For the (cid:12)rst two schemes, the second
largest eigenvalue of the transition matrix was determined. To compare the third scheme
to the (cid:12)rst two, identi(cid:12)able functions are considered. In order to determine an e(cid:11)ec-
tive rate of convergence for these functions, the transition matrix for the sampler is
rewritten in terms of an identi(cid:12)able model. Happily, all of the transition vectors from
each non-identi(cid:12)able state corresponding to a particular con(cid:12)guration to the distinct
con(cid:12)gurations are identical (e.g., the transition probability for moving from the state
s = (1; 1; 3) to the con(cid:12)guration (cid:18)1 = (cid:18)2 = (cid:18)3 is the same as the transition probability
for moving from the state s = (2; 2; 1) to the con(cid:12)guration (cid:18)1 = (cid:18)2 = (cid:18)3). The chain,
in terms of this identi(cid:12)able state space, retains the Markov property. The implication
is that the second largest eigenvalue of the rewritten transition matrix governs the rate
of convergence in the identi(cid:12)able space.

The three Gibbs samplers corresponding to the three labelling schemes were com-
pared by means of the second largest eigenvalue of their transition matrices, presented
in Table 2. The comparison of the three schemes shows that scheme 3, based on the non-
identi(cid:12)able model, produces the best performance. The non-identi(cid:12)able model results
in better mixing.

Simulations were carried out to compare the Type I algorithm to the Type II algo-
rithms. The simulation made use of a non-identi(cid:12)able version of the Type I algorithm.
The estimated second largest eigenvalue of the Type I algorithm appears in Table 2
along the row labelled Type I. Scheme 3 appears to dominate this type of algorithm.
This conclusion agrees with results that suggest a collapse of the state space improves
the convergence rate of a Markov chain, since the scheme 3 algorithm may be con-
structed by adding generations to a Type I algorithm and then collapsing the state
space. Interestingly, this is in opposition to the sometimes expressed intuition that a
two-stage Gibbs sampler, as the Type I method, should show quicker convergence than
a three-stage Gibbs sampler, as the scheme 3 algorithm is. These results in this sim-
ple context are in agreement with the careful simulations for more realistic settings in
Papasiliopoulos and Roberts (2008).

The random scan Gibbs sampler was investigated in a similar fashion. Table 2 con-
tains a summary of the results for 3 transitions (so chosen to match the three transitions

S. N. MacEachern

489

M
scheme 1
scheme 2
scheme 3
Type I

M
scheme 1
scheme 2
scheme 3
Split-merge

1
.222
.222
.0370
.301

1
.559
.559
.171
1.00

5
.327
.0408
.00292
.0837

10
.389
.0139
.000579
.0332

100
.485
.000192
9.42e-7
.000559

5
.630
.395
.0787
.152

10
.669
.352
.0588
.216

100
.726
.303
.0393
.287

Table 2: Second largest eigenvalues for MCMC algorithms. The top table is for (cid:12)xed
scan samplers; the bottom table is for random scan samplers. M is the mass of the base
measure of the Dirichlet process.

of the (cid:12)xed scan sampler). Notice that the second largest eigenvalues are considerably
larger for random scan samplers, corresponding to the potentially long lags between suc-
cessive sampling of a component. Again, scheme 3, corresponding to the non-identi(cid:12)able
model, is preferable to the Type II schemes. The Type III (split-merge) sampler is, for
the larger values of M , preferable to the Type II samplers that impose identi(cid:12)ability.
In this example, it does not mix as well as the non-identi(cid:12)able algorithm. Interestingly,
when M = 1, the sampler yields a periodic Markov chain, and so mixing is poor al-
though estimation (barring an even subsampling rate) is (cid:12)ne. It should be noted that
this periodicity is very special to this example.

4 Heuristics

The simplicity of the example allows us to focus on features of the algorithms that impact
mixing: Comparisons among the Type II algorithms suggest that non-identi(cid:12)ability (of a
certain sort) improves mixing; the comparison between (cid:12)xed and random scans suggests
that (cid:12)xed scans lead to better mixing; a good Type II algorithm leads to better mixing
than a Type I algorithm; for small clusters, the Type II algorithm mixes better than
the Type III algorithm.

Within Type II algorithms, the example shows a remarkable advantage for the non-
identi(cid:12)able model. This appears to follow from the conditioning sets used to create
the Gibbs sampler. The non-identi(cid:12)able model leads to conditioning sets that contain
the conditioning sets arising from the identi(cid:12)able model. To illustrate this point, a
schematic of the transition matrices is provided in Table 3. Comparing the two P1’s, for
instance, under scheme 1 the transition matrix is the identity while under scheme 3 it
is a block diagonal matrix with only two blocks. Both chains are based on conditional
generations. For each current state, the set conditioned upon for the generation under

490

Comment on Article by Jain and Neal

P1
From To
a
b
c
d
e
P2
From To
a
b
c
d
e
P3
From To
a
b
c
d
e

a
x
-
-
-
-

a
x
-
-
-
-

a
x
-
x
x
-

b
-
x
-
-
-

b
-
x
-
x
-

b
-
x
-
-
x

c
-
-
x
-
-

c
-
-
x
-
x

c
x
-
x
x
-

d
-
-
-
x
-

d
-
x
-
x
-

d
x
-
x
x
-

e
-
-
-
-
x

e
-
-
x
-
x

e
-
x
-
-
x

a
x
x
x
-
-

a
x
x
-
x
-

a
x
-
x
x
-

b
x
x
x
-
-

b
x
x
-
x
-

b
-
x
-
-
x

c
x
x
x
-
-

c
-
-
x
-
x

c
x
-
x
x
-

d
-
-
-
x
x

d
x
x
-
x
-

d
x
-
x
x
-

e
-
-
-
x
x

e
-
-
x
-
x

e
-
x
-
-
x

Table 3: Scheme 1 transition matrices on the left, scheme 3 transition matrices on the
right. The states are described in Table 1. A dash indicates that a transition cannot
take place, an x that it can. Note the enlargement of the sets over which conditional
generations take place with scheme 3.

the scheme 3 chain contains the set conditioned upon for the generation under the
scheme 1 chain. Thus the conditioning sets for the scheme 1 chain are nested in those
for the scheme 3 chain. The following result connects the nesting of conditioning sets
to total variation distance.

Proposition 1. Suppose that we have a countable state space, and a distribution
(cid:25) which assigns positive probability to each state. Further suppose that this state
space is partitioned into conditioning sets Ci. De(cid:12)ne row i of the transition matrix
P to consist of the distribution (cid:25), restricted to the conditioning set in which state i
lies. Consider two partitions, A and B, where fCA;ig is a re(cid:12)nement of fCB;ig and the
corresponding transition matrices PA and PB . Then, for any initial distribution, (cid:25)I ,
jj(cid:25)

I PA (cid:0) (cid:25)jj (cid:21) jj(cid:25)

0

0

I PB (cid:0) (cid:25)jj.

Proof. The total variation distance between the distributions F and G is de(cid:12)ned
by jjF (cid:0) Gjj = supA(jF (A) (cid:0) G(A)j + jF (AC ) (cid:0) G(AC )j) where A ranges over all subsets
of the state space. When the initial distribution (cid:25)I is modi(cid:12)ed through a transition
governed by a conditional distribution over a partition, the supremum is attained by a

S. N. MacEachern

491

set A for which each element of the partition is either entirely contained in A or entirely
contained in AC . Since the conditioning sets used to create PA are a re(cid:12)nement of those
used to create PB , we may view the supremum in the former case as being taken over
a larger set. Hence, jj(cid:25)

I PA (cid:0) (cid:25)jj is at least as large as jj(cid:25)

0

0

I PB (cid:0) (cid:25)jj.

Proposition 1 shows that one step of the chain based on larger conditioning sets
(i.e., the sampler based on the non-identi(cid:12)able model) is preferable to one step of the
chain based on the smaller conditioning sets. However, the proof given here does not
extend to more steps. Presumably, the quicker one-step movement toward the posterior
will often carry over into a quicker rate of convergence for the chain, as it does in the
example of Section 3. Consideration of the impact of identi(cid:12)ability underlay, in part,
the development of nonconjugate algorithms in MacEachern and Muller (1998).

As Jain and Neal comment, the Type III algorithms are most bene(cid:12)cial when there
are large clusters of observations. With only a few large clusters, all observations will
frequently have a chance to switch clusters. However, my experience with models involv-
ing the Dirichlet process is that the posterior distribution typically includes a number
of small clusters (in addition to the large clusters). The simple example suggests that
including Type II steps is important to facilitate mixing for these small clusters.

5 Conclusions

The example presented herein, as well as others that I have examined, lead to the follow-
ing viewpoint on the four reasons presented earlier for avoiding non-identi(cid:12)able models.
The (cid:12)rst, interpretation of the model, has no connection to whether MCMC methods
are used to (cid:12)t the model, and so in no way suggests that one restrict themself to use of
identi(cid:12)able models. The second reason seems to be largely irrelevant. The important
convergence rate (if an identi(cid:12)able model is to be considered at all) is convergence for
estimates of identi(cid:12)able functionals. This may be quicker than the convergence rate of
the chain in the non-identi(cid:12)able space. In any event, if an e(cid:11)ective chain can be created
based on the identi(cid:12)able form of the model, the same chain can be created based on
the non-identi(cid:12)able form of the model. The third concern, for numerical stability of
the computations, remains a concern. The fourth issue is one of prior elicitation. Since
models and prior distributions are subjective and situation speci(cid:12)c, any recommenda-
tion for one form of model over another is open to criticism. Nevertheless, some classes
of models seem much more natural than do others. Often, as in the case of the hier-
archical model, these classes contain non-identi(cid:12)able models. A decision to replace a
natural, non-identi(cid:12)able model with an identi(cid:12)able model that seems to be less natural
seems unwise without a demonstrated improvement in the ease or e(cid:11)ectiveness with
which the model is (cid:12)t.

My own view on problems necessitating MCMC methods is this. One should (cid:12)rst
write down the most natural model, whether it be identi(cid:12)able or non-identi(cid:12)able. Next,
lay out several MCMC methods for this version of the model. Further consider expand-
ing the parameter space to create non-identi(cid:12)able models. Particular consideration

492

Comment on Article by Jain and Neal

should be given to inducing non-identi(cid:12)ability by adding symmetries such as the re-
labelling of the clusters in the simple Dirichlet process example. Again, examine a
batch of MCMC algorithms, with attention to generating blocks of parameters and to
marginalizing parameters. Finally, select an algorithm based on the heuristics of pre-
ferring those derived from larger conditioning sets, those that have collapsed the state
space, and those that generate blocks of parameters at a time. To this algorithm, add
steps that target particularly di(cid:14)cult transitions{such as splitting and merging large
clusters.

The hints in Jain and Neal’s paper and the simple example suggest a natural direction
for extension of the split-merge moves: a move away from a random scan (i.e., random
selection of observations i and j that determine the attempted split/merge) and toward
a scan with reduced randomness. The randomness of the scan can be lessened, for
example, by permuting the indices from 1 through n, and using successive pairs for
i and j. This type of permutation bounds the time between successive attempts at
updating each observation’s cluster membership. In turn, this ensures that the number
of iterates until every observation-speci(cid:12)c parameter has had a chance to be updated
is controlled.
I suspect that the bene(cid:12)ts that Jain and Neal have demonstrated of
combining both incremental and split-merge moves in an algorithm are partly due to
the implicit reduction in randomness{a complete incremental Gibbs scan ensures that
all cases have had the opportunity to move.

A second possible extension is to reserve the split/merge moves for clusters of sub-
stantial size. To do so, one could partition the parameter space into two parts{one part
where the combined number of cases in clusters identi(cid:12)ed by observations i and j ex-
ceeds some threshold and the second part where the combined number of cases is small.
If the current state were in the (cid:12)rst part, a split-merge move would be attempted, and
the state after transition would also remain in the (cid:12)rst part. If the current state were
in the second part, slightly modi(cid:12)ed incremental steps would be attempted, with the
modi(cid:12)cation ensuring that the state after transition would also remain in the second
part. Alternatively, for this second part, one could make no transition at all. With
the posterior distribution invariant for each potential step, the posterior distribution
would remain invariant for the chain as a whole. Supplementing this type of move with
incremental Gibbs scans would yield irreducibility of the chain.

References

Antoniak, C. E. (1974). \Mixtures of Dirichlet Processes with Applications to Bayesian

Nonparametric Problems." The Annals of Statistics, 2: 1152{1174.

Berry, D. A. and Christensen, R. (1979). \Empirical Bayes Estimation of a Binomial
Parameter Via Mixtures of Dirichlet Processes." The Annals of Statistics, 7: 558{
568.

Blackwell, D. and MacQueen, J. B. (1973). \Ferguson Distributions Via P(cid:19)olya Urn

Schemes." The Annals of Statistics, 1: 353{355.

S. N. MacEachern

493

Dahl, D. (2005). \Sequentially-allocated merge-split sampler for conjugate and non-
conjugate Dirichlet process mixture models." Technical report, Department of Statis-
tics, Texas A& M University.

Diebolt, J. and Robert, C. P. (1994). \Estimation of Finite Mixture Distributions
through Bayesian Sampling." Journal of the Royal Statistical Society, Series B:
Methodological, 56: 363{375.

Escobar, M. D. (1994). \Estimating Normal Means with a Dirichlet Process Prior."

Journal of the American Statistical Association, 89: 268{277.

Escobar, M. D. and West, M. (1995). \Bayesian Density Estimation and Inference Using

Mixtures." Journal of the American Statistical Association, 90: 577{588.

Ferguson, T. S. (1973). \A Bayesian Analysis of Some Nonparametric Problems." The

Annals of Statistics, 1: 209{230.

Gelfand, A. E. and Kuo, L. (1991). \Nonparametric Bayesian Bioassay Including Or-

dered Polytomous Response." Biometrika, 78: 657{666.

Ishwaran, H. and Zarepour, M. (2000). \Markov Chain Monte Carlo in Approximate
Dirichlet and Beta Two-parameter Process Hierarchical Models." Biometrika, 87(2):
371{390.

Jain, S. and Neal, R. M. (2000). \A split-merge Markov chain Monte Carlo procedure
for the Dirichlet process mixture model." Technical report, Department of Statistics,
University of Toronto.

Kraft, C. H. and van Eeden, C. (1964). \Bayesian Bio-assay." The Annals of Mathe-

matical Statistics, 35: 886{890.

Kuo, L. and Smith, A. F. M. (1992). \Bayesian Computations in Survival Models Via
the Gibbs Sampler (Disc: P22-24)." In Klein, J. P. and Goel, P. K. (eds.), Survival
Analysis: State of the Art, 11{22. Kluwer Academic Publishers Group.

Liu, J. S. (1994). \The Collapsed Gibbs Sampler in Bayesian Computations with Ap-
plications to a Gene Regulation Problem." Journal of the American Statistical Asso-
ciation, 89: 958{966.

Lo, A. Y. (1984). \On a Class of Bayesian Nonparametric Estimates: I. Density Esti-

mates." The Annals of Statistics, 12: 351{357.

MacEachern, S. N. (1994). \Estimating Normal Means with a Conjugate Style Dirichlet
Process Prior." Communications in Statistics: Simulation and Computation, 23: 727{
741.

MacEachern, S. N. and M(cid:127)uller, P. (1998). \Estimating Mixture of Dirichlet Process

Models." Journal of Computational and Graphical Statistics, 7: 223{238.

494

Comment on Article by Jain and Neal

Neal, R. (1991). \Bayesian mixture modelling." In C.R. Smith, G. E. and Neudorfer, P.
(eds.), Maximum Entropy and Bayesian Methods: Proceedings of the 11th Interna-
tional Workshop on Maximum Entropy and Bayesian Methods of Statistical Analysis,
197{211. Kluwer Academic Publishers.

Papaspiliopoulos, O. and Roberts, G. (2008). \Retrospective MCMC for Dirichlet pro-

cess hierarchical models." Biometrika(to appear).

Ramsey, F. L. (1972). \A Bayesian Approach to Bioassay (Com: V29 P225-226, V29

P830)." Biometrics, 28: 841{858.

Acknowledgments

This research was funded by the NSF under grant numbers SES-0437251 and DMS-0605041
and by the NSA under award number H98230-05-1-0065.

Bayesian Analysis (2007)

2, Number 3, pp. 495{500

Rejoinder

Sonia Jain(cid:3) and Radford M. Nealy

We thank discussants Drs. MacEachern, Robert, and Dahl for their thoughtful
comments. Since many of their comments are related, we will address them by topic
below.

1 Creation, Deletion, Identi(cid:12)ability, and Tempering

Our conditionally conjugate split-merge technique belongs to the family of
trans-dimensional MCMC algorithms, which includes,
for example, reversible-jump
MCMC (Green 1995), birth-death MCMC (Stephens 2000a), and split-merge MCMC
(Jain and Neal 2004), (Dahl 2003). Trans-dimensional MCMC algorithms construct
Markov chain transitions between states that vary in dimension. For Dirichlet process
mixture models, this involves the creation or deletion of mixture components.

Of course, even plain Gibbs Sampling updates for this model must be able to create
and delete mixture components, but they do so only in an incremental fashion, in which
a new component must start o(cid:11) explaining only a single observation | which may be
a rather unlikely state. A key strength of trans-dimensional MCMC procedures is the
ability to traverse the parameter space e(cid:14)ciently without having to pass through such
low-probability states. For simple problems, these techniques can save computation
time by reducing the required burn-in, and improving sampling thereafter. For more
complex and di(cid:14)cult problems, such as are encountered in areas such as genetics and
image analysis, these techniques may be essential if the problem is to be solved in any
reasonable amount of time.

The mixture components created are given arbitrary labels, which could be permuted
without a(cid:11)ecting (cid:12)t to the data, or prior probability. This \non-identi(cid:12)ability" has
been seen by some as raising issues with regard to proper interpretation of the results,
as discussed, for example by Stephens (2000b). These issues are of no relevance to our
paper, which is concerned only with e(cid:14)ciently sampling from the posterior distribution.
We agree with MacEachern that forcing the Dirichlet process mixture model to be
identi(cid:12)able is a hindrance to e(cid:14)cient MCMC sampling.

In this regard, one should note that sampling of all equivalent labellings can easily
be obtained by simply introducing an additional MCMC update (applied at any desired
interval) that permutes the labels | though this would be pointless for most purposes,
since the labelling doesn’t matter. Robert demonstrates that Gibbs sampling alone may
fail to move easily between modes with di(cid:11)erent labellings. In itself, this failure is of
no practical signi(cid:12)cance. Lack of movement between these equivalent modes should be

(cid:3)Division of Biostatistics and Bioinformatics, Department of Family and Preventive Medicine, Uni-

versity of California at San Diego, La Jolla, CA, mailto:sojain@ucsd.edu

yDepartment of Statistics and Department of Computer Science, University of Toronto, Toronto,

Ontario, Canada, http://www.cs.toronto.edu/~radford/

c(cid:13) 2007 International Society for Bayesian Analysis

ba0007

496

Rejoinder

worrying only to the extent that one thinks it is a sign that the MCMC method would
also fail to move between non-equivalent modes (if any) that correspond to substantively
di(cid:11)erent interpretations of the data. It is unclear to us that failure to move amongst
equivalent modes is actually indicative of a real problem of this sort. Conversely, there
is no guarantee that a method that moves amongst equivalent modes can also move
easily between non-equivalent modes.

Robert suggests that perhaps global tempering would perform better than a split-
merge procedure, with regard to movement between isolated modes. (We are not sure
which tempering method Robert used for his example, as it is not speci(cid:12)ed.) However,
his example considers the bene(cid:12)ts of tempering only when transitions are done using
Gibbs sampling, without any split-merge updates. Moreover, his example concerns a
fully conjugate model of the type treated in our earlier work (Jain and Neal 2004),
rather than the nonconjugate models discussed in this article.

Also, the comparison looks only at mixing amongst equivalent modes, which as
mentioned above is of no importance in itself. For these reasons, this demonstration
does not convince us that tempering would work better than split-merge methods.
However, we do expect that for some complex problems, such as very high-dimensional
clustering, split-merge may not be su(cid:14)cient. We hypothesize that tempering methods
may also have di(cid:14)culty with such problems, but that applying tempering in conjunction
with split-merge updates might allow for their solution.

2 The Role of Conditional Conjugacy in Our Algorithm

As the discussants highlight, our split-merge method applies only to models in which the
prior for parameters of component distributions exhibits conditional conjugacy. Though
this limits the the usefulness of our algorithm, its domain is perhaps wider than one
might expect. For instance, consider MacEachern (1998), in which he describes how
a nonconjugate model can be treated as a conditionally conjugate problem by using
piecewise log-concavity.

Robert wonders whether it might be possible to extend the algorithm beyond condi-
tionally conjugate models. Conditional conjugacy is needed so that we can do a Gibbs
sampling scan from the launch state, and also compute the probability density for choos-
ing the value chosen at each stage of this scan. The underlying requirement is that we
have a way of proposing a new parameter vector based on the launch state such that
(a) the distribution of the proposed state is similar to the posterior distribution of pa-
rameter values, and (b) we can for this proposed state compute the probability density
of its having been proposed. This allows us to implement e(cid:14)cient Metropolis-Hastings
updates, with a particular update being chosen randomly by the procedure for selecting
a launch state. As an aside, note that from its very origins (Metropolis et al. 1953)
the Metropolis algorithm has commonly been used with proposals that change only a
subset of the variables. It is not necessary to justify such partial Metropolis-Hastings
updates in a special way (as Robert suggests), or to refer to such updates as anything
other than Metropolis-Hastings updates.

S. Jain and R. M. Neal

497

An MCMC transition that leaves the posterior distribution invariant is a natural
way of trying to get a proposal for parameters of split/merged components that comes
from close to the posterior distribution. More than one such transition would be better,
but would make computing the density for a proposal impossible, as that would require
integrating over intermediate states. (Such an integral is avoided in our algorithm by
treating the intermediate Gibbs sampling updates not as part of the proposal distribu-
tion but rather as a procedure for choosing a launch state.) One could certainly imagine
using MCMC transitions other than Gibbs sampling for this purpose. One could, for
example, use a series of Metropolis-Hastings updates applied to each parameter in turn.
The probability density for proposing a state that di(cid:11)ers in all components from the
launch state would then be easily computed, as the product of all the proposal densi-
ties and all the acceptance probabilities. Unfortunately, the probability density for a
state in which any of these Metropolis-Hastings updates was rejected (so that at least
component is the same in the launch state and in the proposal) will be in(cid:12)nite, which
will result in a zero acceptance probability for the split-merge update.

So, although one can imagine such variations, they may not be useful in practice.
One possibility that would be worth investigating is using some approximation to the
posterior distribution (for the model restricted to two components), such as a Gaus-
sian. The conditional distributions from this approximation could be used as proposals
(resulting in Gibbs sampling in the limit as the approximation becomes perfect). If the
rejection rate is small enough, this might work well. Alternatively, the approximation
could be used directly | the validity of our algorithm does not depend on the transi-
tion from the launch state (or the intermediate transitions) leaving the actual posterior
distribution invariant, though use of a bad approximation will of course lead to a low
acceptance rate for the split-merge updates.

3 The Usefulness of Incremental Markov Chain Updates

Together with Split-Merge

Our split-merge algorithm has four tuning parameters, controlling the number of in-
termediate restricted Gibbs sampling scans for splits proposals and merge proposals,
and the number of split-merge updates and incremental Markov chain updates (e.g.
Gibbs sampling scans) done as part of a full iteration. Both MacEachern and Dahl
remark on the importance of a (cid:12)nal incremental Gibbs sampling scan. We agree with
MacEachern that the inclusion of such a step is important to facilitate mixing, as we
have demonstrated in the article. However, though MacEachern emphasizes the role of
such updates in mixing for small clusters, we believe that they are at least as important
for moving observations back and forth between large clusters, as this cannot be done
e(cid:14)ciently with split-merge updates.

Indeed, as we have described in our article and as Dahl observes, the \jitter" that
is observed in the trace plots of the beetle example can be attributed to the (cid:12)nal
auxiliary Gibbs sampling scan. However, we disagree with Dahl’s conclusion from this
that the CPU time spent on split-merge updates is \wasted" when these moves are not

498

Rejoinder

accepted. How can we know a priori that these moves will not be accepted unless they
are proposed? Since split-merge updates are required to obtain a correct solution (in a
reasonable amount of time) for some problems, it is necessary to perform them for all
problems in order to determine if they are actually needed, and hence ensure that the
answer obtained is correct.

Further, Dahl’s demonstration with only auxiliary Gibbs sampling (no split-merge
updates) is not entirely convincing. On close inspection, the lower plots of his Figure
1 show that auxiliary Gibbs sampling is not actually performing that well! For several
thousand iterations, a number of observations seem to have been incorrectly allocated
to small clusters, with the Gibbs sampler making only slow progress in correcting this.
It is possible that just a few split-merge iterations could take care of these orphan
clusters. By performing both incremental and non-incremental split-merge updates,
one can take advantage of both large-scale changes to the cluster con(cid:12)guration via
split-merge moves and small-scale adjustments that move a few observations between
clusters, as is necessary depending on the problem.

4 MCMC Initialization

Robert suggests that sampling from the prior to initialize the intermediate restricted
Gibbs sampling could lead to wasted computational e(cid:11)ort. In higher-dimensional prob-
lems, we agree that overcoming bad initial values could be a problem | i.e. many
restricted Gibbs sampling scan might be required.
In the Discussion section of the
paper, we had suggested alternatives to sampling from the prior to initialize the re-
stricted Gibbs sampling, such as adapting a method used by Dahl (2003), or some other
posterior estimation method.

A feature of the split-merge technique that Dahl discusses is its insensitivity to
the initial value that the Markov chain is started with, whereas the Gibbs sampler
is susceptible to poor choices (as illustrated in the Beetle example). We agree with
this, but are puzzled by the discrepancies in the simulations. We also initialized the
chain by sampling the model parameters from the prior and not by setting the initial
values to the sample mean and precision. One possible explanation is di(cid:11)ering orders
of updates | we sampled the indicators (cid:12)rst and then the model parameters (means
before precisions).

5 Random versus Fixed Scan Sampling

MacEachern investigates in detail how MCMC performance di(cid:11)ers for (cid:12)xed versus ran-
dom scans, in the context of Gibbs sampling. He proposes a systematic scan as an
alternative to the random scan that we utilize to initiate the split-merge process (i.e.
select two observations, denoted as i and j, uniformly at random). MacEachern suggests
permuting the indices from 1 to n and then using successive pairs as i and j, thereby
reducing randomness. This gives a feasible scan length (unlike systematically using all
possible pairs of observations). We agree that this is likely to improve performance, but

S. Jain and R. M. Neal

499

perhaps not by much. Partly, this is because there will still be considerable random-
ness in which clusters are chosen for split/merge operations | in particular, the same
clusters might well be chosen several times in a row.

More generally, however, MacEachern may be overestimating the di(cid:11)erence between
(cid:12)xed and random scans. The interpretation of his Table 2 is perhaps not obvious. The
number of iterations required to reach some small total variation distance is proportional
to (cid:0)1=log(v), where v is the second-largest eigenvalue. So, for example, using scheme
3, with (cid:11) = 1 (which is M = 1 in MacEachern’s notation), the (cid:12)xed scan method is
not better by a factor of 0:171=0:037 = 4:6, as one might naively think, but rather
by a factor of log(0:037)=log(0:171) = 1:9. As (cid:11) approaches in(cid:12)nity (approximated by
(cid:11) = 100, i.e. corresponding to M = 100 in the table), the second largest eigenvalue
for the (cid:12)xed scan approaches zero | all the variables are independent, so a single (cid:12)xed
Gibbs sampling scan immediately reaches equilibrium. The random scan has a non-
zero second eigenvalue in the (cid:11) ! 1 limit, re(cid:13)ecting the fact that after any number of
iterations there is a non-zero probability that some variable could still be left unchanged.
Technically, the asymptotic convergence rate of the (cid:12)xed scan is in(cid:12)nitely better than
that of the random scan, but in practice a modest number of iterations is su(cid:14)cient to
give the correct result with very high probability.

In this small example, Markov chain sampling is based on the prior distribution
of clusterings for three data points, but the likelihood factors deriving from the data
are omitted. However, in practical problems, where many split-merge proposals are
likely to be made for each that is accepted, the randomness in choice of clusters to
split/merge may be negligible compared to the randomness in proposing how to split
or merge them, and in whether or not to accept the result. Finding ways of further
improving the split/merge proposals may be a better focus for future research.

References

Dahl, D. B. (2003). \An improved merge-split sampler for conjugate Dirichlet process
mixture models." Technical Report 1086, Department of Statistics, University of
Wisconsin. 495, 498

Green, P. J. (1995). \Reversible jump Markov chain Monte Carlo computation and

Bayesian model determination." Biometrika, 82: 711{732. 495

Jain, S. and Neal, R. M. (2004). \A split-merge Markov chain Monte Carlo procedure
for the Dirichlet process mixture model." Journal of Computational and Graphical
Statistics, 13: 158{182. 495, 496

MacEachern, S. N. (1998). \Computational methods for mixture of Dirichlet process
models." In Dey, D., M(cid:127)uller, P., and Sinha, D. (eds.), Practical Nonparametric and
Semiparametric Bayesian Statistics, 23{43. New York: Springer-Verlag. 496

Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H., and Teller, E.
(1953). \Equation of state calculations by fast computing machines." Journal of
Chemical Physics, 21: 1087{1092. 496

500

Rejoinder

Stephens, M. (2000a). \Bayesian analysis of mixtures with an unknown number of
components { an alternative to reversible jump methods." Annals of Statistics, 28:
40{74. 495

| (2000b). \Dealing with label-switching in mixture models." Journal of the Royal

Statistical Society, Series B, 62: 795{809. 495

Bayesian Analysis (2007)

2, Number 3, pp. 501{528

Hidden Markov Dirichlet Process: Modeling
Genetic Inference in Open Ancestral Space

Eric P. Xing(cid:3) and Kyung-Ah Sohny

The problem of inferring the population structure, linkage disequi-
Abstract.
librium pattern, and chromosomal recombination hotspots from genetic polymor-
phism data is essential for understanding the origin and characteristics of genome
variations, with important applications to the genetic analysis of disease propen-
sities and other complex traits. Statistical genetic methodologies developed so far
mostly address these problems separately using specialized models ranging from
coalescence and admixture models for population structures, to hidden Markov
models and renewal processes for recombination; but most of these approaches
ignore the inherent uncertainty in the genetic complexity (e.g., the number of ge-
netic founders of a population) of the data and the close statistical and biological
relationships among objects studied in these problems. We present a new statis-
tical framework called hidden Markov Dirichlet process (HMDP) to jointly model
the genetic recombinations among a possibly in(cid:12)nite number of founders and the
coalescence-with-mutation events in the resulting genealogies. The HMDP posits
that a haplotype of genetic markers is generated by a sequence of recombination
events that select an ancestor for each locus from an unbounded set of founders
according to a 1st-order Markov transition process. Conjoining this process with
a mutation model, our method accommodates both between-lineage recombina-
tion and within-lineage sequence variations, and leads to a compact and natural
interpretation of the population structure and inheritance process underlying hap-
lotype data. We have developed an e(cid:14)cient sampling algorithm for HMDP based
on a two-level nested P(cid:19)olya urn scheme, and we present experimental results on
joint inference of population structure, linkage disequilibrium, and recombination
hotspots based on HMDP. On both simulated and real SNP haplotype data, our
method performs competitively or signi(cid:12)cantly better than extant methods in un-
covering the recombination hotspots along chromosomal loci; and in addition it
also infers the ancestral genetic patterns and o(cid:11)ers a highly accurate map of an-
cestral compositions of modern populations.

Keywords: Dirichlet Process, Hierarchical DP, hidden Markov model, MCMC,
statistical genetics, recombination, population structure, SNP.

1 Introduction

The availability of nearly complete genome sequences for organisms such as humans
makes it possible to begin to explore individual di(cid:11)erences between DNA sequences,
known as genetic polymorphisms, on a genome-wide scale, and to search for associations

(cid:3)School

of

Computer

Science,

Carnegie Mellon

University,

Pittsburgh,

PA,

http://www.cs.cmu.edu/~epxing/

ySchool

of

Computer

Science,

Carnegie Mellon

University,

Pittsburgh,

PA,

http://www.cs.cmu.edu/~ksohn/

c(cid:13) 2007 International Society for Bayesian Analysis

ba0007

502

Hidden Markov Dirichlet Process

of such genotypic variations with diseases and other phenotypes. Most human variation
that is in(cid:13)uenced by genes can be related to a particular kind of genetic polymorphism
known as the single nucleotide polymorphisms, or SNPs. A SNP refers to the existence
of two possible kinds of nucleotides from fA; C; G; T g at a single chromosomal locus (i.e.,
a position on the chromosome) in a population; each variant is called an allele (cid:3). A
haplotype is a list of alleles at contiguous sites in a local region of a single chromosome.
Assuming no recombination in this local region, a haplotype is inherited as a unit. But
under many realistic biological or genetic scenarios, repeated recombinations between
ancestral haplotypes during generations of inheritance may confound the genetic origin
of modern haplotypes (Figure 1).

Recombinations between ancestral chromosomes during meiosis play a key role in
shaping the patterns of linkage disequilibrium (LD)|the non-random association of
alleles at di(cid:11)erent loci|in a population. When a recombination occurs between two loci,
it tends to decouple the alleles carried at those loci in its decedents and thus reduce LD;
uneven occurrence of recombination events along chromosomal regions during genetic
history can lead to \block structures" in molecular genetic polymorphisms such that
within each block only low level of diversities are present in a population.

Statistically, for a pair of loci with genetic polymorphic markers, say, X and Y , the
LD between these two loci can be characterized by a number of so-called LD measures.
For example, for bi-allelic markers (i.e., markers that have only two possible states, say
\0" and \1"), LD can be measured by the gametic disequilibrium, D = p00p11 (cid:0) p01p10,
where p00 := Prob(X = 0; Y = 0), p11 := Prob(X = 1; Y = 1), p01 := Prob(X =
0; Y = 1), and p10 := Prob(X = 1; Y = 0), are the empirical frequencies of joint
allele-state con(cid:12)gurations. Another popular LD measure is the p-value for Fisher’s
exact test over samples of X and Y . When D = 0, which means that the two loci of
interest are not arranged randomly during inheritance (due to recombination of their
host chromosomes at a position between the two loci), they often emerge (e.g., from
all possible pairs in a large number of loci being surveyed) as candidates of marker
pairs on the chromosome whose locations are physically close so that there is a low
probability of having recombination events between them. However, to the best of
our knowledge, extant LD-measures remain primarily focused on o(cid:11)ering population-
level descriptive statistics of the sample, rather than on modeling and inferring the
underlying genetic mechanisms and processes that may have generated the data. For
example, the pairwise LD measure ignores the global context and overall pattern of the
genetic polymorphisms, and thus can not distinguish linkages due to spurious statistical
association (e.g., due to problems in sample procedures) from those resulting from true
physical proximity, or from genetic coupling due to co-evolution y. Such an approach also

(cid:3)In general, an allele represents a variant of a SNP, a gene, or some other entity associated with a
locus on DNA. In our case (SNPs), the locus harbors a single nucleotide, and therefore the alleles can
generally be assumed to be binary, re(cid:13)ecting the fact that \lightning doesn’t tend to strike twice in the
same place". That is, nucleotide substitutions (i.e., mutations) do not occur to the same locus twice
during the inheritance course from a common ancestor. More generally, e.g., in case of microsatellite
polymorphism, the allele-state can be k-nary, a scenario to which our proposed model also applies.

yCo-evolution can occur for DNA sequences that are far apart in the genome if they encode genes or
regulatory elements that jointly or corporately perform an indispensable biology function. For example,

Eric P. Xing and Kyung-Ah Sohn

503

provides no information regarding the demographical history and ancestral composites
of each individual in the study population. In this paper, we propose a new model-based
approach to address these issues.

The problem of inferring chromosomal recombination hotspots is essential for under-
standing the origin and characteristics of genome variations; several combinatorial and
statistical approaches have been developed for uncovering optimum block boundaries
from single nucleotide polymorphism haplotypes
(Daly et al. 2001; Anderson and Novembre 2003; Patil et al. 2001; Zhang et al. 2002).
For example, Zhang et al. (2002) proposed a dynamic programming algorithm for parti-
tioning single nucleotide polymorphism (SNP) haplotypes (explained in the sequel) into
low-diversity blocks; Daly et al. (2001) and Greenspan and Geiger (2004a) have de-
veloped hidden Markov models for locating recombination hotspots in haplotypes; and
Anderson and Novembre (2003) proposed a minimum description length (MDL) method
for optimal haplotype block (cid:12)nding. Some recent studies resorted to more sophisticated
population genetics arguments that more explicitly capture the mechanistic and pop-
ulation genetic foundations underlying recombination and LD pattern formation. For
example, Li and Stephens (2003) used a tractable approximation to the recombinational
coalescence, via a (latent) genealogy of the population, to capture the conditional de-
pendencies between haplotypes. Rannala and Reeve (2001) also use a coalescence-based
model and an MCMC method to integrate over the unknown gene genealogy and coales-
cence times. These advances have important applications in genetic analysis of disease
propensities and other complex traits.

The deluge of SNP data also fuels the long-standing interest of analyzing patterns
of genetic variations to reconstruct the evolutionary history and ancestral structures
of human populations, using, for example, variants of admixture models on genetic
polymorphisms (Pritchard et al. 2000; Rosenberg et al. 2002; Falush et al. 2003). These
models are instances of a more general class of hierarchical Bayesian models known as
mixed membership models (Erosheva et al. 2004), which postulate that genetic markers
of each individual are iid (Pritchard et al. 2000) or spatially coupled (Falush et al. 2003)
samples from multiple population-speci(cid:12)c (cid:12)xed-dimensional multinomial-distributions
of marker alleles. However, the admixture models developed so far do model genetic drift
due to mutations from the ancestor allele and therefore do not enable inference of the
founding genetic patterns and the age of the founding alleles (Exco(cid:14)er and Hamilton
2003).

This progress notwithstanding, the statistical methodologies developed so far mostly
deal with LD analysis and ancestral inference separately, using specialized models that
do not capture the close statistical and genetic relationships of these two problems.
Moreover, most of these approaches ignore the inherent uncertainty in the genetic com-
plexity (e.g., the number of genetic founders of a population) of the data and rely on
in(cid:13)exible models built on a pre-(cid:12)xed, closed genetic space. Recently, we have devel-
oped a nonparametric Bayesian framework for modeling genetic polymorphisms based
on the Dirichlet process (DP) mixtures and extensions, which attempts to allow more

proteins that form a complex to carry out enzymatic activities usually co-evolve.

504

Hidden Markov Dirichlet Process

Loci: 1

2

3

4

5

1A
2A
3A

KA

0iH

1iH

....

Inheritance of unknown generations

Figure 1: An illustration of a hidden Markov Dirichlet process for haplotype recombination and inher-
itance. Note that the total number of ancestors is unknown.

(cid:13)exible control over the number of genetic founders than has been provided by the
statistical methods proposed thus far (Xing et al. 2004) . In this paper, we leverage
on this approach and present a uni(cid:12)ed framework to model complex genetic inheri-
tance processes that allows recombinations among possibly in(cid:12)nite founding alleles and
coalescence-with-mutation events in the resulting genealogies.

We assume that individual chromosomes in a modern population are originated from
an unknown number of ancestral haplotypes via biased random recombinations and mu-
tations (Figure 1). The recombinations between the ancestors follow a state-transition
process we refer to as hidden Markov Dirichlet process (originated from the in(cid:12)nite
HMM by Beal et al. (2002)), which travels in an open ancestor space, with nonstation-
ary recombination rates depending on the genetic distances between SNP loci. Our
model draws inspiration from the HMM proposed in Greenspan and Geiger (2004b),
but we employ a two-level P(cid:19)olya urn scheme akin to the hierarchical DP (Teh et al.
2006) to accommodate an open ancestor space, and allow full posterior inference of the
recombination sites, mutation rates, haplotype origin, ancestor patterns, etc., condition-
ing on phased SNP data, rather than estimating them using information theoretic or
maximum likelihood principles. On both simulated and real genetic data, our model and
algorithm show competitive or superior performance on a number of genetic inference
tasks over the state-of-the-art parametric methods.

The remainder of this paper is presented as follows. In section 2, we formulate the
problem, and present details of the proposed model. In section 3, we describe a block
Gibbs sampling algorithm for posterior inference of the latent variables. In section 4, we
present experimental results on a simulated data haplotype data set, and on two pub-
lished real data sets, one from a single population, and the other from two populations.
We conclude with a brief discussion in section 6. A short version of this manuscript was
presented earlier in Sohn and Xing (2006), but the current version o(cid:11)ers more details
on the biological background, the model speci(cid:12)cations, and the experimental results.

Eric P. Xing and Kyung-Ah Sohn

2 The Statistical Model

505

Sequentially choosing recombination targets from a set of ancestral chromosomes can
be modeled as a hidden Markov process (Niu et al. 2002; Greenspan and Geiger 2004b),
in which the hidden states correspond to the index of the candidate chromosomes, the
transition probabilities correspond to the recombination rates between the recombining
chromosome pairs, and the emission model corresponds to a mutation process that
passes the chosen chromosome region in the ancestors to the descents. When the number
of ancestral chromosomes is not known, it is natural to consider an HMM whose state
space is countably in(cid:12)nite (Beal et al. 2002; Teh et al. 2006). In this section, we describe
such an in(cid:12)nite HMM formalism, which we would like to call hidden Markov Dirichlet
process, for modeling recombination in an open ancestral space.

2.1 Dirichlet Process mixtures

For self-containedness, we begin with a quick overview of the fundamentals of the Dirich-
let process and its connection to the coalescent process in population genetics, followed
by a brief recapitulation of the basic Dirichlet process mixture model we proposed
in Xing et al. (2004) for haplytope inheritance without recombination.

As mentioned earlier, a haplotype refers to the joint allele con(cid:12)guration of a contigu-
ous list of SNPs located on a chromosome. Under a well-known genetic model known
as coalescence with in(cid:12)nite-many-alleles (IMA) mutations (but without recombination),
one can treat a haplotype from a modern individual as a descendent of a most recent
common ancestor (MRCA) of unknown haplotype via random mutations that alter the
allelic states of some SNPs (Kingman 1982). Hoppe (1984) observed that a coales-
cent process in an in(cid:12)nite population leads to a partition of the population at every
generation that can be succinctly captured by the following P(cid:19)olya urn scheme.

Consider an urn that at the outset contains a ball of a single color. At each step
we either draw a ball from the urn and replace it with two balls of the same color, or
we are given a ball of a new color which we place in the urn. One can see that such a
scheme leads to a partition of the balls according to their color. Mapping each ball to a
haploid individual z and each color to a possible haplotype, this partition is equivalent
to the one resulting from the coalescence-with-mutation process (Hoppe 1984), and the
probability distribution of the resulting allele spectrum|the numbers of colors (resp.
haplotypes) with every possible number of representative balls (resp. decedents)|is
captured by the well-known Ewens’ sampling formula (Tavare and Ewens 1998).

Letting parameter (cid:11) de(cid:12)ne the probabilities of the two types of draws in the afore-
mentioned P(cid:19)olya urn scheme, and viewing each (distinct) color as a sample from Q0,
and each ball as a sample from Q x, Blackwell and MacQueen (1973) showed that this

zA haploid individual refers to an individual with only one haplotype | a simplifying assumption
often used on population genetics when the paternal and maternal haplotypes of a diploid individual
are inherited independently.

xHere we deviate from the conventional notations in the statistics literature (e.g., Neal (2000);
Escobar and West (2002); Ishwaran and James (2001)) and use Q and Q0, instead of G and G0 (or

506

Hidden Markov Dirichlet Process

P(cid:19)olya urn model yields samples whose distributions are those of Q0 the marginal prob-
abilities under the Dirichlet process (Ferguson 1973). Formally, a random probability
measure Q is generated by a DP if for any measurable partition B1; : : : ; Bk of the sam-
ple space, the vector of random probabilities Q(Bi) follows a Dirichlet distribution:
(Q(B1); : : : ; Q(Bk)) (cid:24) Dir((cid:11)Q0(B1); : : : ; (cid:11)Q0(Bk)), where (cid:11) denotes a scaling param-
eter and Q0 denotes a base measure. The P(cid:19)olya urn model makes explicit that the
association of data points to colors de(cid:12)nes a \clustering" of the data. Speci(cid:12)cally, hav-
ing observed n values ((cid:30)1; : : : ; (cid:30)n) sampled from a Dirichlet process DP ((cid:11); Q0), the
probability of the (n + 1)th value is given by:

(cid:30)n+1j(cid:30)1; : : : ; (cid:30)n; (cid:11); Q (cid:24)

1

n + (cid:11)

(cid:14)(cid:30)i ((cid:1)) +

(cid:11)

n + (cid:11)

Q0((cid:1));

(1)

n

Xi=1

where (cid:14)(cid:30)i ((cid:1)) denotes a point mass at value (cid:30)i. Another very useful representation of DP
is the stick-breaking construction by Sethuraman (1994). This construction is based on
independent sequences of independent random samples f(cid:25)0
i=1 generated
in the following way: (cid:25)0
ij(cid:11); Q0 (cid:24) Beta(1; (cid:11)) and (cid:30)ij(cid:11); Q0 (cid:24) Q0, where Beta(a; b) is the
Beta distribution with parameter a and b. Let (cid:25)i = (cid:25)0
l) (analogous to a
process of repetitively breaking a stick at fraction (cid:25)0
l), Sethuraman showed that the
i=1 (cid:25)i(cid:14)(cid:30)i .
The (cid:30)i’s can be understood as the locations of samples in their space, and the (cid:25)i’s are
the weights of these samples.

random measure arising from DP ((cid:11); Q0) admits the representation Q = P1

iQk(cid:0)1

k;ig1

i=1 and f(cid:30)ig1

l=1 (1 (cid:0) (cid:25)0

The discrete nature of the DP, as obviated from the stick-breaking construction, is
well suited for the problem of placing priors on mixture components in mixture model-
ing. In the context of mixture models, one can associate mixture component centroids
(e.g., haplotype founders, as explained in the sequel) with colors in the P(cid:19)olya urn model
and thereby de(cid:12)ne a \clustering" of the (possibly noisy) data (e.g., modern haplotypes
that are \recognizable" variants of their corresponding founders). This mixture model
is known as a DP mixture (Antoniak 1973; Escobar and West 2002) (also known as
\in(cid:12)nite" mixture model in machine learning community). Note that a DP mixture re-
quires no prior speci(cid:12)cation of the number of components, which is typically unknown in
genetic demography and general data clustering problems. It is important to emphasize
that here DP is used as a prior distribution of mixture components. Multiplying this
prior by a likelihood that relates the mixture components to the actual data yields a pos-
terior distribution of the mixture components, and the design of the likelihood function is
completely up to the modeler based on speci(cid:12)c problems. MCMC algorithms have been
developed to sample from the posterior associated with DP priors (Escobar and West
2002; Neal 2000; Ishwaran and James 2001). This nonparametric Bayesian formalism
forms the technical foundation of the haplotype modeling and inference algorithms to
be developed in this paper.

Back to haplotype modeling, a straightforward statistical genetics argument shows
that the distribution of haplotypes can be formulated as a mixture model, where the set

H), to denote the random probability measure under DP and the base measure of DP, because in
the genetic context, G and H have been used to denote the genotype and haplotype of polymorphic
markers (Pritchard et al. 2000; Stephens et al. 2001; Li and Stephens 2003; Xing et al. 2004).

Eric P. Xing and Kyung-Ah Sohn

507

of mixture components corresponds to the pool of ancestor haplotypes, or founders, of
the population (Exco(cid:14)er and Slatkin 1995; Niu et al. 2002; Kimmel and Shamir 2004).
Crucially, however, the size of this pool is unknown; indeed, knowing the size of the
pool would correspond to knowing something signi(cid:12)cant about the genome and its
history. On the other hand, despite its elegance, with a purely coalescence-based model
for genetic patterns, it is hard to perform statistical inference of ancestral features
and many other interesting genetic variables (for a large population, the number of
hidden variables in a coalescence tree is prohibitively large) (Stephens et al. 2001). In
most practical population genetic problems, usually the detailed genealogical structure
of a population (as provided by the coalescent trees) is of less importance than the
population-level features such as the pattern of major common ancestor alleles (i.e.,
founders) in a population bottleneck {, the age of such alleles, etc. In this case, the
DP mixture o(cid:11)ers a principled approach to generalize the (cid:12)nite mixture model for
haplotypes to an in(cid:12)nite mixture model that models uncertainty regarding the size of
the ancestor haplotype pool; at the same time, it provides a reasonable approximation to
the coalescence model by utilizing the partition structure resulting therefrom (but allows
further mutations within each partite to introduce further diversity among descents of
the same founder, which correspond to the balls with the same color in the P(cid:19)olya
urn metaphor). Without further digression, below we summarize the Dirichlet process
mixture model we proposed in Xing et al. (2004) for haplytope inheritance without
recombination.

Write Hi = [Hi;1; : : : ; Hi;T ] for a haplotype over T SNPs from chromosome i k; let
Ak = [Ak;1; : : : ; Ak;T ] denote an ancestor haplotype (indexed by k) and (cid:18)k denote the
mutation rate of ancestor k; and let Ci denote an inheritance variable that speci(cid:12)es the
ancestor of haplotype Hi. Under a DP mixture, we have the following P(cid:19)olya urn scheme
for sampling modern haplotypes:

(cid:15) Draw (cid:12)rst haplotype:

a1 j DP((cid:28); Q0) (cid:24) Q0((cid:1)), sample the 1st founder;

h1 (cid:24) Ph((cid:1)ja1; (cid:18)1),

sample the 1st haplotype from an inheritance model
de(cid:12)ned on the 1st founder;

(cid:15) for subsequent haplotypes:

{ sample the founder indicator for the ith haplotype:

cijDP((cid:28); Q0) (cid:24)8<
:

p(ci = cj for some j < ijc1; :::; ci(cid:0)1) =

ncj

i(cid:0)1+(cid:11)0

p(ci 6= cj for all j < ijc1; :::; ci(cid:0)1) = (cid:11)0

i(cid:0)1+(cid:11)0

where nci is the occupancy number of class ci|the number of previous samples be-
longing to class ci.

{A stage in coalescence when there are only a very small number of founding haplotype patterns

surviving and giving rise to all the haplotypes in the modern population.

kWe ignore the parental origin index of haplotypes as used in Xing et al. (2004), and assume
that the paternal and maternal haplotypes of each individual are given unambiguously (i.e., phased,
as known in genetics), as is the case in many LD and haplotype-block analyses (Daly et al. 2001;
Anderson and Novembre 2003). But it is noteworthy that our model can generalize straightforwardly
to unphased genotype data by incorporating a simple genotype model as in Xing et al. (2004).

508

Hidden Markov Dirichlet Process

{ sample the founder of haplotype i (indexed by ci):

(cid:30)ci jDP((cid:28); Q0)8>><
>>:

= facj ; (cid:18)cj g

if ci = facj ; (cid:18)cj g for some j < i (i.e., ci refers to an
inherited founder)

(cid:24) Q0(a; (cid:18))

ci 6= cj

if
founder)

for all

j < i (i.e., ci refers to a new

{ sample the haplotype according to its founder:

hi j ci (cid:24) Ph((cid:1)jaci ; (cid:18)ci ).

The usefulness of the DP mixture framework for the haplotype problem should
be clear|using a Dirichlet process prior we in essence maintain a pool of haplotype
founders that grows as observed individual haplotypes are processed. But notice that
the above generative process assumes each modern haplotype originates from a single
ancestor, which is only true for haplotypes spanning a short region on a chromosomal.
Now we consider long haplotypes possibly bearing multiple ancestors due to recombi-
nations between an unknown number of founders.

2.2 Hidden Markov Dirichlet Process (HMDP)

In a standard HMM, state-transitions across a discrete time- or space-interval take
place in a (cid:12)xed-dimensional state space, thus it can be fully parameterized by, say, a
K-dimensional initial-state probability vector (cid:25)0 and a K (cid:2) K state-transition prob-
ability matrix (cid:5)K(cid:2)K . As (cid:12)rst proposed in Beal et al. (2002), and later discussed in
Teh et al. (2006), one can \open" the state space of an HMM by treating the now in(cid:12)-
nite number of discrete states of the HMM as the support of a DP, and the transition
probabilities to these states from some source as the masses associated with these states.
In particular, for each source state (say, state j), the possible transitions to the target
states need to be modeled by a unique DP Qj. Since all possible source states and
target states are taken from the same in(cid:12)nite state space, overall we need an open set
of DPs with di(cid:11)erent mass distributions on the SAME support (to capture the fact that
di(cid:11)erent source states can have di(cid:11)erent transition probabilities to any target state).
In the sequel, we describe such a nonparametric Bayesian HMM using an intuitive hi-
erarchical P(cid:19)olya urn construction. We call this model a hidden Markov Dirichlet
process.

In an HMDP, both the columns and rows of the transition matrix (cid:5) are in(cid:12)nite
dimensional. To construct such an stochastic matrix, we will exploit the fact that in
practice only a (cid:12)nite number of states (although we don’t know what they are) will
be visited by each source state, and we only need to keep track of these states. The
following sampling scheme based on a hierarchical P(cid:19)olya urn scheme captures this spirit
and yields a constructive de(cid:12)nition of HMDP.

We set up a single \stock" urn at the top level, which contains balls of colors that
are represented by at least one ball in one or multiple urns at the bottom level. At
the bottom level, we have a set of distinct urns which are used to de(cid:12)ne the initial and
transition probabilities of the HMDP model (and are therefore referred as HMM-urns).

Eric P. Xing and Kyung-Ah Sohn

509

Speci(cid:12)cally, one of the HMM urns, Q0, is set aside to hold colored balls to be drawn at
the onset of the HMM state-transition sequence (cid:3)(cid:3). Each of the remaining HMM urns
is painted with a color represented by at least one ball in the stock urn, and is used
to hold balls to be drawn during the execution of a Markov chain of state-transitions.
Now let’s suppose that at time t the stock urn contains n balls of K distinct colors
indexed by an integer set C = f1; 2; : : : ; Kg; the number of balls of color k in this urn is
denoted by nk; k 2 C. For urn Q0 and urns Q1; : : : ; QK, let mj;k denote the number of

(cid:28)

balls of color k in urn Qj, and mj =Pk2C mj;k denote the total number of balls in urn

Qj . Suppose that at time t (cid:0) 1, we had drawn a ball with color k0. Then at time t, we
either draw a ball randomly from urn Qk0 , and place back two balls both of that color;
mj +(cid:28) we turn to the top level. From the stock urn, we can either
or with probability
draw a ball randomly and put back two balls of that color to the stock urn and one to
Qk0 , or obtain a ball of a new color K + 1 with probability (cid:13)
n+(cid:13) and put back a ball of
this color to both the stock urn and urn Qk0 of the lower level. Essentially, we have a
master DP Q0 (the stock urn) that serves as a source of atoms for in(cid:12)nite number of
child DPs fQjg (the HMM-urns). As pointed out in Teh et al. (2006), this model can
be viewed as an instance of the hierarchical Dirichlet process mixture model, with an
in(cid:12)nite number of DP mixtures as components. Speci(cid:12)cally, we have:

Q0j(cid:11); F (cid:24) DP((cid:11); F );
Qjj(cid:28); Q0 (cid:24) DP((cid:28); Q0); The HMM DP over target states of source j.

The master DP over target states common for all sources;

From the above equation we see that the base measure of the DP mixture associated
each of the source states in the HMM is itself drawn from a Dirichlet process DP((cid:11); F ).
Since a draw from a DP is a discrete measure with probability 1, atoms drawn from
this measure|atoms which are used as targets for each of the (unbounded number of)
source states|are not generally distinct. Indeed, the transition probabilities from each
of the source states have the same support|the atoms in Q0.

The P(cid:19)olya urn scheme described above is similar in spirit to the \Chinese restaurant
franchise" scheme discussed in Teh et al. (2006), but it di(cid:11)ers in that it avoids having
separate occupancy counters in each lower-level DP for repeated draws of the same atom
from a top-level DP, and it also motivates a simpler sampling scheme for inference as
discussed in Section 3.

Associating each color k with an ancestor con(cid:12)guration (cid:30)k = fak; (cid:18)kg whose values
are drawn from the base measure F , and recalling our discussion in the previous section,
we know that draws from the stock urn can be viewed as marginals from a random
measure distributed as a Dirichlet Process Q0 with parameter ((cid:11); F ). Speci(cid:12)cally, for
n random draws (cid:30) = f(cid:30)1; : : : ; (cid:30)ng from Q0, the conditional prior for ((cid:30)nj(cid:30)(cid:0)n), where
the subscript \(cid:0)n" denotes the index set of all but the n-th ball, is

(cid:30)nj(cid:30)(cid:0)n (cid:24)

nk

n (cid:0) 1 + (cid:11)

(cid:14)(cid:30)(cid:3)

k

((cid:30)n) +

(cid:11)

n (cid:0) 1 + (cid:11)

F ((cid:30)n);

(2)

K

Xk=1

(cid:3)(cid:3)Purposely, we overload the symbol Qj to let it denote both the urns in the hierarchical P(cid:19)olya urn

scheme, and the Dirichlet processes distributions represented by each of these urns.

510

Hidden Markov Dirichlet Process

where (cid:30)(cid:3)
k; k = 1; : : : ; K denote the K distinct values (i.e., colors) of (cid:30) (i.e., all the balls
in the stock urn), nk denote the number of balls of color k in the top urn, and (cid:14)a((cid:30)i)
denotes a unit point mass at (cid:30)i = a.

Conditioning on the Dirichlet process underlying the stock urn, the samples in the

jth bottom-level urn are also distributed as marginals under a Dirichlet measure:

(cid:30)mj j(cid:30)(cid:0)mj (cid:24)

=

K

K

Xk=1
Xk=1

mj;k + (cid:28)

nk

n(cid:0)1+(cid:11)

mj (cid:0) 1 + (cid:28)

(cid:14)(cid:30)(cid:3)

k

((cid:30)mj ) +

(cid:28)

(cid:11)

mj (cid:0) 1 + (cid:28)

n (cid:0) 1 + (cid:11)

F ((cid:30)mj )

(cid:25)j;k(cid:14)(cid:30)(cid:3)

k

((cid:30)mj ) + (cid:25)j;K+1Q0((cid:30)mj );

(3)

mj;k+(cid:28)

nk

n(cid:0)1+(cid:11)

mj (cid:0)1+(cid:28)

n(cid:0)1+(cid:11) . Let (cid:25)j (cid:17) [(cid:25)j;1; (cid:25)j;2; : : :]. Now we
where (cid:25)j;k (cid:17)
have an in(cid:12)nite-dimensional Bayesian HMM that, given F; (cid:11); (cid:28) , and all initial states
and transitions sampled so far, follows an initial states distribution parameterized by
(cid:25)0, and transition matrix (cid:5) whose rows are de(cid:12)ned by f(cid:25)j : j > 0g.

, (cid:25)j;K+1 (cid:17)

mj (cid:0)1+(cid:28)

(cid:28)

(cid:11)

Finally, as in, e.g., Escobar and West (2002) and Rasmussen (2000), we can also
introduce vague priors such as a Gamma or an inverse Gamma for the scaling parameters
(cid:11) and (cid:28) .

2.3 HMDP Model for Recombination and Inheritance

Now we describe a stochastic model, based on an HMDP, for generating individual
haplotypes in a modern population from a hypothetical pool of ancestral haplotypes via
recombination and mutations (i.e., random mating with neutral selection). See Figure
1 for an illustration.

First recall that a base measure F at the top of our hierarchical P(cid:19)olya urn scheme
is de(cid:12)ned as a distribution from which ancestor haplotype templates (cid:30)k are drawn. We
de(cid:12)ne the base measure F as a joint measure on both ancestor A and mutation rate (cid:18),
and let F (A; (cid:18)) = p(A)p((cid:18)), where p(A) is uniform over all possible haplotypes and p((cid:18))
is a beta distribution, Beta((cid:11)h; (cid:12)h), with a small value for (cid:12)h=((cid:11)h + (cid:12)h) corresponding
to a prior expectation of a low mutation rate. For simplicity, we assume each Ak;t (and
also each Hi;t) takes its value from an allele set B.

Now for each modern chromosome i, let Ci = [Ci;1; : : : ; Ci;T ] denote the sequence
of inheritance variables specifying the index of the ancestral chromosome at each SNP
locus. When no recombination takes place during the inheritance process that produces
haplotype Hi (say, from ancestor k), then Ci;t = k; 8t. When a recombination occurs,
say, between loci t and t + 1, we have Ci;t 6= Ci;t+1. We can introduce a Poisson point
process to control the duration of non-recombinant inheritance. That is, given that
Ci;t = k, then with probability e(cid:0)dr + (1 (cid:0) e(cid:0)dr)(cid:25)kk, where d is the physical distance
between two loci, r re(cid:13)ects the rate of recombination per unit distance, and (cid:25)kk is
the self-transition probability of ancestor k de(cid:12)ned by HMDP, we have Ci;t+1 = Ci;t;
otherwise, the source state (i.e., ancestor chromosome k) pairs with a target state (e.g.,

Eric P. Xing and Kyung-Ah Sohn

511

ancestor chromosome k0) between loci t and t + 1, with probability (1 (cid:0) e(cid:0)dr)(cid:25)kk0 .
Hence, each haplotype Hi is a mosaic of segments of multiple ancestral chromosomes
from the ancestral pool fAkg1
k=1. Essentially, the model we described so far is a time-
inhomogeneous in(cid:12)nite HMM. When the physical distance information between loci is
not available, we can simply set r to be in(cid:12)nity (hence e(cid:0)dr (cid:25) 0) so that we are back
to a standard stationary HMDP model with in(cid:12)nite dimensional transition probability
matrix (cid:5)1(cid:2)1 described earlier.

The emission process of the HMDP corresponds to an inheritance model from an
ancestor to the matching descendent. For simplicity, we adopt the single-locus mutation
model in Xing et al. (2004):

p(htjat; (cid:18)) = (cid:18)I(ht=at)(cid:16) 1 (cid:0) (cid:18)

jBj (cid:0) 1(cid:17)I(ht6=at)

;

(4)

where ht and at denote the alleles at locus t of an individual haplotype and its cor-
responding ancestor, respectively; (cid:18) indicates the ancestor-speci(cid:12)c mutation rate; and
jBj denotes the number of possible alleles. As discussed in Liu et al. (2001), this model
corresponds to a star genealogy resulting from infrequent mutations over a shared an-
cestor, and is widely used in statistical genetics as an approximation to a full coalescent
genealogy starting from the shared ancestor.

Assume that the mutation rate (cid:18) admits a Beta prior with hyperparameter ((cid:11)h; (cid:12)h) yy,
the marginal conditional likelihood of all the haplotype instances h = fhi;t
i 2
f1; 2; : : : ; Ig; t 2 f1; 2; : : : ; T gg given the set of ancestors a = fa1; : : : ; aKg and the
ancestor indicators c = fci;t
: i 2 f1; 2; : : : ; Ig; t 2 f1; 2; : : : ; T gg can be obtained by
integrating out (cid:18) from the joint conditional probability starting from Equation (4) as
follows:

:

p(hjc; a) = Yk (cid:16)Z Yi;tjci;t=k

p(hi;t; (cid:18)kjak;t)R((cid:11)h; (cid:12)h)(cid:18)(cid:11)h (cid:0)1

k

(1 (cid:0) (cid:18)k)(cid:12)h(cid:0)1d(cid:18)k(cid:17)

R((cid:11)h; (cid:12)h)

= Yk

(cid:0)((cid:11)h + lk)(cid:0)((cid:12)h + l0
(cid:0)((cid:11)h + (cid:12)h + lk + l0

k)

k)(cid:16)

k

1

jBj (cid:0) 1(cid:17)l0

(5)

where (cid:0)((cid:1)) is the gamma function, R((cid:11)h; (cid:12)h) = (cid:0)((cid:11)h+(cid:12)h)

associated with Beta((cid:11)h; (cid:12)h) (which is a prior distribution for (cid:18)), lk =PtPi

(cid:0)((cid:11)h)(cid:0)((cid:12)h) is the normalization constant
I(hi;t =
ak;t)I(ci;t = k) is the number of alleles that were not mutated with respect to the
ancestral allele, and l0
I(hi;j 6= ak;j )I(ci;t = k) is the number of mutated
alleles. The counting record lk = flk; l0

kg is a su(cid:14)cient statistic for the parameter (cid:18)k.

k = PtPi

The generative process and likelihood functions described above point naturally to
an algorithm for population genetic inference. Unlike the classical coalescence mod-
els for recombination (Hudson 1983), which have been primarily used for theoretical
analysis and simulation, but are hardly feasible for reverse ancestral inference based on

yyFor simplicity, we assume that the mutation rates pertaining to di(cid:11)erent ancestors follow the same

prior Beta((cid:11)h ; (cid:12)h).

512

Hidden Markov Dirichlet Process

observed genetic data, the HMDP model described above for recombination and inheri-
tance provides a semi-parametric Bayesian formalism that is well suited for data-driven
posterior inference on the latent variables that can yield rich information on the pop-
ulation ancestry and genetic structure of the study population. For example, under a
HMDP, given the haplotype data, one can infer the ancestral pattern, LD structure and
recombination hotspot of a population using the posterior distribution of inheritance
variable c and ancestral state a, as we will elaborate in the sequel.
It is also possi-
ble to infer the age of the haplotype alleles and/or the time of recombination events
by exploring the posterior estimates of the mutation and recombination rates under
HMDP.

3 Posterior Inference

In this section, we describe a Gibbs sampling algorithm for posterior inference under
HMDP. Recall that a Gibbs sampler draws samples of each random variable (or subset of
random variables) in the model from the conditional distribution of the variable(s) given
(previously sampled) values of all the remaining variables. The variables of interest in
our model include fCi;tg, the inheritance variables specifying the origins of SNP alleles
of all loci on each haplotype, and fAk;tg, the founding alleles at all loci of each ancestral
haplotype. All other variables in the model, e.g., the mutation rate (cid:18), are integrated
out.

We assume that the individual haplotypes fHie;tg are given unambiguously for the
study population, as is the case in many LD and haplotype-block analyses (Daly et al.
2001; Anderson and Novembre 2003); but it is noteworthy that our model can generalize
straightforwardly to unphased genotype data by incorporating a simple genotype model
as in Xing et al. (2004). Given that haplotypes are unambiguous, we can now treat the
paternal and maternal haplotypes of N individual as 2N iid samples from the HMDP
process and omit the parental index e.

The Gibbs sampler alternates between two sampling stages. First it samples the

inheritance variables fCi;tg, conditioning on all given individual haplotypes
h = fh1; : : : ; h2N g, and the most recently sampled con(cid:12)guration of the ancestor pool
a = fa1; : : : ; aKg; then given h and current values of the Ci;t’s, it samples every ancestor
ak.

To improve the mixing rate, we sample the inheritance variables one block at a time.
That is, every time we sample (cid:14) consecutive states ct+1; : : : ; ct+(cid:14) starting at a randomly
chosen locus t + 1 along a haplotype. (For simplicity we omit the haplotype index i here
and in the forthcoming expositions when it is clear from context that the statements
or formulas apply to all individual haplotypes). Let c(cid:0) denote the set of previously
sampled inheritance variables. Let n denote the totality of occupancy records of the
top-level DP (i.e. the \stock urn") | fng [ fnk
: 8kg, and m denote the totality
of the occupancy records of each lower-level DP (i.e., the urns corresponding to the
: 8k; k0g. Let lk
recombination choices by each ancestor) | fmk
denote the su(cid:14)cient statistics associated with all haplotype instances originating from

: 8kg [ fmk;k0

Eric P. Xing and Kyung-Ah Sohn

513

ancestor k. The predictive distribution of a (cid:14)-block of inheritance variables can be
written as:

p(ct+1:t+(cid:14) jc(cid:0); h; a) / p(ct+1:t+(cid:14) jct; ct+(cid:14)+1; m; n)p(ht+1:t+(cid:14)jact+1;t+1; : : : ; act+(cid:14);t+(cid:14))

/

t+(cid:14)

Yj=t

p(cj+1jcj; m; n)

t+(cid:14)

Yj=t+1

p(hjjacj ;j; lcj ):

(6)

This expression is simply Bayes’ theorem with p(ht+1:t+(cid:14)jact+1;t+1; : : : ; act+(cid:14);t+(cid:14)) playing
the role of the likelihood and p(ct+1:t+(cid:14) jc(cid:0); h; a) playing the role of the posterior. One
should be careful that the su(cid:14)cient statistics n, m and l employed here should exclude
the contributions by samples associated with the (cid:14)-block to be sampled. Note that
naively, the sampling space of an inheritance block of length (cid:14) is jAj(cid:14) where jAj represents
the cardinality of the ancestor pool. However, if we assume that the recombination rate
is low and block length is not too big, then the probability of having two or more
recombination events within a (cid:14)-block is very small and thus can be ignored. This
approximation reduces the sampling space of the (cid:14)-block to O(jAj(cid:14)), i.e., jAj possible
recombination targets times (cid:14) possible recombination locations. Accordingly, Eq. (6)
reduces to:

p(ct+1:t+(cid:14) jc(cid:0); h; a) / p(ct0 jct0

(cid:0)1 = ct; m; n)p(ct+(cid:14)+1 jct+(cid:14) = ct0 ; m; n)

t+(cid:14)

Yj=t0

p(hjjac

t0 ;j; lct0 );

(7)

for some t0 2 [t + 1; t + (cid:14)]. Recall that in an HMDP model for recombination, given that
the total recombination probability between two loci d-units apart is (cid:21) (cid:17) 1 (cid:0) e(cid:0)dr (cid:25) dr
(assuming d and r are both very small), the transition probability from state k to state
k0 is:

p(ct0 = k0 jct0

(cid:0)1 = k; m; n; r; d)

= (cid:26) (cid:21)(cid:25)k;k0 + (1 (cid:0) (cid:21))(cid:14)(k; k0)

(cid:21)(cid:25)k;K+1

for k0 2 f1; :::; Kg, i.e., transition to an existing ancestor,
for k0 = K + 1, i.e., transition to a new ancestor,

(8)

where (cid:25)k represents the transition probability vector for ancestor k under HMDP, as
de(cid:12)ned in Eq. (3). Note that when a new ancestor aK+1 is instantiated, we need to
immediately instantiate a new DP under F to model the transition probabilities from
this ancestor to all instantiated ancestors (including itself). Since the occupancy record
of this DP, mK+1 := fmK+1g [ fmK+1;k : k = 1; : : : ; K + 1g, is not yet de(cid:12)ned at the
onset, with probability 1 we turn to the top-level DP when departing from state K +1 for
the (cid:12)rst time. Speci(cid:12)cally, we de(cid:12)ne p((cid:1)jct0 = K + 1) according to the occupancy record
of ancestors in the stock urn. For example, at the distal border of the (cid:14)-block, since
ct+(cid:14)+1 always indexes a previously inherited ancestor (and therefore must be present in
the stock-urn), we have:

p(ct+(cid:14)+1 jct+(cid:14) = K + 1; m; n) = (cid:21) (cid:2)

nct+(cid:14)+1

n (cid:0) 1 + (cid:11)

:

(9)

Now we can substitute the relevant terms in Eq.
(8) and (9). The
marginal likelihood term in Eq. (6) can be readily computed based on Eq. (4), by
integrating out the mutation rate (cid:18) under a Beta prior (and also the ancestor a under

(6) with Eqs.

514

Hidden Markov Dirichlet Process

a uniform prior if ct0 refers to an ancestor to be newly instantiated) (Xing et al. 2004).
Putting everything together, we have the proposal distribution for a block of inheritance
variables. Upon sampling every ct, we update the su(cid:14)cient statistics n, m and flkg
as follows. First, before drawing the sample, we erase the contribution of ct to these
su(cid:14)cient statistics. In particular, if an ancestor gets no occupancy in either the stock or
the HMM urns afterwards, we remove it from our repository. Then, after drawing a new
ct, we increment the relevant counts accordingly. In particular, if ct = K + 1 (i.e., a new
ancestor is to be drawn), we update n = n+1, set nK+1 = 1, mct = mct +1; mct;K+1 = 1,
and set up a new (empty) HMM urn with color K + 1 (i.e.
instantiating mK+1 with
all elements equal to zero).

Now we move on to sample the founders fak;tg. From the mutation model in Equa-
zz:

tion (4), we can derive the following posterior distribution to sample the founder ak

p(ak;tjc; h) /Z (cid:16) Yijci;t=k

p(hi;tjak;t; (cid:18))(cid:17)Beta((cid:18)j(cid:11)h; (cid:12)h)d(cid:18)

(cid:0)((cid:11)h + lk;t)(cid:0)((cid:12)h + l

0

k;t)

(cid:0)((cid:11)h + (cid:12)h + lk;t + l0

k;t)(jBj (cid:0) 1)l

0

k;t

=

R((cid:11)h; (cid:12)h);

(10)

0

where lk;t is the number of allelic instances originating from ancestor k at locus t that are
identical to the ancestor, when the ancestor has the pattern ak;t; and l
I(ci;t =
kjak;t) (cid:0) lk;t represents the complement. The normalization constant of this proposal
distribution can be computed by summing the R.H.S. of Eq. (10) over all possible allele
If k is not represented previously,
states of an ancestor at the locus being sampled.
we can just set lk;t and l
k;t both to zero. Note that when sampling a new ancestor,
we can only condition on a small segment of an individual haplotype. To instantiate
a complete ancestor, after sampling the alleles in the ancestor corresponding to the
segment according to Eq. (10), we (cid:12)rst (cid:12)ll in the rest of the loci with random alleles.
When another segment of an individual haplotype needs a new ancestor, we do not
naively create a new full-length ancestor; rather, we use the empty slots (those with
random alleles) of one of the previously instantiated ancestors, if any, so that the number
of ancestors does not grow unnecessarily.

k;t =Pi

0

4 Experiments

We applied the HMDP model to both simulated and real haplotype data. Our analy-
ses focus on the following three popular problems in statistical genetics: 1. Ancestral
Inference: estimating the number of founders in a population and reconstructing the
ancestor haplotypes; 2) LD-block Analysis:
inferring the recombination sites in each
individual haplotype and uncover population-level recombination hotspots on the chro-
mosome region; 3) Population Structural Analysis: mapping the genetic origins of all

zzIn deriving Equation (10), instead of assuming a common mutation rate (cid:18)k for all loci of ancestor
ak, we endow each locus with its own mutation parameter (cid:18)k;t, with all parameters admitting the same
prior Beta((cid:11)h ; (cid:12)h). This is arguably a more accurate re(cid:13)ection of reality.

Eric P. Xing and Kyung-Ah Sohn

515

loci of each individual haplotype in a population.

4.1 Analyzing simulated haplotype population

To simulate a population of individual haplotypes, we started with a (cid:12)xed number, Ks
(unknown to the HMDP model), of randomly generated ancestor haplotypes, on each
of which a set of recombination hotspots were pre-speci(cid:12)ed. Then we applied a hand-
speci(cid:12)ed recombination process, which is de(cid:12)ned by a Ks-dimensional HMM, to the
ancestor haplotypes to generate Ns individual haplotypes, via sequentially recombining
segments of di(cid:11)erent ancestors according to the simulated HMM states at each locus,
and mutating certain ancestor SNP alleles according to the emission model. All the
ancestor haplotypes were set to be 100 SNPs long. At the hotspots (pre-speci(cid:12)ed at
every 10-th loci in the ancestor haplotypes), we de(cid:12)ned the recombination rate to be
0.05, otherwise it is 0.00001. We simulated the recombination process for each progeny
haplotype; but to force every progeny haplotype to have at least one recombination, in
the rare cases where no recombination event was simulated for an progeny haplotype,
we sampled one of the hotspots randomly and forced it to recombine with another
ancestor chosen at random at that loci. (Thus our simulated samples were not exactly
distributed according to the generative model we used, but such samples were arguably
more close to the real data.) Overall, 30 datasets each containing 100 individuals (i.e.,
200 haplotypes) with 100 SNPs were generated from Ks = 5 ancestor haplotypes.

As baseline models, we also implemented 3 standard (cid:12)xed-dimensional HMM, with
K 0 equal to 3, 5 (the true number of ancestors for the simulated) and 10 hidden states,
respectively, which correspond to the number of ancestors available for recombination.
For these baseline HMMs, we follow the same mutation model for emission as that
of the HMDP (i.e., Eq. (4)), and we also subject the mutation rate to a Beta prior.
In these HMMs, the SNP-types of the ancestors at every locus, e.g., at;k, are treated
as the mean parameters of the observed SNPs samples at the corresponding locus; the
inheritance variables fCi;tg correspond to the latent states following a 1-st order Markov
process; and the transition models governing recombinations amongst the ancestors as
indicated by the values ci;t’s are parameterized by a K 0-dimensional stochastic matrix.
We estimate these parameters via a maximal likelihood principle using the Balm-Welch
algorithm. Note that since K 0 is chosen a priori, we cannot estimate the number of
ancestors using these HMMs.

Following a collapsed Gibbs sampling scheme (Liu 1994), we integrated out the
mutation rate (cid:18), and sample variables fAk;tg and fCi;tg iteratively. We monitor con-
vergence based on the occupancy counts of the top factors in the master DP. Typically,
convergence was achieved after around 3000 samples (Figure 2), and the samples ob-
tained after convergence (with proper de-autocorrelation, i.e., by using samples from
every 10 iterations over 5000 (cid:24) 10000 samples) are used for computing relevant su(cid:14)cient
statistics. To increase the chance of proper mixing, 10 independent runs of sampling,
with di(cid:11)erent random seeds, are simultaneously performed. Convergence is monitored
at runtime using an on-line minimal pairwise Gelman-Rubin (GR) statistic (Gelman
1998) of scalar summaries of the model parameters (e.g., average occupancy of top fac-

516

Hidden Markov Dirichlet Process

Figure 2: Sampling trace of the top three most occupied factors (ancestor chromosomes). The x-axis
represents the sampling iteration, and the y-axis represent the fraction of the occupancy (i.e., be chosen
as recombination target) of each factor over total occupancy.

tors) obtained in each Markov chain. The total running time for posterior inference on a
simulated data set described below was around 3.5 hours using a matlab implementation
on a Dell PowerEdge 1850 workstation with an Intel Xeon 3.6 GHz processor. (This
computation includes a huge disk-writing overhead for recording the running trace. The
actual CPU time for computing is less than 10% of that. We intend to soon release a
C++ implementation which is expected to further reduce computation cost.)

Ancestral Inference Using HMDP, we successfully recovered the correct number (i.e.,
K = 5) of ancestors in 21 out of 30 simulated populations; for the remaining 9 popula-
tions, we inferred 6 ancestors. From samples of ancestor states fak;tg, we reconstructed
the ancestral haplotypes under the HMDP model. For comparison, we also inferred the
ancestors under the 3 standard HMM using an EM algorithm. We de(cid:12)ne the ancestor
reconstruction error (cid:15)a for each ancestor to be the ratio of incorrectly recovered loci
over all the chromosomal sites. The average (cid:15)a over 30 simulated populations under
4 di(cid:11)erent models are shown in Figure 3a.
In particular, the average reconstruction
errors of HMDP for each of the (cid:12)ve ancestors are 0.026, 0.078, 0.116, 0.168, and 0.335,
respectively. There is a good correlation between the reconstruction quality and the
population frequency of each ancestor. Speci(cid:12)cally, the average (over all simulated
populations) fraction of SNP loci originated from each ancestor among all loci in the
population is 0.472, 0.258, 0.167, 0.068 and 0.034, respectively. As one would expect,
the higher the population frequency of an ancestor is, the better its reconstruction ac-
curacy. Interestingly, under the (cid:12)xed-dimensional HMM, even when we use the correct
number of ancestor states, i.e., K = 5, the reconstruction error is still very high (Fig-
ure 3), typically 2.5 times or higher than the error of HMDP. We conjecture that this
is because the non-parametric Bayesian treatment of the transition rates and ancestor
con(cid:12)gurations under the HMDP model leads to a desirable adaptive smoothing e(cid:11)ect
and also less constraints on the model parameters, which allow them to be more accu-
rately estimated. Whereas under a parametric setting, parameter estimation can easily
be sub-optimal due to lack of appropriate smoothing or prior constraints, or de(cid:12)ciency
of the learning algorithm (e.g., local-optimality of EM).

Eric P. Xing and Kyung-Ah Sohn

517

a

r
o
r
r
e

n
o
i
t
c
u
r
t
s
n
o
c
e
r

r
o
t
s
e
c
n
a

HMDP

HMM (K=3)

t

e
a
r
 
n
o

i
t

i

a
n
b
m
o
c
e
r
 
l

a
c
i
r
i
p
m
E

t

e
a
r
 
n
o

i
t

i

a
n
b
m
o
c
e
r
 
l

a
c
i
r
i
p
m
E

0.6

0.5

0.4

0.3

0.2

0.1

0

0.6

0.5

0.4

0.3

0.2

0.1

0

20

20

80

80

40

60

Position

HMM (K=10)

40

60

Position

80

80

40

60

Position

HMM (K=5)

40

60

Position

b

t

e
a
r
 
n
o

i
t

i

a
n
b
m
o
c
e
r
 
l

a
c
i
r
i
p
m
E

t

e
a
r
 
n
o

i
t

i

a
n
b
m
o
c
e
r
 
l

a
c
i
r
i
p
m
E

0.6

0.5

0.4

0.3

0.2

0.1

0

0.6

0.5

0.4

0.3

0.2

0.1

0

c

20

20

Figure 3: Analysis of simulated haplotype populations. (a) A comparison of ancestor reconstruction
errors for the (cid:12)ve ancestors (indexed along x-axis). The vertical lines show (cid:6)1 standard deviation over
30 populations. (b) Plots of the empirical recombination rates along 100 SNP loci in one of the 30
populations for HMDP and 3 HMMs. The dotted lines show the pre-speci(cid:12)ed recombination hotspots.
(c) The true (panel 1) and estimated (panel 2 for HMDP, and panel 3-5 for 3 HMMs) population maps
of ancestral compositions in a simulated population. Figures were generated using the software distruct
from Rosenberg et al [2002].

518

Hidden Markov Dirichlet Process

threshold

tolerance window

0

0.01
(cid:6) 1

(cid:6) 2

False positive rate
False negative rate

0.16

0.12

0.067

0

0

0

0.03
(cid:6) 1 (cid:6) 2

0.04
0.55

0.03
0.55

0

0.08
0.77

Table 1: False positive and false negative rates for recombination hotspot detection using medians of
the empirical recombination rates over 30 population samples as shown in Figure 4.

LD-block Analysis From samples of the inheritance variables fci;tg under HMDP, we
can infer the recombination status of each locus of each haplotype. We de(cid:12)ne the em-
pirical recombination rates (cid:21)e at each locus to be the ratio of individuals who had
recombinations at that locus over the total number of haploids in the population. Fig-
ure 3b shows plots of the (cid:21)e from HMDP and the 3 HMMs in one of the 30 simulated
populations. We can identify the recombination hotspots directly from such a plot based
on an empirical threshold (cid:21)t (i.e., (cid:21)t = 0:05). For comparison, we also give the true
recombination hotspots (depicted as dotted vertical lines) chosen in the ancestors for
simulating the recombinant population. The inferred hotspots (i.e., the (cid:21)e peaks) show
reasonable agreement with the reference in both HMDP and HMMs, but it appears that
in the HMMs the hotspots around position 20 and 60 are less obvious. Figure 4 shows a
boxplot of the empirical recombination rates at the 100 SNP loci estimated from the the
30 di(cid:11)erent population samples simulated from these ancestors. The gray vertical lines
along the x-axis correspond to the locations of pre-speci(cid:12)ed recombination hotspots.
A simple thresholding at 0.01 would identify 24 hotspots which include all the 9 true
hotspots and 15 false positive sites. This leads to the false negative rate to be 0 and the
false positive rate to be 0.16. To give credit to the false positive sites which are close to
the true hotspots, we may allow small discrepancy between the true hotspots and the
detected ones. By allowing (cid:6)2 sites discrepancy and eliminating possibly redundant
ones in the detection, (e.g., the two detected sites 70 and 71 would be just counted as
1 site of 70), the number of false positive sites decreased to 6, which resulted in the
false positive rate of 0.067 and the false negative rate unchanged. Using a threshold of
0.03, 10 hotspots would be detected, among which two sites agree with the true ones.
After allowing (cid:6)2 sites discrepancy 4 true hotspots could be identi(cid:12)ed with 3 remaining
false positive sites. The false positive and negative rates using these two thresholds are
summarized in Table 1.

Population Structural Analysis Finally, from samples of the inheritance variables fci;tg,
we can also uncover the genetic origins of all loci of each individual haplotype in a
population. For each individual, we de(cid:12)ne an empirical ancestor composition vector
(cid:17)e, which records the fractions of every ancestor in all the ci;t’s of that individuals.
Figure 3c displays a population map constructed from the (cid:17)e’s of all individual.
In
the population map, each individual is represented by a thin vertical line which is
partitioned into colored segments in proportion to the ancestral fraction recorded by (cid:17)e.
Five population maps, corresponding to (1) true ancestor compositions, (2) ancestor
compositions inferred by HMDP, and (3-5) ancestor compositions inferred by HMMs
with 3, 5, 10 states, respectively, are shown in Figure 3c. To assess the accuracy of

Eric P. Xing and Kyung-Ah Sohn

519

0.6

0.5

0.4

0.3

0.2

0.1

0

10

20

30

40

50

Position

60

70

80

90

Figure 4: Boxplot of the empirical recombination rates at the 100 SNP loci over 30 di(cid:11)erent simulated
population samples. The gray vertical lines show the pre-speci(cid:12)ed recombination hotspots used for
simulating the data.

our estimation, we calculated the distance between the true ancestor compositions and
the estimated ones as the mean squared distance between true and the estimated (cid:17)e
over all individuals in a population, and then over all 30 simulated populations. We
found that the distance between the HMDP-derived population map and the true map is
0:190(cid:6)0:0748, whereas the distance between HMM-map and true map is 0:319(cid:6)0:0676,
signi(cid:12)cantly worse than that of HMDP even though the HMM is set to have the true
number of ancestral states (i.e., K = 5). Because of dimensionality incompatibility and
apparent dissimilarity to the true map for other HMMs (i.e., K = 3 and 10), we forgo
the above quantitative comparison for these two cases.

To summarize our analyses on the simulated data, although the (cid:12)xed dimensional
HMMs are fast and easy to implement, they appear to o(cid:11)er much less accurate results
than that of the HMDP model on ancestor reconstruction, and population-map esti-
mation, even when the number of HMM states is set to the true number of haplotype
ancestors (which is in practice unknown). When the number of HMM states is chosen
incorrectly, the inference results degrade signi(cid:12)cantly. For hotspot prediction, quali-
tatively we have not seen signi(cid:12)cant di(cid:11)erences in the accuracy, although the HMDP
model appeared to be slightly better. We will look into this issue via a more quantitative
analysis in our later study.

Hidden Markov Dirichlet Process

520

a

.

b

.

Figure 5: Analysis of the Daly data. (a) A plot of (cid:21)e estimated via HMDP; and the haplotype block
boundaries according to HMDP (black solid line), HMM (Daly et al. 2001) (red dotted line), and
MDL (Anderson and Novembre 2003) (blue dashed line). (b) IT scores for haplotype blocks from each
method. The left panel shows cross-block MI and the right shows the average within-block entropy.
The total number of blocks inferred by each method are given on top of the bars.

Figure 6: The estimated population map of the Daly dataset.

4.2 Analyzing two real haplotype datasets

We applied HMDP to two real haplotype datasets, the single-population Daly data
(Daly et al. 2001), and the two-population (CEPH: Utah residents with northern/western
European ancestry; and YRI: Yoruba in Ibadan and Nigeria) HapMap data (Consortium"
2005; Thorisson et al. 2005). These data consist of trios of genotypes, so most of the
true haplotypes can be directly inferred from the genotype data. Note that for these
real biological data, there is no ground truth regarding the ancestral history, hotspot
location, and population composition, based on which we can validate our results, or
compare to other methods. We present our analysis as a demonstration of the utilities
of our model, which, to our knowledge, are not o(cid:11)ered jointly under a uni(cid:12)ed model
by extant methods in statistical genetics. (As we discuss in the sequel, some extant
methods can perform some of the inference tasks that HMDP does, and in these cases
we show a comparison.)

The single-population Daly dataset We (cid:12)rst analyzed the 256 individuals from Daly
data. This data set consists of the haplotypes 103 SNPs across a 616.7-kb region on
chromosome 5q31 of 129 trios from a European-derived population. Earlier studies
indicate that this region contains a genetic risk factor for Crohn disease. Earlier analysis
of this data set using a hidden Markov model revealed the existence of discrete haplotype

Eric P. Xing and Kyung-Ah Sohn

521

blocks, each with low diversity, in this region (Daly et al. 2001).

We compared the recovered recombination hotspots with those reported in

Daly et al. (2001) (which is based on an HMM employing di(cid:11)erent number of states at
di(cid:11)erent chromosome segments) and in Anderson and Novembre (2003) (which is based
on a minimal description length (MDL) principle applied to Daly’s HMM). Note that the
HMM used by Daly et al. (2001) and Anderson and Novembre (2003) is di(cid:11)erent from
the ones we used in our simulation study in section 4.1. Their HMM models a stochastic
process that selects haplotype-segments from pools of \ancestors" without mutation for
a concatenating list of haplotype-block regions constituting the study SNP sequences.
Each region has their own ancestor pool of possibly unequal sizes; thus between each pair
of adjacent blocks, the HMM needs a unique (possibly rectangular) stochastic matrix
for ancestor transitions. The block boundaries are (cid:12)xed under this HMM (and the only
stochasticity lies in the choice of local \ancestors" for each block), and determining
the block boundaries is treated as a model-selection problem based on a maximal-
likehood (Daly et al. 2001) or MDL (Anderson and Novembre 2003) principle. Strictly
speaking, Daly’s HMM model itself o(cid:11)ers little means to infer recombination events and
the ancestor association map, because the \ancestors" thereof are de(cid:12)ned independently
for each block rather than as whole founding chromosomes; di(cid:11)erent blocks have di(cid:11)erent
number of ancestors; and the determination of these \local ancestors" employs an initial
heuristic scan for regions of low haplotype diversity, whose formal connection to the
HMM model is not clear.

Figure 5a shows the plot of empirical recombination rates estimated under HMDP,
side-by-side with the reported recombination hotspots. There is no ground truth to
judge which one is correct; hence we computed information-theoretic (IT) scores based
on the estimated within-block haplotype frequencies and the between-block transition
probabilities under each model for a comparison. Figure 5b shows a comparison of these
scores for haplotype blocks obtained from HMDP and the other two sources. The left
panel of Figure 5b shows the total pairwise mutual information between adjacent haplo-
type blocks segmented by the recombination hotspots uncovered by the three methods.
The right panel shows the average entropies of haplotypes within each block. The num-
ber above each bar denotes the total number of blocks. The pairwise mutual information
score of the HMDP block structure is similar to that of the Daly structure, but smaller
than that of MDL. Similar tendencies are observed for average entropies. Note that
the Daly and the MDL methods allow the number of haplotype founders to vary across
blocks to get the most compact local ancestor constructions. Thus their reported scores
are an underestimate of the true global score because certain segments of an ancestor
haplotype that are not or rarely inherited are not counted in the score. Thus the low IT
scores achieved by HMDP suggest that HMDP can e(cid:11)ectively avoid inferring spurious
global and local ancestor patterns. This is con(cid:12)rmed by the population map shown
in Figure 6, which shows that HMDP recovered 6 ancestors and among them the 3
dominant ancestors account for 98% of all the modern haplotypes in the population.

We did not compare our results with that of Daly et al. (2001) and

Anderson and Novembre (2003) exhaustively, e.g., on ancestor reconstruction and popu-
lation map estimation, because their methods cannot perform these inferential tasks. In-

522

Hidden Markov Dirichlet Process

deed, to our knowledge there is no single model that does all the inferential tasks HMDP
is capable of. Thus we can only compare HMDP with specialized models on certain
tasks, as described above. Since implementations of the methods in Daly et al. (2001)
and Anderson and Novembre (2003) are not available, we can only compare with their
results reported on the original papers, which are obtained on the Daly data. But we
cannot apply their methods to our simulated data or the HapMap data for more in-
formative comparisons. The total running time of our algorithm on the Daly data set
(with the 3000 burn-in steps, 3000 samples, and 1 per 5 sample deceleration sampling
interval) is about 14hr, which includes the disk-writing overhead for trace-recording.

The two-population HapMap dataset The HapMap data was generated by the Interna-
tional HapMap Project that attempts to identify and catalog genetic similarities and dif-
ferences in human beings of di(cid:11)erent ethnic origins (Consortium" 2005; Thorisson et al.
2005). The current release of the whole HapMap data contains over 1 million SNPs,
from 269 individuals belonging to four populations.
In this study, we only focus on
a small subset of SNPs common to all populations; we use data from two of the four
populations, YRI and CEPH. Speci(cid:12)cally, we have 30 trios of YRI and 30 trios of
CEPH (i.e., 180 individuals in total), of which the 120 unrelated phase-known individu-
als corresponding to the parents in the trios were used in the experiment (the children’s
haplotypes are inherited from the parents and are redundant in the population). We
concern ourselves with 254 SNPs, which are located in the region of EN m010:7p15:2
spanning 497.5 kilo-basepair (kb). The computation time for analyzing this data set is
comparable to that of the Daly data set.

We applied HMDP to the union of the populations, with a random individual order.
Delightfully, the two-population structure is clearly retrieved from the population map
constructed from the population composition vectors (cid:17)e for every individual. As seen
in Figure 7a, the left half of the map clearly represents the CEPH population and the
right half the YRI population. We found that the two dominant haplotypes covered
over 85% of the CEPH population (and the overall breakup among all four ancestors
is 0.5618, 0.3036, 0.0827, 0.0518). On the other hand, the frequencies of each ancestor
in YRI population are 0.2141, 0.1784, 0.3209, 0.1622, 0.1215 and 0.0029, showing that
the YRI population is much more diverse than CEPH. This might explain an earlier
observation that genetic inference on the YRI population appeared to be more di(cid:14)cult
than for CEPH (Marchini et al. 2006). The recombination maps of the two di(cid:11)erent
populations also show noticeably di(cid:11)erent spatial patterns of recombination hotspots
(Figure 7b), which may re(cid:13)ect di(cid:11)erent recombination histories of the founders of the
two populations.

Note that the population partition result reported in Figure 7b is trivial because it
is inferred purely based on SNPs haplotypes without knowledge of ethnic labels of the
samples. In most genetic samples, ethnic labels are either not available or ambiguous
(e.g., the Daly data has no subpopulation details). By discovering the right popula-
tion separation, one can perform hotspot estimation for each population and capture
population-speci(cid:12)c LD (as in Figure 7a); whereas in a mixed population, one may not
be able to correctly estimate such patterns.

Eric P. Xing and Kyung-Ah Sohn

523

a

.

b

.

CEPH

YRI

Figure 7: Result on the two-population (CEPH and YRI) HapMap data. (a) The estimated population
map of the whole dateset with two populations.
(b) The estimated recombination rates along the
chromosomal position in the two populations.

5 Conclusion

We have proposed a new Bayesian approach for joint modeling of genetic recombinations
among possibly in(cid:12)nite founding alleles and coalescence-with-mutation events in the
resulting genealogies. By incorporating a hierarchical DP prior to the stochastic matrix
underlying an HMM, which facilitates a well-de(cid:12)ned transition process between in(cid:12)nitely
many ancestors, our proposed method can e(cid:14)ciently infer a number of important genetic
variables, such as recombination hotspot, mutation rates, haplotype origin, and ancestor
patterns, jointly underly a uni(cid:12)ed statistical framework.

Empirically, on both simulated and real data, our approach compares favorably to
its parametric counterpart|a (cid:12)xed-dimensional HMM (even when the number of its
hidden states, i.e., the ancestors, is correctly speci(cid:12)ed) and a few other specialized
methods, on ancestral inference, haplotype-block uncovering and population structural
analysis. We are interested in further investigating the behavior of an alternative scheme
based on reverse-jump MCMC over Bayesian HMMs with di(cid:11)erent latent states in
comparison with HMDP; we also intend to apply our methods to genome-scale LD and
demographic analysis using the full HapMap data. While our current model employs
only phased haplotype data, it is straightforward to generalize it to unphased genotype
data as provided by the HapMap project. HMDP can also be easily adapted to many
engineering and information retrieval contexts such as object and theme tracking in
open space.

524

Hidden Markov Dirichlet Process

References
Anderson, E. C. and Novembre, J. (2003). \Finding haplotype block boundaries by
using the minimum-description-length principle." Am J Hum Genet, 73: 336{354.
503, 507, 512, 520, 521, 522

Antoniak, C. E. (1973). \Mixtures of Dirichlet processes with applications to Bayesian

nonparametric problems." Annals of Statistics, 2: 1152{1174. 506

Beal, M. J., Ghahramani, Z., and Rasmussen, C. E. (2002). \The In(cid:12)nite Hidden
Markov Model." In Dietterich, T., Becker, S., and Ghahramani, Z. (eds.), Advances
in Neural Information Processing Systems 14, 577{584. MIT Press. 504, 505, 508

Blackwell, D. and MacQueen, J. B. (1973). \Ferguson Distributions Via P(cid:19)olya Urn

Schemes." Annals of Statistics, 1: 353{355. 505

Consortium", I. H. (2005). \A haplotype map of the human genome." Nature, 437:

1299{1320. 520, 522

Daly, M. J., Rioux, J. D., Scha(cid:11)ner, S. F., Hudson, T. J., and Lander, E. S. (2001).
\High-resolution haplotype structure in the human genome." Nature Genetics, 29(2):
229{232. 503, 507, 512, 520, 521, 522

Erosheva, E., Fienberg, S., and La(cid:11)erty, J. (2004). \Mixed-membership models of
scienti(cid:12)c publications." Proc Natl Acad Sci U S A, 101 (Suppl 1): 5220{5227. 503

Escobar, M. D. and West, M. (2002). \Bayesian density estimation and inference using
mixtures." Journal of the American Statistical Association, 90: 577{588. 505, 506,
510

Exco(cid:14)er, L. and Hamilton, G. (2003). \Comment on Genetic Structure of Human

Populations." Science, 300(5627): 1877b{. 503

Exco(cid:14)er, L. and Slatkin, M. (1995). \Maximum-likelihood estimation of molecular
haplotype frequencies in a diploid population." Molecular Biology and Evolution,
12(5): 921{7. 507

Falush, D., Stephens, M., and Pritchard, J. K. (2003). \Inference of population struc-
ture: Extensions to linked loci and correlated allele frequencies." Genetics, 164(4):
1567{1587. 503

Ferguson, T. S. (1973). \A Bayesian analysis of some nonparametric problems." Annals

of Statistics, 1: 209{230. 506

Gelman, A. (1998). \Inference and monitoring convergence." In Gilks, W. E., Richard-
son, S., and Spiegelhalter, D. J. (eds.), Markov Chain Monte Carlo in Practice. Boca
Raton, Florida: Chapman & Hall/CRC. 515

Greenspan, G. and Geiger, D. (2004a). \High density linkage disequilibrium mapping
using models of haplotype block variation." Bioinformatics, 20 (Suppl.1): 137{144.
503

Eric P. Xing and Kyung-Ah Sohn

525

| (2004b). \Model-Based Inference of Haplotype Block Variation." Journal of Com-

putational Biology, 11(2/3): 493{504. 504, 505

Hoppe, F. M. (1984). \P(cid:19)olya-like urns and the Ewens’ sampling formula." Journal of

Math. Biol., 20(1): 91{94. 505

Hudson, R. R. (1983). \Properties of a neutral allele model with intragenic recombina-

tion." Theor Popul Biol., 23(2): 183{201. 511

Ishwaran, H. and James, L. F. (2001). \Gibbs sampling methods for stick-breaking

priors." Journal of the American Statistical Association, 90: 161{173. 505, 506

Kimmel, G. and Shamir, R. (2004). \Maximum likelihood resolution of multi-block
genotypes." In In proceedings of the Eighth Annual International Conference on Re-
search in Computational Molecular Biology (RECOMB 2004), 2{9. The Association
for Computing Machinery. 507

Kingman, J. (1982). \On the genealogy of large populations." J. Appl. Prob., 19A:

27{43. 505

Li, N. and Stephens, M. (2003). \Modelling Linkage Disequilibrium, and identifying
recombination hotspots using SNP data Genetics." Genetics, 165: 2213{2233. 503,
506

Liu, J. S. (1994). \The collapsed Gibbs sampler with applications to a gene regulation

problem." J. Amer. Statist. Assoc, 89: 958{966. 515

Liu, J. S., Sabatti, C., Teng, J., Keats, B., and Risch, N. (2001). \Bayesian analysis of
Haplotypes for Linkage Disequilibrium Mapping." Genome Res., 11: 1716{1724. 511

Marchini, J., Cutler, D., Patterson, N., Stephens, M., Eskin, E., Halperin, E., Lin,
S., Qin, Z., Munro, H., Abecasis, G., Donnelly, P., and Consortium, I. H. (2006).
\A Comparison of Phasing Algorithms for Trios and Unrelated Individuals." The
American Journal of Human Genetics, 78: 437{450. 522

Neal, R. M. (2000). \Markov chain sampling methods for Dirichlet process mixture

models." J. Computational and Graphical Statistics, 9(2): 249{256. 505, 506

Niu, T., Qin, S., Xu, X., and Liu, J. (2002). \Bayesian haplotype inference for multiple
linked single nucleotide polymorphisms." American Journal of Human Genetics, 70:
157{169. 505, 507

Patil, N., Berno, A. J., et al. (2001). \Blocks of Limited Haplotype Diversity Revealed
by High-Resolution Scanning of Human Chromosome 21." Science, 294: 1719{1723.
503

Pritchard, J. K., Stephens, M., Rosenberg, N., and Donnelly, P. (2000). \Association

mapping in structured populations." Am. J. Hum. Genet., 67: 170{181. 503, 506

526

Hidden Markov Dirichlet Process

Rannala, B. and Reeve, J. P. (2001). \High-resolution multipoint linkage-disequilibrium
mapping in the context of a human genome sequence." Am J Hum Genet., 69(1):
159{78. 503

Rasmussen, C. E. (2000). \The In(cid:12)nite Gaussian Mixture Model." In Advances in
Neural Information Processing Systems 12, 554{560. Cambridge, MA: MIT Press.
510

Rosenberg, N. A., Pritchard, J. K., Weber, J. L., Cann, H. M., Kidd, K. K., Zhivotovsky,
L. A., and Feldman, M. W. (2002). \Genetic Structure of Human Populations."
Science, 298: 2381{2385. 503

Sethuraman, J. (1994). \A Constructive De(cid:12)nition of Dirichlet Priors." Statistica Sinica,

1(4): 639{50. 506

Sohn, K.-A. and Xing, E. (2006). \Hidden Markov Dirichlet Process: Modeling Ge-
netic Recombination in Open Ancestral Space." In Advances in Neural Information
Processing Systems 19. Cambridge, MA: MIT Press. 504

Stephens, M., Smith, N., and Donnelly, P. (2001). \A new statistical method for hap-
lotype reconstruction from population data." American Journal of Human Genetics,
68: 978{989. 506, 507

Tavare, S. and Ewens, W. (1998). \The Ewens Sampling Formula." Encyclopedia of

Statistical Sciences, Update Volume 2.: 230{234. 505

Teh, Y., Jordan, M. I., Beal, M., and Blei, D. (2006). \Hierarchical Dirichlet processes."

Journal of the American Statistical Association (to appear). 504, 505, 508, 509

Thorisson, G. A., Smith, A. V., Krishnan, L., and Stein, L. D. (2005). \The International

HapMap Project Web site." Genome Research, 15: 1591{1593. 520, 522

Xing, E., Sharan, R., and Jordan, M. (2004). \Bayesian Haplotype Inference via the
Dirichlet Process." In Proceedings of the 21st International Conference on Machine
Learning, 879{886. New York: ACM Press. 504, 505, 506, 507, 511, 512, 514

Zhang, K., Deng, M., Chen, T., Waterman, M., and Sun, F. (2002). \A dynamic
programming algorithm for haplotype block partitioning." Proc. Natl. Acad. Sci.
USA, 99(11): 7335{39. 503

Eric P. Xing and Kyung-Ah Sohn

527

Acknowledgments

This material is based upon work supported by the National Science Foundation under Grant
No. 0523757, and and by the Pennsylvania Department of Health’s Health Research Program
under Grant No. 2001NF-Cancer Health Research Grant ME-01-739. E.P.X. is also supported
by a NSF CAREER Award under Grant No. DBI-0546594.

528

Hidden Markov Dirichlet Process

Bayesian Analysis (2007)

2, Number 3, pp. 529{556

Re-considering the variance parameterization in

multiple precision models

Yi He(cid:3), James S. Hodgesy, and Bradley P. Carlinz

Abstract. Recent developments in Bayesian computing allow accurate estimation
of integrals, making advanced Bayesian analysis feasible. However, some problems
remain di(cid:14)cult, such as estimating posterior distributions for variance parameters.
For models with three or more variances, this paper proposes a simplex parame-
terization for the variance structure, which has appealing properties and eases the
related burden of specifying a reference prior. This parameterization can be prof-
itably used in several multiple-precision models, including crossed random-e(cid:11)ect
models, many linear mixed models, smoothed ANOVA, and the conditionally au-
toregressive (CAR) model with two classes of neighbor relations, often useful for
spatial data. The simplex parameterization has at least two attractive features.
First, it typically leads to simple MCMC algorithms with good mixing proper-
ties regardless of the parameterization used to specify the model’s reference prior.
Thus, a Bayesian analysis can take computational advantage of the simplex param-
eterization even if its prior was speci(cid:12)ed using another parameterization. Second,
the simplex parameterization suggests a natural reference prior that is proper,
invariant under multiplication of the data by a constant, and which appears to
reduce the posterior correlation of smoothing parameters with the error precision.
We use simulations to compare the simplex parameterization, with its reference
prior, to other parameterizations with their reference priors, according to bias
and mean-squared error of point estimates and coverage of posterior 95% credible
intervals. The results suggest advantages for the simplex approach, particularly
when the error precision is small. We o(cid:11)er results in the context of two real data
sets from the (cid:12)elds of periodontics and prosthodontics.

1 Introduction

Recent developments in Bayesian computing have made it possible to analyze many
previously intractable models, but some problems remain di(cid:14)cult, such as estimat-
ing posterior distributions for variance parameters. This paper considers the class of
multiple-precision linear models, having linear mean structure, normal errors, and at
least three precision parameters. This class includes the conditionally autoregressive
(CAR) model with two types of neighbor relations (2NRCAR; Besag & Higdon 1999,
Reich et al 2007), crossed random-e(cid:11)ects models (Box & Tiao 1992, Chapter 5), some
dynamic linear models (West & Harrison 1999, Chapter 4), smoothed analysis of vari-
ance (Gelman 2005a, Hodges et al 2007), some spatio-temporal models with 1 or 2 spa-

(cid:3)Sano(cid:12)-Aventis Corp,Bridgewater, NJ, mailto:Yi.He@sanofi-aventis.com
yDivision of Biostatistics, School of Public Health, University of Minnesota, Minneapolis,

MN,mailto:hodges@ccbr.umn.edu

zDivision of Biostatistics, School of Public Health, University of Minnesota, Minneapolis,

MN,http://www.biostat.umn.edu/~brad

c(cid:13) 2007 International Society for Bayesian Analysis

ba0007

530

Variance parameterization in multiple precision models

tial neighbor relations and 1 temporal relation (2NRCAR or 3NRCAR), several linear
mixed models (Zhao et al 2006), e.g., additive mixed models and bivariate smoothing,
and, (cid:12)nally, many problem-speci(cid:12)c models (e.g., Gelman & Huang 2007). To make this
discussion concrete, we use the 2NRCAR model applied to periodontal data, as follows.

In periodontics, attachment loss is used to assess cumulative damage to a patient’s
periodontium and to monitor disease progression (Darby & Walsh 1995). Attachment
loss is measured at six sites on each tooth; Figure 1 shows one patient’s data. Each
measurement site is indicated by a small circle whose shade of grey indicates measured
attachment loss, with darker shade indicating larger (worse) attachment loss. Excluding
the four \wisdom teeth" (third molars), a full mouth of 28 teeth gives 168 measurements.
If the two jaws are treated as isolated from each other, this spatial structure has at least
2 \islands", i.e., disconnected groups of measurement sites.

Attachment loss measurements are spatially correlated, but the correlation may not
simply be a function of distance. Instead of using point-data (geostatistical) methods, it
is practical and intuitive to model attachment loss as measurements on a lattice, which
suggests conditionally autoregressive (CAR) models. However, the 168 measurement
sites have a complex topography, so more than one smoothing parameter may be needed
for adequate (cid:12)delity. We consider CAR models with two classes of neighbor relations.
Pairs of neighboring sites come in four types (Figure 2): direct neighbor (Type a), same-
side neighbors crossing the gap between teeth (Type b), opposite-side neighbors on the
same tooth (Type c), and opposite-side neighbors crossing the gap between teeth (Type
d). These four types of neighbor pairs can be grouped into two classes in various ways
(Reich et al 2007). This paper considers the classes shown in Figure 2, with solid and
dashed lines for class 1 and 2 pairs respectively (Grid A in Reich et al., 2007).

Figure 1 summarizes one patient’s data, to which we (cid:12)t the 2NRCAR model, as
follows. Let y = (y1; (cid:1) (cid:1) (cid:1) ; yn)T denote the attachment loss measurements, where the
subscript indexes measurement sites, and specify this 2NRCAR model:

yj(cid:18); (cid:28)0 (cid:24) N ((cid:18); (cid:28)0In)

(cid:18)j(cid:28)1; (cid:28)2 / c((cid:28)1; (cid:28)2)1=2exp(cid:18)(cid:0)

1
2

(cid:18)0f(cid:28)1Q1 + (cid:28)2Q2g(cid:18)(cid:19) ;

(1)

where (cid:28)0; (cid:28)1; and (cid:28)2 are precisions and Q1 and Q2 specify the spatial neighbor relations
smoothed by (cid:28)1 and (cid:28)2 respectively. Qk, k = 1; 2, is n (cid:2) n with o(cid:11)-diagonal entries
qk;ij = (cid:0)1 if sites i and j are class-k neighbors and 0 otherwise, and diagonal entries
qk;ii the number of site i’s class-k neighbors.

Models are often reparameterized to improve computing or interpretation, e.g., a
density with long, narrow contours can be transformed to have more circular contours.
This paper proposes an alternative parameterization for variance-structure parameters,
the simplex parameterization (Besag & Higdon 1999), and a slice sampler for MCMC
draws in this parameterization. The simplex parameterization and its associated ref-
erence prior are then compared to other parameterizations and their reference priors.
Often, the posterior for variance-structure parameters is sensitive to the prior because
the data give little information about them, e.g., because of the spatial structure

He, Y., Hodges, J. S. and Carlin, B. P.

531

(Reich et al 2007). Thus, reference priors for variance-structure parameters are an active
research area (e.g., Browne & Draper 2006; Gelman 2005b).

Section 2 illustrates some problems that can arise in the posterior distributions
of variance-structure parameters, motivating the simplex parameterization. Section 3
develops the new parameterization and a slice sampler for it. Section 4 uses e(cid:11)ec-
tive sample size to compare the computing performance of MCMC algorithms aris-
ing from the simplex parameterizations and three competing parameterizations: pre-
cisions with gamma priors; standard deviations with (cid:13)at priors (Gelman 2005b); and
log precision ratios (de(cid:12)ned below; Reich et al 2007) with (cid:13)at priors. Our MCMC
routine on the simplex parameterization generally outperforms MCMC routines on
other parameterizations, even for reference priors speci(cid:12)ed on those other parame-
terizations. Section 5 uses simulation studies to explore statistical properties of the
reference priors associated with each parameterization. Section 6 summarizes our (cid:12)nd-
ings. The computer code (in R) used for the simplex parameterization is available at
http://www.biostat.umn.edu/~brad/software.html.

2 Problems with commonly-used parameterizations

For Bayesian analysis of multiple-precision models, several parameterizations have been
proposed for the variance structure, including precisions (cid:28)k, standard deviations (cid:27)k =
(cid:28) (cid:0)1=2
(Gelman 2004), precision ratios rk = (cid:28)k=(cid:28)0, k = 1; 2; : : : , and log precision ratios
k
zk = log rk (Reich et al 2004). These parameterizations are often associated with
speci(cid:12)c reference priors. For the precision parameterization, the standard \vague" prior
is (cid:28)k (cid:24) Gamma((cid:15); (cid:15)) for (cid:15) = 0.01 or 0.001. For the standard deviation parameterization,
Gelman (2005b) proposed (cid:27)k (cid:24) U nif (0; L) for a suitable upper bound L. The precision
ratios, rk, are positive and somewhat like precisions, which suggests rk (cid:24) Gamma((cid:15); (cid:15))
as a \vague" prior. Finally, the log precision ratios take values anywhere in the real
line, which suggests zk (cid:24) U nif ((cid:0)L; L) for a suitable L.

These parameterizations are all subject to problems that we illustrate using the
2NRCAR model (1) and Figure 1’s data. Figure 3 suggests how the problems arise.
Speci(cid:12)cally, for each panel in Figure 3, we re-parameterized model (1) in terms of that
panel’s parameterization, applied the reference prior described above, derived the exact
marginal posterior distribution of the smoothing parameters, and plotted its contours.
For the precision ratios (r1; r2) and log precision ratios (z1; z2), Figure 3’s panels c and
d respectively show contours of the log marginal posterior after integrating all other
parameters out of the posterior. Panels a and b show the log conditional posterior for
the precisions ((cid:28)1; (cid:28)2) and standard deviations ((cid:27)1; (cid:27)2) after integrating (cid:18) out of the
posterior and (cid:12)xing (cid:28)0 = 1 and (cid:27)0 = 1, respectively. (These values of (cid:28)0 and (cid:27)0 are
typical of those estimated in calibration studies.)

The contours for ((cid:28)1; (cid:28)2), ((cid:27)1; (cid:27)2), and (r1; r2) (panels a, b, and c, respectively) are
L-shaped with two long arms and modes pressed tightly against one or both coordinate
axes. While each plot assumes a particular reference prior, the same qualitative prob-
lems are present for other reference priors. The contours of (z1; z2)’s posterior are long

532

Variance parameterization in multiple precision models

and narrow here (panel d) but are distinctly L-shaped for other periodontal datasets
(Reich et al 2007). Bimodal posteriors have been observed in the (r1; r2) and (z1; z2)
parameterizations (Reich et al 2007), and indeed bimodality occurs readily even in the
simplest hierarchical models (Liu & Hodges 2003).

Posterior distributions like these create predictable di(cid:14)culties. First, standard
MCMC approaches tend to give chains with high lagged autocorrelations and small
e(cid:11)ective sample sizes. For example, for the parameterizations in Figure 3 a, b, c, the
autocorrelations at lag 10 are 0.2 to 0.4. Second, the parameters can be poorly identi-
(cid:12)ed, that is, either they are highly correlated a posteriori, or the posterior has a large
(cid:13)at mode indicating poor ability to distinguish between possible parameter values. Re-
ich et al (2007) showed that for a variety of 2NRCAR spatial structures, posteriors for
the precision parameters are either very (cid:13)at or have pronounced ridges, inducing bad
MCMC convergence and mixing (Gelfand et al 1995).

Di(cid:11)erent problems a(cid:11)ect other aspects of Bayesian analysis. The posterior corre-
lation between the error precision and the smoothing precisions is often high because
the error precision in e(cid:11)ect speci(cid:12)es the data’s scale, and the data generally give much
more information about this precision than about higher-level precisions. The variance,
precision and standard deviation parameterizations are scale-dependent, so for example
if the measurement unit is changed from centimeters to millimeters, these parameters
are multiplied by 100, 0.01, and 10 respectively. This a(cid:11)ects interpretation of hyperpa-
rameters and makes it di(cid:14)cult to specify a reference prior. The precision ratio and log
precision ratio parameters rk and zk are scale-invariant, i.e., invariant if the data are
multiplied by a constant, but as mentioned are prone to bimodality and highly auto-
correlated MCMC draws. Sections 4.2 and 4.3 illustrate the latter point in detail. The
simplex parameterization (Besag & Higdon 1999), which we now introduce, appears to
avoid or mitigate these di(cid:14)culties.

3 The simplex parameterization and associated methods

3.1 De(cid:12)nition of the simplex parameterization

For a multiple-precision model with precisions ((cid:28)0; (cid:28)1; (cid:1) (cid:1) (cid:1) ; (cid:28)m), de(cid:12)ne the total relative
precision

(cid:21) =

m

Xk=1

rk =

1
(cid:28)0

m

Xk=1

(cid:28)k;

where rk = (cid:28)k=(cid:28)0. De(cid:12)ne the allocation of total relative precision as (cid:12) = ((cid:12)1; (cid:1) (cid:1) (cid:1) ; (cid:12)m),
where

(cid:12)k =

=

rk
(cid:21)

rk
j=1 rj

=

Pm

(cid:28)k
j=1 (cid:28)j

;

Pm

Pm

k=1 (cid:12)k = 1, and (cid:12) = ((cid:12)1; (cid:1) (cid:1) (cid:1) ; (cid:12)m) takes values in the m-dimensional simplex. The 2,
3, and 4-dimensional simplices are a line segment, equilateral triangle, and tetrahedron,
respectively.

He, Y., Hodges, J. S. and Carlin, B. P.

533

This parameterization has two a priori attractive features. First, it is scale-invariant,
that is, it does not change when the data are multiplied by a constant. Also, the simplex
parameter (cid:12) lies in a bounded space, so a natural reference prior, the (cid:13)at prior, is proper
and exchangeable. The rest of this paper uses a (cid:13)at prior on (cid:12) and gamma priors on (cid:21)
and (cid:28)0.

3.2 Computing strategy for the simplex parameterization

For a multiple-precision model like (1), the vector of unknown parameters is ((cid:18); (cid:28)0; (cid:21); (cid:12)),
where (cid:18) is the mean-structure parameters, (cid:28)0 the error precision, (cid:21) the total relative
precision, and (cid:12) the allocation of total relative precision. To avoid MCMC sampling
variation, we analytically integrate (cid:18) and (cid:28)0 out of the joint posterior and run a slice
sampler on the marginal posterior of ((cid:21); (cid:12)). Posterior summaries for (cid:18) and (cid:28)0 are then
obtained by Rao-Blackwellizing.

Suppose the precision parameters in the 2NRCAR model (1) have prior p((cid:28)0; (cid:28)1; (cid:28)2).

Then the joint posterior of all the unknowns is

p((cid:18); (cid:28)0; (cid:28)1; (cid:28)2jy) / p((cid:28)0; (cid:28)1; (cid:28)2)p(yj(cid:18); (cid:28)0)p((cid:18)j(cid:28)1; (cid:28)2)

/ p((cid:28)0; (cid:28)1; (cid:28)2)(cid:28) n=2

0

(cid:28)0

exp(cid:16)(cid:0)

2 X(yi (cid:0) (cid:18)i)2(cid:17)

n(cid:0)G

(cid:2)

Yj=1

((cid:28)1d1j + (cid:28)2d2j )1=2 exp(cid:18)(cid:0)

1
2

(cid:18)0((cid:28)1Q1 + (cid:28)2Q2)(cid:18)(cid:19) ;

(2)

where G is the number of islands in the spatial map and dkj is de(cid:12)ned as follows.
Simultaneously diagonalize the two positive semi-de(cid:12)nite matrices Qk as B0DkB, where
B is nonsingular (Newcomb 1961), and let Dk have jth diagonal element dkj . It is easy
to see (cid:18)jy; (cid:28)0; r1; r2 (cid:24) N ((Qr + In)(cid:0)1X 0y; (cid:28)0(Qr + In)), where Qr = r1Q1 + r2Q2 and
rk = (cid:28)k=(cid:28)0. After integrating out (cid:18),

p((cid:28)0; r1; r2jy) / p((cid:28)0; r1; r2)(cid:28)

n(cid:0)G

2

0

jQr + Inj(cid:0) 1

2

(r1d1j + r2d2j )1=2

n(cid:0)G

Yj=1

Then if (cid:28)0’s prior is Gamma(a0; b0), with mean a0
b0

, integrate out (cid:28)0 to give

(cid:2) exp(cid:16)(cid:0)

(cid:28)0
2

[y0y (cid:0) y0(Qr + In)(cid:0)1y](cid:17) :

p(r1; r2jy) / p(r1; r2)

n(cid:0)G

Yj=1

(r1d1j + r2d2j )1=2jQr + Inj(cid:0) 1

2 R(cid:0)b;

where R = b0 + 1
simplex parameterization (cid:21) = r1 + r2 and (cid:12) = r1=(cid:21), giving

2(cid:2)y0y (cid:0) y0(Qr + In)(cid:0)1y(cid:3), and b = a0 + n(cid:0)G

2 . Now change to the

p((cid:21); (cid:12)jy) / p((cid:21); (cid:12))(cid:21)

n(cid:0)G

2

jI + (cid:21)Q(cid:12)j(cid:0) 1

2

n(cid:0)G

((cid:12)(d1j (cid:0) d2j ) + d2j )

1

2 R(cid:0)b;

Yj=1

534

Variance parameterization in multiple precision models

where R = b0 + 1
2 (y0y (cid:0) y0(I + (cid:21)Q(cid:12))(cid:0)1y), Q(cid:12) = (cid:12)Q1 + (1 (cid:0) (cid:12))Q2, and the Jacobian is
implicit in the change of variables in the prior, from p(r1; r2) to p((cid:21); (cid:12)). B is orthogonal
if and only if Q1Q2 is symmetric, in which case

(cid:0)b

;

(3)

(cid:21)(cid:13)j

1 + (cid:21)(cid:13)j

y(cid:3)2

j 1
3
A
5

p((cid:21); (cid:12)jy) / p((cid:21); (cid:12))

n(cid:0)G

Yj=1 (cid:18) (cid:21)(cid:13)j

1 + (cid:21)(cid:13)j(cid:19)

1

2 2
4b0 +

1

20
@Xj

where y(cid:3) = By and (cid:13)j = (cid:12)(d1j (cid:0) d2j ) + d2j , so (3) depends on (cid:21) and (cid:13)j only through

(cid:21)(cid:13)j

1+(cid:21)(cid:13)j

.

For this problem, we propose a slice sampler with one auxiliary variable. A slice
sampler can be more e(cid:14)cient than an ordinary Metropolis-Hastings algorithm, e.g., Neal
(1997, 2003), Tierney & Mira (1999). Generally, the slice sampler can be described as
follows (Damien et al 1999). Suppose an MCMC has stationary distribution (cid:25)((cid:21); (cid:12)) /
p((cid:21); (cid:12))l((cid:21); (cid:12)). Introduce an auxiliary random variable U with a conditional uniform
distribution U j(cid:12); (cid:21) (cid:24) Unif(0; l((cid:21); (cid:12))). Then ((cid:21); (cid:12); U ) has joint distribution

f ((cid:21); (cid:12); u) / p((cid:21); (cid:12))Ifu<l((cid:21);(cid:12))g((cid:21); (cid:12); u):
The slice sampler is then a special case of the Gibbs sampler:

1. Initialize (cid:12)(0); (cid:21)(0);

2. Generate U j((cid:21); (cid:12)) from a uniform distribution:

U tj((cid:21)t(cid:0)1; (cid:12)t(cid:0)1) / Unif(0; l((cid:21)t(cid:0)1; (cid:12)t(cid:0)1)).

3. Generate (cid:12)j(u; (cid:21)) from p((cid:21); (cid:12)) restricted to l((cid:21); (cid:12)) > u:

(cid:12)tj((cid:21)t(cid:0)1; U t) / p((cid:21); (cid:12))I(l((cid:21)t(cid:0)1; (cid:12)) > U t).

4. Generate (cid:21)j(u; (cid:12)) from p((cid:21); (cid:12)) restricted to l((cid:21); (cid:12)) > u:

(cid:21)tj((cid:12)t; U t) / p((cid:21); (cid:12))I(l((cid:21); (cid:12)t) > U t).

Repeat steps 2-4; after convergence, ((cid:12) t; (cid:21)t) are samples from the stationary distribution
(cid:25)((cid:21); (cid:12)).

choice, candidate (cid:12)j can be generated as Xj=Pm

A natural p((cid:21); (cid:12)) is p((cid:21); (cid:12)) = p1((cid:21))p2((cid:12)), where p1 is a gamma density and p2
is uniform on the simplex, a special case of the Dirichlet distribution. With this
j=1 Xj , where X1; (cid:1) (cid:1) (cid:1) ; Xm are indepen-
dent exponential variates. An informative prior for (cid:12) can be Dirichlet((cid:11)1; (cid:1) (cid:1) (cid:1) ; (cid:11)m),
from which samples can also be generated using draws from gamma distributions. For
2 R(cid:0)b and
the 2NRCAR model, l((cid:21); (cid:12)) = (cid:21)
p((cid:21); (cid:12)) = 1

j=1 ((cid:12)d1j + (1 (cid:0) (cid:12))d2j )

2 jI + (cid:21)Q(cid:12)j(cid:0) 1

(cid:0)(a(cid:21)) (cid:21)a(cid:21)(cid:0)1e(cid:0)b(cid:21)(cid:21)I((cid:12) 2 [0; 1]).

1

n(cid:0)G

2 Qn(cid:0)G

The posterior distributions of (cid:18) and (cid:28)0 can be estimated by Rao-Blackwellizing
(Casella & Robert 1996). For posterior samples ((cid:21)t; (cid:12)t); t = 1; 2; (cid:1) (cid:1) (cid:1) ; M , (cid:18)’s posterior
density can be estimated as

p((cid:18)jy) =Z p((cid:18)j(cid:21); (cid:12); y)p((cid:21); (cid:12)jy)d(cid:21)d(cid:12) (cid:25)

1
M

M

Xt=1

p((cid:18)j(cid:21)t; (cid:12)t; y);

(4)

He, Y., Hodges, J. S. and Carlin, B. P.

535

where p((cid:18)j(cid:21)t; (cid:12)t; y) is (cid:18)’s conditional posterior given ((cid:21)t; (cid:12)t). For the normal-error
model (1), (cid:18)j(cid:21); (cid:12); y is multivariate-t with center (P t)(cid:0)1y, scale (P t)(cid:0)1Rt=b and 2b
degrees of freedom, where Rt = b0 + 1
(cid:12)t)D2)B + In. Thus (cid:18)’s posterior mean and variance are estimated by

2(cid:2)y0y (cid:0) y0(P t)(cid:0)1y(cid:3), and P t = (cid:21)tB0((cid:12)tD1 + (1 (cid:0)

E((cid:18)jy) = E(E((cid:18)j(cid:21); (cid:12); y)) (cid:25)

1
M

E((cid:18)j(cid:21)t; (cid:12)t; y) =

M

Xt=1

V ar((cid:18)jy) = E(V ar((cid:18)jy; (cid:21); (cid:12))) + V ar(E((cid:18)jy; (cid:21); (cid:12)))

(cid:25)

1

M " M
Xt=1

(cid:6)t

(cid:18) +

M

Xt=1

((cid:22)t

(cid:18) (cid:0) (cid:22)(cid:22)(cid:18))((cid:22)t

(cid:18) (cid:0) (cid:22)(cid:22)(cid:18))0# ;

1
M

M

Xt=1

(cid:22)t
(cid:18) = (cid:22)(cid:22)(cid:18)

(5)

and (cid:6)t
(cid:18)

are the posterior mean and variance of p((cid:18)j(cid:21)t; (cid:12)t; y), respectively.
where (cid:22)t
(cid:18)
Similarly, (cid:28)0j(cid:21)t; (cid:12)t; y is gamma distributed with shape b and rate Rt, so posterior sum-
maries for (cid:28)0 can be obtained analogously.

4 MCMC algorithm performance in the di(cid:11)erent param-

eterizations

4.1 E(cid:11)ective sample size (ESS)

E(cid:11)ective sample size (ESS) is commonly used to assess MCMC mixing (e.g., Carlin &
Louis 2000, Chapter 5; Sargent et al 2000; Chen et al 2000; Ridgeway & Madigan 2003).
The ESS of a sampled quantity is de(cid:12)ned (Kass et al 1998) as

ESS =

;

(6)

M

l=1 (cid:26)l

1 + 2P1

where M is the number of MCMC samples for that quantity and (cid:26)l is the estimated
lag l autocorrelation of the samples. ESS can be interpreted as the size of an indepen-
dent, identically distributed sample giving information equivalent to the autocorrelated
MCMC sample. In practice (cid:26)l is estimated with error, and past a certain l the ^(cid:26)l are
dominated by noise (Gilks et al 1996; Chapter 3). To avoid summing noise, Geyer (1992)
proposed the initial convex sequence estimator, which requires a sequence of empirical
(cid:0)m estimates that are positive, monotone, and convex, where (cid:0)m is the sum of two
lagged autocovariances (cid:13)2m and (cid:13)2m+1. The natural estimator of the lagged autoco-
t=1 (Xt (cid:0) (cid:22)X)(Xt+l (cid:0) (cid:22)X), where fXtg
variance is the empirical autocovariance ^(cid:13)l = 1
is the sequence of MCMC samples. Priestley (1981, p. 323) suggests using this \biased"
estimate with divisor M rather than the \unbiased" estimate with divisor M (cid:0) l. De-
(cid:12)ne m(cid:3) as the largest integer such that ^(cid:0)m is a positive, monotonely decreasing, and
convex sequence in m. Then the ESS in (6) sums only estimated autocorrelations ^(cid:26)l for
l (cid:20) 2m(cid:3).

M PM (cid:0)l

536

Variance parameterization in multiple precision models

4.2 Periodontal data analyzed using 2NRCAR

This section compares MCMC algorithms speci(cid:12)ed in each of four parameterizations,
for the 2NRCAR model applied to Figure 1’s data. For each parameterization, the data
were analyzed three times, using three di(cid:11)erent prior distributions, each a reference prior
for one of the parameterizations. This is an unusual simulation study design; the point is
that one may prefer inferences using a reference prior speci(cid:12)ed on one parameterization,
while it is advantageous to specify the MCMC algorithm on a di(cid:11)erent parameterization.

The four parameterizations are simplex, log precision ratios (z1; z2), precisions

((cid:28)0; (cid:28)1; (cid:28)2), and standard deviations ((cid:27)0; (cid:27)1; (cid:27)2). The three reference priors are as follows:
for the simplex parameterization, we put a Gamma(0:01; 0:01) prior on (cid:21), and on (cid:12), a
uniform distribution on the unit interval; for the parameterization with three precisions,
we gave each precision a Gamma(0:01; 0:01) prior; and for the parameterization with
three standard deviations, we gave each standard deviation a uniform prior on the
interval (0; 10). For each parameterization, for each prior, 10000 MCMC draws were
made with 5000 discarded for burn-in. Table 1 describes the MCMC algorithm for
each parameterization. Except for the simplex parameterization, the algorithms were
Metropolis-Hastings with normal candidate draws for the working parameters (Table
1), centered on the current draw. For each working parameter, the sample standard
deviation of the 5000 burn-in draws was used as the standard deviation of the candidate
draws in the subsequent 5000 retained iterations. A dynamic search procedure (see the
Appendix) was used to accelerate the slice sampler.

Table 2 shows e(cid:11)ective sample size (ESS) for the four parameterizations and three
priors. The simplex parameterization has the largest ESS for two priors, and roughly
the same ESS as (z1; z2) for the (cid:13)at prior on ((cid:27)0; (cid:27)1; (cid:27)2). The simplex parameterization’s
sample autocorrelations decrease quickly as lag increases and generally vanish by lag 10,
while the alternatives have much larger autocorrelations at all lags (data not shown).
As currently programmed, the simplex parameterization’s slice sampler usually runs
more slowly than the other algorithms, so it has a smaller advantage in ESS per second
of run time (Table 3), and is roughly tied with the log precision ratio parameterization
(z1; z2).

Section 2 suggested that the simplex parameters ((cid:21); (cid:12)) might have smaller posterior
correlations with the error precision (cid:28)0, compared to other parameterizations’ smoothing
parameters. This was true for the present dataset, with the prior distribution having
little e(cid:11)ect. For each parameterization, we report the posterior correlation only for the
parameter having the largest absolute correlation. In the simplex parameterization, (cid:12)
had the largest absolute posterior correlation with (cid:28)0, about 0.33 for all three priors.
The analogous results for the other three parameterizations were: (z1; z2), 0.53 for z1;
precisions, 0.53 for (cid:28)1; and standard deviations, 0.76 for (cid:27)1. Contrary to our expectation,
(z1; z2) | which, like the simplex parameterization, is invariant when the data are
multiplied by a constant | gave the same maximum absolute posterior correlations as
did the precision parameterization.

Figure 4 shows a contour plot of the log marginal posterior arising from the simplex

He, Y., Hodges, J. S. and Carlin, B. P.

537

parameterization and its reference prior. While this is not especially like a bivariate
normal density, it does seem rather less irregular than the analogous contour plots for
the other parameterizations (Figure 3).

4.3 Smoothed ANOVA (SANOVA) model

The smoothed ANOVA model used here was introduced by Sargent & Hodges (1997)
and fully developed in Hodges et al (2007; see also Smith 1973, Gelman 2005a). Suppose
the experimental design has one error term, c design cells, and n replications per cell.
Parameterize each e(cid:11)ect so the design matrix has orthogonal columns. Group the L
columns for main e(cid:11)ects, including the intercept, into a matrix A1, and the N columns
for interactions into a matrix A2, and scale A1 and A2 so A0
2A2 =
cnIN ; A0

1A2 = 0. The SANOVA model is

1A1 = cnIL and A0

y = A1(cid:2)1 + A2(cid:2)2 + (cid:15);

(7)

where y is the cn-vector of observed outcomes, (cid:15) (cid:24) N (0; (cid:0)1), the grand mean and main
e(cid:11)ects in (cid:2)1 have an improper (cid:13)at prior, the interactions in (cid:2)2 have a N (0; (cid:0)2) prior,
(cid:15) and [(cid:2)1j(cid:2)2] are independent a priori, and the two covariance matrices (cid:0)1 and (cid:0)2 are
speci(cid:12)ed as (cid:0)1 = 1
2 = diag((cid:30)1; (cid:1) (cid:1) (cid:1) ; (cid:30)N ). For a set of distinct smoothing
(cid:28)0
precisions ((cid:28)1; (cid:1) (cid:1) (cid:1) ; (cid:28)s), s (cid:20) N , de(cid:12)ne a deterministic assignment function j(k) that
speci(cid:12)es groups of (cid:30)k within which (cid:30)k = (cid:28)j(k), and let nj be the number of (cid:30)k mapping
to (cid:28)j . The joint posterior after integrating out (cid:2) is

Icn and (cid:0)(cid:0)1

f ((cid:28)0; rjY ) / (cid:25)((cid:28)0; r)(cid:28)

cn(cid:0)L

2

0

exp((cid:0)

1
2

(cid:28)0W (r))

s

Yj=1(cid:18) rj

rj + cn(cid:19)nj =2

;

(8)

where rj = (cid:28)j

(cid:28)0

and W (r) = y0y (cid:0) 1

cn y0A1A0

1y (cid:0) y0A2diag((cn + rj(k))(cid:0)1)A0

2y.

This model has s smoothing precisions (cid:28)1; (cid:1) (cid:1) (cid:1) ; (cid:28)s, so the simplex parameter (cid:12) is s-
, then (cid:28)0’s full conditional

dimensional. If (cid:28)0 has a gamma prior G(a0; b0), with mean a0
b0
posterior is also gamma. After integrating out (cid:28)0, ((cid:21); (cid:12)) has marginal posterior

f ((cid:21); (cid:12)jY ) / (cid:25)((cid:21); (cid:12))

s

Yj=1(cid:20)1 +

cn

(cid:21)(cid:12)j(cid:21)(cid:0)nj =2

R(cid:0)b;

(9)

1y(cid:0) 1

2 y0y(cid:0) 1

2cn y0A1A0

where R = b0 + 1

2 y0A2diag((cn+(cid:21)(cid:12)j(k))(cid:0)1)A0

2y and b = a0 + cn(cid:0)L
Hodges & Sargent (2001, Section 6) applied smoothed ANOVA to a 23 factorial
experiment testing a material’s tensile strength (Lai & Hodges 1999). The three design
factors were the type of mold, presence of pigment, and type of cure, with n = 6
replications per cell. The dataset is in Hodges & Sargent (2001). We used this dataset
to compare MCMC routines for di(cid:11)erent parameterizations and priors, as in Section
4.2’s comparison for the 2NRCAR model, and using the same priors as in Section 4.2.
For all three priors, the MCMC on the simplex parameterization has by far the largest
ESS (Table 4) and the smallest autocorrelations (data not shown). The MCMC on the
simplex parameterization also has the largest ESS/sec for two of the three priors (Table
5). Overall, the smoothed ANOVA results are consistent with the 2NRCAR results.

2

.

538

Variance parameterization in multiple precision models

5 Statistical performance of each parameterization’s ref-

erence prior

5.1

2NRCAR model

To reduce computing time, we simulated periodontal measurements on upper and lower
jaws with 5 teeth each, for 60 total measurements in one \patient". The two neighbor
classes are as in Figure 2. This simulation experiment’s design considered three factors:
(1) true error precision (cid:28)0; (2) the true degree of smoothness in the two classes of neigh-
bor pairs, ((cid:28)1; (cid:28)2); and (3) the 4 parameterizations, each with its associated reference
prior (Table 6). Table 7 gives the speci(cid:12)c true values of ((cid:28)0; (cid:28)1; (cid:28)2).

For each design cell, the 1000 simulated datasets were drawn as follows. By the
spectral decomposition, (cid:28)1Q1 + (cid:28)2Q2 = (cid:0)0(cid:3)(cid:0), where (cid:0) is an orthogonal matrix and (cid:3)
is diagonal. Then (cid:18)(cid:3) = (cid:0)(cid:18) has density

p((cid:18)(cid:3)) / exp((cid:0)

1
2

(cid:18)(cid:3)0(cid:3)(cid:18)(cid:3)) = exp((cid:0)

1
2

(cid:18)(cid:3)0
n(cid:0)G(cid:3)n(cid:0)G(cid:18)(cid:3)

n(cid:0)G)

where the subscript n (cid:0) G indicates the (cid:12)rst n (cid:0) G rows and/or columns. Thus, the
(cid:12)rst n (cid:0) G elements of (cid:18)(cid:3) were drawn from independent normal distributions, for G = 2
islands in the \mouth". The last 2 elements of (cid:18)(cid:3) have (cid:13)at priors under p((cid:18)(cid:3)) and
were drawn from a uniform on ((cid:0)10; 10). Then the sample of true (cid:18) were obtained as
(cid:18) = (cid:0)0(cid:18)(cid:3).

For the simplex and log precision ratio (Z) parameterizations, MCMC samples were
drawn from the marginal posterior after integrating out (cid:18) and (cid:28)0, and the posterior
mean and interval coverage were estimated by Rao-Blackwellizing. For the precision
and SD parameterizations, MCMC samples were drawn from the marginal posterior
after integrating out only (cid:18). For the simplex parameterization, we used the slice sampler
(Section 3.2) with starting values (cid:12)k = 1
s , where s is the number of smoothing precisions,
and for the other parameterizations we used adaptive Metropolis algorithms as described
in Section 4. Trace plots were checked for a sample of arti(cid:12)cial datasets and in all cases
indicated sampler convergence.

The parameterization/reference prior combinations (henceforth, \methods") were
compared according to their results on the standard deviation scale, i.e., (cid:27)k = 1=
(cid:28)k,
the same scale as the data, using bias and MSE of posterior means as point estimates,
and coverage of equal-tailed 95% credible intervals. (The Appendix gives equations for
Rao-Blackwellizing the Z and simplex parameters in the standard deviation scale.) To
remove e(cid:11)ects that obscure comparisons, we report bias as a percent of the true value
and we scale MSE according to the true error variance.

p

Figure 5 displays scaled bias, scaled MSE, and 95% interval coverage for the four
methods. All methods have small biases for the error standard deviation (cid:27)0 except the
Z method in case 3, where the posterior mean overestimates (cid:27)0 by about 30%. By
contrast, the Z method consistently underestimates (cid:27)1, while the other methods have
small biases. For (cid:27)2, all methods have larger bias and the SD method performs worst,

He, Y., Hodges, J. S. and Carlin, B. P.

539

overestimating substantially in all cases. For all methods and cases, the MSEs for (cid:27)0
and (cid:27)1 are small. The Z method has the largest MSE for (cid:27)1. For (cid:27)2, all methods’
MSEs vary a lot, but the simplex method consistently gives the smallest MSE and the
SD method the largest. Finally, all methods give coverage close to 95% for (cid:27)0 and (cid:27)1
except for Z, which gives low coverage. For (cid:27)2, the precision and simplex methods
give coverage 95% or higher for all cases, while the Z and SD methods had quite low
coverage for some cases.

5.2 SANOVA model

This simulation experiment used arti(cid:12)cial data from a 23 design with n = 6 replications
per cell, as in Hodges et al’s (2007, section 3) simulation study. The three design factors
were: (1) the true error precision (cid:28)0 (note that increasing n and (cid:28)0 have the same e(cid:11)ect);
(2) the number of truly present interactions (1 or 3); and (3) the four parameterizations
with associated reference priors, described in Table 6. Two further cases were simulated
to examine the e(cid:11)ect of multiplying the data by a constant. Table 7 gives the design
values for the 8 cases considered.

We again generated 1000 simulated datasets for each \case". The design matrix for
the 23 mean structure was orthogonal, so without loss of generality the true grand mean
and main e(cid:11)ects (cid:18)1; (cid:18)2; (cid:18)3; (cid:18)4 were set to zero. If an interaction term was present, its
(cid:18)k was set to 1, otherwise to zero. The interaction terms were a priori exchangeable
and each was smoothed by its own smoothing precision, so as in Hodges et al. (2007,
Section 3), we need only consider how many interactions are truly present, not which
ones.

The four methods were compared according to their performance for three groups
of parameters: the four interaction (cid:18)k; k = 5; (cid:1) (cid:1) (cid:1) ; 8; the error precision (cid:28)0; and the
eight cell means cj; j = 1; (cid:1) (cid:1) (cid:1) ; 8. For each group of parameters, the methods were
compared according to bias and MSE of posterior means as point estimates, and coverage
probability of the 95% equal-tail credible interval, with one exception: cell-mean bias
is a simple linear function of bias of the interaction (cid:18)k and is thus omitted. By design,
all methods give identical bias and MSE for the grand mean and main e(cid:11)ects, so they
are not considered further. We follow Hodges et al (2007) in calling truly present
interactions \target interactions" and truly absent interactions \null interactions". By
the simulation design’s exchangeability, all target interactions have the same true bias,
MSE, and coverage for a given method, as do all null interactions, so we present average
bias and MSE for the targets and for the nulls. For the interactions (cid:18)k and cell means
cj , we scaled bias and MSE as percents of the true error standard deviation 1p
and the
(cid:28)0
true error variance 1
(cid:28)0
(cid:28)0, we report bias and square root of MSE as percents of (cid:28)0.

, respectively. Similarly, for the estimates of the error precision

Figure 6 displays the bias and MSE of posterior mean estimates of the interaction (cid:18)k,
and coverage of their 95% posterior intervals. For the target interactions, the number
of truly present interactions has little e(cid:11)ect on bias or MSE. Compared to the simplex
method, the SD method has smaller bias (Figure 6a).
In general, the SD method

540

Variance parameterization in multiple precision models

performs better than the precision method, which in turn performs better than the
Z method. For the null interactions, all methods are essentially unbiased and the Z
method has the smallest MSE (Figure 6b). As for 95% posterior intervals (Figure 6c,d),
for the target interactions, the simplex and SD methods give coverage much closer to the
nominal 95% than the Z and precision methods, which are too low for cases with small
error precision. For the null interactions, the simplex and SD methods have about 95%
coverage while coverage for the other two methods is too high. Broadly speaking, for
the interaction (cid:18)k, the simplex method gives good performance that improves relative
to the other methods as the error precision decreases.

Figure 7 shows scaled bias and MSE for the error precision (cid:28)0 (panels a,b), and
MSE and coverage probability for the cell means (panels c,d). For (cid:28)0, the SD method
outperforms the others in both bias and MSE (Figure 7a,b). The 95% CI coverage is
close to the nominal 95% for all methods and cases (data not shown). For the cell
means, Figure 7c,d show the scaled MSE (as a percent of 1
) and 95% interval coverage
(cid:28)0
averaged over the 8 cells. The simplex and SD methods perform similarly. When 1
target interaction is present, these methods have higher bias than the other two, but
when 3 target interactions are present, they have smaller bias. Coverage of 95% credible
intervals is close to the nominal 95% for all methods, except for the Z method for small
error precisions when 3 target interactions are present.

5.3 Crossed random e(cid:11)ect model

The crossed random e(cid:11)ect model (10) has error precision (cid:28)0 and two smoothing preci-
sions (cid:28)1 and (cid:28)2 for rows and columns respectively in the two-way layout, as follows:

yijk = (cid:22) + (cid:11)i + (cid:13)j + (cid:15)ijk i = 1; (cid:1) (cid:1) (cid:1) ; I; j = 1; (cid:1) (cid:1) (cid:1) ; J ; k = 1; (cid:1) (cid:1) (cid:1) ; K;

(10)

where (cid:11)i (cid:24) N (0; (cid:28)1), (cid:13)j (cid:24) N (0; (cid:28)2), and (cid:15)ijk (cid:24) N (0; (cid:28)0) for unknown (cid:28)0; (cid:28)1; (cid:28)2. This
simulation experiment’s design had three factors: (1) the true error precision (cid:28)0; (2) the
true (cid:28)1 and (cid:28)2, considering equal and unequal smoothness in rows and columns; and (3)
the four parameterizations with their reference priors, described in Table 6.

Each of the 1000 arti(cid:12)cial datasets per simulation design cell had 5 row levels ((cid:11)i; i =
1; (cid:1) (cid:1) (cid:1) ; 5), 5 column levels ((cid:13)j; j = 1; (cid:1) (cid:1) (cid:1) ; 5), and 5 replicates ((cid:15)ijk; k = 1; (cid:1) (cid:1) (cid:1) ; 5).
Without loss of generality, the grand mean (cid:22) was set to zero. We generated arti(cid:12)cial
datasets as follows: Generate row e(cid:11)ects (cid:11)1; (cid:1) (cid:1) (cid:1) ; (cid:11)5, column e(cid:11)ects (cid:13)1; (cid:1) (cid:1) (cid:1) ; (cid:13)5, then
in each of the 25 cells, add 5 random normal errors to give 125 total observations.
The algorithms and outcome measures in this simulation study are the same as for the
2NRCAR simulation study (Section 5.1).

Figure 8 shows bias and MSE of posterior means as point estimates and 95% credible
interval coverage, for the three standard deviations (cid:27)0; (cid:27)1, and (cid:27)2. For the error standard
deviation (cid:27)0, all methods are essentially unbiased and have small MSE. However, bias
is complex for the two smoothing standard deviations (cid:27)1 and (cid:27)2. The simplex method
has much smaller bias than the SD method for most cases (Figure 8a), but otherwise
it is di(cid:14)cult to generalize. For MSE (Figure 8b), the simplex method is lower than

He, Y., Hodges, J. S. and Carlin, B. P.

541

the alternatives except for cases 3 and 6 for (cid:27)2. For coverage of 95% intervals (Figure
8c), all methods are consistently close to the nominal 95% for (cid:27)0. For (cid:27)1 and (cid:27)2,
the simplex, precision, and SD methods perform similarly and fairly well, while the Z
method performs worse, particularly for (cid:27)2.

6 Discussion

We have developed a parameterization for multiple-precision models, (cid:12)rst mentioned
for 2NRCAR by Besag & Higdon (1999). Based on Sections 4 & 5, the simplex param-
eterization appears to have two advantages. First, it gives simple MCMC algorithms
with good mixing properties for various reference priors. Thus Bayesian analyses may
bene(cid:12)t from this parameterization even for priors speci(cid:12)ed in another parameteriza-
tion. Second, (cid:12) has a proper natural reference prior that is invariant when the data
are multiplied by a constant; (cid:21) has the same invariance property. Section 5 showed
that compared to other proposed reference priors, this prior yields posterior means with
generally good bias and mean squared error, and 95% credible intervals with close to
nominal coverage, for the range of cases considered.
Its worst performance was for
smoothed ANOVA in Section 5.2. If one were designing a software package solely to
do smoothed ANOVA, these results suggest that the simplex parameterization | with
the reference prior used here | might not be the best choice for a prior distribution.
However, if one were seeking an all- purpose o(cid:11)-the-shelf prior, these results are not so
discouraging: while the simplex parameterization was not the best prior for smoothed
ANOVA, it did not lose badly to the other priors, while each of the other priors did
perform poorly for at least one example.

The obvious question is: can we improve the statistical performance of the simplex
parameterization? The (cid:12)rst consideration in this vein is the reference prior. The allo-
cation parameter (cid:12) has a natural reference prior, but the total relative precision (cid:21) does
not. Sections 4 & 5 used the conventional \vague" Gamma(0.01,0.01) prior, which, with
50th and 90th percentiles 4 (cid:2) 20(cid:0)29 and 0.0015 respectively, is in fact quite informative.
Other priors for (cid:21) may improve statistical or computing performance, though we do not
yet have a (cid:12)rm basis for proposing an alternative. One simple alternative would be a
log-normal prior. In preliminary results from a simulation study of smoothed ANOVA,
giving (cid:21) a lognormal prior with a large variance seems to improve coverage of poste-
rior 95% intervals compared to the gamma prior considered here, but otherwise the
operating characteristics are similar.

It seems pertinent that (cid:21) is unitless or, put another way, that (cid:21) has the same scale
for all problems. Thus, for the smoothed ANOVA and crossed random-e(cid:11)ects models,
it should be possible to determine universally-applicable large and small values of (cid:21),
and perhaps use that information to specify, say, a uniform prior for (cid:21). The 2NRCAR
example is more complicated in a manner that is beyond the present paper’s scope, but
it might be possible to extend this general idea.

Some literature on priors for hierarchical models (e.g., Daniels 1999; Gustafson et.
al. 2007) suggests that a prior may be judged by the relative weight it gives to informa-

542

Variance parameterization in multiple precision models

this idea is to consider, in our notation, (cid:28)k=Ps

tion arising from the data (governed by the error precision (cid:28)0) and information arising
from the model (governed by the smoothing parameters (cid:28)k). One way to implement
j=0 (cid:28)j for k = 0; : : : ; s. The simplex
parameterization lends itself readily to this suggestion. The error precision’s fraction of
total precision is easily shown to be 1=(1 + (cid:21)), which is readily computed in the context
of MCMC. As for the smoothing precisions (cid:28)k, k = 1; : : : ; s, their aggregate fraction of
total precision is (cid:21)=(1 + (cid:21)), and (cid:28)k’s fraction of total precision is (cid:12)k(cid:21)=(1 + (cid:21)), also easily
computed using MCMC. A (cid:13)at prior on (cid:12) treats (cid:28)k; k = 1; : : : ; s, exchangeably; priors
on (cid:21) might be compared according to how they weigh (cid:28)0 against individual (cid:28)k or the
ensemble of (cid:28)ks.

The simplex parameterization extends straightforwardly in two ways. First, it ex-
tends immediately if any of the models presented here is extended by adding one or
more random e(cid:11)ects parameterized by variances or precisions. For example, the 2NR-
CAR model (1) can be extended to a spatio-temporal model for multiple dental visits
by adding a third class of neighbor pairs representing two consecutive observations at a
given measurement site. This adds a third smoothing precision, which can be handled in
the obvious manner. A second extension is for models with many smoothing precisions
that naturally fall into, say, two groups. In such a model, a separate simplex parameter
pair ((cid:21); (cid:12)) can be used for each of the groups of smoothing precisions.

Although the simplex parameterization is applicable to a broad class of models (Sec-
tion 1), extension to models with covariance matrices would be desirable. The approach
of Barnard et al (2000), in which the covariance matrix is decomposed into standard
deviations and correlations, is one possible extension, where the simplex parameteri-
zation would be applied to the vector of standard deviations, after standardizing the
regressors to put them all on the same scale.

Appendix

6.1 Rao-Blackwellizing on the standard deviation scale

In Section 5, the four parameterizations with their associated priors were compared ac-
cording to point-estimate and interval-coverage performance on the standard deviation
scale, with Rao-Blackwellizing done as follows. Suppose (cid:28)0j(cid:21); (cid:12); y (cid:24) Gamma(b; R), then
p((cid:28)0jy) (cid:25) 1
and including

t=1 Gamma((cid:28)0jbt; Rt). Changing variables to (cid:27)0 = (cid:28) (cid:0)1=2

M PM

0

He, Y., Hodges, J. S. and Carlin, B. P.

543

the Jacobian, p((cid:27)0jy) (cid:25) 1

t=1 2(cid:27)(cid:0)3

0 Gamma((cid:27)(cid:0)2

0 jbt; Rt), so

E((cid:27)0jy) (cid:25)

0 Gamma((cid:27)(cid:0)2

0 jbt; Rt)d(cid:27)0

M PM

1
M

=

=

1
M

1
M

M

M

Xt=1Z 2(cid:27)(cid:0)2
Xt=1Z (cid:28)
Xt=1

(cid:0) 1
0

E((cid:28)

M

2

(cid:0) 1
0 Gamma((cid:28)0jbt; Rt)d(cid:28)0

2

jbt; Rt) =

1
M

M

Xt=1

(cid:0)(bt (cid:0) 1
2 )
(cid:0)(bt)

(Rt)

1

2

Similarly, noting that (cid:27)1 = r

2

(cid:0) 1
1

(cid:0) 1
0

2

(cid:28)

and (cid:27)2 = r

2

(cid:0) 1
2

(cid:0) 1
0

2

(cid:28)

,

E((cid:27)1j(cid:21); (cid:12); y) = r

2

(cid:0) 1
1 E((cid:28)

(cid:0) 1
0

2

j(cid:21); (cid:12); y) = r

2

(cid:0) 1
1

(cid:0)(b (cid:0) 1
2 )
(cid:0)(b)

1

2

(R)

E((cid:27)1jy) (cid:25)

E((cid:27)2jy) (cid:25)

1
M

1
M

M

M

Xt=1
Xt=1

(rt

1)(cid:0) 1

2

(rt

2)(cid:0) 1

2

(cid:0)(bt (cid:0) 1
2 )
(cid:0)(bt)

(Rt)

1

2

(cid:0)(bt (cid:0) 1
2 )
(cid:0)(bt)

(Rt)

1

2

(11)

6.2 Dynamic search for the slice sampler

In the simplex parameterization’s slice sampler (Section 3.2), to accept one sample,
generally a large number of samples need to be drawn from p((cid:21); (cid:12)). The slice sampler
can be accelerated by improving this acceptance rate. The following dynamic search is
one approach for a low-dimensional parameter space; we show it for a scalar (cid:12).

1. Choose grid points for (cid:21); (cid:12) by a preliminary analysis, say, (cid:21)1 < (cid:1) (cid:1) (cid:1) < (cid:21)(cid:10) and

(cid:12)1 < (cid:1) (cid:1) (cid:1) < (cid:12)(cid:5).

2. Calculate lij = l((cid:21)i; (cid:12)jjy) at these grid points ((cid:21)i; (cid:12)j ).

3. At the tth MCMC cycle, given (cid:21)t and U t, (cid:12) is conditionally uniform on fl((cid:21)t; (cid:12)) >
U tg. Thus, (cid:12) can be generated from a uniform distribution on (a(cid:12); b(cid:12)) (cid:27) fl((cid:21)t; (cid:12)) >
U tg, chosen as follows.

(a) From the pre-selected grid for (cid:21), (cid:12)nd the two (cid:21)i that bracket (cid:21)t. Call them

L(cid:21) and U(cid:21).

(b) Find the bounds of (cid:12), (a(cid:3)

(cid:12); b(cid:3)

(cid:12)) among (L(cid:21); (cid:12)j ) and (U(cid:21); (cid:12)j ) such that

l(L(cid:21); (cid:12)jy) > U t and l(U(cid:21); (cid:12)jy) > U t.
(c) Extend both ends of the interval (a(cid:3)

(cid:12); b(cid:3)

(cid:12)) until l((cid:21)t; a(cid:3)

(cid:12)jy) (cid:20) U t and

l((cid:21)0; b(cid:3)

(cid:12)jy) (cid:20) U t, giving (a(cid:12); b(cid:12)).

544

Variance parameterization in multiple precision models

4. Draw (cid:12) from U nif (a(cid:12); b(cid:12)), until l((cid:21)t; (cid:12)jy) > U t.

The pre-processing steps 1 and 2 are done before the MCMC draws. The interval
(a(cid:12); b(cid:12)) is in general much narrower than the original (0; 1), so the acceptance rate is
improved.

We present this accelerator as part of a proof of principle and do not claim it can be
used generally. Obviously the e(cid:14)ciency of our slice sampler can and should be improved.

References
Barnard, J., McCulloch, R., and Meng, X.-L. (2000). \Modeling Covariance Matrices
in Terms of Standard Deviations and Correlations, with Application to Shrinkage."
Statistica Sinica, 10: 1281{1311.

Besag, J. and Higdon, D. (1999). \Bayesian Analysis of Agricultural Field Experiments
(Disc: P717-746)." Journal of the Royal Statistical Society, Series B: Statistical
Methodology, 61: 691{717.

Box, G. E. P. and Tiao, G. C. (1992). Bayesian Inference in Statistical Analysis, Classic

Edition. John Wiley & Sons.

Browne, W. J. and Draper, D. (2006). \A Comparison of Bayesian and Likelihood-
Based Methods for Fitting Multilevel Models (with discussion)." Bayesian Analysis,
1: 473{550.

Carlin, B. P. and Louis, T. A. (2000). Bayes and Empirical Bayes Methods for Data

Analysis, 2nd edition. Chapman & Hall Ltd.

Casella, G. and Robert, C. P. (1996). \Rao-Blackwellisation of Sampling Schemes."

Biometrika, 83: 81{94.

Chen, L., Qin, Z., and Liu, J. (2000). \Exploring Hybrid Monte Carlo in Bayesian
Computation." In George, E. I. (ed.), Bayesian Methods with Applications to Science,
Policy, and O(cid:14)cial Statistics. Selected Papers from ISBA 2000, 71{80. International
Society for Bayesian Analysis.

Damien, P., Wake(cid:12)eld, J., and Walker, S. (1999). \Gibbs Sampling for Bayesian Non-
conjugate and Hierarchical Models by Using Auxiliary Variables." Journal of the
Royal Statistical Society, Series B: Statistical Methodology, 61: 331{344.

Daniels, M. J. (1999). \A Prior for the Variance in Hierarchical Models." The Canadian

Journal of Statistics / La Revue Canadienne de Statistique, 27: 567{578.

Darby, M. and Walsh, M. (1995). Periodontal and Oral Hygiene Assessment. Dental

Hygiene Theory and Practice. W.B. Saunders.

Gelfand, A. E., Sahu, S. K., and Carlin, B. P. (1995). \E(cid:14)cient Parametrisations for

Normal Linear Mixed Models." Biometrika, 82: 479{488.

He, Y., Hodges, J. S. and Carlin, B. P.

545

Gelman, A. (2004). \Parameterization and Bayesian Modeling." Journal of the Ameri-

can Statistical Association, 99: 537{545.

| (2005a). \Analysis of Variance { Why It Is More Important Than Ever." The Annals

of Statistics, 33: 1{53.

| (2005b). \Prior Distributions for Variance Parameters in Hierarchical Models."

Bayesian Analysis, 1: 1{19.

Gelman, A., Carlin, J. B., Stern, H. S., and Rubin, D. B. (2004). Bayesian Data

Analysis, 2nd edition. Chapman & Hall/CRC.

Gelman, A. and Huang, Z. (2007). \Estimating Incumbency Advantage and Its Vari-
ation, As An Example of a Before/After Study (with discussion)." Journal of the
American Statistical Association, to appear.

Geyer, C. J. (1992). \Practical Markov Chain Monte Carlo (Disc: P483-503)." Statis-

tical Science, 7: 473{483.

Gilks, W. R. e., Richardson, S. e., and Spiegelhalter, D. J. e. (1998). Markov Chain

Monte Carlo in Practice. Chapman & Hall Ltd.

Gustafson, P., Hossain, S., and MacNab, Y. (2007). \Conservative Prior Distributions
for Covariance Parameters in Hierarchical Models." Canadian Journal of Statistics,
to appear.

Hodges, J., Cui, Y., Sargent, D., and Carlin, B. (2007). \Smoothing Balanced Single-

Error-Term Analysis of Variance." Technometrics, 49: 12{25.

Hodges, J. and Sargent, D. (2001). \Counting Degrees of Freedom in Hierarchical and

Other Richly-Parameterised Models." Biometrika, 88: 367{379.

Hodges, J. S., Carlin, B. P., and Fan, Q. (2003). \On the Precision of the Conditionally

Autoregressive Prior in Spatial Models." Biometrics, 59: 317{322.

Kass, R. E., Carlin, B. P., Gelman, A., and Neal, R. M. (1998). \Markov Chain Monte
Carlo in Practice: A Roundtable Discussion." The American Statistician, 52: 93{
100.

Lai, J. and Hodges, J. S. (1999). \E(cid:11)ects of Processing Parameters on Physical Proper-
ties of the Silicon Maxillofacial Prosthetic Materials." Dental Materials, 15: 450{455.

Liu, J. and Hodges, J. S. (2003). \Posterior Bimodality in the Balanced One-way
Random-e(cid:11)ects Model." Journal of the Royal Statistical Society, Series B: Statistical
Methodology, 65: 247{255.

Neal, R. (1997). \Markov Chain Monte Carlo Methods Based On \Slicing" The Density

Function." Technical Report 9722, Department of Statistics, University of Toronto.

Neal, R. M. (2003). \Slice Sampling." The Annals of Statistics, 31: 705{767.

546

Variance parameterization in multiple precision models

Newcomb, R. (1961). \On The Simultaneous Diagonalization of Two Semi-De(cid:12)nite

Matrices." Quarterly of Applied Mathematics, 19: 144{146.

Priestley, M. B. (1981). Spectral Analysis and Time Series. (Vol. 1): Univariate Series.

Academic Press.

Reich, B., Hodges, J., and Carlin, B. (2007). \Spatial Analysis of Periodontal Data
Using Conditional Autoregressive Priors Having Two Types of Neighbor Relations."
Journal of the American Statistical Association, 102: 44{55.

Ridgeway, G. and Madigan, D. (2003). \A Sequential Monte Carlo Method for Bayesian

Analysis of Massive Datasets." Data Mining and Knowledge Discovery, 7: 301{319.

Sargent, D. and Hodges, J. (1997). \Smoothed ANOVA With Application To Sub-
group Analysis." Research Report rr97-002, Division of Biostatistics, University of
Minnesota, ftp://ftp.biostat.umn.edu/pub/1997/rr97-002.ps.Z.

Sargent, D. J., Hodges, J. S., and Carlin, B. P. (2000). \Structured Markov Chain

Monte Carlo." Journal of Computational and Graphical Statistics, 9: 217{234.

Smith, A. F. M. (1973).
Biometrika, 60: 319{329.

\Bayes Estimates in One-way and Two-way Models."

Tierney, L. and Mira, A. (1999). \Some Adaptive Monte Carlo Methods for Bayesian

Inference." Statistics in Medicine, 18: 2507{2515.

West, M. and Harrison, J. (1999). Bayesian Forecasting and Dynamic Models, 2nd

edition. Springer-Verlag Inc.

Zhao, Y., Staudenmayer, J., Coull, B. A., and Wand, M. P. (2006). \General Design

Bayesian Generalized Linear Mixed Models." Statistical Science, 21: 35{51.

He, Y., Hodges, J. S. and Carlin, B. P.

547

Algorithm

Working par.
Initial values
Initial tuning

constants

Simplex

Slice sampler
with 1 uniform

(z1; z2)
adaptive

Metropolis w/

auxiliary variable

Normal candidate

((cid:21); (cid:12))
(4; 0:2)

|

((cid:28)0; (cid:28)1; (cid:28)2)
adaptive

(z1; z2)
(1; 1)
0.5

((cid:27)0; (cid:27)1; (cid:27)2)
adaptive

Algorithm

Metropolis w/

Metropolis w/

Normal candidate
(log (cid:28)0; log (cid:28)1; log (cid:28)2)

Normal candidate

(log (cid:27)0; log (cid:27)1; log (cid:27)2)

(1; 2; 1)

|

(1; 2; 1)

0.2

Working par.
Initial values
Initial tuning

constants

Table 1: Description of algorithms for the 2NRCAR model

Parameterization used in MCMC algorithm

((cid:21); (cid:12))
Prior
(cid:21) (cid:24) Gamma(0:01; 0:01)
(cid:21): 573
(cid:12) (cid:24) uniform on simplex (cid:12): 1009

(r1; r2)

log(r1): 577
log(r2): 531

Gamma(0:01; 0:01) for
(cid:28)0; (cid:28)1 and (cid:28)2

(cid:21): 1037
(cid:12): 1035

log(r1): 640
log(r2): 713

(cid:13)at for SDs
(cid:27)0; (cid:27)1; (cid:27)2

(cid:21): 1389
(cid:12): 860

log(r1): 648
log(r2): 651

((cid:28)1; (cid:28)2; (cid:28)0)
log((cid:28)1): 275
log((cid:28)2): 591
log((cid:28)0): 301
log((cid:28)1): 261
log((cid:28)2): 697
log((cid:28)0): 248
log((cid:28)1): 175
log((cid:28)2): 406
log((cid:28)0): 215

((cid:27)1; (cid:27)2; (cid:27)0)
log((cid:27)1): 379
log((cid:27)2): 672
log((cid:27)0): 359
log((cid:27)1): 261
log((cid:27)2): 253
log((cid:27)0): 418
log((cid:27)1): 265
log((cid:27)2): 156
log((cid:27)0): 234

Table 2: E(cid:11)ective sample size (ESS) comparison of various parameterizations for the CAR model with
two classes of neighbor relations.

548

Variance parameterization in multiple precision models

Parameterization used in MCMC algorithm

((cid:21); (cid:12))
Prior
(cid:21) (cid:24) Gamma(0:01; 0:01)
(cid:21): 0.43
(cid:12) (cid:24) uniform on simplex (cid:12): 0.75

(r1; r2)

log(r1): 0.94
log(r2): 0.87

Gamma(0:01; 0:01) for
(cid:28)0; (cid:28)1 and (cid:28)2

(cid:21): 0.59
(cid:12): 0.59

log(r1): 1.02
log(r2): 1.14

(cid:13)at for SDs
(cid:27)0; (cid:27)1; (cid:27)2

(cid:21): 0.78
(cid:12): 0.48

log(r1): 1.04
log(r2): 1.05

((cid:28)0; (cid:28)1; (cid:28)2)

log((cid:28)1): 0.17
log((cid:28)2): 0.36
log((cid:28)0): 0.18
log((cid:28)1): 0.17
log((cid:28)2): 0.44
log((cid:28)0): 0.16
log((cid:28)1): 0.11
log((cid:28)2): 0.26
log((cid:28)0): 0.14

((cid:27)0; (cid:27)1; (cid:27)2)
log((cid:27)1): 0.24
log((cid:27)2): 0.43
log((cid:27)0): 0.23
log((cid:27)1): 0.16
log((cid:27)2): 0.26
log((cid:27)0): 0.16
log((cid:27)1): 0.29
log((cid:27)2): 0.17
log((cid:27)0): 0.26

Table 3: E(cid:11)ective sample size per second (ESS/sec) comparison of various parameterizations for the
CAR model with two classes of neighbor relations.

Parameterization used in MCMC algorithm

((cid:21); (cid:12))
Prior
(cid:21) (cid:24) Gamma(0:01; 0:01)
(cid:21): 1615
(cid:12) (cid:24) uniform on simplex (cid:12): 3965
4617
4971

Gamma(0:01; 0:01) for
(cid:28)0; (cid:28)1; (cid:28)2; (cid:28)3; (cid:28)4

(cid:13)at for SDs
(cid:27)0; (cid:27)1; (cid:27)2; (cid:27)3; (cid:27)4

(cid:21): 2498
(cid:12): 4110
5000
5000

(cid:21): 4614
(cid:12): 4657
4576
5000

r

(cid:28)

(cid:27)

log(r1): 336
log(r2): 231
log(r3): 294
log(r4): 331

log(r1): 210
log(r2): 365
log(r3): 244
log(r4): 370

log(r1): 638
log(r2): 752
log(r3): 626
log(r4): 655

log((cid:28)0): 280
log((cid:28)1): 244
log((cid:28)2): 227
log((cid:28)3): 219
log((cid:28)4): 230
log((cid:28)0): 436
log((cid:28)1): 336
log((cid:28)2): 198
log((cid:28)3): 321
log((cid:28)4): 149
log((cid:28)0): 484
log((cid:28)1): 506
log((cid:28)2): 503
log((cid:28)3): 629
log((cid:28)4): 500

log((cid:27)0): 313
log((cid:27)1): 194
log((cid:27)2): 169
log((cid:27)3): 343
log((cid:27)4): 329
log((cid:27)0): 287
log((cid:27)1): 172
log((cid:27)2): 240
log((cid:27)3): 204
log((cid:27)4): 137
log((cid:27)0): 453
log((cid:27)1): 591
log((cid:27)2): 470
log((cid:27)3): 516
log((cid:27)4): 199

Table 4: Comparison of e(cid:11)ective sample size (ESS) in SANOVA model

He, Y., Hodges, J. S. and Carlin, B. P.

549

Parameterization used in MCMC algorithm
(cid:27)

r

(cid:28)

log((cid:28)0): 46.0
log((cid:28)1): 40.1
log((cid:28)2): 37.3
log((cid:28)3): 36.0
log((cid:28)4): 37.8
log((cid:28)0): 75.2
log((cid:28)1): 57.9
log((cid:28)2): 34.1
log((cid:28)3): 55.3
log((cid:28)4): 25.7
log((cid:28)0): 61.2
log((cid:28)1): 64.0
log((cid:28)2): 63.6
log((cid:28)3): 79.5
log((cid:28)4): 63.2

log((cid:27)0): 50.7
log((cid:27)1): 31.4
log((cid:27)2): 27.4
log((cid:27)3): 55.6
log((cid:27)4): 53.3
log((cid:27)0): 50.8
log((cid:27)1): 30.4
log((cid:27)2): 42.5
log((cid:27)3): 36.1
log((cid:27)4): 24.2
log((cid:27)0): 57.9
log((cid:27)1): 75.5
log((cid:27)2): 60.0
log((cid:27)3): 65.9
log((cid:27)4): 25.4

((cid:21); (cid:12))

Prior
(cid:21): 152.5
(cid:21) (cid:24) Gamma(0:01; 0:01)
(cid:12) (cid:24) uniform on simplex (cid:12): 374.4
436:0
469:4

log(r1): 60.1
log(r2): 41.3
log(r3): 52.6
log(r4): 59.2

Gamma(0:01; 0:01) for
(cid:28)0; (cid:28)1; (cid:28)2; (cid:28)3; (cid:28)4

(cid:13)at for SDs
(cid:27)0; (cid:27)1; (cid:27)2; (cid:27)3; (cid:27)4

(cid:21): 215.3
(cid:12): 354.3
431:0
431:0

log(r1): 36.6
log(r2): 63.6
log(r3): 42.5
log(r4): 64.5

(cid:21): 36.5
(cid:12): 36.8
36:2
39:5

log(r1): 80.1
log(r2): 94.4
log(r3): 78.5
log(r4): 82.2

Table 5: Comparison of e(cid:11)ective sample size per second (ESS/sec) in SANOVA model

Table 6: Parameterization and associated reference priors

Method
Simplex

Precision
SD

Z

(cid:28)0

Parameter
(cid:12)k = (cid:28)k
;
P (cid:28)j
(cid:21) = P (cid:28)j
(cid:28)0; (cid:28)1; (cid:1) (cid:1) (cid:1) ; (cid:28)s
(cid:27)0 = 1p
(cid:28)0
(cid:27)k = 1p
(cid:28)k
zk = log( (cid:28)k
(cid:28)0

)

,

Prior
(cid:21) (cid:24) Gamma(0:01; 0:01),

(cid:12) (cid:24) U nif on the simplex
(cid:28)k (cid:24) Gamma(0:01; 0:01); k = 0; (cid:1) (cid:1) (cid:1) ; s
(cid:27)k (cid:24) U nif (0; 100); k = 0; (cid:1) (cid:1) (cid:1) ; s
except SANOVA (cid:27)k (cid:24) U nif (0; 10)
zk (cid:24) U nif ((cid:0)15; 15); k = 1; (cid:1) (cid:1) (cid:1) ; s

Integrate out (cid:28)0?

Yes

No
No

Yes

Table 7: Design values in the simulation studies

Case

1
2
3
4
5
6
7
8

2NRCAR
(cid:28)2
(cid:28)0
1
1
1
1
4
1
1
4
1
1
1
1
4
1
1
4
4
1
1
4
4
1
1
4

(cid:28)1
1
1
1
4
1
4
1
1
1
4
1
4

(cid:28)0
1
1
4
1
16
1
1
4
1
16
1

100
1

(cid:18)1
0
0
0
0
0
0
0
0

(cid:18)2
0
0
0
0
0
0
0
0

SANOVA
(cid:18)3
0
0
0
0
0
0
0
0

(cid:18)4
0
0
0
0
0
0
0
0

(cid:18)5
1
1
1
1
1
1
10
10

(cid:18)6
0
0
0
1
1
1
0
0

(cid:18)7
0
0
0
1
1
1
0
0

(cid:18)8
0
0
0
0
0
0
0
0

Crossed RE
(cid:28)2
(cid:28)0
1
1
4
1
1
16
4
1
1
4
1
1
16
1
1
16
16
1
1
16
1
1
25
{

(cid:28)1
1
1
16
1
16
1
1
16
1
16
1
25
{

100
{

550

Variance parameterization in multiple precision models

7

7

6

6

5

5

4

4

2

3

1

1

2

3

Maxilla

AL(mm)

>5
5

4
3
2
0−1

Mandible

4

4

5

5

6

6

7

7

3

2

3

2

1

1

Figure 1: Attachment loss measurements for one patient. The maxilla is the upper jaw, the mandible
is the lower jaw, the gray boxes are teeth, the small number counting from the center of each jaw is
the tooth number. Small circles indicate the six measurement sites per tooth.

a

a

c

a

a

b

d

c

c

d
b

a

a

a

a

b

d

c

c

d
b

a

a

c

a

a

Figure 2: Neighbor types in periodontal measurements. Letters a-d specify neighbor types. Solid and
dotted lines indicate the two classes of neighbors considered in this paper.

He, Y., Hodges, J. S. and Carlin, B. P.

551

2
t

0
0
5
1

0
0
0
1

0
0
5

0

0
0
4

0
0
3

2
r

0
0
2

0
0
1

0

0
2

5
1

2
s

0
1

5

0

0

10

20

30

40

50

0.0

0.5

1.0

1.5

2.0

2.5

3.0

t1

s1

5
1

0
1

5

2
z

0

5
−

0
1
−

5
1
−

0

20

40

60

80

100

−15

−10

−5

r1

5

10

15

0

z1

Figure 3: 2NRCAR model: Logarithm posterior contour plots with contours at 1 log intervals for
four parameterizations with their own reference priors: (cid:28)0 ; (cid:28)1 ; (cid:28)2 (cid:24) Gamma(0:01; 0:01), (cid:27)0 ; (cid:27)1 ; (cid:27)2 (cid:24)
U nif (0; L), r1 ; r2 (cid:24) Gamma(0:01; 0:01), z1 ; z2 (cid:24) U nif ((cid:0)15; 15). The contours for ((cid:28)0 ; (cid:28)1 ; (cid:28)2) and
((cid:27)0 ; (cid:27)1 ; (cid:27)2) are drawn for the slice (cid:28)0 = 1 and (cid:27)0 = 1, respectively.

552

Variance parameterization in multiple precision models

a
d
b
m
a

l

0
5
2

0
0
2

0
5
1

0
0
1

0
5

0

0.0

0.2

0.4

0.6

0.8

1.0

beta

Figure 4: 2NRCAR model: Log posterior contour plot with contours at 1 log intervals, for the simplex
parameterization with its reference prior.

He, Y., Hodges, J. S. and Carlin, B. P.

553

(a)

(b)

%

0
0
2

0
5
1

0
0
1

0
5

0

0
5
−

0
0
.
1

5
9
.
0

0
9
.
0

5
8
.
0

0
8
0

.

5
7
0

.

0
0
0
1

0
0
8

0
0
6

0
0
4

0
0
2

0

e
c
n
a
i
r
a
v
 
r
o
r
r
e
%

 

|

|

s0

s1

s2

|

|

|

|

|

|
|
1 3 5 7 1 3 5 7 1 3 5 7

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

case

(c)

s0

s1

s2

|

|

|

|

|

|
|
1 3 5 7 1 3 5 7 1 3 5 7

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

s0

s1

s2

|

|

|

|

|

|
|
1 3 5 7 1 3 5 7 1 3 5 7

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

case

simplex
precision
SD
Z

s0: error standard deviation
s1: 1st smoothing standard deviation
s2: 2nd smoothing standard deviation

case

Figure 5: 2NRCAR simulation: Standard deviation bias (as a percent of true standard deviation) and
MSE (divided by the true error variance 1
). (a) scaled bias for (cid:27)0, (cid:27)1, and (cid:27)2; (b) scaled MSE for
(cid:28)0
(cid:27)0, (cid:27)1, and (cid:27)2; (c) 95% interval coverage for (cid:27)0, (cid:27)1, and (cid:27)2.

554

Variance parameterization in multiple precision models

(a)

(b)

5

0

%

5
−

0
1
−

5
1
−

0
0

.

1

5
9

.

0

0
9
0

.

5
8
0

.

0
8
0

.

MSE

MSE

bias

bias

1 target

3 targets

|
1

|

1/4

|

1/16

|
1

|

1/4

|

1/16

t0

(c)

1 target

3 targets

|
1

|

1/4

|

1/16

|
1

|

1/4

|

1/16

0

.

2

5

.

1

0

.

1

%

5

.

0

0
0

.

.

5
0
−

0

.

1
−

0
0

.

1

5
9

.

0

0
9
0

.

5
8
0

.

0
8
0

.

MSE

MSE

bias

3 nulls

bias

1 null

|
1

|

1/4

|

1/16

|
1

|

1/4

|

1/16

t0

(d)

simplex
precision
SD
Z

3 nulls

1 null

|
1

|

1/4

|

1/16

|
1

|

1/4

|

1/16

t0

t0

respectively, for
Figure 6: SANOVA simulation: Average bias and MSE as percents of
(cid:18)k for truly present interactions (a), and truly absent interactions (b). Within each (cid:12)gure, the upper
curves are MSE and the lower curves are biases. 95% interval coverage probability for (cid:18)k for truly
present interactions (c), and truly absent interactions (d).

1
p
(cid:28)0

and 1
(cid:28)0

He, Y., Hodges, J. S. and Carlin, B. P.

555

(a)

(b)

5
1

0
1

a

t

e
%

5

0

5
−

2
2

0
2

8
1

6
1

4
1

2
1

0
1

%

1 target

3 targets

|
1

|

1/4

|

1/16

|
1

|

1/4

|

1/16

t0

(c)

1 target

3 targets

|
1

|

1/4

|

1/16

|
1

|

1/4

|

1/16

0
3

5
2

a

t

e
%

0
2

5
1

0
1

8
9
.
0

7
9
.
0

6
9
.
0

5
9
.
0

4
9
.
0

3
9
0

.

2
9
0

.

1 target

3 targets

|
1

|

1/4

|

1/16

|
1

|

1/4

|

1/16

t0

(d)

simplex
precision
SD
Z

1 target

3 targets

|
1

|

1/4

|

1/16

|
1

|

1/4

|

1/16

t0

t0

Figure 7: SANOVA simulation: (a) error precision bias as a percent of (cid:28)0; (b) square root of MSE as
a percent of (cid:28)0; (c) average cell mean MSE (as a percent of 1
); (d) average cell mean 95% interval
(cid:28)0
coverage probability.

556

Variance parameterization in multiple precision models

(a)

(b)

%

0
8

0
6

0
4

0
2

0

0
2
−

0
0

.

1

5
9

.

0

0
9

.

0

5
8
0

.

e
c
n
a
i
r
a
v
 
r
o
r
r
e
%

 

0
0
2

0
5
1

0
0
1

0
5

0

s0

s1

s2

s0

s1

s2

|

|

|

|

|

|
|
1 3 5 7 1 3 5 7 1 3 5 7

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|

|
|
1 3 5 7 1 3 5 7 1 3 5 7

|

|

|

|

|

|

|

|

|

|

|

|

|

|

case

(c)

case

simplex
precision
SD
Z

s0: error standard deviation
s1: 1st smoothing standard deviation
s2: 2nd smoothing standard deviation

s0

s1

s2

|

|

|

|

|

|

|
|
1 3 5 7 1 3 5 7 1 3 5 7

|

|

|

|

|

|

|

|

|

|

|

|

|

case

Figure 8: Crossed RE simulation: standard deviation bias (as a percent of true standard deviation)
and MSE (divided by the true error variance 1
). (a) scaled bias for (cid:27)0, (cid:27)1, and (cid:27)2; (b) scaled MSE
(cid:28)0
for (cid:27)0, (cid:27)1, and (cid:27)2; (c) 95% interval coverage for standard deviations (cid:27)0, (cid:27)1, and (cid:27)2.

Bayesian Analysis (2007)

2, Number 3, pp. 557{590

Cluster Allocation Design Networks

Ana Maria Madrigal(cid:3)

Abstract. When planning and designing a policy intervention and evaluation,
it is important to di(cid:11)erentiate between (future) policy interventions we want to
evaluate, FT , a(cid:11)ecting \the world", and experimental allocations, AT , a(cid:11)ecting
\our picture of the world". The policy maker usually has to de(cid:12)ne a strategy that
involves policy assignment and recording mechanisms that will a(cid:11)ect the (condi-
tional independence) structure of the data available. Causal inference is sensitive
to the speci(cid:12)cation of these mechanisms. In(cid:13)uence diagrams have been used for
causal reasoning within a Bayesian decision-theoretic framework that introduces
interventions as decision nodes (Dawid 2002). Design Networks expand this frame-
work by including experimental design decision nodes (Madrigal and Smith 2004).
They provide semantics to discuss how a design decision strategy (such as a clus-
ter randomised study) might assist the identi(cid:12)cation of intervention causal e(cid:11)ects.
The Design Network framework is extended to Cluster Allocation. It is used to
assess identi(cid:12)ability when the experimental unit’s level is di(cid:11)erent from the analy-
sis unit’s level, and to discuss the evaluation of cluster- and individual-level future
policies. Cases of ‘pure’ cluster (all individuals in a cluster receiving the same
intervention) and ‘non-pure’ cluster (only a subset receiving the policy) are dis-
cussed in terms of causal e(cid:11)ects. The representation and analysis of a simpli(cid:12)ed
version of a Mexican social policy programme to alleviate poverty (Progresa) is
performed as an illustration of the use of Bayesian hierarchical models to make
causal inferences relating to household and community level interventions.

Keywords: Cluster allocation, In(cid:13)uence diagrams, Causal inference, Identi(cid:12)cation
of policy e(cid:11)ects, DAGs

1 Introduction

Di(cid:11)erent data sets provide di(cid:11)erent types of information. Di(cid:11)erent queries might require
di(cid:11)erent information to obtain answers. When using data for learning, it is important
to consider the conditions and circumstances under which the data were collected. The
distributions that can be learnt (or not) might vary among apparently similar data sets.
This is an important consideration to the analyst before learning model parameters.
Consider the case in which we have two data sets that contain records of whether or
not children in a population take food nutrition supplements (F S) and whether or not
they have gained weight. The (cid:12)rst data set comes from a census sample, and the second
comes from an experiment where half the children were given food supplements and
half of them were not. Suppose we are interested in learning the prevalence of children
taking supplements in the population, p(F S). It is clear that learning from the second
data set that p(F S) = 0:5 only re(cid:13)ects an experimental choice and not a population
prevalence, as would be the case if we were to use the (cid:12)rst data set (e.g. showing how

(cid:3)University of Warwick, UK mailto:am.madrigal@warwickgrad.net

c(cid:13) 2007 International Society for Bayesian Analysis

ba0007

558

Cluster Allocation Design Networks

parents ‘naturally’ choose to give F S to children). The collecting strategy de(cid:12)nes the
structure of the data set. A perfect design of experiments will give a more organised
layout of the data. An observational study might be more ‘disorganised’, as data is
allowed to arise naturally. In this paper, we focus our attention on a particular type of
query related to policy intervention evaluations and discuss which data set structures
do or do not let us answer causal queries and extract appropriate causal e(cid:11)ects (e.g. the
causal e(cid:11)ect of F S on children’s weight). Discussions about the identi(cid:12)ability of causal
e(cid:11)ects have been usually phrased as ‘Is the causal e(cid:11)ect of T on Y identi(cid:12)able?’ (see
Pearl 2000; Lauritzen 2001; Dawid 2002). In this paper, the role of data structures is
made explicit by phrasing the identi(cid:12)ability question as ‘Is the causal e(cid:11)ect of T on Y
identi(cid:12)able from the data available?’.

Intervention has to do with ‘perturbing’ the dynamics of a system. If we say that
a system consists of components which in(cid:13)uence each other and that its dynamics
describe the way these components interact with each other in an equilibrium state,
some examples of systems might be consumption-expenditure patterns, road tra(cid:14)c in
a town or the human body. The system at present has some pre-intervention dynamics
attached to it. When we intervene a system, by introducing a promotion-advertisement
campaign, by adding a red light at a corner, or by giving medicine, we are introducing
a new component into a system that will imply new post-intervention dynamics. The
intervention might have both qualitative e(cid:11)ects, modifying the structure of the system
(maybe by ‘blocking’ the interaction between two of its components), and quantitative
e(cid:11)ects, modifying the value of the components. One of the main interests consists in
describing if and how the intervention a(cid:11)ects the system. Evaluation of the intervention
e(cid:11)ects is required and it is usually measured in terms of a response variable, such as
sales-awareness, number of accidents, or health condition.

Discussions of causal reasoning have been made usually assuming that the graph rep-
resenting the system implicitly includes the underlying (experimental) mechanism that
is generating the data (see Pearl 2000). Then, in this (cid:12)xed ‘natural’ or ‘idle’ system,
whether the future policy intervention FT e(cid:11)ect is identi(cid:12)able and can be obtained is
evaluated. Randomised allocation of treatments to units is a well known practice within
medical clinical trials but, because of ethical, social and (cid:12)nancial issues, complete ran-
domisation within an experiment designed to evaluate a social policy will usually be
unfeasible. Knowing the details of the policy assignment mechanism and a well-planned
recording of the data become very relevant issues in order to obtain all the information
needed to measure the right ‘causal’ e(cid:11)ects (see Rubin 1978). In(cid:13)uence Diagrams (IDs)
are used to represent the system dynamics and interventions graphically; a review of
the main features of the framework used is made in Section 2. Our interpretation of
causal e(cid:11)ects for interventions is Bayesian decision-theoretic, where an intervention on
a system is regarded as a decision. Dawid (2002)’s extended in(cid:13)uence diagrams are
augmented by including ‘experimental design’ decisions nodes within the set of inter-
vention strategies to create what we call a Design Network (DN), to provide semantics
to discuss how a ‘design’ strategy (such as clustering) might assist the systematic iden-
ti(cid:12)cation of intervention causal e(cid:11)ects, to give a taxonomy for design decisions, and to
show how these decisions might alter the graphical (conditional independence) struc-

A.M. Madrigal

559

ture used to evaluate the causal e(cid:11)ect of policy FT . It is maintained that experimental
design decisions are intrinsic to any causal analysis of policy intervention strategies.
Design Networks were introduced in Madrigal and Smith (2004) for random allocation,
and their main characteristics are presented in Section 3. Design Networks for cluster
allocation are discussed in Section 4; the propositions can be derived from the discussion
in Appendix A.

This research was motivated by a Mexican Social Policy Programme (Progresa)
whose objective is to alleviate poverty. It consists of a three-stage mechanism to target
its eligible population, based on community and household characteristics. The policy
involves a collection of interventions at di(cid:11)erent levels (community, household and indi-
vidual). All households are recipients of the community-level interventions (e.g. health
infrastructure and services). Actions at household and individual level (e.g. extra mon-
etary support and nutritional supplements) a(cid:11)ect only ‘poor’ (eligible) households and
vary according to household/individual demographics, so not all units in the community
(cluster) are intervened equally. This motivates the discussion about the data structure
arising from a cluster allocation, the distinction of ‘overall’ and ‘total’ e(cid:11)ects, the dif-
ferences in the inference of cluster- and individual-level interventions, and the nested
structures in design and analysis. The design of the study included a randomised clus-
ter allocation for treatment and control communities. To illustrate some features of
causal analysis in a cluster allocation setting, this paper presents in Section 5, a hierar-
chical model analysis based on Spiegelhalter (2001). Formulation is performed for the
evaluation of cluster- and individual-level interventions based on Progresa data.

2 Intervention Graphical Framework and Causal Infer-

ence

In(cid:13)uence diagrams (IDs) have been used for over 20 years to form the framework for both
describing (see Howard and Matheson 1981; Oliver and Smith 1990) and also devising
e(cid:14)cient algorithms to calculate the e(cid:11)ects of decisions (see Jensen 2001) in complex
systems which implicitly embody strong conditional independence assertions. However,
it is only recently that they have been used to explain causal relationships (Dawid 2000,
2002), and been shown to be much more versatile than Causal Bayesian Networks (Pearl
1993, 1995).

The simplest form of external intervention is when a single variable X is forced to
take on some (cid:12)xed value x0. This is known as an ‘atomic intervention’ and, following
Pearl (2000), it is denoted by do(X = x0). The atomic intervention replaces the original
mechanism: p(x j pa(x)) by p(x j pa(x); do(X = x0)) = 1 if X = x0 where pa(x)
denotes the parent nodes of X. This conditioning by intervention formula has appeared
in various forms (see Pearl 1993; Spirtes et al. 2000; Robins 1986). It cannot be asserted
in general that the e(cid:11)ect of setting the value of X to x0 is the same as the e(cid:11)ect of
observing X = x0. Only in limited circumstances (as when the node for X has no
parents in the graph) will conditioning by intervention and conditioning by observation
coincide. Graphically, interventions are represented by deleting the arrows that enter

560

Cluster Allocation Design Networks

the intervened node in the original graph, making explicit the fact that when the value
is set externally, the parents’ values are not relevant post-intervention. Pearl’s do((cid:1))
corresponds to an external intervention. By recognising interventions as decisions, the
Bayesian decision-theoretical framework embeds Pearl’s doing operation and provides a
stronger framework for causal inference. The strong links between decision theory and
Pearl’s causal model have been discussed by Heckerman and Shachter (2003). Those
who are familiar with Bayesian decision theory will (cid:12)nd comfort, as I have, in these
connections.

Dawid (2002) points out that, traditionally, in IDs conditional distributions are
given for random nodes, but no description is supplied of the functions or distributions
involved at the decision nodes, which are left arbitrarily at the choice of the decision
maker. If we choose to provide some descriptions of the decision rules, then any given
speci(cid:12)cation of the functions or distributions at decision nodes constitutes a decision
strategy, (cid:25). Decisions determine what we may term the partial distribution, p; of
random nodes given decision nodes which is not in general the same as the associated
conditional distributions (see Cowell et al. 1999, section 2.3). If E and D denote the set
of random events and the set of decisions, respectively, then the full joint speci(cid:12)cation
p(cid:25), consisting of decision strategy (cid:25) and partial distribution p for all e 2 E and d 2 D is
given by p(cid:25) (e; d) : The graphical representation of p(cid:25) can be made by using extended IDs
that incorporate non-random parameter nodes ((cid:18)e = p(e j pa0 (e)) and strategy nodes

(cid:0)(cid:25)d = (cid:25)(d j pa0 (d))(cid:1) representing the mechanisms that generate random and decision

nodes respectively. Here, pa0 (:) denotes the set of domain parents of X (i.e. parents
in the original non-extended version of the ID). In what he calls augmented DAGs,
Dawid incorporates intervention nodes F where FX = x corresponds to ‘setting’ the
value of node X to x (in Pearl’s language: FX = do(X = x)), and he introduces a
new value ; such that when FX = ;, X is left to have its ‘natural’ distribution, termed
by Pearl the ‘idle’ system. Figure 1 shows, for a simple case, the usual representation
of IDs as well as its extended and augmented versions, for the set (T; B; Y ) where
T = (T1; T2; ::; Ts) represents a set of policy variables (treatments), B = (B1; B2; ::; Br)
is a set of background variables (potential confounders) and Y is a response variable.

(cid:1)

(cid:2)

(cid:3)

p 

(cid:2)

(cid:1)

q

(cid:3)

q

(cid:1)

(cid:4) 

(cid:2)

(cid:3)

Figure 1: Extended in(cid:13)uence diagrams and augmented DAGs

Causal reasoning is related to prediction in the face of intervention. It relates to the
idea that a variable is a ‘cause’ if setting this variable to a speci(cid:12)c value (by intervention)
changes the distribution of the response. Causal enquiries about the ‘e(cid:11)ect of T on Y ’ are
seen as relating to (comparisons between) the distributions of Y given FT = do(T = t0)
for various settings of t0. The intervention node F of the augmented DAG is used as
an ‘auxiliary’ variable to discuss the identi(cid:12)ability of these e(cid:11)ects under certain DAG

(cid:5)
(cid:6)
(cid:7)
(cid:8)
(cid:9)
(cid:10)
(cid:11)
(cid:12)
(cid:13)
(cid:14)
(cid:12)
(cid:14)
(cid:8)
(cid:15)
(cid:16)
(cid:5)
(cid:17)
(cid:7)
(cid:8)
(cid:15)
(cid:13)
(cid:18)
(cid:19)
(cid:20)
(cid:12)
(cid:13)
(cid:21)
(cid:12)
(cid:8)
(cid:16)
(cid:22)
(cid:17)
(cid:23)
(cid:24)
(cid:17)
(cid:25)
(cid:5)
(cid:21)
(cid:7)
(cid:8)
(cid:26)
(cid:20)
(cid:23)
(cid:25)
(cid:12)
(cid:13)
(cid:11)
(cid:12)
(cid:14)
(cid:8)
(cid:16)
(cid:26)
(cid:27)
A.M. Madrigal

561

structures. In particular, there is interest in establishing if the causal e(cid:11)ect of FT on Y
can be identi(cid:12)ed and estimated correctly from the available data. The structure of the
data available is de(cid:12)ned by the set of conditional independencies that are derived from
the graph.

De(cid:12)nition (Conditional independence) If X; Y; Z are random variables with a joint
distribution P((cid:1)), we say that X is conditionally independent of Y given Z under P , if for
any possible pair of values (y; z) for (Y; Z) such that p(x; y) > 0, P (x j y; z) = P (x j z):
This can be written following Dawid (1979)’s notation as (X??Y j Z)P :

The discussion is conducted in terms of the relevance of learning the strategy that
gave the value t0 to T , namely whether it arose from the original experimental setting
(cid:25)(t j b) (FT = ;) or whether it was set externally (FT = do(T = t0)). Conditional
independencies of the form (Y ??FT j T; (cid:1))dE
are used for this. Di(cid:11)erent examples of
identi(cid:12)able and unidenti(cid:12)able situations are discussed by Pearl (2000), Lauritzen (2001)
and Dawid (2002), each with their particular framework and notation. Imagine the set
(T; B; Y ) is available to us in the data set (cid:1): Figures 2(a) and 2(b) show the cases where
B is said to be irrelevant for Y and where B is said to be white noise of Y (with respect to
T ) respectively. The case where B is an intermediate variable between T and Y (i.e. T
a(cid:11)ects B and B a(cid:11)ects Y ) is shown in Figure 2(c). In these three structures the de(cid:12)nition
of absolute non-confounding given by Y ??FT j T holds (see Dawid 2002, x7). This
asserts that the distribution of Y given T will be the same, whether T arose ‘naturally’
or T is set by intervention. Thus the causal e(cid:11)ect can be estimated directly from the
data available, (cid:1); using p(y j t0; FT = do(T = t0)) = p(y j t0; FT = ;) = p(y j t0): The
de(cid:12)nition of non-confounding (Y ??FT j T ) does not hold for the structure shown in
Figure 2(d). In this latter system, B is said to act as a confounder, as it a(cid:11)ects both
treatment T and response Y . So, in order to obtain the causal e(cid:11)ect, we are required
to know (or observe) the marginal distribution p (B). If this is the case, then the causal
e(cid:11)ect p(y j FT = t0) can be obtained using the ‘back-door formula’ (Pearl 1993) which

‘adjusts’ for B such that p(y j FT = t0) =Pb p(y j t0; b)p (b) :

(cid:1)
(cid:1)

(cid:1)
(cid:1)

(cid:1)
(cid:1)

(cid:1)
(cid:1)

(cid:2)
(cid:2)
(cid:28)
(cid:28)
(cid:4)(cid:5) (cid:6)(cid:7)(cid:8) (cid:9)(cid:9)(cid:10)(cid:11)(cid:10)(cid:12)
(cid:4)(cid:5) (cid:6)(cid:7)(cid:8) (cid:9)(cid:9)(cid:10)(cid:11)(cid:10)(cid:12)

(cid:3)
(cid:3)
(cid:13) (cid:14)(cid:10)
(cid:13) (cid:14)(cid:10)

(cid:28)
(cid:28)
(cid:4)(cid:15) (cid:6)(cid:7)(cid:16)
(cid:4)(cid:15) (cid:6)(cid:7)(cid:16)

(cid:2)
(cid:3)
(cid:2)
(cid:3)
(cid:18) (cid:19)(cid:10)(cid:7)(cid:13) (cid:20)(cid:18) (cid:21)(cid:10)
(cid:18) (cid:19)(cid:10)(cid:7)(cid:13) (cid:20)(cid:18) (cid:21)(cid:10)

(cid:28)
(cid:28)
(cid:4)(cid:14)(cid:6)(cid:7)(cid:8)
(cid:4)(cid:14)(cid:6)(cid:7)(cid:8)

(cid:2)
(cid:2)
(cid:13) (cid:19)(cid:10)(cid:9)(cid:22)(cid:10)(cid:23)
(cid:13) (cid:19)(cid:10)(cid:9)(cid:22)(cid:10)(cid:23)

(cid:5) (cid:19)(cid:10)(cid:7)(cid:12)
(cid:5) (cid:19)(cid:10)(cid:7)(cid:12)

(cid:3)
(cid:3)
(cid:5) (cid:9)(cid:18)
(cid:5) (cid:9)(cid:18)

(cid:28)
(cid:28)
(cid:4)(cid:23) (cid:6)(cid:7)(cid:24) (cid:20)(cid:19)(cid:10)(cid:13) (cid:19)(cid:18)
(cid:4)(cid:23) (cid:6)(cid:7)(cid:24) (cid:20)(cid:19)(cid:10)(cid:13) (cid:19)(cid:18)

(cid:2)
(cid:2)
(cid:5) (cid:11)(cid:7)(cid:25) (cid:20)(cid:13)
(cid:5) (cid:11)(cid:7)(cid:25) (cid:20)(cid:13)

(cid:3)
(cid:3)
(cid:26) (cid:20)(cid:27)
(cid:26) (cid:20)(cid:27)

(cid:15) (cid:11)(cid:10)
(cid:15) (cid:11)(cid:10)

(cid:23) (cid:10)(cid:9)
(cid:23) (cid:10)(cid:9)

(cid:28)
(cid:28)

Figure 2: Possible basic structures

(cid:1)
(cid:1)
(cid:2)
(cid:2)

(cid:1)
(cid:1)
(cid:24) (cid:9)(cid:20)(cid:14)(cid:10)(cid:21)(cid:21)
(cid:24) (cid:9)(cid:20)(cid:14)(cid:10)(cid:21)(cid:21)

(cid:1)
(cid:1)
(cid:3)
(cid:3)

(cid:4)(cid:10)(cid:6)(cid:7)(cid:29)(cid:7)(cid:22)(cid:20)(cid:9)(cid:10)(cid:7)(cid:14)(cid:20)(cid:22)(cid:30)(cid:11)(cid:10)(cid:31)(cid:7)(cid:21) (cid:21)(cid:19)(cid:10)(cid:22)
(cid:4)(cid:10)(cid:6)(cid:7)(cid:29)(cid:7)(cid:22)(cid:20)(cid:9)(cid:10)(cid:7)(cid:14)(cid:20)(cid:22)(cid:30)(cid:11)(cid:10)(cid:31)(cid:7)(cid:21) (cid:21)(cid:19)(cid:10)(cid:22)

Social policies will usually be more complex systems including all irrelevant (BT ) and
white-noise (BY ) background variables, possible confounders (BC ) and an intermediate
process, as shown in Figure 2(e). Most of the examples in social policy interventions FT
involve (a collection of) atomic or contingent interventions. Therefore, the intermediate
process might involve both intermediate variables a(cid:11)ected by T and possible actions G

(cid:5)
(cid:17)
(cid:18)
(cid:5)
(cid:13)
(cid:5)
(cid:17)
(cid:18)
(cid:5)
(cid:13)
562

Cluster Allocation Design Networks

that will be triggered when the policy T is done. The ‘overall’ causal e(cid:11)ect will include
all direct and indirect e(cid:11)ects of do(T = t0) on Y .

3 Introducing experimental nodes

3.1 Policy versus Experimental Decisions

When planning and designing a policy intervention and evaluation, the policy maker
will have to de(cid:12)ne a strategy that involves ‘policy intervention’ actions (DT = fd0g) and
‘experimental design’ actions (DE = fd(cid:3)g). The former includes decisions related to how
the policy is implemented and what (which doses and to whom) will be provided. The
latter is related to the evaluation of the policy and includes experimental design decisions
that de(cid:12)ne the (chosen or controlled) conditions under which the study is carried out
DE g are the components of a
and the data ((cid:1)) recorded.
particular decision strategy (cid:25)D, the interest lies in describing (cid:25)D (D j E). In this sense,
we say that policy intervention actions (DT ) are concerned with intervening ‘the world’,
while experimental design actions (DE) relate to intervening the statistician’s ‘view of
the world’.

If D = fd0

1; ::; d0

DT ; d(cid:3)

1; ::; d(cid:3)

It is important to di(cid:11)erentiate between ‘choosing a policy’ and ‘choosing a design’, as
the goals of these interventions are di(cid:11)erent. The ‘success’ of a policy intervention DT
is measured in terms of its e(cid:14)cacy to provoke ‘better’ values on the response variable
Y through its overall e(cid:11)ects re(cid:13)ected by p(Y j FT = do(T = t0); DE). The e(cid:14)cacy
of an experimental intervention, DE, is measured in terms of its ability to isolate the
policy e(cid:11)ect as much as possible. Making an explicit representation of both types of
interventions will assist decisions of the experimenter and considerations of the analyst,
when the aim is to evaluate the causal e(cid:11)ect of policy FT .

When we, as data-collectors, approach the world, the data we collect depend on our
way of approaching it. The data we observe in the database (available data, (cid:1)dE ) will
re(cid:13)ect the experimental design decisions DE = dE made (or deliberately ‘not made’) at
the time of its collection through p(data j DE). Two extreme cases of designed studies
might be, on the one hand, the ‘perfect’ experiment where all factors are controlled,
balanced and randomised and, on the other hand, the complete observational study
with all the relations that happen in ‘natural’ conditions (approximated by a census
of all population). The available literature discusses broadly the cases for completely
experimental data (see, for example, Chaloner and Verdinelli 1995; Wu and Hamada
2000) or completely observational data (e.g. Rosenbaum 2002). Although in the social
sciences access to perfect experimental data is usually not feasible, the data is not
always completely observational. In some cases, controls are taken at the time of the
design/collection of data, which gives rise to partially experimental data. In this work,
we consider DE to include any experimental conditions that might involve a decision by
the experimenter (data collector). The choice of ‘no control at all’ leads to observational
data ((cid:1);) which is assumed to be a (degenerate) special type of experimental data.

Experimental design interventions, DE = fM; R(B)g, contain the mechanisms M =

A.M. Madrigal

563

fME; MS; MT g through which units are selected and assigned to eligible, sample and
treatment groups, and the recording mechanism R(B) that determines whether the
background variables are observed and available to us in the data (cid:1)dE . In addition,
implementation details which refer to the logistics and how the study will be carried
out are important, as they can introduce some biases. A complete description of these
mechanisms is presented in Madrigal (2004). In this paper we focus on the treatment
assignment mechanisms MT .

0; t0

1; t0

2 and t0

1): ‘Give green FS’; Policy 2 (t0

0): ‘Do not give any FS’; Policy 1 (t0

As an example, imagine a policy will be implemented to increase the nutritional state
(Y ) of ‘poor’ children in a certain geographical area. Suppose there are two di(cid:11)erent
brands of food supplements (FS) in green or red packages. A decision to sign a contract
with the food supplement provider(s) for as long as the policy takes place has to be
made. Imagine the policy maker is faced with four possible policy interventions: Policy
0 (t0
2): ‘Give red FS’;
and Policy 3 (t0
3):‘Give green FS to young babies; and give red FS to older children’.
Once a policy is chosen, all children in the target population will be under the same
policy. In this case, policy intervention strategies (DT ) are de(cid:12)ned for the same target
population (namely, children in poverty), and the future policy interventions (FT ) are
given by t0
3. When evaluating the policy intervention e(cid:11)ect we obtain the
‘overall e(cid:11)ect’ of each of the policies: Although the policy interventions act on children
through the actual FS given, it is important to bear in mind that questions ‘Is policy
i giving better results than policy t0
t0
j ?’ are di(cid:11)erent from the question ‘Is the green
FS working better than the red FS?’.
In this case, they will coincide when we are
comparing policies t0
2; but to draw conclusions about the e(cid:11)ects of green and
red FS from a comparison between, say, t0
3, the
e(cid:11)ect of FS is confounded with age. The policy maker, as an experimenter, has to
choose the experimental design strategy (DE) used to collect data (cid:1)dE . This data is
used to evaluate policy intervention strategies (DT ) and compare the e(cid:11)ects of policies
FT = do(P olicy = t0
Imagine that policy makers in principle have in mind the
implementation of contingent policy t0
3 (against the option of not providing any food
supplement at all t0
0): First, the experimental levels ft(cid:3)g have to be set. These are
allocated through action AT = do(P olicy = t(cid:3)). Choosing some experimental levels
ft(cid:3)g to be equal to future policy levels ft0g, such that t(cid:3)
1 = t0
3, ensures
the positivity condition (see Appendix A), and then ft(cid:3)g = ft(cid:3)
3g. Imagine
the allocation of policies is done randomly with probability of one half. This random
intervention could be expressed as A(cid:18)T = do((cid:18)T = (cid:18)(cid:3)
T = p(AT =
do(P olicy = t(cid:3)
2 for m=1,2. Policy allocation is randomised and it is de(cid:12)ned by
the experimental design strategy, DE.

3 could be dangerous, as in t0

0 and t(cid:3)
2g = ft0

2 = t0
0; t0

1; t(cid:3)

T ) such that it (cid:12)xes (cid:18)(cid:3)

m)) = 1

0 and t0

1 and t0

s).

Dawid (2002)’s framework, although open to di(cid:11)erent strategies for setting the value
of a treatment T = t0, including randomised or atomic de(cid:12)nitions, does not allow us to
represent in the same graph and formulae both the atomic (future) policy intervention
FT 2 DT (allocating treatment T = t0 with probability one) and the (contingent or
randomised) experimental allocation strategy followed when collecting data AT 2 DE
(allocating treatment T = t(cid:3) according to (cid:18)). Neither does it allow us to represent the
impact on the (graphical) data structure of the experimental actions. Therefore, an

564

Cluster Allocation Design Networks

extension is needed.

3.2 Design Networks: Basics

In its simpli(cid:12)ed version, let DE = fA; R(B)g where A contains all the policy assignment
mechanisms and R(B) contains the recording mechanism, such that R(B q) = 1, for
q = 1; 2:::Q, if variable Bq is recorded and R(Bq) = 0 if Bq is either unobservable
or not recorded. Assignment nodes A and recording nodes R can be included in the
DAG as decision nodes to create a design network (DN). The design network shows the
‘natural’ (experimental) mechanisms that generate the data available (cid:1)dE . In general,
no matter whether the data has been collected already or we are planning the design to
generate the data, DE represents decisions to be made at the data collection time.

Consider the set (T; B; Y ). For simplicity, suppose that T and Y are univariate,
that B does not contain intermediate variables between T and Y (i.e. B consists of
pre-intervention variables not a(cid:11)ected by T ), and that the future policy is an atomic
intervention FT = do(T = t0). Figure 3(a) shows the usual in(cid:13)uence diagram represen-
tation of this case, and Figure 3(b) gives the corresponding design network. Note that
A blocks all the paths going from B to the policy node T: This follows from the as-
sumption that A captures all the allocation mechanisms for T that might be in(cid:13)uenced
by the background variables B; so that A is the only parent of the policy node T in the
design network. Recording nodes, R(B), are added for each background variable B q,
introducing the decision to record Bq versus not to record it. A double circle containing
a dashed and solid line is given to each background node B q to show its potential ob-
servability. It is assumed that policy T and response variable Y will be recorded. Figure
3(c) shows an augmented design network in which the future atomic intervention node
FT is added to the design network.

(cid:1)

(cid:2)

(cid:3)

(cid:1)

(cid:4)

(cid:2)



	


(cid:3)

(cid:5)(cid:26) (cid:7)(cid:8)(cid:27) (cid:14)(cid:28)

(cid:29) (cid:23)(cid:10)(cid:14)(cid:22)(cid:10)(cid:8)(cid:9)(cid:12)(cid:26) (cid:13)(cid:19)(cid:26) (cid:24)

(cid:5)(cid:6) (cid:7)(cid:8)(cid:9)(cid:10)(cid:11)(cid:12)(cid:13)(cid:14)(cid:8)(cid:15)(cid:10)(cid:16)(cid:17)(cid:18)(cid:19)(cid:20)



	


(cid:1)

(cid:4)

(cid:2)

(cid:21)
(cid:5)(cid:22)(cid:7)(cid:8)(cid:4)(cid:23)(cid:13)(cid:24)(cid:10)(cid:14)(cid:16)(cid:10)(cid:25)(cid:8)(cid:9)(cid:10)(cid:11)(cid:12)(cid:13)(cid:14)(cid:8)(cid:15)(cid:10)(cid:16)(cid:17)(cid:18)(cid:19)(cid:20)

(cid:3)

Figure 3: Design Networks

By representing simultaneously the design nodes DE = fA; R(B)g and the future
intervention node FT , the augmented design network is useful to make conclusions about
two di(cid:11)erent tasks involved in policy evaluation and design: (1) the identi(cid:12)ability of the
causal e(cid:11)ect of T on Y , given a design dE (cid:26) DE; and (2) the choice of a design strategy
dE to collect data when the interest is to evaluate the e(cid:11)ect of intervention FT . As
mentioned before, the identi(cid:12)ability of intervention FT depends on the data available
(determined by mechanisms dE); and the e(cid:14)cacy of experimental design (dE) is always

A.M. Madrigal

565

to be determined with respect to the e(cid:11)ects it tries to isolate (here FT ). Thus, DE and
FT are and should always be read in the light of each other, and the augmented design
network allows us to do that.

E, namely comparing p(y j t0; FT = do(T = t0); DE = d(cid:3)

For causal reasoning in task (1), to discuss the identi(cid:12)ability of the causal e(cid:11)ect of T
on Y , we are interested in comparing the relevance of the choice of FT given particular
experimental conditions d(cid:3)
E )
and p(y j t0; FT = ;; DE = d(cid:3)
E). This provides us with expressions and guidelines for
control via analysis of possible confounders. On the other hand, in task (2), when
planning the data collection by choosing an experimental design, we are interested in
the relevance (or irrelevance) of the choice of experimental conditions dE, (with respect
to the identi(cid:12)ability of FT ) for di(cid:11)erent experimental choices dE (cid:26) DE. So, we are
interested in making comparisons between di(cid:11)erent settings of d(cid:3)
E and then choosing
the optimal design from all experimental designs available in DE. This provides us with
guidelines for control via design.

Augmented DAGs and the set of conditional independencies derived from them
have been used for causal reasoning in task (1) (see Dawid 2002). If two augmented
DAGs derived from experimental conditions dE1 and dE2 share the same conditional
independence statements, then they are equivalent for causal reasoning. Assignment
actions A might a(cid:11)ect the original collection of conditional independence statements:
Recording decisions will have an e(cid:11)ect on the set of variables that will be available
to us through the available (sic) experimental data. Thus R(B) will not introduce
any new (in)dependencies in the structure, but will be relevant when discussing the
potential identi(cid:12)ability of e(cid:11)ects given assignment actions A = a(cid:3): Some additional
general remarks about the Decision Networks framework can be found in Appendix A.

4 Causal graphical analysis for cluster allocation

Most of the literature in Cluster Randomised Trials (CRTs) has emphasised the fact
that Fisher’s principle is violated, as the experimental unit does not coincide with the
analysis unit, and the di(cid:11)erence in levels where the experimental allocation generating
data available, (cid:1); is at cluster level and the analysis is undertaken for a response at
individual level. When introducing the need for the evaluation of a future intervention
FT using data generated from a (past) experiment dE, it is important to acknowledge
the fact that the future intervention level might di(cid:11)er from the experimental level.
In general, the future intervention (FT = do(T = t0)), the experimental allocation
(AT = do(T = t(cid:3))) and the response variable (Y ) could each be at cluster/individual
level and would not necessarily coincide. The experimental level will de(cid:12)ne the data
structure and the conditional independence statements re(cid:13)ected in the ‘experimental’
causal graph through dE . The future intervention FT level will de(cid:12)ne the ‘future’ causal
graph structure.

The interest could lie in the causal e(cid:11)ect at cluster level or at individual level.
Responses at cluster level will summarise what is observed at a community level, while
responses at individual level are usually more of interest to describe what is the e(cid:11)ect in,

566

Cluster Allocation Design Networks

say, a household within a community. If we are interested in the intervention e(cid:11)ect on an
individual level response, depending on how the future intervention will be implemented,
there are two possible (causal) intervention e(cid:11)ects we might be interested to identify:
namely, the distribution of the individual outcome given a clustered intervention, P (Yjk j
FT j = do(Tj = t0)), and the distribution of the individual outcome given an individual
intervention, P (Yjk j FT jk = do(Tjk = t0)). The former would try to estimate the e(cid:11)ect
of a cluster-level intervention (usually the interest in social policy) and the latter would
try to estimate the e(cid:11)ect of an individual-level intervention (as could be the goal of
many medical trials).

4.1 Cluster design networks

In terms of design decision strategies, a cluster-randomised study implicitly involves two
design decisions: (1) the decision of clustering (i.e. to allocate the treatments to clusters
of individuals) and (2) the decision of randomising (i.e. to perform the allocation using
a random procedure).

When an intervention at cluster level occurs we distinguish between two cases. The
(cid:12)rst is related to the case in which the intervention a(cid:11)ects all individuals in the cluster:
for example, when the improvement of health services is undertaken at community level
and all families within a community are subject to the same infrastructure. In this case,
individuals within the cluster cannot choose not to be a(cid:11)ected by the policy intervention.
In this paper this case will be referred to as a ‘pure-cluster’ intervention (and denoted
by dC = 1). The second case refers to the situation in which, although an intervention
is allocated at cluster level, not all individuals, but only a subset of them within a
cluster, will be subject to the intervention. Actions, in this second case, are ‘done’ at
individual-level to individuals within a cluster, and thus individual characteristics might
have an in(cid:13)uence in the individual’s allocation of treatment

An example of the latter is when the intervention a(cid:11)ects only eligible individuals.
A cluster policy of this type could be seen as: ‘all eligible individuals k in cluster j will
receive policy t0 ’ via AT j = do(Tj = t0). So, allocation of policy is done at cluster level
and two eligible individuals in the same cluster cannot be allocated di(cid:11)erent policies
(contrary to what would happen if the policy allocation was done at individual level).
In the case of Progresa, this will correspond to the case where only poor households are
receiving extra money for nutrition and educational grants. These actions are aimed at
household-level; however, not all households in a community are poor and therefore not
all households within a community receive the same treatment, only the eligible ones.
In a more general setting, the individual choice of treatment might depend on some
possibly unobserved background variables and not necessarily only on some previously
de(cid:12)ned (and observed) eligibility criteria. For example, imagine that some health centres
are allocated a certain restricted quantity of food supplements to be distributed among
families visiting them, but the amount of food supplements is not enough to cover all
families. Then, the fact that a family is receiving the food supplement or not could
depend on the (unobserved) nurses’ choice or on a (cid:12)rst-come-(cid:12)rst-served basis. In any
case, when di(cid:11)erent units within a cluster do not necessarily receive the same treatment

A.M. Madrigal

567

and this is dependent on certain individual-level background variables, the experiment
will be referred as a ‘non-pure’ cluster allocation (and denoted by dC = 0).

4.2 E(cid:11)ects of cluster allocation design decisions

A design network for the general cluster setting for a cluster-level future intervention
FT j is presented in Figure 4. As we are discussing clusters of individuals, naturally
variables are not all at the same level and a simple DAG cannot be used without further
notation The levels are represented in the graph by squares, following Spiegelhalter’s
notation (see WinBUGS), meaning that the same graphical structure applies for each
of the observations at the same level. Design decisions can then be taken at both
individual and cluster level. The structure has been kept similar to that used above,
but now we have the situation replicated for the two levels involved. Let cluster j (for
j = 1; 2; :::J ) have Kj units and let Tj and Tjk be variables for intervention status
(Treatment / Control) at cluster and unit level respectively. Similarly, let Bj and
Bjk represent the background variables at cluster and unit level and Zj some recorded
cluster-level covariates that might be a(cid:11)ected by the policy. Nodes Aj and Ajk will
correspond to the assignment mechanisms to allocate policy at cluster and individual
levels respectively. Action AT j = do(Tj = t(cid:3)) de(cid:12)ned in Aj will imply Tj = 1 if the value
t(cid:3) corresponds to the policy taking place in cluster j, and will imply Tj = 0 if the value
t(cid:3) corresponds to ‘control’. This will work similarly for the individual-level case. The
recording mechanisms could be de(cid:12)ned over the set of cluster and individual background
variables, R(Bj) and R(Bjk), respectively. The response Yjk will correspond to that
observed for individual k in cluster j.

The decision of running a ‘pure’ cluster allocation experiment (dC = 1) will imply
that the intervention is done equally to all members in the cluster. So, once the treat-
ment for cluster j Tj is (cid:12)xed by action AT j = do(Tj = t(cid:3)), this fully implies actions
AT jk = do(Tjk = t(cid:3)); and so the values of the treatments Tjk for all Kj individu-
als in cluster j: Then tj = tjk = tjk0 for all individuals k; k0 = 1; 2; :::Kj in cluster
j. So, the e(cid:11)ect of pure-clustering prohibits individual covariates from in(cid:13)uencing the
choice of treatment, breaking any links that could be present from Bjk (any background
individual-level covariates) to Tjk in the graph. When ‘non-pure’ cluster allocation takes
place (dC = 0), although the experimental allocation is made at cluster level, individual
k within cluster j might be receiving treatment or not depending on some individual-
level covariates Bjk, and thus tjk might di(cid:11)er from tjk0 for k 6= k0.

The assignment mechanism nodes Aj and Ajk could be expanded. This is not done
in Figure 4, to keep the (already complex) graph as simple as possible. The individual
assignment mechanism Ajk is considered to be dependent on the actual policy that was
allocated to cluster j, Tj, and (possibly) on some individual background variables. Thus,
the action assigning policy t(cid:3) to individual k AT jk = do(Tjk = t(cid:3)) is considered to be de-
pendent on Tj and Bjk such that (cid:18)T jk = p (AT jk = do(Tjk = t(cid:3)) j Tj; Bjk) = q(Tj; Bjk).
The ‘pure-cluster’ case will imply that the individual assignment mechanism does not
depend on individual background variables Bjk and
(cid:18)T jk = p (AT jk = do(Tjk = t(cid:3)) j AT j = do(Tj = t(cid:3)); dC = 1) = 1 and the following propo-

568

Cluster Allocation Design Networks

(cid:4)

(cid:1)

(cid:1)(cid:2)(cid:3)

(cid:1)(cid:4) (cid:3)

(cid:1)
(

(cid:5)

(cid:2)

(cid:5)


(cid:2)






(cid:3)



'&

(cid:6)

#%$
$)*+

!

 

!

Figure 4: Design Network for cluster allocation and cluster-level future intervention FT j

sition is thus established.

Proposition (Pure-Clustering) If the experimental design strategy dE includes the
action of performing a ‘pure cluster’ experiment such that fdC = 1g 2 dE, then the
‘structural’ e(cid:11)ect of dC = 1 on the ‘original’ set of conditional independencies, is to
introduce the set of conditional independencies (Tjk??Bjk)dC =1 that will hold on the
data (cid:1)dE generated by dE.

Figure 5 includes a close-up of the individual-level plateau in Figure 4, in which the
design network has been extended for node Ajk and variables Zjk introduced. Now let
us refer to the situation when a ‘pure’ cluster experimental intervention is not feasible,
but when we have a non-pure cluster experiment such that, within each cluster j, indi-
vidual policy allocation follows a deterministic rule based on individual-level observed
covariates Zjk. The (cid:12)nal policy allocated to an individual through Ajk will be a function
of Zjk; and any other possible in(cid:13)uences on Tjk from background variables Bjk (other
than Zjk) are eliminated. The prevalences of Tjk in the experimental data available
(cid:1) will depend on the policy allocated to the cluster Tj and Zjk but not on Bjk.(e.g.
(cid:18)T jk = p (AT jk = do(Tjk = t(cid:3)) j Tj; Zjk) = q(Tj; Zjk)) The structure obtained is similar
to the strati(cid:12)ed allocation presented in Madrigal (2005), and arrow (c) will be deleted
when this ‘deterministic’ allocation takes place.

Proposition If a ‘non-pure cluster’ experiment includes a design strategy in which poli-
cies at individual level are allocated following a ‘deterministic’ rule de(cid:12)ned by the exper-







"
"
"


"
"
"
,
A.M. Madrigal

569

(cid:1)(cid:2)(cid:3)
(cid:1)(cid:2)(cid:3)

(cid:1)
(cid:1)

/)0
/)0

1+2
1+2

(cid:4)
(cid:4)

(cid:5)
(cid:5)

(cid:2)
(cid:2)

(cid:5)
(cid:5)
q.
q.

(cid:3)
(cid:3)

q.
q.
(cid:5)
(cid:5)

(cid:7)
(cid:7)

(cid:3)
(cid:3)

(cid:1)
(cid:1)

@A
@A

/)0
/)0

1+2
1+2

(cid:6)
(cid:6)

DE5F7'9
DE5F7'9

;<9
;<9

GH4
GH4

46587:9
46587:9

;<9
;<9

Figure 5: Close-up of individual-level plateau

imenter based on covariates Zjk , then conditional independencies (Tjk??Bjk j Zjk)dE
are introduced and will hold on the data (cid:1)dE generated by dE.

As introduced in Madrigal and Smith (2004), design decision strategy dE including
random allocation of policies (i.e. A(cid:18)T = do((cid:18)T = (cid:18)(cid:3)
T ) 2 dE) might, qualitatively speak-
ing, modify the structure of the data we are to collect (see Appendix A). By allocating
the treatments completely at random (i.e. A(cid:18)T j = do((cid:18)T j = (cid:18)(cid:3)
T j )) we ensure that the
treatment received is independent of any background variables Bj that, otherwise, might
have an in(cid:13)uence on the policy assignment mechanism. Then, when random allocation
takes place, arrow (r) from Bj to Aj in Figure 4 disappears and the conditional inde-
pendence statement (Tj??Bj )dE
T j , might depend on
possible strati(cid:12)cation observed variables. When randomising at cluster level we there-
fore ensure that the level of treatment that is received by cluster j is independent of
the level received by cluster j 0 (i.e. knowing that cluster j was assigned intervention t(cid:3)
does not give us any further information about the intervention group at cluster j 0).

holds. Again, this probability, (cid:18)(cid:3)

4.3

Identifying cluster-level interventions

The appropriateness and consequences of di(cid:11)erent design decisions dE (cid:26) DE will depend
on the goals of the experiment. The case in which the interest is in the e(cid:11)ect of a cluster-
level future intervention FT j = do(Tj = t0) on a cluster-level response Yj will degenerate
to the one-level case. When the interest lies in an individual-level response Yjk, it can be

-
-
-
.
-
-
3
=
=
=
>
?
-
B
.
-
B
-
B
-
B
-
B
-
B
C
3
.
-
B
=
=
=
-
-
-
.
-
-
3
=
=
=
>
?
-
B
.
-
B
-
B
-
B
-
B
-
B
C
3
.
-
B
=
=
=
570

Cluster Allocation Design Networks

seen in Figure 4 that, if d(cid:3)
E includes a random cluster allocation procedure and arrow
(r) is not present, the conditional independencies (Yjk??FT j j Tj )d(cid:3)
hold for all j,k.
Thus, once the value of the policy assigned to the cluster Tj is known, learning whether
the policy status Tj arose from the future policy implemented FT j = do(Tj = t0)
or from the ‘original’ experimental allocation AT j = do(Tj = t(cid:3)) when FT j = ;, is
irrelevant. Therefore, direct identi(cid:12)ability of the e(cid:11)ect FT j = do(Tj = t0) on Yjk holds
and p(yjk j FT j = do(Tj = t0)) can be directly obtained from data (cid:1)d(cid:3)
available as long
as t0 2 ft(cid:3)g such that

E

E

p(yjk j FT j = do(Tj = t0); (cid:1)d(cid:3)

E

) = p(yjk j Tj = t0; (cid:1)d(cid:3)

E

)

This does not disregard the fact that individuals belonging to the same cluster will
have a positive correlation, which must be taken into account in any model used for the
analysis and estimation of the e(cid:11)ect on an individual-level response.

E , (Yjk??FT j j Tj )d(cid:3)

Again, when non-random cluster allocation is performed as part of the experimental
design d(cid:3)
does not hold anymore, but the conditional independencies
(Yjk??FT j j Tj; Bj )d(cid:3)
hold for all j,k. Thus, the identi(cid:12)ability of the e(cid:11)ect of a future
policy FT j = do(Tj = t0) on Yjk will depend on the recordability of cluster-background
variables R(Bj ) and the causal e(cid:11)ect will need to be obtained through an ‘adjustment’
procedure such that, as before, using the back-door criteria

E

E

p(yjk j FT j = do(Tj = t0)) =Z p(yjk j Tj = t0; Bj )p(Bj )dBj:

Unless we are ready to assume some prior distribution for p(Bj ), the recording of vari-
ables Bj as part of the design (fR(Bj) = 1g 2 d(cid:3)
E) are needed to achieve an ‘adjusted
identi(cid:12)cation’ of the causal e(cid:11)ect.

Di(cid:11)erent recording mechanisms might assist identi(cid:12)cation. For instance, if we were
ready to assume that cluster background variables did not have a direct e(cid:11)ect on the
individual response, such that arrow from Bj to Yjk was deleted, then all the in(cid:13)uence
from Bj would be through individual background variables and the observed cluster-level
variables Zj . In this case, conditioning on Tj , Bjk and Zj would be enough and a design
able to record these variables will provide identi(cid:12)ability. Thus, if cluster background
variables Bj were not accessible to the experiment, this new set of covariates fBjk; Zjg
could assist identi(cid:12)cation.

4.3.1 Bayesian hierarchical models

In a hierarchical setting, data within each cluster j is assumed to depend on parameters
(cid:18)j , which in turn are assumed to be drawn from some population distribution with
parameters  : In an initial model, the response yjk for individual k in cluster j is
assumed to have a Normal distribution, such that

yjk (cid:24) N ((cid:22)jk; (cid:27)2)
(cid:22)jk = uj

(1)

A.M. Madrigal

571

and cluster-speci(cid:12)c random e(cid:11)ects (uj ) are assumed to have a Normal distribution with
mean (cid:30)j and variance (cid:27)2

u, such that

uj (cid:24) N ((cid:30)j ; (cid:27)2
u)
(cid:30)j = (cid:11) + (cid:12)Tj

(2)

where Tj represents the treatment given to the jth cluster. There are many potential
elaborations to this basic model (see Spiegelhalter 2001; Turner et al. 2001). The priors
that need to be speci(cid:12)ed for this model are p((cid:11)); p((cid:12)); p((cid:27)2); p((cid:27)2
u). Making causal
assumptions and a graphical representation of all in(cid:13)uences present in the particular
system analysed could assist recognition of possible confounders and thus assist both
the experimenter’s decisions for control via design and the analyst’s decisions for control
via analysis.

When cluster allocation is done randomly, if we are ready to assume linear relations,
the two-level Bayesian hierarchical model as speci(cid:12)ed in equations (1) and (2) could
be used to estimate the e(cid:11)ect of FT j on Yjk, and coe(cid:14)cient beta can ‘safely’ be given
a causal interpretation as an ‘overall’ e(cid:11)ect. For the non-random case, the analysis
will need the conditioning on the ‘relevant’ background variables. The inclusion of
individual-level and cluster-level covariates in the analysis could be done directly by
including them in equations (1) and (2) respectively. The conclusions just derived hold
for both ‘pure’ cluster and ‘non-pure’ cluster allocations. The ‘overall’ causal e(cid:11)ect will
correspond to a ‘total e(cid:11)ect’ when a ‘pure’ cluster allocation (dC = 1) is done. However,
this will not be the case for dC = 0; where the ‘total e(cid:11)ect’ cannot be obtained. To make
the di(cid:11)erence between ‘overall’ and ‘total’ e(cid:11)ects clearer, the interactions of individuals
in a cluster have to be considered, and this is discussed below in Section 4.5.

4.4

Identifying individual interventions from clustered data

Consider the case where the main interest is in obtaining the individual-level causal
e(cid:11)ect, namely P (Yjk j FT jk = do(Tjk = t0)) from data that is clustered. If randomised
allocation could take place at individual level, then it could be directly identi(cid:12)ed from
the experimental data (cid:1); as individual random allocation will break the possible in(cid:13)u-
ence of cluster background variables on the policy allocated to the individual. Suppose
that it is not feasible to randomise at individual level, but to intervene clusters is possi-
ble. The design network for this case will basically coincide with that shown in Figure 4,
but in this case we assume that the future policy will consist of an individual intervention
FT jk, and that we are interested in identifying e(cid:11)ects at the individual level.

From the design network in Figure 6 it can be seen that (Yjk??FT jk j Tjk) does
not hold even if arrows (r) and (c) are deleted from the graph. So, the e(cid:11)ect of policy
intervention FT jk = do(Tjk = t0) on Yjk cannot be identi(cid:12)ed directly from the data
and some adjustment will be needed.

When a ‘non-pure’ cluster allocation takes place in the experiment, individual policy
assignment will depend on both the policy allocated Tj and individual background

572

Cluster Allocation Design Networks

(cid:1)
(cid:1)

(cid:1)(cid:2)(cid:3)
(cid:1)(cid:2)(cid:3)

(cid:1)(cid:4) (cid:3)
(cid:1)(cid:4) (cid:3)

(cid:1)
(cid:1)
I(P
I(P

(cid:4)
(cid:4)

(cid:2)
(cid:2)

(cid:4)
(cid:4)
I(P
I(P

(cid:2)
(cid:2)
IP
IP

(cid:6)
(cid:6)

IP
IP

JLK
JLK

MN(O
MN(O

(cid:3)
(cid:3)

JK
JK

MN:ZO
MN:ZO

(cid:5)
(cid:5)
IP
IP
X%Y
X%Y
Y[R*S+U
Y[R*S+U

VU
VU

QRTS<U
QRTS<U

VU
VU

Figure 6: Design Network for cluster allocation and individual-level future intervention FT jk

variables Bjk: Conditioning on this set of variables, the irrelevance of FT jk is gained
such that (Yjk??FT jk j Tjk; Tj; Bjk) Thus the recording of Bjk is needed in order to
obtain adjusted identi(cid:12)ability through

p(yjk j FT jk = do(Tjk = t0); dC = 0) =Z p(yjk j Tjk = t0; Tj = t(cid:3); Bjk)p(Tj = t(cid:3); Bjk)dBjkdTj ,

where, if randomisation did not take place at cluster level, Tj and Bjk are not indepen-
dent (both having Bj as an ancestor) and their joint distribution is needed. If recording
at individual level for Bjk is not undertaken, the causal e(cid:11)ect will be unidenti(cid:12)able.

When ‘pure’ cluster allocation is done, and as a result arrow (c) is deleted, then there
are no individual level confounders and all possible confounders will be at cluster-level.
So, ‘pure cluster’ assignment might improve identi(cid:12)cation of individual intervention
e(cid:11)ects, in particular, when randomisation at cluster level is feasible or in the case when
cluster-level confounders (Bj ) are easier to observe and/or control than individual-level
confounders (Bjk).

As shown above, the ‘overall’ e(cid:11)ect of a future cluster-level intervention at cluster
level FT j can be identi(cid:12)ed from the experimental data when policies in the experiment
are allocated randomly at cluster level. Something similar happens when the assignment
is not carried out randomly, but cluster background variables are recordable and an
‘adjustment’ measure is needed. Moreover, for dC = 1, the overall e(cid:11)ect will coincide
with the participants’ total e(cid:11)ect. Thus, if the indirect e(cid:11)ect due to interaction among
neighbours is negligible, as could be the case when vitamin supplements are administered
to children in Progresa, the total e(cid:11)ect measured will be equal to the direct (personal)

I
I
I
I
W
W
W
W
W
W
\
]
I
I
I
I
W
W
W
W
W
W
\
]
A.M. Madrigal

573

e(cid:11)ect. Although this might not be always true for social policies, this might be the case
for some treatments in a medical setting in which, for example, a drug is supposed to
act in an individual regardless of his interaction with other people in the cluster. In
this case, a ‘pure cluster’ design could assist identi(cid:12)cation of individual interventions.
Therefore, a ‘good’ (randomised or controlled) cluster design might be able to provide
more information than a ‘bad’ (with unobserved confounders) individual design. The
distinction of overall, total, direct and indirect e(cid:11)ects will be discussed next.

4.5 Overall versus total e(cid:11)ects

As Koepsell (1998) states, ‘just as infectious agents can be spread from person to per-
son, transmission of attitudes, norms and behaviours among people who are in regular
contact can result in similar responses’. So, when people interact or communicate, their
response to an intervention can be explained (and partitioned) in terms of direct (‘per-
sonal’) e(cid:11)ect and indirect (‘neighbours’) e(cid:11)ect. So, interventions may a(cid:11)ect the whole
population, not just those who participate (or were subject to interventions).

The fact that all individuals in a group follow the same policy, or are encouraged
to take a particular action, has thus an additional ‘interaction e(cid:11)ect’. This is so as
individuals interact with each other, creating a domino e(cid:11)ect. In the case of Progresa
we have, for example, the fact that mothers talk! Thus, if a mother is encouraged to
take children to the health centre for food supplements, besides her possible individual
motivation, the fact that other mothers in the village are encouraged as well, creates an
additional e(cid:11)ect on her (i.e. if everybody is doing it, there is an extra motivation to do
it, and being the only one not doing it will be rare and possibly socially penalised).

If in a cluster, not all individuals are allocated the same intervention, then the e(cid:11)ects
of interventions can be classi(cid:12)ed, following Hayes et al. (2000)’s de(cid:12)nition, according to
the ‘intervention status of the individual’ as participants (treated) or nonparticipants

(controls). Those who participate receive both a direct (cid:0)DE(P )(cid:1) and an indirect e(cid:11)ect
(cid:0)IE(P )(cid:1), which combine to form the total e(cid:11)ect (cid:0)(cid:21)(P ) = DE(P ) + IE(P )(cid:1). The non-

participants receive only an indirect e(cid:11)ect, IE(N P ); so their total e(cid:11)ect contains only
those indirect e(cid:11)ects ((cid:21)(N P ) = IE(N P )). The indirect e(cid:11)ects received by participants
and non-participants may di(cid:11)er in magnitude, so an index is used to distinguish them:

Total e(cid:11)ects

(cid:21)(P ) = DE(P ) + IE(P )

(cid:21)(N P ) = IE(N P )

Participants (P)

Non-participants (NP)

If we are ready to assume that these e(cid:11)ects are equal for all individuals in a cluster,
then the overall e(cid:11)ect observed in a cluster will correspond to the weighted average of
the e(cid:11)ects on participants and non-participants such that

Overall e(cid:11)ect = w(P )(cid:21)(P ) + w(N P )(cid:21)(N P )

(3)

where w(P ) and w(N P ) are just weights that will be functions of the number of partici-
pants and non-participants in the cluster (or in terms of the ‘coverage’ -% of participants-
of the experiment). So, the overall e(cid:11)ect will include a combination of direct and indirect

574

Cluster Allocation Design Networks

e(cid:11)ects. In particular, expression (3) could be extended to be re-written as

Overall e(cid:11)ect = w(P )(cid:0)DE(P ) + IE(P )(cid:1) + w(N P )IE(N P )

= w(P )DE(P ) +(cid:0)w(P )IE(P ) + w(N P )IE(N P )(cid:1)

In the case where indirect e(cid:11)ects, for both participants and non-participants, are as-
sumed to be negligible such that IE(P ) (cid:25) 0 and IE(N P ) (cid:25) 0, then the overall e(cid:11)ect will
be approximately proportional to the direct e(cid:11)ect such that

Overall e(cid:11)ect (cid:25) w(P )DE(P ):

In the case of a ‘pure cluster’ experiment, either all individuals are participants (treated
cluster) or all are non-participants (control cluster). In this situation, contamination
within clusters is completely avoided, and in control clusters no intervention indirect
e(cid:11)ects are observed (i.e. IE(N P ) = 0). For the treated clusters, all individuals are
participants, and overall intervention e(cid:11)ect of the cluster will coincide with the total
participant e(cid:11)ect, denoted by (cid:28)(P ) : namely,

Overall e(cid:11)ect (control cluster) = 0 (cid:1) (cid:21)(P ) + 1 (cid:1) (cid:21)(N P ) = total e(cid:11)ect(N P ) = 0
Overall e(cid:11)ect (treated cluster) = 1 (cid:1) (cid:21)(P ) + 0 (cid:1) (cid:21)(N P ) = total e(cid:11)ect(P ) = (cid:28)(P )

and therefore

Overall e(cid:11)ect(dC =1) = total e(cid:11)ect (P ) = (cid:28)(P ) = DE(P ) + IE(P )

Individually randomised trials typically aim to measure the direct e(cid:11)ect, DE(P ). By
contrast, CRTs measure the total e(cid:11)ect (cid:28)(P ) if all individuals participate, but otherwise
they measure the overall e(cid:11)ect, which will vary according to intervention coverage and
the characteristics of the population.

If individuals are naturally clustered, the magnitude of the indirect e(cid:11)ect of an
intervention is likely to be important in deciding whether a trial should be individually
- or cluster- randomised.
Indirect e(cid:11)ects, due to interaction, will be included in the
outcome measure. As a consequence, if the main interest is in measuring only the direct
e(cid:11)ect that a possible drug/treatment, say, has on an individual and it is known that
indirect e(cid:11)ects could be relevant, then CRTs might not be the best option as they will
measure the overall e(cid:11)ect instead of the direct e(cid:11)ect.

In assessing the value of intervention it is important to take into account their
indirect as well as direct e(cid:11)ects. In some cases it may be better to avoid intervention if
the coverage needed to make it bene(cid:12)cial is too high to be realistically achievable. In
addition, it may be desirable to separate the overall e(cid:11)ect into its direct and indirect
components. Methods for measuring direct and indirect e(cid:11)ects separately have mostly
been developed in the context of vaccination (see Hayes et al. 2000; Longini et al. 1998).
Standard CRT designs measure the overall e(cid:11)ect of intervention, and this is often the
most useful measure for policy makers because it includes all the components, both
direct and indirect, which a population would experience if a cluster policy were to be
implemented.

A.M. Madrigal

575

It should be clear that the stable-unit-treatment-value-assumption (SUTVA, as la-
beled in Rubin 1980), which implies that the response of the unit does not depend on
which treatment was applied to other units, does not hold when units interact and the
indirect neighbours e(cid:11)ects are not negligible. However the Bayesian predictive decision-
theoretic approach that is followed in this paper does not require this assumption, as
would be the case in Rubin’s counterfactual approach. The counterfactual model for
causal inference could lead to ambiguities and pitfalls, as discussed by Dawid (2000).

5 Progresa e(cid:11)ect example using hierarchical models

In this section a hierarchical model analysis based on Spiegelhalter (2001) is performed
for the evaluation of cluster- and individual-level interventions based on Progresa data.
In the programme, communities were randomly allocated either to a treatment or a
control group. The community level interventions G1 (such as the improvement of
health services and educational talks) are received by all households in a ‘treatment’
community.
In addition, all eligible (poor) households that belong to a ‘treatment’
community receive household interventions, such as (cid:12)nancial support, G2. The data
recorded includes a census of eligible and non-eligible households for (treated and con-
trol) communities selected for the study.

Let Tj be the cluster treatment indicator, such that ATj = do(Tj = 1) if community
j was allocated to Progresa programme and AT j = do(Tj = 0) if it was allocated to
control, so we have

Tj =(cid:26) 1 if community Treatment

0 if community Control

Let E be an indicator variable denoting eligibility status. Then Ejk = 1 if household k
in community j is eligible and Ejk = 0 if non-eligible. In Progresa, Ejk = 1 corresponds
to a poor household. Thus,

Ejk =(cid:26) 1 if ‘poor’ household

0 if ‘non-poor’ household

So, household k in community j will be allocated household-level Progresa interventions
Tjk (e.g. (cid:12)nancial support) through an allocation Ajk; in which a household is given
extra money if, in addition to belonging to a treatment cluster, the household is ‘eligible’.
It will not be given extra money if either it is not eligible or if it belongs to a control
community. If we denote by Pjk, the indicator variable for ‘Progresa participant’, such
that Pjk = 1 if household k in cluster j receives economical support and Pjk = 0 if not,
then, Pjk is de(cid:12)ned as

Pjk =(cid:26) 1 if Tj = 1 and Ejk = 1

0 otherwise

From the general formulation of the Design Network for cluster allocation presented
above, a simpli(cid:12)ed version of Progresa’s experimental design containing the main fea-
tures is shown in Figure 7, where Yjk represents the response of household k in commu-
nity j for k = 1; 2; :::Kj; Bj represents the background variables that are shared by all

576

Cluster Allocation Design Networks

individuals in community j. Background variables at individual level were not added
to keep the graph simple, but could be easily incorporated. As allocation at cluster
level was done randomly in Progresa, no arrow is drawn from Bj to Tj . If G1 denotes
Progresa’s cluster-level intervention (action) corresponding to health services and talks
(i.e. the ‘encouragement’ that communities receive to improve their nutrition) and G2
denotes Progresa’s household-level action of giving (cid:12)nancial support to poor households,
then note that the action do(Tj = 1) will trigger both atomic cluster-level intervention
G1 and contingent (on Tj and Ejk) individual-level intervention G2.

(cid:1)

(cid:3)

(cid:5)

(cid:6)

(cid:3)

(cid:2)

(cid:4)

aFbdcfe

g6e

iEj

jkblcfe

ge

Figure 7: Progresa experimental Design Network for basic nodes

5.1 Cluster-level intervention e(cid:11)ect

Imagine that we are interested in the e(cid:11)ect of Progresa on the total food consumption
yjk, measured in terms of the amount of money spent on food in a household. The com-
plete data set, including poor and non-poor households, consists of 20,589 households
in 500 clusters. To begin with, assume we are interested in measuring the overall e(cid:11)ect
of Progresa intervention FT j = do(Tj = t0). A hierarchical model following the setting
presented in Section 4.3 (equations (1) and (2)) is used to estimate this e(cid:11)ect. This
model was run in BUGS using vague priors following Spiegelhalter (2001) with

p((cid:11)) (cid:24) U nif orm((cid:0)10000; 10000)
p((cid:12)(cid:3)) (cid:24) U nif orm((cid:0)10000; 10000)

p((cid:27)(cid:0)2) (cid:24) Gamma(0:01; 0:01)
p((cid:27)(cid:0)2

u ) (cid:24) Gamma(0:01; 0:01)

We encountered no di(cid:14)culties in convergence of this model. The analysis is based on a
sample of 10,000 iterations following a burn-in of 5,000.

The posterior inference (means and 95% intervals) for the parameters involved is
presented in Table 1. As can be seen from the table, the posterior mean for beta

^
^
_
^
`
^
`
h
h
h
h
h
h
m
n
^
`
A.M. Madrigal

577

Parameter

(cid:12)(cid:3)
(cid:11)
(cid:27)(cid:0)2
(cid:27)(cid:0)2
u

Mean

39:48
444:9

95% interval

(20:36; 58:33)
(435:2; 454:6)

2:102E (cid:0) 5
1:054E (cid:0) 4

(2:061E (cid:0) 5; 2:143E (cid:0) 5)
(1:052E (cid:0) 41; :216E (cid:0) 4)

Table 1: Posterior distributions of parameters for cluster-level intervention

is E [(cid:12)(cid:3) j (cid:1)] = 39:48 with a 95% interval of (20.36,58.33). In this case (cid:12) (cid:3) gives the
cluster-level total overall e(cid:11)ect of Progresa intervention on the food expenditure in a
household. So, (cid:12)(cid:3) contains a summary of the e(cid:11)ects of the programme (through G1 and
G2) on Y for all the population in a cluster, by averaging participants (receiving G1
and G2) and non-participants (only receiving G1). Depending on the aims of the study
this total overall e(cid:11)ect might be the relevant causal e(cid:11)ect of interest. In such a case,
it could be said that the causal e(cid:11)ect of Progresa is to increase, on average, the food
expenditure of a household by 39.48 Mexican Pesos. This in relation to the average food
expenditure for a household in a control community that will be of E [(cid:11) j (cid:1)] = 444:9
Mexican Pesos.

5.2

Individual-level e(cid:11)ect

Now imagine that we are interested in obtaining an estimate of the ‘causal’ e(cid:11)ect of the
individual-level intervention FG2 = do(G2 = g0
2 = q(poor)) of giving (cid:12)nancial support
to poor households. The allocation of G2 depends on the cluster-allocated policy Tj
(Treatment/Control) and on the eligibility condition Ejk of a household de(cid:12)ned as
‘poor’: both are assumed to have an e(cid:11)ect on the household expenditure level and thus
act as confounders in this case. So, to identify the individual-level e(cid:11)ect of FG2 ; it is
needed to control by including these two confounders in the analysis.

The hierarchal model used for the cluster-level e(cid:11)ect above can be extended to
include covariates Tj and Ejk at cluster and individual level respectively. The ‘Progresa
participants’ status of a household Pjk acts as an indicator variable of the presence
of economic support provided by G2. We include here the household size Zjk as an
individual covariate to illustrate the possible inclusion of other covariates in the model.
Household size will have an in(cid:13)uence on the total expenses of the household Y; and it
is neither a(cid:11)ected by the policy nor a(cid:11)ecting (at least directly) policy allocation. So
now this is considered part of the white noise (with respect to T and P) at the recorded
individual level. Equations (1) and (2) can be substituted by

yjk (cid:24) N ((cid:22)jk; (cid:27)2)
(cid:22)jk = uj + (cid:12)2Pjk + (cid:14)Ejk + (cid:13)Zjk

uj (cid:24) N ((cid:30)j ; (cid:27)2
u)
(cid:30)j = (cid:11) + (cid:12)1Tj

(4)

(5)

578

Cluster Allocation Design Networks

We chose to use the same priors to estimate this model, as before. Thus all the coe(cid:14)-
cients (namely (cid:11); (cid:12)1; (cid:12)2; (cid:14) and (cid:13)) were given vague uniform distributions a priori. Again,
we encountered no di(cid:14)culties in obtaining convergence and the sample simulated is the
same size as before.

The posterior means and 95% intervals for all the parameters are given in Table 2.
The individual-level e(cid:11)ect here will be measured by the coe(cid:14)cient of Pjk, namely (cid:12)2,
whose posterior mean is given by E [(cid:12)2 j (cid:1)] = 45:71 with a 95% interval of (33.95, 57.49).
In general (cid:12)2 will isolate the e(cid:11)ect of G2 (from the e(cid:11)ect of G1) and we could say that the
e(cid:11)ect of a policy FG2 that provides economic support according to the poverty level of
a household will increase, on average, the food expenditure of a participant household
by 45 Mexican Pesos (regardless of the presence or not of a secondary action G1).
However, (cid:12)2 implicitly includes possible indirect e(cid:11)ects resulting from the interaction
of participant households with non-participant households in a community.

Parameter

(cid:11)
(cid:12)1
(cid:12)2
(cid:14)
(cid:13)

Mean

473.5
16.7
45.71
-34.14
30.56

95% interval

(461.7 , 485.5)
(-3.848 , 36.63)
(33.95 , 57.49)
(-43.54 , -24.69)

(29.54 , 31.6)

Table 2: Posterior distributions of parameters for individual-level intervention

A second reading of this analysis could consider the case in which the total e(cid:11)ect of
Progresa (de(cid:12)ned by G1 and G2) is split in its e(cid:11)ect on food expenditure, due to the
community-level action of educational talks (G1) and the economic support provided
at household level to poor people (G2). Then, (cid:12)1 becomes a parameter of interest
containing the direct e(cid:11)ect of G1 (i.e. the e(cid:11)ect of Progresa on food expenditure that is
not due to economic support) and Pjk is regarded as an intermediate variable. In this
case, and following the reasoning of path analysis (Bollen 1989; Pearl 2000), we can see
that the total overall e(cid:11)ect of Progresa (cid:12)(cid:3) could be written as (cid:12)(cid:3) = (cid:12)1 +(cid:21)(cid:12)(cid:3)
2 where (cid:21) will
contain information about the prevalence of participants within a treated community.
The total overall e(cid:11)ect at household level is here denoted by (cid:12) (cid:3)
2 (= (cid:12)2 + (cid:14)b(E)P ) where,
as before, (cid:12)2 represents the direct individual-level e(cid:11)ect and (cid:14)b(E)P the confounding
e(cid:11)ect, which in this case has been controlled via analysis.. In this case it can be seen
that, although the posterior mean for (cid:12)1 has a value of 16.7, the 95% posterior interval
includes the value zero. So we cannot assert that the direct e(cid:11)ect of the cluster-level
intervention G1 was di(cid:11)erent from zero. A more careful analysis, possibly including
more ‘white noise’ covariates at cluster level, might provide narrower intervals for the
coe(cid:14)cients. However, given that this response variable is measured in money terms, the
main e(cid:11)ect of the programme could be expected to be due to the increase of income of
the participant households derived from G2. This might not be true for other response
variables.

We can notice that one could be tempted to o(cid:11)er a causal interpretation to the

A.M. Madrigal

579

betak sample: 10000

betaj sample: 10000

   0.06
   0.04
   0.02
    0.0

   0.06
   0.04
   0.02
    0.0

   20.0

   40.0

   60.0

  -50.0

    0.0    25.0    50.0

(cid:1)(cid:2)(cid:3)

(cid:1)(cid:4) (cid:3)

Figure 8: Posterior distribution for (a) (cid:12)2 and (b) (cid:12)1

coe(cid:14)cient of eligibility ((cid:14)) that distinguishes between poor and non-poor households.
However, the data available was not properly selected in order to isolate the relationship
between poorness and food expenditure. The level of poorness in Progresa was obtained
through a discriminant function that depends on many household level covariates that
could also a(cid:11)ect the response, becoming confounders for this coe(cid:14)cient. Therefore, if
we are interested in interpreting (cid:14), a data set including these covariates will be needed.
In this analysis, household size is known to be part of the variables used to de(cid:12)ne the
poorness of a household. Its inclusion or not in the model (analysis not shown), although
‘transparent’ for (cid:12)1 and (cid:12)2 (given Ejk), will have an important e(cid:11)ect on the posterior
mean E [(cid:14) j (cid:1)] : Although, in this case the posterior mean seems to be signi(cid:12)cantly
di(cid:11)erent from zero and has the ‘correct’ sign (i.e.
it should be expected that poor
people spend less money than non-poor people), this super(cid:12)cial conclusion might still
be subject to unrecorded confounding.

6 Conclusion

The primary contribution of this paper is to expand on Dawid (2002)’s model for causal-
ity reasoning within the Bayesian decision-theoretic framework: to ‘adapt’ it to policy
analysis, to include experimental nodes, to allow intervention nodes ‘do’ parameters
nodes, to discuss the relevance (or irrelevance) of experimental design and to include
interventions at di(cid:11)erent levels (clusters) of units. Observational data is considered a
degenerate type of experimental data. In addition, there was a need to create some nota-
tion to describe the mechanisms derived from choices, such choices as the experimenter
might make when choosing the ‘lens of the camera’ to picture the world. These choices
a(cid:11)ect the characteristics (units, variables and distributions) of the database. The inspec-
tion of in(cid:13)uence diagrams and, in particular, the augmented DAGs derived from them,
has been shown to be useful to decide if the data available is su(cid:14)cient for obtaining
consistent estimates of the target causal e(cid:11)ect of policy intervention FT = do(T = t0).
If so, we can derive a closed-form expression for the target quantity in terms of distribu-
tions of available quantities. If it is not su(cid:14)cient, this framework can help suggest a set
of observations and experiments that, if performed, would render a consistent estimate
feasible. Design Networks expand the IDs framework to address explicitly experimental
design and provide the semantics to discuss how design can assist identi(cid:12)cation, and
when and how one can identify causal e(cid:11)ects.
Incorporating nodes for experimental
design decisions is useful in demonstrating their impact on the graphical structure and

580

Cluster Allocation Design Networks

on the ‘data structure’ derived from it. Certain policy assignment mechanisms, such
as randomised cluster allocation, will add ‘extra’ independencies to the ID; de(cid:12)ning a
new collection of conditional independencies. To make a causal inference of FT , it is
important that we consider the mechanisms producing the data (dE). Furthermore, we
need to di(cid:11)erentiate between policy interventions we want to evaluate, FT 2 DT ; and
experimental allocations, AT 2 DE. The relevance of DE in assisting the identi(cid:12)cation
and comparison of di(cid:11)erent mechanisms d(cid:3)
E in terms of identi(cid:12)ability can be addressed
using DN for diverse types of assignment. Design networks were introduced for cluster
allocation, and Spiegelhalter (2001)’s Bayesian hierarchical model was extended to in-
clude causal interpretation and used to illustrate a causal analysis of a simpli(cid:12)ed version
of the Progresa programme.

When cluster allocation is done randomly, two-level Bayesian hierarchical models
could be used to obtain the e(cid:11)ect of FT j on Yjk and the relevant coe(cid:14)cient can ‘safely’
be given a causal interpretation as an ‘overall’ e(cid:11)ect. For the non-random case the
analysis will need the conditioning on the ‘relevant’ background variables. Cluster
allocation might help identifying individual-policy e(cid:11)ects in certain cases.

Design of experiments within the Bayesian decision theoretic approach has been
studied broadly in the literature; however, not in terms of causal reasoning and identi-
(cid:12)ability. In most of the literature on Bayesian experimental design the discussions have
been limited to a) a set of options which include the choice of the levels of treatment
and the number of repetitions within each level, and b) to a utility function usually de-
(cid:12)ned in terms of minimising the posterior variance of estimates (or maximising entropy),
which is an important issue. However, an experiment that overlooks identi(cid:12)cation could
lead to the wrong conclusions if causal analysis is of interest. Causal inference imposes
an extra criterion for the evaluation of the designs. This work extends the on-going
discussion to a more general setting where the set of options is extended to include deci-
sions about policy allocation and recording mechanisms and where the utility function
is allowed to include a measurement of identi(cid:12)ability.

Appendix A. Design Networks: General remarks

By allowing the ‘idle’ system in Dawid (2002) to refer to any experimental system, the
list of propositions in this section could be derived directly or are analogous to the
results presented in Dawid (2002).

In general, we will say that the ‘causal’ e(cid:11)ect of T on Y is identi(cid:12)able directly from
the available (experimental) data collected under DE = dE, if learning the value of FT
(i.e. learning if the future policy was set to a value or left to vary ‘naturally’) does not
provide any ‘extra’ information about the response variable Y given the value of T and
) then p(y j t0; FT = do(T =
experimental conditions DE = dE (i:e: if (Y ??FT j T )DE
t0); DE = dE) = p(y j t0; FT = ;; DE = dE): Note that this will hold (or not) regardless
of R(B).

De(cid:12)nition (Direct identi(cid:12)ability) The ‘causal’ e(cid:11)ect of T on Y is identi(cid:12)able directly

A.M. Madrigal

581

from available (experimental) data collected under DE = dE, if (Y ??FT j T )dE

: Then

p(y j FT = do(T = t0); DE = dE) = p(y j t0; DE = dE):

This will imply that the conditional distribution p(y j t0) that is extracted from
data generated according to dE can be used directly to estimate the target causal e(cid:11)ect
p(y j FT = do(T = t0)) regardless of the recordability or the actual values of B.

Let dE1 = fA = a1; R(B) = r1g and dE2 = fA = a2; R(B) = r2g be two experimen-

tal design interventions.

Proposition If two experimental DNs under allocations de(cid:12)ned by A = a1 and A = a2
share the same conditional independencies Sa1 = Sa2 , and (Y ??FT j T )a holds for
a = a1; a2 then experiments dE1 and dE2 share the same ‘direct identi(cid:12)ability’ status
for the causal e(cid:11)ect of T on Y de(cid:12)ned by intervention FT for any recording mechanisms
r1 and r2 Thus, the choice of assignment mechanism (between a1 and a2) is said to be
irrelevant to obtaining direct identi(cid:12)cation.

For instance, if a1= pure random allocation with probability (cid:18)(cid:3)

1 and a2= pure random
T < 1 for all t(cid:3), both assignments lead to
allocation with probability (cid:18)(cid:3)
direct identi(cid:12)ability. Then, regardless of the background variables recorded, the choice
between a1 and a2 is irrelevant for identi(cid:12)cation purposes. Both allocations might be
di(cid:11)erent in terms of a balanced sample and the variance and e(cid:14)cacy of the estimates,
but this is regarded as a secondary goal of the choice of experiment.

2, such that 0 < (cid:18)(cid:3)

Proposition If direct identi(cid:12)ability holds for a1, i.e. (Y ??FT j T )a1 ; but not for a2,
then the choice between dE1 = fa1; rg and dE2 = fa2; rg is not irrelevant for direct
identi(cid:12)ability.

An example of this is when a1 = pure random allocation and a2 = ;. Although
naturally it could be observed that (T ??B); holds, direct identi(cid:12)ability will usually
not hold for a2 = ;: So, the choice between performing a randomised experiment and
observing the original mechanism is not irrelevant for the isolation of e(cid:11)ects and their
direct identi(cid:12)cation.

Direct identi(cid:12)ability of the causal e(cid:11)ect implies assuming (Y ??FT j T )dE

, which
is a very strong assumption that usually will not hold when observational studies or
imperfect experiments take place. However, we might be ready to assume that for a
set B(cid:3) (cid:18) B where B(cid:3)??FT , conditional on B(cid:3) the learning of FT is irrelevant for the
response, such that (Y ??FT j T; B(cid:3))dE

and then

p(y j t0; B(cid:3); FT = do(T = t0); DE = dE ) = p(y j t0; B(cid:3); FT = ;; DE = dE )

so we could ‘substitute’ the future intervened probability with the ‘natural experimental’
distribution available from the data.

582

Cluster Allocation Design Networks

De(cid:12)nition (Conditional identi(cid:12)ability) The ‘causal’ e(cid:11)ect of T on Y conditional on B (cid:3)
is identi(cid:12)able directly from the available (experimental) data collected under DE = dE,
if (Y ??FT j (T; B(cid:3)))dE

: Then

p(y j FT = do(T = t0); B(cid:3); DE = dE) = p(y j t0; B(cid:3); DE = dE ):

Notice that conditional identi(cid:12)ability alone does not imply that procedures like the
back-door formula can be used to calculate the overall e(cid:11)ect of T on Y; which needs
condition (B(cid:3)??FT )dE

to hold as well.

Proposition If direct identi(cid:12)ability does not hold for a1, dE1 = fa1; r1g, then the
choice of the recording mechanism R(B) = r1 in the experimental design de(cid:12)ned by
dE1 = fa1; r1g, is relevant for obtaining ‘adjusted’ identi(cid:12)ability.

When identi(cid:12)ability cannot be obtained directly from the data de(cid:12)ned by DE, iden-
ti(cid:12)ability can still hold for a particular con(cid:12)guration of R(B). Then, we say that the
causal e(cid:11)ect is identi(cid:12)able through an ‘adjustment’ procedure, and this leads to another
de(cid:12)nition.

De(cid:12)nition (Adjusted identi(cid:12)ability) The ‘causal’ e(cid:11)ect of T on Y is identi(cid:12)able through
an ‘adjustment’ procedure if

p(y j t0; FT = do(T = t0); DE = dE) = h(y; t0; B(cid:3) j DE = dE)

such that R(B(cid:3)
distributions of recorded variables under dE:

q ) = 1 for all B(cid:3)

q 2 B(cid:3) (cid:18) B and h is a function of known probabilistic

If we had a complete picture of the systems, then we could observe all background
variables B and their in(cid:13)uences and no unobserved or latent variables would exist.
Then, R(Bq) = 1 would be plausible for all q and we would always be able to (cid:12)nd
a combination R(B(cid:3)) such that p(y j FT = do(T = t0); R(B(cid:3))) would be identi(cid:12)able
through an adjustment procedure. However, our vision as experimenters willing to
collect data is much narrower and is restricted to a partial view in which not all back-
ground variables are accessible and not all settings r are accessible. Nevertheless, we
can still choose among di(cid:11)erent settings of R(B): The design network representation
permits us to evaluate identi(cid:12)ability for di(cid:11)erent choices of the recording mechanism
R(B). In consequence, it could assist the experimenter to choose among a possible set
of recording settings, r, in order to assist identi(cid:12)cation of the e(cid:11)ect of interest. In a
(cid:12)rst raw classi(cid:12)cation, recording mechanisms could be classi(cid:12)ed into those for which
adjusted identi(cid:12)ability holds (h exists) and those for which the e(cid:11)ect remains uniden-
ti(cid:12)able. Di(cid:11)erent recordings might have further consequences in the inference of causal
e(cid:11)ect; however, in terms of identi(cid:12)ability, the choice between two recordings that ensure
adjusted identi(cid:12)ability is irrelevant. Thus we have,

A.M. Madrigal

583

Proposition Let dE1 = fa; r1g and dE2 = fa; r2g be two experimental conditions such
that direct identi(cid:12)ability does not hold for the policy assignment mechanism de(cid:12)ned by
A = a, and where r1 and r2 represent recording mechanisms in which collections B(cid:3)
1 and
B(cid:3)
2 are recorded respectively. If functions h1 and h2 of known probabilistic distributions
can be found for both recordings r1 and r2, then dE1 and dE2 are said to be equivalent
for adjusted identi(cid:12)ability and the choice between recording mechanisms r1 and r2 is
irrelevant for identi(cid:12)ability.

In the case where neither h1 nor h2 can be found, the choice of r1 and r2 is also
irrelevant, but in this case both recordings produce non-identi(cid:12)ability. However, when
h1 exists, but h2 does not, then dE1 and dE2 do not share the same identi(cid:12)ability
status, as the target causal e(cid:11)ect of future intervention FT can be obtained through an
adjustment procedure for dE1 but it is not identi(cid:12)able under dE2.

When ‘adjustment’ is needed, some closed-forms for the function h have been given.
The ‘back-door’ criterion (Pearl 1993), the ‘front-door’ formula (Pearl 1995) and the ‘G-
computation’ formula (Robins 1986) are examples of criteria and formulae that imply
the use of background variables to obtain ‘adjusted’ estimates and are all particular
cases of functions h. A broader discussion of these criteria under di(cid:11)erent approaches
can be found in Pearl (2000), Dawid (2002) and Lauritzen (2001).

If we can assume we are in a situation represented by a system in which potential
confounders exist, pure random allocation will provide a data-generating mechanism
that ensures direct identi(cid:12)ability of the e(cid:11)ect of interest. In this case, we are performing
control via design of the potential confounders that might be a(cid:11)ecting the choice of
policy, like politicians’ preferences to bene(cid:12)t some particular communities. A generating
mechanism that can only provide identi(cid:12)ability through an ‘adjustment’ formulation
will correspond to a situation in which potential confounders have to be controlled via
analysis.

Even if functions h1 and h2 can be found for dE1 and dE2 and adjusted identi(cid:12)ability
can be obtained, further considerations are necessary when choosing an experiment. If
r1 and r2 are such that B1 (cid:18) B2 then dE1 will be generally preferred to dE2, as recording
a larger data set implies a more costly implementation and storage. In this sense, we
would like the set of recorded variables to be minimal, but su(cid:14)cient for identi(cid:12)ability.
De(cid:12)nitions of su(cid:14)cient sets have been made (see Lauritzen 2001; Dawid 2002; Pearl
2000). Functions h1 and h2 might be found for sets B1 6= B2 where neither of them is
a subset of the other. In any case, functions h1 and h2 might not have the same form
and particular estimates might not be equally e(cid:14)cient when derived from h1 than when
derived from h2; re(cid:13)ecting the loss of information associated with our restricted partial
views determined by r1 and r2. An example of this, for the front-door formulation, can
be found in Lauritzen (2001).

When direct or adjusted identi(cid:12)ability holds, the design dE is ignorable. However, as
Rubin (1978) notes, not all ignorable mechanisms can yield data from which inferences
for causal e(cid:11)ects are insensitive to prior speci(cid:12)cations. Direct identi(cid:12)ability gives a
situation where e(cid:11)ects are insensitive to the speci(cid:12)cation of prior distributions of the

584

Cluster Allocation Design Networks

data. However, this will not hold for adjusted identi(cid:12)ability where the causal e(cid:11)ect is
dependent on the prior distribution of background variables, P (B).

A.1 The positivity condition

In general, the set ft(cid:3)g of intervention-values assigned through AT is not necessarily the
same as the set of future-policy-values ft0g de(cid:12)ned by FT . In order to be able to evaluate
the causal e(cid:11)ects of intervention FT = do(T = t0), we need treatment t0 to be observed
under experimental conditions dE. So, we need p(t0 j B(cid:3); FT = ;; DE = dE) > 0. This
requires that treatment assignment mechanism AT includes t0 as one of its allocated
values. In other words, this requires that t0 2 ft(cid:3)g. In a prospective study, this condition
will usually hold. However, when data has been already collected, we might face the
case where t0 =2 ft(cid:3)g. In this case, we would only be able to use the data available if
we could make some parametric assumptions for p(Y j T; (cid:1)) before the policy e(cid:11)ects can
be identi(cid:12)ed. In general, if all the relevant information needed to evaluate the causal
e(cid:11)ect is encoded in a function (cid:28) ((cid:17)) of (cid:17), and the experimental data provides us with
information about (cid:21) ((cid:17)) ; it will su(cid:14)ce if (cid:28) ((cid:17)) (cid:18) (cid:21) ((cid:17)) : In this case, predictively,

p(y j FT = do(T = t0); dE) =Z(cid:17)

p(y j FT = do(T = t0); (cid:28) ((cid:17)))p((cid:21) ((cid:17)) j (cid:1)dE ; dE)d(cid:17):

If di(cid:11)erent policies represent categorical variables, this could be di(cid:14)cult.
In the FS
example, imagine the two supplements provided in the experiment through assignment
AT = do(T = t(cid:3)) are from di(cid:11)erent brands, say brand A (t(cid:3)
B), and
the future policy consists of providing a food supplement from brand C (t0
C ). The data
available, no matter how the actual assignment was made (random or not), will hardly
be useful to conclude anything about the e(cid:11)ect of food supplement C. However, imagine,
that we have a measure in terms of the calorie intake that each supplement provides
and that t(cid:3)
B = 300 kcal, and we know that supplement C has 200
kcal, then if we are ready to assume that (cid:17) contains a summary of the e(cid:11)ect on weight
per each increase of one kcal, then we would be able to estimate its e(cid:11)ect.

A = 100 kcal and t(cid:3)

A) and brand B (t(cid:3)

A.2 Choice of experimental design

The problem of choosing an experiment has been set in Bayesian decision theory using
decision trees (see Lindley 1971; Bernardo and Smith 1994). A DN could be viewed as
its corresponding ID, allowing us to represent in(cid:13)uences between decisions and random
nodes. Optimality can be de(cid:12)ned in various ways, and qualities for the distributions
of estimators, such as minimum variance, are desirable (see Chaloner and Verdinelli
1995). Here we focus on the isolation of the target causal e(cid:11)ect and thus on its identi-
(cid:12)cation. The e(cid:14)cacy of experimental design interventions DE could then be measured
in terms of making the (causal) e(cid:11)ects of FT = do(T = t0) identi(cid:12)able and then two
(or more) experiments can be compared in these terms, and among the experimental
decisions DE we choose the one with highest utility.
‘Pure’ (i.e. non-strati(cid:12)ed) indi-
vidual random allocation contrasted with the ‘no experiment’ choice (i.e. observational

A.M. Madrigal

585

data) is used to introduce this procedure. When the policy assignment is done through
random allocation, two control actions are performed: randomisation and intervention.
So treatment t(cid:3) is done, AT = do(T = t(cid:3)); according to a probability distribution (cid:18)(cid:3)
T
totally (cid:12)xed and controlled by the experimenter through A(cid:18)T = do((cid:18)T = (cid:18)(cid:3)
T ): Node A
might be expanded to show explicitly the mechanisms underlying the assignment and
the new independencies that might be introduced. This expansion involves parameter
and intervention nodes that are included in an augmented-extended design network.

(cid:5)
(cid:5)

(cid:7) (cid:5)(cid:8)
(cid:7) (cid:5)(cid:8)

(cid:1) (cid:1)q(cid:1)(cid:2)(cid:2)(cid:2)(cid:2)(cid:3)(cid:2) (cid:3)(cid:4)
(cid:1) (cid:1)q(cid:1)(cid:2)(cid:2)(cid:2)(cid:2)(cid:3)(cid:2) (cid:3)(cid:4)

(cid:5)(cid:6)
(cid:5)(cid:6)

(cid:7) (cid:5)(cid:8)
(cid:7) (cid:5)(cid:8)

q(cid:1) (cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:5)(cid:11)(cid:10)(cid:9)(cid:12)(cid:9)(cid:11)(cid:13)(cid:10)(cid:3)(cid:9)(cid:13)(cid:3)(cid:12)(cid:14)(cid:7)(cid:3)(cid:15)(cid:14)(cid:11)(cid:9)(cid:15)(cid:7)(cid:3)(cid:11)(cid:16)(cid:3)(cid:17)(cid:18)(cid:3)(cid:9)(cid:10)(cid:3)(cid:19)(cid:7)(cid:16)(cid:12)(cid:3)(cid:12)(cid:11)(cid:3)
q(cid:1) (cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:5)(cid:11)(cid:10)(cid:9)(cid:12)(cid:9)(cid:11)(cid:13)(cid:10)(cid:3)(cid:9)(cid:13)(cid:3)(cid:12)(cid:14)(cid:7)(cid:3)(cid:15)(cid:14)(cid:11)(cid:9)(cid:15)(cid:7)(cid:3)(cid:11)(cid:16)(cid:3)(cid:17)(cid:18)(cid:3)(cid:9)(cid:10)(cid:3)(cid:19)(cid:7)(cid:16)(cid:12)(cid:3)(cid:12)(cid:11)(cid:3)
(cid:21) (cid:6)(cid:22) (cid:3)(cid:3)(cid:23)(cid:13)(cid:21) (cid:12)(cid:24) (cid:6)(cid:21) (cid:19)(cid:19)(cid:22) (cid:25) (cid:21) (cid:15)(cid:15)(cid:11)(cid:6)(cid:8)(cid:9)(cid:13)(cid:26) (cid:3)(cid:12)(cid:11)(cid:3)q(cid:1) (cid:2)(cid:5)(cid:4)(cid:1)(cid:1) (cid:16)(cid:5)(cid:17)(cid:18)(cid:5)
(cid:21) (cid:6)(cid:22) (cid:3)(cid:3)(cid:23)(cid:13)(cid:21) (cid:12)(cid:24) (cid:6)(cid:21) (cid:19)(cid:19)(cid:22) (cid:25) (cid:21) (cid:15)(cid:15)(cid:11)(cid:6)(cid:8)(cid:9)(cid:13)(cid:26) (cid:3)(cid:12)(cid:11)(cid:3)q(cid:1) (cid:2)(cid:5)(cid:4)(cid:1)(cid:1) (cid:16)(cid:5)(cid:17)(cid:18)(cid:5)

(cid:1)
(cid:1)

(cid:5)(cid:9)
(cid:5)(cid:9)
(cid:28) (cid:10)(cid:3)(cid:19)(cid:9)(cid:13)(cid:28) (cid:3)(cid:4)(cid:6)(cid:18)(cid:3)(cid:21) (cid:13)(cid:8)(cid:3)(cid:9)(cid:13)(cid:12)(cid:6)(cid:11)(cid:8)(cid:24) (cid:15)(cid:7)(cid:10)(cid:3)(cid:15)(cid:11)(cid:13)(cid:8)(cid:9)(cid:12)(cid:9)(cid:11)(cid:13)(cid:21) (cid:19)(cid:3)
(cid:28) (cid:10)(cid:3)(cid:19)(cid:9)(cid:13)(cid:28) (cid:3)(cid:4)(cid:6)(cid:18)(cid:3)(cid:21) (cid:13)(cid:8)(cid:3)(cid:9)(cid:13)(cid:12)(cid:6)(cid:11)(cid:8)(cid:24) (cid:15)(cid:7)(cid:10)(cid:3)(cid:15)(cid:11)(cid:13)(cid:8)(cid:9)(cid:12)(cid:9)(cid:11)(cid:13)(cid:21) (cid:19)(cid:3)

(cid:24) (cid:5)(cid:4)q(cid:1) (cid:2) q(cid:3)
(cid:24) (cid:5)(cid:4)q(cid:1) (cid:2) q(cid:3)

(cid:1) (cid:18) (cid:3)
(cid:1) (cid:18) (cid:3)

(cid:21) (cid:5)(cid:22)
(cid:21) (cid:5)(cid:22)

(cid:5)(cid:11)
(cid:5)(cid:11)

(cid:1)(cid:27)(cid:6)(cid:7)(cid:21)
(cid:1)(cid:27)(cid:6)(cid:7)(cid:21)
(cid:9)(cid:13)(cid:8)(cid:7)(cid:5)(cid:7)(cid:13)(cid:8)(cid:7)(cid:13)(cid:15)(cid:9)(cid:7)(cid:10)(cid:29)
(cid:9)(cid:13)(cid:8)(cid:7)(cid:5)(cid:7)(cid:13)(cid:8)(cid:7)(cid:13)(cid:15)(cid:9)(cid:7)(cid:10)(cid:29)

(cid:2)
(cid:2)

(cid:27)
(cid:27)

(cid:17)
(cid:17)

(cid:30)
(cid:30)

(cid:31)(cid:1)   (cid:3)(cid:17)(cid:3)
(cid:31)(cid:1)   (cid:3)(cid:17)(cid:3)

(cid:3)q(cid:1)
(cid:3)q(cid:1)

(cid:4)(cid:1)
(cid:4)(cid:1)

q(cid:1)
q(cid:1)
(cid:3)(cid:1)
(cid:3)(cid:1)

(cid:1)
(cid:1)

Figure 9: Augmented - Extended DN for random allocation

Experimental actions ‘do’ parameter nodes. Random allocation breaks the link
(r) and therefore two experimental structures arise from this choice. For each design
strategy d(cid:3)
E (cid:26) DE taken we can obtain an experimental DN from which independencies
can be easily read. These experimental DNs de(cid:12)ne the data structure or data pattern.

(cid:24)
(cid:24)

(cid:20)(cid:25)(cid:24)(cid:26)
(cid:20)(cid:25)(cid:24)(cid:26)

(cid:24)
(cid:24)

(cid:20)(cid:25)(cid:24)(cid:26)
(cid:20)(cid:25)(cid:24)(cid:26)

(cid:3)(cid:1)
(cid:3)(cid:1)

(cid:1)
(cid:1)
(cid:2)
(cid:2)
(cid:7) (cid:8)(cid:9)(cid:10)(cid:7) (cid:11)(cid:12)(cid:13)(cid:14)(cid:15)(cid:16) (cid:17)(cid:15)
(cid:7) (cid:8)(cid:9)(cid:10)(cid:7) (cid:11)(cid:12)(cid:13)(cid:14)(cid:15)(cid:16) (cid:17)(cid:15)
(cid:18)(cid:19)(cid:8)(cid:15)(cid:20)(cid:13)(cid:11)(cid:21)(cid:19)(cid:10)(cid:15)(cid:22)(cid:14)(cid:14)(cid:19)(cid:23)(cid:13)(cid:12)(cid:9)(cid:19)(cid:11)
(cid:18)(cid:19)(cid:8)(cid:15)(cid:20)(cid:13)(cid:11)(cid:21)(cid:19)(cid:10)(cid:15)(cid:22)(cid:14)(cid:14)(cid:19)(cid:23)(cid:13)(cid:12)(cid:9)(cid:19)(cid:11)

(cid:3)(cid:1)
(cid:3)(cid:1)

(cid:1)
(cid:1)
(cid:2)
(cid:2)
(cid:7) (cid:8)(cid:9)(cid:10)(cid:7) (cid:11)(cid:12)(cid:13)(cid:14)(cid:15)(cid:16) (cid:17)(cid:15)
(cid:7) (cid:8)(cid:9)(cid:10)(cid:7) (cid:11)(cid:12)(cid:13)(cid:14)(cid:15)(cid:16) (cid:17)(cid:15)

(cid:18)(cid:19)(cid:8)(cid:15)(cid:17)(cid:19)(cid:15)(cid:20)(cid:13)(cid:11)(cid:21)(cid:19)(cid:10)(cid:15)(cid:22)(cid:14)(cid:14)(cid:19)(cid:23)(cid:13)(cid:12)(cid:9)(cid:19)(cid:11)
(cid:18)(cid:19)(cid:8)(cid:15)(cid:17)(cid:19)(cid:15)(cid:20)(cid:13)(cid:11)(cid:21)(cid:19)(cid:10)(cid:15)(cid:22)(cid:14)(cid:14)(cid:19)(cid:23)(cid:13)(cid:12)(cid:9)(cid:19)(cid:11)

Figure 10: Experimental DN

Imagine we establish that the utilities associated with obtaining direct identi(cid:12)ability,
adjusted identi(cid:12)ability and unidenti(cid:12)ability are given by UD, UA and UU respectively.
Then for the pure random allocation vs observational case, the four possible combi-
nations of (A; R(B)) are shown in Table 3. Both experimental decisions that include
random allocation, A(cid:18)T = do((cid:18)T = (cid:18)(cid:3)
T ), have the same utility associated in terms of
identi(cid:12)ability and are equivalent in these terms. However, performing an experiment
(randomising and/or recording) will typically involve an associated cost that is not in-
cluded here. The fact that UD 6= UA (and actually we consider UD > UA) is due
to the fact that the recording of B will increase the cost of the experiment and that
p(y j FT = t0; dE = 3) is sensitive to the speci(cid:12)cation of prior distributions of the data.
The choice of (cid:18)(cid:3)
T could have an e(cid:11)ect on the e(cid:14)cacy of the estimators as it could af-

(cid:9)
(cid:10)
(cid:11)
(cid:7)
(cid:12)
(cid:13)
(cid:14)
(cid:9)
(cid:15)
(cid:13)
(cid:7)
(cid:10)
(cid:20)
(cid:8)
(cid:9)
(cid:10)
(cid:11)
(cid:7)
(cid:12)
(cid:19)
(cid:19)
(cid:7)
(cid:20)
(cid:9)
(cid:15)
(cid:13)
(cid:7)
(cid:10)
(cid:23)
(cid:7)
(cid:13)
(cid:10)
(cid:6)
(cid:7)
(cid:9)
(cid:8)
(cid:9)
(cid:10)
(cid:11)
(cid:7)
(cid:12)
(cid:13)
(cid:14)
(cid:9)
(cid:15)
(cid:13)
(cid:7)
(cid:10)
(cid:20)
(cid:8)
(cid:9)
(cid:10)
(cid:11)
(cid:7)
(cid:12)
(cid:19)
(cid:19)
(cid:7)
(cid:20)
(cid:9)
(cid:15)
(cid:13)
(cid:7)
(cid:10)
(cid:23)
(cid:7)
(cid:13)
(cid:10)
(cid:6)
(cid:7)
(cid:9)
(cid:8)
(cid:4)
(cid:5)
(cid:6)
(cid:4)
(cid:5)
(cid:6)
(cid:4)
(cid:5)
(cid:6)
(cid:4)
(cid:5)
(cid:6)
586

Cluster Allocation Design Networks

fect the balance of the experiment, but the actual value (cid:18)(cid:3)
independence structure and the identi(cid:12)ability status derived from it.

T does not a(cid:11)ect the graph

Experimental Decisions
R(B)

A(cid:18)T

DE

1
2
3
4

random
random

;
;

1
0
1
0

Design Consequence
p(y j F T = t0; dE)
direct identi(cid:12)ability
direct identi(cid:12)ability

adjusted identi(cid:12)ability

No identi(cid:12)able

Utility(dE)

U
UD
UD
UA
UU

Table 3: Choice of experimental design for pure random example

A.3 An in(cid:13)uence diagram for policy analysis

Figure 11 shows an in(cid:13)uence diagram of the (simpli(cid:12)ed version of the) complete system
for policy analysis. As before, the policy variable is denoted by a decision node T
that has been augmented to make explicit policy intervention FT . When the policy is
de(cid:12)ned through policy intervention decisions DT , it can contain a collection of actions
G that are triggered when intervention FT = do(T = t0) takes place. Actions G can be
contingent on a set of observed variables Z and are children nodes of T . The de(cid:12)nition of
possible structures and correspondent formulae for the calculation of the overall e(cid:11)ect
of intervention FT in Y through actions G have been discussed in Madrigal (2004).
The policy assignment mechanisms are contained in decision node A; which could be
in(cid:13)uenced by some background variables B. Both, the policy assignment mechanisms,
A; and the recording mechanisms of B; R(B), are de(cid:12)ned as part of the experimental
decisions DE.

(cid:4)

(cid:2)

(cid:1)

(cid:3)

(cid:5)

(cid:1)

(cid:2)

Figure 11: Complete ID for policy analysis

This simple structure shows how the two sets of decisions (namely, policy intervention
decisions DT and experimental decisions DE) could be represented in the same graph. A
more realistic graph should include some possible links between the background variables
B and the variables Z in which actions G are contingent on, and possibly some type of
in(cid:13)uence of Z in the policy assignment mechanism. It is important to use the policy
makers’ expertise and knowledge to be able to represent in the in(cid:13)uence diagram a most
accurate version of the ‘real’ system with all possible in(cid:13)uences. This will assist our

o
A.M. Madrigal

587

causal inferences conclusions and help the choice of actions.

References
Bernardo, J. M. and Smith, A. F. M. (1994). Bayesian Theory. John Wiley & Sons.

584

Bollen, K. A. (1989). Structural Equations with Latent Variables. John Wiley & Sons.

578

Chaloner, K. and Verdinelli, I. (1995). \Bayesian Experimental Design: A Review."

Statistical Science, 10: 273{304. 562, 584

Cowell, R. G., Dawid, A. P., Lauritzen, S. L., and Spiegelhalter, D. J. (1999). Proba-

bilistic Networks and Expert Systems. New York, NY: Springer-Verlag. 560

Dawid, A. P. (1979). \Conditional Independence in Statistical Theory." Journal os the

Royal Statistical Society (series B), 41: 1{31. 561

| (2000). \Causal Inference Without Counterfactuals." Journal of the American Sta-

tistical Association, 95(450): 407{424 (C/R: p424{448). 559, 575

| (2002). \In(cid:13)uence diagrams for causal modelling and inference." International Sta-

tistical Review, 70(2): 161{189. 557, 558, 559, 560, 561, 563, 565, 579, 580, 583

Hayes, R. J., Alexander, N. D. E., Bennett, S., and Cousens, S. N. (2000). \Design
and Analysis Issues in Cluster-randomized Trials of Interventions against Infectious
Diseases." Statistical Methods in Medical Research, 9(2): 95{116. 573, 574

Heckerman, D. and Shachter, R. (2003). \Discussion in Pearl, J. Statistics and Causal

Inference: A Review." Test., 12(2): 327{331. 560

Howard, R. A. and Matheson, J. E. (1981). \In(cid:13)uence Diagrams." Principles and

Applications of Decision Analysis. Menlo Park, CA. 559

Jensen, F. V. (2001). Bayesian Networks and Decision Graphs.. New York: Springer.

559

Koepsell, T. P. (1998). \Epidemiologic Issues in the Design of Community Intervention
Trials." In Brownson, R. C. and Petitti, D. B. (eds.), Applied Epidemiology, Theory
and Practice. New York: Oxford University Press. 573

Lauritzen, S. L. (2001). \Causal Inference from Graphical Models." In Barndor(cid:11)-
Nielsen, O. E., Cox, D. R., and Kl(cid:127)uppelberg, C. (eds.), Complex stochastic systems,
63{107. Chapman & Hall Ltd. 558, 561, 583

Lindley, D. V. (1971). Making Decisions. Chichester: Wiley-Interscience. 584

588

Cluster Allocation Design Networks

Longini, J., Ira M., Sagatelian, K., Rida, W. N., and Halloran, M. E. (1998). \Optimal
Vaccine Trial Design When Estimating Vaccine E(cid:14)cacy for Susceptibility and Infec-
tiousness from Multiple Populations." Statistics in Medicine, 17: 1121{1136 (Corr:
1999 V18 p890). 574

Madrigal, A. M. (2004). \Evaluation of Policy Interventions under Experimental Condi-
tions Using Bayesian In(cid:13)uence Diagrams." Ph.D. thesis, University of Warwick,UK.
563, 586

| (2005). \Design Networks, Policy Interventions and Causal Inference." Technical

Report 448, University of Warwick,UK. 568

Madrigal, A. M. and Smith, J. Q. (2004). \Causal Identi(cid:12)cation in Design Networks." In
R. Monroy, E. E. (ed.), MICAI 2004: Advances in Arti(cid:12)cial Intelligence, (Conference
Proceedings), 517{526. LNAI 2972, Springer-Verlag. 557, 559, 569

Oliver, R. M. and Smith, J. Q. (1990). In(cid:13)uence Diagrams, Belief Nets and Decision

Analysis. John Wiley & Sons. 559

Pearl, J. (1993). \Comment on \Graphical Models"." Statistical Science, 8: 266{269.

559, 561, 583

| (1995). \Causal Diagrams for Empirical Research." Biometrika, 82: 669{688 (Disc:

p688{710). 559, 583

| (2000). Casuality: Models, Reasoning, and Inference. Cambridge University Press.

558, 559, 561, 578, 583

Robins, J. (1986). \A New Approach to Causal Inference in Mortality Studies with a
Sustained Exposure Period - Application to Control of the Healthy Worker Survivor
E(cid:11)ect." Mathematical Modeling, 7: 1393{1512. 559, 583

Rosenbaum, P. (2002). Observational Studies.. New York: Springer Verlag. 562

Rubin, D. B. (1978). \Bayesian Inference for Causal E(cid:11)ects: The Role of Randomiza-

tion." The Annals of Statistics, 6: 34{58. 558, 583

| (1980). \Comments on \Randomization Analysis of Experimental Data: The Fisher
Randomization Test"." Journal of the American Statistical Association, 75: 591{593.
575

Spiegelhalter, D. J. (2001). \Bayesian Methods for Cluster Randomised Trials with

Continuous Responses." Statistics in Medicine, 20: 435{452. 559, 571, 575, 580

Spirtes, P., Glymour, C., and Scheines, R. (2000). Causation, Prediction and Search.

Cambridge, MA: MIT Press. 559

Turner, R. M., Omar, R. Z., and Thompson, S. G. (2001). \Bayesian Methods of
Analysis for Cluster Randomised Trials with Binary Outcome Data." Statistics in
Medicine, 20: 453{472. 571

A.M. Madrigal

589

Wu, C.-F. and Hamada, M. (2000). Experiments: Planning, Analysis, and Parameter

Design Optimization. John Wiley & Sons. 562

Acknowledgments

This work was partially funded by CONACYT. The author thanks PROGRESA for giving
access to databases for this research.

590

Cluster Allocation Design Networks

Bayesian Analysis (2007)

2, Number 3, pp. 591{610

Bayesian Hierarchical Multiresolution Hazard

Model for the Study of Time-Dependent

Failure Patterns in Early Stage Breast Cancer

Vanja Duki(cid:19)c(cid:3) and James Dignamy

Abstract. The multiresolution estimator, developed originally in engineering ap-
plications as a wavelet-based method for density estimation, has been recently ex-
tended and adapted for estimation of hazard functions (Bouman et al. 2005, 2007).
Using the multiresolution hazard (MRH) estimator in the Bayesian framework, we
are able to incorporate any a priori desired shape and amount of smoothness in
the hazard function. The MRH method’s main appeal is in its relatively simple
estimation and inference procedures, making it possible to obtain simultaneous
con(cid:12)dence bands on the hazard function over the entire time span of interest.
Moreover, these con(cid:12)dence bands properly re(cid:13)ect the multiple sources of uncer-
tainty, such as multiple centers or heterogeneity in the patient population. Also,
rather than the commonly employed approach of estimating covariate e(cid:11)ects and
the hazard function separately, the Bayesian MRH method estimates all of these
parameters jointly, thus resulting in properly adjusted inference about any of the
quantities.

In this paper, we extend the previously proposed MRH methods (Bouman et al.
2005, 2007) into the hierarchical multiresolution hazard setting (HMRH), to ac-
commodate the case of separate hazard rate functions within each of several strata
as well as some common covariate e(cid:11)ects across all strata while accounting for
within-stratum correlation. We apply this method to examine patterns of tu-
mor recurrence after treatment for early stage breast cancer, using data from
two large-scale randomized clinical trials that have substantially in(cid:13)uenced breast
cancer treatment standards. We implement the proposed model to estimate the
recurrence hazard and explore how the shape di(cid:11)ers between patients grouped
by a key tumor characteristic (estrogen receptor status) and treatment types, af-
ter adjusting for other important patient characteristics such as age, tumor size
and progesterone level. We also comment on whether the hazards exhibit non-
monotonic patterns consistent with recent hypotheses suggesting multiple hazard
change-points at speci(cid:12)c time landmarks.

Keywords: Multiresolution models, Bayesian survival analysis, hazard esti-

mation

(cid:3)Department

of

Health

Studies,

University

mailto:vdukic@health.bsd.uchicago.edu

yDepartment

of

Health

Studies,

University

of

of

Chicago,

Chicago,

Chicago,

Chicago,

IL,

IL,

mailto:jdigman@health.bsd.uchicago.edu

c(cid:13) 2007 International Society for Bayesian Analysis

ba0007

592

Hierarchical Multiresolution Hazard Models

1 Introduction

In survival analysis, because the hazard function h(t) often exhibits unstable behavior
making it di(cid:14)cult to reliably discern patterns of change or make comparisons between
groups, aggregates of the hazard over time are more frequently used. The cumulative
hazard H(t), or more commonly, functions of H(t) such as survival or cumulative in-
cidence functions, are used for summary and inference on failure risk. While useful
and easy to interpret for most purposes, these summaries can partially obscure im-
portant patterns in the hazard of failure over time. Alternatively, examination of the
hazard function itself in detail can reveal important properties of the failure process
(Aalen and Gjessing 2001). Generally, some type of smoothed estimate of the hazard
function is used to characterize its shape, which is indicative of how failure risk (in the
population) changes with respect to some time origin. While a variety of approaches
toward hazard estimation have been proposed, methodological challenges remain for
both estimation and associated statistical inferential procedures. In particular, (cid:13)exi-
ble estimation and modeling approaches are needed, because in contrast to the hazard
functional form in most parametric survival models, the hazard may exhibit complex
non-unimodal shape with ‘change-points’ that may reveal important information about
the process under study.

In this article, we investigate the hazard of disease recurrence among women treated
for breast cancer and followed over several years. The data originate from large multi-
center randomized clinical trials evaluating the e(cid:11)ect of hormonal or cytotoxic chemother-
apy treatment agents administered after surgery (referred to generally as adjuvant ther-
apy) in women with early stage breast cancer. As we describe in the next section,
there is considerable interest in the patterns of recurrence hazards after breast cancer
diagnosis and initial treatment, both to gain biological insights and to better manage
the disease in a clinical setting. We accommodate the biologically plausible situation
whereby di(cid:11)erent subgroups of women with speci(cid:12)c disease features may have distinct
functional forms of failure hazard that are non-proportional to each other, by construct-
ing a joint model for all separate subgroup hazards, while keeping the e(cid:11)ects of some
factors (such as age or tumor size) common across all strata and proportional within
each stratum.

To model the recurrence hazards, we apply the semiparametric multi-resolution haz-

ard (MRH) estimator recently presented in work by Bouman and colleagues
(Bouman et al. 2005, 2007). Employing a piece-wise constant prior for the hazard rate
which is constructed in a tree-based and self-consistent manner, the MRH approach
permits (cid:13)exible modeling with the ability to incorporate a variety of a priori assump-
tions about the shape and smoothness of hazard functions in each of several de(cid:12)ned
strata. Furthermore, Bayesian modeling allows us to easily address speci(cid:12)c hypotheses
concerning the timing of peaks in the risk of failure and how these may di(cid:11)er with
respect to key biologic and clinical parameters in breast cancer.

In the next section, we describe some of the key questions of interest in modeling of
the breast cancer recurrence hazard, and the data source for this study. In Section 3 we
review the basics of the multiresolution hazard model, and present the extension to the

V. Duki(cid:19)c and J. Dignam

593

hierarchical multiresolution (HMRH) setting. In Section 4 we provide technical details
of the Markov chain Monte Carlo (MCMC) model implementation. Section 5 presents
the analysis. We conclude with a discussion of the (cid:12)ndings in relation to the broader
literature on breast cancer hazards as well as plans for future work on this problem.

2 Recurrence Risk after Surgery for Early Stage Breast

Cancer

Over the past few decades, there has been appreciable progress in therapeutic strategies
for early stage (i.e., localized and operable, as opposed to metastatic) breast cancer,
with a well-developed array of treatment options. Due to increased screening vigilance
and disease awareness, currently over 75% of women diagnosed have early stage tumors.
Despite this progress, the clinical course of breast cancer after diagnosis remains hetero-
geneous from patient to patient and thus highly unpredictable for individuals. Thus, a
signi(cid:12)cant clinical challenge is deciding which and how much adjuvant therapy is needed,
and determining the magnitude of recurrence risk over extended post-treatment follow-
up. Characteristics that prospectively identify which women are at greater or lesser
risk of treatment failure are needed to aid in individually tailoring therapy for optimal
disease management. Answers may also lie partly in gaining a better understanding of
the intermediate and long-term clinical course of the disease, identifying patterns that
can portend time periods of increased recurrence risk.

Apparent patterns in breast cancer recurrence hazard are readily observable from
large cohorts of patients systematically followed over time. It is well known that risk of
recurrence remains elevated for a long period of time after initial diagnosis and tumor
removal, and there is longstanding interest in the the prospect of \cure" after su(cid:14)cient
time tumor-free has been achieved (Berg and Robbins 1966). Some long-term follow-up
studies have suggested that a (cid:12)nite but lengthy\dormancy period" exists (possibly over
20 years), whereby tumor recurrences may still appear (Gordon 1990; Demicheli et al.
1996; Karrison et al. 1999). Studies examining the shape of the recurrence hazard con-
sistently show a sharp peak 12-24 months after initial diagnosis and treatment, fol-
lowed by a decline over time, although the hazard remains persistently elevated relative
to individuals in the population never having had breast cancer (Saphner et al. 1996;
Hess et al. 2003). This pattern stands in sharp contrast to the recurrence hazard for
several other major cancers (e.g., colon, lung), where most recurrences appear within
the (cid:12)rst (cid:12)ve years after discovery and removal of the primary tumor, followed by a
period in which the hazard of recurrence and death from the disease resemble that of
the population at large.

2.1 Data: Randomized Clinical Trials for Early Stage Breast Cancer

The National Surgical Adjuvant Breast and Bowel Project (NSABP) is a U.S. National
Cancer Institute sponsored and funded multi-center cancer clinical trials group that
has investigated a spectrum of treatments for breast and colorectal cancers since the

594

Hierarchical Multiresolution Hazard Models

late 1950s. The group consists of more than 5,000 participating physicians, nurses,
and other research specialists located at over 1,000 medical centers, university and
community hospitals, oncology practice groups, and health maintenance organizations
in North America. The group has enrolled more than 60,000 women and men in clinical
trials for breast and colorectal cancer.

While use of systemic adjuvant therapy began in the mid 1970s for women with
tumors that had spread to the axillary lymph nodes, those with so-called lymph node
negative breast cancer continued to be treated by surgery only, as they were considered
at su(cid:14)ciently favorable prognosis so as not to require further interventions. Because
a signi(cid:12)cant fraction of such women do su(cid:11)er recurrence and eventually die from the
disease, beginning in the 1980’s the NSABP began a series of clinical trials among
patients with axillary lymph node negative breast cancer. These trials were serially
designed, beginning with comparisons surgery alone to post-surgical hormonal or cyto-
toxic adjuvant therapy regimens, and following bene(cid:12)ts seen for the latter, subsequently
comparing di(cid:11)erent adjuvant therapy regimens. These studies provide a unique view to
the long-term prognosis of women with early stage breast cancer, and are an ideal data
source for examining factors in(cid:13)uencing the hazard of disease recurrence.

A key determinant of both expected prognosis and potential choice of adjuvant
therapy type is the presence and quantity of estrogen receptors (ER) on the tumor. From
1982-1988, patients were accrued according to ER status into one of two trials conducted
in parallel: Protocol B-13 randomized 760 patients with ER-negative (ER-) tumors to
no further treatment after surgery (384) or to 12 cycles of cytotoxic chemotherapy
treatment with methotrexate and 5-(cid:13)uorouracil (376). Protocol B-14 randomized 2,892
patients with ER-positive (ER+) tumors to placebo (1,453) or the estrogen antagonist
drug tamoxifen (1,439) after surgery. Primary (cid:12)ndings were (cid:12)rst obtained in 1989,
showing a signi(cid:12)cant reduction in breast cancer recurrence risk for patients receiving the
adjuvant therapy regimens (Fisher et al. 1989b,a). Longer follow-up eventually revealed
a survival advantage for those who received adjuvant therapy (Fisher et al. 1996b,a).
Further details of the trial designs and (cid:12)ndings can be found in the published primary
reports. Follow-up continues to date, with mean follow-up of over 15 years and over 900
recurrence events observed.

Primary endpoints for the trial were overall survival, de(cid:12)ned as time from surgery
to death from any cause, and disease-free survival, de(cid:12)ned as time to (cid:12)rst breast cancer
recurrence at any local, regional, or distant anatomic site, occurrence of a tumor in
the opposite breast, occurrence of other second primary cancers, or death prior to
these events (that is, time to (cid:12)rst event of any kind).
In this study, we model the
cause-speci(cid:12)c hazard for breast recurrence, de(cid:12)ned as time to breast cancer recurrence,
treating the other event types as censored observations. We do this because modeling
the cause-speci(cid:12)c hazard for breast cancer events only may have more clinical relevance,
and furthermore, with the exception of endometrial cancer, which occurs in less than
1.5% of patients but is more frequent among women taking tamoxifen, hazard rates
for non-breast cancer events are essentially equal between the two treatment groups.
As in previous studies modeling these data (Fisher et al. 1989c; Bryant et al. 1997), we
examine tumor size, tumor progesterone receptor level, menopausal status, and age as

V. Duki(cid:19)c and J. Dignam

595

covariates potentially associated with the recurrence hazard.

3 The Multiresolution Hazard Model

In this section, we review the multiresolution approach to modeling hazard rate in a
semiparametric Bayesian context developed and described in more detail in Bouman et
al. (Bouman et al. 2005, 2007). As will be explained below, the method relies on the
clever tree-based construction of the prior for the hazard rate, that ultimately yields a
resolution-invariant and self-consistent prior for an arbitrarily (cid:12)ne piece-wise constant
approximation to the hazard rate. The parameterization of the prior tree uniquely
de(cid:12)nes not only the prior expectations of the hazard rate in each of the intervals, but
it also determines the amount of correlation and smoothness in hazard between the
intervals.

3.1 Approaches to Hazard Modeling

One of the most common approaches to assessing the impact of factors on the hazard of
failure is the Cox proportional hazard model. In this model individual covariates X af-
fect baseline hazard hbase(t) via exp(X0(cid:12)) (Cox 1972). This approach is readily adapted
to modeling the cause-speci(cid:12)c hazard (Prentice et al. 1978). In the typical application
of the Cox model, covariate e(cid:11)ects for the relative hazard are the primary focus, with
the baseline hazard function treated as a nuisance parameter. As we have indicated,
however, interest in our particular study here lies precisely in the estimation of the
hazard functions. Speci(cid:12)cally, we are interested in the estimation of a separate hazard
rate function within certain strata based on treatment type and tumor characteristics,
so that we might compare the shape of the hazards, while simultaneously estimating
and performing inference about other covariate e(cid:11)ects that are reasonably assumed to
be common over strata.

Non-parametric methods for extracting hazard function estimates have been pro-
posed for the Cox model (Gray 1990, 1992), primarily for the purpose of performing
model diagnostics (e.g., changes in the e(cid:11)ects of covariates over time) and correct func-
tional forms for covariates in relation to failure hazard. A more general hazard regression
approach that involves partitioning the time axis more speci(cid:12)cally focused on hazard
estimation within covariate strata (Gray 1996). There are many other approaches and
variations (see Andersen et al. (1993) for detailed review), many of which provide esti-
mates of functionals (S(t), etc.) after the model is (cid:12)t, but most still focus on covariates
and model checking, rather than on e(cid:14)cient hazard function estimation per se. We
discuss the properties and justify the MRH approach to hazard modeling more in the
next section.

596

Hierarchical Multiresolution Hazard Models

3.2 Multiresolution Model for the Baseline Hazard

Multiscale models for estimation of a discretized intensity function were developed for
astrophysics applications by Kolaczyk (Kolaczyk 1999; Nowak and Kolaczyk 2000). De-
tails of how this methodology has been adapted to hazard estimation are summarized
in the following, while a more extensive discussion of its theoretical properties can be
found in Bouman et al. (2005, 2007).

In summary, the multiresolution hazard (MRH) approach yields an estimate of the
baseline (i.e., estimate from which covariate-speci(cid:12)c curves can be generated) survival
function based on the multiresolution baseline hazard estimate. It consists of (cid:12)rst choos-
ing the \time resolution" { a set of time points ft0; t1; :::; tJ g { and then estimating the
underlying baseline survival at those points, Sbase(tj ), based on the the cumulative haz-
hbase(s)ds,
where hbase(t) represents the baseline hazard rate at time t. For those times t such that
tj(cid:0)1 < t < tj; j = 1; : : : ; J , a piecewise-constant hazard rate is assumed.

ard Hbase(t) and its discrete increments dj (cid:17) Hbase(tj ) (cid:0) Hbase(tj(cid:0)1) =R tj

tj(cid:0)1

The MRH model is thus a semiparametric model which is able to estimate the
baseline hazard rate hbase(t) at the resolution times tj , along with covariate e(cid:11)ects (cid:12).
For convenience, we set the number of time intervals J equal to 2M , where M > 0. In
general, these intervals need not be of equal length { one can choose any resolution,
though in practice we would recommend one such that there are multiple failure times
observed in almost all intervals. Furthermore, the resolution should be chosen so that the
average (or total) hazard rates within its intervals are clinically meaningful. However,
in cases when prior information and clinical input about the resolution are vague, the
optimal number of intervals could be chosen via model selection criteria, such as the DIC
(Spiegelhalter et al. 2002). Note that the failure times after tJ become right-censored
at tJ ; hence, J should also be chosen so that a relatively small fraction of failure times
is censored as a result.

After (cid:12)xing the resolution, the discretized hazard is modeled in a way that allows
us to incorporate the prior belief about the shape and smoothness of the true under-
lying hazard function. Following the notation in Bouman et al. (Bouman et al. 2005,
2007), we denote the total cumulative baseline hazard H(tJ ) as H0;0, and the haz-
ard increments d1 as HM;0, d2 as HM;1; : : : ; and dJ as HM;2M (cid:0)1. We then build the
multiresolution hazard tree by recursively de(cid:12)ning Hm(cid:0)1;p = Hm;2p + Hm;2p+1, for
m = 1; : : : ; M , and p = 0; : : : ; 2m(cid:0)1 (cid:0) 1. We refer to m as the level of resolution
and p as the position within that level. Thus, at the top of this hazard tree we have
the total cumulative hazard H(tJ ), which we split into (cid:12)ner components with each
additional level of the tree, until we (cid:12)nally end at the the bottom of the tree (the
highest level of resolution) with the hazard increments d1; : : : ; dJ . If we further de-
(cid:12)ne Rm;p (cid:17) Hm;2p=Hm(cid:0)1;p, we can parametrize the hazard increments by H0;0 and
the \splits" R1;0; : : : ; RM;2M (cid:0)1(cid:0)1 (denoted Rm;p). For example, when M = 3 (imply-
ing J = 8) we have: d1 = H0;0R1;0R2;0R3;0; d2 = H0;0R1;0R2;0(1 (cid:0) R3;0); : : : ; d8 =
H0;0(1 (cid:0) R1;0)(1 (cid:0) R2;1)(1 (cid:0) R3;3):

It is important to note that the piecewise-constant hazard assumption has been em-

V. Duki(cid:19)c and J. Dignam

597

ployed multiple times in the literature (for example, Walker and Mallick (1997). How-
ever, the uniqueness of the MRH model lies in its clever construction of the tree-based
prior for a piece-wise constant function, so that the prior essentially does not depend on
the (cid:12)nal resolution level M (i.e., it is invariant to the height of the tree). More precisely,
integrating out higher-level parameters, one would obtain the exact same prior as if that
level and its parameters had simply not been considered in the (cid:12)rst place. To aid with
the understanding of the MRH model, a simple diagram of the two-level multiresolution
prior is given in Figure 1.

Figure 1: Diagram illustrating the multiresolution prior for the hazard rate function,
with 2 levels (i.e., with resolution 2).

As in Nowak and Kolaczyk (Nowak and Kolaczyk 2000), we place a beta prior on
each Rm;p and a gamma prior on H. The shape parameters of each of these beta
priors and the hyperparameters for H determine the prior expectations of the hazard
increments, d(cid:3)
j ; j = 1; : : : ; J . Furthermore, to allow for extra smoothness in the mul-
tiresolution prior, Bouman et al. (2007) introduce a multiplier for the shape parameters
of the beta priors at each additional level of the hierarchy, denoted by k. Proceeding in
this fashion, the priors for H and each Rm;p when M = 3 and J = 8 are:

Ga(a; (cid:21));

Be(2(cid:13)1;0ka; 2(1 (cid:0) (cid:13)1;0)ka);

H (cid:24)
R1;0 (cid:24)
R2;p (cid:24)
R3;p (cid:24) Be(2(cid:13)3;pk3a; 2(1 (cid:0) (cid:13)3;p)k3a); p = 0; 1; 2; 3:

Be(2(cid:13)2;pk2a; 2(1 (cid:0) (cid:13)2;p)k2a); p = 0; 1

Note that under this prior structure, E(Rm;p) = (cid:13)m;p, which, because H and Rm;p
are independent a priori, easily allows one to choose (cid:13)m;p, (cid:21) and a so that the prior
expectation of the baseline hazard in each time interval j is any value d(cid:3)
j desired. This

598

Hierarchical Multiresolution Hazard Models

particular formulation of the gamma-beta tree also determines the prior correlation of
the dj as a function (among other things) of k and a. Speci(cid:12)cally, when k = 0:5,
the baseline hazard increments dj are a priori independent gamma random variables.
Choosing k less or greater than 0.5 yields, respectively, negative (rougher hazard) or
positive (smoother hazard) prior correlation among the dj ’s (Bouman et al. 2005, 2007).
Smoother hazard may in particular be employed in problems with much censoring.
It is also possible to treat k as a hyperparameter and estimate it jointly with other
parameters.

It is well known that MRH models, because they are based on a tree-like structure,
may have a blocky correlation pattern; for example, it is possible that two neighboring
hazard increments are less correlated than those further apart which happen to share
more ancestral split parameters. Bouman et al. (Bouman et al. 2005) propose placing a
hyperprior on the hazard H shape parameter a to even out the prior correlations among
hazard increments and bypass this rather counterintuitive property.

3.3 Hierarchical Multiresolution Hazard Model

It may sometimes be of interest or necessity to relax the proportional hazards assump-
tion, permitting di(cid:11)erent baseline hazards in particular groups. These strata-speci(cid:12)c
hazards may be treated as (cid:12)xed, or as random (in(cid:12)nite dimensional) strata-speci(cid:12)c pa-
rameters. This could be desirable in particular when we have data from multiple centers
or multiple studies, when one needs to allow for di(cid:11)erences in baseline hazards due to
unobserved covariate processes, or to account for correlation within subjects from the
same strata.

In breast cancer, it is well-known that women with ER- and ER+ tumors have di(cid:11)er-
ent expected prognosis due to association of ER with both tumor pathology and clinical
characteristics such as patient age (Hess et al. 2003). Because we are primarily inter-
ested in estimating the hazard shapes, we wish to avoid imposing any proportionality
constraint on ER in the model. In any case, Figure 2, showing recurrence-free survival
curves by ER and treatment, clearly illustrates deviation from proportional hazards
between ER- and ER+ patients. While proportionality appears to hold better between
treatment groups within ER categories (Fig. 2), we also wish to permit a di(cid:11)erent
hazard shape according to treatment type, as biologic hypotheses concerning the action
of adjuvant therapy would suggest the possibility of di(cid:11)erent shapes (Skipper 1971).
Thus, we de(cid:12)ne strata de(cid:12)ned by the ER by treatment group combinations (i.e., ER-
surgery only, ER- chemotherapy, ER+ surgery only (placebo), ER+ tamoxifen).

More speci(cid:12)cally, in this model we allow the hazard for each of the strata to be
a priori an independent and identically distributed random MRH variable. For each
stratum s; s 2 f1; 2; 3; 4g, we draw the stratum cumulative hazard Hs from a Ga(as; (cid:21)s),
and then draw the stratum set of splits Rm;p;s. For this reason we will call this model
the hierarchical MRH or the HMRH for short.

To complete the prior for the hazard rate, we place a zero-truncated Poisson (ZTP)
a=[a!(1 (cid:0) e(cid:0)(cid:22)a )], and we allow each scale

hyperprior with mean (cid:22)a on each as: e(cid:0)(cid:22)a(cid:22)a

V. Duki(cid:19)c and J. Dignam

599

0
0

.

1

t

e
e
r
F
−
n
e
v
E
n
o

 

i
t
r
o
p
o
r
P

5
7

.

0

0
5

.

0

5
2

.

0

0

12

24

36

48

60

72

84

Months After Surgery

96

108

120

132

144

surgery only, ER−
surgery only, ER+’

chemotherapy, ER−
tamoxifen, ER+

Figure 2: Kaplan-Meier estimates of recurrence-free survival among early stage breast
cancer patients from NSABP clinical trials. Failures continue to occur in all groups
many years after initial diagnosis and surgery.

parameter (cid:21)s, the parameter of the total cumulative hazards Hs(tJ ), to follow an ex-
ponential distribution with mean (cid:22)(cid:21). The parameters ks can be also given exponential
priors with mean (cid:22)k, but they could also be (cid:12)xed if speci(cid:12)c smoothness (positive or
negative correlation) is desired a priori.

The continuous-time complete-data likelihood is based on the proportional-hazards
model: hs(tjX; (cid:12)) = exp(X0(cid:12))hbase;s(t); where s denotes the stratum group. For a
patient i in stratum s, if the failure time Ti;s 2 [0; tJ ] is observed without censoring, the
likelihood function is:

Li;s((cid:12) j Ti;s; Xi;s) = exp(X0

i;s(cid:12))hbase;s(Ti;s)Sbase;s(Ti;s)exp(X 0

i;s(cid:12)):

(1)

When an observation is right-censored, i.e., Ti;s > tcens for some tcens (cid:20) tJ , we have:

Li;s((cid:12) j Ti;s; Xi;s) = Ss(tcensjXi;s; (cid:12)) = Sbase;s(tcens)exp(X 0

i;s(cid:12)):

(2)

600

Hierarchical Multiresolution Hazard Models

Here, each hs is a priori distributed as an MRH variable, with independent total
hazard and split parameters. The vector of covariates Xi;s contains patient and disease
characteristics (age, tumor size, tumor progesterone receptor level, etc.) for patient i in
stratum s.

4 Markov chain Monte Carlo

The Gibbs sampling algorithm used to simulate the parameter posterior and its sequence
of full conditional posterior distributions for parameters from all strata is outlined below.
The details of the simpler model are provided in Bouman et al. (2005, 2007).

The likelihood for patient i in stratum s, whose failure (or censoring) time is Ti;s,

the censoring indicator (cid:14)i;s, and the covariates Xi;s; is

L(Ti;s

j

(cid:14)i;s; (cid:12); Hs; Rm;p;s; Xi;s) =

(cid:2)exp(X0

i;s(cid:12))hbase;s(Ti;s)(cid:3)(cid:14)i;s exp((cid:0) exp(X0

s=1 Ns patients is thus:

i;s(cid:12))Hbase;s(min(Ti;s; tJ ))): (3)

The log-likelihood for all N =P4
(cid:14)0hX(cid:12) + Fs(cid:5) ~Ri (cid:0)
Xs=1

4

exp(X0

i;s(cid:12))Hbase;s(min(Ti;s; tJ ))

(4)

Ns

Xi=1

where (cid:14) is the vector of censoring indicators for all patients, X is the N (cid:2) L ma-
trix of covariates and (cid:5) is the 2M (cid:2) (2M +1 (cid:0) 1) multiresolution tree matrix.
In
that matrix, the (i; j) element is 1 when j = 1 or i 2 [1 + (j mod 2m); : : : ; 2M(cid:0)m +
(j mod 2m)]; m = blog2(j)c, and 0 otherwise. ~R is the multiresolution log-parameter
vector (log(H); log(R1;0); log(1 (cid:0) R1;0); : : : ; log(RM;2M (cid:0)1); log(1 (cid:0) RM;2M (cid:0)1)). F is an
N (cid:2) 2M matrix for which the (i; j) element is 1 if the ith patient (among all patients in
all strata put together) has Ti 2 (tj(cid:0)1; tj ], and 0 otherwise; patients with Ti > tJ have
Fi;j = 0; j = 1; : : : ; J (see Bouman et al. (2005, 2007) for details).

The Gibbs sampler steps (Geman and Geman 1984) for the parameters Hs, Rm;p;s,

(cid:21)s, as, and ks, for all strata s are the same:

1. Sample Hs from its full conditional density:

(cid:25)(Hsj(cid:21)s; as; Rm;p;s) = Ga(cid:16)(as +PNs
with mean (cid:22) = (as +PNs

i=1 (cid:14)i;s)=h(1=(cid:21)s) +PNs

i=1 (cid:14)i;s); 1=h(1=(cid:21)s) +PNs
i=1 F (Ti;s)i ,

where F (Ti;s) = Hs(min(Ti;s; tJ ))=Hs(tJ ) is a function of the Ti;s and Rm;p;s.

i=1 F (Ti;s)i(cid:17),

2. Sample each Rm;p;s from the full conditional (cid:25)(Rm;p;sjks; as; Hs)

3. Sample k from (cid:25)(ksjas; Rm;p;s; (cid:12)), (cid:21)s from (cid:25)((cid:21)sjHs; as; (cid:12)), and

as from (cid:25)(asjHs; Rm;p;s; (cid:21)s; (cid:12)).

4. Sample (cid:12) from (cid:25)((cid:12)jHs; Rm;p;s):

V. Duki(cid:19)c and J. Dignam

601

Similarly as in Bouman et al. (2005, 2007), the full conditional posterior distributions
for Hs are gamma, while the full conditional distributions for each Rm;p;s and (cid:12) are log-
concave and therefore easy to sample from (using Gilks and Wild (1992) algorithm, for
example). On the other hand, the full conditional distributions for hyperparameters (cid:21)s,
as, and ks are in general more di(cid:14)cult to sample from as they are not log-concave. We
recommend following Bouman et al. (2005) who use the rejection Metropolis sampling
(see Gilks et al. (1995)).

5 Analysis of the Recurrence Hazard after Breast Cancer

5.1 Covariate E(cid:11)ect Estimation

The 16- and 32-bin multiresolution model with the \(cid:13)at" prior hazard rate for each
stratum (with all (cid:13)m;p;s and all ks set to 0.5), was (cid:12)t using output from Gibbs sampler
chains with 12,000 iterations each, with the (cid:12)rst 2,000 iterations of each chain discarded
as burn-in. Every 5th iteration was retained to reduce correlation between adjacent
draws. The Gelman-Rubin diagnostics, performed separately for each parameter, were
used to establish convergence.

Table 1: Posterior Credible Intervals for Predictor E(cid:11)ects, 16-bin model

Tumor Size

Standardized PGR Standardized Age

2.5%
50%
97.5%

0.0148
0.0199
0.0251

(cid:0)0.151
(cid:0)0.066
0.007

(cid:0)0.281
(cid:0)0.217
(cid:0)0.149

Table 1 gives marginal 95% posterior credible intervals for covariates considered.
Tumor size was measured in millimeters (mm), ranging from 0 to 60mm. Progesterone
receptor concentration (PGR) was standardized using the sample mean of 139.17 and
standard deviation of 294.59. Age was standardized using the sample mean of 53.27 and
standard deviation of 10.42 years. Larger tumor size and younger age at diagnosis were
found associated with increased recurrence hazard: within each stratum, an increase of
10.42 years (1 standard deviation) in age resulted approximately in 20% reduction, while
an increase of 1mm in tumor size resulted in approximately 2% increase in recurrence
hazard. A higher concentration of progesterone receptors on the tumor is weakly indica-
tive of lesser failure risk. While menopausal status is generally an important factor in
breast cancer prognosis, here it was only marginally associated with recurrence hazard
after strati(cid:12)cation by estrogen receptor status and inclusion of age at diagnosis in the
model, and so was omitted from further consideration. The direction and magnitude of
these covariate e(cid:11)ects are essentially consistent with other prognostic factor studies of
these patients.

602

Hierarchical Multiresolution Hazard Models

5.2 Hazard Function Estimation

Figure 3 displays by treatment/ER strata the median posterior estimates for the 16-
bin baseline hazard increments (corresponding to constant 11.3-month hazard function
values). This 16-dimensional vector is a discrete approximation to the baseline hazard
In Figure 4, we
show a smoothed version of the same hazard estimates. Smoothing was performed via
the median-spline method, with a hazard value of zero included at time zero for each
stratum, to re(cid:13)ect the fact that patients are considered cancer-free immediately after
surgery and essentially do not fail until some time has elapsed.

rate h(t), estimated via the hazard increments dj = R tj

h(s)ds.

tj(cid:0)1

8
0
0

.

6
0
0

.

d
r
a
z
a
h

4
0
0

.

2
0
0

.

0

0

4

8

Time from Surgery in 11−month Increments

12

16

surgery only, ER−
surgery only, ER+

chemotherapy, ER−
tamoxifen, ER+

covariate values: age=53.3, pgr=139.2, tumor size= 20 mm

Figure 3: Discrete recurrence hazard increments for the 16-bin HMRH model. Horizon-
tal sections represent the hazard value within the approximate 11-month increment in
time from surgery.

Several notable features are seen. First, all four groups have the distinctive hazard
peak around 12-24 months after surgery, with the ER- groups experiencing the peak a
bit earlier. This pattern is similar to that noted by Hess et al. (2003) in their study
of recurrence hazards by ER status. Second, the peak is greatest for the ER- patients

V. Duki(cid:19)c and J. Dignam

603

6
0
0

.

4
0
0

.

d
r
a
z
a
h

2
0
0

.

0

0

4

8

Time from Surgery in 11−month Increments

12

16

surgery only, ER−
surgery only, ER+

chemotherapy, ER−
tamoxifen, ER+

covariate values: age=53.3, pgr=139.2, tumor size=20 mm

Figure 4: Smoothed recurrence hazards for the 16-bin model.

receiving surgery only, and is substantially reduced by chemotherapy, being lower than
untreated patients from the more favorable ER+ group. The lowest peak is among ER+
patients randomized to tamoxifen. Interestingly however, at longer follow-up times the
hazard in this group is no lower than that of chemotherapy treated and even untreated
ER- patient groups, both of which have smaller hazard than untreated ER+ patients.
Ultimately, the ER- chemotherapy treated group has the lowest recurrence hazard.

Figure 5 shows pointwise posterior credible intervals, based on 2.5% and 97.5% es-
timated posterior percentiles, for the four strata. With the exception of time points
around the hazard peak, credible intervals for the four hazard estimates tend to over-
lap, particularly at longer follow-up times. Thus, we currently cannot reliably conclude
whether there is a crossover of failure hazard among the groups at later time points.
We should note that in other analyses collapsing across one or the other strati(cid:12)cation
factors (ER or treatment) and treating ER or treatment as covariates in modeling, large
treatment e(cid:11)ects within ER groups were apparent, while di(cid:11)erences between ER groups
within treatment modalities (surgery, adjuvant) were large initially but attenuated over
time, substantively violating the proportional hazards assumption, as is evident in Fig-

604

ure 2.

Hierarchical Multiresolution Hazard Models

Figure 5: Credible intervals for the recurrence hazards from Figure 4 by ER and treat-
ment strata.

5.3 Sensitivity Analysis

We now turn to the question of robustness of our model results. An alternate model was
based on 32 time intervals of length just under 6 months each, instead of the 16 intervals
of approximate length of 1 year. The purpose of this sensitivity check was twofold: (cid:12)rst,
to compare the estimates of the hazard rates for each of the four groups under these
alternative resolutions; and second, to assess the impact of additional resolution level
onto the estimates of the covariates (age, tumor progesterone receptor level, and tumor
size). Note that in this data set, we could not reasonably use a (cid:12)ner resolution than the
32-bin choice for two reasons: 1) although this is a large cohort of patients, prognosis is
relatively favorable and so failures are few if the intervals (bins) are very small, and 2)
follow-up visits do not happen more frequently than once every 3 to 6 months, and while
recurrence events can occur in continuous time, recurrence events that are not clinically
apparent will be detected at these visits, causing clustering of events (and consequently

V. Duki(cid:19)c and J. Dignam

605

arti(cid:12)cial periodicity in the hazard function estimate) if very small time bins are used.

Table 2: Posterior Credible Intervals for Predictor E(cid:11)ects, 32-bin model

Tumor Size

Standardized PGR Standardized Age

2.5%
50%
97.5%

0.0150
0.0200
0.0250

(cid:0)0.147
(cid:0)0.066
0.009

(cid:0)0.281
(cid:0)0.215
(cid:0)0.147

Based on the invariance properties of the MRH prior, one can expect that some
of that invariance is preserved in the posterior as well; and we observe this to a large
degree. For example, a 16-bin hazard estimate obtained by (cid:12)tting the 32-bin model and
then aggregating the neighboring hazard increments is almost indistinguishable from
the original 16-bin model hazard estimate.
In addition, hazard rate plots using the
32-bin model and the 16-bin model are very similar as well; as expected, the 32-bin
estimates are slightly more variable, but all hazard shapes are very similar (not shown).

With respect to covariate e(cid:11)ect estimates, the e(cid:11)ects of increasing the resolution
is minimal. Compared to Table 1, the e(cid:11)ects shown in Table 2 di(cid:11)er by a negligible
amount.

6 Discussion

We have illustrated the application of a (cid:13)exible extension of the familiar Cox pro-
portional hazards model to jointly estimate covariate e(cid:11)ects and separate hazard rate
functions for several patient strata. This approach allows us to incorporate covariate
e(cid:11)ects and perform inference related to shapes and change-points in the hazard over
time, our primary interest in the problem of recurrence after early stage breast can-
cer. The estimation and examination of the hazard functions directly reveals important
patterns not readily apparent from quantities such as the survival functions. However,
the hazard function remains a di(cid:14)cult quantity to draw robust inference from, as even
in this large dataset, estimates suggest potentially important di(cid:11)erences in shape, but
variability estimates preclude any de(cid:12)nitive conclusions pending additional analyses, as
discussed below.

The observation that among those patients with initially higher risk disease (i.e.,
those with ER- tumors), the fraction escaping the early failure risk go on to have sub-
stantially lower long-term failure risk than those with initially more favorable prognosis
(ER+ tumors), has signi(cid:12)cant implications in both clinical management and consider-
ations regarding further developments of adjuvant therapies. In fact, there has been
much recent interest in the development of ‘switching’ strategies whereby women with
ER+ tumors discontinue tamoxifen and begin use of other hormonal treatments, in or-
der to extend and improve on the bene(cid:12)t of this treatment modality. Currently, little

606

Hierarchical Multiresolution Hazard Models

is known about what factors might be key to optimizing the switching strategy. For
ER- patients, newer chemotherapy and molecularly targeted agents that act on speci(cid:12)c
tumor vulnerabilities may o(cid:11)er the best opportunity for a bona (cid:12)de cure once early
failure is avoided.

In addition to the well-known initial wave of failures following surgery, in recent
years investigators have suggested that additional reproducible patterns are manifest in
the recurrence hazard in the intermediate to long-term follow-up period. The notion
of a bimodal or \double-peaked" recurrence hazard has been proposed, where after the
(cid:12)rst period of increased failure risk at 1-5.3.0 years post-surgery, the decline in failure
hazard is followed by a second peak centered roughly around 8 years (Demicheli et al.
1996, 2001; Baum et al. 2005). A number of cancer biologic hypotheses have been
put forth regarding the meaning and cause of a possible double-peaked failure pattern.
For example, it has been conjectured that growth kinetics perturbed by surgery may
contribute to the (cid:12)rst wave in failure hazard, while heterogeneous disseminated tumor
cells that require more time to become established may account for the latter peak in
failure (Demicheli et al. 2001; Baum et al. 2005). This idea may seem to harken back
to the naive concept that cancer surgery \spreads" cancer, but the in(cid:13)uence of surgery
on growth kinetics does have foundation in substantive biologic theory (Fisher et al.
1983). However, before any such interpretations of the hazard shape can be made or
gain further credibility, more rigorous analytic methods such as those proposed here
must be applied. Furthermore, it may be di(cid:14)cult to uniquely ascribe such a pattern
to speci(cid:12)c biologic phenomena, because other circumstances, such as the existence of
patient mixtures due to unrecognized factors present at diagnosis or apparent hazard
spikes caused by clustering of failures in time due to discovery of subclinical disease
around certain time landmarks (e.g., mandatory 5-year post-diagnosis screen) would
be expected to produce similar patterns. Nonetheless, this intriguing concept merits
further investigation.

Our future work on this problem will involve the inclusion of data from trials con-
ducted subsequent to those included here. As the trials are designed in a hierarchical
fashion, these studies share some treatment arms in common with the current data, but
also include newer treatment regimens. Extension of the model to more data sources will
involve incorporation of ‘trial’ e(cid:11)ects to allow for heterogeneity among common treat-
ment arms. The inclusion of additional data will permit a more thorough exploration
of changes in the hazard over time and more robust inference (including \collapsibility"
of neighboring intervals in some regions), due to the considerably larger sample size
that will result in narrower bounds on estimated hazards. This analysis will also have
greater biologic and clinical relevance as we explore more recent drug regimens designed
to reduce recurrence risk in women with breast cancer.

References

Aalen, O. and Gjessing, H. (2001). \Understanding the shape of the hazard rate: A

process point of view." Statistical Science, 16: 1{22. 592

V. Duki(cid:19)c and J. Dignam

607

Andersen, P., Borgan, O., Gill, R., and Keiding, N. (1993). Statistical Methods Based

on Counting Processes. Berlin: Springer{Verlag. 595

Baum, M., Demicheli, R., Hrushesky, W., and Retsky, M. (2005). \Does surgery un-
favourably perturb the "natural history" of early breast cancer by accelerating the
appearance of distant metastases?" European Journal of Cancer, 41: 508{515. 606

Berg, J. and Robbins, G. (1966). \Factors in(cid:13)uencing short and long term survival of
breast cancer patients." Surgical and Gynecologic Obstetrics, 122: 1311{1316. 593

Bouman, P., Dignam, J., Dukic, V., and Meng, X. (2007). \A multiresolution hazard
model for multi-center survival studies: Application to tamoxifen treatment in early
stage breast cancer." Journal of the American Statistical Association, in press. 591,
592, 595, 596, 597, 598, 600, 601

Bouman, P., Dukic, V., and Meng, X. (2005). \Bayesian multiresolution hazard model
with application to an AIDS reporting delay study." Statistica Sinica, 15: 325{357.
591, 592, 595, 596, 598, 600, 601

Bryant, J., Fisher, B., Gunduz, N., Costantino, J., and Emir, B. (1997). \S-phase
fraction combined with other patient and tumor characteristics for the prognosis of
node-negative, estrogen-receptor-positive breast cancer." Breast Cancer Research and
Treatment, 51: 239{253. 594

Cox, D. (1972). \Regression models and life tables." Journal of the Royal Statistical

Society - Series B, 34: 187{220. 595

Demicheli, R., , Valagussa, P., and Bonadonna, G. (2001). \Does surgery modify growth
kinetics of breast cancer micrometastases?" British Journal of Cancer, 85: 490{492.
606

Demicheli, R., Abbattista, A., Miceli, R., Valagussa, P., and Bonadonna, G. (1996).
\Time distribution of the recurrence risk for breast cancer patients undergoing mas-
further support about the concept of tumor dormancy." Breast Cancer
tectomy:
Research and Treatment, 41: 177{185. 593, 606

Fisher, B., Constantino, J., Redmond, C., Poisson, R., Bowman, D., Couture, J., Dim-
itrov, N., Wolmark, N., Wickerham, D., and Fisher, E. (1989a). \A randomized clin-
ical trial evaluating tamoxifen in the treatment of patients with node-negative breast
cancer who have estrogen-receptor-positive tumors." The New England Journal of
Medicine, 320: 479{484. 594

Fisher, B., Dignam, J., Bryant, J., DeCillis, A., Wickerham, D., Wolmark, N., J, J. C.,
Redmond, C., Fisher, E., Bowman, D., Deschenes, D., Dimitrov, N., Margolese, R.,
Robidoux, A., Shibata, H., Terz, J., Paterson, A., Feldman, M., Farrar, W., Evans,
J., and Lickley, H. (1996a). \Five versus more than (cid:12)ve years of tamoxifen therapy
for breast cancer patients with negative lymph nodes and estrogen receptor positive
tumors." Journal of the National Cancer Institute, 88: 1529{1542. 594

608

Hierarchical Multiresolution Hazard Models

Fisher, B., Dignam, J., Mamounas, E., Costantino, J., Wickerham, D., Redmond, C.,
Wolmark, N., Dimitrov, N., Bowman, D., Glass, A., Atkins, J., Abramson, N.,
Sutherland, C., Aron, B., and Margolese, R. (1996b). \Sequential methotrexate
and (cid:13)uorouracil for the treatment of node-negative breast cancer patients with es-
trogen receptor-negative tumors: eight-year results from National Surgical Adjuvant
Breast and Bowel Project (NSABP) B-13 and (cid:12)rst report of (cid:12)ndings from NSABP
B-19 comparing methotrexate and (cid:13)uorouracil with conventional cyclophosphamide,
methotrexate, and (cid:13)uorouracil." Journal of Clinical Oncology, 14: 1982{1992. 594

Fisher, B., Gunduz, N., and Sa(cid:11)er, E. (1983). \In(cid:13)uence of the interval between primary
tumor removal and chemotherapy on kinetics and growth of metastases." Cancer
Research, 43: 1488{1492. 606

Fisher, B., Redmond, C., Dimitrov, N., Bowman, D., Legault-Poisson, S., Wickerham,
D., Wolmark, N., Fisher, E., Margolese, R., and Sutherland, C. (1989b). \A random-
ized clinical trial evaluating sequential methotrexate and (cid:13)uorouracil in the treatment
of patients with node-negative breast cancer who have estrogen-receptor-negative tu-
mors." The New England Journal of Medicine, 320: 473{478. 594

Fisher, B., Redmond, C., Wickerham, D., Wolmark, N., Bowman, D., Couture, J.,
Dimitrov, N., Margolese, R., Legault-Poisson, S., and Robidoux, A. (1989c). \Sys-
temic therapy in patients with node-negative breast cancer. A commentary based on
two National Surgical Adjuvant Breast and Bowel Project (NSABP) clinical trials."
Annals of Internal Medicine, 111: 703{712. 594

Geman, S. and Geman, D. (1984). \Stochastic relaxation, Gibbs distributions and the
Bayesian restoration of images." IEEE Transactions on Pattern Analysis and Machine
Intelligence, 6: 721{741. 600

Gilks, W., Best, N., and Tan, K. (1995). \Adaptive rejection Metropolis sampling."

Applied Statistics, 44: 455{472. 601

Gilks, W. and Wild, P. (1992). \Adaptive rejection sampling for Gibbs sampling."

Applied Statistics, 41: 337{348. 601

Gordon, N. (1990). \Application of the theory of (cid:12)nite mixtures for the estimation of

’cure’." Statistics in Medicine, 9: 397{407. 593

Gray, R. (1990). \Some diagnostic methods for Cox regression models through hazard

smoothing." Biometrics, 46: 93{102. 595

| (1992). \Flexible methods for analyzing survival data using splines, with application
to breast cancer prognosis." Journal of the American Statistical Association, 87:
942{951. 595

| (1996). \Hazard rate regression using ordinary nonparametric regression smoothers."

Journal of Computational and Graphical Statistics, 5: 190{207. 595

V. Duki(cid:19)c and J. Dignam

609

Hess, K., Pusztai, L., Buzdar, A., and Hortobagyi, G. (2003). \Estrogen receptors and
distinct patterns of breast cancer relapse." Breast Cancer Research and Treatment,
78: 105{118. 593, 598, 602

Karrison, T., Ferguson, D., and Meier, P. (1999). \Dormancy of mammary carcinoma

after mastectomy." Journal of the National Cancer Institute, 91: 80{85. 593

Kolaczyk, E. (1999). \Bayesian multiscale models for Poisson processes." Journal of

the American Statistical Association, 94: 920{933. 596

Nowak, R. and Kolaczyk, E. (2000). \A statistical multiscale framework for Poisson
inverse problems." IEEE Transactions on Information Theory, 46: 1811{1825. 596,
597

Prentice, R. L., Kalb(cid:13)eisch, J. D., Peterson, A. V., Flournoy, N., Farewell, V. T., and
Breslow, N. E. (1978). \The Analysis of Failure Times in the Presence of Competing
Risks." Biometrics, 34: 541{554. 595

Saphner, T., Tormey, D., and Gray, R. (1996). \Annual hazard rates of recurrence for
breast cancer after primary therapy." Journal of Clinical Oncology, 14: 2738{2746.
593

Skipper, H. (1971). \Kinetics of mammary tumor cell growth and implications for

therapy." Cancer, 28: 1479{1499. 598

Spiegelhalter, D., Best, N., Carlin, B., and van der Linde, A. (2002). \Bayesian measures
of model complexity and (cid:12)t (with discussion)." Journal of the Royal Statistical Society
- Series B, 64: 583{616. 596

Walker, S. and Mallick, B. (1997). \Hierarchical Generalized Linear Models and Frailty
Models with Bayesian Nonparameteric Mixing." Journal of the Royal Statistical
Society - Series B, 59: 845{860. 597

Acknowledgments

Support for this research was partially provided by the Susan G. Komen Breast Cancer Founda-
tion and Public Health Service grant NCI-U10-CA-69651 from Institutes of Health, Department
of Health and Human Services.

610

Hierarchical Multiresolution Hazard Models

Bayesian Analysis (2007)

2, Number 3, pp. 611{634

A Spatially-adjusted Bayesian Additive

Regression Tree Model to Merge Two Datasets

Song Zhang,(cid:3) Ya-Chen Tina Shihy and Peter M(cid:127)ullerz

Scienti(cid:12)c hypotheses of interest often involve variables that are not
Abstract.
available in a single survey. This is a common problem for researchers working
with survey data. We propose a model-based approach to provide information
about the missing variable. We use a spatial extension of the BART (Bayesian
additive regression tree) model. The imputation of the missing variables and infer-
ence about the relationship between two variables are obtained simultaneously as
posterior inference under the proposed model. The uncertainty due to imputation
is automatically accounted for. A simulation analysis and an application to data
on self-perceived health status and income are presented.

Keywords: BART, CART, Missing variables, Spatial model, Survey

1 Introduction

We consider the problem of inference about the relationship of two variables reported in
two di(cid:11)erent datasets. This is a common problem for researchers working with survey
data. Scienti(cid:12)c hypotheses of interest often involve variables that are not available in a
single survey. Speci(cid:12)cally, we are interested in inference on how a variable z is a(cid:11)ected
by another variable y, when there is no such dataset that collects z and y simultaneously.
Instead, z is only reported in dataset D1 and y is only collected in dataset D2.

Many model-based methods have been developed to deal with missing data prob-
lems, including maximum likelihood (ML) methods, multiple imputation (MI) methods,
weighted estimating equations (WEE), and fully Bayesian (FB) methods. See Little
(1992), Horton and Laird (1999), Schafer and Graham (2002), Ibrahim et al. (2005) and
the references therein for detailed discussions. There are some assumptions associated
with each of these methods. Many ML methods assume a large sample size so that
the ML estimates are approximately unbiased and normally distributed. The likelihood
function is assumed to arise from a parametric model of the complete data. Finally,
ML methods usually require the missing at random (MAR) assumption (Rubin 1976).
MI methods also rely on large-sample approximation and assume a parametric form
for the joint model of the observed and missing data. They require some assumption
about the distribution of missingness, although not limited to MAR. WEE methods are
extensions of generalized estimating equations (GEE). Two models need to be specifed:

(cid:3)Department of Biostatistics, University of Texas MD Anderson Cancer Center, Houston, TX,

mailto:songzhang@mdanderson.org

yDepartment of Biostatistics, University of Texas MD Anderson Cancer Center, Houston, TX,

http://gsbs.uth.tmc.edu/tutorial/shih.html

zDepartment of Biostatistics, University of Texas MD Anderson Cancer Centeri, Houston, TX,

http://odin.mdacc.tmc.edu/~pm/

c(cid:13) 2007 International Society for Bayesian Analysis

ba0007

612

Spatial BART to Merge Datasets

One regression model for the data, and the other describing the missingness mecha-
nism. WEE methods are considered to be doubly robust because the estimates of the
regression parameters remain consistent as long as one of the two models is correctly
speci(cid:12)ed. The MAR assumption and a large sample size are required. FB methods
do not require a large sample size. Specifying a joint probability model, however, they
require assumptions about the sampling model for the data and about the missingness
mechanism. In summary, all the above methods regard missingness as a probabilistic
phenomenon. In contrast, in the following discussion, missingness is not random. The
variable y is missing for all records in D1.

The most commonly applied method to borrow information from one dataset (i.e.,
y in D2) to provide information not collected in another dataset (i.e., D1) is the use of
census-based socioeconomic status (SES) characteristics to supplement individual-level
data, such as medical records, claims or registries
(Gornick et al. 1996; Geronimus and Bound 1998; Devesa and Diamond 1983). The
census-based approach obtains aggregate statistics of SES variables at certain geographic
levels (e.g., census track, county, or zip code) and uses these aggregate numbers as proxy
measures of SES in individual-level data. It has been used extensively in studies of health
disparities. For more examples, see Mandelblatt et al. (1991), Kraus et al. (1986) and
Byrne et al. (1994).

Geronimus and Bound (1998) cautioned that although the census-based approach
is easy to execute, these aggregate measures should not be interpreted as if they were
micro-level variables. The approach has several limitations. It requires detailed residen-
tial information to be collected in D1. If due to privacy concerns this information is not
collected or is not detailed enough (for example, only state code is available), then the
method breaks down. The method only makes use of geographic information. Other
individual-level covariates are ignored. For example, if we are interested in imputing
missing income in D1, then information such as age, gender, education, occupation
could be very informative. Finally, the true value of the missing variable in D1 may not
match the neighborhood pro(cid:12)le. This uncertainty is usually ignored.

In this paper we propose to approach the problem in the framework of Bayesian
hierarchical modeling. A spatially adjusted Bayesian additive regression tree (SBART)
is de(cid:12)ned to impute the missing variable in D1 based on individual-level covariates as
well as geographic information. SBART is an extension of the BART model. The idea
of BART is to model an unknown function as a mixture of tree models. Each tree is
a priori constrained to have a simple structure. It only contributes a small portion to
the overall model. Chipman, George and McCulloch (2006a) demonstrated that the
sum over all trees provides a su(cid:14)ciently rich model to incorporate both direct e(cid:11)ects
and interaction e(cid:11)ects of di(cid:11)erent orders. SBART extends BART by incorporating
spatial random e(cid:11)ects. Correlation among neighboring areas is utilized to improve
inference. Our method implements a full probability model with likelihood and priors.
The imputation of the missing variable and the inference about the relationship between
the two variables are obtained simultaneously as posterior inference under the model,
and the uncertainty due to imputation is accounted for automatically. Unrelated to the
problem of merging datasets that we consider here, a similar spatial extension of the

Zhang, S., Shih, Y., M(cid:127)uller, P.

613

BART model has been developed independently in current work by Chipman, George,
McCulloch and Musio (2006b).

The outline of the paper is as follows. Section 2 introduces notation and presents
the Bayesian hierarchical model. A simulation study is conducted in Section 3. We
illustrate our method with a data analysis example in Section 4. Finally, Section 5
discusses some limitations of our method as well as some possible extension.

2 A Spatial BART Model

Let I be the number of spatial units at the (cid:12)nest level of detail recorded in both datasets.
This could be, for example, census tract, zip code area or county.

sample size of D1 is m = PI

In dataset D1, let mi denote the number of subjects from area i (i = 1; (cid:1) (cid:1) (cid:1) ; I). The
i=1 mi. For the jth subject from area i, we are interested
in the relationship between variables zij and yij , where zij but not yij is recorded in
dataset D1. We use vij to denote a vector of other individual-level covariates reported
in D1.

The variable yij that is missing in D1 is recorded on a di(cid:11)erent set of individuals
in dataset D2. For notational ease, we use the variable name xij rather than yij ,
to distinguish the fact that these variable values are recorded in D2 rather than D1.
Similarly, for the vector of variables vij , we use wij rather than vij for those variables
recorded in D2. We assume wij and vij to be consistent, i.e., they record the same
variables and use the same coding for the values. Because it would be unusual for all
covariates recorded in D1 and D2 to be consistent, we only assume that after suitable
pre-processing a subset of the covariates can be considered consistent across the two
datasets. Let ni be the number of subjects from area i, so j = 1; : : : ; ni in D2. Then

i=1 ni is the sample size of D2.

n =PI

We de(cid:12)ne Z = fzij; i = 1; (cid:1) (cid:1) (cid:1) ; I; j = 1; (cid:1) (cid:1) (cid:1) ; mig. Similarly we use Y , V , X and W

to denote the vector of all yij , vij , xij and wij , respectively.

We describe in words how the proposed approach facilitates learning about the
relationship between Z and Y with Y missing. We assume that (Y ; V ) (in D1) and
(X; W ) (in D2) arise from the same model M . We use the posterior for the parameters
in M , obtained conditional on (X; W ) to impute the missing Y conditional on V .
Finally, the regression of Z on the imputed Y approximates the relationship between
Z and Y . By integrating with respect to Y , the marginal posterior distribution of the
regression parameter (cid:12) accounts for the variability induced by the imputation. The
described learning process is complicated by the need to specify a joint probability
model for (Z; Y ; X j V ; W ). Details are described later.

For the learning process to work we make the key assumption that (X; W ) and
(Y ; V ) are independent samples from the same model. This assumption ensures that we
can apply what we have learned from (X; W ) to (Y ; V ). For example, this assumption
is satis(cid:12)ed if both D1 and D2 are representative samples from the U.S. population.

614

Spatial BART to Merge Datasets

2.1 The Sampling Model

The proposed approach is model-based. We start the model construction with assumed
sampling models for Z, X and Y . In the following description, we use N (m; s2) to
denote a normal distribution with moments m and s2. We assume that a sampling
model p(zij j yij; vij ; (cid:8)) is available for zij , conditional on vij and assumed values for
yij , and indexed by a set of parameters (cid:8). For example, if zij is continuous, we can
assume a linear regression model with zij being the dependent variable, yij and vij
de(cid:12)ning the design matrix, and (cid:8) including the regression coe(cid:14)cients and variance
parameter. If zij is ordinal, an ordinal probit model may be used. Speci(cid:12)c examples of
p(zij j yij; vij ; (cid:8)) are used in the simulation study and the case study.

The model p(xij

j wij ; f; (cid:18); (cid:27)2) describes the relationship between xij and wij .

Speci(cid:12)cally, we assume

xij j wij ; f; (cid:18); (cid:27)2 (cid:24) N(cid:0)f (wij ) + (cid:18)i; (cid:27)2(cid:1) ;

(1)

where (cid:18) = ((cid:18)1; (cid:1) (cid:1) (cid:1) ; (cid:18)I )0 is a vector of random spatial e(cid:11)ects, f (wij ) is an unknown
function associating xij with wij , and (cid:27)2 is the residual variance. We represent the
mean function f (wij ) as a BART model. Since the additional random e(cid:11)ects (cid:18)i introduce
the desired spatial correlation among neighboring areas, we refer to model (1) as the
spatially-adjusted Bayesian additive regression tree (SBART) model.

For reference, and to introduce notation for later use, we give a brief review of the
BART model. See Chipman et al. (2006a) for details. We begin with the notation for
a single tree model. Let T denote a tree. Its nodes can be divided into two categories,
interior nodes and terminal nodes. A splitting rule is de(cid:12)ned at each interior node. We
limit splitting rules to binary splits. Each rule consists of a splitting variable and a
splitting value. The splitting value is a threshold on the splitting variable that de(cid:12)nes
the splitting rule. Starting from the root, an individual with covariates wij selects
branches in the tree according to the splitting rules until it is assigned to a terminal
node. Suppose that there are K terminal nodes. We de(cid:12)ne (cid:22) = ((cid:22)1; (cid:1) (cid:1) (cid:1) ; (cid:22)K )0, with (cid:22)k
being assigned to the kth terminal node. The tree maps each covariate vector wij into
one element of (cid:22). A single tree model is denoted by the pair (T; (cid:22)), and the association
between (cid:22)k and wij through a tree T is written as (cid:22)k = g(wij ; T; (cid:22)).

The BART model de(cid:12)nes a summation of such tree models, as

f (wij ) = g(wij ; T1; (cid:22)1) + g(wij ; T2; (cid:22)2) + (cid:1) (cid:1) (cid:1) + g(wij ; TL; (cid:22)L);

where L is the total number of trees that form the BART. We usually assign a large
value for L (e.g., L = 200) to encourage (cid:13)exibility. On the other hand, to avoid over-
(cid:12)tting, the BART model includes a strong prior on each tree to keep its e(cid:11)ect small,
e(cid:11)ectively making each tree into a \weak learner". But overall, the sum of trees pro-
vides a su(cid:14)ciently rich model to (cid:12)t a variety of functions. For example, (cid:22)k represents
an interaction e(cid:11)ect if its assignment involves more than one component of wij (i.e.,
more than one splitting variable). Furthermore, because f (wij ) can be based on trees
of di(cid:11)erent sizes, the BART model can incorporate both direct e(cid:11)ects and interaction

Zhang, S., Shih, Y., M(cid:127)uller, P.

615

e(cid:11)ects of di(cid:11)erent orders. SBART extends BART by incorporating an additional spatial
e(cid:11)ect into the conditional mean of xij given wij .

BART is closely related to ensemble methods that combine a set of tree models.
Examples of ensemble methods include boosting, bagging and random forests. Boost-
ing (Freund and Schapire 1997; Friedman 2001) (cid:12)ts a sequence of trees. Each tree is
(cid:12)t conditional on data variation that is not explained by the other trees. Bagging
(Breiman 1996; Clyde and Lee 2001) and random forests (Breiman 2001) construct a
large number of independent trees through data randomization and stochastic search.
The methods then use an average of the trees to improve prediction. Ensemble methods
are not derived as coherent inference under a probability model. In contrast, BART is
a model-based approach that reports inference as the summary of a full probabilistic
description of all relevant uncertainties. Bayesian single tree models have been devel-
oped by Chipman et al. (1998) and Denison et al. (1998). Compared with single tree
models, the sum-of-trees models provide vastly more (cid:13)exibility by easily incorporat-
ing additive e(cid:11)ects. Chipman et al. (2006a) provided a posterior Markov chain Monte
Carlo (MCMC) simulation scheme for the BART model. They demonstrated that the
proposed MCMC simulation has good mixing properties.

The third part of the top-level sampling model is an assumed model for yij con-
ditional on the observed covariate vector vij . We assume the same model as for the
regression of xij on wij :

with f ((cid:1)) de(cid:12)ned by the SBART model as before.

yij j vij ; f; (cid:18); (cid:27)2 (cid:24) N(cid:0)f (vij ) + (cid:18)i; (cid:27)2(cid:1) ;

2.2 The Prior Model

We complete the Bayesian hierarchical model with priors p((cid:8)), p(f ), p((cid:18)) and p((cid:27)2),
for (cid:8), f , (cid:18) and (cid:27)2, respectively. We assume a priori independence.

The choice of p((cid:8)) depends on the particular form of p(zij j yij; vij; (cid:8)). For example,
in a linear regression model, conjugate priors are technically convenient choices. That
is, normal priors for the regression coe(cid:14)cients and an inverse Gamma prior for the
residual variance.

The BART model in (1) is indexed by f(Tl; (cid:22)l); l = 1; (cid:1) (cid:1) (cid:1) ; Lg. We use

p(f ) =

L

Yl=1

p(Tl; (cid:22)l) =

L

Yl=1np(Tl) (cid:1) p((cid:22)l j Tl)o:

Following Chipman et al. (2006a), we de(cid:12)ne p(Tl) by three factors, corresponding to a
node being non-terminal, the selection of the splitting variable for a non-terminal node,
and the choice of the splitting value conditional on a chosen splitting variable. The
probability that a node at depth d is nonterminal, is assumed to be

(cid:11)(1 + d)(cid:0)(cid:13);

616

Spatial BART to Merge Datasets

where (cid:11) 2 (0; 1) and (cid:13) 2 [0; 1) are two hyper-parameters re(cid:13)ecting our prior belief
about the tree. For example, if we believe that the depth of the tree should be small, we
can assign a big value for (cid:13), so that the probability decays fast with d. Chipman et al.
(2006a) proposed (cid:11) = 0:95 and (cid:13) = 2 as default values, which implies that with prior
probability 0.05, 0.55, 0.28, 0.09 and 0.03, the tree has 1, 2, 3, 4, and (cid:21) 5 terminal nodes,
respectively. A natural choice for the selection of the splitting variable, conditional on
a node being non-terminal, is a uniform prior over all available variables. A default
choice for the distribution of the splitting value is a uniform distribution over the set of
available splitting values. Finally, we de(cid:12)ne a prior for (cid:22)l. Let (cid:22)lk be the kth element
of (cid:22)l. Conditional on Tl, we assume i.i.d. normal priors for (cid:22)lk. The mean and variance
of the normal prior are speci(cid:12)ed in such a way that each tree is constrained to be a
weak learner, and it plays a small role in the overall (cid:12)t. More details can be found in
Chipman et al. (2006a), Section 3.2.

For the spatial random e(cid:11)ects (cid:18) we use a conditionally autoregressive (CAR) prior.
The key idea of the CAR model is simple. It formalizes the notion that each area is
similar to its neighbors. Speci(cid:12)cally, we de(cid:12)ne p((cid:18)) by the set of conditional distributions

p((cid:18)i j (cid:18)((cid:0)i); (cid:26); (cid:14)2) = N(cid:16) (cid:26)

hi Xj6=i

cij (cid:18)j;

1
hi

(cid:14)2(cid:17); i = 1; (cid:1) (cid:1) (cid:1) ; I;

(2)

where (cid:18)((cid:0)i) denotes all the elements of (cid:18) except (cid:18)i; (cid:26) is a parameter with range ((cid:0)1; 1);
(cid:14)2 is the variance component; cij = 1 (i 6= j) if area i and area j are neighbors, and
j=1 cij is the total number of neighbors

cij = 0 otherwise, including cii = 0; and hi =PI

for area i. The joint distribution p((cid:18)) implied by (2) is

p((cid:18) j (cid:26); (cid:14)2) = N(cid:16)0; (cid:14)2(H (cid:0) (cid:26)C)(cid:0)1(cid:17);

(3)

where C = (cij ) is an I (cid:2) I adjacency matrix, and H is an I (cid:2) I diagonal matrix with
hi being the diagonal elements. Model (2) speci(cid:12)es that given random e(cid:11)ects from all
the other areas, the distribution of (cid:18)i only depends on its neighbors. When (cid:26) = 0, the
variance matrix in (3) is diagonal, implying that (cid:18)i are independent. When (cid:26) = 1, the
conditional mean of (cid:18)i in (2) equals the average of its neighbors. However, (cid:26) = 1 implies
that H (cid:0) (cid:26)C is singular. That is, the covariance matrix of (cid:18) does not exist. Sun et al.
(1999) speci(cid:12)ed (cid:0)1 < (cid:26) < 1 as a smoothing or spatial correlation parameter. It can be
thought of as a measure of spatial association. For more discussion of CAR models, see
Cressie (1993) page 407, Besag et al. (1991), Clayton and Kaldor (1987) and Whittle
(1954).

We complete the prior model with probability models for the hyper-parameters (cid:27)2; (cid:26)
and (cid:14)2. Chipman et al. (2006a) assumed p((cid:27)2) to be an inverse chi-square distribution
(cid:27)2 (cid:24) (cid:23)(cid:21)=(cid:31)2
(cid:23) , where (cid:23) is the degree of freedom. This is a special case of the inverse
Gamma distribution. The key idea to specify the hyper-parameters (cid:23) and (cid:21) is to (cid:12)rst
obtain a preliminary estimate ^(cid:27)2 by exploratory data analysis (for example, through
linear regression of xij and wij ), and then specify (cid:23) and (cid:21) such that ^(cid:27)2 matches the
qth quantile of p((cid:27)2). The default setting recommended by Chipman et al. (2006a) is
((cid:23); q) = (3; 0:90). Finally, we de(cid:12)ne prior distributions for the parameters (cid:26) and (cid:14)2 in

Zhang, S., Shih, Y., M(cid:127)uller, P.

617

the CAR model. It is natural to assume that the spatial e(cid:11)ects are positively correlated.
We therefore assume (cid:26) to be uniform between 0 and 1, i.e., U (0; 1). We assume p((cid:14)2)
to be an inverse Gamma distribution, denoted by IG(a(cid:14); b(cid:14)), with density function

p((cid:14)2) /

1

((cid:14)2)a(cid:14) +1

exp((cid:0)

b(cid:14)
(cid:14)2

):

Here a(cid:14) and b(cid:14) are (cid:12)xed hyperparameters.

For reference, we state the joint probability model on the data Z, Y , X and the

parameters:

p(Z j Y ; V ; (cid:8)) (cid:1) p(X j W ; f; (cid:18); (cid:27)2) (cid:1) p(Y j V ; f; (cid:18); (cid:27)2)

(cid:1) p((cid:8)) (cid:1) p(f ) (cid:1) p((cid:18) j (cid:26); (cid:14)2) (cid:1) p((cid:27)2) (cid:1) p((cid:26)) (cid:1) p((cid:14)2);

(4)

where

I

mi

p(Z j Y ; V ; (cid:8)) =

p(X j W ; f; (cid:18); (cid:27)2) =

p(Y j V ; f; (cid:18); (cid:27)2) =

I

Yi=1
Yi=1
Yi=1

I

p(zij j yij; vij ; (cid:8));

p(xij j wij ; f; (cid:18); (cid:27)2);

p(yij j vij ; f; (cid:18); (cid:27)2):

ni

Yj=1
Yj=1
Yj=1

mi

We are interested in the inference on (cid:8) given all observations, namely p((cid:8) j Z; X; V ; W ).
Carrying out the desired inference requires integration with respect to Y and the other
parameters. This integration does not have a closed form solution. We set up MCMC
simulation and obtain inference based on random samples from the posterior distribu-
tion of (cid:8). Details of the sampling scheme can be found in the Appendix. By integrating
out Y , p((cid:8) j Z; X; V ; W ) automatically accounts for the variability induced by the im-
putation. A byproduct of this process is the imputation of the missing variable Y , which
can be obtained as random samples from p(Y j Z; X; V ; W ).

3 A Simulation Study

We conduct a simulation study to examine the performance of the proposed approach.
We de(cid:12)ne I = 99 spatial areas, with an assumed spatial structure (adjacency matrix
C) equal to that of the 99 counties in the state of Iowa. We also assume ni = 4 and
mi = 2 for i = 1; (cid:1) (cid:1) (cid:1) ; I. Thus we have sample size n = 396 and m = 198.

The simulated data are generated as follows. We assume covariate vectors wij and
vij to be of dimension 10. Each of the 10 elements is generated from independent U (0; 1)
distribution. We generate the simulation truth for the spatial random e(cid:11)ects (cid:18) from a

618

Spatial BART to Merge Datasets

N (0; (cid:14)2(H (cid:0) (cid:26)C)(cid:0)1) distribution, using (cid:26) = 0:3 and (cid:14) = 1. The mean function f (u) is
evaluated as

f (u) = 10 sin((cid:25)u1u2) + 20(u3 (cid:0) 0:5)2 + 10u4 + 5u5;

(5)

where ui is the ith element of u = (u1; (cid:1) (cid:1) (cid:1) ; u10)0. The same function was used in sim-
ulation in Friedman (1991) and Chipman et al. (2006a). The added variables together
with the interactions and nonlinearities make it di(cid:14)cult to (cid:12)t the model by standard
parametric methods. Conditional on the covariates wij , we generate xij by

xij j wij ; f; (cid:18); (cid:27)2 (cid:24) N (f (wij ) + (cid:18)i; (cid:27)2);

using (cid:27) = 0:2. Similarly, we generate yij conditional on vij ,

yij j vij; f; (cid:18); (cid:27)2 (cid:24) N (f (vij ) + (cid:18)i; (cid:27)2):

Thus xij and yij only depend on the (cid:12)rst 5 elements of wij and vij , respectively.

Finally, zij is generated by

zij j yij; vij ; (cid:12); (cid:28) 2 (cid:24) N (h(vij; yij ; (cid:12)); (cid:28) 2);

(6)

where we assume (cid:28) = 0:2, (cid:12) = ((cid:12)0; (cid:1) (cid:1) (cid:1) ; (cid:12)6)0 = (3; (cid:0)3; (cid:0)2:5; (cid:0)1; 1:5; 2; 1)0, and

h(vij ; yij; (cid:12)) = (cid:12)0 + vij4(cid:12)1 + vij5(cid:12)2 + vij6(cid:12)3 + vij7(cid:12)4 + vij8(cid:12)5 + yij(cid:12)6:

Here vijk denotes the kth element of vij . The simulation model for zij is a linear
regression model. We assume that part of the covariates (vij4; vij5) are involved in
the generation of yij and others (vij6; vij7; vij8) are not. Matching the earlier notation
p(zij j yij; vij; (cid:8)), we have (cid:8) = ((cid:12); (cid:28) 2), where (cid:12) is the vector of regression coe(cid:14)cients
and (cid:28) 2 is the variance parameter.

Conditional on the simulated data (Z; X; W ; V ), but pretending that Y is missing,
we generate a Monte Carlo sample from the posterior distribution p((cid:12) j Z; X; V ; W )
under model (4). See the Appendix for details of the posterior simulation.

We repeat the described simulation K = 100 times. For the kth simulation, we

(k)

save the simulation truth Y (k) and (cid:12), the imputed values ^Y
e(cid:11)ects ^(cid:12)
as marginal posterior expectations under p(Y j
Z; X; V ; W ) and p((cid:12) j Z; X; V ; W ), respectively. The mean squared error (MSE) for
Y is de(cid:12)ned as

, and the estimated

. We obtain ^Y

and ^(cid:12)

(k)

(k)

(k)

M SEY =

:

K

1

Km

(^y(k)

Xi;j

Xk=1

ij (cid:0) y(k)

ij )29=
;

8<
:
Xk=1n( ^(cid:12)(k)
p (cid:0) (cid:12)p)2o ; p = 0; 1; (cid:1) (cid:1) (cid:1) ; 6:

Similarly, for (cid:12) we de(cid:12)ne

M SE(cid:12)p =

1
K

K

For comparison we record results under two di(cid:11)erent models.

Zhang, S., Shih, Y., M(cid:127)uller, P.

619

Table 1: MSE from Simulation to Compare SBART and BART

(a)

0.0055
0.0078
0.0017
0.0029
0.0048
0.0079
0.0136
0.596

(b)

0.0062
0.0154
0.0045
0.0030
0.0048
0.0090
0.0321
3.864

(cid:12)0
(cid:12)1
(cid:12)2
(cid:12)3
(cid:12)4
(cid:12)5
(cid:12)6
Y

Column (a) under SBART; Column (b) under BART.

M1: Model (4) with a U (0; 1) prior for (cid:26), an IG(0:001; 0:001) prior for (cid:14)2, and a CAR

prior for (cid:18). This is the proposed SBART model.

M0: Model (4) with (cid:18) = 0. This is a BART model without spatial adjustment. Under

the BART model, the priors p((cid:18) j (cid:26); (cid:14)2), p((cid:26)) and p((cid:14)2) are not needed.

The remaining prior choices include a normal prior for (cid:12), p((cid:12)) = N (0; 100I 6), and an
inverse Gamma prior for (cid:28) 2, p((cid:28) 2) = IG(0:001; 0:001). Here 0 is a vector of 00s and I 6
is an identity matrix of dimension 6. For the hyper-parameters in p(f ) and p((cid:27)2), we
use the default setting recommended by Chipman et al. (2006a).

Table 1 compares the MSE from models M1 and M0. The results suggest that
when spatial correlation is present, incorporating spatial e(cid:11)ects improves the estimation
of regression coe(cid:14)cients. This is particularly true for (cid:12)6, the coe(cid:14)cient of the missing
In the simulation, the MSE of (cid:12)6 is reduced
variable, which is of primary interest.
from 0:0321 to 0:0136. A byproduct of the proposed approach is the inference about
the missing variable, which might be of interest to researchers by itself. Monte Carlo
sample averages evaluate posterior means and provide point estimates of the missing
variables. Other summaries characterize the uncertainty of the imputation. Table 1
shows that incorporating spatial e(cid:11)ects greatly improves the imputation of the missing
variable. The MSE for Y is reduced from 3:864 to 0:596. This improvement can also
be seen in Figure 1, where we plot Y (k) versus ^Y

from one simulation.

(k)

The estimated spatial correlation parameter ^(cid:26)(k) has a mean 0.414 and a standard
deviation 0.091, suggesting a slight overestimation of (cid:26). The histogram of ^(cid:26)(k) is plot-
ted in Figure 2. We also plot (cid:18)(k) against ^(cid:18)
, the true and estimated values of (cid:18),
respectively, from one simulation in Figure 3. The fact that the points fall around the
45 degree line suggests that the method successfully recovers the spatial pattern.

(k)

620

Spatial BART to Merge Datasets

Model (a)

Model (b)

d
e

t

u
p
m

i

5
2

0
2

5
1

0
1

5

0

d
e

t

u
p
m

i

5
2

0
2

5
1

0
1

5

0

0

5

10

15

20

25

0

5

10

15

20

25

true

true

Figure 1: Simulation example. The imputation of Y under M1 and M0 (under one
simulation). M1 uses the SBART model. M0 uses the BART model without spatial
random-e(cid:11)ects.

r

y
t
i
s
n
e
D

6

5

4

3

2

1

0

0.2

0.3

0.4

0.5

0.6

0.7

Figure 2: Simulation example. Histogram of p( ^(cid:26)(k) j data).

Zhang, S., Shih, Y., M(cid:127)uller, P.

621

q

t

d
e
u
p
m

i

.

0
1

.

5
0

0
0

.

0

.

1
−

0

.

2
−

−2.0

−1.5

−1.0

−0.5

0.0

0.5

1.0

true

Figure 3: Simulation example. Simulation truth and imputed values of (cid:18).

622

Spatial BART to Merge Datasets

4 Joint Inference with the CPS and SIPP Surveys

We evaluate the proposed approach with real survey data. In this evaluation, we ap-
ply our method to explore the relationship between self-perceived health status and
income using two di(cid:11)erent surveys. One survey includes data on health status, income,
and other variables (Z; Y ; V ). The second survey reports income and other variables
(X; W ).

We implement inference through the proposed approach without using the observed
values of income Y in the (cid:12)rst survey. That is, we carry out the analysis pretending
that we did not have income (Y ) information in the (cid:12)rst survey.

For comparison, we also implement inference with the observed Y values. Using

data from the (cid:12)rst survey only, we implement posterior simulation in the model

p(Z j Y ; V ; (cid:8)) (cid:1) p((cid:8));

(7)

and summmarize p((cid:8) j Z; Y ; V ). By comparing the inference with Y missing versus
inference conditional on Y , we will validate the proposed model.

4.1 The Datasets

We let D1 be a dataset extracted from the 2001 Current Population Survey (CPS),
March Supplement. The variable Z is self-perceived health status with values 1 to
5, where 1 denotes the best health status and 5 denotes the poorest health status.
The variable Y is de(cid:12)ned to be total personal income. We are interested in the rela-
tionship between Z and Y . The set of individual-level covariates are denoted by V ,
which include age, race, gender, education, health insurance coverage, marital status,
employment, industry and occupation. The dataset D2 comes from the 2001 Survey
of Income and Program Participation(SIPP), where total personal income X and the
other covariates W are collected. Both CPS and SIPP report income, denoted as Y in
CPS and X in SIPP. We pretend, however that Y is missing in D1 to illustrate and
validate the proposed method. CPS and SIPP are two independent surveys that each
collects information from a representative sample of the U.S. civilian noninstitutional
population. It is therefore reasonable to assume that (Y ; V ) and (X; W ) arise from
the same model.

CPS reports annual income while SIPP collects the information of monthly income.
To make the income variables consistent between two datasets, we scale them to a
common range of 0 to 1. Furthermore, personal income is known to be heavily skewed
to the right, which makes the normal assumption in (1) inappropriate. We carry out a
square root transformation to mitigate the problem. Thus eventually Y and X denote
the square root of the scaled personal income.

The (cid:12)nest available spatial area in both datasets is metropolitan statistical area
(MSA), which is de(cid:12)ned as a core area that contains a substantial population nucleus,
together with adjacent communities having a high degree of social and economic inte-
gration with that core. MSAs comprise one or more entire counties. In D1 and D2 there

Zhang, S., Shih, Y., M(cid:127)uller, P.

623

are altogether I = 239 MSAs. The original datasets from CPS and SIPP have more
than 90,000 and 260,000 records, respectively. For this illustrative analysis, we obtain
D1 and D2 by randomly sampling 10,000 observations from each of the two original
datasets.

4.2 Model Speci(cid:12)cation

Health status Z is an ordinal categorical variable. We construct an ordinal probit model
p(Z j Y ; V ; (cid:8)). We de(cid:12)ne the probit model by introducing a latent normal random
variable

(cid:17)ij j (cid:12); (cid:28) 2 (cid:24) N ((cid:12)0 + vij1(cid:12)1 + vij2(cid:12)2 + vij3(cid:12)3 + yij(cid:12)4; (cid:28) 2):

For given values of (cid:17)ij and a set of cut points c1; (cid:1) (cid:1) (cid:1) ; c4, we set

zij j (cid:17)ij = 8><
>:

1;
r;
5;

if (cid:17)ij (cid:20) c1;
if cr(cid:0)1 < (cid:17)ij (cid:20) cr for r = 2; 3; 4;
if (cid:17)ij > c4;

(8)

where (cid:12) = ((cid:12)0; (cid:1) (cid:1) (cid:1) ; (cid:12)4)0. See, for example, Johnson and Albert (1999) for a discussion
of Bayesian inference in ordinal regression models, including the latent variable con-
struction used here. The latent variable (cid:17)ij is assumed to arise from a linear regression
model with covariates being personal income yij , health insurance coverage vij1, gender
vij2, and age vij3, and (cid:12) is the corresponding coe(cid:14)cient vector. Income and age are
continuous; age ranges from 18 to 84; gender is binary with 0 indicating male and 1
indicating female; health insurance coverage is binary with 0 indicating covered and 1
indicating not covered. We de(cid:12)ne (cid:17) to be the collection of (cid:17)ij , and (cid:8) = ((cid:17); (cid:12); (cid:28) 2).
The cutpoints (c1; (cid:1) (cid:1) (cid:1) ; c4) are speci(cid:12)ed as (cid:12)xed. Random cutpoints would provide more
(cid:13)exibility. For example, Johnson and Albert (1999) jointly update the cutpoints and
the latent probit variable. However, the choice of the sampling model for Z j Y is not
directly related to the missing data problem. We assume (cid:12)xed cutpoints to keep the
model simple and keep the discussion focused.

The models p(yij j vij ; f; (cid:18); (cid:27)2) and p(xij j wij ; f; (cid:18); (cid:27)2) are de(cid:12)ned in (1). We
complete the model with priors for ((cid:26); (cid:14)2; (cid:12); (cid:28) 2). We assume di(cid:11)use priors, a uniform
prior for (cid:26), p((cid:26)) = U (0; 1), an inverse Gamma prior for (cid:14)2, p((cid:14)2) = IG(0:001; 0:001),
independent normal priors for (cid:12)p, p((cid:12)p) = N (0; 100), p = 0; (cid:1) (cid:1) (cid:1) ; 4, and an inverse
Gamma prior for (cid:28) 2, p((cid:28) 2) = IG(0:001; 0:001). We use default values recommended in
Chipman et al. (2006a) for the hyper-parameters of p(f ) and p((cid:27)2).

4.3

Implementation Details

Some practical issues arise in the application to real data. First, in (cid:12)tting the model
p(yij j vij ; f; (cid:18); (cid:27)2), we can use the entire vector of vij . There is no need for formal
variable selection. As pointed out by Chipman et al. (2006a), the BART model is a
nonparametric Bayesian regression approach which uses dynamic random basis elements

624

Spatial BART to Merge Datasets

that are dimensionally adaptive. Variable selection is already part of the model.
In
contrast, p(zij j yij; vij ; (cid:8)) is a generalized linear model and inference can be sensitive
to correlation among the covariates (yij; vij ). Like any other regression analysis, the
speci(cid:12)cation of p(zij j yij ; vij; (cid:8)) requires a good understanding of the research questions
to identify the relevant covariates. Importantly, high linear correlation among (yij; vij )
complicates interpretation and should be avoided. With yij missing, we use (xij ; wij )
instead to check for linear correlation among the covariates.

Another issue concerns a bias in the inference on (cid:8) induced by the imputation
of yij . Figure 4 clearly shows a shrinkage e(cid:11)ect. An ideal imputation would have a
scatter plot falling around the 45 degree line.
In Figure 4 the range of the imputed
values is much narrower compared with that of the true values. Chipman et al. (2006a)
observed similar shrinkage in a simulation study, which they attributed to extreme
extrapolation. That is, when we make prediction outside the observed data, because
of lack of information, the prior takes over and the imputed values are shrunk towards
the center. We believe, however, that the cause of shrinkage in Figure 4 is more than
extreme extrapolation. If the shrinkage arises from extrapolation alone, then it should
have equal e(cid:11)ect on both extremes. In Figure 4, we see more shrinkage on the higher
incomes than on the lower incomes. From this observation, we hypothesize that the
shrinkage is caused by a violation of the normality assumption in model (1). If personal
income is heavily skewed to the right, then the square root transformation does not
su(cid:14)ce to achieve normality, and extremely high incomes are not correctly imputed.

We propose to address the issue of shrinkage through the following two steps. First
we carry out a preliminary analysis using model (4). We compare the distribution of
imputed income ^Y based on p(Y j Z; X; V ; W ) with the observed income distribution
from D2. We use a deterministic adjustment to match some features of these two
distributions. For example, in this study we construct a linear transformation of the
imputed values, t(^yij ) = a^yij + b, such that some selected quantiles (for example, the
10th and 90th quantiles) of t(^yij ) match those of X, the incomes observed in D2. In
the second step, we replace p(zij j yij; vij ; (cid:8)) in model (4) by

p(cid:3)(zij j yij; vij ; (cid:8)) (cid:17) p(zij j t(yij ); vij ; (cid:8));

(9)

and proceed with the (cid:12)nal analysis. Because t(yij ) is a one-to-one transformation of
yij , p(zij j yij ; vij; (cid:8)) and p(cid:3)(zij j yij ; vij; (cid:8)) de(cid:12)ne the same conditional distribution.
But the latter provides a better calibrated estimation of (cid:8) by adjusting for the e(cid:11)ect of
shrinkage. See Foster and Stine (2004) for more discussion about calibration.

E(cid:11)ectively, the proposed two steps use the SBART model to impute the rank of the
missing income variable, and use an observed distribution to set speci(cid:12)c values. This
approach is valid because both CPS and SIPP are conducted by the US Census Bureau
to collect information from representative samples of the US population.

This adjustment can be automated in each MCMC iteration, where we readjust the
values of a and b such that the selected quantiles of t(y(k)
ij ) match the corresponding
quantiles in the empirical distribution of X. We conducted a simple simulation study
to assess the performance of the automated adjustment. Because the shrinkage e(cid:11)ect

Zhang, S., Shih, Y., M(cid:127)uller, P.

625

true and imputed income

t

d
e
u
p
m

i

0
1

.

.

8
0

.

6
0

.

4
0

2
0

.

.

0
0

0.0

0.2

0.4

0.6

0.8

1.0

true

Figure 4: CPS survey: True and imputed income. Income is scaled between 0 and 1.
Note the severe shrinkage in the imputed income.

626

Spatial BART to Merge Datasets

Table 2: MSE from Simulation to Check Adjustment

(a)

0.0078
0.0238

(b)

0.0106
0.0240

(cid:12)6
Y

Column (a) with automated adjustment; Column (b) without adjustment.

is more obvious when the sample size is large, we set n = m = 2000. The simulation
truth is similar to the model assumed in Section 3, except that we drop the spatial
component (cid:18) to facilitate computation, and the residual e(cid:11)ects in X and Y are assumed
to have Student t distribution with 3 degree of freedom. The MSE of the estimated
regression coe(cid:14)cients and imputed Y are presented in Table 2. Because Table 2 is
based on simulations with a larger sample size and a simpler model, the MSE are much
smaller than those in Table 1. Our primary interest is in (cid:12)6, the regression coe(cid:14)cient
of Y . Without adjustment, the shrinkage e(cid:11)ect leads to overestimation of (cid:12)6. With the
automated adjustment, the MSE of (cid:12)6 is reduced from 0.0106 to 0.0078.

The simulation indicates that the adjustment can provide better calibrated estimates
when there is some shrinkage e(cid:11)ect induced by imputation of the missing variable.
However, we caution that such an adjustment for shrinkage is ad hoc, and it relies
heavily on the assumption that D1 and D2 are representative samples of the same
population. Researchers should carefully check this assumptions before implementing
the approach.

4.4 Results

Table 3 lists the posterior means and standard deviations of the regression coe(cid:14)cients
(cid:12) under three inference approaches, which are implemented by MCMC simulation.
One set of inference summaries is based on true income and model (7). This serves
as the gold standard. The second set of inferences is based on missing income and
model (4). The third set is based on missing income and model (9). Both model (4)
and model (9) are SBART models, the di(cid:11)erence being that model (9) adjusts for the
shrinkage e(cid:11)ect while model (4) does not. Table 3 shows that if we ignore the shrinkage
e(cid:11)ect, model (4) will lead to a conclusion that overstates the e(cid:11)ect of income. The
posterior means based on model (7) and (9) are similar, suggesting that our method
successfully merges information from two datasets and provides a good estimate of the
relationship between self-perceived health status and income. Due to the uncertainty
induced by imputing the missing income, the standard deviations under model (9) are
slightly larger. The estimated regression coe(cid:14)cients suggest that subjects with higher
income tend to have a better self-perceived health status. Women generally report
better self-perceived health. Additionally, younger age and health insurance coverage
are associated with better self-perceived health status. We plot the imputed income
based on samples from p(Y j Z; X; V ; W ) versus the true income in Figure 4. The
spatial correlation parameter (cid:26) has a posterior mean 0.362 and standard deviation 0.242,

Zhang, S., Shih, Y., M(cid:127)uller, P.

627

indicating a moderate spatial correlation. A histogram of the samples from its posterior
distribution is plotted in Figure 5.

Table 3: Real Data, Posterior Mean (Standard deviation) of (cid:12)

Intercept

Health insurance

Sex
Age

Income

model (7)

model (4)

model (9)

-4.157(0.073)
0.865(0.059)
-0.194(0.047)
0.057(0.001)
-2.513(0.126)

-3.982(0.075)
0.624(0.069)
-0.316(0.054)
0.052(0.001)
-3.392(0.216)

-4.019(0.075)
0.619(0.069)
-0.320(0.055)
0.052(0.001)
-2.677(0.167)

Model (7) uses true income; Model (4) uses SBART to impute \missing" income without
adjusting for shrinkage; Model (10) uses SBART to impute \missing" income and adjusts
for shrinkage.

r

y
t
i
s
n
e
D

5

.

1

0

.

1

5

.

0

0

.

0

0.0

0.2

0.4

0.6

0.8

1.0

Figure 5: CPS and SIPP surveys. Histogram of p((cid:26) j data).

Besides comparing our results with those based on the complete data, we also com-
pare with results from a census-based approach, which supplements missing individual-
level variables with aggregate information based on the neighborhood socioeconomic
pro(cid:12)le. With MSA being the (cid:12)nest available spatial area, we could supplement missing
yij with average personal income from MSAi. However, compared with the average by
census block or census track, the average by MSA is much coarser and would result
in a large imputation error. To achieve a fairer comparison with the proposed method
we instead proceed as follows. In the CPS dataset, about 41:5% of the records contain
county codes. To investigate the performance of census-based methods with (cid:12)ner area

628

Spatial BART to Merge Datasets

units, we create D(cid:3)
1 by randomly sampling 10,000 observations from those that have
county code in the original CPS dataset. We then replace the missing income with

county median income (denoted by eY ) from the US census. Conditioning on (Z; eY ; V )

we report inference on (cid:8) under model (7). This is the result from the census-based
method. Table 4 lists the posterior means (standard deviations) of (cid:12) from three pro-
cedures: (a) based on model (7) and true income; (b) the proposed method, based on
model (9) with missing income; (c) census-based method, based on model (7) and me-
dian income at county level. Because Table 3 is based on D1 while Table 4 is based
on D(cid:3)
1, the estimates in the two tables do not match exactly. The estimates from the
proposed method are close to those based on true incomes. This is not the case for the
estimates based on imputation by county median income. The estimated coe(cid:14)cients
of health insurance coverage and income are quite di(cid:11)erent from those based on true
incomes. Most strikingly, the estimated coe(cid:14)cient of sex switches the sign. This could
lead to very misleading conclusions. In summary, Table 4 shows that our model pro-
vides an improvement of the census-based method. This is true even though we have
improved the latter by using county median income while keeping our proposed method
at the MSA-level, a coarser spatial area.

Table 4: Comparing with Census-Based Method

Intercept

Health insurance

Sex
Age

Income

(a)

(b)

(c)

-4.220(0.073)
0.841(0.059)
-0.088(0.047)
0.054(0.001)
-2.230(0.127)

-4.129(0.075)
0.734(0.066)
-0.165(0.056)
0.052(0.002)
-2.335(0.164)

-4.380(0.076)
1.116(0.057)
0.148(0.046)
0.052(0.001)
-1.673(0.248)

Column (a) uses true income; column (b) uses SBART to impute missing income and
adjusts for shrinkage; column (c) uses county median income as imputation.

5 Discussion

We have developed an approach that allows researchers to borrow information across
surveys and investigate hypotheses that cannot be considered using only one dataset
alone. The proposed method is (cid:13)exible and fully model-based. The key assumption is
that (Y ; V ) and (X; W ) are independent samples from the same model. This assump-
tion allows researchers to apply the knowledge learned from (X; W ) to (Y ; V ). This
facilitates imputation of the missing Y . By specifying a (cid:13)exible SBART model, the
proposed method does not make restrictive assumptions about the speci(cid:12)c model for
(X; W ).

In the simulation study and the data analysis example we have assumed parametric
models for the regression of Z and Y . This parametric form, however, is not a require-
ment for the proposed approach. It is unrelated to the missingness of Y . Alternatively,
a non-parametric regression model could be used. The only caveat is that the increased

Zhang, S., Shih, Y., M(cid:127)uller, P.

629

uncertainty induced by the imputation of Y might make meaningful data analysis with
a non-parametric model di(cid:14)cult.

The proposed imputation of the missing variable is a data-driven procedure. That
is, in each MCMC iteration, we have a large number of trees such that each contributes
a small portion of the conditional mean. Therefore it is di(cid:14)cult to evaluate the re-
lationship between the missing variable and individual covariates. It is not a critical
issue if the primary interest is to explore the relationship between Z and Y , instead
of Y and V .
If the researchers are interested in the the marginal e(cid:11)ect of a single
predictor, partial dependence plots might be a useful tool. See Friedman (2001) and
Chipman et al. (2006a) for details.

Appendix: MCMC Sampling Schemes

We use MCMC posterior simulation to implement inference in model (4). See, for
example, Gamerman (1997) for a review of MCMC methods. In the following discussion
we use [U j (cid:1) (cid:1) (cid:1) ] to indicate that the random variable U is updated conditional on the
currently imputed values of all other parameters. The transition probability for the
implemented MCMC is de(cid:12)ned by the following steps.

Step 1. Updating (cid:8).

[(cid:8) j (cid:1) (cid:1) (cid:1) ] / p(Z j Y ; V ; (cid:8)) (cid:1) p((cid:8)):

The updating of (cid:8) depends on the speci(cid:12)c form of p(Z j Y ; V ; (cid:8)), which in our
example is either a linear regression model or an ordinal probit model. There are
well established methods to update parameters in such models. For example, see
Gelman et al. (2003) and Albert and Chib (1993).

Step 2. Updating f and (cid:27)2.

[f; (cid:27)2 j (cid:1) (cid:1) (cid:1) ] / p(X j W ; f; (cid:18); (cid:27)2)p(Y j V ; f; (cid:18); (cid:27)2)p(f )p((cid:27)2):

(10)

If we de(cid:12)ne x(cid:3)

ij = xij (cid:0) (cid:18)i and y(cid:3)

ij = yij (cid:0) (cid:18)i, then (10) is equivalent to

[f; (cid:27)2 j (cid:1) (cid:1) (cid:1) ] /Ynp(x(cid:3)

ij j wij ; f; (cid:27)2)oYnp(y(cid:3)

ij j vij ; f; (cid:27)2)op(f )p((cid:27)2);

with

(11)

p(x(cid:3)
p(y(cid:3)

ij j wij ; f; (cid:27)2) = N (f (wij ); (cid:27)2);
ij j vij ; f; (cid:27)2) = N (f (vij ); (cid:27)2):

Note that (11) is exactly a BART model with x(cid:3)
ij being the dependent
variable, and the updating algorithm can be found in Chipman et al. (2006a)
Section 4.

ij and y(cid:3)

630

Spatial BART to Merge Datasets

Step 3. Updating (cid:18).

[(cid:18) j (cid:1) (cid:1) (cid:1) ] / p(X j W ; f; (cid:18); (cid:27)2)p(Y j V ; f; (cid:18); (cid:27)2)p((cid:18) j (cid:26); (cid:14)2):

De(cid:12)ne eij = xij (cid:0) f (wij ) and sij = yij (cid:0) f (vij ), and use e and s to denote the
collection of eij and sij , respectively. We (cid:12)nd

[(cid:18) j (cid:1) (cid:1) (cid:1) ] / expn (cid:0)
expn (cid:0)

(cid:1)

1
2(cid:14)2

(cid:18)0(H (cid:0) (cid:26)C)(cid:18)o;

(e (cid:0) U x(cid:18))0(e (cid:0) U x(cid:18)) + (s (cid:0) U y(cid:18))0(s (cid:0) U y(cid:18))

2(cid:27)2

o

where U x and U y are the design matrix of (cid:18) corresponding to X and Y , re-
spectively. We can show that [(cid:18) j (cid:1) (cid:1) (cid:1) ] is a normal distribution with variance
[(U 0
xU x + U 0
yU y)=(cid:27)2 + (H (cid:0)
(cid:26)C)=(cid:14)2](cid:0)1(U 0

yU y)=(cid:27)2 + (H (cid:0) (cid:26)C)=(cid:14)2](cid:0)1 and mean [(U 0
xe + U 0

xU x + U 0

ys)=(cid:27)2.

Step 4. Updating (cid:26) and (cid:14)2.

[(cid:26); (cid:14)2 j (cid:1) (cid:1) (cid:1) ] / p((cid:18) j (cid:26); (cid:14)2)p((cid:26))p((cid:14)2):

CAR is a widely used spatial model and the posterior sampling of (cid:26) and (cid:14)2 has
been discussed extensively in literature. For example, see He and Sun (2000).

Step 5. Updating Y . We update Y one element at a time, i.e.,

[yij j (cid:1) (cid:1) (cid:1) ] / p(zij j yij ; vij; (cid:8))p(yij j vij; f; (cid:18); (cid:27)2):

Under model (6), a linear regression model, we have

[yij j (cid:1) (cid:1) (cid:1) ] / expn (cid:0)

1
2(cid:28) 2

(zij (cid:0) h(cid:3)

ij (cid:0) yij(cid:12)6)2o expn (cid:0)

1
2(cid:27)2

(yij (cid:0) f (vij ) (cid:0) (cid:18)i)2o;

ij = (cid:12)0 + vij4(cid:12)1 + vij5(cid:12)2 + vij6(cid:12)3 + vij7(cid:12)4 + vij8(cid:12)5. We can show that

where h(cid:3)
[yij j (cid:1) (cid:1) (cid:1) ] is normal with variance ((cid:12)2

6 =(cid:28) 2 + 1=(cid:27)2)(cid:0)1 and mean

6
(cid:28) 2

(cid:16) (cid:12)2

+

1

(cid:27)2(cid:17)(cid:0)1(cid:16) 1

(cid:28) 2 (cid:12)6(zij (cid:0) h(cid:3)

ij ) +

1
(cid:27)2

(f (vij ) + (cid:18)i)(cid:17):

Under model (8), an ordinal probit model, we have

[yij j (cid:1) (cid:1) (cid:1) ] / expn (cid:0)

1
2(cid:28) 2

((cid:17)ij (cid:0) h4

ij (cid:0) yij(cid:12)4)2o expn (cid:0)

1
2(cid:27)2

(yij (cid:0) f (vij ) (cid:0) (cid:18)i)2o;

ij = (cid:12)0 + vij1(cid:12)1 + vij2(cid:12)2 + vij3(cid:12)3. Thus [yij j (cid:1) (cid:1) (cid:1) ] is normal with variance

where h4
((cid:12)2

4=(cid:28) 2 + 1=(cid:27)2)(cid:0)1 and mean

+

4
(cid:28) 2

(cid:16) (cid:12)2

1

(cid:27)2(cid:17)(cid:0)1(cid:16) 1

(cid:28) 2 (cid:12)6(zij (cid:0) h4

ij ) +

1
(cid:27)2

(f (vij ) + (cid:18)i)(cid:17):

Zhang, S., Shih, Y., M(cid:127)uller, P.

631

References

Albert, J. H. and Chib, S. (1993). \Bayesian Analysis of Binary and Polychotomous
Response Data." Journal of the American Statistical Association, 88: 669{679. 629

Besag, J., York, J., and Molli, A. (1991). \Bayesian image restoration, with two appli-
cations in spatial statistics." Annals of the Institute of Statistical Mathematics, 43:
1{20, (Disc: pp21{59). 616

Breiman, L. (1996). \Bagging predictors." Machine Learning, 24: 123{140. 615

| (2001). \Random Forests." Machine Learning, 45: 5{32. 615

Byrne, C., Nedelman, J., and Luke, R. (1994). \Race, socioeconomic status, and the
development of end-stage renal disease." American Journal of Kidney Diseases, 23(1):
16{22. 612

Chipman, H. A., George, E. I., and McCulloch, R. E. (1998). \Bayesian CART Model
Search." Journal of the American Statistical Association, 93: 935{948, (C/R: P948{
960). 615

| (2006a). \BART: Bayesian Additive Regression Trees." Technical report, Depart-
ment of Mathematics and Statistics, Acadia University, Canada. 612, 614, 615, 616,
618, 619, 623, 624, 629

Chipman, H. A., George, E. I., McCulloch, R. E., and Musio, M. (2006b). \Spatial
BART." Abstract at the Valencia/ISBA Eighth World Meeting on Bayesian Statis-
tics, Valencia. 613

Clayton, D. and Kaldor, J. (1987). \Empirical Bayes estimates of age-standardized

relative risks for use in disease mapping." Biometrics, 43: 671{681. 616

Clyde, M. and Lee, H. (2001). \Bagging and Bayesian Bootstrap." In Richardson, T.

and Jaakkola, T. (eds.), Arti(cid:12)cial Intelligence and Statistics 2001, 169{174. 615

Cressie, N. A. C. (1993). Statistics for Spatial Data. John Wiley and Sons. 616

Denison, D. G. T., Mallick, B. K., and Smith, A. F. M. (1998). \A Bayesian CART

Algorithm." Biometrika, 85: 363{377. 615

Devesa, S. and Diamond, E. (1983). \Socioeconomic and racial di(cid:11)erences in lung cancer

incidence." American Journal of Epidemiology, 118(6): 818{831. 612

Foster, D. P. and Stine, R. A. (2004). \Variable Selection in Data Mining: Building a
Predictive Model for Bankruptcy." Journal of the American Statistical Association,
99(466): 303{313. 624

Freund, Y. and Schapire, R. (1997). \A decision-theoretic generalization of online learn-
ing and an application to boosting." Journal of Computer and System Sciences, 55:
119{139. 615

632

Spatial BART to Merge Datasets

Friedman, J. H. (1991). \Multivariate Adaptive Regression Splines." The Annals of

Statistics, 19: 1{67, (Disc: P67{141). 618

| (2001). \Greedy Function Approximation: A Gradient Boosting Machine." The

Annals of Statistics, 29(5): 1189{1232. 615, 629

Gamerman, D. (1997). Markov Chain Monte Carlo: Stochastic Simulation for Bayesian

Inference. Chapman & Hall Ltd. 629

Gelman, A., Carlin, J., Stern, H., and Rubin, D. (2003). Bayesian Data Analysis.

Chapman & Hall. 629

Geronimus, A. and Bound, J. (1998). \Use of census-based aggregate variables to proxy
for socioeconomic group: evidence from national samples." American Journal of
Epidemiology, 148(5): 475{486. 612

Gornick, M., Eggers, P., Reilly, T., Mentnech, R., Fitterman, L., Kucken, L., and
Vladeck, B. (1996). \E(cid:11)ects of race and income on mortality and use of services
among Medicare bene(cid:12)ciaries." New England Journal of Medicine, 335(11): 791{799.
612

He, Z. and Sun, D. (2000). \Hierarchical Bayes estimation of Hunting success rates with

spatial correlations." Biometrics, 56(2): 360{367. 630

Horton, N. J. and Laird, N. M. (1999). \Maximum Likelihood Analysis of Generalized
Linear Models with Missing Covariates." Statistical Methods in Medical Research, 8:
37{50. 611

Ibrahim, J. G., Chen, M.-H., Lipsitz, S. R., and Herring, A. H. (2005). \Missing-data
Methods for Generalized Linear Models: A Comparative Review." Journal of the
American Statistical Association, 100(469): 332{346. 611

Johnson, V. E. and Albert, J. H. (1999). Ordinal Data Modeling. Springer-Verlag Inc.

623

Kraus, J., Fife, D., Cox, P., Ramstein, K., and Conroy, C. (1986). \Incidence, sever-
ity, and external causes of pediatric brain injury." American Journal of Diseases of
Children, 140(7): 687{693. 612

Little, R. J. A. (1992). \Regression with Missing X’s: A Review." Journal of the

American Statistical Association, 87: 1227{1237. 611

Mandelblatt, J., Andrews, H., Kerner, J., Zauber, A., and Burnett, W. (1991). \Deter-
minants of late stage diagnosis of breast and cervical cancer: the impact of age, race,
social class, and hospital type." American Journal of Public Health, 81(5): 646{649.
612

Rubin, D. B. (1976). \Inference and Missing Data." Biometrika, 63: 581{590. 611

Schafer, J. L. and Graham, J. W. (2002). \Missing Data: Our View of the State of the

Art." Psychological Methods, 7(2): 147{177. 611

Zhang, S., Shih, Y., M(cid:127)uller, P.

633

Sun, D., Tsutakawa, R. K., and Speckman, P. L. (1999). \Posterior distribution of

hierarchical models using CAR(1) distributions." Biometrika, 86: 341{350. 616

Whittle, P. (1954). \On stationary processes in the plane." Biometrika, 41: 434{449.

616

634

Spatial BART to Merge Datasets

