To appear in Advances in Neural Information Processing Systems 13, eds.
T.K. Leen, T. Dietterich, V. Tresp, MIT Press, 2001.

Propagation Algorithms for Variational

Bayesian Learning

Zoubin Ghahramani and Matthew J. Beal

Gatsby Computational Neuroscience Unit

University College London

17 Queen Square, London WC1N 3AR, England

{zoubin,m.beal}@gatsby.ucl.ac.uk http://www.gatsby.ucl.ac.uk

Abstract

Variational approximations are becoming a widespread tool for
Bayesian learning of graphical models. We provide some theoret-
ical results for the variational updates in a very general family of
conjugate-exponential graphical models. We show how the belief
propagation and the junction tree algorithms can be used in the
inference step of variational Bayesian learning. Applying these re-
sults to the Bayesian analysis of linear-Gaussian state-space models
we obtain a learning procedure that exploits the Kalman smooth-
ing propagation, while integrating over all model parameters. We
demonstrate how this can be used to infer the hidden state dimen-
sionality of the state-space model in a variety of synthetic problems
and one real high-dimensional data set.

1 Introduction
Bayesian approaches to machine learning have several desirable properties. Bayesian
integration does not suﬀer overﬁtting (since nothing is ﬁt to the data). Prior knowl-
edge can be incorporated naturally and all uncertainty is manipulated in a consis-
tent manner. Moreover it is possible to learn model structures and readily compare
between model classes. Unfortunately, for most models of interest a full Bayesian
analysis is computationally intractable.
Until recently, approximate approaches to the intractable Bayesian learning prob-
lem had relied either on Markov chain Monte Carlo (MCMC) sampling, the Laplace
approximation (Gaussian integration), or asymptotic penalties like BIC. The recent
introduction of variational methods for Bayesian learning has resulted in the series
of papers showing that these methods can be used to rapidly learn the model struc-
ture and approximate the evidence in a wide variety of models. In this paper we
will not motivate advantages of the variational Bayesian approach as this is done in
previous papers [1, 5]. Rather we focus on deriving variational Bayesian (VB) learn-
ing in a very general form, relating it to EM, motivating parameter-hidden variable
factorisations, and the use of conjugate priors (section 3). We then present several
theoretical results relating VB learning to the belief propagation and junction tree
algorithms for inference in belief networks and Markov networks (section 4). Fi-
nally, we show how these results can be applied to learning the dimensionality of
the hidden state space of linear dynamical systems (section 5).

2 Variational Bayesian Learning
The basic idea of variational Bayesian learning is to simultaneously approximate the
intractable joint distribution over both hidden states and parameters with a simpler
distribution, usually by assuming the hidden states and parameters are independent;
the log evidence is lower bounded by applying Jensen’s inequality twice:

ln P (y|M) ≥ (cid:90) dθ Qθ(θ)(cid:20)(cid:90) dx Qx(x) ln P (x,y|θ,M)

+ ln P (θ|M)

Qθ(θ) (cid:21) (1)

Qx(x)

= F(Qθ(θ), Qx(x), y)

where y, x, θ and M, are observed data, hidden variables, parameters and model
class, respectively; P (θ|M) is a parameter prior under model class M. The lower
bound F is iteratively maximised as a functional of the two free distributions, Qx(x)
and Qθ(θ). From (1) we can see that this maximisation is equivalent to minimising
the KL divergence between Qx(x)Qθ(θ) and the joint posterior over hidden states
and parameters P (x, θ|y,M).
This approach was ﬁrst proposed for one-hidden layer neural networks [6] under the
restriction that Qθ(θ) is Gaussian. It has since been extended to models with hidden
variables and the restrictions on Qθ(θ) and Qx(x) have been removed in certain
models to allow arbitrary distributions [11, 8, 3, 1, 5]. Free-form optimisation with
respect to the distributions Qθ(θ) and Qx(x) is done using calculus of variations,
often resulting in algorithms that appear closely related to the corresponding EM
algorithm. We formalise this relationship and others in the following sections.

3 Conjugate-Exponential Models
We consider variational Bayesian learning in models that satisfy two conditions:
Condition (1). The complete data likelihood is in the exponential family:

P (x, y|θ) = f(x, y) g(θ) exp(cid:8)φ(θ)(cid:62)

u(x, y)(cid:9)

where φ(θ) is the vector of natural parameters, and u and f and g are the functions
that deﬁne the exponential family.
The list of latent-variable models of practical interest with complete-data likeli-
hoods in the exponential family is very long. We mention a few: Gaussian mixtures,
factor analysis, hidden Markov models and extensions, switching state-space mod-
els, Boltzmann machines, and discrete-variable belief networks.1 Of course, there
are also many as yet undreamed-of models combining Gaussian, Gamma, Poisson,
Dirichlet, Wishart, Multinomial, and other distributions.
Condition (2). The parameter prior is conjugate to the complete data likelihood:

P (θ|η, ν) = h(η, ν) g(θ)η exp(cid:8)φ(θ)(cid:62)

ν(cid:9)

where η and ν are hyperparameters of the prior.
Condition (2) in fact usually implies condition (1). Apart from some irregular cases,
it has been shown that the exponential families are the only classes of distributions
with a ﬁxed number of suﬃcient statistics, hence allowing them to have natural
conjugate priors. From the deﬁnition of conjugacy it is easy to see that the hyper-
parameters of a conjugate prior can be interpreted as the number (η) and values
(ν) of pseudo-observations under the corresponding likelihood. We call models that
satisfy conditions (1) and (2) conjugate-exponential.

1Models whose complete-data likelihood is not in the exponential family (such as ICA
with the logistic nonlinearity, or sigmoid belief networks) can often be approximated by
models in the exponential family with additional hidden variables.

In Bayesian inference we want to determine the posterior over parameters and
hidden variables P (x, θ|y, η, ν). In general this posterior is neither conjugate nor in
the exponential family. We therefore approximate the true posterior by the following
factorised distribution: P (x, θ|y, η, ν) ≈ Q(x, θ) = Qx(x)Qθ(θ), and minimise

KL(Q(cid:107)P ) =(cid:90) dx dθ Q(x, θ) ln

Q(x, θ)

P (x, θ|y, η, ν)

which is equivalent to maximising F(Qx(x), Qθ(θ), y). We provide several general
results with no proof (the proofs follow from the deﬁnitions and Gibbs inequality).
Theorem 1 Given an iid data set y = (y1, . . . yn), if the model satisﬁes conditions
(1) and (2), then at the maxima of F(Q, y) (minima of KL(Q(cid:107)P )):

Qθ(θ) = h(˜η, ˜ν)g(θ)˜η exp(cid:8)φ(θ)(cid:62) ˜ν(cid:9)

i=1 u(yi), and u(yi) = (cid:104)u(xi, yi)(cid:105)Q, using (cid:104)·(cid:105)Q

(a) Qθ(θ) is conjugate and of the form:

to denote expectation under Q.

where ˜η = η + n, ˜ν = ν +(cid:80)n
(b) Qx(x) =(cid:81)n
where φ(θ) = (cid:104)φ(θ)(cid:105)Q.

rameter posterior:

Qxi(xi) ∝ f(xi, yi) exp(cid:8)φ(θ)(cid:62)

i=1 Qxi(xi) and Qxi(xi) is of the same form as the known pa-

u(xi, yi)(cid:9) = P (xi|yi, φ(θ))

Since Qθ(θ) and Qxi(xi) are coupled, (a) and (b) do not provide an analytic so-
lution to the minimisation problem. We therefore solve the optimisation problem
numerically by iterating between the ﬁxed point equations given by (a) and (b), and
we obtain the following variational Bayesian generalisation of the EM algorithm:

VE Step: Compute the expected suﬃcient statistics t(y) = (cid:80)i u(yi)

under the hidden variable distributions Qxi(xi).
VM Step: Compute the expected natural parameters φ(θ) under the
parameter distribution given by ˜η and ˜ν.

∗

This reduces to the EM algorithm if we restrict the parameter density to a point
estimate (i.e. Dirac delta function), Qθ(θ) = δ(θ − θ
), in which case the M step
involves re-estimating θ
Note that unless we make the assumption that the parameters and hidden variables
factorise, we will not generally obtain the further hidden variable factorisation over
n in (b). In that case, the distributions of xi and xj will be coupled for all cases i, j
in the data set, greatly increasing the overall computational complexity of inference.

∗

.

4 Belief Networks and Markov Networks
The above result can be used to derive variational Bayesian learning algorithms for
exponential family distributions that fall into two important special classes.2
Let M be a
Corollary 1: Conjugate-Exponential Belief Networks.
conjugate-exponential model with hidden and visible variables z = (x, y) that sat-
P (z|θ) =(cid:81)j P (zj|zpj , θ). Then the approximating joint distribution for M satis-
isfy a belief network factorisation. That is, each variable zj has parents zpj and

ﬁes the same belief network factorisation:

Qz(z) =(cid:89)

j

Q(zj|zpj , ˜θ)

2A tutorial on belief networks and Markov networks can be found in [9].

where the conditional distributions have exactly the same form as those in the
original model but with natural parameters φ(˜θ) = φ(θ). Furthermore, with the
modiﬁed parameters ˜θ, the expectations under the approximating posterior Qx(x) ∝
Qz(z) required for the VE Step can be obtained by applying the belief propagation
algorithm if the network is singly connected and the junction tree algorithm if the
network is multiply-connected.
This result is somewhat surprising as it shows that it is possible to infer the hidden
states tractably while integrating over an ensemble of model parameters. This result
generalises the derivation of variational learning for HMMs in [8], which uses the
forward-backward algorithm as a subroutine.
Theorem 2: Markov Networks. Let M be a model with hidden and visible vari-
sity can be written as a product of clique-potentials ψj, P (z|θ) = g(θ)(cid:81)j ψj(Cj, θ),
ables z = (x, y) that satisfy a Markov network factorisation. That is, the joint den-
where each clique Cj is a subset of the variables in z. Then the approximating joint
distribution for M satisﬁes the same Markov network factorisation:

Qz(z) = ˜g(cid:89)

j

ψj(Cj)

where ψj(Cj) = exp{(cid:104)ln ψj(Cj, θ)(cid:105)Q} are new clique potentials obtained by averag-
ing over Qθ(θ), and ˜g is a normalisation constant. Furthermore, the expectations
under the approximating posterior Qx(x) required for the VE Step can be obtained
by applying the junction tree algorithm.
Corollary 2: Conjugate-Exponential Markov Networks. Let M be a
imating joint distribution for M is given by Qz(z) = ˜g(cid:81)j ψj(Cj, ˜θ), where the
conjugate-exponential Markov network over the variables in z. Then the approx-

clique potentials have exactly the same form as those in the original model but with
natural parameters φ(˜θ) = φ(θ).
For conjugate-exponential models in which belief propagation and the junction tree
algorithm over hidden variables is intractable further applications of Jensen’s in-
equality can yield tractable factorisations in the usual way [7].
In the following section we derive a variational Bayesian treatment of linear-
Gaussian state-space models. This serves two purposes. First, it will illustrate
an application of Theorem 1. Second, linear-Gaussian state-space models are the
cornerstone of stochastic ﬁltering, prediction and control. A variational Bayesian
treatment of these models provides a novel way to learn their structure, i.e.
to
identify the optimal dimensionality of their state-space.

5 State-space models
In state-space models (SSMs), a sequence of D-dimensional real-valued observation
vectors {y1, . . . , yT}, denoted y1:T , is modeled by assuming that at each time step
t, yt was generated from a K-dimensional real-valued hidden state variable xt, and
that the sequence of x’s deﬁne a ﬁrst-order Markov process. The joint probability
of a sequence of states and observations is therefore given by (Figure 1):

P (x1:T , y1:T ) = P (x1)P (y1|x1)

P (xt|xt−1)P (yt|xt).

T(cid:89)

t=2

We focus on the case where both the transition and output functions are linear and
time-invariant and the distribution of the state and observation noise variables is
Gaussian. This model is the linear-Gaussian state-space model:

xt = Axt−1 + wt,

yt = Cxt + vt

Figure 1: Belief network representation of a state-space model.

where A and C are the state transition and emission matrices and wt and vt are
state and output noise. It is straightforward to generalise this to a linear system
driven by some observed inputs, ut. A Bayesian analysis of state-space models using
MCMC methods can be found in [4].
The complete data likelihood for state-space models is Gaussian, which falls within
the class of exponential family distributions.
In order to derive a variational
Bayesian algorithm by applying the results in the previous section we now turn
to deﬁning conjugate priors over the parameters.
Priors. Without loss of generality we can assume that wt has covariance equal to
the unit matrix. The remaining parameters of a linear-Gaussian state-space model
are the matrices A and C and the covariance matrix of the output noise, vt, which
we will call R and assume to be diagonal, R = diag(ρ)−1, where ρi are the precisions
(inverse variances) associated with each output.
Each row vector of the A matrix, denoted a(cid:62)
i , is given a zero mean Gaussian prior
with inverse covariance matrix equal to diag(α). Each row vector of C, c(cid:62)
i , is
given a zero-mean Gaussian prior with precision matrix equal to diag(ρiβ). The
dependence of the precision of c(cid:62)
i on the noise output precision ρi is motivated by
conjugacy. Intuitively, this prior links the scale of the signal and noise.
The prior over the output noise covariance matrix, R, is deﬁned through the pre-
cision vector, ρ, which for conjugacy is assumed to be Gamma distributed3 with
exp{−bρi}. Here, α, β are
hyperparameters that we can optimise to do automatic relevance determination
(ARD) of hidden states, thus inferring the structure of the SSM.

hyperparameters a and b: P (ρ |a, b) =(cid:81)D

Γ(a) ρa−1

i=1

ba

i

Variational Bayesian learning for SSMs
Since A, C, ρ and x1:T are all unknown, given a sequence of observations y1:T , an
exact Bayesian treatment of SSMs would require computing marginals of the poste-
rior P (A, C, ρ, x1:T|y1:T ). This posterior contains interaction terms up to ﬁfth order
(for example, between elements of C, x and ρ), and is not analytically manageable.
However, since the model is conjugate-exponential we can apply Theorem 1 to de-
rive a variational EM algorithm for state-space models analogous to the maximum-
likelihood EM algorithm [10]. Moreover, since SSMs are singly connected belief
networks Corollary 1 tells us that we can make use of belief propagation, which in
the case of SSMs is known as the Kalman smoother.
Writing out the expression for log P (A, C, ρ, x1:T , y1:T ), one sees that it contains
interaction terms between ρ and C, but none between A and either ρ or C. This
observation implies a further factorisation, Q(A, C, ρ) = Q(A)Q(C, ρ), which falls
out of the initial factorisation and the conditional independencies of the model.
Starting from some arbitrary distribution over the hidden variables, the VM step
obtained by applying Theorem 1 computes the expected natural parameters of
Qθ(θ), where θ = (A, C, ρ).

3More generally, if we let R be a full covariance matrix for conjugacy we would give

−1 a Wishart distribution: P (V |ν, S) ∝ |V |(ν−D−1)/2 exp(cid:8)− 1

its inverse V = R
where tr is the matrix trace operator.

−1(cid:9) ,

2 tr V S

X3 Y3 X1Y1X2Y2XTYT(cid:48))−1U

(cid:62)

t=1 yti(cid:104)x(cid:62)

t

(cid:105) and W

2 , ˜bi = b + 1

t=2(cid:10)xt−1x(cid:62)

i , Ui = (cid:80)T

We proceed to solve for Q(A). We know from Theorem 1 that Q(A) is multivariate
Gaussian, like the prior, so we only need to compute its mean and covariance. A
(cid:62)(diag(α) + W )−1 and each row of A has covariance (diag(α) + W )−1,
has mean S

t=1 (cid:10)xtx(cid:62)
(cid:48) = W +(cid:10)xT x(cid:62)

t(cid:11), W = (cid:80)T−1
t(cid:11), and (cid:104).(cid:105) denotes averaging w.r.t.
where S = (cid:80)T
the Q(x1:T ) distribution.
2 gi, gi =(cid:80)T
Q(C, ρ) is also of the same form as the prior. Q(ρ) is a product of Gamma densities
Q(ρi) = G(ρi; ˜a, ˜bi) where ˜a = a + T
− Ui(diag(β) +
T(cid:11). Given ρ, each row of
t=1 y2
ti
W
(cid:48))−1/ρi and mean ¯ci =
C is Gaussian with covariance Cov(ci) = (diag(β) + W
ρi Ui Cov(ci). Note that S, W and Ui are the expected complete data suﬃcient
statistics u mentioned in Theorem 1(a). Using the parameter distributions the
hyperparameters can also be optimised.4
We now turn to the VE step: computing Q(x1:T ). Since the model is a conjugate-
exponential singly-connected belief network, we can use belief propagation (Corol-
lary 1). For SSMs this corresponds to the Kalman smoothing algorithm, where
every appearance of the natural parameters of the model is replaced with the fol-
lowing corresponding expectations under the Q distribution: (cid:104)ρici(cid:105), (cid:104)ρicic(cid:62)
(cid:105), (cid:104)A(cid:105),
(cid:104)A
(cid:62)
Like for PCA [3], independent components analysis [1], and mixtures of factor
analysers [5], the variational Bayesian algorithm for state-space models can be used
to learn the structure of the model as well as average over parameters. Speciﬁcally,
using F it is possible to compare models with diﬀerent state-space sizes and optimise
the dimensionality of the state-space, as we demonstrate in the following section.

A(cid:105). Details can be found in [2].

i

6 Results
Experiment 1: The goal of this experiment was to see if the variational method
could infer the structure of a variety of state space models by optimising over α and
β. We generated a 200-step time series of 10-dimensional data from three models:5
(a) a factor analyser (i.e. an SSM with A = 0) with 3 factors (static state variables);
(b) an SSM with 3 dynamical interacting state variables, i.e. A (cid:54)= 0; (c) an SSM
with 3 interacting dynamical and 1 static state variables. The variational Bayesian
method correctly inferred the structure of each model in 2-3 minutes of CPU time
on a 500 MHz Pentium III (Fig. 2 (a)–(c)).
Experiment 2: We explored the eﬀect of data set size on complexity of the recov-
ered structure. 10-dim time series were generated from a 6 state-variable SSM. On
reducing the length of the time series from 400 to 10 steps the recovered structure
became progressively less complex (Fig. 2(d)–(j)), to a 1-variable static model (j).
This result agrees with the Bayesian perspective that the complexity of the model
should reﬂect the data support.
Experiment 3 (Steel plant): 38 sensors (temperatures, pressures, etc) were
sampled at 2 Hz from a continuous casting process for 150 seconds. These sensors
covaried and were temporally correlated, suggesting a state-space model could cap-
ture some of its structure. The variational algorithm inferred that 16 state variables
were required, of which 14 emitted outputs. While we do not know whether this is
reasonable structure we plan to explore this as well as other real data sets.

4 The ARD hyperparameters become αk =

K(cid:104)A(cid:62)A(cid:105)kk

, and βk =

aD(cid:80)D

hyperparameters a and b solve the ﬁxed point equations ψ(a) = ln b + 1
b = 1
1
∼ Unif(−5, 5), and A chosen with eigen-values in [0.5, 0.9].

∂w ln Γ(w) is the digamma function.

(cid:104)ρi(cid:105), where ψ(w) = ∂

5Parameters were chosen as follows: R = I, and elements of C sampled from

i=1

i=1

(cid:104)C(cid:62)diag(ρ)C(cid:105)kk

. The
(cid:104)ln ρi(cid:105), and

D

D(cid:80)D

Figure 2: The elements of the A and C matrices after learning are displayed graphically.
A link is drawn from node k in xt−1 to node l in xt iﬀ 1
>  or
αk
> , for a small threshold . Similarly links are drawn from node k of xt to yt if 1
1
> .
αl
βk
Therefore the graph shows the links that take part in the dynamics and the output.

> , and either 1
βl

7 Conclusions
We have derived a general variational Bayesian learning algorithm for models in the
conjugate-exponential family. There are a large number of interesting models that
fall in this family, and the results in this paper should allow an almost automated
protocol for implementing a variational Bayesian treatment of these models.
We have given one example of such an implementation, state-space models, and
shown that the VB algorithm can be used to rapidly infer the hidden state dimen-
sionality. Using the theory laid out in this paper it is straightforward to generalise
the algorithm to mixtures of SSMs, switching SSMs, etc.
For conjugate-exponential models, integrating both belief propagation and the junc-
tion tree algorithm into the variational Bayesian framework simply amounts to com-
puting expectations of the natural parameters. Moreover, the variational Bayesian
algorithm contains EM as a special case. We believe this paper provides the founda-
tions for a general algorithm for variational Bayesian learning in graphical models.

References

[1] H. Attias. A variational Bayesian framework for graphical models. In Advances in

Neural Information Processing Systems 12, Cambridge, MA, 2000. MIT Press.

[2] M.J. Beal and Z. Ghahramani. The variational Kalman smoother. Technical report,

Gatsby Computational Neuroscience Unit, University College London, 2000.

[3] C.M. Bishop. Variational PCA. In Proc. Ninth ICANN, 1999.
[4] S. Fr¨uwirth-Schnatter. Bayesian model discrimination and Bayes factors for linear

Gaussian state space models. J. Royal Stat. Soc. B, 57:237–246, 1995.

[5] Z. Ghahramani and M. J. Beal. Variational inference for Bayesian mixtures of factor

analysers. In Adv. Neur. Inf. Proc. Sys. 12, Cambridge, MA, 2000. MIT Press.

[6] G.E. Hinton and D. van Camp. Keeping neural networks simple by minimizing the de-
scription length of the weights. In Sixth ACM Conference on Computational Learning
Theory, Santa Cruz, 1993.

[7] M.I. Jordan, Z. Ghahramani, T.S. Jaakkola, and L.K Saul. An introduction to vari-

ational methods in graphical models. Machine Learning, 37:183–233, 1999.

[8] D.J.C. MacKay. Ensemble learning for hidden Markov models. Technical report,

Cavendish Laboratory, University of Cambridge, 1997.

[9] J. Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Infer-

ence. Morgan Kaufmann, San Mateo, CA, 1988.

[10] R. H. Shumway and D. S. Stoﬀer. An approach to time series smoothing and forecast-

ing using the EM algorithm. Journal of Time Series Analysis, 3(4):253–264, 1982.

[11] S. Waterhouse, D.J.C. Mackay, and T. Robinson. Bayesian methods for mixtures of

experts. In Adv. Neur. Inf. Proc. Sys. 7, Cambridge, MA, 1995. MIT Press.

Xt−1XtYtXt−1XtYtXt−1XtYtXt−1XtYtXt−1XtYtXt−1XtYtXt−1XtYtXt−1XtYtXt−1XtYtXt−1XtYt