Abstract
Reinforcementlearningisatypeofunsupervisedlearningforsequentialdecisionmaking.Q-
learningisprobablythebest-understoodreinforcementlearningalgorithm.InQ-learning,the
agentlearnsamappingfromstatesandactionstotheirutilities.Animportantassumptionof
Q-learningistheMarkovianenvironmentassumption,meaningthatanyinformationneeded
todeterminetheoptimalactionsisre(cid:13)ectedintheagent'sstaterepresentation.Consider
anagentwhosestaterepresentationisbasedsolelyonitsimmediateperceptualsensations.
Whenitssensorsarenotabletomakeessentialdistinctionsamongworldstates,theMarkov
assumptionisviolated,causingaproblemcalledperceptualaliasing.Forexample,whenfacing
aclosedbox,anagentbasedonitscurrentvisualsensationcannotactoptimallyiftheoptimal
actiondependsonthecontentsofthebox.Therearetwobasicapproachestoaddressingthis
problem|usingmoresensorsorusinghistoryto(cid:12)gureoutthecurrentworldstate.This
paperstudiesthreeconnectionistapproacheswhichlearntousehistorytohandleperceptual
aliasing:thewindow-Q,recurrent-Q,andrecurrent-modelarchitectures.Empiricalstudyof
thesearchitecturesispresented.Theirrelativestrengthsandweaknessesarealsodiscussed.
ThisresearchwassupportedinpartbyFujitsuLaboratoriesLtdandinpartbytheAvionicsLab,Wright
ResearchandDevelopmentCenter,AeronauticalSystemsDivision(AFSC),U.S.AirForce,Wright-Patterson
AFB,OH-underContractF-	-C-,ArpaOrderNo.	.Theviewsandconclusions
containedinthisdocumentarethoseoftheauthorsandshouldnotbeinterpretedasrepresentingtheo(cid:14)cial
policies,eitherexpressedorimplied,ofFujitsuLaboratoriesLtdortheU.S.Government.

Keywords:reinforcementlearning,Markov/non-Markovdecisiontask,actionmodel,time-
delayneuralnetwork,recurrentneuralnetwork


















Contents
Introduction
PerceptualAliasing
ThreeMemoryArchitectures
TheOAONArchitectures
ExperimentalDesignsandResults
.Task:-CupCollection::::::::::::::::::::::::::::::
.Task:TaskWithRandomFeatures::::::::::::::::::::::
.Task:TaskWithControlErrors::::::::::::::::::::::::
.Task:PoleBalancing:::::::::::::::::::::::::::::::
Discussion
.ArchitectureCharacteristics::::::::::::::::::::::::::::
RelatedWork
Conclusion
ATheExperienceReplayAlgorithm
BTheParameterSettingsForTheExperiments
CThePoleBalancingProblem



Introduction
Inthereinforcementlearningparadigm,alearningagentcontinuallyreceivesinputsfromits
sensors,determinesitsactionbasedontheinputsanditscontrolpolicy,executesthataction,
andreceivesascalarreinforcementorpayo(cid:11).Thegoaloftheagentistoconstructanoptimal
policysoastomaximizeitsperformance,whichisoftenmeasuredbythediscountedcumulative
reinforcementthroughthefuture(orforshort,utility).Thebest-understoodreinforcement
learningalgorithmisprobablyQ-learning[Watkins,		][Lin,		].TheideaofQ-learning
istoconstructaQ-function:
()
Q(state;action)!utility
TheQ-functionisusedtopredictthediscountedcumulativereinforcement(alsocalledutility
orQ-value)foreachstate-actionpairgiventhattheagentisinthatstateandexecutesthatac-
tion.GivenaQ-function,thecontrolpolicyissimplytochoosetheactionaforwhichQ(x;a)
ismaximal.TheQ-functionisincrementallyconstructedbasedontemporaldi(cid:11)erence(TD)
methods[Sutton,	],whichcanberelatedtodynamicprogramming[Bartoetal.,		].
Givenastatetransition(x;a;y;r)meaningthatanactionainresponsetoastatexresults
inanewstateyandimmediatepayo(cid:11)r,theQ-functioncanbeupdatedinthefollowingway:
Q(x;a) Q(x;a)+(cid:17)(cid:1)(r+(cid:13)(cid:1)maxkQ(y;k)(cid:0)Q(x;a))
()
where(cid:13)isadiscountfactorand(cid:17)isthelearningrate.Notethatthisupdateruleisbasedon
TD((cid:21))with(cid:21)=.Theupdateruleusedinthisworkwasmoresophisticatedthan()and
used(cid:21)>.Foradetaileddescriptionofthealgorithm,seeAppendixAand[Lin,		][Lin,
		].WatkinshasshownthatQ-learningwillconvergeand(cid:12)ndtheoptimalpolicyundertwoprimary
conditions(alongwithafewweakones):()alook-uptablerepresentationisusedfortheQ-
function,and()theenvironmentisMarkovian.Thelattermeansthatatanygiventime,the
nextstateoftheenvironmentisdeterminedonlybythecurrentstateandtheactiontaken.In
suchenvironments,allinformationneededtodeterminethecurrentoptimalactionisre(cid:13)ected
inthecurrentstaterepresentation.Althoughthe(cid:12)rstconditiondisallowsgeneralization,when
thestatespaceislargeandthelook-uptablemethodisunacceptable,somegeneralization
methodisoftenusedinmodelingtheQ-function.Forexample,Lin[Lin,		][Lin,		]
hassuccessfullycombinedtheconnectionisterrorback-propagationalgorithmandQ-learning
tosolveseveralnontriviallearningproblems.
Considerareinforcementlearningagentwhosestaterepresentationisbasedononlyitsim-
mediatesensation.Whenitssensorsarenotabletomakeessentialdistinctionsamongworld
states,theMarkovassumptionmentionedaboveisviolated,causingaproblemcalledper-
ceptualaliasing[Whitehead&Ballard,		].Forexample,considerapackingtaskwhich
involvessteps:openabox,putagiftintoit,closeit,andsealit.Anagentdrivenonly
byitscurrentvisualperceptscannotaccomplishthistask,becausewhenfacingaclosedbox,
theagentdoesnotknowifagiftisalreadyintheboxandthereforecannotdecidewhether
tosealoropenthebox.Therearetwosolutionstothisproblem.The(cid:12)rstsolutionisto
activelychooseanotherperceptualaction,suchasmeasuringtheweightofthebox,toresolve



theambiguity.Thesecondsolutionistousehistoryinformation,suchaswhetheragiftwas
everputinthebox,tohelpdeterminethecurrentworldstate.
[Whitehead&Ballard,
Thereissomepreviousworkstudyingthe(cid:12)rstkindofsolutions
		][Tan,		].Inthispaperwefocusonthesecondkindofsolutions.Therestofthe
paperisorganizedasfollows.Firstwedistinguishtwotypesofperceptualaliasing,followed
byourthreeconnectionistarchitecturesfortheproblem.AnOAONnetworkarchitectureis
alsoproposedformodellearningandQ-learning.Empiricalstudiesofthesearchitecturesare
presented,andtheirrelativestrengthsandweaknessesarealsodiscussed.Sectiondiscusses
relatedwork.
PerceptualAliasing
Wedistinguishtwotypesofperceptualaliasing:voluntaryandinvoluntary[Chrisman,personal
communication].Consideranagentwithasetofsensingoperationswhichallowtheagent
tomakecompletedistinctionsamongworldstates.Becausesomesensingoperationsarevery
costly(duetoeithertimeconstraintsorphysicalresourceconstraints),theagentmightchoose
toapplyonlysomesubsetofitsavailablesensingoperations.Ifitschoiceoflimitedsensing
operationstendstoperceptualaliasingwhichwouldnotoccurusingallavailableoperations,
thenwecallthisvoluntaryperceptualaliasing.
[Whitehead&Ballard,		]proposedalionalgorithm,whichlearnstofocusperceptual
[Tan,		]
attentioninordertocollectnecessarysensoryinformationforoptimalcontrol.
proposedacost-sensitivealgorithm,whichlearnstochooseminimalsensingoperationsto
disambiguateworldstates.Thesetwoalgorithmsmightbecalledsensorapproachesto(vol-
untary)perceptualaliasing.Incontrast,theapproachespresentedheremaybecharacterized
asmemory-based.
Involuntaryperceptualaliasingisduetolimitationsofsensorsavailabletoanagent.For
example,thecoloroftheobjectinaclosedboxcannotbeidenti(cid:12)edbyvision.Iftheoptimal
actiondependsonthatcolor,theagentmustrelyonitsmemoryratherthanonitssensors.
Usingmemoryorhistoryinformationtocontrolanon-Markovianenvironmentwillbereferred
toasmemoryapproachtoperceptualaliasing.Notethatsomevoluntaryperceptualaliasing
canalsobehandledthroughmemoryapproaches.Ontheotherhand,therearesituations
wherememoryapproachesfail.Considertheexampleinthebeginningofthisparagraph.
Iftheagentnevergotachancetoseethecoloroftheobjectbeforeitwaspackedinthe
box,memoryapproacheswillcertainlyfailinthissituation.Todealwithcomplexreal-world
problemse(cid:14)ciently,bothsensorandmemoryapproacheswillbeneeded.Integrationofboth
typesofapproachesremainstobeinvestigated.
ThreeMemoryArchitectures
Figuredepictsthreememoryarchitecturesforreinforcementlearninginnon-Markovian
domains.Oneisatypeofindirectcontrol,andtheothertwodirectcontrol.Thesearchitectures


utility

Q-net

utility

Q-net

memory

(b)

utility

Q-net

(c)

action

Model

memory

sensation

action

history features

sensation

action

sensation payoff

history features

sensation action

current sensation &
recent N sensations &

recent N actions
(a)

Figure:Threememoryarchitecturesforreinforcementlearninginnon-Markoviandomains:
(a)window-Qarchitecture,(b)recurrent-Qarchitecture,and(c)recurrent-modelarchitecture.
allusetemporaldi(cid:11)erencemethodstoincrementallylearnaQ-function,whichisrepresented
withneuralnetworks.
Insteadofusingjustthecurrentsensationasstaterepresentation,thewindow-Qarchitecture
usesthecurrentsensation,theNmostrecentsensations,andtheNmostrecentactionstaken
alltogetherasstaterepresentation.Inotherwords,thewindow-Qarchitectureallowsadirect
accesstotheinformationinthepastthroughaslidingwindow.Niscalledthewindowsize.
Thewindow-Qarchitectureissimpleandstraightforward,buttheproblemisthatwemaynot
knowtherightwindowsizeinadvance.Ifthewindowsizeischosentobenotlargeenough,
therewillbenotenoughhistoryinformationtoconstructanoptimalQ-function.Moreover,if
thewindowsizeneedstobelarge,thelargenumberofunitsintheinputlayerwillrequirealot
oftrainingpatternsforsuccessfullearningandgeneralization.Inspiteoftheseproblems,itis
stillworthstudy,sincethiskindoftime-delayneuralnetworkshasbeenfoundquitesuccessful
inspeechrecognition[Waibel,		]andseveralotherdomains.
Thewindow-Qarchitectureissortofabruteforceapproachtousinghistoryinformation.An
alternativeistodistilla(small)setofhistoryfeaturesoutofthelargeamountofinformation
inthepast.Thissetoffeaturestogetherwiththecurrentsensationbecometheagent'sstate


output units

hidden units

feedback

input units

context units

Figure:AnElmanNetwork.
representation.Theoptimalcontrolactionsthencanbedeterminedbasedonjustthisnew
staterepresentation,ifthesetofhistoryfeaturesre(cid:13)ectstheinformationneededforoptimal
control.Therecurrent-Qandrecurrent-modelarchitecturesillustratedinFigurearebasedon
thissameidea,butthewaystheyconstructhistoryfeaturesaredi(cid:11)erent.Unlikethewindow-
Qarchitecture,botharchitecturesinprinciplecandiscoverandutilizehistoryfeaturesthat
dependonsensationsarbitrarilydeepinthepast,althoughinpracticeitisdi(cid:14)culttoachieve
this.Recurrentneuralnetworks,suchasElmannetworks[Elman,		],provideawaytoconstruct
historyfeatures.AsillustratedinFigure,theinputlayerofanElmannetworkisdivided
intotwoparts:thetrueinputunitsandthecontextunits.Thecontextunitsholdfeedback
signalscomingfromthenetworkstateataprevioustimestep.Thecontextunits,which
functionasthememoryinFigure,rememberanaggregateofpreviousnetworkstates,and
sotheoutputofthenetworkdependsonthepastaswellasonthecurrentinput.
Therecurrent-QarchitectureusesarecurrentnetworktomodeltheQ-function.Topredict
utilityvaluescorrectly,therecurrentnetwork(calledrecurrentQ-net)willbeforcedtolearn
historyfeatureswhichenablethenetworktoproperlyassigndi(cid:11)erentutilityvaluestostates
withthesamesensation.
Therecurrent-modelarchitectureconsistsoftwoconcurrentlearningcomponents:
learning
anactionmodelandQ-learning.Anactionmodelisafunctionwhichmapsasensationand
anactiontothenextsensationandtheimmediatepayo(cid:11).Topredictthebehaviorofthe
environment,therecurrentactionmodelwillbeforcedtolearnasetofhistoryfeatures.Using
thecurrentsensationandthesetofhistoryfeaturesasstaterepresentation,wecanturna
non-MarkoviantaskintoMarkovianoneandsolveitusingtheconventionalQ-learning,ifa
perfectactionmodelisavailable.Thisissimplybecauseatanygiventime,thenextstateof
theenvironmentnowcanbecompletelydeterminedbythisnewstaterepresentationandthe
actiontaken.
Boththerecurrent-Qandrecurrent-modelarchitectureslearnhistoryfeaturesusingagradient
descentmethod(e.g.,errorback-propagation),buttheydi(cid:11)erinanimportantway.Formodel
learning,thegoalistominimizetheerrorsbetweenactualandpredictedsensationsandpayo(cid:11)s.



Inessence,theenvironmentprovidesalltheneededtraininginformation,whichisconsistent
overtimeaslongastheenvironmentdoesnotchange.ForrecurrentQ-learning,thegoal
istominimizetheerrorsbetweentemporallysuccessivepredictionsofutilityvalues[Sutton,
	]
[Lin,		].Theerrorsignalsherearecomputedbasedpartlyoninformationfrom
theenvironmentandpartlyonthecurrentapproximationtothetrueQ-function.Thelatter
changesovertimeandcarriesnoinformationatallinthebeginningoflearning.Inother
words,theseerrorsignalsareingeneralweak,noisyandeveninconsistentovertime.Because
ofthis,whethertherecurrent-Qarchitecturewilleverworkinpracticemaybequestioned.
Ingeneraltheactionmodelmustbetrainedtopredictnotonlythenewsensationbutalsothe
immediatepayo(cid:11).Consideranotherpackingtaskwhichinvolvessteps:putagiftintoan
openbox,sealtheboxsothatitcannotbeopenedagain,placetheboxintheproperbucket
dependingonthecolorofgiftinthebox.Arewardisgivenonlywhentheboxisplacedinthe
rightbucket.Notethattheagentisneverrequiredtoknowthegiftcolorinordertopredict
futuresensations,sincetheboxcannotbeopenedoncesealed.Thereforeamodelwhichonly
predictssensationswillnothavetherightfeatureswhichareneededtoaccomplishthistask.
However,foreachofthetasksdescribedinSection,theactionmodeldoesnotneedto
predicttheimmediatepayo(cid:11)inordertodiscoverhistoryfeaturesneededforoptimalcontrol.
Itisworthwhiletonotethatcombinationsofthethreearchitecturesarepossible.Forexample,
wecancombinethe(cid:12)rsttwoarchitectures:theinputstotherecurrentQ-netcouldinclude
notjustthecurrentsensationbutalsorecentsensations.Wecanalsocombinethelasttwo
architectures:thememoryissharedbetweenarecurrentmodelandarecurrentQ-net,and
historyfeaturesaredevelopedusingtheerrorsignalscomingfromboththemodelandthe
Q-net.Thispaperisonlyconcernedwiththethreebasicarchitectures.Furtherinvestigation
isneededtoseeifthesekindsofcombinationwillresultinbetterperformancethanthebasic
versions.
TheOAONArchitectures
BoththeQ-functionandactionmodeltaketwokindsofinputs:sensationandaction.There
aretwoalternativenetworkstructurestorealizethem.Oneisamonolithicnetworkwhose
inputunitsencodeboththesensationandtheaction.Fordomainswithdiscreteactions,this
structureisoftenundesirable,becausethemonolithicnetworkistobetrainedtomodela
highlynonlinearfunction|giventhesamesensation(andsamehistoryfeatures),di(cid:11)erent
actionsmaydemandverydi(cid:11)erentoutputsofthenetwork.
AnotheralternativeiswhatwecalltheOAON(OneActionOneNetwork)architectures.We
usemultiplenetworks,oneforeachaction,torepresenttheQ-functionandthemodel.Figure
illustratestwotypesofrecurrentOAONarchitectures:linearandnonlinear.ThelinearOAON
consistsofsingle-layerperceptrons,andthenonlinearoneconsistsofmultilayernetworksof
unitswithanonlinearsquashingfunction.Atanygiventime,onlythenetworkcorresponding
totheselectedactionisusedtocomputethenextvaluesoftheoutputandcontextunits,while
theotherscanbeignored.Itisimportanttonotethatwealsohavetoensurethatthemultiple
networkswillusethesame(distributed)representationofhistoryfeatures.Thisisachieved


output

output

(a)

sensation

history features

(b)

sensation

history features

shared

w

output units

output units

input
units

context
units

input
units

context
units

input
units

context
units

input
units

context
units

}
hidden units

}
hidden units

{{
output units

{{
output units

Figure:TwoOAONarchitecturesforrecurrentmodelsandrecurrentQ-nets:(a)linearand
(b)nonlinear.Thelinearoneconsistsofmultiple(here,)perceptrons,andthenonlinearone
consistsof(modi(cid:12)ed)Elmannetworks.Inbothcases,atanygiventime,onlythenetwork
correspondingtotheselectedactionisusedtocomputethenextvaluesoftheoutputand
contextunits.
byhavingthenetworksshareactivationsofcontextunits,andcanbefurtherreinforced(in
thecaseofthenonlinearone)byhavingthenetworksshareallconnectionsemittingfromthe
hiddenlayer.Asfoundempirically,thesharingofconnectionsdoesnotseemnecessarybut
tendstohelp.
AsshowninFigure,theinputlayerofeachnetwork(eitherlinearornonlinear)isdivided
intotwoparts:thetrueinputunitsandthecontextunits,aswehaveseenintheElman
network(Figure).NotethateachoftherecurrentnetworksinFigure.bisamodi(cid:12)ed
Elmannetwork.Therearethreedi(cid:11)erencesbetweenthe\standard"Elmannetworkandour
modi(cid:12)edversion.FirsttheElmannetworkdoesnotuseback-propagationthroughtime,but
oursdoes.(Alsoseebelow.)Second,thewholehiddenlayeroftheElmannetworkisfedback
totheinputlayer,buthereonlyaportionofhiddenunitsisfedback.Consideranenvironment
withlittleperceptualaliasing;inotherwords,feed-forwardnetworksaresu(cid:14)cienttomodel
mostaspectsoftheenvironment,andjustonehistoryfeatureneedstobediscoveredinorder
tomodelthewholeenvironment.Insuchacase,itmakessensetohavemanyhiddenunits
butjustonecontextunit.Fromourlimitedexperience,theElmannetworktendedtoneed
morecontextunitsthanthismodi(cid:12)edone.Asaconsequence,theformernormallyrequired
moreconnectionsthanthelatter.Furthermore,sincethecontextunitsarealsopartofthe
inputstotheQ-net,morecontextunitsrequiremoretrainingpatternsandtrainingtimeon



thepartofQ-learning.Thethirddi(cid:11)erenceisintheconstantfeedbackweight,thewinFigure
.b.TheElmannetworkusesw=,whilewefoundthatwslightlygreaterthan(say,.)
tendstomakethenetworkconvergesooner.Thereasonitmighthelpisthefollowing:Inthe
earlyphaseoftraining,theactivationsofcontextunitsarenormallyclosetozero(weuseda
symmetricsquashingfunction).Therefore,thecontextunitsappearunimportantcompared
withtheinputs.Bymagnifyingtheiractivations,theback-propagationalgorithmwillpay
moreattentiontothecontextunits.
ThelinearOAONarchitectureshowninFigure.aisinfacttheSLUGarchitectureproposed
by[Mozer&Bachrach,		],whohaveappliedSLUGtolearntomodel(cid:12)nitestateautomata
(FSA)anddemonstrateditssuccessforseveraldomains.(TheydidnotuseSLUGtomodela
Q-function,however).Theyalsofoundthattheconventionalrecurrentnetworks,suchasthe
Elmannetwork,were\spectacularlyunsuccessful"atmodelingFSA's.Thismaybeexplained
bythefactthattheirexperimentstookthemonolithicapproach(onenetworkforallactions).
Incontrast,ourexperimentsusingthenonlinearOAONarchitecturewerequitesuccessful.
ItalwayslearnedperfectlytheFSA'sthatSLUGwasabletolearn,althoughitseemedthat
moretrainingdatawouldbeneededtoacquireperfectmodelsduetothefactthatnonlinear
networksoftenhavemoredegreesoffreedom(connections)thanlinearones.
TomodelFSA's,weprobablydonotneedanonlinearOAON;linearOAONs(SLUG)may
su(cid:14)ceandoutperformnonlinearOAONs.Butformanyapplications,anonlinearOAONis
necessaryandcannotbereplacedbythelinearone.Forexample,Q-functionsareingeneral
nonlinear.Forapolebalancertobediscussedlater,itsactionmodelisalsononlinear.For
thesakeofcomparingresultsofourvariousexperiments,weusednonlinearOAONforallthe
recurrentQ-netsandmodelsinalltheexperimentstobepresentedbelow.
Boththehiddenunitsandtheoutputunits(inthenonlinearcase)usedasymmetricsigmoid
squashingfunctiony==(+e(cid:0)x)(cid:0):.Aslightlymodi(cid:12)edversionoftheback-propagation
algorithm[Rumelhartetal.,	]wasusedtoadjustnetworkweights.Back-propagation
throughtime(BPTT)[Rumelhartetal.,	]wasusedandfoundtobesigni(cid:12)cantimprove-
mentoverback-propagationwithoutBPTT.ToapplyBPTT,therecurrentnetworkswere
completelyunfoldedintime,errorswerethenback-propagatedthroughthewholechainof
networks,and(cid:12)nallytheweightsweremodi(cid:12)edaccordingtothecumulativegradients.To
restrictthein(cid:13)uenceofoutputerrorsattimetonthegradientscomputedattimesmuch
earlierthant,weappliedadecaytotheerrorswhentheywerepropagatedfromthecontext
unitsattimettothehiddenlayerattimet(cid:0).Betterperformancewasfoundwiththis
decaythanwithoutit.Thedecayweusedherewas.	.Thecontextunitswereinitializedto
atthestartofeachtaskinstance.
Wemustclarifywhatwemeanbybeingabletolearnaperfectmodel.Givenany(cid:12)nitesetofinput-output
patternsgeneratedbyanunknown(cid:12)nitestateautomaton(FSA),therearein(cid:12)nitenumberofFSA'swhichcan
(cid:12)tperfectlythetrainingdata.Becauseofthis,atanygivenmoment,amodellearningalgorithmcannever
besurewhetherithaslearnedexactlythesameFSAastheoneproducingthetrainingdata,unlesssome
characteristic(forexample,theupperboundofsize)ofthetargetFSAisknowninadvance.Therefore,when
wesayourarchitectureslearnsomeFSAperfectly,wemeanthatitcanpredictanenvironmentperfectlyfor
alongperiodoftime,sayathousandofsteps.



0 otherwise

3 actions: walk left, walk right & pick up
4 binary inputs: left cup, right cup, left collision & right collision
reward: 1 when the last cup is picked up

2 possible initial states:Figure:Task:A-cupcollectiontask.
ExperimentalDesignsandResults
Inthissectionwedescribeourstudyofthethreearchitecturesforsolvingvariouslearning
problemswithdi(cid:11)erentcharacteristics.Throughthestudy,weexpecttogainmoreinsights
intothesearchitectures,suchaswhetherthesearchitecturesworkandwhichonemaywork
bestforwhichtypesofproblems.
Severalparameterswereusedintheexperiments,suchasthediscountfactor(cid:13),therecency
factor(cid:21)usedinTD((cid:21))methods,thelearningrate,thenumberofcontextunits,thenumber
ofhiddenunits,thewindowsize,etc.Severalofthem,however,were(cid:12)xedforallofthe
experiments.AppendixBgivesadetaileddescriptionoftheparametersettingsweused,
whichwerechosentogiveroughlythebestperformance.
TheexperiencereplayalgorithmdescribedinAppendixAwasusedtosigni(cid:12)cantlyreducethe
numberoftrialsneededtolearnanoptimalcontrolpolicy.Byexperiencereplay,thelearning
agentremembersitspastactionsequencesandrepeatedlyappliesitslearningalgorithmto
eachsequenceinchronologicallybackwardorder.
Inalloftheexperimentshere,onlythe
experiencefromthemostrecenttrialswasreplayedtotraintheQ-function,whilethe
actionmodelwastrainedonalltheexperienceinthepast.
Eachexperimentconsistedoftwointerleavedphases:alearningphaseandatestphase.In
thelearningphase,theagentchoseactionsstochasticallybasedonaBoltzmanndistribution
[Lin,		].Thisrandomnessinactionselectionensuredsu(cid:14)cientexplorationbytheagent
duringlearning.Inthetestphase,theagentalwaystookthebestactionsaccordingtoits
currentpolicy.Thelearningcurvesshownbelowdescribeperformanceinthetestphase.
.Task:-CupCollection
Westartedwithasimple-cupcollectiontask(Figure).Thistaskrequiresthelearning
agenttopickuptwocupslocatedina-Dspace.Theagenthasactions:walkingrightone


cell,walkingleftonecell,andpick-up.Whentheagentexecutesthepick-upaction,itwill
pickupacupifandonlyifthecupislocatedattheagent'scurrentcell.Theagent'ssensation
includesbinarybits:bitsindicatingifthereisacupintheimmediateleftorrightcell,
andbitsindicatingifthepreviousactionresultsinacollisionfromtheleftortheright.An
actionattemptingtomovetheagentoutofthespacewillcauseacollision.
Thecupsareplacedfarenoughapartthatoncetheagentpicksupthe(cid:12)rstcup,itcannot
seetheotherone.Toactoptimally,theagenthastosomehowrememberthelocationofthe
secondcup.Thistaskisnottrivialforseveralreasons:)theagentcannotsenseacupin
frontofit,)theagentgetsnorewarduntilbothcupsarepickedup,and)theagentoften
operateswithnocupinsightespeciallyafterpickingupthe(cid:12)rstcup.Notethatwerestrict
theproblemsuchthatthereareonlytwopossibleinitialstatesasshowninFigure.The
reasonforthisrestrictionistosimplifythetaskandtoavoidperceptualaliasinginthevery
beginningwherenohistoryinformationisavailable.Theoptimalpolicyrequiresstepsto
pickupthecupsineachsituation.
Figureshowsthelearningcurvesforthethreearchitectures.Thesecurvesshowthemean
performanceoverrunswithdi(cid:11)erentseedsfortherandomnumbergenerator.TheYaxis
indicatesthenumberofstepstopickupthefourcupsinbothtaskinstancesshowninFigure
beforetimeout(i.e.,steps).TheXaxisindicatesthenumberoftrialstheagenthas
attemptedsofar.Ineachtrialtheagentstartedwithoneofthetwopossibleinitialstates,
andstoppedeitherwhenbothcupswerepickedup,orelseaftertimeout.
Weexperimentedwithtwodi(cid:11)erentvaluesfortheparameter(cid:21).Twothingsareobviousfrom
thelearningcurves.First,with(cid:21)=:,allofthesearchitecturessuccessfullylearnedthe
optimalpolicy.Second,(cid:21)=:gavemuchbetterperformancethan(cid:21)=.Considerthe
situationinFigure,whereA{Eare(cid:12)veconsecutivestatesandFisabadstatethatdisplays
thesamesensationasStateD.With(cid:21)=,theTDmethodestimatestheutilityofState
C(forexample)basedonjusttheimmediatepayo(cid:11)RcandtheutilityofStateD.Since
StateDdisplaysthesamesensationasthebadstateF,theutilityofStateDwillbemuch
underestimated,untilfeaturesarelearnedtodistinguishStatesDandF.Beforethefeatures
arelearned,theutilitiesofStateCandtheprecedingstateswillalsomuchunderestimated.
Thisproblemismitigatedwhen(cid:21)>isused,becausetheTD((cid:21)>)methodestimates
theutilityofStateCnotonlybasedonRcandStateD,butalsobasedonallthefollowing
rewards(Rd,Re,etc)andstates(StateE,etc).
Thefollowingaresomeadditionalobservations,whicharenotshowninFigure:
ThewindowsizeN.TheperformanceinFigure.awasobtainedwithN=.Asamatter
offact,theoptimalpolicycouldbelearned(butonlyoccasionally)withN=.Imaginethat
theagenthaspickedupthe(cid:12)rstcupandiswalkingtowardsthesecondcup.Forafewsteps,
themostrecentsensations(includingthecurrentone)infactdonotprovidetheagentany
informationatall(namely,allsensationbitsareo(cid:11)).Howcouldtheagenteverlearnthe
optimalpolicywithN=?Thewayitworkedisthefollowing:Afterpickingupthe(cid:12)rst
cup,theagentdeterminestherightdirectiontomove.Lateron,theagentsimplyfollowsthe
generaldirectionthepreviousactionshasbeenheadedfor.Inotherwords,theagent'saction
choicesarethemselvesusedasamediumforpassinginformationfromthepasttothepresent.


Figure:PerformanceforTask:(a)thewindow-Qarchitecture,(b)therecurrent-Qarchi-
tecture,and(c)therecurrent-modelarchitecture.


A

B

C

Re

E

D

F

Rc

Rd

Figure:Adi(cid:14)cultsituationforTD().A{Eare(cid:12)veconsecutivestatesandFisabadstate.
Rc,Rd,andRearethepayo(cid:11)sreceivedonthestatetransitions.Also,StatesDandFdisplay
thesamesensation.
Robustness.Thepolicylearnedbythewindow-Qarchitectureisnotvery\robust",because
wecanfoolitinthefollowingmanner:Supposetheagentisfollowingitslearnedoptimal
policy.Wesuddenlyinterrupttheagentandforceittomoveintheoppositedirectionfora
fewsteps.Theagentthencannotresumethecupcollectiontaskoptimally,becauseitspolicy
willchoosetocontinueonthenewdirection,whichisoppositetotheoptimaldirection(seethe
lastparagraph).Incontrast,thepolicylearnedbytherecurrent-modelarchitecturewasfound
morerobust;itappearedtochooseactionsbasedontheactualworldstateratherthanonthe
actionstakenbefore.Wealsoexaminedthepolicylearnedbytherecurrent-Qarchitecture.It
appearedthattherecurrent-Qarchitecturelearnedapolicysomewherebetween|sometimes
itappearedtochooseactionsbasedontheactualworldstate,andsometimesbasedonits
recentactionchoices.Itwouldbeinstructivetoseewhathistoryfeatureswerelearnedinthe
modelversusintherecurrentQ-net.Weplantostudythisinthefuture.
Imperfectmodel.Theagentneverlearnedaperfectmodelwithintrials.Forinstance,
iftheagenthasnotseenacupforstepsormore,themodelnormallyisnotabletopredict
theappearanceofthecup.ButthisimperfectmodeldidnotpreventQ-learningfromlearning
anoptimalpolicy.Thisrevealsthattherecurrent-modelarchitecturedoesnotneedtolearn
aperfectmodelinordertolearnanoptimalpolicy.
Itneedstolearnonlytheimportant
aspectsoftheenvironment.Butsinceitdoesnotknowinadvancewhatareandwhatarenot
importanttothetask,itwillendupattemptingtolearneverythingincludingthedetailsof
theenvironmentthatarerelevanttopredictingthenextsensationbutunnecessaryforoptimal
control.
Computationtime.Boththerecurrent-Qandrecurrent-modelarchitecturesfoundthe
optimalpolicyafteraboutthesamenumberoftrials,buttheactualCPUtimetakenwasvery
di(cid:11)erent.Theformertookminutestocompletearun,whilethelattertookminutes.
Thisisbecausetherecurrent-modelarchitecturehadtolearnamodelaswellasaQ-netand
themodelnetworkwasmuchbiggerthantherecurrentQ-net.
Thisexperimentrevealedtwolessons:
(cid:15)Allofthethreearchitecturesworkedforthissimpleproblem.
(cid:15)Fortherecurrent-modelarchitecture,justapartiallycorrectmodelmayprovidesu(cid:14)cient
historyfeaturesforoptimalcontrol.Thisisgoodnews,sinceaperfectmodelisoften


di(cid:14)culttoobtain.
.Task:TaskWithRandomFeatures
TaskissimplyTaskwithtworandombitsintheagent'ssensation.Therandombits
simulatetwodi(cid:14)cult-to-predictandirrelevantfeaturesaccessibletothelearningagent.In
therealworld,thereareoftenmanyfeatureswhicharedi(cid:14)culttopredictbutfortunatelynot
relevanttothetasktobesolved.Forexample,predictingwhetheritisgoingtorainoutside
mightbedi(cid:14)cult,butitdoesnotmatterifthetaskistopickupcupsinside.Theability
tohandledi(cid:14)cult-to-predictbutirrelevantfeaturesisimportantforalearningsystemtobe
practical.
Figureshowsthelearningcurvesofthethreearchitecturesforthistask.Again,these
curvesarethemeanperformanceoverruns.Aswecansee,thetworandomfeaturesgave
littleimpactontheperformanceofthewindow-Qarchitectureortherecurrent-Qarchitecture,
whilethenegativeimpactontherecurrent-modelarchitecturewasnoticeable.
Therecurrent-modeldid(cid:12)ndtheoptimalpolicymanytimesaftertrials.Itjustcouldnot
stablizeontheoptimalpolicy;itoscillatedbetweentheoptimalpolicyandsomesub-optimal
policies.Wealsoobservedthatthemodeltriedinvaintoreducethepredictionerrorsonthe
tworandombits.Therearetwopossibleexplanationsforthepoorerperformancecompared
withthatobtainedwhentherearenorandomsensationbits.First,themodelmightfailto
learnthehistoryfeaturesneededtosolvethetask,becausemuchofthee(cid:11)ortwaswastedon
therandombits.Second,becausetheactivationsofthecontextunitsweresharedbetween
themodelnetworkandtheQ-net,achangetotherepresentationofhistoryfeaturesonthe
modelpartcouldsimplydestabilizeawell-trainedQ-net,ifthechangewassigni(cid:12)cant.The
(cid:12)rstexplanationisruledout,sincetheoptimalpolicyindeedwasfoundmanytimes.Totest
thesecondexplanation,we(cid:12)xedthemodelatsomepointoflearningandallowedonlychanges
totheQ-net.Insuchasetup,theagentfoundtheoptimalpolicyandindeedstucktoit.
Thisexperimentrevealstwolessons:
(cid:15)Therecurrent-Qarchitectureismoreeconomicthantherecurrent-modelarchitecturein
thesensethattheformerwillnottrytolearnahistoryfeatureifitdoesnotappearto
berelevanttopredictingutilities.
(cid:15)Apotentialproblemwiththerecurrent-modelarchitectureisthatchangestotherep-
resentationofhistoryfeaturesonthemodelpartmaycauseinstabilityontheQ-net
part.
.Task:TaskWithControlErrors
Noiseanduncertaintyprevailintherealworld.Tostudythecapabilityofthesearchitectures
tohandlenoise,weadded%controlerrorstotheagent'sactuators,sothat%ofthetime
theexecutedactionwouldnothaveanye(cid:11)ectontheenvironment.(Therandombitswere
removed.)Figureshowsthemeanperformanceofthesearchitecturesoverruns.Notethat


Figure:PerformanceforTask:(a)thewindow-Qarchitecture,(b)therecurrent-Qarchi-
tecture,and(c)therecurrent-modelarchitecture.


weturnedo(cid:11)thecontrolerrorswhenwetestedtheperformanceoftheagent,sotheoptimal
numberofstepsinthe(cid:12)gureisstill.
Inoutoftheruns,thewindow-Qarchitecturesuccessfullyfoundtheoptimalpolicy,while
intheothertworuns,itonlyfoundsuboptimalpolicies.Inspiteofthecontrolerrors,the
recurrent-Qarchitecturealwayslearnedtheoptimalpolicy(withlittleinstability).
Therecurrent-modelarchitecturealwaysfoundtheoptimalpolicyaftertrials,butagain
itspolicyoscillatedbetweentheoptimaloneandsomesub-optimalonesduetothechanging
representationofhistoryfeatures,muchashappenedinTask.Ifwecan(cid:12)ndsomewayto
stablizethemodel(forexample,bygraduallydecreasingthelearningratetoattheend),
weshouldbeabletoobtainastableandoptimalpolicy.
Inshort,welearnedtwolessonsfromthisexperiment:
(cid:15)Allofthethreearchitecturescanhandlesmallcontrolerrorstosomedegree.
(cid:15)Amongthearchitectures,recurrent-Qseemstoscalebestinthepresenceofcontrol
errors.
.Task:PoleBalancing
Thetraditionalpolebalancingproblemistobalanceapoleonacartgiventhecartposition,
poleangle,cartvelocity,andpoleangularvelocity.(SeeAppendixCforthedetaileddescrip-
[Anderson,	])as
tionoftheproblem.)Thisproblemhasbeenstudiedmanytimes(e.g.,
anontrivialcontrolproblemduetosparsereinforcementsignals,whichare(cid:0)whenthepole
fallspastdegreesandelsewhere.Taskisthesameproblemexceptthatonlythecart
positionandpoleanglearegiventothelearningagent.Tobalancethepole,theagentmust
learnfeatureslikevelocity.Inthisexperiment,apolicywasconsideredsatisfactorywhenever
thepolecouldbebalancedforoverstepsineachofthetesttrialswherethepolestarts
withanangleof,(cid:6),(cid:6),or(cid:6)degrees.(Themaximuminitialpoleanglewithwhichthe
polecanbebalancedinde(cid:12)nitelyisabout.degrees.)Inthetrainingphase,poleanglesand
cartpositionsweregeneratedrandomly.Theinitialcartvelocityandpolevelocityarealways
setto.WeusedN=inthisexperiment.
Theinputrepresentationweusedwasstraightforward:onereal-valuedinputunitforeachof
thepoleangleandcartposition.Tableshowsthenumberoftrialstakenbyeacharchitecture
beforeasatisfactorypolicywaslearned.Thesenumbersaretheaverageoftheresultsfrom
thebestoutofruns.(Asatisfactorypolicywasnotalwaysfoundwithintrials.).
Incontrast,memorylessQ-learningtook	trialstosolvethetraditionalpolebalancing
problem,inwhichtheagentisgiventhecartposition,poleangle,cartvelocity,andpole
angularvelocity.
Onelessonwaslearnedfromthisexperiment:
Inthispaper,wedidnotexperimentwithsensingerrors,whichwillbefuturework.Aspointedoutin
[Bachrach,		],totrainarecurrentmodelproperlyinnoisyenvironments,wemayneedtoalterthelearning
procedureslightly.Moreprecisely,currentsensationscannotbe%trustedforpredictingfuturesensations.
Instead,weshouldtrustthemodel'spredictedsensationsmoreasthemodelgetsbetter.


Figure:PerformanceforTask:(a)thewindow-Qarchitecture,(b)therecurrent-Qarchi-
tecture,and(c)therecurrent-modelarchitecture.


Table:Performanceforthepolebalancingtask.
method
window-Qrecurrent-Qrecurrent-model
#oftrials



(cid:15)Whiletherecurrent-Qarchitecturewasthemostsuitablearchitectureforthecupcol-
lectiontasks,itwasoutperformedbytheothertwoarchitecturesforthepolebalancing
task.
Discussion
Theaboveexperimentsprovidesomeinsightintotheperformanceofthethreememoryarchi-
tectures.Inthissection,weconsiderproblemcharacteristicsthatdeterminewhicharchitecture
ismostappropriatetowhichtaskenvironments.Thethreearchitecturesexhibitdi(cid:11)erentad-
vantageswhoserelativeimportancevarieswithtaskparameterssuchas:
(cid:15)Memorydepth.Oneimportantproblemparameteristhelengthoftimeoverwhich
theagentmustrememberpreviousinputsinordertorepresentanoptimalcontrolpolicy.
Forexample,thememorydepthforTaskis,asevidencedbythefactthatthewindow-
Qagentwasabletoobtaintheoptimalcontrolbasedonlyonawindowofsize.The
memorydepthforthepolebalancingtaskis.Notethatlearninganoptimalpolicy
mayrequiresalargermemorydepththanthatneededtorepresentthepolicy.
(cid:15)Payo(cid:11)delay.
Incaseswherethepayo(cid:11)iszeroexceptforthegoalstate,wede(cid:12)ne
thepayo(cid:11)delayofaproblemtobethelengthoftheoptimalactionsequenceleading
tothegoal.Thisparameterisimportantbecauseitin(cid:13)uencestheoveralldi(cid:14)culty
ofQ-learning.Asthepayo(cid:11)delayincreases,learninganaccurateQ-functionbecomes
increasinglydi(cid:14)cultduetotheincreasingdi(cid:14)cultyofcreditassignment.
(cid:15)Numberofhistoryfeaturestobelearned.Ingeneral,themoreperceptualalias-
inganagentfaces,themorehistoryfeaturestheagenthastodiscover,andthemore
di(cid:14)cultthetaskbecomes.Ingeneral,predictingsensations(i.e.,amodel)requiresmore
historyfeaturesthanpredictingutilities(i.e.,aQ-net),whichinturnrequiresmorehis-
toryfeaturesthanrepresentingoptimalpolicies.ConsiderTaskforexample.Onlytwo
binaryhistoryfeaturesarerequiredtodeterminetheoptimalactions:\isthereacupin
front?"and\isthesecondcupontheright-handsideorleft-handside?".Butaperfect
Q-functionrequiresmorefeaturessuchas\howmanycupshavebeenpickedupsofar?"
and\howfaristhesecondcupfromhere?".Aperfectmodelforthistaskrequiresthe
samefeaturesastheperfectQ-function.ButaperfectmodelforTaskrequireseven
morefeaturessuchas\whatisthecurrentstateoftherandomnumbergenerator?",while
aperfectQ-functionforTaskrequiresnoextrafeatures.
ItisimportanttonotethatwedonotneedaperfectQ-functionoraperfectmodel
inordertoobtainanoptimalpolicy.AQ-functionjustneedstoassignavaluetoeach


actioninresponsetoagivensituationsuchthattheirrelativevaluesareintheright
order,andamodeljustneedstoprovidesu(cid:14)cientfeaturesforconstructingagood
Q-function.
.ArchitectureCharacteristics
Giventheaboveproblemparameters,wewouldliketounderstandwhichofthethreearchi-
tecturesisbestsuitedtowhichtypesofproblems.Hereweconsiderthekeyadvantagesand
disadvantagesofeacharchitecture,alongwiththeproblemparameterswhichin(cid:13)uencethe
importanceofthesecharacteristics.
(cid:15)Recurrent-modelarchitecture.Thekeydi(cid:11)erencebetweenthisarchitectureandthe
recurrent-Qarchitectureisthatitslearningofhistoryfeaturesisdrivenbylearningan
actionmodelratherthantheQ-function.Onestrengthofthisapproachisthattheagent
canobtainbettertrainingdatafortheactionmodelthanitcanfortheQ-function,mak-
ingthislearningmorereliableande(cid:14)cient.Inparticular,trainingexamplesoftheaction
model(<sensation,action,next-sensation,payo(cid:11)>quadruples)aredirectlyobservable
witheachsteptheagenttakesinitsenvironment.Incontrast,trainingexamplesofthe
Q-function(<sensation,action,utility>triples)arenotdirectlyobservablesincethe
agentmustestimatethetrainingutilityvaluesbasedonitsownchangingapproximation
tothetrueQ-function.
Thesecondstrengthofthisapproachisthatthelearnedfeaturesaredependenton
theenvironmentandindependentoftherewardfunction(eventhoughtheactionmodel
maybetrainedtopredictrewardsaswellassensations),andthereforecanbereusedif
theagenthasseveraldi(cid:11)erentrewardfunctions,orgoals,tolearntoachieve.
(cid:15)Recurrent-Qarchitecture.Whilethisarchitecturesu(cid:11)erstherelativedisadvantage
thatitmustlearnfromindirectlyobservabletrainingexamples,ithastheo(cid:11)setting
advantagethatitneedonlylearnthosehistoryfeaturesthatarerelevanttothecontrol
problem.Thehistoryfeaturesneededtorepresenttheoptimalactionmodelareasuper-
setofthoseneededtorepresenttheoptimalQ-function.Thisiseasilyseenbynoticing
thattheoptimalcontrolactioncaninprinciplebecomputedfromtheactionmodel
(byusinglookaheadsearch).Thus,incaseswhereonlyafewfeaturesarenecessary
forpredictingutilitiesbutmanyareneededtopredictcompletelythenextstate,the
numberofhistoryfeaturesthatmustbelearnedbytherecurrent-Qarchitecturecanbe
muchsmallerthanthenumberneededbytherecurrent-modelarchitecture.
(cid:15)Window-Qarchitecture.Theprimaryadvantageofthisarchitectureisthatitdoes
nothavetolearnthestaterepresentationrecursively(asdotheothertworecurrent
networkarchitectures).Recurrentnetworkstypicallytakemuchlongertotrainthan
non-recurrentnetworks.Thisadvantageiso(cid:11)setbythedisadvantagethatthehistory
informationitcanusearelimitedtothosefeaturesdirectlyobservableinits(cid:12)xedwin-
dowwhichcapturesonlyaboundedhistory.
Incontrast,thetworecurrentnetwork
approachescaninprinciplerepresenthistoryfeaturesthatdependonsensationsthat
arearbitrarilydeepintheagent'shistory.

Giventhesecompetingadvantagesforthethreearchitectures,onewouldimaginethateach
willbethepreferredarchitecturefordi(cid:11)erenttypesofproblems:
(cid:15)Onewouldexpecttheadvantageofthewindow-Qarchitecturetobegreatestintasks
wherethememorydepthsarethesmallest(forexample,thepolebalancingtask).
(cid:15)Onewouldexpecttherecurrent-modelarchitecture'sadvantageofdirectlyavailable
trainingexamplestobemostimportantintasksforwhichthepayo(cid:11)delayisthelongest
(forexample,thepolebalancingtask).Itisinthesesituationsthattheindirectestima-
tionoftrainingQ-valuesismostproblematicfortherecurrent-Qarchitecture.
(cid:15)Onewouldexpecttheadvantageoftherecurrent-Qarchitecture|thatitneedonly
learnthosefeaturesrelevanttocontrol|tobemostpronouncedintaskswherethe
ratiobetweenrelevantandirrelevanthistoryfeaturesisthelowest(forexample,thecup
collectiontaskwithtworandomfeatures).Althoughtherecurrent-modelarchitecture
canacquiretheoptimalpolicyaslongasjusttherelevantfeaturesarelearned,the
drivetolearningtheirrelevantfeaturesmaycauseproblems.Firstofall,representing
theirrelevantfeaturesmayuseupmanyofthelimitedcontextunitsatthesacri(cid:12)ce
oflearninggoodrelevantfeatures.Secondly,aswehaveseenintheexperiments,the
recurrent-modelarchitectureisalsosubjecttoinstabilityduetochangingrepresentation
ofthehistoryfeatures|achangewhichimprovesthemodelisalsolikelytodeteriorate
theQ-function,whichthenneedstobere-learned.
Wedonotexpectthewindow-Qarchitecturetoworkverywellingeneral,becauseitcanfail
todisambiguateworldstatesevenwhenitswindowsizeNischosenlargeenoughtorepresent
theoptimalpolicy.Considerthecupcollectiontask.Afterpickingupthe(cid:12)rstcup,theagent
walksbackandforthforMsteps(M>N).(Notethatthiskindofrandomwalksoftenoccur
duringearlylearning.)Thewindow-Qagentwillnotknowwhichworldstateitiscurrentlyin,
becauseknowingthatrequiresmorehistoryinformationthanthatavailableinthewindow.(If
thewindow-Qagentfollowstheoptimalpath,theboundedhistorysu(cid:14)cestoidentifyevery
worldstatealongthepath.)Incontrast,awell-trainedrecurrentmodelcankeeptrackofstate
transitionsandbeabletoknowtheactualworldstateunderthesamesituation.
Thetappeddelaylinescheme,whichthewindow-Qarchitectureuses,hasbeenwidelyapplied
tospeechrecognition[Waibel,		]andturnedouttobequiteausefultechnique.However,
asmentionedabove,wedonotexpectittoworkaswellforcontroltasksasitdoesforspeech
recognition,becauseofanimportantdi(cid:11)erencebetweenthesetasks.Amajortaskofspeech
recognitionisto(cid:12)ndthetemporalstructurethatalreadyexistsinagivensequenceofspeech
phonemes.Whilelearningtocontrol,theagentmustlookforthetemporalstructuregenerated
byitsownactions.Iftheactionsaregeneratedrandomlyasitisoftenthecaseduringearly
learning,itisunlikelyto(cid:12)ndsensibletemporalstructureswithintheactionsequencesoasto
improveitsactionselectionpolicy.



RelatedWork
AsmentionedinSection,therearetwobasicapproachestoaddressingtheproblemof
[Whitehead&Ballard,		]
perceptualaliasing:sensorapproachesandmemoryapproaches.
and[Tan,		]areexamplesofsensorapproaches.Bothofthesensorapproachesassumethe
existenceofsensoryoperationsbywhichtheactualworldstatecanbeunambiguouslyidenti(cid:12)ed
withouttheneedtolookbackinthepast.Theyalsoassumedeterministicenvironmentsand
lookuptablerepresentationsfortheQ-function.
Inrecentyears,themultilayerneuralnetworkandtherecurrentnetworkhaveemergedand
becomeimportantcomponentsforcontrollingnonlinearsystemswithorwithouthiddenstates
(perceptualaliasing).Figure	illustratesseveralcontrolarchitectures,whichcanbefound
intheliterature.Someofthesearchitecturesuserecurrentnetworks,andsomeusejustfeed-
forwardnetworks.
Inprinciple,allthesearchitecturescanbemodi(cid:12)edtocontrolsystems
withhiddenstatesbyintroducingtime-delaynetworksand/orrecurrentnetworksintothe
architectures.
NotethattherearetwotypesofcriticinFigure	:Q-typeandV-type.TheQ-typecriticisan
evaluationfunctionofstate-actionpairs,andthereforeisequivalenttotheQ-functionexcept
thatthecriticmayhavemultipleoutputs;forexample,oneoutputfordiscountedcumulative
painandoneoutputfordiscountedcumulativepleasure.(TheQ-netsinFigurecanalsobe
modi(cid:12)edtohavemultipleoutputs.)TheV-typecriticisanevaluationfunctionofonlystates.
Temporaldi(cid:11)erence(TD)methodsareoftenemployedtolearnbothtypesofcritic.
Theadaptiveheuristiccritic(AHC)architecture(Figure	.a)was(cid:12)rstproposedby[Sutton,
	]andlaterstudiedbyseveralresearchers(e.g.,
[Lin,		]).Eachtime
[Anderson,	]
anactionistaken,theactionisrewardedorpunishedbasedonwhetheritleadstobetter
orworseresults,asmeasuredbythecritic.Atthesametime,thecriticistrainedusingTD
methods.Thisarchitectureassumesdiscreteactions.
Theback-propagatedadaptivecritic(BAC)architecture(Figure	.b)wasproposedby[Wer-
bos,	]
[Werbos,		].Thisarchitectureassumestheavailabilityofthedesiredutility
atanytime.Forexample,thedesiredutilityforthepolebalancingsystemisallthetime;
thatis,thepoleneverfalls.Underthisassumptionandtheassumptionthatthecriticandthe
actionmodelhavebeenlearnedalready,thecontrolpolicycanbetrainedbyback-propagating
thedi(cid:11)erencebetweenthedesiredutilityandactualcriticoutputthroughthecritictothe
modelandthentothepolicynetwork,asifthethreenetworksformedonelargefeed-forward
network.Herethegradientsobtainedinthepolicynetworkindicatehowtochangethepolicy
soastomaximizetheutility.Notethatthisarchitecturecanhandlecontinuousactions.
ThearchitectureshowninFigure	.cwasdescribedby[Werbos,	]and[Schmidhuber,
		].Thisarchitectureassumestheavailabilityofthedesiredoutputsfromthesystemtobe
controlled.Underthisassumption,theerrorsbetweenactualoutputsanddesiredoutputscan
beback-propagatedthroughthemodeltothepolicynetwork.Thegradientsobtainedinthe
policynetworkindicatehowtochangethepolicysoastoobtainthedesiredoutputsfromthe
system.Notethatthisarchitecturecanhandlecontinuousactions.
[Jordan&Jacobs,		]proposedacontrolarchitecture(Figure	.d)(alsoknownas-net


utility

critic

new state

model

action

policy

state
(b)

new sensation &
reinforcements

action

utility

action

critic

policy

state

(a)

utility

critic

action

model mem

policy

policy

state
(d)

new state &

reinforcements

model

action

policy

state

(c)

utility

critic

new

sensation

model

sensation  action

(f)

utility

critic mem

new sensation &
reinforcements

model mem

critic

(e)

utility

action  sensation

Figure	:Controlarchitectures.


action  sensation
(g)

action  sensation

(h)

architecture)somewhatsimilartotheBACarchitecture;insteadofusingamodelandaV-type
critic,itusesaQ-typecritic.Thisarchitecturealsoassumestheavailabilityofthedesired
utilityandcanhandlecontinuousactions.
ThearchitectureshowninFigure	.ewasdescribedby[ThrunandM(cid:127)oller,		].Givena
well-trainedmodel,thisarchitectureperformsmultiple-steplook-aheadplanningto(cid:12)ndout
thebestactionforagivensituation.Usingtheinputsituationandtheactionthatisfoundbest
asatrainingpattern,thepolicynetworkisthentrainedtomimicthemultiple-stepplanning
processinonestep.Thisarchitecturecanhandlecontinuousactions.
ThearchitectureshowninFigure	.fwasproposedby[Bachrach,		].Assumingthat
theactionmodelhasbeenlearnedalready,thisarchitecturetrainsaV-typecriticusingTD
methods.Thecontrolpolicyissimplytochoosetheactionforwhichthecriticoutputis
maximal.Thisarchitectureassumesdiscreteactions.
Ourproposedrecurrent-modelarchitecture,whichisshowninFigure.candreplicatedin
Figure	.g,issimilartoBachrach'sarchitecture(Figure	.f).Bothassumediscreteactions
anddonotrequiretheavailabilityofthedesiredutility.Twodi(cid:11)erencesbetweenbothare
that:()ourarchitectureusesaQ-typecritic,whilehisaV-typecritic,and()theQ-type
criticusestheactual,currentsensationasinputs,whiletheV-typecriticusesthepredicted,
nextsensationasinputs.BecausecorrectpredictionsbytheV-typecriticstronglyrelieson
correctpredictionsbythemodel,weexpectourrecurrent-modelarchitecturetooutperform
Bachrach'sarchitecture,ifthelearningagenthasrichsensationsandisunabletolearnagood
actionmodel.
Ourrecurrent-modelarchitectureisalsosimilartoacontrolarchitectureproposedby[Chris-
man,		].Amaindi(cid:11)erenceisthatourmodellearningisbasedongradientdescentandhis
basedonmaximumlikelihoodestimation.
Allofthearchitecturesdiscussedinthissectionsofarconsistoftwoormorecomponents.The
recurrent-Qarchitecture(Figure.bandFigure	.h),ontheotherhand,consistsofonlyone
component,acritic.Thecriticisdirectlyusedtochooseactions,assumingthatactionsare
enumerableandthenumberofactionsis(cid:12)nite.
Conclusion
Thispaperpresentsthreememory-basedarchitecturesforreinforcementlearninginnon-
Markoviandomains:thewindow-Q,recurrent-Q,andrecurrent-modelarchitectures.These
architecturesareallbasedontheideaofusinghistoryinformationtodiscriminatesituations
thatareindistinguishablefromimmediatesensations.Aswehaveshown,thesearchitectures
areallcapableoflearningsomenon-Markoviantasks.Theyarealsoabletohandleirrelevant
featuresandsmallcontrolerrorstosomedegree.Wehavediscussedstrengthsandweaknesses
ofthesearchitecturesinsolvingtaskswithvariouscharacteristics.
The(good)performancereportedinthispaperwouldnotbeobtainablewithouttwotech-
niques:experiencereplaywithlarge(cid:21)valuesandback-propagationthroughtime.Thefollow-
ingisourgeneralsummaryoneachofthesearchitectures:


.Surprisingly,therecurrent-Qarchitectureseemstobemuchmorepromisingthanwe
thoughtbeforethisstudy;weinfactdidnotexpectthisarchitecturetoworkatall.As
longasthememorydepthandpayo(cid:11)delayrequiredbyataskarenottoolarge,this
architectureappearstoworke(cid:11)ectively.
.FortaskswhereasmallwindowsizeNissu(cid:14)cienttoremoveperceptualaliasing,the
window-Qarchitectureshouldworkwell.However,itisunabletorepresentanoptimal
controlpolicyifthememorydepthoftheproblemisgreaterthanN.
.Therecurrent-modelapproachseemsquitecostlytoapply,becausemodellearningoften
takesalotmoree(cid:11)ortthanwhatseemstobenecessary.Thedi(cid:14)cultyofthegeneral
problemofmodellearningiswellrecognized.Therearefewmethodsthataretruly
successful.Forexample,thediversity-basedinferenceprocedureproposedby[Rivest
&Schapire,	]isrestrictedtodeterministic,discretedomains.Instance-basedap-
proaches,suchas[Moore,		],aresuitablefornondeterministic,continuousdomains,
[Chrisman,		]studiesamodellearningmethod
butcannotlearnhistoryfeatures.
basedonastatisticaltest.Thatmethodcanhandlenondeterminismandhistoryfea-
tures,butdoesnotseemtoscalewell.Wealsodonotthinkthearchitecturesproposed
herescalewelltoproblemswithverylargememorydepths.Ontheotherhand,oncea
modelislearnedforatask,itmaybere-usableforothertasks.
Finally,asmentionedbefore,combinationsofthesearchitecturesarepossibleandmaygive
betterperformancethanthebasicversions.Furtherstudyofthisremainstobedone.
Acknowledgements
WethankChrisAtkeson,RonaldWilliams,RichSutton,andSebastianThrunforfruitful
discussionsonissuesrelatedtothiswork.WealsothankLonnieChrismanandSebastian
Thrunforhelpfulcommentsonadraftofthispaper.Thisresearchwassupportedinpartby
FujitsuLaboratoriesLtdandinpartbytheAvionicsLab,WrightResearchandDevelopment
Center,AeronauticalSystemsDivision(AFSC),U.S.AirForce,Wright-PattersonAFB,OH
-underContractF-	-C-,ArpaOrderNo.	.
ATheExperienceReplayAlgorithm
Figureshowstheexperiencereplayalgorithm,whichweusedtotrainQ-functionsinthis
work.Firstwede(cid:12)netwoterms.Anexperienceisaquadruple,(xt;at;xt+;rt),meaningthat
attimetactionatinresponsetostatextresultsinthenextstatext+andreinforcement
rt.Alessonisasequenceofexperiencesstartingfromaninitialstatextoa(cid:12)nalstatexn
wherethegoalisachievedorgivenup.Byexperiencereplay,theagentremembersalesson
andrepeatedlypresentstheexperiencesinthelessoninchronologicallybackwardordertothe
algorithmdepictedinFigure,where(cid:13),(cid:20)(cid:13)<,isthediscountfactorand(cid:21),(cid:20)(cid:21)(cid:20),
istherecencyparameterusedinTD((cid:21))methods[Sutton,	].


Toreplayf(x;a;x;r)...(xn;an;xn+;rn)g,do
.t n
.et Q(xt;at)
.ut+ MaxfQ(xt+;k)jkactionsg
.et rt+(cid:13)[((cid:0)(cid:21))ut++(cid:21)et+]
.AdjustthenetworkimplementingQ(xt;at)by
back-propagatingtheerroret(cid:0)etthroughit
.ift=exit;elset t(cid:0);goto
Figure:Theexperiencereplayalgorithm
Theideabehindthisalgorithmisasfollows.Roughlyspeaking,theexpectedreturn(called
TD((cid:21))return)from(xt;at)canbewrittenrecursively[Watkins,		]as
R(xt;at)=rt+(cid:13)[((cid:0)(cid:21))U(xt+)+(cid:21)R(xt+;at+)]
()
where
U(xt)=MaxfQ(xt;k)jkactionsg
()
Notethattheterminthebrackets[]ofEquation,whichistheexpectedreturnfromtime
t+,isaweightedsumof)returnpredictedbythecurrentQ-functionand)returnactually
receivedinthereplayedlesson.With(cid:21)=andbackwardreplay,R(xt;at)isexactlythe
discountedcumulativereinforcementsfromtimettotheendofthelesson.Inthebeginning
oflearning,theQ-functionisusuallyfarfromcorrect,sotheactualreturnisoftenabetter
estimateoftheexpectedreturnthanthatpredictedbytheQ-function.ButwhentheQ-
functionbecomesmoreandmoreaccurate,theQ-functionprovidesabetterprediction.In
particular,whenthereplayedlessonisfarbackinthepastandinvolvesbadchoicesofactions,
theactualreturn(inthatlesson)wouldbesmallerthanwhatcanbereceivedusingthecurrent
policy(Inthiscase,itisbettertouse(cid:21)=).
Ideally,wewantQ(xt;at)=R(xt;at)toholdtrue.Thedi(cid:11)erencebetweenthetwosidesof\="
istheerrorbetweentemporallysuccessivepredictionsabouttheexpectedreturnfrom(xt;at).
Toreducethiserror,thenetworkswhichimplementtheQ-functionareadjustedusingthe
back-propagationalgorithm(Step).Whent=n,(cid:21)mustbesetto,becauseet+inStep
isunde(cid:12)ned.Forfurtherdiscussions,see[Lin,		].
Theexperiencereplayalgorithmcanbeusedinbatchmodeorinincrementalmode.
In
incrementalmodetheQ-netsareadjustedafterreplayingeachexperience,whileinbatch
modetheQ-netsareadjustedonlyafterreplayingawholelesson.Theadvantageofbackward
replayisgreatestwhenthealgorithmisusedinincrementalmode.Inthiswork,non-recurrent
Q-netsweretrainedinincrementalmode,whilerecurrentQ-netsinbatchmodetoavoid
instability.


Table:ParameterSettings
Task
Task
Task
Task
sameas
sameas
N=
N=
window-Q
Task
Taskexcept
(cid:21)=:
(cid:21)=:
(cid:17)q=:
Hq=
Hq=
(cid:17)q=:
(cid:17)q=:
=T=!
=T=!
(cid:21)=:
(cid:21)=:
sameas
(cid:21)=:
recurrent-
Cq=
Cq=
Task
Cq=
Q
Hq=
Hq=
Hq=
(cid:17)q=:
(cid:17)q=:
(cid:17)q=:
=T=!
=T=!
=T=!
(cid:21)=:
recurrent-
(cid:21)=:
(cid:21)=:
(cid:21)=:
model
Cm=
Cm=
Cm=
Cm=
Hm=
Hm=
Hm=
Hm=
(cid:17)m=:
(cid:17)m=:
(cid:17)m=:
(cid:17)m=:
Hq=
Hq=
Hq=
Hq=
(cid:17)q=:
(cid:17)q=:
(cid:17)q=:
(cid:17)q=:
=T=!
=T=!
=T=!
=T=!
BTheParameterSettingsForTheExperiments
Theparametersusedintheexperimentsincluded:
(cid:15)discountfactor(cid:13)((cid:12)xedtobe.	),
(cid:15)recencyfactor(cid:21),
(cid:15)rangeoftherandominitialweightsofnetworks((cid:12)xedtobe.),
(cid:15)momentumfactor((cid:12)xedtobeforperceptronsand.	formultilayernetworks),
(cid:15)windowsize(N),
(cid:15)learningrateforQ-nets((cid:17)q),
(cid:15)learningrateforactionmodels((cid:17)m),
(cid:15)numberofhiddenunitsforQ-nets(Hq),
(cid:15)numberofhiddenunitsforactionmodels(Hm),
(cid:15)numberofcontextunitsforQ-nets(Cq),
(cid:15)numberofcontextunitsforactionmodels(Cm),
(cid:15)temperatureforcontrollingrandomnessofactionselection(T),and
(cid:15)Pl(seebelow).
Tableshowstheparametersettingsusedtogeneratethemeanperformancedescribedin
thispaper.(NotethatHq=meansthatweusedperceptronsforQ-nets.)Thoseparameter
valueswereempiricallychosentogiveroughlythebestperformance.
Theexperiencereplayalgorithmdidnotreplayallexperienceinthepast.Itreplayedonlythe
experiencefromthemostrecenttrials,andreplayedonlytheactionswhoseprobabilities


x

F

Figure:Thepolebalancingproblem.
ofbeingchosenbyastochasticactionselectorweregreaterthanPl.(See[Lin,		]
[Lin,
		]fordiscussions.)ThevalueweusedforPlwasbetween.and.forthecup
collectiontasksandbetween.and.forthepolebalancingtask,dependingon
therecencyoftheexperiencetobereplayed.
CThePoleBalancingProblem
Thedynamicsofthecart-polesystem(Figure)aregivenbythefollowingequationsof
motion[Sutton,	]
[Anderson,	]:
x(t+)=x(t)+(cid:28)_x(t)
_x(t+)=_x(t)+(cid:28)(cid:127)x(t)
(cid:18)(t+)=(cid:18)(t)+(cid:28)_(cid:18)(t)
_(cid:18)(t+)=_(cid:18)(t)+(cid:28)(cid:127)(cid:18)(t)
(cid:127)x(t)=F(t)+mplh_(cid:18)(t)sin(cid:18)(t)(cid:0)(cid:127)(cid:18)(t)cos(cid:18)(t)i(cid:0)(cid:22)csgn(_x(t))
mc+mp
(cid:21)(cid:0)(cid:22)p_(cid:18)(t)mpl
(cid:127)(cid:18)(t)=gsin(cid:18)(t)+cos(cid:18)(t)(cid:20)(cid:0)F(t)(cid:0)mpl_(cid:18)(t)sin(cid:18)(t)+(cid:22)csgn(_x(t))
mc+mp
mc+mpi
lh(cid:0)mpcos(cid:18)(t)


q
where
=	:m=s
g
=accelerationduetogravity,
mc
=.kg
=massofcart,
mp
=.kg
=massofpole,
l
=.m
=halfpolelength,
(cid:22)c
=.
=coe(cid:14)cientoffrictionofcartontrack,
(cid:22)p
=.
=coe(cid:14)cientoffrictionofpoleoncart,
F(t)=(cid:6):newtons=forceappliedtocart'scenterofmassattimet,
=timeinsecondscorrespondingtoonesimulationstep,
=.s
(cid:28)
sgn(x)=(+ifx(cid:21);
{
ifx<:
Thereinforcementfunctionisde(cid:12)nedas:
r(t)=({ifj(cid:18)(t)j>:radianorjx(t)j>:m,

otherwise.
