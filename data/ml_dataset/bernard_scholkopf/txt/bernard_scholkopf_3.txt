A Generalized Representer Theorem

Bernhard Sch¨olkopf1,2,3, Ralf Herbrich1,2, and Alex J. Smola1

1 Australian National University, Department of Engineering, Canberra ACT 0200,

Australia

bs@conclu.de, rherb@microsoft.com, alex.smola@anu.edu.au
2 Microsoft Research Ltd., 1 Guildhall Street, Cambridge, UK

3 New address: Biowulf Technologies, Floor 9, 305 Broadway, New York, NY 10007,

USA

Abstract. Wahba’s classical representer theorem states that the solu-
tions of certain risk minimization problems involving an empirical risk
term and a quadratic regularizer can be written as expansions in terms
of the training examples. We generalize the theorem to a larger class
of regularizers and empirical risk terms, and give a self-contained proof
utilizing the feature space associated with a kernel. The result shows
that a wide range of problems have optimal solutions that live in the
ﬁnite dimensional span of the training examples mapped into feature
space, thus enabling us to carry out kernel algorithms independent of
the (potentially inﬁnite) dimensionality of the feature space.

1 Introduction

Following the development of support vector (SV) machines [23], positive deﬁnite
kernels have recently attracted considerable attention in the machine learning
community. It turns out that a number of results that have now become popular
were already known in the approximation theory community, as witnessed by the
work of Wahba [24]. The present work brings together tools from both areas. This
allows us to formulate a generalized version of a classical theorem from the latter
ﬁeld, and to give a new and simpliﬁed proof for it, using the geometrical view
of kernel function classes as corresponding to vectors in linear feature spaces.

The paper is organized as follows. In the present ﬁrst section, we review
some basic concepts. The two subsequent sections contain our main result, some
examples and a short discussion.

1.1 Positive Deﬁnite Kernels

The question under which conditions kernels correspond to dot products in linear
spaces has been brought to the attention of the machine learning community by
Vapnik and coworkers [1,5,23]. In functional analysis, the same problem has
been studied under the heading of Hilbert space representations of kernels. A
good monograph on the functional analytic theory of kernels is [4]. Most of the
material in the present introductory section is taken from that work. Readers
familiar with the basics of kernels can skip over the remainder of it.

D. Helmbold and B. Williamson (Eds.): COLT/EuroCOLT 2001, LNAI 2111, pp. 416–426, 2001.
c(cid:1) Springer-Verlag Berlin Heidelberg 2001

A Generalized Representer Theorem

417

Suppose we are given empirical data

(x1, y1), . . . , (xm, ym) ∈ X × R.

(1)

Here, the target values yi live in R, and the patterns xi are taken from a domain
X . The only thing we assume about X is that is a nonempty set. In order to
study the problem of learning, we need additional structure. In kernel methods,
this is provided by a similarity measure
k : X × X → R,

(x, x(cid:2)) (cid:5)→ k(x, x(cid:2)).

(2)

The function k is called a kernel [20]. The term stems from the ﬁrst use of this
type of function in the study of integral operators, where a function k giving rise
to an operator Tk via

k(x, x(cid:2))f(x(cid:2)) dx(cid:2)

(3)

(cid:1)

(Tkf)(x) =

X

m(cid:2)

m(cid:2)

is called the kernel of Tk. Note that we will state most results for the more
general case of complex-valued kernels;1 they specialize to the real-valued case
in a straightforward manner. Below, unless stated otherwise, indices i and j will
be understood to run over the training set, i.e. i, j = 1, . . . , m.
Deﬁnition 1 (Gram matrix). Given a kernel k and patterns x1, . . . , xm ∈ X ,
the m × m matrix

K := (k(xi, xj))ij

(4)

is called the Gram matrix of k with respect to x1, . . . , xm.
Deﬁnition 2 (Positive deﬁnite matrix). An m× m matrix K over the com-
plex numbers C satisfying

ci¯cjKij ≥ 0

(5)

i=1

j=1

for all c1, . . . , cm ∈ C is called positive deﬁnite.
Deﬁnition 3 (Positive deﬁnite kernel). Let X be a nonempty set. A function
k : X × X → C which for all m ∈ N, x1, . . . , xm ∈ X gives rise to a positive
deﬁnite Gram matrix is called a positive deﬁnite (pd) kernel.2
Real-valued kernels are contained in the above deﬁnition as a special case. How-
ever, it is not suﬃcient to require that (5) hold for real coeﬃcients ci. If we want
to get away with real coeﬃcients only, we additionally have to require that the
1 We use the notation ¯c to denote the complex conjugate of c.
2 One might argue that the term positive deﬁnite kernel
is slightly misleading. In
matrix theory, the term deﬁnite is sometimes used to denote the case where equality
in (5) only occurs if c1 = ··· = cm = 0. Simply using the term positive kernel, on
the other hand, could be confused with a kernel whose values are positive.

418

B. Sch¨olkopf, R. Herbrich, and A.J. Smola

kernel be symmetric. The complex case is slightly more elegant; in that case, (5)
can be shown to imply symmetry, i.e. k(xi, xj) = k(xj, xi).

Positive deﬁnite kernels can be regarded as generalized dot products. Indeed,
any dot product is a pd kernel; however, linearity does not carry over from dot
products to general pd kernels. Another property of dot products, the Cauchy-
Schwarz inequality, does have a natural generalization: if k is a positive deﬁnite
kernel, and x1, x2 ∈ X , then

|k(x1, x2)|2 ≤ k(x1, x1) · k(x2, x2).

(6)

x (cid:5)→ k(·, x).

φ : X → CX ,

1.2 ... and Associated Feature Spaces
We deﬁne a map from X into the space of functions mapping X into C, denoted
as CX , via [4]

(7)
Here, φ(x) = k(·, x) denotes the function that assigns the value k(x(cid:2), x) to x(cid:2) ∈ X .
Applying φ to x amounts to representing it by its similarity to all other points
in the input domain X . This seems a very rich representation, but it turns out
that the kernel allows the computation of a dot product in that representation.
We shall now construct a dot product space containing the images of the
input patterns under φ. To this end, we ﬁrst need to endow it with the linear
structure of a vector space. This is done by forming linear combinations of the
form

f(·) =

αik(·, xi),

g(·) =

βjk(·, x(cid:2)
j).

(8)

m(cid:1)(cid:2)

j=1

m(cid:2)

i=1

Here, m, m(cid:2) ∈ N, α1, . . . , αm, β1, . . . , βm(cid:1) ∈ C and x1, . . . , xm, x(cid:2)
are arbitrary. A dot product between f and g can be constructed as

1, . . . , x(cid:2)

m(cid:1) ∈ X

m(cid:2)

m(cid:1)(cid:2)

i=1

j=1

(cid:10)f, g(cid:11) :=

¯αiβjk(xi, x(cid:2)
j).

(9)

j, xi) = k(xi, x(cid:2)

To see that this is well-deﬁned, although it explicitly contains the expansion
coeﬃcients (which need not be unique), note that (cid:10)f, g(cid:11) =
j), using
(cid:3)
k(x(cid:2)
j). The latter, however, does not depend on the particular
expansion of f. Similarly, for g, note that (cid:10)f, g(cid:11) =
i ¯αig(xi). This also shows
that (cid:10)·,·(cid:11) is anti-linear in the ﬁrst argument and linear in the second one. It
is symmetric, since (cid:10)f, g(cid:11) = (cid:10)g, f(cid:11). Moreover, given functions f1, . . . , fn, and
coeﬃcients γ1, . . . , γn ∈ C, we have

(cid:3)m(cid:1)
j=1 βjf(x(cid:2)

n(cid:2)

n(cid:2)

(cid:4)
n(cid:2)

(cid:5)

n(cid:2)

¯γiγj(cid:10)fi, fj(cid:11) =

γifi,

γjfj

≥ 0,

(10)

i=1

j=1

i=1

j=1

hence (cid:10)·,·(cid:11) is actually a pd kernel on our function space.

A Generalized Representer Theorem

419

For the last step in proving that it even is a dot product, one uses the
following interesting property of φ, which follows directly from the deﬁnition:
for all functions (8),

(11)
i.e., k is the representer of evaluation. In particular, (cid:10)k(·, x), k(·, x(cid:2))(cid:11) = k(x, x(cid:2)),
the reproducing kernel property [2,4,24], hence (cf. (7)) we indeed have k(x, x(cid:2)) =
(cid:10)φ(x), φ(x(cid:2))(cid:11). Moreover, by (11) and (6) we have

(cid:10)k(·, x), f(cid:11) = f(x),

|f(x)|2 = |(cid:10)k(·, x), f(cid:11)|2 ≤ k(x, x) · (cid:10)f, f(cid:11).

(12)
Therefore, (cid:10)f, f(cid:11) = 0 implies f = 0, which is the last property that was left to
prove in order to establish that (cid:10)·,·(cid:11) is a dot product.

One can complete the space of functions (8) in the norm corresponding to
the dot product, i.e., add the limit points of sequences that are convergent in
that norm, and thus gets a Hilbert space Hk, usually called a reproducing kernel
Hilbert space (RKHS). The case of real-valued kernels is included in the above;
in that case, Hk can be chosen as a real Hilbert space.

2 The Representer Theorem

As a consequence of the last section, one of the crucial properties of kernels is
that even if the input domain X is only a set, we can nevertheless think of the
pair (X , k) as a (subset of a) Hilbert space. From a mathematical point of view,
this is attractive, since we can thus study various data structures (e.g., strings
over discrete alphabets [26,13,18]) in Hilbert spaces, whose theory is very well
developed. From a practical point of view, however, we now face the problem that
for many popular kernels, the Hilbert space is known to be inﬁnite-dimensional
[24]. When training a learning machine, however, we do not normally want to
solve an optimization problem in an inﬁnite-dimensional space.

This is where the main result of this paper will be useful, showing that
a large class of optimization problems with RKHS regularizers have solutions
that can be expressed as kernel expansions in terms of the training data. These
optimization problems are of great interest for learning theory, both since they
comprise a number of useful algorithms as special cases and since their statistical
performance can be analyzed with tools of learning theory (see [23,3], and, more
speciﬁcally dealing with regularized risk functionals, [6]).
Theorem 1 (Nonparametric Representer Theorem). Suppose we are
given a nonempty set X , a positive deﬁnite real-valued kernel k on X × X , a
training sample (x1, y1), . . . , (xm, ym) ∈ X × R, a strictly monotonically increas-
ing real-valued function g on [0,∞[, an arbitrary cost function c : (X × R2)m →
R ∪ {∞}, and a class of functions
∞(cid:2)

(cid:8)

βik(·, zi), βi ∈ R, zi ∈ X ,(cid:17)f(cid:17) < ∞

.

(13)

(cid:6)
f ∈ RX

(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)f(·) =

F =

i=1

B. Sch¨olkopf, R. Herbrich, and A.J. Smola

420
Here, (cid:17) · (cid:17) is the norm in the RKHS Hk associated with k, i.e. for any zi ∈

X , βi ∈ R (i ∈ N), (cid:9)(cid:9)(cid:9)(cid:9)(cid:9) ∞(cid:2)

i=1

(cid:9)(cid:9)(cid:9)(cid:9)(cid:9)2

∞(cid:2)

∞(cid:2)

i=1

j=1

=

βiβjk(zi, zj).

βik(·, zi)

Then any f ∈ F minimizing the regularized risk functional

c ((x1, y1, f(x1)), . . . , (xm, ym, f(xm))) + g ((cid:17)f(cid:17))

admits a representation of the form

f(·) =

m(cid:2)

i=1

αik(·, xi).

(14)

(15)

(16)

m(cid:2)

i=1

Let us give a few remarks before the proof. In its original form, with mean
squared loss

c((x1, y1, f(x1)), . . . , (xm, ym, f(xm))) =

1
m

(yi − f(xi))2,

(17)

or hard constraints on the outputs, and g((cid:17)f(cid:17)) = λ(cid:17)f(cid:17)2 (λ > 0), the theorem
is due to [15]. Note that in our formulation, hard constraints on the solution
are included by the possibility of c taking the value ∞. A generalization to
non-quadratic cost functions was stated by [7], cf. the discussion in [25] (note,
however, that [7] did not yet allow for coupling of losses at diﬀerent points).
The present generalization to g((cid:17)f(cid:17)) is, to our knowledge, new. For a machine
learning point of view on the representer theorem and a variational proof, cf.
[12].

The signiﬁcance of the theorem is that it shows that a whole range of learning
algorithms have solutions that can be expressed as expansions in terms of the
training examples. Note that monotonicity of g is necessary to ensure that the
theorem holds. It does not ensure that the regularized risk functional (15) does
not have multiple local minima. For this, we would need to require convexity of
g and of the cost function c. If we discarded the strictness of the monotonicity
of g, it would no longer follow that each minimizer (there might be multiple
ones) of the regularized risk admits an expansion (16); however, it would still
follow that there is always another solution minimizing (15) that does admit the
expansion. In the SV community, (16) is called the SV expansion.
Proof. As we have assumed that k maps into R, we will use (cf. (7))

φ : X → RX ,

x (cid:5)→ k(·, x).

(18)
Since k is a reproducing kernel, evaluation of the function φ(x) on the point x(cid:2)
yields

(φ(x))(x(cid:2)) = k(x(cid:2), x) = (cid:10)φ(x(cid:2)), φ(x)(cid:11)

(19)

A Generalized Representer Theorem

421

for all x, x(cid:2) ∈ X . Here, (cid:10)·,·(cid:11) denotes the dot product of Hk.

Given x1, . . . , xm, any f ∈ F can be decomposed into a part that lives in the

span of the φ(xi) and a part which is orthogonal to it, i.e.

m(cid:2)

i=1

f =

αiφ(xi) + v

(20)

(21)

(22)

(cid:4)
m(cid:2)

for some α ∈ Rm and v ∈ F satisfying, for all j,
(cid:10)v, φ(xj)(cid:11) = 0.
(cid:5)

Using the latter and (19), application of f to an arbitrary training point xj
yields

m(cid:2)
(cid:3)m
independent of v. Consequently, the ﬁrst term of (15) is independent of v. As
i=1 αiφ(xi), and g is strictly
for the second term, since v is orthogonal to
(cid:15)(cid:16)(cid:16)(cid:17)(cid:9)(cid:9)(cid:9)(cid:9)(cid:9)
monotonic, we get
(cid:2)

αi(cid:10)φ(xi), φ(xj)(cid:11),

αiφ(xi) + v, φ(xj)

g((cid:17)f(cid:17)) = g

(cid:9)(cid:9)(cid:9)(cid:9)(cid:9)
(cid:11)

(cid:9)(cid:9)(cid:9)(cid:9)(cid:9)2

αiφ(xi) + v

= g

+ (cid:17)v(cid:17)2

=

i=1







f(xj) =

i=1

αiφ(xi)

i

(cid:10)(cid:9)(cid:9)(cid:9)(cid:9)(cid:9)
(cid:2)
(cid:10)(cid:9)(cid:9)(cid:9)(cid:9)(cid:9)
(cid:2)

i

i

≥ g

(cid:9)(cid:9)(cid:9)(cid:9)(cid:9)
(cid:11)

m(cid:2)

αiφ(xi)

,

(23)

(cid:3)
with equality occuring if and only if v = 0. Setting v = 0 thus does not aﬀect the
ﬁrst term of (15), while strictly reducing the second term — hence, any minimizer
must have v = 0. Consequently, any solution takes the form f =
i αiφ(xi),
i.e., using (19),

f(·) =

αik(·, xi).

(24)

The theorem is proven.

i=1

The extension to the case where we also include a parametric part is slightly
more technical but straightforward. We state the corresponding result without
proof:
Theorem 2 (Semiparametric Representer Theorem). Suppose that in ad-
dition to the assumptions of the previous theorem we are given a set of M
real-valued functions {ψp}M
p=1 on X , with the property that the m × M matrix
(ψp(xi))ip has rank M. Then any ˜f := f + h, with f ∈ F and h ∈ span{ψp},
minimizing the regularized risk

(cid:21)

(cid:22)
(x1, y1, ˜f(x1)), . . . , (xm, ym, ˜f(xm))

c

+ g ((cid:17)f(cid:17))

(25)

422

B. Sch¨olkopf, R. Herbrich, and A.J. Smola

admits a representation of the form

m(cid:2)

i=1

˜f(·) =

αik(xi,·) +

M(cid:2)

p=1

βpψp(·),

(26)

with unique coeﬃcients βp ∈ R for all p = 1, . . . , M.
Remark 1 (Biased regularization). A straightforward extension of the represen-
ter theorems can be obtained by including a term −(cid:10)f0, f(cid:11) into (15) or (25),
respectively, where f0 ∈ Hk. In this case, if a solution to the minimization prob-
lem exists, it admits an expansion which diﬀers from the above ones in that it
additionally contains a multiple of f0. To see this, decompose the vector v used
in the proof of Theorem 1 into a part orthogonal to f0 and the remainder.
2(cid:17)f(cid:17)2, this can be seen to correspond to an
2(cid:17)f − f0(cid:17)2. Thus, it is no longer the size

In the case where g((cid:17)f(cid:17)) = 1

eﬀective overall regularizer of the form 1
of (cid:17)f(cid:17) that is penalized, but the diﬀerence to f0.

Some explicit applications of Theorems 1 and 2 are given below.

Example 1 (SV regression). For SV regression with the ε–insensitive loss [23] we
have

c

(xi, yi, f (xi))i=1,...,m

=

max (0,|f (xi) − yi| − ε)

(27)

(cid:22)

m(cid:2)

i=1

1
λ

and g ((cid:17)f(cid:17)) = (cid:17)f(cid:17)2, where λ > 0 and ε ≥ 0 are ﬁxed parameters which determine
the trade-oﬀ between regularization and ﬁt to the training set. In addition, a
single (M = 1) constant function ψ1(x) = b (b ∈ R) is used as an oﬀset that is
not regularized by the algorithm [25].

In [22], a semiparametric extension was proposed which shows how to deal

with the case M > 1 algorithmically. Theorem 2 applies in that case, too.
Example 2 (SV classiﬁcation). Here, the targets satisfy yi ∈ {±1}, and we use

(cid:21)

(cid:21)

(cid:22)

m(cid:2)

i=1

=

1
λ

c

(xi, yi, f (xi))i=1,...,m

max (0, 1 − yif (xi)) ,

(28)

the regularizer g ((cid:17)f(cid:17)) = (cid:17)f(cid:17)2, and ψ1(x) = b. For λ → 0, we recover the
hard margin SVM, i.e., the minimizer must correctly classify each training point
(xi, yi). Note that after training, the actual classiﬁer will be sgn (f(·) + b).
Example 3 (SVMs minimizing actual risk bounds). The reason why SVM algo-
rithms such as the ones discussed above use the regularizer g ((cid:17)f(cid:17)) = (cid:17)f(cid:17)2 are
practical ones. It is usually argued that theoretically, we should really be min-
imizing an upper bound on the expected test error, but in practice, we use a
quadratic regularizer, traded oﬀ with an empirical risk term via some constant.
One can show that in combination with certain loss functions (hard constraints,
linear loss, quadratic loss, or suitable combinations thereof), this regularizer

A Generalized Representer Theorem

423

leads to a convex quadratic programming problem [5,23]. In that case, the stan-
dard Kuhn-Tucker machinery of optimization theory [19] can be applied to derive
a so-called dual optimization problem, which consists of ﬁnding the expansion
coeﬃcients α1, . . . , αm rather than the solution f in the RKHS.

From the point of view of learning theory, on the other hand, more general
functions g might be preferable, such as ones that are borrowed from uniform
convergence bounds. For instance, we could take inspiration from the basic pat-
tern recognition bounds of [23] and use, for some small η > 0, the (strictly
monotonic) function

(cid:15)(cid:16)(cid:16)(cid:17) R2(cid:17)f(cid:17)2

(cid:21)

log

(cid:22)
R2(cid:5)f(cid:5)2 + 1

2m

m

g((cid:17)f(cid:17)) :=

− log(η/4)

.

(29)

More sophisticated functions based on other bounds on the test error are con-
ceivable, as well as variants for regression estimation (e.g., [3,6]).

Unlike Wahba’s original version, the generalized representer theorem can help
dealing with these cases. It asserts that the solution still has an expansion in
terms of the training examples. It is thus suﬃcient to minimize the risk bound
over expansions of the form (16) (or (26)). Substituting (16) into (15), we get an
(m-dimensional) problem in coeﬃcient representation (without having to appeal
to methods of optimization theory)

+ g

αiαjk(xi, xj)

i=1

j=1

If g and c are convex, then so will be the dual, thus we can solve it employing
methods of convex optimization (such as interior point approaches often used
even for standard SVMs). If the dual is non-convex, we will typically only be
able to ﬁnd local minima.

Independent of the convexity issue, the result lends itself well to gradient-
based on-line algorithms for minimizing RKHS-based risk functionals [10,9,17,
11,8,16]: for the computation of gradients, we only need the objective function to
be diﬀerentiable; convexity is not required. Such algorithms can thus be adapted
to deal with more general regularizers.

Example 4 (Bayesian MAP estimates). The well-known correspondence to
Bayesian methods is established by identifying (15) with the negative log poste-
rior [14,12]. In this case, exp(−c((xi, yi, f(xi))i=1,...,m)) is the likelihood of the
data, while exp(−g((cid:17)f(cid:17))) is the prior over the set of functions. The well-known
Gaussian process prior (e.g. [24,27]), with covariance function k, is obtained
by using g((cid:17)f(cid:17)) = λ(cid:17)f(cid:17)2 (here, λ > 0, and, as above, (cid:17) · (cid:17) is the norm of
the RKHS associated with k). A Laplacian prior would be obtained by using

min

α1,...,αm∈R

c

αik(x1, xi)), . . . , (xm, ym,

αik(xm, xi)))

(cid:10)

(x1, y1,

m(cid:2)
(cid:15)(cid:16)(cid:16)(cid:17) m(cid:2)
m(cid:2)

i=1




m(cid:2)

(cid:11)


 .

i=1

(30)

B. Sch¨olkopf, R. Herbrich, and A.J. Smola

424
g((cid:17)f(cid:17)) = λ(cid:17)f(cid:17). In all cases, the minimizer of (15) corresponds to a function
with maximal a posteriori probability (MAP).

Example 5 (Kernel PCA). PCA in a kernel feature space can be shown to cor-
respond to the case of

c((xi, yi, f(xi))i=1,...,m) =

f(xj)

= 1

(31)


m(cid:3)
 0
∞ otherwise

if 1
m

i=1

(cid:10)
f(xi) − 1

m

(cid:11)2

m(cid:3)

j=1

with g an arbitrary strictly monotonically increasing function [21]. The con-
straint ensures that we are only considering linear feature extraction functionals
that produce outputs of unit empirical variance. Note that in this case of unsu-
pervised learning, there are no labels yi to consider.

3 Conclusion

We have shown that for a large class of algorithms minimizing a sum of an em-
pirical risk term and a regularization term in a reproducing kernel Hilbert space,
the optimal solutions can be written as kernel expansions in terms of training ex-
amples. This has been known for speciﬁc algorithms; e.g., for the SV algorithm,
where it is a direct consequence of the structure of the optimization problem,
but not in more complex cases, such as the direct minimization of some bounds
on the test error (cf. Example 3). The representer theorem puts these individ-
ual ﬁndings into a wider perspective, and we hope that the reader will ﬁnd the
present generalization useful by either gaining some insight, or by taking it as
a practical guideline for designing novel kernel algorithms: as long as the objec-
tive function can be cast into the form considered in the generalized representer
theorem, one can recklessly carry out algorithms in inﬁnite dimensional spaces,
since the solution will always live in a speciﬁc subspace whose dimensionality
equals at most the number of training examples.

Acknowledgements. Thanks to Bob Williamson, Grace Wahba, Jonathan
Baxter, Peter Bartlett, and Nello Cristianini for useful comments. This work
was supported by the Australian Research Council. AS was supported by DFG
grant SM 62/1-1.

References

1. M. A. Aizerman, ´E. M. Braverman, and L. I. Rozono´er. Theoretical foundations
of the potential function method in pattern recognition learning. Automation and
Remote Control, 25:821–837, 1964.

2. N. Aronszajn. Theory of reproducing kernels. Transactions of the American Math-

ematical Society, 68:337–404, 1950.

A Generalized Representer Theorem

425

3. P. L. Bartlett and J. Shawe-Taylor. Generalization performance of support vector
machines and other pattern classiﬁers. In B. Sch¨olkopf, C. J. C. Burges, and A. J.
Smola, editors, Advances in Kernel Methods — Support Vector Learning, pages
43–54, Cambridge, MA, 1999. MIT Press.

4. C. Berg, J. P. R. Christensen, and P. Ressel. Harmonic Analysis on Semigroups.

Springer-Verlag, New York, 1984.

5. B. E. Boser, I. M. Guyon, and V. N. Vapnik. A training algorithm for optimal
margin classiﬁers.
In D. Haussler, editor, Proceedings of the 5th Annual ACM
Workshop on Computational Learning Theory, pages 144–152, Pittsburgh, PA,
July 1992. ACM Press.

6. O. Bousquet and A. Elisseeﬀ. Algorithmic stability and generalization perfor-
mance. In T. K. Leen, T. G. Dietterich, and V. Tresp, editors, Advances in Neural
Information Processing Systems 13. MIT Press, 2001.

7. D. Cox and F. O’Sullivan. Asymptotic analysis of penalized likelihood and related

estimators. Annals of Statistics, 18:1676–1695, 1990.

8. L. Csat´o and M. Opper. Sparse representation for Gaussian process models. In
T. K. Leen, T. G. Dietterich, and V. Tresp, editors, Advances in Neural Information
Processing Systems 13. MIT Press, 2001.

9. Y. Freund and R. E. Schapire. Large margin classiﬁcation using the perceptron
algorithm. In J. Shavlik, editor, Machine Learning: Proceedings of the Fifteenth
International Conference, San Francisco, CA, 1998. Morgan Kaufmann.

10. T.-T. Frieß, N. Cristianini, and C. Campbell. The kernel adatron algorithm: A fast
and simple learning procedure for support vector machines. In J. Shavlik, editor,
15th International Conf. Machine Learning, pages 188–196. Morgan Kaufmann
Publishers, 1998.

11. C. Gentile. Approximate maximal margin classiﬁcation with respect to an arbitrary

norm. Unpublished.

12. F. Girosi, M. Jones, and T. Poggio. Regularization theory and neural networks

architectures. Neural Computation, 7(2):219–269, 1995.

13. D. Haussler. Convolutional kernels on discrete structures. Technical Report UCSC-
CRL-99-10, Computer Science Department, University of California at Santa Cruz,
1999.

14. G. S. Kimeldorf and G. Wahba. A correspondence between Bayesian estimation on
stochastic processes and smoothing by splines. Annals of Mathematical Statistics,
41:495–502, 1970.

15. G. S. Kimeldorf and G. Wahba. Some results on Tchebycheﬃan spline functions.

J. Math. Anal. Applic., 33:82–95, 1971.

16. J. Kivinen, A. J. Smola, P. Wankadia, and R. C. Williamson. On-line algorithms

for kernel methods. in preparation, 2001.

17. A. Kowalczyk. Maximal margin perceptron.

In A. J. Smola, P. L. Bartlett,
B. Sch¨olkopf, and D. Schuurmans, editors, Advances in Large Margin Classiﬁers,
pages 75–113, Cambridge, MA, 2000. MIT Press.

18. H. Lodhi, J. Shawe-Taylor, N. Cristianini, and C. Watkins. Text classiﬁcation
using string kernels. Technical Report 2000-79, NeuroCOLT, 2000. Published in:
T. K. Leen, T. G. Dietterich and V. Tresp (eds.), Advances in Neural Information
Processing Systems 13, MIT Press, 2001.

19. O. L. Mangasarian. Nonlinear Programming. McGraw-Hill, New York, NY, 1969.
20. J. Mercer. Functions of positive and negative type and their connection with
the theory of integral equations. Philosophical Transactions of the Royal Society,
London, A 209:415–446, 1909.

426

B. Sch¨olkopf, R. Herbrich, and A.J. Smola

21. B. Sch¨olkopf, A. Smola, and K.-R. M¨uller. Kernel principal component analysis.
In B. Sch¨olkopf, C. J. C. Burges, and A. J. Smola, editors, Advances in Kernel
Methods - Support Vector Learning, pages 327–352. MIT Press, Cambridge, MA,
1999.

22. A. Smola, T. Frieß, and B. Sch¨olkopf. Semiparametric support vector and linear
programming machines. In M. S. Kearns, S. A. Solla, and D. A. Cohn, editors,
Advances in Neural Information Processing Systems 11, pages 585–591, Cambridge,
MA, 1999. MIT Press.

23. V. Vapnik. The Nature of Statistical Learning Theory. Springer, NY, 1995.
24. G. Wahba. Spline Models for Observational Data, volume 59 of CBMS-NSF Re-

gional Conference Series in Applied Mathematics. SIAM, Philadelphia, 1990.

25. G. Wahba. Support vector machines, reproducing kernel Hilbert spaces and the
randomized GACV. In B. Sch¨olkopf, C. J. C. Burges, and A. J. Smola, editors,
Advances in Kernel Methods — Support Vector Learning, pages 69–88, Cambridge,
MA, 1999. MIT Press.

26. C. Watkins. Dynamic alignment kernels.

In A. J. Smola, P. L. Bartlett,
B. Sch¨olkopf, and D. Schuurmans, editors, Advances in Large Margin Classiﬁers,
pages 39–50, Cambridge, MA, 2000. MIT Press.

27. C. K. I. Williams. Prediction with Gaussian processes: From linear regression to
linear prediction and beyond. In M. I. Jordan, editor, Learning and Inference in
Graphical Models. Kluwer, 1998.

