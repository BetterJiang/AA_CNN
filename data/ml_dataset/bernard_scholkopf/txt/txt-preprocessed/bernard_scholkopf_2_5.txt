Abstract

The problem of feature selection is a difcult combina-
torial task in Machine Learning and of high practical rele-
vance, e.g. in bioinformatics. Genetic Algorithms (GAs) of-
fer a natural way to solve this problem. In this paper we
present a special Genetic Algorithm, which especially takes
into account the existing bounds on the generalization error
for Support Vector Machines (SVMs). This new approach
is compared to the traditional method of performing cross-
validation and to other existing algorithms for feature se-
lection.

1. Introduction

In many practical pattern classication tasks we are con-
fronted with the problem, that we have a very high dimen-
sional input space and we want to nd out the combination
of the original input features which contribute most to the
classication. Supposed we want to classify cells into can-
cer or not based upon their gene expressions. Surely on one
hand we want to have a combination of genes as small as
possible. On the other hand we want to get the best possi-
ble performance of the learning machine.

Assumed we have training dataD=fxi;yijxi2
X;yi2Y;i=1;:::;g (drawn i.i.d. from some unknown
probability distributionx;y) whereX is a vector space
of dimensiond andY is a nite set of labels. Then the prob-
1. Given(cid:28)d, nd out the features that give the
2. Given a maximum allowable generalization error(cid:13),
nd the smallest number of features.

lem of feature selection can be formally addressed in the
following two ways [19]:

smallest expected generalization error; or

Unlike e.g. Gaussian Processes in regression, Support
Vector Machines (SVMs) do not offer the opportunity of
an automated internal relevance detection and hence algo-
rithms for feature selection play an important role. In the lit-
erature there are known two general approaches to solve the
feature selection problem: The lter approach and the wrap-
per approach [10]: In a lter method feature selection is per-
formed as a preprocessing step to the actual learning algo-
rithm, i.e. before applying the classier to the selected fea-
ture subset. Features are selected with regard to some prede-
ned relevance measure which is independent of the actual
generalization performance of the learning algorithm. This
can mislead the feature selection algorithm [10]. Wrapper
methods, on the other hand, train the classier system with
a given feature subset as an input and return the estimated
generalization performance of the learning machine as an
evaluation of the feature subset. This step is repeated for
each feature subset taken into consideration. Depending on
the bias and variance of the used estimator this method in-
duces some overtting of the wrapper algorithm. Tradition-

allyk-fold cross-validation is used as an estimator of the
thank-fold cross-validation, since we do not have to re-
traink times on each feature subset but just once. Addition-
SVM, like the regularization parameterC in parallel. As far

ally we will see that although in general the error bounds
have a higher bias than cross-validation in practical situa-
tions they often have a lower variance and can thus reduce
the overtting of the wrapper algorithm. The feature selec-
tion process is done in an evolutionary way. The genetic en-
coding also allows us to optimize different parameters of the

In this paper we are taking existing theoretical bounds
on the generalization error for SVMs instead of perform-
ing cross-validation. This is computationally much faster

generalization error.

as we know, this is completely new approach, since previ-
ous work (e.g. [14]) on feature selection by means of Ge-
netic Algorithms (GAs) neither took advantage of the gen-
eralization error bounds nor on the possibility of an inter-

(5)

choice of the sample.

is rather expensive, one can upper bound

is the prob-
ability of test error for the machine trained on a sample of

Note that this theorem assumes a SVM without thresh-
old. Thus in general one has to transform the data in feature

be the tu-
ple of Langragian multipliers which are obtained by max-
imizing functional (1). Supposed we have a SVM without
threshold. Under the assumption that the set of support vec-

Theorem 2. (Vapnik, Chapelle [17, 2]) Let(cid:11)
tors does not change when removing example we have
e(cid:20)1EX=1   (cid:11)
1SV1!
E1
where :R!f0;1g is the step function v=
(cid:26)0:
v(cid:20)0
,SV is the matrix of dot products
1: otherwise
between support vectors in feature space,1
e
size1 and the expectations are taken over the random
space beforehand to ensure this condition, e.g. by(cid:30)xi7!
(cid:30)xi1j=1(cid:30)xj.
As the computation of the inverse of the matrixSV
1SV by
1
kx;x
e(cid:20)1EX=1 (cid:11)kx;x1
bound [9]:E1
SVM has to trained just once rather thank times when per-
formingk-fold cross-validation.
space of size2d
mulitplied by the term10:001#ffeatures selectedg
d

Let us rst put our focus on the case where the number
of features to be selected is not known beforehand (prob-
lem 2 on the preceding page). In this case we have a search
. We can represent this search space by a
standard binary encoding where a 1 indicates the selec-
tion of the feature at the corresponding position. The t-
ness of each feature subset is evaluated by either bound (3)
or by bound (6) or, in the traditional way, by performing
cross-validation. We will call the corresponding algorithms
GAR2W2, GAJH respectively GAAcc. To break a tie for
smaller feature subsets the generalization error estimate is
,
where the constant 0.001 aims to be lower than the stan-
dard deviation of the estimate of the generalization perfor-
mance for a given feature subset.

Other bounds also exist, but they will not taken into con-
sideration here. For the computation of all these bounds the

3. Genetic Algorithms for Feature Selection

3.1. Encodings and Fitness Functions

[3]. Thus one recovers the Jaakkola-Haussler

using SVMs

(6)

mance of a SVM

forming the following optimization procedure:

As well known, SVM training [4, 15] is done by per-

pected generalization error (or risk) over all possible pat-

2. Estimating the Generalization Perfor-

Kronecker symbol (c.f.[3]). This is commonly referred to
as 2-norm C-SVM.

nal kernel parameter optimization. This paper is organized
as follows: In the next section we will rst review differ-
ent possibilities of estimating the generalization error of a
SVM. Afterwards several settings of GAs for feature selec-
tion will be presented. Our new approaches will be com-
pared to each other as well as to the traditional approach by
performing cross-validation and to three popular existing al-
gorithms for feature selection on two articial and two real
life data sets in section 4. Section 5 is a critical discussion,
and section 6 draws a general conclusion.

ax(cid:11)W2(cid:11)=i=1(cid:11)i12i;j=1(cid:11)i(cid:11)jyiyjkxi;xj
i=1(cid:11)iyi=0
subject to0(cid:20)(cid:11)i for alli=1;:::; and
wherek is the kernel function. One can deal with the non-
separable case by e.g. mappingkxi;xj7!kxi;xj
1Cij withC being a regularization parameter and the
A goal of every classierf is to mimimize the ex-
terns drawn from the unknown distributionx;y(see e.g.
R[f=ZXY`x;y;fxdx;y
with` being some loss-function. However, since (2) is not
computable (as we dont know), we are forced to estimate
Theorem 1. (Vapnik [16]) Let(cid:26) be the size of the maximal
margin and(cid:30)x1, ...,(cid:30)x the images of the training pat-
diusR. Let(cid:11)
the training data of size belonging to a sphere of radiusR
are separable with the corresponding margin(cid:26), then the ex-
pectation of the test error probability1
e(cid:20)1E(cid:26)R2(cid:26)2(cid:27)=1ER2W2(cid:11)
E1
where expectation is taken over samples of size1.
Following [15] one can calculateR2
R2=ax(cid:12)i(cid:12)ikxi;xii;j(cid:12)i(cid:12)jkxi;xj
i(cid:12)i=1;(cid:12)i(cid:21)0;i=1;:::;

the generalization performance. Besides statistical general
methods, like cross-validaton, for SVM there exist theoreti-
cal bounds on the leave-one-out error ([16, 17, 2]). Three of
them are cited here:

terns in feature space which are lying within a sphere of ra-
be the tuple of Langragian multipliers which
are obtained by maximizing functional (1). If the images of

by maximizing:

has the bound

subject to

[16])

(3)

(4)

(1)

(2)

The termR2
1i=1kxi;xi12i;j=1kxi;xj, which means that
every(cid:12)i=1 in (4). In the multiclass case one can compute
one of the bounds pair wise for classes
and
0

in bound (3) is estimated by the expression

and sum it

up.

of size

would be inefcient. Therefore in this case

of the feature which is selected. Of course one has to make

In the case where the number of features to be selected
is known beforehand (problem 1 on page 1) a binary encod-
ing in the prior way due to the much smaller search space

The GA used here is the CHC algorithm by Eshelman
[5], which was reported to perform a more aggressive and
faster search strategy than the traditional Simple GA [7].
One of the main ideas of CHC is the so called population
elitist strategy: The best individuals of the child-generation
replace the worst individuals of the parent-generetation. In-
dividuals are selected randomly for recombination, but they
are only allowed to mate, if their genomes are not too close
(with regard to some metric). Then half of the differing
genes between parents are exchanged to produce children.
If no recombination is possible any more, because the indi-
viduals are too similiar, the algorithm is restarted. The best
individual is left untouched and the others are created by
heavy mutation of the best individual.

(cid:18)d(cid:19)
it is reasonable to switch to a decimal encoding
1;:::;

where
i2f1;:::;dgi=1;:::; indicates the number
sure that each
i is unique in the code. Because the genomes
used tness function-GAR2W2,-GAJH respectively
-GAAcc.
least 10,000 generations had been computed. For the-
GAR2W2,-GAJH,-GAAcc at least 1,000 generations
mization of the regularization parameterC. If we have a bi-
nary genomeb1;:::;bdbi2f0;1g we can simply concate-
nate a binary representation of the parameterC to our ex-
lect an optimal feature subset and an optimalC at the same
terC is inuenced by the feature subset taken into account

are much shorter with decimal index encoding than with bi-
nary encoding, the probability to mutate was set from 35%
in the standard CHC to 50% for each gene. In analogy to
the binary case we will call the GAs in dependence of their

GAAcc, GAR2W and GAJH were stopped, if the best
solution didnt change for the last 200 generations at at

isting chromosome and run GAAcc, GAR2W2 respectively
GAJH on this new genome. That means we are trying to se-

3.2. Optimizing Kernel Parameters with Genetic

time. This is reasonable, because the choice of the parame-

were necessary.

Algorithms

The GAs described above can also be used for the opti-

and vice versa. Usually it is not necessary to consider any

arbitrary value ofC, but only certain discrete values, e.g.
0:001;0:01;:::;1000 respectively103;:::;103
bits we can code the numbers0;:::;7, or, if we shift this rep-
resentation by -3, the numbers3;:::;4 which can be inter-

. With just 3

preted as powers of 10. In a similar manner one could use
GAs to optimize e.g. the width of an RBF kernel.

4. Experiments

4.1. Algorithms and Tests

we want to compare these algorithms to GAAcc, GAR2W2
and GAJH, we will consider the situation where the least
test error was reached. To test whether certain observed dif-
ferences between test errors are statistically signicant, the

We will compare the GAs described above to each other
and to the Fisher Criterion Score (e.g. [18]), Relief-F [11]
and Recursive Feature Elimination (RFE) [8] algorithm.
Fisher Criterion Score and Relief-F are two lter methods
and RFE is a wrapper method, which was especially de-
signed for SVMs. Fisher Criterion Score, Relief-F, and RFE
can only select a given number of features and hence one

has to try different numbers of features systemetically. If
corrected resampled-test recently proposed by C. Nadeau
probability of the two classesy=1 andy=1 was
equal. The rst three featuresfX1;X2;X3g are drawn as
Xi=yi;1 and the second three featuresfX4;X5;X6g
were drawn asXi=0;1 with a probability of 0.7, oth-
erwise the rst three were drawn asXi=0;1 and the
second three asXi=yi3;1. The remaining fea-
tures are noiseXi=0;20i=7;:::;202, and the rst

and Y. Bengio is used [12]. This test behaves very conserva-
tively, which means the test might state a certain observed
difference of results is not signicant although it actually is,
more often than one might expect by the prescribed signi-
cance level of 5%.

4.2.1. Toy Data The articial data set was created as de-
scribed in [19]: Six dimensions of 202 are relevant. The

4.2. Data Sets

six features still have redundancy (for more details see [6]).

4.2.2. Real Life Data
Colon Cancer The Colon cancer problem is described e.g.
in [1]: 62 tissue samples probed by DNA microarrays con-
tain 22 normal and 40 colon cancer examples. these two
classes have to be discriminated by the experession of 2,000
genes.
Yeast Data Set The Yeast microarray data set (Brown
Yeast data set, see e.g. [13]) consists of 208 genes that have
to be discriminated into 5 classes based on 79 gene expres-

5. Discussion

above one can state the following:

used which discriminates classes pairwise (one-versus-one

As a general conclusion from the experiments shown

the search process, while this is not the case for the GAAcc
algorithm.

SVM was set by the same procedure as described above.
The results are shown in tables 3 and 6.

algorithm the tness of each individual was determined by
means of 7-fold cross-validation. In the experiment where

GAR2W2. Like in prior experiments m-GAR2W2 and m-
GAJH are always better than m-GAAcc. Again the GAs us-

the number of features to be selected is xed, was set
to10;20;40. A linear kernel with a 2-norm C-SVM was
method). The parameterC in the cost function for the C-
For=10 Relief-F is signicantly worse than 10-
ing bounds seem to win something, ifC is optimized during
(cid:15) GAs using cross-validation to evaluate a given fea-
(cid:15) Using leave-one-out error bounds instead is an alter-
(cid:15) Optimizing kernel parameters within the GA is useful,
(cid:15) GAs using theR2W2
one would need all in all76=42 runs. This takes
422=84 minutes, which is almost 3 times mores

bound show a comparable gen-
eralization performance to RFE. The number of se-
lected features is in most cases a bit higher. If the num-
ber of features is not xed beforehand and kernel pa-
rameters are optimized within the Genetic Algorithm,
one can in fact save time by using such an algorithm in-
stead of running RFE multiple times to determine ker-
nel parameters and an appropriate number of features.
E.g. one run of GAR2W2 with parameter optimization
on the Colon data set needs about 30 minutes on a Pen-
tium IV with 2GHz. One run of RFE takes about 2
minutes, but to determine the regularization parame-
ter (0.001, 0.01, 0.1, 1, 10, 100, 1000) and the appro-
priate number of features (20, 50, 100, 250, 500, 1000)

native. It leads to a better generalization performance
in most cases, but if the number of features to select is
not xed beforehand, a higher number of features is se-
lected than with cross-validation.

ture subset show in some cases a signicant overt-
ting problem.

especially if leave-one-out error bounds are used.

than one run of GAR2W2. If the number of features to
select is known beforehand, however, GAs do not of-
fer these advantages any more.

sion values corresponding to different experimental condi-
tions.

4.3. Results

lems of lter selection methods.

induced the best average test error was picked from the dis-
. Results are shown in tables 1 and
4. The difference between lter methods and wrapper meth-

4.3.1. Toy Data 30 instances of 100 training points cor-
responding 30 trials were created. On each trial the solu-
tion returned by an algorithm was tested on 10,000 inde-
pendent test points. All data was scaled to the interval [-1,
1]. For the GAAcc algorithm the tness of each individ-
ual was determined by means of 4-fold cross-validation. In
the experiment where the number of features to be selected

4.3.2. Real Life Data
Colon Cancer The data was split into a training set of 50
and a test set of 12 for 50 times, and results were averaged
over these 50 trials. The data was normalized to mean 0 and
standard deviation 1 for each feature. For the GAAcc al-
gorithm the tness of each individual was determined by
means of 10-fold cross-validation. In the experiment where

is xed, was set to2;4. A 2-norm C-SVM with a lin-
ear kernel was used. The regularization parameterC which
crete values103;:::;103
ods for=2 is signicant. This shows the principal prob-
the number of features to be selected is xed, was set to
20;50;100;250;500;1000. A linear kernel with a 2-norm
C-SVM was used. The parameterC in the cost function
Form xed m-GAAcc is the overall worst performing
algorithm, especially if is small. For=20 the differ-
 is, the overtting problem becomes less serious with in-
creasing.
If is not xed, GAR2W2 and GAJH are both better
than GAAcc again. ForC=0:01 GAAcc performs sig-
case whereC is not xed, the difference between GAAcc
GAJH seem to win something ifC is optimized by the GA,

ence to the other methods is signicant. This can be inter-
preted as an overtting problem due to a higher variance of
the cross-validation estimate of the generalization error. As
the number of subsets to be evaluated is higher the smaller

for the C-SVM was set by the same procedure as described
above. The results are shown in tables 2 and 5.

and GAR2W2/GAJH is signicant. Both GAR2W2 and

nicantly worse than the best Relief-F algorithm. In the

whereas GAAcc does not. GAR2W2 and GAJH reach ap-
proximately the same test error as the best RFE in this case.
They are only slightly worse than the best Relief-F algo-
rithm.
Yeast Data Set 8-fold cross-validation was performed on
this data for all methods. The data was normalized to mean
0 and standard deviation 1 for each feature. For the GAAcc

[5] L. Eshelman. The CHC Adaptive Search Algorithm, How
toHave Safe Search When Engaging in Non-traditional Ge-
netic Recombination. Morgan Kaufman, 1991.

[6] H. Frhlich.

Feature Selection for Support Vector Ma-
chines by Means of Genetic Algorithms. Masters thesis,
University of Marburg, 2002. http://www-ra/informatik.uni-
tuebingen.de/mitarb/froehlich.

[7] D. Goldberg. Genetic Algorithms in Search, Optimization

and Machine Learning. Addison Wesley, Reading, 1998.

[8] I. Guyon, J. Weston, S. Barnhill, and V. Vapnik. Gene Se-
lection for Cancer Classication using Support Vector Ma-
chines. Machine Learning, 46:389  422, 2002.

[9] T. S. Jaakkola and D. Haussler. Probalistic kernel regression
models. In Proceedings of the 1999 Conference on AI and
Statistics, 1999.

[10] R. Kohavi and G. John. Wrappers for Feature Subset Selec-

tion. Articial Intelligence, 97(12):273  324, 1997.

[11] I. Kononenko and S. J. Hong. Attribute Selection for Mod-
eling. Future Generation Computer Systems, 13(2 - 3):181 
195, 1997.

[12] C. Nadeau and Y. Bengio. Inference for the Generalization
Error.
In S. Solla, T. Leen, and K.-R. Mller, editors, Ad-
vances in Neural Information Processing Systems 12, Cam-
bridge, MA, 2000. MIT Press.

[13] P. Pavlidis, J. Weston, J. Cai, and W. Grundy. Gene func-
tional classication from heteregoneous data.
In Proceed-
ings of the fth International Conference on Computational
Molecular Biology, pages 242  248, 2001.

[14] S. Salcedo-Sanz, M. Prado-Cumplido, F. Perez-Cruz, and
C. Bousono-Calzon. Feature Selection via Genetic Opti-
mization. In Proc. ICANN 2002, pages 547  552, 2002.

[15] B. Schlkopf, C. Burges, and V. Vapnik. Extracting support
data for a given task. In U. N. Fayyad and R. Uthurusamy,
editors, First International Conference for Knowledge Dis-
covery and Data Mining, Menlo Park, 1995. AAAI Press.

[16] V. Vapnik. Statistical Learning Theory.

John Wiley and

Sons, New York, 1998.

[17] V. Vapnik and O. Chapelle. Bounds on error expectation for
Support Vector Machines. Neural Computation, 12(9), 2000.
[18] J. Weston, A. Elisseeff, B. Schlkopf, and M. Tipping. Use of
the zero-norm with linear models and kernel methods. JMLR
special Issue on Variable and Feature Selection, 3:1439 
1461, 2002.

[19] J. Weston, S. Mukherjee, O. Chapelle, M. Pontil, T.Poggio,
and V. Vapnik. Feature selection for SVMs.
In S. Solla,
T. Leen, and K.-R. Mller, editors, Advances in Neural Infor-
mation Processing Systems 13, Cambride, MA, 2001. MIT
Press.

6. Conclusion

In this paper we dealt with the problem of feature se-
lection for SVMs by means of GAs. In contrast to the tra-
ditional way of performing cross-validation to estimate the
generalization error induced by a given feature subset we
proposed to use the theoretical bounds on the generaliza-
tion error for SVMs, which is computationally attractive.
If the number of features to be selected is xed and hence
the search space is much smaller than with a variable num-
ber of selected features, we proposed a decimal encoding,
which is much more efciently than a binary encoding. If
the number of features to be selected is not xed before-
hand, the usual binary encoding was taken. Additionally to
the selection of a feature subset, one can optimize kernel pa-

by means of GAs. This is reasonable, because the choice of
the feature subset has an inuence on the appropriate ker-
nel parameters and vice versa.

rameters such as the regularization parameterC of the SVM
theR2W2
tting in comparison withk-fold cross-validation in most

Existing algorithms such as Fisher Criterion Score,
Relief-F and Recursive Feature Elimination were com-
pared to GAs using cross-validation and to GAs us-
ing two different error bounds on two toy problems and
two DNA micro array data sets. Hereby Recursive Fea-
ture Elimination is a heuristic wrapper algorithm which
was especially designed for SVMs, and Fisher Crite-
rion Score and Relief-F are two lter algorithms. As a con-
clusion of the experiments one can state that GAs using
bound and optimizing various kernel param-
eters are a recommendable alternative, if the number of
features to select is not known beforehand. It reduces over-

of our experiments, because of a lower variance of the gen-
eralization error estimate. Additionally, in comparison with
running RFE multiple times to determine the kernel param-
eters and an appropriate feature subset, one in fact saves
time.

