Multiplicative Updates for Nonnegative Quadratic

Programming in Support Vector Machines

Fei Sha1, Lawrence K. Saul1, and Daniel D. Lee2
1Department of Computer and Information Science
2Department of Electrical and System Engineering

University of Pennsylvania

200 South 33rd Street, Philadelphia, PA 19104

ffeisha,lsaulg@cis.upenn.edu, ddlee@ee.upenn.edu

Abstract

We derive multiplicative updates for solving the nonnegative quadratic
programming problem in support vector machines (SVMs). The updates
have a simple closed form, and we prove that they converge monotoni-
cally to the solution of the maximum margin hyperplane. The updates
optimize the traditionally proposed objective function for SVMs. They
do not involve any heuristics such as choosing a learning rate or deciding
which variables to update at each iteration. They can be used to adjust all
the quadratic programming variables in parallel with a guarantee of im-
provement at each iteration. We analyze the asymptotic convergence of
the updates and show that the coefﬁcients of non-support vectors decay
geometrically to zero at a rate that depends on their margins. In practice,
the updates converge very rapidly to good classiﬁers.

1 Introduction

Support vector machines (SVMs) currently provide state-of-the-art solutions to many prob-
lems in machine learning and statistical pattern recognition[18]. Their superior perfor-
mance is owed to the particular way they manage the tradeoff between bias (underﬁtting)
and variance (overﬁtting). In SVMs, kernel methods are used to map inputs into a higher,
potentially inﬁnite, dimensional feature space; the decision boundary between classes is
then identiﬁed as the maximum margin hyperplane in the feature space. While SVMs pro-
vide the ﬂexibility to implement highly nonlinear classiﬁers, the maximum margin criterion
helps to control the capacity for overﬁtting. In practice, SVMs generalize very well — even
better than their theory suggests.

Computing the maximum margin hyperplane in SVMs gives rise to a problem in nonnega-
tive quadratic programming. The resulting optimization is convex, but due to the nonneg-
ativity constraints, it cannot be solved in closed form, and iterative solutions are required.
There is a large literature on iterative algorithms for nonnegative quadratic programming
in general and for SVMs as a special case[3, 17]. Gradient-based methods are the simplest
possible approach, but their convergence depends on careful selection of the learning rate,
as well as constant attention to the nonnegativity constraints which may not be naturally
enforced. Multiplicative updates based on exponentiated gradients (EG)[5, 10] have been

investigated as an alternative to traditional gradient-based methods. Multiplicative updates
are naturally suited to sparse nonnegative optimizations, but EG updates—like their addi-
tive counterparts—suffer the drawback of having to choose a learning rate.

Subset selection methods constitute another approach to the problem of nonnegative
quadratic programming in SVMs. Generally speaking, these methods split the variables
at each iteration into two sets: a ﬁxed set in which the variables are held constant, and a
working set in which the variables are optimized by an internal subroutine. At the end of
each iteration, a heuristic is used to transfer variables between the two sets and improve
the objective function. An extreme version of this approach is the method of Sequential
Minimal Optimization (SMO)[15], which updates only two variables per iteration. In this
case, there exists an analytical solution for the updates, so that one avoids the expense of a
potentially iterative optimization within each iteration of the main loop.

In general, despite the many proposed approaches for training SVMs, solving the quadratic
programming problem remains a bottleneck in their implementation. (Some researchers
have even advocated changing the objective function in SVMs to simplify the required
optimization[8, 13].) In this paper, we propose a new iterative algorithm, called Multi-
plicative Margin Maximization (M3), for training SVMs. The M3 updates have a simple
closed form and converge monotonically to the solution of the maximum margin hyper-
plane. They do not involve heuristics such as the setting of a learning rate or the switching
between ﬁxed and working subsets; all the variables are updated in parallel. They pro-
vide an extremely straightforward way to implement traditional SVMs. Experimental and
theoretical results conﬁrm the promise of our approach.

2 Nonnegative quadratic programming

We begin by studying the general problem of nonnegative quadratic programming. Con-
sider the minimization of the quadratic objective function

F (v) =

vT Av + bT v;

(1)

1
2

subject to the constraints that vi (cid:21) 0 8i. We assume that the matrix A is symmetric and
semipositive deﬁnite, so that the objective function F (v) is bounded below, and its opti-
mization is convex. Due to the nonnegativity constraints, however, there does not exist an
analytical solution for the global minimum (or minima), and an iterative solution is needed.

2.1 Multiplicative updates

Our iterative solution is expressed in terms of the positive and negative components of the
matrix A in eq. (1). In particular, let A+ and A(cid:0) denote the nonnegative matrices:

A+

ij =(cid:26) Aij

0

if Aij > 0,
otherwise,

and A(cid:0)

ij =(cid:26) jAij j

0

if Aij < 0,
otherwise.

(2)

It follows trivially that A = A+(cid:0)A(cid:0). In terms of these nonnegative matrices, our proposed
updates (to be applied in parallel to all the elements of v) take the form:

vi  (cid:0) vi" (cid:0)bi +pb2

i + 4(A+v)i(A(cid:0)v)i
2(A+v)i

# :

(3)

The iterative updates in eq. (3) are remarkably simple to implement. Their somewhat mys-
terious form will be clariﬁed as we proceed. Let us begin with two simple observations.
First, eq. (3) prescribes a multiplicative update for the ith element of v in terms of the
ith elements of the vectors b, A+v, and A+v. Second, since the elements of v, A+,
and A(cid:0) are nonnegative, the overall factor multiplying vi on the right hand side of eq. (3)
is always nonnegative. Hence, these updates never violate the constraints of nonnegativity.

2.2 Fixed points

We can show further that these updates have ﬁxed points wherever the objective func-
tion, F (v) achieves its minimum value. Let v(cid:3) denote a global minimum of F (v). At
such a point, one of two conditions must hold for each element v (cid:3)
i > 0 and
i = 0 and (@F=@vi)jv(cid:3) (cid:21) 0. The ﬁrst condition applies to the
(@F=@vi)jv(cid:3) = 0, or (ii), v(cid:3)
positive elements of v(cid:3), whose corresponding terms in the gradient must vanish. These
derivatives are given by:

i: either (i) v(cid:3)

@F

@vi(cid:12)(cid:12)(cid:12)(cid:12)v(cid:3)

= (A+v(cid:3))i (cid:0) (A(cid:0)v(cid:3))i + bi:

(4)

The second condition applies to the zero elements of v(cid:3). Here, the corresponding terms of
the gradient must be nonnegative, thus pinning v (cid:3)
i to the boundary of the feasibility region.
The multiplicative updates in eq. (3) have ﬁxed points wherever the conditions for global
minima are satisﬁed. To see this, let

(cid:13)i

4
=

(cid:0)bi +pb2

i + 4(A+v(cid:3))i(A(cid:0)v(cid:3))i
2(A+v(cid:3))i

(5)

denote the factor multiplying the ith element of v in eq. (3), evaluated at v(cid:3). Fixed points
of the multiplicative updates occur when one of two conditions holds for each element vi:
either (i) v(cid:3)
i = 0. It is straightforward to show from eqs. (4–5)
that (@F=@vi)jv(cid:3)= 0 implies (cid:13)i = 1. Thus the conditions for global minima establish the
conditions for ﬁxed points of the multiplicative updates.

i > 0 and (cid:13)i = 1, or (ii) v(cid:3)

2.3 Monotonic convergence

The updates not only have the correct ﬁxed points; they also lead to monotonic improve-
ment in the objective function, F (v). This is established by the following theorem:

Theorem 1 The function F (v) in eq. (1) decreases monotonically to the value of its global
minimum under the multiplicative updates in eq. (3).

The proof of this theorem (sketched in Appendix A) relies on the construction of an auxil-
iary function which provides an upper bound on F (v). Similar methods have been used to
prove the convergence of many algorithms in machine learning[1, 4, 6, 7, 12, 16].

3 Support vector machines

We now consider the problem of computing the maximum margin hyperplane in SVMs[3,
17, 18]. Let f(xi; yi)gN
i=1 denote labeled examples with binary class labels yi = (cid:6)1, and
let K(xi; xj) denote the kernel dot product between inputs. In this paper, we focus on the
simple case where in the high dimensional feature space, the classes are linearly separable
and the hyperplane is required to pass through the origin1.
In this case, the maximum
margin hyperplane is obtained by minimizing the loss function:

L((cid:11)) = (cid:0)Xi

1

2Xij

(cid:11)i +

(cid:11)i(cid:11)jyiyjK(xi; xj);

(6)

subject to the nonnegativity constraints (cid:11)i (cid:21) 0. Let (cid:11)(cid:3) denote the location of the minimum
i yixi

of this loss function. The maximal margin hyperplane has normal vector w = Pi (cid:11)(cid:3)

and satisﬁes the margin constraints yiK(w; xi) (cid:21) 1 for all examples in the training set.

1The extensions to non-realizable data sets and to hyperplanes that do not pass through the origin

are straightforward. They will be treated in a longer paper.

Kernel
Data
Sonar
Breast cancer

Polynomial
k = 4
(cid:27) = 0:3
k = 6
9.6% 9.6% 7.6%
5.1% 3.6% 4.4%

Radial
(cid:27) = 1:0
6.7%
4.4%

(cid:27) = 3:0
10.6%
4.4%

Table 1: Misclassiﬁcation error rates on the sonar and breast cancer data sets after 512
iterations of the multiplicative updates.

3.1 Multiplicative updates

The loss function in eq. (6) is a special case of eq. (1) with Aij = yiyjK(xi; xj) and
bi = (cid:0)1. Thus, the multiplicative updates for computing the maximal margin hyperplane
in hard margin SVMs are given by:

(cid:11)i  (cid:0) (cid:11)i" 1 +p1 + 4(A+(cid:11))i(A(cid:0)(cid:11))i

2(A+(cid:11))i

#

(7)

where A(cid:6) are deﬁned as in eq. (2). We will refer to the learning algorithm for hard margin
SVMs based on these updates as Multiplicative Margin Maximization (M3).

It is worth comparing the properties of these updates to those of other approaches. Like
multiplicative updates based on exponentiated gradients (EG)[5, 10], the M3 updates are
well suited to sparse nonnegative optimizations2; unlike EG updates, however, they do
not involve a learning rate, and they come with a guarantee of monotonic improvement.
Like the updates for Sequential Minimal Optimization (SMO)[15], the M3 updates have
a simple closed form; unlike SMO updates, however, they can be used to adjust all the
quadratic programming variables in parallel (or any subset thereof), not just two at a time.
Finally, we emphasize that the M3 updates optimize the traditional objective function for
SVMs; they do not compromise the goal of computing the maximal margin hyperplane.

3.2 Experimental results

We tested the effectiveness of the multiplicative updates in eq. (7) on two real world prob-
lems: binary classiﬁcation of aspect-angle dependent sonar signals[9] and breast cancer
data[14]. Both data sets, available from the UCI Machine Learning Repository[2], have
been widely used to benchmark many learning algorithms, including SVMs[5]. The sonar
and breast cancer data sets consist of 208 and 683 labeled examples, respectively. Train-
ing and test sets for the breast cancer experiments were created by 80%/20% splits of the
available data.

We experimented with both polynomial and radial basis function kernels. The polynomial
kernels had degrees k = 4 and k = 6, while the radial basis function kernels had variances
of (cid:27) = 0:3; 1:0 and 3:0. The coefﬁcients (cid:11)i were uniformly initialized to a value of one in
all experiments.

Misclassiﬁcation rates on the test data sets after 512 iterations of the multiplicative updates
are shown in Table 1. As expected, the results match previously published error rates on
these data sets[5], showing that the M3 updates do in practice converge to the maximum
margin hyperplane. Figure 1 shows the rapid convergence of the updates to good classiﬁers
in just one or two iterations.

2In fact, the multiplicative updates by nature cannot directly set a variable to zero. However,
a variable can be clamped to zero whenever its value falls below some threshold (e.g., machine
precision) and when a zero value would satisfy the Karush-Kuhn-Tucker conditions.

iteration

support vectors

non-support vectors

  00

  01

  02

  04

  08

  16

  32

  64

s
t
n
e
i
c
i
f
f
e
o
c

0
0

100

200
training examples

300

400

500

 (%)      e
t

   e
 (%)
g
      2.9           3.6

      2.4           2.2

      1.1           4.4

      0.5           4.4

      0.0           4.4

      0.0           4.4

      0.0           4.4

      0.0           4.4

Figure 1: Rapid convergence of the multiplicative updates in eq. (7). The plots show
results after different numbers of iterations on the breast cancer data set with the radial
basis function kernel ((cid:27) = 3). The horizontal axes index the coefﬁcients (cid:11)i of the 546
training examples; the vertical axes show their values. For ease of visualization, the training
examples were ordered so that support vectors appear to the left and non-support vectors,
to the right. The coefﬁcients (cid:11)i were uniformly initialized to a value of one. Note the rapid
attenuation of non-support vector coefﬁcients after one or two iterations. Intermediate error
rates on the training set ((cid:15)t) and test set ((cid:15)g) are also shown.

3.3 Asymptotic convergence

The rapid decay of non-support vector coefﬁcients in Fig. 1 motivated us to analyze their
rates of asymptotic convergence. Suppose we perturb just one of the non-support vector
coefﬁcients in eq. (6)—say (cid:11)i–away from the ﬁxed point to some small nonzero value (cid:14)(cid:11)i.
If we hold all the variables but (cid:11)i ﬁxed and apply its multiplicative update, then the new
displacement (cid:14)(cid:11)0

i after the update is given asymptotically by ((cid:14)(cid:11)0

i) (cid:25) ((cid:14)(cid:11)i)(cid:13)i, where

(cid:13)i =

;

(8)

1 +p1 + 4(A+(cid:11)(cid:3))i(A(cid:0)(cid:11)(cid:3))i

2(A+(cid:11)(cid:3))i

and Aij = yiyjK(xi; xj). (Eq. (8) is merely the specialization of eq. (5) to SVMs.) We can
thus bound the asymptotic rate of convergence—in this idealized but instructive setting—
by computing an upper bound on (cid:13)i, which determines how fast the perturbed coefﬁcient
decays to zero. (Smaller (cid:13)i implies faster decay.) In general, the asymptotic rate of con-
vergence is determined by the overall positioning of the data points and classiﬁcation hy-
perplane in the feature space. The following theorem, however, provides a simple bound in
terms of easily understood geometric quantities.

feature space from xi to the maximum margin hyperplane, and let d = minj dj =

Theorem 2 Let di = jK(xi; w)j=pK(w; w) denote the perpendicular distance in the
1=pK(w; w) denote the one-sided margin of the classiﬁer. Also, let ‘i =pK(xi; xi)

denote the distance of xi to the origin in the feature space, and let ‘ = maxj ‘j denote the
largest such distance. Then a bound on the asymptotic rate of convergence (cid:13)i is given by:

(cid:13)i (cid:20) (cid:20)1 +

1
2

(di (cid:0) d)d

‘i‘

(cid:21)(cid:0)1

:

(9)

+

+

+

l

i

d

i

_

+

d

classification hyperplane

_

_

_

Figure 2: Quantities used to bound the asymptotic rate of convergence in eq. (9); see text.
Solid circles denote support vectors; empty circles denote non-support vectors.

The proof of this theorem is sketched in Appendix B. Figure 2 gives a schematic repre-
sentation of the quantities that appear in the bound. The bound has a simple geometric
intuition:
the more distant a non-support vector from the classiﬁcation hyperplane, the
faster its coefﬁcient decays to zero. This is a highly desirable property for large numeri-
cal calculations, suggesting that the multiplicative updates could be used to quickly prune
away outliers and reduce the size of the quadratic programming problem. Note that while
the bound is insensitive to the scale of the inputs, its tightness does depend on their relative
locations in the feature space.

4 Conclusion

SVMs represent one of the most widely used architectures in machine learning. In this
paper, we have derived simple, closed form multiplicative updates for solving the non-
negative quadratic programming problem in SVMs. The M3 updates are straightforward
to implement and have a rigorous guarantee of monotonic convergence. It is intriguing
that multiplicative updates derived from auxiliary functions appear in so many other areas
of machine learning, especially those involving sparse, nonnegative optimizations. Exam-
ples include the Baum-Welch algorithm[1] for discrete hidden markov models, general-
ized iterative scaling[6] and adaBoost[4] for logistic regression, and nonnegative matrix
factorization[11, 12] for dimensionality reduction and feature extraction. In these areas,
simple multiplicative updates with guarantees of monotonic convergence have emerged
over time as preferred methods of optimization. Thus it seems worthwhile to explore their
full potential for SVMs.

References

[1] L. Baum. An inequality and associated maximization technique in statistical estimation of

probabilistic functions of Markov processes. Inequalities, 3:1–8, 1972.

[2] C. L. Blake and C. J. Merz. UCI repository of machine learning databases, 1998.

[3] C. J. C. Burges. A tutorial on support vector machines for pattern recognition. Knowledge

Discovery and Data Mining, 2(2):121–167, 1998.

[4] M. Collins, R. Schapire, and Y. Singer. Logistic regression, adaBoost, and Bregman distances.
In Proceedings of the Thirteenth Annual Conference on Computational Learning Theory, 2000.

[5] N. Cristianini, C. Campbell, and J. Shawe-Taylor. Multiplicative updatings for support vector

machines. In Proceedings of ESANN’99, pages 189–194, 1999.

[6] J. N. Darroch and D. Ratcliff. Generalized iterative scaling for log-linear models. Annals of

Mathematical Statistics, 43:1470–1480, 1972.

[7] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via

the EM algorithm. Journal of the Royal Statistical Society B, 39:1–37, 1977.

[8] C. Gentile. A new approximate maximal margin classiﬁcation algorithm. Journal of Machine

Learning Research, 2:213–242, 2001.

[9] R. P. Gorman and T. J. Sejnowski. Analysis of hidden units in a layered network trained to

classify sonar targets. Neural Networks, 1(1):75–89, 1988.

[10] J. Kivinen and M. Warmuth. Exponentiated gradient versus gradient descent for linear predic-

tors. Information and Computation, 132(1):1–63, 1997.

[11] D. D. Lee and H. S. Seung. Learning the parts of objects with nonnegative matrix factorization.

Nature, 401:788–791, 1999.

[12] D. D. Lee and H. S. Seung. Algorithms for non-negative matrix factorization. In T. K. Leen,
T. G. Dietterich, and V. Tresp, editors, Advances in Neural and Information Processing Systems,
volume 13, Cambridge, MA, 2001. MIT Press.

[13] O. L. Mangasarian and D. R. Musicant. Lagrangian support vector machines. Journal of Ma-

chine Learning Research, 1:161–177, 2001.

[14] O. L. Mangasarian and W. H. Wolberg. Cancer diagnosis via linear programming. SIAM News,

23(5):1–18, 1990.

[15] J. Platt. Fast training of support vector machines using sequential minimal optimization. In
B. Sch¨olkopf, C. J. C. Burges, and A. J. Smola, editors, Advances in Kernel Methods — Support
Vector Learning, pages 185–208, Cambridge, MA, 1999. MIT Press.

[16] L. K. Saul and D. D. Lee. Multiplicative updates for classiﬁcation by mixture models.

In
T. G. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural and Information
Processing Systems, volume 14, Cambridge, MA, 2002. MIT Press.

[17] B. Sch¨olkopf and A. J. Smola. Learning with Kernels. MIT Press, Cambridge, MA, 2002.
[18] V. Vapnik. Statistical Learning Theory. Wiley, N.Y., 1998.

A Proof of Theorem 1

The proof of monotonic convergence in the objective function F (v), eq. (1), is based on
the derivation of an auxiliary function. Similar techniques have been used for many models
in statistical learning[1, 4, 6, 7, 12, 16]. An auxiliary function G(~v; v) has the two crucial
properties that F (~v) (cid:20) G(~v; v) and F (v) = G(v; v) for all nonnegative ~v,v. From such
an auxiliary function, we can derive the update rule v0 = arg min~vG(~v; v) which never
increases (and generally decreases) the objective function F (v):
F (v0) (cid:20) G(v0; v) (cid:20) G(v; v) = F (v):

(10)

By iterating this procedure, we obtain a series of estimates that improve the objective func-
tion. For nonnegative quadratic programming, we derive an auxiliary function G( ~v; v) by
decomposing F (v) in eq. (1) into three terms and then bounding each term separately:

F (v) =

G(~v; v) =

1

2Xij
2Xi

1

A+

ij vivj (cid:0)

1

A(cid:0)

2Xij
2Xij

1

bivi;

ijvivj +Xi
ij vivj(cid:18)1 + log

A(cid:0)

(A+v)i

vi

~v2
i (cid:0)

It can be shown that F (~v) (cid:20) G(~v; v). The minimization of G(~v; v) is performed by
setting its derivative to zero, leading to the multiplicative updates in eq. (3). The updates

(11)

bi~vi:

(12)

~vi~vj

vivj(cid:19) +Xi

move each element vi in the same direction as (cid:0)@F=@vi, with ﬁxed points occurring only
if v(cid:3)
i = 0 or @F=@vi = 0. Since the overall optimization is convex, all minima of F (v) are
global minima. The updates converge to the unique global minimum if it exists.

B Proof of Theorem 2

The proof of the bound on the asymptotic rate of convergence relies on the repeated use
of equalities and inequalities that hold at the ﬁxed point (cid:11)(cid:3). For example, if (cid:11)(cid:3)
i = 0 is a
non-support vector coefﬁcient, then (@L=@(cid:11)i)j(cid:11)(cid:3) (cid:21) 0 implies (A+(cid:11)(cid:3))i(cid:0)(A(cid:0)(cid:11)(cid:3))i (cid:21) 1. As
shorthand, let z+

i = (A(cid:0)(cid:11)(cid:3))i. Then we have the following result:

i = (A+(cid:11)(cid:3))i and z(cid:0)

1
(cid:13)i

=

(cid:21)

=

2z+
i

1 +q1 + 4z+
1 +q(z+

i z(cid:0)
i
2z+
i
i (cid:0) z(cid:0)

2z+
i
i + z(cid:0)
1 + z+
i (cid:0) z(cid:0)
z+
2z+
i

i
i (cid:0) 1

:

(cid:21) 1 +

i

i z(cid:0)
i )2 + 4z+
i (cid:0) z(cid:0)
z+
i + z(cid:0)
z+

= 1 +

i (cid:0) 1
i + 1

To prove the theorem, we need to express this result in terms of kernel dot products. We
can rewrite the variables in the numerator of eq. (16) as:

yiyjK(xi; xj)(cid:11)(cid:3)

j = yiK(xi; w) = jK(xi; w)j; (17)

we can obtain a bound on the denominator of eq. (16) by:

xjyj is the normal vector to the maximum margin hyperplane. Likewise,

Aij(cid:11)(cid:3)

j = Xj

i (cid:0) z(cid:0)
z+

i = Xj
where w =Pj (cid:11)(cid:3)

j

z+

i = Xj

A+

ij(cid:11)(cid:3)

j

(13)

(14)

(15)

(16)

(18)

(19)

(20)

(21)

(22)

k

k

(cid:11)(cid:3)
j

(cid:11)(cid:3)
j

A+

(cid:20) max

(cid:20) max

ikXj
jK(xi; xk)jXj
k pK(xk; xk)Xj
(cid:20) pK(xi; xi) max
k pK(xk; xk)K(w; w):
= pK(xi; xi) max
k = Xj
k = Xj

j Xk

Ajk(cid:11)(cid:3)

Ajk(cid:11)(cid:3)

j (cid:11)(cid:3)

(cid:11)(cid:3)
j

(cid:11)(cid:3)

K(w; w) = Xjk

The last step in eq. (23) is obtained by recognizing that (cid:11)(cid:3)
j is nonzero only for the coefﬁ-
cients of support vectors, and that in this case the optimality condition (@L=@(cid:11)j)j(cid:11)(cid:3) = 0

impliesPk Ajk(cid:11)(cid:3)

k = 1. Finally, substituting eqs. (17) and (22) into eq. (16) gives:
1
(cid:13)i

jK(xi; w)j (cid:0) 1

(cid:21) 1 +

2pK(xi; xi) maxkpK(xk; xk)K(w; w)

:

This reduces in a straightforward way to the claim of the theorem.

(24)

Eq. (21) is an application of the Cauchy-Schwartz inequality for kernels, while eq. (22)
exploits the observation that:

(cid:11)(cid:3)
j :

(23)

