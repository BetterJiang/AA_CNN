SUMMARY

Gene expression arrays typically have 50 to 100 samples and 1000 to 20 000 variables (genes). There
have been many attempts to adapt statistical models for regression and classication to these data, and in
many cases these attempts have challenged the computational resources. In this article we expose a class
of techniques based on quadratic regularization of linear models, including regularized (ridge) regression,
logistic and multinomial regression, linear and mixture discriminant analysis, the Cox model and neural
networks. For all of these models, we show that dramatic computational savings are possible over naive
implementations, using standard transformations in numerical linear algebra.

Keywords: Eigengenes; Euclidean methods; Quadratic regularization; SVD.

1. INTRODUCTION

Suppose we have an expression array X consisting of n samples and p genes. In keeping with statistical
practice the dimension of X is n rows by p columns; hence its transpose XT gives the traditional biologists
view of the vertical skinny matrix where the ith column is a microarray sample xi . Expression arrays have
orders of magnitude more genes than samples, hence p (cid:2) n. We often have accompanying data that
characterize the samples, such as cancer class, biological species, survival time, or other quantitative
measurements. We will denote by yi such a description for sample i. Acommon statistical task is to build
a prediction model that uses the vector of expression values x for a sample as the input to predict the
output value y.

In this article we discuss the use of standard statistical models in this context, such as the linear
regression model, logistic regression and the Cox model, and linear discriminant analysis, to name a few.
These models cannot be used out of the box, since the standard tting algorithms all require p < n; in
fact the usual rule of thumb is that there be ve or ten times as many samples as variables. But here we
consider situations with n around 50 or 100, while p typically varies between 1000 and 20 000.

There are several ways to overcome this dilemma. These include

dramatically reducing the number of genes to bring down p; this can be done by univariate screening
of the genes, using, for example, t-tests (Tusher et al., 2001, e.g.);
use of a constrained method for tting the model, such as naive Bayes, that does not t all p parameters
freely (Tibshirani et al., 2003);

To whom correspondence should be addressed.






Biostatistics Vol. 5 No. 3 c(cid:3) Oxford University Press 2004; all rights reserved.

D
o
w
n
l
o
a
d
e
d




f
r
o
m
h
t
t
p
:
/
/
b
i
o
s
t
a
t
i
s
t
i
c
s
.
o
x
f
o
r
d
j
o
u
r
n
a
l
s
.

o
r
g
/


b
y



g
u
e
s
t




o
n
O
c
t
o
b
e
r


4

,


2
0
1
6

330


T. HASTIE AND R. TIBSHIRANI

use of a standard tting method along with regularization.

In this article we focus on the third of these approaches, and in particular quadratic regularization,
which has already been proposed a number of times in this context (Eilers et al., 2001; Ghosh, 2003;
West, 2003, for example). We show how all the computations, including cross-validation, can be simply
and dramatically reduced for a large class of quadratically regularized linear models.

2. LINEAR REGRESSION AND QUADRATIC REGULARIZATION

Consider the usual linear regression model yi = x T

 + i and its associated least-squares tting

i

criterion

n(cid:1)
i=1

min


(yi  x T

i

)2.

(2.1)

The textbook solution  = (XTX)1XTy does not work when p > n, since in this case the p  p matrix
XTX has rank at most n, and is hence singular and cannot be inverted. A more accurate description is that
the normal equations that lead to this expression, XTX = XTy, donot have a unique solution for ,
and innitely many solutions are possible. Moreover, they all lead to a perfect t; perfect on the training
data, but unlikely to be of much use for future predictions.

The ridge regression solution to this dilemma (Hoerl and Kennard, 1970) is to modify (2.1) by adding

a quadratic penalty

for some  >0. This gives

n(cid:1)
i=1

min


(yi  x T

i

)2 + T

(2.2)

 = (XTX + I)1XTy,

(2.3)
and the problem has been xed since now XTX+ I is invertible. The effect of this penalty is to constrain
the size of the coefcients by shrinking them toward zero. More subtle effects are that coefcients of
correlated variables (genes, of which there are many) are shrunk toward each other as well as toward zero.

Remarks:


In (2.2) we have ignored the intercept for notational simplicity. Typically an intercept is included,
and hence the model is f (x) = 0 + x T, but we do not penalize 0 when doing the tting. In this
particular case we can rather work with centered variables (from each of the genes subtract its mean),
which implies that the unpenalized estimate 0 is the mean of the yi .
 Often in ridge regression, the predictor variables are measured in different units. To make the penalty
meaningful, it is typically recommended that the variables be standardized rst to have unit sample
variance. In the case of expression arrays, the variables (genes) are all measured in the same units, so
this standardization is optional.
 The tuning parameter  controls the amount of shrinkage, and has to be selected by some external
means. We demonstrate the use of K -fold cross-validation for this purpose in the examples later on.
It appears that the ridge solution (2.3) is very expensive to compute, since it requires the inversion of
a p  p matrix (which takes O( p3) operations). Here we demonstrate a computationally efcient solution
to this problem.

D
o
w
n
l
o
a
d
e
d




f
r
o
m
h
t
t
p
:
/
/
b
i
o
s
t
a
t
i
s
t
i
c
s
.
o
x
f
o
r
d
j
o
u
r
n
a
l
s
.

o
r
g
/


b
y



g
u
e
s
t




o
n
O
c
t
o
b
e
r


4

,


2
0
1
6

Efcient quadratic regularization for expression arrays

331

Let

X = UDVT
= RVT

(2.4)
(2.5)
be the singular-value decomposition (Golub and Van Loan, 1983, SVD) of X; that is, V is p  n with
orthonormal columns, U is n  n orthogonal, and D a diagonal matrix with elements d1 (cid:1) d2 (cid:1) dn (cid:1) 0.
Hence R = UD is also n  n, the matrix of so-called eigengenes (Alter et al., 2000). Plugging this into
(2.3), and after some careful linear algebra, we nd that

 = V(RTR + I)1RTy.

(2.6)
Comparing with (2.3), we see that (2.6) is the ridge-regression coefcient using the much smaller n  n
regression matrix R, pre-multiplied by V. In other words, we can solve the ridge-regression problem
involving p variables, by




reducing the p variables (genes) to n (cid:4) p variables (eigengenes) via the SVD in O( pn2) operations;

solving the n dimensional ridge regression problem in O(n3) operations;

transforming the solution back to to p dimensions in O(np) operations.

Thus the computational cost is reduced from O( p3) to O( pn2) when p > n. For our example in

Section 4.4 this amounts to 0.4 seconds rather than eight days!

3. LINEAR PREDICTORS AND QUADRATIC PENALTIES

There are many other models that involve the variables through a linear predictor. Examples include
logistic and multinomial regression, linear and mixture discriminant analysis, the Cox model, linear
(cid:2)
support-vector machines, and neural networks. We discuss some of these in more detail later in the paper.
All these models produce a function f (x) that involves x via one or more linear functions. They are
n
i=1 L(yi , f (xi ))
typically used in situations where p < n, and are t by minimizing some loss function
over the data. Here L can be squared error, negative log-likelihood, negative partial log-likelihood, etc.
All suffer in a similar fashion when p (cid:2) n, and all can be xed by quadratic regularization:

n(cid:1)
i=1

min
0,

L(yi , 0 + x T

i

) + T.

(3.1)

For the case of more than one set of linear coefcients (multinomial regression, neural networks), we can
simply add more quadratic penalty terms.

We now show that the SVD trick used for ridge regression can be used in exactly the same way for
all these problems: replace the huge gene expression matrix X with p columns (variables or genes) by
the much smaller matrix R with n columns (eigengenes), and t the same model in the smaller space. All
aspects of model evaluation, including cross-validation, can be performed in this reduced space.

D
o
w
n
l
o
a
d
e
d




f
r
o
m
h
t
t
p
:
/
/
b
i
o
s
t
a
t
i
s
t
i
c
s
.
o
x
f
o
r
d
j
o
u
r
n
a
l
s
.

o
r
g
/


b
y



g
u
e
s
t




o
n
O
c
t
o
b
e
r


4

,


2
0
1
6

332

T. HASTIE AND R. TIBSHIRANI

THEOREM 1 Let X = RVT as in (2.5), and denote by ri the ith row of R, a vector of n predictor values
for the ith observation. Consider the pair of optimization problems:

3.1 Reduced space computations

( 0, ) = argmin
0,R p
(0,  ) = argmin
0,Rn

n(cid:1)
n(cid:1)
i=1
i=1

L(yi , 0 + x T

i

) + T;

L(yi , 0 + r T

i

 ) +  T.

(3.2)

(3.3)

Then 0 = 0, and  = V.
The theorem says that we can simply replace the p-vectors xi by the n-vectors ri , and perform our
penalized t as before, except with much fewer predictors. The n-vector solution  is then transformed


i

T = x T

= QTxi and  = QT. Then

back to the p-vector solution via a simple matrix multiplication.
Proof. Let V be p  ( p  n) and span the complementary subspace in R p to V. Then Q = (V : V) is
a p  p orthonormal matrix. Let x
 x
i QQT = x T
 T = TQQT = T.
Hence the criterion (3.2) is equivariant under orthogonal transformations. There is a oneone mapping
rather than . But from the denition of V in
. Hence the loss part of the criterion
T
2 ,

between the location of their minima, so we can focus on 
(2.5), x
(3.2) involves 0 and 
and write (3.2) as

1 . Wecan similarly factor the quadratic penalty into two terms 

1 consists of the rst n elements of 

T = r T

T

1

1

+ 

2

, and

i


i


i

L(yi , 0 + r T

i



1

) + 

1

T

1

(cid:4)

(cid:5)

+

(cid:6)

,



2

T

2

(3.4)

i

1 , where 

(cid:3)
n(cid:1)
i=1

which we can minimize separately. The second part is minimized at 
by noting that the rst part is identical to the criterion in (3.3) with 0 = 0 and  = 
equivariance,

= 0, and the result follows
1 . From the

2

 = Q  = (V : V)

(cid:8)

(cid:7)

0

= V

(3.5)

(cid:1)

3.2 Eigengene weighting

Although Theorem 1 appears to be only about computations, there is an interpretative aspect as well.
The columns of R = UD are the principal components or eigengenes of X (if the columns of X are
centered), and as such they have decreasing variances (proportional to the diagonal elements of D2).
Hence the quadratic penalty in (3.3) favors the larger-variance eigengenes. We formalize this in terms of
the standardized eigengenes, the columns of U.

D
o
w
n
l
o
a
d
e
d




f
r
o
m
h
t
t
p
:
/
/
b
i
o
s
t
a
t
i
s
t
i
c
s
.
o
x
f
o
r
d
j
o
u
r
n
a
l
s
.

o
r
g
/


b
y



g
u
e
s
t




o
n
O
c
t
o
b
e
r


4

,


2
0
1
6

Efcient quadratic regularization for expression arrays

COROLLARY 2 Let ui be the ith row of U. The optimization problem
) + 

L(yi , 0 + uT

( 0, ) = argmin
0,Rn

n(cid:1)
i=1

i

n(cid:1)
j=1

2
j
d2
j

333

(3.6)

is equivalent to (3.3).

This makes explicit the fact that the leading eigengenes are penalized less than the trailing ones. If  is not
too small, and some of the trailing d j are very small, one could reduce the set of eigengenes even further
to some number m < n without affecting the results much.

3.3 Cross-validation

No matter what the loss function, the models in (3.2) are dened up to the regularization parameter .
Often  is selected by k-fold cross-validation. The training data are randomly divided into k groups of
roughly equal size n/k. The model is t to k1
k of the data, k separate times, and the
results averaged. This is done for a series of values for  (typically on the log scale), and a preferred value
is chosen.

and tested on 1

k

COROLLARY 3 The entire model-selection process via cross-validation can be performed using a single
reduced data set R. Hence, when we perform cross-validation, we simply sample from the rows of R.
Proof. Cross-validation relies on predictions x T, which are equivariant under orthogonal rotations. (cid:1)

Although for each training problem of size n k1

k , an even smaller version of R could be constructed,
the computational benet in model tting would be far outweighed by the cost in constructing these k
copies Rk.

3.4 Derivatives

In many situations, such as when the loss function is based on a log-likelihood, we use the criterion itself
and its derivatives as the basis for inference. Examples are prole likelihoods, score tests based on the
rst derivatives, and (asymptotic) variances of the parameter estimates based on the information matrix
(second derivatives). We now see that we can obtain many of these p-dimensional functions from the
corresponding n-dimensional versions.

COROLLARY 4 Dene L() = (cid:2)

VT,

If L is differentiable, then

n

i=1 L(yi , 0 + x T

i

n

i=1 L(yi , 0 + r T

i

 ). Then with  =

), L( ) = (cid:2)

L() = L( ).

 L()



2L()
T

= V
= V

;

 L( )



2L( )
  T VT,

(3.7)

(3.8)

(3.9)

with the partial derivatives in the right-hand side evaluated at  = VT.

D
o
w
n
l
o
a
d
e
d




f
r
o
m
h
t
t
p
:
/
/
b
i
o
s
t
a
t
i
s
t
i
c
s
.
o
x
f
o
r
d
j
o
u
r
n
a
l
s
.

o
r
g
/


b
y



g
u
e
s
t




o
n
O
c
t
o
b
e
r


4

,


2
0
1
6

334

T. HASTIE AND R. TIBSHIRANI

Notes:

 These equations hold at all values of the parameters, not just at the solutions.
 Obvious (simple) modications apply if we include the penalty in these derivatives.
Proof. Equation (3.7) follows immediately from the identity X = RVT, and the fact that x T
i are
the ith rows of X and R. The derivatives (3.8) and (3.9) are simple applications of the chain rule to (3.7).
(cid:1)
The SVD is a standard linear algebra tool, and requires O( pn2) computations with p > n. Itamounts
to a rotation of the observed data in R p to a new coordinate system, in which the data have nonzero
coordinates on only the rst n dimensions. The Q-R decomposition (Golub and Van Loan, 1983) would
do a similar job.

i and r T

4. EXAMPLES OF REGULARIZED LINEAR MODELS

In this section we briey document and comment on a large class of linear models where quadratic
regularization can be used in a similar manner, and the same computational trick of using ri rather than xi
can be used.

4.1 Logistic regression

Logistic regression is the traditional linear model used when the response variable is binary. The class
conditional probability is represented by

The parameters are typically t by maximizing the binomial log-likelihood

Pr(y = 1|x) = e0+x T
1 + e0+x T

.

{yi log pi + (1  yi ) log(1  pi )} ,

(4.1)

(4.2)

n(cid:1)
i=1

n(cid:1)
i=1

where we have used the shorthand notation pi = Pr(y = 1|xi ).

If p > n  1, maximum-likelihood estimation fails for similar reasons as in linear regression, and

several authors have proposed maximizing instead the penalized log-likelihood:

yi log pi + (1  yi ) log(1  pi )  T

(4.3)

(Ghosh, 2003; Eilers et al., 2001; Zhu and Hastie, 2004).
Remarks:
 Sometimes for p < n, and generally always when p (cid:2) n, the two classes can be separated by an afne
boundary. Maximum likelihood estimates for logistic regression are undened (parameters march off
to innity); the regularization xes this, and provides a unique solution in either of the above cases.

In the separable case above, as   0, the sequence of solutions () (suitably normalized) converge



to the optimal separating hyperplane; i.e. the same solution as the support-vector machine (Rosset et
al., 2003); see below.
Theorem 1 tells us that we can t instead a regularized logistic regression using the vector of
eigengenes ri as observations, instead of the xi . Although Eilers et al. (2001) use a similar computational
device, they expose it only in terms of the specic ML score equations deriving from (4.3).

D
o
w
n
l
o
a
d
e
d




f
r
o
m
h
t
t
p
:
/
/
b
i
o
s
t
a
t
i
s
t
i
c
s
.
o
x
f
o
r
d
j
o
u
r
n
a
l
s
.

o
r
g
/


b
y



g
u
e
s
t




o
n
O
c
t
o
b
e
r


4

,


2
0
1
6

Efcient quadratic regularization for expression arrays

335

4.2 Generalized linear models

Linear regression by least squares tting and logistic regression are part of the class of generalized linear
models. For this class we assume the regression function E(y|x) = (x), and that (x) is related to
the inputs via the monotonic link function g: g((x)) = f (x) = 0 + x T. The log-linear model for
responses yi that are counts is another important member of this class. These would all be t by regularized
maximum likelihood if p (cid:2) n.

4.3 The Cox proportional hazards model

This model is used when the response is survival time (possibly censored). The hazard function is modeled
as (t|x) = 0(t )ex T. Here there is no intercept, since it is absorbed into the baseline hazard 0(t ). A
partial likelihood (Cox, 1972) is typically used for inference, regularized if p (cid:2) n.

This model generalizes the logistic regression model when there are K > 2 classes. It has the form

4.4 Multiple logistic regression

Pr (y = j|x) =

.

(4.4)

(cid:7) x

K

(cid:2)
e0 j+T
j x
(cid:7)=1 e0(cid:7)+T
K(cid:1)
j=1

log Pr(yi|xi )  

n(cid:1)
i=1

max
{0 j , j}K
j=1

When p > n, this model would be t by maximum penalized log-likelihood, based on the multinomial
distribution

T
j

 j .

(4.5)

There is some redundancy in the representation (4.4), since we can add a constant cm to all the class
coefcients for any variable xm, and the probabilities do not change. Typically in logistic regression, this
redundancy is overcome by arbitrarily setting the coefcients for one class to zero (typically class K ). Here
this is not necessary, because of the regularization penalty; the cm are chosen automatically to minimize
the L2 norm of the set of coefcients. Since the constant terms 0 j are not penalized, this redundancy
persists, but we still choose the minimum-norm solution. This model is discussed in more detail in Zhu
and Hastie (2004).

Even though there are multiple coefcient vectors  j , it iseasy to see that we can once again t the

multinomial model using the reduced set of eigengenes ri .

Figure 1 shows the results of tting (4.4) to a large cancer expression data set (Ramaswamy et al.,
2001). There are 144 training tumor samples and 54 test tumor samples, spanning 14 common tumor
classes that account for 80% of new cancer diagnoses in the U.S. There are 16 063 genes for each sample.
Hence p = 16 063 and n = 144, in our terminology.

The deviance plot (center panel) measures the t of the model in terms of the tted probabilities, and
is smoother than misclassication error rates. We see that a good choice of  is about 1 for these data;
larger than that and the error rates (CV and test) start to increase.

These error rates might seem fairly high (0.27 or 15 misclassied test observations at best). For these
data the null error rate is 0.89 (assign all test observations to the dominant class), which is indicative of the
difculty of multi-class classication. When this model is combined with redundant feature elimination
(Zhu and Hastie, 2004), the test error rate drops to 0.18 (nine misclassications).

The multinomial model not only learns the classication, but also provides estimates for the
probabilities for each class. These can be used to assign a strength to the classications. For example,

D
o
w
n
l
o
a
d
e
d




f
r
o
m
h
t
t
p
:
/
/
b
i
o
s
t
a
t
i
s
t
i
c
s
.
o
x
f
o
r
d
j
o
u
r
n
a
l
s
.

o
r
g
/


b
y



g
u
e
s
t




o
n
O
c
t
o
b
e
r


4

,


2
0
1
6

336

T. HASTIE AND R. TIBSHIRANI

Misclassification Rates for Multinomial Regression

Deviance for Multinomial Regression

Misclassification Rates for RDA

s
e

t

a
R
n
o



i
t

a
c
i
f
i
s
s
a
c
s
M

i

l

4

.

0

3

.

0

2

.

0

1

.

0

0

.

0

te

te

5

.

1

te

te

te

te

te

te

te

te

te

te

te

te

te

cv

cv

cv

cv

cv

cv

cv

cv

cv

cv

cv

cv

cv

cv

cv

0

.

1

te

te

te

te

te

te

te

te

te

e
c
n
a
v
e
D

i

5

.

0

cv

cv

cv

cv

cv

cv

cv

cv

cv

te

cv

te

te

cv

cv

te

te

cv

cv

te

cv

tr

tr

tr

tr

tr

tr

tr

tr

tr

tr

tr

tr

tr

tr

tr

tr

tr

0

.

0

tr

tr

tr

tr

tr

tr

tr

tr

tr

tr

tr

tr

tr

s
e

t

a
R
n
o



i
t

a
c
i
f
i
s
s
a
c
s
M

i

l

4

.

0

3

.

0

2

.

0

1

.

0

0

.

0

te

cv

te

te

cv

cv

te

te

cv

cv

te te

te te te te

te te te te te te te te te te te te te te te te te te

cv cv cv cv cv cv cv cv cv cv cv cv cv cv cv cv cv cv cv

cv

cv cv

cv

cv

tr tr tr tr tr tr tr tr tr tr tr tr tr tr tr tr tr tr tr tr tr tr tr tr tr tr tr

tr

tr

1e02

1e+00

1e+02

1e02

1e+00

1e+02

1e01

1e+01

1e+03

1e+05







Fig. 1. Misclassication rates and deviance (2 negative log-likelihood) for the 14-class cancer data (left and middle
panel). The labels indicate training data (tr), test data (te), and 8-fold cross-validation (cv). The minimum number of
test errors was 15. The right panel shows the same for RDA (Section 4.5); the minimum number of test errors for
RDA is 12.

one of the misclassied test observations had a probability estimate of 0.46 for the incorrect class, and
0.40 for the correct class; such a close call with 14 classes competing might well be assigned to the unsure
category. For six of the 15 misclassied test observations, the true class had the second highest probability
score.

4.5 Regularized linear discriminant analysis

The LDA model is based on an assumption that the input features have a multivariate Gaussian distribution
in each of the classes, with different mean vectors k, but a common covariance matrix . It isthen easy
to show that the log posterior probability for class k is given (up to a factor independent of class) by the
discriminant function

k (x) = x T1k  1
2

T
k

1k + log k ,

(4.6)

where k is the prior probability or background relative frequency of class k. Note that k (x) is linear in
x. Wethen classify to the class with the largest k (x). Inpractice, estimates

k = nk
n

,

k = 1
nk

xi ,

 = 1
n  k

(xi  k )(xi  k )T

(4.7)

are plugged into (4.6) giving the estimated discriminant functions k (x). However,  is p  p and has
rank at most n  K , and so its inverse in (4.6) is undened. Regularized discriminant analysis or RDA
(Friedman, 1989; Hastie et al., 2001) xes this by replacing  with () =  + I, which is nonsingular

if  >0.

Now (4.6) and (4.7) do not appear to be covered by (3.1) and Theorem 1. In fact, one can view
RDA estimates as an instance of penalized optimal scoring (Hastie et al., 1995, 2001), for which there
is an optimization problem of the form (3.1). However, it is simple to show directly that (4.6) and its
regularized version are invariant under a coordinate rotation, and that appropriate terms can be dropped.

(cid:1)
yi=k

K(cid:1)
k=1

(cid:1)
yi=k

D
o
w
n
l
o
a
d
e
d




f
r
o
m
h
t
t
p
:
/
/
b
i
o
s
t
a
t
i
s
t
i
c
s
.
o
x
f
o
r
d
j
o
u
r
n
a
l
s
.

o
r
g
/


b
y



g
u
e
s
t




o
n
O
c
t
o
b
e
r


4

,


2
0
1
6

Efcient quadratic regularization for expression arrays

337

Hence we can once again use the SVD construction and replace the training xi by their corresponding ri ,
and t the RDA model in the lower-dimensional space. Again the n-dimensional linear coefcients

= (  + I)1 
are mapped back to p-dimensions via k = V 
k .

In this case further simplication is possible by diagonalizing 

k



k

(4.8)

using the SVD. This allows one to
efciently compute the solutions for a series of values of  without inverting matrices each time; see Guo
et al. (2003) for more details.

RDA can also provide class probability estimates

Pr(y = k|x; ) =

(cid:2)

ek (x;)
j=1 e j (x;)

K

.

(4.9)

From (4.9) it is clear that the models used by RDA and multinomial regression (4.4) are of the same
form; they both have linear discriminant functions, but the method for estimating these differ. This issue
is taken up in Hastie et al. (2001, Chapter 4). On these data RDA slightly outperformed multinomial
regression (see Figure 1; 12 vs 15 test errors).

Regularized mixture discriminant analysis (Hastie and Tibshirani, 1996; Hastie et al., 2001) extends
RDA in a exible way, allowing several centers per class. The same computational tricks work there as
well.

4.6 Neural networks
Single layer neural networks have hidden units zm =  (0m + T
m x) that are linear functions of the inputs,
and then another linear/logistic/multilogit model that takes the zm as inputs. Here there are two layers of
linear models, and both can benet from regularization. Once again, quadratic penalties on the m allow
us to re-parametrize the rst layer in terms of the ri rather than the xi . The complicated neural-network
analysis in Khan et al. (2001) could have been dramatically simplied using this device.

4.7 Linear support vector machines

The support vector machine (SVM) (Vapnik, 1996) for two-class classication is a popular method for
classication. This model ts an optimal separating hyperplane between the data points in the two classes,
with built-in slack variables and regularization to handle the case when the data cannot be linearly
separated. The problem is usually posed as an application in convex optimization. With yi coded as
{1,+1}, itcan be shown (Wahba et al., 2000; Hastie et al., 2001) that the problem

(yi  0  x T

i

)+ + T

(4.10)

n(cid:1)
i=1

min
0,

is an equivalent formulation of this optimization problem, and is of the form (3.1). In (4.10) we have used
the hinge loss function for an SVM model, where the + denotes positive part.

Users of SVM technology will recognize that our computational device must amount to some version
of the kernel trick, which has been applied in many of the situations listed above. For linear models,
the kernel trick amounts to a different re-parametrization of the data, also from p down to n dimensions.

Since the solution to (4.10) can be shown to be of the form  = X, the vector of tted values (ignoring

the intercept) is represented as

f = XXT  = K.

(4.11)

D
o
w
n
l
o
a
d
e
d




f
r
o
m
h
t
t
p
:
/
/
b
i
o
s
t
a
t
i
s
t
i
c
s
.
o
x
f
o
r
d
j
o
u
r
n
a
l
s
.

o
r
g
/


b
y



g
u
e
s
t




o
n
O
c
t
o
b
e
r


4

,


2
0
1
6

T. HASTIE AND R. TIBSHIRANI

338
The gram matrix K = XXT represents the n  n inner-products between all pair input vectors in the data.
The new input variables are the n kernel basis functions K (x, xi ) = x Txi , i = 1, . . . ,n .
From (4.11) it is clear that the parametrization recognizes that  = XT is in the row space of X, just
a different parametrization of our  = V. However, with the parametrization (4.11), the general criterion
in (3.1) becomes

L(yi , 0 + kT

i

) + TK,

(4.12)

n(cid:1)
i=1

min
0,

where ki is the ith row of K. Hence our re-parametrization ri includes in addition an orthogonalization
which diagonalizes the penalty in (4.12), leaving the problem in the same form as the original diagonal
penalty problem.

model f (x) = 0 +(cid:2)

The kernel trick allows for more exible modeling, and is usually approached in the reverse order.
(cid:8)) generates a set of n basis functions K (x, xi ), and hence a regression
n
i=1 K (x, xi )i . A popular example of such a kernel is the radial basis function

A positive-denite kernel K (x, x

(Gaussian bump function)

(cid:8)) = e

||xx

(cid:8)||2

.

K (x, x

(4.13)

The optimization problem is exactly the same as in (4.12). What is often not appreciated is that the
roughness penalty on this space is induced by the kernel as well, as is evidenced in (4.12). See Hastie et
al. (2004) for more details.

4.8 Euclidean distance methods

A number of multivariate methods rely on the Euclidean distances between pairs of observations. K -
means clustering and nearest-neighbor classication methods are two popular examples. It is easy to see
that for such methods, we can also work with the ri rather than the original xi , since such methods are
rotationally invariant.
 With K -means clustering, we would run the entire algorithm in the reduced space of eigengenes. The
subclass means rm could then be transformed back into the original space xm = Vrm. The cluster
assignments are unchanged.
 With k-nearest-neighbor classication we would drop the query point x into the n-dimensional
subspace, r = VTx, and then classify according to the labels of the closest k ri .

The same is true for hierarchical clustering, even when the correlation distance is used.

5. DISCUSSION

(cid:2) p

There is one undesirable aspect to quadratically regularized linear models, for example, in the gene
expression applications. The solutions () involve all the genesno selection is done. An alternative
| j| (Tibshirani, 1996), which causes many coefcients to be
is to use the so-called L1 penalty 
exactly zero. In fact, an L1 penalty permits at most n nonzero coefcients (Efron et al., 2002; Zhu et al.,
2003), which can be a problem if n is small. However, our computational trick to address the rst issue
only works with a quadratic penalty. Practice has shown that quadratically regularized models can still
deliver good predictive performance. We have seen that SVMs are of this form, and they have become
quite popular as classiers. There have been several (ad hoc) approaches in the literature to select genes
