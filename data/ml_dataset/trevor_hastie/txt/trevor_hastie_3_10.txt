PRINCIPAL  CURVES  AND  SURFACES

(1) 

2We~r Hastie

Co 
00

I 

Technical  Report  No.  11

November  1984

,

&O~CCCo 803 

S

....

Laboratory  for
Computational
Statistics

ý4

"Department 

of Statistics

Stanford  University

" -

84  12  13 

007

&== 

md do  amuoMd  &i  com.nd *wd 

~ul  upinmt  deb Uabud  ImGw  O" 

ow &wq
MO  thdwk used 1w

mu Ok  0I 

PC. 

cmu ok  6. 

MAow 

-- mm  "rpm am~ Limp 

w  Ii 

w 

au  uew 

Okw 
ad&Af w  ppa.

~

-ued 

A  spf 

.pam  m 

"d&mo 

o

hr mW pop  wmwd 

b avodmu  omw, 

to *a 

-letodd

A.  T§ta 

(ad 

Subeaq 

TypE  e OfRPORT  a  PERIOD  COVIftv!

7. 

ATNOI(*)S. 

CONTRACT  OR  GRAUte  NUMOER(s)

C.  PCROIORMM1G  ONG.  REPORT  NUN SER

Trevor Hastie 

9.  PER10FORMING  ORGANIZATION  NAMEC  AND  ADOORSS 

Department~ of  Statistics  and  Computational Group
Stanford Linear Accelerator  Center
Stanford University.  Stanfold,. !:.-94305

St.  CONTR1OLLINO  OFFICE  NA*9  AND  ADOVICSS 
U.S.  Office of Naval  Research 
Department  of  the  Navy 
Arlington,  VA  22217 
14.  MOGNITORING  AGENCY  NAME  i  ADORESS(Ufo~o  *St*uetS 

Cav.eSheU OflI.) 

N00014-83-K-0472

10.  PROGRAM  ELEMENT.1PROJECT.  TASK

AREA  a  WORK  UNIT  NUMBERS

12.  REPORT  GATE

November  1984
13.  NUMBER  OF  PAGES

103

IL  SECURITY  CLASS.  (of  We  topoeoj

UNCLASSIMIE
15. OCLASSPSICATION/DoWnGRAOING

INCHOULE

le.  CmSTNISUTION  STATEMENT  (of IN*  XhpWQ

Approved  for public  release;  distribution  unlimited

I.SUPPLI[MENTARY  NOTES1
The view,  opinions,  and/or  findings contained  in this report  are those  of  the
author(s). and  should  not  be  construed as  ar, oificial 'Department of  the  Navy
pos~ition,  policy, or  decision, unless so  designated' y other documentation.
Ws KEY  WOUROS 

iw MWsw5o  eei eMMeej

.o uix  DE memp 

" 

aCb.m M 

Principal  components,  non-linear,  smooth,  errors  in  ariables,  orthogonal*
regression

S&.  £MTRACr  (CW11110  MI V*inm  M#  N  0IPWUF  Md &A4P  M  Us"SaiSe

*'Principal carves  resmooth  owe dimensional curves  that paws  throug  the m~a~l  of apj  dimensional'
data  set.  They  mhinima,  the  distance  from  the  poinsa,  and  provide a mom-linear  summary  of  the
data.  The  carves  awe  moe-parametric  wad thebT  shape  is suggested  by  the data.  Similarly,  principal
surfaces  ae  two  6messioeal  surfaces  that  pas  through  the  middle  of  the  data  The curves  &ad
surfaces are found using  sin  itativ  procedure which starts with a  W a  summary  such as  the usual.

*J 

A"  139s~ 

e 

oO.Suni 

UNC  SSIFTED

IECUTY  CL. 

F-MICATWOW Off  TWOS  PNOE  (oboe  note  ante"*

*84. 

12  13 

007

UN(:ASSIFIED

20.

)-principal  component  lin,  or plane.  Each  successive  iteration  is  a  ,mortk or local  average  of  the  p
dimensional  points, where  local is  based on  the  projections  of the  point,  onto  the curve  or surface  of
the previous  iteration.

A aumber  of linear  techniques,  such  as  factor  analysis, and errmr 

in variables  regreilon,  end
up using  the  principal components  as  thenr  estimates  (after  a suitables scaling  of the  co-ordinates).
P~rincipal  curves  and  surface  can  be  viewed  as  the  estimates  of non-hmew  generalizations  of  these
procedures.  We  present  oqae  real data examples, that  illustrate these  applications.

,.Pritncipai Curves  (or surfaces) have  a theoretical  definitiom  for distuibntions:  thei,  wre  the  Self
Con.sistent curves.  A curv 
is  self  consistent  if eachpointoanthe carvis theconditional mean of
the  points  t has  project  there.  The  zmain  theorems  proms  thaprincipal  curves  mre  critical  values  of
the  expected  squared distance  between  the points and the  curve.  Linear  priaci~al comuponents  have
this  property  as wAf 
in  fact,  we  prove  tikt Nf  a  principal  curve  is  straighat,  then  it  is  a  principal
component.  These result. generalist  the  usual duality  between, conditiosial expectation  and distance
mianmisatiosm.  We  also  examine  two  sources  a(  bias  in  the  procedures,  which  it&ve  the  satisfactory
property of  partially cancelling  each 

ioth..

We  compare  thAp*  cia  curve  and  suWace  procedures  to otter  generalisations  of  principal
the  literature; the  usual  gemera~limations  tranisformi  the spae,  whereas  we  transform

components  i 
the mmodel.  There ane  also sting tie  with  mmutidiimeasional. scaling.

UNCLASSIFIED

Principal  Curves  and  Su.1rfaces

Trevor Haetie

Department  of Statistics

Stanford  University 

_

and

Computa~tion  Group

Stanford  Linear  Accelerator  Center

Abstract

Principal curves  are or  ioth  one dimensional  curves that  pan  through  the  middle of a. p dimensional
data set.  They  minismise  the  distance  from  the points.  and  provide  a  non-linear  summary  of the
data.  The  curves  are  ion-parametric  and  their shape  is  suggested  by  the  data.  Similarly,  principal
surfaces  are  two  dimensional  surfaces  that  pan  through  the  middle  of  the  data.  The  curves  and
surfaces  are found using  an iterative procedure  which  starts with  a linear summzry such  as  the  usual
principal  component  line  or  planet.  Each  successive  iteration  is  a  smooth  or  local  average  of the  p
dimensional  points,  where  locul is based  on the  projections  of thes  points onto the curve  or surface  of
the  previous  iteration.

A  number  of  linear  techniques,  such  as factor  analysis  and errors  in  variables  regression,  end
up  using  the  principal components  astheir  estimates  (after a  suitable  scali; of the  co-or dinates).
Principal  curves  and surfaces  can be  viewed  asthe estimates  of  non-livear  generalizations  of  these
procedures.  We  present  some  real data, examples  that  illustrate  these  applications.

Principal  Curves  (or surfaces)  have  a  theoretical  definition  for distributions:  they  are the  sell
C4%uiatents  curves.  A curve  is  sel  consistent  if each  point  on  the  curve  is  th.  conditional  mean  of
the  points  that  project  there.  The  main  theorem  proves'  that  principal  curves  are  critical  values  of
the expected  squared distance  between  the points and the  curv.  Linear  principal  components  have
.this propertyas we%- in fait, we psowethat if  %principal cumvis straight',thenit is aprincipal
component.  These  results generalize  the usual duality  between  conditional  expectation  and  distance
minimization.  We  also  eaamine  two  sources  of  bias  in  the  procedures,  which  hae"  the  satisfactory
property ofpartially  cancelling  each  other.

We  compare  the  prnipal  curv  and surface  procedure 

to  oth~e  generalizations  of principal
components  in  the  literature;  the  usual  generalisations  transform  the *space, whereas  we  transform
the model.  There  are  also  strong  ties. with  multidimensional  scalin g.

Wwrk  muPwe"e  bY  the  DePeilmess  of Ene aff 
r  coesrcU  IDE-AC034eSFUOIS  sad  DE.ATO3.61.ER1U43,
sod-by  the  Met  of  Nava  Research  sadeW comxemi  ONR  N00014i41..K.4340 aad  ONR  N001443.K.0472,  &Wd by
the  US.  Amy  Reseach  O19t.  sad.  coatnc  DAAG2942-K-00O4.

L-

Contents

Contents 

inl

1  Introduc"Lion.............. 

.

.. 

.. 

.. 

..  ...

1

2 

2.1 

2.2 
2.2.1 
.2.2.2 
2.2.3 

2.3, 

2.4 

Background  and Motivation 

..

*.7

Linear  Principal  Components 
A  linear  mcdel, formulation.....................8

... 

.7

Outline  of the linear  model........................8
Estimation... 
........... 
Unite  of measurement.... 

...... 
........... 

A  non-linear  generalization  of the  linear model 

..........

...........

... 

Other  generalizations......... 

........... 

... 

.......

3  The  Principal  Curve  and  Surface  models 

.. 

3.1 
3.1.1 
3.1.2 
3.1.3 
3.1.4 
3.2- 
3.21 
3.2.2 
3.3 

3.4 

3.5 
3.3.1' 
3.5.2 
3.6 
3.6.1 
3.6.2 

4 

4.1 

The  principal  curves  of a  probability  distribution 

One dimnensional  curves..............14
Definition  of principal  curves. 
Existence  of principal  curves' 
The distance  property  of  principal  curves 

...........

.

The  principal  surfaces  of a probability  distribution 

Two dimensional  surfaces..............20
Definition  of principal  surfaces............21

... 
... 

An  algorithm  for  finding  principal  curves  and surfaces 

Fncipal  curves  and surfaces  ior data  sets..........25
Demnstratio;;x  of the procedures........... 

The circle in two-.pace 
The  half-sphere  in  three-space... 

.

...... 

........... 

........

.

Principal surfaces  and  principal  components 

.32

A Variance  decomposition............. .
The  power  method.... 

............ 

...........

Theory  for  principal  cairves  and  surfaces 

... 

The projection  index  is  measureable............3!

.....

....

10

12

13

14
14

1
17
17

20

23

26
27
31

32
35
37

Iv

4.2 

4.3 

4.4 

4.4.1 
4.4.2 

4.4.3 

4.5 

5 

5.1 

5.1.1 
5.1.2 

5.1.3 

5.2 

5.2.1 

5.2.2 
5.2.3 

5.3 

5.3.1 

5.3.2 

6 

5.1 

6.2 
6.. 

6.4 

6.6 

7 
7.1 
7.1.1 
7.1.2 

7.1.3 

.

Proximity  models.. 

.

.

.

... 

Generalized  linear  principal  components 
Multi-dimensional  scaling  ...... 

.

.

.

..... 

.

The stationarity  property  of principal curves 

Some  results  on  the subclass of smooth principal  curves 

Some  results  on  bias  ..................... 

.50

A simple  model  for investigating  bias 
From  the  circle  to the  helix 

.

.

........
........

One  more  bias demonstration 

..............

Principal  curves  of elliptical  distributions 

Algorithmic  details.. 

....... 

Estimation  of curves  and  surfaces. 

... 

One  dimensional  smoothers... 
Two dimensional  smoothers 

. ............

..

.... 

. ......

. ......

.

The local  planar surface  smoother.. 

.......

The projection  dtep 

.

..................

Projecting  by exact  enumeration 

........ 

Projections  using  the k-d  tree. 
RescAling  the A's to  arc-length 

.... 

. .

Span  selection 

...... 

... 

.............. 

Global  procedural  spans 
Mean  squared  error spans 

............ 

.

..............

Examples. 

..... 

.

.

............. 

.... 

.
. ....

.... 

...

Gold  away  pairsn 

............

The helix  in  three-space 
Geokgial  data 

... 

.

. .......... 

......... 

.

The uniform  ball.. 

. ..............

; 

One  dimensional  color  data 

...... 

.... 

.

.... 

L poprotein  data 

.

.

.

.

..

.

...........

• 

DiLcussion  and  conclusions  ..... 

...... 

Alternative  techniques. 

. ................

39

48

50
57

59

60

62

62

62

64

64

65

65

65
35

67

67

68

71

71

75

78

81

83

84

88
88
88
90

91

., 

,.-"

7.1.4 
7.1.5 

7.2 

Non-linear  factor  analysis 
Axis  interchangeable  smoothing 

.

.

.
.

.
.

Conclusions 

Bibliography 

........ 

............

Contents  v

.
.

.

.
.

.

.
.

.

.
.

.

92
92

9

96

Chapter  1

Introduction

Consider  a  data  set  consisting  of  n  observations  on  two  variables,  z  and  y.  We  can
represent  the  n pointa im a scatterplot,  as in  figure  1.1. 
It is natural  to try and  summarize
the joint behaviour  exhibited  by  the  points  in  the  scatterplot.  The form  of summary  we
chose  depends  on  the  goal  of our  analysis.  A  trivial  summary  is the  mean  vector  which
simply locates  the center of the cloud  but conveys no in  rrmation  about the joint behaviour
of the two  variables.

o.-..

0 

00*

@S

Figure  1.1  A bivriat.  data set  represente  by  a ucatterplot.

It -is  often  sensible  to  treat  one  of th~e  variables  as  a  response  variable,  and  the  other
as an  explanatory  Variable.  The  aim  of the  analysis  is then  to seek  a rule  for predicting  the
response  (or average  response)  using  the value  of the  explanatory  variable.  Standard  linear
regression  produces  a  linear  prediction  rule.  The  expectation  of  V is  modeled  as  a  linear

~~~~o  *:.,-*:..."*

2 

Chapter 1:  Jstrodtctio,

function of :  and  is estimated  by  least squares.  This procedure  is equivalent  to finding the
line  that minimizes  the sum of vertic-l squrxn  d errors,  as  depicted in figure  1.2a.

When  looking at such  a regressiov  1. un, it is natural  to think of it  as a summary- of the
data.  However,  m. constructing  this  summary  we  concerned  ourselves  only  with errors  in
the response  variable.  In  many  situations  we  don't  have  a preferred  variable  that we  wish
to label  response,  but  would  still  like  to  summarize  the joint behaviour  of  and V.  The
dashed  line  in figure  1.2a  shows  what  happens  if we  used  x  as the response.  So  simply
assignm 
g  the  role of response  to  one  of the  variables  could  lead to  a  poor summary.  An

obvious alternative  is  to summarize the data  by a straight line  that treats  the two variables

symmetrically.  The first principal  component  line in figure  1.2b does just this -it 

is found

by minimizing  the orthogonal errors.

Linear  regression  has  been  generalized  to  include  nonlinear  functions  of.:  . This  has
been  achieved  using  predefined  parametric  functions,  and  more  recently  non-parametric'
ecatterplot  smoothers such as kernel  smoothers,  (Gasser and Muller  1979),  nearest neighbor
smoothern,  (Cleveland  1979,  Friedman  and  Stustzle  1981),  and spline smoothers  (Reinsch
1967).  In general scatterplor  smoothers  produce  a smooth curve  that attempts  to minimize
the  vertical  errors  as  depicted  in  figure  1.2c.  The  non-parametric  versions  listed  above 
allow the data to dictate the  form of the non-linear  dependency.

In  this  dissertatiGn  we  consider  similar  generalizations  for  the  symmetric  situation.

Instead  of summarizing  the data with  a straight  line, we  use a smooth curve;  in finding  the
curve we  treat the two  variables  symmetrically.  Such  curves  will pass  through  the  midle 
of the data  in a smooth  way  without restricting  smooth  to mean  linear,  or for that  matter
without  implying  that  the 
saiddle  of the data  is  a straight  line.  This  situation  is depicted
in figure  1.2d.'  The  figure  saggests  that  such  curves  minimize  the orthogonal  distances  to
the points.  It turns  out  tbal  for  a  suitable definition  of  middle this is  indeed  the case.  We
name  them  Prncipal  Curvet.  U1,  however,  the  data  cloud  is  ellipsoidal  in  shape  then  one
could  well  imagine  that  a stA  aight line  passes  through  the middle oftth.  cloud.  In this  case
we .*xpect our principal  cum e  to  be  straight  as well.

The principal  component 

plays  roles  other than  that of a data summary:

In  error, in variables  gression  the explanatory  variables  are observed  with  error (as
well as the r-sponse).  T 2is can occur in practice when both vauiables are measurements
of some underlying vari  bles,  and  there  is error in the  measunmenti.  It  also  occurs  in
observational studies wi ere neither variable is fixed 4y design.  ifth* aim of the analysis

-

-

#. 

t 

*  ~  r 

* 

.

, 

.

.

*~ 

V 

*~~~* 

..- 

* 

*  ~  * 

.

.

.

~ 

4

o*. 

Chapter 1:  Introduction 

3

is prediction o.  or regression  and if the z  variable is never observed  without error,  then
the best we can do is condition on the observed  z's and perform  the standard  regression
analysis  (Madansky  1959,  Kendall  and Stuart  1961,  Lindley  1947).  If,  however,  we  do
expect  to observe  x  wiLhout  error  then  we  can  model  the expectation  of y  as  a linear
function  of the  systematic  component  of z.  After  suitably  scaling  the  variables,  this
model  is  estimated  by  the principal  component  line.

e  Often  we  want  to  replace  a  number  of highly  correlated  variables  by  a  single  vari-
able,  such  as  a  normalized  linear  combinatiou  of the  original  set.  The  first  principal
component  is the  normalized  linear  combination  with the  largest  variance.

*  In  factor  analysis  we  model  the  systematic  component  of the  data  as linear  combina-
tions  of a small  subset  of vow  unobservable  variables  called  factors.  In  many  cases
the  models  are  estimated  using  the  linear  principal  components  summary.  Variations
of this  model have  appeared  in  many  different  forms  in  the 'literature.  These  include
linear  functional  and  structural  models,  errors  in  variables  and  total  least  squares.
(Anderson  1982,  Golub and  van  Loan  1979).

In  the  same  spirit  we  propose  using  principal  curves  as  the  estimates  of  the  systematic 
components in non-linear  versions of the mnodels mentioned  above.  This broadens the scope
and  use of such  curves considerably.  This dissertation  deals with  the definition,  description 
and  estimation  of such  principal  curves,  which  are  more  generally  one  dimensional  curves
in p-space.  When we have three or more variables  we,  -n  carry  the  generalizations  further.
We  can .think of modeling the  data with a 2  or more dimensional  surface  in  p space.  Let  us
first consider only three  variables  and  a 2-surface,  and deal with  each of the  four situations
in figure  1.2in  turn.

H If one  of the  variables  is  a response  variable,  then  the  usual  linear  regression  model.
estimates  .he  conditional  expectation  of  y  given 
- (zI,22)  by  the  least  squares
plane.  7his  is a planar  response  surface  which  is  once  again  obtained  by  minimising
the squared e~r ors ilug.  These  errors  .-re the vertical  distances between  g and the point
on  the plane  vertically  above or  below  y.

•  Often a linear response  surface does  not adequately  model the conditional  expectation.
We  then turn to nonlinear  two dimensional  response surfaces which are smo.oth surfaces
that  minimize  the  vertical  errors.  They  are  estimated  by  surface  smoothers  that  are
direct  extensions  of the  scatterplot smoothers  for  curve estimation.

-

7"

-.-

-

-

-. 

-

..

* 

.'  .?:

,---.---..-'.•...- 

A 

Chazpter  1:  Int roduction

Figure  1-2a  The linear regression' line mini- 
mise t~es~r 
of  uard  erorsin te rspone 

variable. 

Figure  1.2b  The  principal  component  line
mnimies  the sum  of squared  errors  in  all  the
variables.

Figure  1-2c  The  smooth  regression  curve 
minimizes  the  smn  of  squared.  errors  in  the 
response  variable,  subject  to  smoothness  con 

Figure  1.2d  The principal c~urve  rni-imixes
the  suam of  squared  emrso 
the  variables,
subject  to smoothness  constraints.

in  Al 

Chapter' I:  Introduction  5

H If all the variables are to be treated symmetrically  the principal component  plane passes
through  the data in such  a way  that the sum of squaced  distances  from  the  points  to
the plane  as  minimized.  Thii in  turn is an  estimate  for  the systematic component  in a
2-dimensional  linear  model for  the  mean  of the three  variables.

*  Finally,  in this symmetric  situation,  it is  often  unnatural  to  assume  that the  best two
dimensional  summary  is  a plane.  Principal  surfaces  are  smooth  surfaces  that  pass
through  the  middle  of  the  data  cloud;  they  minimize  the  sum  of  squared  distances
between  the  points  and  the  surface.  They  can  also be thought of as  a an  estimate  for
thi two dimensional  syrtematic  component  for  the means of the  three variables.

These  sturface  are  easily  generalized  to  2-dimensional  su-faces  in  p  space,  although  they
are hard  to visualize  for p  >  3.

The dissertation  is organized  as follows:

*  In  chapter  2 we  discuss in  more  detail  the linear  principal components  model,  as well
as  the  linear  relationship  model  hinted  at  above.  They  are  identical  in  many  caes,
and  we  attempt to  tie them  together  in the situation'  where'this  is  pomible.  We  then
propose  the  non-linear  generalizations.

'In  Chapter  3  we  define  principal  curves  and  surfaces  in  detail.  We  motivate  an  al-
gnrithm  for estimating  sech  models,  and  demonstr-A~e  the  algorithm  using  simulated
data with  very definite  and  difficult structure.

*  Chapter 4  it theoretical  in nature,  and proves some of the claims in  the previous  chap-
term.  The  main  result  in  this  chapter  is  a theorem  which  shows, that  curves  that pass
through  the  middle  of the  data are  in  fact critical  points  of a distance  function.  The
prncipal  curve  and  ý.*&rface  procedures  are  inherently  biased.  This  chapter  concludes
with  a discussion  of the  various  forms  and  severity  of this  bias.

. Chapter 5 deals  with the algorithms  in detail 

there  is  a brief discussion  of scatterplot
smoothers,  and  we  show  how  to  deal with  the  problem of  finding  the closest  point  on
the curve.  The algorithm  is explained  by  means of simple  examples,  and  a method  for
span selection  is  given.

*  Chapter 6  contains  six  examples  of the  use  and  abilities  of  the  procedures  using  real
and simul(cid:127)ted  data.  Some of the examples  introduce  special features'of the procedures
surh  as  inference  using  the  bootstrap,  robust 'options and  outlir.r  detection.,

-

S 

Ckapter 1:  Istrodrxtion

"e Chapter  7  pro.  des  a  discussion  of related  work  in  the  literature, and  gives  details  of
Ssome of  the  more  recent  ideas.  This  in followed  by some concluding  remarks  on  the

work  covered  in this dimertation.

I

-.

.9

.99

*. 

f.

Chapter  2

.Background and  Motivation

COmelde  a dat  Matrix  X  with  va  rows  and p  columns.  The  matrix  consists  of ns  points  or
vectwor  with p coodiates.  In many  situations  the matrix will have arisen  as ni  observations
of  a vector  random  variable.

.2.1.  Linear Principal  Components.

The AMrs  (linear)  princqpa  compoosent  is'the nomu~lized  linear combination  of the p variables
With  the  largest  sample  variance  It is convenient  to  think  of X  as  a  cloud  of  n  points  in
p~spmc.  The irincipal  componet  is then  the  length  of the  projection  of  the' a  points  onto
a direction  vector.  TMe  vector  is  chosen  so  that  the  var4'mnce  of the  projected  points  along
it is largest.  Any  live parallel  to this  vector will have  the same  property.  To tie  it down  we
ins@4  that it pase  through  the muan  vector.  This lIne  then  has the  appealing  property  of
being  the  line  in  posepw  that  is closest  to  the data.  Closest  is in  terms  of  average  squared
~icwidiaa distance.  We  think  of  the  projection  ws  being  the  best  linear  one  dimensional
swimmary O  the data X.  OfW  cours  this  linear summary  might  be  totally  inadequate  locally
but it  attempts to  proviode  a reasonable  global  summary.

The  theory  and  practical  isses  involved  in  linear  principal  components  analysis  are
wellD  known  (Barnett  1961,  Guanadesikan  1977),  and  the  technique  is'originally  due  to
Spearman  (1904),  and  then  late  developed  by  Hotelling  (1933).  We  can  find  the  the
componsnt,  orthagonal  to the  first,  that  has  the  next  highest  variance.'  The  plane
spanned  by  the tee  vectors  and  including  the  Wean  vector  is  the plano  closest  to the  data.
In &emssal va  can  flm4  the  m,< p dimensional  hyperplane  that contains  the  most  variance,
samd  is 

loesest  to the data.

The  solution  to the  problem  in obtained  by  computing  the  singular  value decomposi-
tics or  basic  structure at X,  (centered  with  respet  to  the sample  mean  vector),  or  equiva-
lently  the  eigen  decomposition  of the  sample  covariance  mnatrix  (Golub  and  Reinsch  1970,
Greenacre  1964).  Without  any lows  in  generality we assume  from  now on that X  is centered.
if  this  is  Wot  the cles,  we  can  center  X, perform  the, analysis,  and  uncenter  the  result.  by

*asecond 
* 

0 

Section Lti:  A  linear model famultation

adding  back  the  mean  vector.

In  particular,  the fir-*  principal  component  direction vector  a is the largest normalized
eigenvector of S,  the  varaple  covariance  matrix.  The  principal  component  itself is  Xo,  an
,is theone dimensional
a vector wihelemntsA,  = z'4where 
sumimary  variable  for  t.he  ith  observation.  The  coordinates in  p-space of the  projection  of
the  ith observation  on  a ame  given  by

it aw of Xand A 

iis the 

(2.1)

Ilaer  is  so underlying  model  in  the  above.  We  merely  regard  the  first  component
io a good  summsary  a(  the  original  variables  if it  accounts for  a large  fraction  of the  total
variance.

2.2.  A  linear  model  formulation.

* 

In this section. we  duscrib.  a linear  model  formulation  for the p variables.  This formulation
inchudes  many- familia  models  such  as  linear  regression  and  factor  analysis.  We  end  up
showing  in  2.2.2  that  the  estimation  of  the systematic  component  of some  of these  models
is Once  again  the  principal  component  procedure.

2.2.1.  Outline  of  the  linear  model.

Consider  a model  for  the observed  data

Xi 

ad+ 

ei(2.2)

where  vi  is  an- unobservable  systematic  component  and  ej  an  unobservable  random  comn-
ponent  (We  only  got  to. a"  their  sum).  We  usually  impose  some  linear  structure  on  uj,
naaedy

* 

ej 

so +AAj 

(2.3)

*where 

u0  is constant  location  !eOctor,  A  is  a  p  x  mn  matrix  and. Aj  is  an  rn-vector.  For  the
procedures  considered  so it always  estimated  by  the L~imple  mean vector 3; without  loss  of
generality  we will  simply  aume that X has be~en  centered  and ignore the term  uo.  We  also

N NX  Is  otcenesred  we  center  it  by  forming  tIr-  X-  is.  Thouash.  pr'ci6pal  component  is,
and the estimate  in p space  for the  projection at. the ith observation  ont* the  principal

W 
I 
-
cempaent 

ae  I+ 

is  a + ea~ 

+ se'(2d  -

)

Ckapter  :  Back  raved and Motivation 

9

I

amume  that  ei  are  mutually  independent  and  identically  distributed  random  vectors  with
mean  0  and  covariance  matri  9  and  are  independent  of the  Aq.

Nf  the  A.  are considered  to be  random  as well,  the  model  is  referred  to  as  the  linear
structural  model,  or more commonly  as the  factor  analysis  model.  If the  k  are  fixed  it  is
refere  as the linear  functional  model.  The  model  (2.3)  includes  some  familiar  models  as
speaial  m

LtA be p x  (p -1)  with rank  (p  -i).  We can write A as

1)since we canpost-multiply Aby
whwe aus a(p-1.) vector&WdI is (p -1)  x(p- 
an arbitrary  non-singular  (p - 1) x  (p - 1)  matrix. and  premultiply  A. by  its  inverse.
Thus we  can write  the model  (2.3)  as

where(.  ) = oand assume .co(,) =diag(,e2,.a,). If;  =as'=...  =2 =  0
then we have the usual linesr regreion model with response  zli  and regressor variables

N if the variances  are not sero we  have the'errors  in variables reqessioa  model.  The  idea
is to Aind  a (p -i)  dimensional  hyperp~ane  in p.-space  that  approximates  the data well.
The model  takes cwe  of errors  in  all the  variables,  whereas  the  usual  linear  regression
model considers errors only in  the response variable.  This is a form of linear functional
analysis.

*  When  tA 

are  random  we have  the usual  factor  analysis  model,  which  includes the

random  effects  Anova.  Thid 

is  alms 

referred  to  as the  linear structural  model.

*  If all  the variances  we ero and the  Ad  ae random  and A is p x p the model  represents
the  principal  component  change  a(  basis.'  In  this situation  it  is clear  that  the  Ai  are
each  functions  of the  si.

For a full  treatment oa the  above  models  see  Anderson  (1962).

%. 

-

.,: 

10  Setio.  5.1:  A  Enter model formulation

2.2.2.  Estimation

We  return for simplicity  to the  cas  where  m 

1. Thus

Si 

+ 

Ci(2.5)

The  systematc  components  &A, ae  points  in  p-space  confined  to  the  line  defined  by  a
multiple  A. of the vectors(cid:127)  We need  to  estimate  A, for each  observation,  and the direction

vector.

We now  state bwne  results  which  can  be  found  in Anderson  (1982).

fe either

*  the ej  are jointly  Normal  with  a  scalar  covariance  cl,  whet  c  is  possibly  unknown,

and  if (cid:127)  are  random or  fixed,  and  we estimate  by  maximum  likelihood

or

Sas  above but we  drop  the Normal  assumption  and  estimate by least squares,

than the estimate of A. isce 
component  direction  vector.  In  both cases  the  quantity  we wish  to minimize  is

again the first principal component and that of& the principal

RSS(A,,)  -

12i - "11- 

(2.6)

It  is easy  to  see  that  for any  a  the a 
point 

i  auto s.  Thu  equation  (2.6)  reduces  to

ppropriate'value 

for A 

is obtained  by  projecting  the

RsS( 

12  - a &41, 

,) = 
- trXX4 - s'X"OX

(2.7)

The  normalized solution  to  (2.7)  is the  largest  esgeavector  of X'X.

If the  errot  coverianc# 

is  general  but  known,  we  can  transform  the  problem  to  the
previous  can.  This is  the same  as  using  the Mahalanobis  distance  defined  in terms  of  t.
In  particular  when  9  is  diagonal the  procedure  amounts to  finding the line  that minimizes

the weighted  distance  to  the  points  and  is depicted  in  figure  (2.1)  below.'

If the error cvariance  is unknown and not scalar then we  require riplicate observations

in order to estimat  it.

;. 

.%,_*..........., 

.

.

.

.

.

.

.

.

.

.. 

.. 

•.' 

...

Chapter 2:  Background and Motivation  11

1d3

Figure  2.1 
distamne  r7.(d~u/vu  + 4/o22) &aom  the points  to the line.

Vf  9  =  diag(Oj,v22)  then  we  minimise  the  weighted

2.2.3.  Units  of mneasurement.

It  is  often  a problem  in  multivariate  data  analyei  that  variables  have  different  error  vari-
ances,  even  though  they  are  measured  in  the  same  units.  A  worm  situation  is  that  often
the variables  are measured  in  completely  different  and  incommensurable  units.  When  we
use least  squares  to estimatie a lower dimesoa  summary, we  explicitly  combine the errs
on  each  variable  using  the  usual  sum  of cw.,ponenta  lons  ftunction,  as  in  (2.6) . This  then
gives equal weight, to each of the components.  The solution  is  thus not invariant  to changes
in  the scale of any,  of  the  variables.  This is easily, demonstrated  by  considering  a spherical
If we  scale up  one  of  the  co-ordinates  an  arbitrary  amount,  we  can  create
point  cloud. 
as  znuch  linear  structure 
so  we  like.  In  this  situation  we  would  really  like  to  weigh  the
w~rot 
in  the  estimation  of  our model  according  to  the variance of  the  measurement  errors,
which  is seldom, known.  The  saest procedure  in  this  situation  is to  standardize  each  of the
coordinates  to have  unit  variance.  This  could  destroy  some of the  structure  that exists but
without  further  knowledge  about  the  scale of  the components  this  yields  a  procedure  that
is invariant  to coordinate  scale  transformatious.

If, on  the other  hand,  it  in known  that  the  variables  are  measured  in  the same  units,
we  should  not  do  any  scaling  as  all.-  An  apparent  counter-example  occurs  if  we  make

12 

Section  1.9:  A 

,s-linter generelixstim os tAke  linesr  ntoda

measuremnts  of  the  same  quantities  in  different  situations,  with  different  measurement

devices.  An  example  might  be  taking  seismic  readings  at  different  sights  at  the  same

instances  with  different  recording  devices. 

If the  error  variances  of the  two  devices  are

different,  we would  want to  scale  the components  differently.

To sum up so far,  the principal  component  summary, besides being  a convenient  data

reduction  technique,  provides  us  with  the  estimate  of a formal  parametric  linear  model
which  cover  a  wide  variety  of situations.  An  original  example  of  the  one  factor  model
given  here is  that of Spearnmn  (1904).  The zx  are scores  on psychological  tests and  the X,
i  some  underlying  unobservable  general  intelligence  factor.

The  estimation  in  all  the  eae  amounts  to  finding  a  m-dimensional  hyperplane  in

p4pace that  is closet  to the points  in  some metric.

2.3.  A  non-linear  generalization  of the linear  model.

The  above  formulation is often  very  restrictive  in that it  assumes that the systematic  com-
ponent  in  (2.2)  is  linear,  a 
in  (2.3).  It  is  true  in  Some  cases  that  we  can  approximate  a
nonlinear surface  by its first order linear component.  In other cases we do not have sufficient
data  to ,stimate  any  more  than  a  linear  component.  Apart  from  thee  cases,  it  is  more
reeso(cid:1)able 

to amume a model  of the form

f  ( 

+ 

(2.8) 

where Aj  is a m-vector as  before  and f  is a p-vector of functions, each with m arguments.  The

functions  are required  to be smooth  relative  to  the errors.  This is a  natural  generalization
of the linear  model.

This  dissertation  deals  with  a generalization  of  the  linear  principal  components.  In- 
stead of finding  lines  and planes  that come  close  to  the  data,, we find  curves and surfaces.
Just 
the linear  principal components  are estimates  for the variety of linear  models  listed
above,  so will  our non-  linear  versions  be  estimates  for  models  of the  form  (2.8)  . So  in
addition  to having  a more general  summary  of multidimensional  data, we  provide  a means
of estimating  the systematic  component  in  a  large  class  of models  suitably  generalized  to
include  no-linearitiss.  We  refer  to  thesm  summaries  as  principal  curves and  surfaces.

So far  the discussion  has concentrated  on data  sets.  We can just as well  formulate  the

above models for p dimensional  probability distributions.  We would then regard the data set

-

-

' 

%  *.***- 

* 

** 

, 

" 

., 

.

.'  , 

.

, 

.

-. .

.' 
" 

..-
' 

, 

" ... 

...

** 
' 

* 

':-

Ckapter R. Background and Motivastion  13

Ws a Sample  from this distribution  and the functions  derived for the data set will be regarded
as estimates  of  the corresponding  functions defined  for  the distribution.  These  models then
define  one  and  two  dimensional  surfaces  that  summarize  the  p  dimensional'distribution.
The  point  1 (A) or. the  surface  that corresponds  to  a  general  point  x  from the  distribution
is  a p dimensional  random  variable  that  can  be summarized  by  a  two  dimensional  random
variable A.

2.4.  Other  generalizations.

There  have  b,--a  a number  of  generalizations  of the  principal  component  model  suggested
iv. tIhe  literature.

*  "Generalized principal componente  usually refers  to the adaptation of the linear model
in which  the coordinates  are  first  transformed,  and  then  the standard  principal  com-
poiwaet  analysis  is carried  out on  the  transformed  coordinates.

*  M~altamensional  scaling  (MDS)  finds  a low  dimensional  representation  for  the. high
dimensional  point  cloud,  such  that  the  sum  of squared  interpoint  distances  are  pre-
served.  Thij constraint  has been  modified  in certain case  to cater only for  points  that
are  4affe  2n  the original  space.

*Prcxinnlty  ana!ysis  ovn'des  parametric  representations  for  data  without  noise.

*Non-linea-r  facter  analysis  is  a  generalization  similar  to  ours,  except  parametric  co-
ordfinate  functions  arm  used.

We  have  been  deliberately  brief in  listing  these  alternatives.  Chapter  7  contains  a detailed
discussion  and comparison  of each  of the above  with  the  principal  curve and surface  models.

Chapter  3

The  Principal  Curve  and  Surface  models

In  his  chapter  we  define  the principal  curve  and  surface  models,  first for  a p dimensional
probability  distribution,  and  then  for  a  p dimensional  finite  data set.  In order  to achieve
wmo continuity in the presentation, we motivate and  then simply state results and theorems
in this chapter,  and prove  them in chapter  4.

3.1.  The  principal  curves  of a  probability  distribution.

We  first give a brief introduction  to one dimensional  surfaces  or curves, and then define  the
principu  esrwe  of smooth  probability distributions  in  p space.

3.L-1.  One  d~imensional  curves.

A one dimensional  curve  f  is a vector  of functions  of a single  variable, which  we denote by 
A.  Thea 
functions  are called  the  coordinate  functions,  and  A provides  an ordering  along 
the curve.  If the  coordinate  functions are smooth,  then  f  will  be  a smooth curve.  We  can
clearly  make any monotone tr-sformaton  to A, say m(A),  and by  modifying the coordinate
functions  appropriately  the  curve  remains  unchanged.  The  parametrization,  however,  is
different.  There  is  &anatural parametrization  for  curves  in  terms  of the  arc-length.  The
arc-length of a curve f  from Ao  to A, is given  by

1 = J  jI'(s)JI  dz.

If IjI'(z)I  aE  1 then I  =  A- 1-A 0 . This is  a rather desirable situation, since it.all the coordinate
variables  ae in  theism.  units  of measurement,  then  A is  also in  those  units.  The  vector 
f(A) is  tangent to the curve at A and is sometimes  &alled the  vel.oiti-vector  at A. A curve
with 11JP11  a  1 is cAed  a unit  speed parametrized  curve.  We can  always  reparametrize  any
unooth  curve to make  it unit  speed.  If  v  is  a unit  vector,  then  I(A)-vo +  Aw  is a  unit
speed  straight curve.

The  vector  fJ(A)  is  called  the  acceleration  of  the  curve  at  A, and  for  a unit  speed

curve,  it is  easy  to check  that  it  is orthogonal  to  the  tangent  vector.  In  this cane  is/lI uI

-
.

""- "'

-.-- ,;,.- 

-

;- 

.-.

'.,,. 

-' 

.

.....-

". 

..

, 

,..%;.... 

'. 

.p.. 

,.t 

'- 

'.-. 

.. 

/ 

.-. 

, 

.. 

• 

.

.

'

Chapter S.  The Principal Curve  and Surface models  15 

i

i. 
I

Figure  (3.1)  The  radius  of curvature  is-the  radius  of the  circle
tangent  to the cumv  with the  same  acceleration  an the  curve.

= 

:/::-:;:I

im called  the  principal normal of the  curve  at  A.  Since  the  acceleration  measures  the  rate
and  direction  in  which  the  tangent  vector  turns, it  is  not surprising  that  the  curvature  of
a parametrized  curve  im defined  in  terms of it.  The  easiest  way  to  think  of curvature  is in
terms of a circle.  We -fit a  circle  tangent  to the curve  at a particular  point  and  lying in  the
plane spanned  by  the velocity  vector  and the principal  normal.  The  circle  is constructed  to
have  the  same  acceleration  as  the  curve,  and  the  radius  of curvature  of the  curve  at  thit
point  is  defined  am  the  radium  of the  circle.  It  is  easy  to  check  that for  a unit  speed  curve
we  get

rf (A)  4-fradius  of curvature  of f  at  A

Theenter of curvature of the curve  at A is  denoted by  ef (A'  and  is  the center  of this circle.

3.1.2.  Definition  of principal  curves.

i*  deie ., 

a5~~. uni ~speed5~  curveS 
We now define what we  mean  by  a curve  that  passoel  through  the  middle of the data -. what
point 
we  call  a  principal curve.  Figure  3.2' represens  such  a curve.  At  any  particular. location

to* ..hec. 

is  eas 

' 

ir  le 

of 

* 
the 

s 

th 

raiu 

It 

tha 

for 

*S.S 

"*(cid:127) 

"

on  the  curve,  we  collect  all  the  points  in  p  space  that  have  that  location  as  their  cloaest
point  on  the  curve.  Loosely. speaking,  we  collect  all  the  points  that  project  there.  Then
the  location  on  the  curve  is  the  average  of these  points.  Any  curve  that  has  this  property

16 

Section  8.1:  The  principal  curves  of  a probability  distribution

0

0  0:-i

0 

0 s 

p0 

0  eteo 

co 

in

Fiue  32) 

oc  pon  ona  rncpl  uveiste  vsa 

f

pont 

00 

tatpojctthre•

0 

0

e  ingire  (3.2)  Each  point on 
points that  p~roject  there.""

n 
tv-age 

principal curve  is the 

of th..

in  called  a principal  curve.  One  might say  that principal  curves  ure  their  own  conditional 
expectation.  We will  prove  later these  curves  are  critical points of a"'distance  function,  as 
wre  the  principal components. 

..-.
.. '-

..

In  the figure  we  have  actually  shown the  points  that project  into  a neighborhood  on
the  curve.  We do  this because  usually  for  finite  data  sets  at most  one  data point  projects
at  any particular  spot, on the  curve.  Notice that  the points  lie  ina segment with  center  at
the center  of curvature  of the  arc  in question.  We  will  discuss  this phenomenon  in  more
detail  in the  section  on  bias in chapter  4.

We  can  formalize  the  above  definition.  Suppose  X  is  a random  vector  in  p"space,
with continuous  probability  density h(x).  Let  g be  the class of differentiable  1-dimensional
curves  in RP,  parametrized  by A, In addition we do not allow curves  that form closed loops,
so  they'may  not intersect themselves  or be  tangent  to themselves.  Suppose  A E Af  for each
f  in  9.  For  f  e g  and  z-E JRP, we  define:the  projection  index'A 1 :  ]R  '-R  Af  by

()= 
()mx{A: 

-

f(A)II 

inIf  1z-  A(S)II}. 

(3.1)

.......... 

n 

.

.

.. 

.

., 

', 

" 

(cid:127) 

(cid:127) 

.'', 

.

... 

(cid:127) 

.

.. 

.

.. 

(cid:127). 

.. 

.

,;

*

Chapter S:  The  Principal  Curve  and  Surface models 

17

The  projection index A,(z)  ofx is  the value of A for which  f(A) is closest to  z.  There might
be  a number  of such  points  (suppose  I  is  a  circle  and  z  is  at  the  center),  so  we  pick  the
largest  such  value of A.  We  will  show  in  chapter  4  that A1 (z)  is  a measureable  mapping
from  BR 

to R"  and  thus  A1 (X) is  a random  variable.

Definition
The Principal Curves  of h are those  members of 9 which  are self conaistent.  A curve f  C  9
is self consistent  if

IC(XIJAf(X)  A)  f (A) VA  Af

We call  the clas  of principal  curves  F(h).

3.1.3.  Existence  of principal  curves.

An immediate  question  might  be whether  such  curves  exist or  not,  and  for  what  kinds of-

distributions.  It is easy  to check  that for ellipsoidal distributions,  the principal components

are in fact principal  curves.  For a spherically  symmetric  distribution,  any  line  through  the

mean vector is  a principal curve.

What about data generated  from a model as in equation 2.8, where A  is 1 dimensional?
Is  f  a  principal  curve  for  this  distribution?  The  answer  in  general  is  no.  Before  we  even
try to answer  it, we  have to  enquire  about  the distribution  of ),  and  ei.  Suppose  that the

data is  weil  behaved  in  that  the  distribution  of ei  has  tight  enough  support,  so  that  no 
points can  fall  beyond  the  centers  of curvature  of f.  This  guarantees  that  each  point  has

a  unique  closest  point  to  the  curve.  We  show  in  the  next  chapter  that  even  under  these

ideal  conditions  (spherically  symmetric  errors,  slowly  changing  curvature)  the  average  of

points  that  project  at  a particular  point  on  the curve  from  which  they  are  generated  lies

outside the circle  of curvature  at  that  point  on  the  curve.  This  means  that  the  principal

curve will  be different  from  the  generating turve.  So in  this situation  an unbiased  estimate

of the principal curve will  be a biased  estimate  of the functional  model.  This bias, hLt 
er,
is small  and decreases  to zero  as  the variance  of the  errors gets small relative  to the radius

of curvature.

3.1.4.  The  distance  property  of principal  curves.

The principalcomponent 

ware  critical points of the squared distance from. the- points to their

projections on  straight curves  (lines).  Is  there  any  analogous  property  for  principal curves?

.:

-- a-- 

.~ 

-9-. 

.

-" 

.. 

C 

t,. 

J.. 

S 

* 

*'~ 

a  a 

a. 

-

.

18 

Section  3.1:  The principal curves of a probability distribution

It turns out  that  there  is.  Let d(x, 1)  denote  the  usual  euclidian  distance  from a point  z  to
its  projection on  ýhe  curve  f:

dc=, f)  1= x- fC 

lC ())l(.2

and define  the function  D2  : 

-

JR1  by

D2(f) d=,  Ed2 (X,f).

We  show  that  if we  restrict  the  curves to  be straight  lines,  then  the principal  components

are the only  critical values of D2(f).  Critical value  here  is in the  variational ".nse: 
g  are straight  lines and we form 

if f  and
then  we  d&fine f  to be-a critical  value of D2  iff

f~  + 
feg, 

dD2(f.dcJ4 =o =  0.

This means  that they  an  minima,  maxima  or saddle  points of this distance function.  If we
restrict  I  and  g  to  be members  of the  subset  of 9 of curves  defined  on a compact  A,  then
principal  curves have  this property  as well.  In  this  case f, describes  a class  of curves  about
f  that  shrink  in  as  e gets  small.  The corresponding  result  is:  dD 2 (f)/Idi.,o =  0 iff f  is
a principal  curve  of h.  This  is  a  key  property  and  is  an  essential  link  to  all  the  previous

models  and motivation  in chapter  2.  This properLy  is  similar to  that enjoyed by conditional

expectations  or projections;  the residual distance  is minimized.  Figure  (3.3)  illustrates the
idea,  and in fact  is  almost  a proof in oue  direction.

Suppose  k is not a principal  curve.  Then' dia curve defined  by  f (A)  - E(X  AAk(XI  -
A) certainly  gets closer  to  the points  in  any  of the  neighborhoods  than  the  original  curve.

This is the property  of conditional  expectatioM.  Now  the points in any neighborhood  defined
b-y  AXimightend  up in  different  neighborhoods  when  projected onto  I,  but this  reduces  the
distances even further.  This shows that k cannot be  a critical value of the distance  function.

An immediate  consequence  of thes 

two results is  that if a principal  curve  is a straight
i!  ;-  a principal component.  Another  result is  that principal  components  are self

line,  P.2ev 

consistent  if we  replace  conditional  expectations  -by linear  project-ns..

3.1.4.1  A  sMooth  subset  of principal  curves.

We  have  defined  principal  curves  in  ;  rather  general  fashion  without  'any smoothness  re-
strictions.  The  distance  theorem  tells  us that  if we  have  a, principal. curve,  we  will  not find

any curves  nearby  with  the same  expected  distance.  We  have  a mental  image  of what  we

Chapter 3:  The Principal  Curve and  Surface  models  19

0

00

0

• 

"0 

0 

f(X

Figure 3.3  The conditional expectation  curve  gets  at least  as close
to the points  as  the original C-Mve.

woulk 

like  the curves  to look  like.  They  should  pas  through  the  data smoothly  enough  so
that each  data point  has  an  unambiguous  closest  point  on the curve.  This  smoothness will
be dictated  by the  density  h.  It  turns out  that we  can  neatly  summarize  this requirement.

Consider  the  subset  1.(h)  C  T(h)  of  principal *curves of  h,  where  f  E  7.(h)  iff  f  E 6 

(h)

and  A,(z)  is  continuous in  s  for all  points  x  in  the support  of h.  In  words th.is  says  that if
two  points  x andp  are  close together,  then  their points of projection  on the  curve  are  close'
together.  This  has a number of implications,  some  of which  are  obvious,  which  we will  list
now  and prove  later.

*  There is  only one  closest  point  on the  principal  curve  for, each  x  in  the  support  of h.

*  The curve  is  globally  well  behaved.  This  means  that  the  curve cannot  bend  back  and

come  too  close  to  itself since  that will  lead  to  ambiguities  in  projection.  (If we  want

to  deal with  ckoeed  curves,  such  as  a circle,, a  technic6-  modification  in  the  definition
of Aisrequired).

"  There are  nopoints  at or  beyond  the  centers  of curvature  of the curve.  This says  that.
the  curve  is  smooth  relative  to  the  variance  of  the  data  about, the  curve.  This  has

intuitive  appeal.  If the data  is very  noisy,  we cannot  hope  to recover  more  than  a very
smooth  curve  (nearly  a straight  fine)  from it.

0.. ........ ... ....... 

k...... .. 

.. 

.. 

... e 

.

e 

.. r.  ..... 

e..

% 

... 

20  Section $.I.  The  prnacip.l surfaces of a pro~baity~ distribution,

II 

II

_  ___ 

__ 

(a) 

1

I 

__ 

__

4b)

Plgtne  3.4  The continmity conmrains  avoids global ainbiguaies a)
,W  local  ambiguiti, 

(b)  in  projection.

Figure  3.4  ilustraws  te  way  in  which  the  continuity  coastraint  avoid* gl)ba  and  local
(h)  epend  on  the denuity  h of X.  We  say  in  the support  of
ambiguities.  Notice  that 

4,  but  if the erros  halve  an  infinite  rang.,  thi  definition  would  only  allow straight  lines.
We can  make some  technca  modifications  to overcome  thi, hurdle, such  as  insisting  that h

has compact  support.  This rules  out  any  theoretical  consideration  of curves, with  gausian

errors, although  in  practice  we  always  have compact  support.  Neverthele;s,  the clam  1(h)

will  prove  to be useful  in understanding  somei  pt the properties of principal  curves.

3.2.  The  principal  surfaces  of a  probability  distribution.

3.2.1.  Two  dimensional  surface..

The  level  of difficulty  increaes,  dramatically  as  we  move  from one  dimensional  surfaces  or
curves to higher dimensional surfaces.  In  this work  we will only deal with 2-dimensional  our-
aces in p space.  In fact we  "iall deal only  with  2-surfaces that admit a  global  parametriza-

tion.  rhis  allows us to define  I  to be  a  smooth  2-dimensional  g!obally  parametrizal  surface

; 

.

.

.. 

.. _ _  _ 

-

.. 

......

'. 

o. 

,.

.

..

Chapter S:  The  Princip  Curve and Surface model.  21

if 

:f 

- RP  for A C_  2R 

is  a vector of smooth  functions:

fi(A)

I(A)

/ 

= 

(A(cid:127),(cid:127)  A2)

(3.3)

Another way  of defining  a 2-eurface  in p space  is to have p-  2 constraints  on the p coordi-
lzJJ = 1}.
nates.  An  example is the  unit  sphere  in  W'.  It can  be defined  as  {(  : z  E 
There  is one constraint.  We  will call this the implcit definition.

, 

Not all 2-surface.  have implicit definitions (mfbius band),  &ad similarly nat all surfaces
have  global  pasmetatonma.  However,  locally  an equivalence  can  be established  (Thorpe
1978).

The concept  of arc-length  generalizes  to surface  area.  However,  we cannot  always re-
paramstrise  the surface  so that units  of area  in the  parameter space correspond  to units of
area in  the surface.  Once  again,  local parametrizations  do permit  this change  of units.

Curvature  also  takes on  another  dimension.  The curvature  of a surface  at any  point
might  be different  depending  on  which  direction  we  look  from.  The  way  this  is  resolved
is  to  look  from  all  pomible  directions,  and  the  first princi(cid:127)-l  curveture is  the  curvature
corresponding  to  the direction  in  which  the  curvatuie  is-greatest.  The  second  prineiral
Curvature  corresponds  to the  largest  curvature  in  a d rection  orthogonal  to  the  first.  For
.2-surfaces  there  are  only  two orthogcnal  directions,  so  we  are  done.

3.2.2.  Definition  of principal  surfaces.

Once  again let X be a random  vector  in p-space,  with  continuous  probability  density  h(z).
Let 9S  be  the class o  differentiable  2-dimensional  surf  ces  in  RP,  parametrized  by' A E  Af,
a  -dimesional  parameter  vecto,.

For I E  (cid:127) 

and  s 4E  BR,  we  define  the projection  index  A,(x)  by

A,(s) =  mama(A:  I1:  - 1(A)II 

AS 

A&

inftix - 1(,)ll). 

(3.4)

-I 

i%- 

.

.

.

..

22 

Sectio 

.

Tke  pricipu  s urfaces of a  probbiity distributio.

The  projectica  index  defines  the  closest  point  on  the surface;  if there  is more  than  one,  it
picks  the  one with  the largest  first  component.  If this  is stAl  not unique,  it then maximizes
over  the second  component.  Once  a 
and  A1(X) is  a random  vector.
Definition

Aj(z)  is a measureable  mapping  from  EP  into I 

R,

The  Principal Surfaces  of h wre  those  members  of  g2  which  ame  self consistent:

1(X  IAg(X)  = A) = 

(A)

Figu 

, (3.5)  demonstrates  the  situation.

410

t 

Ea  pai  on a  principal  surace  is the  averag,  of the

.5 

Fplgre 
Pains" that projet there.

The plane spanned  by the first and second principal components minimizes  the distance
from the points  to their  projections  onto any  plane.  Once  again  let d(z, f)  denote  the usual
euclidian distance  from a point 2  to its projection on the surface  f,  and  D 2 (f) 
-Ed 2 (X,  1).
IU the surfaces  ae restricted  to be  planes,  then  the planes spanned  by any pair of principal

S•  , 

... 

.. 

.

.

_ 

".....*." 

_ 

.

.

.

.. 

o 

• 

o.- 

-

... 

o, 

. -. 

.

(cid:127). 

-. 

a 

.- 

*. 

.'"..*"

Chapter S:  The  Principal Curve  and  Surface  models  23

components  are  the  only  critical  values  of  D 2(f).  There  is  a result  analogous  to  the  one
to  be  proven  for  principal  curves.  If we  restrict  f  to  be  the  members  of  g2  defined  on 
connected  compact  sets in  R 2 , then  the principal  surfaces  of h are  the  only  critical  values
of D2(f).

subset of this clan.  Form the subset  1,(h) q  72(h),  where f  E 1,2(h) if  f 
sooth 

Let 7'(h) c  GI denote the clan  of principal 2-surfaces of h.  Once  again  we  consider  a
.72(h) 
s 
and  A1(z)  is  continuous  in  x  for  all  points  z  in  the support  of h.  Surfaces  in  Y,2(h)  have
the following  properties.

*  There is only  one  closest point  on the  principal  surface for  each  z  in the support  of h. 

0  The surface  is  globally  well  behaved,  in  that  it  cannot  fold  back  upon  itself causing

ambiguities  in projection.

*  We saw that for principal curves  in  Y.(h),  there are no  points at or beyond the  centers 
of curvature  of the  curve.  The  analogous  statement  for  principal  surfaces  in  1,2(h)  is

that there are no points at or beyond  the centers of normal curvature  of any  unit speed
curm in the surac.

3.3.  -An algorithm  for finding principal  curves  and surfaces.

We are AM in the theoretical  situation of finding principal curves or surfaces for a probability
distribution.  We  will  refer  to  curves  (1-dimensional  surfaces)  and  2-dimensional  surfaces
jointly  es surfaces  in  situations where  the distinction  is not important.

When  eking principal  surfaces  or critical  values  of D2(f),  it  is  natural  to look  for  a
smooth curve that corresponds  to a local minimum.  Our strategy  is to start  with  a smooth
curve  and  then  to look  around  it for ,  local  minimum.  Recall  that

D'(f) =  Zi  x- i((Xj 
D[Jx 

lf1.fY 

I 

A ,( 

X) 

Z 

-f(X)(n  1  l (cid:127)f(x). 

We can  write this as  minimization  problem  in  f  andA:  find  f  and  A such  that 

Df(f,A)  =  ] CJX -(A)II' 

(3.5)
(nil
(3.) 

(3.7)

is a minimum.  Clearly,  given  any  candidate solution  I  and  A, I  and  A,  is at least  as good.
Two  key  ideas  emerge from this:

0

0

S

S

".-.

9

, 

,* 

• 

*% 

* 

* 

..

*. 

, 

* 

.

-

... 

.

.

.

.

.

.

..-

•°

24 

Section 3.3:  An  algorithm for findinag  prnacipal curves  and  asrfacc&

"* If we  knoew  f 

s  a function of A, then we  could  minimize  (3.7)  by picking A= Af(z)

at each  point  x in  the  support  of A.

"* Suppose,  on  the ollier hand,  that we  had  a  function  A(x).  We  could  rewrite  (3.7)  as:

y' ,A)  =  A(Z  F 

] E[(X,  - f,((X))2  I A(X)I 

.(3.8)

We  could  minimi(cid:127)e  D2  by  choosing  each  fj  separately  so  as to minimize  the  corre-
sponding  term  in the sum in  (3.8).  This  amounts  to choosing

h (A) =  1(X9  I(X) = A). 

(3.9)

In this  last  step we have  to check  that the  new  f  is differentiable.  One can construct many

situations  where  this is not  the case  by  allowing  the  starting curve-to  be globally  wild.  On
the other hand,  if the  starting  curve  is well  behaved,  the  sets of projection  at a  particular
point 'in  the curve  or  surface  lie  in  the normal  hyperplanes  which  vary smoothly.  Since  the

density  h  is smooth  we  can  expect  that  the  conditional  expectation  in  (3.9)  will  define  a
smooth function.  We  give  mor  details  in  the next chapter.  The  above preamble  motivates

the following  iterative  algorithm.

Principal  surface algorithm

Inltializatlon:  Set  f( 0 )(A)  =  AA  where  A  is either  a column  vector  (principal 
curves)  and  is  the  direction  vector  of the  first  linear  principal
component  of h or A  is a p x 2 matrix  (principal surfaces)  con-
Sifting  of the  first  two  principal  component  direction  vectors.
Set  AM1o =  Ap(e).

repeat:  over  iteration  counter j

1) Set  o)(.)  =  z (X  IAu-)(x)  =( 1

2)  Choose  AM  = 

ij)

3)  Evaluate  D3 U)  - D!y(f) 

AW)).

until:  D2  (J) fails  to decrease.

Although  we  start  with  the  linear  principal  component  solution,  any  reasonable  starting
values can  be  used.

6

Chapter 3:  The Priucipal Curve and Surface models  25

and  AUC-,).  Now  Df(I,"),A(cid:127)

It is easy  to check  that the  criterion  D 2 (A) must  converge.  It  is  positive  and  bounded
below  by  0.  Suppose  we have  ,f(-1) 
1 )) <_ D?(cid:127)(  U-1),,AUC-)) 
by the  properties o  conditional  expectation.  Also  D!(I(cid:127),. AW)  5  D€(fUi), 
(.- 1)) since the
Au)  (cid:127)re  chosen  that way.  Thus  each  step  of the  iteration  is  a decrease,  and  the  criterion
converges.  This does not moat  that the procedure has converged,  since it in conceivable  that
the algorithm  oscillates  between  two or  more  curves  that are  the same  expected  distance 
from  the points.  We have  not found  an  example  of this phenomenon.

€-

The definition  of principal  surfaces  is suggestive  of the  above  algorithm.  We  want  a
moth  surface  that  is  self  consistent.  So  we  start  with  the  plane  (line).  We  then  check
if it  is  indeed  self consistent  by  evaluating  .the conditional  expectation.  If not we  have  a
surface  as  a  by-product.  We  then  check  if this  is  self consistent,  and  so  on.  Once the  self
consistency  condition  is  met,  we  have  a principal  surface.  By  the  theorem  quoted above,
this surface  is a critical  point of the  distance  function.

3.4.  Principal  curves  and  surfaces  for data sets.

So far we  have  considered  the  principal  curves  and surfaces  for a continuous  rmltivariate
probability  distribution.  In  reality, we  usually  have a finite multivariate  data set.  How  do
we define  the principal curves and surfaces for them?  Suppose then that X  is a n x p matrix
of  a  observations  on p  variables.  We  regard  the data  set ans  a sample  from  an  underlying'
probability  distribution,  and  use  it  to  estimate  the  principal  curves  and  surfaces  of  that
distribuf  m-n. We  briefly  describe  the ideas here  and  leave  the details  for  chapters  5  and  6.

*  The  first  step  in  the  algorithm  uses  linear  principal  components  as  starting  values.
We  use  the sample  principal  components  and their  corresponding  direction  vectors  as
initial  estimates of A,  and 

°(0).

SGiven functions  ?U-)  we can rind for each  zi in the sample avalue 
=,'-(cid:127)1 U_.  (). 
This  can  be  done  in  a number  of ways,  using  numerical  optimization  tochniques.  In
practice  we have  JU-1)  evaluated  at n values  of A, in  fact at 
)(1-t)  is evaluated  at other points by  interpolation.  To illustrate  the  idea  let  us con-
eider  a  curve  for  which we  have  *U-i)  evaluated  at 
n.  For each

i  =  1, 

j-)for 

"., 

-, 

V".

point  i  in the  sample  we  can  project  x  onto the line  joining each  pair (?U-)((cid:127)-I),

.

0

-

-( 

l 

Suppose  the distance  to the projection  is 4-h,  and  if the point  projects
is  the  distance  to the closest  endpoint.  Correspond.

beyond  either endpoint,  then  di 
inlg  to  each 

isa  Value 

.1  6  [ 

,^(cid:127)+j 

1"  We  then  let 

be  the  Ai,  that

......

,  .. 

.... 

.

... ,.. 

... 

..  .

..  . .

.. .

... *. ... ... . *. ......  :2  .

::

.

. . .
.
.

.

...  -.-.  .

-. 

........ 

26 

Section  3.4: Principal curves  and  earface.  for data set#

corresponds  to  the  smallest  value  of d,1h.  This  is  an O(n 2).procedure,  and as  such  is
rather  naive.  We  use  it  as  an  illustration  and  will  describe  more  efficient  algorithms

0  We  have to  ematimate  f U).(A) 

1 )  -

,A(cid:127)J 

Z (X  JAU-  1)  -A).  We  restrict ourselves  to estimating
this quantity  at only  n values of AU- 1), namely  V  -1)  ...
which we have already
estimated.  We  require  E(X  IJ-
This says  that  we  have to  gather all
A- 
the observations  that  project  onto  )U-a)  at 
and  find  their mean..  Typically
we  have  only  one such  observation,  namely  zi.  It  is at  this stage  that  we  introduce
the  .cat(cid:127)t(cid:127)rplt  m*oother,  the  fundamental  building  block  in  the  principal  curve  and
surface  procedures  for  finite  data  sets.  We  estimate  the  conditional  expectation  at
is  close  to

Sby  averaging  all  the  observations  zi  in  the  sample  for  which 
As  long  as  these  observations  are  close  enough  and  the  underlying  density  is
smooth,  the  bias  introduced  will  be  small.  On  the ,other hand,  the  variance  of  the

1j-t). 

estimate  decreases  as we include more observations  in the neighborhood-.  Figure  (3.6)
s this local  averaging.  Once  again  we  have just given  the ideas here.  avd

demonst 
wilgo into details  in later chapters.

* 

.-

P.gure 
Z(X  A'J -

3.8 

-isClose to

We  estimate 

the  conditional  expectatio

Al)-) 

by  averaging  the  observations  z  for which 

•.

Chapter S:  The  Principal  Curve  and  Surface models  2T 

*  One property  of  catterplot  smoothers  in  general  is that  they  produce  smooth curves
and surfaces  as output.  The  larger the neighborhood  used for averaging,  the smoother
the output.  Since  we  are  trying  to  estimate  differentiable  curves  and  surfaces,  it  is
convenient  that  our  algorithm,  in  seeking  a  conditional  expectation  estimate,  does
produce  smooth  estimates.  We  will have  to worry  about how  smooth  these  estimates
should  be,  or  rather  how  big  to  make  the  neighborhoods.  This  becomes  a variance
versus bias tradeoff,  a familiar  issue in  non-parametric  regression.

e  Finally, we estimate  D2  (U) in the obvious way, by  adding up the distances of each point

in the sample  from the  current  curve  o: surface.

3.5.  Demonstrations  of the procedures.

We  look  at two  examples,  one  for  curves  and  one  for  surfaces.  They  both  are  generated
from an  underlying  true model  so  that  we  can  easily  check  tnat  the  procedures  are  doing
the correct  thing.

3.5.1.  The  circle  In  two-space.

The series of plota in figure  3.7 show  100 data points generated  from a circle in 2 dimlensions
with independent  Gaussian errors  in both coordinates.  In  fact,  the generating  functions are

X2 , 
(:)Gcs~. 
wher~ianifriny 

CNA  +  C2)o.

)+el)(3.10)

where  A isuniformly  distributed  on  [0, 2x]  and el  and e2  are  independent  NI(0, 1).

The solid curve in  each  picture is the estimated curve  for  the iteration  as  labelled,  and

the  dashed  curve-is  the true function.  The  starting curve  is the  first  principal  component,
in figure.  3.7b.  Figure  3.,a gives  the  usual scatterplot  smooth  of z2  against 'zl,  which  is
clearly  an inappropriate  summary  for this  constructed  data set. 

The curve  in  figure  3.Tk  does  substantially  better  than  the  previous  iterations.  The
the span  of the smoother is reduced.  This means that
figure caption  gives  us a clue why  -
the  ass  of the neighborhood  used  for  local  averaging  is smaller.  We  will  see  in  the  next
chapter  how  the bias  in  the curves  depends  on  this span.

The  square root  of the average  squared  orthogonal  diatance  is displayed  at  each  iter-
ation.  If the  true  curve  was  linear  the  expected  orthogonal  distance  for  any  point  would
=  1.  We  willsee  in  chapter  4  that  for  this  situation,  the  true  circle  does  not

be o 

I

" "

,

*. 

†.,* 

...

,.....,,..†† 

.

.

.

.

.

.. 

., 

: ::-

28 

Section 5.5:- Demonstrations of the  prc-cedurca

... 

.- 

-

.

.. 

.-.... 

.. 

... 

.

* 

~ 

*6.~N

Figur 

3.* 

Th 

ahdcmi*h 

sa 

iue 

37 

h 

ahdcmih

04S. 

/ 

*t. 

f

.0 

A

4 

6

Figure~~~~~~~~~~ 

3 

3-cD?')3 

iue37d0)2)30

Chapter 5:  The  Principal Curve and Surface, model#  29

8* 

0 

3

Figure  M.e  DO(3())=2.64 

Figure  3.7f  D(Y(4)) 

2:'37

41.0

Fiur 

3.7g  D(A..2 

iue 

.hD 

M

30 

Section S.5.  Demonsjtrationse of the  procedures

Figure  3.71  D(?(")) 

Figure  3.7j  D(j(8))  -1.60

P- 

41 

a

bea

U 

s 

6 

e

Figure  3.7k  D(?(')) 
automatically  reduced  -A this stage.

0.97.  The span  is 

Figure  3.71  D(J(10)) 

C.96

Chapter S:  The  Principal Curve  and Surface models  31

minimize  the distance,  but  rather  a circle  with slightly  larger  radius.  Then the  minjiizing

distance  is  approximately  a2(1  - 1/4,p2) 

-

.99.  Our  final  distance  is  even  lower.  We  still

have  to adjust  for  the  overfit  factor or number  of parameters  used  up  in the  fitting  przce-

dure.  This deflation  factor  is  of the  order. n/(n - q)  where  q  is  the number  of parameters.
In  linear  principal  components  we  know  q.  In  chapter  6  we  suggest  sme  rule  of thumb

approximations  for q in this non-parametric  setting.

This  example  presents  the  principal  curve  procedure  with  a  particularly  tough  job.
The  starting  value  is  wholly  inap-nop-iate  and  the pr':jection  of  the  points  onto  this line

does not nearly  represc-n,  the final  ordering of the  points projected  onto the solution curve.
At each  iteration  the coordinate  system for  the  1U() 
is  transferred  from  the previous  curve

to the current  curve.  Points  initially  project  in  a certain  order  on  the  starting  vector,  as
depicted  in figure  3.8a.  The  new  curve  is  a  function  of  !( 0)  measured  along  this  vector
as  in  figure  3.8b  obtained  by  averaging  the  coordinates  of points  local  in  10().  The  new,
(cid:127)(i)  values  are  found  by  projecting  the  points  onto  the  new  curve.  It can  be seen that  the

ordering  of the  projected  points  along  the new  curve  can  be  very  different  to  the  ordering

along  the previous curve.  This enables  the successive  curves  to  bend to  shapes  that could
not be parametrized  in the original  principal  component  coordinate  system.

3.5.2.  The  half-spbere  in  three-space.

Figure  3.9  shows  150  points  generated  from  the  surface  of the  half-sphere  in  3-D.  The

simulated  model  in polar co-ordinates  is

5s3iD(Al) Cos(A 2) 

I 
- (5cos(Ai)cos(A:) 

Z2 

W 

5sin(A2) 

(ei
e 
C+ 

CS)

(3.11) 

for  At  6  [0, 21]  and  A2  E  [0, r/2).  The  vector  e  of  errors  is  nimulated  from  a  M'(0, I)
distribution,  and  the  values  of  A,  and  A2  are  chosen  so  that  the  points  are  distributed
uniformly  in  the  surface.  Figure  3.9a  shows  the  data  and  the  generating  surface.  The
expected  distance  of the points  from the  generating  half-sphere  is to first order  1, which  is 
the  expected  squared  longth  of the  residual  when. projecting  a spherical  standard  gaussian
3-vector  onto a plane through  the origin.  Ideally we would  disrlay this example on a motion
graphics  workstation  in order  to see the  3 dimensions.*

*  This  disertation  is, accompanied  by  a  motion  graphics  movie, called  Principal  Curve  and

Surfa'es. The half-sphere  is one of 4 examples  demonstrated  in  the movie.

4

4

Si

32 

Section  3.6:  P-incipal &urfaceS and principal components

I7.

(*) 

(b)

FIgu~re  3.8  The  curve  of the  the  first  iteration  is  a function  of  1(o)-
measured  along  the  starting  vector  (a).  The  curve  of  the  the  secon'd
iteration  is  a function  of  ~()measured  along  the  curve  of  the  first
iteration  (b).

3.6.  Principal  surfaces  and  principal  components.

In  this  section  we  draw  some  comparisons  between  the  principal  curve  and  surface  models

and  their  linear  counterparts  in  addition  to those  already  mentioned.

3.6.1.  A  Variance  decomposition.'

Usually  linear  principal  components  are  approached  via  variance  considerations.  The  first
-component is that linear combination  of the  variables with  the largest  variiws~.  Th. second
component  is  uncorrelated  with thefirst  and  has largest variance subject  to this  constraint.
Another  way  of saying  this  is  that  the  total  variance  in  the plane  spanned  by  ths  first  two
components  is larger  than  that in  any  other  plane.  By  total  variance  we  mean  the sum  of
the  variiances  of the  data  projected  onto  any  orthonormal  basis  of the subspace  defined  by
the  plane.  The  following  treatment  is  for  one  comporent,  but  the ideas  easily  generalize  to
two.

Ckapter 3:  The  Principal  Curve  and  Surface  model. 

33

* .

..

Figure  3.9a.  The  geeratiag  narface  and 
ti, datA.  D(S)  -1.0 

*. 

., 

•*

FIgure  3.9b. 
plane.  D~OM)) 

The  principal  co'mponent

1.59

*  o*

Figreta9. D($ 
, 

s) ) 

1.00pFlgue 

3. 

D((o)) ) 

0.78

3

I

i

34 

Section 3A:  Principal  urfaces sand  principal  componeats

If A =  (A,,  A,,)'  is  the  first  principal  component  of X,  a n x p data matrix,  and
a  is the  corresponding  diroction  vector,  then  the  following  variance decomposition  is eAsily
derived:

E  Var(zi) =  Var (A) + EIllz- 
3=1

eA12 

(3.12)

where  Va" (-) and E(.) refer to sample variance and expectation.  If the principal  component
was defined  in  the parent  opulbAtion  then the  result is still true and  Var(-)  and E(-)  have
their  utual  meaning.  The  second  term  on  the  right  of  (3.12)  is  the expected  squared
distance of a point to its  projection  onto  the principal  direction.*

The total  variance in  the original  p variables  is decomposed into twn components:  the
variance  explained  by  the  linear  projection  and  the  residual  varian-e in the  distances from
the points to  their projctions.  We would  like  to have  a similar decomposition  for principal
curves  and  surfaces.

Let  w now be any  random variable.  Standard  results on conditional expectation  show

thatL

Va~i 

Z (z 

IC z 

))2  + r  Va(Z(zi I w)). 

(3.1)

jul 

Jul 

-

If  =  Af (z)  and f  is a  principal  cur-e  so that  Z(zi I A(z)) = fi (A1(x)),  we  have

~jVar (zi)  =  Z 

k,. 

-

!(IAj(X))[+ 

11  .%+ 

Vaf(Az)) 

= 

a1  h(% ()

(3.14)

This  gives  us an  analogous  result  to  (3.12)  in  the  distributional  case.  That is,  the  total
is decomposed  into  the variance explained  by  the true  curve
varince  in  the  p coordinate 
and  the  residual  variance  in the  expected  squared  distance  from a point to its true position
on  the  curve.  The sample  version  of  (3.14)  holds only  approximately:

SVarkzj) N r 

Jul 

itl 

x Iz-  loo)II +E Var (/,(4.)) 

jut

(315

The reason for  this is that most  practical scatterplot  smoother.  an not projections, where"
conditional  expectations  ane.

We  r.ake the  following  observations:

•  We  kep  ia  naind  that X  is considriKd  to  b.  centwed,  or altomativiy  that  E (a)  - 0.  The

above  results  are still  true  if this  is  eo the cae,  but  the  equations  we  memier.

- , 

, 

,

Chapter 3:  The  Principal  Curve and Surface model#  35

if fi(A) 

a=A,  the  linear principal  component  function,  then

P 

Z Var(fi(Aflx)))=  Z  a  V.ar(A(cid:127),())

p

j=i 

j=i

SVar 

(A)

since a  has length  1.  Here  we  have  written  A for  the  function  A)(z)  =  d'.

*  if the fj  ae approximately  linear 

u p" 
.,'(cid:127)  the  Delta method  to obtain

,.,Var (fj Of,(x)))  ow  rj(4  E(of(z))) 2  Va  (A (z)).

=Var(A,(W))

sice we reoticit  ou  curves  to be unit speed  and  thus  we  have  have Ilrll  = 1.

3.6.2.  The  power  method.

W.  already  mentio(cid:127)md 
liner  prbici 

that when  the data  is ellipeoidal  the principal curve procedure  yields
We  now  show  that  if  our  smoother  fits  straight  lines,  then
ow=  again  the  principal curve  procedure  yields  linear  principal components  irrespe.tive  of
the starting lnm

ompet. 

Theorem  3.1

If the  smoother  in  the principal  curve  procedure  produces  least squares  straight  line  fits,
and if the initial  fiuntios describe  a straight line,  then  the procedure  converges  to  the first

principal  component.

Proof

*Let e(o)  be  any  starting  vector  which  has  unit length  and  is not  orthogonal  to the  largest
principal  component  of X,  and  assume  X  is centered.  We  find  A0  by  projecting  =,  onto
*(°)  which  we  denote  collectively  by

where  A(O)  ii(cid:127)a 
projecting  the  vector  2.  =  (,.,,j)' 

vector  with  elements  A, 

i= 
onto AM:

,... ,n.Wefind  a,'  by  re.ressing or

",."  .. 

... ,'...-'.'o;(cid:127)'-..(cid:127)'.... 

..-

'%".:. 

.-..... 

.. 

.

.".. 

.

TA7.0T• 
A.7 0 
•. 
* 

.. 

.' 

-

.

," 

.

.

.

.

.

.-

,.,•

GO\,O)z

• 

\..

S • 

.

.

.o 

• 
.

.

.• 
.. 

.o 

o 

.. 

.

, 

o 

.

o  .

.

, 

.

o 

.

• 

.

o 

= 

.

.

.

L 

.

.

0 

, 

.

.

.

,  0 

.0 

" 

.

.6 Wo 

.

0 

.

0,

Section S.6:  PriftipaL  surfaces and principal  components

38 

or

A(0 )'A(0

X'Xa(O)
-Oa X(cid:127)'Xa(cid:127).)

(1)  s renormalized.  It can  now  be seen  that  iteration  of this  procedure  is  equivalent

and 
to' finding  the  largest eigenrector  of  X'X  by  the  power  method  (Wilkinson  1965). 

*

M°

Chapter  4

Theory  for  principal  curves  and  surfaces

In  this  chapter  we  prove  the  results  referred  to  in  chapter  3.  LI  most  cases  we  deal  only

with  the principal  curve  model,  and  suggsst  the analogues  for the principal  surface  model.

4.1.  The  projection  index  is measureable.

Since  the first  thing  we  do  is condition  on A, (X),  it  might  be prudent  to  check  that  it  is
I'- RI1
indeed  a  random  variable.  To  this end  we need  to show  that the  function  Af:  RP 
i  measureable.

Lot  f(A)  be a unit ipeed  parameterized  continuous  curve  in p-space,  defined  for  A Er

[Ao, Al  =  A. Let

where

D()=  inf {d(:,f(A)))  V x YeR

A4EA

d(Z',f(A)) 

1=2  - f (A) ,"

the usual euclidean  dibtance  between  two  vectors.  Now  set

M(z)  =  {A;d(z, I(A))  =  D(z))  .

Since  A  is compact,  M(z)  is not empty.  Since f,  and  hence  d(zx 

(A))  is continuous,  Me(z)

is open,  and hence  M(z)  is  closed.  Finally, for  each  z in  EP we  define.the  projection  index:

Aj(X) =sup M(z)

A,(x)  is attained  because  M(z)  is  closed,  and  we  have  avoided  ambiguities.

Theorem  4.1
A (z)  is a measureable  function  of x.

I am  grateful  to H.  Kinsch  o( ETH,  Zrich, for  getting me  started  on, this proof.

S..... 

2  .

38 

Section 4(cid:127)1:  TIe  pv4ection iadex it measiureable

Proof

In  order  to prove  that  Af,(z)  is  measureable  we need  to show  that for  any  c E  A, the  set
(x IA,(s) < c} is a measureable  set.

Now  x  e  ({  I A(z)  _  4)  -=(cid:127) 

for  any  A E  (c,Ail  there  exists  a  A'  - [Ao,cI  such
that  d(x, 1(A))  > d(z, f(A')).  (i.e.  if there was  equality  then by  our convention  we  choose
A(z) =  A >  e.)  In symbols we  have

{x IA(:)<wc)  =  n  U  {zId(x,l(A))>d(z,(A'))}

dd

The first step  in the proof is to show  that

B.  e  n  U -{: ,id(z,f(A)) > d(s,f(Ad))}

where  Q is the  set of rational numbers.  Since  for  each  A 

U  (x  Id(z,,(A))  > d(,,f(A')))  D  U 
A(cid:127)E[A.,I(cid:127) 
A'E[A.,.J 

(z  1d(x,1(A))  > d(xf(A(cid:127)))},

-

:.

it  follows  that B. 
A C (e,AAi  3 A# E  [Ao, c]  such  that

A .4. We  need  to show  that B.  2 A4.  Suppose z E  A.  i.e.  for any  given

d(z, f(A))  > d(x, f(A')).

For  any  given  such  A and  A' we can  find an  e >  0 such  that

d(-, f(A))  = d(z, 

(A'))  + e

Now  since  f  is continuous  and  the  rationals  are  dense  in RI  we  can. find  a At  e  Q  such
that  All  <  A'. and  d(f (A#),  f(A))  <  e.  (  A' E  Q we  need  go no further).  This  implies  that
d(x, f(A))  >  d(z, f(All))  by  the  pythagorean  property  of euclidean  distance.  This  in  turn
implies that  x e B.  and  thus  A,, 

' C,,and therefore  A.  = B,.

: .

,. 

.

.. . . . . .

.

. .

..

....  .... 

.. 

.

.

. ....

" .

... 

.. 

'... 

". 

".". 

..
? 

;2 

: 

.". 
" 

.
:::: 

.

..

"  :":  :  ? 

."". 
:'- 

.
.
:'"_ 

.

"_': 

?  "

Chapter 4:  Theory for principal curves  and aurfaces  39

The second  step is to show  that

, 

f 

"-B-

U 

{xId(zf (A,)) > d(z,f(A))}

Now  clearly  B.  5  D,.  Suppose  then  that  x e  D.. i.e.  for every  Ag  E  (c,,Afl  n Q,  there
is a A;, E  [AO,  c ln Q  such that  d(z, f(A,))  >  4d(z,  I(f')). Once  again  by  continuity  of f  and
becaue the rationsl-  are dene in  R'  we can  find another  Ae Q,  At  >  ,1 such  that

d(z, f(A))  > d(zf(A;)).

for all A4E[AV,  A;:.Thsmastt

u 

{z1id(z,f(A))  >  d(,f(A;))}

SNE n 
dot

for every  A(cid:127) 

(c, All  n Q.  In other  words

=E 

n  Et;

B..  Finally,  each  of the  sets  in  D,  is  a halfspace,  and  thus
and  we  have  that  D. 
measureable,  D, is a countable  union and  intersection  of measurable  sets,  and is  thus itself
measurable.

4.2.  The  stationarity  property  of principal  curves.

We  first  prove  a  result  for  straight  lines.  This  wi 
lead  into  the  result  for  curves.  The
straight line theorem says  that  a principal component  line is a critical  point of the expected
distance from  the points  to  itself.  The converse  is also  true.

We  first  establish  some  more  notation.  Suppose  f(A)  A 

a  unit  speed  con-
tinuously  eifferentiable  parametrized  curve  in  ]RP,  where  A is an  interval  in  IR.  Let g(A)
be  defined  similarly,  without  the  unit  speed  restriction.  An  e  perturbed  version  of f  is
1f,  =  f) 

+ etg(A).  Suppose X has a  continuous density  in  RP which  we denote by  h, and

9 

i-. 

.

,, 

.

... 

7

40 

Section 4.2:  The  .tatioeaity property of principal curve*

let  D2(h, f,)  be defined  as  before  by

D2(h,tE  =c 

I li-  IE(AI.(X))jjI

where  Af,,(X)  parametrizes  the  point on  f,  closest  to  X.

Definition
The  curve  I  is  a  critical point of the dietance  function  in  the clams  9  iff

dD'(hIE) 

0  VgEg.

(We  have  to  show that  this derivative. exists.)

Theorem  4.2

Let  f (A) =  ]X +  Ago  with  1lvo01  =1,  and  suppose  we  restrict  g(A)  to be  linear  as  well.,
So((cid:127)A)=  Y,  11  =  I  and, 9 
is  a
the  clas  of  all  unit  speed  straight lines.  Then'" 
critical  point  of the  distamce  function  in  f  iff  vo  is an  eigenvector  of (cid:127)  =  COV(X).-

Note,

SWLOG  we  assume  that  EX-O.

* 11.1  iI  is simply for convenience.

Proof

The closest  point  from  x to  any  line  Aw 'through  the  origin  in  found by  projecting  z  onto
w and has  parameter  value

Ilk 

---

Upon  taking expected  values we  get

D2 (h,Aw)  =  tr E*- 

(4.1)

We  now  apply  the  above  to f, instead  of fa,  but  first  make  a simplifying  assumption.  We
can MRSUme  w.l.o.g that .,  -

el  since the  problem  is  invariant  to  rotations.

~ 

.

/ 

.

O'o  O

CUapter 4:  Theory for principal curves  and  surface. 

41

We split 9 into  a component..  =  eel  along el  and  an orthogonal component  Y*.  Thus
to

, =  A((1 + cc)ze  + e&').  We  now  plug  this  into  (4.1) 

=  c- ,  +  9" where  J4* 

=  0.  So 

""

-
::..:

get

(cid:127)'(h/,1=(cid:127)  r.((1+ ,ce(cid:127)e  + e,')'E((1 + ce)ej + e,')..•-""':"'..-

D(f. 

rE-(1 

+  CC)2  +  C2- 

= 

E- (1 + cc)2eEei + 2e(I + ce)te,"v  + e2 Ev, 

(1  +  cc)2 +  62-

(4.2)-

Differentiating  wr.t.  e  and setting e =0 we  get

dD3(h, 

.

=)

If  I  is  a; principal  component  of E  then  this  term  is  azro  for  all  9* and  hence  for  all  v.
Alternatively,  if this term, and  hence  the derivative, is zero for  all v and hence  all Y'e"l  =0,
we  have

0  V veel  =0 

='Eel 
eel 

=)-Eel 

*.ej  is an eigenvector  of E

Note:

Suppose  ,  is in fact  another eigenvector  of E, with  eigenvalue  d, then

2 (h~,f.)  -~ D2(hf)  -

SD

-,0-)

This  shows  that f  might  be  aSmaximum,  a  mi nimum  or  a saddle  point.

Theorem  4.3

Let 9  be the class  of unit  speed  differentiable  curves  defined  on  A,  a closed  interval, of 'the
form  [a, b].  The  curve  f  is a  principal  curve  of h  ifff 
is a  critical  point  of  the  distance
function in  the class  9.

'We  make some observations  before we prove theorem 4.3.  Figure  4.1illustrates  the situation.
The curve  f.  wiggles  about/  and approaches 
as  c approaches  0.  In  fact,  we can  see  that
the curvature  of f,  is close  to that of, f  for  small  e.  The  curvature  of 

is  given  by

1/rIIG) 

2  

N1(A)II

-

:.""". 

.

.".'- 
'-." 

.

o".'." 
"o 

.. 

.- ".-°..".  ".  ".  "'-". 

'• 

• 

2

.

.

.

.

.

.

.

.

.
.

.

.

.

.

.
.

.

.
.

.

.

.

.

.
.

.

.

.

.

.
.

.
.

.

.

.

.

S,, 

, 

.:'(cid:127) 
,  % (cid:127) 

. , 

.

• 

,,.7,%.

.

.

.

.

.

.

.

:. ..

... 

p  *

42  Section  4.2:  TAe  stationarity property of principal curves

F7gure  (4.1) 

1MA)  depicted  as  a function of 1(A). 

-

where  N(A)  is the normal  vector to the curve at A. Thus 1/rf()  <  IlX(A)II/ 
lII(P)ll  since
the curve  is not  unit speed  and  so  the  acceleration  vector  is  slightly  off normal.  Therefore
Whave  rf.1(.)  -> ilf(A)  +  g'C(A)11 2 / Il"(A) +  eell which  converges  to r,'(A)as  c ---1' 0.

The  theorem  is  stated  only. for  curves  f  defined  on  compact  sets.  This  is not  such  a
restriction  an it  might seem at first  glance.  The  notorious  space  filling curves are excluded,
but they  are of little interest  anyway.  If the  density h has infinite support, we  have to box it
in  RF in order that f,  defined on a compact set, can satisfy  either statement of the theorem.
(We  show  this later.)  In practice  this  is not  a restriction.

Proof of theorem  4.3.

We  use  the  dominated  convergence  theorem  (Chung,  1974  pp  42)  to  show  that  we  can

interchange  the orders of integration  and  differentiation  in  the expression

2D(h,I 4 )=  E EAIX- 

(,(X))ll. 

(-3)

We need  to find  a random  variable  Y  which  is  integrable  and  dominates  almost  surely  the
absolute  value of

lix - I. of, (n 

i 

-

-

-

(n
f ('X 

2i 

2

Chapter 4:  Theory for principal curves  and surfaces  43

for all  e>  0.  Notice that by  definition

d

4-0Z  =  -t- 4A(X))I  LO

if this  limit exists.  Now

<  X -

(A (X))" l. 

- f of (X,

Expanding  the first norm  we  get

IIx - O,'. )1,, 

((cid:127)))11 2 = Ix - A,())I  2 Ji+- 

ig(A,(X))11-2E  (X - (A.,(X))) 

g(Af (X)),

, 

and  thus

Z, 5 -2 (X - i,\,.(x))) .,g(A,(X))  + eI,(Ao(x))l1'

where  Y, is some  bounded  random  variable.

Similarly we  have

lix  f ý- f.(A(X))l 

-1 liX -

fI(Af  (X)) jJ2

We expand  the first  norm again,  and  get

i,
_ 

4(x(.)X). 

g(A,,.X)) +.E Ilg(Aj.(X))11 2

Z. 

-2  (x- 
y2Y

* 

where  Y2  is  once  again  some  bounded  random  variable.  These  two bounds  satisfy  the  con-

ditions  of the dominaed  convergence, theorem,  and so the interchange  is justified.  However,

from  the  form  of  the  two  bounds,  and  because  f  and  g  are  continuous  functions,  we  see,
(X)  is continuous  in e at  e  = 0.  Moreover,  this
that the limit  lim,-.o Z, exists whenever  A)i 

limit  is given  by 

u~~z,  =  ~lx-/,(,(x)):-'..I

.

C-40 

~  IIX-hA'()121=0

de 
-2  (x  - I(..,(X)))  • go,(x)).:

We  show  in lemma  4.3.1  that this continuity  condition  is  met almost  surely.

.

•  , 
,, 

,  , 

.-. 

.

.. 

* 

.,. 

. .

. .

.
.

.... 
.

.... ~ .

*

.

. .

. .

..

4[4 

Section 4.2:  The  etationarity property  of principal curves

We  denote  the  distribution  function  of Af(X)  by  h.,  and  get

2 D(h, f,) 

-2'E 

I, 

(X  JAh(X)  A) -

(.X))  -g(.). 

(4.4)

If f (A) is a principal  curve of h, then  E (X  AI (X)  = A) = 1(A)  for  all A in the support

of h.%,  and  thus

Alternatively,  suppose  that

d  2= 
h 

0 

differentiable  g.

z1"(z(X  - f(A)  IA(X)=  A) . g(A))  =0 

(45)

for all  differentiable  g. In  particular we could pick  g(A)  =  E(X  IAf(X)  = A) - !(A).  Then

2I

MIII  E(X IJA(X)= A)1 

(A)  =0

and  consequently  I  is  a principal  curve.  This  choice  of g,  however,  might  not be differen-
tiable,  so some  approximation  is  needed.

S'nce  (4.5)  holds  for all differentiable  g we  can  use  different  g's 'to knock  off different'

A A)  - f(A). 

pieces  of  X(X  A, (X) 
In  fact  we  can  do  it  one  co-ordinate  at a  time.  For
example,  suppose  E(XI  A1 (X)  =  A)  is  positive  for  almost  every  A E  (Ao, A,).  We  s, ggest
why  such  an  interval  will  always  exist.  We  will  show  that  Af(z)  is continuous  at  almost
every  z.  The set  {X  I A(X)  =  A E  (Ao,  AI)}  is the set  of X  which  exist in an open connected
set in the normal plane  at  A, and  these  normal  planes  vary smoothly  a  we  move  along the

curve.  Since  the  density  of X& is  smooth,  it  does  not  change  much  as  we  move  from  one
normal plane  to the  next,  and  thus its  expectation  does  not  change much  either.  We  then

pick  a  differentiable  gi  so  that it  is  also  positive  in  that  interval,  and sero  elsewhere,  and
set  g2- 
g  s  0.  We -apply the  theorem  and'get  E(XI  JIA(X). =  A)  -- f1 f(A)  for
A E  (Ao,  A,).  We  can  do  this  for  all such  intervals,  and  for  each  co-ordinate,  and  thus  the

result  is  true. 

Corollary

Ifa principal  curve  is  a straight  line,  then  it is  a principal  component.

"

,  .

,.4.,, 

... 

.,.,.., 

-.. 

.. 

",  ".'. 

- .

,, 

.. 

-...... 

.

.".. 

.(cid:127)

.

, 

.--.. 

Chapter 4:  Theory for  principal  curves  and  surfaces  45

Proof

If f  is a principal  curve,  then  theorem 4.3  is 'rue  for  all  g,  in particular for  g(A)  =  A.  We
then  invoke  theorem  4.2.

In  order to complete the  proof, we  need  to prove  the following

Lemma  4.3.1

The  projection  function  Af, (z)  is continuous  at e  - 0 for  almost  every  z  in  the support  of

h.

Proof

Let  us  consider  first  where  it  will  not  be  continuous.  Suppose  there  are two  "ints  on 
equidistant  from  z, and no other points on  f  are as close to z.  Thus  3 A0  >  Al  , A,(z)  Ao
and  flz -
f(AdIl[.  It is  easy to  pick  g in  this situation  such  that  A',(z) is not 
continuous  at  e  - 0.  We  call  such  points  ambiguous.  However,  we  prove  in  lemma  4.3.2

f(Ao)I  =  IJz -

that the set, of all ambiguity  points for a finite  length  differentiable  curve  has measure  zero.
Wd thus  exclude  them.

Suppose w  > 0 is given,  and there  is no point on the  curve  as close  to z  as  f(A,(z))

f(Ao).  Thus  IiX  -, 
(Ao)ll  <  II:  - f(A1 )I  V  A, e  [a, b]  n  (Ao  - w, Ao  +  w)9.  (Notice  that at
the  boundaries  the  w  interval  can  be  suitably  redefined.)  Since  this  interral  is  compact,

0

'

.O

and  the  distance  functions  are  differentiable,  we  can find  a 5  > 0  such  that  11z  -
- 6.  Let  M  =  supA[.aI  JIg(A)lJ I-and  co  =  8 /(2M).  Then  Iz  -. 
li  -
llx- 
(Ao  - w, Ao +  w),  and  the continuity  is established.

/(AI)li 
.(AI)li  V  Al  e  [a, b  11  (Ao  - w, Ao +  w) 

f(Ao)ll  <"
<
,€(Ao)ll 
co.  This  implies  that  Af,(z)  e

ind  .e  (cid:143) 

Lemma  4.3.2

The set  of ambiguity  points has  probability  measure  zero.

P roof

We prove  the lemma  for a curve in  2-space,  but  the proof generalizes  to higher dimensions.

Referring  to figure  4.2,  suppose  a is  an  ambiguity point  for the  curve  f  at A.  We  draw the
circle  with center  a and  tangent  to f  at A.  This means  t!iat  f  must be tangent  to the circle
somewhere  else,  say  at /(A').  If 1 
on the  normal  at 1(A)  is also an ambiguity  point,  we  can
draw a similar circle  !or it.  But. this contradicts  the fact  that  f(A) is  the closest  point to  a,

e,  or  e-.

46 

Section 4.2:  The  stationarity property of principal curves

Figazre  4.2  There  are  at  moat  two  ambiguity  points  on  the  normal
to  the  curve; one  on  either  aide  of the  curve.

5imce  the circle  for  b lies  entirely  inside  the  circle for  a, and  by  the  ambiguity  of b, we  know
the curve  must  touch  this inner  circle  somewhere  other  than  at f (A).

Let  I(X)  be  an  indicator  function  for  the  set of ambiguity  points.  Since  ther.. are -at
Af A(X)  =A) =0.  But this also  implies  that the

moat  two at  each  A, we  have that  E (1(X,) 
unconditional  expectation  is  zero.

Corollait

'The projection  index  Af (z)  is  continuous  at  almost  every' 

.,

Proof

We  show  that if Af (x)  is  not  continuous  at  x,  then  x is  tn  ambiguity  point.  But  this set*. 
has  measure  zero  by  lemma  4.3.2.

-

N  f A(x)  is  not  continuous  at  x,  tLere  exists  a. to  .O 0such  that  for  every  6  >  0  3  zF
such  that  lix - x6li  <56  but  A()-A()I>  to.  LettiL..  Z'  o  to ziro, we  mee  that x must

......... 

...........................

Chapter 4:  Theory for principal  curves  and surfaces  47

Sx

o

/

I 

I 
I 

1(a)

rI

p

Flgure 4.3  The set of points  to the right  of  f(a) that project  there
has meauM 

geo

be equidistant  to A 1(z)  and  at  least one other point  on  the curve with  projection  index  at
least  (o  from  ,f  (z).

Theorem  4.3  proves  the  equivaience  of  two  statements:  f  is  a principal  curve  and  f
is  a critical  value  of  the  distance  funct-on.  We  needed  to  assume  that  f  is  defined  on  a

compact  set  A.  This  means  that  the  curve  has  two  ends,  and  any  data  beyond  the  ends
might well  project  at the  endpoints.  This leaves  some doubt as  to wethet  the endpoint  can
be'the  average  of  these  points.  The  next  lemma  shows  that  for  either  statement  of the

theorem  to be  true, some truncation  of the sup  ort of h might be necessary 

if the  support

is unbounded).

Lemnma  4.3.3

If I  is  a principal  curve,  then  (2- 

f( O(f)) 

- I(A(z)) =  0  as.  for  z  in  the  support  of

h.  If 
derivative  from  the  right,  and similarly  from  tbo, left  for r(b).

---- 0  V  differentiable  g,  the 

the  same  6i  true.  By  f'(a) we  mean  the

Proof

If A,(x)  e  (a,b)  the  p.oof  is  immediate.  Su 
ordinates  so  that  f'(a)  =  el.  No  points  to  the  left  of  f(a)  project  there.  Suppose  f  is  a

ose  then  that  Af.(z)  =  a.  Rotate  the  co.

"~~~~~~~~~~~.....'....'.....'..'..'...'....'(cid:127)

(cid:127)...'..'...". 

:<w-w~w <.~ 
• 

"• 

...

'....'.... 

.. 

.i" 

.. "(cid:127)" 
"," 

.

o.*.  K. 
"'.'..'.-'. 

.'.".:. 

.q 
: 

" 

*o°..

".. 

".2'. 

.""
" 

_." 

" 

-

-'..

AS 

Section 4.S:  Some 

resrlts so  the  asishcs  of emooth  princ-ips  curves

principal  curve.  This  then  implies  that  the  set of points  that  ate  to the  right  of f(a) and
project  atf 1(a)  has  conditional  mewire  zero,  ele  the conditional  expectation  would  be  to
the  right.  Thus  they  also  have  unconditional  measure  zero.

Alternatively,  suppose  that ther 

is a  set of z of positive  measure  to the right  of f(a)
that projects  there.  We can  construct  #  such  that  #(a)  =  f(a), and  zero  everywhere  else.
For such  a choice  of #it  is clear that  the derivative cannot  be zero.  However,  this choice of
t  is not continuous.  But we  can construct  a version  of g  that is differentiahle  and does  the
0  V
same job a  #.  We  have then  reached  a  contradiction  to the claim  that 
differentiable  g. 

0

4.3.  Some results on the subclass of smooth principal curves.

We  have  defined  a subset  Y.(h)  of principal  curves.  Thes  a(cid:127)e  principal  curves  for  which
A,(s)  is  a continuous  function  at  each  x  in  the support  of  h.  In  the  previous  section  we
showed  that if A/(z) is not continuous  at x, then  x is an  ambiguity point.  We  now  prove the
converse:  no  points of continuity  wre  ambiguity  points.  This will prove  that the  continuity

constraint  inder4  avoids ambiguitiss  in  projection.

In  figure  4.4& the  curve  is  smooth  but  it wrap.  around  so that  points  close  together
might project  to completely  different  parts  of the  curve.  This  reflects  a global  property  of
the crzve  and presnts an ambiguity  that isunsatisfactory  in a summary of a distribution.

Thsorem  4.4

If .f,(z)  is  continuous  at x, then  z is  not an  ambiguity  point.

Proof

We  prove  by .contradiction.  Suppose  we  have  an  x,  and  At'*  A, such  that

j11 - f(,00I  = 112  - 1(A,)II

= A(x,  P)

It is eayto se that  if AI  yields the closest  point on  the curve for  z,  then At  is the  position
that  yields  the  minimum  for  al 
.,  =  atf(A,)  +  (1 - aj)x for  a e (0, 1).  S-milarly  for A,.
Now  the idea  is to let  at and  oa  get  arbitrarily  small,  and thus .,la.  - x.,1ll  gets small,  but
A 

- A/(x.,)  =  constt  and  this violates  the  continuity  of Af(.)

Figure  4.4b  repristnts  the  other  ambiguous  situation,  this  time  caused  by  a  local
property of the curve.  We  consider only points  inside the curve.  If such points  can occur  at

•  %  '.'.'o'' .' 

• 

;  +. 

.

.

..

.. 

j' 

.. 

.' ." 

: 

." 

.. "..'.... 

.'.,...+.'......',......-...-.. 

, 

, 

.

.

-

-.

+  "...".. 

.

..

,. 

.,•

.. 

,, 

• 

.

.

C"Apter 4:  Theory for principai  curves and surfaces  49

-, 

IfQ 

.) 

g

IN

( (b)

FIgure 4.4  The contiunity co,.traizA  avoids  global  ambi.iu, 
,sid  local  abigaki 

(b) i  projeqtac.

(a)

the center of curvature,  then  there  is  o unique point of projection  on  the curve.  By  inside
we  mean  tha  the' in'n  product  (s  - f(A,*(z)))'.  (ef(Af(x))  -
(A,/(z)))  is  non-negative,
where  e,(A)  i  th  Center o  curvature off  at the, point  1(A).

Theorem  4.5

If A/ (x)  i  continu 

atz, then u  is  not at the  center of curvature of f at A.

Proof

The idea  of the  proofi  illustrated  in  igure .4.4b.  If apoint at.e,(A)  projects  at A, then  it
will  project  at many  other  points immediately  around  A, since locally  f(A)  behaves like  the.
wre  of a circle.with  center  ef(A).  This would  contradict  the continuity of Af.  Furthermore,
if a point  at z  beyond  e,(A)  projects  at A, we would expect  that points  on either  side  of 
would  project  to different  parts of the curve,  and  this would  also contradict  the  continuity
of  A,.

..  : 

(cid:127) 

'_ 

_J  "+- 

(cid:127) 

+"  ""  ''I""(cid:127)

_  _" 

--. ,- 

'

.K." 

50 

Section 4.4:  Some  reslts  on bi

We  now make  theme  ideas  precise.  Assume x  projects  at Af(z)  - AO,  where

f =  (Ao) +  iZ1" 

(o' 

-. 

+8)

i 

and 6  0.  Thus x is on  or beyond the center of curvature of f  at Ao.  Let q(A)  d 
111(A)  - zJ.
By  hypothesis  q(A)  >  q(Ao)  with  equality  holding  iff  = A(cid:127)0. (Otherwise  there would be at
least two points  on the  curve the same distance  from x and  this would violate the continuity
of A1)  This  implies  that
(1) 110o)  =0
(2)  q'(Ao)  > 0 for a strict minimim  to be  achieved.

We evaluate  these  two couditions.

eo)  =f(Ao)  I(A)- 

( 

1 

+))

=  ( P) 
iI1)11  1I(,)II

e(4) = 

(O1).  (1(A) 

() 

-

= J((cid:127)) I/'(11111'(cid:127)''(cid:127)+ 

+ r(A,).  ,f,(Ao)
81+

which contradicts  (2)  above. 

U 

-

4.4.  Some  results  on. bias.

The  principal  qrve procedure  i  inherently  biased.  There  aretwo  forms  of bias  that can
occur  concurrently.  We identify  them as  model  Usi 

and  eetimetion  bia,.

Model bias occurs  in the framework  'of a functional  model,  where the data is generated
from  a model  of  the  form  =s  (A) + e,  and  we  wish  to recove.r 
(A).  In  general,  starting
At  f(A),  the  principal  curve  procedure  will  not  have  f(A)  as  its solution  curve,  but  rather
a biased  version  thereof.  This  bias  goes  to  zero-with  the  ratio .of the  noise  variance  tothe
radius of curvature.

Estimsaion  bias  occurs  because  we  use  scatterplot  smoothers  to  estimate  conditional
expece.aions.  The  bis  is  introduced  because  we  average  over  neighborhoods,  lnd  this
usually  has 

flattening  effect.

I,...

Chapter 4:  Theory for principal curveS  and aurface.  51

1 

S0

Plgwr.  4.5  The  data  is  generated  from  the  arc  of  a  circle  with
radius  p  sad with  iid  JI(Oouzl)  errors.  The  location  on  the  circle  is
selected  wiilwinly.

4.4.1.  A  simple  model  for investigating  bias.

The  scenairio  we  shall  consider  is  the  arc of  a circle -in 2-apace.  This  can  be  parametrized-
by  a unit  speed  curve  1 (A) with  constant  curvature  l1p,  where p is the  radius of the  circle:

1(A)  = 

(4.6)__

for  A e  (-A11 AfJ 
the type  1-.,A41  by  A#..

-

s 
plup.  For  the remainder  of this  section  we  will denote  intervals  of

The  points  x  wre  generated  as  follow.:  First  a A is  selected  uniformly  fr-om  Af.  Given
this value  of  A we  pick  the  Point x from some smooth  symmetric  distrioution  with  first  two
ments  (f (A)I,. 2 ! )  where.a  has yet  to  be  specified.  Lintuitively  it  seems  that more  mass
gets  put  ou.tside the  circle  than  inside,  and  so  the  circle,  nr  arc thereof,  that  gets  closet
to  the  data has radius  larger than p.  Consider  the  point.  that  project  onto  a  small  arc  of
the  circle  (seo  figure  4.5). 
'They lie  in  a  segment  which  fans  out  from  the  origin.  As  we
shrink  this  arc  down  to  a point,  the  segment  shrinks  down  to  the  normal  to  the  curve  at
that  point,  but  there  is  always  more  mass  outside  the  circle  than  inside.  So when  we  take
conditional  expectations,  the  mean  lies  outside  the circle.

One  would  hope  that  the  principal  curve  procedure,  operating  in  distribution  space

52 

Section 4.4:  Some  resudte on  bia.

and  starting  at  the  true  curve,  would  converge  to  this  minimizing  distance  circle  in  this
idealizedý  situation.  It turns  out that  this  is indeed  the  case.

Figure  4.5  depicts  tho  situation.  We  have  in  mind  situi~tions  where  the  ratio  0u/P  is
small  enough  to  guarantee  that  P(IeI  >  p)  su  0.  This  effectively  keeps  the  points  local;
they  will  not. project  to  a  region  on  the  circle  too  far  from  where  they were  Pnerated.

Theorem  4.8

Lot  f (A),  A -E A,  be  the  arc  of a circle  as  described  above.  The  parameter  A is  distributed
uniformly  in the  arc,  and  given A, z=J (A) + e where  the components  of e are fid  with mean
o variance  o,2.  We  concentrate  on  a smaller  arc  A, inside  Af,  and  assume  that  the  ratio
w/p io small enough  to guarantee  that all  the points  that  £p-oject  into A* actually originated
-from somewhere  within  A1 .

Then,-

where

As/p  0/2 and'

Finally  r*-.*p  as a/p -0.

Lemma  4.6.1

.~sin(0/2) 

(4-7)

*  Suppose  A/  =  P.  (We  have  a full  circle.)  The  radius  of the  circle,  with  the same  center

as  f (A),  that  minimizes  the  expected  squared  distance  to  the points  is*

I =  E(+ea)2+eC2

,>p

*  Also r-.as a/P -0.

.1I thank  -Art Owes  for suggesting  this reult.

Chapter 4:  TAeory for principal  curves and surfaces  53

Proof  of lemma  4.6.1

The situation  is  depicted  in  Figure  4.5.  For  a given  point  z  the squared  distance  from  a
circle  with  radius  r is  the radial  distance  and  is  given  by 

.. 

"

aN(z, r) =  (1l:ll - r)..

The expected drop in the squared  distance using  a circle  with radius  r instead  of p  is given
by

ZAD2 (;,rp)  =  Zd 2 (:,p) - EdC(zr)

=  Z(llll  - W)- 
We now  condition  on  A  0 and  expand  (4.8)  toget

E(l-IIl  -..

ZAD2(,.,,I(cid:127)=  O) = 

r2 + 2(r - p) E(cid:127)/I(p  + el)2  + e'.

DifferentiaiLg  w~r.t.  r  we se  that a  maxirmm is  achieved  for

(4.8) 

-

r= 

re 
Z= p /(p 

___

+ CI)2 + (.-)

2

> C 1 1 +  I/,,I"-:'

->pIz(1+ei/,,)l (Jensen)

with strict inequality  if  Var(eq/p)  - o 2 /p 2  =0.  Note  that

3&),2 (Zr',p)  (p-  E  (p+  l)I +e2) 

(4.9)

which  is  non-negative.

When we  condition on some  other value of A, we can  rotate  the system around so  that
A =0 since, the distance  is invariant to such  rotations,  and thus for each  value of A the same
r* maximizes  ZAD(z,r,p IA ), and  thus  maximizes  AD2

,X,p). 

p

n 

Note:  We  can  write  the expression  for  r°  as

re= , 

(cid:127) 

+E 

+ 24 

(4.10) 

-

*.'  s.-'-,..:.-*-  .

* *.

*  "-, 

-. 

.".0 

.

-"-. 

""9  .- .- 

*.- 

,,  ''.(cid:127). 

."'2  ."-"  ,''.- 

-...

, 

-"..  -"..  .".', 

".. 

"-. 

.. 

.".. 

"  .
..

' 
.".. 

,; 

.

.".. 

."*, 

,"• 

.- 

-.......

[...,.".-.

54 

sections  4.4:  Some  ressut.em on  b

where  ei  =  ei/u,  ej 
Taylor's  expansion  we  get

(0, 8),  avd  8  =  o/p.  Expanding  the square  root expression  using  the

,"  p + u2/(2p). 

(4.11) 

i  -:

This yields  an  expected  squared  distance  of

Zd2(X,  ')  ~  2 - o,'/(4p)

which  is smaller than the usual v3.  This expression  was  also  obtained  by  Efiom  (1984).

Proof of theorem  4.6.

We  will  show  that  in  a  segment  of  size  (cid:127)  the  expected  distance  from  the points  in  the
segment  to their  mean  converges  to the  expected  radial  distance  as 
0-*0. If we consider
all  such  segments  of size  0, the  conditional  expectations  will  lie  on  the circumference  of
a circle.  By  definition  the conditional  expectations  minimize  the squared  distances  to the 
points  in their  segments,  and  hence  in  the  limit the  radial  distance  in  each  segment..  But
so did r*,  and  the  results  follow.

Suppose  that  ÷ is  chosen  so  that  2r/O is  a positive  integer.  We  divide  the  circle  up
into  segments  each  with  arc  angle  I.  Consider  E(z  1Af(z)  6  A#),  where  A# and A# are
defined  above.

Figure  4.6depicts the situation.  The  points are  symmetrical  about the'z 1-axis,  so the
expectation  will  be  of the  form  (r, 0)'.  By  the  rotational  invariance  of the problem,  if we
find these conditional  expectations  for each  of the segments  in  the circle,  we end up with  a
circle  of points,  spaced  0 degrees  apart with  radius  r.

-

We 

t show  that as  4--0, r- 

r*.  In'order  to do  this,  let us compare  the  distance
of points from their  mean vector  r = (r, 0)'  in the segment,  to their radial distance from the
radius  r.  If we  let r(z) denote the  radial  projection  of z  onto the circle, we have
circle wi 

z [(z - E (z  Af (z) C AO) 2  IAf (z)  A AI  = 

[(z  -)2, 

IAf(z)  r =  1 

(4.12)

>  Z((x - r())1 I,(fz)  A(cid:127)I, 

---

Also,  we  ave

Z ((X 

r)2 I,%(S)  CE Ai,(cid:127),

f, (z) E A#1  +  C[(r(x) - r) 

((cid:127)A(z) 9 A#]

,(x))12 

Z [(X -
-2 z (I,(x)  - rl Ix -

(z)I  cos('(z))  I A ,() 

r A#)

(4.13)

* 

2.. 

.o 

.

-

-

• 

-

Chapter 4:  Theory for principal  cumes  and  surfaces  55

XS

r~x

- r(x

/2

(r0)/4.(cid:127)%.

F gur,  4.8  The  coaditiomal expectation  of z,  giveas A 

,.() eA#.

ere the angles adepicted  in figure  4.6.  The  second term on the  right of  (4.13)
where  t#() 
is smaller than  (rO/2)2.  We treat separately  the case  when z  is inside  the circle,  and  when
z is outside.

P  When x  is iside the circle,O(z) is  acute and  hence  coe(O(z))  > O.  Thus

' 

•[i; 

S  E  (z -r(z)) 

' IA1(z)  E A ¢]j+ -O (#) 

(4.14)."-":

.'A.-.',

*  When  z  is  outside the  circle,  t(z) is  obtuse  and  cos(O(x))  < 0.  Since  - cos(@(z))
r/2  S  #/4, we  have  that  - cos(O (z)) 
r(z)1)  1A1(z)  e  AJ  in  bounded  since  the.

sin(f#(z)  - r/2) and  from  the  figure  O(x) -
Ia 1z- 
sWn(÷/4)  =  O(0).  Now  Z.[(r(z)'- 
errors  are  assumed  to have  finite second  moments.  Thus  (4.14)  once  again  holds.

i 

<5

So from  (4.12)  and  (4.14)  , as 0  -- 0, the  expected  squared  radial  distance  in the segment
and  the expected  squared  distance to the mean  vector converge  to the same  limit.  Suppose

3z(X  l~ f(X)  =01  = ,'

0I

** 

.**** S."*.-,S 

.,..."

56 

section 4-4:  Some  rwesst  oa  bia

Since  the  conditional  expectation  r'* minimizes  the  expected  squared distance  in  the  seg-
ment, this tells us that a circle with radius  r* minimizes  the radial distance in the segment.
Since, by rotational  symmetry, this is true for each  such segment, we have that r** minimizes

Z,,  z(fIIl  - 0)'  1  W(z) = 

= 

.(IIzII  - r)

This  then  implies that  r*  - r" by  lemma 4.6.1  and  thus

lim  Z(ztAj(z)rEA,#)=  Z(zI(cid:127)A(1)=o)

This is  the conditional  expectation'of points  that project  to  a an arc  of size 0 or simply  a
point.  In order to  get  the conditional expectation  of points that project  onto an  arc of size
0,  we  simply integrate over the  arc:

3(s IAf(z)  e A,) 

Z A(z)EZ E(X IAf,(Z)  =-A)

Suppose A corresponds  to an  angle  x, then

z(A  I /(X) 

A) 

(r"  cos(z)'.
S,.) sin(z))

Thus

Corollary 

-/r°2 

(cid:127) 

dz\"'

II

The above results generalize  exactly  for the situation where  data is generated  from a sphere
in IRS.  The sphere  that gets  closest  to the data has radius

,." = 

/e(pe+ct)too, + 

n + 

h4 

i 

and this is exactly the conditional  expectation of zz fOr points whose projection is at (p, O, 0)'. 

''"

.-.v.-.

..

S" 

"" 

" 

" 

"-..-

Ckapter 4:  Tleory for principal cures and surface*  57 

." 

"

Corollary

If the  data  is  generated  from  the  circumference  of a  circle  as  above,  the  principal  curve
procedure  converges  after one  iteration  if we  start  at  the  model.  This  is  also  true for  the
principal  surface  procedure  if the  data is  generated  from  the surface of a  sphere.

Proof 

After  one  iteration,  we  have  a circle  with  radius  rv.  All  the  points  project  at  exactly  the
same  position,  and  so  the  conditional  expectations  are  the  same.  This is  also  true  for  the
principal  surface  procedure  on  the  sphere. 

4.4.2.  From  the circle  to the helix.

The  circle  gives  us  insight  into  the  behaviour  of the  principal  curve  procedure,,  since  we 
can  imagine any  smooth  curve  as being  made  up  of many  arcs of circles.  Equation  (4.15)
clearly  separates  and demonstrates  the two  forms of bias:

e  Model  bias since r"  >  p.

*  Estimation  Was  since  the co-ordinate  functions  are  shrunk  by a  factor  sin(9/2)/(0/2)

when  we average  within  arcs  or  spans of size  9. 

For a sufficiently  large span,  the estimation  bias will dominate.  Suppose  that in the present
setup,  (cid:127)  =  p/4.  Then  from 
(4.11)  we  have  that  r*  =  1.031p.  From  (4.7)  we  see  that
a smoother  with  span  corresponding  to  0.27s  or  14%  of the  observations  will  cancel  this
effect.  This  is. considered  a small  span  for  moderate  sample  sizes.  Usually  the  estimation
bias-will  'tend to flatten  out curvature.  This  is  not always  the  case,  as  the  circle  example
demonstrates.  In  this special  setup,  the center of curvature 'remains  fixed  and  the result  of
flattening the co-ordinate  functions is to reduce  the radius of the circle.  The central  idea is
still  clear:  model  bias is  in  a direction  away  from. the center  of curvature,  and  estimation
-bias towards  the center.

We can consider  a circle to be a flattened  helix.  We show that as we unflatten the helix, 
the effect  of estimation  bias changes  from reducing  the radius  of curvature  to  increasing  it.

To  fix  ideas  we  consider  again  ,the circle  in  RI.  As  we  have  observed  the  result  of
estimation  and  model  bias is to reduce  the expected  radius  from  1 to r  (for a non-zere  span

I  o 

ano-eO 

spa 

4

-

.

lop

....-

/

,.. 

58 

Section 4.4:  Some  results on  bias

smoother such  that r <  1).  Thus  we have

with II?.'(A)tI 

r.  The  repairameterised  curve  is given  by

ri )c(A)
r sin(A) 

'

and by definition  the radius of curvature  is r <  1. Here  the  center of curvature remains the
same,  but this is  not usually  the  case.

A  unit speed  helix  in  R5  can  be represented  by'

f,(A)  =/sin(A/c)
bA/c 

where  c2  
1 + P.  It is' easy to check  thatr= 1 + b 1, so even  though the helix looks like a
circle with  radius  1 when  we look  down  the  center, it  has a radius of curvature  larger than
1.  This  is  because  the  osculating plane,  or  plane  spanned  by  the  normal  vector  and  the
velocity  vector,  makes  an angle  with  the zi  - z2  plane.  In  the  case of a circle,  the effect  of
the  smoothing  was  to  shrink  the  co-ordinates  by  a  factor  r.  For  a certain  span smoother,
the  helix  co-ordinates  will  become  (rcoo(A/c), r sin(A/c),  bA/c)'.'  Notice  that straight  lines
are preserved  by the  smoother.  Thus  the new  unit  speed  curve  is given  by

,)(A) 

(r cos(X/c*)
r 
|,in(A,/o*) 

,::(cid:127)-.....

bA/c" 

:

"..

where c  =  r2 + V..  The radius  of curvature  is  now  (r' + b2)/r. If we look  at the  difference
in the radii  we get

r) I-" 

r2 +  62:'':::
-- 
, r)€(P  ,,- r)....--.
= ¢ 

+ Vt

r

>  0  if  bP  >  r

This  satisfies  our  intuition.  For  small  b the  helix  is  almost  like  a circle  and  so  we  expect
circular behaviour.  When  b gets large,  the  helix is  stretched out  and  the smoothed version
has a larger radius of curvature.

.. 

.

.* 

t.*. 

.0- 

..

,,:. 

,::.:.:::.,
, -

CAapter 4:  Theory for principal curves  and surfaces 

59

4.4.3.  One  more bias  demonstration.

We conclude this section with one further example.  So far we have discussed bias in a rather
oversimplified  situation  of constant  curvature.

•.............................. 

I.... 

'I''. 

"

3

-3

-a .". 

.- 

..... 

.1....--.-

a 

.

4 

Figu~re  4.7  The  thick  curve  is the'the  principal  curve  using  conditional  expectations  at  the
model,  and  shows  the  model  bias.  The  two  dashed  curves  show  the  compounded  effect  of model
and estimation bias  at  spans  of 30% and  40%.

A  sine  wave  in  BR2  does  not  have  constant  curvature.  In  parametric form  we  have

A simple  caltulation  shows that  the radius  of curvature  rf (A) is  given  by

rj(A) 

(I  +  Cog,2(Ajr))S/2'

tT[0, 21  and  e 

"and  achieves  a  minimum  radius  of I  unit.  The  model  for  the  data  is  X 
e  where
X1(O,  1/4)  independent  of  X.  Figure  4.7shows  the  true  model  (solid
A 
curve),  and  the points  are a  sample  from  the  model,  included  to  give  An  idea  of the  error
,(X A(X)  =A). iHere  is's£  situation  where  the  model
structure.  The  thick  curve  is  Z 
bias results  In  a curve. with  more caturvatre,  namely  a minimum  radius  of 0.88  units.  This

(A) 

60 

Section 4.5:  Principal  curies  of elliptical distributions

curve  was  found  by  simulation,  and  is well  approximated  by  1/0.88sin(Ar).  There  are two
dashed  curves  in the figure.  They  represent  E (X  I Af(X)  E A.(A)),  where  A.(A)  represents
a symmetric  interval of length  sA  about  A (Boundary  effects  were eliminated  by  cyclically
extending  the  range  of  A.) 
'We  see  that  at  s  - 30%  the  estimation  bias  approximately

cancels out  the  model  bias,  whereas  at a  =  40%  there  is a  residual estimation  bias. 

-.

4.5.  Prirh:ipal  curves  of elliptical  distributions.

We have  seen  that for elliptical  distributions  the principal components  are principal  curves.

Are there any  more principal  curves?  We  first of all  consider the  uniform dis  with no  holes.

For  this  distribution  we propose  the following:

Fligure  (4.8)  The  only  principal  curves  in  T,(h)  of a  uniform  disk
ane  the  p~incipal  components..

Proposition

The  only  principal  curves  in  ),(h) ar%,  straight  lines  through  the  center of the  (Usk.

An  informal  proof of this claim is  as  follows:

o  Any  principal  curve  must  enter  the  disk  once  and  leave  it once.  This  must  be  true
since  if it were  to remain  inside  it  would  have  to circle  atound.  But  this would  violate
the  continuity  constraint  imposed  by 
n.(h) 

sincethere  would  have  to  exist points  at

-

-

-.-.. 

Chapter 4:  Theory for principal curves  and asrfaces  61

the centers of curvature  of the curve  at some places.  Furthermore,  it cannot end  inside
the disk  for  reasons  sirailar  to  those used  in lemma  4.3.3.

*  The  curve  enters  and  leaves  the  disk  normal'to  the  circumference.  For  symmetry
reasons  this  must  be  true.  As  it  enters  the  disk  there  must  be  equal  mass  on  both
sides.

*  The  curve  never  bends  (see  figure  4.8).  At  the  first  point  of curvature,  the  normal
to the  curve  will  be  longer on  one  side  than  the other.  The  set of points  that project
at  this  spot  will  not  be  conditionally  u-iformly  distributed  along  the  normal.  This
is  because  the  set  is  the  limit  of a  sequence  of segments  with  center  at  the center  of
curvature  of the  curve  at  the  point  in  question.  Also,  all  points in  the  segment  will
project onto  the arc that generates. the segment;  if not the continuity  constraint would
be  violated.  So in  addition  to  the normal  being  longer, it will  have  more  mass  on  the 
long side  as well.  This contradicts  the fact that  the  mean  lies  on the curve.

Thus the  only curves allowed  are straight  lines, and  they Will then  have  to pass  through  the
center of the  dank.

Suppose now  that we have  a convex  combination  of two disks of different radii  but  the 
same  centers.  A  similar  argument  can  be  used  to  show  that  once  again  the  only  principal
curves  are  the  lines  through  the  center.  This  then  generalizes  to  any  mixture  of uniform"
disks and  hence  to  any  spherically  symmetric  distribution  of this  form.

We conjecture  that for ellipoidal  distributions  the  only principal  curves  are the  prin- 

cipal  components.

XS

................................ 

.

p

9-

Oil

..

o

Chapter  5

Algorithmic  details

In this  chapter  we  describe  in more  detail  the  various  constituents of the  principal  curve
and surf&-.*  algorithms.'

5.1.  Estimation  of curves  and  surfaces.

We  described  a simple  smaotia  or  local  averaging  procedure  in  chapter  4.  There  it  was
convenient  to describe  the  sm-cother  as  a method  of averaging  in p spae.,  although  it bas 
been  pointed  out  that  we  can  do  the  smothing  co-ordinate  wise.  That  simplifies  the
treatment  here,  sin"  we  only  need  to  discuse  smoothers  in  their  more  usual  regression
context.

Usually a icatterplot smootheri  regarded  as an estimate of the conditional expectation-
Z (YjIX), where Yand X  werandom variables.  For our pirpoon  X way beoneor two
dinmenional.  We  will  discuss  one  dimensional  simoothers  first,  since  they  are  easier  to
implement  than  two dimensional  smoothers.

5.1.1.  One  dimenslonil  smsoothers.

Tne following  subset of smaothers  evolved naturally  as estimates of conditional  expectation,
and  are, listed in order of complexity  and  computational  cost.

5.1.1.1  1?%ovinj  aver*agsmboothers.

The  simplest  and  most  natural  estimate  of  E(Y  IX)  is  the  moving averagre  smoother.
Given  a sample  (pits4)#  i =0 1.  ... n,  with  the  xj  in. secending-order,  we define

Smooth  (V~ O~.- 

X 

j 

(5. 1)

[(n 

where  k 
- 1)/21  and  a e  (0, 11  is  called  the  span  of the  smoother.  An  estimate  of
the  conditional expectation  at  zj  is the  zverag,  of  the  yi  for  all  those  observations  with  x
value  equal  to  zj.  Since  we  usually  only  have  one  such  observation,  we  average  the  VIj for

-

Ckspter 5:  Algorithmic  detail.  63

all  those  observations  with  z  value  close  to  xj.  In  the  definition  above,  close  is  defined  in
the ordinal  scale  or  in  ranks.  We  can  also use the  interval  scale  or simply  distance,  but  this
Js computationally  mare  expensive.  This  moving  average  smoother  suffers  fromi  a  number
o(  draiwbacks,  it does  not produce  very  smooth  Uis  and  does  not  even  reproduce  straight
lines unless  the  z,  are  equispaced.  It also  suffers  from  bias  effects  on  the  boundaries.

5.1.12  Local  linewam moothem.

An  improvement  an the  moving  average  smoother  is  the  local  linter smoother  of Friedman
and  Stuetzle  (1981).  Here  the  suioother  estimates  the  conditional  expectation  at  Zi  by
the  fitted  value  from  the  leost  squares  line  fit  of y  on  &. using  only  those  points for  which
xj  e  (xi...h,zj+h).  This suffers  lessfrom boundary  bias than  the moving  average  and  always
reproducas  straight lines exacty.  The cost  of computation  for  both of the  above smoothers
is  0(n) ortie.Of 
course  we  can  think  of  fitting  local  polynomials  as  well,  but  in
practice  the  gain  in  bias  issmall  relative  to  the extra  computational  burden.

5.11.13  Loallyt  weighted  flaeaz  smoothems

Cleveland  (1979)  suggested  using  the  local  linear  smoother,  but  also  suggested  weighting-
the points  in the neighborhood  according  to their distance in  z from mi.  This produces  even
smoother  curves at  the expense  of an incremdul  computation  time of O(kn) operations.  (In
the local  linear smoother, we  can obtain  the fitted  value  at zi~ from that  at zj  by  applying
some simple  updating  algorithm  to  the  latter.  If local  weighting  is perf-bemed,  we  can  no
longer  use  updating formulae.)

5.1.1.4  Kernel  smoother.

The  kernel  smoother  (Gamser  and Muller,  1979)  applies  a weight  function  to every  observa-
tion in  calculating  the  fit at  zi.  'A variety  of weight  functions or kernels  exist  and  a popular
choice  is the gpassian  kevnel  centered  at  zj-  They  produce  the  smoothest  functions  and  are
computationally  the  motexpensive.  The  cost  is  OWn)  operations,  although  in  practice
the  kernels  have a bounded  domain  and this  brings  the  cost down  to 0(en)  for  some  athat
depends  On  the  kea  mi and  the data.

In  all  but  the  kernel  smoother,  the  span  controls  the  smoothness  of  the  estimated
function.  The larger the span,  the smoother the function.  In the case of the kernel  smoother,
there  is  a scale parameter  that controls  the spread  of the  kernel;  and  the  larger  the spread,
the smoother  the function.  We  will  discuss  the  choice  of spans  in  section  5.4.

64 

Sectio,  5.1:  E.stimation of curves and er/aces

For  our particular  application,  it was  found  that  the locally  weighted  linear  smoother
and  the  kernel  smoo;her produced  the most satisfactory  residts.  However,  when  the sample
size  gets large,  these  smoothers  become too  expensive,  and  we  have to sacrifice  smoothness

for  computational  speed.  In  this case  we  would  use  the faster  local linear  smoother.

5.1.2.  Two  dimensional  smoothers.

There  are  substantial  differences  between  one  and  two  dimensional  sinoothers.  When  we

find  neighbors  in  two space,  we  immediately  force  some  metric on  the space  in the way we

define distance.  In  our algorithm  we  simply  use  the euclidean  distance and assume  the  two

variables  are  in  the same scale.

It is also computationally  harder to find  neighbors  in  two dimensions than  in one.  The

k-d .t.e  ( Friedman,  Bently and  Finkel,  1976)  is an efficient algorithm  and data structure for

finding neighbors  in  k dimensions.  The  name  arises  from  the data  structure  used  to speed 

-

up  the  search  time  - a  binary  tree.  The  technique  can  be  thought  of as  a mltivariable
version  of the  binary  search  routine.  Friedman  et  al  show  that  the  computation  required
to  build  the  tree  is O(kn log n)  and  the  expected  search  time  for  the m nearest  neighbors
of any  point  is O(og n).

5.1.3.  The  local  planar  surface  smoother.

We  wish to find  Smooth (p  z0)  where xo  is a 2-vector  not necessarily  present in the sample.
The  following  algorithm  is  analogous  to  the  local  linear  smoother:

*  Build  the  2-d  tree for the  n pairs  (zi,z 2 )," 

,(zI.,zX.).

*  Find  the  nu  nearest  neighbors  of zo,  and  fit  the  least  squares  plane  through  their

associated  y  values.

o  The  smooth  at so  is defined  to  be  the  fitted  value  at so.

This  algorithm  doe.  not  allow  updating 'as  in  the  ono-dimensional  local  linear  smoother.

The  computation  time  for one  fitted  value is  O(log n +  nh).  For this  reason,  we  can  include
weights  at  no  extra order  in  computation  cost.  We  use  gaussian  weights  with  covariance

hI3r and  centered  at -so, snd  h is  another  parameter  of the  procedure.

for  the  ne'neighbonr. 

* A simpler version  of this, smoother  uses the  (gaussian weighted)  average of the  V values
In  the one  dimensional  came,  we  fird  that  fitting  local  straight  lines
reduces  the  bias  at the  boundaries.  In  surface  smoothing,  the  proportion  of  points  on  the

S  ... .. 

.....

%:.. 

.. 

.*.*.*...........'. 

..

,-,......,.','..'.. 

.,,.o......,,. 

....... 

•.  .
.• 

.-.. 

".. 

.

*.' 

,  ;  .. 

.

......... 
-
(cid:127) 

' 

"

,; 

:"

Chapter 5:  ALgoritkmic  details  65

boundary  increases  dramatically  as  we  go  from  one  to  two  dimensions.  This  provides  a

strong  motivation  for  fitting  planes instead  of simple  averages.

5.2.  The  projection  step.

T7e  other step  in  the  principal  curve  and  surface  procedures  is  to  project each  point  onto
the current surface or curve.  In our notation we require APs) (a4)  for each  i.  We have already

described  the  exact  approach  in  chapter  3  for  principal  curves,  which  we  repeat  here  for

5.2.1.  Projecting  by  exact  enumeration.

We project  z. into the  line segment joining every  adjacent  pair of fitted  values of the curve,
and find  the closest such projection.  Into implies that when projecting  we  do not  go beyond
the  two  points in  question.  This procedure  is exact  but computationally  expensive  (O(n)
operations  per  search.)  Nonethelees,  we  have  used  this  method  on  the smaller  data sets
(_S 150 observations.)  There  iso analogue  for  the principal  surface  routine.

5.2.2.  Projections  using  the k-d  tree. 

At esh  of the a  values of  I  we have  a  fitted  p vector.  This is  true for either  the principal
curv  or- surface  procedure.  We  can  build  a p-4  tree,  and  for  each  x.,  find  its  nearest
neighbor  amoungt  thes 
fitted  values.  We  then  proceed  differently  for curves and  surfaces.

o  For  curves  we  project  the point  into  the  segments joining  this  nearest  point  and  its 

left  neighbor.  We do  the sami 

for  the  right neighbor  and  pick  the  closest  projection.

o  For surfacs  we  find  the nearest  fitted' value  as  above.  Suppose  this is  at  fi )(A-')."

We  then  project  *I  oato  the  plancorresponding  to this  fitted  value  and  get  a new
value AV.  (This plane has already  been calculated  in  the smoothing  step and, is stored.)
We  then  evaluate  JOI(A.)  and  check  if it  is  indeed  closer.  (This  precautionary  step

is  similar  to  projecting  j4  into  the  line  segment& in  the case  of curves.)  If  it  is,  we

= 

= 

One  could  think of iterating  this prere,

which  is  simile  to  a  gradient  msarch.  Alternatively  one  could  perform  a  Newton-
Raphson  search  using  deri"vive  information  contained  in  the  least  squares  planes.
These  approaches  ate expensive,- and  in  the many  examples  tested,  made  little  or  no
difflerence  to  the estimate.

_

-

£.  AA-~A..& 

'6 6

66 

Section 5.2:  The projection step

5.2.3.  Rescaling  the  A's  to arc-length.

In  the principal curve  procedure,  as  a  matter  of practice,  we  always  rescale  the A's  to arc-
length.  The estimated  A's are  then  measured  in  the  same  units as the observations.  Let  A:
are  sorted.  We  define  " recursively  as follows:
* denotes the resealed 

$)'s,  and suppose 

0  =0.
* =+ 

Pwa)(S)) - fN01

"5................. 

I......."".

4 

de

103

oo

-1..".

Unsealed  A

1Fgw.  (5.1)  A  A plot for the circle example.  Along the vertica  ala 

pwe pot tenal 

a's at ev(cid:127)y 

itera(cid:127)io'n.
in  the  principal  curve  procedure.  Along  the horisontal  aide we  hive

valu.  for 1j, after  reecaling  the 

H

kke  "Ia's uinag  the pincipalacurv,  procedure  with  no reicaling.-

In  general  there  is no  analogue  of resealing  to  Wr-length  for surface.  Surface  arew is  the
corresponding  quantity.  We  can  adjust  the  parameters  locally  so  that  the  are& of a small
region  in  parametwr  space  has  the same  area  as  the  region  it  defines on the  surface.  But
this adjustment  will  be different  in  other regions  of the surface  having  the same  values for
one  of  the  parameter..  The  exceptions  ane  surfaces  with  sero  gaussian  curvature.  (These
we  surfaces  that can  be obtained  by  .metnW  denting a  hyperplane to form something  like
a corrugted shest.  One can  imagine  that such  a resealing  is  then  posibl).

.

...

.

.(cid:127)

'.. 

*. 

-

.. 

F 

*

Chapter 5:  Algorithmic  details  67

FIgure  (5.2)  Each  iteration  approxmately  preserves  the  metric
from the  previos  one.  The starting  curve  is  unit speed,  and so  the

nanl  curve  in apprdximately  so,  up  to  a constant.

Even though it is not poesible to do such a rescaling for surfaces,  it would be comforting
to know  that  our parametrization  remains  reasonably  consistent  over  the  surface  as  we  go
through the iterations.

Figure  5.1  demonstrates what  happens  if we  use  the principal- curve  procedure  on  the
circle  example,  and  do  not  rescale  the parameter  estimates  at  each  iteration.  The  tnetric
gets preserved,  up  to  a  scalar.  Figure  5.2showe  why  this  is  so.  The  original  metric  gets
transferred  from one iteration to  the next.  As  long as  the curves do'not change draw'mtieally
from one  iteration  to the  next, there  will  not be  much  distortion.

5.3.  Span  selection.

We  consider  there  to be 'two categories  of spans  corresponding  to two  distbict  stages in  the
algorithm.

"5.3.1. Global  proicedural  spans.

The  first  gumss  for  f  is  a  straight  line. 
In  many  of the  interesting  situations,  the  final
curve  will  not  be  a  function  of  the  arc  length  of  this  initial  curve.  The  final  curve  is
reached by  successively  bending the  original  curve.  We  have  found  that if the  initial  spans
of the smoother  awe  too small,  the  curve  will. bend  too  fast,  and  may  get lostl  The  most

C 

, .- 

A  * 

.

. % .

• 

...

.

* 
+'.++*.'-.-+'-....... 

.

...

.

.... 

.

...

(cid:127).-,'.141(cid:127) 

.

.

.

.

.

.

.(cid:127)..'.. 

.+.+....o. 

.

.-

.. 
+'. 

.. 

.- 

.

...

,.....-......-........,...-

., 

.

.

., 

........................................ 

-... -

... 

.

68 

Section  5.*:  Span selection

msuccewafu!  strategy  has  been  to  initially  use  large  spans,  and  then to decrease  them  slowly

In  particular, we  start  with  a span  of 0.5n,  and  let  the  procedure  converse.  We  then drop

the  span  to  0.4n  and  coverge  again.  Finally  the  same  is done  at  0.3n  by  which  time  the

procedure  has  found  the  general  shape  of the  curve.  We  then  switch  to mean  squate  error

(MSE)  span selection  mode.

5.3.2.  Mean  squared  error  spans.

The procedure  has converged  to a self consistent  curve  for the span  last used.  If we reduce

the spcz,  the  average  distance  will decrease.  This situation  arises  in regression  as well.  In

regression,  however,  there is  a remedy.  We  can  use  cros-validation  (Stone  1977)  to select

the span.  We briefly  outline  the  idea.

5.3.2.1  Crosa-valIdation  In  regresalon.

Suppose  we  have  a sample  of r.  independent  pairs  (pi,  z=) from  the  model  Y  =  f(X) +  e.
A  nonparametric  estimate  of f(zo)  is ia(xo)  =  smooth.(y  Io).  The  expected  squared
prediction  error  is

EPE =  E(Y  - f(X)) 2  

(5.2)

where  the  expectation  is  taken  over  everything  random  (i.e.  the sample  used  to  estimate
!(.)  and  the future  pairs  (X,Y)).  We  use  the  residual sum of squares,

RSS(e)  = 

-(X,))2

as  the  natural  estimate  of  EPE.  This  is  however,  a  biassed  estimate,  as  can  be  sen  by
letting  the  span  s  shrink  down  to 0.  The  smpoth  then  estimates  sk  by  itself,  and  RSS  is: 
-zero.  We  call  this  bias due  to  overfitting since  the  bias  is  due  to  the  influence  yj  has  in

7,

forming  its own prediction.  This  also shows  us  that we  cannot  use  RSS  to help  us 
ick  the
span.  We  can,  however,  use  the  cross-validated  residual sum  of squares  (CVRSS).  This  is
defined  as

CVaSS(*) = 

(u' - Smoot(I) 

)2  

(5.3)

where  Smooth 
moved,  and  then  evaluated  at zr. 

(y  !,z) is  the  smooth  calculated  from  the  data  with  the  pair  (W z,)  re-
It  can  be  shown  that  this  estimate  is  approx  mately

unbiaused  for  the true  prediction  error.  In  minimizing  the  prediction  error,  we  als  mini-

.

.

.

.
.

.
.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

..

, 

P  ".- 

-o

Chapter 5:  Algorithmic details  69

miz.  the integrated  mean  square error  EMSE  given  by*

EMSE(a)  =  E(f.(X)  -

(X)) 2

since  they  differ by  a  constant.  We  can decompose  this  expression  into  a sum  of a  variance
and  bias terms,  namely

EMSE(S)  =  IC( Var(f.(X)I  +  EL[(  E(!.(X)  IX) - 1 (;)) 2 1

= VAR(&)  +  BIAS 2 (a).

As.s  gets  smalkc 
the  variance  gets  larger  (averaging  over  less  points)  but  the  bias  gets
smaller  (width  of  the  neighborhoods  gets  smaller),  and  vice  versa.  Thur  if  we  pick  a to
minimise  CVRSS(s)  we  are  trying  to minimize  the  true  prediction  errcv  or equivalently  to
find. the  span' which  optimally  mixes  bias  and  variance.

Getting  back  to  the  curves,  one  thought  is  to  crossvalidate  the  orthogonal  dista nce
fun-tion.  This,  however,  will  not  'work  because  we  would  st.ill  tend  to  use  span  zero.  (In
general  we have  more chance  of being close to the interpohxting  curve. than  any other curve).
Instead,  we cross-validate  the  co-ordinates  separately.

5.3.2.2  Crose-validation  for principal  curvba.
Suppose  f  in is principal  curve  o(  h, for  which  we  have  an  estimate  Jbased  on  a  sample

A  natural requirempent is  to choose  a to minimize  EMSE~s) given  by

EMSE(S)  =  k 

(,X lI(X)) - !.(Af(X))11 2

which  is once again a tiruie-off between  bias  and variance.  Notice  that were we to look  at the
clowsest  distance  between  these curves,  then  the interpolating  curve  would  be  favored.  As  in
the regression  case,  the quantity  ATE(*)  = 
estimates  EMSE(.)  +
D(J),  where  D(f) =  Z' li 
It  is thus  equivalent  to  choose.s  to  minimize
EMSE(a)  or EPE(a). As  in  the  regression  case,  the cross-ali-dated  estimate

,% liX - 1-o(A,(X))ll 

(,X). 

CVRSS(S)  = 

O - Smooth~z 

2~D 

(5.5)

4

70 

Section S.S:  Spas selection

(x(),  attempts  to  do  this.  Since  we  do  not know  >,,  we  pick  A,  - Apk) (zi)
()  is  the  (non  croas-validated)  estimate  off.  In  practice,  we  evaluate  CVRSS(s)

where  >,  -" 
where 
for a few  values  of s and  pick  the  one  that gives  the  minimum.

From  the  computing  angle,  if  the  smoother  is  linear  one  can  easily  find  the  cross-
C Cy  for some  smoother matrix  C, and  the cross-validated  fit

validated  fits.  In  this case 
Sis  given  by  90(,  = E 

(Wahba  1975).

There  are  a number  of issues  connected  with  the  algorithms  that  have  not  yet  been
mentioned,  such  as  a robustness  and  outlier  detection,  what  to  display  and  how  to  do it,

and bootstrap  techniques.  The next  chapter  consists  of many  examples,  and  we  will  deal 

-

with these  issues  as they  arise.

~  .* 

*  %~. 
*. 

~ ~  * 

. ~  ~ ***... 

*  ~  ~ 

* 

..

,..,..,,- 

-

.

-",. 

.

Chapter  6

Examples

This chapter  contains six examples  that demonstrate  the procedures  on real and  simulated
data.  We also introduce some ideas such as bootstrapping, robustness,  and outlier detection.

Example  6.1.  Gold  assay  pairs.

This  real data example  illustrates:

o  A principal  curve in 2-space,

o  non-linear  errors in variables  regression,

"* co-ordinate function  plots,  and

"  bootstrapping  principal  curves.

A California  based company  collects  computer  chip waste  in order  to sell it  for its  content
of gold and other precious metals.  Before bidding  for a particular  cargo,  the company  takes
a sample  in  order to estimate  the  gold  content of the  the whole  lot.  The sample  is split  in
two.  One  sub-sample  is  assayed  by  an, outside, laboratoM, the other  by  their  own  inhouse
laboratory.  (The  names  of  the  company  and' laboratory  are  withheld  by  request).  The
company  wishes  to  eventually  use  only  one  of  the  assays.  It  is  in  their  interest  to  know
which'laboratory  produces  on average  lower  gold  content  assays  for  a given  sample.

The data in figure  6.1a consists of 250  pairs of gold  assays.  Each  point  is represented

by

-' log(l +  assay yield  for ith  away  pair for lab j)  and  where 

where  x 
I =  1 corresponds
to the  inhouse  lab and  j  - 2 the outside  lab.  The  log transformation  tends  to stabilize  the
variance  and produce  a more even scatter of points than  in the untransformed  data.  (There
were  many  more small  assays  (1 os  per ton) than  larger  ones  (>  10 os  per ton)).

S.  .. 

o 

. .-.::- -

72  Ezample 6.1:  Gold assay pairs

o......... 

...../..........

•  ' 

.

.

fi

3(cid:127) 
5I., 

S 2- 

.

S~ 

...-. 

3- 

2 

de" 
IT-

0 0 

2 

3 

4 

Inhouse  Laboratory 

2 

/ 

4 

f 

* 

.

.

Figure.  6.1a  Plot  of the  log  asays for  the 
inhouse and outside  labs.  The solid  curve is  the 
principal  curve,  the  dashed  curve  the  scatter- 
plot smooth.

...

* 

* 

6.1Sb  Estimated'  coordinate  rune-"."'Y

Figure 
tions.  The dashed curve  is the outside  lab,' the
solid  curve  the  inhottse  lab. 

-.

i

L

1

'..

-. 

"""'
.-.

A  standard  analysis  might  be  a paired  t-test  for  an overall  difference  in asasys.  This""-'

would  not  reflect  local  differences  which  can  be  of great  importance  since  the  higher  the""•..o
level  of gold  the  more, important  the difference.

noTh data was actually  analyzed  by  smoothing  the differences  in  log  assays  against the.'.'(cid:127)

average of the  two assays.  This  can  be considered  a form  of symmetric smoothing  and was
suggested  by  Cleyeland  (1983).  We  discuss  the method  further  cin chapter  T.side'lb-"th

Itn  model presented  here for  the  above  data is 

,heinoue.ab

'where ro is the  unknown  true.gold  content for  sample i (or  any  monotone  function  thereof),.
fi(r)  is  the  expected ualy  result  for  lab s 
analyze  the relationship  betw  cn fj  and 

,  and  ejg  is  measurement  error.  We  wish 

be 

for  different  true  gold contents. 

sohgnw

tothe

a 

g 

This is a generalization  of, the errorsin  variables model  or the structural  modelr(if weo

_______I 

-

Chapter 6:  Examples 

73

regard  the  ri  themselves  as  unobservable  random variables),  or  the functional  model  (if the
r,  are considered  fixed).  This model  is' traditionally  expressed  as  a lincar  model:

(zn)  (a+ 

zi)  + (e: 

62

where  f2(ri)  =  z  and

fi (n)  = 

ao fj7-(4) 

(assuming  12  is monotone)

It suffers,  however,. from  the  same.drawback  as  the  t-test  in  that only  global  inference  is

possible.

We  assume  that the eji  are palrwise  independent  and  that  *

Vaz(ei,)=  Var(e 2,)  vi.

The  model  is estimated  using  the  principal  curve  estimate  for  the  data  and  is  repre-
sented by  the solid  curve  in igure  6.1a.  The  dashed  curve  is  the usual  scatterplot smooth

of z2  against  z,  and  is  dearly misleading  as  a scatterplot  summary.  The  curve  lies  above

the 45"  line in  the interval  1.4 

. 4  which represents  an untransformed  assay interval  of 3  to

15  oz/ton.  In  this interval  the inhouse  average  assay  is lower  than  that  of the outside  lab.

The  difference  is  reversed  at  lower  levels,  but  this  is of  less  practical  importance  since  at
,these levels the  cargo is less valuable.  This is  Tpore  clearly seen  by examining  the  estimated

coordinate  function  plots in figure  6.1b.

A natural  question  arising  at this  point  wether  the  kink  in  the curve  is  real  or not.

If  we  had  access  to  more  data  from  the  sae 

population  we  could  simply  calculate  the

principal  curves  for each  and  see how often  th  kink  is reproduced.  We  could then  perhaps
construct  a 95% confidence  tube  for  the  true curve.

In  the  absence  of such  repeated  samples  we  use  the  bootstrap  (Efron  1981,  1982)  to
* simulate  them.  We would  like  to,  but cannot  generate samples  of size  n from  F,  the  true

distribution  of x.  Instead we  generate sample  of size  n from  P, the empirical  or estimated
distribution  function,  which  puts  mass  1/n  €  n  each  of  the  sample  points  xi.  Each  such
sample,  which samples  the points  zi  with repl acement,  is  called  a bootstrap  sample*.

In  the linear model  one usually  requires. thad  Var(ej,) 
relaxed  here.

constanti.  This  assumption' can  be

74 

Ezample  6.1:  Gold aseay pairs

5

4

o 

0

.0

0•

0

.

.

S2 

".

1 

2 
2 

4 4. 

Inhouse  Laboratory

.

F]gure  6.1c  25  bootstrap curves.  The data X  is sampled  25  times
with  replacement,  each  time  yielding  a  bootstrap  sample  X*.  Each
curve  is  the principal  curve  of such  a sample.

Figure  6.1c  shows  the principal  curves  obtained  for  25  such  bootstrap  samplA.  The
450  line  is  included  in  the  figure,  and  we  see  that none  of  the  curves  cross  the  line  in  the

region  of interest.  This  provides strong  evidence  that  the  kink  is indeed  real.

When  we  compute  a  particular  bootstrap  curve,  we  use  the  principal  curve  of  the

original  sample  as  a  starting  value.  Usually  one  or  two  iterations  are  all  that  is  required

for the  procedure  to converge.  Also,  since  each  of the  bootstrap  points occurs  at one  of 'the

sample  sitos,  we  know  where  they  project  onto this  initial  curve.

It  is  tempting  to  extract  from  the  procedure  estimates  of  Pi,  the  true  gold  level  for

sample  i.  However,. fi  need  not  be  the  true  gold  level  at all.  It  may  be  any  variable  that

orders  the pairs  f(f)  along  the curve,  and is  probably  some  monotone  function  of the  true

gold  level.  It  is clear  that  both  labs  could  consistently  produce biased  estimates of the  true

gold  level  and  there  is  thus  no information  at all  in  the data  about  the true level.

Estimates  of ri  do, provide  us with  a  good  summary  variable  for  each  of the  pairs,  if

Chapter 6:  Ezxamples 

75

that is required:

,=  h(zi)

since w  obtain  Pi  by projecting  the  point  xi  onto  the  curve.  Fiaally  we  observe  that  the
above analysis  could be extended  in a straightforward  way to include  3 or more laboratories.
It is hard to  imagine  how  to tackle  the problem  using standard  regression  techniques.

Example  6.2.  The  helix  in  three-space.

This  is  a simulated  example  illustrating:

*  A principal  curve  in  3space,

*  co-ordinate  pkos,  and

*  croms-validation  and  span  selection.

We looked  at the  bias of the  principal  curve  procedure  in estimating  the  helix in chapter  4.
We  now demonstrate  the  procedure  by  generating data from  that model.  We have

min(41)b
co(A) 4zA)  +xe,

where  A  U[0,  11  and. 
JJ(O,.-).  This situation  does  not present  the principal  curve
procedure  with  any  real  problems.  The  reason  is that  the starting  vector  pass  down  the
middle of the helix and  the data projects onto it in nearly  the correct order.  Table  6.1shows
the steps  in the iterations  as the procedure converges at each  of the  procedural spans shown.
At a span  of e  - .2 we use  croum-validation  to find. the minimutm  mre span.

Figure  6.2c  shows  the  CVRSS'curve  used  to  select  the  span,  which  is  0.'1  with  a
value of CVRSS  of 0.1944.  One more  step  is performed  and  the procedure  is  terminated.
Figure  6.2d  shows  the  estimated  co-ordinate  functions  for  this  choice  of span.  We  see
that  the  estimate  of the  linear  co-ordinate  is  rather. wiggly.  It is  clear  that  a smali  span
Wam  required  to  estimate  the  sinusoidal  co-ordinates,, but  a large  span  would  suffice  for
th* linear  co-ordinate.  This suggests  a different  scheme  for  croes-validation--choosing  the
spans separately  for each  co-ordinate.  The  results  are  shown  in  figures  d-.2e  and  6.2f.  As
predicted,  a larger  span  is  chosen  for  the  linear  co-ordinate,  and  its  estimate  is  no  longer
wiggly.  This is  the final  model  referred  to  in  Use  table and represented  in  figure  6.2.

.

.

.

.

.
.

.

.

.

.\

.

...

76  Eampk  5.2:  Tke  heli  is  tree-(cid:127)pace

x. wt 

c"

-dpne. 

" 

: .

• 

* 

*

.. 

S. 

.: 

." 

. .

..

Sspa"
Tabe 

61.TheAm 

inthitraton.* 

Inta. 

l 

th 

prcdr

Figure  S.la  Data  generaed  fron  a  he.- 
lix  writh  bidependent  aerrn  cm  each  coordinte 
helix,  the solid
The dashed curve  a  the orgi 

Figur  6.2b  Another  ye  of the htelix,  the
data  an 

the  principal cmiv..

Table  8.1.  The  u 

n  tP4 0.t40 

4. 

initial the  procedurea&

cu.0 

principal 

start 

estiate

rln 

the 
1.0qr 
o 

1.1 
1poceu10 

cournen 

2l(cid:127),n.0 

pr"hncip (cid:127)al  copon 

ine

0.4p 

0.56 

4.6. 

omnt 

-:

0.meed7uonvale

0.549 
0:376 
0.361 
0.360 
0.222 
0.217 
0.217 

0.162 
0.189 

4.7 
5.7 
5.4
5.4 
7.3 
6.9
'6.9 

9.7

converged
reduce  span

converged
reduce spas

converged

cross-validated

.

o-. 

.o.

~ 
2kr 

3 

4 
.
6 
7 
8 
9 
10 

0.4 
0.3 
0.3 
0.,3 
0.2 
0.2 
'0.2 

Mae  spans

final 

0.07,  0.09,  0.3 

Chapter 6:  Ezam ples  77

027

00.10

Span  9Etmae

Figue 62c 

ko  rowa~aiomcum 
sosCVRSS(s)  wa function  of  the  Spanea. 
On pai  used  for  all13  co-crdints. 

Fiurve2 
fauctiono  for  the helix,  using  the  span  fomnd  in
figure  6.2c.-

The  etmtdc-rlnt

0.07

00 

01

-2

0 

0.1 

0.2' 
Span  9 

0.3 

-.4 

0 

2.5 

5 

7.5 
Estimated  X

10 

12.5

Figure'  6.2i  The  eroe...ualidation  curve 
shows.  CVRSSJ(S)  as  a function  of  the  $pan  a. 
A  separate  &pan is  found  for  each  co-ordinate. 

Figure  6.2f  The estimated co-ordinate  func-
tions  for  the  helix,  using  the  spans  found  in  fig.
ure  6.2f.

78 

Ezample  6.S:  Geolo*ical  dats

The  entry  labelled  d.of.  in  table  6.1is  an  abbreviation  for  degrees  of freedom. 

In
linear regression  the  number of parameters  used  in the  fit is given by  tr(H) where  H is the
projection  or kat matrix.  If the  response  variables  Vi  are  iid with  variance  a2,  then

vea ow  = 

Var (h:  )

=  2 tr (H'H)

.tr  (H)
W2 

We  can  do  the  same  calculation  for a  linear  smoother  matrix  C, and  in  fact  for  the  local
straight lines amoother we even have  tr (CC) = tr (C).  As the span decreases,  the diagonal
entries of C gSt  larger,  and thus the variance  of the estimates  increases, as we would  expect.
One  can  also  approach  this from  the  other side  by  looking  at  the  residual  sum  of squares.
In  the  absence  of bias  we !i.v

• 

ZASS  - z 11(1  - OVIll,

ERSS= (-  C)'(u 

- c)

= tr[(I - C)'(cid:127),  - C)  cow(v)I 
= (n - tr(C))o,

(.

ore  motivation  for  regarding tr(C) as the  number  of parameters  or
if tr(CC) =  t(C).  • 
can  be found  in Cleyland  (1979)  and Tibehirani  (1984).  Same  calculations  similar  to
do(cid:127).. 
thoe  in 3.5.1  show that the expected squared  distance  of X from the  true f  is  D2   2s 2 , or
more precisely  V)  s  2or 2-0,4/(4p 2 ) where p is the radius of curvature,  which  in our example
is  I +  1/0 2 . Thus  D 2  va  0.18.  The  crows  validated  residual  estimate  E  CVIZSSi  was  found
to be 0.189.  The orthogonal  distance from the  fnal curve is  D'("1 ) = 0.162.  This is deflated
due 'to overfitting.  The average value of d.o.f for  the final curve  is (one for each  co-ordinate)
9.7, or, a total of 29.1.  Some simple heuristics show that the we should scale this value up by
by  2n/(2n - d.o.f)  = 300/(300  - 29.1)  =  1.11.  We  then  get  2n/(2n - d.o.f)D2 (11 )  =  0.179
which is. back  in  the  correct  ballpark.

It is  more  convenient  to view  the 3  dimensional  examples  on  a  color graphics  system
(such  as  the  Chromatics  system  of the  Orion  group,  Stanford  University).  This  allows one
to  rotate  the points  in  real  time  and  thus  see  the  3rd  dimension.

For  our smoothers, each  row  of C is the  row  of a  projection  matrix,  and  hence <e,  = C,--

Ckapter 6:  Exam ples  79

Example  6.3.  Geological  data.

This  real  data example  illustrates:

*  Data  modelling  in  3  dimensions,

* noo-linear  factor  analysis,  and

o  outlier  detection  and  robust  fitting.

The data in  this example. consists of measurements of the mineral content of 84 core samples,
each  taken  at different  depths  (Chernoff,  1973).  Measurements  were  made of  10  minerals
in each  sample.  We  simply  label  the  minerals  X 1 , -- X10, and  analyze  the  first  three.

Mineral 

X

Fiur 63 

Te 

ricpa 

crv 

orth 

mnra 

dta 

Vaial

X$ s.  ntothepag).Thespies 

ointhepontsto  hei  prjecio

onth 

crv. he4 

uties 

e  one 

t 

te 

ure 

it 

te 

roe

lineals.

Figure 6.3a  shows the data  and the solution  curve.  (A  final spa n  of 0.35  was manually
selected.)  In  3-D  the  picture  looks  like  a  dragon  with  its  tail  pointing  to  the  left  and  the

60  Ezmpie 6.S:  Geologica  data

SDm

LO

0

IsI

9.0

Depth  order  of  core

Figure  6.3b  The  values  AI(xi)  are  plotted  against  the  deptk
rder of  the  core  samples.

long  (outlier) spikes  could  be  a mane.  The  linear  principal component  explains  55% of the
variance,  whereas  this solution  explains  82%.

The spikes  join  the  observations  to  their  closest  projections  on  the  curve.  This  is  a

useful  device  for  spotting  outliers.  A  robust  version  of the  principal  curve  procedure  was

used  in  this  example.  After  the  first  iteration,  points  receive  a weight  which  is  inversly

proportional  to  their  distance  from  the  curve.  In  the  smoothing  step,  a  weighted  smooth

is  used,  and  if  the  weight  is  below  a  certain  threshhold,  it  is  set  to  0.  Four  points were

identified  as  outliers,  and  are  labelled  differently  in  figure  6.3a  . We  would  really consider

them model  ouItliars,  since in  that region of the  curve  the model  does  not appear  to fit  very
well.

Figure  6.3b  shows  the  relationship  between  the  order  of the points  oa  the curve,  and
the depth order  of the  core samples.  The curve  appears  to recover this variable  for the  most
nart.  The  area  where  it  does  not  recover  the  order  is  where  the  curve  appears  to  fit  the
data  badly  anyway.  So here  we  have uncovered  a hidden variable  or factor  that we are  able
to validate  with  the  additional  information  we  have  about  the  ordering.  The co-ordinate

S 

.

.. 

*.*. 

.

.

*... 

.

.

.

.

.. 

.

.

.

.

.

.

.. 

.

.

-... 

-.  F..*  .. 

.

.

.

.- 

.S 

,,, 

-,- 

,-:(cid:127)-(cid:127)

(cid:127), 

%.' 

'.(cid:127), 

.

Ckapter 6:  Examples  81

............................

00

00

4 

Estimated  X

I

a S 

-

0

4.'L'

S 
ba 

Fignur  6.3c  The estimated co-ordinate functions  or fator loadisng

Mcsrm 

for  the three  mmierals.

plots would  then  represent  the  mean level  of the particular  mineral  at different  depth.  (see
figure  6.3c).  Usually one would  have  to use these  co-ordinate  plots to  identity  the  factors,
just  as  one  us"  the  factor  loading$ in  the linear  case.
Example  6.4.  The  uniform  ball. 

.

This  example illustrates:

A  principal  surface in  3  space,  and

*a connection  to  multidimnensional  scaling.

The data is artificially  constructed,  with  no noise,  by  generating  points  uniformly  from  the
surface of a sphere.  It;is  the same data  used  by  Shepard  and  Carroll (196) 
to demonstrate
their parametric  mapping  algorithm.  (see reference  and  chapter  7).  We simply  use  it here
to  demonstrate  the  ability  of the  principal  surface  algorithm  to  produce  surfaces  that are
not a  function  of the starting  plane  (in  analogy  to  the circle  example  in  chapter  3).

There  are  61  data  points,  as  shown  in  figure  6.4a.  One  point  is  placed  at  each
intersection  of 5  equally  spaced parallels  and  12  equally  spaced meridians.  The  extra  point

S•  ,(cid:127)I(cid:127) 

(cid:127)
82 

Exsample  6-4:  Tke  unsiform  ball

Figure~~~~~ ~ 

~ 

~ 

6.aTedt*onsaepae 

iur 

.bTescn 

wano 

h

in~~~~~~ 

~  ~ 
~ ~ 

ntesraeo 

a  unfr 

p 

ee 

ricplsraepoeuefnsasraeta

*tw 

Figure  6.4c  Ahn 
ineratuifonsm  patrththeufc 

datarmediatestage  plached 
o  pee 

Figure  6.4d  The fnlseconde ptraoduofdthe
principal  surface proeuretindsasrf. 
ha

Chapter 6:  Examples  83

* 

A0

0

Figure  6.4.  Another view  of the  final prin- 
Cipal surface. 

Estimated  A,

Figure  6.Sf  The A map is a&two dimensional
summary  of  the  data. 
It  resembles  a  stereo.
graphic  map of the  world.

is  placed  at the  north pole.  (If we  placed  a point  at  the  south  pole  the  principal  surface
prccedure  would  never move  from  the  starting plane,  which  is  in  fact  a principal  surface.)

Figures  6.4b  to  6.4d  show  various  stages in  the iterative  procedure,  and figure  6.4e  sc.-is
another view  of the final  surface.  Figure  6.4f is  a parameter  map of the two dimensional  A.

It  resembles  a stereographic  map  of the earth.  (A stereographic  map  is obtained  by  placing
the  earth,  or  a model  thereof,  on  a  piece  of paper.,  Each  point  on  the  surface  is  mapped
onto  the paper  by extrapolating  the  line  segment joining  the  north  pole  to  the  point  until

it  reaches  the  paper.)  Points  in  the  southern  hemisphere  are  mapped  on  the  inside  of  a

circle,  points  in  the northern  hemisphere  on  the outside,  and  there  is a  discontinuity  at the

north  pole.  Points close  together  on this  map  are  close  together  in  the  original space,  but

the converse  is  not necessarily  true.  This  map  provides  a  two dimensional summary  of the
original  data.  If we  are presented  with  any. new  observations,  we. -an  easily  locate them  on
.the map  by  finding  their closest  position  on  the surface.

Example  6.5.  One  dimensional  color  data.

This  almost  real  data example  illustrates: 

•

°..-

84  EzmpU 

.6: A  Lipoprotein data

IAA

00

40

-6 

\ 

yell  -. 

.

o/- 

.

.u

/ 

.

.
SI 

.

' 

.. 

.. 

.

.

.

.

.

.

I 

! 

I

r 

principal  .i  "ated 

0 

50 

100 

150 

20 

MO

X  (wavelength)

Figure  6.5a  The .4  dimensional  color data 

Fi 

6.5b  The  estimated  €-ordinte".,-.-

COOS 

* 

*

6005 

prjce  onto the frtpi 
The principal curve,  found  in  the original four., 
space,  is  also  projected  onto this  plane. 

ciplcmponent pae-fuctions plot' .- ,d aint te arc length. of  the
principal  cu:--ze.  This  will be  monotone  with 
the  true waveleugtit. 

i

.""

r 

" 
A  principal  curves in 4-space,  and 
"  a one  dimensional  MDS  exeample. 

c 

dg"Te 

(vlngh
ec 

d

These data were  tsed by Shepard and Carroll  (1966)  (who cite the original source  as Boynton.

and Gordon  (1965))  to illustrate a version of their parametric data representation techniques
called  proximity  analysis.  We  give  more details  of this  technique  in  chapter  7.

Each  of the 23  observations  represents  a spectral  color  at a specific wavelength.  Each
observation  has 4 psychological  variables  associated  with  it.  They  are  the relative  frequen-

ciao with which  100 observers  named  the color  blue,  gy-een,  yellow  and red.  As can be seen  in
figure  6.5a4  there is very  little error in  this data,  and it is  one  dimensional by  construction.
Since  the  color  changes  Slowly  with  wavelength,  so  should  these  relative  frequencies,  and
they  should  thus fall  on a one  dimensional  curve,  as they  do.  The data, by construction  lies

in  a 3  dimensional  simplex  since  the  four variables  add  up  to  1.  The  pictures  we  show  are
projections  of this simplex  onto the  2-D  subspace  spanned  by  the  first two  linear  principal
components.  Figure  6.5a  shows  the  solution  curve  and  figure  6.5b  shows  the  recovered

parameters  and  co-ordinate  functions.  This  solution  is  in  qualitative  agree-nent  with  the
data  and  with  the solution  produced  by  Shepard  and  Carroll.

•..... 

.

...-..-.-.-...-..-......-..-.....

•...... 

.... 
.... 

: 

.....-.... 
,., 

-

Chapter 6: Ezamples  85

Example  6.6.  Lipoprotein  data.

This real  data example  illustrates:

& A  principal surface  in  3  space  with some  interpretations,

& a principal  curve  suggested  by  the  surface,  and

4  co;ordinate  plots  for surfaces.

and Krause (1982)  conducted a study to investigate the inter-relationships between
Wila;(cid:127)m 
the  serum  concentrations  of lipoproteins  at  varying  densities  in  sedentry  men.  We  focus
on  a subset  of the  data, and  consider  the  serum  concentrations  of LDL  3-4  (Low Density
Lipoprotein  with  flotation  rates  between  S3  - 4),  LDL  7-8,  and  HDL  3  (High  Density
Lipoprotein)  in  the  sample of 81  men.  Figures  6.6a-d  are  different  views  of the  principal
surface  found for  the data.  Quantitively  this surface explains  97.4% of the  variability in  the
data, and  accounts for 80% of the residual variance  unexplained  by the principal  component,
plane.  Qualitatively,  we  see  that  the  surface  has  interesting  structure  in  only  two  of  the
co-ordinates,  namely  LDL 3-4  and  LDL  7-8.  We  can  infer  from  the  the  surface  that  the
bow  shaped  relationship  between  these  two  variables  does  not change  for varying  levels of
HDL  3.  It exhibit.  an independent  behaviour.  We  have included  a  co-ordinate  plot  (figure
6.68)  of the  estimated  co-ordinate  function  for  the  variables  LDL  7-8  which  helps confirm
this  claim.  The relationship  between  LDL  7-8  and  (11,12)  depends  mainly  on the  level  of
.l.  Similar  information  is conveyed  by  the other  co-ordinate  plots,  or can  be seen from  the 
estimated surface  directly.  This suggests  a model of the  form

LDL 3-4 
7-8 
HDL3 

(fi(A) 
I. 2(
A (A 2) 

el
e3)

As  specified  A2 is confounded  with HDL  3,  and  is  thus unidentifiable.  We  need  to estimate.
the first  two  compoz.ents  of 'the model.  This  is  a  principal  curve  'model,  and  figure  6.6f
shows the estimated  curve.  It exhibit.  the same  dependence  between' LDL 7.8  and  LDL 3-4
as  did  the surface.  The  curve explains  92.6%  of the variance  in  the  two  variables,  whereas
the  principal  component  line explains  only  80%.

Williams  and  Krauss  performed  a  similar  analysis  look.., 

'at  pairs  of variables  at  a
time.  We  discuss  their techniques  in chapter  7.  Their' results  are  qualitatively  the same  as
ours for the  LDL  pair. 

.

"

"DL 

86  Ezample  6.6;  Lipoprotein dama

8C

40-

LM  3-4 

* 

0  L~~LDL  71-8 

20W
L.....iL; 

LL 3-ý4

0 

2530

0 

25 

504 

75 

100 

3 

150 

200 

00 

35

Figure  6.6a  The  principal  surface  for  the 
serum  concentrations  LDL  7-8,  LDL  3-4  and 
ED)L  3 in  a  sample  of  81  sedentary  men.  Van-. 
able  HDL  3 is  into  the  page.

Figure  6.6b  The  principal  surface  asin
'figure  6.6a from  .& different  viewpoint.  Variable
LDL 7-8  is  into  the  page.

NDL  3

0 

25 

50 

75 

100

Figure  6.6c  The  principal  surface  as  in 
figure  6.8a from  a different  viewpoint.  Variable 
LDL  3- 

is  into  the -page.

HD!.  3

Figure  6.6d  The  principal  surface  as  ini
figure  &6.6  from' a slightly  oblique  perspective.

601 

40 

20 

380

300-

103 

-8 

-

0

80

M 140

20

0

0 

-

L-i0

20 

40 

so 

Mý 

100

Figure  6.6e  The  estimated  co-ordinate 
function  for  LDL  748  versus 
has  little 
effect, 

1. 

Figure  6.6f  The  principal  curve  for  the
serum  concentrations  LDL  7-8  and  LDL 3-4  in
a sample  of 81  sedentry  men.

Chapter  7

Discussion  and  conclusions

In this  chapter we  discuss some of the existing  techniques  for symmetric  smoothing,  as well
as the  various  generalizations  of principal  components  and  factor  analysis.  We  compprre
these  techniques  with  the  methodolcgy  developed  here.  The  chapter  concludes  with  a
summary  of the uses  of principal  curves  and  surfaces.

7.1.  Alternative  techniques.

Other  non-linear  generalizations  of principal  components  exist in the  literature.  They  can
be broadly  classified  according  to two  di&otornies.

*  We can estimate either  the non-linear manifold or the non-linear constraint that defines

the manifold.  In  linear principal  components  the  approaches  are  equivalent.

*  The  non-linearity  can  be  achieved  by  transforming  the  space  or  by transforming  the

model.

The principal  curve and  surface  procedures  model the  hon-linear manifold  by transforming
the  model.

7.1.1.  Generalized  linear  principal  components.

This  approach  corresponds  to  modeling  either  the  nonlinear  constraint' or  the  manifold  by
transforming  the space.  The idea  here is to introduce some  extra variables,  where  each  new
variable  is  some  non-linear  transformation, of the  existing  co-ordinates.  One  then  seeks  a
subspace  of  this non  linear  co-ordinate  system  that  models  the  data. welL  The  subipace
is  found  by  using  the  usual  linear  eigenv'ctor  solution  in  th3  new  enlarged  space.  This
technique  was first suggested  by  Gnanadesikan & Wilk  (1966,  1968),  and a good  description
can  be found  in  Gnanadesikan  (1977).  They  suggested  using  polynomial  functions' of the
original p  co-ordinates.  The  resulting  linear combinations  are  then of the  form  (for  p  (cid:127)  2
and quadratic  polynomials)

A=  alixI +  a2,x2  +  a3izl 2 + 

24 

2 

(7.1) 

"";

Chapter  7:  Diacuasion and  conclusion. 

89

and  the  iýj  wfll  be  eigenvectors  of the appropriate  covariance  matrix.

This  model  has  appeal  mainly  as  a  dimension  reducing  tool.  Typically  the  linear,
combination  with  the smallest  variance  is set  to zero.  This  results in  an  implicit  non-linear
constraint  equation  as  in  (7.1)  where  we set  A =  0.  We  then  have  a rank  one  reduction
that tells us that  the data  lies  close  to  a quadratic  manifold  in  the  original co-ordinates.

The  model  has  been  generalized  farther  to includo'  more  general  transformations  of
the  co-ordinates  other  than  quadratic,  but  the idea  is ementially  the same  as the  above;  a
linear solution is found in a transformed space.  Young, Takane  & de Leeuw  (1978)  and later

Friedman  (1983)  suggested  different  forms of this  generalization  to include  non-parametric
transormations  of the co-ordinates.  The  problem can  be formulated  as follows:  Find  a and

,'(S)=((s(.z),."",(',)S 

)  such  that

or alternatively  such that

X Il(x) -

-&',(x)II'=  mini 

V,-[,',(z)j  = max! 

(7.2)

(7.3)

i(z,).=0,d=  la 1and  Z s2-. (zi 

where 
,,itably and  then find the linea  pr ncipal  components. 
them  we  would  be estimating  the  constraint  in  the  tmnaEormed  space.

1.  The. idea  is  to  trandorm  the  coordinates
f in  (7.3)  we  replaced  max by  min

The  estimation  procedure  alternates  between 

,g 

the  aj(-)  and finding  the  linear

principal  components  in the  transformed  space

*  For a  fixed  vector  c4 function 

,( 
the cu'ariance matrix  Zs(s))e,(cid:127):).

-. "howe  a  to  be the  first principal  component  of

*For  a  known,  (7.2)  can  be w 

tn 

in the form

,h Zj,,(X,)  - D 

P'

'US..'"A -+ 

,, r.v. 

in f(.),......,000, 

(7.4)

and Oi,  we  functions  a(,@  abo 
by

. If. 2 ...

,. av known,. equaio 

(7.4)  is  minimised

*I ,(,)  Z(  b(cid:127)e,, 

(,j) XIx)

is  tMe  for  any  ej,  and suggests  an inner  iterative  loop.  This  inaer  loop  is  very
Th 
similar to the ACE algorithm (Breiman  and Friedman,  1982),  except the normalization

S,.

a

6

90 

Section 7.1:  Alternative teckniqusa

in slightly  different.  Breiman  and Friedman  proved  that  the ACE  algorithm  converges
under  certain  regularity  conditions  in  the  distributional  case.

The  disadvantages  of this techniqu.  awe:

7%Th space  is  transformed,  and  in  order  to  understand -the  resultant  fit,  we  usually
would  need  to  transform  back  to  the  ori-ial apace.  This  can  only  be  achieved  if the
transformations  awe  resficted -to monotone  functions.  In  the  transformed  apace  the
estimated  manifold  is  given  by

Thus if the  .,(.  are monotone,  we  get  untransformed  estimates  of the form'

:;: (x:))(7.5)

where  xa =  .(x). 

rquatiom 

(7.5),  defines  a parametrized  -curve.  The  curve  is  not
copltely  general  since the co-ordinate  functions  are monotone.  Far the same  reason,
for-estimating

Guanadesian  (1978)  expressed  the  desirability, of  having  procedure 
models  ofi the  typ  proposed  in  this  dissertation.

e  We  are estimating manifold  that are  lones  to the data in the transformed  co-ordiznates.
When  the 
non-linear  this  can  result  in  distortion  of the  error
variances  for  individual  variables.  What  'we  really  require  is a method  for, estimating
manifoids  that  are  clones  to  the  data  in  the  original  p  co-ordinatesL  Of course,  if the,
Aunctions are liser, both  approrice  are  identical.

wr~ratceae 

An. advantage  of  the  technique  is  that  it  can  eassily  be  gmneralised to  tak.  care of  higher
dimensional  manifolds,  altkcogh  not  in  an  entirely  general  fashion.  This  is  achieved  by
replacing awith Awhom  Ainp x q.  We  then  got aqIdimesnsionaal  hyperplan*in  the
transformed  space  given  by  AA'.*(xj.  However,  we ead  up  with  a asnsbow  of  implicit
cntraip., equations  which  are hard  to  deal  with  and  interpret.  Despite  the  problems
ainsociated  with  generalized  principal  components,  it remains  a useful  tool  for performing
rank I dimmnsionality  reductions.

Chapter 7:  Discusuion and conclusions  91

7.1.2.  M(cid:127)ulti-dimensional  scaling.

This  is a technique  for  finding  a low  dimensional  representation  of high  dimensional  data.
The original proposal was for data that consists of (2)  dissimilarities  or distances between  n-
objects.  The idea is to find a m (m small,  1, 2  or 3)  dimensional  euclidean representation  for
the objects  such  that  the inter-object  distances  are preserved  as well  as possible.  The  idea
was  introduced  by  Torgerson  (1958),  and  followed  up  by  Shepard  (1962),  Kruskal  (1964a
,1964b),  Shepard  &  Kruskal  (1964)  and  Shepard  &  Carroll  (1966).  Gnanadcsikan  (1978)
gives  a concise  description.

The procedures  have  also been  suggested  for situations where  we  simply  want a lower
lower  dimensional
dimensional  representation  of  high  dimensional  euclidean  data.  'The 
representation  attempts  to reproduce  the  inteepoint  distances  in  the  originai  space.  We
fit a principal  curve  to the color  data  in example  6.5;  these  data were  originally  analyzed
by  Shepard  and  Carroll  (1968)  using  MDS  techniques.  Although  there  have  been  some
intriguing exampk.- of the technique  in the  literature, a  number of problems  exist. 

e  The solution  consists of a vector of m co-ordinates  representing  the location  of points,
on  the low  dimensional  manifold,  but  only  for the  n  data point&. What  we don't  get,
and often  desire is a mapping  of the whole  space.  We  are  unable, for example,  w find
the location  of new  poaints  in  the reduced space.

SThe  procedures  are  computatiomally  expensive  and  unfeasible  for  large  n  (nm >  300
is considered  large).  They  awe  usually  expressed  as  non-linear  optimization  problems
in am parameters,  and differ in the choice  of criterion. 

partially  overcome  both  the  problems  listed
The  princpal  curve  and  surface  proced 
above; they  ae unable  to Bad structures  as  general  as those that  can  be found  by  the MDS
procedures  due  to the  averaging  nature  of  he  scatterplot  smoothers,  but  they  do  provide
""usapping  for  the space.  We  have demonsrated  their  ability  to  model  MDS  type  data  in
emmples  6.4 and 6.5.  They do not,ý  howeve,  provide  a model  for dissimilarities  which was
the oiginal  intention  of multidimensional 

aling.

7.L3.  Poximity models.

Shepard  &  Carroll  (1966)  suggested  a 
ugget  They  required  onlay  to esimat  the 
coumidered  the  data  to  be  functions  t 

ional  model  similar  in  form  to  the  model  we
vectors of m  parameters  for each. point,  and
The  parameters  (aim  altogether)  wre  found

:(cid:127) 

.

• 

-' 

.* 

, 

' 

.,

p

I-

I-

VO 

Section 7.1:  Altemative  techniques

by  direct  search  as  in bMS,, with  a different  criterion  to  be  minimized.  Their  procedure,

however,  was  geared  towards  data without  error,  as  in  the  ball  data  in  example  6.4.  This

becomes evident  when one  examines  the criterion they  used,  which  measures  the continuity

of the data as  a function of the parameters.  When  the data is  not smooth, as is usually  the

cae, we  need  to  estimate functions  that vary  smoothly  wit.h  the  parameters,  and are  close

to the  data.

7.1.4.  Non-linear  factor  analysis.

More  recently, Etesadi-Amoli  and  McDonald  (1983)  approached  the problem of non-linear

factor  analysis  using polynomial  functions.  They use  a  model of the form

X  =(A) 

+*

whei.  f  is  a polynomial  in  the unknown  parameters  or factors.  Their  procedure  for  esti-
maktng  the  unknown  factors  and  coefficients  is  similar  to  ours  in this restricted  setting.  •

Their emphasis  is on the factor  analysis  model,  and once the appropriate  polynomia  terms
have  been  found,  the problem  is  treated  as  an  enlarged  factor analysis problem.  They  do
not  estimae  the  A's  as  we  do, using  the  geometry  of the  problem,  but instead  perform  a
search  in nq  parameter  space,  where  q  is  the dimension  of A  and nt  is  the number of obser-
vstions.  Our  emphasis  is on  providing one  and  two dimensional smmroies  of the data.  In
certain situations,  the..  summarie  can  be  used  as estimates  of the appropriate  non-linear
functiocal  and  factor model."

7.L.5.  Axis  interchangeable  smoothing.

Cleveland  (1983)  describes a technique  for symmetrically  smoothing a scatterplot  which  he
thins  (  which  we will  refer  to  as  AI smoothing).  We  briefly
calls  azis  itevcangesiki  e. 

outlin 

the  idea:

*  standardiseeach  coordinate  by  some  (robust) measure  of scale.

*  rotate  the  coordinate  axes  by  45".  (if the  correlation  is  positive, else  rotate  through

*  smooth  the  transformed  V against the  transformed  z."

0  Their  pape 

ýws  pblihed  in  the  September,  1983  issue  of  Puychometrika,  whereas  Ha"tie

(1963)  appeared  in  Ju.

4ý3.

Chapter 7:  Discussion and  conclusions  93

e  rotate  the  axes  back.

e  unatandardize.

If the standardization  uses regular standard deviations,  then  the rotation is simply  a change

of basis  to the  principal component  basis.  The  revulking curve  minimizes  the distance  from

the points orthogonal to this principal component.  It  has intuitive appeal since the principal

component  is  the line  that  is  closest  in  distance  to  the  points.  We  then  allow  the  points

to  tug in the principal  component  line.  It is  simple  and  fast  to  compute  the Al  Smooth,

and  for  many  scatterplots  it  produces  curves  that  are  very  similar  to the  principal  curve

solution.  This  is not surprising  when w-  consider  the following  theorem:

Theorem  7.1

If the  two  variables  in  a  scatterplot  are  standardized  to  have  unit  standard  deviations,

and  if  the  smoother  used  is  linear  and  reproduces  straight  lines  exactly,  then  the  axis

interchangeable  smooth  is identical  to the curve  of the  first iteration of the  principal  curve

procedure.

Proof

Let  the  variables  s  and  V be  standardized  as  above.  The  AI Smooth  transforms  to  two

new variables

0" 

.(7.6)

Then  the Al  Smooth  rep1aces  (x,  p*)  by (z*,  Smooth(y"  Ix*)).  But  Smooth(z  I:') =
3*  since the  smoother  reproduces  straight  lines exactly.*  Thus  the  Al  Smooth  transforms
back  to

(Smooth(i"  I  ') +  Smooth(V"  x-)

V(cid:127) 

' 

(Smooth  (z 

)  - Smooth(y"  Iii) 

Smnc 

the  smoothe,  is linear,  and in vis*  of  (7.6), 

(7.7)  becomes

A =  Smooth (  Ii')
S=  smnooth(vi:) 

(7.7)"
(7

(7.8)

Any  weighted  local lsear  smoother  has  this property.  Local  averages,  however,  do not  unlems
the predictors  m wvenly  spaced.

94 

Section  7.2:  Conclusiort.

This is exactly the curve found  after the first iteration of the principal curve procedure,

sice (cid:127))(o  = 

..

Williams  and  Krauas  (1982)  extended  the  AI  smooth  by  iterating  the  procedure.  At
the  second  step,  the residuals  are  calculated  locally  by  finding  the tangent  to  the  curve  at
each  point  and  evaluating  the  residuals  from  these  tangents.  The new  fit at that point  is

the  smooth  of thes  residuals  against  their, projection  onto  the  tangent.  This procedure

would  probably  get  closer  to  the  principal  curve  solution  than  the  Al  smooth  (we have

not  implemented  the  Williams  and  Krauss  smooth).  Analytically  one  can  see  that  the

procedures  differ  from the second  step on.

This particular  approach  to symmetric  smoothing  (in  terms of residuals)  suffers  from

several deficiencies:

*  the type of curves that can  be found  are not as  general  as those  found by the principa!

curve  procedure.

*  they are designed  for wAtterplots  and  do not generalize  to curves in higher dimensions.

*  they  lack  the  interpretation  of principal  curves  as a form of conditional, expectation.

2.2.  Conclusions.

In  conclusion  we summarz 
analyis.

the  role  of principal  curves  and surfaces  in statistics  and  data

*  They  generalize  the one and  two dimensional  summaries  of multivariate  data usually

provided  by  the  principal  components.

o  When, the  principal  curves  and  surface  are linear,  they  are  the  principal  component

summaries.

*  Locally  týhy  are the critical points of  the  usual distance  function  for such  summaries;

ths gives  an indication  that there  are  not too  many  of them.

e  They  are defined  in terms of conditional  expectations  which satisfies our  mental image

of  a summary.

*  They  provide  the  least  squares  estimate  for  generalized  versions  of factor  analysis,
functional  models  and the errors  in  variables regression  models.  The  non-linear errors

• 

,. 

... 

.. 

.o 
.

.. 
.... 

.

.... 

,... 

: 

.-. 

.

(cid:127) 

.

.

.

. .... 

-.

,i.. 

.

.. 

.

.2. 

.. 

.

.

.. 

..-.

'

Chapter 7:  Discussion and  conclusions  95 

7

in  variables  model  has  been  used  successfully  a number  of  times  in  practical  data

analysis problems  (notably calibration  problems).

*  In some situations they are a useful alternative  to MDS  techniques, in that they provide

a lower  dimensional summary  of the  space  as  opposed  to  the  data  set.

o  In  some  situations  they  can  be effective  in  identifying  outliers  in  higher  dimensional

space.

0  They  are  a  useful  data  exploratory  tool.  Motion  graphice  6echniques  have  be ;ome
popular  for  looking  at 3  dimensional  point  clouds.  Experience  shows  that it  is  often

impossible  to identify  certain  structures  in  the  data by  simply  rotating  the points.' A
summary such  as that given by the principal  curve and surfaces  can identify structures
that woeld  otherwise  be transparent,  even  if the data  could be  viewed  in a  real  three

dimensional  model.

Acknowledgements

My  great  appreciation  goes  to  my  advisor  Werner  Stuetzle,  who  guided  me  through  all
stages  of this  project.  I  also  thank  Werner  and  Andreas  Buja  for  suggesting  the problem,
and Andrew for many  helpful discussions.  Rob Tibehirani helped  me a great deal, and some 
of the original ideas emerged  whilst we were  suntanning alongside  a river in the  Californian 

ramantains.  Brad  Ehfon,  as usual,  provided  many  insightful  comments.  Thanks  to Jerome
Friadm,-  for  his  idea  ond  constant  support.  In  addition  I  thank  Persi  Diaconis  and  lain
Johnstone  for  their  help  and  comments,  and  Roger  Chaffee 'and  Dave  Parker  for  their
computer  assi  snce.  Finallj  I  thank  the  trustees  of  the  Queen  Victoria,  the  Sir  Robert
Kowk  and  the  Sir Harry  Crossley  scholarships  for their  generous assistence.

_

-

Bibliography

Anderson,  T.W.  (1982),  Estimating  Linear  Structural  Relationships,  Technical  Report  #
399,  Institute  for  Mathematical  studies  in  the  Social  Sciences,  Stanford  University,
California.

Barnett,  V.  (Ed)  (1981),  Interpreting  Multivariate  Data,  Wiley,  Chichester..

Becker,  ILA.  and  Chambers,  J.M.  (1984),  S:  An  Interactive  Environment for Data Analysis

and  Graphics,  Wadsworth,  California.

Boynton,  R.M.  and Gordon,  J.  (1965),  Bezold-Brtike  Hue Shift  Measured  by  Color-Naming

Technique, J.  Opt.  Soc.  Amer,  55  ,  78-86.

Breiman,  L.  and  Friedman,  J.H.  (1982),  Estimating  Optimal  Transformation.  for Multiple
Regrission  and  Correlation,  Dept.  of  Statistics  Tech.  Rept,  Orion  16,  Stanford

University.

Chernoff,  H.  (1973),  The use  of Face.  to Represent Point. in k-dimensional Space Graphi-

cally,  Journal  of the American  Statistical  Association,  68,  #342,  361-368. 

Chung,  K.L.  (1974),  A  Course in Probability Theory, Academic  Press,  ,New York.

Cleveland,  WS.  (1979),  Robust  Locally  Weighted Regression and Smoothing  Scatterplots.

Journal of the  American  Statistical  Association,  74,  829-836.

Cleveland,  W.S.  (1983),  The  Many  Faces of a Scatterplot,  Submitted for  publication.

Craven, P. and Wahba,  G.  (1979),  Smoothing Noisy  Data with Splint Functions:  Estimating

the Correct Degree of Smoothing by the Method of Generalized Cross-validation,  Numer. 

MatL.,  31,  377-403.

do Carmo,  M.P.  (1976),  Differential Geometry of  Curves  and Surfaces, Prentice  Hall,  New

Jer"y.

Etezadi-Amoli,  J.  and McDonald,  R.P. (1983),  A  Second Generation Nonlinear Factor Anal-

ysis,  Psychometriks,  48,  #3,  315-342.

Efron,  B. (1981),  Non-prametric Standard Errors and Confidence Intervas,  Canadian Jour.

nal, of Statistics, 9,  139.1.72.

-

-

Bibliography  97T

Efron,  B.  (1982),  The  Jacknife, the  Bootstrap and other  Resampling  Plans,  SIAM-CBMS,

38.

Efron, S. (1984),  Bootstrap Confidence Intervals for Parametric' Problems, Technical Report

#90,  Division of Biostatistics,  Stanford  University.

Friedman,  J.H.  (1983),  Personal  communication.

Friedman,  J.H,  3ently, J.L. and Finkel,  R.L  (1976),  An  Algorithm for Finding Best Matches

in  Logarithmic Ezpected  Time.  STAN-CS-75-482,  Stanford  University.

Friedman,  J.H.  and  Stuetzle,  W.  (1982),  Smoothing  of 'Scatterplots,  Dept.  of Statistics

Tech.  Rept.  Orion 3,  Stanford  Univerrity.

Gaser,  Th.  and  Muller,  H.G.  (1979),  Kernel  Estimation  of  Regression Functiont,  in
Smoothing Techniques for Curve Estimation, Proceedings,  Heidelberg,  Springer  Ver-

lag.

Gnanadesikan,  R.  (1977),  Methods for Statistical Data Analysis  of  Multivariate  Observa-

tions, Wiley, New  York.

Gnanadesikan,  R.  and Wilk,  M.B.  (1969),  Data Analytic  Method, in Multivariate Statistical
Ataldis, in  Multivariate  Analysis  11  (P.R.  Krishnaiah,  ed.),  Academic  Press,  New
York. 

Golub, G.H.  and  Reinsch,  C.  (1970),  Singular Value Decomposition and Least Square* So.-

lutions, Numer.  Math.  14, 403-420

Golub,  G.E.  anad  van  Loan,  C.  (1979),  Total Least  Squares, in  Smorthing  Techniques for

Curve I&?timation, Proceedings,  Heidelberg,  Springer  Verlag.

Greenacre,  M.  (1984),  Theory  and Applications  of  Correspondence 

.Aadysi, Academic

Press,  London.

Hastie,. TJ.  (1983),.Principal(cid:127)  Curves,  Dept.  of Statistics  Tech.  Rept.  Orion  24,  Stanford

University,

Hastie,  T.J.  and  Stuetzle,  W.  (1984),  Principal  Curves  and  Surfaces,  '(Motion  Graphics

Movie),  Dept.  of Statistics,  Stanford  University.

Hotelling,  I.  (1933),  Analysis  of a  Complez  of Statistical Variables into Principal Compo."

ne(cid:127)

(cid:127),s 

J.  Educ.  Pyh., 

24,  417-441,  498-520. 

"

-

98  Bibliorphy

Kendali,  M.G. and  Stuart,'A. (1961),  The Advanced Theory of Statistica, Volume  2,  Hafner,

New  York.

Kruskal,  J.B.  (19%4&),  Multidimensional  Scaling by  Optimizing Goodness of Fit to a Non-

metric Hypothesis, Psychometrika,  29,  #1,  1-27.

Kruskal,  J.B.  (1964b),  Nonmetric  Multidimensional  Scaling:  a  Numerical  Method,  Psy-

chometrika,  29,  #2,  115-129.

Lindley,  D.V.  (1947),  Regression Lines  and  the Linear  Functional Relationship,  Journal  of.

the  Royal  Statistical  Society,  Supplement,  9, 219-244.

Madansky, A.  (1959),  The Fitting of Straight Lines when both  Variable. are Subject to Error,

Journal  of the  American  Statistical  Society,  54, 173-205.

Mosteiler,  F.  and  Takey,  J.  (1977),  Data Analysis and Regression,  Addison  Wesley,  Mas-

sachusetta.

Reinach,  C.  (1967),  Smoothing by  Spline Functions, Numer.  Math.,  10, 177-183.

Shepard,  R.N.  (1962),  The  Analysis  of Proximities: Multidimensional  Scaling with  an  Un-

known  Distance Function,  Psychometrika,  27,  123-139,  219-246. 

Shepard,  R.N.  and  Carrol,  J.D.  (1966),  Parametric  Representations of Non-  Linear Data

Structures, in  Multivariate  Analysis  (Krishnaiah,  P.R.ed),  Academic  Press, NewYork.

Shepard,  R.N.  and  Kruskal,  J.B.  :1934),  Non-metric  Methods for  Scaling and for  Factor 

Analysis, Amer.  Psychologist  19,  557-558.

Spearman,  C. (1904),  General  late  igence, :Objectively  determined  and Measures,  American

Journal  of Psychology,  15,  20  -293.

Stone,  M.  (1977),  An  Asymptotic  Aoice  of Model  by  Cross-validation and Akaike's  Crite-

rion, Roy.  Stat.  Soc.  B,  7, 

7.

Thorpe,  J.A.  (1978),  Elementary  Topics  in  Differential  Geometry,  Springer-Verlag,  New

York.  Undergraduate  Text  in Mathematics.

Tibehirani,  R.J.  (1984),  Bootstrap  ",onfidence Intervalb,  Technical  Report  #91,  Division  of

Biostatistics,  Stanford  Univer  ity.

Torgeson,  W.S.  (1958),  Theory' an  Methods of Scaling, Wiley, New  York.

Wilkinson,  J.H.  (1965),  The  Algebic Eigenvalue  Problem,  Clarendon  Proes,  Oxford.

-

-

Bibliography  99

Waliba,  G.  and  Wold,  S.  (1975),  A  Completely  Automatic  French Curve:  Fitting Spline

Functions by  Crose-validation, Comm.  Statistics,  4,  1-73.

Williams,  P.T.  and  K-ause,  R.M.  (1982),  Graphical Analysis of the  Sectional Int.-rrelation-
ships  among Subfractiona of  S~erumn  Lip. protein. in  Middle  Aged  Men,  unpublished
manuscript,  Stanford  University.-

Young,  F.W, Takane,  Y, and de Leuuw, J'.  (1978),  The Principal Components a !' Mixed Mea-
suremest Level Multivariate Data: ain Alternating Least Square8'Method with  Optimal
Scaling Features, Psychometrika,  43,  no.2.

