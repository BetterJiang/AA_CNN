The Annals of Statistics
2007, Vol. 35, No. 5, 2173–2192
DOI: 10.1214/009053607000000127
© Institute of Mathematical Statistics, 2007

ON THE “DEGREES OF FREEDOM” OF THE LASSO

BY HUI ZOU, TREVOR HASTIE AND ROBERT TIBSHIRANI

University of Minnesota, Stanford University and Stanford University

We study the effective degrees of freedom of the lasso in the framework
of Stein’s unbiased risk estimation (SURE). We show that the number of
nonzero coefﬁcients is an unbiased estimate for the degrees of freedom of
the lasso—a conclusion that requires no special assumption on the predic-
tors. In addition, the unbiased estimator is shown to be asymptotically consis-
tent. With these results on hand, various model selection criteria—Cp, AIC
and BIC—are available, which, along with the LARS algorithm, provide a
principled and efﬁcient approach to obtaining the optimal lasso ﬁt with the
computational effort of a single ordinary least-squares ﬁt.

(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)y − p(cid:2)

j=1

xj βj

ˆβ = arg min

β

(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)2 + λ

p(cid:2)

j=1

|βj|,

1. Introduction. The lasso is a popular model building technique that simul-
taneously produces accurate and parsimonious models (Tibshirani [22]). Suppose
y = (y1, . . . , yn)T is the response vector and xj = (x1j , . . . , xnj )T , j = 1, . . . , p,
are the linearly independent predictors. Let X = [x1, . . . , xp] be the predictor ma-
trix. Assume the data are standardized. The lasso estimates for the coefﬁcients of
a linear model are obtained by

(1.1)

where λ is called the lasso regularization parameter. What we show in this paper is

that the number of nonzero components of ˆβ is an exact unbiased estimate of the

degrees of freedom of the lasso, and this result can be used to construct adaptive
model selection criteria for efﬁciently selecting the optimal lasso ﬁt.

Degrees of freedom is a familiar phrase for many statisticians. In linear regres-
sion the degrees of freedom is the number of estimated predictors. Degrees of
freedom is often used to quantify the model complexity of a statistical modeling
procedure (Hastie and Tibshirani [10]). However, generally speaking, there is no
exact correspondence between the degrees of freedom and the number of para-
∗ such that
meters in the model (Ye [24]). For example, suppose we ﬁrst ﬁnd xj
| cor(xj
∗ , y)| is the largest among all xj , j = 1, 2, . . . , p. We then use xj
∗ to ﬁt a
simple linear regression model to predict y. There is one parameter in the ﬁtted

Received December 2004; revised November 2006.
AMS 2000 subject classiﬁcations. 62J05, 62J07, 90C46.
Key words and phrases. Degrees of freedom, LARS algorithm, lasso, model selection, SURE,

unbiased estimate.

2173

2174

H. ZOU, T. HASTIE AND R. TIBSHIRANI

model, but the degrees of freedom is greater than one, because we have to take
into account the stochastic search of xj

∗.

Stein’s unbiased risk estimation (SURE) theory (Stein [21]) gives a rigorous
deﬁnition of the degrees of freedom for any ﬁtting procedure. Given a model ﬁtting
method δ, let ˆµ = δ(y) represent its ﬁt. We assume that given the x’s, y is generated
according to y ∼ (µ, σ 2I), where µ is the true mean vector and σ 2 is the common
variance. It is shown (Efron [4]) that the degrees of freedom of δ is

df (ˆµ) = n(cid:2)

cov( ˆµi , yi )/σ 2.

(1.2)
For example, if δ is a linear smoother, that is, ˆµ = Sy for some matrix S indepen-
dent of y, then we have cov(ˆµ, y) = σ 2S, df (ˆµ) = tr(S). SURE theory also reveals
the statistical importance of the degrees of freedom. With df deﬁned in (1.2), we
can employ the covariance penalty method to construct a Cp-type statistic as

i=1

(1.3)

Cp(ˆµ) = (cid:3)y − ˆµ(cid:3)2

+ 2df (ˆµ)

σ 2.

n

n

Efron [4] showed that Cp is an unbiased estimator of the true prediction error, and
in some settings it offers substantially better accuracy than cross-validation and re-
lated nonparametric methods. Thus degrees of freedom plays an important role in
model assessment and selection. Donoho and Johnstone [3] used the SURE theory
to derive the degrees of freedom of soft thresholding and showed that it leads to
an adaptive wavelet shrinkage procedure called SureShrink. Ye [24] and Shen and
Ye [20] showed that the degrees of freedom can capture the inherent uncertainty
in modeling and frequentist model selection. Shen and Ye [20] and Shen, Huang
and Ye [19] further proved that the degrees of freedom provides an adaptive model
selection criterion that performs better than the ﬁxed-penalty model selection cri-
teria.

The lasso is a regularization method which does automatic variable selection. As
shown in Figure 1 (the left panel), the lasso continuously shrinks the coefﬁcients
toward zero as λ increases; and some coefﬁcients are shrunk to exactly zero if λ is
sufﬁciently large. Continuous shrinkage also often improves the prediction accu-
racy due to the bias–variance trade-off. Detailed discussions on variable selection
via penalization are given in Fan and Li [6], Fan and Peng [8] and Fan and Li [7].
In recent years the lasso has attracted a lot of attention in both the statistics and
machine learning communities. It is of great interest to know the degrees of free-
dom of the lasso for any given regularization parameter λ for selecting the optimal
lasso model. However, it is difﬁcult to derive the analytical expression of the de-
grees of freedom of many nonlinear modeling procedures, including the lasso. To
overcome the analytical difﬁculty, Ye [24] and Shen and Ye [20] proposed using a
data-perturbation technique to numerically compute an (approximately) unbiased
estimate for df (ˆµ) when the analytical form of ˆµ is unavailable. The bootstrap

DEGREES OF FREEDOM OF THE LASSO

2175

FIG. 1. Diabetes data with ten predictors. The left panel shows the lasso coefﬁcient estimates

ˆβj , j = 1, 2, . . . , 10, for the diabetes study. The lasso coefﬁcient estimates are piece-wise linear
functions of λ (Osborne, Presnell and Turlach [15] and Efron, Hastie, Johnstone and Tibshirani [5]),
hence they are piece-wise nonlinear as functions of log(1 + λ). The right panel shows the curve of
the proposed unbiased estimate for the degrees of freedom of the lasso.

(Efron [4]) can also be used to obtain an (approximately) unbiased estimator of
the degrees of freedom. This kind of approach, however, can be computationally
expensive. It is an interesting problem of both theoretical and practical importance
to derive rigorous analytical results on the degrees of freedom of the lasso.

In this work we study the degrees of freedom of the lasso in the framework
of SURE. We show that for any given λ the number of nonzero predictors in the
model is an unbiased estimate for the degrees of freedom. This is a ﬁnite-sample
exact result and the result holds as long as the predictor matrix is a full rank matrix.
The importance of the exact ﬁnite-sample unbiasedness is emphasized in Efron [4],
Shen and Ye [20] and Shen and Huang [18]. We show that the unbiased estimator is
also consistent. As an illustration, the right panel in Figure 1 displays the unbiased
estimate for the degrees of freedom as a function of λ for the diabetes data (with
ten predictors).

The unbiased estimate of the degrees of freedom can be used to construct Cp
and BIC type model selection criteria. The Cp (or BIC) curve is easily obtained
once the lasso solution paths are computed by the LARS algorithm (Efron, Hastie,
Johnstone and Tibshirani [5]). Therefore, with the computational effort of a single
OLS ﬁt, we are able to ﬁnd the optimal lasso ﬁt using our theoretical results. Note
that Cp is a ﬁnite-sample result and relies on its unbiasedness for prediction error

2176

H. ZOU, T. HASTIE AND R. TIBSHIRANI

FIG. 2. The diabetes data: Cp and BIC curves with ten (top) and 64 (bottom) predictors. In the top
panel Cp and BIC select the same model with seven nonzero coefﬁcients. In the bottom panel, Cp
selects a model with 15 nonzero coefﬁcients and BIC selects a model with 11 nonzero coefﬁcients.

as a basis for model selection (Shen and Ye [20], Efron [4]). For this purpose,
an unbiased estimate of the degrees of freedom is sufﬁcient. We illustrate the use
of Cp and BIC on the diabetes data in Figure 2, where the selected models are
indicated by the broken vertical lines.

DEGREES OF FREEDOM OF THE LASSO

2177

The rest of the paper is organized as follows. We present the main results in
Section 2. We construct model selection criteria—Cp or BIC—using the degrees
of freedom. In Section 3 we discuss the conjecture raised in [5]. Section 4 contains
some technical proofs. Discussion is in Section 5.

2. Main results. We ﬁrst deﬁne some notation. Let ˆµλ be the lasso ﬁt using
the representation (1.1). ˆµi is the ith component of ˆµ. For convenience, we let
df (λ) stand for df (ˆµλ), the degrees of freedom of the lasso. Suppose M is a matrix
with p columns. Let S be a subset of the indices {1, 2, . . . , p}. Denote by MS the
submatrix MS = [··· Mj ···]j∈S, where Mj is the j th column of M. Similarly,
deﬁne βS = (··· βj ···)j∈S for any vector β of length p. Let Sgn(·) be the sign
function: Sgn(x) = 1 if x > 0; Sgn(x) = 0 if x = 0; Sgn(x) = −1 if x = −1. Let
B = {j : Sgn(β)j (cid:5)= 0} be the active set of β, where Sgn(β) is the sign vector of β
given by Sgn(β)j = Sgn(βj ). We denote the active set of ˆβ(λ) as B(λ) and the
corresponding sign vector Sgn( ˆβ(λ)) as Sgn(λ). We do not distinguish between

the index of a predictor and the predictor itself.

2.1. The unbiased estimator of df (λ). Before delving into the technical de-
tails, let us review some characteristics of the lasso solution (Efron et al. [5]). For
a given response vector y, there is a ﬁnite sequence of λ’s,
λ0 > λ1 > λ2 > ··· > λK = 0,

(2.1)

such that:
• For all λ > λ0, ˆβ(λ) = 0.
• In the interior of the interval (λm+1, λm), the active set B(λ) and the sign vector
Sgn(λ)B(λ) are constant with respect to λ. Thus we write them as Bm and Sgnm
for convenience.
The active set changes at each λm. When λ decreases from λ = λm − 0, some
predictors with zero coefﬁcients at λm are about to have nonzero coefﬁcients; thus
they join the active set Bm. However, as λ approaches λm+1 + 0 there are possibly
some predictors in Bm whose coefﬁcients reach zero. Hence we call {λm} the
transition points. Any λ ∈ [0,∞) \ {λm} is called a nontransition point.
THEOREM 1. ∀λ the lasso ﬁt ˆµλ(y) is a uniformly Lipschitz function on y. The
degrees of freedom of ˆµλ(y) equal the expectation of the effective set Bλ, that is,

df (λ) = E|Bλ|.

(2.2)
The identity (2.2) holds as long as X is full rank, that is, rank(X) = p.
Theorem 1 shows that (cid:3)
(cid:3)

df (λ) = |Bλ| is an unbiased estimate for df (λ). Thus
df (λ) sufﬁces to provide an exact unbiased estimate to the true prediction risk of

2178

H. ZOU, T. HASTIE AND R. TIBSHIRANI

the lasso. The importance of the exact ﬁnite-sample unbiasedness is emphasized
in Efron [4], Shen and Ye [20] and Shen and Huang [18]. Our result is also com-
(cid:3)
putationally friendly. Given any data set, the entire solution paths of the lasso are
computed by the LARS algorithm (Efron et al. [5]); then the unbiased estimator
df (λ) = |Bλ| is easily obtained without any extra effort.

To prove Theorem 1 we shall proceed by proving a series of lemmas whose

proofs are relegated to Section 4 for the sake of presentation.

LEMMA 1.
Then we have

(2.3)

Suppose λ ∈ (λm+1, λm). ˆβ(λ) are the lasso coefﬁcient estimates.
ˆβ(λ)Bm

= (XT

XBm )

Sgnm

(cid:4)

(cid:5)

−1

XT

.

Bm

Bm

y − λ
2

∗

, that is, iadd = (Bm)i

LEMMA 2. Consider the transition points λm and λm+1, λm+1 ≥ 0. Bm is the
active set in (λm+1, λm). Suppose iadd is an index added into Bm at λm and its
∗. Denote by (a)k the kth element of the
index in Bm is i
vector a. We can express the transition point λm as
−1XT
y)i
−1 Sgnm)i

(2.4)
Moreover, if jdrop is a dropped (if there is any) index at λm+1 and jdrop = (Bm)j
then λm+1 can be written as

λm = 2((XT
((XT

XBm )
XBm )

∗
∗ .

∗,

Bm

Bm

Bm

(2.5)

λm+1 = 2((XT
((XT

Bm

XBm )
XBm )

Bm

−1XT
y)j
−1 Sgnm)j

Bm

∗
∗ .

LEMMA 3. ∀λ > 0, ∃ a null set Nλ which is a ﬁnite collection of hyperplanes
in Rn. Let Gλ = Rn \ Nλ. Then ∀y ∈ Gλ, λ is not any of the transition points, that
is, λ /∈ {λ(y)m}.
LEMMA 4. ∀λ, ˆβλ(y) is a continuous function of y.
LEMMA 5. Fix any λ > 0 and consider y ∈ Gλ as deﬁned in Lemma 3. The
active set B(λ) and the sign vector Sgn(λ) are locally constant with respect to y.
LEMMA 6. Let G0 = Rn. Fix an arbitrary λ ≥ 0. On the set Gλ with full mea-
sure as deﬁned in Lemma 3, the lasso ﬁt ˆµλ(y) is uniformly Lipschitz. Precisely,

(cid:3)ˆµλ(y + y) − ˆµλ(y)(cid:3) ≤ (cid:3)y(cid:3)

for sufﬁciently small y.

(2.6)

(2.7)

Moreover, we have the divergence formula

∇ · ˆµλ(y) = |Bλ|.

DEGREES OF FREEDOM OF THE LASSO

2179

PROOF OF THEOREM 1. Theorem 1 is obviously true for λ = 0. We only need
to consider λ > 0. By Lemma 6 ˆµλ(y) is uniformly Lipschitz on Gλ. Moreover,
ˆµλ(y) is a continuous function of y, and thus ˆµλ(y) is uniformly Lipschitz on Rn.
Hence ˆµλ(y) is almost differentiable; see Meyer and Woodroofe [14] and Efron
et al. [5]. Then (2.2) is obtained by invoking Stein’s lemma (Stein [21]) and the
divergence formula (2.7). (cid:1)
2.2. Consistency of the unbiased estimator (cid:3)
that the obtained unbiased estimator (cid:3)

In this section we show
df (λ) is also consistent. We adopt the similar
setup in Knight and Fu [12] for the asymptotic analysis. Assume the following two
conditions:
∗ + εi, where ε1, . . . , εn are i.i.d. normal random variables with mean
1. yi = xi β
denotes the ﬁxed unknown regression coefﬁcients.
2. 1
We consider minimizing an objective function Zλ(β) deﬁned as
|βj|.

0 and variance σ 2, and β
nXT X → C, where C is a positive deﬁnite matrix.

Zλ(β) = (β − β

)T C(β − β

) + λ

p(cid:2)

df (λ).

(2.8)

∗

∗

∗

j=1

λ

∗

df (λ

∗
If λ
n
n

THEOREM 2.

> 0, where λ

is a nontransition point such that

Optimizing (2.8) is a lasso type problem: minimizing a quadratic objective func-
tion with an (cid:7)1 penalty. There are also a ﬁnite sequence of transition points {λ∗m}
associated with optimizing (2.8).
∗ (cid:5)= λ∗m for all m, then (cid:3)
→ λ
∗
n) − df (λ
n) → 0 in probability.
∗
∗
PROOF OF THEOREM 2. Consider ˆβ
∗ = arg minβ Zλ
n. Denote B(n) = {j : ˆβ (n)
lasso solution given in (1.1) with λ = λ
∗
p} and B
First, let us consider any j ∈ B
know that
Sgn( ˆβ (n)
P (B(n) ⊇ B
is the
is not a transition point, by the Karush–Kuhn–Tucker
minimizer of Zλ
(KKT) optimality condition (Efron et al. [5], Osborne, Presnell and Turlach [15]),
we must have
(2.9)

∗ (β) and let ˆβ (n) be the
(cid:5)= 0, 1 ≤ j ≤
) → 1.
. By Theorem 1 in Knight and Fu [12] we
. Then the continuous mapping theorem implies that

(cid:5)= 0, 1 ≤ j ≤ p}. We want to show P (B(n) = B
ˆβ
∗
j ) (cid:5)= 0, since Sgn(x) is continuous at all x but zero. Thus
∗

∗ = {j : ˆβ
∗
ˆβ (n) →p
) →p Sgn( ˆβ
) → 1. Second, consider any j
∗
∗
∗ (β) and λ

(cid:14) = 0. Since ˆβ
∗

. Then ˆβ

/∈ B

∗

∗

∗

∗

(cid:14)

j

j

j

j

∗ − ˆβ

∗

)| > 0. Now

(cid:14) (β

th row vector of C. Let r

(cid:14) is the j

(cid:14)
where Cj
− 2|xT
let us consider rn = λ
∗
(cid:14) (y − X ˆβ
xT
(2.10)
j

n

j

∗
λ

∗ − ˆβ
> 2|Cj
)|,
∗
(cid:14) (β
∗ − 2|Cj
∗ = λ
(cid:14) (y − X ˆβ
n )|. Note that
∗
∗ − ˆβ
n ) + xT
n ) = xT
∗
∗
(cid:14)X(β

j

j

(cid:14) ε.

2180

H. ZOU, T. HASTIE AND R. TIBSHIRANI

∗

r

j

j

∗

j

n
n

n
n

r
n
n

nxT

∗
= λ

(cid:14) ε/n|. Because

− 2| 1
(cid:14)X(β
Thus
(cid:14) ε/n →p 0, we conclude r
xT
Immediately we see (cid:3)
n > 0 implies ˆβ (n)
j
∗
So (cid:3)

∗ − ˆβ
ˆβ (n) →p
n ) + xT
∗
and
→p r
∗
> 0. By the KKT optimality condition,
∗ ⊇ Bn) → 1. Therefore P (B(n) = B
(cid:14) = 0. Thus P (B
) → 1.
∗|. Then invoking the dominated convergence
n) →p |B
∗
n) = E[(cid:3)

∗
n) →p 0. (cid:1)
∗

n) − df (λ
∗

n)] → |B
∗

theorem we have

(2.11)

∗|.

df (λ

df (λ

df (λ

df (λ

ˆβ

∗

∗

(2.12)

2.3. Numerical experiments.

∗ = Xβ + N (0, 1)σ,
y

In this section we check the validity of our ar-
guments by a simulation study. Here is the outline of the simulation. We take the
64 predictors in the diabetes data set, which include the quadratic terms and inter-
actions of the original ten predictors. The positive cone condition is violated on the
64 predictors (Efron et al. [5]). The response vector y is used to ﬁt an OLS model.
ols. Then we consider a synthetic model,

We compute the OLS estimates ˆβols and ˆσ 2
where β = ˆβols and σ = ˆσols.
Given the synthetic model, the degrees of freedom of the lasso can be numer-
ically evaluated by Monte Carlo methods. For b = 1, 2, . . . , B, we independently
i ). Then df =(cid:6)
simulate y∗
(b) from (2.12). For a given λ, by the deﬁnition of df , we need to evalu-
ate covi = cov( ˆµi , y
] = (Xβ)i and note
that covi = E[( ˆµi − ai )(y
− (Xβ)i )] for any ﬁxed known constant ai. Then we
(cid:6)
(cid:7)covi =
compute
b=1( ˆµi (b) − ai )(y
and df =(cid:6)
(cid:7)covi /σ 2. Typically ai = 0 is used in Monte Carlo calculation. In
this work we use ai = (Xβ)i, for it gives a Monte Carlo estimate for df with
(cid:6)
(cid:3)
smaller variance than that given by ai = 0. On the other hand, we evaluate E|Bλ|
df (λ)b/B. We are interested in E|Bλ| − df (λ). Standard errors are cal-
by
culated based on the B replications. Figure 3 shows very convincing pictures to
support the identity (2.2).

i=1 covi /σ 2. Since E[y

i (b) − (Xβ)i )
∗

(2.13)

b=1

i=1

B

∗

∗

∗

B

B

n

n

i

i

2.4. Adaptive model selection criteria. The exact value of df (λ) depends on
the underlying model according to Theorem 1. It remains unknown to us unless
we know the underlying model. Our theory provides a convenient unbiased and
consistent estimate of the unknown df (λ). In the spirit of SURE theory, the good
unbiased estimate for df (λ) sufﬁces to provide an unbiased estimate for the pre-

diction error of ˆµλ as

(2.14)

Cp(ˆµ) = (cid:3)y − ˆµ(cid:3)2

n

+ 2

n

(cid:3)
df (ˆµ)σ 2.

DEGREES OF FREEDOM OF THE LASSO

2181

FIG. 3. The synthetic model with the 64 predictors in the diabetes data. In the top panel we com-
pare E|Bλ| with the true degrees of freedom df (λ) based on B = 20000 Monte Carlo simulations.
◦
line (the perfect match). The bottom panel shows the estimation bias and
The solid line is the 45
its point-wise 95% conﬁdence intervals are indicated by the thin dashed lines. Note that the zero
horizontal line is well inside the conﬁdence intervals.

Consider the Cp curve as a function of the regularization parameter λ. We ﬁnd the
optimal λ that minimizes Cp. As shown in Shen and Ye [20], this model selec-

2182

H. ZOU, T. HASTIE AND R. TIBSHIRANI

tion approach leads to an adaptively optimal model which essentially achieves the
optimal prediction risk as if the ideal tuning parameter were given in advance.

By the connection between Mallows’ Cp (Mallows [13]) and AIC (Akaike [1]),
we use the (generalized ) Cp formula (2.14) to equivalently deﬁne AIC for the
lasso,

The model selection results are identical by Cp and AIC. Following the usual
deﬁnition of BIC [16], we propose BIC for the lasso as

(2.15)

(2.16)

AIC(ˆµ) = (cid:3)y − ˆµ(cid:3)2

nσ 2

+ 2

n

(cid:3)
df (ˆµ).

BIC(ˆµ) = (cid:3)y − ˆµ(cid:3)2

nσ 2

+ log(n)

n

(cid:3)
df (ˆµ).

AIC and BIC possess different asymptotic optimality. It is well known that AIC
tends to select the model with the optimal prediction performance, while BIC tends
to identify the true sparse model if the true model is in the candidate list; see
Shao [17], Yang [23] and references therein. We suggest using BIC as the model
selection criterion when the sparsity of the model is our primary concern.

Using either AIC or BIC to ﬁnd the optimal lasso model, we are facing an

optimization problem,

(cid:3)2

(cid:3)y − ˆµλ
nσ 2

+ wn

(cid:3)

λ(optimal) = arg min

(2.17)
where wn = 2 for AIC and wn = log(n) for BIC. Since the LARS algorithm efﬁ-
ciently solves the lasso solution for all λ, ﬁnding λ(optimal) is attainable in princi-
ple. In fact, we show that λ(optimal) is one of the transition points, which further
facilitates the searching procedure.

df (λ),

n

λ

(cid:3)
THEOREM 3. To ﬁnd λ(optimal), we only need to solve
df (λm);

+ wn

(cid:3)2

(cid:3)y − ˆµλm
nσ 2

∗ = arg min
(2.18)
then λ(optimal) = λm
∗.

m

m

n

PROOF. Let us consider λ ∈ (λm+1, λm). By (2.3) we have

(2.19)

(cid:3)y − ˆµλ
= XBm (XT

(cid:3)2 = yT (I − HBm )y + λ2

SgnT

m(XT
Bm

−1 Sgnm,

XBm )

4

(cid:3)2 is
−1XT
XBm )
where HBm
strictly increasing in the interval (λm+1, λm). Moreover, the lasso estimates are
continuous on λ, hence (cid:3)y − ˆµλm
(cid:3)2. On the other

. Thus we can conclude that (cid:3)y − ˆµλ

(cid:3)2 > (cid:3)y − ˆµλm+1

(cid:3)2 > (cid:3)y − ˆµλ

Bm

Bm

DEGREES OF FREEDOM OF THE LASSO

hand, note that (cid:3)
df (λ) = |Bm| ∀λ ∈ (λm+1, λm) and |Bm| ≥ |B(λm+1)|. Therefore
the optimal choice of λ in [λm+1, λm) is λm+1, which means λ(optimal) ∈ {λm}.
(cid:1)

2183

According to Theorem 3, the optimal lasso model is immediately selected once
we compute the entire lasso solution paths by the LARS algorithm. We can ﬁnish
the whole ﬁtting and tuning process with the computational cost of a single least
squares ﬁt.

3. Efron’s conjecture. Efron et al. [5] ﬁrst considered deriving the analytical
form of the degrees of freedom of the lasso. They proposed a stage-wise algorithm
called LARS to compute the entire lasso solution paths. They also presented the
following conjecture on the degrees of freedom of the lasso:

CONJECTURE 1.

LARS-lasso sequence containing exactly k nonzero predictors. Then df (ˆµmlast

Starting at step 0,

be the index of

the last
) = k.

k

let mlast

k

k

Note that Efron et al. [5] viewed the lasso as a forward stage-wise modeling
algorithm and used the number of steps as the tuning parameter in the lasso: the
lasso is regularized by early stopping. In the previous sections we regarded the
lasso as a continuous penalization method with λ as its regularization parameter.
There is a subtle but important difference between the two views. The λ value
associated with mlast
is a random quantity. In the forward stage-wise modeling
view of the lasso, the conjecture cannot be used for the degrees of freedom of the
lasso at a general step k for a preﬁxed k. This is simply because the number of
LARS-lasso steps can exceed the number of all predictors (Efron et al. [5]). In

contrast, the unbiasedness property of (cid:3)
• We give a much more simpliﬁed proof than that in Efron et al. [5] to show that
the conjecture is true under the positive cone condition.
• Our analysis also indicates that without the positive cone condition the conjec-
ture can be wrong, although k is a good approximation of df (ˆµmlast
• We show that the conjecture works appropriately from the model selection per-
spective. If we use the conjecture to construct AIC (or BIC) to select the lasso
ﬁt, then the selected model is identical to that selected by AIC (or BIC) using
the exact degrees of freedom results in Section 2.4.

In this section we provide some justiﬁcations for the conjecture:

df (λ) holds for all λ.

).

k

First, we need to show that with probability one we can well deﬁne the last
LARS-lasso sequence containing exactly k nonzero predictors. Since the conjec-
ture becomes a simple fact for the two trivial cases k = 0 and k = p, we only need
to consider k = 1, . . . , p − 1. Let k = {m :|Bλm
| = k}, k ∈ {1, 2, . . . , (p − 1)}.
= sup(k). However, it may happen that for some k there is no such
Then mlast

k

2184

H. ZOU, T. HASTIE AND R. TIBSHIRANI

m with |Bλm
| = k. For example, if y is an equiangular vector of all {Xj}, then
the lasso estimates become the OLS estimates after just one step. So k = ∅ for
k = 2, . . . , p − 1. The next lemma shows that the “one at a time” condition (Efron
et al. [5]) holds almost everywhere; therefore mlast
is well deﬁned almost surely.

k

(3.1)

|Vm(y)| ≤ 1

|Wm(y)| ≤ 1 and

LEMMA 7. Let Wm(y) denote the set of predictors that are to be included in
the active set at λm and let Vm(y) be the set of predictors that are deleted from the

active set at λm+1. Then ∃ a set (cid:8)N0 which is a collection of ﬁnite many hyperplanes
in Rn. ∀y ∈ Rn \ (cid:8)N0,
y ∈ Rn\(cid:8)N0 is said to be a locally stable point for k, if ∀y(cid:14)
)(y(cid:14)
ε(y) for a small enough ε(y), the effective set B(λmlast
LS(k) be the set of all locally stable points.
The next lemma helps us evaluate df (ˆµmlast
LEMMA 8. Let ˆµm(y) be the lasso ﬁt at the transition point λm, λm > 0. Then
for any i ∈ Wm, we can write ˆµ(m) as

such that (cid:3)y(cid:14)− y(cid:3) ≤
) = B(λmlast
)(y). Let

∀m = 0, 1, . . . , K(y).

).

k

k

k

(cid:9)

ˆµm(y) =

HB(λm)

(3.2)

B(λm)XB(λm)) Sgn(λm)xT
B(λm)(XT
−xT
i XT
Sgni

B(λm)(XT

B(λm)XB(λm)) Sgn(λm)

i (I − HB(λm))

− XT
=: Sm(y)y,

(cid:10)

y

(3.3)
where HB(λm) is the projection matrix on the subspace of XB(λm). Moreover
(3.4)

tr(Sm(y)) = |B(λm)|.

Note that |B(λmlast

k

(3.5)

)| = k. Therefore, if y ∈ LS(k), then
(y)) = k.

(y) = tr(Smlast

∇ · ˆµmlast

k

k

k

If the positive cone condition holds then the lasso solution paths are monotone
(Efron et al. [5]), hence Lemma 7 implies that LS(k) is a set of full measure. Then
) = k. However, it should be pointed out that
by Lemma 8 we know that df (mlast
k − df (mlast
) can be nonzero for some k when the positive cone condition is vi-
olated. Here we present an explicit example to show this point. We consider the
synthetic model in Section 2.3. Note that the positive cone condition is violated on
the 64 predictors [5]. As done in Section 2.3, the exact value of df (mlast
) can be
computed by Monte Carlo and then we evaluate the bias k − df (mlast
). In the syn-
thetic model (2.12) the signal/noise ratio Var(X ˆβols)
is about 1.25. We repeated the

k

k

k

ˆσ 2

ols

DEGREES OF FREEDOM OF THE LASSO

2185

FIG. 4. B = 20000 replications were used to assess the bias of (cid:3)

) = k. The 95% point-wise
conﬁdence intervals are indicated by the thin dashed lines. This simulation suggests that when the
) (cid:5)= k for some k. However, the bias is small (the maxi-
positive cone condition is violated, df (mlast
mum absolute bias is about 0.8), regardless of the size of the signal/noise ratio.

df (mlast

k

k

same simulation procedure with (β = ˆβols, σ = ˆσols
10 ) in the synthetic model and
the corresponding signal/noise ratio became 125. As shown clearly in Figure 4,
the bias k − df (mlast
) is not zero for some k. However, even if the bias exists, its
maximum magnitude is less than one, regardless of the size of the signal/noise
ratio, which suggests that k is a good estimate of df (mlast

).

k

Let us pretend the conjecture is true in all situations and then deﬁne the model

k

selection criteria as

(cid:3)2

(cid:3)y − ˆµmlast
nσ 2

k

+ wn

(3.7)

(3.6)
wn = 2 for AIC and wn = log(n) for BIC. Treat k as the tuning parameter of the
lasso. We need to ﬁnd k(optimal) such that

k.

n

k(optimal) = arg min

(cid:3)y − ˆµmlast
nσ 2
selected by (2.17) and (3.7) coincide, that is, ˆµλ

∗ = k(optimal). Theorem 3 implies that the models
∗ . This observation sug-
gests that although the conjecture is not always true, it actually works appropriately
for the purpose of model selection.

∗ = λ(optimal) and k

∗ = ˆµmlast

Suppose λ

+ wn

(cid:3)2

k

k.

n

k

k

2186

H. ZOU, T. HASTIE AND R. TIBSHIRANI

4. Proofs of the lemmas. First, let us introduce the following matrix repre-

sentation of the divergence. Let ∂ ˆµ
∂y be a n × n matrix whose elements are
= ∂ ˆµi

(cid:4)

(cid:5)

(4.1)

∂ ˆµ
∂y

,

i,j

∂yj

Then we can write

(4.2)

∇ · ˆµ = tr

The above trace expression will be used repeatedly.

PROOF OF LEMMA 1. Let
(cid:7)(β, y) =

(4.3)

i, j = 1, 2, . . . , n.
(cid:4)
(cid:5)

∂ ˆµ
∂y

.

(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)2 + λ

p(cid:2)

j=1

|βj|.

xj βj

(cid:1)(cid:1)(cid:1)(cid:1)(cid:1)y − p(cid:2)
(cid:12)

j=1

ˆβ(λ) is the minimizer of (cid:7)(β, y). For those j ∈ Bm we must have

for j ∈ Bm.

ˆβ(λ)j . Thus the

xj

Given y,
∂(cid:7)(β,y)

∂βj

(4.4)

(cid:11)
= 0, that is,
−2xT

y − p(cid:2)

j

xj

j=1

Since ˆβ(λ)i = 0 for all i /∈ Bm, then
y − XBm

equations in (4.4) become
−2XT

ˆβ(λ)j
(cid:13)

+ λ Sgn( ˆβ(λ)j ) = 0,
(cid:6)p
ˆβ(λ)j =(cid:6)
(cid:14) + λ Sgnm
j=1 xj
ˆβ(λ)Bm

j∈Bλ
= 0,

Bm

(4.5)
which gives (2.3). (cid:1)
PROOF OF LEMMA 2. We adopt the matrix notation used in SPLUS: M[i,·]
means the ith row of M. iadd joins Bm at λm; then ˆβ(λm)iadd
= 0. Consider ˆβ(λ)
(cid:5)
for λ ∈ (λm+1, λm). Lemma 1 gives
= (XT
(4.6)

(cid:4)

ˆβ(λ)Bm

Sgnm

XT

.

Bm

Bm

−1

XBm )

y − λ
2
ˆβ(λ)iadd, taking the limit of the i
−1[i

,·]XT

∗

∗

Bm

2{(XT

By the continuity of
λ → λm − 0, we have
XBm )
(4.7)

XBm )
The second {·} is a nonzero scalar, otherwise ˆβ(λ)iadd

th element of (4.6) as
−1[i
= 0 for all λ ∈ (λm+1, λm),
which contradicts the assumption that iadd becomes a member of the active set Bm.
Thus we have
λm =

}y = λm{(XT
(cid:10)

y =: v(Bm, i

,·] Sgnm

(4.8)

(cid:9)

(XT

)XT

}.

XT

y,

Bm

Bm

2

∗

∗

XBm )
−1[i

Bm
XBm )

−1[i
,·]
∗
,·] Sgnm
∗

Bm

Bm

(XT

Bm

DEGREES OF FREEDOM OF THE LASSO

2187

∗

∗

−1[i

) = {2((XT
,·])/((XT
where v(Bm, i
ranging (4.8), we get (2.4).
element of (4.6) as λ → λm+1 + 0 to conclude that
(cid:9)
(cid:10)

XBm )

Bm

−1[i

XBm )

∗

Bm

,·] Sgnm)}. Rear-
∗

Similarly, if jdrop is a dropped index at λm+1, we take the limit of the j

th

(4.9)

λm+1 =
∗

(XT

XBm )
−1[j

Bm
XBm )

(XT

2
) = {2((XT

Bm

where v(Bm, j
ranging (4.9), we get (2.5). (cid:1)

Bm

XBm )

−1[j
,·]
∗
,·] Sgnm
∗
−1[j

XT
,·])/((XT

∗

Bm

∗

y =: v(Bm, j
−1[j
∗

XBm )

Bm

y,

)XT
,·] Sgnm)}. Rear-

Bm

PROOF OF LEMMA 3. Suppose for some y and m, λ = λ(y)m. λ > 0 means m

is not the last lasso step. By Lemma 2 we have

∗

)XT

(4.10)

}y =: α(Bm, i

λ = λm = {v(Bm, i
) = v(Bm, i
∗
)XT
) by considering all the possible combinations of Bm, i

Obviously α(Bm, i
is a nonzero vector. Now let αλ be the to-
∗
tality of α(Bm, i
and
the sign vector Sgnm. αλ depends only on X and is a ﬁnite set, since at most p
predictors are available. Thus ∀α ∈ αλ, αy = λ deﬁnes a hyperplane in Rn. We
deﬁne

)y.

Bm

Bm

∗

∗

∗

Nλ = {y : αy = λ for some α ∈ αλ}

and Gλ = Rn \ Nλ.

Then on Gλ (4.10) is impossible. (cid:1)

ˆβ(y)ols = (XT X)

PROOF OF LEMMA 4. For writing convenience we omit the subscript λ. Let
−1XT y be the OLS estimates. Note that we always have the in-

equality

(4.11)

| ˆβ(y)|1 ≤ | ˆβ(y)ols|1.

Fix an arbitrary y0 and consider a sequence of {yn} (n = 1, 2, . . .) such
that yn → y0. Since yn → y0, we can ﬁnd a Y such that (cid:3)yn(cid:3) ≤ Y for all
n = 0, 1, 2, . . . . Consequently (cid:3) ˆβ(yn)ols(cid:3) ≤ B for some upper bound B (B is deter-
mined by X and Y ). By Cauchy’s inequality and (4.11), we have | ˆβ(yn)|1 ≤ √
for all n = 0, 1, 2, . . . . Thus to show ˆβ(yn) → ˆβ(y0), it is equivalent to show that
for every converging subsequence of { ˆβ(yn)}, say { ˆβ(ynk )}, the subsequence con-
verges to ˆβ(y). Now suppose ˆβ(ynk ) converges to ˆβ∞ as nk → ∞. We show
ˆβ∞ = ˆβ(y0). The lasso criterion (cid:7)(β, y) is written in (4.3). Let (cid:7)(β, y, y(cid:14)
) =
(cid:7)(β, y) − (cid:7)(β, y(cid:14)
(4.12)

). By the deﬁnition of ˆβnk , we must have
(cid:7)( ˆβ(y0), ynk ) ≥ (cid:7)( ˆβ(ynk ), ynk ).

pB

(cid:7)( ˆβ(y0), y0) = (cid:7)( ˆβ(y0), ynk ) + (cid:7)( ˆβ(y0), y0, ynk )
≥ (cid:7)( ˆβ(ynk ), ynk ) + (cid:7)( ˆβ(y0), y0, ynk )
= (cid:7)( ˆβ(ynk ), y0) + (cid:7)( ˆβ(ynk ), ynk , y0)
+ (cid:7)( ˆβ(y0), y0, ynk ).

(cid:13) ˆβ(ynk ) − ˆβ(y0)
(cid:14)
(cid:7)( ˆβ(ynk ), ynk , y0) + (cid:7)( ˆβ(y0), y0, ynk )

= 2(y0 − ynk )XT

.

(4.13)

We observe

(4.14)

2188

H. ZOU, T. HASTIE AND R. TIBSHIRANI

Then (4.12) gives

Let nk → ∞; the right-hand side of (4.14) goes to zero. Moreover, (cid:7)( ˆβ(ynk ), y0) →
(cid:7)( ˆβ∞, y0). Therefore (4.13) reduces to

(cid:7)( ˆβ(y0), y0) ≥ (cid:7)( ˆβ∞, y0).

However, ˆβ(y0) is the unique minimizer of (cid:7)(β, y0), and thus ˆβ∞ = ˆβ(y0). (cid:1)
PROOF OF LEMMA 5. Fix an arbitrary y0 ∈ Gλ. Denote by Ball(y, r) the
n-dimensional ball with center y and radius r. Note that Gλ is an open set, so we
can choose a small enough ε such that Ball(y0, ε) ⊂ Gλ. Fix ε. Suppose yn → y as
n → ∞. Then without loss of generality we can assume yn ∈ Ball(y0, ε) for all n.
So λ is not a transition point for any yn.
By deﬁnition ˆβ(y0)j (cid:5)= 0 for all j ∈ B(y0). Then Lemma 4 says that ∃ an N1,
and as long as n > N1, we have ˆβ(yn)j (cid:5)= 0 and Sgn( ˆβ(yn)) = Sgn( ˆβ(yn)), for all
j ∈ B(y0). Thus B(y0) ⊆ B(yn) ∀n > N1.
(cid:14)(cid:15)(cid:15)
(cid:13)
(cid:14)(cid:15)(cid:15)
(cid:13)
y0 − X ˆβ(y0)
y0 − X ˆβ(y0)

(4.16)
Using Lemma 4 again, we conclude that ∃ an N > N1 such that ∀j /∈ B(y0)
the strict inequalities (4.16) hold for yn provided n > N. Thus Bc(y0) ⊆ Bc(yn)
∀n > N. Therefore we have B(yn) = B(y0) ∀n > N. Then the local constancy of
the sign vector follows the continuity of ˆβ(y). (cid:1)

On the other hand, we have the equiangular conditions (Efron et al. [5])

∀j ∈ B(y0),
∀j /∈ B(y0).

λ = 2
λ > 2

(4.15)

PROOF OF LEMMA 6.

If λ = 0, then the lasso ﬁt is just the OLS ﬁt. The
conclusions are easy to verify. So we focus on λ > 0. Fix an y. Choose a small
enough ε such that Ball(y, ε) ⊂ Gλ.

(cid:15)(cid:15)xT
(cid:15)(cid:15)xT

j

j

Since λ is not any transition point, using (2.3) we observe

ˆµλ(y) = X ˆβ(y) = Hλ(y)y − λωλ(y),

(4.17)

DEGREES OF FREEDOM OF THE LASSO

2189

where Hλ(y) = XBλ (XT
and ωλ(y) = 1
2XBλ (XT
Bλ
ˆµλ(y + y) = Hλ(y + y)(y + y) − λωλ(y + y).
(4.18)

−1XT
−1 SgnBλ

is the projection matrix on the space XBλ
. Consider (cid:3)y(cid:3) < ε. Similarly, we get

Bλ
XBλ )

XBλ )

Bλ

Lemma 5 says that we can further let ε be sufﬁciently small such that both the
effective set Bλ and the sign vector Sgnλ stay constant in Ball(y, ε). Now ﬁx ε.
Hence if (cid:3)y(cid:3) < ε, then
(4.19)

and ωλ(y + y) = ωλ(y).

Hλ(y + y) = Hλ(y)

Then (4.17) and (4.18) give

(4.20)
But since (cid:3)Hλ(y)y(cid:3) ≤ (cid:3)y(cid:3), (2.6) is proved.

ˆµλ(y + y) − ˆµλ(y) = Hλ(y)y.

By the local constancy of H (y) and ω(y), we have

∂ ˆµλ(y)
∂y

= Hλ(y).

(4.21)

(4.23)

Therefore

Then the trace formula (4.2) implies that

(4.22)

∇ · ˆµλ(y) = tr(Hλ(y)) = |Bλ|.

(cid:1)
PROOF OF LEMMA 7. Suppose at step m, |Wm(y)| ≥ 2. Let iadd and jadd be
∗
add be their indices in the current

two of the predictors in Wm(y), and let i
active set A. Note the current active set A is Bm in Lemma 2. Hence we have

∗
add and j

λm = v[A, i

∗]XT

Ay and λm = v[A, j

∗]XT

Ay.

A

0 = {[v(A, i

add)− v(A, j
∗

add) − v(A, j
add)]XT
∗
∗
(4.24)
We claim αadd = [v(A, i
add)]XT
∗
A is not a zero vector. Otherwise, since
{Xj} are linearly independent, αadd = 0 forces v(A, i
add) = 0. Then
∗
we have

}y =: αaddy.
add) − v(A, j
∗
−1[j
,·]
∗
,·] SgnA
∗
−1 is a full rank matrix.
Similarly, if idrop and jdrop are dropped predictors, then

AXA)
−1[i
which contradicts the fact (XT

AXA)
−1[i

(XT
AXA)

= (XT

AXA)

(4.25)

(XT

(XT

,

(4.26)
and αdrop = [v(A, i

0 = {[v(A, i
drop) − v(A, j
∗

}y =: αdropy,
drop)]XT
∗
A is a nonzero vector.

A

−1[i
,·]
∗
,·] SgnA
∗
AXA)
drop) − v(A, j
∗
drop)]XT
∗

2190

H. ZOU, T. HASTIE AND R. TIBSHIRANI

Let M0 be the totality of αadd and αdrop by considering all the possible com-
binations of A, (iadd, jadd), (idrop, jdrop) and SgnA. Clearly M0 is a ﬁnite set and

(4.27)

depends only on X. Let(cid:8)N0 = {y : αy = 0 for some α ∈ M0}.
Then on Rn \ (cid:8)N0 the conclusion holds. (cid:1)
Lemma 1 and taking the limit of λ → λm, we have

PROOF OF LEMMA 8. Note that

(cid:12)

(cid:11)

j

xj

y − p(cid:2)
ˆβ(λm)j
−2xT
ˆβ(λm)j =(cid:6)
(cid:6)p
ˆβ(λm) =(cid:13)

j=1 xj

j=1

XT
B(λm)XB(λm)

j∈B(λm) xj

(cid:13)

ˆµm(y) = XB(λm)

(4.28)

However,

(4.29)

Hence

(4.30)

ˆβ(λ) is continuous on λ. Using (4.4) in

for j ∈ B(λm).
(cid:5)

+ λm Sgn( ˆβ(λm)j ) = 0,
(cid:14)−1

ˆβ(λm)j . Thus we have
(cid:4)
B(λm)y − λm
(cid:4)
(cid:14)−1

XT

2

Sgn(λm)

.

(cid:5)

XT
B(λm)XB(λm)

(cid:13)

XT

B(λm)y − λm
(cid:14)−1 Sgn(λm)

2

Sgn(λm)

λm
2

.

= HB(λm)y − XB(λm)
(cid:13)

Since i ∈ Wm, we must have the equiangular condition
(cid:14) = λm

y − ˆµ(m)

.

XT
B(λm)XB(λm)

Sgni xT

i

(4.31)

2

Substituting (4.30) into (4.31), we solve λm/2 and obtain

(4.32)

=

λm
2

i (I − HB(λm))y
xT
B(λm)(XT

Sgni

i XT

B(λm)XB(λm)) Sgn(λm)

.

−xT
(cid:4)

Then putting (4.32) back to (4.30) yields (3.2).

Using the identity tr(AB) = tr(BA), we observe
(cid:13)
Sm(y) − HB(λm)

(cid:14) = tr

(XT

tr

B(λm)XB(λm)) Sgn(λm)xT
Sgni
= tr(0) = 0.

B(λm)(XT

−xT

i XT

So tr(Sm(y)) = tr(HB(λm)) = |B(λm)|. (cid:1)

(cid:5)

i (I − HB(λm))XT
B(λm)XB(λm)) Sgn(λm)

B(λm)

DEGREES OF FREEDOM OF THE LASSO

2191

5. Discussion.

In this article we have proven that the number of nonzero co-
efﬁcients is an unbiased estimate of the degrees of freedom of the lasso. The un-
biased estimator is also consistent. We think it is a neat yet surprising result. Even
in other sparse modeling methods, there is no such clean relationship between the
number of nonzero coefﬁcients and the degrees of freedom. For example, the num-
ber of nonzero coefﬁcients is not an unbiased estimate of the degrees of freedom
of the elastic net (Zou [26]). Another possible counterexample is the SCAD (Fan
and Li [6]) whose solution is even more complex than the lasso. Note that with
orthogonal predictors, the SCAD estimates can be obtained by the SCAD shrink-
age formula (Fan and Li [6]). Then it is not hard to check that with orthogonal
predictors the number of nonzero coefﬁcients in the SCAD estimates cannot be an
unbiased estimate of its degrees of freedom.

The techniques developed in this article can be applied to derive the degrees of
freedom of other nonlinear estimating procedures, especially when the estimates
have piece-wise linear solution paths. Gunter and Zhu [9] used our arguments to
derive an unbiased estimate of the degrees of freedom of support vector regression.
Zhao, Rocha and Yu [25] derived an unbiased estimate of the degrees of freedom
of the regularized estimates using the CAP penalties.

Bühlmann and Yu [2] deﬁned the degrees of freedom of L2 boosting as the trace
of the product of a series of linear smoothers. Their approach takes advantage of
the closed-form expression for the L2 ﬁt at each boosting stage. It is now well
known that ε-L2 boosting is (almost) identical to the lasso (Hastie, Tibshirani and
Friedman [11], Efron et al. [5]). Their work provides another look at the degrees of
freedom of the lasso. However, it is not clear whether their deﬁnition agrees with
the SURE deﬁnition. This could be another interesting topic for future research.

Acknowledgments. Hui Zou sincerely thanks Brad Efron, Yuhong Yang and
Xiaotong Shen for their encouragement and suggestions. We sincerely thank the
Co-Editor Jianqing Fan, an Associate Editor and two referees for helpful com-
ments which greatly improved the manuscript.

REFERENCES

[1] AKAIKE, H. (1973). Information theory and an extension of the maximum likelihood principle.
In Second International Symposium on Information Theory (B. N. Petrov and F. Csáki,
eds.) 267–281. Académiai Kiadó, Budapest. MR0483125

[2] BÜHLMANN, P. and YU, B. (2005). Boosting, model selection, lasso and nonnegative garrote.

Technical report, ETH Zürich.

[3] DONOHO, D. and JOHNSTONE, I. (1995). Adapting to unknown smoothness via wavelet

shrinkage. J. Amer. Statist. Assoc. 90 1200–1224. MR1379464

[4] EFRON, B. (2004). The estimation of prediction error: Covariance penalties and cross-

validation (with discussion). J. Amer. Statist. Assoc. 99 619–642. MR2090899

[5] EFRON, B., HASTIE, T., JOHNSTONE, I. and TIBSHIRANI, R. (2004). Least angle regression

(with discussion). Ann. Statist. 32 407–499. MR2060166

2192

H. ZOU, T. HASTIE AND R. TIBSHIRANI

[6] FAN, J. and LI, R. (2001). Variable selection via nonconcave penalized likelihood and its oracle

properties. J. Amer. Statist. Assoc. 96 1348–1360. MR1946581

[7] FAN, J. and LI, R. (2006). Statistical challenges with high dimensionality: Feature selection
in knowledge discovery. In Proc. International Congress of Mathematicians 3 595–622.
European Math. Soc., Zürich. MR2275698

[8] FAN, J. and PENG, H. (2004). Nonconcave penalized likelihood with a diverging number of

parameters. Ann. Statist. 32 928–961. MR2065194

[9] GUNTER, L. and ZHU, J. (2007). Efﬁcient computation and model selection for the support

vector regression. Neural Computation 19 1633–1655.

[10] HASTIE, T. and TIBSHIRANI, R. (1990). Generalized Additive Models. Chapman and Hall,

London. MR1082147

[11] HASTIE, T., TIBSHIRANI, R. and FRIEDMAN, J. (2001). The Elements of Statistical Learning;

Data Mining, Inference and Prediction. Springer, New York. MR1851606

[12] KNIGHT, K. and FU, W. (2000). Asymptotics for lasso-type estimators. Ann. Statist. 28

1356–1378. MR1805787

[13] MALLOWS, C. (1973). Some comments on CP . Technometrics 15 661–675.
[14] MEYER, M. and WOODROOFE, M. (2000). On the degrees of freedom in shape-restricted

regression. Ann. Statist. 28 1083–1104. MR1810920

[15] OSBORNE, M., PRESNELL, B. and TURLACH, B. (2000). A new approach to variable selection

in least squares problems. IMA J. Numer. Anal. 20 389–403. MR1773265

[16] SCHWARZ, G. (1978). Estimating the dimension of a model. Ann. Statist. 6 461–464.

MR0468014

[17] SHAO, J. (1997). An asymptotic theory for linear model selection (with discussion). Statist.

Sinica 7 221–264. MR1466682

[18] SHEN, X. and HUANG, H.-C. (2006). Optimal model assessment, selection and combination.

J. Amer. Statist. Assoc. 101 554–568. MR2281243

[19] SHEN, X., HUANG, H.-C. and YE, J. (2004). Adaptive model selection and assessment for

exponential family distributions. Technometrics 46 306–317. MR2082500

[20] SHEN, X. and YE, J. (2002). Adaptive model selection. J. Amer. Statist. Assoc. 97 210–221.

MR1947281

[21] STEIN, C. (1981). Estimation of the mean of a multivariate normal distribution. Ann. Statist. 9

1135–1151. MR0630098

[22] TIBSHIRANI, R. (1996). Regression shrinkage and selection via the lasso. J. Roy. Statist. Soc.

Ser. B 58 267–288. MR1379242

[23] YANG, Y. (2005). Can the strengths of AIC and BIC be shared?—A conﬂict between model

identiﬁcation and regression estimation. Biometrika 92 937–950. MR2234196

[24] YE, J. (1998). On measuring and correcting the effects of data mining and model selection. J.

Amer. Statist. Assoc. 93 120–131. MR1614596

[25] ZHAO, P., ROCHA, G. and YU, B. (2006). Grouped and hierarchical model selection through
composite absolute penalties. Technical report, Dept. Statistics, Univ. California, Berke-
ley.

[26] ZOU, H. (2005). Some perspectives of sparse statistical modeling. Ph.D. dissertation, Dept.

Statistics, Stanford Univ.

H. ZOU
SCHOOL OF STATISTICS
UNIVERSITY OF MINNESOTA
MINNEAPOLIS, MINNESOTA 55455
USA
E-MAIL: hzou@stat.umn.edu

T. HASTIE
R. TIBSHIRANI
DEPARTMENT OF STATISTICS
STANFORD UNIVERSITY
STANFORD, CALIFORNIA 94305
USA
E-MAIL: hastie@stanford.edu

tibs@stanford.edu

