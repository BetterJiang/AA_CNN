Abstract. We consider a novel online semi-supervised learning set-
ting where (mostly unlabeled) data arrives sequentially in large volume,
and it is impractical to store it all before learning. We propose an on-
line manifold regularization algorithm. It diers from standard online
learning in that it learns even when the input point is unlabeled. Our
algorithm is based on convex programming in kernel space with stochas-
tic gradient descent, and inherits the theoretical guarantees of standard
online algorithms. However, nave implementation of our algorithm does
not scale well. This paper focuses on ecient, practical approximations;
we discuss two sparse approximations using buering and online random
projection trees. Experiments show our algorithm achieves risk and gen-
eralization accuracy comparable to standard batch manifold regulariza-
tion, while each step runs quickly. Our online semi-supervised learning
setting is an interesting direction for further theoretical development,
paving the way for semi-supervised learning to work on real-world life-
long learning tasks.

1 Introduction

Consider a robot with a video camera. The robot continuously takes high frame-
rate video of its surroundings, and wants to learn the names of various objects
in the video. However, like a child learning in the real world, the robot receives
names from humans only very rarely. The robot is thus in a semi-supervised
learning situation: most objects are unlabeled, while only a few are labeled by
humans.

There are several challenges that distinguish this situation from standard
semi-supervised learning. The robot cannot aord to store the massive amount
of mostly unlabeled video before learning; it requires an anytime classier
that is ready to use at all times, yet is continuously improving; training must
be cheap; and since the world is changing, it must adapt to non-stationarity in
classication.

These challenges are well-studied in online learning. However, our situation
is also dierent from standard online learning. Online learning (classication)
traditionally assumes that every input point is fully labeled; it cannot take ad-
vantage of unlabeled data. But in the robot case, the vast majority of the input

will be unlabeled. It seems wasteful to throw away the unlabeled input, as it
may contain useful information.

We address this situation by combining semi-supervised learning with on-
line learning. The resulting online semi-supervised learning algorithm is based
on convex programming with stochastic gradient descent in kernel space. This
combination is novel. To the best of our knowledge, the closest prior work is
the multiview hidden Markov perceptron ([1], Section 4), which heuristically
combines multiview learning with online perceptron. However, that work did
not enjoy the theoretical guarantees aorded by the online learning literature,
nor did it directly apply to other semi-supervised learning methods. In contrast,
our method can lift any batch semi-supervised learning methods with convex
regularized risks to the online setting. As a special case, we will discuss online
manifold regularization in detail.

The focus of the present work is to introduce a novel learning setting, and
to develop practical algorithms with experimental verication. It is important
to consider the eciency issues, as we do in Section 3, for the algorithm to
be practically relevant. Our online semi-supervised learning algorithm inherits
no-regret bounds from online convex programming but does not provide new
bounds. It is our hope that the novel setting where most of the incoming data
stream is unlabeled will inspire future work on improved bounds. Some of the
future directions are laid out at the end of the paper.

2 Online Semi-Supervised Learning

We build online semi-supervised learning with two main ingredients: online con-
vex programming [2] and regularized risk minimization for semi-supervised learn-
ing (see the overview in [3, 4]). Although kernel-based online convex program-
ming is well-understood [5], we are not aware of prior application in the semi-
supervised learning setting.

Consider an input sequence x1 . . . xT , where xt  Rd is the feature vector
of the t-th data point. Most (possibly even the vast majority) of the points are
unlabeled. Only occasionally is a point xt accompanied by its label yt  Y. This
setting diers dramatically from traditional online learning where all points are
labeled. Let K be a kernel over x and HK the corresponding reproducing kernel
Hilbert space (RKHS) [6]. Our goal is to learn a good predictor f  HK from
the sequence. Importantly, learning proceeds in an iterative fashion:

1. At time t an adversary picks xt and yt, not necessarily from any distribution
P (x, y) (although we will later assume iid for predicting future data). The
adversary presents xt to the learner.

2. The learner makes prediction ft(xt) using its current predictor ft.
3. With a small probability pl, the adversary reveals the label yt. Otherwise,

the adversary abstains, and xt remains unlabeled.

4. The learner updates its predictor to ft+1 based on xt and the adversarys

feedback yt, if any.

We hope the functions f1 . . . fT do well on the sequence, and on future
input if the data is indeed iid. The exact performance criteria is dened below.

2.1 Batch Semi-Supervised Risks

Before introducing our online learning algorithm, we rst review batch semi-
supervised learning, where the learner has access to the labeled and unlabeled
data all at once. A unifying framework for batch semi-supervised learning is risk
minimization with specialized semi-supervised regularizers. That is, one seeks
the solution f  = argminf HK J(f ), where the batch semi-supervised regularized
risk is

J(f ) =

1
l

T

Xt=1

(yt)c(f (xt), yt) +

1
2 kfk2

K + 2(f ),

where l is the number of labeled points, (yt) is an indicator function equal to 1
if yt is present (labeled) and 0 otherwise, c is a convex loss function, 1, 2 are
regularizer weights, kfkK is the RKHS norm of f , and  is the semi-supervised
regularizer which depends on f and x1 . . . xT . Specic choices of  lead to fa-
miliar semi-supervised learning methods:

i) Manifold regularization [79]:

 =

1
2T

T

Xs,t=1

(f (xs)  f (xt))2wst.

The edge weights wst dene a graph over the T points, e.g., a fully connected
graph with Gaussian weights wst = ekxsxtk2/22
. In this case,  is known as
the energy of f on the graph. It encourages label smoothness over the graph:
similar examples (large w) tend to have similar labels.

ii) Multiview learning [1012] optimizes multiple functions f1 . . . fM simulta-

neously. The semi-supervised regularizer

 =

M

T

Xi,j=1

Xt=1

(fi(xt)  fj(xt))2

penalizes dierences among the learners predictions for the same point.

iii) Semi-supervised support vector machines (S3VMs) [1315]:

 =

1
T  l

T

Xt=1

(1  (yt)) max(1  |f (xt)|, 0).

This is the average hat loss on unlabeled points. The hat loss is zero if f (x)
is outside (1, 1), and is the largest when f (x) = 0. It encourages the deci-
sion boundary f (x) = 0 to be far away from any unlabeled points (outside the
margin), thus avoiding cutting through dense unlabeled data regions.

2.2 From Batch to Online

A key observation is that for certain semi-supervised learning methods, the batch
risk J(f ) is the sum of convex functions in f . These methods include mani-
fold regularization and multiview learning, but not S3VMs whose hat loss is
non-convex. For these convex semi-supervised learning methods, one can derive
a corresponding online semi-supervised learning algorithm using online convex
programming. The remainder of the paper will focus on manifold regularization,
with the understanding that online versions of multiview learning and other
convex semi-supervised learning methods can be derived similarly.

We follow the general approach in [2, 5]. Recall the batch risk for our version

of manifold regularization in Section 2.1 is

J(f ) =

1
l

T

Xt=1

(yt)c(f (xt), yt) +

1
2 kfk2

K +

2
2T

T

Xs,t=1

(f (xs)  f (xt))2wst, (1)

and f  is the batch solution that minimizes J(f ). In online learning, the learner
only has access to the input sequence up to the current time. We thus dene the
instantaneous regularized risk Jt(f ) at time t to be

Jt(f ) =

T
l

(yt)c(f (xt), yt) +

1
2 kfk2

K + 2

t1

Xi=1

(f (xi)  f (xt))2wit.

(2)

The last term in Jt(f ) involves the graph edges from xt to all previous points
up to time t. The astute reader might notice that this poses a computational
challengewe will return to this issue in Section 3. While T appears in (2), Jt(f )
depends only on the ratio T /l. This is the empirical estimate of the inverse label
probability 1/pl, which we assume is given and easily determined based on the
rate at which humans can label the data at hand.

All the Jts are convex. They are intimately connected to the batch risk J:

Proposition 1 J(f ) = 1

t=1 Jt(f ).

T PT

Our online algorithm constructs a sequence of functions f1 . . . fT . Let f1 = 0.
The online algorithm simply performs a gradient descent step that aims to reduce
the instantaneous risk in each iteration:

ft+1 = ft  t

Jt(f )

f

(cid:12)(cid:12)(cid:12)(cid:12)ft

.

(3)

The step size t needs to decay at a certain rate, e.g., t = 1/t. Under mild
conditions, this seemingly nave online algorithm has a remarkable guarantee
that on any input sequence, there is asymptotically no regret compared to the
batch solution f . Specically, let the average instantaneous risk incurred by
the online algorithm be Jair(T )  1
t=1 Jt(ft). Note Jair involves a varying

T PT

sequence of functions f1 . . . fT . As a standard quality measure in online learning,
we compare Jair to the risk of the best xed function in hindsight:

Jair(T )  min
= Jair(T )  min

f

f

T

Jt(f )

1
Xt=1
T
J(f ) = Jair(T )  J(f ),

where we used Proposition 1. This dierence is known as the average regret. Ap-
plying Theorem 1 in [2] results in the no-regret guarantee lim supT  Jair(T )
J(f )  0. It is in this sense that the online algorithm performs as well as the
batch algorithm on the sequence.
To compute (3) for manifold regularization, we rst express the functions

f1 . . . fT using a common set of representers x1 . . . xT [16]

ft =

t1

Xi=1

(t)
i K(xi,).

(4)

The problem of nding ft+1 becomes computing the coecients (t+1)
, . . . , (t+1)
Again, this will be a computational issue when T is large, and will be addressed
in Section 3. We extend the kernel online supervised learning approach in [5] to
semi-supervised learning by writing the gradient Jt(f )/f in (3) as

1

t

.

T
l

(yt)c(f (xt), yt)K(xt,) + 1f

+22

t1

Xi=1

(f (xi)  f (xt))wit(K(xi,)  K(xt,)),

(5)

where we used the reproducing property of RKHS in computing the derivative:
f (x)/f = hf, K(x,)i/f = K(x,). c is the (sub)gradient of the loss func-
tion c. For example, when c(f (x), y) is the hinge loss max(1 f (x)y, 0), we may
dene c(f (x), y) = y if f (x)y  1, and 0 otherwise. Putting (5) back in (3),
and replacing ft with its kernel expansion (4), it can be shown that ft+1 has the
following coecients:

(t+1)
i

(t+1)
t

= (1  t1)(t)
= 2t2

t1

Xi=1

(ft(xi)  ft(xt))wit  t

i  2t2(ft(xi)  ft(xt))wit,

i = 1 . . . t  1

T
l

(yt)c(f (xt), yt).

(6)

We now have a basic online manifold regularization algorithm; see Algorithm 1.

When the data is iid, the generalization risk of the average function f =
t=1 ft approaches that of f  [17]. The average function f involves all
representers x1, . . . , xT . For basic online manifold regularization, it is possible
to incrementally maintain the exact f as time increases. However, for the sparse

1/T PT

Algorithm 1 Online Manifold Regularization

Parameters: edge weight function w, kernel K, weights 1, 2, loss function c, label
ratio T /l, step sizes t
Initialize t = 1, f1 = 0
loop

receive xt, predict ft(xt) using (4)
(occasionally) receive yt
update ft to ft+1 using (6)
store xt, let t = t + 1

end loop

approximations introduced below, the basis changes over time. Therefore, in
those cases f can be maintained only approximately using matching pursuit [18].
In our experiments, we compare the classication accuracy of f vs. f  on a
separate test set, which is of practical interest.

3 Sparse Approximations

Unfortunately, Algorithm 1 will not work in practice because it needs to store
every input point and soon runs out of memory; it also has time complexity
O(T 2). In particular, the instantaneous risk (2) and the kernel representation (4)
both involve the sequence up to the current time. To be useful, it is imperative
to sparsify both terms. In this section, we present two distinct approaches for
this purpose: i) using a small buer of points, and ii) constructing a random
projection tree that represents the manifold structure.

3.1 Buering

