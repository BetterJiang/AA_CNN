LLNL-CONF-466413

A Framework for Incorporating General
Domain Knowledge into Latent Dirichlet
Allocation using First-Order Logic

D. Andrzejewski, X. Zhu, M. Craven, B. Recht

January 19, 2011

Twenty-second International Joint Conference on Artificial
Intelligence (IJCAI 2011)
Barcelona, Spain, Spain
July 16, 2011 through July 22, 2011

Disclaimer 
 
This document was prepared as an account of work sponsored by an agency of the United States 
government. Neither the United States government nor Lawrence Livermore National Security, LLC, 
nor any of their employees makes any warranty, expressed or implied, or assumes any legal liability or 
responsibility for the accuracy, completeness, or usefulness of any information, apparatus, product, or 
process disclosed, or represents that its use would not infringe privately owned rights. Reference herein 
to any specific commercial product, process, or service by trade name, trademark, manufacturer, or 
otherwise does not necessarily constitute or imply its endorsement, recommendation, or favoring by the 
United States government or Lawrence Livermore National Security, LLC. The views and opinions of 
authors expressed herein do not necessarily state or reflect those of the United States government or 
Lawrence Livermore National Security, LLC, and shall not be used for advertising or product 
endorsement purposes. 
 

A Framework for Incorporating General Domain Knowledge

into Latent Dirichlet Allocation using First-Order Logic

David Andrzejewski
Lawrence Livermore
National Laboratory

andrzejewski1@llnl.gov

Xiaojin Zhu
University of

Wisconsin–Madison
jerryzhu@cs.wisc.edu

Abstract

Topic models have been used successfully for a va-
riety of problems, often in the form of application-
speciﬁc extensions of the basic Latent Dirichlet
Allocation (LDA) model. Because deriving these
new models in order to encode domain knowledge
can be difﬁcult and time-consuming, we propose
the Fold·all model, which allows the user to spec-
ify general domain knowledge in First-Order Logic
(FOL). However, combining topic modeling with
FOL can result in inference problems beyond the
capabilities of existing techniques. We have there-
fore developed a scalable inference technique using
stochastic gradient descent which may also be use-
ful to the Markov Logic Network (MLN) research
community. Experiments demonstrate the expres-
sive power of Fold·all, as well as the scalability of
our proposed inference method.

1 Introduction
Building upon the success of Latent Dirichlet Allocation
(LDA) [Blei et al., 2003], a large number of latent-topic-
model variants have been proposed for many application do-
mains. Often, these variants are custom-built by incorpo-
rating external knowledge speciﬁc to the target domain, see
e.g., [Wang et al., 2009; Gerrish and Blei, 2010]. However,
deriving a “custom” latent topic model, along with an efﬁ-
cient inference scheme, requires machine-learning expertise
not common among application domain experts. Further-
more, such effort must be duplicated whenever a new type of
domain knowledge is to be used, preventing domain experts
from taking full advantage of topic modeling approaches.
Previous work has integrated word-level knowledge into topic
models for both batch [Andrzejewski et al., 2009; Petter-
son et al., 2010] and interactive [Hu et al., 2011] settings,
but these approaches do not incorporate constraints involving
documents, topics, or general side information.
The main contribution of this paper is Fold·all (First-Order
Logic latent Dirichlet ALLocation), a framework for incorpo-
rating general domain knowledge into LDA. A domain expert
only needs to specify her domain knowledge as First-Order
Logic (FOL) rules, and Fold·all will automatically incorpo-
rate them into LDA inference to produce topics shaped by

Mark Craven
University of

Wisconsin–Madison

craven@biostat.wisc.edu

Benjamin Recht

University of

Wisconsin–Madison
brecht@cs.wisc.edu

both the data and the rules. This approach enables domain
experts to focus on high-level modeling goals instead of the
low-level issues involved in creating a custom topic model.
In fact, some previous topic model variants can be expressed
within the Fold·all framework.
Internally, Fold·all converts the FOL rules into a Markov
Random Field, and combines it with the LDA probabilistic
model. As such, it can be viewed as an instance of a Hy-
brid Markov Logic Network (HMLN) [Wang and Domingos,
2008], which is itself a generalization of a Markov Logic Net-
work (MLN) [Richardson and Domingos, 2006]. However,
existing inference schemes developed for HMLNs and MLNs
do not scale well for typical topic modeling applications. An-
other contribution of this paper is a scalable stochastic op-
timization algorithm for Fold·all, which is potentially useful
for general MLN research, too.
2 The Fold·all Framework
We now brieﬂy review the standard LDA model [Blei et al.,
2003]. While we describe variables in terms of text (e.g.,
words and documents), note that both LDA and Fold·all are
general and can be applied to non-text data as well. Let w =
w1 . . . wN be a text corpus containing N tokens, with d =
d1 . . . dN being the document indices of each word token and
z = z1 . . . zN being the hidden topic assignments of each
token. Each topic t = 1 . . . T is represented by a multinomial
φt over a W -word-type vocabulary. The φ’s have a Dirichlet
prior with parameter β. Likewise, each document j = 1 . . . D
is associated with a multinomial θj over topics, with another
Dirichlet prior with parameter α. The generative model is
P (w, z, φ, θ | α, β, d) ∝

(cid:32) T(cid:89)

(cid:33) D(cid:89)

(cid:32) N(cid:89)

(cid:33)

p(φt|β)

p(θj|α)

φzi(wi)θdi(zi)

(1)

t

j

i

where φzi(wi) is the wi-th element in vector φzi, and θdi(zi)
is the zi-th element in vector θdi. One important goal of topic
modeling is to estimate the topics φ given a corpus (w, d).
The key to our Fold·all framework is to allow domain
knowledge, speciﬁed in FOL, to inﬂuence the values of the
hidden topics z, indirectly inﬂuencing φ and θ. FOL provides
a powerful and ﬂexible way to specify domain knowledge.
For example, an analyst working on a congressional debate

∀i : W(i, taxes) ∧ Speaker(di, Rep) ⇒ Z(i, 77),

corpus where each speech is a document may specify the rule
(2)
which states that for any word token wi = “taxes” that ap-
pears in a speech by a Republican, the corresponding latent
topic should be zi = 77. We brieﬂy review some FOL con-
cepts [Domingos and Lowd, 2009] for Fold·all.

We deﬁne logical predicates for each of the standard LDA
variables, letting Z(i, t) be true if the hidden topic zi = t,
and false otherwise. Likewise, W(i, v) and D(i, j) are true if
wi = v and di = j, respectively. In addition, Fold·all can
incorporate other variables beyond those modeled by stan-
dard LDA. In our previous example, a domain expert deﬁnes
a predicate Speaker(di, Rep), which is true if the speaker
for document di is a member of the Republican political party.
We use o to collectively denote these other observed variables
and their corresponding logical predicate values.

The domain expert speciﬁes her background knowledge in
the form of a weighted FOL knowledge base using these pred-
icates: KB = {(λ1, ψ1), . . . , (λL, ψL)}. The KB is in Con-
junctive Normal Form, consisting of L pairs where each rule
ψl is an FOL clause, and λl ≥ 0 is its weight which the do-
main expert sets to represent the importance of ψl. Thus, in
general Fold·all treats such rules as soft preferences rather
than hard constraints. The knowledge base KB is tied to our
probabilistic model via its groundings. For each FOL rule
ψl, let G(ψl) be the set of groundings, each mapping the
free variables in ψl to speciﬁc values. For the “taxes” ex-
ample above, G consists of all N propositional rules where
i = 1 . . . N. For each grounding g ∈ G(ψl), we deﬁne an
indicator function

1g(z, w, d, o) =

0, otherwise.

if g is true under (z, w, d, o)

(cid:26) 1,

and z100 = 88,

For example, if w100 = “taxes”, Speaker(d100, Rep) =
true,
then the grounding g =
(W(100, taxes) ∧ Speaker(d100, Rep) ⇒ Z(100, 77)) will
have 1g(z, w, d, o) = 0 because of the mismatch in z100.

To combine the KB and LDA, we deﬁne a Markov Random
Field over latent topic assignments z, topic-word multinomi-
als φ, and document-topic multinomials θ, treating words w,
documents d, and side information o as observed. Speciﬁ-
cally, in this Markov Random Field the conditional probabil-
ity P (z, φ, θ | α, β, w, d, o, KB) is proportional to

 L(cid:88)
(cid:32) T(cid:89)

exp

l

(cid:88)
(cid:33) D(cid:89)

g∈G(ψl)

 ×
(cid:32) N(cid:89)

λl1g(z, w, d, o)

(3)

(cid:33)

p(φt|β)

p(θj|α)

φzi(wi)θdi(zi)

.

t

j

i

This Markov Random Field has two parts: the ﬁrst term acts
as a prior from the KB, and the remaining terms are identical
to LDA (1). Each satisﬁed grounding of FOL rule ψl con-
tributes exp(λl) to the potential function. Note in general,
the ﬁrst term couples all the elements of z, although the ac-
tual dependencies are determined by the particular form of
the KB. The factor graph for the Fold·all Markov Random

Figure 1: Fold·all factor graph with “mega” logic factor (in-
dicated by arrow) connected to d, z, w, o.

Field is shown in Figure 1, with a special “mega factor node”
corresponding to the ﬁrst term.

The ﬁrst term in (3) is equivalent to a Markov Logic Net-
work (MLN) [Richardson and Domingos, 2006]. The re-
maining terms in (3) involve continuous variables such as
θ, φ. This combination has been proposed in the MLN com-
munity under the name of Hybrid Markov Logic Networks
(HMLN) [Wang and Domingos, 2008], but to our knowledge
previous HMLN research has not combined logic with LDA.
3 Scalable Inference in Fold·all
Since exact inference is intractable for both LDA and MLN
models, it is unsurprising that Fold·all inference is difﬁcult
as well. In fact, the combination of logic and topic model-
ing components presents a unique scalability challenge which
cannot be addressed by existing techniques.
We are interested in inferring the most likely φ and θ in
Fold·all. However, as in standard LDA, the latent topic as-
signments z cannot be marginalized out. We instead aim to
ﬁnd the Maximum a Posteriori (MAP) estimate of (z, φ, θ)
jointly. This can be formulated as maximizing the logarithm
of the unnormalized probability (3):

T(cid:88)

(cid:88)

L(cid:88)
D(cid:88)

l

+

g∈G(ψl)

λl1g(z, w, d, o) +

log p(φt|β)

N(cid:88)

t

log φzi(wi)θdi(zi). (4)

log p(θj|α) +

argmax

z,φ,θ

j

i

This non-convex problem is particularly challenging due
to the fact that the summations over groundings G(ψl) are
combinatorial: on a corpus with length N, an FOL rule
with k universally quantiﬁed variables will produce N k
groundings. This explosion resulting from propositional-
ization is a well-known problem in the MLN community,
and has been the subject of considerable research [Singla
and Domingos, 2008; Kersting et al., 2009; Riedel, 2008;
Huynh and Mooney, 2009]. For instance, one can usually
greatly reduce the problem size by considering only non-
trivial groundings [Shavlik and Natarajan, 2009]. As an ex-
ample, the rule in (2) is trivially true for all indices i such
that wi (cid:54)= “taxes”, and these indices can be excluded from
computation. Unfortunately, even after this pre-processing,
there may be an unacceptably large number of groundings.
Furthermore, the inclusion of the LDA terms and the scale

Algorithm 1: Alternating Optimization with Mirror De-
scent for Fold·all.
Input: w, d, o, α, β, KB
for Nouter iterations do
set φ, θ with (5) (6)
set z \ zKB with (7)
for Ninner iterations do

sample term f from (9)
update zit’s in f with (10)
end
set zi ∈ zKB with arg maxt zit

end
return (z, φ, θ)

of our domain prevent us from directly taking advantage of
many techniques developed for MLNs. In what follows, we
describe a stochastic gradient descent algorithm, Alternating
Optimization with Mirror Descent, to ﬁnd a local maximum
of (4). This approach may also be applied to standard MLNs,
although we leave that application as future work.

3.1 Alternating Optimization with Mirror Descent
We propose Alternating Optimization with Mirror Descent
(Mir) to optimize (4). The complete procedure is presented
in Algorithm 1; it proceeds by alternating between optimiz-
ing the multinomial parameters (φ, θ) while holding z ﬁxed,
and vice versa. The optimal (φ, θ) for ﬁxed z can be easily
found in closed-form as the MAP estimate of the Dirichlet
posterior:

φt(w) ∝ ntw + β − 1
θj(t) ∝ njt + α − 1

(5)
(6)

where ntw is the number of times word w is assigned to topic
t in topic assignments z. Similarly, njt is the number of times
topic t is assigned to a word in document j.

Optimizing z while holding (φ, θ) ﬁxed is more difﬁ-
cult. One can divide z into an “easy part” and a “difﬁcult
part.” The easy part consists of all zi which only appear
in trivial groundings, where a trivial grounding is deﬁned as
any grounding g such that the corresponding indicator func-
tion 1g is insensitive to the latent topic assignment z. For
example, if the knowledge base consists of only one rule
ψ1 = (∀i : W(i, apple) ⇒ Z(i, 1)), then the majority of the
zi’s (those with wi (cid:54)= apple) appear in groundings which are
trivially true. These zi’s only appear in the last term in (4).
Consequently, the optimizer is simply

zi = argmax
t=1...T

φt(wi)θdi(t).

(7)

The difﬁcult part of z consists of those zi appearing in non-
trivial groundings, subsequently in the ﬁrst term of (4). De-
note this part zKB. We use stochastic gradient descent to op-
timize zKB. The key idea is to ﬁrst relax (4) into a continuous
optimization problem, and then randomly sample groundings
from the knowledge base, such that each sampled grounding
provides a stochastic gradient to the relaxed problem.

Table 1: Step-by-step example of the logic polynomial proce-
dure for the formula g = Z(i, 1) ∨ ¬Z(j, 2) with T = 3 (i.e.,
t ∈ {1, 2, 3}).

Original formula g
1: Take complement ¬g
2: Remove negations (¬g)+
3: Binary zit ∈ {0, 1}
4: Polynomial 1g(z)
5: Relax discrete zit

Z(i, 1) ∨ ¬Z(j, 2)
¬Z(i, 1) ∧ Z(j, 2)
(Z(i, 2) ∨ Z(i, 3)) ∧ Z(j, 2)
(zi2 + zi3) ∗ zj2
1 − (zi2 + zi3) ∗ zj2
zit ∈ {0, 1} → zit ∈ [0, 1]

Here we describe a procedure for converting the logic
grounding indicator 1g into a continuous polynomial over re-
laxed zit variables. Table 1 gives a simple step-by step ex-
ample of this procedure; individual steps in the following text
reference the corresponding steps in this example.
Step 1: Because we assume the knowledge base KB is in
Conjunctive Normal Form, each non-trivial grounding g con-
sists of a disjunction of Z(i, t) atoms (positive or negative),
whose logical complement ¬g is therefore a conjunction of
Z(i, t) atoms (each negated from the original grounding g).
Step 2: In order to standardize each grounding, let (·)+ be
an operator which returns a logical formula equivalent to its
argument where we replace all negated atoms ¬Z(i, t) with
equivalent disjunctions over positive atoms Z(i, 1) ∨ . . . ∨
Z(i, t − 1) ∨ Z(i, t + 1) ∨ . . . ∨ Z(i, T ), and eliminate any
duplicate atoms. We now have (¬g)+, which is the logical
complement of our original formula g expressed entirely in
terms of non-negated literals.
Step 3: We convert this Boolean formula over logical predi-
cates to a polynomial over binary variables. To do this, we re-
place each Z(i, t) with a binary indicator variable zit ∈ {0, 1}
deﬁned to be equal to 1 if Z(i, t) is true and 0 otherwise. Each
conjunction ∧ is then replaced with multiplication ∗, and each
disjunction ∨ is replaced with addition +. In this way, the
conjunction of disjunctions ¬g is converted into a product of
sums over binary zit variables.
Step 4: We now have a binary polynomial that is equivalent
to ¬g, the negation of our original formula g.
In order to
remove this negation, we take the binary complement of this
expression (i.e., 1 − x where x is the result of Step 3).

We now have a binary polynomial over zit that is exactly
equivalent to our original logical formula g. We can formally
express this result as

1g(z) = 1 − (cid:89)

 (cid:88)



zit

i:gi(cid:54)=∅

Z(i,t)∈(¬gi)+

(8)

where gi is the set of atoms in g which involve index i. For
example, if g = Z(0, 1) ∨ Z(0, 2) ∨ Z(1, 0), then g0 =
{Z(0, 1), Z(0, 2)}. Note the observed variables w, d, o are
no longer in (8) because g is a non-trivial grounding where
the disjunction of w, d, o atoms is always false.
Step 5: With our polynomial representation in hand, we re-
lax the binary variables zit ∈ {0, 1} to continuous values
t zit = 1 for all i. Under

zit ∈ [0, 1], with the constraint(cid:80)

this relaxation, Equation (8) takes on values in the interval
[0, 1], which can be interpreted as the expectation of the orig-
inal Boolean indicator function under a distribution where
each relaxed zit represents the multinomial probability that
Z(i, t) is true. We note that this function is non-convex due
to bilinearity in zit.

Dropping terms that are constant w.r.t.

zKB and re-
introducing the LDA objective function terms yields the con-
tinuous optimization problem

L(cid:88)

(cid:88)

g∈G(ψl)

l

argmax
z∈[0,1]|zKB|

s.t.

zit ≥ 0,

(cid:88)

λl1g(z) +

i,t

zit = 1.

T(cid:88)

t

zit log φt(wi)θdi(t)

(9)

This relaxation allows us to use gradient methods on (9).
However a potentially huge number of groundings in
∪lG(ψl) may still render the full gradient impractical to com-
pute. Critically, the next step is to use stochastic gradi-
ent descent for scalability, speciﬁcally the Entropic Mirror
Descent Algorithm (EMDA) [Beck and Teboulle, 2003], of
which the Exponentiated Gradient (EG) [Kivinen and War-
muth, 1997] algorithm is a special case. Unlike approaches
[Collins et al., 2008] which randomly sample training exam-
ples to produce a stochastic approximation to the gradient,
we randomly sample terms in (9). A term f is either the
polynomial 1g(z) on a particular grounding g, or an LDA
t zit log φt(wi)θdi(t) for some index i. We use a
weighted sampling scheme. Let Λ be a length L + 1 weight
vector, where Λl = λl|G(ψl)| for l = 1 . . . L, and the entry
ΛL+1 = |zKB| represents the LDA part. To sample individ-
ual terms, we ﬁrst choose one of the L + 1 entries according
to weights Λ. If an FOL rule ψl is chosen, we then sample
a grounding g ∈ G(ψl) uniformly. If the LDA part is cho-
sen, we uniformly sample an index i from zKB. Once a term
f is sampled, we take its gradient ∇f and perform a mirror
descent update with step size η:

term (cid:80)

(cid:80)
zit ← zit exp (η∇zitf )
t(cid:48) zit(cid:48) exp (η∇zit(cid:48) f )

.

(10)

The process of sampling terms and taking gradient steps is
then repeated for a prescribed number of iterations. Finally,
we recover a hard zKB assignment by rounding each zi to
arg maxt zit. The key advantage of this approach is that it
requires only a means to sample groundings g for each rule
ψl, and can avoid fully grounding the FOL rules. We now
consider several alternatives to the Mir approach.
3.2 MaxWalkSAT
A simple alternative means of introducing logic into LDA is
to perform standard LDA inference and then post-process the
latent topic vector z in order to maximize the weight of satis-
ﬁed ground logic clauses in the KB (i.e., optimize the MLN
objective in (4) only). This can be done using a weighted
satisﬁability solver such as MaxWalkSAT (MWS) [Selman
et al., 1995], a stochastic local search algorithm that selects
an unsatisﬁed grounding and satisﬁes it by ﬂipping the truth

state of a single atom, repeating for Ninner iterations. Choos-
ing which atom to ﬂip is done either randomly (with prob-
ability p) or greedily w.r.t.
the change ∆KB to the global
weighted satisfaction objective function. We keep the best
(highest satisﬁed weight) assignment z found, although the
fact that MWS does not take the learned topics into account
means that this may actually decrease the full objective (4).
3.3 Alternating Optimization with MWS+LDA
A more principled approach is to integrate the logic and LDA
objectives.
In Algorithm 1, we replace the zKB inner loop
with MWS+LDA (M+L), a form of MWS modiﬁed to incor-
porate the LDA objective in the greedy selection criterion by
selecting an atom according to ∆ = ∆KB + ∆LDA, where
∆LDA is the change to the LDA objective. This aims to max-
imize the objective (4), balancing the gain from satisfying a
logic clause and the gain of a topic assignment given the cur-
rent φ and θ parameters. We initialize using standard LDA,
and the inner MWS+LDA loop keeps the best zKB found with
respect to both satisﬁed logic weight and the LDA objective.
3.4 Collapsed Gibbs Sampling
We also perform collapsed Gibbs sampling (CGS) with re-
spect to the full Fold·all distribution (3). While Gibbs sam-
pling is not aimed at maximizing the objective, the hope
is that the sampler will explore high probability regions of
the z space. The collapsed Gibbs sampler iteratively re-
samples zi at each corpus position i, with the probability
of candidate topic assignment zi = t given by P (zi =
t|z−i, w, d, o, KB, α, β) ∝

(cid:33)(cid:32)

(cid:32)

(cid:80)T
n(−i)
dit + αt
(cid:88)
t(cid:48) (n(−i)

(cid:88)

dit(cid:48) + αt(cid:48))

exp

g∈G(ψl):gi(cid:54)=∅

l

(cid:80)W
n(−i)
twi + βwi
w(cid:48)(n(−i)

tw(cid:48) + βw(cid:48))
λl1g(z−i ∪ {zi = t})

where each −i indicates that we exclude the word token at po-
sition i. Note that (11) is the product of the standard LDA col-
lapsed Gibbs sampler [Grifﬁths and Steyvers, 2004] and the
MLN Gibbs sampling equation [Richardson and Domingos,
2006]. We keep the sample which maximizes (4). This ap-
proach may suffer from poor mixing in the presence of highly
weighted logic rules [Poon and Domingos, 2006].
4 Experiments
Our experiments evaluate the generalization of the Fold·all
model by measuring whether learned topics reﬂect both cor-
pus statistics and the user-deﬁned logic rules when applied to
unseen documents and associated logic rule groundings. Si-
multaneously, we evaluate the scalability of inference meth-
ods by applying Fold·all to datasets and KBs with large num-
bers of non-trivial groundings. Our experiments demonstrate
i) Fold·all successfully incorporates logic into topic
that:
modeling, and ii) Mir is a scalable and effective inference
method for Fold·all that works when other methods fail.
We conduct experiments on several datasets and corre-
sponding KBs using the four Fold·all inference methods de-
veloped in Section 3 (Mir, MWS, M+L, and CGS). We also

×

(cid:33)
,

(11)

√

use two baseline methods which do not integrate topic mod-
eling and logic: topic modeling alone (standard LDA infer-
ence with a collapsed Gibbs sampler), and logic alone (MAP
inference with the Alchemy MLN software package [Kok et
al., 2009]). These baselines use existing techniques to model
the LDA and MLN components in isolation.

√
Ninner/

For each KB, all free variables are universally quantiﬁed
and we set logic rule weights λ to make the scale of the logic
contribution comparable to the LDA contribution in the ob-
jective function (4). Table 2 shows details such as the λ’s
used and the number of non-trivial groundings | ∪l G(ψl)|.
We set (Nouter, Ninner) to (102, 105) and run Gibbs sam-
plers for 2,000 samples. The Mir inner loop (Algorithm 1)
step size decays as ηm =
Ninner + m for inner
iteration m. We initialize all Fold·all algorithms with the ﬁnal
collapsed Gibbs sample from standard LDA, and ﬁx Dirichlet
parameters to α = 50/T and β = 0.01. We now present ex-
ample datasets and KBs along with qualitative assessments.
Synthetic Cannot-Link (Synth): This small synthetic
dataset demonstrates the ability of Fold·all to encode the
Cannot-Link preference [Andrzejewski et al., 2009], which
states that occurrences of a pair of words should not be as-
signed to the same topic. We encode Cannot-Link (A, B) as
W(i, A) ∧ W(j, B) ⇒ ¬Z(i, t) ∨ ¬Z(j, t) (the opposite Must-
Link can be encoded similarly). Alchemy and Fold·all are
able to enforce the KB, while standard LDA often does not.
Comp.* newsgroups (Comp): This dataset consists of on-
line posts made to comp.* news groups from 20 newsgroups.
We consider a user wishing to construct two separate topics
around the concepts hardware and software. Our KB encour-
ages the recovery of these topics using {hardware, machine,
memory, cpu} and {software, program, version, shareware}
as seed words in two rules: W(i, hardware)∨. . .∨W(i, cpu) ⇒
Z(i, 0) and W(i, software) ∨ . . . ∨ W(i, shareware) ⇒ Z(i, 1).
The topics found by Fold·all inference methods align with
our intended concepts: Topic 0 tends to consist of hardware-
related terms: {drive, disk, ide, bus, install}, while new Topic
1 terms are software-oriented: {code, image, data, analysis}.
Congress (Con): This dataset consists of ﬂoor-debate
transcripts from the United States House of Representa-
tives [Thomas et al., 2006]. Each speech is labeled with
the political party of the speaker: Speaker(d, Rep) or
Speaker(d, Dem). The predicate HasWord(d, w) is true
if word w appears in document d. We consider an analyst
wishing to identity interesting political topics using a KB con-
taining a seed word rule putting {chairman, yield, madam} in
Topic 0, as well as two rules exploiting political party labels:

Speaker(d, Rep) ∧ HasWord(d, taxes) ∧ D(i, d)

⇒ Z(i, 1) ∨ Z(i, 2) ∨ Z(i, 3)

Speaker(d, Dem) ∧ HasWord(d, workers) ∧ D(i, d)

⇒ Z(i, 4) ∨ Z(i, 5) ∨ Z(i, 6).

The ﬁrst rule pulls uninteresting procedural words (e.g., “Mr.
Chairman, I want to thank the gentlewoman for yielding...”)
into their own Topic 0. The other rules aim to discover inter-
esting political topics associated with Rep on taxes and Dem
on workers. As intended, Topic 0 pulls in other procedural
words ({gentleman, thank, colleague}), improving the qual-
ity of the other topics. The special Rep taxes topics uncover

Figure 2: Fold·all movie / ﬁlm topics.

interesting themes ({budget, billion, deﬁcit, health, educa-
tion, security, jobs, economy, growth}), as do the Dem work-
ers topics ({pension, beneﬁts, security, osha, safety, prices,
gas}). This KB demonstrates how Fold·all can exploit side
information to inﬂuence topic discovery.
Polarity (Pol): This dataset consists of positive and negative
movie reviews [Pang and Lee, 2004]. We posit an expert ana-
lyst who wishes to study the difference between usage of the
word “movie” versus the word “ﬁlm” in these reviews, plac-
ing Cannot-Link rule between those two words. The size of
the groundings is too large for all logic-based methods except
Mir, which is able to discover topics obeying the KB which
reveal subtle sentiment differences associated with the two
words (e.g., the movie topic contains “bad”, while the ﬁlm
topic contains “great”). Figure 2 shows word clouds1 for a
pair of Mir topics containing “ﬁlm” or “movie” only.
Human Development Genes (HDG): This dataset consists
of PubMed abstracts; the goal is to learn topics for six con-
cepts formulated by an actual biologist interested in human
development. The expert provided seed words for each con-
cept, which were translated into FOL rules. For example,
the Topic 2 seed words are {hematopoietic, blood, endothe-
lium}. However, using seed rules alone yields concept topics
which stray from basic biology, and are polluted by more clin-
ical terms such as {pressure, hypertension} and {leukemia,
acute, myeloid}. We therefore seed an additional disease
Topic 7 and enforce that this topic not co-occur in the same
sentence as our original development concept topic. Let
s = s1, . . . , sN be a vector of sentence indices analogous
to d, with logical predicate S(i, s) being true if si = s. Our
exclusion rule for the concept Topic 2 is

S(i, s) ∧ S(j, s) ∧ Z(i, 7) ⇒¬(Z(j, 1) ∨ . . . ∨ Z(j, 6)).
In order to further encourage the recovery of development-
oriented topics, we also deﬁne a development Topic 8 with
seed words {differentiation, . . ., develops}, and deﬁne an in-
clusion rule enforcing that our concept topics not be used

1http://www.wordle.net

Table 2: Fold·all generalization experiments, showing dataset and KB details along with objective function (4) values (with
magnitudes in parentheses) averaged over test folds. All bolded values are signiﬁcantly different from all non-bolded values in
the same row at p < 10−6 under Tukey’s Honestly Signiﬁcant Difference (HSD) test. Failing runs are indicated with “−”.

Fold·all

Synth (×101)
Comp (×105)
Con (×105)
Pol (×105)
HDG (×106)

Mir

M+L

CGS MWS

9.86
2.40
2.51
5.67
10.66

11.13
2.45
2.56
−
−

8.33
2.40
2.51
−
−

11.13
2.40
2.51
−
−

Baselines

LDA Alchemy
−1.73
−2.18
−
1.19
−
1.09
−
5.67
−
3.59

Dataset+KB details

D T
3
20
25
20
50

100
5000
2740
2000
24073

λ
1.5 × 10−1
1 × 103
1 × 102
2 × 100
1 × 10−5

| ∪l G(ψl)|
1.2 × 105
6.3 × 103
2.9 × 103
9.6 × 108
2.3 × 108

within a sentence unless Topic 8 also occurs (similar to to the
exclusion rule). This KB results in more “on-concept” topics,
including new terms {epo, peripheral, erythroid} for Topic 2.
While a full discussion is infeasible due to space constraints,
blind relevance judgments by our biological collaborator con-
ﬁrm the effectiveness of Fold·all in discovering new terms re-
lated to the target concepts [Andrzejewski, 2010].
4.1 Generalization
We assess the quality of the learned topics φ by examining
their generalization to unseen documents via cross validation.
At training time, we perform inference with one of the six
methods on the training documents and KB (if applicable)
to estimate the topic-word multinomials φ. At test time, we
hold φ ﬁxed and perform LDA-style inference over z on the
testing documents. Note the logic KB is not used during the
test phase, allowing us to see whether the KB “generalizes”
to the test corpus via the learned topics φ.

We measure such generalization by evaluating the joint
logic and LDA objective (4) on the test documents. The re-
sults are presented in Table 2, where each cell contains the
test set value of (4) averaged across folds. We collectively re-
fer to Mir, M+L, CGS, and MWS as Fold·all inference meth-
ods because they consider both LDA and logic components
of Fold·all, and the results show that they are indeed better at
optimizing the joint logic and LDA objective (4) than topic
modeling alone (LDA) or logic alone (Alchemy), and there-
fore better at integrating FOL into topic modeling.

We also directly examine the number of satisﬁed ground-
ings on held-aside test documents for both Mir and LDA.
Across all KBs and folds, the topics learned by Mir result in
the satisfaction of as many, or more, test set groundings than
the topics learned by standard LDA. For example, on the ﬁrst
test fold of Synth, inference with the standard LDA topics
satisﬁes 1,040 the 1,600 non-trivial Cannot-Link groundings,
while the topics learned using the KB (all Fold·all methods
plus Alchemy) result in the satisfaction of all 1,600 ground-
ings even though the KB is not used for test set inference.
Similar results across all experiments demonstrate that the
learned topics transfer KB inﬂuence to the new documents.
4.2 Scalability
We say that an experimental run fails (indicated by a “−” in
Table 2) if it does not complete within 24 hours on a stan-
dard workstation with a 2.33 GHz processor and 16 GB of

memory. For example, even though Pol is a relatively small
corpus with a straightforward KB, the two free variables i and
j cause the number of non-trivial groundings to grow O(N 2)
with corpus length N. This causes the failure of Fold·all in-
ference schemes which work directly with rule groundings
(MWS, M+L, and CGS). By sampling groundings, Mir is
able to learn topics across the range of datasets and KBs, even
in the presence of exponentially many groundings. Mir infer-
ence completes in roughly 5 minutes on the full HDG corpus,
consuming less than 5 GB memory.

5 Discussion
The Fold·all model can also reformulate some prior LDA ex-
tensions, although we stress that rule weights must be user-
supplied, not learned. Furthermore, inference for a logic-
based encoding may not be more efﬁcient than a custom in-
ference procedure tailored to the speciﬁc model.
Concept-Topic Model [Chemudugunta et al., 2008] ties
special concept topics to speciﬁc concepts by constraining
these special topics to only emit words from carefully cho-
sen subsets of the vocabulary. The rule Z(i, t) ⇒ W(i, wc1) ∨
W(i, wc2)∨ . . .∨ W(i, wcK) enforces that concept topic t only
emits the words wc1, wc2, . . . , wcK.
Hidden Markov Topic Model [Gruber et al., 2007] en-
forces that the same topic be used for an entire sentence,
allowing topic transitions only between sentences. Using
our previously introduced sentence predicate S(i, s), we can
express the intra-sentence topic consistency constraint as
S(i, s)∧S(j, s)∧Z(i, t) ⇒ Z(j, t). The probabilities of inter-
sentence topic transitions can be set by carefully choosing
weights for rules of the form S(i, s)∧¬S(i+1, s)∧Z(i, t) ⇒
Z(i + 1, t(cid:48)) for all transition pairs (t, t(cid:48)).
∆LDA [Andrzejewski et al., 2007], Discriminative LDA
[Lacoste-Julien et al., 2008], Labeled LDA [Ramage et
al., 2009] all allow the speciﬁcation of special “restricted”
topics which can be used only in specially labeled docu-
ments. The goal of this constraint is to encourage these spe-
cial topics to capture interesting patterns associated with the
document labels.
If topic t should only be used in docu-
ments with label (cid:96), we can encode this type of constraint as
Z(i, t) ∧ D(i, d) ⇒ HasLabel(d, (cid:96)).
We have introduced Fold·all and given some simple exam-
ples of how it can incorporate domain knowledge into topic
modeling. We have also provided a scalable inference solu-

tion Mir. Experimental results conﬁrm that the inﬂuence of
the user-deﬁned KB generalizes to unseen documents, and
that Mir enables inference when other approaches fail.
Future work could allow the formulation of relational infer-
ence problems in Fold·all by deﬁning additional query atoms
other than Z(i, t), as in general MLNs. For example, we could
deﬁne unobserved predicate Citation(d, d(cid:48)) to be true if
document d cites document d(cid:48). Another direction is to inves-
tigate the utility of Mir for general MLN MAP inference.

Acknowledgments
This work was performed under the auspices of the U.S. De-
partment of Energy by Lawrence Livermore National Labo-
ratory under Contract DE-AC52-07NA27344 (LLNL-CONF-
466413), with additional support from NSF IIS-0953219,
AFOSR FA9550-09-1-0313, and NIH/NLM R01 LM07050.
We would like to thank Ron Stewart for his participation in
the HDG experiments.

References
[Andrzejewski et al., 2007] D. Andrzejewski, A. Mulhern, B. Lib-
lit, and X. Zhu. Statistical debugging using latent topic models.
In ECML, pages 6–17. Springer-Verlag, 2007.

[Andrzejewski et al., 2009] D. Andrzejewski, X. Zhu,

and
Incorporating domain knowledge into topic
In ICML, pages 25–32.

M. Craven.
modeling via Dirichlet forest priors.
Omnipress, 2009.

[Andrzejewski, 2010] D. Andrzejewski.

Incorporating Domain
Knowledge in Latent Topic Models. PhD thesis, University of
Wisconsin–Madison, 2010.

[Beck and Teboulle, 2003] A. Beck and M. Teboulle. Mirror de-
scent and nonlinear projected subgradient methods for convex
optimization. Operations Research Letters, 31(3):167 – 175,
2003.

[Blei et al., 2003] D. Blei, A. Ng, and M. Jordan. Latent Dirichlet

allocation. JMLR, 3:993–1022, 2003.

[Chemudugunta et al., 2008] C. Chemudugunta, A. Holloway,
P. Smyth, and M. Steyvers. Modeling documents by combin-
ing semantic concepts with unsupervised statistical learning. In
ISWC, pages 229–244. Springer, 2008.

[Collins et al., 2008] M. Collins, A. Globerson, T. Koo, X. Car-
reras, and P.L. Bartlett. Exponentiated gradient algorithms for
conditional random ﬁelds and max-margin Markov networks.
JMLR, 9:1775–1822, 2008.

[Domingos and Lowd, 2009] P. Domingos and D. Lowd. Markov
logic: An interface layer for artiﬁcial intelligence. Synthesis Lec-
tures on Artiﬁcial Intelligence and Machine Learning, 3(1):1–
155, 2009.

[Gerrish and Blei, 2010] S. Gerrish and D. Blei. A language-based
approach to measuring scholarly impact. In ICML, pages 375–
382. Omnipress, 2010.

[Grifﬁths and Steyvers, 2004] T.L. Grifﬁths and M. Steyvers. Find-

ing scientiﬁc topics. PNAS, 101(Suppl 1):5228–5235, 2004.

[Gruber et al., 2007] A. Gruber, M. Rosen-Zvi, and Y. Weiss. Hid-
den topic Markov models. In AISTATS, pages 163–170. Omni-
press, 2007.

[Hu et al., 2011] Y. Hu, J. Boyd-Graber, and B. Satinoff. Interactive

topic modeling. In ACL. ACL, 2011.

[Huynh and Mooney, 2009] T.N. Huynh and R.J. Mooney. Max-
margin weight learning for Markov logic networks. In ECML-
PKDD, pages 564–579. Springer, 2009.

[Kersting et al., 2009] K. Kersting, B. Ahmadi, and S. Natarajan.
In UAI, pages 277–284. AUAI

Counting belief propagation.
Press, 2009.

[Kivinen and Warmuth, 1997] J. Kivinen and M.K. Warmuth. Ex-
ponentiated gradient versus gradient descent for linear predictors.
Information and Computation, 132(1):1–63, 1997.

[Kok et al., 2009] S. Kok, M. Sumner, M. Richardson, P. Singla,
H. Poon, D. Lowd, J. Wang, and P. Domingos. The Alchemy
System for Statistical Relational AI. Technical report, Depart-
ment of Computer Science and Engineering, University of Wash-
ington, Seattle, WA, 2009.

[Lacoste-Julien et al., 2008] S. Lacoste-Julien, F. Sha, and M. Jor-
dan. DiscLDA: Discriminative learning for dimensionality re-
duction and classiﬁcation. In NIPS, pages 897–904. MIT Press,
2008.

[Pang and Lee, 2004] B. Pang and L. Lee. A sentimental educa-
tion: Sentiment analysis using subjectivity summarization based
on minimum cuts. In ACL, pages 271–278. ACL, 2004.

[Petterson et al., 2010] J. Petterson, A. Smola, T. Caetano, W. Bun-
tine, and S. Narayanamurthy. Word features for latent Dirichlet
allocation. In NIPS, pages 1921–1929. MIT Press, 2010.

[Poon and Domingos, 2006] H. Poon and P. Domingos. Sound and
efﬁcient inference with probabilistic and deterministic dependen-
cies. In AAAI. AAAI Press, 2006.

[Ramage et al., 2009] D. Ramage, D. Hall, R. Nallapati, and C.D.
Manning. Labeled LDA: a supervised topic model for credit at-
tribution in multi-labeled corpora. In EMNLP, pages 248–256.
ACL, 2009.

[Richardson and Domingos, 2006] M. Richardson and P. Domin-
gos. Markov logic networks. Machine Learning, 62(1-2):107–
136, 2006.

[Riedel, 2008] S. Riedel. Improving the accuracy and efﬁciency of
MAP inference for Markov logic. In UAI, pages 468–475. AUAI
Press, 2008.

[Selman et al., 1995] B. Selman, H. Kautz, and B. Cohen. Local
search strategies for satisﬁability testing. In DIMACS: Series in
Discrete Mathematics and Theoretical Computer Science, pages
521–532. AMS, 1995.

[Shavlik and Natarajan, 2009] J. Shavlik and S. Natarajan. Speed-
ing up inference in Markov logic networks by preprocessing to
In IJCAI,
reduce the size of the resulting grounded network.
pages 1951–1956. Morgan Kaufmann, 2009.

[Singla and Domingos, 2008] P. Singla and P. Domingos. Lifted
ﬁrst-order belief propagation. In AAAI, pages 1094–1099. AAAI
Press, 2008.

[Thomas et al., 2006] M. Thomas, B. Pang, and L. Lee. Get out
the vote: Determining support or opposition from Congressional
ﬂoor-debate transcripts. In EMNLP, pages 327–335. ACL, 2006.
[Wang and Domingos, 2008] J. Wang and P. Domingos. Hybrid
Markov logic networks. In AAAI, pages 1106–1111. AAAI Press,
2008.

[Wang et al., 2009] C. Wang, D. Blei, and F. Li. Simultaneous im-
In CVPR, pages 1903–1910.

age classiﬁcation and annotation.
IEEE, 2009.

