NAACLHLT2009Semi-supervisedLearningforNaturalLanguageProcessingProceedingsoftheWorkshopJune4,2009Boulder,Coloradoii

ProductionandManufacturingbyOmnipressInc.2600AndersonStreetMadison,WI53707USAc(cid:13)2009TheAssociationforComputationalLinguisticsOrdercopiesofthisandotherACLproceedingsfrom:AssociationforComputationalLinguistics(ACL)209N.EighthStreetStroudsburg,PA18360USATel:+1-570-476-8006Fax:+1-570-476-0860acl@aclweb.orgISBN978-1-932432-38-1iii

IntroductionWelcometotheNAACLHLTWorkshoponSemi-supervisedLearningforNaturalLanguageProcessing!Willsemi-supervisedlearning(SSL)becomethenextde-factostandardforbuildingnaturallanguageprocessing(NLP)systems,justassupervisedlearninghastransformedtheﬁeldinthelastdecade?Orwillitremainasaniceideathatdoesn’talwaysworkinpractice?Semi-supervisedlearninghasbecomeanimportanttopicduetothepromisethathigh-qualitylabeleddataandabundantunlabeleddata,ifleveragedappropriately,canachievesuperiorperformanceatlowercost.Asresearchersinsemi-supervisedlearningreachcriticalmass,webelieveitistimetotakeastepbackandthinkbroadlyaboutwhetherwecandiscovergeneralinsightsfromthevarioustechniquesdevelopedfordifferentNLPtasks.ThegoalofthisworkshopistohelpbuildacommunityofSSL-NLPresearchersandfosterdiscussionsaboutinsights,speculations,andresults(bothpositiveandnegative)thatmayotherwisenotappearinatechnicalpaperatamajorconference.Inourcall-for-paper,weposedsomeopenquestions:1.ProblemStructure:WhatarethedifferentclassesofNLPproblemstructures(e.g.sequences,trees,N-bestlists)andwhatalgorithmsarebestsuitedforeachclass?Forinstance,cangraph-basedalgorithmsbesuccessfullyappliedtosequence-to-sequenceproblemslikemachinetranslation,orareself-trainingandfeature-basedmethodstheonlyreasonablechoicesfortheseproblems?2.BackgroundKnowledge:WhatkindsofNLP-speciﬁcbackgroundknowledgecanweexploittoaidsemi-supervisedlearning?Recentlearningparadigmssuchasconstraint-drivenlearningandprototypelearningtakeadvantageofourdomainknowledgeaboutparticularNLPtasks;theyrepresentamoveawayfrompurelydata-agnosticmethodsandaregoodexamplesofhowlinguisticintuitioncandrivealgorithmdevelopment.3.Scalability:NLPdata-setsareoftenlarge.Whatarethescalabilitychallengesandsolutionsforapplyingexistingsemi-supervisedlearningalgorithmstoNLPdata?4.EvaluationandNegativeResults:Whatcanwelearnfromnegativeresults?Canwemakeaneducatedguessastowhensemi-supervisedlearningmightoutperformsupervisedorunsupervisedlearningbasedonwhatweknowabouttheNLPproblem?5.ToUseorNotToUse:Shouldsemi-supervisedlearningonlybeemployedinlow-resourcelanguages/tasks(i.e.littlelabeleddata,muchunlabeleddata),orshouldweexpectgainseveninhigh-resourcescenarios(i.e.expectingsemi-supervisedlearningtoimproveonasupervisedsystemthatisalreadymorethan95%accurate)?Wereceived17submissionsandselected10papersafterarigorousreviewprocess.Thesepaperscoveravarietyoftasks,rangingfrominformationextractiontospeechrecognition.Someintroducenewtechniques,whileotherscomparedexistingmethodsunderavarietyofsituations.Wearepleasedtopresentthesepapersinthisvolume.OurworkshopwillbekickedoffwithakeynotetalkbyJasonEisner(JohnsHopkinsUniversity).Weiv

willendwithapaneldiscussiononthefutureofSSL-NLP,whichwillfeatureinvitedpositionpapersfromseveralprominentresearchers.(Someareincludedinthisvolume;otherswillbeonlineattheworkshopwebsite:http://sites.google.com/site/sslnlp/).Weareespeciallygratefultotheprogramcommitteefortheirhardworkandthepresentersfortheirexcellentpapers.Wewouldalsoliketothankthefollowingpeoplefortheirmanyhelpandsupport:HalDaume,SajibDasgupta,JasonEisner,NizarHabash,MarkHasegawa-Johnson,AndrewMcCallum,VincentNg,AnoopSarkar,EricRingger,andJerryZhu.Bestregards,QinIrisWang,KevinDuh,DekangLinSSL-NLPWorkshopOrganizers26April2009v

Organizers:QinIrisWang,AT&TKevinDuh,UniversityofWashingtonDekangLin,GoogleResearchProgramCommittee:StevenAbney(UniversityofMichigan,USA)YaseminAltun(MaxPlanckInstituteforBiologicalCybernetics,Germany)TimBaldwin(UniversityofMelbourne,Australia)ShaneBergsma(UniversityofAlberta,Canada)AntalvandenBosch(TilburgUniversity,TheNetherlands)JohnBlitzer(UCBerkeley,USA)Ming-WeiChang(UIUC,USA)WalterDaelemans(UniversityofAntwerp,Belgium)HalDaumeIII(UniversityofUtah,USA)KevinGimpel(CarnegieMellonUniversity,USA)AndrewGoldberg(UniversityofWisconsin,USA)LiangHuang(GoogleResearch,USA)RieJohnson[formerly,Ando](RJResearchConsulting)KatrinKirchhoff(UniversityofWashington,USA)PercyLiang(UCBerkeley,USA)GaryGeunbaeLee(POSTECH,Korea)Gina-AnneLevow(UniversityofChicago,USA)GideonMann(Google,USA)DavidMcClotsky(BrownUniversity,USA)RayMooney(UTAustin,USA)HweeTouNg(NationalUniversityofSingapore,Singapore)VincentNg(UTDallas,USA)MilesOsborne(UniversityofEdinburgh,UK)MariOstendorf(UniversityofWashington,USA)ChrisPinchak(UniversityofAlberta,Canada)DragomirRadev(UniversityofMichigan,USA)DanRoth(UIUC,USA)AnoopSarkar(SimonFraserUniversity,Canada)DaleSchuurmans(UniversityofAlberta,Canada)AkiraShimazu(JAIST,Japan)JunSuzuki(NTT,Japan)YeeWhyeTeh(UniversityCollegeLondon,UK)KristinaToutanova(MicrosoftResearch,USA)JasonWeston(NEC,USA)TongZhang(RutgersUniversity,USA)vi

MingZhou(MicrosoftResearchAsia,China)Xiaojin(Jerry)Zhu(UniversityofWisconsin,USA)InvitedSpeaker:JasonEisner,JohnsHopkinsUniversityTable of Contents

Coupling Semi-Supervised Learning of Categories and Relations

Andrew Carlson, Justin Betteridge, Estevam Rafael Hruschka Junior and Tom M. Mitchell . . . . . 1

Surrogate Learning - From Feature Independence to Semi-Supervised Classiﬁcation

Sriharsha Veeramachaneni and Ravi Kumar Kondadadi . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10

Keepin’ It Real: Semi-Supervised Learning with Realistic Tuning

Andrew B. Goldberg and Xiaojin Zhu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19

Is Unlabeled Data Suitable for Multiclass SVM-based Web Page Classiﬁcation?

Arkaitz Zubiaga, V´ıctor Fresno and Raquel Mart´ınez . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28

A Comparison of Structural Correspondence Learning and Self-training for Discriminative Parse Se-
lection

Barbara Plank . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37

Latent Dirichlet Allocation with Topic-in-Set Knowledge

David Andrzejewski and Xiaojin Zhu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43

An Analysis of Bootstrapping for the Recognition of Temporal Expressions

Jordi Poveda, Mihai Surdeanu and Jordi Turmo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49

A Simple Semi-supervised Algorithm For Named Entity Recognition

Wenhui Liao and Sriharsha Veeramachaneni . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58

Can One Language Bootstrap the Other: A Case Study on Event Extraction

Zheng Chen and Heng Ji . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66

On Semi-Supervised Learning of Gaussian Mixture Models for Phonetic Classiﬁcation

Jui-Ting Huang and Mark Hasegawa-Johnson . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75

Discriminative Models for Semi-Supervised Natural Language Learning

Sajib Dasgupta and Vincent Ng . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84

vii

Conference Program

Thursday, June 4, 2009

8:30–9:00

Coffee Service

9:00–9:10

Opening Remarks

9:10–10:10

Invited Talk by Jason Eisner

10:10–10:30 Coupling Semi-Supervised Learning of Categories and Relations

Andrew Carlson, Justin Betteridge, Estevam Rafael Hruschka Junior and Tom M.
Mitchell

10:30–11:00 Morning Break

11:00–11:20

Surrogate Learning - From Feature Independence to Semi-Supervised Classiﬁcation
Sriharsha Veeramachaneni and Ravi Kumar Kondadadi

11:25–11:45 Keepin’ It Real: Semi-Supervised Learning with Realistic Tuning

Andrew B. Goldberg and Xiaojin Zhu

11:50–12:10

Is Unlabeled Data Suitable for Multiclass SVM-based Web Page Classiﬁcation?
Arkaitz Zubiaga, V´ıctor Fresno and Raquel Mart´ınez

12:15–12:35

A Comparison of Structural Correspondence Learning and Self-training for Dis-
criminative Parse Selection
Barbara Plank

12:35–2:00

Lunch Break

2:00–2:20

2:25–2:45

2:50–3:10

Latent Dirichlet Allocation with Topic-in-Set Knowledge
David Andrzejewski and Xiaojin Zhu

An Analysis of Bootstrapping for the Recognition of Temporal Expressions
Jordi Poveda, Mihai Surdeanu and Jordi Turmo

A Simple Semi-supervised Algorithm For Named Entity Recognition
Wenhui Liao and Sriharsha Veeramachaneni

ix

Thursday, June 4, 2009 (continued)

3:15–3:35

Can One Language Bootstrap the Other: A Case Study on Event Extraction
Zheng Chen and Heng Ji

3:35–4:00

Afternoon Break

4:00–4:20

On Semi-Supervised Learning of Gaussian Mixture Models for Phonetic Classiﬁcation
Jui-Ting Huang and Mark Hasegawa-Johnson

4:25–5:25

Panel Discusstion

Discriminative Models for Semi-Supervised Natural Language Learning
Sajib Dasgupta and Vincent Ng

5:25–5:40

Workshop Wrap-up

x

Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 1–9,

Boulder, Colorado, June 2009. c(cid:13)2009 Association for Computational Linguistics

1

CouplingSemi-SupervisedLearningofCategoriesandRelationsAndrewCarlson1,JustinBetteridge1,EstevamR.HruschkaJr.1,2andTomM.Mitchell11SchoolofComputerScienceCarnegieMellonUniversityPittsburgh,PA15213{acarlson,jbetter,tom.mitchell}@cs.cmu.edu2FederalUniversityofSaoCarlosSaoCarlos,SP-Brazilestevam@dc.ufscar.brAbstractWeconsidersemi-supervisedlearningofinformationextractionmethods,especiallyforextractinginstancesofnouncategories(e.g.,‘athlete,’‘team’)andrelations(e.g.,‘playsForTeam(athlete,team)’).Semi-supervisedapproachesusingasmallnumberoflabeledexamplestogetherwithmanyun-labeledexamplesareoftenunreliableastheyfrequentlyproduceaninternallyconsistent,butneverthelessincorrectsetofextractions.Weproposethatthisproblemcanbeover-comebysimultaneouslylearningclassiﬁersformanydifferentcategoriesandrelationsinthepresenceofanontologydeﬁningconstraintsthatcouplethetrainingoftheseclassiﬁers.Experimentalresultsshowthatsimultaneouslylearningacoupledcollectionofclassiﬁersfor30categoriesandrelationsresultsinmuchmoreaccurateextractionsthantrainingclassiﬁersindividually.1IntroductionAgreatwealthofknowledgeisexpressedonthewebinnaturallanguage.Translatingthisintoastruc-turedknowledgebasecontainingfactsaboutenti-ties(e.g.,‘Disney’)andrelationsbetweenthoseen-tities(e.g.CompanyIndustry(‘Disney’,‘entertain-ment’))wouldbeofgreatusetomanyapplications.Althoughfullysupervisedmethodsforlearningtoextractsuchfactsfromtextworkwell,thecostofcollectingmanylabeledexamplesofeachtypeofknowledgetobeextractedisimpractical.Re-searchershavealsoexploredsemi-supervisedlearn-ingmethodsthatrelyprimarilyonunlabeleddata,Figure1:Weshowthatsigniﬁcantimprovementsinac-curacyresultfromcouplingthetrainingofinformationextractorsformanyinter-relatedcategoriesandrelations(B),comparedwiththesimplerbutmuchmoredifﬁculttaskoflearningasingleinformationextractor(A).buttheseapproachestendtosufferfromthefactthattheyfaceanunder-constrainedlearningtask,result-inginextractionsthatareofteninaccurate.Wepresentanapproachtosemi-supervisedlearn-ingthatyieldsmoreaccurateresultsbycouplingthetrainingofmanyinformationextractors.Theintu-itionbehindourapproach(summarizedinFigure1)isthatsemi-supervisedtrainingofasingletypeofextractorsuchas‘coach’ismuchmoredifﬁcultthansimultaneouslytrainingmanyextractorsthatcoveravarietyofinter-relatedentityandrelationtypes.Inparticular,priorknowledgeabouttherelation-shipsbetweenthesedifferententitiesandrelations(e.g.,that‘coach(x)’implies‘person(x)’and‘notsport(x)’)allowsunlabeleddatatobecomeamuchmoreusefulconstraintduringtraining.Althoughpreviousworkhascoupledthelearningofmultiplecategories,orusedstaticcategoryrec-ognizerstocheckargumentsforlearnedrelationex-2

tractors,ourworkistheﬁrstweknowoftocouplethesimultaneoussemi-supervisedtrainingofmulti-plecategoriesandrelations.Ourexperimentsshowthatthiscouplingresultsinmoreaccurateextrac-tions.Basedonourresultsreportedhere,wehy-pothesizethatsigniﬁcantaccuracyimprovementsininformationextractionwillbepossiblebycouplingthetrainingofhundredsorthousandsofextractors.2ProblemStatementItwillbehelpfultoﬁrstexplainouruseofcommonterms.Anontologyisacollectionofunaryandbi-narypredicates,alsocalledcategoriesandrelations,respectively.1Aninstanceofacategory,oracate-goryinstance,isanounphrase;aninstanceofarela-tion,orarelationinstance,isapairofnounphrases.Instancescanbepositiveornegativewithrespecttoaspeciﬁcpredicate,meaningthatthepredicateholdsordoesnotholdforthatparticularinstance.Apromotedinstanceisaninstancewhichouralgo-rithmbelievestobeapositiveinstanceofsomepred-icate.Alsoassociatedwithbothcategoriesandrela-tionsarepatterns:stringsoftokenswithplacehold-ers(e.g.,‘gameagainstarg1’and‘arg1,headcoachofarg2’).Apromotedpatternisapatternbelievedtobeahigh-probabilityindicatorforsomepredicate.Thechallengeaddressedbythisworkistolearnextractorstoautomaticallypopulatethecategoriesandrelationsofaspeciﬁedontologywithhigh-conﬁdenceinstances,startingfromafewseedpos-itiveinstancesandpatternsforeachpredicateandalargecorpusofsentencesannotatedwithpart-of-speech(POS)tags.Wefocusonextractingfactsthatarestatedmultipletimesinthecorpus,whichwecanassessprobabilisticallyusingcorpusstatistics.Wedonotresolvestringstoreal-worldentities—theproblemsofsynonymresolutionanddisambiguationofstringsthatcanrefertomultipleentitiesareleftforfuturework.3RelatedWorkWorkonmultitasklearninghasdemonstratedthatsupervisedlearningofmultiple“related”functionstogethercanyieldhigheraccuracythanlearningthefunctionsseparately(Thrun,1996;Caruana,1997).Semi-supervisedmultitasklearninghasbeenshown1Wedonotconsiderpredicatesofhigherarityinthiswork.toincreaseaccuracywhentasksarerelated,allow-ingonetouseapriorthatencouragessimilarpa-rameters(Liuetal.,2008).Ourworkalsoinvolvessemi-supervisedtrainingofmultiplecoupledfunc-tions,butdiffersinthatweassumeexplicitpriorknowledgeoftheprecisewayinwhichourmulti-plefunctionsarerelated(e.g.,thatthevaluesofthefunctionsappliedtothesameinputaremutuallyex-clusive,orthatoneimpliestheother).Inthispaper,wefocusona‘bootstrapping’methodforsemi-supervisedlearning.Bootstrap-pingapproachesstartwithasmallnumberofla-beled‘seed’examples,usethoseseedexamplestotrainaninitialmodel,thenusethismodeltola-belsomeoftheunlabeleddata.Themodelisthenretrained,usingtheoriginalseedexamplesplustheself-labeledexamples.Thisprocessiterates,graduallyexpandingtheamountoflabeleddata.Suchapproacheshaveshownpromiseinapplica-tionssuchaswebpageclassiﬁcation(BlumandMitchell,1998),namedentityclassiﬁcation(CollinsandSinger,1999),parsing(McCloskyetal.,2006),andmachinetranslation(Uefﬁng,2006).Bootstrappingapproachestoinformationextrac-tioncanyieldimpressiveresultswithlittleinitialhumaneffort(Brin,1998;AgichteinandGravano,2000;RavichandranandHovy,2002;Pascaetal.,2006).However,aftermanyiterations,theyusu-allysufferfromsemanticdrift,whereerrorsinlabel-ingaccumulateandthelearnedconcept‘drifts’fromwhatwasintended(Curranetal.,2007).Couplingthelearningofpredicatesbyusingpositiveexam-plesofonepredicateasnegativeexamplesforoth-ershasbeenshowntohelplimitthisdrift(RiloffandJones,1999;Yangarber,2003).Additionally,ensur-ingthatrelationargumentsareofcertain,expectedtypescanhelpmitigatethepromotionofincorrectinstances(Pas¸caetal.,2006;RosenfeldandFeld-man,2007).Ourworkbuildsontheseideastocou-plethesimultaneousbootstrappedtrainingofmulti-plecategoriesandmultiplerelations.Ourapproachtoinformationextractionisbasedonusinghighprecisioncontextualpatterns(e.g.,‘ismayorofarg1’suggeststhatarg1isacity).Anearlypattern-basedapproachtoinformationextractionac-quired‘isa’relationsfromtextusinggenericcon-textualpatterns(Hearst,1992).ThisapproachwaslaterscaleduptothewebbyEtzionietal.(2005).3

Otherresearchexploresthetaskof‘openinforma-tionextraction’,wherethepredicatestobelearnedarenotspeciﬁedinadvance(ShinyamaandSekine,2006;Bankoetal.,2007),butemergeinsteadfromanalysisofthedata.Incontrast,ourapproachre-liesstronglyonknowledgeintheontologyaboutthepredicatestobelearned,andrelationshipsamongthem,inordertoachievehighaccuracy.Changetal.(2007)presentaframeworkforlearningthatoptimizesthedatalikelihoodplusconstraint-basedpenaltytermsthancapturepriorknowledge,anddemonstrateitwithsemi-supervisedlearningofsegmentationmodels.Constraintsthatcapturedomainknowledgeguidebootstraplearn-ingofastructuredmodelbypenalizingordisallow-ingviolationsofthoseconstraints.Whilesimilarinspirit,ourworkdiffersinthatweconsiderlearningmanymodels,ratherthanonestructuredmodel,andthatweareconsideramuchlargerscaleapplicationinadifferentdomain.4Approach4.1CouplingofPredicatesAsmentionedabove,ourapproachhingesontheno-tionofcouplingthelearningofmultiplefunctionsinordertoconstrainthesemi-supervisedlearningproblemweface.Oursystemlearnsfourdifferenttypesoffunctions.Foreachcategoryc:1.fc,inst:NP(C)→[0,1]2.fc,patt:PattC(C)→[0,1]andforeachrelationr:1.fr,inst:NP(C)×NP(C)→[0,1]2.fr,patt:PattR(C)→[0,1]whereCistheinputcorpus,NP(C)isthesetofvalidnounphrasesinC,PattC(C)isthesetofvalidcategorypatternsinC,andPattR(C)isthesetofvalidrelationpatternsinC.“Valid”nounphrases,categorypatterns,andrelationpatternsaredeﬁnedinSection4.2.2.Thelearningofthesefunctionsiscoupledintwoways:1.Sharingamongsame-aritypredicatesaccordingtologicalrelations2.Relationargumenttype-checkingThesemethodsofcouplingaremadepossiblebypriorknowledgeintheinputontology,beyondthelistsofcategoriesandrelationsmentionedabove.Weprovidegeneraldescriptionsofthesemethodsofcouplinginthenextsections,whilethedetailsaregiveninsection4.2.4.1.1Sharingamongsame-aritypredicatesEachpredicatePintheontologyhasalistofothersame-aritypredicateswithwhichPismutuallyexclusive,wheremutuallyExclusive(P,P0)≡(P(arg1)⇒¬P0(arg1))∧(P0(arg1)⇒¬P(arg1)),andsimilarlyforrelations.Thesemu-tuallyexclusiverelationshipsareusedtocarryoutthefollowingsimplebutcrucialcoupling:ifpredi-cateAismutuallyexclusivewithpredicateB,A’spositiveinstancesandpatternsbecomenegativein-stancesandnegativepatternsforB.Forexample,if‘city’,havinganinstance‘Boston’andapattern‘mayorofarg1’,ismutuallyexclusivewith‘scien-tist’,then‘Boston’and‘mayorofarg1’willbecomeanegativeinstanceandanegativepatternrespec-tivelyfor‘scientist.’Suchnegativeinstancesandpatternsprovidenegativeevidencetoconstrainthebootstrappingprocessandforestalldivergence.Somecategoriesaredeclaredtobeasubsetofoneoftheothercategoriesbeingpopulated,wheresubset(P,P0)≡P(arg1)⇒P0(arg1),(e.g.,‘ath-lete’isasubsetof‘person’).Thispriorknowledgeisusedtoshareinstancesandpatternsofthesubcat-egory(e.g.,‘athlete’)aspositiveinstancesandpat-ternsforthesuper-category(e.g.,‘person’).4.1.2Relationargumenttype-checkingThelasttypeofpriorknowledgeweusetocouplethelearningoffunctionsistypecheckinginforma-tionwhichcouplesthelearningofrelationswithcat-egories.Forexample,theargumentsofthe‘ceoOf’relationaredeclaredtobeofthecategories‘person’and‘company’.Ourapproachdoesnotpromoteapairofnounphrasesasaninstanceofarelationun-lessthetwonounphrasesareclassiﬁedasbelongingtothecorrectargumenttypes.Additionally,whenarelationinstanceispromoted,theargumentsbecomepromotedinstancesoftheirrespectivecategories.4.2AlgorithmDescriptionInthissection,wedescribeouralgorithm,CBL(CoupledBootstrapLearner),indetail.TheinputstoCBLarealargecorpusofPOS-taggedsentencesandaninitialontologywithpre-4

Algorithm1:CBLAlgorithmInput:AnontologyO,andtextcorpusCOutput:Trustedinstances/patternsforeachpredicateSHAREinitialinstances/patternsamongpredicates;fori=1,2,...,∞doforeachpredicatep∈OdoEXTRACTcandidateinstances/patterns;FILTERcandidates;TRAINinstance/patternclassiﬁers;ASSESScandidatesusingclassiﬁers;PROMOTEhighest-conﬁdencecandidates;endSHAREpromoteditemsamongpredicates;enddeﬁnedcategories,relations,mutuallyexclusivere-lationshipsbetweensame-aritypredicates,subsetre-lationshipsbetweensomecategories,seedinstancesforallpredicates,andseedpatternsforthecate-gories.Categoriesintheinputontologyalsohaveaﬂagindicatingwhetherinstancesmustbepropernouns,commonnouns,orwhethertheycanbeei-ther(e.g.,instancesof‘city’arepropernouns).Algorithm1givesasummaryoftheCBLalgo-rithm.First,seedinstancesandpatternsaresharedamongpredicatesusingtheavailablemutualexclu-sion,subset,andtype-checkingrelations.Then,foranindeﬁnitenumberofiterations,CBLexpandsthesetsofpromotedinstancesandpatternsforeachpredicate,asdetailedbelow.CBLwasdesignedtoallowlearningmanypred-icatessimultaneouslyfromalargesampleoftextfromtheweb.Ineachiterationofthealgorithm,theinformationneededfromthetextcorpusisgatheredintwopassesthroughthecorpususingtheMapRe-duceframework(DeanandGhemawat,2008).Thisallowsustocompleteaniterationofthesystemin1hourusingacorpuscontainingmillionsofwebpages(seeSection5.3fordetailsonthecorpus).4.2.1SharingAtthestartofexecution,seedinstancesandpat-ternsaresharedamongpredicatesaccordingtothemutualexclusion,subset,andtype-checkingcon-straints.Newlypromotedinstancesandpatternsaresharedattheendofeachiteration.4.2.2CandidateExtractionCBLﬁndsnewcandidateinstancesbyusingnewlypromotedpatternstoextractthenounphrasesthatco-occurwiththosepatternsinthetextcorpus.Tokeepthesizeofthissetmanageable,CBLlim-itsthenumberofnewcandidateinstancesforeachpredicateto1000byselectingtheonesthatoccurwiththemostnewlypromotedpatterns.Ananalo-gousprocedureisusedtoextractcandidatepatterns.CandidateextractionisperformedforallpredicatesinasinglepassthroughthecorpususingtheMapRe-duceframework.Thecandidateextractionprocedurehasdeﬁni-tionsforvalidinstancesandpatternsthatlimitex-tractiontoinstancesthatlooklikenounphrasesandpatternsthatarelikelytobeinformative.Hereweprovidebriefdescriptionsofthosedeﬁnitions.CategoryInstancesIntheplaceholderofacate-gorypattern,CBLlooksforanounphrase.Itusespart-of-speechtagstosegmentnounphrases,ignor-ingdeterminers.Propernounscontainingprepo-sitionsaresegmentedusingareimplementationoftheLexalgorithm(Downeyetal.,2007).Cate-goryinstancesareonlyextractediftheyobeytheproper/commonnounspeciﬁcationofthecategory.CategoryPatternsIfapromotedcategoryin-stanceisfoundinasentence,CBLextractsthepre-cedingwordsasacandidatepatterniftheyareverbsfollowedbyasequenceofadjectives,prepositions,ordeterminers(e.g.,‘beingacquiredbyarg1’)ornounsandadjectivesfollowedbyasequenceofad-jectives,prepositions,ordeterminers(e.g.,‘formerCEOofarg1’).CBLextractsthewordsfollowingtheinstanceasacandidatepatterniftheyareverbsfollowedoption-allybyanounphrase(e.g.,‘arg1brokethehomerunrecord’),orverbsfollowedbyapreposition(e.g.,‘arg1saidthat’).RelationInstancesIfapromotedrelationpattern(e.g.,‘arg1ismayorofarg2’)isfound,acandi-daterelationinstanceisextractedifbothplacehold-ersarevalidnounphrases,andiftheyobeytheproper/commonspeciﬁcationsfortheircategories.RelationPatternsIfbothargumentsfromapro-motedrelationinstancearefoundinasentencethen5

theinterveningsequenceofwordsisextractedasacandidaterelationpatternifitcontainsnomorethan5tokens,hasacontentword,hasanuncapitalizedword,andhasatleastonenon-noun.4.2.3CandidateFilteringCandidateinstancesandpatternsareﬁlteredtomaintainhighprecision,andtoavoidextremelyspe-ciﬁcpatterns.Aninstanceisonlyconsideredforas-sessmentifitco-occurswithatleasttwopromotedpatternsinthetextcorpus,andifitsco-occurrencecountwithallpromotedpatternsisatleastthreetimesgreaterthanitsco-occurrencecountwithneg-ativepatterns.Candidatepatternsareﬁlteredinthesamemannerusinginstances.Allco-occurrencecountsneededbytheﬁlteringstepareobtainedwithanadditionalpassthroughthecorpususingMapReduce.Thisimplementa-tionismuchmoreefﬁcientthanonethatreliesonwebsearchqueries.CBLtypicallyrequiresco-occurrencecountsofatleast10,000instanceswithanyofatleast10,000patterns,whichwouldrequire100millionhitcountqueries.4.2.4CandidateAssessmentNext,foreachpredicateCBLtrainsadiscretizedNa¨ıveBayesclassiﬁertoclassifythecandidatein-stances.Itsfeaturesincludepointwisemutualinfor-mation(PMI)scores(Turney,2001)ofthecandidateinstancewitheachofthepositiveandnegativepat-ternsassociatedwiththeclass.Thecurrentsetsofpromotedandnegativeinstancesareusedastrainingexamplesfortheclassiﬁer.Attributesarediscretizedbasedoninformationgain(FayyadandIrani,1993).Patternsareassessedusinganestimateofthepre-cisionofeachpatternp:Precision(p)=Pi∈Icount(i,p)count(p)whereIisthesetofpromotedinstancesforthepredicatecurrentlybeingconsidered,count(i,p)istheco-occurrencecountofinstanceiwithpatternp,andcount(p)isthehitcountofthepatternp.Thisisapessimisticestimatebecauseitassumesthattherestoftheoccurrencesofpatternparenotwithpos-itiveexamplesofthepredicate.Wealsopenalizeextremelyrarepatternsbythresholdingthedenomi-natorusingthe25thpercentilecandidatepatternhitcount(McDowellandCafarella,2006).Alloftheco-occurrencecountsneededfortheas-sessmentsteparecollectedinthesameMapReducepassasthoserequiredforﬁlteringcandidates.4.2.5CandidatePromotionCBLthenranksthecandidatesaccordingtotheirassessmentscoresandpromotesatmost100in-stancesand5patternsforeachpredicate.5ExperimentalEvaluationWedesignedourexperimentalevaluationtotrytoanswerthefollowingquestions:CanCBLiteratemanytimesandstillachievehighprecision?Howhelpfularethetypesofcouplingthatweemploy?Canweextendexistingsemanticresources?5.1ConﬁgurationsoftheAlgorithmWeranouralgorithminthreeconﬁgurations:•Full:ThealgorithmasdescribedinSection4.2.•NoSharingAmongSame-ArityPredicates(NS):Thisconﬁgurationcouplespredicatesonlyus-ingtype-checkingconstraints.Itusesthefullalgorithm,exceptthatpredicatesofthesamear-itydonotsharepromotedinstancesandpatternswitheachother.Seedinstancesandpatternsareshared,though,soeachpredicatehasasmall,ﬁxedpoolofnegativeevidence.•NoCategory/Relationcoupling(NCR):Thisconﬁgurationcouplespredicatesusingmutualexclusionandsubsetconstraints,butnottype-checking.Itusesthefullalgorithm,exceptthatrelationinstanceargumentsarenotﬁl-teredorassessedusingtheirspeciﬁedcategories,andargumentsofpromotedrelationsarenotsharedaspromotedinstancesofcategories.Theonlytype-checkinginformationusedisthecom-mon/propernounspeciﬁcationsofargumentsforﬁlteringoutimplausibleinstances.5.2InitialontologyOurontologycontainedcategoriesandrelationsre-latedtotwodomains:companiesandsports.Ex-tracategorieswereaddedtoprovidenegativeevi-dencetothedomain-relatedcategories:‘hobby’for‘economicsector’;‘actor,’‘politician,’and‘scien-tist’for‘athlete’and‘coach’;and‘boardgame’for‘sport’.Table1listseachpredicateintheleftmostcolumn.Categorieswerestartedwith10–20seed6

5iterations10iterations15iterationsPredicateFullNSNCRFullNSNCRFullNSNCRActor93100100939710010097100Athlete1001001001009310010073100BoardGame937693892793893093City10010010010097100100100100Coach1006373975343974747Company10010010097909710090100Country604060304327402340EconomicSector776373576767506340Hobby676367404057202330Person979790979397939793Politician939397735390905387Product9787909087100979077ProductType939390707397778067Scientist10090979763979360100Sport10090100936783972790SportsTeam1009710097701009050100CategoryAverage928489827084836379Acquired(Company,Company)777780678047706347CeoOf(Person,Company)9787100908797908083CoachesTeam(Coach,SportsTeam)1001001001001009710010090CompetesIn(Company,Econ.Sector)9797801009367976360CompetesWith(Company,Company)938060777037706043HasOfﬁcesIn(Company,City)979340939027935730HasOperationsIn(Company,Country)10095501009740908313HeadquarteredIn(Company,City)77902070772770607LocatedIn(City,Country)906757635043735030PlaysFor(Athlete,SportsTeam)1001000100977100430PlaysSport(Athlete,Sport)100100279380101004030TeamPlaysSport(SportsTeam,Sport)100100771009780938367Produces(Company,Product)918390839367938057HasType(Product,ProductType)736317336733405727RelationAverage928857848448846642All928674837668846462Table1:Precision(%)foreachpredicate.Resultsarepresentedafter5,10,and15iterations,fortheFull,NoSharing(NS),andNoCategory/RelationCoupling(NCR)conﬁgurationsofCBL.NotethatweexpectFullandNCRtoperformsimilarlyforcategories,butforFulltooutperformNCRonrelationsandforFulltooutperformNSonbothcategoriesandrelations.7

instancesand5seedpatterns.Theseedinstanceswerespeciﬁedbyahuman,andtheseedpatternswerederivedfromthegenericpatternsofHearstforeachpredicate(Hearst,1992).Relationswerestartedwithsimilarnumbersofseedinstances,andnoseedpatterns(itislessobvioushowtogener-ategoodseedpatternsfromrelationnames).Mostpredicatesweredeclaredasmutuallyexclusivewithmostothers,exceptforspecialcases(e.g.,‘hobby’and‘sport’;‘university’and‘sportsteam’;and‘hasofﬁcesin’and‘headquarteredin’).5.3CorpusOurtextcorpuswasfroma200-millionpagewebcrawl.WeparsedtheHTML,ﬁlteredoutnon-Englishpagesusingastopwordratiothreshold,thenﬁlteredoutwebspamandadultcontentusinga‘badword’list.Thepageswerethensegmentedintosen-tences,tokenized,andtaggedwithparts-of-speechusingtheOpenNLPpackage.Finally,weﬁlteredthesentencestoeliminatethosethatwerelikelytobenoisyandnotusefulforlearning(e.g.,sentenceswithoutaverb,withoutanylowercasewords,withtoomanywordsthatwereallcapitalletters).Thisyieldedacorpusofroughly514-millionsentences.5.4ExperimentalProcedureWeraneachconﬁgurationfor15iterations.Toeval-uatetheprecisionofpromotedinstances,wesam-pled30instancesfromthepromotedsetforeachpredicateineachconﬁgurationafter5,10,and15it-erations,pooledtogetherthesamplesforeachpred-icate,andthenjudgedtheircorrectness.Thejudgedidnotknowwhichrunaninstancewassampledfrom.Weestimatedtheprecisionofthepromotedinstancesfromeachrunafter5,10,and15itera-tionsasthenumberofcorrectpromotedinstancesdividedbythenumbersampled.Whilesamplesof30instancesdonotproducetightconﬁdenceinter-valsaroundindividualestimates,theyaresufﬁcientfortestingfortheeffectsinwhichweareinterested.5.5ResultsTable1showstheprecisionofeachofthethreeal-gorithmconﬁgurationsforeachcategoryandrela-tionafter5,10,and15iterations.Asisapparentinthistable,fullycoupledtraining(Full)outper-formstrainingwhencouplingisremovedbetweencategoriesandrelations(NCR),andalsowhencou-plingisremovedamongpredicatesofthesamear-ity(NS).Theneteffectissubstantial,asisappar-entfromthebottomrowofTable1,whichshowsthattheprecisionofFulloutperformsNSby6%andNCRby18%aftertheﬁrst5iterations,andbyanevenlarger20%and22%after15iterations.Thisincreasinggapinprecisionasiterationsincreasere-ﬂectstheabilityofcoupledlearningtoconstrainthesystemtoreducetheotherwisecommondriftasso-ciatedwithself-trainedclassiﬁers.UsingStudent’spairedt-test,wefoundthatforcategories,thedifferenceinperformancebetweenFullandNSisstatisticallysigniﬁcantafter5,10,and15iterations(p-value<0.05).2NosigniﬁcantdifferencewasfoundbetweenFullandNCRforcat-egories,butthisisnotasurprise,becauseNCRstillusesmutuallyexclusiveandsubsetconstraints.ThesametestﬁndsthatthedifferencesbetweenFullandNSaresigniﬁcantforrelationsafter15iterations,andthedifferencesbetweenFullandNCRaresig-niﬁcantafter5,10,and15iterationsforrelations.Theworst-performingcategoriesafter15itera-tionsofFullare‘country,’‘economicsector,’and‘hobby.’TheFullconﬁgurationofCBLpromoted1637instancesfor‘country,’farmorethanthenum-berofcorrectanswers.Manyofthesearegeneralgeographicregionslike‘BayﬁeldPeninsula’and‘BalticRepublics.’Inthe‘hobby’case,promotingpatternslike‘thetypesofarg1’ledtothecategorydriftingintoagenerallistofpluralcommonnouns.‘Economicsector’driftedintoacademicﬁeldslike‘BehavioralScience’and‘PoliticalSciences.’Weexpectthatthelearningofthesecategorieswouldbesigniﬁcantlybetteriftherewereevenmorecat-egoriesbeinglearnedtoprovideadditionalnegativeevidenceduringtheﬁlteringandassessmentstepsofthealgorithm.Atthisstageofdevelopment,obtaininghighre-callisnotaprioritybecauseourintentistocreateacontinuouslyrunningandcontinuouslyimprovingsystem;itisourhopethathighrecallwillcomewithtime.However,toveryroughlyconveythecom-pletenessofthecurrentresultsweshowinTable2theaveragenumberofinstancespromotedforcate-2Ourselectionofthepairedt-testwasmotivatedbytheworkofSmuckeretal.(2007),buttheWilcoxonsignedranktestgivesthesameresults.8

CategoriesRelationsConﬁgurationInstancesPrec.InstancesPrec.Full9708319184NS13376330766NCR9167945842Table2:Averagenumbersofpromotedcategoryandre-lationinstancesandestimatesoftheirprecisionforeachconﬁgurationofCBLafter15iterations.Figure2:ExtractedfactsfortwocompaniesdiscoveredbyCBLFull.Thesetwocompanieswereextractedbythelearned‘company’extractor,andtherelationsshownwereextractedbylearnedrelationextractors.goriesandrelationsforeachofthethreeconﬁgura-tionsofCBLafter15iterations.Forcategories,notsharingexamplesresultsinfewernegativeexamplesduringtheﬁlteringandassessmentsteps.Thisyieldsmorepromotedinstancesonaverage.Forrelations,notusingtypecheckingyieldshigherrelativerecall,butatamuchlowerlevelofprecision.Figure2givesoneviewofthetypeofinformationextractedbythecollectionoflearnedcategoryandrelationclassiﬁers.NotetheinitialseedexamplesprovidedtoCBLdidnotincludeinformationabouteithercompanyoranyoftheserelationinstances.35.6ComparisontoanExistingDatabaseToestimatethecapacityofouralgorithmtocon-tributeadditionalfactstopubliclyavailableseman-ticresources,wecomparedthecompletelistsofin-stancespromotedduringtheFull15iterationrunforcertaincategoriestocorrespondinglistsintheFreebasedatabase(MetawebTechnologies,2009).Excludingthecategoriesthatdidnothaveadi-rectlycorrespondingFreebaselist,wecomputedforeachcategory:Precision×|CBLInstances|−|Matches|,wherePrecisionistheestimatedpre-cisionfromourrandomsampleof30instances,|CBLInstances|isthetotalnumberofinstancespromotedforthatcategory,and|Matches|isthe3Seehttp://rtw.ml.cmu.edu/sslnlp09forre-sultsfromafullrunofthesystem.Est.CBLFreebaseEst.NewCategoryPrec.InstancesMatchesInstancesActor10052246557Athlete1001175463BoardGame8918610City10017991665134Company1001937995942Econ.Sector501541137634Politician9096274792Product97125901221SportsTeam90414139234Sport97613134461Table3:Estimatednumbersof“newinstances”(correctinstancespromotedbyCBLintheFull15iterationrunwhichdonothaveamatchinFreebase)andthevaluesusedincalculatingthem.numberofpromotedinstancesthathadanexactmatchinFreebase.Whileexactmatchesmayunder-estimatethenumberofmatches,itshouldbenotedthatratherthanmakedeﬁnitiveclaims,ourintenthereissimplytogiveroughestimates,whichareshowninTable3.Theseapproximatenumbersin-dicateapotentialtouseCBLtoextendexistingse-manticresourceslikeFreebase.6ConclusionWehavepresentedamethodofcouplingthesemi-supervisedlearningofcategoriesandrelationsanddemonstratedempiricallythatthecouplingforestallstheproblemofsemanticdriftassociatedwithboot-straplearningmethods.Wesuspectthatlearningadditionalpredicatessimultaneouslywillyieldevenmoreaccuratelearning.Anapproximatecompari-sonwithanexistingrepositoryofsemanticknowl-edge,Freebase,suggeststhatourmethodscancon-tributenewfactstoexistingresources.AcknowledgmentsThisworkissupportedinpartbyDARPA,Google,aYahoo!FellowshiptoAndrewCarlson,andtheBrazilianresearchagencyCNPq.WealsogratefullyacknowledgeJamieCallanformakingavailablehiscollectionofwebpages,Yahoo!foruseoftheirM45computingcluster,andtheanonymousreviewersfortheircomments.9

ReferencesEugeneAgichteinandLuisGravano.2000.Snowball:Extractingrelationsfromlargeplain-textcollections.InJCDL.MicheleBanko,MichaelJ.Cafarella,StephenSoderland,MattBroadhead,andOrenEtzioni.2007.Openinfor-mationextractionfromtheweb.InIJCAI.AvrimBlumandTomMitchell.1998.Combiningla-beledandunlabeleddatawithco-training.InCOLT.SergeyBrin.1998.Extractingpatternsandrelationsfromtheworldwideweb.InWebDBWorkshopat6thInternationalConferenceonExtendingDatabaseTechnology.RichCaruana.1997.Multitasklearning.MachineLearning,28:41–75.Ming-WeiChang,Lev-ArieRatinov,andDanRoth.2007.Guidingsemi-supervisionwithconstraint-drivenlearning.InACL.MichaelCollinsandYoramSinger.1999.Unsupervisedmodelsfornamedentityclassiﬁcation.InEMNLP.JamesR.Curran,TaraMurphy,andBernhardScholz.2007.Minimisingsemanticdriftwithmutualexclu-sionbootstrapping.InPACLING.JeffreyDeanandSanjayGhemawat.2008.Mapreduce:simpliﬁeddataprocessingonlargeclusters.Commun.ACM,51(1):107–113.DougDowney,MatthewBroadhead,andOrenEtzioni.2007.Locatingcomplexnamedentitiesinwebtext.InIJCAI.OrenEtzioni,MichaelCafarella,DougDowney,Ana-MariaPopescu,TalShaked,StephenSoderland,DanielS.Weld,andAlexanderYates.2005.Unsu-pervisednamed-entityextractionfromtheweb:anex-perimentalstudy.Artif.Intell.,165(1):91–134.UsamaM.FayyadandKekiB.Irani.1993.Multi-intervaldiscretizationofcontinuous-valuedattributesforclassiﬁcationlearning.InUAI.MartiA.Hearst.1992.Automaticacquisitionofhy-ponymsfromlargetextcorpora.InCOLING.QiuhuaLiu,XuejunLiao,andLawrenceCarin.2008.Semi-supervisedmultitasklearning.InNIPS.DavidMcClosky,EugeneCharniak,andMarkJohnson.2006.Effectiveself-trainingforparsing.InNAACL.LukeK.McDowellandMichaelCafarella.2006.Ontology-driveninformationextractionwithon-tosyphon.InISWC.MetawebTechnologies.2009.Freebasedatadumps.http://download.freebase.com/datadumps/.MariusPas¸ca,DekangLin,JeffreyBigham,AndreiLif-chits,andAlpaJain.2006.Namesandsimilaritiesontheweb:factextractioninthefastlane.InACL.MariusPasca,DekangLin,JeffreyBigham,AndreiLif-chits,andAlpaJain.2006.Organizingandsearch-ingtheworldwideweboffacts-stepone:Theone-millionfactextractionchallenge.InAAAI.DeepakRavichandranandEduardHovy.2002.Learningsurfacetextpatternsforaquestionansweringsystem.InACL.EllenRiloffandRosieJones.1999.Learningdictionar-iesforinformationextractionbymulti-levelbootstrap-ping.InAAAI.BenjaminRosenfeldandRonenFeldman.2007.Us-ingcorpusstatisticsonentitiestoimprovesemi-supervisedrelationextractionfromtheweb.InACL.YusukeShinyamaandSatoshiSekine.2006.Preemp-tiveinformationextractionusingunrestrictedrelationdiscovery.InHLT-NAACL.MarkD.Smucker,JamesAllan,andBenCarterette.2007.Acomparisonofstatisticalsigniﬁcancetestsforinformationretrievalevaluation.InCIKM.SebastianThrun.1996.Islearningthen-ththinganyeasierthanlearningtheﬁrst?InNIPS.PeterD.Turney.2001.Miningthewebforsynonyms:Pmi-irversuslsaontoeﬂ.InEMCL.NicolaUefﬁng.2006.Self-trainingformachinetrans-lation.InNIPSworkshoponMachineLearningforMultilingualInformationAccess.RomanYangarber.2003.Counter-trainingindiscoveryofsemanticpatterns.InACL.Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 10–18,

Boulder, Colorado, June 2009. c(cid:13)2009 Association for Computational Linguistics

10

SurrogateLearning-FromFeatureIndependencetoSemi-SupervisedClassiﬁcationSriharshaVeeramachaneniandRaviKumarKondadadiThomsonReutersResearchandDevelopmentEagan,MN55123,USA[harsha.veeramachaneni,ravikumar.kondadadi]@thomsonreuters.comAbstractWeconsiderthetaskoflearningaclassi-ﬁerfromthefeaturespaceXtothesetofclassesY={0,1},whenthefeaturescanbepartitionedintoclass-conditionallyinde-pendentfeaturesetsX1andX2.Weshowthattheclass-conditionalindependencecanbeusedtorepresenttheoriginallearningtaskintermsof1)learningaclassiﬁerfromX2toX1(inthesenseofestimatingtheprob-abilityP(x1|x2))and2)learningtheclass-conditionaldistributionofthefeaturesetX1.Thisfactcanbeexploitedforsemi-supervisedlearningbecausetheformertaskcanbeac-complishedpurelyfromunlabeledsamples.Wepresentexperimentalevaluationoftheideaintworealworldapplications.1IntroductionSemi-supervisedlearningissaidtooccurwhenthelearnerexploits(apresumablylargequantityof)un-labeleddatatosupplementarelativelysmalllabeledsample,foraccurateinduction.Thehighcostofla-beleddataandthesimultaneousplenitudeofunla-beleddatainmanyapplicationdomains,hasledtoconsiderableinterestinsemi-supervisedlearninginrecentyears(Chapelleetal.,2006).Weshowasomewhatsurprisingconsequenceofclass-conditionalfeatureindependencethatleadstoaprincipledandeasilyimplementablesemi-supervisedlearningalgorithm.Whenthefeaturesetcanbepartitionedintotwoclass-conditionallyin-dependentsets,weshowthattheoriginallearningproblemcanbereformulatedintermsoftheproblemoflearningaﬁrstpredictorfromoneofthepartitionstotheother,plusasecondpredictorfromthelatterpartitiontoclasslabel.Thatis,thelatterpartitionactsasasurrogatefortheclassvariable.Assum-ingthatthesecondpredictorcanbelearnedfromarelativelysmalllabeledsamplethisresultsinaneffectivesemi-supervisedalgorithm,sincetheﬁrstpredictorcanbelearnedfromonlyunlabeledsam-ples.Inthenextsectionwepresentthesimpleyetin-terestingresultonwhichoursemi-supervisedlearn-ingalgorithm(whichwecallsurrogatelearning)isbased.Wepresentexamplestoclarifytheintuitionbehindtheapproachandpresentaspecialcaseofourapproachthatisusedintheapplicationssection.Wethenexaminerelatedideasinpreviousworkandsituateouralgorithmamongpreviousapproachestosemi-supervisedlearning.Wepresentempiricalevaluationontworealworldapplicationswheretherequiredassumptionsofouralgorithmaresatisﬁed.2SurrogateLearningWeconsidertheproblemoflearningaclassiﬁerfromthefeaturespaceXtothesetofclassesY={0,1}.LetthefeaturesbepartitionedintoX=X1×X2.Therandomfeaturevectorx∈Xwillberepresentedcorrespondinglyasx=(x1,x2).Sincewerestrictourconsiderationtoatwo-classproblem,theconstructionoftheclassiﬁerinvolvestheesti-mationoftheprobabilityP(y=0|x1,x2)ateverypoint(x1,x2)∈X.Wemakethefollowingassumptionsonthejointprobabilitiesoftheclassesandfeatures.11

1.P(x1,x2|y)=P(x1|y)P(x2|y)fory∈{0,1}.Thatis,thefeaturesetsx1andx2areclass-conditionallyindependentforbothclasses.Notethat,whenX1andX2areone-dimensional,thisconditionisidenticaltotheNaiveBayesassumption,althoughingeneralourassumptionisweaker.2.P(x1|x2)6=0,P(x1|y)6=0andP(x1|y=0)6=P(x1|y=1).Theseassumptionsaretoavoiddivide-by-zeroproblemsinthealge-brabelow.Ifx1isadiscretevaluedrandomvariableandnotirrelevantfortheclassiﬁcationtask,theseconditionsareoftensatisﬁed.WecannowshowthatP(y=0|x1,x2)canbewrittenasafunctionofP(x1|x2)andP(x1|y).WhenweconsiderthequantityP(y,x1|x2),wemayderivethefollowing.P(y,x1|x2)=P(x1|y,x2)P(y|x2)⇒P(y,x1|x2)=P(x1|y)P(y|x2)(fromtheindependenceassumption)⇒P(y|x1,x2)P(x1|x2)=P(x1|y)P(y|x2)⇒P(y|x1,x2)P(x1|x2)P(x1|y)=P(y|x2)(1)SinceP(y=0|x2)+P(y=1|x2)=1,Equa-tion1impliesP(y=0|x1,x2)P(x1|x2)P(x1|y=0)+P(y=1|x1,x2)P(x1|x2)P(x1|y=1)=1⇒P(y=0|x1,x2)P(x1|x2)P(x1|y=0)+(1−P(y=0|x1,x2))P(x1|x2)P(x1|y=1)=1(2)SolvingEquation2forP(y=0|x1,x2),weob-tainP(y=0|x1,x2)=P(x1|y=0)P(x1|x2)·P(x1|y=1)−P(x1|x2)P(x1|y=1)−P(x1|y=0)(3)WehavesucceededinwritingP(y=0|x1,x2)asafunctionofP(x1|x2)andP(x1|y).AlthoughthisresultwaspreviouslyobservedinadifferentcontextbyAbneyin(Abney,2002),hedoesnotuseittoderiveasemi-supervisedlearningalgorithm.Thisresultcanleadtoasigniﬁcantsimpliﬁcationofthelearningtaskwhenalargeamountofunlabeleddataisavailable.Thesemi-supervisedlearningalgorithminvolvesthefollowingtwosteps.1.FromunlabeleddatalearnapredictorfromthefeaturespaceX2tothespaceX1topredictP(x1|x2).Thereisnorestrictiononthelearnerthatcanbeusedaslongasitoutputsposteriorclassprobabilityestimates.2.EstimatethequantityP(x1|y)fromalabeledsamples.Incasex1isﬁnitevalued,thiscanbedonebyjustcounting.IfX1haslowcar-dinalitytheestimationproblemrequiresveryfewlabeledsamples.Forexample,ifx1isbinary,thenestimatingP(x1|y)involvesesti-matingjusttwoBernoulliprobabilities.Thus,wecandecouplethepredictionproblemintotwoseparatetasks,oneofwhichinvolvespredict-ingx1fromtheremainingfeatures.Inotherwords,x1servesasasurrogatefortheclasslabel.Fur-thermore,forthetwostepsabovethereisnoneces-sityforcompletesamples.Thelabeledexamplescanhavethefeaturex2missing.Attesttime,aninputsample(x1,x2)isclassiﬁedbycomputingP(x1|y)andP(x1|x2)fromthepre-dictorsobtainedfromtraining,andpluggingthesevaluesintoEquation3.Notethatthesetwoquanti-tiesarecomputedfortheactualvalueofx1takenbytheinputsample.Thefollowingexampleillustratessurrogatelearn-ing.—————————————–Example1Considerthefollowingvariationonaproblemfrom(Dudaetal.,2000)ofclassifyingﬁshonacon-veryorbeltaseithersalmon(y=0)orseabass(y=1).Thefeaturesdescribingtheﬁsharex1,abinaryfeaturedescribingwhethertheﬁshislight(x1=0)ordark(x1=1),andx2describesthelengthoftheﬁshwhichisreal-valued.Assume(un-realistically)thatP(x2|y),theclass-conditionaldis-tributionofx2,thelengthforsalmonisGaussian,12

andfortheseabassisLaplacianasshowninFig-ure1.−4−202400.5x2P(x2|y=0)P(x2|y=1)Figure1:Class-conditionalprobabilitydistributionsofthefeaturex2.Becauseoftheclass-conditionalfeaturein-dependenceassumption,thejointdistributionP(x1,x2,y)=P(x2|y)P(x1,y)cannowbecompletelyspeciﬁedbyﬁxingthejointprobabil-ityP(x1,y).LetP(x1=0,y=0)=0.3,P(x1=0,y=1)=0.1,P(x1=1,y=0)=0.2,andP(x1=1,y=1)=0.4.I.e.,asalmonismorelikelytobelightthandarkandaseabassismorelikelytobedarkthanlight.ThefulljointdistributionisdepictedinFigure2.AlsoshowninFigure2aretheconditionaldistribu-tionsP(x1=0|x2)andP(y=0|x1,x2).Assumethatwebuildapredictortodecidebe-tweenx1=lightandx1=darkfromthelengthus-ingadatasetofunlabeledﬁsh.Onarandomsalmon,thispredictorwillmostlikelydecidethatx1=light(because,forasalmon,x1=lightismorelikelythanx1=dark,andsimilarlyforaseabassthepredictoroftendecidesthatx1=dark.Conse-quentlythepredictorprovidesinformationaboutthetrueclasslabely.Thiscanalsobeseeninthesim-ilaritiesbetweenthecurvesP(y=0|x1,x2)tothecurveP(x1|x2)inFigure2.AnotherwaytointerprettheexampleistonotethatifapredictorforP(x1|x2)werebuiltononlythesalmonsthenP(x1=light|x2)willbeacon-stantvalue(0.6).SimilarlythevalueofP(x1=light|x2)forseabasseswillalsobeaconstantvalue(0.2).Thatis,thevalueofP(x1=light|x2)forasampleisagoodpredictorofitsclass.However,−4−20240.5x1 = 0x1 = 1x2P(x1=1,y=0,x2)P(x1=1,y=1,x2)P(x1=0,y=1,x2)P(x1=0,y=0,x2)P(y=0|x1=1,x2)P(y=0|x1=0,x2)P(x1=0|,x2)Figure2:Thejointdistributionsandtheposteriordistri-butionsoftheclassyandthesurrogateclassx1.surrogatelearningbuildsthepredictorP(x1|x2)onunlabeleddatafrombothtypesofﬁshandthere-foreadditionallyrequiresP(x1|y)toestimatetheboundarybetweentheclasses.2.1ASpecialCaseTheindependenceassumptionsmadeinthesettingabovemayseemtoostrongtoholdinrealproblems,especiallybecausethefeaturesetsarerequiredtobeclass-conditionallyindependentforbothclasses.Wenowspecializethesettingoftheclassiﬁcationproblemtotheonerealizedintheapplicationswepresentlater.WestillwishtolearnaclassiﬁerfromX=X1×X2tothesetofclassesY={0,1}.Wemakethefollowingslightlymodiﬁedassumptions.1.x1isabinaryrandomvariable.Thatis,X1={0,1}.2.P(x1,x2|y=0)=P(x1|y=0)P(x2|y=0).Werequirethatthefeaturex1beclass-conditionallyindependentoftheremainingfea-turesonlyfortheclassy=0.3.P(x1=0,y=1)=0.Thisassumptionsaysthatx1isa‘100%recall’featurefory=11.Assumption3simpliﬁesthelearningtasktotheestimationoftheprobabilityP(y=0|x1=1,x2)foreverypointx2∈X2.Wecanproceedasbefore1Thisassumptioncanbeseentotriviallyenforcetheinde-pendenceofthefeaturesforclassy=1.13

toobtaintheexpressioninEquation3.P(y=0|x1=1,x2)=P(x1=1|y=0)P(x1=1|x2)......P(x1=1|y=1)−P(x1=1|x2)P(x1=1|y=1)−P(x1=1|y=0)=P(x1=1|y=0)P(x1=1|x2)·1−P(x1=1|x2)1−P(x1=1|y=0)=P(x1=1|y=0)P(x1=1|x2)·P(x1=0|x2)P(x1=0|y=0)=P(x1=1|y=0)P(x1=0|y=0)·P(x1=0|x2)(1−P(x1=0|x2))(4)Equation4showsthatP(y=0|x1=1,x2)isamonotonicallyincreasingfunctionofP(x1=0|x2).ThismeansthatafterwebuildapredictorfromX2toX1,weonlyneedtoestablishthethresh-oldonP(x1=0|x2)toyieldtheoptimumclassi-ﬁcationbetweeny=0andy=1.Thereforethelearningproceedsasfollows.1.FromunlabeleddatalearnapredictorfromthefeaturespaceX2tothebinaryspaceX1topre-dictthequantityP(x1|x2).2.Uselabeledsampletoestablishthethresh-oldonP(x1=0|x2)toachievethedesiredprecision-recalltrade-offfortheoriginalclas-siﬁcationproblem.Becauseofourassumptions,forasamplefromclassy=0itisimpossibletopredictwhetherx1=0orx1=1betterthanrandombylookingatthex2feature,whereasasamplefromtheposi-tiveclassalwayshasx1=1.Thereforethesampleswithx1=0servetodelineatethepositiveexam-plesamongthesampleswithx1=1.Wethereforecallthesamplesthathavex1=1asthetargetsam-plesandthosethathavex1=0asthebackgroundsamples.3RelatedWorkAlthoughtheideaofusingunlabeleddatatoim-proveclassiﬁeraccuracyhasbeenaroundforseveraldecades(NagyandShelton,1966),semi-supervisedlearninghasreceivedmuchattentionrecentlyduetoimpressiveresultsinsomedomains.Thecom-pilationofchapterseditedbyChappelleetal.isanexcellentintroductiontothevariousapproachestosemi-supervisedlearning,andtherelatedpracticalandtheoreticalissues(Chapelleetal.,2006).Similartooursetup,co-trainingassumesthatthefeaturescanbesplitintotwoclass-conditionallyindependentsetsor‘views’(BlumandMitchell,1998).Alsoassumedisthesufﬁciencyofeitherviewforaccurateclassiﬁcation.Theco-trainingal-gorithmiterativelyusestheunlabeleddataclassiﬁedwithhighconﬁdencebytheclassiﬁerononeview,togeneratelabeleddataforlearningtheclassiﬁerontheother.Theintuitionunderlyingco-trainingisthattheer-rorscausedbytheclassiﬁerononeviewareinde-pendentoftheotherview,hencecanbeconceivedasuniform2noiseaddedtothetrainingexamplesfortheotherview.Consequently,thenumberofla-belerrorsinaregioninthefeaturespaceispropor-tionaltothenumberofsamplesintheregion.Iftheformerclassiﬁerisreasonablyaccurate,thepropor-tionallydistributederrorsare‘washedout’bythecorrectlylabeledexamplesforthelatterclassiﬁer.Seegershowedthatco-trainingcanalsobeviewedasaninstanceoftheExpectation-Maximizational-gorithm(Seeger,2000).Themaindistinctionofsurrogatelearningfromco-trainingisthelearningofapredictorfromoneviewtotheother,asopposedtolearningpredictorsfrombothviewstotheclasslabel.Wecanthere-foreeliminatetherequirementthatbothviewsbesufﬁcientlyinformativeforreasonablyaccuratepre-diction.Furthermore,unlikeco-training,surrogatelearninghasnoiterativecomponent.AndoandZhangproposeanalgorithmtoregu-larizethehypothesisspacebysimultaneouslycon-sideringmultipleclassiﬁcationtasksonthesamefeaturespace(AndoandZhang,2005).Theythenusetheirso-calledstructurallearningalgorithmforsemi-supervisedlearningofoneclassiﬁcationtask,bytheartiﬁcialconstructionof‘related’problemsonunlabeleddata.Thisisdonebycreatingprob-lemsofpredictingobservablefeaturesofthedataandlearningthestructuralregularizationparame-tersfromthese‘auxiliary’problemsandunlabeleddata.Morerecentlyin(AndoandZhang,2007)they2Whetherornotalabeliserroneousisindependentofthefeaturevaluesofthelatterview.14

showedthat,withconditionallyindependentfeaturesetspredictingfromonesettotheotherallowstheconstructionofafeaturerepresentationthatleadstoaneffectivesemi-supervisedlearningalgorithm.Ourapproachdirectlyoperatesontheoriginalfea-turespaceandcanbeviewedanotherjustiﬁcationforthealgorithmin(AndoandZhang,2005).MultipleInstanceLearning(MIL)isalearningsettingwheretrainingdataisprovidedaspositiveandnegativebagsofsamples(Dietterichetal.,1997).Anegativebagcontainsonlynegativeex-ampleswhereasapositivebagcontainsatleastonepositiveexample.SurrogatelearningcanbeviewedasartiﬁciallyconstructingaMILproblem,withthetargetsactingasonepositivebagandtheback-groundsactingasonenegativebag(Section2.1).Theclass-conditionalfeatureindependenceassump-tionforclassy=0translatestotheidenticalandindependentdistributionofthenegativesamplesinbothbags.4TwoApplicationsWeappliedthesurrogatelearningalgorithmtotheproblemsofrecordlinkageandparaphrasegenera-tion.Asweshallsee,theapplicationssatisfytheassumptionsinoursecond(100%recall)setting.4.1RecordLinkage/EntityResolutionRecordlinkageistheprocessofidentiﬁcationandmergingofrecordsofthesameentityindifferentdatabasesortheuniﬁcationofrecordsinasingledatabase,andconstitutesanimportantcomponentofdatamanagement.Thereaderisreferredto(Win-kler,1995)foranoverviewoftherecordlinkageproblem,strategiesandsystems.Innaturallanguageprocessingrecordlinkageproblemsariseduringres-olutionofentitiesfoundinnaturallanguagetexttoagazetteer.Ourproblemconsistedofmergingeachof≈20000physicianrecords,whichwecalltheupdatedatabase,totherecordofthesamephysicianinamasterdatabaseof≈106records.Theupdatedatabasehasﬁeldsthatareabsentinthemasterdatabaseandviceversa.Theﬁeldsincommonin-cludethename(ﬁrst,lastandmiddleinitial),sev-eraladdressﬁelds,phone,specialty,andtheyear-of-graduation.Althoughthelastnameandyear-of-graduationareconsistentwhenpresent,thead-dress,specialtyandphoneﬁeldshaveseveralincon-sistenciesowingtodifferentwaysofwritingthead-dress,newaddresses,differenttermsforthesamespecialty,missingﬁelds,etc.However,thenameandyearaloneareinsufﬁcientfordisambiguation.Wehadaccessto≈500manuallymatchedupdaterecordsfortrainingandevaluation(about40oftheseupdaterecordswerelabeledasunmatchableduetoinsufﬁcientinformation).Thegeneralapproachtorecordlinkageinvolvestwosteps:1)blocking,whereasmallsetofcan-didaterecordsisretrievedfromthemasterrecorddatabase,whichcontainsthecorrectmatchwithhighprobability,and2)matching,wheretheﬁeldsoftheupdaterecordsarecomparedtothoseofthecandidatesforscoringandselectingthematch.Weperformedblockingbyqueryingthemasterrecorddatabasewiththelastnamefromtheupdaterecord.Matchingwasdonebyscoringafeaturevectorofsimilaritiesoverthevariousﬁelds.Thefeatureval-ueswereeitherbinary(verifyingtheequalityofaparticularﬁeldintheupdateandamasterrecord)orcontinuous(somekindofnormalizedstringeditdis-tancebetweenﬁeldslikestreetaddress,ﬁrstnameetc.).Thesurrogatelearningsolutiontoourmatchingproblemwassetupasfollows.Wedesignatedthebinaryfeatureofequalityofyearofgraduation3asthe‘100%recall’featurex1,andtheremainingfea-turesarerelegatedtox2.Therequiredconditionsforsurrogatelearningaresatisﬁedbecause1)inourdataitishighlyunlikelyfortworecordswithdiffer-entyear-of-graduationtobelongtothesamephysi-cianand2)ifitisknownthattheupdaterecordandamasterrecordbelongtotwodifferentphysi-cians,thenknowingthattheyhavethesame(ordif-ferent)year-of-graduationprovidesnoinformationabouttheotherfeatures.Thereforeallthefeaturevectorswiththebinaryfeatureindicatingequalityofyear-of-graduationaretargetsandtheremainingarebackgrounds.First,weusedfeaturevectorsobtainedfromtherecordsinallblocksfromall20000updaterecordstoestimatetheprobabilityP(x1|x2).Weusedlo-3Webelievethattheequalityofthemiddleintialwouldhaveworkedjustaswellforx1.15

Table1:PrecisionandRecallforrecordlinkage.TrainingPrecisionRecallproportionSurrogate0.960.95Supervised0.50.960.94Supervised0.20.960.91gisticregressionforthispredictiontask.Forlearn-ingthelogisticregressionparameters,wediscardedthefeaturevectorsforwhichx1wasmissingandperformedmeanimputationforthemissingvaluesofotherfeatures.Second,theprobabilityP(x1=1|y=0)(theprobabilitythattwodifferentran-domlychosenphysicianshavethesameyearofgraduation)wasestimatedstraightforwardlyfromthecountsofthedifferentyears-of-graduationinthemasterrecorddatabase.TheseestimateswereusedtoassignthescoreP(y=1|x1=1,x2)totherecordsinablock(cf.Equation4).Thescoreof0isassignedtofeaturevectorswhichhavex1=0.Theonlycaveatiscal-culatingthescoreforfeaturevectorsthathadmiss-ingx1.ForsuchrecordsweassignthescoreP(y=1|x2)=P(y=1|x1=1,x2)P(x1=1|x2).Wehaveestimatesforbothquantitiesontherighthandside.Thehighestscoringrecordineachblockwasﬂaggedasamatchifitexceededsomeappropriatethreshold.Wecomparedtheresultsofthesurrogatelearn-ingapproachtoasupervisedlogisticregressionbasedmatcherwhichusedaportionofthemanualmatchesfortrainingandtheremainingfortesting.Table1showsthematchprecisionandrecallforboththesurrogatelearningandthesupervisedap-proaches.Forthesupervisedalgorithm,weshowtheresultsforthecasewherehalfthemanuallymatchedrecordswereusedfortrainingandhalffortesting,aswellasforthecasewhereaﬁfthoftherecordsoftrainingandtheremainingfour-ﬁfthsfortesting.Inthelattercase,everyrecordparticipatedinexactlyonetrainingfoldbutinfourtestfolds.Theresultsindicatethatthesurrogatelearnerper-formsbettermatchingbyexploitingtheunlabeleddatathanthesupervisedlearnerwithinsufﬁcienttrainingdata.Theresultsalthoughnotdramaticarestillpromising,consideringthatthesurrogatelearn-ingapproachusednoneofthemanuallymatchedrecords.4.2ParaphraseGenerationforEventExtractionSentenceclassiﬁcationisoftenapreprocessingstepforeventorrelationextractionfromtext.Oneofthechallengesposedbysentenceclassiﬁcationisthedi-versityinthelanguageforexpressingthesameeventorrelationship.Wepresentasurrogatelearningap-proachtogeneratingparaphrasesforexpressingthemerger-acquisition(MA)eventbetweentwoorgani-zationsinﬁnancialnews.Ourgoalistoﬁndpara-phrasesentencesfortheMAeventfromanunla-beledcorpusofnewsarticles,thatmighteventuallybeusedtotrainasentenceclassiﬁerthatdiscrimi-natesbetweenMAandnon-MAsentences.Weassumethattheunlabeledsentencecorpusistime-stampedandnamedentitytaggedwithorga-nizations.WefurtherassumethataMAsentencemustmentionatleasttwoorganizations.Ourap-proachtogenerateparaphrasesisthefollowing.Weﬁrstextractalltheso-calledsourcesentencesfromthecorpusthatmatchafewhigh-precisionseedpat-terns.AnexampleofaseedpatternusedfortheMAeventis‘<ORG1>acquired<ORG2>’(where<ORG1>and<ORG2>areplaceholdersforstringsthathavebeentaggedasorganizations).Anexampleofasourcesentencethatmatchestheseedis‘Itwasannouncedyesterdaythat<ORG>GoogleInc.<ORG>acquired<ORG>Youtube<ORG>’.ThepurposeoftheseedpatternsistoproducepairsofparticipantorganizationsinanMAeventwithhighprecision.Wethenextracteverysentenceinthecorpusthatcontainsatleasttwoorganizations,suchthatatleastoneofthemmatchesanorganizationinthesourcesentences,andhasatime-stampwithinatwomonthtimewindowofthematchingsourcesentence.Ofthissetofsentences,allthatcontaintwoormoreor-ganizationsfromthesamesourcesentencearedes-ignatedastargetsentences,andtherestaredesig-natedasbackgroundsentences.WespeculatethatsinceanorganizationisunlikelytohaveaMArelationshipwithtwodifferentorga-nizationsinthesametimeperiodthebackgroundsareunlikelytocontainMAsentences,andmore-overthelanguageofthenon-MAtargetsentencesis16

Table2:Patternsusedasseedsandthenumberofsourcesentencesmatchingeachseed.Seedpattern#ofsources1<ORG>acquired<ORG>572<ORG>bought<ORG>703offerfor<ORG>2874tobuy<ORG>3965mergerwith<ORG>294indistinguishablefromthatofthebackgroundsen-tences.Torelatetheapproachtosurrogatelearning,wenotethatthebinary“organization-pairequality”feature(bothorganizationsinthecurrentsentencebeingthesameasthoseinasourcesentence)servesasthe‘100%recall’featurex1.Wordunigram,bi-gramandtrigramfeatureswereusedasx2.Thissetupsatisﬁestherequiredconditionsforsurrogatelearningbecause1)ifasentenceisaboutMA,theorganizationpairmentionedinitmustbethesameasthatinasourcesentence,(i.e.,ifonlyoneoftheorganizationsmatchthoseinasourcesentence,thesentenceisunlikelytobeaboutMA)and2)ifanun-labeledsentenceisnon-MA,thenknowingwhetherornotitsharesanorganizationwithasourcedoesnotprovideanyinformationaboutthelanguageinthesentence.Iftheoriginalunlabeledcorpusissufﬁcientlylarge,weexpectthetargetsettocovermostoftheparaphrasesfortheMAeventbutmaycontainmanynon-MAsentencesaswell.Thetaskofgeneratingparaphrasesinvolvesﬁlteringthetargetsentencesthatarenon-MAandﬂaggingtherestofthetar-getsasparaphrases.Thisisdonebyconstructingaclassiﬁerbetweenthetargetsandbackgrounds.Thefeaturesetusedforthistaskwasabagofwordun-igrams,bigramsandtrigrams,generatedfromthesentencesandselectedbyrankingthen-gramsbythedivergenceoftheirdistributionsinthetargetsandbackgrounds.Asupportvectormachine(SVM)wasusedtolearntoclassifybetweenthetargetsandbackgroundsandthesentenceswererankedaccord-ingtothescoreassignedbytheSVM(whichisaproxyforP(x1=1|x2)).Wethenthresholdedthescoretoobtaintheparaphrases.Ourapproachissimilarinprincipletothe‘Snow-ball’systemproposedin(AgichteinandGravano,2000)forrelationextraction.Similartous,‘Snow-ball’looksforknownparticipantsinarelationshipinanunlabeledcorpus,andusesthenewlydiscoveredcontextstoextractmoreparticipanttuples.How-ever,unlikesurrogatelearning,whichcanusearichsetoffeaturesforrankingthetargets,‘Snowball’scoresthenewlyextractedcontextsaccordingtoasinglefeaturevaluewhichisconﬁdencemeasurebasedonlyonthenumberofknownparticipanttu-plesthatarefoundinthecontext.Example2belowlistssomesentencestoillustratethesurrogatelearningapproach.Notethatthetar-getsmaycontainbothMAandnon-MAsentencesbutthebackgroundsareunlikelytobeMA.—————————————–Example2SeedPattern“offerfor<ORG>”SourceSentences1.<ORG>USAirways<ORG>saidWednesdayitwillincreaseitsofferfor<ORG>Delta<ORG>.TargetSentences(SVMscore)1.<ORG>USAirways<ORG>weretocombinewithastandalone<ORG>Delta<ORG>.(1.0008563)2.<ORG>USAirways<ORG>arguedthatthenearly$10billionacquisitionof<ORG>Delta<ORG>wouldresultinanefﬁcientlyruncarrierthatcouldofferlowfarestoﬂiers.(0.99958149)3.<ORG>USAirways<ORG>isasking<ORG>Delta<ORG>’sofﬁcialcreditorscommit-teetosupportpostponingthathearing.(-0.99914371)BackgroundSentences(SVMscore)1.Thecitieshavemadevariousoverturesto<ORG>USAirways<ORG>,includingapromisefrom<ORG>AmericaWestAirlines<ORG>andtheformer<ORG>USAirways<ORG>.(0.99957752)2.<ORG>USAirways<ORG>sharesrose8centstocloseat$53.35onthe<ORG>NewYorkStockExchange<ORG>.(-0.99906444)—————————————–Wetestedouralgorithmonanunlabeledcorpusofapproximately700000ﬁnancialnewsarticles.WeexperimentedwiththeﬁveseedpatternsshowninTable2.Weextractedatotalof870sourcesentencesfromtheﬁveseeds.ThenumberofsourcesentencesmatchingeachoftheseedsisalsoshowninTable2.Notethatthenumbersaddtomorethan870becauseitispossibleforasourcesentencetomatchmorethanoneseed.Theparticipantsthatwereextractedfromsources17

Table3:Precision/RecallofsurrogatelearningontheMAparaphraseproblemforvariousthresholds.ThebaselineofusingallthetargetsasparaphrasesforMAhasaprecisionof66%andarecallof100%.ThresholdPrecisionRecall0.00.830.94-0.20.820.95-0.80.790.99correspondedtoapproximately12000targetsen-tencesandapproximately120000backgroundsen-tences.Forthepurposeofevaluation,500randomlyselectedsentencesfromthetargetsweremanuallycheckedleadingto330beingtaggedasMAandtheremaining170asnon-MA.Thiscorrespondstoa66%precisionofthetargets.WethenrankedthetargetsaccordingtothescoreassignedbytheSVMtrainedtoclassifybetweenthetargetsandbackgrounds,andselectedallthetargetsaboveathresholdasparaphrasesforMA.Table3presentstheprecisionandrecallonthe500manu-allytaggedsentencesasthethresholdvaries.Theresultsindicatethatourapproachprovidesaneffec-tivewaytorankthetargetsentencesaccordingtotheirlikelihoodofbeingaboutMA.Toevaluatethecapabilityofthemethodtoﬁndparaphrases,weconductedﬁveseparateexperi-mentsusingeachpatterninTable2individuallyasaseedandcountingthenumberofobtainedsentencescontainingeachoftheotherpatterns(usingathresh-oldof0.0).Thesenumbersareshowninthediffer-entcolumnsofTable4.Althoughnewpatternsareobtained,theirdistributiononlyroughlyresemblestheoriginaldistributioninthecorpus.Weattributethistothecorrelationinthelanguageusedtode-scribeaMAeventbasedonitstype(mergervs.ac-quisition,hostiletakeovervs.seekingabuyer,etc.).Finallyweusedtheparaphrases,whichwerefoundbysurrogatelearning,toaugmentthetrain-ingdataforaMAsentenceclassiﬁerandevaluateditsaccuracy.WeﬁrstbuiltaSVMclassiﬁeronlyonaportionofthelabeledtargetsandclassiﬁedtheremaining.Thisapproachyieldedanaccuracyof76%onthetestset(withtwo-foldcrossvalidation).Wethenaddedallthetargetsscoredaboveathresh-oldbysurrogatelearningaspositiveexamples(4000Table4:Numberofsentencesfoundbysurrogatelearn-ingmatchingeachoftheremainingseedpatterns,whenonlyoneofthepatternswasusedasaseed.Eachcolumnisforoneexperimentwiththecorrespondingpatternusedastheseed.Forexample,whenonlytheﬁrstpatternwasusedastheseed,weobtained18sentencesthatmatchthefourthpattern.Seeds12345122512567534615210341816935753919557positivesentencesinallwereadded),andallthebackgroundsthatscoredbelowalowthresholdasnegativeexamples(27000sentences),tothetrainingdataandrepeatedthetwo-foldcrossvalidation.Theclassiﬁerlearnedontheaugmentedtrainingdataim-provedtheaccuracyonthetestdatato86%.Webelievethatbetterdesignedfeatures(thanwordn-grams)willprovideparaphraseswithhigherprecisionandrecalloftheMAsentencesfoundbysurrogatelearning.Toapplyourapproachtoaneweventextractionproblem,thedesignstepalsoin-volvestheselectionofthex1featuresuchthatthetargetsandbackgroundssatisfyourassumptions.5ConclusionsWepresentedsurrogatelearning–aneasilyimple-mentablesemi-supervisedlearningalgorithmthatcanbeappliedwhenthefeaturessatisfytherequiredindependenceassumptions.Wepresentedtwoappli-cations,showedhowtheassumptionsaresatisﬁed,andpresentedempiricalevidencefortheefﬁcacyofouralgorithm.Wehavealsoappliedsurrogatelearn-ingtoproblemsininformationretrievalanddocu-mentzoning.WeexpectthatsurrogatelearningissufﬁcientlygeneraltobeappliedinmanyNLPap-plications,ifthefeaturesarecarefullydesigned.Webrieﬂynotethatasurrogatelearningmethodbasedonregressionandrequiringonlymeanindependenceinsteadoffullstatisticalindependencecanbede-rivedusingtechniquessimilartothoseinSection2–thismodiﬁcationiscloselyrelatedtotheproblemandsolutionpresentedin(Quadriantoetal.,2008).18

ReferencesS.Abney.2002.Bootstrapping.InInProceedingsofthe40thAnnualMeetingoftheAssociationforComputa-tionalLinguistics,pages360–367.E.AgichteinandL.Gravano.2000.Snowball:Extract-ingRelationsfromLargePlain-TextCollections.InProceedingsofthe5thACMInternationalConferenceonDigitalLibraries(ACMDL),pages85–94,June,2-7.R.K.AndoandT.Zhang.2005.Aframeworkforlearn-ingpredictivestructuresfrommultipletasksandunla-beleddata.JMLR,6:1817–1853.R.K.AndoandT.Zhang.2007.Two-viewfeaturegen-erationmodelforsemi-supervisedlearning.InICML,pages25–32.A.BlumandT.Mitchell.1998.Combininglabeledandunlabeleddatawithco-training.InCOLT,pages92–100.O.Chapelle,B.Sch¨olkopf,andA.Zien,editors.2006.Semi-SupervisedLearning.MITPress,Cambridge,MA.T.G.Dietterich,R.H.Lathrop,andT.Lozano-Perez.1997.Solvingthemultipleinstanceproblemwithaxis-parallelrectangles.ArtiﬁcialIntelligence,89(1-2):31–71.R.O.Duda,P.E.Hart,andD.G.Stork.2000.PatternClassiﬁcation.Wiley-IntersciencePublication.G.NagyandG.L.Shelton.1966.Self-correctivecharac-terrecognitionsystem.IEEETrans.InformationThe-ory,12(2):215–222.N.Quadrianto,A.J.Smola,T.S.Caetano,andQ.V.Le.2008.Estimatinglabelsfromlabelproportions.InICML’08:Proceedingsofthe25thinternationalcon-ferenceonMachinelearning,pages776–783.M.Seeger.2000.Input-dependentregularizationofconditionaldensitymodels.Technicalre-port,InstituteforANC,Edinburgh,UK.Seehttp://www.dai.ed.ac.uk/˜seeger/papers.html.W.E.Winkler.1995.Matchingandrecordlinkage.InBusinessSurveyMethods,pages355–384.Wiley.Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 19–27,

Boulder, Colorado, June 2009. c(cid:13)2009 Association for Computational Linguistics

19

Keepin’ItReal:Semi-SupervisedLearningwithRealisticTuningAndrewB.GoldbergComputerSciencesDepartmentUniversityofWisconsin-MadisonMadison,WI53706,USAgoldberg@cs.wisc.eduXiaojinZhuComputerSciencesDepartmentUniversityofWisconsin-MadisonMadison,WI53706,USAjerryzhu@cs.wisc.eduAbstractWeaddresstwocriticalissuesinvolvedinap-plyingsemi-supervisedlearning(SSL)toareal-worldtask:parametertuningandchoos-ingwhich(ifany)SSLalgorithmisbestsuitedforthetaskathand.Togainabetterun-derstandingoftheseissues,wecarryoutamedium-scaleempiricalstudycomparingsu-pervisedlearning(SL)totwopopularSSLal-gorithmsoneightnaturallanguageprocessingtasksunderthreeperformancemetrics.Wesimulatehowapractitionerwouldgoabouttacklinganewproblem,includingparametertuningusingcrossvalidation(CV).Weshowthat,undersuchrealisticconditions,eachoftheSSLalgorithmscanbeworsethanSLonsomedatasets.However,wealsoshowthatCVcanselectSL/SSLtoachieve“agnosticSSL,”whoseperformanceisalmostalwaysnoworsethanSL.WhileCVisoftendismissedasunreliableforSSLduetothesmallamountoflabeleddata,weshowthatitisinfacteffectiveforaccuracyevenwhenthelabeleddatasetsizeisassmallas10.1IntroductionImagineyouareareal-worldpractitionerworkingonamachinelearningprobleminnaturallanguageprocessing.Ifyouhaveunlabeleddata,shouldyouusesemi-supervisedlearning(SSL)?WhichSSLal-gorithmshouldyouuse?Howshouldyousetitspa-rameters?Orcoulditactuallyhurtperformance,inwhichcaseyoumightbebetteroffwithsupervisedlearning(SL)?AlargenumberofSSLalgorithmshavebeende-velopedinrecentyearsthatallowonetoimproveperformancewithunlabeleddata,intaskssuchastextclassiﬁcation,sequencelabeling,andpars-ing(Zhu,2005;Chapelleetal.,2006;BrefeldandScheffer,2006).However,manyofthemaretestedon“SSL-friendly”datasets,suchas“twomoons,”USPS,andMNIST.Furthermore,thealgorithms’parametersareoftenchosenbasedontestsetper-formanceormanuallysetbasedonheuristicsandresearcherexperience.TheseissuescreatepracticalconcernsfordeployingSSLintherealworld.Wenotethat(Chapelleetal.,2006)’sbenchmarkchapterexplorestheseissuestosomeextentbycom-paringseveralSSLmethodsonseveralrealandar-tiﬁcialdatasets.Theauthorsreachtheconclusionsthatparametertuningisdifﬁcultwithlittlelabeleddataandthatnomethodisuniversallysuperior.WereexaminetheseissuesinthecontextofNLPtasksandofferasimpleattemptatovercomingtheseroad-blockstopracticalapplicationofSSL.Thecontributionsofthispaperinclude:•Wepresentamedium-scaleempiricalstudycomparingSLtotwopopularSSLalgorithmsoneightless-familiartasksusingthreeper-formancemetrics.Importantly,wetunepa-rametersrealisticallybasedoncrossvalidation(CV),asapractitionerwoulddoinreality.•Weshowthat,undersuchrealisticconditions,eachoftheSSLalgorithmscanbeworsethanSLonsomedatasets.•However,thiscanbeprevented.WeshowthatCVcanbeusedtoselectSL/SSLtoachieveagnosticSSL,whoseperformanceisalmostal-waysnoworsethanSL.Traditionally,CVis20

oftendismissedasunreliableforSSLbecauseofthesmalllabeleddatasetsize.ButweshowthatCViseffectivewhenusingaccuracyasanoptimizationcriterion,evenwhenthelabeleddatasetsizeisassmallas10.•Weshowthepowerofcloudcomputing:wewereabletocompleteroughly3monthsworthofexperimentsinlessthanaweek.2SSLwithRealisticTuningGivenaparticularlabeledandunlabeleddataset,howshouldyousetparametersforaparticularSSLmodel?Themostrealisticapproachforapracti-tioneristouseCVtotuneparametersonagrid.WethereforearguethatthemodelparametersobtainedinthiswaytrulydeterminehowSSLwillperforminpractice.Algorithm1describesaparticularin-stance1ofCVindetail.Wecallit“RealSSL,”asthisisallarealusercanhopetodowhenapplyingSSL(andSLtoo)inarealisticproblemscenario.3ExperimentalProcedureGiventheRealSSLprocedureinAlgorithm1,wedesignedanexperimentalsetuptosimulatediffer-entsettingsthatareal-worldpractitionermightfacewhengivenanewtaskandasetofalgorithmstochoosefrom(someofwhichuseunlabeleddata).Thiswillallowustocomparealgorithmsacrossdatasetsinavarietyofsituations.Algorithm2measurestheperformanceofonealgorithmononedatasetforseveraldifferentlanducombinations.Speciﬁcally,weconsiderl∈{10,100}andu∈{100,1000}.Foreachcombination,weperformmultipletrials(T=10here)usingdifferentrandomassignmentsofdatatoDlabeledandDunlabeled,toobtainconﬁdenceintervalsaroundourperformancemeasurements.Allrandomselectionsofsubsetsofdataarethesameacrossdifferentalgorithms’runs,topermitpairedt-testsforevaluation.Notethat,whenl6=max(L)oru6=max(U),aportionofDpoolisnotusedfortraining.Also,theRealSSLprocedureensuresthatallparametersaretunedbycross-validationwithouteverseeingtheheld-out1Theparticularchoiceof5-foldCV,thewaytosplitlabeledandunlabeleddata,andtheparametergrid,isimportanttoo.ButweviewthemassecondarytothefactthatwearetuningSSLbyCV.testsetDtest.Lastly,westressthatthesamegridofalgorithm-speciﬁcparametervalues(discussedinSection5)isconsideredforalldatasets.4DatasetsTable1summarizesthedatasetsusedforthecom-parisons.Inthisstudyweconsideronlybinaryclas-siﬁcationtasks.Notethatdisthenumberofdimen-sions,P(y=1)istheproportionofinstancesinthefulldatasetbelongingtoclassy=1,and|Dtest|referstothesizeofthetestset(theinstancesremain-ingaftermax(L)+max(U)=1100havebeensetasidefortrainingtrials).[MacWin]istheMacversusWindowstextclas-siﬁcationdatafromthe20-newsgroupsdataset,pre-processedbytheauthorsof(Sindhwanietal.,2005).[Interest]isabinaryversionofthewordsensedisambiguationdatafrom(BruceandWiebe,1994).Thetaskistodistinguishthesenseof“interest”meaning“moneypaidfortheuseofmoney”fromtheotherﬁvesenses(e.g.,“readinesstogiveatten-tion,”“ashareinacompanyorbusiness”).Thedatacomesfromacorpusofpart-of-speech(POS)taggedsentencescontainingtheword“interest.”Eachinstanceisabag-of-word/POSvector,exclud-ingwordscontainingtheroot“interest”andthosethatappearedinlessthanthreesentencesoverall.Datasets[aut-avn]and[real-sim]aretheauto/aviationandreal/simulatedtextclassiﬁcationdatasetsfromtheSRAAcorpusofUseNetarticles.The[ccat]and[gcat]datasetsinvolveidentifyingcorporateandgovernmentarticles,respectively,intheRCV1corpus.Weusetheversionsofthesedatasetspreparedbytheauthorsof(Sindhwanietal.,2006).Finally,thetwoWISHdatasetscomefrom(Gold-bergetal.,2009)andinvolvediscriminatingbe-tweensentencesthatcontainwishfulexpressionsandthosethatdonot.Theinstancesin[WISH-politics]correspondtosentencestakenfromapo-liticaldiscussionboard,while[WISH-products]isbasedonsentencesfromAmazonproductreviews.Thefeaturesareacombinationofwordandtemplatefeaturesasdescribedin(Goldbergetal.,2009).21

Input:datasetDlabeled={xi,yi}li=1,Dunlabeled={xj}uj=1,algorithm,performancemetricRandomlypartitionDlabeledinto5equally-sizeddisjointsubsets{Dl1,Dl2,Dl3,Dl4,Dl5}.RandomlypartitionDunlabeledinto5equally-sizeddisjointsubsets{Du1,Du2,Du3,Du4,Du5}.Combinepartitions:LetDfoldk=Dlk∪Dukforallk=1,...,5.foreachparameterconﬁgurationingriddoforeachfoldkdoTrainmodelusingalgorithmon∪i6=kDfoldi.EvaluatemetriconDfoldk.endComputetheaveragemetricvalueacrossthe5folds.endChooseparameterconﬁgurationthatoptimizesaveragemetric.TrainmodelusingalgorithmandthechosenparametersonDlabeledandDunlabeled.Output:Optimalmodel;Averagemetricvalueachievedbyoptimalparametersduringtuning.Algorithm1:RealSSLprocedureforrunninganSSL(orSL,simplyignoretheunlabeleddata)algorithmonaspeciﬁclabeledandunlabeleddatasetusingcross-validationtotuneparameters.Input:datasetD={xi,yi}ni=1,algorithm,performancemetric,setL,setU,trialsTRandomlydivideDintoDpool(ofsizemax(L)+max(U))andDtest(therest).foreachlinLdoforeachuinUdoforeachtrial1uptoTdoRandomlyselectDlabeled={xj,yj}lj=landDunlabeled={xk}uk=1fromDpool.RunRealSSL(Dlabeled,Dunlabeled,algorithm,metric)toobtainmodelandtuningperformancevalue(seeAlgorithm1).UsemodeltoclassifyDunlabeledandrecordtransductivemetricvalue.UsemodeltoclassifyDtestandrecordtestmetricvalue.endendendOutput:Tuning,transductive,andtestperformanceforTrunsofalgorithmusingalllanducombinations.Algorithm2:Experimentalprocedureusedforallcomparisons.22

NamedP(y=1)|Dtest|[MacWin]75110.51846[Interest]26870.531268[aut-avn]207070.6570075[real-sim]209580.3171209[ccat]472360.4722019[gcat]472360.3022019[WISH-politics]136100.344999[WISH-products]48230.12129Table1:Datasetsusedinbenchmarkcomparison.Seetextfordetails.5AlgorithmsWeconsideronlylinearclassiﬁersforthisstudy,sincetheytendtoworkwellfortextproblems.Infuturework,weplantoexplorearangeofkernelsandothernon-linearclassiﬁers.Asabaselinesupervisedlearningmethod,weuseasupportvectormachine(SVM),asimplementedbySVMlight(Joachims,1999).Thisbaselinesimplyignoresalltheunlabeleddata(xl+1,...,xn).Recallthissolvesthefollowingregularizedriskminimiza-tionproblemminf12||f||22+ClXi=1max(0,1−yif(xi)),(1)wheref(x)=w>x+b,andCisaparame-tercontrollingthetrade-offbetweentraininger-rorsandmodelcomplexity.Usingtheprocedureoutlinedabove,wetuneCoveragridofvalues{10−6,10−5,10−4,10−3,10−2,10−1,1,10,100}.WeconsidertwopopularSSLalgorithms,whichmakedifferentassumptionsaboutthelinkbetweenthemarginaldatadistributionPxandtheconditionallabeldistributionPy|x.Iftheassumptiondoesnotholdinaparticulardataset,theSSLalgorithmcouldusetheunlabeleddata“incorrectly,”andperformworsethanSL.TheﬁrstSSLalgorithmweuseisasemi-supervisedsupportvectormachine(S3VM),whichmakestheclusterassumption:theclassesarewell-separatedclustersofdata,suchthatthedecisionboundaryfallsintoalowdensityregioninthefea-turespace.Whilemanyimplementationsexist,wechosethedeterministicannealing(DA)algo-rithmimplementedintheSVMlinpackage(Sind-hwanietal.,2006;SindhwaniandKeerthi,2007).ThisDAalgorithmoftenachievedthebestaccu-racyacrossseveraldatasetsintheempiricalcom-parisonin(SindhwaniandKeerthi,2007),despitebeingslowerthanthemulti-switchalgorithmpre-sentedinthesamepaper.NotethatthetransductiveSVMimplementedinSVMlightwouldhavebeenprohibitivelyslowtocarryouttherangeofexper-imentsconductedhere.RecallthatanS3VMseeksanoptimalclassiﬁerf∗thatcutsthrougharegionoflowdensitybetweenclustersofdata.Onewaytoviewthisisthatittriestoﬁndthebestpossiblela-belingoftheunlabeleddatasuchtheclassiﬁermaxi-mizesthemarginonbothlabeledandunlabeleddatapoints.Thisisachievedbysolvingthefollowingnon-convexminimizationproblemminf,y0∈{−1,1}uλ2||f||22+1llXi=1V(yif(xi)+λ0unXj=l+1V(y0jf(xj)),subjecttoaclass-balanceconstraint.NotethatVisalossfunction(typicallythehingelossasin(1)),andtheparametersλ,λ0controltherelativeimportanceofmodelcomplexityversuslocatingalow-densityregionwithintheunlabeleddata.Wetunebothparametersinagridofvalues{10−6,10−5,10−4,10−3,10−2,10−1,1,10,100}.Inpaststudies(Sindhwanietal.,2006),λwassetto1,andλ0wastunedoveragridcontainingasubsetofthesevalues.Finally,asanexampleofagraph-basedSSLmethod,weconsidermanifoldregularization(MR)(Belkinetal.,2006),usingtheimplementa-tionprovidedontheauthors’Website.2Thisalgo-rithmmakesthemanifoldassumption:thelabelsare“smooth”withrespecttoagraphconnectinglabeledandunlabeledinstances.Inotherwords,iftwoin-stancesareconnectedbyastrongedge(e.g.,theyarehighlysimilartooneanother),theirlabelstendtobethesame.Manifoldregularizationrepresentsafamilyofmethods;wespeciﬁcallyusetheLaplacianSVM,whichextendsthebasicSVMoptimization2http://manifold.cs.uchicago.edu/manifoldregularization/software.html23

problemwithagraph-basedregularizationterm.minfγA||f||22+1llXi=1max(0,1−yif(xi))+γInXi=1nXj=1wij(f(xi)−f(xj))2,whereγAandγIareparametersthattradeoffam-bientandintrinsicsmoothness,andwijisagraphweightbetweeninstancesxiandxj.Inthispa-per,weconsiderkNNgraphswithk∈{3,10,20}.Edgeweightsareformedusingaheatkernelwij=exp(−||xi−xj||22σ2),whereσissettobethemeandis-tancebetweennearestneighborsinthegraph,asin(Chapelleetal.,2006).Theγparametersareeachtunedoverthegrid{10−6,10−4,10−2,1,100}.Ofcourse,manyotherSSLalgorithmsexist,someofwhichcombinedifferentassumptions(ChapelleandZien,2005;Karlenetal.,2008),andotherswhichexploitmultiple(realorartiﬁcial)viewsofthedata(BlumandMitchell,1998;SindhwaniandRosenberg,2008).WeplantoextendourstudytoincludemanymorediverseSSLalgorithmsinthefuture.6ChoosinganAlgorithmforaReal-WorldTaskGiventhechoiceofseveralalgorithms,howshouldonechoosethebestonetoapplytoaparticularlearn-ingsetting?Traditionally,CVisusedformodelselectioninsupervisedlearningsettings.However,withonlyasmallamountoflabeleddatainsemi-supervisedsettings,modelselectionwithCVisof-tenviewedasunreliable.WeexplicitlytestedthishypothesisbyusingCVtonotonlychoosethepa-rametersofthemodel,butalsochoosethetypeofmodelitself.ThemaingoalistoautomaticallychoosebetweenSVM,S3VM,andMRforapar-ticularlearningsetting,inanattempttoensurethattheﬁnalperformanceisneverhurtbyincludingun-labeleddata(whichmightbecalledagnosticSSL).Givenasetofalgorithms(e.g.,oneSL,severalSSL),theprocedureisthefollowing:1.TunetheparametersofeachalgorithmonthelabeledandunlabeledtrainingsetusingAlgo-rithm1.32.Comparethebesttuningperformance(5-foldCVaverage)achievedbytheoptimalparame-tersforeachofthealgorithms.•Iftherearenoties,selectthealgorithmwiththehighesttuningperformance.•Ifthereisatie,andSLisamongthebest,selectSL.•IfthereisatiebetweenSSLalgorithms,selectoneofthematrandom.3.Usetheselected“BestTuning”algorithm(andthetunedparameters)tobuildamodelonallthetrainingdata;thenapplyittothetestdata.NotethattheprocedureisconservativeinthatitprefersSLinthecaseofties.Inthispaper,weusethissimple“besttuningperformance”heuristic.Finally,westressthefactthat,whenapplyingthisprocedurewithinthecontextofAlgorithm2,apotentiallydifferentalgorithmischosenineachofthe10trialsforaparticularsetting.Thissimulatesthereal-worldscenariowhereoneonlyhasasingleﬁxedtrainingsetoflabeledandunlabeleddataandmustchooseasinglealgorithmtoproduceamodelforfuturepredictions.7PerformanceMetricsWecomparedifferentalgorithms’performanceus-ingthreemetricsoftenusedforevaluationinNLPtasks:accuracy,maxF1,andAUROC.Accuracyissimplythefractionofinstancescorrectlyclassi-ﬁed.MaxF1isthemaximalF1value(harmonicmeanofrecallandprecision)achievedovertheen-tireprecision-recallcurve(CaiandHofmann,2003).AUROCistheareaundertheROCcurve(Fawcett,2004).Throughoutthepaper,whenwediscussaresultinvolvingaparticularmetric,thealgorithmsusethismetricasthecriterionforparametertuning,andweuseitfortheﬁnalevaluation.Wearenotsimplyevaluatingasingleexperimentusingmulti-plemetrics—theexperimentsarefundamentallydif-ferentandproducedifferentlearnedmodels.3Weensureeachalgorithmusesthesame5partitionsduringthetuningstep.24

8ResultsWenowreporttheresultsofourempiricalcompar-isonofSLandSSLontheeightNLPdatasets.Weﬁrstconsidereachdatasetseparatelyandexaminehowofteneachtypeofalgorithmoutperformstheother.Wethenexaminecross-datasetperformance.8.1DetailedResultsTable2containsallresultsforSVM,S3VM,andMRforalldatasetsandallmetrics.4Notethatwithineachl,ucellforaparticulardatasetandevaluationmetric,weshowthemaximumvalueineachrow(tune,transductive,ortest)inboldface.Resultsthatarenotstatisticallysigniﬁcantlydifferentusingapairedt-testarealsoshowninboldface.SeveralthingsareimmediatelyobviousfromTa-ble2.First,noalgorithmissuperiorinalldatasetsorsettings.Inseveralcases,allalgorithmsarestatis-ticallyindistinguishable.Mostimportantly,though,eachoftheSSLalgorithmscanbeworsethanSLonsomedatasetsusingsomemetric.Weusedpairedt-teststocomparetransductiveandtestperformanceofeachSSLalgorithmwithSVMforaparticularl,ucombinationanddataset(32settingstotalperevalu-ationmetric).Intermsofaccuracy,MRtransductiveperformanceissigniﬁcantlyworsethanSVMin5settings,whileMRtestperformanceissigniﬁcantlyworsein7settings.MRisalsosigniﬁcantlyworsein4settingsbasedontransductivemaxF1,in3settingsbasedontransductiveAUROC,and1settingbasedontestAUROC.S3VMissigniﬁcantlyworsethanSVMin2settingsbasedontransductivemaxF1,2settingsbasedontransductiveAUROC,andin1set-tingbasedontestAUROC.Whilethesenumbersmayseemrelativelylow,itisimportanttorealizethateachalgorithmmaybeworsethanSSLmanytimesonatrial-by-trialbasis,whichisthemorereal-isticscenario:apractitionerhasonlyasingledatasettoworkwith.Resultsbasedonindividualtrialsarediscussedbelowshortly.4Notethattheresultshereforaparticulardatasetandalgo-rithmcombinationmaybequalitativelyandquantitativelydif-ferentthaninpreviouspublishedwork,duetodifferencesinparametertuning,choicesofparametergrids,landusizes,andrandomization.Wearenottryingtoreplicateorraisedoubtaboutpastresults:wesimplyintendtocomparealgorithmsonawidearrayofdatasetsusingthestandardizedproceduresout-linedabove.Wealsoappliedour“BestTuning”modelselec-tionproceduretoautomaticallychooseasingleal-gorithmforeachtrialineachsetting.WecompareaverageSLtestperformanceversustheaveragetestperformanceoftheBestTuningselectionsacrossthe10trials(notshowninTable2).Comparisonsbasedontransductiveperformancearesimilar.Whentheperformancemetricistestaccuracy,theBestTuningalgorithmperformsstatisticallysigniﬁcantlybetterthanSLin24settingsandworseinonly6settings.Intheremaining2settings,BestTuningchoseSLinall10trials,sotheyareequivalent.Theseresultssuggestthataccuracy-basedtuningisavalidmethodforchoosingaSSLalgorithmtoimproveaccuracyontestdata.Tosomeextent,thisholdsformaxF1,too:theBestTuningselectionsperformbetterthanSL(onaverage)in18settingsandworsein14set-tingswhentuningandtestevaluationisbasedonmaxF1.However,whenusingAUROCastheper-formancemetric,crossvalidationseemstobeunre-liable:BestTuningproducesabetterresultinonly11outofthe32settings.8.2ResultsAggregatedAcrossDatasetsWenowaggregatethedetailedresultstobetterun-derstandtherelativeperformanceofthedifferentmethodsacrossalldatasets.Weperformthissum-maryevaluationintwoways,basedontestsetperformance(transductiveperformanceissimilar).First,wecomparetheSSLalgorithmsacrossalldatasetsbasedonthenumbersoftimeseachisworsethan,thesameas,orbetterthanSL.Foreachofthe80trialsofaparticularl,u,metriccombination,wecomparetheperformanceofS3VM,MR,andBestTuningtoSVM.Notethateachofthesecom-parisonsisakintoareal-worldscenariowhereapractitionerwouldhavetochooseanalgorithmtouse.Table3liststuplesoftheform“(#trialsworsethanSVM,#trialsequaltoSVM,#trialsbetterthanSVM).”Notethatthenumbersineachtuplesumto80.TheperfectSSLalgorithmwouldhaveatupleof“(0,0,80),”meaningthatitalwaysoutperformsSL.Intermsofaccuracy(Table3,top)andmaxF1(Ta-ble3,middle),theBestTuningmethodturnsouttodoworsethanSVMlessoftenthaneitherS3VMorMRdoes(i.e.,theﬁrstnumberinthetuplesforBestTuningislowerthanthecorrespondingnumbersfortheotheralgorithms).Atthesametime,BestTuning25

accuracymaxF1AUROCu=100u=1000u=100u=1000u=100u=1000DatasetlSVMS3VMMRSVMS3VMMRSVMS3VMMRSVMS3VMMRSVMS3VMMRSVMS3VMMR[MacWin]100.600.720.830.600.720.860.660.670.670.660.670.670.630.690.670.630.690.69Tune0.510.510.700.510.500.690.740.770.800.740.740.750.720.750.820.720.710.80Trans0.530.500.710.530.500.680.740.750.790.740.750.740.730.720.830.730.710.76Test1000.870.870.910.870.870.900.940.950.950.940.950.950.960.970.970.960.960.96Tune0.890.890.890.890.890.890.910.930.920.910.900.900.970.970.960.970.970.96Trans0.890.890.910.890.890.900.920.920.920.920.910.910.970.970.970.970.970.97Test[Interest]100.680.750.780.680.750.790.730.770.770.730.780.770.520.660.660.520.680.64Tune0.520.560.560.520.560.560.720.720.720.720.710.710.550.540.540.550.560.61Trans0.520.570.570.520.570.580.680.690.690.680.690.690.580.560.610.580.580.62Test1000.770.780.760.770.780.770.840.850.850.840.850.840.890.900.890.890.850.84Tune0.790.790.710.790.790.770.840.830.820.840.810.810.910.910.890.910.790.87Trans0.810.800.780.810.800.790.820.810.810.820.810.810.900.910.890.900.810.88Test[aut-avn]100.720.760.820.720.760.790.890.920.910.890.920.910.580.670.650.580.670.65Tune0.650.630.670.650.610.690.830.830.840.830.810.820.710.670.730.710.650.72Trans0.620.610.670.620.610.670.800.810.820.800.810.810.710.700.730.710.650.69Test1000.750.820.870.750.820.860.940.940.950.940.940.940.930.940.940.930.940.93Tune0.770.790.880.770.830.870.920.920.910.920.910.900.930.930.910.930.940.93Trans0.770.820.890.770.830.870.910.910.910.910.910.910.950.940.950.950.950.95Test[real-sim]100.530.630.820.530.630.780.650.660.660.650.660.650.770.810.810.770.810.77Tune0.640.630.720.640.640.700.570.660.700.570.620.560.650.750.790.650.740.67Trans0.650.660.740.650.660.680.530.580.630.530.590.530.640.730.800.640.740.66Test1000.740.730.860.740.730.840.880.900.900.880.910.890.930.940.940.930.940.93Tune0.780.760.840.780.780.850.810.830.790.810.810.810.940.930.910.940.940.94Trans0.790.780.850.790.780.850.780.790.780.780.790.790.930.930.930.930.940.93Test[ccat]100.540.600.820.540.600.810.840.850.850.840.850.840.740.780.780.740.780.74Tune0.500.490.650.500.510.670.690.690.730.690.670.690.600.610.710.600.590.72Trans0.490.520.640.490.520.660.660.660.690.660.670.670.610.630.720.610.590.71Test1000.800.800.840.800.800.840.890.890.900.890.890.890.910.920.920.910.920.91Tune0.800.790.800.800.810.830.830.850.840.830.820.820.910.910.890.910.900.91Trans0.810.800.810.810.800.820.800.810.810.800.810.810.900.900.900.900.900.90Test[gcat]100.740.830.820.740.790.810.440.470.460.440.470.460.690.790.750.690.790.75Tune0.690.680.750.690.720.760.600.620.690.600.590.620.710.730.820.710.690.76Trans0.660.670.730.660.710.740.580.610.660.580.600.590.690.690.810.690.690.75Test1000.770.770.900.770.770.910.920.920.930.920.920.920.970.960.970.970.960.96Tune0.810.800.890.810.810.900.880.880.840.880.860.850.960.970.950.960.960.96Trans0.800.800.890.800.800.900.860.860.850.860.860.860.960.960.960.960.960.96Test[WISH-politics]100.700.770.790.700.770.820.610.620.610.610.620.610.740.780.740.740.780.76Tune0.500.560.630.500.620.560.580.580.610.580.550.530.620.620.690.620.620.61Trans0.520.560.600.520.620.530.520.530.530.520.540.520.570.580.610.570.620.60Test1000.750.750.750.750.750.740.740.750.760.740.750.750.790.800.800.790.800.80Tune0.730.730.710.730.730.700.650.660.670.650.640.640.760.740.750.760.750.76Trans0.750.750.720.750.750.710.640.630.630.640.630.640.780.760.770.780.760.77Test[WISH-products]100.890.890.670.890.890.670.190.220.160.190.220.160.760.800.740.760.800.74Tune0.870.870.660.870.870.610.310.290.320.310.240.250.560.520.580.560.540.56Trans0.900.900.670.900.900.610.220.230.300.220.240.270.500.530.620.500.540.59Test1000.900.900.820.900.900.810.490.500.540.490.520.520.730.730.770.730.780.75Tune0.880.880.810.880.880.800.340.280.370.340.270.300.600.550.570.600.570.61Trans0.900.900.790.900.910.760.330.280.330.330.320.380.590.560.600.590.560.60TestTable2:Benchmarkcomparisonresults.Allnumbersareaveragesover10trials.Withineachcellofninenumbers,theboldfaceindicatesthemaximumvalueineachrow,aswellasothersintherowthatarenotstatisticallysigniﬁcantlydifferentbasedonapairedt-test.u=100u=1000MetriclS3VMMRBestTuningS3VMMRBestTuningaccuracy10(14,27,39)(27,0,53)(8,31,41)(14,25,41)(27,0,53)(8,29,43)Test100(27,7,46)(38,0,42)(20,16,44)(27,6,47)(37,0,43)(16,19,45)TestMetriclS3VMMRBestTuningS3VMMRBestTuningmaxF110(29,2,49)(16,1,63)(14,55,11)(27,0,53)(24,0,56)(13,53,14)Test100(39,0,41)(34,4,42)(31,15,34)(39,1,40)(44,4,32)(26,21,33)TestMetriclS3VMMRBestTuningS3VMMRBestTuningAUROC10(26,0,54)(11,0,69)(12,57,11)(25,0,55)(25,0,55)(11,56,13)Test100(43,0,37)(37,0,43)(38,8,34)(38,0,42)(46,0,34)(28,24,28)TestTable3:AggregatetestperformancecomparisonsversusSVMin80trialspersetting.Eachcellcontainsatupleoftheform“(#trialsworsethanSVM,#trialsequaltoSVM,#trialsbetterthanSVM).”26

u=100u=1000MetriclSVMS3VMMRBestTuningSVMS3VMMRBestTuningaccuracy100.610.620.670.680.610.630.640.67Test1000.810.820.830.850.810.820.830.85TestMetriclSVMS3VMMRBestTuningSVMS3VMMRBestTuningmaxF1100.590.610.640.590.590.610.610.59Test1000.760.750.760.750.760.760.760.76TestMetriclSVMS3VMMRBestTuningSVMS3VMMRBestTuningAUROC100.630.640.720.610.630.640.670.61Test1000.870.870.870.870.870.860.870.86TestTable4:Aggregatetestresultsaveragedoverthe80trials(8datasets,10trialseach)inaparticularsetting.outperformsSVMinfewertrialsthantheotheralgo-rithmsinsomesettingsforthesetwometrics.ThisisbecauseBestTuningconservativelyselectsSVMinmanytrials.ThetakehomemessageisthattuningusingCVbasedonaccuracy(andtoalesserextentmaxF1)appearstomitigatesomeriskinvolvedinapplyingSSL.AUROC,ontheotherhand,doesnotappearaseffectiveforthispurpose.Table3(bottom)showsthat,foru=1000,BestTuningisworsethanSVMfewertimes,butforu=100,MRachievesbetterperformanceoverall.Wealsocompareoverallaveragetestperformance(acrossdatasets)foreachmetricandl,ucombina-tion.Table4reportstheseresultsforaccuracy,maxF1,andAUROC.Intermsofaccuracy,weseethattheBestTuningapproachleadstobetterper-formancethanSVM,S3VM,orMRinallsettingswhenaveragedoverdatasets.Weappeartoachievesomesynergyindynamicallychoosingadifferentalgorithmineachtrial.IntermsofmaxF1,BestTuning,S3VM,andMRareallatleastasgoodasSLinthreeofthefourl,usettings,andnearlyasgoodinthefourth.BasedonAUROC,though,theresultsaremixeddependingonthespeciﬁcsetting.Notably,though,BestTuningconsistentlyleadstoworseperformancethanSLwhenusingthismetric.8.3ANoteonCloudComputingTheexperimentswerecarriedoutusingtheCondorHigh-ThroughputComputingplatform(Thainetal.,2005).Weranmanytrialsperalgorithm(usingdif-ferentdatasets,l,u,andmetrics).Eachtrialin-volvedtraininghundredsofmodelsusingdifferentparameterconﬁgurationsrepeatedacrossﬁvefolds,andthentrainingoncemoreusingtheselectedpa-rameters.Intheend,wetrainedagrandtotalof794,880individualmodelstoproducetheresultsinTable2.Throughdistributedcomputingonapproxi-mately50machinesinparallel,wewereabletorunalltheseexperimentsinlessthanaweek,whileus-ingroughlythreemonthsworthofCPUtime.9ConclusionsWehaveexplored“realisticSSL,”whereallparame-tersaretunedvia5-foldcrossvalidation,tosimulateareal-worldexperienceoftryingtouseunlabeleddatainanovelNLPtask.Ourmedium-scaleempir-icalstudyofSVM,S3VM,andMRrevealedthatnoalgorithmisalwayssuperior,andfurthermorethattherearecasesinwhicheachSSLalgorithmweex-aminedcanperformworsethanSVM(insomecasessigniﬁcantlyworseacross10trials).Tomitigatesuchrisks,weproposedasimplemeta-levelproce-durethatselectsoneofthethreemodelsbasedontuningperformance.WhilecrossvalidationisoftendismissedformodelselectioninSSLduetoalackoflabeleddata,thisBestTuningapproachprovesef-fectiveinhelpingtoensurethatincorporatingunla-beleddatadoesnothurtperformance.Interestingly,thisworkswellonlywhenoptimizingaccuracydur-ingtuning.Forfuturework,weplantoextendthisstudytoincludeadditionaldatasets,algorithms,andtuningcriteria.Wealsoplantodevelopmoreso-phisticatedtechniquesforchoosingwhichSL/SSLalgorithmtouseinpractice.AcknowledgmentsA.GoldbergissupportedinpartbyaYahoo!KeyTechnicalChallengesGrant.27

ReferencesMikhailBelkin,ParthaNiyogi,andVikasSindhwani.2006.Manifoldregularization:Ageometricframe-workforlearningfromlabeledandunlabeledexam-ples.JournalofMachineLearningResearch,7:2399–2434,November.AvrimBlumandTomMitchell.1998.Combin-inglabeledandunlabeleddatawithco-training.InCOLT:ProceedingsoftheWorkshoponComputa-tionalLearningTheory.UlfBrefeldandTobiasScheffer.2006.Semi-supervisedlearningforstructuredoutputvariables.InICML06,23rdInternationalConferenceonMachineLearning,Pittsburgh,USA.R.BruceandJ.Wiebe.1994.Word-sensedisambigua-tionusingdecomposablemodels.InProceedingsofthe32ndAnnualMeetingoftheAssociationforCom-putationalLinguistics,pages139–146.LijuanCaiandThomasHofmann.2003.Textcatego-rizationbyboostingautomaticallyextractedconcepts.InSIGIR’03:Proceedingsofthe26thannualinterna-tionalACMSIGIRconferenceonResearchanddevel-opmentininformaionretrieval.OlivierChapelleandAlexanderZien.2005.Semi-supervisedclassiﬁcationbylowdensityseparation.InProceedingsoftheTenthInternationalWorkshoponArtiﬁcialIntelligenceandStatistics(AISTAT2005).OlivierChapelle,AlexanderZien,andBernhardSch¨olkopf,editors.2006.Semi-supervisedlearning.MITPress.TomFawcett.2004.ROCgraphs:Notesandpracticalconsiderationsforresearchers.TechnicalReportHPL-2003-4.AndrewB.Goldberg,NathanaelFillmore,DavidAndrze-jewski,ZhitingXu,BryanGibson,andXiaojinZhu.2009.Mayallyourwishescometrue:Astudyofwishesandhowtorecognizethem.InProceedingsofNAACLHLT.ThorstenJoachims.1999.Makinglarge-scalesvmlearningpractical.InB.Sch¨olkopf,C.Burges,andA.Smola,editors,AdvancesinKernelMethods-Sup-portVectorLearning.MITPress.M.Karlen,J.Weston,A.Erkan,andR.Collobert.2008.Largescalemanifoldtransduction.InAndrewMcCal-lumandSamRoweis,editors,Proceedingsofthe25thAnnualInternationalConferenceonMachineLearn-ing(ICML2008),pages448–455.Omnipress.VikasSindhwaniandS.SathiyaKeerthi.2007.New-tonmethodsforfastsolutionofsemi-supervisedlinearSVMs.InLeonBottou,OlivierChapelle,DennisDe-Coste,andJasonWeston,editors,Large-ScaleKernelMachines.MITPress.V.SindhwaniandD.Rosenberg.2008.Anrkhsformulti-viewlearningandmanifoldco-regularization.InAndrewMcCallumandSamRoweis,editors,Pro-ceedingsofthe25thAnnualInternationalConferenceonMachineLearning(ICML2008),pages976–983.Omnipress.VikasSindhwani,ParthaNiyogi,andMikhailBelkin.2005.Beyondthepointcloud:fromtransductivetosemi-supervisedlearning.InICML05,22ndInterna-tionalConferenceonMachineLearning.VikasSindhwani,SathiyaKeerthi,andOlivierChapelle.2006.Deterministicannealingforsemi-supervisedkernelmachines.InICML06,23rdInternationalCon-ferenceonMachineLearning,Pittsburgh,USA.DouglasThain,ToddTannenbaum,andMironLivny.2005.Distributedcomputinginpractice:thecondorexperience.Concurrency-PracticeandExperience,17(2-4):323–356.XiaojinZhu.2005.Semi-supervisedlearningliteraturesurvey.TechnicalReport1530,DepartmentofCom-puterSciences,UniversityofWisconsin,Madison.Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 28–36,

Boulder, Colorado, June 2009. c(cid:13)2009 Association for Computational Linguistics

28

IsUnlabeledDataSuitableforMulticlassSVM-basedWebPageClassiﬁcation?ArkaitzZubiaga,V´ıctorFresno,RaquelMart´ınezNLP&IRGroupatUNEDLenguajesySistemasInform´aticosE.T.S.I.Inform´atica,UNED{azubiaga,vfresno,raquel}@lsi.uned.esAbstractSupportVectorMachinespresentaninterest-ingandeffectiveapproachtosolveautomatedclassiﬁcationtasks.Althoughitonlyhan-dlesbinaryandsupervisedproblemsbyna-ture,ithasbeentransformedintomulticlassandsemi-supervisedapproachesinseveralworks.Apreviousstudyonsupervisedandsemi-supervisedSVMclassiﬁcationoverbi-narytaxonomiesshowedhowthelatterclearlyoutperformstheformer,provingthesuitabilityofunlabeleddataforthelearningphaseinthiskindoftasks.However,thesuitabilityofun-labeleddataformulticlasstasksusingSVMhasneverbeentestedbefore.Inthiswork,wepresentastudyonwhetherunlabeleddatacouldimproveresultsformulticlasswebpageclassiﬁcationtasksusingSupportVectorMa-chines.Asaconclusion,weencouragetorelyonlyonlabeleddata,bothforimproving(oratleastequaling)performanceandforreducingthecomputationalcost.1IntroductionTheamountofwebdocumentsisincreasinginaveryfastwayinthelastyears,whatmakesmoreandmorecomplicateditsorganization.Forthisrea-son,webpageclassiﬁcationhasgainedimportanceasatasktoeaseandimproveinformationaccess.Webpageclassiﬁcationcanbedeﬁnedasthetaskoflabelingandorganizingwebdocumentswithinasetofpredeﬁnedcategories.Inthiswork,wefocusonwebpageclassiﬁcationbasedonSupportVec-torMachines(SVM,(Joachims,1998)).Thiskindofclassiﬁcationtasksrelyonapreviouslylabeledtrainingsetofdocuments,withwhichtheclassi-ﬁeracquirestherequiredabilitytoclassifynewun-knowndocuments.Differentsettingscanbedistinguishedforwebpageclassiﬁcationproblems.Ontheonehand,at-tendingtothelearningtechniquethesystembaseson,itmaybesupervised,withallthetrainingdocu-mentspreviouslylabeled,orsemi-supervised,whereunlabeleddocumentsarealsotakenintoaccountduringthelearningphase.Ontheotherhand,attend-ingtothenumberofclasses,theclassiﬁcationmaybebinary,whereonlytwopossiblecategoriescanbeassignedtoeachdocument,ormulticlass,wherethreeormorecategoriescanbeset.Theformeriscommonlyusedforﬁlteringsystems,whereasthelatterisnecessaryforbiggertaxonomies,e.g.topi-calclassiﬁcation.Althoughmultiplestudieshavebeenmadefortextclassiﬁcation,itsapplicationtothewebpageclassiﬁcationarearemainswithoutenoughattention(QiandDavison,2007).Analyzingthenatureofawebpageclassiﬁcationtask,wecanconsiderittobe,generally,multiclassproblems,whereitisusualtoﬁndnumerousclasses.Inthesameway,ifwetakeintoaccountthatthenumberofavailablelabeleddocumentsistinycomparedtothesizeoftheWeb,thistaskbecomessemi-supervisedbesidesmulticlass.However,theoriginalSVMalgorithmsupportsneithersemi-supervisedlearningnormulticlasstax-onomies,duetoitsdichotomicandsupervisedna-ture.Tosolvethisissue,differentstudiesforbothmulticlassSVMandsemi-supervisedSVMap-proacheshavebeenproposed,butalittleefforthas29

beeninvestedinthecombinationofthem.(Joachims,1999)comparessupervisedandsemi-supervisedapproachesforbinarytasksusingSVM.Itshowsencouragingresultsforthetransductivesemi-supervisedapproach,clearlyimprovingthesu-pervised,andsoheprovedunlabeleddatatobesuitabletooptimizebinarySVMclassiﬁers’results.Ontheotherhand,thefewworkspresentedforsemi-supervisedmulticlassSVMclassiﬁcationdonotprovideclearinformationonwhethertheunla-beleddataimprovestheclassiﬁcationresultsincom-parisonwiththeonlyuseoflabeleddata.Inthiswork,weperformedanexperimentamongdifferentSVM-basedmulticlassapproaches,bothsupervisedandsemi-supervised.Theexperimentswerefocusedonwebpageclassiﬁcation,andwerecarriedoutoverthreebenchmarkdatasets:BankSearch,WebKBandYahoo!Science.Usingtheresultsofthecomparison,weanalyzeandstudythesuitabilityofunlabeleddataformulticlassSVMclassiﬁcationtasks.Wediscusstheseresultsandevaluatewhetheritisworthytorelyonasemi-supervisedSVMapproachtoconductthiskindoftasks.Theremainderofthisdocumentisorganizedasfollows.Next,insection2,webrieﬂyexplainhowSVMclassiﬁersworkforbinaryclassiﬁcations,bothforasupervisedandasemi-supervisedview.Insec-tion3,wecontinuewiththeadaptationofSVMtomulticlassenvironments,andshowwhathasbeendoneintheliterature.Section4presentsthedetailsoftheexperimentscarriedoutinthiswork,aimatevaluatingthesuitabilityofunlabeleddataformul-ticlassSVMclassiﬁcation.Insection5weshowanddiscusstheresultsoftheexperiments.Finally,insection6,weconcludewithourthoughtsandfuturework.2BinarySVMInthelastdecade,SVMhasbecomeoneofthemoststudiedtechniquesfortextclassiﬁcation,duetothepositiveresultsithasshown.Thistechniqueusesthevectorspacemodelforthedocuments’representa-tion,andassumesthatdocumentsinthesameclassshouldfallintoseparablespacesoftherepresenta-tion.Uponthis,itlooksforahyperplanethatsepa-ratestheclasses;therefore,thishyperplaneshouldmaximizethedistancebetweenitandthenearestdocuments,whatiscalledthemargin.Thefollowingfunctionisusedtodeﬁnethehyperplane(seeFigure1):f(x)=w·x+bFigure1:AnexampleofbinarySVMclassiﬁcation,sep-aratingtwoclasses(blackdotsfromwhitedots)Inordertoresolvethisfunction,allthepossiblevaluesshouldbeconsideredand,afterthat,theval-uesofwandbthatmaximizethemarginshouldbeselected.Thiswouldbecomputationallyexpensive,sothefollowingequivalentfunctionisusedtorelaxit(Boseretal.,1992)(CortesandVapnik,1995):min"12||w||2+ClXi=1ξdi#Subjectto:yi(w·xi+b)≥1−ξi,ξi≥0whereCisthepenaltyparameter,ξiisanstackvariablefortheithdocument,andlisthenumberoflabeleddocuments.Thisfunctioncanonlyresolvelinearlyseparableproblems,thustheuseofakernelfunctioniscom-monlyrequiredfortheredimensionofthespace;inthismanner,thenewspacewillbelinearlysepara-ble.Afterthat,theredimensionisundone,sothefoundhyperplanewillbetransformedtotheoriginalspace,respectingtheclassiﬁcationfunction.Best-knownkernelfunctionsincludelinear,polynomial,radialbasisfunction(RBF)andsigmoid,amongoth-ers.Differentkernelfunctions’performancehasbeenstudiedin(Sch¨olkopfandSmola,1999)and(Kivinenetal.,2002).30

Notethatthefunctionabovecanonlyresolvebi-naryandsupervisedproblems,sodifferentvariantsarenecessarytoperformsemi-supervisedormulti-classtasks.2.1Semi-supervisedLearningforSVM(S3VM)Semi-supervisedlearningapproachesdifferinthelearningphase.Asopposedtosupervisedap-proaches,unlabeleddataisusedduringthelearn-ingphase,andsoclassiﬁer’spredictionsoverthemisalsoincludedaslabeleddatatolearn.Thefactoftakingintoaccountunlabeleddatatolearncanim-provetheclassiﬁcationdonebysupervisedmethods,speciallywhenitspredictionsprovidenewusefulin-formation,asshowninﬁgure2.However,thenoiseaddedbyerroneuspredictionscanmakeworsethelearningphaseand,therefore,itsﬁnalperformance.Thismakesinterestingthestudyonwhetherrelyingonsemi-supervisedapproachesissuitableforeachkindoftask.Semi-supervisedlearningforSVM,alsoknownasS3VM,wasﬁrstintroducedby(Joachims,1999)inatransductiveway,bymodifyingtheoriginalSVMfunction.Todothat,heproposedtoaddanadditionaltermtotheoptimizationfunction:min12·||ω||2+C·lXi=1ξdi+C∗·uXj=1ξ∗djwhereuisthenumberofunlabeleddata.Nevertheless,theadaptationofSVMtosemi-supervisedlearningsigniﬁcantlyincreasesitscom-putationalcost,duetothenon-convexnatureoftheresultingfunction,andsoobtainingtheminimumvalueisevenmorecomplicated.Inordertorelaxthefunction,convexoptimizationtechniquessuchassemi-deﬁniteprogrammingarecommonlyused(Xuetal.,2007),whereminimizingthefunctiongetsmucheasier.Bymeansofthisapproach,(Joachims,1999)demonstratedalargeperformancegapbetweentheoriginalsupervisedSVMandhissemi-supervisedproposal,infavourofthelatterone.Heshowedthatforbinaryclassiﬁcationtasks,thesmalleristhetrainingsetsize,thelargergetsthedifferenceamongthesetwoapproaches.AlthoughheworkedFigure2:SVMvsS3VM,wherewhiteballsareunla-beleddocumentswithmulticlassdatasets,hesplittedtheproblemsintosmallerbinaryones,andsohedidnotdemon-stratewhetherthesameperformancegapoccursformulticlassclassiﬁcation.Thispapertriestocoverthisissue.(Chapelleetal.,2008)presentacompre-hensivestudyonS3VMapproaches.3MulticlassSVMDuetothedichotomicnatureofSVM,itcameuptheneedtoimplementnewmethodstosolvemulti-classproblems,wheremorethantwoclassesmustbeconsidered.Differentapproacheshavebeenpro-posedtoachievethis.Ontheonehand,asadirectapproach,(Weston,1999)proposedmodifyingtheoptimizationfunctiongettingintoaccountallthekclassesatonce:min12kXm=1||wm||2+ClXi=1Xm6=yiξmiSubjectto:wyi·xi+byi≥wm·xi+bm+2−ξmi,ξmi≥0Ontheotherhand,theoriginalbinarySVMclas-siﬁerhasusuallybeencombinedtoobtainamulti-classsolution.AscombinationsofbinarySVMclas-siﬁers,twodifferentapproachestok-classclassiﬁerscanbeemphasized(HsuandLin,2002):•one-against-allconstructskclassiﬁersdeﬁningthatmanyhyperplanes;eachofthemseparatestheclassifromtherestk-1.Forinstance,foraproblemwith4classes,1vs2-3-4,2vs1-3-4,3vs1-2-4and4vs1-2-3classiﬁerswould31

becreated.Newdocumentswillbecategorizedintheclassoftheclassiﬁerthatmaximizesthemargin:ˆCi=argmaxi=1,...,k(wix+bi).Asthenumberofclassesincreases,theamountofclassiﬁerswillincreaselinearly.•one-against-oneconstructsk(k−1)2classiﬁers,oneforeachpossiblecategorypair.Forin-stance,foraproblemwith4classes,1vs2,1vs3,1vs4,2vs3,2vs4and3vs4clas-siﬁerswouldbecreated.Afterthat,itclassi-ﬁeseachnewdocumentbyusingalltheclas-siﬁers,whereavoteisaddedforthewinningclassovereachclassiﬁer;themethodwillpro-posetheclasswithmorevotesastheresult.Asthenumberofclassesincreases,theamountofclassiﬁerswillincreaseinanexponentialway,andsotheproblemcouldbecameveryexpen-siveforlargetaxonomies.Both(Weston,1999)and(HsuandLin,2002)comparethedirectmulticlassapproachtotheone-against-oneandone-against-allbinaryclassiﬁercombiningapproaches.Theyagreeconcludingthatthedirectapproachdoesnotoutperformtheresultsbyone-against-onenorone-against-all,althoughitconsiderablyreducesthecomputationalcostbe-causethenumberofsupportvectormachinesitconstructsislower.Amongthebinarycombin-ingapproaches,theyshowtheperformanceofone-against-onetobesuperiortoone-against-all.Althoughtheseapproacheshavebeenwidelyusedinsupervisedlearningenvironments,theyhavescarcelybeenappliedtosemi-supervisedlearning.Becauseofthis,webelievethestudyonitsappli-cabilityandperformanceforthistypeofproblemscouldbeinteresting.3.1MulticlassS3VMWhenthetaxonomyisdeﬁnedbymorethantwoclassesandthenumberofpreviouslylabeleddoc-umentsisverysmall,thecombinationofbothmul-ticlassandsemi-supervisedapproachescouldbere-quired.Thatis,amulticlassS3VMapproach.Theusualwebpageclassiﬁcationproblemmeetswiththesecharacteristics,sincemorethantwoclassesareusuallyneeded,andthetinyamountoflabeleddocumentsrequirestheuseofunlabeleddataforthelearningphase.Actually,thereareafewworksfocusedontrans-formingSVMintoasemi-supervisedandmulticlassapproach.Asadirectapproach,aproposalby(Ya-jimaandKuo,2006)canbefound.TheymodifythefunctionformulticlassSVMclassiﬁcationandgetitusableforsemi-supervisedtasks.Theresultingop-timizationfunctionisasfollows:min12hXi=1βiTK−1βi+ClXj=1Xi6=yjmax{0,1−(βyjj−βij)}2whereβrepresentstheproductofavectorofvari-ablesandakernelmatrixdeﬁnedbytheauthor.Ontheotherhand,someotherworksarebasedondifferentapproachestoachieveamulticlassS3VMclassiﬁer.(Qietal.,2004)useFuzzyC-Means(FCM)topredictlabelsforunlabeleddocuments.Afterthat,multiclassSVMisusedtolearnwiththeaugmentedtrainingset,classifyingthetestset.(XuySchu-urmans,2005)relyonaclustering-basedapproachtolabeltheunlabeleddata.Afterwards,theyap-plyamulticlassSVMclassiﬁertothefullylabeledtrainingset.(Chapelleetal.,2006)presentadirectmulticlassS3VMapproachbyusingtheContinua-tionMethod.Ontheotherhand,thisistheonlywork,tothebestofourknowledge,thathastestedtheone-against-allandone-against-oneapproachesinasemi-supervisedenvironment.Theyapplythesemethodstosomenewsdatasets,forwhichtheygetlowperformance.Additionally,theyshowthatone-against-oneisnotsufﬁcientforreal-worldmulti-classsemi-supervisedlearning,sincetheunlabeleddatacannotberestrictedtothetwoclassesunderconsideration.Itisnoteworthythatmostoftheaboveworksonlypresentedtheirapproachesandcomparedthemtoothersemi-supervisedclassifyingmethods,suchasExpectation-Maximization(EM)orNaiveBayes.Asanexception,(Chapelleetal.,2006)comparedasemi-supervisedandasupervisedSVMapproach,butonlyoverimagedatasets.Againstthis,wefelttheneedtoevaluateandcomparemulticlassSVMandmulticlassS3VMapproaches,forthesakeofdiscoveringwhetherlearningwithunlabeledweb32

documentsishelpfulformulticlassproblemswhenusingSVMasaclassiﬁer.4MulticlassSVMversusMulticlassS3VMThemaingoalofthisworkistoevaluatetherealcontributionofunlabeleddataformulticlassSVM-basedwebpageclassiﬁcationtasks.Thereareafewworksusingsemi-supervisedmulticlassSVMclas-siﬁers,butnobodyhasdemonstrateditimprovessu-pervisedSVMclassiﬁer’sperformance.Next,wedetailtheexperimentswecarriedouttoclearupanydoubtsandtoensurewhichisbetterformulticlassSVM-basedwebpageclassiﬁcations.4.1ApproachesInordertoevaluateandcomparemulticlassSVMandmulticlassS3VM,wedecidedtousethreediffer-entbutequivalentapproachesforeachview,super-visedandsemi-supervised.Forfurtherinformationontheseapproaches,seesection3.Weaddasufﬁx,-SVMor-S3VM,tothenamesoftheapproaches,todifferentiatewhethertheyarebasedinasupervisedorasemi-supervisedalgorithm.Onthepartofthesemi-supervisedview,thefol-lowingthreeapproacheswereselected:•2-steps-SVM:wecalled2-steps-SVMtothetechniquebasedonthedirectmulticlasssu-pervisedapproachexposedinsection3.Thismethodworks,onitsﬁrststep,withthetrain-ingcollection,learningwiththelabeleddocu-mentsandpredictingtheunlabeledones;afterthat,thelatterdocumentsarelabeledbasedonthegeneratedpredictions.Onthesecondstep,nowwithafullylabeledtrainingset,theusualsupervisedclassiﬁcationprocessisdone,learn-ingwiththetrainingdocumentsandpredictingthedocumentsinthetestset.Thisapproachissomehowsimilartothosepro-posedby(Qietal.,2004)and(XuySchu-urmans,2005).Nonetheless,the2-steps-SVMapproachusesthesamemethodforboththeﬁrstandsecondsteps.AsupervisedmulticlassSVMisusedtoincreasethelabeledsetand,af-terthat,toclassifythetestset.•one-against-all-S3VM:theone-against-allap-proachhasnotsufﬁcientlybeentestedforsemi-supervisedenvironments,andseemsinterest-ingtoevaluateitsperformance.•one-against-one-S3VM:theone-against-onedoesnotseemtobesuitableforsemi-supervisedenvironments,sincetheclassiﬁerisnotabletoignoretheinadecuateunlabeleddoc-umentsforeach1-vs-1binarytask,asstatedby(Chapelleetal.,2006).Anyway,sinceithasscarcelybeentested,wealsoconsiderthisap-proach.Ontheotherhand,theapproachesselectedforthesupervisedviewwerethese:(1)1-step-SVM;(2)one-against-all-SVM,and(3)one-against-one-SVM.Thethreeapproachesmentionedaboveareanal-ogoustothesemi-supervisedapproaches,2-steps-SVM,one-against-all-S3VMandone-against-one-S3VM,respectively.Theydifferinthelearningphase:unlikethesemi-supervisedapproaches,thesethreesupervisedapproachesonlyrelyonthelabeleddocumentsforthelearningtask,butafterthattheyclassifythesametestdocuments.Theseapproachesallowtoevaluatewhethertheunlabeleddocumentsarecontributinginapositiveornegativewayinthelearningphase.4.2DatasetsFortheseexperimentswehaveusedthreewebpagebenchmarkdatasetspreviouslyusedforclassiﬁca-tiontasks:•BankSearch(SinkaandCorne,2002),acol-lectionof11,000webpagesover11classes,withverydifferenttopics:commercialbanks,buildingsocieties,insuranceagencies,java,c,visualbasic,astronomy,biology,soccer,mo-torsportsandsports.Weremovedthecategorysports,sinceitincludesbothsoccerandmotor-sportsinit,asaparentcategory.Thisresults10,000webpagesover10categories.4,000in-stanceswereassignedtothetrainingset,whiletheother6,000wereleftonthetestset.•WebKB1,withatotalof4,518documentsof4universities,andclassiﬁedinto7classes1http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/33

(student,faculty,personal,department,course,projectandother).Theclassnamedotherwasremovedduetoitsambiguity,andsoweﬁnallygot6classes.2,000instancesfellintothetrain-ingset,and2,518tothetestset.•Yahoo!Science(Tan,2002),with788scien-tiﬁcdocuments,classiﬁedinto6classes(agri-culture,biology,earthscience,math,chemistryandothers).Weselected200documentsforthetrainingset,and588forthetestset.Withinthetrainingset,foreachdataset,multipleversionswerecreated,modifyingthenumberofla-beleddocuments,whiletherestwereleftunlabeled.Thus,thesizeoflabeledsubsetwithinthetrainingsetchanges,rangingfrom50webdocumentstothewholetrainingset.4.3DocumentRepresentationSVMrequiresavectorialrepresentationofthedocu-mentsasaninputfortheclassiﬁer,bothfortrainandtestphases.Toobtainthisvectorialrepresentation,weﬁrstconvertedtheoriginalhtmlﬁlesintoplaintextﬁles,removingallthehtmltags.Afterthat,weremovedthenoisytokens,suchasURLs,emailad-dressesorsomestopwords.Fortheseediteddocu-ments,thetf-idftermweightingfunctionwasusedtodeﬁnethevaluesfortheunitermsfoundonthetexts.Asthetermdimensionalitybecametoolarge,wethenremovedtheleast-frequenttermsbyitsdoc-umentfrequency;termsappearinginlessthan0.5%ofthedocumentswereremovedfortherepresenta-tion.Theremainingunitermsdeﬁnethevectorspacedimensions.Thatderivedtermvectorswith8285di-mensionsforBankSearchdataset,3115forWebKBand8437forYahoo!Science.4.4ImplementationTocarryoutourexperiments,webasedonfreelyavailableandalreadytestedandexperimentedsoft-ware.DifferentSVMclassiﬁerswereneededtoim-plementthemethodsdescribedinsection4.1.SVMlight2wasusedtoworkwithbinarysemi-supervisedclassiﬁersfortheone-against-all-S3VMandone-against-one-S3VMapproaches.Inthesameway,weimplementedtheirsupervisedversions,2http://svmlight.joachims.orgone-against-all-SVMandone-against-one-SVM,inordertoevaluatethecontributionofunlabeleddata.Toachievethesupervisedapproaches,weignoredtheunlabeleddataduringthetrainingphaseand,af-terthat,testedwiththesametestsetusedforsemi-supervisedapproaches.Thedefaultsettingsusingapolynomialkernelwereselectedfortheexperi-ments.SVMmulticlass3wasusedtoimplementthe2-steps-SVMapproach,byusingittwotimes.Firstly,totrainthelabeleddataandclassifyunlabeleddata.Afterthat,totrainwiththewholetrainingsetlabeledwithclassiﬁer’spredictions,andtotestwiththetestset.Inthesameway,the1-step-SVMmethodwasimplementedbyignoringunlabeleddataandtrain-ingonlythelabeleddata.Thismethodallowstoevaluatethecontributionofunlabeleddataforthe2-steps-SVMmethod.4.5EvaluationMeasuresFortheevaluationoftheexperimentsweusedtheaccuracytomeasuretheperformance,sinceithasbeenfrequentlyusedfortextclassiﬁcationprob-lems,speciallyformulticlasstasks.Theaccuracyoffersthepercentofthecorrectpredictionsforthewholetestset.Wehaveconsideredthesameweightforallthecorrectguessesforanyclass.Acorrectpredictioninanyoftheclasseshasthesamevalue,thusnoweightingexists.Ontheotherhand,anaveragedaccuracyevalu-ationisalsopossibleforthebinarycombiningap-proaches.Anaveragedaccuracymakespossibletoevaluatetheresultsbyeachbinaryclassiﬁer,andprovidesanaveragedvalueforthewholebinaryclassiﬁerset.Itisworthtonotethatthesevaluesdonotprovideanyinformationfortheevaluationofthecombinedmulticlassresults,butonlyforevaluatingeachbinaryclassiﬁerbeforecombiningthem.5ResultsandDiscussionNext,weshowanddiscusstheresultsofourexperi-ments.Itisremarkablethatbothone-against-one-SVMandone-against-one-S3VMapproacheswereveryinferiortotherest,andsowedecidednottoplottheminordertomaintaingraphs’clarity.Hence,ﬁgures3,4and5showtheresultsinaccordance3http://www.cs.cornell.edu/People/tj/svmlight/svmmulticlass.html34

withthelabeledsubsetsizeforthe2-steps-SVM,1-step-SVM,one-against-all-S3VMandone-against-all-SVMapproacheswithinourexperiments.Fortheresultstobemorerepresentative,nineexecu-tionsweredoneforeachsubset,obtainingthemeanvalue.Thesenineexecutionsvaryonthelabeledsubsetwithinthetrainingset.Thefactthatone-against-one-S3VMhasbeentheworstapproachforourexperimentsconﬁrmsthatthenoiseaddedbytheunlabeleddocumentswithineach1-vs-1binaryclassiﬁcationtaskisharmfultothelearningphase,anditisnotcorrectedwhenmergingallthebinarytasks.Theaveragedaccuracyforthecombinedbi-naryclassiﬁersallowstocomparetheone-against-oneandone-against-allviews.Theaveragedac-curacyforone-against-one-S3VMshowsverylowperformance(about60%inmostcases),whereasthesamevalueforone-against-all-S3VMismuchhigher(about90%inmostcases).Thisisobvi-oustohappenfortheone-against-allview,sinceitismucheasiertopredictdocumentsnotpertain-ingtotheclassunderconsiderationforeach1-vs-allbinaryclassiﬁer.Althougheachbinaryclassiﬁergetsabout90%accuracyfortheone-against-one-S3VMapproach,thisvaluefallsconsiderablywhencombiningthemtogetthemulticlassresult.Thisshowstheadditionaldifﬁcultyformulticlassprob-lemscomparedtobinaryones.Hence,thedifﬁcultytocorrectlypredictunlabeleddataincreasesformul-ticlasstasks,anditismorelikelytoaddnoiseduringthelearningphase.Figure3:ResultsforBankSearchdatasetFigure4:ResultsforWebKBdatasetFigure5:ResultsforYahoo!SciencedatasetForallthedatasetsweworkedwith,thereisanoticeableperformancegapbetweendirectmulti-classandbinarycombiningapproaches.Both2-steps-SVMand1-step-SVMarealwaysonthetopofthegraphs,andone-against-all-S3VMandone-against-all-SVMapproachesaresofarfromcatch-ingupwiththeirresults,exceptforWebKBdataset,wherethegapisnotsonoticeable.Thisseemsen-couraging,sinceconsideringlesssupportvectorsinadirectmulticlassapproachreducesthecomputa-tionalcostandimprovestheﬁnalresults.Comparingthetwoanalogousapproachesamongthem,differentconclusionscouldbeextracted.Ontheonehand,one-against-all-S3VMshowsslightlybetterresultsthanone-against-all-SVM,andsoconsideringunlabeleddocumentsseemstobe35

favourablefortheone-against-allview.Ontheotherhand,thedirectmulticlassviewshowsvaryingre-sults.Both2-steps-SVMand1-step-SVMshowverysimilarresultsforBankSearchandYahoo!Sciencedatasets,butsuperiorfor1-step-SVMovertheWe-bKBdataset.Asaconclusionofthis,ignoringun-labeleddocumentsbymeansofthe1-step-SVMap-proachseemstobeadvisable,sinceitreducesthecomputationcost,obtainingatleastthesameresultsthanthesemi-supervised2-steps-SVM.Althoughtheirresultsaresopoor,aswesaidabove,thesupervisedapproachwinsfortheone-against-oneview;thisconﬁrms,again,thattheone-against-oneviewisnotanadecuateviewtobeap-pliedinasemi-supervisedenvironment,duetothenoiseexistingduringthelearningphase.Whenanalyzingtheperformancegapsbetweentheanalogousapproaches,ageneralconclusioncanbeextracted:thesmalleristhelabeledsubsetthebiggeristheperformancegap,exceptfortheYa-hoo!Sciencedataset.Comparingthetwobestapproaches,1-step-SVMand2-steps-SVM,theper-formancegapincreaseswhenthenumberofla-beleddocumentsdecreaseforBankSearch;forthisdataset,theaccuracyby1-step-SVMis0.92timestheoneby2-steps-SVMwhenthenumberoflabeleddocumentsisonly50,butthisproportiongoesto0.99with500labeleddocuments.Thisreﬂectshowthecontributionofunlabeleddatadecreaseswhilethelabeledsetincreases.ForWebKB,theperfor-mancegapisinfavourof1-step-SVM,andvariesbetween1.01and1.05times2-steps-SVMmethod’saccuracy,evenwithonly50labeleddocuments.Again,increasingthelabeledsetnegativelyaffectssemi-supervisedalgorithm’sperformance.Last,forYahoo!Science,theperformancegapamongthesetwoapproachesisnotconsiderable,sincetheirre-sultsareverysimilar.Ourconjecturefortheperformancedifferencebe-tween1-step-SVMand2-steps-SVMforthethreedatasetsisthenatureoftheclasses.Theaccuracybysemi-supervised2-steps-SVMisslightlyhigherforBankSearchandYahoo!Science,wheretheclassesarequiteheterogeneous.Ontheotherhand,theaccuracybysupervised1-step-SVMisclearlyhigherforWebKB,wherealltheclassesareanaca-demictopic,andsomorehomogeneous.Thesemi-supervisedclassiﬁersshowamajorproblemforpre-dictingtheunlabeleddocumentswhenthecollectionismorehomogeneous,andsomoredifﬁculttodifferbetweenclasses.Insummary,themainideaisthatunlabeleddoc-umentsdonotseemtocontributeastheywouldformulticlasstasksusingSVM.Withintheapproacheswetested,thesupervised1-step-SVMapproachshowsthebest(orverysimilartothebestinsomecases)resultsinaccuracyand,takingintoaccountitistheleast-expensiveapproach,westronglyen-couragetousethisapproachtosolvemulticlasswebpageclassiﬁcationtasks,mainlywhentheclassesunderconsiderationarehomogeneous.6ConclusionsandOutlookWehavestudiedandanalyzedthecontributionofconsideringunlabeleddataduringthelearningphaseformulticlasswebpageclassiﬁcationtasksusingSVM.Ourresultsshowthatignoringunlabeleddoc-umenttolearnreducescomputationalcostand,ad-ditionaly,obtainssimilarorslightlyworseaccuracyvaluesforheterogeneustaxonomies,buthigherforhomogeneousones.Thereforeweshowthat,unlikeforbinarycases,aswasshownby(Joachims,1999),asupervisedviewoutperformsasemi-supervisedoneformulticlassenvironments.Ourthoughtisthatpredictingunlabeleddocuments’classismuchmoredifﬁcultwhenthenumberofclassesincreases,andso,themistakenlabeleddocumentsareharmfulforclassiﬁer’slearningphase.Asafuturework,adirectsemi-supervisedmulti-classapproach,suchasthoseproposedby(YajimaandKuo,2006)and(Chapelleetal.,2006),shouldalsobeconsidered,aswellassettingtheclassiﬁerwithdifferentparametersorkernels.Balancingtheweightofpreviouslyandnewlylabeleddatacouldalsobeinterestingtoimprovesemi-supervisedap-proaches’results.AcknowledgmentsWewishtothanktheanonymousreviewersfortheirhelpfulandinstructivecomments.ThisworkhasbeensupportedbytheResearchNetworkMAVIR(S-0505/TIC-0267),theRegionalMinistryofEd-ucationoftheCommunityofMadrid,andbytheSpanishMinistryofScienceandInnovationprojectQEAVis-Catiex(TIN2007-67581-C02-01).36

ReferencesB.E.Boser,I.GuyonandV.Vapnik.1992.ATrainingAlgorithmforOptimalMarginClassiﬁers.Proceed-ingsofthe5thAnnualWorkshoponcomputationalLearningTheory.C.Campbell.2000.AlgorithmicApproachestoTrainingSupportVectorMachines:ASurveyProceedingsofESANN’2000,EuropeanSymposiumonArtiﬁcialNeu-ralNetworks.O.Chapelle,M.ChiyA.Zien2006.AContinuationMethodforSemi-supervisedSVMs.ProceedingsofICML’06,the23rdInternationalConferenceonMa-chineLearning.O.Chapelle,V.Sindhwani,S.Keerthi2008.Optimiza-tionTechniquesforSemi-SupervisedSupportVectorMachines.J.Mach.Learn.Res..C.CortesandV.Vapnik.1995.SupportVectorNetwork.MachineLearning.C.-H.HsuandC.-J.Lin.2002.AComparisonofMeth-odsforMulticlassSupportVectorMachines.IEEETransactionsonNeuralNetworks.T.Joachims.1998.TextCategorizationwithSupportVectorMachines:LearningwithmanyRelevantFea-tures.ProceedingsofECML98,10thEuropeanCon-ferenceonMachineLearning.T.Joachims.1999.TransductiveInferenceforTextClassiﬁcationUsingSupportVectorMachines.Pro-ceedingsofICML99,16thInternationalConferenceonMachineLearning.J.KivinenandE.J.SmolaandR.C.Williamson.2002.LearningwithKernels.T.Mitchell.1997.MachineLearning.McGrawHill.H.-N.Qi,J.-G.Yang,Y.-W.ZhongyC.Deng2004.Multi-classSVMBasedRemoteSensingImageClassiﬁcationanditsSemi-supervisedImprovementScheme.Proceedingsofthe3rdICMLC.X.QiandB.D.Davison.2007.WebPageClassiﬁcation:FeaturesandAlgorithms.TechnicalReportLU-CSE-07-010.B.Sch¨olkopfandA.Smola.1999.AdvancesinKernelMethods:SupportVectorLearning.MITPress.F.Sebastiani.2002.MachineLearninginAutomatedTextCategorization.ACMComputingSurveys,pp.1-47.M.P.SinkaandD.W.Corne.2002.ANewBenchmarkDatasetforWebDocumentClustering.SoftComput-ingSystems.C.M.Tan,Y.F.WangandC.D.Lee.2002.TheUseofBigramstoEnhanceTextCategorization.InformationProcessingandManagement.J.WestonandC.Watkins.1999.Multi-classSupportVectorMachines.ProceedingsofESAAN,theEuro-peanSymposiumonArtiﬁcialNeuralNetworks.L.XuyD.Schuurmans.2005.UnsupervisedandSemi-supervisedMulticlassSupportVectorMachines.Pro-ceedingsofAAAI’05,the20thNationalConferenceonArtiﬁcialIntelligence.Z.Xu,R.Jin,J.Zhu,I.KingandM.R.Lyu.2007.Ef-ﬁcientConvexOptimizationforTransductiveSupportVectorMachine.AdvancesinNeuralInformationPro-cessingSystems.Y.YajimaandT.-F.Kuo.2006.OptimizationAp-proachesforSemi-SupervisedMulticlassClassiﬁca-tion.ProceedingsofICDM’06Workshops,the6thIn-ternationalConferenceonDataMining.Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 37–42,

Boulder, Colorado, June 2009. c(cid:13)2009 Association for Computational Linguistics

37

AComparisonofStructuralCorrespondenceLearningandSelf-trainingforDiscriminativeParseSelectionBarbaraPlankUniversityofGroningen,TheNetherlandsb.plank@rug.nlAbstractThispaperevaluatestwosemi-supervisedtechniquesfortheadaptationofaparseselec-tionmodeltoWikipediadomains.Thetech-niquesexaminedareStructuralCorrespon-denceLearning(SCL)(Blitzeretal.,2006)andSelf-training(Abney,2007;McCloskyetal.,2006).ApreliminaryevaluationfavorstheuseofSCLoverthesimplerself-trainingtech-niques.1IntroductionandMotivationParseselectionconstitutesanimportantpartofmanyparsingsystems(Haraetal.,2005;vanNoordandMalouf,2005;McCloskyetal.,2006).Yet,thereislittletonoworkfocusingontheadaptationofparseselectionmodelstonoveldomains.Thisismostprobablyduetothefactthatpotentialgainsforthistaskareinherentlyboundedbytheunder-lyinggrammar.Thefewstudiesonadaptingparsedisambiguationmodels,likeHaraetal.(2005),havefocusedexclusivelyonsuperviseddomainadapta-tion,i.e.onehasaccesstoacomparablysmall,butlabeledamountoftargetdata.Incontrast,insemi-superviseddomainadaptationonehasonlyunla-beledtargetdata.Itisamorerealisticsituation,butatthesametimealsoconsiderablymoredifﬁcult.Inthispaperweevaluatetwosemi-supervisedapproachestodomainadaptationofadiscrimina-tiveparseselectionmodel.WeexamineStruc-turalCorrespondenceLearning(SCL)(Blitzeretal.,2006)forthistask,andcompareittoseveralvariantsofSelf-training(Abney,2007;McCloskyetal.,2006).Forempiricalevaluation(section4)weusetheAlpinoparsingsystemforDutch(vanNoordandMalouf,2005).Astargetdomain,weexploitWikipediaasprimarytestandtrainingcollection.2PreviousWorkSofar,StructuralCorrespondenceLearninghasbeenappliedsuccessfullytoPoStaggingandSen-timentAnalysis(Blitzeretal.,2006;Blitzeretal.,2007).AnattemptwasmadeintheCoNLL2007sharedtasktoapplySCLtonon-projectivede-pendencyparsing(ShimizuandNakagawa,2007).However,thesystemjustendedupatrank7outof8teams.Basedonannotationdifferencesinthedatasets(Dredzeetal.,2007)andabugintheirsys-tem(ShimizuandNakagawa,2007),theirresultsareinconclusive.Arecentattempt(Plank,2009)showspromisingresultsonapplyingSCLtoparsedisam-biguation.Inthispaper,weextendthatlineofworkandcompareSCLtobootstrappingapproachessuchasself-training.Studiesonself-traininghavefocusedmainlyongenerative,constituentbasedparsing(Steedmanetal.,2003;McCloskyetal.,2006;ReichartandRap-poport,2007).Steedmanetal.(2003)aswellasRe-ichartandRappoport(2007)examineself-trainingforPCFGparsinginthesmallseedcase(<1kla-beleddata),withdifferentresults.Incontrast,Mc-Closkyetal.(2006)focusonlargeseedsandexploitareranking-parser.Improvementsareobtained(Mc-Closkyetal.,2006;McCloskyandCharniak,2008),showingthatarerankerisnecessaryforsuccessfulself-traininginsuchahigh-resourcescenario.Whiletheyself-trainedagenerativemodel,weexamineself-trainingandSCLforsemi-supervisedadapta-tionofadiscriminativeparseselectionsystem.38

3Semi-supervisedDomainAdaptation3.1StructuralCorrespondenceLearningStructuralCorrespondenceLearning(Blitzeretal.,2006)exploitsunlabeleddatafrombothsourceandtargetdomaintoﬁndcorrespondencesamongfea-turesfromdifferentdomains.Thesecorrespon-dencesarethenintegratedasnewfeaturesinthela-beleddataofthesourcedomain.TheoutlineofSCLisgiveninAlgorithm1.ThekeytoSCListoexploitpivotfeaturestoau-tomaticallyidentifyfeaturecorrespondences.Piv-otsarefeaturesoccurringfrequentlyandbehavingsimilarlyinbothdomains(Blitzeretal.,2006).TheycorrespondtoauxiliaryproblemsinAndoandZhang(2005).Foreverysuchpivotfeature,abinaryclassiﬁeristrained(step2ofAlgorithm1)bymask-ingthepivotfeatureinthedataandtryingtopredictitwiththeremainingnon-pivotfeatures.Non-pivotsthatcorrelatewithmanyofthesamepivotsareas-sumedtocorrespond.Thesepivotpredictorweightvectorsthusimplicitlyalignnon-pivotfeaturesfromsourceandtargetdomain.Intuitively,ifweareabletoﬁndgoodcorrespondencesthrough’linking’piv-ots,thentheaugmentedsourcedatashouldtransferbettertoatargetdomain(Blitzeretal.,2006).Algorithm1SCL(Blitzeretal.,2006)1:Selectmpivotfeatures.2:Trainmbinaryclassiﬁers(pivotpredictors).Cre-atematrixWn×mofbinarypredictorweightvectorsW=[w1,..,wm],withnnumberofnonpivots.3:DimensionalityReduction.ApplySVDtoW:Wn×m=Un×nDn×mVTm×mandselectθ=UT[1:h,:](thehtopleftsingularvectorsofW).4:Trainanewmodelontheoriginalandnewfeaturesobtainedbyapplyingtheprojectionx·θ.SCLforDiscriminativeParseSelectionSofar,pivotfeaturesonthewordlevelwereused(Blitzeretal.,2006;Blitzeretal.,2007).However,forparsedisambiguationbasedonaconditionalmodeltheyareirrelevant.Hence,wefollowPlank(2009)andactuallyﬁrstparsetheunlabeleddata.Thisallowsapossiblynoisy,butmoreabstractrepresentationoftheunderlyingdata.Featuresthuscorrespondtopropertiesofparses:applicationofgrammarrules(r1,r2features),dependencyrelations(dep),PoStags(f1,f2),syntacticfeatures(s1),precedence(mf),bilexicalpreferences(z),apposition(appos)andfur-therfeaturesforunknownwords,temporalphrases,coordination(h,inyearandp1,respectively).ThesefeaturesarefurtherdescribedinvanNoordandMal-ouf(2005).SelectionofpivotfeaturesAspivotfeaturesshouldbecommonacrossdomains,herewerestrictourpivotstobeofthetyper1,p1,s1(themostfre-quentlyoccurringfeaturetypes).Inmoredetail,r1indicateswhichgrammarruleapplied,p1whethercoordinationconjunctsareparallel,ands1whetherlocal/non-localextractionoccurred.Wecounthowofteneachfeatureappearsintheparsedsourceandtargetdomaindata,andselectthoser1,p1,s1fea-turesaspivotfeatures,whosecountis>t,wheretisaspeciﬁedthreshold.Inallourexperiments,wesett=5000.Inthiswayweobtainedonaverage360pivotfeatures,onthedatasetsdescribedinSec-tion4.3.2Self-trainingSelf-training(Algorithm2)isasimplesingle-viewbootstrappingalgorithm.Inself-training,thenewlylabeledinstancesaretakenatfacevalueandaddedtothetrainingdata.Therearemanypossiblewaystoinstantiateself-training(Abney,2007).Onevariant,introducedinAbney(2007)isthenotionof’(in)delibility’:inthedeliblecasetheclassiﬁerrelabelsalloftheunla-beleddatafromscratchineveryiteration.Theclas-siﬁermaybecomeunconﬁdentaboutpreviouslyse-lectedinstancesandtheymaydropout(StevenAb-ney,personalcommunication).Incontrast,intheindeliblecase,labelsonceassigneddonotchangeagain(Abney,2007).Inthispaperwelookatthefollowingvariantsofself-training:•singleversusmultipleiterations,•selectionversusnoselection(takingallself-labeleddataorselectingpresumablyhigherqualityinstances);differentscoringfunctionsforselection,•delibilityversusindelibilityformultipleitera-tions.39

Algorithm2Self-training(indelible)(Abney,2007).1:L0islabeled[seed]data,Uisunlabeleddata2:c←train(L0)3:repeat4:L←L+select(label(U−L,c))5:c←train(L)6:untilstoppingcriterionismetScoringmethodsWeexaminethreesimplescor-ingfunctionsforinstanceselection:i)Entropy(−Py∈Y(s)p(ω|s,θ)logp(ω|s,θ)).ii)Numberofparses(|Y(s)|);andiii)SentenceLength(|s|).4ExperimentsandResultsExperimentalDesignThesystemusedinthisstudyisAlpino,atwo-stagedependencyparserforDutch(vanNoordandMalouf,2005).TheﬁrststageconsistsofaHPSG-likegrammarthatconsti-tutestheparsegenerationcomponent.ThesecondstageisaMaximumEntropy(MaxEnt)parseselec-tionmodel.TotraintheMaxEntmodel,parame-tersareestimatedbasedoninformativesamples(Os-borne,2000).Aparseisaddedtothetrainingdatawithascoreindicatingits“goodness”(vanNoordandMalouf,2005).Thescoreisobtainedbycom-paringitwiththegoldstandard(ifavailable;other-wisethescoreisapproximatedthroughparseproba-bility).ThesourcedomainistheAlpinoTreebank(vanNoordandMalouf,2005)(newspapertext;approx.7,000sentences;145ktokens).WeuseWikipediabothastestsetandasunlabeledtargetdatasource.Weassumethatinordertoparsedatafromaveryspeciﬁcdomain,sayabouttheartistPrince,thendatarelatedtothatdomain,likeinformationabouttheNewPowerGeneration,thePurplerainmovie,orotherAmericansingersandartists,shouldbeofhelp.Thus,weexploitWikipedia’scategorysystemtogatherdomain-speciﬁctargetdata.Inourempiri-calsetup,wefollowBlitzeretal.(2006)andbalancethesizeofsourceandtargetdata.Thus,dependingonthesizeoftheresultingtargetdomaindataset,andthe“broadness”ofthecategoriesinvolvedincreat-ingit,wemightwishtoﬁlteroutcertainpages.Weimplementedaﬁltermechanismthatexcludespagesofacertaincategory(e.g.asupercategorythatishy-pothesizedtobe“toobroad”).Furtherdetailsaboutthedatasetconstructionaregivenin(Plank,2009).Table1providesinformationonthetargetdomaindatasetsconstructedfromWikipedia.RelatedtoArticlesSentsTokensRelationshipPrince2909,772145,504ﬁlteredsuperPaus4458,832134,451allDeMorgan3948,466132,948allTable1:Sizeofrelatedunlabeleddata;relationshipin-dicateswhetherallrelatedpagesareusedorsomeareﬁlteredout.ThesizeofthetargetdomaintestsetsisgiveninTable2.Asevaluationmeasureconceptaccuracy(CA)(vanNoordandMalouf,2005)isused(similartolabeleddependencyaccuracy).Thetrainingdataforthepivotpredictorsarethe1-bestparsesofsourceandtargetdomaindataasselectedbytheoriginalAlpinomodel.WereportonresultsofSCLwithdimensionalityparametersettoh=25,andremainingsettingsidenticaltoPlank(2009)(i.e.,nofeature-speciﬁcregularizationandnofeaturenormalizationandrescaling).BaselineTable2showsthebaselineaccuracies(modeltrainedonlabeledout-of-domaindata)ontheWikipediatestsets(lastcolumn:sizeinnumberofsentences).Thesecondandthirdcolumnindicatelower(ﬁrstparse)andupper-(oracle)bounds.WikipediaarticlebaselineﬁrstoraclesentPrince(musician)85.0371.9588.70357PausJohannesPaulusII85.7274.3089.09232AugustusDeMorgan80.0970.0883.52254Table2:SupervisedBaselineresults.SCLandSelf-trainingresultsTheresultsforSCL(Table3)showasmall,butconsistentincreaseinabsoluteperformanceonalltestsetsoverthebase-lines(upto+0.27absoluteCAor7.34%relativeerrorreduction,whichissigniﬁcantatp<0.05ac-cordingtosigntest).Incontrast,basicself-training(Table3)achievesroughlyonlybaselineaccuracyandlowerperfor-mancethanSCL,withoneexception.OntheDe-Morgantestset,self-trainingscoresslightlyhigherthanSCL.However,theimprovementsofbothSCLandself-trainingarenotsigniﬁcantonthisrather40

smalltestset.Indeed,self-trainingscoresbetterthanthebaselineononly5parsesoutof254,whileitsperformanceisloweron2,leavingonly3parsesthataccountforthedifference.CAφRel.ERPrincebaseline85.0378.060.00SCL⋆85.3079.677.34Self-train(all-at-once)85.0878.381.46Pausbaseline85.7277.230.00SCL85.8277.872.81Self-train(all-at-once)85.7877.621.71DeMorganbaseline80.0974.440.00SCL80.1574.921.88Self-train(all-at-once)80.2475.634.65Table3:ResultsofSCLandself-training(singleitera-tion,noselection).Entriesmarkedwith⋆arestatisticallysigniﬁcantatp<0.05.Theφscoreincorporatesupper-andlower-bounds.Togaugewhetherotherinstantiationsofself-trainingaremoreeffective,weevaluatedtheself-trainingvariantsintroducedinsection3.2onthePrincedataset.Intheiterativesetting,wefol-lowSteedmanetal.(2003)andparse30sentencesfromwhich20areselectedineveryiteration.Withregardtothecomparisonofdelibleversusindelibleself-training(whetherlabelsmaychange),ourempiricalﬁndingsshowsthatthetwocasesachieveverysimilarperformance;thetwocurveshighlyoverlap(Figure1).Theaccuraciesofbothcurvesﬂuctuatearound85.13,showingnoupwardordownwardtrend.Ingeneral,however,indelibilityispreferredsinceittakesconsiderablylesstime(theclassiﬁerdoesnothavetorelabelUfromscratchineveryiteration).Inaddition,wetestedEM(whichusesallunlabeleddataineachiteration).Itsper-formanceisconsistentlylower,varyingaroundthebaseline.Figure2comparesseveralself-trainingvariantswiththesupervisedbaselineandSCL.Itsumma-rizestheeffectofi)selectionversusnoselection(andvariousselectiontechniques)aswellasii)sin-gleversusmultipleiterationsofself-training.Forclarity,theﬁgureshowsthelearningcurveofthebestselectiontechniqueonly,butdepictstheperfor-manceofthevariousselectiontechniquesinasingleiteration(non-solidlines).Intheiterativesetting,takingthewholeself-labeleddataandnotselectingcertaininstances(greycurveinFigure2)degradesperformance.Incon-trast,selectingshortersentencesslightlyimprovesaccuracy,andisthebestselectionmethodamongtheonestested(shortersentences,entropy,fewerparses).Forallself-traininginstantiations,runningmulti-pleiterationsisonaveragejustthesameasrunningasingleiteration(thenon-solidlinesareroughlytheaverageofthelearningcurves).Thusthereisnorealneedtorunseveraliterationsofself-training.ThemainconclusionisthatincontrasttoSCL,noneoftheself-traininginstantiationsachievesasigniﬁcantimprovementoverthebaseline.5ConclusionsandFutureWorkThepapercomparesStructuralCorrespondenceLearning(Blitzeretal.,2006)with(variousin-stancesof)self-training(Abney,2007;McCloskyetal.,2006)fortheadaptationofaparseselectionmodeltoWikipediadomains.Theempiricalﬁndingsshowthatnoneoftheeval-uatedself-trainingvariants(delible/indelible,singleversusmultipleiterations,variousselectiontech-niques)achievesasigniﬁcantimprovementoverthebaseline.Themore’indirect’exploitationofunla-beleddatathroughSCLismorefruitfulthanpureself-training.Thus,favoringtheuseofthemorecomplexmethod,althoughtheﬁndingsarenotcon-ﬁrmedonalltestsets.Ofcourse,ourresultsarepreliminaryand,ratherthanwarrantingyetmanydeﬁniteconclusions,en-couragefurtherinvestigationofSCL(varyingsizeoftargetdata,pivotsselection,biggertestsetsaswellasotherdomainsetc.)aswellasrelatedsemi-supervisedadaptationtechniques.AcknowledgmentsThankstoGertjanvanNoordandtheanonymousre-viewersfortheircomments.TheLinuxclusteroftheHigh-PerformanceComputingCenteroftheUniver-sityofGroningenwasusedinthiswork.41

05010015020085.0085.0585.1085.1585.2085.2585.30number of iterationsaccuracyIndelibility versus delibilitybaselineSCLIndelible SelfTrainDelible SelfTrainEMFigure1:DelibleversusIndelibleself-trainingandEM.Delibleandindelibleself-trainingachieveverysimilarper-formance.However,indelibilityispreferredoverdelibilitysinceitisconsiderablyfaster.05010015020085.0085.0585.1085.1585.2085.2585.30number of iterationsaccuracyshorter sententropyfewer parses / no selectionbaselineSCLIndelibility with different selection techniquesselect shorter sentno selectionFigure2:Self-trainingvariantscomparedtosupervisedbaselineandSCL.Theeffectofvariousselectiontechniques(Sec.3.2)inasingleiterationisdepicted(non-solidlines;fewerparsesandnoselectionachieveidenticalresults).Forclarity,theﬁgureshowsthelearningcurveforthebestselectiontechniqueonly(shortersent)versusnoselection.Onaveragerunningmultipleiterationsisjustthesameasasingleiteration.InallcasesSCLstillperformsbest.42

ReferencesStevenAbney.2007.Semi-supervisedLearningforComputationalLinguistics.Chapman&Hall.RieKubotaAndoandTongZhang.2005.Aframeworkforlearningpredictivestructuresfrommultipletasksandunlabeleddata.JournalofMachineLearningRe-search,6:1817–1853.JohnBlitzer,RyanMcDonald,andFernandoPereira.2006.Domainadaptationwithstructuralcorrespon-dencelearning.InConferenceonEmpiricalMethodsinNaturalLanguageProcessing,Sydney,Australia.JohnBlitzer,MarkDredze,andFernandoPereira.2007.Biographies,bollywood,boom-boxesandblenders:Domainadaptationforsentimentclassiﬁcation.InAssociationforComputationalLinguistics,Prague,CzechRepublic.MarkDredze,JohnBlitzer,PrathaPratimTalukdar,Kuz-manGanchev,JoaoGraca,andFernandoPereira.2007.Frustratinglyharddomainadaptationforpars-ing.InProceedingsoftheCoNLLSharedTaskSession-ConferenceonNaturalLanguageLearning,Prague,CzechRepublic.TadayoshiHara,MiyaoYusuke,andJun’ichiTsujii.2005.Adaptingaprobabilisticdisambiguationmodelofanhpsgparsertoanewdomain.InProceedingsoftheInternationalJointConferenceonNaturalLan-guageProcessing.DavidMcCloskyandEugeneCharniak.2008.Self-trainingforbiomedicalparsing.InProceedingsofACL-08:HLT,ShortPapers,pages101–104,Colum-bus,Ohio,June.AssociationforComputationalLin-guistics.DavidMcClosky,EugeneCharniak,andMarkJohnson.2006.Effectiveself-trainingforparsing.InProceed-ingsoftheHumanLanguageTechnologyConferenceoftheNAACL,MainConference,pages152–159,NewYorkCity.AssociationforComputationalLinguistics.MilesOsborne.2000.Estimationofstochasticattribute-valuegrammarsusinganinformativesample.InPro-ceedingsoftheEighteenthInternationalConferenceonComputationalLinguistics(COLING2000).BarbaraPlank.2009.Structuralcorrespondencelearn-ingforparsedisambiguation.InProceedingsoftheStudentResearchWorkshopatEACL2009,Athens,Greece,April.RoiReichartandAriRappoport.2007.Self-trainingforenhancementanddomainadaptationofstatisticalparserstrainedonsmalldatasets.InProceedingsofAssociationforComputationalLinguistics,Prague.NobuyukiShimizuandHiroshiNakagawa.2007.Struc-turalcorrespondencelearningfordependencyparsing.InProceedingsoftheCoNLLSharedTaskSessionofEMNLP-CoNLL2007.MarkSteedman,MilesOsborne,AnoopSarkar,StephenClark,RebeccaHwa,JuliaHockenmaier,PaulRuhlen,StevenBaker,andJeremiahCrim.2003.Bootstrap-pingstatisticalparsersfromsmalldatasets.InInPro-ceedingsoftheEACL,pages331–338.GertjanvanNoordandRobertMalouf.2005.Widecoverageparsingwithstochasticattributevaluegram-mars.Draft.ApreliminaryversionofthispaperwaspublishedintheProceedingsoftheIJCNLPworkshopBeyondShallowAnalyses,HainanChina,2004.Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 43–48,

Boulder, Colorado, June 2009. c(cid:13)2009 Association for Computational Linguistics

43

LatentDirichletAllocationwithTopic-in-SetKnowledge∗DavidAndrzejewskiComputerSciencesDepartmentUniversityofWisconsin-MadisonMadison,WI53706,USAandrzeje@cs.wisc.eduXiaojinZhuComputerSciencesDepartmentUniversityofWisconsin-MadisonMadison,WI53706,USAjerryzhu@cs.wisc.eduAbstractLatentDirichletAllocationisanunsupervisedgraphicalmodelwhichcandiscoverlatenttop-icsinunlabeleddata.Weproposeamech-anismforaddingpartialsupervision,calledtopic-in-setknowledge,tolatenttopicmod-eling.Thistypeofsupervisioncanbeusedtoencouragetherecoveryoftopicswhicharemorerelevanttousermodelinggoalsthanthetopicswhichwouldberecoveredotherwise.Preliminaryexperimentsontextdatasetsarepresentedtodemonstratethepotentialeffec-tivenessofthismethod.1IntroductionLatenttopicmodelssuchasLatentDirichletAlloca-tion(LDA)(Bleietal.,2003)haveemergedasause-fulfamilyofgraphicalmodelswithmanyinterestingapplicationsinnaturallanguageprocessing.OneofthekeyvirtuesofLDAisitsstatusasafullygenera-tiveprobabilisticmodel,allowingprincipledexten-sionsandvariationscapableofexpressingrichprob-lemdomainstructure(Newmanetal.,2007;Rosen-Zvietal.,2004;Boyd-Graberetal.,2007;Grifﬁthsetal.,2005).LDAisanunsupervisedlearningmodel.ThisworkaimstoaddsupervisedinformationintheformoflatenttopicassignmentstoLDA.Traditionally,topicassignmentshavebeendenotedbythevariablezinLDA,andwewillcallsuchsupervisedinforma-tion“z-labels.”Inparticular,az-labelistheknowl-∗WewouldliketoacknowledgetheassistanceofBrandiGancarzwiththebiologicalannotations.ThisworkissupportedinpartbytheWisconsinAlumniResearchFoundation.edgethatthetopicassignmentforagivenwordpo-sitioniswithinasubsetoftopics.Assuch,thisworkisacombinationofunsupervisedmodelandsuper-visedknowledge,andfallsintothecategorysimi-lartoconstrainedclustering(Basuetal.,2008)andsemi-superviseddimensionalityreduction(Yangetal.,2006).1.1RelatedWorkAsimilarbutsimplertypeoftopiclabelinginfor-mationhasbeenappliedtocomputervisiontasks.Topicmodelingapproacheshavebeenappliedtoscenemodeling(Sudderthetal.,2005),segmen-tation,andclassiﬁcationordetection(WangandGrimson,2008).Insomeofthesevisionapplica-tions,thelatenttopicsthemselvesareassumedtocorrespondtoobjectlabels.Iflabeleddataisavail-able,eitherall(WangandMori,2009)orsome(CaoandFei-Fei,2007)ofthezvaluescanbetreatedasobserved,ratherthanlatent,variables.Ourmodelextendsz-labelsfromsinglevaluestosubsets,thusofferadditionalmodelexpressiveness.Ifthetopic-basedrepresentationsofdocumentsaretobeusedfordocumentclusteringorclassi-ﬁcation,providingz-labelsforwordscanbeseenassimilartosemi-supervisedlearningwithlabeledfeatures(Drucketal.,2008).Herethewordsarefeatures,andz-labelguidanceactsasafeaturela-bel.ThisdiffersfromothersupervisedLDAvari-ants(BleiandMcAuliffe,2008;Lacoste-Julienetal.,2008)whichusedocumentlabelinformation.The∆LDAmodelforstatisticalsoftwaredebug-ging(Andrzejewskietal.,2007)partitionsthetopicsinto2sets:“usage”topicswhichcanappearinall44

documents,and“bug”topicswhichcanonlyappearinaspecialsubsetofdocuments.Thiseffectwasachievedbyusingdifferentαhyperparametersforthe2subsetsofdocuments.z-labelscanachievethesameeffectbyrestrictingthez’sindocumentsout-sidethespecialsubset,sothatthez’scannotassumethe“bug”topicvalues.Therefore,thepresentap-proachcanbeviewedasageneralizationof∆LDA.Anotherperspectiveisthatourz-labelsmayguidethetopicmodeltowardsthediscoveryofsec-ondaryornon-dominantstatisticalpatternsinthedata(ChechikandTishby,2002).Thesetopicsmaybemoreinterestingorrelevanttothegoalsoftheuser,butstandardLDAwouldignoretheminfavorofmoreprominent(andperhapsorthogonal)struc-ture.2OurModel2.1ReviewofLatentDirichletAllocationWebrieﬂyreviewLDA,followingthenotationof(GrifﬁthsandSteyvers,2004)1.LettherebeTtopics.Letw=w1...wnrepresentacor-pusofDdocuments,withatotalofnwords.Weuseditodenotethedocumentofwordwi,andzithehiddentopicfromwhichwiisgenerated.Letφ(w)j=p(w|z=j),andθ(d)j=p(z=j)fordocumentd.LDAinvolvesthefollowinggenerativemodel:θ∼Dirichlet(α)(1)zi|θ(di)∼Multinomial(θ(di))(2)φ∼Dirichlet(β)(3)wi|zi,φ∼Multinomial(φzi),(4)whereαandβarehyperparametersforthedocument-topicandtopic-wordDirichletdistribu-tions,respectively.Eventhoughtheycanbevectorvalued,forsimplicityweassumeαandβarescalars,resultinginsymmetricDirichletpriors.Givenourobservedwordsw,thekeytaskisin-ferenceofthehiddentopicsz.Unfortunately,thisposteriorisintractableandweresorttoaMarkovChainMonteCarlo(MCMC)samplingscheme,speciﬁcallyCollapsedGibbsSampling(GrifﬁthsandSteyvers,2004).Thefullconditionalequation1Weenclosesuperscriptsinparenthesesinthispaper.usedforsamplingindividualzivaluesfromthepos-teriorisgivenbyP(zi=v|z−i,w,α,β)∝ n(d)−i,v+αPTu(n(d)−i,u+α)! n(wi)−i,v+βPWw0(β+n(w0)−i,v)!(5)wheren(d)−i,visthenumberoftimestopicvisusedindocumentd,andn(wi)−i,visthenumberoftimeswordwiisgeneratedbytopicv.The−inotationsigniﬁesthatthecountsaretakenomittingthevalueofzi.2.2Topic-in-SetKnowledge:z-labelsLetqiv= n(d)−i,v+αPTu(n(d)−i,u+α)! n(wi)−i,v+βPWw0(β+n(w0)−i,v)!.Wenowdeﬁneourz-labels.LetC(i)bethesetofpossiblez-labelsforlatenttopiczi.WesetahardconstraintbymodifyingtheGibbssamplingequa-tionwithanindicatorfunctionδ(v∈C(i)),whichtakesonvalue1ifv∈C(i)andis0otherwise:P(zi=v|z−i,w,α,β)∝qivδ(v∈C(i))(6)Ifwewishtorestrictzitoasinglevalue(e.g.,zi=5),thiscannowbeaccomplishedbysettingC(i)={5}.Likewise,wecanrestrictzitoasubsetofval-ues{1,2,3}bysettingC(i)={1,2,3}.Finally,forunconstrainedziwesimplysetC(i)={1,2,...,T},inwhichcaseourmodiﬁedsampling(6)reducestothestandardGibbssampling(5).Thisformulationgivesusaﬂexiblemethodforin-sertingpriordomainknowledgeintotheinferenceoflatenttopics.WecansetC(i)independentlyforev-erysinglewordwiinthecorpus.Thisallowsus,forexample,toforcetwooccurrencesofthesameword(e.g.,“Applepie”and“AppleiPod”)tobeexplainedbydifferenttopics.Thiseffectwouldbeimpossibletoachievebyusingtopic-speciﬁcasymmetricβvec-torsandsettingsomeentriestozero.Thishardconstraintmodelcanberelaxed.Let0≤η≤1bethestrengthofourconstraint,whereη=1recoversthehardconstraint(6)andη=0recoversunconstrainedsampling(5):P(zi=v|z−i,w,α,β)∝qiv(cid:16)ηδ(v∈C(i))+1−η(cid:17).45

Whilewepresentthez-labelconstraintsasame-chanicalmodiﬁcationtotheGibbssamplingequa-tions,itcanbederivedfromanundirectedextensionofLDA(omittedhere)whichencodesz-labels.ThesoftconstraintGibbssamplingequationarisesnat-urallyfromthisformulation,whichisthebasisfortheFirst-OrderLogicconstraintsdescribedlaterinthefutureworksection.3ExperimentsWenowpresentpreliminaryexperimentalresultstodemonstratesomeinterestingapplicationsfortopic-in-setknowledge.Unlessotherwisespeciﬁed,sym-metrichyperparametersα=.5andβ=.1wereusedandallMCMCchainswererunfor2000sam-plesbeforeestimatingφandθfromtheﬁnalsample,asin(GrifﬁthsandSteyvers,2004).3.1ConceptExpansionWeexploretheuseoftopic-in-setforidentifyingwordsrelatedtoatargetconcept,givenasetofseedwordsassociatedwiththatconcept.Forex-ample,abiologicalexpertmaybeinterestedintheconcept“translation”.Theexpertwouldthenpro-videasetofseedwordswhicharestronglyrelatedtothisconcept,hereweassumetheseedwordset{translation,trna,anticodon,ribosome}.Weaddthehardconstraintthatzi=0foralloccurrencesofthesefourwordsinourcorpusofapproximately9,000yeast-relatedabstracts.WeranLDAwiththenumberoftopicsT=100,bothwithandwithoutthez-labelknowledgeontheseedwords.Table1showsthemostprobablewordsinselectedtopicsfrombothruns.Table1ashowsTopic0fromtheconstrainedrun,whileTable1bshowsthetopicswhichcontainedseedwordsamongthetop50mostprobablewordsfromtheuncon-strainedrun.Inordertobetterunderstandtheresults,thesetopwordswereannotatedforrelevancetothetar-getconcept(translation)byanoutsidebiologicalex-pert.ThewordsinTable1werethencoloredblueiftheywereoneoftheoriginalseedwords,rediftheywerejudgedasrelevant,andleftblackother-wise.Fromaquickglance,wecanseethatTopic0fromtheconstrainedruncontainsmorerelevanttermsthanTopic43fromthestandardLDArun.Topic31hasasimilarnumberofrelevantterms,buttakentogetherwecanseethattheemphasisofTopic31isslightlyoff-target,morefocusedon“mRNAturnover”than“translation”.Likewise,Topic73seemsmorefocusedontheribosomeitselfthantheprocessoftranslation.Overall,theseresultsdemon-stratethepotentialeffectivenessofz-labelinforma-tionforguidingtopicmodelstowardsauser-seededconcept.3.2ConceptExplorationSupposethatauserhaschosenasetoftermsandwishestodiscoverdifferenttopicsrelatedtotheseterms.Byconstrainingthesetermstoonlyappearinarestrictedsetoftopics,thesetermswillbecon-centratedinthesetoftopics.ThesplitwithinthosesetoftopicsmaybedifferentfromwhatastandardLDAwillproduce,thusrevealingnewinformationwithinthedata.Tomakethisconcrete,sayweareinterestedinthelocation“UnitedKingdom”.Weseedthiscon-ceptwiththefollowingLOCATION-taggedterms{britain,british,england,uk,u.k.,wales,scotland,london}.Thesetermsarethenrestrictedtoap-pearonlyintheﬁrst3topics.Ourcorpusisanentity-taggedReutersnewswirecorpususedfortheCoNLL-2003sharedtask(TjongKimSangandDeMeulder,2003).Inordertofocusonourtar-getlocation,wealsorestrictallotherLOCATION-taggedtokenstonotappearintheﬁrst3topics.ForthisexperimentwesetT=12,arrivedatbytrial-and-errorinthebaseline(standardLDA)case.The50mostprobablewordsforeachtopicareshowninFigure2,andtaggedentitiesarepreﬁxedwiththeirtagsforeasyidentiﬁcation.Table2ashowsthetopwordsfortheﬁrst3topicsofourz-labelrun.ThesethreetopicsareallrelatedtothetargetLOCATIONUnitedKingdom,buttheyalsosplitnicelyintobusiness,cricket,andsoccer.Wordswhicharehighlyrelevanttoeachofthese3conceptsarecoloredblue,red,andgreen,respectively.Incontrast,inTable2bweshowtopicsfromstan-dardLDAwhichcontainanyofthe“UnitedKing-dom”LOCATIONterms(whichareunderlined)amongthe50mostprobablewordsforthattopic.Wemakeseveralobservationsaboutthesetopics.First,standardLDATopic0ismostlyconcernedwithpoliticalunrestinRussia,whichisnotparticu-46

Topic0translation,ribosomal,trna,rrna,initiation,ribosome,protein,ribosomes,is,factor,processing,translationalnucleolar,pre-rrna,synthesis,small,60s,eukaryotic,biogenesis,subunit,trnas,subunits,large,nucleolusfactors,40,synthetase,free,modiﬁcation,rna,depletion,eif-2,initiator,40s,ef-3,anticodon,maturation18s,eif2,mature,eif4e,associated,synthetases,aminoacylation,snornas,assembly,eif4g,elongation(a)Topic0withz-labelTopic31mrna,translation,initiation,mrnas,rna,transcripts,3,transcript,polya,factor,5,translational,decay,codondecapping,factors,degradation,end,termination,eukaryotic,polyadenylation,cap,required,efﬁciencysynthesis,show,codons,abundance,rnas,aug,nmd,messenger,turnover,rna-binding,processing,eif2,eif4eeif4g,cf,occurs,pab1p,cleavage,eif5,cerevisiae,major,primary,rapid,tail,efﬁcient,upf1p,eif-2Topic43type,is,wild,yeast,trna,synthetase,both,methionine,synthetases,class,trnas,enzyme,whereas,cytoplasmicbecause,direct,efﬁciency,presence,modiﬁcation,aminoacylation,anticodon,either,eukaryotic,betweendifferent,speciﬁc,discussed,results,similar,some,met,compared,aminoacyl-trna,able,initiator,samnot,free,however,recognition,several,arc1p,fully,same,forms,leads,identical,responsible,found,only,wellTopic73ribosomal,rrna,protein,is,processing,ribosome,ribosomes,rna,nucleolar,pre-rrna,rnase,small,biogenesisdepletion,subunits,60s,subunit,large,synthesis,maturation,nucleolus,associated,essential,assemblycomponents,translation,involved,rnas,found,component,mature,rp,40s,accumulation,18s,40,particlessnornas,factors,precursor,during,primary,rrnas,35s,has,21s,speciﬁcally,results,ribonucleoprotein,early(b)StandardLDATopicsFigure1:Conceptseedwordsarecoloredblue,otherwordsjudgedrelevanttothetargetconceptarecoloredred.larlyrelatedtothetargetlocation.Second,Topic2issimilartoourpreviousbusinesstopic,butwithamoreUS-orientedslant.Notethat“dollar”ap-pearswithhighprobabilityinstandardLDATopic2,butnotinourz-labelLDATopic0.StandardLDATopic8appearstobeamixofbothsoccerandcricketwords.Therefore,itseemsthatourtopic-in-setknowledgehelpsindistillingtopicsrelatedtotheseedwords.Giventhispromisingresult,weattemptedtorepeatthisexperimentwithsomeothernations(UnitedStates,Germany,China),butwithoutmuchsuccess.WhenwetriedtorestricttheseLOCATIONwordstotheﬁrstfewtopics,thesetopicstendedtobeusedtoexplainotherconceptsunrelatedtothetargetlocation(oftenothersports).Weareinvesti-gatingthepossiblecausesofthisproblem.4ConclusionsandFutureWorkWehavedeﬁnedTopic-in-SetknowledgeanddemonstrateditsusewithinLDA.Asshownintheexperiments,thepartialsupervisionprovidedbyz-labelscanencourageLDAtorecovertopicsrele-vanttouserinterests.Thisapproachcombinesthepattern-discoverypowerofLDAwithuser-providedguidance,whichwebelievewillbeveryattractivetopracticalusersoftopicmodeling.Futureworkwilldealwithatleasttwoimpor-tantissues.First,whenwillthisformofpartialsupervisionbemosteffectiveorappropriate?Ourexperimentalresultssuggestthatthisapproachwillstruggleiftheuser’stargetconceptsaresimplynotprevalentinthetext.Second,canwemodifythisapproachtoexpressricherformsofpartialsuper-vision?Moresophisticatedformsofknowledgemayallowuserstospecifytheirpreferencesorpriorknowledgemoreeffectively.Towardsthisend,weareinvestigatingtheuseofFirst-OrderLogicinspecifyingpriorknowledge.Notethatthesetz-labelspresentedherecanbeexpressedassimplelog-icalformulas.Extendingourmodeltogenerallog-icalformulaswouldallowtheexpressionofmorepowerfulrelationalpreferences.ReferencesDavidAndrzejewski,AnneMulhern,BenLiblit,andXi-aojinZhu.2007.Statisticaldebuggingusinglatenttopicmodels.InStanMatwinandDunjaMladenic,editors,18thEuropeanConferenceonMachineLearn-ing,Warsaw,Poland.47

Topic0million,company,’s,year,shares,net,proﬁt,half,group,[I-ORG]corp,market,sales,share,percentexpected,business,loss,stock,results,forecast,companies,deal,earnings,statement,price,[I-LOC]londonbillion,[I-ORG]newsroom,industry,newsroom,pay,pct,analysts,issue,services,analyst,proﬁts,saleadded,ﬁrm,[I-ORG]london,chief,quarter,investors,contract,note,tax,ﬁnancial,months,costsTopic1[I-LOC]england,[I-LOC]london,[I-LOC]britain,cricket,[I-PER]m.,overs,test,wickets,scores,[I-PER]ahmed[I-PER]paul,[I-PER]wasim,innings,[I-PER]a.,[I-PER]akram,[I-PER]mushtaq,day,one-day,[I-PER]mark,ﬁnal[I-LOC]scotland,[I-PER]waqar,[I-MISC]series,[I-PER]croft,[I-PER]david,[I-PER]younis,match,[I-PER]iantotal,[I-MISC]english,[I-PER]khan,[I-PER]mullally,bat,declared,fall,[I-PER]d.,[I-PER]g.,[I-PER]j.bowling,[I-PER]r.,[I-PER]robert,[I-PER]s.,[I-PER]steve,[I-PER]c.captain,golf,tour,[I-PER]sohail,extras[I-ORG]surreyTopic2soccer,division,results,played,standings,league,matches,halftime,goals,attendance,points,won,[I-ORG]stdrawn,saturday,[I-MISC]english,lost,premier,[I-MISC]french,result,scorers,[I-MISC]dutch,[I-ORG]united[I-MISC]scottish,sunday,match,[I-LOC]london,[I-ORG]psv,tabulate,[I-ORG]hapoel,[I-ORG]sydney,fridaysummary,[I-ORG]ajax,[I-ORG]manchester,tabulated,[I-MISC]german,[I-ORG]munich,[I-ORG]city[I-MISC]european,[I-ORG]rangers,summaries,weekend,[I-ORG]fc,[I-ORG]shefﬁeld,wednesday,[I-ORG]borussia[I-ORG]fortuna,[I-ORG]paris,tuesday(a)Topicswithsetz-labelsTopic0police,’s,people,killed,[I-MISC]russian,friday,spokesman,[I-LOC]moscow,told,rebels,group,ofﬁcials[I-PER]yeltsin,arrested,found,miles,km,[I-PER]lebed,capital,thursday,tuesday,[I-LOC]chechnya,newssaturday,town,authorities,airport,man,government,state,agency,plane,reported,security,forcescity,monday,air,quoted,students,region,area,local,[I-LOC]russia,[I-ORG]reuters,military,[I-LOC]londonheld,southern,diedTopic2percent,’s,market,thursday,july,tonnes,week,year,lower,[I-LOC]u.s.,rate,prices,billion,cents,dollarfriday,trade,bank,closed,trading,higher,close,oil,bond,fell,markets,index,points,rosedemand,june,rates,september,traders,[I-ORG]newsroom,day,bonds,million,price,shares,budget,governmentgrowth,interest,monday,[I-LOC]london,economic,august,expected,riseTopic5’s,match,team,win,play,season,[I-MISC]french,lead,home,year,players,[I-MISC]cup,back,minuteschampion,victory,time,n’t,game,saturday,title,side,set,made,wednesday,[I-LOC]englandleague,run,club,top,good,ﬁnal,scored,coach,shot,world,left,[I-MISC]american,captain[I-MISC]world,goal,start,won,champions,round,winner,end,years,defeat,lostTopic8division,[I-LOC]england,soccer,results,[I-LOC]london,[I-LOC]pakistan,[I-MISC]english,matches,playedstandings,league,points,[I-ORG]st,cricket,saturday,[I-PER]ahmed,won,[I-ORG]united,goals[I-PER]wasim,[I-PER]akram,[I-PER]m.,[I-MISC]scottish,[I-PER]mushtaq,drawn,innings,premier,lost[I-PER]waqar,test,[I-PER]croft,[I-PER]a.,[I-PER]younis,declared,wickets,[I-ORG]hapoel,[I-PER]mullally[I-ORG]sydney,day,[I-ORG]manchester,[I-PER]khan,ﬁnal,scores,[I-PER]d.,[I-MISC]german,[I-ORG]munich[I-PER]sohail,friday,total,[I-LOC]ovalTopic10[I-LOC]germany,’s,[I-LOC]italy,[I-LOC]u.s.,metres,seconds,[I-LOC]france,[I-LOC]britain,[I-LOC]russiaworld,race,leading,[I-LOC]sweden,[I-LOC]australia,[I-LOC]spain,women,[I-MISC]world,[I-LOC]belgium[I-LOC]netherlands,[I-PER]paul,[I-LOC]japan,[I-MISC]olympic,[I-LOC]austria,[I-LOC]kenya,men,timeresults,[I-LOC]brussels,[I-MISC]cup,[I-LOC]canada,ﬁnal,minutes,record,[I-PER]michael,meeting,round[I-LOC]norway,friday,scores,[I-PER]mark,[I-PER]van,[I-LOC]ireland,[I-PER]peter,[I-MISC]grand[I-MISC]prix,points,saturday,[I-LOC]ﬁnland,cycling,[I-ORG]honda(b)StandardLDATopicsFigure2:Topicscontaining“UnitedKingdom”locationwords.Wordsrelatedtobusinessarecoloredblue,cricketred,andsoccergreen.SugatoBasu,IanDavidson,andKiriWagstaff,edi-tors.2008.ConstrainedClustering:AdvancesinAlgorithms,Theory,andApplications.Chapman&Hall/CRCPress.DavidBleiandJonMcAuliffe.2008.Supervisedtopicmodels.InJ.C.Platt,D.Koller,Y.Singer,andS.Roweis,editors,AdvancesinNeuralInformationProcessingSystems20,pages121–128.MITPress,48

Cambridge,MA.DavidM.Blei,AndrewY.Ng,andMichaelI.Jordan.2003.LatentDirichletallocation.JournalofMachineLearningResearch,3:993–1022.JordanBoyd-Graber,DavidBlei,andXiaojinZhu.2007.Atopicmodelforwordsensedisambiguation.InProceedingsofthe2007JointConferenceonEmpir-icalMethodsinNaturalLanguageProcessingandComputationalNaturalLanguageLearning(EMNLP-CoNLL),pages1024–1033.LiangliangCaoandLiFei-Fei.2007.Spatiallycoher-entlatenttopicmodelforconcurrentsegmentationandclassiﬁcationofobjectsandscenes.InICCV,pages1–8.GalChechikandNaftaliTishby.2002.Extractingrel-evantstructureswithsideinformation.InNIPS15,pages857–864.MITpress.GregoryDruck,GideonMann,andAndrewMcCallum.2008.Learningfromlabeledfeaturesusinggeneral-izedexpectationcriteria.InSIGIR2008,pages595–602,NewYork,NY,USA.ACM.ThomasGrifﬁthsandMarkSteyvers.2004.Findingsci-entiﬁctopics.ProceedingsoftheNationalAcademyofSciences,101(suppl.1):5228–5235.ThomasL.Grifﬁths,MarkSteyvers,DavidM.Blei,andJoshuaB.Tenenbaum.2005.Integratingtopicsandsyntax.InNIPS17.S.Lacoste-Julien,F.Sha,andM.Jordan.2008.Disclda:Discriminativelearningfordimensionalityreductionandclassiﬁcation.InAdvancesinNeuralInformationProcessingSystems21(NIPS08).DavidNewman,KatHagedorn,ChaitanyaChemudugunta,andPadhraicSmyth.2007.Subjectmetadataenrichmentusingstatisticaltopicmodels.InJCDL’07:Proceedingsofthe7thACM/IEEE-CSjointconferenceonDigitallibraries,pages366–375,NewYork,NY,USA.ACM.MichalRosen-Zvi,ThomasGrifﬁths,MarkSteyvers,andPadhraicSmyth.2004.Theauthor-topicmodelforau-thorsanddocuments.InProceedingsofthe20thcon-ferenceonUncertaintyinartiﬁcialintelligence(UAI),pages487–494,Arlington,Virginia,UnitedStates.AUAIPress.ErikB.Sudderth,AntonioB.Torralba,WilliamT.Free-man,andAlanS.Willsky.2005.Learninghierar-chicalmodelsofscenes,objects,andparts.InICCV,pages1331–1338.ErikTjongKimSangandFienDeMeulder.2003.In-troductiontotheconll-2003sharedtask:Language-independentnamedentityrecognition.InProceedingsofCoNLL-2003,pages142–147,Edmonton,Canada.XiaogangWangandEricGrimson.2008.Spatiallatentdirichletallocation.InJ.C.Platt,D.Koller,Y.Singer,andS.Roweis,editors,NIPS20,pages1577–1584.MITPress,Cambridge,MA.YangWangandGregMori.2009.Humanactionrecog-nitionbysemi-latenttopicmodels.InIEEETransac-tionsonPatternAnalysisandMachineIntelligence.XinYang,HaoyingFu,HongyuanZha,andJesseBarlow.2006.Semi-supervisednonlineardimensionalityre-duction.InICML-06,23ndInternationalConferenceonMachineLearning.Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 49–57,

Boulder, Colorado, June 2009. c(cid:13)2009 Association for Computational Linguistics

49

AnAnalysisofBootstrappingfortheRecognitionofTemporalExpressionsJordiPovedaTALPResearchCenterTechnicalUniversityofCatalonia(UPC)Barcelona,Spainjpoveda@lsi.upc.eduMihaiSurdeanuNLPGroupStanfordUniversityStanford,CAmihais@stanford.eduJordiTurmoTALPResearchCenterTechnicalUniversityofCatalonia(UPC)Barcelona,Spainturmo@lsi.upc.eduAbstractWepresentasemi-supervised(bootstrapping)approachtotheextractionoftimeexpressionmentionsinlargeunlabelledcorpora.Becausetheonlysupervisionisintheformofseedexamples,itbecomesnecessarytoresorttoheuristicstorankandﬁlteroutspuriouspat-ternsandcandidatetimeexpressions.Theapplicationofbootstrappingtotimeexpres-sionrecognitionis,tothebestofourknowl-edge,novel.Inthispaper,wedescribeonesucharchitectureforbootstrappingInforma-tionExtraction(IE)patterns—suitedtotheextractionofentities,asopposedtoeventsorrelations—andsummarizeourexperimentalﬁndings.Thesepointouttothefactthatapatternsetwithagoodincreaseinrecallwithrespecttotheseedsisachievablewithinourframeworkwhile,ontheotherside,thede-creaseinprecisioninsuccessiveiterationsissuccesfullycontrolledthroughtheuseofrank-ingandselectionheuristics.Experimentsarestillunderwaytoachievethebestuseoftheseheuristicsandotherparametersoftheboot-strappingalgorithm.1IntroductionTheproblemoftimeexpressionrecognitionreferstotheidentiﬁcationinfree-formatnaturallanguagetextoftheoccurrencesofexpressionsthatdenotetime.Time-denotingexpressionsappearinagreatdiversityofforms,beyondthemostobviousab-solutetimeordatereferences(e.g.11pm,Febru-ary14th,2005):timereferencesthatanchoronan-othertime(threehoursaftermidnight,twoweeksbe-foreChristmas),expressionsdenotingdurations(afewmonths),expressionsdenotingrecurringtimes(everythirdmonth,twiceinthehour),context-dependenttimes(today,lastyear),vaguereferences(somewhereinthemiddleofJune,thenearfuture)ortimesthatareindicatedbyanevent(thedayG.Bushwasreelected).ThisproblemisasubpartofataskcalledTERN(TemporalExpressionRecog-nitionandNormalization),wheretemporalexpres-sionsareﬁrstidentiﬁedintextandthenitsintendedtemporalmeaningisrepresentedinacanonicalfor-mat.TERNwasﬁrstproposedasanindependenttaskinthe2004editionoftheACEconferences1.ThemostwidelyusedstandardfortheannotationoftemporalexpressionsisTIMEX(Ferroetal.,2005).Themostcommonapproachtotemporalexpres-sionrecognitioninthepasthasbeentheuseofhand-madegrammarstocapturetheexpressions(see(Wiebeetal.,1998;FilatovaandHovy,2001;Sa-queteetal.,2004)forexamples),whichcanthenbeeasilyexpandedwithadditionalattributesforthenormalizationtask,basedoncomputingdistanceanddirection(pastorfuture)withrespecttoaref-erencetime.ThisapproachachievesanF1-measureofapproximately85%forrecognitionandnormal-ization.Theuseofmachinelearningtechniques—mainlystatistical—forthistaskisamorerecentdevelopment,eitheralongsidethetraditionalhand-grammarapproachtolearntodistinguishspeciﬁcdifﬁcultcases(ManiandWilson,2000),oronitsown(Haciogluetal.,2005).ThelatterapplySVMstotherecognitiontaskalone,usingtheoutputofsev-eralhuman-madetaggersasadditionalfeaturesfortheclassiﬁer,andreportanF1-measureof87.8%.1http://www.nist.gov/speech/tests/ace/50

BootstrappingtechniqueshavebeenusedforsuchdiverseNLPproblemsas:wordsensedisambigua-tion(Yarowsky,1995),namedentityclassiﬁcation(CollinsandSinger,1999),IEpatternacquisition(Riloff,1996;Yangarberetal.,2000;Yangarber,2003;StevensonandGreenwood,2005),documentclassiﬁcation(Surdeanuetal.,2006),factextractionfromtheweb(Pas¸caetal.,2006)andhyponymyre-lationextraction(Kozarevaetal.,2008).(Yarowsky,1995)usedbootstrappingtotrainde-cisionlistclassiﬁerstodisambiguatebetweentwosensesofaword,achievingimpressiveclassiﬁcationaccuracy.(CollinsandSinger,1999)appliedboot-strappingtoextractrulesfornamedentity(NE)clas-siﬁcation,seedingthesytemwithafewhandcraftedrules.Theirmaininnovationwastosplittrainingintwoalternatestages:duringonestep,onlycon-textualrulesaresought;duringthesecondstep,thenewcontextualrulesareusedtotagfurtherNEsandtheseareusedtoproducenewspellingrules.Bootstrappingapproachesareemployedin(Riloff,1996),(Yangarberetal.,2000),(Yangarber,2003),and(StevensonandGreenwood,2005)inordertoﬁndIEpatternsfordomain-speciﬁceventextraction.(Pas¸caetal.,2006)employabootstrappingprocesstoextractgeneralfactsfromtheWeb,viewedastwo-termrelationships(e.g[DonaldKnuth,1938]couldbeaninstanceofa“borninyear”relationship).(Surdeanuetal.,2006)usedbootstrappingco-trainedwithanEMclassiﬁerinordertoperformtopicclassiﬁcationofdocumentsbasedonthepresenceofcertainlearnedsyntactic-semanticpatterns.In(Kozarevaetal.,2008),bootstrappingisappliedtoﬁndingnewmembersofcertainclassofobjects(i.e.an“is-a”relationship),byprovidingamemberoftherequiredclassasseedandusinga“suchas”typeoftextualpatterntolocatenewinstances.Therecognitionoftemporalexpressionsiscru-cialformanyapplicationsinNLP,amongthem:IE,QuestionAnswering(QA)andAutomaticSumma-rization(forthetemporalorderingofevents).Workonslightlysupervisedapproachessuchasbootstrap-pingisjustiﬁedbythelargeavailabilityofunla-belledcorpora,asopposedtotaggedones,fromwhichtolearnmodelsforrecognition.2ArchitectureFigure1illustratesthebuildingblocksofthealgo-rithmandtheirinteractions,alongwithinputandoutputdata.Theinputstothebootstrappingalgorithmaretheunlabelledtrainingcorpusandaﬁleofseedex-amples.Theunlabelledcorpusisalargecollec-tionofdocumentswhichhasbeentokenized,POStagged,lemmatized,andsyntacticallyanalyzedforbasicsyntacticconstituents(shallowparsing)andheadwords.Thesecondinputisasetofseedexam-ples,consistingofaseriesoftokensequenceswhichweassumetobecorrecttimeexpressions.Theseedsaresuppliedwithoutadditionalfeatures,andwithoutcontextinformation.Ourbootstrappingalgorithmworkswithtwoal-ternativeviewsofthesametargetdata(timeexpres-sions),thatis:patternsandexamples(i.e.anin-stanceofapatterninthecorpus).Apatternisagen-eralizedrepresentationthatcanmatchanysequenceoftokensmeetingtheconditionsexpressedinthepattern(thesecanbemorphological,semantic,syn-tacticandcontextual).Anexampleisanactualcan-didateoccurrenceofatimeexpression.Patternsaregeneratedfromexamplesfoundinthecorpusand,initsturn,newexamplesarefoundbysearchingformatchesofnewpatterns.Bothpatternsandex-amplesmaycarrycontextualinformation,thatis,awindowoftokensleftandrightofthecandidatetimeexpression.Outputexamplesandoutputpatternsaretheout-putsofthebootstrappingprocess.Boththesetofoutputexamplesandthesetofoutputpatternsareincreasedwitheachnewiteration,byaddingthenewcandidateexamples(respectively,patterns)thathavebeen“accepted”duringthelastiteration(i.e.thosethathavepassedtherankingandselectionstep).Initially,asinglepassthroughthecorpusisper-formedinordertoﬁndoccurrencesoftheseedsinthetext.Thus,webootstrapaninitialsetofexam-ples.Fromthenon,thebootstrappingprocesscon-sistsofasuccessionofiterationswiththefollowingsteps:1.Rankingandselectionofexamples:Eachex-ampleproducedduringanyofthepreviousit-erations,0toi−1,isassignedascore(rank-ing).Thetopnexamplesareselectedtogrowthesetofoutputexamples(selection)andwill51

Figure1:Blockdiagramofbootstrappingalgorithmbeusedforthenextstep.ThedetailsaregiveninSection4.2.2.Generationofcandidatepatterns:Candidatepatternsforthecurrentiterationaregeneratedfromtheselectedexamplesofthepreviousstep(discussedinSection3).3.Rankingandselectionofcandidatepatterns:Eachpatternfromthecurrentiterationisas-signedascoreandthetopmpatternsarese-lectedtogrowthesetofoutputpatternsandtobeusedinthenextstep(discussedinSection4.1).Thisstepalsoinvolvesaprocessofanaly-sisofsubsumptions,performedsimultaneouslywithselection,inwhichthesetofselectedpat-ternsisexaminedandthosethataresubsumedbyotherpatternsarediscarded.4.Searchforinstancesoftheselectedpatterns:Thetrainingcorpusistraversed,inordertosearchforinstances(matches)oftheselectedpatterns,which,togetherwiththeacceptedex-amplesfromallpreviousiterations,willformthesetofcandidateexamplesforiterationi+1.Also,inordertorelaxthematchingofpat-ternstocorpustokensandoftokenformsamongthemselves,thematchingoftokenformsiscase-insensitive,andallthedigitsinatokenaregen-eralizedtoagenericdigitmarker(forinstance,“12-23-2006”isinternallyrewrittenas“@@-@@-@@@@”).Eventhoughourarchitectureisbuiltonatradi-tionalboostrappingapproach,thereareseveralele-mentsthatarenovel,atleastinthecontextoftem-poralexpressionrecognition:a)ourpatternrepre-sentationincorporatesfullsyntaxanddistributionalsemanticsinauniﬁedmodel(seeSection3);b)ourpatternranking/selectionapproachincludesasub-sumptionmodeltolimitredundancy;c)theformu-laeinourexampleranking/selectionapproacharedesignedtoworkwithvariable-lengthexpressionsthatincorporateacontext.3PatternrepresentationPatternscaptureboththesequenceoftokensthatintegrateapotentialtimeexpression(i.e.atimeexpressionmention),andinformationfromtheleftandrightcontextwhereitoccurs(uptoaboundedlength).Letuscallpreﬁxthepartofthepatternthatrepresentstheleftcontext,inﬁxthepartthatrepre-sentsapotentialtimeexpressionmentionandpostﬁxthepartthatrepresentstherightcontext.TheEBNFgrammarthatencodesourpatternrep-resentationisgiveninFigure2.Patternsarecom-posedofmultiplepatternelements(PEs).Apatternelementistheminimalunitthatismatchedagainstthetokensinthetext,andasinglepatternelementcanmatchtooneorseveraltokens,dependingonthepatternelementtype.Apatternisconsideredtomatchasequenceoftokensinthetextwhen:ﬁrst,allthePEsfromtheinﬁxarematched(thisgivesthepotentialtimeexpressionmention)and,second,allthePEsfromthepreﬁxandthepostﬁxarematched(thisgivestheleftandrightcontextinformationforthenewcandidateexample,respectively).There-fore,patternswithalargercontextwindowaremorerestrictive,becauseallofthePEsinthepreﬁxandthepostﬁxhavetobematched(ontopoftheinﬁx)forthepatterntoyieldamatch.Wedistinguishamongtoken-levelgeneralizations52

pattern::=prefixSEPinfixSEPpostfixSEP(modifiers)*prefix::=(pattern-elem)*infix::=(pattern-elem)+postfix::=(pattern-elem)*pattern-elem::=FORM"("token-form")"|SEMCLASS"("token-form")"|POS"("pos-tag")"|LEMMA"("lemma-form")"|SYN"("syn-type","head")"|SYN-SEM"("syn-type","head")"modifiers::=COMPLETE-PHRASEFigure2:TheEBNFGrammarforPatterns(i.e.PEs)andchunk-levelgeneralizations.Thefor-merhavebeengeneratedfromthefeaturesofasin-gletokenandwillmatchtoasingletokeninthetext.Thelatterhavebeengeneratedfromandmatchtoasequenceoftokensinthetext(e.g.abasicsyntacticchunk).PatternsarebuiltfromthefollowingtypesofPEs(whichcanbeseeninthegrammarfromFig-ure2):1.TokenformPEs:Themorerestrictive,onlymatchagiventokenform.2.SemanticclassPEs:Matchtokens(sometimesmultiwords)thatbelongtoagivensemanticsimilarityclass.Thisconceptisdeﬁnedbelow.3.POStagPEs:MatchtokenswithagivenPOS.4.LemmaPEs:Matchtokenswithagivenlemma.5.SyntacticchunkPEs:Matchasequenceofto-kensthatisasyntacticchunkofagiventype(e.g.NP)andwhoseheadwordhasthesamelemmaasindicated.6.GeneralizedsyntacticPEs:Sameastheprevi-ous,butthelemmaoftheheadwordmaybeanyinagivensemanticsimilarityclass.Thesemanticsimilarityclassofawordisdeﬁnedastheworditselfplusagroupofothersemanti-callysimilarwords.Forcomputingthese,weem-ployLin’scorpusofpairwisedistributionalsimilari-tiesamongwords(nouns,verbsandadjectives)(Lin,1998),ﬁlteredtoincludeonlythosewordswhosesimilarityvalueisabovebothanabsolute(highestn)andrelative(tothehighestsimilarityvalueintheclass)threshold.Evenafterﬁltering,Lin’ssimilari-tiescanbe“noisy”,sincethecorpushasbeencon-structedrelyingonpurelystatisticalmeans.There-fore,weareemployinginadditionasetofmanu-allydeﬁnedsemanticclasses(hardcodedlists)sen-sitivetoourdomainoftemporalexpressions,suchthattheselists“override”theLin’ssimilaritycor-puswheneverthesemanticclassofawordpresentinthemisinvolved.Themanuallydeﬁnedsemanticclassesinclude:thewrittenformofcardinals;ordi-nals;daysoftheweek(plustoday,tomorrowandyesterday);monthsoftheyear;datetriggerwords(e.g.day,week);timetriggerwords(e.g.hour,sec-ond);frequencyadverbs(e.g.hourly,monthly);dateadjectives(e.g.two-day,@@-week-long);andtimeadjectives(e.g.three-hour,@@-minute-long).Weuseadynamicwindowfortheamountofcon-textthatisencodedintoapattern,thatis,wegen-erateallthepossiblepatternswiththesameinﬁx,andanythingbetween0andthespeciﬁedlengthofthecontextwindowPEsinthepreﬁxandthepostﬁx,andlettheselectionstepdecidewhichvariationsgetacceptedintothenextiteration.Themodiﬁersﬁeldinthepatternrepresenta-tionhasbeendevisedasanextensionmecha-nism.Currentlytheonlyimplementedmod-iﬁerisCOMPLETE-PHRASE,whichwhenat-tachedtoapattern,“rounds”theinstance(i.e.candidatetimeexpression)capturedbyitsinﬁxtoincludetheclosestcompletebasicsyntacticchunk(e.g.“LEMMA(end)LEMMA(of)SEM-CLASS(January)”wouldmatch“theendofDe-cember2009”insteadofonly“endofDecember”againstthetext“...BytheendofDecember2009,...”).Thismodiﬁerwasimplementedinviewofthefactthatmosttemporalexpressionscorrespondwithwholenounphrasesoradverbialphrases.FromtheabovetypesofPEs,wehavebuiltthefollowingtypesofpatterns:1.All-lemmapatterns(includingthepreﬁxandpostﬁx).2.All-semanticclasspatterns.3.Combinationsoftokenformwithsem.class.4.Combinationsoflemmawithsem.class.5.All-POStagpatterns.6.CombinationsoftokenformwithPOStag.7.CombinationsoflemmawithPOStag.8.All-syntacticchunkpatterns.9.All-generalizedsyntacticpatterns.4Rankingandselectionofpatternsandlearningexamples4.1PatternsForthepurposesofthissection,letusdeﬁnethecontrolsetCasbeingformedbytheseedexamplesplusalltheselectedexamplesoverthepreviousit-erations(onlytheinﬁxconsidered,notthecontext).53

Notethat,exceptfortheseedexamples,thisisonlyassumedcorrect,butcannotbeguaranteedtobecor-rect(unsupervised).Inaddition,letusdeﬁnethein-stancesetIpofacandidatepatternpasthesetofalltheinstancesofthepatternfoundinafractionoftheunlabelledcorpus(onlyinﬁxoftheinstancecon-sidered).Eachcandidatepatternpatisassignedtwopartialscores:1.Afrequency-basedscorefreqsc(p)thatmea-suresthecoverageofthepatternin(asectionof)theunsupervisedcorpus:freqsc(p)=Card(Ip∩C)2.Aprecisionscoreprecsc(p)thatevaluatestheprecisionofthepatternin(asectionof)theun-supervisedcorpus,measuredagainstthecon-trolset:precsc(p)=Card(Ip∩C)Card(Ip)Thesetwoscoresarecomputedonlyagainstafractionoftheunlabelledcorpusfortimeefﬁ-ciency.Thereremainsanissuewithwhethermulti-sets(countingeachrepeatedinstanceseveraltimes)ornormalsets(countingthemonlyonce)shouldbeusedfortheinstancesetsIp.Ourexperimentsindi-catethatthebestresultsareobtainedbyemployingmultisetsforthefrequency-basedscoreandnormalsetsfortheprecisionscore.Giventhetwopartialscoresabove,wehavetriedthreedifferentstrategiesforcombiningthem:•Multiplicativecombination:λ1log(1+freqsc(p))+λ2log(2+precsc(p))•Thestrategysuggestedin(CollinsandSinger,1999):Patternsareﬁrstﬁlteredbyimposingathresholdontheirprecisionscore.Onlyforthosepatternsthatpassthisﬁrstﬁlter,theirﬁnalscoreisconsideredtobetheirfrequency-basedscore.•Thestrategysuggestedin(Riloff,1996):(cid:26)precsc(p)·log(freqsc(p))ifprecsc(p)≥thr0otherwise4.1.1AnalysisofsubsumptionsIntertwinedwiththeselectionstep,ananalysisofsubsumptionsisperformedamongtheselectedpat-terns,andthepatternsfoundtobesubsumedbyoth-ersinthesetarediscarded.Thisisrepeateduntilei-theramaximumofmpatternswithnosubsumptionsamongthemareselected,orthelistofcandidatepat-ternsisexhausted,whicheverhappensﬁrst.Thepur-poseofthisanalysisofsubsumptionsistwofold:ontheonehand,itresultsinacleaneroutputpatternsetbygettingridofredundantpatterns;ontheotherhand,itimprovestemporalefﬁciencybyreducingthenumberofpatternsbeinghandledinthelaststepofthealgorithm(i.e.searchingfornewcandidateexamples).Inourscenario,apatternp1withinstancesetIp1issubsumedbyapatternp2withinstancesetIp2ifIp1⊂Ip2.Wemakeadistinctionamong“theo-retical”and“empirical”subsumptions.Theoreticalsubsumptionsarethosethatcanbejustiﬁedbasedontheoreticalgroundsalone,fromobservingtheformofthepatterns.Empiricalsubsumptionsarethosecaseswhereinfactonepatternsubsumesanotherac-cordingtotheformerdeﬁnition,butthiscouldonlybedetectedbyhavingcalculatedtheirrespectivein-stancesetsapriori,whichbeatsoneofthepurposesoftheanalysisofsubsumptions—namely,tempo-ralefﬁciency—.Weareonlydealingwiththeoreti-calsubsumptionshere.Apatterntheoreticallysub-sumesanotherpatternwheneitherofthesecondi-tionsoccur:•Theﬁrstpatternisidenticaltothesecond,ex-ceptthattheﬁrsthasfewercontextualPEsinthepreﬁxand/orthepostﬁx.•PartorallofthePEsoftheﬁrstpatternareidenticaltothecorrespondingPEsinthesec-ondpattern,exceptforthefactthattheyareofamoregeneraltype(element-wise);there-mainingPEsareidentical.Tothisend,wehavedeﬁnedapartialorderofgeneralityinthePEtypes(seesection3),asfollows:FORM≺LEMMA≺SEMCLASS;FORM≺POS;SYN≺SYN-SEMC•Boththeaboveconditions(fewercontextualPEsandofamoregeneraltype)happenatthesametime.4.2LearningExamplesAnexampleiscomposedofthetokenswhichhavebeenidentiﬁedasapotentialtimeexpression(whichweshallcalltheinﬁx)plusacertainamountofleftandrightcontext(fromnowon,thecontext)en-codedalongsidetheinﬁx.Forrankingandselecting54

examples,weﬁrstassignascoreandselectanum-bernofdistinctinﬁxesand,inasecondstage,weassignascoretoeachcontextofappearanceofaninﬁxandselect(atmost)mcontextsperinﬁx.Ourscoringsystemfortheinﬁxesisadaptedfrom(Pas¸caetal.,2006).Eachdistinctinﬁxreceivesthreepar-tialscoresandtheﬁnalscorefortheinﬁxisalinearcombinationofthese,withtheλibeingparameters:λ1simsc(ex)+λ2pcsc(ex)+λ3ctxtsc(ex)1.Asimilarity-basedscore(simsc(ex)),whichmeasuresthesemanticsimilarity(aspertheLin’ssimilaritycorpus(Lin,1998))oftheinﬁxwithrespecttosetof“accepted”outputexamplesfromallpreviousiterationsplustheinitialseeds.Ifw1,...,wnarethetokensintheinﬁx(excludingstopwords);ej,1,...,ej,mjarethetokensinthej-thexampleofthesetEofseedplusoutputexamples;andsv(x,y)representsasimilarityvalue,thesimilaritySim(wi)ofthei-thwordoftheinﬁxwrttheseedsandoutputisgivenbySim(wi)=P|E|j=1max(sv(wi,ej,1),...,sv(wi,ej,mj)),andthesimilarity-basedscoreofanin-ﬁxcontainingnwordsisgivenbyPni=1log(1+Sim(wi))n.2.Aphrase-completenessscore(pcsc(ex)),whichmeasuresthelikelihoodthattheinﬁxisacompletetimeexpressionandnotmerelyapartofone,overtheentiresetofcandidateexample:count(INFIX)count(∗INFIX∗)3.Acontext-basedscore(ctxtsc(ex)),intendedasameasureoftheinﬁx’srelevance.Foreachcontext(uptoalength)wherethisinﬁxappearsinthecorpus,thefrequencyofthewordwithmaximumrelativefrequency(overthewordsinalltheinﬁx’scontexts)istaken.Thesumisthenscaledbytherelativefrequencyofthisparticularinﬁx.Apartfromthescoreassociatedwiththeinﬁx,eachexample(i.e.inﬁxplusacontext)receivestwoadditionalfrequencyscoresfortheleftandrightcontextpartoftheexamplerespectively.Eachoftheseisgivenbytherelativefrequencyofthetokenwithmaximumfrequencyofthatcontext,computedoverallthetokensthatappearinallthecontextsofallthecandidateexamples.Foreachselectedinﬁx,themcontextswithbestscoreareselected.5Experiments5.1ExperimentalsetupAsunsuperviseddataforourexperiments,weusetheNW(newswire)categoryofLDC’sACE2005UnsupervisedDataPool,containing456Mbytesofdatain204Kdocumentsforatotalofover82mil-liontokens.Simultaneously,weuseamuchsmallerlabelledcorpus(wherethecorrecttimeexpressionsaretagged)tomeasuretheprecision,recallandF1-measureofthepatternsetlearnedbythebootstrap-pingprocess.ThisistheACE2005corpus,contain-ing550documentswith257Ktokensandapprox.4650timeexpressionmentions.Thelabelledcorpusissplitintwohalves:onehalfisusedtoobtaintheinitialseedexamplesfromamongthetimeexpres-sionsfoundtherein;theotherhalfisusedforeval-uation.Wearerequiringthatapatterncapturesthetargettimeexpressionmentionexactly(nomisalign-mentallowedattheboundaries),inordertocountitasaprecisionorrecallhit.Wewillalsobeinterestedinmeasuringthegaininrecall,thatis,thedifferencebetweentherecallinthebestiterationandtheinitialrecallgivenbytheseeds.Alsoimportantisthenumberofiter-ationsafterwhichthebootstrappingprocesscon-verges.InthecasewherethesameF1-measuremarkisachievedintwoexperimentalsettings,ear-lierconvergenceofthealgorithmwillbeprefered.Otherwise,betterF1andgaininrecallarethepri-marygoals.Inordertostartwithasetofseedswithhighpre-cision,weselectthemautomatically,imposingthataseedtimeexpressionmusthaveprecisionaboveacertainvalue(understoodasthepercentage,ofalltheappearancesofthesequenceoftokensinthesu-pervisedcorpus,thoseinwhichitistaggedasacor-recttimeexpression).Intheexperimentspresentedbelow,thisthresholdforprecisionoftheseedsis90%—inthehalfofthesupervisedcorpusreservedforextractionofseeds—.Fromthosethatpassthisﬁlter,theonesthatappearwithgreaterfrequencyareselected.Fortimeexpressionsthathaveanidenti-caldigitpattern(e.g.twodates“@@December”ortwoyears“@@@@”,where@standsforanydigit),onlyoneseedistaken.Thisapproachsim-ulatesthehumandomainexpert,whichtypicallyistheﬁrststepinbootstrappingIEmodels55

Unlessspeciﬁcallystatedotherwise,alltheexper-imentspresentedbelowsharethefollowingdefaultsettings:•Onlytheﬁrst2.36Mbytesoftheunsupervisedcorpusareused(10Mbytesaftertokenizationandfeatureextraction),thatis0.5%oftheavailabledata.Thisistokeeptheexecutiontimeofexperimentslow,wheremultipleexper-imentsneedtoberuntooptimizeacertainpa-rameter.•WeusetheCollinsandSingerstrategy(seesection4.1)withaprecisionthresholdof0.50forsub-scorecombinationinpatternselection.Thisstrategyfavourspatternswithslightlyhigherprecision.•Themaximumlengthofpreﬁxandpostﬁxis1and0elements,respectively.Thiswasdeter-minedexperimentally.•100seedexamplesareused(outofamaximumof605available).•Intherankingofexamples,theλiweightsforthethreesub-scoresforinﬁxesare0.5forthe“similarity-basedscore”,0.25for“phrase-completeness”and0.25for“context-basedscore”.•Intheselectionofexamples,themaximumnumberofnewinﬁxesacceptedperiterationis200,withamaximumof50differentcontextsperinﬁx.Intheselectionofpatterns,themax-imumnumberofnewacceptedpatternsperit-erationis5000(althoughthisnumberisneverreachedduetotheanalysisofsubsumptions).•Intheselectionofpatterns,multisetsareusedforcomputingtheinstancesetofapatternforthefrequency-basedscoreandnormalsetsfortheprecisionscore(determinedexperimen-tally).•ThePOStagtypeofgeneralization(patternel-ement)hasbeendeactivated,thatis,neitherall-POSpatterns,norpatternsthatarecombina-tionsofPOSPEswithanotheraregenerated.Afterananalysisoferrors,itwasobservedthatPOSgeneralizations(becauseofthefactthattheyarenotlexicalizedlike,forinstance,thesyntacticPEswithagivenheadword)giverisetoaconsiderablenumberofprecisionerrors.•AllpatternsaregeneratedwithCOMPLETE-PHRASEmodiﬁerautomaticallyattached.Itwasdeterminedexperimentallythatitwasbesttousethisheuristicinallcases(seesection3).5.2VariationofthenumberofseedsWehaveperformedexperimentsusing1,5,10,20,50,100,200and500seeds.Thegeneraltrendsob-servedwereasfollows.Theﬁnalprecision(whenthebootstrappingconverges)decreasesmoreorlessmonotonicallyasthenumberofseedsincreases,al-thoughthereareslightﬂuctuations;besides,thedif-ferenceinthisrespectbetweenusingfewseeds(20to50)ormore(100to200)isofonlyaround3%.However,abigleapcanbeobservedinmovingfrom200to500seeds,whereboththeinitialprecision(oftheseeds)andﬁnalprecision(atpointofcon-vergence)dropby10%wrttousing200seeds.Theﬁnalrecallincreasesmonotonicallyasthenumberofseedsincreases—sincemoresupervisedinforma-tionisprovided—.TheﬁnalF1-measureﬁrstin-creasesandthendecreaseswithanincreasingnum-berofseeds,withanoptimumvaluebeingreachedsomewherebetweenthe50and100seeds.Thelargestgaininrecall(differencebetweenre-calloftheseedsandrecallatthepointofcon-vergence)isachievedwith20seeds,foragainof16.38%(initialrecallis20.08%andﬁnalis36.46%).ThebestmarkinF1-measureisachievedwith100seeds,after6iterations:60.43%(theﬁnalprecisionis69.29%andtheﬁnalrecallis53.58%;thedropinprecisionis6.5%andthegaininrecallis14.28%).Figure3showsalineplotofprecisionvsrecallfortheseexperiments.Thisexperimentsug-geststhattheproblemoftemporalexpressionrecog-nitioncanbecapturedwithminimalsupervisedin-formation(100seeds)andlargeramountsofunsu-pervisedinformation.Figure3:Effectofvaryingthenumberofseeds56

5.3VariationofthetypeofgeneralizationsusedinpatternsIntheseexperiments,wehavedeﬁnedfourdiffer-entssetsofgeneralizations(i.e.typesofpatternele-mentsamongthosespeciﬁedinsection3)toevalu-atehowsemanticandsyntacticgeneralizationscon-tributetoperformanceofthealgorithm.Thesefourexperimentsarelabelledasfollows:NONEincludesonlyPEsoftheLEMMAtype;SYNincludesPEsofthelemmatypeandofthenot-generalizedsyn-tacticchunk(SYN)type;SEMincludesPEsofthelemmatypeandofthesemanticclass(SEMCLASS)type,aswellascombinationsoflemmawithSEM-CLASSPEs;andlastly,SYN+SEMincludesevery-thingthatbothSYNandSEMexperimentsinclude,plusPEsofthegeneralizedsyntacticchunk(SYN-SEMC)type.Onecanobservethanneithertypeofgeneraliza-tion,syntacticorsemantic,isspecially“effective”whenusedinisolation(onlya3.5%gaininrecallinbothcases).Itisonlythecombinationofbothtypesthatgivesagoodgaininrecall(14.28%inthecaseofthisexperiment).Figure4showsalineplotofthisexperiment.Theﬁgureindicatesthattheproblemoftemporalexpressionrecognition,eventhoughappar-entlysimple,requiresbothsyntacticandsemanticinformationforefﬁcientmodeling.Figure4:Effectofusingsyntacticand/orsemanticgen-eralizations5.4VariationofthesizeofunsuperviseddatausedWeperformedexperimentsusingincreasingamountsofunsuperviseddatafortraininginthebootstrapping:1,5,10,50and100Mbytesofpreprocessedcorpus(tokenizedandwithfeatureextraction).Theamountsofplaintextdataareroughlyaﬁfthpart,respectively.Theobjectiveoftheseexperimentsistodeterminewhetherperformanceimprovesastheamountoftrainingdataisincreased.Thenumberofseedspassedtothebootstrappingis68.Themaximumnumberofnewinﬁxes(thepartofanexamplethatcontainsacandidatetimeexpression)acceptedperiterationhasbeenincreasedfrom200to1000,becauseitwasobservedthatlargeramountsofunsupervisedtrainingdataneedagreaternumberofselection“slots”inordertorenderanimprovement(thatis,amore“reckless”bootstrapping),otherwisetheywillﬁllupalltheallowedselectionslots.Theobservedeffectisthatboththedropinpreci-sion(fromtheinitialiterationtothepointofconver-gence)andthegaininrecallimprovemoreorlessconsistentlyasalargeramountoftrainingdataistaken,orotherwisethesamerecallpointisachievedinanearlieriteration.Theseimprovementsarenev-erthelessslight,intheorderofbetween0.5%and2%.Thebiggestimprovementisobservedinthe100Mbytesexperiment,whererecallafter5iterationsis6%betterthaninthe50Mbytesexperimentafter7iterations.Thedropinprecisioninthe100Mbytesexperimentis13.05%,foragaininrecallof21.36%(ﬁnalprecisionis71.02%,ﬁnalrecall52.84%andﬁnalF160.59%).Figure5showsalineplotofthisexperiment.Thisexperimentindicatesthatincreas-ingamountsofunsuperviseddatacanbeusedtoim-provetheperformanceofourmodel,butthetaskisnottrivial.Figure5:Effectofvaryingtheamountofunsupervisedtrainingdata6ConclusionsandfutureresearchWehavepresentedaslightlysupervisedalgorithmfortheextractionofIEpatternsfortherecognition57

oftimeexpressions,basedonbootstrapping,whichintroducesanovelrepresentationofpatternssuitedtothistask.Ourexperimentsshowthatwitharel-ativelysmallamountofsupervision(50to100ini-tialcorrectexamplesorseeds)andusingacombina-tionofsyntacticandsemanticgeneralizations,itispossibletoobtainanimprovementofaround15%-20%inrecall(withregardtotheseeds)andF1-measureover60%learningexclusivelyfromunla-belleddata.Furthermore,usingincreasingamountsofunlabelledtrainingdata(ofwhichthereisplentyavailable)isaworkablewaytoobtainsmallim-provementsinperformance,attheexpenseoftrain-ingtime.Ourcurrentfocusisonaddressingspeciﬁcproblemsthatappearoninspectionoftheprecisionerrorsintest,whichcanimprovebothprecisionandrecalltoadegree.FutureplannedlinesofresearchincludeusingWordNetforimprovingthesemanticaspectsofthealgorithm(semanticclassesandsimi-larity),andstudyingformsofcombiningthepatternsobtainedinthissemi-supervisedapproachwithsu-pervisedlearning.ReferencesM.CollinsandY.Singer.1999.Unsupervisedmod-elsfornamedentityclassiﬁcation.InProceedingsoftheJointSIGDATConferenceonEmpiricalMethodsinNaturalLanguageProcessingandVeryLargeCor-pora,pages100–110,CollegePark,MD.ACL.L.Ferro,L.Gerber,I.Mani,B.Sundheim,andG.Wil-son.2005.Tides2005standardfortheannotationoftemporalexpressions.Technicalreport,MITRECor-poration.E.FilatovaandE.Hovy.2001.Assigningtime-stampstoevent-clauses.InProceedingsofthe2001ACLWork-shoponTemporalandSpatialInformationProcessing,pages88–95.K.Hacioglu,Y.Chen,andB.Douglas.2005.Automatictimeexpressionlabellingforenglishandchinesetext.InProc.ofthe6thInternationalConferenceonIntel-ligentTextProcessingandComputationalLinguistics(CICLing),pages548–559.Springer.Z.Kozareva,E.Riloff,andE.Hovy.2008.Seman-ticclasslearningfromthewebwithhyponympatternlinkagegraphs.InProc.oftheAssociationforCom-putationalLinguistics2008(ACL-2008:HLT),pages1048–1056.D.Lin.1998.Automaticretrievalandclusteringofsim-ilarwords.InProceedingsofthe17thInternationalConferenceonComputationalLinguisticsandthe36thAnnualMeetingoftheAssociationforComputationalLinguistics(COLING-ACL-98),pages768–774,Mon-treal,Quebec.ACL.I.ManiandG.Wilson.2000.Robusttemporalprocess-ingofnews.InProceedingsofthe38thAnnualMeet-ingoftheAssociationforComputationalLinguistics,pages69–76,Morristown,NJ,USA.ACL.M.Pas¸ca,D.Lin,J.Bigham,A.Lifchits,andA.Jain.2006.Namesandsimilaritiesontheweb:Factextrac-tioninthefastlane.InProceedingsofthe21thIn-ternationalConferenceonComputationalLinguisticsand44thAnnualMeetingoftheACL,pages809–816.ACL.E.Riloff.1996.Automaticallygeneratingextractionpat-ternsfromuntaggedtext.InProceedingsoftheThir-teenthNationalConferenceonArtiﬁcialIntelligence(AAAI-96),pages1044–1049.AAAI/MITPress.E.Saquete,R.Mu˜noz,andP.Mart´ınez-Barco.2004.Eventorderingusingterseosystem.InProc.ofthe9thInternationalConferenceonApplicationofNatu-ralLanguagetoInformationSystems(NLDB),pages39–50.Springer.M.StevensonandM.Greenwood.2005.AsemanticapproachtoIEpatterninduction.InProceedingsofthe43rdMeetingoftheAssociationforComputationalLinguistics,pages379–386.ACL.M.Surdeanu,J.Turmo,andA.Ageno.2006.Ahybridapproachfortheacquisitionofinformationextractionpatterns.InProceedingsoftheEACL2006WorkshoponAdaptiveTextExtractionandMining(ATEM2006).ACL.J.M.Wiebe,T.P.O’Hara,T.Ohrstrom-Sandgren,andK.J.McKeever.1998.Anempiricalapproachtotem-poralreferenceresolution.JournalofArtiﬁcialIntelli-genceResearch,9:247–293.R.Yangarber,R.Grishman,P.Tapanainen,andS.Hutunen.2000.Automaticacquisitionofdomainknowledgeforinformationextraction.InProceedingsofthe18thInternationalConferenceofComputationalLinguistics,pages940–946.R.Yangarber.2003.Counter-trainingindiscoveryofsemanticpatterns.InProceedingsofthe41stAnnualMeetingoftheAssociationforComputationalLinguis-tics.ACL.D.Yarowsky.1995.Unsupervisedwordsensedisam-biguationrivalingsupervisedmethods.InProceed-ingsofthe33rdAnnualMeetingoftheAssociationforComputationalLinguistics,pages189–196,Cam-bridge,MA.ACL.Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 58–65,

Boulder, Colorado, June 2009. c(cid:13)2009 Association for Computational Linguistics

58

ASimpleSemi-supervisedAlgorithmForNamedEntityRecognitionWenhuiLiaoandSriharshaVeeramachaneniResearchandDevelpment,ThomsonReuters610OppermanDrive,EaganMN55123{wenhui.liao,harsha.veeramachaneni}@thomsonreuters.comAbstractWepresentasimplesemi-supervisedlearningalgorithmfornamedentityrecognition(NER)usingconditionalrandomﬁelds(CRFs).Thealgorithmisbasedonexploitingevidencethatisindependentfromthefeaturesusedforaclassiﬁer,whichprovideshigh-precisionla-belstounlabeleddata.Suchindependentev-idenceisusedtoautomaticallyextracthigh-accuracyandnon-redundantdata,leadingtoamuchimprovedclassiﬁeratthenextiteration.Weshowthatouralgorithmachievesanaver-ageimprovementof12inrecalland4inpre-cisioncomparedtothesupervisedalgorithm.Wealsoshowthatouralgorithmachieveshighaccuracywhenthetrainingandtestsetsarefromdifferentdomains.1IntroductionNamedentityrecognition(NER)ortaggingisthetaskofﬁndingnamessuchasorganizations,persons,locations,etc.intext.Sincewhetherornotawordisanameandtheentitytypeofanamearedeterminedmostlybythecontextofthewordaswellasbytheentitytypeofitsneighbors,NERisoftenposedasasequenceclassiﬁcationproblemandsolvedbymeth-odssuchashiddenMarkovmodels(HMM)andcon-ditionalrandomﬁelds(CRF).Automaticallytaggingnamedentities(NE)withhighprecisionandrecallrequiresalargeamountofhand-annotateddata,whichisexpensivetoobtain.ThisproblempresentsitselftimeandagainbecausetaggingthesameNEsindifferentdomainsusuallyrequiresdifferentlabeleddata.However,inmostdomainsoneoftenhasaccesstolargeamountsofunlabeledtext.Thisfactmotivatessemi-supervisedapproachesforNER.Semi-supervisedlearninginvolvestheutilizationofunlabeleddatatomitigatetheeffectofinsuf-ﬁcientlabeleddataonclassiﬁeraccuracy.Onevarietyofsemi-supervisedlearningessentiallyat-temptstoautomaticallygeneratehigh-qualitytrain-ingdatafromanunlabeledcorpus.Algorithmssuchasco-training(BlumandMitchell,1998)(CollinsandSinger,1999)(PierceandCardie,2001)andtheYarowskyalgorithm(Yarowsky,1995)makeas-sumptionsaboutthedatathatpermitsuchanap-proach.Themainrequirementfortheautomaticallygen-eratedtrainingdatainadditiontohighaccuracy,isthatitcoversregionsinthefeaturespacewithlowprobabilitydensity.Furthermore,itisneces-sarythatalltheclassesarerepresentedaccordingtotheirpriorprobabilitiesineveryregioninthefea-turespace.Oneapproachtoachievethesegoalsistoselectunlabeleddatathathasbeenclassiﬁedwithlowconﬁdencebytheclassiﬁertrainedontheorig-inaltrainingdata,butwhoselabelsareknownwithhighprecisionfromindependentevidence.Herein-dependencemeansthatthehigh-precisiondecisionrulethatclassiﬁestheselowconﬁdenceinstancesusesinformationthatisindependentofthefeaturesusedbytheclassiﬁer.Weproposetwowaysofobtainingsuchinde-pendentevidenceforNER.Theﬁrstisbasedonthefactthatmultiplementionsofcapitalizedto-kensarelikelytohavethesamelabelandoccurinindependentlychosencontexts.Wecallthisthe59

multi-mentionproperty.Thesecondisbasedonthefactthatentitiessuchasorganizations,persons,etc.,havecontextthatishighlyindicativeoftheclass,yetisindependentoftheothercontext(e.g.com-panysufﬁxeslikeInc.,Co.,etc.,persontitleslikeMr.,CEO,etc.).Wecallsuchcontexthighprecisionindependentcontext.Letusﬁrstlookattwoexamples.Example1:1)saidHarryYou,CEOofHearingPoint....2)Forthisyear’ssecondquarter,Yousaidthecompany’s...Theclassiﬁertags“HarryYou”asperson(PER)correctlysinceitscontext(said,CEO)makesitanobviousname.However,inthesecondsentence,theclassiﬁerfailstotag“You”asapersonsince“You”isusuallyastopword.Thesecondsentenceisex-actlythetypeofdataneededinthetrainingset.Example2:(1)MedtronicInc4Qproﬁtsrise10percent...(2)Medtronic4Qproﬁtsrise10percent...Theclassiﬁertags“Medtronic”correctlyintheﬁrstsentencebecauseofthecompanysufﬁx“Inc”whileitfailstotag“Medtronic”inthesecondsentencesince“4Qproﬁts”isanewpatternand“Medtronic”isunseeninthetrainingdata.Thusthesecondsentenceiswhatweneedinthetrainingset.Thetwoexampleshaveonethingincommon.Inbothcases,thesecondsentencehasanewpatternandincorrectlabels,whichcanbeﬁxedbyusingeithermulti-mentionorhigh-precisioncontextfromtheﬁrstsentence.WeactuallyartiﬁciallyconstructthesecondsentencetobeaddedtothetrainingsetinExample2althoughonlytheﬁrstsentenceexistsintheunlabeledcorpus.Byleveragingsuchindependentevidence,ouralgorithmcanautomaticallyextracthigh-accuracyandnon-redundantdatafortraining,andthusob-tainanimprovedmodelforNER.Speciﬁcally,ouralgorithmstartswithamodeltrainedwithasmallamountofgolddata(manuallytaggeddata).Thismodelisthenusedtoextracthigh-conﬁdencedata,whichisthenusedtodiscoverlow-conﬁdencedatabyusingotherindependentfeatures.Theselow-conﬁdencedataarethenaddedtothetrainingdatatoretrainthemodel.Thewholeprocessrepeatsuntilnosigniﬁcantimprovementcanbeachieved.Ourexperimentsshowthatthealgorithmisnotonlymuchbetterthantheinitialmodel,butalsobetterthanthesupervisedlearningwhenalargeamountofgolddataareavailable.Especially,evenwhenthedomainfromwhichtheoriginaltrainingdataissampledisdifferentfromthedomainofthetestingdata,ouralgorithmstillprovidessigniﬁcantgainsinclassiﬁcationaccuracy.2RelatedWorkTheYarowskyalgorithm(Yarowsky,1995),orig-inallyproposedforwordsensedisambiguation,makestheassumptionthatitisveryunlikelyfortwooccurrencesofawordinthesamediscoursetohavedifferentsenses.Thisassumptionisexploitedbyselectingwordsclassiﬁedwithhighconﬁdenceac-cordingtosenseandaddingothercontextsofthesamewordsinthesamediscoursetothetrainingdata,eveniftheyhavelowconﬁdence.Thisallowsthealgorithmtolearnnewcontextsforthesensesleadingtohigheraccuracy.Ouralgorithmalsousesmulti-mentionfeatures.However,theapplicationoftheYarowskyalgorithmtoNERinvolvesseveraldomain-speciﬁcchoicesaswillbecomeevidentbe-low.WongandNg(WongandNg,2007)usethesameideaofmultiplementionsofatokensequencebe-ingtothesamenamedentityforfeatureengineer-ing.Theyuseanamedentityrecognitionmodelbasedonthemaximumentropyframeworktotagalargeunlabeledcorpus.Thenthemajoritytagsofthenamedentitiesarecollectedinlists.Themodelisthenretrainedbyusingtheselistsasextrafea-tures.Thismethodrequiresasufﬁcientamountofmanuallytaggeddatainitiallytowork.Theirpapershowsthat,iftheinitialmodelhasalowF-score,themodelwiththenewfeaturesleadstolowF-scoretoo.Ourmethodworkswithasmallamountofgolddatabecause,insteadofconstructingnewfeatures,weuseindependentevidencetoenrichthetrainingdatawithhigh-accuracyandnon-redundantdata.Theco-trainingalgorithmproposedbyBlumandMitchell(BlumandMitchell,1998)assumesthatthefeaturescanbesplitintotwoclass-conditionallyindependentsetsor“views”andthateachviewissufﬁcientforaccurateclassiﬁcation.Theclassiﬁerbuiltononeoftheviewsisusedtoclassifyalargeunlabeledcorpusandthedataclassiﬁedwithhigh-60

conﬁdenceareaddedtothetrainingsetonwhichtheclassiﬁerontheotherviewistrained.Thispro-cessisiteratedbyinterchangingtheviews.Themainreasonthatco-trainingworksisthat,becauseoftheclass-conditionalindependenceassumptions,thehigh-conﬁdencedatafromoneview,inadditiontobeinghighlyprecise,isunbiasedwhenaddedtothetrainingsetfortheotherview.Wecouldnotapplyco-trainingforsemi-supervisednamedentityrecognitionbecauseofthedifﬁcultyofﬁndinginfor-mativeyetclass-conditionallyindependentfeaturesets.Collinsetal.(CollinsandSinger,1999)proposedtwoalgorithmsforNERbymodifyingYarowsky’smethod(Yarowsky,1995)andtheframeworksug-gestedby(BlumandMitchell,1998).However,alltheirfeaturesareatthewordsequencelevel,insteadofatthetokenlevel.Atthetokenlevel,theseedrulestheyproposeddonotnecessarilywork.Inad-dition,parsingsentencesintowordsequencesisnotatrivialtask,andalsonotnecessaryforNER,inouropinion.Jiaoetal.proposesemi-supervisedconditionalrandomﬁelds(Jiaoetal.,2006)thattrytomaxi-mizetheconditionallog-likelihoodonthetrainingdataandsimultaneouslyminimizetheconditionalentropyoftheclasslabelsontheunlabeleddata.Thisapproachisreminiscentofthesemi-supervisedlearningalgorithmsthattrytodiscouragethebound-aryfrombeinginregionswithhighdensityofunla-beleddata.Theresultingobjectivefunctionisnolongerconvexandmayresultinlocaloptima.OurapproachincontrastavoidschangingtheCRFtrain-ingprocedure,whichguaranteesglobalmaximum.3NamedEntityRecognitionAslongasindependentevidenceexistsforonetypeofNE,ourmethodcanbedirectlyappliedtoclassifysuchNE.Asanexample,wedemonstratehowtoap-plyourmethodtoclassifythreetypesofNEs:orga-nization(ORG),person(PER),andlocation(LOC)sincetheyarethemostcommonones.Anon-NEisannotatedasO.3.1ConditionalRandomFieldsforNERWeuseCRFtoperformclassiﬁcationinourframe-work.CRFsareundirectedgraphicalmodelstrainedtomaximizetheconditionalprobabilityofase-quenceoflabelsgiventhecorrespondinginputse-quence.LetX,X=x1...xN,beaninputsequence,andY,Y=y1....yN,bethelabelsequencefortheinputsequence.TheconditionalprobabilityofYgivenXis:P(Y|X)=1Z(X)exp(NXn=1Xkλkfk(yn−1,yn,X,n))(1)whereZ(X)isanormalizationterm,fkisafeaturefunction,whichoftentakesabinaryvalue,andλkisalearnedweightassociatedwiththefeaturefk.Theparameterscanbelearnedbymaximizinglog-likelihood‘whichisgivenby‘=XilogP(Yi|Xi)−Xkλ2k2σ2k(2)whereσ2kisthesmoothing(orregularization)pa-rameterforfeaturefk.Thepenaltyterm,usedforregularization,basicallyimposesapriordistributionontheparameters.Ithasbeenshownthat‘isconvexandthusaglobaloptimumisguaranteed(McCallum,2003).InferringlabelsequenceforaninputsequenceXinvolvesﬁndingthemostprobablelabelsequence,Y∗=argmaxYP(Y|X),whichisdonebytheViterbialgorithm(Forney,1973).3.2FeaturesforNEROnebigadvantageofCRFisthatitcannaturallyrepresentrichdomainknowledgewithfeatures.3.2.1StandardFeaturesPartofthefeaturesweusedforourCRFclassiﬁerarecommonfeaturesthatarewidelyusedinNER(McCallumandLi,2003),asshownbelow.1)Lexicon.Eachtokenisitselfafeature.2)Orthography.Orthographicinformationisusedtoidentifywhetheratokeniscapitalized,oranacronym,orapurenumber,orapunctuation,orhasmixedlettersanddigits,etc.3)Single/multiple-tokenlist.Eachlistisacollec-tionofwordsthathaveacommonsematicmeaning,suchaslastname,ﬁrstname,organization,companysufﬁx,city,university,etc.61

4)Jointfeatures.Jointfeaturesaretheconjunc-tionsofindividualfeatures.Forexample,ifatokenisinalastnamelistanditsprevioustokenisinatitlelist,thetokenwillhaveajointfeaturecalledasTitle+Name.5)Featuresofneighbors.Afterextractingtheabovefeaturesforeachtoken,itsfeaturesarethencopiedtoitsneighbors(Theneighborsofatokenin-cludetheprevioustwoandnexttwotokens)withapositionid.Forexample,iftheprevioustokenofatokenhasafeature“Cap@0”,thistokenwillhaveafeature“Cap@-1”.3.2.2LabelFeaturesOneuniqueandimportantfeatureusedinoural-gorithmiscalledLabelFeatures.Alabelfeatureistheoutputlabelofatokenitselfifitisknown.Wedesignedsomesimplehigh-precisionrulestoclassifyeachtoken,whichtakeprecedenceovertheCRF.Speciﬁcally,ifatokendoesnotincludeanyuppercaseletter,isnotanumber,anditisnotinthenocaplist(whichincludesthetokensthatarenotcapitalizedbutstillcouldbepartofanNE,suchasal,at,in,-,etc),thelabelofthistokenis“O”.Table1:AnexampleofextractedfeaturesTokensFeatureMondayW=Monday@0O@0viceW=vice@0O@0chairmanW=chairman@0title@0O@0GoffW=Goff@0CAP@0Lastname@0W=chairman@-1title@-1O@-1W=vice@-2O@-2W=said@1O@1W=it@2O@2saidW=said@0O@0theW=it@0O@0companyW=company@0O@0Inaddition,ifatokenissurroundedby“O”to-kensandisinaStopwordlist,orinaTimelist(acollectionofdate,timerelatedtokens),orinano-caplist,oranonNElist(acollectionoftokensthatareunlikelytobeanNE),orapurenumber,itslabelis“O”aswell.Forexample,inthesentence“Fordhassaidthereisnoplantolayoffworkers”,allthetokensexcept“Ford”have“O”labels.MorerulescanbedesignedtoclassifyNElabels.Forexample,ifatokenisinanunambiguousORGlist,ithasalabel“ORG”.Foranytokenwithaknownlabel,unlessitisaneighborofatokenwithitslabelunknown(i.e.,notpretaggedwithhighprecision),itsfeaturesincludeonlyitslexiconanditslabelitself.Nofeatureswillbecopiedfromitsneighborseither.Table1givesanexampletodemonstratethefeaturesusedinouralgorithm.Forthesentence“MondayvicechairmanGoffsaidthecompany...”,only“Goff”includesitsownfeaturesandfeaturescopiedfromitsneighbors,whilemostoftheothertokenshaveonlytwofea-turessincetheyare“O”tokensbasedonthehigh-precisionrules.Usually,morethanhalfthetokenswillbeclassi-ﬁedas“O”.Thisstrategygreatlysavesfeatureex-tractiontime,trainingtime,andinferencetime,aswellasimprovingtheaccuracyofthemodel.Mostimportantly,thisstrategyisnecessaryinthesemi-supervisedlearning,whichwillbeexplainedinthenextsection.4Semi-supervisedLearningAlgorithmOursemi-supervisedalgorithmisoutlinedinTa-ble2.WeassumethatwehaveasmallamountoflabeleddataLandaclassiﬁerCkthatistrainedonL.WeexploitalargeunlabeledcorpusUfromthetestdomainfromwhichweautomaticallyandgrad-uallyaddnewtrainingdataDtoL,suchthatLhastwoproperties:1)accuratelylabeled,meaningthatthelabelsassignedbyautomaticannotationoftheselectedunlabeleddataarecorrect,and2)non-redundant,whichmeansthatthenewdataisfromregionsinthefeaturespacethattheoriginaltrainingsetdoesnotadequatelycover.ThustheclassiﬁerCkisexpectedtogetbettermonotonicallyasthetrain-ingdatagetsupdated.Table2:Thesemi-supervisedNERalgorithmGiven:L-asmallsetoflabeledtrainingdataU-unlabeleddataLoopforkiterations:Step1:TrainaclassiﬁerCkbasedonL;Step2:ExtractnewdataDbasedonCk;Step3:AddDtoL;Ateachiteration,theclassiﬁertrainedonthepre-vioustrainingdata(usingthefeaturesintroducedintheprevioussection)isusedtotagtheunlabeleddata.Inaddition,foreachOtokenandNEseg-ment,aconﬁdencescoreiscomputedusingthecon-62

strainedforward-backwardalgorithm(CulottaandMcCallum,2004),whichcalculatestheLcX,thesumoftheprobabilitiesofallthepathspassingthroughtheconstrainedsegment(constrainedtobetheas-signedlabels).Onewaytoincreasethesizeofthetrainingdataistoaddallthetokensclassiﬁedwithhighconﬁ-dencetothetrainingset.Thisschemeisunlikelytoimprovetheaccuracyoftheclassiﬁeratthenextiterationbecausethenewlyaddeddataisunlikelytoincludenewpatterns.Instead,weusethehighconﬁdencedatatotagotherdatabyexploitinginde-pendentfeatures.•TaggingORGIfasequenceoftokenshasbeenclassiﬁedas“ORG”withhighconﬁdencescore(>T)1,weforcethelabelsofotheroccurrencesofthesamesequenceinthesamedocument,tobe“ORG”andaddallsuchduplicatesequencesclassiﬁedwithlowconﬁdencetothetrainingdataforthenextiteration.Inadditionifahighconﬁdencesegmentendswithcompanysuf-ﬁx,weremovethecompanysufﬁxandcheckthemulti-mentionsoftheremainingsegmentalso.Inadditiontothat,wereclassifythesen-tenceafterremovingthecompanysufﬁxandcheckifthelabelsarestillthesamewithhigh-conﬁdence.Ifnot,thesequencewillbeaddedtothetrainingdata.AsshowninExample4,“Safewaysharesticked”isaddedtotrainingdatabecause“Safeway”haslowconﬁdenceaf-terremoving“Inc.”.Example4:High-conﬁdenceORG:SafewayInc.sharestickedup...Low-conﬁdenceORG:1)Safewaysharestickedup...2)WallStreetexpectsSafewaytopostearnings...•TaggingPERIfaPERsegmenthasahighconﬁdencescoreandincludesatleasttwotokens,boththisseg-1Throughtherestofthepaper,ahighconﬁdencescoremeansthescoreislargerthanT.Inourexperiments,Tissetas0.98.Alowconﬁdencescoremeansthescoreislowerthan0.8.mentandthelasttokenofthissegmentareusedtoﬁndtheirothermentions.Similarly,weforcetheirlabelstobePERandaddthemtothetrainingdataiftheirconﬁdencescoreislow.However,ifthesementionsarefollowedbyanycompanysufﬁxandarenotclassiﬁedasORG,theirlabels,aswellasthecompanysufﬁxareforcedtobeORG(e.g.,Jefferies&Co.).Werequirethehigh-conﬁdencePERseg-menttoincludeatleasttwotokensbecausetheclassiﬁermayconfusesingle-tokenORGwithPERduetotheircommoncontext.Forex-ample,“Tesoroproposed1.63billionpurchaseof...”,Tesorohashigh-conﬁdencebasedonthemodel,butitrepresentsTesoroCorpinthedoc-umentandthusisanORG.Inaddition,thetitlefeaturecanbeusedsimi-larlyasthecompanysufﬁxfeatures.IfaPERwithatitlefeaturehasahighconﬁdencescore,buthasalowscoreafterthetitlefeatureisremoved,thePERanditsneighborswillbeputintotrainingdataafterremovingthetitle-relatedtokens.Example5:High-conﬁdencePER:1)InvestorABappointsJohanByggeasCFO...2)HeisreplacingChiefCEOAvallone...Low-conﬁdencePER:1)Byggeisemployedat...2)HeisreplacingAvallone...(Itisobviousforahuman-beingthatByggeisPERbecauseoftheexistenceof“employed”.However,whenthetrainingdatadoesn’tin-cludesuchcases,theclassiﬁerjustcannotrec-ognizeit.)•TaggingLOCThesameapproachisusedforaLOCsegmentwithahighconﬁdencescore.Weforcethela-belsofitsothermentionstobeLOCandaddthemtothetrainingdataiftheirconﬁdencescoreislow.Again,ifanyofthesementionsfollowsorisfollowedbyanORGsegmentwithahighconﬁdencescore,weforcethelabelstobeORGaswell.ThisisbecausewhenaLOCisaroundanORG,theLOCisusuallytreatedaspartofanORG,e.g.,GoogleChina.63

Example6:High-conﬁdenceLOC:TheformerSovietre-publicofAzerbaijanis...Low-conﬁdencePER:Azerbaijanenergyreservesbetterthan...ChangeLOCtoORG:shareholdersoftheChicagoBoardofTrade...•TaggingOSincealltheNEsegmentsaddedtothetrain-ingdatahavelowconﬁdencescoresbasedontheoriginalmodel,andespeciallysincemanyofthemwereincorrectlyclassiﬁedbeforecor-rection,thesesegmentsformgoodtrainingdatacandidates.However,allofthemarepositiveexamples.Tobalancethetrainingdata,weneednegativeexamplesaswell.Ifatokenisclassiﬁedas“O”withhighconﬁdencescoreanddoesnothavealabelfeature“O”,thisto-kenwillbeusedasanegativeexampletobeaddedtothetrainingdata.Sincethefeaturesofeachtokenincludethefea-turescopiedfromitsneighbors,inadditiontothoseextractedfromthetokenitself,itsneighborsneedtobeaddedtothetrainingsetalso.Iftheconﬁdenceoftheneighborsarelow,theneighborswillberemovedfromthetrainingdataaftercopyingtheirfeaturestothetokenofinterest.Iftheconﬁdencescoresoftheneighborsarehigh,wefurtherextendtotheneigh-borsoftheneighborsuntillow-conﬁdencetokensarereached.Weremovelow-conﬁdenceneighborsinordertoreducethechancesofaddingtrainingex-ampleswithfalselabels.Table3:Step2ofthesemi-supervisedalgorithmStep2:ExtractnewdataDbasedonCki)ClassifykthportionofUandcomputeconﬁdencescores;ii)Findhigh-conﬁdenceNEsegmentsandusethemtotagotherlow-conﬁdencetokensiii)FindqualiﬁedOtokensiv)ExtractselectedNEandOtokensaswellastheirneighborsv)ShufﬂepartoftheNEsintheextracteddatavi)AddextracteddatatoDNowwehavebothnegativeandpositivetrainingexamples.However,oneproblemwiththepositivedataisthatthesameNEmayappeartoomanytimessincethemulti-mentionpropertyisused.Forex-ample,theword“Citigroup”mayappearhundredsoftimesinrecentﬁnancialarticlesbecauseofthesubprimecrisis.ToaccountforthisbiasinthedatawerandomlyreplacetheseNEs.Speciﬁcally,wereplaceaportionofsuchNEswithNEsrandomlychosenfromourNElists.ThesizeoftheportionisdecidedbytheratiooftheNEsthatarenotinourNElistoveralltheNEsinthegolddata.Table3summarizesthekeysub-stepsinStep2ofthealgorithm.Ateachstep,morenon-redundantandhigh-accuracydataisaddedintothetrainingsetandthusimprovesthemodelgradually.5ExperimentsThedatasetusedintheexperimentsisexplainedinTable4.Althoughwehave1000labelednewsdocumentsfromtheThomsonFinancial(TF)Newssource,only60documentsareusedastheinitialtrainingdatainouralgorithm.Fortheevaluation,thegolddatawassplitintotrainingandtestsetsasappropriate.ThetoolboxweusedforCRFisMallet(McCallum,2002).Table4:Datasource.Tokensincludewords,punctuationandsentencebreaks.GoldData1000docsfromTFnews(around330tokensperdoc)UnlabeledCorpus100,000docsfromTFnews0.50.60.70.80.9939495969798Confidence Score ThresholdAccuracyFigure1:Tokenaccuracyvsconﬁdencescore.Weﬁrstinvestigatedourassumptionthatahighconﬁdencescoreindicateshighclassiﬁcationaccu-racy.Figure1illustrateshowaccuracyvariesasCRFconﬁdencescorechangeswhen60documents64

areusedastrainingdataandtheremainingareusedastestingdata.Whenthethresholdis0.98,thetokenaccuracyiscloseto99%.Webelievethisaccuracyissufﬁcientlyhightojustifyusingthehighconﬁdencescoretoextracttokenswithcorrectlabels.Table5:Precisionandrecalloftheautomaticallyex-tractedtrainingdataNEPrecision%Recall%F-score%LOC94.596.895.6ORG96.693.494.9PER95.089.692.2Wewishedtostudytheaccuracyofourtrainingdatagenerationstrategyfromhowwellitdoesonthegolddata.Wetreattheremaininggolddata(exceptthedatatrainedfortheinitialmodel)asiftheywereunlabeled,andthenappliedourdataextractionstrat-egyonthem.Table5illustratestheprecisionandre-callforthethreetypesofNEsoftheextracteddata,whichonlyaccountsforasmallpartofthegolddata.TheaverageF-scoreiscloseto95%.Althoughtheprecisionandrecallarenotperfect,webelievetheyaregoodenoughforthetrainingpurpose,consider-ingthathumantaggeddataisseldommoreaccurate.Wecomparedthesemi-supervisedalgorithmwithasupervisedalgorithmusingthesamefeatures.Thesemi-supervisedalgorithmstartswith60labeleddocuments(around20,000tokens)andendswitharound1.5milliontokens.Wetrainedthesupervisedalgorithmwithtwodatasets:usingonly60docu-ments(around20,000tokens)andusing700doc-uments(around220,000tokens)respectively.Thereasonforthechoiceofthetrainingsetsizeisthefactthat20,000tokensareareasonablysmallamountofdataforhumantotag,and220,000tokensaretheamountusuallyusedforsupervisedalgo-rithms(CoNLL2003EnglishNER(SangandMeul-der,2003)trainingsethasaround220,000tokens).Table6illustratestheresultswhen300docu-mentsareusedfortesting.AsshowninTable6,startingwithonly6%ofthegolddata,thesemi-supervisedalgorithmachievesmuchbetterresultsthanthesemi-supervisedalgorithmwhenthesameamountofgolddataisused.ForLOC,ORG,andPER,therecallincreases5.5,16.8,and8.2respec-tively,andtheprecisionincreases2.4,1.5,and6.8respectively.Evencomparedwiththemodeltrainedwith220,000tokens,thesemi-supervisedlearningalgorithmisbetter.Especially,forPER,thepre-cisionandrecallincrease2.8and4.6respectively.Figure2illustrateshowtheclassiﬁerisimprovedateachiterationinthesemi-supervisedlearningalgo-rithm.Table6:Classiﬁcationresults.P/RrepresentsPreci-sion/Recall.Thenumbersinsidetheparenthesesaretheresultdifferenceswhenthemodeltrainedfrom60docsisusedasbaseline.TrainingDataP/R(LOC)P/R(ORG)P/R(PER)60docs88.1/85.686.0/64.274.5/81.2700docs91.2/88.290.5/76.678.3/84.8(3.1/3.6)(4.5/12.4)(3.8/3.6)semi-supervised90.5/91.187.5/81.081.1/89.4(60docs)(2.4/5.5)(1.5/16.8)(6.6/8.2)12345678982838485IterationOverall F−ScoreFigure2:OverallF-scorevsiterationnumbersTable7comparestheresultswhenthemulti-mentionpropertyisalsousedintestingasahigh-precisionrule.ComparingTable7toTable6,wecanseethatwiththesametrainingdata,usingmulti-mentionpropertyhelpsimproveclassiﬁcationre-sults.However,thisimprovementislessthanthatobtainedbyusingthispropertytoextracttrainingdatathusimprovethemodelitself.(Forafaircom-parison,themodelusedinthesemi-supervisedalgo-rithminTable6onlyusesmulti-mentionpropertytoextractdata.)Ourlastexperimentistotesthowthismethodcanbeusedwhentheinitialgolddataandthetestingdataarefromdifferentdomains.WeusetheCoNLL2003EnglishNER(SangandMeulder,2003)train-ingsetastheinitialtrainingdata,andautomaticallyextracttrainingdatafromtheTFﬁnancialnewscor-pus.TheCoNLLdataisacollectionofnewswire65

documentsfromtheReutersCorpus,whileTFdataincludesﬁnancial-relatednewsonly.Table8illus-tratestheresults.Asshowninthetable,withonlyCoNLLdata,althoughitcontainsaround220,000tokens,theresultsarenotbetterthantheresultswhenonly60TFdocs(Table6)areusedfortrain-ing.ThisindicatesthatdatafromdifferentdomainscanadverselyaffectNERaccuracyforsupervisedlearning.However,thesemi-supervisedalgorithmachievesreasonablyhighaccuracy.ForLOC,ORG,andPER,therecallincreases16,20.3,and4.7re-spectively,andtheprecisionincreases4.5,5.5,and4.7respectively.Thereforeoursemi-supervisedap-proachiseffectiveforsituationwherethetestandtrainingdataarefromdifferentsources.Table7:Classiﬁcationresultswhenmulti-mentionprop-erty(M)isusedintestingTrainigDataP/R(LOC)P/R(ORG)P/R(PER)60docs+M89.9/87.682.4/71.478.2/87.3700docs+M91.2/89.190.2/78.379.4/91.1(1.3/1.5)(7.8/6.9)(1.2/3.8)semi-supervised90.0/91.086.6/82.481.3/90.6+M(60docs)(1.1/3.4)(4.2/11.0)(3.1/3.3)Table8:ClassiﬁcationresultstrainedonCoNLLdataandtestonTFdata.Trainingdataforthesemi-supervisedalgorithmareautomaticallyextractedusingbothmulti-mentionandhigh-precisioncontextfromTFcorpus.TrainingDataP/R(LOC)P/R(ORG)P/R(PER)CoNLL85.6/74.775.2/65.972.4/85.2Semi-supervised90.1/90.781.7/86.277.1/90.5(CoNLL)(4.5/16)(5.5/20.3)(4.7/4.7)6ConclusionWepresentedasimplesemi-supervisedlearningal-gorithmforNERusingconditionalrandomﬁelds(CRFs).Inadditionweproposedusinghighpreci-sionlabelfeaturestoimproveclassiﬁcationaccuracyaswellastoreducetrainingandtesttime.Comparedtoothersemi-supervisedlearningal-gorithm,ourproposedalgorithmhasseveraladvan-tages.Itisdomainanddataindependent.Althoughitrequiresasmallamountoflabeledtrainingdata,thedataisnotrequiredtobefromthesamedomainastheoneinwhichareinterestedtotagNEs.ItcanbeappliedtodifferenttypesofNEsaslongasin-dependentevidenceexists,whichisusuallyavail-able.Itissimpleand,webelievenotlimitedbythechoiceoftheclassiﬁer.AlthoughweusedCRFsinourframework,othermodelscanbeeasilyincorpo-ratedtoourframeworkaslongastheyprovideaccu-rateconﬁdencescores.Withonlyasmallamountoftrainingdata,ouralgorithmcanachieveabetterNEtaggingaccuracythanasupervisedalgorithmwithalargeamountoftrainingdata.ReferencesA.BlumandT.Mitchell.1998.Combininglabeledandunlabeleddatawithco-training.ProceedingsoftheWorkshoponComputationalLearningTheory,pages92–100.MichaelCollinsandYoramSinger.1999.Unsupervisedmodelsfornamedentityclassiﬁcation.ProceedingsoftheJointSIGDATConferenceonEmpiricalMeth-odsinNaturalLanguageProcessingandVeryLargeCorpora.A.CulottaandA.McCallum.2004.Conﬁdenceestima-tionforinformationextraction.HLT-NAACL.G.D.Forney.1973.Theviterbialgorithm.ProceedingsoftheIEEE,61(3):268–278.FengJiao,ShaojunWang,ChiH.Lee,RussellGreiner,andDaleSchuurmans.2006.Semi-supervisedcondi-tionalrandomﬁeldsforimprovedsequencesegmen-tationandlabeling.InProceedingsofthe21stIn-ternationalConferenceonComputationalLinguistics,pages209–216,July.AndrewMcCallumandWeiLi.2003.Earlyresultsfornamedentityrecognitionwithconditionalrandomﬁelds,featureinductionandweb-enhancedlexicons.CoNLL.A.K.McCallum.2002.Mallet:Amachinelearningforlanguagetoolkit.http://mallet.cs.umass.edu.AndrewMcCallum.2003.Efﬁcientlyinducingfeaturesofconditionalrandomﬁelds.DavidPierceandClaireCardie.2001.Limitationsofco-trainingfornaturallanguagelearningfromlargedatasets.InEMNLP.ErikF.TjongKimSangandFienDeMeulder.2003.Introductiontotheconll-2003sharedtask:Language-independentnamedentityrecognition.CoNLL,pages142–147.YingchuanWongandHweeTouNg.2007.Oneclasspernamedentity:Exploitingunlabeledtextfornamedentityrecognition.IJCAI,pages1763–1768.DavidYarowsky.1995.Unsupervisedwordsensedisam-biguationrivalingsupervisedmethods.InMeetingoftheAssociationforComputationalLinguistics,pages189–196.Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 66–74,

Boulder, Colorado, June 2009. c(cid:13)2009 Association for Computational Linguistics

66

Can One Language Bootstrap the Other:  A Case Study on Event Extraction  Zheng Chen Heng Ji Department of Computer Science The Graduate Center Queens College and The Graduate Center The City University of New York 365 Fifth Avenue, New York, NY 10016, USA zchen1@gc.cuny.edu hengji@cs.qc.cuny.edu   Abstract This paper proposes a new bootstrapping framework using cross-lingual information pro-jection. We demonstrate that this framework is particularly effective for a challenging NLP task which is situated at the end of a pipeline and thus suffers from the errors propagated from up-stream processing and has low-performance baseline. Using Chinese event extraction as a case study and bitexts as a new source of infor-mation, we present three bootstrapping tech-niques. We first conclude that the standard mono-lingual bootstrapping approach is not so effective. Then we exploit a second approach that potentially benefits from the extra informa-tion captured by an English event extraction sys-tem and projected into Chinese. Such a cross-lingual scheme produces significant performance gain. Finally we show that the combination of mono-lingual and cross-lingual information in bootstrapping can further enhance the perform-ance. Ultimately this new framework obtained 10.1% relative improvement in trigger labeling (F-measure) and 9.5% relative improvement in argument-labeling. 1Introduction Bootstrapping methods can reduce the efforts needed to develop a training set and have shown promise in improving the performance of many tasks such as name tagging (Miller et al., 2004; Ji and Grishman, 2006), semantic class extraction (Lin et al., 2003), chunking (Ando and Zhang, 2005), coreference resolution (Bean and Riloff, 2004) and text classification (Blum and Mitchell, 1998). Most of these bootstrapping methods im-plicitly assume that: • There exists a high-accuracy ‘seed set’ or ‘seed model’ as the baseline; • There exists unlabeled data which is reliable and relevant to the test set in some aspects, e.g. from similar time frames and news sources; and therefore the unlabeled data supports the acquisition of new information, to provide new evidence to be incorporated to bootstrap the model and reduce the sparse data problem. • The seeds and unlabeled data won’t make the old estimates worse by adding too many incor-rect instances. However, for some more comprehensive and challenging tasks such as event extraction, the per-formance of the seed model suffers from the lim-ited annotated training data and also from the errors propagated from upstream processing such as part-of-speech tagging and parsing. In addition, simply relying upon large unlabeled corpora can-not compensate for these limitations because more errors can be propagated from upstream processing such as entity extraction and temporal expression identification. Inspired from the idea of co-training (Blum and Mitchell, 1998), in this paper we intend to boot-strap an event extraction system in one language (Chinese) by exploring new evidences from the event extraction system in another language (Eng-lish) via cross-lingual projection. We conjecture that the cross-lingual bootstrapping for event ex-traction can naturally fit the co-training model:  a same event is represented in two “views” (de-scribed in two languages). Furthermore, the cross-lingual bootstrapping can benefit from the different sources of training data.  For example, the Chinese training corpus includes articles from Chinese new agencies in 2000 while most of English training data are from the US news agencies in 2003, thus English and Chinese event extraction systems have 67

the nature of generating different results on parallel documents and may complement each other. In this paper, we explore approaches of exploiting the increasingly available bilingual parallel texts (bitexts).  We first investigate whether we can improve a Chinese event extraction system by simply using the Chinese side of bitexts in a regular monolin-gual bootstrapping framework. By gradually in-creasing the size of the corpus with unlabeled data, we did not get much improvement for trigger label-ing and even observed performance deterioration for argument labeling. But then by aligning the texts at the word level, we found that the English event extraction results can be projected into Chi-nese for bootstrapping and lead to significant im-provement. We also obtained clear further improvement by combining mono-lingual and cross-lingual bootstrapping. The main contributions of this paper are two- fold. We formulate a new algorithm of cross-lingual bootstrapping, and demonstrate its effec-tiveness in a challenging task of event extraction; and we conclude that, for some applications be-sides machine translation, effective use of bitexts can be beneficial.  The remainder of the paper is organized as fol-lows. Section 2 formalizes the event extraction task addressed in this paper. Section 3 discusses event extraction bootstrapping techniques. Section 4 re-ports our experimental results. Section 5 presents related work. Section 6 concludes this paper and points out future directions. 2Event Extraction  2.1 Task Definition and Terminology The event extraction that we address in this paper is specified in the Automatic Content Extraction (ACE)1 program. The ACE 2005 Evaluation de-fines the following terminology for the event ex-traction task: • event trigger: the word that most clearly ex-presses an event’s occurrence • event argument:  an entity, a temporal expres-sion or a value that plays a certain role in the event instance • event mention: a phrase or sentence with a distinguished trigger and participant arguments                                                              1 http://www.nist.gov/speech/tests/ace/  The event extraction task in our paper is to de-tect certain types of event mentions that are indi-cated by event triggers (trigger labeling), recognize the event participants e.g., who, when, where, how (argument labeling) and merge the co-referenced event mentions into a unified event (post-processing). In this paper, we focus on dis-cussing trigger labeling and argument labeling. In the following example,  Mike got married in 2008. The event extraction system should identify “married” as the event trigger which indicates the event type of “Life” and subtype of “Marry”. Fur-thermore, it should detect “Mike” and “2008” as arguments in which “Mike” has a role of “Person” and “2008” has a role of “Time-Within”. 2.2 A Pipeline of Event Extraction Our pipeline framework of event extraction in-cludes trigger labeling, argument labeling and post-processing, similar to (Grishman et al., 2005), (Ahn, 2006) and (Chen and Ji, 2009). We depict the framework as Figure 1.    Figure 1. Pipeline of Event Extraction  The event extraction system takes raw docu-ments as input and conducts some pre-processing steps. The texts are automatically annotated with word segmentation, Part-of-Speech tags, parsing structures, entities, time expressions, and relations. Argument labeling  Trigger labeling Trigger classification Trigger identification Argument identification Pre-processing Argument classification Post-processing 68

The annotated documents are then sent to the fol-lowing four components. Each component is a classifier and produces confidence values; • Trigger identification: the classifier recognizes a word or a phrase as the event trigger. • Trigger classification: the classifier assigns an event type to an identified trigger.  • Argument identification: the classifier recog-nizes whether an entity, temporal expression or value is an argument associated with a particu-lar trigger in the same sentence. • Argument classification: the classifier assigns a role to the argument.  The post-processing merges co-referenced event mentions into a unified representation of event. 2.3 Two Monolingual Event Extraction Sys-tems We use two monolingual event extraction systems, one for English, and the other for Chinese. Both systems employ the above framework and use Maximum Entropy based classifiers. The corre-sponding classifiers in both systems also share some language-independent features, for example, in trigger identification, both classifiers use the “previous word” and “next word” as features, however, there are some language-dependent fea-tures that only work well for one monolingual sys-tem, for example, in argument identification, the next word of the candidate argument is a good fea-ture for Chinese system but not for English system. To illustrate this, in the Chinese “(cid:1445)” (of) structure, the word “(cid:1445)” (of) strongly suggests that the entity on the left side of “(cid:1445)” is not an argument. For a specific example, in “(cid:2919)(cid:4351)(cid:3431)(cid:1445)(cid:3431)(cid:1225)” (The mayor of New York City), “(cid:2919)(cid:4351)(cid:3431)” (New York City) on the left side of “(cid:1445)” (of) cannot be considered as an argument because it is a modifier of the noun “(cid:3431)(cid:1225)”(mayor). Unlike Chinese, “of” (“(cid:1445)”) appears ahead of the entity in the English phrase. Table 1 lists the overall Precision (P), Recall (R) and F-Measure (F) scores for trigger labeling and argument labeling in our two monolingual event extraction systems.  For comparison, we also list the performance of an English human annotator and a Chinese human annotator.   Table 1 shows that event extraction is a difficult NLP task because even human annotators cannot achieve satisfying performance. Both monolingual systems relied on expensive human labeled data (much more expensive than other NLP tasks due to the extra tagging tasks of entities and temporal ex-pressions), thus a natural question arises: can the  monolingual system benefit from bootstrapping techniques with a relative small set of training data? The other question is: can a monolingual sys-tem benefit from the other monolingual system by cross-lingual bootstrapping? Trigger  Labeling Argument Labeling Performance  System/ Human P R F P R F English  System 64.3 59.4 61.8 49.2 34.7 40.7 Chinese  System 78.8 48.3 59.9 60.6 34.3 43.8 English  Annotator 59.2 59.4 59.3 51.6 59.5 55.3 Chinese  Annotator 75.2 74.6 74.9 58.6 60.9 59.7 Table 1.Performance of Two Monolingual Event Extraction Systems and Human Annotators 3Bootstrapping Event Extraction 3.1 General Bootstrapping Algorithm Bootstrapping algorithms have attracted much at-tention from researchers because a large number of unlabeled examples are available and can be util-ized to boost the performance of a system trained on a small set of labeled examples. The general bootstrapping algorithm is depicted in Figure 2, similar to (Mihalcea, 2004). Self-training and Co-training are two most commonly used bootstrapping methods. A typical self-training process is described as follows: it starts with a set of training examples and builds a classifier with the full integrated fea-ture set. The classifier is then used to label an addi-tional portion of the unlabeled examples. Among the resulting labeled examples, put the most confi-dent ones into the training set, and re-train the clas-sifier. This iterates until a certain condition is satisfied (e.g., all the unlabeled examples have been labeled, or it reaches a certain number of it-erations). Co-training(Blum and Mitchell, 1998) differs from self-training in that it assumes that the data can be represented using two or more separate “views” (thus the whole feature set is split into dis-69

joint feature subsets) and each classifier can be trained on one view of the data. For each iteration, both classifiers label an additional portion of the unlabeled examples and put the most confident ones to the training set. Then the two classifiers are retrained on the new training set and iterate until a certain condition is satisfied. Both self-training and co-training can fit in the general bootstrapping process. If the number of classifiers is set to one, it is a self-training process, and it is a co-training process if there are two dif-ferent classifiers that interact in the bootstrapping process.   Figure 2. General Bootstrapping Algorithm. In the following sections, we adapt the boot-strapping techniques discussed in this section to a larger scale (system level). In other words, we aim to bootstrap the overall performance of the system which may include multiple classifiers, rather than just improve the performance of a single classifier in the system. It is worth noting that for the pipe-line event extraction depicted in Section 2.2, there are two major steps that determine the overall sys-tem performance: trigger labeling and argument labeling. Furthermore, the performance of trigger labeling can directly affect the performance of ar-gument labeling because the involving arguments are constructed according to the trigger. If a trigger is wrongly recognized, all the involving arguments will be considered as wrong arguments. If a trigger is missing, all the attached arguments will be con-sidered as missing arguments. 3.2 Monolingual Self-training It is rather smooth to adapt the idea of traditional self-training to monolingual self-training if we consider our monolingual event extraction system as a black box or even a single classifier that de-termines whether an event combining the result of trigger labeling and argument labeling is a report-able event.  Thus the monolingual self-training procedure for event extraction is quite similar with the one de-scribed in Section 3.1. The monolingual event ex-traction system is first trained on a starting set of labeled documents, and then tag on an additional portion of unlabeled documents. Note that in each labeled document, multiple events could be tagged and confidence score is assigned to each event. Then the labeled documents are added into the training set and the system is retrained based on the events with high confidence. This iterates until all the unlabeled documents have been tagged. 3.3 Cross-lingual Co-Training We extend the idea of co-training to cross-lingual co-training. The intuition behind cross-lingual co-training is that the same event has different “views” described in different languages, because the lexical unit, the grammar and sentence con-struction differ from one language to the other. Thus one monolingual event extraction system probably utilizes the language dependent features that cannot work well for the other monolingual event extraction systems. Blum and Mitchell (1998) derived PAC-like guarantees on learning under two assumptions: 1) the two views are individually suf-ficient for classification and 2) the two views are conditionally independent given the class. Obvi-ously, the first assumption can be satisfied in cross-lingual co-training for event extraction, since each monolingual event extraction system is suffi-cient for event extraction task. However, we re-serve our opinion on the second assumption. Although the two monolingual event extraction systems may apply the same language-independent features such as the part-of-speech, the next word and the previous word, the features are exhibited in their own context of language, thus it is too subjec-tive to conclude that the two feature sets are or are Input:  L: a set of labeled examples,  U: a set of unlabeled examples   {iC}: a set of classifiers Initialization:  Create a pool U¢of examples by choosing Prandom examples from U Loop until a condition is satisfied (e.g., U=Æ, or iteration counter reaches a preset number I)  Train each classifier iC on L, and label the examples in U¢  For each classifier iC,select the most con-fidently labeled examples (e.g., the confi-dence score is above a preset threshold qor the top  K) and add them to L  Refill U¢with examples from U, and keep the size of U¢ as constant P 70

not conditionally independent. It is left to be an unsolved issue which needs further strict analysis and supporting experiments. The cross-lingual co-training differs from tradi-tional co-training in that the two systems in cross-lingual co-training are not initially trained from the same labeled data. Furthermore, in the bootstrap-ping phase, each system only labels half portion of the bitexts in its own language. In order to utilize the labeling result by the other system, we need to conduct an extra step named cross-lingual projec-tion that transforms tagged events from one lan-guage to the other. 3.3.1 A Cross-lingual Co-training Algorithm The algorithm for cross-lingual co-training is de-picted in Figure 3.   Figure 3. Cross-lingual Co-training Algorithm 3.3.2 Cross-lingual Semi-co-training Cross-lingual semi-co-training is a variation of cross-lingual co-training, and it differs from cross-lingual co-training in that it tries to bootstrap only one system by the other fine-trained system. This technique is helpful when we have relatively large amount of training data in one language while we have scarce data in the other language.  Thus we only need to make a small modification in the cross-lingual co-training algorithm so that it can soon be adapted to cross-lingual semi-co-training, i.e., we retrain one system and do not re-train the other. In this paper, we will conduct ex-periments to investigate whether a fine-trained English event extraction system can bootstrap the Chinese event extraction system, starting from a small set of training data. 3.3.3 Cross-lingual Projection Cross-lingual projection is a key operation in the cross-lingual co-training algorithm. In the case of event extraction, we need to project the triggers and the participant arguments from one language into the other language according to the alignment information provided by bitexts. Figure 4 shows an example of projecting an English event into the corresponding Chinese event.  Figure 4. An Example of Cross-lingual Projection Input:  1L: a set of labeled examples in language A 2L: a set of labeled examples in language B U: a set of unlabeled bilingual examples  (bitexts) with alignment information {12,SS}: two monolingual systems, one for language A and the other for language B. Initialization:  Create a pool U¢of examples by choosing Prandom examples from U Loop until a condition is satisfied (e.g., U=Æ, or iteration counter reaches a preset number I)  Train 1Son 1Land 2Son 2L  Use1Sto label the examples in U¢ (the por-tion in Language A) and use 2Sto label the examples in U¢(the portion in Language B)  For1S, select the most confidently labeled examples (e.g., the confidence score is above a preset threshold qor the top K) , apply the operation of cross-lingual projec-tion, transform the selected examples from Language A to Language B, and put them into 2L. The same procedure applies to2S.  Refill U¢with examples from U, and keep the size of U¢ as constant P 71

4Experiments and Results 4.1 Data and Scoring Metric We used the ACE 2005 corpus to set up two mono-lingual event extraction systems, one for English, the other for Chinese. The ACE 2005 corpus contains 560 English documents from 6 sources: newswire, broadcast news, broadcast conversations, weblogs, news-groups and conversational telephone speech; meanwhile the corpus contains 633 Chinese docu-ments from 3 sources: newswire, broadcast news and weblogs.  We then use 159 texts from the LDC Chinese Treebank English Parallel corpus with manual alignment for our cross-lingual bootstrapping ex-periments. We define the following standards to determine the correctness of an event mention: • A trigger is correctly labeled if its event type and offsets match a reference trigger. • An argument is correctly labeled if its event type, offsets, and role match any of the refer-ence argument mentions. 4.2 Monolingual Self-training on ACE 2005 Data We first investigate whether our Chinese event extraction system can benefit from monolingual self-training on ACE data. We reserve 66 Chinese documents for testing purpose and set the size of seed training set to 100. For a single trial of the experiment, we randomly select 100 documents as training set and use the remaining documents as self-training data. For each iteration of the self-training, we keep the pool size as 50, in other words, we always pick another 50 ACE documents to self-train the system. The iteration continues until all the unlabeled ACE documents have been tagged and thus it completes one trial of the ex-periment. We conduct the same experiment for 100 trials and compute the average scores.  The most important motivation for us to conduct self-training experiments on ACE data is that the ACE data provide ground-truth entities and tempo-ral expressions so that we do not have to take into account the effects of propagated errors from up-stream processing such as entity extraction and temporal expression identification. For one setting of the experiments, we set the confidence threshold to 0, in other words, we keep all the labeling results for retraining. The results are given in Figure 5 (trigger labeling) and Figure 6 (argument labeling). It shows that when the number of self-trained ACE documents reaches 450, we obtain a gain of 3.4% (F-Measure) above the baseline for trigger labeling and a gain of 1.4% for argument labeling. For the other setting of the experiments, we set the confidence threshold to 0.8, and the results are presented in Figure 7 and Figure 8. Surprisingly, retraining on the high confidence examples does not lead to much improvement. We obtain a gain of 3.7% above the baseline for trigger labeling and 1.5% for argument labeling when the number of self-trained documents reaches 450.    Figure 5. Self-training for trigger labeling  (confidence threshold = 0)    Figure 6. Self-training for argument labeling  (confidence threshold= 0) 72

 Figure 7. Self-training for trigger labeling  (confidence threshold = 0.8)   Figure 8. Self-training for argument labeling  (confidence threshold = 0.8)  4.3 Cross-lingual Semi-co-training on Bitexts The experiments in Section 4.2 show that we can obtain gain in performance by monolingual self-training on data with ground-truth entities and temporal expressions, but what if we do not have such ground-truth data, then how the errors propa-gated from entity extraction and temporal expres-sion identification will affect the overall performance of our event extraction system? And if these errors are compounded in event extraction, can the cross-lingual semi-co-training alleviate the impact? To investigate all these issues, we use 159 texts from LDC Chinese Treebank English Parallel cor-pus to conduct cross-lingual semi-co-training.  The experimental results are summarized in Figure 9 and Figure 10.   For monolingual self-training on the bitexts, we conduct experiments exactly as section 4.2 except that the entities are tagged by the IE system and the labeling pool size is set to 20. When the number of bitexts reaches 159, we obtain a little gain of 0.4% above the baseline for trigger labeling and a loss of 0.1% below the baseline for argument labeling. The deterioration tendency of the self-training curve in Figure 10 indicates that entity extraction errors do have counteractive impacts on argument labeling. We then conduct the cross-lingual semi-co-training experiments as follows: we set up an Eng-lish event extraction system trained on a relative large training set (500 documents). For each trial of the experiment, we randomly select 100 ACE Chinese document as seed training set, and then it enters a cross-lingual semi-co-training process: for each iteration, the English system labels the Eng-lish portions of the 20 bitexts and by cross-lingual projection, the labeled results are transformed into Chinese and put into the training set of Chinese system. From Figure 9 and Figure 10 we can see that when the number of bitexts reaches 159, we obtain a gain of 1.7% for trigger labeling and 0.7% for argument labeling. We then apply a third approach to bootstrap our Chinese system: during each iteration, the Chinese system also labels the Chinese portions of the 20 bitexts. Then we combine the results from both monolingual systems using the following rules:   If the event labeled by English system is not labeled by Chinese system, add the event to Chinese system  If the event labeled by Chinese system is not labeled by English system, keep the event in the Chinese system  If both systems label the same event but with different event types and arguments, select the one with higher confidence From Figure 9 and Figure 10 we can see that this approach leads to even further improvement in per-formance, shown as the “Combined-labeled” curves. When the number of bitexts reaches 159, we obtain a gain of 3.1% for trigger labeling and 2.1% for argument labeling.  In order to check how robust our approach  is, we conducted the Wilcoxon Matched-Pairs Signed-Ranks Test on F-measures for all these 100 trials. The results show that we can reject  the hypotheses that the improvements using Cross-lingual Semi-co-training were random at a 99.99% confidence level, for both trigger labeling and ar-gument labeling. 73

 Figure 9. Self-training, and Semi-co-training  (English- labeled & Combined-labeled)  for Trigger Labeling   Figure 10. Self-training, and Semi-co-training  (English- labeled & Combined-labeled)  for Argument Labeling 5Related Work There is a huge literature on utilizing parallel cor-pus for monolingual improvement. To our knowl-edge, it can retrace to (Dagan et.al 1991). We apologize to those whose work is not cited due to space constraints. The work described here com-plements some recent research using bitexts or translation techniques as feedback to improve en-tity extraction. Huang and Vogel (2002) presented an effective integrated approach that can improve the extracted named entity translation dictionary and the entity annotation in a bilingual training corpus. Ji and Grishman (2007) expanded this idea of alignment consistency to the task of entity ex-traction in a monolingual test corpus without refer-ence translations, and applied sophisticated inference rules to enhance both entity extraction and translation. Zitouni and Florian (2008) applied English mention detection on translated texts and added the results as additional features to improve mention detection in other languages.  In this paper we share the similar idea of import-ing evidences from English with richer resources to improve extraction in other languages. However, to the best of our knowledge this is the first work of incorporating cross-lingual feedback to improve the event extraction task. More importantly, it is the first attempt of combining cross-lingual projec-tion with bootstrapping methods, which can avoid the efforts of designing sophisticated inference rules or features. 6(cid:0)Conclusions and Future Work Event extraction remains a difficult task not only because it is situated at the end of an IE pipeline and thus suffers from the errors propagated from upstream processing, but also because the labeled data are expensive and thus suffers from data scar-city. In this paper, we proposed a new co-training framework using cross-lingual information projec-tion and demonstrate that the additional informa-tion from English system can be used to bootstrap a Chinese event extraction system.  To move a step forward, we would like to con-duct experiments on cross-lingual co-training and investigate whether the two systems on both sides can benefit from each other. A main issue existing in cross-lingual co-training is that the cross-lingual projection may not be perfect due to the word alignment problem. In this paper, we used a corpus with manual alignment, but in the future we intend to investigate the effect of automatic alignment errors.  We believe that the proposed cross-lingual boot-strapping framework can also be applied to many other challenging NLP tasks such as relation ex-traction. However, we still need to provide a theo-retical analysis of the framework. Acknowledgments This material is based upon work supported by the Defense Advanced Research Projects Agency un-der Contract No. HR0011-06-C-0023 via 27-001022, and the CUNY Research Enhancement Program and GRTI Program.(cid:1) 74

References  Avrim Blum and Tom Mitchell. 1998. Combining La-beled and Unlabeled Data with Co-training. Proc. of the Workshop on Computational Learning Theory. Morgan Kaufmann Publishers. David Ahn. 2006. The stages of event extraction. Proc. COLING/ACL 2006 Workshop on Annotating and Reasoning about Time and Events. Sydney, Austra-lia. David Bean and Ellen Riloff. 2004. Unsupervised Learning of Contextual Role Knowledge for Corefer-ence Resolution. Proc.  HLT-NAACL2004. pp. 297-304. Boston, USA. Fei Huang and Stephan Vogel. 2002. Improved Named Entity Translation and Bilingual Named Entity Ex-traction. Proc. ICMI 2002. Pittsburgh, PA, US. Heng Ji and Ralph Grishman. 2006. Data Selection in Semi-supervised Learning for Name Tagging. In ACL 2006 Workshop on Information Extraction Be-yond the Document:48-55. Sydney, Australia.  Heng Ji and Ralph Grishman. 2007. Collaborative En-tity Extraction and Translation. Proc. International Conference on Recent Advances in Natural Lan-guage Processing 2007. Borovets, Bulgaria. Ido Dagan, Alon Itai and Ulrike Schwall. 1991. Two languages are more informative than one. Proc. ACL 1991. Imed Zitouni and Radu Florian. 2008. Mention Detec-tion Crossing the Language Barrier. Proc. EMNLP. Honolulu, Hawaii. Michael Collins and Yoram Singer. 1999. Unsupervised Models for Named Entity Classification. Proc. of EMNLP/VLC-99. Rada Mihalcea. 2004. Co-training and self-training for word sense disambiguation. In Proceedings of the Conference on Computational Natural Language Learning (CoNLL-2004). Ralph Grishman, David Westbrook and Adam Meyers. 2005. NYU’s English ACE 2005 System Descrip-tion. Proc. ACE 2005 Evaluation Workshop. Wash-ington, US. Rie Ando and Tong Zhang. 2005. A High-Performance Semi-Supervised Learning Methods for Text Chunk-ing. Proc.  ACL2005. pp. 1-8. Ann Arbor, USA Scott Miller, Jethran Guinness and Alex Zama-nian.2004. Name Tagging with Word Clusters and Discriminative Training. Proc. HLT-NAACL2004. pp. 337-342. Boston, USA Winston Lin, Roman Yangarber and Ralph Grishman. 2003. Bootstrapping Learning of Semantic Classes from Positive and Negative Examples. Proc.  ICML-2003 Workshop on The Continuum from Labeled to Unlabeled Data. Washington, D.C. Zheng Chen and Heng Ji. 2009. Language Specific Is-sue and Feature Exploration in Chinese Event Extrac-tion. Proc. HLT-NAACL 2009. Boulder, Co.  Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 75–83,

Boulder, Colorado, June 2009. c(cid:13)2009 Association for Computational Linguistics

75

OnSemi-SupervisedLearningofGaussianMixtureModelsforPhoneticClassiﬁcation∗Jui-TingHuangandMarkHasegawa-JohnsonDepartmentofElectricalandComputerEngineeringUniversityofIllinoisatUrbana-ChampaignIllinois,IL61801,USA{jhuang29,jhasegaw}@illinois.eduAbstractThispaperinvestigatessemi-supervisedlearn-ingofGaussianmixturemodelsusinganuni-ﬁedobjectivefunctiontakingbothlabeledandunlabeleddataintoaccount.Twomethodsarecomparedinthiswork–thehybriddis-criminative/generativemethodandthepurelygenerativemethod.Theydifferinthecrite-riontypeonlabeleddata;thehybridmethodusestheclassposteriorprobabilitiesandthepurelygenerativemethodusesthedatalike-lihood.WeconductedexperimentsontheTIMITdatabaseandastandardsyntheticdatasetfromUCIMachineLearningrepository.Theresultsshowthatthetwomethodsbe-havesimilarlyinvariousconditions.Forbothmethods,unlabeleddataimprovetrainingonmodelsofhighercomplexityinwhichthesu-pervisedmethodperformspoorly.Inaddition,thereisatrendthatmoreunlabeleddatare-sultsinmoreimprovementinclassiﬁcationac-curacyoverthesupervisedmodel.Wealsoprovidedexperimentalobservationsontherel-ativeweightsoflabeledandunlabeledpartsofthetrainingobjectiveandsuggestedacriti-calvaluewhichcouldbeusefulforselectingagoodweighingfactor.1IntroductionSpeechrecognitionacousticmodelscanbetrainedusinguntranscribedspeechdata(WesselandNey,2005;Lameletal.,2002;L.WangandWoodland,2007).Mostsuchexperimentsbeginbyboostraping∗ThisresearchisfundedbyNSFgrants0534106and0703624.aninitialacousticmodelusingalimitedamountofmanuallytranscribeddata(normallyinascalefrom30minutestoseveralhours),andthentheinitialmodelisusedtotranscribearelativelylargeamountofuntranscribeddata.Onlythetranscriptionswithhighconﬁdencemeasures(WesselandNey,2005;L.WangandWoodland,2007)orhighagreementwithclosedcaptions(Lameletal.,2002)arese-lectedtoaugmentthemanuallytranscribeddata,andnewacousticmodelsaretrainedontheaugmenteddataset.Thegeneralproceduredescribedaboveexactlyliesinthecontextofsemi-supervisedlearningprob-lemsandcanbecategorizedasaself-trainingalgo-rithm.Self-trainingisprobablythesimplestsemi-supervisedlearningmethod,butitisalsoﬂexibletobeappliedtocomplexclassiﬁerssuchasspeechrecognitionsystems.Thismaybethereasonwhylittleworkhasbeendoneonexploitingothersemi-supervisedlearningmethodsinspeechrecognition.Thoughnotincorporatedtospeechrecognizersyet,therehasbeensomeworkonsemi-supervisedlearn-ingofHiddenMarkovModels(HMM)forsequen-tialclassiﬁcation.InoueandUeda(2003)treatedtheunknownclasslabelsoftheunlabeleddataashiddenvariablesandusedtheexpectation-maximization(EM)algorithmtooptimizethejointlikelihoodoflabeledandunlabeleddata.RecentlyJietal.(2009)appliedahomotopymethodtoselecttheoptimalweighttobalancebetweentheloglikelihoodofla-beledandunlabeleddatawhentrainingHMMs.Besidesgenerativetrainingofacousticmodels,discriminativetrainingisanotherpopularparadigmintheareaofspeechrecognition,butonlywhen76

thetranscriptionsareavailable.WangandWood-land(2007)usedtheself-trainingmethodtoaug-mentthetrainingsetfordiscriminativetraining.HuangandHasegawa-Johnson(2008)investigatedanotheruseofdiscriminativeinformationfromla-beleddatabyreplacingthelikelihoodoflabeleddatawiththeclassposteriorprobabilityoflabeleddatainthesemi-supervisedtrainingobjectiveforGaussianMixtureModels(GMM),resultinginahybriddis-criminative/generativeobjectivefunction.Theirex-perimentalresultsinbinaryphoneticclassiﬁcationshowedsigniﬁcantimprovementinclassiﬁcationac-curacywhenlabeleddataarescarce.Asimilarstrat-egycalled”‘multi-conditionallearning”’waspre-sentedin(Drucketal.,2007)appliedtoMarkovRandomFieldmodelsfortextclassiﬁcationtasks,withthedifferencethatthelikelihoodoflabeleddataisalsoincludedintheobjective.Thehybriddis-criminative/generativeobjectivefunctioncanbein-terpretedashavinganextraregularizationterm,thelikelihoodofunlabeleddata,inthediscriminativetrainingcriterionforlabeleddata.However,bothmethodsin(HuangandHasegawa-Johnson,2008)and(Drucketal.,2007)encounteredthesameissueaboutdeterminingtheweightsforlabeledandun-labeledpartintheobjectivefunctionandchosetouseadevelopmentsettoselecttheoptimalweight.Thispaperprovidesanexperimentalanalysisontheeffectoftheweight.Withtheultimategoalofapplyingsemi-supervisedlearninginspeechrecognition,thispa-perinvestigatesthelearningcapabilityofalgorithmswithinGaussianMixtureModelsbecauseGMMisthebasicmodelinsideaHMM,therefore1)theup-dateequationsderivedfortheparametersofGMMcanbeconvenientlyextendedtoHMMforspeechrecognition.2)GMMcanserveasaninitialpointtohelpusunderstandmoredetailsaboutthesemi-supervisedlearningprocessofspectralfeatures.Thispapermakesthefollowingcontribution:•itprovidesanexperimentalcomparisonofhy-bridandpurelygenerativetrainingobjectives.•itstudiestheimpactofmodelcomplexityonlearningcapabilityofalgorithms.•itstudiestheimpactoftheamountofunlabeleddataonlearningcapabilityofalgorithms.•itanalyzestheroleoftherelativeweightsoflabeledandunlabeledpartsofthetrainingob-jective.2AlgorithmSupposealabeledsetXL=(x1,...,xn,...,xNL)hasNLdatapointsandxn∈Rd.YL=(y1,...,yn,...,yNL)arethecorrespondingclasslabels,whereyn∈{1,2,...,Y}andYisthenum-berofclasses.Inaddition,wealsohaveanunla-beledsetXU=(x1,...,xn,...,xNU)withoutcor-respondingclasslabels.EachclassisassignedaGaussianMixturemodel,andallmodelsaretrainedgivenXLandXU.Thissectionﬁrstpresentsthehybriddiscriminative/generativeobjectivefunctionfortrainingandthenthepurelygenerativeobjectivefunction.Theparameterupdateequationsarealsoderivedhere.2.1HybridObjectiveFunctionThehybriddiscriminative/generativeobjectivefunc-tioncombinesthediscriminativecriterionforla-beleddataandthegenerativecriterionforunlabeleddata:F(λ)=logP(YL|XL;λ)+αlogP(XU;λ),(1)andwechosetheparameterssothat(1)ismaxi-mized:ˆλ=argmaxλF(λ).(2)Theﬁrstcomponentconsidersthelogposteriorclassprobabilityofthelabeledsetwhereasthesec-ondcomponentconsiderstheloglikelihoodoftheunlabeledsetweightedbyα.InASRcommunity,modeltrainingbasedtheﬁrstcomponentisusuallyreferredtoasMaximumMutualInformationEsti-mation(MMIE)andthesecondcomponentMaxi-mumLikelihoodEstimation(MLE),thereforeinthispaperweuseabriefnotationfor(1)justforconve-nience:F(λ)=F(DL)MMI(λ)+αF(DU)ML(λ).(3)Thetwocomponentsaredifferentinscale.First,thesizeofthelabeledsetisusuallysmallerthanthesizeoftheunlabeledsetinthescenarioofsemi-supervisedlearning,sothesumsoverthedatasetsinvolvedifferentnumbersofterms;Second,the77

scalesoftheposteriorprobabilityandthelikeli-hoodareessentiallydifferent,soaretheirgradients.Whiletheweightαbalancestheimpactsoftwocomponentsonthetrainingprocess,itmayalsoim-plicitlynormalizethescalesofthetwocomponents.Insection(3.2)wewilldiscussandprovideafurtherexperimentalanalysis.Inthispaper,themodelstobetrainedareGaus-sianmixturemodelsofcontinuousspectralfeaturevectorsforphoneticclasses,whichcanbefurtherextendedtoHiddenMarkovModelswithextrapa-rameterssuchastransitionprobabilities.Themaximizationof(1)followsthetechniquesin(Povey,2003),whichusesauxiliaryfunctionsforobjectivemaximization;Ineachiteration,astrongorweaksenseauxiliaryfunctionismaximized,suchthatiftheauxiliaryfunctionconvergesafteritera-tions,theobjectivefunctionwillbeatalocalmaxi-mumaswell.Theobjectivefunction(1)canberewrittenasF(λ)=logP(XL|YL;λ)−logP(XL;λ)+αlogP(XU;λ),(4)wherethetermlogP(YL;λ)isremovedbecauseitisindependentofacousticmodelparameters.Theauxiliaryfunctionatthecurrentparameterλoldfor(4)isG(λ,λ(old))=Gnum(λ,λ(old))−Gden(λ,λ(old))+αGden(λ,λ(old);DU)+Gsm(λ,λ(old)),(5)wheretheﬁrstthreetermsarestrong-senseauxiliaryfunctionsfortheconditionallikelihood(referredtoasthenumerator(num)modelbecauseitappearsinthenumeratorwhencomputingtheclassposteriorprobability)logP(XL|YL;λ)andthemarginallike-lihoods(referredtoasthedenominator(den)modellikewise)logP(XL;λ)andαlogP(XU;λ)respec-tively.Thelasttermisasmoothingfunctionthatdoesn’taffectthelocaldifferentialbutensuresthatthesumoftheﬁrstthreetermisatleastaconvexweak-senseauxiliaryfunctionforgoodconvergenceinoptimization.Maximizationof(5)leadstotheupdateequationsfortheclassjandmixturemgivenasfollows:ˆµjm=1γjm xxxnumjm,−xxxdenjm+αxxxdenjm(DU)+Djmµjm(cid:1)(6)ˆσ2jm=1γjm sssnumjm−sssdenjm+αsssdenjm(DU)+Djm σ2jm+µ2jm(cid:1)(cid:1)−ˆµ2jm,(7)whereforclaritythefollowingsubstitutionisused:γjm=γnumjm−γdenjm+αγdenjm(DU)+Djm(8)andγjmisthesumoftheposteriorprobabilitiesofoccupationofmixturecomponentmofclassjoverthedataset:γnumjm(X)=Xxi∈X,yi=jp(m|xi,yi=j)γdenjm(X)=Xxi∈Xp(m|xi)(9)andxxxjmandsssjmarerespectivelytheweightedsumofxiandx2ioverthewholedatasetwiththeweightp(m|xi,yi=j)orp(m|xi),dependingonwhetherthesuperscriptisthenumeratorordenomi-natormodel.Djmisaconstantsettobethegreateroftwicethesmallestvaluethatguaranteespositivevariancesorγdenjm(Povey,2003).There-estimationformulaformixtureweightsisalsoderivedfromtheExtendedBaum-Welchalgorithm:ˆcjm=cjmn∂F∂cjm+CoPm′cjm′n∂F∂cjm+Co,(10)wherethederivativewasapproximated(Merialdo,1988)inthefollowingformforpracticalrobustnessforsmall-valuedparameters:∂FMMI∂cjm≈γnumjmPm′γnumjm′−γdenjmPm′γdenjm′.(11)Underourhybridframework,thereisanextratermγdenjm(DU)/Pm′γdenjm′(DU)thatshouldexistin(11),butinpracticewefoundthataddingthistermtotheapproximationisnotbetterthantheoriginalform.Therefore,wekeepusingMMI-onlyupdateformix-tureweights.TheconstantCischosensuchthatallparameterderivativesarepositive.78

2.2PurelyGenerativeObjectiveInthispaperwecomparethehybridobjectivewiththepurelygenerativeone:F(λ)=logP(XL|YL;λ)+αlogP(XU;λ),(12)wherethetwocomponentsaretotalloglikelihoodoflabeledandunlabeleddatarespectively.(12)doesn’tsufferfromtheproblemofcombiningtwoheteroge-neousprobabilisticitems,andtheweightαbeingequaltoonemeansthattheobjectiveisajointdatalikelihoodoflabeledandunlabeledsetwiththeas-sumptionthatthetwosetsareindependent.How-ever,DLorDUmightjustbeasampledsetofthepopulationandmightnotreﬂectthetrueproportion,sowekeepαtoallowaﬂexiblecombinationoftwocriteria.Ontopofthat,weneedtoadjusttherelativeweightsofthetwocomponentsinpracticalexperi-ments.TheparameterupdateequationisareducedformoftheequationsinSection(2.1):ˆµjm=xxxnumjm,+αxxxdenjm(DU)γnumjm+αγdenjm(DU)(13)ˆσ2jm=sssnumjm+αsssdenjm(DU)γnumjm+αγdenjm(DU)−ˆµ2jm(14)3ResultsandDiscussionThepurposeofdesigningthelearningalgorithmsisforclassiﬁcation/recognitionofspeechsounds,soweconductedphoneticclassiﬁcationexperimentsusingtheTIMITdatabase(Garofoloetal.,1993).Wewouldliketoinvestigatetherelationoflearningcapabilityofsemi-supervisedalgorithmstootherfactorsandgeneralizeourobservationstootherdatasets.Therefore,weusedanothersyntheticdatasetWaveformfortheevaluationofsemi-supervisedlearningalgorithmsforGaussianMixturemodel.TIMIT:Weusedthesame48phoneclassesandfurthergroupedinto39classesaccordingto(LeeandHon,1989)asourﬁnalsetofphoneclassestomodel.Weextracted50speakersoutoftheNISTcompletetestsettoformthedevelopmentset.Allofourexperimentalanalyseswereonthedevelop-mentset.Weusedsegmentalfeatures(Halberstadt,1998)inthephoneticclassiﬁcationtask.Foreachphoneoccurrence,aﬁxed-lengthvectorwascalcu-latedfromtheframe-basedspectralfeatures(12PLPcoefﬁcientsplusenergy)witha5msframerateanda25msHammingwindow.Morespeciﬁcally,wedividedtheframesforeachphoneintothreeregionswith3-4-3proportionandcalculatedthePLPav-erageovereachregion.Threeaveragesplusthelogdurationofthatphonegavea40-dimensional(13×3+1)measurementvector.Waveform:WeusedthesecondversionsoftheWaveformdatasetavailableattheUCIreposi-tory(AsuncionandNewman,2007).Therearethreeclassesofdata.Eachtokenisdescribedby40realattributes,andtheclassdistributioniseven.Forwaveform,becausetheclasslabelsareequallydistributed,wesimplyassignedequalnumberofmixturesforeachclass.ForTIMIT,thephoneclassesareunevenlydistributed,soweassignedvariablenumberofGaussianmixturesforeachclassbycontrollingtheaverageddatacountspermixture.Forallexperiments,theinitialmodelisanMLEmodeltrainedwithlabeleddataonly.Toconstructamixedlabeled/unlabeleddataset,theoriginaltrainingsetwererandomlydividedintothelabeledandunlabeledsetswithdesiredratio,andtheclasslabelsintheunlabeledsetareassumedtobeunknown.Toavoidthattheclassiﬁerperformancemayvarywithparticularportionsofdata,weranﬁvefoldsforeveryexperiment,eachfoldcorrespondingtodifferentdivisionoftrainingdataintolabeledandunlabeledset,andtooktheaveragedperformance.3.1ModelComplexityThissectionanalyzesthelearningcapabilityofsemi-supervisedlearningalgorithmsfordifferentmodelcomplexities,thatis,thenumberofmix-turesforGaussianmixturemodel.Inthisexperi-ment,thesizesoflabeledandunlabeledsetareﬁxed(|DL|:|DU|=1:10andtheaveragedtokencountsperclassisaround140forbothdatasets),aswevariedthetotalnumberofmixturesandeval-uatedtheupdatedmodelbyitsclassiﬁcationaccu-racy.Forwaveform,numberofmixtureswassetfrom2to7;forTIMIT,becausethenumberofmix-turesperclassisdeterminedbytheaverageddatacountspermixturec,wesetcto25,20and15asthehighercgiveslessnumberofmixturesintotal.Figure3.1plotstheaveragedclassiﬁcationaccura-79

Figure1:Meanclassiﬁcationaccuraciesvs.αfordifferentmodelcomplexity.TheaccuraciesfortheinitialMLEmodelsareindicatedintheparentheses.(a)waveform:trainingwiththehybridobjective.(b)waveform:purelygenerativeobjective.(c)TIMIT:trainingwiththehybridobjective.(d)TIMIT:purelygenerativeobjective.0.00.51.01.52.079808182838485(d) Accuracy (%) m = 2 (83.02%) m = 3 (82.08%) m = 4 (81.56%) m = 5 (80.18%) m = 6 (79.61%)(a)0.000.050.100.150.205354555657(c)  Accuracy (%) c = 25 (55.34%) c = 20 (55.36%) c  = 15 (54.72%)0.00.51.01.52.079808182838485(b) Accuracy (%)   m = 2 (83.02%) m = 3 (82.08%) m = 4 (81.56%) m = 5 (80.18%) m = 6 (79.61%)0.000.050.100.150.205354555657   Accuracy (%) c = 25 (55.34%) c = 20 (55.36%) c = 15 (54.72%)ciesoftheupdatedmodelversusthevalueofαwithdifferentmodelcomplexities.TherangesofαaredifferentforwaveformandTIMITbecausethevalueofαforeachdatasethasdifferentscales.Firstofall,thehybridmethodandpurelygen-erativemethodhaveverysimilarbehaviorsinbothwaveformandTIMIT;thedifferencesbetweenthetwomethodsareinsigniﬁcantregardlessofα.Thehybridmethodwithα=0meanssupervisedMMI-trainingwithlabeleddataonly,andthepurelygener-ativemethodwithα=0meansextraseveralroundsofsupervisedMLE-trainingiftheconvergencecri-terionisnotachieved.Withthesmallamountofla-beleddata,mostofhybridcurvesstartslightlylowerthanthepurelygenerativeonesatα=0,butin-creasetoashighasthepurelygenerativeonesasαincreases.Forwaveform,theaccuraciesincreasewithαin-creasesforallcasesexceptforthe2-mixturemodel.Table1summarizesthenumbersfromFigure3.1.Exceptforthe2-mixturecase,theimprovementoverthesupervisedmodel(α=0)ispositivelycorre-latedtothemodelcomplexity,asthelargestim-provementsoccuratthe5-mixtureand6-mixturemodelforthehybridandpurelygenerativemethodrespectively.However,thehighestcomplexitydoesnotnecessarilygivesthebestclassiﬁcationaccu-racy;the3-mixturemodelachievesthebestaccu-racyamongallmodelsaftersemi-supervisedlearn-ingwhereasthe2-mixturemodelisthebestmodelforsupervisedlearningusinglabeleddataonly.ExperimentsonTIMITshowasimilarbehavior1;asshowninbothFigure3.1andTable2,theim-provementoverthesupervisedmodel(α=0)isalsopositivelycorrelatedtothemodelcomplexity,1Notethatourbaselineperformance(theinitialMLEmodel)ismuchworsethanbenchmarkbecauseonly10%ofthetrain-ingdatawereused.Wejustiﬁedourbaselinemodelbyusingthewholetrainingdataandasimilaraccuracy(74%)tootherwork(e.g.(ShaandSaul,2007))wasobtained.80

Table1:Theaccuracies(%)oftheinitialMLEmodel,thesupervisedmodel(α=0),thebestaccuracieswithunlabeleddataandtheabsoluteimprovements(∆)overα=0fordifferentmodelcomplexitiesforwaveform.Theboldednumberisthehighestvaluealongthesamecolumn.HybridPurelygenerative#.mixinit.acc.α=0bestacc.∆α=0bestacc.∆283.0281.7383.742.0182.9683.140.18382.0881.6684.693.0382.1884.582.40481.5680.5383.933.4081.3484.132.79580.1880.1483.823.6880.1683.843.68679.6179.4083.193.7979.7183.313.60Table2:Theaccuracies(%)oftheinitialMLEmodel,thesupervisedmodel(α=0),thebestaccuracieswithunlabeleddataandtheabsoluteimprovements(∆)overα=0fordifferentmodelcomplexitiesforTIMIT.Theboldednumberisthehighestvaluealongthesamecolumn.HybridPurelygenerativecinit.acc.α=0bestacc.∆α=0bestacc.∆2555.3455.4756.581.1155.3256.71.382055.3655.6756.721.0555.256.251.051554.7253.7155.391.6853.756.092.39asthemostimprovementsoccuratc=25forbothhybridandpurelygenerativemethods.Thesemi-supervisedmodelconsistentlyimprovesoverthesu-pervisedmodel.Tosummarize,unlabeleddataim-provetrainingonmodelsofhighercomplexity,andsometimesithelpsachievethebestperformancewithamorecomplexmodel.3.2SizeofUnlabeledDataInFigure2,weﬁxedthesizeofthelabeledset(4%ofthetrainingset)andplottedtheaveragedclassi-ﬁcationaccuraciesforlearningwithdifferentsizesofunlabeleddata.Firstofall,thehybridmethodandpurelygenerativemethodstillbehavesimilarlyinbothwaveformandTIMIT.Forbothdatasets,theﬁguresclearlyillustratethatmoreunlabeleddatacontributesmoreimprovementoverthesupervisedmodelregardlessofthevalueofα.Generally,adatadistributioncanbeexpectedmorepreciselywithalargersamplesizefromthedatapool,thereforeweexpectthemoreunlabeleddatathemoreprecisein-formationaboutthepopulation,whichimprovesthelearningcapability.3.3DiscussionofαDuringtraining,theweightedsumofFMMIandFMLinequation(15)increaseswithiterations,howeverFMMIandFMLarenotguaranteedtoincreaseindi-vidually.Figure3illustrateshowαaffectsthere-spectivechangeofthetwocomponentsforapartic-ularsettingforwaveform.Whenα=0,theob-jectivefunctiondoesnottakeunlabeleddataintoaccount,soFMMIincreaseswhileFMLdecreases.FMLstartstoincreasefornonzeroα;α=0.01correspondstothecasewherebothobjectivesin-creases.Asαkeepsgrowing,FMMIstartstode-creasewhereasFMLkeepsrising.Inthispartic-ularexample,α=0.05isthecriticalvalueatwhichFMMIchangesfromincreasingtodecreas-ing.Accordingtoourobservation,thevalueofαdependsonthedatasetandtherelativesizeofla-beled/unlabeleddata.Table3showsthecriticalval-uesforwaveformandTIMITfordifferentsizesoflabeleddata(5,10,15,20%ofthetrainingset)withaﬁxedsetofunlabeleddata(80%.)Thenumbersareverydifferentacrossthedatasets,butthereisacon-sistentpatternwithinthedataset–thecriticalvalueincreasesasthesizeoflabeledsetincreases.Onepossibleexplanationisthatαcontainsannormal-81

Figure2:Meanclassiﬁcationaccuraciesvs.αfordifferentamountsofunlabeleddata(thepercentageinthetrainingset).TheaveragedaccuracyfortheinitialMLEmodelis81.66%forwaveformand59.41%forTIMIT.(a)waveform:trainingwiththehybridobjective.(b)waveform:purelygenerativeobjective.(c)TIMIT:trainingwiththehybridobjective.(d)TIMIT:purelygenerativeobjective.0.00.51.01.52.081828384  Accuracy (%) u = 20% u = 40% u = 60% u = 80%0.000.050.1059.459.659.860.060.2  Accuracy (%) u = 10% u = 50% u = 80%0.00.51.01.52.0828384Accuracy (%)    u = 20% u = 40% u = 60% u = 80%0.000.050.1059.459.659.860.0   Accuracy (%) u = 10% u = 50% u = 80%(a)(b)(c)(d)izationfactorwithrespecttotherelativesizeofla-beled/unlabeledset.Theobjectivefunctionin(15)canberewrittenintermsofthenormalizedobjectivewithrespecttothedatasize:F(λ)=|DL|F(DL)MMI(λ)+α|DU|F(DU)ML(λ).(15)whereF(X)meanstheaveragedvalueoverthedatasetX.Whenthelabeledsetsizeincreases,αmayhavetoscaleupaccordinglysuchthattherelativechangeofthetwoaveragedcomponentremainsinthesamescale.Althoughαcontrolsthedominanceofthecrite-riononlabeleddataoronunlabeleddata,thefactthatwhichdominatestheobjectiveorthecriticalvaluedoesnotnecessaryindicatethebestα.How-ever,weobservedthatthebestαisusuallyclosetoorlargerthanthecriticalvalue,buttheexactvaluevarieswithdifferentdata.Atthispoint,itmightstillbeeasiertoﬁndthebestweightusingasmallde-velopmentset.Butthisobservationalsoprovidesaguideaboutthereasonablerangetosearchthebestα–searchingstartingfromthecriticalvalueanditshouldreachtheoptimalvaluesoonaccordingtotheplotsinFigure3.1.Table3:ThecriticalvaluesforwaveformandTIMITfordifferentsizesoflabeleddata(percentageoftrainingdata)withaﬁxedsetofunlabeleddata(80%.)SizeoflabeleddatawaveformTIMIT5%0.09-0.110.03-0.0410%0.12-0.140.07-0.0815%0.5-0.60.08-0.0920%1-1.50.11-0.1282

Figure3:Accuracy(left),FMMI(center),andFML(right)atdifferentvaluesofalpha.012345670.770.780.790.800.810.820.830.84 Accuracy (%)Iteration Number  = 0  = 0.01  = 0.05  = 0.501234567-30-20-100 F_MMIIteration Number  = 0  = 0.01  = 0.05  = 0.501234567-198-196-194-192-190-188 F_ML / 1000Iteration Number  = 0  = 0.01  = 0.05  = 0.53.4HybridCriterionvs.PurelyGenerativeCriterionFromthepreviousexperiments,wefoundthatthehybridcriterionandpurelygenerativecriterional-mostmatcheachotherinperformanceandareabletolearnmodelsofthesamecomplexity.Thisimpliesthatthecriteriononlabeleddatahaslessimpactontheoveralltrainingdirectionthanunlabeleddata.InSection3.2,wementionedthatthebestαisusuallylargerthanorclosetothecriticalvaluearoundwhichtheunlabeleddatalikelihoodtendstodominatethetrainingobjective.Thisagainsuggeststhatlabeleddatacontributelesstothetrainingobjectivefunctioncomparedtounlabeleddata,andthecriteriononla-beleddatadoesn’tmatterasmuchasthecriteriononunlabeleddata.Itispossiblethatmostofthecon-tributionsfromlabeleddatahavealreadybeenusedfortraininganinitialMLEmodel,thereforelittlein-formationcouldbeextractedinthefurthertrainingprocess.4ConclusionRegardlessofthedatasetandthetrainingobjectivetypeonlabeleddata,therearesomegeneralprop-ertiesaboutthesemi-supervisedlearningalgorithmsstudiedinthiswork.First,whilelimitedamountoflabeleddatacanatmosttrainmodelsoflowercom-plexitywell,theadditionofunlabeleddatamakestheupdatedmodelsofhighercomplexitymuchim-provedandsometimesperformbetterthanlesscom-plexmodels.Second,theamountofunlabeleddatainoursemi-supervisedframeworkgenerallyfollows‘the-more-the-better’principle;thereisatrendthatmoreunlabeleddataresultsinmoreimprovementinclassiﬁcationaccuracyoverthesupervisedmodel.Wealsofoundthattheobjectivetypeonlabeleddatahaslittleimpactontheupdatedmodel,inthesensethathybridandpurelygenerativeobjectivesbehavesimilarlyinlearningcapability.Theobser-vationthatthebestαoccursaftertheMMIcriterionbeginstodecreasesupportsthefactthatthecriteriononlabeleddatacontributeslessthanthecriteriononunlabeleddata.Thisobservationisalsohelpfulindeterminingthesearchrangeforthebestαonthedevelopmentsetbylocatingthecriticalvalueoftheobjectiveasastartpointtoperformsearch.Theuniﬁedtrainingobjectivemethodhasaniceconvergencepropertywhichself-trainingmethodscannotguarantee.ThenextstepistoextendthesimilarframeworktospeechrecognitiontaskwhereHMMsaretrainedandphoneboundariesareseg-mented.Itwouldbeinterestingtocompareitwithself-trainingmethodsindifferentaspects(e.g.per-formance,reliability,stabilityandcomputationalef-ﬁciency).83

ReferencesA.AsuncionandD.J.Newman.2007.UCImachinelearningrepository.GregoryDruck,ChrisPal,AndrewMcCallum,andXiao-jinZhu.2007.Semi-supervisedclassiﬁcationwithhy-bridgenerative/discriminativemethods.InKDD’07:Proceedingsofthe13thACMSIGKDDinternationalconferenceonKnowledgediscoveryanddatamining,pages280–289,NewYork,NY,USA.ACM.J.S.Garofolo,L.F.Lamel,W.M.Fisher,J.G.Fiscus,D.S.Pallett,andN.L.Dahlgren.1993.Darpatimitacousticphoneticcontinuousspeechcorpus.AndrewK.Halberstadt.1998.HeterogeneousAcous-ticMeasurementsandMultipleClassiﬁersforSpeechRecognition.Ph.D.thesis,MassachusettsInstituteofTechnology.J.-T.HuangandMarkHasegawa-Johnson.2008.Max-imummutualinformationestimationwithunlabeleddataforphoneticclassiﬁcation.InInterspeech.MasashiInoueandNaonoriUeda.2003.Exploitationofunlabeledsequencesinhiddenmarkovmodels.IEEETrans.OnPatternAnalysisandMachineIntelligence,25:1570–1581.ShihaoJi,LayneT.Watson,andLawrenceCarin.2009.Semisupervisedlearningofhiddenmarkovmodelsviaahomotopymethod.IEEETrans.PatternAnal.Mach.Intell.,31(2):275–287.M.J.F.GalesL.WangandP.C.Woodland.2007.Un-supervisedtrainingformandarinbroadcastnewsandconversationtranscription.InProc.IEEEConfer-enceonAcoustics,Speech,andSignalProcessing(ICASSP),volume4,pages353–356.LoriLamel,Jean-LucGauvain,andGillesAdda.2002.Lightlysupervisedandunsupervisedacousticmodeltraining.16:115–129.K.-F.LeeandH.-W.Hon.1989.Speaker-independentphonerecognitionusinghiddenmarkovmodels.IEEETransactionsonSpeechandAudioProcessing,37(11):1641–1648.B.Merialdo.1988.Phoneticrecognitionusinghid-denmarkovmodelsandmaximummutualinformationtraining.InProc.IEEEConferenceonAcoustics,Speech,andSignalProcessing(ICASSP),volume1,pages111–114.DanielPovey.2003.DiscriminativeTrainingforLargeVocabularySpeechRecognition.Ph.D.thesis,Cam-bridgeUniversity.FeiShaandLawrenceK.Saul.2007.Largemarginhid-denmarkovmodelsforautomaticspeechrecognition.InB.Sch¨olkopf,J.Platt,andT.Hoffman,editors,Ad-vancesinNeuralInformationProcessingSystems19,pages1249–1256.MITPress,Cambridge,MA.FrankWesselandHermannNey.2005.Unsupervisedtrainingofacousticmodelsforlargevocabularycon-tinuousspeechrecognition.IEEETransactionsonSpeechandAudioProcessing,13(1):23–31,January.Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 84–85,

Boulder, Colorado, June 2009. c(cid:13)2009 Association for Computational Linguistics

84

DiscriminativeModelsforSemi-SupervisedNaturalLanguageLearningSajibDasguptaandVincentNgHumanLanguageTechnologyResearchInstituteUniversityofTexasatDallasRichardson,TX75083-0688{sajib,vince}@hlt.utdallas.edu1Discriminativevs.GenerativeModelsAninterestingquestionsurroundingsemi-supervisedlearningforNLPis:shouldweusediscriminativemodelsorgenerativemodels?De-spitethefactthatgenerativemodelshavebeenfrequentlyemployedinasemi-supervisedsettingsincetheearlydaysofthestatisticalrevolutioninNLP,weadvocatetheuseofdiscriminativemodels.Theabilityofdiscriminativemodelstohandlecomplex,high-dimensionalfeaturespacesandtheirstrongtheoreticalguaranteeshavemadethemaveryappealingalternativetotheirgen-erativecounterparts.Perhapsmoreimportantly,discriminativemodelshavebeenshowntooffercompetitiveperformanceonavarietyofsequentialandstructuredlearningtasksinNLPthataretraditionallytackledviagenerativemodels,suchasletter-to-phonemeconversion(Jiampojamarnetal.,2008),semanticrolelabeling(Toutanovaetal.,2005),syntacticparsing(Taskaretal.,2004),languagemodeling(Roarketal.,2004),andmachinetranslation(Liangetal.,2006).Whilegenerativemodelsallowtheseamlessintegrationofpriorknowledge,discriminativemodelsseemtooutperformgenerativemodelsina“noprior”,agnosticlearningsetting.SeeNgandJordan(2002)andToutanova(2006)forinsightfulcomparisonsofgenerativeanddiscriminativemodels.2DiscriminativeEM?Anumberofsemi-supervisedlearningsystemscanbootstrapfromsmallamountsoflabeleddatausingdiscriminativelearners,includingself-training,co-training(BlumandMitchell,1998),andtransduc-tiveSVM(Joachims,1999).However,noneofthemseemstooutperformtheothersacrossdifferentdo-mains,andeachhasitsprosandcons.Self-trainingcanbeusedincombinationwithanydiscriminativelearningmodel,butitdoesnottakeintoaccounttheconﬁdenceassociatedwiththelabelofeachdatapoint,forinstance,byplacingmoreweightonthe(perfectlylabeled)seedsthanonthe(presumablynoisilylabeled)bootstrappeddataduringthelearn-ingprocess.Co-trainingisanaturalchoiceifthedatapossessestwoindependent,redundantfeaturesplits.However,thisconditionalindependenceas-sumptionisafairlystrictassumptionandcanrarelybesatisﬁedinpractice;worsestill,itistypicallynoteasytodeterminetheextenttowhichadatasetsat-isﬁesthisassumption.TransductiveSVMtendstolearnbettermax-marginhyperplaneswiththeuseofunlabeleddata,butitsoptimizationprocedureisnon-trivialanditsperformancetendstodeteriorateifasufﬁcientlylargeamountofunlabeleddataisused.Recently,BrefeldandScheffer(2004)havepro-posedanewsemi-supervisedlearningtechnique,EM-SVM,whichisinterestinginthatitincorpo-ratesadiscriminativemodelinanEMsetting.Un-likeself-training,EM-SVMtakesintoaccounttheconﬁdenceofthenewlabels,ensuringthatthein-stancesthatarelabeledwithlessconﬁdencebytheSVMhavelessimpactonthetrainingprocessthantheconﬁdently-labeledinstances.Sofar,EM-SVMhasbeentestedontextclassiﬁcationproblems,out-performingtransductiveSVM.Itwouldbeinterest-ingtoseewhetherEM-SVMcanbeatexistingsemi-supervisedlearnersforotherNLPtasks.85

3EffectivenessofBootstrappingHoweffectivearetheaforementionedsemi-supervisedlearningsystemsinbootstrappingfromsmallamountsoflabeleddata?Whiletherearequiteafewsuccessstoriesreportingconsiderableperfor-mancegainsoveraninductivebaseline(e.g.,parsing(McCloskyetal.,2008),coreferenceresolution(NgandCardie,2003),andmachinetranslation(Ueff-ingetal.,2007)),therearenegativeresultstoo(seePierceandCardie(2001),HeandGildea(2006),DuhandKirchhoff(2006)).Bootstrappingperfor-mancecanbesensitivetothesettingoftheparam-etersofthesesemi-supervisedlearners(e.g.,whentostop,howmanyinstancestobeaddedtothela-beleddataineachiteration).Todate,however,re-searchershavereliedonvariousheuristicsforpa-rameterselection,butwhatweneedisaprincipledmethodforaddressingthisproblem.Recently,Mc-Closkyetal.(2008)havecharacterizedthecondi-tionsunderwhichself-trainingwouldbeeffectiveforsemi-supervisedsyntacticparsing.WebelievethattheNLPcommunityneedstoperformmorere-searchofthiskind,whichfocusesonidentifyingthealgorithm(s)thatachievegoodperformanceunderagivensetting(e.g.,fewinitialseeds,largeamountsofunlabeleddata,complexfeaturespace,skewedclassdistributions).4DomainAdaptationDomainadaptationhasrecentlybecomeapopularresearchtopicintheNLPcommunity.Labeleddataforonedomainmightbeusedtotrainainitialclassi-ﬁerforanother(possiblyrelated)domain,andthenbootstrappingcanbeemployedtolearnnewknowl-edgefromthenewdomain(Blitzeretal.,2007).Itwouldbeinterestingtoseeifwecancomeupwithasimilarsemi-supervisedlearningmodelforpro-jectingresourcesfromaresource-richlanguagetoaresource-scarcelanguage.ReferencesJohnBlitzer,MarkDredze,andFernandoPereira.2007.Biographies,bollywood,boom-boxesandblenders:Domainadaptationforsentimentclassiﬁcation.InProceedingsoftheACL.AvrimBlumandTomMitchell.1998.Combiningla-beledandunlabeleddatawithco-training.InProceed-ingsofCOLT.UlfBrefeldandTobiasScheffer.2004.Co-EMsupportvectorlearning.InProceedingsofICML.KevinDuhandKatrinKirchhoff.2006.Lexiconacqui-sitionfordialectalArabicusingtransductivelearning.InProceedingsofEMNLP.ShanHeandDanielGildea.2006.Self-trainingandco-trainingforsemanticrolelabeling.SittichaiJiampojamarn,ColinCherry,andGrzegorzKondrak.2008.Jointprocessinganddiscriminativetrainingforletter-to-phonemeconversion.InProceed-ingsofACL-08:HLT.ThorstenJoachims.1999.Transductiveinferencefortextclassiﬁcationusingsupportvectormachines.InProceedingsofICML.PercyLiang,AlexandreBouchard,DanKlein,andBenTaskar.2006.Anend-to-enddiscriminativeapproachtomachinetranslation.InProceedingsoftheACL.DavidMcClosky,EugeneCharniak,andMarkJohnson.2008.Whenisself-trainingeffectiveforparsing?InProceedingsofCOLING.VincentNgandClaireCardie.2003.Weaklysupervisednaturallanguagelearningwithoutredundantviews.InProceedingsofHLT-NAACL.AndrewNgandMichaelJordan.2002.Ondiscrimina-tivevs.generativeclassiﬁers:AcomparisonoflogisticregressionandNaiveBayes.InAdvancesinNIPS.DavidPierceandClaireCardie.2001.Limitationsofco-trainingfornaturallanguagelearningfromlargedatasets.InProceedingsofEMNLP.BrianRoark,MuratSaraclar,MichaelCollins,andMarkJohnson.2004.Discriminativelanguagemodelingwithconditionalrandomﬁeldsandtheperceptronal-gorithm.InProceedingsoftheACL.BenTaskar,DanKlein,MichaelCollins,DaphneKoller,andChristopherManning.2004.Max-marginpars-ing.InProceedingsofEMNLP.KristinaToutanova,AriaHaghighi,,andChristopherD.Manning.2005.Jointlearningimprovessemanticrolelabeling.InProceedingsoftheACL.KristinaToutanova.2006.Competitivegenerativemod-elswithstructurelearningforNLPclassiﬁcationtasks.InProceedingsofEMNLP.NicolaUefﬁng,GholamrezaHaffari,andAnoopSarkar.2007.Transductivelearningforstatisticalmachinetranslation.InProceedingsoftheACL.Author Index

Andrzejewski, David, 43

Betteridge, Justin, 1

Carlson, Andrew, 1
Chen, Zheng, 66

Dasgupta, Sajib, 84

Fresno, V´ıctor, 28

Goldberg, Andrew B., 19

Hasegawa-Johnson, Mark, 75
Hruschka Junior, Estevam Rafael, 1
Huang, Jui-Ting, 75

Ji, Heng, 66

Kondadadi, Ravi Kumar, 10

Liao, Wenhui, 58

Mart´ınez, Raquel, 28
Mitchell, Tom M., 1

Ng, Vincent, 84

Plank, Barbara, 37
Poveda, Jordi, 49

Surdeanu, Mihai, 49

Turmo, Jordi, 49

Veeramachaneni, Sriharsha, 10, 58

Zhu, Xiaojin, 19, 43
Zubiaga, Arkaitz, 28

87

