Stick-breaking Construction for the Indian Buffet Process

Yee Whye Teh

Dilan G¨or¨ur

Gatsby Computational Neuroscience Unit

MPI for Biological Cybernetics

University College London

17 Queen Square

London WC1N 3AR, UK
ywteh@gatsby.ucl.ac.uk

Dept. Sch¨olkopf
Spemannstrasse 38

72076 T¨ubingen, Germany

dilan.gorur@tuebingen.mpg.de

Zoubin Ghahramani

Department of Engineering
University of Cambridge

Trumpington Street

Cambridge CB2 1PZ, UK
zoubin@eng.cam.ac.uk

Abstract

The Indian buffet process (IBP) is a Bayesian
nonparametric distribution whereby objects are
modelled using an unbounded number of latent
features. In this paper we derive a stick-breaking
representation for the IBP. Based on this new rep-
resentation, we develop slice samplers for the
IBP that are efﬁcient, easy to implement and
are more generally applicable than the currently
available Gibbs sampler. This representation,
along with the work of Thibaux and Jordan [17],
also illuminates interesting theoretical connec-
tions between the IBP, Chinese restaurant pro-
cesses, Beta processes and Dirichlet processes.

1 INTRODUCTION

The Indian Buffet Process (IBP) is a distribution over bi-
nary matrices consisting of N > 0 rows and an unbounded
number of columns [6]. These binary matrices can be inter-
preted as follows: each row corresponds to an object, each
column to a feature, and a 1 in entry (i, k) indicates object
i has feature k. For example, objects can be movies like
“Terminator 2”, “Shrek” and “Shanghai Knights”, while
features can be “action”, “comedy”, “stars Jackie Chan”,
and the matrix can be [101; 010; 110] in Matlab notation.

Like the Chinese Restaurant Process (CRP) [1], the IBP
provides a tool for deﬁning nonparametric Bayesian mod-
els with latent variables. However, unlike the CRP, in
which each object belongs to one and only one of inﬁnitely
many latent classes, the IBP allows each object to possess
potentially any combination of inﬁnitely many latent fea-
tures. This added ﬂexibility has resulted in a great deal of
interest in the IBP, and the development of a range of inter-
esting applications. These applications include models for
choice behaviour [5], protein-protein interactions [2], the
structure of causal graphs [19], dyadic data for collabora-
tive ﬁltering applications [10], and human similarity judg-
ments [11].

In this paper, we derive a new, stick-breaking represen-
tation for the IBP, a development which is analogous
to Sethuraman’s seminal stick-breaking representation for
CRPs [15]. In this representation, as we will see in Sec-
tion 3, the probability of each feature is represented explic-
itly by a stick of length between 0 and 1. Sethuraman’s
representation paved the way for both novel samplers for
and generalizations of CRPs [7]. Similarly, we show how
our novel stick-breaking representation of the IBP can be
used to develop new slice samplers for IBPs that are ef-
ﬁcient, easy to implement and have better applicability to
non-conjugate models (Sections 4, 5.2, 6). This new repre-
sentation also suggests generalizations of the IBP (such as
a Pitman-Yor variant, in Section 3.2). Moreover, although
our stick-breaking representation of the IBP was derived
from a very different model than the CRP, we demonstrate
a surprising duality between the sticks in these two repre-
sentations which suggests deeper connections between the
two models (Section 3.2). The theoretical developments we
describe here, which show a stick-breaking representation
which is to the IBP what Sethuraman’s construction is to
the CRP, along with the recent work of Thibaux and Jordan
[17], showing that a particular subclass of Beta processes
is to the IBP as the Dirichlet process is to the CRP, ﬁrmly
establish the IBP in relation to the well-known classes of
Bayesian nonparametric models.

2 INDIAN BUFFET PROCESSES

The IBP is deﬁned as the limit of a corresponding distri-
bution over matrices with K columns, as the number of
columns K → ∞. Let Z be a random binary N × K ma-
trix, and denote entry (i, k) in Z by zik. For each feature k
let µk be the prior probability that feature k is present in an
object. We place a Beta( α
K , 1) prior on µk, with α being
the strength parameter of the IBP. The full model is:

µk ∼ Beta( α

K , 1)

zik|µk ∼ Bernoulli(µk)

indepedently ∀k
indepedently ∀i, k

(1a)
(1b)

Let us now consider integrating out the µk’s and taking the

limit of K → ∞ to obtain the IBP. For the ﬁrst object,
the chance of it having each particular feature k is inde-
K once µk is integrated out, thus the distribu-
pendently α
tion over the number of features it has is Binomial( α
K , K).
As K → ∞, this approaches Poisson(α). For subse-
quent objects i = 2, . . . , N, the probability of it also
having a feature k already belonging to a previous object
is
is the number of objects prior to i with feature k. Re-
peating the argument for the ﬁrst object, object i will
also have Poisson( α
i ) new features not belonging to pre-
vious objects. Note that even though the total number of
available features is unbounded, the actual number K + of
used features is always ﬁnite (and in fact is distributed as

i where m<i k = Pj<i zjk > 0

α
K +m<i k
K +1+i−1 → m<i k
α

Poisson(αPN

i=1

1
i )).

The above generative process can be understood using the
metaphor of an Indian buffet restaurant. Customers (ob-
jects) come into the restaurant one at a time, and can sample
an inﬁnite number of dishes (features) at the buffet counter.
Each customer will try each dish that previous customers
have tried with probabilities proportional to how popular
each dish is; in addition the customer will try a number of
new dishes that others have not tried before.

To complete the model, let θk be parameters associated
with feature k and xi be an observation associated with ob-
ject i. Let

θk ∼ H
xi ∼ F (zi,:, θ:)

independently ∀k
independently ∀i

(2a)
(2b)

where H is the prior over parameters, F (zi,:, θ:) is the data
k=1 correspond-
distribution given the features zi,: = {zik}∞
ing to object i and feature parameters θ: = {θk}∞
k=1. We
assume that F (zi,:, θ:) depends only on the parameters of
the present features.

2.1 GIBBS SAMPLER

The above generative process for the IBP can be used di-
rectly in a Gibbs sampler for posterior inference of Z and θ
given data x = {xi} [6]. The representation consists of the
number K + of used (active) features, the matrix Z1:N,1:K+
of occurrences among the K + active features, and their pa-
rameters θ1:K+. The superscript + denotes active features.
The sampler iterates through i = 1, . . . , N, for each object
i it updates the feature occurrences for the currently used
features, then considers adding new features to model the
data xi.

For the already used features k = 1, . . . , K +, the condi-
tional probability of zik = 1 given other variables is just

p(zik = 1|rest) ∝ m¬i k

N f (xi|zi,¬k, zik = 1, θ1:K+)

(3)

where m¬i k = Pj6=i zjk. The fraction is the conditional

prior of zik = 1, obtained by using exchangeability among

the customers and taking customer i to be the last customer
to enter the restaurant; the second term f (·|·) is the data
likelihood of xi if zik = 1. It is possible to integrate θ1:K+
out from the likelihood term if H is conjugate to F . In fact
it is important for H to be conjugate to F when we consider
the probabilities of new features being introduced, because
all possible parameters for these new features have to be
taken into account.
If Li is the number of new features
introduced, we have

N

N )Li e− α
p(Li|rest) ∝ ( α
Z f (xi|zi,1:K+ , z◦

Li!

×

i,1:Li = 1, θ1:K+, θ◦

1:Li) dh(θ◦

1:Li)

(4)

i,1:Li

are occurrences for the new features and
where z◦
are their parameters, the superscript ◦ denoting cur-
θ◦
1:Li
rently unused (inactive) features. The fraction comes
from the probability of introducing Li new features under
N ) while the second term is the data likelihood,
Poisson( α
with the parameters θ◦
integrated out with respect to the
prior density h(·).

1:Li

The need to integrate out the parameters for new features
is similar to the need to integrate out parameters for new
clusters in the Dirichlet process (DP) mixture model case
(see [13]). To perform this integration efﬁciently, conju-
gacy is important, but the requirement for conjugacy limits
the applicability of the IBP in more elaborate settings. It is
possible to devise samplers in the non-conjugate case anal-
ogous to those developed for DP mixture models [10, 5].
In the next section we develop a different representation of
the IBP in terms of a stick-breaking construction, which
leads us to an easy to implement slice sampler for the non-
conjugate case.

3 STICK BREAKING CONSTRUCTION

In this section, we describe an alternative representation of
the IBP where the feature probabilities are not integrated
out, and a speciﬁc ordering is imposed on the features.
We call this the stick-breaking construction for the IBP.
We will see that the new construction bears strong rela-
tionships with the standard stick-breaking construction for
CRPs, paving the way to novel generalizations of and in-
ference techniques for the IBP.

3.1 DERIVATION

Let µ(1) > µ(2) > . . . > µ(K) be a decreasing ordering
of µ1:K = {µ1, . . . , µK}, where each µl is Beta( α
K , 1).
We will show that in the limit K → ∞ the µ(k)’s obey the
following law, which we shall refer to as the stick-breaking
construction for the IBP,

ν(k)

i.i.d.
∼ Beta(α, 1) µ(k) = ν(k)µ(k−1) =

k

Yl=1

ν(l)

(5)

We start by considering µ(1). For ﬁnite K it is

µ(1) = max

l=1,...,K

µl

where each µl is Beta( α

K , 1) and has density:

p(µl) = α

α
K −1
K µ
l

I(0 ≤ µl ≤ 1)

(6)

(7)

where I(A) is the indicator function for a condition (mea-
surable set) A: I(A) = 1 if A is true, and 0 otherwise. The
cumulative distribution function (cdf) for µl is then:

F (µl) = Z µl

−∞
α
= µ
K
l

α

K t α

K −1I(0 ≤ t ≤ 1) dt

I(0 ≤ µl ≤ 1) + I(1 < µl)

(8)

Since the µl’s are independent, the cdf of µ(1) is just the
product of the cdfs of each µl, so

F (µ(1)) = (cid:16)µ

α
K
(1)

I(0 ≤ µ(1) ≤ 1) + I(1 < µ(1) < ∞)(cid:17)K

= µα
(1)

I(0 ≤ µ(1) ≤ 1) + I(1 < µ(1))

(9)

Differentiating, we see that the density of µ(1) is

p(µ(1)) = αµα−1
(1)

I(0 ≤ µ(1) ≤ 1)

(10)

and therefore µ(1) ∼ Beta(α, 1).
We now derive the densities for subsequent µ(k)’s. For
each k ≥ 1 let lk be such that µlk = µ(k) and let Lk =
{1, . . . , K}\{l1, . . . , lk}. Since µ(1:k) = {µ(1), . . . , µ(k)}
are the k largest values among µ1:K, we have

µl ≤ min
k0≤k

µ(k0) = µ(k)

(11)

for each l ∈ Lk. Restricting the range of µl to [0, µ(k)], the
cdf becomes

F (µl|µ(1:k)) = R µl
R µ(k)

0

0

=µ

− α
α
(k) µ
K
K
l

α

K t α
K t α

K −1 dt
K −1 dt

α

I(0 ≤ µl ≤ µ(k)) + I(µ(k) < µl)

(12)

Now µ(k+1) = maxl∈Lk µl with each µl independent given
µ(1:k). The cdf of µ(k+1) is again the product of the cdfs of
µl over l ∈ Lk,

F (µ(k+1)|µ(1:k))

(13)

K α

=µ

− K−k
(k)
→µ−α

(k) µα

(k+1)

K−k

µ

I(0 ≤ µ(k+1) ≤ µ(k)) + I(µ(k) < µ(k+1))

K α
(k+1)
I(0 ≤ µ(k+1) ≤ µ(k)) + I(µ(k) < µ(k+1))

as K → ∞. Differentiating, the density of µ(k+1) is,

p(µ(k+1)|µ(1:k))

=αµ−α

(k) µα−1

(k+1)

I(0 ≤ µ(k+1) ≤ µ(k))

(14)

Notice that the µ(k)’s have a Markov structure, with µ(k+1)
conditionally independent of µ(1:k−1) given µ(k).
Finally, instead of working with the variables µ(k) directly,
we introduce a new set of variables ν(k) = µ(k)
with
µ(k−1)
range [0, 1]. Using a change of variables, the density of
ν(k) is derived to be,

p(ν(k)|µ(1:k−1)) = ανα−1
(k)

I(0 ≤ ν(k) ≤ 1)

(15)

Thus ν(k) are independent from µ(1:k−1) and are simply
Beta(α, 1) distributed. Expanding µ(k) = ν(k)µ(k−1) =

Qk
l=1 ν(l), we obtain the stick-breaking construction (5).

The construction (5) can be understood metaphorically as
follows. We start with a stick of length 1. At iteration
k = 1, 2, . . ., we break off a piece at a point ν(k) relative
to the current length of the stick. We record the length µ(k)
of the stick we just broke off, and recurse on this piece,
discarding the other piece of stick.

3.2 RELATION TO DP

In iteration k of the construction (5), after breaking the stick
in two we always recurse on the stick whose length we de-
note by µ(k). Let π(k) be the length of the other discarded
stick. We have,

π(k) = (1 − ν(k))µ(k−1) = (1 − ν(k))

Yl=1
Making a change of variables v(k) = 1 − ν(k),

k−1

ν(l)

(16)

v(k)

i.i.d.
∼ Beta(1, α)

π(k) = v(k)

k−1

Yl=1

(1 − v(l))

(17)

thus π(1:∞) are the resulting stick lengths in a standard
stick-breaking construction for DPs [15, 7].

In both constructions the ﬁnal weights of interest are the
lengths of the sticks.
In DPs, the weights π(k) are the
lengths of sticks discarded, while in IBPs, the weights µ(k)
are the lengths of sticks we have left. This difference leads
to the different properties of the weights: for DPs, the stick
lengths sum to a length of 1 and are not decreasing, while in
IBPs the stick lengths need not sum to 1 but are decreasing.
Both stick-breaking constructions are shown in Figure 1.
In both the weights decrease exponentially quickly in ex-
pectation.

The direct correspondence to stick-breaking in DPs implies
that a range of techniques for and extensions to the DP can
be adapted for the IBP. For example, we can generalize the
IBP by replacing the Beta(α, 1) distribution on ν(k)’s with
other distributions. One possibility is a Pitman-Yor [14]
extension of the IBP, deﬁned as

ν(k) ∼ Beta(α + kd, 1 − d)

µ(k) =

k

Yl=1

ν(l)

(18)

(1)µ
(2)µ
(3)µ
(4)µ
(5)µ
(6)µ

(4)π

(5)π

(6)π

(2)π

(3)π

π
(1)

Figure 1: Stick-breaking construction for the DP and IBP.
The black stick at top has length 1. At each iteration the
vertical black line represents the break point. The brown
dotted stick on the right is the weight obtained for the DP,
while the blue stick on the left is the weight obtained for
the IBP.

where d ∈ [0, 1) and α > −d. The Pitman-Yor IBP
weights decrease in expectation as a O(k− 1
d ) power-law,
and this may be a better ﬁt for some naturally occurring
data which have a larger number of features with signiﬁ-
cant but small weights [4].

An example technique for the DP which we could adapt to
the IBP is to truncate the stick-breaking construction after a
certain number of break points and to perform inference in
the reduced space. [7] gave a bound for the error introduced
by the truncation in the DP case which can be used here as
well. Let K ∗ be the truncation level. We set µ(k) = 0 for
each k > K ∗, while the joint density of µ(1:K ∗) is,

p(µ(1:K ∗)) =

K ∗

Yk=1

p(µ(k)|µ(k−1))

(19)

=αK ∗

µα

(K ∗)

K ∗

Yk=1

µ−1
(k)

I(0 ≤ µ(K ∗) ≤ · · · ≤ µ(1) ≤ 1)

The conditional distribution of Z given µ(1:K ∗) is simply1

p(Z|µ(1:K ∗)) =

N

K ∗

Yi=1

Yk=1

µzik
(k)(1 − µ(k))1−zik

(20)

with zik = 0 for k > K ∗. Gibbs sampling in this represen-
tation is straightforward, the only point to note being that
adaptive rejection sampling (ARS) [3] should be used to
sample each µ(k) given other variables (see next section).

4 SLICE SAMPLER

Gibbs sampling in the truncated stick-breaking construc-
tion is simple to implement, however the predetermined
truncation level seems to be an arbitrary and unneces-
sary approximation.
In this section, we propose a non-
approximate scheme based on slice sampling, which can be

1Note that we are making a slight abuse of notation by using
Z both to denote the original IBP matrix with arbitrarily ordered
columns, and the equivalent matrix with the columns reordered to
decreasing µ’s. Similarly for the feature parameters θ’s.

seen as adaptively choosing the truncation level at each it-
eration. Slice sampling is an auxiliary variable method that
samples from a distribution by sampling uniformly from
the region under its density function [12]. This turns the
problem of sampling from an arbitrary distribution to sam-
pling from uniform distributions. Slice sampling has been
successfully applied to DP mixture models [8], and our ap-
plication to the IBP follows a similar thread.

In detail, we introduce an auxiliary slice variable,

s|Z, µ(1:∞) ∼ Uniform[0, µ∗]

(21)

where µ∗ is a function of µ(1:∞) and Z, and is chosen to be
the length of the stick for the last active feature,

µ∗ = min(cid:26)1, min

k: ∃i,zik=1

µ(k)(cid:27) .

(22)

The joint distribution of Z and the auxiliary variable s is

p(s, µ(1:∞), Z) = p(Z, µ(1:∞)) p(s|Z, µ(1:∞))

(23)

where p(s|Z, µ(1:∞)) = 1
I(0 ≤ s ≤ µ∗). Clearly, integrat-
µ∗
ing out s preserves the original distribution over µ(1:∞) and
Z, while conditioned on Z and µ(1:∞), s is simply drawn
from (21). Given s, the distribution of Z becomes:

p(Z|x, s, µ(1:∞)) ∝ p(Z|x, µ(1:∞)) 1
µ∗

I(0 ≤ s ≤ µ∗) (24)

which forces all columns k of Z for which µ(k) < s to be
zero. Let K ∗ be the maximal feature index with µ(K ∗) > s.
Thus zik = 0 for all k > K ∗, and we need only consider
updating those features k ≤ K ∗. Notice that K ∗ serves
as a truncation level insofar as it limits the computational
costs to a ﬁnite amount without approximation.

Let K † be an index such that all active features have in-
dex k < K † (note that K † itself would be an inactive fea-
ture). The computational representation for the slice sam-
pler consists of the slice variables and the ﬁrst K † features:
hs, K ∗, K †, Z1:N,1:K †, µ(1:K †), θ1:K †i. The slice sampler
proceeds by updating all variables in turn.

Update s. The slice variable is drawn from (21). If the new
value of s makes K ∗ ≥ K † (equivalently, s < µ(K †)), then
we need to pad our representation with inactive features
until K ∗ < K †. In the appendix we show that the stick
lengths µ(k) for new features k can be drawn iteratively
from the following distribution:

p(µ(k)|µ(k−1), z:,>k = 0) ∝ exp(αPN

µα−1
(k) (1 − µ(k))N I(0 ≤ µ(k) ≤ µ(k−1))

i=1

1
i (1 − µ(k))i)
(25)

We used ARS to draw samples from (25) since it is log-
concave in log µ(k). The columns for these new features
are initialized to z:,k = 0 and their parameters drawn from
their prior θk ∼ H.

Update Z. Given s, we only need to update zik for each i
and k ≤ K ∗. The conditional probabilities are:

p(zik = 1|rest) ∝

µ(k)
µ∗ f (xi|zi,¬k, zik = 1, θ1:K †)

(26)

The µ∗ denominator is needed when different values of zik
induces different values of µ∗ by changing the index of the
last active feature.

Update θk. For each k = 1, . . . , K †, the conditional prob-
ability of θk is:

p(θk|rest) ∝ h(θk)

N

Yi=1

f (xi|zi,1:K † , θ¬k, θk)

(27)

Update µ(k). For k = 1, . . . , K † − 1, combining (19) and
(20), the conditional probability of µ(k) is

p(µ(k)|rest) ∝µm·k−1

(k)

(1 − µ(k))N −m·k
I(µ(k+1) ≤ µ(k) ≤ µ(k−1))

(28)

where m·k = PN

i=1 zik. For k = K †, in addition to tak-
ing into account the probability of features K † is inactive,
we also have to take into account the probability that all
columns of Z beyond K † are inactive as well. The ap-
pendix shows that the resulting conditional probability of
µ(K †) is given by (25) with k = K †. We draw from both
(28) and (25) using ARS.

5 CHANGE OF REPRESENTATIONS

Both the stick-breaking construction and the standard IBP
representation are different representations of the same
nonparametric object. In this section we consider updates
which change from one representation to the other. More
precisely, given a posterior sample in the stick-breaking
representation we wish to construct a posterior sample in
the IBP representation and vice versa. Such changes of
representation allow us to make use of efﬁcient MCMC
moves in both representations, e.g. interlacing split-merge
moves in IBP representation [10] with the slice sampler
in stick-breaking representation. Furthermore, since both
stick lengths and the ordering of features are integrated out
in the IBP representation, we can efﬁciently update both
in the stick-breaking representation by changing to the IBP
representation and back.

We appeal to the inﬁnite limit formulation of both repre-
sentations to derive the appropriate procedures. In particu-
lar, we note that the IBP is obtained by ignoring the order-
ing on features and integrating out the weights µ(1:K) in an
arbitrarily large ﬁnite model, while the stick-breaking con-
struction is obtained by enforcing an ordering with decreas-
ing weights. Thus, given a sample in either representations,
our approach is to construct a corresponding sample in an

arbitrarily large ﬁnite model, then to either ignore the or-
dering and weights (to get IBP) or to enforce the decreasing
weight ordering (to get stick-breaking).

Changing from stick-breaking to the standard IBP repre-
sentation is easy. We simply drop the stick lengths as
well as the inactive features, leaving us with the K + active
feature columns along with the corresponding parameters.
To change from IBP back to the stick-breaking represen-
tation, we have to draw both the stick lengths and order
the features in decreasing stick lengths, introducing inac-
tive features into the representation if required. We may
index the K + active features in the IBP representation as
k = 1, . . . , K + in the ﬁnite model. Let Z1:N,1:K+ be the
feature occurrence matrix. Suppose that we have K (cid:29) K+
features in the ﬁnite model. For the active features, the pos-
terior for the lengths are simply

µ+
k |z:,k ∼ Beta(

α
K

+ m·k, 1 + N − m·k)

→ Beta(m·k, 1 + N − m·k)

(29)

(1) > µ◦

as K → ∞. For the rest of the K − K + inactive fea-
tures, it is sufﬁcient to consider only those inactive features
with stick lengths larger than mink µ+
k . Thus we consider
a decreasing ordering µ◦
(2) > · · · on these lengths.
(25) gives their densities in the K → ∞ limit and ARS
k . Fi-
can be used to draw µ◦
nally, the stick-breaking representation is obtained by re-
ordering µ+
(1:K ◦) in decreasing order, with the fea-
ture columns and parameters taking on the same ordering
(columns and parameters corresponding to inactive features
are set to 0 and drawn from their prior respectively), giving
us K + + K ◦ features in the stick-breaking representation.

(K ◦) < mink µ+

(1:K ◦) until µ◦

1:K+ , µ◦

5.1 SEMI-ORDERED STICK-BREAKING

In deriving the change of representations from the IBP to
the stick-breaking representation, we made use of an in-
termediate representation whereby the active features are
unordered, while the inactive ones have an ordering of de-
creasing stick lengths. It is in fact possible to directly work
with this representation, which we shall call semi-ordered
stick-breaking.

The representation consists of K + active and unordered
features, as well as an ordered sequence of inactive fea-
tures. The stick lengths for the active features have condi-
tional distributions:

µ+
k |z:,k ∼ Beta(m·,k, 1 + N − m·,k)

(30)

while for the inactive features we have a Markov property:

p(µ◦

(k)|µ◦
(µ◦

(k−1), z:,>k = 0) ∝ exp(PN
(k))α−1(1 − µ◦
(k))N I(0 ≤ µ◦

1
i (1 − µ◦
(k−1))

i=1
(k) ≤ µ◦

(k))i))
(31)

5.2 SLICE SAMPLER

To use the semi-ordered stick-breaking construction as a
representation for inference, we can again use the slice
sampler to adaptively truncate the representation for inac-
tive features. This gives an inference scheme which works
in the non-conjugate case, is not approximate, has an adap-
tive truncation level, but without the restrictive ordering
constraint of the stick-breaking construnction. The repre-
sentation hs, K +, Z1:N,1:K+, µ+
1:K+, θ1:K+i consists only
of the K + active features and the slice variable s,

s ∼ Uniform[0, µ∗] µ∗ = min(cid:26)1, min

1≤k≤K+

k(cid:27) (32)
µ+

Once a slice value is drawn, we generate K ◦ inactive
features, with their stick lengths drawn from (31) until
(K ◦+1) < s. The associated feature columns Z ◦
µ◦
1:N,1:K ◦
are initialized to 0 and the parameters θ◦
1:K ◦ drawn from
their prior. Sampling for the feature entries and parameters
for both the active and just generated inactive features pro-
ceed as before. Afterwards, we drop from the list of active
features any that became inactive, while we add to the list
any inactive feature that became active. Finally, the stick
lengths for the new list of active features are drawn from
their conditionals (30).

6 EXPERIMENT

In this section we compare the mixing performance of the
two proposed slice samplers against Gibbs sampling. We
chose a simple synthetic dataset so that we can be assured
of convergence to the true posterior and that mixing times
can be estimated reliably in a reasonable amount of compu-
tation time. We also chose to apply the three samplers on a
conjugate model since Gibbs sampling requires conjugacy,
although our implementation of the two slice samplers did
not make use of this. In the next section we demonstrate
the modelling performance of a non-conjugate model us-
ing the semi-ordered slice sampler on a dataset of MNIST
handwritten digits.

We used the conjugate linear-Gaussian binary latent fea-
ture model for comparing the performances of the different
samplers [6]. Each data point xi is modelled using a spher-
ical Gaussian with mean zi,:A and variance σ2
X , where zi,:
is the row vector of feature occurrences corresponding to
xi, and A is a matrix whose kth row forms the parameters
for the kth feature. Entries of A are drawn i.i.d. from a zero
A. We generated 1, 2 and
mean Gaussian with variance σ2
3 dimensional datasets from the model with data variance
ﬁxed at σ2
X = 1, varying values of the strength parameter
α = 1, 2 and the latent feature variance σ2
A = 1, 2, 4, 8. For
each combination of parameters we produced ﬁve datasets
with 100 data points, giving a total of 120 datasets. For all
datasets, we ﬁxed σ2
A to the generating values and
learned the feature matrix Z and α.

X and σ2

103

102

101

e
m

i
t
 

i

g
n
x
m

i

Stick−Breaking Semi−Ordered   Gibbs Sampling

Figure 2: Autocorrelation times for K + for the slice sam-
pler in decreasing stick lengths ordering, in semi-ordered
stick-breaking representation, and for the Gibbs sampler.

For each dataset and each sampler, we repeated 5 runs of
15, 000 iterations. We used the autocorrelation coefﬁcients
of the number of represented features K + and α (with a
maximum lag of 2500) as measures of mixing time. We
found that mixing in K + is slower than in α for all datasets
and report results only for K + here. We also found that
in this regime the autocorrelation times do not vary with
A. In Figure 2 we report the auto-
dimensionality or with σ2
correlation times of K + over all runs, all datasets, and all
three samplers. As expected, the slice sampler using the de-
creasing stick lengths ordering was always slower than the
semi-ordered one. Surprisingly, we found that the semi-
ordered slice sampler was just as fast as the Gibbs sampler
which fully exploits conjugacy. This is about as well as we
would expect a more generally applicable non-conjugate
sampler to perform.

7 DEMONSTRATION

In this section we apply the semi-ordered slice sampler to
1000 examples of handwritten images of 3’s in the MNIST
dataset. The model we worked with is a generalization of
that in Section 6, where in addition to modelling feature
occurrences, we also model per object features values [6].
In particular, let Y be a matrix of the same size as Z, with
i.i.d. zero mean unit variance Gaussian entries. We model
each xi as

xi|Z, Y, A, σ2

X ∼ N ((zi,: (cid:12) yi,:)A, σ2

X I),

(33)

where (cid:12) is elementwise multiplication. Speciﬁcation for
the rest of the model is as in Section 6. We can integrate Y
or A out while maintaining tractability, but not both.

The handwritten digit images are ﬁrst preprocessed by pro-
jecting on to the ﬁrst 64 PCA components, and the sampler
ran for 10000 iterations. The trace plot of the log likeli-
hood and the distribution of the number of active features
are shown in Figure 3. The model succesfully ﬁnds latent
features to reconstruct the images as shown in Figure 4.
Some of the latent features found are shown in Figure 5.
Most appear to model local shifts of small edge segments

×105
−4

−4.1

−4.2

−4.3

300

200

100

d
o
o
h

i
l

e
k

i
l
 
 

g
o

l

)
k
 

e
r
u

t

a
e

f
 

h

t
i

w
 
s
t
c
e
b
o

j

 

#
(
 

k

m

0

20

40

60

80

k (feature label)

400

300

200

100

s
n
o

i
t

a
r
e

t
i
 

#

2500

5000

iterations

7500

10000

0
50

100

# active feats

150

j

s
t
c
e
b
o
#

 

150

100

50

0

5

10

# active feats

15

100

120

Figure 3: Top-left: the log likelihood trace plot. The sam-
pler quickly ﬁnds a high likelihood region. Top-right: his-
togram of the number of active features over the 10000 iter-
ations. Bottom-left: number of images sharing each feature
during the last MCMC iteration. Bottom-right: histogram
of the number of active features used by each input image.
Note that about half of the features are used by only a few
data points, and each data point is represented by a small
subset of the active features.

Figure 5: Features that are shared between many digits.

form A = P∞

ments. A direct consequence of our stick-breaking con-
struction is that a draw from such a Beta process has the
k=1 µ(k)δθk with µ(k) drawn from (5) and
θk drawn i.i.d. from the base measure H. This is a par-
ticularly simply case of a more general construction called
the inverse L´evy measure [18, 9]. Generalizations to us-
ing other stick-breaking constructions automatically lead
to generalizations of the Beta process, and we are currently
exploring a number of possibilities, including the Pitman-
Yor extension. Finally, the duality observed in Section 3.2
seems to be a hitherto unknown connection between the
Beta process and the DP which we are currently trying to
understand.

As an aside, it is interesting to note the importance of fea-
ture ordering in the development of the IBP. To make the
derivation rigorous, [6] had to carefully ignore the feature
ordering by considering permutation-invariant equivalence
classes before taking the inﬁnite limit. In this paper, we de-
rived the stick-breaking construction by imposing a feature
ordering with decreasing feature weights.

To conclude, our development of a stick-breaking construc-
tion for the IBP has lead to interesting insights and connec-
tions, as well as practical algorithms such as the new slice
samplers.

ACKNOWLEDGEMENTS

Figure 4: Last column: original digits. Second last column:
reconstructed digits. Other columns: features used for re-
construction.

We thank the reviewers for insightful comments. YWT
thanks the Lee Kuan Yew Endowment Fund for funding.

of the digits, and are reminiscent of the result of learning
models with sparse priors (e.g. ICA) on such images [16].

8 DISCUSSION AND FUTURE WORK

We have derived novel stick-breaking representations of the
Indian buffet process. Based on these representations new
MCMC samplers are proposed that are easy to implement
and work on more general models than Gibbs sampling.
In experiments we showed that these samplers are just as
efﬁcient as Gibbs without using conjugacy.

[17] have recently showed that the IBP is a distribution
on matrices induced by the Beta process with a constant
strength parameter of 1. This relation to the Beta process
is proving to be a fertile ground for interesting develop-

REFERENCES

[1] D. Aldous. Exchangeability and related topics.

In
´Ecole d’ ´Et´e de Probabilit´es de Saint-Flour XIII–
1983, pages 1–198. Springer, Berlin, 1985.

[2] W. Chu, Z. Ghahramani, R. Krause, and D. L. Wild.
Identifying protein complexes in high-throughput
protein interaction screens using an inﬁnite latent fea-
ture model. In BIOCOMPUTING: Proceedings of the
Paciﬁc Symposium, 2006.

[3] W.R. Gilks and P. Wild. Adaptive rejection sampling
for Gibbs sampling. Applied Statistics, 41:337–348,
1992.

[4] S. Goldwater, T.L. Grifﬁths, and M. Johnson. Interpo-
lating between types and tokens by estimating power-

law generators.
Processing Systems, volume 18, 2006.

In Advances in Neural Information

[5] D. G¨or¨ur, F. J¨akel, and C. E. Rasmussen. A choice
model with inﬁnitely many latent features.
In Pro-
ceedings of the International Conference on Machine
Learning, volume 23, 2006.

[19] F. Wood, T. L. Grifﬁths, and Z. Ghahramani. A
non-parametric Bayesian method for inferring hidden
causes. In Proceedings of the Conference on Uncer-
tainty in Artiﬁcial Intelligence, volume 22, 2006.

APPENDIX

[6] T. L. Grifﬁths and Z. Ghahramani.

Inﬁnite latent
In
feature models and the Indian buffet process.
Advances in Neural Information Processing Systems,
volume 18, 2006.

Recall from the construction of µ(k+1:∞) that it is simply
the K → ∞ limit of a decreasing ordering of µLk. Since
reordering does not affect the probabilities of zil’s given
the corresponding µl for each l ∈ Lk,

p(z:,>k = 0|µ(k))

= lim

K→∞Z p(µLk |µ(k))p(z:,Lk = 0|µLk ) dµLk

Given µ(k), µl’s and zil’s are conditionally i.i.d. across dif-
ferent l’s, with cdf of µl as given in (12). Thus we have

= lim

K→∞(cid:18)Z µ(k)

0

(1 − µ)N α

K µ

− α
(k) µ α
K

K −1 dµ(cid:19)K−k

(34)

Applying change of variables ν = µ/µ(k) to the integral,

[7] H. Ishwaran and L.F. James. Gibbs sampling meth-
ods for stick-breaking priors. Journal of the American
Statistical Association, 96(453):161–173, 2001.

[8] M. Kalli and S. G. Walker.

Slice sampling for
the Dirichlet process mixture model. Poster pre-
sented at the Eighth Valencia International Meeting
on Bayesian Statistics, 2006.

[9] P. L´evy.

Th´eorie de L’Addition des Variables

Al´eatoires. Paris: Gauthier-Villars, 1937.

[10] E. Meeds, Z. Ghahramani, R. Neal, and S. T. Roweis.
Modeling dyadic data with binary latent factors. In
Advances in Neural Information Processing Systems,
volume 19, to appear 2007.

[11] D. J. Navarro and T. L. Grifﬁths. A nonparametric
Bayesian method for inferring features from similar-
ity judgements. In Advances in Neural Information
Processing Systems, volume 19, to appear 2007.

[12] R. M. Neal. Slice sampling. Annals of Statistics,

31:705–767, 2003.

[13] R.M. Neal. Markov chain sampling methods for
Dirichlet process mixture models. Journal of Compu-
tational and Graphical Statistics, 9:249–265, 2000.

[14] J. Pitman and M. Yor. The two-parameter Poisson-
Dirichlet distribution derived from a stable subordi-
nator. Annals of Probability, 25:855–900, 1997.

0

Z 1
=Z 1
=Z 1

0

0

N

N

=

=

Xi=0
Xi=0
=(cid:18)

(1 − νµ(k))N α

K ν α

K −1 dν

(1 − ν + ν(1 − µ(k)))N α

K ν α

K −1 dν

N

Xi=0

i ) (1 − ν)N −i(ν(1 − µ(k)))i α
(N

K ν

α

K −1 dν

i ) (1 − µ(k))i α
(N

K

Γ( α
Γ( α

K +i)Γ(N −i+1)
K +i+N −i+1)

K

N !

(N −i)!i! (1 − µ(k))i α
K +j(cid:19)(cid:18)1 + α

N !
α

j=1

QN

Qi−1

j=0( α
QN

K +j)(N −i)!
j=0( α

K +j)

K PN

i=1

Qi−1
j=1
i!

α
K +j

(1 − µ(k))i(cid:19)

[15] J. Sethuraman. A constructive deﬁnition of Dirichlet

Finally, plugging the above into (34) and taking K → ∞,

priors. Statistica Sinica, 4:639–650, 1994.

[16] Y. W. Teh, M. Welling, S. Osindero, and G. E. Hinton.
Energy-based models for sparse overcomplete repre-
sentations. Journal of Machine Learning Research,
4:1235–1260, Dec 2003.

[17] R. Thibaux and M. I. Jordan. Hierarchical beta pro-
cesses and the Indian buffet process. This volume,
2007.

[18] R. L. Wolpert and K. Ickstadt. Simulations of l´evy
random ﬁelds.
In Practical Nonparametric and
Semiparametric Bayesian Statistics, pages 227–242.
Springer-Verlag, 1998.

p(z:,>k = 0|µ(k))

= exp(cid:0) − αHN + αPN

i=1

1

i (1 − µ(k))i(cid:1)

To obtain (25), we note that the conditional for µ(k) is the
posterior conditioned on both z:,k = 0 and z:,>k = 0. The
prior given µ(k−1) is (14), the probability of z:,k = 0 is
just (1 − µ(k))N , while the probability of z:,>k = 0 is (35);
multiplying all three gives (25).

(35)

