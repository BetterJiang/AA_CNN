Hierarchical Bayesian Nonparametric Models

with Applications

Yee Whye Teh

Gatsby Computational Neuroscience Unit

University College London

17 Queen Square

London WC1N 3AR, United Kingdom

Michael I. Jordan

Department of Statistics

Department of Electrical Engineering and Computer Science

University of California, Berkeley

Berkeley, CA 94720, USA

February 14, 2009

Abstract

Hierarchical modeling is a fundamental concept in Bayesian statistics.
The basic idea is that parameters are endowed with distributions which
may themselves introduce new parameters, and this construction recurses.
In this review we discuss the role of hierarchical modeling in Bayesian non-
parametrics, focusing on models in which the inﬁnite-dimensional parame-
ters are treated hierarchically. For example, we consider a model in which
the base measure for a Dirichlet process is itself treated as a draw from
another Dirichlet process. This yields a natural recursion that we refer
to as a hierarchical Dirichlet process. We also discuss hierarchies based
on the Pitman-Yor process and on completely random processes. We
demonstrate the value of these hierarchical constructions in a wide range
of practical applications, in problems in computational biology, computer
vision and natural language processing.

Introduction

1
Hierarchical modeling is a fundamental concept in Bayesian statistics. The basic
idea is that parameters are endowed with distributions which may themselves
introduce new parameters, and this construction recurses. A common motif
in hierarchical modeling is that of the conditionally independent hierarchy, in
which a set of parameters are coupled by making their distributions depend

1

on a shared underlying parameter. These distributions are often taken to be
identical, based on an assertion of exchangeability and an appeal to de Finetti’s
theorem.

Hierarchies help to unify statistics, providing a Bayesian interpretation of
frequentist concepts such as shrinkage and random eﬀects. Hierarchies also
provide ways to specify non-standard distributional forms, obtained as integrals
over underlying parameters. They play a role in computational practice in the
guise of variable augmentation. These advantages are well appreciated in the
world of parametric modeling, and few Bayesian parametric modelers fail to
make use of some aspect of hierarchical modeling in their work.

Nonparametric Bayesian models also typically include many classical ﬁnite-
dimensional parameters, including scale and location parameters, and hierar-
chical modeling concepts are often invoked in specifying distributions for these
parameters. For example, the Dirichlet process DP(α, G0) involves a concentra-
tion parameter α, which is generally given a prior distribution in nonparametric
(and semiparametric) models that make use of the Dirichlet process. Moreover,
the base measure, G0, is often taken to be a parametric distribution and its
parameters are endowed with prior distributions as well.

In this chapter we discuss a more thoroughgoing exploitation of hierarchi-
cal modeling ideas in Bayesian nonparametric statistics. The basic idea is that
rather than treating distributional parameters such as G0 parametrically, we
treat them nonparametrically. In particular, the base measure G0 in the Dirich-
let process can itself be viewed as a random draw from some distribution on
measures—speciﬁcally it can be viewed as a draw from the Dirichlet process.
This yields a natural recursion that we refer to as a hierarchical Dirichlet pro-
cess. Our focus in this chapter is on nonparametric hierarchies of this kind,
where the tools of Bayesian nonparametric modeling are used recursively.

The motivations for the use of hierarchical modeling ideas in the nonpara-
metric setting are at least as strong as they are in the parametric setting. In
particular, nonparametric models involve large numbers of degrees of freedom,
and hierarchical modeling ideas provide essential control over these degrees of
freedom. Moreover, hierarchical modeling makes it possible to take the build-
ing blocks provided by simple stochastic processes such as the Dirichlet process
and construct models that exhibit richer kinds of probabilistic structure. This
breathes life into the nonparametric framework.

The chapter is organized as follows. In Section 2, we discuss the hierarchical
Dirichlet process, showing how it can be used to link multiple Dirichlet processes.
We present several examples of real-world applications in which such models are
natural. Section 3 shows how the hierarchical Dirichlet process can be used to
build nonparametric hidden Markov models; these are hidden Markov models in
which the cardinality of the state space is unbounded. We also discuss extensions
to nonparametric hidden Markov trees and nonparametric probabilistic context
free grammars. In Section 4 we consider a diﬀerent nonparametric hierarchy
based on the Pitman-Yor model, showing that it is natural in domains such
as natural language processing in which data often exhibit power-law behavior.
Section 5 discusses the beta process, an alternative to the Dirichlet process

2

which yields sparse featural representations. We show that the counterpart of
the Chinese restaurant process is a distribution on sparse binary matrices known
as the Indian buﬀet process. We also consider hierarchical models based on the
beta process. In Section 6, we consider some semiparametric models that are
based on nonparametric hierarchies. Finally, in Section 7 we present an overview
of some of the algorithms that have been developed for posterior inference in
hierarchical Bayesian nonparametric models.

In all of these cases, we use practical applications to motivate these con-
structions and to make our presentation concrete. Our applications range from
problems in biology to computational vision to natural language processing.
Several of the models that we present provide state-of-the-art performance in
these application domains. This wide range of successful applications serves
notice as to the growing purview of Bayesian nonparametric methods.

2 Hierarchical Dirichlet Processes
The Dirichlet process (DP) is useful in models for which a component of the
model is a discrete random variable of unknown cardinality. The canonical
example of such a model is the DP mixture model, where the discrete variable
is a cluster indicator. The hierarchical Dirichlet process (HDP) is useful in
problems in which there are multiple groups of data, where the model for each
group of data incorporates a discrete variable of unknown cardinality, and where
we wish to tie these variables across groups (Teh et al., 2006). For example,
the HDP mixture model allows us to share clusters across multiple clustering
problems.
The basic building block of a hierarchical Dirichlet process is a recursion in
which the base measure G0 for a Dirichlet process G ∼ DP(α, G0) is itself a draw
from a Dirichlet process: G0 ∼ DP(γ, H). This recursive construction has the
eﬀect of constraining the random measure G to place its atoms at the discrete
locations determined by G0. The major application of such a construction is to
the setting of conditionally independent hierarchical models of grouped data.
More formally, consider an indexed collection of DPs, {Gj}, one for each of a
countable set of groups and deﬁned on a common probability space (Θ, Ω). The
hierarchical Dirichlet process ties these random measures probabilistically by
letting them share their base measure and letting this base measure be random:

G0 | γ, H ∼ DP(γ, H)
Gj | α, G0 ∼ DP(α, G0)

for j ∈ J ,

(1)

where J is the index set. This conditionally independent hierarchical model
induces sharing of atoms among the random measures Gj since each inherits its
set of atoms from the same G0. To understand the precise nature of the sharing
induced by the HDP it is helpful to consider representations akin to the stick-
breaking and Chinese restaurant representations of the DP. We consider these
representations in the next three subsections before turning to a discussion of
applications of the HDP.

3

Figure 1: The HDP stick-breaking construction. The left panel depicts a draw
of β, and the remaining panels depict draws of π1, π2 and π3 conditioned on
β.

Note that the recursive construction of the HDP can be generalized to ar-
bitrary hierarchies in the obvious way. Each Gj is given a DP prior with base
measure Gpa(j), where pa(j) is the parent index of j in the hierarchy. As in
the two-level hierarchy in Eq. (1), the set of atoms at the top level is shared
throughout the hierarchy, while the multi-level hierarchy allows for a richer de-
pendence structure on the weights of the atoms. Section 4 presents an instance
of such a hierarchy in the setting of Pitman-Yor processes.

Other ways to couple multiple Dirichlet processes have been proposed in the
literature; in particular the dependent Dirichlet process of MacEachern et al.
(2001) provides a general formalism. Ho et al. (2006) gives a complementary
view of the HDP and its Pitman-Yor generalizations in terms of coagulation
operators. See Teh et al. (2006) and Chapter ?? for overviews.

2.1 Stick-Breaking Construction
In this section we develop a stick-breaking construction for the HDP. This rep-
resentation provides a concrete representation of draws from an HDP and it
provides insight into the sharing of atoms across multiple DPs.
We begin with the stick-breaking representation for the random base mea-
sure G0, where G0 ∼ DP(γ, H). Given that this base measure is distributed
according to a DP, we have (Sethuraman, 1994; Ishwaran and James, 2001, also
see Section ?? in Chapter ??):

βkδθ∗∗

k

,

for k = 1, . . . ,∞

(2)

(3)

G0 =

where

vk | γ ∼ Beta(1, γ)
k−1(cid:89)
(1 − vl)

βk = vk
k | H ∼ H.
θ∗∗

l=1

∞(cid:88)

k=1

4

010203000.20.40.60.8βweights010203000.20.40.60.8π1010203000.20.40.60.8π2010203000.20.40.60.8π3We refer to the joint distribution on the inﬁnite sequence (β1, β2, . . .) as the
GEM(γ) distribution (Pitman, 2002) (“GEM” stands for Griﬃths, Engen and
McCloskey).

The random measures Gj are also distributed (conditionally) according to
a DP. Moreover, the support of each Gj is contained within the support of G0.
Thus the stick-breaking representation for Gj is a reweighted sum of the atoms
in G0:

Gj =

πjkδθ∗∗

k

.

(4)

k=1

The problem reduces to ﬁnding a relationship between the weights β = (β1, β2, . . .)
and πj = (πj1, πj2, . . .). Let us interpret these weight vectors as probability
measures on the discrete space {1, . . . ,∞}. Taking partitions over integers in-
duced by partitions on Θ, the deﬁning property of the DP (Ferguson, 1973)
implies:

∞(cid:88)

Some algebra then readily yields the following explicit construction for πj con-
ditioned on β:

πj | α, β ∼ DP(α, β).
(cid:33)(cid:33)
(cid:32)
vjk | α, β1, . . . , βk ∼ Beta
k−1(cid:89)
(1 − vjl).

(cid:32)
1 − k(cid:88)

πjk = vjk

αβk, α

βl

l=1

(5)

for k = 1, . . . ,∞ (6)

l=1

Figure 1 shows a sample draw of β along with draws from π1, π2 and π3 given
β.
From Eq. (3) we see that the mean of βk is E[βk] = γk−1(1 + γ)−k which
decreases exponentially in k. The mean for πj is simply its base measure β; thus
E[πjk] = E[βk] = γk−1(1+γ)−k as well. However the law of total variance shows
that πjk has higher variance than βk: Var[πjk] = E[ βk(1−βk)
]+Var[βk] > Var[βk].
The higher variance is reﬂected in Figure 1 by the sparser nature of πj relative
to β.

1+α

2.2 Chinese Restaurant Franchise
The Chinese restaurant process (CRP) describes the marginal probabilities of
the DP in terms of a random partition obtained from a sequence of customers
sitting at tables in a restaurant. There is an analogous representation for the
HDP which we refer to as a Chinese restaurant franchise (CRF). In a CRF the
metaphor of a Chinese restaurant is extended to a set of restaurants, one for
each index in J . The customers in the jth restaurant sit at tables in the same
manner as the CRP, and this is done independently in the restaurants. The

5

coupling among restaurants is achieved via a franchise-wide menu. The ﬁrst
customer to sit at a table in a restaurant chooses a dish from the menu and all
subsequent customers who sit at that table inherit that dish. Dishes are chosen
with probability proportional to the number of tables (franchise-wide) which
have previously served that dish.

More formally, label the ith customer in the jth restaurant with a random
variable θji that is distributed according to Gj. Similarly, let θ∗
jt denote a
random variable corresponding to the tth table in the jth restaurant; these
variables are drawn independently and identically distributed (iid) according to
G0. Finally, the dishes are iid variables θ∗∗
k distributed according to the base
measure H. We couple these variables as follows. Each customer sits at one
table and each table serves one dish; let customer i in restaurant j sit at table
tji, and let table t serve dish kjt. Then let θji = θ∗

= θ∗∗

.

Let njtk be the number of customers in restaurant j seated around table t
and being served dish k, let mjk be the number of tables in restaurant j serving
dish k, and let K be the number of unique dishes served in the entire franchise.
We denote marginal counts with dots; e.g., nj·k is the number of customers in
restaurant j served dish k.

To show that the CRF captures the marginal probabilities of the HDP, we
integrate out the random measures Gj and G0 in turn from the HDP. We start
by integrating out the random measure Gj; this yields a set of conditional
distributions for the θji described by a P´olya urn scheme:

jtji

kjtji

θji | θj1, . . . , θj,i−1, α, G0 ∼ mj·(cid:88)

njt·

α + nj··

+

δθ∗

jt

α

α + nj··

G0.

(7)

t=1

A draw from this mixture can be obtained by drawing from the terms on the
right-hand side with probabilities given by the corresponding mixing propor-
tions. If a term in the ﬁrst summation is chosen then the customer sits at an
already occupied table: we increment njt·, set θji = θ∗
jt and let tji = t for the
chosen t. If the second term is chosen then the customer sits at a new table:
∼ G0, set θji = θ∗
we increment mj· by one, set njmj·· = 1, draw θ∗
and
tji = mj·.

jt is drawn iid from G0 in the P´olya urn scheme in Eq. (7),
and this is the only reference to G0 in that equation. Thus we can readily
integrate out G0 as well, obtaining a P´olya urn scheme for the θ∗
jt:

Notice that each θ∗

jmj·

jmj·

j,t−1, γ, H ∼ K(cid:88)

k=1

jt | θ∗
θ∗

11, . . . , θ∗

1m1·

, . . . , θ∗

m·k

γ + m··

+

δθ∗∗

k

γ

γ + m··

H,

(8)

where we have presumed for ease of notation that J = {1, . . . ,|J |}. As
promised, we see that the kth dish is chosen with probability proportional to
the number of tables franchise-wide that previously served that dish (m·k).

The CRF is useful in understanding scaling properties of the clustering in-
duced by an HDP. In a DP the number of clusters scales logarithmically (An-
toniak, 1974). Thus mj· ∈ O(α log nj··
α ) where mj· and nj·· are respectively the

6

(cid:80)

γ ) = O(γ log( α
mj·

from a DP, we have that K ∈ O(γ log(cid:80)

total number of tables and customers in restaurant j. Since G0 is itself a draw
If
we assume that there are J groups and that the groups (the customers in the
diﬀerent restaurants) have roughly the same size N, nj·· ∈ O(N), we see that
K ∈ O(γ log α
α ). Thus the number of
clusters scales doubly logarithmically in the size of each group, and logarithmi-
cally in the number of groups. The HDP thus expresses a prior belief that the
number of clusters grows very slowly in N. If this prior belief is inappropriate
for a given problem, there are alternatives; in particular, in Section 4.3.1 we
discuss a hierarchical model that yields power-law scaling.

γ + γ log J + γ log log N

α ) = O(γ log α

γ J log N

j log nj··

α )).

γ

j

2.3 Posterior Structure of the HDP
The Chinese restaurant franchise is obtained by integrating out the random
measures Gj and then integrating out G0. Integrating out the random measures
Gj yields a Chinese restaurant for each group as well as a sequence of iid draws
from the base measure G0, which are used recursively in integrating out G0.
Having obtained the CRF, it is of interest to derive conditional distributions that
condition on the CRF; this not only illuminates the combinatorial structure of
the HDP but it also prepares the ground for a discussion of inference algorithms
(see Section 7), where it can be useful to instantiate the CRF explicitly.
k }k=1,...,K, the
table tji at which the ith customer sits, and the dish kjt served at the tth table.
As functions of the state of the CRF, we also have the numbers of customers
n = {njtk}, the numbers of tables m = {mjk}, the customer labels θ = {θji}
jt}. The relationship between the customer labels
and the table labels θ
and the table labels is given as follows: θ∗
Consider the distribution of G0 conditioned on the state of the CRF. G0 is
∗,
independent from the rest of the CRF when we condition on the iid draws θ
because the restaurants interact with G0 only via the iid draws. The posterior
thus follows from the usual posterior for a DP given iid draws:

The state of the CRF consists of the dish labels θ

and θji = θ∗

jt = θ∗∗
jkjt

∗∗ = {θ∗∗

∗ = {θ∗

jtji

.

G0 | γ, H, θ

∗ ∼ DP

k=1 m·kδθ∗∗

k

.

(9)

(cid:32)

γH +(cid:80)K

γ + m··,

γ + m··
∗∗ are determined given θ

(cid:33)

Note that values for m and θ
the unique values and their counts among θ
constructed as follows (using the deﬁning property of a DP):

∗, since they are simply
∗1. A draw from Eq. (9) can be

β0, β1, . . . , βK | γ, G0, θ

∗ ∼ Dirichlet(γ, m·1, . . . , m·K)

(10)

0 | γ, H ∼ DP(γ, H)
K(cid:88)
G(cid:48)

G0 = β0G(cid:48)

0 +

βkδθ∗∗

k

.

1Here we make the simplifying assumption that H is a continuous distribution so that

draws from H are unique. If H is not continuous then additional bookkeeping is required.

k=1

7

We see that the posterior for G0 is a mixture of atoms corresponding to the
dishes and an independent draw from DP(γ, H).

Conditioning on this draw of G0 as well as the state of the CRF, the posteri-
ors for the Gj are independent. In particular, the posterior for each Gj follows
from the usual posterior for a DP, given its base measure G0 and iid draws θj:

(cid:32)

αG0 +(cid:80)K

(cid:33)

Gj | α, G0, θj ∼ DP

α + nj··,

k=1 nj·Kδθ∗∗

k

α + nj··

.

(11)

Note that nj and θ
θj. Making use of the decomposition of G0 into G(cid:48)
dishes θ

∗∗ are simply the unique values and their counts among the
0 and atoms located at the

∗∗, a draw from Eq. (11) can thus be constructed as follows:

πj0, πj1, . . . , πjK | α, θj ∼ Dirichlet(αβ0, αβ1 + nj·1, . . . , αβK + nj·K)

(12)

j | α, G0 ∼ DP(αβ0, G(cid:48)
G(cid:48)
K(cid:88)
0)

Gj = πj0G(cid:48)

j +

πjkδθ∗∗

.

k

k=1

We see that Gj is a mixture of atoms at θ∗∗
DP, where the concentration parameter depends on β0.

k and an independent draw from a

The posterior over the entire HDP is obtained by averaging the conditional
distributions of G0 and Gj over the posterior state of the Chinese restaurant
franchise given θ.

This derivation shows that the posterior for the HDP can be split into a
“discrete part” and a “continuous part.” The discrete part consists of atoms at
∗∗, with diﬀerent weights on these atoms for each DP. The
the unique values θ
continuous part is a separate draw from an HDP with the same hierarchical
structure as the original HDP and global base measure H, but with altered
concentration parameters. The continuous part consists of an inﬁnite series of
atoms at locations drawn iid from H. Although we have presented this posterior
representation for a two-level hierarchy, the representation extends immediately
to general hierarchies.

2.4 Applications of the HDP
In this section we consider several applications of the HDP. These models use the
HDP at diﬀerent depths in an overall Bayesian hierarchy. In the ﬁrst example
the random measures obtained from the HDP are used to generate data directly,
and in the second and third examples these random measures generate latent
parameters.

2.4.1 Information Retrieval
The growth of modern search engines on the World Wide Web has brought new
attention to a classical problem in the ﬁeld of information retrieval (IR)—how

8

should a collection of documents be represented so that relevant documents can
be returned in response to a query? IR researchers have studied a wide variety
of representations and have found empirically that a representation known as
term frequency-inverse document frequency, or “tf-idf,” yields reasonably high-
quality rankings of documents (Salton and McGill, 1983). The general intuition
is that the relevance of a document to a query should be proportional to the
frequency of query terms it contains (“term frequency”), but that query terms
that appear in many documents should be downweighted since they are less
informative (“inverse document frequency”).

Cowans (2004, 2006) has shown that the HDP provides statistical justiﬁ-
cation for the intuition behind tf-idf. Let xji denote the ith word in the jth
document in some corpus of documents, where the range of xji is a discrete
vocabulary Θ. Consider the following simple model for documents:

G0 | γ, H ∼ DP(γ, H)
Gj | α, G0 ∼ DP(α, G0)

xji | Gj ∼ Gj

(13)

for j ∈ J
for i = 1, . . . , nj,

where H is the global probability measure over the vocabulary Θ and where nj is
the number of words in the jth document. (Note that nj = nj·· where the latter
refers to the general notation introduced in Section 2.2; here and elsewhere we
use nj as a convenient shorthand.) In this model, Gj is a discrete measure over
the vocabulary associated with document j and G0 is a discrete measure over
the vocabulary that acts to tie together word usages across the corpus. The
model is presented as a graphical model in the left panel of Figure 2.
following marginal probabilities for words θ ∈ Θ in the jth document:

Integrating out G0 and the Gj as discussed in Section 2.2, we obtain the

pj(θ) =
p0(θ) = m(cid:48)

n(cid:48)
jθ + αp0(θ)
n(cid:48)
j· + α
·θ + γH(θ)
m(cid:48)·· + γ

,

(14)

where n(cid:48)
jθ is the term frequency—the number of occurrences of θ in document
j—and m(cid:48)
jθ is the number of tables serving dish θ in restaurant j in the CRF
representation.
(Note that the need for the specialized “prime” notation in
this case is driven by the fact that Θ is a discrete space in this example. In
particular, for each θ ∈ Θ there may be multiple k such that θ∗∗
k = θ. The
term frequency n(cid:48)
=θ nj·k is the number of customers eating dish θ
regardless of which menu entry they picked. Similarly, m(cid:48)

jθ = (cid:80)

jθ =(cid:80)

If we make the approximation that the number of tables serving a partic-
ular dish in a particular restaurant is at most one, then m(cid:48)
·θ is the document
frequency—the number of documents containing word θ in the corpus. We now
rank documents by computing a “relevance score” R(j, Q)—the log probability

k

k:θ∗∗

k

k:θ∗∗

=θ mjk.)

9

Figure 2: Graphical representations of HDP-based models. Left: An HDP
model for information retrieval. Center: An HDP mixture model for haplotype
phasing. Right: The HDP-LDA model for topic or admixture modeling.

10

xjiG0GjHγαi=1,...,njj∈Jθji1xjiG0GjHγαi=1,...,njj∈Jθji2θjixjiG0GjHγαi=1,...,njj∈Jof a query Q under each document j:

R(j, Q) =(cid:88)
=(cid:88)

θ∈Q

θ∈Q

log pj(θ)

log

1 +

 − log(n(cid:48)

j· + α) + log(αp0(θ))

(15)

 .

n(cid:48)
jθ
α m(cid:48)
+γH(θ)
·θ
m(cid:48)··+γ

In this score the ﬁrst term is akin to a tf-idf score, the second term is a nor-
malization penalizing large documents, and the third term can be ignored as it
does not depend on document identity j. Thus we see that a simple application
of the HDP provides a principled justiﬁcation for the use of inverse document
frequency and document length normalization. Moreover, in small-scale experi-
ments, Cowans (2004, 2006) found that this score improves upon state-of-the-art
relevance scores (Robertson et al., 1992; Hiemstra and Kraaij, 1998).

2.4.2 Multi-Population Haplotype Phasing
We now consider a class of applications in which the HDP provides a distribution
on latent parameters rather than on the observed data.

Haplotype phasing is an interesting problem in statistical genetics that can
be formulated as a mixture model (Stephens et al., 2001). Consider a set of M
binary markers along a chromosome. Chromosomes come in pairs for humans,
so let θi1 and θi2 denote the binary-valued vectors of markers for a pair of
chromosomes for the ith individual. These vectors are referred to as haplotypes,
and the elements of these vectors are referred to as alleles. A genotype xi is
a vector which records the unordered pair of alleles for each marker; that is,
the association of alleles to chromosome is lost. The haplotype phasing problem
is to restore haplotypes (which are useful for predicting disease associations)
from genotypes (which are readily assayed experimentally whereas haplotypes
are not).

Under standard assumptions from population genetics, we can write the

probability of the ith genotype as a mixture model:

p(xi) = (cid:88)

θi1,θi2∈H

p(θi1)p(θi2)p(xi | θi1, θi2),

(16)

where H is the set of haplotypes in the population and where p(xi | θi1, θi2)
reﬂects the loss of order information as well as possible measurement error.
Given that the cardinality of H is unknown, this problem is naturally formulated
as a DP mixture modeling problem where a “cluster” is a haplotype (Xing et al.,
2007).

Let us now consider a multi-population version of the haplotype phasing
problem in which the genotype data can be classiﬁed into (say) Asian, European
and African subsets. Here it is natural to attempt to identify the haplotypes in
each population and to share these haplotypes among populations. This can be

11

achieved with the following HDP mixture model:

G0 | γ, H ∼ DP(γ, H)
Gj | α, G0 ∼ DP(α, G0)

θji1, θji2 | Gj
xji | θji1, θji2 ∼ Fθji1,θji2

iid∼ Gj

,

for each population j ∈ J
for each individual i = 1, . . . , nj

(17)

where θji1, θji2 denote the pair of haplotypes for the ith individual in the jth
population. The model is presented as a graphical model in the center panel of
Figure 2. Xing et al. (2006) showed that this model performs eﬀectively in multi-
population haplotype phasing, outperforming methods that lump together the
multiple populations or treat them separately.

2.4.3 Topic Modeling
A topic model or mixed membership model is a generalization of a ﬁnite mixture
model in which each data point is associated with multiple draws from a mixture
model, not a single draw (Blei et al., 2003; Erosheva, 2003). As we will see,
while the DP is the appropriate tool to extend ﬁnite mixture models to the
nonparametric setting, the appropriate tool for nonparamametric topic models
is the HDP.

To motivate the topic model formulation, consider the problem of modeling
the word occurrences in a set of newspaper articles (e.g., for the purposes of
classifying future articles). A simple clustering methodology might attempt to
place each article in a single cluster. But it would seem more useful to be able
to cross-classify articles according to “topics”; for example, an article might be
mainly about Italian food, but it might also refer to health, history and the
weather. Moreover, as this example suggests, it would be useful to be able to
assign numerical values to the degree to which an article treats each topic.

Topic models achieve this goal as follows. Deﬁne a topic to be a probability
distribution across a set of words taken from some vocabulary W . A document is
modeled as a probability distribution across topics. In particular, let us assume
the following generative model for the words in a document. First choose a
probability vector π from the K-dimensional simplex, and then repeatedly (1)
select one of the K topics with probabilities given by the components of π and
(2) choose a word from the distribution deﬁned by the selected topic. The vector
π thus encodes the expected fraction of words in a document that are allocated
to each of the K topics. In general a document will be associated with multiple
topics.

Another natural example of this kind of problem arises in statistical genet-
ics. Assume that for each individual in a population we can assay the state
of each of M markers, and recall that the collection of markers for a single
individual is referred to as a genotype. Consider a situation in which K sub-
populations which have hitherto remained separate are now thoroughly mixed
(i.e., their mating patterns are that of a single population). Individual geno-
types will now have portions that arise from the diﬀerent subpopulations. This

12

is referred to as “admixture.” We can imagine generating a new admixed geno-
type by ﬁxing a distribution π across subpopulations and then repeatedly (1)
choosing a subpopulation according to π and (2) choosing the value of a marker
(an “allele”) from the subpopulation-speciﬁc distribution on the alleles for that
marker. This formulation is essentially isomorphic to the document modeling
formulation. (The diﬀerence is that in the document setting the observed words
are generally assumed to be exchangeable, whereas in the genetics setting each
marker has its own distribution over alleles).

To fully specify a topic model we require a distribution for π. Taking this
distribution to be symmetric Dirichlet, we obtain the latent Dirichlet allocation
(LDA) model, developed by Blei et al. (2003) and Pritchard et al. (2000) as a
model for documents and admixture, respectively. This model has been widely
used not only in the ﬁelds of information retrieval and statistical genetics, but
also in computational vision, where a “topic” is a distribution across visual
primitives, and an image is modeled as a distribution across topics (Fei-Fei and
Perona, 2005).

Let us now turn to the problem of developing a Bayesian nonparametric
version of LDA in which the number of topics is allowed to be open-ended.
As we have alluded to, this requires the HDP, not merely the DP. To see this,
consider the generation of a single word in a given document. According to
LDA, this is governed by a ﬁnite mixture model, in which one of K topics is
drawn and then a word is drawn from the corresponding topic distribution.
Generating all of the words in a single document requires multiple draws from
this ﬁnite mixture.
If we now consider a diﬀerent document, we again have
a ﬁnite mixture, with the same mixture components (the topics), but with a
diﬀerent set of mixing proportions (the document-speciﬁc vector π). Thus we
have multiple ﬁnite mixture models. In the nonparametric setting they must be
linked so that the same topics can appear in diﬀerent documents.

We are thus led to the following model, which we refer to as HDP-LDA:

G0 | γ, H ∼ DP(γ, H)
Gj | α, G0 ∼ DP(α, G0)

θji | Gj ∼ Gj
xji | θji ∼ Fθji

,

for each document j ∈ J
for each word i = 1, . . . , nj

(18)

where xji is the ith word in document j, H is the prior distribution over topics
and Fθji
is the distribution over words. The model is presented as a graphical
model in the right panel of Figure 2. Note that the atoms present in the ran-
dom distribution G0 are shared among the random distributions Gj. Thus, as
desired, we have a collection of tied mixture models, one for each document.

Topic models can be generalized in a number of other directions. For exam-
ple, in applications to document modeling it is natural to ask that topics occur
at multiple levels of resolution. Thus, at a high level of resolution, we might
wish to obtain topics that give high probability to words that occur throughout
the documents in a corpus, while at a lower level we might wish to ﬁnd topics
that are focused on words that occur in specialized subsets of the documents.

13

A Bayesian nonparametric approach to obtaining this kind of abstraction hier-
archy has been presented by Blei et al. (2004). In the model presented by these
authors, topics are arranged into a tree, and a document is modeled as a path
down the tree. This is achieved by deﬁning the tree procedurally in terms of a
linked set of Chinese restaurants.

3 Hidden Markov Models with Inﬁnite State Spaces
Hidden Markov models (HMMs) are widely used to model sequential data and
time series data (Rabiner, 1989). An HMM is a doubly-stochastic Markov chain
in which a state sequence, θ1, θ2, . . . , θτ , is drawn according to a Markov chain
on a discrete state space Θ with transition kernel π(θt, θt+1). A corresponding
sequence of observations, x1, x2, . . . , xτ , is drawn conditionally on the state se-
quence, where for all t the observation xt is conditionally independent of the
other observations given the state θt. We let Fθt
(xt) denote the distribution of
xt conditioned on the state θt; this is referred to as the “emission distribution.”
In this section we show how to use Bayesian nonparametric ideas to obtain
an “inﬁnite HMM”—an HMM with a countably inﬁnite state space (Beal et al.,
2002; Teh et al., 2006). The idea is similar in spirit to the passage from a ﬁnite
mixture model to a DP mixture model. However, as we show, the appropriate
nonparametric tool is the HDP, not the DP. The resulting model is thus referred
to as the hierarchical Dirichlet process hidden Markov model (HDP-HMM). We
present both the HDP formulation and a stick-breaking formulation in this
section; the latter is particularly helpful in understanding the relationship to
ﬁnite HMMs. It is also worth noting that a Chinese restaurant franchise (CRF)
representation of the HDP-HMM can be developed, and indeed Beal et al. (2002)
presented a precursor to the HDP-HMM that was based on an urn model akin
to the CRF.

To understand the need for the HDP rather than the DP, note ﬁrst that a
classical HMM speciﬁes a set of ﬁnite mixture distributions, one for each value
of the current state θt. Indeed, given θt, the observation xt+1 is chosen by ﬁrst
picking a state θt+1 and then choosing xt+1 conditional on that state. Thus the
transition probability π(θt, θt+1) plays the role of a mixing proportion and the
emission distribution Fθt
plays the role of the mixture component. It is natural
to consider replacing this ﬁnite mixture model by a DP mixture model. In so
doing, however, we must take into account the fact that we obtain a set of DP
mixture models, one for each value of the current state. If these DP mixture
models are not tied in some way, then the set of states accessible in a given
value of the current state will be disjoint from those accessible for some other
value of the current state. We would obtain a branching structure rather than
a chain structure. The solution to this problem is straightforward—we use the
HDP to tie the DPs.

More formally, let us consider a collection of random transition kernels,

14

Figure 3: HDP hidden Markov model.

{Gθ : θ ∈ Θ}, drawn from an HDP:
G0 | γ, H ∼ DP(γ, H)
Gθ | α, G0 ∼ DP(α, G0)

for θ ∈ Θ,

(19)

where H is a base measure on the probability space (Θ,T ). As we shall see,
the random base measure G0 allows the transitions out of each state to share
0 ∈ Θ be a predeﬁned initial state. The
the same set of next states. Let θ0 = θ∗∗
conditional distributions of the sequence of latent state variables θ1, . . . , θτ and
observed variables x1, . . . , xτ are:

θt | θt−1, Gθt−1

∼ Gθt−1
xt | θt ∼ Fθt

.

for t = 1, . . . , τ

(20)

A graphical model representation for the HDP-HMM is shown in Figure 3.

We have deﬁned a probability model consisting of an uncountable number of
DPs, which may raise measure-theoretic concerns. These concerns can be dealt
with, however, essentially due to the fact that the sample paths of the HDP-
HMM only ever encounter a ﬁnite number of states. To see this more clearly,
and to understand the relationship of the HDP-HMM to the parametric HMM,
it is helpful to consider a stick-breaking representation of the HDP-HMM. This
representation is obtained directly from the stick-breaking representation of the

15

θ0θ1θ2θτx1x2xτG0Gθθ∈ΘHγαunderlying HDP:

∞(cid:88)
∞(cid:88)

k=1

k=1

G0 =

Gθ∗∗

l

=

βkδθ∗∗

k

πθ∗∗

l

kδθ∗∗

k

where

k | H ∼ H
θ∗∗
β | γ ∼ GEM(γ)
| α, β ∼ DP(α, β).

πθ∗∗

k

(21)

for l = 0, 1, . . . ,∞,

for k = 1, . . . ,∞

(22)

0 and the atoms θ∗∗

1 , θ∗∗

k are shared across G0 and the transition distributions Gθ∗∗

The atoms θ∗∗
. Since
all states visited by the HMM are drawn from the transition distributions, the
states possibly visited by the HMM with positive probability (given G0) will
consist only of the initial state θ∗∗
2 , . . .. Relating to the
parametric HMM, we see that the transition probability from state θ∗∗
to state
θ∗∗
k is given by πθ∗∗
k and the distribution on the observations is given by Fθ∗∗
.
This relationship to the parametric HMM can be seen even more clearly if we
k with the integer k, for k = 0, 1, . . . ,∞, and if we introduce
identify the state θ∗∗
integer-valued variables zt to denote the state at time t. In particular, if θt = θ∗∗
is the state at time t, we let zt take on value k, and write πk instead of πθ∗∗
.
The HDP-HMM can now be expressed as:
zt | zt−1, πzt−1
xt | zt, θ∗∗

∼ πzt−1
∼ Fθ∗∗

(23)

,

k

k

k

l

l

l

zt

zt

with priors on the parameters and transition probabilities given by Eq. (23).
This construction shows explicitly that the HDP-HMM can be interpreted as
an HMM with a countably inﬁnite state space.

A diﬃculty with the HDP-HMM as discussed thus far is that it tends to
be poor at capturing state persistence; it has a tendency to create redundant
states and rapidly switch among them. This may not be problematic for ap-
plications in which the states are nuisance variables and it is overall predictive
likelihood that matters, but it can be problematic for segmentation or parsing
applications in which the states are the object of inference and when state per-
sistence is expected. This problem can be solved by giving special treatment
to self-transitions. In particular, let Gθ denote the transition kernel associated
with state θ. Fox et al. (2009) proposed the following altered deﬁnition of Gθ
(compare to Eq. (19)):

(cid:19)

Gθ | α, κ, G0, θ ∼ DP

α + κ,

αG0 + κδθ

α + κ

,

(24)

(cid:18)

16

where δθ is a point mass at θ and where κ is a parameter that determines the
extra mass placed on a self-transition. To see in more detail how this aﬀects state
persistence, consider the stick-breaking weights πθ∗∗
associated with one of the
countably many states θ∗∗
k that can be visited by the HMM. The stick-breaking
is altered as follows (compare to Eq. (23)):
representation of Gθ∗∗

k

(cid:18)

k

| α, β, κ ∼ DP

πθ∗∗

k

(cid:19)

α + κ,

αβ + κδθ∗∗

k

α + κ

.

(25)

Fox et al. (2009) further place a vague gamma prior on α + κ and a beta prior
on κ/(α + κ). The hyperparameters of these distributions allow prior control of
state persistence. See also Beal et al. (2002), who develop a related prior within
the framework of their hierarchical urn scheme.

3.1 Applications of the HDP-HMM
In the following sections we describe a number of applications and extensions of
the HDP-HMM. An application that we will not discuss but is worth mention-
ing is the application of HDP-HMMs to the problem of modeling recombination
hotspots and ancestral haplotypes for short segments of single nucleotide poly-
morphisms (Xing and Sohn, 2007).

3.1.1 Speaker Diarization
Speech recognition has been a major application area for classical parametric
HMMs (Huang et al., 2001). In a typical application, several dozen states are
used, roughly corresponding to the number of phoneme-like segments in speech.
The observations xt are spectral representations of speech over short time slices.
In many applications, however, the number of states is more fundamentally
part of the inferential problem and it does not suﬃce to simply ﬁx an arbi-
trary value. Consider an audio recording of a meeting in which the number
of people participating in the meeting is unknown a priori. The problem of
speaker diarization is that of segmenting the audio recording into time intervals
associated with individual speakers (Wooters and Huijbregts, 2007). Here it
is natural to consider an HDP-HMM model, where a state corresponds to an
individual speaker and the observations are again short-term spectral represen-
tations. Posterior inference in the HDP-HMM yields estimates of the spectral
content of each speaker’s voice, an estimate of the number of speakers partici-
pating in the meeting, and a diarization of the audio stream.

Such an application of the HDP-HMM has been presented by Fox et al.
(2009), who showed that the HDP-HMM approach yielded a state-of-the-art
diarization method. A noteworthy aspect of their work is that they found that
the special treatment of self-transitions discussed in the previous section was
essential; without this special treatment the HDP-HMM’s tendency to rapidly
switch among redundant states led to poor speaker diarization performance.

17

3.1.2 Word Segmentation
As another application of the HDP-HMM to speech, consider the problem of
segmenting an audio stream into a sequence of words. Speech is surprisingly
continuous with few obvious breaks between words and the problem of word seg-
mentation—that of identifying coherent segments of “words” and their bound-
aries in continuous speech—is nontrivial. Goldwater et al. (2006b) proposed
a statistical approach to word segmentation based upon the HDP-HMM. The
latent states of the HMM correspond to words. An HDP-HMM rather than a
parametric HMM is required for this problem, since there are an unbounded
number of potential words.

In the model, an utterance is viewed as a sequence of phonemes, ρ1, ρ2, . . . , ρτ .
The sequence is modeled by an HDP-HMM in which words are the latent states.
A word is itself a sequence of phonemes. The model speciﬁcation is as follows.
First, the number of words n is drawn from a geometric distribution. Then a
sequence of n words, θ1, θ2, . . . , θn, is drawn from an HDP-HMM:

G0 | γ, H ∼ DP(γ, H)
Gθ | α, G0 ∼ DP(α, G0)

θi | θi−1, Gθi−1

∼ Gθi−1

(26)

for θ ∈ Θ
for i = 1, . . . , n.

where θ0 ∼ G∅ is a draw from an initial state distribution. Each Gθ is the
transition distribution over next words, given the previous word θ. This is
deﬁned for every possible word θ, with Θ the set of all possible words (including
the empty word θ0 which serves as an initial state for the Markov chain). The
base measure H over words is a simple independent phonemes model: the length
of the word, l ≥ 1, is ﬁrst drawn from another geometric distribution, then each
phoneme ri is drawn independently from a prior over phonemes:

l(cid:89)

H(θ = (r1, r2, . . . , rl)) = η0(1 − η0)l−1

H0(rt),

(27)

t=1

where H0 is a probability measure over individual phonemes. The probability
of the observed utterance is then a sum over probabilities of sequences of words
such that their concatenation is ρ1, ρ2, . . . , ρτ .

Goldwater et al. (2006b) have shown that this HDP-HMM approach leads

to signiﬁcant improvements in segmentation accuracy.

3.1.3 Trees and Grammars
A number of other structured probabilistic objects are amenable to a nonpara-
metric treatment based on the HDP. In this section we brieﬂy discuss some
recent developments which go beyond the chain-structured HMM to consider
objects such as trees and grammars.

A hidden Markov tree (HMT) is a directed tree in which the nodes correspond
to states, and in which the probability of a state depends (solely) on its unique

18

parent in the tree. To each state there is optionally associated an observation,
where the probability of the observation is conditionally independent of the
other observations given the state (Chou et al., 1994).

We can generalize the HDP-HMM to a hierarchical Dirichlet process hidden
Markov tree (HDP-HMT) model in which the number of states is unbounded.
This is achieved by a generalization of the HDP-HMM model in which the tran-
sition matrix along each edge of the HMT is replaced with sets of draws from
a DP (one draw for each row of the transition matrix) and these DPs are tied
with the HDP. This model has been applied to problems in image processing
(denoising, scene recognition) in which the HDP-HMT is used to model corre-
lations among wavelet coeﬃcients in multiresolution models of images (Kivinen
et al., 2007a,b).

As a further generalization of the HDP-HMM, several groups have considered
nonparametric versions of probabilistic grammars (Johnson et al., 2007; Liang
et al., 2007; Finkel et al., 2007). These grammars consist of collections of rules, of
the form A → BC, where this transition from a symbol A to a pair of symbols
BC is modeled probabilistically. When the number of grammar symbols is
unknown a priori, it is natural to use the HDP to generate symbols and to tie
together the multiple occurrences of these symbols in a parse tree.

4 Hierarchical Pitman-Yor Processes
As discussed in Chapter ??, a variety of alternatives to the DP have been ex-
plored in the Bayesian nonparametrics literature. These alternatives can provide
a better ﬁt to prior beliefs than the DP. It is therefore natural to consider hi-
erarchical models based on these alternatives. In this section we shall describe
one such hierarchical model, the hierarchical Pitman-Yor (HPY) process, which
is based on the Pitman-Yor process (also known as the two-parameter Poisson-
Dirichlet process). We brieﬂy describe the Pitman-Yor process here; Section ??
in Chapter ?? as well as Perman et al. (1992), Pitman and Yor (1997) and Ish-
waran and James (2001) present further material on the Pitman-Yor process.
In Section 4.3.1 we describe an application of the HPY process to language
modeling. Section 4.3.2 presents a spatial extension of the HPY process and an
application to image segmentation.

4.1 Pitman-Yor Processes
The Pitman-Yor process is a two-parameter generalization of the DP, with a dis-
count parameter 0 ≤ d < 1 and a concentration parameter α > −d. When d = 0
the Pitman-Yor process reduces to a DP with concentration parameter α. We
write G ∼ PY(d, α, H) if G is a Pitman-Yor process with the given parameters
and base measure H. The stick-breaking construction and the Chinese restau-
rant process have natural generalizations in the Pitman-Yor process. A draw G

19

from the Pitman-Yor process has the following stick-breaking construction:

∞(cid:88)

G =

βkδθ∗

k

,

(28)

where the atoms θ∗
follows:

k are drawn iid from H, and the weights are obtained as

k=1

vk | d, α ∼ Beta(1 − d, α + kd)

for k = 1, . . . ,∞

(29)

k−1(cid:89)
(1 − vl).

l=1

βk = vk

We refer to the joint distribution over β1, β2, . . . as the GEM(d, α) distribution,
this being a two-parameter generalization of the one-parameter GEM(α) asso-
ciated with the DP. Suppose that H is a smooth distribution and let θ1, θ2, . . .
be iid draws from G. Marginalizing out G, the distribution of θi conditioned on
θ1, . . . , θi−1 follows a generalization of the P´olya urn scheme:

θi | θ1, . . . , θi−1, d, α, H ∼ K(cid:88)

t=1

nt − d
α + i − 1 δθ∗

t

+ α + Kd

α + i − 1 H,

(30)

where θ∗
t is the tth unique value among θ1, . . . , θi−1, there being nt occurrences
of θ∗
t , and K such unique values. In the Chinese restaurant analogy, each θi is
t corresponds to a table, and customer i sits at table t if θi = θ∗
a customer, θ∗
t .
There are two salient properties of this generalized Chinese restaurant process.
First, the rich-gets-richer property of the original Chinese restaurant process is
preserved, which means that there are a small number of large tables. Second,
there are a large number of small tables since the probability of occupying new
tables grows along with the number of occupied tables, and the discount d
decreases the probabilities of new customers sitting at small tables.

When 0 < d < 1 the Pitman-Yor process yields power-law behavior (Pitman,
2002; Goldwater et al., 2006a; Teh, 2006a, see also Chapter ??). It is this power-
law behavior which makes the Pitman-Yor process more suitable than the DP
for many applications involving natural phenomena. The power-law nature of
the Pitman-Yor process can be expressed in several ways. First, under Eq. (29)
we have E[βk] ∈ O(k−1/d) if 0 < d < 1, which indicates that cluster sizes decay
according to a power law. Second, Zipf’s Law can be derived from the Chinese
restaurant process; that is, the proportion of tables with n customers scales
as O(n−1−d). Finally the Chinese restaurant process also yields Heaps’ Law,
where the total number of tables in a restaurant with n customers scales as
O(nd). Note that the discount parameter d is the key parameter governing the
power-law behavior. These various power laws are illustrated in Figure 4.

20

Figure 4: Power-law behavior of the Pitman-Yor process. Left: E[βk] vs. k.
Middle: number of tables in restaurant vs. number of customers. Right: number
of tables vs. number of customers at each table. Each plot shows the results of
10 draws (small dots) and their mean (large dots). The log-log plots are well
approximated by straight lines, indicating power laws.

4.2 Hierarchical Pitman-Yor Processes
The hierarchical Pitman-Yor (HPY) process is deﬁned in the obvious manner:

G0 | η, γ, H ∼ PY(η, γ, H)
Gj | d, α, G0 ∼ PY(d, α, G0)

for j ∈ J ,

(31)

where G0 is the common base measure shared across the diﬀerent Pitman-Yor
processes Gj, and is itself given a Pitman-Yor process prior. Similarly to the
HDP, this hierarchical construction generalizes immediately to a multiple-level
hierarchy.

Recall that one of the useful facts about the HDP is that it can be represented
using both a stick-breaking representation and a Chinese restaurant franchise
representation. It would be of interest to consider generalizations of these ob-
jects to the HPY process. As we shall see in the following, the Chinese restaurant
franchise can be readily generalized to an HPY analog. Unfortunately, however,
there is no known analytic form for the stick-breaking representation of the HPY
process.
Recall that in the Chinese restaurant franchise representation, each Gj cor-
responds to a restaurant, draws θji ∼ Gj correspond to customers, tables t in
jt ∼ G0, and dishes correspond to draws
restaurant j corresponds to draws θ∗
k ∼ H. Let njtk be the number of customers in restaurant j seated at table t
θ∗∗
and eating dish k, mjk be the number of tables in restaurant j serving dish k,
and K be the number of dishes served throughout the franchise. The conditional
distributions given by the Chinese restaurant franchise for the HPY process are

21

as follows:

θji | θj1, . . . , θj,i−1, α, d, G0 ∼ mj·(cid:88)
j,t−1, γ, η, H ∼ K(cid:88)

, . . . , θ∗

t=1

1m1·

k=1

jt | θ∗
θ∗

11, . . . , θ∗

njt· − d
α + nj··
m·k − η
γ + m··

δθ∗

jt

δθ∗∗

k

+ α + mj·d
α + nj··
+ γ + Kη
γ + m··

G0

(32)

H,

(33)

which is a natural generalization of the CRF for the HDP (cf. Eq. (7) and
Eq. (8)).

4.3 Applications of the Hierarchical Pitman-Yor Process
In this section we describe an application of the HPY process to language mod-
eling and another application to image segmentation.

τ(cid:89)

4.3.1 Language Modeling
Statistical models of sentences in a natural language (e.g. English) are an in-
dispensable component of many systems for processing linguistic data, includ-
ing speech recognition, handwriting recognition and machine translation sys-
tems (Manning and Sch¨utze, 1999). In this section we describe an application
of the hierarchical Pitman-Yor process in statistical language modeling.

Most statistical language models treat sentences as drawn from Markov mod-
els of ﬁxed order larger than one. That is, the probability of a sentence consisting
of a sequence of words (θ1, θ2, . . . , θτ ) is modeled as

p(θ1, . . . , θτ ) =

p(θt | θt−n+1, . . . , θt−1),

(34)

t=1

where for simplicity θ−n+2, . . . , θ0 are special “start-of-sentence” symbols, and
n ≥ 2 is one plus the order of the Markov model. Such models are known as
n-gram models. In typical applications n = 3, corresponding to a second-order
Markov model and a context consisting of just the previous two words.

In natural languages the size of the vocabulary typically consists of more
than 104 words. This means that in a 3-gram model the number of param-
eters is in excess of 1012, making maximum likelihood estimation infeasible.
In fact a na¨ıve prior treating parameters corresponding to diﬀerent contexts
independently performs badly as well—it is important to model dependencies
across diﬀerent contexts for a language model to be successful. In the language
modeling community such dependencies are achieved by a variety of heuristic
smoothing algorithms, which combine the counts associated with diﬀerent con-
texts in various ways (Chen and Goodman, 1999).

It is also possible to take a hierarchical Bayesian point of view on smoothing,
and indeed such an approach was considered in a parametric setting by MacKay
and Peto (1994). However, word occurrences in natural languages tend to follow

22

power laws, and a nonparametric model such as the HPY process provides a
more natural prior for this domain (Teh, 2006a,b; Goldwater et al., 2006a).
Indeed, the most successful heuristic smoothing methods are closely related to
an HPY model.

Given a context u consisting of a sequence of words, let Gu be the distri-
bution over the next word following the context u. That is, Gu(θ) = p(θt =
θ | θt−n+1, . . . , θt−1 = u) in Eq. (34). We place a Pitman-Yor prior on Gu, with
base measure Gpa(u), where pa(u) is the context with the ﬁrst word dropped
from u:

Gu | d|u|, α|u|, Gpa(u) ∼ PY(d|u|, α|u|, Gpa(u)).

(35)

The parameters of the Pitman-Yor process depend on the length of the context
|u|. We recursively place a Pitman-Yor prior on Gpa(u), dropping words from
the front of the context until G∅, the distribution over next words given the
empty context ∅. Finally we place a Pitman-Yor prior on G∅:

G∅ | d0, α0, G0 ∼ PY(d0, α0, G0),

(36)

where G0 is the uniform distribution over the vocabulary. The structure of this
hierarchical prior reﬂects the notion that more recent words in the context are
more informative in predicting the next word.

Teh (2006a,b) applied the HPY language model to a 14-million word corpus,
and found that it produces state-of-the-art prediction results, closely matching
results using interpolated and modiﬁed Kneser-Ney, two of the most widely-
used smoothing algorithms (Chen and Goodman, 1998). Moreover, the HPY
language model has been shown to outperform modiﬁed Kneser-Ney in the con-
text of an application to dialog transcription (Huang and Renals, 2007). These
results are unsurprising, as Teh (2006a,b) and Goldwater et al. (2006a) showed
that interpolated Kneser-Ney can be derived as an approximation to the CRF
representation of the HPY language model. In particular, interpolated Kneser-
Ney assumes that the number of tables in each restaurant serving each dish is
at most one. This is the same approximation as in Section 2.4.1.

4.3.2 Image Segmentation
Models based on the Pitman-Yor process have also had impact in the ﬁeld of
image processing, a ﬁeld that shares with the language modeling domain the fact
that power laws characterize many of the statistics within the domain. In par-
ticular, using a database of images that were manually segmented and labeled
by humans (Oliva and Torralba, 2001), Sudderth and Jordan (2009) have shown
that both the segment sizes and the label occurrences (e.g., “sky,” “grass”) fol-
low long-tailed distributions that are well captured by the Pitman-Yor process.
This suggests considering models in which the marginal distributions at each
site in an image are governed by Pitman-Yor processes. Moreover, to share
information across a collection of images it is natural to consider HPY priors.
In this section we describe a model based on such an HPY prior (Sudderth

23

and Jordan, 2009). Our focus is the problem of image segmentation, where the
observed data are a collection of images (an image is a collection of gray-scale
or color values at each point in a two-dimensional grid) and the problem is to
output a partition of each image into segments (a segment is a coherent region
of the image, as deﬁned by human labelings).

Let us consider a generative model for image texture and color, simplifying
at ﬁrst in two ways: (1) we focus on a single image and (2) we neglect the
issue of spatial dependency within the image. Thus, for now we focus simply on
obtaining Pitman-Yor marginal statistics for segment sizes and segment labels
within a single image. Let us suppose that the image is represented as a large
collection of sites, where a site is a local region in the image (often referred to as
a pixel or a super-pixel). Let π ∼ GEM(d, α) be a draw from the two-parameter
GEM distribution. For each site i, let ti denote the segment assignment of site
i, where ti ∼ Discrete(π) are independent draws from π. Given a large number
of sites of equal size, the total area assigned to segment t will be roughly πt,
and segment sizes will follow Pitman-Yor statistics.
We also assign a label to each segment, again using a two-parameter GEM
distribution. In particular, let β ∼ GEM(η, γ) be a distribution across labels.
For each segment t we label the segment by drawing kt ∼ Discrete(β) indepen-
dently. We also let θ∗∗
k denote an “appearance model”2 for label type k, where
the θ∗∗
k are drawn from some prior distribution H. Putting this together, the
label assigned to site i is denoted kti
. The visual texture and color at site i are
then generated by a draw from the distribution θ∗∗
kti

To obtain a spatially dependent Pitman-Yor process, Sudderth and Jordan
(2009) adapt an idea of Duan et al. (2007), who used a latent collection of
Gaussian processes to deﬁne a spatially dependent set of draws from a Dirich-
let process. In particular, to each index t we associate a zero-mean Gaussian
process, ut. At a given site i, we thus have an inﬁnite collection of Gaussian
random variables, {uti}t=1,...,∞. By an appropriate choice of thresholds for this
inﬁnite sequence of Gaussian variables, it is possible to mimic a draw from the
distribution π (by basing the selection on the ﬁrst Gaussian variable in the se-
quence that is less than its threshold). Indeed, for a single site, this is simply
a change-of-variables problem from a collection of beta random variables to a
collection of Gaussian random variables. The Gaussian process framework cou-
ples the choice of segments at nearby sites via the covariance function. Figure 5
gives an example of three draws from this model, showing the underlying ran-
dom distribution π (truncated to four values), the corresponding collection of
draws from Gaussian processes (again truncated), and the resulting segmented
image.

.

This framework applies readily to multiple images by coupling the label
k across multiple images. Letting j ∈ J
distribution β and appearance models θ∗∗
index the images in the collection, we associate a segment distribution πj with
each image and associate a set of Gaussian processes with each image to describe

2This parameter is generally a multinomial parameter encoding the probabilities of various

discrete-valued texture and color descriptors.

24

Figure 5: Draws from dependent Pitman-Yor processes. Top: the random pro-
portions πj. Middle: draws from Gaussian processes, one for each entry in πj.
Bottom: resulting segmentation.

the segmentation of that image.

The image segmentation problem can be cast as posterior inference in this
HPY-based model. Given an image represented as a collection of texture and
color descriptors, we compute the maximum a posteriori set of segments for
the sites. Sudderth and Jordan (2009) have shown that this procedure yields a
state-of-the-art unsupervised image segmentation algorithm.

5 The Beta Process and the Indian Buﬀet Pro-

cess

The DP mixture model embodies the assumption that the data can be parti-
tioned or clustered into discrete classes. This assumption is made particularly
clear in the Chinese restaurant representation, where the table at which a data
point sits indexes the class (the mixture component) to which it is assigned. If
we represent the restaurant as a binary matrix in which the rows are the data
points and the columns are the tables, we obtain a matrix with a single one in
each row and all other elements equal to zero.

A diﬀerent assumption that is natural in many settings is that objects can be

25

described in terms of a collection of binary features or attributes. For example,
we might describe a set of animals with features such as diurnal/nocturnal,
avian/non-avian, cold-blooded/warm-blooded, etc. Forming a binary matrix
in which the rows are the objects and the columns are the features, we obtain
a matrix in which there are multiple ones in each row. We will refer to such a
representation as a featural representation.

A featural representation can of course be converted into a set of clusters
if desired: if there are K binary features, we can place each object into one of
2K clusters. In so doing, however, we lose the ability to distinguish between
classes that have many features in common and classes that have no features
in common. Also, if K is large, it may be infeasible to consider models with
2K parameters. Using the featural representation, we might hope to construct
models that use on the order of K parameters to describe 2K classes.

In this section we discuss a Bayesian nonparametric approach to featural
representations. In essence, we replace the Dirichlet/multinomial probabilities
that underlie the Dirichlet process with a collection of beta/Bernoulli draws.
This is achieved via the beta process, a stochastic process whose realizations
provide a countably inﬁnite collection of coin-tossing probabilities. We also
discuss some other representations of the beta process that parallel those for
the DP. In particular we describe a stick-breaking construction as well as an
analog of the Chinese restaurant process known as the Indian buﬀet process.

5.1 The Beta Process and the Bernoulli Process
The beta process is an instance of a general class of stochastic processes known
as completely random measures (Kingman, 1967, see also Chapter ??). The key
property of completely random measures is that the random variables obtained
by evaluating a random measure on disjoint subsets of the probability space
are mutually independent. Moreover, draws from a completely random measure
are discrete (up to a ﬁxed deterministic component). Thus we can represent
such a draw as a weighted collection of atoms on some probability space, as
we do for the DP. (Note, however, that the DP is not a completely random
measure because the weights are constrained to sum to one for the DP; thus,
the independence assertion does not hold for the DP. The DP can be obtained
by normalizing a completely random measure (speciﬁcally the gamma process;
see ??).

Applications of the beta process in Bayesian nonparametric statistics have
mainly focused on its use as a model for random hazard functions (Hjort, 1990,
see also Chapter ??).
In this case, the probability space is the real line and
it is the cumulative integral of the sample paths that is of interest (yielding a
random, nondecreasing step function). In the application of the beta process to
featural representations, on the other hand, it is the realization itself that is of
interest and the underlying space is no longer restricted to be the real line.

Following Thibaux and Jordan (2007), let us thus consider a general proba-
bility space (Θ, Ω) endowed with a ﬁnite base measure B0 (note that B0 is not
a probability measure; it does not necessarily integrate to one). Intuitively we

26

path and the red curve is the corresponding cumulative integral(cid:82) x

Figure 6: (a) A draw B ∼ BP(1, U[0, 1]). The set of blue spikes is the sample
−∞ B(dθ). (b)
100 samples from BeP(B), one sample per row. Note that a single sample is a
set of unit-weight atoms.

wish to partition Θ into small regions, placing atoms into these regions accord-
ing to B0 and assigning a weight to each atom, where the weight is a draw from a
beta distribution. A similar partitioning occurs in the deﬁnition of the DP, but
in that case the aggregation property of Dirichlet random variables immediately
yields a consistent set of marginals and thus an easy appeal to Kolmogorov’s
theorem. Because the sum of two beta random variables is not a beta random
variable, the construction is somewhat less straightforward in the beta process
case.

The general machinery of completely random processes deals with this issue
in an elegant way. Consider ﬁrst the case in which B0 is absolutely continuous
and deﬁne the L´evy measure on the product space [0, 1] ⊗ Θ in the following
way:

ν(dω, dθ) = cω−1(1 − ω)c−1dωB0(dθ),

(37)
where c > 0 is a concentration parameter. Now sample from a nonhomogeneous
Poisson process with the L´evy measure ν as its rate measure. This yields a
set of atoms at locations (ω1, θ1), (ω2, θ2) . . .. Deﬁne a realization of the beta
process as:

∞(cid:88)

B =

ωkδθk

,

(38)

k=1

where δθk
is an atom at θk with ωk its mass in B. We denote this stochastic
process as B ∼ BP(c, B0). Figure 6(a) provides an example of a draw from
BP(1, U[0, 1]), where U[0, 1] is the uniform distribution on [0, 1].

We obtain a countably inﬁnite set of atoms from this construction because
the L´evy measure in Eq. (37) is σ-ﬁnite with inﬁnite mass. Indeed, consider

27

0101201050Drawpartitioning the product space [0, 1]⊗Θ into stripes having equal integral under
this density. These stripes have the same ﬁnite rate under the Poisson process,
and there are an inﬁnite number of such stripes. Note also that the use of a
limiting form of the beta density implies that most of the atoms are associated
with very small weights. Campbell’s Theorem shows that the sum of these

weights is ﬁnite with probability one, since(cid:82) ων(dω, dθ) < ∞.

If B0 contains atoms, then these are treated separately. In particular, denote
the measure of the kth atom as qk (assumed to lie in (0, 1)). The realization B
necessarily contains that atom, with the corresponding weight ωk deﬁned as an
independent draw from Beta(cqk, c(1 − qk)). The overall realization B is a sum
of the weighted atoms coming from the continuous component and the discrete
component of B0.

Let us now deﬁne a Bernoulli process BeP(B) with an atomic base measure
B as a stochastic process whose realizations are collections of atoms of unit
mass on Θ. Atoms can only appear at the locations of atoms of B. Whether
or not an atom appears is determined by independent tosses of a coin, where
the probability of success is the corresponding weight of the atom in B. After n
draws from BeP(B) we can ﬁll a binary matrix that has n rows and an inﬁnite
number of columns (corresponding to the atoms of B arranged in some order).
Most of the entries of the matrix are zero while a small (ﬁnite) number of the
entries are equal to one. Figure 6(b) provides an example.

The beta process and the Bernoulli process are conjugate. Consider the

speciﬁcation:

B | c, B0 ∼ BP(c, B0)
Zi | B ∼ BeP(B),

for i = 1, . . . , n,

(39)

where Z1, . . . , Zn are conditionally independent given B. The resulting posterior
distribution is itself a beta process, with updated parameters:

(cid:33)

n(cid:88)

i=1

Zi

.

(40)

(cid:32)

B | Z1, . . . , Zn, c, B0 ∼ BP

c + n,

c

c + n

B0 +

1

c + n

a → a +(cid:80)

This formula can be viewed as an analog of standard ﬁnite-dimensional beta/Bernoulli
updating. Indeed, given a prior Beta(a, b), the standard update takes the form
i zi. In Eq. (40), c plays the role of a + b and

i zi and b → b + n −(cid:80)

cB0 is analogous to a.

5.2 The Indian Buﬀet Process
Recall that the Chinese restaurant process can be obtained by integrating out
the Dirichlet process and considering the resulting distribution over partitions.
In the other direction, the Dirichlet process is the random measure that is guar-
anteed (by exchangeability and De Finetti’s theorem) to underlie the Chinese
restaurant process. In this section we discuss the analog of these relationships
for the beta process.

28

We begin by deﬁning a stochastic process known as the Indian buﬀet process
(IBP). The IBP was originally deﬁned directly as a distribution on (equivalence
classes of) binary matrices by Griﬃths and Ghahramani (2006) and Ghahra-
mani et al. (2007). The IBP is an inﬁnitely exchangeable distribution on these
equivalence classes, thus it is of interest to discover the random measure that
must underlie the IBP according to De Finetti’s Theorem. Thibaux and Jordan
(2007) showed that the underlying measure is the beta process; that is, the IBP
is obtained by integrating over the beta process B in the hierarchy in Eq. (39).
The IBP is deﬁned as follows. Consider an Indian buﬀet with a countably-
inﬁnite number of dishes and customers that arrive in sequence in the buﬀet line.
Let Z∗ denote a binary-valued matrix in which the rows are customers and the
columns are the dishes, and where Z∗
nk = 1 if customer n samples dish k. The
ﬁrst customer samples Poisson(α) dishes, where α = B0(Θ) is the total mass
c+n−1, where
of B0. A subsequent customer n samples dish k with probability mk
mk is the number of customers who have previously sampled dish k; that is,
nk ∼ Bernoulli( mk
Z∗
c+n−1). Having sampled from the dishes previously sampled
by other customers, customer n then goes on to sample an additional number
of new dishes determined by a draw from a Poisson(
To derive the IBP from the beta process, consider ﬁrst the distribution
Eq. (40) for n = 0; in this case the base measure is simply B0. Drawing from B ∼
BP(B0) and then drawing Z1 ∼ BeP(B) yields atoms whose locations are dis-
tributed according to a Poisson process with rate B0; the number of such atoms
is Poisson(α). Now consider the posterior distribution after Z1, . . . , Zn−1 have
been observed. The updated base measure is
i=1 Zi. Treat
the discrete component and the continuous component separately. The discrete
component,
i=1 Zi, can be reorganized as a sum over the unique values
of the atoms; let mk denote the number of times the kth atom appears in one of
the previous Zi. We thus obtain draws ωk ∼ Beta((c+n−1)qk, (c+n−1)(1−qk)),
where qk = mk
c+n−1 and thus (under Bernoulli
sampling) this atom appears in Zn with probability mk
c+n−1. From the continu-
ous component,
c+n−1 α) new atoms. Equating
“atoms” with “dishes,” and rows of Z∗ with draws Zn, we have obtained exactly
the probabilistic speciﬁcation of the IBP.

c+n−1. The expected value of ωk is mk

c+n−1 B0, we generate Poisson(

c+n−1 α) distribution.

c

c+n−1 B0 + 1

c+n−1

(cid:80)n−1

c

c

c

(cid:80)n−1

1

c+n−1

5.3 Stick-Breaking Constructions
The stick-breaking representation of the DP is an elegant constructive character-
ization of the DP as a discrete random measure (Chapter ??). This construction
can be viewed in terms of a metaphor of breaking oﬀ lengths of a stick, and it
can also be interpreted in terms of a size-biased ordering of the atoms.
In
this section, we consider analogous representations for the beta process. Draws
B ∼ BP(c, B0) from the beta process are discrete with probability one, which
gives hope that such representations exist.
Indeed, we will show that there
are two stick-breaking constructions of B, one based on a size-biased ordering
of the atoms (Thibaux and Jordan, 2007), and one based on a stick-breaking
representation known as the inverse L´evy measure (Wolpert and Ickstadt, 1998).

29

The size-biased ordering of Thibaux and Jordan (2007) follows straightfor-
wardly from the discussion in Section 5.2. Recall that the Indian buﬀet process
is deﬁned via a sequence of draws from Bernoulli processes. For each draw, a
Poisson number of new atoms are generated, and the corresponding weights in
the base measure B have a beta distribution. This yields the following truncated
representation:

N(cid:88)

Kn(cid:88)

BN =

ωnkδθnk

,

n=1

k=1

where

Kn | c, B0 ∼ Poisson(

c+n−1 α)
ωnk | c ∼ Beta(1, c + n − 1)
θnk | B0 ∼ B0/α.

c

(41)

(42)

for n = 1, . . . ,∞
for k = 1, . . . , Kn

cα

cα

It can be shown that this size-biased construction BN converges to B with prob-
ability one. The expected total weight contributed at step N is
(c+N )(c+N−1),
while the expected total weight remaining, in B − BN , is
c+N . The expected
total weight remaining decreases to zero as N → ∞, but at a relatively slow
rate. Note also that we are not guaranteed that atoms contributed at later
stages of the construction will have small weight—the sizes of the weights need
not be in decreasing order.

The stick-breaking construction of Teh et al. (2007) can be derived from the
inverse L´evy measure algorithm of Wolpert and Ickstadt (1998). This algorithm
starts from the L´evy measure of the beta process, and generates a sequence of
weights of decreasing size using a nonlinear transformation of a one-dimensional
Poisson process to one with uniform rate. In general this approach does not lead
to closed forms for the weights; inverses of the incomplete Beta function need to
be computed numerically. However for the one-parameter beta process (where
c = 1) we do obtain a simple closed form:

(43)

(44)

for k = 1, . . . ,∞

K(cid:88)

BK =

ωkδθk

,

where

k=1

vk | α ∼ Beta(1, α)
k(cid:89)
(1 − vl)
θk | B0 ∼ B0/α.

ωk =

l=1

Again BK → B as K → ∞, but in this case the expected weights decrease
exponentially to zero. Further, the weights are generated in strictly decreasing
order, so we are guaranteed to generate the larger weights ﬁrst.

30

Figure 7: Stick-breaking construction for the DP and the one-parameter BP.
The lengths πi are the weights for the DP and the lengths ωi are the weights
for the BP.

The stick-breaking construction for the one-parameter beta process has an
intriguing connection to the stick-breaking construction for the DP. In par-
ticular, both constructions use the same beta-distributed breakpoints vk; the
diﬀerence is that for the DP we use the lengths of the sticks just broken oﬀ
as the weights while for the beta process we use the remaining lengths of the
sticks. This is depicted graphically in Figure 7.

5.4 Hierarchical Beta Processes
Recall the construction of the hierarchical Dirichlet process: a set of Dirichlet
processes are coupled via a random base measure. A similar construction can
be carried out in the case of the beta process:
let the common base measure
for a set of beta processes be drawn from an underlying beta process (Thibaux
and Jordan, 2007). Under this hierarchical Bayesian nonparametric model, the
featural representations that are chosen for one group will be related to the
featural representations that are used for other groups.

We accordingly deﬁne a hierarchical beta process (HBP) as follows:

B0 | κ, A ∼ BP(κ, A)
Bj | c, B0 ∼ BP(c, B0)
Zji | Bj ∼ BeP(Bj)

(45)

for j ∈ J
for i = 1, . . . , nj,

where J is the set of groups and there are nj individuals in group j. The
hyperparameter c controls the degree of coupling among the groups:
larger
values of c yield realizations Bj that are closer to B0 and thus a greater degree
of overlap among the atoms chosen in the diﬀerent groups.
As an example of the application of the HBP, Thibaux and Jordan (2007)
considered the problem of document classiﬁcation, where there are |J | groups
of documents and where the goal is to classify a new document into one of
these groups. In this case, Zji is a binary vector that represents the presence or
absence in the ith document of each of the words in the vocabulary Θ. The HBP
yields a form of regularization in which the group-speciﬁc word probabilities

31

ω1ω2ω3ω4π4π3π2π1are shrunk towards each other. This can be compared to standard Laplace
smoothing, in which word probabilities are shrunk towards a ﬁxed reference
point. Such a reference point can be diﬃcult to calibrate when there are rare
words in a corpus, and Thibaux and Jordan (2007) showed empirically that the
HBP yielded better predictive performance than Laplace smoothing.

5.5 Applications of the Beta Process
In the following sections we describe a number of applications of the beta process
to hierarchical Bayesian featural models. Note that this is a rather diﬀerent class
of applications than the traditional class of applications of the beta process to
random hazard functions.

5.5.1 Sparse Latent Variable Models
Latent variable models play an essential role in many forms of statistical anal-
ysis. Many latent variable models take the form of a regression on a latent
vector; examples include principal component analysis, factor analysis and in-
dependent components analysis. Paralleling the interest in the regression liter-
ature in sparse regression models, one can also consider sparse latent variable
models, where each observable is a function of a relatively small number of la-
tent variables. The beta process provides a natural way of constructing such
models. Indeed, under the beta process we can work with models that deﬁne
a countably-inﬁnite number of latent variables, with a small, ﬁnite number of
variables being active (i.e., non-zero) in any realization.

Consider a set of n observed data vectors, x1, . . . , xn. We use a beta process
to model a set of latent features, Z1, . . . , Zn, where we capture interactions
among the components of these vectors as follows:

B | c, B0 ∼ BP(c, B0)
Zi | B ∼ BeP(B)

for i = 1, . . . , n.

(46)

As we have seen, realizations of beta and Bernoulli processes can be expressed
as weighted sums of atoms:

∞(cid:88)
∞(cid:88)

k=1

B =

Zi =

ωkδθk

Z∗
ikδθk

.

(47)

k=1

We view θk as parametrizing feature k, while Zi denotes the features that are
active for item i. In particular, Z∗
ik = 1 if feature k is active for item i. The
data point xi is modeled as follows:

yik | H ∼ H

xi | Zi, θ, yi ∼ F{θk,yik}k:Z∗

ik

for k = 1, . . . ,∞

(48)

,

=1

32

ik

=1

where yik is the value of feature k if it is active for item i, and the distribu-
tion F{θk,yik}k:Z∗
depends only on the active features, their values, and their
parameters.

Note that this approach deﬁnes a latent variable model with an inﬁnite
number of sparse latent variables, but for each data item only a ﬁnite number
of latent variables are active. The approach would often be used in a predictive
setting in which the latent variables are integrated out, but if the sparseness
pattern is of interest per se, it is also possible to compute a posterior distribution
over the latent variables.

observation of the linear combination (cid:80)

There are several speciﬁc examples of this sparse latent variable model in the
literature. One example is an independent components analysis model with an
inﬁnite number of sparse latent components (Knowles and Ghahramani, 2007;
Teh et al., 2007), where the latent variables are real-valued and xi is a noisy
ikyikθk. Another example is the
“noisy-or” model of Wood et al. (2006), where the latent variables are binary
and are interpreted as presence or absence of diseases, while the observations xi
are binary vectors indicating presence or absence of symptoms.

k Z∗

5.5.2 Relational Models
The beta process has also been applied to the modeling of relational data (also
known as dyadic data). In the relational setting, data are relations among pairs
of objects (Getoor and Taskar, 2007); examples include similarity judgments
between two objects, protein-protein interactions, user choices among a set of
options, and ratings of products by customers.

We ﬁrst consider the case in which there is a single set of objects and relations
are deﬁned among pairs of objects in that set. Formally, deﬁne an observation as
a relation xij between objects i and j in a collection of n objects. Each object is
modeled using a set of latent features as in Eq. (46) and Eq. (47). The observed
relation xij between objects i and j then has a conditional distribution that is
dependent only on the features active in objects i and j. For example, Navarro
and Griﬃths (2007) modeled subjective similarity judgments between objects
jk; note that this is
a weighted sum of features active in both objects. Chu et al. (2006) modeled
high-throughput protein-protein interaction screens where the observed bind-
ing aﬃnity of proteins i and j is related to the number of overlapping features
jk, with each feature interpreted as a potential protein complex con-
sisting of proteins containing the feature. G¨or¨ur et al. (2006) proposed a non-
parametric elimination by aspects choice model where the probability of a user
choosing object i over object j is modeled as proportional to a weighted sum,
jk), across features active for object i that are not active for
object j. Note that in these examples, the parameters of the model, θk, are the
atoms of the beta process.

i and j as normally distributed with mean (cid:80)∞
(cid:80)∞
(cid:80)∞

ik(1 − Z∗

k=1 θkZ∗

k=1 Z∗

k=1 θkZ∗

ikZ∗

ikZ∗

Relational data involving separate collections of objects can be modeled with
the beta process as well. Meeds et al. (2007) modeled movie ratings, where the
collections of objects are movies and users, and the relational data consists of

33

ratings of movies by users. The task is to predict the ratings of movies not
yet rated by users, using these predictions to recommend new movies to users.
These tasks are called recommender systems or collaborative ﬁltering. Meeds
et al. (2007) proposed a featural model where movies and users are modeled
using separate IBPs. Let Z∗ be the binary matrix of movie features and Y ∗
the matrix of user features. The rating of movie i by user j is modeled as
jl. Note that this dyadic
model cannot be represented using two independent beta processes, since there
is a parameter θkl for each combination of features in the two IBPs. The question
of what random measure underlies this model is an interesting one.

normally distributed with mean (cid:80)∞

(cid:80)∞

l=1 θklZ∗

ikY ∗

k=1

6 Semiparametric Models
The nonparametric priors introduced in previous sections can be combined with
more traditional ﬁnite-dimensional priors, as well as hierarchies of such priors.
In the resulting semiparametric models, the object of inference may be the
ﬁnite-dimensional parameter, with the nonparametric component treated as a
nuisance parameter to be integrated out. In other cases, the ﬁnite-dimensional
parameters are to be integrated out and aspects of the nonparametric component
are the inferential focus. In this section we describe two such semiparametric
models based on the HDP. The ﬁrst model couples the stick-breaking represen-
tation of the HDP with a Gaussian hierarchy, while the other is based on the
Chinese restaurant franchise representation of the HDP.

6.1 Hierarchical DPs with Random Eﬀects
An important characteristic of the HDP is that the same atoms appear in dif-
ferent DPs, allowing clusters to be shared across the diﬀerent groups. The hier-
archical DP with random eﬀects (HDP+RE) model of Kim and Smyth (2007)
generalizes the HDP by allowing atoms in diﬀerent DPs to diﬀer from each other
to better capture group-speciﬁcity of cluster parameters. This model is based
on the stick-breaking representation for HDPs. We begin with the standard
representation for the common random base measure G0 ∼ DP(γ, H):

β | γ ∼ GEM(γ)
k | H ∼ H
∞(cid:88)
θ∗∗

G0 =

βkδθ∗∗

k

k=1

for k = 1, . . . ,∞

(49)

.

34

For each group j ∈ J , the weights and atoms for the group-speciﬁc Gj diﬀer
from G0 in the following way:

πj | β ∼ DP(α, β)
jk | θ∗∗
θ∗

k ∼ Tθ∗∗
∞(cid:88)
Gj =

k

πjkδθ∗

jk

,

for j ∈ J
for k = 1, . . . ,∞

(50)

k=1

where Tθ is a distribution centered at θ; for example, Tθ might be a normal
distribution with mean θ.

Kim and Smyth (2007) used the HDP+RE model to model bumps in func-
tional magnetic resonance imaging (fMRI) data.
fMRI analyses report areas
of high metabolic activity in the brain that are correlated with external stim-
uli in an attempt to discover the function of local brain regions. Such areas
of activities often show up as bumps in fMRI images, and each bump can be
modeled well using a normal density. An fMRI image then consists of multiple
bumps and can be modeled with a DP mixture. Each individual brain might
have slightly diﬀerent structure and might react diﬀerently to the same stimuli,
while each fMRI machine has diﬀerent characteristics. The HDP+RE model
naturally captures such variations while sharing statistical strength across indi-
viduals and machines.

6.2 Analysis of Densities and Transformed DPs
In this section we describe another approach to introducing group-speciﬁc pa-
rameters within the DP framework. The common base measure G0 is still given
a DP prior as in Eq. (49), while the group-speciﬁc random measures are deﬁned
diﬀerently:

∞(cid:88)

H0 =

βkTθ∗∗
Gj | H0 ∼ DP(α, H0)

k=1

k

(51)

for j ∈ J .

In the particular case in which H and Tθ are normal distributions with ﬁxed
variances, this model has been termed the analysis of densities (AnDe) model
by Tomlinson and Escobar (2003), who used it for sharing statistical strength
among multiple density estimation problems.

Sudderth et al. (2008) called the model given by Eq. (49) and Eq. (51) a
transformed DP. The transformed DP is very similar to an HDP, the diﬀerence
being that the atoms in G0 are replaced by distributions parametrized by the
atoms. If these distributions are smooth the measures Gj will not share atoms
as in the HDP. Instead each atom in Gj is drawn from Tθ∗∗
with probability βk.
Identifying an atom of Gj with θ∗∗
k , the Chinese restaurant franchise representa-
tion for the HDP can be generalized to the transformed DP. We have customers
(draws from Gj) going into restaurants (Gj) and sitting around tables (draws

k

35

from H0), while tables are served dishes (atoms in G0) from a franchise-wide
menu (G0). In the HDP the actual dish served at the diﬀerent tables that order
the same dish are identical. For the transformed DP the dishes that are served
at diﬀerent tables ordering the same dish on the menu can take on distinct
values.

Sudderth et al. (2008) used the transformed DP as a model for visual scene
analysis that can simultaneously segment, detect and recognize objects within
the scenes. Each image is ﬁrst preprocessed into a set of low-level descriptors
of local image appearances, and Eq. (49) and Eq. (51) are completed with a
mixture model for these descriptors:

θji | Gj ∼ Gj
xji | θji ∼ Fθji

,

for j ∈ J and i = 1, . . . , nj

(52)

where xji is one of nj image descriptors in image j and Fθji
over image descriptors parameterized by θji.

is a distribution

The Chinese restaurant franchise representation of the transformed DP trans-
lates to a hierarchical representation of visual scenes, with scenes consisting of
multiple objects and objects consisting of descriptors of local image appear-
ances. To see this, note that customers (xji) are clustered into tables (object
instances), and tables are served dishes from a global menu (each object in-
stance belongs to an object category). There could be multiple tables in the
same restaurant serving variations (diﬀerent “seasonings”) on a given dish from
the global menu. This corresponds to the fact that there could be multiple in-
stances of the same object category in a single visual scene, with each instance
being in a diﬀerent location or having diﬀerent poses or lighting conditions (thus
yielding transformed versions of an object category template).

7

Inference for Hierarchical Bayesian Nonpara-
metric Models

In this section we discuss algorithmic aspects of inference for the hierarchical
Bayesian nonparametric models that we have discussed in earlier sections. Our
treatment will be brief and selective; in particular, we focus on relatively simple
algorithms that help to convey basic methodology and provide a sense of some
of the options that are available. An underlying theme of this section is that
the various mathematical representations available for nonparametric models—
including stick-breaking representations, urn models and truncations—can be
combined in various ways to yield a wide range of possible algorithmic imple-
mentations.

While we focus on sampling-based inference methods throughout this sec-
tion, we also note that there is a growing literature on variational methods for in-
ference in hierarchical Bayesian nonparametric models; examples include Liang
et al. (2007); Sudderth and Jordan (2009) and Teh et al. (2008).

36

Inference for Hierarchical Dirichlet Processes

7.1
We begin by considering posterior inference for a simple HDP mixture model. In
this model, the random measures Gj are drawn from an HDP model according
to Eq. (1), and this HDP prior is completed as follows:

θji | Gj ∼ Gj
xji | θji ∼ Fθji

,

for i = 1, . . . , nj

(53)

where the ith observation in the jth group is denoted xji and where this obser-
vation is drawn from a distribution Fθji
indexed by θji. The latent parameter
θji is drawn from Gj and can be viewed as indexing the mixture component
associated with the data point xji. We shall assume that H is conjugate to
Fθ for simplicity. Nonconjugate models can be treated by adapting techniques
from the DP mixture literature (cf. Neal, 2000).

Teh et al. (2006) presented sampling algorithms for the HDP mixture model
based on both the CRF representation and the stick-breaking representation.
In the following section we describe the CRF-based sampler. We then turn to
an alternative sampler that is based on the posterior representation of the HDP
described in Section 2.3.

7.1.1 Chinese Restaurant Franchise Sampler
Recall our notation for the CRF representation of the HDP. Customer i in
restaurant j is associated with an iid draw from Gj and sits at table tji. Table t
in restaurant j is associated with an iid draw from G0 and serves a dish kjt from
a franchise-wide menu. Dish k is associated with an iid draw from H. If H is an
absolutely continuous measure then each such dish is unique with probability
one. There are njtk customers in restaurant j sitting at table t and eating dish
k, and there are mjk tables in restaurant j serving dish k.

Given this setup, we describe a Gibbs sampler in which the table and dish
assignment variables are iteratively sampled conditioned on the state of all other
variables. The variables consist of {tji}j∈J ,i=1,...,nj
and {kjt}j∈J ,t=1,...,mj·. The
parameters θji are integrated out analytically (recall our assumption of conju-
gacy). Consider the assignment of customer i in restaurant j to a table tji. To
resample tji we make use of exchangeability and imagine customer i being the
last customer to enter restaurant j. The customer can sit at an already occupied
table, can sit at a new table and be served an existing dish, or can sit at a new
table and be served a new dish. The probabilities of these events are:



with probability ∝ n¬ji
tji = t
jt·
n¬ji
j·· +α
with probability ∝ α
tji = tnew, kjtnew = k
n¬ji
j·· +α
tji = tnew, kjtnew = knew with probability ∝ α
n¬ji
j·· +α

37

fkjt
m¬ji
·k
m¬ji
·· +γ
γ
m¬ji

·· +γ

({xji})

fk({xji})
fknew({xji}),

(54)

where tnew and knew denote a new table and new dish, respectively, and where
superscript ¬ji denotes counts in which customer i in restaurant j is removed
from the CRF (if this empties a table we also remove that table from the CRF
along with the dish served on it). The fractional terms are the conditional
priors given by the CRF in Eq. (7) and Eq. (8), and fk({xji}) is deﬁned using
the following general notation:

(cid:90)
(cid:90)

h(θ) (cid:89)
h(θ) (cid:89)

j(cid:48)i(cid:48)∈Dk∪D

j(cid:48)i(cid:48)∈Dk\D

fk({xji}ji∈D) =

fθ(xj(cid:48)i(cid:48))dθ

,

fθ(xj(cid:48)i(cid:48))dθ

(55)

where D is an arbitrary index set, where Dk = {j(cid:48)i(cid:48) : kj(cid:48)t
j(cid:48) i(cid:48) = k} denotes the
set of indices of data items currently associated with dish k, and where h(·) and
fθ(·) denote the densities of H and Fθ respectively. In particular, fk({xji}) is
the marginal conditional probability of the singleton data point xji in cluster k,
given all of the other data points currently assigned to cluster k.

The Gibbs update for the dish kjt served at table t in restaurant j is derived

similarly. The probabilities of the relevant events in this case are:
fk({xji : tji = t})
fknew({xji : tji = t}).

with probability ∝ m¬jt
·k
m¬jt
·· +γ
γ
m¬jt

knew with probability ∝

kjt =

·· +γ

k

(56)

While the computational cost of the Gibbs updates is generally dominated
by the computation of the marginal conditional probabilities fk(·), the number
of possible events that can occur at one Gibbs step is one plus the total number
of tables or dishes in all restaurants that are ancestors of j, and this number
can be large in deep or wide hierarchies.

A drawback of the CRF sampler is that it couples sampling in the various
restaurants (since all DPs are integrated out). This coupling makes deriving a
CRF sampler for certain models (e.g. the HDP-HMM) diﬃcult. An alternative
is to construct samplers that use a mixed representation—some DPs in stick-
breaking representation and some in CRP representation—and thereby decouple
the restaurants (Teh et al., 2006).

The CRF-based sampler can be easily extended to arbitrary hierarchies.
It can also be extended to the hierarchical Pitman-Yor process discussed in
Section 4.

7.1.2 Posterior Representation Sampler
In Section 2.3 we showed that the posterior of the HDP consists of a discrete
part corresponding to mixture components associated with data and a contin-
uous part corresponding to components not associated with data. This repre-
sentation can be used to develop a sampler which represents only the discrete
part explicitly. In particular, referring to Eq. (10) and Eq. (12), the posterior

38

representation sampler maintains only the weights β and {πj}j∈J . (The atoms
{θ∗∗
k }k=1,...,K can be integrated out in the conjugate setting.) We also make use
of cluster index variables zji, deﬁned so that θji = θ∗∗
in the
zji
CRF representation).
The sampler iterates between two phases: the sampling of the cluster indices
{zji}, and the sampling of the weights β and {πj}. The sampling of the cluster
indices is a simple variation on the Gibbs updates in the CRF sampler described
in Section 7.1.1. In particular, we deﬁne the following Gibbs conditionals:

(i.e., zji = kjtji

(cid:40)k

zji =

with probability ∝ πjkfk({xji})

knew with probability ∝ πj0fknew({xji}).

(57)

(58)

If a new component knew is chosen, the corresponding atom is instantiated in
the sampler. Speciﬁcally, the weights corresponding to this new atom can be
generated as follows:

for j ∈ J

v0 | γ ∼ Beta(γ, 1)
K+1) = (β0v0, β0(1 − v0))

, βnew

(βnew
vj | α, β0, v0 ∼ Beta(αβ0v0, αβ0(1 − v0))
j0 , πnew

0

(πnew

j K+1) = (πj0vj, πj0(1 − vj)).
Finally we set zji = K + 1 and increment K.
The second phase resamples the weights {πj}j∈J and β conditioned on the
cluster indices {zji}. The approach is to ﬁrst integrate out the random measures,
leaving a CRP representation as in Section 2.2, then the weights {πj}j∈J and
β can be sampled conditionally on the state of the CRF using Eq. (10) and
Eq. (12). Because we are conditioning on {zji}, and customers with diﬀerent
values of zji cannot be assigned to the same table, each restaurant eﬀectively
gets split into independent “sub-restaurants,” one for each value of k.
(See
also the related direct assignment sampler in Teh et al. (2006).) Let nj·k be
the number of observations in group j assigned to component k, and let mjk
be the random number of tables in a sub-restaurant with nj·k customers and
concentration parameter αβk. The {mjk} are mutually independent and thus
a draw for each of them can be simulated using the CRP. We can now sample
the β and {πj} using Eq. (10) and Eq. (12).

Inference for HDP Hidden Markov Models

7.2
The posterior representation sampler of Section 7.1.2 can also be used to derive
a Gibbs sampler for the HDP-HMM. Consider the formulation of the HDP-
HMM given in Eq. (23) and Eq. (24) where we make use of a sequence of latent
indicator variables z1, . . . , zτ . We again assume that H is conjugate to Fθ.
Note that the posterior of the HDP prior for the model (given z1, . . . , zτ ) can
be decomposed into a discrete part consisting of K atoms (corresponding to the
K states currently visited by z1, . . . , zτ ), as well as a continuous part consisting
of unused atoms. The weights on the K atoms (equivalently the transition

39

probabilities among the K states currently used by the HDP-HMM) can be
constructed from a CRF representation of the HDP:

(β0, β1, . . . , βK) ∼ Dirichlet(γ, m·1, . . . , m·K)

(59)

for j = 1, . . . , K,

(πj0, πj1, . . . , πjK) ∼ Dirichlet(αβ0, αβ1 + nj·1, . . . , αβK + nj·K)
where nj·k is the number of transitions from state j to state k (equivalently the
number of customers eating dish k in restaurant j), while m·k is the number of
tables serving dish k in the CRF representation of the HDP. The conditional
(cid:40)k
probabilities for the Gibbs update of zt are as follows:
with probability ∝ πzt−1kπkzt+1
knew with probability ∝ πzt−10βzt+1

fk({xt})
fknew({xt}).

zt =

(60)

The three factors on the right-hand side are the probability of transitioning into
the current state, the probability of transitioning out of the current state, and
the conditional probability of the current observation xt respectively. The βzt+1
factor arises because transitions from the new state knew have not been observed
before so we need to use the conditional prior mean β. The weights β and
transition probabilities πj can be updated as for the posterior representation
sampler for plain HDPs.

This simple Gibbs sampler can converge very slowly due to strong depen-
dencies among the latent states (Scott, 2002). To obtain a faster algorithm we
would like to update the latent states in a block via the forward-backward al-
gorithm for HMMs; the traditional form of this algorithm cannot, however, be
applied directly to the HDP-HMM since there are an inﬁnite number of possi-
ble states. The solution is to limit the number of states to a ﬁnite number so
that the forward-backward algorithm becomes feasible. Fox et al. (2009) pro-
posed doing this via a truncation of the stick-breaking process (cf. Ishwaran and
James, 2001), while Van Gael et al. (2008) proposed a slice sampling approach
which adaptively limits the number of states to a ﬁnite number (Neal, 2003;
Walker, 2007).

Inference for Beta Processes

7.3
In this section we describe a Gibbs sampler for the beta process latent variable
model described in Section 5.5.1. This sampler is based on the stick-breaking
representation of the beta process.
Recall that the model is deﬁned in terms of a set of feature weights {ωk}k=1,...,∞
and the atoms (feature parameters) {θk}k=1,...,∞. Moreover, corresponding to
each data item xi, we have a set of binary feature “activities” {Z∗
ik}k=1,...,∞
and latent feature values {yik}k=1,...,∞. The observed data item xi depends on
{θk, yik}k:Z∗
The conditional distributions deﬁning the model are given in Eq. (44) and
ik = 1| ωk) = ωk. Gibbs sampling in this model is straight-
Eq. (48), where p(Z∗
forward except for a few diﬃculties which we describe below along with their
resolution.

=1.

ik

40

The main diﬃculty with a Gibbs sampler is that there are an inﬁnite number
of random variables that need to be sampled. To circumvent this problem,
Teh et al. (2007) propose to use slice sampling (Neal, 2003; Walker, 2007) to
adaptively truncate the representation to a ﬁnite number of features. Consider
an auxiliary variable s with conditional distribution:

(cid:20)

(cid:21)

K(cid:89)

s| Z∗,{ωk}k=1,...,∞ ∼ Uniform

0, min
k:∃i,Z∗

ik

=1

ωk

,

(61)

where the supremum in the range of s is the smallest feature weight ωk among
the currently active features. Conditioned on the current state of the other
variables a new value for s can easily be sampled. Conditioned on s, features
for which ωk < s are forced to be inactive since making them active would
make s lie outside its range. This means that we only need to update the ﬁnite
number of features for which ωk > s. This typically includes all the active
features, along with a small number of inactive features (needed for the sampler
to explore the use of new features).

A related issue concerns the representation of the model within the ﬁnite
memory of the computer. Using the auxiliary variable s it is clear that we need
only represent features 1, . . . , K, where K is such that ωK+1 < s; that is, the
model is truncated after feature K. As the values of s and the feature weights
change over the course of Gibbs sampling this value of K changes as well. If
K is decreased we simply delete the last few features, while if K is increased
we sample the variables ωk, θk and yik corresponding to these new features
from their conditional distributions given the current state of the represented
features.

The ﬁnal issue is the problem of sampling the feature weights ω1, . . . , ωK.
Unlike the case of DPs, it is easier in this case to work with the weights directly
instead of the stick-breaking variables vk. In particular, Teh et al. (2007) showed
that the joint probability for the weights is:

p(ω1, . . . , ωK) = I(0 ≤ ωK ≤ ··· ≤ ω1 ≤ 1)αKωα

K

ω−1
k ,

(62)

where I(·) = 1 if the predicate is true and 0 otherwise. For k = 1, . . . , K − 1 the
conditional probability of ωk given the other variables can be computed from
Eq. (62) and the conditional probability of Z∗
nk given ωk. For ωK we
also have to condition on Z∗
ik = 0 for all i and k > K; this probability can be
computed using the L´evy-Khintchine representation for the beta process (Teh
et al., 2007).

1k, . . . , Z∗

k=1

Inference for Hierarchical Beta Processes

7.4
In this section we present an inference algorithm for the hierarchical beta pro-
cess given in Eq. (45). The observed data are the variables Zji; these binary
vectors denote (in the language of document classiﬁcation) the presence or ab-
sence of words in document i of group j. The underlying measure space Θ is

41

interpreted as the vocabulary. (Each element in Θ is referred to as a “word”).
Let θ1, . . . , θK ∈ Θ denote the words that are observed among the documents.
That is, these are the θ ∈ Θ such that Zji(θ) = 1 for some i and j.
Because both the beta and Bernoulli processes are completely random mea-
sures, the posterior over B0 and Bj for j ∈ J decomposes into a discrete
part over the observed vocabulary {θ1, . . . , θK} and a continuous part over
Θ\{θ1, . . . , θK}. The discrete part further factorizes over each observed word
θk. Thus it is suﬃcient to focus separately on inference for each observed word
and for the continuous part corresponding to unobserved words.

For a ﬁxed θk, let a = A(θk), ω0 = B0(θk), ωj = Bj(θk) and zji = Zji(θk).
The slice of the HBP corresponding to θk has the following joint distribution:

ω0 | c0, a ∼ Beta(c0a, c0(1 − a))
ωj | cj, ω0 ∼ Beta(cjω0, cj(1 − ω0))

zji | ωj ∼ Bernoulli(ωj)

(63)

for j ∈ J
for i = 1, . . . , nj.

Note that the prior over ω0 is improper if A is continuous and a = 0. This beta
hierarchy is a special case of the ﬁnite Dirichlet hierarchy of Eq. (10) and Eq. (12)
and it is straightforward to use the posterior representation sampler described
in Section 7.1 to sample from the posterior given the observed zji. Thibaux
and Jordan (2007) described an alternative where the ωj are integrated out and
rejection sampling is used to sample from ω0.

Finally, we consider the continuous part of the posterior. This component
is not simply the prior, since we have to condition on the fact that no words in
Θ\{θ1, . . . , θK} have been observed among the documents. Thibaux and Jordan
(2007) solved this problem by noting that the posterior factors over the levels
indexed by n in the size-biased ordering in Eq. (42). Focusing on each level
separately, they derived a posterior distribution on the number of atoms in each
level, combining this with the posterior over the level-speciﬁc weights to obtain
the overall posterior.

8 Discussion
Our goal in this chapter has been to place hierarchical modeling in the same
central role in Bayesian nonparametrics that it plays in other areas of Bayesian
statistics. Indeed, one of the principal arguments for hierarchical modeling in
parametric statistics is that it provides control over the large numbers of degrees
of freedom that arise, for example, in random eﬀects models. Such an argument
holds a fortiori in the nonparametric setting.

Nonparametric priors generally involve hyperparameters, some of which are
ﬁnite-dimensional and some of which are inﬁnite-dimensional. Sharing the ﬁnite-
dimensional parameter among multiple draws from such a prior is a natural
modeling strategy that mimics classical hierarchical modeling concepts.
It is
our contention, however, that this form of control is far too limited, and that
the inﬁnite-dimensional parameters should generally also be shared. We have

42

made this point principally by considering examples in applied problem domains.
In domains such as computational vision, information retrieval and genetics,
nonparametric models provide natural descriptions of the complex objects under
study; in particular, it is natural to describe an image, a document or a genome
as the realization of a stochastic process. Now, in considering collections of
such objects it is natural to want to share details of the realization among the
objects in the collection—we wish to share parts of objects, features, recurring
phrases and motifs. This can be achieved by coupling multiple draws from a
nonparametric prior via their inﬁnite-dimensional parameters.

Another advantage of hierarchical modeling in the classical setting is that
it expands the repertoire of distributional forms that can be considered. For
example, heavy-tailed distributions can be obtained by placing a prior on the
scale parameter of lighter-tailed distributions. Although this point has been
little explored to date in the nonparametric setting, we expect that it will be
a fruitful direction for further research. In particular, there are stringent com-
putational constraints that limit the nonparametric repertoire, and hierarchical
constructions oﬀer one way forward. Indeed, as we have seen, computationally
oriented constructions such as urn models and stick-breaking representations
often carry over naturally to hierarchical nonparametric models.

Finally, it is worth noting a diﬃculty that is raised by hierarchical mod-
eling. Although Bayesian hierarchies help to control hyperparameters, they
do not remove the need to specify distributions for hyperparameters. Indeed,
when hyperparameters are placed high in a hierarchy it can be diﬃcult to give
operational meaning to such hyperparameters. One approach to coping with
this issue involves considering the marginal probabilities that are induced by a
nonparametric prior. For example, we argued that the marginals induced by a
Pitman-Yor prior exhibit long tails that provide a good match to the power-law
behavior found in textual data and image statistics. Further research is needed
to develop this kind of understanding for a wider range of hierarchical Bayesian
nonparametric models and problem domains.

8.1 Acknowledgements
We would like to thank David Blei, Jan Gasthaus, Sam Gershman, Tom Grif-
ﬁths, Kurt Miller, Vinayak Rao and Erik Sudderth for their helpful comments
on the manuscript.

References
Antoniak, C. E. (1974). Mixtures of Dirichlet processes with applications to

Bayesian nonparametric problems. Annals of Statistics, 2(6):1152–1174.

Beal, M. J., Ghahramani, Z., and Rasmussen, C. E. (2002). The inﬁnite hid-
den Markov model. In Advances in Neural Information Processing Systems,
volume 14.

43

Blei, D. M., Griﬃths, T. L., Jordan, M. I., and Tenenbaum, J. B. (2004). Hier-
archical topic models and the nested Chinese restaurant process. In Advances
in Neural Information Processing Systems, volume 16.

Blei, D. M., Ng, A. Y., and Jordan, M. I. (2003). Latent Dirichlet allocation.

Journal of Machine Learning Research, 3:993–1022.

Chen, S. F. and Goodman, J. T. (1998). An empirical study of smoothing
techniques for language modeling. Technical Report TR-10-98, Computer
Science Group, Harvard University.

Chen, S. F. and Goodman, J. T. (1999). An empirical study of smoothing
techniques for language modeling. Computer Speech and Language, 13(4):359–
393.

Chou, K. C., Willsky, A. S., and Benveniste, A. (1994). Multiscale recursive
estimation, data fusion, and regularization. IEEE Transactions on Automatic
Control, 39(3):464–478.

Chu, W., Ghahramani, Z., Krause, R., and Wild, D. L. (2006).

Identifying
protein complexes in high-throughput protein interaction screens using an
inﬁnite latent feature model. In BIOCOMPUTING: Proceedings of the Paciﬁc
Symposium.

Cowans, P. (2004). Information retrieval using hierarchical Dirichlet processes.
In Proceedings of the Annual International Conference on Research and De-
velopment in Information Retrieval, volume 27, pages 564–565.

Cowans, P. (2006). Probabilistic Document Modelling. PhD thesis, University

of Cambridge.

Duan, J. A., Guindani, M., and Gelfand, A. E. (2007). Generalized spatial

Dirichlet process models. Biometrika, 94(4):809–825.

Erosheva, E. (2003). Bayesian estimation of the grade of membership model.

In Bayesian Statistics, volume 7, pages 501–510.

Fei-Fei, L. and Perona, P. (2005). A Bayesian hierarchical model for learning
natural scene categories. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition.

Ferguson, T. S. (1973). A Bayesian analysis of some nonparametric problems.

Annals of Statistics, 1(2):209–230.

Finkel, J. R., Grenager, T., and Manning, C. D. (2007). The inﬁnite tree.
In Proceedings of the Annual Meeting of the Association for Computational
Linguistics.

Fox, E., Sudderth, E., Jordan, M. I., and Willsky, A. (2009). An HDP-HMM for
systems with state persistence. In Advances in Neural Information Processing
Systems, volume 21, Cambridge, MA. MIT Press.

44

Getoor, L. and Taskar, B., editors (2007). Introduction to Statistical Relational

Learning. MIT Press.

Ghahramani, Z., Griﬃths, T. L., and Sollich, P. (2007). Bayesian nonparametric
latent feature models (with discussion and rejoinder). In Bayesian Statistics,
volume 8.

Goldwater, S., Griﬃths, T., and Johnson, M. (2006a). Interpolating between
types and tokens by estimating power-law generators. In Advances in Neural
Information Processing Systems, volume 18.

Goldwater, S., Griﬃths, T. L., and Johnson, M. (2006b). Contextual depen-
dencies in unsupervised word segmentation. In Proceedings of the 21st Inter-
national Conference on Computational Linguistics and 44th Annual Meeting
of the Association for Computational Linguistics.

G¨or¨ur, D., J¨akel, F., and Rasmussen, C. E. (2006). A choice model with in-
ﬁnitely many latent features. In Proceedings of the International Conference
on Machine Learning, volume 23.

Griﬃths, T. L. and Ghahramani, Z. (2006). Inﬁnite latent feature models and
In Advances in Neural Information Processing

the Indian buﬀet process.
Systems, volume 18.

Hiemstra, D. and Kraaij, W. (1998). Twenty-one at TREC-7: Ad-hoc and

cross-language track. In Text REtrieval Conference, pages 174–185.

Hjort, N. L. (1990). Nonparametric Bayes estimators based on beta processes

in models for life history data. Annals of Statistics, 18(3):1259–1294.

Ho, M. W., James, L. F., and Lau, J. W. (2006). Coagulation fragmentation laws
induced by general coagulations of two-parameter Poisson-Dirichlet processes.
http://arxiv.org/abs/math.PR/0601608.

Huang, S. and Renals, S. (2007). Hierarchical Pitman-Yor language models for
ASR in meetings. In Proceedings of the IEEE Workshop on Automatic Speech
Recognition and Understanding, volume 10.

Huang, X., Acero, A., and Hon, H.-W. (2001). Spoken Language Processing.

Prentice-Hall, Upper Saddle River, NJ.

Ishwaran, H. and James, L. F. (2001). Gibbs sampling methods for stick-
breaking priors. Journal of the American Statistical Association, 96(453):161–
173.

Johnson, M., Griﬃths, T. L., and Goldwater, S. (2007). Adaptor grammars: A
framework for specifying compositional nonparametric Bayesian models. In
Advances in Neural Information Processing Systems, volume 19.

Kim, S. and Smyth, P. (2007). Hierarchical dirichlet processes with random

eﬀects. In Advances in Neural Information Processing Systems, volume 19.

45

Kingman, J. F. C. (1967). Completely random measures. Paciﬁc Journal of

Mathematics, 21(1):59–78.

Kivinen, J., Sudderth, E., and Jordan, M. I. (2007a).

Image denoising with
nonparametric hidden Markov trees. In IEEE International Conference on
Image Processing (ICIP), San Antonio, TX.

Kivinen, J., Sudderth, E., and Jordan, M. I. (2007b). Learning multiscale repre-
sentations of natural scenes using Dirichlet processes. In IEEE International
Conference on Computer Vision (ICCV), Rio de Janeiro, Brazil.

Knowles, D. and Ghahramani, Z. (2007).

Inﬁnite sparse factor analysis and
inﬁnite independent components analysis.
In International Conference on
Independent Component Analysis and Signal Separation, volume 7 of Lecture
Notes in Computer Science. Springer.

Liang, P., Petrov, S., Jordan, M. I., and Klein, D. (2007). The inﬁnite PCFG
In Proceedings of the Conference on

using hierarchical Dirichlet processes.
Empirical Methods in Natural Language Processing.

MacEachern, S., Kottas, A., and Gelfand, A. (2001). Spatial nonparametric
Bayesian models. Technical Report 01-10, Institute of Statistics and Deci-
sion Sciences, Duke University. http://ftp.isds.duke.edu/WorkingPapers/01-
10.html.

MacKay, D. and Peto, L. (1994). A hierarchical Dirichlet language model.

Natural Language Engineering.

Manning, C. D. and Sch¨utze, H. (1999). Foundations of Statistical Natural

Language Processing. The MIT Press.

Meeds, E., Ghahramani, Z., Neal, R. M., and Roweis, S. T. (2007). Modeling
dyadic data with binary latent factors. In Advances in Neural Information
Processing Systems, volume 19.

Navarro, D. J. and Griﬃths, T. L. (2007). A nonparametric Bayesian method
for inferring features from similarity judgements. In Advances in Neural In-
formation Processing Systems, volume 19.

Neal, R. M. (2000). Markov chain sampling methods for Dirichlet process mix-
ture models. Journal of Computational and Graphical Statistics, 9:249–265.

Neal, R. M. (2003). Slice sampling. Annals of Statistics, 31:705–767.
Oliva, A. and Torralba, A. (2001). Modeling the shape of the scene: A holistic
International Journal of Computer

representation of the spatial envelope.
Vision, 42(3):145–175.

Perman, M., Pitman, J., and Yor, M. (1992). Size-biased sampling of Pois-
son point processes and excursions. Probability Theory and Related Fields,
92(1):21–39.

46

Pitman, J. (2002). Combinatorial stochastic processes. Technical Report 621,
Department of Statistics, University of California at Berkeley. Lecture notes
for St. Flour Summer School.

Pitman, J. and Yor, M. (1997). The two-parameter Poisson-Dirichlet distribu-
tion derived from a stable subordinator. Annals of Probability, 25:855–900.
Pritchard, J., Stephens, M., and Donnelly, P. (2000). Inference of population

structure using multilocus genotype data. Genetics, 155:945–959.

Rabiner, L. (1989). A tutorial on hidden Markov models and selected applica-

tions in speech recognition. Proceedings of the IEEE, 77:257–285.

Robertson, S. E., Walker, S., Hancock-Beaulieu, M., Gull, A., and Lau, M.

(1992). Okapi at TREC. In Text REtrieval Conference, pages 21–30.

Salton, G. and McGill, M. (1983). An Introduction to Modern Information

Retrieval. McGraw-Hill, New York.

Scott, S. L. (2002). Bayesian methods for hidden Markov models: Recursive
computing in the 21st century. Journal of the American Statistical Associa-
tion, 97(457):337–351.

Sethuraman, J. (1994). A constructive deﬁnition of Dirichlet priors. Statistica

Sinica, 4:639–650.

Stephens, M., Smith, N., and Donnelly, P. (2001). A new statistical method for
haplotype reconstruction from population data. American Journal of Human
Genetics, 68:978–989.

Sudderth, E. and Jordan, M. I. (2009). Shared segmentation of natural scenes
using dependent Pitman-Yor processes. In Advances in Neural Information
Processing Systems, volume 21.

Sudderth, E., Torralba, A., Freeman, W., and Willsky, A. (2008). Describing
visual scenes using transformed objects and parts. International Journal of
Computer Vision, 77.

Teh, Y. W. (2006a). A Bayesian interpretation of interpolated Kneser-Ney.
Technical Report TRA2/06, School of Computing, National University of
Singapore.

Teh, Y. W. (2006b). A hierarchical Bayesian language model based on Pitman-
Yor processes. In Proceedings of the 21st International Conference on Com-
putational Linguistics and 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 985–992.

Teh, Y. W., G¨or¨ur, D., and Ghahramani, Z. (2007). Stick-breaking construction
for the Indian buﬀet process. In Proceedings of the International Conference
on Artiﬁcial Intelligence and Statistics, volume 11.

47

Teh, Y. W., Jordan, M. I., Beal, M. J., and Blei, D. M. (2006). Hierar-
chical Dirichlet processes. Journal of the American Statistical Association,
101(476):1566–1581.

Teh, Y. W., Kurihara, K., and Welling, M. (2008). Collapsed variational in-
In Advances in Neural Information Processing Systems,

ference for HDP.
volume 20.

Thibaux, R. and Jordan, M. I. (2007). Hierarchical beta processes and the Indian
In Proceedings of the International Workshop on Artiﬁcial

buﬀet process.
Intelligence and Statistics, volume 11.

Tomlinson, G. and Escobar, M. (2003). Analysis of densities. Talk given at the

Joint Statistical Meeting.

Van Gael, J., Saatci, Y., Teh, Y. W., and Ghahramani, Z. (2008). Beam sam-
pling for the inﬁnite hidden Markov model. In Proceedings of the International
Conference on Machine Learning, volume 25.

Walker, S. G. (2007). Sampling the Dirichlet mixture model with slices. Com-

munications in Statistics - Simulation and Computation, 36:45.

Wolpert, R. L. and Ickstadt, K. (1998). Simulations of l´evy random ﬁelds. In
Practical Nonparametric and Semiparametric Bayesian Statistics, pages 227–
242. Springer-Verlag.

Wood, F., Griﬃths, T. L., and Ghahramani, Z. (2006). A non-parametric
Bayesian method for inferring hidden causes. In Proceedings of the Conference
on Uncertainty in Artiﬁcial Intelligence, volume 22.

Wooters, C. and Huijbregts, M. (2007). The ICSI RT07s speaker diarization

system. In Lecture Notes in Computer Science. Springer.

Xing, E. P., Jordan, M. I., and Sharan, R. (2007). Bayesian haplotype inference

via the Dirichlet process. Journal of Computational Biology, 14:267–284.

Xing, E. P. and Sohn, K. (2007). Hidden Markov Dirichlet process: Modeling

genetic recombination in open ancestral space. Bayesian Analysis, 2(2).

Xing, E. P., Sohn, K., Jordan, M. I., and Teh, Y. W. (2006). Bayesian multi-
population haplotype inference via a hierarchical Dirichlet process mixture. In
Proceedings of the International Conference on Machine Learning, volume 23.

48

