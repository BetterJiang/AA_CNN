The IBP Compound Dirichlet Process

and its Application to Focused Topic Modeling

Sinead Williamson
Department of Engineering, University of Cambridge, Trumpington Street, Cambridge, UK
Chong Wang
Department of Computer Science, Princeton University, 35 Olden Street, Princeton, NJ 08540, USA
Katherine A. Heller
Department of Engineering, University of Cambridge, Trumpington Street, Cambridge, UK
David M. Blei
Department of Computer Science, Princeton University, 35 Olden Street, Princeton, NJ 08540, USA

HELLER@GATSBY.UCL.AC.UK

BLEI@CS.PRINCETON.EDU

SAW56@CAM.AC.UK

CHONGW@CS.PRINCETON.EDU

Abstract

The hierarchical Dirichlet process (HDP) is
a Bayesian nonparametric mixed membership
model—each data point is modeled with a col-
lection of components of different proportions.
Though powerful, the HDP makes an assumption
that the probability of a component being exhib-
ited by a data point is positively correlated with
its proportion within that data point. This might
be an undesirable assumption. For example, in
topic modeling, a topic (component) might be
rare throughout the corpus but dominant within
those documents (data points) where it occurs.
We develop the IBP compound Dirichlet process
(ICD), a Bayesian nonparametric prior that de-
couples across-data prevalence and within-data
proportion in a mixed membership model. The
ICD combines properties from the HDP and the
Indian buffet process (IBP), a Bayesian nonpara-
metric prior on binary matrices. The ICD as-
signs a subset of the shared mixture components
to each data point. This subset, the data point’s
“focus”, is determined independently from the
amount that each of its components contribute.
We develop an ICD mixture model for text, the
focused topic model (FTM), and show superior
performance over the HDP-based topic model.

Appearing in Proceedings of the 27 th International Conference
on Machine Learning, Haifa, Israel, 2010. Copyright 2010 by the
author(s)/owner(s).

1. Introduction
Finite mixture models are widely used for clustering
data (McLachlan & Peel, 2000). When ﬁt to data, the com-
ponents of a mixture model reﬂect similarity patterns, and
each data point is probabilistically assigned to one of the
components. When data are groups, i.e., each data point is
a collection of observations, then mixed membership mod-
els are appropriate. Mixed membership models are a hier-
archical variant of ﬁnite mixtures for grouped data where
each data point exhibits multiple components. The com-
ponents are shared across all data, and each data point ex-
hibits them with different proportions. Mixed membership
models are an effective tool for capturing complex data het-
erogeneity (Erosheva et al., 2004).
Mixed membership models, like ﬁnite mixtures, require
an a priori choice of the number of components. To ad-
dress this issue, Teh et al. (2006) developed the hierarchical
Dirichlet process (HDP), a Bayesian nonparametric mixed
membership model. The HDP allows for a potentially in-
ﬁnite number of components a priori so that, when con-
ditioned on data, its posterior places a distribution over
how many are exhibited. The HDP provides more ﬂexible
mixed membership modeling, avoiding costly model com-
parisons in order to determine an appropriate number of
components.
However, the HDP makes a hidden assumption: A com-
ponent’s overall prevalence across data is positively corre-
lated with the component’s average proportion within data.
The reason for this is that the HDP centers the random com-
ponent proportions for each data point around the same
global proportions.
This assumption may not be sensible. Consider modeling

Indian Buffet Process Compound Dirichlet Process

a text corpus with an HDP. This is called “topic modeling”
because the posterior components (called “topics”) tend to
reﬂect the semantic themes of the documents (Blei et al.,
2003). The HDP assumption is that a frequent topic will,
on average, occur frequently within each document. How-
ever, there is no reason to correlate the number of articles
on a topic, such as baseball, with how much that topic con-
tributes to any particular article. Baseball may be a rare
topic, but articles about baseball often devote themselves
exclusively to it. In this paper, we build a Bayesian non-
parametric mixed membership model that allays this as-
sumption. Our model decorrelates prevalence and propor-
tion, allowing rarely seen components to occur with high
proportion and frequently seen components to occur with
low proportion.
We develop the IBP compound Dirichlet process (ICD),
a Bayesian nonparametric mixed membership model that
decorrelates across-data prevalence and within-data propor-
tion. The ICD uses a random binary matrix drawn from the
Indian buffet process (IBP, Grifﬁths & Ghahramani, 2005)
to select which components are used in each data point
(across-data prevalence), and an inﬁnite series of gamma
random variables to model how much they are used (within-
data proportions). We use the ICD in the focused topic
model (FTM), a generative model of document collections.
The central challenge in using the ICD is posterior infer-
ence. Sampling the IBP-distributed binary matrix directly
leads to slow convergence, but integrating it out exactly
is intractable due to the inﬁnite combinatorial space of la-
tent variables. We present an approximation to this integral,
based on a technique used in Wang & Blei (2009), and use
this approximation to develop an efﬁcient collapsed Gibbs
sampler.
We compare the FTM to the HDP topic model on three text
corpora. We see that the FTM reduces the correlation be-
tween across-data prevalence and within-data proportion,
which allows for a more compact representation of the data
than the HDP provides. As a consequence, the FTM ob-
tains a better ﬁt to language and achieves substantially bet-
ter perplexity on held out data.

2. IBP Compound Dirichlet Process
In this section we present the IBP compound Dirichlet
process (ICD), a Bayesian nonparametric prior which ad-
dresses the limitations of the hierarchical Dirichlet process
(HDP). We develop the focused topic model (FTM), an ap-
plication of the ICD to document analysis. We assume the
reader is familiar with the Dirichlet process (DP); for a re-
view, see Ghosal (2010).

2.1. Hierarchical Dirichlet Processes

The hierarchical Dirichlet process (HDP, Teh et al., 2006)
is a prior appropriate for Bayesian nonparametric mixed
membership modeling. In an HDP, each data point is as-
sociated with a draw from a Dirichlet process (DP), which
determines how much each member of a shared set of mix-
ture components contributes to that data point. The base
measure of this data-level DP is itself drawn from a DP,
which ensures that there is a single discrete set of compo-
nents shared across the data. More precisely, the generative
process for the per-data distribution Gm is:

G0 ∼ DP(ζ, H),
Gm ∼ DP(τ, G0)

for each m.

Each distribution Gm is a sample from a DP with concen-
tration parameter τ and base probability measure G0. This
base measure G0 is itself a draw from a DP, with concentra-
tion parameter ζ and base measure H. The base measure
G0 is thus discrete and, consequently, the per-data distri-
butions Gm are also discrete with common support deter-
mined by the locations of the atoms of G0.
Each atom represents a component and is described by a
location, a weight in Gm, and a weight in G0. The loca-
tion is identical in both G0 and Gm; it gives the parameters
associated with the component, e.g., a Gaussian mean or a
distribution over terms. The weight in Gm gives the pro-
portion for that component in the mth data point.
The weight of a component in Gm is drawn from a dis-
tribution centered around the corresponding weight in G0.
Thus, the weight for any given component is drawn from
the same distribution across all the data, and that distribu-
tion controls both how prevalent the component is and its
proportion within each data point. For example, if a compo-
nent has low weight in G0 then it will also have low weight
in most Gm. That component is unlikely to contribute to
data points and, when it does, that contribution will be very
small.
As mentioned in the introduction, this is not necessarily a
desirable modeling assumption. Rather than control these
two properties via a single variable, as is the case in the
HDP, we wish to model them separately. We develop a
model where an infrequently occurring component can still
have high proportion when it does occur, and vice versa.

2.2. Indian Buffet Process

Our model uses the Indian buffet process (IBP, Grifﬁths &
Ghahramani, 2005) to control component occurrence sepa-
rately from component proportion. The IBP deﬁnes a dis-
tribution over binary matrices with an inﬁnite number of
columns, only a ﬁnite number of which contain non-zero
entries. It can be derived by taking the limit as K → ∞

Indian Buffet Process Compound Dirichlet Process

of a ﬁnite M × K binary matrix B, with elements bmk dis-
tributed according to,

πk ∼ Beta(α/K, 1),
bmk ∼ Bernoulli(πk)

for each m,

where the mth row of B is bm, the kth cell of bm is bmk,
and πk is the probability of observing a non-zero value in
column k. As K tends to inﬁnity, we can obtain a strictly
decreasing ordering of the latent probabilities πk by start-
ing with a “stick” of unit length and recursively breaking
it at a point Beta(α, 1) along its length, discarding the ex-
cess (Teh et al., 2007), for k = 1, 2, . . . :

πk =Qk

µk ∼ Beta(α, 1),
j=1 µj,

bmk ∼ Bernoulli(πk)

for each m.

(1)

In our model, the rows of the IBP matrix represent data
points, the columns represent components, and the cells in-
dicate which components contribute to which data points.

2.3. IBP compound Dirichlet process

We now develop a prior over a set of discrete probability
distributions that decorrelates which components occur and
in what proportion. Rather than assigning positive proba-
bility mass to all components for every data point, as in the
HDP, our model assigns positive probability to only a sub-
set of components, selected independently of their masses.
The IBP provides a method for selecting subsets from a
countably inﬁnite set of components. Thus, one way of
achieving our goal is to introduce the IBP directly into the
HDP, using the mth row of the IBP to determine a subset of
the inﬁnite set of atoms present in the top level DP sample
G0. This deﬁnes an (unnormalized) measure that can be
used as the base measure for a data-speciﬁc DP.
This can be seen as an inﬁnite spike and slab model. Spike
and slab models describe a mixture model between a contin-
uous distribution (the “slab”)1 and the measure degenerate
at zero. A “spike” distribution determines which variables
are drawn from the slab, and which are zero. In the model
above, the spikes are provided by the IBP, and the slab is
provided by the top level DP.
However, better choices for the top-level “slab” distribution
are available. Draws from the DP “slab” are constrained
to sum to one, which restricts the distribution over com-
ponent proportions and introduces dependencies between

1In its original form, the slab was a uniform distribution. How-
ever, the concept and terminology have also been employed in
models where the slab is not the uniform distribution – see for
example (Ishwaran & Rao, 2005) – and it is in this more general
sense that we use the term.

atom masses which can lead to difﬁculties in developing
inference schemes. While we wish to ensure that the base
measure of the lower level DP is still normalizable (i.e. is
drawn from a convergent process), we do not need to rely
on the slab to enforce this constraint. Since the IBP selects
a ﬁnite number of components for each data point, it is suf-
ﬁcient merely to ensure that the sum of any ﬁnite subset
of top-level atoms is ﬁnite. Thus, rather than drawing the
atoms of the slab from a DP, we sample their masses as in-
dependent gamma random variables. This eliminates the re-
strictions on component proportions imposed by a DP and,
since the resulting component proportions are independent,
makes inference much easier.
The model assumes the following generative process,

1. for k = 1, 2, . . . ,

(a) Sample the stick length πk according to Eq. 1.
(b) Sample the relative mass φk ∼ Gamma(γ, 1).
(c) Sample the atom location βk ∼ H.

2. for m = 1, . . . , M,

(a) Sample a binary vector bm according to Eq. 1.
(b) Sample the lower level DP,

Gm ∼ DP(P

k bmkφk,

k bmkφkδβk

k bmkφk

).

P
P

In sampling the lower level DP, masses are assigned to the
atoms δβk independent of their locations. Since the num-
ber of locations selected by the binary vector bm is ﬁnite
almost surely, these masses can be sampled from a Dirich-
let distribution deﬁned over the selected φk:

θm ∼ Dirichlet(b · φ)

Gm =P

k θmkδβk ,

where b · φ is the Hadamard product of b and φ. If we
marginalize out the sparse binary matrix B and the gamma
random variables φk, the atom masses are distributed ac-
cording to a mixture of Dirichlet distributions governed by
the IBP:

Z

dφP

p(θm|γ, α) =

where,

B p(θm|B, φ)p(B|α)p(φ|γ),

(2)

B ∼ IBP(α),
φk ∼ Gamma(γ, 1),
θm ∼ Dirichlet(bm · φ).

We call this model the IBP compound Dirichlet process
(ICD), since the IBP provides the mixing measure for a mix-
ture of Dirichlet distributions. Like the HDP, this model is a
form of dependent Dirichlet process (MacEachern, 2000).
The ICD achieves our goal of decoupling how often the
components occur and in what proportion. The IBP draw

Indian Buffet Process Compound Dirichlet Process

B selects a subset of atoms for each distribution, and the
gamma random variables φ determine the relative masses
associated with these atoms.

2.4. Focused Topic Models

Suppose H parametrizes distributions over words. Then,
the ICD deﬁnes a generative topic model, where it is used
to generate a set of sparse distributions over an inﬁnite num-
ber of components, called “topics.” Each topic is drawn
from a Dirichlet distribution over words. In order to specify
a fully generative model, we sample the number of words
for each document from a negative binomial distribution,
n(m)
The generative model for M documents is

· ∼ NB(P

k bmkφk, 1/2).2

1. for k = 1, 2, . . . ,

(a) Sample the stick length πk according to Eq. 1.
(b) Sample the relative mass φk ∼ Gamma(γ, 1).
(c) Draw the topic distribution over words,

βk ∼ Dirichlet(η).

2. for m = 1, . . . , M,

(a) Sample a binary vector bm according to Eq. 1.
(b) Draw the total number of words,

· ∼ NB(P

(c) Sample the distribution over topics,

k bmkφk, 1/2).

n(m)
θm ∼ Dirichlet(bm · φ).

(d) For each word wmi, i = 1, . . . , n(m)

·

,

i. Draw the topic index zmi ∼ Discrete(θm).
ii. Draw the word wmi ∼ Discrete(βzmi

).

We call this the focused topic model (FTM) because the
inﬁnite binary matrix B serves to focus the distribution
over topics onto a ﬁnite subset (see Figure 1). The number
of topics within a single document is almost surely ﬁnite,
though the total number of topics is unbounded. The topic
distribution for the mth document, θm, is drawn from a
Dirichlet distribution over the topics selected by bm. The
Dirichlet distribution models uncertainty about topic pro-
portions while maintaining the restriction to a sparse set of
topics.
The ICD models the distribution over the global topic pro-
portion parameters φ separately from the distribution over
the binary matrix B. This captures the idea that a topic may
appear infrequently in a corpus, but make up a high propor-
tion of those documents in which it occurs. Conversely, a
topic may appear frequently in a corpus, but only with low
proportion.

2Notation n(m)

is the number of words assigned to the kth
topic of the mth document, and we use a dot notation to represent
summation - i.e. n(m)

· =P

.

k

k n(m)

k

Figure 1. Graphical model for the focused topic model.

3. Related Models
Titsias (2007) introduced the inﬁnite gamma-Poisson pro-
cess, a distribution over unbounded matrices of non-
negative integers, and used it as the basis for a topic model
of images.
In this model, the distribution over features
for the mth image is given by a Dirichlet distribution over
the non-negative elements of the mth row of the inﬁnite
gamma-Poisson process matrix, with parameters propor-
tional to the values at these elements. While this results in
a sparse matrix of distributions, the number of zero entries
in any column of the matrix is correlated with the values
of the non-zero entries. Columns which have entries with
large values will not typically be sparse. Therefore, this
model will not decouple across-data prevalence and within-
data proportions of topics. In the ICD the number of zero
entries is controlled by a separate process, the IBP, from
the values of the non-zero entries, which are controlled by
the gamma random variables.
The sparse topic model (SparseTM, Wang & Blei, 2009)
uses a ﬁnite spike and slab model to ensure that each topic
is represented by a sparse distribution over words. The
spikes are generated by Bernoulli draws with a single topic-
wide parameter. The topic distribution is then drawn from a
symmetric Dirichlet distribution deﬁned over these spikes.
The ICD also uses a spike and slab approach, but allows
an unbounded number of “spikes” (due to the IBP) and a
more globally informative “slab” (due to the shared gamma
random variables). We extend the SparseTM’s approxima-
tion of the expectation of a ﬁnite mixture of Dirichlet dis-
tributions, to approximate the more complicated mixture of
Dirichlet distributions given in Eq. 2.
Recent work by Fox et al. (2009) uses draws from an IBP
to select subsets of an inﬁnite set of states, to model multi-
ple dynamic systems with shared states. (A state in the dy-
namic system is like a component in a mixed membership
model.) The probability of transitioning from the ith state
to the jth state in the mth dynamic system is drawn from a
Dirichlet distribution with parameters bmjγ + τ δi,j, where
γ and τ are constant. This model does not allow sharing

απγϕθzn.bMβ∞ηw(m)Indian Buffet Process Compound Dirichlet Process

of information about the within-data probability of a state
between data points, which is modeled in the ICD via the
gamma-distributed φk. An alternative inference scheme is
also used, where the IBP matrix is sampled instead of being
integrated out.
A number of other models have been proposed to ad-
dress the rigid topic correlation structure assumed in the
LDA and HDP topic models, including the correlated topic
model (CTM, Blei & Lafferty, 2005) and the pachinko allo-
cation model (PAM, Li & McCallum, 2006). Our aim is dif-
ferent. The FTM reduces undesirable correlations between
the prevalence of a topic across the corpus and its propor-
tion within any particular document, rather than adding
new correlations between topics. Correlations among top-
ics could be integrated into the FTM in future work.

4. Posterior Inference
We use Gibbs sampling for posterior inference over the la-
tent variables. The algorithm cyclically samples the value
for a single variable from its conditional distribution given
the remaining variables. To improve mixing time, we use a
collapsed Gibbs sampler, integrating out the topic-speciﬁc
word distributions β, the topic mixture distributions θ, and
the sparsity pattern B. We use an approximate method,
described in the appendix, to integrate out the inﬁnite-
dimensional sparsity pattern B. We sample only the global
topic proportion variables φ, the global topic sparsity prob-
ability variables π, and the topic assignments z.

4.1. Sampling z

The conditional distribution of the topic assignment of the
ith word in the mth document depends on the posterior dis-
tribution of the topic proportion θm for that document:

Z

p(zmi = k|z¬mi, wmi, w¬mi, Ψ)

·

k

•

, n(m)

k,¬i + η)

∝ p(wmi|zmi = k, z¬mi, w¬mi)p(zmi = k|z¬mi, Ψ)
∝ (n(wmi)

dθm p(zmi = k|θm)p(θm|z¬mi, Ψ),
•

, α, γ}, and n(w)

where Ψ = {π•, φ
is the number
of times word w has been assigned to topic k in the vec-
, π• to represent those
tor of assignments z. We use φ
elements of φ and π associated with topics which are cur-
, π◦ to represent
rently represented in the corpus, and φ
the remaining elements, which are associated with unused
topics and whose values are therefore unknown.
Conditioned on the sparse binary vector bm and the gamma
random variables φ, the topic mixture distribution, θm, is
distributed according to a Dirichlet distribution. The sparse
vector bm determines the subset of topics over which the
Dirichlet distribution is deﬁned, and the gamma random
variables φ determine the values of the Dirichlet parame-

◦

ters at these points. If we integrate out the sparse binary
vector bm, rather than sampling θm from a single Dirichlet
distribution, we must sample it from an inﬁnite mixture of
Dirichlet distributions, with the IBP determining the mix-
ing proportions:

Z

p(θm|z¬mi, Ψ)
∝

◦P

dφ

Dirichlet(θm|(n(m)¬i + φ) · bm)
, n(m)
·

, π•, γ, α),

|φ

•

bm
◦

p(bm, φ

(3)

ues of θm given z¬mi and Ψ, since R dθmp(zmi =

where n(m)¬i
is the topic assignment statistic excluding
word wmi. However, we cannot integrate out the sparse
binary vector bm exactly due to its combinatorial na-
ture.
Fortunately, we only ever use the expected val-
k|θm)p(θm|z¬mi, Ψ) = E[θmk|z¬mi, Ψ] (from Eq. 3).
This expectation can be efﬁciently approximated via a pro-
cedure detailed in the appendix.

4.2. Sampling π and φ
To sample π and φ, we re-instantiate the binary matrix B
as an auxiliary variable, and iteratively sample B, π and φ.
We categorize the columns of B as “active” if n(·)
k > 0, and
“inactive” otherwise.
The total number of words in the mth document assigned
to the kth topic is distributed according to NB(bmkφk, 1/2).
The joint probability of φk and the total number of words
assigned to the kth topic is given by,

k |bk, γ) = p(φk|γ)QM

p(φk, n(·)
= φγ−1

k

Γ(γ)

e−φk

Q

m=1 p(n(m)

k

|bmk, φk)

m:bmk=1

Γ(φk)n

(m)
k

Γ(φk+n

)

(m)
k
!2φk+nm

k

.

(4)

This is log differentiable with respect to φk and γ. Thus we
use Hybrid Monte Carlo (MacKay, 2002) to sample from
the posteriors of φk and γ.
To sample the πk, we follow a similar approach to the semi-
ordering stick-breaking scheme of (Teh et al., 2007). The
“active” features are distributed according to:

(cid:18)PM
m=1 bmk, 1+M−PM

(cid:19)

p(πk|B) ∼ Beta

m=1 bmk

(5)

and the “inactive” features are strictly ordered as suggested
by Eq. 1.
(Note that the deﬁnition given here of “ac-
tive” and “inactive” features differs slightly from that given
in Teh et al. (2007), as we consider a feature where n(·)
k = 0
to be inactive, and therefore subject to strict ordering, re-

gardless of whetherP

m bmk > 0.)

Since the binary matrix B is discarded after this step, and
we only use the active πk to sample the topic allocations,

Indian Buffet Process Compound Dirichlet Process

Figure 2. Experimental comparison between FTM (dark blue) and
HDP (pale blue) on three datasets. Each point represents the re-
sult on one fold, and is computed with the other folds as training
data. Dashed lines connect the results from the same fold. Top.
Test set perplexities. Lower numbers indicate better performance.
Bottom. Correlation between topic presence frequency and topic
proportion. The FTM reduces the correlation between them.

we have no interest in the inactive features and only need
to sample the active elements of B and π. The elements of
B are sampled according to the following probabilities:
p(bmk|πk, φk, n(m)

) =

k



2φk (1−πk)

bmk
πk+2φk (1−πk)
πk+2φk (1−πk)

πk

k > 0

if n(m)
if bmk = 0 and n(m)
if bmk = 1 and n(m)

k = 0
k = 0.

(6)

With equations 3, 4, 5 and 6 we have speciﬁed the full
Gibbs sampler for the FTM model3. From the states of
this sampler, we can compute topics, topic proportions, and
sparsity patterns.

5. Empirical study
We compared the performance of the FTM to the HDP with
three datasets:

• PNAS: This is a collection of 1766 abstracts from
the Proceedings of the National Academy of Sciences
(PNAS) from between 1991 and 2001. The vocabu-
lary contains 2452 words.
• 20 Newsgroups: This is a collection of 1000 randomly
selected articles from the 20 newsgroups dataset.4 The
vocabulary contains 1407 words.
• Reuters-21578: This is a collection of 2000 randomly
selected documents from the Reuters-21578 dataset.5
The vocabulary contains 1472 words.

For each dataset, the vocabulary excluded stop-words and
words occurring in fewer than 5 documents.

3Matlab code is available from the authors
4http://people.csail.mit.edu/jrennie/20Newsgroups/
5http://kdd.ics.uci.edu/databases/reuters21578/

Figure 3. (a) Histogram of the number of topics a word appears
in for the FTM (dark blue, left) and the HDP (pale blue, right)
models on 20 Newsgroups data. In the FTM, words are generally
(b) Histogram of the number of
associated with fewer topics.
documents a topic appears in for the FTM (dark blue, left) and
HDP (pale blue, right) models on 20 Newsgroups data. In both
models, topics appearing in more than 200 documents have been
excluded to focus on the low frequency topics. The HDP has
many more topics which only appear in a very few documents.

In both the FTM and HDP topic models, we ﬁxed the topic
distribution hyper-parameter η to 0.1. Following Teh et al.
(2006), we used priors of ζ ∼ Gamma(5, 0.1) and τ ∼
Gamma(0.1, 0.1) in the HDP. In the FTM, we used prior
γ ∼ Gamma(5, 0.1), and we ﬁxed α = 5.
First, we examined test-set perplexity, a measure of how
well the models generalize to new data. Figure 2 (top)
shows test set perplexities obtained on each dataset for the
two models, using 5-fold cross-validation. To obtain these
measurements, we ran both Gibbs samplers for 1000 iter-
ations, discarding the ﬁrst 500.
In each case, the FTM
achieves better (lower) perplexity on the held out data.
We developed the FTM to decorrelate the probability of
a topic being active within a document and its proportion
within the documents attributed to it. To consider whether
this is observed in the posterior, we next compared two
statistics for each topic found. The topic presence fre-
quency for a given topic is the fraction of the documents
within the corpus that contain at least one incidence of that
topic. The topic proportion for a topic is the fraction of the
words within the corpus attributed to that topic.
The correlation between topic presence frequencies and
topic proportions is shown in Figure 2 (bottom). In each
case we see lower correlation for the FTM. In fact, the
dataset which exhibits the greatest improvement in held-
out perplexity under the FTM, also exhibits the greatest
decrease in correlation between frequency and proportion.
In our study, we observed that the FTM posterior prefers a
more compact representation. It contains fewer topics over-
all than the HDP, but more topics per document. While
the HDP uses a larger number of topics to model the cor-
pus than the FTM, most of these topics appear in only a
handful of documents. In addition, individual words in the

1502002503003504000.750.800.850.900.9520 NewsgroupsllllllllllFTMHDPPNASllllllllllFTMHDPReutersllllllllllFTMHDPHeld out perplexityProp/prev correlation051015202530350100200300400500Number of topicsNumber of words(a)  FTMHDP0501001502000102030405060Number of documentsNumber of topics(b)  FTMHDPIndian Buffet Process Compound Dirichlet Process

vocabulary are, on average, associated with more topics un-
der the HDP, meaning that FTM topics are generally more
distinct. These ﬁndings are illustrated in Figure 3. (These
results are for 20 Newsgroups. Other data exhibited similar
properties.)
Finally, we note that the computational complexity of both
models grows with the number of topics represented. When
the number of topics is equal, a single iteration of the FTM
algorithm is more costly than a single iteration of the HDP.
However, the more compact representation of the FTM
yields similar runtime. In the data we analyzed, the FTM
analysis was as fast as the HDP analysis.

6. Discussion
We developed the IBP compound Dirichlet process (ICD), a
Bayesian nonparametric prior over discrete, dependent dis-
tributions, which is an alternative to the hierarchical Dirich-
let process (HDP). The ICD decouples the relationship be-
tween across-data prevalence and within-data proportion.
We have used the ICD to construct the focused topic model
(FTM), a generative model for collections of documents.
We demonstrated that the FTM provides a better ﬁt to text
than the HDP topic model. The representations obtained
by the FTM reﬂect lower correlation between across-data
prevalence and within-data proportion than the representa-
tions obtained by the HDP.
We have concentrated on correlation in HDPs. This cor-
relation does not occur in ﬁnite mixed membership mod-
els with mixture components drawn from a Dirichlet(α),
where α is ﬁxed, since the draws from Dirichlet(α) are
i.i.d. However, restriction to a ﬁxed Dirichlet distribution
limits the ﬂexibility of mixed membership models, and if
α is unknown, there will be a similar correlation bias as
with the HDP. Although easily derived, there is currently
no parametric counterpart of the ICD.
The idea of removing correlation between global and local
probabilities is potentially applicable to a range of models.
For example, in a state transition sequence, a certain state
may be inaccessible from most other states, but occur with
high probability following a small subset of states. Such a
relationship will be poorly captured using the HDP-based
Hidden Markov model (Teh et al., 2006), as it tends to cor-
relate the global occurrence of a state with the probability
of transitioning to it from another state. A model based on
the ICD could better capture such relationships. Exploring
which HDP applications beneﬁt most from this decoupling
is an avenue for further research.

Acknowledgments. We thank anonymous reviewers for
valuable comments. David M. Blei is supported by ONR
175-6343 and NSF CAREER 0745520.

References
Blei, D. M. and Lafferty, J. Correlated topic models. In

NIPS, volume 18, 2005.

Blei, D. M., Jordan, M. I., and Ng, A. Y. Latent Dirichlet

allocation. JMLR, pp. 993–1022, 2003.

Erosheva, E., Fienberg, S., and Lafferty, J. Mixed-
membership models of scientiﬁc publications. PNAS,
101(Suppl 1):5220–5227, April 2004.

Fox, E. B., Sudderth, E. B., Jordan, M. I., and Willsky,
A. S. Sharing features among dynamical systems with
beta processes. In NIPS, volume 22, 2009.

Ghosal, S. Dirichlet process, related priors and posterior
asymptotics. In Hjort, N. L., Holmes, C., M¨uller, P., and
Walker, S. G. (eds.), Bayesian Nonparametrics, Cam-
bridge Series in Statistical and Probabilistic Mathemat-
ics, pp. 35–79. Cambridge University Press, 2010.

Grifﬁths, T. L. and Ghahramani, Z. Inﬁnite latent feature
In NIPS, vol-

models and the Indian buffet process.
ume 18, 2005.

Ishwaran, H. and Rao, J. S. Spike and slab variable se-
lection: Frequentist and Bayesian strategies. Annals of
Statistics, 33:730, 2005.

Li, W. and McCallum, A. Pachinko allocation: DAG-
In

structured mixture models of topic correlations.
ICML, volume 23, 2006.

MacEachern, S. N. Dependent Dirichlet processes. Tech-
nical report, Department of Statistics, Ohio State Univer-
sity, 2000.

MacKay, D. J. C. Information Theory, Inference & Learn-

ing Algorithms. Cambridge University Press, 2002.

McLachlan, G. and Peel, D. Finite Mixture Models. John

Wiley & Sons, 2000.

Teh, Y. W., Jordan, M. I., Beal, M. J., and Blei, D. M.
Hierarchical Dirichlet processes. JASA, 101(476):1566–
1581, 2006.

Teh, Y. W., G¨or¨ur, D., and Ghahramani, Z. Stick-breaking
construction for the Indian buffet process. In AISTATS,
volume 11, 2007.

Titsias, M. K. The inﬁnite Gamma-Poisson feature model.

In NIPS, volume 21, 2007.

Wang, C. and Blei, D. M. Decoupling sparsity and smooth-
In

ness in the discrete hierarchical Dirichlet process.
NIPS, volume 22, 2009.

Indian Buffet Process Compound Dirichlet Process

•

A. Approximating the expectation of θm
, π• denote those elements of φ and
First we recall that φ
π associated with the topics that are currently represented
◦
, π◦ denote the rest. As described
in the corpus, and φ
in Section 4.1, in order to sample the topic allocations zi,
we wish to approximate the expectation of θm given the
posterior distribution in Equation 3. This is an extension
of the technique used to approximate the expectation of the
distribution over words in Wang & Blei (2009).
The expectation of the kth element of θm is:
E[θmk|z¬mi, Ψ]
∝

k,¬i + φk)p(bm, φ

, n(m)

(n(m)

, π•, γ, α)

|φ

◦

•

·

(n(m)

k,¬i + φk)p(b◦
P
j bmj φj (n(m)
2

·,¬i +P

m|π•, α)p(φ

◦|γ)
j bmjφj)

.

b◦
m:
bmk=1

dφ

Z
◦ X
Z
◦ X
We divideP

dφ

∝

b◦
m:
bmk=1

j bmjφj into a “known” term X, correspond-
ing to those topics with which words in the mth document
are associated,and an “unknown” term, corresponding to
those topics with which no words in the mth document are

associated:X

j

bmjφj = X
|
{z

j:n

(m)
j,¬i>0

X

+ X
|

j:n

φj

}

(m)
j,¬i=0

.

bmjφj

}

{z

Y

Let gX,k(Y ) , bmk(n
. Then we can write the
expectation of θmk in terms of the expectation of gX,k(Y ):

(m)
k,¬i+φk)
(m)
·,¬i+X+Y )

2X+Y (n

E[θmk|Ψ, nm¬i] ∝ E[gX,k(Y )|π•, φ

•].

(7)

2. n(m)

1. n(m)

Consider approximating this expectation in three scenarios:
k,¬i > 0: the kth topic is currently represented in the
document under consideration;
k,¬i = 0 and n(·)
k,¬i > 0: the kth topic is not currently
represented in the document under consideration, but
is currently represented in the corpus;
k,¬i = 0: the kth topic is not currently represented
in the corpus.

3. n(·)

In the ﬁrst case, where n(m)
so the expectation in equation 7 is

k,¬i > 0, we know that bmk = 1,

•

E[gX,k(Y )|π•, φ
k,¬i + φk)E

= (n(m)

(cid:21)

(8)

, n(m)]

(cid:20)
and Y † =P

2X+Y (n

1
(m)
·,¬i+X+Y )

Y ∗ =P

We divide Y into two components Y = Y ∗ + Y †, where

j∈J ∗

m

bmjφj

j∈J † bmjφj.

j,¬i > 0, and
j = 0. The expectation and

m is the set of j such that n(m)

j,¬i = 0, n(·)
where J ∗
J † is the set of j such that n(·)
•
variance of Y conditioned on π•, φ
, α and γ, are
, α, γ] = E[Y ∗|π•, φ
•] + E[Y †|α, γ]
•] + V[Y †|α, γ],
, α, γ] = V[Y ∗|π•, φ

E[Y |π•, φ
V[Y |π•, φ

•
•

(9)

where

E[Y ∗|π•, φ
V[Y ∗|π•, φ

•] =P
•] =P

m

j∈J •
j∈J •

m

πjφj
j πj(1 − πj)
φ2

E[Y †|α, γ] = αγ
V[Y †|α, γ] = αγ(γ + 1) − α2γ2
2α+1 .

(10)
In summary, Y ∗ is a summation over a ﬁnite number of top-
ics that grows in expectation as α(log(M) − 1), where M
is the total number of documents; and Y † is a summation
over a countably inﬁnite number of topics. If the number
of documents, and therefore the expected total number of
topics, is large, then the central limit theorem suggests that
we can approximate (X + Y ) using a Gaussian distribution
with mean and variance as described in equations 9 and 10.
We can then approximate the expectation in equation 8 us-
ing a second order Taylor expansion.

In the second case, where n(m)
expectation in equation 7 becomes:

k,¬i = 0 and n(·)

k,¬i > 0, the

E[gX,k(Y )|π•, φ

•]

(cid:20)

(cid:21)

,

(cid:21)

.

= φkπkE

2X+Y¬k+φk (n

1
(m)
k,¬i+X+Y¬k+φk)

where Y¬k = Y − bmkφk. We approximate the expectation
as described for the ﬁrst case (where n(m)
k,¬i > 0), noting
the slight changes in the expectation and variance of Y¬k
compared with Y .
In the third case, where n(·)
k,¬i = 0, we consider the proba-
bility of assigning the ith word to one of the inﬁnite number
of unobserved classes. Let z = u indicate assignment to a
previously unseen topic. Equation 7 becomes:

E[gX,u(Y )|π•, φ

•]

(cid:20)

= E

2X+Y ∗+Y †

(n

Y †
·,¬i+X+Y ∗+Y †)
(m)

(11)

•, α and γ.

We approximate the expectation in equation 11 using a mul-
tivariate second order Taylor expansion, noting that Y ∗ and
Y † are independent conditioned on π•, φ
We have evaluated the quality of this approximation by
comparing the approximated expectations with those esti-
mated using Monte Carlo simulations, and found that the
approximation holds well, provided γ is not large. The val-
ues of γ learned by the model in the experiments described
in section 5 fall into this paradigm.

