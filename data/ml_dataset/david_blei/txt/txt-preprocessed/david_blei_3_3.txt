Abstract

The hierarchical Dirichlet process (HDP) is
a Bayesian nonparametric mixed membership
modeleach data point is modeled with a col-
lection of components of different proportions.
Though powerful, the HDP makes an assumption
that the probability of a component being exhib-
ited by a data point is positively correlated with
its proportion within that data point. This might
be an undesirable assumption. For example, in
topic modeling, a topic (component) might be
rare throughout the corpus but dominant within
those documents (data points) where it occurs.
We develop the IBP compound Dirichlet process
(ICD), a Bayesian nonparametric prior that de-
couples across-data prevalence and within-data
proportion in a mixed membership model. The
ICD combines properties from the HDP and the
Indian buffet process (IBP), a Bayesian nonpara-
metric prior on binary matrices. The ICD as-
signs a subset of the shared mixture components
to each data point. This subset, the data points
focus, is determined independently from the
amount that each of its components contribute.
We develop an ICD mixture model for text, the
focused topic model (FTM), and show superior
performance over the HDP-based topic model.

Appearing in Proceedings of the 27 th International Conference
on Machine Learning, Haifa, Israel, 2010. Copyright 2010 by the
author(s)/owner(s).

1. Introduction
Finite mixture models are widely used for clustering
data (McLachlan & Peel, 2000). When t to data, the com-
ponents of a mixture model reect similarity patterns, and
each data point is probabilistically assigned to one of the
components. When data are groups, i.e., each data point is
a collection of observations, then mixed membership mod-
els are appropriate. Mixed membership models are a hier-
archical variant of nite mixtures for grouped data where
each data point exhibits multiple components. The com-
ponents are shared across all data, and each data point ex-
hibits them with different proportions. Mixed membership
models are an effective tool for capturing complex data het-
erogeneity (Erosheva et al., 2004).
Mixed membership models, like nite mixtures, require
an a priori choice of the number of components. To ad-
dress this issue, Teh et al. (2006) developed the hierarchical
Dirichlet process (HDP), a Bayesian nonparametric mixed
membership model. The HDP allows for a potentially in-
nite number of components a priori so that, when con-
ditioned on data, its posterior places a distribution over
how many are exhibited. The HDP provides more exible
mixed membership modeling, avoiding costly model com-
parisons in order to determine an appropriate number of
components.
However, the HDP makes a hidden assumption: A com-
ponents overall prevalence across data is positively corre-
lated with the components average proportion within data.
The reason for this is that the HDP centers the random com-
ponent proportions for each data point around the same
global proportions.
This assumption may not be sensible. Consider modeling

Indian Buffet Process Compound Dirichlet Process

a text corpus with an HDP. This is called topic modeling
because the posterior components (called topics) tend to
reect the semantic themes of the documents (Blei et al.,
2003). The HDP assumption is that a frequent topic will,
on average, occur frequently within each document. How-
ever, there is no reason to correlate the number of articles
on a topic, such as baseball, with how much that topic con-
tributes to any particular article. Baseball may be a rare
topic, but articles about baseball often devote themselves
exclusively to it. In this paper, we build a Bayesian non-
parametric mixed membership model that allays this as-
sumption. Our model decorrelates prevalence and propor-
tion, allowing rarely seen components to occur with high
proportion and frequently seen components to occur with
low proportion.
We develop the IBP compound Dirichlet process (ICD),
a Bayesian nonparametric mixed membership model that
decorrelates across-data prevalence and within-data propor-
tion. The ICD uses a random binary matrix drawn from the
Indian buffet process (IBP, Grifths & Ghahramani, 2005)
to select which components are used in each data point
(across-data prevalence), and an innite series of gamma
random variables to model how much they are used (within-
data proportions). We use the ICD in the focused topic
model (FTM), a generative model of document collections.
The central challenge in using the ICD is posterior infer-
ence. Sampling the IBP-distributed binary matrix directly
leads to slow convergence, but integrating it out exactly
is intractable due to the innite combinatorial space of la-
tent variables. We present an approximation to this integral,
based on a technique used in Wang & Blei (2009), and use
this approximation to develop an efcient collapsed Gibbs
sampler.
We compare the FTM to the HDP topic model on three text
corpora. We see that the FTM reduces the correlation be-
tween across-data prevalence and within-data proportion,
which allows for a more compact representation of the data
than the HDP provides. As a consequence, the FTM ob-
tains a better t to language and achieves substantially bet-
ter perplexity on held out data.

2. IBP Compound Dirichlet Process
In this section we present the IBP compound Dirichlet
process (ICD), a Bayesian nonparametric prior which ad-
dresses the limitations of the hierarchical Dirichlet process
(HDP). We develop the focused topic model (FTM), an ap-
plication of the ICD to document analysis. We assume the
reader is familiar with the Dirichlet process (DP); for a re-
view, see Ghosal (2010).

2.1. Hierarchical Dirichlet Processes

The hierarchical Dirichlet process (HDP, Teh et al., 2006)
is a prior appropriate for Bayesian nonparametric mixed
membership modeling. In an HDP, each data point is as-
sociated with a draw from a Dirichlet process (DP), which
determines how much each member of a shared set of mix-
ture components contributes to that data point. The base
measure of this data-level DP is itself drawn from a DP,
which ensures that there is a single discrete set of compo-
nents shared across the data. More precisely, the generative
process for the per-data distribution Gm is:

G0  DP(, H),
Gm  DP(, G0)

for each m.

Each distribution Gm is a sample from a DP with concen-
tration parameter  and base probability measure G0. This
base measure G0 is itself a draw from a DP, with concentra-
tion parameter  and base measure H. The base measure
G0 is thus discrete and, consequently, the per-data distri-
butions Gm are also discrete with common support deter-
mined by the locations of the atoms of G0.
Each atom represents a component and is described by a
location, a weight in Gm, and a weight in G0. The loca-
tion is identical in both G0 and Gm; it gives the parameters
associated with the component, e.g., a Gaussian mean or a
distribution over terms. The weight in Gm gives the pro-
portion for that component in the mth data point.
The weight of a component in Gm is drawn from a dis-
tribution centered around the corresponding weight in G0.
Thus, the weight for any given component is drawn from
the same distribution across all the data, and that distribu-
tion controls both how prevalent the component is and its
proportion within each data point. For example, if a compo-
nent has low weight in G0 then it will also have low weight
in most Gm. That component is unlikely to contribute to
data points and, when it does, that contribution will be very
small.
As mentioned in the introduction, this is not necessarily a
desirable modeling assumption. Rather than control these
two properties via a single variable, as is the case in the
HDP, we wish to model them separately. We develop a
model where an infrequently occurring component can still
have high proportion when it does occur, and vice versa.

2.2. Indian Buffet Process

Our model uses the Indian buffet process (IBP, Grifths &
Ghahramani, 2005) to control component occurrence sepa-
rately from component proportion. The IBP denes a dis-
tribution over binary matrices with an innite number of
columns, only a nite number of which contain non-zero
entries. It can be derived by taking the limit as K  

Indian Buffet Process Compound Dirichlet Process

of a nite M  K binary matrix B, with elements bmk dis-
tributed according to,

k  Beta(/K, 1),
bmk  Bernoulli(k)

for each m,

where the mth row of B is bm, the kth cell of bm is bmk,
and k is the probability of observing a non-zero value in
column k. As K tends to innity, we can obtain a strictly
decreasing ordering of the latent probabilities k by start-
ing with a stick of unit length and recursively breaking
it at a point Beta(, 1) along its length, discarding the ex-
cess (Teh et al., 2007), for k = 1, 2, . . . :

k =Qk

k  Beta(, 1),
j=1 j,

bmk  Bernoulli(k)

for each m.

(1)

In our model, the rows of the IBP matrix represent data
points, the columns represent components, and the cells in-
dicate which components contribute to which data points.

2.3. IBP compound Dirichlet process

We now develop a prior over a set of discrete probability
distributions that decorrelates which components occur and
in what proportion. Rather than assigning positive proba-
bility mass to all components for every data point, as in the
HDP, our model assigns positive probability to only a sub-
set of components, selected independently of their masses.
The IBP provides a method for selecting subsets from a
countably innite set of components. Thus, one way of
achieving our goal is to introduce the IBP directly into the
HDP, using the mth row of the IBP to determine a subset of
the innite set of atoms present in the top level DP sample
G0. This denes an (unnormalized) measure that can be
used as the base measure for a data-specic DP.
This can be seen as an innite spike and slab model. Spike
and slab models describe a mixture model between a contin-
uous distribution (the slab)1 and the measure degenerate
at zero. A spike distribution determines which variables
are drawn from the slab, and which are zero. In the model
above, the spikes are provided by the IBP, and the slab is
provided by the top level DP.
However, better choices for the top-level slab distribution
are available. Draws from the DP slab are constrained
to sum to one, which restricts the distribution over com-
ponent proportions and introduces dependencies between

1In its original form, the slab was a uniform distribution. How-
ever, the concept and terminology have also been employed in
models where the slab is not the uniform distribution  see for
example (Ishwaran & Rao, 2005)  and it is in this more general
sense that we use the term.

atom masses which can lead to difculties in developing
inference schemes. While we wish to ensure that the base
measure of the lower level DP is still normalizable (i.e. is
drawn from a convergent process), we do not need to rely
on the slab to enforce this constraint. Since the IBP selects
a nite number of components for each data point, it is suf-
cient merely to ensure that the sum of any nite subset
of top-level atoms is nite. Thus, rather than drawing the
atoms of the slab from a DP, we sample their masses as in-
dependent gamma random variables. This eliminates the re-
strictions on component proportions imposed by a DP and,
since the resulting component proportions are independent,
makes inference much easier.
The model assumes the following generative process,

1. for k = 1, 2, . . . ,

(a) Sample the stick length k according to Eq. 1.
(b) Sample the relative mass k  Gamma(, 1).
(c) Sample the atom location k  H.

2. for m = 1, . . . , M,

(a) Sample a binary vector bm according to Eq. 1.
(b) Sample the lower level DP,

Gm  DP(P

k bmkk,

k bmkkk

k bmkk

).

P
P

In sampling the lower level DP, masses are assigned to the
atoms k independent of their locations. Since the num-
ber of locations selected by the binary vector bm is nite
almost surely, these masses can be sampled from a Dirich-
let distribution dened over the selected k:

m  Dirichlet(b  )

Gm =P

k mkk ,

where b   is the Hadamard product of b and . If we
marginalize out the sparse binary matrix B and the gamma
random variables k, the atom masses are distributed ac-
cording to a mixture of Dirichlet distributions governed by
the IBP:

Z

dP

p(m|, ) =

where,

B p(m|B, )p(B|)p(|),

(2)

B  IBP(),
k  Gamma(, 1),
m  Dirichlet(bm  ).

We call this model the IBP compound Dirichlet process
(ICD), since the IBP provides the mixing measure for a mix-
ture of Dirichlet distributions. Like the HDP, this model is a
form of dependent Dirichlet process (MacEachern, 2000).
The ICD achieves our goal of decoupling how often the
components occur and in what proportion. The IBP draw

Indian Buffet Process Compound Dirichlet Process

B selects a subset of atoms for each distribution, and the
gamma random variables  determine the relative masses
associated with these atoms.

2.4. Focused Topic Models

Suppose H parametrizes distributions over words. Then,
the ICD denes a generative topic model, where it is used
to generate a set of sparse distributions over an innite num-
ber of components, called topics. Each topic is drawn
from a Dirichlet distribution over words. In order to specify
a fully generative model, we sample the number of words
for each document from a negative binomial distribution,
n(m)
The generative model for M documents is

  NB(P

k bmkk, 1/2).2

1. for k = 1, 2, . . . ,

(a) Sample the stick length k according to Eq. 1.
(b) Sample the relative mass k  Gamma(, 1).
(c) Draw the topic distribution over words,

k  Dirichlet().

2. for m = 1, . . . , M,

(a) Sample a binary vector bm according to Eq. 1.
(b) Draw the total number of words,

  NB(P

(c) Sample the distribution over topics,

k bmkk, 1/2).

n(m)
m  Dirichlet(bm  ).

(d) For each word wmi, i = 1, . . . , n(m)



,

i. Draw the topic index zmi  Discrete(m).
ii. Draw the word wmi  Discrete(zmi

).

We call this the focused topic model (FTM) because the
innite binary matrix B serves to focus the distribution
over topics onto a nite subset (see Figure 1). The number
of topics within a single document is almost surely nite,
though the total number of topics is unbounded. The topic
distribution for the mth document, m, is drawn from a
Dirichlet distribution over the topics selected by bm. The
Dirichlet distribution models uncertainty about topic pro-
portions while maintaining the restriction to a sparse set of
topics.
The ICD models the distribution over the global topic pro-
portion parameters  separately from the distribution over
the binary matrix B. This captures the idea that a topic may
appear infrequently in a corpus, but make up a high propor-
tion of those documents in which it occurs. Conversely, a
topic may appear frequently in a corpus, but only with low
proportion.

2Notation n(m)

is the number of words assigned to the kth
topic of the mth document, and we use a dot notation to represent
summation - i.e. n(m)

 =P

.

k

k n(m)

k

Figure 1. Graphical model for the focused topic model.

3. Related Models
Titsias (2007) introduced the innite gamma-Poisson pro-
cess, a distribution over unbounded matrices of non-
negative integers, and used it as the basis for a topic model
of images.
In this model, the distribution over features
for the mth image is given by a Dirichlet distribution over
the non-negative elements of the mth row of the innite
gamma-Poisson process matrix, with parameters propor-
tional to the values at these elements. While this results in
a sparse matrix of distributions, the number of zero entries
in any column of the matrix is correlated with the values
of the non-zero entries. Columns which have entries with
large values will not typically be sparse. Therefore, this
model will not decouple across-data prevalence and within-
data proportions of topics. In the ICD the number of zero
entries is controlled by a separate process, the IBP, from
the values of the non-zero entries, which are controlled by
the gamma random variables.
The sparse topic model (SparseTM, Wang & Blei, 2009)
uses a nite spike and slab model to ensure that each topic
is represented by a sparse distribution over words. The
spikes are generated by Bernoulli draws with a single topic-
wide parameter. The topic distribution is then drawn from a
symmetric Dirichlet distribution dened over these spikes.
The ICD also uses a spike and slab approach, but allows
an unbounded number of spikes (due to the IBP) and a
more globally informative slab (due to the shared gamma
random variables). We extend the SparseTMs approxima-
tion of the expectation of a nite mixture of Dirichlet dis-
tributions, to approximate the more complicated mixture of
Dirichlet distributions given in Eq. 2.
Recent work by Fox et al. (2009) uses draws from an IBP
to select subsets of an innite set of states, to model multi-
ple dynamic systems with shared states. (A state in the dy-
namic system is like a component in a mixed membership
model.) The probability of transitioning from the ith state
to the jth state in the mth dynamic system is drawn from a
Dirichlet distribution with parameters bmj +  i,j, where
 and  are constant. This model does not allow sharing

zn.bMw(m)Indian Buffet Process Compound Dirichlet Process

of information about the within-data probability of a state
between data points, which is modeled in the ICD via the
gamma-distributed k. An alternative inference scheme is
also used, where the IBP matrix is sampled instead of being
integrated out.
A number of other models have been proposed to ad-
dress the rigid topic correlation structure assumed in the
LDA and HDP topic models, including the correlated topic
model (CTM, Blei & Lafferty, 2005) and the pachinko allo-
cation model (PAM, Li & McCallum, 2006). Our aim is dif-
ferent. The FTM reduces undesirable correlations between
the prevalence of a topic across the corpus and its propor-
tion within any particular document, rather than adding
new correlations between topics. Correlations among top-
ics could be integrated into the FTM in future work.

4. Posterior Inference
We use Gibbs sampling for posterior inference over the la-
tent variables. The algorithm cyclically samples the value
for a single variable from its conditional distribution given
the remaining variables. To improve mixing time, we use a
collapsed Gibbs sampler, integrating out the topic-specic
word distributions , the topic mixture distributions , and
the sparsity pattern B. We use an approximate method,
described in the appendix, to integrate out the innite-
dimensional sparsity pattern B. We sample only the global
topic proportion variables , the global topic sparsity prob-
ability variables , and the topic assignments z.

4.1. Sampling z

The conditional distribution of the topic assignment of the
ith word in the mth document depends on the posterior dis-
tribution of the topic proportion m for that document:

Z

p(zmi = k|zmi, wmi, wmi, )



k



, n(m)

k,i + )

 p(wmi|zmi = k, zmi, wmi)p(zmi = k|zmi, )
 (n(wmi)

dm p(zmi = k|m)p(m|zmi, ),


, , }, and n(w)

where  = {, 
is the number
of times word w has been assigned to topic k in the vec-
,  to represent those
tor of assignments z. We use 
elements of  and  associated with topics which are cur-
,  to represent
rently represented in the corpus, and 
the remaining elements, which are associated with unused
topics and whose values are therefore unknown.
Conditioned on the sparse binary vector bm and the gamma
random variables , the topic mixture distribution, m, is
distributed according to a Dirichlet distribution. The sparse
vector bm determines the subset of topics over which the
Dirichlet distribution is dened, and the gamma random
variables  determine the values of the Dirichlet parame-



ters at these points. If we integrate out the sparse binary
vector bm, rather than sampling m from a single Dirichlet
distribution, we must sample it from an innite mixture of
Dirichlet distributions, with the IBP determining the mix-
ing proportions:

Z

p(m|zmi, )


P

d

Dirichlet(m|(n(m)i + )  bm)
, n(m)


, , , ),

|



bm


p(bm, 

(3)

ues of m given zmi and , since R dmp(zmi =

where n(m)i
is the topic assignment statistic excluding
word wmi. However, we cannot integrate out the sparse
binary vector bm exactly due to its combinatorial na-
ture.
Fortunately, we only ever use the expected val-
k|m)p(m|zmi, ) = E[mk|zmi, ] (from Eq. 3).
This expectation can be efciently approximated via a pro-
cedure detailed in the appendix.

4.2. Sampling  and 
To sample  and , we re-instantiate the binary matrix B
as an auxiliary variable, and iteratively sample B,  and .
We categorize the columns of B as active if n()
k > 0, and
inactive otherwise.
The total number of words in the mth document assigned
to the kth topic is distributed according to NB(bmkk, 1/2).
The joint probability of k and the total number of words
assigned to the kth topic is given by,

k |bk, ) = p(k|)QM

p(k, n()
= 1

k

()

ek

Q

m=1 p(n(m)

k

|bmk, k)

m:bmk=1

(k)n

(m)
k

(k+n

)

(m)
k
!2k+nm

k

.

(4)

This is log differentiable with respect to k and . Thus we
use Hybrid Monte Carlo (MacKay, 2002) to sample from
the posteriors of k and .
To sample the k, we follow a similar approach to the semi-
ordering stick-breaking scheme of (Teh et al., 2007). The
active features are distributed according to:

(cid:18)PM
m=1 bmk, 1+MPM

(cid:19)

p(k|B)  Beta

m=1 bmk

(5)

and the inactive features are strictly ordered as suggested
by Eq. 1.
(Note that the denition given here of ac-
tive and inactive features differs slightly from that given
in Teh et al. (2007), as we consider a feature where n()
k = 0
to be inactive, and therefore subject to strict ordering, re-

gardless of whetherP

m bmk > 0.)

Since the binary matrix B is discarded after this step, and
we only use the active k to sample the topic allocations,

Indian Buffet Process Compound Dirichlet Process

Figure 2. Experimental comparison between FTM (dark blue) and
HDP (pale blue) on three datasets. Each point represents the re-
sult on one fold, and is computed with the other folds as training
data. Dashed lines connect the results from the same fold. Top.
Test set perplexities. Lower numbers indicate better performance.
Bottom. Correlation between topic presence frequency and topic
proportion. The FTM reduces the correlation between them.

we have no interest in the inactive features and only need
to sample the active elements of B and . The elements of
B are sampled according to the following probabilities:
p(bmk|k, k, n(m)

) =

k



2k (1k)

bmk
k+2k (1k)
k+2k (1k)

k

k > 0

if n(m)
if bmk = 0 and n(m)
if bmk = 1 and n(m)

k = 0
k = 0.

(6)

With equations 3, 4, 5 and 6 we have specied the full
Gibbs sampler for the FTM model3. From the states of
this sampler, we can compute topics, topic proportions, and
sparsity patterns.

5. Empirical study
We compared the performance of the FTM to the HDP with
three datasets:

 PNAS: This is a collection of 1766 abstracts from
the Proceedings of the National Academy of Sciences
(PNAS) from between 1991 and 2001. The vocabu-
lary contains 2452 words.
 20 Newsgroups: This is a collection of 1000 randomly
selected articles from the 20 newsgroups dataset.4 The
vocabulary contains 1407 words.
 Reuters-21578: This is a collection of 2000 randomly
selected documents from the Reuters-21578 dataset.5
The vocabulary contains 1472 words.

For each dataset, the vocabulary excluded stop-words and
words occurring in fewer than 5 documents.

3Matlab code is available from the authors
4http://people.csail.mit.edu/jrennie/20Newsgroups/
5http://kdd.ics.uci.edu/databases/reuters21578/

Figure 3. (a) Histogram of the number of topics a word appears
in for the FTM (dark blue, left) and the HDP (pale blue, right)
models on 20 Newsgroups data. In the FTM, words are generally
(b) Histogram of the number of
associated with fewer topics.
documents a topic appears in for the FTM (dark blue, left) and
HDP (pale blue, right) models on 20 Newsgroups data. In both
models, topics appearing in more than 200 documents have been
excluded to focus on the low frequency topics. The HDP has
many more topics which only appear in a very few documents.

In both the FTM and HDP topic models, we xed the topic
distribution hyper-parameter  to 0.1. Following Teh et al.
(2006), we used priors of   Gamma(5, 0.1) and  
Gamma(0.1, 0.1) in the HDP. In the FTM, we used prior
  Gamma(5, 0.1), and we xed  = 5.
First, we examined test-set perplexity, a measure of how
well the models generalize to new data. Figure 2 (top)
shows test set perplexities obtained on each dataset for the
two models, using 5-fold cross-validation. To obtain these
measurements, we ran both Gibbs samplers for 1000 iter-
ations, discarding the rst 500.
In each case, the FTM
achieves better (lower) perplexity on the held out data.
We developed the FTM to decorrelate the probability of
a topic being active within a document and its proportion
within the documents attributed to it. To consider whether
this is observed in the posterior, we next compared two
statistics for each topic found. The topic presence fre-
quency for a given topic is the fraction of the documents
within the corpus that contain at least one incidence of that
topic. The topic proportion for a topic is the fraction of the
words within the corpus attributed to that topic.
The correlation between topic presence frequencies and
topic proportions is shown in Figure 2 (bottom). In each
case we see lower correlation for the FTM. In fact, the
dataset which exhibits the greatest improvement in held-
out perplexity under the FTM, also exhibits the greatest
decrease in correlation between frequency and proportion.
In our study, we observed that the FTM posterior prefers a
more compact representation. It contains fewer topics over-
all than the HDP, but more topics per document. While
the HDP uses a larger number of topics to model the cor-
pus than the FTM, most of these topics appear in only a
handful of documents. In addition, individual words in the

1502002503003504000.750.800.850.900.9520 NewsgroupsllllllllllFTMHDPPNASllllllllllFTMHDPReutersllllllllllFTMHDPHeld out perplexityProp/prev correlation051015202530350100200300400500Number of topicsNumber of words(a)  FTMHDP0501001502000102030405060Number of documentsNumber of topics(b)  FTMHDPIndian Buffet Process Compound Dirichlet Process

vocabulary are, on average, associated with more topics un-
der the HDP, meaning that FTM topics are generally more
distinct. These ndings are illustrated in Figure 3. (These
results are for 20 Newsgroups. Other data exhibited similar
properties.)
Finally, we note that the computational complexity of both
models grows with the number of topics represented. When
the number of topics is equal, a single iteration of the FTM
algorithm is more costly than a single iteration of the HDP.
However, the more compact representation of the FTM
yields similar runtime. In the data we analyzed, the FTM
analysis was as fast as the HDP analysis.

6. Discussion
We developed the IBP compound Dirichlet process (ICD), a
Bayesian nonparametric prior over discrete, dependent dis-
tributions, which is an alternative to the hierarchical Dirich-
let process (HDP). The ICD decouples the relationship be-
tween across-data prevalence and within-data proportion.
We have used the ICD to construct the focused topic model
(FTM), a generative model for collections of documents.
We demonstrated that the FTM provides a better t to text
than the HDP topic model. The representations obtained
by the FTM reect lower correlation between across-data
prevalence and within-data proportion than the representa-
tions obtained by the HDP.
We have concentrated on correlation in HDPs. This cor-
relation does not occur in nite mixed membership mod-
els with mixture components drawn from a Dirichlet(),
where  is xed, since the draws from Dirichlet() are
i.i.d. However, restriction to a xed Dirichlet distribution
limits the exibility of mixed membership models, and if
 is unknown, there will be a similar correlation bias as
with the HDP. Although easily derived, there is currently
no parametric counterpart of the ICD.
The idea of removing correlation between global and local
probabilities is potentially applicable to a range of models.
For example, in a state transition sequence, a certain state
may be inaccessible from most other states, but occur with
high probability following a small subset of states. Such a
relationship will be poorly captured using the HDP-based
Hidden Markov model (Teh et al., 2006), as it tends to cor-
relate the global occurrence of a state with the probability
of transitioning to it from another state. A model based on
the ICD could better capture such relationships. Exploring
which HDP applications benet most from this decoupling
is an avenue for further research.

Acknowledgments. We thank anonymous reviewers for
valuable comments. David M. Blei is supported by ONR
175-6343 and NSF CAREER 0745520.

