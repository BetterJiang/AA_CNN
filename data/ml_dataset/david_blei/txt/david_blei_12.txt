Modeling Annotated Data

David M. Blei

Division of Computer Science

University of California, Berkeley

Berkeley, CA 94720

Michael I. Jordan

Division of Computer Science
and Department of Statistics

University of California, Berkeley

Berkeley, CA 94720

ABSTRACT
We consider the problem of modeling annotated data|data
with multiple types where the instance of one type (such as
a caption) serves as a description of the other type (such as
an image). We describe three hierarchical probabilistic mix-
ture models which aim to describe such data, culminating in
correspondence latent Dirichlet allocation, a latent variable
model that is e(cid:11)ective at modeling the joint distribution of
both types and the conditional distribution of the annota-
tion given the primary type. We conduct experiments on
the Corel database of images and captions, assessing per-
formance in terms of held-out likelihood, automatic annota-
tion, and text-based image retrieval.

Categories and Subject Descriptors
G.3 [Mathematics of Computing]: Probability and Statis-
tics|statistical computing, multivariate statistics

Keywords
Probabilistic graphical models, empirical Bayes, variational
methods, automatic image annotation, image retrieval

1.

INTRODUCTION

Traditional methods of information retrieval are organized
around the representation and processing of a document in
a (high-dimensional) word-space. Modern multimedia doc-
uments, however, are not merely collections of words, but
can be collections of related text, images, audio, and cross-
references. When working with a corpus of such documents,
there is much to be gained from representations which can
explicitly model associations among the di(cid:11)erent types of
data.

In this paper, we consider probabilistic models for docu-
ments that consist of pairs of data streams. Our focus is on
problems in which one data type can be viewed as an anno-
tation of the other data type. Examples of such data include
images and their captions, papers and their bibliographies,
and genes and their functions. In addition to the traditional

goals of retrieval, clustering, and classi(cid:12)cation, annotated
data lends itself to tasks such as automatic data annota-
tion and retrieval of unannotated data from annotation-type
queries.

A number of recent papers have considered generative
probabilistic models for such multi-type or relational data [2,
6, 4, 13]. These papers have generally focused on mod-
els that jointly cluster the di(cid:11)erent data types, basing the
clustering on latent variable representations that capture
low-dimensional probabilistic relationships among interact-
ing sets of variables.

In many annotation problems, however, the overall goal
appears to be that of (cid:12)nding a conditional relationship be-
tween types, and in such cases improved performance may be
found in methods with a more discriminative (cid:13)avor. In par-
ticular, the task of annotating an unannotated image can be
viewed formally as a classi(cid:12)cation problem|for each word
in the vocabulary we must make a yes/no decision. Stan-
dard discriminative classi(cid:12)cation methods, however, gener-
ally make little attempt to uncover the probabilistic struc-
ture of either the input domain or the output domain. This
seems ill-advised in the image/word setting|surely there
are relationships among the words labeling an image, and
these relationships re(cid:13)ect corresponding relationships among
the regions in that image. Moreover, it seems likely that
capturing these relationships would be helpful in annotat-
ing new images. With these issues in mind, we approach
the annotation problem within a framework that exploits
the best of both the generative and the discriminative tra-
ditions.

In this paper, we build a set of increasingly sophisticated
models for a database of annotated images, culminating in
correspondence latent Dirichlet allocation (Corr-LDA), a
model that (cid:12)nds conditional relationships between latent
variable representations of sets of image regions and sets
of words. We show that, in this class of models, only Corr-
LDA succeeds in providing both an excellent (cid:12)t of the joint
data and an e(cid:11)ective conditional model of the caption given
an image. We demonstrate its use in automatic image anno-
tation, automatic region annotation, and text-based image
retrieval.

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
SIGIR’03, July 28–August 1, 2003, Toronto, Canada.
Copyright 2003 ACM 1-58113-646-3/03/0007 ...$5.00.

2.

IMAGE/CAPTION DATA

Our work has focused on images and their captions from
the Corel database. Following previous work [2], each im-
age is segmented into regions by the N-cuts algorithm [12].
For each region, we compute a set of real-valued features
representing visual properties such as size, position, color,

z

r

w

N

M

D

Figure 1: The GM-Mixture model of images and
captions. Following the standard graphical model
formalism, nodes represent random variables and
edges indicate possible dependence. Shaded nodes
are observed random variables; unshaded nodes are
latent random variables. The joint distribution can
be obtained from the graph by taking the product of
the conditional distribution of nodes given their par-
ents (see Eq. 1). Finally, the box around a random
variable is a \plate," a notational device to denote
replication. The box around r denotes N replicates
of r (this gives the (cid:12)rst product in Eq. 1).

texture, and shape. Each image and its corresponding cap-
tion is represented as a pair (r; w). The (cid:12)rst element r =
fr1; : : : ; rN g is a collection of N feature vectors associated
with the regions of the image. The second element w =
fw1; : : : ; wM g is the collection of M words of the caption.

We consider hierarchical probabilistic models of image/caption

data which involve mixtures over underlying discrete and
continuous variables. Conditional on the values of these la-
tent variables, the region feature vectors are assumed to be
distributed as a multivariate Gaussian distribution with di-
agonal covariance, and the caption words are assumed to be
distributed as a multinomial distribution over the vocabu-
lary.

We are interested in models that can perform three tasks:
modeling the joint distribution of an image and its cap-
tion, modeling the conditional distribution of words given an
image, and modeling the conditional distribution of words
given a particular region of an image. The (cid:12)rst task can
be useful for clustering and organizing a large database of
images. The second task is useful for automatic image an-
notation and text-based image retrieval. The third task is
useful for automatically labeling and identifying a particular
region of an image.

3. GENERATIVE MODELS

In this section, we describe two hierarchical mixture mod-
els of image/caption data, noting their strengths and limi-
tations with respect to the three tasks described above. We
introduce a third model, correspondence latent Dirichlet al-
location, in which the underlying probabilistic assumptions
are appropriate for all three tasks.
3.1 A Gaussian-multinomial mixture model

We begin by considering a simple (cid:12)nite mixture model|
the model underlying most previous work on the probabilis-
tic modeling of multi-type data [2, 13].
In this model|

the Gaussian-multinomial mixture (GM-Mixture) shown
in Figure 1|a single discrete latent variable z is used to
represent a joint clustering of an image and its caption. As
shown in the (cid:12)gure, an image/caption is assumed to be gen-
erated by (cid:12)rst choosing a value of z, and then repeatedly
sampling N region descriptions rn and M caption words
wm conditional on the chosen value of z. The variable z is
sampled once per image/caption, and is held (cid:12)xed during
the process of generating its components. The joint distri-
bution of the hidden factor z and the image/caption (r; w)
is:

p(z; r; w) = p(z j (cid:21))   N
n=1 p(rn j z; (cid:22); (cid:27))(cid:1)
  M
m=1 p(wm j z; (cid:12)):

(1)

Given a (cid:12)xed number of factors K and a corpus of im-
ages/captions, the parameters of a GM-Mixture model can
be estimated by the EM algorithm1. This yields K Gaus-
sian distributions over features and K multinomial distri-
butions over words which together describe a clustering of
the images/captions. Since each image and its caption are
assumed to have been generated conditional on the same fac-
tor, the resulting multinomial and Gaussian parameters will
correspond. An image with high probability under a certain
factor will likely contain a caption with high probability in
the same factor.

Let us consider the three tasks under this model. First,
the joint probability of an image/caption can be computed
by simply marginalizing out the hidden factor z from Eq. (1).
Second, we can obtain the conditional distribution of words
given an image by invoking Bayes’ rule to (cid:12)nd p(z j r) and
marginalizing out the hidden factor:

p(w j r) =

z p(z j r)p(w j z):

Finally, we would like to compute a region-speci(cid:12)c distribu-
tion over words. This task, however, is beyond the scope of
the GM-Mixture model. Conditional on the latent factor
variable z, regions and words are generated independently,
and the correspondence between speci(cid:12)c regions and speci(cid:12)c
words is necessarily ignored.
3.2 Gaussian-Multinomial LDA

The latent Dirichlet allocation (LDA) model is a latent
variable model that allows factors to be allocated repeatedly
within a given document or image [3]. Thus, di(cid:11)erent words
in a document or di(cid:11)erent regions in an image can come from
di(cid:11)erent underlying factors, and the document or image as
a whole can be viewed as containing multiple \topics." This
idea can be applied directly to the joint modeling of images
and captions.

Gaussian-multinomial LDA (GM-LDA), shown in Fig-

ure 2, assumes the following generative process:

1. Sample a Dirichlet random variable (cid:18); this provides a
probability distribution over the latent factors, Mult((cid:18)).

2. For each of the N image regions:

(a) Sample zn (cid:24) Mult((cid:18)).

(b) Sample a region description rn conditional on zn.

3. For each of the M words:

1In Section 4.2.1, we consider Bayesian versions of all of
our models in which some the parameters are endowed with
prior distributions.

s
b
m
l

z

v

N

M

r

w

D

z

y

N

M

r

w

D

Figure 2: The GM-LDA model of images and cap-
tions. Unlike GM-Mixture (Figure 1), each word
and image region can potentially be drawn from a
di(cid:11)erent hidden factor.

(a) Sample vm (cid:24) Mult((cid:18)).
(b) Sample wm conditional on vm.

Note the importance of the plates. Within an image, all
the region descriptions and words are generated with (cid:18) held
(cid:12)xed; the latent factors for each word and region descrip-
tion can (potentially) vary. Each new image/caption is gen-
erated by again selecting from the Dirichlet variable (cid:18) and
repeating the entire process. Thus, we can view (cid:18) as a high-
level representation of the ensemble of image/caption pairs
in terms of a probability distribution over factors that each
image/caption can be assembled from.

The resulting joint distribution on image regions, caption

words, and latent variables is given as follows:

p(r; w; (cid:18); z; v) = p((cid:18) j (cid:11))

  N
n=1 p(zn j (cid:18))p(rn j zn; (cid:22); (cid:27))
  M
m=1 p(vm j (cid:18))p(wm j vm; (cid:12))

(cid:1)

:

As in the simpler LDA model, it is intractable to compute
the conditional distributions of latent variables given ob-
served data under this joint distribution, but e(cid:14)cient vari-
ational inference methods are available to compute approx-
imations to these conditionals (see [2] for details). Fur-
thermore, we can use variational inference methods to (cid:12)nd
the conditional probability p(w j r) needed for image anno-
tation/retrieval and the conditional probability p(w j r; rn)
needed for region labeling.

LDA provides signi(cid:12)cant improvements in predictive per-
formance over simpler mixture models in the domain of text
data [3], and we expect for GM-LDA to provide similar
advantages over GM-Mixture. Indeed, we will see in Sec-
tion 5 that GM-LDA does model the image/caption data
better than GM-Mixture. We will also see, however, that
good models of the joint probability of images and captions
do not necessarily yield good models of the conditional prob-
abilities that are needed for automatic annotation, text-
based image retrieval, and region labeling. We will argue
that this is due to the lack of a dependency between the
latent factors zn and vm which respectively generated the
images and their captions. In the next section, we turn to a
model that aims to correct this problem.
3.3 Correspondence LDA

We introduce correspondence LDA (Corr-LDA) as a model

that combines the (cid:13)exibility of GM-LDA with the associa-

Figure 3: The graphical model representation of the
Corr-LDA model. Note that the variables ym are
conditioned on N , the number of image regions.

bility of GM-Mixture. With this model, we achieve si-
multaneous dimensionality reduction in the representation
of region descriptions and words, while also modeling the
conditional correspondence between their respective reduced
representations.

Corr-LDA is depicted in Figure 3. The model can be
viewed in terms of a generative process that (cid:12)rst generates
the region descriptions and subsequently generates the cap-
tion words.
In particular, we (cid:12)rst generate N region de-
scriptions rn from an LDA model. Then, for each of the M
caption words, one of the regions is selected from the image
and a corresponding caption word wm is drawn, conditioned
on the factor that generated the selected region.

Formally, let z = fz1; z2; : : : ; zN g be the latent factors
that generate the image, and let y = fy1; y2; : : : ; yM g be
discrete indexing variables that take values from 1 to N
with equal probability. Conditioned on N and M , a K-
factor Corr-LDA model assumes the following generative
process for an image/caption (r; w):

1. Sample (cid:18) (cid:24) Dir((cid:18) j (cid:11)).

2. For each image region rn, n 2 f1; : : : ; N g:

(a) Sample zn (cid:24) Mult((cid:18))

(b) Sample rn (cid:24) p(r j zn; (cid:22); (cid:27)) from a multivariate

Gaussian distribution conditioned on zn.

3. For each caption word wm, m 2 f1; : : : ; M g:

(a) Sample ym (cid:24) Unif(1; : : : ; N )

(b) Sample wm (cid:24) p(w j ym; z; (cid:12)) from a multinomial

distribution conditioned on the zym factor.

Corr-LDA thus speci(cid:12)es the following joint distribution

on image regions, caption words, and latent variables:

p(r; w; (cid:18); z; y) = p((cid:18) j (cid:11))

  N
n=1 p(zn j (cid:18))p(rn j zn; (cid:22); (cid:27))
  M
m=1 p(ym j N )p(wm j ym; z; (cid:12))

(cid:1)

:

The independence assumptions of the Corr-LDA model
are a compromise between the extreme correspondence en-
forced by the GM-Mixture model, where the entire image
and caption are conditional on the same factor, and the lack
of correspondence in the GM-LDA model, where the image
regions and caption words can conceivably be conditional on
two disparate sets of factors. Under the Corr-LDA model,

a
q
s
b
m
 

 

q
a
s
b
m
 

 

the regions of the image can be conditional on any ensemble
of factors but the words of the caption must be conditional
on factors which are present in the image.
In e(cid:11)ect, our
model captures the notion that the image is generated (cid:12)rst
and the caption annotates the image.

Finally, note that the correspondence implemented by
Corr-LDA is not a one-to-one correspondence, but is more
(cid:13)exible: all caption words could come from a subset of the
image regions, and multiple caption words can come from
the same region.

4.

INFERENCE AND ESTIMATION

In this section, we describe approximate inference and pa-
rameter estimation for the Corr-LDA model. As a side
e(cid:11)ect of the inference method, we can compute approx-
imations to our three distributions of interest : p(w j r),
p(w j r; rn), and p(w; r).
4.1 Variational inference

Exact probabilistic inference for Corr-LDA is intractable;
as before, we avail ourselves of variational inference meth-
ods [7] to approximate the posterior distribution over the
latent variables given a particular image/caption.

In particular, we de(cid:12)ne the following factorized distribu-

tion on the latent variables:

q((cid:18); z; y) = q((cid:18) j (cid:13))

  N
n=1 q(zn j (cid:30)n)

  M
m=1 q(ym j (cid:21)m)

;

with free (variational) parameters (cid:13), (cid:30), and (cid:21). Each vari-
ational parameter is appropriate to its respective random
variable. Thus (cid:13) is a K-dimensional Dirichlet parameter,
(cid:30)n are N K-dimensional multinomial parameters, and (cid:21)m
are M N -dimensional multinomial parameters.

Following the general recipe for variational approxima-
tion, we minimize the KL-divergence between this factorized
distribution and the true posterior thus inducing a depen-
dence on the data (r; w). Taking derivatives with respect to
the variational parameters, we obtain the following coordi-
nate ascent algorithm:

1. Update the posterior Dirichlet parameters:

(cid:13)i = (cid:11)i +

N
n=1 (cid:30)ni:

2. For each region, update the posterior distribution over
factors. Note that this update takes into account the
likelihood that each caption word was generated by
the same factor as the region:

(cid:30)ni / p(rn j zn = i; (cid:22); (cid:27)) exp fEq[log (cid:18)i j (cid:13)]g (cid:1)

exp

M
m=1 (cid:21)mn log p(wm j ym = n; zm = i; (cid:12))

;

where Eq[log (cid:18)i j (cid:13)] = (cid:9)((cid:13)i) (cid:0) (cid:9)(
digamma function.

(cid:13)j), and (cid:9) is the

3. For each word, update the approximate posterior dis-

tribution over regions:

p(w j r; rn). In annotation, we approximate the conditional
distribution over words as follows:

p(w j r) (cid:25)

N

n=1 zn

q(zn j (cid:30)n)p(w j zn; (cid:12)):

In region labeling, the distribution over words conditioned
on an image and a region is approximated by:

p(w j r; rn) (cid:25)  zn

q(zn j (cid:30)n)p(w j zn; (cid:12)):

4.2 Parameter estimation

Given a corpus of image/caption data, D = f(rd; wd)gD

d=1,
we (cid:12)nd maximum likelihood estimates of the model param-
eters with a variational EM procedure that maximizes the
lower bound on the log likelihood of the data induced by
the variational approximation described above. In particu-
lar, the E-step computes the variational posterior for each
image and caption given the current setting of the param-
eters. The M-step subsequently (cid:12)nds maximum likelihood
estimates of the model parameters from expected su(cid:14)cient
statistics taken under the variational distribution. The vari-
ational EM algorithm alternates between these two steps
until the bound on the expected log likelihood converges.
4.2.1 Smoothing with empirical Bayes

In Section 5, we show that over(cid:12)tting can be a serious
problem, particularly when working with the conditional
distributions for image annotation. We deal with this issue
by taking a more thoroughgoing Bayesian approach, impos-
ing a prior distribution on the word multinomial parame-
ters (cid:12). We represent (cid:12) as a matrix whose columns are the
K multinomial parameters for the latent factors. We treat
each column as a sample from an exchangeable Dirichlet dis-
tribution (cid:12)i (cid:24) Dir((cid:17); (cid:17); : : : ; (cid:17)) where (cid:17) is a scalar parameter.
In place of a point estimate, we now have a smooth pos-
terior p((cid:12) j D). A variational approach can again be used to
(cid:12)nd an approximation to this posterior distribution [1]. In-
troducing variational Dirichlet parameters (cid:26)i for each of the
K multinomials, we (cid:12)nd that the only change to our earlier
algorithm is to replace the maximization with respect to (cid:12)
with the following variational update:

D

d=1 

(cid:26)ij = (cid:17) +

M
m=1 1(wdm = j)

N
n=1 (cid:30)ni(cid:21)mn;

and we replace all instances of (cid:12) in the variational inference
algorithm by expfE[log (cid:12) j (cid:26)]g.

Bayesian methods often assume a noninformative prior
which, in the case of the exchangeable Dirichlet, means set-
ting (cid:17) = 1. With K draws from the prior distribution,
however, we are in a good position to take the empirical
Bayes perspective [9] and compute a maximum likelihood
estimate of (cid:17). This amounts to a variant of the ML proce-
dure for a Dirichlet with expected su(cid:14)cient statistics under
(cid:26). Analogous smoothing algorithms are readily derived for
the GM-Mixture and GM-LDA models.

(cid:21)mn / exp

K
i=1 (cid:30)ni log p(wm j ym = n; zn = i; (cid:12))

:

5. RESULTS

These update equations are invoked repeatedly until the
change in KL divergence is small.

With the approximate posterior in hand, we can (cid:12)nd a
lower bound on the joint probability, p(w; r), and also com-
pute the conditional distributions of interest: p(w j r) and

In this section, we present an evaluation of all three mod-
els on 7000 images and captions from the Corel database.
We held out 25% of the data for testing purposes and used
the remaining 75% to estimate parameters. Each image is
segmented into 6-10 regions and is associated with 2-4 cap-
tion words. The vocabulary contains 168 unique terms.

 

 









0
5
6

0
0
6

0
5
5

0
0
5

0
5
4

0
0
4

0
5
3

y
t
i
l
i

b
a
b
o
r
p
 
g
o
l
 
e
v
i
t
a
g
e
n
 
e
g
a
r
e
v
A

Corr−LDA
GM−Mixture
GM−LDA
ML

0

50

100

150

200

Number of factors

Figure 4: The per-image average negative log prob-
ability of the held-out test set as a function of the
number of hidden factors (lower numbers are bet-
ter). The horizontal line is the model that treats
the regions and captions as an independent Gaus-
sian and multinomial, respectively.

5.1 Test set likelihood

To evaluate how well a model (cid:12)ts the data, we computed
the per-image average negative log likelihood of the test set
on all three models for various values of K. A model which
better (cid:12)ts the data will assign a higher likelihood to the test
set (i.e., lower numbers are better in negative likelihood).

Figure 5 illustrates the results. As expected, GM-LDA
provides a much better (cid:12)t than GM-Mixture. Further-
more, Corr-LDA provides as good a (cid:12)t as GM-LDA. This
is somewhat surprising since GM-LDA is a less constrained
model. However, both models have the same number of pa-
rameters; their similar performance indicates that, on aver-
age, the number of hidden factors used to model a particular
image is adequate to model its caption.2
5.2 Automatic annotation

Given a segmented image without its caption, we can use
the mixture models described in Section 3 to compute a
distribution over words conditioned on the image, p(w j r).
This distribution re(cid:13)ects a prediction of the missing caption
words for that image.

5.2.1 Caption perplexity

To measure the annotation quality of the models, we com-
puted the perplexity of the given captions under p(w j r) for
each image in the test set. Perplexity, which is used in the
language modeling community, is equivalent algebraically
to the inverse of the geometric mean per-word likelihood
(again, lower numbers are better):

perplexity = expf(cid:0)

Md
m=1 log p(wm j rd)=

D
d=1 Mdg:

D

d=1 

2Empirically, when K = 200, we (cid:12)nd that in only two images
of the test set does the GM-LDA model use more hidden
factors for the caption than it does for the image.

Figure 5 (Left) shows the perplexity of the held-out cap-
tions under the maximum likelihood estimates of each model
for di(cid:11)erent values of K. We see that over(cid:12)tting is a seri-
ous problem in the GM-Mixture model, and its perplexity
immediately grows o(cid:11) the graph (e.g., when K = 200, the
perplexity is 2922). Note that in related work [2], many
of the models considered are variants of GM-Mixture and
rely heavily on an ad-hoc smoothing procedure to correct
for over(cid:12)tting.

Figure 5 (Right) illustrates the caption perplexity under
the smoothed estimates of each model using the empirical
Bayes procedure from Section 4.2.1. The over(cid:12)tting of GM-
Mixture has been corrected. Once smoothed, it performs
better than GM-LDA despite the GM-LDA model’s supe-
rior performance in joint likelihood.

We found that GM-LDA does not provide good con-
it is \over-
ditional distributions for two reasons. First,
smoothed." Computing p(w j r) requires integrating a dif-
fuse posterior (due to the small number of regions) over all
the factor dimensions. Thus, the factors to which each re-
gion is associated are essentially washed out and, as K gets
large, the model’s performance approaches the performance
of the simple maximum likelihood estimate of the caption
words.

Second, GM-LDA easily allows caption words to be gen-
erated by factors that did not contribute to generating the
image regions (e.g., when K = 200, 54% of the caption
words in the test set are assigned to factors that do not
appear in their corresponding images). With this freedom,
the estimated conditional Gaussian parameters do not nec-
essarily re(cid:13)ect regions that are correctly annotated by the
corresponding conditional multinomial parameters. While
it better models the joint distribution of words and regions,
it fails to model the relationship between them.

Most notably, Corr-LDA (cid:12)nds much better predictive

distributions of words than either GM-LDA or GM-Mixture.
It provides as (cid:13)exible a joint distribution as GM-LDA but
guarantees that the latent factors in the conditional Gaus-
sian (for image regions) correspond with the latent factors
in the conditional multinomial (for caption words). Fur-
thermore, by allowing caption words to be allocated to dif-
ferent factors, the Corr-LDA model achieves superior per-
formance to the GM-Mixture which is constrained to as-
sociating the entire image/caption to a single factor. Thus,
with Corr-LDA, we can achieve a competitive (cid:12)t of the
joint distribution and (cid:12)nd superior conditional distributions
of words given images.

5.2.2 Annotation examples

Figure 6 shows ten sample annotations|the top (cid:12)ve words
from p(w j r)|computed by each of the three models for
K = 200. These examples illustrate the limitations and
power of the probabilistic models described in Section 3
when used for a practical discriminative task.

The GM-LDA model, as shown quantitatively in the pre-
vious section, gives the least impressive performance of the
three. First, we see washing out e(cid:11)ect described above by
the fact that many of the most common words in the corpus
| words like \water" and \sky" | occur in the predicted
captions for all of the pictures. Second, the predicted cap-
tion rarely predicts the object or objects that are in the pic-
ture. For example, it misses \jet" in the picture captioned
clouds, jet, plane, a word that both other models predict



Maximum likelihood

Empirical Bayes smoothed

0
0
1

0
9

0
8

0
7

0
6

0
5

0
4

0
3

l

y
t
i
x
e
p
r
e
p
n
o

 

Corr−LDA
GM−Mixture
GM−LDA
ML

i
t

p
a
C

0
0
1

0
9

0
8

0
7

0
6

0
5

0
4

0
3

Corr−LDA
GM−Mixture
GM−LDA
ML

l

y
t
i
x
e
p
r
e
p
n
o

 

i
t

p
a
C

0

50

100

150

200

0

50

100

150

200

Number of factors

Number of factors

Figure 5: (Left) Caption perplexity on the test set for the ML estimates of the models (lower numbers are
better). Note the serious over(cid:12)tting problem in GM-Mixture (values for K greater than (cid:12)ve are o(cid:11) the graph)
and the slight over(cid:12)tting problem in Corr-LDA. (Right) Caption perplexity for the empirical Bayes smoothed
estimates of the models. The over(cid:12)tting problems in GM-Mixture and Corr-LDA have been corrected.

True caption
market people
Corr−LDA
people market pattern textile display
GM−LDA
people tree light sky water
GM−Mixture
people market street costume temple

True caption
scotland water
Corr−LDA
scotland water flowers hills tree
GM−LDA
tree water people mountain sky
GM−Mixture
water sky clouds sunset scotland

True caption
bridge sky water
Corr−LDA
sky water buildings people mountain
GM−LDA
sky water people tree buildings
GM−Mixture
sky plane jet water snow

True caption
sky tree water
Corr−LDA
tree water sky people buildings
GM−LDA
sky tree fish water people
GM−Mixture
tree vegetables pumpkins water gardens

True caption
birds tree
Corr−LDA
birds nest leaves branch tree
GM−LDA
water birds nest tree sky
GM−Mixture
tree ocean fungus mushrooms coral

True caption
fish reefs water
Corr−LDA
fish water ocean tree coral
GM−LDA
water sky vegetables tree people
GM−Mixture
fungus mushrooms tree flowers leaves

True caption
mountain sky tree water
Corr−LDA
sky water tree mountain people
GM−LDA
sky tree water people buildings
GM−Mixture
buildings sky water tree people

True caption
clouds jet plane
Corr−LDA
sky plane jet mountain clouds
GM−LDA
sky water people tree clouds
GM−Mixture
sky plane jet clouds pattern

Figure 6: Example images from the test set and their automatic annotations under di(cid:11)erent models.

3

4

2

1

6

5

Corr−LDA:
1. PEOPLE, TREE
2. SKY, JET
3. SKY, CLOUDS
4. SKY, MOUNTAIN
5. PLANE, JET
6. PLANE, JET

GM−LDA:
1. HOTEL, WATER
2. PLANE, JET
3. TUNDRA, PENGUIN
4. PLANE, JET
5. WATER, SKY
6. BOATS, WATER

Figure 7: An example of automatic region labeling.

with high accuracy.

The GM-Mixture model performs better than GM-LDA,
but we can see how this model relies on the average im-
age features and fails to predict words for regions that may
not generally occur in other similar images. For example,
it omits \tree" from the picture captioned scotland, water
since the trees are only a small part on the left side of the
frame. Furthermore, the GM-Mixture predicts completely
incorrect words if the average features do not easily corre-
spond to a common theme. For example, the background
of (cid:12)sh, reefs, water is not the usual blue and GM-Mixture
predicts words like \fungus", \tree", and \(cid:13)owers."

Finally, as re(cid:13)ected by the term perplexity results above,
the Corr-LDA model gives the best performance and cor-
rectly labels most of the example pictures. Unlike the GM-
Mixture model,
it can assign each region to a di(cid:11)erent
cluster and the (cid:12)nal distribution over words re(cid:13)ects the en-
semble of clusters which were assigned to the image regions.
Thus, the Corr-LDA model (cid:12)nds the trees in the picture
labeled scotland, water and can correctly identify the (cid:12)sh,
even without its usual blue background.

As described in Section 3, the Corr-LDA and GM-LDA
models can furthermore compute a region-based distribution
over words, p(w j r; rn). Figure 7 illustrates a sample region
labeling on an image in the test set.3 Though both models
hypothesize the word \plane" and \jet," Corr-LDA places
them in the reasonable regions 2, 5, and 6 while GM-LDA
places them in regions 2 and 4. Furthermore, Corr-LDA
recognizes the top region as \sky, clouds" while GM-LDA
provides the enigmatic \tundra, penguin."
5.3 Text-based image retrieval

There has been a signi(cid:12)cant amount of computer science
research on content-based image retrieval in which a partic-
ular query image (possibly a sketch or primitive graphic) is
used to (cid:12)nd matching relevant images [5]. In another line of
research, multimedia information retrieval, representations
of di(cid:11)erent data types (such as text and images) are used to
retrieve documents that contain both [8].

Less attention, however, has been focused on text-based
image retrieval, an arguably more di(cid:14)cult task where a user
submits a text query to (cid:12)nd matching images for which
there is no related text. Previous approaches have essen-
tially treated this task as a classi(cid:12)cation problem, handling
speci(cid:12)c queries from a vocabulary of about (cid:12)ve words [10].
In contrast, by using the conditional distribution of words
given an image, our approach can handle arbitrary queries
from a large vocabulary.

We use a unique form of the language modeling approach

3We cannot quantitatively evaluate this task (i.e., compute
the region perplexity) because our data does not provide
ground-truth for the region labels.

to information retrieval [11] where the document language
models are derived from images rather than words. For each
unannotated image, we obtain an image-speci(cid:12)c distribu-
tion over words by computing the conditional distribution
p(w j r) which is available for the models described in Sec-
tion 3. This distribution provides a description of each image
in word-space which we use to (cid:12)nd images that are similar
to the words of the query.

More formally, denote an N -word query by q = fq1; : : : ; qN g.

For each image ri, its score relative to the query is:

scorei =   N

n=1 p(qn j ri);

where p(qn j ri) is the probability of the nth query word un-
der the distribution p(w j ri). After computing the score for
each image, we return a list of images ranked in descending
order by conditional likelihood.

Figure 8 illustrates three queries performed on the three
models with 200 factors and the held-out test set of images.
We consider an image to be relevant if its true caption con-
tains the query words (recall that we make no reference to
the true caption in the retrieval process). As illustrated by
the precision/recall curves, the Corr-LDA model achieves
superior retrieval performance. It is particularly strong with
di(cid:14)cult queries such as \people and (cid:12)sh." In this example,
there are only six relevant images in the test set and two of
them appear in the top (cid:12)ve. This is due to the ability of
Corr-LDA to assign di(cid:11)erent regions to di(cid:11)erent clusters.
The model can independently learn the salient features of
\(cid:12)sh" and \people" and e(cid:11)ectively combine them to per-
form retrieval.

6. SUMMARY

We have developed Corr-LDA, a powerful model for an-
notated data that combines the advantages of probabilis-
tic clustering for dimensionality reduction with an explicit
model of the conditional distribution from which data an-
notations are generated.
In the setting of image/caption
data, we have shown that this model can achieve a competi-
tive joint likelihood and superior conditional distribution of
words given an image.

Corr-LDA provides a clean probabilistic model for per-
forming various tasks associated with multi-type data such
as images and their captions. We have demonstrated its
use in automatic image annotation, automatic image region
annotation, and text-based image retrieval.
It is impor-
tant to note that this model is not specially tailored for
image/caption data. Given good features, the Corr-LDA
model can be applied to any kind of annotated data such as
video/closed-captions, music/text, and gene/functions.

Acknowledgments

The authors thank David Forsyth and Kobus Barnard for
numerous conversations and the well-curated data on which
this work is based. This work was supported by a grant
from the Intel Corporation, the National Science Founda-
tion (NSF grant IIS-9988642), and the Multidisciplinary
Research Program of the Department of Defense (MURI
N00014-00-1-0637). David M. Blei was additionally sup-
ported by a grant from the Microsoft Corporation.

candy

sunset

people & fish

i

i

n
o
s
c
e
r
P

Corr−LDA
GM−Mixture
GM−LDA

0

.

1

8

.

0

6

.

0

4

.

0

2

.

0

0

.

0

i

i

n
o
s
c
e
r
P

Corr−LDA
GM−Mixture
GM−LDA

0

.

1

8

.

0

6

.

0

4

.

0

2

.

0

0

.

0

i

i

n
o
s
c
e
r
P

Corr−LDA
GM−Mixture
GM−LDA

0

.

1

8

.

0

6

.

0

4

.

0

2

.

0

0

.

0

0.0

0.2

0.4

0.6

0.8

1.0

0.0

0.2

0.4

0.6

0.8

1.0

0.0

0.2

0.4

0.6

0.8

1.0

Recall

Recall

Recall

Candy

Sunset

People
& Fish

Figure 8: Three examples of text-based image retrieval. (Top) Precision/recall curves for three queries on a
200-factor Corr-LDA model. The horizontal lines are the mean precision for each model. (Bottom) The top
(cid:12)ve returned images for the same three queries.

7. REFERENCES
[1] H. Attias. A variational Bayesian framework for

graphical models. In Advances in Neural Information
Processing Systems 12, 2000.

[2] K. Barnard, P. Duygulu, N. de Freitas, D. Forsyth,

D. Blei, and M. Jordan. Matching words and pictures.
Journal of Machine Learning Research, 3:1107{1135,
2003.

[3] D. Blei, A. Ng, and M. Jordan. Latent Dirichlet

allocation. Journal of Machine Learning Research,
3:993{1022, January 2003.

[4] D. Cohn and T. Hofmann. The missing link|A

probabilistic model of document content and
hypertext connectivity. In Advances in Neural
Information Processing Systems 13, 2001.

[5] A. Goodrum. Image information retrieval: An

overview of current research. Informing Science,
3(2):63{67, 2000.

[6] J. Jeon, V. Lavrenko, and R. Manmatha. Automatic

image annotation and retrieval using cross-media
relevance models. In ACM SIGIR 2003, July 2003.

[7] M. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul.

Introduction to variational methods for graphical
models. Machine Learning, 37:183{233, 1999.

[8] C. Meghini, F. Sebastiani, and U. Straccia. A model

of multimedia information retrieval. Journal of the
ACM (JACM), 48(5):909{970, 2001.

[9] C. Morris. Parametric empirical Bayes inference:

Theory and applications. Journal of the American
Statistical Association, 78(381):47{65, 1983.

[10] M. Naphade and T. Huang. A probabilistic framework

for semantic video indexing, (cid:12)ltering and retrieval.
IEEE Transactions on Multimedia, 3(1):141{151,
March 2001.

[11] J. Ponte and B. Croft. A language modeling approach

to information retrieval. In ACM SIGIR 1998, pages
275{281.

[12] J. Shi and J. Malik. Normalized cuts and image

segmentation. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 22(9):888{905, 2000.

[13] B. Taskar, E. Segal, and D. Koller. Probabilistic
clustering in relational data. In IJCAI-01, pages
870{876, 2001.

