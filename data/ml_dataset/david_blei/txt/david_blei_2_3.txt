Context,Learning,andExtinctionSamuelJ.Gershman,DavidM.Blei,andYaelNivPrincetonUniversityA.Redishetal.(2007)proposedareinforcementlearningmodelofcontext-dependentlearningandextinctioninconditioningexperiments,usingtheideaof“stateclassification”tocategorizenewobservationsintostates.Inthecurrentarticle,theauthorsproposeaninterpretationofthisideaintermsofnormativestatisticalinference.Theyfocusonrenewalandlatentinhibition,2conditioningparadigmsinwhichcontextualmanipulationshavebeenstudiedextensively,andshowthatonlineBayesianinferencewithinamodelthatassumesanunboundednumberoflatentcausescancharacterizeadiversesetofbehavioralresultsfromsuchmanipulations,someofwhichposeproblemsforthemodelofRedishetal.Moreover,inbothparadigms,contextdependenceisabsentinyoungeranimals,orifhippocampallesionsaremadepriortotraining.Theauthorssuggestanexplanationintermsofarestrictedcapacitytoinfernewcauses.Keywords:classicalconditioning,renewal,latentinhibition,Bayesian,hippocampusAnenduringprobleminthestudyofclassicalconditioningishowanimalslearnaboutthecausalstructureoftheirenvironment(Blaisdell,Sawa,Leising,&Waldmann,2006).Mosttheoriesframeconditioningasthelearningofassociationsbetweenstimuliandreinforcement(Pearce&Bouton,2001;Rescorla&Wagner,1972).Underastatisticalinterpretation,theseassociationsareparametersofagenerativemodelinwhichstimulicausereinforce-ment(Kakade&Dayan,2002).However,evidencesuggeststhatanimalsmayemploymoreflexiblemodels,learning,forexample,thatsomestimuliarecausallyunrelatedtoreinforcement(Dayan,Kakade,&Montague,2000;Dayan&Long,1998).Amoreradicaldeparturearelatentcausemodels(Courville,2006;Cour-ville,Daw,Gordon,&Touretzky,2004;Courville,Daw,&Touretzky,2002),inwhichbothstimuliandreinforcementareattributedtocausesthatarehiddenfromobservation.Onemoti-vationforsuchmodelsisthefindingthatlearnedrelationshipsbetweencuesandreinforcementarenotnecessarilyerasedfollow-ingextinction:Returningtheanimaltotheoriginaltrainingcontextafterextinctioninadifferentcontextcanleadtorenewaloftheconditionedresponse(Bouton,2004;Bouton&Bolles,1979).Theseandrelateddatacanbecharacterizedbyalatentcausemodelinwhichdifferentlatentcausesareassociatedwiththetrainingandextinctioncontexts.Oneproblemwithlatentcausemodelsisthatthenumberofdifferentlatentcausesisingeneralunknown.Thechallenge,then,istoformulatealearningalgorithmthatcaninfernewcausesasitgathersobservations,aswellaslearnthestatisticalrelationshipsbetweencausesandobservations.Recently,Redish,Jensen,Johnson,andKurth-Nelson(2007)formulatedsuchatheoryofextinctionlearning,combiningthewell-studiedframeworkoftemporaldiffer-encereinforcementlearning(Schultz,Dayan,&Montague,1997;Sutton&Barto,1998)withastateclassificationmechanismthatallowsthestatespacetoexpandadaptively.Intheirmodel,statescanbelooselyinterpretedaslatentcauses,servingtoexplainbothstimuliandreinforcementintermsofanunderlyingdiscretevariable.Wesuggestanewmodeloflatentcauseinferenceinanimallearningthatisbasedinanormativestatisticalframework.Withthismodel,weaddresscertainlimitationsofthetheoryofRedishetal.(2007),whilestillcapturingitsessentialinsights.Weagreewiththeirassertionthatthecomputationalproblemtheanimalmustsolveisoneofstructurelearning.Wepositthatthecompu-tationalprinciplesatworkinstructurelearningarebasedonagenerativemodeloftheenvironmentthatspecifiestheanimal’saprioribeliefsabouthowitsobservationsweregeneratedbylatentcauses.Givenasetofobservations,theproblemfacingtheanimalistocombineitspriorbeliefswiththeevidenceprovidedbytheobservationstoinferwhichcauseswereinaction.Atthealgorith-miclevel,weidentifyseveralfeaturesofRedishetal.’smodelthatmakeitdifficulttoaccountforrelevantdataandshowhowtheseareobviatedinourmodel.Finally,drawingonasuggestionbyRedishetal.,wemakeexplicitthecomputationalroleplayedbythehippocampusinourmodelandusethistoexplaindevelop-mentalchangesinlearning.Redishetal.’s(2007)ModelThedatamotivatingthemodelofRedishetal.(2007)camefromaconditioningprocedurestudiedbyBoutonandBolles(1979):Intheconditioningphase,theanimalisplacedinContextAandexposedineachtrialtobothastimuluscueandareinforcer;eventuallythecuecomestoevokeaconditionedresponse.Intheextinctionphase,theanimalisthenplacedinanewcontext(B)andexposedineachtrialtothecueintheabsenceofreinforce-ment,untilthecueceasestoevoketheconditionedresponse.ItSamuelJ.GershmanandYaelNiv,PsychologyDepartmentandNeu-roscienceInstitute,PrincetonUniversity;DavidM.Blei,ComputerScienceDepartment,PrincetonUniversity.DavidM.BleiissupportedbygrantsfromGoogleandMicrosoftResearch.WethankStephenMaren,MarieMonfils,andDavidRedishforcommentsonthearticle.WethankNathanielDaw,MichaelTodd,AnatoleGershman,andKenNormanforhelpfuldiscussions.CorrespondenceconcerningthisarticleshouldbeaddressedtoSamuelJ.Gershman,PsychologyDepartment,PrincetonUniversity,Princeton,NJ08540.E-mail:sjgershm@princeton.eduPsychologicalReview©2010AmericanPsychologicalAssociation2010,Vol.117,No.1,197–2090033-295X/10/$12.00DOI:10.1037/a0017808197wouldappear,atfirstglance,thattheanimalhas“unlearned”itsresponsetothecue.However,iftheanimalisreturnedtotheoriginalcontext(A)inatestphaseandpresentedwiththecue,theresponseisrestored,stronglysuggestingotherwise.Rather,itseemsthattheanimalhaslearnedanewrelationshipbetweenthecueandthereinforcerduringextinctionthatwassomewhatlimitedtoContextB.Thisphenomenon,knownasABArenewal,isexplainedbyRedishetal.’s(2007)modelintermsoftwosimultaneouspro-cesses:avalue-learningprocessandastate-classificationprocess.Thefirstupdatesvaluesassociatedwithstates(andpotentiallyactions),usingaformofthetemporaldifferencelearningalgo-rithm(Sutton&Barto,1998),whichiscloselyrelatedtotheRescorla-Wagnerupdaterule(Rescorla&Wagner,1972).Astate’svaluerepresentsapredictionaboutfuturereinforcementinthatstate.Thetemporaldifferencelearningruleincrementallyupdatesvaluesinproportiontothediscrepancybetweenpredictedandreceivedreinforcement(thepredictionerror).InthePavlovianversionoftherenewalparadigmdescribedhere,theanimal’sconditionedresponseispresumedtobeproportionaltothecurrentstate’svalue(seeDayan,Niv,Seymour,&Daw,2006).IntheoperantversionmodeledbyRedishetal.,theprobabilityoftheanimaltakingaparticularaction(e.g.,leverpress)isproportionaltothestate-actionvalue.TheinnovationofRedishetal.(2007)istointroduceastate-classificationprocessthatdetermineswhatstatetheanimaliscurrentlyinandcreatesnewstateswhentheobservationstatis-ticschange.Theobservations,inthiscase,aredefinedtobetuplesconsistingof{context,stimulus,immediatereinforcement,timesincelastreinforcement}.Theactualmechanicsofthestateclas-sificationarequitesophisticated,andwereferthereadertotheoriginalarticle.Inbrief,acompetitivelearningmodelusingradialbasisfunctionsandclassifierexpansion(Grossberg,1976;Hertz,Krogh,&Palmer,1991)partitionstheinputspaceintomultivariateGaussianstateprototypes;temporaldifferencelearningthenoper-atesonthesestates.Forourpurposes,theimportantaspectofthisprocessisthateachstateisassociatedwithanobservationproto-type,andanewobservationisclassifiedasaparticularstateonthebasisofitsmatchtothestate’sprototype.Whentheobservationfailstomatchanyprototype(asdeterminedbyathreshold),anewstate/prototypeisinferred.Alocalestimateoftheaveragenegativepredictionerrormodulatesthisprocess:Whenpredictionerrorsaretonicallynegative,anewstateismorelikelytobeinferred.Accordingtothismodel,acquisitionintheABArenewalpar-adigmproceedsaccordingtothevalue-learningprocess,withalltrainingobservationsbeingassignedtothesamestate(becausetheirstatisticsarehomogeneous).Duringextinction,theabsenceofthepredictedreinforcementresultsintonicnegativepredictionerrors.Combinedwithacontextchange,thisresultsinthestate-classificationprocesscreatinganewstate.Thus,onestateisassociatedwithreinforcement,andanotherstateisassociatedwithnoreinforcement.Whentheanimalisreturnedtothetrainingcontext,itidentifiesitsobservationsasbelongingtothestateassociatedwithreinforcement(onthebasisofthecontextualcue)andthereforeproducestheconditionedresponse.Oneimplicationofthismodelisthatnewstatesareunlikelytobeinferredwhenpredictionerrorsaretonicallypositive.Evidenceincontradictionofthishypothesiscomesfromthecontextdependenceoflatentinhibition(Hall&Honey,1989).Thelatentinhibitionprocedureis,insomesense,aconcomitantmanipulationtoextinction:Ananimalisfirstexposedtoastimulusintheabsenceofreinforce-mentandislaterconditionedwithpairingofthestimulusandrein-forcement.Inthiscase,animalsareslowertoacquireaconditionedresponsecomparedwithanimalsthathavenotbeenpre-exposed.However,ifthepre-exposureandconditioningphasesareconductedindifferentcontexts,thelatentinhibitioneffectisdiminished.Heretheconditioningphaseisaccompaniedbypositivepredictionerrors(asthereinforcementisunexpectedfollowingthepre-exposure),which,accordingtoRedishetal.’s(2007)model,shouldnotresultintheinferenceofanewstate.Hence,theirmodelmispredictsthatashiftincontextwillnotaffectlatentinhibition.1ThisproblemwasalsonotedbyRedishetal.Anotherproblemisthatbecausethevaluesassociatedwithnewstatesareinitializedtozero,Redishetal.’s(2007)modeldoesnotpredictABCandAABrenewal(Bouton&Bolles,1979;Bouton&King,1983),inwhichthetesttrialsoccurinacompletelynewcontext.Inboththesecases,conditionedrespondingreturnsduringthetestphase.Thiscanbefairlyeasilyaccomodatedbyinitializingthevaluesofnewstatestosomepriorbeliefaboutstatevalues,aswediscusshereinconnectionwithourmodel.Apartfromtheseproblems,theideaofstateclassificationonthebasisofobservationstatisticsisafundamentalcontribution.Infor-mulatingaquantitativetheoryofhowanimalssolvethisproblem,wewouldliketounderstandthestatisticalprinciplesunderlyingthisinsight.Tothisend,weproposeanewmodelthatisconceptuallyalignedwiththatofRedishetal.(2007)butmoredirectlydescendedfromthelatentcausetheoryofCourville(2006).Therestofthisarticleisorganizedasfollows:Wefirstdescribeaninfinite-capacitymixturemodelandaparticlefilteralgorithmforperforminginferenceinthismodel.Wethenpresenttheresultsofsimulationsoflatentinhibitionandrenewalparadigms,includ-ingdevelopmentalandhippocampalmanipulations.Inthediscus-sion,wecompareourmodelwiththatofRedishetal.(2007),aswellasseveralothermodels.Finally,wediscusssomelimitationsofourmodelandproposedirectionsforitsfuturedevelopment.ANewModel:StatisticalInferenceinanInfinite-CapacityMixtureModelThecentralclaimofouraccountisthat,toadaptivelypredicteventsintheirenvironment,animalsattempttopartitionobserva-tionsintoseparategroupsonthebasisoftheirproperties.Thistaskisknownasclusteringincomputerscienceandstatistics,andhence,wecallthesegroupsclusters.Weassumethattheanimal’sgoalistoassignobservationstoclusterssuchthattheclusterscorrespondtodifferentlatentcauses.Renewalcanthenbeunder-stoodastheresultofthisclusteringprocess.Specifically,wesuggestthattheanimallearnstopartitionitsobservationsonthebasisoftheirfeaturesintotwodistinctclusters,correspondingtotheacquisitionandextinctionphases(whichareimplicitlythecausesoftheanimal’sobservations).1Redishetal.’s(2007)modelwilldemonstratelatentinhibitionifthemodulationofstateclassificationbytonicpredictionerrorisweak.Inthiscase,stateclassificationisdrivenprimarilybythematchbetweenthecurrentobservationandtheprototypes.However,thisscenarioisatoddswiththecentralroleplayedbytonicpredictionerrorinRedishetal.’smodel.198GERSHMAN,BLEI,ANDNIVOurbasicapproachistofirstformulateasetofassumptionsthatweimputetotheanimalandthendescribehow,onthebasisoftheseassumptions,theanimalcanmakerationalinferencesaboutlatentcausesgivenasetofobservations.Thesetofassumptionsconstitutesthegenerativemodel,whichrepresentstheanimal’spriorbeliefsaboutthestatisticalstructureandprobabilisticdependenciesbetweenvariables(bothhiddenandobserved)intheenvironment.Thegener-ativemodelexpressesthestateoftheanimal’sbeliefspriortomakinganyobservations.Givenasetofobservations,weexpecttheanimal’sbeliefs(orinference)abouttheactualcausesoftheseobservationstochange.Inparticular,thisnewstateofknowledgeisexpressedbyaposteriordistributionoverunobserved(hidden)variablesgiventheobservedvariables.Werefertothisastheanimal’sinferencemodel.2Inthecontextoftheclassicalconditioningdatathatwemodel,theinferencemodelrepresentstheanimal’sbeliefsaboutthelatentcausesofitsobservations.GenerativeModelWeassumethattheanimal’sobservationontrialttakestheformofadiscrete-valuedmultidimensionalvectorft,withthefollowingdimensions:reinforcement(ft,1),cue(ft,2)andcontext(ft,3).Thereinforcementdimensionrepresentsabinaryuncondi-tionedstimulusdeliveredtotheanimal(e.g.,ft,1僆{reinforcement,noreinforcement}).ThecuedimensionrepresentsatypicalPav-loviancue(oritsabsence;e.g.,ft,2僆{tone,notone}).3Thecontextdimensionisanabstractionofthecontextmanipulationstypicalinrenewalparadigms(e.g.,boxcolor,shape,odor),whichwesimplifyintodiscretevalues:ft,3僆{ContextA,ContextB,ContextC}.Thegenerativemodelweimputetotheanimalisoneinwhich,oneachtrial,asinglelatentcauseisresponsibleforgeneratingalltheobservationfeatures(reinforcement,cue,context).Insuchamixturemodel,eachtrialisassumedtobegeneratedbyfirstsamplingacausect(fromaknownsetofcauses)accordingtoamixingdistributionP(c)andthensamplingobservationfeaturesconditionedonthecausefromanobservationdistributionP(f|ct).Thistypeofgenerativemodelisareasonablepriorbeliefformanyenvironments.Infact,itcorrectlyexpresses,toafirstapproxima-tion,theprocessbywhichmanyconditioningproceduresaregenerated:Firstaphase(e.g.,conditioning,extinction,test)isselected,andthenasetofstimuliareselectedconditionedonthephase.Iftheanimalassumesthateachobservationisprobabilis-ticallygeneratedbyasinglelatentcause,thenclusteringistheprocessofrecoveringthesecausesonthebasisofitsobservations.4Themixturemodeldescribedsofarimplicitlyassumesthattheanimalknowshowmanypossiblecausesthereareintheenviron-ment.Thisseemsanunreasonableassumptionaboutthestructureoftheanimal’senvironment,aswellastheanimal’saprioriknowledgeaboutitsobservations.Furthermore,aswediscusslater,thereisevidencethatanimalscanflexiblyinfertheexistenceofnewcausesasmoreobservationsaremade.Wethususeagenerativemodelthatallowsforanunbounded(expanding)num-beroflatentcauses(aninfinitecapacitymixturemodel,asde-scribedbelow).Inthismodel,theanimalprefersasmallnumberofcausesbutcan,atanytime,infertheoccurrenceofanewlatentcausewhenthedatasupportitsexistenceandthusdecidetoassignitscurrentobservationtoacompletelynewcluster.Formally,letusdenoteapartitionofobservations(trials)1,...,tbythevectorc1:t.Apartitionspecifieswhichobservationsweregeneratedbywhichcauses,suchthatct⫽kindicatesthattheobservationtwasassignedtoclusterk.Inourmodel,theanimal’sprioroverpartitionstakestheformofasequentialstochasticgenerativeprocess(Aldous,1985;Pitman,2002)thatgeneratescausekwithprobabilityP共ct⫹1⫽k兩c1:t兲⫽冦Nkt⫹␣ifkⱕKt共i.e.,kisanoldcause兲␣t⫹␣ifk⫽Kt⫹1共i.e.,kisanewcause兲,(1)whereNkisthenumberofobservationsalreadygeneratedbycausek(bydefaultitisassumedthatc1⫽1)andKtisthenumberofuniquecausesgeneratedforobservations1tot.Thenumberofcausesgeneratingobservations1,...,tisnowarandomvariableandcanbeanynumberfrom1tot.Theconcentrationparameter␣specifiestheanimal’spriorbeliefaboutthenumberofcausesintheenvironment.When␣⫽0,theanimalassumesthatallobser-vationsaregeneratedbyasinglecause;when␣approaches⬁,theanimalassumesthateachobservationisgeneratedbyauniquecause.Ingeneral,for␣⬍⬁,theanimalassumesthatobservationstendtobegeneratedbyasmallnumberofcauses.5Theanimalfurtherassumesthatonceacausehasbeensampledforatrial,anobservationissampledfromanobservationdistri-butionconditionalonthecause.Eachcauseisassociatedwithamultinomialobservationdistributionoverfeatures,parameterizedby␾,where␾i,j,kistheprobabilityofobservingvaluej(e.g.,2Thisisalsosometimesreferredtoastherecognitionmodel(Dayan&Abbott,2001).3Thechoiceofdiscrete-valuedobservationsisnotcrucialtoourfor-malism;wehaveusedreal-valuedfeaturesandobtainedessentiallythesameresults.4Weusethetermcauseinconnectionwiththegenerativemodelandthetermclusterinconnectionwiththeinferenceprocedure.Theclustersinferredbytheanimalmaynotbeidenticaltothetruecausesofitsobservations.5This“infinite-capacity”distributionovercausesisknowninstatisticsandmachinelearningasaChineserestaurantprocess(Aldous,1985;Pitman,2002).WeusetheChineserestaurantprocessasanintra-agentprioronthestructureoftheenvironment,whichisthebasis(andprovidestheconstraints)fortheinferenceprocessonceobservationsareseen.AChineserestaurantprocessisaprobabilitydistributionoverpartitionsofobservations,whereapartitionisavectorindicatingthelatentcauseofeachobservation.Itsnamecomesfromthefollowingmetaphor:ImagineaChineserestaurantwithanunboundednumberoftables(causes).Thefirstcustomer(observation)entersandsitsatthefirsttable.Subsequentcus-tomerssitatanoccupiedtablewithaprobabilityproportionaltohowmanypeoplearealreadysittingthere,andatanewtablewithprobabilityproportionaltoalpha,aconcentrationparameter.Onceallthecustomersareseated,onehasapartitionofobservationsintocauses.InaChineserestaurantprocessmixturemodel(alsoknownasaDirichletprocessmixturemodel),eachcauseisassociatedwithaparameterizeddistributionoverfeaturessothatanobservation’sfeaturepropertiesaredeterminedbyitslatentcause(anditsassociatedparameters).Observationsgeneratedbythesamecausewilltendtohavesimilarfeaturesbyvirtueofsharingtheseparameters.199CONTEXT,LEARNING,ANDEXTINCTIONreinforcement)forfeaturei,givenlatentcausek.Acommonassumptioninmixturemodels(whichweadopthere)isthat,inthegenerativemodel,eachfeatureisconditionallyindependentofalltheotherfeaturesgivenalatentcauseandthemultinomialparam-eters.Forinstance,alatentcausethatcanbelabeledas“trainingtrial”mightgenerateacuewithprobability␾2,tone,k⫽“training”⫽1and,independently,generatereinforcementwithsomeprobabil-ity␾1,reinforcement,k⫽“training”(possiblylessthan1),whereasalatentcauselabeledas“extinctiontrial”mightgenerateacuewithprobability1andreinforcementwithprobability0.Theconditionalindependenceassumptionexpressestheideathat,giventheiden-tityofthelatentcause,cuesandreinforcementareseparatelygenerated,eachaccordingtoitsassociatedprobability␾i,j,k.WeassumethatthemultinomialparametersthemselvesaredrawnfromaDirichletdistribution(theconjugatepriorforthemultinomialdistribution).Thispriorexpressestheanimal’spre-dictionsabouttheexperimentbeforeithasmadeanyobservations.Giventhattheanimalisunlikelytohavestrongaprioripredictionsabouttheexperimentbeforeithasbegun,wechosetheparametersoftheDirichletdistributionsothatallpossiblemultinomialpa-rametershaveequalprobabilityundertheprior.Notethateachcauseisendowedwithitsownmultinomialdistributionoverfeatures;thisallowsdifferentcausestobeassociatedwithdifferentobservationstatistics.EverytimeanewcauseiscreatedbyEqua-tion1,theparametersofitscorrespondingmultinomialdistributionaredrawnfromtheDirichletprior.ItmayatfirstappearoddthatcausesinEquation1aregeneratedpurelyonthebasisofhowmanytimesaparticularcausewasgeneratedinthepastandthatfeaturesaregeneratedindependentlyfromoneanothergivenacauseandmultinomialparameters.Intuitively,onewouldexpectthatsimilarobserva-tionswouldbegeneratedbythesamecause.Indeed,thisintuitionisfaithfullyembodiedinthemodel.Animportantpointtokeepinmindisthatinthegenerativemodel,observa-tionswillbesimilarbecausetheyweregeneratedbythesamecause.Similarly,featureswillexhibitcorrelationsbecausetheyarecoupledbyacommoncause(e.g.,thelatentcauseassoci-atedwiththetrainingphaseofaconditioningexperimentwilltendtogenerateboththecueandthereinforcement).Whenfacedwithuncertaintyaboutthelatentcausesofitsobserva-tions,bothofthesepropertieswillinfluencetheanimal’sbe-liefsintheinferencemodel(describedinthenextsection).First,theanimalwillusethesimilaritybetweentrialstoinferwhatlatentcausetheycamefrom.Asaresult,thebeliefaboutthecausesofonetrialwillnolongerbeindependentoftheothertrials.Second,theanimal’sbeliefsaboutthefuturevalueofonefeature(e.g.,reinforcement)willdependintheinferencemodelonitsknowledgeaboutotherfeatures(e.g.,contextandcue).Inotherwords,observationfeatureswillbeconditionallydepen-dentwhenthelatentcauseisunknown(i.e.,inallrealisticscenarios).InferenceModelTherearetwocomponentstotheinferenceproblemfacingtheanimal:identifyingthelatentcausesofitsobservationsandpre-dictingreinforcementgivenapartialobservation(contextandcues).Becauseinourmodelpredictiondependsoninferencesaboutlatentcauses,weaddresseachoftheseinturn.Denotetheobservationsintrials1,...,tbythevectorF1:t.Givenasetofobservationsuptotrialt,whataretheanimal’sbeliefsaboutthelatentcausesoftheseobservations?AccordingtoBayesianstatisticalinference,thesebeliefsarerepresentedbytheposteriordistributionoverpartitionsgiventheobservations:P共c1:t兩F1:t兲⫽P共F1:t兩c1:t兲P共c1:t兲P共F1:t兲.(2)Exactcomputationoftheposteriorinthismodeliscomputation-allydemanding.Moreover,forsuchamodeltobeplausiblyrealizedbyanimals,learningandinferencemustbeincrementalandonline.Oneapproximateinferencealgorithmwhichisbothtractableandincrementalistheparticlefilter(Fearnhead,2004).Thisalgorithmapproximatestheposteriordistributionoverparti-tionswithasetofweightedsamplesandhasbeenusedsuccess-fullytomodelanumberoflearningphenomena(Brown&Steyvers,2009;Daw&Courville,2008;Sanborn,Griffiths,&Navarro,2006).Theessentialideainparticlefilteringistocreateasetofmhypotheticalparticles,eachofwhichisaspecificpartitionofallthetrialsintocauses,andthenweighttheseparticlesbyhowlikelytheyaretohavegeneratedtheparticularsetofobservationsthathasbeenseen.Theweightswilldependonfactorssuchaswhethersimilarobservationsareclusteredtogetherinaparticularparticleandthenumberoflatentcausesinthepartition.Theywillalsodependonmultiplicativeinteractionsbetweenfeatures,suchthataparticlewillreceivelargerweighttotheextentthatitpredictsconsistentconfigurationsoffeaturevalues.Adetaileddescriptionoftheparticle-filteralgorithmcanbefoundintheAppendix.Weassumethattheanimal’sgeneralgoalinaclassicalcondi-tioningexperimentistopredicttheprobabilityofreinforcementwhenobservinga“test”observationvectorthatlacksthefirstfeature(i.e.,whereitisnotyetspecifiedwhetherreinforcementwillorwillnotoccur).Thispredictioncanrelyonthepresenceorabsenceoftheotherfeatures(contextandcue)aswellasalloftheanimal’spreviousexperience.Inourmodel,thispredictionisaccomplishedbyaugmentingeachparticlewithaclusterassign-mentofthetestobservationandthenaveragingtheprobabilityofreinforcementoveralltheparticles,weightedbytheposteriorprobabilityofthetestclusterassignment(seetheAppendixforthecorrespondingequations).Weassumethattheanimal’scondi-tioned(Pavlovian)responseisproportionaltothepredictedprob-abilityofreinforcement(Dayanetal.,2006)andsoreportthereinforcementpredictionintheresults.ResultsExceptwhereotherwisementioned,forthesimulationsreportedhere,weuseduniformDirichletpriorsoverallfeaturesand␣⫽.1astheconcentrationparameter.6Allthesimulationsused3,0006Althoughalphacanbelearnedstraightforwardlywiththeparticlefilter,oursimulationssuggestthatthisaddedflexibilitydoesnotchangetheresultssubstantially,sowehavefixedittoaconstantvalue.200GERSHMAN,BLEI,ANDNIVparticles.7Foreachphase(pre-exposure,conditioning,extinction),trialswereidenticalreplicasofeachother(i.e.,therewasnonoiseinjectedintotheobservations).Althoughtheoutputoftheparticlefilterisstochastic(becauseofthesample-generatingprocess),itreturnseffectivelythesameresultsonmultiplerunsbyaveragingoveralargenumberofparticles.AcquisitionandExtinctionAsapreliminaryillustrationofthemodel’sbasicbehavior,Figure1showssimulatedconditionedrespondingbeforeandafteracontextchange.Thesolidlinerepresentsconditionedrespondingwhentheanimalcontinuestobereinforcedafterthecontextchange.Thedashedlinerepresentsconditionedrespondingwhentheanimalisnolongerreinforcedafterthecontextchange(i.e.,extinction).Asexpected(andinagreementwithmostmodelsofconditioning),respondingincreasedduringtheconditioningphaseanddecreasedduringtheextinctionphase.Notethatthecondi-tionedresponsebeganat50%,whichfollowsfromtheuniformprioroverthereinforcementfeature.Inothersettings,adifferentpriormaybemoresuitable(e.g.,toexpresstheanimal’spriorexpectationthatitwillnotgetreinforced).RenewalFigure2ashowsexperimentaldatafromarenewalparadigm(Bouton&Bolles,1979),inwhichratsweregiventraininginContextAandextinctioninContextBandwerethentestedineitherthetrainingcontext(A),extinctioncontext(B)oranovelcontext(C).Theconditionedresponsemeasuredattestwas,inthiscase,conditionedsuppression(butsimilarresultshavebeenob-tainedwithmanyotherpreparations;seeBouton,2004).Condi-tionedrespondingwasrenewedbothinthetrainingcontext(ABArenewal)andinthenovelcontext(ABCrenewal)butnotintheextinctioncontext.Figures2cand2dshowtheresultsofsimulatingourmodelwithconditioninginContextA(f⫽[reinforcement,tone,A]for20trials);extinctioninContextB(f⫽[noreinforcement,tone,B]for50trials);andtestingineitherA(f⫽[?,tone,B]),B(f⫽[?,tone,B]),orC(f⫽[?,tone,C]),demonstratingthatourmodelrepli-catestheABAandABCrenewaleffects.SimilarinspirittoRedishetal.’s(2007)model,ourmodelpredictsABArenewalasaconsequenceoftheanimal’sinferencethatdifferentlatentcausesareactiveduringconditioningandextinction.Whentheanimalisreturnedtotheconditioningcontextinthetestphase,itinfers(becauseofthepresenceofcontextualcues)thatthefirstlatentcauseisonceagainactive.Becausetrialswiththesamelatentcausehavesimilarproperties,theanimalpredictsthatreinforce-mentislikelytooccuronthetesttrialsandthereforeemitstheconditionedbehavior.Thus,theimportanceofcontextinourtheoryderivesfromitsusefulnessindisambiguatingthelatentcausesofobservations(seealsoBouton,1993).ABArenewalisobservedinourmodeltotheextentthatatesttrialmatches(initsobservationfeatures)trialsfromthecondition-ingphasemorethantrialsfromtheextinctionphase.ABCrenewal7Weusedalargenumberofparticlestoaccuratelyapproximatetheposterior.Forthisreason,otherinferencealgorithms,suchasGibbssam-pling,willproduceeffectivelythesamepredictions.10203040506000.20.40.60.81TrialConditioned responseFigure1.Acquisitionandextinction.Simulatedconditionedrespondingduringconditioningandextinctionindifferentcontexts.Thedottedlinedemarcatesthecontextchange.Thesolidlinerepresentsconditionedrespondingwhentheanimalcontinuestobereinforcedafterthecontextchange.Thedashedlinerepresentsconditionedrespondingwhentheanimalisnolongerreinforcedafterthecontextchange(extinction).Sim-ulatedconditionedrespondingisdirectlydeterminedbydegreeofexpec-tationofreinforcement.ABC0.40.50.60.70.8a: Experimental data           Suppression ratioTest contextABC00.51Conditioned responseTest contextb: Simulation                       12300.51ClusterPosterior probabilityc: Cluster distribution          AB12300.51ClusterPosterior probabilityd: Cluster distribution (test)  ACBFigure2.Renewal.Experimental(a)andsimulated(b)conditionedre-spondingtoastimulusduringatestphaseafterconditioningandextinction.Inallplots,experimentallyobservedconditionedresponsesareplottedusingtheiroriginalmeasurementunits.a.Bothreturningthesubjecttotheconditioningcontextandplacingitinanovelcontextresultinrenewalofconditionedresponding.DatareplottedfromBoutonandBolles(1979).b.SimulatedconditionedrespondingduringtestintheconditioningContextA,extinctionContextBandanovelContextC.c.PosteriordistributionofclusterassignmentsafterconditioninginContextAandextinctioninContextB.Conditioningandextinctiontrialstendedtobeassignedtodifferentclusters,asevidencedbydifferentmodesoftheposteriorinthetwophases.d.PosteriordistributionofclusterassignmentsonthefirsttesttrialinContextsA,BandC.201CONTEXT,LEARNING,ANDEXTINCTIONmaybeobservedinatleastthreedifferentscenarios.IfCissubstantiallydifferentfromAorB,suchthatanewclusteriscreated,ABCrenewalwillbeobservedtotheextentthatthepriorexpectationofreinforcementinanewclusterisgreaterthanzero.IfCisnotdifferentenoughtowarrantanewcluster,ABCrenewalmaystillbeobservedifCisequallysimilartoAandB,sothatitgetsassignedinequalproportionstotheirassociatedclusters.Yetanotherpossibility8isthatwhentherearemanymoreAtrialsthanBtrials,theCobservationwillbeassignedtotheclusterassociatedwithAbecauseofEquation1(whichintheinferencemodelwilltendtoassignobservationstomorepopularclusters).Althoughthesimulationspresentedheremanifestthesecondscenario(inwhichtrialsinCareassociatedequallywiththetrainingclusterandtheextinctioncluster),wenotethatdifferentparameterizations(par-ticularlythevalueofalpha)orfeaturerepresentationsmayresultinthefirstscenario,inwhichanewclusterisinferred,whichwouldalsoleadtorenewal.Whentheanimalistestedinthesamecontextastheextinctionphase,norenewalisobserved(seeFigure2).Similarly,norenewalisobservedwhenallthreephasestakeplaceinthesamecontext(resultsnotshown).Theseresultsfollowfromthemodel’spredic-tionthatthesamelatentcauseisactiveduringextinctionandtestandhencepredictstheabsenceofreward.Furtherinsightintothemechanismsunderlyingrenewalinourmodelcanbegainedbyexaminingtheposteriordistributionofclusters,showninFigure2cfortheconditioningphaseandinFigure2dfortheextinctionphase.Aspredicted,ourmodeltendstoassigntheconditioningandextinctiontrialstodifferentclusters.WhenthetesttrialoccursinContextA,theobservationisassignedtotheconditioningcluster,whereaswhenitoccursinContextB,itisassignedtotheextinctioncluster.WhenthetesttrialoccursinanewContextC,inferenceregardingitslatentcauseisdividedbetweenthecondi-tioningandextinctionclusters(andtoalesserextentanewcluster).Thisisduetothefactthatasclustersaccruemoreobservations,theycometodominatetheposterior.LatentInhibitionOurmodelsimilarlyexplainsthecontextdependenceoflatentinhibitionintermsofthepartitionstructureoftheanimal’sexpe-rience.Wesimulatedlatentinhibitionwith15pre-exposuretrialsand15conditioningtrials(Figures3aand3b).Whentheanimalreceivespre-exposure(f⫽[noreinforcement,tone,A])andcon-ditioning(f⫽[reinforcement,tone,A])inthesamecontext,itismorelikelytoattributeacommonlatentcausetobothphases,andthusthepropertiesofbothpre-exposureandconditioningobser-vationsareaveragedtogetherinmakingpredictionsaboutrein-forcementintheconditioningphase,leadingtoalowerpredictionandsloweracquisition.Incontrast,whentheanimalreceivespre-exposure(f⫽[noreinforcement,tone,A])andconditioning(f⫽[reinforcement,tone,B])indifferentcontexts,itismorelikelytoassignobservationsfromeachphasetodifferentclus-ters—thatis,toinferthatdifferentlatentcauseswereactiveduringpre-exposureandconditioning.Inthiscase,thereinforcementstatisticslearnedfromtheconditioningtrialsaresegregatedfromthereinforcementstatisticsofthepre-exposuretrials,eliminatingtheretardingeffectofpre-exposureonlearning,ascanbeseeninFigures3aand3b.PathologiesoftheModelNumerousstudieshaveshownthatdamagetothehippocampusdisruptsthecontextdependenceoflearningandextinction(forareview,seeJi&Maren,2007).Animalswithpretrainingelectro-lyticlesionsofthedorsalhippocampusfailtoshowrenewalofconditionedresponding(Ji&Maren,2005).Likewise,animalswithhippocampallesionsexhibitintactlatentinhibitionevenwhenpre-exposureandconditioningoccurindifferentcontexts(Honey&Good,1993).Thesefindingsareparalleledbyasimilarlackofcontextdependenceinthebehaviorofthedevelopingrat:Beforetheageof⬃22days,ratsdonotshowrenewalortheattenuationoflatentinhibitionbyconditioninginanewcontext(Yap&Richardson,2005,2007).Weproposeaunifiedexplanationforthesephenomenaintermsofapathologyinourmodel’scapacitytoinfernewlatentcauses.Ourtheoryalsosuggestsanexplanationforwhythecontextdependenceofrenewalandlatentinhibitionisonlyimpairedwhenboththeconditioningandextinctionphases(forrenewal)orpre-exposureandconditioningphases(forlatentinhibition)occurbeforematuration(Yap&Richardson,2005,2007).Hippocampallesions.Weproposethathippocampallesionsdisrupttheabilityoftheanimaltoinfernewclusters,restrictingitsinferencetoalready-establishedclusters.Weimplementedthisby8Wethankananonymousreviewerforpointingoutthispossibility.12345624681012SessionResponse ratea: Experimental data             SameDiff12345624681012SessionResponse ratec: Experimental data            Same HPCDiff HPC1234560.10.20.30.4Conditioned responseSessionb: Simulation                       1234560.10.20.30.4Conditioned responseSessiond: Simulation                       Figure3.Latentinhibition.Experimental(a,c)andsimulated(b,d)acquisitioncurvesofconditionedrespondingtoastimuluspairedwithreinforcementasafunctionofwhetherunpairedstimuluspre-exposureoccurredinthesameorinadifferent(Diff)context.a.Pre-exposureinthesamecontextasconditioningretardstheacquisitionofconditionedre-sponding.Thisretardingeffectisattenuatedbypre-exposingthestimulusinadifferentcontext.b.Simulatedrespondingusingthemixturemodel.c.Subjectsgivenhippocampallesionsbeforeconditioning(HPC)showre-tardedacquisitionregardlessofwhetherpre-exposureisperformedinthesameorinadifferentcontext.DatareplottedfromHoneyandGood(1993).d.Simulatedrespondingusingthemixturemodelafterhippocam-pallesions,whichweresimulatedbyrestrictingthemodel’sabilitytoinfernewclusters.NotethatSameHPCisindistinguishablefromDiffHPC.202GERSHMAN,BLEI,ANDNIVsettingalphatozeroatthetimeofthelesion.Inlatentinhibition,whenthisrestrictionwasappliedduringpre-exposure,thepre-exposureandconditioningobservationswereassignedtothesamecluster,regardlessofthecontextsthatwereinplaceforthetwophases(inotherwords,ourmodeldegeneratedintoasingledis-tributionoverobservationfeatures).Thepredictionofreinforce-mentduringconditioningwasthenbasedonanaverageofbothpre-exposureandpriorconditioningtrials,leadingtoslowerac-quisition(seeFigures3cand3d).AlthoughearlystudiesreportedintactABArenewalwithpre-trainingelectrolyticlesionsofthefimbria/fornix(Wilson,Brooks,&Bouton,1995)orneurotoxiclesionsoftheentirehippocampus(Frohardt,Guarraci,&Bouton,2000),JiandMaren(2005)foundthatratswithpretrainingelectrolyticlesionsofthedorsalhip-pocampusshowedimpairedrenewalintheABAparadigm.9Figure4showstheseexperimentaldataandsimulateddatafromourmodel,demonstratingimpairedrenewalinourmodelafterrestrictingthecapacitytoinfernewclusterspriortotraining.Developmentaltrajectories.YapandRichardson(2005)havereportedthatinyoungrats,latentinhibitioniscontextinde-pendent,withbehaviorbeingstrikinglysimilartothatexhibitedbyratswithpretraininghippocampallesions.AsshowninFigure5a,whenratswerepre-exposed,conditionedandtestedat18dayspostnatal(PN18),theyshowedslowacquisitionregardlessofwhetherpre-exposureandconditioningoccurredinthesameordifferentcontexts.Inasecondexperiment,YapandRichardson(2005)foundthatiftestingwasconductedatPN25,thecontextindependenceoflatentinhibitionwasstillobserved.Inathirdexperiment,pre-exposureatPN18withconditioningatPN24andtestingatPN25resultedinintactcontext-dependentlatentinhibi-tion.Wesimulatedthesedifferentconditionsbyonceagainre-strictingourmodel’scapacitytoinfernewclusters(setting␣⫽0)duringthephaseswhentheanimalisyoungerthanPN22,andinstatingthiscapacity(setting␣⫽.1)whentheanimalreachesPN22.Figure5bshowsthatwiththismanipulationthemixturemodeldemonstratesapatternofcontextdependencesimilartothatobservedexperimentally.Theexplanationofthesesimulatedre-sultsisthesameasfortheeffectsofpretraininghippocampallesionsdescribedabove.RenewalhasalsobeensystematicallyinvestigatedbyYapandRichardson(2007)inthedevelopingrat.Figure6ashowscondi-tionedrespondinginContextsAandBafterconditioninginAandextinctioninBatdifferentages,replottedfromYapandRichard-son(2007).Themainresultisthatifbothconditioningandextinctionareperformedbeforematurity,noABArenewalisobserved,butifextinctionisperformedaftermaturityisreached,ABArenewalisintact.Figure6bshowssimulationsoftheseexperiments,demonstratingthesamepatternofresults.Onlywhenourmodel’scapacityforinferringnewclustersisrestrictedduringbothconditioningandextinctionwilltheybeassignedtothesamecluster.Ifextinctionoccursaftermaturation,theanimalcanassignextinctionobservationstoanewcluster,preventinginterferencebetweenconditioningandextinctiontrialsandthusenablingtheconditionedresponseintheconditioningcontexttoberenewedafterextinction.DiscussionStartingfromanormativestatisticalframework,weformalizedamixturemodelofanimallearninginwhichcontext-dependentbehavioristheresultofinferenceoverthelatentcausalstructureoftheenvironment.Weshowedthatthismodelcanexplainseveralbehavioralphenomenainlatentinhibitionandrenewalparadigms.Wealsoshowedthatrestrictingthemodel’scapacitytoinfernewclusterscanreproduceeffectsofhippocampallesionsanddevel-opmentalchangesintheseparadigms.9AsdiscussedbyJiandMaren(2005),electrolyticlesionsbothdamageneuronsinthedorsalhippocampusanddisruptfibersofpassagetosub-corticalstructures,whereasfornixlesionsonlydisruptfibersofpassageandneurotoxiclesionsdamageneurons,leavingfibersofpassageintact.Thus,itisconceivablethattheseproceduresfailedtofindimpairmentinrenewalbecauseitisnecessarytodamagebothfibersofpassageandhippocampalneurons.CONHPC00.20.40.60.8Suppression ratioa: Experimental data             ABCONHPC00.20.40.60.81Conditioned responseb: Simulation                       Figure4.EffectofhippocampallesionsonABArenewal.a.Experimen-talconditionedrespondingtoacueduringthetestphaseincontrolrats(CON)andthosethatreceivedpretrainingelectrolyticlesionsofthedorsalhippocampus(HPC).DatareplottedfromJiandMaren(2005).b.Simu-latedconditionedrespondingfollowingrestrictionofthemodel’scapacitytoinfernewclusterspriortotraining.18/18/1818/19/2518/24/2525/25/250.30.40.50.60.7Freezinga: Experimental data                                                                         SameDiff18/18/1818/19/2518/24/2525/25/250.20.30.40.5Conditioned responseb: Simulation                                                                                   Figure5.Developmentoflatentinhibition.Experimental(a)andsimu-lated(b)conditionedrespondingduringthetestphasefollowingpre-exposureandconditioninginthesameorinadifferent(Diff)context.Labelsonthex-axisrefertotheage(indays)atwhicheachphase(pre-exposure/conditioning/test)wasconducted.a.Freezingtothestimulusinthetestcontext.DatareplottedfromYapandRichardson(2005).b.Simulatedconditionedresponding.203CONTEXT,LEARNING,ANDEXTINCTIONOurmodelextendsandlendsstatisticalclarificationtotheinsightsdevelopedintheworkofRedishetal.(2007).Inaddition,wehaveaddressedsomespecificshortcomingsoftheirmodel.Itisimportanttonotethatthedependenceofnewstateinferenceonnegativepredictionerrors,whichpreventedRedishetal.’smodelfromexplainingthecontextspecificityoflatentinhibition,iswhollyabsentfromouraccount.Wehavealsomadeaspecificproposalregardingtheroleofthehippocampusinthesetasks,whichwasalludedtobyRedishetal.andisdiscussedinmoredetailtofollow.Butfirst,tounderstandthetheoreticalmotivationforamixturemodelanditsrelationshipwithothermodels,itisusefultoconsiderataxonomyofmodelsorganizedalongfourdimensions:computationalproblem,causalstructure,capacity,andinferencealgorithm.Inthenextfoursectionswedetaileachofthesedimensions;wethendiscusstheroleofhippocampusinlearningandconcludewithadiscussionoflimitationsandpossibleextensionsofourmodel.ComputationalProblem:GenerativeVersusDiscriminativeMarr(1980)arguedthattounderstandaninformation-processingsystem,onemustunderstandthecomputationalprob-lemitwasdesignedtosolve.Mostmodelsofanimallearningarediscriminative,implicitlyorexplicitlyassumingthatanimalsaimtopredicttheprobabilityofreinforcementgiventherestoftheirexperience(Pearce&Bouton,2001).Generativemodels,incon-trast,assumethatanimalsaimtolearnthejointdistributionoverallvariablesintheirinternalmodeloftheenvironment,includingbutnotlimitedtoreinforcement.Courville(2006)developedagener-ativemodelofanimallearningusingsigmoidbeliefnetsthatbearsmanysimilaritiestoourmixturemodel;wediscusssimilaritiesanddifferencesinthefollowingsections.Onemightreasonablyask“Whyfavoragenerativeaccountoveradiscriminativeone?”Oneproblemwithdiscriminativemodelsisthattheyhavenomeansofexplainingbehavioralphenomenainwhichtheanimalappearstolearninformationabouttheenviron-mentindependentofreinforcement.Aclassicexampleofthisissensorypreconditioning(Brogden,1939):Afterinitiallypairingtwoneutralstimuli,AandB,intheabsenceofreinforcement,Aispairedwithreinforcement;subsequently,theanimalexhibitssig-nificantrespondingtoB,suggestingthatanassociationbetweenAandBwaslearnedinthefirstphase,despitetheabsenceofreinforcement.Courville(2006)reviewednumerousotherphe-nomenathatsupportagenerativeaccount.LarrauriandSchmajuk(2008)proposedadiscriminativecon-nectionistmodeltoaccountforrenewalandseveralothercontext-dependentbehaviors.Theyarguedthatacombinationofatten-tional,associativeandconfiguralmechanismscancollectivelyaccountforthesedata.AspointedoutbyCourvilleandcolleagues(Courvilleetal.,2002;Courville,2006),manyoftheideasbehindconfiguralmechanismscanbecapturedbylatentvariablemodels.Whereasinconnectionistmodels,observationfeaturesarecoupledviaconvergentprojectionsto“configural”units,latentvariablemodelscapturethiscouplinggenerativelybyhavingobservationfeaturesshareacommoncause.Generalizationbetweenfeaturesisthenaccomplishedbylearningabouttheirlatentcauses.Inmod-elingtheroleofcontextinlearning,wehaveadoptedthissameinsight,showinghowcontextcanaffectlearningaboutreinforcersbymeansofacommonlatentcause.Abasicpropertyofconnectionistmodels,suchasthatofLar-rauriandSchmajuk(2008),isthattheyeffectivelytransposethestructurelearningproblemintoaparameterlearningproblembyencodingallpossiblestructureswithinthenetwork,allowingthecausalstructuretobeuncoveredthroughexperience-dependentadjustmentoftheconnectionweights(seealsoGluck&Myers,1993).Oneproblemwiththisapproachisthatitignorespriorbeliefsaboutthestructureoftheenvironment,whichservetoconstrainthekindsofstructuresthatcanbelearned(Kemp&Tenenbaum,2008;Courvilleetal.,2004).Ourgenerativemodelisamiddlegroundbetweenconnectionistmodelsthatassumenopriorstructuralbeliefsandmodelsthatusehand-codedfeaturesforparticulartasks(e.g.,Brandon,Vogel,&Wagner,2000).Inourmodel,whereexactlyinthismiddlegroundtheanimal’sstructuralbeliefslieisdeterminedbothbyitsexperienceanditspriorbeliefs.Theanimalmayinitiallyexpectonlyasmallnumberoflatentcauses(specifiedbysettingalphaclosetozero),butitsgenerativemodelisflexibleenoughtoallowrevisionofthisbelieftoaccom-modatemorecausesinlightofnewobservations.CausalStructure:ProductsVersusMixturesRecallthatinamixturemodel,observationsareassumedtobecausedbyagenerativemodelinwhichasinglediscretecauseissampledandthenanobservationissampledconditionalonthiscause.Analternativegenerativemodel,knownasaproductmodel,assumesthatobservationsaregeneratedbyalinearcombinationofseverallatentcauses,anynumberofwhichcanbepresentatthesametime.AsfarbackastheinfluentialmodelofRescorlaandWagner(1972),thepredominantmathematicalrepresentationinmodelsofanimallearningistheproduct.Indiscriminativemodels,forex-16/16/1616/17/2316/22/2300.10.20.30.4Freezinga: Experimental data                                                                         AB16/16/1616/17/2316/22/2300.51b: Simulation                                                                                   Conditioned responseFigure6.Developmentofrenewal.Experimental(a)andsimulated(b)conditionedrespondingduringthetestphaseinContextAorBfollowingconditioninginAandextinctioninB.Labelsonthex-axisrefertotherat’sage(indays)ateachphase(conditioning/extinction/test).a.Freezingtothecueinthetestcontext.DatareplottedfromYapandRichardson(2007).b.Simulatedconditionedresponding.204GERSHMAN,BLEI,ANDNIVample,thepredictionofreinforcementiscomputedbytakingalinearcombinationoffeaturevariables.ThisistruenotjustfortheRescorla-Wagnermodelbutformostmodelsinthestatisticalandconnectionisttraditionsaswell(Dayan&Long,1998;Schmajuk,Buhusi,&Gray,1996).InCourville’s(2006)generativemodel,theprobabilityofeachobservedvariableisalinearcombinationoflatentvariablespassedthroughalogisticsigmoidfunction.AnexceptionisthecompetitivemixtureofexpertsmodelofDayanandcolleagues(Dayan&Long,1998;Dayanetal.,2000),inwhichreinforcementisassumedtobegeneratedbyasingleobservablecause.Inthatmodel,theprobabilityofreinforcementisthesumofconditionedprobabilitiesofreinforcementgiveneachcause,weightedbytheprobabilityofobservingthatcause(its“mixingprobability”).Onemotivationforusingmixturesratherthanproducts,articulatedbyDayanetal.(2000),isthatinferencewithinamixtureprovidesanelegantmodelofcompetitiveatten-tionalallocationinanimallearning(wherebystimulusfeaturesareattendedinproportiontotheposteriorprobabilitythattheycausedreinforcement)andmaybenecessarytoexplaineffectslikedown-wardunblocking(Holland,1988).Ourmodel,althoughconsistentwithacompetitiveattentionalaccount,putsmixturestoadifferentusebyassumingthatreinforcementisgeneratedbyalatentcause.Therearemanysituationsinwhichthisassumptionisreasonable.Indeed,ifonecontemplatesthedesignsofmostconditioningexperiments(includingthosemodeledbyDayan&Long,1998),thestimuluspatternspresentedtotheanimalsaregeneratedbydiscrete,latentphasesoftheexperiment(e.g.,conditioning,ex-tinction);theanimalneverdirectlyobservesthesephases,butinferringthemiskeytopredictingreinforcement.FuhsandTouretzky(2007)proposedalatentcausetheorytoexplainhippocampalplacecellremappingthatissimilarinspirittoourmixturemodel.Theydefinedcontextasastatisticallystationarydistributionofobservationsandcontextlearningasthetaskofclusteringobservationstogetherintogroupswithlocalstatisticsthatarestationaryintime.Incontrastwithourstaticmixturemodel,theyusedadynamicmixturemodelandformalizedcontextlearningintermsofBayesianmodelselection,showingthatthiscanpredictwhenplacecellswillremapinresponsetoenvironmentalchange.AswithCourville’s(2006)model,theyselectedthebestfinite-capacitymixturemodel(seethenextsec-tion),whereasweemployaninfinite-capacitymixturemodelthatautomaticallyselectsthenumberofclustersonthebasisofitsobservations.Becausetheyappliedthismodeltoneuraldataandbehavioralparadigmssomewhatremovedfromourfocusinthisarticle,adirectcomparisonbetweenthetwomodelsisdifficult.Nonetheless,theideathathippocampalplacecellsareimportantforinferringlatentcausesisconsonantwiththegeneralviewofthehippocampussetforthinthisarticle(seeTheHippocampusandContextsection).Capacity:FiniteVersusInfiniteAspecialproblemvexesmodelswithlatentvariablesinwhichthenumberoflatentvariablesisunknown.Onecanalmostalwaysincreasethelikelihoodofthedataunderamodelbyincreasingthenumberofparametersinthemodel.Thenumberofparameters,ormoregenerallythecomplexityofthemodel,issometimesreferredtoasits“capacity.”Increasingcapacitycanleadtoaphenomenonknownas“overfitting,”whereinextraparametersarejustcaptur-ingnoise,leadingtopoorpredictivepower.Aprincipledstatisticalapproachtothisproblemistorepresentuncertaintyoverthemodel’sstructureexplicitlyandinferboththestructureandthevaluesofthelatentvariables.ThiswastheapproachadoptedbyCourville(2006),whousedaMarkovchainMonteCarloalgo-rithmtoselectthebestfinitecapacitymodel(amodelwithafixednumberofparameters)giventhedata.Analternativetoselectingbetweendifferentfinitecapacitymodelsistoallowthenumberofparameterstogrowwiththedata(i.e.,infinitecapacity).Thisis,infact,thespiritofRedishetal.’s(2007)model,inwhichheuristicmodificationstoareinforcementlearningalgorithmallowittoincreaseitscapacity(byexpandingthestatespace)duringlearning.Tocontroloverfitting,onecanplaceapriordistributionoverparametersthatexpressesaprefer-enceforsimplermodels.Thisapproach,adoptedinourmodel,satisfiescertainintuitionsaboutananimal’srepresentationofitsenvironment.Itseemsunreasonabletoassumethattheanimalknowsinadvancehowmanyhiddencausestowhichitmightbeexposed.Amorereasonableassumptionisthatitinfersthatanewhiddencauseisactivewhenthestatisticsofitsobservations(e.g.,lights,tones,odors)change,whichispreciselytheinferencepro-cedureimputedtotheanimalbythemixturemodel.SimilarargumentshavealsobeenmadebySanbornetal.(2006)intheirmixturemodelofhumancategorization.AnotheraspectthatourmodelshareswithRedishetal.’s(2007)modelisthatclusterassignment(stateclassification)andclustercreation(expansionofthestatespace)arebothdeterminedbythesimilaritybetweenthecurrentobservationandtheexistingstates.Acurrentobservationisassignedtoanexistingclustertotheextentthatitissimilartotheotherobservationsassignedtothatcluster;ifnoclusterissufficientlysimilar,anewclusteriscreatedforthatobservation.Inessence,theparticle-filteralgorithmat-temptstocreateclusterswithmaximalwithin-clustersimilarityandminimalbetween-clustersimilarity.Thestate-classificationmechanisminRedishetal.’smodelalsoattemptstoachievethisgoal,butitlacksadirectstatisticalinterpretationintermsofawell-definedinferenceprocedure.Redishetal.’smodeldoesnotrepresentuncertaintyaboutthestateclassifications,whereastheparticlefiltermaintainsanap-proximationofthefullposteriordistributionoverclusters.10Thisispotentiallyimportantincaseswherepreviousclusteringneedstobereevaluatedinlightoflaterinformation.Forexample,imaginecominghomeandseeingthehouseflooded.Youcouldclassifythisaseitherresultingfromthe(latent)cause“ithasrained”ortheapriorimuchlessprobable(latent)cause“therewasafireandfiretruckssprayedmyhouse.”Laterhearingonthenewsthatithadbeenanexceptionallyhotanddryday,youmightreevaluatethefirehypothesis.Suchretrospectiverevaluationphenomena(inwhichapreviouslydisfavoredinterpretationbecomesfavoredinlightofnewinformation)supporttheideathathumansandanimalsrepresentuncertaintyaboutpastinterpretations,ratherthanmakinghardassignments(Daw&Courville,2008).10Whenthenumberofparticlesissmall,particlefilteringwillbehavesimilarlytohardassignment(seeDaw&Courville,2008;Sanbornetal.,2006).205CONTEXT,LEARNING,ANDEXTINCTIONInferenceAlgorithm:BatchVersusIncrementalOneofthereasonsforappealingtostatisticalmodelsoflearningisthattheyprovideaformaldescriptionofthecomputationalproblemthatthelearningsystemisdesignedtosolve.However,acompleteanalysisofaninformationprocessingsystemrequiresdescriptionsattwootherlevels(Marr,1980).Thealgorithmiclevelspecifiestheoperationsandrepresentationsrequiredtosolvethecomputationalproblem.Inastatisticalmodel,therepresenta-tionsareprobabilitydistributionsandtheoperationsareusuallysomeformoftheproductandsumrulesfromprobabilitytheory.Theimplementationallevelspecifieshowthesecomputationsarephysicallyrealized(e.g.,inthebrain).Statisticalmodelsofanimallearningvaryintheirplausibilityatthesetwolevelsofanalysis(wediscusstheimplementationallevelinthenextsection).Atthealgorithmiclevel,themaindesideratumforplausibilityisthattheinferenceprocedurebeabletoincorporatenewdataincrementally(Anderson,1991;Sanbornetal.,2006).Reinforce-mentlearningandconnectionistupdatessatisfythisdesideratum.ThebatchMarkovchainMonteCarloalgorithmproposedbyCourvilleetal.(2002,2004),whichmustbererunonallpastobservationsaftereachtrial,suffersinthisregard,althoughlaterworkattemptedtoremedythisdrawback(Courville,2006;Daw&Courville,2008).Weusedtheparticlefiltertoperforminferenceinourmodelbecauseitprovidesacognitivelyplausibleincremen-talalgorithmforanimallearning.However,itwouldbeprematuretocommittotheparticlefilterasanalgorithmic-leveldescriptionoftheconditioningdatathatwemodel,becausegiventhelargenumberofparticlesweuse,thisalgorithmwillmakebehavioralpredictionsessentiallyidenticaltothosemadebyanyotheralgo-rithmthatadequatelyapproximatestheposterior(e.g.,MarkovchainMonteCarlosampling).Withfewersamples,theparticlefilterapproximatestheposterioronlycrudely.Ithasbeenarguedthatthismightbethereasonforcertainkindsofresourcelimita-tionsonbehavior(Brown&Steyvers,2009;Daw&Courville,2008;Sanbornetal.,2006);itisanopenquestionwhethersuchresourcelimitationsareevidentintherenewalorlatentinhibitiondata.TheHippocampusandContextThehippocampushaslongbeenimplicatedincontextlearning,buttheorieshavedifferedintheirformalcharacterizationofthisrole(Fuhs&Touretzky,2007;Hasselmo&Eichenbaum,2005;Hirsh,1974;Howard,Fotedar,Datey,&Hasselmo,2005;Jarrard,1993;Nadel,1995;Nadel&MacDonald,1980;O’Keefe&Nadel,1978;Redish,1999;Rudy&O’Reilly,1999).Herewehaveproposedonepossibleroleforthehippocampusininferringlatentcauses.Weshowedthatrestrictingourmodel’sabilitytoinfernewclustersresultsinbehaviorqualitativelysimilartothatobservedinratswithhippocampallesions(seealsoLove&Gureckis,2007,forasimilarinterpretationofhumandata).Webelievethatthehippocampusissuitedforthisrole,withitsabilitytoextractsparsecodesfromsensoryinputs,whichcouldsupportthelearningofdiscretelatentcauses(Doboli,Minai,&Best,2000).Inparticular,sparseprojectionsfromthedentategyrustoCA3arethoughttobecrucialforpatternseparation(Marr,1971),anoperationthatcouldservetoseparatedifferentobservations(inputs)intodistinctacti-vationpatternsinCA3.Whenapartialpattern(e.g.,astimulusandcontext)ispresented,themissingpartofthepattern(e.g.,rein-forcer)isactivatedbymeansofrecurrentconnectionsinCA3,whichmayfunctionasanattractornetwork(McNaughton&Morris,1987).Theseattractorsmaythuscorrespondtoinferredclusters,withnewattractorsbeingformedwhentheinputstatisticschangedramatically.Ourmodelmayalsoshednewlightonalong-standingquestionaboutthehippocampusandmemoryingeneral(Marr,1971;McNaughton&Morris,1987):Whenanewobservationismade,underwhatcircumstancesisanewtraceencodedoranoldtraceretrieved?Ourmodelframesthisasachoicebetweenassigninganobservationtoanexistingclusterortoanewcluster.O’ReillyandMcClelland(1994)extensivelyanalyzedamodelofthehippocam-pusandarguedthatitsanatomicalandphysiologicalpropertiesmightservetominimizethetrade-offbetweenpatternseparation(encoding)andpatterncompletion(retrieval).Ourmodeloffersanormativemotivationforhowthistrade-offshouldbebalancedonthebasisoftheanimal’sobservationstatisticsandpriorbeliefs,andfutureworkshouldbedirectedatconnectingittotheunder-lyingneurophysiologicalmechanismsidentifiedbyO’ReillyandMcClelland(1994),aswellastherolesofthetaoscillationsandcholinergicinputdiscussedbyHasselmo,Bodelo´n,andWyble(2002).Wewouldliketoemphasizethattheostensiblynonstatisticalfunctionsofthehippocampus,suchasrapidconjunctiveencoding(McClelland,McNaughton,&O’Reilly,1995),arenotincompat-iblewithastatisticalaccount.Mostdistinctionsofthissorthaveidentifiedstatisticallearningwithextractionofthecovariationstructureofsensoryinputsbyneocortex(cf.Gluck&Myers,1993).Inneuralnetworkmodels,thisisimplementedthroughgradualsynapticweightchange.Ourmodelattemptstobroadenthisviewofstatisticallearningtoincludethelearningofdiscretepartitionstructure,afunctionthatwearguedfitswithexistingcomputationalmodelsofthehippocampus.Thefactthatinfantratsshowalackofcontextdependencesimilartoratswithhippocampaldamage(Yap&Richardson,2005,2007)suggeststhatthesamecausalinferencemechanismmayunderliebothphenomena(Martin&Berthoz,2002;Rudy,1993),butmoreresearchonthebehavioralconsequencesofhip-pocampalmaturationisneededtotestthisidea.Otherbrainstruc-tures,notablytheprefrontalcortex,alsoundergomaturationdur-ingthisperiod,anditisunclearwhatspecificcontributionstheymaymaketocontext-dependentlearningandextinction(Quirk,Garcia,&Gonza`lez-Lima,2005;Rhodes&Killcross,2007).WehavealsosaidlittleaboutoneofthemainmotivationsforRedishetal.’s(2007)model,namelytheroleofthedopaminesysteminlearning.Evidencehasbeguntoaccumulatesuggestingthatthehippocampalanddopaminesystemsareintricatelyinter-twined(Lisman&Grace,2005);however,thebehavioralsignifi-canceofthisrelationshipispoorlyunderstood(butseeFoster,Morris,&Dayan,2000;Johnson,vanderMeer,&Redish,2007).Finally,itisimportanttonotethatwedonotviewtheroleofthehippocampusincausalinferencesuggestedheretobeanall-encompassingfunctionaldescriptionofthehippocampus.Thehippocampusmayperformseveralfunctionsorsomemoregeneralfunctionthatincludescausalinferenceasasubcomponent.Fur-thermore,inferencemayrelyontheinteractionbetweenthehip-pocampusandotherregionsinthemedialtemporallobeandelsewhere(seeCorbit&Balleine,2000).206GERSHMAN,BLEI,ANDNIVLimitationsandExtensionsIntheirarticle,Redishetal.(2007)alsomodeledthepartialreinforcementextinctioneffect,theobservationthatextinctionisslowerwhenstimuliareonlyintermittentlypairedwithreinforcementduringtraining(Capaldi,1957,1958).Ourmodel,withoutfurtherassumptions,cannotdemonstratethiseffect,whichdependscruciallyonusingreinforcementrateasacontextualcue.Ourmodelassumesthatreinforcementsacrosstrialsareconditionallyindependentgivetheirlatentcauses,andthusithasnorepresentationofreinforcementrate.TheessentialexplanationgivenbyRedishetal.andothers(e.g.,Courville,2006)isthatthetrainingandextinctioncontextsarehardertodiscriminateinthepartialreinforcementconditionbecauseofsmallerdifferencesinreinforcementrate,andthusextinctiontrialsarelesslikelytobeassignedtoanewcluster.Redishetal.wereabletoshowthiseffectprimarilybecausetheyincludedthetimesincelastreinforcement,whichisinverselycorrelatedwithreinforcementrate,intheirprototyperepresentation.Wehavefoundinsimulations(notshownhere)thataugmentingtheobservationvectorwithanadditionalcontextualfeaturethatdiffersbetweentrainingandextinction(whichcouldbeinter-pretedasareinforcementratecue)issufficienttoproducethepartialreinforcementextinctioneffect.However,analternativeapproachtomodelingthisphenomenonistoincorporateanexplicitmodelofdynamicsandchangeovertime.Otherex-tinctionphenomenaalsodependonaricherrepresentationoftimethanwehaveemployedhere.Forexample,inspontaneousrecovery,simplywaiting48hrafterextinctionisenoughtoproducerenewedrespondingtothecue.Weleavedevelopmentofatemporallysophisticatedmixturemodeltofuturework(see,e.g.,Ren,Dunson,&Carin,2008).Finally,wewouldliketonotethatalthoughtheformalismemployedhereappearstobeasubstantialdeparturefromthetypeofreinforcementlearningmodelusedbyRedishetal.(2007),thedifferenceisnotsogreatatitseems.Notethatlearningaboutreinforcementinourmodelessentiallyrequiresthattheanimalmaintainandupdateasetofsufficientstatisticsaboutitsbeliefs—specifically,theaveragereinforcementineachclusterforeachfeaturevalue.Wesuspectthatsuchsufficientstatisticsmightbelearnedbyamechanismsimilartotemporaldifferencelearningand,hence,maysimilarlyrelyonthedopaminesystem(seeDaw,Courville,&Touretzky,2006,forrelatedideas).However,thepotentiallyrichconnectionsbetweentheseformalismsremaintobestudiedmorethoroughly.ConclusionsWehavearguedthatawealthofbehavioraldataareconsis-tentwithanaccountofanimallearninginwhichtheanimalinfersthelatentcausesofitsobservations.DrawingoninsightsfromRedishetal.(2007),weformalizedthisideaasamixturemodelandshowedhowaparticle-filteralgorithmcanbeusedtoperforminference.Simulationsshowthatthisframeworkcanreproducepatternsofcontext-dependentbehaviorinlatentin-hibitionandrenewalparadigms.Wealsoshowedthatrestrict-ingthemodel’sabilitytoinfernewclusterscanreproducepatternsofhippocampaldamageanddevelopmentalchange.Ourmodelplacescontext-dependentlearningphenomenainanormativestatisticalframework,whichweseeasprovidingacomputational-levelanalysisofthesameproblemsaddressedbyRedishetal.(2007).ReferencesAldous,D.(1985).Exchangeabilityandrelatedtopics.InE´coled’E´te´deprobabilite´sdeSaint-Flourxiii(pp.1–198).Berlin,Germany:Springer.Anderson,J.(1991).Theadaptivenatureofhumancategorization.Psy-chologicalReview,98,409–429.Blaisdell,A.P.,Sawa,K.,Leising,K.J.,&Waldmann,M.R.(2006,February17).Causalreasoninginrats.Science,311,1020–1022.Bouton,M.E.(1993).Context,time,andmemoryretrievalintheinter-ferenceparadigmsofPavlovianlearning.PsychologicalBulletin,114,80–99.Bouton,M.E.(2004).Contextandbehavioralprocessesinextinction.LearningandMemory,11,485–494.Bouton,M.E.,&Bolles,R.C.(1979).Contextualcontroloftheextinctionofconditionedfear.LearningandMotivation,10,445–466.Bouton,M.E.,&King,D.A.(1983).Contextualcontroloftheextinctionofconditionedfear:Testsfortheassociativevalueofthecontext.JournalofExperimentalPsychology:AnimalBehavioralProcesses,9,248–265.Brandon,S.E.,Vogel,E.H.,&Wagner,A.R.(2000).AcomponentialviewofconfiguralcuesingeneralizationanddiscriminationinPavlov-ianconditioning.BehavioralBrainResearch,110,67–72.Brogden,W.(1939).Sensorypreconditioning.JournalofExperimentalPsychology,25,323–332.Brown,S.,&Steyvers,M.(2009).Detectingandpredictingchanges.CognitivePsychology,58,49–67.Capaldi,E.(1957).Theeffectofdifferentamountsofalternatingpartialreinforcementonresistancetoextinction.AmericanJournalofPsychol-ogy,70,451–452.Capaldi,E.(1958).Theeffectofdifferentamountsoftrainingontheresistancetoextinctionofdifferentpatternsofpartiallyreinforcedre-sponses.JournalofComparativeandPhysiologicalPsychology,51,367–371.Corbit,L.H.,&Balleine,B.W.(2000).Theroleofthehippocampusininstrumentalconditioning.JournalofNeuroscience,20,4233–4239.Courville,A.C.(2006).Alatentcausetheoryofclassicalconditioning.Unpublisheddoctoraldissertation,Pittsburgh,PA.Courville,A.C.,Daw,N.,Gordon,G.J.,&Touretzky,D.S.(2004).Modeluncertaintyinclassicalconditioning.InS.Thrun,L.Saul,&B.Scho¨lkopf(Eds.),Advancesinneuralinformationprocessingsystems(pp.977–984).Cambridge,MA:MITPress.Courville,A.C.,Daw,N.D.,&Touretzky,D.S.(2002).Similarityanddiscriminationinclassicalconditioning:Alatentvariableaccount.InL.K.Saul,Y.Weiss,&L.Bottou(Eds.),Advancesinneuralinforma-tionprocessingsystems(pp.313–320).Cambridge,MA:MITPress.Daw,N.,&Courville,A.(2008).Theratasparticlefilter.InJ.Platt,D.Koller,Y.Singer,&S.Roweis(Eds.),Advancesinneuralinformationprocessingsystems(pp.369–376).Cambridge,MA:MITPress.Daw,N.,Courville,A.,&Touretzky,D.(2006).Representationandtimingintheoriesofthedopaminesystem.NeuralComputation,18,1637–1677.Dayan,P.,&Abbott,L.(2001).Theoreticalneuroscience:Computationalandmathematicalmodelingofneuralsystems.Cambridge,MA:MITPress.Dayan,P.,Kakade,S.,&Montague,P.R.(2000).Learningandselectiveattention.NatureNeuroscience,3(Suppl),1218–1223.Dayan,P.,&Long,T.(1998).Statisticalmodelsofconditioning.InS.Thrun,L.Saul,&B.Scho¨lkopf(Eds.),Advancesinneuralinformationprocessingsystems(pp.117–123).Cambridge,MA:MITPress.Dayan,P.,Niv,Y.,Seymour,B.,&Daw,N.(2006).Themisbehaviorofvalueandthedisciplineofthewill.NeuralNetworks,19,1153–1160.207CONTEXT,LEARNING,ANDEXTINCTIONDoboli,S.,Minai,A.A.,&Best,P.J.(2000).Latentattractors:Amodelforcontext-dependentplacerepresentationsinthehippocampus.NeuralComputation,12,1009–1043.Fearnhead,P.(2004).Particlefiltersformixturemodelswithanunknownnumberofcomponents.JournalofStatisticsandComputing,14,11–21.Foster,D.J.,Morris,R.G.,&Dayan,P.(2000).Amodelofhippocampallydependentnavigation,usingthetemporaldifferencelearningrule.Hip-pocampus,10,1–16.Frohardt,R.J.,Guarraci,F.A.,&Bouton,M.E.(2000).Theeffectsofneurotoxichippocampallesionsontwoeffectsofcontextafterfearextinction.BehavioralNeuroscience,114,227–240.Fuhs,M.C.,&Touretzky,D.S.(2007).Contextlearningintherodenthippocampus.NeuralComputation,19,3173–3215.Gelman,A.,Carlin,J.,Stern,H.,&Rubin,D.(2003).Bayesiandataanalysis.BocaRaton,FL:ChapmanandHall/CRC.Gluck,M.,&Myers,C.(1993).Hippocampalmediationofstimulusrepresentation:Acomputationaltheory.Hippocampus,3,491–516.Grossberg,S.(1976).Adaptivepatternclassificationanduniversalrecod-ing:I.Paralleldevelopmentandcodingofneuralfeaturedetectors.BiologicalCybernetics,23,121–134.Hall,G.,&Honey,R.C.(1989).Contextualeffectsinconditioning,latentinhibition,andhabituation:Associativeandretrievalfunctionsofcon-textualcues.JournalofExperimentalPsychology:AnimalBehaviorProcesses,15,232–241.Hasselmo,M.E.,Bodelo´n,C.,&Wyble,B.(2002).Aproposedfunctionforhippocampalthetarhythm:Separatephasesofencodingandretrievalenhancereversalofpriorlearning.NeuralComputation,14,793–817.Hasselmo,M.E.,&Eichenbaum,H.(2005).Hippocampalmechanismsforthecontext-dependentretrievalofepisodes.NeuralNetworks,18,1172–1190.Hertz,J.,Krogh,A.,&Palmer,R.(1991).Introductiontothetheoryofneuralcomputation.Boston,MA:AddisonWesley.Hirsh,R.(1974).Thehippocampusandcontextualretrievalofinformationfrommemory:Atheory.BehavioralBiology,12,421–444.Holland,P.C.(1988).Excitationandinhibitioninunblocking.JournalofExperimentalPsychology:AnimalBehavioralProcesses,14,261–279.Honey,R.C.,&Good,M.(1993).Selectivehippocampallesionsabolishthecontextualspecificityoflatentinhibitionandconditioning.Behav-ioralNeuroscience,107,23–33.Howard,M.W.,Fotedar,M.S.,Datey,A.V.,&Hasselmo,M.E.(2005).Thetemporalcontextmodelinspatialnavigationandrelationallearning:Towardacommonexplanationofmedialtemporallobefunctionacrossdomains.PsychologicalReview,112,75–116.Jarrard,L.(1993).Ontheroleofthehippocampusinlearningandmemoryintherat.BehavioralandNeuralBiology,60,9–26.Ji,J.,&Maren,S.(2005).Electrolyticlesionsofthedorsalhippocampusdisruptrenewalofconditionalfearafterextinction.LearningandMem-ory,12,270–276.Ji,J.,&Maren,S.(2007).Hippocampalinvolvementincontextualmod-ulationoffearextinction.Hippocampus,17,749–758.Johnson,A.,vanderMeer,M.A.,&Redish,A.D.(2007).Integratinghippocampusandstriatumindecision-making.CurrentOpinioninNeu-robiology,17,692–697.Kakade,S.,&Dayan,P.(2002).Acquisitionandextinctioninautoshaping.PsychologicalReview,109,2002.Kemp,C.,&Tenenbaum,J.B.(2008).Thediscoveryofstructuralform.ProceedingsoftheNationalAcademyofSciencesoftheUnitedStatesofAmerica,105,10687–10692.Larrauri,J.A.,&Schmajuk,N.A.(2008).Attentional,associative,andconfiguralmechanismsinextinction.PsychologicalReview,115,640–676.Lisman,J.E.,&Grace,A.A.(2005).Thehippocampal-VTAloop:Controllingtheentryofinformationintolong-termmemory.Neuron,46,703–713.Love,B.C.,&Gureckis,T.M.(2007).Modelsinsearchofabrain.Cognitive,Affective,&BehavioralNeuroscience,7,90–108.Marr,D.(1971).Simplememory:Atheoryforarchicortex.PhilosophicalTransactionsoftheRoyalSocietyLondonB,BiologicalSciences,262,23–81.Marr,D.(1980).Vision.SanFrancisco,CA:Freeman.Martin,P.D.,&Berthoz,A.(2002).Developmentofspatialfiringinthehippocampusofyoungrats.Hippocampus,12,465–480.McClelland,J.L.,McNaughton,B.L.,&O’Reilly,R.C.(1995).Whytherearecomplementarylearningsystemsinthehippocampusandneocortex:Insightsfromthesuccessesandfailuresofconnectionistmodelsoflearningandmemory.PsychologicalReview,102,419–457.McNaughton,B.L.,&Morris,R.G.M.(1987).Hippocampalsynapticenhancementandinformationstoragewithinadistributedmemorysys-tem.TrendsinNeurosciences,10,408–415.Nadel,L.(1995).Theroleofthehippocampusindeclarativememory:AcommentonZola-Morgan,Squire,andRamus(1994).Hippocampus,5,232–239.Nadel,L.,&MacDonald,L.(1980).Hippocampus:Cognitivemaporworkingmemory?BehavioralandNeuralBiology,29,405–409.O’Keefe,J.,&Nadel,L.(1978).Thehippocampusasacognitivemap.NewYork,NY:OxfordUniversityPress.O’Reilly,R.C.,&McClelland,J.L.(1994).Hippocampalconjunctiveencoding,storage,andrecall:Avoidingatrade-off.Hippocampus,4,661–682.Pearce,J.M.,&Bouton,M.E.(2001).Theoriesofassociativelearninginanimals.AnnualReviewofPsychology,52,111–139.Pitman,J.(2002).Combinatorialstochasticprocesses(NotesforSaintFlourSummerSchool;TechnicalReportno.621).DepartmentofSta-tistics,UniversityofCalifornia,Berkeley.Quirk,G.J.,Garcia,R.,&Gonza`lez-Lima,F.(2006).Prefrontalmecha-nismsinextinctionofconditionedfear.BiologicalPsychiatry,60,337–343.Redish,A.(1999).Beyondthecognitivemap:Fromplacecellstoepisodicmemory.Cambridge,MA:MITPress.Redish,A.,Jensen,S.,Johnson,A.,&Kurth-Nelson,A.(2007).Recon-cilingreinforcementlearningmodelswithbehavioralextinctionandrenewal:Implicationsforaddiction,relapse,andproblemgambling.PsychologicalReview,114,784–805.Ren,L.,Dunson,D.B.,&Carin,L.(2008).Thedynamichierarchicaldirichletprocess.InProceedingsofthe25thinternationalconferenceonmachinelearning(pp.824–831).Helsinki,Finland:ACM.Rescorla,R.A.,&Wagner,A.R.(1972).AtheoryofPavloviancondi-tioning:Variationsintheeffectivenessofreinforcementandnonrein-forcement.InA.Black&W.Prokasy(Eds.),Classicalconditioningii:Currentresearchandtheory(pp.64–99).NewYork:Appleton-Century-Crofts.Rhodes,S.E.,&Killcross,A.S.(2007).Lesionsofratinfra-limbiccortexenhancerenewalofextinguishedappetitivePavlovianresponding.Eu-ropeanJournalofNeuroscience,25,2498–2503.Rudy,J.W.(1993).Contextualconditioningandauditorycueconditioningdissociateduringdevelopment.BehavioralNeuroscience,107,887–891.Rudy,J.W.,&O’Reilly,R.C.(1999).Contextualfearconditioning,conjunctiverepresentations,patterncompletion,andthehippocampus.BehavioralNeuroscience,113,867–880.Sanborn,A.N.,Griffiths,T.L.,&Navarro,D.J.(2006).Amorerationalmodelofcategorization.InR.Sun&N.Miyake(Eds.),Proceedingsofthe28thAnnualConferenceoftheCognitiveScienceSociety(pp.726–731).Mahwah,NJ:Erlbaum.Schmajuk,N.A.,Buhusi,C.,&Gray,J.A.(1996).Anattentional-configuralmodelofclassicalconditioning.JournalofMathematicalPsychology,40,358.Schultz,W.,Dayan,P.,&Montague,P.R.(1997,March14).Aneuralsubstrateofpredictionandreward.Science,275,1593–1599.208GERSHMAN,BLEI,ANDNIVSutton,R.S.,&Barto,A.G.(1998).Reinforcementlearning:Anintro-duction.Cambridge,MA:MITPress.Wilson,A.,Brooks,D.C.,&Bouton,M.E.(1995).Theroleoftherathippocampalsysteminseveraleffectsofcontextinextinction.Behav-ioralNeuroscience,109,828–836.Yap,C.S.,&Richardson,R.(2005).Latentinhibitioninthedevelopingrat:Anexaminationofcontext-specificeffects.DevelopmentalPsycho-biology,47,55–65.Yap,C.S.,&Richardson,R.(2007).Extinctioninthedevelopingrat:Anexaminationofrenewaleffects.DevelopmentalPsychobiology,49,565–575.AppendixParticleFilterAlgorithmRecallthatfortrials1...t,thevectorc1:tdenotesapartitionofthetrialsintoclustersandthevectorF1:tdenotestheobservationsforthesetrials.Inourimplementation,theparticlesaregeneratedbysamplingfromthegenerativemodel;thisinvolves,foreachparticle1...m,sequentiallydrawingahypotheticalsetofclusterassignmentsfromEquation1.Theposterioristhenapproximatedbyaweightedsumofdeltafunctionsplacedattheparticles:P共c1:t⫽c兩F1:t兲⬇冘l⫽1mwt共l兲␦关c1:t共l兲,c兴,(A1)wherect(l)isthepartitioninparticleland␦[䡠,䡠]is1whenitsargumentsareequaland0otherwise.Theimportanceweightwt(l)isproportionaltothelikelihoodofobservationftundertheparti-tioninparticlel:wt共l兲⬀P共ft兩c1:t共l兲,F1:t⫺1兲⫽写iP共ft,i兩c1:t共l兲,F1:t⫺1兲.(A2)Notethattheweightdependsonlyonthelikelihoodofthecurrentobservationbecausetheparticlesareresampledaccordingtotheirweightsatthebeginningofeachtrial(seebelow);afterresampling,the(unweighted)particlesaredistributedaccordingtotheposterior.UsingastandardcalculationfortheDirichlet-Multinomialmodel(Gelman,Carlin,Stern,&Rubin,2003),wecananalyticallyintegrateoutthemultinomialparameters␾kasso-ciatedwitheachcausetoobtainthefollowingexpressionforthelikelihood:P共ft,i⫽j兩ct共l兲⫽k,c1:t⫺1共l兲,F1:t⫺1兲⫽冕␾kP共ft,i⫽j兩ct共l兲⫽k,c1:t⫺1共l兲,F1:t⫺1,␾k兲P共␾k兲d␾k⫽Ni,j,k共l兲⫹1冘j共Ni,j,k共l兲⫹1兲,(A3)whereNi,j,k(l)isthenumberofobservationswithfeaturevaluejondimensionithatweregeneratedbycausekinparticlel.NotethatNi,j,k(l)dependsonF1:t⫺1.Althoughnotimmediatelyevidentintheseequations,learningoccursthroughmaintainingandupdatingthesufficientstatisticsofeachcluster,namelythecluster-featureco-occurrencecounts(encodedbyNi,j,k(l)).Asweshowbelow,thesesufficientstatisticscanbeusedtopredictreinforcementgivenasubsetoftheobservationfeatures(i.e.,atestobservation).Theparticlefilteralgorithmproceedsoneachtrialby1.Sampling(withreplacement)fromthecurrentsetofparticlesaccordingtotheimportanceweights.2.Foreachparticle,samplingahypotheticalclusterassign-mentforthenextobservationusingEquation1.3.RecomputingtheweightsgiventhenextobservationusingEquationA2.Twothingsshouldbenotedaboutthisalgorithm.First,particleswillreceivehigherweighttotheextentthatobservationsassignedtothesameclusteraresimilar;thiscanbeseeninEquationA3.Second,thefeaturesinteractmultiplicativelyinEquationA2:Aparticlewillreceivealargeweightonlyifalltheobservedfeaturesarelikelyundertheparticle’spartition.Theprobabilityofreinforcementforatestobservationiscal-culatedaccordingtoP共ft,1⫽reinforcement兩ft,2:3,F1:t⫺1兲⫽冘c1:tP共ft,1⫽reinforcement兩ft,2:3,F1:t⫺1,c1:t兲⬇冘l⫽1mrt共l兲P共ft,1⫽reinforcement兩F1:t⫺1,c1:t共l兲兲,(A4)wherethepredictiveweightrt(l)isproportionaltothelikelihoodoftheobservedfeatures(contextandcue):rt共l兲⬀写i⑀兵2,3其P共ft,i兩c1:t共l兲,F1:t⫺1兲.(A5)ReceivedMarch12,2009RevisionreceivedAugust31,2009AcceptedAugust31,2009䡲209CONTEXT,LEARNING,ANDEXTINCTION