we consider the problem of semi - supervised learning to ex - tract categories ( e . g . , academic elds , athletes ) and relations ( e . g . , playssport ( athlete , sport ) ) from web pages , starting with a handful of labeled training examples of each category or relation , plus hundreds of millions of unlabeled web doc - uments .
semi - supervised training using only a few labeled examples is typically unreliable because the learning task is underconstrained .
this paper pursues the thesis that much greater accuracy can be achieved by further constraining the learning task , by coupling the semi - supervised training of many extractors for dierent categories and relations .
we characterize several ways in which the training of category and relation extractors can be coupled , and present exper - imental results demonstrating signicantly improved accu - racy as a result .
categories and subject descriptors i . 123 ( articial intelligence ) : learningknowledge acqui - sition; i . 123 ( articial intelligence ) : natural language
semi - supervised learning , bootstrap learning , information extraction , web mining
machine learning approaches have been shown to be very useful for information extraction from text , including ap - proaches that learn to extract various categories of entities ( e . g . , athlete , city ) and relations ( e . g . , companyproduce - sproduct ) from structured and unstructured text ( 123 , 123 ) .
however , supervised training of accurate entity and rela - tion extractors is costly , requiring a substantial number of labeled training examples for each type of entity and rela - tion to be extracted .
because of this , many researchers have explored semi - supervised learning methods that use only a small number of labeled examples of the predicate to be extracted , along with a large volume of unlabeled text ( 123 , 123 , 123 ) .
while such semi - supervised learning methods are promising , they often exhibit unacceptable accuracy because the limited number of initial labeled examples is insucient to reliably constrain the learning process .
figure 123 : we show that signicant improvements in accuracy result from coupling the training of information extractors for many interrelated categories and relations ( b ) , compared with the simpler but much more dicult task of learning a single information extractor ( a ) .
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for prot or commercial advantage and that copies bear this notice and the full citation on the rst page .
to copy otherwise , to republish , to post on servers or to redistribute to lists , requires prior specic permission and / or a fee .
wsdm123 , february 123 , 123 , new york city , new york , usa .
copyright 123 acm 123 - 123 - 123 - 123 - 123 / 123 / 123 . . . $123 .
the thesis explored in this paper is that we can achieve much higher accuracy in semi - supervised learning by cou - pling the simultaneous training of many extractors , as sug - gested in figure 123
the intuition here is that the undercon - strained semi - supervised learning task can be made easier by adding new constraints that arise from coupling the training of many extractors .
we present an approach in which the input to the semi - supervised learner is an ontology dening a set of target categories and relations to be learned , a handful of seed ex - amples for each , and a set of constraints that couple the various categories and relations ( e . g . , person and sport are mutually exclusive ) .
we show that given this input and millions of unlabeled documents , a semi - supervised learning procedure can achieve very signicant accuracy improve - ments by coupling the training of extractors for dozens of categories and relations .
we show that our general ap - proach improves accuracies when training both contextual - pattern extractors that extract information from freeform text ( e . g . , the pattern mayor of arg123 as an extractor for the category city ) and wrappers which extract informa - tion from semi - structured documents ( e . g . , the wrapper <td class= " cty " >arg123 < / td> from some specic url ) .
based on results reported here , we hypothesize that even greater accuracy improvements will be possible by forming a more dense network of inter - constrained learning tasks .
to - ward this end , we explore two more specic points .
first , we identify three general types of coupling among target functions that can be combined to form a dense network of coupled learning problems .
second , we explore the impact of coupling the training of extractors that use freeform text with extractors that leverage semi - structured web pages , based on the intuition that these dierent techniques should make independent errors .
we believe that the novel contributions of this work are as follows : our work is the rst to couple the simultaneous semi - supervised training of category and relation extractors .
it is also the rst to couple the training of multiple wrap - per inducers by using mutual exclusion and type checking relationships .
finally , this work is the rst to couple the training ( rather than the nal outputs ) of freeform - text ex - tractors and semi - structured web page wrapper inducers by assuming that they make independent errors , a method that we show provides higher accuracies than using either method alone .
more generally , this paper advocates large - scale cou - pled training as a strategy to signicantly improve accuracy in semi - supervised learning , identies three distinct types of coupling , and experimentally evaluates their utility .
related work
in this paper , we focus on a bootstrapping method for semi - supervised learning .
bootstrapping approaches start with a small number of labeled seed examples and itera - tively grow the set of labeled examples using high - condence labels from the current model .
such approaches have shown promise in applications such as web page classication ( 123 ) , named entity classication ( 123 ) , parsing ( 123 ) , and machine translation ( 123 ) .
bootstrapping approaches to information extraction can yield impressive results ( 123 , 123 , 123 ) .
however , after many iterations , accuracy typically declines because er - rors in labeling accumulate , a problem that has been called semantic drift ( 123 ) .
to reduce errors introduced in underconstrained semi - supervised learning , several methods have been considered .
coupling the learning of category extractors by using posi - tive examples of one category as negative examples for oth - ers has been shown to help limit this decline in accuracy ( 123 , 123 ) .
co - training methods exploit conditionally independent partitions of the feature space to avoid labeling errors ( 123 ) .
other non - bootstrapping techniques have used the intuition
that dierent extraction methods should make independent errors to motivate combining predictions from multiple ex - tractors to improve extraction accuracy ( 123 , 123 , 123 ) .
type checking relation arguments using available entity recogniz - ers can help avoid incorrect labels ( 123 , 123 ) .
our work builds on these dierent ideas and uses them to couple the simulta - neous bootstrapped training of many category and relation extractors in many dierent ways .
several machine learning frameworks have been proposed that penalize violations of constraints on unlabeled data ( 123 , 123 , 123 ) .
while similar in spirit , our work diers in that we consider using many dierent kinds of constraints to learn many dierent functions simultaneously .
in multitask learning , supervised training of related func - tions together can yield higher accuracy than learning them separately ( 123 , 123 ) .
semi - supervised multitask learning has used a prior that encourages related models to have similar parameters ( 123 ) .
these methods require that related tasks share similar representations; our work exploits additional ways in which functions can be related , and makes no as - sumptions regarding similarity of representations across the dierent functions being learned .
coupled training
central to this work is the idea of coupling the semi - supervised learning of multiple functions to constrain our learning problem .
our method iteratively trains classiers in a self - supervised manner .
it starts by training classiers using a small amount of labeled data , then uses these classi - ers to label unlabeled data .
the most condent new labels are added to the pool of data used to train the models ( we say that these new labeled examples are promoted ) , and the process repeats .
the iterative training is coupled by con - straints that restrict allowable candidates and promotions .
123 types of coupling
we have identied three general types of coupling : 123
output constraints : for two functions fa : x ya and fb : x yb , if we know some constraint on values ya and yb for an input x , we can require fa and fb to satisfy this constraint .
for example , if fa and fb are boolean - valued functions and fa ( x ) fb ( x ) , we could constrain fb ( x ) to have value 123 whenever fa ( x ) = 123
compositional constraints : for two functions f123 : x123 y123 and f123 : x123 x123 y123 , we may have a constraint on valid y123 and y123 pairs for a given x123 and any x123
we can require f123 and f123 to satisfy this constraint .
for example , f123 could type check valid rst arguments of f123 , so that x123 , x123 , f123 ( x123 , x123 ) f123 ( x123 ) .
multi - view - agreement constraints : for a function f : x y , if x can be partitioned into two views where we write x = ( cid : 123 ) x123 , x123 ( cid : 123 ) and we assume that both x123 and x123 can predict y , then we can learn f123 : x123 y and f123 : x123 y and constrain them to agree .
for example , y could be a set of possible categories for a web page , x123 could represent the words in a page , and x123 could represent words in hyperlinks pointing to that page ( this example was used for the co - training setting ( 123 ) ) .
123 coupling constraints used in this paper in this work , the functions that we learn are category and relation extractors , which decide if a noun phrase or pair
of noun phrases is an instance of some category or relation ( generally referred to as a predicate in the rest of this paper ) .
the general types of coupling discussed above are used to learn these functions in these specic ways :
mutual exclusion : the input to our learner has a list of pairs of predicates which are mutually exclusive .
these relationships are used to enforce an output constraint over instances : mutually exclusive predicates cannot both be satised by the same input x .
relation argument type checking : we couple the learn - ing of relation extractors with the learning of category extractors using type checking .
for example , the argu - ments of the companyisineconomicsector relation are declared to be of the categories company and economic - sector .
this is an example of a compositional constraint .
unstructured and semi - structured text features : noun phrases on the web appear in two types of contexts : freeform textual contexts and semi - structured contexts .
for example , pittsburgh occurs on the web with a dis - tribution of freeform textual contexts such as mayor of arg123 , and it also appears with a distribution of semi - structured contexts such as the html tags for a list item at a particular url .
we assume that either of these distributions is sucient to classify a noun phrase , and that the two distributions are conditionally independent given the class label of the noun phrase .
we therefore train two noun phrase classiers , one using each type of context distribution , and require that the two classiers agree on the label for each given noun phrase .
this is an example of a multi - view constraint .
in this section , we present algorithms with which we inves - tigate the feasibility of improving semi - supervised learning for information extraction with coupling .
the general prob - lem addressed by these algorithms is to learn extractors to automatically populate the predicates of a specied ontology with high - condence instances , starting from a small set of seed instances for each predicate and a large corpus of web pages .
we focus on extracting facts that are stated multi - ple times , which we can assess probabilistically using corpus statistics .
we do not resolve strings to real - world entities : the problems of synonym resolution and disambiguation of strings that can refer to multiple entities are left for future work .
we focus our consideration of predicates on unary relations ( categories ) and binary relations ( ones with two arguments , referred to as relations in this paper ) .
the specic inputs to our algorithms are : a large text cor - pus and an initial ontology with predened categories , re - lations , mutual - exclusion relationships between same - arity predicates , and seed instances for all predicates .
each rela - tion has an ordered pair of argument types , which specify categories that relation instance arguments must be mem - bers of .
some additional information is only used by freeform - text extraction methods : seed extraction patterns for cate - gories , and a ag for each category indicating whether in - stances must be proper nouns , common nouns , or can be either ( e . g . , instances of city are proper nouns ) .
the rst algorithm , coupled pattern learner ( cpl ) , is a bootstrapping algorithm that leverages mutual - exclusion and type - checking constraints to learn high - precision con - textual patterns that are accurate extractors of predicate
algorithm 123 : coupled pattern learner ( cpl ) input : an ontology o , and text corpus c output : trusted instances / contextual patterns for
for i = 123 , 123 ,
foreach predicate p o do
extract new candidate instances / contextual patterns using recently promoted filter candidates that violate coupling; rank candidate instances / patterns; promote top candidates;
instances .
the second , coupled seal ( cseal ) , is a set expansion algorithm that learns wrappers to extract in - stances from semi - structured documents and exploits the same two types of coupling constraints .
these two algo - rithms serve to demonstrate that output and compositional coupling techniques can improve the accuracy of bootstrapped training of multiple types of extractors .
the nal algorithm , meta - bootstrap learner ( mbl ) , couples the training of sub - ordinate extraction algorithms like cpl and cseal using 123 coupled pattern learner
the coupled pattern learner ( cpl ) algorithm learns to extract category and relation instances from unstructured text , and is summarized in algorithm 123
cpl learns con - textual patterns that are high - precision extractors for each predicate ( e . g . , arg123 and other software rms and arg123 scored a goal for arg123 ) and uses them to grow a set of high - precision predicate instances .
noun phrases that ll in the arg123 and arg123 blanks of patterns in sentences in the text corpus are said to co - occur with those patterns .
at the start of execution , cpl initializes sets of pro - moted instances and patterns with the seed instances and patterns provided as input .
in each iteration , cpl expands these sets of promoted instances and patterns for each predi - cate while obeying mutual exclusion and type checking con - straints .
this is accomplished by ltering out candidates that co - occur with instances or patterns from mutually ex - clusive classes and by requiring arguments of candidate rela - tions to be candidates for the relevant categories .
each step of cpl is discussed in more detail below : 123 . 123 extracting candidates to start each iteration , cpl nds new candidate instances by using the patterns promoted in the last iteration to ex - tract noun phrases that co - occur with those patterns in the text corpus ( in the rst iteration , the seed patterns are used ) .
to keep the size of this set manageable , for each predicate , cpl selects the 123 candidates that occur with the most patterns .
an analogous procedure is used to ex - tract candidate patterns using recently promoted instances .
we use part - of - speech - tag heuristics to limit extraction to instances that appear to be noun phrases and patterns that are likely to be informative .
these are described next : category instances : in the blank of a category pattern , it uses part - of - speech
cpl looks for a noun phrase .
tags to segment noun phrases and ignores determiners .
proper noun phrases containing prepositions or conjunc - tions are segmented using a reimplementation of the lex algorithm ( 123 ) .
category instances are required to obey the proper / common noun specication of the category .
category patterns : when a promoted category instance is found , cpl extracts the preceding words as a can - didate pattern if they are verbs followed by a sequence of adjectives , prepositions , or determiners and option - ally preceded by nouns ( e . g . , being acquired by arg123 or companies acquired by arg123 ) or nouns and adjec - tives followed by a sequence of adjectives , prepositions , or determiners ( e . g . , former ceo of arg123 ) .
cpl ex - tracts the words following the instance as a candidate pattern if they are verbs followed optionally by a noun phrase ( e . g . , arg123 broke the home run record ) , or verbs followed by a preposition ( e . g . , arg123 said that ) .
relation instances : if a promoted relation pattern ( e . g . , arg123 is mayor of arg123 ) is found , a candidate relation instance is extracted if both placeholders are valid noun phrases ( according to our part - of - speech - tag heuristics ) , and if they obey the proper / common specications for
relation patterns : if both arguments from a promoted relation instance are found in a sentence then the in - tervening sequence of words is extracted as a candidate relation pattern if it contains no more than ve tokens , has a content word , and has an uncapitalized word .
123 . 123 filtering candidates using coupling candidate instances and patterns are ltered to enforce mutual exclusion and type checking constraints .
a candi - date instance is rejected unless the number of times it co - occurs with a promoted pattern is at least three times more than the number of times it co - occurs with patterns from mutually exclusive predicates .
this soft constraint is much more tolerant of the inevitable noise in web text as well as ambiguous noun phrases than a hard constraint .
candidate patterns are ltered in the same manner using promoted
123 . 123 ranking candidates next , for each predicate cpl ranks candidate instances using the number of promoted patterns that they co - occur with so that candidates that occur with more patterns are ranked higher .
candidate patterns are ranked using an es - timate of the precision of each pattern p :
p recision ( p ) =
ii count ( i , p )
where i is the set of promoted instances for the predicate under consideration , count ( i , p ) is the number of times in - stance i co - occurs with pattern p in the text corpus , and count ( p ) is the number of times pattern p occurs in the cor -
123 . 123 promoting candidates for each predicate , cpl then promotes at most 123 in - stances and 123 patterns according to the rankings from the previous step .
instances and patterns are only promoted if they co - occur with at least two promoted patterns or in - stances , respectively .
relation instances are only promoted
algorithm 123 : coupled seal ( cseal ) input : an ontology o , and text corpus c output : trusted instances / wrappers for each predicate for i = 123 , 123 ,
foreach predicate p o do
begin call existing seal code to :
query for documents containing recently learn wrappers for each document returned; extract new candidates using wrappers;
filter wrappers that extract candidates that rank candidate instances; promote top candidates;
if their arguments are candidates for the specied categories ( that is , they co - occur with at least one promoted pattern for the category , and are not promoted instances of a mutu - ally exclusive category ) .
123 . 123 large - scale implementation cpl was designed to allow ecient learning of many pred - icates simultaneously from a large corpus of sentences ex - tracted from web text .
gathering the statistics needed from the text corpus is the most expensive part of the algorithm .
the statistics needed come from two types of queries .
first , in the extraction step , cpl has a list of promoted instances and patterns , and needs to know which patterns and in - stances co - occur with those instances and patterns .
sec - ond , in the ltering and ranking steps , cpl needs to know which candidate patterns occur with which promoted in - stances , and which candidate instances occur with which promoted patterns .
cpl gathers these statistics from a pre - processed text corpus which species how many times each noun phrase occurs with each category pattern in the cor - pus , and also how many times each pair of noun phrases occurs with each relation pattern .
the preprocessing can be done quickly using using the mapreduce framework ( 123 ) .
in each iteration of cpl , cpl gathers corpus statistics from this data set by scanning through the preprocessed data in two passes : one for extracting candidates and one for count - ing co - occurrences .
cpl can perform one pass in about 123 minutes from a data set derived from 123 million web pages ( see section 123 . 123 for details on the corpus ) .
123 . 123 uncoupled pattern learner in our experiments , we use a variant of cpl called un - coupled pattern learner ( upl ) which removes the coupling constraints from cpl .
candidates are not ltered using mu - tual exclusion with other predicates , and relation arguments are not type checked .
upl is equivalent to independent semi - supervised learning of each extractor .
123 coupled seal
cpl is an example of a semi - supervised text pattern learn - ing algorithm that is aided by coupling .
to demonstrate how coupling can improve a dierent , already implemented ex - traction algorithm we consider seal ( 123 ) , a wrapper induc -
. html " class= " shopcp " >arg123 parts< / a> <br> acura , audi , bmw , buick , cadillac , chevrolet , chevy , chrysler , daewoo , daihatsu , dodge , eagle , ford , . . .
< / a><br> <a href= " auto_reviews / arg123 / acura , audi , bmw , buick , cadillac , chevrolet , chrysler , dodge , ford , gmc , honda , hyundai , inniti , isuzu , . . .
<li class= " franchise arg123 " > <h123><a href= " # " > buick , chevrolet , chrysler , dodge , ford , gmc , isuzu , jeep , lincoln , mazda , mercury , nissan , pontiac , scion , . . .
table 123 : examples of wrappers constructed by cseal for various web pages given the seeds : ford , nissan , toyota .
in the table , arg123 is a placeholder for extracting instances .
tion algorithm , and how we can add coupling constraints on top of an existing implementation that we treat as a black box .
first , we will describe the existing algorithm , and then we will describe how we add coupling constraints .
seal is a set - expansion system that accepts input ele - ments ( seeds ) of some target set s and automatically nds other probable elements of s in semi - structured documents such as web pages by querying the web using the seeds .
the algorithm implemented in seal constructs page - specic ex - traction rules , or wrappers , that are independent of the hu - man language and markup language of the web pages .
seal can expand sets of category instances as well as binary rela - tion instances .
every category wrapper is dened by char - acter strings , which specify the left context and right con - text necessary for an entity to be extracted from a page .
relation instance wrappers also are dened using an inx context that separates the two arguments of the instance .
these context strings are selected to be maximally - long con - texts that bracket at least one occurrence of every seed on a page .
table 123 shows a few examples of such wrappers for categories .
an instance is extracted by a wrapper if it is found anywhere in the document with left and right con - text identical to that of the wrapper .
when given large sets of seeds , seal can be congured to subsample the seeds some number of times ( 123 ) .
subsampling samples a subset of the seeds and uses that subset as a query to a search engine , which is necessary because using all examples in one query would typically not yield any matched results .
seal does not have a mechanism for exploiting mutual - exclusion or type - checking constraints .
wrappers for each predicate are learned independently in seal .
our algo - rithm , coupled seal ( cseal ) , adds these constraints on top of seal .
cseal is summarized in algorithm 123
each iteration of bootstrapping , we invoke seal using the recently promoted instances .
seal returns a list of new candidate instances and documents that they were extracted from .
cseal lters out any document that extracts a can - didate instance that is a member of a mutually exclusive predicate .
additionally , cseal only considers candidate relation instances if their arguments are candidate instances for the respective category types .
these forms of coupling should lter out cases where a subsampled set of seeds hap - pens to occur on a page but that page does not in fact con - tain a valid list of predicate instances .
they should also lter out cases where instances of a predicate that is more general than the one being learned are listed ( e . g . , if a long list of locations of various types is present on a page , but we are learning some specic type of location ) .
after ltering , cseal ranks all candidate instances by the number of unltered wrappers that extracted them , and promotes the top 123 instances that were extracted by at
algorithm 123 : meta - bootstrap learner ( mbl ) input : an ontology o , a set of extractors e output : trusted instances for each predicate for i = 123 , 123 ,
foreach predicate p o do foreach extractor e e do
extract new candidates for p using e with recently promoted instances;
filter candidates that violate mutual - exclusion or promote candidates that were extracted by all
least two wrappers .
to deal with web pages from the same domain that repeat the same list , only one page from a do - main is counted in ranking candidates .
without limiting consideration to domains , navigational and other template - generated elements that repeat many times can dramatically skew the results .
in our experiments below , cseal refers to the algorithm described here , and seal refers to cseal without the l - tering step : seal does not lter out wrappers that ex - tract candidates that violate mutual - exclusion relations , and seal does not enforce relation instance type checking .
123 meta - bootstrap learner
meta - bootstrap learner ( mbl ) couples the training of multiple extraction techniques using a multi - view constraint that requires them to agree .
mbl is summarized in algo - rithm 123
mbl is based on the intuition that the errors made by dierent extraction techniques should be independent .
in this paper , the subordinate algorithms used with mbl are cseal and cpl .
when using cseal and cpl with mbl , the subordinate algorithms do not promote instances on their own .
instead , they skip the promotion step and report evidence about each candidate to mbl , and mbl is responsible for promoting instances .
mbl uses a simple combination method : mbl promotes any instance that has been recommended by both techniques while obeying the mutual - exclusion and type - checking constraints specied in
experimental evaluation
we designed experiments to explore three main questions : first , does coupling learning using mutual - exclusion and
type - checking constraints improve the performance of cpl relative to uncoupled , independent learning using upl ? sec - ond , do mutual - exclusion and type - checking constraints im - prove the performance of cseal relative to the uncoupled methods of seal ? finally , does mbl achieve better perfor - mance than cpl and cseal by combining their outputs with a multi - view constraint ?
to answer these questions , we ran cpl , upl , cseal , seal , and mbl with cpl and cseal as subordinate ex - tractors for 123 iterations of learning .
we then compared the dierences in performance between several pairs of methods to see the eects of coupling .
direct comparison to previous work is dicult for a num - ber of reasons , including the lack of availability of implemen - tations and the lack of a large shared web corpus .
however , our evaluation directly tests the usefulness of the coupled approach that we are advocating in this paper .
we believe that the uncoupled baselines are reasonable and competitive large - scale uncoupled approaches .
123 experimental methodology
the ontology used in all experiments contained categories and relations from two main domains : companies and sports .
extra categories were added to provide negative evidence to the domain - related categories ( e . g . , hobby for economicsec - tor; actor , politician , and scientist for athlete and coach; and boardgame for sport ) and also to provide wider variety for experiments ( e . g . , shape , emotion ) .
table 123 lists all of the categories in the leftmost column , and table 123 lists the relations in the leftmost column .
categories were initialized with 123 seed instances and 123 seed patterns .
the seed in - stances were specied by a human , and the seed patterns for each category were derived from the generic patterns of hearst ( 123 ) .
relations were initialized with 123 seed in - stances , 123 seed negative instances ( typically incorrect varia - tions of positive seed examples ) , and no seed patterns ( since it is not obvious how to generate good seed patterns from relation names ) .
most predicates were declared as mutually exclusive with one another ( examples of exceptions include sportsteam and university; kitchenitem and producttype; and company and product ) .
123 . 123 corpus for cpl the text corpus used by cpl was from a 123 - million - page web crawl .
we parsed the html , ltered out non - english pages using a stop - word - ratio threshold , then ltered out web spam and adult content using a bad word list .
the pages were then segmented into sentences , tokenized , and tagged with parts - of - speech using the opennlp package .
finally , we ltered the sentences to eliminate those that were likely to be noisy and not useful for learning ( e . g . , sentences without a verb , without any lowercase words , with too many words that were all capital letters ) .
this yielded a corpus of roughly 123 million sentences .
as discussed in section 123 . 123 , we processed these sentences to create a data set of noun phrase and contextual pattern co - occurrence counts .
to manage the size of the data set , we ltered out all noun phrases and contexts that only occurred once in the corpus .
this yielded a data set that contained 123 million unique contextual patterns for categories , 123 million unique noun phrases , 123 million unique pairs of
noun phrases that co - occur together , and 123 million unique contextual patterns for relations .
123 . 123 parameters for seal in our experiments with cseal and seal , we used an implementation provided by the original authors of seal .
seal was congured to subsample the examples provided 123 times for categories and 123 times for relations to mitigate the relatively higher sparsity of relations .
seal downloaded up to 123 web pages for each search query using results from the google search engine .
thus , the corpus for seal was the web as indexed by google .
the minimum context length for a wrapper was set to 123 , which meant that each part of a wrapper needed to be at least 123 characters long .
123 . 123 general experimental procedure when comparing two algorithms , we ran each algorithm for 123 iterations of bootstrapping , and then assessed the instances promoted by the algorithms .
to evaluate the pre - cision of all instances promoted by an algorithm on a per - predicate basis , we sampled 123 instances from the set of promoted instances for each predicate , pooled together the samples , and submitted the instances to mechanical turk for labeling .
this gave an estimate of how accurate all of the instances were and measured the degree to which a par - ticular method avoided semantic drift .
we also compared algorithms at matching levels of recall .
for each predicate , we only considered the rst k instances promoted by each algorithm , where k was the minimum number of instances promoted for that predicate between the two algorithms .
we refer to this method of comparison as the minimum recall method in the results below .
we sampled 123 instances from each of these two sets of instances , and also submitted them to mechanical turk .
while samples of 123 instances do not produce tight con - dence intervals for individual estimates of precision for a single predicate , they are sucient for testing for the eects in which we are interested .
cpl can reliably extract the proper case of an instance , but lists of items on the web often use arbitrary case con - ventions , so cseal cannot reliably extract the proper case of an instance .
because of this , our evaluation ignored case , and presented all instances to the evaluators in lower case .
123 . 123 mechanical turk labeling the various estimates of precision required for our eval - uation yielded 123 unique instances .
we submitted each of these instances to mechanical turk for labeling and had three dierent individuals label each instance .
mechanical turk has been shown to be an inexpensive and fast method for obtaining labels for language tasks ( 123 ) .
to estimate the accuracy of the labels produced by this procedure , we sampled 123 instances at random , and manually judged the accuracy of their labels .
we found that 123 out of the 123 were correctly labeled using the majority vote .
the four er - rors were : a false positive with entomology there labeled as an academicfield ( the labelers ignored the segmentation error ) , and three false negatives : informs as a profession - alorganization , love seats as furniture , and the relation in - stance companycompeteswithcompany ( bhp , rio ) .
this suggests that the labels may be biased towards false nega - tives , which in turn suggests that our precision estimates in the remainder of the paper may be pessimistic .
promoted instances ( # )
cpl upl cseal seal mbl
cpl upl cseal seal mbl
table 123 : precision ( % ) and counts of promoted instances for each category using cpl , upl , cseal , seal , and
promoted instances ( # )
cpl upl cseal seal mbl
cpl upl cseal seal mbl
table 123 : precision ( % ) and counts of promoted instances for each relation using cpl , upl , cseal , seal , and mbl .
123 . 123 supplementary online materials several dierent types of materials from our evaluation are posted online at http : / / rtw . ml . cmu . edu / wsdm123_online :
seeds for all predicates .
all instances promoted by mbl , cpl , upl , cseal ,
all textual patterns promoted by pattern learning in the
mbl , cpl , and upl experiments .
browseable knowledge bases in xml format of all pro - moted instances and candidate instances from the runs of mbl , cpl , and cseal , with patterns and urls that extracted each instance .
all judgments obtained from mechanical turk .
an example screenshot from a mechanical turk task .
templates used to create the mechanical turk tasks ,
which may be of general use .
123 mutual exclusion and type checking
to explore the eects of coupling predicates using mutual - exclusion and type - checking constraints , we compared cou - pled and uncoupled methods for learning contextual pat - terns for freeform text : cpl : the algorithm as described in section 123 .
upl : this method is an uncoupled version of cpl; it does not couple predicates using mutual - exclusion con - straints or type checking .
relation instance arguments are not ltered using their categories and candidate in -
stances and patterns are not ltered out based on vio - lations of mutual exclusion .
the common / proper noun specications of arguments are used to lter out implau -
we also compared coupled and uncoupled methods for learning wrappers to extract lists of instances from semi - structured web pages : cseal : the algorithm as described in section 123 .
seal : this method uses the implementation of seal provided by the authors of seal .
as in the upl method , it does not couple the learning of predicates using mutual - exclusion constraints or type checking .
table 123 gives estimates of the precision of promoted in - stances for each category for each algorithm , as well as the number of promoted instances for each category after 123 iter - ations .
the average row averages across all predicates for which instances were promoted .
the weighted average is an estimate of the instance - level precision across all predi - cates obtained by weighting the precision for each predicate by the number of instances promoted for that predicate .
ta - ble 123 gives this information for each relation , as well .
across all categories and relations , cpl has higher average preci - sion than upl , and cseal has higher average precision than seal .
these results suggest that coupling using type checking and mutual exclusion signicantly reduces the error rates of the learned extractors .
another method of comparing which algorithms perform the best is to use the sign test , which is a non - parametric
cpl vs .
upl cseal vs .
seal mbl vs .
cpl mbl vs
123 123 vs .
123 123 vs .
123 123 vs
123 123 vs .
123 123 vs .
123 123 vs
table 123 : various pairs of methods compared based on the precision of all promotions for each predicate ( all promotions ) and the precision of the instances promoted cut o at the minimum recall out of the pair for each predicate ( minimum recall ) .
wins record how many predicates had superior precision for each method , and the p - value according to a sign test is given .
all re - sults are statistically signicant at the 123% level except for cseal vs .
seal at minimum recall .
figure 123 : examples of extracted facts .
the generaliza - tions of software were given in the seed ontology; all other facts were discovered by cpl .
values shown for productinstances and companiesinsector for soft - ware are a subset of the full set of promoted values .
hypothesis test .
the test statistic needed to compare , for ex - ample , cpl with upl , is obtained by counting the number of predicates for which cpl performed better than upl , and vice versa , ignoring ties .
this test gracefully handles predicates where only one method promoted instances : we prefer the method which extracted some instances rather than none for such predicates .
table 123 compares cpl vs .
upl and cseal vs .
seal for the precision of all promoted instances for each predicate , as well as the minimum recall sample discussed above .
cpl performs statistically signicantly better than upl for both methods of sampling .
cseal is signicantly better than seal with respect to the precision of all promotions , but is not signicantly better when thresholding recall to the minimum recall for each predicate .
these results conrm that coupling yields signicantly higher accuracies across all predicates than using independent , uncoupled learning .
the results for cseal vs .
seal suggest that coupling prevents cseal from promoting some incorrect instances but with some loss in recall .
figure 123 gives some examples of the type of information extracted in our experiments .
the initial seed examples pro - vided specied that software is a producttype and an eco - nomicsector; the rest of the information in the gure was extracted by cpl .
123 multiple extraction techniques
tables 123 and 123 also give estimates of the precision of pro - moted instances for each predicate for mbl after 123 itera - tions .
across both relations and categories , mbl has the highest precision of promoted instances out of all of the al -
gorithms considered , which indicates that adding the multi - view - agreement constraint results in further avoidance of se - mantic drift .
table 123 gives sign test results for comparing mbl vs .
cpl and mbl vs .
cseal , which allows us to judge whether or not mbl improves over its subordinate algorithms .
all sign tests show statistically signicant dif - ferences : mbl is superior to both cpl and cseal when comparing both the precision of all promoted instances as well as the precision of promoted instances at the minimum recall of either method .
this suggests that coupling cpl and cseal with a multi - view coupling constraint that as - sumes independent errors yields more accurate learning than either method used alone .
the results presented here demonstrate that adding more
coupling improves the accuracy of our learned extractors .
one of the biggest challenges in applying bootstrap learn - ing algorithms is determining when to stop the bootstrap - ping process .
ideally , an algorithm would be able to respect the boundaries of a closed set .
in this respect , the results for the country category for mbl are particularly compelling .
mbl promoted 123 instances of countries with an estimated precision of 123% .
cseal promoted 123 instances with an estimated precision of 123% .
without coupling , country per - forms poorly , drifting into a more general location category .
the categories for which the coupled algorithms still have the most diculty ( e . g . , producttype , sportsequipment , traits , vehicles ) tend to be common nouns .
we expect that a more complete hierarchy of common nouns would better constrain these categories and yield better accuracies .
the coupled algorithms generally had high accuracies for relations , but suered from sparsity .
sportusessportsequip - ment suered because the sportsequipment category per - formed poorly , resulting in bad type checking .
statehascap - ital and companyheadquarteredincity drifted to the more general relations of statecontainscity and companyhas - operationsincity .
these latter two cases can be improved by adding the ability to infer negative examples using the knowledge that these are functional relations : patterns that extract multiple capitals for the same city could be ltered out using this knowledge .
our experiments included ve relations for which no in - stances were promoted by any algorithms : coachcoaches - athlete , athleteplaysinstadium , coachwonawardtrophy - tournament , sportplaysgamesinstadium , and athleteis - teammateofathlete .
these relations show that some re - lations are not easy to extract using the extraction methods used in this paper .
however , many of these relations could be inferred from instances promoted in our current work .
we plan to investigate learning to infer such relations .
we have presented methods of coupling the semi - supervised learning of category and relation instance ex - tractors and demonstrated empirically that coupling fore - stalls the problem of semantic drift associated with boot - strap learning methods .
this empirical evidence leads us to advocate large - scale coupled training as a strategy to signif - icantly improve accuracy in semi - supervised learning .
software isa : product type , economic sector productinstances : itunes , excel , adobe photoshop , microsoft outlook , autocad , kazaa companiesinsector : infosys , sap , microsoft , ibm , wipro , symantectigers isa : mammal , sports team teamhomestadium : comerica park teamcoach : les miles teamwontrophy : world series teamplaysagainstteam : yankees , royals , sox , white sox , red sox , warriors acknowledgments this work is supported in part by darpa , google , a yahoo ! fellowship to andrew carlson , and the brazilian research agencies cnpq and capes .
we also gratefully acknowl - edge jamie callan for making available his collection of web pages , yahoo ! for use of their m123 computing cluster , and the anonymous reviewers for their comments .
