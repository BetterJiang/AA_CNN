the world wide web is a vast source of information accessible to computers , but understandable only to humans .
the goal of the research described here is to automatically create a computer understandable knowledge base whose content mirrors that of the world wide web .
such a knowl - edge base would enable much more effective retrieval of web information , and promote new uses of the web to support knowledge - based inference and problem solving .
our approach is to develop a trainable information extraction system that takes two inputs .
the first is an ontology that defines the classes ( e . g . , company , person , employee , product ) and relations ( e . g . , employed . by , produced . by ) of interest when creating the knowledge base .
the second is a set of training data consisting of labeled regions of hypertext that represent instances of these classes and relations .
given these inputs , the system learns to extract information from other pages and hyperlinks on the web .
this paper describes our general approach , several machine learning algorithms for this task , and promising initial results with a prototype system that has created a knowledge base describing university people , courses , and research projects .
this research has been supported in part by the darpa hpkb program under research contract f123 - 123 - 123 - 123
the views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies , either expressed or implied , of darpa or the
dtic quality inspected 123
keywords : information extraction , machine learning , world wide web , knowledge
bases , web spider , text classification , relational learning .
the opportunity 123
overview of the webkb system 123
problem formulation 123
experimental testbed 123
learning to recognize class instances 123
statistical text classification 123
approach 123
experimental evaluation 123
first - order text classification 123
approach 123
experimental evaluation 123
combining learners 123
approach
experimental evaluation 123
identifying multi - page segments 123
approach 123
experimental evaluation ; 123
section summary 123
learning to recognize relation instances 123
problem representation 123
learning methods 123
experimental evaluation 123
learning to extract text fields 123
approach 123
experimental evaluation
related work 123
document classification 123
information extraction 123
extracting semantic information from hypertext 123
extracting knowledge bases from the web 123
web agents 123
conclusions and future work 123
a obtaining more evenly distributed scores from naive bayes 123
the opportunity
the rise of the world wide web has made it possible for your workstation to retrieve over 123 , 123 , 123 web pages for your personal perusal .
the web has already become one of the largest and most diverse sources of information on the planet , and many expect it to grow into the world ' s primary knowledge resource over the next decade .
the research described here is motivated by a simple observation : although your work - station can currently retrieve over 123 , 123 , 123 web pages , it currently understands none of these web pages .
the goal of our webkb research project is to automatically create a computer - understandable knowledge base whose content mirrors that of the world wide web .
such a " world wide knowledge base " would consist of computer understandable as - sertions in symbolic , probabilistic form ( e . g . , employed . by ( markcraven , carnegiemellonuniv ) , probability= . 123 ) .
we expect such a world wide knowledge base would have many uses .
at a minimum , it would allow much more effective information retrieval by supporting queries such as " find all universities within 123 miles of pittsburgh that offer evening courses on java programming . " going a step further , it would enable new uses of the web to support knowledge - based inference and problem solving .
for example , it would provide the knowl - edge base needed by a software travel agent that might handle requests such as " make me hotel and flight arrangements for the upcoming acm conference . " notice that information about the acm conference , nearby hotels , and flights is already available in human - readable form , spread across multiple text pages on the web .
a knowledge base that makes this in - formation computer - understandable would support a variety of intelligent knowledge - based
how might we construct and maintain such a world wide knowledge base ? the thesis explored in this paper is that one can develop such a knowledge base by ( 123 ) using machine learning to create information extraction methods for each of the desired types of knowl - edge , then ( 123 ) applying these learned information extraction methods to extract symbolic , probabilistic statements directly from web hypertext .
each assertion in the knowledge base can therefore carry with it a justification , in terms of the web sources and information extraction method , that provide its evidential support .
as the web evolves over time , the current validity of knowledge base facts can be tested by verifying their continued evidential
this paper explores the above thesis by proposing and evaluating several learning algo - rithms relevant to this information extraction task , and by presenting the prototype we - bkb system which has successfully built up a knowledge base containing several thousand assertions about computer science departments using these learned information extractors .
we begin by briefly surveying the capabilities of the webkb system in the next section .
the subsequent section considers in detail the representational assumptions underlying our approach .
the remaining sections present our experimental testbed , several learning algo - rithms and experimental results for the various information extraction tasks , related work ,
( " fundamentals of cs home page
instructors . of : jim , tom
research : webwatcher project
! courses . taught . by : fund , of cs , . . .
fundamentals of cs home page
jim ' s home page
i teach several courses : - fundamentals of cs .
my research includes
intelligent web agents
figure 123 : the inputs and outputs of the web kb system .
the top part of the figure shows an ontology that defines the classes and relations of interest .
the bottom part shows two web pages identified as training examples of the classes course and faculty .
together , these two pages also constitute a training example for the relations instructors . of and courses . taught . by .
given the ontology and a set of training data , webkb learns to interpret additional web pages and hyperlinks to add new instances to the knowledge base , such as those shown in the middle of the figure .
overview of the webkb system
the webkb system is first trained to extract information of the desired types , and is then allowed to browse new web sites in order to automatically populate a knowledge base with new assertions .
when training this system , the user must provide two inputs :
a specification of the classes and relations of interest .
this is the ontology that defines the vocabulary for the target knowledge base .
an example of such an ontology is provided in the top half of figure 123
this particular ontology defines a hierarchy of classes including person , student , research . project , course , etc .
it also defines relations between these classes such as advisors . of ( which relates an instance of a student to the instances of faculty who are the advisors of the given student ) .
this ontology constitutes the initial version of the knowledge base .
training examples that describe instances of the ontology classes and relations .
for example , the two web pages shown at the bottom of figure 123 represent instances of course and faculty classes .
furthermore , this pair of pages represents an instance of the relation courses . taught . by ( i . e . , the courses . taught . by jim includes fundamen -
given such an ontology and a set of training examples , the webkb system learns general procedures for extracting new instances of these classes and relations from the web .
in the current prototype , this is accomplished by learning to classify arbitrary web pages and hyperlink paths according to the classes and relations defined by the ontology .
when exploring the web , the webkb system starts from a given input url and explores pages using a breadth - first search to follow links .
each explored page is examined , using learned class descriptions , to see if it represents a member of one of the ontology classes .
if a page is a class member , an entity representing that page is placed into the knowledge base , and the ontology relations for that page are instantiated based on learned rules and the known local structure of the web .
if a page is not a class member , the search is truncated , and links from this page are not followed .
in one such crawling experiment the system was given a training set of approximately 123 , 123 web pages and 123 , 123 web - page pairs taken from the computer science department web sites at four universities ( cornell , university of texas at austin , university of washington , and university of wisconsin ) .
these training examples were hand labeled according to the ontology shown in figure 123
the system was then allowed to explore the web site of a fifth computer science department ( at carnegie mellon university ) , and to add new knowledge base entries based on information extracted from this new web site .
two new instances added by the system to its knowledge base , as a result of browsing this new university web site , are shown in figure 123
the top instance describes a new faculty added to the knowledge base , as a result of examining a web page that the system classified into this category .
as a result , the system created the new faculty instance in the knowledge base , and extracted several relations involving this instance .
for example , it determined ( cor - rectly ) the name of this faculty member , a course taught by the faculty member , and three
( name " david garlan " )
( members . of . project ( +bridget - spitznagel* *david - garlan* *ralph - melton*
figure 123 : two of the entities automatically extracted from the cmu computer science department web site after training on four other university computer science sites .
these entities are added as new instances of faculty and project to the knowledge base shown in figure 123
student faculty person research . project course department overall
table 123 : class instance recognition accuracy when exploring cmu computer science department web site , after training on computer science department at four other universities .
instances of the projects . of relation for this faculty member : *architectural - mismatch* , *cmu - cs - cgmp123sable - s123ftware - systems - h123me - page* , and *able - project* .
these three projects are themselves instances of the research . project class , extracted from other web pages .
the description of one of these , the *able - pr123 ject* , is shown at the bottom of the
how accurate is the system in extracting such information ? in this experiment , the system visited 123 web pages at the new carnegie mellon site , and as a result added 123 new class instances to its knowledge base .
the fraction of correctly extracted instances is summarized in table 123
for example , this table indicates that the system created 123 new knowledge base instances of the class course .
of these 123 new instances , 123 in fact represented courses and the other 123 did not .
its accuracy in extracting relation instances is summarized in a similar fashion in table 123
note that since we don ' t have a labelling for all the pages and relations at carnegie mellon , we have no way of calculating coverage results for these tasks .
figure 123 shows the information displayed by the system as it browses .
note this display shows the web page that is currently being visited ( top right ) , and the information extracted by the system from this web page ( middle left ) .
the interface also contains a control panel
123attxdb#*aa* tt^ttlftlf kt ^123&tmttm&123i flmmhaamjlijiiia -
edit vtew go bookmarks options directory window
t - ^ a t w ( open i flint ; fmd
! i honei
what ' s hew ? ) what ' s cool ? - uestwalions net sparch ieiiplp sultw;in>
command : ;contjniie |baek |su ? p ( quit
stntkb ? vf# w>|jtt|> / j ' vww . e# . <siu , ecfu / ftka>td<123o
avrim l .
blum associate professor of computer science
department of computer science carnegie mellon university pittsburgh pa 123 - 123
office : wean 123 tel : ( 123 ) 123s123 - 123
me hakte htcn : ffw<g , eb , c
astf^ *s faclty with seers - 123^123
my mean res earch interests are machine learning theory , approximation algorithms , and on - line algorithms .
i also am worldng onoraphplan .
a graph - algorithmbasedplannerfor strips - like domains .
i was recently program committee co - chair for colt ' 123 ( the ninth annual conference on computational l earning theory ) , and on the pro gram c ommittee for soda ' 123 ( the eighth annual acm - siam symposium on discrete hierarchy curroundiiig u . enttty
faculty fcainas 123 courses found : 123 123te123et ' toun* 123
figure 123 : the web interface to the web kb system .
the upper left pane serves as a control panel .
the middle left pane describes the current activity of the web kb system , and the lower left panes summarizes the session .
the upper right pane shows the page currently being processed , and the lower right pane provides a mechanism for browsing the extracted knowledge base .
instructors . of members . of .
project department . of overall
table 123 : relation instance recognition accuracy when exploring cmu computer science department web site , after training on computer science department at four other universities .
that allows the user to interact with the system ( top left ) , and to browse the growing knowledge base ( bottom right ) .
problem formulation
as summarized in the previous section , the webkb system provides experimental support for our thesis that a system can be trained to automatically populate a knowledge base by browsing web hypertext .
later sections describe in detail the learning algorithms used by this webkb system .
in this section , we consider the precise problem formulation and the representational assumptions that underlie our current approach .
to summarize , we are interested in the following general problem :
- an ontology that defines the classes ( e . g . , person ) and relations ( e . g . , instructor . of )
- training examples from the web that describe instances of these classes and rela -
- general procedures capable of extracting additional instances of these classes and
relations by browsing the rest of the web .
note that we do not necessarily extract new instances for all of the classes and relations in the ontology .
for example , our ontology may have a class country along with instances for all of the countries in the world .
in this case , since we already know all instances , we do not need to learn procedures to recognize new ones .
to pursue the problem of learning to extract instances from the web , we must make some assumptions about the types of knowledge to be extracted from the web , and the way in which this knowledge is represented in hypertext on the web .
these assumption are :
assumptions about how class instances are described on the web .
we assume that each instance of an ontology class is represented by one or more contiguous segments of hypertext on the web .
by " contiguous segment of hypertext " we mean either a single web page , or a contiguous string of text within a web page , or a collection of several web pages interconnected by hyperlinks .
for example , an instance of a person might be described by a single page ( the person ' s home page ) , or by a reference to the person in a string of text within an arbitrary web page , or by a collection of interconnected web pages that jointly describe the person .
assumptions about how relation instances are described on the web .
consider an ar - bitrary instance r ( a , b ) of a relation r .
we assume that each instance of a relation is represented on the web in one of three ways .
first , the instance r ( a , b ) may be represented by a segment of hypertext that connects the segment representing a to the segment representing b .
for example , the bottom of figure 123 shows two hyperlinks that connect the segment representing jim with the segment representing fundamen - tals . of . cs .
these hyperlinks represent the relation instructor . of ( fundamentals . of . cs , jim ) .
second , the instance r ( a , b ) may alternatively be represented by a contiguous segment of text representing a that contains the segment that represents b .
for exam - ple , the relation instance research . of ( jim , human . computer . interaction ) is represented in figure 123 by the fact that jim ' s home page contains the phrase " human computer in - teraction " in a particular context .
finally , the instance r ( a , b ) may be represented by the fact that the hypertext segment for a satisfies some learned model for relatedness to b .
for example , we might extract the instance research . of ( jim , artificial . intelligence ) by classifying jim ' s page using a statistical model of the words typically found in pages describing ai research .
in addition to these assumptions about the mapping between web hypertext and the ontology , we make several simplifying assumptions in our initial research reported in this paper .
we plan to relax the following assumptions in the future as our research progresses .
we assume in this paper that each class instance is represented by a single web page ( e . g . , a person is represented by their home page ) .
if an instance happens to be described by multiple pages ( e . g . , if a person is described by their home page plus a collection of neighboring pages describing their publications , hobbies , etc . ) , our current system is trained to classify only the primary home page as the description of the person , and to ignore the neighboring affiliated pages .
alternatively , if an instance happens to be described by a text fragment , our system does not currently create a knowledge base instance for this .
it does , however , extract certain relation values from such text fragments ( e . g . , the name of the person , as illustrated in figure 123 ) .
we assume that each class instance is represented by a single contiguous segment of hypertext .
in other words , if the system encounters two non - contiguous web pages that represent instances of the same class , it creates two distinct instances of this class in its knowledge base .
while this assumption will often be satisfied ( e . g . , two distinct personal home pages typically represent two distinct people ) , there are clearly
exceptions ( e . g . , there are many different web pages describing elvis ) .
overcoming this " multiple elvis problem " will require methods that hypothesize equivalences between independently discovered instances .
we assume that all relations are two - place relations; that is , ' each relation has only two arguments .
we believe that it will be fairly easy to relax this assumption .
given this problem definition and our current set of assumptions , we view the following as the three primary learning tasks that are involved in extracting knowledge - base instances from the web :
recognizing class instances by classifying bodies of hypertext .
section 123 looks at this problem , using both statistical and relational learning techniques .
it also examines how to relax our assumption about class instances being represented by single web
recognizing relation instances by classifying chains of hyperlinks .
section 123 investi -
gates a relational learning solution to this problem .
recognizing class and relation instances by extracting small fields of text from web
section 123 looks at this task and also uses a relational learning approach .
experimental testbed
all experiments reported in this paper are based on the ontology for computer science departments shown in figure 123
this ontology includes the classes department , faculty , staff , student , research . project , and course .
our web page classification experiments also use the class other as the label for web pages that fall into none of these ontology classes .
each ontology class has an associated set of slots , or relations , that exist among instances of this class and other class instances in the ontology .
for example , the course class has a slot called instructors . of that relates courses to people .
we assembled two data sets123 for the experiments reported here .
the first is a set of pages and hyperlinks drawn from four cs departments : university of texas at austin , cornell university , university of washington , and university of wisconsin .
the second is a set of pages from numerous other computer science departments .
the four - department set includes 123 , 123 pages and 123 , 123 hyperlinks interconnecting them .
the second set includes 123 , 123 additional pages .
the pages for most of the classes in our data set were collected using " index " pages for our classes of interest ( e . g . , a page that has hyperlinks to all of the students in a department ) , so labeling this data was straightforward .
after gathering this initial set of pages , we then collected every page that was both ( i ) pointed to by a hyperlink
123 these data sets are publicly available at http : / / www .
edu / af s / cs .
edu / project / theo - 123 /
in the initial set , and ( ii ) from the same university as the page pointing to it .
most of the pages gathered in the second step were labeled as other .
in addition to labeling pages , we also hand - labeled relation instances .
each of these relation instances consists of a pair of pages corresponding to the class instances involved in the relation .
for example , an instance of the instructors . of relation consists of a course home page and a person home page .
our data set of relation instances comprises 123 instructors . of instances , 123 members . of . project instances , and 123 members . of . department instances .
these instances are all from the four - department set .
finally , we also labeled the name of the owner of pages in the person class .
this was done automatically by tagging any text fragment in the person ' s home page that matched the name as it appeared in the hyperlink pointing to the page from the index page .
the matching heuristics were conservative , favoring precision over recall .
consequently , we believe that , although some name occurrences were missed , there were no false positives .
from 123 person pages , this procedure yielded 123 distinct name occurrences .
these instances are all from the four - department set as well .
for all of the subsequent experiments in this paper , we use a four - fold cross - validation methodology to evaluate our algorithms .
we conduct four runs in which we train classifiers using data from three of the universities in our data set ( plus the second set of pages where applicable ) , and test the classifiers using data from the remaining university .
on each iteration we hold out a different university for the test set .
learning to recognize class instances
the first task for our system is to identify new instances of ontology classes from the text sources on the web .
in this section we address the case in which class instances are repre - sented by web pages; for example , a given instance of the student class is represented by the student ' s home page .
in the first part of this section we discuss a statistical bag - of - words approach to classi - fying web pages .
we use this method along with three different representations of pages .
in the second part of this section we discuss learning first - order rules to classify web pages .
this approach is appealing in that first - order rules can describe page classes using a rich de - scription of the local graph structure around the page .
finally , we evaluate the effectiveness of combining the predictions made by all four of these classifiers .
statistical text classification
in this section we consider classifying web pages using statistical methods .
our approach is similar to a growing body of work in text classification that involves using a so - called bag of words or unigram representation .
however , we apply our method in novel ways that take advantage of the redundancy of hypertext .
specifically , we train three independent classifiers
which use different representations for page classification :
full - text : the words that occur anywhere in the page ,
title / heading : the words that occur in the title and html headings of the page ,
hyperlink : the words that occur in hyperlinks ( i . e . , the words in the anchor text ) that
point to the page .
our approach involves building a probabilistic model of each class using labeled training data , and then classifying newly seen pages by selecting the class that is most probable given the evidence of words describing the new page .
the method that we use for classifying web pages is naive bayes , with minor modifications based on kullback - leibler divergence .
given a document d to classify , we calculate a score for each class c as follows :
where n is the number of words in d , t is the size of the vocabulary , and w ( is the tth word in the vocabulary .
pr ( wi\c ) thus represents the probability of drawing to , - given a document from class c , and pr ( wi\d ) represents the frequency of occurrence of w ( in document d .
the class predicted by the method for a given document is simply the class with the greatest score .
this method makes exactly the same classifications as naive bayes , but produces classification scores that are less extreme .
below we explain naive bayes; in appendix 123 we detail our modifications to it .
the probabilistic models we use ignore the sequence in which the words occur .
these models are often called unigram or bag - of - words models because they are based on statistics about single words in isolation .
since the unigram model naively assumes that the presence of each word in a document is conditionally independent of all other words in the the document given its class , this approach , when used with bayes rule is often called naive bayes .
the independence as - sumption is clearly false , and it produces obviously incorrect class - membership probabilities .
however , it is often the case that even when the assumption does not hold and naive bayes produces inaccurate probability estimates , it still is able to classify test examples with high
there are two common approaches to naive bayes text classification .
one , the multi - variate bernoulli model , is a bayesian network with no dependencies between words and binary word counts; the document is considered to be the " event " and the words are features
ofthat event .
the other approach , the multinomial model , is a unigram language model with integer word counts; the words are considered to " events " and the document is comprised of a collection of these events .
we use the second approach , since it has been found to out - perform the first on several data sets ( 123 ) .
we formulate naive bayes for text classification as follows .
given a set of classes c = ( ci , . . . c|c| ) and a document consisting of n words , ( wi , w123 , . . . wn ) , we classify the document as a member of the class , c , that is most probable , given the words in the document :
c = argmaxcpr ( c|ti , . . . , t ( ;n )
we transform pr ( c|iii , . . . , wn ) into a computable expression by repeatedly applying bayes rule to the individual words in pr ( c|wi , . . . , wn ) ( eq .
123 ) and assuming that words are inde - pendent of each other ( eq .
we also drop the denominator ( eq .
123 ) , since this term is a constant across all classes .
pr ( c\w123 , . . . , wn ) = pre ) ) . ) . - ; r ~ ( 123 )
" pr ( iu , - |c ) , . , - pr ( c ) il p / \ ( 123 )
oc pr ( c ) nprklc ) ( 123 )
the modifications that transform this traditional formulation of naive bayes into the form
we use ( shown in equation 123 ) are described in appendix 123
estimating word probabilities
a key step in implementing naive bayes is estimating the word probabilities , pr ( to , - |c ) .
to make our probability estimates more robust with respect to infrequently encountered words , we use a smoothing method to modify the probabilities that would have been obtained by simple event counting .
one important effect of smoothing is that it avoids assigning probability values of zero to words that do not occur in the training data for a particular class .
since naive bayes involves taking a product of word probabilities , a single zero for a class would prevent that class from being the maximum even if there are many other words that strongly indicate that class .
rather than smoothing with the common laplace estimates ( i . e . , adding one to all the word counts for a class ) , we use witten - bell smoothing ( 123 ) , which we have found to perform better on several data sets .
witten - bell sets pr ( to , - |c )
where at ( iw , - , c ) is the count of the number of times word wi occurs in the training data for
class c , tc is the total number of unique words in class c , and t is the total number of unique words across all classes .
another important implementation issue is deciding upon the vocabulary size to be used for the problem domain .
we have found empirically that we get slightly more accurate classifications when using a restricted vocabulary size .
thus we limit our vocabulary to 123 words in all of our experiments .
the vocabulary is selected by ranking words according to their average mutual information with respect to the class labels .
we write w ( for a random variable indicating whether word to , - is present or absent in a document , and write v ( ( wu ^wi ) for the values it takes on .
we write c for a random variable taking values of all the class labels , c <e c .
then , average mutual information is
i ( c;wi ) = h ( c ) - h ( c\wi ) ( 123 ) = - epr ( c ) log ( pr ( c ) ) ( 123 )
- - e pr ( 123 ) ep^k ) log ( pr ( ck ) ) ( 123 )
e erlieg f^f . ) ( io )
this feature selection method has been found to perform best among several alternat
( 123 ) , and has been used in many text classification studies ( 123 , 123 , 123 , 123 , 123 ) .
experimental evaluation
we evaluate our method using the cross - validation methodology described in section 123
on each iteration of the cross - validation run , we train a classifier for each of the page repre - sentations described at the beginning of this section : full - text , title / heading , and hyperlink .
table 123 shows the resulting confusion matrix ( summed over the four test sets ) for the full - text classifiers .
each column of the matrix represents one class and shows how the instances of this class are classified .
each row represents the instances that are predicted to belong to a given class , and shows the true classes of these instances .
this table illustrates several interesting results .
first , note that for most classes , our classifiers are quite accurate .
for example , 123% of the course and 123% of the faculty instances are correctly classified .
the notable exception to this trend is the other class; only 123% of the instances belonging to this class are correctly classified .
we discuss this result in more detail below .
a second interest - ing result is that many of the remaining mistakes made by the classifiers involve confusing different subclasses of person .
for example , although only 123% of the staff instances are correctly assigned to the staff category , 123% of them are correctly classified into the more general class of person .
as this result suggests , not all mistakes are equally harmful; even when we fail to correctly classify an instance into one of the leaf classes in our ontology , we
123 - > o <u . c u k .
123 123 123 123 123 123
v - 123 o
table 123 : a confusion matrix showing full - text classification results combined across all of the four - university test runs .
the overall coverage and accuracy are also shown .
can still make many correct inferences about the instance if we correctly assign it to a more
the low level of classification accuracy for the other class is largely explained by the nature of this class .
recall from section 123 that the instances of this class were collected by gathering pages that were one hyperlink away from the instances in the other six classes .
for this reason , many of the instances of the other class have content , and hence word statistics , very similar to instances in one of the " core " classes .
for example , whereas the home page for a course will belong to the course class , " secondary " pages for the course , such as a page describing reading assignments , will belong to the other class .
although the content of many of the pages in the other class might suggest that they properly belong in one of the core classes , our motivation for not including them in these classes is the following .
when our system is browsing the web and adding new instances to the knowledge base , we want to ensure that we do not add multiple instances that correspond to the same real - world object .
for example , we should not add two new instances to the knowledge base when we encounter a course home page and its secondary page listing the reading assignments .
because of this requirement , we have framed our page classification task as one of correctly recognizing the " primary " pages for the classes of interest .
as table 123 indicates , this is a very difficult task , but as we will show shortly , by combining several sources of evidence for each page , it is one we can perform with high accuracy .
one way to obtain insight into the learned classifiers is to ask which words contribute most highly to the quantity scorec ( d ) for each class .
to measure this , we used one of our
table 123 : the top ten most highly weighted words .
for each class , the table shows the ten words that are most highly weighted by one of our learned full - text models .
the weights shown represent the weighted log - odds ratio of the words given the class .
the symbol d is used to represent an arbitrary digit .
for example , the top word shown for the faculty class , dddd , represents any four - digit token ( such as that occurring in a phone number ) .
training sets to calculate
for each word wi and class c .
figure 123 shows the ten words for each class that have the greatest value of this weighted log - odds ratio .
as the table illustrates , most of the highly weighted words are intuitively prototypical for their class .
the exceptions to this general - ization are mostly from the staff class , for which there is little training data , and the other class , which represents an extremely diverse set of pages .
another interesting result illustrated by this table is that many words which are conven - tionally included in stop lists123 are highly weighted by our models .
for example , the words my , me , and am are typical stop - list words but they are among the top ten words for the student class .
although these are common words , they are clearly predictive of the student class since first - person pronouns and verb conjugations do not appear frequently on pages in the other classes .
this result suggests that it is advantageous to select a vocabulary in a domain specific way ( as we did using mutual information ) , instead of using a general purpose
one approach to improving classification accuracy is to limit the predictions made by the classifiers to just those predictions in which they are most confident .
this is easily achieved with our method because the quantity scorec ( d ) calculated when classifying a page can be taken as a measure of the confidence in the classification .
by setting a minimum threshold on this confidence , we can select a point that sacrifices some coverage in order to obtain increased accuracy .
given our goal of automatically extracting knowledge base information from the web , it is desirable to begin with a high - accuracy classifier , even if we need to limit coverage to only 123% of the 123 , 123 , 123 pages available on the web .
the effect of trading off coverage for accuracy using our full - text classifiers is shown in figure 123
the horizontal axis on this plot represents coverage : the percentage of pages for a given class that are correctly classified as belonging to the class .
the vertical axis represents accuracy : the percentage of pages classified into a given class that are actually members of that class .
to understand these results , consider , for example , the class student .
as the results in table 123 show , when the classifiers predict that a page belongs to the student class they are correct 123% of the time .
the rightmost point on the student curve in the table 123 corresponds to this point .
as we raise the confidence threshold for this class , however , the accuracy of our predictions rises .
for example , at a coverage of 123% , accuracy reaches a level of 123% .
so far , we have discussed the results only for the full - text classifiers .
figures 123 and 123 show the accuracy / coverage curves for the hyperlink and title / heading classifiers , respectively .
as before , these curves show the aggregate results for all four test sets in our cross - validation
123 a stop list is a set of words that are removed from documents before they are processed by an information - retrieval or text - classification system .
there are standard stop lists which include words generally thought to convey little information about the document topic .
research .
project *
figure 123 : accuracy / coverage tradeoff for full - text classifiers .
predictions within each class are ordered according to their confidence .
each curve shows the behavior of the classifier as a threshold on this confidence is varied .
the z - axis represents the percentage of pages for a given class that are correctly classified belonging to the class .
the j / - axis represents the percentage of pages assigned to a given class that actually members of that class .
123% 123% 123%
figure 123 : accuracy / coverage tradeoff for hyperlink classifiers .
>> i 123
uuyo 123 123 123 123
student - +
123% : l k .
\ " el . .
*
\ \ " b ,
b - ^ - - b - q .
' * o n n " ^^fi , .
i123 ^ v - v - $t^ * x . . .
\ ia
" x - x \
x \ \
; < 123 123 123 123 123
figure 123 : accuracy / coverage tradeoff for title / heading classifiers .
as we discussed earlier , one of the aspects that distinguishes learning in hypertext from learning in flat - text domains is that hypertext provides multiple , somewhat independent sources of evidence for the meaning of a given piece of text .
as we hypothesized , the results in figures 123 and 123 indicate that these multiple sources of evidence can be potentially exploited to make better predictions .
consider , for example , the accuracy of the department predictions made by the hyperlink classifiers .
whereas the full - text classifiers are only 123% accurate at full coverage , the hy - perlink classifiers are 123% accurate .
moreover , the department accuracy / coverage curve for the hyperlink classifiers is uniformly superior to the curve for the full - text classifiers .
the reason for this difference in accuracy is that although our data set includes few department pages from which to generalize , it includes many hyperlinks that point to department pages .
thus the hyperlink classifiers have relatively large samples of data from which to learn the word statistics of hyperlinks that point to department pages , and similarly they have a fairly large number of hyperlinks on which to base their prediction when classifying a page after
the title / heading classifiers also illustrate cases in which using a hypertext - based repre - sentation for page classification can result in better predictive accuracy than simply using a flat - text representation .
the title / heading classifiers ' curve for both the faculty and re - search . project classes , for example , are better than the corresponding curves for the full - text classifiers at coverage levels of 123% and less .
one explanation for this result is that titles and headings provide something of a summary of a given page and thus tend to contain words that are highly predictive of the page ' s class .
first - order text classification
as noted previously , the hypertext structure of the web can be thought of as a graph in which web pages are the nodes of the graph and hyperlinks are the edges .
the methods for classifying web pages that we discussed in the previous sections consider the words in either a single node of the graph or in a set of edges impinging on the same node .
however , these methods do not allow us to learn models that take into account such features as the pattern of connectivity around a given page , or the words occurring in neighboring pages .
it might be profitable to learn , for example , a rule of the form " a page is a course home page if it contains the words textbook and ta and is linked to a page that contains the word assignment . " rules of this type , that are able to represent general characteristics of a graph , require a first - order representation .
in this section , we consider the task of learning to classify pages using a learner that is able to induce first - order rules .
the learning algorithm that we use in this section is quinlan ' s foil algorithm ( 123 , 123 ) .
foil is a greedy covering algorithm for learning function - free horn clauses123
foil induces each horn clause by beginning with an empty tail and using a hill - climbing search to add literals to the tail until the clause covers only ( mostly ) positive instances .
the evaluation function used for the hill - climbing search is an information - theoretic measure .
the representation we provide to the learning algorithm consists of the following back -
has_worc ? ( page ) : this set of relations indicate which words occur in which pages .
each boolean relation indicates the pages in which the word word occurs .
the vocabulary for this set includes stemmed123 words that have at least 123 occurrences but that do not occur in more than 123% of the training - set pages .
these two constraints were selected with the intention of assembling a vocabulary of reasonable size that would likely include the words with the most discrimination power .
we had between 123 and 123 of these predicates in each of the cross - validation runs .
link_to ( page , page ) : this relation represents the hyperlinks that interconnect the pages in the data set .
we apply foil to learn a separate set of clauses for six of the seven classes considered in the previous section123
we do not learn a description of the other class , but instead treat it as a default class .
123we use the terms clause and rule interchangeably
123stemming refers to the process of heuristically reducing words to their root form .
for example the words
compute , computers and computing would be stemmed to the root comput .
123there is a vefsion of foil specifically designed for multi - class problems such as ours .
we found , however ,
that the inductive bias of this version is not well suited to our particular task .
when classifying test instances , we calculate an associated measure of confidence along with each prediction .
we calculate these confidence values for two reasons .
first , we use them to resolve conflicting predictions from our six independently learned rule sets .
second , we are interested in measuring how the accuracy of our learned rule sets varies as we adjust
we use the following procedure to calculate the confidence of each of our predictions .
first , we estimate the error rate of each of our learned clauses by calculating an m - estimate ( 123 ) ( with m = 123 ) of the rule ' s error over the training examples .
we then use these scores to sort the clauses in order of descending accuracy . 123 to integrate the predictions of our six independently learned classifiers , we use the following procedure :
if no classifier had a rule that matched the given page , then we predict other with
if only one classifier had a matching rule , then we predict the associated class with con - fidence corresponding to the rule ' s score .
the other class is predicted with confidence of one minus this score .
if more than one classifier has a matching rule for the given example , then we predict each class with confidence equal to the score of its best matching rule divided by the total number of classifiers that had matching rules .
the other class is predicted with a confidence value that would make the total confidence sum to one .
experimental evaluation
for the experiments reported here , we used release 123 of foil with the default settings .
as with the experiments in section 123 , we use a four - fold cross - validation methodology .
the resulting accuracy / coverage plot for each class is shown in figure 123
comparing these results to those in figure 123 , one can see that although the first - order rules generally provide lower coverage than the statistical classifiers , they provide superior accuracy for several classes .
figure 123 shows three of the rules123 learned by foil in its various cross - validation runs .
the learned rule for course shown here illustrates the power of a first - order representation .
this rule classifies a page as the home page for a course if it passes three groups of tests :
the page has the word instructor , but doesn ' t have the word good .
123this change does not affect the classifications made by a learned set of clauses .
it affects only our
confidence associated with each prediction .
123throughout the paper , we use a prolog - like syntax for learned rules .
the symbol : - represents the implication operator , with the head of the rule on the left side of the operator and the body on the right side .
constants , such as the names of our ontology classes and relations , start with lowercase letters .
variables start with uppercase letters .
figure 123 : accuracy / coverage tradeoff for foil page classifiers .
the page contains a hyperlink to a page which does not contain any hyperlinks to
this linked page contains the word assign .
this rul can be summarized as follows :
predict that a page is a course home page if it has the word instructor and is linked to a leaf page which talks about assignments .
the sample rule learned for the student class comes from the cross - validation run leaving pages from the university of washington out of the training set .
notice that this rule refers to a page ( bound to the variable b ) that has two common first names on it ( paul and jame , the stemmed version of james ) .
this rule ( and similar rules learned with the other three training sets ) illustrates that foil has learned to exploit " student directory " pages in order to identify student home pages .
for example , when washington is the test set , all of the correct applications of the rule bind b to a page entitled " graduate students at uw cs&e " .
the faculty rule will not classify a page as faculty unless there is a page containing the stemmed variant of faculty that points into the given page .
all three of these rules show how web - page classification is different from ordinary text classification in that neighboring pages may provide strong evidence about the class of a given page .
learning methods which can use this information effectively should perform better than standard techniques in this domain .
student ( a ) : - not ( has . cfafa ( a ) ) , not ( has_commerc ( a ) ) , link_to ( b , a ) , has_j ' ame ( b ) , bas . paul ( b ) , not ( has_mcn123 ( b ) ) .
training set : 123 pos , 123 neg; test set : 123 pos , 123 neg
faculty ( a ) : - has_pro / essor ( a ) , has_p / ? ( a ) , link_to ( b , a ) , has_ / acw ( b ) .
training set : 123 pos , 123 neg; test set : 123 pos , 123 neg
course ( a ) : - hasjnstructor ( a ) , not ( has_#oo<i ( a ) ) , link_to ( a , b ) , not ( link_to ( b , _l ) ) , has - assign ( b ) .
training set : 123 pos , 123 neg; test set : 123 pos , 123 neg
figure 123 : a few of the rules learned by foil for classifying pages .
combining learners
the previous experiments show that the best representation for page classification depends on the class .
this observation suggests that it might be profitable to combine the predictions made by our four classifiers .
in this section , we describe and evaluate a simple approach to
the method that we employ for combining the predictions of our classifiers takes advantage of the fact that each classifier produces a measure of confidence along with each prediction .
the method we use is a simple voting scheme that uses confidence values as tie - breakers .
that is , given the predictions made by our four classifiers for a given web page , we predict the class that has a plurality of the votes made by the individual classifiers , if there is one .
if no class has a plurality , then we select the class associated with the highest confidence
in order to ensure that the confidence measures output by our different classifiers are comparable , we calibrate each classifier by inducing a mapping from its output scores to the probability of a prediction being correct .
we do this by simply binning the scores produced by each classifier for a given class , and then measuring the training - set accuracy of the scores that fall into each bin .
experimental evaluation
figure 123 shows the accuracy / coverage curves for the voting predictors .
by comparing this figure to the accuracy / coverage curves for the full - text classifiers shown in figure 123 one can see that , in general , more accurate predictions are achieved by considering evidence other than full - text when classifying pages .
at high levels of coverage , the voting classifiers are more accurate than the full - text classifiers for the course and department classes .
addi - tionally , the research . project predictions made by the voting classifier are significantly more accurate than the full - text predictions , although the coverage attained by the voting classifier is not as good .
figure 123 : accuracy / coverage tradeoff for combined classifiers with vocabulary size of 123 words .
although figure 123 indicates that predictive accuracy is helped in some cases by com - bining multiple classifiers , the results of this experiment are somewhat disappointing .
the accuracy / coverage curves for the voting classifiers are not uniformly better than the corre - sponding curves of the constituent classifiers .
ideally , we would like the accuracy / coverage curve for each class to be as good or better than the best counterpart curve among the
we believe that the results shown in figure 123 are disappointing because our method for combining the predictions of multiple classifiers is overly simple .
specifically , we believe that the method fails to accurately map classification scores to estimated accuracies , and thus the combining function often does not " listen " to the right classifiers .
interestingly , we have observed that the voting method performs much better when our statistical classifiers are limited to very small vocabularies .
figure 123 shows the accuracy / coverage curves for voting when we use statistical classifiers trained with a vocabulary size of 123 words .
in comparing this figure to our baseline full - text classifier ( figure 123 ) , one can see that the curves produced by the small - vocabulary voting method are generally superior to the full - text classifier curves .
moreover , the small - vocabulary voting classifiers achieved this result using constituent classifiers that were not as accurate as their 123 - word vocabulary counterparts .
in future work , we plan to train neural networks to perform the combining function .
we hypothesize that such a combining method will be better able to exploit the specialized areas of expertise exhibited by our individual classifiers .
123% 123% 123%
figure 123 : accuracy / coverage tradeoff for combined classifiers with vocabulary size of 123 words .
identifying multi - page segments
as discussed in section 123 , our representational assumption is that each class instance in the knowledge base corresponds to some contiguous segment of hypertext on the web .
this allows , for example , that a particular student might be represented on the web by a single web page , or by a cluster of interlinked web pages centered around their home page .
in the experiments reported thus far , we have effectively made a simpler assumption : that each instance is represented by a single web page .
in fact , in labeling our training data , we encountered a variety of students ( and instances of other ontology classes ) that were described by a several interlinked web pages rather than a single page .
in these cases we hand labeled the primary home page as student , and labeled any interlinked pages associated with the same student as other .
to remove this simplifying assumption we must develop methods for identifying sets of interlinked pages that represent a single knowledge base instance .
in this section we present a set of hand - written heuristics that identify groups of related pages and also identify the " primary " home page in the group .
we show here that classification accuracy in the previous sections is significantly improved when these heuristics are used to group pages and to automatically assign the label other to non - primary pages , to fit the assumption we made while hand labeling the data .
consider the web pages of a prototypical faculty member .
she has a main page ( http : / / www . my . edu / user / jdoe / index . html ) , a page listing her publications ( http : / / www . my .
edu / user / jdoe / pubs . html ) , and a page describing her research interests ( http : / / www .
my . edu / user / jdoe / work / research . html ) .
our working assumption about entity - web re - lationships indicates that we should recognize that these pages correspond to a single entity , identify the best representative page for that entity , classify that page as a faculty , and clas - sify the rest of the pages as other .
we accomplish this by solving two subtasks : grouping related pages together , and identifying the most representative page of a group .
spertus ( 123 ) identifies regularities in url structure and naming , and presents several heuristics for discovering page groupings and identifying representative home page .
we use a similar , slightly expanded , approach .
in an ideal scenario , one could imagine trying to learn these heuristics from examples .
in the following experiment we have instead provided these rules by hand .
the most obvious groupings that can be extracted from a url are based on direc - tory structure prefixes .
key directory components of a url indicate a logical grouping of web pages into an entity .
for example , given the url http : / / www . my . edu / user / jdoe / research . html , we can deduce the existence of an entity corresponding to the url prefix http : / / www . my . edu / user / jdoe / , because the keyword / user / in the penultimate directory position typically indicates the presence of a person entity in the directory space denoted by jdoe .
other typical penultimate prefix markers are / faculty / , / people / , / home / , and / projects / .
three ultimate prefix markers ( in unix - style globbing pattern ) are / cs ? ? ? / , / www / and / ~ * / , the first being a typical indicator of a course , and the last being a typi - cal indicator of the username of a person or organization .
our algorithm groups urls by their longest directory prefix that matches one of these given patterns .
in the event that no pattern matches , the entire directory prefix is used for the grouping .
in our example above , the three urls would each have the entity prefix as http : / / www . my . edu / user / jdoe / , and thus would be grouped together .
applying these grouping heuristics results in sets of web pages that approximate a sin - gle ontology entity .
from these sets , we identify the single primary page that is most representative of that entity .
usually this corresponds to the " home page " of the en - tity .
thus , we take any page that has the filename pattern " index . html " , " home . html " , " homepage . html " , or " cs ? ? ? . html " and label it the primary page .
additionally , any page in which the complete url is the directory prefix , ( for example , the url http : / / www . my . edu / user / jdoe / ) or one in which the filename matches the directory above it ( as in http : / / www . my . edu / user / jdoe / jdoe . html ) is also identified as a primary page .
all pages that do not match any of these patterns in a group , are classified automatically as other .
in the event that no page in a group matches any of these heuristics , the page with the highest ( non - other ) classification confidence is labeled the primary page .
in our exam - ple , http : / / www . my . edu / user / jdoe / index . html would be classified as faculty ( assuming our classifier was correct ) , and the other pages would be classified as other regardless of the
, p* 123 123 123
\ h ' 123 \ h ^
\ ~ +\ course -
i<^^^ ~ *^ x . \ \ student - + - - .
. b - b x\ * faculty - q -
' et ' ' \ ^^ - research . project *
' ba ' \^123^ department - * ' " " ej - .
\ . b ^^ staff - *
. x " " ' " " x x\ \
v ' ' x \ x * \ \
i i i i
123% 123% 123%
figure 123 : accuracy / coverage tradeoff for the full - text classifier after the application of url heuristics .
experimental evaluation
the impact of using the url heuristics with the original full - text page classifier is summa - rized in figure 123
comparing these curves to figure 123 one can see the striking increase in accuracy for any given level of coverage across all classes .
also note some degradation in total coverage .
this occurs because some pages that were previously correctly classified have been misidentified as being " secondary " pages
section summary
this section focused on the task of recognizing class instances by web page classification .
we showed that , because hypertext provides much redundant information , web pages can be classified using several sources of information : the full text of pages , the text in titles and headings , the text associated with hyperlinks , text in neighboring pages , and the file organization represented in urls .
our experiments suggest that none of these approaches alone is sufficient for recognizing instances of ontology classes with high accuracy .
in the experiments described in section 123 , we used both full - text classifiers and url heuristics .
we also showed in this section that one promising line of research is to combine the predictions of multiple classifiers that use different sources of evidence .
learning to recognize relation instances
in the previous section we discussed the task of learning to extract instances of ontology classes from the web .
our approach to this task assumed that the class instances of interest are represented by whole web pages or by clusters of web pages .
in this section , we dis - cuss the task of learning to recognize relations of interest that exist among extracted class instances .
the hypothesis underlying our approach is that relations among class instances are often represented by hyperlink paths in the web .
thus , the task of learning to recognize instances of such relations involves inducing rules that characterize the prototypical paths of the relation .
for example , an instance of the instructors . of relation might be represented by a hyperlink directly from the home page of a course to the home page of the instructor , as described by the following rule :
instructors_of ( a , b ) : - course ( a ) , person ( b ) , link_to ( a , b ) .
here , the variables a and b represent web pages , the literals course ( b ) and person ( a ) rep - resent the predicted classifications of the pages , and the literal link_to ( a , b ) tests for the existence of a hyperlink from page a to page b .
problem representation
because this task involves discovering hyperlink paths of unknown and variable size , we employ a learning method that uses a first - order representation for its learned rules .
specif - ically , the algorithm we have developed for this task is based on the foil algorithm ( 123 , 123 ) which we used for page classification in section 123 .
we discuss our algorithm in more detail
the problem representation we use for our relation learning tasks consists of the following
class ( page ) : for each class in the set of page classes considered in section 123 , the class relation lists the pages that represent instances of class .
for pages in the training set , the instances of these relations are determined using the actual classes of the pages .
for pages in the test set , however , we use the predicted page classes given by the classifiers discussed in section 123
since the webkb system has access only to predicted page classes , our test set conditions are representative of those the system faces .
link_to ( hyperlink , page , page ) : this relation represents web hyperlinks .
for a given hyperlink , the first argument of the relation specifies an identifier for the hyperlink , the second argument specifies the page in which the hyperlink is located , and the third argument indicates the page to which the hyperlink points .
has_w;or^ ( hyperlink ) : this set of relations indicates the words that are found in the anchor ( i . e . , underlined ) text of each hyperlink .
the vocabulary for this set of relations includes words that occur at least n times ( we set n = 123 in our experiments ) in the hyperlinks of the training set .
note that whereas the has . word relations used in section 123 describes web pages , the set used here characterizes hyperlinks .
all_words_capitalized ( hyperlink ) : the instances of this relation are those hyperlinks in
which all of the words in the anchor text start with a capital letter .
has_alphanumeric_word ( hyperlink ) : the instances of this relation are those hyperlinks which contain a word with both alphabetic and numeric characters ( e . g . , i teach cs123 ) .
has_neighborhood_;orrf ( hyperlink ) : this set of relations indicates the words that are found in the " neighborhood " of each hyperlink .
the neighborhood of a hyperlink includes words in a single paragraph , list item , table entry , title or heading in which the hyperlink is contained .
the vocabulary for this set of relations includes the top 123 most frequently occurring words in each training set , except for words on a stoplist .
we learn definitions for the following target relations from the data set described in section 123 : members_of_project ( page , page ) , instructorsjof_course ( page , page ) , and depart - ment_of_person ( page , page ) .
in addition to the positive instances for these relations , our training sets include approximately 123 , 123 negative examples .
we form the set of nega - tive training instances for each target relation by enumerating each pair of non - other pages from the same university that is not a positive instance of the target relation .
addition - ally , for the department . of . person relation we augment the negative instances with each person - department pair which is not a positive instance .
learning methods
as stated above , the algorithm we use for learning relation rules is similar to foil in that it uses a greedy covering approach to learn a set of horn clauses .
the two primary differences between our method and foil are twofold .
first , unlike foil our method does not simply use hill - climbing when searching for the next clause to add to a concept definition .
second , our method uses a different evaluation function for this search process .
we discuss each of these differences in turn .
as described in section 123 , foil constructs clauses using a hill - climbing search through a space of candidate literals .
we have found that , for our relation : learning tasks , such a hill - climbing strategy is unable to learn rules for paths consisting of more than one hyperlink .
the search process that our method employs instead consists of two phases .
in the first phase , the " path " part of the clause is learned , and in the second phase , additional literals are added to the clause using a hill - climbing search .
our algorithm for constructing the path part of a clause is a variant of richards and mooney ' s relational pathfinding method ( 123 ) .
this method is designed to alleviate the basic weakness of hill - climbing search , namely that to learn good definitions it is often necessary
input : training set of negative and uncovered positive instances
for each uncovered positive instance
find a path ( up to bounded length ) using the background relations
select the most common path prototype for which clause search hasn ' t yet failed generalize the path into an initial clause do hill - climbing to refine the clause if hill - climbing fails to find an acceptable clause , backtrack to step 123
return : learned clause
figure 123 : the procedure for learning a clause in our deterministic variant of relational pathfinding .
expand subgraphs return path
figure 123 : finding a path in the background relations .
on the left is shown a graph of constants linked by a single binary relation .
this graph can be thought of as representing web pages connected by hyperlinks .
suppose the pair ( p123t p123 ) is an uncovered positive instance .
pathfinding proceeds by expanding the subgraphs around the two constants until an intersection is detected , and then returning the path that links the two
to take a step in the search space which does not exhibit any immediate gain .
the basic idea underlying relational pathfinding is that a relational problem domain can be thought of as a directed graph in which the nodes are the domain ' s constants and the edges correspond to relations which hold among constants .
the relational - pathfinding algorithm tries to find a small number of prototypical paths in this graph that characterize the instances of the
figure 123 provides an overview of our pathfinding procedure for learning a single clause .
this procedure is iterated until a complete definition has been learned .
the first step in the method is to find the shortest path of a bounded length ( when one exists ) for each positive instance ( of the target relation ) that has not been covered by a previously learned clause .
this process , illustrated in figure 123 involves expanding a subgraph around each of the constants in the instance .
each subgraph is expanded by finding all constants which can be reached using an instance of one of the background relations to connect to a constant at the frontier of the subgraph .
after finding such a path for each uncovered positive instance , the most common path
find path for each positive instance
return most common path
pi i p123 i tim
figure 123 : finding the most common path for a set of positive instances .
given the graph shown in figure 123 , suppose that the positive instances are ( pi , p123 ) , ( p123 , p123 ) , ( p123 , p123 ) , and ( p123 , p123 ) .
our algorithm finds the shortest path for each instance and then returns the most common path prototype .
in this example the first three instances have the same path prototype , whereas the instance ( p123 , p123 ) has different one ( notice the direction of the hyperlinks ) .
this path prototype is converted into an initial clause .
prototype is used for the initial clause . 123 a path prototype specifies the number of hyperlinks in the path and their directions , but it does not reference the particular pages and hyperlinks in any particular instance .
the notion of the most common path prototype is illustrated in figure 123
the initial clause is formed by replacing each constant in the path with a unique variable .
this clause is then further refined by a simple hill - climbing search , such as that used in foil .
if the hill - climbing search fails to find an acceptable clause , then the procedure backtracks by removing the last selected path prototype from the list of candidates and then trying the next most common prototype .
we further bias the search for clauses by initializing each one with the classes of the pair of pages in the relation .
for example , when learning clauses for the target relation mem - bers_of_project ( a , b ) , we initialize the tail of each clause with the literal research_project ( a ) and person ( b ) .
this bias takes advantage of domain knowledge which is present in the on - tology given to the webkb system .
the second difference between our relation - learning algorithm and foil is that whereas foil uses an information - theoretic measure to guide its hill - climbing search , our method , like dzeroski and bratko ' s ra - foil ( 123 ) , uses ra - estimates of a clause ' s error to guide its construction .
we have found that using this evaluation function causes the algorithm to learn fewer , more general clauses than when foil ' s information gain measure is used .
experimental evaluation
we evaluate our approach to learning relation rules using the four - fold cross - validation methodology described in section 123
on each iteration , we learn the target relations us -
123if the method is constrained from learning recursive definitions , the path for each positive instance needs to be found only once since it will not change as clauses are added for the target relation .
in this case , before learning each new clause the algorithm needs only to update counts indicating the number of instances covered by each path prototype .
instructors_of ( a , b ) : - course ( a ) , person ( b ) , link_to ( c , b , a ) .
test set : 123 pos , 123 neg
department_of ( a , b ) : - person ( a ) , department ( b ) , link_to ( c , d , a ) , link_to ( e , f , d ) , link_to ( g , b , f ) ,
test set : 123 pos , 123 neg
members_of_project ( a , b ) : - research_project ( a ) , person ( b ) , link_to ( c , a , d ) , link . to ( e , d , b ) ,
test set : 123 pos , 123 neg
figure 123 : a few of the rules learned for recognizing relation instances .
ing training instances from three of the universities in our data set , and test learned clauses using instances from the fourth university .
figure 123 shows a learned clause for each of the instructors . of , department . of , and mem - bers .
project relations .
on average , there were 123 , 123 , and 123 clauses learned for these target concepts respectively .
along with each rule , we show how well the rule classified test - set instances .
each of these rules was learned on more than one of the training sets , therefore the test - set statistics represent aggregates over the four test sets .
the rules learned for the instructors . of relation are the simplest among the three target relations .
the learned rule shown for this relation , for example , looks for cases in which a course page has a hyperlink pointing to a person page .
the rule shown for the mem - bers . of . project relation is more interesting .
it describes members . of . project instances in which the project ' s home page points to an intermediate page which points to personal home pages .
the hyperlink from the project page to the intermediate page must have the word " people " near it .
this rule covers cases in which the members of a research project are listed on a subsidiary " members " page instead of on the home page of the project .
the rule shown for the department . of relation involves a three - hyperlink path that links a department home page to a personal home page .
the rule requires that the word " graduate " occur near the second hyperlink in the path .
in this case , the algorithm has learned to exploit the fact that departments often have a page that serves as a graduate student directory , and that any student whose home page is pointed to by this directory is a member of the department .
along with each of our predicted relation instances , we calculate an associated confidence in the prediction .
we can then vary the coverage of our learned rule sets by varying a threshold on these confidence values .
we calculate the confidence of each prediction by considering where most of the uncertainty in the prediction lies : in the page classifications that are tested by each learned clause .
the confidence measure for a predicted relation instance is simply the product of the confidence measures for the page classifications that factor into the relation prediction .
123% 123% 123% 123% 123%
figure 123 : accuracy / coverage tradeoff for learned relation rules .
using these confidence measures , figure 123 shows the test - set accuracy / coverage curves for the three target relations .
the accuracy levels of all three rule sets are fairly high .
the members . of . project rules are better than 123% accurate at coverage levels of up to about 123% .
the instructors . of rules are over 123% accurate at coverage levels of 123% and above .
the department . of rules are at least 123% accurate at coverage levels of up to 123% .
the limited coverage levels of the learned rules is due primarily to the limited coverage of our page classifiers .
note that all of the learned rules include literals which test predicted page classifications .
as figure 123 shows , the coverage exhibited by our page classifiers is below
% for most classes .
learning to extract text fields
in some cases , the information we want to extract will not be represented by web pages or relations among pages , but by small fragments of text embedded in pages .
for example , given a personal home page , we might be interested in extracting the person ' s name .
this type of task is commonly called information extraction .
this section discusses our approach to learning rules for such information extraction tasks .
we have developed an information extraction learning algorithm called srv for " sequence rules with validation . " srv is a top - down first - order learner in the spirit of foil
shares with foil its general search heuristic and gain metric , but differs in the kind of input it expects and the comprehensiveness of its search .
input to srv is a set of pages , labeled to identify instances of the field we want to extract , and a set of attributes defined over tokens .
output is a set of information extraction rules .
the extraction process involves examining every possible text fragment of appropriate size to see whether it matches any of the rules .
as in foil , " growing " a rule in srv means hill - climbing through a space of possible literals , at each step adding a literal that matches as many positive examples as possible while excluding a large number of previously covered negative examples .
when a rule is deemed good enough ( either it covers only positive examples , or further specialization is judged to be unproductive ) , all positive examples matching it are removed from the training set , and the process is repeated .
in our particular domain , a positive example is a labeled text fragmenta sequence of tokensin one of our training documents; a negative example is any unlabeled token sequence having the same size as some positive example .
during training we assess the goodness of a literal using all such negative examples .
the representation used by our rule learner attempts to express the salient characteristics of positive examples mainly in terms of the individual tokens contained within them and surrounding them .
at each step in rule growth , several different types of predicate may be
length ( sequence , relop , l\l ) : the learner can assert that the length of a field , in terms
of number of tokens , is less than , greater than , or equal to some integer .
some ( sequence , var , path , attr , value ) : the learner can posit an attribute - value test for some token in the sequence ( e . g . , " the field contains some token that is capitalized " ) .
one argument to this predicate is a variable .
each such variable binds to a distinct token .
thus , if the learner uses a variable already in use in the current rule , it is specializing the description of a single token; if the variable is a new one , it describes a previously unbound token .
position ( sequence , var , from , relop , n ) : the learner can say something about the position of a token bound by a some - predicate in the current rule .
the position is specified relative to the beginning or end of the sequence .
relpos ( sequence , varl , var123 , relop , n ) : where at least two variables have been intro - duced by some - predicates in the current rule , the learner can specify their ordering and distance from each other .
like foil , srv can exploit relational structure in the domain .
for srv , the only possible form of relational structure is that relating tokens to each other .
the most obvious example is the successor relation , which connects adjacent tokens , but more interesting kinds of structure can be exploited , such as syntactic structure .
in asserting a some predicate , the learner has the option of adding an arbitrary path of relational attributes to the test , so that it can make statements of the form , " some token which is followed by a token which is followed by a token that is capitalized . " thus , although some predicates only refer to tokens
ownername ( fragment ) : - some ( fragment , b , ( ) , irutitle , true ) ,
length ( fragment , < , 123 ) , some ( fragment , b , ( prev_token ) , word , " gmt " ) , some ( fragment , a , ( ) , longp , true ) , some ( fragment , b , ( ) , word , unknown ) , some ( fragment , b , ( ) , quadrupletonp , false )
figure 123 : an extraction rule for name of home page owner .
the english rendering of this rule is , " a sequence of two tokens , one of which ( a ) is in a html title field and longer than four characters , the other of which ( b ) is preceded by the token gmt , is unknown from training , and is not a four - character token . " this is a high - accuracy rule , achieving 123 correct out of 123 matched on a validation set .
inside a field , the learner can exploit information available in text surrounding the field , as well .
initially , the learner may only use paths of length one; whenever such a path is used in a some predicate , the system makes longer paths available .
in this way , the computational expense that this facility entails is kept under control .
experimental evaluation
as in the previous experiments , we followed the leave - one - university - out methodology , re - peatedly holding the pages belonging to one of the four universities out for testing and training on the remaining three .
the data set for the present experiment consists of all person pages in the data set .
the unit of measurement in this experiment is an individual page .
if srv ' s most confident prediction on a page corresponds exactly to some instance of the page owner ' s name , or if it makes no prediction for a page containing no name , its behavior is counted as correct .
otherwise , it is counted as an error .
last - modified : wednesday , 123 - jun - 123 123 : 123 : 123 gmt
<title> bruce randall donald< / title>
bruce randall donald<br>
figure 123 : an example html fragment which the above rule matches .
in this case , the fragment bruce randall in the title is extracted .
note that this is an erroneous prediction since it misses the last name of
figures 123 and 123 show a learned rule and its application to a test case .
figure 123 shows the accuracy - coverage curve for srv on the name - extraction task .
under the criteria described above , it achieves 123% accuracy when all pages are processed .
a full 123% of the files did not contain their owners ' names , however , and a large part of the learner ' s error is because of spurious predictions over these files .
if we consider only the pages containing names , srv ' s performance is 123% .
figure 123 : accuracy / coverage tradeoff using srv for name extraction .
a prediction on a file that does not contain a name is counted as an error .
related work
there are several significant bodies of research that are related to the tasks and methods discussed in this paper .
in this section we briefly review the main areas of related work .
document classification
our work is related to research in document classification , such as that reported at recent text retrieval conferences ( trec ) ( 123 , 123 ) .
a wide variety of methods have been applied to the document - classification task .
the tfidf approach to information retrieval is the basis for the rocchio classification .
algorithm which has become a standard baseline algorithm for text classification ( 123 , 123 , 123 ) .
its " word - vector " approach involves describing classes with a vector of weights , where each weight indicates how important the corresponding word is to the class .
this representation has been used with many different learning algorithms , including memory based reason - ing ( 123 ) , neural networks ( 123 , 123 ) , linear discriminant analysis ( 123 ) , logistic regression ( 123 ) , widrow - hoff and the exponentiated gradient ( eg ) algorithm ( 123 ) .
another useful line of research in text classification comes from basic ideas in probability and information theory .
bayes rule has been the starting point for a number of classification algorithms ( 123 , 123 , 123 , 123 , 123 , 123 ) , and the minimum description length principle has been used as the basis of an algorithm as well ( 123 ) .
another line of research has been to use symbolic learning methods for text classification .
numerous studies have used algorithms such as decision trees , swap - 123 , ripper and charade can be found in ( 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 ) .
these studies indicate that these algorithms are quite competitive with statistical - based methods .
information extraction
the problem that we are addressing is related to the traditional information extraction task , such as the research done in the message understanding ( muc ) ( 123 , 123 ) community .
the work in the muc community has considered problems such as extracting symbolic descriptions of terrorist attacks from news articles , constructing case frames that indicate fields such as the perpetrator , victim , etc .
one key difference between this work and the research reported here is that we are concerned with extracting information from hypertext , whereas the muc work has focused on ordinary flat text .
in addition , our approach relies heavily on machine learning methods that can be trained to extract information , whereas most early work in the muc community relied on hand - crafted methods for extracting information .
recently , the problem of using machine - learning methods to induce information - extraction routines has received more attention .
palka ( 123 ) and autoslog ( 123 ) are machine learning systems which learn extraction patterns from collections of parsed documents that have been annotated to identify fragments of interest .
these patterns are then reviewed and manu - ally installed into a larger information extraction system .
autoslog - ts ( 123 ) removes the requirement that documents be annotated .
crystal ( 123 ) and rapier ( 123 ) both demonstrate that machine learning techniques can be used to learn rules that perform extraction autonomously .
crystal is a covering algorithm which takes parsed , annotated sentences as input and produces rules for extracting from novel sentences .
rapier uses ideas from relational learning and relaxes somewhat the reliance on syntactic pre - processing .
starting with maximally specific extraction patterns , both systems learn by dropping constraints and merging patterns .
this contrasts with the general - to - specific approach introduced here .
several researchers have explored the problem of text extraction from the web and other internet sources .
one example is ila ( 123 ) , a system designed to learn extraction patterns over the human - readable output of online databases .
ila exploits prior expectations about the probable contents of a database to learn how records are formatted for output by a particular system .
similar , but specifically designed for use with html is shopbot ( 123 ) , a bargain hunting agent .
these approaches are related to the general problem of " wrapper induction " ( 123 ) , learning extraction patterns for highly regular sources .
at the same time , ideas that have proven useful for general text have also been shown to work well for web pages .
webfoot ( 123 ) is a modification of crystal in which parsed sentence fragments are replaced by segments of html .
extracting semantic information from hypertext
several other research groups have considered the semantic information that can be auto - matically inferred and extracted from hypertext .
spertus ( 123 ) presents a set of heuristics that relate hypertext conventions to semantic relationships .
specifically , she considers rela - tionships that can often be inferred from hyperlink structure , file system organization , and html page structure .
monge and elkan ( 123 ) have developed a system that finds the web page for a paper given a bibliographic citation to it .
part of the task performed by this system is to find the personal home page and the publications page of an author starting from the home page of the person ' s institution .
for this task , monge and elkan use search - control rules which are somewhat similar to the relation - recognition rules we learned in section 123
their rules look for certain keywords in hyperlinks to decide which ones to follow in the search .
whereas their rules are hand - coded for a specific task , our work considers the problem of learning such rules for arbitrary relations .
pirolli et al .
( 123 ) consider the task of classifying pages into functional categories such as head , index and reference .
they characterize the classes using features such as file size , number of incoming and outgoing hyperlinks , average depth of children pages in the hyper - link graph , etc .
whereas our work has not directly involved learning functional classes of pages , we have observed that our first - order learners for both page and relation classifica - tion often implicitly learn such functional categories .
recall , for example , that our learned first - order rules for recognizing student pages prominently exploited the class of person index pages .
the features we use also differ somewhat from those of pirolli et al . , but common to both approaches is the central importance of vector - based text similarity and hyperlink
extracting knowledge bases from the web
other groups have worked on extracting propositional knowledge - base information from the web .
luke et al .
( 123 ) have proposed an extension to html called shoe whereby web page authors can encode ontological information on their pages .
the have also developed a system , expose , that extracts shoe - encoded information from web pages , and stored it in a local knowledge base .
their hope is that a library of standard ontologies will come into common usage , enabling agents such as expose to learn the information encoded on the
the start information server ( 123 ) provides a natural language interface to a knowl - edge base collected from the web .
the knowledge base contains meta - information about the content of the web , so that a query to start returns relevant hypertext segments .
start builds its knowledge base by discovering mostly manually added natural language annotations on web pages .
the most significant recent development in this area is the advent of extensible markup
language ( xml ) ( 123 ) .
whereas html is designed to describe the layout of information in a page , xml can be used to describe information about the contents of the page .
as with shoe , web page authors can use xml to encode ontological information about their pages .
since xml is a world wide web consortium standard , however , it is sure to be widely used .
we believe that methods for annotating the contents of web pages , such as shoe and xml , can assist with the task of extracting knowledge bases from the web , but do not obviate the need for our webkb approach .
there are two notable limitations of approaches such as shoe and xml .
first , they are of no use when web page authors do not employ them .
second , they presuppose a universal ontology .
that is , since individual web page authors are responsible for annotating web pages , the success of these approaches hinges on the extent to which authors employ standard , shared ontologies in a consistent manner .
moreover , ontological decisions are largely in the hands of web page authors in this approach .
there may be cases where the ontological categories used to describe a given web page are not appropriate or relevant categories for the tasks to which an extracted knowledge base will be applied .
in the webkb approach , on the other hand , these ontological decisions can be made by the users of the system .
one interesting way in the xml and webkb approaches can potentially be combined , is by exploiting xml - annotated pages as pre - labeled training data .
we plan to explore this issue in future research .
web agents
the webkb system described here is an example of a web agent that browses the web , extracting information as it goes .
many other web agents have been developed over the past few years , including several that involve some form of learning .
however , the vast majority of these systems use learning to improve their ability to retrieve text information , rather that to extract computer - understandable information .
for example , joachims et al .
( 123 ) describe a web agent called web watcher that serves as a tour guide for users browsing the web .
web watcher learns to suggest appropriate hyperlinks given users ' interests , based on the hyperlinks followed by previous users with similar interests .
as such , it involves learning to classify hyperlinks - a task similar to the work reported here on learning to extract relational information .
a system with a similar goal is letizia ( 123 ) , which learns the interests of a single user , in contrast to web watcher which learns from a community of users .
syskill and webert ( 123 ) offers a more restricted way of browsing than web watcher and letizia .
starting from a manually constructed index page for a particular topic , the user can rate hyperlinks off this page .
the system uses the ratings to learn a user specific topic profile that can be used to suggest unexplored hyperlinks on the page .
syskill and webert can also use search engines like lycos to retrieve pages by turning the topic profile into a query .
lira ( 123 ) works in an off - line setting .
a general model of one user ' s interest is learned by asking the user to rate pages .
lira uses the model to browse the web off - line and returns a set of pages that match the user ' s interest .
one related system that is closer in spirit to our work is shakes et a / . ' s ( 123 ) ahoy system , which attempts to locate the home page of a person , given information such as the person ' s name , organizational affiliation etc .
ahoy uses knowledge of home page placement conventions to search for personal home pages , and in fact learns these conventions from experience .
conclusions and future work
we began this paper with a goal and a thesis .
the goal is to automatically create a large knowledge base whose content mirrors that of the world wide .
the thesis is that one can automatically create knowledge bases from the web by first using machine learning algo - rithms to create information extraction methods for each of the desired types of knowledge , and then applying these methods to extract probabilistic , symbolic statements directly from
this paper provides support for our thesis by proposing and testing a variety of machine learning algorithms for information extraction , and by describing the webkb system that incorporates the learned information extractors to browse web sites and populate a knowl - edge base .
as shown in section 123 and elsewhere , our system has achieved an accuracy of better than 123% at coverage levels of approximately 123% when using these learned infor - mation extractors to populate its university knowledge base while browsing new web sites .
these results provide encouraging initial support for our thesis , and suggest many routes for
we have explored a variety of learning methods for this task , including statistical bag - of - words classifiers , first - order rule learners , and multi - strategy learning methods .
we have found that statistical bag of words methods , derived from document classification methods in information retrieval , work well for classifying individual web pages .
however , these methods do not take advantage of the special hypertext structure available on the web .
therefore , we developed first - order learning algorithms both for learning to classify pages and learning to recognize relations among several pages .
these first - order methods are capable of describing patterns that occur across multiple web pages , their hyperlinks , and specific words that appear on these pages and hyperlinks .
our experiments indicate that these methods tend to have higher accuracy than the bag of words classifiers , though they frequently provide lower coverage .
in addition to these first - order learning methods that " look outward " from the page to consider its neighbors , we also have developed methods that " look inward " to consider the detailed structure of hypertext and specific text fragments within a single web page .
the srv algorithm described in section 123 learns relational rules that extract specific types of text fields within a web page , such as a person ' s name .
we believe that the toolbox of methods we have described here will be applicable to a wide range of problem domains .
for new domains , however , we may apply and combine the methods in ways not explored in this paper .
for example , in current work in a new problem domain , we are using page classifiers to recognize instances of a particular relation ( the economic sector of a company ) , whereas in the work described here we used page classifiers to recognize class instances .
in short , the most appropriate method for recognizing instances for a particular class or relation will depend on how these instances tend to be represented in the web .
based on the initial results reported here , we are optimistic about the future prospects for automatically constructing and maintaining a symbolic knowledge base by interpreting hypertext on the web .
key questions remain , however .
for example , what level of accuracy
can be achieved by learned procedures for extracting information from the web , and what level of accuracy will be required of them ? for some tasks the required accuracy will be quite high ( e . g . , for an intelligent system that automatically invests money on behalf of its user ) .
however , for tasks such as information retrieval on the web , the system need only be sufficiently accurate to outperform the current keyword - based retrieval systems that have no real notion of an ontology .
although further research toward stronger learning methods is warranted , we conjecture that there will be a steady stream of applications where even an approximately correct knowledge base will outperform current keyword retrieval methods .
a second type of question for our system is how much effort will be required to train the system for each new ontology , or for each extension to the growing ontology ? in the experiments reported here , the system was trained using thousands of hand - labeled web pages that were collected at a cost of approximately one or two per son - weeks of effort .
in newer work we are beginning to explore methods for reducing the dependence on hand labeled data .
below is a list of these and other research opportunities that merit further research :
develop learning methods that exploit the hierarchical relationships that exist among classes in the hierarchy .
for example , in recent work we have shown that the accuracy of our bayesian bag of words classifier can be improved by using the class hierarchy to obtain more accurate estimates of class conditional word probabilities ( 123 ) .
use the vast pool of unlabeled web pages to supplement the available hand - labeled data to improve learning accuracy .
recently we have shown that the em algorithm can be used to combine labeled and unlabeled data to boost accuracy ( 123 ) .
we are also exploring the combination of em with pool - based training for active learning in which the learner requests labels for specific web pages whose label will be especially
co - training multiple classifiers .
for example , consider a problem setting in which one web page classifier examines the words on the page , and a second classifier examines instead the words on the incoming hyperlinks to that page .
in recent work , we have proposed a method by which each classifier acts as a trainer for the other , and we have provided initial experiments and theoretical analysis showing the promise of this
exploit more linguistic structure .
we plan to explore ways in which noun , verb , and prepositional phrases extracted from the text can be used as features for information extraction .
we have conducted preliminary experiments that show improved accuracy in some cases when our bag of words representation is augmented by these extracted phrases ( 123 ) .
we conjecture that such linguistic features will be even more useful for tasks with few words , such as classifying individual hyperlinks .
explore multiple strategies for learning to extract text fields from web pages .
we have developed a number of approaches to this task ( 123 , 123 , 123 ) , including multi - strategy
integrate statistical bag - of - words methods into first - order learning tasks .
we have begun developing methods that augment first - order learning with the ability to use bag -
of - words classifiers to invent new predicates for characterizing the pages and hyperlinks referenced in learned rules ( 123 ) .
exploit more html structure .
we plan to investigate the utility of representing the html structure of pages when learning rules for relation classification and information extraction .
we have investigated one approach to representing html structure and exploiting it for learning tasks ( 123 ) .
learn regularities over the growing knowledge base .
we plan to use learning methods to discover interesting regularities over the facts that have been extracted from the web , and to use these learned facts to improve future fact extraction .
for example , in the university knowledge base we might expect to learn how to predict the department of a faculty member based on the department of her student advisees .
extend the ontology to new problem domains .
we are currently applying our methods to the task of extracting information about companies from the web .
a obtaining more evenly distributed scores from naive bayes
while naive bayes often provides accurate classifications , it presents problems when one wants to interpret the score for each class as an estimate of uncertainty .
per - class scores for the winning class tend to gravitate toward 123 and scores for the losing class tend toward 123 .
often the effect is so strong that floating - point round - off error causes the probability to be calculated as exactly 123 for the winning class and 123 for the others .
these extreme values are an artifact of the independence assumption .
if for each word , the value of pr ( u;|c ) between different classes differs by one order of magnitude , then the final probabilities with differ by as many orders of magnitude as there are words in the document .
class - conditional word probabilities would be much more similar across classes if word dependencies were taken
we would like scores that accurately reflect the uncertainty in each prediction and enable us to sensibly compare the scores of multiple documents .
we attempt to counter the extreme values , while still avoiding the complexity of modeling word - dependencies , in two steps .
first , instead of using the product of the word likelihoods , we use the geometric mean of the likelihoods .
this approach is closely related to the concept of perplexity in language modeling for speech recognition ( 123 ) .
perplexity is a measure of the likelihood of some data given a model , where the likelihood is normalized for the length of the data .
we begin with naive bayes ( eq .
123 ) , rewrite the sum to an equivalent expression that sums over all words in the vocabulary t instead of just the words in the document ( eq .
123 ) , take the log , ( eq .
123 ) , and divide by the number of words in the document ( eq .
this results in the log of the geometric mean of the word likelihoods , plus a term for the class prior .
pr ( c ) nprk - |c ) = pr ( c ) h prk ' k ) ^*^ ( 123 )
oc log ( ? i ( c ) ) + j123n ( w^d ) log ( pr ( u; , - |c ) ) ( 123 )
+ ^^log ( prmc ) ) ( 123 )
log ( pr ( c ) ) , tn ( wi , d )
if we interpret n ( wi , d ) / n as pr ( wi\d ) , the right - hand term of this expression is the negative cross entropy ( 123 ) between the distribution of words induced by the document with the distribution of words induced by the class :
log ( pr ( c ) ) +^pr ( u> . |<j ( ) ^ ( pr^ic ) )
thus , the second term specifies that the class c with the highest score will be the one with the lowest cross entropythe class that could " compress " the document most efficiently .
this expression results in scores for each class that vary smoothly , without tendencies toward
cross entropy in equation 123 can be intuitively understood as the average number of bits necessary to encode a word from the document using an encoding that is optimal for the distribution of words independently drawn from the class .
cross entropy does not , however , account for the varying difficulty of encoding different documentssome documents are more complex , and inherently require more bits on average to encode .
we want scores that can be sensibly compared between documents .
a way to account for differences between documents is to use kulback - leibler divergencethat is , to subtract the average number of bits it would take to encode the document using its optimal encoding ( assuming again , that the words are independent of one another ) .
this results in an expression that can be intuitively understood as the average number of extra bits required because we are using a suboptimal encoding instead of the optimal encoding .
we modify the second term of equation 123 so that it expresses the kl divergence score for each class :
m^ + f;prk . wlog ( |l - j| )
n i=123 \pt ( wi\d ) j
we also normalize the scores across all classes so that they sum to a constant .
this nor - malization also has the effect of increasing our confidence in the classification of documents with high word entropy .
this is intuitively desirable because high - entropy documents have more unique words , which can be considered as stronger evidence , and more likely to result in a correct classification .
note that the modifications to 123 do not change the ordering of class estimates for a given document .
consequently , the classifications made by naive bayes are not affected .
these modifications only serve to provide well - distributed , comparable scores .
note that none of the changes since straightforward naive bayes in equation 123 has changed the scored ordering of different classes for the same documentthey have not changed classification that would have resulted from naive bayes .
they have only served to provide well - distributed , comparable scores .
