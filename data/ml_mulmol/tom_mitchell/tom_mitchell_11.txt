abstract .
over the past decade , functional magnetic resonance imaging ( fmri ) has emerged as a powerful new instrument to collect vast quantities of data about activity in the human brain .
a typical fmri experiment can produce a three - dimensional image related to the human subjects brain activity every half second , at a spatial resolution of a few millimeters .
as in other modern empirical sciences , this new instrumentation has led to a ood of new data , and a corresponding need for new data analysis methods .
we describe recent research applying machine learning methods to the problem of classifying the cognitive state of a human subject based on frmi data observed over a single time interval .
in particular , we present case studies in which we have successfully trained classiers to distinguish cognitive states such as ( 123 ) whether the human subject is looking at a picture or a sentence , ( 123 ) whether the subject is reading an ambiguous or non - ambiguous sentence , and ( 123 ) whether the word the subject is viewing is a word describing food , people , buildings , etc .
this learning problem provides an interesting case study of classier learning from extremely high dimensional ( 123 features ) , extremely sparse ( tens of training examples ) , noisy data .
this paper summarizes the results obtained in these three case studies , as well as lessons learned about how to successfully apply machine learning methods to train classiers in such settings .
selection , bayesian classier , support vector machine , nearest neighbor , brain image analysis
scientic data analysis , functional magnetic resonance imaging , high dimensional data , feature
the study of human brain function has received a tremendous boost in recent years from the advent of functional magnetic resonance imaging ( fmri ) , a brain imaging method that dramatically improves our ability to observe correlates of neural brain activity in human subjects at high spatial resolution ( several millimeters ) , across the entire brain .
this fmri technology offers the promise of revolutionary new approaches to studying human cognitive processes , provided we can develop appropriate data analysis methods to make sense of this huge volume of data .
a twenty - minute fmri session with a single human subject produces
mitchell et al .
a series of three dimensional brain images each containing approximately 123 , 123 voxels , collected once per second , yielding tens of millions of data observations .
since its advent , fmri has been used to conduct hundreds of studies that identify specic regions of the brain that are activated on average when a human performs a particular cognitive function ( e . g . , reading , mental imagery ) .
the vast majority of this published work reports descriptive statistics of brain activity , calculated by averaging together fmri data collected over multiple time intervals , in which the subject responds to repeated stimuli of some type ( e . g . , reading a variety of words ) .
in this paper we consider a different goal : training machine learning classiers to au - tomatically decode the subjects cognitive state at a single time instant or interval .
the goal here is to make it possible to detect transient cognitive states , rather than characterize activity averaged over many episodes .
this capability would clearly be useful in tracking the hidden cognitive states of a subject performing a single , specic task .
such classier learning approaches are also potentially applicable to medical diagnosis problems which are often cast as classication problems , such as diagnosing alzheimers disease .
while the approaches we discuss here are still in their infancy , and the more traditional approach of reporting descriptive statistics continues to dominate fmri research , this alternative data analysis approach based on machine learning has already begun to gain acceptance within the neuroscience and medical informatics research communities ( e . g . , strother et al . , 123; cox & savoy , 123; mitchell et al . , 123 ) .
this problem domain is also quite interesting from the perspective of machine learning , because it provides a case study of classier learning from extremely high dimensional , sparse , and noisy data .
in our case studies we encounter problems where the examples are described by 123 , 123 features , and where we have less than a dozen , very noisy , training examples per class .
although conventional wisdom might suggest classier learning would be impossible in such extreme settings , in fact we have found it is possible in this problem domain , by design of appropriate feature selection , feature abstraction and classier training methods tuned to these problem characteristics .
in this paper we rst provide a brief introduction to fmri , then describe several fmri data sets we have analyzed , the machine learning approaches we explored , and lessons learned about how best to apply machine learning approaches to the problem of classifying cognitive states based on single interval fmri data .
functional magnetic resonance imaging
functional magnetic resonance imaging ( fmri ) is a technique for obtaining three - dimensional images related to neural activity in the brain through time .
more precisely , fmri measures the ratio of oxygenated hemoglobin to deoxygenated hemoglobin in the blood with respect to a control baseline , at many individual locations within the brain .
it is widely believed that blood oxygen level is inuenced by local neural activity , and hence this blood oxygen level dependent ( bold ) response is generally taken as an indicator of
an fmri scanner measures the value of the fmri signal ( bold response ) at all the points in a three dimensional grid , or image .
in the studies described in this paper , a three
learning to decode cognitive states from brain images
dimensional image is captured every 123 , 123 , or 123 seconds .
we refer to the cells within this three - dimensional image as voxels ( volume elements ) .
the voxels in a typical fmri study have a volume of a few tens of cubic millimeters , and a typical three dimensional brain image typically contains 123 , 123 to 123 , 123 voxels which contain cortical matter and are thus of interest .
while the spatial resolution of fmri is dramatically better than that provided by earlier brain imaging methods , each voxel nevertheless contains on the order of hundreds of thousands of neurons .
the temporal response of the fmri bold signal is smeared over several seconds .
given an impulse of neural activity , such as the activity in visual cortex in response to a ash of light , the fmri bold response associated with this impulse of neural activity endures for many seconds .
it typically increases to a maximum after approximately four to ve seconds , returning to baseline levels after another ve to seven seconds .
despite this prolonged temporal response , some researchers ( e . g . , menon et al . , 123 ) have reported that the relative timing of events can be resolved to within a few tens of milliseconds ( e . g .
to distinguish the relative timing of two ashes of lightone in the left eye and one in the right eye ) , providing hope that at least some temporal characteristics of brain function can be studied at subsecond resolution using fmri .
a small portion of fmri data is illustrated in gure 123
this gure shows data collected over a fteen second interval during which the subject read a word , decided whether it was a noun or verb ( in this case , it was a verb ) , then waited for another word .
this data was sampled once per second for fteen seconds , over sixteen planar slices , one of which is shown in the gure .
related work analyzing fmri data
over recent years there has been a growing interest within the computer science community in data processing for fmri .
one popular style of processing involves using generalized linear models ( glm ) as in friston et al .
( 123a , 123b ) and bly ( 123 ) .
here a regression is performed for each voxel , to predict the signal value at that voxel , based on properties of the stimulus .
the degree to which voxel activity can be predicted from stimulus features is taken as an indication of the degree to which the voxels activity is related to the stimulus .
notice this regression problem ( predict voxel activity given the stimulus ) is roughly the inverse of the problem we consider here ( predict cognitive state given all voxel activities ) .
others have used t - statistics to determine relevant active voxels , and yet others have used more complex statistical methods to estimate parameters of the bold response in the presence of noise ( genovese , 123 ) .
various methods for modelling time series have been used for analyzing fmri data .
for example , hojen - sorensen , hansen , and rasmussen ( 123 ) used hidden markov models ( hmm ) to learn a model of activity in the visual cortex resulting from a ashing light stimulus .
although the program was not told the stimulus , the on - off stimulus was recovered as the hidden state by the hmm .
a variety of unsupervised learning methods have also been used for exploratory analysis of fmri data .
for example , goutte et al .
( 123 ) discussed the use of clustering methods for fmri data .
one particular approach ( penny , 123 ) involved the application of expectation
mitchell et al .
figure 123
typical fmri data .
the top portion of the gure shows fmri data for a selected set of voxels in the cortex , from a two - dimensional image plane through the brain .
a fteen second interval of fmri data is plotted at each voxel location .
the anterior portion of the brain is at the top of the gure , posterior at bottom .
the left side of the brain is shown on the right , according to standard radiological convention .
the full three - dimensional brain image consists of sixteen such image planes .
the bottom portion of the gure shows one of these plots in greater detail .
during this interval the subject was presented a word , answered whether the word was a noun or verb , then waited for another word .
maximization to estimate mixture models to cluster the data .
others have used principle components analysis and independent components analysis ( mckeown et al . , 123 ) to determine spatial - temporal factors that can be linearly combined to reconstruct the fmri
learning to decode cognitive states from brain images
while there has been little work on our specic problem of training classiers to decode cognitive states , there are several papers describing work with closely related goals .
for example , haxby et al .
( 123 ) showed that different patterns of fmri activity are generated when a human subject views a photograph of a face versus a house , versus a shoe , versus a chair .
while they did not specically use these discovered patterns to classify subsequent single - event data , they did report that by dividing the fmri data for each photograph category into two samples , they could automatically match the sample means related to the same category .
more recently ( cox & savoy , 123 ) applied support vector machine and linear discriminant analysis to a similar set of data to successfully classify patterns of fmri activation evoked by the presentation of photographs of various categories of objects .
others ( wagner et al . , 123 ) reported that they have been able to make better - than - random predictions regarding whether a visually presented word will be remembered later , based on the magnitude of activity within certain parts of left prefrontal and temporal cortices during that presentation .
in addition to work on fmri , there has been related recent work applying machine learning methods to data from other devices measuring brain activity .
for example , blankertz , curio , and mller ( 123 ) describe experiments training classiers of brain states for single trial eeg data , while ( kjems et al . , 123; strother et al . , 123 ) report training brain state classiers for images obtained via positron emission tomography ( pet ) .
the work reported in the current paper builds on our earlier research described in mitchell
( 123 ) and wang , hutchinson , and mitchell ( 123 ) .
this section briey describes our approach to data preprocessing , training classiers , and
data acquisition and preprocessing
in the fmri studies considered here , data were collected from normal students from the university community .
typical studies involved between ve and fteen subjects , and we generally selected a subset of these subjects with the strongest , least noisy fmri signal to train our classiers .
data were preprocessed to remove artifacts due to head motion , signal drift , and other sources , using the fiasco program ( eddy et al . , 123 ) . 123 all voxel activity values were represented by the percent difference from their mean value during rest conditions ( when the subject is asked to relax , and not perform any particular task ) .
these preprocessed images were used as input to our classiers .
in several cases , we found it useful to identify specic anatomically dened regions of interest ( rois ) within the brain of each subject .
to achieve this , two types of brain images were collected for each subject .
the rst type of image , which has been discussed up to this point in the paper , captures brain activation via the bold response , and is referred to as a functional image .
the second type of image , called a structural image , captures the static physical brain structure at higher resolution .
for each subject , this structural image was used to identify the anatomical regions of interest , using the parcellation scheme of
mitchell et al .
caviness et al .
( 123 ) and rademacher et al .
( 123 ) .
for each subject , the mean of their functional images was then co - registered to the structural image , so that individual voxels in the functional images could be associated with the rois identied in the structural image .
learning methods
in this paper we explore the use of machine learning methods to approximate classication functions of the following form
f : fmri - sequence ( t123 , t123 ) cognitivestate
where fmri - sequence ( t123 , t123 ) is the sequence of fmri images collected during the con - tiguous time interval ( t123 , t123 ) , and where cognitivestate is the set of cognitive states to be discriminated .
each of our data sets includes fmri data from multiple human subjects .
except where otherwise noted , we trained a separate classication function for each subject .
we explored a variety of methods for encoding fmri - sequence ( t123 , t123 ) as input to the classier .
in some cases we encoded the input as a vector of individual voxel activities , a different activity for each voxel and for each image captured during the interval ( t123 , t123 ) .
this can be an extremely high dimensional feature vector , consisting of hundreds of thousands of features given that a typical image contains 123 , 123 to 123 , 123 voxels , and a training example can include dozens of images .
therefore , we explored a variety of approaches to reducing the dimension of this feature vector , including methods for feature selection , as well as methods that replace multiple feature values by their mean .
these feature selection and feature abstraction methods are described in detail in section 123 .
we explored a number of classier training methods , including :
gaussian naive bayes ( gnb ) .
the gnb classier uses the training data to estimate the probability distribution over fmri observations , conditioned on the subjects cognitive state .
it then classies a new example ( cid : 123 ) x = ( cid : 123 ) x123 .
xn ( cid : 123 ) by estimating the probability p ( ci | ( cid : 123 ) x ) of cognitive state ci given fmri observation ( cid : 123 ) x .
it estimates this p ( ci | ( cid : 123 ) x ) using bayes rule , along with the assumption that the features x j are conditionally independent given the class :
p ( ci | ( cid : 123 ) x ) =
p ( x j | ci ) p ( x j | ck )
where p denotes distributions estimated by the gnb from the training data .
each dis - tribution of the form p ( x j | ci ) is modelled as a univariate gaussian , using maximum likelihood estimates of the mean and variance derived from the training data .
distri - butions of the form p ( ci ) are modelled as bernoulli , again using maximum likelihood estimates based on training data .
given a new example to be classied , the gnb outputs posterior probabilities for each cognitive state , calculated using the above formula .
p ( x j | ci ) .
we considered two variants of the gnb , which differ only in their approach to
the univariate gaussian distributions
estimating the variances of
learning to decode cognitive states from brain images
gnb - sharedvariance , it is assumed that the variance of voxel x j is identical for all classes ci .
this single variance is estimated by the sample variance of the pooled data for x j taken from all classes ( with the class mean subtracted out of each value ) .
in gnb - distinctvariance , the variance is estimated separately for each voxel and class .
support vector machine ( svm ) .
we used a linear kernel support vector machine ( see ,
for instance , burges ( 123 ) ) .
k nearest neighbor ( knn ) .
we use k nearest neighbor with a euclidean distance metric ,
considering values of 123 , 123 , 123 , 123 and 123 for k ( see , for instance , mitchell ( 123 ) ) .
evaluating results
trained classiers were evaluated by their cross - validated classication error when learning boolean - valued classication functions .
when more than two classes are involved , our classiers output a rank - ordered list of the potential classes from most to least likely .
in this case , we scored the success of each prediction by the normalized rank of the correct class in this sorted list , which we refer to as the normalized rank error .
thus , the normalized rank error ranges from 123 when the correct class is ranked most likely , to 123 when it is ranked least likely .
note this normalized rank error is a natural extension of classication error when multiple classes are involved , and is identical to classication error when exactly two classes are involved .
note also that random guessing yields an expected normalized rank error of 123 regardless of the number of classes under consideration .
to evaluate classiers , we generally employ k - fold cross - validation , leaving out one example per class on each fold .
in the data sets considered in this paper , the competing classes are balanced ( i . e . , the number of available examples is the same for each competing class ) .
thus , by leaving out one example per class we retain a balanced training set for each fold , which correctly reects the class priors .
because the fmri bold response is blurred out over several seconds , a strict leave - out - one - example - per - class evaluation can sometimes produce optimistic estimates of the true classier error .
the reason is straightforward : when holding out a test image occurring at time t , the training images at times t + 123 and t 123 will be highly correlated with this test image .
therefore , if the images at t 123 and t + 123 belong to the same class as the image at t , and are included in the training set , this can lead to optimistically biased error estimates for the held out example .
when faced with this situation ( i . e . , in the semantic categories study described below ) , we avoid the optimistic bias by removing from the training set all images that occur within 123 seconds of the held out test image .
in this case , our cross validation procedure involves holding out one test example per class , and also removing temporally proximate images from the training set .
case studies
this section describes three distinct fmri studies , the data collected in each , and the classi - ers trained for each .
in this section we summarize the success of the best classier obtained for each of these studies .
section 123 discusses more generally the lessons learned across these three case studies .
mitchell et al .
picture versus sentence study
in this fmri study ( keller , just , & stenger , 123 ) , subjects experienced a collection of trials .
during each trial they were shown in sequence a sentence and a simple picture , then answered whether the sentence correctly described the picture .
we used this data to explore the feasibility of training classiers to distinguish whether the subject is examining a sentence or a picture during a particular time interval .
in half of the trials the picture was presented rst , followed by the sentence .
in the remaining trials , the sentence was presented rst , followed by the picture .
in either case , the rst stimulus ( sentence or picture ) was presented for 123 seconds , followed by a blank screen for 123 seconds .
the second stimulus was then presented for up to 123 seconds , ending when the subject pressed the mouse button to indicate whether the sentence correctly described the picture .
finally , a rest period of 123 seconds was inserted before the next trial began .
thus , each trial lasted approximately 123 seconds .
pictures were geometric arrangements of the symbols + , and / or $ , such as
sentences were descriptions such as it is true that the plus is below the dollar .
half of the sentences were negated ( e . g . , it is not true that the star is above the plus . ) and the other half were afrmative sentences .
each subject was presented a total of 123 trials as described above , interspersed with ten additional rest periods .
during each of these rest periods , the subject was asked to relax while staring at a xed point on the screen .
fmri images were collected every 123 msec .
the learning task we consider for this study is to train a classier to determine , given a particular 123 - second interval of fmri data , whether the subject is viewing a sentence or a picture during this interval .
in other words , we wish to learn a separate classier for each subject , of the following form
f : fmri - sequence ( t123 , t123 + 123 ) ( picture , sentence )
where t123 is the time of stimulus ( picture or sentence ) onset .
thus , the input to the classier is an 123 - second interval of fmri data beginning when the picture or sentence is rst presented to the subject .
although the stimulus was presented for a maximum duration of only 123 seconds , we chose this 123 - second interval in order to capture the full fmri activity associated with the stimulus ( recall from section 123 that the fmri bold signal often extends for 123 seconds beyond the neural activity of interest ) . 123
there were a total of 123 examples available from each subject ( 123 examples per class ) .
the fmri - sequence was itself described by the activities of all voxels in cortex .
the average number of cortex voxels per subject was approximately 123 , 123 , and varied signicantly by subject , based in large part on the size of the subjects head .
note that the eight second interval considered by the classier contains 123 images ( images were captured twice per second ) , yielding an input feature vector containing approximately 123 , 123 features , before
learning to decode cognitive states from brain images
the expected classication error of the default classier ( guessing the most common class ) is 123 in this case .
the average error obtained for the most successful trained classier , using the most successful feature selection strategy , was 123 , averaged over 123 subjects , with the best subject reaching 123 ( refer to section 123 for more details ) .
these results are statistically highly signicant , and indicate that it is indeed possible to train classiers to distinguish these two cognitive states reliably .
in addition to these single - subject classiers , we also experimented with training classi - ers that operate across multiple subjects .
in this case , we evaluated the classication error using a leave - one - subject - out regime in which we held out each of the 123 subjects in turn while training on the other 123
the mean error over the held out subject for the most suc - cessful combination of feature selection and classier was 123 .
again , this is signicantly better than the expected 123 error from the default classier , indicating that it is possible to train classiers for this task that operate on human subjects who where not part of the training set .
these results are described in detail in section 123 .
syntactic ambiguity study
in this fmri study ( see mason et al . , 123 ) subjects were presented with sentences , some of which were ambiguous , and were asked to respond to a yes - no question about the content of each sentence .
the questions were designed to ensure that the subject was in fact processing the sentence .
the learning task for this study was to distinguish whether the subject was currently reading an ambiguous sentence ( e . g . , the experienced soldiers warned about the dangers conducted the midnight raid .
) or an unambiguous sentence ( e . g . , the experienced soldiers spoke about the dangers before the midnight raid . ) . 123
ten sentences of each of type were presented to each subject .
each sentence was presented for 123 seconds .
next a question was presented , and the subject was given 123 seconds to answer .
after the subject answered the question , or 123 seconds elapsed , an x appeared on the screen for a 123 second rest period .
the scanner collected one fmri image every 123 seconds .
we are interested here in learning a classier that takes as input an interval of fmri activity , and determines which of the two types of sentence the subject is reading .
using our earlier notation , for each subject we trained classiers of the form
f : fmri - sequence ( t123 + 123 , t123 + 123 ) sentencetype
where sentencetype = ( ambiguous , unambiguous ) , and where t123 is the time at which the sentence is rst presented to the subject .
note the classier input describes fmri activity during the interval from 123 to 123 seconds following initial presentation of the sentence .
this is the interval during which the fmri activity is most intense .
there were a total of 123 examples for each subject ( 123 examples per class ) .
the fmri - sequence was described using only the voxels from four rois considered to be most relevant by a domain expert .
these 123 rois contained a total of 123 to 123 voxels , depending on the subject .
note the 123 second interval considered by the classier contains 123 images ( images were captured every 123 seconds ) , yielding a classier input vector containing from 123 , 123 to 123 , 123 features , depending on the human subject , before feature selection .
mitchell et al .
the expected classication error of the default classier ( guessing the most common class ) in this case is 123 , given the equal number of examples from both classes .
the average error obtained by the most successful combination of feature selection and classier was 123 , averaged over 123 subjects , with the best single - subject classier reaching an error of 123 ( refer to section 123 for more details ) .
semantic categories study
in this study , 123 subjects were presented with individual nouns belonging to twelve distinct semantic categories ( e . g . , fruits , tools ) , and asked to determine whether the word belonged to a particular category .
we used this data to explore the feasibility of training classiers to detect which of the semantic categories of word the subject was examining .
the trials in this study were divided into twelve blocks .
in each block , the name of a semantic category was rst displayed for 123 seconds .
following this , the subject was shown a succession of 123 words , each presented for 123 msec and followed by 123 msec of blank screen .
after each word was presented , the subject clicked a mouse button to indicate whether the word belonged to the semantic category named at the beginning of the block .
in fact , nearly all words belonged to the named category ( half the blocks contained no out - of - category words , and the remaining blocks contained just one out - of - category word ) .
a multi - second blank screen rest period was inserted between each of the twelve blocks .
the twelve semantic categories of words presented were sh , four - legged animals , trees , owers , fruits , vegetables , family members , occupations , tools , kitchen items , dwellings , and building parts .
words were chosen from lists of high frequency words of each category , as given in battig and montague ( 123 ) , in order to avoid obscure or multiple - meaning words .
fmri images were acquired once per second .
the learning task we considered for this study is to distinguish which of the twelve semantic categories the subject is considering , based on a single observed fmri image .
following our earlier notation , we wish to learn a classier of the form :
f : fmri ( t ) wordcategory
where fmri ( t ) is a single fmri image , and where wordcategory is the set of 123 semantic categories described above .
a total of 123 example images were collected for each subject ( 123 examples per class , times 123 classes ) .
all voxels from 123 rois were used , yielding a total of 123 , 123 to 123 , 123 voxels , depending on the subject .
in this case the classier input is a single image , so the classier input dimension is equal to the number of voxels , prior to feature selection .
the trained classier outputs a rank - ordered list of the 123 categories , ranked from most to least probable .
we therefore evaluate classier error using the normalized rank error described in section 123 , where the default classier ( guessing the most frequent class ) yields an expected normalized rank error of 123 .
the normalized rank error for the most successful combination of feature selection and classier was 123 ( i . e .
on average the correct word category was ranked rst or second out of the twelve categories ) , over 123 subjects , with the best subject reaching 123 .
( refer to section 123 for details ) .
learning to decode cognitive states from brain images
figure 123
color plots show locations of voxels that best predict the word semantic category , for three different human subjects .
for each voxel , the color indicates the normalized rank error over the test set , for a gnb classier based on this single voxel .
note the spatial clustering of highly predictive voxels , and the similar regions of predictability across these three subjects .
the range of normalized rank errors is ( red 123 , dark blue 123 ) , with other colors intermediate between these two extremes .
each image corresponds to a single two - dimensional plane through the brain of one subject .
one reasonable question that can be raised regarding these classier results is whether the classier is indeed learning the pattern of brain activity predictive of semantic categories , or whether it is instead learning patterns related to some other time - varying phenomenon that inuences fmri activation .
one unfortunate property of the experimental protocol for collecting data , from this point of view , is that all of the words belonging to a single category are presented within a single time interval ( i . e . , a single experiment block ) .
in fact we do believe this temporal adjacency may be inuencing our results , but we also be - lieve the classier is indeed capturing regularities primarily related to semantic categories .
one strong piece of supporting evidence is that classiers trained for different human sub - jects tend to rely on the same brain locations to make their predictions , and that these regions have been reported by others as related to semantic categorization .
figure 123 il - lustrates the brain regions containing the most informative fmri signal for classication , across three subjects .
in this gure , red and yellow indicate the voxels whose activity allows most accurate classication .
note the highly discriminating voxels are clustered together , in similar regions across these subjects .
these locations for discriminability match those reported in earlier work on semantic categorization by chao , haxby , and martin ( 123 ) , chao , weisberg , and martin ( 123 ) , ishai et al .
( 123 ) and aguirre , zarahn , and desposito ( 123 ) , as well as some novel areas that are currently under investigation .
lessons learned
can one learn to decode mental states from fmri ?
the primary goal leading to this research was to determine whether it is feasible to use machine learning methods to decode mental states from single interval fmri data .
the successful results reported above for all three data sets indicate that this is indeed feasible in
mitchell et al .
a variety of interesting cases .
however , it is important to note that while our empirical results demonstrate the ability to successfully distinguish among a predened set of states occurring at specic times while the subject performs specic tasks , they do not yet demonstrate that trained classiers can reliably detect cognitive states occurring at arbitrary times while the subject performs arbitrary tasks .
while our current results may already be of use in cognitive science research , we intend to pursue this more general goal in future work .
we also attempted but failed to train successful classiers for several other classication functions dened over these same data sets .
for example , we were unable to train an accurate classier to distinguish the processing of negated versus afrmative sentences in the picture versus sentence study .
we were also unsuccessful in attempts to train classiers to distinguish the processing of true versus false sentences , or sentences which the subject answered correctly versus incorrectly .
it may be that these failures could be reversed given larger training sets or more effective learning algorithms .
alternatively , it may be the case that the fmri data simply lacks the information needed to make these distinctions .
this line of research is still very new , and while the above results demonstrate the feasibility of discriminating a variety of cognitive states based on fmri , at this point the question of exactly which cognitive states can be reliably discriminated remains an open empirical question .
given our initial successes plus those reported in cox and savoy ( 123 ) , as well as likely advances in brain imaging technology and likely progress in developing machine learning methods specically for this type of application , we are optimistic that over time we will be able to decode an increasingly useful collection of cognitive states in an increasingly open ended set of experimental settings .
interestingly , we found that the accuracy of our single - subject classiers varied signif - icantly among subjects , even within the same study .
for example , when training an svm for the 123 different subjects in the picture versus sentence study , the error rates of the 123 single - subject classiers were 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 , and 123 ( additional single - subject data is reported in tables 123 ) .
subjects producing high accuracies with one classier were typically the same who produced high accuracies for other classiers , and it is likely that the data for the worst - performing classiers were corrupted by various kinds of noise ( e . g . , signicant head motion during imaging ) .
consid - erable subject - to - subject variation in fmri responses has been widely reported in the fmri
before leaving the topic of whether one can train classiers in this domain , it is worth considering the question of whether our reported better - than - random classier error rates are simply the result of having tried many learning algorithms and feature selection methods , then reporting results for the approach that worked best .
this is a general issue whenever one experiments with many learning approaches , then reports accuracy for the one that performs best over the test data .
in the following subsections we describe in detail the different feature selection methods and learning methods we explored .
for the purposes of this discussion , however , there are two important points to be made .
first , we found that every learning algorithm ( gnb , svm , and knn ) produced signicantly better than random classication accuracies for every case study , supporting our conclusion that it is feasible to learn classiers in this domain .
second , when performing feature selection , features were chosen over a training set distinct from the test data , using a leave - one - out
learning to decode cognitive states from brain images
cross validation approach , so that each test example had no inuence on which features were selected .
the only way that the test data inuenced feature selection was in choosing the number of features , n , for which results are reported .
this single parameter n was chosen for each learning algorithm and study , to maximize the mean accuracy over all single - subject classiers trained by that algorithm for that study .
given that the number of single - subject classiers ranged from ve to thirteen , depending on the study , we conjectured that the choice of this single parameter n would exert only a very minor inuence and that we could safely consider our reported accuracies to be very close to true accuracies .
to test this conjecture we conducted an experiment to compare this biased estimate ( using the value of n which maximized test set performance ) to an unbiased estimate obtained using a more elaborate and computationally intensive approach .
to obtain the fully unbiased estimate , we employed a nested cross validation approach to partition the data repeatedly into three sets : a training set , optimization set , and nal test set .
the training and optimization sets were used to train a gnb classier , to choose the feature selection method , and to choose the number n of features to be included .
the nal test set had no inuence on training or selecting what to report , and was used only to provide a nal unbiased estimate of accuracy .
we ran this experiment over the data from all 123 subjects in our sentence versus picture data set .
the resulting unbiased error estimate provided by the nal test set was 123 , whereas the original biased estimate provided by our standard two - set approach was 123 .
furthermore , the number of features n selected by these two approaches were nearly identical .
this experimental outcome supports our conjecture that our reported classier accuracies , while in theory slightly biased , are in fact very close to their true accuracies .
which classier works best ?
as discussed earlier , we experimented with three classier learning methods : a gaussian naive bayes ( gnb ) classier , k - nearest neighbor ( knn ) , and linear support vector ma - chines ( svm ) .
these classiers were selected because they have been used successfully in other applications involving high dimensional data .
for example , naive bayes classi - ers , knn , and svm have all been used for text classication problems ( nigam et al . , 123; joachims , 123; yang , 123 ) , where the dimension of the data is approximately 123 , corresponding to the size of the natural language vocabulary .
to test the relative performance of our classiers , we performed two sets of experiments .
first , for each fmri study we analyzed the performance of gnb , linear svm , and knn ( with k ( 123 , 123 , 123 , 123 , 123 ) ) using as input to the classier all voxels in the rois selected for those studies .
here the performance metric is classication error , except for the semantic categories study where the metric is normalized rank error .
the performance reported for a specic study is the mean error over all single - subject classiers trained for that study , as obtained by leave - one - example - out - from - each - class cross validation .
because the semantic categories study is not a binary classication task , we did not experiment with svms on this specic study .
the results are shown in table 123 in the rows indicating no feature selection .
the table reports results for the better - performing variant of gnb in each study .
in the syntactic
mitchell et al .
table 123
error rates for classiers across all studies .
each table entry indicates the mean test error averaged over all single - subject classiers trained for a particular fmri study and learning method .
the rows with feature selection no show results when using all voxels within the available rois .
the rows with feature selection yes show results of the feature selection method that produced the lowest errors .
in every case except one , this was the active feature selection method described in section 123 . 123
the exception is the entry marked with the * , for which roiactive feature selection worked best .
the variant of gnb which produced the strongest results ( and which is therefore reported in this table ) is gnb - sharedvariance for the syntactic ambiguity study , and gnb - distinctvariance for the other two studies .
ambiguity study this was gnb - sharedvariance , and in the other two studies it was gnb -
as can be seen in the table , the gnb and svm classiers outperformed knn .
examining the performance of knn , one can also see a trend that performance generally improves with increasing values of k .
our second set of experiments examined the performance of the classiers when used in conjunction with feature selection .
the specic feature selection methods we considered are described in detail in the next subsection .
for each study and learning method the table reports results using the most successful feature selection method , in the table row indicating feature selection yes .
in all cases except one ( the table entry marked by the * ) , the most successful feature selection method was the active method described in section 123 . 123
as can be seen in table 123 , performing feature selection produced a large and consistent improvement in classication error across all studies and learning methods .
as in the exper - iments with no feature selection , gnb and svm outperform knn when feature selection is used , and the performance of knn improves as k increases .
analysis .
one clear trend in this data is that knn fared less well than gnb or svms across all studies and conditions .
in retrospect , this is not too surprising given the high dimensional , sparse training data sets .
it is well known that the knn classier is sensitive to irrelevant features , as these features add in irrelevant ways to the distance between train and test examples ( mitchell , 123 ) .
this explanation for the poor performance of knn is also consistent with the dramatic improvement in knn performance resulting from feature selection .
as the table results indicate , feature selection sometimes reduces knn error by a factor of two or more , presumably by removing many of these irrelevant , misleading
as discussed in section 123 , the two variants of gnb we considered differ only in the number of distinct parameters estimated when modeling variances in the class conditional
learning to decode cognitive states from brain images
distributions of voxel activities .
gnb - sharedvariance estimates a single variance indepen - dent of the class , whereas gnb - distinctvariance estimates a distinct variance per class .
we found that gnb - sharedvariance performed better in the study containing the fewest exam - ples per class ( syntactic ambiguity ) , whereas gnb - distinctvariance performed better in the other studies .
this empirical result is consistent with a general bias - variance tradeoff : pooling data in order to estimate fewer parameters generally leads to lower variance esti - mates , but to higher bias .
thus , in general as the number of available examples increases , we expect gnb - distinctvariance to outperform gnb - sharedvariance , all other things being
notice that the svm outperformed gnb in the picture vs .
sentence study for which there were 123 examples per class , but not in the semantic categories study for which there were only 123 examples per class .
while this may be due to various factors , it is interest - ing to observe that this trend is consistent with recent results of ng and jordan ( 123 ) , in which they provide empirical and theoretical arguments that gnb often outperforms logistic regression when data is scarce , but not when data becomes more plentiful .
they explain this by pointing out that although logistic regression asymptotically ( in the num - ber of training examples ) outperforms gnb , logistic regression requires o ( n ) examples to reach its asymptote while gnb requires only o ( log n ) , where n is the number of fea - tures .
in our case we note that svm , like logistic regression , requires o ( n ) examples , in comparison to the o ( logn ) required by gnb .
while our empirical results are too sparse to prove a statistically signicant trend for the relative performance of svm and gnb , it is nevertheless interesting to note our results are consistent with the analysis of ng and jordan
in summary , we found when training fmri classiers across a variety of data sets and target functions that gnb and svm outperformed knn quite consistently .
furthermore , feature selection has a large and consistent benecial impact across all studies and learning
which feature selection method works best ?
given that our classication problem involves very high dimensional , noisy , sparse training data , it is natural to consider feature selection methods to reduce the dimensionality of the data before training the classier .
as we discussed in the previous section , and as summa - rized in table 123 , feature selection leads to large and statistically signicant improvements in classication error across all three of our case studies .
this section discusses in detail the feature selection methods explored in our work , and some surprising lessons learned regarding which feature selection methods worked best .
approach .
within the eld of machine learning , the most common approach to feature selection when training classiers is to select those features that best discriminate the target classes .
for example , given the goal of learning a target classication function f : x y , one common approach to feature selection is to rank order the features of x by their mutual information with respect to the class variable y , then to greedily select the n highest scoring features .
mitchell et al .
given the nature of classication problems in the fmri domain ( and a variety of other domains as well ) , a second general approach to feature selection is also possible .
to illustrate , consider the problem of learning a boolean classier f : x y where y = ( 123 , 123 ) , given training examples labeled as belonging to either class 123 or class 123 ( e . g . , learning to distinguish whether the subject is viewing a picture or sentence ) .
in fmri studies , we naturally obtain three classes of data rather than two .
in addition to data representing class 123 and class 123 , we also obtain data corresponding to a third xation or rest condition .
this xation condition contains data observed during the time intervals between trials , during which the subject is generally at rest ( e . g . , they are examining neither a picture nor a sentence , but are instead staring at a xation point ) .
thus , we can view the data associated with class 123 and class 123 as containing some signal conditioned on the class variable y , whereas the data associated with xation contains no such signal , and instead contains only background noise relative to our classication problem .
in this setting , we can consider a second general approach to feature selection : score each feature by how well it discriminates the class 123 or class 123 data from this zero signal data .
in the terminology of fmri , we score each feature based on how active it is during the class 123 or class 123 intervals , relative to the xation intervals .
the intuition behind this feature selection method is that it emphasizes choosing voxels with large signal - to - noise ratios , though it ignores whether the feature actually distinguishes the target classes .
we refer to this general setting as the zero signal learning setting , summarized in gure 123
notice many classication problems involving sensor data can be modeled in
figure 123
the zero signal learning setting .
boolean classication problems in the fmri domain naturally give rise to three types of data : data corresponding to the two target classes plus data collected when the subject is in the xation or rest condition .
we assume the data from class 123 and class 123 are composed of some underlying signal plus noise , whereas data from the xation condition contains no relevant signal but only noise .
in such settings , feature selection methods can consider both voxel discriminability ( how well the feature distinguishes class 123 from class 123 ) , and voxel activity ( how well the feature distinguishes class 123 or class 123 from the zero signal
learning to decode cognitive states from brain images
terms of this zero signal learning setting ( e . g . , classifying which of two people is speaking based on voice data , where the zero signal condition corresponds to background noise when neither person is speaking ) .
therefore , understanding how to perform feature selection and classication within this setting has relevance beyond the domain of fmri .
in fact , within the fmri literature it is common to use activity to select a subset of relevant voxels , and then to compare the behavior of this selected subset over various conditions .
in the experiments summarized below , we consider feature selection methods that select voxels based on both their ability to distinguish the target classes from one another ( which we call discriminability ) , and on their ability to distinguish the target classes from the xation condition ( which we call activity ) .
although each feature consists of the value of a single voxel at a single time , we group the features involving the same voxel together for the purpose of feature selection , and thus focus on selecting a subset of voxels .
in greater detail , the feature selection ( voxel selection ) methods we consider here are :
select the n most discriminating voxels ( discrim ) .
in this method , a separate classier is trained for each voxel , using only the observed fmri data associated with that voxel .
the accuracy of this single - voxel classier over the training data is taken as a measure of the discriminating power of the voxel , and the n voxels that score highest according to this measure are selected .
note when reporting cross validation errors on nal classiers using this feature selection method , features are selected separately for each cross - validation fold in order to avoid using data from the test fold during the feature selection process .
thus , the voxels selected may vary from fold to fold .
select the n most active voxels ( active ) .
in this method , voxels are selected based on their ability to distinguish either target class from the xation condition .
more specically , for each voxel , v , and each target class yi , a t - test is applied to compare the voxels fmri activity in examples belonging to class yi to its activity in examples belonging to xation periods .
the rst voxels are then selected by choosing for each target class yi the voxel with the greatest t statistic .
the next voxels are selected by picking the second most active voxel for each class , and so on , until n voxels are chosen .
notice the selected voxels may distinguish just one target class from xation , or may distinguish both target classes from xation .
select the n most active voxels per region of interest ( roiactive ) .
this is similar to the active method above , but ensures that voxels are selected uniformly from all regions of interest ( rois ) within the brain .
more precisely , given m prespecied rois , this method applies the active method to each roi , selecting n / m voxels from each .
the union of these voxel sets is returned as the set of n selected voxels .
the approaches above for selecting voxels can be combined with methods for averaging the values of multiple features ( in space or time ) , and with methods that select data over a sub - interval in time .
we experimented with various combinations of such approaches , and report here on the above three methods ( discrim , active , roiactive ) as well as a fourth method derived from roiactive :
calculate the mean of active voxels per roi ( roiactiveavg ) .
this method rst selects n / m voxels for each of the m rois using the roiactive method .
it then creates a single
mitchell et al .
supervoxel for each roi , whose activity at time t is the mean activity of the selected roi voxels at time t .
results .
we experimented with each of these four feature selection methods , over each of the three case study data sets .
for comparison purposes we also considered using all available features ( denoted allfeatures in our results tables ) .
tables 123 present summarized results of feature selection experiments for each of the three fmri studies .
each table shows the best errors obtained for each feature selection
table 123
picture vs .
sentence studygnb classier errors by subject and feature selection method .
123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123
the rst column indicates the feature selection method , along with the number of features selected ( allfeatures indicates using all available features , which varies by subject ) .
the second column indicates the average error over all 123 single - subject classiers when using the feature selection method .
remaining columns indicate errors for individual subjects a through m .
for each method , the number of features was chosen to minimize the average error over all subjects .
table 123
syntactic ambiguity studygnb classier errors by subject and feature selection method .
average error a
123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123
results are presented using the same format as table 123
table 123
semantic categoriesgnb errors by subject and feature selection method .
feature selection average error
results are presented using the same format as table 123
learning to decode cognitive states from brain images
method and for each subject considered in the study , when using a gnb classier .
here the best error refers to the lowest error achieved by varying the number of selected voxels .
the optimal number of voxels selected varied by study and feature selection method , typically ranging from 123 to 123% of the total number available within the selected rois , hence we focused experiments within this range .
for each feature selection method and study , the number of features was chosen to minimize the mean error of all single - subject classiers trained for this study .
this number is shown in parentheses next to the feature selection method in the table .
the specic numbers of features considered were 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 , and 123 for the picture versus sentence study , 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 , and 123 for the semantic categories study , and 123 , 123 , 123 , 123 , 123 for the syntactic ambiguity study .
these results indicate that using feature selection leads to improved classier error in all three studies , and that all of our feature selection methods improve over no feature selection in the vast majority of cases .
for example , the active feature selection method outperforms the approach of using all features in 123 of the 123 single - subject classiers trained over the three studies , and yields equivalent performance in the remaining 123 cases .
a second strong trend in the results is the dominance of feature selection methods based on activity ( active , roiactive , roiactiveavg ) over those based on discriminability ( discrim ) .
as can be seen in the tables , every activity - based feature selection method outperforms the discriminability - based method , on every case study .
of these activity - based methods , the active method yields best accuracy , and it outperforms the discrim method in 123 of the 123 single - subject classiers trained across the three studies , yielding inferior performance in only 123 of the 123 cases .
note also that the active method selects substantially fewer features than discrim in all three fmri studies .
it is at rst surprising to observe that selecting features based on their activity level works dramatically better than selecting them based on their ability to dis - criminate the target classes .
given that the end goal is to discriminate the target classes , and that selecting features based on discriminability is the norm in machine learning applica - tions , one might well expect discriminability to have been the dominant method .
below we look deeper into why we observe the opposite result in all three fmri studies .
one situation in which we might expect activity - based feature selection to outperform discrimination - based methods is when data dimensionality is very high , noise levels are high , training data are sparse , and very few voxels contain a signal related to the target classes .
in such cases , we should expect to nd that some voxels that are truly irrelevant appear nonetheless to be good discriminators over the sparse sample of training dataeven when using cross validation to test their discrimination power .
the larger the set of such irrelevant voxels , the more likely that a feature selection strategy focused on discrimination would select such overtting voxels , and be unable to distinguish these from truly informative discriminating voxels .
however , in this same case we might expect that choosing voxels with high signal - to - noise ratios would be a useful strategy , as it would remove from consideration the large number of irrelevant voxels ( i . e . , those with no signal , but only noise ) .
in fact , our activity - based feature selection strategies select exactly this kind of high signal - to - noise ratio voxels .
the bottom line is that each feature selection strategy runs its own risk :
mitchell et al .
discrimination - based methods run the risk of selecting voxels that only coincidentally t the noisy training sample , whereas activity - based methods run the risk of choosing high signal - to - noise voxels that cannot discriminate the target classes .
which risk is greater depends on the exact problem , but the relative risk for the discrimination - based method grows more quickly with increasing data dimension , increasing noise level , decreasing training set size , and an increasing fraction of irrelevant features .
to explore this conjecture , let us examine the actual characteristics of the voxels selected by these two methods in our data .
in particular , we will focus on a single subject in the semantic categories study : subject g , whose best average normalized rank error ( 123 ) is obtained by a gnb classier using 123 voxels chosen using the active feature selection strategy .
the discrim method also obtains its best error ( 123 ) using 123 voxels for this particular subject .
what is the difference in these two sets of voxels selected using these two methods ? are there in fact differences in the degree of overtting between the two
let us consider three sets of voxels from subject g : those chosen by active feature selection but not by discrim ( activeonly ) , those chosen by discrim by not by active ( discriminatingonly ) , and those chosen independently by both methods ( intersection ) .
for this particular subject , there are 123 voxels in the intersection set , and 123 in each of activeonly and discriminatingonly .
training a gnb classier using only the voxels in intersection yields an error of ( 123 ) , slightly but not signicantly better than the error from the active voxels .
figure 123 shows the degree of overtting for each of these three sets of voxels .
on the left , panel ( a ) provides a scatterplot of training set error ( horizontal axis ) versus test set error ( vertical axis ) .
the straight line indicates where training error equals test error .
notice all three sets of voxels overt to some degree ( i . e . , test error is generally greater than or equal to training error ) , but the cluster of voxels ranges furthest from the straight line for the discriminatingonly voxels .
on the right , panel ( b ) provides a histogram showing the number of voxels in each set that overt to varying degrees .
note the discriminatingonly voxel set contains many more voxels that overt to a large degree .
based on the data summarized in this gure , it is clear that the degree of overtting is indeed greater in this case for voxels selected by discrim than those selected by active .
it is also clear that the voxels in intersection suffer the least overtting .
a different view into the character of these three voxel sets is provided by gure 123
the top portion of this gure plots the 123 voxels with the best training set error from each of the three sets .
each voxel plot shows the learned gaussian model for each of the twelve target classes .
notice the greater spread of these models for the voxels chosen by the discrim method ( discriminatingonly and intersection ) than for the activeonly set .
the bottom portion of gure 123 provides a scatter plot of standard deviation ( horizontal axis ) versus test error ( vertical axis ) for the three voxel sets .
notice the signicantly lower standard deviation for the activeonly set .
above we suggested that the discrim method for feature selection carries a risk of se - lecting voxels that overt the data .
the above data , especially from gure 123 , indicates that in fact overtting is greater for discrim than for active in our data .
we also suggested the active method carries the counterbalancing risk of selecting irrelevant voxels .
is this
learning to decode cognitive states from brain images
( a ) scatter plots of training set error ( horizontal axis ) against test set error ( vertical axis ) for the voxels in each subset ( row ) .
( b ) histograms depicting for each voxel subset the number of voxels that overt to various degrees .
the horizontal axis in this case is ( test error minus training error ) , measuring the degree of overtting .
in fact occurring in our case ? the plots in gure 123 show that the activeonly set of vox - els does appear to contain voxels that are poor discriminators among the twelve target
to understand the impact of poor discriminators ( irrelevant voxels ) selected by the active method , consider the relative weight of the relevant versus irrelevant voxels used by a gnb .
given an instance ( cid : 123 ) x to be classied , the log odds assigned by the gnb for two classes ci and c j is
p ( ci | ( cid : 123 ) x ) p ( c j | ( cid : 123 ) x )
p ( c j )
p ( xk | ci ) p ( xk | c j )
where xk is the observed value for the kth feature of ( cid : 123 ) x , and where p denotes distributions estimated by gnb based on the training data .
note the gnb classier will predict class ci if the above log odds ratio is positive , and c j if it is negative .
thus , the decision of the gnb is determined by a linear sum , where each voxel contributes one term to the sum .
mitchell et al .
in the top half of the gure , each plot shows the 123 learned class probability densities p ( xk | ci ) for a single voxel xk , for subject g in the semantic categories study .
the x axis ranges from 123 to 123
each row contains the 123 voxels with the lowest training set errors from each voxel subset , sorted by increasing error .
for reference , the leftmost and rightmost plots in each row have their ( training set , test set ) error values below them .
the bottom half of the gure provides scatterplots depicting the average class standard deviation ( horizontal axis ) against the test error ( vertical axis ) for each voxel subset ( row ) .
note the higher variance for the discriminating voxels ( intersection and discriminatingonly ) .
learning to decode cognitive states from brain images
now let us consider a voxel xk which is truly irrelevant to the classication ( i . e . , where the true distributions p ( xk | ci ) and p ( xk | c j ) are identical ) .
first , consider the situation in which the learned estimates p ( xk | ci ) and p ( xk | c j ) are also identical .
in this case the fraction involving xk will be equal to 123 , its log will be equal to 123 regardless of the observed value of xk , and voxel xk will therefore have no inuence on the nal gnb de - cision .
now consider the situation in which p ( xk | ci ) and p ( xk | c j ) differ ( e . g . , due to overtting ) despite the fact that p ( xk | ci ) = p ( xk | c j ) .
in this case , the xk term will in fact be non - zero , and will have a detrimental , randomizing inuence on the nal gnb classication .
is this in fact occurring in our data ? the plots in gure 123 suggest that the active voxels that are irrelevant ( i . e . , those in activeonly ) do indeed have strongly overlap - ping p ( xk | ci ) distributions , limiting the magnitude of their contribution to the nal gnb
to summarize , we nd clear empirical evidence that feature selection consistently im - proves classication accuracy in our domain .
furthermore , we nd clear empirical evi - dence that activity - based feature selection methods consistently outperform discrimination - based methods ( every activity - based feature selection method we considered outperformed discrimination - based feature selection , in each of our three fmri studies ) .
the general char - acteristic of our problem domain that enables this kind of feature selection is summarized in gure 123
in particular , the key characteristic is the availability of a third category of data in which neither of the target classes is representeddata which we refer to as the zero signal class of data .
we conjecture , and support with a variety of empirical observations , that activity - based feature selection may outperform discrimination - based feature selection in zero - signal classication problems , especially with increasing data dimension , noise , and data sparsity , and as the proportion of truly relevant features decreases .
given that a variety of sensor - based classication problems t this zero - signal learning setting , we believe one of the most signicant lessons learned from the fmri domain is the utility of activity - based feature selection in such domains .
examples of such sensor - based classication problems include speaker voice identication ( where zero signal data corresponds to background mi - crophone noise when nobody is speaking ) , and video object classication in xed - camera settings ( where zero signal data corresponds to background images containing no objects of interest ) .
additional research is needed to formalize this zero - signal learning setting and explore its relevance in these and other domains .
one especially promising direction for future work is to understand how best to blend activity - based and discrimination - based feature selection to optimize learning accuracy .
can one train classiers across multiple subjects ?
all results discussed so far in this paper have focused on the problem of training subject - specic classiers .
this section considers the question of whether it is possible to train classiers that apply across multiple human subjects , including human subjects who are not part of the training set .
this is an important goal because if it is feasible it opens the possibility of discovering subject - independent regularities in brain activity associated with different types of cognitive processing , it opens the possibility of sharing trained classiers among researchers analyzing fmri data collected from many different people , and it makes
mitchell et al .
it possible to pool training data from multiple subjects to alleviate the sparse data problem in this domain .
the biggest obstacle to analysis of multiple - subject fmri data is anatomical variability among subjects .
different brains vary signicantly in their shapes and sizes , as is apparent from the images taken from three different brains in gure 123
this variability makes it prob - lematic to register the many thousands of voxels in one brain to their precise corresponding locations in a different brain .
one common approach to this problem is to transform ( geo - metrically morph ) fmri data from different subjects into some standard anatomical space , such as talairach coordinates ( talairach & tournoux , 123 ) .
the drawback of this method is that the morphing transformation typically introduces an error on the order of a centimeter , in aligning the millimeter - scale voxels from different brains .
the alternative that we con - sider here is to instead abstract the fmri activation , using the mean activation within each roi as the classier input , instead of using individual voxel activations .
in other words , we treat each roi as a large supervoxel whose activation is dened by the mean activation over all the voxels it contains .
note this is equivalent to using our roiactiveaverage feature selection method , including all voxels within each roi .
despite the anatomical variability in brain sizes and shapes , it is relatively easy for trained experts to manually identify the same set of rois in each subject .
a second difculty that arises when training multiple - subject classiers is that the inten - sity of fmri response to a particular stimulus is often different across subjects .
to partially address this issue , we employ a normalization method that linearly rescales the data from different subjects into the same maximum - minimum range .
while there are many cross - subject differences that cannot be addressed by this simple linear transformation , we have found this normalization to be useful .
we have also found that a similar normalization method can sometimes reduce classication error for single - subject classiers , when used to normalize data across different trials associated with a single subject .
of course a third difculty that arises is simply that different people may think differently , and we have no reason to assume a priori that we would nd the same spatial - temporal patterns of fmri activation in two subjects even if we could perfectly align their brains spatially , and perfectly normalize the relative intensities of their fmri readings .
thus , the question of whether one can train multiple - subject classiers is partly a question of whether different brains behave sufciently similarly to exhibit a common pattern of activation .
we performed experiments to train multiple - subject classiers using two data sets : the picture versus sentence data , and the syntactic ambiguity data .
the following two subsec - tions describe these two experiments in turn .
we did not attempt to train multiple - subject classiers for the third dataset , semantic categories , because we expected the detailed pat - terns of activity that distinguish word categories would be undetectable once the voxel - level data was abstracted to mean roi activity .
sentence versus picture study .
we trained multiple - subject classiers for the sen - tence versus picture study , to discriminate whether the subject was viewing a picture or a sentence .
multiple - subject classiers were trained using data from 123 of the 123 subjects , abstracting the data from each subject into seven roi supervoxels .
to evaluate the error of these trained classiers , we used leave - one - subject - out cross validation .
in particular , for
learning to decode cognitive states from brain images
table 123
errors for multiple - subject classier , sentence versus picture study .
the right column shows the error of a multi - subject classier when applied to subjects withheld from the training set , using leave - one - subject - out cross validation .
results are obtained using normaliza - tion and 123 rois .
all classiers are trained by aver - aging all voxels in an roi into a supervoxel .
123% condence intervals are computed under the as - sumption that test examples are identically , inde - pendently bernoulli distributed .
the error of a ran - dom classier is 123 .
in this analysis we employed the gnb - distinctvariance version of gnb .
each subject we trained on the remaining 123 subjects , measured the error on this held out subject , then calculated the mean error over all held out subjects .
notice that in this case there are 123 examples available to train this multiple - subject classier , in contrast to the 123 examples available to train the single - subject classiers described earlier .
the results , summarized in table 123 , show that the linear svm learns a multiple - subject classier that achieves error of 123 123 over the left out subject .
this is highly sta - tistically signicant compared to the 123 error expected of a default classier guessing the majority class .
this indicates that it is indeed possible to train a classier to capture signicant subject - independent regularities in brain activity that are sufciently strong to detect cognitive states in human subjects who are not part of the training set .
as in earlier experiments , we note that svm and gnb again outperform knn , and that the performance of knn improves with increasing values of k .
although these results demonstrate that it is possible to learn multiple - subject classiers with accuracies better than random , the accuracies are below those achieved by the single - subject classiers summarized in table 123
for example , the multiple - subject svm classier error of 123 is less accurate than the mean single - subject classier error of 123 reported in table 123
this difference could be due to a variety of factors , ranging from the lower spatial resolution encoding of fmri inputs in the multiple - subject classier ( i . e . , using supervoxels instead of millimeter - scale voxels ) , to possible differences in brain activation over different
in a second set of experiments we directly compared training single - subject versus multiple - subject classiers , this time using identical training methods and identical en - codings for the classier input ( roiactiveaverage , using all voxels within seven manually selected rois ) .
in these experiments we used the same picture versus sentence data , but
mitchell et al .
table 123
errors for single - subject and multiple - subject classiers , when trained on p - then - s , and s - then - p data .
the third column shows the average error of classiers trained for single subjects .
the fourth column shows the error of multi - subject classiers applied to subjects withheld from the training set .
results are obtained using normalization .
all classiers are trained based upon averaging all voxels in an roi into a supervoxel .
123% condence intervals are computed under the assumption that test examples are i . i . d .
bernoulli distributed .
the error of a random classier is 123 .
this time we partitioned the data into two disjoint subsets : trials in which the sentence was presented before the picture ( which we will refer to as s - then - p ) , and trials in which the picture was presented before the sentence ( which we will call p - then - s ) .
notice that separating the data in this fashion results in an easier classication problem , because all examples of one stimulus ( e . g . , sentences ) occur in the same temporal context ( e . g . , fol - lowing only the rest period in the s - then - p dataset , or following only the picture stimulus in the p - then - s dataset ) , and hence exhibit less variability .
for each of these two data sub - sets we trained both multiple - subject and single - subject classiers , using gnb , svm , and knn classiers , and employing roiactiveaverage feature selection with all voxels in the selected rois .
the results are summarized in table 123
note for comparison we present both the leave - 123 - subject - out error of the multiple - subject classiers , and the average leave - one - example - per - class - out error of the corresponding single - subject classiers .
as in the rst experiment , the multiple - subject classiers achieve accuracies signicantly greater than the 123 expected from random guessing .
interestingly , the multiple - subject classiers achieve error rates comparable to those of the single - subject classiers , and in a few cases achieve error rates superior to those of the single - subject classiersdespite the fact that the multiple - subject classiers are being evaluated on subjects outside the training set .
presumably this better performance by the multiple - subject classier can be explained by the fact that it is trained using an order of magnitude more training examples , from twelve subjects rather than one .
we interpret these results as strong support for the feasibility of training high accuracy classiers that apply to novel human subjects .
note these classiers are generally more accurate than those in the rst experiment , presumably due to the easier classication task as discussed above .
learning to decode cognitive states from brain images
syntactic ambiguity study .
we also attempted to train multiple - subject classiers for the syntactic ambiguity study , to discriminate whether the subject was reading an am - biguous or non - ambiguous sentence .
in this case , the error of the multi - subject classier was 123 123 under leave - one - subject - out cross validation , and correspondingly the average error of single - subject classiers is 123 123 .
the setting which produced this result was using the gnb classier , minimum - maximum normalization , and the feature selection method roiactiveavg , averaging the 123 most active voxels from each of four pre - dened rois ( left and right brocca , and left and right temporal regions ) into supervoxels .
these errors are signicantly better than expected from a random classier , 123 .
unlike the sentence versus picture study , however , these results are quite sensitive to the partic - ular selection of learning method and feature selection .
although we cannot draw strong conclusions from this result , it does provide modest additional support for the feasibility of training multiple - subject classiers .
summary and conclusions
we have presented results from three different fmri studies demonstrating the feasibility of training classiers to distinguish a variety of cognitive states , based on single - interval fmri observations .
this problem is interesting both because of its relevance to studying human cognition , and as a case study of machine learning in high dimensional , noisy , sparse data
our comparison of classiers indicates that gaussian naive bayes ( gnb ) and linear support vector machine ( svm ) classiers outperform k nearest neighbor across all three studies , and that feature selection methods consistently improve classication error in all three studies .
in comparing gnb to svm , we found trends consistent with the observations in ng and jordan ( 123 ) , that the relative performance of generative versus discriminative classiers depends in a predictable fashion on the number of training examples and data dimension .
in particular , our experiments are consistent with the hypothesis that the accuracy of svms increases relatively more quickly than the accuracy of gnb as the data dimension is reduced via feature selection , and as the number of training examples increases .
feature selection is an important aspect in the design of classiers for high dimensional , sparse , noisy data .
we dened a new classier setting ( the zero signal setting ) that captures an important aspect of our fmri classication problem , as well as a variety of other classi - cation problems involving sensor data .
in this setting , the available data includes not only examples of the classes to be discriminated ( e . g . , data when the subject is viewing a picture or a sentence ) , but also a class of zero signal data ( e . g . , when the subject is viewing neither a picture nor a sentence , but is simply xating on the screen ) .
our experiments show that within our domain activity - based feature selection methods which take advantage of this zero signal data consistently outperform traditional discrimination - based feature selection methods that use only data from the target classes .
our data and our analysis also suggest that the relative benet of activity - based versus discrimination - based feature selection will increase as data becomes more sparse , more noisy , higher dimensional , and as the fraction of relevant features decreases .
as is clear from our description of the fmri data sets , this domain represents a fairly extreme point along all four of these dimensions .
we plan further
mitchell et al .
research to develop a more precise formal model of this zero signal setting , and to develop and experiment with feature selection strategies tuned to take maximal advantage of this
in addition to training classiers to detect cognitive states in single subjects , we also ex - plored the feasibility of training cross - subject classiers to make predictions across multiple human subjects .
in this case , we found it useful to abstract the fmri data by using the mean fmri activity in each of several anatomically dened brain regions .
using this approach , it was possible to train classiers to distinguish , e . g . , whether the subject was viewing a picture or a sentence describing a picture , and to apply these successfully to subjects outside the training set .
in some cases , the classication accuracy for subjects outside the training set equalled the accuracy achieved by training on data from just this single subject .
given this success in training multiple - subject classiers , we plan additional research to explore a number of alternative approaches to cross - subject classication ( e . g . , instead of abstracting the data for each subject , map the different brain structures to a standard coordinate system such as talairach coordinates ) .
there are many additional opportunities for machine learning research in the context of fmri data analysis .
for example , it would be useful to learn models of temporal behavior , in contrast to the work reported here which considers only data at a single time or time interval .
machine learning methods such as hidden markov models and dynamic bayesian networks appear relevant to this problem .
a second research direction is to develop learning methods that take advantage of data from multiple studies , in contrast to the single study efforts described here .
in our own lab , for example , we have accumulated fmri data from over 123 human subjects .
in order to develop learning methods to take advantage of such data , it will be necessary to address both how to combine data from multiple subjects and how to combine data from subjects presented with differing stimuli .
a third research topic is to develop machine learning methods that could take as a starting point computational cognitive models of human processing , such as act - r ( anderson et al . , 123 ) and 123caps ( just , carpenter , & varma , 123 ) , using these as prior knowledge for guiding the analysis of fmri data , and automatically rening these models to better t observed experimental
as with many real - world machine learning case studies , our exploration of the fmri problem domain has drawn on lessons learned from previous research in machine learning , and has yielded new lessons of its own .
given that fmri is a problem involving very high dimensional , sparse data sets , we drew heavily on previously learned lessons from similar domains such as text classication .
this led us to employ svm , gnb , and knn classication algorithms that have previously proven useful in such domains , and led us to aggressively explore feature selection methods for reducing the dimension of the data .
the most signicant new insight about learning to arise from our fmri studies thus far is the identication of the zero - signal learning setting and the development of new and highly effective feature selection methods for this setting .
in particular , our discovery of the unexpected dominance of activity - based feature selection methods over commonly used discrimination - based methods was an essential step toward training successful classiers in this domain , and suggests that similar feature selection approaches may be useful in other high dimensional , sparse domains that t the zero - signal learning setting .
looking
learning to decode cognitive states from brain images
forward , we expect machine learning methods to have an increasing impact on the analysis of fmri data as this eld matures , and foresee additional opportunities for the fmri domain to drive novel machine learning research , especially in problems related to discovery of representations for merging data from multiple subjects , and in learning temporal models of cognitive processes .
we are grateful to luis j .
barrios for helpful discussions and detailed comments on various drafts of this paper .
thanks to vladimir cherkassky and joel welling for useful observations and suggestions during the course of this work , and to paul bennett for many helpful discussions and for writing part of the code used for the semantic categories study .
we are also grateful for the detailed comments of two anonymous reviewers , which led to signicant improvements to the nal version of this paper .
radu stefan niculescu was supported by a graduate fellowship from the merck compu - tational biology and chemistry program at carnegie mellon university established by the merck company foundation and by national science foundation ( nsf ) grant no .
ccr - 123
francisco pereira was funded by the center for neural basis of cognition , a praxis xxi scholarship from fundacao para a ciencia e tecnologia , portugal ( iii quadro comunitario de apoio , comparticipado pelo fundo social europeu ) and a phd scholar - ship from fundacao calouste gulbenkian , portugal .
rebecca hutchinson was supported by an nsf graduate fellowship .
support for collecting the fmri data sets was provided by grant number n123 - 123 - 123 - 123 from the multidisciplinary university research initiative ( muri ) of the ofce of the secretary of defense .
fiasco is available at http : / / www . stat . cmu . edu / asco .
notice this classication task is made more difcult by the fact that the rst stimulus is always presented for
four seconds , whereas the second stimulus is terminated as soon as the subject responds with a button press .
the experiment included four types of sentences .
we consider here only two types , corresponding to the most
and least ambiguous .
