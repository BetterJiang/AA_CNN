how can artificial neural nets generalize better from fewer examples ? in order to generalize successfully , neural network learning methods typically require large training data sets .
we introduce a neural network learning method that generalizes rationally from many fewer data points , relying instead on prior knowledge encoded in previously learned neural networks .
for example , in robot control learning tasks reported here , previously learned networks that model the effects of robot actions are used to guide subsequent learning of robot control functions .
for each observed training example of the target function ( e . g .
the robot control policy ) , the learner explains the observed example in terms of its prior knowledge , then analyzes this explanation to infer additional information about the shape , or slope , of the target function .
this shape knowledge is used to bias generalization when learning the target function .
results are presented applying this approach to a simulated robot task based on reinforcement learning .
neural network learning methods generalize from observed training data to new cases based on an inductive bias that is similar to smoothly interpolating between observed training points .
theoretical results ( valiant , 123 ) , ( baum and haussler , 123 ) on learnability , as well as practical experience , show that such purely inductive methods require significantly larger training data sets to learn functions of increasing complexity .
this paper introduces explanation - based neural network learning ( ebnn ) , a method that generalizes successfully from fewer training examples , relying instead on prior knowledge encoded in previously learned neural networks .
ebnn is a neural network analogue to symbolic explanation - based learning methods ( ebl ) ( dejong and mooney , 123 ) , ( mitchell et al . , 123 symbolic ebl methods generalize based upon pre - specified domain knowledge represented by collections of symbolic rules .
mitchell and thrun
for example .
in the task of learning general rules for robot control ebl can use prior know ledge about the effects of robot actions to analytically generalize from specific training examples of successful control actions .
this is achieved by a .
observing a sequence of states and actions leading to some goal .
explaining ( i . e .
post - facto predicting ) the outcome of this sequence using the domain theory .
then c .
analyzing this explanation in order to determine which features of the initial state are relevant to achieving the goal of the sequence .
and which are nol in previous approaches to ebl .
the initial domain knowledge has been represented symbolically .
typically by propositional rules or hom clauses .
and has typically been assumed to be complete and correct
123 ebnn : integrating inductive and analytical learning
ebnn extends explanation - based learning to cover situations in which prior knowledge ( also called the domain theory ) is approximate and is itself123earned from scratch .
in ebnn , this domain theory is represented by real - valued neural networks .
by using neural network representations .
it becomes possible to learn the domain theory using training algorithms such as the backpropagation algorithm ( rumelhart et al . , 123 in the robot domains addressed in this paper .
such domain theory networks correspond to action models .
i . e . , networks that model the effect of actions on the state of the world m : s x a - + sf ( here a denotes an action , s a state .
and sf the successor state ) .
this domain theory is used by ebnn to bias the learning of the ' robot control function .
because the action models may be only approximately correct .
we require that ebnn be robust with respect to severe errors in the domain theory .
the remainder of this section describes the ebnn learning algorithm .
assume that the robot agent ' s action space is discrete .
and that its domain knowledge is represented by a collection of pre - trained action models mi : s - + sf .
one for each discrete action i .
the learning task of the robot is to learn a policy for action selection that maximizes the reward , denoted by r .
which defines the task .
more specifically .
the agent has to learn an evaluation function q ( s , a ) .
which measures the cumulativefuture expected reward when action a is executed at state s .
once learned .
the function q ( s , a ) allows the agent to select actions that maximize the reward r ( greedy policy ) .
hence learning control reduces to learning the evaluation function q .
123 how can the agent use its previously learned action models to focus its learning of q ? to illustrate .
consider the episode shown in figure 123
the ebnn learning algorithm for learning the target function q consists of two components , an inductive learning component and an analytical learning component .
123 the inductive component of ebnn the observed episode is used by the agent to construct training examples , denoted by q , for the evaluation function q :
q could for example be realized by a monolithic neural network , or by a collection of net ( cid : 123 ) works trained with the backpropagation training procedure .
as observed training episodes are accumulated , q will become increasingly accurate .
such pure inductive learning typ -
ithis approach to learning a policy is adopted from recent research on reinforcenuml learning
( barto et al . , 123 ) .
explanation - based neural network learning for robot control
_ - - - - - ~ reward : r
figure 123 : episode : starting with the initial state si .
the action sequence ai , az , a123 was observed to produce the final reward r .
the domain knowledge represented by neural network action models is used to post - facto predict and analyze each step of the observed episode .
ically requires large amounts of training data ( which will be costly in the case of robot
123 the analytical component or ebnn
in ebnn , the agent exploits its domain lrnowledge to extract additional shape lrnowledge about the target function q .
to speed convexgence and reduce the number of training examples required .
this shape lrnowledge .
represented by the estimated slope of the target function q .
is then used to guide the generalization process .
more specifically .
ebnn combines the above inductive learning component with an analytical learning component that performs the following three steps for each observed training episode :
explain : post - facto predict the obsexved episode ( states and final reward ) , using the action models mi ( c . f .
note that thexe may be a deviation between predicted and observed states .
since the domain lrnowledge is only approximately correct .
analyze : analyze the explanation to estimate the slope of the target function for each observed state - action pair ( 123 : , a123 : ) ( k = 123 . 123 ) .
extract the derivative of the final reward r with respect to the features of the states 123 : .
according to the action models mi .
for instance .
consider the explanation of the episode shown in fig .
the domain theory networks mi represent differentiable functions .
therefore it is possible to extract the derivative of the final reward r with respect to the preceding state 123
denoted by " v ' 123r .
using the chain rule of differentiation .
the derivatives of the final reward r with respect to all states 123 : can be extracted .
these derivatives " v , , , r describe the dependence of the final reward upon features of the previous states .
they provide the target slopes .
denoted by " v , , , q .
for the target function q :
" v ' 123 q 123 , a123 )
" v ' 123 r
om123 , ( 123 ) om123 ( 123 )
om123 ( 123 ) 123m123 ( 123 ) 123m123 ( 123 )
learn : update the learned target function to better fit both the target values and target slopes .
123 illustrates training information extracted by both the inductive ( values ) and the analytical ( slopes ) components ofebnn .
assume that the " true " q - function
mitchell and thrun
figure123 : fitting slopes : let / bea target function for which tbreeexampies ( xl , i ( xt ) ) .
( x123 , i ( x123 ) ) .
and ( x123 , 123 ( x123 ) ) are known .
based on these points the learner might generate the hypothesis g .
if the slopes are also known .
the learner can do much better : h .
is shown in fig .
123a , and that three training instances at xl , x123 and x123 are given .
when only values are used for learning , i . e . , as in standard inductive learning , the learner might conclude the hypothesis g depicted in fig .
if the slopes are known as well , the learner can better estimate the target function ( fig .
from this example it is clear that the analysis in ebnn may reduce the need for training data , provided that the estimated slopes extracted from the explanations are sufficiently accurate .
in ebnn , the function q is learned by a real - valued function approximator that fits both the target values and target slopes .
if this approximator is a neural network , an extended version of the backpropagation algorithm can be employed to fit these slope constraints as well , as originally shown by ( simard et al . , 123 their algorithm " tangent prop " extends the backpropagation error function by a second term measuring the mean square error of the slopes .
gradient descent in slope space is then combined with backpropagation to minimize both error functions .
in the experiments reported here , however , we used an instance - based function approximation technique described in
123 accommodating imperfect domain theories
notice that the slopes extracted from explanations will be only approximately correct , since they are derived from the approximate action models mi .
if this domain knowledge is weak , the slopes can be arbitrarily poor , which may mislead generalization .
ebnn reduces this undesired effect by estimating the accuracy of the extracted slopes and weighting the analytical component of learning by these estimated slope accuracies .
generally speaking , the accuracy of slopes is estimated by the prediction accuracy of the explanation ( this heuristic has been named lob * ) .
more specifically , each time the domain theory is used to post - facto predict a state sk+123 , its prediction st ~ icted may deviate from the observed state sr+ied hence the i - step prediction accuracy at state sk , denoted by cl ( i ) , is defined as 123 minus the normalized prediction error :
123 _ ii st ~ cted - skb+red ii
for a given episode we define the n - step accuracy cn ( i ) as the product of the i - step accuracies in the next n steps .
the n - step accuracy , which measures the accuracy of the deri ved slopes n steps away from the end of the episode , posseses three desireable properties : a .
it is i if the learned domain theory is perfectly correct , b .
it decreases monotonically as the length of the chain of inferences increases , and c .
it is bounded below by o .
the n - step accuracy is used to determine the ratio with which the analytical and inductive components
explanation - based neural network learning for robot control
are weighted when learning the target concept .
if an observation is n steps away from the end of the episode .
the analytically derived training information ( slopes ) is weighted by the n - step accuracy times the weight of the inductive component ( values ) .
although the experimental results reported in section 123 are promising .
the generality of this approach is an open question .
due to the heuristic nature of the assumption lob * .
123 ebnn and reinforcement learning
to make ebnn applicable to robot learning , we extend it here to a more sophisticated scheme for learning the evaluation function q .
namely watkins ' q - learning ( watkins , 123 ) combined with sutton ' s temporal difference methods ( sutton , 123 the reason for doing so is the problem 123 / suboptimal action choices in robot learning : robots must explore their environment .
i . e . , they must select non - optimal actions .
such non - optimal actions can have a negative impact on the final reward of an episode which results in both underestimating target values and misleading slope estimates .
watkins ' q - learning ( watkins , 123 ) permits non - optimal actions during the course of in his algorithm targets for q are constructed recursively , based on the maximum possible q - value at the next state : 123
q ( sk , ak ) =
if k is the final step and r final reward
, m ~ q ( sk+l , a ) otherwise
here , ( o ~ , ~ i ) is a discount / actor that discounts reward over time , which is commonly used for minimizing the number of actions .
sutton ' s td ( a ) ( sutton , 123 ) can be used to combine both watkins ' q - learning and the non - recursive q - estimation scheme underlying the previous section .
here the parameter a ( 123 ~ a ~ 123 ) determines the ratio between recursive and non - recursive components :
if k final step ( i - a ) , max a q ( sk+l , a ) + a , q ( sk+l , ak+d otherwise ( 123 eq .
( 123 ) describes the extended inductive component of the ebnn learning algorilhm .
the extension of the analytical component in ebnn is straightforward .
slopes are extracted via the derivative of eq .
( 123 ) , which is computed via the derivative of both the models ! iii and the derivative of q .
if k last step
123 experimental results
ebnn has been evaluated in a simulated robot navigation domain .
the world and the action space are depicted in fig .
the learning task is to find a q function , for which the greedy policy navigates the agent to its goal location ( circle ) from arbitrary starting locations , while avoiding collisions with the walls or the obstacle ( square ) .
states are
123 order to simplify the notation .
we assume that reward is only received at the end of the episode ,
and is also modeled by the action models .
the extension to more general cases is straightforward .
mitchell and thrun
figure 123 : a .
the simulated robot world .
actions .
the squared generalization error of the domain theory networks decreases monotonically as the amount of training data increases .
these nine alternative domain theories were used in the experiments .
described by the local view of the agent .
in terms of distances and angles to the center of the goal and to the center of the obstacle .
note that the world is deterministic in these experiments , and that there is no sensor noise .
we applied watkins ' q - learning and td ( ~ ) as described in the previous section with a=123 and a discount factor ;=123 .
each of the five actions was modeled by a separate neural network ( 123 hidden units ) and each had a separate q evaluation function .
the latter functions were represented by a instance - based local approximation technique .
in a nutshell , this technique memorizes all training instances and their slopes explicitly , and fits a local quadratic model over the ( =123 nearest neighbors to the query point , fitting both target values and target slopes .
we found empirically that this technique outperformed tangent prop in the domain at hand . 123 we also applied an experience replay technique proposed by lin ( lin , 123 in order to optimally exploit the information given by the observed training fig .
123 shows average performance curves for ebnn using nine different domain theories ( action models ) trained to different accuracies , with ( fig .
123a ) and without ( fig .
123b ) taking the n - step accuracy of the slopes into account .
123a shows the main result .
it shows clearly that ( 123 ) ebnn outperfonns purely inductive learning , ( 123 ) more accurate domain theories yield better performance than less accurate theories , and ( 123 ) ebnn learning de ( cid : 123 ) grades gracefully as the accuracy of the domain theory decreases , eventually matching the performance of purely inductive learning .
in the limit , as the size of the training data set grows , we expect all methods to converge to the same asymptotic performance .
explanation - based neural network learning .
compared to purely inductive learning , gen ( cid : 123 ) eralizes more accurately from less training data .
it replaces the need for large training data sets by relying instead on a previously learned domain theory .
represented by neural networks .
in this paper , ebnn has been described and evaluated in terms of robot learning tasks .
because the learned action models mi are independent of the particular control task ( reward function ) , this knowledge acquired during one task transfers directly to other tasks .
123note that in a second experiment not reported here , we applied ebnn using neural network
representation for q and tangent prop successfully in a real robot domain .
explanation - based neural network learning for robot control
" . . . . .
figure 123 : how does domain knowledge improve generalization ? a .
averaged results for ebnn domain theories of differing accuracies , pre - trained with from 123 to 123 123 training examples for each action model network .
in contrast , the bold grey line reflects the learning curve for pure inductive learning , i . e . , q - leaming and td ( a ) .
same experiments , but without weighting the analytical component of ebnn by its accuracy , illustrating the importance of the wb* heuristic .
all curves are averaged over 123 runs and are also locally window - averaged .
the perfonnance ( vertical axis ) is measured on an independent test set of starting positions .
ebnn differs from other approaches to knowledge - based neural network learning , such as shavlik / fowell ' s kbanns ( shavlik and towell , 123 ) .
in that the domain knowledge and the target function are strictly separated , and that both are learned from scratch .
a major difference from other model - based approaches to robot learning , such as sutton ' s dyna architecture ( sutton , 123 ) or jordan ! rumelhart ' s distal teacher method ( jordan and rumelhart , 123 ) , is the ability of ebnn to operate across the spectrum of strong to weak domain theories ( using lob* ) .
ebnn has been found to degrade gracefully as the accuracy of the domain theory decreases .
we have demonstrated the ability of ebnn to transfer knowledge among robot learning tasks .
however , there are several open questions which will drive future research , the most significant of which are : a .
can ebnn be extended to real - valued , parameterized
mitchell and thrun
action spaces ? so far we assume discrete actions .
can ebnn be extended to handle first - order predicate logic .
which is common in symbolic approaches to ebl ? c .
how will ebnn perform in highly stochastic domains ? d .
can knowledge other than slopes ( such as higher order derivatives ) be extracted via explanations ? e .
is it feasible to automatically partition / modularize the domain theory as well as the target function , as this is the case with symbolic ebl methods ? more research on these issues is warranted .
we thank ryusuke masuoka , long - ji lin .
the cmu robot learning group .
jude shavlik , and mike jordan for invaluable discussions and suggestions .
this research was sponsored in part by the avionics lab , wright research and development center .
aeronautical systems division ( apsc ) .
air force .
wright - patterson afb .
oh 123 - 123 under contract f123 - 123 - c - 123arpa order no .
123 and by a grant from siemens corporation .
