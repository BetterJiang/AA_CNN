abstract .
this paper shows that the accuracy of learned text classiers can be improved by augmenting a small number of labeled training documents with a large pool of unlabeled documents .
this is important because in many text classication problems obtaining training labels is expensive , while large quantities of unlabeled documents are readily available .
we introduce an algorithm for learning from labeled and unlabeled documents based on the combination of expectation - maximization ( em ) and a naive bayes classier .
the algorithm rst trains a classier using the available labeled documents , and probabilistically labels the unlabeled documents .
it then trains a new classier using the labels for all the documents , and iterates to convergence .
this basic em procedure works well when the data conform to the generative assumptions of the model .
however these assumptions are often violated in practice , and poor performance can result .
we present two extensions to the algorithm that improve classication accuracy under these conditions : ( 123 ) a weighting factor to modulate the contribution of the unlabeled data , and ( 123 ) the use of multiple mixture components per class .
experimental results , obtained using text from three different real - world tasks , show that the use of unlabeled data reduces classication error by up to 123% .
combining labeled and unlabeled data , bayesian learning
text classication , expectation - maximization , integrating supervised and unsupervised learning ,
consider the problem of automatically classifying text documents .
this problem is of great practical importance given the massive volume of online text available through the world wide web , internet news feeds , electronic mail , corporate databases , medical patient records and digital libraries .
existing statistical text learning algorithms can be trained to approximately classify documents , given a sufcient set of labeled training examples .
these text classication algorithms have been used to automatically catalog news articles ( lewis & gale , 123; joachims , 123 ) and web pages ( craven et al . , 123; shavlik & eliassi - red ,
nigam et al .
123 ) , automatically learn the reading interests of users ( pazzani , muramatsu , & billsus , 123; lang , 123 ) , and automatically sort electronic mail ( lewis & knowles , 123; sahami et al . , 123 ) .
one key difculty with these current algorithms , and the issue addressed by this paper , is that they require a large , often prohibitive , number of labeled training examples to learn accu - rately .
labeling must often be done by a person; this is a painfully time - consuming process .
take , for example , the task of learning which usenet newsgroup articles are of interest to a particular person reading usenet news .
systems that lter or pre - sort articles and present only the ones the user nds interesting are highly desirable , and are of great commercial interest today .
work by lang ( 123 ) found that after a person read and labeled about 123 articles , a learned classier achieved a precision of about 123% when making predictions for only the top 123% of documents about which it was most condent .
most users of a practical system , however , would not have the patience to label a thousand articlesespecially to obtain only this level of precision .
one would obviously prefer algorithms that can provide accurate classications after hand - labeling only a few dozen articles , rather than thousands .
the need for large quantities of data to obtain high accuracy , and the difculty of obtaining labeled data , raises an important question : what other sources of information can reduce the need for labeled data ?
this paper addresses the problem of learning accurate text classiers from limited num - bers of labeled examples by using unlabeled documents to augment the available labeled documents .
in many text domains , especially those involving online sources , collecting unlabeled documents is easy and inexpensive .
the ltering task above , where there are thousands of unlabeled articles freely available on usenet , is one such example .
it is the labeling , not the collecting of documents , that is expensive .
how is it that unlabeled data can increase classication accuracy ? at rst consideration , one might be inclined to think that nothing is to be gained by access to unlabeled data .
however , they do provide information about the joint probability distribution over words .
suppose , for example , that using only the labeled data we determine that documents con - taining the word homework tend to belong to the positive class .
if we use this fact to estimate the classication of the many unlabeled documents , we might nd that the word lecture occurs frequently in the unlabeled examples that are now believed to belong to the positive class .
this co - occurrence of the words homework and lecture over the large set of unlabeled training data can provide useful information to construct a more accurate classier that considers both homework and lecture as indicators of positive examples .
in this paper , we explain that such correlations are a helpful source of information for increasing classication rates , specically when labeled data are scarce .
this paper uses expectation - maximization ( em ) to learn classiers that take advantage of both labeled and unlabeled data .
em is a class of iterative algorithms for maximum like - lihood or maximum a posteriori estimation in problems with incomplete data ( dempster , laird , & rubin , 123 ) .
in our case , the unlabeled data are considered incomplete because they come without class labels .
the algorithm rst trains a classier with only the available labeled documents , and uses the classier to assign probabilistically - weighted class labels to each unlabeled document by calculating the expectation of the missing class labels .
it then trains a new classier using all the documentsboth the originally labeled and the formerly unlabeledand iterates .
in its maximum likelihood formulation , em performs hill - climbing
text classification using em
in data likelihood space , nding the classier parameters that locally maximize the likeli - hood of all the databoth the labeled and the unlabeled .
we combine em with naive bayes , a classier based on a mixture of multinomials , that is commonly used in text classication .
we also propose two augmentations to the basic em scheme .
in order for basic em to improve classier accuracy , several assumptions about how the data are generated must be satised .
the assumptions are that the data are generated by a mixture model , and that there is a correspondence between mixture components and classes .
when these assumptions are not satised , em may actually degrade rather than improve classier accuracy .
since these assumptions rarely hold in real - world data , we propose extensions to the basic em / naive - bayes combination that allow unlabeled data to still improve classication accuracy , in spite of violated assumptions .
the rst extension introduces a weighting factor that dynamically adjusts the strength of the unlabeled datas contribution to parameter estimation in em .
the second reduces the bias of naive bayes by modeling each class with multiple mixture components , instead of a single component .
over the course of several experimental comparisons , we show that ( 123 ) unlabeled data can signicantly increase performance , ( 123 ) the basic em algorithm can suffer from a mist between the modeling assumptions and the unlabeled data , and ( 123 ) each extension mentioned above often reduces the effect of this problem and improves classication .
the reduction in the number of labeled examples needed can be dramatic .
for example , to identify the source newsgroup for a usenet article with 123% classication accuracy , a tradi - tional learner requires 123 labeled examples; alternatively our algorithm takes advantage of 123 unlabeled examples and requires only 123 labeled examples to achieve the same accuracy .
thus , in this task , the technique reduces the need for labeled training examples by more than a factor of three .
with only 123 labeled documents ( two per class ) , accuracy is improved from 123% to 123% by adding unlabeled data .
these ndings illustrate the power of unlabeled data in text classication problems , and also demonstrate the strength of the algorithms proposed here .
the remainder of the paper is organized as follows .
section 123 describes , from a theoretical point of view , the problem of learning from labeled and unlabeled data .
sections 123 and 123 present the formal framework for naive bayes .
in section 123 , we present the combination of em and naive bayes , and our extensions to this algorithm .
section 123 describes a systematic experimental comparison using three classication domains : newsgroup articles , web pages , and newswire articles .
the rst two domains are multi - class classication problems where each class is relatively frequent .
the third domain is treated as binary classication , with the positive class having a frequency between 123% and 123% , depending on the task .
related work is discussed in section 123
finally , advantages , limitations , and future research directions are discussed in section 123
argument for the value of unlabeled data
how are unlabeled data useful when learning classication ? unlabeled data alone are generally insufcient to yield better - than - random classication because there is no infor - mation about the class label ( castelli & cover , 123 ) .
however , unlabeled data do contain information about the joint distribution over features other than the class label .
because of
nigam et al .
figure 123
classication by a mixture of gaussians .
if unlimited amounts of unlabeled data are available , the mixture components can be fully recovered , and labeled data are used to assign labels to the individual components , converging exponentially quickly to the bayes - optimal classier .
this they can sometimes be usedtogether with a sample of labeled datato signicantly increase classication accuracy in certain problem settings .
to see this , consider a simple classication problemone in which instances are gener - ated using a gaussian mixture model .
here , data are generated according to two gaussian distributions , one per class , whose parameters are unknown .
figure 123 illustrates the bayes - optimal decision boundary ( x > d ) , which classies instances into the two classes shown by the shaded and unshaded areas .
note that it is possible to calculate d from bayes rule if we know the gaussian mixture distribution parameters ( i . e . , the mean and variance of each gaussian , and the mixing parameter between them ) .
consider when an innite amount of unlabeled data is available , along with a nite number of labeled samples .
it is well known that unlabeled data alone , when generated from a mixture of two gaussians , are sufcient to recover the original mixture components ( mclachlan & krishnan , 123 , section 123 ) .
however , it is impossible to assign class labels to each of the gaussians without any labeled data .
thus , the remaining learning problem is the problem of assigning class labels to the two gaussians .
for instance , in gure 123 , the means , variances , and mixture parameter can be learned with unlabeled data alone .
labeled data must be used to determine which gaussian belongs to which class .
this problem is known to converge exponentially quickly in the number of labeled samples ( castelli & cover , 123 ) .
informally , as long as there are enough labeled examples to determine the class of each component , the parameter estimation can be done with unlabeled data
it is important to notice that this result depends on the critical assumption that the data indeed have been generated using the same parametric model as used in classication , something that almost certainly is untrue in real - world domains such as text classication .
this raises the important empirical question as to what extent unlabeled data can be useful in practice in spite of the violated assumptions .
in the following sections we address this by describing in detail a parametric generative model for text classication and by presenting empirical results using this model on real - world data .
text classification using em
the probabilistic framework
this section presents a probabilistic framework for characterizing the nature of documents and classiers .
the framework denes a probabilistic generative model for the data , and embodies two assumptions about the generative process : ( 123 ) the data are produced by a mixture model , and ( 123 ) there is a one - to - one correspondence between mixture components and classes . 123 the naive bayes text classier we will discuss later falls into this framework , as does the example in section 123
in this setting , every document is generated according to a probability distribution dened by a set of parameters , denoted ( cid : 123 ) .
the probability distribution consists of a mixture of components c j 123c dfc123; : : : ; cjcjg .
each component is parameterized by a disjoint subset of ( cid : 123 ) .
a document , di , is created by rst selecting a mixture component according to the mixture weights ( or class prior probabilities ) , p . c j j ( cid : 123 ) / , then having this selected mixture component generate a document according to its own parameters , with distribution p . di j c ji ( cid : 123 ) / . 123 thus , we can characterize the likelihood of document di with a sum of total probability over all
p . di j ( cid : 123 ) / d
p . c j j ( cid : 123 ) / p . di j c ji ( cid : 123 ) / :
each document has a class label .
we assume that there is a one - to - one correspondence between mixture model components and classes , and thus ( for the time being ) use c j to indicate the jth mixture component as well as , the jth class .
the class label for a particular document di is written yi .
if document di was generated by mixture component c j we say yi d c j .
the class label may or may not be known for a given document .
text classication with naive bayes
this section presents naive bayesa well - known probabilistic classierand describes its application to text .
naive bayes is the foundation upon which we will later build in order to incorporate unlabeled data .
the learning task in this section is to estimate the parameters of a generative model using labeled training data only .
the algorithm uses the estimated parameters to classify new documents by calculating which class was most likely to have generated the given document .
the generative model
naive bayes assumes a particular probabilistic generative model for text .
the model is a specialization of the mixture model presented in the previous section , and thus also makes the two assumptions discussed there .
additionally , naive bayes makes word independence assumptions that allow the generative model to be characterized with a greatly reduced number of parameters .
the rest of this subsection describes the generative model more formally , giving a precise specication of the model parameters , and deriving the probability that a particular document is generated given its class label ( eq
nigam et al .
first let us introduce some notation to describe text .
a document , di , is considered to be an ordered list of word events , hwdi;123 ; : : : i .
we write wdi;k for the word wt in position k of document di , where wt is a word in the vocabulary v d hw123; w123; : : : ; wjvji .
when a document is to be generated by a particular mixture component , c j , a document length , jdij , is chosen independently of the component .
( note that this assumes that doc - ument length is independent of class . 123 ) then , the selected mixture component generates a word sequence of the specied length .
we furthermore assume it generates each word independently of the length .
thus , we can expand the second term from eq .
( 123 ) , and express the probability of a document given a mixture component in terms of its constituent features : the document length and the words in the document .
note that , in this general setting , the probability of a word event must be conditioned on all the words that precede it .
c ji ( cid : 123 )
c ji ( cid : 123 ) i wdi;q
; q < k
p . di j c ji ( cid : 123 ) / d p
; : : : ; wdi;jdi j
next we make the standard naive bayes assumption : that the words of a document are generated independently of context , that is , independently of the other words in the same document given the class label .
we further assume that the probability of a word is independent of its position within the document; thus , for example , the probability of seeing the word homework in the rst position of a document is the same as seeing it in any other position .
we can express these assumptions as :
c ji ( cid : 123 ) i wdi;q
; q < k
combining these last two equations gives the naive bayes expression for the probability
c ji ( cid : 123 ) c ji ( cid : 123 )
of a document given its class : p . di j c ji ( cid : 123 ) / d p . jdij /
thus the parameters of an individual mixture component are a multinomial distribution over words , i . e .
the collection of word probabilities , each written ( cid : 123 ) wtjc j , such that ( cid : 123 ) wtjc j t p . wt j c ji ( cid : 123 ) / d 123
since we assume that for p . wt j c ji ( cid : 123 ) / , where t d f123; : : : ;jvjg and all classes , document length is identically distributed , it does not need to be parameterized for classication .
the only other parameters of the model are the mixture weights ( class prior probabilities ) , written ( cid : 123 ) c j , which indicate the probabilities of selecting the different mixture components .
thus the complete collection of model parameters , ( cid : 123 ) , is a set of multinomials and prior probabilities over those multinomials : ( cid : 123 ) df ( cid : 123 ) wtjc j : wt 123 v ; c j 123 ci ( cid : 123 ) c j : c j 123 cg .
training a classier
learning a naive bayes text classier consists of estimating the parameters of the generative model by using a set of labeled training data , d d fd123; : : : ; djdjg .
this subsection derives a method for calculating these estimates from the training data .
text classification using em
the estimate of ( cid : 123 ) is written o ( cid : 123 ) .
naive bayes uses the maximum a posteriori estimate , thus nding arg max ( cid : 123 ) p . ( cid : 123 ) j d / .
this is the value of ( cid : 123 ) that is most probable given the evidence of the training data and a prior .
the parameter estimation formulae that result from this maximization are the familiar
ratios of empirical counts .
the estimated probability of a word given a class , o ( cid : 123 ) wtjc j , is simply the number of times word wt occurs in the training data for class c j , divided by the total number of word occurrences in the training data for that classwhere counts in both the numerator and denominator are augmented with pseudo - counts ( one for each word ) that come from the prior distribution over ( cid : 123 ) .
the use of this type of prior is sometimes referred to as laplace smoothing .
smoothing is necessary to prevent zero probabilities for infrequently
the word probability estimates o ( cid : 123 ) wtjc j are : id123 n . wt ; di / p . yi d c j j di / id123 n . ws ; di / p . yi d c j j di /
p . wt j c ji o ( cid : 123 ) / d
where n . wt ; di / is the count of the number of times word wt occurs in document di and where p . yi d c j j di / 123 f123; 123g as given by the class label .
the class prior probabilities , o ( cid : 123 ) c j , are estimated in the same manner , and also involve a
ratio of counts with smoothing :
p . c j j o ( cid : 123 ) / d 123 cpjdj
id123 p . yi d c j j di / jcj c jdj
dirichlet distribution : p . ( cid : 123 ) / / q
the derivation of these ratios of counts formulae comes directly from maximum a posteriori parameter estimation , and will be appealed to again later when deriving parameter estimation formulae for em and augmented em .
finding the ( cid : 123 ) that maximizes p . ( cid : 123 ) j d / is accomplished by rst breaking this expression into two terms by bayes rule : p . ( cid : 123 ) j d / / p . d j ( cid : 123 ) / p . ( cid : 123 ) / .
the rst term is calculated by the product of all the document likelihoods ( from eq .
the second term , the prior distribution over parameters , we represent by a / 123 / , where is a parameter that effects the strength of the prior , and is some constant greater than zero . 123 in this paper , we set d 123 , which ( with maximum a posteriori estimation ) is equivalent to laplace smoothing .
the whole expression is maximized by solving the system of partial derivatives of log . p . ( cid : 123 ) j d / / , using lagrange multipliers to enforce the constraint that the word probabilities in a class must sum to one .
this maximization yields the ratio of counts seen above .
c j123c . . ( cid : 123 ) c j
using a classier
given estimates of these parameters calculated from the training documents according to eqs .
( 123 ) and ( 123 ) , it is possible to turn the generative model backwards and calculate the probability that a particular mixture component generated a given document .
we derive this
nigam et al .
by an application of bayes rule , and then by substitutions using eqs .
( 123 ) and ( 123 ) :
p . yi d c j j dii o ( cid : 123 ) / d p . c j j o ( cid : 123 ) / p . di j c ji o ( cid : 123 ) / c ji o ( cid : 123 )
p . di j o ( cid : 123 ) /
p . c j j o ( cid : 123 ) / rd123 p . cr j o ( cid : 123 ) /
if the task is to classify a test document di into a single class , then the class with the highest posterior probability , arg max j p . yi d c j j dii o ( cid : 123 ) / , is selected .
note that all four assumptions about the generation of text documents ( mixture model , one - to - one correspondence between mixture components and classes , word independence , and document length distribution ) are violated in real - world text data .
documents are often mixtures of multiple topics .
words within a document are not independent of each other grammar and topicality make this so .
despite these violations , empirically the naive bayes classier does a good job of clas - sifying text documents ( lewis & ringuette , 123; craven et al . , 123; yang & pederson , 123; joachims , 123; mccallum et al . , 123 ) .
this observation is explained in part by the fact that classication estimation is only a function of the sign ( in binary classication ) of the function estimation ( domingos & pazzani , 123; friedman , 123 ) .
the word independence assumption causes naive bayes to give extreme ( almost 123 or 123 ) class probability estimates .
however , these estimates can still be poor while classication accuracy remains high .
the above formulation of naive bayes uses a generative model that accounts for the number of times a word appears in a document .
it is a multinomial ( or in language modeling terms , unigram ) model , where the classier is a mixture of multinomials ( mccallum & nigam , 123 ) .
this formulation has been used by numerous practitioners of naive bayes text classication ( lewis & gale , 123; joachims , 123; li & yamanishi , 123; mitchell , 123; mccallum et al . , 123; lewis , 123 ) .
however , there is another formulation of naive bayes text classication that instead uses a generative model and document representation in which each word in the vocabulary is a binary feature , and is modeled by a mixture of multi - variate bernoullis ( robertson & sparck - jones , 123; lewis , 123; larkey & croft , 123; koller & sahami , 123 ) .
empirical comparisons show that the multinomial formulation yields classiers with consistently higher accuracy ( mccallum & nigam , 123 ) .
incorporating unlabeled data with em
we now proceed to the main topic of this paper : how unlabeled data can be used to improve a text classier .
when naive bayes is given just a small set of labeled training
text classification using em
data , classication accuracy will suffer because variance in the parameter estimates of the generative model will be high .
however , by augmenting this small set with a large set of unlabeled data , and combining the two sets with em , we can improve the parameter
em is a class of iterative algorithms for maximum likelihood or maximum a posteri - ori estimation in problems with incomplete data ( dempster , laird , & rubin , 123 ) .
in our case , the unlabeled data are considered incomplete because they come without class applying em to naive bayes is quite straightforward .
first , the naive bayes parameters , o ( cid : 123 ) , are estimated from just the labeled documents .
then , the classier is used to assign probabilistically - weighted class labels to each unlabeled document by calculating expec - tations of the missing class labels , p . c j j dii o ( cid : 123 ) / .
next , new classier parameters , o ( cid : 123 ) , are estimated using all the documentsboth the originally and newly labeled .
these last two steps are iterated until o ( cid : 123 ) does not change .
as shown by dempster , laird , & rubin ( 123 ) , at each iteration , this process is guaranteed to nd model parameters that have equal or higher likelihood than at the previous iteration .
this section describes em and our extensions within the probabilistic framework of naive
bayes text classication .
basic em we are given a set of training documents d and the task is to build a classier in the form of the previous section .
however , unlike previously , in this section we assume that only some subset of the documents di 123 dl come with class labels yi 123 c , and for the rest of the documents , in subset du , the class labels are unknown .
thus we have a disjoint partitioning of d , such that d d dl ( du .
as in section 123 , learning a classier is approached as calculating a maximum a posteriori estimate of ( cid : 123 ) , i . e .
arg max ( cid : 123 ) p . ( cid : 123 ) / p . d j ( cid : 123 ) / .
consider the second term of the maximization , the probability of all the training data , d .
the probability of all the data is simply the product over all the documents , because each document is independent of the others , given the model .
for the unlabeled data , the probability of an individual document is a sum of total probability over all the classes , as in eq .
for the labeled data , the generating component is already given by labels yi , and we do not need to refer to all mixture componentsjust the one corresponding to the class .
thus , the probability of all the data is :
p . d j ( cid : 123 ) / d
p . c j j ( cid : 123 ) / p . di j c ji ( cid : 123 ) /
p . yi d c j j ( cid : 123 ) / p . di j yi d c ji ( cid : 123 ) / :
instead of trying to maximize p . ( cid : 123 ) j d / directly we work with log . p . ( cid : 123 ) j d / / instead , as a step towards making maximization ( by solving the system of partial derivatives ) tractable .
let l . ( cid : 123 ) j d / log . p . ( cid : 123 ) / p . d j ( cid : 123 ) / / .
then , using eq .
( 123 ) , we write
l . ( cid : 123 ) j d / d log . p . ( cid : 123 ) / / c
p . c j j ( cid : 123 ) / p . di j c ji ( cid : 123 ) / log . p . yi d c j j ( cid : 123 ) / p . di j yi d c ji ( cid : 123 ) / / :
nigam et al .
notice that this equation contains a log of sums for the unlabeled data , which makes a maximization by partial derivatives computationally intractable .
consider , though , that if we had access to the class labels of all the documentsrepresented as the matrix of binary indicator variables z , zi d hzi123; : : : ; zijcji , where zi j d 123 iff yi d c j else zi j d 123then we could express the complete log likelihood of the parameters , lc . ( cid : 123 ) j d; z / , without a log of sums , because only one term inside the sum would be non - zero .
lc . ( cid : 123 ) j di z / d log . p . ( cid : 123 ) / / c
zi j log . p . c j j ( cid : 123 ) / p . di j c ji ( cid : 123 ) / /
if we replace zi j by its expected value according to the current model , then eq .
( 123 ) bounds from below the incomplete log likelihood from eq .
this can be shown by an application of jensens inequality ( e . g .
e ( log . x / ) log . e ( x ) / / .
as a result one can nd a locally maximum o ( cid : 123 ) by a hill climbing procedure .
this was formalized as the expectation - maximization ( em ) algorithm by dempster , laird , & rubin ( 123 ) .
the iterative hill climbing procedure alternately recomputes the expected value of z and the maximum a posteriori parameters given the expected value of z , e ( z ) .
note that for the labeled documents zi is already known .
it must , however , be estimated for the unlabeled documents .
let oz . k / and o ( cid : 123 ) . k / denote the estimates for z and ( cid : 123 ) at iteration k .
then , the algorithm nds a local maximum of l . ( cid : 123 ) j d / by iterating the following two steps : e - step : set oz . kc123 / d e ( zjdi o ( cid : 123 ) . k / ) .
m - step : set o ( cid : 123 ) . kc123 / d arg max ( cid : 123 ) p . ( cid : 123 ) jdi oz . kc123 / / .
in practice , the e - step corresponds to calculating probabilistic labels p . c j j dii o ( cid : 123 ) / for the unlabeled documents by using the current estimate of the parameters , o ( cid : 123 ) , and eq .
the m - step , maximizing the complete likelihood equation , corresponds to calculating a new maximum a posteriori estimate for the parameters , o ( cid : 123 ) , using the current estimates for p . c j j dii o ( cid : 123 ) / , and eqs .
( 123 ) and ( 123 ) .
our iteration process is initialized with a priming m - step , in which only the labeled documents are used to estimate the classier parameters , o ( cid : 123 ) , as in eqs .
( 123 ) and ( 123 ) .
then the cycle begins with an e - step that uses this classier to probabilistically label the unlabeled documents for the rst time .
the algorithm iterates over the e - and m - steps until it converges to a point where o ( cid : 123 ) does not change from one iteration to the next .
algorithmically , we determine that convergence has occurred by observing a below - threshold change in the log - probability of the parameters ( eq .
( 123 ) ) , which is the height of the surface on which em is hill - climbing .
table 123 gives an outline of the basic em algorithm from this section .
text classification using em
table 123
the basic em algorithm described in section 123 .
build an initial naive bayes classier , o ( cid : 123 ) , from the labeled documents , dl , only .
use maximum a posteriori loop while classier parameters improve , as measured by the change in lc . ( cid : 123 ) jdi z / ( the complete log
inputs : collections dl of labeled documents and du of unlabeled documents .
parameter estimation to nd o ( cid : 123 ) d arg max ( cid : 123 ) p . d j ( cid : 123 ) / p . ( cid : 123 ) / ( see eqs .
( 123 ) and ( 123 ) ) .
probability of the labeled and unlabeled data , and the prior ) ( see eq .
( e - step ) use the current classier , o ( cid : 123 ) , to estimate component membership of each unlabeled document ,
i . e . , the probability that each mixture component ( and class ) generated each document ,
( m - step ) re - estimate the classier , o ( cid : 123 ) , given the estimated component membership of each document .
p . c j j dii o ( cid : 123 ) / ( see eq .
use maximum a posteriori parameter estimation to nd o ( cid : 123 ) d arg max ( cid : 123 ) p . d j ( cid : 123 ) / p . ( cid : 123 ) / ( see eqs .
( 123 ) and ( 123 ) ) .
output : a classier , o ( cid : 123 ) , that takes an unlabeled document and predicts a class label .
in summary , em nds a o ( cid : 123 ) that locally maximizes the likelihood of its parameters given all the databoth the labeled and the unlabeled .
it provides a method whereby unlabeled data can augment limited labeled data and contribute to parameter estimation .
an interesting empirical question is whether these higher likelihood parameter estimates will improve classication accuracy .
section 123 discusses the fact that naive bayes usually performs classication well despite violations of its assumptions .
will em also have this property ? note that the justications for this approach depend on the assumptions stated in section 123 , namely , that the data is produced by a mixture model , and that there is a one - to - one cor - respondence between mixture components and classes .
when these assumptions do not holdas certainly is the case in real - world textual datathe benets of unlabeled data are
our experimental results in section 123 show that this method can indeed dramatically improve the accuracy of a document classier , especially when there are only a few labeled documents .
but on some data sets , when there are a lot of labeled and a lot of unlabeled documents , this is not the case .
in several experiments , the incorporation of unlabeled data decreases , rather than increases , classication accuracy .
next we describe changes to the basic em algorithm described above that aim to address
performance degradation due to violated assumptions .
augmented em
this section describes two extensions to the basic em algorithm described above .
the extensions help improve classication accuracy even in the face of somewhat violated assumptions of the generative model .
in the rst we add a new parameter to modulate the degree to which em weights the unlabeled data; in the second we augment the model to relax one of the assumptions about the generative model .
nigam et al .
weighting the unlabeled data .
as described in the introduction , a common scenario is that few labeled documents are on hand , but many orders of magnitude more unlabeled documents are readily available .
in this case , the great majority of the data determining ems parameter estimates comes from the unlabeled set .
in these circumstances , we can think of em as almost entirely performing unsupervised clustering , since the model is mostly positioning the mixture components to maximize the likelihood of the unlabeled documents .
the number of labeled data is so small in comparison to the unlabeled , that the only signicant effect of the labeled data is to initialize the classier parameters ( i . e .
determining ems starting point for hill climbing ) , and to identify each component with a class label .
when the two mixture model assumptions are true , and the natural clusters of the data are in correspondence with the class labels , then unsupervised clustering with many unla - beled documents will result in mixture components that are useful for classication ( c . f .
section 123 , where innite amounts of unlabeled data are sufcient to learn the parameters of the mixture components ) .
however , when the mixture model assumptions are not true , the natural clustering of the unlabeled data may produce mixture components that are not in correspondence with the class labels , and are therefore detrimental to classication accu - racy .
this effect is particularly apparent when the number of labeled documents is already large enough to obtain reasonably good parameter estimates for the classier , yet the orders of magnitude more unlabeled documents still overwhelm parameter estimation and thus badly skew the estimates .
this subsection describes a method whereby the inuence of the unlabeled data is mod - ulated in order to control the extent to which em performs unsupervised clustering .
we introduce a new parameter , 123 123 , into the likelihood equation which decreases the contribution of the unlabeled documents to parameter estimation .
we term the resulting method em - .
instead of using em to maximize eq .
( 123 ) , we instead maximize :
lc . ( cid : 123 ) jdi z / d log . p . ( cid : 123 ) / / c
zi j log . p . c j j ( cid : 123 ) / p . di j c ji ( cid : 123 ) / /
zi j log . p . c j j ( cid : 123 ) / p . di j c ji ( cid : 123 ) / /
notice that when is close to zero , the unlabeled documents will have little inuence on the shape of ems hill - climbing surface .
when d 123 , each unlabeled document will be weighted the same as a labeled document , and the algorithm is the same as the original em
when iterating to maximize eq .
( 123 ) , the e - step is performed exactly as before .
the m - step is different , however , and entails the following substitutes for eqs .
( 123 ) and ( 123 ) .
first dene 123i / to be the weighting factor whenever di in the unlabeled set , and to be 123 whenever di is in the labeled set :
123i / d
if di 123 du if di 123 dl :
text classification using em
then the new estimate o ( cid : 123 ) wtjc j is again a ratio of word counts , but where the counts of the
o ( cid : 123 ) wt j c j
p . wt j c ji o ( cid : 123 ) / d
unlabeled documents are decreased by a factor of :
123i / n . wt ; di / p . yi d c j j di / class prior probabilities , o ( cid : 123 ) c j , are modied similarly : 123i / p . yi d c j j di /
p . c j j o ( cid : 123 ) / d 123 cpjdj
123i / n . ws ; di / p . yi d c j j di /
jcj c jdlj c jduj
these equations can be derived by again solving the system of partial derivatives using lagrange multipliers to enforce the constraint that probabilities sum to one .
in this paper we select the value of that maximizes the leave - one - out cross - validation classication accuracy of the labeled training data .
experimental results with this technique are described in section 123 .
as shown there , setting to some value between 123 and 123 can result in classication accuracy higher than either d 123 or d 123 , indicating that there can be value in the unlabeled data even when its natural clustering would result in poor
multiple mixture components per class .
the em - technique described above addresses violated mixture model assumptions by reducing the effect of those violated assumptions on parameter estimation .
an alternative approach is to attack the problem head - on by removing or weakening a restrictive assumption .
this subsection takes exactly this approach by relaxing the assumption of a one - to - one correspondence between mixture components and classes .
we replace it with a less restrictive assumption : a many - to - one correspondence between mixture components and classes .
for textual data , this corresponds to saying that a class may be comprised of several dif - ferent sub - topics , each best captured with a different word distribution .
furthermore , using multiple mixture components per class can capture some dependencies between words .
for example , consider a sports class consisting of documents about both hockey and baseball .
in these documents , the words ice and puck are likely to co - occur , and the words bat and base are likely to co - occur .
however , these dependencies cannot be captured by a single multinomial distribution over words in the sports class .
on the other hand , with mul - tiple mixture components per class , one multinomial can cover the hockey sub - topic , and another the baseball sub - topicthus more accurately capturing the co - occurrence patterns of the above four words .
for some or all of the classes we now allow multiple multinomial mixture components .
note that as a result , there are now missing values for the labeled as well as the unlabeled documentsit is unknown which mixture component , among those covering the given label , is responsible for generating a particular labeled document .
parameter estimation will still be performed with em except that , for each labeled document , we must now estimate which mixture component the document came from .
nigam et al .
let us introduce the following notation for separating mixture components from classes .
instead of using c j to denote both a class and its corresponding mixture component , we will now write ta for the ath class ( topic ) , and c j will continue to denote the jth mixture compo - nent .
we write p . ta j c ji o ( cid : 123 ) / 123 f123; 123g for the pre - determined , deterministic , many - to - one
mapping between mixture components and classes .
parameter estimation is again done with em .
the m - step is the same as basic em , build - ing maximum a posteriori parameter estimates for the multinomial of each component .
in the e - step , unlabeled documents are treated as before , calculating probabilistically -
weighted mixture component membership , p . c j j dii o ( cid : 123 ) / .
for labeled documents , the previ - ous p . c j j dii o ( cid : 123 ) / 123 f123; 123g that was considered to be xed by the class label is now allowed
to vary between 123 and 123 for mixture components assigned to that documents class .
thus , the algorithm also calculates probabilistically - weighted mixture component membership
for the labeled documents .
note , however , that all p . c j j dii o ( cid : 123 ) / , for which p . yi d ta j c ji o ( cid : 123 ) /
is zero , are clamped at zero , and the rest are normalized to sum to one .
multiple mixture components for the same class are initialized by randomly spreading the labeled training data across the mixture components matching the appropriate class label .
that is , components are initialized by performing a randomized e - step in which
p . c j j dii o ( cid : 123 ) / is sampled from a uniform distribution over mixture components for which p . ta d yi j c ji o ( cid : 123 ) / is one .
when there are multiple mixture components per class , classication becomes a matter of probabilistically classifying documents into the mixture components , and then summing the mixture component probabilities into class probabilities :
p . c j j o ( cid : 123 ) / rd123 p . cr j o ( cid : 123 ) /
c ji o ( cid : 123 )
p . ta j dii o ( cid : 123 ) / d
p . ta j c ji o ( cid : 123 ) /
in this paper , we select the number of mixture components per class by cross - validation .
table 123 gives an outline of the em algorithm with the extensions of this and the previous
experimental results from this technique are described in section 123 .
as shown there , when the data are not naturally modeled by a single component per class , the use of unlabeled data with em degrades performance .
however , when multiple mixture components per class are used , performance with unlabeled data and em is superior to naive bayes .
experimental results
in this section , we provide empirical evidence that combining labeled and unlabeled training documents using em outperforms traditional naive bayes , which trains on labeled docu - ments alone .
we present experimental results with three different text corpora : usenet news articles ( 123 newsgroups ) , web pages ( webkb ) , and newswire articles ( reuters ) . 123
results show that improvements in accuracy due to unlabeled data are often dramatic , especially when the number of labeled training documents is low .
for example , on the 123 newsgroups data set , classication error is reduced by 123% when trained with 123 labeled and 123 unlabeled documents .
text classification using em
table 123
the algorithm described in this paper , and used to generate the experimental results in section 123
the algorithm enhancements for em - that vary the contribution of the unlabeled data ( section 123 . 123 ) are indicated by ( weighted only ) .
the optional use of multiple mixture components per class ( section 123 . 123 ) is indicated by ( multiple only ) .
unmarked paragraphs are common to all variations of the algorithm .
inputs : collections dl of labeled documents and du of unlabeled documents .
( weighted only ) : set the discount factor of the unlabeled data , , by cross - validation ( see sections 123 and 123 ) .
( multiple only ) : set the number of mixture components per class by cross - validation ( see sections 123 and 123 ) .
( multiple only ) : for each labeled document , randomly assign p . c j j dii o ( cid : 123 ) / for mixture components
build an initial naive bayes classier , o ( cid : 123 ) , from the labeled documents only .
use maximum a posteriori loop while classier parameters improve ( 123 : 123 < 123lc . ( cid : 123 ) jdi z / , the change in complete log probability of the labeled and unlabeled data , and the prior ) ( see eq .
( 123 ) : ( e - step ) use the current classier , o ( cid : 123 ) , to estimate the component membership of each document , i . e
that correspond to the documents class label , to initialize each mixture component .
parameter estimation to nd o ( cid : 123 ) d arg max ( cid : 123 ) p . d j ( cid : 123 ) / p . ( cid : 123 ) / ( see eqs .
( 123 ) and ( 123 ) ) .
probability that each mixture component generated each document , p . c j j dii o ( cid : 123 ) / ( see eq .
( multiple only ) : restrict the membership probability estimates of labeled documents to be zero for components associated with other classes , and renormalize .
( m - step ) re - estimate the classier , o ( cid : 123 ) , given the estimated component membership of each document .
use maximum a posteriori parameter estimation to nd o ( cid : 123 ) d arg max ( cid : 123 ) p . d j ( cid : 123 ) / p . ( cid : 123 ) / ( see eqs .
( 123 ) and ( 123 ) ) .
( weighted only ) : when counting events for parameter estimation , word and document counts from unlabeled documents are reduced by a factor ( see eqs .
( 123 ) and ( 123 ) ) .
output : a classier , o ( cid : 123 ) , that takes an unlabeled document and predicts a class label .
on certain data sets , however , ( and especially when the number of labeled documents is high ) , the incorporation of unlabeled data with the basic em scheme may reduce rather than increase accuracy .
we show that the application of the em extensions described in the previous section increases performance beyond that of naive bayes .
datasets and protocol
the 123 newsgroups data set ( joachims , 123; mccallum , et al . , 123; mitchell , 123 ) , collected by ken lang , consists of 123 articles divided almost evenly among 123 different usenet discussion groups .
the task is to classify an article into the one newsgroup ( of twenty ) to which it was posted .
many of the categories fall into confusable clusters; for example , ve of them are comp . * discussion groups , and three of them discuss religion .
when words from a stoplist of common short words are removed , there are 123 unique words that occur more than once; other feature selection is not used .
when tokenizing this data , we skip the usenet headers ( thereby discarding the subject line ) ; tokens are formed from contiguous alphabetic characters , which are left unstemmed .
the word counts of each document are scaled such that each document has constant length , with potentially fractional
nigam et al .
word counts .
our preliminary experiments with 123 newsgroups indicated that naive bayes classication was better with this word count normalization .
the 123 newsgroups data set was collected from usenet postings over a period of several months in 123
naturally , the data have time dependenciesarticles nearby in time are more likely to be from the same thread , and because of occasional quotations , may contain many of the same words .
in practical use , a classier for this data set would be asked to classify future articles after being trained on articles from the past .
to preserve this scenario , we create a test set of 123 documents by selecting by posting date the last 123% of the articles from each newsgroup .
an unlabeled set is formed by randomly selecting 123 documents from those remaining .
labeled training sets are formed by partitioning the remaining 123 documents into non - overlapping sets .
the sets are created with equal numbers of documents per class .
for experiments with different labeled set sizes , we create up to ten sets per size; obviously , fewer sets are possible for experiments with labeled sets containing more than 123 documents .
the use of each non - overlapping training set comprises a new trial of the given experiment .
results are reported as averages over all trials of the experiment .
the webkb data set ( craven et al . , 123 ) contains 123 web pages gathered from univer - sity computer science departments .
the collection includes the entirety of four departments , and additionally , an assortment of pages from other universities .
the pages are divided into seven categories : student , faculty , staff , course , project , department and other .
in this paper , we use the four most populous non - other categories : student , faculty , course and projectall together containing 123 pages .
the task is to classify a web page into the appropriate one of the four categories .
for consistency with previous studies with this data set ( craven et al . , 123 ) , when tokenizing the webkb data , numbers were converted into a time or a phone number token , if appropriate , or otherwise a sequence - of - length - n token .
we did not use stemming or a stoplist; we found that using a stoplist actually hurt performance .
for example , my is an excellent indicator of a student homepage and is the fourth - ranked word by information gain .
we limit the vocabulary to the 123 most informative words , as measured by average mutual information with the class variable .
this feature selection method is commonly used for text ( yang & pederson , 123; koller & sahami , 123; joachims , 123 ) .
we selected this vocabulary size by running leave - one - out cross - validation on the training data to optimize classication accuracy .
the webkb data set was collected as part of an effort to create a crawler that ex - plores previously unseen computer science departments and classies web pages into a knowledge - base ontology .
to mimic the crawlers intended use , and to avoid reporting performance based on idiosyncrasies particular to a single department , we test using a leave - one - university - out approach .
that is , we create four test sets , each containing all the pages from one of the four complete computer science departments .
for each test set , an unlabeled set of 123 pages is formed by randomly selecting from the remaining web pages .
non - overlapping training sets are formed by the same method as in 123 newsgroups .
also as before , results are reported as averages over all trials that share the same number of labeled training documents .
the reuters 123 distribution 123 data set consists of 123 articles and 123 topic categories from the reuters newswire .
following several other studies ( joachims , 123;
text classification using em
liere & tadepalli , 123 ) we build binary classiers for each of the ten most populous classes to identify the news topic .
we use all the words inside the <text> tags , including the title and the dateline , except that we remove the reuter and &# tags that occur at the top and bottom of every document .
we use a stoplist , but do not stem .
in reuters , classiers for different categories perform best with widely varying vocab - ulary sizes ( which are chosen by average mutual information with the class variable ) .
this variance in optimal vocabulary size is unsurprising .
as previously noted ( joachims , 123 ) , categories like wheat and corn are known for a strong correspondence between a small set of words ( like their title words ) and the categories , while categories like acq are known for more complex characteristics .
the categories with narrow denitions attain best classication with small vocabularies , while those with a broader denition require a large vocabulary .
the vocabulary size for each reuters trial is selected by optimizing accuracy as measured by leave - one - out cross - validation on the labeled training set .
as with the 123 newsgroups data set , there are time dependencies in reuters .
the standard modapte train / test split divides the articles by time , such that the later 123 documents form the test set , and the earlier 123 are available for training .
in our experi - ments , 123 documents from this training set are randomly selected to form the unlabeled set .
from the remaining training documents , we randomly select up to ten non - overlapping training sets of ten positively labeled documents and 123 negatively labeled documents , as previously described for the other two data sets .
we use non - uniform number of labelings across the classes because the negative class is much more frequent than the positive class in all of the binary reuters classication tasks .
results on reuters are reported as precision - recall breakeven points , a standard informa - tion retrieval measure for binary classication .
accuracy is not a good performance metric here because very high accuracy can be achieved by always predicting the negative class .
the task on this data set is less like classication than it is like lteringnd the few positive examples from a large sea of negative examples .
recall and precision capture the inherent duality of this task , and are dened as :
recall d # of correct positive predictions precision d # of correct positive predictions
# of positive examples
# of positive predictions
the classier can achieve a trade - off between precision and recall by adjusting the de - cision boundary between the positive and negative class away from its previous default of
p . c j j dii o ( cid : 123 ) / d 123 : 123
the precision - recall breakeven point is dened as the precision and
recall value at which the two are equal ( e . g .
joachims , 123 ) .
the algorithm used for experiments with em is described in table 123
in this section , when leave - one - out cross - validation is performed in conjunction with em , we make one simplication for computational efciency .
we rst run em to convergence with all the training data , and then subtract the word counts of each labeled document in turn before testing that document .
thus , when performing cross - validation for a specic combination of parameter settings , only one run of em is required instead of one run of em
nigam et al .
figure 123
classication accuracy on the 123 newsgroups data set , both with and without 123 , 123 unlabeled documents .
with small amounts of training data , using em yields more accurate classiers .
with large amounts of labeled training data , accurate parameter estimates can be obtained without the use of unlabeled data , and the two methods begin to converge .
per labeled example .
note , however , that there are still some residual effects of the held - out
the computational complexity of em , however , is not prohibitive .
each iteration requires classifying the training documents ( e - step ) , and building a new classier ( m - step ) .
in our experiments , em usually converges after about 123 iterations .
the wall - clock time to read the document - word matrix from disk , build an em model by iterating to convergence , and classify the test documents is less than one minute for the webkb data set , and less than 123 minutes for 123 newsgroups .
the 123 newsgroups data set takes longer because it has more documents and more words in the vocabulary .
em with unlabeled data increases accuracy
we rst consider the use of basic em to incorporate information from unlabeled documents .
figure 123 shows the effect of using basic em with unlabeled data on the 123 newsgroups data set .
the vertical axis indicates average classier accuracy on test sets , and the horizontal axis indicates the amount of labeled training data on a log scale .
we vary the amount of labeled training data , and compare the classication accuracy of traditional naive bayes ( no unlabeled data ) with an em learner that has access to 123 unlabeled documents .
em performs signicantly better .
for example , with 123 labeled documents ( 123 docu - ments per class ) , naive bayes reaches 123% accuracy while em achieves 123% .
this represents a 123% reduction in classication error .
note that em also performs well even with a very small number of labeled documents; with only 123 documents ( a single labeled document per class ) , naive bayes obtains 123% , em 123% .
as expected , when there is a lot of labeled data , and the naive bayes learning curve is close to a plateau , having unlabeled data does not help nearly as much , because there is already enough labeled data to accurately estimate the classier parameters .
with 123 labeled documents ( 123 per class ) , classication accuracy increases from 123% to 123% .
each of these results is statistically signicant ( p < 123 : 123 ) . 123
text classification using em
figure 123
classication accuracy while varying the number of unlabeled documents .
the effect is shown on the 123 newsgroups data set , with 123 different amounts of labeled documents , by varying the amount of unlabeled data on the horizontal axis .
having more unlabeled data helps .
note the dip in accuracy when a small amount of unlabeled data is added to a small amount of labeled data .
we hypothesize that this is caused by extreme , almost 123
or 123 , estimates of component membership , p . c j j di ; o ( cid : 123 ) / , for the unlabeled documents ( as caused by naive bayes
word independence assumption ) .
these results demonstrate that em nds parameter estimates that improve classication accuracy and reduce the need for labeled training examples .
for example , to reach 123% classication accuracy , naive bayes requires 123 labeled examples , while em requires only 123 labeled examples to achieve the same accuracy .
in gure 123 we consider the effect of varying the amount of unlabeled data .
for ve different quantities of labeled documents , we hold the number of labeled documents constant , and vary the number of unlabeled documents in the horizontal axis .
naturally , having more unlabeled data helps , and it helps more when there is less labeled data .
notice that adding a small amount of unlabeled data to a small amount of labeled data actually hurts performance .
we hypothesize that this occurs because the word independence
assumption of naive bayes leads to overly - condent p . c j j di ; o ( cid : 123 ) / estimates in the e - step ,
and the small amount of unlabeled data is distributed too sharply .
( without this bias in naive bayes , the e - step would spread the unlabeled data more evenly across the classes . ) when the number of unlabeled documents is large , however , this problem disappears because the unlabeled set provides a large enough sample to smooth out the sharp discreteness of naive bayes overly - condent classication .
we now move on to a different data set .
to provide some intuition about why em works , we present a detailed trace of one example from the webkb data set .
table 123 shows the evolution of the classier over the course of two em iterations .
each column shows the ordered list of words that the model indicates are most predictive of the course class .
words are judged to be predictive using a weighted log likelihood ratio . 123 the symbol d indicates an arbitrary digit .
at iteration 123 , the parameters are estimated from a randomly - chosen single labeled document per class .
notice that the course document seems to be about a specic articial intelligence course at dartmouth .
after two em iterations with 123 unlabeled documents , we see that em has used the unlabeled data to nd words that
nigam et al .
table 123
lists of the words most predictive of the course class in the webkb data set , as they change over iterations of em for a specic trial .
by the second iteration of em , many common course - related words appear .
the symbol d indicates an arbitrary digit .
are more generally indicative of courses .
the classier corresponding to the rst column achieves 123% accuracy; when em converges , the classier achieves 123% accuracy .
varying the weight of the unlabeled data
when graphing performance on this data set , we see that the incorporation of unlabeled data can also decrease , rather than increase , classication accuracy .
the graph in gure 123 shows the performance of basic em ( with 123 unlabeled documents ) on webkb .
again , em improves accuracy signicantly when the amount of labeled data is small .
when there are four labeled documents ( one per class ) , traditional naive bayes attains 123% accuracy , while em reaches 123% .
when there is a lot of labeled data , however , em hurts performance slightly .
with 123 labeled documents , naive bayes obtains 123% accuracy , while em does worse at 123% .
both of these differences in performance are statistically signicant ( p < 123 : 123 ) , for three and two of the university test sets , respectively .
as discussed in section 123 . 123 , we hypothesize that em hurts performance here because the data do not t the assumptions of the generative modelthat is , the mixture components
text classification using em
figure 123
classication accuracy on the webkb data set , both with and without 123 unlabeled documents .
when there are small numbers of labeled documents , em improves accuracy .
when there are many labeled documents , however , em degrades performance slightlyindicating a mist between the data and the assumed
that best explain the unlabeled data are not in precise correspondence with the class labels .
it is not surprising that the unlabeled data can throw off parameter estimation when one considers that the number of unlabeled documents is much greater than the number of labeled documents ( e . g .
123 versus 123 ) , and thus , even at the points in gure 123 with the largest amounts of labeled data , the great majority of the probability mass used in the m - step to estimate the classier parameters actually comes from the unlabeled data .
to remedy this dip in performance , we use em - to reduce the weight of the unlabeled data by varying in eqs .
( 123 ) and ( 123 ) .
figure 123 plots classication accuracy while varying to achieve the relative weighting indicated in the horizontal axis , and does so for three different amounts of labeled training data .
the bottom curve is obtained using 123 labeled documentsa vertical slice in gure 123 at a point where em with unlabeled data gives higher accuracy than naive bayes .
here , the best weighting of the unlabeled data is high , indicating that classication can be improved by augmenting the sparse labeled data with heavy reliance on the unlabeled data .
the middle curve is obtained using 123 labeled documentsa slice near the point where em and naive bayes performance cross .
here , the best weighting is in the middle , indicating that em - performs better than either naive bayes or basic em .
the top curve is obtained using 123 labeled documentsa slice where unweighted em performance is lower than traditional naive bayes .
less weight should be given to the unlabeled data at this point .
note the inverse relationship between the labeled data set size and the best weighting factorthe smaller labeled data set , the larger the best weighting of the unlabeled data .
this trend holds across all amounts of labeled data .
intuitively , when em has very little labeled training data , parameter estimation is so desperate for guidance that em with all the unlabeled data helps in spite of the somewhat violated assumptions .
however , when there is enough labeled training data to sufciently estimate the parameters , less weight should be given to the unlabeled data .
finally , note that the best - performing values of are somewhere between the extremes , remembering that the right - most point corresponds to em with the
nigam et al .
figure 123
the effects of varying , the weighting factor on the unlabeled data in em - .
these three curves from the webkb data set correspond to three different amounts of labeled data .
when there is less labeled data , accuracy is highest when more weight is given to the unlabeled data .
when the amount of labeled data is large , accurate parameter estimates are attainable from the labeled data alone , and the unlabeled data should receive less weight .
with moderate amounts of labeled data , accuracy is better in the middle than at either extreme .
note the magnied vertical scale .
weighting used to generate gure 123 , and the left - most to regular naive bayes .
paired t - tests across the trials of all the test universities show that the best - performing points on these curves are statistically signicantly higher than either end point , except for the difference between the maxima and basic em with 123 labeled documents ( p < 123 : 123 ) .
in practice the value of the tuning parameter can be selected by cross - validation .
in our experiments we select by leave - one - out cross - validation on the labeled training set for each trial , as discussed in section 123 .
figure 123 shows the accuracy for the best possible , and the accuracy when selecting via cross - validation .
basic em and naive bayes accuracies are also shown for comparison .
when is perfectly selected , its accuracy dominates the basic em and naive bayes curves .
cross - validation selects s that , for small amounts of labeled documents , perform about as well as em .
for large amounts of labeled documents , cross - validation selects s that do not suffer from the degraded performance seen in basic em , and also performs at least as well as naive bayes .
for example , at the 123 document level seen before , the picked by cross - validation gives only 123% of the weight to the unlabeled data , instead of the 123% given by basic em .
doing so provides an accuracy of 123% , compared to 123% for naive bayes and 123% for basic em .
this is not statistically signicantly different from naive bayes , and is statistically signicantly higher than basic em for two of the four test sets ( both p < 123 : 123 ) .
these results indicate that we can automatically avoid ems degradation in accuracy at large training set sizes and still preserve the benets of em seen with small labeled training sets .
these results also indicate that when the training set size is very small improved methods of selecting could signicantly increase the practical performance of em - even further .
note that in these cases , cross - validation has only a few documents with which to choose .
the end of section 123 suggests some methods that may perform better than cross -
text classification using em
figure 123
classication accuracy on the webkb data set , with modulation of the unlabeled data by the weighting factor .
the top curve shows accuracy when using the best value of .
in the second curve , is chosen by cross - validation .
with small amounts of labeled data , the results are similar to basic em; with large amounts of labeled data , the results are more accurate than basic em .
thanks to the weighting factor , large amounts of unlabeled data no longer degrades accuracy , as it did in gure 123 , and yet the algorithm retains the large improvements with small amounts of labeled data .
note the magnied vertical axis to facilitate the comparisons .
multiple mixture components per class
faced with data that do not t the assumptions of our model , the - tuning approach described above addresses this problem by allowing the model to incrementally ignore the unlabeled data .
another , more direct approach , described in section 123 . 123 , is to change the model so that it more naturally ts the data .
flexibility can be added to the mapping between mixture components and class labels by allowing multiple mixture components per class .
we expect this to improve performance when data for each class is , in fact , multi - modal .
with an eye towards testing this hypothesis , we apply em to the reuters corpus .
since the documents in this data set can have multiple class labels , each category is traditionally evaluated with a binary classier .
thus , the negative class covers 123 distinct categories , and we expect this task to strongly violate the assumption that all the data for the negative class are generated by a single mixture component .
for this reason , we model the positive class with a single mixture component and the negative class with between one and forty mixture components , both with and without unlabeled data .
table 123 contains a summary of results on the test set for modeling the negative class with multiple mixture components .
the nb123 column shows precision - recall breakeven points from standard naive bayes ( with just the labeled data ) , that models the negative class with a single mixture component .
the nb* column shows the results of modeling the negative class with multiple mixture components ( again using just the labeled data ) .
in the nb* column , the number of components has been selected to optimize the best precision - recall breakeven point .
the median number of components selected across trials is indicated in parentheses beside the breakeven point .
note that even before we consider the effect of unlabeled data , using this more complex representation on this data improves performance over traditional naive bayes .
nigam et al .
table 123
precision - recall breakeven points showing performance of binary classiers on reuters with traditional naive bayes ( nb123 ) , multiple mixture components using just labeled data ( nb* ) , basic em ( em123 ) with labeled and unlabeled data , and multiple mixture components em with labeled and unlabeled data ( em* ) .
for nb* and em* , the number of components is selected optimally for each trial , and the median number of components across the trials used for the negative class is shown in parentheses .
note that the multi - component model is more natural for reuters , where the negative class consists of many topics .
using both unlabeled data and multiple mixture components per class increases performance over either alone , and over naive bayes .
em* vs
em* vs
the column labeled em123 shows results with basic em ( i . e .
with a single negative component ) .
notice that here performance is often worse than naive bayes ( nb123 ) .
we hypothesize that , because the negative class is truly multi - modal , tting a single naive bayes class with em to the data does not accurately capture the negative class word distribution .
the column labeled em* shows results of em with multiple mixture components , again selecting the best number of components .
here performance is better than both nb123 ( tra - ditional naive bayes ) and nb* ( naive bayes with multiple mixture components per class ) .
this increase , measured over all trials of reuters , is statistically signicant ( p < 123 : 123 ) .
this indicates that while the use of multiple mixture components increases performance over traditional naive bayes , the combination of unlabeled data and multiple mixture com - ponents increases performance even more .
furthermore , it is interesting to note that , on average , em* uses more mixture components than nb*suggesting that the addition of unlabeled data reduces variance and supports the use of a more expressive model .
tables 123 and 123 show the complete results for experiments using multiple mixture compo - nents with and without unlabeled data , respectively .
note that in general , using too many or too few mixture components hurts performance .
with too few components , our assumptions are overly restrictive .
with too many components , there are more parameters to estimate from the same amount of data .
table 123 shows the same results as table 123 , but for classi - cation accuracy , and not precision - recall breakeven .
the general trends for accuracy are the same as for precision - recall .
however , for accuracy , the optimal number of mixture components for the negative class is greater than for precision - recall , because by its na - ture precision - recall focuses more on modeling the positive class , where accuracy focuses
text classification using em
table 123
performance of em using different numbers of mixture components for the negative class and 123 unlabeled documents .
precision - recall breakeven points are shown for experiments using between one and forty mixture components .
note that using too few or too many mixture components results in poor performance .
table 123
performance of em using different numbers of mixture components for the negative class , but with no unlabeled data .
precision - recall breakeven points are shown for experiments using between one and forty mixture
more on modeling the negative class , because it is much more frequent .
by allowing more mixture components for the negative class , a more accurate model is achieved .
one obvious question is how to select the best number of mixture components without having access to the test set labels .
as with selection of the weighting factor , , we use leave - one - out cross - validation , with the computational short - cut that entails running em only once ( as described at the end of section 123 ) .
results from this technique ( em*cv ) , compared to naive bayes ( nb123 ) and the best em ( em* ) , are shown in table 123
note that cross - validation does not perfectly select the number of components that perform best on the test set .
the results consistently show that selection by cross - validation chooses a smaller number of components than is best .
nigam et al .
table 123
classication accuracy on reuters with traditional naive bayes ( nb123 ) , multiple mixture components using just labeled data ( nb* ) , basic em ( em123 ) with labeled and unlabeled data , and multiple mixture components em with labeled and unlabeled data ( em* ) , as in table 123
em* vs
em* vs
table 123
performance of using multiple mixture components when the number of components is selected via cross - validation ( em*cv ) compared to optimal selection ( em* ) and straight naive bayes ( nb123 ) .
note that cross - validation usually selects too few components .
em*cv vs
by using the cross - validation with the computational short - cut , we bias the model towards the held - out document , which , we hypothesize , favors the use of fewer components .
the computationally expensive , but complete , cross - validation should perform better .
other model selection methods may perform better , while also remaining computationally efcient .
these include : more robust methods of cross - validation , such as that of ng ( 123 ) ; minimum description length ( rissanen , 123 ) ; and schuurmans metric - based approach , which also uses unlabeled data ( 123 ) .
research on improved methods of model selection for our algorithm is an area of future work .
text classification using em
related work
