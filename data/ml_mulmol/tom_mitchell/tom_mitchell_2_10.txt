most computational models of supervised learning rely only on labeled training examples , and ignore the possible role of unlabeled data .
this is true both for cognitive science models of learning such as soar newell and act - r anderson , et al .
, and for machine learning and data mining algorithms such as decision tree learning and inductive logic programming see , e . g . , mitchell .
in this paper we consider the potential role of unlabeled data in supervised learning .
we present an algorithm and experimental results demonstrating that unlabeled data can signicantly improve learning accuracy in certain practical problems .
we then identify the abstract problem structure that enables the algorithm to successfully utilize this unlabeled data , and prove that unlabeled data will boost learning accuracy for problems in this class .
the problem class we identify includes problems where the features describing the examples are redundantly sucient for classifying the example; a notion we make precise in the paper .
this problem class includes many natural learning problems faced by humans , such as learning a semantic lexicon over noun phrases in natural language , and learning to recognize objects from multiple sensor inputs .
we argue that models of human and animal learning should consider more strongly the potential role of unlabeled data , and that many natural learning problems t the class we identify .
a supervised learning problem : learning to classify web pages
to illustrate the role of unlabeled data in supervised learning , consider the problem of learning to classify pages of hypertext from the world wide web , given labeled training data consisting of individual web pages along with their correct classications figure .
we have been studying this problem as part of our larger research goal of automatically extracting information from the web craven et al .
professor john smith
hi ! i teach computer courses and advise students , including
in my spare time i work on a number of research projects , including machine learning for web
active learning for robot control
figure : training example of a faculty home page . " the task of classifying a web page can be achieved by considering just the words on the web page .
alternatively , the page can be classied using only the words on hyperlinks that point to the web page e . g . , my advisor " professor smith " .
when examples are described by such redundantly sucient features , unlabeled data can be used to boost the accuracy for supervised learning .
for example , one specic task we have considered is training a system to classify web pages into categories such as student home page , " faculty home page , " home page of an academic course , "
this web page classication problem can be formulated as a typical supervised learning problem .
by supervised learning problem " we mean that there is some set x of instances , and there is some target function f : x ! y to be learned , given a set of training examples of the form fhxi; f xiig .
in our case , x is the set of all web pages , y is the set of possible classications e . g . , student " faculty " neither " , and the function f to be learned in the function that maps an arbitrary page x to its correct classication .
for the ith labeled training example , xi is a particular web page , and f xi is the correct classication for xi , provided by an external teacher .
as discussed in craven et al .
, a number of supervised learning algorithms can be applied to the problem of learning to classify web pages .
one common approach is to rst represent each web page by a large feature vector , where each possible word in the language corresponds to a feature , and the value of the feature is the number of times the word occurs in the web page .
this bag of words " representation obviously ignores information about the sequence in which words occur .
however , this representation of web pages as large feature vectors enables us to apply standard supervised learning algorithms for classication of feature vectors .
for example , if we train a naive bayes classier mitchell on a set of approximately , labeled web pages , we achieve accuracies of approximately in classifying new web pages into the categories mentioned above craven et al . , .
set l of labeled training examples
set u of unlabeled examples
learn hyperlink - based classier h from l
learn full - text classier f from l
allow h to label p positive and n negative examples from u
allow f to label p positive and n negative examples from u
add these self - labeled examples to l
table : a cotraining algorithm , for training two classiers and using unlabeled data to boost their
while we can achieve reasonable accuracy after training on thousands of hand labeled web pages , in this paper we are interested in the question of how unlabeled data can be useful .
in fact , this is a crucial question in our web page classication task , because it is costly to hand label thousands of web pages , and because it is very easy to obtain hundreds of millions of unlabeled pages from the web .
how can we use unlabeled web pages to more accurately learn to classify future pages ? the answer can be seen by rst observing that the web contains an interesting kind of redundant information about each web page , as shown in gure .
as shown in this gure , we could classify the web page either by considering the words on the page itself as suggested in the paragraph above , or we could classify the page by ignoring the words on the page and instead considering the words on hyperlinks that point to the page .
in many cases , the words on the hyperlinks will be sucient to classify the example , and the words on the page itself will also be sucient .
in such cases , we will say these two sets of features the hyperlink words , and the web page words are redundantly sucient to classify the example .
we will see below that in general if the instances x for some supervised learning task can be factored into sets of redundantly sucient features , then unlabeled data can be used to boost the accuracy of classiers trained with limited labeled data .
learning with redundantly sucient features
how can we use unlabeled data to boost the learning accuracy for our web classication problem ?
page - based classier hyperlink - based classier combined classier
table : error rate in percent for classifying web pages as course home pages .
the top row shows errors when training on only the labeled examples .
bottom row shows errors when co - training , using both labeled and unlabeled examples .
the rightmost column shows the classication accuracy when the two classiers are provided equal votes on the nal classication .
the results reported here are the average of ve independent runs of the algorithm with dierent random starting examples .
the key idea is that we will train two independent classiers rather than one .
one classier will use only the words on the web page , and the other classier will use only the words on the hyperlinks .
we begin by training both classiers using whatever labeled training examples are available .
presumably this will result in two classiers that are imperfect , but better than random .
now we use the unlabeled data as follows : each classier is allowed to examine the unlabeled data and to pick its most condently predicted positive and negative examples , and add these to the set of labeled examples .
in other words , each classier is allowed to augment the pool of labeled examples .
both classiers are now retrained on this augmented set of labeled examples , and the process is repeated as long as desired .
table summarizes this co - training algorithm .
why should this co - training algorithm lead to more accurate classiers ? the intuition is that if the hyperlink classier nds an easily classied " hyperlink in the unlabeled data e . g . , one that is quite similar to one of the labeled examples on which it was trained , the web page that it points to will be added to the labeled pool of examples as well .
of course just because the hyperlink happened to be easy to classify does not mean the web page will be easily classied by the other classier .
if not , then the hyperlink classier has added useful training information to improve the other classier .
similarly , the web page classier can add examples that are easy for it to classify , but that provide useful information to improve the accuracy of the hyperlink classier .
in experiments , we have found that this cotraining algorithm does improve classication accuracy in one experiment blum and mitchell , , summarized when learning to classify web pages .
in table , we trained a classier to label web pages as home pages of academic courses .
experiment we provided just labeled examples , and approximately unlabeled pages drawn from computer science department web sites .
on each iteration of co - training , each classier was allowed to add new positive and new negative examples to the pool of labeled examples .
after iterations of the cotraining algorithm , the accuracy of the combined classier was , compared to when only the labeled data was used .
in this case , the impact of cotraining was to reduce the error by better than a factor of two .
given the experimental evidence that cotraining can be useful in at least one case , it is useful to characterize the general problem setting in which this kind of algorithm can benet from unlabeled data .
here we dene a problem setting in which we can prove that unlabeled data will be of help .
to dene the general problem setting that captures the essential structure of our web classication example , recall that earlier we dened a supervised learning problem as one where we have some set of instances x , a target function f : x ! y , and labeled examples fhxi; f xiig .
in the cotraining setting , we assume such a supervised learning problem in which the instances are drawn from x according to some xed possibly unknown probability distribution d .
we further assume that x = x x; that is , the instances in x can be factored into two parts e . g , .
the hyperlink words and the web page words .
we require also that x and x each contain information sucient to classify the example .
in other words , we require that there exist some function g : x ! y and some function g : x ! y such that for all x x; gx = gx = f x .
note the learner is not expected to know f , g or g in advance .
we simply require that it be possible to express f in terms of x and in terms of x i . e . , that x and x both be sucient to classify x according to f .
if the above constraints on x and x are satised , then we will say that x and x are redundantly sucient to classify x with respect to f .
in blum and mitchell we show that if f : x ! y is pac learnable from noisy labeled data , x and x are redundantly sucient to classify x with respect to f , and if xb and xb are conditionally independent given f x , then f can be pac learned given a weak initial classier plus only unlabeled data .
what this means is that if the labeled data are sucient to train a better than random initial classier , if x and x are distributed independently for each target value of f , and if f can be learned accurately given an arbitrary number of labeled examples , then f can be learned to similar accuracy given just this weak initial classier and an arbitrary number of unlabeled examples .
the signicance is that this proves that unlabeled examples can substitute for labeled examples , under the conditions of the theorem .
in blum and mitchell we also show that unlabeled data is of potential value under the more general setting in which x and x are not independently distributed .
there we show that if the training data is noise - free , then unless x and x are deterministically related e . g . , one value of x is always paired with the same x , the unlabeled data can be of use .
other learning tasks with redundantly sucient features
given the success of cotraining as a method for using unlabeled data when learning to classify web pages , and given the formal characterization of the class of problems for which cotraining can be of use , it is interesting to ask what other natural learning problems allow this approach .
here we summarize a number of tasks :
learning to classify noun phrases into semantic classes .
in rilo and jones , an approach similar to cotraining was applied to the problem of learning a semantic lexicon over noun phrases in english .
in this paper , one task was to learn to classify noun phrases as positive or negative examples of locations e . g . , san sebastian " is positive , blue " is negative .
in this case each example , x , is a sentence such as we are located in lovely pittsburgh . " the factorization of x into two redundantly sucient feature sets is done as follows : x is the noun phrase itself e . g . , pittsburgh " in the above sentence , and x is the linguistic context in which the noun phrase appears e . g . , we are located in lovely | " .
note that in most cases it is possible to determine whether the noun phrase is a location given either x or x .
note also that the values of x and x are distributed fairly independently i . e . , with might see the same x as above , but with a dierent x .
starting with a list of just known noun phrases , their system used an algorithm similar to cotraining to learn dozens of additional locations .
learning to select word sense .
in yarowsky , , an approach similar to cotraining was applied to learning to disambiguate word senses e . g . , to determine whether the word plant " refers to a manufacturing plant or to a botanical plant .
here , each instance x corresponds to a context containing the word in question similar to the rilo and jones example above .
however , the factorization of x is into more than two components , with each word serving as a candidate component , and this algorithm seems to not map directly into cotraining .
but it is quite similar , and the problem could be attacked using a cotraining approach .
learning to recognize phonemes in speech .
in de sa and ballard , , an approach to fully unsupervised learning was applied to a problem with nearly redundantly sucient features .
in this case , the task is to learn to classify speech phonemes , based on both the audio signal and the video signal watching the speakers lips .
here each instance x corresponds to the full data , x corresponds to the audio signal , and x corresponds to the video signal .
again , we may roughly assume that either of x or x is sucient to determine the phoneme , and therefore can train two classiers on limited labeled data , and let them train each other over the unlabeled data .
in fact , de sa and ballard use no labeled data at all , instead performing a kind of clustering in which the audio signal must predict the video , and vice versa .
their system was able to learn clusters corresponding to the spoken phonemes in the data .
object recognition in multimedia data .
consider the problem of learning to recognize objects in multimedia data , such as the continuous stream of audio , video , and other sensory input a person receives .
a very similar task is to learn to classify television segments , based on the ongoing stream of video , audio , and close captioned text .
in this latter case , consider the problem of learning to spot television segments in which boris yeltsin appears .
here we could consider each instance x to be a snapshot containing the audio , video and text at a particular time .
here x could be the audio , x the video , and x the text .
note that in some cases we might see easy " example of video a full face of yeltsin , or an easy audio his voice without background
noise , or an easy text the word yeltsin .
thus , we could expect cotraining to provide a useful approach to using unlabeled data to train classiers .
to my knowledge , this experiment has not
discussion and conclusions
we have described a class of supervised learning problems for which unlabeled data can be proven to improve learning accuracy .
the key dening features of this problem class are that the instances x can be factored into two or more components x , x : : : , which are redundantly sucient to classify the example , and these components covary , so that a particular xi does not always co - occur with the same xj . .
we have described the cotraining algorithm that uses unlabeled data for such problems , and have presented experimental and theoretical results showing that in these problems unlabeled data can indeed be useful to improve accuracy for supervised learning .
the connection to human and animal learning appears to be a rich area for future study .
humans and other animals have a rich set of sensory input , which includes redundantly sucient data for many tasks .
for example , smell , vision , sound can all be useful in trying to classify whether food is nearby , and in many cases the data from just one of these channels is sucient .
de sa and ballards success with a cotraining - like algorithm for learning to classify phonemes from video and audio similar to that observed by humans is also suggestive .
perhaps people rely less than we suspect on labeled data to achieve successful learning for various classication tasks .
thanks to my collaborator on this work , avrim blum , for many interesting discussions and ideas .
sebastian thrun , rosie jones , andrew mccallum , and kamal nigam have also contributed ideas and suggestions along the way .
this work has been supported by darpa under research contract
