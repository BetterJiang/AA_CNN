we propose a new unsupervised learning technique for ex - tracting information from large text collections .
we model documents as if they were generated by a two - stage stochas - tic process .
each author is represented by a probability distribution over topics , and each topic is represented as a probability distribution over words for that topic .
the words in a multi - author paper are assumed to be the result of a mixture of each authors topic mixture .
the topic - word and author - topic distributions are learned from data in an unsupervised manner using a markov chain monte carlo al - gorithm .
we apply the methodology to a large corpus of 123 , 123 abstracts and 123 , 123 authors from the well - known citeseer digital library , and learn a model with 123 topics .
we discuss in detail the interpretation of the results dis - covered by the system including speci ( cid : 123 ) c topic and author models , ranking of authors by topic and topics by author , signi ( cid : 123 ) cant trends in the computer science literature between 123 and 123 , parsing of abstracts by topics and authors and detection of unusual papers by speci ( cid : 123 ) c authors .
an on - line query interface to the model is also discussed that allows interactive exploration of author - topic models for corpora such as citeseer .
categories and subject descriptors i . 123 ( arti ( cid : 123 ) cial intelligence ) : learning
unsupervised learning , gibbs sampling , text modeling
with the advent of the web and various specialized digi - tal libraries , the automatic extraction of useful information from text has become an increasingly important research
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro ( cid : 123 ) t or commercial advantage and that copies bear this notice and the full citation on the ( cid : 123 ) rst page .
to copy otherwise , to republish , to post on servers or to redistribute to lists , requires prior speci ( cid : 123 ) c permission and / or a fee .
kdd123 , august 123 ( cid : 123 ) 123 , 123 , seattle , washington , usa .
copyright 123 acm 123 / 123 / 123 . . . $123 .
area in data mining .
in this paper we discuss a new algo - rithm that extracts both the topics expressed in large text document collections and models how the authors of docu - ments use those topics .
the methodology is illustrated using a sample of 123 , 123 abstracts and 123 , 123 authors from the well - known citeseer digital library of computer science re - search papers ( lawrence , giles , and bollacker , 123 ) .
the algorithm uses a probabilistic model that represents top - ics as probability distributions over words and documents as being composed of multiple topics .
a novel feature of our model is the inclusion of author models , in which au - thors are modeled as probability distributions over topics .
the author - topic models can be used to support a variety of interactive and exploratory queries on the set of docu - ments and authors , including analysis of topic trends over time , ( cid : 123 ) nding the authors who are most likely to write on a given topic , and ( cid : 123 ) nding the most unusual paper written by a given author .
bayesian unsupervised learning is used to ( cid : 123 ) t the model to a document collection .
supervised learning techniques for automated categoriza - tion of documents into known classes or topics has received considerable attention in recent years ( e . g . , yang , 123 ) .
for many document collections , however , neither prede ( cid : 123 ) ned topics nor labeled documents may be available .
further - more , there is considerable motivation to uncover hidden topic structure in large corpora , particularly in rapidly chang - ing ( cid : 123 ) elds such as computer science and biology , where pre - de ( cid : 123 ) ned topic categories may not accurately re ( cid : 123 ) ect rapidly
automatic extraction of topics from text , via unsuper - vised learning , has been addressed in prior work using a number of di ( cid : 123 ) erent approaches .
one general approach is to represent the high - dimensional term vectors in a lower - dimensional space .
local regions in the lower - dimensional space can then be associated with speci ( cid : 123 ) c topics .
for ex - ample , the websom system ( lagus et al .
123 ) uses non - linear dimensionality reduction via self - organizing maps to represent term vectors in a two - dimensional layout .
lin - ear projection techniques , such as latent semantic indexing ( lsi ) , are also widely used ( berry , dumais , and o brien , 123 ) .
for example , deerwester et al .
( 123 ) , while not using the term \topics " per se , state :
roughly speaking , these factors may be thought of as arti ( cid : 123 ) cial concepts; they represent extracted common meaning components of many di ( cid : 123 ) erent
in : 123th acm sigkdd conference knowledge discovery and data mining ( seattle , 123 ) words and documents .
a somewhat di ( cid : 123 ) erent approach is to cluster the docu - ments into groups containing similar semantic content , using any of a variety of well - known document clustering tech - niques ( e . g . , cutting et al . , 123; mccallum , nigam , and ungar , 123; popescul et al . , 123 ) .
each cluster of docu - ments can then be associated with a latent topic ( e . g . , as represented by the mean term vector for documents in the cluster ) .
while clustering can provide useful broad informa - tion about topics , clusters are inherently limited by the fact that each document is ( typically ) only associated with one cluster .
this is often at odds with the multi - topic nature of text documents in many contexts .
in particular , combina - tions of diverse topics within a single document are di ( cid : 123 ) cult to represent .
for example , this present paper contains at least two signi ( cid : 123 ) cantly di ( cid : 123 ) erent topics : document modeling and bayesian estimation .
for this reason , other representa - tions ( such as those discussed below ) that allow documents to be composed of multiple topics generally provide better models for sets of documents ( e . g . , better out of sample pre - dictions , blei , ng , and jordan ( 123 ) ) .
hofmann ( 123 ) introduced the aspect model ( also re - ferred to as probabilistic lsi , or plsi ) as a probabilistic alternative to projection and clustering methods .
in plsi , topics are modeled as multinomial probability distributions over words , and documents are assumed to be generated by the activation of multiple topics .
while the plsi model produced impressive results on a number of text document problems such as information retrieval , the parameterization of the model was susceptible to over ( cid : 123 ) tting and did not pro - vide a straightforward way to make inferences about new documents not seen in the training data .
blei , ng , and jordan ( 123 ) addressed these limitations by proposing a more general bayesian probabilistic topic model called la - tent dirichlet allocation ( lda ) .
the parameters of the lda model ( the topic - word and document - topic distributions ) are estimated using an approximation technique known as variational em , since standard estimation methods are in - tractable .
gri ( cid : 123 ) ths and steyvers ( 123 ) showed how gibbs sampling , a markov chain monte carlo technique , could be applied in this model , and illustrated this approach using 123 years of abstract data from the proceedings of the national academy of sciences .
our focus here is to extend the probabilistic topic mod - els to include authorship information .
joint author - topic modeling has received little or no attention as far as we are aware .
the areas of stylometry , authorship attribution , and forensic linguistics focus on the problem of identify - ing what author wrote a given piece of text .
for example , mosteller and wallace ( 123 ) used bayesian techniques to infer whether hamilton or madison was the more likely au - thor of disputed federalist papers .
more recent work of a similar nature includes authorship analysis of a purported poem by shakespeare ( thisted and efron , 123 ) , identifying authors of software programs ( gray , sallis , and macdonell , 123 ) , and the use of techniques such as support vector ma - chines ( diederich et al . , 123 ) for author identi ( cid : 123 ) cation .
these author identi ( cid : 123 ) cation methods emphasize the use of distinctive stylistic features ( such as sentence length ) that characterize a speci ( cid : 123 ) c author .
in contrast , the models we present here focus on extracting the general semantic con - tent of a document , rather than the stylistic details of how it was written .
for example , in our model we omit common
\stop " words since they are generally irrelevant to the topic of the document|however , the distributions of stop words can be quite useful in stylometry .
while \topic " information could be usefully combined with stylistic features for author classi ( cid : 123 ) cation we do not pursue this idea in this particular
graph - based and network - based models are also frequently used as a basis for representation and analysis of relations among scienti ( cid : 123 ) c authors .
for example , newman ( 123 ) , mutschke ( 123 ) and erten et al .
( 123 ) use methods from bibliometrics , social networks , and graph theory to ana - lyze and visualize co - author and citation relations in the scienti ( cid : 123 ) c literature .
kautz , selman , and shah ( 123 ) de - veloped the interactive referralweb system for exploring networks of computer scientists working in arti ( cid : 123 ) cial intel - ligence and information retrieval , and white and smyth ( 123 ) used pagerank - style ranking algorithms to analyze co - author graphs .
in all of this work only the network con - nectivity information is used|the text information from the underlying documents is not used in modeling .
thus , while the grouping of authors via these network models can implic - itly provide indications of latent topics , there is no explicit representation of the topics in terms of the text content ( the words ) of the documents .
the novelty of the work described in this paper lies in the proposal of a probabilistic model that represents both authors and topics , and the application of this model to a large well - known document corpus in computer science .
as we will show later in the paper , the model provides a general framework for exploration , discovery , and query - answering in the context of the relationships of author and topics for large document collections .
the outline of the paper is as follows : in section 123 we de - scribe the author - topic model and outline how the parame - ters of the model ( the topic - word distributions and author - topic distributions ) can be learned from training data con - sisting of documents with known authors .
section 123 illus - trates the application of the model to a large collection of abstracts from the citeseer system , with examples of spe - ci ( cid : 123 ) c topics and speci ( cid : 123 ) c author models that are learned by the algorithm .
in section 123 we illustrate a number of appli - cations of the model , including the characterization of topic trends over time ( which provides some interesting insights on the direction of research in computer science ) , and the characterization of which papers are most typical and least typical for a given author .
an online query interface to the system is described in section 123 , allowing users to query the model over the web|an interesting feature of the model is the coupling of bayesian sampling and relational database technology to answer queries in real - time .
section 123 con - tains a brief discussion of future directions and concluding
an overview of the authortopic
123 the probabilistic generative model
the author - topic model reduces the process of writing a scienti ( cid : 123 ) c document to a simple series of probabilistic steps .
the model not only discovers what topics are expressed in a document , but also which authors are associated with each topic .
to simplify the representation of documents , we use
given the set of
choose an author
choose a topic given the author
choose a word given the topic
figure 123 : the graphical model for the author - topic model using plate notation .
a bag of words assumption that reduces each document to a vector of counts , where each vector element corresponds to the number of times a term appears in the document .
each author is associated with a multinomial distribution over topics .
a document with multiple authors has a dis - tribution over topics that is a mixture of the distributions associated with the authors .
when generating a document , an author is chosen at random for each individual word in the document .
this author picks a topic from his or her multinomial distribution over topics , and then samples a word from the multinomial distribution over words associ - ated with that topic .
this process is repeated for all words in the document .
in the model , the authors produce words from a set of t topics .
when t is kept relatively small relative to the number of authors and vocabulary size , the author - topic model applies a form of dimensionality reduction to docu - ments; topics are learned which capture the variability in word choice across a large set of documents and authors .
in our simulations , we use 123 topics ( see rosen - zvi et al .
( 123 ) for an exploration of di ( cid : 123 ) erent numbers of topics ) .
figure 123 illustrates the generative process with a graph - ical model using plate notation .
for readers not familiar with plate notation , shaded and unshaded variables indi - cate observed and latent variables respectively .
an arrow indicates a conditional dependency between variables and plates ( the boxes in the ( cid : 123 ) gure ) indicate repeated sampling with the number of repetitions given by the variable in the bottom ( see buntine ( 123 ) for an introduction ) .
author - topic model , observed variables not only include the words w in a document but also the set of coauthors ad on each document d .
currently , the model does not specify the generative process of how authors choose to collaborate .
in - stead , we assume the model is provided with the authorship information on every document in the collection .
each author ( from a set of k authors ) is associated with a multinomial distribution over topics , represented by ( cid : 123 ) .
each topic is associated with a multinomial distribution over words , represented by ( cid : 123 ) .
the multinomial distributions ( cid : 123 ) and ( cid : 123 ) have a symmetric dirichlet prior with hyperparame - ters ( cid : 123 ) and ( cid : 123 ) ( see rosen - zvi et al .
( 123 ) for details ) .
for each word in the document , we sample an author x uni - formly from ad , then sample a topic z from the multinomial distribution ( cid : 123 ) associated with author x and sample a word w from a multinomial topic distribution ( cid : 123 ) associated with topic z .
this sampling process is repeated n times to form
123 bayesian estimation of the model param
the author - topic model includes two sets of unknown parameters|the k author - topic distributions ( cid : 123 ) , and the t topic distributions ( cid : 123 ) |as well as the latent variables corre - sponding to the assignments of individual words to topics z and authors x .
the expectation - maximization ( em ) algo - rithm is a standard technique for estimating parameters in models with latent variables , ( cid : 123 ) nding a mode of the poste - rior distribution over parameters .
however , when applied to probabilistic topic models ( hofmann , 123 ) , this approach is susceptible to local maxima and computationally ine ( cid : 123 ) - cient ( see blei , ng , and jordan , 123 ) .
we pursue an alter - native parameter estimation strategy , outlined by gri ( cid : 123 ) ths and steyvers ( 123 ) , using gibbs sampling , a markov chain monte carlo algorithm to sample from the posterior distri - bution over parameters .
instead of estimating the model parameters directly , we evaluate the posterior distribution on just x and z and then use the results to infer ( cid : 123 ) and ( cid : 123 ) .
for each word , the topic and author assignment are sam -
p ( zi = j; xi = kjwi = m; z ( cid : 123 ) i; x ( cid : 123 ) i ) /
mj + ( cid : 123 )
kj + ( cid : 123 )
m123j + v ( cid : 123 )
pm123 cw t
pj 123 c at
kj 123 + t ( cid : 123 )
where zi = j and xi = k represent the assignments of the ith word in a document to topic j and author k respec - tively , wi = m represents the observation that the ith word is the mth word in the lexicon , and z ( cid : 123 ) i; x ( cid : 123 ) i represent all topic and author assignments not including the ith word .
furthermore , c w t is the number of times word m is as - signed to topic j , not including the current instance , and is the number of times author k is assigned to topic j , not including the current instance , and v is the size of the
during parameter estimation , the algorithm only needs to keep track of a v ( cid : 123 ) t ( word by topic ) count matrix , and a k ( cid : 123 ) t ( author by topic ) count matrix , both of which can be represented e ( cid : 123 ) ciently in sparse format .
from these count matrices , we can easily estimate the topic - word distributions ( cid : 123 ) and author - topic distributions ( cid : 123 ) by :
mj + ( cid : 123 )
m123j + v ( cid : 123 )
kj + ( cid : 123 )
pm123 cw t pj 123 c at
kj 123 + t ( cid : 123 )
where ( cid : 123 ) mj is the probability of using word m in topic j , and ( cid : 123 ) kj is the probability of using topic j by author k .
these values correspond to the predictive distributions over new words w and new topics z conditioned on w and z .
we start the algorithm by assigning words to random top - ics and authors ( from the set of authors on the document ) .
each gibbs sample then constitutes applying equation ( 123 ) to every word token in the document collection .
this sam - pling process is repeated for i iterations .
in this paper we primarily focus on results based on a single sample so that speci ( cid : 123 ) c topics can be identi ( cid : 123 ) ed and interpreted|in tasks in - volving prediction of words and authors one can average over topics and use multiple samples when doing so ( rosen - zvi
figure 123 : eight example topics extracted from the citeseer database .
each is illustrated with the 123 most likely words and authors with corresponding
et al . , 123 ) .
authortopics for citeseer 123 learning the model
our collection of citeseer abstracts contains d = 123; 123 abstracts with k = 123; 123 authors .
we preprocessed the text by removing all punctuation and common stop words .
this led to a vocabulary size of v = 123; 123 , and a total of 123; 123; 123 word tokens .
there is inevitably some noise in data of this form given that many of the ( cid : 123 ) elds ( paper title , author names , year , ab - stract ) were extracted automatically by citeseer from pdf or postscript or other document formats .
we chose the sim - ple convention of identifying authors by their ( cid : 123 ) rst initial and second name , e . g . , a einstein , given that multiple ( cid : 123 ) rst ini - tials or fully spelled ( cid : 123 ) rst names were only available for a rela - tively small fraction of papers .
this means of course that for some very common names ( e . g . , j wang or j smith ) there will be multiple actual individuals represented by a single name in the model .
this is a known limitation of working with this type of data ( e . g . , see newman ( 123 ) for further discussion ) .
there are algorithmic techniques that could be used to automatically resolve these identity problems|
figure 123 : the four most similar topics to the top - ics in the bottom row of figure 123 , obtained from a di ( cid : 123 ) erent markov chain run .
however , in this paper , we dont pursue these options and instead for simplicity work with the ( cid : 123 ) rst - initial / last - name representation of individual authors .
in our simulations , the number of topics t was ( cid : 123 ) xed at 123 and the smoothing parameters ( cid : 123 ) and ( cid : 123 ) ( figure 123 ) were set at 123 : 123 and 123 : 123 respectively .
we ran 123 independent gibbs sampling chains for 123 iterations each .
on a 123ghz pc workstation , each iteration took 123 seconds , leading to a total run time on the order of several days per chain .
123 authortopic and topicword models for
the citeseer database
we now discuss the author - topic and topic - word distribu - tions learned from the citeseer data .
figure 123 illustrates eight di ( cid : 123 ) erent topics ( out of 123 ) , obtained at the 123th iteration of a particular gibbs sampler run .
each table in figure 123 shows the 123 words that are most likely to be produced if that topic is activated , and the 123 authors who are most likely to have produced a word if it is known to have come from that topic .
the words associated with each topic are quite intuitive and , indeed , quite precise in the sense of conveying a semantic summary of a particular ( cid : 123 ) eld of research .
the authors associated with each topic are also quite representative|note that the top 123 authors associated with a topic by the model are not necessarily the most well - known authors in that area , but rather are the authors who tend to produce the most words for that topic ( in the citeseer abstracts ) .
the ( cid : 123 ) rst 123 topics at the top of figure 123 , topics #123 , #123 and #123 show examples of 123 quite speci ( cid : 123 ) c and precise topics on string matching , human - computer interaction , and as - tronomy respectively .
the bottom four topics ( #123 , #123 , #123 , and #123 ) are examples of topics with direct relevance to data mining|namely data mining itself , probabilistic learning , information retrieval , and database querying and indexing .
the model includes several other topics related to data mining , such as predictive modeling and neural net - works , as well as topics that span the full range of research areas encompassed by documents in citeseer .
the full list is available at http : / / www . datalab . uci . edu / author - topic .
topic #123 ( top right figure 123 ) provides an example of a topic that is not directly related to a speci ( cid : 123 ) c research area .
a fraction of topics , perhaps 123 to 123% , are devoted to \non - research - speci ( cid : 123 ) c " topics , the \glue " that makes up our re - search papers , including general terminology for describing methods and experiments , funding acknowledgments and parts of addresses ( which inadvertently crept in to the ab - stracts ) , and so forth .
we found that the topics obtained from di ( cid : 123 ) erent gibbs sampling runs were quite stable .
for example , figure 123 shows the 123 most similar topics to the topics in the bot - tom row of figure 123 , but from a di ( cid : 123 ) erent run .
there is some variability in terms of ranking of speci ( cid : 123 ) c words and authors for each topic , and in the exact values of the associ - ated probabilities , but overall the topics match very closely .
applications of the authortopic
model to citeseer 123 topic trends over time
of the original 123 , 123 abstracts in our data set , estimated years of publication were provided by citeseer for 123; 123 of these abstracts .
there is a steady ( and well - known ) increase year by year in the number of online documents through the 123s .
from 123 through 123 , however , the number of documents for which the year is known drops o ( cid : 123 ) sharply| the years 123 and 123 in particular are under - represented in this set .
this is due to fact that it is easier for citeseer to determine the date of publication of older documents , e . g . , by using citations to these documents .
we used the yearly data to analyze trends in topics over time .
using the same 123 topic model described earlier , the documents were partitioned by year , and for each year all of the words were assigned to their most likely topic using the model .
the fraction of words assigned to each topic for a given year was then calculated for each of the 123 topics and for each year from 123 to 123
these fractions provide interesting and useful indicators of relative topic popularity in the research literature in recent years .
figure 123 shows the results of plotting several di ( cid : 123 ) erent topics .
each topic is indicated in the legend by the ( cid : 123 ) ve most probable words in the topic .
the top left plot shows a steady increase ( roughly three - fold ) in machine learning and data mining topics .
the top right plot shows a \tale of two topics " : an increase in information - retrieval coupled to an apparent decrease in natural language processing .
on the second row , on the left we see a steady decrease in two \classical " computer science topics , operating systems and programming languages .
on the right , however , we see the reverse behavior , namely a corresponding substantial growth in web - related topics .
in the third row , the left plot illustrates trends within
database research : a decrease in the transaction and concurrency - related topic , query - related research holding steady over time , and a slow but steady increase in integration - related database research .
the plot on the right in the third row illustrates the changing fortunes of security - related research|a decline in the early 123s but then a seemingly dramatic upward trend starting around 123
the lower left plot on the bottom row illustrates the some - what noisy trends of three topics that were \hot " in the 123s : neural networks exhibits a steady decline since the early 123s ( as machine learning has moved on to areas such as support vector machines ) , genetic algorithms appears to
be relatively stable , and wavelets may have peaked in the 123 ( 123 time period .
finally , as with any large data set there are always some surprises in store .
the ( cid : 123 ) nal ( cid : 123 ) gure on the bottom right shows two somewhat unexpected \topics " .
the ( cid : 123 ) rst topic consists entirely of french words ( in fact the model discovered 123 such french language topics ) .
the apparent peaking of french words in the mid - 123s is likely to be an artifact of how cite - seer preprocesses data rather than any indication of french research productivity .
the lower curve corresponds to a topic consisting of largely greek letters , presumably from more theoretically oriented papers|fans of theory may be somewhat dismayed to see that there is an apparent steady decline in the relative frequency of greek letters in abstracts since the mid - 123s !
the time - trend results above should be interpreted with some caution .
as mentioned earlier , the data for 123 and 123 are relatively sparse compared to earlier years .
in addi - tion , the numbers are based on a rather skewed sample ( on - line documents obtained by the citeseer system for which years are known ) .
furthermore , the fractions per year only indicate the relative number of words assigned to a topic by the model and make no direct assessment of the quality or importance of a particular sub - area of computer science .
nonetheless , despite these caveats , the results are quite in - formative and indicate substantial shifts in research topics within the ( cid : 123 ) eld of computer science .
in terms of related work , popescul et al .
( 123 ) investi - gated time trends in citeseer documents using a document clustering approach .
123k documents were clustered into 123 clusters based on co - citation information while the text in - formation in the documents was not used .
our author - topic model uses the opposite approach .
in e ( cid : 123 ) ect we use the text information directly to discover topics and do not explic - itly model the \author network " ( although implicitly the co - author connections are used by the model ) .
a direct quantitative comparison is di ( cid : 123 ) cult , but we can say that our model with 123 topics appears to produce much more no - ticeable and precise time - trends than the 123 - cluster model .
123 topics and authors for new documents
in many applications , we would like to quickly assess the topic and author assignments for new documents not con - tained in our subset of the citeseer collection .
because our monte carlo algorithm requires signi ( cid : 123 ) cant processing time for 123k documents , it would be computationally ine ( cid : 123 ) cient to rerun the algorithm for every new document added to the collection ( even though from a bayesian inference viewpoint this is the optimal approach ) .
our strategy instead is to apply an e ( cid : 123 ) cient monte carlo algorithm that runs only on the word tokens in the new document , leading quickly to likely assignments of words to authors and topics .
we start by assigning words randomly to co - authors and topics .
we then sample new assignments of words to topics and authors by applying equation 123 only to the word tokens in the new document each time temporarily updating the count matri - ces cw t and c at .
the resulting assignments of words to authors and topics can be saved after a few iterations ( 123 iterations in our simulations ) .
figure 123 shows an example of this type of inference .
ab - stracts from two authors , b scholkopf and a darwiche were combined together into 123 \pseudo - abstract " and the docu - ment treated as if they had both written it .
these two au -
figure 123 : topic trends for research topics in computer science .
( auth123=scholkopf_b ( 123% , 123% ) ) ( auth123=darwiche_a ( 123% , 123% ) )
a method123 is described which like the kernel123 trick123 in support123 vector123 machines123 svms123 lets us generalize distance123 based123 algorithms to operate in feature123 spaces usually nonlinearly related to the input123 space this is done by identifying a class of kernels123 which can be represented as norm123 based123 distances123 in hilbert spaces it turns123 out that common kernel123 algorithms such as svms123 and kernel123 pca123 are actually really distance123 based123 algorithms and can be run123 with that class of kernels123 too as well as providing123 a useful new insight123 into how these algorithms work the present123 work can form the basis123 for conceiving new algorithms
this paper presents123 a comprehensive approach for model123 based123 diagnosis123 which includes proposals for characterizing and computing123 preferred123 diagnoses123 assuming that the system123 description123 is augmented with a system123 structure123 a directed123 graph123 explicating the interconnections between system123 components123 specifically we first introduce the notion of a consequence123 which is a syntactically123 unconstrained propositional123 sentence123 that characterizes all consistency123 based123 diagnoses123 and show123 that standard123 characterizations of diagnoses123 such as minimal conflicts123 correspond to syntactic123 variations123 on a consequence123 second we propose a new syntactic123 variation on the consequence123 known as negation123 normal form nnf and discuss its merits compared to standard variations third we introduce a basic algorithm123 for computing consequences in nnf given a structured system123 description we show that if the system123 structure123 does not contain cycles123 then there is always a linear size123 consequence123 in nnf which can be computed in linear time123 for arbitrary123 system123 structures123 we show a precise connection between the complexity123 of computing123 consequences and the topology of the underlying system123 structure123 finally we present123 an algorithm123 that enumerates123 the preferred123 diagnoses123 characterized by a consequence123 the algorithm123 is shown123 to take linear time123 in the size123 of the consequence123 if the preference criterion123 satisfies some general conditions
figure 123 : automated labeling of a pseudo - abstract from two authors by the model .
thors work in relatively di ( cid : 123 ) erent but not entirely unrelated sub - areas of computer science : scholkopf in machine learn - ing and darwiche in probabilistic reasoning .
the document is then parsed by the model .
i . e . , words are assigned to these authors .
we would hope that the author - topic model , condi - tioned now on these two authors , can separate the combined abstract into its component parts .
figure 123 shows the results after the model has classi ( cid : 123 ) ed each word according to the most likely author .
note that the model only sees a bag of words and is not aware of the word order that we see in the ( cid : 123 ) gure .
for readers viewing this in color , the more red a word is the more likely it is to have been generated ( according to the model ) by scholkopf ( and blue for darwiche ) .
for readers viewing the ( cid : 123 ) gure in black and white , the superscript 123 indicates words classi ( cid : 123 ) ed by the model for scholkopf , and superscript 123 for darwiche .
the results show that all of the signi ( cid : 123 ) cant content words ( such as kernel , support , vector , diagnoses , directed , graph ) are classi ( cid : 123 ) ed correctly .
as we might expect most of the \er - rors " are words ( such as \based " or \criterion " ) that are not speci ( cid : 123 ) c to either authors area of research .
were we to use word order in the classi ( cid : 123 ) cation , and classify ( for example ) whole sentences , the accuracy would increase further .
as it is , the model correctly classi ( cid : 123 ) es 123% of scholkopfs words and 123% of darwiches .
123 detecting the most surprising and least
surprising papers for an author
in tables 123 through 123 we used the model to score papers attributed to three well - known researchers in computer sci - ence ( christos faloutsos , michael jordan , and tom mitchell ) .
for each document for each of these authors we calculate a perplexity score .
perplexity is widely used in language modeling to assess the predictive power of a model .
it is a measure of how surprising the words are from the models perspective , loosely equivalent to the e ( cid : 123 ) ective branching fac - tor .
formally , the perplexity score of a new unobserved doc - ument d that contains a set of words wd and conditioned
on a topic model for a speci ( cid : 123 ) c author a is :
perplexity ( wdja ) = exp ( cid : 123 ) ( cid : 123 )
where p ( wdja ) is the probability assigned by the author topic model to the words wd conditioned on the single au - thor a , and jwdj is the number of words in the document .
even if the document was written by multiple authors we evaluate the perplexity score relative to a single author in order to judge perplexity relative to that individual .
our goal here is not to evaluate the out - of - sample predic - tive power of the model , but to explore the range of per - plexity scores that the model assigns to papers from speci ( cid : 123 ) c authors .
lower scores imply that the words w are less sur - prising to the model ( lower bounded by zero ) . in particular we are interested in the abstracts that the model consid - ers most surprising ( highest perplexity ) and least surprising ( lowest perplexity ) |in each table we list the 123 abstracts with the highest perplexity scores , the median perplexity , and the 123 abstracts with the lowest perplexity scores .
table 123 for christos faloutsos shows that the two papers with the highest perplexities have signi ( cid : 123 ) cantly higher per - plexity scores than the median and the two lowest perplexity papers .
the high perplexity papers are related to \query by example " and the qbic image database system , while the low perplexity papers are on high - dimensional indexing .
as far as the topic model for faloutsos is concerned , the index - ing papers are much more typical of his work than the query by example papers .
tables 123 and 123 provide interesting examples in that the most perplexing papers ( from the models viewpoint ) for each author are papers that the author did not write at all .
as mentioned earlier , by combining all t mitchells and m jordans together , the data set may contain authors who are di ( cid : 123 ) erent from tom mitchell at cmu and michael jor - dan at berkeley .
thus , the highest perplexity paper for t mitchell is in fact authored by a toby mitchell and is on the topic of estimating radiation doses ( quite di ( cid : 123 ) erent from the machine learning work of tom mitchell ) .
similarly , for michael jordan , the most perplexing paper is on software
table 123 : papers ranked by perplexity for c .
faloutsos , from 123 documents .
paper title perplexity score
mindreader : querying databases through multiple examples e ( cid : 123 ) cient and e ( cid : 123 ) ective querying by image content beyond uniformity and independence : analysis of r - trees using the concept of fractal dimension the tv - tree : an index structure for high - dimensional data
table 123 : papers ranked by perplexity for m .
jordan , from 123 documents .
paper title perplexity score
software con ( cid : 123 ) guration management in an object oriented database are arm trajectories planned in kinematic or dynamic coordinates ? an adaptation study on convergence properties of the em algorithm for gaussian mixtures supervised learning from incomplete data via an em approach
table 123 : papers ranked by perplexity for t .
mitchell from 123 documents .
paper title perplexity score
a method for estimating occupational radiation dose to individuals , using weekly dosimetry data text classi ( cid : 123 ) cation from labeled and unlabeled documents using em learning one more thing explanation based learning for mobile robot perception
con ( cid : 123 ) guration management and was written by mick jordan of sun microsystems .
in fact , of the 123 most perplexing pa - pers for m jordan , 123 are on software management and the java programming language , all written by mick jordan .
however , the 123nd most perplexing paper was in fact co - authored by michael jordan , but in the area of modeling of motor planning , which is a far less common topic compared to the machine learning papers that jordan typically writes .
an authortopic browser
we have built a java - based query interface tool that sup - ports interactive querying of the model123
the tool allows a user to query about authors , topics , documents , or words .
for example , given a query on a particular author the tool retrieves and displays the most likely topics and their prob - abilities for that author , the 123 most probable words for each topic , and the document titles in the database for that au - thor .
figure 123 ( a ) ( top panel ) shows the result of querying on pazzani m and the resulting topic distribution ( highly - ranked topics include machine learning , classi ( cid : 123 ) cation , rule - based systems , data mining , and information retrieval ) .
mouse - clicking on one of the topics ( e . g . , the data mining topic as shown in the ( cid : 123 ) gure ) produces the screen display to the left ( figure 123 ( b ) ) .
the most likely words for this topic and the most likely authors given a word from this topic are then displayed .
we have found this to be a useful technique for interactively exploring topics and authors , e . g . , which authors are active in a particular research area .
similarly , one can click on a particular paper ( e . g . , the paper a learning agent for wireless news access as shown in the lower screenshot ( figure 123 ( c ) ) and the display in the panel to the right is then produced .
this display shows the words in the documents and their counts , the probability distribution over topics for the paper given the word counts
123a prototype online version of the tool can be accessed at
( ranked by highest probability ( cid : 123 ) rst ) , and a probability dis - tribution over authors , based on the proportion of words assigned by the model to each topic and author respectively .
the system is implemented using a combination of a re - lational database and real - time bayesian estimation ( a rela - tively rare combination of these technologies for a real - time query - answering system as far as we are aware ) .
we use a database to store and index both ( a ) the sparse author - topic and topic - word count matrices that are learned by our algorithm from the training data , and ( b ) various tables de - scribing the data such as document - word , document - author , and document - title tables .
for a large document set such as citeseer ( and with 123 topics ) these tables can run into the hundreds of megabytes of memory|thus , we do not load them into main memory automatically but instead issue sql commands to retrieve the relevant records in real - time .
for most of the queries we have implemented to date the queries can be answered by simple table lookup followed by appropriate normalization ( if needed ) of the stored counts to generate conditional probabilities .
for example , display - ing the topic distribution for a speci ( cid : 123 ) c author is simply a matter of retrieving the appropriate record .
however , when a document is the basis of a query ( e . g . , as in the lower screenshot , figure 123 ( c ) ) we must compute in real - time the conditional distribution of the fraction of words assigned to each topic and author , a calculation that cannot be com - puted in closed form .
this requires retrieving all the rele - vant word - topic counts for the words in the document via sql , then executing the estimation algorithm outlined in section 123 in real - time using gibbs sampling , and display - ing the results to the user .
the user can change adjust the burn - in time , the number of samples and the lag time in the sampling algorithm|typically we have found that as few as 123 gibbs samples gives quite reasonable results ( and takes on the order of 123 or 123 seconds depending on the machine being used other factors ) .
figure 123 : examples of screenshots from the interactive query browser for the author - topic model with ( a ) querying on author pazzani m , ( b ) querying on a topic ( data mining ) relevant to that author , and ( c ) querying on a particular document written by the author .
we have introduced a probabilistic algorithm that can that can automatically extract information about authors , topics , and documents from large text corpora .
the method uses a generative probabilistic model that links authors to observed words in documents via latent topics .
we demon - strated that bayesian estimation can be used to learn such author - topic models from very large text corpora , using cite - seer abstracts as a working example .
the resulting citeseer author - topic model was shown to extract substantial novel \hidden " information from the set of abstracts , including topic time - trends , author - topic relations , unusual papers for speci ( cid : 123 ) c authors and so forth .
other potential applications not discussed here include recommending potential review - ers for a paper based on both the words in the paper and the names of the authors .
even though the underlying proba - bilistic model is quite simple , and ignores several aspects of real - world document generation ( such as topic correlation , author interaction , and so forth ) , it nonetheless provides a useful ( cid : 123 ) rst step in understanding author - topic structure in large text corpora .
we would like to thank steve lawrence , c .
lee giles , and isaac council for providing the citeseer data used in this paper .
we also thank momo alhazzazi , amnon meyers , and joshua omadadhain for assistance in software devel - opment and data preprocessing .
the research in this paper was supported in part by the national science foundation under grant iri - 123 via the knowledge discovery and dissemination ( kd - d ) program .
