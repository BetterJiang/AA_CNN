we dene a probability distribution over equivalence classes of binary matrices with a nite number of rows and an unbounded number of columns .
this distribution is suitable for use as a prior in probabilistic models that represent objects using a potentially innite array of features .
we identify a simple generative process that results in the same distribu - tion over equivalence classes , which we call the indian buffet process .
we illustrate the use of this distribution as a prior in an innite latent fea - ture model , deriving a markov chain monte carlo algorithm for inference in this model and applying the algorithm to an image dataset .
the statistical models typically used in unsupervised learning draw upon a relatively small repertoire of representations .
the simplest representation , used in mixture models , asso - ciates each object with a single latent class .
this approach is appropriate when objects can be partitioned into relatively homogeneous subsets .
however , the properties of many objects are better captured by representing each object using multiple latent features .
for instance , we could choose to represent each object as a binary vector , with entries indicat - ing the presence or absence of each feature ( 123 ) , allow each feature to take on a continuous value , representing objects with points in a latent space ( 123 ) , or dene a factorial model , in which each feature takes on one of a discrete set of values ( 123 , 123 ) .
a critical question in all of these approaches is the dimensionality of the representation : how many classes or features are needed to express the latent structure expressed by a set of objects .
often , determining the dimensionality of the representation is treated as a model selection problem , with a particular dimensionality being chosen based upon some measure of simplicity or generalization performance .
this assumes that there is a single , nite - dimensional representation that correctly characterizes the properties of the observed objects .
an alternative is to assume that the true dimensionality is unbounded , and that the observed objects manifest only a nite subset of classes or features ( 123 ) .
this alternative is pursued in nonparametric bayesian models , such as dirichlet process mixture models ( 123 , 123 , 123 , 123 ) .
in a dirichlet process mixture model , each object is assigned to a latent class , and each class is associated with a distribution over observable properties .
the prior dis - tribution over assignments of objects to classes is dened in such a way that the number of classes used by the model is bounded only by the number of objects , making dirichlet process mixture models innite mixture models ( 123 ) .
the prior distribution assumed in a dirichlet process mixture model can be specied in
terms of a sequential process called the chinese restaurant process ( crp ) ( 123 , 123 ) .
in the crp , n customers enter a restaurant with innitely many tables , each with innite seating capacity .
the ith customer chooses an already - occupied table k with probability mk where mk is the number of current occupants , and chooses a new table with probability i123+ .
customers are exchangeable under this process : the probability of a particular seating arrangement depends only on the number of people at each table , and not the order in which they enter the restaurant .
if we replace customers with objects and tables with classes , the crp species a distribu - tion over partitions of objects into classes .
a partition is a division of the set of n objects into subsets , where each object belongs to a single subset and the ordering of the subsets does not matter .
two assignments of objects to classes that result in the same division of objects correspond to the same partition .
for example , if we had three objects , the class assignments ( c123 , c123 , c123 ) = ( 123 , 123 , 123 ) would correspond to the same partition as ( 123 , 123 , 123 ) , since all that differs between these two cases is the labels of the classes .
a partition thus denes an equivalence class of assignment vectors .
the distribution over partitions implied by the crp can be derived by taking the limit of the probability of the corresponding equivalence class of assignment vectors in a model where class assignments are generated from a multinomial distribution with a dirichlet prior ( 123 , 123 ) .
in this paper , we derive an innitely exchangeable distribution over innite binary matrices by pursuing this strategy of taking the limit of a nite model .
we also de - scribe a stochastic process ( the indian buffet process , akin to the crp ) which generates this distribution .
finally , we demonstrate how this distribution can be used as a prior in statisti - cal models in which each object is represented by a sparse subset of an unbounded number of features .
further discussion of the properties of this distribution , some generalizations , and additional experiments , are available in the longer version of this paper ( 123 ) .
123 a distribution on innite binary matrices
in a latent feature model , each object is represented by a vector of latent feature values f i , and the observable properties of that object xi are generated from a distribution determined by its latent features .
latent feature values can be continuous , as in principal component analysis ( pca ) ( 123 ) , or discrete , as in cooperative vector quantization ( cvq ) ( 123 , 123 ) .
in the remainder of this section , we will assume that feature values are continuous .
using the ma -
trix f = ( cid : 123 ) f t
123 f t
n ( cid : 123 ) t to indicate the latent feature values for all n objects , the model
is specied by a prior over features , p ( f ) , and a distribution over observed property ma - trices conditioned on those features , p ( x|f ) , where p ( ) is a probability density function .
these distributions can be dealt with separately : p ( f ) species the number of features and the distribution over values associated with each feature , while p ( x|f ) determines how these features relate to the properties of objects .
our focus will be on p ( f ) , showing how such a prior can be dened without limiting the number of features .
we can break f into two components : a binary matrix z indicating which features are pos - sessed by each object , with zik = 123 if object i has feature k and 123 otherwise , and a matrix v indicating the value of each feature for each object .
f is the elementwise product of z and v , f = z v , as illustrated in figure 123
in many latent feature models ( e . g . , pca ) objects have non - zero values on every feature , and every entry of z is 123
in sparse latent feature models ( e . g . , sparse pca ( 123 , 123 ) ) only a subset of features take on non - zero values for each object , and z picks out these subsets .
a prior on f can be dened by specifying priors for z and v , with p ( f ) = p ( z ) p ( v ) , where p ( ) is a probability mass function .
we will focus on dening a prior on z , since the effective dimensionality of a latent feature model is determined by z .
assuming that z is sparse , we can dene a prior for innite la - tent feature models by dening a distribution over innite binary matrices .
our discussion of the chinese restaurant process provides two desiderata for such a distribution : objects
figure 123 : a binary matrix z , as shown in ( a ) , indicates which features take non - zero values .
elementwise multiplication of z by a matrix v of continuous values produces a represen - tation like ( b ) .
if v contains discrete values , we obtain a representation like ( c ) .
should be exchangeable , and posterior inference should be tractable .
it also suggests a method by which these desiderata can be satised : start with a model that assumes a nite number of features , and consider the limit as the number of features approaches innity .
123 a nite feature model
we have n objects and k features , and the possession of feature k by object i is indicated by a binary variable zik .
the zik form a binary n k feature matrix , z .
assume that each object possesses feature k with probability k , and that the features are generated independently .
under this model , the probability of z given = ( 123 , 123 , .
, k ) , is
p ( z| ) =
p ( zik|k ) =
k ( 123 k ) n mk ,
on by assuming that each k follows a beta distribution , to give
i=123 zik is the number of objects possessing feature k .
we can dene a prior
where mk =pn
k | beta ( zik | k bernoulli ( k )
k , 123 )
each zik is independent of all other assignments , conditioned on k , and the k are gener - ated independently .
we can integrate out to obtain the probability of z , which is
p ( z ) =
k ( mk +
k ) ( n mk + 123 )
( n + 123 +
this distribution is exchangeable , since mk is not affected by the ordering of the objects .
123 equivalence classes
in order to nd the limit of the distribution specied by equation 123 as k , we need to dene equivalence classes of binary matrices the analogue of partitions for class assign - ments .
our equivalence classes will be dened with respect to a function on binary matri - ces , lof ( ) .
this function maps binary matrices to left - ordered binary matrices .
lof ( z ) is obtained by ordering the columns of the binary matrix z from left to right by the magnitude of the binary number expressed by that column , taking the rst row as the most signicant bit .
the left - ordering of a binary matrix is shown in figure 123
in the rst row of the left - ordered matrix , the columns for which z123k = 123 are grouped at the left .
in the second row , the columns for which z123k = 123 are grouped at the left of the sets for which z123k = 123
this grouping structure persists throughout the matrix .
the history of feature k at object i is dened to be ( z123k , .
, z ( i123 ) k ) .
where no object is specied , we will use history to refer to the full history of feature k , ( z123k , .
, zn k )
figure 123 : left - ordered form .
a binary matrix is transformed into a left - ordered binary matrix by the function lof ( ) .
the entries in the left - ordered matrix were generated from the indian buffet process with = 123
empty columns are omitted from both matrices .
and k+ =p123n
will individuate the histories of features using the decimal equivalent of the binary numbers corresponding to the column entries .
for example , at object 123 , features can have one of four histories : 123 , corresponding to a feature with no previous assignments , 123 , being a feature for which z123k = 123 but z123k = 123 , 123 , being a feature for which z123k = 123 but z123k = 123 , and 123 , being a feature possessed by both previous objects were assigned .
kh will denote the number of features possessing the history h , with k123 being the number of features for which mk = 123 h=123 kh being the number of features for which mk > 123 , so k = k123 + k+ .
two binary matrices y and z are lof - equivalent if lof ( y ) = lof ( z ) .
the lof - equivalence class of a binary matrix z , denoted ( z ) , is the set of binary matrices that are lof - equivalent to z .
lof - equivalence classes play the role for binary matrices that parti - tions play for assignment vectors : they collapse together all binary matrices ( assignment vectors ) that differ only in column ordering ( class labels ) .
lof - equivalence classes are pre - served through permutation of the rows or the columns of a matrix , provided the same permutations are applied to the other members of the equivalence class .
performing infer - ence at the level of lof - equivalence classes is appropriate in models where feature order is not identiable , with p ( x|f ) being unaffected by the order of the columns of f .
any model in which the probability of x is specied in terms of a linear function of f , such as pca or cvq , has this property .
the cardinality of the lof - equivalence class ( z ) is
, where kh is the number of columns with full history h .
123 taking the innite limit
under the distribution dened by equation 123 , the probability of a particular lof - equivalence class of binary matrices , ( z ) , is
p ( ( z ) ) = xz ( z )
p ( z ) =
k ( mk +
k ) ( n mk + 123 )
( n + 123 +
rearranging terms , and using the fact that ( x ) = ( x 123 ) ( x 123 ) for x > 123 , we can compute the limit of p ( ( z ) ) as k approaches innity
k123 ! k k+
( n mk ) ! ( mk 123 ) !
where hn is the nth harmonic number , hn = pn
this distribution is innitely exchangeable , since neither kh nor mk are affected by the ordering on objects .
technical details of this limit are provided in ( 123 ) .
123 the indian buffet process
the probability distribution dened in equation 123 can be derived from a simple stochastic process .
due to the similarity to the chinese restaurant process , we will also use a culinary metaphor , appropriately adjusted for geography .
indian restaurants in london offer buffets with an apparently innite number of dishes .
we will dene a distribution over innite binary matrices by specifying how customers ( objects ) choose dishes ( features ) .
in our indian buffet process ( ibp ) , n customers enter a restaurant one after another .
each customer encounters a buffet consisting of innitely many dishes arranged in a line .
the rst customer starts at the left of the buffet and takes a serving from each dish , stopping after a poisson ( ) number of dishes .
the ith customer moves along the buffet , sampling dishes in proportion to their popularity , taking dish k with probability mk i , where mk is the number of previous customers who have sampled that dish .
having reached the end of all previous sampled dishes , the ith customer then tries a poisson ( i ) number of new dishes .
we can indicate which customers chose which dishes using a binary matrix z with n rows and innitely many columns , where zik = 123 if the ith customer sampled the kth dish .
using k ( i ) bility of any particular matrix being produced by the ibp is
to indicate the number of new dishes sampled by the ith customer , the proba -
p ( z ) =
i=123 k ( i )
( n mk ) ! ( mk 123 ) !
the matrices produced by this process are generally not in left - ordered form .
these ma - trices are also not ordered arbitrarily , because the poisson draws always result in choices of new dishes that are to the right of the previously sampled dishes .
customers are not exchangeable under this distribution , as the number of dishes counted as k ( i ) upon the order in which the customers make their choices .
however , if we only pay at - tention to the lof - equivalence classes of the matrices generated by this process , we obtain the innitely exchangeable distribution p ( ( z ) ) given by equation 123 : qn generated via this process map to the same left - ordered form , and p ( ( z ) ) is obtained by multiplying p ( z ) from equation 123 by this quantity .
a similar but slightly more compli - cated process can be dened to produce left - ordered matrices directly ( 123 ) .
123 conditional distributions
to dene a gibbs sampler for models using the ibp , we need to know the conditional distribution on feature assignments , p ( zik = 123|z ( ik ) ) .
in the nite model , where p ( z ) is given by equation 123 , it is straightforward to compute this conditional distribution for any zik .
integrating over k gives
p ( zik = 123|zi , k ) =
where zi , k is the set of assignments of other objects , not including i , for feature k , and mi , k is the number of objects possessing feature k , not including i .
we need only condi - tion on zi , k rather than z ( ik ) because the columns of the matrix are independent .
in the innite case , we can derive the conditional distribution from the ( exchangeable ) ibp .
choosing an ordering on objects such that the ith object corresponds to the last customer to visit the buffet , we obtain
p ( zik = 123|zi , k ) =
for any k such that mi , k > 123
the same result can be obtained by taking the limit of equation 123 as k .
the number of new features associated with object i should be
drawn from a poisson ( same kind of limiting argument as that presented above .
n ) distribution .
this can also be derived from equation 123 , using the
123 a linear - gaussian binary latent feature model
to illustrate how the ibp can be used as a prior in models for unsupervised learning , we derived and tested a linear - gaussian latent feature model in which the features are binary .
in this case the feature matrix f reduces to the binary matrix z .
as above , we will start with a nite model and then consider the innite limit .
in our nite model , the d - dimensional vector of properties of an object i , xi is generated from a gaussian distribution with mean zia and covariance matrix x = 123 zi is a k - dimensional binary vector , and a is a k d matrix of weights .
notation , e ( x ) = za .
if z is a feature matrix , this is a form of binary factor analysis .
the distribution of x given z , a , and x is matrix gaussian with mean za and covariance x i , where i is the identity matrix .
the prior on a is also matrix gaussian , with mean 123 and covariance matrix 123
integrating out a , we have
p ( x|z , x , a ) =
( 123 ) n d / 123 ( n k ) d
a |zt z + 123
tr ( xt ( i z ( zt z +
i ) 123zt ) x ) )
this result is intuitive : the exponentiated term is the difference between the inner product of x and its projection onto the space spanned by z , regularized to an extent determined by the ratio of the variance of the noise in x to the variance of the prior on a .
it follows that p ( x|z , x , a ) depends only on the non - zero columns of z , and thus remains well - dened when we take the limit as k ( for more details see ( 123 ) ) .
we can dene a gibbs sampler for this model by computing the full conditional distribution
p ( zik|x , z ( i , k ) , x , a ) p ( x|z , x , a ) p ( zik|zi , k ) .
the two terms on the right hand side can be evaluated using equations 123 and 123 respectively .
the gibbs sampler is then straightforward .
assignments for features for which mi , k > 123 are drawn from the distribution specied by equation 123
the distribution over the number of new features for each object can be approximated by truncation , computing probabilities for a range of values of k ( i ) 123 up to an upper bound .
for each value , p ( x|z , x , a ) can be computed from equation 123 , and the prior on the number of new features is poisson ( we will demonstrate this gibbs sampler for the innite binary linear - gaussian model on a dataset consisting of 123 123 123 pixel images .
we represented each image , xi , using a 123 - dimensional vector corresponding to the weights of the mean image and the rst 123 principal components .
each image contained up to four everyday objects a $123 bill , a klein bottle , a prehistoric handaxe , and a cellular phone .
each object constituted a single latent feature responsible for the observed pixel values .
the images were generated by sampling a feature vector , zi , from a distribution under which each feature was present with probability 123 , and then taking a photograph containing the appropriate objects using a logitech digital webcam .
sample images are shown in figure 123 ( a ) .
the gibbs sampler was initialized with k+ = 123 , choosing the feature assignments for the rst column by setting zi123 = 123 with probability 123 .
a , x , and were initially set to 123 , 123 , and 123 respectively , and then sampled by adding metropolis steps to the mcmc algorithm .
figure 123 shows trace plots for the rst 123 iterations of mcmc for the number of features used by at least one object , k+ , and the model parameters a , x , and .
all of these quantities stabilized after approximately 123 iterations , with the algorithm
123 123 123 123
123 123 123 123
123 123 123 123
123 123 123 123
figure 123 : data and results for the demonstration of the innite linear - gaussian binary latent feature model .
( a ) four sample images from the 123 in the dataset .
each image had 123 123 pixels , and contained from zero to four everyday objects .
( b ) the posterior mean of the weights ( a ) for the four most frequent binary features from the 123th sample .
each image corresponds to a single feature .
these features perfectly indicate the presence or absence of the four objects .
the rst feature indicates the presence of the $123 bill , the other three indicate the absence of the klein bottle , the handaxe , and the cellphone .
( c ) reconstructions of the images in ( a ) using the binary codes inferred for those images .
these reconstructions are based upon the posterior mean of a for the 123th sample .
for example , the code for the rst image indicates that the $123 bill is absent , while the other three objects are not .
the lower panels show trace plots for the dimensionality of the representation ( k+ ) and the parameters , x , and a over 123 iterations of sampling .
the values of all parameters stabilize after approximately 123 iterations .
nding solutions with approximately seven latent features .
the four most common features perfectly indicated the presence and absence of the four objects ( shown in figure 123 ( b ) ) , and three less common features coded for slight differences in the locations of those objects .
we have shown that the methods that have been used to dene innite latent class models ( 123 , 123 , 123 , 123 , 123 , 123 , 123 ) can be extended to models in which objects are represented in terms of a set of latent features , deriving a distribution on innite binary matrices that can be used as a prior for such models .
while we derived this prior as the innite limit of a simple distribution on nite binary matrices , we have shown that the same distribution can be specied in terms of a simple stochastic process the indian buffet process .
this distribution satises our two desiderata for a prior for innite latent feature models : objects are exchangeable , and inference remains tractable .
our success in transferring the strategy of taking the limit of a nite model from latent classes to latent features suggests that a similar approach could be applied with other representations , expanding the forms of latent structure that can be recovered through unsupervised learning .
