inference algorithms for topic models are typ - ically designed to be run over an entire col - lection of documents after they have been observed .
however , in many applications of these models , the collection grows over time , making it infeasible to run batch algorithms repeatedly .
this problem can be addressed by using online algorithms , which update es - timates of the topics as each document is observed .
we introduce two related rao - blackwellized online inference algorithms for the latent dirichlet allocation ( lda ) model incremental gibbs samplers and particle l - ters and compare their runtime and perfor - mance to that of existing algorithms .
probabilistic topic models are often used to analyze collections of documents , each of which is represented as a mixture of topics , where each topic is a proba - bility distribution over words .
applying these mod - els to a document collection involves estimating the topic distributions and the weight each topic receives in each document .
a number of algorithms exist for solving this problem ( e . g . , hofmann , 123; blei et al . , 123; minka and laerty , 123; griths and steyvers , 123 ) , most of which are intended to be run in batch mode , being applied to all the documents once they are collected .
however , many applications of topic mod - els are in contexts where the collection of documents is growing .
for example , when inferring the topics of news articles or communications logs , documents
appearing in proceedings of the 123th international confe - rence on articial intelligence and statistics ( aistats ) 123 , clearwater beach , florida , usa .
volume 123 of jmlr : w&cp 123
copyright 123 by the authors .
arrive in a continuous stream , and decisions must be made on a regular basis , without waiting for future documents to arrive .
in these settings , repeatedly run - ning a batch algorithm can be infeasible or wasteful .
in this paper , we explore the possibility of using online inference algorithms for topic models , whereby the rep - resentation of the topics in a collection of documents is incrementally updated as each document is added .
in addition to providing a solution to the problem of growing document collections , online algorithms also open up dierent routes for parallelization of infer - ence from batch algorithms , providing ways to draw on the enhanced computing power of multiprocessor systems , and dierent tradeos in runtime and perfor - mance from other algorithms .
we discuss algorithms for a particular topic model : la - tent dirichlet allocation ( lda ) ( blei et al . , 123 ) .
the state space from which these algorithms draw samples is dened at time i to be all possible topic assignments to each of the words in the documents observed up to time i .
the result is a rao - blackwellized sampling scheme ( doucet et al . , 123 ) , analytically integrating out the distributions over words associated with topics and the per - document weights of those topics .
the plan of the paper is as follows .
section 123 intro - duces the lda model in more detail .
section 123 dis - cusses one batch and three online algorithms for sam - pling from lda .
section 123 discusses the ecient im - plementation of one of the online algorithms particle lters .
section 123 describes a comparative evaluation of the algorithms , and section 123 concludes the paper .
123 inferring topics
latent dirichlet allocation ( blei et al . , 123 ) is widely used for identifying the topics in a set of documents , building on previous work by hofmann ( 123 ) .
in this model , each document is represented as a mixture of a xed number of topics , with topic z receiving weight
123 online inference of topics with latent dirichlet allocation
in document d , and each topic is a probability dis - tribution over a nite vocabulary of words , with word w having probability ( z ) w in topic z .
the generative model assumes that documents are produced by inde - pendently sampling a topic z for each word from ( d ) and then independently sampling the word from ( z ) .
the independence assumptions mean that the docu - ment is treated as a bag of words , so word ordering is irrelevant to the model .
symmetric dirichlet priors are placed on ( d ) and ( z ) , with ( d ) dirichlet ( ) and ( z ) dirichlet ( ) , where and are hyper - parameters that aect the relative sparsity of these distributions .
the complete probability model is thus
wi|zi , ( zi ) discrete ( ( zi ) ) ,
i = 123 , .
, n , z = 123 , .
, t , i = 123 , .
, n , d = 123 ,
where n is the total number of words in the collection , t is the number of topics , d is the number of docu - ments , and di and zi are , respectively , the document and topic of the ith word , wi .
the goal of inference in this model is to identify the values of and , given a document collection represented by the sequence of n words wn = ( w123 , .
, wn ) .
estimation is complicated by the latent variables zn = ( z123 , .
, zn ) , the topic assignments of the words .
various algorithms have been proposed for solving this problem , including a variational expectation - maximization algorithm ( blei et al . , 123 ) and expectation - propagation ( minka and laerty , 123 ) .
in the collapsed gibbs sampling algo - rithm of griths and steyvers ( 123 ) , and are ana - lytically integrated out of the model to collect samples from p ( zn|wn ) .
the use of conjugate dirichlet priors on and makes this analytic integration straightfor - ward , and also makes it easy to recover the posterior distribution on and given zn and wn , meaning that a set of samples from p ( zn|wn ) is sucient to estimate and .
existing inference algorithms provide users with sev - eral options in trading o bias and runtime .
however , most of these algorithms are designed to be run over an entire document collection , requiring multiple sweeps to produce good estimates of and .
while some ap - plications of these models involve the analysis of static databases , more typically , users work with document collections that grow over time .
in the remainder of the paper , we outline three related algorithms that can be used for inference in such a setting .
in this section , we describe a batch sampling algorithm for lda .
we then discuss ways in which this algorithm can be extended to yield three online algorithms .
algorithm 123 batch gibbs sampler for lda 123 : initialize zn randomly from ( 123 ,
choose j from ( 123 , .
, n ) sample zj from p ( zj|zn\j , wn )
123 batch gibbs sampler
griths and steyvers ( 123 ) presented a collapsed gibbs sampler for lda , where the state space is the set of all possible topic assignments to the words in every document .
the gibbs sampler is collapsed because the variables and are analytically integrated out , and only the latent topic variables zn are sampled .
the topic assignment of word j is sampled according to its conditional distribution p ( zj|zn\j , wn ) n ( wj )
zj , n\j + zj , n\j + w
zj , n\j + , n\j + t
where zn\j indicates ( z123 , .
, zj123 , zj+123 , .
, zn ) , w is the size of the vocabulary , n ( wj ) zj , n\j is the number of times word wj is assigned to topic zj , n ( ) zj , n\j is the total number of words assigned to topic zj , n ( dj ) zj , n\j is the number of times a word in document dj is assigned to topic zj , and n ( dj ) , n\j is the total number of words in document dj , and all the counts are taken over words 123 through n , excluding the word at position j itself ( hence the n\j subscripts ) .
the gibbs sampling procedure , outlined in algo - rithm 123 , converges to the desired posterior distribu - tion p ( zn|wn ) .
this batch gibbs sampler can be extended in several ways , leading to ecient online sampling algorithms for lda .
a simple modication of the batch gibbs sampler yields an online algorithm presented by song et al .
( 123 ) and called o - lda by banerjee and basu ( 123 ) .
this procedure , outlined in algorithm 123 , rst applies the batch gibbs sampler to a prex of the full dataset , then samples the topic of each new word i by conditioning on the words observed so far123 :
p ( zi|zi123 , wi ) n ( wi )
zi , i\i + w
, i\i + t
123the o - lda algorithm as presented by banerjee and basu ( 123 ) samples the next topic by conditioning only on the topics of the words up to the end of the previous document , rather than all previous words .
the algorithm presented here is slightly slower , but more accurate .
123 canini , shi , griths
123 = p 123 for p = 123 ,
algorithm 123 particle lter for lda 123 : initialize weights ( p ) 123 : for i = 123 , .
, n do
for p = 123 , .
, p do
i = ( p )
from p ( z ( p )
normalize weights i to sum to 123 if ( cid : 123 ) i ( cid : 123 ) 123 ess threshold then
for j in r ( i ) do
for p = 123 , .
, p do
i = p 123 for p = 123 ,
from p ( z ( p )
algorithm 123 o - lda ( initialized with rst words ) 123 : sample z using batch gibbs sampler 123 : for i = + 123 , .
, n do
sample zi from p ( zi|zi123 , wi )
algorithm 123 incremental gibbs sampler for lda 123 : for i = 123 , .
, n do sample zi from p ( zi|zi123 , wi ) for j in r ( i ) do
sample zj from p ( zj|zi\j , wi )
after its batch initialization phase , o - lda applies equation ( 123 ) incrementally for each new word wi , never resampling old topic variables .
for this reason , its performance depends critically on the accuracy of the topics inferred during the batch phase .
if the doc - uments used to initialize o - lda are not representative of the full dataset , it could be led to make poor infer - ences .
also , because each topic variable is sampled by conditioning only on previous words and topics , sam - ples drawn with o - lda are not distributed according to the true posterior distribution p ( zn|wn ) .
to rem - edy these issues , we consider online algorithms that re - vise their decisions about previous topic assignments .
incremental gibbs sampler
extending o - lda to occasionally resample topic vari - ables , we introduce the incremental gibbs sampler , an algorithm that rejuvenates old topic assignments in light of new data .
the incremental gibbs sampler , outlined in algorithm 123 , does not have a batch initial - ization phase like o - lda , but it does use equation ( 123 ) to sample topic variables of new words .
after each step i , the incremental gibbs sampler resamples the topics of some of the previous words .
the topic assignment zj of each index j in the rejuvenation sequence r ( i ) is drawn from its conditional distribution
p ( zj|zi\j , wi ) n ( wj )
zj , i\j + zj , i\j + w
zj , i\j + , i\j + t
if these rejuvenation steps are performed often enough ( depending on the mixing time of the induced markov chain ) , the incremental gibbs sampler closely approxi - mates the posterior distribution p ( zi|wi ) at every step i .
indeed , convergence is guaranteed as the number of times each zj is resampled goes to innity , since the al - gorithm becomes a batch gibbs sampler for p ( zi|wi ) in the limit .
more generally , the incremental gibbs sampler is an instance of the decayed mcmc frame - work introduced by marthi et al .
( 123 ) .
the choice of the number of rejuvenation steps to perform deter - mines the runtime of the incremental gibbs sampler .
if |r ( i ) | is bounded as a function of i , then the overall runtime is linear .
however , if |r ( i ) | grows logarith - mically or linearly with i , then the overall runtime is log - linear or quadratic , respectively .
r ( i ) can also be chosen to be nonempty only at certain intervals , lead - ing to an incremental gibbs sampler that only rejuve - nates itself periodically ( for example , whenever there is time to spare between observing documents ) .
an alternative approach to frequently resampling pre - vious topic assignments is to concurrently maintain multiple samples of zi , rejuvenating them less fre - quently .
this option is desirable because it allows the algorithm to simultaneously explore several regions of the state space .
it is also useful in a multi - processor environment , since it is simpler to parallelize multiple samples dedicating each sample to a single machine than it is to parallelize operations on one sample .
an ensemble of independent samples from the incremen - tal gibbs sampler could be used to approximate the posterior distribution p ( zn|wn ) ; however , if the sam - ples are not rejuvenated often enough , they will not have the desired distribution .
with this motivation , we turn to particle lters , which perform importance weighting on a set of sequentially - generated samples .
123 particle filter
particle lters are a sequential monte carlo method commonly used for approximating a probability dis - tribution over a latent variable as observations are ac - quired ( doucet et al . , 123 ) .
we can extend the incre - mental gibbs sampler to obtain a rao - blackwellized particle lter ( doucet et al . , 123 ) , again analytically integrating out and to sample from p ( zi|wi ) .
this use of particle lters is slightly nonstandard , since the state space grows with each observation .
the particle lter for lda , outlined in algorithm 123 , updates samples from p ( zi123|wi123 ) to generate sam -
123 online inference of topics with latent dirichlet allocation
ples from the target distribution p ( zi|wi ) after each it does this by rst generat - word wi is observed .
ing a value of z ( p ) for each particle p from a proposal i123 , wi ) .
the prior distribution , i123 , wi123 ) , is typically used for the proposal because it is often infeasible to sample from the pos - terior , p ( z ( p ) i123 , wi ) .
however , since zi is drawn from a constant , nite set of values , we can use the posterior , which is given by equation ( 123 ) and min - imizes the variance of the resulting particle weights ( doucet et al . , 123 ) .
next , the unnormalized impor - tance weights of the particles are calculated using the standard iterative equation
= p ( wi|z ( p )
, wi123 ) p ( z ( p )
the weights are then normalized to sum to 123
equa - tion ( 123 ) is designed so that after the weight normaliza - tion step , the particle lter approximates the posterior distribution over topic assignments as follows :
p ( zi|wi ) p ( cid : 123 )
where 123zi ( ) is the indicator function for zi .
as p , the right side converges to the left side , since zi can assume only a nite number of values .
over time , the weights assigned to particles diverge signicantly , as a few particles come to provide a sig - nicantly better account of the observed data than the others .
resampling addresses this issue by producing a new set of particles that are more highly concentrated on states with high weight whenever the variance of the weights becomes large .
a standard measure of weight variance is an approximation to the eective sample size , ess ( cid : 123 ) ( cid : 123 ) 123 , and a threshold can be expressed as some proportion of the number of particles , p .
the simplest form of resampling is to draw from the multinomial distribution dened by the normal - ized weights .
however , more sophisticated resampling methods also exist , such as stratied sampling ( kita - gawa , 123 ) , quasi - deterministic methods ( fearnhead , 123 ) , and residual resampling ( liu and chen , 123 ) , which produce more diverse sets of particles .
residual resampling was used in our evaluations .
whenever the particles are resampled , their weights are all reset to p 123 , since each is now a draw from the same distri - bution and the previous weights are reected in their relative resampling frequencies .
as in the resample - move algorithm of gilks and berzuini ( 123 ) , markov chain monte carlo ( mcmc )
is used after particle resampling to restore diversity to the particle set in the same way that the incremen - tal gibbs sampler rejuvenates its samples , by choosing a rejuvenation sequence r ( i ) of topic variables to re - sample .
the length of r ( i ) can be chosen to trade o runtime against performance , and the variables to be resampled can be randomly selected either uniformly or using a decayed distribution that favors more recent history , as in marthi et al .
( 123 ) .
while a uniform schedule visits earlier sites more overall , using a distri - bution that approaches zero quickly enough for sites in the past ensures that in expectation , each site is sampled the same number of times .
123 efficient implementation
in order to be feasible as an online algorithm , the par - ticle lter must be implemented with an ecient data in particular , the amount of time it takes to incrementally process a document must not grow with the amount of data previously seen .
in ini - tial implementations of the algorithm , individual par - ticles were represented as linear arrays of topic assign - ment values , consistent with the interpretation of the state space zi = ( z123 , .
, zi ) as a sequence of variables stored in an array .
it was found that nearly all of the computing time was spent resampling the parti - cles ( line 123 of algorithm 123 ) .
this is due to the fact that when a particle is resampled more than once , the nave implementation makes copies of the zi ar - ray for each child particle .
since these structures grow linearly with the observed data and resampling is per - formed at a roughly constant rate , the total time spent resampling particles grows quadratically .
this problem can be alleviated by using a shared rep - resentation of the particles , exploiting the high degree of redundancy among particles with common lineages .
when a particle is resampled multiple times , the re - sulting copies all share the same parent particle and implicitly inherit its zi vector as their history of topic assignments .
each particle maintains a hash table that is used to store the dierences between its topic as - signments and its parents; consequently , the compu - tational complexity of the resampling step is reduced from quadratic to linear .
as illustrated in figure 123 , the particles are thus stored as a directed tree123 , with parent - child relationships indicating the hierarchy of inheritance for topic variables .
to look up the value z ( p ) of topic assignment i in par - ticle p , the particles hash table is consulted rst .
if the value is missing , the particles parents hash table is checked , recursing up the tree towards the root and
123more precisely , a forest of directed trees , since it is
possible that not all particles share a common ancestor .
123 canini , shi , griths
idence conrms that the time overhead of maintaining a directed tree of hash tables is negligible compared to the increase in speed and decrease in memory usage it aords .
specically , by reducing the resampling step to have linear runtime , this implementation detail is the key to making the particle lter feasible to run .
in online topic modeling settings , such as news article clustering , we care about two aspects of performance : the quality of the solutions recovered and runtime .
as documents arrive and are incrementally processed , we would like online algorithms to maintain high - quality inferences and to produce topic labels quickly for new documents .
since there is a tradeo between runtime and inference quality , the algorithms were evaluated by comparing the quality of their inferences while con - straining the amount of time spent per document .
we compared the performance of the three online algo - rithms presented in section 123 : o - lda , the incremental gibbs sampler , and the particle lter .
our evaluation is a variation of the comparison of o - lda to other on - line algorithms by banerjee and basu ( 123 ) , using the same datasets and performance metric .
the datasets used to test the algorithms are each a collection of categorized documents .
they consist of four subsets derived from the 123 newsgroups corpus123 : ( 123 ) diff - 123 ( 123 documents , 123 word types , 123 cate - gories ) , ( 123 ) rel - 123 ( 123 documents , 123 word types , 123 categories ) , ( 123 ) sim - 123 ( 123 documents , 123 word types , 123 categories ) , and ( 123 ) subset - 123 ( 123 docu - ments , 123 word types , 123 categories ) , represent - ing dierent levels of size and diculty , as well as news articles harvested from the slashdot website : ( 123 ) slash - 123 ( 123 documents , 123 word types , 123 cate - gories ) and ( 123 ) slash - 123 ( 123 documents , 123 word types , 123 categories ) .
our testing methodology is designed to approximate a real - world online inference task .
for each dataset , the algorithms were given the rst 123% of the documents to use for initialization .
a single sample drawn using the batch gibbs sampler on this initial set was used to initialize all of the online algorithms .
this constituted the explicit batch initialization phase of o - lda , and the other two online algorithms used the same starting
123available online at http : / / people . csail . mit . edu /
figure 123 : an example of the directed tree of hashta - bles implementation of the particle lter .
particle 123 is the root , so all other particles descended from it .
particle 123 directly depends on particle 123 , altering the topics of the words choosing and where .
the other child of particle 123 is not itself an active sample , but an inactive remnant of an old particle that was not resam - pled .
it is retained because multiple active particles depend on its hashed topic values .
if either particle 123 or particle 123 is not resampled in the future , the remain - ing one will be merged with its parent , maintaining the bound on the tree depth .
eventually terminating when the value is found in an ancestors hash table .
to change the value z ( p ) new value is inserted into particle ps hash table , and the old value is inserted into each of particle ps chil - drens hash tables ( if they dont already have an entry ) to ensure consistency .
in order to ensure that variable lookup is a constant - time operation , it is necessary that the depth of the tree does not grow with the amount of data .
this can be ensured by selectively pruning the tree just after the resampling step .
after resampling is performed , a node is called active if it has been sampled one or more times , and inactive otherwise .
if an entire sub - tree of nodes is inactive , it is deleted .
if an inactive node has only one active descendant depending on its history , that descendants hash table is merged with its own .
when this operation is performed after each resampling step , it can be shown that the depth of the tree is never greater than p .
furthermore , merging the hash tables takes linear amortized time .
empirical ev -
123 ! " #$%& ( # ) * ! + , - . ( ) - / 123 - 123 ! 123$ - / / - 123 ( . 123 ! ) 123 - 123 - ! " 123$123 - 123 - +123 ! " : - 123; - &123$ ( $ - , < - ) - , = - # ( . & - / > - ) 123$ - , ? - 123 ! " $ - / ! " #$%& ( ) * ! " #$%& ( # ) * ! +123 - +123 ! " : - 123; - &123$ ( $ - 123 ! " #$%& ( ) + ! " #$%& ( # ) * ! + , - . ( ) - 123 - +123 ! " : - 123= - # ( . & - 123 , - . ) ! " #$%& ( / ! " #$%& ( # ) * ! + , - . ( ) - / ! " #$%& ( ) 123 ! " #$%& ( # ) * ! +; - &123$ ( $ - / ? - 123 ! " $ - 123 ! " #$%& ( ) 123 online inference of topics with latent dirichlet allocation
after the batch initialization set is chosen , the o - lda algorithm has no remaining parameters and is the fastest of the three online algorithms , since it does not rejuvenate its topic assignments .
the incremental gibbs sampler has one parameter : the choice of re - juvenation sequence r ( i ) .
the particle lter has two parameters : the eective sample size ( ess ) threshold , which controls how often the particles are resampled , and the choice of rejuvenation sequence .
since the run - time and performance of the incremental gibbs sam - pler and the particle lter depend on these parameters , there is a compromise to be made .
we set the param - eters so that these algorithms ran within roughly 123 times the amount of time taken by o - lda on each dataset .
for the incremental gibbs sampler , r ( i ) was chosen to be a set of 123 indices from 123 to i chosen uni - formly at random .
for the particle lter , the ess threshold was set at 123 for the diff - 123 , rel - 123 , and sim - 123 datasets and at 123 for the subset - 123 , slash - 123 , and slash - 123 datasets , |r ( i ) | was set at 123 for the diff - 123 , rel - 123 , and sim - 123 datasets and at 123 for the subset - 123 , slash - 123 , and slash - 123 datasets , and the particular values of r ( i ) were chosen uniformly at ran - dom from 123 to i .
the runtime of these algorithms can be chosen to t any constraints , but we selected one point that we felt was reasonable .
indeed , an impor - tant strength of these two online algorithms is that they can take full advantage of any amount of comput - ing power by appropriate choice of their parameters .
the lda hyperparameters and were both set to be 123 .
the particle lter was run with 123 particles; to allow the other algorithms the same advantage of multiple samples , they were each run 123 times inde - pendently , with the sample having highest posterior probability at each step being used for evaluation .
since the datasets are collections of documents with known category memberships , we evaluated how well the clustering implied by the inferred topics matched the true categories .
that is , for each dataset , the num - ber of topics t was set equal to the number of cate - gories , and the documents were clustered according to their most frequent topic .
normalized mutual infor - mation ( nmi ) was used to measure the similarity of this implied partition to the true document categories ( banerjee and basu , 123 ) .
scores are between 123 and 123 , with a perfect match receiving a score of 123
two dierent evaluations were made for each algo - rithm on each dataset .
first , we evaluated how well the algorithms clustered the documents on which they were trained .
that is , at regular intervals through - out each dataset , the sample with maximum posterior probability was drawn , and the quality of the induced clustering of the documents observed so far was mea - sured .
second , we evaluated how well the algorithms
clustered a randomly - chosen held - out set consisting of 123% of the documents , as a function of the amount of the training set that had been observed so far .
that is , at regular intervals throughout the training set , each algorithm was run on the held - out documents as if they were the next ones to be observed , the nmi score was calculated for the held - out documents , and the algo - rithm was returned to its original state and position in the training set .
the results of the training set evaluation are shown in figure 123
the particle lter and incremental gibbs sampler perform about equally well , with the parti - cle lter performing better for some datasets .
as ex - pected , o - lda consistently has the lowest score of the three algorithms .
the dashed horizontal line in each gure represents the performance of the batch gibbs sampler on the entire dataset , which is approx - imately the best possible performance an online algo - rithm could achieve using the lda model .
the results of the evaluation on the held - out set are shown in figure 123
for each held - out set , the mean performance of the particle lter is consistently better than that of the incremental gibbs sampler , which is consistently better than that of o - lda .
with the ex - ception of the sim - 123 and subset - 123 datasets , the al - gorithms performances are separated by at least two standard deviations .
interestingly , the performance of all the algorithms on all the held - out document sets does not change signicantly as more training data is observed .
this seems to indicate that a majority of the information about the topics comes from the rst 123% of the documents .
in rel - 123 , performance on the held - out set seems to decrease as more of the training set is observed .
this could be because the held - out documents are more closely related to those at the be - ginning of the training set than those at the end .
as mentioned earlier , the algorithms performance strongly depends on their parameters; allowing more time for rejuvenation of old topic assignments would improve the performance of the particle lter and the incremental gibbs sampler .
table 123 summarizes the total runtimes of each algorithm on each dataset .
al - though there is some variation , the incremental gibbs sampler and particle lter each take about 123 times longer than o - lda .
the top ten words from 123 of the 123 topics found by the particle ltering algorithm on the subset - 123 dataset are listed in table 123
although the normalized mutual information is not as high as that of the batch gibbs sampler , the recovered topics seem intelligible .
we also noticed that the particle lter used signi -
123 canini , shi , griths
figure 123 : nmi traces for each algorithm on each dataset .
the algorithms were initialized with the same cong - uration on the rst 123% of the documents .
each dashed horizontal line represents the nmi score for the batch gibbs sampler on an entire dataset .
solid lines show mean performance over 123 runs , and shading indicates plus and minus one sample standard deviation .
figure 123 : nmi traces for each algorithm on held - out test sets , as a function of the amount of the training set observed .
the algorithms were initialized with the same conguration on the rst 123% of the documents .
each dashed horizontal line represents the nmi score on the held - out set for the batch gibbs sampler given the entire training set .
solid lines show mean performance over 123 runs , and shading indicates plus and minus one sample
123 online inference of topics with latent dirichlet allocation
table 123 : runtimes of algorithms in seconds .
numbers in parentheses give multiples of o - lda runtime .
o - lda inc
this work was supported by the darpa calo project and nsf grant bcs - 123
the authors thank jason wolfe for helpful discussions and sugato basu for providing the datasets and nmi code used for evaluation .
