semantic networks produced from human data have statistical properties that cannot be easily captured by spatial representations .
we explore a probabilis - tic approach to semantic representation that explic - itly models the probability with which words occur in di ( cid : 123 ) erent contexts , and hence captures the proba - bilistic relationships between words .
we show that this representation has statistical properties consis - tent with the large - scale structure of semantic net - works constructed by humans , and trace the origins of these properties .
contemporary accounts of semantic representa - tion suggest that we should consider words to be either points in a high - dimensional space ( eg .
lan - dauer & dumais , 123 ) , or interconnected nodes in a semantic network ( eg .
collins & loftus , 123 ) .
both of these ways of representing semantic information provide important insights , but also have shortcom - ings .
spatial approaches illustrate the importance of dimensionality reduction and employ simple al - gorithms , but are limited by euclidean geometry .
semantic networks are less constrained , but their graphical structure lacks a clear interpretation .
in this paper , we view the function of associa - tive semantic memory to be e ( cid : 123 ) cient prediction of the concepts likely to occur in a given context .
we take a probabilistic approach to this problem , mod - eling documents as expressing information related to a small number of topics ( cf .
blei , ng , & jordan , 123 ) .
the topics of a language can then be learned from the words that occur in di ( cid : 123 ) erent documents .
we illustrate that the large - scale structure of this representation has statistical properties that corre - spond well with those of semantic networks produced by humans , and trace this to the ( cid : 123 ) delity with which it reproduces the natural statistics of language .
approaches to semantic representation spatial approaches latent semantic analysis ( lsa; landauer & dumais , 123 ) is a procedure for ( cid : 123 ) nding a high - dimensional spatial representation for words .
lsa uses singular value decomposition to factorize a word - document co - occurrence matrix .
an approximation to the original matrix can be ob - tained by choosing to use less singular values than
its rank .
one component of this approximation is a matrix that gives each word a location in a high di - mensional space .
distances in this space are predic - tive in many tasks that require the use of semantic information .
performance is best for approximations that used less singular values than the rank of the matrix , illustrating that reducing the dimensional - ity of the representation can reduce the e ( cid : 123 ) ects of statistical noise and increase e ( cid : 123 ) ciency .
while the methods behind lsa were novel in scale and subject , the suggestion that similarity relates to distance in psychological space has a long history ( shepard , 123 ) .
critics have argued that human similarity judgments do not satisfy the properties of euclidean distances , such as symmetry or the tri - angle inequality .
tversky and hutchinson ( 123 ) pointed out that euclidean geometry places strong constraints on the number of points to which a par - ticular point can be the nearest neighbor , and that many sets of stimuli violate these constraints .
the number of nearest neighbors in similarity judgments has an analogue in semantic representation .
nelson , mcevoy and schreiber ( 123 ) had people perform a word association task in which they named an as - sociated word in response to a set of target words .
steyvers and tenenbaum ( submitted ) noted that the number of unique words produced for each target fol - lows a power law distribution : if k is the number of words , p ( k ) / k ( cid : 123 ) .
for reasons similar to those of tversky and hutchinson , it is di ( cid : 123 ) cult to produce a power law distribution by thresholding cosine or dis - tance in euclidean space .
this is shown in figure 123
power law distributions appear linear in log - log co - ordinates .
lsa produces curved log - log plots , more consistent with an exponential distribution .
semantic networks semantic networks were pro - posed by collins and quillian ( 123 ) as a means of storing semantic knowledge .
the original net - works were inheritance hierarchies , but collins and loftus ( 123 ) generalized the notion to cover arbi - trary graphical structures .
the interpretation of this graphical structure is vague , being based on connect - ing nodes that \activate " one another .
steyvers and tenenbaum ( submitted ) constructed a semantic net - work from the word association norms of nelson et
in : proceedings of the twenty - fourth annual conference of cognitive science society 123
word association data
latent semantic analysis
d = 123 d = 123 d = 123
figure 123 : the left panel shows the distribution of the number of associates named for each target in a word association task .
the right shows the distribution of the number of words above a cosine threshold for each target in lsa spaces of dimension d , where the threshold was chosen to match the empirical mean .
( 123 ) , connecting words that were produced as responses to one another .
in such a semantic net - work , the number of associates of a word becomes the number of edges of a node , termed its \degree " .
steyvers and tenenbaum found that the resulting graph had the statistical properties of \small world " graphs , of which a power law degree distribution is a feature ( barabasi & albert , 123 ) .
the fact that semantic networks can display these properties re ( cid : 123 ) ects their ( cid : 123 ) exibility , but there is no in - dication that the same properties would emerge if such a representation were learned rather than con - structed by hand .
in the remainder of the paper , we present a probabilistic method for learning a rep - resentation from word - document co - occurences that reproduces some of the large - scale statistical prop - erties of semantic networks constructed by humans .
a probabilistic approach
andersons ( 123 ) rational analysis of memory and categorization takes prediction as the goal of the learner .
analogously , we can view the function of associative semantic memory to be the prediction of which words are likely to arise in a given con - text , ensuring that relevant semantic information is available when needed .
simply tracking how often words occur in di ( cid : 123 ) erent contexts is insu ( cid : 123 ) cient for this task , as it gives no grounds for generalization .
if we assume that the words that occur in di ( cid : 123 ) erent contexts are drawn from t topics , and each topic can be characterized by a probability distribution over words , then we can model the distribution over words in any one context as a mixture of those top -
p ( wi ) =
p ( wijzi = j ) p ( zi = j )
where zi is a latent variable indicating the topic from which the ith word was drawn and p ( wijzi = j ) is the probability of the ith word under the jth topic .
the words likely to be used in a new context can be determined by estimating the distribution over topics for that context , corresponding to p ( zi ) .
intuitively , p ( wjz = j ) indicates which words are important to a topic , while p ( z ) is the prevalence of those topics within a document .
for example , imagine a world where the only topics of conversa - tion are love and research .
in such a world we could capture the probability distribution over words with two topics , one relating to love and the other to re - search .
the di ( cid : 123 ) erence between the topics would be re ( cid : 123 ) ected in p ( wjz = j ) : the love topic would give high probability to words like joy , pleasure , or heart , while the research topic would give high probability to words like science , mathematics , or experiment .
whether a particular conversation concerns love , re - search , or the love of research would depend upon the distribution over topics , p ( z ) , for that particu -
formally , our data consist of words w = fw123; : : : ; wng , where each wi belongs to some doc - ument di , as in a word - document co - occurrence ma - trix .
for each document we have a multinomial dis - tribution over the t topics , with parameters ( cid : 123 ) ( di ) , so for a word in document di , p ( zi = j ) = ( cid : 123 ) ( di ) the jth topic is represented by a multinomial dis - tribution over the w words in the vocabulary , with parameters ( cid : 123 ) ( j ) , so p ( wijzi = j ) = ( cid : 123 ) ( j ) wi .
to make predictions about new documents , we need to as - sume a prior distribution on the parameters ( cid : 123 ) ( di ) .
the dirichlet distribution is conjugate to the multi - nomial , so we take a dirichlet prior on ( cid : 123 ) ( di ) .
this probability model is a generative model :
gives a procedure by which documents can be gen - erated .
first we pick a distribution over topics from the prior on ( cid : 123 ) , which determines p ( zi ) for words in that document .
each time we want to add a word to the document , we pick a topic according to this distribution , and then pick a word from that topic according to p ( wijzi = j ) , which is determined by ( cid : 123 ) ( j ) .
this generative model was introduced by blei et al .
( 123 ) , improving upon hofmanns ( 123 ) probabilistic latent semantic indexing ( plsi ) .
us - ing few topics to represent the probability distribu - tions over words in many documents is a form of dimensionality reduction , and has an elegant geo - metric interpretation ( see hofmann , 123 ) .
this approach models the frequencies in a word - document co - occurrence matrix as arising from a simple statistical process , and explores the parame - ters of this process .
the result is not an explicit rep - resentation of words , but a representation that cap - tures the probabilistic relationships among words .
this representation is exactly what is required for predicting when words are likely to be used .
be - cause we treat the entries in a word - document co - occurrence matrix as frequencies , the representation developed from this information is sensitive to the natural statistics of language .
using a generative model , in which we articulate the assumptions about how the data were generated , ensures that we are
in : proceedings of the twenty - fourth annual conference of cognitive science society able to form predictions about which words might be seen in a new document .
blei et al .
( 123 ) gave an algorithm for ( cid : 123 ) nding estimates of ( cid : 123 ) ( j ) and the hyperparameters of the prior on ( cid : 123 ) ( di ) that correspond to local maxima of the likelihood , terming this procedure latent dirich - let allocation ( lda ) .
here , we use a symmetric dirichlet ( ( cid : 123 ) ) prior on ( cid : 123 ) ( di ) for all documents , a sym - metric dirichlet ( ( cid : 123 ) ) prior on ( cid : 123 ) ( j ) for all topics , and markov chain monte carlo for inference .
an advan - tage of this approach is that we do not need to ex - plicitly represent the model parameters : we can in - tegrate out ( cid : 123 ) and ( cid : 123 ) , de ( cid : 123 ) ning model simply in terms of the assignments of words to topics indicated by the zi
markov chain monte carlo is a procedure for ob - taining samples from complicated probability distri - butions , allowing a markov chain to converge to the target distribution and then drawing samples from the markov chain ( see gilks , richardson & spiegel - halter , 123 ) .
each state of the chain is an assign - ment of values to the variables being sampled , and transitions between states follow a simple rule .
we use gibbs sampling , where the next state is reached by sequentially sampling all variables from their dis - tribution when conditioned on the current values of all other variables and the data .
we will sample only the assignments of words to topics , zi .
the condi - tional posterior distribution for zi is given by
p ( zi = jjz ( cid : 123 ) i; w ) /
( cid : 123 ) i;j + ( cid : 123 ) ( cid : 123 ) i;j + w ( cid : 123 )
( cid : 123 ) i;j + ( cid : 123 ) ( cid : 123 ) i; ( cid : 123 ) + t ( cid : 123 )
where z ( cid : 123 ) i is the assignment of all zk such that k 123= i , ( cid : 123 ) i;j is the number of words assigned to topic j that are the same as wi , n ( ( cid : 123 ) ) ( cid : 123 ) i;j is the total number of words assigned to topic j , n ( di ) ( cid : 123 ) i;j is the number of words from document di assigned to topic j , and ( cid : 123 ) i; ( cid : 123 ) is the total number of words in document di , all not counting the assignment of the current word wi .
( cid : 123 ) ; ( cid : 123 ) are free parameters that determine how heavily these empirical distributions are smoothed .
the monte carlo algorithm is then straightfor - ward .
the zi are initialized to values between 123 and t , determining the initial state of the markov chain .
the chain is then run for a number of iterations , each time ( cid : 123 ) nding a new state by sampling each zi from the distribution speci ( cid : 123 ) ed by equation 123
af - ter enough iterations for the chain to approach the target distribution , the current values of the zi are recorded .
subsequent samples are taken after an ap - propriate lag , to ensure that their autocorrelation is low .
gibbs sampling is used in each of the following simulations in order to explore the consequences of this probabilistic approach .
123a detailed derivation of the conditional probabilities
used here is given in a technical report available at
learning topics with gibbs sampling the aim of this simulation was to establish the sta - tistical properties of the sampling procedure and to qualitatively assess its results , as well as to demon - strate that complexities of language like polysemy and behavioral asymmetries are naturally captured by our approach .
we took a subset of the tasa corpus ( landauer , foltz , & laham , 123 ) , using the 123 words that occurred both in the word associa - tion norm data and at least 123 times in the complete corpus , together with a random set of 123 docu - ments .
the total number of words occurring in this subset of the corpus , and hence the number of zi to be sampled , was n = 123
we set the parame - ters of the model so that 123 topics would be found ( t = 123 ) , with ( cid : 123 ) = 123 : 123 , ( cid : 123 ) = 123 : 123
the initial state of the markov chain was estab - lished with an online learning procedure .
initially , none of the wi were assigned to topics .
the zi were then sequentially drawn according to equation 123 where each of the frequencies involved , as well as w , re ( cid : 123 ) ected only the words that had already been as - signed to topics . 123 this initialization procedure was used because it was hoped that it would start the chain at a point close to the true posterior distribu - tion , speeding convergence .
ten runs of the markov chain were conducted , each lasting for 123 iterations .
on each iteration we computed the average number of topics to which a word was assigned , hki , which was used to evaluate the sampling procedure for large scale properties of the representation .
speci ( cid : 123 ) cally , we were concerned about convergence and the autocorrelation between samples .
the rate of convergence was assessed using the gelman - rubin statistic ^r , which remained be - low 123 : 123 after 123 iterations .
the autocorrelation was less than 123 : 123 after a lag of 123 iterations .
a single sample was drawn from the ( cid : 123 ) rst run of the markov chain after 123 iterations .
a subset of the 123 topics found by the model are displayed in table 123 , with words in each column corresponding to one topic , and ordered by the frequency with which they were assigned to that topic .
the topics displayed are not necessarily the most interpretable found by the model , having been selected only to highlight the way in which polysemy is naturally dealt with by this representation .
more than 123 of the 123 topics appeared to have coherent interpretations . 123
the word association data of nelson et al .
( 123 ) contain a number of asymmetries ( cases where peo - ple were more likely to produce one word in response to the other .
such asymmetries are hard to ac -
123random numbers used in all simulations were gener - ated with the mersenne twister , which has an extremely deep period ( matsumoto & nishimura , 123 ) .
123the 123 most frequent words in these topics are listed
in : proceedings of the twenty - fourth annual conference of cognitive science society sun
table 123 : nine topics from the single sample in simulation 123
each column shows 123 words from one topic , ordered by the number of times that word was assigned to the topic .
adjacent columns share at least one word .
shared words are shown in boldface , providing some clear examples of polysemy
count for in spatial representations because distance is symmetric .
the generative structure of our model allows us to calculate p ( w123jw123 ) , the probability that the next word seen in a novel context will be w123 , given that the ( cid : 123 ) rst word was w123
since this is a conditional probability , it is inherently asymmetric .
the asymmetries in p ( w123jw123 ) predict 123% of the asymmetries in the word association norms of nel - son et al .
( 123 ) , restricted to the 123 words used in the simulation .
these results are driven by word frequency : p ( w123 ) should be close to p ( w123jw123 ) , and 123% of the asymmetries could be predicted by the frequency of words in this subset of the tasa cor - pus .
the slight improvement in performance came from cases where word frequencies were very similar or polysemy made overall frequency a poor indicator of the frequency of a particular sense of a word .
bipartite semantic networks
the standard conception of a semantic network is a graph with edges between word nodes .
such a graph is unipartite : there is only one type of node , and those nodes can be interconnected freely .
contrast , bipartite graphs consist of nodes of two types , and only nodes of di ( cid : 123 ) erent types can be con - nected .
we can form a bipartite semantic network by introducing a second class of nodes that medi - ate the connections between words .
one example of such a network is a thesaurus : words are organized topically , and a bipartite graph can be formed by connecting words to the topics in which they occur , as illustrated in the left panel of figure 123
steyvers and tenenbaum ( submitted ) discovered that bipartite semantic networks constructed by hu - mans , such as that corresponding to rogets ( 123 ) thesaurus , share the statistical properties of unipar - tite semantic networks .
in particular , the number of topics in which a word occurs , or the degree of that word in the graph , follows a power law distribution as shown in the right panel of figure 123
this result is reminiscent of zipfs ( 123 ) \law of meaning " : the
number of meanings of a word follows a power law distribution .
zipfs law was established by analyz - ing dictionary entries , but appears to describe the same property of language .
g = 123 <k> = 123
figure 123 : the left panel shows a bipartite semantic network .
the right shows the degree distribution a network constructed from rogets thesaurus .
our probabilistic approach speci ( cid : 123 ) es a probability distribution over the allocation of words to topics .
if we form a bipartite graph by connecting words to the topics in which they occur , we obtain a probability distribution over such graphs .
the existence of an edge between a word and a topic indicates that the word has some signi ( cid : 123 ) cant probability of occurring in that topic .
in the following simulations , we explore whether the distribution over bipartite graphs re - sulting from our approach is consistent with the sta - tistical properties of rogets thesaurus and zipfs law of meaning .
in particular , we examine whether we obtain structures that have a power law degree
power law degree distributions
we used gibbs sampling to obtain samples from the posterior distribution of the zi for two word - document co - occurrence matrices : the matrix with the 123 words from the word association norms used in simulation 123 , and a second matrix using
in : proceedings of the twenty - fourth annual conference of cognitive science society random words , 123 topics
random words , 123 topics
random words , 123 topics
g = 123 <k> = 123
g = 123 <k> = 123
g = 123 <k> = 123
g = 123 <k> = 123
g = 123 <k> = 123
norm words , 123 topics
norm words , 123 topics
norm words , 123 topics
g = 123 <k> = 123
g = 123 <k> = 123
g = 123 <k> = 123
g = 123 <k> = 123
g = 123 <k> = 123
figure 123 : degree distributions for networks constructed in simulations 123 and 123
all are on the same axes .
123 words drawn at random from those occurring at least 123 times in the tasa corpus ( n = 123 ) .
both matrices used the same 123 random docu - ments .
for each matrix , 123 samples were taken with t = 123; 123; 123; 123 and 123
since the re - sults seemed una ( cid : 123 ) ected by the number of topics , we will focus on t = 123; 123; 123
ten samples were obtained in each of 123 separate runs with a burn - in of 123 iterations in which no samples were drawn , and a between - sample lag of 123 iterations .
for each sample , a bipartite semantic network was constructed by connecting words to the topics to which they were assigned .
for each network , the degree of each word node was averaged over the 123 samples . 123 the resulting distributions were clearly power - law , as shown in figure 123
the ( cid : 123 ) coe ( cid : 123 ) cients remained within a small range and were all close to ( cid : 123 ) = ( cid : 123 ) 123 : 123 for rogets thesaurus .
as is to be expected , the average degree increased as more top - ics were made available , and was generally higher than rogets .
semantic networks in which edges are added for each assignment tend to be quite densely connected .
sparser networks can be produced by setting a more conservative threshold for the inclu - sion of an edge , such as multiple assignments of a word to a topic , or exceeding some baseline proba - bility in the distribution represented by that topic .
our probabilistic approach produces power law degree distributions , in this case indicating that the number of topics to which a word is assigned follows a power law .
this result is very similar to the prop - erties of rogets thesaurus and zipfs observations about dictionary de ( cid : 123 ) nitions .
this provides an op -
123since power law distributions can be produced by av - eraging exponentials , we also inspected individual sam - ples to con ( cid : 123 ) rm that they had the same characteristics .
portunity to establish the origin of this distribution , to see whether it is a consequence of the modeling approach or a basic property of language .
origins of the power law
to investigate the origins of the power law , we ( cid : 123 ) rst established that our initialization procedure was not responsible for our results .
using t = 123 and the matrix with random words , we obtained 123 samples of the degree distribution immediately following ini - tialization .
as can be seen in figure 123 , this produced a curved log - log plot and higher values of ( cid : 123 ) and hki than in simulation 123
the remaining analyses employed variants of this co - occurrence matrix , and their results are also pre - sented in figure 123
the ( cid : 123 ) rst variant kept word fre - quency constant , but assigned instances of words to documents at random , disrupting the co - occurrence structure .
interestingly , this appeared to have only a weak e ( cid : 123 ) ect on the results , although the curva - ture of the resulting plot did increase .
the second variant forced the frequencies of all words to be as close as possible to the median frequency .
this was done by dividing all entries in the matrix by the frequency of that word , multiplying by the median frequency , and rounding to the nearest integer .
the total number of instances in the resulting matrix was n = 123
this manipulation reduced the aver - age density in the resulting graph considerably , but the distribution still appeared to follow a power law .
the third variant held the number of documents in which a word participated constant .
word frequen - cies were only weakly a ( cid : 123 ) ected by this manipulation , which spread the instances of each word uniformly over the top ( cid : 123 ) ve documents in which it occurred
in : proceedings of the twenty - fourth annual conference of cognitive science society and then rounded up to the nearest integer , giving n = 123
five was the median number of docu - ments in which words occurred , and documents were chosen at random for words below the median .
this manipulation had a strong e ( cid : 123 ) ect on the degree dis - tribution , which was no longer power law , or even
the distribution of the number of topics in which a word participates was strongly a ( cid : 123 ) ected by the dis - tribution of the number of documents in which a word occurs .
examination of the latter distribution in the tasa corpus revealed that it follows a power law .
our approach produces a power law degree dis - tribution because it accurately captures the natural statistics of these data , even as it constructs a lower -
we have taken a probabilistic approach to the prob - lem of semantic representation , motivated by con - sidering the function of associative semantic mem - ory .
we assume a generative model where the words that occur in each context are chosen from a small number of topics .
this approach produces a lower - dimensional representation of a word - document co - occurrence matrix , and explicitly models the fre - quencies in that matrix as probability distributions .
simulation 123 showed that our approach could ex - tract coherent topics , and naturally deal with issues like polysemy and asymmetries that are hard to ac - count for in spatial representations .
in simulation 123 , we showed that this probabilistic approach was also capable of producing representations with a large - scale structure consistent with semantic networks constructed from human data .
in particular , the number of topics to which a word was assigned fol - lowed a power law distribution , as in rogets ( 123 ) thesaurus and zipfs ( 123 ) law of meaning .
in sim - ulation 123 , we discovered that the only manipulation that would remove the power law was altering the number of documents in which words participate , which follows a power law distribution itself .
steyvers and tenenbaum ( submitted ) suggested that power law distributions in language might be traced to some kind of growth process .
our results indicate that this growth process need not be a part of the learning algorithm , if the algorithm is faith - ful to the statistics of the data .
while we were able to establish the origins of the power law distribu - tion in our model , the growth processes described by steyvers and tenenbaum might contribute to under - standing the origins of the power law distribution in dictionary meanings , thesaurus topics , and the num - ber of documents in which words participate .
the representation learned by our probabilistic approach is not explicitly a representation of words , in which each word might be described by some set of features .
instead , it is a representation of the prob - abilistic relationships between words , as expressed
by their probabilities of arising in di ( cid : 123 ) erent contexts .
we can easily compute important statistical quan - tities from this representation , such as p ( w123jw123 ) , the probability of w123 arising in a particular context given that w123 was observed , and more complicated conditional probabilities .
one advantage of an ex - plicitly probabilistic representation is that we gain the opportunity to incorporate this representation into other probabilistic models .
in particular , we see great potential for using this kind of represen - tation in understanding the broader phenomena of
acknowledgments the authors were supported by a hackett studentship and a grant from ntt communi - cations sciences laboratory .
we thank tania lombrozo , penny smith and josh tenenbaum for comments , and tom landauer and darrell laham for the tasa corpus .
shawn cokus wrote the mersenne twister code .
