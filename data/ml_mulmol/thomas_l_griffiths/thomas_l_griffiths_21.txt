abstract : shepard has argued that a universal law should govern generalization across different domains of perception and cognition , as well as across organisms from different species or even different planets .
starting with some basic assumptions about natural kinds , he derived an exponential decay function as the form of the universal generalization gradient , which accords strikingly well with a wide range of empirical data .
however , his original formulation applied only to the ideal case of generalization from a single encountered stim - ulus to a single novel stimulus , and for stimuli that can be represented as points in a continuous metric psychological space .
here we re - cast shepards theory in a more general bayesian framework and show how this naturally extends his approach to the more realistic sit - uation of generalizing from multiple consequential stimuli with arbitrary representational structure .
our framework also subsumes a version of tverskys set - theoretic model of similarity , which is conventionally thought of as the primary alternative to shepards continuous metric space model of similarity and generalization .
this unification allows us not only to draw deep parallels between the set - theoretic and spatial approaches , but also to significantly advance the explanatory power of set - theoretic models .
keywords : additive clustering; bayesian inference; categorization; concept learning; contrast model; features; generalization; psycho - logical space; similarity
consider the hypothetical case of a doctor trying to deter - mine how a particular hormone , naturally produced by the human body , affects the health of patients .
it seems likely that patients with too little of the hormone in their blood suffer negative effects , but so do patients with too much of the hormone .
assume that the possible concentration lev - els of this hormone can be represented as real numbers be - tween 123 and 123 on some arbitrary measuring scale , and that one healthy patient has been examined and found to have a hormone level of 123
what other hormone levels should the doctor consider healthy ?
now imagine a baby robin whose mother has just given it its first worm to eat .
the worms in this robins environ - ment vary in level of skin pigmentation , and only worms with some intermediate density of pigmentation are good to eat; too dark or too light worms are unhealthy .
finally , suppose for simplicity that robins are capable of detecting shades of worm coloration between 123 and 123 on some ar - bitrary scale , and that the first worm our baby robin has been given scores a skin pigmentation level of 123
assum - ing the mother has chosen a worm that is good to eat , what other pigmentation levels should our baby robin consider good to eat ?
these two scenarios are both cases of shepards ( 123b; 123 ) ideal generalization problem : given an encounter with a single stimulus ( a patient , a worm ) that can be rep - resented as a point in some psychological space ( a hormone level or pigmentation level of 123 ) , and that has been found to have some particular consequence ( healthy , good to eat ) , what other stimuli in that space should be expected to have
the same consequence ? shepard observes that across a wide variety of experimental situations , including both hu - man and animal subjects , generalization gradients tend to fall off approximately exponentially with distance in an appropriately scaled psychological space ( as obtained by multidimensional scaling , or mds ) .
he then gives a ratio - nal probabilistic argument for the origin of this universal law , starting with some basic assumptions about the geom - etry of natural kinds in psychological spaces , which could be expected to apply equally well to doctors or robins , or even aliens from another galaxy .
the argument makes no distinction in principle between conscious , deliberate , cognitive inferences , such as the healthy hormone levels scenario , and unconscious , automatic , or perceptual in - ferences , such as the good - to - eat worms scenario , as long as they satisfy the conditions of the ideal generalization
in the opening sentences of his first paper on the uni -
joshua b .
tenenbaum is assistant professor of psy - chology at stanford university .
in 123 , he received a ph . d .
in brain and cognitive sciences from mit .
his research focuses on learning and inference in humans and machines , with specific interests in concept learn - ing and generalization , similarity , reasoning , causal in - duction , and learning perceptual representations .
thomas l .
griffiths is a doctoral student in the de - partment of psychology at stanford university .
his re - search interests concern the application of mathemati - cal and statistical models to human cognition .
123 cambridge university press
tenenbaum & griffiths : generalization , similarity , and bayesian inference
versal law of generalization , shepard ( 123b ) invokes new - tons universal law of gravitation as the standard to which he aspires in theoretical scope and significance .
the analogy holds more strongly than might have been anticipated .
newtons law of gravitation was expressed in terms of the attraction between two point masses : every object in the universe attracts every other object with a force directed along the line connecting their centers of mass , propor - tional to the product of their masses and inversely propor - tional to the square of their separation .
however , most of the interesting gravitational problems encountered in the universe do not involve two point masses .
in order to model real - world gravitational phenomena , physicists following newton have developed a rich theory of classical mechan - ics that extends his law of gravitation to address the inter - actions of multiple , arbitrarily extended bodies .
likewise , shepard formulated his universal law with re - spect to generalization from a single encountered stimulus to a single novel stimulus , and he assumed that stimuli could be represented as points in a continuous metric psychologi - cal space .
however , many of the interesting problems of generalization in psychological science do not fit this mold .
they involve inferences from multiple examples , or stimuli that are not easily represented in strictly spatial terms .
for example , what if our doctor observes the hormone levels of not one but three healthy patients : 123 , 123 , and 123
how should that change the generalization gradient ? or what if the same numbers had been observed in a different context , as examples of a certain mathematical concept presented by a teacher to a student ? certain features of the numbers that were not salient in the hormone context , such as be - ing even or being multiples of ten , now become very impor - tant in a mathematical context .
consequently , a simple one - dimensional metric space representation may no longer be appropriate : 123 may be more likely than 123 to be an instance of the mathematical concept exemplified by 123 , 123 , and 123 , while given the same examples in the hormone context , 123 may be more likely than 123 to be a healthy level .
just as physi - cists now see newtons original two - point - mass formulation as a special case of the more general classical theory of grav - itation , so would we like a more general theory of general - ization , which reduces to shepards original two - points - in - psychological - space formulation in the appropriate special cases , but which extends his approach to handle generaliza - tion from multiple , arbitrarily structured examples .
in this article we outline the foundations of such a the - ory , working with the tools of bayesian inference and in the spirit of rational analysis ( anderson 123; chater & oaks - ford 123; 123; marr 123 ) .
much of our proposal for ex - tending shepards theory to the cases of multiple examples and arbitrary stimulus structures has already been intro - duced in other papers ( griffiths & tenenbaum 123; tenenbaum 123; 123a; 123b; tenenbaum & xu 123 ) .
our goal here is to make explicit the link to shepards work and to use our framework to make connections between his work and other models of learning ( feldman 123; gluck & shanks 123; haussler et al .
123; kruschke 123; mitchell 123 ) , generalization ( heit 123; nosofsky 123 ) , and similarity ( chater & hahn 123; medin et al .
123; tversky 123 ) .
in particular , we will have a lot to say about how our generalization of shepards theory relates to tver - skys ( 123 ) well - known set - theoretic models of similarity .
tverskys set - theoretic approach and shepards metric space approach are often considered the two classic and
behavioral and brain sciences ( 123 ) 123 : 123
classically opposed theories of similarity and generaliza - tion .
by demonstrating close parallels between tverskys approach and our bayesian generalization of shepards ap - proach , we hope to go some way towards unifying these two theoretical approaches and advancing the explanatory power of each .
the plan of our article is as follows .
in section 123 , we re - cast shepards analysis of generalization in a more general bayesian framework , preserving the basic principles of his approach in a form that allows us to apply the theory to sit - uations with multiple examples and arbitrary ( nonspatially represented ) stimulus structures .
sections 123 and 123 describe those extensions , and section 123 concludes by discussing some implications of our theory for the internalization of
a bayesian framework for generalization
shepard ( 123b ) formulates the problem of generalization as follows .
we are given one example , x , of some conse - quence c , such as a healthy person or a good - to - eat worm .
we assume that x can be represented as a point in a continuous metric psychological space , such as the one - dimensional space of hormone levels between 123 and 123 , and that c corresponds to some region the consequential region of that space .
our task is then to infer the proba - bility that some newly encountered object y will also be an instance of c , that is , that y will fall in the consequential re - gion for c .
formalizing this induction problem in probabil - istic terms , we are asking for p ( y ( cu x ) , the conditional probability that y falls under c given the observation of the
the theory of generalization that shepard develops and that we will extend here can best be understood by consid - ering how it addresses three crucial questions of learning ( after chomsky 123 ) :
what constitutes the learners knowledge about the
how to generalize ?
how does the learner use that knowledge to decide
how can the learner acquire that knowledge from the
our commitment to work within the paradigm of bayesian probabilistic inference leads directly to rational answers for each of these questions .
the rest of this section presents these answers and illustrates them concretely us - ing the hormone or pigmentation levels tasks introduced above .
our main advance over shepards original analysis comes in introducing the size principle ( tenenbaum 123; 123a; 123b ) for scoring hypotheses about the true conse - quential region based on their size , or specificity .
although it makes little difference for the simplest case of general - ization studied by shepard , the size principle will provide the major explanatory force when we turn to the more re - alistic cases of generalizing from multiple examples ( sect .
123 ) with arbitrary structure ( sect
what constitutes the learners knowledge
about the consequential region ?
the learners knowledge about the consequential region is represented as a probability distribution p ( hux ) over an a priori - specified hypothesis space h of possible consequen -
tial regions h ( h .
h forms a set of exhaustive and mutu - ally exclusive possibilities; that is , one and only one element of h is assumed to be the true consequential region for c ( although the different candidate regions represented in h may overlap arbitrarily in the stimuli that they include ) .
the learners background knowledge , which may include both domain - specific and domain - general components , will of - ten translate into constraints on which subsets of objects be - long to h .
shepard ( 123 ) suggests the general constraint that consequential regions for basic natural kinds should correspond to connected subsets of psychological space .
applying the connectedness constraint to the domains of hormone levels or worm pigmentation levels , where the rel - evant stimulus spaces are one - dimensional continua , the hypothesis spaces would consist of intervals , or ranges of stimuli between some minimum and maximum conse - quential levels .
figure 123 shows a number of such intervals which are consistent with the single example of 123
for sim - plicity , we have assumed in figure 123 that only integer stim - ulus values are possible , but in many cases both the stimu - lus and hypothesis spaces will form true continua .
at all times , the learners knowledge about the conse - quential region consists of a probability distribution over h .
prior to observing x , this distribution is the prior prob - ability p ( h ) ; after observing x , it is the posterior probability p ( hux ) .
as probabilities , p ( h ) and p ( hux ) are numbers be - tween 123 and 123 reflecting the learners degree of belief that h is in fact the true consequential region corresponding to c .
in figure 123 , p ( hux ) for each h is indicated by the thick - ness ( height ) of the corresponding bar .
the probability of
tenenbaum & griffiths : generalization , similarity , and bayesian inference
any h that does not contain x will be zero , because it cannot be the true consequential region if it does not contain the one observed example .
hence , figure 123 shows only hy - potheses consistent with x 123 123
how does the learner use that knowledge
to decide how to generalize ?
the generalization function p ( y ( cux ) is computed by summing the probabilities p ( hux ) of all hypothesized con - sequential regions that contain y : 123
we refer to this computation as hypothesis averaging , be - cause it can be thought of as averaging the predictions that each hypothesis makes about ys membership in c , weighted by the posterior probability of that hypothesis .
be - cause p ( hux ) is a probability distribution , normalized to sum to 123 over all h ( h , the structure of equation 123 ensures that p ( y ( cux ) will always lie between 123 and 123
in general , the hypothesis space need not be finite or even countable .
in the case of a continuum of hypotheses , such as the space of all intervals of real numbers , all probability distributions over h become probability densities and the sums over h ( in equations 123 and following ) become integrals .
the top panel of figure 123 shows the generalization gra - dient that results from averaging the predictions of the in - teger - valued hypotheses shown below , weighted by their
figure 123
an illustration of the bayesian approach to generalization from x 123 123 in a one - dimensional psychological space ( inspired by shepard 123 , august ) .
for the sake of simplicity , only intervals with integer - valued endpoints are shown .
all hypotheses of a given size are grouped together in one bracket .
the thickness ( height ) of the bar illustrating each hypothesis h represents p ( hux ) , the learners de - gree of belief that h is the true consequential region given the observation of x .
the curve at the top of the figure illustrates the gradient of generalization obtained by integrating over just these consequential regions .
the profile of generalization is always concave regard - less of what values p ( hux ) takes on , as long as all hypotheses of the same size ( in one bracket ) take on the same probability .
behavioral and brain sciences ( 123 ) 123 : 123
tenenbaum & griffiths : generalization , similarity , and bayesian inference
probabilities .
note that the probability of generalization equals 123 only for y 123 x , when every hypothesis containing x also contains y .
as y moves further away from x , the num - ber of hypotheses containing x that also contain y decreases , and the probability of generalization correspondingly de - creases .
moreover , figure 123 shows the characteristic pro - file of shepards universal generalization function : con - cave , or negatively accelerated as y moves away from x .
if we were to replace the integer - valued interval hypotheses with the full continuum of all real - valued intervals , the sum in equation 123 would become an integral , and the piecewise linear gradient shown in figure 123 would become a smooth function with a similar concave profile , much like those de - picted in the top panels of figures 123 and 123
figure 123 demonstrates that shepards approximately ex - ponential generalization gradient emerges under one par - ticular assignment of p ( hux ) , but it is reasonable to ask how sensitive this result is to the choice of p ( hux ) .
shepard ( 123b ) showed that the shape of the gradient is remark - ably insensitive to the probabilities assumed .
as long as the probability distribution p ( hux ) is isotropic , that is , indepen - dent of the location of h , the generalization function will al - ways have a concave profile .
the condition of isotropy is equivalent to saying that p ( hux ) depends only on uhu , the size of the region h; notice how this constraint is satisfied in fig -
how can the learner acquire that knowledge
from the example encountered ?
after observing x as an example of the consequence c , the learner updates her beliefs about the consequential region from the prior p ( h ) to the posterior p ( hux ) .
here we con -
sider how a rational learner arrives at p ( hux ) from p ( h ) , through the use of bayes rule .
we will not have much to say about the origins of p ( h ) until section 123; shepard ( 123b ) and tenenbaum ( 123a; 123b ) discuss several reasonable alternatives for the present scenarios , all of which are isotropic and assume little or no knowledge about the true bayes rule couples the posterior to the prior via the like - lihood , p ( xuh ) , the probability of observing the example x given that h is the true consequential region , as follows :
) ( ) p x h p h
) ( ) p x h p h p x h p h
what likelihood function we use is determined by how we think the process that generated the example x relates to the true consequential region for c .
shepard ( 123b ) argues for a default assumption that the example x and consequential re - gion c are sampled independently , and x just happens to land inside c .
this assumption is standard in the machine learn - ing literature ( haussler et al .
123; mitchell 123 ) , and also maps onto heits ( 123 ) recent bayesian analysis of inductive reasoning .
tenenbaum ( 123; 123a ) argues that under many conditions , it is more natural to treat x as a random positive example of c , which involves the stronger assumption that x was explicitly sampled from c .
we refer to these two models as weak sampling and strong sampling , respectively .
under weak sampling , the likelihood just measures in a binary fashion whether or not the hypothesis is consistent with the observed example :
figure 123
the effect of example variability on bayesian generalization ( under the assumptions of strong sampling and an erlang prior , m 123 123 ) .
filled circles indicate examples .
the first curve is the gradient of generalization with a single example , for the purpose of com - parison .
the remaining graphs show that the range of generalization increases as a function of the range of examples .
behavioral and brain sciences ( 123 ) 123 : 123
tenenbaum & griffiths : generalization , similarity , and bayesian inference
figure 123
the effect of the number of examples on bayesian generalization ( under the assumptions of strong sampling and an erlang prior , m 123 123 ) .
filled circles indicate examples .
the first curve is the gradient of generalization with a single example , for the purpose of comparison .
the remaining graphs show that the range of generalization decreases as a function of the number of examples .
p ( x|h ) 123 123 if x ( h
multiple examples
under strong sampling , the likelihood is more informative .
assuming x is sampled from a uniform distribution over the objects in h , we have :
if x ( h
where uhu indicates the size of the region h .
for discrete stimulus spaces , uhu is simply the cardinality of the subset corresponding to h .
for continuous spaces such as the hor - mone or pigmentation levels , the likelihood becomes a probability density and uhu is the measure of the hypothesis in one dimension , just the length of the interval . 123 equa - tion 123 implies that smaller , more specific hypotheses will tend to receive higher probabilities than larger , more gen - eral hypotheses , even when both are equally consistent with the observed consequential stimulus .
we will call this ten - dency the size principle .
it is closely related to principles of genericity that have been proposed in models of visual per - ception and categorization ( feldman 123; knill & rich - ards 123 ) .
figure 123 depicts the application of the size prin -
note that both equations 123 and 123 are isotropic , and thus the choice between strong sampling and weak sampling has no effect on shepards main result that generalization gra - dients are universally concave .
however , as we now turn to look at the phenomena of generalization from multiple stimuli with arbitrary , nonspatially represented structures , we will see that the size principle implied by strong sam - pling carries a great deal of explanatory power not present in shepards original analysis .
in this section , we extend the above bayesian analysis to sit - uations with multiple consequential examples .
such situa - tions arise quite naturally in the generalization scenarios we have already discussed .
for instance , how should our doc - tor generalize after observing hormone levels of 123 , 123 , and 123 in three healthy patients ? we first discuss some basic phenomena that arise with multiple examples and then turn to the extension of the theory .
finally , we compare our ap - proach to some alternative ways in which shepards theory has been adapted to apply to multiple examples .
phenomena of generalization
from multiple examples
we focus on two classes of phenomena : the effects of ex - ample variability and the number of examples .
example variability .
all other things being equal , the lower the variability in the set of observed examples , the lower the probability of generalization outside their range .
the probability that 123 is a healthy hormone level seems greater given the three examples ( 123 , 123 , 123 ) than given the three examples ( 123 , 123 , 123 ) , and greater given ( 123 , 123 , 123 ) than given ( 123 , 123 , 123 ) .
effects of exemplar variability on generalization have been documented in several other cat - egorization and inductive inference tasks ( fried & holyoak 123; osherson et al .
123; rips 123 ) .
number of examples .
all other things being equal , the more examples observed within a given range , the lower the probability of generalization outside that range
behavioral and brain sciences ( 123 ) 123 : 123
tenenbaum & griffiths : generalization , similarity , and bayesian inference
probability that 123 is a healthy hormone level seems greater given the two examples ( 123 , 123 ) than given the four exam - ples ( 123 , 123 , 123 , 123 ) , and greater given ( 123 , 123 , 123 , 123 ) than given ( 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 ) .
this effect is most dra - matic when there is very little variability in the observed ex - amples .
consider the three sets of examples ( 123 ) , ( 123 , 123 , 123 ) , and ( 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 ) .
with just two more examples , the probability of generalizing to 123 from ( 123 , 123 , 123 ) already seems much lower than given ( 123 ) alone , and the probability given ( 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 ) seems close to zero .
extending the theory let x 123 ( x123 , .
xn ) denote a sequence of n examples of some consequence c , and let y denote a novel object for which we want to compute the probability of generalizing , p ( y ( cux ) .
all we have to do to make the theory of sec - tion 123 applicable here is to replace x , wherever it appears , with x , and to adopt the assumption of strong sampling rather than shepards original proposal of weak sampling .
the rest of the formalism is unchanged .
the only compli - cation this introduces comes in computing the likelihood p ( xuh ) .
if we make the simplifying assumption that the ex - amples are sampled independently of each other ( a stan - dard assumption in bayesian analysis ) , then equation 123 be -
p x hi
if x123 ,
hence the size principle of equation 123 has been general - ized to include the influence of n : smaller hypotheses re - ceive higher likelihoods than larger hypotheses , by a factor that increases exponentially with the number of examples observed .
figures 123 and 123 depict the bayesian gradients of generalization that result for several different numbers and ranges of examples , assuming p ( xuh ) based on strong sam - pling and an erlang distribution ( shepard 123b ) for p ( h ) .
in addition to showing the universal concave profile , these gradients display the appropriate sensitivity to the number and variability of examples .
to understand how the size principle generates these ef - fects , consider how equation 123 weights two representative hypotheses : h123 , the smallest interval containing all the ex - amples in x , and h123 , a broader interval centered on h123 but extending by d / 123 units on either side , so that uh123 d .
after observing n examples , the relative probabilities are proportional to the likelihood ratio :
u 123 uh123
l is always less than 123 , because d and uh123 u are both positive .
u increases , but the other quantities remain fixed , l increases .
thus , as we see in figure 123 , the relative proba - bility that c extends a given distance d beyond the exam - ples increases as the range spanned by the examples in - creases .
as n increases while the other quantities remain fixed , l quickly approaches 123
thus , as we see in figure 123 , the probability that c extends a distance d beyond the examples rapidly decreases as the number of examples
behavioral and brain sciences ( 123 ) 123 : 123
increases within a fixed range .
the tighter the examples , the u is , and the faster l decreases with increasing n , thus accounting for the interaction between these two fac - tors pointed to earlier .
we can also now see why shepards original assumption of weak sampling would not generate these phenomena .
under weak sampling , the likelihoods of any two consistent hypotheses are always both 123
thus l 123 123 always , and nei - ther the range nor the number of examples have any effect on how hypotheses are weighted .
in general , we expect that both strong sampling and weak sampling models will have their uses .
real - world learning situations may often require a combination of the two , if some examples are generated by mere observation of consequential stimuli ( strong sam - pling ) and others by trial - and - error exploration ( weak sam -
figure 123 illustrates an extension to generalizing in two separable dimensions , such as inferring the healthy levels of two independent hormones ( for more details , see tenen - baum 123b ) .
following shepard ( 123b ) , we assume that the consequential regions correspond to axis - aligned rec - tangles in this two - dimensional space , with independent priors in each dimension .
then , as shown in figure 123 , the size principle acts to favor generalization along those di - mensions for which the examples have high variability and to restrict generalization along dimensions for which they have low variability .
tenenbaum ( 123b ) reports data from human subjects that are consistent with these predictions for a task of estimating the healthy levels of two biochemi - cal compounds .
more studies need to be done to test these predictions in multidimensional perceptual spaces of the sort with which shepard has been most concerned .
alternative approaches a number of other computational models may be seen as alternative methods of extending shepards approach to the case of multiple examples , but only the framework we de - scribe here preserves what we take to be the two central features of shepards original analysis : a hypothesis space of possible consequential regions and a bayesian inference procedure for updating beliefs about the true consequen - tial region .
the standard exemplar models of classification ( e . g . , nosofsky 123; 123a ) take shepards exponential law of generalization as a primitive , used to justify the assump - tion that exemplar activation functions decay exponentially with distance in psychological space .
a different approach is based on connectionist networks ( gluck 123; shanks & gluck 123; shepard & kannapan 123; shepard & tenen - baum 123 ) , in which input or hidden units represent con - sequential regions , and error - driven learning rather than bayesian inference is used to adjust the weights from con - sequential region inputs to response outputs .
a third class of models ( kruschke 123; love & medin 123 ) combines aspects of the first two , by embedding shepards exponen - tial law within the activation functions of hidden units in a connectionist network for classification learning .
space does not permit a full comparison of the various al - ternative models with our proposals .
one important point of difference is that for most of these models , the general - ization gradients produced by multiple examples of a given consequence are essentially just superpositions of the ex - ponential decay gradients produced by each individual ex - ample .
consequently , those models cannot easily explain
tenenbaum & griffiths : generalization , similarity , and bayesian inference
figure 123
bayesian generalization from multiple examples in two separable dimensions .
examples are indicated by filled circles .
con - tours show posterior probability , in increments of 123 .
black contours illustrate the points at which p ( y ( cux ) 123 123 .
the range of gen - eralization is affected by both the number of examples and the variability along a given dimension .
the phenomena discussed above , in which encountering ad - ditional consequential stimuli causes the probability of gen - eralizing to some new stimulus to decrease , even when the additional examples are more similar to the new stimulus than the original example was .
exemplar and exemplar / connectionist hybrid models are frequently equipped with variable attentional weights that scale distances along a given input dimension by a greater or lesser amount , in or - der to produce variations in the contours of generalization like those in figure 123
such models could account for our phenomena by postulating that a dimensions length scale is initially large and decreases as the number of examples increases or the variability of the examples decreases , but nothing in the formal structure of these models necessarily implies such a mechanism .
our bayesian analysis , in con - trast , necessarily predicts these effects as rational conse - quences of the size principle .
arbitrary stimulus structure
shepard ( 123b ) assumed that objects can be represented as points in a continuous metric psychological space , and that the consequential subsets correspond to regions in that space with some convenient properties , such as connected - ness or central symmetry .
in general , though , we do not
need to assume that the hypothesized consequential sub - sets correspond to regions in any continuous metric space; the notion of a consequential subset is sufficient for defin - ing a bayesian account of generalization .
in this section we examine how arbitrary , nonspatially represented stimulus structures are modeled within the bayesian framework .
several authors , including shepard himself , have de - scribed extensions of the original theory of generalization to conjunctive feature structures , in which objects are rep - resented in terms of the presence or absence of primitive binary features and the possible consequential subsets con - sist of all objects sharing different conjunctions of features .
for these cases , generalization gradients can still be shown to follow an exponential - like decay function of some ap - propriately defined distance measure ( gluck 123; russell 123; shepard 123; 123 ) .
however , the bayesian analysis of generalization is more widely applicable than this .
as we will show here , the analysis applies even when there is no independent notion of distance between stimuli and noth - ing like an exponential gradient emerges from the sum over
to motivate our analysis , consider a new generalization scenario .
a computer has been programmed with a variety of simple mathematical concepts defined over the integers 123 subsets of numbers that share a common , mathe - matically consequential property such as even number ,
behavioral and brain sciences ( 123 ) 123 : 123
tenenbaum & griffiths : generalization , similarity , and bayesian inference
power of two , or square number .
the computer will se - lect one of these subsets at random , choose one or more numbers at random from that subset to show you as exam - ples , and then quiz you by asking if certain other numbers belong to this same concept .
suppose that the number 123 is offered as one example of a concept the computer has chosen .
what is the probability that the computer will ac - cept 123 ? how about 123 , 123 , or 123 ? syntactically , this task is almost identical to the hormone levels scenario above .
but now , instead of generalization following a monotonic func - tion of proximity in numerical magnitude , it seems more likely to follow some measure of mathematical similarity .
for instance , the number 123 shares more mathematical properties with 123 than with 123 , making 123 perhaps a bet - ter bet than 123 to be accepted given the one example of 123 , even though 123 is closer in magnitude to 123 and therefore a better bet for the doctor trying to determine healthy hor -
in our bayesian framework , the difference between the two scenarios stems from the very different consequential subsets ( elements of h ) that are considered .
for the doc - tor , knowing something about healthy levels of hormones in general , it is quite natural to assume that the true conse - quential subset corresponds to some unknown interval , which gives rise to a generalization function monotonically related to proximity in magnitude .
to model the number game , we can identify each mathematical property that the learner knows about with a possible consequential subset in h .
figure 123 shows a generalization function that results under a set of 123 simple hypotheses , as calculated from the size principle ( eq .
123 ) and hypothesis averaging ( eq .
the generalization function appears much more jagged than in figures 123 because the mathematical hypothesis space does not respect proximity in the dimension of numerical magnitude ( corresponding to the abscissa of the figures ) .
more generally , numerical cognition may incorporate both the spatial , magnitude properties as well as the nonspatial , mathematical properties of numbers .
to investigate the na - ture of mental representations of numbers , shepard et al .
( 123 ) collected human similarity judgments for all pairs of integers between 123 and 123 , under a range of different con - texts .
by submitting these data to an additive clustering analysis ( shepard & arabie 123; tenenbaum 123 ) , we can construct the hypothesis space of consequential subsets that best accounts for peoples similarity judgments .
table 123 shows that two kinds of subsets occur in the best - fitting
additive clustering solution ( tenenbaum 123 ) : numbers sharing a common mathematical property , such as ( 123 , 123 , 123 ) and ( 123 , 123 , 123 ) , and consecutive numbers of similar magni - tude , such as ( 123 , 123 , 123 , 123 ) and ( 123 , 123 , 123 , 123 , 123 ) .
tenenbaum ( 123 ) studied how people generalized concepts in a ver - sion of the number game that made both mathematical and magnitude properties salient .
he found that a bayesian model using a hypothesis space inspired by these additive clustering results , but defined over the integers 123 , yielded an excellent fit to peoples generalization judg - ments .
the same flexibility in hypothesis space structure that allows the bayesian framework to model both the spa - tial hormone level scenario and the nonspatial number game scenario there allows it to model generalization in a more generic context , by hypothesizing a mixture of conse - quential subsets for both spatial , magnitude properties and nonspatial , mathematical properties .
in fact , we can define a bayesian generalization function not just for spatial , feat - ural , or simple hybrids of these representations , but for al - most any collection of hypothesis subsets h whatsoever .
the only restriction is that we be able to define a prior prob - ability measure ( discrete or continuous ) over h , and a mea - sure over the space of objects , required for strong sampling to make sense .
even without a measure over the space of objects , a bayesian analysis using weak sampling will still be
relations between generalization
and set - theoretic models of similarity
classically , mathematical models of similarity and gener - alization fall between two poles : continuous metric space models such as in shepards theory , and set - theoretic match - ing models such as tverskys ( 123 ) contrast model .
the lat - ter strictly include the former as a special case , but are most commonly applied in domains where a set of discrete con - ceptual features , as opposed to a low - dimensional contin - uous space , seems to provide the most natural stimulus representation ( shepard 123 ) .
our number game is such a domain , and indeed , when we generalize shepards bayes - ian analysis from consequential regions in continuous met - ric spaces to apply to arbitrary consequential subsets , the model comes to look very much like a version of tverskys set - theoretic models .
making this connection explicit al - lows us not only to unify the two classically opposing ap - proaches to similarity and generalization , but also to explain
figure 123
bayesian generalization in the number game , given one example x 123 123
the hypothesis space includes 123 mathematically consequential subsets ( with equal prior probabilities ) : even numbers , odd numbers , primes , perfect squares , perfect cubes , multiples of a small number ( 123 ) , powers of a small number ( 123 ) , numbers ending in the same digit ( 123 ) , numbers with both digits equal , and all numbers less than 123
behavioral and brain sciences ( 123 ) 123 : 123
tenenbaum & griffiths : generalization , similarity , and bayesian inference
some significant aspects of similarity that tverskys original treatment did not attempt to explain .
tverskys ( 123 ) contrast model expresses the similarity
of y to x as
s ( y , x ) 123 uf ( y > x ) 123 af ( y 123 x ) 123 bf ( x 123 y ) ,
where x and y are the feature sets representing x and y , respectively , f denotes some measure over the feature sets , and u , a , b are free parameters of the model .
similarity thus involves a contrast between the common features of y and x , y > x , and their distinctive features , those possessed by y but not x , y 123 x , and those possessed by x but not y , x 123 y .
tversky also suggested an alternative form for the matching function , the ratio model , which can be written
the ratio model is remarkably similar to our bayesian model of generalization , which becomes particularly apparent when the bayesian model is expressed in the following form ( mathematically equivalent to eq
h x h y h
( , ) p h x
( , ) p h x
h x y h
here , p ( h , x ) 123 p ( xuh ) p ( h ) represents the weight assigned to hypothesis h in light of the example x , which depends on both the prior and the likelihood .
the bottom sum ranges over all hypotheses that include both x and y , while the top sum ranges over only those hypotheses that include x but do not include y .
if we identify each feature k in tverskys framework with a hypothesized subset h , where an object belongs to h if and only if it possesses feature k , and if we make the standard assumption that the measure f is addi - tive , then the bayesian model as expressed in equation 123 corresponds formally to the ratio model with a 123 123 , b 123 123
it is also monotonically related to the contrast model , un - der the same parameter settings .
interpreting this formal correspondence between our bayesian model of generalization and tverskys set - theo - retic models of similarity is complicated by the fact that in general the relation between similarity and generalization is not well understood .
a number of authors have proposed that similarity is the more primitive cognitive process and forms ( part of ) the basis for our capacity to generalize in - ductively ( goldstone 123; osherson et al .
123; quine 123; rips 123; smith 123 ) .
but from the standpoint of reverse - engineering the mind and explaining why human similarity or generalization computations take the form that they do , a satisfying theory of similarity is more likely to de - pend upon a theory of generalization than vice versa .
the problem of generalization can be stated objectively and given a principled rational analysis , while the question of how similar two objects are is notoriously slippery and un - derdetermined ( goodman 123 ) .
we expect that , depend - ing on the context of judgment , the similarity of y to x may involve the probability of generalizing from x to y , or from y to x , or some combination of those two .
it may also de - pend on other factors altogether .
qualifications aside , in - teresting consequences nonetheless follow just from the hypothesis that similarity somehow depends on generaliza - tion , without specifying the exact nature of the depen -
the syntax of similarity .
most fundamentally , our bayesian analysis provides a rational basis for the qualita - tive form of set - theoretic models of similarity .
for instance , it explains why similarity should in principle depend on both the common and the distinctive features of objects .
tversky ( 123 ) asserted as an axiom that similarity is a func - tion of both common and distinctive features , and he pre - sented some empirical evidence consistent with that as - sumption , but he did not attempt to explain why it should hold in general .
indeed , there exist both empirical models ( shepard 123 ) and theoretical arguments ( chater & hahn 123 ) that have successfully employed only common or dis - tinctive features .
our rational analysis ( eq .
123 ) , in contrast , explains why both kinds of features should matter in gen - eral , under the assumption that similarity depends on gen - eralization .
the more hypothesized consequential subsets that contain both x and y ( common features of x and y ) , rel - ative to the number that contain only x ( distinctive features of x ) , the higher the probability that a subset known to con - tain x will also contain y .
along similar lines , the hypothesis that similarity de - pends in part on generalization explains why similarity may in principle be an asymmetric relationship , that is , why the similarity of x to y may differ from the similarity of y to x .
tversky ( 123 ) presented compelling demonstrations of such asymmetries and showed that they could be modeled in his set - theoretic framework if the two subsets of distinc - tive features x 123 y and y 123 x have different measures under f and are given different weights in equations 123 or 123
but tverskys formal theory does not explain why those two subsets should be given different weights; it merely al - lows this as one possibility .
in contrast , the probability of generalizing from x to y is intrinsically an asymmetric func - tion , depending upon the distinctive features of x but not those of y .
likewise , the probability of generalizing from y to x depends only on the distinctive features of y , not those of x .
to the extent that similarity depends on either or both of these generalization probabilities , it inherits their intrin - sic asymmetry .
note that generalization can still be sym - metric , when the distinctive features of x and y are equal in number and weight .
this condition holds in the spatial sce - narios considered above and in shepards work , which ( not coincidentally ) are also the domains in which similarity is found to be most nearly symmetric ( tversky 123 ) .
finally , like shepards analysis of generalization , tver - skys contrast model was originally defined only for the com - parison of two individual objects .
however , our bayesian framework justifies a natural extension to the problem of computing the similarity of an object y to a set of objects x 123 ( x123 , .
xn ) as a whole , just as it did for shepards the - ory in section 123
heit ( 123a ) proposed on intuitive grounds that the contrast model should still apply in this situation , but with the feature set x for the examples as a whole iden - tified with >n i , the intersection of the feature sets of all the individual examples .
our bayesian analysis ( replac - ing x with x in eq .
123 ) explains why the intersection , as op - posed to some other combination mechanism such as the union , is appropriate .
only those hypotheses consistent with all the examples in x corresponding to those features belonging to the intersection of all the feature sets x ceive non - zero likelihood under equation 123
the semantics of similarity .
perhaps the most per - sistent criticisms of the contrast model and its relatives fo -
behavioral and brain sciences ( 123 ) 123 : 123
tenenbaum & griffiths : generalization , similarity , and bayesian inference
cus on semantic questions : what qualifies as a feature ? what determines the feature weights ? how do the weights change across judgment contexts ? the contrast model has such broad explanatory scope because it allows any kind of features and any feature weights whatsoever , but this same lack of constraint also prevents the model from explaining the origins of the features or weights .
our bayesian model likewise offers no constraints about what qualifies as a fea - ture , but it does explain some aspects of the origins and the dynamics of feature weights .
the bayesian feature weight p ( h , x ) 123 p ( xuh ) p ( h ) decomposes into prior and likelihood terms .
the prior p ( h ) is not constrained by our analysis; it can accommodate arbitrary flexibility across contexts but explains none of that flexibility .
in contrast , the likelihood p ( xuh ) is constrained by the assumption of strong sampling to follow the size principle .
one direct implication of this constraint is that , in a given context , features belonging to fewer objects correspond - ing to hypotheses with smaller sizes should be assigned higher weights .
this prediction can be tested using additive clustering analyses , which recover a combination of feature extensions and feature weights that best fit a given similar - ity data set .
for instance , the additive clustering analysis of the integers 123 presented in table 123 is consistent with our prediction , with a negative correlation ( r 123 123 ) between the number of stimuli in each cluster and the correspond - ing feature weights .
similar relationships can be found in several other additive clustering analyses ( arabie & carroll 123; chaturvedi & carroll 123; lee , submitted; tenen - baum 123 ) ; see tenenbaum et al .
( in preparation ) for a comprehensive study .
tversky ( 123 ) proposed several general principles of feature weighting , such as the diag - nosticity principle , but he did not explicitly propose a cor - relation between feature specificity and feature weight , nor was his formal model designed to predict these effects .
a second implication of the size principle is that certain kinds of features should tend to receive higher weights in similarity comparisons , if they systematically belong to fewer objects .
medin et al .
( 123 ) have argued that primi - tive features are often not as important as are relational fea - tures , that is , higher - order features defined by relations be - tween primitives .
yet in some cases a relation appears less important than a primitive feature .
consider which bottom stimulus , a or b , is more similar to the top stimulus in each panel of figure 123 ( inspired by medin et al . s comparisons ) .
in the left panel , the top stimulus shares a primitive feature with b ( triangle on top ) and a relational feature with a ( all different shapes ) .
in an informal survey , 123 out of 123
observers chose b the primitive feature match as more similar at first glance .
in the right panel , however , a differ - ent relation ( all same shape ) dominates over the same primitive feature ( 123 out of 123 different observers chose a as more similar ) .
goldstone et al .
( 123 ) report several other cases where same relations are weighted more highly than different relations in similarity comparisons .
if sim - ilarity depends in part upon bayesian generalization , then the size principle can explain the relative salience of these features in figure 123
let m be the number of distinct shapes ( square , triangle , etc . ) that can appear in the three positions of each stimulus pattern .
then the consequential subset for all same shape contains exactly m distinct stimuli , the sub - set for triangle on top contains m123 stimuli , and the subset for all different shapes contains m ( m 123 123 ) ( m 123 123 ) stim - uli .
thus feature saliency is inversely related to subset size , just as we would expect under the size principle .
more care - ful empirical tests of this hypothesis are required , but we conjecture that much of the relative importance of rela - tional features versus primitive features may be explained by their differing specificities .
a final implication arises from the interaction of the size principle with multiple examples .
recall that in generaliz - ing from multiple examples , the likelihood preference for smaller hypotheses increases exponentially in the number of examples ( eq .
the same effect can be observed with the weights of features in similarity judgments .
for instance , in assessing the similarity of a number to 123 , the feature multiple of ten may or may not receive slightly greater weight than the feature even number .
but in assessing similarity to the set of numbers ( 123 , 123 , 123 , 123 ) as a whole , even though both of those features are equally consistent with the full set of examples , the more specific feature multiple of ten appears to be much more salient .
conclusions : learning , evolution ,
and the origins of hypothesis spaces
we have described a bayesian framework for learning and generalization that significantly extends shepards theory in two principal ways .
in addressing generalization from mul - tiple examples , our analysis is a fairly direct extension of
table 123
additive clustering of similarity judgments
for the integers 123 ( from tenenbaum 123 )
stimuli in class
123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123
powers of two multiples of three
behavioral and brain sciences ( 123 ) 123 : 123
figure 123
the relative weight of relations and primitive features depends on the size of the set of objects that they identify .
most observers choose b ( the primitive feature match ) as more similar to the top stimulus in the left panel , but choose a ( the relational match ) in the right panel , in part because the relation all same shape identifies a much smaller subset of objects than the rela - tion all different shapes .
tenenbaum & griffiths : generalization , similarity , and bayesian inference
shepards original ideas , making no substantive additional assumptions other than strong sampling .
in contrast , our analysis of generalization with arbitrarily structured stimuli represents a more radical broadening of shepards ap - proach , in giving up the notion that generalization is con - strained by the metric properties of an evolutionarily in - ternalized psychological space .
on the positive side , this step allows us to draw together tverskys set - theoretic models of similarity and shepards continuous metric space models of generalization under a single rational frame - work , and even to advance the explanatory power of tver - skys set - theoretic models using the same tools chiefly , the size principle that we used to advance shepards analysis of generalization .
yet it also opens the door to some large unanswered questions , which we close our article by
in discussing similarity or generalization with arbitrarily structured stimuli , our bayesian analysis explains only one piece of the puzzle of how features or hypotheses are weighted .
weights are always a product of both size - based likelihoods and priors , and while the size principle follows rationally from the assumption of strong sampling , the as - signment of prior probabilities lies outside the scope of a basic bayesian analysis .
thus , we can never say anything for certain about the relative weights of any two particular fea - tures or hypotheses merely based on their relative sizes; any size difference can always be overruled by a greater differ - ence in prior probability .
the ability of prior probability differences to overrule an opposing size - based likelihood difference is hardly patho - logical; on the contrary , it is essential in every successful in - ductive generalization .
consider as a hypothesis in the number game that the computer accepts all multiples of ten , except 123 and 123
multiples of ten , except 123 and 123 is slightly more specific than all multiples of ten , and thus should receive higher probability under the size principle given a set of examples that is consistent with both hy - potheses , such as ( 123 , 123 , 123 , 123 ) .
but obviously , that does not happen in most peoples minds .
our bayesian frame - work can accommodate this phenomenon by stipulating that while the former hypothesis receives a somewhat higher likelihood , it receives a very much lower prior prob - ability , and thus a significantly lower posterior probability when the prior and likelihood are combined .
it is by now almost a truism that without some reasonable a priori constraints on the hypotheses that learners should consider , there will always be innumerable bizarre hy - potheses such as all multiples of ten , except 123 and 123 that will stand in the way of reasonable inductive generalizations ( goodman 123; 123; mitchell 123 ) .
trying to determine the nature and origin of these constraints is one of the ma - jor goals of much current research ( e . g . , medin et al .
123; schyns et al .
shepards original analysis of general - ization was so compelling in part because it proposed an - swers to these questions : sufficient constraints on the form of generalization are provided merely by the representation of stimuli as points in a continuous metric psychological space ( together with the assumption that hypotheses corre - spond to a suitable family of regions in that space ) , and our psychological spaces themselves are the products of an evo - lutionary process that has shaped them optimally to reflect the structure of our environment .
in proposing a theory of generalization that allows for arbitrarily structured hypoth - esis spaces , we owe some account of where those hypothe -
sis spaces and priors might come from .
evolution alone is not sufficient to explain why hypotheses such as multiples of ten are considered natural while hypotheses such as all multiples of ten , except 123 and 123 are not .
the major alternative to evolution as the source of hy - pothesis space structure is some kind of prior learning .
most directly , prior experience that all and only those ob - jects belonging to some particular subset h tend to possess a number of important consequences may lead learners to increase p ( h ) for new consequences of the same sort .
un - supervised learning observation of the properties of ob - jects without any consequential input may also be ex - tremely useful in forming a hypothesis space for supervised ( consequential ) learning .
noting that a subset of objects tend to cluster together , to be more similar to each other than to other objects on some primitive features , may in - crease a learners prior probability that this subset is likely to share some important but as - yet - unencountered conse - quence .
the machine learning community is now intensely interested in improving the inductive generalizations that a supervised learning agent can draw from a few labeled ex - amples , by building on unsupervised inferences that the agent can draw from a large body of unlabeled examples ( e . g . , mitchell 123; poggio & shelton 123 ) .
we expect this to become a critical issue in the near future for cogni - tive science as well .
our proposal that the building blocks of shepards per - ceptual - cognitive universals come into our heads via learn - ing , and not just evolution , resonates with at least one other contribution to this issue ( see barlows target article ) .
how - ever , we fundamentally agree with an earlier statement of shepards , that learning is not an alternative to evolution but itself depends on evolution .
there can be no learning in the absence of principles of learning; yet such principles , being themselves unlearned , must have been shaped by evolution ( shepard 123a , p .
ultimately , we believe that it may be difficult or impossible to separate the contri - butions that learning and evolution each make to the inter - nalization of world structure , given the crucial role that each process plays in making the other an ecologically vi - able means of adaptation .
rather , we think that it may be more worthwhile to look for productive synergies of the two processes , tools which evolution might have given us for ef - ficiently learning those hypothesis spaces that will lead us to successful bayesian generalizations .
such tools might in - clude appropriately tuned stimulus metrics and topologies , as shepard proposes , but also perhaps : unsupervised clus - tering algorithms that themselves exploit the size principle as defined over these metrics; a vocabulary of templates for the kinds of hypothesis spaces continuous spaces , taxo - nomic trees , conjunctive feature structures that seem to recur over and over as the basis for mental representations across many domains; and the ability to recursively com - pose hypothesis spaces in order to build up structures of
we believe that the search for universal principles of learning and generalization has only just begun with shep - ards work .
the universality , invariance , and elegance of shepards exponential law ( to quote from his article re - printed in this volume ) are in themselves impressive , but perhaps ultimately of less significance than the spirit of ra - tional analysis that he has pioneered as a general avenue for the discovery of perceptual - cognitive universals .
here we have shown how this line of analysis can be extended
behavioral and brain sciences ( 123 ) 123 : 123
tenenbaum & griffiths : generalization , similarity , and bayesian inference
haustive and mutually exclusive set of possibilities , we can expand the generalization function as
, ) ( h x p h x
note that p ( y ( cuh , x ) is in fact independent of x .
it is simply 123 if y ( h , and 123 otherwise .
thus we can rewrite equation 123 in the form of equation 123
note that in a continuous space , when uhu , 123 , p ( xuh ) will be greater than 123 ( for x ( h ) .
this occurs because p ( xuh ) is a prob - ability density , not a probability distribution; probability density functions may take on values greater than 123 , as long as they inte - grate to 123 over all x .
to yield what may yet prove to be another universal : the size principle , which governs generalization from one or more examples of arbitrary structure .
we speculate that further universal principles will result from turning our attention in the future to the interface of learning and evo -
the writing of this article was supported in part by nsf grant dbs - 123 and a gift from mitsubishi electric research labs .
the second author was supported by a hackett studentship .
we derive equation 123 as follows .
because h denotes an ex -
behavioral and brain sciences ( 123 ) 123 : 123
