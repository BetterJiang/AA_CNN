we introduce the author - topic model , a gen - erative model for documents that extends la - tent dirichlet allocation ( lda; blei , ng , & jordan , 123 ) to include authorship informa - tion .
each author is associated with a multi - nomial distribution over topics and each topic is associated with a multinomial distribution over words .
a document with multiple au - thors is modeled as a distribution over topics that is a mixture of the distributions associ - ated with the authors .
we apply the model to a collection of 123 , 123 nips conference pa - pers and 123 , 123 citeseer abstracts .
exact inference is intractable for these datasets and we use gibbs sampling to estimate the topic and author distributions .
we compare the performance with two other generative mod - els for documents , which are special cases of the author - topic model : lda ( a topic model ) and a simple author model in which each au - thor is associated with a distribution over words rather than a distribution over top - ics .
we show topics recovered by the author - topic model , and demonstrate applications to computing similarity between authors and entropy of author output .
characterizing the content of documents is a standard problem addressed in information retrieval , statistical natural language processing , and machine learning .
a representation of document content can be used to or - ganize , classify , or search a collection of documents .
recently , generative models for documents have begun to explore topic - based content representations , model - ing each document as a mixture of probabilistic top - ics ( e . g . , blei , ng , & jordan , 123; hofmann , 123 ) .
here , we consider how these approaches can be used to address another fundamental problem raised by large document collections : modeling the interests of au -
by modeling the interests of authors , we can answer a range of important queries about the content of document collections .
with an appropriate author model , we can establish which subjects an author writes about , which authors are likely to have writ - ten documents similar to an observed document , and which authors produce similar work .
however , re - search on author modeling has tended to focus on the problem of authorship attribution ( who wrote which document ( for which discriminative models based on relatively super ( cid : 123 ) cial features are often su ( cid : 123 ) cient .
for example , the \stylometric " approach ( e . g . , holmes & forsyth , 123 ) ( cid : 123 ) nds stylistic features ( e . g . , frequency of certain stop words , sentence lengths , diversity of an authors vocabulary ) that discriminate between di ( cid : 123 ) er -
in this paper we describe a generative model for doc - ument collections , the author - topic model , that simul - taneously models the content of documents and the interests of authors .
this generative model represents each document with a mixture of topics , as in state - of - the - art approaches like latent dirichlet allocation ( blei et al . , 123 ) , and extends these approaches to author modeling by allowing the mixture weights for di ( cid : 123 ) erent topics to be determined by the authors of the document .
by learning the parameters of the model , we obtain the set of topics that appear in a corpus and their relevance to di ( cid : 123 ) erent documents , as well as identifying which topics are used by which authors .
the paper is organized as follows .
in section 123 , we discuss generative models for documents using authors and topics , and introduce the author - topic model .
we devote section 123 to describing the gibbs sampler used for inferring the model parameters , and in section 123 we present the results of applying this algorithm to two collections of computer science documents|nips
uai 123rosen - zvi et al . 123 conference papers and abstracts from the citeseer database .
we conclude and discuss further research directions in section 123
123 generative models for documents
we will describe three generative models for docu - ments : one that models documents as a mixture of topics ( blei et al . , 123 ) , one that models authors with distributions over words , and one that models both authors and documents using topics .
all three models use the same notation .
a document d is a vector of nd words , wd , where each wid is chosen from a vocabulary of size v , and a vector of ad authors ad , chosen from a set of authors of size a .
a collection of d documents is de ( cid : 123 ) ned by d = f ( w123; a123 ) ; : : : ; ( wd; ad ) g .
123 modeling documents with topics
a number of recent approaches to modeling document content are based upon the idea that the probabil - ity distribution over words in a document can be ex - pressed as a mixture of topics , where each topic is a probability distribution over words ( e . g . , blei , et al . , 123; hofmann , 123 ) .
we will describe one such model ( latent dirichlet allocation ( lda; blei et al . , in lda , the generation of a document col - lection is modeled as a three step process .
first , for each document , a distribution over topics is sampled from a dirichlet distribution .
second , for each word in the document , a single topic is chosen according to this distribution .
finally , each word is sampled from a multinomial distribution over words speci ( cid : 123 ) c to the
this generative process corresponds to the hierarchical bayesian model shown ( using plate notation ) in fig - ure 123 ( a ) .
in this model , ( cid : 123 ) denotes the matrix of topic distributions , with a multinomial distribution over v vocabulary items for each of t topics being drawn in - dependently from a symmetric dirichlet ( ( cid : 123 ) ) prior .
( cid : 123 ) is the matrix of document - speci ( cid : 123 ) c mixture weights for these t topics , each being drawn independently from a symmetric dirichlet ( ( cid : 123 ) ) prior .
for each word , z de - notes the topic responsible for generating that word , drawn from the ( cid : 123 ) distribution for that document , and w is the word itself , drawn from the topic distribution ( cid : 123 ) corresponding to z .
estimating ( cid : 123 ) and ( cid : 123 ) provides information about the topics that participate in a cor - pus and the weights of those topics in each document respectively .
a variety of algorithms have been used
123the model we describe is actually the smoothed lda model ( blei et al . , 123 ) with symmetric dirichlet priors ( gri ( cid : 123 ) ths & steyvers , 123 ) as this is closest to the author -
to estimate these parameters , including variational in - ference ( blei et al . , 123 ) , expectation propagation ( minka & la ( cid : 123 ) erty , 123 ) , and gibbs sampling ( grif - ( cid : 123 ) ths & steyvers , 123 ) .
however , this topic model provides no explicit information about the interests of authors : while it is informative about the content of documents , authors may produce several documents ( often with co - authors ( and it is consequently unclear how the topics used in these documents might be used to describe the interests of the authors .
123 modeling authors with words
topic models illustrate how documents can be mod - eled as mixtures of probability distributions .
this sug - gests a simple method for modeling the interests of au - thors .
assume that a group of authors , ad , decide to write the document d .
for each word in the document an author is chosen uniformly at random , and a word is chosen from a probability distribution over words that is speci ( cid : 123 ) c to that author .
this model is similar to a mixture model proposed by mccallum ( 123 ) and is equivalent to a variant of lda in which the mixture weights for the di ( cid : 123 ) er - ent topics are ( cid : 123 ) xed .
the underlying graphical model is shown in figure 123 ( b ) .
x indicates the author of a given word , chosen uniformly from the set of authors ad .
each author is associated with a probability dis - tribution over words ( cid : 123 ) , generated from a symmetric dirichlet ( ( cid : 123 ) ) prior .
estimating ( cid : 123 ) provides information about the interests of authors , and can be used to an - swer queries about author similarity and authors who write on subjects similar to an observed document .
however , this author model does not provide any in - formation about document content that goes beyond the words that appear in the document and the au - thors of the document .
123 the author - topic model
the author - topic model draws upon the strengths of the two models de ( cid : 123 ) ned above , using a topic - based rep - resentation to model both the content of documents and the interests of authors .
as in the author model , a group of authors , ad , decide to write the document d .
for each word in the document an author is chosen uniformly at random .
then , as in the topic model , a topic is chosen from a distribution over topics speci ( cid : 123 ) c to that author , and the word is generated from the
the graphical model corresponding to this process is shown in figure 123 ( c ) .
as in the author model , x indi - cates the author responsible for a given word , chosen from ad .
each author is associated with a distribution over topics , ( cid : 123 ) , chosen from a symmetric dirichlet ( ( cid : 123 ) )
123rosen - zvi et al . uai 123 topic ( lda )
figure 123 : generative models for documents .
( a ) latent dirichlet allocation ( lda; blei et al . , 123 ) , a topic model .
( b ) an author model .
( c ) the author - topic model .
the mixture weights corresponding to the cho - sen author are used to select a topic z , and a word is generated according to the distribution ( cid : 123 ) correspond - ing to that topic , drawn from a symmetric dirichlet ( ( cid : 123 ) )
the author - topic model subsumes the two models de - scribed above as special cases : topic models like lda correspond to the case where each document has one unique author , and the author model corresponds to the case where each author has one unique topic .
by estimating the parameters ( cid : 123 ) and ( cid : 123 ) , we obtain informa - tion about which topics authors typically write about , as well as a representation of the content of each docu - ment in terms of these topics .
in the remainder of the paper , we will describe a simple algorithm for estimat - ing these parameters , compare these di ( cid : 123 ) erent models , and illustrate how the results produced by the author - topic model can be used to answer questions about which which authors work on similar topics .
123 gibbs sampling algorithms
a variety of algorithms have been used to estimate the parameters of topic models , from basic expectation - maximization ( em; hofmann , 123 ) , to approximate inference methods like variational em ( blei et al . , 123 ) , expectation propagation ( minka & la ( cid : 123 ) erty , 123 ) , and gibbs sampling ( gri ( cid : 123 ) ths & steyvers , 123 ) .
generic em algorithms tend to face problems with local maxima in these models ( blei et al . , 123 ) , suggesting a move to approximate methods in which some of the parameters|such as ( cid : 123 ) and ( cid : 123 ) |can be in - tegrated out rather than explicitly estimated .
in this paper , we will use gibbs sampling , as it provides a sim - ple method for obtaining parameter estimates under dirichlet priors and allows combination of estimates
from several local maxima of the posterior distribu -
the lda model has two sets of unknown parameters ( the d document distributions ( cid : 123 ) , and the t topic distri - butions ( cid : 123 ) ( as well as the latent variables correspond - ing to the assignments of individual words to topics z .
by applying gibbs sampling ( see gilks , richardson , & spiegelhalter , 123 ) , we construct a markov chain that converges to the posterior distribution on z and then use the results to infer ( cid : 123 ) and ( cid : 123 ) ( gri ( cid : 123 ) ths & steyvers , 123 ) .
the transition between successive states of the markov chain results from repeatedly drawing z from its distribution conditioned on all other variables , sum - ming out ( cid : 123 ) and ( cid : 123 ) using standard dirichlet integrals :
p ( zi = jjwi = m; z ( cid : 123 ) i; w ( cid : 123 ) i ) / mj + ( cid : 123 )
dj + ( cid : 123 )
m123j + v ( cid : 123 )
pm123 cw t
dj 123 + t ( cid : 123 )
pj 123 c dt
where zi = j represents the assignments of the ith word in a document to topic j , wi = m represents the observation that the ith word is the mth word in the lexicon , and z ( cid : 123 ) i represents all topic assignments not including the ith word .
furthermore , c w t number of times word m is assigned to topic j , not including the current instance , and c dt is the num - ber of times topic j has occurred in document d , not including the current instance .
for any sample from this markov chain , being an assignment of every word to a topic , we can estimate ( cid : 123 ) and ( cid : 123 ) using
mj + ( cid : 123 )
m123j + v ( cid : 123 )
pm123 cw t pj 123 c dt
dj + ( cid : 123 )
dj 123 + t ( cid : 123 )
uai 123rosen - zvi et al . 123 where ( cid : 123 ) mj is the probability of using word m in topic j , and ( cid : 123 ) dj is the probability of topic j in document d .
these values correspond to the predictive distributions over new words w and new topics z conditioned on w
an analogous approach can be used to derive a gibbs sampler for the author model .
speci ( cid : 123 ) cally , we have
p ( xi = kjwi = m; x ( cid : 123 ) i; w ( cid : 123 ) i; ad ) /
mk + ( cid : 123 )
m123k + v ( cid : 123 )
pm123 cw a
where xi = k represents the assignments of the ith word in a document to author k and c w a mk is the num - ber of times word m is assigned to author k .
an esti - mate of ( cid : 123 ) can be obtained via
similar to equation 123
mk + ( cid : 123 )
m123k + v ( cid : 123 )
pm123 cw a
in the author - topic model , we have two sets of latent variables : z and x .
we draw each ( zi; xi ) pair as a block , conditioned on all other variables :
p ( zi = j; xi = kjwi = m; z ( cid : 123 ) i; x ( cid : 123 ) i; w ( cid : 123 ) i; ad ) / kj + ( cid : 123 ) kj 123 + t ( cid : 123 )
m123j + v ( cid : 123 )
mj + ( cid : 123 )
pm123 cw t
pj 123 c at
where zi = j and xi = k represent the assignments of the ith word in a document to topic j and author k re - spectively , wi = m represents the observation that the ith word is the mth word in the lexicon , and z ( cid : 123 ) i; x ( cid : 123 ) i represent all topic and author assignments not includ - ing the ith word , and c at is the number of times author k is assigned to topic j , not including the cur - rent instance .
equation 123 is the conditional probabil - ity derived by marginalizing out the random variables ( cid : 123 ) ( the probability of a word given a topic ) and ( cid : 123 ) ( the probability of a topic given an author ) .
these random variables are estimated from samples via
mj + ( cid : 123 )
m123j + v ( cid : 123 )
pm123 cw t pj 123 c at
kj + ( cid : 123 ) kj 123 + t ( cid : 123 )
in the examples considered here , we do not estimate the hyperparameters ( cid : 123 ) and ( cid : 123 ) |instead the smoothing parameters are ( cid : 123 ) xed at 123=t and 123 : 123 respectively .
each of these algorithms requires tracking only small amounts of information from a corpus .
for example , in the author - topic model , the algorithm only needs to keep track of a v ( cid : 123 ) t ( word by topic ) count ma - trix , and an a ( cid : 123 ) t ( author by topic ) count matrix , both of which can be represented e ( cid : 123 ) ciently in sparse
format .
we start the algorithm by assigning words to random topics and authors ( from the set of authors on the document ) .
each iteration of the algorithm in - volves applying equation 123 to every word token in the document collection , which leads to a time complex - ity that is of order of the total number of word tokens in the training data set multiplied by the number of topics , t ( assuming that the number of authors on each document has negligible contribution to the com - plexity ) .
the count matrices are saved at the 123th iteration of this sampling process .
we do this 123 times so that 123 samples are collected in this manner ( the markov chain is started 123 times from random initial
123 experimental results
in our results we used two text data sets consisting of technical papers|full papers from the nips confer - ence123 and abstracts from citeseer ( lawrence , giles , & bollacker , 123 ) .
we removed extremely common words from each corpus , a standard procedure in \bag of words " models .
this leads to a vocabulary size of v = 123; 123 unique words in the nips data set and v = 123; 123 unique words in the citeseer data set .
our collection of nips papers contains d = 123; 123 pa - pers with k = 123; 123 authors and a total of 123; 123; 123 word tokens .
our collection of citeseer abstracts con - tains d = 123; 123 abstracts with k = 123; 123 authors and a total of 123; 123; 123 word tokens .
123 examples of topic and author
the nips data set contains papers from the nips conferences between 123 and 123
the conference is characterized by contributions from a number of di ( cid : 123 ) erent research communities in the general area of learning algorithms .
figure 123 illustrates examples of 123 topics ( out of 123 ) as learned by the model for the nips corpus .
the topics are extracted from a single sample at the 123th iteration of the gibbs sampler .
each topic is illustrated with ( a ) the top 123 words most likely to be generated conditioned on the topic , and ( b ) the top 123 most likely authors to have generated a word conditioned on the topic .
the ( cid : 123 ) rst 123 topics we selected for display ( left to right across the top and the ( cid : 123 ) rst two on the left on the bottom ) are quite speci ( cid : 123 ) c representations of di ( cid : 123 ) erent topics that have been pop - ular at the nips conference over the time - period 123 ( 123 : em and mixture models , handwritten character recognition , reinforcement learning , svms and kernel methods , speech recognition , and bayesian learning .
123the nips data set in matlab format is available on - line at http : / / www . cs . toronto . edu / ~ roweis / data . html .
123rosen - zvi et al . uai 123 topic 123
figure 123 : an illustration of 123 topics from a 123 - topic solution for the nips collection .
each topic is shown with the 123 words and authors that have the highest probability conditioned on that topic .
for each topic , the top 123 most likely authors are well - known authors in terms of nips papers written on these topics ( e . g . , singh , barto , and sutton in rein - forcement learning ) .
while most ( order of 123 to 123% ) of the 123 topics in the model are similarly speci ( cid : 123 ) c in terms of semantic content , the remaining 123 topics we display illustrate some of the other types of \top - ics " discovered by the model .
topic 123 is somewhat generic , covering a broad set of terms typical to nips papers , with a somewhat ( cid : 123 ) atter distribution over au - thors compared to other topics .
topic 123 is somewhat oriented towards geo ( cid : 123 ) hintons group at the univer - sity of toronto , containing the words that commonly appeared in nips papers authored by members of that research group , with an author list largely consisting of hinton plus his past students and postdocs .
figure 123 shows similar types of results for 123 selected topics from the citeseer data set , where again top - ics on speech recognition and bayesian learning show up .
however , since citeseer is much broader in con - tent ( covering computer science in general ) compared to nips , it also includes a large number of topics not
figure 123 : an illustration of 123 topics from a 123 - topic solution for the citeseer collection .
each topic is shown with the 123 words and authors that have the highest probability conditioned on that topic .
seen in nips , from user interfaces to solar astrophysics ( figure 123 ) .
again the author lists are quite sensible| for example , ben shneiderman is a widely - known se - nior ( cid : 123 ) gure in the area of user - interfaces .
for the nips data set , 123 iterations of the gibbs sampler took 123 hours of wall - clock time on a stan - dard pc workstation ( 123 seconds per iteration ) .
cite - seer took 123 hours for 123 iterations ( 123 minutes per iteration ) .
the full list of tables can be found at http : / / www . datalab . uci . edu / author - topic , for both the 123 - topic nips model and the 123 - topic cite - in addition there is an online java browser for interactively exploring authors , topics , and
the results above use a single sample from the gibbs sampler .
across di ( cid : 123 ) erent samples each sample can contain somewhat di ( cid : 123 ) erent topics i . e . , somewhat dif - ferent sets of most probable words and authors given the topic , since according to the author - topic model there is not a single set of conditional probabilities , ( cid : 123 ) and ( cid : 123 ) , but rather a distribution over these conditional probabilities .
in the experiments in the sections below , we average over multiple samples ( restricted to 123 for computational convenience ) in a bayesian fashion for
123 evaluating predictive power
in addition to the qualitative evaluation of topic - author and topic - word results shown above , we also evaluated the proposed author - topic model in terms of perplexity , i . e . , its ability to predict words on new unseen documents .
we divided the d = 123; 123 nips papers into a training set of 123; 123 papers with a total of 123; 123; 123 words , and a test set of 123 papers of
uai 123rosen - zvi et al . 123 which 123 are single - authored papers .
we chose the test data documents such that each of the 123; 123 au - thors of the nips collection authored at least one of the training documents .
perplexity is a standard measure for estimating the performance of a probabilistic model .
the perplex - ity of a set of test words , ( wd; ad ) for d 123 dtest , is de ( cid : 123 ) ned as the exponential of the negative normalized predictive likelihood under the model ,
perplexity ( wdjad ) = exp ( cid : 123 ) ( cid : 123 )
better generalization performance is indicated by a lower perplexity over a held - out document .
the derivation of the probability of a set of words given the authors is a straightforward calculation in the author - topic model :
p ( wdjad ) = z d ( cid : 123 ) z d ( cid : 123 ) p ( ( cid : 123 ) jdtrain ) p ( ( cid : 123 ) jdtrain )
the term in the brackets is simply the probability for the word wm given the set of authors ad .
we approx - imate the integrals over ( cid : 123 ) and ( cid : 123 ) using the point esti - mates obtained via equations 123 and 123 for each sample of assignments x; z , and then average over samples .
for documents with a single author this formula be -
wmj are point estimates from sample s , s is the number of samples used , and ad is no longer a vector but a scalar that stands for the author of the
in the ( cid : 123 ) rst set of experiments we compared the topic model ( lda ) of section 123 , the author model of sec - tion 123 , and our proposed author - topic model from section 123 .
for each test document , a randomly gen - erated set of n ( train ) training words were selected and combined with the training data .
each model then made predictions on the other words in each test doc - ument , conditioned on the combination of both ( a ) the documents in the training data corpus and ( b ) the words that were randomly selected from the document .
this simulates the process of observing some of the words in a document and making predictions about the rest .
we would expect that as n ( train ) the predictive power of each model would improve as it adapts to the document .
the author - topic and author
figure 123 : perplexity of the 123 single - authored test documents from the nips collection , conditioned both on the correct author and authors ranked by perplexity using the model , as described in the text .
models were both also conditioned on the identity of the true authors of the document .
in all models , the topic and author distributions were all updated to new predictive distributions given the combination of the training words for the document being pre - dicted and the full training data corpus .
we averaged over 123 samples from the gibbs sampler when making predictions for each word .
figure 123 shows the results for the 123 models being com - pared .
the author model is clearly poorer than either of the topic - based models , as illustrated by its high perplexity .
since a distribution over words has to be estimated for each author , ( cid : 123 ) tting this model involves ( cid : 123 ) nding the values of a large number of parameters , limiting its generalization performance .
the author - topic model has lower perplexity early on ( for small values of n ( train ) ) since it uses knowledge of the au - thor to provide a better prior for the content of the document .
however , as n ( train ) increases we see a cross - over point where the more ( cid : 123 ) exible topic model adapts better to the content of this particular docu - ment .
since no two scienti ( cid : 123 ) c papers are exactly the same , the expectation that this document will match the previous output of its authors begins to limit the predictive power of the author - topic model .
for larger numbers of topics , this crossover occurs for smaller val - ues of n ( train ) , since the topics pick out more speci ( cid : 123 ) c areas of the subject domain .
to illustrate the utility of these models in predicting words conditioned on authors , we derived the perplex - ity for each of the 123 singled - authored test documents in the nips collection using the full text of each docu - ment and s = 123
the averaged perplexity as a func - tion of the number of topics t is presented in fig -
123rosen - zvi et al . uai 123 y
123 123 123 123 123
123 123 123 123 123
123 123 123 123 123
123 123 123 123 123
123 123 123 123 123
123 123 123 123 123
123 123 123
figure 123 : perplexity versus n ( train )
for di ( cid : 123 ) erent numbers of topics , for the author , author - topic , and topic ( lda )
ure 123 ( thick line ) .
we also derived the perplexity of the test documents conditioned on each one of the au - thors from the nips collection , perplexity ( wdja ) for a = 123; : : : ; k .
this results in k = 123; 123 di ( cid : 123 ) erent per - plexity values .
then we ranked the results and various percentiles from this ranking are presented in figure 123
one can see that making use of the authorship information signi ( cid : 123 ) cantly improves the predictive log - likelihood : the model has accurate expectations about the content of documents by particular authors .
as the number of topics increases the ranking of the cor - rect author improves , where for 123 topics the aver - aged ranking of the correct author is within the 123 highest ranked authors ( out of 123 , 123 possible authors ) .
consequently , the model provides a useful method for identifying possible authors for novel documents .
table 123 : symmetric kl divergence for pairs of authors
bartlett p ( 123 ) shawe - taylor j ( 123 ) barto a ( 123 ) singh s ( 123 ) amari s ( 123 ) yang h ( 123 ) singh s ( 123 ) sutton r ( 123 ) moore a ( 123 ) sutton r ( 123 )
n t=123 t=123 t=123
note : n is number of common papers in nips dataset .
illustrative applications of the model
the author - topic model could be used for a variety of applications such as automated reviewer recommenda - tions , i . e . , given an abstract of a paper and a list of the authors plus their known past collaborators , generate a list of other highly likely authors for this abstract who might serve as good reviewers .
such a task re - quires computing the similarity between authors .
to illustrate how the model could be used in this respect , we de ( cid : 123 ) ned the distance between authors i and j as the symmetric kl divergence between the topics distribu - tion conditioned on each of the authors :
skl ( i; j ) =
+ ( cid : 123 ) jt log
as earlier , we derived the averaged symmetric kl di - vergence by averaging over samples from the posterior
we searched for similar pairs of authors in the nips data set using the distance measure above .
we searched only over authors who wrote more than 123 papers in the full nips data set|there are 123 such authors out of the full set of 123
table 123 shows the 123 pairs of authors with the highest averaged skl for the 123 - topic model , as well as the median and min - imum .
results for the 123 and 123 - topic models are also shown as are the number of papers in the data set for each author ( in parentheses ) and the number of co - authored papers in the data set ( 123nd column ) .
all results were averaged over 123 samples from the gibbs
again the results are quite intuitive .
for example , although authors bartlett and shawe - taylor did not have any co - authored documents in the nips collec -
uai 123rosen - zvi et al . 123 table 123 : author entropies
n t=123 t=123 t=123
jordan m 123 brand m 123 note : n is the number of papers by each author .
tion , they have in fact co - authored on other papers .
similarly , although a .
moore and r .
sutton have not co - authored any papers to our knowledge , they have both ( separately ) published extensively on the same topic of reinforcement learning .
the distances between the authors ranked highly ( in table 123 ) are signi ( cid : 123 ) cantly lower than the median distances between pairs of au -
the topic distributions for di ( cid : 123 ) erent authors can also be used to assess the extent to which authors tend to address a single topic in their work , or cover multi - ple topics .
we calculated the entropy of each authors distribution over topics on the nips data , for di ( cid : 123 ) er - ent numbers of topics .
table 123 shows the 123 authors with the highest averaged entropy ( for 123 topics ) as well as the median and the minimum|also shown are the entropies for 123 and 123 topics .
the top - ranked author , michael jordan , is well known for producing nips papers on a variety of topics .
the papers associ - ated with the other authors are also relatively diverse , e . g . , for author terrence fine one of his papers is about forecasting demand for electric power while another concerns asymptotics of gradient - based learning .
the number of papers produced by an author is not neces - sarily a good predictor of topic entropy .
sejnowski t , for example , who generated the greatest number of pa - pers in our nips collection , 123 of the training papers , is the 123th highest entropy author , with an entropy of 123 : 123 for t = 123
the author - topic model proposed in this paper pro - vides a relatively simple probabilistic model for ex - ploring the relationships between authors , documents , topics , and words .
this model provides signi ( cid : 123 ) cantly improved predictive power in terms of perplexity com - pared to a more impoverished author model , where the interests of authors are directly modeled with proba - bility distributions over words .
when compared to the lda topic model , the author - topic model was shown to have more focused priors when relatively little is
known about a new document , but the lda model can better adapt its distribution over topics to the content of individual documents as more words are observed .
the primary bene ( cid : 123 ) t of the author - topic model is that it allows us to explicitly include authors in document models , providing a general framework for answering queries and making predictions at the level of authors as well as the level of documents .
possible future direc - tions for this work include using citation information to further couple documents in the model ( c . f .
cohn & hofmann , 123 ) , combining topic models with stylom - etry models for author identi ( cid : 123 ) cation , and applications such as automated reviewer list generation given sets of documents for review .
the research in this paper was supported in part by the national science foundation under grant iri - 123 via the knowledge discovery and dissemination ( kd - d ) program .
we would like to thank steve lawrence and c .
lee giles for kindly providing us with the cite - seer data used in this paper .
