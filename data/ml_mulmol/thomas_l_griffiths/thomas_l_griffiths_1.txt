one of the rst problems infants must solve as they are acquiring language is word segmentation : word boundaries in continuous speech .
about 123% of utter - ances directed at english - learning infants consist of iso - lated words ( brent & siskind , 123 ) , but there is no obvious way for children to know from the outset which utterances these are .
since multi - word utterances gener - ally have no apparent pauses between words , children must be using other cues to identify word boundaries .
in fact , there is evidence that infants use a wide range of weak cues for word segmentation .
these cues include phonotac -
* corresponding author .
tel . : +123 123 123 123
e - mail addresses : sgoldwat@inf . ed . ac . uk , sgwater@gmail . com ( s
123 - 123 / $ - see front matter 123 elsevier b . v .
all rights reserved .
tics ( mattys , jusczyk , luce , & morgan , 123 ) , allophonic variation ( jusczyk , hohne , & bauman , 123 ) , metrical ( stress ) patterns ( jusczyk , houston , & newsome , 123; morgan , bonamo , & travis , 123 ) , effects of coarticulation ( johnson & jusczyk , 123 ) , and statistical regularities in the sequences of syllables found in speech ( saffran , aslin , & newport , 123 ) .
this last source of information can be used in a language - independent way , and seems to be used by infants earlier than most other cues , by the age of 123 months ( thiessen & saffran , 123 ) .
these facts have caused some researchers to propose that strategies based on statistical sequencing information are a crucial rst step in bootstrapping word segmentation ( thiessen & saffran , 123 ) , and have provoked a great deal of interest in these strategies ( aslin , saffran , & newport , 123; saffran , new - port , & aslin , 123; saffran et al . , 123; toro , sinnett , & soto - faraco , 123 ) .
in this paper , we use computational
goldwater et al .
/ cognition 123 ( 123 ) 123
modeling techniques to examine some of the assumptions underlying much of the research on statistical word
most previous work on statistical word segmentation is based on the observation that transitions from one syl - lable or phoneme to the next tend to be less predictable at word boundaries than within words ( harris , 123; saf - fran et al . , 123 ) .
behavioral research has shown that in - fants are indeed sensitive to this kind of predictability , as measured by statistics such as transitional probabilities ( aslin et al . , 123; saffran et al . , 123 ) .
this research , however , is agnostic as to the mechanisms by which in - fants use statistical patterns to perform word segmenta - tion .
a number of researchers in both cognitive science and computer science have developed algorithms based on transitional probabilities , mutual similar statistics of predictability in order to clarify how these statistics can be used procedurally to identify words or word boundaries ( ando & lee , 123; cohen & adams , 123; feng , chen , deng , & zheng , 123; swingley , 123 ) .
here , we take a different approach : we seek to identify the assumptions the learner must make about the nature of language in order to correctly segment nat - ural language input .
observations about predictability at word boundaries are consistent with two different kinds of assumptions about what constitutes a word : either a word is a unit that is statistically independent of other units , or it is a unit that helps to predict other units ( but to a lesser degree than the beginning of a word predicts its end ) .
in most articial lan - guage experiments on word segmentation , assumption is adopted implicitly by creating stimuli through random ( or near - random ) concatenation of nonce words .
this kind of random concatenation is often neces - sary for controlled experiments with human subjects , and has been useful in demonstrating that humans are sensitive to the statistical regularities in such randomly generated sequences .
however , it obviously abstracts away from many of the complexities of natural language , where regularities exist not only in the relationships between sub - word units , but also in the relationships between words themselves .
we know that humans are able to use sub - word regularities to begin to extract words; it is natu - ral to ask whether attending to these kinds of regularities is sufcient for a statistical learner to succeed with word segmentation in a more naturalistic setting .
in this paper , we use computer simulations to examine learning from natural , rather than articial , language input .
we ask what kinds of words are identied by a learner who assumes that words are statistically independent , or ( alternatively ) by a learner who assumes as well that words are predictive of later words .
we investigate this question by developing two different bayesian models of word segmentation incorporating each of these two different assumptions .
these models can be seen as ideal learners : they are de - signed to behave optimally given the available input data , in this case a corpus of phonemically transcribed child - di -
using our ideal learning approach , we nd in our rst set of simulations that the learner who assumes that words are statistically independent units tends to undersegment
the corpus , identifying commonly co - occurring sequences of words as single words .
these results seem to conict with those of several earlier models ( batchelder , 123; brent , 123; venkataraman , 123 ) , where systematic undersegmentation was not found even when words were assumed to be independent .
however , we argue here that these previous results are misleading .
although each of these learners is based on a probabilistic model that de - nes an optimal solution to the segmentation problem , we provide both empirical and analytical evidence that the segmentations found by these learners are not the opti - mal ones .
rather , they are the result of limitations imposed by the particular learning algorithms employed .
further mathematical analysis shows that undersegmentation is the optimal solution to the learning problem for any rea - sonably dened model that assumes statistical indepen - dence between words .
moving on to our second set of simulations , we nd that permitting the learner to gather information about word - to - word dependencies greatly reduces the problem of undersegmentation .
the corpus is segmented in a much more accurate , adult - like way .
these results indi - cate that , for an ideal learner to identify words based on statistical patterns of phonemes or syllables , it is impor - tant to take into account that frequent predictable pat - terns may occur either within words or across words .
this kind of dual patterning is a result of the hierarchical structure of language , where predictable patterns occur at many different levels .
a learner who considers predict - ability at only one level ( sub - word units within words ) will be less successful than a learner who considers also the predictability of larger units ( words ) within their sen - tential context .
the second , more nuanced interpretation of the statistical patterns in the input leads to better
our work has important implications for the under - standing of human word segmentation .
we show that suc - assumptions that the learner makes about the nature of words .
these assumptions constrain the kinds of infer - ences that are made when the learner is presented with naturalistic input .
our ideal learning analysis allows us to examine the kinds of constraints that are needed to suc - cessfully identify words , and suggests that infants or young children may need to account for more subtle statistical ef - fects than have typically been discussed in the literature .
to date , there is little direct evidence that very young lan - guage learners approximate ideal learners .
nevertheless , this suggestion is not completely unfounded , given the accumulating evidence in favor of humans as ideal learners in other domains or at other ages ( frank , goldwater , mans - inghka , grifths , & tenenbaum , 123; schulz , bonawitz , & grifths , 123; xu & tenenbaum , 123 ) .
in order to further examine whether infants behave as ideal learners , or the ways in which they depart from the ideal , it is important to rst understand what behavior to expect from an ideal learner .
the theoretical results presented here provide a characterization of this behavior , and we hope that they will provide inspiration for future experimental work investigating the relationship between human learners and ideal learners .
goldwater et al .
/ cognition 123 ( 123 ) 123
the remainder of this paper is organized as follows .
first , we briey review the idea of transitional probabilities and how they relate to the notion of words , and provide some background on the probabilistic modeling approach taken here .
we draw a distinction between two kinds of probabilistic model - based systems those based on maxi - mum - likelihood and bayesian estimation and argue in fa - vor of the bayesian approach .
we discuss in some detail the strengths and weaknesses of the model - based dy - namic programming ( mbdp - 123 ) system , a bayesian learner described by brent ( 123 ) .
next , we introduce our own bayesian unigram model and learning algorithm , which address some of the weaknesses of mbdp - 123
we provide the results of simulations using this model and compare them to the results of previously proposed models .
we then generalize our unigram modeling results using addi - tional empirical and theoretical arguments , revealing some deep mathematical similarities between our unigram mod - el and mbdp - 123
finally , we extend our model to incorpo - rate bigram dependencies , present the results of this bigram model , and conclude by discussing the implications of our work .
words and transitional probabilities
the question of how infants begin to segment words from continuous speech has inspired a great deal of re - search over the years ( jusczyk , 123 ) .
while many differ - ent cues have been shown to be important , here we focus on one particular cue : statistical regularities in the se - quences of sounds that occur in natural language .
the idea that word and morpheme boundaries may be discov - ered through the use of statistical information is not new , but originally these methods were seen primarily as ana - lytic tools for linguists ( harris , 123 , 123 ) .
more re - cently , evidence that infants are sensitive to statistical dependencies between syllables has lent weight to the idea that this kind of information may actually be used by human learners for early word segmentation ( saffran et al . , 123; thiessen & saffran , 123 ) .
in particular , re - search on statistical word segmentation has focused on the notion of transitional probabilities between sub - word units ( e . g . , segments or syllables ) .
the transitional proba - bility from ( say ) syllable x to syllable y is simply the con - ditional probability of y given x .
in natural language , there is a general tendency towards lower transitional probabil - ities at word boundaries than within words ( harris , 123; saffran et al . , 123 ) , a tendency which infants seem able to exploit in order to segment word - like units from con - tinuous speech ( aslin et al . , 123; saffran et al . , 123 ) .
while other cues are also important for word segmenta - tion , and may in fact take precedence over transitional probabilities in older infants , transitional probabilities seem to be one of the earliest cues that infants are able to use for this task ( johnson & jusczyk , 123; thiessen & saffran , 123 ) .
much of the experimental work devoted to studying word segmentation and related linguistic tasks has fo - cused on exploring the kinds of statistical information that human learners are or are not sensitive to , e . g . , tran - sitional probabilities vs .
frequencies ( aslin et al . , 123 ) ,
syllables vs .
phonemes ( newport , weiss , aslin , & wonna - in preparation ) , adjacent vs .
non - adjacent depen - dencies ( newport & aslin , 123 ) , and the ways in which transitional probabilities interact with other kinds of cues ( johnson & jusczyk , 123; thiessen & saffran , 123 , 123 ) .
in addition , many researchers have explored the extent to which word segmentation based on transi - tional probabilities can be viewed as a special case of more general pattern - or sequence - learning mechanisms that operate over a range of cognitive domains ( creel , newport , & aslin , 123; fiser & aslin , 123 ) .
a question that has received less explicit attention is how the notion transitional probabilities relates to the notion of words .
transitional probabilities are a property of the boundaries between words ( or units within words ) , but ultimately it is the words themselves , rather than the boundaries , that are of interest to the language learner / it behooves us , then , to consider what possible properties of words ( or , more accurately , word se - quences ) could give rise to the patterns of transitional probabilities that are typically discussed in the literature , i . e .
lower probabilities at word boundaries and higher probabilities within words .
given a lexicon of words , one way that the standard patterns of transitional probabilities can arise is by choos - ing words independently at random from the lexicon and stringing them together to form a sequence .
to a rst approximation , this is the procedure that is typically used to generate stimuli for most of the experiments mentioned above . 123 there are many good reasons to generate experi - mental stimuli in this way , especially when the focus of re - search is on transitional probabilities : choosing words at random controls for possible ordering effects and other con - founds , and leads to simple and systematic patterns of tran - sitional probabilities .
however , there is clearly another way we could generate a sequence of words , by choosing each word conditioned on the previous word or words .
depend - ing on the strength of the word - to - word dependencies , tran - sitional probabilities between words may be low or high .
in general , if the strength of the dependencies between words is variable , then in a non - independent sequence of words , word boundaries will still tend to be associated with lower transitional probabilities ( since many pairs of words will not be highly dependent ) .
however , there will also be word boundaries with relatively high transitional probabilities ( where two words are highly associated , as in rubber ducky or thats a ) .
the models that we develop in this paper are designed to examine the results of making these two different assumptions about the nature of language : that words are statistically independent units , or that they are predic - tive units .
in thinking about the differences between learn - ers making each of these two kinds of assumptions , we frame the issue in terms of the space of linguistic hypoth -
123 the words in experimental stimuli are never chosen completely independently , due to restrictions against immediate repetition of words .
when the lexicon is small , this leads to signicant deviations from independence .
however , as the lexicon size grows , sequences without repetition will become more and more similar to truly independent
goldwater et al .
/ cognition 123 ( 123 ) 123
eses ( loosely , grammars ) that each learner considers .
no - tice that a learner who assumes that utterances are formed from sequences of independently chosen words is more re - stricted than a learner who assumes that words may pre - dict other words .
the second learner is able to learn grammars that describe either predictive or non - predictive sequences of words , while the rst learner can only learn grammars for non - predictive sequences of words .
if words are truly independent , then the rst learner may have an advantage due to the presence of the stronger constraint , because this learner has a much smaller space of hypothe - ses to consider .
on the other hand , the second learner will have an advantage in the case where words are not inde - pendent , because the learner who assumes independence will never be able to converge on the correct hypothesis .
before describing our implementation of these two kinds of learners , we rst outline our general approach and pro - vide a summary of related work .
probabilistic models for word segmentation
behavioral work in the vein of saffran et al .
( 123 ) has provided a wealth of information regarding the kinds of statistics human learners are sensitive to , at what ages , and to what degree relative to other kinds of segmentation cues .
computational modeling provides a complementary method of investigation that can be used to test specic hypotheses about how statistical information might be used procedurally to identify word boundaries or what underlying computational problem is being solved .
using the terminology of marr ( 123 ) , these two kinds of ques - tions can be investigated by developing models at ( respec - tively ) the algorithmic level or computational level of the acquisition system .
typically , researchers investigate algo - rithmic - level questions by implementing algorithms that are believed to incorporate cognitively plausible mecha - nisms of information processing .
algorithmic - level ap - proaches to word segmentation include a variety of neural network models ( allen et al . , 123; cairns & shill - cock , 123; christiansen , allen , & seidenberg , 123; elman , 123 ) as well as several learning algorithms based on tran - sitional probabilities , mutual information , and similar sta - tistics ( ando & lee , 123; cohen & adams , 123; feng et al . , 123; swingley , 123 ) ( with most of the latter group coming from the computer science literature ) .
in contrast to these proposals , our work provides a computational - level analysis of the word segmentation problem .
a computational - level approach focuses on identifying the problem facing the learner and determin - ing the logic through which it can be solved .
for problems of induction such as those facing the language learner , probability theory provides a natural developing computational - level models .
a probabilistic is a set of declarative mathematical statements specifying the goals of the learning process and the kinds of information that will be used to achieve them .
of course , these declarative statements must be paired with some algorithm that can be used to achieve the specic goal , but generally the algorithm is not seen as the focus of research .
rather , computational - level investigations of - ten take the form of ideal learner analyses , examining the
behavior of a learner who behaves optimally given the assumptions of the model . 123
very generally , we can view the goal of a language lear - ner as identifying some abstract representation of the ob - served data ( e . g . , a grammar ) that will allow novel linguistic input to be correctly interpreted , and novel out - put to be correctly produced .
many different representa - tions are logically possible , so the learner must have some way to determine which representation is most likely to be correct .
probabilistic models provide a natural way to make this determination , by creating a probability distribution over different hypothesized representations given the observed data .
a learner who is able to correctly identify this posterior distribution over hypotheses can use this information to process future input and output in an optimal way ( i . e . , in a way that is as similar as possible to the correct generating process the adult grammar ) .
under this view , then , the posterior distribution over grammars is the outcome of the learning process .
how does the learner go about identifying the posterior distribution ? bayes rule tells us that the probability of a hypothesized grammar h given the observed data d can be computed as
where the sum in the denominator ranges over all hypoth - eses h123 within the hypothesis space .
here , pdjh ( known as the likelihood ) is the probability of the observed data given a particular hypothesis , and tells us how well that hypoth - esis explains the data .
ph ( the prior probability of h ) tells us how good a linguistic hypothesis h is , regardless of any data .
the prior can be viewed as a learning bias or measure of linguistic naturalness : hypotheses with high prior prob - ability may be adopted based on less evidence than hypotheses with low prior probability .
bayes rule states that the posterior probability phjd is proportional to the product of the likelihood and the prior , with the denomina - tor acting as a normalizing constant to ensure that phjd sums to one over all hypotheses .
the learner can compute the posterior probabilities of different hypotheses by eval - uating each one according to its explanatory power ( likeli - hood ) and the learners prior expectations .
bayes rule answers the question of how to determine the posterior probability of a hypothesis if the prior prob - ability and likelihood are known , but it does not tell us how to compute those terms in the rst place .
we turn rst to the calculation of the likelihood .
typically , the likelihood is computed by dening a generative model : a probabilistic
123 a note on terminology : the word model unfortunately encompasses two related but ( importantly ) distinct senses .
it can be used to describe either ( 123 ) a proposal about the nature of learning or its implementation ( as in connectionist model , exemplar model ) ; or ( 123 ) a specic mathemat - ical statement regarding the process generating a set of data ( as in probabilistic model , generative model ) .
a probabilistic model ( second sense ) together with its learning algorithm can be viewed as an instance of a learning model ( rst sense ) .
to avoid confusion , we will generally use the term model only for the second sense , and the terms system or learner to describe the fully implemented combination of a probabilistic model and learning algorithm .
goldwater et al .
/ cognition 123 ( 123 ) 123
process for generating the observed data given the hypoth - esis under consideration .
as a simple non - linguistic exam - ple , imagine the data d consists of the results of 123 coin ips and we wish to determine the probability of d given hypotheses h that differ in the probability of ipping heads .
we assume a generative model in which each observation is the result of ipping the same coin , and that coin always has probability p of landing on heads independently of any previous outcomes .
the set of hypotheses under consider - ation consists of all possible values for p .
we therefore have pdjh p pnh123 pnt , where nh and nt are the number of heads and tails observed in a particular se - quence of ips .
maximum - likelihood estimation
a standard method of using a probabilistic generative model for learning is to perform maximum - likelihood esti - mation , i . e . , to choose the hypothesis ^h that maximizes the value of the likelihood function .
this is equivalent to choosing the hypothesis with maximum posterior proba - bility , assuming a uniform prior distribution over hypoth - eses with respect to the given parameterization .
in the coin ip example , it is easy to show using elementary cal - culus that the likelihood is maximized when h nh thus , if we observe a sequence consisting of four tails and six heads , the maximum - likelihood estimate for h is
for more complex generative models such as those typ - ically used in language modeling , it is usually impossible to identify the maximum - likelihood hypothesis analytically .
if the number of possible hypotheses is small , it may be feasible to explicitly compute the likelihood for each possi - ble hypothesis and choose the best one .
however , in gen - eral , it will be necessary to design some sort of algorithm for searching through the space of hypotheses without evaluating all of them .
the ideal algorithm would be one
that is guaranteed to nd the globally optimal hypothesis .
in many cases , however , approximate search algorithms are used .
these algorithms generally work by seeking the opti - mal hypothesis within some local region of the search space .
approximate search algorithms may be used for practical reasons ( when an exact procedure is not known , as in anderson ( 123 ) ) or for theoretical reasons ( if the re - searcher wishes to incorporate particular assumptions about human learning , as in sanborn , grifths , & navarro ( 123 ) ) .
in either case , certain hypotheses are excluded from consideration by the algorithm itself .
consequently , the use of an approximate search procedure can make a purely computational - level analysis difcult .
the kinds of generalizations made by the learner are determined both by the explicit constraints specied by the probabilistic model , and the implicit constraints specied by the search procedure .
examples of this type of learning system are de - scribed by venkataraman ( 123 ) and batchelder ( 123 ) .
the models underlying these systems are very similar; we describe only venkataramans work in detail .
venkataraman proposes a method of word segmenta - tion based on maximum - likelihood estimation .
he dis - cusses three different generative models of increasing complexity; we focus our analysis on the simplest of these , although our argument can be extended to all three .
this model is a standard unigram model , i . e . , it assumes that words are generated independently at observed data consists of a corpus of phonemically tran - scribed child - directed speech , where utterance boundaries ( corresponding to pauses in the input ) are known , but word boundaries are unknown ( see fig .
the probabilis - tic model underlying this system describes how to gener - ate a corpus given u , the number of utterances in the corpus; the distinguished symbol $ , which is used to mark utterance boundaries; and r , the phonemic symbol alpha - bet ( which does not include $ ) :
an excerpt from the beginning of the corpus used as input to venkataramans ( 123 ) word segmentation system , showing ( a ) the actual input corpus and ( b ) the corresponding standard orthographic transcription .
the corpus was originally prepared by brent and cartwright ( 123 ) using data from bernstein - ratner ( 123 ) , and was also used as input to brents ( 123 ) mbdp - 123 system .
goldwater et al .
/ cognition 123 ( 123 ) 123
repeat u times :
repeat until $ is generated : 123
generate the next word , w , with probability pww .
generate $ with probability p$ .
where pw is some probability distribution over r , the set of all possible words .
as each word is generated , it is con - catenated onto the previously generated sequence of words .
no boundary marker is added unless the end - of - utterance marker , $ , is generated .
under this model , the probability of generating words w123 .
wn as a single utter -
and the probability of generating the unsegmented utter - ance u is found by summing over all possible sequences of words that could be concatenated to form u :
the probability of the entire corpus is the product of the probabilities of the individual utterances .
the hypothesis space for this model consists of all the possible assign - ments of probability values to words and the utterance i . e . , possible values for p$ and the parameters of pw .
notice that in the hypothesis space just dened , some choices of pw may assign non - zero probability to only a - nite subset of potential words .
different hypotheses will have different sizes , i . e . , different numbers of words will have non - zero probability .
crucially , however , no prefer - ence is given to hypotheses of any particular size the maximum - likelihood assumption states that we should choose whichever hypothesis assigns the highest probabil - ity to the observed data .
what is the maximum - likelihood hypothesis under this model ? it is straightforward to show that , in general , the maximum - likelihood solution for a model is the probabil - ity distribution that is closest to the empirical distribution ( relative frequencies ) of observations in the corpus , where the distance is computed by an information theoretic measure called the kullbackleibler divergence ( bishop , 123 , p .
123 , inter al . ) .
in the above model , there is one hypothesis that is able to match the empirical distribution of the corpus exactly .
this hypothesis treats each utterance as a single word , with probability equal to the empirical probability of that utterance in the corpus , and assumes p$ 123
in other words , this solution memorizes the entire data set without segmenting utterances at all , and assigns zero probability to any unobserved utterance .
intuitively , any solution that does hypothesize word boundaries will require p$ < 123 , which means that some unobserved utter - ances will have non - zero probability those that can be created by , for example , concatenating two observed utter - ances , or rearranging the hypothesized words into novel
orderings .
since some probability mass is allocated to these unobserved utterances , the probability of the ob - served data must be lower than in the case where p$ 123 and no generalization is possible .
for a maximum - likelihood learner using the model in eq .
( 123 ) , then , only a trivial segmentation will be found un - less some constraint is placed on the kinds of hypotheses that are considered .
crucially , however , this argument does not depend on the particular form of pw used in eq .
( 123 ) , where words are assumed to be generated indepen - dent of context .
many other possible distributions over words would yield the same result .
venkataraman , for example , presents two other models in which the unigram distribution pw is replaced with a bigram or trigram distri - bution : rather than generating words independent of con - text , each word is generated conditioned on either one or two previous words .
that is , the bigram model denes
wn$ pw123j$
essentially , the reason that all of these models yield the same maximum - likelihood solution ( an unsegmented cor - pus ) is that they are allowed to consider hypotheses with arbitrary numbers of items .
when comparing hypotheses with different levels of complexity ( corre - sponding here to the number of word types in the hypoth - esis ) , a maximum - likelihood learner will generally prefer a more complex hypothesis over a simple one .
this leads to the problem of overtting , where the learner chooses a hypothesis that ts the observed data very well , but gener - alizes very poorly to new data .
in the case of word segmen - tation , the solution of complete memorization allows the learner to t the observed data perfectly .
since we know that this is not the solution found by venkataramans learners , we must conclude that the algorithm he proposes to search the space of possible hypotheses must be impos - ing additional constraints beyond those of the models themselves .
it should be clear from the previous discussion that this is not the approach advocated here , since it ren - ders constraints implicit and difcult to examine .
batchel - ders ( 123 ) maximum - likelihood learning system uses an explicit external constraint to penalize lexical items that are too long .
this approach is a step in the right direction , but is less mathematically principled than bayesian model - in which a ( non - uniform ) prior distribution over hypotheses is used within the model itself to constrain learning .
we now review several of the bayesian models that served as inspiration for our own work .
bayesian models
in the previous section , we argued that unconstrained maximum - likelihood estimation is a poor way to choose between hypotheses with different complexities .
in bayes - ian modeling , the effect of the likelihood can be counter - balanced by choosing a prior distribution that favors simpler hypotheses .
simpler hypotheses will tend not to t the observed data as well , but will tend to generalize more successfully to novel data .
by considering both the likelihood and prior in determining the posterior probabil - ity of each hypothesis , bayesian learners naturally avoid
goldwater et al .
/ cognition 123 ( 123 ) 123
the kind of overtting that maximum - likelihood learners encounter .
the trade - off between t and generalization will depend on exactly how the prior is dened; we now describe several methods that have been used to dene priors in previous bayesian models .
perhaps the most well - known framework for dening bayesian models is known as minimum description length ( mdl ) ( rissanen , 123 ) , and is exemplied by the work of de marcken ( 123 ) and brent and cartwright ( 123 ) .
mdl is a particular formulation of bayesian learning that has been used successfully in a number of other areas of language acquisition as well ( creutz & lagus , 123; dow - man , 123; ellison , 123; goldsmith , 123; goldwater & johnson , 123 ) .
the basic idea behind mdl is to dene some encoding scheme that can be used to encode the cor - pus into a more compact representation .
in word segmen - tation , for example , a code might consist of a list of lexical items along with a binary representation for each one .
with appropriate choices for the lexical items and binary representations ( with shorter representations assigned to more common words ) , the length of the corpus could be reduced by replacing each word with its binary code .
in this framework , the learners hypotheses are different pos - sible encoding schemes .
the minimum description length principle states that the optimal hypothesis is the one that minimizes the combined length , in bits , of the hypothesis it - self ( the codebook ) and the encoded corpus .
using results from information theory , it can be shown that choosing a hypothesis using the mdl principle is equivalent to choos - ing the maximum a posteriori ( map ) hypothesis the hypothesis with the highest posterior probability under a bayesian model where the prior probability of a hypoth - esis decreases exponentially with its length .
words , mdl corresponds to a particular choice of prior dis - tribution over hypotheses , where hypotheses are preferred if they can be described more succinctly .
although mdl models can in principle produce good word segmentation results , there are no standard search algorithms for these kinds of models , and it is often dif - cult to design efcient model - specic algorithms .
for example , brent and cartwright ( 123 ) were forced to limit their analysis to a very short corpus ( about 123 utterances ) due to efciency concerns .
in later research , brent devel - oped another bayesian model for word segmentation with a more efcient search algorithm ( brent , 123 ) .
he named this system model - based dynamic programming ( mbdp - 123 ) . 123 since we will be returning to this model at various points throughout this paper , we now describe mbdp - 123 in
unlike models developed within the mdl framework , where hypotheses correspond to possible encoding meth - ods , mbdp - 123 assumes that the hypotheses under consider - ation are actual sequences of words , where each word is a sequence of phonemic symbols .
the input corpus consists of phonemically transcribed utterances of child - directed speech , as in fig .
some word sequences , when concate - nated together to remove word boundaries , will form ex -
123 the 123 in mbdp - 123 was intended as a version number , although brent
never developed any later versions of the system .
actly the string of symbols found in the corpus , while others will not .
the probability of the observed data given a particular hypothesized sequence of words will therefore either be equal to 123 ( if the concatenated words form the corpus ) or 123 ( if not ) .
consequently , only hypotheses that are consistent with the corpus must be considered .
for each possible segmentation of the corpus , the posterior probability of that segmentation will be directly propor - tional to its prior probability .
the prior probability , in turn , is computed using a generative model .
this model assumes that the sequence of words in the corpus was created in a sequence of four steps : 123
step 123 : generate the number of types that will be in the
step 123 : generate a token frequency for each lexical type .
step 123 : generate the phonemic representation of each type ( except for the single distinguished utter - ance boundary type , $ ) .
step 123 : generate an ordering for the set of tokens .
each step in this process is associated with a probability distribution over the possible outcomes of that step , so together these four steps dene the prior probability distri - bution over all possible segmented corpora .
we discuss the specic distributions used in each step in appendix b; here it is sufcient to note that these distributions tend to assign higher probability to segmentations containing fewer and shorter lexical items , so that the learner will prefer to split utterances into words .
to search the space of possible segmentations of the corpus , brent develops an efcient online algorithm .
the algorithm makes a single pass through the corpus , seg - menting one utterance at a time based on the segmenta - tions found for all previous utterances .
the online nature of this algorithm is intended to provide a more realistic simulation of human word segmentation than earlier batch learning algorithms ( brent & cartwright , 123; de marc - ken , 123 ) , which assume that the entire corpus of data is available to the learner at once ( i . e . , the learner may iter - ate over the data many times ) .
in the remainder of this paper , we will describe two new bayesian models of word segmentation inspired , in part , by brents work .
like brent , we use a generative mod - el - based bayesian framework to develop our learners .
moreover , as we prove in appendix b , our rst ( unigram ) model is mathematically very similar to the mbdp - 123 mod - el .
however , our work differs from brents in two respects .
first , our models are more exible , which allows us to more easily investigate the effects of different modeling assumptions .
in theory , each step of brents model can be individually modied , but in practice the mathematical statement of the model and the approximations necessary for the search procedure make it difcult to modify the model in any interesting way .
in particular , the fourth step assumes a uniform distribution over orderings , which
123 our presentation involves a small change from brent ( 123 ) , switching the order of steps 123 and 123
this change makes no difference to the model , but provides a more natural grouping of steps for purposes of our analysis in appendix b .
goldwater et al .
/ cognition 123 ( 123 ) 123
creates a unigram constraint that cannot easily be changed .
we do not suppose that brent was theoretically motivated in his choice of a unigram model , or that he would be op - posed to introducing word - to - word dependencies , merely that the modeling choices available to him were limited by the statistical techniques available at the time of his work .
in this paper , we make use of more exible recent techniques that allow us to develop both unigram and bi - gram models of word segmentation and explore the differ - ences in learning that result .
the second key contribution of this paper lies in our fo - cus on analyzing the problem of word segmentation at the computational level by ensuring , to the best of our ability , that the only constraints on the learner are those imposed by the model itself .
we have already shown that the mod - el - based approaches of venkataraman ( 123 ) and batchel - der ( 123 ) are constrained by their choice of search algorithms; in the following section we demonstrate that the approximate search procedure used by brent ( 123 ) prevents his learner , too , from identifying the optimal solution under his model .
although in principle one could develop a bayesian model within the mdl or mbdp frame - works that could account for word - to - word dependencies , the associated search procedures would undoubtedly be even more complex than those required for the current unigram models , and thus even less likely to identify opti - mal solutions .
because our own work is based on more re - cent bayesian techniques , we are able to develop search procedures using a standard class of algorithms known as markov chain monte carlo methods ( gilks , richardson , & spiegelhalter , 123 ) , which produce samples from the pos - terior distribution over hypotheses .
we provide evidence that the solutions identied by our algorithms are indeed optimal or near - optimal , which allows us to draw conclu - sions using ideal observer arguments and to avoid the obfuscating effects of ad hoc search procedures .
unigram model
generative model
like mbdp - 123 , our models assume that the hypotheses under consideration by the learner are possible segmenta - tions of the corpus into sequences of words .
word se - quences that are consistent with the corpus have a likelihood of 123 , while others have a likelihood of 123 , so the posterior probability of a segmentation is determined by its prior probability .
also as in mbdp - 123 , we compute the prior probability of a segmentation by assuming that the sequence of words in the segmentation was created according to a particular probabilistic generative process .
let w w123 .
. w n be the words in the segmentation .
set - ting aside the complicating factor of utterance boundaries , our unigram model assumes that the ith word in the se - quence , wi , is generated as follows :
( 123 ) decide if wi is a novel lexical item .
. x m ) for wi .
if so , generate a phonemic form ( phonemes
if not , choose an existing lexical form for wi .
we assign probabilities to each possible choice as follows :
, pwi is not novel n ( 123 ) pwi is novel a123 ( 123 ) a .
pwi x123 .
. x m j wi is novel b .
pwi j wi is not novel n
where a123 is a parameter of the model , n is the number of previously generated words ( i 123 ) , n is the number of times lexical item has occurred in those n words , and p# is the probability of generating a word boundary .
taken together , these denitions yield the following distribution over wi given the previous words wi fw123 .
. w i123g :
i 123 a123
i 123 a123
where we use p123 to refer to the unigram phoneme distribu - tion in step 123a .
( the p# and 123 p# factors in this distribu - tion result from the process used to generate a word from constituent phonemes : after each phoneme is generated , a word boundary is generated with probability p# and the process ends , or else no word boundary is generated with probability 123 p# and another phoneme is generated . )
we now provide some intuition for the assumptions that are built into this model .
first , notice that in step 123 , when n is small , the probability of generating a novel lexical item is relatively large .
as more word tokens are generated and n increases , the relative probability of generating a novel item decreases , but never disappears entirely .
this part of the model means that segmentations with too many different lexical items will have low proba - bility , providing pressure for the learner to identify a seg - mentation consisting of relatively few lexical items .
in step 123a , we dene the probability of a novel lexical item as the product of the probabilities of each of its phonemes .
this ensures that very long lexical items will be strongly dispreferred .
finally , in step 123b , we say that the probability of generating an instance of the lexical item is propor - tional to the number of times has already occurred .
in ef - fect , the learner assumes that a few lexical items will tend to occur very frequently , while most will occur only once or twice .
in particular , the distribution over word frequen - cies produced by our model becomes a power - law distri - bution for large corpora ( arratia , barbour , & tavare , 123 ) , the kind of distribution that is found in natural lan - guage ( zipf , 123 ) .
the model we have just described is an instance of a kind of model known in the statistical literature as a dirich - let process ( ferguson , 123 ) .
the dirichlet process is com - monly used in bayesian statistics as a non - parametric prior for clustering models , and is closely related to ander - sons ( 123 ) rational model of categorization ( sanborn et al . , 123 ) .
the dirichlet process has two parameters : the concentration parameter a123 and the base distribution p123
the concentration parameter determines how many clusters will typically be found in a data set of a particular size ( here , how many word types for a particular number of tokens ) , and the base distribution determines the typical characteristics of a cluster ( here , the particular phonemes in a word type ) .
a more detailed mathematical treatment
goldwater et al .
/ cognition 123 ( 123 ) 123
of our model and its relationship to the dirichlet process is provided in appendix a , but this connection leads us to re - fer to our unigram model of word segmentation as the dirichlet process ( dp ) model .
so far , the model we have described assigns probabili - ties to sequences of words where there are no utterance boundaries .
however , because the input corpus contains utterance boundaries , we need to extend the model to ac - count for them .
in the extended model , each hypothesis consists of a sequence of words and utterance boundaries , and hypotheses are consistent with the input if removing word boundaries ( but not utterance boundaries ) yields the input corpus .
to compute the probability of a sequence of words and utterance boundaries , we assume that this sequence was generated using the model above , with the addition of an extra step : after each word is generated , an utterance boundary marker $ is generated with proba - bility p$ ( or not , with probability 123 p$ ) .
for simplicity , we will suppress this portion of the model in the main body of this paper , and refer the reader to appendix a for
we have now dened a generative model that allows us to compute the probability of any segmentation of the input corpus .
we are left with the problem of inference , or actually identifying the highest probability segmenta - tion from among all possibilities .
we used a method known as gibbs sampling ( geman & geman , 123 ) , a type of markov chain monte carlo algorithm ( gilks et al . , 123 ) in which variables are repeatedly sampled from their conditional posterior distribution given the current values of all other variables in the model .
gibbs sampling is an iterative procedure in which ( after a number of iterations used as a burn - in period to allow the sam - pler to converge ) each successive iteration produces a sample from the full posterior distribution phjd .
our sampler , the variables of interest are potential word boundaries , each of which can take on two possible val - ues , corresponding to a word boundary or no word boundary .
boundaries may be initialized at random or using any other method; initialization does not matter since the sampler will eventually converge to sampling from the posterior distribution . 123 each iteration of the sampler consists of stepping through every possible boundary location and resampling its value conditioned on all other current boundary placements .
since each set of assignments to the boundary variables uniquely deter - mines a segmentation , sampling boundaries is equivalent to sampling sequences of words as our hypotheses .
although gibbs sampling is a batch learning algorithm , where the entire data set is available to the learner at once , we note that there are other sampling techniques known as particle lters ( doucet , andrieu , & godsill , 123; sanborn et al . , 123 ) that can be used to produce
123 of course , our point that initialization does not matter is a theoretical one; in practice , some initializations may lead to faster convergence than others , and checking that different initializations lead to the same results is one way of testing for convergence of the sampler , as we do in appendix a .
approximations of the posterior distribution in an online fashion ( examining each utterance in turn exactly once ) .
we return in the general discussion to the question of how a particle lter might be developed for our own model in the future .
full details of our gibbs sampling algorithm are provided in appendix a .
to facilitate comparison to previous models of word segmentation , we report results on the same corpus used by brent ( 123 ) and venkataraman ( 123 ) .
the data is de - rived from the bernsteinratner corpus ( bernstein - rat - ner , 123 ) of the childes database ( macwhinney & snow , 123 ) , which contains orthographic transcriptions of utterances directed at 123 - to 123 - month - olds .
the data was post - processed by brent , who removed disuencies and non - words , discarded parental utterances not direc - ted at the children , and converted the rest of the words into a phonemic representation using a phonemic dictio - nary ( i . e .
each orthographic form was always given the same phonemic form ) .
the resulting corpus contains 123 utterances , with 123 , 123 word tokens and 123 un - ique types .
the average number of words per utterance is 123 and the average word length ( in phonemes ) is 123 .
the word boundaries in the corpus are used as the gold standard for evaluation , but are not provided in the input to the system ( except for word boundaries that are also utterance boundaries ) .
the process used to create this corpus means that it is missing many of the complexities of real child - directed speech .
not the least of these is the acoustic variability with which different tokens of the same word are pro - duced , a factor which presumably makes word segmenta - tion more difcult .
on the other hand , the corpus is also missing many cues which could aid in segmentation , such as coarticulation information , stress , and duration .
while this idealization of child - directed speech is somewhat unrealistic , the corpus does provide a way to investigate the use of purely distributional cues for segmentation , and permits direct comparison to other word segmenta -
evaluation procedure
for quantitative evaluation , we adopt the same mea - sures used by brent ( 123 ) and venkataraman ( 123 ) : pre - cision ( number of correct items found out of all items found ) and recall ( number of correct items found out of all correct items ) .
these measures are widespread in the computational linguistics community; the same measures are often known as accuracy and completeness in the cogni - tive science community ( brent & cartwright , 123; chris - tiansen et al . , 123 ) .
we also report results in terms of f123 ( another common metric used in computational lin - guistics , also known as f - measure or f - score ) .
f123 is the geo - metric average of precision and recall , dened as , and penalizes results where precision and recall are very different .
we report the following scores for each model we propose :
goldwater et al .
/ cognition 123 ( 123 ) 123
aries must be correctly identied to count as correct .
p , r , f : precision , recall , and f123 on words : both bound - lp , lr , lf : precision , recall , and f123 on the lexicon , i . e .
bp , br , bf : precision , recall , and f123 on potentially ambig - uous boundaries ( i . e .
utterance boundaries are not included in the counts ) .
as an example , imagine a one - utterance corpus whose cor - rect segmentation is look at the big dog there , where instead we nd the segmentation look at the bigdo g the re .
there are seven words in the found segmentation , and six in the true segmentation; three of these words match .
we report all scores as percentages , so p = 123% ( 123 / 123 ) , r = 123% ( 123 / 123 ) , and f = 123% .
similarly , bp = 123% ( 123 / 123 ) , br = 123% ( 123 / 123 ) , bf = 123% , lp = 123% ( 123 / 123 ) , lr = 123% ( 123 / 123 ) , and lf = 123% .
note that if the learner correctly identies all of the boundaries in the true solu - tion , but also proposes extra boundaries ( oversegmenta - tion ) , then boundary recall will reach 123% , but boundary precision and boundary f123 will be lower .
conversely , if the learner proposes no incorrect boundaries , but fails to identify all of the true boundaries ( undersegmentation ) , then boundary precision will be 123% , but boundary recall and f123 will be lower .
in either case , scores for word tokens and lexical items will be below 123% .
for comparison , we report scores as well for brents mbdp - 123 system ( brent , 123 ) and venkataramans n - gram segmentation systems ( venkataraman , 123 ) , which we will refer to as ngs - u and ngs - b ( for the unigram and bi - gram models ) .
both brent and venkataraman use online search procedures ( i . e . , their systems make a single pass through the data , segmenting each utterance in turn ) , so in their papers they calculate precision and recall sepa - rately on each 123 - utterance block of the corpus and graph the results to show how scores change as more data is pro - cessed .
they do not report lexicon recall or boundary pre - cision and recall .
their results are rather noisy , but performance seems to stabilize rapidly , after about 123 utterances .
to facilitate comparison with our own results , we calculated scores for mbdp - 123 and ngs over the whole corpus , using venkataramans implementations of these
since our algorithm produces random segmentations sampled from the posterior distribution rather than a sin - gle optimal solution , there are several possible ways to evaluate its performance .
for most of our simulations , we evaluated a single sample taken after 123 , 123 iterations .
we used a method known as simulated annealing ( aarts & korst , 123 ) to speed convergence of the sampler , and in some cases ( noted below ) to obtain an approximation of the map solution by concentrating samples around the mode of the posterior .
this allowed us to examine possible differences between a random sample of the posterior and a sample more closely approximating the map segmenta - tion .
details of the annealing and map approximation pro - cedures can be found in appendix a .
123 the implementations are available at http : / / www . speech . sri . com /
( a ) varying p# with
123 = 123
value of p#
123 with p# = . 123
f123 for words ( f ) and lexical items ( lf ) in the dp model ( a ) as a function of p# , with a123 123 and ( b ) as a function of a123 , with p# : 123
results and discussion
the dp model we have described has two free parame - ters : p# ( the prior probability of a word boundary ) , and a123 ( which affects the number of word types proposed ) . 123 fig .
123 shows the effects of varying of p# and a123
lower values of p# result in more long words , which tends to improve recall ( and thus f123 ) in the lexicon .
the accompanying decrease in token accuracy is due to an increasing tendency for the mod - el to concatenate short words together , a phenomenon we discuss further below .
higher values of a123 allow more novel words , which also improves lexicon recall , but begins to de - grade precision after a point .
due to the negative correlation between token accuracy and lexicon accuracy , there is no single best value for either p# or a123
in the remainder of this section , we focus on the results for p# : 123; a123 123 ( though others are qualitatively similar we discuss these briey
in table 123 , we compare the results of our system to those of mbdp - 123 and ngs - u .
although our system has higher lexicon accuracy than the others , its token accuracy is much worse .
performance does not vary a great deal be - tween different samples , since calculating the score for a single sample already involves averaging over many ran - dom choices the choices of whether to place a boundary at each location or not .
table 123 shows the mean and stan - dard deviation in f123 scores and posterior probabilities over samples taken from 123 independent runs of the algorithm with different random initializations .
the same statistics
123 the dp model actually contains a third free parameter , q , used as a prior over the probability of an utterance boundary ( see appendix a ) .
given the large number of known utterance boundaries , the value of q should have little effect on results , so we simply xed q 123 for all simulations .
goldwater et al .
/ cognition 123 ( 123 ) 123
word segmentation accuracy of unigram systems .
note : p , r , and f are precision , recall , and f123 for word tokens; bp , lp , etc .
are the corresponding scores for ambiguous boundaries and lexical items .
best scores are shown in bold .
dp results are with p# : 123 and a123 123
results of the dp model , averaged over multiple samples .
samples ( 123 runs ) samples ( 123 run )
note : token f123 ( f ) , lexicon f123 ( lf ) , and negative log posterior probability were averaged over 123 samples from independent runs of our gibbs sampler , over 123 samples from a single run , and over 123 samples from independent runs of our map approximation ( see text ) .
standard devia - tions are shown in parentheses .
are also provided for ten samples obtained from a single run of the sampler .
samples from a single run are not inde - pendent , so to reduce the amount of correlation between these samples they were taken at 123 - iteration intervals ( at iterations 123 , 123 , .
. , 123 ) .
nevertheless , they show less variability than the truly independent samples .
in both cases , lexicon accuracy is more variable than token accuracy , probably because there are far fewer lexical items to average over within a single sample .
finally , table 123 provides results for the approximate map evaluation procedure .
this procedure is clearly imperfect , since if it were able to identify the true map solution , there would be no difference in results across multiple runs of the algo - rithm .
in fact , compared to the standard sampling proce - dure , there is only slightly less variation in f123 scores , and greater variation in probability . 123 nevertheless , the map approximation does succeed in nding solutions with signif - icantly higher probabilities .
these solutions also have higher lexicon accuracy , although token accuracy remains low .
the reason that token accuracy is so low with the dp model is that it often mis - analyzes frequently occurring words .
many instances of these words occur in common collocations such as whats that and do you , which the sys - tem interprets as a single words .
this pattern of errors is apparent in the boundary scores : boundary precision is very high , indicating that when the system proposes a boundary , it is almost always correct .
boundary recall is low , indicating undersegmentation .
we analyzed the behavior of the system more carefully by examining the segmented corpus and lexicon .
a full 123% of the proposed lexicon and nearly 123% of tokens consist of
123 the large standard deviation in the probabilities of the approximate map solutions is due to a single outlier .
the standard deviation among the remaining nine solutions is 123 , well below the standard deviation in the sample solutions , where there are no outliers .
undersegmentation ( collocation ) errors , while only 123% of types and 123% of tokens are other non - words .
( some addi - tional token errors , under 123% , are caused by proposing a correct word in an incorrect location . ) about 123% of collo - cations ( both types and tokens ) are composed of two words , nearly all the rest are three words .
to illustrate the phenomenon , we provide the systems segmentation of the rst 123 utterances in the corpus in fig .
123 , and the 123 most frequently found lexical items in fig .
the 123 most frequent collocations identied as single words by the system are shown in fig
it is interesting to examine the collocations listed in fig .
123 with reference to the existing literature on childrens early representation of words .
peters ( 123 ) , in particular , provides a number of examples of childrens underseg - mentation errors ( using their productions as evidence ) .
several of the full sentences and social conventions in fig .
123 ( e . g . , thank you , thats right , bye bye , look at this ) are included among her examples .
in addition , some of the other collocation errors found by our unigram system match the examples of formulaic frames given by peters : the verb introducer can you and noun introducers its a , this is , those are , and see the .
phonological reductions in adult speech also suggest that a few of the collocations found by the system ( e . g . , did you , what do you ) may even be treated as single units by adults in some circumstances .
however , the extent and variety of collocations found by the system is certainly much broader than what research - ers have so far found evidence for in young children .
we will defer for the moment any further discussion of whether childrens early word representations are similar to those found by our dp model ( we return to this issue in the general discussion ) , and instead turn to the question of why these units are found .
the answer seems clear : groups of words that frequently co - occur violate the uni - gram assumption in the model , since they exhibit strong word - to - word dependencies .
the only way the learner can capture these dependencies is by assuming that these collocations are in fact words themselves .
as an example , consider the word that .
in our corpus , the empirical proba - bility of the word that is 123 / 123 . 123
however , the empirical probability of that following the word whats is far higher : 123 / 123 . 123
since the strong correlation be - tween whats and that violates the independence assump - tion of the model , the learner concludes that whatsthat must be a single word .
note that by changing the values of the parameters a123 and p# , it is possible to reduce the level of undersegmenta - tion , but only slightly , and at the cost of introducing other
goldwater et al .
/ cognition 123 ( 123 ) 123
the rst 123 utterances in the corpus as segmented by the dp model ( represented orthographically for readability ) , illustrating that the model undersegments the corpus .
the stochastic nature of the gibbs sampling procedure is apparent : some sequences , such as youwantto and getit , receive two
errors .
for example , raising the value of p# to 123 strongly increases the models preference for short lexical items , but collocations still make up 123% of both types and tokens in this case .
measures of token accuracy increase by a few points , but are still well below those of previous systems .
the main qualitative difference between results with p# : 123 and p# : 123 is that with the higher value , infre - quent words are more likely to be oversegmented into very short one - or two - phoneme chunks ( reected in a drop in lexicon accuracy ) .
however , frequent words still tend to be undersegmented as before .
it is also worth noting that , although the proportion of collocations in the lexicons found by mbdp - 123 and ngs - u is comparable to the proportion found by our own model ( 123% ) , only 123% of tokens found by these systems are collo - cation errors .
this fact seems to contradict our analysis of the failures of our own unigram model , and raises a num - ber of questions .
why dont these other unigram models exhibit the same problems as our own ? is there some other weakness in our model that might be causing or com - pounding the problems with undersegmentation ? is it pos - sible to design a successful unigram model for word segmentation ? we address these questions in the follow -
other unigram models
mbdp - 123 and search
in the previous section , we showed that the optimal segmentation under our unigram model is one that identi - es common collocations as individual words .
our earlier
discussion of venkataramans ( 123 ) ngs models demon - strated that the optimal solution under those models is a completely unsegmented corpus .
what about brent ( 123 ) mbdp - 123 model ? while the denition of this uni - gram model makes it difcult to determine what the opti - mal solution is , our main concern was whether it exhibits the same problems with undersegmentation as our own unigram model .
the results presented by brent do not indi - cate undersegmentation , but it turns out that these results , like venkataramans , are inuenced by the approximate search procedure used .
we determined this by calculating the probability of various segmentations of the corpus un - der each model , as shown in table 123
the results indicate that the mbdp - 123 model assigns higher probability to the solution found by our gibbs sampler than to the solution found by brents own incremental search algorithm .
in other words , the model underlying mbdp - 123 does favor the lower - accuracy collocation solution , but brents approximate search algorithm nds a different solution that has higher accuracy but lower probability under the
we performed two simulations suggesting that our own inference procedure does not suffer from similar problems .
first , we initialized the gibbs sampler in three different ways : with no utterance - internal boundaries , with a boundary after every character , and with random bound - aries .
the results were virtually the same regardless of ini - tialization ( see appendix a for details ) .
second , we created an articial corpus by randomly permuting all the words in the true corpus and arranging them into utterances with the same number of words as in the true corpus .
this manipulation creates a corpus where the unigram assump -
goldwater et al .
/ cognition 123 ( 123 ) 123
responsible for the poor segmentation performance on the natural language corpus .
in particular , the unigram assumption of the model seems to be at fault .
in the fol - lowing section we present some additional simulations de - signed to further test this hypothesis .
in these simulations , we change the model of lexical items used in step 123a of the model , which has so far assumed that lexical items are cre - ated by choosing phonemes independently at random .
if the original poor lexical model is responsible for the dp models undersegmentation of the corpus , then improving the lexical model should improve performance .
however , if the problem is that the unigram assumption fails to ac - count for sequential dependencies in the corpus , then a better lexical model will not make much difference .
the impact of the lexical model on word segmentation
one possible improvement to the lexical model is to re - place the assumption of a uniform distribution over pho - nemes with the more realistic assumption that phonemes have different probabilities of occurrence .
this assumption is more in line with the mbdp - 123 and ngs models .
in ngs , phoneme probabilities are estimated online according to their empirical distribution in the corpus .
in mbdp - 123 , pho - neme probabilities are also estimated online , but according to their empirical distribution in the current lexicon .
for models like mbdp - 123 and the dp model , where the pho - neme distribution is used to generate lexicon items rather than word tokens , the latter approach makes more sense .
it is relatively straightforward to extend the dp model to in - fer the phoneme distribution in the lexicon simultaneously with inferring the lexicon itself .
before implementing this extension , however , we tried simply xing the phoneme distribution to the empirical distribution in the true lexi - con .
this procedure gives an upper bound on the perfor - mance that could be expected if the distribution were learned .
we found that this change improved lexicon f123 somewhat ( to 123 , with a123 123 and p# : 123 ) , but made al - most no difference on token f123 ( 123 ) .
inference of the pho - neme distribution was therefore not implemented .
the 123 most frequent items in the lexicon found by the dp model ( left ) and in the correct lexicon ( right ) .
except for the phonemes z and s , lexical items are represented orthographically for readability .
different possible spellings of a single phonemic form are separated by slashes .
the frequency of each lexical item is shown to its left .
items in the segmented lexicon are indicated as correct ( + ) or incorrect ( ) .
frequencies of correct items in the segmented lexicon are lower than in the true lexicon because many occurrences of these items are accounted for by collocations .
tion is correct .
if our inference procedure works properly , the unigram system should be able to correctly identify the words in the permuted corpus .
this is exactly what we found , as shown in table 123
the performance of the dp model jumps dramatically , and most errors occur on infrequent words ( as evidenced by the fact that token accuracy is much higher than lexicon accuracy ) .
in con - trast , mbdp - 123 and ngs - u receive a much smaller benet from the permuted corpus , again indicating the inuence
these results imply that the dp model itself , rather than the gibbs sampling procedure we used for inference , is
other changes could be made to the lexical model in or - der to create a better model of word shapes .
for example , using a bigram or trigram phoneme model would allow the learner to acquire some notion of phonotactics .
basing the model on syllables rather than phonemes could incor - porate constraints on the presence of vowels or syllable weight .
rather than testing all these different possibilities , we designed a simulation to determine an approximate upper bound on performance in the unigram dp model .
in this simulation , we provided the model with informa - tion that no infant would actually have access to : the set of word types that occur in the correctly segmented cor - pus .
the lexical model is dened as follows :
ptruewi 123 123
where l is the true set of lexical items in the data , and is some small mixing constant .
in other words , this model is a mixture between a uniform distribution over the true lex - ical items and the basic model p123
if 123 , the model is
goldwater et al .
/ cognition 123 ( 123 ) 123
the 123 most frequently occurring items in the segmented lexicon that consist of multiple words from the true lexicon .
these items are all identied as single words; the true word boundaries have been inserted for readability .
the frequency of each item is shown to its left .
negative log probabilities of various segmentations under each unigram
accuracy of the various systems on the permuted corpus .
note : row headings identify the models used to evaluate each segmen - tation .
column headings identify the different segmentations : the true segmentation , the segmentation with no utterance - internal boundaries , and the segmentation found by each system .
actual log probabilities are 123 those shown .
constrained so that segmentations may only contain words from the true lexicon .
if > 123 , a small amount of noise is introduced so that new lexical items are possible , but have much lower probability than the true lexical items .
if the model still postulates collocations when is very small , we have evidence that the unigram assumption , rather
note : p , r , and f are precision , recall , and f123 for word tokens; bp , lp , etc .
are the corresponding scores for ambiguous boundaries and lexical items .
best scores are shown in bold .
dp results are with p# : 123 and a123 123
than any failure in the lexicon model , is responsible for
the results from this model are shown in table 123
not surprisingly , the lexicon f123 scores in this model are very high , and there is a large improvement in token f123 scores against previous models .
however , amount of information provided to the model , its scores are still surprisingly low , and collocations remain a prob - lem , especially for frequent items .
goldwater et al .
/ cognition 123 ( 123 ) 123
results of the dp model using ptrue .
note : shown , for each value of , is token f123 ( f ) , lexicon f123 ( lf ) , and the percentage of tokens and lexical items that are multi - word collocations .
considering the case where 123 yields some in - sight into the performance of these models with improved lexical models .
the solution found , with a lexicon consist - ing of 123% collocations , has higher probability than the true solution .
this is despite the fact that the most proba - ble incorrect lexical items are about ve orders of magni - tude less probable than the true lexical incorrect lexical items are proposed despite their extremely low probability because only the rst occurrence of each word is accounted for by the lexical model .
subsequent occurrences are accounted for by the part of the model that generates repeated words , where probabilities are propor - tional to the number of previous occurrences .
therefore , low - probability lexical items incur no penalty ( beyond that of any other word ) after the rst occurrence .
this is why the collocations remaining in the dp model using ptrue are the highest - frequency collocations : over many occurrences , the probability mass gained by modeling these collocations as single words outweighs the mass lost in generating the
the results of this simulation suggest that the many of collocations found by the unigram dp model are not due to the weakness of the lexical model .
regardless of how good the lexical model is , it will not be able to completely overcome the inuence of the unigram assumption gov - erning word tokens when modeling the full corpus .
in or - der to reduce the number of collocations , it is necessary to account for sequential dependencies between words .
before showing how to do so , however , we rst present theoretical results regarding the generality of our conclu - sions about unigram models .
mbdp - 123 , the dp model , and other unigram models
the probabilistic models used in mbdp - 123 and our dirichlet process model appeal to quite different genera - tive processes .
to generate a corpus using mbdp , the num - ber of word types is sampled , then the token frequencies , then the forms of the words in the lexicon , and nally an ordering for the set of tokens .
using the dp model , the length of the corpus ( number of word tokens ) must be cho -
123 there are 123 lexical items in the corpus , so under the lexical model , the probability of each of these is approximately 123
there are 123 phonemes and p# : 123 , so a single - character word has probability . 123 under p123
multiplying by the discount factor 123 yields ptrue 123 for one - character words not in the true lexicon .
longer incorrect words will have much lower probability .
sen , and then the sequence of words is generated , implic - itly determining the number of word types and the lexicon .
although these two approaches to generating a corpus are very different , it is possible to show that , by varying the specic distributions assumed at each step of the mbdp - 123 generative process , the two approaches can result in exactly the same distribution over word se - quences .
in appendix b we show that by changing how the size of the lexicon and the token frequencies are cho - sen in steps 123 and 123 of the mbdp model , we can produce distributions over words that are equivalent to the distri - bution given by the dp model when conditioned on the to - tal number of words .
