as the availability and importance of relational datasuch as the friendships sum - marized on a social networking websiteincreases , it becomes increasingly im - portant to have good models for such data .
the kinds of latent structure that have been considered for use in predicting links in such networks have been relatively limited .
in particular , the machine learning community has focused on latent class models , adapting bayesian nonparametric methods to jointly infer how many la - tent classes there are while learning which entities belong to each class .
we pursue a similar approach with a richer kind of latent variablelatent featuresusing a bayesian nonparametric approach to simultaneously infer the number of features at the same time we learn which entities have each feature .
our model combines these inferred features with known covariates in order to perform link prediction .
we demonstrate that the greater expressiveness of this approach allows us to im - prove performance on three datasets .
statistical analysis of social networks and other relational data has been an active area of research for over seventy years and is becoming an increasingly important problem as the scope and availability of social network datasets increase ( 123 ) .
in these problems , we observe the interactions between a set of entities and we wish to extract informative representations that are useful for making predictions about the entities and their relationships .
one basic challenge is link prediction , where we observe the relationships ( or links ) between some pairs of entities in a network ( or graph ) and we try to predict unobserved links .
for example , in a social network , we might only know some subset of people are friends and some are not , and seek to predict which other people are likely to get along .
our goal is to improve the expressiveness and performance of generative models based on extracting latent structure representing the properties of individual entities from the observed data , so we will focus on these kinds of models .
this rules out approaches like the popular p model that uses global quantities of the graph , such as how many edges or triangles are present ( 123 , 123 ) .
of the approaches that do link prediction based on attributes of the individual entities , these can largely be classied into class - based and feature - based approaches .
there are many models that can be placed under these approaches , so we will focus on the models that are most comparable to our approach .
most generative models using a class - based representation are based on the stochastic blockmodel , introduced in ( 123 ) and further developed in ( 123 ) .
in the most basic form of the model , we assume there are a nite number of classes that entities can belong to and that these classes entirely determine the structure of the graph , with the probability of a link existing between two entities depending only on the classes of those entities .
in general , these classes are unobserved , and inference reduces to assigning entities to classes and inferring the class interactions .
one of the important issues that arise in working with this model is determining how many latent classes there are for a given problem .
the innite relational model ( irm ) ( 123 ) used methods from nonparametric bayesian statistics to tackle this problem , allowing the number of classes to be determined at inference time .
the innite hidden relational model ( 123 ) further elaborated on this model and the mixed membership stochastic blockmodel ( mmsb ) ( 123 ) extended it to allow entities to have mixed memberships .
all these class - based models share a basic limitation in the kinds of relational structure they natu - rally capture .
for example , in a social network , we might nd a class which contains male high school athletes and another which contains male high school musicians .
we might believe these two classes will behave similarly , but with a class - based model , our options are to either merge the classes or duplicate our knowledge about common aspects of them .
in a similar vein , with a limited amount of data , it might be reasonable to combine these into a single class male high school stu - dents , but with more data we would want to split this group into athletes and musicians .
for every new attribute like this that we add , the number of classes would potentially double , quickly leading to an overabundance of classes .
in addition , if someone is both an athlete and a musician , we would either have to add another class for that or use a mixed membership model , which would say that the more a student is an athlete , the less he is a musician .
an alternative approach that addresses this problem is to use features to describe the entities .
there could be a separate feature for high school student , male , athlete , and musician and the presence or absence of each of these features is what denes each person and determines their relationships .
one class of latent - feature models for social networks has been developed by ( 123 , 123 , 123 ) , who proposed real - valued vectors as latent representations of the entities in the network where depending on the model , either the distance , inner product , or weighted combination of the vectors corresponding to two entities affects the likelihood of there being a link between them .
however , extending our high school student example , we might hope that instead of having arbitrary real - valued features ( which are still useful for visualization ) , we would infer binary features where each feature could correspond to an attribute like male or athlete .
continuing our earlier example , if we had a limited amount of data , we might not pick up on a feature like athlete .
however , as we observe more interactions , this could emerge as a clear feature .
instead of doubling the numbers of classes in our model , we simply add an additional feature .
determining the number of features will therefore be of extreme importance .
in this paper , we present the nonparametric latent feature relational model , a bayesian nonpara - metric model in which each entity has binary - valued latent features that inuences its relations .
in addition , the relations depend on a set of known covariates .
this model allows us to simultaneously infer how many latent features there are while at the same time inferring what features each entity has and how those features inuence the observations .
this model is strictly more expressive than the stochastic blockmodel .
in section 123 , we describe a simplied version of our model and then the full model .
in section 123 , we discuss how to perform inference .
in section 123 , we illustrate the properties of our model using synthetic data and then show that the greater expressiveness of the latent feature representation results in improved link prediction on three real datasets .
finally , we conclude in section 123
123 the nonparametric latent feature relational model
assume we observe the directed relational links between a set of n entities .
let y be the n n binary matrix that contains these links .
that is , let yij y ( i , j ) = 123 if we observe a link from entity i to entity j in that relation and yij = 123 if we observe that there is not a link .
unobserved links are left unlled .
our goal will be to learn a model from the observed links such that we can predict the values of the unlled entries .
123 basic model
in our basic model , each entity is described by a set of binary features .
we are not given these features a priori and will attempt to infer them .
we assume that the probability of having a link from one entity to another is entirely determined by the combined effect of all pairwise feature if there are k features , then let z be the n k binary matrix where each row corresponds to an entity and each column corresponds to a feature such that zik z ( i , k ) = 123 if the ith entity has feature k and zik = 123 otherwise .
and let zi denote the feature vector corresponding to entity i .
let w be a k k real - valued weight matrix where wkk123 w ( k , k123 ) is the weight that affects the probability of there being a link from entity i to entity j if both entity i has feature k and entity j has feature k123
we assume that links are independent conditioned on z and w , and that only the features of entities i and j inuence the probability of a link between those entities .
this denes the likelihood
pr ( y |z , w ) =
pr ( yij|zi , zj , w )
where the product ranges over all pairs of entities .
given the feature matrix z and weight matrix w , the probability that there is a link from entity i to entity j is
pr ( yij = 123|z , w ) =
where ( ) is a function that transforms values on ( , ) to ( 123 , 123 ) such as the sigmoid function 123+exp ( x ) or the probit function ( x ) = ( x ) .
an important aspect of this model is that all - zero columns of z do not affect the likelihood .
we will take advantage of this in section 123 .
this model is very exible .
with a single feature per entity , it is equivalent to a stochastic block - model .
however , since entities can have more than a single feature , the model is more expressive .
in the high school student example , each feature can correspond to an attribute like male , musician , and athlete .
if we were looking at the relation friend of ( not necessarily symmetric ! ) , then the weight at the ( athlete , musician ) entry of w would correspond to the weight that an athlete would be a friend of a musician .
a positive weight would correspond to an increased probability , a negative weight a decreased probability , and a zero weight would indicate that there is no correlation between those two features and the observed relation .
the more positively correlated features people have , the more likely they are to be friends .
another advantage of this representation is that if our data contained observations of students in two distant locations , we could have a geographic feature for the different locations .
while other features such as athlete or musician might indicate that one person could be a friend of another , the geographic features could have extremely negative weights so that people who live far from each other are less likely to be friends .
however , the parameters for the non - geographic features would still be tied for all people , allowing us to make stronger in - ferences about how they inuence the relations .
class - based models would need an abundance of classes to capture these effects and would not have the same kind of parameter sharing .
given the full set of observations y , we wish to infer the posterior distribution of the feature matrix z and the weights w .
we do this using bayes theorem , p ( z , w|y ) p ( y |z , w ) p ( z ) p ( w ) , where we have placed an independent prior on z and w .
without any prior knowledge about the features or their weights , a natural prior for w involves placing an independent n ( 123 , 123 w ) prior on each wij .
however , placing a prior on z is more challenging .
if we knew how many features there were , we could place an arbitrary parametric prior on z .
however , we wish to have a exible prior that allows us to simultaneously infer the number of features at the same time we infer all the entries in z .
the indian buffet process is such a prior .
123 the indian buffet process and the basic generative model
as mentioned in the previous section , any features which are all - zero do not affect the likelihood .
that means that even if we added an innite number of all - zero features , the likelihood would remain the same .
the indian buffet process ( ibp ) ( 123 ) is a prior on innite binary matrices such that with probability one , a feature matrix drawn from it for a nite number of entities will only have a nite number of non - zero features .
moreover , any feature matrix , no matter how many non - zero features
it contains , has positive probability under the ibp prior .
it is therefore a useful nonparametric prior to place on our latent feature matrix z .
the generative process to sample matrices from the ibp can be described through a culinary metaphor that gave the ibp its name .
in this metaphor , each row of z corresponds to a diner at an indian buffet and each column corresponds to a dish at the innitely long buffet .
if a customer takes a particular dish , then the entry that corresponds to the customers row and the dishs column is a one and the entry is zero otherwise .
the culinary metaphor describes how people choose the dishes .
in the ibp , the rst customer chooses a poisson ( ) number of dishes to sample , where is a parameter of the ibp .
the ith customer tries each previously sampled dish with probability proportional to the number of people that have already tried the dish and then samples a poisson ( / i ) number of new dishes .
this process is exchangeable , which means that the order in which the customers enter the restaurant does not affect the conguration of the dishes that people try ( up to permutations of the dishes as described in ( 123 ) ) .
this insight leads to a straightforward gibbs sampler to do posterior inference that we describe in section 123
using an ibp prior on z , our basic generative latent feature relational model is :
wkk123 n ( 123 , 123
yij ( cid : 123 ) ziw z>
for all k , k123 for which features k and k123 are non - zero for each observation .
123 full nonparametric latent feature relational model
we have described the basic nonparametric latent feature relational model .
we now combine it with ideas from the social network community to get our full model .
first , we note that there are many instances of logit models used in statistical network analysis that make use of covariates in link prediction ( 123 ) .
here we will focus on a subset of ideas discussed in ( 123 ) .
let xij be a vector that inuences the relation yij , let xp , i be a vector of known attributes of entity i when it is the parent of a link , and let xc , i be a vector of known attributes of entity i when it is a child of a link .
for example , in section 123 , when y represents relationships amongst countries , xij is a scalar representing the geographic similarity between countries ( xij = exp ( d ( i , j ) ) ) since this could inuence the relationships and xp , i = xc , i is a set of known features associated with each country ( xp , i and xc , i would be distinct if we had covariates specic to each countrys roles ) .
we then let c be a normally distributed scalar and , p , c , a , and b be normally distributed vectors in our full model in which
pr ( yij = 123|z , w , x , , a , b , c ) =
xij + (
p xp , i + ai ) + (
c xc , j + bj ) + c
if we do not have information about one or all of x , xp , and xc , we drop the corresponding term ( s ) .
in this model , c is a global offset that affects the default likelihood of a relation and ai and bj are entity and role specic offsets .
so far , we have only considered the case of observing a single relation .
it is not uncommon to observe multiple relations for the same set of entities .
for example , in addition to the friend of relation , we might also observe the admires and collaborates with relations .
we still believe that each entity has a single set of features that determines all its relations , but these features will not affect each relation in the same way .
if we are given m relations , label them y 123 , y 123 , .
we will use the same features for each relation , but we will use an independent weight matrix w i for each relation y i .
in addition , covariates might be relation specic or common across all relations .
regardless , they will interact in different ways in each relation .
our full model is now
pr ( y 123 , .
, y m|z , ( w i , x i , i , ai , bi , ci ) m
pr ( y i|z , w i , x i , i , ai , bi , ci ) .
123 variations of the nonparametric latent feature relational model
the model that we have dened is for directed graphs in which the matrix y i is not assumed to be symmetric .
for undirected graphs , we would like to dene a symmetric model .
this is easy to do by restricting w i to be symmetric .
if we further believe that the features we learn should not interact , we can assume that w i is diagonal .
123 related nonparametric latent feature models
there are two models related to our nonparametric latent feature relational model that both use the ibp as a prior on binary latent feature matrices .
the most closely related model is the binary matrix factorization ( bmf ) model of ( 123 ) .
the bmf is a general model with several concrete variants , the most relevant of which was used to predict unobserved entries of binary matrices for image reconstruction and collaborative ltering .
if y is the observed part of a binary matrix , then in this variant , we assume that y |u , v , w ( u w v > ) where ( ) is the logistic function , u and v are independent binary matrices drawn from the ibp , and the entries in w are independent draws from a normal distribution .
if y is an n n matrix where we assume the rows and columns have the same features ( i . e . , u = v ) , then this special case of their model is equivalent to our basic ( covariate - free ) model .
while ( 123 ) were interested in a more general formalization that is applicable to other tasks , we have specialized and extended this model for the task of link prediction .
the other related model is the adclus model ( 123 ) .
this model assumes we are given a symmetric matrix of nonnegative similarities y and that y = zw z> + where z is drawn from the ibp , w is a diagonal matrix with entries independently drawn from a gamma distribution , and is independent gaussian noise .
this model does not allow for arbitrary feature interactions nor does it allow for negative feature
exact inference in our nonparametric latent feature relational model is intractable ( 123 ) .
however , the ibp prior lends itself nicely to approximate inference via markov chain monte carlo ( 123 ) .
we rst describe inference in the single relation , basic model , later extending it to the full model .
in our basic model , we must do posterior inference on z and w .
since with probability one , any sample of z will have a nite number of non - zero entries , we can store just the non - zero columns of each sample of the innite binary matrix z .
since we do not have a conjugate prior on w , we must also sample the corresponding entries of w .
our sampler is as follows :
given w , resample z we do this by resampling each row zi in succession .
when sampling entries in the ith row , we use the fact that the ibp is exchangeable to assume that the ith customer in the ibp was the last one to enter the buffet .
therefore , when resampling zik for non - zero columns k , if mk is the number of non - zero entries in column k excluding row i , then pr ( zik = 123|zik , w , y ) mk pr ( y |zik = 123 , zik , w ) .
we must also sample zik for each of the innitely many all - zero columns to add features to the representation .
here , we use the fact that in the ibp , the prior distribution on the number of new features for the last customer is poisson ( / n ) .
as described in ( 123 ) , we must then weight this by the likelihood term for having that many new features , computing this for 123 , 123 , .
. kmax new features for some maximum number of new features kmax and sampling the number of new features from this normalized distribution .
the main difculty arises because we have not sampled the values of w for the all - zero columns and we do not have a conjugate prior on w , so we cannot compute the likelihood term exactly .
we can adopt one of the non - conjugate sampling approaches from the dirichlet process ( 123 ) to this task or use the suggestion in ( 123 ) to include a metropolis - hastings step to propose and either accept or reject some number of new columns and the corresponding weights .
we chose to use a stochastic monte carlo approximation of the likelihood .
once the number of new features is sampled , we must sample the new values in w as described below .
given z , resample w we sequentially resample each of the weights in w that correspond to non - zero features and drop all weights that correspond to all - zero features .
since we do not have a conjugate prior on w , we cannot directly sample w from its posterior .
if ( ) is the probit , we adapt the auxiliary sampling trick from ( 123 ) to have a gibbs sampler for the entries of w .
if ( ) is the logistic function , no such trick exists and we resort to using a metropolis - hastings step for each weight in which we propose a new weight from a normal distribution centered around the old one .
hyperparameters we can also place conjugate priors on the hyperparameters and w and per - form posterior inference on them .
we use the approach from ( 123 ) for sampling of .
figure 123 : features and corresponding observations for synthetic data .
in ( a ) , we show features that could be explained by a latent - class model that then produces the observation matrix in ( b ) .
white indicates one values , black indicates zero values , and gray indicates held out values .
in ( c ) , we show the feature matrix of our other synthetic dataset along with the corresponding observations in ( d ) .
( e ) shows the feature matrix of a randomly chosen sample from our gibbs sampler .
each i as above .
however , when we resample z , we must compute
in the case of multiple relations , we can sample wi given z independently for
pr ( zik = 123|zik , ( w , y ) m
pr ( y i|zik = 123 , zik , w i ) .
in the full model , we must also update ( i , i
by conditioning on these , the update equations for z and w i take the same form , but with equation ( 123 ) used for the c , ai , bi , ci ) are likelihood .
when we condition on z and w i , the posterior updates for ( i , i independent and can be derived from the updates in ( 123 ) .
c , ai , bi , ci ) m
implementation details despite the ease of writing down the sampler , samplers for the ibp often mix slowly due to the extremely large state space full of local optima .
even if we limited z to have k columns , there are 123n k potential feature matrices .
in an effort to explore the space better , we can augment the gibbs sampler for z by introducing split - merge style moves as described in ( 123 ) as well as perform annealing or tempering to smooth out the likelihood .
however , we found that the most signicant improvement came from using a good initialization .
a key insight that was mentioned in section 123 is that the stochastic blockmodel is a special case of our model in which each entity only has a single feature .
stochastic blockmodels have been shown to perform well for statistical network analysis , so they seem like a reasonable way to initialize the feature matrix .
in the results section , we compare the performance of a random initialization to one in which z is initialized with a matrix learned by the innite relational model ( irm ) .
to get our initialization point , we ran the gibbs sampler for the irm for only 123 iterations and used the resulting class assignments to seed z .
we rst qualitatively analyze the strengths and weaknesses of our model on synthetic data , estab - lishing what we can and cannot expect from it .
we then compare our model against two class - based generative models , the innite relational model ( irm ) ( 123 ) and the mixed membership stochastic blockmodel ( mmsb ) ( 123 ) , on two datasets from the original irm paper and a nips coauthorship dataset , establishing that our model does better than the best of those models on those datasets .
123 synthetic data
we rst focus on the qualitative performance of our model .
we applied the basic model to two very simple synthetic datasets generated from known features .
these datasets were simple enough that the basic model could attain 123% accuracy on held - out data , but were different enough to address the qualitative characteristics of the latent features inferred .
in one dataset , the features were the class - based features seen in figure 123 ( a ) and in the other , we used the features in figure 123 ( c ) .
the observations derived from these features can be seen in figure 123 ( b ) and figure 123 ( d ) , respectively .
on both datasets , we initialized z and w randomly .
with the very simple , class - based model , 123% of the sampled feature matrices were identical to the generating feature matrix with another 123% differing by a single bit .
however , on the other dataset , only 123% of the samples were at most a single bit different than the true matrix .
it is not the case that the other 123% of the samples were bad samples , though .
a randomly chosen sample of z is shown in figure 123 ( e ) .
though this matrix is different from the true generating features , with the appropriate weight matrix it predicts just as well as the true feature matrix .
these tests show that while our latent feature approach is able to learn features that explain the data well , due to subtle interactions between sets of features and weights , the features themselves will not in general correspond to interpretable features .
however , we can expect the inferred features to do a good job explaining the data .
this also indicates that there are many local optima in the feature space , further motivating the need for good initialization .
123 multi - relational datasets
in the original irm paper , the irm was applied to several datasets ( 123 ) .
these include a dataset containing 123 relations of 123 countries ( such as exports to and protests ) along with 123 given features of the countries ( 123 ) and a dataset containing 123 kinship relationships of 123 people in the alyawarra tribe in central australia ( 123 ) .
see ( 123 , 123 , 123 ) for more details on the datasets .
our goal in applying the latent feature relational model to these datasets was to demonstrate the effectiveness of our algorithm when compared to two established class - based algorithms , the irm and the mmsb , and to demonstrate the effectiveness of our full algorithm .
for the alyawarra dataset , we had no known covariates .
for the countries dataset , xp = xc was the set of known features of the countries and x was the country distance similarity matrix described in section 123 .
as mentioned in the synthetic data section , the inferred features do not necessarily have any inter - pretable meaning , so we restrict ourselves to a quantitative comparison .
for each dataset , we held out 123% of the data during training and we report the auc , the area under the roc ( receiver oper - ating characteristic ) curve , for the held - out data ( 123 ) .
we report results for inferring a global set of features for all relations as described in section 123 which we refer to as global as well as results when a different set of features is independently learned for each relation and then the aucs of all relations are averaged together , which we refer to as single .
in addition , we tried initializing our sampler for the latent feature relational model with either a random feature matrix ( lfrm rand ) or class - based features from the irm ( lfrm w / irm ) .
we ran our sampler for 123 iterations for each conguration using a logistic squashing function ( though results using the probit are similar ) , throwing out the rst 123 samples as burn - in .
each method was given ve random restarts .
table 123 : auc on the countries and kinship datasets .
bold identies the best performance .
lfrm w / irm 123 123
countries global alyawarra single alyawarra global
results of these tests are in table 123
as can be seen , the lfrm with class - based initialization out - performs both the irm and mmsb .
on the individual relations ( single ) , the lfrm with random initialization also does well , beating the irm initialization on both datasets .
however , the random initialization does poorly at inferring the global features due to the coupling of features and the weights for each of the relations .
this highlights the importance of proper initialization .
to demon - strate that the covariates are helping , but that even without them , our model does well , we ran the global lfrm with class - based initialization without covariates on the countries dataset and the auc dropped to 123 123 , which is still the best performance .
on the countries data , the latent feature model inferred on average 123 - 123 features when seeded with the irm and 123 - 123 with a random initialization .
on the kinship data , it inferred 123 - 123 features when seeded with the irm and 123 - 123 when seeded randomly .
( a ) true relations
( b ) feature predictions
( c ) irm predictions
( d ) mmsb predictions
figure 123 : predictions for all algorithms on the nips coauthorship dataset .
in ( a ) , a white entry means two people wrote a paper together .
in ( b - d ) , the lighter an entry , the more likely that algorithm predicted the corresponding people would interact .
123 predicting nips coauthorship
as our nal example , highlighting the expressiveness of the latent feature relational model , we used the coauthorship data from the nips dataset compiled in ( 123 ) .
this dataset contains a list of all papers and authors from nips 123 - 123
we took the 123 authors who had published with the most other people and looked at their coauthorship information .
the symmetric coauthor graph can be seen in figure 123 ( a ) .
we again learned models for the latent feature relational model , the irm and the mmsb training on 123% of the data and using the remaining 123% as a test set .
for the latent feature model , since the coauthorship relationship is symmetric , we learned a full , symmetric weight matrix w as described in section 123 .
we did not use any covariates .
a visualization of the predictions for each of these algorithms can be seen in figure 123 ( b - d ) .
figure 123 really drives home the difference in expressiveness .
stochastic blockmodels are required to group authors into classes , and assumes that all members of classes interact similarly .
for visualization , we have ordered the authors by the groups the irm found .
these groups can clearly be seen in figure 123 ( c ) .
the mmsb , by allowing partial membership is not as restrictive .
however , on this dataset , the irm outperformed it .
the latent feature relational model is the most expressive of the models and is able to much more faithfully reproduce the coauthorship network .
the latent feature relational model also quantitatively outperformed the irm and mmsb .
we again ran our sampler for 123 samples initializing with either a random feature matrix or a class - based feature matrix from the irm and reported the auc on the held - out data .
using ve restarts for each method , the lfrm w / irm performed best with an auc of 123 , the lfrm rand was next with 123 and much lower were the irm at 123 and the mmsb at 123 ( all at most 123 ) .
on average , the latent feature relational model inferred 123 - 123 features when initialized with the irm and 123 - 123 features when initialized randomly .
we have introduced the nonparametric latent feature relational model , an expressive nonparametric model for inferring latent binary features in relational entities .
this model combines approaches from the statistical network analysis community , which have emphasized feature - based methods for analyzing network data , with ideas from bayesian nonparametrics in order to simultaneously infer the number of latent binary features at the same time we infer the features of each entity and how those features interact .
existing class - based approaches infer latent structure that is a special case of what can be inferred by this model .
as a consequence , our model is strictly more expressive than these approaches , and can use the solutions produced by these approaches for initialization .
we showed empirically that the nonparametric latent feature model performs well at link prediction on several different datasets , including datasets that were originally used to argue for class - based approaches .
the success of this model can be traced to its richer representations , which make it able to capture subtle patterns of interaction much better than class - based models .
acknowledgments ktm was supported by the u . s .
department of energy contract de - ac123 - 123na123 through lawrence livermore national laboratory .
tlg was supported by grant number fa123 - 123 - 123 - 123 from the air force ofce of scientic research .
