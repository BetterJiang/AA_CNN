many modern visual recognition algorithms in - corporate a step of spatial pooling , where the outputs of several nearby feature detectors are combined into a local or global bag of features , in a way that preserves task - related information while removing irrelevant details .
pooling is used to achieve invariance to image transforma - tions , more compact representations , and better robustness to noise and clutter .
several papers have shown that the details of the pooling oper - ation can greatly inuence the performance , but studies have so far been purely empirical .
in this paper , we show that the reasons underlying the performance of various pooling methods are ob - scured by several confounding factors , such as the link between the sample cardinality in a spa - tial pool and the resolution at which low - level features have been extracted .
we provide a de - tailed theoretical analysis of max pooling and av - erage pooling , and give extensive empirical com - parisons for object recognition tasks .
modern computer vision architectures often comprise a spatial pooling step , which combines the responses of feature detectors obtained at nearby locations into some statistic that summarizes the joint distribution of the fea - tures over some region of interest .
the idea of feature pooling originates in hubel and wiesels seminal work on complex cells in the visual cortex ( 123 ) , and is re - lated to koenderinks concept of locally orderless im - ages ( 123 ) .
pooling features over a local neighborhood to create invariance to small transformations of the input
appearing in proceedings of the 123 th international conference on machine learning , haifa , israel , 123
copyright 123 by the
is used in a large number of models of visual recogni - tion .
the pooling operation is typically a sum , an av - erage , a max , or more rarely some other commutative independent of the order of the contributing fea - tures ) combination rule .
biologically - inspired models of image recognition that use feature pooling include the neocognitron ( fukushima & miyake , 123 ) , convolutional networks which use average pooling ( lecun et al . , 123; 123 ) , or max pooling ( ranzato et al . , 123; jarrett et al . , 123 ) , the hmax class of models which uses max pool - ing ( serre et al . , 123 ) , and some models of the primary visual cortex area v123 ( pinto et al . , 123 ) which use av - erage pooling .
many popular methods for feature ex - traction also use pooling , including sift ( lowe , 123 ) , histograms of oriented gradients ( hog ) ( dalal & triggs , 123 ) and their variations .
in these methods , the domi - nant gradient orientations are measured in a number of re - gions , and are pooled over a neighborhood , resulting in a local histogram of orientations .
recent recognition sys - tems often use pooling at a higher level to compute lo - cal or global bags of features .
this is done by vector - quantizing feature descriptors and by computing the code - word counts over local or global areas ( sivic & zisserman , 123; lazebnik et al . , 123; zhang et al . , 123; yang et al . , 123 ) , which is equivalent to average - pooling vectors con - taining a single 123 at the index of the codeword , and 123 ev - erywhere else ( 123 - of - k codes ) .
in general terms , the objective of pooling is to transform the joint feature representation into a new , more usable one that preserves important information while discarding ir - relevant detail , the crux of the matter being to determine what falls in which category .
for example , the assumption underlying the computation of a histogram is that the aver - age feature activation matters , but exact spatial localization does not .
achieving invariance to changes in position or lighting conditions , robustness to clutter , and compactness of representation , are all common goals of pooling .
the success of the spatial pyramid model ( lazebnik et al . ,
a theoretical analysis of feature pooling
123 ) , which obtains large increases in performance by performing pooling over the cells of a spatial pyramid rather than over the whole image as in plain bag - of - features models ( zhang et al . , 123 ) , illustrates the importance of the spatial structure of pooling neighborhoods .
perhaps more intriguing is the dramatic inuence of the way pool - ing is performed once a given region of interest has been chosen .
thus , jarrett et al .
( 123 ) have shown that pooling type matters more than careful unsupervised pretraining of features for classication problems with little training data , obtaining good results with random features when appro - priate pooling is used .
yang et al .
( 123 ) report much better classication performance on several object or scene clas - sication benchmarks when using the maximum value of a feature rather than its average to summarize its activity over a region of interest .
but no theoretical justication of these ndings is given .
in previous work ( boureau et al . , 123 ) , we have shown that using max pooling on hard - vector quantized features ( which produces a binary vector that records the presence of a feature in the pool ) in a spa - tial pyramid brings the performance of linear classication to the level of that obtained by lazebnik et al .
( 123 ) with an intersection kernel , even though the resulting feature is binary .
however , it remains unclear why max pooling per - forms well in a large variety of settings , and indeed whether similar or different factors come into play in each case .
this paper proposes to ll the gap and to conduct a thor - ough theoretical investigation of pooling .
we compare dif - ferent pooling operations in a categorization context , and examine how the behavior of the corresponding statistics may translate into easier or harder subsequent classica - tion .
we provide experiments in the context of visual object recognition , but the analysis applies to all tasks which in - corporate some form of pooling ( e . g . , text processing from which the bag - of - features method was originally adapted ) .
the main contributions of this paper are ( 123 ) an extensive analytical study of the discriminative powers of different pooling operations , ( 123 ) the discrimination of several fac - tors affecting pooling performance , including smoothing and sparsity of the features , ( 123 ) the unication of several popular pooling types as belonging to a single continuum .
pooling binary features
consider a two - class categorization problem .
intuitively , classication is easier if the distributions from which points of the two classes are drawn have no overlap .
if the distributions are simply shifted versions of one an - other ( e . g . , two gaussian distributions with same vari - ance ) , linear separability increases monotonically with the magnitude of the shift ( e . g . , with the distance between the means of two gaussian distributions of same vari - ance ) ( bruckstein & cover , 123 ) .
in this section , we ex -
amine how pooling affects the separability of the resulting feature distributions when the features being pooled are bi - nary vectors ( e . g . , 123 - of - k codes obtained by vector quanti - zation in bag - of - features models ) .
let us examine the contribution of a single feature in a bag - of - features representation ( i . e . , if the unpooled data is a p k matrix of 123 - of - k codes taken at p locations , we extract a single p - dimensional column v of 123s and 123s , in - dicating the absence or presence of the feature at each lo - cation ) .
for simplicity , we model the p components of v as i . i . d .
bernoulli random variables .
the independence as - sumption is clearly false since nearby image features are strongly correlated , but the analysis of this simple model nonetheless yields useful predictions that can be veried empirically .
the vector v is reduced by a pooling operation f to a single scalar f ( v ) ( which would be one component of the k - dimensional representation using all features , e . g . , one bin in a histogram ) .
we consider two pooling types : average pooling fa ( v ) = 123 i=123 vi , and max pooling fm ( v ) = maxi vi .
distribution separability
given two classes c123 and c123 , we examine the separation of conditional distributions p ( fm|c123 ) and p ( fm|c123 ) , and p ( fa|c123 ) and p ( fa|c123 ) .
viewing separability as a signal - to - noise problem , better separability can be achieved by ei - ther increasing the distance between the means of the two class - conditional distributions , or reducing their standard
we rst consider average pooling .
the sum over p i . i . d .
bernoulli variables of mean follows a binomial distri - bution b ( p , ) .
consequently , the distribution of fa is a scaled - down version of the binomial distribution , with mean a = , and variance 123 a = ( 123 ) / p .
the ex - pected value of fa is independent of sample size p , and the variance decreases like 123 p ; therefore the separation ra - tio of means difference over standard deviation decreases monotonically like 123p max pooling is slightly less straightforward , so we examine means separation and variance separately in the next two
means separation of max - pooled
fm is a bernoulli variable of mean m = 123 ( 123 ) p and variance 123 m = ( 123 ( 123 ) p ) ( 123 ) p .
the mean in - creases monotonically from 123 to 123 with sample size p .
let denote the separation of class - conditional expectations of
a theoretical analysis of feature pooling
( p ) , |e ( fm|c123 ) e ( fm|c123 ) | = | ( 123 ) p ( 123 ) p| , where 123 , p ( vi = 123|c123 ) and 123 , p ( vi = 123|c123 ) .
we abuse notation by using to refer both to the function dened on sample cardinality p and its extension to r .
it is easy to show that is increasing between 123 and
pm , ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 )
log ( cid : 123 ) log ( 123 123 )
log ( 123 123 ) ( cid : 123 ) / log ( cid : 123 ) 123 123
and decreasing between pm and , with lim123 = lim = 123
noting that ( 123 ) = |123 123| is the distance between the class - conditional expectations of average - pooled features , there exists a range of pooling cardinalities for which the distance is greater with max pooling than average pooling if and only if pm > 123
assuming 123 > 123 , it is easy to show that pm 123 123 > 123 123 e > 123 .
this implies that the feature is selected to represent more than half the patches on average , which in practice does not happen in usual bag - of - features contexts , where codebooks comprise more than a hundred codewords .
variance of max - pooled features
the variance of the max - pooled feature is 123 m = ( 123 ( 123 ) p ) ( 123 ) p .
a simple analysis of the continuous exten - sion of this function to real numbers shows that it has limit 123 at 123 and , and is increasing then decreasing , reaching its maximum of 123 at log ( 123 ) / | log ( 123 ) | .
the increase of the variance can play against the better separation of the ex - pectations of the max - pooled feature activation , when pa - rameter values 123 and 123 are too close for the two classes .
several regimes for the variation of means separation and standard deviations are shown in fig
conclusions and predictions
our simplied analysis leads to several predictions :
max pooling is particularly well suited to the separa - tion of features that are very sparse ( i . e . , have a very low probability of being active )
using all available samples to perform the pooling
may not be optimal
the optimal pooling cardinality should increase with
the rst point can be formalized by observing that the char - acteristic pooling cardinality | in the case 123 ) , scales the transition to the asymptotic regime ( low
log ( 123 ) | ( 123
variance , high probability of activation ) : the maximum of the variance is reached at p = log ( 123 ) / | log ( 123 ) | , and :
p ( fm ( v ) = 123 ) > p >
consequently , the range of cardinalities for which max pooling achieves good separation between two classes dou - bles if the probability of activation of the feature for both classes is divided by two .
a particularly favorable regime is 123 123 123 that is , a feature which is rare , but rela - tively much more frequent in one of the two classes; in that case , both classes reach their asymptotic regime for very different sample cardinalities ( 123
we have recently conducted preliminary experiments re - lated to the second point ( boureau et al . , 123 ) namely , that better performance can be obtained by using smaller pooling cardinalities .
we have compared the performance of whole - image pooling , regular two - level spatial pyramid pooling , and a two - level pyramid where the smaller pools are taken randomly instead of spatially .
in the random pyramid setting , the performance of max pooling is inter - mediate between that obtained with whole - image and spa - tial pyramid pooling , while the classication using aver - age pooling becomes worse than with whole - image pool - ing .
however , a number of concurrent factors could explain the increased accuracy : ( 123 ) smaller pooling cardinality , ( 123 ) smoothing over multiple estimates ( one per ner cell of the pyramid ) , ( 123 ) estimation of two distinct features ( the max - imum over the full and partial cardinalities , respectively ) .
the more comprehensive experiments presented in the next section resolve this ambiguity by isolating each factor .
finally , the increase of optimal pooling cardinality with dictionary size is related to the link underlined above be - tween the sparsity of the features ( dened here as the prob - ability of them being 123 ) and the discriminative power of max - pooling , since the expected feature activations sum to one in the general bag - of - features setting ( exactly one fea - ture is activated at each location ) , resulting in a mean ex - pected activation of 123 k with a k - word codebook .
thus , k gives an order of magnitude for the characteristic cardinal - ity scale of the transition to the asymptotic regime , for a large enough codebook .
conjectures by running experiments ( lazebnik et al . , 123 ) and caltech - on the scenes 123 ( fei - fei et al . , 123 ) datasets , which respectively comprise 123 object categories ( plus a background cate - gory ) and fteen scene categories .
in all experiments , the features being pooled are local codes representing 123 123 sift descriptors that have been densely extracted using the parameters yielding the best accuracy in our previous
a theoretical analysis of feature pooling
123 + 123
( a ) 123 = 123 , 123 = 123
( b ) 123 = 123 , 123 = 123
( c ) 123 = 123 , 123 = 123
figure 123
( p ) = | ( 123 123 ) p ( 123 123 ) p| , 123 and 123 denote the distance between the expectations of the max - pooled respectively .
max = / ( 123 + 123 ) and avg = features of mean activation 123 and 123 , and their standard deviations , |123 123| . p / ( p123 ( 123 123 ) + p123 ( 123 123 ) ) give a measure of separability for max and average pooling .
reaches its peak
at smaller cardinalities than max .
( a ) when features have relatively large activations , the peak of separability is obtained for small cardinalities ( b ) with sparser feature activations , the range of the peak is much larger ( note the change of scale in the x axis ) .
( c ) when one feature is much sparser than the other , max can be larger than avg for some cardinalities ( shaded area ) .
best viewed in color .
work ( 123 ) ( every 123 pixels for the scenes and every 123 pixels for caltech - 123 ) .
the codes jointly represent 123 123 neighborhoods of sift descriptors , with subsampling of 123 and 123 for the scenes and caltech - 123 , respectively .
features are pooled over the whole image using either average or max pooling .
classication is performed with a one - versus - one support vector machine ( svm ) using a linear kernel , and 123 and 123 training images per class for the scenes and caltech - 123 datasets , respectively , and the rest for testing , following the usual experimental setup .
we report the average per - class recognition rate , averaged over 123 random splits of training and testing images .
optimal pooling cardinality
we rst test whether recognition can indeed improve for some codebook sizes when max pooling is performed over samples of smaller cardinality , as predicted by our analysis .
recognition performance is compared using either average or max pooling , with various combinations of codebook sizes and pooling cardinalities .
we use whole - image rather than pyramid or grid pooling , since having several cells of same cardinality provides some smoothing that is hard to quantify .
results are presented in fig .
recognition per - formance of average - pooled features ( average in fig .
123 ) drops with pooling cardinality for all codebook sizes , as expected; performance also drops with max pooling ( 123 es - timate in fig .
123 ) when the codebook size is large .
however , noticeable improvements appear at intermediate cardinal - ities for the smaller codebook sizes ( compare blue , solid curves on the left and right of fig .
123 ) , as predicted by our
next , we examine whether better recognition can be achieved when using a smoother estimate of the expected max - pooled feature activation .
we consider two ways of rening the estimate .
first , if only a fraction of all sam - ples is used , a smoother estimate can be obtained by re - placing the single max by an empirical average of the max
over different subsamples , the limit case as pool cardinal - ity decreases being average pooling .
the second approach directly applies the formula for the expectation of the max -
imum ( 123 ( 123 ) p , using the same notation as before ) to the empirical mean computed using all samples .
this has the benet of removing the constraint that p be smaller than the number of available samples , in addition to be - ing computationally very simple .
results using these two smoothing strategies are plotted in fig .
123 under labels em - pirical and expectation , respectively .
smoothing the esti - mate of the max - pooled features always helps , especially at smaller pooling cardinalities .
the best performance is then obtained with pooling cardinalities smaller than the full cardinality in all our experiments .
as predicted , the maximum of the curve shifts towards larger cardinality as codebook size increases .
the best estimate of the max - pooled feature is the expectation computed from the em -
pirical mean , 123 ( 123 ) p .
p here simply becomes the
parameter of a nonlinear function applied to the mean .
in all cases tested , using this nonlinear function with the opti - mal p outperforms both average and max pooling .
combining multiple pooling
the maximum over a pool of smaller cardinality is not merely an estimator of the maximum over a large pool; therefore , using different pool cardinalities ( e . g . , using a spatial pyramid instead of a grid ) may provide a more powerful feature , independently of the difference in spa - tial structure .
using a codebook of size 123 , we com - pare recognition rates using jointly either one , two , or three different pooling cardinalities , with average pooling , max pooling with a single estimate per pooling cardinality , or max pooling smoothed by using the theoretical expectation .
results presented in table 123 show that combining cardinal - ities improves performance with max pooling only if the estimate has not been smoothed .
thus , the simultaneous presence of multiple cardinalities does not seem to provide
a theoretical analysis of feature pooling
pool cardinality , log scale
pool cardinality , log scale
pool cardinality , log scale
pool cardinality , log scale
( a ) 123 codewords
( b ) 123 codewords
( c ) 123 codewords
( d ) 123 codewords
pool cardinality , log scale
pool cardinality , log scale
pool cardinality , log scale
pool cardinality , log scale
( e ) 123 codewords
( f ) 123 codewords
( g ) 123 codewords
( h ) 123 codewords
figure 123
inuence of pooling cardinality and smoothing on performance .
top row : scenes dataset .
bottom row : caltech - 123 dataset .
123 estimate : max computed over a single pool .
empirical : empirical average of max - pooled features over several subsamples ( not plotted
for smaller sizes , when it reaches the expectation ) expectation : theoretical expectation of the maximum over p samples 123 ( 123 ) p , computed from the empirical average .
average : estimate of the average computed over a single pool .
best viewed in color .
any benet beyond that of an approximate smoothing .
table 123
classication results with whole - image pooling over bi - nary codes ( k = 123 ) .
one indicates that features are pooled using a single cardinality , joint that the larger cardinalities are
also used .
sm : smooth maximum ( 123 ( 123 ) p ) .
caltech 123 avg , one 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 scenes avg , one 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123
practical consequences
in papers using a spatial pyramid ( lazebnik et al . , 123; yang et al . , 123 ) , there is a coupling between the pool - ing cardinality and other parameters of the experiment : the pooling cardinality is the density at which the underlying low - level feature representation have been extracted ( e . g . , sift features computed every 123 pixels in ( lazebnik et al . , 123 ) ) multiplied by the spatial area of each spatial pool .
while using all available samples is optimal for average
pooling , this is usually not the case with max pooling over binary features , particularly when the size of the codebook is small .
instead , the pooling cardinality for max pooling should be adapted to the dictionary size , and the remaining samples should be used to smooth the estimate .
another , simpler way to achieve similar or better performance is to apply to the average - pooled feature the nonlinear transfor - mation corresponding to the expectation of the maximum ,
( i . e . , 123 ( 123 ) p , using the same notation as before ) ; in addition , the parameter p is then no longer limited by the number of available samples in a pool , which may be important for very large codebooks .
our experiments us - ing binary features in a three - level pyramid show that this transformation yields improvement over max pooling for all codebook sizes ( table 123 ) .
the increase in accuracy is small , however the difference is consistently positive when looking at experimental runs individually instead of the dif - ference in the averages over ten runs .
table 123
recognition accuracy with 123 - level pyramid pooling over binary codes .
one - vs - all classication has been used in this ex - periment .
max : max pooling using all samples .
sm : smooth max - imum ( expected value of the maximum computed from the aver -
age 123 ( 123 ) p ) , using a pooling cardinality of p = 123 for codebook sizes 123 and 123 , p = 123 for codebook size 123
caltech 123 max
123 scenes max
sm 123 123 123 123 123 123 sm 123 123 123 123 123 123
a theoretical analysis of feature pooling
pooling continuous sparse codes
sparse codes have proven useful in many image applica - tions such as image compression and deblurring .
com - bined with max pooling , they have led to state - of - the - art image recognition performance with a linear classi - er ( yang et al . , 123; boureau et al . , 123 ) .
however , the analysis developed for binary features in the previous sec - tion does not apply , and the underlying causes for this good performance seem to be different .
inuence of pooling cardinality
in the case of binary features , and when no smoothing is performed , we have seen above that there is an optimal pooling cardinality , which increases with the sparsity of the features .
smoothing the features displaces that opti - mum towards smaller cardinalities .
in this section , we per - form the same analysis for continuous features , and show that ( 123 ) it is always better to use all samples for max pool - ing when no smoothing is performed , ( 123 ) however the in - crease in signal - to - noise ratio ( between means separation and standard deviation ) does not match the noise reduction obtained by averaging over all samples .
let p denote cardinality of the pool .
for a gaussian dis - tribution , a classical result is that the expectation of the max over samples from a distribution of variance 123 grows
asymptotically ( when p ) like p123 log ( p )
the separation of the maxima over two gaussian samples increases indenitely with sample size if their standard de - viations are different , but the rate of growth is very slow .
and variance 123
the cumulative distribution function of the max - pooled
exponential distribution ( or laplace distributions for fea - ture values that may be negative ) are often preferred to gaussian distributions to model visual feature responses because they are highly kurtotic .
in particular , they are a better model for sparse codes .
assume the distribution of the value of a feature for each patch is an exponen - tial distribution with mean 123 123 .
the cor - responding cumulative distribution function is 123 ex .
feature is ( 123 ex ) p .
the mean and variance of the distribution can be shown to be respectively m = h ( p ) m = 123 l ( 123h ( l ) h ( p ) ) , where h ( k ) = i denotes the harmonic series .
thus , for all p , , and the distributions will be better sep - arated if the scaling factor of the mean is bigger than the scaling factor of the standard deviations , i . e . , h ( p ) > l ( 123h ( l ) h ( p ) ) , which is true for all p .
fur - thermore , since h ( p ) = log ( p ) + + o ( 123 ) when p ( where is eulers constant ) , it can be shown that l ( 123h ( l ) h ( p ) ) = log ( p ) + o ( 123 ) , so that the distance between the means grows faster ( like log ( p ) )
than the standard deviation , which grows like plog ( p ) .
two conclusions can be drawn from this : ( 123 ) when no smoothing is performed , larger cardinalities provide a bet - ter signal - to - noise ratio , but ( 123 ) this ratio grows slower than when simply using the additional samples to smooth the es -
timate ( 123 / p assuming independent samples , although in
reality smoothing is less favorable since the independence assumption is clearly false in images ) .
we perform the same experiments as in the previous section to test the inuence of codebook size and pooling cardinali - ties , using continuous sparse codes instead of binary codes .
results are presented in fig .
as expected from our anal - ysis , using larger pooling cardinalities is always better with continuous codes when no smoothing is performed ( blue solid curve ) : no bump is observed even with smaller dictio - naries .
max pooling performs better than average pooling on the caltech dataset ( bottom row in fig .
123 ) ; this is not predicted by the analysis using our very simple model .
on the scenes dataset ( top row in fig .
123 ) , max pooling and av - erage pooling perform equally well when the largest dictio - nary size tested ( 123 ) is used .
slightly smoothing the esti - mate of max pooling by using a smaller sample cardinality results in a small improvement in performance; since the grid ( or pyramid ) pooling structure performs some smooth - ing ( by providing several estimates for the sample cardinal - ities of the ner levels ) , this may explain why max pooling performs better than average pooling with grid and pyra - mid smoothing , even though average pooling may perform as well when a single estimate is given .
mixture distribution
our simple model does not account for the better discrim - ination sometimes achieved by max pooling for continu - ous sparse codes with large dictionaries .
in a previous paper ( boureau et al . , 123 ) , we showed that max pool - ing may perform better than average pooling with expo - nential features sampled from mixture distributions , with one of the components of the mixture being shared be - tween classes and having a lower expected activation .
this may play a role in the better performance of max pooling .
in fact , the sparse code vectors extracted on images have an overwhelming number of zero components , and may thus be better modeled as a mixture distribution of a dirac delta function and an exponential distribution , than as a sin - gle exponential distribution .
assuming the mixture coef - cients vary between images , the mean of the distribution computed over an image shifts between 123 and the mean of the exponential; this may result in a larger overlap between class - conditional distributions when using average pooling than max pooling , as illustrated in our work ( boureau et al . ,
a theoretical analysis of feature pooling
pool cardinality , log scale
pool cardinality , log scale
pool cardinality , log scale
pool cardinality , log scale
( a ) 123 codewords
( b ) 123 codewords
( c ) 123 codewords
( d ) 123 codewords
pool cardinality , log scale
pool cardinality , log scale
pool cardinality , log scale
pool cardinality , log scale
( e ) 123 codewords
( f ) 123 codewords
( g ) 123 codewords
( h ) 123 codewords
figure 123
inuence of pooling cardinality and smoothing on performance .
top row : scenes dataset .
bottom row : caltech - 123 dataset .
123 estimate : maximum computed over a single pool .
empirical : empirical average of max - pooled features over several subsamples of smaller cardinality .
average : estimate of the average computed over a single pool .
best viewed in color .
transition from average to max pooling
the previous sections have shown that depending on the data and features , either max or average pooling may per - form best .
the optimal pooling type for a given classi - cation problem may be neither max nor average pooling , but something in between; in fact , we have shown that it is often better to take the max over a fraction of all avail - able feature points , rather than over the whole sample .
this can be viewed as an intermediate position in a parametriza - tion from average pooling to max pooling over a sample of xed size , where the parameter is the number of feature points over which the max is computed : the expected value of the max computed over one feature is the average , while the max computed over the whole sample is obviously the
this is only one of several possible parametrizations that continuously transition from average to max pooling .
the p - norm of a vector ( more accurately , a version of it nor - malized by the number of samples n ) is another well -
known one : fp ( v ) = ( cid : 123 ) 123 , which gives the average for p = 123 and the max for p .
this parametrization accommodates square - root pooling ( for p = 123 ) and absolute value pooling ( for p = 123 ) , that have also used in the literature ( e . g .
( yang et al . , 123 ) ) .
a third parametrization is the sum of samples weighted by a softmax function : pi xi .
this gives average pooling for = 123 and max pooling for .
finally , a fourth parametrization is 123 n pi exp ( xi ) , which gives the average for 123 and the max for .
as with the p - norm , the result only depends on the empir - ical feature activation mean in the case of binary vectors; thus , these functions can be applied to an already obtained
123 plots the recognition rate obtained on the scenes dataset using sparse codes and each of the four parametriza - tions mentioned .
instead of using the expectation of the maximum for exponential distributions , we have used the
expectation of the maximum of binary codes ( 123 ( 123 ) p ) ,
applied to the average , as we have observed that it works well; we refer to this function as the expectation of the maximum ( maxexp in fig .
123 ) , although it does not con - verge to the maximum when p for continuous codes .
both this parametrization and the p - norm perform better than the two other pooling functions tested , which present a marked dip in performance for intermediate values .
this paper has looked more closely into the factors un - derlying the recognition performance of pooling opera - tions .
by carefully adjusting the pooling step of fea - ture extraction , relatively simple systems of local features and classiers can become competitive to more complex ones : our previous work ( boureau et al . , 123 ) had already demonstrated that it was possible to achieve similar lev - els of performance with a linear kernel as with lazebnik et al .
( 123 ) s intersection kernel , using vector quantized binary codes .
here , we have investigated what properties of max pooling may account for this good performance , and shown that this pooling strategy was well adapted to
a theoretical analysis of feature pooling
parameter p , log scale
parameter p , log scale
parameter p , log scale
parameter p , log scale
( a ) 123 codewords
( b ) 123 codewords
( c ) 123 codewords
( d ) 123 codewords
figure 123
recognition rate obtained using several pooling functions that perform a continuous transition from average to max pooling when varying parameter p ( see text ) .
best viewed in color .
features with a low probability of activation .
we have pro - posed several methods for further improving pooling : ( 123 ) use directly the formula for the expectation of the maxi - mum to obtain a smoother estimate in the case of binary codes , ( 123 ) pool over smaller samples and take the average .
when using sparse coding , some limited improvement may be obtained by pooling over subsamples of smaller cardi - nalities and averaging , and conducting a search for the opti - mal pooling cardinality , but this is not always the case .
fur - ther directions we envision include pooling across several feature types , and adapting pooling parameters separately for each feature .
we hope that paying more attention to the choice of the pooling function will lead to better designed
acknowledgments .
this work was funded in part by nsf grant efri / copn - 123 to nyu , and onr con - tract n123 - 123 - 123 - 123 to nyu .
