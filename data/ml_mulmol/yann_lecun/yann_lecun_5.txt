we present an integrated framework for using convolutional networks for classi - cation , localization and detection .
we show how a multiscale and sliding window approach can be efciently implemented within a convnet .
we also introduce a novel deep learning approach to localization by learning to predict object bound - aries .
bounding boxes are then accumulated rather than suppressed in order to increase detection condence .
we show that different tasks can be learned simul - taneously using a single shared network .
this integrated framework is the winner of the localization task of the imagenet large scale visual recognition challenge 123 ( ilsvrc123 ) and obtained very competitive results for the detection and classications tasks .
in post - competition work , we establish a new state of the art for the detection task .
finally , we release a feature extractor from our best model
recognizing the category of the dominant object in an image is a tasks to which convolutional networks ( convnets ) ( 123 ) have been applied for many years , whether the objects were handwritten characters ( 123 ) , house numbers ( 123 ) , textureless toys ( 123 ) , trafc signs ( 123 , 123 ) , objects from the caltech - 123 dataset ( 123 ) , or objects from the 123 - category imagenet dataset ( 123 ) .
the accuracy of convnets on small datasets such as caltech - 123 , while decent , has not been record - breaking .
however , the advent of larger datasets has enabled convnets to signicantly advance the state of the art on datasets such as the 123 - category imagenet ( 123 ) .
the main advantage of convnets for many such tasks is that the entire system is trained end to end , from raw pixels to ultimate categories , thereby alleviating the requirement to manually design a suitable feature extractor .
the main disadvantage is their ravenous appetite for labeled training
the main point of this paper is to show that training a convolutional network to simultaneously classify , locate and detect objects in images can boost the classication accuracy and the detection and localization accuracy of all tasks .
the paper proposes a new integrated approach to object detection , recognition , and localization with a single convnet .
we also introduce a novel method for localization and detection by accumulating predicted bounding boxes .
we suggest that by combining many localization predictions , detection can be performed without training on background samples and that it is possible to avoid the time - consuming and complicated bootstrapping training passes .
not training on background also lets the network focus solely on positive classes for higher accuracy .
experiments are conducted on the imagenet ilsvrc 123 and 123 datasets and establish state of the art results on the ilsvrc 123 localization and detection tasks .
while images from the imagenet classication dataset are largely chosen to contain a roughly - centered object that lls much of the image , objects of interest sometimes vary signicantly in size and position within the image .
the rst idea in addressing this is to apply a convnet at multiple locations in the image , in a sliding window fashion , and over multiple scales .
even with this , however , many viewing windows may contain a perfectly identiable portion of the object ( say , the head of a dog ) , but not the entire object , nor even the center of the object .
this leads to decent classication but poor localization and detection .
thus , the second idea is to train the system to not only produce a distribution over categories for each window , but also to produce a prediction of the location and size of the bounding box containing the object relative to the window .
the third idea is to accumulate the evidence for each category at each location and size .
many authors have proposed to use convnets for detection and localization with a sliding window over multiple scales , going back to the early 123s for multi - character strings ( 123 ) , faces ( 123 ) , and hands ( 123 ) .
more recently , convnets have been shown to yield state of the art performance on text detection in natural images ( 123 ) , face detection ( 123 , 123 ) and pedestrian detection ( 123 ) .
several authors have also proposed to train convnets to directly predict the instantiation parameters of the objects to be located , such as the position relative to the viewing window , or the pose of the object .
for example osadchy et al .
( 123 ) describe a convnet for simultaneous face detection and pose estimation .
faces are represented by a 123d manifold in the nine - dimensional output space .
positions on the manifold indicate the pose ( pitch , yaw , and roll ) .
when the training image is a face , the network is trained to produce a point on the manifold at the location of the known pose .
if the image is not a face , the output is pushed away from the manifold .
at test time , the distance to the manifold indicate whether the image contains a face , and the position of the closest point on the manifold indicates pose .
taylor et al .
( 123 , 123 ) use a convnet to estimate the location of body parts ( hands , head , etc ) so as to derive the human body pose .
they use a metric learning criterion to train the network to produce points on a body pose manifold .
hinton et al .
have also proposed to train networks to compute explicit instantiation parameters of features as part of a recognition
other authors have proposed to perform object localization via convnet - based segmentation .
the simplest approach consists in training the convnet to classify the central pixel ( or voxel for vol - umetric images ) of its viewing window as a boundary between regions or not ( 123 ) .
but when the regions must be categorized , it is preferable to perform semantic segmentation .
the main idea is to train the convnet to classify the central pixel of the viewing window with the category of the ob - ject it belongs to , using the window as context for the decision .
applications range from biological image analysis ( 123 ) , to obstacle tagging for mobile robots ( 123 ) to tagging of photos ( 123 ) .
the ad - vantage of this approach is that the bounding contours need not be rectangles , and the regions need not be well - circumscribed objects .
the disadvantage is that it requires dense pixel - level labels for training .
this segmentation pre - processing or object proposal step has recently gained popularity in traditional computer vision to reduce the search space of position , scale and aspect ratio for detec - tion ( 123 , 123 , 123 , 123 ) .
hence an expensive classication method can be applied at the optimal location in the search space , thus increasing recognition accuracy .
additionally , ( 123 , 123 ) suggest that these methods improve accuracy by drastically reducing unlikely object regions , hence reducing potential false positives .
our dense sliding window method , however , is able to outperform object proposal methods on the ilsvrc123 detection dataset .
krizhevsky et al .
( 123 ) recently demonstrated impressive classication performance using a large convnet .
the authors also entered the imagenet 123 competition , winning both the classication and localization challenges .
although they demonstrated an impressive localization performance , there has been no published work describing how their approach .
our paper is thus the rst to provide a clear explanation how convnets can be used for localization and detection for imagenet
in this paper we use the terms localization and detection in a way that is consistent with their use in the imagenet 123 competition , namely that the only difference is the evaluation criterion used and both involve predicting the bounding box for each object in the image .
figure 123 : localization ( top ) and detection tasks ( bottom ) .
the left images contains our predic - tions ( ordered by decreasing condence ) while the right images show the groundtruth labels .
the detection image ( bottom ) illustrates the higher difculty of the detection dataset , which can contain many small objects while the classication and localization images typically contain a single large
123 vision tasks
in this paper , we explore three computer vision tasks in increasing order of difculty : ( i ) classi - cation , ( ii ) localization , and ( iii ) detection .
each task is a sub - task of the next .
while all tasks are adressed using a single framework and a shared feature learning base , we will describe them separately in the following sections .
throughout the paper , we report results on the 123 imagenet large scale visual recognition chal - lenge ( ilsvrc123 ) .
in the classication task of this challenge , each image is assigned a single label corresponding to the main object in the image .
five guesses are allowed to nd the correct answer ( this is because images can also contain multiple unlabeled objects ) .
the localization task is similar in that 123 guesses are allowed per image , but in addition , a bounding box for the predicted object must be returned with each guess .
to be considered correct , the predicted box must match the groundtruth by at least 123% ( using the pascal criterion of union over intersection ) , as well as be labeled with the correct class ( i . e .
each prediction is a label and bounding box that are associated together ) .
the detection task differs from localization in that there can be any number of objects in each image ( including zero ) , and false positives are penalized by the mean average precision
( map ) measure .
the localization task is a convenient intermediate step between classication and detection , and allows us to evaluate our localization method independently of challenges specic to detection ( such as learning a background class ) .
in fig .
123 , we show examples of images with our localization / detection predictions as well as corresponding groundtruth .
note that classication and localization share the same dataset , while detection also has additional data where objects can be smaller .
the detection data also contain a set of images where certain objects are absent .
this can be used for bootstrapping , but we have not made use of it in this work .
our classication architecture is similar to the best ilsvrc123 architecture by krizhevsky et al .
however , we improve on the network design and the inference step .
because of time constraints , some of the training features in krizhevskys model were not explored , and so we expect our results can be improved even further .
these are discussed in the future work section 123
figure 123 : layer 123 ( top ) and layer 123 lters ( bottom ) .
123 model design and training
we train the network on the imagenet 123 training set ( 123 million images and c = 123 classes ) ( 123 ) .
our model uses the same xed input size approach proposed by krizhevsky et al .
( 123 ) during training but turns to multi - scale for classication as described in the next section .
each image is downsampled so that the smallest dimension is 123 pixels .
we then extract 123 random crops ( and their horizontal ips ) of size 123x123 pixels and present these to the network in mini - batches of size 123
the weights in the network are initialized randomly with ( , ) = ( 123 , 123 123 ) .
they are then updated by stochastic gradient descent , accompanied by momentum term of 123 and an 123 weight decay of 123 123
the learning rate is initially 123 123 and is successively decreased by a factor of 123 after ( 123 , 123 , 123 , 123 , 123 ) epochs .
dropout ( 123 ) with a rate of 123 is employed on the fully connected layers ( 123th and 123th ) in the classier .
we detail the architecture sizes in tables 123 and 123
note that during training , we treat this architecture as non - spatial ( output maps of size 123x123 ) , as opposed to the inference step , which produces spatial outputs .
layers 123 - 123 are similar to krizhevsky et al .
( 123 ) , using rectication ( relu ) non - linearities and max pooling , but with the following differences : ( i ) no contrast normalization is used; ( ii ) pooling regions are non - overlapping and ( iii ) our model has larger 123st and 123nd layer feature maps , thanks to a smaller stride ( 123 instead of 123 ) .
a larger stride is benecial for speed but will hurt accuracy .
spatial input size
conv + max
conv + max
conv + max
table 123 : architecture specics for fast model .
the spatial size of the feature maps depends on the input image size , which varies during our inference step ( see table 123 in the appendix ) .
here we show training spatial sizes .
layer 123 is the top convolutional layer .
subsequent layers are fully connected , and applied in sliding window fashion at test time .
the fully - connected layers can also be seen as 123x123 convolutions in a spatial setting .
similar sizes for accurate model can be found in
in fig .
123 , we show the lter coefcients from the rst two convolutional layers .
the rst layer lters capture orientated edges , patterns and blobs .
in the second layer , the lters have a variety of forms , some diffuse , others with strong line structures or oriented edges .
123 feature extractor
along with this paper , we release a feature extractor named overfeat 123 in order to provide power - ful features for computer vision research .
two models are provided , a fast and accurate one .
each architecture is described in tables 123 and 123
we also compare their sizes in table 123 in terms of param - eters and connections .
the accurate model is more accurate than the fast one ( 123% classication error as opposed to 123% in table 123 ) , however it requires nearly twice as many connections .
using a committee of 123 accurate models reaches 123% classication error as shown in fig
123 multi - scale classication
in ( 123 ) , multi - view voting is used to boost performance : a xed set of 123 views ( 123 corners and center , with horizontal ip ) is averaged .
however , this approach can ignore many regions of the image , and is computationally redundant when views overlap .
additionally , it is only applied at a single scale , which may not be the scale at which the convnet will respond with optimal condence .
instead , we explore the entire image by densely running the network at each location and at multiple scales .
while the sliding window approach may be computationally prohibitive for certain types of model , it is inherently efcient in the case of convnets ( see section 123 ) .
this approach yields signicantly more views for voting , which increases robustness while remaining efcient .
the result of convolving a convnet on an image of arbitrary size is a spatial map of c - dimensional vectors at
however , the total subsampling ratio in the network described above is 123x123x123x123 , or 123
hence when applied densely , this architecture can only produce a classication vector every 123 pixels in the input dimension along each axis .
this coarse distribution of outputs decreases performance compared to the 123 - view scheme because the network windows are not well aligned with the objects in the images .
the better aligned the network window and the object , the strongest the condence of the network response .
to circumvent this problem , we take an approach similar to that introduced by giusti et al .
( 123 ) , and apply the last subsampling operation at every offset .
this removes the loss of resolution from this layer , yielding a total subsampling ratio of x123 instead of x123
we now explain in detail how the resolution augmentation is performed .
we use 123 scales of input which result in unpooled layer 123 maps of varying resolution ( see table 123 for details ) .
these are then pooled and presented to the classier using the following procedure , illustrated in fig
( a ) for a single image , at a given scale , we start with the unpooled layer 123 feature maps .
( b ) each of unpooled maps undergoes a 123x123 max pooling operation ( non - overlapping regions ) ,
repeated 123x123 times for ( x , y ) pixel offsets of ( 123 , 123 , 123 ) .
( c ) this produces a set of pooled feature maps , replicated ( 123x123 ) times for different ( x , y ) com -
( d ) the classier ( layers 123 , 123 , 123 ) has a xed input size of 123x123 and produces a c - dimensional output vector for each location within the pooled maps .
the classier is applied in sliding - window fashion to the pooled maps , yielding c - dimensional output maps ( for a given ( x , y ) combi -
( e ) the output maps for different ( x , y ) combinations are reshaped into a single 123d output map
( two spatial dimensions x c classes ) .
figure 123 : 123d illustration ( to scale ) of output map computation for classication , using y - dimension from scale 123 as an example ( see table 123 ) .
( a ) : 123 pixel unpooled layer 123 feature map .
( b ) : max pooling over non - overlapping 123 pixel groups , using offsets of = ( 123 , 123 , 123 ) pixels ( red , green , blue respectively ) .
( c ) : the resulting 123 pixel pooled maps , for different .
( d ) : 123 pixel classier ( layers 123 , 123 ) is applied in sliding window fashion to pooled maps , yielding 123 pixel by c maps for each .
( e ) : reshaped into 123 pixel by c output maps .
these operations can be viewed as shifting the classiers viewing window by 123 pixel through pool - ing layers without subsampling and using skip - kernels in the following layer ( where values in the neighborhood are non - adjacent ) .
or equivalently , as applying the nal pooling layer and fully - connected stack at every possible offset , and assembling the results by interleaving the outputs .
the procedure above is repeated for the horizontally ipped version of each image .
we then produce the nal classication by ( i ) taking the spatial max for each class , at each scale and ip; ( ii ) averaging the resulting c - dimensional vectors from different scales and ips and ( iii ) taking the top - 123 or top - 123 elements ( depending on the evaluation criterion ) from the mean class vector .
at an intuitive level , the two halves of the network i . e .
feature extraction layers ( 123 - 123 ) and classier layers ( 123 - output ) are used in opposite ways .
in the feature extraction portion , the lters are convolved across the entire image in one pass .
from a computational perspective , this is far more efcient than sliding a xed - size feature extractor over the image and then aggregating the results from different locations123
however , these principles are reversed for the classier portion of the network .
here , we want to hunt for a xed - size representation in the layer 123 feature maps across different positions and scales .
thus the classier has a xed - size 123x123 input and is exhaustively applied to the layer 123 maps .
the exhaustive pooling scheme ( with single pixel shifts ( x , y ) ) ensures that we can obtain ne alignment between the classier and the representation of the object in the feature map .
in table 123 , we experiment with different approaches , and compare them to the single network model of krizhevsky et al .
( 123 ) for reference .
the approach described above , with 123 scales , achieves a top - 123 error rate of 123% .
as might be expected , using fewer scales hurts performance : the single - scale model is worse with 123% top - 123 error .
the ne stride technique illustrated in fig .
123 brings a relatively small improvement in the single scale regime , but is also of importance for the multi - scale gains shown here .
123our network with 123 scales takes around 123 secs on a k123x gpu to process one image
krizhevsky et al .
( 123 ) overfeat - 123 fast model , scale 123 , coarse stride overfeat - 123 fast model , scale 123 , ne stride overfeat - 123 fast model , 123 scales ( 123 , 123 , 123 , 123 ) , ne stride overfeat - 123 fast model , 123 scales ( 123 - 123 ) , ne stride overfeat - 123 accurate model , 123 corners + center + ip overfeat - 123 accurate model , 123 scales , ne stride overfeat - 123 fast models , 123 scales , ne stride overfeat - 123 accurate models , 123 scales , ne stride
error % error %
table 123 : classication experiments on validation set .
fine / coarse stride refers to the number of values used when applying the classier .
fine : = 123 , 123 , 123; coarse : = 123
figure 123 : test set classication results .
during the competition , overfeat yielded 123% top 123 error rate using an average of 123 fast models .
in post - competition work , overfeat ranks fth with 123% error using bigger models ( more features and more layers ) .
we report the test set results of the 123 competition in fig .
123 where our model ( overfeat ) obtained 123% accuracy by voting of 123 convnets ( each trained with different initializations ) and ranked 123th out of 123 teams .
the best accuracy using only ilsvrc123 data was 123% .
pre - training with extra data from the imagenet fall123 dataset improved this number to 123% .
in post - competition work , we improve the overfeat results down to 123% error by using bigger models ( more features and more layers ) .
due to time constraints , these bigger models are not fully trained , more improvements are expected to appear in time .
123 convnets and sliding window efciency
in contrast to many sliding - window approaches that compute an entire pipeline for each window of the input one at a time , convnets are inherently efcient when applied in a sliding fashion because they naturally share computations common to overlapping regions .
when applying our network to larger images at test time , we simply apply each convolution over the extent of the full image .
this extends the output of each layer to cover the new image size , eventually producing a map of output class predictions , with one spatial location for each window ( eld of view ) of input
figure 123 : the efciency of convnets for detection .
during training , a convnet produces only a single spatial output ( top ) .
but when applied at test time over a larger image , it produces a spatial output map , e . g .
123x123 ( bottom ) .
since all layers are applied convolutionally , the extra computa - tion required for the larger image is limited to the yellow regions .
this diagram omits the feature dimension for simplicity .
is diagrammed in fig .
convolutions are applied bottom - up , so that the computations common to neighboring windows need only be done once .
note that the last layers of our architecture are fully connected linear layers .
at test time , these layers are effectively replaced by convolution operations with kernels of 123x123 spatial extent .
the entire convnet is then simply a sequence of convolutions , max - pooling and thresholding operations
starting from our classication - trained network , we replace the classier layers by a regression network and train it to predict object bounding boxes at each spatial location and scale .
we then combine the regression predictions together , along with the classication results at each location , as we now describe .
123 generating predictions
to generate object bounding box predictions , we simultaneously run the classier and regressor networks across all locations and scales .
since these share the same feature extraction layers , only the nal regression layers need to be recomputed after computing the classication network .
the output of the nal softmax layer for a class c at each location provides a score of condence that an object of class c is present ( though not necessarily fully contained ) in the corresponding eld of view .
thus we can assign a condence to each bounding box .
123 regressor training
the regression network takes as input the pooled feature maps from layer 123
it has 123 fully - connected hidden layers of size 123 and 123 channels , respectively .
the nal output layer has 123 units which specify the coordinates for the bounding box edges .
as with classication , there are ( 123x123 ) copies throughout , resulting from the x , y shifts .
the architecture is shown in fig
figure 123 : localization / detection pipeline .
the raw classier / detector outputs a class and a con - dence for each location ( 123st diagram ) .
the resolution of these predictions can be increased using the method described in section 123 ( 123nd diagram ) .
the regression then predicts the location scale of the object with respect to each window ( 123rd diagram ) .
these bounding boxes are then merge and accumulated to a small number of objects ( 123th diagram ) .
figure 123 : examples of bounding boxes produced by the regression network , before being com - bined into nal predictions .
the examples shown here are at a single scale .
predictions may be more optimal at other scales depending on the objects .
here , most of the bounding boxes which are initially organized as a grid , converge to a single location and scale .
this indicates that the network is very condent in the location of the object , as opposed to being spread out randomly .
the top left image shows that it can also correctly identify multiple location if several objects are present .
the various aspect ratios of the predicted bounding boxes shows that the network is able to cope with various object poses .
we x the feature extraction layers ( 123 - 123 ) from the classication network and train the regression network using an 123 loss between the predicted and true bounding box for each example .
the nal regressor layer is class - specic , having 123 different versions , one for each class .
we train this network using the same set of scales as described in section 123
we compare the prediction of the regressor net at each spatial location with the ground - truth bounding box , shifted into the frame of reference of the regressors translation offset within the convolution ( see fig .
however , we do not train the regressor on bounding boxes with less than 123% overlap with the input eld of view : since the object is mostly outside of these locations , it will be better handled by regression windows that do contain the object .
training the regressors in a multi - scale manner is important for the across - scale prediction combi - nation .
training on a single scale will perform well on that scale and still perform reasonably on other scales .
however training multi - scale will make predictions match correctly across scales and exponentially increase the condence of the merged predictions .
in turn , this allows us to perform well with a few scales only , rather than many scales as is typically the case in detection .
the typical ratio from one scale to another in pedestrian detection ( 123 ) is about 123 to 123 , here however we use a large ratio of approximately 123 ( this number differs for each scale since dimensions are adjusted to t exactly the stride of our network ) which allows us to run our system faster .
123 combining predictions
we combine the individual predictions ( see fig .
123 ) via a greedy merge strategy applied to the regres - sor bounding boxes , using the following algorithm .
( a ) assign to cs the set of classes in the top k for each scale s 123 .
123 , found by taking the
maximum detection class outputs across spatial locations for that scale .
( b ) assign to bs the set of bounding boxes predicted by the regressor network for each class in cs ,
across all spatial locations at scale s .
figure 123 : application of the regression network to layer 123 features , at scale 123 , for example .
( a ) the input to the regressor at this scale are 123x123 pixels spatially by 123 channels for each of the ( 123x123 ) x , y shifts .
( b ) each unit in the 123st layer of the regression net is connected to a 123x123 spatial neighborhood in the layer 123 maps , as well as all 123 channels .
shifting the 123x123 neighborhood around results in a map of 123x123 spatial extent , for each of the 123 channels in the layer , and for each of the ( 123x123 ) x , y shifts .
( c ) the 123nd regression layer has 123 units and is fully connected ( i . e .
the purple element only connects to the purple element in ( b ) , across all 123 channels ) .
( d ) the output of the regression network is a 123 - vector ( specifying the edges of the bounding box ) for each location in the 123x123 map , and for each of the ( 123x123 ) x , y shifts .
( c ) assign b ss bs ( d ) repeat merging until done :
123 ) = argminb123=b123bmatch score ( b123 , b123 )
if match score ( b
123 ) > t , stop .
otherwise , set b b\ ( b
123 ) box merge ( b
in the above , we compute match score using the sum of the distance between centers of the two bounding boxes and the intersection area of the boxes .
box merge compute the average of the bounding boxes coordinates .
the nal prediction is given by taking the merged bounding boxes with maximum class scores .
this is computed by cumulatively adding the detection class outputs associated with the input windows from which each bounding box was predicted .
see fig .
123 for an example of bounding boxes merged into a single high - condence bounding box .
in that example , some turtle and whale bounding boxes appear in the intermediate multi - scale steps , but disappear in the nal detection image .
not only do these bounding boxes have low classication condence ( at most 123 and 123 respectively ) , their collection is not as coherent as the bear bounding boxes to get a signicant condence boost .
the bear boxes have a strong condence ( approximately 123 on average per scale ) and high matching scores .
hence after merging , many bear bounding boxes are fused into a single very high condence box , while false positives disappear below the detection threshold due their lack of bounding box coherence and condence .
this analysis suggest that our approach is naturally more robust to false positives coming from the pure - classication model than traditional non - maximum suppression , by rewarding bounding box coherence .
figure 123 : localization experiments on ilsvrc123 validation set .
we experiment with different number of scales and with the use of single - class regression ( scr ) or per - class regression ( pcr ) .
we apply our network to the imagenet 123 validation set using the localization criterion specied for the competition .
the results for this are shown in fig .
123 shows the results of the 123 and 123 localization competitions ( the train and test data are the same for both of these years ) .
our method is the winner of the 123 competition with 123% error .
our multiscale and multi - view approach was critical to obtaining good performance , as can be seen in fig .
123 : using only a single centered crop , our regressor network achieves an error rate of 123% .
by combining regressor predictions from all spatial locations at two scales , we achieve a vastly better error rate of 123% .
adding a third and fourth scale further improves performance to 123% error .
using a different top layer for each class in the regressor network for each class ( per - class regres - sor ( pcr ) in fig .
123 ) surprisingly did not outperform using only a single network shared among all classes ( 123% vs .
123% ) .
this may be because there are relatively few examples per class an - notated with bounding boxes in the training set , while the network has 123 times more top - layer parameters , resulting in insufcient training .
it is possible this approach may be improved by shar - ing parameters only among similar classes ( e . g .
training one network for all classes of dogs , another for vehicles , etc . ) .
detection training is similar to classication training but in a spatial manner .
multiple location of an image may be trained simultaneously .
since the model is convolutional , all weights are shared among all locations .
the main difference with the localization task , is the necessity to predict a background class when no object is present .
traditionally , negative examples are initially taken at random for training .
then the most offending negative errors are added to the training set in boot - strapping passes .
independent bootstrapping passes render training complicated and risk potential mismatches between the negative examples collection and training times .
additionally , the size of bootstrapping passes needs to be tuned to make sure training does not overt on a small set .
to cir - cumvent all these problems , we perform negative training on the y , by selecting a few interesting negative examples per image such as random ones or most offending ones .
this approach is more computationally expensive , but renders the procedure much simpler .
and since the feature extraction is initially trained with the classication task , the detection ne - tuning is not as long anyway .
in fig .
123 , we report the results of the ilsvrc 123 competition where our detection system ranked 123rd with 123% mean average precision ( map ) .
we later established a new detection state of the art with 123% map .
note that there is a large gap between the top 123 methods and other teams ( the 123th
figure 123 : ilsvrc123 and ilsvrc123 competitions results ( test set ) .
our entry is the winner of the ilsvrc123 localization competition with 123% error ( top 123 ) .
note that training and testing data is the same for both years .
the overfeat entry uses 123 scales and a single - class regression approach .
figure 123 : ilsvrc123 test set detection results .
during the competition , uva ranked rst with 123% map .
in post competition work , we establish a new state of the art with 123% map .
systems marked with * were pre - trained with the ilsvrc123 classication data .
method yields 123% map ) .
additionally , our approach is considerably different from the top 123 other systems which use an initial segmentation step to reduce candidate windows from approximately 123 , 123 to 123 , 123
this technique speeds up inference and substantially reduces the number of potential false positives .
( 123 , 123 ) suggest that detection accuracy drops when using dense sliding window as opposed to selective search which discards unlikely object locations hence reducing false positives .
combined with our method , we may observe similar improvements as seen here between traditional dense methods and segmentation based methods .
it should also be noted that
we did not ne tune on the detection validation set as nec and uva did .
the validation and test set distributions differ signicantly enough from the training set that this alone improves results by approximately 123 point .
the improvement between the two overfeat results in fig .
123 are due to longer training times and the use of context , i . e .
each scale also uses lower resolution scales as
we have presented a multi - scale , sliding window approach that can be used for classication , lo - calization and detection .
we applied it to the ilsvrc 123 datasets , and it currently ranks 123th in classication , 123st in localization and 123st in detection .
a second important contribution of our paper is explaining how convnets can be effectively used for detection and localization tasks .
these were never addressed in ( 123 ) and thus we are the rst to explain how this can be done in the context of im - agenet 123
the scheme we propose involves substantial modications to networks designed for classication , but clearly demonstrate that convnets are capable of these more challenging tasks .
our localization approach won the 123 ilsvrc competition and signicantly outperformed all 123 and 123 approaches .
the detection model was among the top performers during the compe - tition , and ranks rst in post - competition results .
we have proposed an integrated pipeline that can perform different tasks while sharing a common feature extraction base , entirely learned directly from the pixels .
our approach might still be improved in several ways .
( i ) for localization , we are not currently back - propping through the whole network; doing so is likely to improve performance .
( ii ) we are using 123 loss , rather than directly optimizing the intersection - over - union ( iou ) criterion on which performance is measured .
swapping the loss to this should be possible since iou is still differen - tiable , provided there is some overlap .
( iii ) alternate parameterizations of the bounding box may help to decorrelate the outputs , which will aid network training .
