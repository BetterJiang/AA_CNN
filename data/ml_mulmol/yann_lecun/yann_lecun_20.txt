them as mid - level features .
many successful models for scene or object recognition transform low - level descriptors ( such as gabor lter re - sponses , or sift descriptors ) into richer representations of intermediate complexity .
this process can often be bro - ken down into two steps : ( 123 ) a coding step , which per - forms a pointwise transformation of the descriptors into a representation better adapted to the task , and ( 123 ) a pool - ing step , which summarizes the coded features over larger neighborhoods .
several combinations of coding and pool - ing schemes have been proposed in the literature .
the goal of this paper is threefold .
we seek to establish the rela - tive importance of each step of mid - level feature extrac - tion through a comprehensive cross evaluation of several types of coding modules ( hard and soft vector quantization , sparse coding ) and pooling schemes ( by taking the aver - age , or the maximum ) , which obtains state - of - the - art per - formance or better on several recognition benchmarks .
we show how to improve the best performing coding scheme by learning a supervised discriminative dictionary for sparse coding .
we provide theoretical and empirical insight into the remarkable performance of max pooling .
by teasing apart components shared by modern mid - level feature ex - tractors , our approach aims to facilitate the design of better
popular examples of mid - level features include bags of features ( 123 ) , spatial pyramids ( 123 ) , and the upper units of convolutional networks ( 123 ) or deep belief networks ( 123 , 123 ) .
extracting these mid - level features involves a sequence of interchangeable modules similar to that identied by winder and brown for local image descriptors ( 123 ) .
in this paper , we focus on two types of modules :
coding : input features are locally transformed into representations that have some desirable properties such as compactness , sparseness ( i . e . , most compo - nents are 123 ) , or statistical independence .
the code is typically a vector with binary ( vector quantization ) or continuous ( hog , sparse coding ) entries , obtained by decomposing the original feature on some codebook ,
spatial pooling : the codes associated with local im - age features are pooled over some image neighborhood ( e . g . , the whole image for bags of features , a coarse grid of cells for the hog approach to pedestrian de - tection , or a coarse hierarchy of cells for spatial pyra - mids ) .
the codes within each cell are summarized by a single semi - local feature vector , common examples being the average of the codes ( average pooling ) or their maximum ( max pooling ) .
finding good image features is critical in modern ap - proaches to category - level image classication .
many methods rst extract low - level descriptors ( e . g . , sift ( 123 ) or hog descriptors ( 123 ) ) at interest point locations , or nodes in a dense grid .
this paper considers the problem of com - bining these local features into a global image representa - tion suited to recognition using a common classier such as a support vector machine .
since global features built upon low - level ones typically remain close to image - level infor - mation without attempts at high - level , structured image de - scription ( in terms of parts for example ) , we will refer to
123willow project - team , laboratoire dinformatique de lecole nor -
male superieure , ens / inria / cnrs umr 123
the same coding and pooling modules can be plugged into various architectures .
for example , average pooling is found in convolutional nets ( 123 ) , bag - of - features meth - ods , and hog descriptors; max pooling is found in convo - lutional nets ( 123 , 123 ) , hmax nets ( 123 ) , and state - of - the - art variants of the spatial pyramid model ( 123 ) .
the nal global vector is formed by concatenating with suitable weights the semi - local vectors obtained for each pooling region .
high levels of performance have been reported for spe - cic pairings of coding and pooling modules ( e . g . , sparse coding and max pooling ( 123 ) ) , but it is not always clear whether the improvement can be factored into independent contributions of each module ( e . g . , whether the better per - formance of max pooling would generalize to systems us - ing vector quantization instead of sparse coding ) .
in this
work , we address this concern by presenting a comprehen - sive set of product pairings across known coding ( hard and soft vector quantization , sparse coding ) and pooling ( aver - age and max pooling ) modules .
we have chosen to restrict ourselves to the spatial pyramid framework since it has al - ready been used in several comparative studies ( 123 , 123 ) , dening the state of the art on several benchmarks; but the insights gained within that framework should easily gen - eralize to other models ( e . g . , models using interest point detectors , convolutional networks , deep belief networks ) .
two striking results of our evaluation are that ( 123 ) sparse coding systematically outperforms the other coding mod - ules , irrespective of the pooling module , and ( 123 ) max pool - ing dramatically improves linear classication performance irrespective of the coding module , to the point that the worst - performing coding module ( hard vector quantization ) paired with max pooling outperforms the best coding mod - ule ( sparse coding ) paired with average pooling .
the rest of our paper builds on these two ndings .
noting that the dic - tionary used to perform sparse coding is trained to minimize reconstruction error , which might be suboptimal for classi - cation , we propose a new supervised dictionary learning algorithm .
as for the superiority of max pooling in lin - ear classication , we complement the empirical nding by a theoretical analysis and new experiments .
our article thus makes three contributions :
we systematically explore combinations of known modules appearing in the unied model presented in this paper , obtaining state - of - the - art results on two benchmarks ( sec
we introduce a novel supervised sparse dictionary
learning algorithm ( sec
we present theoretical and experimental insights into the much better linear discrimination performance ob - tained with max pooling compared to average pooling , in a large variety of settings ( sec
notation and related work
in this section , we introduce some notation used through - out this paper , and present coding and pooling modules pre - viously used by other authors .
let an image i be repre - sented by a set of low - level descriptors ( e . g . , sift ) xi at n locations identied with their indices i = 123 , , n .
m regions of interests are dened on the image ( e . g . , the 123 = 123 + 123 + 123 cells of a three - level spatial pyramid ) , with nm denoting the set of locations / indices within region m .
let f and g denote some coding and pooling operators , respectively .
the vector z representing the whole image is obtained by sequentially coding , pooling over all regions ,
i = f ( xi ) , i = 123 , , n hm = g ( cid : 123 ) ( i ) inm ( cid : 123 ) , m = 123 , , m
zt = ( ht
the goal is to determine which operators f and g provide the best classication performance using z as input to either a non - linear intersection kernel svm ( 123 ) , or a linear svm .
in the usual bag - of - features framework ( 123 ) , f mini - mizes the distance to a codebook , usually learned by an un - supervised algorithm ( e . g . , k - means ) , and g computes the average over the pooling region :
i ( 123 , 123 ) k , i , j = 123 iff j = argmin
kk kxi dkk123
where dk denotes the k - th codeword .
note that averaging and using uniform weighting is equivalent ( up to a constant multiplicator ) to using histograms with weights inversely proportional to the area of the pooling regions , as in ( 123 ) .
van gemert et al .
( 123 ) have obtained improvements by
replacing hard quantization by soft quantization :
k=123 exp ( kxi dkk123
where is a parameter that controls the softness of the soft assignment ( hard assignment is the limit when ) .
this amounts to coding as in the e - step of the expectation - maximization algorithm to learn a gaussian mixture model , using codewords of the dictionary as centers .
sparse coding ( 123 ) uses a linear combination of a small number of codewords to approximate the xi .
yang et al .
( 123 ) have obtained state - of - the - art results by using sparse coding and max pooling :
i = argmin
hm , j = max
l ( , d ) , kxi dk123 i , j , for j = 123 , , k ,
123 + kk123 ,
where kk123 denotes the 123 norm of , is a parameter that controls the sparsity , and d is a dictionary trained by minimizing the average of l ( i , d ) over all samples , al - ternatively over d and the i .
it is well known that the 123 penalty induces sparsity and makes the problem tractable ( e . g . , ( 123 , 123 ) ) .
systematic evaluation of unsupervised mid -
this section offers comprehensive comparisons of unsu - pervised coding schemes .
in all experiments , we use the
caltech - 123 , 123 training examples
123 scenes , 123 training examples
results with basic features , sift extracted each 123 pixels
123 123 ( 123 )
123 123 ( 123 ) hard quantization , linear kernel hard quantization , intersection kernel 123 123 ( 123 ) ( 123 ) 123 123 ( 123 ) 123 123 ( 123 ) soft quantization , linear kernel soft quantization , intersection kernel sparse codes , linear kernel sparse codes , intersection kernel
123 123 ( 123 ) 123 123 ( 123 ) ( 123 ) 123 123 ( 123 ) 123 123 ( 123 ) 123 123 ( 123 )
123 123 ( 123 ) 123 123 ( 123 ) 123 123 ( 123 ) ( 123 ) 123 123 ( 123 ) 123 123 ( 123 ) 123 123 ( 123 ) 123 123 ( 123 ) ( 123 ) 123 123 ( 123 )
123 123 ( 123 ) ( 123 ) 123 123 ( 123 ) 123 123 ( 123 ) ( 123 ) 123 123 ( 123 )
123 123 ( 123 ) ( 123 ) 123 123 ( 123 ) ( 123 )
results with macrofeatures and denser sift sampling
123 123 ( 123 ) hard quantization , linear kernel hard quantization , intersection kernel 123 123 ( 123 ) soft quantization , linear kernel soft quantization , intersection kernel sparse codes , linear kernel sparse codes , intersection kernel
123 123 ( 123 ) 123 123 ( 123 ) 123 123 ( 123 ) 123 123 ( 123 )
123 123 ( 123 ) 123 123 ( 123 ) 123 123 ( 123 ) 123 123 ( 123 ) 123 123 ( 123 ) 123 123 ( 123 )
123 123 ( 123 ) 123 123 ( 123 ) 123 123 ( 123 ) 123 123 ( 123 ) 123 123 ( 123 ) 123 123 ( 123 )
123 123 ( 123 ) 123 123 ( 123 ) 123 123 ( 123 ) 123 123 ( 123 ) 123 123 ( 123 ) 123 123 ( 123 )
table 123
average recognition rate on caltech - 123 and 123 - scenes benchmarks , for various combinations of coding , pooling , and classier types .
the codebook size shown inside brackets is the one that gives the best results among 123 , 123 and 123
linear and histogram intersection kernels are identical when using hard quantization with max pooling ( since taking the minimum or the product is the same for binary vectors ) , but results have been included for both to preserve the symmetry of the table .
top : results with the baseline sift sampling density of 123 pixels and standard features .
bottom : results with the set of parameters for sift sampling density and macrofeatures giving the best performance for sparse coding .
boiman et al .
( 123 ) jain et al .
( 123 ) lazebnik et al .
( 123 ) van gemert et al .
( 123 ) yang et al .
( 123 ) yang et al .
( 123 ) zhang et al .
( 123 ) zhou et al
nearest neighbor + spatial correspondence fast image search for learned metrics ( 123 ) sp + hard quantization + kernel svm ( 123 ) sp + soft quantization + kernel svm ( 123 ) sp + sparse codes + max pooling + linear svm ( 123 ) sp + sparse codes + max pooling + kernel svm sp + gaussian mixture
caltech 123 tr .
caltech 123 tr .
table 123
results obtained by several recognition schemes using a single type of descriptors .
bold numbers in parentheses preceding the method description indicate methods reimplemented in this paper .
sp : spatial pyramid .
caltech - 123 ( 123 ) and scenes datasets ( 123 ) as benchmarks .
these datasets respectively comprise 123 object categories ( plus a background category ) and fteen scene categories .
following the usual procedure ( 123 , 123 ) , we use 123 train - ing images and the rest for testing ( with a maximum of 123 test images ) on the caltech - 123 dataset , and 123 training images and the rest for testing on the scenes dataset .
ex - periments are conducted over 123 random splits of the data , and we report the mean accuracy and its standard devia - tion .
hyperparameters of the model are selected by cross - validation within the training set .
the general architecture follows ( 123 ) .
low - level descriptors xi are 123 - dimensional sift descriptors ( 123 ) of 123 123 patches .
the descriptors are extracted on a dense grid rather than at interest points , as this procedure has been shown to yield superior scene classication ( 123 ) .
pooling regions m comprise the cells of 123 123 , 123 123 and 123 123 grids ( forming a three - level pyramid ) .
we use the spams toolbox ( 123 ) to compute sparse codes .
interaction between modules
here , we perform a systematic cross evaluation of all the coding , pooling and classier types presented in sec .
123 , with sift descriptors extracted densely every 123 pixels .
results are presented on table 123
the ranking of performance when changing a particular module ( e . g . , coding ) is quite consis -
sparse coding improves over soft quantization , which
improves over hard quantization;
max pooling almost always improves over average
pooling , dramatically so when using a linear svm;
the intersection kernel svm performs similarly or
better than the linear svm .
in particular , the global feature obtained when using hard vector quantization with max pooling achieves high accu -
racy with a linear classier , while being binary , and merely recording the presence or absence of each codeword in the pools .
while much research has been devoted to devising the best possible coding module , our results show that with linear classication , switching from average to max pooling increases accuracy more than switching from hard quanti - zation to sparse coding .
these results could serve as guide - lines for the design of future architectures .
for comparison , previously published results obtained using one type of descriptors on the same dataset are shown on table 123
note that better performance has been re - ported with multiple descriptor types ( e . g . , methods using multiple kernel learning have achieved 123% 123 ( 123 ) and 123% 123 ( 123 , 123 ) on caltech - 123 with 123 train - ing examples ) , or subcategory learning ( 123% on caltech - 123 ( 123 ) ) .
the coding and pooling module combinations used in ( 123 , 123 ) are included in our comparative evaluation ( bold numbers in parentheses on tables 123 and 123 ) .
over - all , our results conrm the experimental ndings in these works , except that we do not nd superior performance for the linear svm , compared to the intersection kernel svm , with sparse codes and max pooling , contrary to yang et al .
results of our reimplementation are similar to those in ( 123 ) .
the better performance than that reported by van gemert et al .
( 123 ) or yang et al .
( 123 ) on the scenes is not surprising since their baseline accuracy for the method in ( 123 ) is also lower , which they attributed to implementa - tion differences .
discrepancies with results from yang et al .
( 123 ) may arise from their using a differentiable quadratic hinge loss instead of the standard hinge loss in the svm , and a different type of normalization for sift descriptors .
macrofeatures and denser sift sampling
in convolutional neural networks ( e . g . , ( 123 , 123 ) ) , spa - tial neighborhoods of low - level features are encoded jointly .
on the other hand , codewords in bag - of - features methods usually encode low - level features at a single location ( see fig .
we propose to adapt the joint encoding scheme to the spatial pyramid framework .
jointly encoding l descriptors in a local spatial neigh -
borhood li amounts to replacing eq .
( 123 ) by :
figure 123
standard features encode the sift features at a single spatial point .
macrofeatures jointly encode small spatial neigh - borhoods of sift features ( i . e . , the input of the coding module is formed by concatenating nearby sift descriptors ) .
we have experimented with different macrofeature pa - rameters , and denser sampling of the underlying sift de - scriptor map ( e . g . , extracting sift every 123 pixels instead of 123 pixels as in the baseline of ( 123 ) ) .
we have tested sampling densities of 123 to 123 , and macrofeatures of side length 123 to 123 and subsampling parameter 123 to 123
when using sparse cod - ing and max pooling , the best parameters ( selected by cross - validation within the training set ) for sift sampling den - sity , macrofeature side length and subsampling parameter are respectively of 123 , 123 , 123 for the caltech - 123 dataset , and 123 , 123 , 123 for the scenes dataset .
our results ( table 123 , bottom ) show that large improvements can be gained on the caltech - 123 benchmark , by merely sampling sift descriptors more nely , and jointly representing nearby descriptors , yielding a classication accuracy of 123% , which to the best of our knowledge is signicantly better than all published classi - cation schemes using a single type of low - level descriptor .
however , we have not found ner sampling and joint encod - ing to help recognition signicantly on the scenes dataset .
i = f ( ( xt
il ) t ) , i123 , , il li .
discriminative dictionaries
in the following , we call macrofeatures vectors that jointly encode a small neighborhood of sift descriptors .
the encoded neighborhoods are squares determined by two the side of the square ( e . g . , 123 123 square on fig .
123 ) , and a subsampling parameter determining how many sift descriptors to skip along each dimension when selecting neighboring features .
for example , a 123 123 macro - feature with a subsampling parameter of 123 jointly encodes 123 descriptors out of a 123 123 grid , skipping every other column
the feature extraction schemes presented so far are all unsupervised .
when using sparse coding , an adaptive dic - tionary is learned by minimizing a regularized reconstruc - tion error .
while this ensures that the parameters of the dic - tionary are adapted to the statistics of the data , the dictio - nary is not optimized for the classication task .
in this sec - tion , we introduce a novel supervised method to learn the
several authors have proposed methods to obtain dis - criminative codebooks .
lazebnik and raginsky ( 123 ) incor -
porate discriminative information by minimizing the loss of mutual information between features and labels during the quantization step .
winn et al .
( 123 ) prune a large codebook iteratively by fusing codewords that do not contribute to dis - crimination .
however these methods are optimized for vec - tor quantization .
mairal et al .
( 123 ) have proposed an algo - rithm to train discriminative dictionaries for sparse coding , but it requires each encoded vector to be labelled .
instead , the approach we propose is adapted to global image statis -
with the same notation as before , let us consider the ex - traction of a global image representation by sparse coding and average pooling over the whole image i :
i = ( xt
i = argmin
il ) , i123 , , il li , l ( , d ) , k xi dk123
z = h .
123 + kk123 ,
consider a binary classication problem .
let z ( n ) de - note the global image representation for the n - th training image , and yn ( 123 , 123 ) the image label .
a linear classi - er is trained by minimizing with respect to parameter the regularized logistic cost :
log ( cid : 123 ) 123 + eynt z ( n ) ( cid : 123 ) + rkk123
where r denotes a regularization parameter .
we use logis - tic regression because its level of performance is typically similar to that of linear svms but unlike svms , its loss function is differentiable .
we want to minimize the super - vised cost cs with respect to d to obtain a more discrimi - native dictionary .
using the chain rule , we obtain :
yn ( cid : 123 ) 123 ( yn . z ( n ) ) ( cid : 123 ) t z ( n )
|i ( n ) | xii ( n )
( x ) = 123 / ( 123 + exp ( x ) ) .
we need to compute the gradient d ( i ) .
since the i minimize eq .
( 123 ) , they
t x sign ( ) ) ,
where we have dropped subscript i to limit notation clutter , and d denotes the columns corresponding to the active set of ( i . e . , the few columns of d used in the decomposi - tion of the input ) .
note that this formula cannot be used to
compute , as parts of the right - hand side of the equation depend on itself , but it can be used to compute a gradient once is known .
when perturbations of the dictionary are small , the active set of often stays the same ( since the cor - relation between the atoms of the dictionary and the input vector varies continuously with the dictionary ) .
assuming that it is constant , we can compute the gradient of the active coefcients with respect to the active columns of d ( setting it to 123 elsewhere ) :
= biakj j cki ,
a , ( d b , x d , c , ad
where k denotes the k - th non - zero component of .
we train the discriminative dictionary by stochastic gra - dient descent ( 123 , 123 ) .
recomputing the sparse decompo - sitions i at each location of a training image at each it - eration is costly .
to speed - up the computation while re - maining closer to global image statistics than with individ - ual patches , we approximate z ( n ) by pooling over a random sample of ten locations of the image .
furthermore , we up - date only a random subset of coordinates at each iteration , since computation of the gradient is costly .
we then test the dictionary with max pooling and a three - layer spatial pyra - mid , using either a linear or intersection kernel svm .
123 123 123 123 123 123 123 123
intersect 123 123 123 123 123 123 123 123
table 123
results of learning discriminative dictionaries on the scenes dataset , for dictionaries of size 123 ( left ) and 123 ( right ) , with 123 macrofeatures and grid resolution of 123 pixels ,
we compare performance of dictionaries of sizes 123 and 123 on the scenes dataset , encoding 123 neighbor - hoods of sift .
results ( table 123 ) show that discriminative dictionaries perform signicantly better than unsupervised dictionaries .
a discriminative dictionary of 123 code - words achieves 123% correct recognition performance , which to the best of our knowledge is the highest pub - lished classication accuracy on that dataset for a single fea - ture type .
discriminative training of dictionaries with our method on the caltech - 123 dataset has yielded only very little improvement , probably due to the scarcity of training
comparing average and max pooling
one of the most striking results of our comparative evaluation is that the superiority of max pooling over av -
erage pooling generalizes to many combinations of cod - ing schemes and classiers .
several authors have already stressed the efciency of max pooling ( 123 , 123 ) , but they have not given theoretical explanations to their ndings .
in this section , we study max pooling in more details theoreti - cally and experimentally .
a theoretical comparison of pooling strategies
with the same notation as before , consider a binary lin - ear classication task over cluttered images .
pooling is per - formed over the whole image , so that the pooled feature h is the global image representation .
linear classication requires distributions of h over examples from positive and negative classes ( henceforth denoted by + and ) to be well we model the distribution of image patches of a given class as a mixture of two distributions ( 123 ) : patches are taken from the actual class distribution ( foreground ) with probability ( 123 w ) , and from a clutter distribution ( back - ground ) with probability w , with clutter patches being present in both classes ( + or ) .
crucially , we model the amount of clutter w as varying between images ( while being xed for a given image ) .
there are then two sources of variance for the distribu - tion p ( h ) : the intrinsic variance caused by sampling from a nite pool for each image ( which causes the actual value of h over foreground patches to deviate from its expectation ) , and the variance of w ( which causes the expectation of h itself to uctuate from image to image depending on their if the pool cardinality n is large , average pooling is robust to intrinsic foreground variability , since the variance of the average decreases in 123 n .
this is usually not the case with max pooling , where the variance can in - crease with pool cardinality depending on the foreground
however , if the amount of clutter w has a high variance , it causes the distribution of the average over the image to spread , as the expectation of h for each image depends on w .
even if the foreground distributions are well separated , variance in the amount of clutter creates overlap between the mixture distributions if the mean of the background dis - tribution is much lower than that of the foreground distri - butions .
conversely , max pooling can be robust to clutter if the mean of the background distribution is sufciently low .
this is illustrated on fig .
123 , where we have plotted the empirical distributions of the average of 123 pooled features sharing the same parameters .
simulations are run using 123 images of each class , composed of n = 123 patches .
for each image , the clutter level w is drawn from a truncated normal distribution with either low ( top ) or high ( bottom ) variance .
local feature values at each patch are drawn from a mixture of exponential distributions , with a lower mean for background patches than foreground patches of either
average pooling , class + average pooling , class max pooling , class + max pooling , class
average pooling , class + average pooling , class max pooling , class + max pooling , class
figure 123
empirical probability densities of x = 123 simulated for two classes classes of images forming pools of car - dinality n = 123
the local features are drawn from one of three exponential distributions .
when the clutter is homogeneous across images ( top ) , the distributions are well separated for average pool - ing and max pooling .
when the clutter level has higher variance ( bottom ) , the max pooling distributions ( dashed lines ) are still well separated while the average pooling distributions ( solid lines ) start
when the clutter has high variance ( fig .
123 , bottom ) , distributions remain well separated with max pooling , but have signicant overlap with average pooling .
we now rene our analysis in two cases : sparse codes
and vector quantized codes .
123 . 123 sparse codes .
in the case of a positive decomposition over a dictionary , we model the distribution of the value of feature j for each patch by an exponential distribution with mean j , variance j , and density f ( x ) = 123 j .
the choice of an expo - nential distribution ( or a laplace distribution when decom - positions are not constrained to be positive ) to model sparse codes seems appropriate because it is highly kurtotic and sparse codes have heavy tails .
f ( x ) = 123 e x the max - pooled feature is f n ( x ) = ( 123e x
the corresponding cumulative distribution function is j .
the cumulative distribution function of j ) n for a pool of size n .
clutter patches are sampled from a distribution of mean b .
let nf and nb denote respectively the num - ber of foreground and background patches , n = nf + nb .
assuming nf and nb are large , taylor expansions of the cumulative distribution functions of the maxima yield that 123% of the probability mass of the maximum over the back - ground patches will be below 123% of the probability mass of the maximum over the foreground patches provided that
in a binary discrimi - nation task between two comparatively similar classes , if an image is cluttered by many background patches , with
nb < | log ( 123 ) | ( cid : 123 )
j and b j , max - pooling can be relatively
immune to background patches , while average - pooling can create overlap between the distributions ( see fig .
for example , if b < 123j and nf = 123 , having fewer than nb < 123 background patches virtually guarantees that the clutter will have no inuence on the value of the maxi - mum .
conversely , if nb < nf nf , clutter will have little inuence for b up to j .
thus , max - pooling creates immunity to two different types of clutter : ubiqui - tous with low feature activation , and infrequent with higher
123 | log ( 123 ) |
however , a downside is that the ratio of the mean to the standard deviation of the maximum distribution does not , as in the case of the distribution of the average .
in fact , the mean and variance of the maximum distribution over n samples can be shown to be :
= ( h ( n ) ) . j , 123 = n where h ( k ) =pk
( 123h ( l ) h ( n ) ) ! . 123
grows like log ( k ) .
it can be shown that :
i denotes the harmonic series , which
( 123h ( l ) h ( n ) ) = log ( n ) + o ( 123 ) ,
so that the ratio
thus , if the
pool cardinality is too small , the distributions of foreground patches from both classes will be better separated with av - erage pooling than max pooling .
123 . 123 vector quantization .
we model binary patch codes for feature j by i . i . d .
bernoulli random variables of mean j .
the distribu - tion of the average - pooled feature also has mean j , and its variance decreases like 123 n .
the maximum is a bernoulli variable of mean 123 ( 123 j ) n and variance ( 123 ( 123 j ) n ) ( 123 j ) n .
thus , it is 123 with probability 123 if n log ( 123 ) , and 123 with probability 123 if n log ( 123 ) , for j 123
the sep - arability of classes depends on sample cardinality n .
there exists a sample cardinality n for which the maximum over class + is 123 with probability 123 , while the maximum over class is 123 with probability 123 , if :
log ( 123j ) | log ( 123 ) | log ( 123j ) | log ( 123 ) |
aspj j = 123 in the context of vector quantization , j be - comes very small on average if the codebook is very large .
for j 123 , the characteristic scale of the transition from 123 to 123 is 123 , hence the pooling cardinality range correspond - ing to easily separable distributions can be quite large if the mean over foreground patches from one class is much higher than both the mean over foreground patches from the other class and the mean over background patches .
experimental validation
our analysis suggests that there may be a purely statis - tical component to the improvement seen with max pool - ing when using pyramids instead of plain bags of features .
taking the maximum over several pools of smaller cardinal - ity may lead to a richer estimate , since max pooling differs from average pooling in two important ways :
the maximum over a pool of smaller cardinality is not merely an estimator of the maximum over a larger
the variance of the maximum is not inversely propor - tional to pool cardinality , so that summing over sev - eral estimates ( one for each smaller pool ) can provide a smoother output than if pooling had merely been per - formed over the merged smaller pools .
we have tested this hypothesis by comparing three types of pooling procedures : standard whole - image and two - level pyramid pooling , and random two - level pyramid pooling , where local features are randomly permuted before being pooled , effectively removing all spatial information .
for this experiment , sift features are extracted densely every 123 pixels , and encoded by hard quantization over a codebook of size 123 for caltech - 123 , 123 for the scenes .
the pooled features are concatenated and classied with a linear svm , trained on 123 and 123 examples for caltech - 123 and the scenes , respectively .
avg , random 123 123 123 123 123 123 123 123 max , random 123 123 123 123 123 123 123 123
table 123
classication accuracy for different sets of pools and
results ( table 123 ) show that with max pooling , a substan - tial part of the increase in accuracy seen when using a two - level pyramid instead of a plain bag of features is indeed still present when locations are randomly shufed .
on the contrary , the performance of average pooling tends to dete - riorate with the pyramid , since the added smaller , random pools only contribute noisier , redundant information .
by deconstructing the mid - level coding step of a well - accepted recognition architecture , it appears that any pa - rameter in the architecture can contribute to recognition per - formance; in particular , surprisingly large performance in - creases can be obtained by merely sampling the low - level descriptor map more nely , and representing neighboring descriptors jointly .
we have presented a scheme to train su - pervised discriminative dictionaries for sparse coding; our ongoing research focuses on extending this framework to the much harder pascal datasets , on which methods very similar to the ones discussed in this paper ( 123 ) currently dene the state of the art .
we plan to combine our discrimi - native sparse training algorithm with the various techniques ( e . g . , local coordinate coding ) that have been successful on pascal .
another research direction we are pursuing is the analysis of pooling schemes .
understanding pooling opera - tors is crucial to good model design , since common heuris - tics suited to average pooling may be suboptimal in other contexts .
in this paper , we have only briey touched upon the statistical properties of max pooling .
we are currently investigating how to expand these theoretical insights , and turn them into guidelines for better architecture design .
acknowledgements .
this work was funded in part by nsf grant efri / copn - 123 to nyu , and onr con - tract n123 - 123 - 123 - 123 to nyu .
we would like to thank sylvain arlot , olivier duchenne and julien mairal for help -
