we present a method for training a similarity metric from data .
the method can be used for recognition or verication applications where the number of categories is very large and not known during training , and where the number of training samples for a single category is very small .
the idea is to learn a function that maps input patterns into a target space such that the norm in the target space ap - proximates the semantic distance in the input space .
the method is applied to a face verication task .
the learning process minimizes a discriminative loss function that drives the similarity metric to be small for pairs of faces from the same person , and large for pairs from different persons .
the mapping from raw to the target space is a convolutional net - work whose architecture is designed for robustness to geo - metric distortions .
the system is tested on the purdue / ar face database which has a very high degree of variability in the pose , lighting , expression , position , and articial occlu - sions such as dark glasses and obscuring scarves .
traditional approaches to classication using discrimi - native methods , such as neural networks or support vec - tor machines , generally require that all the categories be known in advance .
they also require that training exam - ples be available for all the categories .
furthermore , these methods are intrinsically limited to a fairly small number of categories ( on the order of 123 ) .
those methods are un - suitable for applications where the number of categories is very large , where the number of samples per category is small , and where only a subset of the categories is known at the time of training .
such applications include face recog - nition and face verication : the number of categories can be in the hundreds or thousands , with only a few examples
per category .
a common approach to this kind of problem is distance - based methods , which consist in computing a similarity metric between the pattern to be classied or ver - ied and a library of stored prototypes .
another common approach is to use non - discriminative ( generative ) proba - bilistic methods in a reduced - dimension space , where the model for one category can be trained without using exam - ples from other categories .
to apply discriminative learn - ing techniques to this kind of application , we must devise a method that can extract information about the problem from the available data , without requiring specic informa - tion about the categories .
the solution presented in this paper is to learn a similar - ity metric from data .
this similarity metric can later be used to compare or match new samples from previously - unseen categories ( e . g .
faces from people not seen during training ) .
we present a new type of discriminative training method that is used to train the similarity metric .
the method can be applied to classication problems where the number of categories is very large and / or where examples from all cat - egories are not available at the time of training .
the main idea is to nd a function that maps input pat - terns into a target space such that a simple distance in the target space ( say the euclidean distance ) approximates the semantic distance in the input space .
more precisely , given a family of functions such that the we seek to nd a value of the parameter
is small if belong to the same category , and large if they belong to different categories .
the system is trained on pairs of patterns taken from a training set .
the loss func - tion minimized by training minimizes
are from the same category , and maximizes
when they belong to different categories
assumption is made about the nature of differentiability with respect to .
because the same func - with the same parameter is used to process both
inputs , the similarity metric is symmetric .
this is called a siamese architecture ( 123 ) .
to build a face verication system with this method , we rst train the model to produce output vectors that are nearby for pairs of images from the same person , and far away for pairs of images from different persons .
the model can then be used as a similarity metric between face images of new persons that were not seen during training .
an important aspect of the proposed method is that we .
in partic - have complete freedom in the choice of ular , we will use architectures which are designed to extract representations that are robust to geometric distortions of the input , such as convolutional networks ( 123 ) .
the resulting similarity metric will be robust to small differences of the pose between the pairs of images .
since the dimension of the target space is low and the natural distance in that space is invariant to irrelevant dis - tortions of the input , we can easily estimate probabilistic models of each new category from a very small number of
previous work
the idea of mapping face images to low dimensional tar - get spaces before comparison has a long history , starting with the pca - based eigenface method ( 123 ) in which
is a linear projection trained non - discriminatively to maxi - mize the variance .
the lda - based fisherface method ( 123 ) is also linear , but trained discriminatively so as to maxi - mize the ratio of inter - class and intra - class variances .
non - linear extensions based on kernel - pca and kernel - lda have been discussed ( 123 ) .
see ( 123 ) for a review of subspace methods for face recognition .
one major shortcoming of all those approaches is that they are very sensitive to geo - metric transformations of the input images ( shift , scaling , rotation ) and to other variabilities ( changes in facial expres - sion , glasses , and obscuring scarves ) .
some authors have described similarity metrics that are locally invariant to a set of known transformations .
one example is the tangent distance method ( 123 ) .
another example , which has been applied to face recognition , is elastic matching ( 123 ) .
oth - ers have advocated warping - based normalization algorithms to maximally reduce the variations of appearance due to pose ( 123 ) .
the invariance properties of all these models are hand - designed in advance .
in the method described in this paper , the invariance properties do not come from prior knowledge about the task , but they are learned from data .
when used with a convolutional network as the mapping function , the proposed method can learn a wide range of invariances present in the data .
our approach is somewhat similar to that of ( 123 ) , which uses a siamese architecture for signature verication .
the main difference between their method and ours is the na - ture of the loss function minimized by the training process .
our loss function is derived from the discriminative learn - ing framework for energy - based models ( ebm ) .
our method is very different from other dimensional - ity reduction techniques such as multi - dimensional scal - ing ( mds ) ( 123 ) and local linear embedding ( lle ) ( 123 ) mds computes a target vector from each input object in the training set based on known pairwise dissimilarities , with - out constructing a mapping .
by contrast , our method pro - duces a non - linear mapping that can map any input vector to its corresponding low - dimensional version .
the general framework
while probabilistic models assign a normalized proba - bility to every possible conguration of the variables be - ing modeled , energy - based models ( ebm ) assign an un - normalized energy to those congurations ( 123 , 123 ) .
predic - tion in such systems is performed by searching for cong - urations of the variables that minimize the energy .
ebms are used in situations where the energies for various con - gurations must be compared in order to make a decision ( classication , verication , etc ) .
a trainable similarity met - ric can be seen as associating an energy pairs of input patterns .
in the simplest face verication set - ting , we simply set to all the available images of the claimed identity and compare the minimum to a pre - determined threshold .
the advantage of ebms over traditional probabilistic models , particularly generative models , is that there is no need for estimating normalized probability distributions over the input space .
the absence of normalization saves us from computing partition functions that may be intractable .
it also gives us considerably more freedom in the choice of architectures for the model ( 123 ) .
learning is performed by nding the
a suitably designed loss function , evaluated over a training set .
at rst glance , we might think that simply minimizing averaged over a set of pairs of inputs from
the same category would be sufcient .
but this generally leads to a catastrophic collapse : the energy and the loss can be made zero by simply making a constant func - tion .
therefore our loss function needs a contrastive term to ensure not only that the energy for a pair of inputs from the same category is low , but also that the energy for a pair from different categories is large .
this problem does not oc - cur with properly normalized probabilistic models because making the probability of a particular pair high automati - cally makes the probability of other pairs low .
face verication with learned similarity met -
the task of face verication ( 123 ) , is to accept or reject the claimed identity of a subject in an image .
performance
is assessed using two measures : percentage of false accepts and the percentage of false rejects .
a good system should minimize both measures simultaneously .
our approach is to build a trainable system that non - linearly maps the raw images of faces to points in a low di - mensional space so that the distance between these points is small if the images belong to the same person and large oth - erwise .
learning the similarity metric is realized by train - ing a network that consists of two identical convolutional networks that share the same set of weights - a siamese ar - chitecture ( 123 ) ( see gure 123 ) .
the energy function of the ebm
the architecture of our learning machine is given in g -
the details of the architecture of in section 123 .
figure 123
siamese architecture .
ing machine .
let& uine pair ) and&
be a pair of images shown to our learn - belong to the same person ( a gen -
be a binary label of the pair , & * ) otherwise ( an impostor pair )
be the shared parameter vector that is subject to learn - be the two points in the
ing , and let low - dimensional space that are generated by mapping
then our system can be viewed as a scalar energy that measures the compatibility be -
it is dened as
given a genuine pair from the training set
an impostor pair from the training set , the ma - chine behaves in a desirable manner if the following condi -
condition 123 - .
/ 123 ' , such that
to simplify notations
the positive number .
can be interpreted as a margin .
is written as
for the remainder of the paper .
contrastive loss function used for training
we assume that the loss function depends on the input and the parameters only indirectly through the energy .
our loss function is of the form :
the number of
tial loss function for an impostor pair , andg
of a pair of images and a label ( genuine or impostor ) , is the partial loss function for a genuine pair ,
is thef - th sample , which is composed 123 should be designed in such a 123 monotonically in - 123 monotonically decreasing .
however , there
way that the minimization of will decrease the energy of genuine pairs and increase the energy of impostor pairs .
a simple way to achieve that is to make is a more general set of conditions under which minimizing will make the machine approach condition 123
our argu - ments are similar to those given by lecun et al in ( 123 ) .
we will consider a training set composed of one genuine pair
and one impostor pair
let us dene :
the half plane
for all values of
as the total loss function for the two pairs .
we will assume is convex in its two arguments ( note : we do not as - ) .
we also assume that sume convexity with respect to there exists a for single training sample such that condi - tion 123 is satised .
the following conditions must hold for
the loss functionh condition 123 the minima ofh this condition clearly guarantees that when we minimizeh forh whose minima lie at innity ( see gure 123 ) , the condition 123 the negative of the gradient ofh
with respect to the solution satises condition 123
on the margin line uct with the direction ( - 123 , 123 ) .
, the machine is driven to a region where
following condition is sufcient :
has a positive dot prod -
should be inside
to prove this , we state and prove the following theorem .
be convex in
in its domain
would lead to nding a
that satises condition 123
and have a minimum at innity .
assume that there exists for a sample point such that condition 123 is satised .
theorem 123 leth if condition 123 holds , then minimizingh with respect to be denoted byh
consider the positive quadrant of the plane formed ( see gure 123 ) .
let the two half planes
respectively .
we minimizeh
intersects the half planeh
be the region for all the values of inside the plane formed by .
in the most general to all the values in the domain of could be non - convex and could lie anywhere in the plane .
however by our assumption that there exists at such that condition 123 is satised , we can con - clude that a part of .
in order to prove the theorem in the light of condition 123 , we need to show that there exists at least one point in the intersection at this point is less than the loss at all the points in the intersection of
123 be the point on the margin line
since the negative of the gradient ofh
( condition 123 ) , by convexity ofh we can con -
, and inside the half planeh
at all the points on the margin line is in the direction which is inside the half
using a rst - order taylor expansion , we can write the above
, such that the lossh
now consider a point at a distance
that is the
is minimum .
that is ,
by condition 123 , the second term on the right hand side of equation 123 is negative .
thus for sufciently small
thus there exists a point in the intersection of the region
at which the loss function is less
and the half planeh
than at any point in the intersection of the claim follows .
is a monotonically increasing function , and
a monotonically decreasing function .
note that condition 123 clearly holds for any h when -
the exact loss function that we use for a single sample is
ture , the components of
bounded .
the constant+
in our architec - one can clearly see that the above loss function is mono - and monotonically decreasing tonically increasing in hence by the above arguments we conclude that minimiz - ing this loss function would lead the machine to a it would behave in the desired manner .
, and it is convex with respect to both
is set to the upper bound of
are bounded , hence
figure 123
graph of the loss function
we make two further remarks concerning our loss func - tions .
first , the constants in the loss function are explained .
the optimization algorithm that we use to minimize our loss function is based on the gradient .
these constants are cho - sen so as to make sure that the direction of the negative of the gradient of the loss function on the margin line always point inside the region .
this is required to avoid the situa - tion where our algorithm is stuck at a point on the boundary .
in such a case a gradient based algorithm may identify that point as a local minimum of the loss function and terminate .
with the gradient pointing outside
second , we must emphasize that using the square norm
instead of the priate .
indeed , if the energy were the square norm of the dif - ference between the output vectors of the two patterns , the
e ) norm for the energy would not be appro -
of 123 subjects , with variations in lighting , facial expression , accessories , and head position .
each image is 123x123 pix - els , gray scale , and closely cropped to include the face only .
see gure 123
figure 123
plot showing the two half planes
gradient of the energy with respect to the parameter would vanish as the energy approached zero .
this would create a dangerous plateau in the loss function .
this could lead to failure of the machine to learn in cases where the two images are impostors and the corresponding energy is near
convolutional networks
in order to map the raw images to points in a low dimen - sional space and hence to realize a learned similarity met - ric , we use two identical convolutional networks ( 123 ) with a common parameter vector ( see gure 123 ) .
convolutional net - works are trainable , multi - layer , non - linear systems that can operate at the pixel level and learn low - level features and high - level representations in an integrated manner .
convo - lutional nets are trained end - to - end to map pixel images to outputs .
their main advantage is that they can learn optimal shift - invariant local feature detectors and build representa - tions that are robust to geometric distortions of the input image .
the exact specications of the network we use are given in section 123 .
the model and architecture described in the previous section was trained on 123 databases of face images , and tested on 123 of those databases .
we will discuss the databases in detail and then explain the training protocol and architec -
datasets and data processing
the rst round of training and testing was done with a relatively small dataset of 123 images from the at&t database of faces ( 123 ) .
the dataset contains 123 images each
figure 123
top : images from at&t dataset .
middle : images from the ar dataset .
bottom : images from feret dataset .
each graphic shows a genuine pair , an impostor pair and images from a
there was no need to pre - process the images for size or lighting normalization , since one of the stated goals was to train an architecture that would be resilient to such varia - tions .
however , we did reduce the resolution of the images to 123x123 using 123x123 subsampling .
the second set of training and testing experiments was performed by combining two datasets : the ar database of faces , created at purdue university and publicly available ( 123 ) , and a subset of the grayscale feret database ( 123 ) .
im - age pairs from both of these datasets were used in training , but only images from the ar dataset were used for testing .
the ar dataset comprises 123 , 123 images of 123 subjects with 123 images per subject .
each 123 - image set for a sub - ject is made up of 123 sets of 123 images taken 123 days apart .
within each set of 123 images , there are 123 images with ex -
pression variations , 123 images with lighting variations , 123 im - ages with dark sunglasses and lighting variations , and 123 images with face - obscuring scarves and lighting variations .
this dataset is extremely challenging because of the wide variation of appearance between images of a single sub - ject .
examples can be seen in gure 123
since the faces were not well centered in the images , a simple correlation - based centering algorithm was applied .
the images were then cropped and reduced to 123x123 pixels .
although the centering was sufcient for the purposes of cropping , there remained substantial variations in head position in many im -
the feret database , distributed by the national institute of standards and technology , comprises 123 , 123 images col - lected from 123 , 123 subjects .
we used a subset of the full database solely for training .
our subset consists of 123 images , that is , 123 images each of 123 subjects .
the only preprocessing was cropping and subsampling to 123x123 pix -
partitioning for the purpose of generating a test set con - sisting of images of the subjects that are not seen by the machine during training , we split the datasets into two dis - joint sets , namely set123 and set123
each image in each of these sets was paired up with every other image in that set to generate the maximum number of genuine pairs and im -
for the at&t data , set123 consisted of 123 images of rst 123 subjects and set123 consisted of 123 images of last 123 subjects .
this way a total of 123 genuine and 123 impostor pairs were generated from set123 and 123 genuine and 123 impostor pairs were generated from set123
train - ing was done using only the image pairs generated from set123
testing ( verication ) was done using the image pairs from set123 and the unused image pairs from set123
for the ar / feret data , set123 contained all the feret im - ages and 123 , 123 images from 123 subjects in the ar database .
set123 contained the 123 , 123 images from the remaining 123 subjects in the ar database .
taking all combinations of 123 images resulted in 123 , 123 genuine and 123 , 123 , 123 impos - tor pairs .
the actual training set that was used contained 123 , 123 image pairs that were evenly split between genuine and impostor .
the testing set was drawn from the 123 , 123 , 123 pairs in set123
thus , only subjects that had not been seen in training were used for testing .
training protocol and architecture
siamese architecture the siamese framework com - prises two identical networks and one cost module .
the input to the system is a pair of images and a label .
the images are passed through the sub - networks , yielding two outputs which are passed to the cost module which pro - duces the scalar energy as discussed in section 123 .
the loss
function combines the label with energy .
the gradient of the loss function with respect to the parameter vector con - trolling both subnets is computed using back - propagation .
the parameter vector is updated with a stochastic gradient method using the sum of the gradients contributed by the
the rst set of experiments , using the small at&t dataset , explored 123 different sub - net architectures : one 123 - layer fully - connected neural network and ve convolutional networks that varied in number and size of layers and con - volution kernel size .
based on those experiments , the sec - ond set of experiments focused on a single convolutional network architecture .
we only describe the best - performing denotes a con - architecture in the following sections .
denotes a fully connected layer , where the basic architecture is
is the layer index .
trainable parameters : 123; connections : 123
fully connected with the input .
feature maps : 123; size 123x123; kernel size : 123x123
denotes a sub - sampling layer , and
feature maps : 123; size : 123x123; field of view :
trainable parameters : 123; connections : 123
feature maps : 123; size : 123x123; kernel size : 123x123
trainable parameters : 123 ; connections : 123
partially connected to .
the exact connections are in a pattern similar to that used in ( 123 ) .
the motivation behind this was to break symmetry , thereby pushing the feature maps to extract and learn different features .
feature maps : 123; size : 123x123; field of view : 123x123
trainable parameters : 123; connections : 123
feature maps : 123; size 123x123; kernel size : 123x123
trainable connections : 123
fully connected to
number of units : 123
trainable parameters :
123; connections : 123
training protocol training requires two sets of data : the training set , for actually learning the weights of the system , and the validation set , for testing the performance of the system during training .
periodical performance evaluation with the validation set allows us to control over - tting .
training the network was done with pairs of images taken from set123
one half of the image pairs were genuine and one half were impostor , produced by randomly pairing images of different subjects .
the validation set was com - posed of 123 image pairs , taken from the unused pairs of set123 , and in the same 123% genuine , 123% impostor ratio as the training set .
the performance of the network was measured by a cal - culation of the percentage of impostor pairs accepted ( fa ) ,
number of subjects
genuine images no .
impostor images
123% 123% 123%
table 123
above : details of the validation and test sets for the two datasets .
below : false reject percentage for different false accept
figure 123
internal state of the convolutional network for a particu -
and the percentage of genuine pairs rejected ( fr ) .
this cal - culation was made by measuring the norm of the difference between the outputs of a pair , then picking a threshold value that sets a given trade - off between the fa and fr percent -
testing and results
figure 123 shows the internal state of the convolutional net - work for a particular test image .
the rst layer extracts various types of local gradient features , as well as smooth
the system was tested for a face verication scenario .
the system is given an image and asked to conrm the claimed identity of the subject in that image .
we perform verication by comparing the test image with a gaussian model of images of the claimed subject .
the method is dis -
testing ( verication ) was done on a test set of size 123
it consisted of 123 genuine and 123 impostor pairs .
for the at&t experiments the test images were from 123 sub - jects unseen in training .
for ar / feret experiments the test images were from 123 unseen subjects in the more difcult
the output from one of the subnets of the siamese net - work is a feature vector of the input image of the subject .
we assume that the feature vectors of each subjects im - age form a multivariate normal density .
a model is con - structed of each subject by calculating the mean feature vector and the variance - covariance matrix using the feature vectors generated from the rst ve images of each subject .
found by evaluating the normal density of the test image on the model of the concerned subject .
the likelihood of a test image being an impostor , , is assumed to be a con - stant whose value is estimated by calculating the average value of all the impostor images of the concerned
subject .
the probability that the given image is genuine is
the likelihood that a test image is genuine ,
the values of the percentage of falsely rejected images and the falsely accepted images are plotted for all possible values of the threshold probability .
the optimal threshold probability is the value that partitions the test set into gen - uine and impostor pairs and minimizes fa and fr rates .
the verication rates obtained from testing the at&t database and the ar / purdue database are strikingly differ - ent ( see table 123 and gures 123 and 123 ) , underlining the differ - ences in difculty in the two databases .
the at&t dataset is relatively small , and our system required only 123 train - ing samples to achieve very high performance on the test set .
the ar / purdue dataset is very large and diverse , with huge variations in expression , lighting , and added occlu - sions .
our higher error rates reect this level of difculty .
conclusion and outlook
we present a general discriminative method for learn - ing complex similarity metrics .
the method is best suited for classication or verication scenarios where the num - ber of classes is very large , and / or where examples of all the classes are not available at the time of training .
we il - lustrate the method with a face verication application .
we propose a loss function and show that minimizing this function causes the system to approach the desired be - havior .
our loss function is discriminative in the sense that it drives the system to make the right decision , but does not cause it to produce probability estimates .
the method is
roc : false reject vs .
false accept
figure 123
at&t dataset : percent false reject vs .
false accept .
roc : false reject v / s false accept
123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123
cowan and g .
tesauro ( eds ) advances in neural information processing systems , 123
hsuan yang , n .
ahuja , and d .
kriegman .
face recogni - tion using kernel eigenfaces .
in proc .
of the 123 ieee in - ternational conference on image processing ( icip ) , 123 : 123 123 , september 123
lades , j .
vorbruggen , j .
buhmann , j .
lange , c .
von der malsburg , r .
wurtz , and w .
distortion - invariant object recognition in the dynamic link architecture .
ieee trans .
computers , 123 ( 123 ) : 123 , 123
lawrence , c .
lee giles , a .
chung tsoi , and a .
face recognition : a convolutional neural network approach .
ieee transactions on neural networks , special issue on neural networks , 123
lecun , l .
bottou , y .
bengio , and p .
haffner .
gradient - based learning applied to document recognition .
proceed - ings of the ieee , 123 ( 123 ) : 123 , 123
lecun and f .
jie huang .
loss functions for discrimina -
tive training of energy - based models .
ai - stats , 123
( 123 ) a .
martinez .
recognizing imprecisely localized , par - tially occluded and expression variant faces from a single sample per class .
ieee trans .
on pattern analysis and ma - chine intelligence , 123 ( 123 ) : 123 , 123
( 123 ) a .
martinez and r .
benavente .
cvc technical report , 123 ,
http : / / rvl123ecn . purdue . edu / aleix / aleix face db . html .
standards and technology , 123
( 123 ) s .
rizvi , p .
figure 123
ar / purdue dataset : percent false reject vs .
false accept .
different from probabilistic density models in that there is no attempt to estimate a density for each class in the input space .
this gives us an additional amount of exibility in the choice of , because we do need to worry about normalization .
we chose to use a convolutional network ar - chitecture which exhibits robustness to geometric variations of the input , thereby reducing the need for accurate registra - tion of the face images .
trainable similarity metrics have numerous applications beyond the one described in this paper .
among other things , they can be used to build invariant kernel functions with which to build support vector machines and other kernel - based models ( 123 ) .
