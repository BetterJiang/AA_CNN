we describe a novel method for simultaneously detecting faces and estimating their pose in real time .
the method employs a convolutional network to map images of faces to points on a low - dimensional manifold parametrized by pose , and images of non - faces to points far away from that manifold .
given an image , detecting a face and estimating its pose is viewed as minimizing an en - ergy function with respect to the face / non - face binary variable and the continuous pose parameters .
the system is trained to minimize a loss function that drives correct combinations of labels and pose to be associated with lower energy values than incorrect ones .
the system is designed to handle very large range of poses without retraining .
the performance of the system was tested on three standard data setsfor frontal views , rotated faces , and proles is comparable to previous systems that are designed to handle a single one of these data sets .
we show that a system trained simuiltaneously for detection and pose estimation is more accu -
rate on both tasks than similar systems trained for each task separately . 123 keywords : face detection , pose estimation , convolutional networks , energy based models , object
the detection of human faces in natural images and videos is a key component in a wide variety of applications of human - computer interaction , search and indexing , security , and surveillance .
many real - world applications would prot from view - independent detectors that can detect faces under a wide range of poses : looking left or right ( yaw axis ) , up or down ( pitch axis ) , or tilting left or right
in this paper we describe a novel method that can not only detect faces independently of their poses , but also simultaneously estimate those poses .
the system is highly reliable , runs in real time
a more preliminary version of this work appears as : osadchy et al .
( 123 ) .
c ( cid : 123 ) 123 margarita osadchy , yan le cun and matthew l .
miller .
osadchy , lecun and miller
on standard hardware , and is robust to variations in yaw ( ( cid : 123 ) 123 ( cid : 123 ) ) , roll ( ( cid : 123 ) 123 ( cid : 123 ) ) , pitch ( ( cid : 123 ) 123 ( cid : 123 ) ) , as well as partial occlusions .
the method is motivated by the idea that multi - view face detection and pose estimation are so closely related that they should not be performed separately .
the tasks are related in the sense that they could use similar features and internal representations , and must be robust against the same sorts of variation : skin color , glasses , facial hair , lighting , scale , expressions , etc .
we suspect that , when trained together , each task can serve as an inductive bias for the other , yielding better generalization or requiring fewer training examples ( caruana , 123 ) .
to exploit the synergy between these two tasks , we train a learning machine to map input images to points in a low - dimensional space .
in the low - dimensional output space we embed a face mani - fold which is parameterized by facial pose parameters ( e . g . , pitch , yaw , and roll ) .
a convolutional network is trained to map face images to points on the face manifold that correspond to the pose of the faces and non - face images to points far away from that manifold .
after training , a detection is performed by measuring whether the distance of the output point from the manifold is lower than a threshold .
if the point is close to the manifold , indicating that a face is present in the image , its pose parameters can be inferred from the position of the projection of the point onto the manifold .
to map input images to points in the low - dimensional space , we employ a convolutional network architecture ( lecun et al . , 123 ) .
convolutional networks are specically designed to learn invariant representation of images .
they can easily learn the type of shift - invariant local features that are relevant to face detection and pose estimation .
more importantly , they can be replicated over large images ( applied to every sub - windows in a large image ) at a small fraction of the cost of applying more traditional classiers to every sub - windows in an image .
this is a considerable advantage for building real - time systems .
as a learning machine we use the recently proposed energy - based models ( ebm ) that provide a description and the inference process and the learning process in a single , well - principled framework ( lecun and huang , 123; lecun et al . , 123 ) .
given an input ( an image ) , an energy - based model associates an energy to each conguration of the variables to be modeled ( the face / non - face label and the pose parameters in our case ) .
making an inference with an ebm consists in searching for a conguration of the variables to be predicted that minimizes the energy , or comparing the energies of a small number of congurations of those variables .
ebms have a number of advantages over probabilistic models : ( 123 ) there is no need to compute partition functions ( normalization constants ) that may be intractable; ( 123 ) because there is no requirement for normalization , the repertoire of possible model architectures that can be used is considerably richer .
in our application we dene an energy - based model as a scalar - valued energy function of three variables : image , label , and pose , and we treat pose as a deterministic latent variable .
thus both label of an image and pose are inferred through the energy - minimization
training an ebm consists in nding values of the trainable parameters ( which parameterize the energy function ) that associate low energies to desired congurations of variables , and high energies to undesired congurations .
with probabilistic models , making the probability of some values large automatically makes the probabilities of other values small because of the normaliza - tion .
with ebms making the energy of desired congurations low may not necessarily make the energies of other congurations high .
therefore , one must be very careful when designing loss functions for ebms .
in our application to face detection we derive a new type of contrastive loss function that is tailored to such detection tasks .
synergistic face detection and pose estimation with energy - based models
the paper is organized as follows .
first , some of the relevant prior works on multi - view face detection are briey discussed .
section 123 discusses the synergy between pose estimation and face detection , and describes the basic methods for integrating them .
section 123 discusses the learning machine , and section 123 gives the results of experiments conducted with our system .
section 123 draws
123 previous work
learning - based approaches to face detection abound , including real - time methods ( viola and jones , 123 ) , and approaches based on convolutional networks ( vaillant et al . , 123; garcia and delakis , 123 ) .
most multi - view systems take a view - based approach , which involves building separate detectors for different views and either applying them in parallel ( pentland et al . , 123; sung and poggio , 123; schneiderman and kanade , 123; li et al . , 123 ) or using a pose estimator to select the most appropriate detector ( jones and viola , 123; huang et al . , 123 ) .
another approach is to estimate and correct in - plane rotations before applying a single pose - specic detector ( rowley et al . , 123b ) .
some attempts have been done in integrating pose search and detection , but in much smaller space of pose parameters ( fleuret and geman , 123 ) .
closer to our approach is that of li et al .
( 123 ) , in which a number of support vector regressors are trained to approximate smooth functions , each of which has a maximum for a face at a particular pose .
another machine is trained to convert the resulting values to estimates of poses , and a third machine is trained to convert the values into a face / non - face score .
the resulting system is rather slow .
see yang et al .
( 123 ) for survey of face detection methods .
integrating face detection and pose estimation
to exploit the posited synergy between face detection and pose estimation , we must design a system that integrates the solutions to the two problems .
merely cascading two systems where the answer to one problem is used to assist in solving the other will not optimally take advantage of the synergy .
therefore , both answers must be derived from one underlying analysis of the input , and both tasks must be trained together .
our approach is to build a trainable system that can map raw images x to points in a low - dimensional space ( figure 123 ) .
in that space , we pre - dene a face manifold f ( z ) that we parameter - ize by the pose z .
we train the system to map face images with known poses to the corresponding points on the manifold .
we also train it to map images of non - faces to points far away from the manifold .
during recognition , the system maps the input image x to a point in the low dimensional space g ( x ) .
the proximity of g ( x ) to the manifold then tells us whether or not an image x is a face .
by nding the pose parameters z that correspond to the point on the manifold that is closest to the point g ( x ) ( projection ) , we obtain an estimate of the pose ( figure 123 ) .
123 parameterizing the face manifold
we will now describe the details of the parameterizations of the face manifold .
three criteria di - rected the design of the face manifold : ( 123 ) preserving the topology and geometry of the problem; ( 123 ) providing enough space for mapping the background images far from the manifold ( since the proximity to the manifold indicates whether the input image contains a face ) ; and ( 123 ) minimizing
osadchy , lecun and miller
low dimensional space
parameterized by pose
figure 123 : manifold mappingtraining
low dimensional space
parameterized by pose
figure 123 : manifold mapping recognition and pose estimation .
synergistic face detection and pose estimation with energy - based models
the computational cost of nding the parameters of the closest point on the manifold to any point in
lets start with the simplest case of one pose parameter z = q
, representing , say , yaw .
if we want to preserve the natural topology and geometry of the problem ( the rst criterion ) , the face manifold under yaw variations in the interval ( ( cid : 123 ) 123 ( cid : 123 ) ;123 ( cid : 123 ) ) should be a half circle ( with constant curvature ) .
the natural way of representing a circle is with sine and cosine functions .
in this case we embed the angle parameter in two dimensional space .
now images of faces will be mapped to points on the half circle manifold corresponding to q , and non - face images will be mapped to points in the rest of the two dimensional space .
having lots of free space to represent non - face images may be necessary , due to the considerable amount of variability in non - face images .
so increasing the dimension of embedding might help us in better separation of face and non - face images ( the second criterion ) .
in the case of single pose parameter we suggest 123d embedding .
to make the projection and parameter estimation simple ( the third criterion ) , we embed this half - circle in a three - dimensional space using three equally - spaced shifted cosine functions ( figure
fi ( q ) = cos ( q ( cid : 123 ) a
a = f ( cid : 123 ) is f ( q ) = ( f123 ( q ) ; f123 ( q ) ; f123 ( q ) ) .
a point on the face manifold parameterized by the yaw angle q when we run the network on an image x , it outputs a vector g ( x ) .
the yaw angle q corresponding to the point on the manifold that is closest to g ( x ) can be expressed analytically as :
q = ( ( cid : 123 )
i = 123;123;123;
q = arctan
the point on the manifold closest to g ( x ) is just f ( q ) .
the function choice is not limited to cosine .
however cosines are preferable since they allow computing the pose analytically from the output of the network .
without this property , nding the pose could be an expensive optimization process , or even require the use of a second learning
the same idea can be generalized to any number of pose parameters .
let us consider the set of all faces with yaw in ( ( cid : 123 ) 123;123 ) and roll in ( ( cid : 123 ) 123;123 ) .
in an abstract way , this set is isomorphic to a portion of a sphere .
consequently , we can represent a point on the face manifold as a function of the two pose parameters by 123 basis functions that are the cross - products of three shifted cosines for one of the angles , and three shifted cosines for the other angle :
fi j ( q ;f ) = cos ( q ( cid : 123 ) a
i ) cos ( f ( cid : 123 ) b
i; j = 123;123;123 :
for convenience , we rescale the roll angles to the range ( ( cid : 123 ) 123;123 ) which allows us to set b
with this parameterization , the manifold has constant curvature , which ensures that the effect of er - rors will be the same regardless of pose .
given a 123 - dimensional output vector from the convolutional network gi j ( x ) , we compute the corresponding yaw and roll angles q ;f as follows :
i j gi j ( x ) cos ( a i j gi j ( x ) sin ( a
i j gi j ( x ) cos ( a i j gi j ( x ) sin ( a q = 123 : 123 ( atan123 ( cs + sc; cc ( cid : 123 ) ss ) + atan123 ( sc ( cid : 123 ) cs; cc + ss ) ) f = 123 : 123 ( atan123 ( cs + sc; cc ( cid : 123 ) ss ) ( cid : 123 ) atan123 ( sc ( cid : 123 ) cs; cc + ss ) )
cc = ( cid : 123 ) sc = ( cid : 123 )
cs = ( cid : 123 ) ss = ( cid : 123 )
the process can easily be extended to include pitch in addition to yaw and roll , as well as other parameters if necessary .
osadchy , lecun and miller
figure 123 : left : face manifold embedding; right : manifold parametrization by single pose parame - ter .
the value of each cosine function for one pose angle constitute the three components of a point on the face manifold corresponding to that pose .
learning machine
to map input images to points in the low - dimensional space , we employ a convolutional network architecture trained using energy minimization framework .
next we present the details of the
123 energy minimization framework
we propose the following conguration of the energy based model ( lecun and huang , 123; lecun et al . , 123 ) .
consider a scalar - valued function ew ( y; z; x ) , where x is a raw image , z is a facial pose ( e . g . , yaw and roll as dened above ) , y is a binary label : y = 123 for face , y = 123 for non - face .
w is a parameter vector subject to learning .
ew ( y; z; x ) can be interpreted as an energy function that measures the degree of compatibility between the values of x; z;y .
the inference process consists in clamping x to the observed value ( the image ) , and searching for congurations of z and y that minimize the energy ew ( y; z; x ) :
( y ; z ) = argminy123fyg; z123fzg
ew ( y; z; x )
where fyg = f123;123g and fzg = ( ( cid : 123 ) 123;123 ) ( cid : 123 ) ( ( cid : 123 ) 123;123 ) for yaw and roll variables .
ideally , if the input x is the image of a face with pose z , then a properly trained system should give a lower energy to the face label y = 123 than to the non - face label y = 123 for any pose : ew ( 123; z; x ) < ew ( 123; z123; x ) , 123z123
for accurate pose estimation , the system should give a lower en - ergy to the correct pose than to any other pose : ew ( 123; z123; x ) > ew ( 123; z; x ) , 123z123 123= z .
training a machine to satisfy those two conditions for any image will guarantee that the energy - minimizing inference process will produce the correct answer .
transforming energies to probabilities can easily be done via gibbs distribution :
p ( y; zjx ) = exp ( ( cid : 123 ) b ew ( y; z; x ) ) =zy123fyg;z123fzg
exp ( ( cid : 123 ) b ew ( y; z; x ) )
is an arbitrary positive constant , and fyg and fzg are the sets of possible values of y and z .
with this formulation , we can easily interpret the energy minimization with respect to y and z as
synergistic face detection and pose estimation with energy - based models
- f ( z )
a naly tical mapping onto face manifold
figure 123 : architecture of the minimum energy machine .
a maximum conditional likelihood estimation of y and z .
this probabilistic interpretation assumes that the integral in the denominator ( the partition function ) converges .
it is easy to design such an energy function for our case .
however , a proper probabilistic formulation would require us to use the negative log - likelihood of the training samples as our loss function for training .
this will require us to compute the derivative of the denominator with respect to the trainable parameters w .
this is unnecessary complication which can be alleviated by adopting the energy - based formulation .
removing the necessity for normalization gives us complete freedom in the choice of the internal architecture and parameterization of ew ( y; z; x ) , as well as considerable exibility in the choice of the loss function for training .
our energy function for a face ew ( 123; z; x ) is dened as the distance between the point produced
by the network gw ( x ) and the point with pose z on the manifold f ( z ) :
ew ( 123; z; x ) = kgw ( x ) ( cid : 123 ) f ( z ) k :
the energy function for a non - face ew ( 123; z; x ) is equal to a constant t that we can interpret as a threshold ( it is independent of z and x ) .
the complete energy function is :
ew ( y; z; x ) = ykgw ( x ) ( cid : 123 ) f ( z ) k + ( 123 ( cid : 123 ) y ) t :
the architecture of the machine is depicted in figure 123
operating this machine ( nding the output label and pose with the smallest energy ) comes down to rst nding : z = argminz123fzgjjgw ( x ) ( cid : 123 ) f ( z ) jj , and then comparing this minimum distance , kgw ( x ) ( cid : 123 ) f ( z ) k , to the threshold t .
if its smaller than t , then x is classied as a face , otherwise x is classied as a non - face .
this decision is implemented in the architecture as a switch , that depends upon the binary variable y .
for simplicity we x t to be a constant .
although it is also possible to make t a function of
osadchy , lecun and miller
123 convolutional network
we employ a convolutional network as the basic architecture for the gw ( x ) function that maps image points in the face - space .
the architecture of convolutional nets is somewhat inspired by the structure of biological visual systems .
convolutional nets have been used successfully in a number of vision applications such as handwriting recognition ( lecun et al . , 123 , 123 ) , and generic object recognition ( lecun et al . , 123 ) .
several authors have advocated the use of convolutional networks for object detection ( vaillant et al . , 123; nowlan and platt , 123; lecun et al . , 123; garcia and
convolutional networks are end - to - end trainable system that can operate on raw pixel images and learn low - level features and high - level representation in an integrated fashion .
each layer in a convolutional net is composed units organized in planes called feature maps .
each unit in a feature map takes inputs from a small neighborhood within the feature maps of the previous layer .
neighboring units in a feature map are connected to neighboring ( possibly overlapping ) windows .
each unit computes a weighted sum of its inputs and passes the result through a sigmoid saturation function .
all units within a feature map share the same weights .
therefore , each feature map can be seen as convolving the feature maps of the previous layers with small - size kernels , and passing the sum of those convolutions through sigmoid functions .
units in a feature map detect local features at all locations on the previous layer .
convolutional nets are advantageous because they can operate on raw images and can easily learn the type of shift - invariant local features that are relevant to image recognition .
furthermore , they are very efcient computationally for detection and recognition tasks involving a sliding win - dow over large images ( vaillant et al . , 123; lecun et al . , 123 ) .
the network architecture used for training is shown in figure 123
it is similar to lenet123 ( lecun et al . , 123 ) , but contains more feature maps .
the network input is a 123 ( cid : 123 ) 123 pixel gray - scale image .
the rst layer c123 is a convolutional layer with 123 feature maps of size 123 ( cid : 123 ) 123
each unit in each feature map is connected to a 123 ( cid : 123 ) 123 neighborhood in the input .
contiguous units in c123 take input from neighborhood on the input that overlap by 123 pixels .
the next layer , s123 , is a so - called subsampling layer with 123 feature maps of size 123 ( cid : 123 ) 123
each unit in each map is connected to a 123 ( cid : 123 ) 123 neighborhood in the corresponding feature map in c123
contiguous units in s123 take input from contiguous , non - overlapping 123x123 neighborhoods in the corresponding map in c123
c123 is convolutional with 123 feature maps of size 123 ( cid : 123 ) 123
each unit in each feature map is connected to several 123 ( cid : 123 ) 123 neighborhoods at identical locations in a subset of s123s feature maps .
different c123 maps take input from different subsets of s123 to break the symmetry and to force the maps to extract different features .
s123 is a subsampling layer with 123 ( cid : 123 ) 123 subsampling ratios containing 123 feature maps of size 123 ( cid : 123 ) 123
layer c123 is a convolutional layer with 123 feature maps of size 123 ( cid : 123 ) 123 with 123 ( cid : 123 ) 123 kernels .
each c123 map takes input from all 123 of s123s feature maps .
the output layer has 123 outputs ( since the face manifold is nine - dimensional ) and is fully connected to c123 ( such a full connection can be seen as a convolution with 123 ( cid : 123 ) 123 kernels ) .
123 training with a contrastive loss function
the vector w contains all 123;123 weights and kernel coefcients in the convolutional network .
they are all subject to training by minimizing a single loss function .
a key element , and a novel contribution , of this paper is the design of the loss function .
synergistic face detection and pose estimation with energy - based models
figure 123 : architecture of convolutional network used for training .
this represents one slice of the network with with a 123 ( cid : 123 ) 123 input window .
the slice includes all the elements that are necessary to compute a single output vector .
the trained network is replicated over the full input image , producing one output vector for each 123 ( cid : 123 ) 123 window stepped every 123 pixels horizontally and vertically .
the process is repeated at multiple scales .
the training set s is composed of two subsets : the set s123 of training samples ( 123; x i; zi ) contain - ing a face annotated with the pose; and the set s123 of training sample ( 123; x i ) containing a non - face image ( background ) .
the loss function l ( w;s ) is dened as the average over s123 of a per - sample loss function l123 ( w; zi; x i ) , plus the average over s123 of a per - sample loss function l123 ( w; x i ) :
l ( w;s ) =
l123 ( w; zi; x i ) +
l123 ( w; x i ) :
face samples whose pose is unknown can easily be accommodated by viewing z as a deterministic latent variable over which the energy must be minimized .
however , experiments reported in this paper only use training samples manually labeled with the pose .
for a particular positive training sample ( x i; zi;123 ) , the per - sample loss l123 should be designed in such a way that its minimization with respect to w will make the energy of the correct answer lower than the energies of all possible incorrect answers .
minimizing such a loss function will make the machine produce the right answer when running the energy - minimizing inference procedure .
we can write this condition as :
ew ( y i = 123; zi; x i ) < ew ( y; z; x i ) for y 123= y i or z 123= zi : satisfying this condition can be done by satisfying the two following conditions
ew ( 123; zi; x i ) < t and ew ( 123; zi; x i ) < min
ew ( 123; z; x i ) :
osadchy , lecun and miller
following lecun and huang ( 123 ) , we assume that the loss is a functional that depends on x only through the set of energies associated with x and all the possible values of z and y .
this assumption allows us to decouple the design of the loss function from the internal structure ( architecture ) of the energy function .
we also assume that there exist a w for which condition 123 is satised .
this is a reasonable assumption , we merely ensure that the learning machine can produce the correct output for any single sample .
we now show that , with our architecture , if we choose l123 to be a strictly monotonically increasing function of ew ( 123; zi; x i ) ( over the domain of e ) , then minimizing l123 with respect to w will cause the machine to satisfy condition 123
the rst inequality in 123 will obviously be satised by minimizing such a loss .
the second inequality will be satised if ew ( 123; z; x i ) has a single ( non - degenerate ) global minimum as a function of z .
the minimization of l123 with respect to w will place this minimum at zi , and therefore will ensure that all other values of z will have higher energy .
our energy function ew ( 123; z; x ) = jjgw ( x ) ( cid : 123 ) f ( z ) jj indeed has a single global minimum as a function of z , because f ( z ) is injective and the norm is convex .
the single global minimum is attained for gw ( x ) = f ( z ) .
for our experiments , we simply chose :
l123 ( w;123; z; x ) = ew ( 123; z; x ) 123 :
for a particular negative ( non - face ) training sample ( x i;123 ) , the per - sample loss l123 should be designed in such a way that its minimization with respect to w will make the energy for y = 123 and any value of z higher than the energy for y = 123 ( which is equal to t ) .
minimizing such a loss function will make the machine produce the right answer when running the energy - minimizing inference procedure .
we can write the condition for correct output as :
which can be re - written as :
ew ( 123; z; x i ) > t 123z
ew ( 123; z; x i ) > t z = argminzew ( 123; z; x i ) :
again , we assume that there exists a w for which condition 123 is satised .
it is easy to see that , with our architecture , if we choose l123 to be a strictly monotonically decreasing function of ew ( 123; z; x i ) ( over the domain of e ) , then minimizing l123 with respect to w will cause the machine to satisfy condition 123
for our experiments , we simply chose :
l123 ( w;123; x i ) = k exp ( ( cid : 123 ) e ( 123; z; x i ) )
where k is a positive constant .
a nice property of this function is that it is bounded below by 123 , and that its gradient vanishes we approach the minimum .
the entire system was trained by minimizing average value of the loss function in eq .
( 123 ) with respect to the parameter w .
we used a stochastic version of the levenberg - marquardt algorithm with diagonal approximation of the hessian ( lecun et al . , 123 ) .
synergistic face detection and pose estimation with energy - based models
figure 123 : screenshot from annotation tool .
123 running the machine
the detection system operates on raw grayscale images .
the convolutional network is applied to all 123 ( cid : 123 ) 123 sub - windows of the image , stepped every 123 pixels horizontally and vertically .
because the layers are convolutional , applying two replicas of the network in figure 123 to two overlapping input windows leads to a considerable amount of redundant computation .
eliminating the redundant computation yields a dramatic speed up : each layer of the convolutional network is extended so as to cover the entire input image .
the output is also replicated the same way .
due to the two 123 ( cid : 123 ) 123 subsampling layers , we obtain one output vector every 123 ( cid : 123 ) 123 pixels .
to detect faces in a size - invariant fashion , the network is applied to multiple down - scaled ver - sions of the image over a range of scales stepped by a factor of p123
at each scale and location , the networks 123 - dimensional output vector is compared to the closest point on the face manifold ( whose position indicates the pose of the candidate face ) .
the system collects a list of all locations and scales of output vectors closer to the face manifold than the detection threshold .
after examining all scales , the system identies groups of overlapping detections in the list and discards all but the strongest ( closest to the manifold ) from each group within an exclusion area of a preset size .
no attempt is made to combine detections or apply any voting scheme .
experiments and results
using the architecture described in section 123 , we built a detector to locate faces and estimate two pose parameters : yaw from left to right prole , and in - plane rotation from ( cid : 123 ) 123 to 123 degrees .
the machine was trained to be robust against pitch variation .
in this section , we rst describe the training protocol for this network , and then give the results of two sets of experiments .
the rst set of experiments tests whether training for the two tasks together improves performance on both .
the second set allows comparisons between our system and other published multi - view detectors .
osadchy , lecun and miller
the images we used for training were collected at nec labs .
all face images were manually annotated with appropriate poses .
the annotation process was greatly simplied by using a simple tool for specifying a location and approximate pose of a face .
the user interface for this tool is shown in figure 123
annotation process is done by rst clicking on the midpoint between the eyes and on the center of the mouth .
the tool then draws a perspective grid in front of the face and the user adjusts it to be parallel to the face plane .
this process yields estimates for all six pose parameters : location ( x; y ) , three angles ( yaw , pitch , and roll ) and scale .
the images were annotated in such a way that the midpoint between the eyes and on the center of the mouth are positioned in the center of the image .
this allows these two points to stay xed when the pose changes from left to right prole .
the downside is that proles occupy only half of the image .
in this manner we annotated about 123 , 123 faces in images from various sources .
each face was then cropped and scaled so that the eye midpoint and the mouth midpoint appeared in canonical positions , 123 pixels apart , in 123 ( cid : 123 ) 123 - pixel image with some moderate variation of location and scale .
the resulting images where mirrored horizontally , to yield roughly 123 , 123 faces .
we removed some portion of faces from this set to yield a roughly uniform distribution of poses from left prole to right prole .
unfortunately , the amount of variation in pitch ( up / down ) was not sufcient to do the same .
this was the reason for training our system to be robust against pitch variation instead of estimating the pitch angle .
the roll variation was added by randomly rotating the images in the range of ( ( cid : 123 ) 123;123 ) degrees .
the resulting training set consisted of 123;123 , 123x123 grey - level images of faces with uniform distribution of poses .
the initial set of negative training samples consisted of 123;123 image patches chosen randomly from non - face areas in a variety of images .
for the second set of tests , half of these images were replaced with image patches obtained by running the initial version of the detector on the training images and collecting false detections .
each training image was used 123 times during training , with random variations in scale ( from
xp123 to x ( 123 +p123 ) ) , in - plane rotation ( ( cid : 123 ) 123 ( cid : 123 ) ) , brightness ( ( cid : 123 ) 123 ) , and contrast ( from 123 to 123 ) .
to train the network , we made 123 passes through this data , though it mostly converged after about the rst 123 passes .
the training system was implemented in the lush language ( bottou and lecun , 123 ) .
the total training time was about 123 hours on a 123ghz pentium 123
at the end of training , the network had converged to an equal error rate of 123% on the training data and 123% on a separate test set of 123 , 123 images .
a standalone version of the detection system was implemented in the c language .
it can detect , locate , and estimate the pose of faces that are between 123 and 123 pixels high in a 123 ( cid : 123 ) 123 image at roughly 123 frames per second on a 123ghz pentium 123
123 synergy tests
the goal of the synergy test was to verify that both face detection and pose estimation benet from learning and running in parallel .
to test this claim we built three networks with almost identical architectures , but trained to perform different tasks .
the rst one was trained for simultaneous face detection and pose estimation ( combined ) , the second was trained for detection only and the third for pose estimation only .
the detection only network had only one output for indicating whether or not its input was a face .
the pose only network was identical to the combined network , but trained on faces only ( no negative examples ) .
figure 123 shows the results of running these networks
synergistic face detection and pose estimation with energy - based models
pose + detection
false positive rate
pose + detection
yaw - error tolerance ( degrees )
figure 123 : synergy test .
left : roc curves for the pose - plus - detection and detection - only networks .
( the x axis is the false positive rate per image ) .
right : frequency with which the pose - plus - detection and pose - only networks correctly estimated the yaws within various error
rotated in plane
false positives per image
pose - error tolerance ( degrees )
figure 123 : results on standard data sets .
left : roc curves for our detector on the three data sets .
the x axis is the average number of false positives per image over all three sets , so each point corresponds to a single detection threshold .
right : frequency with which yaw and roll are estimated within various error tolerances .
on our 123 , 123 test images .
in both these graphs , we see that the pose - plus - detection network had better performance , conrming that training for each task benets the other .
123 standard data sets
there is no standard data set that spans the range of poses our system is designed to handle .
there are , however , data sets that have been used to test more restricted face detectors , each set focusing on a particular variation in pose .
by testing a single detector with all of these sets , we can compare our performance against published systems .
as far as we know , we are the rst to publish results for a single detector on all these data sets .
the details of these sets are described below : ( cid : 123 ) mit+cmu ( sung and poggio , 123; rowley et al . , 123a ) 123 images for testing frontal face
osadchy , lecun and miller
detectors .
we count 123 faces in this set , but the standard tests only use a subset of 123 faces , because 123 faces are in the wrong pose or otherwise not suitable for the test .
( note : about 123% of the faces in the standard subset are badly - drawn cartoons , which we do not intend our system to detect .
nevertheless , we include them in the results we report . ) ( cid : 123 ) tilted ( rowley et al . , 123b ) 123 images of frontal faces with in - plane rotations .
123 faces out of 123 are in the standard subset .
( note : about 123% of the faces in the standard subset are outside of the ( cid : 123 ) 123 ( cid : 123 ) rotation range for which our system is designed .
again , we still include these in our ( cid : 123 ) profile ( schneiderman and kanade , 123 ) 123 images of faces in prole .
there seems to be some disagreement about the number of faces in the standard set of annotations : schneiderman and kanade ( 123 ) reports using 123 faces of the 123 that we found , jones and viola ( 123 ) reports using 123 , and we found 123 annotations .
however , these discrepancies should not signicantly effect the reported results .
we counted a face as being detected if 123 ) at least one detection lay within a circle centered on the midpoint between the eyes , with a radius equal to 123 times the distance from that point to the midpoint of the mouth , and 123 ) that detection came at a scale within a factor of two of the correct scale for the faces size .
we counted a detection as a false positive if it did not lie within this range for any of the faces in the image , including those faces not in the standard subset .
the left graph in figure 123 shows roc curves for our detector on the three data sets .
figures 123 , 123 show detection results on various poses .
table 123 shows our detection rates compared against other multi - view systems for which results were given on these data sets .
we want to stress here that all these systems are tested in a pose specic manner : for example , a detector tested on tilted set is trained only on frontal tilted faces .
such a detector will not be able to detect non frontal tilted faces .
combining all pose variations in one system obviously will increase the number of false positives , since false positives of view - based detectors are not necessarily correlated .
our system is designed to handle all pose variations .
this makes the comparison in table 123 somewhat unfair to our system , but we dont see any other way of comparison against other systems .
the table shows that our results on the tilted and profile sets are similar to those of the two jones & viola detectors , and even approach those of the rowley et al and schneiderman & kanade non - real - time detectors .
those detectors , however , are not designed to handle all variations in pose , and do not yield pose estimates .
more recent system reported in huang et al .
( 123 ) is also real - time and can handle all pose variation , but doesnt yield pose estimates .
unfortunately , they also report the results of pose specic detectors .
these results are not shown in table 123 , because they report different points on roc curve in the profile experiment ( 123 : 123% for 123 f . p per image ) and they didnt test on the tilted set .
even though they trained a combined detector for all pose variations , they did not test it the way we did .
their test consists in running the full detector on the profile set rotated by ( - 123 , 123 ) degrees in - plane .
unfortunately , they do not provide enough details to recreate their test set .
the right side of figure 123 shows our performance at pose estimation .
to make this graph , we xed the detection threshold at a value that resulted in about 123 false positives per image over all three data sets .
we then compared the pose estimates for all detected faces ( including those not in the standard subsets ) against our manual pose annotations .
note that this test is more difcult than typical tests of pose estimation systems , where faces are rst localized by hand .
when we hand - localize these faces , 123% of yaws and 123% of in - plane rotations are correctly estimated to within
synergistic face detection and pose estimation with energy - based models
figure 123 : some example face detections .
each white box shows the location of a detected face .
the angle of each box indicates the estimated in - plane rotation .
the black crosshairs within each box indicate the estimated yaw .
osadchy , lecun and miller
figure 123 : more examples of face detections .
synergistic face detection and pose estimation with energy - based models
false positives per image ! 123
jones and viola ( 123 ) ( tilted ) jones and viola ( 123 ) ( prole ) rowley et al .
( 123a ) schneiderman and kanade ( 123 )
data set !
123% 123% 123% 123% 123% 123%
table 123 : comparisons of our results with other multi - view detectors .
each column shows the detec - tion rates for a given average number of false positives per image ( these rates correspond to those for which other authors have reported results ) .
results for real - time detectors are shown in bold .
note that ours is the only single detector that can be tested on all data sets
the system we have presented here integrates detection and pose estimation by training a convolu - tional network to map faces to points on a manifold , parameterized by pose , and non - faces to points far from the manifold .
the network is trained by optimizing a loss function of three variables image , pose , and face / non - face label .
when the three variables match , the energy function is trained to have a small value , when they do not match , it is trained to have a large value .
this system has several desirable properties :
( cid : 123 ) the use of a convolutional network makes it fast .
at typical webcam resolutions , it can process 123 frames per second on a 123ghz pentium 123
( cid : 123 ) it is robust to a wide range of poses , including variations in yaw up to ( cid : 123 ) 123 ( cid : 123 ) , in - plane rotation up to ( cid : 123 ) 123 ( cid : 123 ) , and pitch up to ( cid : 123 ) 123 ( cid : 123 ) .
this has been veried with tests on three standard data sets , each designed to test robustness against a single dimension of pose variation .
( cid : 123 ) at the same time that it detects faces , it produces estimates of their pose .
on the standard data sets , the estimates of yaw and in - plane rotation are within 123 ( cid : 123 ) of manual estimates over 123% and 123% of the time , respectively .
we have shown experimentally that our systems accuracy at both pose estimation and face
detection is increased by training for the two tasks together .
