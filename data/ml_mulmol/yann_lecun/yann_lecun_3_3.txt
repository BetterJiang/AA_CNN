adaptive sparse coding methods learn a possibly overcomplete set of basis functions , such that natural image patches can be reconstructed by linearly combining a small subset of these bases .
the applicability of these methods to visual object recognition tasks has been limited because of the prohibitive cost of the optimization algorithms required to compute the sparse representation .
in this work we propose a simple and ecient algorithm to learn basis functions .
after training , this model also provides a fast and smooth approximator to the optimal representation , achieving even better accuracy than exact sparse coding algorithms on visual object
object recognition is one of the most challenging tasks in computer vision .
most methods for visual recognition rely on handcrafted features to represent images .
it has been shown that making these representations adaptive to image data can improve performance on vision tasks as demonstrated in ( 123 ) in a supervised
presented at opt 123 optimization for machine learning workshop , neural informa -
tion processing systems , 123
learning framework and in ( 123 , 123 ) using unsupervised learning .
learning sparse representations can be advantageous since features are more likely to be linearly separable in a high - dimensional space and they are more robust to noise .
many sparse coding algorithms have been shown to learn good local feature extractors for natural images ( 123 , 123 , 123 , 123 , 123 ) .
however , application of these methods to vision problems has been limited due to prohibitive cost of calculating sparse representations for a given image ( 123 ) .
in this work , we propose an algorithm named predictive sparse decomposi - tion ( psd ) that can simultaneously learn an overcomplete linear basis set , and produce a smooth and easy - to - compute approximator that predicts the optimal sparse representation .
experiments demonstrate that the predictor is over 123 times faster than the fastest sparse optimization algorithm , and yet produces features that yield better recognition accuracy on visual object recognition tasks than the optimal representations produced through optimization .
123 sparse coding algorithms
finding a representation z rm for a given signal y rn by linear combina - tion of an overcomplete set of basis vectors , columns of matrix b rnm with m > n , has innitely many solutions .
in optimal sparse coding , the problem is
min ||z||123 s . t .
y = bz
where the 123 norm is dened as the number of non - zero elements in a given vector .
unfortunately , the solution to this problem requires a combinatorial search , intractable in high - dimensional spaces .
matching pursuit methods ( 123 ) oer a greedy approximation to this problem .
another way to approximate this problem is to make a convex relaxation by turning the 123 norm into an 123 norm ( 123 ) .
this problem , dubbed basis pursuit in the signal processing community , has been shown to give the same solution to eq .
( 123 ) , provided that the solution is sparse enough ( 123 ) .
furthermore , the problem can be written as an unconstrained optimization problem :
l ( y , z; b ) =
123 + ||z||123
this particular formulation , called basis pursuit denoising , can be seen as min - imizing an objective that penalizes the reconstruction error using a linear basis set and the sparsity of the corresponding representation .
many recent works have focused on eciently solving the problem in eq .
( 123 ) ( 123 , 123 , 123 , 123 , 123 , 123 ) .
yet , inference requires running some sort of iterative minimization algorithm that is always computationally expensive .
additionally , some algorithms are also able to learn the set of basis functions .
the learning procedure nds the b matrix that minimizes the same loss of eq .
the columns of b are constrained to have unit norm in order to prevent trivial solutions where the loss is minimized by scaling down the coecients
123 the algorithm
while scaling up the bases .
learning proceeds by alternating the optimization over z to infer the representation for a given set of bases b , and the minimization over b for the given set of optimal z found at the previous step .
loosely speaking , basis functions learned on natural images under sparsity constraints are localized oriented edge detectors reminiscent of gabor wavelets .
123 the algorithm
in order to make inference ecient , we train a non - linear regressor that maps in - put patches y to sparse representations z .
we consider the following nonlinear
f ( y ; g , w , d ) = g tanh ( w y + d )
where w rmn is a lter matrix , d rm is a vector of biases , tanh is the hyperbolic tangent non - linearity , and g rmm is a diagonal matrix of gain coecients allowing the outputs of f to compensate for the scaling of the input , given that the reconstruction performed by b uses bases with unit norm .
let pf collectively denote the parameters that are learned in this predictor , pf = ( g , w , d ) .
the goal of the algorithm is to make the prediction of the regressor , f ( y ; pf ) as close as possible to the optimal set of coecients : z = arg minz l ( y , z; b ) in eq .
this optimization can be carried out separately after the problem in eq .
( 123 ) has been solved .
however , training becomes much faster by jointly optimizing the pf and the set of bases b all together .
this is achieved by adding another term to the loss function in eq .
( 123 ) , enforcing the representation z to be as close as possible to the feed - forward prediction f ( y ; pf ) :
l ( y , z; b , pf ) = ky bzk123
123 + kzk123 + kz f ( y ; pf ) k123
minimizing this loss with respect to z produces a representation that simul - taneously reconstructs the patch , is sparse , and is not too dierent from the predicted representation .
if multiple solutions to the original loss ( without the prediction term ) exist , minimizing this compound loss will drive the system towards producing basis functions and optimal representations that are easily predictable .
after training , the function f ( y ; pf ) will provide good and smooth approximations to the optimal sparse representations .
note that , a linear map - ping would not be able to produce sparse representations using an overcomplete set because of the non - orthogonality of the lters , therefore a non - linear map - ping is required .
the goal of learning is to nd the optimal value of the basis functions b , as well as the value of the parameters in the regressor pf .
learning proceeds by an on - line block coordinate gradient descent algorithm , alternating the following two steps for each training sample y :
123 the algorithm
keeping the parameters pf and b constant , minimize l ( y , z; b , pf ) of eq .
( 123 ) with respect to z , starting from the initial value provided by the regressor f ( y ; pf ) .
in our experiments we use gradient descent , but any other optimization method can be used;
using the optimal value of the coecients z provided by the previous step , update the parameters pf and b by one step of stochastic gradient descent; the update is : u u l u , where u collectively denotes ( pf , b ) and is the step size .
the columns of b are then re - scaled to
interestingly , we recover dierent algorithms depending on the value of the
the loss of eq .
( 123 ) reduces to the one in eq .
the learning algorithm becomes similar to olshausen and fields sparse coding algo - rithm ( 123 ) .
the regressor is trained separately from the set of basis functions
( 123 , + ) .
the parameters are updated taking into account also the constraint on the representation , using the same principle employed by sesm training ( 123 ) , for instance .
the additional constraint on the representation ( the third term in eq .
( 123 ) ) becomes an equality , i . e .
z = f ( y ; pf ) , and the model becomes similar to an auto - encoder neural network with a sparsity regularization term acting on the internal representation z instead of a regularization acting on the parameters pf and b .
in this paper , we always set = 123
however , sec .
123 shows that training the regressor after training the set of bases b yields similar performance in terms of recognition accuracy .
when the regressor is trained afterwards , the approxi - mate representation is usually less sparse and the overall training time increases considerably .
finally , additional experiments not reported here show that train - ing the system as an auto - encoder ( + ) provides a very fast and ecient algorithm that can produce good representations when the dimensionality of the representation is not much greater than the input dimensionality , i . e .
when the sparse representation is highly overcomplete the block - coordinate de - scent algorithm with ( 123 , + ) provides better features .
once the parameters are learned , inferring the representation z can be done in optimal inference consists of setting the representation to z = arg minz l , where l is dened in eq .
( 123 ) , by running an iterative gradient descent algorithm involving two possibly large matrix - vector multiplications at each iteration ( one for computing the value of the objective , and one for computing the derivatives
approximate inference , on the other hand sets the representation to the value produced by f ( y ; pf ) as given in eq .
( 123 ) , involving only a forward prop - agation through the regressor , i . e .
a single matrix - vector multiplication .
first , we demonstrate that the proposed algorithm ( psd ) is able to produce good features for recognition by comparing to other unsupervised feature extrac - tion algorithms , principal components analysis ( pca ) , restricted boltzman machine ( rbm ) ( 123 ) , and sparse encoding symmetric machine ( sesm ) ( 123 ) .
then , we compare the recognition accuracy and inference time of psd feed - forward approximation to feature sign algorithm ( 123 ) , on the caltech 123 dataset ( 123 ) .
finally we investigate the stability of representations under naturally changing
123 comparison against pca , rbm and sesm on the
the mnist dataset has a training set with 123 , 123 handwritten digits of size 123x123 pixels , and a test set with 123 , 123 digits .
each image is preprocessed by normalizing the pixel values so that their standard deviation is equal to in this experiment the sparse representation has 123 units .
this internal representation is used as a global feature vector and fed to a linear regularized logistic regression classier .
123 shows the comparison between psd ( using feed - forward approximate codes ) and , pca , sesm ( 123 ) , and rbm ( 123 ) .
even though psd provides the worst reconstruction error , it can achieve the best recognition accuracy on the test set under dierent number of training samples per class .
figure 123 : classication error on mnist as a function of reconstruction error using raw pixel values and , pca , rbm , sesm and psd features .
left - to - right : 123 - 123 - 123 samples per class are used for training a linear classier on the features .
the unsupervised algorithms were trained on the rst 123 , 123 training samples of the mnist dataset ( 123 ) .
table 123 : comparison between representations produced by fs ( 123 ) and psd .
in order to compute the snr , the noise is dened as ( signal approximation ) .
comparison ( signal / approximation ) 123
psd optimal / psd predictor 123
fs / psd optimal 123
fs / psd predictor 123
fs / regressor
signal to noise ratio ( snr )
123 comparison with exact algorithms
in order to quantify how well our jointly trained predictor given in eq .
( 123 ) approximates the optimal representations obtained by minimizing the loss in eq .
( 123 ) and the optimal representations that are produced by an exact algorithm minimizing eq .
( 123 ) such as feature sign ( 123 ) ( fs ) , we measure the average signal to noise ratio123 ( snr ) over a test dataset of 123 , 123 natural image patches of size 123x123
the data set of images was constructed by randomly picking 123x123 patches from the images of the berkeley dataset converted to gray - scale values , and these patches were normalized to have zero mean and unit standard deviation .
the algorithms were trained to learn sparse codes with 123 units123
we compare representations obtained by psd predictor using the approx - imate inference , psd optimal using the optimal inference , fs minimizing eq .
( 123 ) with ( 123 ) , and regressor that is separately trained to approximate the exact optimal codes produced by fs .
the results given in table 123 show that psd direct predictor achieves about the same snr on the true optimal sparse representations produced by fs , as the regressor that was trained to predict
despite the lack of absolute precision in predicting the exact optimal sparse codes , psd predictor achieves even better performance in recognition .
the caltech 123 dataset is pre - processed in the following way : 123 ) each image is converted to gray - scale , 123 ) it is down - sampled so that the longest side is 123 pixels , 123 ) the mean is subtracted and each pixel is divided by the image standard deviation , 123 ) the image is locally normalized by subtracting the weighted local mean from each pixel and dividing it by the weighted norm if this is larger than 123 with weights forming a 123x123 gaussian window centered on each pixel , and 123 ) the image is 123 - padded to 123x123 pixels .
123 feature detectors ( either produced by fs or psd predictor ) were plugged into an image classication system that a ) used the sparse coding algorithms convolutionally to produce 123 feature maps of size 123x123 for each pre - processed image , b ) applied an absolute value rectication , c ) computed an average down - sampling to a spatial resolution of 123x123 and d ) used a linear svm classier to recognize the object
123sn r = 123log123 ( 123 123principal component analysis shows that the eective dimensionality of 123x123 natural image patches is about 123 since the rst 123 principal components capture the 123% of the variance in the data .
hence , a 123 - dimensional feature vector is actually an overcomplete representation for these 123x123 image patches .
figure 123 : a ) 123 basis functions of size 123x123 learned by psd , trained on the berkeley dataset .
each 123x123 block is a column of matrix b in eq .
( 123 ) , i . e .
a basis function .
b ) object recognition architecture : linear adaptive lter bank , followed by abs rectication , average down - sampling and linear svm classier .
figure 123 : a ) speed up for inferring the sparse representation achieved by psd predictor over fs for a code with 123 units .
the feed - forward extraction is more than 123 times faster .
b ) recognition accuracy versus measured sparsity ( average 123 norm of the representation ) of psd predictor compared to the to the representation of fs algorithm .
a dierence within 123% is not statistically signicant .
c ) recognition accuracy as a function of number of basis functions .
in the image ( see g .
using this system with 123 training images per class we can achieve 123% accuracy on caltech 123 dataset .
since fs nds exact sparse codes , its representations are generally sparser than those found by psd predictor trained with the same value of sparsity penalty .
hence , we compare the recognition accuracy against the measured sparsity level of the representation as shown in g .
psd is not only able to achieve better accuracy than exact sparse coding algorithms , but also , it does it much more eciently .
123 ( a ) demonstrates that our feed - forward predictor extracts features more than 123 times faster than feature sign .
in fact , the speed up is over 123 when the sparsity is set to the value that gives the highest accuracy shown in g
finally , we observe that these sparse coding algorithms are somewhat inef - cient when applied convolutionally .
many feature detectors are the translated versions of each other as shown in g .
hence , the resulting feature maps are highly redundant .
this might explain why the recognition accuracy tends to saturate when the number of lters is increased as shown in g
123 summary and future work
in order to quantify the stability of psd and fs , we investigate their behavior under naturally changing input signals .
for this purpose , we train a basis set with 123 elements , each of size 123x123 , using the psd algorithm on the berkeley ( 123 ) dataset .
this basis set is then used with fs on the standard foreman test video together with the psd predictor .
we extract 123 uniformly distributed patches from each frame with a total of 123 frames .
figure 123 : conditional probabilities for sign transitions between two consecutive frames .
for instance , p ( |+ ) shows the conditional probability of a unit being negative given that it was positive in the previous frame .
the gure on the right is used as baseline , showing the conditional probabilities computed on pairs of
for each patch , a 123 dimensional representation is calculated using both fs and the psd predictor .
the stability is measured by the number of times a unit of the representation changes its sign , either negative , zero or positive , between two consecutive frames .
since the psd predictor does not generate exact zero values , we threhsold its output units in such a way that the average number of zero units equals the one produced by fs ( roughly , only the 123% of the units are non - zero ) .
the transition probabilities are given in figure 123
it can be seen from this gure that the psd predictor generates a more stable representation of slowly varying natural frames compared to the representation produced by the exact optimization algorithm .
123 summary and future work
sparse coding algorithms can be used as pre - processor in many vision applica - tions and , in particular , to extract features in object recognition systems .
to the best of our knowledge , no sparse coding algorithm is computationally ecient because inference involves some sort of iterative optimization .
we showed that sparse codes can actually be approximated by a feed - forward regressor with - out compromising the recognition accuracy , but making the recognition process very fast and suitable for use in real - time systems .
we proposed a very simple algorithm to train such a regressor .
in the future , we plan to train the model convolutionally in order to make the sparse representation more ecient , and to build hierarchical deep models by sequentially replicating the model on the representation produced by the previous stage as successfully proposed in ( 123 ) .
