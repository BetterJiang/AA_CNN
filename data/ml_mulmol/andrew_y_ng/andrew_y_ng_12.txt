motivated in part by the hierarchical organization of the cortex , a number of al - gorithms have recently been proposed that try to learn hierarchical , or deep , structure from unlabeled data .
while several authors have formally or informally compared their algorithms to computations performed in visual area v123 ( and the cochlea ) , little attempt has been made thus far to evaluate these algorithms in terms of their delity for mimicking computations at deeper levels in the cortical hier - archy .
this paper presents an unsupervised learning model that faithfully mimics certain properties of visual area v123
specically , we develop a sparse variant of the deep belief networks of hinton et al .
( 123 ) .
we learn two layers of nodes in the network , and demonstrate that the rst layer , similar to prior work on sparse coding and ica , results in localized , oriented , edge lters , similar to the gabor functions known to model v123 cell receptive elds .
further , the second layer in our model encodes correlations of the rst layer responses in the data .
specically , it picks up both colinear ( contour ) features as well as corners and junctions .
more interestingly , in a quantitative comparison , the encoding of these more complex corner features matches well with the results from the ito & komatsus study of biological v123 responses .
this suggests that our sparse variant of deep belief networks holds promise for modeling more higher - order features .
the last few years have seen signicant interest in deep learning algorithms that learn layered , hierarchical representations of high - dimensional data .
( 123 , 123 , 123 , 123 ) .
much of this work appears to have been motivated by the hierarchical organization of the cortex , and indeed authors frequently compare their algorithms output to the oriented simple cell receptive elds found in visual area v123
( e . g . , ( 123 , 123 , 123 ) ) indeed , some of these models are often viewed as rst attempts to elucidate what learning algorithm ( if any ) the cortex may be using to model natural image statistics .
however , to our knowledge no serious attempt has been made to directly relate , such as through quantitative comparisons , the computations of these deep learning algorithms to areas deeper in the cortical hierarchy , such as to visual areas v123 , v123 , etc .
in this paper , we develop a sparse variant of hintons deep belief network algorithm , and measure the degree to which it faithfully mimics biological measurements of v123
specically , we take ito & komatsu ( 123 ) s characterization of v123 in terms of its responses to a large class of angled bar stimuli , and quantitatively measure the degree to which the deep belief network algorithm generates similar responses .
deep architectures attempt to learn hierarchical structure , and hold the promise of being able to rst learn simple concepts , and then successfully build up more complex concepts by composing together the simpler ones .
for example , hinton et al .
( 123 ) proposed an algorithm based on learning individual layers of a hierarchical probabilistic graphical model from the bottom up .
bengio et al .
( 123 ) proposed a similarly greedy algorithm , one based on autoencoders .
ranzato et al .
( 123 ) developed an energy - based hierarchical algorithm , based on a sequence of sparsied autoencoders / decoders .
in related work , several studies have compared models such as these , as well as non - hierarchical / non - deep learning algorithms , to the response properties of neurons in area v123
a study by van hateren and van der schaaf ( 123 ) showed that the lters learned by independent components analysis ( ica ) ( 123 ) on natural image data match very well with the classical receptive elds of v123 simple cells .
( filters learned by sparse coding ( 123 , 123 ) also similarly give responses similar to v123 simple cells . ) our work takes inspiration from the work of van hateren and van der schaaf , and represents a study that is done in a similar spirit , only extending the comparisons to a deeper area in the cortical hierarchy , namely visual area v123
123 biological comparison 123 features in early visual cortex : area v123 the selectivity of neurons for oriented bar stimuli in cortical area v123 has been well documented ( 123 , 123 ) .
the receptive eld of simple cells in v123 are localized , oriented , bandpass lters that resemble gabor lters .
several authors have proposed models that have been either formally or informally shown to replicate the gabor - like properties of v123 simple cells .
many of these algorithms , such as ( 123 , 123 , 123 , 123 ) , compute a ( approximately or exactly ) sparse representation of the natural stimuli data .
these results are consistent with the efcient coding hypothesis which posits that the goal of early visual processing is to encode visual information as efciently as possible ( 123 ) .
some hierarchical extensions of these models ( 123 , 123 , 123 ) are able to learn features that are more complex than simple oriented bars .
for example , hierarchical sparse models of natural images have accounted for complex cell receptive elds ( 123 ) , topography ( 123 , 123 ) , colinearity and contour coding ( 123 ) .
other models , such as ( 123 ) , have also been shown to give v123 complex cell - like properties .
123 features in visual cortex area v123 it remains unknown to what extent the previously described algorithms can learn higher order fea - tures that are known to be encoded further down the ventral visual pathway .
in addition , the response properties of neurons in cortical areas receiving projections from area v123 ( e . g . , area v123 ) are not nearly as well documented .
it is uncertain what type of stimuli cause v123 neurons to respond opti - mally ( 123 ) .
one v123 study by ( 123 ) reported that the receptive elds in this area were similar to those in the neighboring areas v123 and v123
the authors interpreted their ndings as suggestive that area v123 may serve as a place where different channels of visual information are integrated .
however , quantitative accounts of responses in area v123 are few in number .
in the literature , we identied two sets of quantitative data that give us a good starting point for making measurements to determine whether our algorithms may be computing similar functions as area v123
in one of these studies , ito and komatsu ( 123 ) investigated how v123 neurons responded to angular stim - uli .
they summarized each neurons response with a two - dimensional visualization of the stimuli set called an angle prole .
by making several axial measurements within the prole , the authors were able to compute various statistics about each neurons selectivity for angle width , angle ori - entation , and for each separate line component of the angle ( see figure 123 ) .
approximately 123% of the neurons responded to specic angle stimuli .
they found neurons that were selective for only one line component of its peak angle as well as neurons selective for both line components .
these neurons yielded angle proles resembling those of cell 123 and cell 123 in figure 123 , respectively .
in addition , several neurons exhibited a high amount of selectivity for its peak angle producing angle proles like that of cell 123 in figure 123
no neurons were found that had more elongation in a di - agonal axis than in the horizontal or vertical axes , indicating that neurons in v123 were not selective for angle width or orientation .
therefore , an important conclusion made from ( 123 ) was that a v123 neurons response to an angle stimulus is highly dependent on its responses to each individual line component of the angle .
while the dependence was often observed to be simply additive , as was the case with neurons yielding proles like those of cells 123 and 123 in figure 123 ( right ) , this was not always the case .
123 neurons had very small peak response areas and yielded proles like that of cell 123 in figure 123 ( right ) , thus indicating a highly specic tuning to an angle stimulus .
while the former responses suggest a simple linear computation of v123 neural responses , the latter responses suggest a nonlinear computation ( 123 ) .
the analysis methods adopted in ( 123 ) are very useful in characterizing the response properties , and we use these methods to evaluate our own model .
another study by hegde and van essen ( 123 ) studied the responses of a population of v123 neurons to complex contour and grating stimuli .
they found several v123 neurons responding maximally for angles , and the distribution of peak angles for these neurons is consistent with that found by ( 123 ) .
in addition , several v123 neurons responded maximally for shapes such as intersections , tri - stars , ve - point stars , circles , and arcs of varying length .
figure 123 : ( images from ( 123 ) ; courtesy of ito and komatsu ) left : visualization of angle proles .
the upper - right and lower - left triangles contain the same stimuli .
( a , b ) darkened squares correspond to stimuli that elicited a large response .
the peak responses are circled .
( c ) the arrangement of the gure is so that one line component remains constant as one moves along any vertical or horizontal axis .
( d ) the angles width remains constant as one moves along a the diagonal indicated ( e ) the angle orientation remains constant as one moves along the diagonal indicated .
after identifying the optimal stimuli for a neuron in the prole , the number of stimuli along these various axes ( as in c , d , e ) eliciting responses larger than 123% of the peak response measure the neurons tolerance to perturbations to the line components , peak angle width , and orientation , respectively .
right : examples of 123 typical angle proles .
as before , stimuli eliciting large responses are highlighted .
cell 123 has a selective response to a stimulus , so there is no elongation along any axis .
cell 123 has one axis of elongation , indicating selectivity for one orientation .
cell 123 has two axes of elongation , and responds strongly so long as either of two edge orientations is present .
cell 123 has no clear axis of elongation .
hinton et al .
( 123 ) proposed an algorithm for learning deep belief networks , by treating each layer as a restricted boltzmann machine ( rbm ) and greedily training the network one layer at a time from the bottom up ( 123 , 123 ) .
in general , however , rbms tend to learn distributed , non - sparse representations .
based on results from other methods ( e . g . , sparse coding ( 123 , 123 ) , ica ( 123 ) , heavy - tailed models ( 123 ) , and energy based models ( 123 ) ) , sparseness seems to play a key role in learning gabor - like lters .
therefore , we modify hinton et al . s learning algorithm to enable deep belief nets to learn sparse 123 sparse restricted boltzmann machines we begin by describing the restricted boltzmann machine ( rbm ) , and present a modied version of it .
an rbm has a set of hidden units h , a set of visible units v , and symmetric connections weights between these two layers represented by a weight matrix w .
suppose that we want to model k dimensional real - valued data using an undirected graphical model with n binary hidden units .
the negative log probability of any state in the rbm is given by the following energy function : 123
log p ( v , h ) = e ( v , h ) =
here , is a parameter , hj are hidden unit variables , vi are visible unit variables .
informally , the maximum likelihood parameter estimation problem corresponds to learning wij , ci and bj so as to minimize the energy of states drawn from the data distribution , and raise the energy of states that are improbable given the data .
under this model , we can easily compute the conditional probability distributions .
holding either h or v xed , we can sample from the other as follows :
p ( vi|h ) = n ( cid : 123 ) j wijhj , 123 ( cid : 123 ) p ( hj|v ) = logistic ( cid : 123 ) 123 123 ( bj +p j bjhj +p i civi +p
i wijvi ) ( cid : 123 ) .
123due to space constraints , we present an energy function only for the case of real - valued visible units .
it is also straightforward to formulate a sparse rbm with binary - valued visible units; for example , we can write the
energy function as e ( v , h ) = 123 / 123 ( p
i , j viwijhj ) ( see also ( 123 ) ) .
h p ( v ( l ) , h ( l ) ) + pn
minimize ( wij , ci , bj ) pm
here , n ( ) is the gaussian density , and logistic ( ) is the logistic function .
for training the parameters of the model , the objective is to maximize the log - likelihood of the data .
we also want hidden unit activations to be sparse; thus , we add a regularization term that penalizes a deviation of the expected activation of the hidden units from a ( low ) xed level p . 123 thus , given a training set ( v ( 123 ) , .
, v ( m ) ) comprising m examples , we pose the following optimization problem : where e ( ) is the conditional expectation given the data , is a regularization constant , and p is a constant controlling the sparseness of the hidden units hj .
thus , our objective is the sum of a log - likelihood term and a regularization term .
in principle , we can apply gradient descent to this problem; however , computing the gradient of the log - likelihood term is expensive .
fortunately , the contrastive divergence learning algorithm gives an efcient approximation to the gradient of the log - likelihood ( 123 ) .
building upon this , on each iteration we can apply the contrastive divergence update rule , followed by one step of gradient descent using the gradient of the regularization term . 123 the details of our procedure are summarized in algorithm 123
algorithm 123 sparse rbm learning algorithm
j=123 | p 123
update the parameters using contrastive divergence learning rule .
more specically ,
wij : = wij + ( hvihjidata hvihjirecon ) ci : = ci + ( hviidata hviirecon ) bj : = bj + ( hbjidata hbjirecon ) ,
where is a learning rate , and hirecon is an expectation over the reconstruction data , estimated using one iteration of gibbs sampling ( as in equations 123 , 123 ) .
update the parameters using the gradient of the regularization term .
repeat steps 123 and 123 until convergence .
123 learning deep networks using sparse rbm once a layer of the network is trained , the parameters wij , bj , cis are frozen and the hidden unit values given the data are inferred .
these inferred values serve as the data used to train the next higher layer in the network .
hinton et al .
( 123 ) showed that by repeatedly applying such a procedure , one can learn a multilayered deep belief network .
in some cases , this iterative greedy algorithm can further be shown to be optimizing a variational bound on the data likelihood , if each layer has at least as many units as the layer below ( although in practice this is not necessary to arrive at a desirable solution; see ( 123 ) for a detailed discussion ) .
in our experiments using natural images , we learn a network with two hidden layers , with each layer learned using the sparse rbm algorithm described in section 123 .
123 learning strokes from handwritten digits we applied the sparse rbm algorithm to the mnist handwritten digit dataset . 123 we learned a sparse rbm with 123 visible units and 123 hidden units .
the learned bases are shown in figure 123
( each basis corresponds to one column of the weight matrix w left - multiplied by the unwhitening matrix . ) many bases found by the al - gorithm roughly represent different strokes of which handwritten digits are comprised .
this is consistent
figure 123 : bases learned from mnist data
123less formally , this regularization ensures that the ring rate of the model neurons ( corresponding to the latent random variables hj ) are kept at a certain ( fairly low ) level , so that the activations of the model neurons are sparse .
similar intuition was also used in other models ( e . g . , see olshausen and field ( 123 ) ) .
123to increase computational efciency , we made one additional change .
note that the regularization term is dened using a sum over the entire training set; if we use stochastic gradient descent or mini - batches ( small subsets of the training data ) to estimate this term , it results in biased estimates of the gradient .
to ameliorate this , we used mini - batches , but in the gradient step that tries to minimize the regularization term , we update only the bias terms bjs ( which directly control the degree to which the hidden units are activated , and thus their sparsity ) , instead of updating all the parameters bj and wijs .
123downloaded from http : / / yann . lecun . com / exdb / mnist / .
each pixel was normalized to the unit interval , and we used pca whitening to reduce the dimension to 123 principal components for computational efciency .
( similar results were obtained without whitening . )
figure 123 : 123 rst layer bases learned from the van hateren natural image dataset , using our algorithm .
figure 123 : visualization of 123 second layer bases ( model v123 receptive elds ) , learned from natural images .
each small group of 123 - 123 ( arranged in a row ) images shows one model v123 unit; the leftmost patch in the group is a visualization of the model v123 basis , and is obtained by taking a weighted linear combination of the rst layer v123 bases to which it is connected .
the next few patches in the group show the rst layer bases that have the strongest weight connection to the model v123 basis .
with results obtained by applying different algorithms to learn sparse representations of this data set ( e . g . , ( 123 , 123 ) ) .
123 learning from natural images we also applied the algorithm to a training set a set of 123 - by - 123 natural image patches , taken from a dataset compiled by van hateren . 123 we learned a sparse rbm model with 123 visible units and 123 hidden units .
the learned bases are shown in figure 123; they are oriented , gabor - like bases and resemble the receptive elds of v123 simple cells . 123 123 learning a two - layer model of natural images using sparse rbms we further learned a two - layer network by stacking one sparse rbm on top of another ( see sec - tion 123 for details . ) 123 after learning , the second layer weights were quite sparsemost of the weights were very small , and only a few were either highly positive or highly negative .
positive
123the images were obtained from http : / / hlab . phys . rug . nl / imlib / index . html .
we used 123 , 123 123 - by - 123 image patches randomly sampled from an ensemble of 123 images; each subset of 123 patches was used as a mini - batch .
123most other authors experiments to date using regular ( non - sparse ) rbms , when trained on such data , seem to have learned relatively diffuse , unlocalized bases ( ones that do not represent oriented edge lters ) .
while sensitive to the parameter settings and requiring a long training time , we found that it is possible in some cases to get a regular rbm to learn oriented edge lter bases as well .
but in our experiments , even in these cases we found that repeating this process to build a two layer deep belief net ( see section 123 ) did not encode a signicant number of corners / angles , unlike one trained using the sparse rbm; therefore , it showed signicantly worse match to the ito & komatsu statistics .
for example , the fraction of model v123 neurons that respond strongly to a pair of edges near right angles ( formally , have peak angle in the range 123 - 123 degrees ) was 123% for the regular rbm , whereas it was 123% for the sparse rbm ( and ito & komatsu reported 123% ) .
see section 123 for more details .
123for the results reported in this paper , we trained the second layer sparse rbm with real - valued visible units; however , the results were very similar when we trained the second layer sparse rbm with binary - valued visible units ( except that the second layer weights became less sparse ) .
figure 123 : top : visualization of four learned model v123 neurons .
( visualization in each row of four or ve patches follows format in figure 123 ) bottom : angle stimulus response prole for model v123 neurons in the top row .
the 123*123 grid of stimuli follows ( 123 ) , in which the orientation of two lines are varied to form different angles .
as in figure 123 , darkened patches represent stimuli to which the model v123 neuron responds strongly; also , a small black square indicates the overall peak response .
weights represent excitatory connections between model v123 and model v123 units , whereas negative elements represent inhibitory connections .
by visualizing the second layer bases as shown in fig - ure 123 , we observed bases that encoded co - linear rst layer bases as well as edge junctions .
this shows that by extending the sparse rbm to two layers and using greedy learning , the model is able to learn bases that encode contours , angles , and junctions of edges .
123 evaluation experiments we now more quantitatively compare the algorithms learned responses to biological measure - 123 method : ito - komatsu paper protocol we now describe the procedure we used to compare our model with the experimental data in ( 123 ) .
we generated a stimulus set consisting of the same set of angles ( pairs of edges ) as ( 123 ) .
to identify the center of each model neurons receptive eld , we translate all stimuli densely over the 123x123 input image patch , and identify the position at which the maximum response is elicited .
all measures are then taken with all angle stimuli centered at this position . 123 using these stimuli , we compute the hidden unit probabilities from our model v123 and v123 neurons .
in other words , for each stimulus we compute the rst hidden layer activation probabilities , then feed this probability as data to the second hidden layer and compute the activation probabilities again in the same manner .
following a protocol similar to ( 123 ) , we also eliminate from consideration the model neurons that do not respond strongly to corners and edges . 123 some representative results are shown in figure 123
( the four angle proles shown are fairly typical of those obtained in our experiments . ) we see that all the v123 bases in figure 123 have maximal response when its strongest v123 - basis components are aligned with the stimulus .
thus , some of these bases do indeed seem to encode edge junctions or crossings .
we also compute similar summary statistics as ( 123 ) ( described in figure 123 ( c , d , e ) ) , that more quanti - tatively measure the distribution of v123 or model v123 responses to the different angle stimuli .
figure 123 plots the responses of our model , together with v123 data taken from ( 123 ) .
along many dimensions , the results from our model match that from the macaque v123 fairly well .
123the results we report below were very insensitive to the choices of and .
we set to 123 and 123 for the rst and second layers ( chosen to be on the same scale as the standard deviation of the data and the rst - layer activations ) , and = 123 / p in each layer .
we used p = 123 and 123 for the rst and second layers .
123other details : the stimulus set is created by generating a binary - mask image , that is then scaled to nor - malize contrast .
to determine this scaling constant , we used single bar images by translating and rotating to all possible positions , and xed the constant such that the top 123% ( over all translations and rotations ) of the stimuli activate the model v123 cells above 123 .
this normalization step corrects for the rbm having been trained on a data distribution ( natural images ) that had very different contrast ranges than our test stimulus set .
123in detail , we generated a set of random low - frequency stimulus , by generating small random kxk ( k=123 , 123 , 123 ) images with each pixel drawn from a standard normal distribution , and rescaled the image using bicubic interpolation to 123x123 patches .
these stimulus are scaled such that about 123% of the v123 bases res maximally to these random stimuli .
we then exclude the v123 bases that are maximally activated to these ran - dom stimuli from the subsequent analysis .
figure 123 : images show distributions over stimulus response statistics ( averaged over 123 trials ) from our algo - rithm ( blue ) and in data taken from ( 123 ) ( green ) .
the ve gures show respectively ( i ) the distribution over peak angle response ( ranging from 123 to 123 degrees; each bin represents a range of 123 degrees ) , ( ii ) distribution over tolerance to primary line component ( figure 123c , in dominant vertical or horizontal direction ) , ( iii ) distribution over tolerance to secondary line component ( figure 123c , in non - dominant direction ) , ( iv ) tolerance to angle width ( figure 123d ) , ( v ) tolerance to angle orientation ( figure 123e ) .
see figure 123 caption , and ( 123 ) , for details .
figure 123 : visualization of a number of model v123 neurons that maximally respond to various complex stimuli .
each row of seven images represents one v123 basis .
in each row , the leftmost image shows a linear combination of the top three weighted v123 components that comprise the v123 basis; the next three images show the top three optimal stiimuli; and the last three images show the top three weighted v123 bases .
the v123 bases shown in the gures maximally respond to acute angles ( left ) , obtuse angles ( middle ) , and tri - stars and junctions ( right ) .
123 complex shaped model v123 neurons our second experiment represents a comparison to a subset of the results described in hegde and van essen ( 123 ) .
we generated a stimulus set comprising some ( 123 ) s complex shaped stimuli : angles , single bars , tri - stars ( three line segments that meet at a point ) , and arcs / circles , and measured the response of the second layer of our sparse rbm model to these stimuli . 123 we observe that many v123 bases are activated mainly by one of these different stimulus classes .
for example , some model v123 neurons activate maximally to single bars; some maximally activate to ( acute or obtuse ) angles; and others to tri - stars ( see figure 123 ) .
further , the number of v123 bases that are maximally activated by acute angles is signicantly larger than the number of obtuse angles , and the number of v123 bases that respond maximally to the tri - stars was much smaller than both preceding cases .
this is also consistent with the results described in ( 123 ) .
we presented a sparse variant of the deep belief network model .
when trained on natural images , this model learns local , oriented , edge lters in the rst layer .
more interestingly , the second layer captures a variety of both colinear ( contour ) features as well as corners and junctions , that in a quantitative comparison to measurements of v123 taken by ito & komatsu , appeared to give responses that were similar along several dimensions .
this by no means indicates that the cortex is a sparse rbm , but perhaps is more suggestive of contours , corners and junctions being fundamental to the statistics of natural images . 123 nonetheless , we believe that these results also suggest that sparse
123all the stimuli were 123 - by - 123 pixel image patches .
we applied the protocol described in section 123 to the
stimulus data , to compute the model v123 and v123 responses .
123in preliminary experiments , we also found that when these ideas are applied to self - taught learning ( 123 ) ( in which one may use unlabeled data to identify features that are then useful for some supervised learning task ) , using a two - layer sparse rbm usually results in signicantly better features for object recognition than using only a one - layer network .
