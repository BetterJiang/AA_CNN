in recent years , deep learning approaches have gained signicant interest as a way of building hierarchical representations from unlabeled data .
however , to our knowledge , these deep learning approaches have not been extensively stud - ied for auditory data .
in this paper , we apply convolutional deep belief net - works to audio data and empirically evaluate them on various audio classication tasks .
in the case of speech data , we show that the learned features correspond to phones / phonemes .
in addition , our feature representations learned from unlabeled audio data show very good performance for multiple audio classication tasks .
we hope that this paper will inspire more research on deep learning approaches applied to a wide range of audio recognition tasks .
understanding how to recognize complex , high - dimensional audio data is one of the greatest chal - lenges of our time .
previous work ( 123 , 123 ) revealed that learning a sparse representation of auditory signals leads to lters that closely correspond to those of neurons in early audio processing in mam - mals .
for example , when sparse coding models are applied to natural sounds or speech , the learned representations ( basis vectors ) showed a striking resemblance to the cochlear lters in the auditory cortex .
in related work , grosse et al .
( 123 ) proposed an efcient sparse coding algorithm for auditory signals and demonstrated its usefulness in audio classication tasks .
however , the proposed methods have been applied to learn relatively shallow , one - layer representa - tions .
learning more complex , higher - level representation is still a non - trivial , challenging problem .
recently , many promising approaches have been proposed to learn the processing steps of the sec - ond stage and beyond ( 123 , 123 , 123 , 123 , 123 ) .
these deep learning algorithms try to learn simple features in the lower layers and more complex features in the higher layers .
however , to the best of our knowledge , these deep learning approaches have not been extensively applied to auditory data .
the deep belief network ( 123 ) is a generative probabilistic model composed of one visible ( observed ) layer and many hidden layers .
each hidden layer unit learns a statistical relationship between the units in the lower layer; the higher layer representations tend to become more complex .
the deep belief network can be efciently trained using greedy layerwise training , in which the hidden layers are trained one at a time in a bottom - up fashion ( 123 ) .
recently , convolutional deep belief networks ( 123 ) have been developed to scale up the algorithm to high - dimensional data .
similar to deep belief networks , convolutional deep belief networks can be trained in a greedy , bottom - up fashion .
by applying these networks to images , lee et al .
( 123 ) showed good performance in several visual recognition tasks ( 123 ) .
in this paper , we will apply convolutional deep belief networks to unlabeled auditory data ( such as speech and music ) and evaluate the learned feature representations on several audio classication tasks .
in the case of speech data , we show that the learned features correspond to phones / phonemes .
in addition , our feature representations outperform other baseline features ( spectrogram and mfcc )
for multiple audio classication tasks .
in particular , our method compares favorably with other state - of - the - art algorithms for the speaker identication task .
for the phone classication task , mfcc features can be augmented with our features to improve accuracy .
we also show for certain tasks that the second - layer features produce higher accuracy than the rst - layer features , which justies the use of deep learning approaches for audio classication .
finally , we show that our features give better performance in comparison to other baseline features for music classication tasks .
in our experiments , the learned features often performed much better than other baseline features when there was only a small number of labeled training examples .
to the best of our knowledge , we are the rst to apply deep learning algorithms to a range of audio classication tasks .
we hope that this paper will inspire more research on deep learning approaches applied to audio recognition tasks .
123 convolutional deep belief networks
we rst briey review convolutional restricted boltzmann machines ( crbms ) ( 123 , 123 , 123 ) as building blocks for convolutional deep belief networks ( cdbns ) .
we will follow the formulation of ( 123 ) and adapt it to a one dimensional setting .
for the purpose of this explanation , we assume that all inputs to the algorithm are single - channel time - series data with nv frames ( an nv dimensional vector ) ; however , the formulation can be straightforwardly extended to the case of multiple channels .
the crbm is an extension of the regular rbm ( 123 ) to a convolutional setting , in which the weights between the hidden units and the visible units are shared among all locations in the hidden layer .
the crbm consists of two layers : an input ( visible ) layer v and a hidden layer h .
the hidden units are binary - valued , and the visible units are binary - valued or real - valued .
consider the input layer consisting of an nv dimensional array of binary units .
to construct the hidden layer , consider k nw - dimensional lter weights w k ( also referred to as bases throughout this paper ) .
the hidden layer consists of k groups of nh - dimensional arrays ( where nh ( cid : 123 ) nv nw + 123 ) with units in group k sharing the weights w k .
there is also a shared bias bk for each group and a shared bias c for the visible units .
the energy function can then be dened as :
e ( v , h ) = k ( cid : 123 )
e ( v , h ) =
r vj+r123 k ( cid : 123 ) r vj+r123 k ( cid : 123 )
similarly , the energy function of crbm with real - valued visible units can be dened as :
the joint and conditional probability distributions are dened as follows :
p ( v , h ) = j = 123|v ) = sigmoid ( ( w k v v ) j + bk )
p ( vi = 123|h ) = sigmoid ( ( cid : 123 ) p ( vi|h ) = n ormal ( ( cid : 123 )
( w k f hk ) i + c )
( for binary visible units )
( w k f hk ) i + c , 123 )
( for real visible units ) ,
where v is a valid convolution , f is a full convolution , 123 and w k nw j+123
since all units in one layer are conditionally independent given the other layer , inference in the network can be efciently performed using block gibbs sampling .
lee et al .
( 123 ) further developed a convolutional rbm with probabilistic max - pooling , where the maxima over small neighborhoods of hidden units are computed in a probabilistically sound way .
( see ( 123 ) for more details . ) in this paper , we use crbms with probabilistic max - pooling as building blocks for convolutional deep belief networks .
( cid : 123 ) w k
123given an m - dimensional vector and an n - dimensional kernel ( where m > n ) , valid convolution gives a
( m n + 123 ) - dimensional vector , and full convolution gives a ( m + n 123 ) - dimensional vector .
for training the convolutional rbms , computing the exact gradient for the log - likelihood term is in - tractable .
however , contrastive divergence ( 123 ) can be used to approximate the gradient effectively .
since a typical crbm is highly overcomplete , a sparsity penalty term is added to the log - likelihood objective ( 123 , 123 ) .
more specically , the training objective can be written as
llikelihood ( w , b , c ) + lsparsity ( w , b , c ) ,
where llikelihood is a negative log - likelihood that measures how well the crbm approximates the input data distribution , and lsparsity is a penalty term that constrains the hidden units to having sparse average activations .
this sparsity regularization can be viewed as limiting the capacity of the network , and it often results in more easily interpretable feature representations .
once the parameters for all the layers are trained , we stack the crbms to form a convolutional deep belief network .
for inference , we use feed - forward approximation .
123 application to audio data for the application of cdbns to audio data , we rst convert time - domain signals into spectro - grams .
however , the dimensionality of the spectrograms is large ( e . g . , 123 channels ) .
we apply pca whitening to the spectrograms and create lower dimensional representations .
thus , the data we feed into the cdbn consists of nc channels of one - dimensional vectors of length nv , where nc is the number of pca components in our representation .
similarly , the rst - layer bases are comprised of nc channels of one - dimensional lters of length nw .
123 unsupervised feature learning 123 training on unlabeled timit data we trained the rst and second - layer cdbn representations using a large , unlabeled speech dataset .
first , we extracted the spectrogram from each utterance of the timit training data ( 123 ) .
the spec - trogram had a 123 ms window size with 123 ms overlaps .
the spectrogram was further processed using pca whitening ( with 123 components ) to reduce the dimensionality .
we then trained 123 rst - layer bases with a lter length ( nw ) of 123 and a max - pooling ratio ( local neighborhood size ) of 123
we further trained 123 second - layer bases using the max - pooled rst - layer activations as input , again with a lter length of 123 and a max - pooling ratio of 123
in this section , we illustrate what the network learns through visualization .
we visualize the rst - layer bases by multiplying the inverse of the pca whitening on each rst - layer basis ( figure 123 ) .
each second - layer basis is visualized as a weighted linear combination of the rst - layer bases .
figure 123 : visualization of randomly selected rst - layer cdbn bases trained on the timit data .
each column represents a temporal receptive eld of a rst - layer basis in the spectrogram space .
the frequency channels are ordered from the lowest frequency ( bottom ) to the highest frequency ( top ) .
all gures in the paper are best viewed in color .
123 . 123 phonemes and the cdbn features in figure 123 , we show how our bases relate to phonemes by comparing visualizations of each phoneme with the bases that are most activated by that phoneme .
for each phoneme , we show ve spectrograms of sound clips of that phoneme ( top ve columns in each phoneme group ) , and the ve rst - layer bases with the highest average activations on the given phoneme ( bottom ve columns in each phoneme group ) .
many of the rst - layer bases closely match the shapes of phonemes .
there are prominent horizontal bands in the lower frequencies of the rst - layer bases that respond most to vowels ( for example , ah and oy ) .
the bases that respond most
high freq .
low freq . figure 123 : visualization of the four different phonemes and their corresponding rst - layer cdbn bases .
for each phoneme : ( top ) the spectrograms of the ve randomly selected phones; ( bottom ) ve rst - layer bases with the highest average activations on the given phoneme .
to fricatives ( for example , s ) typically take the form of widely distributed areas of energy in the high frequencies of the spectrogram .
both of these patterns reect the structure of the corresponding closer inspection of the bases provides slight evidence that the rst - layer bases also capture more ne - grained details .
for example , the rst and third oy bases reect the upward - slanting pattern in the phoneme spectrograms .
the top el bases mirror the intensity patterns of the corresponding phoneme spectrograms : a high intensity region appears in the lowest frequencies , and another region of lesser intensity appears a bit higher up .
123 . 123 speaker gender information and the cdbn features
in figure 123 , we show an analysis of two - layer cdbn feature representations with respect to the gen - der classication task ( section 123 ) .
note that the network was trained on unlabeled data; therefore , no information about speaker gender was given during training .
figure 123 : ( left ) ve spectrogram samples of ae phoneme from female ( top ) / male ( bottom ) speak - ( middle ) visualization of the ve rst - layer bases that most differentially activate for fe - male / male speakers .
( right ) visualization of the ve second - layer bases that most differentially activate for female / male speakers .
for comparison with the cdbn features , randomly selected spectrograms of female ( top left ve columns ) and male ( bottom left ve columns ) pronunciations of the ae phoneme from the timit dataset are shown .
spectrograms for the female pronunciations are qualitatively distinguishable by a ner horizontal banding pattern in low frequencies , whereas male pronunciations have more blurred
example phones ( " ah " ) example phones ( " oy " ) example phones ( " el " ) example phones ( " s " ) first layer basesfirst layer basesfirst layer basesfirst layer basesexample phones ( female ) first layer bases ( " female " ) second layer bases ( " female " ) example phones ( male ) first layer bases ( " male " ) second layer bases ( " male " ) patterns .
this gender difference in the vowel pronunciation patterns is typical across the timit only the bases that are most biased to activate on either male or female speech are shown .
the bases that are most active on female speech encode the horizontal band pattern that is prominent in the spectrograms of female pronunciations .
on the other hand , the male - biased bases have more blurred patterns , which again visually matches the corresponding spectrograms .
123 application to speech recognition tasks in this section , we demonstrate that the cdbn feature representations learned from the unlabeled speech corpus can be useful for multiple speech recognition tasks , such as speaker identication , gender classication , and phone classication .
in most of our experiments , we followed the self - taught learning framework ( 123 ) .
the motivation for self - taught learning comes from situations where we are given only a small amount of labeled data and a large amount of unlabeled data;123 therefore , one of our main interests was to evaluate the different feature representations given a small number of labeled training examples ( as often assumed in self - taught learning or semi - supervised learning settings ) .
more specically , we trained the cdbn on unlabeled timit data ( as described in section 123 ) ; then we used the cdbn features for classication on labeled training / test data123 that were randomly selected from the timit corpus . 123
123 speaker identication
we evaluated the usefulness of the learned cdbn representations for the speaker identication task .
the subset of the timit corpus that we used for speaker identication has 123 speakers and 123 utterances ( sentences ) per speaker , resulting in a total of 123 utterances .
we performed 123 - way classication on this set .
for each number of utterances per speaker , we randomly selected training utterances and testing utterances and measured the classication accuracy; we report the results averaged over 123 random trials . 123 to construct training and test data for the classication task , we extracted a spectrogram from each utterance in the timit corpus .
we denote this spectrogram representation as raw features .
we computed the rst and second - layer cdbn features using the spectrogram as input .
we also computed mfcc features , widely - used standard features for generic speech recognition tasks .
as a result , we obtained spectrogram / mfcc / cdbn representations for each utterance with multiple ( typically , several hundred ) frames .
in our experiments , we used simple summary statistics ( for each channel ) such as average , max , or standard deviation over all the frames .
we evaluated the features using standard supervised classiers , such as svm , gda , and knn .
the choices of summary statistics and hyperparameters for the classiers were done using cross - validation .
we report the average classication accuracy ( over 123 random trials ) with a varying number of training examples .
table 123 shows the average classication accuracy for each feature representation .
the results show that the rst and second cdbn representations both outperform baseline features ( raw and mfcc ) .
the numbers compare mfcc and cdbn features with as many of the same factors ( such as preprocessing and classication algorithms ) as possible .
further , to make a fair comparison between cdbn features and mfcc , we used the best performing implementation123 among several standard implementations for mfcc .
our results suggest that without special preprocessing or postprocess -
123in self - taught learning , the labeled data and unlabeled data dont need to share the same labels or the same
123there are two disjoint timit data sets .
we drew unlabeled data from the larger of the two for unsupervised feature learning , and we drew labeled data from the other data set to create our training and test set for the
123in the case of phone classication , we followed the standard protocol ( e . g . , ( 123 ) ) rather than self - taught
learning framework to evaluate our algorithm in comparison to other methods .
123details : there were some exceptions to this; for the case of eight training utterances , we followed reynolds ( 123 ) ( 123 ) ; more specically , we used eight training utterances ( 123 sa sentences , 123 si sentences and rst 123 sx sentences ) ; the two testing utterances were the remaining 123 sx sentences .
we used cross validation for selecting hyperparameters for classication , except for the case of 123 utterance per speaker , where we used a randomly selected validation sentence per speaker .
123we used dan ellis implementation available at : http : / / labrosa . ee . columbia . edu / matlab /
table 123 : test classication accuracy for speaker identication using summary statistics raw mfcc cdbn l123 cdbn l123 cdbn l123+l123 #training utterances per speaker
table 123 : test classication accuracy for speaker identication using all frames
#training utterances per speaker mfcc ( ( 123 ) s method ) cdbn mfcc ( ( 123 ) ) + cdbn
ing ( besides the summary statistics which were needed to reduce the number of features ) , the cdbn features outperform mfcc features , especially in a setting with a very limited number of labeled we further experimented to determine if the cdbn features can achieve competitive performance in comparison to other more sophisticated , state - of - the - art methods .
for each feature representation , we used the classier that achieved the highest performance .
more specically , for the mfcc fea - tures we replicated reynolds ( 123 ) s method , 123 and for the cdbn features we used a svm based ensemble method . 123 as shown in table 123 , the cdbn features consistently outperformed mfcc fea - tures when the number of training examples was small .
we also combined both methods by taking a linear combination of the two classier outputs ( before taking the nal classication prediction from each algorithm ) . 123 the resulting combined classier performed the best , achieving 123% accuracy for the case of 123 training utterances per speaker .
123 speaker gender classication
we also evaluated the same cdbn features which were learned for the speaker identication task on the gender classication task .
we report the classication accuracy for various quantities of training examples ( utterances ) per gender .
for each number of training examples , we randomly sampled training examples and 123 testing examples; we report the test classication accuracy averaged over 123 trials .
as shown in table 123 , both the rst and second cdbn features outperformed the baseline features , especially when the number of training examples were small .
the second - layer cdbn features consistently performed better than the rst - layer cdbn features .
this suggests that the second - layer representation learned more invariant features that are relevant for speaker gender classication , justifying the use of deep architectures .
123 phone classication
finally , we evaluated our learned representation on phone classication tasks .
for this experiment , we treated each phone segment as an individual example and computed the spectrogram ( raw ) and mfcc features for each phone segment .
similarly , we computed the rst - layer cdbn representa - tions .
following the standard protocol ( 123 ) , we report the 123 way phone classication accuracy on the test data ( timit core test set ) for various numbers of training sentences .
for each number of training examples , we report the average classication accuracy over 123 random trials .
the summary
123details : in ( 123 ) , mfcc features ( with multiple frames ) were computed for each utterance; then a gaussian mixture model was trained for each speaker ( treating each individual mfcc frame as a input example to the gmm .
for the a given test utterance , the prediction was made by determining the gmm model that had the highest test log - likelihood .
123in detail , we treated each single - frame cdbn features as an individual example .
then , we trained a multi - class linear svm for these individual frames .
for testing , we computed svm prediction score for each speaker , and then aggregated predictions from all the frames .
overall , the highest scoring speaker was selected for the
123the constant for the linear combination was xed across all the numbers of training utterances , and it was
selected using cross validation .
table 123 : test accuracy for gender classication problem
#training utterances per gender
raw mfcc cdbn l123 cdbn l123 cdbn l123+l123
table 123 : test accuracy for phone classication problem
raw mfcc mfcc ( ( 123 ) s method ) cdbn l123 mfcc+cdbn l123 ( ( 123 ) )
results are shown in table 123
in this experiment , the rst - layer cdbn features performed better than spectrogram features , but they did not outperform the mfcc features .
however , by combining mfcc features and cdbn features , we could achieve about 123% accuracy improvement consis - tently over all the numbers of training utterances .
in the realm of phone classication , in which signicant research effort is often needed to achieve even improvements well under a percent , this is a signicant improvement .
( 123 , 123 , 123 , 123 ) this suggests that the rst - layer cdbn features learned somewhat informative features for phone classication tasks in an unsupervised way .
in contrast to the gender classication task , the second - layer cdbn features did not offer much improvement over the rst - layer cdbn features .
this result is not unexpected considering the fact that the time - scale of most phonemes roughly corre - sponds to the time - scale of the rst - layer cdbn features .
123 application to music classication tasks in this section , we assess the applicability of cdbn features to various music classication tasks .
table 123 : test accuracy for 123 - way music genre classication
raw mfcc cdbn l123 cdbn l123 cdbn l123+l123
123 music genre classication
for the task of music genre classication , we trained the rst and second - layer cdbn representa - tions on an unlabeled collection of music data . 123 first , we computed the spectrogram ( 123 ms window size with 123 ms overlaps ) representation for individual songs .
the spectrogram was pca - whitened and then fed into the cdbn as input data .
we trained 123 rst - layer bases with a lter length of 123 and a max - pooling ratio of 123
in addition , we trained 123 second - layer bases with a lter length of 123 and a max - pooling ratio of 123
we evaluated the learned cdbn representation for 123 - way genre classication tasks .
the training and test songs for the classication tasks were randomly sampled from 123 genres ( classical , electric , jazz , pop , and rock ) and did not overlap with the unlabeled data .
we randomly sampled 123 - second segments from each song and treated each segment as an individual training or testing example .
we report the classication accuracy for various numbers of training examples .
for each number of training examples , we averaged over 123 random trials .
the results are shown in table 123
in this task , the rst - layer cdbn features performed the best overall .
123available from http : / / ismir123ismir . net / ismir_contest . html .
123 music artist classication furthermore , we evaluated whether the cdbn features are useful in identifying individual artists . 123 following the same procedure as in section 123 , we trained the rst and second - layer cdbn rep - resentations from an unlabeled collection of classical music data .
some representative bases are shown in figure 123
then we evaluated the learned cdbn representation for 123 - way artist identi - cation tasks .
the disjoint sets of training and test songs for the classication tasks were randomly sampled from the songs of four artists .
the unlabeled data and the labeled data did not include the same artists .
we randomly sampled 123 - second segments from each song and treated each segment as an individual example .
we report the classication accuracy for various quantities of training ex - amples .
for each number of training examples , we averaged over 123 random trials .
the results are shown in table 123
the results show that both the rst and second - layer cdbn features performed better than the baseline features , and that either using the second - layer features only or combining the rst and the second - layer features yielded the best results .
this suggests that the second - layer cdbn representation might have captured somewhat useful , higher - level features than the rst - layer
figure 123 : visualization of randomly selected rst - layer cdbn bases trained on classical music data .
table 123 : test accuracy for 123 - way artist identication
raw mfcc cdbn l123 cdbn l123 cdbn l123+l123
123 discussion and conclusion modern speech datasets are much larger than the timit dataset .
while the challenge of larger datasets often lies in considering harder tasks , our objective in using the timit data was to restrict the amount of labeled data our algorithm had to learn from .
it remains an interesting problem to apply deep learning to larger datasets and more challenging tasks .
in this paper , we applied convolutional deep belief networks to audio data and evaluated on various audio classication tasks .
by leveraging a large amount of unlabeled data , our learned features often equaled or surpassed mfcc features , which are hand - tailored to audio data .
furthermore , even when our features did not outperform mfcc , we could achieve higher classication accuracy by combining both .
also , our results show that a single cdbn feature representation can achieve high performance on multiple audio recognition tasks .
we hope that our approach will inspire more research on automatically learning deep feature hierarchies for audio data .
we thank yoshua bengio , dan jurafsky , yun - hsuan sung , pedro moreno , roger grosse for helpful discussions .
we also thank anonymous reviewers for their constructive comments .
this work was supported in part by the national science foundation under grant efri - 123 , and in part by the ofce of naval research under muri n123
123in our experiments , we found that artist identication task was more difcult than the speaker identication
task because the local sound patterns can be highly variable even for the same artist .
