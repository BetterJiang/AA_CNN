semantic word spaces have been very use - ful but cannot express the meaning of longer phrases in a principled way .
further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation re - sources and more powerful models of com - position .
to remedy this , we introduce a sentiment treebank .
it includes ne grained sentiment labels for 123 , 123 phrases in the parse trees of 123 , 123 sentences and presents new challenges for sentiment composition - ality .
to address them , we introduce the recursive neural tensor network .
when trained on the new treebank , this model out - performs all previous methods on several met - it pushes the state of the art in single sentence positive / negative classication from 123% up to 123% .
the accuracy of predicting ne - grained sentiment labels for all phrases reaches 123% , an improvement of 123% over bag of features baselines .
lastly , it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases .
semantic vector spaces for single words have been widely used as features ( turney and pantel , 123 ) .
because they cannot capture the meaning of longer phrases properly , compositionality in semantic vec - tor spaces has recently received a lot of attention ( mitchell and lapata , 123; socher et al . , 123; zanzotto et al . , 123; yessenalina and cardie , 123; socher et al . , 123; grefenstette et al . , 123 ) .
how - ever , progress is held back by the current lack of large and labeled compositionality resources and
figure 123 : example of the recursive neural tensor net - work accurately predicting 123 sentiment classes , very neg - ative to very positive ( , , 123 , + , + + ) , at every node of a parse tree and capturing the negation and its scope in this
models to accurately capture the underlying phe - nomena presented in such data .
to address this need , we introduce the stanford sentiment treebank and a powerful recursive neural tensor network that can accurately predict the compositional semantic effects present in this new corpus .
the stanford sentiment treebank is the rst cor - pus with fully labeled parse trees that allows for a complete analysis of the compositional effects of sentiment in language .
the corpus is based on the dataset introduced by pang and lee ( 123 ) and consists of 123 , 123 single sentences extracted from it was parsed with the stanford parser ( klein and manning , 123 ) and includes a total of 123 , 123 unique phrases from those parse trees , each annotated by 123 human judges .
this new dataset allows us to analyze the intricacies of senti - ment and to capture complex linguistic phenomena .
123 shows one of the many examples with clear compositional structure .
the granularity and size of
123this123lm123does123nt123+care+123about+++++cleverness123 , 123wit123or+123any123other+kind+123of++intelligent++humor123 this dataset will enable the community to train com - positional models that are based on supervised and structured machine learning techniques .
while there are several datasets with document and chunk labels available , there is a need to better capture sentiment from short comments , such as twitter data , which provide less overall signal per document .
in order to capture the compositional effects with higher accuracy , we propose a new model called the recursive neural tensor network ( rntn ) .
recur - sive neural tensor networks take as input phrases of any length .
they represent a phrase through word vectors and a parse tree and then compute vectors for higher nodes in the tree using the same tensor - based composition function .
we compare to several super - vised , compositional models such as standard recur - sive neural networks ( rnn ) ( socher et al . , 123b ) , matrix - vector rnns ( socher et al . , 123 ) , and base - lines such as neural networks that ignore word order , naive bayes ( nb ) , bi - gram nb and svm .
all mod - els get a signicant boost when trained with the new dataset but the rntn obtains the highest perfor - mance with 123% accuracy when predicting ne - grained sentiment for all nodes .
lastly , we use a test set of positive and negative sentences and their re - spective negations to show that , unlike bag of words models , the rntn accurately captures the sentiment change and scope of negation .
rntns also learn that sentiment of phrases following the contrastive conjunction but dominates .
the complete training and testing code , a live demo and the stanford sentiment treebank dataset are available at http : / / nlp . stanford . edu /
123 related work
this work is connected to ve different areas of nlp research , each with their own large amount of related work to which we cannot do full justice given space semantic vector spaces .
the dominant ap - proach in semantic vector spaces uses distributional similarities of single words .
often , co - occurrence statistics of a word and its context are used to de - scribe each word ( turney and pantel , 123; baroni and lenci , 123 ) , such as tf - idf .
variants of this idea use more complex frequencies such as how often a
word appears in a certain syntactic context ( pado and lapata , 123; erk and pado , 123 ) .
however , distributional vectors often do not properly capture the differences in antonyms since those often have similar contexts .
one possibility to remedy this is to use neural word vectors ( bengio et al . , 123 ) .
these vectors can be trained in an unsupervised fashion to capture distributional similarities ( collobert and weston , 123; huang et al . , 123 ) but then also be ne - tuned and trained to specic tasks such as sen - timent detection ( socher et al . , 123b ) .
the models in this paper can use purely supervised word repre - sentations learned entirely on the new corpus .
compositionality in vector spaces .
the compositionality algorithms and related datasets capture two word compositions .
mitchell and la - pata ( 123 ) use e . g .
two - word phrases and analyze similarities computed by vector addition , multiplica - tion and others .
some related models such as holo - graphic reduced representations ( plate , 123 ) , quan - tum logic ( widdows , 123 ) , discrete - continuous models ( clark and pulman , 123 ) and the recent compositional matrix space model ( rudolph and giesbrecht , 123 ) have not been experimentally val - idated on larger corpora .
yessenalina and cardie ( 123 ) compute matrix representations for longer phrases and dene composition as matrix multipli - cation , and also evaluate on sentiment .
grefen - stette and sadrzadeh ( 123 ) analyze subject - verb - object triplets and nd a matrix - based categorical model to correlate well with human judgments .
we compare to the recent line of work on supervised in particular we will de - scribe and experimentally compare our new rntn model to recursive neural networks ( rnn ) ( socher et al . , 123b ) and matrix - vector rnns ( socher et al . , 123 ) both of which have been applied to bag of words sentiment corpora .
a related eld that tackles com - positionality from a very different angle is that of trying to map sentences to logical form ( zettlemoyer and collins , 123 ) .
while these models are highly interesting and work well in closed domains and on discrete sets , they could only capture sentiment distributions using separate mechanisms beyond the currently used logical forms .
deep learning .
apart from the above mentioned
work on rnns , several compositionality ideas re - lated to neural networks have been discussed by bot - tou ( 123 ) and hinton ( 123 ) and rst models such as recursive auto - associative memories been exper - imented with by pollack ( 123 ) .
the idea to relate inputs through three way interactions , parameterized by a tensor have been proposed for relation classi - cation ( sutskever et al . , 123; jenatton et al . , 123 ) , extending restricted boltzmann machines ( ranzato and hinton , 123 ) and as a special layer for speech recognition ( yu et al . , 123 ) .
apart from the above - mentioned work , most approaches in sentiment anal - ysis use bag of words representations ( pang and lee , 123 ) .
snyder and barzilay ( 123 ) analyzed larger reviews in more detail by analyzing the sentiment of multiple aspects of restaurants , such as food or atmosphere .
several works have explored sentiment compositionality through careful engineering of fea - tures or polarity shifting rules on syntactic structures ( polanyi and zaenen , 123; moilanen and pulman , 123; rentoumi et al . , 123; nakagawa et al . , 123 ) .
123 stanford sentiment treebank
bag of words classiers can work well in longer documents by relying on a few words with strong sentiment like awesome or exhilarating .
how - ever , sentiment accuracies even for binary posi - tive / negative classication for single sentences has not exceeded 123% for several years .
for the more difcult multiclass case including a neutral class , accuracy is often below 123% for short messages on twitter ( wang et al . , 123 ) .
from a linguistic or cognitive standpoint , ignoring word order in the treatment of a semantic task is not plausible , and , as we will show , it cannot accurately classify hard ex - amples of negation .
correctly predicting these hard cases is necessary to further improve performance .
in this section we will introduce and provide some analyses for the new sentiment treebank which in - cludes labels for every syntactically plausible phrase in thousands of sentences , allowing us to train and evaluate compositional models .
we consider the corpus of movie review excerpts from the rottentomatoes . com website orig - inally collected and published by pang and lee ( 123 ) .
the original dataset includes 123 , 123 sen -
figure 123 : the labeling interface .
random phrases were shown and annotators had a slider for selecting the senti - ment and its degree .
tences , half of which were considered positive and the other half negative .
each label is extracted from a longer movie review and reects the writers over - all intention for this review .
the normalized , lower - cased text is rst used to recover , from the origi - nal website , the text with capitalization .
remaining html tags and sentences that are not in english are deleted .
the stanford parser ( klein and man - ning , 123 ) is used to parses all 123 , 123 sentences .
in approximately 123 , 123 cases it splits the snippet into multiple sentences .
we then used amazon me - chanical turk to label the resulting 123 , 123 phrases .
123 shows the interface annotators saw .
the slider has 123 different values and is initially set to neutral .
the phrases in each hit are randomly sampled from the set of all phrases in order to prevent labels being inuenced by what follows .
for more details on the dataset collection , see supplementary material .
123 shows the normalized label distributions at each n - gram length .
starting at length 123 , the ma - jority are full sentences .
one of the ndings from labeling sentences based on readers perception is that many of them could be considered neutral .
we also notice that stronger sentiment often builds up in longer phrases and the majority of the shorter phrases are neutral .
another observation is that most annotators moved the slider to one of the ve po - sitions : negative , somewhat negative , neutral , posi - tive or somewhat positive .
the extreme values were rarely used and the slider was not often left in be - tween the ticks .
hence , even a 123 - class classication into these categories captures the main variability of the labels .
we will name this ne - grained senti - ment classication and our main experiment will be to recover these ve labels for phrases of all lengths .
nerdy folks|verynegative|negative|somewhatnegative|neutral|somewhatpositive|positive|verypositivephenomenal fantasy best sellers|verynegative|negative|somewhatnegative|neutral|somewhatpositive|positive|verypositive figure 123 : normalized histogram of sentiment annotations at each n - gram length .
many shorter n - grams are neutral; longer phrases are well distributed .
few annotators used slider positions between ticks or the extreme values .
hence the two strongest labels and intermediate tick positions are merged into 123 classes .
123 recursive neural models
the models in this section compute compositional vector representations for phrases of variable length and syntactic type .
these representations will then be used as features to classify each phrase .
123 displays this approach .
when an n - gram is given to the compositional models , it is parsed into a binary tree and each leaf node , corresponding to a word , is represented as a vector .
recursive neural mod - els will then compute parent vectors in a bottom up fashion using different types of compositional - ity functions g .
the parent vectors are again given as features to a classier .
for ease of exposition , we will use the tri - gram in this gure to explain all
we rst describe the operations that the below re - cursive neural models have in common : word vector representations and classication .
this is followed by descriptions of two previous rnn models and
each word is represented as a d - dimensional vec - tor .
we initialize all word vectors by randomly sampling each value from a uniform distribution : u ( r , r ) , where r = 123 .
all the word vec - tors are stacked in the word embedding matrix l rd|v | , where |v | is the size of the vocabulary .
ini - tially the word vectors will be random but the l ma - trix is seen as a parameter that is trained jointly with the compositionality models .
we can use the word vectors immediately as parameters to optimize and as feature inputs to a softmax classier .
for classication into ve classes , we compute the posterior probability over
figure 123 : approach of recursive neural network mod - els for sentiment : compute parent vectors in a bottom up fashion using a compositionality function g and use node vectors as features for a classier at that node .
this func - tion varies for the different models .
labels given the word vector via :
ya = softmax ( wsa ) ,
where ws r123d is the sentiment classication matrix .
for the given tri - gram , this is repeated for vectors b and c .
the main task of and difference between the models will be to compute the hidden vectors pi rd in a bottom up fashion .
123 rnn : recursive neural network the simplest member of this family of neural net - work models is the standard recursive neural net - work ( goller and kuchler , 123; socher et al . , 123a ) .
first , it is determined which parent already has all its children computed .
in the above tree ex - ample , p123 has its two childrens vectors since both are words .
rnns use the following equations to compute the parent vectors :
123n - gram length123%123%123%123%123%123%% of sentiment valuesneutralsomewhat positivepositivevery positivesomewhat negativenegativevery negative ( a ) ( a ) ( b ) ( b ) ( c ) ( c ) ( d ) ( d ) distributions of sentiment values for ( a ) unigrams , ( b ) 123 - grams , ( c ) 123 - grams , and ( d ) full sentences .
not very good . . .
a b c p123 =g ( b , c ) p123 = g ( a , p123 ) 123+++ - ( cid : 123 )
p123 = f
, p123 = f
where f = tanh is a standard element - wise nonlin - earity , w rd123d is the main parameter to learn and we omit the bias for simplicity .
the bias can be added as an extra column to w if an additional 123 is added to the concatenation of the input vectors .
the parent vectors must be of the same dimensionality to be recursively compatible and be used as input to the next composition .
each parent vector pi , is given to the same softmax classier of eq .
123 to compute its
this model uses the same compositionality func - tion as the recursive autoencoder ( socher et al . , 123b ) and recursive auto - associate memories ( pol - lack , 123 ) .
the only difference to the former model is that we x the tree structures and ignore the re - construction loss .
in initial experiments , we found that with the additional amount of training data , the reconstruction loss at each node is not necessary to obtain high performance .
123 mv - rnn : matrix - vector rnn
the mv - rnn is linguistically motivated in that most of the parameters are associated with words and each composition function that computes vec - tors for longer phrases depends on the actual words being combined .
the main idea of the mv - rnn ( socher et al . , 123 ) is to represent every word and longer phrase in a parse tree as both a vector and a matrix .
when two constituents are combined the matrix of one is multiplied with the vector of the other and vice versa .
hence , the compositional func - tion is parameterized by the words that participate in
each words matrix is initialized as a dd identity matrix , plus a small amount of gaussian noise .
sim - ilar to the random word vectors , the parameters of these matrices will be trained to minimize the clas - sication error at each node .
for this model , each n - gram is represented as a list of ( vector , matrix ) pairs , together with the parse tree .
for the tree with ( vec -
the mv - rnn computes the rst parent vector and its matrix via two equations :
p123 = f
, p123 = f
where wm rd123d and the result is again a d d matrix .
similarly , the second parent node is com - puted using the previously computed ( vector , matrix ) pair ( p123 , p123 ) as well as ( a , a ) .
the vectors are used for classifying each phrase using the same softmax classier as in eq
123 rntn : recursive neural tensor network one problem with the mv - rnn is that the number of parameters becomes very large and depends on the size of the vocabulary .
it would be cognitively more plausible if there was a single powerful com - position function with a xed number of parameters .
the standard rnn is a good candidate for such a function .
however , in the standard rnn , the input vectors only implicitly interact through the nonlin - earity ( squashing ) function .
a more direct , possibly multiplicative , interaction would allow the model to have greater interactions between the input vectors .
motivated by these ideas we ask the question : can a single , more powerful composition function per - form better and compose aggregate meaning from smaller constituents more accurately than many in - put specic ones ? in order to answer this question , we propose a new model called the recursive neu - ral tensor network ( rntn ) .
the main idea is to use the same , tensor - based composition function for all fig .
123 shows a single tensor layer .
we dene the output of a tensor product h rd via the follow - ing vectorized notation and the equivalent but more detailed notation for each slice v ( i ) rdd :
; hi =
where v ( 123 : d ) r123d123dd is the tensor that denes multiple bilinear forms .
softmax classier trained on its vector representa - tion to predict a given ground truth or target vector t .
we assume the target distribution vector at each node has a 123 - 123 encoding .
if there are c classes , then it has length c and a 123 at the correct label .
all other entries are 123
we want to maximize the probability of the cor - rect prediction , or minimize the cross - entropy error between the predicted distribution yi rc123 at node i and the target distribution ti rc123 at that node .
this is equivalent ( up to a constant ) to mini - mizing the kl - divergence between the two distribu - tions .
the error as a function of the rntn parame - ters = ( v , w , ws , l ) for a sentence is : j + ( cid : 123 ) ( cid : 123 ) 123
j log yi
the derivative for the weights of the softmax clas - sier are standard and simply sum up from each nodes error .
we dene xi to be the vector at node i ( in the example trigram , the xi rd123s are ( a , b , c , p123 , p123 ) ) .
we skip the standard derivative for ws .
each node backpropagates its error through to the recursively used weights v , w .
let i , s rd123 be the softmax error vector at node i :
i , s = ( cid : 123 ) w t
s ( yi ti ) ( cid : 123 ) f ( cid : 123 ) ( xi ) ,
where is the hadamard product between the two vectors and f ( cid : 123 ) is the element - wise derivative of f which in the standard case of using f = tanh can be computed using only f ( xi ) .
the remaining derivatives can only be computed in a top - down fashion from the top node through the tree and into the leaf nodes .
the full derivative for v and w is the sum of the derivatives at each of the nodes .
we dene the complete incoming error messages for a node i as i , com .
the top node , in our case p123 , only received errors from the top nodes softmax .
hence , p123 , com = p123 , s which we can use to obtain the standard backprop derivative for w ( goller and kuchler , 123; socher et al . , 123 ) .
for the derivative of each slice k = 123 , .
, d , we get :
is just the kth element of this vector .
now , we can compute the error message for the two
figure 123 : a single layer of the recursive neural ten - sor network .
each dashed box represents one of d - many slices and can capture a type of inuence a child can have on its parent .
the rntn uses this denition for computing p123 :
p123 = f
where w is as dened in the previous models .
the next parent vector p123 in the tri - gram will be com - puted with the same weights :
p123 = f
the main advantage over the previous rnn model , which is a special case of the rntn when v is set to 123 , is that the tensor can directly relate in - put vectors .
intuitively , we can interpret each slice of the tensor as capturing a specic type of compo -
an alternative to rntns would be to make the compositional function more powerful by adding a second neural network layer .
however , initial exper - iments showed that it is hard to optimize this model and vector interactions are still more implicit than in
123 tensor backprop through structure we describe in this section how to train the rntn model .
as mentioned above , each node has a
slices of standard tensor layer layerp = f v ( 123 : 123 ) + wneural tensor layerbcbcbctp = f + children of p123 :
w t p123 , com + s
v ( k ) ( cid : 123 ) t ( cid : 123 ) ( cid : 123 ) a
v ( k ) +
where we dene
the children of p123 , will then each take half of this vector and add their own softmax error message for the complete .
in particular , we have
p123 , com = p123 , s + p123 , down ( d + 123 : 123d ) ,
where p123 , down ( d + 123 : 123d ) indicates that p123 is the right child of p123 and hence takes the 123nd half of the error , for the nal word vector derivative for a , it will be p123 , down ( 123 : d ) .
the full derivative for slice v ( k ) for this trigram
tree then is the sum at each node :
and similarly for w .
for this nonconvex optimiza - tion we use adagrad ( duchi et al . , 123 ) which con - verges in less than 123 hours to a local optimum .
we include two types of analyses .
the rst type in - cludes several large quantitative evaluations on the test set .
the second type focuses on two linguistic phenomena that are important in sentiment .
for all models , we use the dev set and cross - validate over regularization of the weights , word vector size as well as learning rate and minibatch size for adagrad .
optimal performance for all mod - els was achieved at word vector sizes between 123 and 123 dimensions and batch sizes between 123 and 123
performance decreased at larger or smaller vec - tor and batch sizes .
this indicates that the rntn does not outperform the standard rnn due to sim - ply having more parameters .
the mv - rnn has or - ders of magnitudes more parameters than any other model due to the word matrices .
the rntn would usually achieve its best performance on the dev set after training for 123 - 123 hours .
table 123 : accuracy for ne grained ( 123 - class ) and binary predictions at the sentence level ( root ) and for all nodes .
showed that the recursive models worked signi - cantly worse ( over 123% drop in accuracy ) when no nonlinearity was used .
we use f = tanh in all ex -
we compare to commonly used methods that use bag of words features with naive bayes and svms , as well as naive bayes with bag of bigram features .
we abbreviate these with nb , svm and binb .
we also compare to a model that averages neural word vectors and ignores word order ( vecavg ) .
the sentences in the treebank were split into a train ( 123 ) , dev ( 123 ) and test splits ( 123 ) and these splits are made available with the data release .
we also analyze performance on only positive and negative sentences , ignoring the neutral class .
this lters about 123% of the data with the three sets hav - ing 123 / 123 / 123 sentences .
123 fine - grained sentiment for all phrases the main novel experiment and evaluation metric analyze the accuracy of ne - grained sentiment clas - sication for all phrases .
123 showed that a ne grained classication into 123 classes is a reasonable approximation to capture most of the data variation .
123 shows the result on this new corpus .
the rntn gets the highest performance , followed by the mv - rnn and rnn .
the recursive models work very well on shorter phrases , where negation and composition are important , while bag of features baselines perform well only with longer sentences .
the rntn accuracy upper bounds other models at most n - gram lengths .
table 123 ( left ) shows the overall accuracy numbers for ne grained prediction at all phrase lengths and
figure 123 : accuracy curves for ne grained sentiment classication at each n - gram lengths .
left : accuracy separately for each set of n - grams .
right : cumulative accuracy of all n - grams .
123 full sentence binary sentiment this setup is comparable to previous work on the original rotten tomatoes dataset which only used full sentence labels and binary classication of pos - itive / negative .
hence , these experiments show the improvement even baseline methods can achieve with the sentiment treebank .
table 123 shows results of this binary classication for both all phrases and for only full sentences .
the previous state of the art was below 123% ( socher et al . , 123 ) .
with the coarse bag of words annotation for training , many of the more complex phenomena could not be captured , even by more powerful models .
the combination of the new sentiment treebank and the rntn pushes the state of the art on short phrases up to 123% .
123 model analysis : contrastive conjunction in this section , we use a subset of the test set which includes only sentences with an x but y structure : a phrase x being followed by but which is followed by a phrase y .
the conjunction is interpreted as an argument for the second conjunct , with the rst functioning concessively ( lakoff , 123; blakemore , 123; merin , 123 ) .
123 contains an example .
we analyze a strict setting , where x and y are phrases of different sentiment ( including neutral ) .
the ex - ample is counted as correct , if the classications for both phrases x and y are correct .
furthermore , the lowest node that dominates both of the word but and the node that spans y also have to have the same correct sentiment .
for the resulting 123 cases , the rntn obtains an accuracy of 123% compared to mv - rnn ( 123 ) , rnn ( 123 ) and binb ( 123 ) .
123 model analysis : high level negation we investigate two types of negation .
for each type , we use a separate dataset for evaluation .
figure 123 : example of correct prediction for contrastive conjunction x but y .
set 123 : negating positive sentences .
the rst set contains positive sentences and their negation .
this set , the negation changes the overall sentiment of a sentence from positive to negative .
hence , we compute accuracy in terms of correct sentiment re - versal from positive to negative .
123 shows two examples of positive negation the rntn correctly classied , even if negation is less obvious in the case of least .
table 123 ( left ) gives the accuracies over 123 positive sentences and their negation for all models .
the rntn has the highest reversal accuracy , show - ing its ability to structurally learn negation of posi - tive sentences .
but what if the model simply makes phrases very negative when negation is in the sen - tence ? the next experiments show that the model captures more than such a simplistic negation rule .
set 123 : negating negative sentences .
ond set contains negative sentences and their nega - tion .
when negative sentences are negated , the sen - timent treebank shows that overall sentiment should become less negative , but not necessarily positive .
for instance , the movie was terrible is negative but the the movie was not terrible says only that it was less bad than a terrible one , not that it was good ( horn , 123; israel , 123 ) .
hence , we evaluate ac -
123n - gramlength123 . 123 . 123accuracy123n - gramlength123 . 123 . 123cumulativeaccuracymodelrntnmv - rnnrnnbinbnb++123there123are123slow123andrepetitive123parts123 , 123but+123it+123has123just123enough++spice+123to+123keep+123it+interesting123 figure 123 : rntn prediction of positive and negative ( bottom right ) sentences and their negation .
negated positive negated negative
table 123 : accuracy of negation detection .
negated posi - tive is measured as correct sentiment inversions .
negated negative is measured as increases in positive activations .
curacy in terms of how often each model was able to increase non - negative activation in the sentiment of the sentence .
table 123 ( right ) shows the accuracy .
in over 123% of cases , the rntn correctly increases the positive activations .
123 ( bottom right ) shows a typical case in which sentiment was made more positive by switching the main class from negative to neutral even though both not and dull were nega - tive .
123 shows the changes in activation for both sets .
negative values indicate a decrease in aver -
figure 123 : change in activations for negations .
only the rntn correctly captures both types .
it decreases positive sentiment more when it is negated and learns that negat - ing negative phrases ( such as not terrible ) should increase neutral and positive activations .
age positive activation ( for set 123 ) and positive values mean an increase in average positive activation ( set 123 ) .
the rntn has the largest shifts in the correct di - rections .
therefore we can conclude that the rntn is best able to identify the effect of negations upon both positive and negative sentiment sentences .
most positive n - grams engaging; best; powerful; love; beautiful excellent performances; a masterpiece; masterful lm; wonderful movie; marvelous performances an amazing performance; wonderful all - ages tri - umph; a wonderful movie; most visually stunning nicely acted and beautifully shot; gorgeous im - agery , effective performances; the best of the year; a terric american sports movie; refresh - ingly honest and ultimately touching one of the best lms of the year; a love for lms shines through each frame; created a masterful piece of artistry right here; a masterful lm from a master lmmaker ,
most negative n - grams bad; dull; boring; fails; worst; stupid; painfully worst movie; very bad; shapeless mess; worst thing; instantly forgettable; complete failure for worst movie; a lousy movie; a complete fail - ure; most painfully marginal; very bad sign silliest and most incoherent movie; completely crass and forgettable movie; just another bad movie .
a cumbersome and cliche - ridden movie; a humorless , disjointed mess a trashy , exploitative , thoroughly unpleasant ex - perience ; this sloppy drama is an empty ves - sel . ; quickly drags on becoming boring and pre - dictable . ; be the worst special - effects creation of
table 123 : examples of n - grams for which the rntn predicted the most positive and most negative responses .
we introduced recursive neural tensor networks and the stanford sentiment treebank .
the combi - nation of new model and data results in a system for single sentence sentiment detection that pushes state of the art by 123% for positive / negative sen - tence classication .
apart from this standard set - ting , the dataset also poses important new challenges and allows for new evaluation metrics .
for instance , the rntn obtains 123% accuracy on ne - grained sentiment prediction across all phrases and captures negation of different sentiments and scope more ac - curately than previous models .
we thank rukmani ravisundaram and tayyab tariq for the rst version of the online demo .
richard is partly supported by a microsoft re - search phd fellowship .
the authors gratefully ac - knowledge the support of the defense advanced re - search projects agency ( darpa ) deep exploration and filtering of text ( deft ) program under air force research laboratory ( afrl ) prime contract no .
fa123 - 123 - 123 - 123 , the darpa deep learning program under contract number fa123 - 123 - c - 123 and nsf iis - 123
any opinions , ndings , and conclusions or recommendations expressed in this material are those of the authors and do not neces - sarily reect the view of darpa , afrl , or the us
figure 123 : average ground truth sentiment of top 123 most positive n - grams at various n .
the rntn correctly picks the more negative and positive examples .
123 model analysis : most positive and
we queried the model for its predictions on what the most positive or negative n - grams are , measured as the highest activation of the most negative and most positive classes .
table 123 shows some phrases from the dev set which the rntn selected for their
due to lack of space we cannot compare top phrases of the other models but fig .
123 shows that the rntn selects more strongly positive phrases at most n - gram lengths compared to other models .
for this and the previous experiment , please nd additional examples and descriptions in the supple -
