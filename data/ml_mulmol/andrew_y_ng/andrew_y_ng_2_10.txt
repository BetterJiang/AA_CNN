we present a new algorithm for learning hypernym ( is - a ) relations from text , a key problem in machine learning for natural language under - standing .
this method generalizes earlier work that relied on hand - built lexico - syntactic patterns by introducing a general - purpose formalization of the pattern space based on syntactic dependency paths .
we learn these paths automatically by taking hypernym / hyponym word pairs from wordnet , nding sentences containing these words in a large parsed cor - pus , and automatically extracting these paths .
these paths are then used as features in a high - dimensional representation of noun relationships .
we use a logistic regression classier based on these features for the task of corpus - based hypernym pair identication .
our classier is shown to outperform previous pattern - based methods for identifying hypernym pairs ( using wordnet as a gold standard ) , and is shown to outperform those methods as well as wordnet on an independent test set .
semantic taxonomies and thesauri like wordnet ( 123 , 123 ) are a key source of knowledge for natural language processing applications , giving structured information about semantic relations between words .
building such taxonomies , however , is an extremely slow and knowledge - intensive process , and furthermore any particular semantic taxonomy is bound to be limited in its scope and domain .
thus a wide variety of recent research has focused on nding methods for automatically learning taxonomic relations and constructing semantic hierarchies ( 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 ) .
in this paper we focus on building an automatic classier for the hypernym / hyponym relation .
a word x is a hyponym of word y if x is a subtype or instance of y .
thus shake - speare is a hyponym of author , ( and conversely author is a hypernym of shake - speare ) dog is a hyponym of canine , table is a hyponym of furniture , and so on .
much of the previous research on automatic semantic classication of words has focused on a key insight rst articulated by hearst in ( 123 ) , that the presence of certain lexico - syntactic patterns can indicate a particular semantic relationship between two nouns .
hearst no - ticed , for example , that linking two noun phrases ( nps ) via the constructions such n py as n px , or n px and other n py , often implies the relation hyponym ( n px , n py ) , that n px is a kind of n py .
since then , a broad swath of researchers has used a small number ( typically less than 123 ) of hand - created patterns like those of hearst to au - tomatically label such semantic relations ( 123 , 123 , 123 , 123 , 123 ) .
while these patterns have been
figure 123 : minipar dependency tree example with transform
successful at identifying some examples of relationships like hypernymy , this method of lexicon construction is tedious and subject to the bias of the designer; further , such pattern lexicons contain only a small subset of the actual patterns found to occur in natural text .
our goal is to use a machine learning paradigm to automatically replace this hand - built in our new approach to the hypernym - labeling task , based on extending a suggestion from ( 123 ) , patterns indicative of hypernymy are learned automatically under in - direct or distant supervision from a thesaurus , as follows :
( a ) extract examples of all hypernym pairs ( pairs of words in a hyper -
nym / hyponym relation ) from wordnet .
( b ) for each hypernym pair , nd sentences in which both words occur .
( c ) parse the sentences , and automatically extract patterns from the parse tree
which are good cues for hypernymy .
( d ) train a hypernym classier based on these features .
( a ) given a pair of words in the test set , extract features and use the classier to
decide if the word - pair is in the hypernym / hyponym relation or not .
the next section introduces our method for automatically discovering patterns indicative of hypernymy .
section 123 then describes the setup of our experiments .
in section 123 we analyze our feature space , and in section 123 we describe a combined classier based on these features which achieves high accuracy at the task of hypernym identication .
section 123 shows how this classier can be improved by adding a new source of knowledge , coordinate terms .
123 representing lexico - syntactic patterns with dependency paths the rst goal of our work is to automatically identify lexico - syntactic patterns indicative of hypernymy .
in order to do this , we need a representation space for expressing these pat - terns .
we propose the use of dependency paths as a general - purpose formalization of the space of lexico - syntactic patterns , based on the broad - coverage dependency parser mini - par ( 123 ) .
dependency paths have been used successfully in the past to represent lexico - syntactic relations suitable for semantic processing ( 123 ) .
a dependency parser produces a dependency tree that represents the syntactic relations be - tween words by a list of edge tuples of the form : ( word123 , category123 : relation : category123 , word123 ) .
here each word is the stemmed form of the word or multi - word phrase ( so that authors becomes author ) , and corre - sponds to a specic node in the dependency tree; each category is the part of speech label of the corresponding word ( e . g .
n for noun or prep for preposition ) ; and the relation is the directed syntactic relationship exhibited from word123 to word123 ( e . g .
obj for object , mod for modier , or conj for conjunct ) , and corresponds to a specic link in the tree .
we may then dene our space of lexico - syntactic patterns to be all shortest paths of four links or less between any two nouns in a dependency tree .
figure 123 shows the partial dependency tree for the sentence fragment . . . suchauthorsasherrickandshakespeare .
we then remove the original words in the noun pair to create a more general pattern .
each dependency path may then be presented as an ordered list of dependency tuples .
we extend
. . . authorssuch - n : pre : predetas - n : mod : prepherrick - prep : pcomp - n : nshakespeare - prep : pcomp - n : nand - n : punc : u - n : conj : n n px and other n py : n px or other n py : n py such as n px : such n py as n px : n py including n px : n py , especially n px :
n : pcomp - n : prep , such as , such as , prep : mod : n n : obj : v , include , include , v : i : c , dummy node , dummy node , c : rel : n
table 123 : dependency path representations of hearsts patterns
this basic minipar representation in two ways : rst , we wish to capture the fact that cer - tain function words like such ( in such np as np ) or other ( in np and other nps ) are important parts of lexico - syntactic patterns .
we implement this by adding optional satel - lite links to each shortest path , i . e .
single links not already contained in the dependency path added on either side of each noun .
second , we capitalize on the distributive nature of the syntactic conjunct relation ( e . g .
and , or , and comma - separated noun lists ) by dis - tributing dependency links across such conjuncts .
as an example , in the simple 123 - member conjunct chain of herrick and shakespeare in figure 123 , we add the entrance link as , - prep : pcomp - n : n to the single element shakespeare ( as a dotted line in the gure ) .
our extended dependency notation is able to capture the power of the hand - engineered pat - terns described in the literature .
table 123 shows the six patterns used in ( 123 , 123 , 123 ) and their corresponding dependency path formalizations .
123 experimental paradigm our goal is to build a classier which is given an ordered pair of words and makes a binary decision as to whether the nouns are related by hypernymy or not .
all of our experiments are based on a corpus of over 123 million newswire sentences . 123 we rst parsed each of the sentences in the corpus using minipar .
we extract every pair of nouns from each sentence .
123 , 123 of the resulting unique noun pairs were labeled as known hypernym or known non - hypernym using wordnet123
a noun pair ( n123 , n123 ) is labeled known hypernym if n123 is an ancestor of the rst sense of n123 in the wordnet hypernym taxonomy , and if the only frequently - used 123 sense of each word is the rst noun sense listed in wordnet .
note that n123 is considered a hypernym of n123 regardless of how much higher in the hierarchy it is with respect to n123
a noun pair may be assigned to the second set of known non - hypernym pairs if both nouns are contained within wordnet , but neither word is an ancestor of the other in the wordnet hypernym taxonomy for any senses of either word .
of our collected noun pairs , 123 , 123 were known hypernym pairs , and we assign the 123 , 123 most fre - quently occurring known non - hypernym pairs to the second set; this number is selected to preserve the roughly 123 : 123 ratio of hypernym - to - non - hypernym pairs observed in our hand - labeled test set ( discussed below ) .
we evaluated our binary classiers in two ways .
for both sets of evaluations , our classier was given a pair of words from an unseen sentence and had to make a hypernym vs .
non - hypernym decision .
in the rst style of evaluation , we compared the performance of our classiers against the known hypernym versus known non - hypernym labels assigned by wordnet .
this provides a metric for how well our classiers do at recreating wordnet .
for the second set of evaluations we hand - labeled a test set of 123 , 123 noun pairs from randomly - selected paragraphs within our corpus ( with part - of - speech labels assigned by minipar ) .
the annotators are instructed to label each ordered noun pair as one of
123the corpus contains articles from the associated press , wall street journal , and los angeles
times , drawn from the tipster 123 , 123 , 123 , and trec 123 corpora ( 123 ) .
123we access wordnet 123 via jason rennies wordnet : : querydata interface .
123a noun sense is determined to be frequently - used if it occurs at least once in the sense - tagged brown corpus semantic concordance les ( as reported in the cntlist le distributed as part of wordnet 123 ) .
this determination is made so as to reduce the number of false hypernym / hyponym classications due to highly polysemous words .
figure 123 : hypernym pre / re for all features
figure 123 : hypernym classiers
hyponym - to - hypernym , hypernym - to - hyponym , coordinate , or unrelated ( the co - ordinate relation will be dened below ) .
as expected , the vast majority of pairs ( 123 , 123 ) were found to be unrelated by these measures; the rest were split evenly between hyper - nym and coordinate pairs ( 123 and 123 , resp . ) .
interannotator agreement was obtained between four labelers ( all native speakers of en - glish ) on a held - out set of 123 noun pairs , and determined for each task according to the averaged f - score across all pairs of the four labelers .
agreement was 123% and 123% for the hypernym and coordinate term classication tasks , respectively .
123 features : pattern discovery our rst study focused on discovering which dependency paths ( lexico - syntactic patterns ) might prove useful features for our classiers .
to evaluate these features , we construct a binary classier for each pattern , which simply classies a noun pair as hypernym / hyponym if and only if the specic pattern occurs at least once for that noun pair .
figure 123 depicts the precision and recall of all such classiers ( with recall at least . 123 ) on the wordnet - labeled data set123
using this formalism we have been able to capture a wide variety of repeatable patterns between hypernym / hyponym noun pairs; in particular , we have been able to rediscover the hand - designed patterns originally proposed in ( 123 ) ( the rst ve features , marked in red123 ) , in addition to a number of new patterns not previously discussed ( of which four are marked as blue triangles in figure 123 and listed in table 123
this analysis gives a quantitative justication to hearsts initial intuition as to the power of hand - selected patterns; nearly all of hearsts patterns are at the high - performance boundary of precision and recall for individual features .
n py like n px : n py called n px : n px is a n py : n px , a n py ( appositive ) : n : appo : n
table 123 : dependency path representations of other high - scoring patterns
123 a hypernym - only classier our rst hypernym classier is based on the intuition that unseen noun pairs are likely to be in a hypernymy relation if they occur in the test set in one or more lexico - syntactic patterns indicative of hypernymy .
123redundant features consisting of an identical base path to an identied pattern but differing only
by an additional satellite link are marked in figure 123 by smaller versions of the same symbol .
123we mark the single generalized conjunct other pattern - n : conj : n , ( other , a : mod : n ) to rep -
resent both of hearsts original and other and or other patterns
123= - 123 - 123 . 123 . 123 . 123 . 123x and / or other yy such as xsuch y as xy including x y , especially xy like xy called xx is yx , a y ( appositive ) - 123individual feature analysisrecall ( log ) precision123 . 123 . 123 . 123 . 123 . 123 . 123 . 123 . 123 . 123logistic regression ( buckets ) logistic regression ( binary ) hearst ' s patternsconjunct - other patternrecallprecisionhypernym classifiers on wordnet - labeled dev set best logistic regression ( buckets ) : best logistic regression ( binary ) : best multinomial naive bayes : best complement naive bayes :
table 123 : average maximum f - score for cross validation on wordnet - labeled training set
from the 123 million word corpus , we created a feature lexicon which contained each de - pendency path that occurred between at least ve unique noun pairs in our corpus .
this results in a feature lexicon of approximately 123 , 123 dependency paths .
next , we record in our noun pair lexicon each noun pair that occurs within our corpus with at least ve unique paths from this lexicon .
we then create a feature count vector for each noun pair .
each dimension of the 123 , 123 - dimension vector represents a particular dependency path , and contains the total number of times in our corpus that that path was the shortest path connecting that noun pair in some dependency tree .
we thus dene as our task the binary classication of noun pair hypernymy or non - hypernymy based on its feature vector of dependency paths .
we use the wordnet - labeled known hypernym / known not - hypernym training set de - ned in the previous section .
we train a variety of classiers on this data set , including multinomial naive bayes , complement naive bayes ( 123 ) , and logistic regression .
we per - form model selection using 123 - fold cross validation on this training set , evaluating each model based on its maximum hypernym f - score averaged across all folds .
the summary of average maximum f - scores is presented in table 123 , and the precision / recall plot of our best models is presented in figure 123
for comparison , we evaluate two simple classiers based on past work with a handful of hand - engineered features; the rst simply detects the presence of at least one of hearsts pattern , arguably the previous best classier consisting only of lexico - syntactic patterns , and as implemented for hypernym discovery in ( 123 ) .
the second classier consists of only the np and / or other np subset of hearsts patterns , as used in the automatic construction of noun - labeled hypernym taxonomies in ( 123 ) .
in our tests we found greatest performance from a binary logistic regression model with 123 redundant threshold buckets spaced at the exponentially increasing intervals ( 123 , 123 , 123 , . . . 123 , 123 ) ; our resulting feature space consists of 123 , 123 distinct binary features .
these buckets are dened such that a feature corresponding to pattern p at threshold t will be activated by a noun pair n if and only if p has been observed to occur as a shortest dependency path between n at least t times .
our classier shows a dramatic improvement over previous classiers; in particular , using our best logistic regression classier , we observe a 123% relative improvement of average maximum f - score over the classier based on hearsts patterns .
123 using coordinate terms to improve hypernym classication while our hypernym - only classier performed better than previous classiers based on hand - built patterns , there is still much room for improvement .
as ( 123 ) point out , one prob - lem with pattern - based hypernym classiers in general is that within - sentence hypernym pattern information is quite sparse .
patterns are useful only to classify noun pairs which happen to occur in the same sentence; many hypernym / hyponym pairs may simply not oc - cur in the same sentence in the corpus .
for this reason ( 123 ) , following ( 123 ) suggest relying on a second source of knowledge : coordinate relations between words .
the coordinate term relation is dened in the wordnet glossary as : y is a coordinate term of x if x and y share a hypernym .
the coordinate relation is a symmetric relation between words that are the same kind of thing , i . e .
that share at least one common ancestor in the hy - pernym taxonomy .
many methods exist for inferring that two words are coordinate term ( a common subtask in automatic thesaurus induction ) .
thus we expect that using coordi - nate information might increase the recall of our hypernym classier : if we are condent
distributional similarity vector space model for : thresholded conjunct classier : best wordnet f - score :
table 123 : summary of maximum f - scores on hand - labeled coordinate pairs
figure 123 : coordinate classiers on hand - labeled test set
figure 123 : hypernym classiers on hand - labeled test set
that two entities ei , ej are coordinate terms , and that ej is a hyponym of ek , we may then infer with higher probability that ei is similarly a hyponym of ek despite never having encountered the pair ( ei , ek ) within a single sentence .
123 coordinate term classication prior work for classifying the coordinate relation include automatic word sense clustering methods based on distributional similarity ( e . g .
( 123 , 123 ) ) or on pattern - based techniques , specically using the coordination pattern x , y , and z ( e . g .
we construct both types of classier .
first we construct a vector - space model similar to ( 123 ) using single minipar dependency links as our distributional features .
using the same 123 million minipar - parsed sentences used in our hypernym training set , we rst construct a feature lexicon of the 123 , 123 most frequent single dependency edges summed across all edges connected to any noun in our corpus; we then construct feature count vectors for each of the most frequently occurring 123 , 123 individual nouns .
we normalize these feature counts with pointwise mutual information , and compute as our measure of similarity the cosine coefcient be - tween these normalized vectors .
we evaluate this classier on our hand - labeled test set , where of 123 , 123 total pairs , 123 are labeled as coordinate .
for purposes of comparison we construct a series of classiers from wordnet , which makes the simple binary decision of determining whether two words are coordinate according to whether they share a common ancestor within n words higher up in the hypernym taxonomy , for all n from 123 to 123
also , we compare a simple pattern - based classier based on the conjunct pattern ( e . g .
x and y ) , which thresholds simply on the number of conjunct patterns found between a given pair .
results of this experiment are shown in table 123 and figure 123
the strong performance of the simple conjunct pattern model suggests that it may be worth pursuing an extended pattern - based coordinate classier along the lines of our hypernym classier; for now , we proceed with our simple distributional similarity vector space model ( with a 123% relative f - score improvement over the conjunct model ) in the construction of a combined hypernym - coordinate hybrid classier .
123 hybrid hypernym - coordinate classication finally we would like to combine our hypernym and coordinate models in order to improve hypernym classication .
thus we dene two probabilities of pair relationships between
123 . 123 . 123 . 123 . 123 . 123 . 123 . 123 . 123 . 123interannotator agreementdistributional similarityconjunct patternwordnetcoordinate term classifiers on hand - labeled test setrecallprecision123 . 123 . 123 . 123 . 123 . 123 . 123 . 123 . 123 . 123interannotator agreementhybrid hypernym / coordinate modelhypernym only classifierwordnethearsts patternsconjunct other patterncombined wordnet / hybrid classifierhypernym classifiers on hand - labeled test setrecallprecision interannotator agreement : combined wordnet / hypernym / coordinate model : combined linear interpolation hypernym / coordinate model : best hypernym - only classier ( logistic regression ) : best wordnet f - score : hearst pattern classier : and / or other pattern classier :
table 123 : final evaluation of hypernym classication on hand - labeled test set
ej ) and p ( ei
ej ) , representing the probabilities that entity ei has ej entities : p ( ei < as an ancestor in its hypernym hierarchy , and that entities ei and ej are coordinate terms , that they share a common hypernym ancestor at some level , respectively .
dening the probability produced by our best hypernym - only classier as pold ( ei < ek ) , and a probability score obtained by normalizing the similarity score from our coordinate classier as p ( ei ej ) , we apply a simple linear interpolation scheme to compute a new hypernymy probability; specically , for each pair of entities ( ei , ek ) , we recompute the probability that ek is a hypernym of ei as :
ek ) = 123pold ( ei <
ek ) + 123
ej ) p ( ej <
we constrain our parameters 123 , 123 such that 123 + 123 = 123 , and then set these parameters using 123 - fold cross - validation on our hand - labeled test set .
for our nal evaluation we use 123 = 123 .
our hand - labeled dataset allows us to compare the performance of our classier directly against wordnet itself .
figure 123 contains a plot of precision / recall vs .
wordnet , as well as the methods in the previous comparison , now using the human labels as ground truth .
we compared multiple classiers based on the wordnet hypernym taxonomy , using a vari - ety of parameters including maximum number of senses of a hyponym to nd hypernyms for , maximum distance between the hyponym and its hypernym in the wordnet taxonomy , and whether or not to allow synonyms .
the best wordnet - based results are plotted in fig - ure 123; the model achieving the maximum f - score uses only the rst sense of a hyponym , allows a maximum distance of 123 between a hyponym and hypernym , and allows any mem - ber of a hypernym synset to be a hypernym .
our logistic regression hypernym - only model has a 123% relative maximum f - score improvement over the best wordnet classier , while the combined hypernym / coordinate model has a 123% relative maximum f - score improve - ment , and a combined wordnet / hybrid model ( a simple and of the two classiers ) has a in table 123 we analyze the disagreements between the highest f - score wordnet classier and our combined hypernym / coordinate classier .
there are 123 such disagreements , with wordnet agreeing with the human labels on 123 and our hybrid model agreeing on the other 123
here we inspect the types of noun pairs where our model improves upon wordnet , and nd that at least 123% of our models improvements are not restricted to named entities; given that the distribution of named entities among the labeled hypernyms in our test set is over 123% , this leads us to expect that our classier will perform well at the task of hypernym induction in more general , non - newswire domains .
our experiments demonstrate that automatic methods can be competitive with wordnet for the identication of hypernym pairs in newswire corpora .
in future work we plan to apply our technique to other general knowledge corpora .
further , we plan on extending our algorithms to automatically generate exible , statistically - grounded hypernym taxonomies directly from corpora .
type of noun pair named entity : person named entity : place named entity : company named entity : other not named entity :
john f .
kennedy / president , marlin fitzwater / spokesman diamond bar / city , france / place american can / company , simmons / company is elvis alive / book earthquake / disaster , soybean / crop
table 123 : analysis of improvements over wordnet
