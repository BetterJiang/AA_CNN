abstract we consider the task of 123 - d depth estimation from a single still image .
we take a supervised learning ap - proach to this problem , in which we begin by collecting a training set of monocular images ( of unstructured indoor and outdoor environments which include forests , sidewalks , trees , buildings , etc . ) and their corresponding ground - truth depthmaps .
then , we apply supervised learning to predict the value of the depthmap as a function of the image .
depth estimation is a challenging problem , since local features alone are insufcient to estimate depth at a point , and one needs to consider the global context of the image .
our model uses a hierarchical , multiscale markov random field ( mrf ) that incorporates multiscale local - and global - image features , and models the depths and the relation between depths at different points in the image .
we show that , even on unstructured scenes , our algorithm is frequently able to recover fairly accurate depthmaps .
we further propose a model that incorporates both monocular cues and stereo ( tri - angulation ) cues , to obtain signicantly more accurate depth estimates than is possible using either monocular or stereo keywords monocular vision learning depth 123d reconstruction dense reconstruction markov random eld depth estimation monocular depth stereo vision hand - held camera visual modeling
saxena ( ( cid : 123 ) ) s . h .
chung a . y .
ng computer science department , stanford university , stanford , ca 123 , usa
recovering 123 - d depth from images is a basic problem in computer vision , and has important applications in robot - ics , scene understanding and 123 - d reconstruction .
most work on visual 123 - d reconstruction has focused on binocular vi - sion ( stereopsis ) ( scharstein and szeliski 123 ) and on other algorithms that require multiple images , such as struc - ture from motion ( forsyth and ponce 123 ) and depth from defocus ( das and ahuja 123 ) .
these algorithms con - sider only the geometric ( triangulation ) differences .
beyond stereo / triangulation cues , there are also numerous monocu - lar cuessuch as texture variations and gradients , defocus , color / haze , etc . that contain useful and important depth in - formation .
even though humans perceive depth by seam - lessly combining many of these stereo and monocular cues , most work on depth estimation has focused on stereovision .
depth estimation from a single still image is a difcult task , since depth typically remains ambiguous given only local image features .
thus , our algorithms must take into ac - count the global structure of the image , as well as use prior knowledge about the scene .
we also view depth estimation as a small but crucial step towards the larger goal of image understanding , in that it will help in tasks such as under - standing the spatial layout of a scene , nding walkable ar - eas in a scene , detecting objects , etc .
in this paper , we apply supervised learning to the problem of estimating depthmaps ( fig .
123b ) from a single still image ( fig .
123a ) of a variety of unstructured environments , both indoor and outdoor , con - taining forests , sidewalks , buildings , people , bushes , etc .
our approach is based on modeling depths and relation - ships between depths at multiple spatial scales using a hi - erarchical , multiscale markov random field ( mrf ) .
tak - ing a supervised learning approach to the problem of depth estimation , we used a 123 - d scanner to collect training data ,
123 a a single still image , and b the corresponding ( ground - truth ) depthmap .
colors in the depthmap indicate estimated distances from
which comprised a large set of images and their correspond - ing ground - truth depthmaps .
( this data has been made pub - lically available on the internet . ) using this training set , we model the conditional distribution of the depths given the monocular image features .
though learning in our mrf model is approximate , map inference is tractable via lin -
we further consider how monocular cues from a single image can be incorporated into a stereo system .
we be - lieve that monocular cues and ( purely geometric ) stereo cues give largely orthogonal , and therefore complementary , types of information about depth .
we show that combining both monocular and stereo cues gives better depth estimates than is obtained with either alone .
we also apply these ideas to autonomous obstacle avoid - ance .
using a simplied version of our algorithm , we drive a small remote - controlled car at high speeds through various unstructured outdoor environments containing both man - made and natural obstacles .
this paper is organized as follows .
section 123 gives an overview of various methods used for 123 - d depth reconstruc - tion .
section 123 describes some of the visual cues used by humans for depth perception , and sect .
123 describes the im - age features used to capture monocular cues .
we describe our probabilistic model in sect .
in sect .
123 , we describe our setup for collecting aligned image and laser data .
the results of depth prediction on single images are presented in sect .
section 123 also describes the use of a sim - plied version of our algorithm in driving a small remote - controlled car autonomously .
we describe how we incorpo - rate monocular and stereo cues into our model in sect .
finally , we conclude in sect
123 related work
int j comput vis ( 123 ) 123 : 123
tion techniques , such as : explicit measurements with laser or radar sensors ( quartulli and datcu 123 ) , using two ( or more than two ) images ( scharstein and szeliski 123 ) , and using video sequences ( cornelis et al .
among the vision - based approaches , most work has focused on stere - ovision ( see scharstein and szeliski 123 for a review ) , and on other algorithms that require multiple images , such as optical ow ( barron et al .
123 ) , structure from motion ( forsyth and ponce 123 ) and depth from defocus ( das and ahuja 123 ) .
frueh and zakhor ( 123 ) constructed 123d city models by merging ground - based and airborne views .
a large class of algorithms reconstruct the 123 - d shape of known objects , such as human bodies , from images and laser data ( thrun and wegbreit 123; anguelov et al .
structured lighting ( scharstein and szeliski 123 ) offers an - other method for depth reconstruction .
there are some algorithms that can perform depth recon - struction from single images in very specic settings .
nagai et al .
( 123 ) performed surface reconstruction from single images for known , xed , objects such as hands and faces .
methods such as shape from shading ( zhang et al .
123; maki et al .
123 ) and shape from texture ( lindeberg and garding 123; malik and rosenholtz 123; malik and per - ona 123 ) generally assume uniform color and / or texture , 123 and hence would perform very poorly on the complex , un - constrained , highly textured images that we consider .
hertz - mann and seitz ( 123 ) reconstructed high quality 123 - d mod - els from several images , but they required that the images also contain assistant objects of known shapes next to the target object .
torresani and hertzmann ( 123 ) worked on re - constructing non - rigid surface shapes from video sequences .
torralba and oliva ( 123 ) studied the fourier spectrum of the images to compute the mean depth of a scene .
michels et al .
( 123 ) used supervised learning to estimate 123 - d dis - tances to obstacles , for the application of autonomously driving a small car .
delage et al .
( 123 , 123 ) generated 123 - d models of indoor environments containing only walls and oor , from single monocular images .
single view metrol - ogy ( criminisi et al .
123 ) assumes that vanishing lines and points are known in a scene , and calculates angles between parallel lines to infer 123 - d structure from manhattan images .
we presented a method for learning depths from a sin - gle image in ( saxena et al .
123 ) and extended our method to improve stereo vision using monocular cues in ( saxena et al .
in work that is contemporary to ours , hoiem et al .
( 123a , 123b ) built a simple pop - up type 123 - d model from an image by classifying the image into ground , vertical and sky .
their method , which assumes a simple ground - vertical structure of the world , fails on many environments
although our work mainly focuses on depth estimation from a single still image , there are many other 123 - d reconstruc -
123also , most of these algorithms assume lambertian surfaces , which means the appearance of the surface does not change with viewpoint .
int j comput vis ( 123 ) 123 : 123
that do not satisfy this assumption and also does not give ac - curate metric depthmaps .
building on these concepts of sin - gle image 123 - d reconstruction , hoiem et al .
( 123 ) and sud - derth et al .
( 123 ) integrated learning - based object recogni - tion with 123 - d scene representations .
saxena et al .
( 123b ) extended these ideas to create 123 - d models that are both visu - ally pleasing as well as quantitatively accurate .
our approach draws on a large number of ideas from computer vision such as feature computation and multiscale representation of images .
a variety of image features and representations have been used by other authors , such as gabor lters ( nestares et al .
123 ) , wavelets ( strang and nguyen 123 ) , sift features ( mortensen et al .
123 ) , etc .
many of these image features are used for purposes such as recognizing objects ( murphy et al .
123; serre et al .
123 ) , faces ( zhao et al .
123 ) , facial expressions ( saxena et al .
123 ) , grasps ( saxena et al .
123a ) ; image segmentation ( konishi and yuille 123 ) , computing the visual gist of a scene ( oliva and torralba 123 ) and computing sparse rep - resentations of natural images ( olshausen and field 123 ) .
stereo and monocular image features have been used to - gether for object recognition and image segmentation ( kol - mogorov et al .
123 ) .
our approach is based on learning a markov random field ( mrf ) model .
mrfs are a workhorse of machine learning , and have been successfully applied to numer - ous problems in which local features were insufcient and more contextual information had to be used .
examples in - clude image denoising ( moldovan et al .
123 ) , stereo vi - sion and image segmentation ( scharstein and szeliski 123 ) , text segmentation ( lafferty et al .
123 ) , object classication ( murphy et al .
123 ) , and image labeling ( he et al .
for the application of identifying man - made structures in natural images , kumar and hebert used a discriminative ran - dom elds algorithm ( kumar and hebert 123 ) .
since mrf learning is intractable in general , most of these models are trained using pseudo - likelihood; sometimes the models pa - rameters are also hand - tuned .
123 monocular cues
humans use monocular cues such as texture variations , tex - ture gradients , interposition , occlusion , known object sizes , light and shading , haze , defocus , etc .
( bulthoff et al .
123 ) for example , many objects texture will look different at dif - ferent distances from the viewer .
texture gradients , which capture the distribution of the direction of edges , also help to indicate depth ( malik and perona 123 ) .
for example , a tiled oor with parallel lines will appear to have tilted lines in an image .
the distant patches will have larger variations in the line orientations , and nearby patches with almost par - allel lines will have smaller variations in line orientations .
similarly , a grass eld when viewed at different distances will have different texture gradient distributions .
haze is an - other depth cue , and is caused by atmospheric light scatter - ing ( narasimhan and nayar 123 ) .
many monocular cues are contextual information , in the sense that they are global properties of an image and cannot be inferred from small image patches .
for example , occlusion cannot be determined if we look at just a small portion of an occluded object .
although local information such as the texture and color of a patch can give some in - formation about its depth , this is usually insufcient to ac - curately determine its absolute depth .
for another example , if we take a patch of a clear blue sky , it is difcult to tell if this patch is innitely far away ( sky ) , or if it is part of a blue object .
due to ambiguities like these , one needs to look at the overall organization of the image to determine depths .
123 stereo cues
each eye receives a slightly different view of the world and stereo vision combines the two views to perceive 123 - d depth ( wandell 123 ) .
an object is projected onto different lo - cations on the two retinae ( cameras in the case of a stereo system ) , depending on the distance of the object .
the reti - nal ( stereo ) disparity varies with object distance , and is in - versely proportional to the distance of the object .
disparity is typically not an effective cue for estimating small depth variations of objects that are far away .
123 visual cues for depth perception
123 motion parallax and focus cues
humans use numerous visual cues to perceive depth .
such cues are typically grouped into four distinct categories : monocular , stereo , motion parallax , and focus cues ( loomis 123; schwartz 123 ) .
humans combine these cues to un - derstand the 123 - d structure of the world ( welchman et al .
123; porrill et al .
123; wu et al .
123; loomis 123 ) .
be - low , we describe these cues in more detail .
our probabilistic model will attempt to capture a number of monocular cues ( sect .
123 ) , as well as stereo triangulation cues ( sect
as an observer moves , closer objects appear to move more than further objects .
by observing this phenomenon , called motion parallax , one can estimate the relative distances in a scene ( wexler et al .
humans have the ability to change the focal lengths of the eye lenses by controlling the curvature of lens , thus helping them to focus on ob - jects at different distances .
the focus , or accommodation , cue refers to the ability to estimate the distance of an object from known eye lens conguration and the sharpness of the image of the object ( harkness 123 ) .
int j comput vis ( 123 ) 123 : 123
123 the convolutional lters used for texture energies and gradients .
the rst nine are 123 123 laws masks .
the last six are the oriented edge detectors spaced at 123 intervals .
the nine laws masks are used to perform local averaging , edge detection and spot detection
123 the absolute depth feature vector for a patch , which includes features from its immediate neighbors and its more distant neighbors ( at larger scales ) .
the relative depth features for each patch use histograms of the lter outputs
123 feature vector
in our approach , we divide the image into small rectangular patches , and estimate a single depth value for each patch .
we use two types of features : absolute depth featuresused to estimate the absolute depth at a particular patchand relative features , which we use to estimate relative depths ( magnitude of the difference in depth between two patches ) .
these features try to capture two processes in the human visual system : local feature processing ( absolute features ) , such as that the sky is far away; and continuity features ( relative features ) , a process by which humans understand whether two adjacent patches are physically connected in 123 - d and thus have similar depths . 123
we chose features that capture three types of local cues : texture variations , texture gradients , and color .
texture in - formation is mostly contained within the image intensity channel ( wandell 123 ) , 123 so we apply laws masks ( davies 123; michels et al .
123 ) to this channel to compute the texture energy ( fig .
haze is reected in the low fre - quency information in the color channels , and we capture this by applying a local averaging lter ( the rst laws mask ) to the color channels .
lastly , to compute an estimate
123if two neighboring patches of an image display similar features , hu - mans would often perceive them to be parts of the same object , and therefore to have similar depth values .
123we represent each image in ycbcr color space , where y is the inten - sity channel , and cb and cr are the color channels .
of texture gradient that is robust to noise , we convolve the intensity channel with six oriented edge lters ( shown in
one can envision including more features to capture other cues .
for example , to model atmospheric effects such as fog and haze , features computed from the physics of light scat - tering ( narasimhan and nayar 123 ) could also be included .
similarly , one can also include features based on surface - shading ( maki et al .
123 ) .
123 features for absolute depth
we rst compute summary statistics of a patch i in the im - age i ( x , y ) as follows .
we use the output of each of the 123 ( 123 laws masks , 123 color channels and 123 texture gradients ) l - ters fn , n = 123 , .
, 123 as : ei ( n ) = ( cid : 123 ) where k ( 123 , 123 ) give the sum absolute energy and sum squared energy respectively . 123 this gives us an initial feature vector of dimension 123
to estimate the absolute depth at a patch , local image features centered on the patch are insufcient , and one has to use more global properties of the image .
we attempt to capture this information by using image features extracted at multiple spatial scales ( image resolutions ) . 123 ( see fig
123our experiments using k ( 123 , 123 , 123 ) did not improve performance no - 123the patches at each spatial scale are arranged in a grid of equally sized non - overlapping regions that cover the entire image .
we use 123 scales in our experiments .
more globally about the spatial structure of the scene .
we capture the spatial structure of the image by modeling the relationships between depths in different parts of the image .
although the depth of a particular patch depends on the fea - tures of the patch , it is also related to the depths of other parts of the image .
for example , the depths of two adja - cent patches lying in the same building will be highly corre - lated .
we will use a hierarchical multiscale markov random field ( mrf ) to model the relationship between the depth of a patch and the depths of its neighboring patches ( fig .
in addition to the interactions with the immediately neighbor - ing patches , there are sometimes also strong interactions be - tween the depths of patches which are not immediate neigh - bors .
for example , consider the depths of patches that lie on a large building .
all of these patches will be at similar depths , even if there are small discontinuities ( such as a win - dow on the wall of a building ) .
however , when viewed at the smallest scale , some adjacent patches are difcult to recog - nize as parts of the same object .
thus , we will also model interactions between depths at multiple spatial scales .
123 gaussian model
our rst model will be a jointly gaussian markov random field ( mrf ) as shown in ( 123 ) .
pg ( d|x; , )
( di ( 123 ) xt
i r ) 123
( di ( s ) dj ( s ) ) 123
to capture the multiscale depth relations , we will model the depths di ( s ) for multiple scales s = 123 , 123 , 123
in our experi -
int j comput vis ( 123 ) 123 : 123
objects at different depths exhibit very different behaviors at different resolutions , and using multiscale features allows us to capture these variations ( willsky 123 ) .
for exam - ple , blue sky may appear similar at different scales , but tex - tured grass would not .
in addition to capturing more global information , computing features at multiple spatial scales also helps to account for different relative sizes of objects .
a closer object appears larger in the image , and hence will be captured in the larger scale features .
the same object when far away will be small and hence be captured in the small scale features .
features capturing the scale at which an object appears may therefore give strong indicators of
to capture additional global features ( e . g .
occlusion re - lationships ) , the features used to predict the depth of a par - ticular patch are computed from that patch as well as the four neighboring patches .
this is repeated at each of the three scales , so that the feature vector at a patch includes features of its immediate neighbors , its neighbors at a larger spatial scale ( thus capturing image features that are slightly further away in the image plane ) , and again its neighbors at an even larger spatial scale; this is illustrated in fig .
lastly , many structures ( such as trees and buildings ) found in outdoor scenes show vertical structure , in the sense that they are vertically connected to themselves ( things cannot hang in empty air ) .
thus , we also add to the features of a patch additional summary features of the column it lies in .
for each patch , after including features from itself and its 123 neighbors at 123 scales , and summary features for its 123 column patches , our absolute depth feature vector x is 123 123 = 123 dimensional .
123 features for relative depth
we use a different feature vector to learn the dependencies between two neighboring patches .
specically , we compute a 123 - bin histogram of each of the 123 lter outputs |i fn| , giving us a total of 123 features yis for each patch i at scale s .
these features are used to estimate how the depths at two different locations are related .
we believe that learn - ing these estimates requires less global information than pre - dicting absolute depth , but more detail from the individual patches .
for example , given two adjacent patches of a dis - tinctive , unique , color and texture , we may be able to safely conclude that they are part of the same object , and thus that their depths are close , even without more global features .
hence , our relative depth features yij s for two neighboring patches i and j at scale s will be the differences between their histograms , i . e . , yij s = yis yj s .
123 probabilistic model
since local images features are by themselves usually in - sufcient for estimating depth , the model needs to reason
123 the multiscale mrf model for modeling relation between fea - tures and depths , relation between depths at same scale , and relation between depths at different scales .
( only 123 out of 123 scales , and a subset of the edges , are shown )
int j comput vis ( 123 ) 123 : 123
ments , we enforce a hard constraint that depths at a higher scale are the average of the depths at the lower scale . 123 more formally , we dene di ( s + 123 ) = ( 123 / 123 ) jns ( i ) ( i ) dj ( s ) .
here , ns ( i ) are the 123 neighbors of patch i at scale s . 123
in ( 123 ) , m is the total number of patches in the image ( at the lowest scale ) ; z is the normalization constant for the model; xi is the absolute depth feature vector for patch i; and and are parameters of the model .
in detail , we use different parameters ( r , 123r , 123r ) for each row r in the im - age , because the images we consider are taken from a hori - zontally mounted camera , and thus different rows of the im - age have different statistical properties .
for example , a blue patch might represent sky if it is in upper part of image , and might be more likely to be water if in the lower part of the
our model is a conditionally trained mrf , in that its model of the depths d is always conditioned on the image features x; i . e . , it models only p ( d|x ) .
we rst estimate the parameters r in ( 123 ) by maximizing the conditional log likelihood ( cid : 123 ) ( d ) = log p ( d|x; r ) of the training data .
since the model is a multivariate gaussian , the maximum likeli - hood estimate of parameters r is obtained by solving a lin - ear least squares problem .
the rst term in the exponent above models depth as a function of multiscale features of a single patch i .
the sec - ond term in the exponent places a soft constraint on the depths to be smooth .
if the variance term 123 123rs is a xed constant , the effect of this term is that it tends to smooth depth estimates across nearby patches .
however , in practice the dependencies between patches are not the same every - where , and our expected value for ( di dj ) 123 may depend on the features of the local patches .
therefore , to improve accuracy we extend the model to capture the variance term 123 123rs in the denominator of the second term as a linear function of the patches i and j s rel - ative depth features yij s ( discussed in sect .
we model |yij s| .
this helps determine which the variance as 123 neighboring patches are likely to have similar depths; for example , the smoothing effect is much stronger if neigh - boring patches are similar .
this idea is applied at multiple scales , so that we learn different 123 123rs for the different scales s ( and rows r of the image ) .
the parameters urs are learned 123rs to the expected value of ( di ( s ) dj ( s ) ) 123 , with to t 123 a constraint that urs 123 ( to keep the estimated 123 negative ) , using a quadratic program ( qp ) .
123one can instead have soft constraints relating the depths at higher scale to depths at lower scale .
one can also envision putting more con - straints in the mrf , such as that points lying on a long straight edge in an image should lie on a straight line in the 123 - d model , etc .
123our experiments using 123 - connected neighbors instead of 123 - connected neighbors yielded minor improvements in accuracy at the cost of a much longer inference time .
similar to our discussion on 123
123rs , we also learn the vari - ance parameter 123 r xi as a linear function of the fea - tures .
since the absolute depth features xi are non - negative , the estimated 123 123r is also non - negative .
the parameters vr 123r to the expected value of ( di ( r ) t are chosen to t 123 r xi ) 123 , subject to vr 123
this 123 123r term gives a measure of the un - certainty in the rst term , and depends on the features .
this is motivated by the observation that in some cases , depth cannot be reliably estimated from the local features .
in this case , one has to rely more on neighboring patches depths , as modeled by the second term in the exponent .
after learning the parameters , given a new test - set image we can nd the map estimate of the depths by maximizing ( 123 ) in terms of d .
since ( 123 ) is gaussian , log p ( d|x; , ) is quadratic in d , and thus its maximum is easily found in closed form ( taking at most 123 seconds per image ) .
more details are given in appendix 123
123 laplacian model
we now present a second model ( see ( 123 ) ) that uses lapla - cians instead of gaussians to model the posterior distribu - tion of the depths .
pl ( d|x; , )
|di ( 123 ) xt
|di ( s ) dj ( s ) |
our motivation for doing so is three - fold .
first , a histogram of the relative depths ( di dj ) is empirically closer to lapla - cian than gaussian ( fig .
123 , see ( huang et al .
123 ) for more details on depth statistics ) , which strongly suggests that it is better modeled as one . 123 second , the laplacian distribu - tion has heavier tails , and is therefore more robust to out - liers in the image features and to errors in the training - set depthmaps ( collected with a laser scanner; see sect .
third , the gaussian model was generally unable to give depthmaps with sharp edges; in contrast , laplacians tend to model sharp transitions / outliers better .
this model is parametrized by r ( similar to ( 123 ) ) and by 123r and 123rs , the laplacian spread parameters .
maximum - likelihood parameter estimation for the laplacian model is not tractable ( since the partition function depends on r ) .
however , by analogy to the gaussian case , we approximate
123although the laplacian distribution ts the log - histogram of multi - scale relative depths reasonably well , there is an unmodeled peak near zero .
a more recent model ( saxena et al .
123b ) attempts to model this peak , which arises due to the fact that the neighboring depths at the nest scale frequently lie on the same object .
int j comput vis ( 123 ) 123 : 123
laser range nding equipment on a lagr ( learning ap - plied to ground robotics ) robot ( fig .
the lagr vehicle is equipped with sensors , an onboard computer , and point grey research bumblebee stereo cameras , mounted with a baseline distance of 123 cm ( saxena et al .
we collected a total of 123 image+depthmap pairs , with an image resolution of 123 123 and a depthmap resolu - tion of 123 123
in the experimental results reported here , 123% of the images / depthmaps were used for training , and the remaining 123% for hold - out testing .
the images com - prise a wide variety of scenes including natural environ - ments ( forests , trees , bushes , etc . ) , man - made environments ( buildings , roads , sidewalks , trees , grass , etc . ) , and purely indoor environments ( corridors , etc . ) .
due to limitations of the laser , the depthmaps had a maximum range of 123m ( the maximum range of the laser scanner ) , and had minor ad - ditional errors due to reections , missing laser scans , and mobile objects .
prior to running our learning algorithms , we transformed all the depths to a log scale so as to emphasize multiplicative rather than additive errors in training .
data used in the experiments is available at : http : / / ai . stanford . edu /
we tested our model on real - world test - set images of forests ( containing trees , bushes , etc . ) , campus areas ( buildings , people , and trees ) , and indoor scenes ( such as corridors ) .
table 123 shows the test - set results with different feature combinations of scales , summary statistics , and neighbors , on three classes of environments : forest , campus , and indoor .
the baseline model is trained without any features , and pre - dicts the mean value of depth in the training depthmaps .
we see that multiscale and column features improve the algo - rithms performance .
including features from neighboring patches , which help capture more global information , re - duces the error from 123 orders of magnitude to 123 or - ders of magnitude . 123 we also note that the laplacian model performs better than the gaussian one , reducing error to 123 orders of magnitude for indoor scenes , and 123 orders of magnitude when averaged over all scenes .
em - pirically , the laplacian model does indeed give depthmaps with signicantly sharper boundaries ( as in our discussion in sect .
123; also see fig
figure 123 shows that modeling the spatial relationships in the depths is important .
depths estimated without using the second term in the exponent of ( 123 ) , i . e . , depths predicted us - ing only image features with row - sensitive parameters r ,
123errors are on a log123 scale .
thus , an error of means a multiplicative error of 123 in actual depth .
e . g . , 123 = 123 , which thus repre - sents an 123% multiplicative error .
123 the log - histogram of relative depths .
empirically , the distribu - tion of relative depths is closer to laplacian than gaussian
this by solving a linear system of equations xr r dr to ||dr xr r||123
minimize l123 ( instead of l123 ) error , i . e . , minr here xr is the matrix of absolute depth features .
following the gaussian model , we also learn the laplacian spread pa - rameters in the denominator in the same way , except that the instead of estimating the expected values of ( di dj ) 123 and r xi ) 123 , we estimate the expected values of |di dj| ( di ( r ) t and |di ( r ) t r xi| , as a linear function of urs and vr re - spectively .
this is done using a linear program ( lp ) , with urs 123 and vr 123
even though maximum likelihood ( ml ) parameter esti - mation for r is intractable in the laplacian model , given a new test - set image , map inference for the depths d is tractable and convex .
details on solving the inference prob - lem as a linear program ( lp ) are given in appendix 123
remark we can also extend these models to combine gaussian and laplacian terms in the exponent , for exam - ple by using a l123 norm term for absolute depth , and a l123 norm term for the interaction terms .
map inference remains tractable in this setting , and can be solved using convex op - timization as a qp ( quadratic program ) .
123 data collection
we used a 123 - d laser scanner to collect images and their cor - responding depthmaps ( fig .
the scanner uses a laser de - vice ( sick lms - 123 ) which gives depth readings in a verti - cal column , with a 123 resolution .
to collect readings along the other axis ( left to right ) , the sick laser was mounted on a panning motor .
the motor rotates after each vertical scan to collect laser readings for another vertical column , horizontal angular resolution .
we reconstruct with a 123 the depthmap using the vertical laser scans , the motor read - ings and known relative position and pose of the laser de - vice and the camera .
we also collected data of stereo pairs with corresponding depthmaps ( sect .
123 ) , by mounting the
int j comput vis ( 123 ) 123 : 123
123 results for a varied set of environments , showing a original image , b ground truth depthmap , c predicted depthmap by gaussian model , d predicted depthmap by laplacian model .
( best viewed in color )
int j comput vis ( 123 ) 123 : 123
123 the custom built 123 - d scanner for collecting depthmaps with stereo image pairs , mounted on the lagr robot
or when there are signicant changes in the ground texture .
in contrast , our algorithm appears to be robust to luminance variations , such as shadows ( fig .
123 , 123th row ) and camera ex - posure ( fig .
123 , 123nd and 123th rows ) .
some of the errors of the algorithm can be attributed to errors or limitations of the training set .
for example , the maximum value of the depths in the training and test set is 123 m; therefore , far - away objects are all mapped to the distance of 123 m .
further , laser readings are often incorrect for reective / transparent objects such as glass; therefore , our algorithm also often estimates depths of such objects in - correctly .
quantitatively , our algorithm appears to incur the largest errors on images which contain very irregular trees , in which most of the 123 - d structure in the image is dominated by the shapes of the leaves and branches .
however , arguably even human - level performance would be poor on these im -
we note that monocular cues rely on prior knowledge , learned from the training set , about the environment .
this is because monocular 123 - d reconstruction is an inherently am - biguous problem .
thus , the monocular cues may not gener - alize well to images very different from ones in the training set , such as underwater images or aerial photos .
to test the generalization capability of the algorithm , we also estimated depthmaps of images downloaded from the internet ( images for which camera parameters are not known ) . 123 the model ( using monocular cues only ) was able
123since we do not have ground - truth depthmaps for images down - loaded from the internet , we are unable to give a quantitative com - parisons on these images .
further , in the extreme case of orthogonal cameras or very wide angle perspective cameras , our algorithm would need to be modied to take into account the eld of view of the camera .
123 the 123 - d scanner used for collecting images and the correspond -
are very noisy ( fig .
123d ) . 123 modeling the relations between the neighboring depths at multiple scales through the sec - ond term in the exponent of ( 123 ) also gave better depthmaps ( fig .
finally , fig .
123c shows the models prior on depths; the depthmap shown reects our models use of image - row sensitive parameters .
in our experiments , we also found that many features / cues were given large weights; therefore , a model trained with only a few cues ( e . g . , the top 123 chosen by a feature selection method ) was not able to predict reasonable depths .
our algorithm works well in a varied set of environments , as shown in fig .
123 ( last column ) .
a number of vision al - gorithms based on ground nding ( e . g . , gini and marchi 123 ) appear to perform poorly when there are discontinu - ities or signicant luminance variations caused by shadows ,
123this algorithm gave an overall error of 123 , compared to our full models error of 123 .
int j comput vis ( 123 ) 123 : 123 table 123 effect of multiscale and column features on accuracy .
the average absolute errors ( rms errors gave very similar trends ) are on a log scale ( base 123 ) .
h123 and h123 represent summary statistics for k = 123 , 123
s123 , s123 and s123 represent the 123 scales .
c represents the column features .
baseline is trained with only the bias term ( no features )
gaussian ( s123 , s123 , s123 , h123 , h123 , no neighbors ) gaussian ( s123 , h123 , h123 ) gaussian ( s123 , s123 , h123 , h123 ) gaussian ( s123 , s123 , s123 , h123 , h123 ) gaussian ( s123 , s123 , s123 , c , h123 ) gaussian ( s123 , s123 , s123 , c , h123 , h123 )
123 a original image , b ground truth depthmap , c prior depthmap ( trained with no features ) , d features only ( no mrf relations ) , e full laplacian model .
( best viewed in color )
to produce reasonable depthmaps on most of the images ( fig .
informally , our algorithm appears to predict the relative depths quite well ( i . e . , their relative distances to the camera ) ;123 even for scenes very different from the training set , such as a sunower eld , an oil - painting scene , moun - tains and lakes , a city skyline photographed from sea , a city during snowfall , etc .
car driving experiments michels et al .
( 123 ) used a simplied version of the monocular depth estimation algo - rithm to drive a remote - controlled car ( fig .
the al - gorithm predicts ( 123 - d ) depths from single still images , cap - tured from a web - camera with 123 123 pixel resolution .
the learning algorithm can be trained on either real cam - era images labeled with ground - truth ranges to the closest obstacle in each direction , or on a training set consisting of synthetic graphics images .
the resulting algorithm , trained on a combination of real and synthetic data , was able to learn monocular visual cues that accurately estimate the rel - ative depths of obstacles in a scene ( fig .
we tested the algorithm by driving the car at four different locations ,
123for most applications such as object recognition using knowledge of depths , robotic navigation , or 123 - d reconstruction , relative depths are sufcient .
the depths could be rescaled to give accurate absolute depths , if the camera parameters are known or are estimated .
ranging from man - made environments with concrete tiles and trees , to uneven ground in a forest environment with rocks , trees and bushes where the car is almost never further than 123 m from the nearest obstacle .
the mean time before crash ranged from 123 to more than 123 seconds , depend - ing on the density of the obstacles ( michels et al .
the unstructured testing sites were limited to areas where no training or development images were taken .
videos of the algorithm driving the car autonomously are available at :
123 improving performance of stereovision using
consider the problem of estimating depth from two images taken from a pair of stereo cameras ( fig .
the most common approach for doing so is stereopsis ( stereovision ) , in which depths are estimated by triangulation using the two images .
over the past few decades , researchers have developed very good stereovision systems ( see scharstein and szeliski 123 for a review ) .
although these systems work well in many environments , stereovision is fundamen - tally limited by the baseline distance between the two cam - eras .
specically , their depth estimates tend to be inaccurate when the distances considered are large ( because even very
int j comput vis ( 123 ) 123 : 123
123 typical examples of the predicted depthmaps for images downloaded from the internet .
( best viewed in color )
that monocular cues and ( purely geometric ) stereo cues give largely orthogonal , and therefore complementary , types of information about depth .
stereo cues are based on the dif - ference between two images and do not depend on the con - tent of the image .
even if the images are entirely random , it would still generate a pattern of disparities ( e . g . , random dot stereograms , bulthoff et al .
on the other hand , depth estimates from monocular cues are entirely based on the evi - dence about the environment presented in a single image .
in this section , we investigate how monocular cues can be in - tegrated with any reasonable stereo system , to obtain better depth estimates than the stereo system alone .
123 disparity from stereo correspondence
123 a the remote - controlled car driven autonomously in various cluttered unconstrained environments , using our algorithm .
b a view from the car , with the chosen steering direction indicated by the red square; the estimated distances to obstacles in the different directions are shown by the bar graph below the image
small triangulation / angle estimation errors translate to very large errors in distances ) .
further , stereovision also tends to fail for textureless regions of images where correspondences cannot be reliably found .
on the other hand , humans perceive depth by seamlessly combining monocular cues with stereo cues .
we believe
depth estimation using stereovision from two images ( taken from two cameras separated by a baseline distance ) involves three steps : first , establish correspondences between the two images .
then , calculate the relative displacements ( called disparity ) between the features in each image .
finally , de - termine the 123 - d depth of the feature relative to the cameras , using knowledge of the camera geometry .
123 two images taken from a stereo pair of cameras , and the depthmap calculated by a stereo
int j comput vis ( 123 ) 123 : 123
stereo correspondences give reliable estimates of dispar - ity , except when large portions of the image are featureless ( i . e . , correspondences cannot be found ) .
further , for a given baseline distance between cameras , the accuracy decreases as the depth values increase .
in the limit of very distant objects , there is no observable disparity , and depth estima - tion generally fails .
empirically , depth estimates from stereo tend to become unreliable when the depth exceeds a certain
our stereo system nds good feature correspondences between the two images by rejecting pixels with little tex - ture , or where the correspondence is otherwise ambiguous .
more formally , we reject any feature where the best match is not signicantly better than all other matches within the search window .
we use the sum - of - absolute - differences cor - relation as the metric score to nd correspondences ( forsyth and ponce 123 ) .
our cameras ( and algorithm ) allow sub - pixel interpolation accuracy of 123 pixels of disparity .
even though we use a fairly basic implementation of stereopsis , the ideas in this paper can just as readily be applied together with other , perhaps better , stereo systems .
123 modeling uncertainty in stereo
the errors in disparity are often modeled as either gaussian ( das and ahuja 123 ) or via some other , heavier - tailed dis - tribution ( e . g . , szeliski 123 ) .
specically , the errors in dis - parity have two main causes : ( a ) assuming unique / perfect correspondence , the disparity has a small error due to image noise ( including aliasing / pixelization ) , which is well mod - eled by a gaussian .
( b ) occasional errors in correspondence cause larger errors , which results in a heavy - tailed distribu - tion for disparity ( szeliski 123 ) .
if the standard deviation is g in computing disparity g from stereo images ( because of image noise , etc . ) , then the standard deviation of the depths123 will be d , stereo g / g .
for our stereo system , we have that g is about 123 pix -
123using the delta rule from statistics : var ( f ( x ) ) ( f rived from a second order taylor series approximation of f ( x ) .
the depth d is related to disparity g as d = log ( c / g ) , with camera parame - ters determining c .
els;123 this is then used to estimate d , stereo .
note therefore that d , stereo is a function of the estimated depth , and specif - ically , it captures the fact that variance in depth estimates is larger for distant objects than for closer ones .
123 probabilistic model
we use our markov random field ( mrf ) model , which models relations between depths at different points in the to incorporate both monocular and stereo cues .
therefore , the depth of a particular patch depends on the monocular features of the patch , on the stereo disparity , and is also related to the depths of other parts of the image .
pg ( d|x; , ) + ( di ( 123 ) xt i r ) 123
( di ( 123 ) di , stereo ) 123
( di ( s ) dj ( s ) ) 123
pl ( d|x; , )
( cid : 123 ) |di ( 123 ) di , stereo|
|di ( s ) dj ( s ) |
+ |di ( 123 ) xt
in our gaussian and laplacian mrfs ( see ( 123 ) and ( 123 ) ) , we now have an additional term di , stereo , which is the depth estimate obtained from disparity . 123 this term models the re -
123one can also envisage obtaining a better estimate of g as a func - tion of a match metric used during stereo correspondence ( brown et al .
123 ) , such as normalized sum of squared differences; or learning g as a function of disparity / texture based features .
123in this work , we directly use di , stereo as the stereo cue .
in ( saxena et al .
123c ) , we use a library of features created from stereo depths as the cues for identifying a grasp point on objects .
int j comput vis ( 123 ) 123 : 123
lation between the depth and the estimate from stereo dispar - ity .
the other terms in the models are similar to ( 123 ) and ( 123 ) in sect
table 123 the average errors ( rms errors gave very similar trends ) for various cues and models , on a log scale ( base 123 )
123 results on stereo for these experiments , we collected 123 stereo pairs + depthmaps in a wide - variety of outdoor and indoor envi - ronments , with an image resolution of 123 123 and a depthmap resolution of 123 123
we used 123% of the im - ages / depthmaps for training , and the remaining 123% for
we quantitatively compare the following classes of algo - rithms that use monocular and stereo cues in different ways :
( i ) baseline : this model , trained without any features , predicts the mean value of depth in the training depth -
( ii ) stereo : raw stereo depth estimates , with the missing values set to the mean value of depth in the training
( iii ) stereo ( smooth ) : this method performs interpolation and region lling; using the laplacian model without the second term in the exponent in ( 123 ) , and also without using monocular cues to estimate 123 as a function of the
( iv ) mono ( gaussian ) : depth estimates using only monoc - ular cues , without the rst term in the exponent of the gaussian model in ( 123 ) .
( v ) mono ( lap ) : depth estimates using only monocular cues , without the rst term in the exponent of the laplacian model in ( 123 ) .
( vi ) stereo+mono : depth estimates using the full model .
table 123 shows that although the model is able to pre - dict depths using monocular cues only ( mono ) , the per - formance is signicantly improved when we combine both mono and stereo cues .
the algorithm is able to estimate depths with an error of 123 orders of magnitude , ( i . e . , 123% of multiplicative error because 123 = 123 ) which represents a signicant improvement over stereo ( smooth ) performance of 123 .
figure 123 shows that
is able to predict depthmaps ( column 123 ) in a variety of environments .
it also demonstrates how the model takes the best estimates from both stereo and monocular cues to estimate more accurate depthmaps .
for example , in row 123 ( fig .
123 ) , the depthmap generated by stereo ( column 123 ) is very inaccurate; however , the monocular - only model predicts depths fairly accurately ( column 123 ) .
the combined model uses both sets of cues to produce a better depthmap ( column 123 ) .
in row 123 , stereo cues give a better estimate than monocular ones .
we again see that our combined mrf model , which uses both monocu - lar and stereo cues , gives an accurate depthmap ( column 123 )
correcting some mistakes of stereo , such as some far - away regions that stereo predicted as close .
in fig .
123 , we study the behavior of the algorithm as a function of the 123 - d distance from the camera .
at small dis - tances , the algorithm relies more on stereo cues , which are more accurate than the monocular cues in this regime .
how - ever , at larger distances , the performance of stereo degrades , and the algorithm relies more on monocular cues .
since our algorithm models uncertainties in both stereo and monoc - ular cues , it is able to combine stereo and monocular cues
we note that monocular cues rely on prior knowledge , learned from the training set , about the environment .
this is because monocular 123 - d reconstruction is an inherently am - biguous problem .
in contrast , the stereopsis cues we used are purely geometric , and therefore should work well even on images taken from very different environments .
for ex - ample , the monocular algorithm fails sometimes to predict correct depths for objects which are only partially visible in the image ( e . g . , fig .
123 , row 123 : tree on the left ) .
for a point lying on such an object , most of the points neighbors lie outside the image; hence the relations between neighboring depths are less effective here than for objects lying in the middle of an image .
however , in many of these cases , the stereo cues still allow an accurate depthmap to be estimated ( row 123 , column 123 ) .
over the last few decades , stereo and other triangula - tion cues have been successfully applied to many impor - tant problems , including robot navigation , building 123 - d mod - els of urban environments , and object recognition .
unlike triangulation - based algorithms such as stereopsis and struc - ture from motion , we have developed a class of algorithms that exploit a largely orthogonal set of monocular cues .
we presented a hierarchical , multiscale markov random field ( mrf ) learning model that uses such cues to estimate depth from a single still image .
these monocular cues can not only
int j comput vis ( 123 ) 123 : 123
123 results for a varied set of environments , showing one image of the stereo pairs ( column 123 ) , ground truth depthmap collected from 123 - d laser scanner ( column 123 ) , depths calculated by stereo ( column 123 ) , depths predicted by using monocular cues only ( column 123 ) , depths pre -
dicted by using both monocular and stereo cues ( column 123 ) .
the bottom row shows the color scale for representation of depths .
closest points are 123 m , and farthest are 123 m .
( best viewed in color )
int j comput vis ( 123 ) 123 : 123
appendix 123 : map inference for laplacian model exact map inference of the depths d rm can be obtained by maximizing log p ( d|x; , ) in terms of d ( see ( 123 ) ) .
= arg max = arg min
log p ( d|x; , ) |d xr| + ct
123 the average errors ( on a log scale , base 123 ) as a function of the distance from the camera
be combined with triangulation ones , but also scale better than most triangulation - based cues to depth estimation at large distances .
although our work has been limited to depth estimation , we believe that these monocular depth and shape cues also hold rich promise for many other applications in
acknowledgements we give warm thanks to jamie schulte , who designed and built the 123 - d scanner , and to andrew lookingbill , who helped us with collecting the data used in this work .
we also thank jeff michels , larry jackel , sebastian thrun , min sun and pieter abbeel for helpful discussions .
this work was supported by the darpa lagr program under contract number fa123 - 123 - c - 123
appendix 123 : map inference for gaussian model
a ( d xar )
we can rewrite ( 123 ) as a standard multivariate gaussian , pg ( d|x; , ) ( d xar ) t where xa = ( 123 x , with 123 and 123 representing the matrices of the variances 123 123 , i in the rst and second terms in the exponent of ( 123 ) respectively . 123 q is a matrix such that rows of qd give the differences of the depths in the neighboring patches at multiple scales ( as in the second term in the exponent of ( 123 ) ) .
our map esti - mate of the depths is , therefore , d during learning , we iterate between learning and esti - mating .
empirically , 123 ( cid : 123 ) 123 , and xa is very close to x; therefore , the algorithm converges after 123 iterations .
= xar .
123 , i and 123
123note that if the variances at each point in the image are constant , then xa = ( i + 123 123x .
i . e . , xa is essentially a smoothed version of x .
123 qt q )
where , c123 rm with c123 , i = 123 / 123 , i , and c123 r123m with c123 , i = 123 / 123 , i .
our features are given by x rmxk and the learned parameters are r rk , which give a naive estimate d = xr rm of the depths .
q is a matrix such that rows
of qd give the differences of the depths in the neighbor - ing patches at multiple scales ( as in the second term in the exponent of ( 123 ) ) .
we add auxiliary variables 123 and 123 to pose the problem
as a linear program ( lp ) : = arg min 123 123 + ct s . t .
123 d d 123 123 qd 123
in our experiments , map inference takes about 123 seconds for an image .
