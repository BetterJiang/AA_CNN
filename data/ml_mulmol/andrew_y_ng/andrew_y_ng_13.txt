a great deal of research has focused on al - gorithms for learning features from unla - beled data .
indeed , much progress has been made on benchmark datasets like norb and cifar by employing increasingly complex unsupervised learning algorithms and deep models .
in this paper , however , we show that several simple factors , such as the number of hidden nodes in the model , may be more im - portant to achieving high performance than the learning algorithm or the depth of the model .
specically , we will apply several o - the - shelf feature learning algorithms ( sparse auto - encoders , sparse rbms , k - means clus - tering , and gaussian mixtures ) to cifar , norb , and stl datasets using only single - layer networks .
we then present a detailed analysis of the eect of changes in the model setup : the receptive eld size , number of hid - den nodes ( features ) , the step - size ( stride ) between extracted features , and the eect of whitening .
our results show that large numbers of hidden nodes and dense fea - ture extraction are critical to achieving high performanceso critical , in fact , that when these parameters are pushed to their limits , we achieve state - of - the - art performance on both cifar - 123 and norb using only a sin - gle layer of features .
more surprisingly , our best performance is based on k - means clus - tering , which is extremely fast , has no hyper - parameters to tune beyond the model struc - ture itself , and is very easy to implement .
de - spite the simplicity of our system , we achieve accuracy beyond all previously published re - sults on the cifar - 123 and norb datasets ( 123% and 123% respectively ) .
appearing in proceedings of the 123th international con - ference on articial intelligence and statistics ( aistats ) 123 , fort lauderdale , fl , usa .
volume 123 of jmlr : w&cp 123
copyright 123 by the authors .
much recent work in machine learning has focused on learning good feature representations from unlabeled input data for higher - level tasks such as classication .
current solutions typically learn multi - level represen - tations by greedily pre - training several layers of fea - tures , one layer at a time , using an unsupervised learn - ing algorithm ( 123 , 123 , 123 ) .
for each of these layers a number of design parameters are chosen : the number of features to learn , the locations where these features will be computed , and how to encode the inputs and outputs of the system .
in this paper we study the ef - fect of these choices on single - layer networks trained by several feature learning methods .
our results demon - strate that several key ingredients , orthogonal to the learning algorithm itself , can have a large impact on large numbers of features , and dense feature extraction can all be major advan - tages .
even with very simple algorithms and a sin - gle layer of features , it is possible to achieve state - of - the - art performance by focusing eort on these choices rather than on the learning system itself .
a major drawback of many feature learning systems is their complexity and expense .
in addition , many algorithms require careful selection of multiple hyper - parameters like learning rates , momentum , sparsity penalties , weight decay , and so on that must be cho - sen through cross - validation , thus increasing running times dramatically .
though it is true that recently in - troduced algorithms have consistently shown improve - ments on benchmark datasets like norb ( 123 ) and cifar - 123 ( 123 ) , there are several other factors that af - fect the nal performance of a feature learning sys - tem .
specically , there are many meta - parameters dening the network architecture , such as the recep - tive eld size and number of hidden nodes ( features ) .
in practice , these parameters are often determined by computational constraints .
for instance , we might use the largest number of features possible considering the running time of the algorithm .
in this paper , how - ever , we pursue an alternative strategy : we employ very simple learning algorithms and then more care -
123 an analysis of single - layer networks in unsupervised feature learning
fully choose the network parameters in search of higher if ( as is often the case ) larger repre - sentations perform better , then we can leverage the speed and simplicity of these learning algorithms to use larger representations .
to this end , we will begin in section 123 by describing a simple feature learning framework that incorporates an unsupervised learning algorithm as a black box module within .
for this black box , we have im - plemented several o - the - shelf unsupervised learning algorithms : sparse auto - encoders , sparse rbms , k - means clustering , and gaussian mixture models .
we then analyze the performance impact of several dif - ferent elements in the feature learning framework , in - cluding : ( i ) whitening , which is a common pre - process in deep learning work , ( ii ) number of features trained , ( iii ) step - size ( stride ) between extracted features , and ( iv ) receptive eld size .
it will turn out that whitening , large numbers of fea - tures , and small stride lead to uniformly better perfor - mance regardless of the choice of unsupervised learning algorithm .
on the one hand , these results are some - what unsurprising .
for instance , it is widely held that highly over - complete feature representations tend to give better performance than smaller - sized represen - tations ( 123 ) , and similarly with small strides between features ( 123 ) .
however , the main contribution of our work is demonstrating that these considerations may , in fact , be critical to the success of feature learning algorithmspotentially more important even than the choice of unsupervised learning algorithm .
indeed , it will be shown that when we push these parameters to their limits that we can achieve state - of - the - art perfor - mance , outperforming many other more complex algo - rithms on the same task .
quite surprisingly , our best results are achieved using k - means clustering , an algo - rithm that has been used extensively in computer vi - sion , but that has not been widely adopted for deep feature learning .
specically , we achieve the test accu - racies of 123% on cifar - 123 and 123% on norb better than all previously published results .
we will start by reviewing related work on feature learning , then move on to describe a general feature learning framework that we will use for evaluation in section 123
we then present experimental analysis and results on cifar - 123 ( 123 ) as well as norb ( 123 ) in sec -
123 related work
since the introduction of unsupervised pre - training ( 123 ) , many new schemes for stacking layers of to build deep representations have been proposed .
most have focused on creating new training algo - rithms to build single - layer models that are composed to build deeper structures .
among the algorithms
considered in the literature are sparse - coding ( 123 , 123 , 123 ) , rbms ( 123 , 123 ) , sparse rbms ( 123 ) , sparse auto - encoders ( 123 , 123 ) , denoising auto - encoders ( 123 ) , fac - tored ( 123 ) and mean - covariance ( 123 ) rbms , as well as many others ( 123 , 123 ) .
thus , amongst the many com - ponents of feature learning architectures , the unsuper - vised learning module appears to be the most heavily some work , however , has considered the impact of other choices in these feature learning systems , es - pecially the choice of network architecture .
( 123 ) , for instance , have considered the impact of changes to the pooling strategies frequently em - ployed between layers of features , as well as dierent forms of normalization and rectication between lay - ers .
similarly , boureau et al .
have considered the im - pact of coding strategies and dierent types of pooling , both in practice ( 123 ) and in theory ( 123 ) .
our work fol - lows in this vein , but considers instead the structure of single - layer networksbefore pooling , and orthogonal to the choice of algorithm or coding scheme .
many common threads from the computer vision lit - erature also relate to our work and to feature learning more broadly .
for instance , we will use the k - means clustering algorithm as an alternative unsupervised learning module .
k - means has been used less widely in deep learning work but has enjoyed wide adoption in computer vision for building codebooks of visual words ( 123 , 123 , 123 , 123 ) , which are used to dene higher - level image features .
this method has also been ap - plied recursively to build multiple layers of features ( 123 ) .
the eects of pooling and choice of activation func - tion or coding scheme have similarly been studied for these models ( 123 , 123 , 123 ) .
van gemert et al . , for in - stance , demonstrate that soft activation functions ( kernels ) tend to work better than the hard assign - ment typically used with visual words models .
this paper will compare results along some of the same axes as these prior works ( e . g . , we will consider both hard and soft activation functions ) , but our conclu - sions dier somewhat : while we conrm that some feature - learning schemes are better than others , we also show that the dierences can often be outweighed by other factors , such as the number of features .
thus , even though more complex learning schemes may im - prove performance slightly , these advantages can be overcome by fast , simple learning algorithms that are able to handle larger networks .
123 unsupervised feature learning
in this section , we describe a common framework used for feature learning .
for concreteness , we will focus on the application of these algorithms to learning fea - tures from images , though our approach is applicable
123 adam coates , honglak lee , andrew y
to other forms of data as well .
the framework we use involves several stages and is similar to those employed in computer vision ( 123 , 123 , 123 , 123 , 123 ) , as well as other feature learning work ( 123 , 123 , 123 ) .
at a high - level , our system performs the following steps to learn a feature representation :
extract random patches from unlabeled training
apply a pre - processing stage to the patches .
learn a feature - mapping using an unsupervised
given the learned feature mapping and a set of labeled training images we can then perform feature extraction
extract features from equally spaced sub - patches
covering the input image .
pool features together over regions of the input
image to reduce the number of feature values .
train a linear classier to predict the labels given
the feature vectors .
we will now describe the components of this pipeline and its parameters in more detail .
123 feature learning as mentioned above , the system begins by extract - ing random sub - patches from unlabeled input images .
each patch has dimension w - by - w and has d channels , 123 with w referred to as the receptive eld size .
each w - by - w patch can be represented as a vector in rn of pixel intensity values , with n = w w d .
we then construct a dataset of m randomly sampled patches , x = ( x ( 123 ) , . . . , x ( m ) ) , where x ( i ) rn .
given this dataset , we apply the pre - processing and unsupervised
it is common practice to perform several simple nor - malization steps before attempting to generate fea - tures from data .
in this work , we assume that every patch x ( i ) is normalized by subtracting the mean and dividing by the standard deviation of its elements .
for visual data , this corresponds to local brightness and after normalizing each input vector , the entire dataset x may optionally be whitened ( 123 ) .
while this process
if the input image is represented in
( r , g , b ) colors , then it has three channels .
is commonly used in deep learning work ( e . g . , ( 123 ) ) it is less frequently employed in computer vision .
we will present experimental results obtained both with and without whitening to determine whether this compo - nent is generally necessary .
123 . 123 unsupervised learning
after pre - processing , an unsupervised learning algo - rithm is used to discover features from the unlabeled data .
for our purposes , we will view an unsupervised learning algorithm as a black box that takes the dataset x and outputs a function f : rn rk that maps an input vector x ( i ) to a new feature vector of k features , where k is a parameter of the algorithm .
we denote the kth feature as fk .
in this work , we will use several dierent unsupervised learning methods123 in this role : ( i ) sparse auto - encoders , ( ii ) sparse rbms , ( iii ) k - means clustering , and ( iv ) gaussian mixtures .
we briey summarize how these algorithms are em - ployed in our system .
sparse auto - encoder : we train an auto - encoder with k hidden nodes using back - propagation to minimize squared reconstruction error with an additional penalty term that en - courages the units to maintain a low average ac - tivation ( 123 , 123 ) .
the algorithm outputs weights w rkn and biases b rk such that the feature mapping f is dened by :
f ( x ) = g ( w x + b ) ,
where g ( z ) = 123 / ( 123 + exp ( z ) ) is the logistic sigmoid function , applied component - wise to the there are several hyper - parameters used by the training algorithm ( e . g . , weight decay , and target activation ) .
these parameters were chosen using cross - validation for each choice of the receptive eld size , w . 123
sparse restricted boltzmann machine : the restricted boltzmann machine ( rbm ) is an undi - rected graphical model with k binary hidden variables .
sparse rbms can be trained using the contrastive divergence approximation ( 123 ) with the same type of sparsity penalty as the auto - encoders .
the training also produces weights
123these algorithms were chosen since they can scale up straight - forwardly to the problem sizes considered in our
123ideally , we would perform this cross - validation for ev - ery choice of parameters , but the expense is prohibitive for the number of experiments we perform here .
this is a ma - jor advantage of the k - means algorithm , which requires no
123 an analysis of single - layer networks in unsupervised feature learning
w and biases b , and we can use the same fea - ture mapping as the auto - encoder ( as in equa - tion ( 123 ) ) thus , these algorithms dier primarily in their training method .
also as above , the nec - essary hyper - parameters are determined by cross - validation for each receptive eld size .
k - means clustering : we apply k - means clus - tering to learn k centroids c ( k ) from the input data .
given the learned centroids c ( k ) , we con - sider two choices for the feature mapping f .
the rst is the standard 123 - of - k , hard - assignment cod -
123 if k = arg minj ||c ( j ) x||123
this is a ( maximally ) sparse representation that has been used frequently in computer vision ( 123 ) .
it has been noted , however , that this may be too terse ( 123 ) .
thus our second choice of feature map - ping is a non - linear mapping that attempts to be softer than the above encoding while also keep - ing some sparsity :
fk ( x ) = max ( 123 , ( z ) zk )
where zk = ||xc ( k ) ||123 and ( z ) is the mean of the elements of z .
this activation function outputs 123 for any feature fk where the distance to the centroid c ( k ) is above average .
in practice , this means that roughly half of the features will be set to 123
this can be thought of as a very simple form of competition between features .
we refer to these in our results as k - means ( hard ) and k - means ( triangle ) respectively .
gaussian mixtures : gaussian mixture models ( gmms ) represent the density of input data as a mixture of k gaussian distributions and is widely used for clustering .
gmms can be trained using the expectation - maximization ( em ) algorithm as in ( 123 ) .
we run a single iteration of k - means to ini - tialize the mixture model . 123 the feature mapping f maps each input to the posterior membership
k ( x c ( k ) )
where k is a diagonal covariance and k are the cluster prior probabilities learned by the em al -
123when k - means is run to convergence we have found that the mixture model does not learn features substan - tially dierent from the k - means result .
123 feature extraction and classication the above steps , for a particular choice of unsuper - vised learning algorithm , yield a function f that trans - forms an input patch x rn to a new representation y = f ( x ) rk .
using this feature extractor , we now apply it to our ( labeled ) training images for classica -
123 . 123 convolutional extraction using the learned feature extractor f : rn rk , given any w - by - w image patch , we can now compute a representation y rk for that patch .
we can thus dene a ( single layer ) representation of the entire im - age by applying the function f to many sub - patches .
specically , given an image of n - by - n pixels ( with d channels ) , we dene a ( n w + 123 ) - by - ( n w + 123 ) representation ( with k channels ) , by computing the representation y for each w - by - w subpatch of the input image .
more formally , we will let y ( ij ) be the k - dimensional representation extracted from location i , j of the input image .
for computational eciency , we may also step our w - by - w feature extractor across the image with some step - size ( or stride ) s greater than 123
this is illustrated in figure 123
it is standard practice to re - duce the dimensionality of the image representation by pooling .
for a stride of s = 123 , our feature mapping produces a ( nw +123 ) - by - ( nw +123 ) - by - k representa - tion .
we can reduce this by summing up over local re - gions of the y ( ij ) s extracted as above .
this procedure is commonly used ( in many variations ) in computer vision ( 123 ) as well as deep feature learning ( 123 ) .
in our system , we use a very simple form of pooling .
specically , we split the y ( ij ) s into four equal - sized quadrants , and compute the sum of the y ( ij ) s in each .
this yields a reduced ( k - dimensional ) representation of each quadrant , for a total of 123k features that we use for classication .
given these pooled ( 123k - dimensional ) feature vectors for each training image and a label , we apply standard linear classication algorithms .
in our experiments we use ( l123 ) svm classication .
the regularization pa - rameter is determined by cross - validation .
123 experiments and analysis the above framework includes a number of parameters that can be changed : ( i ) whether to use whitening or not , ( ii ) the number of features k , ( iii ) the stride s , and ( iv ) receptive eld size w .
in this section , we present our experimental results on the impact of these parameters on performance .
first , we will evaluate the eects of these parameters using cross - validation
123 adam coates , honglak lee , andrew y
figure 123 : illustration showing feature extraction using a w - by - w receptive eld and stride s .
we rst extract w - by - w patches separated by s pixels each , then map them to k - dimensional feature vectors to form a new image representation .
these vectors are then pooled over 123 quadrants of the image to form a feature vector for classication .
( for clarity we have drawn the leftmost gure with a stride greater than w , but in practice the stride is almost always smaller than w . )
on the cifar - 123 training set .
we will then report the results achieved on both cifar - 123 and norb test sets using each unsupervised learning algorithm and the parameter settings that our analysis suggests is best overall ( i . e . , in our nal results , we use the same settings for all algorithms ) . 123 our basic testing procedure is as follows .
for each un - supervised learning algorithm in section 123 . 123 , we will train a single - layer of features using either whitened data or raw data and a choice of the parameters k , s , and w .
we then train a linear classier as described in section 123 . 123 , then test the classier on a holdout set ( for our main analysis ) or the test set ( for our nal
before we present classication results , we rst show visualizations of the learned feature representations .
the bases ( or centroids ) learned by sparse autoen - coders , sparse rbms , k - means , and gaussian mix - ture models are shown in figure 123 for 123 pixel recep - it is well - known that autoencoders and rbms yield localized lters that resemble gabor l - ters and we can see this in our results both when us - ing whitened data and , to a lesser extent , raw data .
however , these visualizations also show that similar results can be achieved using clustering algorithms .
in particular , while clustering raw data leads to cen - troids consistent with those in ( 123 ) and ( 123 ) , we see that clustering whitened data yields sharply localized lters that are very similar to those learned by the other al - gorithms .
thus , it appears that such features are easy to learn with clustering methods ( without any param - eter tweaking ) as a result of whitening .
123to clarify : the parameters used in our nal evaluation are those that achieved the best ( average ) cross - validation performance across all models : whitening , 123 pixel stride , 123 pixel receptive eld , and 123 features .
figure 123 : eect of whitening and number of bases ( or
123 eect of whitening we now move on to our characterization of perfor - mance on various axes of parameters , starting with the eect of whitening123 , which visibly changes the learned bases ( or centroids ) as seen in figure 123
figure 123 shows the performance for all of our algorithms as a function of the number of features ( which we will discuss in the next section ) both with and without whitening .
these experiments used a stride of 123 pixel and 123 pixel recep - for sparse autoencoders and rbms , the eect of whitening is somewhat ambiguous .
when using only 123 features , there is a signicant benet of whiten - ing for sparse rbms , but this advantage disappears with larger numbers of features .
for the clustering algorithms , however , we see that whitening is a cru - cial pre - process since the clustering algorithms cannot handle the correlations in the data . 123
123in our experiments , we use zero - phase whitening ( 123 ) 123our gmm implementation uses diagonal covariances
123# featurescrossvalidation accuracy ( % ) performance for raw and whitened inputs kmeans ( tri ) raw kmeans ( hard ) raw gmm raw autoencoder raw rbm raw kmeans ( tri ) white kmeans ( hard ) white gmm white autoencoder white rbm white123 an analysis of single - layer networks in unsupervised feature learning
( a ) k - means ( with and without whitening )
( b ) gmm ( with and without whitening )
( c ) sparse autoencoder ( with and without whitening )
( d ) sparse rbm ( with and without whitening )
figure 123 : randomly selected bases ( or centroids ) trained on cifar - 123 images using dierent learning algorithms .
best viewed in color .
figure 123 : eect of stride .
clustering algorithms have been applied successfully to raw pixel inputs in the past ( 123 , 123 ) but these appli - cations did not use whitened input data .
our results suggest that improved performance might be obtained by incorporating whitening .
123 number of features our experiments considered feature representations with 123 , 123 , 123 , 123 , 123 , and 123 learned fea - tures . 123 figure 123 clearly shows the eect of increasing
and k - means uses euclidean distance .
123we found that training gaussian mixture models with more than 123 components was often dicult and always extremely slow .
thus we only ran this algorithm with up to 123 components .
figure 123 : eect of receptive eld size .
the number of learned features : all algorithms gen - erally achieved higher performance by learning more features as expected .
surprisingly , k - means clustering coupled with the tri - angle activation function and whitening achieves the highest performance .
this is particularly notable since k - means requires no tuning whatsoever , unlike the sparse auto - encoder and sparse rbms which require us to choose several hyper - parameters for best results .
123 eect of stride
the stride s used in our framework is the spacing between patches where feature values will be extracted ( see figure 123 ) .
frequently , learning systems will use
123stride between extracted features ( pixels ) crossvalidation accuracy ( % ) performance vs .
feature stride kmeans ( tri ) kmeans ( hard ) autoencoder rbm123receptive field size ( pixels ) crossvalidation accuracy ( % ) performance vs .
receptive field size kmeans ( tri ) kmeans ( hard ) autoencoder rbm123 adam coates , honglak lee , andrew y
a stride s > 123 because computing the feature map - ping is very expensive .
for instance , sparse coding requires us to solve an optimization problem for each such patch , which may be prohibitive for a stride of 123
it is reasonable to ask , then , how much this compro - mise costs in terms of performance for the algorithms we consider ( which all have the property that their feature mapping can be computed extremely quickly ) .
in this experiment , we xed the number of features ( 123 ) and receptive eld size ( 123 pixels ) , and vary the stride over 123 , 123 , 123 , and 123 pixels .
the results are shown in figure 123
( we do not report results with gmms , since training models of this size was impractical . ) the plot shows a clear downward trend in performance with increasing step size as expected .
however , the magnitude of the change is striking : for even a stride of s = 123 , we suer a loss of 123% or more accuracy , and for s = 123 we lose at least 123% .
these dierences can be signicant in comparison to the choice of algo - rithm .
for instance , a sparse rbm with stride of 123 performed comparably to the simple hard - assignment k - means scheme using a stride of 123one of the sim - plest possible algorithms we could have chosen for un - supervised learning ( and certainly much simpler than a sparse rbm ) .
123 eect of receptive eld size
finally , we also evaluated the eect of receptive eld size .
given a scalable algorithm , its possible that leveraging it to learn larger receptive elds could al - low us to recognize more complex features that cover a larger region of the image .
on the other hand , this increases the dimensionality of the space that the al - gorithm must cover and may require us to learn more features or use more data .
as a result , given the same amount of data and using the same number of features , it is not clear whether this is a worthwhile investment .
in this experiment , we tested receptive eld sizes of 123 , 123 , and 123 pixels .
for other parameters , we used whitening , stride of 123 pixel , and 123 bases ( or cen - the summary results are shown in figure 123
overall , the 123 pixel receptive eld worked best .
meanwhile , 123 pixel receptive elds were similar or worse than 123 or 123 pixels .
thus , if we have computational resource to spare , our results suggest that it is better to spend it on reducing stride and expanding the number of learned unfortunately , unlike for the other parameters , the re - ceptive eld size does appear to require cross valida - tion in order to make an informed choice .
our ex - periments do suggest , though , that even very small receptive elds can work well ( with pooling ) and are worth considering .
this is especially important if re - ducing the input size allows us to use a smaller stride
table 123 : test recognition accuracy on cifar - 123 raw pixels ( reported in ( 123 ) ) 123 - way factored rbm ( 123 layers ) ( 123 ) mean - covariance rbm ( 123 layers ) ( 123 ) improved local coord .
coding ( 123 ) conv .
deep belief net ( 123 layers ) ( 123 ) k - means ( triangle , 123 features )
table 123 : test recognition accuracy ( and error ) for
neural network ( 123 ) deep boltzmann machine ( 123 ) deep belief network ( 123 ) ( best result of ( 123 ) ) deep neural network ( 123 ) k - means ( triangle , 123 features )
or more features which both have large positive impact
123 final classication results
we have shown that whitening , a stride of 123 pixel , a 123 pixel receptive eld , and a large number of fea - tures works best on average across all algorithms for cifar - 123
using these parameters we ran our full pipeline on the entire cifar - 123 training set , trained a svm classier and tested on the standard cifar - 123 test set .
our nal test results on the cifar - 123 data set with these settings are reported in table 123 along with results from other publications .
quite surpris - ingly , the k - means ( triangle ) algorithm attains very high performance ( 123% ) with 123 features .
based on this success , we sought to improve the results fur - ther simply by increasing the number of features to 123
using these features , our test accuracy increased based on our analysis here , we have also run each of these algorithms on the norb normalized uniform dataset .
we use all of the same parameters as for cifar - 123 , including the 123 pixel receptive eld size .
the results are summarized in table 123
here , all of the algorithms achieve very high performance .
again , k - means with the triangle activation achieves the highest performance .
when using 123 features as for
123 an analysis of single - layer networks in unsupervised feature learning
table 123 : test recognition accuracy on stl - 123
k - means ( triangle 123 features )
cifar , we achieve 123% accuracy .
we note , how - ever , that the other results are very similar regardless of the algorithm used .
this suggests that the main source of performance here is from our choice of net - work structure , not from the particular choice of un - supervised learning algorithm .
finally , we also ran our system on the new stl - 123 dataset123
this dataset uses higher resolution ( 123x123 ) images , but allows many fewer training examples ( 123 per class ) , while providing a large unlabeled train - ing setthus forcing algorithms to rely heavily on ac - quired prior knowledge of image statistics .
we applied the same system as used for cifar on downsampled versions of the stl images ( 123x123 pixels ) .
case , the performance is much lower than on the com - parable cifar dataset on account of the small la - beled datasets : 123% ( 123% ) ( compared to 123% ( 123% ) for raw pixels ) .
this suggests that the method proposed here is strongest when we have large labeled training sets as with norb and cifar .
our results above may seem inexplicable considering the simplicity of the systemit is not clear , on rst inspection , exactly what in our experiments allows us to achieve such high performance compared to prior work .
we believe that the main explanation for the performance gain is , in fact , our choice of network pa - rameters since almost all of the algorithms performed favorably relative to previous results .
each of the network parameters ( feature count , stride and receptive eld size ) weve tested potentially con - fers a signicant benet on performance .
for instance , large numbers of features ( regardless of how theyre trained ) gives us many non - linear projections of the data .
unlike simple linear projections , which have lim - ited representational power , it is well - known that us - ing extremely large numbers of non - linear projections can make data closer to linearly separable and thus easier to classify .
hence , larger numbers of features may be uniformly benecial , regardless of the training the dramatic impact of changes to the stride parame - ter may be partly explained by the work of boureau ( 123 ) .
by setting the stride small , a larger number of sam - ples are incorporated into each pooling area which was shown both theoretically and empirically to improve it is also likely that high - frequency features
( edges ) are more accurately identied using a dense finally , the receptive eld size , which we chose by cross - validation appears to be important as well .
appears that large receptive elds result in a space that is simply too large to cover eectively with a small number of nonlinear features .
for instance , be - cause our features often include shifted copies of edges , increasing the receptive eld size also increases the amount of redundancy we can expect in our lters .
this caveat might be ameliorated by training convo - lutionally ( 123 , 123 , 123 ) .
note that small receptive elds might also increase the number of samples used in pooling and thus have a small eect similar to using a
in this paper we have conducted extensive experiments on the cifar - 123 dataset using multiple unsupervised feature learning algorithms to characterize the eect of various parameters on classication performance .
while conrming the basic nding that more features and dense extraction are useful , we have shown more importantly that these elements can , in fact , be as im - portant as the unsupervised learning algorithm itself .
surprisingly , we have shown that even the k - means clustering algorithman extremely simple learning al - gorithm with no parameters to tuneis able to achieve state - of - the - art performance on both cifar - 123 and norb datasets when used with the network parame - ters that we have identied in this work .
weve also shown more generally that smaller stride and larger numbers of features yield monotonically improving performance , which suggests that while more complex algorithms may have greater representational power , simple but fast algorithms can be highly competitive .
