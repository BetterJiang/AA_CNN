recursive structure is commonly found in the inputs of dierent modalities such as natural scene images or natural language sentences .
discovering this recursive structure helps us to not only identify the units that an image or sentence contains but also how they interact to form a whole .
we introduce a max - margin structure prediction architecture based on re - cursive neural networks that can successfully recover such structure both in complex scene images as well as sentences .
the same algo - rithm can be used both to provide a competi - tive syntactic parser for natural language sen - tences from the penn treebank and to out - perform alternative approaches for semantic scene segmentation , annotation and classi - cation .
for segmentation and annotation our algorithm obtains a new level of state - of - the - art performance on the stanford background dataset ( 123% ) .
the features from the im - age parse tree outperform gist descriptors for scene classication by 123% .
recursive structure is commonly found in dierent modalities , as shown in fig .
the syntactic rules of natural language are known to be recursive , with noun phrases containing relative clauses that them - selves contain noun phrases , e . g . , .
the church which has nice windows .
similarly , one nds nested hier - archical structuring in scene images that capture both part - of and proximity relationships .
for instance , cars are often on top of street regions .
a large car region
appearing in proceedings of the 123 th international con - ference on machine learning , bellevue , wa , usa , 123
copyright 123 by the author ( s ) / owner ( s ) .
figure 123
illustration of our recursive neural network ar - chitecture which parses images and natural language sen - tences .
segment features and word indices ( orange ) are rst mapped into semantic feature space ( blue ) and then recursively merged by the same neural network until they represent the entire image or sentence .
both mappings and mergings are learned .
can be recursively split into smaller car regions depict - ing parts such as tires and windows and these parts can occur in other contexts such as beneath airplanes or in houses .
we show that recovering this structure helps in understanding and classifying scene images .
in this paper , we introduce recursive neural networks ( rnns ) for predicting recursive structure in multiple modali - ties .
we primarily focus on scene understanding , a central task in computer vision often subdivided into segmentation , annotation and classication of scene images .
we show that our algorithm is a general tool
parsing natural scenes and natural language with recursive neural networks
for predicting tree structures by also using it to parse natural language sentences .
123 outlines our approach for both modalities .
im - ages are oversegmented into small regions which of - ten represent parts of objects or background .
from these regions we extract vision features and then map these features into a semantic space using a neu - ral network .
using these semantic region representa - tions as input , our rnn computes ( i ) a score that is higher when neighboring regions should be merged into a larger region , ( ii ) a new semantic feature rep - resentation for this larger region , and ( iii ) its class la - bel .
class labels in images are visual object categories such as building or street .
the model is trained so that the score is high when neighboring regions have the same class label .
after regions with the same ob - ject label are merged , neighboring objects are merged to form the full scene image .
these merging decisions implicitly dene a tree structure in which each node has associated with it the rnn outputs ( i ) - ( iii ) , and higher nodes represent increasingly larger elements of
the same algorithm is used to parse natural language sentences .
again , words are rst mapped into a se - mantic space and then they are merged into phrases in a syntactically and semantically meaningful order .
the rnn computes the same three outputs and at - taches them to each node in the parse tree .
the class labels are phrase types such as noun phrase ( np ) or verb phrase ( vp ) .
this is the rst deep learning method to achieve state - of - the - art results on segmen - tation and annotation of complex scenes .
our recur - sive neural network architecture predicts hierarchical tree structures for scene images and outperforms other methods that are based on conditional random elds or combinations of other methods .
for scene classi - cation , our learned features outperform state of the art methods such as gist descriptors .
furthermore , our algorithm is general in nature and can also parse natural language sentences obtaining competitive per - formance on maximum length 123 sentences of the wall street journal dataset .
code for the rnn model is available at www . socher . org .
related work five key research areas inuence and motivate our method .
we briey outline connections and dierences between them .
due to space constraints , we cannot do justice to the complete literature .
scene understanding has become a central task in computer vision .
the goal is to understand what ob -
jects are in a scene ( annotation ) , where the objects are located ( segmentation ) and what general scene type the image shows ( classication ) .
some meth - ods for this task such as ( aude & torralba , 123; schmid , 123 ) rely on a global descriptor which can do very well for classifying scenes into broad cat - egories .
however , these approaches fail to gain a deeper understanding of the objects in the scene .
at the same time , there is a myriad of dierent approaches for image annotation and semantic seg - mentation of objects into regions ( rabinovich et al . , 123; gupta & davis , 123 ) .
recently , these ideas have been combined to provide more detailed scene understanding ( hoiem et al . , 123; li et al . , 123; gould et al . , 123; socher & fei - fei , 123 ) .
our algorithm parses an image; that is , it recursively merges pairs of segments into super segments in a se - mantically and structurally coherent way .
many other scene understanding approaches only consider a at set of regions .
some approaches such as ( gould et al . , 123 ) also consider merging operations .
for merged super segments , they compute new features .
in con - trast , our rnn - based method learns a representation for super segments .
this learned representation to - gether with simple logistic regression outperforms the original vision features and complex conditional ran - dom eld models .
furthermore , we show that the im - age parse trees are useful for scene classication and outperform global scene features such as gist descrip - tors ( aude & torralba , 123 ) .
syntactic parsing of natural language sentences is a central task in natural language processing ( nlp ) because of its importance in mediating between lin - guistic expression and meaning .
our rnn architec - ture jointly learns how to parse and how to represent phrases in a continuous vector space of features .
this allows us to embed both single lexical units and un - seen , variable - sized phrases in a syntactically coher - ent order .
the learned feature representations cap - ture syntactic and compositional - semantic informa - tion .
we show that they can help inform accurate parsing decisions and capture interesting similarities between phrases and sentences .
using nlp techniques in computer vision .
the connection between nlp ideas such as parsing or grammars and computer vision has been explored be - fore ( zhu & mumford , 123; tighe & lazebnik , 123; zhu et al . , 123; siskind et al . , 123 ) , among many others .
our approach is similar on a high level , how - ever , more general in nature .
we show that the same neural network based architecture can be used for both natural language and image parsing .
parsing natural scenes and natural language with recursive neural networks
images which are useful
deep learning in vision applications nd lower dimensional representations for xed size ( hinton & salakhutdinov , 123 ) .
recently , lee et al .
( 123 ) were able to scale up deep networks to more realistic image sizes .
using images of single objects which were all in roughly the same scale , they were able to learn parts and classify the images into ob - ject categories .
our approach diers in several funda - mental ways to any previous deep learning algorithm .
( i ) instead of learning features from raw , or whitened pixels , we use o - the - shelf vision features of segments obtained from oversegmented full scene images .
instead of building a hierarchy using a combination of convolutional and max - pooling layers , we recursively apply the same network to merged segments and give each of these a semantic category label .
( iii ) this is the rst deep learning work which learns full scene seg - mentation , annotation and classication .
the objects and scenes vary in scale , viewpoint , lighting etc .
using deep learning for nlp applications has been investigated by several people ( inter alia bengio et al . , 123; henderson , 123; collobert & weston , 123 ) .
in most cases , the inputs to the neural networks are mod - ied to be of equal size either via convolutional and max - pooling layers or looking only at a xed size win - dow around a specic word .
our approach is dierent in that it handles variable sized sentences in a nat - ural way and captures the recursive nature of natu - ral language .
furthermore , it jointly learns parsing decisions , categories for each phrase and phrase fea - ture embeddings which capture the semantics of their constituents .
in ( socher et al . , 123 ) we developed an nlp specic parsing algorithm based on rnns .
that algorithm is a special case of the one developed in this
mapping segments and words into
this section contains an explanation of the inputs used to describe scene images and natural language sen - tences and how they are mapped into the space in which the rnn operates .
input representation of scene images closely follow the procedure described in ( gould et al . , 123 ) to compute image features .
first , we oversegment an image x into superpix - els ( also called segments ) using the algorithm from ( comaniciu & meer , 123 ) .
instead of computing multiple oversegmentations , we only choose one set of parameters .
in our dataset , this results in an av - erage of 123 segments per image .
we compute 123
features for the segments as described in sec .
123 of ( gould et al . , 123 ) .
these features include color and texture features ( shotton et al . , 123 ) , boosted pixel classier scores ( trained on the labeled training data ) , as well as appearance and shape features .
next , we use a simple neural network layer to map these features into the semantic n - dimensional space in which the rnn operates .
let fi be the features described above for each segment i = 123; : : : ; nsegs in an image .
we then compute the representation :
ai = f ( w semfi + bsem ) ;
where w sem 123 rn ( cid : 123 ) 123 is the matrix of parameters we want to learn , bsem is the bias and f is applied element - wise and can be any sigmoid - like function .
in our vision experiments , we use the original sigmoid function f ( x ) = 123= ( 123 + e
input representation for natural
to eciently use neural networks
language models ( bengio et al . , 123; collobert & weston , 123 ) map words to a vector rep - resentation .
these representations are stored in a word embedding matrix l 123 rn ( cid : 123 ) jv j , where jv j is the size of the vocabulary and n is the dimensionality of the semantic space .
this matrix usually captures co - occurrence statistics and its values are learned .
as - sume we are given an ordered list of nwords words from a sentence x .
each word i = 123; : : : ; nwords has an associated vocabulary index k into the columns of the embedding matrix .
the operation to retrieve the i - th words semantic representation can be seen as a simple projection layer where we use a binary vector ek which is zero in all positions except at the k - th index ,
ai = lek 123 rn :
as with the image segments , the inputs are now mapped to the semantic space of the rnn .
recursive neural networks for
in our discriminative parsing architecture , the goal is to learn a function f : x ! y , where y is the set of all possible binary parse trees .
an input x consists of two parts : ( i ) a set of activation vectors fa123; : : : ; ansegs which represent input elements such as image segments or words of a sentence .
( ii ) a symmetric adjacency matrix a , where a ( i; j ) = 123 , if segment i neighbors j .
this matrix denes which elements can be merged .
for sentences , this matrix has a special form with 123s only on the rst diagonal below and above the main
parsing natural scenes and natural language with recursive neural networks
f ( cid : 123 ) ( x ) = arg max
s ( rnn ( ( cid : 123 ) ; x; y ) ) ;
where ( cid : 123 ) are all the parameters needed to compute a score s with an rnn .
the score of a tree y is high if the algorithm is condent that the structure of the tree is correct .
tree scoring with rnns will be explained in detail below .
in the max - margin estimation frame - work ( taskar et al . , 123; ratli et al . , 123 ) , we want to ensure that the highest scoring tree is in the set
of correct trees : f ( cid : 123 ) ( xi ) 123 y ( xi; li ) for all training in - stances ( xi; li ) , i = 123; : : : ; n .
furthermore , we want the score of the highest scoring correct tree yi to be larger up to a margin dened by the loss .
123i; y 123 t ( xi ) : s ( rnn ( ( cid : 123 ) ; xi; yi ) ) ( cid : 123 ) s ( rnn ( ( cid : 123 ) ; xi; y ) ) + ( xi; li; y ) :
these desiderata lead us to the following regularized
ri ( ( cid : 123 ) ) = max
s ( rnn ( ( cid : 123 ) ; xi; yi ) )
s ( rnn ( ( cid : 123 ) ; xi; y ) ) + ( xi; li; y )
minimizing this objective maximizes the correct trees score and minimizes ( up to a margin ) the score of the highest scoring but incorrect tree .
now that we dened the general learning framework , we will explain in detail how we predict parse trees and compute their scores with rnns .
greedy structure predicting rnns
tivation vectors fa123; : : : ; ansegs
we can now describe the rnn model that uses the ac - g and adjacency matrix a ( as dened above ) as inputs .
there are more than exponentially many possible parse trees and no e - cient dynamic programming algorithms for our rnn setting .
therefore , we nd a greedy approximation .
we start by explaining the feed - forward process on a
using the adjacency matrix a , the algorithm nds the pairs of neighboring segments and adds their activa - tions to a set of potential child node pairs :
c = f ( ai; aj ) : a ( i , j ) =123g :
in the small toy image of fig .
123 , we would have the fol -
lowing pairs : f ( a123; a123 ) ; ( a123; a123 ) ; ( a123; a123 ) ; ( a123; a123 ) ; ( a123; a123 ) ; ( a123; a123 ) ; ( a123; a123 ) ; ( a123; a123 ) ; ( a123; a123 ) ; ( a123; a123 ) g .
each pair of
activations is concatenated and given as input to a
figure 123
illustration of the rnn training inputs : an ad - jacency matrix of image segments or words .
a training image ( red and blue are dierently labeled regions ) denes a set of correct trees which is oblivious to the order in which segments with the same label are merged .
see text
we denote the set of all possible trees that can be constructed from an input x as t ( x ) .
when training the visual parser , we have labels l for all segments .
using these labels , we can dene an equivalence set of correct trees y ( x; l ) .
a visual tree is correct if all adja - cent segments that belong to the same class are merged into one super segment before merges occur with super segments of dierent classes .
this equivalence class over trees is oblivious to how object parts are inter - nally merged or how complete , neighboring objects are merged into the full scene image .
for training the lan - guage parser , the set of correct trees only has one el - ement , the annotated ground truth tree : y ( x ) = fyg .
123 illustrates this .
max - margin estimation
similar to ( taskar et al . , 123 ) , we dene a structured margin loss ( x; l; y ) for proposing a parse y for in - put x with labels l .
the loss increases when a seg - ment merges with another one of a dierent label be - fore merging with all its neighbors of the same label .
we can formulate this by checking whether the sub - tree subt ree ( d ) underneath a nonterminal node d in y appears in any of the ground truth trees of y ( x; l ) :
( x; l; y ) = ( cid : 123 )
123fsubt ree ( d ) =123 y ( x; l ) g;
where n ( y ) is the set of non - terminal nodes and ( cid : 123 ) is a parameter .
the loss of the language parser is the sum over incorrect spans in the tree , see ( manning & schutze , 123 ) .
given the training set , we search for a function f with small expected loss on unseen inputs .
we consider the
parsing natural scenes and natural language with recursive neural networks
s = w scorep p = f ( w ( c123; c123 ) + b )
figure 123
one recursive neural network which is replicated for each pair of possible input vectors .
this network is dierent to the original rnn formulation in that it predicts a score for being a correct merging decision .
neural network .
the network computes the potential parent representation for these possible child nodes :
p ( i;j ) = f ( w ( ci; cj ) + b ) :
with this representation we can compute a local score using a simple inner product with a row vector w score 123 r123 ( cid : 123 ) n :
s ( i;j ) = w scorep ( i;j ) :
the network performing these functions is illustrated in fig .
training will aim to increase scores of good segment pairs ( with the same label ) and decrease scores of pairs with dierent labels , unless no more good pairs are left .
after computing the scores for all pairs of neighboring segments , the algorithm selects the pair which received the highest score .
let the score sij be the highest score; we then ( i ) remove ( ai; aj ) from c , as well as all other pairs with either ai or aj in them .
( ii ) update the adjacency matrix with a new row and column that reects that the new segment has the neighbors of both child segments .
( iii ) add potential new child pairs to
c = c ( cid : 123 ) f ( ai; aj ) g ( cid : 123 ) f ( aj; ai ) g c = c ( f ( p ( i;j ) ; ak ) : ak has boundary with i or jg in the case of the image in fig .
123 , if we merge ( a123; a123 ) , then c = f ( a123; a123 ) ; ( a123; a123 ) ; ( a123; a123 ) ; ( a123; p ( 123;123 ) ) ; ( a123; a123 ) ; ( a123; p ( 123;123 ) ) ; ( p ( 123;123 ) ; a123 ) ; ( p ( 123;123 ) ; a123 ) g .
the new potential parents and corresponding scores of new child pairs are computed with the same neural net - work of eq .
for instance , we compute , p ( 123; ( 123;123 ) ) = f ( w ( a123; p ( 123;123 ) ) +b ) ; p ( 123; ( 123;123 ) ) = f ( w ( a123; p ( 123;123 ) ) +b ) ; etc .
the process repeats ( treating the new pi;j just like any other segment ) until all pairs are merged and only one parent activation is left in the set c .
this activation then represents the entire image .
hence , the same network ( with parameters w; b; w score ) is recursively applied until all vector pairs are collapsed .
the tree is then recovered by unfolding the collapsed decisions down to the original segments which are the leaf nodes
of the tree .
the nal score that we need for structure prediction is simply the sum of all the local decisions :
s ( rnn ( ( cid : 123 ) ; xi; y ) ) =
to nish the example , assume the next highest score was s ( ( 123;123 ) ;123 ) , so we merge the ( 123; 123 ) super segment with segment 123 , so c = f ( a123; a123 ) ; ( a123; p ( ( 123;123 ) ;123 ) ) ; ( a123; a123 ) ; ( a123; p ( ( 123;123 ) ;123 ) ) ; ( p ( ( 123;123 ) ;123 ) ; a123 ) ; ( p ( ( 123;123 ) ;123 ) ; a123 ) g .
if we then merge segments ( 123; 123 ) , we get c = f ( p ( 123;123 ) ; p ( ( 123;123 ) ;123 ) ) ; ( p ( ( 123;123 ) ;123 ) ; p ( 123;123 ) ) g , leaving us with only the last choice
of merging the dierently labeled super segments .
this results in the bottom tree in fig
category classiers in the tree
one of the main advantages of our approach is that each node of the tree built by the rnn has associated with it a distributed feature representation ( the par - ent vector p ) .
we can leverage this representation by adding to each rnn parent node ( after removing the scoring layer ) a simple softmax layer to predict class labels , such as visual or syntactic categories :
labelp = sof tmax ( w labelp ) :
when minimizing the cross - entropy error of this soft - max layer , the error will backpropagate and inuence both the rnn parameters and the word representa -
improvements for language parsing
since in a sentence each word only has 123 neighbors , less - greedy search algorithms such as a bottom - up beam search can be used .
in our case , beam search lls in elements of the chart in a similar fashion as the cky algorithm .
however , unlike standard cnf gram - mars , in our grammar each constituent is represented by a continuous feature vector and not just a discrete category .
hence we cannot prune based on category equality .
we could keep the k - best subtrees in each cell but initial tests showed no improvement over just keeping the single best constituent in each cell .
since there is only a single correct tree the second max - imization in the objective of eq .
123 can be dropped .
for further details see ( socher et al . , 123 ) .
our objective j of eq .
123 is not dierentiable due to the hinge loss .
therefore , we will generalize gradi - ent descent via the subgradient method ( ratli et al . , 123 ) which computes a gradient - like direction called the subgradient .
let ( cid : 123 ) = ( w sem; w; w score; w label ) be the set of our model parameters , 123 then the gradi -
123in the case of natural language parsing , w semis re -
placed by the look - up table l .
parsing natural scenes and natural language with recursive neural networks
table 123
pixel level multi - class segmentation accuracy of other methods and our proposed rnn architecture on the stanford background dataset .
tl ( 123 ) methods are re - ported in ( tighe & lazebnik , 123 ) .
where s ( yi ) = s ( rnn ( ( cid : 123 ) ; xi; ymax ( t ( xi ) ) ) ) and s ( yi ) = s ( rnn ( ( cid : 123 ) ; xi; ymax ( y ( xi;li ) ) ) ) .
in order to compute eq .
123 we calculate the derivative by using backprop - agation through structure ( goller & kuchler , 123 ) , a simple modication to general backpropagation where error messages are split at each node and then propa - gated to the children .
we use l - bfgs over the complete training data to minimize the objective .
generally , this could cause problems due to the non - dierentiable objective func - tion .
however , we did not observe problems in prac -
we evaluate our rnn architecture on both vision and nlp tasks .
the only parameters to tune are n , the size of the hidden layer; ( cid : 123 ) , the penalization term for in - correct parsing decisions and ( cid : 123 ) , the regularization pa - rameter .
we found that our method is robust to these parameters , varying in performance by only a few per - cent for some parameter combinations .
with proper regularization , training accuracy was highly correlated with test performance .
we chose n = 123 , ( cid : 123 ) = 123 : 123 and ( cid : 123 ) = 123 : 123
scene understanding : segmentation and
the vision experiments are performed on the stan - ford background dataset123
we rst provide accuracy of multiclass segmentation where each pixel is labeled with a semantic class .
like ( gould et al . , 123 ) , we run ve - fold cross validation and report pixel level ac - curacy in table 123
after training the full rnn model which inuences the leaf embeddings through back - propagation , we can simply label the superpixels by their most likely class based on the multinomial dis - tribution from the softmax layer at the leaf nodes .
as shown in table 123 , we outperform previous methods that report results on this data , such as the recent methods of ( tighe & lazebnik , 123 ) .
we report ac - curacy of an additional logistic regression baseline to show the improvement using the neural network layer instead of the raw vision features .
we also tried us - ing just a single neural network layer followed by a softmax layer .
this corresponds to the leaf nodes of the rnn and performed about 123% worse than the full
123the dataset is available at
method and semantic pixel accuracy in
pixel crf , gould et al . ( 123 ) log .
on superpixel features region - based energy , gould et al . ( 123 )
rnn ( our method )
figure 123
results of multi - class image segmentation and pixel - wise labeling with recursive neural networks .
best viewed in color .
on a 123ghz laptop our matlab implementation needs 123 seconds to parse 123 test images .
we show seg - mented and labeled scenes in fig
scene classication
the stanford background dataset can be roughly cat - egorized into three scene types : city , countryside and sea - side .
we label the images with these three la - bels and train a linear svm using the average over all nodes activations in the tree as features .
hence , we use the entire parse tree and the learned feature repre -
parsing natural scenes and natural language with recursive neural networks
center phrase and nearest neighbors
all the gures are adjusted for seasonal variations 123
all the numbers are adjusted for seasonal uctuations 123
all the gures are adjusted to remove usual seasonal 123
all nasdaq industry indexes nished lower , with - nancial issues hit the hardest
knight - ridder would nt comment on the oer 123
harsco declined to say what country placed the order 123
coastal would nt disclose the terms 123
censorship is nt a marxist invention
sales grew almost 123% to $unk m .
from $unk m .
sales rose more than 123% to $123 m .
from $123 m .
sales surged 123% to unk b .
yen from unk b .
revenues declined 123% to $123 b .
from$ 123 b .
fujisawa gained 123 to unk 123
mead gained 123 to 123 unk 123
ogden gained 123 unk to 123 123
kellogg surged 123 unk to 123
the dollar dropped 123
the dollar retreated 123
the dollar gained 123
bond prices rallied
figure 123
nearest neighbors phrase trees .
the learned fea - ture representations of higher level nodes capture interest - ing syntactic and semantic similarities between the phrases .
supervised parsing
in all experiments our word and phrase representations are 123 - dimensional .
we train all models on the wall street journal section of the penn treebank using the standard training ( 123 ) , development ( 123 ) and test
the nal unlabeled bracketing f - measure ( manning & schutze , 123 ) for details ) of our lan - guage parser is 123% , compared to 123% for the widely used berkeley parser ( petrov et al . , 123 ) ( de - velopment f123 is virtually identical with 123% for the rnn and 123% for the berkeley parser ) .
unlike most previous systems , our parser does not provide a parent with information about the syntactic cate - gories of its children .
this shows that our learned , continuous representations capture enough syntactic information to make good parsing decisions .
while our parser does not yet perform as well as the current version of the berkeley parser , it performs re - spectably ( 123% dierence in unlabeled f123 ) .
on a 123ghz laptop our matlab implementation needs 123 seconds to parse 123 sentences of length less than 123
nearest neighbor phrases
in the same way we collected nearest neighbors for nodes in the scene tree , we can compute nearest neigh - bor embeddings of multi - word phrases .
we embed complete sentences from the wsj dataset into the
figure 123
nearest neighbor image region trees ( of the rst region in each row ) : the learned feature representations of higher level nodes capture interesting visual and semantic properties of the merged segments below them .
sentations of the rnn .
with an accuracy of 123% , we outperform the state - of - the art features for scene cate - gorization , gist descriptors ( aude & torralba , 123 ) , which obtain only 123% .
we also compute a baseline using our rnn .
in the baseline we use as features only the very top node of the scene parse tree .
we note that while this captures enough information to perform well above a random baseline ( 123% vs .
123% ) , it does lose some information that is captured by averaging all tree nodes .
nearest neighbor scene subtrees
in order to show that the learned feature representa - tions capture important appearance and label infor - mation even for higher nodes in the tree , we visualize nearest neighbor super segments .
we parse all test images with the trained rnn .
we then nd subtrees whose nodes have all been assigned the same class la - bel by our algorithm and save the top nodes vector representation of that subtree .
this also includes ini - tial superpixels .
using this representation , we com - pute nearest neighbors across all images and all such subtrees ( ignoring their labels ) .
123 shows the re - sults .
the rst image is a random subtrees top node and the remaining regions are the closest subtrees in the dataset in terms of euclidean distance between the
parsing natural scenes and natural language with recursive neural networks
syntactico - semantic feature space .
in fig .
123 we show several example sentences which had similar sentences in the dataset .
our examples show that the learned features capture several interesting semantic and syn - tactic similarities between sentences or phrases .
hinton , g .
and salakhutdinov , r .
reducing the dimensionality of data with neural networks .
science ,
hoiem , d . , efros , a . a . , and hebert , m .
putting objects
in perspective .
cvpr , 123
we have introduced a recursive neural network ar - chitecture which can successfully merge image seg - ments or natural language words based on deep learned semantic transformations of their original features .
our method outperforms state - of - the - art approaches in segmentation , annotation and scene classication .
we gratefully acknowledge the support of the defense advanced research projects agency ( darpa ) machine reading program under air force research laboratory ( afrl ) prime contract no .
fa123 - 123 - c - 123 and the darpa deep learning program under contract number fa123 - 123 - c - 123
any opinions , ndings , and conclusion or recommendations expressed in this material are those of the author ( s ) and do not necessarily reect the view of darpa , afrl , or the us government .
we would like to thank tianshi gao for helping us with the feature com - putation , as well as jiquan ngiam and quoc le for many
