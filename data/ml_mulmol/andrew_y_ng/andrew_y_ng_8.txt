abstract we consider the problem of estimating detailed image of an unstructured 123 - d structure from a single still environment .
our goal is to create 123 - d models which are both quantitatively accurate as well as visually pleasing .
for each small homogeneous patch in the image , we use a markov random field ( mrf ) to infer a set of plane parame - ters that capture both the 123 - d location and 123 - d orientation of the patch .
the mrf , trained via supervised learning , models both image depth cues as well as the relationships between different parts of the image .
other than assuming that the environment is made up of a number of small planes , our model makes no explicit assumptions about the structure of the scene; this enables the algorithm to capture much more detailed 123 - d structure than does prior art , and also give a much richer experience in the 123 - d ythroughs created using image - based rendering , even for scenes with signicant non - vertical structure .
using this approach , we have created qualitatively correct 123 - d models for 123% of 123 images downloaded from the internet .
we have also extended our model to produce large scale 123d models from a few images . 123
index terms machine learning , monocular vision , learning depth , vision and scene understanding , scene analysis : depth
upon seeing an image such as fig .
123a , a human has no difculty understanding its 123 - d structure ( fig .
however , inferring such 123 - d structure remains extremely challenging for current computer vision systems .
indeed , in a narrow mathematical sense , it is impossible to recover 123 - d depth from a single image , since we can never know if it is a picture of a painting ( in which case the depth is at ) or if it is a picture of an actual 123 - d environment .
yet in practice people perceive depth remarkably well given just one image; we would like our computers to have a similar sense of depths in a scene .
understanding 123 - d structure is a fundamental problem of computer vision .
for the specic problem of 123 - d reconstruction , most prior work has focused on stereovision ( 123 ) , structure from motion ( 123 ) , and other methods that require two ( or more ) images .
these geometric algorithms rely on triangulation to estimate depths .
however , algorithms relying only on geometry often end up ignoring the numerous additional monocular cues that can also be used to obtain rich 123 - d information .
in recent work , ( 123 ) ( 123 ) exploited some of these cues to obtain some 123 - d information .
saxena , chung and ng ( 123 ) presented an algorithm for predicting depths from monocular image features .
( 123 ) used monocular depth perception to drive a remote - controlled car autonomously .
( 123 ) , ( 123 ) built models using a strong assumptions that the scene consists of ground / horizontal planes and vertical walls ( and possibly sky ) ;
ashutosh saxena , min sun and andrew y .
ng are with computer science department , stanford university , stanford , ca 123
email :
123parts of this work were presented in ( 123 ) , ( 123 ) and ( 123 ) .
( a ) an original image .
( b ) oversegmentation of the image to obtain superpixels .
( c ) the 123 - d model predicted by the algorithm .
( d ) a screenshot of the textured 123 - d model .
these methods therefore do not apply to the many scenes that are not made up only of vertical surfaces standing on a horizontal oor .
some examples include images of mountains , trees ( e . g . , fig .
123b and 123d ) , staircases ( e . g . , fig .
123a ) , arches ( e . g . , fig .
123a and 123k ) , rooftops ( e . g . , fig .
123m ) , etc .
that often have much richer 123 - d structure .
in this paper , our goal is to infer 123 - d models that are both quantitatively accurate as well as visually pleasing .
we use the insight that most 123 - d scenes can be segmented into many small , approximately planar surfaces .
( indeed , modern computer graphics using opengl or directx models extremely complex scenes this way , using triangular facets to model even very complex shapes . ) our algorithm begins by taking an image , and attempting to segment it into many such small planar surfaces .
using a superpixel segmentation algorithm , ( 123 ) we nd an over - segmentation of the image that divides it into many small regions ( superpixels ) .
an example of such a segmentation is shown in fig .
because we use an over - segmentation , planar surfaces in the world may be broken up into many superpixels; however , each superpixel is likely to ( at least approximately ) lie entirely on only one planar surface .
for each superpixel , our algorithm then tries to infer the 123 - d position and orientation of the 123 - d surface that it came from .
this 123 - d surface is not restricted to just vertical and horizontal directions , but can be oriented in any direction .
inferring 123 - d position from a single image is non - trivial , and humans do it using many different visual depth cues , such as texture ( e . g . , grass has a very different texture when viewed close up than when viewed far away ) ; color ( e . g . , green patches are more likely to be grass on
the ground; blue patches are more likely to be sky ) .
our algorithm uses supervised learning to learn how different visual cues like these are associated with different depths .
our learning algorithm uses a markov random eld model , which is also able to take into account constraints on the relative depths of nearby superpixels .
for example , it recognizes that two adjacent image patches are more likely to be at the same depth , or to be even co - planar , than being very far apart .
having inferred the 123 - d position of each superpixel , we can now build a 123 - d mesh model of a scene ( fig .
we then texture - map the original image onto it to build a textured 123 - d model ( fig .
123d ) that we can y through and view at different angles .
other than assuming that the 123 - d structure is made up of a number of small planes , we make no explicit assumptions about the structure of the scene .
this allows our approach to generalize well , even to scenes with signicantly richer structure than only vertical surfaces standing on a horizontal ground , such as moun - tains , trees , etc .
our algorithm was able to automatically infer 123 - d models that were both qualitatively correct and visually pleasing for 123% of 123 test images downloaded from the internet .
we further show that our algorithm predicts quantitatively more accurate depths than both previous work .
extending these ideas , we also consider the problem of creating 123 - d models of large novel environments , given only a small , sparse , set of images .
in this setting , some parts of the scene may be visible in multiple images , so that triangulation cues ( structure from motion ) can be used to help reconstruct them; but larger parts of the scene may be visible only in one image .
we extend our model to seamlessly combine triangulation cues and monocular image cues .
this allows us to build full , photo - realistic 123 - d models of larger scenes .
finally , we also demonstrate how we can incorporate object recognition information into our model .
for example , if we detect a standing person , we know that people usually stand on the oor and thus their feet must be at ground - level .
knowing approximately how tall people are also helps us to infer their depth ( distance ) from the camera; for example , a person who is 123 pixels tall in the image is likely about twice as far as one who is 123 pixels tall .
( this is also reminiscent of ( 123 ) , who used a car and pedestrian detector and the known size of cars / pedestrians to estimate the position of the
the rest of this paper is organized as follows .
section ii discusses the prior work .
section iii describes the intuitions we draw from human vision .
section iv describes the representation we choose for the 123 - d model .
section v describes our probabilistic models , and section vi describes the features used .
section vii describes the experiments we performed to test our models .
section viii extends our model to the case of building large 123 - d models from sparse views .
section ix demonstrates how information from object recognizers can be incorporated into our models for 123 - d reconstruction , and section x concludes .
prior work
for a few specic settings , several authors have developed methods for depth estimation from a single image .
examples in - clude shape - from - shading ( 123 ) , ( 123 ) and shape - from - texture ( 123 ) , ( 123 ) ; however , these methods are difcult to apply to surfaces that do not have fairly uniform color and texture .
nagai et al .
( 123 ) used hidden markov models to performing surface reconstruction from single images for known , xed objects such as hands and
hassner and basri ( 123 ) used an example - based approach to estimate depth of an object from a known object class .
han and zhu ( 123 ) performed 123 - d reconstruction for known specic classes of objects placed in untextured areas .
criminisi , reid and zisserman ( 123 ) provided an interactive method for computing 123 - d geometry , where the user can specify the object segmentation , 123 - d coordinates of some points , and reference height of an object .
torralba and oliva ( 123 ) studied the relationship between the fourier spectrum of an image and its mean depth .
in recent work , saxena , chung and ng ( scn ) ( 123 ) , ( 123 ) presented an algorithm for predicting depth from monocular image features; this algorithm was also successfully applied for improving the performance of stereovision ( 123 ) .
michels , saxena and ng ( 123 ) also used monocular depth perception and reinforce - learning to drive a remote - controlled car autonomously in unstructured environments .
delage , lee and ng ( dln ) ( 123 ) , ( 123 ) and hoiem , efros and hebert ( heh ) ( 123 ) assumed that the environment is made of a at ground with vertical walls .
dln considered indoor images , while heh considered outdoor scenes .
they classied the image into horizontal / ground and vertical regions ( also possibly sky ) to produce a simple pop - up type y - through from an image .
our approach uses a markov random field ( mrf ) to model monocular cues and the relations between various parts of the image .
mrfs are a workhorse of machine learning , and have been applied to various problems in which local features were insufcient and more contextual information had to be used .
examples include stereovision ( 123 ) , ( 123 ) , image segmentation ( 123 ) , and object classication ( 123 ) .
there is also ample prior work in 123 - d reconstruction from multiple images , as in stereovision and structure from motion .
it is impossible for us to do this literature justice here , but recent surveys include ( 123 ) and ( 123 ) , and we discuss this work further in
visual cues for scene understanding
images are formed by a projection of the 123 - d scene onto two dimensions .
thus , given only a single image , the true 123 - d structure is ambiguous , in that an image might represent an innite number of 123 - d structures .
however , not all of these possible 123 - d structures are equally likely .
the environment we live in is reasonably structured , and thus humans are usually able to infer a ( nearly ) correct 123 - d structure , using prior experience .
given a single image , humans use a variety of monocular cues to infer the 123 - d structure of the scene .
some of these cues are based on local properties of the image , such as texture variations and gradients , color , haze , and defocus ( 123 ) , ( 123 ) , ( 123 ) .
the texture of surfaces appears different when viewed at different distances or orientations .
a tiled oor with parallel lines will also appear to have tilted lines in an image , such that distant regions will have larger variations in the line orientations , and nearby regions will have smaller variations in line orientations .
similarly , a grass eld when viewed at different orientations / distances will appear different .
we will capture some of these cues in our model .
however , we note that local image cues alone are usually insufcient to infer the 123 - d structure .
for example , both blue sky and a blue object would give similar local features; hence it is difcult to estimate depths from local features
( left ) an image of a scene .
( right ) oversegmented image .
each small segment ( superpixel ) lies on a plane in the 123d world .
( best viewed in
the ability of humans to integrate information over space , i . e .
understand the relation between different parts of the image , is crucial to understanding the scenes 123 - d structure .
( 123 , chap .
123 ) for example , even if part of an image is a homogeneous , featureless , gray patch , one is often able to infer its depth by looking at nearby portions of the image , so as to recognize whether this patch is part of a sidewalk , a wall , etc .
therefore , in our model we will also capture relations between different parts of the image .
humans recognize many visual cues , such that a particular shape may be a building , that the sky is blue , that grass is green , that trees grow above the ground and have leaves on top of them , and so on .
in our model , both the relation of monocular cues to the 123 - d structure , as well as relations between various parts of the image , will be learned using supervised learning .
specically , our model will be trained to estimate depths using a training set in which the ground - truth depths were collected using a laser
a 123 - d illustration to explain the plane parameter and rays r from
our goal is to create a full photo - realistic 123 - d model from an image .
following most work on 123 - d models in computer graphics and other related elds , we will use a polygonal mesh representation of the 123 - d model , in which we assume the world is made of a set of small planes . 123 in detail , given an image of the scene , we rst nd small homogeneous regions in the image , called superpixels ( 123 ) .
each such region represents a coherent region in the scene with all the pixels having similar properties .
( see fig .
123 ) our basic unit of representation will be these small planes in the world , and our goal is to infer the location and orientation of each one .
123this assumption is reasonably accurate for most articial structures , such as buildings .
some natural structures such as trees could perhaps be better represented by a cylinder .
however , since our models are quite detailed , e . g . , about 123 planes for a small scene , the planar assumption works quite well
( left ) original image .
( right ) superpixels overlaid with an illustration of the markov random field ( mrf ) .
the mrf models the relations ( shown by the edges ) between neighboring superpixels .
( only a subset of nodes and
more formally , we parametrize both the 123 - d location and orientation of the innite plane on which a superpixel lies by using a set of plane parameters r123
123 ) ( any point q r123 lying on the plane with parameters satises t q = 123 ) the value 123 / || is the distance from the camera center to the closest point on the plane , and the normal vector = the orientation of the plane .
if ri is the unit vector ( also called the ray ri ) from the camera center to a point i lying on a plane with parameters , then di = 123 / rt i is the distance of point i from the camera center .
probabilistic model
it is difcult to infer 123 - d information of a region from local cues alone ( see section iii ) , and one needs to infer the 123 - d information of a region in relation to the 123 - d information of other regions .
in our mrf model , we try to capture the following properties
of the images :
image features and depth : the image features of a super - pixel bear some relation to the depth ( and orientation ) of the
connected structure : except in case of occlusion , neigh - boring superpixels are more likely to be connected to each
co - planar structure : neighboring superpixels are more likely to belong to the same plane , if they have similar features and if there are no edges between them .
co - linearity : long straight lines in the image plane are more likely to be straight lines in the 123 - d model .
for example , edges of buildings , sidewalk , windows .
note that no single one of these four properties is enough , by itself , to predict the 123 - d structure .
for example , in some cases , local image features are not strong indicators of the depth ( and orientation ) ( e . g . , a patch on a blank feature - less wall ) .
thus , our approach will combine these properties in an mrf , in a way that depends on our condence in each of these properties .
here , the condence is itself estimated from local image cues , and will vary from region to region in the image .
our mrf is composed of ve types of nodes .
the input to the mrf occurs through two variables , labeled x and .
these variables correspond to features computed from the image pixels ( see section vi for details . ) and are always observed; thus the mrf is conditioned on these variables .
the variables
( a ) connected structure and ( b ) co - planarity .
illustration explaining effect of the choice of si and sj on enforcing
planarity , connectedness and co - linearity , we formulate our mrf
p ( |x , , y , r; ) =
f123 ( i|xi , i , ri; )
f123 ( i , j |yij , ri , rj )
where , i is the plane parameter of the superpixel i .
for a total of si points in the superpixel i , we use xi , si to denote the features for point si in the superpixel i .
xi = ( xi , si r123 : si = 123 , . . . , si ) are the features for the superpixel i .
( section vi - a ) similarly , ri = ( ri , si : si = 123 , . . . , si ) is the set of rays for superpixel i . 123 is the condence in how good the ( local ) image features are in predicting depth ( more details later ) .
the rst term f123 ( ) models the plane parameters as a function of the image features xi , si .
we have rt i , si i = 123 / di , si ( where ri , si is the ray that connects the camera to the 123 - d location of point si ) , and if the estimated depth di , si = xt i , si r , then the fractional error would be
( di , si ) 123 = rt
i , si r ) 123
therefore , to minimize the aggregate fractional error over all the points in the superpixel , we model the relation between the plane parameters and the image features as
f123 ( i|xi , i , ri; ) = exp123
i , si r ) 123
the parameters of this model are r r123
we use different parameters ( r ) for rows r = 123 , . . . , 123 in the image , because the images we consider are roughly aligned upwards ( i . e . , the direction of gravity is roughly downwards in the image ) , and thus it allows our algorithm to learn some regularities in the imagesthat different rows of the image have different statistical properties .
e . g . , a blue superpixel might be more likely to be sky if it is in the upper part of image , or water if it is in the lower part of the image , or that in the images of environments available on the internet , the horizon is more likely to be in the middle one - third of the image .
( in our experiments , we obtained very similar results using a number of rows ranging from 123 to 123 ) here , i = ( i , si : si = 123 , . . . , si ) indicates the condence
123the rays are obtained by making a reasonable guess on the camera intrinsic parametersthat the image center is the origin and the pixel - aspect - ratio is oneunless known otherwise from the image headers .
( left ) an image of a scene .
( right ) inferred soft values of yij ( 123 , 123 ) .
( yij = 123 indicates an occlusion boundary / fold , and is shown in black . ) note that even with the inferred yij being not completely accurate , the plane parameter mrf will be able to infer correct 123 - d models .
indicate our degree of condence in a depth estimate obtained only from local image features .
the variables y indicate the presence or absence of occlusion boundaries and folds in the image .
these variables are used to selectively enforce coplanarity and connectivity between superpixels .
finally , the variables are the plane parameters that are inferred using the mrf , which we call plane parameter mrf . 123 occlusion boundaries and folds : we use the variables yij ( 123 , 123 ) to indicate whether an edgel ( the edge between two neighboring superpixels ) is an occlusion boundary / fold or not .
the inference of these boundaries is typically not completely accurate; therefore we will infer soft values for yij .
( see fig .
123 ) more formally , for an edgel between two superpixels i and j , yij = 123 indicates an occlusion boundary / fold , and yij = 123 indicates none ( i . e . , a planar surface ) .
in many cases , strong image gradients do not correspond to the occlusion boundary / fold , e . g . , a shadow of a building falling on a ground surface may create an edge between the part with a shadow and the one without .
an edge detector that relies just on these local image gradients would mistakenly produce an edge .
however , there are other visual cues beyond local image gradients that better indicate whether two planes are connected / coplanar or not .
using learning to combine a number of such visual features makes the inference more accurate .
in ( 123 ) , martin , fowlkes and malik used local brightness , color and texture for learning segmentation boundaries .
here , our goal is to learn occlusion boundaries and folds .
in detail , we model yij using a logistic response as p ( yij = 123|ij ; ) = 123 / ( 123 + exp ( t ij ) ) .
where , ij are features of the superpixels i and j ( section vi - b ) , and are the parameters of the model .
during inference , we will use a mean eld - like approximation , where we replace yij with its mean value under the logistic model .
now , we will describe how we model the distribution of the plane parameters , conditioned on y .
fractional depth error : for 123 - d reconstruction , the fractional ( or relative ) error in depths is most meaningful; it is used in structure for motion , stereo reconstruction , etc .
( 123 ) , ( 123 ) for ground - truth depth d , and estimated depth d , fractional error is dened as ( d d ) / d = d / d 123
therefore , we will be penalizing fractional errors in our mrf .
mrf model : to capture the relation between the plane param - eters and the image features , and other properties such as co -
123for comparison , we also present an mrf that only models the 123 - d location
of the points in the image ( point - wise mrf , see appendix ) .
a 123 - d illustration to explain the co - planarity term .
the distance of the point sj on superpixel j to the plane on which superpixel i lies along the
is given by d123 d123
co - linearity .
( a ) two superpixels i and j lying on a straight line in the 123 - d image , ( b ) an illustration showing that a long straight line in the image plane is more likely to be a straight line in 123 - d .
( a ) 123 - d image
( b ) 123 - d world , top view
of the features in predicting the depth di , si at point si . 123 if the local image features were not strong enough to predict depth for point si , then i , si = 123 turns off the effect of the term
the second term f123 ( ) models the relation between the plane parameters of two superpixels i and j .
it uses pairs of points si and sj to do so :
i , si r ) 123
f123 ( ) = q ( si , sj ) n hsi , sj ( )
we will capture co - planarity , connectedness and co - linearity , by different choices of h ( ) and ( si , sj ) .
connected structure : we enforce this constraint by choosing si and sj to be on the boundary of the superpixels i and j .
as shown in fig .
123a , penalizing the distance between two such points ensures that they remain fully connected .
the relative ( fractional ) distance between points si and sj is penalized by hsi , sj ( i , j , yij , ri , rj ) = expyij | ( rt
i , si i rt
j , sj j ) d| ( 123 )
in detail , rt the term ( rt
i , si i = 123 / di , si and rt i , si i rt
| ( di , si dj , sj ) / pdi , si dj , sj | for d = q dsi
j , sj j = 123 / dj , sj ; therefore , j , sj j ) d gives the fractional distance dsj .
note that in case of occlusion , the variables yij = 123 , and hence the two superpixels will not be forced to be connected .
co - planarity : we enforce the co - planar structure by choosing a third pair of points s j in the center of each superpixel along with ones on the boundary .
123b ) to enforce co - planarity , we penalize the relative ( fractional ) distance of point j from the plane in which superpixel i lies , along the ray rj , s ( see fig
i and s
( i , j , yij , rj , s
) = expyij | ( rt
j ) ds
note that if the two superpixels ( ) = hs are coplanar , then hs = 123
to enforce co - planarity between two distant planes that are not connected , we can choose three such points and use the above penalty .
co - linearity : consider two superpixels i and j lying on a long straight line in a 123 - d image ( fig .
there are an innite number
123the variable i , si is an indicator of how good the image features are in predicting depth for point si in superpixel i .
we learn i , si from the monocular image features , by estimating the expected value of |di xt r xi with logistic response , with r as the parameters of the model , features xi and di as ground - truth depths .
of curves that would project to a straight line in the image plane; however , a straight line in the image plane is more likely to be a straight one in 123 - d as well ( fig .
in our model , therefore , we will penalize the relative ( fractional ) distance of a point ( such as sj ) from the ideal straight line .
in detail , consider two superpixels i and j that lie on planes parameterized by i and j respectively in 123 - d , and that lie on a straight line in the 123 - d image .
for a point sj lying on superpixel j , we will penalize its ( fractional ) distance along the ray rj , sj from the 123 - d straight line passing through superpixel i
hsj ( i , j , yij , rj , sj ) = expyij | ( rt with hsi , sj ( ) = hsi ( ) hsj ( ) .
in detail , rt
j , sj i = 123 / d
the term ( rt
j , sj i rt
j , sj j ) d| ( 123 ) j , sj j = 123 / dj , sj and j , sj j ) d for d =
j , sj i rt
j , sj ) / qdj , sj d
gives the fractional distance
the condence yij depends on the length of the line and its curvaturea long straight line in 123 - d is more likely to be a straight line in 123 - d .
parameter learning and map inference : exact parameter learning of the model is intractable; therefore , we use multi - conditional learning ( mcl ) for approximate learning , where the graphical model is approximated by a product of several marginal conditional likelihoods ( 123 ) , ( 123 ) .
in particular , we estimate the r parameters efciently by solving a linear program ( lp ) .
( see appendix for more details . )
map inference of the plane parameters , i . e . , maximizing the conditional likelihood p ( |x , , y , r; ) , is efciently performed by solving a lp .
we implemented an efcient method that uses the sparsity in our problem , so that inference can be performed in about 123 - 123 seconds for an image having about 123 superpixels on a single - core intel 123ghz cpu with 123 gb ram .
( see appendix for more details . )
for each superpixel , we compute a battery of features to capture some of the monocular cues discussed in section iii .
we also compute features to predict meaningful boundaries in the images , such as occlusion and folds .
we rely on a large number of different types of features to make our algorithm more robust and to make it generalize even to images that are very different from the training set .
the convolutional lters used for texture energies and gradients .
the rst 123 are 123x123 laws masks .
the last 123 are the oriented edge detectors at 123
the rst nine laws masks do local averaging , edge detection and spot detection .
the 123 laws mask are applied to the image to the y channel of the image .
we apply only the rst averaging lter to the color channels cb and cr; thus obtain 123 lter responses , for each of which we calculate energy and kurtosis to obtain 123 features of each patch .
the feature vector .
( a ) the original image , ( b ) superpixels for the image , ( c ) an illustration showing the location of the neighbors of superpixel s123c at multiple scales , ( d ) actual neighboring superpixels of s123c at the nest scale , ( e ) features from each neighboring superpixel along with the superpixel - shape features give a total of 123 features for the superpixel s123c .
( best viewed in color . )
monocular image features
for each superpixel at location i , we compute both texture - based summary statistic features and superpixel shape and loca - tion based features .
similar to scn , we use the output of 123 lters ( 123 laws masks , 123 color channels in ycbcr space and 123 oriented edges , see fig .
these are commonly used lters that capture the texture of a 123x123 patch and the edges at various orientations .
the lters outputs fn ( x , y ) , n = 123 , . . . , 123 are incorporated into |i ( x , y ) fn ( x , y ) |k , where k = 123 , 123 gives the ei ( n ) = p ( x , y ) si energy and kurtosis respectively .
this gives a total of 123 values for each superpixel .
we compute features for each superpixel to improve performance over scn , who computed them only for xed rectangular patches .
our superpixel shape and location based features ( 123 , computed only for the superpixel ) included the shape and location based features in section 123 of ( 123 ) , and also the eccentricity of the superpixel .
( see fig
we attempt to capture more contextual information by also including features from neighboring superpixels ( we pick the largest four in our experiments ) , and at multiple spatial scales ( three in our experiments ) .
( see fig .
123 ) the features , therefore , contain information from a larger portion of the image , and thus are more expressive than just local features .
this makes the feature vector xi of a superpixel 123 ( 123 + 123 ) 123 + 123 = 123
features for boundaries
another strong cue for 123 - d structure perception is boundary information .
if two neighboring superpixels of an image display different features , humans would often perceive them to be parts of different objects; therefore an edge between two superpixels with distinctly different features , is a candidate for a occlusion boundary or a fold .
to compute the features ij between su - perpixels i and j , we rst generate 123 different segmentations for each image for 123 different scales for 123 different properties based on textures , color , and edges .
we modied ( 123 ) to create
segmentations based on these properties .
each element of our 123 dimensional feature vector ij is then an indicator if two superpixels i and j lie in the same segmentation .
for example , if two superpixels belong to the same segments in all the 123 segmentations then it is more likely that they are coplanar or connected .
relying on multiple segmentation hypotheses instead of one makes the detection of boundaries more robust .
the features ij are the input to the classier for the occlusion boundaries and folds .
data collection
we used a custom - built 123 - d scanner to collect images ( e . g . , fig .
123a ) and their corresponding depthmaps using lasers ( e . g . , fig .
we collected a total of 123 images+depthmaps , with an image resolution of 123x123 and a depthmap resolution of 123x123 , and used 123 for training our model .
these images were collected during daytime in a diverse set of urban and natural areas in the city of palo alto and its surrounding regions .
we tested our model on rest of the 123 images ( collected using our 123 - d scanner ) , and also on 123 internet images .
the internet images were collected by issuing keywords on google image search .
to collect data and to perform the evaluation of the algorithms in a completely unbiased manner , a person not associated with the project was asked to collect images of environments ( greater than 123x123 size ) .
the person chose the following keywords to collect the images : campus , garden , park , house , building , college , university , church , castle , court , square , lake , temple , scene .
the images thus collected were from places from all over the world , and contained environments that were signicantly different from the training set , e . g .
hills , lakes , night scenes , etc .
the person chose only those images which were of environments , i . e .
she removed images of the geometrical
( a ) original image , ( b ) ground truth depthmap , ( c ) depth from image features only , ( d ) point - wise mrf , ( e ) plane parameter mrf .
( best viewed
typical depthmaps predicted by our algorithm on hold - out test set , collected using the laser - scanner .
( best viewed in color . )
typical results from our algorithm .
( top row ) original images , ( bottom row ) depthmaps ( shown in log scale , yellow is closest , followed by red and then blue ) generated from the images using our plane parameter mrf .
( best viewed in color . )
gure square when searching for keyword square; no other pre - ltering was done on the data .
in addition , we manually labeled 123 images with ground - truth boundaries to learn the parameters for occlusion boundaries and
results and discussion
we performed an extensive evaluation of our algorithm on 123 internet test images , and 123 test images collected using the laser
in table i , we compare the following algorithms :
( a ) baseline : both for pointwise mrf ( baseline - 123 ) and plane pa - rameter mrf ( baseline - 123 ) .
the baseline mrf is trained without any image features , and thus reects a prior depthmap of sorts .
( b ) our point - wise mrf : with and without constraints ( connec - tivity , co - planar and co - linearity ) .
( c ) our plane parameter mrf ( pp - mrf ) : without any constraint , with co - planar constraint only , and the full model .
( d ) saxena et al .
( scn ) , ( 123 ) , ( 123 ) applicable for quantitative errors
typical results from heh and our algorithm .
row 123 : original image .
row 123 : 123 - d model generated by heh , row 123 and 123 : 123 - d model generated by our algorithm .
( note that the screenshots cannot be simply obtained from the original image by an afne transformation . ) in image 123 , heh makes mistakes in some parts of the foreground rock , while our algorithm predicts the correct model; with the rock occluding the house , giving a novel view .
in image 123 , heh algorithm detects a wrong ground - vertical boundary; while our algorithm not only nds the correct ground , but also captures a lot of non - vertical structure , such as the blue slide .
in image 123 , heh is confused by the reection; while our algorithm produces a correct 123 - d model .
in image 123 , heh and our algorithm produce roughly equivalent resultsheh is a bit more visually pleasing and our model is a bit more detailed .
in image 123 , both heh and our algorithm fail; heh just predict one vertical plane at a incorrect location .
our algorithm predicts correct depths of the pole and the horse , but is unable to detect their boundary; hence making it qualitatively incorrect .
results : quantitative comparison of various methods .
correct % planes
( e ) hoiem et al .
( heh ) ( 123 ) .
for fairness , we scale and shift their depthmaps before computing the errors to match the global scale of our test images .
without the scaling and shifting , their error is much higher ( 123 for relative depth error ) .
we compare the algorithms on the following metrics : ( a ) % of models qualitatively correct , ( b ) % of major planes correctly
identied , 123 ( c ) depth error | log d log d| on a log - 123 scale , averaged over all pixels in the hold - out test set , ( d ) average relative depth error |d d| .
( we give these two numerical errors on only the 123 test images that we collected , because ground - truth laser depths are not available for internet images . )
table i shows that both of our models ( point - wise mrf and plane parameter mrf ) outperform the other algorithms in quantitative accuracy in depth prediction .
plane parameter mrf gives better relative depth accuracy and produces sharper depthmaps ( fig .
123 , 123 and 123 ) .
table i also shows that by capturing the image properties of connected structure , co - planarity and co - linearity , the models produced by the algorithm become signicantly better .
in addition to reducing quantitative errors , pp - mrf does indeed produce signicantly better 123 - d models .
when producing 123 - d ythroughs , even a small number of erroneous planes make the 123 - d model visually unacceptable , even though
123for the rst two metrics , we dene a model as correct when for 123% of the major planes in the image ( major planes occupy more than 123% of the area ) , the plane is in correct relationship with its nearest neighbors ( i . e . , the relative orientation of the planes is within 123 degrees ) .
note that changing the numbers , such as 123% to 123% or 123% , 123% to 123% or 123% , and 123 degrees to 123 or 123 degrees , gave similar trends in the results .
percentage of images for which heh is better , our pp - mrf is
better , or it is a tie .
the quantitative numbers may still show small errors .
our algorithm gives qualitatively correct models for 123% of images as compared to 123% by heh .
the qualitative evaluation was performed by a person not associated with the project following the guidelines in footnote 123
delage , lee and ng ( 123 ) and heh generate a popup effect by folding the images at ground - vertical boundariesan assumption which is not true for a signicant number of images; therefore , their method fails in those images .
some typical examples of the 123 - d models are shown in fig .
( note that all the test cases shown in fig .
123 , 123 , 123 and 123 are from the dataset downloaded from the internet , except fig .
123a which is from the laser - test dataset . ) these examples also show that our models are often more detailed , in that they are often able to model the scene with a multitude ( over a hundred )
we performed a further comparison .
even when both algo - rithms are evaluated as qualitatively correct on an image , one result could still be superior .
therefore , we asked the person to compare the two methods , and decide which one is better , or is a tie . 123 table ii shows that our algorithm outputs the better model in 123% of the cases , while heh outputs better model in 123% cases ( tied in the rest ) .
full documentation describing the details of the unbiased human judgment process , along with the 123 - d ythroughs produced by our algorithm , is available online at :
some of our models , e . g .
in fig .
123j , have cosmetic defects e . g .
stretched texture; better texture rendering techniques would make the models more visually pleasing .
in some cases , a small mistake ( e . g . , one person being detected as far - away in fig .
123h , and the banner being bent in fig .
123k ) makes the model look bad , and hence be evaluated as incorrect .
finally , in a large - scale web experiment , we allowed users to upload their photos on the internet , and view a 123 - d ythrough produced from their image by our algorithm .
about 123 unique users uploaded ( and rated ) about 123 images . 123 users rated 123% of the models as good .
if we consider the images of scenes only , i . e . , exclude images such as company logos , cartoon characters , closeups of objects , etc . , then this percentage was 123% .
we have made the following website available for downloading datasets / code , and for converting an image to a 123 - d
123to compare the algorithms , the person was asked to count the number of errors made by each algorithm .
we dene an error when a major plane in the image ( occupying more than 123% area in the image ) is in wrong location with respect to its neighbors , or if the orientation of the plane is more than 123 degrees wrong .
for example , if heh fold the image at incorrect place ( see fig .
123 , image 123 ) , then it is counted as an error .
similarly , if we predict top of a building as far and the bottom part of building near , making the building tiltedit would count as an error .
123no restrictions were placed on the type of images that users can upload .
users can rate the models as good ( thumbs - up ) or bad ( thumbs - down ) .
our algorithm , trained on images taken in daylight around the city of palo alto , was able to predict qualitatively correct 123 - d models for a large variety of environmentsfor example , ones that have hills or lakes , ones taken at night , and even paintings .
( see fig .
123 and the website . ) we believe , based on our experiments with varying the number of training examples ( not reported here ) , that having a larger and more diverse set of training images would improve the algorithm signicantly .
larger 123 - d models from multiple images
a 123 - d model built from a single image will almost invariably be an incomplete model of the scene , because many portions of the scene will be missing or occluded .
in this section , we will use both the monocular cues and multi - view triangulation cues to create better and larger 123 - d models .
given a sparse set of images of a scene , it is sometimes possible to construct a 123 - d model using techniques such as structure from motion ( sfm ) ( 123 ) , ( 123 ) , which start by taking two or more photographs , then nd correspondences between the images , and nally use triangulation to obtain 123 - d locations of the points .
if the images are taken from nearby cameras ( i . e . , if the baseline distance is small ) , then these methods often suffer from large triangulation errors for points far - away from the camera .
123 if , conversely , one chooses images taken far apart , then often the change of viewpoint causes the images to become very different , so that nding correspondences becomes difcult , sometimes leading to spurious or missed correspondences .
( worse , the large baseline also means that there may be little overlap between the images , so that few correspondences may even exist . ) these difculties make purely geometric 123 - d reconstruction algorithms fail in many cases , specically when given only a small set of
however , when tens of thousands of pictures are available for example , for frequently - photographed tourist attractions such as national monumentsone can use the information present in many views to reliably discard images that have only few correspondence matches .
doing so , one can use only a small subset of the images available ( 123% ) , and still obtain a 123 - d point cloud for points that were matched using sfm .
this approach has been very successfully applied to famous buildings such as the notre dame; the computational cost of this algorithm was signicant , and required about a week on a cluster of
the reason that many geometric triangulation - based methods sometimes fail ( especially when only a few images of a scene are available ) is that they do not make use of the information present in a single image .
therefore , we will extend our mrf model to seamlessly combine triangulation cues and monocular image cues to build a full photo - realistic 123 - d model of the scene .
using monocular cues will also help us build 123 - d model of the parts that are visible only in one view .
123i . e . , the depth estimates will tend to be inaccurate for objects at large distances , because even small errors in triangulation will result in large errors
typical results from our algorithm .
original image ( top ) , and a screenshot of the 123 - d ythrough generated from the image ( bottom of the image ) .
the 123 images ( a - g , l - t ) were evaluated as correct and the 123 ( h - k ) were evaluated as incorrect .
an image showing a few matches ( left ) , and the resulting 123 - d model ( right ) without estimating the variables y for condence in the 123 - d matching .
the noisy 123 - d matches reduce the quality of the model .
( note the cones erroneously projecting out from the wall . )
the ( fractional ) error in the triangulated depths dt i and di = i i ) .
for kn points for which the triangulated depths are available , we therefore have
f123 ( |dt , yt , r , q )
t i 123 .
this term places a soft constraint on a point in the plane to
have its depth equal to its triangulated depth .
map inference : for map inference of the plane param - eters , we need to maximize the conditional log p ( |x , y , dt ; ) .
all the terms in eq .
123 are l123 norm of a linear function of ; therefore map inference is efciently solved using a linear program ( lp ) .
triangulation matches
in this section , we will describe how we obtained the corre - spondences across images , the triangulated depths dt and the condences yt in the f123 ( ) term in section viii - b .
we start by computing 123 surf features ( 123 ) , and then calculate matches based on the euclidean distances between the features found .
then to compute the camera poses q = ( rotation , translation ) r123 and the depths dt of the points matched , we use bundle adjustment ( 123 ) followed by using monocular approximate depths to remove the scale ambiguity .
however , many of these 123 - d correspondences are noisy; for example , local structures are often repeated across an image ( e . g . , fig .
123 , 123 and 123 ) . 123 therefore , we also model the condence yt i in the ith match by using logistic regression to estimate the probability p ( yt i = 123 ) of the match being correct .
for this , we use neighboring 123 - d matches as a cue .
for example , a group of spatially consistent 123 - d matches is more likely to be correct than
123increasingly many cameras and camera - phones come equipped with gps , and sometimes also accelerometers ( which measure gravity / orientation ) .
many photo - sharing sites also offer geo - tagging ( where a user can specify the longitude and latitude at which an image was taken ) .
therefore , we could also use such geo - tags ( together with a rough user - specied estimate of camera orientation ) , together with monocular cues , to improve the performance of correspondence algorithms .
in detail , we compute the approximate depths of the points using monocular image features as d = xt ; this requires only computing a dot product and hence is fast .
now , for each point in an image b for which we are trying to nd a correspondence in image a , typically we would search in a band around the corresponding epipolar line in image a .
however , given an approximate depth estimated from from monocular cues , we can limit the search to a rectangular window that comprises only a subset of this band .
( see fig .
123 ) this would reduce the time required for matching , and also improve the accuracy signicantly when there are repeated structures in the scene .
( see ( 123 ) for more details . )
an illustration of the markov random field ( mrf ) for inferring 123 - d structure .
( only a subset of edges and scales shown . )
given two small plane ( superpixel ) segmentations of two images , there is no guarantee that the two segmentations are consistent , in the sense of the small planes ( on a specic object ) in one image having a one - to - one correspondence to the planes in the second image of the same object .
thus , at rst blush it appears non - trivial to build a 123 - d model using these segmentations , since it is impossible to associate the planes in one image to those in another .
we address this problem by using our mrf to reason simultaneously about the position and orientation of every plane in every image .
if two planes lie on the same object , then the mrf will ( hopefully ) infer that they have exactly the same 123 - d position .
more formally , in our model , the plane parameters n of each small ith plane in the nth image are represented by a node in our markov random field ( mrf ) .
because our model uses l123 penalty terms , our algorithm will be able to infer models for which n i = m j , which results in the two planes exactly overlapping each other .
probabilistic model
in addition to the image features / depth , co - planarity , connected structure , and co - linearity properties , we will also consider the depths obtained from triangulation ( sfm ) the depth of the point is more likely to be close to the triangulated depth .
similar to the probabilistic model for 123 - d model from a single image , most of these cues are noisy indicators of depth; therefore our mrf model will also reason about our condence in each of them , using latent variables yt ( section viii - c ) .
let qn = ( rotation , translation ) r123 ( technically se ( 123 ) ) be the camera pose when image n was taken ( w . r . t .
a xed reference , such as the camera pose of the rst image ) , and let dt be the depths obtained by triangulation ( see section viii - c ) .
we formulate our mrf as
p ( |x , y , dt ; ) yn
f123 ( n|x n , n , rn , qn; n )
f123 ( n|yn , rn , qn )
t , yn
t , rn , qn )
where , the superscript n is an index over the images , for an image n , n is the plane parameter of superpixel i in image n .
sometimes , we will drop the superscript for brevity , and write in place of n when it is clear that we are referring to a particular
the rst term f123 ( ) and the second term f123 ( ) capture the monocular properties , and are same as in eq .
we use f123 ( ) the errors in the triangulated depths , and penalize
a single isolated 123 - d match .
we capture this by using a feature vector that counts the number of matches found in the present superpixel and in larger surrounding regions ( i . e . , at multiple spatial scales ) , as well as measures the relative quality between the best and second best match .
approximate monocular depth estimates help to limit the search area for nding correspondences .
for a point ( shown as a red dot ) in image b , the corresponding region to search in image a is now a rectangle ( shown in red ) instead of a band around its epipolar line ( shown in blue ) in image a .
phantom planes
this cue enforces occlusion constraints across multiple cam - eras .
concretely , each small plane ( superpixel ) comes from an image taken by a specic camera .
therefore , there must be an unoccluded view between the camera and the 123 - d position of that small planei . e . , the small plane must be visible from the camera location where its picture was taken , and it is not plausible for any other small plane ( one from a different image ) to have a 123 - d position that occludes this view .
this cue is important because often the connected structure terms , which informally try to tie points in two small planes together , will result in models that are inconsistent with this occlusion constraint , and result in what we call phantom planesi . e . , planes that are not visible from the camera that photographed it .
we penalize the distance between the offending phantom plane and the plane that occludes its view from the camera by nding additional correspondences .
this tends to make the two planes lie in exactly the same location ( i . e . , have the same plane parameter ) , which eliminates the phantom / occlusion
in this experiment , we create a photo - realistic 123 - d model of a scene given only a few images ( with unknown location / pose ) , even ones taken from very different viewpoints or with little overlap .
123 , 123 , 123 and 123 show snapshots of some 123 - d models created by our algorithm .
using monocular cues , our algorithm is able to create full 123 - d models even when large portions of the images have no overlap ( fig .
123 , 123 and 123 ) .
in fig .
123 , monocular predictions ( not shown ) from a single image gave approximate 123 - d models that failed to capture the arch structure in the images .
however , using both monocular and triangulation cues , we were able to capture this 123 - d arch structure .
the models are available at :
incorporating object information
in this section , we will demonstrate how our model can also incorporate other information that might be available , for example , from object recognizers .
in prior work , sudderth et
( left ) original images , ( middle ) snapshot of the 123 - d model without using object information , ( right ) snapshot of the 123 - d model that uses object
( 123 ) showed that knowledge of objects could be used to get crude depth estimates , and hoiem et al .
( 123 ) used knowledge of objects and their location to improve the estimate of the horizon .
in addition to estimating the horizon , the knowledge of objects and their location in the scene give strong cues regarding the 123 - d structure of the scene .
for example , that a person is more likely to be on top of the ground , rather than under it , places certain restrictions on the 123 - d models that could be valid for a given
here we give some examples of such cues that arise when information about objects is available , and describe how we can encode them in our mrf :
( a ) object a is on top of object b
( the constraint corresponds to 123 ) , we choose an mrf
this constraint could be encoded by restricting the points si r123 on object a to be on top of the points sj r123 on object b , i . e . , i z st j z ( if z denotes the up vector ) .
in practice , we actually use a probabilistic version of this constraint .
we represent this inequality in plane - parameter space ( si = ridi = ri / ( t to penalize the fractional error = rt j zrii d potential hsi , sj ( . ) = exp`yij ( + || ) , where yij represents the uncertainty in the object recognizer output .
note that for yij ( corresponding to certainty in the object recognizer ) , this becomes a hard constraint rt j rj ) .
in fact , we can also encode other similar spatial - relations by choosing the vector z appropriately .
for example , a constraint object a is in front of object b can be encoded by choosing
i ri ) rt
j j rt
the top part of the gure shows portions of the ground correctly modeled as lying either within or beyond the arch .
( a , b , c ) three original images from different viewpoints; ( d , e , f ) snapshots of the 123 - d model predicted by our algorithm .
( f ) shows a top - down view;
( a , b ) two original images with only a little overlap , taken from the same camera location .
( c , d ) snapshots from our inferred 123 - d model .
( a , b ) two original images with many repeated structures; ( c , d ) snapshots of the 123 - d model predicted by our algorithm .
z to be the ray from the camera to the object .
( b ) object a is attached to object b
for example , if the ground - plane is known from a recognizer , then many objects would be more likely to be attached to the ground plane .
we easily encode this by using our connected -
( c ) known plane orientation
detector ( 123 ) to detect pedestrians .
for these objects , we encoded the ( a ) , ( b ) and ( c ) constraints described above .
123 shows that using the pedestrian and ground detector improves the accuracy of the 123 - d model .
also note that using soft constraints in the mrf ( section ix ) , instead of hard constraints , helps in estimating correct 123 - d models even if the object recognizer makes a mistake .
if orientation of a plane is roughly known , e . g .
that a person is more likely to be vertical , then it can be easily encoded
by adding to eq .
123 a term f ( i ) = expwi|t
represents the condence , and z represents the up vector .
we implemented a recognizer ( based on the features described in section vi ) for ground - plane , and used the dalal - triggs
i z|; here , wi
we presented an algorithm for inferring detailed 123 - d structure from a single still image .
compared to previous approaches , our algorithm creates detailed 123 - d models which are both quantita - tively more accurate and visually more pleasing .
our approach
( a , b , c , d ) four original images; ( e , f ) two snapshots shown from a larger 123 - d model created using our algorithm .
begins by over - segmenting the image into many small homoge - neous regions called superpixels and uses an mrf to infer the 123 - d position and orientation of each .
other than assuming that the environment is made of a number of small planes , we do not make any explicit assumptions about the structure of the scene , such as the assumption by delage et al .
( 123 ) and hoiem et al .
( 123 ) that the scene comprises vertical surfaces standing on a horizontal oor .
this allows our model to generalize well , even to scenes with signicant non - vertical structure .
our algorithm gave signicantly better results than prior art; both in terms of quantitative accuracies in predicting depth and in terms of fraction of qualitatively correct models .
finally , we extended these ideas to building 123 - d models using a sparse set of images , and showed how to incorporate object recognition information into our method .
the problem of depth perception is fundamental to computer vision , one that has enjoyed the attention of many researchers and seen signicant progress in the last few decades .
however , the vast majority of this work , such as stereopsis , has used multiple image geometric cues to infer depth .
in contrast , single - image cues offer a largely orthogonal source of information , one that has heretofore been relatively underexploited .
given that depth and shape perception appears to be an important building block for many other applications , such as object recognition ( 123 ) , ( 123 ) , grasping ( 123 ) , navigation ( 123 ) , image compositing ( 123 ) , and video retrieval ( 123 ) , we believe that monocular depth perception has the potential to improve all of these applications , particularly in settings where only a single image of a scene is available .
we thank rajiv agarwal and jamie schulte for help in col - lecting data .
we also thank jeff michels , olga russakovsky and sebastian thrun for helpful discussions .
this work was supported by the national science foundation under award cns - 123 , by the ofce of naval research under muri n123 , and by pixblitz studios .
a . 123 parameter learning
since exact parameter learning based on conditional likelihood for the laplacian models is intractable , we use multi - conditional
learning ( mcl ) ( 123 ) , ( 123 ) to divide the learning problem into smaller learning problems for each of the individual densities .
mcl is a framework for optimizing graphical models based on a product of several marginal conditional likelihoods each relying on common sets of parameters from an underlying joint model and predicting different subsets of variables conditioned on other
in detail , we will rst focus on learning r given the ground - truth depths d ( obtained from our 123 - d laser scanner , see sec - tion vii - a ) and the value of yij and i , si .
for this , we maximize the conditional pseudo log - likelihood log p ( |x , , y , r; r ) as
r = arg max
log f123 ( i|xi , i , ri; r )
log f123 ( i , j |yij , ri , rj )
i , si r ) 123
r = arg minr pipsi
now , from eq .
123 note that f123 ( ) does not depend on r; therefore the learning problem simplies to minimizing the l123 norm , i . e . ,
in the next step , we learn the parameters of the logistic regression model for estimating in footnote 123
parameters of a logistic regression model can be estimated by maximizing the conditional log - likelihood .
( 123 ) now , the parameters of the logistic regression model p ( yij |ij ; ) for occlusion boundaries and folds are similarly estimated using the hand - labeled ground - truth ground - truth training data by maximizing its conditional log -
a . 123 map inference
when given a new test - set image , we nd the map estimate of the plane parameters by maximizing the conditional log - likelihood log p ( |x , , y , r; r ) .
note that we solve for as a continuous variable optimization problem , which is unlike many other techniques where discrete optimization is more popular , e . g . , ( 123 ) .
from eq .
123 , we have
= arg max
log p ( |x , , y , r; r )
= arg max
f123 ( i|xi , i , ri; r ) yi , j
f123 ( i , j |yij , ri , rj )
note that the partition function z does not depend on .
there - fore , from eq .
123 , 123 and 123 and for d = xt r , we have
= arg minpk + xjn ( i ) xsi , sj bij + xjn ( i ) xsj cj
i , si i ) di , si 123 j , sj j ) dsi , sj j , sj j ) dsj
i , si i rt
j , sj i rt
where k is the number of superpixels in each image; n ( i ) is the set of neighboring superpixelsone whose relations are modeledof superpixel i; bij is the set of pair of points on the boundary of superpixel i and j that model connectivity; cj is the center point of superpixel j that model co - linearity and co - planarity; and dsi , sj = q dsi dsj .
note that each of terms is a l123 norm of a linear function of ; therefore , this is a l123 norm minimization problem , ( 123 , chap .
123 . 123 ) and can be compactly
arg minx kax bk123 + kbxk123 + kcxk123
where x r123k123 is a column vector formed by rearranging the three x - y - z components of i r123 as x123i123 = ix , x123i123 = iy and x123i = iz; a is a block diagonal matrix such that di , si i , si and b123 r123k123 is a column vector formed from i , si .
b and c are all block diagonal matrices composed of rays r , d and y; they represent the cross terms modeling the connected structure , co - planarity and co - linearity properties .
( 123i 123 ) : 123i = rt
l=123 sl ) + si ,
in general , nding the global optimum in a loopy mrf is dif - cult .
however in our case , the minimization problem is an linear program ( lp ) , and therefore can be solved exactly using any linear programming solver .
( in fact , any greedy method including a loopy belief propagation would reach the global minima . ) for fast inference , we implemented our own optimization method , one that captures the sparsity pattern in our problem , and by approximating the l123 norm with a smooth function :
kxk123 = ( x ) = 123
log ( 123 + exp ( x ) ) + log ( 123 + exp ( x ) ) note that kxk123 = lim kxk , and the approximation can be made arbitrarily close by increasing during steps of the optimization .
then we wrote a customized newton method based solver that computes the hessian efciently by utilizing the
point - wise mrf
for comparison , we present another mrf , in which we use points in the image as basic unit , instead of the superpixels; and infer only their 123 - d location .
the nodes in this mrf are a dense grid of points in the image , where the value of each node represents its depth .
the depths in this model are in log scale to emphasize fractional ( relative ) errors in depth .
unlike scns xed rectangular grid , we use a deformable grid , aligned with structures in the image such as lines and corners to improve performance .
further , in addition to using the connected structure property ( as in scn ) , our model also captures co - planarity and co - linearity .
finally , we use logistic response to identify occlusion and folds , whereas scn learned the variances .
we formulate our mrf as
p ( d|x , y , r; ) =
f123 ( di|xi , yi; ) yi , jn
f123 ( di , dj |yij , ri , rj )
f123 ( di , dj , dk|yijk , ri , rj , rk )
the rst
where , di r is the depth ( in log scale ) at a point xi are the image features at point models the relation between depths and the image features as f123 ( di|xi , yi; ) = expyi|di xt i r ( i ) | .
the second term f123 ( ) models connected structure by penalizing differences in the depths of neighboring points as f123 ( di , dj |yij , ri , rj ) = exp`yij || ( ridi rj dj ) ||123
the third term f123 ( ) depends on three points i , j and k , and models co - planarity and co - linearity .
for modeling co - linearity , we choose three points qi , qj , and qk lying on a straight line , and penalize the curvature of the line :
f123 ( di , dj , dk|yijk , ri , rj , rk ) =
exp`yijk||rj dj 123ridi + rkdk||123
where yijk = ( yij +yjk +yik ) / 123
here , the condence term yij is similar to the one described for plane parameter mrf; except in cases when the points do not cross an edgel ( because nodes in this mrf are a dense grid ) , when we set yij to zero .
enforcing local co - planarity by using ve points .
we also enforce co - planarity by penalizing two terms h ( di , j123 , di , j , di , j+123 , yi , ( j123 ) : ( j+123 ) , ri , j123 , ri , j , ri , j+123 ) , h ( di123 , j , di , j , di+123 , j , y ( i123 ) : ( i+123 ) , j , ri123 , j , ri , j , ri+123 , j ) .
each term enforces the two sets of three points to lie on the same line in 123 - d; therefore in effect enforcing ve points qi123 , j , qi , j , qi+123 , j , qi , j123 , and qi , j+123 lie on the same plane in 123 - d
learning is done similar
to the one in plane parameter mrf .
map inference of depths , log p ( d|x , y , r; ) is performed by solving a linear program ( lp ) .
however , the size of lp in this mrf is larger than in the plane
