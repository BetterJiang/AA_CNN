we present a new machine learning frame - work called \self - taught learning " for using unlabeled data in supervised classi ( cid : 123 ) cation tasks .
we do not assume that the unla - beled data follows the same class labels or generative distribution as the labeled data .
thus , we would like to use a large number of unlabeled images ( or audio samples , or text documents ) randomly downloaded from the internet to improve performance on a given image ( or audio , or text ) classi ( cid : 123 ) cation task .
such unlabeled data is signi ( cid : 123 ) cantly eas - ier to obtain than in typical semi - supervised or transfer learning settings , making self - taught learning widely applicable to many practical learning problems .
we describe an approach to self - taught learning that uses sparse coding to construct higher - level fea - tures using the unlabeled data .
these fea - tures form a succinct input representation and signi ( cid : 123 ) cantly improve classi ( cid : 123 ) cation per - formance .
when using an svm for classi ( cid : 123 ) - cation , we further show how a fisher kernel can be learned for this representation .
labeled data for machine learning is often very di ( cid : 123 ) - cult and expensive to obtain , and thus the ability to use unlabeled data holds signi ( cid : 123 ) cant promise in terms of vastly expanding the applicability of learning meth - ods .
in this paper , we study a novel use of unlabeled data for improving performance on supervised learn - ing tasks .
to motivate our discussion , consider as a running example the computer vision task of classi - fying images of elephants and rhinos .
for this task , it is di ( cid : 123 ) cult to obtain many labeled examples of ele - phants and rhinos; indeed , it is di ( cid : 123 ) cult even to obtain many unlabeled examples of elephants and rhinos .
( in fact , we ( cid : 123 ) nd it di ( cid : 123 ) cult to envision a process for col - lecting such unlabeled images , that does not immedi -
appearing in proceedings of the 123 th international confer - ence on machine learning , corvallis , or , 123
copyright 123 by the author ( s ) / owner ( s ) .
ately also provide the class labels . ) this makes the classi ( cid : 123 ) cation task quite hard with existing algorithms for using labeled and unlabeled data , including most semi - supervised learning algorithms such as the one by nigam et al .
( 123 ) .
in this paper , we ask how un - labeled images from other object classes|which are much easier to obtain than images speci ( cid : 123 ) cally of ele - phants and rhinos|can be used .
for example , given unlimited access to unlabeled , randomly chosen im - ages downloaded from the internet ( probably none of which contain elephants or rhinos ) , can we do better on the given supervised classi ( cid : 123 ) cation task ? our approach is motivated by the observation that even many randomly downloaded images will contain basic visual patterns ( such as edges ) that are similar to those in images of elephants and rhinos .
if , there - fore , we can learn to recognize such patterns from the unlabeled data , these patterns can be used for the su - pervised learning task of interest , such as recognizing elephants and rhinos .
concretely , our approach learns a succinct , higher - level feature representation of the in - puts using unlabeled data; this representation makes the classi ( cid : 123 ) cation task of interest easier .
although we use computer vision as a running exam - ple , the problem that we pose to the machine learning community is more general .
formally , we consider solving a supervised learning task given labeled and unlabeled data , where the unlabeled data does not share the class labels or the generative distribution of the labeled data .
for example , given unlimited access to natural sounds ( audio ) , can we perform better speaker identi ( cid : 123 ) cation ? given unlimited access to news articles ( text ) , can we perform better email foldering of \icml reviewing " vs .
\nips reviewing " emails ? like semi - supervised learning ( nigam et al . , 123 ) , our algorithms will therefore use labeled and unlabeled data .
but unlike semi - supervised learning as it is typ - ically studied in the literature , we do not assume that the unlabeled data can be assigned to the supervised learning tasks class labels .
to thus distinguish our formalism from such forms of semi - supervised learn - ing , we will call our task self - taught learning .
there is no prior general , principled framework for incorporating such unlabeled data into a supervised
learning algorithm .
semi - supervised learning typically makes the additional assumption that the unlabeled data can be labeled with the same labels as the clas - si ( cid : 123 ) cation task , and that these labels are merely unob - served ( nigam et al . , 123 ) .
transfer learning typi - cally requires further labeled data from a di ( cid : 123 ) erent but related task , and at its heart typically transfers knowl - edge from one supervised learning task to another; thus it requires additional labeled ( and therefore often expensive - to - obtain ) data , rather than unlabeled data , for these other supervised learning tasks . 123 ( thrun , 123; caruana , 123; ando & zhang , 123 ) because self - taught learning places signi ( cid : 123 ) cantly fewer restric - tions on the type of unlabeled data , in many practi - cal applications ( such as image , audio or text classi - ( cid : 123 ) cation ) it is much easier to apply than typical semi - supervised learning or transfer learning methods .
for example , it is far easier to obtain 123 , 123 internet im - ages than to obtain 123 , 123 images of elephants and rhinos; far easier to obtain 123 , 123 newswire articles than 123 , 123 articles on icml reviewing and nips reviewing , and so on .
using our running example of image classi ( cid : 123 ) cation , figure 123 illustrates these crucial distinctions between the self - taught learning problem that we pose , and previous , related formalisms .
we pose the self - taught learning problem mainly to formalize a machine learning framework that we think has the potential to make learning signi ( cid : 123 ) cantly eas - ier and cheaper .
and while we treat any biologi - cal motivation for algorithms with great caution , the self - taught learning problem perhaps also more accu - rately re ( cid : 123 ) ects how humans may learn than previous formalisms , since much of human learning is believed to be from unlabeled data .
consider the following in - formal order - of - magnitude argument . 123 a typical adult human brain has about 123 synapses ( connections ) , and a typical human lives on the order of 123 seconds .
thus , even if each synapse is parameterized by just a one bit parameter , a learning algorithm would require about 123=123 = 123 bits of information per second to \learn " all the connections in the brain .
it seems extremely unlikely that this many bits of labeled infor - mation are available ( say , from a humans parents or teachers in his / her youth ) .
while this argument has many ( known ) ( cid : 123 ) aws and is not to be taken too seri - ously , it strongly suggests that most of human learn - ing is unsupervised , requiring only data without any labels ( such as whatever natural images , sounds , etc .
one may encounter in ones life ) .
123we note that these additional supervised learning tasks can sometimes be created via ingenious heuristics , as in ando & zhang ( 123 ) .
123this argument was ( cid : 123 ) rst described to us by geo ( cid : 123 ) rey hinton ( personal communication ) but appears to re ( cid : 123 ) ect a view that is fairly widely held in neuroscience .
figure 123
machine learning formalisms for classifying im - ages of elephants and rhinos .
images on orange background are labeled; others are unlabeled .
top to bottom : super - vised classi ( cid : 123 ) cation uses labeled examples of elephants and rhinos; semi - supervised learning uses additional unlabeled examples of elephants and rhinos; transfer learning uses ad - ditional labeled datasets; self - taught learning just requires additional unlabeled images , such as ones randomly down - loaded from the internet .
inspired by these observations , in this paper we present largely unsupervised learning algorithms for improving performance on supervised classi ( cid : 123 ) cation tasks .
our algorithms apply straightforwardly to di ( cid : 123 ) erent input modalities , including images , audio and text .
our ap - proach to self - taught learning consists of two stages : first we learn a representation using only unlabeled data .
then , we apply this representation to the la - beled data , and use it for the classi ( cid : 123 ) cation task .
once the representation has been learned in the ( cid : 123 ) rst stage , it can then be applied repeatedly to di ( cid : 123 ) erent classi ( cid : 123 ) - cation tasks; in our example , once a representation has been learned from internet images , it can be applied not only to images of elephants and rhinos , but also to other image classi ( cid : 123 ) cation tasks .
problem formalism
; y ( 123 ) ) ; ( x ( 123 )
learning , we are given a labeled training set of m examples f ( x ( 123 ) : : : ; ( x ( m ) ; y ( m ) ) g drawn i . i . d .
from some distribution d .
here , each x ( i ) l 123 rn is an input feature vector ( the \l " subscript indicates that it is a labeled example ) , and y ( i ) 123 f123; : : : ; cg is the corresponding class label .
in addition , we are given a set of k unlabeled examples u ; x ( 123 ) u 123 rn .
crucially , we do not assume that the unlabeled data x ( j ) u was drawn from the same distribution as , nor that it can be associated with the
u ; : : : ; x ( k )
figure 123
left : example sparse coding bases learned from image patches ( 123x123 pixels ) drawn from random grayscale images of natural scenery .
each square in the grid repre - sents one basis .
right : example acoustic bases learned by the same algorithm , using 123ms sound samples from speech data .
each of the four rectangles in the 123x123 grid shows the 123ms long acoustic signal represented by a basis vector .
same class labels as , the labeled data .
clearly , as in transfer learning ( thrun , 123; caruana , 123 ) , the labeled and unlabeled data should not be completely irrelevant to each other if unlabeled data is to help the classi ( cid : 123 ) cation task .
for example , we would typically expect that x ( i ) come from the same input \type " or \modality , " such as images , audio , text , etc .
given the labeled and unlabeled training set , a self - taught learning algorithm outputs a hypothesis h : rn ! f123; : : : ; cg that tries to mimic the input - label relationship represented by the labeled training data; this hypothesis h is then tested under the same distri - bution d from which the labeled data was drawn .
a self - taught learning algorithm
we hope that the self - taught learning formalism that we have proposed will engender much novel research in machine learning .
in this paper , we describe just one approach to the problem .
u ( and x ( i )
we present an algorithm that begins by using the un - labeled data x ( i ) u to learn a slightly higher - level , more succinct , representation of the inputs .
for example , if the inputs x ( i ) l ) are vectors of pixel intensity values that represent images , our algorithm will use u to learn the \basic elements " that comprise an im - age .
for example , it may discover ( through examining the statistics of the unlabeled images ) certain strong correlations between rows of pixels , and therefore learn that most images have many edges .
through this , it then learns to represent images in terms of the edges that appear in it , rather than in terms of the raw pixel intensity values .
this representation of an image in terms of the edges that appear in it|rather than the raw pixel intensity values|is a higher level , or more abstract , representation of the input .
by applying this learned representation to the labeled data x ( i ) , we ob - tain a higher level representation of the labeled data also , and thus an easier supervised learning task .
learning higher - level representations
we learn the higher - level representation using a mod - i ( cid : 123 ) ed version of the sparse coding algorithm due to ol -
figure 123
the features computed for an image patch ( left ) by representing the patch as a sparse weighted combina - tion of bases ( right ) .
these features act as robust edge
figure 123
left : an example platypus image from the cal - tech 123 dataset .
right : features computed for the platy - pus image using four sample image patch bases ( trained on color images , and shown in the small colored squares ) by computing features at di ( cid : 123 ) erent locations in the image .
in the large ( cid : 123 ) gures on the right , white pixels represents highly positive feature values for the corresponding basis , and black pixels represents highly negative feature values .
these activations capture higher - level structure of the in - ( bases have been magni ( cid : 123 ) ed for clarity; best viewed in color . )
shausen & field ( 123 ) , which was originally proposed as an unsupervised computational model of low - level sensory processing in humans .
more speci ( cid : 123 ) cally , given the unlabeled data fx ( 123 ) u 123 rn , we pose the following optimization problem :
u g with each x ( i )
u ; : : : ; x ( k )
minimizeb;a pi kx ( i )
u ( cid : 123 ) pj a ( i ) kbjk123 ( cid : 123 ) 123;
123 + ( cid : 123 ) ka ( i ) k123 ( 123 )
123j 123 123; : : : ; s
the optimization variables in this problem are the ba - sis vectors b = fb123; b123; : : : ; bsg with each bj 123 rn , and the activations a = fa ( 123 ) ; : : : ; a ( k ) g with each a ( i ) 123 rs; here , a ( i ) is the activation of basis bj for u .
the number of bases s can be much larger than the input dimension n .
the optimization objec - tive ( 123 ) balances two terms : ( i ) the ( cid : 123 ) rst quadratic term encourages each input x ( i ) u to be reconstructed well as a weighted linear combination of the bases bj ( with corresponding weights given by the activations j ) ; and ( ii ) it encourages the activations to have low l123 norm .
the latter term therefore encourages the ac - tivations a to be sparse|in other words , for most of its elements to be zero .
( tibshirani , 123; ng , 123 ) this formulation is actually a modi ( cid : 123 ) ed version of ol - shausen & fields , and can be solved signi ( cid : 123 ) cantly more e ( cid : 123 ) ciently .
speci ( cid : 123 ) cally , the problem ( 123 ) is convex over each subset of variables a and b ( though not jointly convex ) ; in particular , the optimization over activa - tions a is an l123 - regularized least squares problem , and the optimization over basis vectors b is an l123 - constrained least squares problem .
these two convex sub - problems can be solved e ( cid : 123 ) ciently , and the objec -
tive in problem ( 123 ) can be iteratively optimized over a and b alternatingly while holding the other set of variables ( cid : 123 ) xed .
( lee et al . , 123 ) as an example , when this algorithm is applied to small 123x123 images , it learns to detect di ( cid : 123 ) erent edges in the image , as shown in figure 123 ( left ) .
exactly the same algorithm can be applied to other input types , such as audio .
when applied to speech sequences , sparse cod - ing learns to detect di ( cid : 123 ) erent patterns of frequencies , as shown in figure 123 ( right ) .
importantly , by using an l123 regularization term , we obtain extremely sparse activations|only a few bases are used to reconstruct any input x ( i ) u ; this will give us a succinct representation for x ( i ) u ( described later ) .
we note that other regularization terms that result in most of the a ( i ) j being non - zero ( such as that used in the original olshausen & field algorithm ) do not lead to good self - taught learning performance; this is described in more detail in section 123
unsupervised feature construction
it is often easy to obtain large amounts of unlabeled data that shares several salient features with the la - beled data from the classi ( cid : 123 ) cation task of interest .
in image classi ( cid : 123 ) cation , most images contain many edges and other visual structures; in optical character recog - nition , characters from di ( cid : 123 ) erent scripts mostly com - prise di ( cid : 123 ) erent pen \strokes " ; and for speaker identi ( cid : 123 ) - cation , speech even in di ( cid : 123 ) erent languages can often be broken down into common sounds ( such as phones ) .
building on this observation , we propose the follow - ing approach to self - taught learning : we ( cid : 123 ) rst apply sparse coding to the unlabeled data x ( i ) u 123 rn to learn a set of bases b , as described in section 123 .
then , for each training input x ( i ) l 123 rn from the classi ( cid : 123 ) cation task , we compute features ^a ( x ( i ) l ) 123 rs by solving the following optimization problem : this is a convex l123 - regularized least squares problem and can be solved e ( cid : 123 ) ciently ( efron et al . , 123; lee et al . , 123 ) .
it approximately expresses the input x ( i ) as a sparse linear combination of the bases bj .
the sparse vector ^a ( x ( i ) l ) is our new representation for x ( i ) using a set of 123 learned image bases ( as in fig - ure 123 , left ) , figure 123 illustrates a solution to this op - timization problem , where the input image x is ap - proximately expressed as a combination of three ba - sis vectors b123; b123; b123
the image x can now be represented via the vector ^a 123 r123 with ^a123 = 123 : 123 , ^a123 = 123 : 123 , ^a123 = 123 : 123
figure 123 shows such features ^a computed for a large image .
in both of these cases , the computed features capture aspects of the higher - level structure of the input images .
this method applies
l ) = arg mina ( i ) kx ( i )
l ( cid : 123 ) pj a ( i )
123 + ( cid : 123 ) ka ( i ) k123 ( 123 )
equally well to other input types; the features com - puted on audio samples or text documents similarly detect useful higher - level patterns in the inputs .
we use these features as input to standard supervised classi ( cid : 123 ) cation algorithms ( such as svms ) .
to classify a test example , we solve ( 123 ) to obtain our representation ^a for it , and use that as input to the trained classi ( cid : 123 ) er .
algorithm 123 summarizes our algorithm for self - taught
algorithm 123 self - taught learning via sparse coding input labeled training set
t = f ( x ( 123 ) ; y ( 123 ) ) ; ( x ( 123 ) unlabeled data fx ( 123 )
; y ( 123 ) ) ; : : : ; ( x ( m )
u ; : : : ; x ( k )
u ; x ( 123 )
output learned classi ( cid : 123 ) er for the classi ( cid : 123 ) cation task .
algorithm using unlabeled data fx ( i )
u g , solve the op -
l ) ; y ( i ) ) gm
timization problem ( 123 ) to obtain bases b .
to obtain a new labeled training set 123 + ( cid : 123 ) ka ( i ) k123
learn a classi ( cid : 123 ) er c by applying a supervised learning algorithm ( e . g . , svm ) to the labeled training set ^t .
return the learned classi ( cid : 123 ) er c .
l ) = arg mina ( i ) kx ( i )
l ( cid : 123 ) pj a ( i )
comparison with other methods
it seems that any algorithm for the self - taught learning problem must , at some abstract level , detect structure using the unlabeled data .
many unsupervised learn - ing algorithms have been devised to model di ( cid : 123 ) erent aspects of \higher - level " structure; however , their ap - plication to self - taught learning is more challenging than might be apparent at ( cid : 123 ) rst blush .
principal component analysis ( pca ) is among the most commonly used unsupervised learning algo - it identi ( cid : 123 ) es a low - dimensional subspace of maximal variation within unlabeled data .
terestingly , the top t ( cid : 123 ) n principal components b123; b123; : : : ; bt are a solution to an optimization problem that is cosmetically similar to our formulation in ( 123 ) :
u ( cid : 123 ) pj a ( i )
b123; b123; : : : ; bt are orthogonal
pca is convenient because the above optimization problem can be solved e ( cid : 123 ) ciently using standard nu - merical software; further , the features a ( i ) can be com - puted easily because of the orthogonality constraint , and are simply a ( i ) when compared with sparse coding as a method for constructing self - taught learning features , pca has two limitations .
first , pca results in linear feature extraction , in that the features a ( i ) j are simply a linear function of the input . 123 second , since pca assumes
j = bt
123as an example of a nonlinear but useful feature for im -
123 images of outdoor song snippets from 123 123 , 123 news articles 123 , 123 news articles
caltech123 image classi ( cid : 123 ) - handwritten english char - font characters ( \a " / \a " ( song snippets from 123 dif - ( from dmoz hierarchy ) categorized usenet posts ( from \sraa " dataset )
classes raw features
in 123x123 pixel
in 123x123 pixel
in 123x123 pixel
over 123ms time windows bag - of - words with 123 word bag - of - words with 123 word
table 123
details of self - taught learning applications evaluated in the experiments .
number of regions
published baseline ( fei - fei et al . , 123 )
123% 123% 123% 123% 123% 123% 123% 123%
table 123
classi ( cid : 123 ) cation accuracy on the caltech 123 image classi ( cid : 123 ) cation dataset .
for pca and sparse coding results , each image was split into the speci ( cid : 123 ) ed number of regions , and features were aggregated within each region by taking the maximum absolute value .
the bases bj to be orthogonal , the number of pca fea - tures cannot be greater than the dimension n of the input .
sparse coding does not have either of these lim - itations .
its features ^a ( x ) are an inherently nonlinear function of the input x , due to the presence of the l123 term in equation ( 123 ) . 123 further , sparse coding can use more basis vectors / features than the input dimension n .
by learning a large number of basis vectors but using only a small number of them for any particular input , sparse coding gives a higher - level representation in terms of the many possible \basic patterns , " such as edges , that may appear in an input .
section 123 further discusses other unsupervised learning algorithms .
we apply our algorithm to several self - taught learn - ing tasks shown in table 123
note that the unlabeled data in each case cannot be assigned the labels from the labeled task .
for each application , the raw input examples x were represented in a standard way : raw pixel intensities for images , the frequency spectrogram for audio , and the bag - of - words ( vector ) representation for text .
for computational reasons , the unlabeled data was preprocessed by applying pca to reduce its
ages , consider the phenomenon called end - stopping ( which is known to occur in biological visual perception ) in which a feature is maximally activated by edges of only a speci ( cid : 123 ) c orientation and length; increasing the length of the edge further signi ( cid : 123 ) cantly decreases the features activation .
a linear response model cannot exhibit end - stopping .
stopping ( lee et al . , 123 ) .
note also that even though sparse coding attempts to express x as a linear combina - tion of the bases bj , the optimization problem ( 123 ) results in the activations aj being a non - linear function of x .
dimension;123 the sparse coding basis learning algorithm was then applied in the resulting principal component space . 123 then , the learned bases were used to construct features for each input from the supervised classi ( cid : 123 ) ca - tion task . 123 for each such task , we report the result from the better of two standard , o ( cid : 123 ) - the - shelf super - vised learning algorithms : a support vector machine ( svm ) and gaussian discriminant analysis ( gda ) .
( a classi ( cid : 123 ) er speci ( cid : 123 ) cally customized to sparse coding fea - tures is described in section 123 )
we compare our self - taught learning algorithm against two baselines , also trained with an svm or gda : us - ing the raw inputs themselves as features , and using principal component projections as features , where the principal components were computed on the unlabeled
123we picked the number of principal components to pre -
serve approximately 123% of the unlabeled data variance .
smooth approximation such as ( pj qa123
123reasonable bases can often be learned even using a j + ( cid : 123 ) ) to the l123 - norm sparsity penalty kak123
however , such approximations do not produce sparse features , and in our experiments , we found that classi ( cid : 123 ) cation performance is signi ( cid : 123 ) cantly worse if such approximations are used to compute ^a ( x ) .
since the labeled and unlabeled data can sometimes lead to very di ( cid : 123 ) erent numbers of non - zero coe ( cid : 123 ) cients ai , in our exper - iments ( cid : 123 ) was also recalibrated prior to computing the la - beled datas representations ^a ( xl ) .
123partly for scaling and computational reasons , an ad - ditional feature aggregation step was applied to the image and audio classi ( cid : 123 ) cation tasks ( since a single image is several times larger than the individual / small image patch bases that can be learned tractably by sparse coding ) .
we aggre - gated features for the large image by extracting features for small image patches in di ( cid : 123 ) erent locations in the large im - age , and then aggregating the features per - basis by taking the feature value with the maximum absolute value .
the aggregation procedure e ( cid : 123 ) ectively looks for the \strongest " occurrence of each basis pattern within the image .
( even better performance is obtained by aggregating features over a kxk grid of regions , thus looking for strong activations separately in di ( cid : 123 ) erent parts of the large image; see ta - ble 123 ) these region - wise aggregated features were used as input to the classi ( cid : 123 ) cation algorithms ( svm or gda ) .
features for audio snippets were similarly aggregated by computing the maximum activation per basis vector over 123ms windows in the snippet .
training set size
figure 123
left : example images from the handwritten digit dataset ( top ) , the handwritten character dataset ( middle ) and the font character dataset ( bottom ) .
right : example sparse coding bases learned on handwritten digits .
digits ! english handwritten characters
training set size
handwritten characters ! font characters
training set size
123% 123% 123% ( 123% ) 123% 123% 123% ( 123% )
table 123
top : classi ( cid : 123 ) cation accuracy on 123 - way handwrit - ten english character classi ( cid : 123 ) cation , using bases trained on handwritten digits .
bottom : classi ( cid : 123 ) cation accuracy on 123 - way english font character classi ( cid : 123 ) cation , using bases trained on english handwritten characters .
the numbers in parentheses denote the accuracy using raw and sparse coding features together .
here , sparse coding features alone do not perform as well as the raw features , but per - form signi ( cid : 123 ) cantly better when used in combination with the raw features .
data ( as described in section 123 ) .
in the pca re - sults presented in this paper , the number of principal components used was always ( cid : 123 ) xed at the number of principal components used for preprocessing the raw input before applying sparse coding .
this control ex - periment allows us to evaluate the e ( cid : 123 ) ects of pca pre - processing and the later sparse coding step separately , but should therefore not be treated as a direct evalua - tion of pca as a self - taught learning algorithm ( where the number of principal components could then also be tables 123 - 123 report the results for various domains .
sparse coding features , possibly in combination with raw features , signi ( cid : 123 ) cantly outperform the raw features alone as well as pca features on most of the domains .
on the 123 - way caltech 123 image classi ( cid : 123 ) cation task with 123 training images per class ( table 123 ) , sparse cod - ing features achieve a test accuracy of 123% .
in com - parison , the ( cid : 123 ) rst published supervised learning algo - rithm for this dataset achieved only 123% test accuracy even with computer vision speci ( cid : 123 ) c features ( instead of raw pixel intensities ) . 123
123since the time we ran our experiments , other re - searchers have reported better results using highly spe - cialized computer vision algorithms ( zhang et al . , 123 : 123%; lazebnik et al . , 123 : 123% ) .
we note that our algorithm was until recently state - of - the - art for this well -
table 123
accuracy on 123 - way music genre classi ( cid : 123 ) cation .
design , company , product , work , market car , sale , vehicle , motor , market , import infect , report , virus , hiv , decline , product
share , disney , abc , release , o ( cid : 123 ) ce , movie , pay
table 123
text bases learned on 123 , 123 reuters newswire documents .
top : each row represents the basis most ac - tive on average for documents with the class label at the left .
for each basis vector , the words corresponding to the largest magnitude elements are displayed .
bottom : each row represents the basis that contains the largest magni - tude element for the word at the left .
the words corre - sponding to other large magnitude elements are displayed .
figure 123 shows example inputs from the three char - acter datasets , and some of the learned bases .
the learned bases appear to represent \pen strokes . " in it is thus not surprising that sparse cod - ing is able to use bases ( \strokes " ) learned on dig - its to signi ( cid : 123 ) cantly improve performance on handwrit - ten characters|it allows the supervised learning algo - rithm to \see " the characters as comprising strokes , rather than as comprising pixels .
for audio classi ( cid : 123 ) cation , our algorithm outperforms the original ( spectral ) features ( table 123 ) . 123 when applied to text , sparse coding discovers word relations that might be useful for classi ( cid : 123 ) cation ( table 123 ) .
the per - formance improvement over raw features is small ( ta - ble 123 ) . 123 this might be because the bag - of - words rep - resentation of text documents is already sparse , unlike the raw inputs for the other applications . 123 we envision self - taught learning as being most use - ful when labeled data is scarce .
table 123 shows that with small amounts of labeled data , classi ( cid : 123 ) cation per - formance deteriorates signi ( cid : 123 ) cantly when the bases ( in sparse coding ) or principal components ( in pca ) are
known dataset , even with almost no explicit computer - vision engineering , and indeed it signi ( cid : 123 ) cantly outperforms many carefully hand - designed , computer - vision speci ( cid : 123 ) c methods published on this task ( e . g . , fei - fei et al . , 123 : 123%; serre et al . , 123 : 123%; holub et al . , 123 : 123% ) .
123details : we learned bases over songs from 123 genres , and used these bases to construct features for a music genre classi ( cid : 123 ) cation over songs from 123 di ( cid : 123 ) erent genres ( with dif - ferent artists , and possibly di ( cid : 123 ) erent instruments ) .
each training example comprised a labeled 123ms song snippet; each test example was a 123 second song snippet .
123details : learned bases were evaluated on 123 binary webpage category classi ( cid : 123 ) cation tasks .
pca applied to text documents is commonly referred to as latent semantic anal - ysis .
( deerwester et al . , 123 )
123the results suggest that algorithms such as lda ( blei et al . , 123 ) might also be appropriate for self - taught learn - ing on text ( though lda is speci ( cid : 123 ) c to a bag - of - words rep - resentation and would not apply to the other domains ) .
reuters news ! webpages
training set size
reuters news ! usenet articles
training set size
table 123
classi ( cid : 123 ) cation accuracy on webpage classi ( cid : 123 ) cation ( top ) and usenet article classi ( cid : 123 ) cation ( bottom ) , using bases trained on reuters news articles .
table 123
accuracy on the self - taught learning tasks when sparse coding bases are learned on unlabeled data ( third column ) , or when principal components / sparse coding bases are learned on the labeled training set ( fourth / ( cid : 123 ) fth column ) .
since tables 123 - 123 already show the results for pca trained on unlabeled data , we omit those results from this table .
the performance trends are qualitatively preserved even when raw features are appended to the sparse coding
learned on the labeled data itself , instead of on large amounts of additional unlabeled data . 123 as more and more labeled data becomes available , the performance of sparse coding trained on labeled data approaches ( and presumably will ultimately exceed ) that of sparse coding trained on unlabeled data .
self - taught learning empirically leads to signi ( cid : 123 ) cant gains in a large variety of domains .
an important theoretical question is characterizing how the \simi - larity " between the unlabeled and labeled data a ( cid : 123 ) ects the self - taught learning performance ( similar to the analysis by baxter , 123 , for transfer learning ) .
we leave this question open for further research .
learning a kernel via sparse coding a fundamental problem in supervised classi ( cid : 123 ) cation is de ( cid : 123 ) ning a \similarity " function between two input ex - amples .
in the experiments described above , we used the regular notions of similarity ( i . e . , standard svm kernels ) to allow a fair comparison with the baseline
algorithms .
however , we now show that the sparse coding model also suggests a speci ( cid : 123 ) c specialized simi - larity function ( kernel ) for the learned representations .
the sparse coding model ( 123 ) can be viewed as learn - ing the parameter b of the following linear generative model , that posits gaussian noise on the observations x and a laplacian ( l123 ) prior over the activations : p ( a ) / exp ( ( cid : 123 ) ( cid : 123 ) pj jajj )
p ( x = pj ajbj + ( cid : 123 ) j b; a ) / exp ( ( cid : 123 ) k ( cid : 123 ) k123
once the bases b have been learned using unlabeled data , we obtain a complete generative model for the input x .
thus , we can compute the fisher kernel to measure the similarity between new inputs .
( jaakkola & haussler , 123 ) in detail , given an input x , we ( cid : 123 ) rst compute the corresponding features ^a ( x ) by ef - ( cid : 123 ) ciently solving optimization problem ( 123 ) .
then , the fisher kernel implicitly maps the input x to the high - dimensional feature vector ux = rb log p ( x; ^a ( x ) jb ) , where we have used the map approximation ^a ( x ) for the random variable a . 123 importantly , for the sparse coding generative model , the corresponding kernel has a particularly intuitive form , and for inputs x ( s ) and x ( t ) can be computed e ( cid : 123 ) ciently as : k ( x ( s ) ; x ( t ) ) = ( cid : 123 ) ^a ( x ( s ) ) t ^a ( x ( t ) ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) r ( s ) t where r = x ( cid : 123 ) pj ^ajbj represents the residual vec - tor corresponding to the map features ^a .
note that the ( cid : 123 ) rst term in the product above is simply the inner product of the map feature vectors , and corresponds to using a linear kernel over the learned sparse rep - resentation .
the second term , however , compares the two residuals as well .
we evaluate the performance of the learned kernel on the handwritten character recognition domain , since it does not require any feature aggregation .
as a base - line , we compare against all choices of standard kernels ( linear / polynomials of degree 123 and 123 / rbf ) and fea - tures ( raw features / pca / sparse coding features ) .
ta - ble 123 shows that an svm with the new kernel outper - forms the best choice of standard kernels and features , even when that best combination was picked on the test data ( thus giving the baseline a slightly unfair ad - vantage ) .
in summary , using the fisher kernel derived from the generative model described above , we obtain a classi ( cid : 123 ) er customized speci ( cid : 123 ) cally to the distribution of sparse coding features .
discussion and other methods in the semi - supervised learning setting , several authors have previously constructed features using data from the same domain as the labeled data ( e . g . , hinton & salakhutdinov , 123 ) .
in contrast , self - taught learning
123for the sake of simplicity ( and due to space con - straints ) , we performed this comparison only for the do - mains that the basic sparse coding algorithm applies to , and that do not require the extra feature aggregation step .
123in our experiments , the marginalized kernel ( tsuda et al . , 123 ) , that takes an expectation over a ( computed by mcmc sampling ) rather than the map approximation , did not perform better .
training set size
table 123
classi ( cid : 123 ) cation accuracy using the learned sparse coding kernel in the handwritten characters domain , com - pared with the accuracy using the best choice of standard kernel and input features .
( see text for details . )
poses a harder problem , and requires that the struc - ture learned from unlabeled data be \useful " for rep - resenting data from the classi ( cid : 123 ) cation task .
several ex - isting methods for unsupervised and semi - supervised learning can be applied to self - taught learning , though many of them do not lead to good performance .
for example , consider the task of classifying images of en - glish characters ( \a " |\z " ) , using unlabeled images of digits ( \123 " |\123 " ) .
for such a task , manifold learn - ing algorithms such as isomap ( tenenbaum et al . , 123 ) or lle ( roweis & saul , 123 ) can learn a low - dimensional manifold using the unlabeled data ( dig - its ) ; however , these manifold representations do not generalize straightforwardly to the labeled inputs ( en - glish characters ) that are dissimilar to any single unla - beled input ( digit ) .
we believe that these and several other learning algorithms such as auto - encoders ( hin - ton & salakhutdinov , 123 ) or non - negative matrix factorization ( hoyer , 123 ) might be modi ( cid : 123 ) ed to make them suitable for self - taught learning .
we note that even though semi - supervised learning was originally de ( cid : 123 ) ned with the assumption that the unlabeled and labeled data follow the same class la - bels ( nigam et al . , 123 ) , it is sometimes conceived as \learning with labeled and unlabeled data . " un - der this broader de ( cid : 123 ) nition of semi - supervised learning , self - taught learning would be an instance ( a particu - larly widely applicable one ) of it .
examining the last two decades of progress in ma - chine learning , we believe that the self - taught learning framework introduced here represents the natural ex - trapolation of a sequence of machine learning problem formalisms posed by various authors|starting from purely supervised learning , through semi - supervised learning , to transfer learning|where researchers have considered problems making increasingly little use of expensive labeled data , and using less and less re - lated data .
in this light , self - taught learning can also be described as \unsupervised transfer " or \transfer learning from unlabeled data . " most notably , ando & zhang ( 123 ) propose a method for transfer learning that relies on using hand - picked heuristics to generate labeled secondary prediction problems from unlabeled data .
it might be possible to adapt their method to several self - taught learning applications .
it is encour - aging that our simple algorithms produce good results across a broad spectrum of domains .
with this paper , we hope to initiate further research in this area .
