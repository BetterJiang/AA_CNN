we consider the problem of grasping novel objects , specically ones that are being seen for the rst time through vision .
grasping a previously un - known object , one for which a 123 - d model is not available , is a challenging problem .
further , even if given a model , one still has to decide where to grasp the object .
we present a learning algorithm that neither requires , nor tries to build , a 123 - d model of the object .
given two ( or more ) images of an ob - ject , our algorithm attempts to identify a few points in each image corresponding to good locations at which to grasp the object .
this sparse set of points is then triangulated to obtain a 123 - d location at which to attempt a grasp .
this is in contrast to standard dense stereo , which tries to triangulate every single point in an image ( and often fails to return a good 123 - d model ) .
our algorithm for identifying grasp locations from an image is trained via supervised learning , using synthetic images for the training set .
we demonstrate this approach on two robotic ma - nipulation platforms .
our algorithm successfully grasps a wide variety of objects , such as plates , tape - rolls , jugs , cellphones , keys , screwdrivers , sta - plers , a thick coil of wire , a strangely shaped power horn , and others , none of which were seen in the training set .
we also apply our method to the task of unloading items from dishwashers . 123
in this paper , we address the problem of grasping novel ob - jects that a robot is perceiving for the rst time through vision .
modern - day robots can be carefully hand - programmed or scripted to carry out many complex manipulation tasks , ranging from using tools to assemble complex machinery , to balancing a spinning top on the edge of a sword ( shin - ichi and satoshi , 123 ) .
however , autonomously grasping a pre - viously unknown object still remains a challenging problem .
if we are trying to grasp a previously known object , or if we
123a preliminary version of this work was described in ( saxena et
al . , 123b; 123a ) .
figure 123 : our robot unloading items from a dishwasher .
are able to obtain a full 123 - d model of the object , then var - ious approaches such as ones based on friction cones ( ma - son and salisbury , 123 ) , form - and force - closure ( bicchi and kumar , 123 ) , pre - stored primitives ( miller et al . , 123 ) , or other methods can be applied .
however , in practical scenar - ios it is often very difcult to obtain a full and accurate 123 - d reconstruction of an object seen for the rst time through vision .
for stereo systems , 123 - d reconstruction is difcult for objects without texture , and even when stereopsis works well , it would typically reconstruct only the visible portions of the object .
even if more specialized sensors such as laser scan - ners ( or active stereo ) are used to estimate the objects shape , we would still only have a 123 - d reconstruction of the front face of the object .
in contrast to these approaches , we propose a learning al - gorithm that neither requires , nor tries to build , a 123 - d model of the object .
instead it predicts , directly as a function of the images , a point at which to grasp the object .
informally , the algorithm takes two or more pictures of the object , and then tries to identify a point within each 123 - d image that cor - responds to a good point at which to grasp the object .
( for example , if trying to grasp a coffee mug , it might try to iden -
figure 123 : some examples of objects on which the grasping algorithm was tested .
tify the mid - point of the handle . ) given these 123 - d points in each image , we use triangulation to obtain a 123 - d position at which to actually attempt the grasp .
thus , rather than trying to triangulate every single point within each image in order to estimate depthsas in dense stereowe only attempt to triangulate one ( or at most a small number of ) points corre - sponding to the 123 - d point where we will grasp the object .
this allows us to grasp an object without ever needing to obtain its full 123 - d shape , and applies even to textureless , translucent or reective objects on which standard stereo 123 - d reconstruction fares poorly ( see figure 123 ) .
to the best of our knowledge , our work represents the rst algorithm capable of grasping novel objects ( ones where a 123 - d model is not available ) , including ones from novel object classes , that we are perceiving for the rst time using vision .
this paper focuses on the task of grasp identication , and thus we will consider only objects that can be picked up without performing complex manipulation . 123 we will attempt to grasp a number of common ofce and household objects such as toothbrushes , pens , books , cellphones , mugs , mar - tini glasses , jugs , keys , knife - cutters , duct - tape rolls , screw - drivers , staplers and markers ( see figure 123 ) .
we will also address the problem of unloading items from dishwashers .
the remainder of this paper is structured as follows .
section 123 , we describe related work .
in section 123 , we describe our learning approach , as well as our probabilistic model for inferring the grasping point .
in section 123 , we describe our robotic manipulation platforms .
in section 123 , we describe the motion planning / trajectory planning for moving the ma - nipulator to the grasping point .
in section 123 , we report the results of extensive experiments performed to evaluate our algorithm , and section 123 concludes .
123for example , picking up a heavy book lying at on a table might require a sequence of complex manipulations , such as to rst slide the book slightly past the edge of the table so that the manipulator can place its ngers around the book .
123 related work
most work in robot manipulation assumes availability of a complete 123 - d or 123 - d model of the object , and focuses on designing control and planning methods to achieve a suc - cessful and stable grasp .
here , we will discuss in detail prior work that uses learning or vision for robotic manip - ulation , and refer the reader to ( bicchi and kumar , 123; mason and salisbury , 123; shimoga , 123 ) for a more gen - eral survey of past work in robotic manipulation .
in simulation environments ( without real world experi - ments ) , learning has been applied to robotic manipulation for several different purposes .
for example , ( pelossof et al . , 123 ) used support vector machines ( svm ) to esti - mate the quality of a grasp given a number of features de - ( hsiao et al . , 123; scribing the grasp and the object .
hsiao and lozano - perez , 123 ) used partially observable markov decision processes ( pomdp ) to choose optimal con - trol policies for two - ngered hands .
they also used imitation learning to teach a robot whole - body grasps .
( miller et al . , 123 ) used heuristic rules to generate and evaluate grasps for three - ngered hands by assuming that the objects are made of basic shapes such as spheres , boxes , cones and cylinders each with pre - computed grasp primitives .
all of these meth - ods assumed full knowledge of the 123 - d model of the object .
further , these methods were not tested through real - world ex - periments , but were instead modeled and evaluated in a sim -
some work has been done on using vision for real world grasping experiments; however most were limited to grasp - ing 123 - d planar objects .
for uniformly colored planar ob - jects lying on a uniformly colored table top , one can nd the 123 - d contour of the object quite reliably .
using local visual features ( based on the 123 - d contour ) and other prop - erties such as form - and force - closure , the methods dis - cussed below decide the 123 - d locations at which to place ( two or three ) ngertips to grasp the object .
coelho et al . , 123 ) estimated 123 - d hand orientation using k - means clustering for simple objects ( specically , square , tri -
( a ) martini glass
figure 123 : the images ( top row ) with the corresponding labels ( shown in red in the bottom row ) of the ve object classes used for training .
the classes of objects used for training were martini glasses , mugs , whiteboard erasers , books and pencils .
angle and round blocks ) .
( morales et al . , 123a; 123b ) calculated 123 - d positions of three - ngered grasps from 123 - d ob - ject contours based on feasibility and force closure criteria .
( bowers and lumia , 123 ) also considered the grasping of planar objects and chose the location of the three ngers of a hand by rst classifying the object as circle , triangle , square or rectangle from a few visual features , and then using pre - ( kamon et al . , 123 ) scripted rules based on fuzzy logic .
used q - learning to control the arm to reach towards a spheri - cal object to grasp it using a parallel plate gripper .
if the desired location of the grasp has been identied , tech - niques such as visual servoing that align the gripper to the de - sired location ( kragic and christensen , 123 ) or haptic feed - back ( petrovskaya et al . , 123 ) can be used to pick up the object .
( platt et al . , 123 ) learned to sequence together ma - nipulation gaits for four specic , known 123 - d objects .
how - ever , they considered fairly simple scenes , and used online learning to associate a controller with the height and width of the bounding ellipsoid containing the object .
for grasping known objects , one can also use learning - by - demonstration ( hueser et al . , 123 ) , in which a human operator demon - strates how to grasp an object , and the robot learns to grasp that object by observing the human hand through vision .
the task of identifying where to grasp an object ( of the sort typically found in the home or ofce ) involves solving a difcult perception problem .
this is because the objects vary widely in appearance , and because background clut - ter ( e . g . , dishwasher prongs or a table top with a pattern ) makes it even more difcult to understand the shape of a scene .
there are numerous robust learning algorithms that can infer useful information about objects , even from a clut - tered image .
for example , there is a large amount of work on recognition of known object classes ( such as cups , mugs , etc . ) , e . g . , ( schneiderman and kanade , 123 ) .
the perfor - mance of these object recognition algorithms could proba -
bly be improved if a 123 - d model of the object were available , but they typically do not require such models .
for exam - ple , that an object is cup - shaped can often be inferred di - rectly from a 123 - d image .
our approach takes a similar di - rection , and will attempt to infer grasps directly from 123 - d images , even ones containing clutter .
( saxena et al . , 123; 123d ) also showed that given just a single image , it is often possible to obtain the 123 - d structure of a scene .
while knowing the 123 - d structure by no means implies knowing good grasps , this nonetheless suggests that most of the information in the 123 - d structure may already be contained in the 123 - d images , and suggests that an approach that learns directly from 123 - d images holds promise .
indeed , ( marotta et al . , 123 ) showed that hu - mans can grasp an object using only one eye .
our work also takes inspiration from ( castiello , 123 ) , which showed that cognitive cues and previously learned knowledge both play major roles in visually guided grasping in humans and in monkeys .
this indicates that learning from previous knowledge is an important component of grasping
further , ( goodale et al . , 123 ) showed that there is a disso - ciation between recognizing objects and grasping them , i . e . , there are separate neural pathways that recognize objects and that direct spatial control to reach and grasp the object .
thus , given only a quick glance at almost any rigid object , most pri - mates can quickly choose a grasp to pick it up , even without knowledge of the object type .
our work represents perhaps a rst step towards designing a vision grasping algorithm which can do the same .
123 learning the grasping point we consider the general case of grasping objectseven ones not seen beforein 123 - d cluttered environments such as in a home or ofce .
to address this task , we will use an image of the object to identify a location at which to grasp it .
because even very different objects can have similar sub - there are certain visual features that indicate good grasps , and that remain consistent across many different ob - jects .
for example , jugs , cups , and coffee mugs all have handles; and pens , white - board markers , toothbrushes , screw - drivers , etc .
are all long objects that can be grasped roughly at their mid - point ( figure 123 ) .
we propose a learning ap - proach that uses visual features to predict good grasping points across a large range of objects .
in our approach , we will rst predict the 123 - d location of the grasp in each image; more formally , we will try to iden - tify the projection of a good grasping point onto the image plane .
then , given two ( or more ) images of an object taken from different camera positions , we will predict the 123 - d posi - tion of a grasping point .
if each of these points can be per - fectly identied in each image , then we can easily triangu - late from these images to obtain the 123 - d grasping point .
( see figure 123a . ) in practice it is difcult to identify the projec - tion of a grasping point into the image plane ( and , if there are multiple grasping points , then the correspondence problem i . e . , deciding which grasping point in one image corresponds to which point in another imagemust also be solved ) .
this problem is further exacerbated by imperfect calibration be - tween the camera and the robot arm , and by uncertainty in the camera position if the camera was mounted on the arm it - self .
to address all of these issues , we develop a probabilistic model over possible grasping points , and apply it to infer a good position at which to grasp an object .
123 grasping point for most objects , there is typically a small region that a human ( using a two - ngered pinch grasp ) would choose to grasp it; with some abuse of terminology , we will informally refer to this region as the grasping point , and our training set will contain labeled examples of this region .
examples of grasping points include the center region of the neck for a martini glass , the center region of the handle for a coffee mug , etc .
( see figure 123 )
for testing purposes , we would like to evaluate whether the robot successfully picks up an object .
for each object in our test set , we dene the successful grasp region to be the re - gion where a human / robot using a two - ngered pinch grasp would ( reasonably ) be expected to successfully grasp the ob - ject .
the error in predicting the grasping point ( reported in table 123 ) is then dened as the distance of the predicted point from the closest point lying in this region .
( see figure 123; this region is usually somewhat larger than that used in the train - ing set , dened in the previous paragraph . ) since our gripper ( figure 123 ) has some passive compliance because of attached foam / rubber , and can thus tolerate about 123cm error in po - sitioning , the successful grasping region may extend slightly past the surface of the object .
( e . g . , the radius of the cylinder in figure 123 is about 123cm greater than the actual neck of the
figure 123 : an illustration showing the grasp labels .
the la - beled grasp for a martini glass is on its neck ( shown by a black cylinder ) .
for two predicted grasping points p123 and p123 , the error would be the 123 - d distance from the grasping region , i . e . , d123 and d123 respectively .
the grasping point in each image .
collecting real - world data of this sort is cumbersome , and manual labeling is prone to errors .
thus , we instead chose to generate , and learn from , synthetic data that is automatically labeled with the correct
in detail , we generate synthetic images along with cor - rect grasps ( figure 123 ) using a computer graphics ray tracer .
123 there is a relation between the quality of the synthetically generated images and the accuracy of the algorithm .
the better the quality of the synthetically generated images and graphical realism , the better the accuracy of the algorithm .
therefore , we used a ray tracer instead of faster , but cruder , ( michels et al . , 123 ) used syn - opengl style graphics .
thetic opengl images to learn distances in natural scenes .
however , because opengl style graphics have less realism , their learning performance sometimes decreased with added graphical details in the rendered images .
the advantages of using synthetic images are multi - fold .
first , once a synthetic model of an object has been created , a large number of training examples can be automatically gen - erated by rendering the object under different ( randomly cho - sen ) lighting conditions , camera positions and orientations , etc .
in addition , to increase the diversity of the training data generated , we randomized different properties of the objects such as color , scale , and text ( e . g . , on the face of a book ) .
the time - consuming part of synthetic data generation was the cre - ation of the mesh models of the objects .
however , there are many objects for which models are available on the internet that can be used with only minor modications .
we generated 123 examples from synthetic data , comprising objects from ve object classes ( see figure 123 ) .
using synthetic data also al - lows us to generate perfect labels for the training set with the exact location of a good grasp for each object .
in contrast ,
123 synthetic data for training we apply supervised learning to identify patches that contain grasping points .
to do so , we require a labeled training set , i . e . , a set of images of objects labeled with the 123 - d location of
123ray tracing ( glassner , 123 ) is a standard image rendering method from computer graphics .
it handles many real - world opti - cal phenomenon such as multiple specular reections , textures , soft shadows , smooth curves , and caustics .
we used povray , an open source ray tracer .
figure 123 : examples of different edge and texture lters ( 123 laws masks and 123 oriented edge lters ) used to calculate the
figure 123 : ( a ) an image of textureless / transparent / reective ( b ) depths estimated by our stereo system .
the grayscale value indicates the depth ( darker being closer to the camera ) .
black represents areas where stereo vision failed to return a depth estimate .
collecting and manually labeling a comparably sized set of real images would have been extremely time - consuming .
we have made the data available online at :
in our approach , we begin by dividing the image into small rectangular patches , and for each patch predict if it contains a projection of a grasping point onto the image plane .
instead of relying on a few visual cues such as presence of edges , we will compute a battery of features for each rect - angular patch .
by using a large number of different visual features and training on a huge training set ( section 123 ) , we hope to obtain a method for predicting grasping points that is robust to changes in the appearance of the objects and is also able to generalize well to new objects .
we start by computing features for three types of lo - cal cues : edges , textures , and color .
( saxena et al . , 123c; 123a ) we transform the image into ycbcr color space , where y is the intensity channel , and cb and cr are color channels .
we compute features representing edges by con - volving the intensity channel with 123 oriented edge lters ( fig - ure 123 ) .
texture information is mostly contained within the image intensity channel , so we apply 123 laws masks to this channel to compute the texture energy .
for the color chan - nels , low frequency information is most useful for identifying grasps; our color features are computed by applying a local averaging lter ( the rst laws mask ) to the 123 color channels .
we then compute the sum - squared energy of each of these lter outputs .
this gives us an initial feature vector of dimen -
to predict if a patch contains a grasping point , local image features centered on the patch are insufcient , and one has to use more global properties of the object .
we attempt to capture this information by using image features extracted at multiple spatial scales ( 123 in our experiments ) for the patch .
objects exhibit different behaviors across different scales , and using multi - scale features allows us to capture these vari - ations .
in detail , we compute the 123 features described above from that patch as well as the 123 neighboring patches ( in a 123x123 window centered around the patch of interest ) .
this gives us a feature vector x of dimension 123 123 123 + 123 123 = 123
although we rely mostly on image - based features for pre - dicting the grasping point , some robots may be equipped with range sensors such as a laser scanner or a stereo camera .
in these cases ( section 123 ) , we also compute depth - based fea - tures to improve performance .
more formally , we apply our texture based lters to the depth image obtained from a stereo camera , append them to the feature vector used in classica - tion , and thus obtain a feature vector xs r123
applying these texture based lters this way has the effect of comput - ing relative depths , and thus provides information about 123 - d properties such as curvature .
however , the depths given by a stereo system are sparse and noisy ( figure 123 ) because many objects we consider are textureless or reective .
even after normalizing for the missing depth readings , these features im - proved performance only marginally .
123 probabilistic model using our image - based features , we will rst predict whether each region in the image contains the projection of a grasp - ing point .
then in order to grasp an object , we will statisti - cally triangulate our 123 - d predictions to obtain a 123 - d grasp -
in detail , on our manipulation platforms ( section 123 ) , we have cameras mounted either on the wrist of the robotic arm ( figure 123 ) or on a frame behind the robotic arm ( figure 123 ) .
when the camera is mounted on the wrist , we command the arm to move the camera to two or more positions , so as to acquire images of the object from different viewpoints .
how - ever , there are inaccuracies in the physical positioning of the arm , and hence there is some slight uncertainty in the posi - tion of the camera when the images are acquired .
we will now describe how we model these position errors .
formally , let c be the image that would have been taken if the actual pose of the camera was exactly equal to the mea - sured pose ( e . g . , if the robot had moved exactly to the com - manded position and orientation , in the case of the camera being mounted on the robotic arm ) .
however , due to posi - tioning error , instead an image c is taken from a slightly dif - ferent location .
let ( u , v ) be a 123 - d position in image c , and let ( u , v ) be the corresponding image position in c .
thus c ( u , v ) = c ( u , v ) , where c ( u , v ) is the pixel value at ( u , v ) in image c .
the errors in camera position / pose should usu - ally be small , 123 and we model the difference between ( u , v ) and ( u , v ) using an additive gaussian model : u = u + u , v = v + v , where u , v n ( 123 , 123 ) .
123the robot position / orientation error is typically small ( position is usually accurate to within 123mm ) , but it is still important to model this error .
from our experiments ( see section 123 ) , if we set 123 = 123 , the triangulation is highly inaccurate , with average error in predict - ing the grasping point being 123 cm , as compared to 123 cm when appropriate 123 is chosen .
( a ) coffee pot
( e ) synthetic martini glass figure 123 : grasping point classication .
the red points in each image show the locations most likely to be a grasping point , as predicted by our logistic regression model .
( best viewed in color . )
( b ) duct tape
now , to predict which locations in the 123 - d image are grasp - ing points ( figure 123 ) , we dene the class label z ( u , v ) as fol - lows .
for each location ( u , v ) in an image c , z ( u , v ) = 123 if ( u , v ) is the projection of a grasping point onto the image plane , and z ( u , v ) = 123 otherwise .
for a corresponding loca - tion ( u , v ) in image c , we similarly dene z ( u , v ) to indicate whether position ( u , v ) represents a grasping point in the im - age c .
since ( u , v ) and ( u , v ) are corresponding pixels in c and c , we assume z ( u , v ) = z ( u , v )
p ( z ( u , v ) = 123|c ) = p ( z ( u , v ) = 123| c )
= zu zv
p ( u , v ) p ( z ( u + u , v + v ) = 123| c ) dudv ( 123 )
here , p ( u , v ) is the ( gaussian ) density over u and v .
we then use logistic regression to model the probability of a 123 - d position ( u + u , v + v ) in c being a good grasping point : p ( z ( u + u , v + v ) = 123| c ) = p ( z ( u + u , v + v ) = 123|x; )
= 123 / ( 123 + ext )
where x r123 are the features for the rectangular patch centered at ( u + u , v + v ) in image c ( described in sec - tion 123 ) .
the parameter of this model r123 is learned using standard maximum likelihood for logistic regression :
= arg maxqi p ( zi|xi; ) , where ( xi , zi ) are the syn -
thetic training examples ( image patches and labels ) , as de - scribed in section 123 .
figure 123a - d shows the result of apply - ing the learned logistic regression model to some real ( non - 123 - d grasp model : given two or more images of a new ob - ject from different camera positions , we want to infer the 123 - d position of the grasping point .
( see figure 123 ) because lo - gistic regression may have predicted multiple grasping points per image , there is usually ambiguity in the correspondence problem ( i . e . , which grasping point in one image corresponds to which graping point in another ) .
to address this while also taking into account the uncertainty in camera position , we propose a probabilistic model over possible grasping points in 123 - d space .
in detail , we discretize the 123 - d work - space of the robotic arm into a regular 123 - d grid g r123 , and associate with each grid element j a random variable yj , so that yj = 123 if grid cell j contains a grasping point , and yj = 123 otherwise .
figure 123 : ( a ) diagram illustrating rays from two images c123 and c123 intersecting at a grasping point ( shown in dark blue ) .
( best viewed in color . )
from each camera location i = 123 , . . . , n , one image is in image ci , let the ray passing through ( u , v ) be denoted ri ( u , v ) .
let gi ( u , v ) g be the set of grid - cells through which the ray ri ( u , v ) passes .
let r123 , . . . rk gi ( u , v ) be the indices of the grid - cells lying on the ray ri ( u , v ) .
we know that if any of the grid - cells rj along the ray rep - resent a grasping point , then its projection is a grasp point .
more formally , zi ( u , v ) = 123 if and only if yr123 = 123 or yr123 = 123 or .
or yrk = 123
for simplicity , we use a ( ar - guably unrealistic ) naive bayes - like assumption of indepen - dence , and model the relation between p ( zi ( u , v ) = 123|ci ) and p ( yr123 = 123 or .
or yrk = 123|ci ) as
p ( zi ( u , v ) = 123|ci ) = p ( yr123 = 123 , . . . , yrk = 123|ci )
p ( yrj = 123|ci )
assuming that any grid - cell along a ray is equally likely to be a grasping point , this therefore gives
p ( yrj = 123|ci ) = 123 ( 123 p ( zi ( u , v ) = 123|ci ) ) 123 / k ( 123 )
next , using another naive bayes - like independence as - sumption , we estimate the probability of a particular grid - cell
yj g being a grasping point as :
p ( yj = 123|c123 , . . . , cn ) =
p ( yj = 123 ) p ( c123 , . . . , cn |yj = 123 )
p ( yj = 123 )
p ( c123 , . . . , cn )
p ( yj = 123 )
p ( c123 , . . . , cn )
p ( yj = 123|ci )
p ( c123 , . . . , cn )
p ( ci|yj = 123 )
p ( yj = 123|ci ) p ( ci )
p ( yj = 123 )
where p ( yj = 123 ) is the prior probability of a grid - cell being a grasping point ( set to a constant value in our experiments ) .
one can envision using this term to incorporate other avail - able information , such as known height of the table when the robot is asked to pick up an object from a table .
using equa - tions 123 , 123 , 123 and 123 , we can now compute ( up to a constant of proportionality that does not depend on the grid - cell ) the probability of any grid - cell yj being a valid grasping point , given the images . 123 stereo cameras : some robotic platforms have stereo cam - eras ( e . g . , the robot in figure 123 ) ; therefore we also discuss how our probabilistic model can incorporate stereo images .
from a stereo camera , since we also get a depth value w ( u , v ) for each location ( u , v ) in the image , 123 we now obtain a 123 - d image c ( u , v , w ( u , v ) ) for 123 - d positions ( u , v , w ) .
however because of camera positioning errors ( as dis - cussed before ) , we get c ( u , v , w ( u , v ) ) instead of actual im - age c ( u , v , w ( u , v ) ) .
we again model the difference be - tween ( u , v , w ) and ( u , v , w ) using an additive gaussian : u = u + u , v = v + v , w = w + w , where u , v n ( 123 , 123 w ) .
now , for our class label z ( u , v , w ) in the 123 - d w n ( 123 , 123 image , we have :
p ( z ( u , v , w ) = 123|c ) = p ( z ( u , v , w ) = 123| c )
=zu zv zw p ( z ( u + u , v + v , w + w ) = 123| c ) dudvdw ( 123 )
p ( u , v , w )
123in about 123% of the trials described in section 123 , grasping failed because the algorithm found points in the images that did not actually correspond to each other .
( e . g . , in one image the point se - lected may correspond to the midpoint of a handle , and in a different image a different point may be selected that corresponds to a differ - ent part of the same handle . ) thus , triangulation using these points results in identifying a 123 - d point that does not lie on the object .
by ensuring that the pixel values in a small window around each of the corresponding points are similar , one would be able to reject some of these spurious correspondences .
123the depths estimated from a stereo camera are very sparse , i . e . , the stereo system nds valid points only for a few pixels in the im - age .
( see figure 123 ) therefore , we still mostly rely on image features to nd the grasp points .
the pixels where the stereo camera was unable to obtain a depth are treated as regular ( 123 - d ) image pixels .
here , p ( u , v , w ) is the ( gaussian ) density over u , v and w .
now our logistic regression model is p ( z ( u , v , w ( u , v ) ) = 123| c ) = p ( z ( u , v , w ( u , v ) ) = 123|xs; s )
= 123 / ( 123 + ext
where xs r123 are the image and depth features for the rectangular patch centered at ( u , v ) in image c ( described in section 123 ) .
the parameter of this model s r123 is
now to use a stereo camera in estimating the 123 - d grasping
point , we use
p ( yj = 123|ci ) = p ( zi ( u , v , w ( u , v ) ) = 123|ci )
123 in place of eq .
123 when ci represents a stereo camera image with depth information at ( u , v ) .
this framework allows predictions from both regular and stereo cameras to be used together seamlessly , and also al - lows predictions from stereo cameras to be useful even when the stereo system failed to recover depth information at the predicted grasp point .
123 map inference given a set of images , we want to infer the most likely 123 - d location of the grasping point .
therefore , we will choose the grid cell j in the 123 - d robot workspace that maximizes the con - ditional log - likelihood log p ( yj = 123|c123 , . . . , cn ) in eq .
more formally , let there be nc regular cameras and n nc stereo cameras .
now , from eq .
123 and 123 , we have :
arg maxj log p ( yj = 123|c123 , . . . , cn )
= arg max
= arg max
p ( yj = 123|ci )
log ( cid : 123 ) 123 ( 123 p ( zi ( u , v ) = 123|ci ) ) 123 / k ( cid : 123 )
log ( p ( zi ( u , v , w ( u , v ) ) = 123|ci ) ) ( 123 )
where p ( zi ( u , v ) = 123|ci ) is given by eq .
123 and 123 and p ( zi ( u , v , w ( u , v ) ) = 123|ci ) is given by eq .
123 and 123
a straightforward implementation that explicitly computes the sum above for every single grid - cell would give good grasping performance , but be extremely inefcient ( over 123 seconds ) .
since there are only a few places in an image where p ( zi ( u , v ) = 123|ci ) is signicantly greater than zero , we im - plemented a counting algorithm that explicitly considers only grid - cells yj that are close to at least one ray ri ( u , v ) .
( grid - cells that are more than 123 distance away from all rays are highly unlikely to be the grid - cell that maximizes the sum - mation in eq .
123 ) this counting algorithm efciently accu - mulates the sums over the grid - cells by iterating over all n images and rays ri ( u , v ) , 123 and results in an algorithm that 123in practice , we found that restricting attention to rays where p ( zi ( u , v ) = 123|ci ) > 123 allows us to further reduce the number of rays to be considered , with no noticeable degradation in perfor -
identies a 123 - d grasping position in 123 sec .
123 robot platforms our experiments were performed on two robots built for the stair ( stanford ai robot ) project . 123 each robot has an arm and other equipment such as cameras , computers , etc .
( see fig .
123 and 123 ) the stair platforms were built as part of a project whose long - term goal is to create a general purpose household robot that can navigate in indoor environments , pick up and interact with objects and tools , and carry out tasks such as tidy up a room or prepare simple meals .
our algo - rithms for grasping novel objects represent perhaps a small step towards achieving some of these goals .
stair 123 uses a harmonic arm ( katana , by neuronics ) , and is built on top of a segway robotic mobility platform .
123 - dof arm is position - controlled and has a parallel plate grip - per .
the arm has a positioning accuracy of 123 mm , a reach of 123cm , and can support a payload of 123g .
our vision system used a low - quality webcam ( logitech quickcam pro 123 ) mounted near the end effector and a stereo camera ( bumble - bee , by point grey research ) .
in addition , the robot has a laser scanner ( sick lms - 123 ) mounted approximately 123m above the ground for navigation purposes .
( we used the we - bcam in the experiments on grasping novel objects , and the bumblebee stereo camera in the experiments on unloading items from dishwashers . ) stair 123 sits atop a holonomic mo - bile base , and its 123 - dof arm ( wam , by barrett technologies ) can be position or torque - controlled , is equipped with a three - ngered hand , and has a positioning accuracy of 123 mm .
it has a reach of 123m and can support a payload of 123kg .
its vision system uses a stereo camera ( bumblebee123 , by point grey re -
we used a distributed software framework called switch - yard ( quigley , 123 ) to route messages between different devices such as the robotic arms , cameras and computers .
switchyard allows distributed computation using tcp mes - sage passing , and thus provides networking and synchroniza - tion across multiple processes on different hardware plat -
figure 123 : stair 123 platform .
this robot is equipped with a 123 - dof arm and a parallel plate gripper .
after identifying a 123 - d point at which to grasp an object , we need to nd an arm pose that realizes the grasp , and then plan a path to reach that arm pose so as to pick up the object .
given a grasping point , there are typically many end - effector orientations consistent with placing the center of the gripper at that point .
the choice of end - effector orientation should also take into account other constraints , such as loca - tion of nearby obstacles , and orientation of the object .
123 - dof arm .
when planning in the absence of obstacles , we found that even fairly simple methods for planning worked well .
specically , on our 123 - dof arm , one of the degrees of freedom is the wrist rotation , which therefore does not affect planning to avoid obstacles .
thus , we can separately consider planning an obstacle - free path using the rst 123 - dof , and de - ciding the wrist rotation .
to choose the wrist rotation , using a
123see http : / / www . cs . stanford . edu / group / stair for details .
figure 123 : stair 123 platform .
this robot is equipped with 123 - dof barrett arm and three - ngered hand .
figure 123 : the robotic arm picking up various objects : screwdriver , box , tape - roll , wine glass , a solder tool holder , coffee pot , powerhorn , cellphone , book , stapler and coffee mug .
( see section 123 . )
simplied version of our algorithm in ( saxena et al . , 123b ) , we learned the 123 - d value of the 123 - d grasp orientation projected onto the image plane ( see appendix ) .
thus , for example , if the robot is grasping a long cylindrical object , it should rotate the wrist so that the parallel - plate grippers inner surfaces are parallel ( rather than perpendicular ) to the main axis of the cylinder .
further , we found that using simple heuristics to decide the remaining degrees of freedom worked well . 123
when grasping in the presence of obstacles , such as when unloading items from a dishwasher , we used a full motion planning algorithm for the 123 - dof as well as for the opening of the gripper ( a 123th degree of freedom ) .
specically , having
123four degrees of freedom are already constrained by the end - effector 123 - d position and the chosen wrist angle .
to decide the fth degree of freedom in uncluttered environments , we found that most grasps reachable by our 123 - dof arm fall in one of two classes : down - ward grasps and outward grasps .
these arise as a direct consequence of the shape of the workspace of our 123 dof robotic arm ( figure 123 ) .
a downward grasp is used for objects that are close to the base of the arm , which the arm will grasp by reaching in a downward direc - tion ( figure 123 , rst image ) , and an outward grasp is for objects further away from the base , for which the arm is unable to reach in a downward direction ( figure 123 , second image ) .
in practice , to simplify planning we rst plan a path towards an approach position , which is set to be a xed distance away from the predicted grasp point towards the base of the robot arm .
then we move the end - effector in a straight line forward towards the target grasping point .
our grasping experiments in uncluttered environments ( section 123 ) were performed using this heuristic .
identied possible goal positions in conguration space us - ing standard inverse kinematics ( mason and salisbury , 123 ) , we plan a path in 123 - dof conguration space that takes the end - effector from the starting position to a goal position , avoiding obstacles .
for computing the goal orientation of the end - effector and the conguration of the ngers , we used a criterion that attempts to minimize the opening of the hand without touching the object being grasped or other nearby obstacles .
our planner uses probabilistic road - maps ( prms ) ( schwarzer et al . , 123 ) , which start by randomly sampling points in the conguration space .
it then constructs a road map by nding collision - free paths between nearby points , and nally nds a shortest path from the starting po - sition to possible target positions in this graph .
we also ex - perimented with a potential eld planner ( khatib , 123 ) , but found the prm method gave better results because it did not get stuck in local optima .
123 - dof arm .
on the stair 123 robot , which uses a 123 - dof arm , we use the full algorithm in ( saxena et al . , 123b ) , for pre - dicting the 123 - d orientation of a grasp , given an image of an object .
this , along with our algorithm to predict the 123 - d grasping point , determines six of the seven degrees of free - dom ( i . e . , the end - effector location and orientation ) .
for de - ciding the seventh degree of freedom , we use a criterion that maximizes the distance of the arm from the obstacles .
sim - ilar to the planning on the 123 - dof arm , we then apply a prm planner to plan a path in the 123 - dof conguration space .
table 123 : mean absolute error in locating the grasping point for different objects , as well as grasp success rate for picking up the different objects using our robotic arm .
( although training was done on synthetic images , testing was done on the real robotic arm and real objects . )
objects similar to ones trained on
mean absolute grasp - success
mean absolute grasp - success
123 experiment 123 : synthetic data we rst evaluated the predictive accuracy of the algorithm on synthetic images ( not contained in the training set ) .
( see fig - ure 123e . ) the average accuracy for classifying whether a 123 - d image patch is a projection of a grasping point was 123% ( evaluated on a balanced test set comprised of the ve objects in figure 123 ) .
even though the accuracy in classifying 123 - d regions as grasping points was only 123% , the accuracy in predicting 123 - d grasping points was higher because the proba - bilistic model for inferring a 123 - d grasping point automatically aggregates data from multiple images , and therefore xes some of the errors from individual classiers .
123 experiment 123 : grasping novel objects we tested our algorithm on stair 123 ( 123 - dof robotic arm , with a parallel plate gripper ) on the task of picking up an object placed on an uncluttered table top in front of the robot .
the location of the object was chosen randomly ( and we used cardboard boxes to change the height of the object , see fig - ure 123 ) , and was completely unknown to the robot .
the orien - tation of the object was also chosen randomly from the set of orientations in which the object would be stable , e . g . , a wine glass could be placed vertically up , vertically down , or in a random 123 - d orientation on the table surface ( see figure 123 ) .
( since the training was performed on synthetic images of ob - jects of different types , none of these scenarios were in the
in these experiments , we used a web - camera , mounted on the wrist of the robot , to take images from two or more loca - tions .
recall that the parameters of the vision algorithm were trained from synthetic images of a small set of ve object classes , namely books , martini glasses , white - board erasers , mugs / cups , and pencils .
we performed experiments on cof - fee mugs , wine glasses ( empty or partially lled with water ) , pencils , books , and erasersbut all of different dimensions and appearance than the ones in the training setas well as a large set of objects from novel object classes , such as rolls of duct tape , markers , a translucent box , jugs , knife - cutters , cellphones , pens , keys , screwdrivers , staplers , tooth - brushes , a thick coil of wire , a strangely shaped power horn ,
( see figures 123 and 123 ) we note that many of these objects are translucent , textureless , and / or reective , mak - ing 123 - d reconstruction difcult for standard stereo systems .
( indeed , a carefully - calibrated point gray stereo system , the bumblebee bb - col - 123 , with higher quality cameras than our web - camerafails to accurately reconstruct the visible portions of 123 out of 123 objects .
see figure 123 )
in extensive experiments ,
the algorithm for predicting grasps in images appeared to generalize very well .
despite being tested on images of real ( rather than synthetic ) objects , including many very different from ones in the training set , it was usually able to identify correct grasp points .
we note that test set error ( in terms of average absolute error in the predicted position of the grasp point ) on the real images was only somewhat higher than the error on synthetic images; this shows that the algorithm trained on synthetic images trans - fers well to real images .
( over all 123 object types used in the synthetic data , average absolute error was 123cm in the syn - thetic images; and over all the 123 real test objects , average er - ror was 123cm . ) for comparison , neonate humans can grasp simple objects with an average accuracy of 123cm .
( bower et
table 123 shows the errors in the predicted grasping points on the test set .
the table presents results separately for objects which were similar to those we trained on ( e . g . , coffee mugs ) and those which were very dissimilar to the training objects ( e . g . , duct tape ) .
for each entry in the table , a total of four trials were conducted except for staplers , for which ten trials were conducted .
in addition to reporting errors in grasp posi - tions , we also report the grasp success rate , i . e . , the fraction of times the robotic arm was able to physically pick up the object .
for a grasp to be counted as successful , the robot had to grasp the object , lift it up by about 123ft , and hold it for 123 seconds .
on average , the robot picked up the novel objects 123% of the time .
for simple objects such as cellphones , wine glasses , keys , toothbrushes , etc . , the algorithm performed perfectly in our experiments ( 123% grasp success rate ) .
however , grasping objects such as mugs or jugs ( by the handle ) allows only a narrow trajectory of approachwhere one nger is in - serted into the handleso that even a small error in the grasp - ing point identication causes the arm to hit and move the ob -
figure 123 : example of a real dishwasher image , used for training in the dishwasher experiments .
figure 123 : grasping point detection for objects in a dish - ( only the points in top ve grasping regions are
ject , resulting in a failed grasp attempt .
although it may be possible to improve the algorithms accuracy , we believe that these problems can best be solved by using a more advanced robotic arm that is capable of haptic ( touch ) feedback .
in many instances ,
the algorithm was able to pick up completely novel objects ( a strangely shaped power - horn , duct - tape , solder tool holder , etc . ; see figures 123 and 123 ) .
perceiving a transparent wine glass is a difcult problem for standard vision ( e . g . , stereopsis ) algorithms because of reections , etc .
however , as shown in table 123 , our algorithm successfully picked it up 123% of the time .
videos showing the robot grasping the objects are available at
123 experiment 123 : unloading items from
the goal of the stair project is to build a general purpose household robot .
as a step towards one of stairs envi - sioned applications , in this experiment we considered the task of unloading items from dishwashers ( figures 123 and 123 ) .
this is a difcult problem because of the presence of background clutter and occlusion between objectsone object that we are trying to unload may physically block our view of a second object .
our training set for these experiments also included some hand - labeled real examples of dishwasher images ( fig - ure 123 ) , including some images containing occluded objects; this helps prevent the algorithm from identifying grasping points on the background clutter such as dishwasher prongs .
along with the usual features , these experiments also used the depth - based features computed from the depth image ob - tained from the stereo camera ( section 123 ) .
further , in these experiments we did not use color information , i . e . , the images fed to the algorithm were grayscale .
in detail , we asked a person to arrange several objects neatly ( meaning inserted over or between the dishwasher prongs , and with no pair of objects lying ush against each other; figures 123 and 123 show typical examples ) in the upper tray of the dishwasher .
to unload the items from the dish - washer , the robot rst identies grasping points in the image .
figure 123 shows our algorithm correctly identifying grasps
table 123 : grasp - success rate for unloading items from a dish -
on multiple objects even in the presence of clutter and occlu - sion .
the robot then uses these grasps and the locations of the obstacles ( perceived using a stereo camera ) to plan a path while avoiding obstacles , to pick up the object .
when plan - ning a path to a grasping point , the robot chooses the grasp - ing point that is most accessible to the robotic arm using a criterion based on the grasping points height , distance to the robot arm , and distance from obstacles .
the robotic arm then removes the rst object ( figure 123 ) by lifting it up by about 123ft and placing it on a surface on its right using a pre - written script , and repeats the process above to unload the next item .
as objects are removed , the visual scene also changes , and the algorithm will nd grasping points on objects that it had
we evaluated the algorithm quantitatively for four object classes : plates , bowls , mugs and wine glasses .
successfully unloaded items from multiple dishwashers; how - ever we performed quantitative experiments only on one dish - washer . ) we performed ve trials for each object class ( each trial used a different object ) .
we achieved an average grasp - ing success rate of 123% in a total of 123 trials ( see table 123 ) .
our algorithm was able to successfully pick up objects such as plates and wine glasses most of the time .
however , due to the physical limitations of the 123 - dof arm with a parallel plate gripper , it is not possible for the arm to pick up certain ob - jects , such as bowls , if they are in certain congurations ( see figure 123 ) .
for mugs , the grasp success rate was low because of the problem of narrow trajectories discussed in section 123 .
we also performed tests using silverware , specically ob - jects such as spoons and forks .
these objects were often placed ( by the human dishwasher loader ) against the corners or walls of the silverware rack .
the handles of spoons and
figure 123 : dishwasher experiments ( section 123 ) : our robotic arm unloads items from a dishwasher .
forks are only about 123cm thick; therefore a larger clearance than 123cm was needed for them to be grasped using our par - allel plate gripper , making it physically extremely difcult to do so .
however , if we arrange the spoons and forks with part of the spoon or fork at least 123cm away from the walls of the silverware rack , then we achieve a grasping success rate of
some of the failures were because some parts of the object were not perceived; therefore the arm would hit and move the object resulting in a failed grasp .
in such cases , we believe an algorithm that uses haptic ( touch ) feedback would signi - cantly increase grasp success rate .
some of our failures were also in cases where our algorithm correctly predicts a grasp - ing point , but the arm was physically unable to reach that grasp .
therefore , we believe that using an arm / hand with more degrees of freedom , will signicantly improve perfor - mance for the problem of unloading a dishwasher .
figure 123 : dishwasher experiments : failure case .
for some congurations of certain objects , it is impossible for our 123 - dof robotic arm to grasp it ( even if a human were controlling
123 experiment 123 : grasping kitchen and ofce
our long - term goal is to create a useful household robot that can perform many different tasks , such as fetching an object
in response to a verbal request and cooking simple kitchen in situations such as these , the robot would know which object it has to pick up .
for example , if the robot was asked to fetch a stapler from an ofce , then it would know that it needs to identify grasping points for staplers only .
there - fore , in this experiment we study how we can use information about object type and location to improve the performance of the grasping algorithm .
consider objects lying against a cluttered background such as a kitchen or an ofce .
if we predict the grasping points using our algorithm trained on a dataset containing all ve objects , then we typically obtain a set of reasonable grasping point predictions ( figure 123 , left column ) .
now suppose we know the type of object we want to grasp , as well as its ap - proximate location in the scene ( such as from an object recog - nition algorithm ( gould et al . , 123 ) ) .
we can then restrict our attention to the area of the image containing the object , and apply a version of the algorithm that has been trained using only objects of a similar type ( i . e . , using object - type specic parameters , such as using bowl - specic parameters when picking up a cereal bowl , using spoon - specic param - eters when picking up a spoon , etc ) .
with this method , we obtain object - specic grasps , as shown in figure 123 ( right
achieving larger goals , such as cooking simple kitchen meals , requires that we combine different algorithms such as object recognition , navigation , robot manipulation , etc .
these results demonstrate how our approach could be used in conjunction with other complementary algorithms to ac - complish these goals .
123 experiment 123 : grasping using 123 - dof arm and
in this experiment , we demonstrate that our grasping point prediction algorithm can also be used with other robotic manipulation platforms .
we performed experiments on the stair 123 robot , which is equipped with a 123 - dof arm and a three - ngered barrett hand .
this is a more capable manipu - lator than a parallel plate gripper , in that its ngers can form a large variety of congurations; however , in this experiment we will use the hand only in a limited way , specically , a con - guration with the two ngers opposing the third one , with all ngers closing simultaneously .
while there is a large space of hand congurations that one would have to consider in or - der to fully take advantage of the capabilities of such a hand , identifying a point at which to grasp the object still remains an important aspect of the problem , and is the focus of the
figure 123 : grasping point classication in kitchen and ofce scenarios : ( left column ) top ve grasps predicted by the grasp classier alone .
( right column ) top two grasps for three different object - types , predicted by the grasp classier when given the object types and locations .
the red points in each image are the predicted grasp locations .
( best viewed in
in particular , we asked a person to place several objects in front of the stair 123 robot .
the bowls were placed upright at a random location on a table ( with height unknown to the robot ) , and the plates were stacked neatly in a rack ( also in a random location ) .
using our algorithm trained on a dataset containing the ve synthetic objects described in section 123 , the robot chose the best grasp predicted from the image , and attempted to pick up the object ( figure 123 ) .
it achieved a grasping success rate of 123% for cereal bowls , and 123% for plates ( 123 trials for each object ) .
we proposed an algorithm for enabling a robot to grasp an object that it has never seen before .
our learning algorithm neither tries to build , nor requires , a 123 - d model of the ob - ject .
instead it predicts , directly as a function of the images , a point at which to grasp the object .
in our experiments , the algorithm generalizes very well to novel objects and environ - ments , and our robot successfully grasped a wide variety of objects in different environments such as dishwashers , ofce
the ability to pick up novel objects represents perhaps a tiny rst step towards the stair projects larger goal of en - abling robots to perform a large variety of household tasks , such as fetching an item in response to a verbal request , tidy - ing up a room , and preparing simple meals in a kitchen .
in the short term , we are working on applying variations of the
figure 123 : barrett arm grasping an object using our algo -
algorithms described in this paper to try to enable stair to prepare simple meals using a normal home kitchen .
we give warm thanks to morgan quigley for help with the robotic hardware and communication software .
we also thank justin kearns , lawson wong , david ho , chioma os - ondu , and brad gulko for help in running the experiments .
this work was supported by the darpa transfer learning program under contract number fa123 - 123 - 123 - 123 , and by the national science foundation under award number cns -
appendix : predicting orientation in ( saxena et al . , 123b ) , we presented an algorithm for pre - dicting the 123 - d orientation of an object from its image .
here , for the sake of simplicity , we will present the learning al - gorithm in the context of grasping using our 123 - dof arm on
as discussed in section 123 , our task is to predict the 123 - d wrist orientation of the gripper ( at the predicted grasping point ) given the image .
for example , given a picture of a closed book ( which we would like to grasp at its edge ) , we should choose an orientation in which the robots two ngers are parallel to the books surfaces , rather than perpendicular to the books cover .
since our robotic arm has a parallel plate gripper compris - ing two ngers that close in parallel , a rotation of results in similar conguration of the gripper .
this results in a discon - tinuity at = , in that the orientation of the gripper at = is equivalent to = 123
therefore , to handle this symmetry , we will represent angles via y ( ) = ( cos ( 123 ) , sin ( 123 ) ) r123
thus , y ( + n ) = ( cos ( 123 + 123n ) , sin ( 123 + 123n ) ) = y ( ) .
now , given images features x , we model the conditional
distribution of y as a multi - variate gaussian :
p ( y|x; w , k ) = ( 123 ) n / 123|k|123 / 123
( y wt x ) t k ( y wt x ) ( cid : 123 )
where k 123 is a covariance matrix .
the parameters of this model w and k are learnt by maximizing the conditional log
likelihood logqi p ( yi|xi; w ) .
now when given a new image , our map estimate for y is
given as follows .
since ||y||123 = 123 , we will choose
y = arg max
log p ( y|x; w , k ) = arg max
for q = wt x .
( this derivation assumed k = 123i for some 123 , which will roughly hold true if is chosen uniformly in the training set . ) the closed form solution of this is y =
in our robotic experiments , typically 123 accuracy is re - quired to successfully grasp an object , which our algorithm almost always attains .
in an example , figure 123 shows the predicted orientation for a pen .
figure 123 : predicted orientation at the grasping point for pen .
dotted line represents the true orientation , and solid line rep - resents the predicted orientation .
