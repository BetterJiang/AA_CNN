we consider learning in a markov decision process where we are not explicitly given a re - ward function , but where instead we can ob - serve an expert demonstrating the task that we want to learn to perform .
this setting is useful in applications ( such as the task of driving ) where it may be di ( cid : 123 ) cult to write down an explicit reward function specifying exactly how di ( cid : 123 ) erent desiderata should be traded o ( cid : 123 ) .
we think of the expert as try - ing to maximize a reward function that is ex - pressible as a linear combination of known features , and give an algorithm for learning the task demonstrated by the expert .
our al - gorithm is based on using \inverse reinforce - ment learning " to try to recover the unknown reward function .
we show that our algorithm terminates in a small number of iterations , and that even though we may never recover the experts reward function , the policy out - put by the algorithm will attain performance close to that of the expert , where here per - formance is measured with respect to the ex - perts unknown reward function .
given a sequential decision making problem posed in the markov decision process ( mdp ) formalism , a num - ber of standard algorithms exist for ( cid : 123 ) nding an optimal or near - optimal policy .
in the mdp setting , we typi - cally assume that a reward function is given .
given a reward function and the mdps state transition prob - abilities , the value function and optimal policy are ex - the mdp formalism is useful for many problems be - cause it is often easier to specify the reward function than to directly specify the value function ( and / or op - timal policy ) .
however , we believe that even the re - ward function is frequently di ( cid : 123 ) cult to specify manu - ally .
consider , for example , the task of highway driv - ing .
when driving , we typically trade o ( cid : 123 ) many dif -
appearing in proceedings of the 123 st international confer - ence on machine learning , ban ( cid : 123 ) , canada , 123
copyright 123 by the authors .
ferent desiderata , such as maintaining safe following distance , keeping away from the curb , staying far from any pedestrians , maintaining a reasonable speed , per - haps a slight preference for driving in the middle lane , not changing lanes too often , and so on .
to specify a reward function for the driving task , we would have to assign a set of weights stating exactly how we would like to trade o ( cid : 123 ) these di ( cid : 123 ) erent factors .
despite being able to drive competently , the authors do not believe they can con ( cid : 123 ) dently specify a speci ( cid : 123 ) c reward function for the task of \driving well . " 123 in practice , this means that the reward function is of - ten manually tweaked ( cf .
reward shaping , ng et al . , 123 ) until the desired behavior is obtained .
from con - versations with engineers in industry and our own ex - perience in applying reinforcement learning algorithms to several robots , we believe that , for many problems , the di ( cid : 123 ) culty of manually specifying a reward function represents a signi ( cid : 123 ) cant barrier to the broader appli - cability of reinforcement learning and optimal control when teaching a young adult to drive , rather than telling them what the reward function is , it is much easier and more natural to demonstrate driving to them , and have them learn from the demonstration .
the task of learning from an expert is called appren - ticeship learning ( also learning by watching , imitation learning , or learning from demonstration ) .
a number of approaches have been proposed for ap - prenticeship learning in various applications .
most of these methods try to directly mimic the demonstrator by applying a supervised learning algorithm to learn a direct mapping from the states to the actions .
this literature is too wide to survey here , but some ex - amples include sammut et al .
( 123 ) ; kuniyoshi et al .
( 123 ) ; demiris & hayes ( 123 ) ; amit & mataric ( 123 ) ; pomerleau ( 123 ) .
one notable exception is given in atkeson & schaal ( 123 ) .
they considered the
123we note that this is true even though the reward func - tion may often be easy to state in english .
for instance , the \true " reward function that we are trying to maximize when driving is , perhaps , our \personal happiness . " the practical problem however is how to model this ( i . e . , our happiness ) explicitly as a function of the problems states , so that a reinforcement learning algorithm can be applied .
problem of having a robot arm follow a demonstrated trajectory , and used a reward function that quadrat - ically penalizes deviation from the desired trajectory .
note however , that this method is applicable only to problems where the task is to mimic the experts tra - jectory .
for highway driving , blindly following the ex - perts trajectory would not work , because the pattern of tra ( cid : 123 ) c encountered is di ( cid : 123 ) erent each time .
given that the entire ( cid : 123 ) eld of reinforcement learning is founded on the presupposition that the reward func - tion , rather than the policy or the value function , is the most succinct , robust , and transferable de ( cid : 123 ) nition of the task , it seems natural to consider an approach to apprenticeship learning whereby the reward function is the problem of deriving a reward function from ob - served behavior is referred to as inverse reinforcement learning ( ng & russell , 123 ) .
in this paper , we assume that the expert is trying ( without necessar - ily succeeding ) to optimize an unknown reward func - tion that can be expressed as a linear combination of known \features . " even though we cannot guarantee that our algorithms will correctly recover the experts true reward function , we show that our algorithm will nonetheless ( cid : 123 ) nd a policy that performs as well as the expert , where performance is measured with respect to the experts unknown reward function .
a ( ( cid : 123 ) nite - state ) markov decision process ( mdp ) is a tu - ple ( s; a; t; ( cid : 123 ) ; d; r ) , where s is a ( cid : 123 ) nite set of states; a is a set of actions; t = fpsag is a set of state transition probabilities ( here , psa is the state transition distribu - tion upon taking action a in state s ) ; ( cid : 123 ) 123 ( 123; 123 ) is a discount factor; d is the initial - state distribution , from which the start state s123 is drawn; and r : s 123 ! a is the reward function , which we assume to be bounded in absolute value by 123
we let mdpnr denote an mdp without a reward function , i . e . , a tuple of the form ( s; a; t; ( cid : 123 ) ; d ) .
we assume that there is some vector of features ( cid : 123 ) : s ! ( 123; 123 ) k over states , and that there is some \true " reward function r ( cid : 123 ) ( s ) = w ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( s ) , where w ( cid : 123 ) 123 rk
123a related idea is also seen in the biomechanics and cog - nitive science , where researchers have pointed out that sim - ple reward functions ( usually ones constructed by hand ) of - ten su ( cid : 123 ) ce to explain complicated behavior ( policies ) .
ex - amples include the minimum jerk principle to explain limb movement in primates ( hogan , 123 ) , and the minimum torque - change model to explain trajectories in human mul - tijoint arm movement . ( uno et al . , 123 ) related examples are also found in economics and some other literatures .
( see the discussion in ng & russell , 123 )
123the case of state - action rewards r ( s; a ) o ( cid : 123 ) ers no ad - ditional di ( cid : 123 ) culties; using features of the form ( cid : 123 ) : s ( cid : 123 ) a ! ( 123; 123 ) k , and our algorithms still apply straightforwardly .
in order to ensure that the rewards are bounded by 123 , we also assume kw ( cid : 123 ) k123 ( cid : 123 ) 123
in the driving domain , ( cid : 123 ) might be a vector of features indicating the di ( cid : 123 ) erent desiderata in driving that we would like to trade o ( cid : 123 ) , such as whether we have just collided with another car , whether were driving in the middle lane , and so on .
the ( unknown ) vector w ( cid : 123 ) speci ( cid : 123 ) es the relative weighting between these desiderata .
a policy ( cid : 123 ) is a mapping from states to probability distributions over actions .
the value of a policy ( cid : 123 ) is
es123 ( cid : 123 ) d ( v ( cid : 123 ) ( s123 ) ) = e ( p123 = w ( cid : 123 ) e ( p123
t=123 ( cid : 123 ) tw ( cid : 123 ) ( cid : 123 ) ( st ) j ( cid : 123 ) )
here , the expectation is taken with respect to the ran - dom state sequence s123; s123; : : : drawn by starting from a state s123 ( cid : 123 ) d , and picking actions according to ( cid : 123 ) .
we de ( cid : 123 ) ne the expected discounted accumulated fea - ture value vector ( cid : 123 ) ( ( cid : 123 ) ) , or more succinctly the feature expectations , to be
( cid : 123 ) ( ( cid : 123 ) ) = e ( p123
t=123 ( cid : 123 ) t ( cid : 123 ) ( st ) j ( cid : 123 ) ) 123 rk :
using this notation , the value of a policy may be writ - ten es123 ( cid : 123 ) d ( v ( cid : 123 ) ( s123 ) ) = w ( cid : 123 ) ( cid : 123 ) ( ( cid : 123 ) ) .
given that the reward r is expressible as a linear combination of the fea - tures ( cid : 123 ) , the feature expectations for a given policy ( cid : 123 ) completely determine the expected sum of discounted rewards for acting according to that policy .
let ( cid : 123 ) denote the set of stationary policies for an mdp .
given two policies ( cid : 123 ) 123; ( cid : 123 ) 123 123 ( cid : 123 ) , we can construct a new policy ( cid : 123 ) 123 by mixing them together .
speci ( cid : 123 ) cally , imag - ine that ( cid : 123 ) 123 operates by ( cid : 123 ) ipping a coin with bias ( cid : 123 ) , and with probability ( cid : 123 ) picks and always acts according to ( cid : 123 ) 123 , and with probability 123 ( cid : 123 ) ( cid : 123 ) always acts according to ( cid : 123 ) 123
from linearity of expectation , clearly we have that ( cid : 123 ) ( ( cid : 123 ) 123 ) = ( cid : 123 ) ( cid : 123 ) ( ( cid : 123 ) 123 ) + ( 123 ( cid : 123 ) ( cid : 123 ) ) ( cid : 123 ) ( ( cid : 123 ) 123 ) .
note that the randomization step selecting between ( cid : 123 ) 123 and ( cid : 123 ) 123 occurs only once at the start of a trajectory , and not on ev - ery step taken in the mdp .
more generally , if we have found some set of policies ( cid : 123 ) 123; : : : ; ( cid : 123 ) d , and want to ( cid : 123 ) nd a new policy whose feature expectations vector is a convex combination pn i=123 ( cid : 123 ) i ( cid : 123 ) ( ( cid : 123 ) i ) ( ( cid : 123 ) i ( cid : 123 ) 123;pi ( cid : 123 ) i = 123 ) of these policies , then we can do so by mixing together the policies ( cid : 123 ) 123; : : : ; ( cid : 123 ) d , where the probability of picking ( cid : 123 ) i is given by ( cid : 123 ) i .
we assume access to demonstrations by some expert ( cid : 123 ) e .
speci ( cid : 123 ) cally , we assume the ability to observe trajectories ( state sequences ) generated by the expert starting from s123 ( cid : 123 ) d and taking actions according to ( cid : 123 ) e .
it may be helpful to think of the ( cid : 123 ) e as the optimal policy under the reward function r ( cid : 123 ) = w ( cid : 123 ) t ( cid : 123 ) , though we do not require this to hold .
for our algorithm , we will require an estimate of the experts feature expectations ( cid : 123 ) e = ( cid : 123 ) ( ( cid : 123 ) e ) .
speci ( cid : 123 ) -
cally , given a set of m trajectories fs ( i ) 123 ; : : : gm generated by the expert , we denote the empirical esti - mate for ( cid : 123 ) e by123
123 ; s ( i )
^ ( cid : 123 ) e = 123
in the sequel , we also assume access to a reinforcement learning ( rl ) algorithm that can be used to solve an mdpnr augmented with a reward function r = wt ( cid : 123 ) .
for simplicity of exposition , we will assume that the rl algorithm returns the optimal policy .
the general - ization to approximate rl algorithms o ( cid : 123 ) ers no special di ( cid : 123 ) culties; see the full paper .
( abbeel & ng , 123 )
the problem is the following : given an mdpnr , a feature mapping ( cid : 123 ) and the experts feature expecta - tions ( cid : 123 ) e , ( cid : 123 ) nd a policy whose performance is close to that of the experts , on the unknown reward function r ( cid : 123 ) = w ( cid : 123 ) t ( cid : 123 ) .
to accomplish this , we will ( cid : 123 ) nd a policy ~ ( cid : 123 ) such that k ( cid : 123 ) ( ~ ( cid : 123 ) ) ( cid : 123 ) ( cid : 123 ) ek123 ( cid : 123 ) ( cid : 123 ) .
for such a ~ ( cid : 123 ) , we would have that for any w 123 rk ( kwk123 ( cid : 123 ) 123 ) ,
t=123 ( cid : 123 ) tr ( st ) j ( cid : 123 ) e ) ( cid : 123 ) e ( p123 = jwt ( cid : 123 ) ( ~ ( cid : 123 ) ) ( cid : 123 ) wt ( cid : 123 ) ej ( cid : 123 ) kwk123k ( cid : 123 ) ( ~ ( cid : 123 ) ) ( cid : 123 ) ( cid : 123 ) ek123 ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 ) = ( cid : 123 )
the ( cid : 123 ) rst inequality follows from the fact that jxt yj ( cid : 123 ) kxk123kyk123 , and the second from kwk123 ( cid : 123 ) kwk123 ( cid : 123 ) 123
so the problem is reduced to ( cid : 123 ) nding a policy ~ ( cid : 123 ) that induces feature expectations ( cid : 123 ) ( ~ ( cid : 123 ) ) close to ( cid : 123 ) e .
our apprenticeship learning algorithm for ( cid : 123 ) nding such a policy ~ ( cid : 123 ) is as follows :
randomly pick some policy ( cid : 123 ) ( 123 ) , compute ( or approx - imate via monte carlo ) ( cid : 123 ) ( 123 ) = ( cid : 123 ) ( ( cid : 123 ) ( 123 ) ) , and set i = 123
compute t ( i ) = maxw : kwk123 ( cid : 123 ) 123 minj123f123 : : ( i ( cid : 123 ) 123 ) g wt ( ( cid : 123 ) e ( cid : 123 ) ( cid : 123 ) ( j ) ) , and let w ( i ) be the value of w that attains this
if t ( i ) ( cid : 123 ) ( cid : 123 ) , then terminate .
using the rl algorithm , compute the optimal policy
( cid : 123 ) ( i ) for the mdp using rewards r = ( w ( i ) ) t ( cid : 123 ) .
compute ( or estimate ) ( cid : 123 ) ( i ) = ( cid : 123 ) ( ( cid : 123 ) ( i ) ) .
set i = i + 123 , and go back to step 123
upon termination , the algorithm returns f ( cid : 123 ) ( i ) : i = 123 : : : ng .
let us examine the algorithm in detail .
on iteration i , we have already found some policies ( cid : 123 ) ( 123 ) ; : : : ; ( cid : 123 ) ( i ( cid : 123 ) 123 ) .
the optimization in step 123 can be viewed as an inverse reinforcement learning step in which we try to guess
123in practice we truncate the trajectories after a ( cid : 123 ) nite if h = h ( cid : 123 ) = log ( cid : 123 ) ( ( cid : 123 ) ( 123 ( cid : 123 ) ( cid : 123 ) ) ) is the number h of steps .
( cid : 123 ) - horizon time , then this introduces at most ( cid : 123 ) error into
figure 123
three iterations for max - margin algorithm .
the reward function being optimized by the expert .
the maximization in that step is equivalently written
wt ( cid : 123 ) e ( cid : 123 ) wt ( cid : 123 ) ( j ) + t; j = 123; : : : ; i ( cid : 123 ) 123 jjwjj123 ( cid : 123 ) 123
( s123 ) ) + t .
from eq .
( 123 ) , we see the algorithm is trying to ( cid : 123 ) nd a reward function r = w ( i ) ( cid : 123 ) ( cid : 123 ) such that es123 ( cid : 123 ) d ( v ( cid : 123 ) e ( s123 ) ) ( cid : 123 ) es123 ( cid : 123 ) d ( v ( cid : 123 ) ( i ) i . e . , a re - ward on which the expert does better , by a \margin " of t , than any of the i policies we had found previously .
this step is similar to one used in ( ng & russell , 123 ) , but unlike the algorithms given there , because of the 123 - norm constraint on w it cannot be posed as a linear program ( lp ) , but only as a quadratic program . 123 familiar with support vector machines ( svms ) will also recognize this optimization as be - ing equivalent to ( cid : 123 ) nding the maximum margin hyper - plane separating two sets of points .
( vapnik , 123 ) the equivalence is obtained by associating a label 123 with the experts feature expectations ( cid : 123 ) e , and a label ( cid : 123 ) 123 with the feature expectations f ( cid : 123 ) ( ( cid : 123 ) ( j ) ) : j = 123 : : ( i ( cid : 123 ) 123 ) g .
the vector w ( i ) we want is the unit vector orthogonal to the maximum margin separating hyperplane .
so , an svm solver can also be used to ( cid : 123 ) nd w ( i ) .
( the svm problem is a quadratic programming problem ( qp ) , so we can also use any generic qp solver . ) in figure 123 we show an example of what the ( cid : 123 ) rst three iterations of the algorithm could look like geo - metrically .
shown are several example ( cid : 123 ) ( ( cid : 123 ) ( i ) ) , and the w ( i ) s given by di ( cid : 123 ) erent iterations of the algorithm .
now , suppose the algorithm terminates , with t ( n+123 ) ( cid : 123 ) ( cid : 123 ) .
( whether the algorithm terminates is discussed in section 123 ) then directly from eq .
( 123 - 123 ) we have :
123w with kwk123 ( cid : 123 ) 123 123i s . t .
wt ( cid : 123 ) ( i ) ( cid : 123 ) wt ( cid : 123 ) e ( cid : 123 ) ( cid : 123 ) :
since jjw ( cid : 123 ) jj123 ( cid : 123 ) jjw ( cid : 123 ) jj123 ( cid : 123 ) 123 , this means that there is at least one policy from the set returned by the al - gorithm , whose performance under r ( cid : 123 ) is at least as good as the experts performance minus ( cid : 123 ) .
thus , at this stage , we can ask the agent designer to manually test / examine the policies found by the algorithm , and
123although we previously assumed that the w ( cid : 123 ) specify - ing the \true " rewards statisfy kw ( cid : 123 ) k123 ( cid : 123 ) 123 ( and our theo - retical results will use this assumption ) , we still implement the algorithm using kwk123 ( cid : 123 ) 123 , as in eq
pick one with acceptable performance .
a slight exten - sion of this method ensures that the agent designer has to examine at most k + 123 , rather than all n + 123 , di ( cid : 123 ) erent policies ( see footnote 123 ) .
if we do not wish to ask for human help to select a policy , alternatively we can ( cid : 123 ) nd the point closest to ( cid : 123 ) e in the convex closure of ( cid : 123 ) ( 123 ) ; : : : ; ( cid : 123 ) ( n ) by solving the following qp :
min jj ( cid : 123 ) e ( cid : 123 ) ( cid : 123 ) jj123; s : t : ( cid : 123 ) = pi ( cid : 123 ) i ( cid : 123 ) ( i ) ; ( cid : 123 ) i ( cid : 123 ) 123;pi ( cid : 123 ) i = 123 : because ( cid : 123 ) e is \separated " from the points ( cid : 123 ) ( i ) by a margin of at most ( cid : 123 ) , we know that for the solution ( cid : 123 ) we have jj ( cid : 123 ) e ( cid : 123 ) ( cid : 123 ) jj123 ( cid : 123 ) ( cid : 123 ) .
further , by \mixing " together the policies ( cid : 123 ) ( i ) according to the mixture weights ( cid : 123 ) i as discussed previously , we obtain a policy whose feature expectations are given by ( cid : 123 ) .
following our previous discussion ( eq .
123 - 123 ) , this policy attains performance near that of the experts on the unknown reward func - note that although we called one step of our algorithm an inverse rl step , our algorithm does not necessarily recover the underlying reward function correctly .
the performance guarantees of our algorithm only depend on ( approximately ) matching the feature expectations , not on recovering the true underlying reward function .
a simpler algorithm the algorithm described above requires access to a qp ( or svm ) solver .
it is also possible to change the algorithm so that no qp solver is needed .
we will call the previous , qp - based , algorithm the max - margin method , and the new algorithm the projec - tion method .
brie ( cid : 123 ) y , the projection method replaces step 123 of the algorithm with the following :
- set ( cid : 123 ) ( cid : 123 ) ( i ( cid : 123 ) 123 ) =
( cid : 123 ) ( cid : 123 ) ( i ( cid : 123 ) 123 ) + ( ( cid : 123 ) ( i ( cid : 123 ) 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( i ( cid : 123 ) 123 ) ) t ( ( cid : 123 ) e ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( i ( cid : 123 ) 123 ) ) ( ( cid : 123 ) ( i ( cid : 123 ) 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( i ( cid : 123 ) 123 ) ) t ( ( cid : 123 ) ( i ( cid : 123 ) 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( i ( cid : 123 ) 123 ) ) ( this computes the orthogonal projection of ( cid : 123 ) e onto the line through ( cid : 123 ) ( cid : 123 ) ( i ( cid : 123 ) 123 ) and ( cid : 123 ) ( i ( cid : 123 ) 123 ) . ) - set w ( i ) = ( cid : 123 ) e ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( i ( cid : 123 ) 123 ) - set t ( i ) = k ( cid : 123 ) e ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( i ( cid : 123 ) 123 ) k123
in the ( cid : 123 ) rst iteration , we also set w ( 123 ) = ( cid : 123 ) e ( cid : 123 ) ( cid : 123 ) ( 123 ) and ( cid : 123 ) ( cid : 123 ) ( 123 ) = ( cid : 123 ) ( 123 ) .
the full justi ( cid : 123 ) cation for this method is deferred to the full paper ( abbeel and ng , 123 ) , but in sections 123 and 123 we will also give convergence results for it , and empirically compare it to the max - margin
123in k - dimensional space , any point that is a convex combination of a set of n points , with n > k + 123 , can be written as a convex combination of a subset of only k + 123 points of the original n points ( caratheodorys theorem , rockafeller , 123 ) .
applying this to ( cid : 123 ) = i=123 we ob - tain a set of k + 123 policies which is equally close to the experts feature expectations and thus have same perfor - mance guarantees .
( co denotes convex hull . )
i=123 jj ( cid : 123 ) e ( cid : 123 ) ( cid : 123 ) jj123 and f ( cid : 123 ) ( ( cid : 123 ) ( i ) ) gn
figure 123
three iterations for projection algorithm .
algorithm .
an example showing three iterations of the projection method is shown in figure 123
theoretical results most of the results in the previous section were predi - cated on the assumption that the algorithm terminates with t ( cid : 123 ) ( cid : 123 ) .
if the algorithm sometimes does not ter - minate , or if it sometimes takes a very ( perhaps ex - ponentially ) large number of iterations to terminate , then it would not be useful .
the following shows that this is not the case .
theorem 123
let an mdpnr , features ( cid : 123 ) : s 123 ! ( 123; 123 ) k , and any ( cid : 123 ) > 123 be given .
then the apprenticeship learn - ing algorithm ( both max - margin and projection ver - sions ) will terminate with t ( i ) ( cid : 123 ) ( cid : 123 ) after at most
n = o ( cid : 123 )
the previous result ( and all of section 123 ) had assumed that ( cid : 123 ) e was exactly known or calculated .
tice , it has to be estimated from monte carlo samples ( eq .
we can thus ask about the sample complex - ity of this algorithm; i . e . , how many trajectories m we must observe of the expert before we can guarantee we will approach its performance .
theorem 123
let an mdpnr , features ( cid : 123 ) : s 123 ! ( 123; 123 ) k , and any ( cid : 123 ) > 123; ( cid : 123 ) > 123 be given .
suppose the appren - ticeship learning algorithm ( either max - margin or pro - jection version ) is run using an estimate ^ ( cid : 123 ) e for ( cid : 123 ) e obtained by m monte carlo samples .
in order to en - sure that with probability at least 123 ( cid : 123 ) ( cid : 123 ) the algorithm terminates after at most a number of iterations n given by eq .
( 123 ) , and outputs a policy ~ ( cid : 123 ) so that for any true reward r ( cid : 123 ) ( s ) = w ( cid : 123 ) t ( cid : 123 ) ( s ) ( kw ( cid : 123 ) k123 ( cid : 123 ) 123 ) we have it su ( cid : 123 ) ces that
t=123 ( cid : 123 ) tr ( cid : 123 ) ( st ) j ~ ( cid : 123 ) ) ( cid : 123 ) e ( p123
t=123 ( cid : 123 ) tr ( cid : 123 ) ( st ) j ( cid : 123 ) e ) ( cid : 123 ) ( cid : 123 ) ; ( 123 )
( ( cid : 123 ) ( 123 ( cid : 123 ) ( cid : 123 ) ) ) 123 log 123k
the proofs of these theorems are in appendix a .
in the case where the true reward function r ( cid : 123 ) does not lie exactly in the span of the basis functions ( cid : 123 ) , the algorithm still enjoys a graceful degradation of perfor - mance .
speci ( cid : 123 ) cally , if r ( cid : 123 ) ( s ) = w ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( s ) + " ( s ) for
some residual ( error ) term " ( s ) , then our algorithm will have performance that is worse than the experts by no more than o ( k " k123 ) .
in our ( cid : 123 ) rst set of experiments , we used 123 by 123 gridworlds with multiple / sparse rewards .
the reward is not known to the algorithm , but we can sample tra - jectories from an experts ( optimal ) policy .
the agent has four actions to try to move in each of the four com - pass directions , but with 123% chance an action fails and results in a random move .
the grid is divided into non - overlapping regions of 123 by 123 cells; we call these 123x123 regions \macrocells . " a small number of the re - sulting 123 macrocells have positive rewards .
for each value of i = 123; : : : ; 123 , there is one feature ( cid : 123 ) i ( s ) indi - cating whether that state s is in macrocell i .
thus , the rewards may be written r ( cid : 123 ) = ( w ( cid : 123 ) ) t ( cid : 123 ) .
the weights w ( cid : 123 ) are generated randomly so as to give sparse rewards , which leads to fairly interesting / rich optimal policies . 123 in the basic version , the algorithm is run using the 123 - dimensional features ( cid : 123 ) .
we also tried a version in which the algorithm knows exactly which macrocells have non - zero rewards ( but not their values ) , so that the dimension of ( cid : 123 ) is reduced to contain only features corresponding to non - zero rewards .
in figure 123 , we compare the max - margin and projec - tion versions of the algorithm , when ( cid : 123 ) e is known ex - actly .
we plot the margin t ( i ) ( distance to experts pol - icy ) vs .
the number of iterations , using all 123 macro - cells as features .
the experts policy is the optimal policy with respect to the given mdp .
the two al - gorithms exhibited fairly similar rates of convergence , with the projection version doing slightly better .
the second set of experiments illustrates the perfor - mance of the algorithm as we vary the number m of sampled expert trajectories used to estimate ( cid : 123 ) e .
the performance measure is the value of the best policy in the set output by the algorithm .
we ran the al - gorithm once using all 123 features , and once using only the features that truly correspond to non - zero rewards . 123 we also report on the performance of three
123details : we used ( cid : 123 ) = 123 : 123 , so the expected horizon is of the order of the gridsize .
the true reward function was generated as follows : for each macrocell i ( i = 123; : : : ; 123 ) , with probability 123 the reward there is zero ( w ( cid : 123 ) i = 123 ) , and with probability 123 a weight w ( cid : 123 ) i is sampled uniformly from ( 123 , 123 ) .
finally , w ( cid : 123 ) is renormalized so that kw ( cid : 123 ) k123 = instances with fewer than two non - zero entries in w ( cid : 123 ) are non - interesting and were discarded .
the initial state distribution is uniform over all states .
123note that , as in the text , our apprenticeship learning algorithm assumes the ability to call a reinforcement learn - ing subroutine ( in this case , an exact mdp solver using in these experiments , we are interested
number of iterations
figure 123
a comparison of the convergence speeds of the max - margin and projection versions of the algorithm on a 123x123 grid .
euclidean distance to the experts feature expectations is plotted as a function of the number of it - erations .
we rescaled the feature expectations by ( 123 ( cid : 123 ) ( cid : 123 ) ) such that they are in ( 123; 123 ) k .
the plot shows averages over 123 runs , with 123 s . e .
errorbars .
irl only nonzero weight features irl all features parameterized policy stochastic parameterized policy majority vote mimic the expert
log123 ( number of sample trajectories )
figure 123
plot of performance vs .
number of sampled tra - jectories from the expert .
( shown in color , where avail - able . ) averages over 123 instances are plotted , with 123 s . e .
errorbars .
note the base - 123 logarithm scale on the x - axis .
other simple algorithms .
the \mimic the expert " al - gorithm picks whatever action the expert had taken if it ( cid : 123 ) nds itself in a state in which it had previously observed the expert , and picks an action randomly oth - erwise .
the \parameterized policy stochastic " uses a stochastic policy , with the probability of each action constant over each macrocell and set to the empiri - cal frequency observed for the expert in the macrocell .
the \parameterized policy majority vote " algorithm takes deterministically the most frequently observed action in the macrocell .
results are shown in fig - ure 123
using our algorithm , only a few sampled expert trajectories|far fewer than for the other methods| are needed to attain performance approaching that of ( note log scale on x - axis . ) 123 thus , by
mainly in the question of how many times an expert must demonstrate a task before we learn to perform the same task .
in particular , we do not rely on the experts demon - strations to learn the state transition probabilities .
123the parameterized policies never reach the experts performance , because their policy class is not rich enough .
their restricted policy class is what makes them do better
with other cars; we also prefer the right lane over the middle lane over the left lane , over driving
nasty : hit as many other cars as possible .
right lane nice : drive in the right lane , but go
o ( cid : 123 ) - road to avoid hitting cars in the right lane .
right lane nasty : drive o ( cid : 123 ) - road on the right , but get back onto the road to hit cars in the right lane .
middle lane : drive in the middle lane , ignoring all other cars ( thus crashing into all other cars in the middle lane ) .
after each style was demonstrated to the algorithm ( by one of the authors driving in the simulator for 123 minutes ) , apprenticeship learning was used to try to ( cid : 123 ) nd a policy that mimics demonstrated style .
videos of the demonstrations and of the resulting learned poli - cies are available at
in every instance , the algorithm was qualitatively able to mimic the demonstrated driving style .
since no \true " reward was ever speci ( cid : 123 ) ed or used in the experi - ments , we cannot report on the results of the algorithm according to r ( cid : 123 ) .
however , table 123 shows , for each of the ( cid : 123 ) ve driving styles , the feature expectations of the expert ( as estimated from the 123 minute demonstra - tion ) , and the feature expectations of the learned con - troller for the more interesting features .
also shown are the weights w used to generate the policy shown .
while our theory makes no guarantee about any set of weights w found , we note that the values there gener - ally make intuitive sense .
for instance , in the ( cid : 123 ) rst driving style , we see negative rewards for collisions and for driving o ( cid : 123 ) road , and larger positive rewards for driving in the right lane than for the other lanes .
discussion and conclusions we assumed access to demonstrations by an expert that is trying to maximize a reward function express - ible as a linear combination of known features , and pre - sented an algorithm for apprenticeship learning .
our method is based on inverse reinforcement learning , ter - minates in a small number of iterations , and guaran - tees that the policy found will have performance com - parable to or better than that of the expert , on the experts unknown reward function .
our algorithm assumed the reward function is express - ible as a linear function of known features .
if the set of features is su ( cid : 123 ) ciently rich , this assumption is fairly unrestrictive .
( in the extreme case where there is a separate feature for each state - action pair , fully gen - eral reward functions can be learned . ) however , it remains an important problem to develop methods for learning reward functions that may be non - linear func - tions of the features , and to incorporate automatic fea -
figure 123
screenshot of driving simulator .
learning a compact representation of the reward func - tion , our algorithm signi ( cid : 123 ) cantly outperforms the other methods .
we also observe that when the algorithm is told in advance which features have non - zero weight in the true reward function , it is able to learn using fewer expert trajectories .
car driving simulation for our second experiment , we implemented a car - driving simulation , and applied apprenticeship learn - ing to try to learn di ( cid : 123 ) erent \driving styles . " a screen - shot of our simulator is shown in figure 123
we are driving on a highway at 123m / s ( 123mph ) , which is faster than all the other cars .
the mdp has ( cid : 123 ) ve di ( cid : 123 ) erent ac - tions , three of which cause the car to steer smoothly to one of the lanes , and two of which cause us to drive o ( cid : 123 ) ( but parallel to ) the road , on either the left or the right side .
because our speed is ( cid : 123 ) xed , if we want to avoid hitting other cars it is sometimes necessary to drive o ( cid : 123 ) the road .
the simulation runs at 123hz , and in the experiments that follow , the experts features were estimated from a single trajectory of 123 samples ( corresponding to 123 minutes of driving time ) .
there were features in - dicating what lane the car is currently in ( including o ( cid : 123 ) road - left and o ( cid : 123 ) road - right , for a total of ( cid : 123 ) ve fea - tures ) , and the distance of the closest car in the current lane . 123 note that a distance of 123 from the nearest car implies a collision .
when running the apprenticeship learning algorithm , the step in which reinforcement learning was required was implemented by solving a discretized version of the problem .
in all of our exper - iments , the algorithm was run for 123 iterations , and a policy was selected by inspection ( per the discussion in section 123 ) .
we wanted to demonstrate a variety of di ( cid : 123 ) erent driv - ing styles ( some corresponding to highly unsafe driv - ing ) to see if the algorithm can mimic the same \style " in every instance .
we considered ( cid : 123 ) ve styles :
nice : the highest priority is to avoid collisions
than the \mimic the expert " algorithm initially .
123more precisely , we used the distance to the single clos - est car in its current lane , discretized to the nearest car length between - 123 to +123 , for a total of 123 features .
table 123
feature expectations of teacher ^ ( cid : 123 ) e and of selected / learned policy ( cid : 123 ) ( ~ ( cid : 123 ) ) ( as estimated by monte carlo ) .
and weights w corresponding to the reward function that had been used to generate the policy shown .
( note for compactness , only 123 of the more interesting features , out of a total of 123 features , are shown here . )
collision o ( cid : 123 ) road left
rightlane o ( cid : 123 ) road right
ture construction and feature selection ideas into our it might also be possible to derive an alternative ap - prenticeship learning algorithm using the dual to the lp that is used to solve bellmans equations .
( manne , in this lp the variables are the state / action visitation rates , and it is possible to place constraints on the learned policys stationary distribu - tion directly .
while there are few algorithms for ap - proximating this dual ( as opposed to primal ) lp for large mdps and exact solutions would be feasible only for small mdps , we consider this an interesting direc - tion for future work .
acknowledgements .
this work was supported by the department of the interior / darpa under con - tract number nbchd123
