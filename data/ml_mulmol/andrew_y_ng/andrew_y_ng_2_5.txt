single - word vector space models have been very successful at learning lexical informa - tion .
however , they cannot capture the com - positional meaning of longer phrases , prevent - ing them from a deeper understanding of lan - guage .
we introduce a recursive neural net - work ( rnn ) model that learns compositional vector representations for phrases and sen - tences of arbitrary syntactic type and length .
our model assigns a vector and a matrix to ev - ery node in a parse tree : the vector captures the inherent meaning of the constituent , while the matrix captures how it changes the mean - ing of neighboring words or phrases .
this matrix - vector rnn can learn the meaning of operators in propositional logic and natural language .
the model obtains state of the art performance on three different experiments : predicting ne - grained sentiment distributions of adverb - adjective pairs; classifying senti - ment labels of movie reviews and classifying semantic relationships such as cause - effect or topic - message between nouns using the syn - tactic path between them .
semantic word vector spaces are at the core of many useful natural language applications such as search query expansions ( jones et al . , 123 ) , fact extrac - tion for information retrieval ( pasca et al . , 123 ) and automatic annotation of text with disambiguated wikipedia links ( ratinov et al . , 123 ) , among many others ( turney and pantel , 123 ) .
in these mod - els the meaning of a word is encoded as a vector computed from co - occurrence statistics of a word and its neighboring words .
such vectors have been shown to correlate well with human judgments of word similarity ( grifths et al . , 123 ) .
figure 123 : a recursive neural network which learns se - mantic vector representations of phrases in a tree struc - ture .
each word and phrase is represented by a vector and a matrix , e . g . , very = ( a , a ) .
the matrix is applied to neighboring vectors .
the same function is repeated to combine the phrase very good with movie .
despite their success , single word vector models are severely limited since they do not capture com - positionality , the important quality of natural lan - guage that allows speakers to determine the meaning of a longer expression based on the meanings of its words and the rules used to combine them ( frege , 123 ) .
this prevents them from gaining a deeper understanding of the semantics of longer phrases or sentences .
recently , there has been much progress in capturing compositionality in vector spaces , e . g . , ( mitchell and lapata , 123; baroni and zamparelli , 123; zanzotto et al . , 123; yessenalina and cardie , 123; socher et al . , 123c ) ( see related work ) .
we extend these approaches with a more general and powerful model of semantic composition .
we present a novel recursive neural network model for semantic compositionality .
in our context , compositionality is the ability to learn compositional vector representations for various types of phrases and sentences of arbitrary length .
123 shows an illustration of the model in which each constituent ( a word or longer phrase ) has a matrix - vector ( mv )
very good movie . . .
( a , a ) ( b , b ) ( c , c ) recursive matrix - vector modelf ( ba , ab ) = ba= ab= - vector - matrix . . . representation .
the vector captures the meaning of that constituent .
the matrix captures how it modies the meaning of the other word that it combines with .
a representation for a longer phrase is computed bottom - up by recursively combining the words ac - cording to the syntactic structure of a parse tree .
since the model uses the mv representation with a neural network as the nal merging function , we call our model a matrix - vector recursive neural network
we show that the ability to capture semantic com - positionality in a syntactically plausible way trans - lates into state of the art performance on various tasks .
the rst experiment demonstrates that our model can learn ne - grained semantic composition - ality .
the task is to predict a sentiment distribution over movie reviews of adverb - adjective pairs such as unbelievably sad or really awesome .
the mv - rnn is the only model that is able to properly negate sen - timent when adjectives are combined with not .
the mv - rnn outperforms previous state of the art mod - els on full sentence sentiment prediction of movie reviews .
the last experiment shows that the mv - rnn can also be used to nd relationships between words using the learned phrase vectors .
the rela - tionship between words is recursively constructed and composed by words of arbitrary type in the variable length syntactic path between them .
on the associated task of classifying relationships be - tween nouns in arbitrary positions of a sentence the model outperforms all previous approaches on the semeval - 123 task 123 competition ( hendrickx et al . , 123 ) .
it outperforms all but one of the previous ap - proaches without using any hand - designed semantic resources such as wordnet or framenet .
by adding wordnet hypernyms , pos and ner tags our model outperforms the state of the art that uses signicantly more resources .
the code for our model is available
123 mv - rnn : a recursive matrix - vector
the dominant approach for building representations of multi - word units from single word vector repre - sentations has been to form a linear combination of the single word representations , such as a sum or weighted average .
this happens in information re -
trieval and in various text similarity functions based on lexical similarity .
these approaches can work well when the meaning of a text is literally the sum of its parts , but fails when words function as oper - ators that modify the meaning of another word : the meaning of extremely strong cannot be captured as the sum of word representations for extremely
the model of socher et al .
( 123c ) provided a new possibility for moving beyond a linear combi - nation , through use of a matrix w that multiplied the word vectors ( a , b ) , and a nonlinearity function g ( such as a sigmoid or tanh ) .
they compute the parent vector p that describes both words as
and apply this function recursively inside a binarized parse tree so that it can compute vectors for multi - word sequences .
even though the nonlinearity al - lows to express a wider range of functions , it is al - most certainly too much to expect a single xed w matrix to be able to capture the meaning combina - tion effects of all natural language operators .
after all , inside the function g , we have the same linear transformation for all possible pairs of word vectors .
recent work has started to capture the behavior of natural language operators inside semantic vec - tor spaces by modeling them as matrices , which would allow a matrix for extremely to appropri - ately modify vectors for smelly or strong ( ba - roni and zamparelli , 123; zanzotto et al . , 123 ) .
these approaches are along the right lines but so far have been restricted to capture linear functions of pairs of words whereas we would like nonlinear functions to compute compositional meaning repre - sentations for multi - word phrases or full sentences .
the mv - rnn combines the strengths of both of these ideas by ( i ) assigning a vector and a matrix to every word and ( ii ) learning an input - specic , non - linear , compositional function for computing vector and matrix representations for multi - word sequences of any syntactic type .
assigning vector - matrix rep - resentations to all words instead of only to words of one part of speech category allows for greater ex - ibility which benets performance .
if a word lacks operator semantics , its matrix can be an identity ma - trix .
however , if a word acts mainly as an operator ,
such as extremely , its vector can become close to zero , while its matrix gains a clear operator mean - ing , here magnifying the meaning of the modied word in both positive and negative directions .
in this section we describe the initial word rep - resentations , the details of combining two words as well as the multi - word extensions .
this is followed by an explanation of our training procedure .
123 matrix - vector neural word representation we represent a word as both a continuous vector and a matrix of parameters .
we initialize all word vectors x rn with pre - trained 123 - dimensional word vectors from the unsupervised model of col - lobert and weston ( 123 ) .
using wikipedia text , their model learns word vectors by predicting how likely it is for each word to occur in its context .
sim - ilar to other local co - occurrence based vector space models , the resulting word vectors capture syntactic and semantic information .
every word is also asso - ciated with a matrix x .
in all experiments , we ini - tialize matrices as x = i + , i . e . , the identity plus a small amount of gaussian noise .
if the vectors have dimensionality n , then each words matrix has di - mensionality x rnn .
while the initialization is random , the vectors and matrices will subsequently be modied to enable a sequence of words to com - pose a vector that can predict a distribution over se - mantic labels .
henceforth , we represent any phrase or sentence of length m as an ordered list of vector - matrix pairs ( ( a , a ) , .
, ( m , m ) ) , where each pair is retrieved based on the word at that position .
123 composition models for two words we rst review composition functions for two words .
in order to compute a parent vector p from two consecutive words and their respective vectors a and b , mitchell and lapata ( 123 ) give as their most general function : p = f ( a , b , r , k ) , where r is the a - priori known syntactic relation and k is
there are many possible functions f .
for our models , there is a constraint on p which is that it has the same dimensionality as each of the input vectors .
this way , we can compare p easily with its children and p can be the input to a composition with another word .
the latter is a requirement that will become clear in the next section .
this excludes
tensor products which were outperformed by sim - pler weighted addition and multiplication methods in ( mitchell and lapata , 123 ) .
we will explore methods that do not require any manually designed semantic resources as back - ground knowledge k .
no explicit knowledge about the type of relation r is used .
instead we want the model to capture this implicitly via the learned ma - trices .
we propose the following combination func - tion which is input dependent :
p = fa , b ( a , b ) = f ( ba , ab ) = g
where a , b are matrices for single words , the global w rn123n is a matrix that maps both transformed words back into the same n - dimensional space .
the element - wise function g could be simply the identity function but we use instead a nonlinearity such as the sigmoid or hyperbolic tangent tanh .
such a non - linearity will allow us to approximate a wider range of functions beyond purely linear functions .
we can also add a bias term before applying g but omit this for clarity .
rewriting the two transformed vectors as one vector z , we get p = g ( w z ) which is a single layer neural network .
in this model , the word ma - trices can capture compositional effects specic to each word , whereas w captures a general composi -
this function builds upon and generalizes several recent models in the literature .
the most related work is that of ( mitchell and lapata , 123; zan - zotto et al . , 123 ) who introduced and explored the composition function p = ba + ab for word pairs .
this model is a special case of eq .
123 when we set w = ( ii ) ( i . e .
two concatenated identity matri - ces ) and g ( x ) = x ( the identity function ) .
baroni and zamparelli ( 123 ) computed the parent vector of adjective - noun pairs by p = ab , where a is an adjective matrix and b is a vector for a noun .
this cannot capture nouns modifying other nouns , e . g . , disk drive .
this model too is a special case of the above model with b = 123nn .
lastly , the models of ( socher et al . , 123b; socher et al . , 123c; socher et al . , 123a ) as described above are also special cases with both a and b set to the identity matrix .
we will compare to these special cases in our experiments .
ba , which we name the linear matrix - vector re - cursion model ( linear mvr ) .
previously , this model had not been trained for multi - word sequences .
123 talks about alternatives for compositionality .
123 objective functions for training one of the advantages of rnn - based models is that each node of a tree has associated with it a dis - tributed vector representation ( the parent vector p ) which can also be seen as features describing that phrase .
we train these representations by adding on top of each parent node a simple softmax classier to predict a class distribution over , e . g . , sentiment or relationship classes : d ( p ) = softmax ( w labelp ) .
if there are k labels , then d rk is a k - dimensional multinomial distribution .
for the applications below ( excluding logic ) , the corresponding error function e ( s , t , ) that we minimize for a sentence s and its tree t is the sum of cross - entropy errors at all nodes .
the only other methods that use this type of ob - jective function are ( socher et al . , 123b; socher et al . , 123c ) , who also combine it with either a score or reconstruction error .
hence , for compar - isons to other related work , we need to merge vari - ations of computing the parent vector p with this classier .
the main difference is that the mv - rnn has more exibility since it has an input specic re - cursive function fa , b to compute each parent .
the following applications , we will use the softmax classier to predict both sentiment distributions and
let = ( w , wm , w label , l , lm ) be our model pa - rameters and a vector with regularization hyperpa - rameters for all model parameters .
l and lm are the sets of all word vectors and word matrices .
the gra - dient of the overall objective function j becomes :
e ( x , t; )
to compute this gradient , we rst compute all tree nodes ( pi , pi ) from the bottom - up and then take derivatives of the softmax classiers at each node in the tree from the top down .
derivatives are com - puted efciently via backpropagation through struc - ture ( goller and kuchler , 123 ) .
even though the
figure 123 : example of how the mv - rnn merges a phrase with another word at a nonterminal node of a parse tree .
123 recursive compositions of multiple words
this section describes how we extend a word - pair matrix - vector - based compositional model to learn vectors and matrices for longer sequences of words .
the main idea is to apply the same function f to pairs of constituents in a parse tree .
for this to work , we need to take as input a binary parse tree of a phrase or sentence and also compute matrices at each nonterminal parent node .
the function f can be readily used for phrase vectors since it is recur - sively compatible ( p has the same dimensionality as its children ) .
for computing nonterminal phrase ma - trices , we dene the function
p = fm ( a , b ) = wm
where wm rn123n , so p rnn just like each
after two words form a constituent in the parse tree , this constituent can now be merged with an - other one by applying the same functions f and fm .
for instance , to compute the vectors and ma - trices depicted in fig .
123 , we rst merge words a and b and their matrices : p123 = f ( ba , ab ) , p123 = fm ( a , b ) .
the resulting vector - matrix pair ( p123 , p123 ) can now be used to compute the full phrase when combining it with word c and computing p123 = f ( cp123 , p123c ) , p123 = fm ( p123 , c ) .
the model com - putes vectors and matrices in a bottom - up fashion , applying the functions f , fm to its own previous out - put ( i . e .
recursively ) until it reaches the top node of the tree which represents the entire sentence .
for experiments with longer sequences we will compare to standard rnns and the special case of the mv - rnn that computes the parent by p = ab +
very good movie ( a , a ) ( b , b ) ( c , c ) matrix - vector recursive neural network ( p123 , p123 ) ( p123 , p123 ) p123 = g ( w ) p123 = wmcp123 p123c ( ) p123c ( ) objective is not convex , we found that l - bfgs run over the complete training data ( batch mode ) mini - mizes the objective well in practice and convergence is smooth .
for more information see ( socher et al . ,
123 low - rank matrix approximations if every word is represented by an n - dimensional vector and additionally by an n n matrix , the di - mensionality of the whole model may become too large with commonly used vector sizes of n = 123
in order to reduce the number of parameters , we rep - resent word matrices by the following low - rank plus
a = u v + diag ( a ) ,
where u rnr , v rrn , a rn and we set the rank for all experiments to r = 123
123 discussion : evaluation and generality evaluation of compositional vector spaces is a com - plex task .
most related work compares similarity judgments of unsupervised models to those of hu - man judgments and aims at high correlation .
these evaluations can give important insights .
however , even with good correlation the question remains how these models would perform on downstream nlp tasks such as sentiment detection .
we ex - perimented with unsupervised learning of general vector - matrix representations by having the mv - rnn predict words in their correct context .
tializing the models with these general representa - tions , did not improve the performance on the tasks we consider .
for sentiment analysis , this is not sur - prising since antonyms often get similar vectors dur - ing unsupervised learning from co - occurrences due to high similarity of local syntactic contexts .
in our experiments , the high prediction performance came from supervised learning of meaning representations using labeled data .
while these representations are task - specic , they could be used across tasks in a multi - task learning setup .
however , in order to fairly compare to related work , we use only the super - vised data of each task .
before we describe our full - scale experiments , we analyze the models expres -
123 model analysis
this section analyzes the model with two proof - of - concept studies .
first , we examine its ability to learn operator semantics for adverb - adjective pairs .
if a model cannot correctly capture how an adverb op - erates on the meaning of adjectives , then theres lit - tle chance it can learn operators for more complex relationships .
the second study analyzes whether the mv - rnn can learn simple boolean operators of propositional logic such as conjunctives or negation from truth values .
again , if a model did not have this ability , then theres little chance it could learn these frequently occurring phenomena from the noisy lan - guage of real texts such as movie reviews .
123 predicting sentiment distributions of
the rst study considers the prediction of ne - grained sentiment distributions of adverb - adjective pairs and analyzes different possibilities for com - puting the parent vector p .
the results show that the mv - rnn operators are powerful enough to cap - ture the operational meanings of various types of ad - verbs .
for example , very is an intensier , pretty is an attenuator , and not can negate or strongly attenuate the positivity of an adjective .
for instance not great is still pretty good and not terrible; see potts ( 123 )
we use a publicly available imdb dataset of ex - tracted adverb - adjective pairs from movie reviews . 123 the dataset provides the distribution over star rat - ings : each consecutive word pair appears a certain number of times in reviews that have also associ - ated with them an overall rating of the movie .
after normalizing by the total number of occurrences , one gets a multinomial distribution over ratings .
only word pairs that appear at least 123 times are kept .
of the remaining pairs , we use 123 randomly sampled ones for training and a separate set of 123 for test - ing .
we never give the algorithm sentiment distribu - tions for single words , and , while single words over - lap between training and testing , the test set consists of never before seen word pairs .
the softmax classier is trained to minimize the cross entropy error .
hence , an evaluation in terms of kl - divergence is the most reasonable choice
123 ( a + b )
p = 123 p = a b p = ( a; b ) p = ab
figure 123 : left : average kl - divergence for predicting sentiment distributions of unseen adverb - adjective pairs of the test set .
see text for p descriptions .
lower is better .
the main difference in the kl divergence comes from the few negation pairs in the test set .
right : predicting sentiment distributions ( over 123 - 123 stars on the x - axis ) of adverb - adjective pairs .
each row has the same adverb and each column the same adjective .
many predictions are similar between the two models .
the rnn and linear mvr are not able to modify the sentiment correctly : not awesome is more positive than fairly awesome and not annoying has a similar shape as unbelievably annoying .
predictions of the linear mvr model are almost identical to the standard rnn for these examples .
dened as kl ( g||p ) = ( cid : 123 )
i gi log ( gi / pi ) , where g is
the gold distribution and p is the predicted one .
p = 123 ( a + b ) , vector average
we compare to several baselines and ablations of the mv - rnn model .
an ( adverb , adjective ) pair is described by its vectors ( a , b ) and matrices ( a , b ) .
p = a b , element - wise vector multiplication 123
p = ( a; b ) , vector concatenation 123
p = ab , similar to ( baroni and lenci , 123 ) 123
p = g ( w ( a; b ) ) , rnn , similar to socher et al .
p = ab + ba , linear mvr , similar to ( mitchell and lapata , 123; zanzotto et al . , 123 ) 123
p = g ( w ( ba; ab ) ) , mv - rnn the nal distribution is always predicted by a softmax classier whose inputs p vary for each of the models .
this objective function ( see sec .
123 ) is different to all previously published work except that of ( socher et al . , 123c ) .
we cross - validated all models over regulariza - tion parameters for word vectors , the softmax clas - the rnn parameter w and the word op - erators ( 123 , 123 ) and word vector sizes ( n = 123 , 123 , 123 , 123 , 123 , 123 ) .
all models performed best at vector sizes of below 123
hence , it is the models power and not the number of parameters that deter -
mines the performance .
the table in fig .
123 shows the average kl - divergence on the test set .
it shows that the idea of matrix - vector representations for all words and having a nonlinearity are both impor - tant .
the mv - rnn which combines these two ideas is best able to learn the various compositional ef - fects .
the main difference in kl divergence comes from the few negation cases in the test set .
123 shows examples of predicted distributions .
many of the predictions are accurate and similar between the top models .
however , only the mv - rnn has enough expressive power to allow negation to com - pletely shift the sentiment with respect to an adjec - tive .
a negated adjective carrying negative senti - ment becomes slightly positive , whereas not awe - some is correctly attenuated .
all three top models correctly capture the u - shape of unbelievably sad .
this pair peaks at both the negative and positive spectrum because it is ambiguous .
when referring to the performance of actors , it is very negative , but , when talking about the plot , many people enjoy sad and thought - provoking movies .
the p = ab model does not perform well because it cannot model the fact that for an adjective like sad , the operator of unbelievably behaves differently .
123 . 123 . 123fairly annoying mvrnnrnn123 . 123 . 123fairly awesome mvrnnrnn123 . 123 . 123fairly sad mvrnnrnn123 . 123 . 123not annoying mvrnnrnn123 . 123 . 123not awesome mvrnnrnn123 . 123 . 123not sad training pair123 . 123 . 123unbelievably annoying mvrnnrnn123 . 123 . 123unbelievably awesome mvrnnrnn123 . 123 . 123unbelievably sad mvrnnrnn false
figure 123 : training trees for the mv - rnn to learn propositional operators .
the model learns vectors and operators for ( and ) and ( negation ) .
the model outputs the exact representations of false and true respectively at the top node .
hence , the operators can be combined recursively an arbitrary number of times for more complex logical functions .
123 logic - and vector - based compositionality another natural question is whether the mv - rnn can , in general , capture some of the simple boolean logic that is sometimes found in language .
in other words , can it learn some of the propositional logic operators such as and , or , not in terms of vectors and matrices from a few examples .
answering this ques - tion can also be seen as a rst step towards bridg - ing the gap between logic - based , formal semantics ( montague , 123 ) and vector space models .
the logic - based view of language accounts nicely for compositionality by directly mapping syntac - tic constituents to lambda calculus expressions .
at the word level , the focus is on function words , and nouns and adjectives are often dened only in terms of the sets of entities they denote in the world .
most words are treated as atomic symbols with no rela - tion to each other .
there have been many attempts at automatically parsing natural language to a logi - cal form using recursive compositional rules .
conversely , vector space models have the attrac - tive property that they can automatically extract knowledge from large corpora without supervision .
unlike logic - based approaches , these models allow us to make ne - grained statements about the seman - tic similarity of words which correlate well with hu - man judgments ( grifths et al . , 123 ) .
logic - based approaches are often seen as orthogonal to distribu - tional vector - based approaches .
however , garrette et al .
( 123 ) recently introduced a combination of a vector space model inside a markov logic network .
one open question is whether vector - based mod - els can learn some of the simple logic encountered in language such as negation or conjunctives .
to this end , we illustrate in a simple example that our mv - rnn model and its learned word matrices ( op - erators ) have the ability to learn propositional logic operators such as , , ( and , or , not ) .
this is a necessary ( though not sufcient ) condition for the ability to pick up these phenomena in real datasets
and tasks such as sentiment detection which we fo - cus on in the subsequent sections .
our setup is as follows .
we train on 123 strictly right - branching trees as in fig .
we consider the 123 - dimensional case and x the representation for true to ( t = 123 , t = 123 ) and false to ( f = 123 , f = 123 ) .
fixing the operators to the 123 123 identity matrix 123 is essentially ignoring them .
the objective is then to create a perfect reconstruction of ( t , t ) or ( f , f ) ( depending on the formula ) , which we achieve by the least squares error between the top vectors rep - resentation and the corresponding truth value , e . g .
for f alse : min||ptop t||123 + ||ptop t||123
as our function g ( see eq .
123 ) , we use a linear threshold unit : g ( x ) = max ( min ( x , 123 ) , 123 ) .
giving the derivatives computed for the objective function for the examples in fig .
123 to a standard l - bfgs op - timizer quickly yields a training error of 123
hence , the output of these 123 examples has exactly one of the truth representations , making it recursively compati - ble with further combinations of operators .
thus , we can combine these operators to construct any propo - sitional logic function of any number of inputs ( in - cluding xor ) .
hence , this mv - rnn is complete in terms of propositional logic .
123 predicting movie review ratings in this section , we analyze the models performance on full length sentences .
we compare to previous state of the art methods on a standard benchmark dataset of movie reviews ( pang and lee , 123; nak - agawa et al . , 123; socher et al . , 123c ) .
this dataset consists of 123 , 123 positive and negative sin - gle sentences describing movie sentiment .
and the next experiment we use binarized trees from the stanford parser ( klein and manning , 123 ) .
we use the exact same setup and parameters ( regulariza - tion , word vector size , etc . ) as the published code of socher et al .
( 123c ) . 123
tree - crf ( nakagawa et al . , 123 ) rae ( socher et al . , 123c )
table 123 : accuracy of classication on full length movie review polarity ( mr ) .
review sentence
the lm is bright and ashy in all the right ways .
not always too whimsical for its own good this strange hybrid of crime thriller , quirky character study , third - rate romance and female empowerment fantasy never really nds the tonal or thematic glue doesnt come close to justifying the hype that sur - rounded its debut at the sundance lm festival two director hoffman , his writer and klines agent should serve detention .
a bodice - ripper for intellectuals .
table 123 : hard movie review examples of positive ( 123 ) and negative ( 123 ) sentiment ( s . ) that of all methods only the mv - rnn predicted correctly ( c : ) or could not classify as correct either ( c : x ) .
table 123 shows comparisons to the system of ( nak - agawa et al . , 123 ) , a dependency tree based classi - cation method that uses crfs with hidden variables .
the state of the art recursive autoencoder model of socher et al .
( 123c ) obtained 123% accuracy .
our new mv - rnn gives the highest performance , out - performing also the linear mvr ( sec
table 123 shows several hard examples that only the mv - rnn was able to classify correctly .
none of the methods correctly classied the last two examples which require more world knowledge .
123 classication of semantic relationships
the previous task considered global classication of an entire phrase or sentence .
in our last experiment we show that the mv - rnn can also learn how a syn - tactic context composes an aggregate meaning of the semantic relationships between words .
in particular , the task is nding semantic relationships between pairs of nominals .
for instance , in the sentence my ( apartment ) e123 has a pretty large ( kitchen ) e123 , we want to predict that the kitchen and apartment are in a component - whole relationship .
predicting such
figure 123 : the mv - rnn learns vectors in the path con - necting two words ( dotted lines ) to determine their se - mantic relationship .
it takes into consideration a variable length sequence of various word types in that path .
semantic relations is useful for information extrac - tion and thesaurus construction applications .
many approaches use features for all words on the path between the two words of interest .
we show that by building a single compositional semantics for the minimal constituent including both terms one can achieve a higher performance .
this task requires the ability to deal with se - quences of words of arbitrary type and length in be - tween the two nouns in question . fig .
123 explains our method for classifying nominal relationships .
we rst nd the path in the parse tree between the two words whose relation we want to classify .
we then select the highest node of the path and classify the relationship using that nodes vector as features .
we apply the same type of mv - rnn model as in senti - ment to the subtree spanned by the two words .
we use the dataset and evaluation framework of semeval - 123 task 123 ( hendrickx et al . , 123 ) .
there are 123 ordered relationships ( with two direc - tions ) and an undirected other class , resulting in 123 classes .
among the relationships are : message - topic , cause - effect , instrument - agency ( etc .
see ta - ble 123 for list ) .
a pair is counted as correct if the order of the words in the relationship is correct .
table 123 lists results for several competing meth - ods together with the resources and features used by each method .
we compare to the systems of the competition which are described in hendrickx et al .
( 123 ) as well as the rnn and linear mvr .
most systems used a considerable amount of hand - designed semantic resources .
in contrast to these methods , the mv - rnn only needs a parser for the tree structure and learns all semantics from unla - beled corpora and the training data .
only the se - meval training dataset is specic to this task , the re -
the ( movie ) showed ( wars ) mv - rnn for relationship classificationclassifier : message - topic relationship
sentence with labeled nouns for which to predict relationships avian ( inuenza ) e123 is an infectious disease caused by type a strains of the inuenza ( virus ) e123
the ( mother ) e123 left her native ( land ) e123 about the same time and they were married in that city .
roadside ( attractions ) e123 are frequently advertised with ( billboards ) e123 to attract tourists .
a child is told a ( lie ) e123 for several years by their ( parents ) e123 before he / she realizes that . . .
the accident has spread ( oil ) e123 into the ( ocean ) e123
the siege started , with a ( regiment ) e123 of lightly armored ( swordsmen ) e123 ramming down the gate .
the core of the ( analyzer ) e123 identies the paths using the constraint propagation ( method ) e123
the size of a ( tree ) e123 ( crown ) e123 is strongly correlated with the growth of the tree .
the hidden ( camera ) e123 , found by a security guard , was hidden in a business card - sized ( leaet box ) e123 placed at an unmanned atm in tokyos minato ward in early september .
table 123 : examples of correct classications of ordered , semantic relations between nouns by the mv - rnn .
note that the nal classier is a recursive , compositional function of all the words in the syntactic path between the bracketed words .
the paths vary in length and the words vary in type .
pos , stemming , syntactic patterns word pair , words in between pos , wordnet , stemming , syntactic pos , wordnet , morphological tures , thesauri , google n - grams pos , wordnet , morphological tures , noun compound system , sauri , google n - grams pos , wordnet , prexes and other morphological features , pos , depen - dency parse features , levin classes , propbank , framenet , nomlex - plus , google n - grams , paraphrases , tex -
table 123 : learning methods , their feature sets and f123 results for predicting semantic relations between nouns .
the mv - rnn outperforms all but one method without any additional feature sets .
by adding three such features , it obtains state of the art performance .
maining inputs and the training setup are the same as in previous sentiment experiments .
the best method on this dataset harabagiu , 123 ) obtains 123% f123
in order to see whether our system can improve over this sys - tem , we added three features to the mv - rnn vec - tor and trained another softmax classier .
the fea - tures and their performance increases were pos tags ( +123 ) ; wordnet hypernyms ( +123 ) and named en -
tity tags ( ner ) of the two words ( +123 ) .
features were computed using the code of ciaramita and al - tun ( 123 ) . 123 with these features , the performance improved over the state of the art system .
table 123 shows random correct classication examples .
123 related work
