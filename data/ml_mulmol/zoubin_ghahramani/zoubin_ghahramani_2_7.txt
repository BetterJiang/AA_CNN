we present a novel algorithm for agglomer - ative hierarchical clustering based on evalu - ating marginal likelihoods of a probabilistic model .
this algorithm has several advan - tages over traditional distance - based agglom - erative clustering algorithms .
( 123 ) it de ( cid : 123 ) nes a probabilistic model of the data which can be used to compute the predictive distribu - tion of a test point and the probability of it belonging to any of the existing clusters in the tree .
( 123 ) it uses a model - based criterion to decide on merging clusters rather than an ad - hoc distance metric .
( 123 ) bayesian hypoth - esis testing is used to decide which merges are advantageous and to output the recom - mended depth of the tree .
( 123 ) the algorithm can be interpreted as a novel fast bottom - up approximate inference method for a dirich - let process ( i . e .
countably in ( cid : 123 ) nite ) mixture model ( dpm ) .
it provides a new lower bound on the marginal likelihood of a dpm by sum - ming over exponentially many clusterings of the data in polynomial time .
we describe procedures for learning the model hyperpa - rameters , computing the predictive distribu - tion , and extensions to the algorithm .
exper - imental results on synthetic and real - world data sets demonstrate useful properties of the
hierarchical clustering is one of the most frequently used methods in unsupervised learning .
given a set of data points , the output is a binary tree ( dendrogram ) whose leaves are the data points and whose internal nodes represent nested clusters of various sizes .
the tree organizes these clusters hierarchically , where the hope is that this hierarchy agrees with the intuitive
appearing in proceedings of the 123 nd international confer - ence on machine learning , bonn , germany , 123
copy - right 123 by the author ( s ) / owner ( s ) .
organization of real - world data .
hierarchical struc - tures are ubiquitous in the natural world .
for ex - ample , the evolutionary tree of living organisms ( and consequently features of these organisms such as the sequences of homologous genes ) is a natural hierarchy .
hierarchical structures are also a natural representa - tion for data which was not generated by evolutionary processes .
for example , internet newsgroups , emails , or documents from a newswire , can be organized in increasingly broad topic domains .
the traditional method for hierarchically clustering data as given in ( duda & hart , 123 ) is a bottom - up agglomerative algorithm .
it starts with each data point assigned to its own cluster and iteratively merges the two closest clusters together until all the data be - longs to a single cluster .
the nearest pair of clusters is chosen based on a given distance measure ( e . g .
eu - clidean distance between cluster means , or distance between nearest points ) .
there are several limitations to the traditional hier - archical clustering algorithm .
the algorithm provides no guide to choosing the \correct " number of clusters or the level at which to prune the tree .
it is often dif - ( cid : 123 ) cult to know which distance metric to choose , espe - cially for structured data such as images or sequences .
the traditional algorithm does not de ( cid : 123 ) ne a probabilis - tic model of the data , so it is hard to ask how \good " a clustering is , to compare to other models , to make predictions and cluster new data into an existing hier - archy .
we use statistical inference to overcome these limitations .
previous work which uses probabilistic methods to perform hierarchical clustering is discussed in section 123
our bayesian hierarchical clustering algorithm uses marginal likelihoods to decide which clusters to merge and to avoid over ( cid : 123 ) tting .
basically it asks what the probability is that all the data in a potential merge were generated from the same mixture component , and compares this to exponentially many hypotheses at lower levels of the tree ( section 123 ) .
the generative model for our algorithm is a dirichlet process mixture model ( i . e .
a countably in ( cid : 123 ) nite mix -
bayesian hierarchical clustering
ture model ) , and the algorithm can be viewed as a fast bottom - up agglomerative way of performing approxi - mate inference in a dpm .
instead of giving weight to all possible partitions of the data into clusters , which is intractable and would require the use of sampling methods , the algorithm e ( cid : 123 ) ciently computes the weight of exponentially many partitions which are consistent with the tree structure ( section 123 ) .
our bayesian hierarchical clustering algorithm is sim - ilar to traditional agglomerative clustering in that it is a one - pass , bottom - up method which initializes each data point in its own cluster and iteratively merges pairs of clusters .
as we will see , the main di ( cid : 123 ) erence is that our algorithm uses a statistical hypothesis test to choose which clusters to merge .
let d = fx ( 123 ) ; : : : ; x ( n ) g denote the entire data set , and di ( cid : 123 ) d the set of data points at the leaves of the subtree ti .
the algorithm is initialized with n trivial trees , fti : i = 123 : : : ng each containing a single data point di = fx ( i ) g .
at each stage the algorithm consid - ers merging all pairs of existing trees .
for example , if ti and tj are merged into some new tree tk then the associated set of data is dk = di ( dj ( see ( cid : 123 ) gure 123 ( a ) ) .
in considering each merge , two hypotheses are com - pared .
the ( cid : 123 ) rst hypothesis , which we will denote hk is that all the data in dk were in fact generated in - dependently and identically from the same probabilis - tic model , p ( xj ( cid : 123 ) ) with unknown parameters ( cid : 123 ) .
let us imagine that this probabilistic model is a multivariate gaussian , with parameters ( cid : 123 ) = ( ( cid : 123 ) ; ( cid : 123 ) ) , although it is crucial to emphasize that for di ( cid : 123 ) erent types of data , di ( cid : 123 ) erent probabilistic models may be appropriate .
to evaluate the probability of the data under this hypoth - esis we need to specify some prior over the parameters of the model , p ( ( cid : 123 ) j ( cid : 123 ) ) with hyperparameters ( cid : 123 ) .
we now have the ingredients to compute the probability of the data dk under hk
123 ) = z p ( dkj ( cid : 123 ) ) p ( ( cid : 123 ) j ( cid : 123 ) ) d ( cid : 123 )
= z h yx ( i ) 123dk
this calculates the probability that all the data in dk were generated from the same parameter values as - suming a model of the form p ( xj ( cid : 123 ) ) .
this is a natural model - based criterion for measuring how well the data ( cid : 123 ) t into one cluster .
if we choose models with conjugate priors ( e . g .
normal - inverse - wishart priors for normal continuous data or dirichlet priors for multinomial
figure 123
( a ) schematic of a portion of a tree where ti and tj are merged into tk , and the associated data sets di and dj are merged into dk .
( b ) an example tree with 123 data points .
the clusterings ( 123 123 123 ) ( 123 ) and ( 123 123 ) ( 123 ) ( 123 ) are tree - consistent par - titions of this data .
the clustering ( 123 ) ( 123 123 ) ( 123 ) is not a tree -
discrete data ) this integral is tractable .
throughout this paper we use such conjugate priors so the inte - grals are simple functions of su ( cid : 123 ) cient statistics of dk .
for example , in the case of gaussians , ( 123 ) is a function of the sample mean and covariance of the data in dk .
the alternative hypothesis to hk 123 would be that the data in dk has two or more clusters in it .
summing over the exponentially many possible ways of dividing dk into two or more clusters is intractable .
however , if we restrict ourselves to clusterings that partition the data in a manner that is consistent with the subtrees ti and tj , we can compute the sum e ( cid : 123 ) ciently using re - cursion .
( we elaborate on the notion of tree - consistent partitions in section 123 and ( cid : 123 ) gure 123 ( b ) ) .
the probabil - ity of the data under this restricted alternative hy - 123 , is simply a product over the subtrees 123 ) = p ( dijti ) p ( djjtj ) where the probability of a data set under a tree ( e . g .
p ( dijti ) ) is de ( cid : 123 ) ned below .
123 and hk
combining the probability of the data under hypothe - 123 , weighted by the prior that all points in dk belong to one cluster , ( cid : 123 ) k 123 ) , we obtain the marginal probability of the data in tree tk :
p ( dkjtk ) = ( cid : 123 ) kp ( dkjhk
123 ) + ( 123 ( cid : 123 ) ( cid : 123 ) k ) p ( dijti ) p ( djjtj ) ( 123 )
this equation is de ( cid : 123 ) ned recursively , there the ( cid : 123 ) rst term considers the hypothesis that there is a single cluster in dk and the second term e ( cid : 123 ) ciently sums over all other clusterings of the data in dk which are con - sistent with the tree structure ( see ( cid : 123 ) gure 123 ( a ) ) .
section 123 we show that equation 123 can be used to de - rive an approximation to the marginal likelihood of a dirichlet process mixture model , and in fact provides a new lower bound on this marginal likelihood . 123 we
123it is important not to confuse the marginal likelihood in equation 123 , which integrates over the parameters of one cluster , and the marginal likelihood of a dpm , which integrates over all clusterings and their parameters .
bayesian hierarchical clustering
also show that the prior for the merged hypothesis , ( cid : 123 ) k , can be computed bottom - up in a dpm .
the posterior probability of the merged hypothesis rk is obtained using bayes rule :
123 ) + ( 123 ( cid : 123 ) ( cid : 123 ) k ) p ( dijti ) p ( djjtj )
this quantity is used to decide greedily which two trees to merge , and is also used to determine which merges in the ( cid : 123 ) nal hierarchy structure were justi ( cid : 123 ) ed .
the general algorithm is very simple ( see ( cid : 123 ) gure 123 ) .
input : data d = fx ( 123 ) : : : x ( n ) g , model p ( xj ( cid : 123 ) ) ,
initialize : number of clusters c = n , and
di = fx ( i ) g for i = 123 : : : n
while c > 123 do
find the pair di and dj with the highest probability of the merged hypothesis :
merge dk di ( dj , tk ( ti; tj ) delete di and dj , c c ( cid : 123 ) 123
output : bayesian mixture model where each
tree node is a mixture component
the tree can be cut at points where rk < 123 : 123
figure 123
bayesian hierarchical clustering algorithm
our bayesian hierarchical clustering algorithm has many desirable properties which are absent in tradi - tional hierarchical clustering .
for example , it allows us to de ( cid : 123 ) ne predictive distributions for new data points , it decides which merges are advantageous and suggests natural places to cut the tree using a statistical model comparison criterion ( via rk ) , and it can be customized to di ( cid : 123 ) erent kinds of data by choosing appropriate mod - els for the mixture components .
approximate inference in a dirichlet
process mixture model
the above algorithm is an approximate inference method for dirichlet process mixture models ( dpm ) .
dirichlet process mixture models consider the limit of in ( cid : 123 ) nitely many components of a ( cid : 123 ) nite mixture model .
allowing in ( cid : 123 ) nitely many components makes it possible to more realistically model the kinds of complicated distributions which we expect in real problems .
we brie ( cid : 123 ) y review dpms here , starting from ( cid : 123 ) nite mixture
consider a ( cid : 123 ) nite mixture model with c components
p ( x ( i ) j ( cid : 123 ) j ) p ( ci = jjp )
where ci 123 f123; : : : ; cg is a cluster indicator variable for data point i , p are the parameters of a multinomial dis - tribution with p ( ci = jjp ) = pj , ( cid : 123 ) j are the parameters of the jth component , and ( cid : 123 ) = ( ( cid : 123 ) 123; : : : ; ( cid : 123 ) c; p ) .
let the parameters of each component have conjugate priors p ( ( cid : 123 ) j ( cid : 123 ) ) as in section 123 , and the multinomial parameters also have a conjugate dirichlet prior
given a data set d = fx ( 123 ) : : : ; x ( n ) g , the marginal likelihood for this mixture model is
p ( x ( i ) j ( cid : 123 ) ) # p ( ( cid : 123 ) j ( cid : 123 ) ; ( cid : 123 ) ) d ( cid : 123 )
p ( dj ( cid : 123 ) ; ( cid : 123 ) ) =z " n where p ( ( cid : 123 ) j ( cid : 123 ) ; ( cid : 123 ) ) = p ( pj ( cid : 123 ) ) qc p ( dj ( cid : 123 ) ; ( cid : 123 ) ) =xc
likelihood can be re - written as
j=123 p ( ( cid : 123 ) jj ( cid : 123 ) ) .
this marginal
where c = ( c123; : : : ; cn ) and p ( cj ( cid : 123 ) ) =r p ( cjp ) p ( pj ( cid : 123 ) ) dp
is a standard dirichlet integral .
the quantity ( 123 ) is well - de ( cid : 123 ) ned even in the limit c ! 123
although the number of possible settings of c grows as c n and there - fore diverges as c ! 123 , the number of possible ways of partitioning the n points remains ( cid : 123 ) nite ( roughly o ( nn ) ) .
using v to denote the set of all possible par - titioning of n data points , we can re - write ( 123 ) as :
p ( dj ( cid : 123 ) ; ( cid : 123 ) ) = xv123v
rasmussen ( 123 ) provides a thorough analysis of dpms with gaussian components , and a markov chain monte carlo ( mcmc ) algorithm for sampling from the partitionings v .
dpms have the interesting prop - erty that the probability of a new data point belonging to a cluster is propotional to the number of points al - ready in that cluster ( blackwell & macqueen , 123 ) , where ( cid : 123 ) controls the probability of the new point cre - ating a new cluster .
for an n point data set , each possible clustering is a di ( cid : 123 ) erent partition of the data , which we can de - note by placing brackets around data point indices : e . g .
( 123 123 ) ( 123 ) ( 123 ) .
each individual cluster , e . g .
( 123 123 ) , is a nonempty subset of data , yielding 123n ( cid : 123 ) 123 pos - sible clusters , which can be combined in many ways
bayesian hierarchical clustering
to form clusterings ( i . e .
partitions ) of the whole data set .
we can organize a subset of these clusters into a tree .
combining these clusters one can obtain all tree - consistent partitions of the data ( see ( cid : 123 ) gure 123 ( b ) ) .
rather than summing over all possible partitions of the data using mcmc , our algorithm computes the sum over all exponentially many tree - consistent par - titions for a particular tree built greedily bottom - up .
this can be seen as a fast and deterministic alternative to mcmc approximations .
returning to our algorithm , since a dpm with concen - tration hyperparameter ( cid : 123 ) de ( cid : 123 ) nes a prior on all parti - tions of the nk data points in dk ( the value of ( cid : 123 ) is directly related to the expected number of clusters ) , the prior on the merged hypothesis is the relative mass of all nk points belonging to one cluster versus all the other partitions of those nk data points consistent with the tree structure .
this can be computed bottom - up as the tree is being built ( ( cid : 123 ) gure 123 ) .
initialize each leaf i to have di = ( cid : 123 ) , ( cid : 123 ) i = 123 for each internal node k do dk = ( cid : 123 ) ( cid : 123 ) ( nk ) + dleftk drightk ( cid : 123 ) k = ( cid : 123 ) ( cid : 123 ) ( nk )
figure 123
algorithm for computing prior on merging , where rightk ( leftk ) indexes the right ( left ) subtree of tk and drightk ( dleftk ) is the value of d computed for the right ( left ) child of internal node k .
lemma 123 the marginal likelihood of a dpm is :
p ( dk ) = xv123v
where v is the set of all possible partitionings of dk , mv is the number of clusters in partitioning v , and nv
is the number of points in cluster of partitioning v .
this follows from equation ( 123 ) where the ( cid : 123 ) rst ( frac - tional ) term in the sum is p ( v ) , the second ( product ) term is p ( dkjv ) , and the explicit dependence on ( cid : 123 ) and ( cid : 123 ) has been dropped .
theorem 123 the bayesian hierarchical clustering algorithm is :
computed by the
proof rewriting equation ( 123 ) using algorithm 123 to substitute in for ( cid : 123 ) k we obtain :
p ( dkjtk ) = p ( dkjhk
we will proceed to give a proof by induction .
in the base case , at a leaf node , the second term in this equa - tion drops out since there are no subtrees .
( cid : 123 ) ( nk = 123 ) = 123 and dk = ( cid : 123 ) yielding p ( dkjtk ) = p ( dkjhk 123 ) as we should expect at a leaf node .
for the inductive step , we note that the ( cid : 123 ) rst term is al - ways just the trivial partition with all nk points into a single cluster .
according to our inductive hypothesis :
p ( dijti ) = xv123 123vti
and similarly for p ( djjtj ) , where vti ( vtj ) is the set of all tree - consistent partitionings of di ( dj ) .
combining terms we obtain :
@ xv123 123vti b@ xv123 123vtj
where vntt is the set of all non - trivial tree - consistent partionings of dk .
for the trivial partition , mv = 123 123 = nk .
by combining the trivial and non - trivial terms we get a sum over all tree - consistent partitions yielding the result in theorem 123
this completes the proof .
another way to get this result is to expand out p ( djt ) and substitute for ( cid : 123 ) using algorithm 123
corollary 123 for any binary tree tk with the data points in dk at its leaves , the following is a lower bound on the marginal likelihood of a dpm :
( cid : 123 ) ( nk + ( cid : 123 ) )
p ( dkjtk ) ( cid : 123 ) p ( dk )
p ( dkjtk ) = xv123vt
where vt is the set of all tree - consistent partitionings
proof proof of corollary 123 follows trivially by multi - plying p ( dkjtk ) by a ratio of its denominator and the denominator from p ( dk ) from lemma 123 ( i . e .
and from the fact that tree - consistent partitions are a subset of all partitions of the data .
bayesian hierarchical clustering
proposition 123 the number of tree - consistent parti - tions is exponential in the number of data points for balanced binary trees .
proof if ti has ci tree - consistent partitions of di and tj has cj tree - consistent partitions of dj , then tk = ( ti; tj ) merging the two has cicj + 123 tree - consistent partitions of dk = di ( dj , obtained by combining all partitions and adding the partition where all data in dk are in one cluster .
at the leaves ci = 123
therefore , for a balanced binary tree of depth the number of tree - consistent partitions grows as o ( 123 ) whereas the number of data points n grows as o ( 123 )
in summary , p ( djt ) sums the probabilities for all tree - consistent partitions , weighted by the prior mass as - signed to each partition by the dpm .
the computa - tional complexity of constructing the tree is o ( n123 ) , the complexity of computing the marginal likelihood is o ( n log n ) , and the complexity of computing the predictive distribution ( see section 123 ) is o ( n ) .
learning and prediction
learning hyperparameters .
for any given set - ting of the hyperparameters , the root node of the tree approximates the probability of the data given those particular hyperparameters .
in our model the hyper - paramters are the concentration parameter ( cid : 123 ) from the dpm , and the hyperparameters ( cid : 123 ) of the probabilistic model de ( cid : 123 ) ning each component of the mixture .
we can use the root node marginal likelihood p ( djt ) to do model comparison between di ( cid : 123 ) erent settings of the hyperparameters .
for a ( cid : 123 ) xed tree we can optimize over the hyperparameters by taking gradients .
the gradi - combines the results of this computation at the subtrees of tk with @p ( dkjhk .
these gradients can be computed bottom - up as the tree is being built .
@ ( cid : 123 ) , which in turn de - pends on @di @ ( cid : 123 ) , which can be propagated up from the the subtrees .
this allows us to construct an em - like algorithm where we ( cid : 123 ) nd the best tree structure in the ( viterbi - like ) e step and then optimize over the hy - perparameters in the m step .
in our experiments we have only optimized one of the hyperparameters with a simple line search for gaussian components .
a sim - ple empirical approach is to set the hyperparameters ( cid : 123 ) by ( cid : 123 ) tting a single model to the whole data set .
details on hyperparameter optimization can be found in our tech report ( heller & ghahramani , 123 ) .
depends on @ ( cid : 123 ) k
predictive distribution .
for any tree , the prob - ability of a new test point given the data can be computed by recursing through the tree starting at
figure 123
log marginal likelihood ( evidence ) vs .
purity over 123 iterations of hyperparameter optimization
the root node .
each node k represents a cluster , with an associated predictive distribution p ( xjdk ) =
r p ( xj ( cid : 123 ) ) p ( ( cid : 123 ) jdk; ( cid : 123 ) ) d ( cid : 123 ) .
the overall predictive distribu -
tion sums over all nodes weighted by their posterior
p ( xjd ) = xk123n
where n is the set of all nodes in the tree , ! k
( 123 ( cid : 123 ) ri ) is the weight on cluster k , and nk is the set of nodes on the path from the root node to the parent of node k .
this expression can be derived by rearranging the sum over all tree - consistent parti - tionings into a sum over all clusters in the tree ( noting that a cluster can appear in many partitionings ) .
for gaussian components with conjugate priors , this results in a predictive distribution which is a mixture of multivariate t distributions .
we show some exam - ples of this in the results section .
we compared bayesian hierarchical clustering to tra - ditional hierarchical clustering using average , single , and complete linkage , using a euclidean distance met - ric , over 123 datasets ( 123 real and 123 synthetic ) .
we also compared our algorithm to average linkage hierarchi - cal clustering on several toy 123d problems ( ( cid : 123 ) gure 123 ) .
on these problems we were able to compare the di ( cid : 123 ) er - ent hierarchies generated by the two algorithms , and visualize clusterings and predictive distributions .
the 123 real datasets we used are the spambase ( 123 ran - dom examples from each class , 123 classes , 123 attributes ) and glass ( 123 examples , 123 classes , 123 attributes ) datasets from the uci repository , the cedar buf - falo digits ( 123 random examples from each class , 123 classes , 123 attributes ) , and the cmu 123newsgroups dataset ( 123 examples , 123 classes - rec . sport . baseball ,
bayesian hierarchical clustering
single linkage complete linkage average linkage
123 : 123 ( cid : 123 ) 123 : 123 123 : 123 ( cid : 123 ) 123 : 123 123 : 123 ( cid : 123 ) 123 : 123 123 : 123 ( cid : 123 ) 123 : 123 123 : 123 ( cid : 123 ) 123 : 123 123 : 123 ( cid : 123 ) 123 : 123
123 : 123 ( cid : 123 ) 123 : 123 123 : 123 ( cid : 123 ) 123 : 123 123 : 123 ( cid : 123 ) 123 : 123 123 : 123 ( cid : 123 ) 123 : 123 123 : 123 ( cid : 123 ) 123 : 123 123 : 123 ( cid : 123 ) 123 : 123
123 : 123 ( cid : 123 ) 123 : 123 123 : 123 ( cid : 123 ) 123 : 123 123 : 123 ( cid : 123 ) 123 : 123 123 : 123 ( cid : 123 ) 123 : 123 123 : 123 ( cid : 123 ) 123 : 123 123 : 123 ( cid : 123 ) 123 : 123
123 : 123 ( cid : 123 ) 123 : 123 123 : 123 ( cid : 123 ) 123 : 123 123 : 123 ( cid : 123 ) 123 : 123 123 : 123 ( cid : 123 ) 123 : 123 123 : 123 ( cid : 123 ) 123 : 123 123 : 123 ( cid : 123 ) 123 : 123
table 123
purity scores for 123 kinds of traditional agglomerative clustering , and bayesian hierarchical clustering .
the mean scores over 123 - fold cross - validation along with standard errors are shown .
rec . sport . hockey , rec . autos , and sci . space , 123 at - tributes ) .
we also used synthetic data generated from a mixture of gaussians ( 123 examples , 123 classes , 123 attributes ) .
the synthetic , glass and toy datasets were modeled using gaussians , while the digits , spam - base , and newsgroup datasets were binarized and mod - eled using bernoullis .
we binarized the digits dataset by thresholding at a greyscale value of 123 out of 123 through 123 , and the spambase dataset by whether each attribute value was zero or non - zero .
we ran the algorithms on 123 digits ( 123 , 123 , 123 ) , and all 123 digits .
the newsgroup dataset was constucted using rain - bow ( mccallum , 123 ) , where a stop list was used and words appearing fewer than 123 times were ignored .
the dataset was then binarized based on word pres - ence / absence in a document .
for these classi ( cid : 123 ) cation datasets , where labels for the data points are known , we computed a measure between 123 and 123 of how well a dendrogram clusters the known labels called the den - drogram purity . 123 we found that the marginal like - lihood of the tree structure for the data was highly correlated with the purity .
over 123 iterations with dif - ferent hyperparameters this correlation was 123 : 123 ( see ( cid : 123 ) gure 123 ) .
table 123 shows the results on these datasets .
on all datasets except glass bhc found the highest purity trees .
for glass , the gaussian assumption may have been poor .
this highlights the importance of model choice .
similarly , for classical distance - based hierarchical clustering methods , a poor choice of dis - tance metric may result in poor clusterings .
another advantage of the bhc algorithm , not fully addressed by purity scores alone , is that it tends to create hierarchies with good structure , particularly at high levels .
figure 123 compares the top three levels ( last
123let t be a tree with leaves 123; : : : ; n and c123; : : : ; cn be the known discrete class labels for the data points at the leaves .
pick a leaf uniformly at random; pick another leaf j uniformly in the same class , i . e .
c = cj .
find the smallest subtree containing and j .
measure the fraction of leaves in that subtree which are in the same class ( c ) .
the expected value of this fraction is the dendrogram purity , and can be computed exactly in a bottom up recursion on the dendrogram .
the purity is 123 i ( cid : 123 ) all leaves in each class are contained in some pure subtree .
figure 123
top level structure , of bhc ( left ) vs .
average linkage hc , for the newsgroup dataset .
the 123 words shown at each node have the highest mutual information between the cluster of documents at that node versus its sibling , and occur with higher frequency in that cluster .
the number of documents at each cluster is also given .
three merges ) of the newsgroups hierarchy ( using 123 examples and the 123 words with highest information gain ) from bhc and alhc .
continuing to look at lower levels does not improve alhc .
clhc and slhc perform similarly to alhc .
full dendrograms for this dataset and dendrograms of the 123digits dataset are available in the tech report ( heller & ghahramani ,
related work
the work in this paper is related to and inspired by several previous probabilistic approaches to cluster - ing123 , which we brie ( cid : 123 ) y review here .
stolcke and omo - hundro ( 123 ) described an algorithm for agglomera - tive model merging based on marginal likelihoods in the context of hidden markov model structure induc - tion .
williams ( 123 ) and neal ( 123 ) describe gaus - sian and di ( cid : 123 ) usion - based hierarchical generative mod - els , respectively , for which inference can be done us - ing mcmc methods .
similarly , kemp et al .
( 123 ) present a hierarchical generative model for data based on a mutation process .
ban ( cid : 123 ) eld and raftery ( 123 ) present an approximate
123there has also been a considerable amount of decision tree based work on bayesian tree structures for classi ( cid : 123 ) cation and regression , but this is not closely related to work presented here .
bayesian hierarchical clustering
method based on the likelihood ratio test statistic to compute the marginal likelihood for c and c ( cid : 123 ) 123 clusters and use this in an agglomerative algorithm .
vaithyanathan and dom ( 123 ) perform hierarchical clustering of multinomial data consisting of a vector of features .
the clusters are speci ( cid : 123 ) ed in terms of which subset of features have common distributions .
segal et al .
( 123 ) present probabilistic abstraction hierarchies ( pah ) which learn a hierarchical model in which each node contains a probabilistic model and the hierarchy favors placing similar models at neighboring nodes in the tree ( as measured by a dis - tance function between probabilstic models ) .
moni et al .
( 123 ) present an agglomerative algorithm for merging time series based on greedily maximizing marginal likelihood .
friedman ( 123 ) has also recently proposed a greedy agglomerative algorithm based on marginal likelihood which simultaneously clusters rows and columns of gene expression data .
the algorithm in our paper is di ( cid : 123 ) erent from the above algorithms in several ways .
first , unlike ( williams , 123; neal , 123; kemp et al . , 123 ) it is not in fact a hierarchical generative model of the data , but rather a hierarchical way of organizing nested clusters .
sec - ond our algorithm is derived from dirichlet process mixtures .
third the hypothesis test at the core of our algorithm tests between a single merged hypoth - esis and the alternative is exponentially many other clusterings of the same data ( not one vs two clusters at each stage ) .
lastly , our algorithm does not use any iterative method , like em , or require sampling , like mcmc , and is therefore signi ( cid : 123 ) cantly faster than most of the above algorithms .
we have presented a novel algorithm for bayesian hier - archical clustering based on dirichlet process mixtures .
this algorithm has several advantages over traditional approaches , which we have highlighted throughout the paper .
we have presented prediction and hyperparam - eter optimization procedures and shown that the al - gorithm provides competitive clusterings of real - world data as measured by purity with respect to known la - bels .
the algorithm can also be seen as an extremely fast alternative to mcmc inference in dpms .
the limitations of our algorithm include its inher - ent greediness , a computational complexity which is quadratic in the number of data points , and the lack of any incorporation of tree uncertainty .
in future work , we plan to try bhc on more com - plex component models for other realistic data|this
is likely to require approximations of the component marginal likelihoods ( 123 ) .
we also plan to extend bhc to systematically incorporate hyperparameter opti - mization and improve the running time to o ( n log n ) by exploiting a randomized version of the algorithm .
we need to compare this novel , fast inference algo - rithm for dpms to other inference algorithms such as mcmc ( rasmussen , 123 ) , ep ( minka & ghahra - mani , 123 ) and variational bayes ( blei & jordan , 123 ) .
we also hope to explore the idea of computing several alternative tree structures in order to create a manipulable tradeo ( cid : 123 ) between computation time and tightness of our lower bound .
there are many exciting avenues for further work in this area .
acknowledgements : thanks to david mackay , members of the gatsby unit , and members of cald at cmu for useful comments .
this project was par - tially supported by the eu pascal network of ex - cellence and zg was partially supported at cmu by darpa under the calo project .
