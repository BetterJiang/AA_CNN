factor analysis , a statistical method for modeling the covariance structure of high dimensional data using a small number of latent variables , can be extended by allowing di ( cid : 123 ) erent local factor models in di ( cid : 123 ) erent regions of the input space .
this results in a model which concurrently performs clustering and dimensionality reduction , and can be thought of as a reduced dimension mixture of gaussians .
we present an exact expectation ( maximization algorithm for ( cid : 123 ) tting the parameters of this mixture of factor
clustering and dimensionality reduction have long been considered two of the fundamental problems in unsupervised learning ( duda & hart , ; chapter ) .
in clustering , the goal is to group data points by similarity between their features .
conversely , in dimensionality reduction , the goal is to group ( or compress ) features that are highly correlated .
in this paper we present an em learning algorithm for a method which combines one of the basic forms of dimensionality reduction|factor analysis|with a basic method for clustering|the gaussian mixture model .
what results is a statistical method which concurrently performs clustering and , within each cluster , local dimensionality reduction .
local dimensionality reduction presents several bene ( cid : 123 ) ts over a scheme in which clustering and dimensionality reduction are performed separately .
first , di ( cid : 123 ) erent features may be correlated within di ( cid : 123 ) erent clusters and thus the metric for dimensionality reduction may need to vary between di ( cid : 123 ) erent clusters .
conversely , the metric induced in dimensionality reduction may guide the process of cluster formation|i . e .
di ( cid : 123 ) erent clusters may appear more separated depending on the local metric .
recently , there has been a great deal of research on the topic of local dimensionality reduction , resulting in several variants on the basic concept with successful applications to character and face recognition ( bregler and omohundro , ; kambhatla and leen , ; sung and poggio , ; schwenk and milgram , ; hinton et al . , ) .
the algorithm used by these authors for dimensionality reduction is principal components analysis ( pca ) .
figure : the factor analysis generative model ( in vector form ) .
pca , unlike maximum likelihood factor analysis ( fa ) , does not de ( cid : 123 ) ne a proper density model for the data , as the cost of coding a data point is equal anywhere along the principal component subspace ( i . e .
the density is un - normalized along these directions ) .
furthermore , pca is not robust to independent noise in the features of the data ( see hinton et al . , , for a comparison of pca and fa models ) .
hinton , dayan , and revow ( ) , also exploring an application to digit recognition , were the ( cid : 123 ) rst to extend mixtures of principal components analyzers to a mixture of factor analyzers .
their learning algorithm consisted of an outer loop of approximate em to ( cid : 123 ) t the mixture components , combined with an inner loop of gradient descent to ( cid : 123 ) t each individual factor model .
in this note we present an exact em algorithm for mixtures of factor analyzers which obviates the need for an outer and inner loop .
this simpli ( cid : 123 ) es the implementation , reduces the number of heuristic parameters ( i . e .
learning rates or steps of conjugate gradient descent ) , and can potentially result in speed - ups .
in the next section we present background material on factor analysis and the em al - gorithm .
this is followed by the derivation of the learning algorithm for mixture of factor analyzers in section .
we close with a discussion in section .
in maximum likelihood factor analysis ( fa ) , a p - dimensional real - valued data vector x is modeled using a k - dimensional vector of real - valued factors , z , where k is generally much smaller than p ( everitt , ) .
the generative model is given by :
x = ( cid : 123 ) z + u;
where ( cid : 123 ) is known as the factor loading matrix ( see figure ) .
the factors z are assumed to be n ( ; i ) distributed ( zero - mean independent normals , with unit variance ) .
the p - dimensional random variable u is distributed n ( ; ( cid : 123 ) ) , where ( cid : 123 ) is a diagonal matrix .
the diagonality of ( cid : 123 ) is one of the key assumptions of factor analysis : the observed variables are independent given the factors .
according to this model , x is therefore distributed with zero mean and covariance ( cid : 123 ) ( cid : 123 ) + ( cid : 123 ) ; and the goal of factor analysis is to ( cid : 123 ) nd the ( cid : 123 ) and ( cid : 123 ) that best model the covariance structure of x .
the factor variables z model correlations between the elements of x , while the u variables account for independent noise in each element of x .
the k factors play the same role as the principal components in pca : they are infor - mative projections of the data .
given ( cid : 123 ) and ( cid : 123 ) , the expected value of the factors can be
computed through the linear projection :
where ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( ( cid : 123 ) + ( cid : 123 ) ( cid : 123 ) ) ( cid : 123 ) , a fact that results from the joint normality of data and factors :
e ( zjx ) = ( cid : 123 ) x;
p " x
z # ! = n "
# ; " ( cid : 123 ) ( cid : 123 ) + ( cid : 123 ) ( cid : 123 )
i # ! :
note that since ( cid : 123 ) is diagonal , the p ( cid : 123 ) p matrix ( ( cid : 123 ) + ( cid : 123 ) ( cid : 123 ) ) , can be e ( cid : 123 ) ciently inverted using the matrix inversion lemma :
( ( cid : 123 ) + ( cid : 123 ) ( cid : 123 ) ) ( cid : 123 ) = ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( i + ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 )
where i is the k ( cid : 123 ) k identity matrix .
furthermore , it is possible ( and in fact necessary for em ) to compute the second moment of the factors ,
e ( zzjx ) = var ( zjx ) + e ( zjx ) e ( zjx )
= i ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) + ( cid : 123 ) xx
which provides a measure of uncertainty in the factors , a quantity that has no analogue in
the expectations ( ) and ( ) form the basis of the em algorithm for maximum likelihood
factor analysis ( see appendix a and rubin & thayer , ) :
e - step : compute e ( zjxi ) and e ( zzjxi ) for each data point xi , given ( cid : 123 ) and ( cid : 123 ) .
( cid : 123 ) new = n i ( cid : 123 ) ( cid : 123 ) new
where the diag operator sets all the o ( cid : 123 ) - diagonal elements of a matrix to zero .
mixture of factor analyzers
assume we have a mixture of m factor analyzers indexed by ! j , j = ; : : : ; m .
the generative model now obeys the following mixture distribution ( see figure ) :
p ( x ) =
xj=z p ( xjz; ! j ) p ( zj ! j ) p ( ! j ) dz :
as in regular factor analysis , the factors are all assumed to be n ( ; i ) distributed , therefore ,
p ( zj ! j ) = p ( z ) = n ( ; i ) :
( cid : 123 ) j ; ( cid : 123 ) j
figure : the mixture of factor analysis generative model .
whereas in factor analysis the data mean was irrelevant and was subtracted before ( cid : 123 ) tting the model , here we have the freedom to give each factor analyzer a di ( cid : 123 ) erent mean , ( cid : 123 ) j , thereby allowing each to model the data covariance structure in a di ( cid : 123 ) erent part of input space ,
p ( xjz; ! j ) = n ( ( cid : 123 ) j + ( cid : 123 ) jz; ( cid : 123 ) ) :
the parameters of this model are f ( ( cid : 123 ) j ; ( cid : 123 ) j ) m
j=; ( cid : 123 ) ; ( cid : 123 ) g; the vector ( cid : 123 ) parametrizes the adaptable mixing proportions , ( cid : 123 ) j = p ( ! j ) .
the latent variables in this model are the factors z and the mixture indicator variable ! , where wj = when the data point was generated by ! j .
for the e - step of the em algorithm , one needs to compute expectations of all the interactions of the hidden variables that appear in the log likelihood .
fortunately , the following statements can be easily veri ( cid : 123 ) ed ,
e ( wjzjxi ) = e ( wjjxi ) e ( zj ! j ; xi )
e ( wjzzjxi ) = e ( wjjxi ) e ( zzj ! j ; xi ) :
hij = e ( wjjxi ) / p ( xi; ! j ) = ( cid : 123 ) jn ( xi ( cid : 123 ) ( cid : 123 ) j ; ( cid : 123 ) j ( cid : 123 )
j + ( cid : 123 ) )
and using equations ( ) and ( ) we obtain
e ( wjzjxi ) = hij ( cid : 123 ) j ( xi ( cid : 123 ) ( cid : 123 ) j ) ;
where ( cid : 123 ) j ( cid : 123 ) ( cid : 123 )
j ( ( cid : 123 ) + ( cid : 123 ) j ( cid : 123 )
j ) ( cid : 123 ) .
similarly , using equations ( ) and ( ) we obtain
the em algorithm for mixtures of factor analyzers therefore becomes :
e ( wjzzjxi ) = hij ( cid : 123 ) i ( cid : 123 ) ( cid : 123 ) j ( cid : 123 ) j + ( cid : 123 ) j ( xi ( cid : 123 ) ( cid : 123 ) j ) ( xi ( cid : 123 ) ( cid : 123 ) j )
e - step : compute hij , e ( zjxi; ! j ) and e ( zzjxi; ! j ) for all data points i and mixture
m - step : solve a set of linear equations for ( cid : 123 ) j , ( cid : 123 ) j , ( cid : 123 ) j and ( cid : 123 ) ( see appendix b ) .
the mixture of factor analyzers is , in essence , a reduced dimensionality mixture of gaus - sians .
each factor analyzer ( cid : 123 ) ts a gaussian to a portion of the data , weighted by the posterior probabilities , hij .
since the covariance matrix for each gaussian is speci ( cid : 123 ) ed through the lower dimensional factor loading matrices , the model has mkp + p , rather than mp ( p + ) = , parameters dedicated to modeling covariance structure .
note that each model can also be allowed to have a separate ( cid : 123 ) matrix .
this , however , changes its
interpretation as sensor noise .
we have described an em algorithm for ( cid : 123 ) tting a mixture of factor analyzers .
matlab source code for the algorithm can be obtained from ftp : / / ftp . cs . toronto . edu / pub / zoubin / mfa . tar . gz .
an extension of this architecture to time series data , in which both the factors z and the discrete variables ! depend on their value at a previous time step , is currently
one of the important issues not addressed in this note is model selection .
in ( cid : 123 ) tting a mixture of factor analyzers the modeler has two free parameters to decide : the number of factor analyzers to use ( m ) , and the number of factor in each analyzer ( k ) .
one method by which these can be selected is cross - validation : several values of m and k are ( cid : 123 ) t to the data and the log likelihood on a validation set is used to select the ( cid : 123 ) nal values .
greedy methods based on pruning or growing the mixture may be more e ( cid : 123 ) cient at the cost of some performance loss .
alternatively , a full - ( cid : 123 ) edged bayesian analysis , in which these model parameters are integrated over , may also be possible .
we thank c .
bishop for comments on the manuscript .
the research was funded by grants from the canadian natural science and engineering research council and the ontario information technology research center .
geh is the nesbitt - burns fellow of the canadian institute for advanced research .
a em for factor analysis
the expected log likelihood for factor analysis is
q = e " logyi
( xi ( cid : 123 ) ( cid : 123 ) z ) ( cid : 123 ) ( cid : 123 ) ( xi ( cid : 123 ) ( cid : 123 ) z ) g#
= c ( cid : 123 ) n
= c ( cid : 123 ) n
log j ( cid : 123 ) j ( cid : 123 ) xi log j ( cid : 123 ) j ( cid : 123 ) xi ( cid : 123 )
i ( cid : 123 ) ( cid : 123 ) xi ( cid : 123 ) x
i ( cid : 123 ) ( cid : 123 ) xi ( cid : 123 ) x
i ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) e ( zjxi ) +
trh ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) e ( zzjxi ) i ( cid : 123 ) ;
where c is a constant , independent of the parameters , and tr is the trace operator .
to re - estimate the factor loading matrix we set
e ( zzjxl ) ! = xi
from which we get equation ( ) .
we re - estimate the matrix ( cid : 123 ) through its inverse , setting
( cid : 123 ) new ( cid : 123 ) xi ( cid : 123 )
i ( cid : 123 ) ( cid : 123 ) new
e ( zzjxi ) ( cid : 123 ) new ( cid : 123 ) = :
substituting equation ( ) ,
( cid : 123 ) new = xi
and using the diagonal constraint ,
i ( cid : 123 ) ( cid : 123 ) new
b em for mixture of factor analyzers
the expected log likelihood for mixture of factor analysis is
logyi yj ( cid : 123 ) ( ( cid : 123 ) ) p=j ( cid : 123 ) j ( cid : 123 ) = expf ( cid : 123 )
( xi ( cid : 123 ) ( cid : 123 ) j ( cid : 123 ) ( cid : 123 ) jz ) ( cid : 123 ) ( cid : 123 ) ( xi ( cid : 123 ) ( cid : 123 ) j ( cid : 123 ) ( cid : 123 ) jz ) g ( cid : 123 ) wj
to jointly estimate the mean ( cid : 123 ) j and the factor loadings ( cid : 123 ) j it is useful to de ( cid : 123 ) ne an
augmented column vector of factors
~ z = " z
and an augmented factor loading matrix ~ ( cid : 123 ) j = ( ( cid : 123 ) j ( cid : 123 ) j ) .
the expected log likelihood is then q = e = c ( cid : 123 ) n
( xi ( cid : 123 ) ~ ( cid : 123 ) j ~ z ) ( cid : 123 ) ( cid : 123 ) ( xi ( cid : 123 ) ~ ( cid : 123 ) j ~ z ) g ( cid : 123 ) wj
i ( cid : 123 ) ( cid : 123 ) ~ ( cid : 123 ) j e ( ~ zjxi; ! j ) +
i ( cid : 123 ) ( cid : 123 ) xi ( cid : 123 ) hijx
j ( cid : 123 ) ( cid : 123 ) ~ ( cid : 123 ) j e ( ~ z ~ zjxi; ! j ) i
logyi yj ( cid : 123 ) ( ( cid : 123 ) ) p=j ( cid : 123 ) j ( cid : 123 ) = expf ( cid : 123 ) log j ( cid : 123 ) j ( cid : 123 ) xi;j
where c is a constant .
to estimate ~ ( cid : 123 ) j we set
hij ( cid : 123 ) ( cid : 123 ) xie ( ~ zjxi; ! j ) + hij ( cid : 123 ) ( cid : 123 ) ~ ( cid : 123 ) new
j e ( ~ z ~ zjxi; ! j ) = :
this results in a linear equation for re - estimating the means and factor loadings ,
i = ~ ( cid : 123 ) new
j = xi
hijxie ( ~ zjxi; ! j ) ! xl
hlj e ( ~ z ~ zjxl; ! j ) ! ( cid : 123 )
e ( ~ zjxi; ! j ) = " e ( zjxi; ! j )
e ( ~ z ~ zjxl; ! j ) = " e ( zzjxl; ! j ) e ( zjxl; ! j )
we re - estimate the matrix ( cid : 123 ) through its inverse , setting
i ( cid : 123 ) hij ~ ( cid : 123 ) new
j e ( ~ zjxi; ! j ) x
j e ( ~ z ~ zjxi; ! j ) ~ ( cid : 123 ) new
substituting equation ( ) for ~ ( cid : 123 ) j and using the diagonal constraint on ( cid : 123 ) we obtain ,
hij ( cid : 123 ) xi ( cid : 123 ) ~ ( cid : 123 ) new
j e ( ~ zjxi; ! j ) ( cid : 123 ) x
finally , to re - estimate the mixing proportions we use the de ( cid : 123 ) nition ,
( cid : 123 ) j = p ( ! j ) = z p ( ! jjx ) p ( x ) dx :
since hij = p ( ! jjxi ) , using the empirical distribution of the data as an estimate of p ( x ) we
