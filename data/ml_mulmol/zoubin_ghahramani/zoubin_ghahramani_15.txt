an approach to semi - supervised learning is pro - posed that is based on a gaussian random eld model .
labeled and unlabeled data are rep - resented as vertices in a weighted graph , with edge weights encoding the similarity between in - stances .
the learning problem is then formulated in terms of a gaussian random eld on this graph , where the mean of the eld is characterized in terms of harmonic functions , and is efciently obtained using matrix methods or belief propa - gation .
the resulting learning algorithms have intimate connections with random walks , elec - tric networks , and spectral graph theory .
we dis - cuss methods to incorporate class priors and the predictions of classiers obtained by supervised learning .
we also propose a method of parameter learning by entropy minimization , and show the algorithms ability to perform feature selection .
promising experimental results are presented for synthetic data , digit classication , and text clas -
in many traditional approaches to machine learning , a tar - get function is estimated using labeled data , which can be thought of as examples given by a teacher to a student .
labeled examples are often , however , very time consum - ing and expensive to obtain , as they require the efforts of human annotators , who must often be quite skilled .
for in - stance , obtaining a single labeled example for protein shape classication , which is one of the grand challenges of bio - logical and computational science , requires months of ex - pensive analysis by expert crystallographers .
the problem of effectively combining unlabeled data with labeled data is therefore of central importance in machine learning .
the semi - supervised learning problem has attracted an in - creasing amount of interest recently , and several novel ap - proaches have been proposed; we refer to ( seeger , 123 ) for an overview .
among these methods is a promising fam - ily of techniques that exploit the manifold structure of the data; such methods are generally based upon an assumption that similar unlabeled examples should be given the same classication .
in this paper we introduce a new approach to semi - supervised learning that is based on a random eld model dened on a weighted graph over the unlabeled and labeled data , where the weights are given in terms of a sim - ilarity function between instances .
unlike other recent work based on energy minimization and random elds in machine learning ( blum & chawla , 123 ) and image processing ( boykov et al . , 123 ) , we adopt gaussian elds over a continuous state space rather than random elds over the discrete label set .
this re - laxation to a continuous rather than discrete sample space results in many attractive properties .
in particular , the most probable conguration of the eld is unique , is character - ized in terms of harmonic functions , and has a closed form solution that can be computed using matrix methods or loopy belief propagation ( weiss et al . , 123 ) .
in contrast , for multi - label discrete random elds , computing the low - est energy conguration is typically np - hard , and approxi - mation algorithms or other heuristics must be used ( boykov et al . , 123 ) .
the resulting classication algorithms for gaussian elds can be viewed as a form of nearest neigh - bor approach , where the nearest labeled examples are com - puted in terms of a random walk on the graph .
the learning methods introduced here have intimate connections with random walks , electric networks , and spectral graph the - ory , in particular heat kernels and normalized cuts .
in our basic approach the solution is solely based on the structure of the data manifold , which is derived from data features .
in practice , however , this derived manifold struc - ture may be insufcient for accurate classication
proceedings of the twentieth international conference on machine learning ( icml - 123 ) , washington dc , 123
to take val -
is the combinatorial
our strategy is to rst compute a real - valued function
, we form is an inverse is the partition function , which normalizes over
it is not difcult to show that the minimum energy function is harmonic; namely , it satises
weightings are possible , of course , and may be more appro - is discrete or symbolic .
for our purposes the fully species the data manifold structure ( see
intuitively , we want unlabeled points that are nearby in the graph to have similar labels .
this motivates the choice of the quadratic energy function
on123 with certain nice properties , and to then assign labels based on_ .
we constrain_ k on the labeled databd ! e . f@ .
to assign a probability distribution on functions_ the gaussian eldkl temperature parameter , andwxl all functions constrained to_ on the labeled data .
, and is equal to_ ! + on unlabeled data points; on the labeled data points123 laplacian , given in matrix form as is the diagonal matrix with entries ) km is the weight matrix .
the harmonic property means that the value of_ unlabeled data point is the average of_ _ with respect to the graph .
expressed slightly differently , principle of harmonic functions ( doyle & snell , 123 ) , _ unique and is either a constant or it satises+c matrix operations , we split the weight matrixe ) into 123 blocks after the th row and column :
denotes the values on the un - labeled data points , the harmonic solution is given by
to compute the harmonic solution explicitly in terms of
which is consistent with our prior notion of smoothness of
because of the maximum
figure123the random elds used in this work are constructed on labeled and unlabeled examples .
we form a graph with weighted edges between instances ( in this case scanned digits ) , with labeled data items appearing as special boundarypoints , and unlabeled points as interiorpoints .
we consider gaussian random elds on this graph .
show how the extra evidence of class priors can help classi - cation in section 123
alternatively , we may combine exter - nal classiers using vertex weights or assignment costs , as described in section 123
encouraging experimental re - sults for synthetic data , digit classication , and text clas - sication tasks are presented in section 123
one difculty with the random eld approach is that the right choice of graph is often not entirely clear , and it may be desirable to learn it from data .
in section 123 we propose a method for learning these weights by entropy minimization , and show the algorithms ability to perform feature selection to better characterize the data manifold .
basic framework
we suppose there are labeled points
and unlabeled points ; typically let " ! #%$& be the total number of data points .
to be - gin , we assume the labels are binary : ( ' * ) , + - . / .
consider a connected graph123 ! 123 with nodes123 ing to the data points , with nodes123 " ! 123 ) 123% : / corre - sponding to the labeled points with labels
nodes;< ! = ) >$ ? . 123@%$a / corresponding to the unla - beled points .
our task is to assign labels to nodes; assume an cbd symmetric weight matrixe of the graph is given .
for example , when< ' gfih
is the ) - th component of instance
as a vector
are length scale hyperparameters for each dimension .
thus , nearby points in euclidean space are assigned large edge weight
on the edges
weight matrix can be
figure123demonstration of harmonic energy minimization on two synthetic datasets .
large symbols indicate labeled data , other points are unlabeled .
in this paper we focus on the above harmonic function as a basis for semi - supervised classication .
however , we em - phasize that the gaussian random eld model from which this function is derived provides the learning framework with a consistent probabilistic semantics .
in the following , we refer to the procedure described above as harmonic energy minimization , to underscore the har - monic property ( 123 ) as well as the objective function being minimized .
figure 123 demonstrates the use of harmonic en - ergy minimization on two synthetic datasets .
the left gure
shows that the data has three bands , with123 ! ,
gfg ; the right gure shows two spirals , with .
, and ( .
here we see harmonic
energy minimization clearly follows the structure of data , while obviously methods such as knn would fail to do so .
interpretation and connections
as outlined briey in this section , the basic framework pre - sented in the previous section can be viewed in several fun - damentally different ways , and these different viewpoints provide a rich and complementary set of techniques for rea - soning about this approach to the semi - supervised learning
random walks and electric networks
is the probability that
label 123
here the labeled data is viewed as an absorbing boundary for the random walk .
imagine a particle walking along the graph123 from an unlabeled nodeb , it moves to a nodei with proba - knm after one step .
the walk continues until the par - ticle hits a labeled node .
then_ the particle , starting from nodeb , hits a labeled node with ences .
first , we x the value of_ on the labeled points ,
this view of the harmonic solution indicates that it is closely related to the random walk approach of szummer and jaakkola ( 123 ) , however there are two major differ -
and second , our solution is an equilibrium state , expressed in terms of a hitting time , while in ( szummer & jaakkola ,
graph kernels
we will return to this point when discussing heat kernels .
is the voltage in the resulting electric network on each of
the solution to the heat equation on the graph with initial
be chosen using an auxiliary technique , for example cross -
and lafferty ( 123 ) propose this as an appropriate kernel for machine learning with categorical data .
when used in a kernel method such as a support vector machine , the kernel
an electrical network interpretation is given in ( doyle & to be resistors with to a positive
harmonic property here follows from kirchoffs and ohms laws , and the maximum principle then shows that this is precisely the same solution obtained in ( 123 ) .
123 ) the walk crucially depends on the time parameter .
snell , 123 ) .
imagine the edges of123 .
we connect nodes labeled .
to ground .
then_ voltage source , and points labeled+ the unlabeled nodes .
furthermore_ minimizes the energy for the given_ dissipation of the electric network123 the solution_ can be viewed from the viewpoint of spec - tral graph theory .
the heat kernel with time parameter on the graph123
is dened as conditions being a point source atb at time ! + .
kondor
i ) can be viewed as a solution to the heat equation with initial heat sources on the labeled data .
the time parameter must , however , dent of , the diffusion time .
let , be the lower right laplacian restricted to the unlabeled nodes in123 describes heat diffusion on the unlabeled subgraph with ! ? , which can be expressed in terms of the integral over time of the heat kernel
a kernel classier with the kernel laplacian . ) from ( 123 ) we also see that the spectrum of is the spectrum of / , where ) %$ > .
this indicates
expression ( 123 ) shows that this approach can be viewed as and a specic form of kernel machine .
( see also ( chung & yau , 123 ) , where a normalized laplacian is used instead of the combinatorial
a connection to the work of chapelle et al .
( 123 ) , who ma - nipulate the eigenvalues of the laplacian to create various
dirichlet boundary conditions on the labeled nodes .
the is the inverse operator of the restricted
our algorithm uses a different approach which is indepen -
the harmonic solution ( 123 ) can then be written as
the heat kernel on this submatrix :
, it is the
kernels .
a related approach is given by belkin and niyogi
spectral clustering and graph mincuts
the normalized cut approach of shi and malik ( 123 ) has as its objective function the minimization of the raleigh
smallest eigenvector of the generalized eigenvalue problem .
yu and shi ( 123 ) add a grouping bias to the normalized cut to specify which points should be in the same group .
since labeled data can be encoded into such pairwise grouping constraints , this technique can be applied to semi - supervised learning as well .
is close to block diagonal , it can be shown that data points are tightly clustered in the eigenspace spanned ( ng et al . , 123a; meila & shi , 123 ) , leading to various spectral clustering algo -
( 123 ) , who propose to regularize functions on123 by select - ing the topk normalized eigenvectors of to the smallest eigenvalues , thus obtaining the best t to_ in the least squares sense .
we remark that our_ ts the labeled data exactly , while the orderk approximation may subject to the constraint_ .
the solution is the second by the rst few eigenvectors of for this work is also a weighted graph123 minimum - cut , where negative labeled data is connected ( with large weight ) to a special source node , and positive labeled data is connected to a special sink node .
a mini - - cut , which is not necessarily unique , minimizes the and corresponds to a function_#` . 123y$ . f / ; the $ . / , but the eld is pinned on over the label space )
perhaps the most interesting and substantial connection to the methods we propose here is the graph mincut approach proposed by blum and chawla ( 123 ) .
the starting point , but the semi - supervised learning problem is cast as one of nding a
the labeled entries .
because of this constraint , approxima - tion methods based on rapidly mixing markov chains that apply to the ferromagnetic ising model unfortunately can - not be used .
moreover , multi - label extensions are generally np - hard in this framework .
in contrast , the harmonic so - lution can be computed efciently using matrix methods , even in the multi - label case , and inference for the gaussian random eld can be efciently and accurately carried out using loopy belief propagation ( weiss et al . , 123 ) .
solutions can be obtained using linear programming .
the corresponding random eld model is a traditional eld
incorporating class prior knowledge
to labels , the obvious decision rule is to
tends to produce severely unbalanced classication .
we call this rule the harmonic threshold ( abbreviated thresh below ) .
in terms of the random walk interpreta -
more likely to reach a positively labeled point before a neg - atively labeled point .
this decision rule works well when the classes are well separated .
however in real datasets ,
, which species the data manifold , is often poorly estimated in practice and does not reect the classication goal .
in other words , we should not fully trust the graph structure .
the class priors are a valuable piece of complementary information
to go from_ assign label 123 to nodeb z , and label 123 other - z , then starting atb , the random walk is classes are often not ideally separated , and using_ the problem stems from the fact thate assume the desirable proportions for classes 123 and 123 are
, respectively , where these values are either given the mass of class 123 to be b , and the mass of class 123 b .
class mass normalization scales these masses so that an unlabeled pointb
by an oracle or estimated from labeled data .
we adopt a simple procedure called class mass normalization ( cmn ) to adjust the class distributions to match the priors
this method extends naturally to the general multi - label
is classied as class 123
incorporating external classiers
often we have an external classier at hand , which is con - structed on labeled data alone .
in this section we suggest how this can be combined with harmonic energy minimiza -
the original graph , we attach a dongle node which is a la -
assume the external classier produces labels % on the unlabeled data; can be 123 / 123 or soft labels in with harmonic energy minimization by a sim - ple modication of the graph .
for each unlabeled nodeb k , let the transition probability from beled node with value b to its dongle be , and discount all other transitions fromb
we then perform harmonic energy minimization on this augmented graph .
thus , the external classier in - troduces assignment costs to the energy function , which play the role of vertex potentials in the random eld .
is not difcult to show that the harmonic solution on the augmented graph is , in the random walk view ,
we note that throughout the paper we have assumed the labeled data to be noise free , and so clamping their values
makes sense .
if there is reason to doubt this assumption , it would be reasonable to attach dongles to labeled nodes as well , and to move the labels to these new nodes .
learning the weight matrix
shown to be useful as a feature selection mechanism which better aligns the graph structure with the data .
the usual parameter learning criterion is to maximize the likelihood of labeled data .
however , the likelihood crite -
and xed .
in this section , we investigate learning weight functions of the form given by equation ( 123 ) .
we will learn
is the entropy of the eld at the individual unlabeled data relying on the maximum principle of harmonic functions
labeled data are xed during training , and moreover likeli - hood doesnt make sense for the unlabeled data because we do not have a generative model .
we propose instead to use average label entropy as a heuristic criterion for parameter learning .
the average label entropy
previously we assumed that the weight matrixe w s from both labeled and unlabeled data; this will be rion is not appropriate in this case because the_ values for of the eld_
pointb .
here we use the random walk interpretation of_ forb $ .
small which guarantees that+c entropy implies that_ the intuition that a goode / ) should result in a condent labeling .
we are constraining_ on the labeled datamost of these small and lends itself well to tuning the ( minimum at 123 as ( + .
as the length scale approaches that is closest to some labeled point ; s label , put ( 123 ) label with
zero , the tail of the weight function ( 123 ) is increasingly sen - sitive to the distance .
in the end , the label predicted for an unlabeled example is dominated by its nearest neighbors label , which results in the following equivalent labeling procedure : ( 123 ) starting from the labeled data set , nd the
in the labeled set and re - peat .
since these are hard labels , the entropy is zero .
this solution is desirable only when the classes are extremely well separated , and can be expected to be inferior other -
there are of course many arbitrary labelings of the data that have low entropy , which might suggest that this criterion will not work .
however , it is important to point out that
arbitrary low entropy labelings are inconsistent with this constraint .
in fact , we nd that the space of low entropy labelings achievable by harmonic energy minimization is
is close to 123 or 123; this captures ( equivalently , a good set of hy -
there is a complication , however , which is that
is the uniform matrix
the gradient is computed as
this complication can be avoided by smoothing the tran - sition matrix .
inspired by analysis of the pagerank algo -
rithm in ( ng et al . , 123b ) , we replace with the smoothed we use gradient descent to nd the hyperparameters (
w can be read off the vector
w , which is given by where the values
w are sub - matrices of
using the fact that )
since the original transition matrix tained by normalizing the weight matrixe
in the above derivation we use_ as label probabilities di - rectly; that is , k class b .
if we incorpo -
and we use this probability in place of_
rate class prior information , or combine harmonic energy minimization with other classiers , it makes sense to min - imize entropy on the combined probabilities .
for instance , if we incorporate a class prior using cmn , the probability is given by
in ( 123 ) .
the derivation of the gradient descent rule is a straightforward extension of the above analysis .
, we have that
experimental results
we rst evaluate harmonic energy minimization on a hand - written digits dataset , originally from the cedar buffalo binary digits database ( hull , 123 ) .
the digits were pre - processed to reduce the size of each image down to a grid by down - sampling and gaussian smooth - ing , with pixel values ranging from 123 to 123 ( le cun et al . , 123 ) .
each image is thus represented by a 123 - dimensional vector .
we compute the weight matrix ( 123 ) with tested , we perform
for each labeled set size
labeled set size
cmn + vp thresh + vp
labeled set size
labeled set size
figure123harmonic energy minimization on digits 123vs .
123 ( left ) and on all 123 digits ( middle ) and combining voted - perceptron with harmonic energy minimization on odd vs .
even digits ( right )
labeled set size
labeled set size
labeled set size
figure123harmonic energy minimization on pc vs .
mac ( left ) , baseball vs .
hockey ( middle ) , and ms - windows vs .
mac ( right )
123 trials .
in each trial we randomly sample labeled data from the entire dataset , and use the rest of the images as unlabeled data .
if any class is absent from the sampled la - beled set , we redo the sampling .
for methods that incorpo - from the labeled set with
rate class priors , we estimate
laplace ( add one ) smoothing .
we consider the binary problem of classifying digits 123 vs .
123 , with 123 images in each class .
we report aver - age accuracy of the following methods on unlabeled data : thresh , cmn , 123nn , and a radial basis function classier
rbf and 123nn are used simply as baselines .
the results are shown in figure 123
clearly thresh performs poorly , because
ity of examples are classied as digit 123
this shows the inadequacy of the weight function ( 123 ) based on pixel - wise
( rbf ) which classies to class 123 iffe* the values of_ are generally close to 123 , so the major - euclidean distance .
however the relative rankings of_
, so that the class proportion ts the prior method is inferior to cmn due to the error in estimating
are useful , and when coupled with class prior information signicantly improved accuracy is obtained .
the greatest improvement is achieved by the simple method cmn .
we could also have adjusted the decision threshold on threshs
and it is not shown in the plot .
these same observations are also true for the experiments we performed on several other binary digit classication problems .
we also consider the 123 - way problem of classifying digits 123 through 123
we report the results on a dataset with in - tentionally unbalanced class sizes , with 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 , 123 examples per class , respec - tively ( noting that the results on a balanced dataset are sim - ilar ) .
we report the average accuracy of thresh , cmn , rbf , and 123nn .
these methods can handle multi - way classica - tion directly , or with slight modication in a one - against - all fashion .
as the results in figure 123 show , cmn again im - proves performance by incorporating class priors .
next we report the results of document categorization ex - periments using the 123 newsgroups dataset .
we pick three binary problems : pc ( number of documents : 123 ) vs .
mac ( 123 ) , ms - windows ( 123 ) vs .
mac , and base - ball ( 123 ) vs .
hockey ( 123 ) .
each document is minimally processed into a tf . idf vector , without applying header re - moval , frequency cutoff , stemming , or a stopword list
123 nearest neighbors or if bors , as measured by cosine similarity .
we use the follow - ing weight function on the edges :
is among s 123 nearest neigh -
are connected by an edge if
we use one - nearest neighbor and the voted perceptron al - gorithm ( freund & schapire , 123 ) ( 123 epochs with a lin -
ear kernel ) as baselinesour results with support vector ma - chines are comparable .
the results are shown in figure 123
as before , each point is the average of 123 random tri - als .
for this data , harmonic energy minimization performs much better than the baselines .
the improvement from the class prior , however , is less signicant .
an explanation for why this approach to semi - supervised learning is so effec - tive on the newsgroups data may lie in the common use of
quotations within a topic thread : document quotes part of
although documents far apart in the thread may be quite different , they are linked by edges in the graphical repre - sentation of the data , and these links are exploited by the
z quotes part z , and so on
incorporating external classiers
we use the voted - perceptron as our external classier .
for each random trial , we train a voted - perceptron on the la - beled set , and apply it to the unlabeled set .
we then use the
123 / 123 hard labels for dongle values , and perform harmonic energy minimization with ( 123 ) .
we use ! +j
we evaluate on the articial but difcult binary problem of classifying odd digits vs .
even digits; that is , we group 123 , 123 , 123 , 123 , 123 and 123 , 123 , 123 , 123 , 123 into two classes .
there are 123 images per digit .
we use second order polynomial kernel in the voted - perceptron , and train for 123 epochs .
figure 123 shows the results .
the accuracy of the voted - perceptron on unlabeled data , averaged over trials , is marked vp in the plot .
independently , we run thresh and cmn .
next we combine thresh with the voted - perceptron , and the result is marked thresh+vp .
finally , we perform class mass nor - malization on the combined result and get cmn+vp .
the combination results in higher accuracy than either method alone , suggesting there is complementary information used
, results on a toy dataset are shown in figure 123
the upper grid is slightly tighter than the lower grid , and they are connected by a few data points .
there are two labeled examples , marked with large symbols .
we learn the optimal length scales for this dataset by minimizing entropy on unlabeled data .
learning the weight matrixe to demonstrate the effects of estimatinge the two dimensions , so there is only a single parameter ( approaches the minimum at 123 as ( + .
under such con -
ditions , the results of harmonic energy minimization are usually undesirable , and for this dataset the tighter grid invades the sparser one as shown in figure 123 ( a ) .
with smoothing , the nuisance minimum at 123 gradually disap - pears as the smoothing factor grows , as shown in figure
to simplify the problem , we rst tie the length scales in
to learn .
as noted earlier , without smoothing , the entropy
figure123the effect of parameter mization .
( a ) if unsmoothed , as performs poorly .
( b ) result at optimal
on harmonic energy mini - , and the algorithm
, smoothed with ( c ) smoothing helps to remove the entropy minimum .
when we set
and we reach a minimum entropy of 123 bits .
length scale is shown in figure 123 ( b ) , which is able to dis - tinguish the structure of the two grids .
for each dimension , parameter learning is more dramatic .
with the same smoothing of
stabilizes at 123 , is legitimate; it means that the learning al -
on both the labeled and unlabeled data .
harmonic energy minimization under these parameters gives the same clas - sication as shown in figure 123 ( b ) .
, the minimum entropy is 123 .
harmonic energy minimization under this if we allow a separate ( ( keeps growing towards innity ( we use
for computation ) while ( gorithm has identied the - direction as irrelevant , based next we learn ( s for all 123 dimensions on the 123 vs .
123 dimensions sharing the same ( f+ as in previous ex - periments .
then we compute the derivatives of ( increase .
the learned ( s shown in the rightmost plot of figure 123 range from 123 ( black ) to 123 ( white ) .
a small ( k ( white ) .
we can discern the shapes of a black 123 and
dimension separately , and perform gradient descent to min - imize the entropy .
the result is shown in table 123
as entropy decreases , the accuracy of cmn and thresh both
digits dataset .
for this problem we minimize the entropy with cmn probabilities ( 123 ) .
we randomly pick a split of 123 labeled and 123 unlabeled examples , and start with all
( black ) indicates that the weight is more sensitive to varia - tions in that dimension , while the opposite is true for large
a white 123 in this gure; that is , the learned parameters
123 % 123 123 % 123
table123entropy of cmn and accuracies before and after learning s on the 123vs .
123dataset .
figure123learned s for 123vs .
123dataset .
from left to right : average 123 , average 123 , initial s , learned s .
exaggerate variations within class 123 while suppressing variations within class 123
we have observed that with the default parameters , class 123 has much less variation than class 123; thus , the learned parameters are , in effect , compensating for the relative tightness of the two classes in
we have introduced an approach to semi - supervised learn - ing based on a gaussian random eld model dened with respect to a weighted graph representing labeled and unla - beled data .
promising experimental results have been pre - sented for text and digit classication , demonstrating that the framework has the potential to effectively exploit the structure of unlabeled data to improve classication accu - racy .
the underlying random eld gives a coherent proba - bilistic semantics to our approach , but this paper has con - centrated on the use of only the mean of the eld , which is characterized in terms of harmonic functions and spectral graph theory .
the fully probabilistic framework is closely related to gaussian process classication , and this connec - tion suggests principled ways of incorporating class priors and learning hyperparameters; in particular , it is natural to apply evidence maximization or the generalization er - ror bounds that have been studied for gaussian processes ( seeger , 123 ) .
our work in this direction will be reported in a future publication .
