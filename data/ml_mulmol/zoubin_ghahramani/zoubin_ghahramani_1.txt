real - world learning tasks may involve high - dimensional data sets with arbitrary patterns of missing data .
in this paper we present a framework based on maximum likelihood density estimation for learning from such data set . s .
vve use mixture models for the den ( cid : 123 ) sity estimates and make two distinct appeals to the expectation ( cid : 123 ) maximization ( em ) principle ( dempster et al . , 123 ) in deriving a learning algorithm - em is used both for the estimation of mix ( cid : 123 ) ture components and for coping wit . h missing dat . a .
the result ( cid : 123 ) ing algorithm is applicable t . o a wide range of supervised as well as unsupervised learning problems .
result . s from a classification benchmark - t . he iris data set - are presented .
adaptive systems generally operate in environments t . hat are fraught with imper ( cid : 123 ) fections; nonet . heless they must cope with these imperfections and learn to extract as much relevant information as needed for their part . icular goals .
one form of imperfection is incomplet . eness in sensing information .
incompleteness can arise ex ( cid : 123 ) trinsically from the data generation process and intrinsically from failures of the system ' s sensors .
for example , an object recognition system must be able to learn to classify images with occlusions , and a robotic controller must be able to integrate multiple sensors even when only a fraction may operate at any given time .
in this paper we present a .
fra . mework - derived from parametric statistics - for learn -
supervised learning from incomplete data via an em approach
ing from data sets with arbitrary patterns of incompleteness .
learning in this frame ( cid : 123 ) work is a classical estimation problem requiring an explicit probabilistic model and an algorithm for estimating the parameters of the model .
a possible disadvantage of parametric methods is their lack of flexibility when compared with nonparamet ( cid : 123 ) ric methods .
this problem , however , can be largely circumvented by the use of mixture models ( mclachlan and basford , 123 ) .
mixture models combine much of the flexibility of nonparametric methods with certain of the analytic advantages of
mixture models have been utilized recently for supervised learning problems in the form of the " mixtures of experts " architecture ( jacobs et al . , 123; jordan and jacobs , 123 ) .
this architecture is a parametric regression model with a modular structure similar to the nonparametric decision tree and adaptive spline models ( breiman et al . , 123; friedman , 123 ) .
the approach presented here differs from these regression - based approaches in that the goal of learning is to estimate the density of the data .
no distinction is made between input and output variables; the joint density is estimated and this estimate is then used to form an input / output map .
similar approaches have been discussed by specht ( 123 ) and tresp et al .
( 123 ) .
to estimate the vector function y = i ( x ) the joint density p ( x , y ) is esti ( cid : 123 ) mated and , given a particular input x , the conditional density p ( ylx ) is formed .
to obtain a single estimate of y rather than the full conditional density one can evaluate y = e ( ylx ) , the expectation of y given x .
the density - based approach to learning can be exploited in several ways .
first , having an estimate of the joint density allows for the representation of any rela ( cid : 123 ) tion between the variables .
from p ( x , y ) , we can estimate y = i ( x ) , the inverse x = 123 - 123 ( y ) , or any other relation between two subsets of the elements of the con ( cid : 123 ) catenated vector ( x , y ) .
second , this density - based approach is applicable both to supervised learning and unsupervised learning in exactly the same way .
the only distinction between su ( cid : 123 ) pervised and unsupervised learning in this framework is whether some portion of the data vector is denoted as " input " and another portion as " target " .
third , as we discuss in this paper , the density - based approach deals naturally with incomplete data , i . e .
missing values in the data set .
this is because the problem of estimating mixture densities can itself be viewed as a missing data problem ( the " labels " for the component densities are missing ) and an expectation - maximization ( em ) algorithm ( dempster et al . , 123 ) can be developed to handle both kinds of
123 density estimation using em
this section outlines the basic learning algorithm for finding the maximum like ( cid : 123 ) lihood parameters of a mixture model ( dempster et al . , 123; duda and hart , 123; nowlan , 123 ) .
\ive assume that .
t . he data .
. : t ' = ( xl , . . .
, xn ) are generated independently from a mixture density
p ( xi ) = lp ( xi iwj; ( ) j ) p ( wj ) ,
ghahramani and jordan
where each component of the mixture is denoted wj and parametrized by ( ) j .
from equation ( 123 ) and the independence assumption we see that the log likelihood of the parameters given the data set is
l ( ( ) ix ) = lloglp ( xilwj;oj ) p ( wj ) .
by the maximum likelihood principle the best model of the data has parameters that maximize l ( oix ) .
this function , however , is not easily maximized numerically because it involves the log of a sum .
intuitively , there is a " credit - assignment " problem : it is not clear which component of the mixture generated a given data point and thus which parameters to adjust to fit that data point .
the em algorithm for mixture models is an iterative method for solving this credit - assignment problem .
the intuition is that if one had access to a " hidden " random variable z that indicated which data point was genera . ted by which component , then the maximization problem would decouple into a set of simple maximizations .
using the indicator variable z , a " complete - data " log likelihood function can be written
lc ( ( ) ix , z ) = l l zij log p ( xdzi; o ) p ( zi; ( ) ) ,
which does not involve a log of a summation .
since z is unknown lc cannot be utilized directly , so we instead work with its ex ( cid : 123 ) pectation , denoted by q ( oi ( ) k ) ' as shown by ( dempster et ai . , 123 ) , l ( oix ) can be maximized by iterating the following two steps :
the e ( expectation ) step computes the expected complete data log likelihood and the m ( maximization ) step finds the parameters that maximize this likelihood .
these two steps form the basis of the em algorithm; in the next two sections we will outline how they can be used for real and discrete density estimation .
123 real - valued data : inixture of gaussians
real - valued data can be modeled as a mixture of gaussians .
for this model the
e - step simplifies to computing hij = e ( zijlxi , ok ) , the probability that gaussian j ,
as defined by the parameters estimated at time step k , generated data point i .
itj 123 - 123 / 123 exp ( - ~ ( xi -
itj ) tt;l , k ( xi - itj ) )
l ~ 123 iefl - l / 123exp ( - ~ ( xi - it123 ) te , i , k ( xi - it123 ) ) '
the m - step re - estimates the data set weighted by the hii=
means and covariances of the gaussians123 using the
) ~ k+l _ l ~ l hijxi
123 though this derivation assumes equal priors for the gaussians , if the priors arc viewed
as mixing parameters they can also be learned in the maximization step .
supervised learning from incomplete data via an em approach
123 discrete - valued data : inixture of bernoullis
d - dimensional binary data x = ( xl , .
xd ) , xd e ( o , 123 ) , can be modeled as a mixture of ! ii bernoulli densities .
that is ,
p ( xio ) = l p ( wj ) it / - ljd ( 123 - / - ljd ) ( l - xd ) .
for this model the e - step involves computing
px , ld ( 123 _ p .
) ( 123 - xld )
' ef ' ! l nf=l p123j d ( 123 - pld ) ( 123 - xld ) ,
and the m - step again re - estimates the parameters by
~ k+l _ ' e ~ l hijxi
more generally , discrete or categorical data can be modeled as generated by a mix ( cid : 123 ) ture of multinomial densities and similar derivations for the learning algorithm can be applied .
finally , the extension to data with mixed real , binary .
and categorical dimensions can be readily derived by assuming a joint density with mixed compo ( cid : 123 ) nents of the three types .
123 learning from inco123plete data
in the previous section we presented one aspect of the em algorithm : learning mixture models .
another important application of em is to learning from data sets with missing values ( little and rubin , 123; dempster et ai . , 123 ) .
this application has been pursued in the statistics literature for non - mixture density estimation problems; in this paper we combine this application of em with that of learning mixture parameters .
we assume that .
the data set , l : ' = ( xl . .
, xn ) is divided into an observed com ( cid : 123 ) ponent , yo and a missing component ;t ' m .
similarly , each data vector xi is divided into ( xi , xi ) where each data vector can have different missing components - this would be denoted by superscript dli and oj .
but we have simplified the notation for the sake of clarity .
to handle missing data we rewrite the em algorithm as follows
e ( ic ( fjl , t ' , ;t ' m , z ) i;t '
comparing to equation ( 123 ) we see that aside from t . he indicator variables z we have added a second form of incomplete data , ;t ' m , corresponding to the missing values in the data set .
the e - step of the algorithm estimates both these forms of missing information; in essence it uses the current estimate of the data density to complete the missing values .
ghahramani and jordan
123 real - valued data : mixture of gaussians
we start by writing the log likelihood of the complete data ,
ic ( oixo , xm , z ) = l l zij log p ( xdzj , 123 ) + l l zij log p ( zdo ) .
we can ignore the second term since we will only be estimating the parameters of the p ( xdzi , 123 ) .
using equation ( 123 ) for the mixture of gaussians we not . e that if only the indicator variables zi are missing , the e step can be reduced to estimating e ( zij lxi , 123 ) .
for the case we are interested in , with two types of missing data zi and xi , we expand equation ( 123 ) using m and 123 superscripts to denote subvectors and submatrices of the parameters matching the missing and observed components of
ic ( oixo , xm , z ) = l l zij ( n log123r + ! log iej 123 - ! ( xi - l123 - jf e;l , oo ( xi - l123 - j )
m ) 123 ( m - 123 xi
- xi -
note that after taking the expectation , the sufficient statistics for the parameters involve three unknown terms , zij , zijxi , and zijxixit .
thus we must compute : e ( zijlx ? , ok ) ' e ( zijxilx ? , ok ) , and e ( zijxixintlx ? , ok ) .
one intuitive approach to dealing with missing data is to use the current estimate of the data density to compute the expectat . ion of the missing data in an e - step , complete the data with these expectations , and then use this completed data to re ( cid : 123 ) estimate parameters in an m - step .
however , this intuition fails even when dealing with a single two - dimensional gaussian; the expectation of the missing data always lies along a line , which biases the estimate of the covariance .
on the other hand , the approach arising from application of the em algorithm specifies that one should use the current density estimate to compute the expectation of whatever incomplete terms appear in the likelihood maximization .
for the mixture of gaussians these incomplete terms involve interactions between the indicator variable : ;ij and the first and second moments of xi .
thus , simply computing the expectation of the missing data zi and xi from our model and substituting those values into the m step is not sufficient to guarantee an increase in the likelihood of the parameters .
the above terms can be computed as follows : e ( zij lxi , ok ) is again hij , the proba ( cid : 123 ) bility as defined in ( 123 ) measured only on the observed dimensions of xi , and e ( zijxilxi , ok ) = hije ( xilzij = 123 , xi , od = hij ( l123 - j + ejoejo - l ( xi - il . ' ) , ( 123 ) defining xi ) = e ( xi izij = 123 , xi , ok ) , the regression of xi on xi using gaussian j ,
m mti 123 ) _ h . .
( ~ mm ~ mo ~ oo - l ~ mot ~ m ~ mt )
xi ' k -
the m - step uses these expectations substituted into equations ( 123 ) a and ( 123 ) b to re - estimate the means and covariances .
to re - estimate the mean vector , i123 - j ' we substitute the values e ( xilzij = 123 , xi , ok ) for the missing components of xi in equation ( 123 ) a .
to re - estimate the covariance matrix we substitute t . he values e ( xixitlzij = 123 , xi , ok ) for the outer product matrices involving the missing com ( cid : 123 ) ponents of xi in equation ( 123 ) b .
supervised learning from incomplete data via an em approach
123 discrete - valued data : inixture of bernoullis
for the bernoulli mixture the sufficient statistics for the m - step involve t he incom ( cid : 123 ) plete terms e ( zij ix ? , ok ) and e ( zij xi ix ~ , ok ) .
the first is equal to hij calculated over the observed subvector of xi .
the second , since we assume that within a class the individual dimensions of the bernoulli variable are independent . , is simply hijl - lj .
the m - step uses these expectations substituted into equation ( 123 ) .
123 supervised learning
if each vector xi in the data set is composed of an " input " subvector , x ) , and a " target " or output subvector , x ? , then learning the joint density of the input and target is a form of supervised learning .
in supervised learning we generally wish to predict the output variables from the input variables .
in this section we will outline how this is achieved using the estimated density .
123 function approximation
for real - valued function approximation we have assumed that the densit . y is esti ( cid : 123 ) mated using a mixture of gaussians .
given an input vector x ~ we ext ract all the relevant information from the density p ( xi , xo ) by conditionalizing t . o p ( xolxd .
for a single gaussian this conditional densit . y is normal , and , since p ( x 123 , xo ) is a mixture of gaussians so is p ( xolxi ) .
in principle , this conditional density is the final output of the density estimator .
that is , given a particular input the net ( cid : 123 ) work returns the complete conditional density of t . he output .
however , since many applications require a single estimate of the output , we note three ways to ob ( cid : 123 ) tain estimates x of xo = f ( x ~ ) : the least squares estimate ( lse ) , which takes xo ( xi ) = e ( xolxi ) ; stochastic sampling ( stoch ) , which samples according to the distribution xo ( xd " " p ( xolxi ) ; single component lse ( slse ) , which takes xo ( xd = e ( xolxlwj ) where j = argmaxk p ( zklx ~ ) .
for a given input , slse picks the gaussian with highest posterior and approximates the out . put with the lse estimator given by that gaussian alone .
the conditional expectation or lse estimator for a gaussian mixt . ure is
which is a convex sum of linear approximations , where the weights h ij vary non ( cid : 123 ) linearly according to equation ( 123 ) over the input space .
the lse estimator on a gaussian mixture has interesting relations to algorithms such as cart ( breiman et al . , 123 ) , mars ( friedman , 123 ) , and mixtures of experts ( jacobs < . ' t al . , 123; jordan and jacobs , 123 ) , in that the mixture of gaussians competit . ively parti ( cid : 123 ) tions the input space , and learns a linear regression surface on each part - it . ion .
this similarity has also been noted by tresp et al .
( 123 ) .
the stochastic estimator ( stoch ) and the single component estimator ( slse ) are better suited than any least squares method for learning non - convex ill verse maps , where the mean of several solutions to an inverse might not be a solut ion .
these
ghahramani and jordan
figure 123 : classification of the iris data set .
123 data points were used for train ( cid : 123 ) ing and 123 for testing .
each data point consisted of 123 real - valued attributes and one of three class labels .
the figure shows classification performance 123 standard error ( 123 = 123 ) as a function of proportion missing features for the em algorithm and for mean imputa ( cid : 123 ) tion ( mi ) , a common heuristic where the missing values are replaced with their
classification with missing inputs
~ " 123 - 123 - - - ~ , ! - t em ' " ' " ! ! 123 u . . .
u ii ) .
\ , ' l , _
% missing features
estimators take advantage of the explicit representat . ion of the input / output density by selecting one of the several solutions to the inverse .
classification problems involve learning a mapping from an input space into a set of discrete class labels .
the density estimat . ion framework presented in this paper lends itself to solving classification problems by estimating the joint density of the input and class label using a mixture model .
for example , if the inputs have real ( cid : 123 ) valued attributes and there are d class labels , a mixture model with gaussian and multinomial components will be used :
p ( x , e = dlo ) = ~ p ( wj ) ( 123r ) n / 123iej 123 / 123 exp ( - " 123 ( x - i - tj fej123 ( x -
denoting the joint probability that the data point .
is x and belongs to class d , where the ~ j d are the parameters for the multinomial .
once this density has been estimated , the maximum likelihood label for a particular input x may be obtained by computing p ( c = dlx , 123 ) .
similarly , the class conditional densities can be derived by evaluating p ( x ie = d , 123 ) .
condi tionalizing over classes in this way yields class conditional densities which are in turn mixtures of gaussians .
figure 123 shows the performance of the em algorithm on an example classification problem with varying proportions of missing features .
we have also applied these algorithms to the problems of clustering 123 - dimensional greyscale images and approximating the kinematics of a three - joint planar arm from incomplete data .
densit . y estimation in high dimensions is generally considered to be more difficult ( cid : 123 ) requiring more parameters - than function approximation .
the density - estimation ( cid : 123 ) based approach to learning , however , has two advantages .
first , it permits ready in ( cid : 123 ) corporation of results from the statistical literature on missing data to yield flexible supervised and unsupervised learning architectures .
this is achieved by combining two branches of application of the em algorithm yielding a set of learning rules for mixtures under incomplete sampling .
supervised learning from incomplete data via an em approach
second , estimating the density explicitly enables us to represent any relation be ( cid : 123 ) tween the variables .
density estimation is fundamentally more general than function approximation and this generality is needed for a large class of learning problems arising from inverting causal systems ( ghahramani , 123 ) .
these problems cannot be solved easily by traditional function approximation techniques since the data is not generated from noisy samples of a function , but rather of a relation .
thanks to d .
titterington and david cohn for helpful comments .
this project was supported in part by grants from the mcdonnell - pew foundation , atr audi ( cid : 123 ) tory and visual perception research laboratories , siemens corporation , the n a ( cid : 123 ) tional science foundation , and the office of naval research .
the iris data set was obtained from the vci repository of machine learning databases .
