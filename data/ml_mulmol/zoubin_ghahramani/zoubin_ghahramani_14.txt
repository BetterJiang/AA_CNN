an introduction to hidden markov models and
gatsby computational neuroscience unit , university college ,
london , london wc123n 123ar , uk
e - mail : zoubin@gatsby . ucl . ac . uk
we provide a tutorial on learning and inference in hidden markov models in the context of the recent literature on bayesian networks .
this perspective makes it possible to con - sider novel generalizations of hidden markov models with multiple hidden state variables , multiscale representations , and mixed discrete and continuous variables .
although exact inference in these generalizations is usually intractable , one can use approximate infer - ence algorithms such as markov chain sampling and variational methods .
we describe how such methods are applied to these generalized hidden markov models .
we conclude this review with a discussion of bayesian methods for model selection in generalized
keywords : dynamic bayesian networks; hidden markov models; state space models; em algorithms; variational methods; gibbs sampling .
hidden markov models ( hmms ) are a ubiquitous tool for modeling time series data .
they are used in almost all current speech recognition systems , in numerous applications in computational molecular biology , in data compression , and in other areas of arti ( cid : 123 ) cial intelligence and pattern recognition .
recently hmms have also been used in computer vision applications | the topic of this special issue | such as image sequence modeling and object tracking .
the goal of this paper is to answer the following questions about hmms , independently of what application the hmm is used for : ( cid : 123 ) what are hidden markov models ? hmms will be de ( cid : 123 ) ned in sec .
( cid : 123 ) how do they relate to other markov models and bayesian networks in general ? simply stated , hidden markov models are a particular kind of bayesian network .
in sec .
123 , we will provide a short tutorial on bayesian networks and describe how hmms and other markov models relate to them .
( cid : 123 ) what are the algorithms for inference and learning in hmms and bayesian net - works ? in order to use an hmm to track an object , segment speech , or group amino - acid sequences into protein families , we need solutions to the inference
february 123 , 123 123 : 123 wspc / 123 - ijprai
ghahramani
and learning problems .
in sec .
123 , we will describe inference and learning algo - rithms for hmms and how they relate to the general belief propagation and em algorithms for bayesian networks .
( cid : 123 ) what are the limitations of hmms and how can these be overcome ? while a lot of mileage has been obtained out of hmms , these models are quite limited .
in sec .
123 , we discuss these limitations , and some generalizations of hmms that overcome these limitations .
unfortunately , more complex models also require more complex ( and sometimes approximate ) algorithms for inference and learning .
these will be described in sec .
( cid : 123 ) how does one avoid over ( cid : 123 ) tting and select model structures in hmms ? one of the last frontiers in the study of hmms is how to select model structures and how to ( cid : 123 ) t a complex model to a small data set without ( cid : 123 ) tting noise in the data .
a bayesian solution to these two ( closely related ) problems is given in sec
while there have been several tutorials and review articles written about hmms ( e . g .
in ref .
123 ) , our understanding of hmms has changed considerably since the realization that they are a kind of bayesian network . 123 namely , we can now relate them to more complex and interesting models , and we can discuss general solutions to the problems of approximate inference , parameter learning , and model selec - tion .
the hope is that this article will introduce the reader to a state - of - the - art understanding of hmms .
a bayesian network specifying conditional independence relations for a hidden markov
what are hidden markov models ?
a hidden markov model is a tool for representing probability distributions over sequences of observations .
let us denote the observation at time t by the variable yt .
this can be symbol from a discrete alphabet , a real - valued variable , an integer , or any other object , as long as we can de ( cid : 123 ) ne a probability distribution over it .
we assume that the observations are sampled at discrete , equally - spaced time intervals , so t can be an integer - valued time index .
the hidden markov model gets its name from two de ( cid : 123 ) ning properties .
first , it assumes that the observation at time t was generated by some process whose state st is hidden from the observer .
second , it assumes that the state of this hidden process satis ( cid : 123 ) es the markov property : that is , given the value of st123 , the current
february 123 , 123 123 : 123 wspc / 123 - ijprai
an introduction to hidden markov models and bayesian networks
state st is independent of all the states prior to t 123a in other words , the state
at some time encapsulates all we need to know about the history of the process in order to predict the future of the process .
the outputs also satisfy a markov property with respect to the states : given st; yt is independent of the states and observations at all other time indices .
taken together , these markov properties mean that the joint distribution of a
sequence of states and observations can be factored in the following way
p ( s123 : t ; y123 : t ) = p ( s123 ) p ( y123js123 )
p ( stjst123 ) p ( ytjst )
where we have used the notation x123 : t to mean x123; : : : ; xt .
this factorization of the joint probability can be drawn graphically in the form shown in fig .
this graph , known as a bayesian network , belief network , probabilistic graphical model , or probabilistic independence network , shows the dependencies between the variables in the model .
we will de ( cid : 123 ) ne bayesian networks more fully in the following section , but for now it is su ( cid : 123 ) cient to note that each variable is represented by a node in the graph , and each node receives directed arcs from nodes on which it is conditionally dependent in the factorization of the joint distribution .
a third assumption of the hidden markov model is that the hidden state variable
is discrete : st can take on k values which we will denote by the integers f123; : : : ; kg .
left to specify is a probability distribution over the initial state p ( s123 ) , the k ( cid : 123 ) k state transition matrix de ( cid : 123 ) ning p ( stjst123 ) and the output model de ( cid : 123 ) ning p ( ytjst ) .
to de ( cid : 123 ) ne a probability distribution over sequences of observations , all that is
hmms usually assume that the state transition matrices and output models are not dependent on t | in other words the model is time invariant ( except for the initial state ) .
if the observables are discrete symbols taking on one of l values , the output model can be fully speci ( cid : 123 ) ed by a k ( cid : 123 ) l observation ( or emission ) matrix .
for real - valued observation vectors , p ( ytjst ) can be modeled in many di ( cid : 123 ) erent
forms , such as a gaussian , mixture of gaussians , or a neural network .
for high - dimensional real - valued observations , a very useful output model is obtained by replacing the gaussian by a factor analyzer . 123 factor analysis ( fa ) is a method for modeling correlations in high - dimensional data , and is closely related to principal components analysis ( pca ) .
the relationships between fa , pca , mixture models , hmms , and other models are reviewed in ref
hmms can be augmented to allow for input variables , ut , in such a way that there is an input dependent state transition probability , p ( stjst123; ut ) . 123;123;123 the
system then models the conditional distribution of a sequence of output observations given a sequence of input observations .
hmms have been applied extensively to problems in speech recognition , 123 computational biology123;123 and fault detection . 123
athis is a ( cid : 123 ) rst - order markov property .
an nth order markov process is one in which st given st123; : : : ; stn is independent of s ( cid : 123 ) for ( cid : 123 ) < t n .
february 123 , 123 123 : 123 wspc / 123 - ijprai
ghahramani
we now turn to bayesian networks , a more general framework than hidden markov models which will allow us both to understand the algorithms for inference and learning in hmms and to formulate natural extensions to the hmm .
a bayesian network tutorial
a bayesian network is a graphical model for representing conditional independencies between a set of random variables .
consider four random variables , w; x; y , and z .
from basic probability theory we know that we can factor the joint probability as a product of conditional probabilities
p ( w; x; y; z ) = p ( w ) p ( xjw ) p ( y jw; x ) p ( zjw; x; y ) :
this factorization does not tell us anything useful about the joint probability distribution : each variable can potentially depend on every other variable .
how - ever , consider the following factorization
p ( w; x; y; z ) = p ( w ) p ( x ) p ( y jw ) p ( zjx; y ) :
the above factorization implies a set of conditional independence relations .
a variable ( or set of variables ) a is conditionally independent from b given c if p ( a; bjc ) = p ( ajc ) p ( bjc ) for all a; b and c such that p ( c ) 123= 123
from the above factorization we can show that given the values of x and y; w and z are
p ( w; x; y; z )
p ( x; y ) p ( w ) p ( x ) p ( y jw ) p ( zjx; y )
p ( w ) p ( x ) p ( y jw ) p ( zjx; y ) dw dz p ( w ) p ( y jw ) p ( zjx; y )
p ( w; zjx; y ) =
= p ( wjy ) p ( zjx; y ) :
p ( y )
a bayesian network is a graphical way to represent a particular factorization of a joint distribution .
each variable is represented by a node in the network .
a directed arc is drawn from node a to node b if b is conditioned on a in the factorization of the joint distribution .
for example , to represent the factorization ( 123 ) we would draw an arc from w to y but not from w to z .
the bayesian network representing the factorization ( 123 ) is shown in fig
some basic de ( cid : 123 ) nitions from graph theory will be necessary at this point .
the node a is a parent of another node b if there is a directed arc from a to b; if so , b is a child of a .
the descendents of a node are its children , childrens children , and so on .
a directed path from a to b is a sequence of nodes starting from a and ending in b such that each node in the sequence is a parent of the following node in the sequence .
an undirected path from a to b is a sequence of nodes starting
february 123 , 123 123 : 123 wspc / 123 - ijprai
an introduction to hidden markov models and bayesian networks
a directed acyclic graph ( dag ) consistent with the conditional independence relations in p ( w; x; y; z ) .
from a and ending in b such that each node in the sequence is a parent or child of the following node .
the semantics of a bayesian network are simple : each node is conditionally independent from its nondescendents given its parents . b more generally , two disjoint sets of nodes a and b are conditionally independent given c , if c d - separates a and b , that is , if along every undirected path between a node in a and a node in b there is a node d such that : ( 123 ) d has converging arrowsc and neither d nor its descendents are in c , or ( 123 ) d does not have converging arrow and d is in c . 123 from visual inspection of the graphical model it is therefore easy to infer many independence relations without explicitly grinding through bayes rule .
for example , w is conditionally independent from x given the set c = fy; zg , since y 123 c is along the only path between w and x , and y does not have converging arrows .
however , we cannot infer from the graph that w is conditionally independent from x given z .
notice that since each factorization implies a strict ordering of the variables , the connections obtained in this manner de ( cid : 123 ) ne a directed acyclic graph . d furthermore , there are many ways to factorize a joint distribution , and consequently there are many bayesian networks consistent with a particular joint .
a bayesian network g is said to be an independency map i - map for a distribution p if every d - separation displayed in g corresponds to a valid conditional independence relation in p .
g is a minimal i - map if no arc can be deleted from g without removing the i - map
the absence of arcs in a bayesian networks implies conditional independence re - lations which can be exploited to obtain e ( cid : 123 ) cient algorithms for computing marginal
bsince there is a one - to - one correspondence between nodes and variables , we will often talk about conditional independence relations between nodes meaning conditional independence relations between the variables associated with the nodes .
cthat is , d is a child of both the previous and following nodes in the path .
dundirected graphical models ( markov networks ) are another important tool for representing probability distributions , and have a di ( cid : 123 ) erent set of semantics . 123;123 we will deal exclusively with directed graphical models in this paper .
february 123 , 123 123 : 123 wspc / 123 - ijprai
ghahramani
and conditional probabilities .
for singly connected networks , in which the under - lying undirected graph has no loops , there exists a general algorithm called belief propagation . 123;123 for multiply connected networks , in which there can be more than one undirected path between any two nodes , there exists a more general algorithm known as the junction tree algorithm . 123;123 i will provide the essence of the belief propagation algorithm ( since the exact inference methods used throughout this paper are based on it ) and refer the reader to relevant texts123;123;123 for details .
assume we observe some evidence : the value of some variables in the network .
the goal of belief propagation is to update the marginal probabilities of all the variables in the network to incorporate this new evidence .
this is achieved by local message passing : each node , n sends a message to its parents and to its children .
since the graph is singly connected , n separates the graph , and therefore the evi - dence , into two mutually exclusive sets : e+ ( n ) , consisting of the parents of n , the nodes connected to n through its parents , e and n itself , and e ( n ) consisting of the children n and the nodes connected to n through its children ( fig .
the message from n to each of its children is the probability of each setting of n given the evidence observed in the set e+ ( n ) .
if n is a k - valued discrete variable , then this message is a k - dimensional vector .
for real - valued variables , the message is a probability density over the domain of values n can take .
the message from n to each of its parents is the probability , given every setting of the parent , of the ( n ) ( fng .
the marginal probability of a node is evidence observed in the set e proportional to the product of the messages obtained from its parents , weighted by the conditional probability of the node given its parents , and the message obtained
from its children .
if the parents of n are fp123; : : : ; pkg and the children of n are fc123; : : : ; cg , then
p ( nje ) /
p ( njp123; : : : ; pk )
p ( cj ; e
where the summation ( or more generally the integral ) extends over all settings of fp123; : : : ; pkg .
for example , for the bayesian network in fig .
123 , given the evidence e = fx = x; z = zg ,
p ( y jx = x; z = z ) /
p ( y jw ) p ( w ) dw
p ( z = z; x = xjy )
/ p ( y ) p ( z = zjx = x; y ) p ( x = x )
where p ( w ) is the message passed from w to y since e+ ( w ) = ; , and p ( z = z; x = xjy ) is the message passed from z to y .
variables in the evi - dence set are referred to as observable variables , while those that are not in the evidence set are referred to as hidden variables .
ethat is , the nodes for which the undirected path to n goes through a parent of n .
february 123 , 123 123 : 123 wspc / 123 - ijprai
an introduction to hidden markov models and bayesian networks
separation of evidence in singly connected graphs .
a bayesian network representing a ( cid : 123 ) rst - order markov process .
dynamic bayesian networks
hidden markov models fall in a subclass of bayesian networks known as dynamic bayesian networks , which are simply bayesian networks for modeling time series data .
in time series modeling , the assumption that an event can cause another event in the future , but not vice - versa , simpli ( cid : 123 ) es the design of the bayesian network : directed arcs should ow forward in time .
assigning a time index t to each variable ,
one of the simplest causal models for a sequence of data fy123; : : : ; ytg is the ( cid : 123 ) rst -
order markov model we have already mentioned , in which each variable is directly inuenced only by the previous variable ( fig
p ( y123 : t ) = p ( y123 ) p ( y123jy123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) p ( ytjyt123 ) :
having observed fy123; : : : ; ytg , the model will only make use of yt to predict yt+123
one simple way of extending markov models is to allow higher order interac - tions between variables , for example , an nth - order markov model allows arcs from fytn; : : : ; yt123g to yt .
another way to extend markov models is to posit that the
observations are dependent on a hidden variable , which we will call the state , and that the sequence of states is a markov process ( fig .
hidden markov models fall into this class of dynamic bayesian network .
another very well - known model in this class is the linear - gaussian state - space model , also known as the kalman ( cid : 123 ) lter , which can be thought of as the continuous - state version of hmms .
february 123 , 123 123 : 123 wspc / 123 - ijprai
ghahramani
state - space models
in state - space models , a sequence of d - dimensional real - valued observation vectors
fy123; : : : ; ytg , is modeled by assuming that at each time step yt was generated from a k - dimensional real - valued hidden state variable xt , and that the sequence of xs de ( cid : 123 ) ne a ( cid : 123 ) rst - order markov process :
p ( x123 : t ; y123 : t ) = p ( x123 ) p ( y123jx123 )
p ( xtjxt123 ) p ( ytjxt ) :
this factorization of the joint probability means that the bayesian network for state - space models is identical to that of hmms ( fig .
123 ) except that the hidden s variables are replaced by hidden x variables .
the state transition probability p ( xtjxt123 ) can be decomposed into determin -
istic and stochastic components :
xt = ft ( xt123 ) + wt
where ft is the deterministic transition function determining the mean of xt given xt123 , and wt is a zero - mean random noise vector .
similarly , the observation prob - ability p ( ytjxt ) can be decomposed as
yt = gt ( xt ) + vt :
if both the transition and output functions are linear and time - invariant and the distribution of the state and observation noise variables is gaussian , the model becomes a linear - gaussian state - space model :
xt = axt123 + wt
yt = cxt + vt
where a is the state transition matrix and c is the observation matrix .
often , the observations can be divided into a set of input ( or predictor ) variables and output ( or response ) variables .
again , assuming linearity and gaussian noise we can write the state transition function as
xt = axt123 + but + wt
where ut is the input observation vector and b is the input matrix .
the bayesian network corresponding to this model would include a sequence of nodes u123 : t each of which is a parent of the corresponding xt .
linear - gaussian state - space models are used extensively in all areas of control and signal processing .
what makes hidden markov models and state - space models special is that their hidden state spaces are closed under their respective state transition probabilities and output models .
in hmms , the hidden state is assumed to have a k - valued discrete ( a . k . a .
multinomial ) distribution .
after multiplying by a k ( cid : 123 ) k transition matrix one obtains another k - valued multinomial distribution .
in ssms the hidden state is assumed to be gaussian distributed .
after the dynamics consisting of a
february 123 , 123 123 : 123 wspc / 123 - ijprai
an introduction to hidden markov models and bayesian networks
linear transformation with gaussian noise added , one again obtains a gaussian distributed hidden state .
this closed property of hmms and ssms makes inference and learning particularly simple and appealing in these models .
learning and inference
a bayesian approach to learning starts with some a priori knowledge about the model structure | the set of arcs in the bayesian network | and model parameters .
this initial knowledge is represented in the form of a prior probability distribution over model structures and parameters , and is updated using the data to obtain a posterior probability distribution over models and parameters .
more formally , assuming a prior distribution over model structures p ( m ) and a prior distribution over parameters for each model structure p ( ( cid : 123 ) jm ) , a data set d is used to form a posterior distribution over models using bayes rule
p ( mjd ) =
p ( dj ( cid : 123 ) ;m ) p ( ( cid : 123 ) jm ) d ( cid : 123 ) p ( m )
which averages over the uncertainty in the parameters .
for a given model structure , we can compute the posterior distribution over the parameters
p ( dj ( cid : 123 ) ;m ) p ( ( cid : 123 ) jm )
p ( ( cid : 123 ) jm;d ) =
if the data set is some sequence of observations d = fy123; : : : ; ytg and we wish to predict the next observation , yt +123 based on our data and models , then the bayesian
p ( yt +123jd ) =
p ( yt +123j ( cid : 123 ) ;m;d ) p ( ( cid : 123 ) jm;d ) p ( mjd ) d ( cid : 123 ) dm
averages over both the uncertainty in the model structure and in the parameters .
this is known as the predictive distribution for the model .
we obtain a somewhat impoverished but nonetheless useful limiting case of the bayesian approach to learning if we assume a single model structure m and we estimate the parameter vector ^ ( cid : 123 ) that maximizes the likelihood p ( dj ( cid : 123 ) ;m ) under that model .
in the limit of a large data set and an uninformative ( e . g .
uniform ) prior over the parameters , the posterior p ( ( cid : 123 ) jm;d ) will be sharply peaked around the maxima of the likelihood , and therefore the predictions of a single maximum likelihood ( ml ) model will be similar to those obtained by bayesian integration over the parameters .
for now we focus on the problem of estimating ml parameters for a given model structure .
although this is the most widely used approach to learning hidden markov models , it only crudely approximates bayesian learning , and can perform catastrophically when data is scarce and / or the model is complex .
in practice an exact bayesian analysis of hmms is impractical , which is why most research has focused on ml approaches or regularized ml approaches .
in sec .
123 , we will tackle practical approaches to full - edged bayesian learning .
february 123 , 123 123 : 123 wspc / 123 - ijprai
ghahramani
ml estimation with complete data
assume we are given a data set of independent and identically distributed observa - tions d = fy ( 123 ) ; : : : ; y ( n ) g , each of which can be a vector or time series of vectors .
the likelihood of the data set is a function of the parameters of the model
p ( dj ( cid : 123 ) ;m ) =
p ( y ( i ) j ( cid : 123 ) ;m ) :
for notational convenience we henceforth drop the implicit conditioning on the model structure , m .
the ml parameters are obtained by maximizing the likelihood , or equivalently the log likelihood
log p ( y ( i ) j ( cid : 123 ) ) :
if the observation vector includes all the variables in the bayesian network , then each term in the log likelihood further factors as
log p ( y ( i ) j ( cid : 123 ) ) = log
p ( y ( i ) jy ( i )
log p ( y ( i ) jy ( i )
where j indexes the nodes in the bayesian network , pa ( j ) is the set of parents of j , and ( cid : 123 ) j are the parameters that de ( cid : 123 ) ne the conditional probability of yj given its parents .
the likelihood therefore decouples into local terms involving each node and its parents , simplifying the ml estimation problem .
for example , if the y variables are discrete and ( cid : 123 ) j is the conditional probability table for yj given its parents , then the ml estimate of ( cid : 123 ) j is simply a normalized table containing counts of each setting of yj given each setting of its parents in the data set .
ml estimation with hidden variables : the em algorithm
with hidden variables the log likelihood cannot be decomposed as in ( 123 ) .
rather ,
l ( ( cid : 123 ) ) = log p ( y j ( cid : 123 ) ) = log
p ( y; xj ( cid : 123 ) )
where x is the set of hidden variables , and x is the sum ( or integral ) over x required to obtain the marginal probability of the data .
( we have dropped the superscript ( i ) in ( 123 ) by evaluating the log likelihood for a single observation , again for notational convenience . ) maximizing ( 123 ) directly is often di ( cid : 123 ) cult because the log of the sum can potentially couple all of the parameters of the model .
we can simplify the problem of maximizing l with respect to ( cid : 123 ) by making use of the
february 123 , 123 123 : 123 wspc / 123 - ijprai
an introduction to hidden markov models and bayesian networks
following insight .
any distribution q ( x ) over the hidden variables de ( cid : 123 ) nes a lower bound on l :
p ( y; xj ( cid : 123 ) ) = log
p ( y; xj ( cid : 123 ) ) p ( x; y j ( cid : 123 ) )
q ( x ) log p ( x; y j ( cid : 123 ) )
= f ( q; ( cid : 123 ) )
q ( x ) log q ( x )
where the inequality is known as jensens inequality and follows from the fact that the log function is concave .
if we de ( cid : 123 ) ne the energy of a global con ( cid : 123 ) guration ( x; y ) to be log p ( x; y j ( cid : 123 ) ) , then some readers may notice that the lower bound f ( q; ( cid : 123 ) ) ( cid : 123 ) l ( ( cid : 123 ) ) is the negative of a quantity known in statistical physics as the free energy : the expected energy under q minus the entropy of q . 123 the expectation ( maximization ( em ) algorithm123;123 alternates between maximizing f with respect to q and ( cid : 123 ) , respectively , holding the other ( cid : 123 ) xed .
starting from some initial
qk+123 arg max ( cid : 123 ) k+123 arg max
f ( qk+123; ( cid : 123 ) ) :
it is easy to show that the maximum in the e step is obtained by set -
ting qk+123 ( x ) = p ( xjy; ( cid : 123 ) k ) , at which point the bound becomes an equality : f ( qk+123; ( cid : 123 ) k ) = l ( ( cid : 123 ) k ) .
the maximum in the m step is obtained by maximizing
the ( cid : 123 ) rst term in ( 123 ) , since the entropy of q does not depend on ( cid : 123 ) :
( cid : 123 ) k+123 arg max
p ( xjy; ( cid : 123 ) k ) log p ( x; y j ( cid : 123 ) ) :
this is the expression most often associated with the em algorithm , 123 but it ob - scures the elegant interpretation of em as coordinate ascent in f .
since f = l at the beginning of each m step , and since the e step does not change ( cid : 123 ) , we are guaranteed not to decrease the likelihood after each combined em step .
it is worthwhile to point out that it is usually not necessary to explicitly evaluate
the posterior distribution p ( xjy; ( cid : 123 ) k ) .
since log p ( x; y j ( cid : 123 ) ) contains both hidden and
observed variables in the network , it can be factored as before as the sum of log probabilities of each node given its parents .
consequently , the quantities required
for the m step are the expected values , under the posterior distribution p ( xjy; ( cid : 123 ) k ) ,
of the analogous su ( cid : 123 ) cient statistics required for ml estimation in the complete data
february 123 , 123 123 : 123 wspc / 123 - ijprai
ghahramani
example 123 : learning hidden markov models using em
to derive the em algorithm for learning the parameters of an hmm we ( cid : 123 ) rst need to write out the log probability of the hidden variables and observations :
log p ( s123 : t ; y123 : t ) = log p ( s123 ) +
log p ( ytjst ) +
log p ( stjst123 ) :
let us represent the k - valued discrete state st using k - dimensional unit column vectors , e . g .
the state at time t taking on the value \123 " is represented as st = ( 123 : : : ) .
each of the terms in ( 123 ) can be decomposed into summations over s .
for example , the transition probability is
p ( stjst123 ) =
where ( cid : 123 ) ij is the probability of transitioning from state j to state i , arranged in a k ( cid : 123 ) k matrix ( cid : 123 )
log p ( stjst123 ) =
st;ist123;j log ( cid : 123 ) ij
t ( log ( cid : 123 ) ) st123
using matrix notation , where is the matrix transpose ( not to be confused with the sequence length t ) , and logarithms of vectors and matrices are taken elementwise .
similarly , if the initial state probabilities are arranged in a vector ( cid : 123 ) , then
log p ( s123 ) = s
123 log ( cid : 123 ) :
finally , the emission probabilities depend on the form of the observations .
if yt is a discrete variable which can take on d values , then we again represent it using d - dimensional unit vectors and obtain
log p ( ytjst ) = y
t ( log e ) st
where e is a d ( cid : 123 ) k emission probability matrix .
the parameter set for the hmm is ( cid : 123 ) = f ( cid : 123 ) ; ( cid : 123 ) ; eg .
since the state variables are hidden we cannot compute ( 123 ) directly .
the em algorithm , which in the case of hmms is known as the baum ( welch algorithm , 123 allows us to circumvent this problem by computing the expectation of ( 123 ) under the posterior distribution of the hidden states given the observations .
we denote the expected value of some quantity f ( x ) with respect to the posterior distribution of x by hf ( x ) i ,
hf ( x ) i =
f ( x ) p ( xjy; ( cid : 123 ) ) dx :
the expected value of ( 123 ) can be expressed as a function of hsti and hsts ( cid : 123 ) rst term , hsti , is a vector containing the probability that the hmm was in each of
february 123 , 123 123 : 123 wspc / 123 - ijprai
an introduction to hidden markov models and bayesian networks
the k states at time t given its current parameters and the entire sequence of ob - i , is a matrix containing the joint probability servations . f the second term , hsts that the hmm was in each of the k 123 pairs of states at times t 123 and t .
in the hmm notation of ref .
123 , hsti corresponds to t and hsts i corresponds to ( cid : 123 ) t .
given these expectations , the m step is straightforward : we take derivatives of ( 123 ) with respect to the parameters , set to zero , and solve subject to the sum - to - one constraints that ensure valid transition , emission and initial state probabilities .
for discrete yt coded in the same way as st ( i . e .
yt is coded as a d - dimensional binary unit vector ) , the m step is :
( cid : 123 ) ij / tx
the necessary expectations are computed using the forward ( backward algorithm .
the forward ( backward algorithm
the forward ( backward algorithm is an instance of belief propagation applied to the bayesian network corresponding to a hidden markov model ( see ref .
123 for a recent treatment ) .
the forward pass recursively computes ( cid : 123 ) t , de ( cid : 123 ) ned as the joint probability of st and the sequence of observations y123 to yt
123 p ( ytjst )
p ( st123; y123 : t123 ) p ( stjst123 )
123 p ( ytjst ) :
( cid : 123 ) t = p ( st; y123 : t )
( cid : 123 ) t = p ( yt+123 : tjst )
the backward pass computes the conditional probability of the observations yt+123 to yt given st
p ( yt+123 : tjst+123 ) p ( st+123jst ) p ( yt+123jst+123 )
f when learning from a data set containing multiple sequences , this quantity has to be computed separately for each sequence .
for clarity , we will describe the single sequence case only .
february 123 , 123 123 : 123 wspc / 123 - ijprai
ghahramani
( cid : 123 ) t+123p ( st+123jst ) p ( yt+123jst+123 ) :
from these it is easy to compute the expectations needed for em
hst;ii = ti =
hst;ist123;ji = ( cid : 123 ) tij =
k; ( cid : 123 ) t123;k ( cid : 123 ) kp ( ytjst; ) ( cid : 123 ) t;
in practice , for long sequences both ( cid : 123 ) t and ( cid : 123 ) t become vanishingly small as the recursions progress .
they are therefore usually renormalized to sum to one at each step of the recursions .
this makes the computation of the relevant expectations much more numerically well - behaved , and has the nice side - e ( cid : 123 ) ect that the sum of the log normalizations in the forward pass is the log likelihood of the observation
occasionally , it is also useful to compute the single most probable state sequence .
the solution to this problem is given by the viterbi algorithm , 123 which is very similar to the forward ( backward algorithm except that some of the summations are replaced by maximizations ( see ref .
123 for a tutorial on hmms , especially as applied to speech recognition ) .
example 123 : learning state - space models using em
using eq .
( 123 ) , the log probability of the hidden states and observations for linear - gaussian state - space models can be written as
log p ( x123 : t ; y123 : t ) = log p ( x123 ) +
log p ( ytjxt ) +
log p ( xtjxt123 ) :
each of the above probability densities is gaussian , and therefore the overall ex - pression is a sum of quadratics .
for example , using eq
log p ( ytjxt ) = 123
123 ( yt cxt ) 123
jrj + const
where r is the covariance of the observation noise vt and j ( cid : 123 ) j is the matrix
if all the random variables were observed , then the ml parameters could be solved for by maximizing ( 123 ) .
taking derivatives of ( 123 ) , we obtain a linear system of equations .
for example , the ml estimate of the matrix c is
february 123 , 123 123 : 123 wspc / 123 - ijprai
an introduction to hidden markov models and bayesian networks
since the states are in fact hidden , in the m step we use expected values wherever we do not have access to the actual observed values .
then , the m step for c is
similar m steps can be derived for all the other parameters by taking derivatives of the expected log probability . 123;123;123;g in general we require all terms of the kind i .
these terms can be computed using the kalman
i and hxtx
kalman smoothing
the kalman smoother solves the problem of estimating the state at time t of a linear - gaussian state - space model given the model parameters and a sequence
of observations fy123; : : : ; yt; : : : ; ytg .
it consists of two parts : a forward recursion which uses the observations from y123 to yt , known as the kalman ( cid : 123 ) lter , 123 and a backward recursion which uses the observations from yt to yt+123;h we have already seen that in order to compute the marginal probability of a variable in a bayesian network one must take into account both the evidence above and below the variable .
in fact , the kalman smoother is simply a special case of the belief propagation algorithm we have already encountered for bayesian networks .
the kalman smoothing algorithm and the forward ( backward algorithm are conceptually identical , although of course the details di ( cid : 123 ) er since in one gaussian densities are propagated and in the other discrete distributions are propagated .
limitations of hmms and generalizations
linear - gaussian state - space models and hidden markov models provide an interest - ing starting point for designing dynamic bayesian networks .
however , they su ( cid : 123 ) er from important limitations when it comes to modeling real - world time series .
in the case of linear - gaussian state - space models the limitations are advertized in the name : in many realistic applications , both the state dynamics and the relation be - tween states and observations can be nonlinear , and the noise can be non - gaussian .
for hidden markov models , the situation is more subtle .
hmms are a dynamical extension of mixture models , and unconstrained mixture models can be used to model any distribution in the limit of an in ( cid : 123 ) nite number of mixture components .
furthermore , if the state transition matrix is unconstrained , any arbitrary nonlinear dynamics can also be modeled .
so where does the limitation lie ?
gthe parameters of a linear - gaussian state - space model can also be estimated using methods from on - line recursive identi ( cid : 123 ) cation . 123 hthe forward and backward recursions together are also known as the rauch - tung - streibel ( rts ) smoother .
thorough treatments of kalman ( cid : 123 ) ltering and smoothing can be found in refs
february 123 , 123 123 : 123 wspc / 123 - ijprai
ghahramani
consider the problem of modeling the movement of several objects in a sequence of images .
if there are m objects , each of which can occupy k positions and ori - entations in the image , there are k m possible states of the system underlying an image .
a hidden markov model would require km distinct states to model this sys - tem .
this representation is not only ine ( cid : 123 ) cient but di ( cid : 123 ) cult to interpret .
we would much prefer that our \hmm " could capture the underlying state space by using m di ( cid : 123 ) erent k - dimensional variables .
more seriously , an unconstrained hmm with km states has of order k 123m parameters in the transition matrix .
unless the data set captures all these possible transitions or a priori knowledge is used to constrain the parameters , severe over - ( cid : 123 ) tting may result .
in this section , we describe three ways in which hmms and state - space models can be extended to overcome some of these limitations .
the ( cid : 123 ) rst of these represents the hidden state of an hmm using a set of distinct state variables .
we call this hmm with a distributed state representation , a factorial hidden markov model . 123
extension 123 : factorial hmms
we generalize the hmm by representing the state using a collection of discrete state
st = s ( 123 )
; : : : s ( m )
; : : : ; s ( m )
each of which can take on k ( m ) values .
the state space of this model consists of the cross product of these state variables .
for simplicity , we will assume that k ( m ) = k , for all m , although the algorithms we present can be trivially generalized to the case of di ( cid : 123 ) ering k ( m ) .
given that the state space of this factorial hmm consists of all km combinations of the s ( m ) variables , placing no constraints on the state transition structure would result in a km ( cid : 123 ) km transition matrix .
such an unconstrained system is uninteresting for several reasons : it is equivalent to an hmm with km states , it is unlikely to discover any interesting structure in the k state variables , as all variables are allowed to interact arbitrarily , and both the time complexity and sample complexity of the estimation algorithm are exponential in m .
we therefore focus on factorial hmms in which the underlying state transitions are constrained .
a natural structure to consider is one in which each state variable evolves according to its own dynamics , and is a priori uncoupled from the other
p ( stjst123 ) =
t123 ) :
a bayesian network representing this model is shown in fig .
the transition structure for this model can be parameterized using m distinct k ( cid : 123 ) k matrices .
as shown in fig .
123 , the observation at time step t can depend on all the state variables at that time step in a factorial hmm .
for real - valued observations , one
february 123 , 123 123 : 123 wspc / 123 - ijprai
an introduction to hidden markov models and bayesian networks
simple form for this dependence is linear - gaussian , that is , the observation yt is a gaussian random vector whose mean is a linear function of the state variables .
we represent the state variables as k ( cid : 123 ) 123 vectors , where each of the k discrete values corresponds to a 123 in one position and 123 elsewhere .
the resulting probability density for a d ( cid : 123 ) 123 observation vector yt is
p ( ytjst ) = jrj123=123 ( 123 ( cid : 123 ) )
each w ( m ) matrix is a d ( cid : 123 ) k matrix whose columns are the contributions to the means for each of the settings of s ( m )
, and r is a d ( cid : 123 ) d covariance matrix .
one way to understand the observation model in eqs .
( 123 ) and ( 123 ) is to consider the marginal distribution for yt , obtained by summing over the possible states .
there are k settings for each of the m state variables , and thus there are k m possible mean vectors obtained by forming sums of m columns where one column is chosen from each of the w ( m ) matrices .
the resulting marginal density of yt is thus a gaussian mixture model with km mixture components each having a constant covariance matrix r .
this static mixture model , without inclusion of the time index and the markov dynamics , is a factorial parameterization of the standard mixture of gaussians model that has interest in its own right . 123;123;123 the model we have just presented extends the static model by allowing markov dynamics in the discrete state variables underlying the mixture .
a model of that combines features of the factorial hmm and factor analysis has been recently applied to an image tracking problem with impressive results . 123
a bayesian network representing the conditional independence relations in a factorial hmm with m = 123 underlying markov chains .
( we only show here a portion of the bayesian network around time slice t . )
february 123 , 123 123 : 123 wspc / 123 - ijprai
ghahramani
tree structured hidden markov models .
extension 123 : tree structured hmms
in factorial hmms , the state variables at one time step are assumed to be a priori independent given the state variables at the previous time step .
this assumption can be relaxed in many ways by introducing coupling between the state variables in a single time step . 123 one interesting way to couple the variables is to order for 123 ( cid : 123 ) n < m .
furthermore , if all the state them , such that s ( m ) variables and the output also depend on an observable input variable , xt , we obtain the bayesian network shown in fig
depends on s ( n )
this architecture can be interpreted as a probabilistic decision tree with marko - vian dynamics linking the decision variables .
consider how this model would gener - ate data at the ( cid : 123 ) rst time step , t = 123
given input x123 , the top node s ( 123 ) can take on k values .
this stochastically partitions x - space into k decision regions .
the next node down the hierarchy , s ( 123 ) 123 , subdivides each of these regions into k subregions , and so on .
the output y123 is generated from the input x123 and the k - way decisions at each of the m hidden nodes .
at the next time step , a similar procedure is used to generate data from the model , except that now each decision in the tree is de - pendent on the decision taken at that node in the previous time step .
this model generalizes the \hierarchical mixture of experts " 123 and other related decision tree models such as cart123 and mars123 by giving the decision markovian dynamics .
tree structured hmms provide a useful starting point for modeling time series with both temporal and spatial structure at multiple resolutions .
we have explored this generalization of factorial hmms in ref
extension 123 : switching state - space models
both factorial hmms and tree structured hmms use discrete hidden state represen - tations .
to model time series with continuous but nonlinear dynamics , it is possible to combine the real - valued hidden state of linear - gaussian state - space models and
february 123 , 123 123 : 123 wspc / 123 - ijprai
an introduction to hidden markov models and bayesian networks
the discrete state of hmms .
one natural way to do this is the switching state - space
in switching state - space models , the sequence of observations y123 : t is modeled using a hidden state space comprising m real - valued state vectors , x ( m ) , and one discrete state vector st .
the discrete state , st , is a multinomial variable that can take on m values : st 123 f123; : : : ; mg , for reasons that will become obvious we refer
to it as the switch variable .
the joint probability of observations and hidden states can be factored as
p ( s123 : t ; x ( 123 )
123 : t ; : : : ; x ( m )
123 : t ; y123 : t )
p ( x ( m )
p ( x ( m )
p ( ytjx ( 123 )
; : : : ; x ( m )
= p ( s123 )
which corresponds graphically to the conditional independencies represented in fig .
conditioned on a setting of the switch state , st = m , the observable is multivariate gaussian with output equation given by state - space model m .
in other words , the probability density of the observation vector yt is
p ( ytjx ( 123 )
; : : : ; x ( m ) 123 jrj 123
; st = m )
( yt c ( m ) x ( m )
123 ( y123 c ( m ) x ( m )
where d is the dimension of the observation vector , r is the observation noise co - variance matrix , and c ( m ) is the output matrix for state - space model m ( cf .
( 123 ) for a single linear - gaussian state - space model ) .
each real - valued state vector evolves according to the linear - gaussian dynamics of a state - space model with di ( cid : 123 ) ering ini - tial state , transition matrix , and state noise ( eq .
the switch state itself evolves according to the discrete markov dynamics speci ( cid : 123 ) ed by initial state probabilities
p ( s123 ) and an m ( cid : 123 ) m state transition matrix p ( stjst123 ) .
this model can be seen as an extension of the \mixture of experts " architecture for modular learning in neural networks . 123;123;123 each state - space model is a linear expert with gaussian output noise and linear - gaussian dynamics .
the switch state \gates " the outputs of the m state - space models , and therefore plays the role of a gating network with markovian dynamics . 123;123
approximate inference and intractability
the problem with all the extensions of hidden markov models and state - space models presented in the previous section is that , given a sequence of observations , most probabilities of interest are intractable to compute .
consider , for example , computing the likelihood of a factorial hmm | the
marginal probability of a sequence of observations given the parameters , p ( y123 : tj ( cid : 123 ) ) .
february 123 , 123 123 : 123 wspc / 123 - ijprai
ghahramani
bayesian network representation for switching state - space models .
st is the discrete switch variable and x ( m )
are the real - valued state vectors .
this is the sum over all possible hidden state sequences of the joint probability of the sequence and the observations :
p ( y123 : tj ( cid : 123 ) ) =
p ( s123 : t ; y123 : tj ( cid : 123 ) ) :
there are km possible states at each time step , and therefore kmt hidden state sequences of length t , assuming none of the transition probabilities is exactly 123
the brute - force approach of evaluating all such sequences can be avoided by making use of the conditional independencies represented in the bayesian network .
for example , directly applying the forward pass of the forward ( backward algorithm outlined in sec .
123 , we can compute the likelihood by summing the ( cid : 123 ) s at the last time step
p ( y123 : tj ( cid : 123 ) ) =
p ( st ; y123; : : : ; ytj ( cid : 123 ) )
for the factorial hmm , ( cid : 123 ) t is a vector of size equal to the full state space at time t , i . e .
it has km elements .
this results in a recursive algorithm that computes the like - lihood using o ( t k 123m ) operations .
this can be further improved upon by using the fact that the state transitions are de ( cid : 123 ) ned via m matrices of size k ( cid : 123 ) k rather than a single km ( cid : 123 ) km matrix , resulting in a recursive algorithm using o ( t m km+123 ) operations ( see ref .
123 , appendix b ) .
unfortunately , this time complexity cannot be improved upon .
given the observation at time t , the k - valued state variables become coupled in an m th - order interaction .
it is not possible to sum over each variable independently .
like the likelihood , computing the posterior probability of a single state variable given the observation sequence , p ( s ( m ) ponential in m .
similar exponential time complexity results hold for the likelihoods and posterior probabilities of tree structured hmms and switching state - space
jy123; : : : ; yt ) , is also ex -
february 123 , 123 123 : 123 wspc / 123 - ijprai
an introduction to hidden markov models and bayesian networks
approximation 123 : gibbs sampling
one approach to computing approximate marginal probabilities is to make use of monte carlo integration .
since the log likelihood can be expressed as
log p ( y123 : tj ( cid : 123 ) ) =
p ( s123 : tjy123 : t ; ( cid : 123 ) ) ( log p ( s123 : t ; y123 : tj ( cid : 123 ) ) log p ( s123 : tjy123 : t ; ( cid : 123 ) ) )
by sampling from the posterior distribution , p ( s123 : tjy123 : t ; ( cid : 123 ) ) , the log likelihood can
be approximated using the above expression , which is just the negative of the free energy ( 123 ) .
to learn the parameters of the model , samples from the posterior are used to evaluate the expectations required for em .
of course , for intractable models sampling directly from the posterior distributions is computationally prohibitive .
however , it is often easy to set up a markov chain that will converge to samples from the posterior .
one of the simplest methods to achieve this is gibbs sampling ( for a review of gibbs sampling and other markov chain monte carlo methods , see
for a given observation sequence y123 : t , gibbs sampling starts with a random setting of the hidden states s123 : t .
at each step of the sampling process , each state variable is updated stochastically according to its probability distribution condi - tioned on the setting of all the other state variables .
the graphical model is again useful here , as each node is conditionally independent of all other nodes given its markov blanket , de ( cid : 123 ) ned as the set of children , parents , and parents of the children of a node .
for example , to sample from a typical state variable s ( m ) in a factorial hmm we only need to examine the states of a few neighboring nodes :
( cid : 123 ) p ( s ( m ) jfs ( n ) = p ( s ( m ) / p ( s ( m ) t123 ) p ( s ( m )
: ( cid : 123 ) 123= t _ n 123= mg; y123 : t ) : n 123= mg; s ( m ) t123 ; s ( m ) t+123 ; yt )
; : : : ; s ( m )
; : : : ; s ( m )
where ( cid : 123 ) denotes \sampled from " .
sampling once from each of the t m hidden variables in the model results in a new sample of the hidden state of the model and requires o ( t m k ) operations .
the sequence of states resulting from each pass of gibbs sampling de ( cid : 123 ) nes a markov chain over the state space of the model .
this markov chain is guaranteed to converge to the posterior probabilities of the states given the observations123 as long as none of the probabilities in the model is exactly zero . i thus , after some suitable time , samples from the markov chain can be taken as approximate samples from the posterior probabilities .
the ( cid : 123 ) rst - and second - order statistics needed to estimate hs ( m ) i are collected using the states visited and the probabilities estimated during this sampling process and are used in the approximate e step of em . j monte carlo methods for learning in dynamic bayesian networks have been explored in refs .
123 , 123 , 123 and 123
i and hs ( m )
iactually , the weaker assumption of ergodicity is su ( cid : 123 ) cient to ensure convergence .
ja more bayesian treatment of the learning problem , in which the parameters are also considered hidden random variables , can be handled by gibbs sampling by replacing the \m step " with
february 123 , 123 123 : 123 wspc / 123 - ijprai
ghahramani
approximation 123 : variational methods
another approach to approximating a probability distribution p is to de ( cid : 123 ) ne a parameterized distribution q and vary its parameters so as to minimize the distance between q and p .
in the context of the em algorithm , we have already seen that the log likelihood l ( ( cid : 123 ) ) is lower bounded by the negative free energy , f ( q; ( cid : 123 ) ) .
the di ( cid : 123 ) erence between l and f is given by the kullback ( leibler divergence between q and the posterior distribution of the hidden variables
l ( ( cid : 123 ) ) f ( q; ( cid : 123 ) ) = kl ( q ( s123 : tj ( cid : 123 ) ) kp ( s123 : tjy123 : t ; ( cid : 123 ) ) )
p ( s123 : tjy123 : t ; ( cid : 123 ) )
where ( cid : 123 ) are the parameters of the distribution q .
the variational approach uses a tractable q to approximate the intractable posterior .
the tractability of computing expectations with respect to q depends both on its parametric form and on its conditional independence relations . k the art is to choose a family of qs that have an analytic form and a tractable structure | a bayesian network that eliminates some of the dependencies in p | but that can approximate p adequately .
given this structure , the parameters of q are varied so as to obtain the tightest possible bound , which minimizes ( 123 ) .
we will refer to the general strategy of using a parameterized approximating distribution as a variational approximation and refer to the free parameters of the q distribution as
example 123 : mean ( cid : 123 ) eld for factorial hmms
we illustrate this approach using the simplest variational approximation to the posterior distribution in factorial hmms : all state variables in q are independent
the variational parameters , ( cid : 123 ) = f ( cid : 123 ) ( m )
g , are the means of the state variables , where , as before , a state variable s ( m ) is represented as a k - dimensional vector with a 123 in the kth position and 123 elsewhere , if the mth markov chain is in state k at time t .
the elements of the vector ( cid : 123 ) ( m ) therefore de ( cid : 123 ) ne the state occupation
sampling from the conditional distribution of the parameters given the other hidden variables ( for example , see ref .
kwe will see later how choosing the conditional independence relations of q sometimes deter - mines the optimal parametric form of q .
this is \true " variational optimization , since calculus of variations is used to optimize over all distributions q .
february 123 , 123 123 : 123 wspc / 123 - ijprai
an introduction to hidden markov models and bayesian networks
( a ) the completely factorized variational approximation assuming that all the state vari - ables are independent ( conditional on the observation sequence ) .
( b ) a structured variational approximation assuming that the state variables retain their markov structure within each chain , but are independent across chains .
probabilities of the multinomial variable s ( m )
under the q distribution
t;k where s ( m )
123 f123; 123g;
t;k = 123 :
a completely factorized approximation of this kind is often used in statistical physics , where it provides the basis for simple yet powerful mean ( cid : 123 ) eld approxi - mations to statistical mechanical systems . 123
to make the bound as tight as possible we vary ( cid : 123 ) separately for each observation sequence so as to minimize the kl divergence .
taking the derivatives of ( 123 ) with respect to ( cid : 123 ) ( m ) and setting them to zero , we obtain the set of ( cid : 123 ) xed point equations
123 ~ y ( m )
( cid : 123 ) ( m ) + ( log ( cid : 123 ) ( m ) ) ( cid : 123 ) ( m )
t123 + ( log ( cid : 123 ) ( m ) )
where ~ y ( m ) variables not including m
is the reconstruction error in yt given the predictions from all the state
( cid : 123 ) ( m ) is the vector of diagonal elements of w ( m ) operator , which maps a vector a into a vector b of the same size , with elements
123w ( m ) , and f ( cid : 123 ) g is the softmax
and log ( cid : 123 ) ( m ) denotes the element - wise logarithm of the transition matrix ( cid : 123 ) ( m ) ( see appendix c in ref .
123 for details of the derivation ) .
the ( cid : 123 ) rst term of ( 123 ) is the projection of the reconstruction error onto the weights of state vector m | the more a particular setting of a state vector can reduce this error , the larger its associated variational mean .
the second term arises
( cid : 123 ) yt mx
february 123 , 123 123 : 123 wspc / 123 - ijprai
ghahramani
i evaluated under the from the fact that the second - order correlation hs ( m ) variational distribution is a diagonal matrix composed of the elements of ( cid : 123 ) ( m ) last two terms introduce dependencies forward and backward in time . l therefore , although the posterior distribution over the hidden variables is approximated with a completely factorized distribution , the ( cid : 123 ) xed point equations couple the parameters associated with each node with the parameters of its markov blanket .
in this sense , the ( cid : 123 ) xed point equations propagate information along the same pathways as those de ( cid : 123 ) ning the exact algorithms for probability propagation .
the following may provide an intuitive interpretation of the approximation made by this distribution .
given a particular observation sequence , the hidden state vari - ables for the m markov chains at time step t are stochastically coupled .
this stochastic coupling is approximated by a system in which the hidden variables are uncorrelated but have coupled means .
the variational or \mean - ( cid : 123 ) eld " equa - tions solve for the deterministic coupling of the means that best approximates the stochastically coupled system .
each hidden state vector is updated in turn using ( 123 ) , with a time complexity of o ( t m k 123 ) per iteration .
convergence is determined by monitoring the kl di - vergence in the variational distribution between successive time steps; in practice convergence is very rapid ( about 123 to 123 iterations of ( 123 ) ) .
convergence to a global minimum of the kl divergence is not required , and in general this procedure will converge to a local minimum .
once the ( cid : 123 ) xed point equations have converged , the expectations required for the e step can be obtained as a simple function of the
example 123 : structured approximation for factorial hmms
the approximation presented in the previous section factors the posterior probabil - ity into a product of statistically independent distributions over the state variables .
here we present another approximation which is tractable yet preserves many of the probabilistic dependencies in the original system .
in this scheme , the poste - rior distribution of the factorial hmm is approximated by m uncoupled hmms as shown in fig .
within each hmm , e ( cid : 123 ) cient and exact inference is implemented via the forward ( backward algorithm .
since this approximation is allowed to have dependencies between the hidden variables it will generally be superior to the com - pletely factorized mean - ( cid : 123 ) eld approximation presented in the previous section , that is , the lower bound will be higher and the kl - divergence lower .
the approach of exploiting such tractable substructures was ( cid : 123 ) rst suggested in the machine learning literature by saul and jordan ( 123 ) . 123
we write the structured variational approximation as
t123 ; ( cid : 123 ) )
lthe ( cid : 123 ) rst term is replaced by log ( cid : 123 ) ( m ) for t = 123 the second term does not appear for t = t .
february 123 , 123 123 : 123 wspc / 123 - ijprai
an introduction to hidden markov models and bayesian networks
where zq is a normalization constant ensuring that q sums to one .
the parameters of q are ( cid : 123 ) = f ( cid : 123 ) ( m ) ; ( cid : 123 ) ( m ) ; h ( m ) g | the original priors and state transition matrices of the factorial hmm and a time - varying bias for each state variable .
the prior and transition probabilities for q are
t123 ; ( cid : 123 ) ) =
where the last equality follows from the fact that s ( m ) t123 is a vector with a 123 in one position and 123 elsewhere .
comparing eqs .
( 123 ) ( ( 123 ) to eq .
( 123 ) , we can see that the k ( cid : 123 ) 123 vector h ( m ) plays the role of the probability of an observation ( p ( ytjst ) in ( 123 ) ) 123;j p ( s ( m ) for each of the k settings of s ( m ) 123j ( cid : 123 ) ) is equivalent to having an observation at time t = 123 that under state s ( m ) 123;j = 123 has probability h ( m )
123;j = 123j ( cid : 123 ) ) = h ( m )
for example , q ( s ( m )
intuitively , this approximation uncouples the m markov chains and attaches to each state variable a distinct ( cid : 123 ) ctitious observation .
the probability of this ( cid : 123 ) ctitious observation can be varied so as to minimize the kl divergence between q and p .
applying the same arguments as before , we obtain a set of ( cid : 123 ) xed point equations
that minimize kl ( qkp ) : ( cid : 123 ) yt mx
123 ~ y ( m )
where ( cid : 123 ) ( m ) is de ( cid : 123 ) ned as before , and where we rede ( cid : 123 ) ne the residual error to be
the parameter h ( m ) obtained from these ( cid : 123 ) xed point equations is the observation probability associated with state variable s ( m ) in hidden markov model m .
using these probabilities , the forward ( backward algorithm is used to compute a new set of expectations for hs ( m ) i , which are fed back into ( 123 ) and ( 123 ) .
the forward ( backward algorithm is therefore used as a subroutine in the minimization of the
notice the similarity between eqs .
( 123 ) and ( 123 ) and eqs .
( 123 ) and ( 123 ) for the completely factorized approximation .
in the completely factorized approximation , , the ( cid : 123 ) xed point equations can be written explicitly in terms
i = ( cid : 123 ) ( m )
february 123 , 123 123 : 123 wspc / 123 - ijprai
ghahramani
i on h ( m )
of the variational parameters .
in the structured approximation , the dependence of is computed via the forward ( backward algorithm .
also , the ( cid : 123 ) xed point eq .
( 123 ) do not contain terms involving the prior , ( cid : 123 ) ( m ) , or transition matrix , ( cid : 123 ) ( m ) .
these terms are handled exactly by our choice of approximation .
the other intractable dynamic bayesian networks we have presented are also amenable to structured variational approximations .
in the case of tree structured hmms there are two natural choices for the substructures to be retained in the approximation .
one choice is to remove the arcs within a time step and retain the temporal dependencies , resulting in the bayesian network shown in fig .
the other choice is to retain the arcs within a time step and eliminate the arcs between consecutive time steps .
both of these approximations , along with an approximation based on the viterbi algorithm are pursued in ref
for switching state - space models , the natural approximation is to make the m state - space models ( ssms ) and the discrete markov process controlling the switch variable stochastically independent .
again , the variational approximation couples all the ssms and the switch variable deterministically , but this coupling can be computed tractably using kalman smoothing on each state - space model separately and the forward ( backward algorithm on the markov switching process .
the vari - ational parameters are the real - valued \responsibilities " of each state - space model for each observation in the sequence .
to determine the best variational parameters we start from some responsibilities and compute the posterior probability of the state in each ssm using kalman smoothing , with the data weighted by the respon - sibilities .
a weighting of 123 corresponds to applying the normal kalman smoothing equations , whereas a weighting of 123 corresponds to assuming that the data was not observed at all; intermediate weighting are implemented by dividing the r matrix in ( 123 ) by the responsibility .
we then recompute responsibilities by running the forward ( backward algorithm on the switch process using the prediction error of each ssm .
iterating this procedure until the responsibilities converge decreases the kl - divergence .
details of this structured variational approximation for switching state - space models and experimental results are presented in ref
bayesian hmms
we now turn to two very important issues in the learning of hmms and other graphical models : over ( cid : 123 ) tting and model selection .
over ( cid : 123 ) tting refers to the scenario where the model ( cid : 123 ) ts the training set very well but generalizes poorly to a test set chosen from the same data distribution .
over ( cid : 123 ) tting is most prevalent when the training set is small relative to the complexity ( i . e .
number of free parameters ) of the model .
there is nothing in the maximum likelihood ( cid : 123 ) tting procedure itself to avoid it .
model selection , or learning model structure , is the closely related problem of picking a particular structure amongst several alternatives ( or learning a distribution over these alternatives ) .
in the case of hmms the \model structure " would include everything from the number of hidden states to the form of the
february 123 , 123 123 : 123 wspc / 123 - ijprai
an introduction to hidden markov models and bayesian networks
state transition matrix and output probabilities .
model selection would also include whether to opt for a regular hmm or the more complex factorial hmm , for example .
to learn model structure it is necessary to compare models of di ( cid : 123 ) erent complexity and again , there is nothing in ml parameter ( cid : 123 ) tting that does this automatically .
there are three main ways to deal with the over ( cid : 123 ) tting and model selection regularization , and bayesian integration .
cross - validation repeatedly splits the training data into two sets : a new training set and a validation set .
this provides an estimate of the true generalization error but can become computationally prohibitive if more than a few model structure parameters have to be determined .
regularization augments the likelihood objective function with a penalty term that favors simpler models over more complex models .
for function approximation problems this regularizer is often of the form of a smoothness penalty on the function classes .
in the case of neural network models the regularizer is usually expressed as some sort of weight decay term .
while there are many ad hoc ways of picking regularizers , in the context of probabilistic modeling it is often illuminating to view regularizers are expressing a prior over the parameters , and the regularized ml ( cid : 123 ) tting procedure as ( cid : 123 ) nding maximum a posteriori ( map ) parameters under such a prior .
thus the choice of regularizer can be assessed subjectively by asking whether the implicit prior over parameters \makes sense " to the modeler .
for hmms with discrete outputs , a natural choice of prior over the parameters is given by the dirichlet distribution .
there are two main reasons for this . m first , the dirichlet distribution has the mathematically convenient property of being con - jugate to the multinomial distribution .
a family of priors is said to be conjugate to a family of likelihoods if the posterior obtained by multiplying the prior by the likelihood is in the same family as the prior .
for example , since the likelihood of the initial state given the parameter vector ( cid : 123 ) is multinomial ( cf
if the prior probability of ( cid : 123 ) is dirichlet ,
p ( s123j ( cid : 123 ) ) =
p ( ( cid : 123 ) ) =
with hyperparameter vector u = ( u123; : : : ; uk ) and normalization constant z , then the posterior is also dirichlet
p ( ( cid : 123 ) js123 ) =
similar dirichlet priors can be set up for columns of the transition matrix ( cid : 123 ) and emission matrix e .
mother , more theoretically motivated reasons are provided in ref
february 123 , 123 123 : 123 wspc / 123 - ijprai
ghahramani
second the dirichlet distribution has the desirable property that its hyper - parameters can be interpreted as hypothetical counts of observations .
in the above
example , if ui = 123 and uj = 123 for j 123= i , the map estimate of ( cid : 123 ) is identical to an
ml estimate of ( cid : 123 ) with a training set augmented with one additional observation of the initial state being in state i .
this makes it possible to implement map estimation with dirichlet priors as a minor variant of the baum ( welch procedure .
it also gives some theoretical justi ( cid : 123 ) cation for the seemingly ad hoc but very common regularization method for hmms which just adds a small positive number to all elements of the parameter vector .
as outlined in the beginning of sec .
123 , a bayesian approach to learning treats all unknown quantities as random variables , assigns priors to these quantities , and infers posterior probabilities having observed the data .
in the case of hmms , these unknown quantities comprise the structure of the hmm ( e . g .
number of states ) , the parameters , and the hidden states .
unlike ml and map , which ( cid : 123 ) nd point estimates of the parameters , we can now compare between model structures , but we need to integrate over both the parameters and the hidden states .
we call this approach
there are several methods for approximating the required integrals , which for hmms and their extensions are intractable .
we briey mention four of these meth - ods .
first let us make clear which integral we are referring to .
to compare models it is necessary to compute the posterior probability of a model , which is proportional to the product of the prior and the marginal likelihood , also known as the evidence ( cf
p ( mjd ) / p ( m )
p ( dj ( cid : 123 ) ;m ) p ( ( cid : 123 ) jm ) d ( cid : 123 )
the evidence , bracketed here , is a high - dimensional , often multimodal , intractable
monte carlo methods approximate the integral by taking samples from regions of high probability .
this can itself be hard , but can be made easier by setting up a markov chain to converge to the correct equilibrium distribution . 123
laplace approximations invoke the central limit theorem , which for well - behaved priors and data asserts that the parameter posterior will converge in the limit of large number of training samples to a gaussian around the map estimate of the parameters . n to estimate the evidence using the laplace approximation , map parameters are found in the usual optimization routines and then the curvature ( hessian ) of the log likelihood is computed at the map estimate .
the evidence is approximated by evaluating the ratio p ( ( cid : 123 ) ;d ) =p ( ( cid : 123 ) jd ) at the map estimate of ( cid : 123 ) , using the gaussian approximation in the denominator .
the laplace approxima - tion su ( cid : 123 ) ers from several disadvantages; here we mention two .
first , computing the
nin fact , this is never the case for unconstrained hmms which su ( cid : 123 ) er from an identi ( cid : 123 ) ability prob - lem , i . e .
the identity of the states can be permuted with no e ( cid : 123 ) ect on the likelihood .
because of this , the posterior for hmms converges to a mixture of gaussians .
february 123 , 123 123 : 123 wspc / 123 - ijprai
an introduction to hidden markov models and bayesian networks
hessian matrix for the parameters is usually computationally costly .
second , the gaussian approximation is not very good for models with parameters which are positive and sum to one , especially when there are many parameters relative to the size of the data set .
to our knowledge the laplace approximation has not been used
stolcke and omohundro123 presented an ingenious method for approximating the bayesian integrals for hmms .
if the states of the hmm were observed rather than hidden and if the parameter priors are dirichlet , the parameter posteriors also become dirichlet and the evidence integral factors into a product of easy dirich - let integrals .
so the intractability of the evidence integral for hmms stems from the fact that both the states and parameters are hidden .
stolcke and omohundro used a viterbi - like algorithm to determine a single most likely sequence of hidden states , and treated this sequence as if it had been observed .
they could then do the evidence integrals easily .
by iterating between these two steps they incrementally searched over model structures , merging or splitting states based on comparisons of this ( approximate ) evidence .
their approach , which trades o ( cid : 123 ) integrating over hid - den variables for integrating over parameters , attained impressive results recovering some simple ( cid : 123 ) nite state grammars .
the fourth approach to approximate bayesian integration is known both as ensemble learning and the variational bayesian method .
the basic idea is to si - multaneously approximate the distribution over both hidden states and parameters with a simpler distribution , usually by assuming the hidden states and parameters are independent .
more speci ( cid : 123 ) cally , the evidence can be lower bounded by applying jensens inequality twice log p ( djm ) = log
log p ( dj ( cid : 123 ) ;m ) + log
d ( cid : 123 ) p ( d; ( cid : 123 ) jm )
p ( d; ( cid : 123 ) jm )
d ( cid : 123 ) q ( ( cid : 123 ) ) log
( cid : 123 ) f ( q ( ( cid : 123 ) ) ; q ( s ) ) :
the variational bayesian approach iteratively maximizes f as a functional of the two free distributions , q ( s ) and q ( ( cid : 123 ) ) .
from ( 123 ) , we can see that this maximization is equivalent to minimizing the kl divergence between q ( s ) q ( ( cid : 123 ) ) and the joint posterior over hidden states and parameters p ( s; ( cid : 123 ) jd;m ) .
this approach was ( cid : 123 ) rst proposed for one - hidden layer neural networks ( which have no hidden state ) by hinton and van camp123 using the restriction that q ( ( cid : 123 ) ) is gaussian . 123 it has since been applied to various other models with hidden states and
february 123 , 123 123 : 123 wspc / 123 - ijprai
ghahramani
no restrictions on q ( ( cid : 123 ) ) and q ( s ) other than the assumption that they factorize in some way . 123;123;123;123 with only these factorization assumptions , free - form optimization with respect to the distributions q ( ( cid : 123 ) ) and q ( s ) is done using calculus of variations , and often results in a modi ( cid : 123 ) ed em - like algorithm .
mackay123 ( cid : 123 ) rst presented a variational bayesian approach to learning in hmms .
by assuming that the parameter prior was dirichlet and approximating the poste - rior to have independent parameters and hidden states , he showed that the optimal q ( ( cid : 123 ) ) was a dirichlet distribution .
furthermore , he showed that the optimal q ( s ) could be obtained by applying the forward ( backward algorithm to an hmm with d ( cid : 123 ) q ( ( cid : 123 ) ) log ( cid : 123 ) g , which can be evaluated for pseudoparameters given by ( cid : 123 ) dirichlet distributions .
thus the whole variational bayesian algorithm can be im - plemented as a simple modi ( cid : 123 ) cation of the baum ( welch algorithm .
the variational bayesian method contains as special cases both the map approach and a simple form of the stolcke and omohundro approach .
while very promising , especially given that it has been used successfully for nontrivial model structure learning in other models , 123 its potential has not been fully explored for hmms and their
in this paper we have reviewed hidden markov models in the context of recent ad - vances in the understanding of bayesian networks .
we have shown how hmms are a kind of bayesian network , and as such , the algorithms for learning and inference in hmms can be derived from more general algorithms for bayesian net - works .
it is possible to invent many generalizations of the hmm | such as facto - rial hmms , tree structured hmms , and switching state - space models | which , by using richer hidden representations , can model more interesting temporal relation - ships than hmms .
however having richer hidden state representations invariably leads to computational intractability in the algorithms for inferring the hidden state from observations .
monte carlo methods , such as gibbs sampling , and variational methods are two ways of handling this intractability .
finally , we discussed avoiding over ( cid : 123 ) tting and learning the model structure .
we presented several approaches , including one which makes a variational approxima - tion to full bayesian integration .
the author would like to thank geo ( cid : 123 ) rey e .
hinton , michael i .
jordan , david j .
mackay and lawrence k .
saul who were collaborators on some of the work reviewed in this paper .
february 123 , 123 123 : 123 wspc / 123 - ijprai
an introduction to hidden markov models and bayesian networks
