abstract .
this paper describes a scalable algorithm for the simultaneous mapping and local - ization ( slam ) problem .
slam is the problem of determining the location of environmen - tal features with a roving robot .
many of todays popular techniques are based on extended kalman lters ( ekfs ) , which require update time quadratic in the number of features in the map .
this paper develops the notion of sparse extended information lters ( seifs ) , as a new method for solving the slam problem .
seifs exploit structure inherent in the slam prob - lem , representing maps through local , web - like networks of features .
by doing so , updates can be performed in constant time , irrespective of the number of features in the map .
this paper presents several original constant - time results of seifs , and provides simulation results that show the high accuracy of the resulting maps in comparison to the computationally more cumbersome ekf solution .
the simultaneous localization and mapping ( slam ) problem is the problem of ac - quiring a map of an unknown environment with a moving robot , while simultane - ously localizing the robot relative to this map ( 123 , 123 ) .
the slam problem addresses situations where the robot lacks a global positioning sensor , and instead has to rely on a sensor of incremental ego - motion for robot position estimation ( e . g . , odometry , inertial navigation ) .
such sensors accumulate error over time , making the problem of acquiring an accurate map a challenging one .
within mobile robotics , the slam problem is often referred to as one of the most challenging ones ( 123 ) .
in recent years , the slam problem has received considerable attention by the scientic community , and a urry of new algorithms and techniques has emerged , as attested , for example , by a recent workshop on this topic ( 123 ) .
existing algorithms can be subdivided into batch and online techniques .
the former provide sophisti - cated techniques to cope with perceptual ambiguities ( 123 , 123 , 123 ) , but they can only generate maps after extensive batch processing .
online techniques are specically suited to acquire maps as the robot navigates ( 123 , 123 ) , which is of great practical im - portance in many navigation and exploration problems ( 123 ) .
todays most widely used online algorithms are based on extended kalman lters ( ekfs ) , based on a seminal series of papers ( 123 , 123 , 123 ) .
ekfs calculate gaussian posteriors over the locations of environmental features and the robot itself .
thrun , d .
koller , z .
ghahramani , h .
durrant - whyte , and andrew y
a key bottleneck of ekfswhich has been subject to intense researchis their computational complexity .
the standard ekf approach requires time quadratic in the number of features in the map , for each incremental update .
this computational burden restricts ekfs to relatively sparse maps with no more than a few hundred features .
recently , several researchers have developed hierarchical techniques that decompose maps into collections of smaller , more manageable submaps ( 123 , 123 , 123 ) .
while in principle , hierarchical techniques can solve this problem in linear time , many of these techniques still require quadratic time per update .
one recent tech - nique updates the estimate in constant time ( 123 ) by restricting all computation to the submap in which the robot presently operates .
using approximation techniques for transitioning between submaps , this work demonstrated that consistent error bounds can be maintained with a constant - time algorithm .
however , the method does not propagate information to previously visited submaps unless the robot subsequently revisits these regions .
hence , this method suffers a slower rate of convergence in comparison to the o ( n 123 ) full covariance solution .
alternative methods based on de - composition into submaps , such as the sequential map joining techniques described in ( 123 , 123 ) can achieve the same rate of convergence as the full ekf solution , but in - cur a o ( n123 ) computational burden .
a different line of research has relied on particle lters for efcient mapping ( 123 ) .
the fastslam algorithm ( 123 ) and related mapping algorithms ( 123 ) require time logarithmic in the number of features in the map , but they depend linearly on a particle - lter specic parameter ( the number of particles ) , whose scaling with environmental size is still poorly understood .
none of these ap - proaches , however , offer constant time updating while simultaneously maintaining global consistency of the map .
more recently ( and motivated by this paper ) , thin junction trees have been applied to the slam problem by paskin ( 123 ) .
this work establishes a viable alternative to the approach proposed here , with somewhat dif - ferent computational properties .
this paper proposes a new slam algorithm whose updates require constant time , independent of the number of features in the map .
our approach is based on the well - known information form of the ekf , also known as the extended informa - tion lter ( eif ) ( 123 ) .
to achieve constant time updating , we develop an approximate eif which maintains a sparse representation of environmental dependencies .
em - pirical simulation results provide evidence that the resulting maps are comparable in accuracy to the computationally much more cumbersome ekf solution , which is still at the core of most work in the eld .
our approach is best motivated by investigating the workings of the ekf .
fig - ure 123 shows the result of ekf mapping in an environment with 123 landmarks .
the left panel shows a moving robot , along with its gaussian estimates of the location of all 123 point features .
the central information maintained by the ekf solution is a covariance matrix of these different estimates .
the normalized covariance , i . e . , the correlation , is visualized in the center panel of this gure .
each of the two axes lists the robot pose ( x - y location and orientation ) followed by the x - y - locations of the 123 landmarks .
dark entries indicate strong correlations .
it is known that in the limit of slam , all x - coordinates and all y - coordinates become fully correlated ( 123 ) .
the checkerboard appearance of the correlation matrix illustrates this fact .
maintain - ing these cross - correlationsof which there are quadratically many in the number
slam with sparse extended information filters : theory and initial results
figure 123
typical snapshots of ekfs applied to the slam problem : shown here is a map ( left panel ) , a correlation ( center panel ) , and a normalized information matrix ( right panel ) .
notice that the normalized information matrix is naturally almost sparse , motivating our approach of using sparse information matrices in slam .
of features in the mapare essential to the slam problem .
this observation has given rise to the ( false ) suspicion that online slam is inherently quadratic in the number of features in the map .
the key insight that motivates our approach is shown in the right panel of fig - ure 123
shown there is the inverse covariance matrix ( also known as information matrix ( 123 , 123 ) ) , normalized just like the correlation matrix .
elements in this nor - malized information matrix can be thought of as constraints , or links , between the locations of different features : the darker an entry in the display , the stronger the link .
as this depiction suggests , the normalized information matrix appears to be naturally sparse : it is dominated by a small number of strong links , and possesses a large number of links whose values , when normalized , are near zero .
furthermore , link strength is related to distance of features : strong links are found only between geometrically nearby features .
the more distant two landmarks , the weaker their link .
this observation suggest that the ekf solution to slam possesses important structure that can be exploited for more efcient solutions .
while any two features are fully correlated in the limit , the correlation arises mainly through a network of local links , which only connect nearby landmarks .
our approach exploits this structure by maintaining a sparse information ma - trix , in which only nearby features are linked through a non - zero element .
the re - sulting network structure is illustrated in the right panel of figure 123 , where disks corresponds to point features and dashed arcs to links , as specied in the infor - mation matrix visualized on the left .
shown also is the robot , which is linked to a small subset of all features only , called active features and drawn in black .
storing a sparse information matrix requires linear space .
more importantly , updates can be performed in constant time , regardless of the number of features in the map .
the resulting lter is a sparse extended information lter , or seif .
we show empiri - cally that the seifs tightly approximate conventional extended information lters , which previously applied to slam problems in ( 123 , 123 ) and which are functionally equivalent to the popular ekf solution .
our technique is probably most closely related to work on slam lters that represent relative distances , such as newmans geometric projection lter ( 123 ) and extensions ( 123 ) , and csorbas relative lter ( 123 ) .
it is also highly related to prior work
thrun , d .
koller , z .
ghahramani , h .
durrant - whyte , and andrew y
figure 123
illustration of the network of landmarks generated by our approach .
shown on the left is a sparse information matrix , and on the right a map in which entities are linked whose information matrix element is non - zero .
as argued in the paper , the fact that not all landmarks are connected is a key structural element of the slam problem , and at the heart of our constant time solution .
by lu and milion ( 123 ) , who use poses as basic state variables in slam , between which they dene local constraints obtained via scam matching .
the locality of these constraints is similar to the local constraints in seifs , despite the fact that lu and milios do not formulate their lter in the information form .
the problem of calculating posterior over paths is that both the computation and the memory grows with the path length , even in environments of limited size .
it appears fea - sible to condense this information by subsuming multiple traversals of the same area into a single variable .
we suspect that such a step would be aproximate , and that it would require similar approximations as proposed in this paper .
at present , neither of these approaches permit constant time updating in slam , even though it appears that several of these techniques could be developed into constant time algorithms .
our work is also related to the rich body of literature on topological mapping ( 123 , 123 , 123 , 123 ) , which typically does not explicitly represent dependencies and correlations in the representation of uncertainty .
one can view seifs as a repre - sentation of local relative information between nearby landmarks; a feature shared by many topological approaches to mapping .
123 extended information filters
this section reviews the extended information lter ( eif ) , which forms the basis of our work .
eifs are computationally equivalent to extended kalman lters ( ekfs ) , but they represent information differently : instead of maintaining a covariance ma - trix , the eif maintains an inverse covariance matrix , also known as information matrix .
eifs have previously been applied to the slam problem , most notably by nettleton and colleagues ( 123 , 123 ) , but they are much less common than the ekf
most of the material in this section applies equally to linear and non - linear l - ters .
we have chosen to present all material in the extended , non - linear form , since robots are inherently non - linear .
slam with sparse extended information filters : theory and initial results
123 information form of the slam problem let xt denote the pose of the robot at time t .
for rigid mobile robots operating in a planar environment , the pose is given by its two cartesian coordinates and the robots heading direction .
let n denote the number of features ( e . g . , landmarks ) in the environment .
the variable yn with 123 ( cid : 123 ) n ( cid : 123 ) n denotes the pose of the n - th feature .
for example , for point landmarks in the plane , yn may comprise the two - dimensional cartesian coordinates of this landmark .
in slam , it is usually assumed that features do not change their pose ( or location ) over time .
the robot pose xt and the set of all feature locations y together constitute the
state of the environment .
it will be denoted by the vector ( cid : 123 ) t = ( cid : 123 ) xt y123 : : : yn ( cid : 123 ) t ,
where the superscript t refers to the transpose of a vector .
in the slam problem , it is impossible to sense the state ( cid : 123 ) t directlyotherwise there would be no mapping problem .
instead , the robot seeks to recover a proba - bilistic estimate of ( cid : 123 ) t .
written in a bayesian form , our goal shall be to calculate a posterior distribution over the state ( cid : 123 ) t .
this posterior p ( ( cid : 123 ) t j zt; ut ) is conditioned on past sensor measurements zt = z123; : : : ; zt and past controls ut = u123; : : : ; ut .
sen - sor measurements zt might , for example , specify the approximate range and bearing to nearby features .
controls ut specify the robot motion command asserted in the time interval ( t ( cid : 123 ) 123; t ) .
following the rich ekf tradition in the slam literature , our approach repre - sents the posterior p ( ( cid : 123 ) t j zt; ut ) by a multivariate gaussian distribution over the state ( cid : 123 ) t .
the mean of this distribution will be denoted ( cid : 123 ) t , and covariance matrix ( cid : 123 ) t :
p ( ( cid : 123 ) t j zt; ut ) / exp ( cid : 123 ) ( cid : 123 ) 123
123 ( ( cid : 123 ) t ( cid : 123 ) ( cid : 123 ) t ) t ( cid : 123 ) ( cid : 123 ) 123
( ( cid : 123 ) t ( cid : 123 ) ( cid : 123 ) t ) ( cid : 123 )
the proportionality sign replaces a constant normalizer that is easily recovered from the covariance ( cid : 123 ) t .
the representation of the posterior via the mean ( cid : 123 ) t and the co - variance matrix ( cid : 123 ) t is the basis of the ekf solution to the slam problem ( and to ekfs in general ) .
information lters represent the same posterior through a so - called information matrix ht and an information vector btinstead of ( cid : 123 ) t and ( cid : 123 ) t .
these are obtained by multiplying out the exponent of ( 123 ) :
t ( cid : 123 ) t ( cid : 123 ) 123 ( cid : 123 ) t t ( cid : 123 ) t + ( cid : 123 ) t
= exp ( cid : 123 ) ( cid : 123 ) 123 = exp ( cid : 123 ) ( cid : 123 ) 123 we now observe that the last term in the exponent , ( cid : 123 ) 123 t ( cid : 123 ) t does not contain the free variable ( cid : 123 ) t and hence can be subsumed into the constant normalizer .
this gives us the form :
t ( cid : 123 ) t ( cid : 123 ) 123
t ( cid : 123 ) t + ( cid : 123 ) t
/ expf ( cid : 123 ) 123
( cid : 123 ) t + ( cid : 123 ) t
| ( z )
the information matrix ht and the information vector bt are now dened as indi -
ht = ( cid : 123 ) ( cid : 123 ) 123
bt = ( cid : 123 ) t
thrun , d .
koller , z .
ghahramani , h .
durrant - whyte , and andrew y
using these notations , the desired posterior can now be represented in what is com - monly known as the information form of the kalman lter :
p ( ( cid : 123 ) t j zt; ut ) / exp ( cid : 123 ) ( cid : 123 ) 123
t ht ( cid : 123 ) t + bt ( cid : 123 ) t ( cid : 123 )
as the reader may easily notice , both representations of the multi - variate gaus - sian posterior are functionally equivalent ( with the exception of certain degenerate cases ) : the ekf representation of the mean ( cid : 123 ) t and covariance ( cid : 123 ) t , and the eif representation of the information vector bt and the information matrix ht .
in partic - ular , the ekf representation can be recovered from the information form via the
( cid : 123 ) t = h ( cid : 123 ) 123
( cid : 123 ) t = h ( cid : 123 ) 123
t = ( cid : 123 ) tbt
the advantage of the eif over the ekf will become apparent further below , when the concept of sparse eifs will be introduced .
of particular interest will be the geometry of the information matrix .
this matrix
is symmetric and positive - denite :
hxt;xt hxt;y123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) hxt;yn hy123;xt hy123;y123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) hy123;yn
hyn ;xt hyn ;y123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) hyn ;yn
each element in the information matrix constraints one ( on the main diagonal ) or two ( off the main diagonal ) elements in the state vector .
we will refer to the off - diagonal elements as links : the matrices hxt;yn link together the robot pose estimate for n 123= n123 and the location estimate of a specic feature , and the matrices hyn;yn123 link together two feature locations yn and yn123
although rarely made explicit , the manipulation of these links is the very essence of gaussian solutions to the slam problem .
it will be an analysis of these links that ultimately leads to a constant - time solution to the slam problem .
123 measurement updates in slam , measurements zt carry spatial information on the relation of the robots pose and the location of a feature .
for example , zt might be the approximate range and bearing to a nearby landmark .
without loss of generality , we will assume that each measurement zt corresponds to exactly one feature in the map .
sightings of multiple features at the same time may easily be processed one - after - another .
figure 123 illustrates the effect of measurements on the information matrix ht .
suppose the robot measures the approximate range and bearing to the feature y123 , as illustrated in figure 123a .
this observation links the robot pose xt to the location of y123
the strength of the link is given by the level of noise in the measurement .
updat - ing eifs based on this measurement involves the manipulation of the off - diagonal elements hxt;y and their symmetric counterparts hy;xt that link together xt and y .
additionally , the on - diagonal elements hxt;xt and hy123;y123 are also updated .
these updates are additive : each observation of a feature y increases the strength of the
slam with sparse extended information filters : theory and initial results
figure 123
the effect of measurements on the information matrix and the associated network of features : ( a ) observing y123 results in a modication of the information matrix elements hxt;y123
( b ) similarly , observing y123 affects hxt;y123
both updates can be carried out in constant total link between the robot pose and this very feature , and with it the total infor - mation in the lter .
figure 123b shows the incorporation of a second measurement of a different feature , y123
in response to this measurement , the eif updates the links ( and hxt;xt and hy123;y123 ) .
as this example suggests , measurements hxt;y123 = h t introduce links only between the robot pose xt and observed features .
measure - ments never generate links between pairs of landmarks , or between the robot and
for a mathematical derivation of the update rule , we observe that bayes rule
enables us to factor the desired posterior into the following product : p ( ( cid : 123 ) t j zt; ut ) / p ( zt j ( cid : 123 ) t; zt ( cid : 123 ) 123; ut ) p ( ( cid : 123 ) t j zt ( cid : 123 ) 123; ut )
= p ( zt j ( cid : 123 ) t ) p ( ( cid : 123 ) t j zt ( cid : 123 ) 123; ut )
the second step of this derivation exploited common ( and obvious ) independences in slam problems ( 123 ) .
for the time being , we assume that p ( ( cid : 123 ) t j zt ( cid : 123 ) 123; ut ) is represented by ( cid : 123 ) ht and ( cid : 123 ) bt .
those will be discussed in the next section , where robot motion will be addressed .
the key question addressed in this section , thus , concerns the representation of the probability distribution p ( zt j ( cid : 123 ) t ) and the mechanics of carrying out the multiplication above .
in the extended family of lters , a com - mon model of robot perception is one in which measurements are governed via a deterministic non - linear measurement function h with added gaussian noise :
zt = h ( ( cid : 123 ) t ) + " t
here " t is an independent noise variable with zero mean , whose covariance will be denoted z .
put into probabilistic terms , ( 123 ) species a gaussian distribution over the measurement space of the form
p ( zt j ( cid : 123 ) t ) / exp ( cid : 123 ) ( cid : 123 ) 123
123 ( zt ( cid : 123 ) h ( ( cid : 123 ) t ) ) t z ( cid : 123 ) 123 ( zt ( cid : 123 ) h ( ( cid : 123 ) t ) ) ( cid : 123 )
following the rich literature of ekfs , eifs approximate this gaussian by linearizing the measurement function h .
more specically , a taylor series expansion of h gives
h ( ( cid : 123 ) t ) ( cid : 123 ) h ( ( cid : 123 ) t ) + r ( cid : 123 ) h ( ( cid : 123 ) t ) ( ( cid : 123 ) t ( cid : 123 ) ( cid : 123 ) t )
thrun , d .
koller , z .
ghahramani , h .
durrant - whyte , and andrew y
where r ( cid : 123 ) h ( ( cid : 123 ) t ) is the rst derivative ( jacobian ) of h with respect to the state variable ( cid : 123 ) , taken ( cid : 123 ) = ( cid : 123 ) t .
for brevity , we will write ^zt = h ( ( cid : 123 ) t ) to indicate that this is a pre - diction given our state estimate ( cid : 123 ) t .
the transpose of the jacobian matrix r ( cid : 123 ) h ( ( cid : 123 ) t ) and will be denoted ct .
with these denitions , equation ( 123 ) reads as follows :
h ( ( cid : 123 ) t ) ( cid : 123 ) ^zt + c t
t ( ( cid : 123 ) t ( cid : 123 ) ( cid : 123 ) t )
this approximation leads to the following gaussian approximation of the measure - ment density ( 123 ) :
123 ( zt ( cid : 123 ) ^zt ( cid : 123 ) c t
t ( cid : 123 ) t + c t
t ( cid : 123 ) t ) t z ( cid : 123 ) 123 ( zt ( cid : 123 ) ^zt ( cid : 123 ) c t
t ( cid : 123 ) t + c t
multiplying out the exponent and regrouping the resulting terms gives us
t ctz ( cid : 123 ) 123c t
t ( cid : 123 ) t + ( zt ( cid : 123 ) ^zt + c t t ( cid : 123 ) t ) t z ( cid : 123 ) 123 ( zt ( cid : 123 ) ^zt + c t
t ( cid : 123 ) t ) t z ( cid : 123 ) 123c t
as before , the nal term in the exponent does not depend on the variable ( cid : 123 ) t and hence can be subsumed into the proportionality factor :
t ( cid : 123 ) t + ( zt ( cid : 123 ) ^zt + c t
t ( cid : 123 ) t ) t z ( cid : 123 ) 123c t
we are now in the position to state the measurement update equation , which imple - ment the probabilistic law ( 123 ) .
p ( zt j ( cid : 123 ) t ) / exp ( cid : 123 ) ( cid : 123 ) 123 = exp ( cid : 123 ) ( cid : 123 ) 123 123 ( zt ( cid : 123 ) ^zt + c t
t ctz ( cid : 123 ) 123c t
/ exp ( cid : 123 ) ( cid : 123 ) 123 p ( ( cid : 123 ) t j zt; ut ) / exp ( cid : 123 ) ( cid : 123 ) 123 ( cid : 123 ) exp ( cid : 123 ) ( cid : 123 ) 123 = expf ( cid : 123 ) 123
t ctz ( cid : 123 ) 123c t t ( ( cid : 123 ) ht + ctz ( cid : 123 ) 123c t
( cid : 123 ) ht ( cid : 123 ) t + ( cid : 123 ) bt ( cid : 123 ) t ( cid : 123 ) t ( cid : 123 ) t ) t z ( cid : 123 ) 123c t t ( cid : 123 ) t + ( zt ( cid : 123 ) ^zt + c t ) ( cid : 123 ) t + ( ( cid : 123 ) bt + ( zt ( cid : 123 ) ^zt + c t
t ( cid : 123 ) t ) t z ( cid : 123 ) 123c t
thus , the measurement update of the eif is given by the following additive rule :
ht = ( cid : 123 ) ht + ctz ( cid : 123 ) 123c t bt = ( cid : 123 ) bt + ( zt ( cid : 123 ) ^zt + c t
t ( cid : 123 ) t ) t z ( cid : 123 ) 123c t
in the general case , these updates may modify the entire information matrix ht and vector bt , respectively .
a key observation of all slam problems is that the jacobian ct is sparse .
in particular , ct is zero except for the elements that correspond to the robot pose xt and the feature yt observed at time t .
ct = ( cid : 123 ) @h
123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) 123 @h
this sparseness is due to the fact that measurements zt are only a function of the relative distance and orientation of the robot to the observed feature .
as a pleasing consequence , the update ctz ( cid : 123 ) 123c t to the information matrix in ( 123 ) is only non - zero in four places : the off - diagonal elements that link the robot pose xt with the observed feature yt , and the main - diagonal elements that correspond to xt and yt .
thus , the update equations ( 123 ) and ( 123 ) are well in tune with our intuitive descrip - tion given in the beginning of this section , where we argued that measurements only
slam with sparse extended information filters : theory and initial results
figure 123
the effect of motion on the information matrix and the associated network of fea - tures : ( a ) before motion , and ( b ) after motion .
if motion is non - deterministic , motion updates introduce new links ( or reinforce existing links ) between any two active features , while weak - ening the links between the robot and those features .
this step introduces links between pairs strengthen the links between the robot pose and observed features , in the informa -
to compare this to the ekf solution , we notice that even though the change of the information matrix is local , the resulting covariance usually changes in non - local ways .
put differently , the difference between the old covariance ( cid : 123 ) ( cid : 123 ) t = ( cid : 123 ) h ( cid : 123 ) 123 new covariance matrix ( cid : 123 ) t = h ( cid : 123 ) 123
is usually non - zero everywhere .
123 motion updates the second important step of slam concerns the update of the lter in accordance to robot motion .
in the standard slam problem , only the robot pose changes over time .
the environment is static .
the effect of robot motion on the information matrix ht are slightly more com - plicated than that of measurements .
figure 123a illustrates an information matrix and the associated network before the robot moves , in which the robot is linked to two ( previously observed ) landmarks .
if robot motion was free of noise , this link struc - ture would not be affected by robot motion .
however , the noise in robot actuation weakens the link between the robot and all active features .
hence hxt;y123 and hxt;y123 are decreased by a certain amount .
this decrease reects the fact that the noise in motion induces a loss of information of the relative location of the features to the robot .
not all of this information is lost , however .
some of it is shifted into between - landmark links hy123;y123 , as illustrated in figure 123b .
this reects the fact that even though the motion induced a loss of information of the robot relative to the features , no information was lost between individual features .
robot motion , thus , has the effect that features that were indirectly linked through the robot pose become linked
to derive the update rule , we begin with a bayesian description of robot mo - tion .
updating a lter based on robot motion motion involves the calculation of the
exploiting the common slam independences ( 123 ) leads to
p ( ( cid : 123 ) t j zt ( cid : 123 ) 123; ut ) =z p ( ( cid : 123 ) t j ( cid : 123 ) t ( cid : 123 ) 123; zt ( cid : 123 ) 123; ut ) p ( ( cid : 123 ) t ( cid : 123 ) 123 j zt ( cid : 123 ) 123; ut ) d ( cid : 123 ) t ( cid : 123 ) 123 =z p ( ( cid : 123 ) t j ( cid : 123 ) t ( cid : 123 ) 123; ut ) p ( ( cid : 123 ) t ( cid : 123 ) 123 j zt ( cid : 123 ) 123; ut ( cid : 123 ) 123 ) d ( cid : 123 ) t ( cid : 123 ) 123
thrun , d .
koller , z .
ghahramani , h .
durrant - whyte , and andrew y
the term p ( ( cid : 123 ) t ( cid : 123 ) 123 j zt ( cid : 123 ) 123; ut ( cid : 123 ) 123 ) is the posterior at time t ( cid : 123 ) 123 , represented by ht ( cid : 123 ) 123 and bt ( cid : 123 ) 123
our concern will therefore be with the remaining term p ( ( cid : 123 ) t j ( cid : 123 ) t ( cid : 123 ) 123; ut ) , which characterizes robot motion in probabilistic terms .
similar to the measurement model above , it is common practice to model robot
motion by a non - linear function with added independent gaussian noise :
( cid : 123 ) t = ( cid : 123 ) t ( cid : 123 ) 123 + ( cid : 123 ) t
( cid : 123 ) t = g ( ( cid : 123 ) t ( cid : 123 ) 123; ut ) + sx ( cid : 123 ) t
here g is the motion model , a vector - valued function which is non - zero only for the robot pose coordinates , as feature locations are static in slam .
the term labeled ( cid : 123 ) t constitutes the state change at time t .
the stochastic part of this change is modeled by ( cid : 123 ) t , a gaussian random variable with zero mean and covariance ut .
this gaussian variable is a low - dimensional variable dened for the robot pose only .
here sx is a projection matrix of the form sx = ( i 123 : : : 123 ) t , where i is an identity matrix of the same dimension as the robot pose vector xt and as of ( cid : 123 ) t .
each 123 in this matrix refers to a null matrix , of which there are n in sx .
the product sx ( cid : 123 ) t , hence , give the following generalized noise variable , enlarged to the dimension of the full state vector ( cid : 123 ) : sx ( cid : 123 ) t = ( ( cid : 123 ) t 123 : : : 123 ) t .
in eifs , the function g in ( 123 ) is approximated by its rst degree taylor series expansion :
g ( ( cid : 123 ) t ( cid : 123 ) 123; ut ) ( cid : 123 ) g ( ( cid : 123 ) t ( cid : 123 ) 123; ut ) + r ( cid : 123 ) g ( ( cid : 123 ) t ( cid : 123 ) 123; ut ) ( ( cid : 123 ) t ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 ) t ( cid : 123 ) 123 )
= ^ ( cid : 123 ) t + at ( cid : 123 ) t ( cid : 123 ) 123 ( cid : 123 ) at ( cid : 123 ) t ( cid : 123 ) 123
here at = r ( cid : 123 ) g ( ( cid : 123 ) t ( cid : 123 ) 123; ut ) is the derivative of g with respect to ( cid : 123 ) at ( cid : 123 ) = ( cid : 123 ) t ( cid : 123 ) 123 and ut .
the symbol ^ ( cid : 123 ) t is short for the predicted motion effect , g ( ( cid : 123 ) t ( cid : 123 ) 123; ut ) .
plugging this approximation into ( 123 ) leads to an approximation of ( cid : 123 ) t , the state at time t : ( cid : 123 ) t ( cid : 123 ) ( i + at ) ( cid : 123 ) t ( cid : 123 ) 123 + ^ ( cid : 123 ) t ( cid : 123 ) at ( cid : 123 ) t ( cid : 123 ) 123 + sx ( cid : 123 ) t hence , under this approximation the random variable ( cid : 123 ) t is again gaussian dis - tributed .
its mean is obtained by replacing ( cid : 123 ) t and ( cid : 123 ) t in ( 123 ) by their respective ( cid : 123 ) ( cid : 123 ) t = ( i + at ) ( cid : 123 ) t ( cid : 123 ) 123 + ^ ( cid : 123 ) t ( cid : 123 ) at ( cid : 123 ) t ( cid : 123 ) 123 + sx123 = ( cid : 123 ) t ( cid : 123 ) 123 + ^ ( cid : 123 ) t the covariance of ( cid : 123 ) t is simply obtained by scaled and adding the covariance of the gaussian variables on the right - hand side of ( 123 ) : ( cid : 123 ) ( cid : 123 ) t = ( i + at ) ( cid : 123 ) t ( cid : 123 ) 123 ( i + at ) t + 123 ( cid : 123 ) 123 + sxutst
= ( i + at ) ( cid : 123 ) t ( cid : 123 ) 123 ( i + at ) t + sxutst
update equations ( 123 ) and ( 123 ) are in the ekf form , that is , they are dened over means and covariances .
the information form is now easily recovered from the denition of the information form in ( 123 ) and its inverse in ( 123 ) .
in particular , we have
( cid : 123 ) ht = ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) 123
( cid : 123 ) bt = ( cid : 123 ) ( cid : 123 ) t
t = ( cid : 123 ) ( i + at ) ( cid : 123 ) t ( cid : 123 ) 123 ( i + at ) t + sxutst = ( cid : 123 ) ( i + at ) h ( cid : 123 ) 123 t ( cid : 123 ) 123 ( i + at ) t + sxutst ( cid : 123 ) ht =h ( cid : 123 ) t ( cid : 123 ) 123 + ^ ( cid : 123 ) tit t ( cid : 123 ) 123 + ^ ( cid : 123 ) tit ( cid : 123 ) ht = hh ( cid : 123 ) 123 t ( cid : 123 ) 123 + ^ ( cid : 123 ) t
slam with sparse extended information filters : theory and initial results
these equations appear computationally involved , in that they require the inversion of large matrices .
in the general case , the complexity of the eif is therefore cubic in the size of the state space .
in the next section , we provide the surprising result that both ( cid : 123 ) ht and ( cid : 123 ) bt can be computed in constant time if ht ( cid : 123 ) 123 is sparse .
123 sparse extended information filters
the central , new algorithm presented in this paper is the sparse extended informa - tion filter , or seif .
seif differ from the extended information lter described in the previous section in that is maintains a sparse information matrix .
an informa - tion matrix ht is considered sparse if the number of links to the robot and to each feature in the map is bounded by a constant that is independent of the number of features in the map .
the bound for the number of links between the robot pose and other features in the map will be denoted ( cid : 123 ) x; the bound on the number of links for each feature ( not counting the link to the robot ) will be denoted ( cid : 123 ) y .
the motivation for maintaining a sparse information matrix was already given above : in slam , the normalized information matrix is already almost sparse .
this suggests that by enforcing sparseness , the induced approximation error is small .
123 constant time results we begin by proving three important constant time results , which form the backbone of seifs .
all proofs can be found in the appendix .
this lemma ensures that measurements can be incorporated in constant time .
notice that this lemma does not require sparseness of the information matrix; rather , it is a well - known property of information lters in slam .
less trivial is the following lemma : lemma123 : iftheinformationmatrixissparseand at = 123 , themotionupdatein
t + st
lt = sx ( u ( cid : 123 ) 123 ( cid : 123 ) ht = ht ( cid : 123 ) 123 ( cid : 123 ) ht ( cid : 123 ) 123lt ( cid : 123 ) bt = bt ( cid : 123 ) 123 + ^ ( cid : 123 ) t
t ht ( cid : 123 ) 123 ( cid : 123 ) bt ( cid : 123 ) 123lt + ^ ( cid : 123 ) t
this result addresses the important special case at = 123 , that is , the jacobian of pose change with respect to the absolute robot pose is zero .
this is the case for robots with linear mechanics , and with non - linear mechanics where there is no cross - talk between absolute coordinates and the additive change due to motion .
case is addressed by the next lemma :
in general , at 123= 123 , since the x - y update depends on the robot orientation .
this requires constant time if the mean ( cid : 123 ) t is available for the robot pose and all active
( cid : 123 ) t = i ( cid : 123 ) sx ( i + ( st
thrun , d .
koller , z .
ghahramani , h .
durrant - whyte , and andrew y
h123t ( cid : 123 ) 123 = ( cid : 123 ) t ( cid : 123 ) ht = h123t ( cid : 123 ) 123sx ( u ( cid : 123 ) 123 ( cid : 123 ) ht = h123t ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 ) ht ( cid : 123 ) bt = bt ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 ) t
t + st
t ( cid : 123 ) 123 ( ( cid : 123 ) ht ( cid : 123 ) ht ( cid : 123 ) 123 + h123t ( cid : 123 ) 123 ) + ^ ( cid : 123 ) t
for at 123= 123 , a constant time update requires knowledge of the mean ( cid : 123 ) t ( cid : 123 ) 123 before the motion command , for the robot pose and all active landmarks ( but not the pas - sive features ) .
this information is not maintained by the standard information lter , and extracting it in the straightforward way ( via equation ( 123 ) ) requires more than constant time .
a constant - time solution to this problem will now be presented .
123 amortized approximated map recovery before deriving an algorithm for recovering the state estimate ( cid : 123 ) t from the informa - tion form , let us briey consider what parts of ( cid : 123 ) t are needed in seifs , and when .
seifs need the state estimate ( cid : 123 ) t of the robot pose and the active features in the map .
these estimates are needed at three different occasions : ( 123 ) the linearization of the non - linear measurement and motion model , ( 123 ) the motion update according to lemma 123 , and ( 123 ) the sparsication technique described further below .
for lin - ear systems , the means are only needed for the sparsication ( third point above ) .
we also note that we only need constantly many of the values in ( cid : 123 ) t , namely the estimate of the robot pose and of the locations of active features .
as stated in ( 123 ) , the mean vector ( cid : 123 ) t is a function of ht and bt : ( cid : 123 ) t = h ( cid : 123 ) 123
t = ( cid : 123 ) tbt
unfortunately , calculating ( 123 ) directly involves inverting a large matrix , which would requires more than constant time .
the sparseness of the matrix ht allows us to recover the state incrementally .
in particular , we can do so on - line , as the data is being gathered and the estimates b and h are being constructed .
to do so , it will prove convenient to pose ( 123 ) as an
lemma123 : thestate ( cid : 123 ) t isthemode ^ ( cid : 123 ) t : = argmax ( cid : 123 ) t p ( ( cid : 123 ) t ) ofthegaussiandis -
p ( ( cid : 123 ) t ) = const : ( cid : 123 ) exp ( cid : 123 ) ( cid : 123 ) 123
t ht ( cid : 123 ) t + bt
here ( cid : 123 ) t is a vector of the same form and dimensionality as ( cid : 123 ) t .
this lemma suggests that recovering ( cid : 123 ) t is equivalent to nding the mode of ( 123 ) .
thus , it transforms a matrix inversion problem into an optimization problem .
for this optimization prob - lem , we will now describe an iterative hill climbing algorithm which , thanks to the sparseness of the information matrix , requires only constant time per optimization
our approach is an instantiation of coordinate descent .
for simplicity , we state it here for a single coordinate only; our implementation iterates a constant number k of such optimizations after each measurement update step .
the mode ^ ( cid : 123 ) t of ( 123 ) is attained at :
^ ( cid : 123 ) t = argmax
p ( ( cid : 123 ) t ) = argmax
t ht ( cid : 123 ) t + bt
slam with sparse extended information filters : theory and initial results
figure 123
sparsication : a feature is deactivated by eliminating its link to the robot .
to compensate for this change in information state , links between active features and / or the robot are also updated .
the entire operation can be performed in constant time .
t ht ( cid : 123 ) t ( cid : 123 ) bt
we note that the argument of the min - operator in ( 123 ) can be written in a form that makes the individual coordinate variables ( cid : 123 ) i;t ( for the i - th coordinate of ( cid : 123 ) t ) explicit :
t ht ( cid : 123 ) t ( cid : 123 ) bt
t ( cid : 123 ) t = 123
where hi;j;t is the element with coordinates ( i; j ) in ht , and bi;t if the i - th com - ponent of the vector bt .
taking the derivative of this expression with respect to an arbitrary coordinate variable ( cid : 123 ) i;t gives us
hi;j;t ( cid : 123 ) j;t ( cid : 123 ) bt
setting this to zero leads to the optimum of the i - th coordinate variable ( cid : 123 ) i;t given all other estimates ( cid : 123 ) j;t :
i;t = h ( cid : 123 ) 123
i hbt ( cid : 123 ) ht ( cid : 123 ) ( k )
the same expression can conveniently be written in matrix notation , were si is a projection matrix for extracting the i - th component from the matrix ht :
i;t = ( st
t + htsist
all other estimates ( cid : 123 ) i123;t with i123 123= i remain unchanged in this update step , that is , i123;t = ( cid : 123 ) ( k ) as is easily seen , the number of elements in the summation in ( 123 ) , and hence the vector multiplication in ( 123 ) , is constant if ht is sparse .
hence , each update requires constant time .
to maintain the constant - time property of our slam algorithm , we can afford a constant number of updates k per time step .
this will generally not lead to convergence , but the relaxation process takes place over multiple time steps , resulting in small errors in the overall estimate .
thrun , d .
koller , z .
ghahramani , h .
durrant - whyte , and andrew y
the nal step in seifs concerns the sparsication of the information matrix ht .
sparsication is necessarily an approximative step , since information matrices in slam are naturally not sparseeven though normalized information matrices tend to be almost sparse .
in the context of slam , it sufces to remove links ( deactivate ) between the robot pose and individual features in the map; if done correctly , this also limits the number of links between pairs of features .
to see , let us briey consider the two circumstances under which a new link may be introduced .
first , observing a passive feature activates this feature , that is , intro - duces a new link between the robot pose and the very feature .
thus , measurement updates potentially violate the bound ( cid : 123 ) x .
second , motion introduces links between any two active features , and hence lead to violations of the bound ( cid : 123 ) y .
this consider - ation suggests that controlling the number of active features can avoid violation of both sparseness bounds .
our sparsication technique is illustrated in figure 123
shown there is the situa - tion before and after sparsication .
the removal of a link in the network corresponds to setting an element in the information matrix to zero; however , this requires the manipulation of other links between the robot and other active landmarks .
the re - sulting network is only an approximation to the original one , whose quality depends on the magnitude of the link before removal .
we will now present a constant - time sparsication technique .
to do so , it will
prove useful to partition the set of all features into three subsets : y = y + ) y 123 ) y ( cid : 123 ) where y + is the set of all active features that shall remain active .
y 123 are one or more active features that we seek to deactivate ( remove the link to the robot ) .
finally , y ( cid : 123 ) are all currently passive features .
the sparsication is best derived from rst principles .
if y+ ) y 123 contains all
currently active features , the posterior can be factored as follows : p ( xt; y j zt; ut ) = p ( xt; y 123; y +; y ( cid : 123 ) j zt; ut )
= p ( xt j y 123; y +; y ( cid : 123 ) ; zt; ut ) p ( y 123; y +; y ( cid : 123 ) j zt; ut ) = p ( xt j y 123; y +; y ( cid : 123 ) = 123; zt; ut ) p ( y 123; y +; y ( cid : 123 ) j zt; ut ) ( 123 ) in the last step we exploited the fact that if we know the active features y 123 and y + , the variable xt does not depend on the passive features y ( cid : 123 ) .
we can hence set y ( cid : 123 ) to an arbitrary value without affecting the conditional posterior over xt , p ( xt j y 123; y +; y ( cid : 123 ) ; zt; ut ) .
here we simply chose y ( cid : 123 ) = 123
to sparsify the information matrix , the posterior is approximated by the follow - ing distribution , in which we simply drop the dependence on y 123 in the rst term .
it is easily shown that this distribution minimizes the kl divergence to the exact , ~ p ( xt; y j zt; ut ) = p ( xt j y +; y ( cid : 123 ) = 123; zt; ut ) p ( y 123; y +; y ( cid : 123 ) j zt; ut ) p ( y 123; y +; y ( cid : 123 ) j zt; ut )
p ( xt; y + j y ( cid : 123 ) = 123; zt; ut ) p ( y + j y ( cid : 123 ) = 123; zt; ut )
slam with sparse extended information filters : theory and initial results
this posterior is calculated in constant time .
in particular , we begin by calculating the information matrix for the distribution p ( xt; y 123; y + j y ( cid : 123 ) = 123 ) of all variables but y ( cid : 123 ) , and conditioned on y ( cid : 123 ) = 123
this is obtained by extracting the submatrix of all state variables but y ( cid : 123 ) :
h123t = sx;y +;y 123st
x;y +;y 123 htsx;y +;y 123 st
x;y +;y 123
with that , the inversion lemma leads to the following information matrices for the terms p ( xt; y + j y ( cid : 123 ) = 123; zt; ut ) and p ( y + j y ( cid : 123 ) = 123; zt; ut ) , denoted h 123 t , respectively : t = h123t ( cid : 123 ) h123tsy123 ( st t = h123t ( cid : 123 ) h123tsx;y123 ( st
x;y123 h123tsx;y123 ) ( cid : 123 ) 123st
here the various s - matrices are projection matrices , analogous to the matrix sx dened above .
the nal term in our approximation ( 123 ) , p ( y123; y +; y ( cid : 123 ) j zt; ut ) , has the following information matrix :
t = ht ( cid : 123 ) htsxt ( st
xt htsxt ) ( cid : 123 ) 123st
putting these expressions together according to equation ( 123 ) yields the following information matrix , in which the landmark y 123 is now indeed deactivated :
~ ht = h 123
t + h 123
t ( cid : 123 ) h 123
t = ht ( cid : 123 ) h123tsy123 ( st
y123 h123tsy123 ) ( cid : 123 ) 123st x;y123 h123t ( cid : 123 ) htsxt ( st
the resulting information vector is now obtained by the following simple consider -
~ bt = ( cid : 123 ) t ~ ht = ( cid : 123 ) t t ht + ( cid : 123 ) t
t ( ht ( cid : 123 ) ht + ~ ht ) t ( ~ ht ( cid : 123 ) ht ) = bt + ( cid : 123 ) t
t ( ~ ht ( cid : 123 ) ht )
all equations can be computed in constant time .
the effect of this approximation is the deactivation of the features y 123 , while introducing only new links between active features .
the sparsication rule requires knowledge of the mean vector ( cid : 123 ) t for all active features , which is obtained via the approximation technique described in the previous section .
from ( 123 ) , it is obvious that the sparsication does not t = ( ~ ht ) ( cid : 123 ) 123 ( ~ bt ) t .
furthermore , our approximation affect the mean ( cid : 123 ) t , that is , h ( cid : 123 ) 123 minimizes the kl divergence to the correct posterior .
these property is essential for the consistency of our approximation .
the sparsication is executed whenever a measurement update or a motion up - date would violate a sparseness constraint .
active features are chosen for deactiva - tion in reverse order of the magnitude of their link .
this strategy tends to deactivate features whose last sighting is furthest away in time .
empirically , it induces approx - imation errors that are negligible for appropriately chosen sparseness constraints ( cid : 123 ) x
thrun , d .
koller , z .
ghahramani , h .
durrant - whyte , and andrew y
figure 123
comparison of ekfs with seifs using a simulation with n = 123 landmarks .
in both diagrams , the left panels show the nal lter result , which indicates higher certainties for our approach due to the approximations involved in maintaining a sparse information matrix .
the center panels show the links ( red : between the robot and landmarks; green : between land - marks ) .
the right panels show the resulting covariance and normalized information matrices for both approaches .
notice the similarity ! 123 experimental results
our present experiments are preliminary : they only rely on simulated data , and they require known data associations .
our primary goal was to compare seifs to the computationally more cumbersome ekf solution that is currently in widespread
an example situation comparing ekfs with our new lter can be found in fig - ure 123
this result is typical and was obtained using a sparse information matrix with ( cid : 123 ) x = 123 , ( cid : 123 ) x = 123 , and a constant time implementation of coordinate descent that updates k = 123 random landmark estimates in addition to the landmark esti - mates connected to the robot at any given time .
the key observation is the apparent similarity between the ekf and the seif result .
both estimates are almost indis - tinguishable , despite the fact that ekfs use quadratic update time whereas seif require only constant time .
we also performed systematic comparisons of three algorithms : ekfs , seifs , and a variant of seifs in which the exact state estimate ( cid : 123 ) t is available .
the latter was implemented using matrix inversion ( hence does not run in constant time ) .
it allowed us to tease apart the error introduced by the amortized mean recovery step , from the error induced through sparsication .
the following table depicts results for n = 123 landmarks , after 123 update cycles , at which point all three approaches are
seif with exact ( cid : 123 ) t seif ( constant time )
nal # of links
( with 123% conf .
interval ) ( with 123% conf .
interval ) ( per update ) ( 123 : 123 ( cid : 123 ) 123 : 123 ) ( cid : 123 ) 123 ( cid : 123 ) 123 ( 123 : 123 ( cid : 123 ) 123 : 123 ) ( cid : 123 ) 123 ( cid : 123 ) 123 ( 123 : 123 ( cid : 123 ) 123 : 123 ) ( cid : 123 ) 123 ( cid : 123 ) 123
123 ( cid : 123 ) 123 : 123 123 ( cid : 123 ) 123 : 123
slam with sparse extended information filters : theory and initial results
as these results suggest , our approach approximates ekf very tightly .
the residual map error of our approach is with 123 : 123 ( cid : 123 ) 123 ( cid : 123 ) 123 approximately 123% higher than that of the extended kalman lter .
this error appears to be largely caused by the coordinate descent procedure , and is possibly inated by the fact that k = 123 is a small value given the size of the map .
enforcing the sparseness constraint seems not to have any negative effect on the overall error of the resulting map , as the results for our sparse lter implementation suggest .
experimental results using a real - world data set can be found in ( 123 ) .
this paper proposed a constant time algorithm for the slam problem .
our ap - proach adopted the information form of the ekf to represent all estimates .
based on the empirical observation that in the information form , most elements in the normalized information matrix are near - zero , we developed a sparse extended in - formation lter , or seif .
this lter enforces a sparse information matrix , which can be updated in constant time .
in the linear slam case , all updates can be performed in constant time; in the non - linear case , additional state estimates are needed that are not part of the regular information form of the ekf .
we proposed a amortized constant - time coordinate descent algorithm for recovering these state estimates from the information form .
the approach has been fully implemented and compared to the ekf solution .
overall , we found that seifs produce results that differ only marginally from that of the ekfs .
given the computational advantages of seifs over ekfs , we believe that seifs should be a viable alternative to ekf solutions when building high -
our approach puts a new perspective on the rich literature on hierarchical map - ping , briey outlined in the introduction to this paper .
like seif , these techniques focus updates on a subset of all features , to gain computational efcienc y .
seifs , however , composes submaps dynamically , whereas past work relied on the deni - tion of static submaps .
we conjecture that our sparse network structures capture the natural dependencies in slam problems much better than static submap decom - positions , and in turn lead to more accurate results .
they also avoid problems that frequently occur at the boundary of submaps , where the estimation can become un - stable .
however , the verication of these claims will be subject to future research .
a related paper discusses the application of constant time techniques to information exchange problems in multi - robot slam ( 123 ) .
the authors would like to acknowledge invaluable contributions by the following researchers : wolfram burgard , geoffrey gordon , kevin murphy , eric nettleton , michael stevens , and ben wegbreit .
this research has been sponsored by darpas mars program ( contracts n123 - 123 - c - 123 and nbch123 ) , darpas coabs program ( contract f123 - 123 - 123 - 123 ) , and darpas mica program ( con - tract f123 - 123 - c - 123 ) , all of which is gratefully acknowledged .
the authors fur -
thrun , d .
koller , z .
ghahramani , h .
durrant - whyte , and andrew y
thermore acknowledge support provided by the national science foundation ( ca - reer grant number iis - 123 and regular grant number iis - 123 ) .
