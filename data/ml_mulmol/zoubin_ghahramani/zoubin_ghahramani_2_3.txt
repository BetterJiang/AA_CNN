feedforward connections into successively more abstract in successive hidden layers .
such models are biologically unrealistic because they do not allow for top - down eects when perceiving noisy or ambiguous data ( mumford 123; gregory 123 ) and they do not explain the prevalence of connections in the cortex .
in this paper , we take seriously the idea that vision is inverse graphics ( horn 123 ) and so we start with a stochastic , generative neural network that uses top - down connections to convert an abstract representation of a scene into an intensity image .
this neurally instantiated graphics model is learned and the top - down connection strengths contain the networks visual knowledge of the world . visual perception consists of inferring the under - lying state of the stochastic graphics model using the false but useful assumption that the observed sensory input was generated by the model .
since the top - down graphics model is stochastic there are usually many dierent states of the hidden units that could have generated the same image , hidden state congurations are typically much more probable than others .
for the simplest generative models , it is tractable to represent the entire posterior probability distribution over hidden congurations from observing an image .
for more complex models , we shall have to be content with a perceptual inference process that picks one or a few congurations roughly according to their posterior probabilities ( hinton & sejnowski 123 ) .
though some of
one advantage of starting with a generative model is that it provides a natural specication of what visual
for example ,
exactly how top - down expectations should be used to disambiguate noisy data without unduly distorting reality .
another advantage is that it provides a sensible objective function for unsupervised learning .
learning can be viewed as maximizing the likelihood of the observed data under the generative model .
this is to discovering ecient ways of coding the sensory data , because the data could be communicated to a receiver by sending the underlying states of the generative model and this is an ecient code if and only if the generative model assigns high probability to the sensory data .
in this paper we present a sequence of progressively more sophisticated generative models .
for each model , the procedures for performing perceptual inference and for learning the top - down weights follow naturally from the generative model itself .
we start with two very simple models , factor analysis and mixtures of gaussians , that were rst developed by statisticians .
many of the existing models of how the cortex learns are actually even simpler versions of these statistical approaches in which certain variances have been set to zero .
we explain factor analysis and mixtures of gaus - in some detail .
to clarify the relationships between these statistical methods and neural network models , we describe the statistical methods as neural networks that can both generate data using top - down connections and perform perceptual interpretation of observed data using bottom - up connections .
we then describe a historical sequence of more sophisticated hierarchical , nonlinear generative models and the learning algorithms that go with them .
we conclude with a new model , the rectied gaussian belief net , and present examples where it is very eective at disco - vering hierarchical , sparse , distributed representations of the type advocated by barlow ( 123 ) and olshausen & field ( 123 ) .
the new model makes strong sugges - tions about the role of both top - down and lateral
b ( 123 ) 123 , 123^123 printed in great britain
& 123 the royal society
123 g .
hinton and z .
ghahramani generative models for discovering sparse distributed representations
p ( cid : 123 ) djsj ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 ) q
figure 123
a generative neural network for mixtures of
connections in the cortex and it also suggests why topo - graphic maps are so prevalent .
m i xt u r es of gaus si a ns
a mixture of gaussians is a model that describes some real data points in terms of underlying gaussian clusters .
there are three aspects of this model which we shall discuss .
first , given parameters that specify the means , variances and mixing proportions of the clus - ters , the model denes a generative distribution which assigns a probability to any possible data point .
second , given the parameter values and a data point , posterior probability that the data came from each of the clusters .
third , given a set of observed data points , the learning process adjusts the parameter values to maximize the probability that the generative model would produce the observed data .
viewed as a neural network , a mixture of gaussians consists of a layer of visible units whose state vector represents a data point and a layer of hidden units each of which represents a cluster ( see gure 123 ) .
to generate a data point we rst pick one of the hidden units , j , with a probability ( cid : 123 ) j and give it a state sj ( cid : 123 ) 123
all other hidden states are set to 123
the generative weight vector of the hidden unit , g j , represents the mean of a gaussian cluster .
when unit j is activated it sends a top - down input of gji to each visible unit , i .
local , zero - mean , gaussian noise with variance ( cid : 123 ) 123 added to the top - down input to produce a sample from an axis - aligned gaussian that has mean gj and a covar - iance matrix that has the ( cid : 123 ) 123 i terms along the diagonal and zero elsewhere .
the probability of generating a particular vector of visible states , d with elements di , is
interpreting a data point , d , consists of computing the posterior probability that it was generated from each of the hidden units , assuming that it must have come from one of them .
each hidden unit , computes the probability density of the data point under its gaussian model :
b ( 123 )
these conditional probabilities are then weighted by the mixing proportions , ( cid : 123 ) j , and normalized to give the posterior probability or responsibility of each hidden unit , j , for the data point .
by bayes theorem :
p ( cid : 123 ) sj ( cid : 123 ) 123jd ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) jp ( djsj ( cid : 123 ) 123 ) ( cid : 123 ) k ( cid : 123 ) kp ( djsk ( cid : 123 ) 123 )
the computation of p ( djsj ( cid : 123 ) 123 ) in equation ( 123 ) can be done very simply by using recognition connections , rij , from the visible to the hidden units .
the recognition connections are set equal to the generative connections , rij ( cid : 123 ) gji .
the normalization in equation ( 123 ) could be done by using direct lateral connections or interneur - ones to ensure that the total activity in the hidden layer is a constant .
learning consists of adjusting the generative para - meters g , ( cid : 123 ) , ( cid : 123 ) so as to maximize the product of the probabilities assigned to all the observed data points by equation ( 123 ) .
an ecient way to perform the learning is to sweep through all the observed data points computing p ( sj ( cid : 123 ) 123jd ) for each hidden unit and then to reset all the generative parameters in parallel .
angle brackets are used to denote averages over the
gj ( new ) ( cid : 123 ) hp ( sj ( cid : 123 ) 123jd ) di=hp ( sj ( cid : 123 ) 123jd ) i ,
i ( cid : 123 ) new ( cid : 123 ) ( cid : 123 )
p ( sj ( cid : 123 ) 123jd ) ( di ( cid : 123 ) gji ) 123
( cid : 123 ) j ( new ) ( cid : 123 ) hp ( sj ( cid : 123 ) 123jd ) i .
this is a version of the expectation and maximiza - al .
123 ) and is tion algorithm ( dempster guaranteed to raise the likelihood of the observed data unless it is already at a local optimum .
the computa - tion of the posterior probabilities of the hidden states given the data ( i . e .
perceptual inference ) is called the e - step and the updating of the parameters is called
instead of performing an m - step after a full sweep through the data it is possible to use an on - line gradient algorithm that uses the same posterior probabilities of hidden states but updates each generative weight using a version of the delta rule with a learning rate of ( cid : 123 ) :
( cid : 123 ) gji ( cid : 123 ) ( cid : 123 ) p ( sj ( cid : 123 ) 123jd ) ( di ( cid : 123 ) gji ) .
the k - means algorithm ( a form of vector quantiza - tion ) is the limiting case of a mixture of gaussians model where the variances are assumed equal and in - nitesimal and the ( cid : 123 ) j are assumed equal .
under these assumptions the posterior probabilities in equation ( 123 ) go to binary values with p ( sj ( cid : 123 ) 123jd ) ( cid : 123 ) 123 for the gaus - sian whose mean is closest to d and 123 otherwise .
competitive learning algorithms ( e . g .
rumelhart & zipser 123 ) can generally be viewed as ways of tting mixture of gaussians generative models .
they are usually inecient because they do not use a full m - step and slightly wrong because they pick a single
generative models for discovering sparse distributed representations g .
hinton and z .
ghahramani 123
winner among the hidden units instead of making the states proportional to the posterior probabilities .
kohonens self - organizing maps ( kohonen 123 ) , durbin & willshaws elastic net ( 123 ) , and the genera - tive topographic map ( bishop et al .
123 ) are variations of vector quantization or mixture of gaussian models in which additional constraints are imposed that force neighbouring hidden units to have similar generative weight vectors .
these constraints typically lead to a model of the data that is worse when measured by equation ( 123 ) .
so in these models , topographic maps are not a natural consequence of trying to maximize the the data .
they are imposed on the mixture model to make the solution easier to interpret and more brain - like .
by contrast , the algorithm we present later has to produce topographic maps to maxi - mize the data likelihood in a sparsely connected net .
because the recognition weights are just the trans - pose of the generative weights and because many researchers do not think in terms of generative models , neural network models that perform competitive learning typically only have the recognition weights required for perceptual inference .
the weights are learned by applying the rule that is appropriate for the generative weights .
this makes the model much simpler to implement but harder to understand .
neural net models of unsupervised learning that are derived from mixtures have simple learning rules and produce representations that are a highly nonlinear function of the data , but they suer from a disastrous weakness in their representational abilities .
each data point is represented by the identity of the winning hidden unit ( i . e .
the cluster it belongs to ) .
so for the representation to contain , on average , n bits of informa - tion about the data , there must be at least 123n hidden units .
( this point is often obscured by the fact that the posterior distribution is a vector of real - valued states across the hidden units .
this vector contains a lot of information about the data and supervised radial basis function networks make use of this rich information .
however , from the generative or coding viewpoint , the posterior distribution must be viewed as a probability distribution across discrete impoverished representa - tions , not a real - valued representation . )
factor a na lysi s
in factor analysis , correlations among observed vari - ables are explained in terms of shared hidden variables called factors which have real - valued states .
viewed as a neural network , the generative model underlying a standard version of factor analysis assumes that the state , yj of each hidden unit , j , is chosen independently from a zero - mean , unit - variance gaussian and the state , di , of each visible unit , i , is then chosen from a j yjgji .
so the gaussian with variance ( cid : 123 ) 123 only dierence from the mixture of gaussians model is that the hidden state vectors are continuous and gaus - sian distributed rather than discrete vectors contain a single 123
i and mean
the probability of generating a particular vector of is obtained by integrating over all
visible states , d ,
b ( 123 )
possible states , y , of the hidden units , weighting each hidden state vector by its probability under the genera -
because the network is linear and the noise is gaus -
sian , this integral is tractable .
maximum likelihood factor analysis ( everitt 123 ) consists of nding generative weights and local noise levels for the visible units so as to maximize the likeli - hood of generating the observed data .
without loss of generality , the generative noise model for the hidden units can be set to be a zero mean gaussian with a covariance equal to the identity matrix .
( a ) computing posterior distributions
given some generative parameters and an observed data point , the perceptual inference problem is to compute the posterior distribution in the continuous hidden space .
this is not as simple as computing the discrete posterior distribution for a mixture of gaus - sians model .
fortunately , the posterior distribution in the continuous factor space is a gaussian whose mean linearly on the data point and can be computed using recognition connections .
in general , equal
there are several reasons why the correct recogni - tion weights are not , generative weights .
visible units with lower local noise variances will have larger recognition weights , all else being equal .
but even if all the visible noise variances are equal , the recognition weights need not be propor - tional to the generative weights because the generative weight vectors of the hidden units ( known as the factor loadings ) do not need to be orthogonal .
( if an invertible linear transformation , l , is applied to all the generative weight vectors and l123 is applied to the prior noise distribution of the hidden units , the likelihood of the data is unaected .
the only consequence is that l123 gets applied to the posterior distributions in hidden space .
this means that the generative weight vectors can always be forced to be orthogonal , but only if a full covariance matrix is used for the prior . ) generative weight vectors that are not orthogonal give rise to a very important phenomenon known as explaining away that occurs during perceptual inference ( pearl
suppose that the visible units all have equal noise variances and that two hidden units have generative weight vectors that have a positive scalar product .
even though the states of the two hidden units are uncorrelated in the generative model , they will be anti - correlated in the posterior distribution for a given data point .
when one of the units is highly active it explains the part of the data point that projects on to it and so there is no need for the other hidden unit to be so active ( gure 123 ) .
by using appropriate recognition
123 g .
hinton and z .
ghahramani generative models for discovering sparse distributed representations
weights it is possible to correctly handle the data - dependent eects of explaining away on the mean of the posterior distribution .
but learning these recogni - tion weights in a neural net is tricky ( neal & dayan
when the generative weight vectors of the hidden units are not orthogonal , the posterior probability distribution in hidden space has a full covariance matrix .
this matrix does not depend on the data , but it does depend on the generative weights ( gure 123 ) .
the diculty of representing and computing this full covariance posterior in a neural network probably explains why factor analysis has seldom been put forward as a neural model .
however , a simplied version of factor analysis called principal components analysis has been very popular .
as we shall see in ) 123c ,
figure 123
( a ) the hidden - to - visible generative weights , and the hidden and visible local noise variances for a simple factor analysis model .
( b ) the surprising visible - to - hidden that compute the data - dependent mean of the posterior distribution in the two - dimensional hidden space .
( c ) samples from the posterior distribution in hidden space given the data point ( 123 , 123 ) .
the covariance of the posterior depends on the scalar product of the genera - tive weight vectors of the two hidden units .
in this example the positive scalar product leads to explaining away , which shows up as a negative correlation of the two hidden vari -
b ( 123 )
a sensible neural network implementation of analysis is possible and the new model we present in ) 123 is a nonlinear generalization of it .
( b ) principal components analysis
just as mixtures of gaussians can be reduced to vector quantization by making the variances of the gaussians equal and innitesimal , factor analysis can be reduced to principal components analysis by letting the variances associated with the visible units be equal and innitesimal , and the variances associated with the hidden units be non - innitesimal .
in this limiting case , the posterior distribution in the hidden space shrinks to a single point .
if the generative weight vectors are forced to be orthogonal , this point can be found by projecting the data on to the plane spanned by the generative weight vectors , and the weight matrix that does this projection is just the transpose of the genera - tive weight matrix .
principal components analysis has several advantages over full factor analysis as a neural network model .
it eliminates the need to compute or represent a full covariance posterior distribution in the hidden state space , and it makes weights that convert data into hidden representations identical to the generative weights , so the neural network does not need to explicitly represent the generative weights .
however , the price of this simpli - city is that it cannot be generalized to multilayer , nonlinear , stochastic , generative models .
t h e n e ed for spa rse di st r i bu t ed r e pr esen tat ions
in factor analysis ,
the visible units
factor analysis and mixtures of gaussians are at opposite ends of a spectrum of possible learning algo - the representation is componential or distributed because it involves states of all of the hidden units .
however , it is also linear and is therefore limited to capturing the information in the pairwise covariances of higher - order structure is invisible to it .
at the other the spectrum , mixtures of gaussians have localist representations because each data point assumed to be generated from a single hidden unit .
this is an exponentially inecient representation , but it is nonlinear and with enough hidden units it can capture all of the higher - order structure in the data .
( if the structure involves multiple interacting causes , a mixture of gaussians models cannot make the separate causes explicit in the hidden units , but it can model the probability density to any accuracy required . )
the really interesting generative models lie in the middle of the spectrum .
they use nonlinear distributed representations .
to see why such representations are needed , consider a typical image that contains multiple objects .
to represent the pose and deformation of each object we want a componential representation of the objects parameters .
to represent objects we need several of these componential represen - tations at once .
there is another way of thinking about the advan -
sparse distributed representations
generative models for discovering sparse distributed representations g .
hinton and z .
ghahramani 123
advantageous to represent images in terms of basis functions ( as factor analysis does ) , but for dierent classes of images , dierent basis functions are appro - priate .
so it is useful to have a large repertoire of basis functions and to select the subset that are optimal for representing the current image .
if an ecient algorithm can be found for tting models of this type it is likely to prove even more fruitful than the ecient back - propagation algorithm for multilayer nonlinear regression ( rumelhart et al .
the diculty lies in the computation of the posterior distribution over hidden states when given a data point .
this distribution , or an approximation to it , is required both for learning the generative model and for perceptual inference once the model has been learned .
mixtures of gaussians and factor analysis are standard statistical models precisely because the exact computation of the posterior distribution is tractable .
f rom boltz m a n n m ac h i n es to logi st ic be l i e f n ets
figure 123
units in a belief network .
the boltzmann machine ( hinton & sejnowski 123 ) was , perhaps , the rst neural network learning algo - rithm to be based on an explicit generative model that used distributed , nonlinear representations .
boltzmann machines use stochastic binary units and , with h hidden units , the number of possible representations of each data point is 123h .
it would take exponential time to compute the posterior distribution across all of these possible representations and most of them would typi - cally have probabilities very close to 123 , boltzmann machine uses a monte - carlo method known as gibbs sampling ( hinton & sejnowski 123; geman & geman 123 ) to pick stochastically represen - tations according to their posterior probabilities .
both the gibbs sampling for perceptual inference and the learning rule for following the gradient of the log like - lihood of the data are remarkably simple to implement in a network of symmetrically connected stochastic binary units .
unfortunately , the learning algorithm is extremely slow and , as a result , the unsupervised form of the algorithm never really succeeded in extracting interesting hierarchical representations .
the boltzmann machine learning algorithm is slow for two reasons .
first , the perceptual inference is slow because it must spend a long time doing gibbs sampling before the probabilities are correct .
second , the learning signal for the weight between two units is the dierence between their sampled correlation in two dierent conditions .
when the two correlations are the sampling noise makes their dierence
neal ( 123 ) realized that learning is considerably more ecient if , instead of symmetric connections , the generative model uses directed connections that form an acyclic graph .
this kind of generative model called a belief network and it has an important prop - is missing in models whose generative compute the joint probability of a data point and a conguration of states of all the hidden units .
in a boltzmann machine , this probability depends not only
b ( 123 )
on the particular hidden states but also on an additional normalization term called the partition function that involves all possible congurations of states .
it is the derivatives of the partition function that make boltz - mann machine learning so inecient .
investigated logistic belief nets ( lbn ) that consist of multiple layers of binary stochastic units ( gure 123 ) .
to generate data , each unit , j , picks a binary state , sj , based on a top - down expectation ^sj which is determined by its generative bias , g123j , the binary states of units , k , in the layer above and the weights on the generative connections coming from those units :
p ( sj ( cid : 123 ) 123 ) ( cid : 123 ) ^sj ( cid : 123 ) ( cid : 123 ) ( g123j ( cid : 123 ) p
k sk gkj ) ,
where ( cid : 123 ) ( x ) ( cid : 123 ) ( 123 ( cid : 123 ) exp ( ( cid : 123 ) x ) ) ( cid : 123 ) 123
( a ) perceptual inference in a logistic belief net
as with boltzmann machines ,
it is exponentially expensive to compute the exact posterior distribution over the hidden units of an lbn when given a data point , so neal used gibbs sampling .
with a particular data point clamped on the visible units , the hidden units are visited one at a time .
each time hidden unit u is visited , its state is stochastically selected to be 123 or 123 in proportion to two probabilities .
the rst , p ( cid : 123 ) jsu ( cid : 123 ) 123 , is the joint probability of generating the states of all the units in the network ( including u ) if u has state 123 and all the others have the state dened by the current congura - tion of states , a .
the second , p ( cid : 123 ) jsu ( cid : 123 ) 123 , is the same quantity if u has state 123
when calculating these prob - abilities , the states of all the other units are held constant .
it can be shown that repeated application of this stochastic decision rule eventually leads to hidden state congurations being selected according to their
because the lbn is acyclic it is easy to compute the joint probability p a of a conguration , a , of states of all the units .
the units that send generative connections to unit i are called the parentsof i and we denote the states of these parents in global conguration a by pa ( i , a ) :
123 g .
hinton and z .
ghahramani generative models for discovering sparse distributed representations
p ( cid : 123 ) ( cid : 123 ) q
is the binary state of unit i in conguration a .
it is convenient to work in the domain of negative log probabilities which are called energies by analogy with statistical physics .
we dene e ( cid : 123 ) to be ( cid : 123 ) ln p ( cid : 123 ) , u ) ln ( 123 ( cid : 123 ) ^s ( cid : 123 )
e ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) p
u ( cid : 123 ) ( 123 ( cid : 123 ) s ( cid : 123 )
u ln ^s ( cid : 123 )
u is the binary state of unit u in conguration a , u is the top - down expectation generated by the layer above , and u is an index over all the units in the net .
the rule for stochastically picking a new state for u requires the ratio of two probabilities and hence the dierence of two energies
u ( cid : 123 ) e ( cid : 123 ) jsu ( cid : 123 ) 123 ( cid : 123 ) e ( cid : 123 ) jsu ( cid : 123 ) 123 , p ( su ( cid : 123 ) 123j ( cid : 123 ) ) ( cid : 123 ) ( cid : 123 ) ( ( cid : 123 ) e ( cid : 123 )
all the contributions to the energy of conguration ( cid : 123 ) that do not depend on sj can be ignored when leaves a contribution that depends on the top - down expectation ^sj generated by the units in the layer above ( see equation ( 123 ) ) and a contribution that depends on both the states , si and the top - down expectations , ^si , of units in the layer below
j ( cid : 123 ) ln ^s ( cid : 123 )
j ( cid : 123 ) ln ( 123 ( cid : 123 ) ^s ( cid : 123 ) ( cid : 123 ) ( 123 ( cid : 123 ) s ( cid : 123 ) i ln ^s ( cid : 123 ) jsj ( cid : 123 ) 123 i ln ^s ( cid : 123 ) jsj ( cid : 123 ) 123
( cid : 123 ) ( 123 ( cid : 123 ) s ( cid : 123 )
i ) ln ( 123 ( cid : 123 ) ^s ( cid : 123 ) jsj ( cid : 123 ) 123
i ) ln ( 123 ( cid : 123 ) ^s ( cid : 123 ) jsj ( cid : 123 ) 123
given samples from the posterior distribution , the generative weights of a lbn can be learned by using the on - line delta rule which performs gradient ascent in the log likelihood of the data :
( cid : 123 ) gji ( cid : 123 ) ( cid : 123 ) sj ( si ( cid : 123 ) ^si ) .
until very recently ( lewicki & sejnowski 123 ) , logistic belief nets were widely ignored as models of neural computation .
this may be because the computa - tion of ( cid : 123 ) e ( cid : 123 ) j in equation ( 123 ) requires unit j to observe not only the states of units in the layer below but also their top - down expectations .
in ) 123c we show how this problem can be nessed but rst we describe an alterna - tive way of making lbns biologically plausible .
t h e wa k e ^ sl e e p a lg or i t h m
there is an approximate method of performing perceptual inference in a lbn that leads to a very simple implementation which would be biologically quite plausible if only it were better at extracting the hidden causes of data ( hinton et al .
instead of using gibbs sampling , we use a separate set of bottom - up recognition connections to pick binary states for units in one layer given the already selected binary states of units in the layer below .
the learning rule for the top - down generative weights is the same as for an lbn .
it can be shown that this learning rule , instead of following the gradient of the log likelihood now
b ( 123 )
follows the gradient of the penalized log likelihood where the penalty term is the kullback^liebler diver - gence between the true posterior distribution and the distribution produced by the recognition connections .
the penalized log likelihood acts as a lower bound on the log likelihood of the data and the eect of learning is to improve this lower bound .
in attempting to raise the bound , the learning tries to adjust the generative model so that the true posterior distribution is as close as possible to the distribution actually computed by the
the recognition weights are learned by introducing a sleep phase in which the generative model is run top - down to produce fantasy data .
the network knows the true hidden causes of the fantasy data and the recogni - tion connections are adjusted to maximize likelihood of recovering these causes .
this is just a simple application of the delta rule where the learning signal is obtained by comparing the probability that the recognition connections would turn a unit on with the state it actually had when the fantasy data was gener -
the attractive properties of the wake^sleep algo - rithm are that the perceptual inference is simple and fast , and the learning rules for the generative and recognition weights are simple and entirely local .
unfortunately it has some serious disadvantages .
( i ) the recognition process does not do correct prob - abilistic inference based on the generative model .
it does not handle explaining away in a principled manner and it does not allow for top - down eects in
( ii ) the sleep phase of the learning algorithm only approximately follows the gradient of the penalized
( iii ) continuous quantities such as intensities , dis - tances or orientations have to be represented using stochastic binary neurones , which is inecient .
( iv ) although the learning algorithm works reason - for some tasks , considerable parameter - tweaking is necessary to get it to produce easily interpretable hidden units on toy tasks such as the one described in 123
on other tasks , such as the one described in 123 , it consistently fails to capture obvious hidden structure .
this is probably because of its inabil - ity to handle explaining away correctly .
r ect i f i ed gaus si a n be l i e f n ets
we now describe a new model called the rectied gaussian belief net ( rgbn ) which seems to work much better than the wake^sleep algorithm .
the rgbn uses units with states that are either positive real values or zero , so it can represent real - valued latent variables directly .
its main disadvantage is that the recognition process involves gibbs sampling which could be very time consuming .
in practice , however , ten to 123 samples per unit have proved adequate for some small but interesting tasks .
we rst describe the rgbn without considering neural plausibility .
then we show how lateral inter - actions within a layer can be used to perform explaining away correctly .
this makes the rgbn far
generative models for discovering sparse distributed representations g .
hinton and z .
ghahramani 123
figure 123
( a ) schematic of the posterior density of an unrec - tied state of a unit .
( b ) bottom - up and top - down energy functions corresponding to ( a ) .
where h is an index over all the units in the same layer as j including j itself , so yj inuences the right - hand side of equation ( 123 ) via ( cid : 123 ) yj ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) max ( yj , 123 ) .
terms that do not depend on yj have been omitted from equation ( 123 ) .
for values of yj below zero there is a quadratic energy function which leads to a gaussian posterior distribution .
the same is true for values of yj above zero , but it is a dierent quadratic ( gure 123b ) .
the gaussian posterior distributions corresponding to the two quadratics must agree at yj ( cid : 123 ) 123 ( gure 123a ) .
because the posterior distribution is piecewise gaussian it is possible to perform gibbs sampling exactly and fairly eciently ( see the appendix ) .
( b ) learning the parameters of a rgbn
given samples from the posterior distribution , the generative weights of a rgbn can be learned by using the on - line delta rule to perform gradient ascent in the log likelihood of the data :
( cid : 123 ) gji ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) yj ( cid : 123 ) ( cid : 123 ) ( yi ( cid : 123 ) ^yi ) .
the variance of the local gaussian noise of each unit , j , can be learned by an on - line rule : j ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( yj ( cid : 123 ) ^yj ) 123 ( cid : 123 ) ( cid : 123 ) 123
j can be xed at one for all hidden units and the eective local noise level can be controlled by scaling the generative weights .
( c ) the role of lateral connections
lee & seung ( 123 ) introduced a clever trick in which lateral connections are used to handle explaining away eects .
the trick is most easily understood for a linear generative model of the type used in factor analysis .
one contribution , ebelow , to the energy of the state of the network is the squared dierences between the states of the units in the bottom layer , yj , and the top - down expectations , ^yj generated by the states of units in the layer above .
another contribution , eabove , is the squared dierence between the states in the top layer , yk , and their top - down expectations , ^yk .
assuming the local noise models for the lower layer units all have unit variance , and ignoring biases and constant terms that are unaected by the states of the units
figure 123
the rectied gaussian .
more plausible as a neural model and leads to a very natural explanation for the prevalence of topographic maps in cortex .
the generative model for rgbns consists of multiple layers of units each of which has a real - valued unrecti - ed state , yj , and a rectied state , ( cid : 123 ) yj ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) max ( yj , 123 ) .
this rectication is the only nonlinearity in the network .
the value of yj is gaussian distributed with a standard deviation ( cid : 123 ) j and mean , ^yj that is determined by the generative bias , g123j , and the combined eects of the rectied states of units , k , in the layer above :
^yj ( cid : 123 ) g123j ( cid : 123 ) p
given the states of its parents , the rectied state ( cid : 123 ) yj ( cid : 123 ) ( cid : 123 ) therefore has a gaussian distribution above zero , but all of the mass of the gaussian that falls below zero is concentrated in an innitely dense spike at zero as shown in gure 123
this innite density creates problems if we attempt to use gibbs sampling over the rectied states , so we perform gibbs sampling on the unrectied
( a ) sampling from the posterior distribution in an
consider a unit , j , in some intermediate layer of a multilayer rgbn ( gure 123 ) .
suppose that we x the unrectied states of all the other units in the net .
( actu - ally it is only necessary to x the unrectied states of units in the layer above that send a generative connec - tion to j , units in the layer below to which j sends a generative connection , and units in the same layer that send generative connections to units directly aected by j . ) to perform gibbs sampling , we need to stochastically select a value for yj according to its posterior distribu - tion given the unrectied states of all the other units .
if we think in terms of energies that correspond to negative log probabilities , the rectied states of the units in the layer above contribute a quadratic energy term by determining ^yj .
the unrectied states of units , i , in the layer below contribute nothing if ( cid : 123 ) yj ( cid : 123 ) ( cid : 123 ) is 123 , and if ( cid : 123 ) yj ( cid : 123 ) ( cid : 123 ) is positive they each contribute a quadratic term because of the eect of ( cid : 123 ) yj ( cid : 123 ) ( cid : 123 ) on ^yi ,
e ( yj ) ( cid : 123 ) ( yj ( cid : 123 ) ^yj ) 123
( yi ( cid : 123 ) ( cid : 123 ) h ( cid : 123 ) yh ( cid : 123 ) ( cid : 123 ) ghi ) 123
b ( 123 )
123 g .
hinton and z .
ghahramani generative models for discovering sparse distributed representations
figure 123
a network that , in addition to the generative connections , has bottom - up and lateral connections for performing perceptual inference .
this expression can be rearranged to give
( yj ( cid : 123 ) ^yj ) 123 ( cid : 123 ) p ( yj ( cid : 123 ) ( cid : 123 ) setting rjk ( cid : 123 ) gkj and mkl ( cid : 123 ) ( cid : 123 ) p
j ( cid : 123 ) 123
j ( cid : 123 ) 123
yk yl ( ( cid : 123 ) ( cid : 123 ) jgkjglj ) .
jgkjglj , we get
the way in which ebelow depends on each activity in the layer above , yk , is determined by the second and third terms in equation ( 123 ) .
so if unit k computes ( cid : 123 ) j yjrjk using the bottom - up recognition connections , and ( cid : 123 ) l ylmkl using the lateral connections it has all of the information it requires about ebelow to perform function ebelow + gibbs sampling in the potential eabove ( gure 123 ) .
if we are willing to use gibbs sampling , seungs trick allows a proper implementation of factor analysis in a neural network because it makes it possible to sample from the full covariance posterior distribution in the hidden state space .
seungs trick can also be used in a rgbn and it eliminates the most neurally implausible aspect of this model which is that a unit in one layer appears to need to send both its state y and the top - its state ^y to units in the layer down prediction of above .
using the lateral connections , the units in the layer above can , in eect , compute all they need to know about the top - down predictions .
the actual computation that needs to take place inside the unit is non - trivial , but the communication with other units is
there is one remaining diculty that is a conse - quence of our decision to perform gibbs sampling on the unrectied states .
a unit needs to send its unrecti - ed state to units in the layer above and its rectied state to units in the layer below .
currently , we do not know how to x this diculty .
( d ) learning the lateral and recognition
in computer simulations , we can simply set each lateral connection mkl to be ( cid : 123 ) ( cid : 123 ) jgkjglj .
the same eect
b ( 123 )
can be achieved in a more biologically plausible way .
suppose units in one layer are driven by independent , unit - variance gaussian noise and are allowed to drive units in the layer above using recognition weights that are equal to the generative weights .
the covariance of the states yk and yl of two units in the layer above will be equal to ( cid : 123 ) jgkjglj .
the lateral interaction can then be learned by a simple anti - hebbian rule : ( cid : 123 ) mkl ( cid : 123 ) ( cid : 123 ) ( ( cid : 123 ) mkl ( cid : 123 ) yk yl ) .
a similar approach can be used to set rjk equal to gkj .
if units in one layer are driven by independent , unit - variance gaussian noise and their generative weights are used to drive units in the layer below , then the covariance of yk and yi will equal gkj and hebbian learning can be used to set rjk :
( cid : 123 ) rjk ( cid : 123 ) ( cid : 123 ) ( ( cid : 123 ) rjk ( cid : 123 ) yj yk ) .
slightly more complicated rules are needed if states cannot be negative and there are probably many other relatively simple ways of achieving the same end .
the point of presenting this simple rule is to show that it is not a major problem to learn the appropriate lateral and recognition connections because they are related to the generative weights in such a simple way .
( e ) a reason for topographic maps
it is infeasible to interconnect all pairs of units in a cortical area .
if we assume that direct lateral inter - actions ( or interactions mediated by interneurones ) are primarily local , then widely separated units will not have the connections required for explaining away .
consequently the computation of the posterior distri - bution will be incorrect unless the generative weight vectors of widely separated units are orthogonal .
if the generative weights are constrained to be positive , the only way two vectors can be orthogonal is for one to have zeros where the other has non - zeros .
it follows that widely separated units must attend to dierent parts of the image and units can only attend to overlap - ping patches if they are laterally interconnected .
we have not described a mechanism for the formation of topographic maps , but we have given a good computa - tional reason for their existence .
r esu lts on a toy ta sk
a simple problem that illustrates the need for sparse distributed representations is the noisy bars problem ( hinton et al .
consider the following multistage for k123k images .
the top level decides with equal probabilities whether the image will consist solely of vertical or horizontal bars .
given this choice , the second level decides independently for each of the k bars of the appropriate orientation whether it is present or not , with a probability of 123 of being present .
if a bar is present , its intensity is determined by a uniformly distributed random variable .
finally , independent gaussian noise is added to each pixel in the image .
sample images generated from this process are shown in gure 123a .
generative models for discovering sparse distributed representations g .
hinton and z .
ghahramani 123
we trained a three - layer rgbn consisting of 123 visible units , 123 units in the rst hidden layer and one unit in the second hidden layer on the 123 bars problem .
while there are 123 combinations of possible bars ( not accounting for the real - valued intensities and gaussian noise ) , a distributed representation with only 123 hidden units in the rst layer can capture the presence or absence of each bar .
with this representa - tion in the rst hidden layer , the second hidden layer can then capture higher - order structure by detecting that vertical bars are correlated with other vertical bars and not with horizontal bars .
the network was trained for ten passes through a data set of 123 images using a dierent , random order for each pass .
for each image we used 123 itera - tions of gibbs sampling to approximate the posterior distribution over hidden states .
each iteration consisted of sampling every hidden unit once in a xed order .
the states on every other iteration were used for learning , with a learning rate of 123 and a weight decay parameter of 123 .
since the top level of the generative process makes a discrete decision between vertical and horizontal bars , we tried both the rgbn and a trivial extension of the rgbn in which the top level unit saturates both at zero and one .
this resulted in slightly cleaner representations at the top level .
results were relatively insensitive to other parametric changes .
after learning , each of the 123 possible bars is repre - sented by a separate unit in the rst hidden layer ( gure 123c ) .
the remaining hidden units in that layer inactive through strong inhibitory biases ( gure 123b ) .
the unit in the top hidden layer strongly excites the vertical bar units in the rst hidden layer , and inhibits the horizontal bar units .
indeed , when presented with images and allowed to randomly sample its states for ten gibbs samples , the top unit is active for 123% of novel images containing vertical bars and inactive for 123% of images containing hori - zontal bars .
a random sample of images produced by generating from the model after learning is shown in
as a control , it is interesting to examine the results produced by a mixture of gaussians and a factor analyser trained on the same data .
a factor analyser with 123 hidden units discovers global both excitatory and inhibitory components ( gure 123a ) .
the representation is distributed , but not sparse .
in contrast , the mixture of gaussians discovers 123 good prototypes for the images ( gure 123b ) .
while some single bar images are represented in the hidden units , others represent frequent combinations of bars .
more importantly , in order to capture all 123 combinations of possible bar locations the mixture of gaussians network would need 123 hidden units .
the representa - tion is sparse but not distributed .
figure 123 ( a ) sample data from the 123 noisy bars problem .
( b ) sample outputs generated by the model after learning .
b ( 123 )
di scov er i ng de p t h i n si m pl i f i ed st er eo pa i rs
another problem in which discovering the higher - order structure of a dataset has presented diculties for some previous unsupervised learning algorithms is
123 g .
hinton and z .
ghahramani generative models for discovering sparse distributed representations
figure 123
generative weights of a three - layered rgbn after being trained on the noisy bars problem .
( a ) weights from the top - layer hidden unit to the 123 middle - layer hidden units .
( b ) biases of the middle - layer hidden units .
( c ) weights from the hidden units to the 123 visible array , arranged in the same manner as in ( a ) .
the one - dimensional stereo disparity problem ( becker & hinton 123 ) .
we tested the rgbn on a version of this problem with the following generative process .
random dots of uniformly distributed intensities are scattered sparsely on a one - dimensional surface , and the image is blurred with a gaussian lter .
this surface is then randomly placed at one of two dierent depths , giving rise to two possible left - to - right dispari - ties between the images seen by each eye .
separate gaussian noise is then added to the image seen by each eye .
eight example images generated in this manner are shown in gure 123a .
using the very same architecture and training para - meters as in the previous example , we trained an rgbn on images from this stereo disparity problem .
as in the previous example , each of the 123 hidden units in the rst hidden layer was connected to the entire array of 123 visible units , i . e .
it had inputs from both eyes .
twelve of these hidden units learned to become local left - disparity detectors , while the other twelve became local right - disparity detectors ( gure 123c ) .
unlike the previous problem , in which there were too many hidden units for the problem , here there were too few for the 123 pixel locations .
the unit in the second hidden layer has positive weights connecting it to leftward disparity detecting hidden units in the layer below , and negative weights for the rightward units ( gure 123a ) .
when presented with novel input images the top unit is active for 123% of images with leftward disparity and inactive for 123% of images with rightward disparity .
a random sample images generated by the model after learning is shown in gure 123b .
b ( 123 )
di scus sion
the units used in an rgbn have a number of dierent properties and it is interesting to ask which of these properties are essential and which are arbitrary .
if we want to achieve neural plausibility by using lateral interactions to handle explaining away , it is essential that gaussian noise is used to convert ^yj into yj in the generative model .
without this , the expression for ebelow in equation ( 123 ) would not be quadratic and it would not be possible to take the summation over j inside the summation over k in equation ( 123 ) .
in the rgbn , there are two linear regimes .
either ( cid : 123 ) y ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) y or ( cid : 123 ) y ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) 123 depending on the value of y .
clearly , the idea can be generalized to any number of regimes and the only constraint on each regime is that it should be linear so that exact gibbs sampling is possible .
( by using a few linear regimes we can crudely approximate units whose output is a smooth nonlinear function of y ( frey 123 ) and still perform exact gibbs sampling . ) if we use two constant regimes and replace ( cid : 123 ) y ( cid : 123 ) ( cid : 123 ) by a binary output s which is one when y is positive and zero otherwise we get a probit belief net that is very similar to the logistic belief net described in 123 but has the advantage that the lateral connection trick can be used for perceptual inference .
probit units and linear or rectied linear units can easily be combined in the same network by making the output of a unit of one type contribute to the top - down expectation , ^y of a unit of the other type .
they can also be combined in a more interesting way .
the discrete output of a probit unit si can multiply the output of a linear unit yj and exact gibbs sampling is
generative models for discovering sparse distributed representations g .
hinton and z .
ghahramani 123
figure 123
( a ) sample data from the simplied stereo disparity problem .
the top and bottom row of each 123 image are the inputs to the left and right eye , respectively .
notice that the high pixel noise makes it dicult to infer the disparity in some images .
( b ) sample outputs generated by the model after learning .
feasible .
this allows the probit unit to decide should be used without inuencing the value of yj if it is used .
this is useful if , for example , yj represents the size of an object and si represents whether it exists .
if a probit unit and a linear unit share the same y value their combination is exactly a rectied linear unit .
if they merely share the same ^y value but use inde - pendent local noise to get dierent y values , we get a softer blending of the linear and the constant regime .
figure 123
( a ) generative weights of a factor analyser with 123 hidden units trained on the same data as the rgbn .
( b ) generative weights of a mixture of 123 gaussians trained on the same data .
b ( 123 )
123 g .
hinton and z .
ghahramani generative models for discovering sparse distributed representations
it remains to be seen how rgbns fare on larger , more realistic datasets .
we hope that they will be able to discover many dierent about properties like surface depth and surface orienta - tion in natural images and that their method of inference will combine these dierent sources correctly when interpreting a single
it is possible that the number of iterations of gibbs sampling required will increase signicantly with the size of the input and the number of layers .
this would certainly happen if interpreting an image was a typical combinatorial optimization problem in which the best solution to one part of the problem considered in isola - tion is usually incompatible with the best solution to another part of the problem .
this is called a frustrated system and is just what vision is not like .
it is generally easier to interpret two neighbouring patches of an image than to interpret one patch in isolation because imagine two separate networks , one for each image patch .
when we interconnect should settle faster , not slower .
simulations will demon - strate whether this conjecture is correct .
an interesting way to reduce the time required for gibbs sampling is to initialize the state of the network to an interpretation of the data that is approximately correct .
for data that has temporal coherence this could be done by using a predictive causal model for initialization .
for data that lack temporal coherence it is still possible to initialize the network sensibly by learning a separate set of bottom - up connection strengths which are used in a single pass for initializa - tion .
these connection strengths can be learned using the delta rule , where the results of gibbs sampling dene the desired initial states .
the initialization connections save time by caching an approximation to the results of gibbs sampling on previous , similar data .
we thank peter dayan , brendan frey , georey goodhill , michael jordan , david mackay , radford neal and mike revow for numerous insights and david mackay for greatly improving the manuscript .
the research was funded by grants from the canadian natural science and engineering research council and the ontario information technology research center .
is the nesbitt - burns fellow of the canadian institute for advanced research .
a ppen di x 123
deta i ls of gi bbs sa m pl i ng
to perform gibbs sampling in a rectied gaussian belief net we need to select stochastically a value for the unrectied state yj of each hidden unit according to its probability density given the unrectied states of all the other units .
for simplicity , we will call this conditional probability density p ( yj ) .
the energy corresponding to p ( yj ) is given by equation ( 123 ) , which can be decomposed into two dierent quadratic energy terms associated with negative and positive values of yj :
e ( yjj yj 123 123 ) ( cid : 123 ) ( yj ( cid : 123 ) ^yj ) 123
figure 123
generative weights of a three - layered rgbn after being trained on the stereo disparity problem .
( a ) weights from the top - layer hidden unit to the 123 middle - layer hidden units .
( b ) biases of the middle - layer hidden units .
( c ) weights from the hidden units to the 123
it is also feasible to combine a generalization of the probit unit that uses its y value to pick deterministically one of m possibilities with a generalization of the linear unit that has m dierent linear regimes .
this is a generative version of the mixture of experts model ( jacobs et al .
the rgbn is a particularly interesting case because the innite density of ( cid : 123 ) y ( cid : 123 ) ( cid : 123 ) at zero means that it is very cheap , in coding terms , for units to have outputs of zero , so the network develops sparse representations .
each hidden unit can be viewed as a linear basis func - tion for representing the states in the layer below , but only a subset of these basis functions are used for a given data point .
because the network can select which basis functions are appropriate for the data , it can tailor a basis function to a rare , complex feature without incurring the cost of representing the projection on to this basis function for every single data point .
other ( olshausen & field 123; lee & seung 123 ) rely on generative models that have a non - gaussian local noise model for the hidden units , so the lateral connec - tion trick does not work when they are generalized to multiple hidden layers .
b ( 123 )
generative models for discovering sparse distributed representations g .
hinton and z .
ghahramani 123
e ( yjj yj > 123 ) ( cid : 123 ) ( yj ( cid : 123 ) ^yj ) 123
( yi ( cid : 123 ) yjgji ( cid : 123 ) ( cid : 123 ) h123 ( cid : 123 ) j ( cid : 123 ) yh ( cid : 123 ) ( cid : 123 ) ghi ) 123
where c is a constant that ensures that the two energy terms are equal at yj ( cid : 123 ) 123
equation ( 123 ) can be re - arranged as a quadratic in yj :
( cid : 123 ) c 123 ,
e ( yjj yj > 123 ) ( cid : 123 ) ( yj ( cid : 123 ) ( cid : 123 ) j ) 123
( cid : 123 ) j ( cid : 123 ) ( cid : 123 ) 123
( yi ( cid : 123 ) ( cid : 123 ) h123 ( cid : 123 ) j ( cid : 123 ) yh ( cid : 123 ) ( cid : 123 ) ghi ) gji
we will refer to the gaussian with mean ^yj and j , which denes the density for negative values of yj , as gn and the gaussian with mean ( cid : 123 ) j and j corresponding to positive values of yj as gp .
we now describe two methods of producing exact samples from p ( yj ) assuming that we have primitives that can sample from gaussian , binomial and exponen - tial random variables ( see devroye ( 123 ) for a review of basic sampling methods ) .
associated with each method are also heuristics for selecting when the method is applicable and ecient given particular values of ^yj , ( cid : 123 ) j , ( cid : 123 ) j and ( cid : 123 ) j .
( a ) method i
^yj < 123 and ( cid : 123 ) j > 123 then p ( yj ) is bimodal : gn has most of its mass below zero and gp has most of its mass above zero ( as in the example shown in gure 123 ) .
in this case , we can use a procedure based on the idea of rejec - tion sampling from a mixture of gn and gp , which is
( i ) compute the densities of gn and gp at zero .
( ii ) sample from a mixture of gn and gp where the mixing proportions are given by mn ( cid : 123 ) gp ( 123 ) = ( gn ( 123 ) ( cid : 123 ) gp ( 123 ) ) and mp ( cid : 123 ) 123 ( cid : 123 ) mn .
( iii ) reject the sample and go back to step ( ii ) if the sample came from gn and was positive or if it came from gp and was negative; otherwise , accept the sample and terminate .
since the probability of rejecting a sample is less than 123 , the mean time for this procedure to produce an accepted sample from p ( yj ) is at most two steps .
( b ) method ii
if ^yj > 123 or ( cid : 123 ) j < 123 then it becomes necessary to sample from the tail of gn , gp or both , and the above procedure may be very inecient .
the following is a more ecient procedure which can be used in this case .
( i ) compute the mass of gn below 123 , weighted by the
mixing proportion as previously dened
b ( 123 )
mn ( cid : 123 ) mn
and similarly for the mass of gp above zero .
( of course , this integral cannot be solved analytically and will require a call to the erf function . ) ( ii ) with probability mn= ( mn ( cid : 123 ) mp ) stochastically decide to sample from the negative side of gn , otherwise select the positive side of gp .
call this selection g and the side we want to sample from the correct side of g .
( iii ) if the correct side of g has a substantial prob - ability mass , for example , if g ( cid : 123 ) gn and ^yj= ( cid : 123 ) j <123 / 123 , then sample from g repeatedly , accepting the rst sam - ple that comes from the correct side of g .
selected in step ( ii ) ,
( iv ) if the correct side of g does not have a substan - tial probability mass , that is , it is the tail of a gaussian , then we upper bound it by an exponential distribution and again use rejection sampling .
assum - ing gp was let f ( y ) be an exponential density in y : f ( y ) ( cid : 123 ) ( 123= ( cid : 123 ) ) e ( cid : 123 ) y= ( cid : 123 ) , with decay constant ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) 123 j = ( cid : 123 ) j , chosen to match the decay of the gaussian tail at zero .
( to sample from gn we simply reverse the sign of y and dene ( cid : 123 ) in terms of ( cid : 123 ) j and ^yj . ) sample y from f ( y ) until y is accepted , where the acceptance probability is g ( y ) f ( 123 ) / f ( y ) g ( 123 ) .
this acceptance probability is obtained by scaling the gaus - sian tail to match the exponential at y ( cid : 123 ) 123 , and then computing the ratio of the scaled gaussian tail at y to the exponential at y .
the condition in step ( iii ) of this method ensures that the mean time to produce an accepted sample from p ( yj ) will be at most about three steps .
for step ( iv ) the quality of the exponential bound ( and therefore the mean number of samples until acceptance ) depends on how far in the tail of the gaussian we are sampling .
for a tail starting from 123 / 123 standard devia - tions from the mean of the gaussian , the acceptance probability for the exponential approximation is on average about 123 , and this probability increases for gaussian tails further from the mean .
finally , we should point out that these are just some of the methods that can be used to sample from p ( yj ) .
implementations using other sampling methods , such as adaptive rejection sampling ( gilks & wild 123 ) , are also possible .
r e f er enc es barlow , h .
123 unsupervised learning .
neural comput
becker , s .
& hinton , g .
123 a self - organizing neural network that discovers surfaces in random - dot stereograms .
nature 123 , 123^123
bishop , c .
m . , svensen , m .
& williams , c .
123 gtm : a principled alternative to the self - organizing map .
neural comput .
( in the press . )
dempster , a . , laird , n .
& rubin , d .
123 maximum likeli - hood from incomplete data via the em algorithm .
statist .
b 123 , 123^123
devroye , l .
123 non - uniform random variate generation
durbin , r .
& willshaw , d .
123 an analogue approach to the travelling salesman problem using an elastic net method .
nature 123 , 123^123
123 g .
hinton and z .
ghahramani generative models for discovering sparse distributed representations
everitt , b .
123 an introduction to latent variable models .
kohonen , t .
123 self - organized formation of topologically
correct feature maps .
cybern .
123 , 123^123
lee , d .
& seung , h .
123 unsupervised learning by convex and conic coding .
in advances in neural information processing systems 123 ( ed .
mozer , m .
jordan & t .
petsche ) .
cambridge , ma : mit press .
lewicki , m .
& sejnowski , t .
123 bayesian unsupervised learning of higher order structure .
in advances in neural infor - mation processing systems 123 ( ed .
mozer , m .
jordan & t .
petsche ) .
cambridge , ma : mit press .
mumford , d .
123 neuronal architectures for pattern - theo - retic problems .
in large - scale neuroneal theories of the brain ( ed .
koch & j .
davis ) , pp . 123^123
cambridge , ma :
neal , r .
123 connectionist learning of belief networks .
intell .
123 , 123^123
neal , r .
& dayan , p .
123 factor analysis using delta - rule wake^sleep learning .
technical report no .
123 , dept .
of statistics , university of toronto , canada .
olshausen , b .
& field , d .
123 emergence of simple - cell receptive eld properties by learning a sparse code for natural images .
nature 123 , 123^123
pearl , j .
123 probabilistic reasoning in intelligent systems : networks
of plausible inference .
san mateo , ca : morgan kaufmann .
rumelhart , d .
& zipser , d .
123 feature discovery by
competitive learning .
123 , 123^123
rumelhart , d .
e . , hinton , g .
& williams , r .
123 learning internal representations by back - propagating errors .
nature 123 , 123^123
london : chapman & hall .
frey , b .
123 continuous sigmoidal belief networks trained using slice sampling .
in advances in neural information processing systems123 ( ed .
mozer , m .
jordan & t .
petsche ) .
cambridge ,
geman , s .
& geman , d .
123 stochastic relaxation , gibbs distributions , and the bayesian restoration of images .
ieee trans .
on pattern analysis and machine intelligence 123 , 123^123
gilks , w .
& wild , p .
123 adaptive rejection sampling for
gibbs sampling .
statist .
123 , 123^123
gregory , r .
123 the intelligent eye .
london : wiedenfeld &
hinton , g .
& sejnowski , t .
123 learning and relearning in boltzmann machines .
in parallel distributed processing : explorations in the microstructure of cognition .
123 : foundations ( ed .
rumelhart & j .
mcclelland ) .
cambridge , ma : mit press .
hinton , g .
& sejnowski , t .
123 optimal perceptual inference .
in proc .
of the ieee computer society conf .
on and pattern recognition , pp .
123^123
hinton , g .
e . , dayan , p . , frey , b .
& neal , r .
123 the wake^sleep algorithm for unsupervised neural networks .
science 123 , 123^123
horn , b .
123 understanding image intensities .
artif .
intell .
123 , 123^123
jacobs , r .
a . , jordan , m .
i . , nowlan , s .
& hinton , g .
e . 123 adaptive mixture of local experts .
neural comput .
123 , 123^123
b ( 123 )
