galakis et al .
( 123 ) made a similar reintroduction and extension in the speech processing community .
once again we mention the book by elliott et al .
( 123 ) , which also covers learning in this context .
the basis of all the learning algorithms presented by these authors is the powerful em algorithm ( baum & petrie , 123; dempster , laird , & ru - bin , 123 ) .
the objective of the algorithm is to maximize the likelihood of the observed data ( equation 123 ) in the presence of hidden variables .
let us denote the observed data by y = ( y123 , .
, y ) , the hidden variables by x = ( x123 , .
, x ) , and the parameters of the model by .
maximizing the likelihood as a function of is equivalent to maximizing the log - likelihood :
l ( ) = log p ( y| ) = log
p ( x , y| ) dx .
using any distribution q over the hidden variables , we can obtain a lower bound on l :
p ( y , x| ) dx = log = f ( q , ) ,
q ( x ) p ( x , y| ) p ( x , y| )
q ( x ) log p ( x , y| ) dx
q ( x ) log q ( x ) dx ( 123c )
where the middle inequality is known as jensens inequality and can be proved using the concavity of the log function .
if we dene the energy of a global conguration ( x , y ) to be log p ( x , y| ) , then some readers may notice that the lower bound f ( q , ) l ( ) is the negative of a quantity known in statistical physics as the free energy : the expected energy un - der q minus the entropy of q ( neal & hinton , 123 ) .
the em algorithm alternates between maximizing f with respect to the distribution q and the parameters , respectively , holding the other xed .
starting from some initial parameters 123 :
qk+123 arg max
f ( q , k )
a unifying review of linear gaussian models f ( qk+123 , ) .
k+123 arg max
it is easy to show that the maximum in the e - step results when q is exactly the conditional distribution of x : qk+123 ( x ) = p ( x|y , k ) , at which point the bound becomes an equality : f ( qk+123 , k ) = l ( k ) .
the maximum in the m - step is obtained by maximizing the rst term in equation 123c , since the entropy of q does not depend on : k+123 arg max
p ( x|y , k ) log p ( x , y| ) dx .
this is the expression most often associated with the em algorithm , but it obscures the elegant interpretation of em as coordinate ascent in f ( neal & hinton , 123 ) .
since f = l at the beginning of each m - step and since the e - step does not change , we are guaranteed not to decrease the likelihood after each combined em - step .
therefore , at the heart of the em learning procedure is the following idea : use the solutions to the ltering and smoothing problem to estimate the unknown hidden states given the observations and the current model parameters .
then use this ctitious complete data to solve for new model parameters .
given the estimated states obtained from the inference algo - rithm , it is usually easy to solve for new parameters .
for linear gaussian models , this typically involves minimizing quadratic forms such as equa - tion 123 , which can be done with linear regression .
this process is repeated using these new model parameters to infer the hidden states again , and so on .
we shall review the details of particular algorithms as we present the various cases; however , we now touch on one general point that often causes confusion .
our goal is to maximize the total likelihood ( see equation 123 ) ( or equivalently maximize the total log likelihood ) of the observed data with respect to the model parameters .
this means integrating ( or summing ) over all ways in which the generative model could have produced the data .
as a consequence of using the em algorithm to do this maximization , we nd ourselves needing to compute ( and maximize ) the expected log - likelihood of the joint data , where the expectation is taken over the distribution of hidden values predicted by the current model parameters and the observa - tions .
thus , it appears that we are maximizing the incorrect quantity , but doing so is in fact guaranteed to increase ( or keep the same ) the quantity of interest at each iteration of the algorithm .
123 continuous - state linear gaussian systems
having described the basic model and learning procedure , we now focus on specic linear instances of the model in which the hidden state variable x is continuous and the noise processes are gaussian .
this will allow us to
sam roweis and zoubin ghahramani
elucidate the relationship among factor analysis , pca , and kalman lter models .
we divide our discussion into models that generate static data and those that generate dynamic data .
static data have no temporal dependence; no information would be lost by permuting the ordering of the data points yt; whereas for dynamic data , the time ordering of the data points is crucial .
123 static data modeling : factor analysis , spca , and pca .
in many situations we have reason to believe ( or at least to assume ) that each point in our data set was generated independently and identically .
in other words , there is no natural ( temporal ) ordering to the data points; they merely form a collection .
in such cases , we assume that the underlying state vector x has no dynamics; the matrix a is the zero matrix , and therefore x is simply a constant ( which we take without loss of generality to be the zero vector ) corrupted by noise .
the new generative model then becomes :
a = 123 x = w
y = cx + v
w n ( 123 , q ) v n ( 123 , r ) .
notice that since xt is driven only by the noise w and since yt depends only on xt , all temporal dependence has disappeared .
this is the motivation for the term static and for the notations x and y above .
we also no longer use a separate distribution for the initial state : x123 x w n ( 123 , q ) .
tion 123 to obtain the marginal distribution of y , which is the gaussian ,
this model is illustrated in figure 123
we can analytically integrate equa -
123 , cqct + r
two things are important to notice .
first , the degeneracy mentioned above persists between the structure in q and c . 123 this means there is no loss of generality in restricting q to be diagonal .
furthermore , there is ar - bitrary sharing of scale between a diagonal q and c .
typically we either restrict the columns of c to be unit vectors or make q the identity matrix to resolve this degeneracy .
in what follows we will assume q = i without loss of generality .
second , the covariance matrix r of the observation noise must be re - stricted in some way for the model to capture any interesting or informative projections in the state x .
if r were not restricted , learning could simply choose c = 123 and then set r to be the sample covariance of the data , thus trivially achieving the maximum likelihood model by explaining all of the
123 if we diagonalize q and rewrite the covariance of y , the degeneracy becomes clear : .
to make q diagonal , we simply replace c with
123 , ( ce123 / 123 ) ( ce123 / 123 ) t + r
a unifying review of linear gaussian models
figure 123 : static generative model ( continuous state ) . the covariance matrix of the input noise w is q and the covariance matrix of the output noise v is r .
in the network model below , the smaller circles represent noise sources and all units are linear .
outgoing weights have only been drawn from one hidden unit .
this model is equivalent to factor analysis , spca and pca models depending on the output noise covariance .
for factor analysis , q = i and r is diagonal .
for spca , q = i and r = i .
for pca , q = i and r = lim123 i .
structure in the data as noise .
( remember that since the model has reduced to a single gaussian distribution for y , we can do no better than having the covariance of our model equal the sample covariance of our data . ) note that restricting r , unlike making q diagonal , does constitute some loss of generality from the original model of equations 123 .
there is an intuitive spatial way to think about this static generative model .
we use white noise to generate a spherical ball ( since q = i ) of density in k - dimensional state - space .
this ball is then stretched and rotated into p - dimensional observation space by the matrix c , where it looks like a k - dimensional pancake .
the pancake is then convolved with the covariance density of v ( described by r ) to get the nal covariance model for y .
we want the resulting ellipsoidal density to be as close as possible to the ellipsoid given by the sample covariance of our data .
if we restrict the shape of the v covariance by constraining r , we can force interesting information to appear in both r and c as a result .
finally , observe that all varieties of ltering and smoothing reduce to the same problem in this static model because there is no time dependence .
we over a single hidden state are seeking only the posterior probability p given the corresponding single observation .
this inference is easily done by
sam roweis and zoubin ghahramani
linear matrix projection , and the resulting density is itself gaussian :
( cid : 123 ) = p ( cid : 123 ) = n ( cid : 123 ) y , i c
= n ( cx , r ) |y n ( 123 , i ) |x
123 , cct + r = ct ( cct + r ) 123 ,
from which we obtain not only the expected value y of the unknown state but also an estimate of the uncertainty in this value in the form of the covariance i c .
computing the likelihood of a data point y is merely an evaluation under the gaussian in equation 123 .
the learning problem now consists of identifying the matrices c and r .
there is a family of em algorithms to do this for the various cases discussed below , which are given in detail at the end of this review .
123 factor analysis .
if we restrict the covariance matrix r that controls the observation noise to be diagonal ( in other words , the covariance ellipsoid of v is axis aligned ) and set the state noise q to be the identity matrix , then we recover exactly a standard statistical model known as maximum likelihood factor analysis .
the unknown states x are called the factors in this context; the matrix c is called the factor loading matrix , and the diagonal elements of r are often known as the uniquenesses .
( see everitt , 123 , for a brief and clear introduction . ) the inference calculation is done exactly as in equation 123b .
the learning algorithm for the loading matrix and the uniquenesses is exactly an em algorithm except that we must take care to constrain r properly ( which is as easy as taking the diagonal of the unconstrained maximum likelihood estimate; see rubin & thayer , 123; ghahramani & hinton , 123 ) .
if c is completely free , this procedure is called exploratory factor analysis; if we build a priori zeros into c , it is conrmatory factor analysis .
in exploratory factor analysis , we are trying to model the covariance structure of our data with p + pk k ( k 123 ) / 123 free parameters123 instead of the p ( p + 123 ) / 123 free parameters in a full covariance matrix .
the diagonality of r is the key assumption here .
factor analysis attempts to explain the covariance structure in the observed data by putting all the variance unique to each coordinate in the matrix r and putting all the cor - relation structure into c ( this observation was rst made by lyttkens , 123 , in response to work by wold ) .
in essence , factor analysis considers the axis rotation in which the original data arrived to be special because observation noise ( often called sensor noise ) is independent along the coordinates in these axes .
however , the original scaling of the coordinates is unimportant .
if we were to change the units in which we measured some of the components of y , factor analysis could merely rescale the corresponding entry in r and
123 the correction k ( k 123 ) / 123 comes in because of degeneracy in unitary transformations
of the factors .
see , for example , everitt ( 123 ) .
a unifying review of linear gaussian models
row in c and achieve a new model that assigns the rescaled data identical likelihood .
on the other hand , if we rotate the axes in which we measure the data , we could not easily x things since the noise v is constrained to have axis aligned covariance ( r is diagonal ) .
em for factor analysis has been criticized as being quite slow ( rubin & thayer , 123 ) .
indeed , the standard method for tting a factor analysis model ( joreskog , 123 ) is based on a quasi - newton optimization algorithm ( fletcher & powell , 123 ) , which has been found empirically to converge faster than em .
we present the em algorithm here not because it is the most efcient way of tting a factor analysis model , but because we wish to emphasize that for factor analysis and all the other latent variable models reviewed here , em provides a unied approach to learning .
finally , recent work in online learning has shown that it is possible to derive a family of em - like algorithms with faster convergence rates than the standard em algorithm ( kivinen & warmuth , 123; bauer , koller , & singer , 123 ) .
123 spca and pca .
if instead of restricting r to be merely diagonal , we require it to be a multiple of the identity matrix ( in other words , the covariance ellipsoid of v is spherical ) , then we have a model that we will call sensible principal component analysis ( spca ) ( roweis , 123 ) .
the columns of c span the principal subspace ( the same subspace found by pca ) , and we will call the scalar value on the diagonal of r the global noise level .
note that spca uses 123+ pk k ( k 123 ) / 123 free parameters to model the covariance .
once again , inference is done with equation 123b and learning by the em algorithm ( except that we now take the trace of the maximum likelihood estimate for r to learn the noise level; see ( roweis , 123 ) ) .
unlike factor analysis , spca considers the original axis rotation in which the data arrived to be unimportant : if the measurement coordinate system were rotated , spca could ( left ) multiply c by the same rotation , and the likelihood of the new data would not change .
on the other hand , the original scaling of the coordinates is privileged because spca assumes that the observation noise has the same variance in all directions in the measurement units used for the observed data .
if we were to rescale one of the components of y , the model could not be easily corrected since v has spherical covariance ( r = i ) .
the spca model is very similar to the independently proposed probabilistic principal component analysis ( tipping & bishop , 123 ) .
if we go even further and take the limit r = lim123 i ( while keeping the diagonal elements of q nite ) 123 then we obtain the standard principal component analysis ( pca ) model .
the directions of the columns of c are
123 since isotropic scaling of the data space is arbitrary , we could just as easily take the limit as the diagonal elements of q became innite while holding r nite or take both limits at once .
the idea is that the noise variance becomes innitesimal compared to the scale of the data .
sam roweis and zoubin ghahramani
known as the principal components .
inference now reduces to simple least
( cid : 123 ) |x , = lim y , i c = ( x ( ctc ) 123cty ) .
ct ( cct + i ) 123
since the noise has become innitesimal , the posterior over states collapses to a single point , and the covariance becomes zero .
there is still an em algorithm for learning ( roweis , 123 ) , although it can learn only c .
for pca , we could just diagonalize the sample covariance of the data and take the leading k eigenvectors multiplied by their eigenvalues to be the columns of c .
this approach would give us c in one step but has many problems . 123 the em learning algorithm amounts to an iterative procedure for nding these leading eigenvectors without explicit diagonalization .
an important nal comment is that ( regular ) pca does not dene a proper density model in the observation space , so we cannot ask directly about the likelihood assigned by the model to some data .
we can , however , examine a quantity that is proportional to the negative log - likelihood in the limit of zero noise .
this is the sum squared deviation of each data point from its projection .
it is this cost that the learning algorithm ends up minimizing and is the only available evaluation of how well a pca model ts new data .
this is one of the most critical failings of pca : translating points by arbitrary amounts inside the principal subspace has no effect on the model error .
123 recall that if c is pk with p > k and is rank k , then left multiplication by ct ( cct ) 123 ( which appears not to be well dened because cct is not invertible ) is exactly equivalent to left multiplication by ( ctc ) 123ct .
this is the same as the singular value decomposition idea of dening the inverse of the diagonal singular value matrix as the inverse of an element unless it is zero , in which case it remains zero .
the intuition is that although cct truly is not invertible , the directions along which it is not invertible are exactly those that ct is about to project out .
123 it is computationally very hard to diagonalize or invert large matrices .
it also requires an enormous amount of data to make a large sample covariance matrix full rank .
if we are working with patterns in a large ( thousands ) number of dimensions and want to extract only a few ( tens ) principal components , we cannot naively try to diagonalize the sample covariance of our data .
techniques like the snapshot method ( sirovich , 123 ) attempt to address this but still require the diagonalization of an n n matrix where n is the number of data points .
the em algorithm approach solves all of these problems , requiring no explicit diagonalization whatsoever and the inversion of only a kk matrix .
it is guaranteed to converge to the true principal subspace ( the same subspace spanned by the principal components ) .
empirical experiments ( roweis , 123 ) indicate that it converges in a few iterations , unless the ratio of the leading eigenvalues is near unity .
a unifying review of linear gaussian models
123 time - series modeling : kalman filter models .
we use the term dy - namic data to refer to observation sequences in which the temporal ordering is important .
for such data , we do not want to ignore the state evolution dynamics , which provides the only aspect of the model capable of capturing temporal structure .
systems described by the original dynamic generative model , shown in equations 123a and 123b , are known as linear dynamical systems or kalman lter models and have been extensively investigated by the engineering and control communities for decades .
the emphasis has traditionally been on inference problems : the famous discrete kalman lter ( kalman , 123; kalman & bucy , 123 ) gives an efcient recursive solution to the optimal ltering and likelihood computation problems , while the rts recursions ( rauch , 123; rauch et al . , 123 ) solve the optimal smoothing problem .
learning of unknown model parameters was studied by shum - way and stoffer ( 123 ) ( c known ) and by ghahramani and hinton ( 123a ) and digalakis et al .
( 123 ) ( all parameters unknown ) .
figure 123 illustrates this model , and the appendix gives pseudocode for its implementation .
we can extend our spatial intuition of the static case to this dynamic model .
as before , any point in state - space is surrounded by a ball ( or ovoid ) of density ( described by q ) , which is stretched ( by c ) into a pancake in observation space and then convolved with the observation noise covari - ance ( described by r ) .
however , unlike the static case , in which we always centered our ball of density on the origin in state - space , the center of the state - space ball now ows from time step to time step .
the ow is accord - ing to the eld described by the eigenvalues and eigenvectors of the matrix a .
we move to a new point according to this ow eld; then we center our ball on that point and pick a new state .
from this new state , we again ow to a new point and then apply noise .
if a is the identity matrix ( not the zero matrix ) , then the ow does not move us anywhere , and the state just evolves according to a random walk of the noise set by q .
123 discrete - state linear gaussian models
we now consider a simple modication of the basic continuous state model in which the state at any time takes on one of a nite number of discrete val - ues .
many real - world processes , especially those that have distinct modes of operation , are better modeled by internal states that are not continuous .
( it is also possible to construct models that have a mixed continuous and discrete state . ) the state evolution is still rst - order markovian dynamics , and the observation process is still linear with additive gaussian noise .
the modication involves the use of the winner - take - all nonlinearity wta ( ) , dened such that wta ( x ) for any vector x is a new vector with unity in the position of the largest coordinate of the input and zeros in all other positions .
the discrete - state generative model is now simply :
xt+123 = wta ( axt + wt ) = wta ( axt + w ) yt = cxt + vt = cxt + v
sam roweis and zoubin ghahramani
figure 123 : discrete state generative model for dynamic data .
the wta ( ) block 123 block is a unit delay .
the implements the winner - take - all nonlinearity .
the z covariance matrix of the input noise w is q and the covariance matrix of the output noise v is r .
in the network model below , the smaller circles represent noise sources and the hidden units x have a winner take all behaviour ( indicated by dashed lines ) .
outgoing weights have only been drawn from one hidden unit .
this model is equivalent to a hidden markov model with tied output
where a is no longer known as the state transition matrix ( although we will see that matrix shortly ) .
as before , the k - vector w and p - vector v are temporally white and spatially gaussian distributed noises independent of each other and of x and y .
the initial state x123 is generated in the obvious
x123 = wta ( n ( cid : 123 )
( though we will soon see that without loss of generality q123 can be restricted to be the identity matrix ) .
this discrete state generative model is illustrated in figure 123
123 static data modeling : mixtures of gaussians and vector quanti - zation .
just as in the continuous - state model , we can consider situations in
a unifying review of linear gaussian models
which there is no natural ordering to our data , and so set the matrix a to be the zero matrix .
in this discrete - state case , the generative model becomes :
w n ( , q ) v n ( 123 , r ) .
a = 123 x = wta ( w ) y = cx + v
x = ej
each state x is generated independently123 according to a xed discrete probability histogram controlled by the mean and covariance of w .
specif - is the probability assigned by the gaussian n ( , q ) ically , j = p to the region of k - space in which the jth coordinate is larger than all others .
( here ej is the unit vector along the jth coordinate direction . ) notice that to obtain nonuniform priors j with the wta ( ) nonlinearity , we require a nonzero mean for the noise w .
once the state has been chosen , the cor - responding output y is generated from a gaussian whose mean is the jth column of c and whose covariance is r .
this is exactly the standard mixture of gaussian clusters model except that the covariances of all the clusters are constrained to be the same .
the probabilities j = p to the mixing coefcients of the clusters , and the columns of c are the clus - ter means .
constraining r in various ways corresponds to constraining the shape of the covariance of the clusters .
this model is illustrated in figure 123
x = ej
to compute the likelihood of a data point , we can explicitly perform the sum equivalent to the integral in equation 123 since it contains only k terms ,
( cid : 123 ) = k ( cid : 123 )
x = ej , y
n ( ci , r ) |y p ( x = ei )
n ( ci , r ) |y i ,
( cid : 123 ) = k ( cid : 123 )
where ci denotes the ith column of c .
again , all varieties of inference and ltering are the same , and we are simply seeking the set of discrete probabil - j = 123 , .
in other words , we need to do probabilistic classication .
the problem is easily solved by computing the responsibilities x that each cluster has for the data point y : x = ej , y x = ej n ( ci , r ) |y p ( x = ei )
x = ej|y ( x ) j = n ( cid : 123 ) ( x ) j = p x = ej|y
x = ei , y
( cid : 123 ) = p
x = ej , y
123 as in the continuous static case , we again dispense with any special treatment of
the initial state .
sam roweis and zoubin ghahramani
figure 123 : static generative model ( discrete state ) .
the wta ( ) block implements the winner - take - all nonlinearity .
the covariance matrix of the input noise w is q and the covariance matrix of the output noise v is r .
in the network model below , the smaller circles represent noise sources and the hidden units x have a winner take all behaviour ( indicated by dashed lines ) .
outgoing weights have only been drawn from one hidden unit .
this model is equivalent to a mixture of gaussian clusters with tied covariances r or to vector quantization ( vq ) when r = lim123 i .
n ( ci , r ) |y i
the mean x of the state vector given a data point is exactly the vector of responsibilities for that data point .
this quantity denes the entire posterior distribution of the discrete hidden state given the data point .
as a measure of the randomness or uncertainty in the hidden state , one could evaluate the entropy or normalized entropy123 of the discrete distribution corresponding to x .
although this may seem related to the variance of the posterior in factor analysis , this analogy is deceptive .
since x denes the entire distri - bution , no other variance measure is needed .
learning consists of nding the cluster means ( columns of c ) , the covariance r , and the mixing coef - cients j .
this is easily done with em and corresponds exactly to maximum likelihood competitive learning ( duda & hart , 123; nowlan , 123 ) , except
123 the entropy of the distribution divided by the logarithm of k so that it always lies
between zero and one .
a unifying review of linear gaussian models
that all the clusters share the same covariance .
later we introduce extensions to the model that remove this restriction .
as in the continuous - state case , we can consider the limit as the ob - servation noise becomes innitesimal compared to the scale of the data .
what results is the standard vector quantization model .
the inference ( clas - sication ) problem is now solved by the one - nearest - neighbor rule , using euclidean distance if r is a multiple of the identity matrix , or mahalanobis distance in the unscaled matrix r otherwise .
similarly to pca , since the observation noise has disappeared , the posterior collapses to have all of its mass on one cluster ( the closest ) , and the corresponding uncertainty ( entropy ) becomes zero .
learning with em is equivalent to using a batch version of the k - means algorithm such as that proposed by lloyd ( 123 ) .
as with pca , vector quantization does not dene a proper density in the observation space .
once again , we examine the sum squared deviation of each point from its closest cluster center as a quantity proportional to the likelihood in the limit of zero noise .
batch k - means algorithms minimize this cost in lieu of maximizing a proper likelihood .
xt+123 = ej|xt = ei
123 time - series modeling : hidden markov models .
we return now to the fully dynamic discrete - state model introduced in equations 123a and 123b .
our key observation is that the dynamics described by equation 123a are exactly equivalent to the more traditional discrete markov chain dynamics using a state transition matrix t , where tij = p .
it is easy to see how to compute the equivalent state transition matrix t given a and q above : tij is the probability assigned by the gaussian whose mean is the ith column of a ( and whose covariance is q ) to the region of k - space in which the jth coordinate is larger than all others .
it is also true that for any transition matrix t ( whose rows each sum to unity ) , there exist matrices a and q such that the dynamics are equivalent . 123 similarly , the initial probability mass function for x123 is easily computed from 123 and q123 and for any desired histogram over the states for x123 there exist a 123 and q123 that achieve it .
similar degeneracy exists in this discrete - state model as in the continuous - state model except that it is now between the structure of a and q .
since for any noise covariance q , the means in the columns of a can be chosen to set any equivalent transition probabilities tij , we can without loss of generality restrict q to be the identity matrix and use only the means in the columns
123 although harder to see .
sketch of proof : without loss of generality , always set the covariance to the identity matrix .
next , set the dot product of the mean vector with the k - vector having unity in all positions to be zero since moving along this direction does not change the probabilities .
now there are ( k 123 ) degrees of freedom in the mean and also in the probability model .
set the mean randomly at rst ( except that it has no projection along the all - unity direction ) .
move the mean along a line dened by the constraint that all probabilities but two should remain constant until one of those two probabilities has the desired value .
repeat this until all have been set correctly .
sam roweis and zoubin ghahramani of a to set probabilities .
equivalently , we can restrict q123 = i and use only the mean 123 to set the probabilities for the initial state x123
thus , this generative model is equivalent to a standard hmm except that the emission probability densities are all constrained to have the same co - variance .
likelihood and ltering computations are performed with the so - called forward ( alpha ) recursions , while complete smoothing is done with the forward - backward ( alpha - beta ) recursions .
the em algorithm for learn - ing is exactly the well - known baum - welch reestimation procedure ( baum & petrie , 123; baum et al . , 123 ) .
there is an important and peculiar consequence of discretizing the state that affects the smoothing problem .
the state sequence formed by taking the most probable state of the posterior distribution at each time ( as computed by the forward - backward recursions given the observed data and model parameters ) is not the single state sequence most likely to have produced the observed data .
in fact , the sequence of states obtained by concatenating the states that individually have maximum posterior probability at each time step may have zero probability under the posterior .
this creates the need for separate inference algorithms to nd the single most likely state sequence given the observations .
such algorithms for ltering and smoothing are called viterbi decoding methods ( viterbi , 123 ) .
why was there no need for similar decoding in the continuous - state case ? it turns out that due to the smooth and unimodal nature of the posterior probabilities for individual states in the continuous case ( all posteriors are gaussian ) , the sequence of maximum a posteriori states is exactly the single most likely state trajectory , so the regular kalman lter and rts smoothing recursions sufce .
it is possible ( see , for example , rabiner & juang , 123 ) to learn the discrete - state model parameters based on the results of the viterbi decoding instead of the forward - backward smoothingin other words , to maximize the joint likelihood of the observations and the single most likely state sequence rather than the total likelihood summed over all possible paths through
123 independent component analysis
there has been a great deal of recent interest in the blind source separation problem that attempts to recover a number of source signals from obser - vations resulting from those signals , using only the knowledge that the orig - inal sources are independent .
in the square - linear version of the problem , the observation process is characterized entirely by a square and invertible matrix c .
in other words , there are as many observation streams as sources , and there is no delay , echo , or convolutional distortion .
recent experience has shown the surprising result that for nongaussian distributed sources , this problem can often be solved even with no prior knowledge about the sources or about c .
it is widely believed ( and beginning to be proved theo -
a unifying review of linear gaussian models
retically; see mackay , 123 ) that high kurtosis source distributions are most
we will focus on a modied , but by now classic , version due to bell and sejnowski ( 123 ) and baram and roth ( 123 ) of the original independent component analysis algorithm ( comon , 123 ) .
although bell and sejnowski derived it from an information - maximization perspective , this modied algorithm can also be obtained by dening a particular prior distribution over the components of the vector xt of sources and then deriving a gradient learning rule that maximizes the likelihood of the data yt in the limit of zero output noise ( amari , cichocki , & yang , 123; pearlmutter & parra , 123; mackay , 123 ) .
the algorithm , originally derived for unordered data , has also been extended to modeling time series ( pearlmutter & parra , 123 ) .
we now show that the generative model underlying ica can be obtained by modifying slightly the basic model we have considered thus far .
the modication is to replace the wta ( ) nonlinearity introduced above with a general nonlinearity g ( ) that operates componentwise on its input .
our generative model ( for static data ) then becomes :
x = g ( w ) y = cx + v
w n ( 123 , q ) v n ( 123 , r ) .
the role of the nonlinearity is to convert the gaussian distributed prior for w into a nongaussian prior for x .
without loss of generality , we can set q = i , since any covariance structure in q can be be obtained by a linear transformation of a n ( 123 , i ) random variable , and this linear transformation can be subsumed into the nonlinearity g ( ) .
assuming that the generative nonlinearity g ( ) is invertible and differentiable , any choice of the generative nonlinearity results in a corresponding prior distribution on each source given by the probability density function :
px ( x ) = n ( 123 , 123 ) |g123 ( x )
it is important to distinguish this generative nonlinearity from the non - linearity found in the ica learning rule .
we call this the learning rule non - f ( ) , and clarify the distinction between the two nonlinearities classic ica is dened for square and invertible c in the limit of vanishing noise , r = lim123 i .
under these conditions , the posterior density of x given y is a delta function at x = c 123y , and the ica algorithm can be dened in terms of learning the recognition ( or unmixing ) weights w = c rather than the generative ( mixing ) weights c .
the gradient learning rule to increase the likelihood is
t + f ( wy ) yt ,
sam roweis and zoubin ghahramani where the learning rule nonlinearity f ( ) is the derivative of the implicit log prior : f ( x ) = d log px ( x ) ( mackay , 123 ) .
therefore , any generative non - linearity g ( ) results in a nongaussian prior px ( ) , which in turn results in a nonlinearity f ( ) in the maximum likelihood learning rule .
somewhat frus - tratingly from the generative models perspective , ica is often discussed in terms of the learning rule nonlinearity without any reference to the implicit prior over the sources .
a popular choice for the ica learning rule nonlinearity f ( ) is the tanh ( ) function , which corresponds to a heavy tailed prior over the sources
from equation 123 , we obtain a general relationship between the cumulative distribution function of the prior on the sources , cdfx ( x ) , and of the zero - mean , unit variance noise w ,
cdfx ( g ( w ) ) = cdfw ( w ) = 123
for monotonic g , where erf ( z ) is the error function 123 / relationship can often be solved to obtain an expression for g .
for example , if px ( x ) =
cosh ( x ) , we nd that setting
123 + erf ( w /
g ( w ) = ln
causes the generative model of equations 123 to generate vectors x in which each component is distributed exactly according to 123 / ( cosh ( x ) ) .
this non - linearity is shown in figure 123
ica can be seen either as a linear generative model with nongaussian priors for the hidden variables or as a nonlinear generative model with gaussian priors for the hidden variables .
it is therefore possible to derive an em algorithm for ica , even when the observation noise r is nonzero and there are fewer sources than observations .
the only complication is that the posterior distribution of x given y will be the product of a nongaussian prior and a gaussian likelihood term , which can be difcult to evaluate .
given this posterior , the m step then consists of maximizing the expected log of the joint probability as a function of c and r .
the m - step for c is
c arg max
log p ( x ) + log p ( yi|x , c , r )
where i indexes the data points and ( cid : 123 ) ( cid : 123 ) i denotes expectation with respect to the posterior distribution of x given yi , p ( x|yi , c , r ) .
the rst term does not
a unifying review of linear gaussian models
123 123 123 123 123
figure 123 : the nonlinearity g ( ) from equation 123 which converts a gaussian dis - tributed source w n ( 123 , 123 ) into one distributed as x = g ( w ) 123 / ( cosh ( x ) ) .
depend on c , and the second term is a quadratic in c , so taking derivatives with respect to c , we obtain a linear system of equations that can be solved in the usual manner :
a similar m - step can be derived for r .
since , given x , the generative model is linear , the m - step requires only evaluating the rst and second moments of the posterior distribution of x : ( cid : 123 ) x ( cid : 123 ) i and ( cid : 123 ) xxt ( cid : 123 ) i .
it is not necessary to know anything else about the posterior if its rst two moments can be computed .
these may be computed using gibbs sampling or , for certain source priors , using table lookup or closed - form computation . 123 in particular , moulines et al .
( 123 ) and attias and schreiner ( 123 ) have independently proposed using a gaussian mixture to model the prior for each component of the source , x .
the posterior distribution over x is then also a gaussian mixture , which can be evaluated analytically and used to derive an em algorithm for both the mixing matrix and the source densities .
the only caveat is that the number of gaussian components in the posterior grows exponentially in the number of sources , 123 which limits the applicability of this method to models with only a few sources .
123 in the limit of zero noise , r = 123 , the em updates derived in this manner degenerate to c c and r r .
since this does not decrease the likelihood , it does not contradict the convergence proof for the em algorithm .
however , it also does not increase the likelihood , which might explain why no one uses em to t the standard zero - noise ica model .
123 if each source is modeled as a mixture of k gaussians and there are m sources , then
there are km components in the mixture .
sam roweis and zoubin ghahramani
alternatively , we can compute the posterior distribution of w given y , which is the product of a gaussian prior and a nongaussian likelihood .
again , this may not be easy , and we may wish to resort to gibbs sampling ( geman & geman , 123 ) or other markov chain monte carlo methods ( neal , 123 ) .
another option is to employ a deterministic trick recently used by bishop , svenson , and williams ( 123 ) in the context of the generative topo - graphic map ( gtm ) , which is a probabilistic version of kohonens ( 123 ) self - organized topographic map .
we approximate the gaussian prior via a nite number ( n ) of xed points ( this is the trick ) .
in other words ,
p ( w ) = n ( 123 , i ) p ( w ) = n ( cid : 123 )
where the wjs are a nite sample from n ( 123 , i ) .
the generative model then takes these n points , maps them through a xed nonlinearity g , an adaptable linear mapping c , and adds gaussian noise with covariance r to produce y .
the generative model is therefore a constrained mixture of n gaussians , where the constraint comes from the fact that the only way the centers can move is by varying c .
then , computing the posterior over w amounts to computing the responsibility under each of the n gaussians for the data point .
given these responsibilities , the problem is again linear in c , which means that it can be solved using equation 123 .
for the traditional zero - noise limit of ica , the responsibilities will select the center closest to the data point in exactly the same manner as standard vector quantization .
therefore , ica could potentially be implemented using em for gtms in the limit of zero
123 network interpretations and regularization
early in the modern history of neural networks , it was realized that pca could be implemented using a linear autoencoder network ( baldi & hornik , 123 ) .
the data are fed as both the input and target of the network , and the network parameters are learned using the squared error cost function .
in this section , we show how factor analysis and mixture of gaussian clusters can also be implemented in this manner , albeit with a different cost function .
to understand how a probabilistic model can be learned using an au - toencoder it is very useful to make a recognition - generation decomposition of the autoencoder ( hinton & zemel , 123; hinton , dayan , & revow , 123 ) .
an autoencoder takes an input y , produces some internal representation in the hidden layer x , and generates at its output a reconstruction of the input y in figure 123
we call the mapping from hidden to output layers the generative part of the network since it generates the data from a ( usually more compact ) representation under some noise model .
conversely , we call the mapping from input to hidden units the recognition part of the network
a unifying review of linear gaussian models
figure 123 : a network for state inference and for learning parameters of a static data model .
the input y is clamped to the input units ( bottom ) , and the mean x of the posterior of the estimated state appears on the hidden units above .
the covariance of the state posterior is constant at i c which is easily computed if the weights are known .
the inference computation is a trivial linear projection , but learning the weights of the inference network is difcult .
the input to hidden weights are always constrained to be a function of the hidden to output weights , and the network is trained as an autoencoder using self - supervised learning .
outgoing weights have only been drawn from one input unit and one hidden
since it produces some representation in the hidden variables given the in - put .
because autoencoders are usually assumed to be deterministic , we will think of the recognition network as computing the posterior mean of the hidden variables given the input .
the generative model for factor analysis assumes that both the hidden states and the observables are normally distributed , from which we get the posterior probabilities for the hidden states in equation 123b .
if we assume that the generative weight matrix from the hidden units to the outputs is c and the noise model at the output is gaussian with covariance r , then the posterior mean of the hidden variables is x = y , where = ct ( cct + r ) 123
therefore , the hidden units can compute the posterior mean exactly if they are linear and the weight matrix from input to hidden units is .
notice that is tied to c and r , so we only need to estimate c and r during learning .
we denote expectations under the posterior state distribution by ( cid : 123 ) ( cid : 123 ) , for example ,
xp ( x|y ) dx = x .
from the theory of the em algorithm ( see section 123 ) , we know that one way to maximize the likelihood is to maximize the expected value of the log of the joint probability under the posterior distribution of the hidden
( cid : 123 ) log p ( x , y|c , r ) ( cid : 123 ) .
sam roweis and zoubin ghahramani
changing signs and ignoring constants , we can equivalently minimize the following cost function : c = ( cid : 123 ) ( y cx ) tr = yt r = ( y cx ) tr
123 ( y cx ) ( cid : 123 ) + log|r| 123c ( cid : 123 ) x ( cid : 123 ) + ( cid : 123 ) xt ctr 123 ( y cx ) + log|r| + trace ( ctr
123cx ( cid : 123 ) + log|r|
123y 123yt r
here we have dened to be the posterior covariance of x ,
( cid : 123 ) xxt ( cid : 123 ) ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) x ( cid : 123 ) t = i c ,
and in the last step we have reorganized terms and made use of the fact that
123cx ( cid : 123 ) = trace ( ctr
the rst two terms of cost function in equation 123c are just a squared error cost function evaluated with respect to a gaussian noise model with covariance r .
they are exactly the terms minimized when tting a standard neural network with this gaussian noise model .
the last term is a regular - ization term that accounts for the posterior variance in the hidden states given the inputs . 123 when we take derivatives of this cost function , we do not differentiate x and with respect to c and r .
as is usual for the em algorithm , we differentiate the cost given the posterior distribution of the hidden variables .
taking derivatives with respect to c and premultiplying by r , we obtain a weight change rule ,
c ( y cx ) xt c .
the rst term is the usual delta rule .
the second term is simply a weight - decay term decaying the columns of c with respect to the posterior co - variance of the hidden variables .
intuitively , the higher the uncertainty in a hidden unit , the more its outgoing weight vector is shrunk toward zero .
to summarize , factor analysis can be implemented in an autoassociator by ty - ing the recognition weights to the generative weights and using a particular regularizer in addition to squared reconstruction error during learning .
we now analyze the mixture of gaussians model in the same manner .
the recognition network is meant to produce the mean of the hidden variable given the inputs .
since we assume that the discrete hidden variable is repre - sented as a unit vector , its mean is just the vector of probabilities of being in each of its k settings given the inputs , that is , the responsibilities .
assuming equal mixing coefcients , p ( x = ej ) = p ( x = ei ) ij , the responsibilities
hidden states has zero variance ( 123 ) and the regularizer vanishes ( ctr
123 pca assumes innitesimal noise , and therefore the posterior distribution over the
a unifying review of linear gaussian models
dened in equation 123b are
( x ) j = p ( x = ej|y ) =
i=123 exp ( 123 i=123 exp ( iy i ) , 123 and j = 123
( y ci ) tr123 ( y ci ) )
equation 123b where we have dened j describes a recognition model that is linear followed by the softmax nonlin - earity , , written in full matrix notation : x = ( y ) .
in other words , a simple network could do exact inference with linear input to hidden weights and softmax hidden units .
appealing again to the em algorithm , we obtain a cost function that when minimized by an autoassociator will implement the mixture of gaussians . 123 the log probability of the data given the hidden variables can be written as 123 ( y cx ) + log|r| + const .
123 log p ( y|x , c , r ) = ( y cx ) tr
c = ( y cx ) tr
using this and the previous derivation , we obtain the cost function ,
123 ( y cx ) + log|r| + trace ( ctr
where = ( cid : 123 ) xxt ( cid : 123 ) ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) x ( cid : 123 ) t .
the second - order term , ( cid : 123 ) xxt ( cid : 123 ) , evaluates to a matrix with x along its diagonal and zero elsewhere .
unlike in factor analysis , now depends on the input .
to summarize , the mixture of gaussians model can also be implemented using an autoassociator .
the recognition part of the network is linear , fol - lowed by a softmax nonlinearity .
the cost function is the usual squared er - ror penalized by a regularizer of exactly the same form as in factor analysis .
similar network interpretations can be obtained for the other probabilistic
123 comments and extensions
there are several advantages , both theoretical and practical , to a unied treatment of the unsupervised methods reviewed here .
from a theoretical viewpoint , the treatment emphasizes that all of the techniques for infer - ence in the various models are essentially the same and just correspond to probability propagation in the particular model variation .
similarly , all the learning procedures are nothing more than an application of the em
123 our derivation assumes tied covariance and equal mixing coefcients .
slightly more
complex equations result for the general case .
sam roweis and zoubin ghahramani
algorithm to increase the total likelihood of the observed data iteratively .
furthermore , the origin of zero - noise - limit algorithms such as vector quan - tization and pca is easy to see .
a unied treatment also highlights the relationship between similar questions across the different models .
for ex - ample , picking the number of clusters in a mixture model or state dimension in a dynamical system or the number of factors or principal components in a covariance model or the number of states in an hmm are all really the
from a practical standpoint , a unied view of these models allows us to apply well - known solutions to hard problems in one area to similar prob - lems in another .
for example , in this framework it is obvious how to deal properly with missing data in solving both the learning and inference prob - lems .
this topic has been well understood for many static models ( little & rubin , 123; tresp , ahmad , & neuneier , 123; ghahramani & jordan , 123 ) but is typically not well addressed in the linear dynamical systems literature .
as another example , it is easy to design and work with models having a mixed continuous - and discrete - state vector , ( for example , hidden lter hmms ( fraser & dimitriadis , 123 ) , which is something not directly addressed by the individual literatures on discrete or continuous models .
another practical advantage is the ease with which natural extensions to the basic models can be developed .
for example , using the hierarchi - cal mixture - of - experts formalism developed by jordan and jacobs ( 123 ) we can consider global mixtures of any of the model variants discussed .
in fact , most of these mixtures have already been considered : mixtures of linear dynamical systems are known as switching state - space models ( see shumway & stoffer , 123; ghahramani & hinton , 123b ) ; mixtures of factor analyzers ( ghahramani and hinton , 123 ) and of pancakes ( pca ) ( hin - ton et al . , 123 ) ; and mixtures of hmms ( smyth , 123 ) .
a mixture of m of our constrained mixtures of gaussians each with k clusters gives a mixture model with mk components in which there are only m possible covariance matrices .
this tied covariance approach is popular in speech modeling to reduce the number of free parameters .
( for k = 123 , this corresponds to a full unconstrained mixture of gaussians model with m clusters . )
it is also possible to consider local mixtures in which the conditional is no longer a single gaussian but a more complicated density such as a mixture of gaussians .
for our ( constrained ) mixture of gaussians model , this is another way to get a full mixture .
for hmms , this is a well - known extension and is usually the standard approach for emission density modeling ( rabiner & juang , 123 ) .
it is even possible to use constrained mixture models as the output density model for an hmm ( see , for example , saul & rahim , 123 , which uses factor analysis as the hmm output density ) .
however , we are not aware of any work that considers this variation in the continuous - state cases , for either static or dynamic data .
another important natural extension is spatially adaptive observation noise .
the idea here is that the observation noise v can have different statis -
a unifying review of linear gaussian models
tics in different parts of ( state or observation ) space rather than being de - scribed by a single matrix r .
for discrete - mixture models , this idea is well known , and it is achieved by giving each mixture component a private noise model .
however , for continuous - state models , this idea is relatively unex - plored and is an interesting area for further investigation .
the crux of the problem is how to parameterize a positive denite matrix over some space .
we propose some simple ways to achieve this .
one possibility is replacing the single covariance shape q for the observation noise with a conic123 lin - ear blending of k basis covariance shapes .
in the case of linear dynamical systems or factor analysis , this amounts to a novel type of model in which the local covariance matrix r is computed as a conic linear combination of several canonical covariance matrices through a tensor product between the current state vector x ( or equivalently the noiseless observation cx ) and a master noise tensorr . 123 another approach would be to drop the conic restriction ( allow general linear combinations ) and then add a multiple of the identity matrix to the resulting noise matrix in order to make it positive denite .
a third approach is to represent the covariance shape as the com - pression of an elastic sphere due to a spatially varying force eld .
this rep - resentation is easier to work with because the parameterization of the eld is unconstrained , but it is hard to learn the local eld from measurements of the effective covariance shape .
bishop ( 123 , sec .
123 ) and others have considered simple nonparametric methods for estimating input - dependent noise levels in regression problems .
goldberg , williams , and bishop ( 123 ) have explored this idea in the context of gaussian processes .
it is also interesting to consider what happens to the dynamic models when the output noise tends to zero .
in other words , what are the dynamic analogs of pca and vector quantization ? for both linear dynamical systems and hmms , this causes the state to no longer be hidden .
in linear dynamical systems , the optimal observation matrix is then found by performing pca on the data and using the principal components as the columns of c; for hmms , c is found by vector quantization of the data ( using the codebook vectors as the columns of c ) .
given these observation matrices , the state is no longer hidden .
all that remains is to identify a rst - order markov dynamics in state - space : this is a simple ar ( 123 ) model in the continuous case or a rst - order markov chain in the discrete case .
such zero - noise limits are not only interesting models in their own right , but are also valuable as good choices for initialization of learning in linear dynamical systems and hmms .
123 a conic linear combination is one in which all the coefcients are positive .
123 for mixtures of gaussians or hidden markov models , this kind of linear blend - ing merely selects the jth submatrix of the tensor if the discrete state is ej .
this is yet another way to recover the conventional full or unconstrained mixture of gaussians or hidden markov model emission density in which each cluster or state has its own private covariance shape for observation noise .
sam roweis and zoubin ghahramani
in this appendix we review in detail the inference and learning algorithms for each of the models .
the goal is to enable readers to implement these algorithms from the pseudocode provided .
for each class of model , we rst present the solution to the inference problem , and then the em algorithm for learning the model parameters .
for this appendix only , we adopt the , not xt .
we notation that the transpose of a vector or matrix is written as x use t instead of to denote the length of a time series .
we also dene the binary operator ( cid : 123 ) to be the element - wise product of two equal - size vectors or matrices .
comments begin with the symbol % .
a . 123 factor analysis , spca , and pca .
a . 123 inference .
for factor analysis and related models , the posterior probability of the hidden state given the observations , p ( x|y ) , is gaus - sian .
the inference problem therefore consists of computing the mean and covariance of this gaussian , x and v = cov ( x ) :
( cid : 123 ) + r ) 123
v i c return x , v since the observation noise matrix r is assumed to be diagonal and x is of smaller dimension than y , can be more efciently computed using the matrix inversion lemma : i c ( i + c
computing the ( log ) likelihood of an observation is nothing more than an
evaluation under the gaussian n ( cid : 123 )
( cid : 123 ) + r
the sensible pca ( spca ) algorithm is a special case of factor analysis in which the observation noise is assumed to be spherically symmetric : r = i .
inference in spca is therefore identical to inference for factor analysis .
the traditional pca algorithm can be obtained as a limiting case of factor analysis : r = lim123 i .
the inverse used for computing in factor analysis is no longer well dened .
however , the limit of is well dened : = .
also , the posterior collapses to a single point , so v = cov ( x ) = i c = 123
pcainference ( y , c ) % projection onto principal components
a unifying review of linear gaussian models
a . 123 learning .
the em algorithm for learning the parameters of a fac - tor analyzer with k factors from a zero - mean data set y = ( y123 , .
, yn ) ( each column of the p n matrix y is a data point ) is
initialize c , r compute sample covariance s of y while change in log likelihood >
% e step x , v factoranalysisinference ( y , c , r ) ( cid : 123 ) + nv % m step set diagonal elements of r to rii ( s c
return c , r
here factoranalysisinference ( y , c , r ) has the obvious interpre - tation of the inference function applied to the entire matrix of observations .
since and v do not depend on y , this can be computed efciently in matrix form .
since the data appear only in outer products , we can run fac - tor analysis learning with just the sample covariance .
note also that the ( cid : 123 ) + r| + const .
log - likelihood is computed as 123 the em algorithm for spca is identical to the em algorithm for factor analysis , except that since the observation noise covariance is spherically
symmetrical , the m - step for r is changed to r i , where ( cid : 123 ) p
( cid : 123 ) + r ) 123y + n
the em algorithm for pca can be obtained in a similar manner :
while change in squared reconstruction error >
% e step % m step
sam roweis and zoubin ghahramani
since pca is not a probability model ( it assumes zero noise ) , the likeli - hood is undened , so convergence is assessed by monitoring the squared
a . 123 mixtures of gaussians and vector quantization .
a . 123 inference .
we begin by discussing the inference problem for mix - tures of gaussians and then discuss the inference problem in vector quanti - zation as a limiting case .
the hidden variable in a mixture of gaussians is a discrete variable that can take on one of k values .
we represent this variable using a vector x of length k , where each setting of the hidden variable cor - responds to x taking a value of unity in one dimension and zero elsewhere .
the probability distribution of the discrete hidden variable , which has k 123 degrees of freedom ( since it must sum to one ) , is fully described by the mean of x .
therefore , the inference problem is limited to computing the posterior mean of x given a data point y and the model parameters , which are ( the prior mean of x ) , c ( the matrix whose k columns are the means of y given each of the k settings of x ) and r ( the observation noise covariance
mixtureofgaussiansinference ( y , c , r , ) % compute for i = 123 to k i ( y ci ) ( cid : 123 )
a measure of the randomness of the hidden state can be obtained by evaluating the entropy of the discrete distribution corresponding to x .
standard vq corresponds to the limiting case r = lim123 i and equal priors i = 123 / k .
inference in this case is performed by the well known
vqinference ( y , c ) % 123 - nearest - neighbor for i = 123 to k
i ( y ci ) ( cid : 123 ) ( y ci ) endx ej for j = arg min i
a unifying review of linear gaussian models
as before , ej is the unit vector along the jth coordinate direction .
the squared distances i can be generalized to a mahalanobis metric with respect to some matrix r , and nonuniform priors i can easily be incorporated .
as was the case with pca , the posterior distribution has zero entropy .
a . 123 learning .
the em algorithm for learning the parameters of a mix -
ture of gaussian is :
mixtureofgaussianslearn ( y , k , ) % ml soft competitive learning
initialize c , r , while change in log likelihood > initialize 123 , 123 , 123 % e step for i = 123 to n
% m step for j = 123 to k for i = 123 to n
+ xij ( yi cj ) ( yi cj ) ( cid : 123 )
return c , r ,
we have assumed a common covariance matrix r for all the gaussians; the extension to different covariances for each gaussian is straightforward .
the k - means vector quantization learning algorithm results when we
take the appropriate limit of the above algorithm :
vqlearn ( y , k , ) % k - means
while change in squared reconstruction error >
% e step initialize 123 , 123 for i = 123 to n
sam roweis and zoubin ghahramani
% m step for j = 123 to k
a . 123 linear dynamical systems .
inference in a linear dynamical system involves com - puting the posterior distributions of the hidden state variables given the sequence of observations .
as in factor analysis , all the hidden state variables are assumed gaussian and are therefore fully described by their means and covariance matrices .
the algorithm for computing the posterior means and covariances consists of two parts : a forward recursion , which uses the ob - servations from y123 to yt , known as the kalman lter ( kalman , 123 ) , and a backward recursion , which uses the observations from yt to yt+123 ( rauch , 123 ) .
the combined forward and backward recursions are known as the kalman or rauch - tung - streibel ( rts ) smoother .
t and vs
to describe the smoothing algorithm it will be useful to dene the follow - ing quantities : xs t are , respectively , the mean and covariance matrix of xt given observations ( y123 , .
ys ) ; xt xt t are the full smoother estimates .
to learn the a matrix using em , it is also necessary to compute the covariance across time between xt and xt123 :
t and vt vt
for t = 123 to t % kalman lter ( forward pass )
123 ) % kalman smoother
initialize vt , t123 = ( i ktc ) avt123 for t = t to 123 % rauch recursions ( backward pass )
+ jt123 ( xt axt123 + jt123 ( vt vt123 + jt ( vt+123 , t avt t123 if t < t
t123 if t > 123 ( cid : 123 ) + q if t > 123 ( cid : 123 ) + r ) 123 + kt ( yt cxt123
a unifying review of linear gaussian models
return xt , vt , vt , t123 for all t
a . 123 learning .
the em algorithm for learning a linear dynamical sys - tem ( shumway & stoffer , 123; ghahramani & hinton , 123a ) is given below , assuming for simplicity that we have only a single sequence of observations :
initialize a , c , q , r , x123
while change in log likelihood > initialize 123 , 123 , 123 for t = 123 to t + vt , t123 if t > 123
123 ) % e step
% m step r ( c
return a , c , q , r , x123
a . 123 hidden markov models .
a . 123 inference .
the forward - backward algorithm computes the poste - rior probabilities of the hidden states in an hmm and therefore forms the basis of the inference required for em .
we use the following standard de -
t = p ( xt , y123 ,
= p ( yt+123 , .
, yt|xt ) ,
sam roweis and zoubin ghahramani
where both and are vectors of the same length as x .
we present the case where the observations yt are real - valued p - dimensional vectors and the probability density of an observation given the corresponding state ( the output model ) is assumed to be a single gaussian with mean cxt and covariance r .
the parameters of the model are therefore a k k transition matrix t , initial state prior probability vector , an observation mean matrix c , and an observation noise matrix r that is tied across states :
hmminference ( y , t , , c , r ) % forwardbackward algorithm for t = 123 to t % forward pass
( cid : 123 ) |r|123 / 123 ( 123 ) p / 123
for i = 123 to k
if ( t = 123 ) t ( cid : 123 ) b123 else t ( t
for t = t 123 to 123 % backward pass
t123 ) ( cid : 123 ) bt end
t ( cid : 123 ) t
return t , t , t for all t
the denitions of and in equations a . 123a and a . 123b correspond to running the above algorithm without the scaling factors t .
these factors , however , are essential to the numerical stability of the algorithm; otherwise , for long sequences , both and become vanishingly small .
furthermore , from the s we can compute the log - likelihood of the sequence
log p ( y123 , .
, yt ) = t ( cid : 123 )
which is why it is useful for the above function to return them .
a . 123 learning .
again , we assume for simplicity that we have a single sequence of observations from which we wish to learn the parameters of an hmm .
the em algorithm for learning these parameters , known as the
a unifying review of linear gaussian models
baum - welch algorithm , is :
hmmlearn ( y , k , ) % baum - welch
initialize t , , c , r while change in log likelihood > hmminference ( y , t , , c , r ) % e step % m step
( cid : 123 ) ti ( cid : 123 ) for all i , j t t , j for all j ( yt cj ) ( yt cj ) ( cid : 123 ) / t
return t , , c , r
we thank carlos brody , sanjoy mahajan , and erik winfree for many fruitful discussions in the early stages , the anonymous referees for helpful com - ments , and geoffrey hinton and john hopeld for providing outstanding intellectual environments and guidance .
was supported in part by the center for neuromorphic systems engineering as a part of the national sci - ence foundation engineering research center program under grant eec - 123 and by the natural sciences and engineering research council of canada under an nserc 123 award .
was supported by the ontario information technology research centre .
