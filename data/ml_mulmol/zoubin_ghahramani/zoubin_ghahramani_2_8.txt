we present a probabilistic kernel approach to ordinal regression based on gaussian processes .
a threshold model that generalizes the probit function is used as the likelihood function for ordinal variables .
two inference techniques , based on the laplace approximation and the expectation prop - agation algorithm respectively , are derived for hyperparameter learning and model selection .
we compare these two gaussian process approaches with a previous ordinal regression method based on support vector machines on some benchmark and real - world data sets , including applications of ordinal regression to collaborative ltering and gene expression analysis .
experimental results on these data sets verify the usefulness of our approach .
keywords : gaussian processes , ordinal regression , approximate bayesian inference , collaborative ltering , gene expression analysis , feature selection
practical applications of supervised learning frequently involve situations exhibiting an order among the different categories , e . g .
a teacher always rates his / her students by giving grades on their overall performance .
in contrast to metric regression problems , the grades are usually discrete and nite .
these grades are also different from the class labels in classication problems due to the existence of ranking information .
for example , grade labels have the ordering f < d < c < b < a .
this is a learning task of predicting variables of ordinal scale , a setting bridging between metric regression and classication referred to as ranking learning or ordinal regression .
there is some literature about ordinal regression in the domain of machine learning .
kramer et al .
( 123 ) investigated the use of a regression tree learner by mapping the ordinal variables into numeric values .
however there might be no principled way of devising an appropriate mapping function .
frank and hall ( 123 ) converted an ordinal regression problem into nested binary clas - sication problems that encode the ordering of the original ranks , and then the results of standard binary classiers can be organized for prediction .
har - peled et al .
( 123 ) proposed a constraint classication approach for ranking problems based on binary classiers .
cohen et al .
( 123 ) con - sidered general ranking problems in the form of preference judgements .
herbrich et al .
( 123 ) applied the principle of structural risk minimization ( vapnik , 123 ) to ordinal regression lead - ing to a new distribution - independent learning algorithm based on a loss function between pairs of ranks .
shashua and levin ( 123 ) generalized the formulation of support vector machines to or -
c ( cid : 123 ) 123 wei chu and zoubin ghahramani .
chu and ghahramani
dinal regression and the numerical results they presented shows a signicant improvement on the performance compared with the on - line algorithm proposed by crammer and singer ( 123 ) .
in the statistics literature , most of the approaches are based on generalized linear models ( mc - cullagh and nelder , 123 ) .
the cumulative model ( mccullagh , 123 ) is well - known in classical statistical approaches for ordinal regression , in which they rely on a specic distributional assump - tion on the unobservable latent variables and a stochastic ordering of the input space .
johnson and albert ( 123 ) described bayesian inference on parametric models for ordinal data using sampling techniques .
tutz ( 123 ) presented a general framework for semiparametric models that extends generalized additive models ( hastie and tibshirani , 123 ) by incorporating nonparametric parts .
the nonparametric components of the regression model are tted by maximizing penalized log likelihood , and model selection is carried out using aic .
gaussian processes ( ohagan , 123; neal , 123 ) have provided a promising non - parametric bayesian approach to metric regression ( williams and rasmussen , 123 ) and classication prob - lems ( williams and barber , 123 ) .
the important advantage of gaussian process models ( gps ) over other non - bayesian models is the explicit probabilistic formulation .
this not only provides prob - abilistic predictions but also gives the ability to infer model parameters such as those that control the kernel shape and the noise level .
the gps are also different from the semiparametric approach of tutz ( 123 ) in several ways .
first , the additive models ( fahrmeir and tutz , 123 ) are dened by functions in each input dimension , whereas the gps can have more general non - additive covariance functions; second , the kernel trick allows to use innite basis function expansions; third , the gps perform bayesian inference in the space of the latent functions .
in this paper , we present a probabilistic kernel approach to ordinal regression in gaussian pro - cesses .
we impose a gaussian process prior distribution on the latent functions , and employ an appropriate likelihood function for ordinal variables which can be regarded as a generalization of the probit function .
two bayesian inference techniques are applied to implement model adapta - tion by using the laplace approximation ( mackay , 123 ) and the expectation propagation ( minka , 123 ) respectively .
comparisons of the generalization performance against the support vector ap - proach ( shashua and levin , 123 ) on some benchmark and real - world data sets , such as movie ranking and gene expression analysis , verify the usefulness of this approach .
the paper is organized as follows : in section 123 , we describe the bayesian framework in gaus - sian processes for ordinal regression; in section 123 , we discuss the bayesian techniques for hyperpa - rameter inference; in section 123 , we present the predictive distribution for probabilistic prediction; in section 123 , we give some extensive discussion on these techniques; in section 123 , we report the results of numerical experiments on some benchmark and real - world data sets; we conclude this paper in
bayesian framework consider a data set composed of n samples .
each of the samples is a pair of input vector xi r d and the corresponding target yi y where y is a nite set of r ordered categories .
without loss of generality , these categories are denoted as consecutive integers y = ( 123 , 123 , .
, r ) that keep the known ordering information .
the main idea is to assume an unobservable latent function f ( xi ) r associated with xi in a gaussian process , and the ordinal variable yi dependent on the latent function f ( xi ) by modelling the ranks as intervals on the real line .
a bayesian framework is described with more details in the following .
gaussian processes for ordinal regression
123 gaussian process prior the latent functions ( f ( xi ) ) are usually assumed as the realizations of random variables indexed by their input vectors in a zero - mean gaussian process .
the gaussian process can then be fully specied by giving the covariance matrix for any nite set of zero - mean random variables ( f ( xi ) ) .
the covariance between the functions corresponding to the inputs xi and x j can be dened by mercer kernel functions ( wahba , 123; scholkopf and smola , 123 ) , e . g .
gaussian kernel which is dened
cov ( f ( xi ) , f ( x j ) ) = k ( xi , x j ) = exp i denotes the v - th element of xi . 123 thus , the prior probability of these latent
where k > 123 and x functions ( f ( xi ) ) is a multivariate gaussian
p ( f ) =
where f = ( f ( x123 ) , f ( x123 ) , .
, f ( xn ) ) t , z f = ( 123p ) i j - th element is dened as in ( 123 ) .
f t s 123 f ( cid : 123 ) 123 , and s
is the n n covariance matrix whose
123 likelihood for ordinal variables
the likelihood is the joint probability of observing the ordinal variables given the latent functions , denoted as p ( d| f ) where d denotes the target set ( yi ) .
generally , the likelihood can be evaluated as a product of the likelihood function on individual observation :
p ( d| f ) =
p ( yi| f ( xi ) )
where the likelihood function p ( yi| f ( xi ) ) could be intuitively dened as if byi123 < f ( xi ) byi ,
pideal ( yi| f ( xi ) ) = ( cid : 123 ) 123
and br = +
i with positive padding variables d
where b123 = are dened subsidiarily , b123 r and the other threshold variables can i and i = 123 , .
the be further dened as b j = b123 + ( cid : 123 ) role of b123 < b123 < .
< br123 is to divide the real line into r contiguous intervals; these intervals map the real function value f ( xi ) into the discrete variable yi while enforcing the ordinal constraints .
the likelihood function ( 123 ) is used for ideally noise - free cases .
in the presence of noise from inputs or targets , we may explicitly assume that the latent functions are contaminated by a gaussian noise with zero mean and unknown variance s 123 n ( d ; , s 123 ) is used to denote a gaussian random variable d with mean and variance s 123 henceforth .
then the ordinal likelihood function becomes
p ( yi| f ( xi ) ) = z pideal ( yi| f ( xi ) + d
i = f ( cid : 123 ) zi
123 ( cid : 123 ) f ( cid : 123 ) zi
other mercer kernel functions , such as polynomial kernels and spline kernels etc . , can also be used in the covariance
in principle , any distribution rather than a gaussian can be assumed for the noise on the latent functions .
chu and ghahramani
ordinal likelihood function p ( y|f ( x ) )
d - ln p ( y|f ( x ) ) / df ( x )
d123 - ln p ( y|f ( x ) ) / d 123f ( x )
figure 123 : the graph of the likelihood function for an ordinal regression problem with r = 123 , along with the rst and second order derivatives of the loss function ( negative logarithm of the likelihood function ) , where the noise variance s 123 = 123 , and the two thresholds are b123 = 123 and b123 = +123
byi f ( xi )
byi123 f ( xi )
, and f ( z ) = r z
n ( v ;123 , 123 ) dv .
note that binary classication is a special case of ordinal regression when r = 123 , and in this case the likelihood function ( 123 ) be - comes the probit function .
the quantity ln p ( yi| f ( xi ) ) is usually referred to as the loss function ` ( yi , f ( xi ) ) .
the derivatives of the loss function with respect to f ( xi ) are needed in some approx - imate bayesian inference methods .
the rst order derivative of the loss function can be written
we present graphs of the ordinal likelihood function ( 123 ) and the derivatives of the loss function in figure 123 as an illustration .
note that the rst order derivative ( 123 ) is a monotonically increasing function of f ( xi ) , and the second order derivative ( 123 ) is always a positive value between 123 and 123 s 123 .
i;123 , s 123 ) is also log - given the facts that pideal ( yi| f ( xi ) + d concave , as pointed out by pratt ( 123 ) , the convexity of the loss function follows , because the integral of a log - concave function with respect to some of its arguments is a log - concave function of its remaining arguments ( brascamp and lieb , 123 , cor
i ) is log - concave in ( f ( xi ) , d
i ) and n ( d
123 posterior probability
based on bayes theorem , the posterior probability can then be written as
p ( f|d ) =
p ( yi| f ( xi ) ) p ( f )
where the prior probability p ( f ) is dened as in ( 123 ) , the likelihood function p ( yi| f ( xi ) ) is dened as in ( 123 ) , and p ( d ) = r p ( d| f ) p ( f ) d f .
` ( yi , f ( xi ) )
123;123 , 123 ) n ( zi 123 ) f ( zi
and the second order derivative can be given as 123;123 , 123 ) n ( zi 123 ) f ( zi
s 123 ( cid : 123 ) n ( zi
123` ( yi , f ( xi ) )
123 f ( xi )
123 ) f ( zi
gaussian processes for ordinal regression
the bayesian framework we described above is conditional on the model parameters including in the covariance function ( 123 ) that control the kernel shape , the threshold in the likelihood function ( 123 ) .
all these param - , which is the hyperparameter vector .
the normalization factor p ( d ) , a yardstick for model selection .
in the
the kernel parameters k parameters ( b123 , d 123 , .
, d eters can be collected into q in ( 123 ) , more exactly p ( d|q ) , is known as the evidence for q
r123 ) and the noise level s
next section , we discuss techniques for hyperparameter learning .
model adaptation in a full bayesian treatment , the hyperparameters q must be integrated over the q - space .
monte carlo methods ( neal , 123 ) can be adopted here to approximate the integral effectively .
however these might be prohibitively expensive to use in practice .
alternatively , we consider model se - lection by determining an optimal setting for q .
the optimal values of hyperparameters q can be simply inferred by maximizing the posterior probability p ( q |d ) , where p ( q |d ) ( cid : 123 ) p ( d|q ) p ( q ) .
the prior distribution on the hyperparameters p ( q ) can be specied by domain knowledge , or al - ternatively some vague uninformative distribution .
the evidence is given by a high dimensional integral , p ( d|q ) = r p ( d| f ) p ( f ) d f .
a popular idea for computing the evidence is to approxi - mate the posterior distribution p ( f|d ) as a gaussian , and then the evidence can be calculated by an explicit formula ( mackay , 123; csato et al . , 123; minka , 123 ) .
in this section , we describe two bayesian techniques for model adaptation by using the laplace approximation and the expectation
123 map approach with laplace approximation
the evidence can be calculated analytically after applying the laplace approximation at the max - imum a posteriori ( map ) estimate , and gradient - based optimization methods can then be used to infer the optimal hyperparameters by maximizing the evidence .
the map estimate on the latent functions is referred to f map = argmax f p ( f|d ) , which is equivalent to the minimizer of negative logarithm of p ( f|d ) , i . e .
s ( f ) =
` ( yi , f ( xi ) ) +
f t s 123 f
where ` ( yi , f ( xi ) ) = ln p ( yi| f ( xi ) ) is known as the loss function .
note that positive denite matrix , where l given as in ( 123 ) .
thus , this is a convex programming problem with a unique solution . 123 the laplace approximation of s ( f ) refers to carrying out the taylor expansion at the map point and retaining the terms up to the second order ( mackay , 123 ) .
since the rst order derivative with respect to f vanishes at f map , s ( f ) can also be written as
is a diagonal matrix whose ii - th entry is
123` ( yi , f ( xi ) )
123 f ( xi )
123s ( f )
f t = s 123 + l
s ( f ) s ( f map ) +
( f f map ) t ( cid : 123 ) s 123 + l map ( cid : 123 ) ( f f map )
where l map denotes the matrix l at the map estimate .
this is equivalent to approximating the pos - terior distribution p ( f|d ) as a gaussian distribution centered on f map with the covariance matrix
the newton - raphson formula can be used to nd the solution for simple cases .
chu and ghahramani
( s 123 + l map ) 123 , i . e .
p ( f|d ) n ( f ; f map , ( s 123 + l map ) 123 ) .
using the laplace approximation ( 123 ) and z f dened as in ( 123 ) , the evidence can be computed analytically as follows
p ( d|q ) =
z f z exp ( s ( f ) ) d f exp ( s ( f map ) ) |i + s
l map| 123
where i is an n n identity matrix .
the gradients of the logarithm of the evidence ( 123 ) with respect to the hyperparameters q can be derived analytically .
then gradient - based optimization methods can be employed to search for the maximizer of the evidence .
refer to appendix a for the detailed gradient formulae and the outline of our algorithm for model adaptation .
123 expectation propagation with variational methods
the expectation propagation algorithm ( ep ) is an approximate bayesian inference method ( minka , 123 ) , which can be regarded as an extension of assumed - density - lter ( adf ) .
the ep algorithm has been applied in gaussian process classication along with variational methods for model selec - tion ( seeger , 123; kim and ghahramani , 123 ) .
in the setting of gaussian processes , ep attempts to approximate p ( f|d ) as a product distribution in the form of q ( f ) = ( cid : 123 ) i=123 ti ( f ( xi ) ) p ( f ) where ti ( f ( xi ) ) = si exp ( 123 123 pi ( f ( xi ) mi ) 123 ) .
the parameters ( si , mi , pi ) in ( ti ) are successively optimized by minimizing the following kullback - leibler divergence ,
i = argmin
kl ( cid : 123 ) q ( f )
q ( f )
p ( yi| f ( xi ) ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 )
since q ( f ) is in the exponential family , this minimization can be simply solved by moment match - ing up to the second order .
a detailed updating scheme can be found in appendix b .
at the equilibrium of q ( f ) , we obtain an approximate posterior distribution as p ( f|d ) n ( f ; ( s 123 + p ) 123p m , ( s 123 +p ) 123 ) where p is a diagonal matrix whose ii - th entry is pi and m = ( m123 , m123 , .
, mn ) t .
variational methods can be used to optimize the hyperparameters q by maximizing the lower
bound on the logarithm of the evidence .
by applying jensens inequality , we have
log p ( d|q ) = logr p ( d| f ) p ( f ) = r q ( f ) log p ( d| f ) d f +r q ( f ) log p ( f ) d f r q ( f ) logq ( f ) d f = f ( q ) .
q ( f ) q ( f ) d f r q ( f ) log p ( d| f ) p ( f )
q ( f )
the lower bound f ( q ) can be written as an explicit expression at the equilibrium of q ( f ) , and then the gradients with respect to q can be derived by neglecting the possible dependency of q ( f ) on q the detailed formulation can be found in appendix c .
we have described two techniques , the map approach and the ep approach , to infer the optimal model .
at the optimal hyperparameters we inferred , denoted as q , let us take a test case x for which the target yx is unknown .
the latent variable f ( x ) and the column vector f containing the n zero - mean random variables ( f ( xi ) ) n i=123 have the prior joint multivariate gaussian distribution , i . e .
f ( x ) ( cid : 123 ) n ( cid : 123 ) ( cid : 123 ) 123
123 ( cid : 123 ) , ( cid : 123 ) s
kt k ( x , x ) ( cid : 123 ) ( cid : 123 )
gaussian processes for ordinal regression
where k = ( k ( x , x123 ) , k ( x , x123 ) , .
, k ( x , xn ) ) t .
the conditional distribution of f ( x ) given f is a gaussian too , denoted as p ( f ( x ) | f , q ) with mean f t s 123k and variance k ( x , x ) kt s 123k .
the predictive distribution of p ( f ( x ) |d , q ) can be computed as an integral over f - space , which can be
p ( f ( x ) |d , q ) = z p ( f ( x ) | f , q ) p ( f |d , q ) d f .
the posterior distribution p ( f|d , q ) can be approximated as a gaussian by the map approach or the ep approach ( refer to section 123 ) .
the predictive distribution ( 123 ) can then be simplied as a gaussian n ( f ( x ) ; x , s 123
x ) with mean x and variance s 123
in the map approach , we reach
x = kt s 123 f map
x = k ( x , x ) kt ( s + l 123
while in the ep approach , we get
x = kt ( s + p 123 ) 123m and
x = k ( x , x ) kt ( s + p 123 ) 123k .
the predictive distribution over ordinal targets yx is
p ( yx|x , d , q ) = r p ( yx| f ( x ) , q ) p ( f ( x ) |d , q ) d f ( x )
the predictive ordinal scale can be decided as argmax
= f ( cid : 123 ) byxxs 123+s 123
x ( cid : 123 ) f ( cid : 123 ) byx123x s 123+s 123 p ( yx = i|x , d , q ) .
in the map approach , the mean of the predictive distribution depends on the map estimate f map , which is unique and can be found by solving a convex programming problem .
evidence maximiza - tion is useful if the laplace approximation around the mode point f map gives a good summary of the posterior distribution p ( f|d ) .
while in the approach of expectation propagation , the mean of the predictive distribution depends on the approximate mean of the posterior distribution .
when the true shape of p ( f|d ) is far from a gaussian centered on the mode , the ep approach can have a great advantage over the laplace approximation .
however the ep algorithm cannot guarantee convergence , though it usually works well in practice .
, the inversion of the matrix s
the gradient - based optimization method usually requests evidence evaluation at tens of different settings of q before the minimum is found .
for each q is required that costs time at o ( n123 ) , where n is the number of training samples .
recently , csato and opper ( 123 ) proposed a fast training algorithm for gaussian processes in which the set of basis vectors are determined on - line for sparse representation .
lawrence et al .
( 123 ) proposed a greedy selection with criteria based on information - theoretic principles for sparse gaussian processes ( seeger , 123 ) .
tresp ( 123 ) proposed the bayesian committee machines to divide and conquer large data sets , while using innite mixtures of gaussian processes ( rasmussen and ghahramani , 123 ) is another promising technique .
these algorithms can be applied directly in the settings of ordinal regression
feature selection is an essential part in modelling .
in gaussian processes , the automatic rele - vance determination ( ard ) method proposed by mackay ( 123 ) and neal ( 123 ) can be embedded
chu and ghahramani
into the covariance function ( 123 ) as follows :
cov ( f ( xi ) , f ( x j ) ) = k ( xi , x j ) = exp
v > 123 is the ard parameter . 123 the gradients with respect to the variables ( lnk v ) can also be derived analytically for model adaptation .
the optimal value of the ard parameter k the relevance of the v - th input feature to the target .
the form of feature selection we use here results in a type of feature weighting .
furthermore , the linear combination of heterogeneous kernels with positive coefcients is still a valid covariance function .
lanckriet et al .
( 123 ) suggest to learn the kernel matrix with semidenite programming .
in the bayesian framework , these positive coefcients for kernels could be treated as hyperparameters , and optimized using the evidence as a criterion for optimization .
note that binary classication is a special case of ordinal regression with r = 123 , and the like - lihood function ( 123 ) becomes the probit function when r = 123
both of the probit function and the logistic function can be used as the likelihood function in binary classication , while they have different origins .
due to the dichotomous nature in the classes of multi - classication , discriminant functions are constructed for each class and then compete again others via the softmax function to determine the likelihood .
the logistic function , as a special case of the softmax function , comes from general classication problems .
in metric regression , warped gaussian processes ( snelson et al . , 123 ) assume that there is a nonlinear , monotonic , and continuous warping function relating the observed targets and some latent variables in a gaussian process .
the warping function , which is learned from the data , can be thought of as a pre - processing transformation applied before modelling with a gaussian process .
a different ( and very common ) approach to dealing with this preprocessing is to discretize the target values into r different bins .
these discrete values are clearly ordinal , and applying ordinal regression to these discrete values seems the natural choice .
interestingly , as the number of discretization bins r is increased , the ordinal regression model becomes very similar to the warped gaussian processes model .
in particular , by varying the thresholds in our ordinal regression model , it can approximate any continuous warping function .
numerical experiments
we start this section with a simple synthetic data set to visualize the behavior of these algorithms , and report the experimental results on sixteen benchmark data sets . 123 then we perform experiments on a collaborative ltering problem using the eachmovie data , and on gleason score prediction from gene microarray data related to prostate cancer .
shashua and levin ( 123 ) generalized the sup - port vector formulation by nding multiple thresholds to dene parallel discriminant hyperplanes for ordinal scales , and reported that the performance of the support vector approach is better than that of the on - line algorithm ( crammer and singer , 123 ) .
the problem size in the large - margin ranking algorithm of herbrich et al .
( 123 ) is a quadratic function of the training data size making the algorithmic complexity o ( n123 ) o ( n123 ) .
this makes the experiments on large data sets computa - tionally difcult .
thus , we decide to limit our comparisons to the support vector approach ( svm )
these ard parameters control the covariance length - scale of the gaussian process along each input dimension .
these data sets are publicly available at http : / / www . liacc . up . pt / ltorgo / regression / datasets . html .
gaussian processes for ordinal regression
of shashua and levin ( 123 ) and the two versions of our approach , the map approach with laplace approximation ( map ) and the ep algorithm with variational methods ( ep ) .
in our implementation , 123 we used the routine l - bfgs - b ( byrd et al . , 123 ) as the gradient - based optimization package , and started from the initial values of hyperparameters to infer the optimal values in the criterion of the approximate evidence ( 123 ) for map or the variational lower bound ( 123 ) for ep respectively . 123 the improved smo algorithm ( keerthi et al . , 123 ) was adapted to implement the svm approach ( refer to chu and keerthi ( 123 ) for detailed description and extensive discussion ) , 123 and 123 - fold cross vali - dation was used to determine the optimal values of model parameters ( the kernel parameter k and the regularization factor c ) involved in the problem formulations .
the initial search was done on a 123 coarse grid linearly spaced in the region ( ( log123c , log123 followed by a ne search on a 123 123 uniform grid linearly spaced by 123 in the ( log123c , log123 space .
we have utilized two evaluation metrics which quantify the accuracy of predictive ordinal scales ( y123 , .
, yt ) with respect to true targets ( y123 , .
, yt ) :
k ) | 123 log123c 123 , 123 log123
mean absolute error is the average deviation of the prediction from the true target , i . e .
i=123| yi yi| , in which we treat the ordinal scales as consecutive integers;
mean zero - one error gives an error of 123 to every incorrect prediction that is the fraction of
123 articial data
figure 123 presents the behavior of the three algorithms using the gaussian kernel ( 123 ) on a synthetic 123d data with three ordinal scales .
in the support vector approach , the optimal thresholds were determined by the smo algorithm and 123 - fold cross validation was used to decide the optimal values of the kernel parameter and the regularization factor .
as for the gaussian process algorithms , model adaptation ( see section 123 ) was used to determine the optimal values of the kernel parameter , the noise level and the thresholds automatically .
the gure shows that all the algorithms are working reasonably well on this task .
123 benchmark data
we collected nine benchmark data sets ( set i in table 123 ) that were used for metric regression prob - lems .
the target values were discretized into ordinal quantities using equal - length binning .
these bins divide the range of target values into a given number of intervals that are of same length .
the resulting rank values are ordered , representing these intervals of the original metric quantities .
for each data set , we generated two versions by discretizing the target values into ve and ten intervals respectively .
we randomly partitioned each data set into training / test splits as specied in table 123
the partition was repeated 123 times independently .
the gaussian kernel ( 123 ) was used in these three algorithms .
the test results are recorded in tables 123 and 123
the performance of the map and ep approaches are closely matching .
our gaussian process algorithms often yield better results than
the two versions of our proposed approach were implemented in ansi c , and the source code is accessible at
in numerical experiments , the initial values of the hyperparameters were usually chosen as s 123 = 123 , k = 123 / d for gaussian kernel , the threshold b123 = 123 and d i = 123 / r .
we suggest to try several starting points in practice , and then 123
the source code in ansi c is available at http : / / www . gatsby . ucl . ac . uk / chuwei / code / svorim . tar .
choose the best model by the objective functional .
chu and ghahramani
the svm approach
the map approach
the ep approach
the svm approach
the map approach
the ep approach
figure 123 : the performance of the three algorithms on a synthetic three - rank ordinal regression problem .
the discriminant function values of the svm approach , and the predictive mean values of the two gaussian process approaches are presented as contour graphs in - dexed by the two thresholds .
the upper graphs are for the case of lower noise level , while the lower graphs are for the case of higher noise level .
the training samples we used are presented in these graphs .
the dots denote the training samples of rank 123 , the crosses denote the training samples of rank 123 and the circles denote the training samples of rank
the support vector approach on the average value , especially when the number of training samples
in the next experiment , we selected seven very large metric regression data sets ( set ii in table 123 ) .
the input vectors were normalized to zero mean and unit variance coordinate - wise .
the target values of these data sets were discretized into 123 ordinal quantities using equal - frequency binning .
for each data set , a small subset was randomly selected for training and then tested on the remaining samples , as specied in table 123
the partition was repeated 123 times independently .
to show the advantage of explicitly modelling the ordinal nature of the targets , we also employed the standard gaussian process algorithm ( williams and rasmussen , 123 ) for metric regression ( gpr ) 123 to tackle these ordinal regression tasks , where the ordinal targets were naively treated as continuous values and the predictions for test cases were rounded to the nearest ordinal scale .
the gaussian kernel ( 123 ) was used in the four algorithms .
from the test results in table 123 , the ordinal regression algo -
in the gpr , the type - ii maximum likelihood was used for model selection .
gaussian processes for ordinal regression
wisconsin breast cancer
instances for test
table 123 : data sets and their characteristics .
attributes state the number of numerical and nominal attributes .
training instances and instances for test specify the size of training / test partition .
the partitions we generated and the test results on individual partitions can be accessed at http : / / www . gatsby . ucl . ac . uk / chuwei / ordinalregression . html .
mean zero - one error
mean absolute error
123 . 123% 123 . 123% 123 . 123% 123 . 123 123 . 123 123 . 123 123 . 123% 123 . 123% 123 . 123% 123 . 123 123 . 123 123 . 123 123 . 123% 123 . 123% 123 . 123% 123 . 123 123 . 123 123 . 123 ? 123 . 123% 123 . 123% 123 . 123% 123 . 123 123 . 123 123 . 123 123 . 123% 123 . 123% 123 . 123% 123 . 123 123 . 123 123 . 123 auto mpg ? 123 . 123% 123 . 123% 123 . 123% 123 . 123 123 . 123 123 . 123 123 . 123% 123 . 123% 123 . 123% 123 . 123 123 . 123 123 . 123 123 . 123% 123 . 123% 123 . 123% 123 . 123 123 . 123 123 . 123 123 . 123% 123 . 123% 123 . 123% 123 . 123 123 . 123 ? 123 . 123
table 123 : test results of the three algorithms using a gaussian kernel .
the targets of these bench - mark data sets were discretized by 123 equal - length bins .
the results are the averages over 123 trials , along with the standard deviation .
we use the bold face to indicate the cases in which the average value is the lowest in the results of the three algorithms .
the symbols ? are used to indicate the cases in which the indicated entry is signicantly worse than the winning entry; a p - value threshold of 123 in wilcoxon rank sum test was used to decide
rithms are clearly superior to the naive approach of applying standard metric regression .
we also observed that the performance of gaussian process algorithms are signicantly better than that of the support vector approach on six of the seven data sets .
this veries our judgement in the previous experiment that our gaussian process algorithms yield better performance than the support vector approach on small data sets .
although the ep approach often yields better results of mean zero - one error than the map approach on these tasks , we have not detected any statistically signicant dif - ference on their performance .
in table 123 we also report their negative logarithm of the likelihood in prediction ( nll ) .
the performance of the map and ep approaches are closely matching too with no statistically signicant difference .
chu and ghahramani
mean zero - one error
mean absolute error
? 123 . 123% 123 . 123% 123 . 123% 123 . 123 123 . 123 pyrimidines 123 . 123% 123 . 123% 123 . 123% 123 . 123 123 . 123 ? 123 . 123% 123 . 123% 123 . 123% 123 . 123 123 . 123 ? 123 . 123% 123 . 123% 123 . 123% 123 . 123 123 . 123 123 . 123% 123 . 123% 123 . 123% 123 . 123 123 . 123 auto mpg 123 . 123% 123 . 123% 123 . 123% 123 . 123 123 . 123 123 . 123% 123 . 123% 123 . 123% 123 . 123 123 . 123 123 . 123% ? 123 . 123% ? 123 . 123% 123 . 123 ? 123 . 123 ? 123 . 123 123 . 123% 123 . 123% 123 . 123% 123 . 123 123 . 123
table 123 : test results of the three algorithms using a gaussian kernel .
the targets of these bench - mark data sets were discretized by 123 equal - length bins .
the results are the averages over 123 trials , along with the standard deviation .
we use the bold face to indicate the cases in which the average value is the lowest in the results of the three algorithms .
the symbols ? are used to indicate the cases in which the indicated entry is signicantly worse than the winning entry; a p - value threshold of 123 in wilcoxon rank sum test was used to decide
mean zero - one error
? 123 123 % 123 123 % 123 123 % 123 123 % 123 123 123 123 ? 123 123 % ? 123 123 % 123 123 % 123 123 % 123 123 123 123 compact ( 123 ) ? 123 123 % ? 123 123 % 123 123 % 123 123 % 123 123 123 123 compact ( 123 ) ? 123 123 % ? 123 123 % 123 123 % 123 123 % 123 123 123 123 ? 123 123 % ? 123 123 % 123 123 % 123 123 % 123 123 123 123 ? 123 123 % ? 123 123 % 123 123 % 123 123 % 123 123 123 123 ? 123 123 % ? 123 123 % 123 123 % 123 123 % 123 123 123 123
table 123 : test results of the four algorithms using a gaussian kernel .
the targets of these bench - mark data sets were discretized by 123 equal - frequency bins .
the results are the average over 123 trials , along with the standard deviation .
gpr denotes the standard algorithm of gaussian process metric regression that treats the ordinal scales as continuous values .
nll denotes the negative logarithm of the likelihood in prediction .
we use the bold face to indicate the cases in which the average value is the lowest mean zero - one error of the four algorithms .
the symbols ? are used to indicate the cases in which the indicated entry is signicantly worse than the winning entry; a p - value threshold of 123 in wilcoxon rank sum test was used to decide statistical signicance .
for these data sets , the overall training time of map and ep approaches was substantially less than that of the svm approach .
this is because the map and ep approaches can tune the model parameters by gradient descent that usually required evidence evaluations at tens of different settings , whereas k - fold cross validation for the svm approach required evaluations at 123 different nodes of q on the grid for every fold .
for larger data sets , the svm approach may still have an advantage on training time due to the sparseness property in its computation .
gaussian processes for ordinal regression
123 collaborative filtering
collaborative ltering exploits correlations between ratings across a population of users .
the goal is to predict a persons rating on new items given the persons past ratings on similar items and the ratings of other people on all the items ( including the new item ) .
the ratings are ordered , making collaborative ltering an ordinal regression problem .
we carried out ordinal regression on a subset of the eachmovie data ( compaq , 123 ) . 123 the rates given by the user with id number 123 on 123 movies were used as the targets , in which the numbers of zero - to - ve star are 123 , 123 , 123 , 123 , 123 and 123 respectively .
we selected 123 users who contributed the most ratings on these 123 movies as the input features .
the ratings given by the 123 users on each movie were used as the input vector accordingly .
in the 123 123 input matrix , about 123% elements were observed .
we randomly selected a subset with size ( 123 , 123 , .
, 123 ) of the 123 movies for training , and then tested on the remaining movies .
at each size , the random selection was carried out 123 times
pearson correlation coefcient is the most popular correlation measure ( basilico and hofmann , 123 ) , which corresponds to a dot product between normalized rating vectors .
for instance , if applied to the movies , we can dene the so - called z - scores as r ( v , u ) ( v )
z ( v , u ) =
where u indexes users , v indexes movies , and r ( v , u ) is the rating on the movie v given by the user u .
( v ) and s ( v ) are the movie - specic mean and standard deviation respectively .
this correlation coefcient , dened as
k ( v , v123 ) = ( cid : 123 )
z ( v , u ) z ( v123 , u )
where ( cid : 123 ) u denotes summing over all the users , was used as the covariance / kernel function in our experiments for the three algorithms .
as not all ratings are observed in the input vectors , we con - sider two ad hoc strategies to deal with missing values : mean imputation and weighted low - rank approximation .
in the rst case , unobserved values are identied with the mean value , that means their corresponding z - score is zero .
in the second case , we applied the em procedure described by srebro and jaakkola ( 123 ) to ll in the missing data with the estimate .
in the input matrix , observed elements were weighted by one and missing data were given weight zero .
the low rank was xed at 123
in figure 123 , we present the test results of the two cases at different training data size .
using mean imputation , svm produced a bit more accurate results than gaussian processes on mean absolute error .
in the cases with low rank approximation as preprocessing , the performance of the three algorithms are highly competitive , and more interestingly , we observed about 123 im - provement on mean absolute error for all the three algorithms .
a serious treatment on the missing data could be an interesting research topic for future work .
123 gene expression analysis
singh et al .
( 123 ) carried out microarray expression analysis on 123 genes to identify genes that might anticipate the clinical behavior of prostate cancer .
fifty - two samples of prostate tumor were investigated .
for each sample , the gleason score ranging from 123 to 123 , was given by the
the compaq system research center ran the eachmovie service for 123 months .
123 users entered a total of
123 numeric ratings on 123 movies , i . e .
about 123% are rated by zero - to - ve star .
chu and ghahramani
with mean imputation
with weighted lowrank approximation
training data size
training data size
training data size
training data size
figure 123 : the test results of the three algorithms on the subset of eachmovie data over 123 trials .
the grouped boxes represent the results of svm ( left ) , map ( middle ) and ep ( right ) respectively at different training data size .
the notched - boxes have lines at the lower quartile , median , and upper quartile values .
the whiskers are lines extending from each end of the box to the most extreme data value within 123iqr ( interquartile range ) of the box .
outliers are data with values beyond the ends of the whiskers , which are displayed by dots .
the higher graphs are for the results of mean absolute error and the lower graphs are for mean zero - one error .
the cases of mean imputation are presented in the left graphs , and the cases with weighted low - rank approximation as preprocessing are presented in the right graphs .
gaussian processes for ordinal regression
the svm approach
the map approach
the ep approach
123 123 123 123 123 123 123 123
number of selected genes
the svm approach
123 123 123 123 123 123 123 123
number of selected genes
123 123 123 123 123 123 123 123
number of selected genes
the map approach
123 123 123 123 123 123 123 123
number of selected genes
123 123 123 123 123 123 123 123
number of selected genes
the ep approach
123 123 123 123 123 123 123 123
number of selected genes
figure 123 : the test results of the three algorithms using a linear kernel on the prostate cancer data of selected genes .
the horizonal axes are indexed on log123 scale .
the rungs in these boxes indicate the mean values , and the heights of these vertical boxes indicate the standard deviations over the 123 trials .
chu and ghahramani
pathologist reecting the level of differentiation of the glands in the prostate tumor .
predicting the gleason score from the gene expression data is thus a typical ordinal regression problem .
since only 123 samples had a score greater than 123 , we merged them as the top level , leading to three levels ( = 123 , = 123 , 123 ) with 123 , 123 and 123 samples respectively .
we randomly partitioned the data into 123 folds for training and test and repeated this partitioning 123 times independently .
an ard linear kernel k ( xi , x j ) = ( cid : 123 ) dv =123 j was used to evaluate feature relevance .
these ard parameters v ) were optimized by evidence maximization .
according to the optimal values of these ard parameters , the genes were ranked from irrelevant to relevant .
we then removed the irrelevant genes gradually based on the rank list .
the gene number was reduced from 123 to 123
at each number of selected genes , a linear kernel k ( xi , x j ) = ( cid : 123 ) dv =123 x j was used in the three algorithms for a fair comparison .
figure 123 presents the test results of the three algorithms for different numbers of selected genes .
we observed great and steady improvement using the subset of genes selected by the ard technique .
the best validation output is achieved around 123 top - ranked features .
in this case , with only 123 training samples , the bayesian approaches perform much better than the svm , and the ep approach is generally better than the map approach but the difference is not statistically
ordinal regression is an important supervised learning problem with properties of both metric re - gression and classication .
in this paper , we proposed a simple yet novel nonparametric bayesian approach to ordinal regression based on a generalization of the probit likelihood function for gaus - sian processes .
two approximate inference procedures were derived in detail for evidence evalua - tion and model adaptation .
the approach intrinsically incorporates ard feature selection and pro - vides probabilistic prediction .
the existent fast algorithms for gaussian processes can be adapted directly to tackle relatively large data sets .
experiments on benchmark and real - world data sets show that the generalization performance is competitive and often better than support vector methods .
the main part of this work was carried out at institute for pure and applied mathematics ( ipam ) of ucla .
we thank david l .
wild for stimulating this work and for many discussions .
we also thank david j .
mackay for valuable comments .
wei chu was supported by the national institutes of health and its national institute of general medical sciences division under grant number 123 p123 gm123
zoubin ghahramani was partially supported from cmu by darpa under the calo project .
the reviewers thoughtful comments are gratefully appreciated .
appendix a .
gradient formulae for evidence maximization
evidence maximization is equivalent to nding the minimizer of the negative logarithm of the evi - dence which can be written in an explicit expression as follows
ln p ( d|q )
` ( yi , fmap ( xi ) ) +
maps 123 f map +
ln|i + s
gaussian processes for ordinal regression
initialization choose a favorite gradient - descent optimization package
select the starting point q
for the optimization package
looping while the optimization package requests evidence / gradient evaluation at q
nd the map estimate by solving the convex programming problem ( 123 ) 123
evaluate the negative logarithm of the evidence ( 123 ) at the map 123
calculate the gradients with respect to q 123
feed the evidence and gradients to the optimization package
exit return the optimal q
found by the optimization package
table 123 : the outline of our algorithm for model adaptation using the map approach with laplace
we usually collect ( lnk , lns
r123 ) as the set of variables to tune .
this denition of tunable variables is helpful to convert the constrained optimization problem into an unconstrained optimization problem .
the outline of our algorithm for model adaptation is described in table 123
, b123 , lnd 123 ,
the derivatives of ln p ( d|q ) with respect to these variables can be derived as follows :
note that at the map estimate s 123 f map = ( cid : 123 ) n
runs from 123 to 123 , zi
is denoted as l given in the following :
ii , which is dened as in ( 123 ) , i . e
byi f ( xi )
f ( xi ) = 123
s 123 ( 123 ( v123 ) 123 + 123v123v123 + v123 v123 ) .
ln p ( d|q )
ln p ( d|q ) ln p ( d|q ) ln p ( d|q )
map + s ) 123
map + s ) 123s
s 123 f map
map + s ) 123s
map + s ) 123s
map + s ) 123s
( cid : 123 ) ( cid : 123 ) ( cid : 123 ) f = f map
` ( yi , f ( xi ) )
for more details , let us dene
123 ) f ( zi 123 ) f ( zi .
the ii - th entry of the diagonal matrix l byi123 f ( xi ) ii = 123 s 123 v123
the detailed derivatives are
s 123 ( v123 ) 123 + 123
chu and ghahramani
s 123 f .
k = l 123 ( l 123 + s ) 123 ` ( yi , f ( xi ) )
ii + 123
s = 123 s = l 123 ( l 123 +s ) 123s
f ( xi ) +
= l 123 ( l 123 + s ) 123s
s 123 ( 123v123v123 + 123 ( v123 ) 123v123 v123 + ( v123 ) 123 + v123 ) +
s , where y
is a column vector whose i - th element is 123
s 123 ( v123 v123v123
b , where y
b is a column vector whose i - th element is l
if yi > i ; if yi = i ;
if yi > i ; if yi = i ;
f ( xi ) +
` ( yi , f ( xi ) )
s 123 ( s123 123v123s123 123 ( v123 ) 123s123 s123 v123s123 ) .
i = l 123 ( l 123 + s ) 123s
ii i . e
s 123 ( ( v123 ) 123 + v123 )
s 123 ( v123s123 + s123 )
if yi > i ; if yi = i ;
d , where y
is a column vector whose i - th element is dened as
appendix b .
approximate posterior distribution by ep the expectation propagation algorithm attempts to approximate p ( f|d ) in form of a product of gaussian distributions q ( f ) = ( cid : 123 ) 123 pi ( f ( xi ) mi ) 123 ) .
the updating scheme is given as follows .
i=123 t ( f ( xi ) ) p ( f ) where t ( f ( xi ) ) = si exp ( 123
the initial states :
individual mean mi = 123 i ; individual inverse variance pi = 123 i ; individual amplitude si = 123 i ; posterior covariance a = ( s 123 + p ) 123 , where p = diag ( p123 , p123 , .
, pn ) ; posterior mean h = ap m , where m = ( m123 , m123 , .
, mn ) t .
looping i from 123 to n until there is no signicant change in ( mi , pi , si ) n
t ( f ( xi ) ) is removed from q ( f ) to get a leave - one - out posterior distribution q\i ( f ) having
t ( f ( xi ) ) in q ( f ) is updated by incorporating the message p ( yi| f ( xi ) ) into q\i ( f ) :
gaussian processes for ordinal regression
variance of f ( xi ) : l \i mean of f ( xi ) : h\i others with j 123= i : l \i
i = aii i = hi + l \i
i pi ( hi mi ) ; j = h j .
j = a j j and h\i
i , l \i i ) d f ( xi ) = f ( z123 ) f ( z123 ) zi = r p ( yi| f ( xi ) ) n ( f ( xi ) ; h\i i +s 123
where z123 =
and z123 =
i +s 123 i +s 123 ) ( cid : 123 ) z123n ( z123;123 , 123 ) z123n ( z123;123 , 123 ) f ( z123 ) f ( z123 ) i +s 123 ( cid : 123 ) n ( z123;123 , 123 ) n ( z123;123 , 123 ) f ( z123 ) f ( z123 )
i < 123
i = g 123 i + l \i i = h\i i = h\i i = ziql \i
note that pnew
i + 123exp ( cid : 123 ) g 123
i > 123 all the time , because 123 < u
i +s 123 and then l \i
mean h and covariance a as follows : i where r =
i pi , skip this sample and this updating; otherwise update ( pi , mi , si ) , the posterior a new = a r aiat hnew = h + h ai where h =
and ai is the i - th column of a .
i is dened as in ( 123 ) .
as a byproduct , we can get the approximate evidence p ( d|q ) at the ep solution , which can be
where b = ( cid : 123 )
i j ai j ( mi pi ) ( m j p j ) ( cid : 123 )
123 ( p 123 ) 123 ( s + p 123 )
appendix c .
gradient formulae for variational bound at the equilibrium of q ( f ) , the variational bound f ( q ) can be analytically calculated as follows :
f ( q ) =
z n ( f ( xi ) ; hi , aii ) ln ( p ( yi| f ( xi ) ) ) d f ( xi ) trace ( ( i + s
mt ( s + p 123 ) 123s ( s + p 123 ) 123m +
ln|i + s
chu and ghahramani
note that ( s + p 123 ) 123m can be directly obtained by ( g with respect to the variables ( lnk , lns , b123 , lnd 123 , .
, lnd log p ( f )
f ( q ) lnk = k z q ( f )
i ) dened as in ( 123 ) .
the gradient of f ( q ) r123 ) can be given in the following :
f ( q )
ht s 123 k ( cid : 123 ) +
k ( cid : 123 ) + trace ( cid : 123 ) ( p 123 + s ) 123 f ( q ) lns = s i=123r n ( f ( xi ) ; hi , aii ) ( 123yi<r ) r n ( cid : 123 ) f ( xi ) ; his 123+aiibyi ( 123<yir ) r n ( cid : 123 ) f ( xi ) ; his 123+aiibyi123 i=123r n ( f ( xi ) ; hi , aii ) ( 123yi<r ) r n ( f ( xi ) ; his 123+aiibyi ( 123<yir ) r n ( f ( xi ) ; his 123+aiibyi123 i=123r n ( f ( xi ) ; hi , aii ) ( i yi<r ) r n ( f ( xi ) ; his 123+aiibyi ( i <yir ) r n ( f ( xi ) ; his 123+aiibyi123
= ( cid : 123 ) n
ln p ( yi| f ( xi ) )
f ( q )
s 123h +
( p 123 + s ) 123m ,
mt ( p 123 + s ) 123
ln p ( yi| f ( xi ) )
d f ( xi )
byi f ( xi ) 123p ( s 123+aii )
byi123 f ( xi ) 123p ( s 123+aii )
123 ( s 123+aii ) ( cid : 123 )
p ( yi| f ( xi ) ) p ( yi| f ( xi ) )
d f ( xi )
d f ( xi ) ,
d f ( xi )
123p ( s 123+aii )
123p ( s 123+aii )
d f ( xi )
123 ( s 123+aii ) ( cid : 123 )
p ( yi| f ( xi ) p ( yi| f ( xi ) )
d f ( xi ) ,
ln p ( yi| f ( xi ) )
d f ( xi )
123p ( s 123+aii )
123p ( s 123+aii )
123 ( s 123+aii ) ( cid : 123 )
p ( yi| f ( xi ) p ( yi| f ( xi ) )
d f ( xi )
d f ( xi ) ,
dimensional integrals can be approximated using gaussian quadrature or calculated by romberg integration at some appropriate accuracy .
means summing over all the samples whose targets satisfy i < yi r , and these one -
