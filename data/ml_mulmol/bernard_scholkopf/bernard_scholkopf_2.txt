we consider the general problem of learning from labeled and unlabeled data , which is often called semi - supervised learning or transductive in - ference .
a principled approach to semi - supervised learning is to design a classifying function which is suf ( cid : 123 ) ciently smooth with respect to the intrinsic structure collectively revealed by known labeled and unlabeled points .
we present a simple algorithm to obtain such a smooth solution .
our method yields encouraging experimental results on a number of clas - si ( cid : 123 ) cation problems and demonstrates effective use of unlabeled data .
we consider the general problem of learning from labeled and unlabeled data .
given a point set x = fx123; : : : ; xl; xl+123; : : : ; xng and a label set l = f123; : : : ; cg; the ( cid : 123 ) rst l points have labels fy123; : : : ; ylg 123 l and the remaining points are unlabeled .
the goal is to predict the labels of the unlabeled points .
the performance of an algorithm is measured by the error rate on these unlabeled points only .
such a learning problem is often called semi - supervised or transductive .
since labeling often requires expensive human labor , whereas unlabeled data is far easier to obtain , semi - supervised learning is very useful in many real - world problems and has recently attracted a considerable amount of research ( 123 ) .
a typical application is web categorization , in which manually classi ( cid : 123 ) ed web pages are always a very small part of the entire web , and the number of unlabeled examples is large .
the key to semi - supervised learning problems is the prior assumption of consistency , which means : ( 123 ) nearby points are likely to have the same label; and ( 123 ) points on the same struc - ture ( typically referred to as a cluster or a manifold ) are likely to have the same label .
this argument is akin to that in ( 123 , 123 , 123 , 123 , 123 ) and often called the cluster assumption ( 123 , 123 ) .
note that the ( cid : 123 ) rst assumption is local , whereas the second one is global .
orthodox super - vised learning algorithms , such as k - nn , in general depend only on the ( cid : 123 ) rst assumption of to illustrate the prior assumption of consistency underlying semi - supervised learning , let us consider a toy dataset generated according to a pattern of two intertwining moons in figure 123 ( a ) .
every point should be similar to points in its local neighborhood , and furthermore , points in one moon should be more similar to each other than to points in the other moon .
the classi ( cid : 123 ) cation results given by the support vector machine ( svm ) with a rbf kernel
( a ) toy data ( two moons )
( b ) svm ( rbf kernel )
labeled point 123 labeled point +123
( c ) ideal classification
figure 123 : classi ( cid : 123 ) cation on the two moons pattern .
( a ) toy data set with two labeled points; ( b ) classifying result given by the svm with a rbf kernel; ( c ) k - nn with k = 123; ( d ) ideal classi ( cid : 123 ) cation that we hope to obtain .
and k - nn are shown in figure 123 ( b ) & 123 ( c ) respectively .
according to the assumption of consistency , however , the two moons should be classi ( cid : 123 ) ed as shown in figure 123 ( d ) .
the main differences between the various semi - supervised learning algorithms , such as spectral methods ( 123 , 123 , 123 ) , random walks ( 123 , 123 ) , graph mincuts ( 123 ) and transductive svm ( 123 ) , lie in their way of realizing the assumption of consistency .
a principled approach to formalize the assumption is to design a classifying function which is suf ( cid : 123 ) ciently smooth with respect to the intrinsic structure revealed by known labeled and unlabeled points .
here we propose a simple iteration algorithm to construct such a smooth function inspired by the work on spreading activation networks ( 123 , 123 ) and diffusion kernels ( 123 , 123 , 123 ) , recent work on semi - supervised learning and clustering ( 123 , 123 , 123 ) , and more speci ( cid : 123 ) cally by the work of zhu et al .
the keynote of our method is to let every point iteratively spread its label information to its neighbors until a global stable state is achieved .
we organize the paper as follows : section 123 shows the algorithm in detail and also discusses possible variants; section 123 introduces a regularization framework for the method; section 123 presents the experimental results for toy data , digit recognition and text classi ( cid : 123 ) cation , and section 123 concludes this paper and points out the next researches .
given a point set x = fx123; : : : ; xl; xl+123; : : : ; xng ( cid : 123 ) rm and a label set l = f123; : : : ; cg; the ( cid : 123 ) rst l points xi ( i ( cid : 123 ) l ) are labeled as yi 123 l and the remaining points xu ( l+123 ( cid : 123 ) u ( cid : 123 ) n ) are unlabeled .
the goal is to predict the label of the unlabeled points .
let f denote the set of n ( cid : 123 ) c matrices with nonnegative entries .
a matrix f = n ) t 123 f corresponds to a classi ( cid : 123 ) cation on the dataset x by labeling each 123 ; : : : ; f t point xi as a label yi = arg maxj ( cid : 123 ) c fij : we can understand f as a vectorial function f : x ! rc which assigns a vector fi to each point xi : de ( cid : 123 ) ne a n ( cid : 123 ) c matrix y 123 f with yij = 123 if xi is labeled as yi = j and yij = 123 otherwise .
clearly , y is consistent with the
initial labels according the decision rule .
the algorithm is as follows :
form the af ( cid : 123 ) nity matrix w de ( cid : 123 ) ned by wij = exp ( ( cid : 123 ) kxi ( cid : 123 ) xjk123=123 ( cid : 123 ) 123 ) if i 123= j 123
construct the matrix s = d ( cid : 123 ) 123=123w d ( cid : 123 ) 123=123 in which d is a diagonal matrix with
and wii = 123 :
its ( i; i ) - element equal to the sum of the i - th row of w :
in ( 123; 123 ) :
iterate f ( t + 123 ) = ( cid : 123 ) sf ( t ) + ( 123 ( cid : 123 ) ( cid : 123 ) ) y until convergence , where ( cid : 123 ) is a parameter 123
let f ( cid : 123 ) denote the limit of the sequence ff ( t ) g : label each point xi as a label
yi = arg maxj ( cid : 123 ) c f ( cid : 123 )
this algorithm can be understood intuitively in terms of spreading activation networks ( 123 , 123 ) from experimental psychology .
we ( cid : 123 ) rst de ( cid : 123 ) ne a pairwise relationship w on the dataset x with the diagonal elements being zero .
we can think that a graph g = ( v; e ) is de ( cid : 123 ) ned on x , where the the vertex set v is just x and the edges e are weighted by w : in the second step , the weight matrix w of g is normalized symmetrically , which is necessary for the convergence of the following iteration .
the ( cid : 123 ) rst two steps are exactly the same as in spectral clustering ( 123 ) .
during each iteration of the third step each point receives the information from its neighbors ( ( cid : 123 ) rst term ) , and also retains its initial information ( second term ) .
the parameter ( cid : 123 ) speci ( cid : 123 ) es the relative amount of the information from its neighbors and its initial label information .
it is worth mentioning that self - reinforcement is avoided since the diagonal elements of the af ( cid : 123 ) nity matrix are set to zero in the ( cid : 123 ) rst step .
moreover , the information is spread symmetrically since s is a symmetric matrix .
finally , the label of each unlabeled point is set to be the class of which it has received most information during the iteration process .
let us show that the sequence ff ( t ) g converges and f ( cid : 123 ) = ( 123 ( cid : 123 ) ( cid : 123 ) ) ( i ( cid : 123 ) ( cid : 123 ) s ) ( cid : 123 ) 123y : without loss of generality , suppose f ( 123 ) = y : by the iteration equation f ( t + 123 ) = ( cid : 123 ) sf ( t ) + ( 123 ( cid : 123 ) ( cid : 123 ) ) y used in the algorithm , we have
f ( t ) = ( ( cid : 123 ) s ) t ( cid : 123 ) 123y + ( 123 ( cid : 123 ) ( cid : 123 ) )
since 123 < ( cid : 123 ) < 123 and the eigenvalues of s in ( - 123 , 123 ) ( note that s is similar to the stochastic matrix p = d ( cid : 123 ) 123w = d ( cid : 123 ) 123=123sd123=123 ) ,
( ( cid : 123 ) s ) t ( cid : 123 ) 123 = 123; and lim
( ( cid : 123 ) s ) i = ( i ( cid : 123 ) ( cid : 123 ) s ) ( cid : 123 ) 123 :
f ( cid : 123 ) = lim
f ( t ) = ( 123 ( cid : 123 ) ( cid : 123 ) ) ( i ( cid : 123 ) ( cid : 123 ) s ) ( cid : 123 ) 123y;
for classi ( cid : 123 ) cation , which is clearly equivalent to
f ( cid : 123 ) = ( i ( cid : 123 ) ( cid : 123 ) s ) ( cid : 123 ) 123y :
now we can compute f ( cid : 123 ) directly without iterations .
this also shows that the iteration result does not depend on the initial value for the iteration .
in addition , it is worth to notice that ( i ( cid : 123 ) ( cid : 123 ) s ) ( cid : 123 ) 123 is in fact a graph or diffusion kernel ( 123 , 123 ) .
now we discuss some possible variants of this method .
the simplest modi ( cid : 123 ) cation is to repeat the iteration after convergence , i . e .
f ( cid : 123 ) = ( i ( cid : 123 ) ( cid : 123 ) s ) ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( i ( cid : 123 ) ( cid : 123 ) s ) ( cid : 123 ) 123y = ( i ( cid : 123 ) ( cid : 123 ) s ) ( cid : 123 ) py; where p is an arbitrary positive integer .
in addition , since that s is similar to p; we can consider to substitute p for s in the third step , and then the corresponding closed form is f ( cid : 123 ) = ( i ( cid : 123 ) ( cid : 123 ) p ) ( cid : 123 ) 123y : it is also interesting to replace s with p t ; the transpose of p : then the classifying function is f ( cid : 123 ) = ( i ( cid : 123 ) ( cid : 123 ) p t ) ( cid : 123 ) 123y : it is not hard to see this is equivalent to f ( cid : 123 ) = ( d ( cid : 123 ) ( cid : 123 ) w ) ( cid : 123 ) 123y : we will compare these variants with the original algorithm in the
123 regularization framework
here we develop a regularization framework for the above iteration algorithm .
the cost function associated with f is de ( cid : 123 ) ned to be
q ( f ) =
f 123f q ( f ) :
f ( cid : 123 ) = arg min
xi=123 ( cid : 123 ) ( cid : 123 ) fi ( cid : 123 ) yi ( cid : 123 ) ( cid : 123 )
where ( cid : 123 ) > 123 is the regularization parameter .
then the classifying function is
the ( cid : 123 ) rst term of the right - hand side in the cost function is the smoothness constraint , which means that a good classifying function should not change too much between nearby points .
the second term is the ( cid : 123 ) tting constraint , which means a good classifying function should not change too much from the initial label assignment .
the trade - off between these two competing constraints is captured by a positive parameter ( cid : 123 ) : note that the ( cid : 123 ) tting constraint contains labeled as well as unlabeled data .
we can understand the smoothness term as the sum of the local variations , i . e .
the local changes of the function between nearby points .
as we have mentioned , the points involving pairwise relationships can be be thought of as an undirected weighted graph , the weights of which represent the pairwise relationships .
the local variation is then in fact measured on each edge .
we do not simply de ( cid : 123 ) ne the local variation on an edge by the difference of the function values on the two ends of the edge .
the smoothness term essentially splits the function value at each point among the edges attached to it before computing the local changes , and the value assigned to each edge is proportional to its weight .
differentiating q ( f ) with respect to f , we have
@f ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) f =f ( cid : 123 )
which can be transformed into
f ( cid : 123 ) ( cid : 123 ) let us introduce two new variables ,
= f ( cid : 123 ) ( cid : 123 ) sf ( cid : 123 ) + ( cid : 123 ) ( f ( cid : 123 ) ( cid : 123 ) y ) = 123;
123 + ( cid : 123 )
sf ( cid : 123 ) ( cid : 123 )
123 + ( cid : 123 )
y = 123 :
123 + ( cid : 123 )
; and ( cid : 123 ) =
123 + ( cid : 123 )
note that ( cid : 123 ) + ( cid : 123 ) = 123 : then
( i ( cid : 123 ) ( cid : 123 ) s ) f ( cid : 123 ) = ( cid : 123 ) y; f ( cid : 123 ) = ( cid : 123 ) ( i ( cid : 123 ) ( cid : 123 ) s ) ( cid : 123 ) 123y :
since i ( cid : 123 ) ( cid : 123 ) s is invertible , we have which recovers the closed form expression of the above iteration algorithm .
similarly we can develop the optimization frameworks for the variants f ( cid : 123 ) = ( i ( cid : 123 ) ( cid : 123 ) p ) ( cid : 123 ) 123y and f ( cid : 123 ) = ( d ( cid : 123 ) ( cid : 123 ) w ) ( cid : 123 ) 123y .
we omit the discussions due to lack of space .
we used k - nn and one - vs - rest svms as baselines , and compared our method to its two variants : ( 123 ) f ( cid : 123 ) = ( i ( cid : 123 ) ( cid : 123 ) p ) ( cid : 123 ) 123y ; and ( 123 ) f ( cid : 123 ) = ( d ( cid : 123 ) ( cid : 123 ) w ) ( cid : 123 ) 123y : we also compared to zhu et al . s harmonic gaussian ( cid : 123 ) eld method coupled with the class mass normalization ( cmn ) ( 123 ) , which is closely related to ours .
to the best of our knowledge , there is no reliable approach for model selection if only very few labeled points are available .
hence we let all algorithms use their respective optimal parameters , except that the parameter ( cid : 123 ) used in our methods and its variants was simply ( cid : 123 ) xed at 123 .
( a ) t = 123
( b ) t = 123
( c ) t = 123
( d ) t = 123
figure 123 : classi ( cid : 123 ) cation on the pattern of two moons .
the convergence process of our iteration algorithm with t increasing from 123 to 123 is shown from ( a ) to ( d ) .
note that the initial label information are diffused along the moons .
figure 123 : the real - valued classifying function becomes ( cid : 123 ) atter and ( cid : 123 ) atter with respect to the two moons pattern with increasing t .
note that two clear moons emerge in ( d ) .
( a ) svm ( rbf kernel )
( b ) smooth with global consistency
labeled point 123 labeled point +123
figure 123 : smooth classi ( cid : 123 ) cation results given by supervised classi ( cid : 123 ) ers with the global con - sistency : ( a ) the classi ( cid : 123 ) cation result given by the svm with a rbf kernel; ( b ) smooth the result of the svm using the consistency method .
123 toy problem
i123 + f ( cid : 123 )
i123 ( cid : 123 ) f ( cid : 123 )
in this experiment we considered the toy problem mentioned in section 123 ( figure 123 ) .
the af ( cid : 123 ) nity matrix is de ( cid : 123 ) ned by a rbf kernel but the diagonal elements are set to zero .
the convergence process of our iteration algorithm with t increasing from 123 to 123 is shown in figure 123 ( a ) - 123 ( d ) .
note that the initial label information are diffused along the moons .
the assumption of consistency essentially means that a good classifying func - tion should change slowly on the coherent structure aggregated by a large amount of data .
this can be illustrated by this toy problem very clearly .
let us de ( cid : 123 ) ne a function i123 ) and accordingly the decision function is sign ( f ( xi ) ) ; f ( xi ) = ( f ( cid : 123 ) which is equivalent to the decision rule described in section 123
in figure 123 , we show that f ( xi ) becomes successively ( cid : 123 ) atter with respect to the two moons pattern from figure 123 ( a ) - 123 ( d ) with increasing t .
note that two clear moons emerge in the figure 123 ( d ) .
the basic idea of our method is to construct a smooth function .
it is natural to consider using this method to improve a supervised classi ( cid : 123 ) er by smoothing its classifying result .
in other words , we use the classifying result given by a supervised classi ( cid : 123 ) er as the input of our algorithm .
this conjecture is demonstrated by a toy problem in figure 123
figure 123 ( a ) is the classi ( cid : 123 ) cation result given by the svm with a rbf kernel .
this result is then assigned to y in our method .
the output of our method is shown in figure 123 ( b ) .
note that the points classi ( cid : 123 ) ed incorrectly by the svm are successfully smoothed by the consistency method .
123 digit recognition
in this experiment , we addressed a classi ( cid : 123 ) cation task using the usps handwritten 123x123 digits dataset .
we used digits 123 , 123 , 123 , and 123 in our experiments as the four classes .
there are 123 , 123 , 123 , and 123 examples for each class , for a total of 123
the k in k - nn was set to 123
the width of the rbf kernel for svm was set to 123 , and for the harmonic gaussian ( cid : 123 ) eld method it was set to 123 .
in our method and its variants , the af ( cid : 123 ) nity matrix was constructed by the rbf kernel with the same width used as in the harmonic gaussian method , but the diagonal elements were set to 123
the test errors averaged over 123 trials are summarized in the left panel of figure 123
samples were chosen so that they contain at least one labeled point for each class .
our consistency method and one of its variant are clearly superior to the orthodox supervised learning algorithms k - nn and svm , and also better than the harmonic gaussian method .
note that our approach does not require the af ( cid : 123 ) nity matrix w to be positive de ( cid : 123 ) nite .
this enables us to incorporate prior knowledge about digit image invariance in an elegant way , e . g . , by using a jittered kernel to compute the af ( cid : 123 ) nity matrix ( 123 ) .
other kernel methods are
knn ( k = 123 ) svm ( rbf kernel ) variant consistency ( 123 ) variant consistency ( 123 )
knn ( k = 123 ) svm ( rbf kernel ) variant consistency ( 123 ) variant consistency ( 123 )
# labeled points
# labeled points
figure 123 : left panel : the error rates of digit recognition with usps handwritten 123x123 digits dataset for a total of 123 ( a subset containing digits from 123 to 123 ) .
right panel : the error rates of text classi ( cid : 123 ) cation with 123 document vectors in a 123 - dimensional space .
samples are chosen so that they contain at least one labeled point for each class .
known to have problems with this method ( 123 ) .
in our case , jittering by 123 pixel translation leads to an error rate around 123 for 123 labeled points .
123 text classi ( cid : 123 ) cation
in this experiment , we investigated the task of text classi ( cid : 123 ) cation using the 123 - newsgroups dataset .
we chose the topic rec which contains autos , motorcycles , baseball , and hockey from the version 123 - news - 123
the articles were processed by the rainbow software package with the following options : ( 123 ) passing all words through the porter stemmer before counting them; ( 123 ) tossing out any token which is on the stoplist of the smart system; ( 123 ) skipping any headers; ( 123 ) ignoring words that occur in 123 or fewer documents .
no further preprocessing was done .
removing the empty documents , we obtained 123 document vectors in a 123 - dimensional space .
finally the documents were normalized into tfidf representation .
the distance between points xi and xj was de ( cid : 123 ) ned to be d ( xi; xj ) = 123 ( cid : 123 ) hxi; xji=kxikkxjk ( 123 ) .
the k in k - nn was set to 123 : the width of the rbf kernel for svm was set to 123 : 123 , and for the harmonic gaussian method it was set to 123 : 123
in our methods , the af ( cid : 123 ) nity matrix was constructed by the rbf kernel with the same width used as in the harmonic gaussian method , but the diagonal elements were set to 123
the test errors averaged over 123 trials are summarized in the right panel of figure 123
samples were chosen so that they contain at least one labeled point for each class .
it is interesting to note that the harmonic method is very good when the number of labeled points is 123 , i . e .
one labeled point for each class .
we think this is because there are almost equal proportions of different classes in the dataset , and so with four labeled points , the pro - portions happen to be estimated exactly .
the harmonic method becomes worse , however , if slightly more labeled points are used , for instance , 123 labeled points , which leads to pretty poor estimation .
as the number of labeled points increases further , the harmonic method works well again and somewhat better than our method , since the proportions of classes are estimated successfully again .
however , our decision rule is much simpler , which in fact corresponds to the so - called naive threshold , the baseline of the harmonic method .
the key to semi - supervised learning problems is the consistency assumption , which essen - tially requires a classifying function to be suf ( cid : 123 ) ciently smooth with respect to the intrinsic structure revealed by a huge amount of labeled and unlabeled points .
we proposed a sim - ple algorithm to obtain such a solution , which demonstrated effective use of unlabeled data in experiments including toy data , digit recognition and text categorization .
in our further research , we will focus on model selection and theoretic analysis .
we would like to thank vladimir vapnik , olivier chapelle , arthur gretton , and andre elis - seeff for their help with this work .
we also thank andrew ng for helpful discussions about spectral clustering , and the anonymous reviewers for their constructive comments .
special thanks go to xiaojin zhu , zoubin ghahramani , and john lafferty who communicated with us on the important post - processing step class mass normalization used in their method and also provided us with their detailed experimental data .
