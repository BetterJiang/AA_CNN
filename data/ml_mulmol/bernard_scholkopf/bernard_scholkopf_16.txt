abstract .
practical experience has shown that in order to obtain the best possible performance , prior knowledge about invariances of a classication problem at hand ought to be incorporated into the training procedure .
we describe and review all known methods for doing so in support vector machines , provide experimental results , and discuss their respective merits .
one of the signicant new results reported in this work is our recent achievement of the lowest reported test error on the well - known mnist digit recognition benchmark task , with svm training times that are also signicantly faster than previous svm methods .
support vector machines , invariance , prior knowledge , image classication , pattern recognition
in 123 , lecun et al .
published a pattern recognition performance comparison noting the
the optimal margin classier has excellent accuracy , which is most remarkable , because unlike the other high performance classiers , it does not include a priori know - ledge about the problem .
in fact , this classier would do just as well if the image pixels were permuted by a xed mapping .
( . . . ) however , improvements are expected as the technique is relatively new .
two things have changed in the years since this statement was made .
first , optimal margin classiers , or support vector machines ( svms ) ( boser , guyon , & vapnik , 123; cortes & vapnik , 123; vapnik , 123 ) , have turned into a mainstream method which is part of the standard machine learning toolkit .
second , methods for incorporating prior knowledge into optimal margin classiers have become part of the standard sv methodology .
these two things are actually closely related .
initially , svms had been considered a theoretically elegant spin - off of the general but , allegedly , largely useless vc - theory of statistical learning .
in 123 , using the rst methods for incorporating prior knowledge ( scholkopf , burges , & vapnik , 123 ) , svms became competitive with the state of the art in the handwritten digit classication benchmarks ( lecun et al . , 123 ) that were popularized
decoste and b .
sch olkopf
in the machine learning community by at&t and bell labs .
at that point , practitioners who are not interested in theory , but in results , could no longer ignore svms .
in this sense , the methods to be described below actually helped pave the way to make svm a widely used machine learning tool .
the present paper tries to provide a snapshot of the state of the art in methods to incorpo - rate prior knowledge about invariances of classication problems .
it gathers material from various sources .
partly , it reviews older material that has appeared elsewhere ( scholkopf , burges , & vapnik , 123 , 123a ) , but never in a journal paper , and partly , it presents new techniques , applications , and experimental results .
it is organized as follows .
the next sec - tion introduces the concept of prior knowledge , and discusses what types of prior knowledge are used in pattern recognition .
the present paper focuses on prior knowledge about invari - ances , and section 123 describes methods for incorporating invariances into svms .
section 123 introduces a specic way which combines a method for invariant svms with the widely used smo training algorithm .
section 123 reports experimental results on all presented methods , and section 123 discusses our ndings .
prior knowledge in pattern recognition
by prior knowledge we refer to all information about the learning task which is available in addition to the training examples .
in this most general form , only prior knowledge makes it possible to generalize from the training examples to novel test examples .
for instance , any classiers incorporate general smoothness assumptions about the prob - lem .
a test patterns which is similar to one of the training examples will thus tend to be assigned to the same class .
for svms , it can be shown that using a kernel function k amounts to enforcing smoothness with a regularizer ( cid : 123 ) pf ( cid : 123 ) 123 , where f is the estimated function , and k is a greens function of p is the adjoint operator of p ( ( smola , scholkopf , & muller , 123; girosi , 123 ) , cf .
also poggio and girosi ( 123 ) ) .
in a bayesian maximum - a - posteriori setting , this corresponds to a smoothness prior of exp ( ( cid : 123 ) pf ( cid : 123 ) 123 ) ( kimeldorf &
p , where p
a second method for incorporating prior knowledge , already somewhat more specic , consists of selecting features which are thought to be particularly informative or reliable for the task at hand .
for instance , in handwritten character recognition , correlations between image pixels that are nearby tend to be more reliable than the ones of distant pixels .
the intuitive reason for this is that variations in writing style tends to leave the local structure of a handwritten digit fairly unchanged , while the global structure is usually quite variable .
in the case of svms , this type of prior knowledge is readily incorporated by designing poly - nomial kernels which compute mainly products of nearby pixels ( scholkopf et al . , 123a ) ( cf .
table 123 ) .
while the example given here applies to handwritten character recognition , it is clear that a great deal of problems have a similar local structure , for instance , problems of computational biology involving nucleotide sequences .
zien et al .
( 123 ) have used this analogy to engineer kernels that outperform other approaches in the problem of recognizing translation initiation sites on dna or mrna sequences , i . e . , positions which mark regions coding proteins .
haussler ( 123 ) has gone further than that and constructed convolutional kernels which compute features targeted at problems such as those of bioinformatics .
training invariant support vector machines
one way to look at feature selection is that it changes the representation of the data , and in this , it is not so different from another method for incorporating prior knowledge in svms that has recently attracted attention .
in this method , it is assumed that we have knowledge about probabilistic models generating the data .
specically , let p ( x | ) be a generative model that characterizes the probability of a pattern x given the underlying parameter .
it is possible to construct a class of kernels which are invariant with respect to reparametrizations of and which , loosely speaking , have the property that k ( x , x subject to the assumption that they both stem from the generative the similarity of x and x model .
these kernels are called fisher kernels ( jaakkola & haussler , 123 ) .
it can be shown that they induce regularizers which , in turn , take account of the underlying model p ( x | ) ( oliver , scholkopf , & smola , 123 ) .
a different approach to designing kernels based on probabilistic models is the one of watkins ( 123 ) .
finally , we get to the type prior knowledge that the present paper will deal with : prior knowledge about invariances .
for instance , in image classication tasks , there exist trans - formations which leave class membership invariant ( e . g .
translations ) .
incorporating invariances into svms
in the present section , we describe methods for incorporating invariances into svms .
we distinguish three types of methods , to be addressed in the following three subsections :
engineer kernel functions which lead to invariant svms generate articially transformed examples from the training set , or subsets thereof ( e . g .
the set of svs )
combine the two approaches by making the transformation of the examples part of the
this paper will mainly focus on the latter two methods .
however , for completeness , we also include some material on the rst method , which we will presently describe .
invariant kernel functions
this section briey reviews a method for engineering kernel functions leading to invariant svms .
following scholkopf et al .
( 123a ) , we consider linear decision functions f ( x ) = sgn ( g ( x ) ) , where
vi ( bx bxi ) + b .
g ( x ) : = ( cid : 123 ) ( cid : 123 )
g ( ltx j )
to get local invariance under transformations forming a lie group ( lt ) , 123 one can minimize
decoste and b .
sch olkopf
one can show that this is equivalent to performing the usual svm training after prepro - cessing the data with
in other words : the modication of the kernel function is equivalent to whitening the data with respect to the covariance matrix of the vectors | t=123ltxi .
to get a clearer un - derstanding of what this means , let us diagonalize b using the matrix d containing the eigenvalues of c , as 123 = sd
the preprocessing of the data thus consists of projecting x onto the eigenvectors of c , and then dividing by the square roots of the eigenvalues .
in other words , the directions of |t=123ltxthe directions that are most affected by the transformation are scaled back .
note that the leading s in ( 123 ) can actually be omitted , since it is unitary ( since c is symmetric ) , and it thus does not affect the value of the dot product .
moreover , in practice , it is advisable to use c : = ( 123 ) c + i rather than c .
this way , we can balance the two goals of maximizing the margin and of getting an invariant decision function by the parameter ( 123 , 123 ) .
in the nonlinear case , the above method still applies , using d
scholkopf et al .
( 123a ) have obtained performance improvements by applying the above technique in the linear case .
however , these improvements are not nearly as large as the ones that we will get with the methods to be described in the next section , 123 and thus we put the main emphasis in this study on the latter .
( cid : 123 ) ( x ) .
here , is a feature map corresponding to the kernel chosen , i . e . , a ( usually nonlinear ) map satisfy - ing k ( x , x ( cid : 123 ) ) = ( ( x ) ( x ( cid : 123 ) ) ) .
to compute projections of the mapped data ( x ) onto the eigenvectors of s , one needs to essentially carry out kernel pca on s ( cf .
( scholkopf , 123; chapelle & scholkopf , 123 ) ) .
for further material on methods to construct invariant kernel functions , cf .
vapnik ( 123 ) and burges ( 123 ) .
virtual examples and support vectors
one way to make classiers invariant is to generate articial training examples , or virtual examples , by transforming the training examples accordingly ( baird , 123; poggio & vetter , 123 ) .
it is then hoped that the learning machine will extract the invariances from the articially enlarged training data .
simard et al .
( 123 ) report that , not surprisingly , training a neural network on the arti - cially enlarged data set is signicantly slower , due to both correlations in the articial data
training invariant support vector machines
and the increase in training set size .
if the size of a training set is multiplied by a number of desired invariances ( by generating a corresponding number of articial examples for each training pattern ) , the resulting training sets can get rather large ( as the ones used by drucker , schapire , & simard , 123 ) .
however , the method of generating virtual examples has the advantage of being readily implemented for all kinds of learning machines and symmetrics , even discrete ones such as reections ( which do not form lie groups ) .
it would thus be desirable to construct a method which has the advantages of the virtual examples approach without its computational cost .
the virtual sv method ( scholkopf , burges , & vapnik , 123 ) , to be described in the present section , retains the exibility and simplicity of virtual examples approaches , while cutting down on their computational cost signicantly .
in scholkopf , burges , and vapnik ( 123 ) and vapnik ( 123 ) , it was observed that the sv set contains all information necessary to solve a given classication task .
it particular , it was possible to train any one of three different types of sv machines solely on the sv set extracted by another machine , with a test performance not worse than after training on the full databasesv sets seemed to be fairly robust characteristics of the task at hand .
this led to the conjecture that it might be sufcient to generate virtual examples from the support vectors only .
after all , one might hope that it does not add much information to generate virtual examples of patterns which are not close to the boundary .
in high - dimensional cases , however , care has to be exercised regarding the validity of this intuitive picture .
experimental tests on high - dimensional real - world problems hence were imperative , and they have conrmed that the method works very well ( scholkopf , burges , & vapnik , 123 ) .
it proceeds as follows ( cf .
gure 123 ) :
train a support vector machine to extract the support vector set 123
generate articial examples , termed virtual support vectors , by applying the desired
invariance transformations to the support vectors
train another support vector machine on the generated examples . 123
if the desired invariances are incorporated , the curves obtained by applying lie symmetry transformations to points on the decision surface should have tangents parallel to the latter ( simard et al . , 123 ) .
if we use small lie group transformations to generate the virtual examples , this implies that the virtual support vectors should be approximately as close to the decision surface as the original support vectors .
hence , they are fairly likely to become support vectors after the second training run; in this sense , they add a considerable amount of information to the training set .
however , as noted above , the method is also applicable if the invariance transformations do not form lie groups .
for instance , performance improvements in object recognition of bilaterally symmetric objects have been obtained using reections ( scholkopf , 123 ) .
kernel jittering
an alternative to the vsv approach of pre - expanding a training set by applying vari - ous transformations is to perform those transformations inside the kernel function itself ( decoste & burl , 123; simard , lecun , & denker , 123 ) .
decoste and b .
sch olkopf
figure 123
suppose we have prior knowledge indicating that the decision function should be invariant with respect to horizontal translations .
the true decision boundary is drawn as a dotted line ( top left ) ; however , as we are just given a limited training sample , different separating hyperplanes are conceivable ( top right ) .
the sv algorithm nds the unique separating hyperplane with maximal margin ( bottom left ) , which in this case is quite different from the true boundary .
for instance , it would lead to wrong classication of the ambiguous point indicated by the question mark .
making use of the prior knowledge by generating virtual support vectors from the support vectors found in a rst training run , and retraining on these , yields a more accurate decision boundary ( bottom right ) .
note , moreover , that for the considered example , it is sufcient to train the sv machine only on virtual examples generated from the support vectors from scholkopf , 123
in particular , we have recently explored the notion of jittering kernels , in which any two given examples are explicitly jittered around in the space of discrete invariance distortions until the closest match between them is found .
for any kernel k ( xi , x j ) ki j suitable for traditional use in a svm , consider a jittering kernel form k j ( xi , x j ) k j
i j , dened procedurally as follows :
training invariant support vector machines
consider all jittered forms123 of example xi ( including itself ) in turn , and select the one ( xq ) closest to xj ; specically , select xq to minimize the metric distance between xq and x j in the space induced by the kernel .
this distance is given by : 123
kqq 123kq j + k j j .
= kq j .
let k j
multiple invariances ( e . g . , both translations and rotations ) could be considered as well , by considering cross - products ( i . e .
all valid combinations ) during the generation of all jittered forms of xi in step 123 above .
however , we have not yet investigated the practicality of such multiple invariance via kernel jittering in any empirical work to date .
for some kernels , such as radial - basis functions ( rbfs ) , simply selecting the maximum kq j value to be the value for k j i j sufces , since the kqq and k j j terms are constant ( e . g .
123 ) in that case .
this similarly holds for translation jitters , as long as sufcient padding exists so that no image pixels fall off the image after translations .
in general , a jittering kernel may have to consider jittering either or both examples .
however , for symmetric invariances such as translation , it sufces to jitter just one .
we refer to the use of jittering kernels as the jsv approach .
a major motivation for considering jsv approachs is that vsv approaches scale at least quadratically in the number ( j ) of jitters considered , whereas under favorable conditions jittering kernel approaches can scale only linearly in j , as discussed below .
jittering kernels are j times more expensive to compute than non - jittering kernels , since each k j i j computation involves nding a minimum over j ki j computations .
however , the potential benet is that the training set can be j times smaller than the corresponding vsv method , since the vsv approach can expand the training set by a factor of j .
known svm training methods scale at least quadratically in the number of training examples .
thus , the potential net gain is that jsv training may only scale linearly in j instead of quadratically as in vsv .
furthermore , through comprehensive use of kernel caching , as is common in modern practical svm implementations , even the factor of j slow - down in kernel computations using jittering kernels may be largely amortized away , either over multiple trainings ( e . g .
for different regularization c values ) or within one training ( simply because the same kernel value is often requested many times during training ) .
as long as the distance metric conditions are satised under such jittering ( i . e .
non - negative , symmetry , triangularity inequality ) , the kernel matrix dened by k j i j will be positive denite and suitable for traditional use in svm training ( i . e .
it will satisfy mercers conditions , at least for the given set of training examples ) .
in practice , those conditions seem to almost always be satised and we rarely detect the symptoms of a non - positive denite kernel matrix during svm training with jittering kernels .
the smo method seems to tolerate such minor degrees of non - positivity , although , as discussed in platt ( 123 ) this still somewhat reduces convergence speed .
this rarity seems particularly true for simple translation invariances which we have focused on to date in empirical work .
the one exception is that the triangular inequality is sometimes
decoste and b .
sch olkopf
violated .
for example , imagine three simple images a , b , and c consisting of a single row of three binary pixels , with a= ( 123 , 123 , 123 ) , b= ( 123 , 123 , 123 ) , and c= ( 123 , 123 , 123 ) .
the minimal be 123
however , the distance between a and c will be positive ( e . g .
d ( a , c ) = jittered distances ( under 123 - pixel translation ) between a and b and between b and c will 123 for a linear kernel ) .
thus , the triangular inequality d ( a , b ) + d ( b , c ) d ( a , c ) is violated in
note that with a sufciently large jittering set ( i . e .
such as one including both 123 - pixel and 123 - pixel translations for the above example ) , the triangularity inequality is not violated .
we suspect that for reasonably complete jittering sets , the odds of triangularity inequality violations will become small .
based on our limited preliminary experiments with kernel jittering to date , it is still unclear how much impact any such violations will typically have on generalization performance in practice .
jittering kernels have one other potential disadvantage compared to vsv approaches : the kernels must continue to jitter at test time .
in contrast , the vsv approach effectively compiles the relevant jittering into the nal set of svs it produces .
in cases where the nal jsv sv set size is much smaller than the nal vsv sv set size , the jsv approach can actually be as fast or even faster at test time .
we have indeed observed this result to be frequent , at least for the relatively small tests tried to date .
we do not yet know how common this case is for large - scale applications .
efcient invariance training
recent developments in decomposition methods for svm training have demonstrated sig - nicant improvements in both space and time costs for training svms on many tasks .
in particular , the smo ( platt , 123 ) and svmlight ( joachims , 123 ) implementations have be - come popular practical methods for large - scale svm training .
in this section , we discuss some improvements to the style of svm training represented by approaches such as smo and svmlight .
we have found these improvements to lead to signicantly faster training times , in some cases by more than an order of magnitude .
our specic implementation and tests are based on enhancements to the variant of smo described in keerthi et al .
( 123 ) .
however , these ideas should be applicable to the original smo specication , as well as svmlight .
for simplicity , we will cast our discussion below in terms of the well - known smo specication ( platt , 123 ) .
for the sake of the rest of this section , we assume the svm training task consists of the
following quadratic programming ( qp ) dual formulation :
i j yi y j k ( xi , x j )
123 i c , i yi = 123 ,
training invariant support vector machines
where ( cid : 123 ) is the number of training examples , yi is the label ( +123 for positive example , 123 for negative ) for the ith training example ( xi ) , and k ( xi , x j ) denotes the value of the svm kernel function for ith and j - th examples .
the output prediction of the svm , for any example x is
f ( x ) = sign
i yi k ( x , xi ) + b
where scalar b ( bias ) and vector of alphas ( of length ( cid : 123 ) ) are the variables determined by the above qp optimization problem .
key efciency issue : maximizing cache reuse
employing general qp solvers to the svm optimization problem typically involves o ( ( cid : 123 ) 123 ) space and o ( ( cid : 123 ) 123 ) time complexities .
smo can avoid the o ( ( cid : 123 ) 123 ) space cost by avoiding the need for an explicit full kernel matrixit computes ( and recomputes ) elements as needed .
evaluating the current svm outputs during training for each of the ( cid : 123 ) examples involves ( cid : 123 ) summations , each over all current support vectors .
thus , smo ( and any other approach which similarly checks kkt conditions during training ) necessarily suffers time complexity of at least o ( l ( cid : 123 ) ) , where l is the nal number of support vectors , since it would require at least one nal full pass over all ( cid : 123 ) examples simply to check that all kkt conditions are
in practice , computing elements of the kernel matrix often dominates training time .
the same element will typically be required many times , since many ( e . g .
tens or hundreds ) of passes over the training set may be required before convergence to the nal solution .
thus , modern smo implementations try to cache as many kernel computations as possible .
however , caching all computed kernels is generally not possible , for two key reasons :
l is often a signicant fraction of ( cid : 123 ) ( e . g .
123% or more ) and thus the space required to
avoid any kernel recomputations would often prohibitively be o ( ( cid : 123 ) 123 ) .
the intermediate ( working ) set of ( candidate ) support vectors is typically much larger
than the nal size l . 123
since full passes over all examples are required in smo whenever the working set of candidate support vectors becomes locally optimal , the kernel cache is often implemented simply as the most frequently or recently accessed rows of the implicit full kernel matrix .
for instance , for the 123 , 123 training examples of the mnist data set ( to be discussed in section 123 ) , 123 megabytes of computer memory reserved for the kernel cache , and ieee single precision ( 123 byte ) representations of the kernel values , only about 123 rows can be cached at one time ( e . g .
only about half of all the support vectors for some of the mnist binary recognition tasks ) .
of particular concern is that during smo training , when the nal set of support vectors is not yet known , it turns out to be common for examples which will eventually not be support vectors to end up grabbing and hoarding cache rows rst , requiring other kernel
decoste and b .
sch olkopf
rows ( including those of nal support vectors ) to be continually recomputed until those examples cease to be support vectors and release their cache memory .
we will refer to this problem as intermediate support vector bulge .
this problem is particularly critical for vsv methods , since they greatly enlarge the number of effective training examples .
addressing this key issue will be the subject of section 123 .
digestion : reducing intermediate sv bulge
smo alternates between full and inbounds iterations , in which either all examples or just the examples with alphas between 123 and c are considered , respectively .
in each inbounds stage , optimization continues until not further alpha changes occur .
in each full stage , each example is optimized only once ( with some other heuristically - selected example ) , to see if it becomes part of the working inbounds set and triggers new work for the next inbounds stage .
during the course of smo full iterations , it is not uncommon for the number of working candidate support vectors to become orders of magnitude larger than the number that will result at the end of the following inbounds iteration .
when the kernel cache is sufciently large ( i . e .
r near ( cid : 123 ) ) then such sv bulge behavior is not critically problematic .
however , even in that case , the computation of the svm outputs for each example can be quite excessive , since the time cost of computing an output is linear in the number of current support vectors .
so , it would be generally useful to keep the the size of the support vector set closer to minimal .
a key problem arises when the size of the working candidate support vector set has already exceeded r .
in that case , any additional support vectors will not be able to cache their kernel computations .
to address this key problem , we have extended the smo algorithm to include a concept we call digestion .
the basic idea is to jump out of full smo iterations early , once the working candidate support vector set grows by a large amount .
this switches smo into an inbounds iteration in which it fully digests the working candidate sv set , reducing it to a locally minimal size , before switching back to a full iteration ( and returning rst to the example at which it had previously jumped out ) .
digestion allows us to better control the size of the intermediate sv bulge , so as to best tradeoff the cost of overowing the kernel cache against the cost of doing more inbounds iterations that standard smo would .
digestion is similar to other ( non - smo ) chunking methods , in that it performs full optimizations over various large subsets of the data .
however , it differs signicantly in that these full optimizations are triggered by heuristics based on maximizing the likely reuse of the kernel cache , instead of using some predetermined and xed chunk size .
we suspect that digestion would be even more useful and distinct when using sparse kernel caches ( e . g .
hash tables ) , although our current implementation caches entire rows of the kernel matrix ( much like other published implementations , such as svmlight ) .
our current approach involves user - dened settings to reect tradeoff heuristics .
we expect that future work could provide meta - learning methods for identifying the best values for these settings , tuned for the particular nature of specic domains ( e . g .
costs of each kernel computation ) , over the course of multiple training sessions .
our settings were choosen to
training invariant support vector machines
computations , force a digestion .
new ( currently uncached ) kernel values are computed before digesting again .
be particularly reasonable for training set sizes on the order of ( cid : 123 ) = 123 , 123 to 123 , 123 and a kernel cache of about 123 megabytes , which is the case for our mnist experiments ( see section 123 . 123 ) .
our heuristics were ( highest - priority rst ) : 123
once digestion has been performed , wait until at least some number ( default= 123 ) of 123
after the current full iteration has already required some ( default= 123 , 123 ) new kernel 123
otherwise , wait at least as long between digestions as some factor ( default= 123 ) times the time spent in the previous inbounds iteration .
the idea is that if the inbounds iterations are expensive themselves , one must tolerate longer full iterations as well .
when the kernel cache is full ( i . e .
working candidate sv set size > r ) , digest as soon as the net number of new working candidate svs grows past some threshold ( default= 123 ) .
the idea here is that the kernel values computed for each such new sv cannot be cached , so it is becomes critical to free rows in the kernel cache , if at all possible .
digest whenever the net number of new svs grows past some threshold ( default= 123 ) .
the intuition behind this is simply that output computations involve summations over all candidate svs in the working set .
thus , it is often worthwhile to periodically make sure that this set is not many times larger than it should be .
one can easily imagine promising alternative heuristics as well , such as ones based on when the working candidate sv set grows by more than some percentage of the working candidate sv size at the start of a full iteration .
the key point is that heuristics to induce digestion have proven to be useful in signicantly reducing the complexity of smo training on particularly difcult large - scale problems .
for example , we observed speedups of over 123 times for training some of the svms in the mnist examples of section 123 . 123 when using the above heuristics and default settings .
handwritten digit recognition
we start by reporting results on two widely known handwritten digit recognition bench - marks , the usps set and the mnist set .
let us rst described the databases .
both of them are available from http : / / www .
kernel - machines . org / data . html .
the us postal service ( usps ) database ( see gure 123 ) contains 123 handwritten digits ( 123 for training , 123 for testing ) , collected from mail envelopes in buffalo ( lecun et al . , 123 ) .
each digit is a 123 123 image , represented as a 123 - dimensional vector with entries between 123 and 123
preprocessing consisted of smoothing with a gaussian kernel of width = 123 .
it is known that the usps test set is rather difcultthe human error rate is 123% ( bromley & sackinger , 123 ) .
for a discussion , see ( simard , lecun , & denker , 123 ) .
note , moreover , that some of the results reported in the literature for the usps set have been obtained with an enhanced training set .
for instance , drucker , schapire , and simard
decoste and b .
sch olkopf
figure 123
the rst 123 usps training images , with class labels .
( 123 ) used an enlarged training set of size 123 , containing some additional machine - printed digits , and note that this improves the accuracy on the test set .
similarly , bot - tou and vapnik ( 123 ) used a training set of size 123
since there are no machine - printed digits in the test set that is commonly used ( size 123 ) , this addition distorts the original learning problem to a situation where results become somewhat hard to in - terpret .
for our experiments , we only had the original 123 training examples at our
the mnist database ( gure 123 ) contains 123 handwritten digits , equally divided into training and test set .
the database is modied version of nist special database 123 and nist test data 123
training and test set consist of patterns generated by different writers .
the images were rst size normalized to t into a 123 123 pixel box , and then centered in a 123 123 image ( lecun et al . , 123 ) .
training invariant support vector machines
figure 123
the rst 123 mnist training images , with class labels .
test results on the mnist database which are given in the literature ( e . g .
( lecun et al . , 123; lecun et al . , 123 ) ) commonly do not use the full mnist test set of 123 characters .
instead , a subset of 123 characters is used , consisting of the test set patterns from 123 to 123
to obtain results which can be compared to the literature , we also use this test set , although the larger one would be preferable from the point of view of obtaining more reliable test error estimates .
the mnist benchmark data is available from
123 . 123 .
virtual sv methodusps database .
the rst set of experiments was conducted on the usps database .
this database has been used extensively in the literature , with a lenet123 convolutional network achieving a test error rate of 123% ( lecun et al . , 123 )
decoste and b .
sch olkopf
table 123
comparison of support vector sets and performance for training on the original database and training on the generated virtual support vectors .
in both training runs , we used polynomial classier of degree 123
classir trained on
full training set overall sv set virtual sv set virtual patterns from full db
of svs
virtual support vectors were generated by simply shifting the images by one pixel in the four principal directions .
adding the unchanged support vectors , this leads to a training set of the second classier which has ve times the size of the rst classiers overall support vector set ( i . e .
the union of the 123 support vector sets of the binary classiers , of size 123note that due to some overlap , this is smaller than the sum of the ten support set sizes ) .
note that training on virtual patterns generated from all training examples does not lead to better results han in the virtual sv case; moreover , although the training set in this case is much larger , it hardly leads to more svs .
figure 123
different invariance transformations in the case of handwritten digit recognition .
in all three cases , the central pattern , taken from the mnist database , is the original which is transformed into virtual examples ( marked by grey frames ) with the same class membership by applying small transformations ( from scholkopf ,
used the regularization constant c = 123
this value was taken from earlier work ( scholkopf , burges , & vapnik , 123 ) , no additional model selection was performed .
virtual support vectors were generated for the set of all different support vectors of the ten classiers .
alternatively , one can carry out the procedure separately for the ten binary classiers , thus dealing with smaller training sets during the training of the second machine .
table 123 shows that incorporating only translational invariance already improves performance signicantly , from 123% to 123% error rate .
for other types of invariances ( gure 123 ) , we also found improvements , albeit smaller ones : generating virtual support
training invariant support vector machines
table 123
summary of results on the usps set .
optimal margin classier linear hyperplane on kpca features virtual svm , local kernel boosted neural nets human error rate
( simard et al . , 123 ) ( lecun et al . , 123 ) ( boser et al . , 123 ) ( scholkopf et al . , 123 ) ( scholkopf et al . , 123b ) ( bottou and vapnik , 123 ) ( scholkopf et al . , 123 ) ( drucker et al . , 123 ) ( simard et al . , 123 ) ( bromley and sackinger , 123 )
note that two variants of this database have been used in the literature; one of them ( denoted by usps been enhanced by a set of machine - printed characters which have been found to improve the test error .
note that the virtual sv systems perform best out of all systems trained on the original usps set .
vectors by rotation or by the line thickness transformation of drucker , schapire , and simard ( 123 ) , we constructed polynomial classiers with 123% error rate ( in both cases ) .
for purposes of comparison , we have summarized the main results on the usps database in
note , moreover , that generating virtual examples from the full database rather than just from the sv sets did not improve the accuracy , nor did it enlarge the sv set of the nal classier substantially .
this nding was conrmed using gaussian rbf kernels ( scholkopf , 123 ) : in that case , similar to table 123 , generating virtual examples from the full database led to identical performance , and only slightly increased sv set size .
from this , we conclude that for the considered recognition task , it is sufcient to generate virtual examples only from the svsvirtual examples generated from the other patterns do not add much useful
123 . 123 .
virtual sv methodmnist database .
the larger a database , the more information about invariances of the decision function is already contained in the differences between patterns of the same class .
to show that it is nevertheless possible to improve classication accuracies with our technique , we applied the method to the mnist database of 123 handwritten digits .
this database has become the standard for performance comparisons at at&t and bell labs .
using virtual support vectors generated by 123 - pixel translations , we improved a degree 123 polynomial sv classier from 123% to 123% error rate on the 123 element test set .
in this case , we applied our technique separately for all ten support vector sets of the binary classiers ( rather than for their union ) in order to avoid having to deal with large training sets in the retraining stage .
note , moreover , that for the mnist database , we did
decoste and b .
sch olkopf
table 123
summary of results on the mnist set .
at 123% ( 123% before rounding ) , the system described in section 123 . 123 performs best .
lenet123 , local learning dual - channel vision model virtual svm , 123 - pixel translation
( lecun et al . , 123 ) ( lecun et al . , 123 ) ( simard et al . , 123 )
( lecun et al . , 123 )
( lecun et al . , 123 ) ( lecun et al . , 123 ) ( lecun et al . , 123 ) ( teow and loe , 123 ) ( lecun et al . , 123 ) this paper; see section 123 . 123
not compare results of the vsv technique to those for generating virtual examples from the whole database : the latter is computationally exceedingly expensive , as it entails training on a very large training set . 123
after retraining , the number of svs more than doubled .
thus , although the training sets for the second set of binary classiers were substantially smaller than the original database ( for four virtual svs per sv , four times the size of the original sv sets , in our case amounting to around 123 ) , we concluded that the amount of data in the region of interest , close to the decision boundary , had more than doubled .
therefore , we reasoned that it should be possible to use a more complex decision function in the second stage ( note that typical vc risk bounds depends on the ratio vc - dimension and training set size ( vapnik , 123 ) ) .
indeed , using a degree 123 polynomial led to an error rate of 123% , very close to the record performance of 123% .
the main results on the mnist set are summarized in table 123
prior to the present work , the best system on the mnist set was a boosted ensemble of lenet123 neural networks , trained on a huge database of articially generated virtual examples .
note that a difference in error rates on the ist set which is at least 123% may be considered signicant ( lecun et al . , 123 ) .
it should be noted that while it is much slower in training , the lenet123 ensemble also has the advantage of a faster runtime speed .
especially when the number of svs is large , svms tend to be slower at runtime than neural networks of comparable capacity .
this is particularly so for virtual sv systems , which work by increasing the number of sv .
hence , if runtime speed is an issue , the systems have to be sped up .
burges and scholkopf ( 123 ) have shown that one can reduce the number of svs to about 123% of the original number with very minor losses in accuracy .
in a study on the mnist set , they started with a vsv system performing at 123% test error rate , and sped it up by a factor of 123 with the accuracy
training invariant support vector machines
degrading only slightly to 123% . 123 note that there are neural nets which are still faster than that ( cf . , lecun et al .
( 123 ) ) .
smo vsv experiments .
mnist data set .
in this section we summarize our newest results on the
following the last section , we used a polynomial kernel of degree 123
we normalized so that dot - products giving values within ( 123 , 123 ) yield kernel values within ( 123 , 123 ) ; specically :
k ( u , v ) 123
( u v + 123 ) 123
this ensures that kernel values of 123 and 123 have the same sort of canonical meaning that holds for others , such as radial - basis function ( rbf ) kernels .
namely , a kernel value of 123 corresponds to the minimum distance between ( identical ) examples in the kernel - induced metric distance and 123 corresponds to the maximum distance .
we ensured any dot - product was within ( 123 , 123 ) by normalizing each example by their 123 - norm scalar value ( i . e .
such that each example dot - producted against itself gives a value of 123 ) .
we used this normalization ( a form of brightness or amount of ink equalization ) by default because it is the sort of normalization that we routinely use in our nasa space image detection applications .
this also appears to be a good normalization for the mnist since our polynomial kernel value normalization gives a kernel value of 123 special signif - icance , we suspected that a svm regularization parameter setting of c= 123 would probably be too low .
we also determined , by trying a large value ( c= 123 ) for training a binary recognizer for digit 123 , that no training example reached an alpha value above 123 .
by looking at the histogram of the 123 , 123 alpha values for that case , we realized that only a handful of examples in each of the 123 digit classes had alpha values above 123 .
under the assumption that only a few training examples in each class are particularly noisy and that digit 123 is one of the harder digits to recognize , for simplicity we used c= 123 for training each svm .
we have not yet attempted to nd better c values , and it is most likely that our simplistic choice was relatively suboptimal .
in future work , we will determine the effect of more careful selection of c , such as via cross - validation and approximations based on generalization error upper - bound estimates .
experiment 123 : 123 - pixel translations
tables 123 , 123 , and 123 summarize our vsv results on the mnist data sets using 123 - pixel translations in all 123 directions .
these experiments employed our new smo - based methods , as described in section 123 ( including our digestion technique ) .
to see whether our faster training times could be put to good use , we tried the vsv method with more invariance than was practical in previous experiments .
specically , we used 123 123 box jitter of the image centers , which corresponds to translation of each image for a distance of one pixel in any of the 123 directions ( horizontal , vertical , or both ) .
total training time , for obtaining 123 binary recognizers for each of the
decoste and b .
sch olkopf
table 123
errors on mnist ( 123 , 123 test examples ) , using vsv with 123 - pixel translation .
test error rate
table 123
binary - recognition errors on mnist ( 123 , 123 test examples ) , using vsv with 123 - pixel translation .
svm for digit
errors for each binary recognizer
table 123
number of support vectors for mnist ( 123 , 123 training examples ) , using vsv with 123 - pixel translation .
number of support vectors for each binary recognizer
123 < i < c i = c
123 < i < c i = c
base sv and the vsv stages , was about 123 days ( 123 hours ) on an sun ultra123 workstation with a 123 mhz processor and 123 gigabytes of ram ( allowing an 123 mb kernel cache ) .
this training time compares very favorably to other recently published results , such as learning just a single binary recognizer even without vsv ( e . g .
training digit 123 took about 123 hours in platt , 123 ) .
the vsv stage is also signicantly more expensive that the base sv stage ( averaging about 123 hours versus 123 hour ) a majority of examples given to vsv training typically end up being support vectors .
training invariant support vector machines
experiment 123 : deslanted images and additional translations
some previous work on the mnist data have achieved their best results by deslanting each image before training and testing .
deslanting is performed by computing each images principal axis and horizontally translating each row of pixels to best approximate the process of rotating each image to make its principal axis become vertical .
for example , under such deslanting , tilted 123 digits all become very similar and all close to vertical .
tables 123 , 123 , and 123 summarize our newest results , using deslanted versions of the mnist training and test sets .
other than using deslanted data , the same training conditions were used as the previous experiments ( i . e .
same kernel , same c= 123 , same 123 123 box jitter for
interestingly , using deslanted images did not improve the test error rates for our exper - iments .
in fact , the same test error rate was reached using either version of the data set for either sv ( 123 errors ) or vsv ( 123 errors ) , although the specic test examples with errors varied somewhat .
this result seems to provide evidence that the polynomial kernel is already doing a good job in capturing some of the rotational invariance from the training
table 123
errors on deslanted mnist ( 123 , 123 test examples ) , using vsv with 123 - pixel translation .
test error rate
table 123
binary - recognition errors on deslanted mnist ( 123 , 123 test examples ) , using vsv with 123 - pixel
svm for digit
errors for each binary recognizer
decoste and b .
sch olkopf
table 123
number of support vectors for deslanted mnist ( 123 , 123 training examples ) , using vsv with 123 - pixel
number of support vectors for each binary recognizer
123 < i < c i = c
123 < i < c i = c
123 < i < c i = c
set alone .
nevertheless , comparing table 123 with 123 shows that deslanted data does lead to signicantly fewer svs and vsvs , with a particularly high percentage of reduction for digit 123 ( as one might expect , since deslanted 123 digits are especially similar to each other ) .
to investigate whether further jitters would lead to even better results ( or perhaps worse ones ) , we tried the vsv method using 123 123 box jitter combined with four additional translations by 123 - pixels ( i . e .
horizontally or vertically , but not both ) , using the same training conditions as the other experiments .
although this only enlarged the number of training examples by about 123% ( from 123 to 123 per original sv ) , it require approximately 123 times more training time , due to the large number of vsvs that resulted .
the results of this experiment are labelled as vsv123 in tables 123 , 123 , and 123
figure 123 shows the 123 misclassied test examples that resulted .
it is interesting to note that the largest number of vsvs ( 123 , 123 ) is still only about a third of the size of the full ( 123 , 123 ) training set , despite the large number of translation jitters explored in this case .
other distortions , such as scale , rotation , and line thickness , have all been reported to also help signicantly in this domain .
it seems clear from gure 123 that many of the remaining misclassications are due to failure to fully account for such other invariances .
smo jittering kernels experiments .
table 123 summarizes some initial experiments in comparing vsv and jsv methods on a small subset of the mnist training set .
specif - ically , 123 123 digit examples and 123 123 digit examples from the training set and all 123 123 and 123 digits from the 123 , 123 example test set .
we have not yet run more conclusion comparisons using the entire data set .
we again used a polynomial kernel of degree 123
these experiments illustrate typical relative behaviors , such as the jsv test times being much faster than the worst case ( of j times slower than vsv test times ) , even though jsv must jitter at test time , due to jsv having many fewer nal svs than for vsv .
furthermore , both jsv and vsv typically beat standard svms ( i . e .
no invariance ) .
they also both
training invariant support vector machines
figure 123
the 123 errors for deslanted 123 , 123 mnist test data ( vsv123 ) .
the number in the lower - left corner of each image box indicates the test example number ( 123 thru 123 , 123 ) .
the rst number in the upper - right corner indicates the predicted digit label and the second indicates the true label .
typically beat query jitter as well , in which the test examples are jittered inside the kernels during svm output computations .
query jitter effectively uses jittering kernels at test time , even though the svm was not specically trained for a jittering kernel .
we tested that case simply as a control in such experiments , to verify that training the svm with the actual jittering kernel used at test time is indeed important .
relative test errors between vsv and jsv varysometimes vsv is substantially better ( as in the 123 123 box jitter example ) and sometime it is somewhat worse .
although our results to date are preliminary , it does seem that vsv methods are substantially more robust than jsv ones , in terms of variance in generalization errors .
nasa volcanoe recognition
we are currently applying these invariance approaches to several difcult nasa object detection tasks .
in particular , we are focusing on improving known results in volcanoe
decoste and b .
sch olkopf
123 vs 123 results for mnist ( all 123 test examples , rst 123 123s and 123 123s from training set ) .
using 123 123 box jitter
query jitter test
using 123 box jitter and deslant preprocessing ( principal axis vertical )
query jitter test
detection ( burl et al . , 123 ) and in enabling useful initial results for a crater detection application ( burl , 123 ) .
these nasa applications have in fact motivated much of our recent work on efcient training of invariant svms .
in this section we present some preliminary results on a subset of the nasa volcanoe data , specically that which is currently publicly available ( the uci kdd archive volcanoes data ( burl , 123 ) ) .
figure 123 shows some examples from this data set .
as described in burl et al .
( 123 ) , all examples were determined from a focus of attention ( foa ) matched lter that was designed to quickly scan large images and select 123 - pixel - by - 123 - pixel subimages with relatively high false alarm rates but low miss rates ( about 123 : 123 ) .
the previous best overall results on this data were achieved using a quadratic classier , modeling the two classes with one gaussian for each class ( burl et al . , 123 ) .
to overcome the high dimensionality of the 123 - pixel example vectors , pca was performed to nd the six principal axes of only the positive examples and then all examples were projected to those axes .
mean and covariance matrices were computed over those six dimensions for
figures 123 and 123 compare roc123 curves for our initial svms ( without any explicit in - variance ) versus the best previous gaussian models . 123 there are ve experiments , varying from homogeneous collections of volcanoes from the same regions of venus to heteroge - neous collections of examples from across the planet , with various k - fold partitionings into training and test sets for each experiment .
the roc curves represent the leave - out - fold cross - validation performances for each experiment .
training invariant support vector machines
figure 123
first 123 positive ( left ) and rst 123 negative ( right ) test examples of volcanoe data set hom123 ( partition a123 )
+ = 123c
the number of example images vary from thousands to tens of thousands across the ve experiments .
due to the much greater number of negative examples than positive examples in each experiment ( by factors ranging from 123 to 123 ) , we trained svms used sv m light , employing its implemented ability to use two regularization parameters , c , instead of a single c .
that was done to reect the importance of a high detection rate despite the relatively scarcity of positive training examples .
specically , we used c ( u v + 123 ) 123 , based solely on some manual tuning on data set hom123 ( experiment a ) .
burl et al .
( 123 ) similarly used hom123 to determine settings for the remaining four experiments .
it is interesting to note that hom123 is the only one of the experiments for which our ( svm ) results seem to be very similar to theirs .
we also adopted the same normalization ( of the 123 - bit pixel values ) used in their original experiments , namely so that the mean pixel value of each example image is 123 and the standard deviation is 123
and the polynomial kernel k ( u , v ) 123
to extract roc curves from each trained svm , we added a sliding offset value to the svm output , with an offset of 123 giving the trained response .
interesting , our svms beat the previous best ( gaussian ) model for each of the ve experiments when offset= 123 , i . e .
at the operating point for which the svm was specically trained .
this suggests that the svm - derived roc curves might be even higher overall if a svm was explicitly trained for multiple operating points , instead of just one .
it is also interesting that the svm approach does well without the sort of feature extraction effort ( i . e .
pca ) that the previous work had required .
one disadvantage of our svm approach was that training time for all ve experiments was somewhat slowerabout 123 minutes , versus 123 minutes for the gaussian model , on the same machine ( a 123 mhz sun ultra123 ) .
this nasa volcanoe detection problem is somewhat challenging for vsv techniques to make further progress , since many potential invariances have already been normalized away .
for example , the sun light source is from the same angle and intensity for each example considered , so rotation transformations would not be useful .
furthermore , the focus of attention mechanism already centers each volcanoe example quite well .
nevertheless , the volcanoe examples ( e . g .
gure 123 ) do indicate some slight variance in the location of the
decoste and b .
sch olkopf
figure 123
roc curves for the rst four experiments described by burl et al .
( 123 ) .
the solid curve is the roc for our svm and the dashed curve is the roc for the best two - gaussian model from burl et al .
( 123 ) .
the mark on the svm curve indicates the operating point of the trained svm ( i . e .
offset= 123 ) .
the dashed line near the
top of each plot box indicates the best performance possible , which is less than a detection rate of 123 due to the focus of attention mechanism having a non - zero miss rate .
characteristic bright middle of each volcanoe , so we have been investigating whether vsv translations can still help .
due to the relatively large number of negative examples , we rst explored whether performing vsv transformations only on the positive examples might sufce .
this was inspired by the previous work succeeding with pca limited to capturing the variance only in the positive examples .
furthermore , it seemed possible that the large set of neg - ative examples might already contain considerable translation variation .
however , we found that this lead to signicantly worse performance than using no vsv .
the stan - dard vsv approach of transforming both negative and positive svs indeed seems critical , perhaps because otherwise the new positive instances have free reign to claim large ar - eas of kernel feature space for which it ( incorrectly ) appears negative instances would not
training invariant support vector machines
figure 123
roc curves for the fth experiment ( as in gure 123 ) .
figure 123 shows a comparison of roc curves for a baseline svm and its vsv svm , using four 123 - pixel translations ( horizontal or vertical ) for all svs of the baseline svm and the same training conditions as above .
however , unlike for the mnist domain , in this domain replacing a pixel which moves away from an edge is not as simple as shifting in a constant background ( e . g .
zero ) pixel .
for simplicity , we rst tried replicating the shifted row ( or column ) , but found this did not work well , presumably because this conicted with the higher - order correlations modeled by the nonlinear kernel .
so , instead for each image we did the following : ( 123 ) apply the translation to each raw 123 - bit - pixel image ( without concern for shift - in values ) , ( 123 ) shrink from 123 - by - 123 to 123 - by - 123 images ( i . e .
ignore the borders ) , ( 123 ) normalize each 123 123 example .
this shrinking was done for the baseline svm as well , which apparently did not inuence the roc curve signicantly ( as comparison of hom123 in gures 123 and 123 indicates ) .
histograms of the svm outputs suggest that a key reason for the overall improvements in the roc curve for the vsv svm is that it is signicantly more condent in its outputs ( e . g .
they are much larger in magnitude ) , especially for negative examples .
for comparison , we have also tried translating all examples ( not just svs ) .
as in earlier
work , we found the results to be essentially identical , although much slower to train .
our work with vsv on the volcanoe data is still preliminary , so we are still focussing on the hom123 data and remaining blind to vsv performance on the others .
to date we have observed that vsv does not seem to hurt overall roc performance , as long as the
decoste and b .
sch olkopf
figure 123
dashed roc curve is from baseline svm ( mark xshows threshold= 123 operating point ) .
solid roc curve is from svm including vsv with four 123 - pixel translations ( mark shows threshold= 123 operating point ) .
above warnings are heeded , and sometimes helps .
we suspect that with signicant further work ( e . g .
employing scale transforms , starting with larger 123 - by - 123 images to shrink after translations , using more comprehensive model selection , etc . ) signicantly better results can still be achieved in this domain .
this paper has described a variety of methods for incorporating prior knowledge about invariances into svms .
in experimental work , we have demonstrated that these techniques allow state - of - the - art performance , both in terms of generalization error and in terms of svm training time .
as mentioned throughout this paper , there are several promising lines of future work .
these include experiments with a wider assortment of distortions ( e . g .
rotations , scale , line thickness ) and across multiple domains ( e . g .
further nasa space applications , in addition to the traditional digit recognition tasks ) .
an interesting issue is under what conditions applying additional distortions to training data leads to signicantly worse test errors , even when using careful cross - validation during training .
one would expect that obviously - excessive distortions , such as rotating 123 digits
training invariant support vector machines
so far as to confuse them with 123 digits , would be easily detected as counter - productive during such cross - validations .
thus , an open issue is to what extent distortions reecting known invariances can be safely applied during training whenever computationally feasible and to what extent their use makes controlling generalization error more difcult .
given our demonstrated success in training invariant svms that generalize very well , a key issue for future work is in lowering their test time costs .
for example , the best neural networks ( e . g .
( lecun et al . , 123 ) ) still appear to be much faster at test time than our best svms , due to the large number of ( virtual ) support examples that are each compared to each test example during kernel computations .
further work in reducing the number of such comparisons per test example ( with minimal increase in test errors ) , perhaps along the lines of reduced sets ( burges & scholkopf , 123 ) , would seem particularly useful at this point .
thanks to chris burges , michael c .
burl , olivier chapelle , sebastian mika , patrice simard , alex smola , and vladimir vapnik for useful discussions .
parts of this research were carried out by the jet propulsion laboratory , california insti - tute of technology , under contract with the national aeronautics and space administration .
the reader who is not familiar with this concept may think of it as a group of transformations where each element is labelled by a set of continuously variable parameters , cf .
also ( simard et al . , 123; simard , lecun , & denker , 123 ) .
such a group may be considered also to be a manifold , where the parameters are the coordinates .
for lie groups , it is required that all group operations are smooth maps .
it follows that one can , for instance , compute derivatives with respect to the parameters .
examples of lie groups are the translation group , the rotation group , or the lorentz group; for details , see e . g .
( crampin & pirani , 123 ) .
we think that this is because that study focused on the linear case .
the nonlinear case has recently been studied
by chapelle and scholkopf ( 123 ) .
clearly , the scheme can be iterated; however , care has to exercised , since the iteration of local invariances would lead to global ones which are not always desirablecf .
the example of a 123 rotating into a 123 ( simard , lecun , & denker , 123 ) .
for example , gure 123 shows some jittered forms of a particular example of digit 123
this corresponds to euclidian distance in the feature space corresponding to the kernel , using the denition
of the two - norm :
( cid : 123 ) zi z j ( cid : 123 ) 123 = ( zi zi ) 123 ( zi z j ) + ( z j z j ) .
incidentally , this is why we expect , for large - scale problems , that jsv approaches may be able to effectively amortize away much of their extra jittering kernel costs ( j times slower than vsvs standard kernels ) .
that is , the jsv working set will be j times smaller than for the corresponding vsv , and thus might reuse a limited cache more effectively .
we did , however , make a comparison for a subset of the mnist database , utilizing only the rst 123 training example .
there , a degree 123 polynomial classier was improved from 123% to 123% error by the virtual sv method , with an increase of the average sv set sizes from 123 to 123
by generating virtual examples from the full training set , and retraining on these , we obtained a system which had slightly more svs ( 123 ) , but an unchanged error rate .
note this study was done before vsv systems with higher accuracy were obtained .
decoste and b .
sch olkopf
the difference in cpus ( i . e .
our sparc ultra - ii 123 mhz versus platts intel pentium ii 123 mhz ) seems not as signicant as one might imagine .
experiments on both cpus indicate that the sparc is effectively only 123% faster than the pentium in dot - product computations , which dominate the training time in our experiments .
as in burl et al .
( 123 ) , these are actually froc ( free - response roc ) curves , showing probability of detection
versus the number of false alarms per unit area .
the volcanoe data from the uci kdd archive ( burl .
123 ) are slightly different from the data used in burl et al .
( 123 ) , due to recent changes in their foa mechanism .
the author of those experiments ( burl ) reran them for us , so that all our roc plots here involve the same data .
this accounts for our slight differences from the roc curves shown in burl et al .
( 123 ) .
