we propose a general framework for learning from labeled and unlabeled data on a directed graph in which the structure of the graph in - cluding the directionality of the edges is con - sidered .
the time complexity of the algo - rithm derived from this framework is nearly linear due to recently developed numerical in the absence of labeled in - stances , this framework can be utilized as a spectral clustering method for directed graphs , which generalizes the spectral clus - tering approach for undirected graphs .
we have applied our framework to real - world web classication problems and obtained encour -
given a directed graph , the vertices in a subset of the graph are labeled .
our problem is to classify the re - maining unlabeled vertices .
typical examples of this kind are web page categorization based on hyperlink structure and document classication based on cita - tion graphs ( fig .
the main issue to be resolved is to determine how to eectively exploit the structure of directed graphs .
one may assign a label to an unclassied vertex on the basis of the most common label present among the classied neighbors of the vertex .
however we want to exploit the structure of the graph globally rather than locally such that the classication or clustering is consistent over the whole graph .
such a point of
appearing in proceedings of the 123 nd international confer - ence on machine learning , bonn , germany , 123
copy - right 123 by the author ( s ) / owner ( s ) .
view has been considered previously in the method of zhou et al .
it is motivated by the frame - work of hubs and authorities ( kleinberg , 123 ) , which separates web pages into two categories and uses the following recursive notion : a hub is a web page with links to many good authorities , while an authority is a web page that receives links from many good hubs .
in contrast , the approach that we will present is in - spired by the ranking algorithm pagerank used by the google search engine ( page et al . , 123 ) .
dier - ent from the the framework of hubs and authorities , pagerank is based on a direct recursion as follows : an authoritative web page is one that receives many links from other authoritative web page .
when the under - lying graph is undirected , the approach that we will present reduces to the method of zhou et al .
( 123 ) .
there has been a large amount of activity on how to exploit the link structure of the web for ranking web pages , detecting web communities , nding web pages similar to a given web page or web pages of interest to a given geographical region , and other applications .
we may refer to ( henzinger , 123 ) for a comprehensive survey .
unlike those work , the present work is on how to classify the unclassied vertices of a directed graph in which some vertices have been classied by globally exploiting the structure of the graph .
classifying a - nite set of objects in which some are labeled is called transductive inference ( vapnik , 123 ) .
in the absence of labeled instances , our approach reduces to a spectral clustering method for directed graphs , which general - izes the work of shi and malik ( 123 ) that may be the most popular spectral clustering scheme for undirected graphs .
we would like to mention that understanding how eigenvectors partition a directed graph has been proposed as one of six algorithmic challenges in web search engines by henzinger ( 123 ) .
the framework of probabilistic relational models may also be used to
learning from labeled and unlabeled data on a directed graph
for all 123 r k 123 each edge ( u; v ) 123 e with u 123 vr has v 123 vr+123; where vk = v123; and k is maximal , that is , there is no other such partition v = v
> k : when k = 123 , we say that the graph is aperiodic; otherwise we say that the graph is periodic .
123 ( ( v
figure 123
the world wide web can be thought of as a directed graph , in which the vertices represent web pages , and the directed edges hyperlinks .
deal with structured data like the web ( e . g .
getoor et al .
( 123 ) ) .
in contrast to the spirit of the present work however , it focuses on modeling the probabilistic distribution over the attributes of the related entities in the model .
the structure of the paper is as follows .
we rst introduce some basic notions from graph theory and markov chains in section 123
the framework for learn - ing from directed graphs is presented in section 123
in the absence of labeled instances , as shown in section 123 , this framework can be utilized as a spectral clustering approach for directed graphs .
in section 123 , we develop discrete analysis for directed graphs , and characterize this framework in terms of discrete analysis .
experi - mental results on web classication problems are de - scribed in section 123
a directed graph g = ( v; e ) consists of a nite set v; together with a subset e ( cid : 123 ) v v : the elements of v are the vertices of the graph , and the elements of e are the edges of the graph .
an edge of a directed graph is an ordered pair ( u; v ) where u and v are the vertices of the graph .
when u = v the edge is called a loop .
a graph is simple if it has no loop .
we say that the vertex v is adjacent from the vertex u; and the the vertex u is adjacent to the vertex v; and the edge ( u; v ) is incident from the vertex u and incident to the vertex v :
a path in a directed graph is a tuple of vertices ( v123; v123; : : : ; vp ) with the property that ( vi; vi+123 ) 123 e for 123 i p 123 : we say that a directed graph is strongly connected when for every pair of vertices u and v there is a path in which v123 = u and vp = v : for a strongly connected graph , there is an integer k 123 and a unique partition v = v123 ( v123 ( ( vk123 such that
a graph is weighted when there is a function w : e ! r+ which associates a positive value w ( ( u; v ) ) with each edge ( u; v ) 123 e : the function w is called a weight function .
typically , we can equip a graph with a canonical weight function dened by w ( ( u; v ) ) : = 123 at each edge ( u; v ) 123 e : given a weighted directed graph and a vertex v of this graph , the in - degree function d : v ! r+ and out - degree function d+ : v ! r+ are respectively dened by d ( v ) : = pu ! v w ( ( u; v ) ) ; and d+ ( v ) : = puv w ( ( v; u ) ) ; where u ! v denotes the set of vertices adjacent to the vertex v , and u v the set of vertices adjacent from the vertex v : let h ( v ) denote the space of functions , in which each one f : v ! r assigns a real value f ( v ) to each vertex v : a function in h ( v ) can be thought of as a col - umn vector in rjv j; where jv j denotes the number of the vertices in v .
the function space h ( v ) then can be endowed with the standard inner product in rjv j as hf; gih ( v ) = pv123v f ( v ) g ( v ) for all f; g 123 h ( v ) : similarly , dene the function space h ( e ) consisting of the real - valued functions on edges .
when the function space of the inner product is clear in its context , we omit the subscript h ( v ) or h ( e ) : for a given weighted directed graph , there is a nat - ural random walk on the graph with the transition probability function p : v v ! r+ dened by p ( u; v ) = w ( ( u; v ) ) =d+ ( u ) for all ( u; v ) 123 e; and 123 oth - erwise .
the random walk on a strongly connected and aperiodic directed graph has a unique stationary dis - tribution ; i . e .
a unique probability distribution satis -
fying the balance equations ( v ) = pu ! v ( u ) p ( u; v ) ; for all v 123 v : moreover , ( v ) > 123 for all v 123 v :
regularization framework
given a directed graph g = ( v; e ) and a label set y = f123;123g; the vertices in a subset s v is labeled .
the problem is to classify the vertices in the complement of s : the graph g is assumed to be strongly connected and aperiodic .
later we will discuss how to dispose assume a classication function f 123 h ( v ) ; which as - signs a label sign f ( v ) to each vertex v 123 v : on the one hand , similar vertices should be classied into the same class .
more specically , a pair of vertices linked by an edge are likely to have the same label .
moreover , vertices lying on a densely linked subgraph are likely
learning from labeled and unlabeled data on a directed graph
to have the same label .
thus we dene a functional
( f ) : =
( u ) p ( u; v ) f ( u )
which sums the weighted variation of a function on each edge of the directed graph .
on the other hand , the initial label assignment should be changed as little as possible .
let y denote the function in h ( v ) dened by y ( v ) = 123 or 123 if vertex v has been labeled as pos - itive or negative respectively , and 123 if it is unlabeled .
thus we may consider the optimization problem
f 123h ( v ) ' ( f ) + kf yk123 ;
where > 123 is the parameter specifying the tradeo between the two competitive terms .
we will provide the motivations for the functional de - ned by ( 123 ) .
in the end of this section , this functional will be compared with another choice which may seem more natural .
the comparison may make us gain an insight into this functional .
in section 123 , it will be shown that this functional may be naturally derived from a combinatorial optimization problem .
tion 123 , we will further characterize this functional in terms of discrete analysis on directed graphs .
for an undirected graph , it is well - known that the sta - tionary distribution of the natural random walk has a
d ( v ) denotes the degree of the vertex v : substituting the closed form expression into ( 123 ) , we have
closed form expression ( v ) = d ( v ) =pu123v d ( u ) ; where
w ( ( u; v ) ) f ( u )
which is exactly the regularizer of the transductive in - ference algorithm of zhou et al .
( 123 ) operating on
( f ) =
for solving the optimization problem ( 123 ) , we introduce an operator : h ( v ) ! h ( v ) dened by
( f ) ( v ) =
( u ) p ( u; v ) f ( u )
( v ) p ( v; u ) f ( u )
let denote the diagonal matrix with ( v; v ) = ( v ) for all v 123 v : let p denote the transition probability matrix and p t the transpose of p : then
123=123p 123=123 + 123=123p t 123=123
lemma 123 .
let = i ; where i denotes the identity .
then ( f ) = hf; fi : proof .
the idea is to use summation by parts , a dis - crete analogue of the more common integration by
123 xv123v xu ! v 123 xv123v xu ! v 123 xv123v xuv
( u ) p ( u; v ) f ( u ) ( u ) p ( u; v ) f ( u ) ( v ) p ( v; u ) f ( v ) p ( u; v ) f 123 ( u ) + xu ! v p ( v; u ) f 123 ( v ) + xuv
( u ) p ( u; v ) f ( u ) f ( v )
( v ) p ( v; u ) f ( v ) f ( u )
the rst term on the right - hand side may be written
= xu123v xvu
p ( u; v ) f 123 ( u ) = xu123v xvu p ( u; v ) ! f 123 ( u ) = xu123v ( v ) ! f 123 ( v ) = xv123v
and the second term
p ( u; v ) f 123 ( u )
f 123 ( u ) = xv123v
similarly , for the fourth and fth terms , we can show
p ( v; u ) f 123 ( v ) = xv123v
( f ) = xv123v f 123 ( v )
f 123 ( u ) = xv123v
( v ) p ( v; u ) f ( v ) f ( u )
which completes the proof .
( u ) p ( u; v ) f ( u ) f ( v )
learning from labeled and unlabeled data on a directed graph
lemma 123 .
the eigenvalues of the operator are in ( 123; 123 ) ; and the eigenvector with the eigenvalue equal to 123 is p :
it is easy to see that is similar to the operator
: h ( v ) ! h ( v ) dened by =p + 123p t =123 : hence and have the same set of eigenvalues .
as - sume that f is the eigenvector of with eigenvalue : choose a vertex v such that jf ( v ) j = maxu123v jf ( u ) j : then we can show that jj 123 by
jjjf ( v ) j =
( v; u ) f ( u ) = jf ( v ) j
p ( v; u ) + xu ! v
( v; u ) jf ( v ) j
= jf ( v ) j :
in addition , we can show that p = p by
theorem 123 .
the solution of ( 123 ) is f = ( 123 ) ( i ) 123y; where = 123= ( 123 + ) :
( u ) p ( u; v ) +p ( v ) xuv
from lemma 123 , dierentiating ( 123 ) with re - spect to function f; we have ( i ) f + ( f y ) = 123 : dene = 123= ( 123 + ) : this system may be written ( i ) f = ( 123 ) y : from lemma 123 , we easily know that ( i ) is positive denite and thus in - vertible .
this completes the proof .
at the beginning of this section , we assume the graph to be strongly connected and aperiodic such that the natural random walk over the graph converges to a unique and positive stationary distribution .
obviously this assumption cannot be guaranteed for a general di - rected graph .
to remedy this problem , we may intro - duce the so - called teleporting random walk ( page et al . , 123 ) as the replacement of the natural one
that we are currently at vertex u with d+ ( u ) > 123; the next step of this random walk proceeds as follows : ( 123 ) with probability 123 jump to a vertex chosen uni - formly at random over the whole vertex set except u; and ( 123 ) with probability w ( ( u; v ) ) =d+ ( u ) jump to a vertex v adjacent from u : if we are at vertex u with d+ ( u ) = 123; just jump to a vertex chosen uniformly at random over the whole vertex set except u :
algorithm .
given a directed graph g = ( v; e ) and a label set y = f123;123g; the vertices in a subset s v are labeled .
then the remaining unlabeled vertices may be classied as follows :
dene a random walk over g with a transition probability matrix p such that it has a unique sta - tionary distribution , such as the teleporting ran -
let denote the diagonal matrix with its di - agonal elements being the stationary distribu - tion of the random walk .
compute the matrix = ( 123=123p 123=123 + 123=123p t 123=123 ) =123 :
dene a function y on v with y ( v ) = 123 or 123 if vertex v is labeled as 123 or 123; and 123 if v is unla - beled .
compute the function f = ( i ) 123y; where is a parameter in ) 123; 123 ( ; and classify each unlabeled vertex v as sign f ( v ) :
it is worth mentioning that the approach of zhou ( 123 ) can also be derived from this algo - rithmic framework by dening a two - step random walk .
assume a directed graph g = ( v; e ) with d+ ( v ) > 123 and d ( v ) > 123 for all v 123 v : given that we are currently at vertex u; the next step of this random walk proceeds as follows : rst jump back - ward to a vertex h adjacent to u with probability p ( u; h ) = w ( ( h; u ) ) =d ( u ) ; then jump forward to a vertex v adjacent from u with probability p+ ( h; v ) = w ( ( h; v ) ) =d+ ( h ) : thus the transition probability from
to show that the stationary distribution of the random
u to v is p ( u; v ) = ph123v p ( u; h ) p+ ( h; v ) : it is easy walk is ( v ) = d ( v ) =pu123v d ( u ) for all v 123 v : sub -
stituting the quantities of p ( u; v ) and ( v ) into ( 123 ) , we then recover one of the two regularizers proposed by zhou et al .
( 123 ) .
the other one can also be recov - ered simply by reversing this two - step random walk .
now we discuss implementation issues .
the closed form solution shown in theorem 123 involves a ma - trix inverse .
given an n n invertible matrix a; the time required to compute the inverse a123 is gener - ally o ( n123 ) and the representation of the inverse re - quires ( n123 ) space .
recent progress in numerical anal -
learning from labeled and unlabeled data on a directed graph
ysis ( spielman & teng , 123 ) , however , shows that , for an n n symmetric positive semi - denite , diag - onally dominant matrix a with m non - zero entries and a n - vector b; we can obtain a vector ~ x within rel - ative distance of the solution to ax = b in time o ( m123 : 123 log ( nf ( a ) = ) o ( 123 ) ) ; where f ( a ) is the log of the ratio of the largest to smallest non - zero eigenvalue of a : it can be shown that our approach can benet from this numerical technique .
from theorem 123 ,
123=123p 123=123 + 123=123p t 123=123
f = ( 123 ) y;
which may be transformed into
p + p t
let a = is diagonally dominant .
p + p t
( 123=123f ) = ( 123 ) 123=123y :
: it is easy to verify that a
for well understanding this regularization framework , we may compare it with an alternative approach in which the regularizer is dened by
( f ) = x ( u;v ) 123e
w ( ( u; v ) ) f ( u )
a similar closed form solution can be obtained from the corresponding optimization problem .
clearly , for undirected graphs , this functional also reduces to that in ( zhou et al . , 123 ) .
at rst glance , this functional may look natural , but in the later experiments we will show that the algorithm based on this functional does not work as well as the previous one .
this is because the directionality is only slightly taken into account by this functional via the degree normalization such that much valuable information for classication conveyed by the directionality is ignored by the corresponding algorithm .
once we remove the degree normalization from this functional , then the resulted functional is totally insensitive to the directionality .
directed spectral clustering
in the absence of labeled instances , this framework can be utilized in an unsupervised setting as a spectral clustering method for directed graphs .
we rst dene a combinational partition criterion , which generalizes the normalized cut criterion for undirected graphs ( shi & malik , 123 ) .
then relaxing the combinational op - timization problem into a real - valued one leads to the functional dened in section 123
given a subset s of the vertices of a directed graph g;
dene the volume of s by vol s : =pv123s ( v ) : clearly ,
figure 123
a subset s and its complement s c .
note that there is only one edge in the out - boundary of s :
vol s is the probability with which the random walk occupies some vertex in s and consequently vol v = 123 : let sc denote the complement of s ( fig .
the out - @s of s is dened by @s : = f ( u; v ) ju 123 s; v 123 scg : the value vol @s : = p ( u;v ) 123@s ( u ) p ( u; v ) is called the volume of @s : note that vol @s is the probability with which one sees a jump from s to s c :
generalizing the normalized cut criterion for undi - rected graphs is based on a key observation stated by proposition 123 .
vol @s = vol @s c :
it immediately follows from that the probabil - ity with which the random walk leaves a vertex equals the probability with which the random walk arrives at this vertex .
formally , for each vertex v in v; it is easy to see that
( u ) p ( u; v ) xuv
( v ) p ( v; u ) = 123 :
summing the above equation over the vertices of s ( see also fig .
123 ) , then we have
= x ( u;v ) 123@s c
( u ) p ( u; v ) xu ! v ( u ) p ( u; v ) x ( u;v ) 123@s
( u ) p ( u; v ) = 123;
which completes the proof .
from proposition 123 , we may partition the vertex set of a directed graph into two nonempty parts s and s c
ncut ( s ) = vol @s ( cid : 123 ) 123
vol sc ;
which is a directed generalization of the normalized cut criterion for undirected graphs .
clearly , the ra - tio of vol @s to vol s is the probability with which the
learning from labeled and unlabeled data on a directed graph
random walk leaves s in the next step under the con - dition that it is in fact in s now .
similarly understand the ratio of vol @sc to vol sc :
in the following , we show that the functional ( 123 ) can be recovered from ( 123 ) .
dene an indicator function h 123 h ( v ) by h ( v ) = 123 if v 123 s; and 123 if v 123 s c : denote by the volume of s : clearly , we have 123 < < 123 due to s g : then ( 123 ) may be written ( u ) p ( u; v ) ( h ( u ) h ( v ) ) 123
ncut ( s ) = p ( u;v ) 123e
dene another function g 123 h ( v ) by g ( v ) = 123 ( 123 ) if v 123 s; and 123 if v 123 sc : we easily know that sign g ( v ) = sign h ( v ) for all v 123 v and h ( u ) h ( v ) = g ( u ) g ( v ) for all u; v 123 v : moreover , it is not hard to see that pv123v ( v ) g ( v ) = 123; and pv123v ( v ) g123 ( v ) = 123 ( 123 ) : therefore ncut ( s ) = p ( u;v ) 123e
( u ) p ( u; v ) ( g ( u ) g ( v ) ) 123
dene another function f = pg : then the above equation may be further transformed into
ncut ( s ) = p ( u;v ) 123e
( u ) p ( u; v ) f ( u )
if we allow the function f to take arbitrary real values , then the graph partition problem ( 123 ) becomes
f 123h ( v )
kfk = 123; hf; pi = 123 :
from lemma 123 , it is easy to see that the solution of ( 123 ) is the normalized eigenvector of the operator with the second largest eigenvalue .
algorithm .
given a directed graph g = ( v; e ) ; it may be partitioned into two parts as follows :
dene a random walk over g with a transition probability matrix p such that it has a unique
let denote the diagonal matrix with its di - agonal elements being the stationary distribu - tion of the random walk .
compute the matrix = ( 123=123p 123=123 + 123=123p t 123=123 ) =123 :
compute the eigenvector ' of corresponding to the second largest eigenvalue , and then partition the vertex set v of g into two parts s = fv 123 v j ' ( v ) 123g and sc = fv 123 v j ' ( v ) < 123g :
it is easy to extend this approach to k - partition .
as - sume a k - partition to be v = v123 ( v123 ( ( vk; where vi \ vj = ; for all 123 i; j k : let pk denote a k - partition .
then we may obtain a k - partition by mini -
ncut ( pk ) = x123ik
it is not hard to show that the solution of the corre - sponding relaxed optimization problem of ( 123 ) can be any orthonormal basis for the linear space spanned by the eigenvectors of pertaining to the k largest eigen -
discrete analysis
we develop discrete analysis on directed graphs .
the regularization framework in section 123 is then recon - structed and generalized using discrete analysis .
this work is the discrete analogue of classic regularization theory ( tikhonov & arsenin , 123; wahba , 123 ) .
we dene the graph gradient to be an operator r : h ( v ) ! h ( e ) which satises
( rf ) ( ( u; v ) ) : =p ( u ) s p ( u; v )
f ( v ) s p ( u; v )
f ( u ) ! :
for an undirected graph , equation ( 123 ) reduces to
( rf ) ( ( u; v ) ) =s w ( ( u; v ) )
f ( v ) s w ( ( u; v ) )
we may also dene the graph gradient of function f at each vertex v as rf ( v ) : = f ( rf ) ( ( v; u ) ) j ( v; u ) 123 eg; which is often denoted by rvf : then the norm of the graph gradient rf at v is dened by
and the p - dirichlet form
( rf ) 123 ( ( v; u ) ) !
p ( f ) : =
krvfkp; p 123 ( 123;123 ( :
note that 123 ( f ) = ( f ) : intuitively , the norm of the graph gradient measures the smoothness of a function around a vertex , and the p - dirichlet form the smooth - ness of a function over the whole graph .
in addition , we dene krf ( ( v; u ) ) k : = krvfk : note that krfk is dened in the space h ( e ) as krfk = hrf;rfi123=123
we dene the graph divergence to be an operator div : h ( e ) ! h ( v ) which satises
hrf; gih ( e ) = hf; div gih ( v )
learning from labeled and unlabeled data on a directed graph
for any two functions f and g in h ( e ) : equation ( 123 ) is a discrete analogue of the stokes theorem 123
it is not hard to show that
( div g ) ( v ) =
p ( v ) ( cid : 123 ) xuvp ( v ) p ( ( v; u ) ) g ( ( v; u ) ) xu ! vp ( u ) p ( u; v ) g ( ( u; v ) ) :
intuitively , we may think of the graph divergence ( div g ) ( v ) as the net out ( cid : 123 ) ow of the function g at the vertex v : for a function c : e ! r dened by c ( ( u; v ) ) =p ( u ) p ( u; v ) ; it follows from equation ( 123 ) that ( div c ) ( v ) = 123 at any vertex v in v : we dene the graph laplacian to be an operator : h ( v ) ! h ( v ) which satises 123
we easily know that the graph laplacian is linear , self - adjoint and positive semi - denite .
substituting ( 123 ) and ( 123 ) into ( 123 ) , we obtain
( f ) ( v ) = f ( v )
( u ) p ( u; v ) f ( u )
( v ) p ( v; u ) f ( u )
in matrix notation , can be written as
123=123p 123=123 + 123=123p t 123=123
which is just the laplace matrix for directed graphs proposed by chung ( to appear ) .
for an undirected graph , equation ( 123 ) clearly reduces to the laplacian for undirected graphs ( chung , 123 ) .
we dene the graph p - laplacian to be an operator p : h ( v ) ! h ( v ) which satises
clearly , 123 = ; and p ( p 123= 123 ) is nonlinear .
addition , p ( f ) = hpf; fi : now we consider the optimization problem
f 123h ( v ) ' p ( f ) + kf yk123 :
123given a compact riemannian manifold ( m; g ) with a function f 123 c123 ( m ) and a vector eld x 123 x ( m ) ;
it follows from the stokes theorem that rm hrf; xi = rm ( div x ) f :
123the laplace - beltrami operator : c 123 ( m ) ! c123 ( m ) is dene by f = div ( rf ) : the additional fac - tor 123=123 in ( 123 ) is due to edges being oriented .
let f denote the solution of ( 123 ) .
it is not hard to
ppf + 123 ( f y ) = 123 :
when p = 123; f + ( f y ) = 123; as we have shown before , which leads to the closed form solution in the - orem 123 .
when p 123= 123; we are not aware of any closed
we address the web categorization task on the webkb dataset ( see http : / / www - 123cs . cmu . edu / ~ webkb / ) .
we only consider a subset containing the pages from the four universities cornell , texas , washington and wis - consin , from which we remove the isolated pages , i . e . , the pages which have no incoming and outgoing links , resulting in 123 , 123 , 123 and 123 pages respectively , for a total of 123
these pages have been manually classied into the following seven categories : student , faculty , sta , department , course , project and other .
we may assign a weight to each hyperlink according to the textual content or the anchor text .
however , here we are only interested in how much we can obtain from link structure only and hence adopt the canonical
we compare the approach in section 123 with its coun - terpart based on ( 123 ) .
moreover , we also compare both methods with the schemes in ( zhou et al . , 123; zhou et al . , 123 ) .
for the last approach , we transform a directed graph into an undirected one by dening a symmetric weight function as w ( ( u; v ) ) = 123 if ( u; v ) or ( v; u ) in e : to distinguish among these approaches , we refer to them as distribution regularization , degree regularization , second - order regularization and undi - rected regularization respectively .
as we have shown , both the distribution and degree regularization ap - proaches are generalizations of the undirected regu -
the investigated task is to discriminate the student pages in a university from the non - student pages in the same university .
we further remove the isolated pages in each university .
other categories including faculty and course are considered as well .
for all approaches , the regularization parameter is set to = 123 : 123 as in ( zhou et al . , 123 ) .
in the distribution regularization approach , we adopt the teleporting random walk with a small jumping probability = 123 : 123 for obtaining a unique and positive stationary distribution .
the test - ing errors are averaged over 123 trials .
in each trial , it is randomly decided which of the training points if there is no labeled point existed for some class , we sample again .
the experimental re -
learning from labeled and unlabeled data on a directed graph
# labeled points
# labeled points
# labeled points
( a ) cornell ( student )
( b ) texas ( student )
( c ) washington ( student )
# labeled points
# labeled points
# labeled points
( d ) wisconsin ( student )
( e ) cornell ( faculty )
( f ) cornell ( course )
figure 123
classication on the webkb dataset .
( a ) - ( d ) depict the test errors of the regularization approaches on the classication problem of student vs .
non - student in each university .
( e ) - ( f ) illustrate the test errors of these methods on the classication problems of faculty vs .
non - faculty and course vs .
non - course in cornell university .
sults are shown in fig .
the distribution regulariza - tion approach shows signicantly improved results in comparison to the degree regularization method .
fur - thermore , the distribution regularization approach is comparable with the second - order regularization one .
in contrast , the degree regularization approach shows similar performance to the undirected regularization one .
therefore we can conclude that the degree reg - ularization approach almost does not take the direc - tionality into account .
