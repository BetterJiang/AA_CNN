the google search engine has enjoyed huge success with its web page ranking algorithm , which exploits global , rather than local , hyperlink structure of the web using random walks .
here we propose a simple universal ranking algorithm for data lying in the euclidean space , such as text or image data .
the core idea of our method is to rank the data with respect to the intrinsic manifold structure collectively revealed by a great amount of data .
encouraging experimental results from synthetic , image , and text data illustrate the validity of our method .
the google search engine ( 123 ) accomplishes web page ranking using pagerank algorithm , which exploits the global , rather than local , hyperlink structure of the web ( 123 ) .
intuitively , it can be thought of as modelling the behavior of a random surfer on the graph of the web , who simply keeps clicking on successive links at random and also periodically jumps to a random page .
the web pages are ranked according to the stationary distribution of the random walk .
empirical results show pagerank is superior to the naive ranking method , in which the web pages are simply ranked according to the sum of inbound hyperlinks , and accordingly only the local structure of the web is exploited .
our interest here is in the situation where the objects to be ranked are represented as vectors in euclidean space , such as text or image data .
our goal is to rank the data with respect to the intrinsic global manifold structure ( 123 , 123 ) collectively revealed by a huge amount of data .
we believe for many real world data types this should be superior to a local method , which rank data simply by pairwise euclidean distances or inner products .
let us consider a toy problem to explain our motivation .
we are given a set of points constructed in two moons pattern ( figure 123 ( a ) ) .
a query is given in the upper moon , and the task is to rank the remaining points according to their relevances to the query .
intuitively , the relevant degrees of points in the upper moon to the query should decrease along the moon shape .
this should also happen for the points in the lower moon .
furthermore , all of the points in the upper moon should be more relevant to the query than the points in the lower moon .
if we rank the points with respect to the query simply by euclidean distance , then the left - most points in the lower moon will be more relevant to the query than the right - most points in the upper moon ( figure 123 ( b ) ) .
apparently this result is not consistent with our intuition ( figure 123 ( c ) ) .
we propose a simple universal ranking algorithm , which can exploit the intrinsic manifold
( a ) two moons ranking problem
( b ) ranking by euclidean distance
( c ) ideal ranking
figure 123 : ranking on the two moons pattern .
the marker sizes are proportional to the ranking in the last two ( cid : 123 ) gures .
( a ) toy data set with a single query; ( b ) ranking by the euclidean distances; ( c ) ideal ranking result we hope to obtain .
structure of data .
this method is derived from our recent research on semi - supervised learn - ing ( 123 ) .
in fact the ranking problem can be viewed as an extreme case of semi - supervised learning , in which only positive labeled points are available .
an intuitive description of our method is as follows .
we ( cid : 123 ) rst form a weighted network on the data , and assign a positive ranking score to each query and zero to the remaining points which are ranked with respect to the queries .
all points then spread their ranking score to their nearby neighbors via the weighted network .
the spread process is repeated until a global stable state is achieved , and all points except queries are ranked according to their ( cid : 123 ) nal ranking scores .
the rest of the paper is organized as follows .
section 123 describes the ranking algorithm in detail .
section 123 discusses the connections with pagerank .
section 123 further introduces a variant of pagerank , which can rank the data with respect to the speci ( cid : 123 ) c queries .
finally , section 123 presents experimental results on toy data , on digit image , and on text documents , and section 123 concludes this paper .
given a set of point x = fx123; : : : ; xq; xq+123; : : : ; xng ( cid : 123 ) rm; the ( cid : 123 ) rst q points are the queries and the rest are the points that we want to rank according to their relevances to the queries .
let d : x ( cid : 123 ) x ( cid : 123 ) ! r denote a metric on x , such as euclidean distance , which assigns each pair of points xi and xi a distance d ( xi; xj ) : let f : x ( cid : 123 ) ! r denote a ranking function which assigns to each point xi a ranking value fi : we can view f as a vector f = ( f123; : : ; fn ) t : we also de ( cid : 123 ) ne a vector y = ( y123; : : ; yn ) t , in which yi = 123 if xi is a query , and yi = 123 otherwise .
if we have prior knowledge about the con ( cid : 123 ) dences of queries , then we can assign different ranking scores to the queries proportional to their respective the algorithm is as follows :
sort the pairwise distances among points in ascending order .
repeat connecting the two points with an edge according the order until a connected graph is ob -
form the af ( cid : 123 ) nity matrix w de ( cid : 123 ) ned by wij = exp ( ( cid : 123 ) d123 ( xi; xj ) =123 ( cid : 123 ) 123 ) if there is an edge linking xi and xj : note that wii = 123 because there are no loops in the
symmetrically normalize w by s = d ( cid : 123 ) 123=123w d ( cid : 123 ) 123=123 in which d is the diagonal
matrix with ( i; i ) - element equal to the sum of the i - th row of w :
iterate f ( t + 123 ) = ( cid : 123 ) sf ( t ) + ( 123 ( cid : 123 ) ( cid : 123 ) ) y until convergence , where ( cid : 123 ) is a parameter
in ( 123; 123 ) :
let f ( cid : 123 )
i denote the limit of the sequence ffi ( t ) g : rank each point xi according its
ranking scores f ( cid : 123 )
i ( largest ranked ( cid : 123 ) rst ) .
this iteration algorithm can be understood intuitively .
first a connected network is formed in the ( cid : 123 ) rst step .
the network is simply weighted in the second step and the weight is symmetrically normalized in the third step .
the normalization in the third step is necessary to prove the algorithms convergence .
in the forth step , all points spread their ranking score to their neighbors via the weighted network .
the spread process is repeated until a global stable state is achieved , and in the ( cid : 123 ) fth step the points are ranked according to their ( cid : 123 ) nal ranking scores .
the parameter ( cid : 123 ) speci ( cid : 123 ) es the relative contributions to the ranking scores from neighbors and the initial ranking scores .
it is worth mentioning that self - reinforcement is avoided since the diagonal elements of the af ( cid : 123 ) nity matrix are set to zero in the second step .
in addition , the information is spread symmetrically since s is a symmetric matrix .
about the convergence of this algorithm , we have the following theorem : theorem 123 the sequence ff ( t ) g converges to f ( cid : 123 ) = ( cid : 123 ) ( i ( cid : 123 ) ( cid : 123 ) s ) ( cid : 123 ) 123y; where ( cid : 123 ) = 123 ( cid : 123 ) ( cid : 123 ) : see also ( 123 ) for the rigorous proof .
here we only demonstrate how to obtain such a closed form expression .
suppose f ( t ) converges to f ( cid : 123 ) : substituting f ( cid : 123 ) for f ( t + 123 ) and f ( t ) in the iteration equation f ( t + 123 ) = ( cid : 123 ) sf ( f ) + ( 123 ( cid : 123 ) ( cid : 123 ) ) y; we have
f ( cid : 123 ) = ( cid : 123 ) f ( cid : 123 ) + ( 123 ( cid : 123 ) ( cid : 123 ) ) y;
which can be transformed into
since ( i ( cid : 123 ) ( cid : 123 ) s ) is invertible , we have
( i ( cid : 123 ) ( cid : 123 ) s ) f ( cid : 123 ) = ( 123 ( cid : 123 ) ( cid : 123 ) ) y :
f ( cid : 123 ) = ( 123 ( cid : 123 ) ( cid : 123 ) ) ( i ( cid : 123 ) ( cid : 123 ) s ) ( cid : 123 ) 123y :
clearly , the scaling factor ( cid : 123 ) does not make contributions for our ranking task .
hence the closed form is equivalent to
f ( cid : 123 ) = ( i ( cid : 123 ) ( cid : 123 ) s ) ( cid : 123 ) 123y :
we can use this closed form to compute the ranking scores of points directly .
in large - scale real - world problems , however , we prefer using iteration algorithm .
our experiments show that a few iterations are enough to yield high quality ranking results .
123 connections with google
let g = ( v; e ) denote a directed graph with vertices .
let w denote the n ( cid : 123 ) n adjacency matrix w; in which wij = 123 if there is a link in e from vertex xi to vertex xj; and wij = 123 otherwise .
note that w is possibly asymmetric .
de ( cid : 123 ) ne a random walk on g determined by the following transition probability matrix
p = ( 123 ( cid : 123 ) ( cid : 123 ) ) u + ( cid : 123 ) d ( cid : 123 ) 123w;
where u is the matrix with all entries equal to 123=n .
this can be interpreted as a probability ( cid : 123 ) of transition to an adjacent vertex , and a probability 123 ( cid : 123 ) ( cid : 123 ) of jumping to any point on the graph uniform randomly .
then the ranking scores over v computed by pagerank is given by the stationary distribution ( cid : 123 ) of the random walk .
in our case , we only consider graphs which are undirected and connected .
clearly , w is symmetric in this situation .
if we also rank all points without queries using our method , as is done by google , then we have the following theorem :
theorem 123 for the task of ranking data represented by a connected and undirected graph without queries , f ( cid : 123 ) and pagerank yield the same ranking list .
we ( cid : 123 ) st show that the stationary distribution ( cid : 123 ) of the random walk used in google is proportional to the vertex degree if the graph g is undirected and connected .
let 123 denote the 123 ( cid : 123 ) n vector with all entries equal to 123
we have
123dp = 123d ( ( 123 ( cid : 123 ) ( cid : 123 ) ) u + ( cid : 123 ) d ( cid : 123 ) 123w ) = ( 123 ( cid : 123 ) ( cid : 123 ) ) 123du + ( cid : 123 ) 123dd ( cid : 123 ) 123w
= ( 123 ( cid : 123 ) ( cid : 123 ) ) 123d + ( cid : 123 ) 123w = ( 123 ( cid : 123 ) ( cid : 123 ) ) 123d + ( cid : 123 ) 123d = 123d :
let vol g denote the volume of g; which is given by the sum of vertex degrees .
the stationary distribution is then
( cid : 123 ) = 123d=vol g :
note that ( cid : 123 ) does not depend on ( cid : 123 ) : hence ( cid : 123 ) is also the the stationary distribution of the random walk determined by the transition probability matrix d ( cid : 123 ) 123w : now we consider the ranking result given by our method in the situation without queries .
the iteration equation in the fourth step of our method becomes
a standard result ( 123 ) of linear algebra states that if f ( 123 ) is a vector not orthogonal to the principal eigenvector , then the sequence ff ( t ) g converges to the principal eigenvector of s .
let 123 denotes the n ( cid : 123 ) 123 vector with all entries equal to 123 : then
f ( t + 123 ) = sf ( t ) :
sd123=123 = d ( cid : 123 ) 123=123w d ( cid : 123 ) 123=123d123=123 = d ( cid : 123 ) 123=123w 123 = d ( cid : 123 ) 123=123d123 = d123=123 :
further , noticing that the maximal eigenvalue of s is 123 ( 123 ) , we know the principal eigen - vector of s is d123=123 : hence
f ( cid : 123 ) = d123=123 :
comparing ( 123 ) with ( 123 ) , it is clear that f ( cid : 123 ) and ( cid : 123 ) give the same ranking list .
this completes
123 personalized google
although pagerank is designed to rank all points without respect to any query , it is easy to modify for query - based ranking problems .
let p = d ( cid : 123 ) 123w : the ranking scores given by pagerank are the elements of the convergence solution ( cid : 123 ) ( cid : 123 ) of the iteration equation
by analogy with the algorithm in section 123 , we can add a query term on the right - hand side of ( 123 ) for the query - based ranking ,
( cid : 123 ) ( t + 123 ) = ( cid : 123 ) p t ( cid : 123 ) ( t ) :
( cid : 123 ) ( t + 123 ) = ( cid : 123 ) p t ( cid : 123 ) ( t ) + ( 123 ( cid : 123 ) ( cid : 123 ) ) y :
this can be viewed as the personalized version of pagerank .
we can show that the se - quence f ( cid : 123 ) ( t ) g converges to ( cid : 123 ) ( cid : 123 ) = ( 123 ( cid : 123 ) ( cid : 123 ) ) ( i ( cid : 123 ) ( cid : 123 ) p t ) ( cid : 123 ) 123y as before , which is equivalent
( cid : 123 ) ( cid : 123 ) = ( i ( cid : 123 ) ( cid : 123 ) p t ) ( cid : 123 ) 123y :
now let us analyze the connection between ( 123 ) and ( 123 ) .
note that ( 123 ) can be transformed
( cid : 123 ) ( cid : 123 ) = ( ( d ( cid : 123 ) ( cid : 123 ) w ) d ( cid : 123 ) 123 ) ( cid : 123 ) 123y = d ( d ( cid : 123 ) ( cid : 123 ) w ) ( cid : 123 ) 123y :
in addition , f ( cid : 123 ) can be represented as
f ( cid : 123 ) = ( d ( cid : 123 ) 123=123 ( d ( cid : 123 ) ( cid : 123 ) w ) d ( cid : 123 ) 123=123 ) ( cid : 123 ) 123y = d123=123 ( d ( cid : 123 ) ( cid : 123 ) w ) ( cid : 123 ) 123d123=123y :
hence the main difference between ( cid : 123 ) ( cid : 123 ) and f ( cid : 123 ) is that in the latter the initial ranking score yi of each query xi is weighted with respect to its degree .
the above observation motivates us to propose a more general personalized pagerank
in which we assign different importance to queries with respect to their degree .
the closed form of ( 123 ) is given by
( cid : 123 ) ( t + 123 ) = ( cid : 123 ) p t ( cid : 123 ) ( t ) + ( 123 ( cid : 123 ) ( cid : 123 ) ) dky;
( cid : 123 ) ( cid : 123 ) = ( i ( cid : 123 ) ( cid : 123 ) p t ) ( cid : 123 ) 123dky :
if k = 123; ( 123 ) is just ( 123 ) ; and if k = 123; we have
( cid : 123 ) ( cid : 123 ) = ( i ( cid : 123 ) ( cid : 123 ) p t ) ( cid : 123 ) 123dy = d ( d ( cid : 123 ) ( cid : 123 ) w ) ( cid : 123 ) 123dy;
which is almost as same as ( 123 ) .
we can also use ( 123 ) for classi ( cid : 123 ) cation problems without any modi ( cid : 123 ) cation , besides setting the elements of y to 123 or - 123 corresponding to the positive or negative classes of the labeled points , and 123 for the unlabeled data .
this shows the ranking and classi ( cid : 123 ) cation problems are closely related .
we can do a similar analysis of the relations to kleinbergs hits ( 123 ) , which is another popular web page ranking algorithm .
the basic idea of this method is also to iteratively spread the ranking scores via the existing web graph .
we omit further discussion of this method due to lack of space .
we validate our method using a toy problem and two real - world domains : image and text .
in our following experiments we use the closed form expression in which ( cid : 123 ) is ( cid : 123 ) xed at 123 : 123 : as a true labeling is known in these problems , i . e .
the image and document categories ( which is not true in real - world ranking problems ) , we can compute the ranking error using the receiver operator characteristic ( roc ) score ( 123 ) to evaluate ranking algorithms .
the returned score is between 123 and 123 , a score of 123 indicating a perfect ranking .
123 toy problem
in this experiment we considered the toy ranking problem mentioned in the introduction section .
the connected graph described in the ( cid : 123 ) rst step of our algorithm is shown in figure 123 ( a ) .
the ranking scores with different time steps : t = 123; 123; 123; 123 are shown in figures 123 ( b ) - ( e ) .
note that the scores on each moon decrease along the moon shape away from the query , and the scores on the moon containing the query point are larger than on the other moon .
ranking by euclidean distance is shown in figure 123 ( f ) , which fails to capture the two moons structure .
it is worth mentioning that simply ranking the data according to the shortest paths ( 123 ) on the graph does not work well .
in particular , we draw the readers attention to the long edge in figure 123 ( a ) which links the two moons .
it appears that shortest paths are sensitive to the small changes in the graph .
the robust solution is to assemble all paths between two points , and weight them by a decreasing factor .
this is exactly what we have done .
note that the closed form can be expanded as f ( cid : 123 ) = pi ( cid : 123 ) isiy :
in this experiment we address a task of ranking on the usps handwritten 123x123 digits dataset .
we rank digits from 123 to 123 in our experiments .
there are 123 , 123 , 123 , 123 , 123 and 123 examples for each class , for a total of 123 examples .
( a ) connected graph
figure 123 : ranking on the pattern of two moons .
( a ) connected graph; ( b ) - ( e ) ranking with the different time steps : t = 123; 123; 123; 123; ( f ) ranking by euclidean distance .
( a ) query digit 123
( d ) query digit 123
( b ) query digit 123
( e ) query digit 123
( c ) query digit 123
( e ) query digit 123
figure 123 : roc on usps for queries from digits 123 to 123
note that this experimental results also provide indirect proof of the intrinsic manifold structure in usps .
figure 123 : ranking digits on usps .
the top - left digit in each panel is the query .
the left panel shows the top 123 by the manifold ranking; and the right panel shows the top 123 by the euclidean distance based ranking .
note that there are many more 123s with knots in the
we randomly select examples from one class of digits to be the query set over 123 trials , and then rank the remaining digits with respect to these sets .
we use a rbf kernel with the width ( cid : 123 ) = 123 : 123 to construct the af ( cid : 123 ) nity matrix w; but the diagonal elements are set to zero .
the euclidean distance based ranking method is used as the baseline : given a query set fxsg ( s 123 s ) , the points x are ranked according to that the highest ranking is given to the point x with the lowest score of mins123skx ( cid : 123 ) xsk : the results , measured as roc scores , are summarized in figure 123; each plot corresponds to a different query class , from digit one to six respectively .
our algorithm is comparable to the baseline when a digit 123 is the query .
for the other digits , however , our algorithm signi ( cid : 123 ) cantly outperforms the baseline .
this experimental result also provides indirect proof of the underlying manifold structure in the usps digit dataset ( 123 , 123 ) .
the top ranked 123 images obtained by our algorithm and euclidean distance , with a random digit 123 as the query , are shown in figure 123 : the top - left digit in each panel is the query .
note that there are some 123s in the right panel .
furthermore , there are many curly 123s in the right panel , which do not match well with the query : the 123s in the left panel are more similar to the query than the 123s in the right panel .
this subtle superiority makes a great deal of sense in the real - word ranking task , in which users are only interested in very few leading ranking results .
the roc measure is too simple to re ( cid : 123 ) ect this subtle superiority
123 text ranking
in this experiment , we investigate the task of text ranking using the 123 - newsgroups dataset .
we choose the topic rec which contains autos , motorcycles , baseball and hockey from the the articles are processed by the rainbow software package with the following options : ( 123 ) passing all words through the porter stemmer before counting them; ( 123 ) tossing out any token which is on the stoplist of the smart system; ( 123 ) skipping any headers; ( 123 ) ignoring words that occur in 123 or fewer documents .
no further preprocessing was done .
removing the empty documents , we obtain 123 document vectors in a 123 - dimensional space .
finally the documents are normalized into tfidf representation .
we use the ranking method based on normalized inner product as the baseline .
the af ( cid : 123 ) nity matrix w is also constructed by inner product , i . e .
linear kernel .
the roc scores for 123 randomly selected queries for each class are given in figure 123
figure 123 : roc score scatter plots of 123 random queries from the category autos , motor - cycles , baseball and hockey contained in the 123 - newsgroups dataset .
future research should address model selection .
potentially , if one was given a small la - beled set or a query set greater than size 123 , one could use standard cross validation tech - niques .
in addition , it may be possible to look to the theory of stability of algorithms to choose appropriate hyperparameters .
there are also a number of possible extensions to the approach .
for example one could implement an iterative feedback framework : as the user speci ( cid : 123 ) es positive feedback this can be used to extend the query set and improve the ranking output .
finally , and most importantly , we are interested in applying this algorithm to wide - ranging real - word problems .
