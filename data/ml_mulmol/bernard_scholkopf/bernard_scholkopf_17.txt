we explore the use of the so - called zero - norm of the parameters of linear models in learning .
minimization of such a quantity has many uses in a machine learning context : for variable or feature selection , minimizing training error and ensuring sparsity in solutions .
we derive a simple but practical method for achieving these goals and discuss its relationship to existing techniques of minimizing the zero - norm .
the method boils down to implementing a simple modication of vanilla svm , namely via an iterative multiplicative rescaling of the training data .
applications we investigate which aid our discussion include variable and feature selection on biological microarray data , and multicategory classication .
in pattern recognition , many learning machines , given training data xi 123 rn , i = 123 , .
, m with corre - sponding function values yi 123 f ( cid : 123 ) 123g , i = 123 , .
, m , make predictions g ( x ) about points x 123 rn of the
g ( x ) = sign ( w ( cid : 123 ) x + b )
where w 123 rn and b 123 r .
the machines based on linear models include the well - known perceptron of rosenblatt ( 123 ) and support vector machines ( svms ) , ( see e . g . , vapnik , 123 ) , as well as other combination methods , for example those of freund and schapire ( 123 ) .
perhaps because of their limited power , linear models were not used a lot in the late 123s .
at that time , neural networks were very popular and the complex and non - linear decision functions they produced were appealing compared to the simplicity and somewhat limited discrimination capabilities of linear machines .
the latter were thus forgotten for a while until boser et al .
( 123 ) introduced the optimal margin classier that will be later called svm .
like some earlier work ( see for instance mangasarian , 123 , vapnik , 123 ) , the svm connects machine learning to optimization techniques , however , the gathering of theoretical analysis and mathematical programming was original and gave birth to the revival of linear models in the machine learning community .
in particular , the use of the kernel trick to transform a linear machine into a non - linear one was not new , it was already introduced by aizerman et al .
( 123 ) , but the idea to combine it with mathematical programming was a big step .
it strengthens the interest for such machines which are now used in many applications .
c ( cid : 123 ) 123 jason weston , andre elisseeff , bernhard scholkopf and mike tipping .
weston , elisseeff , sch olkopf and tipping
many generalizations of the original work of boser , guyon and vapnik have been proposed .
for instance , bradley and mangasarian ( 123 ) describe the following general procedure is given to discriminate linearly separable data :
subject to : where kwkp =
yi ( w ( cid : 123 ) xi + b ) ( cid : 123 ) 123
is the ( cid : 123 ) p - norm of w .
when p = 123 , this procedure is the same as the optimal margin method .
the generalization capabilities of such linear models have been studied by many researchers who have shown that , roughly speaking , minimizing the ( cid : 123 ) p - norm of w is good for generalization .
such results hold for p ( cid : 123 ) 123
in this paper , we study machines that are obtained as in problem 123 in the extreme case when p ! 123 , which we will call , in a slight abuse of terminology , 123 the minimization of the zero - norm of w , the latter being dened as :
= cardfwijwi 123= 123g
where card is set cardinality .
pattern classication , especially in the context of regularization that enforces sparsity of the weight vector , is deeply connected to the problem of feature selection ( blum and langley , 123 , pro - vide an introduction ) .
in the feature selection problem one would like to select a subset of features while preserving or improving the discriminative ability of a classier .
in many supervised learning problems123 feature selection is important for a variety of reasons : generalization performance , run - ning time requirements , and constraints and interpretational issues imposed by the problem itself .
minimization of the zero - norm provides a natural way of directly addressing the feature selection and pattern classication objectives in a single optimization .
however , this is achieved at the cost of having to solve a very difcult optimization problem which will not necessarily generalize well .
np - hardness for p = 123 , amaldi and kann ( 123 ) show that problem 123 is np - hard : it cannot even be approximated within 123log123e ( n ) , 123e > 123 unless np ( cid : 123 ) dtime ( npoly log ( n ) ) where dtime ( x ) is the class of deterministic algorithms ending in o ( x ) steps .
it means that under rather general assumptions , there is no polynomial time algorithm that can approximate the value of the objective function at optimum n123 within less than n123 ( 123log123e ( n ) ) for all e > 123
the minimization of problem 123 is thus hopeless and very specic approximations need to be dened with well motivated discussions and experiments .
this is the aim of this paper which presents a novel approximation in the context of machine learning .
it turns out that in such a context a simple approximation of problem 123 leads to a sufciently small value of its objective function .
the term sufciently is relative to the goal to be achieved .
for instance , nding a small number of features so that there exists a linear model consistent with a training set does not require nding the smallest number of features .
most of the time reducing the size of the input space is done in order to get better generalization performance and not in order to have a compact representation of the inputs .
so , sometimes nding the smallest 123
note that for p < 123 , k . kp is not a norm , and that we consider here k . kp p rather than k . kp so that the limit exists when
p ! 123
in our experiments we restrict ourselves to the case of pattern recognition .
however , the reasoning also applies to
use of the zero - norm with linear models and kernel methods
subset of features is not desirable since it could lead to overtting if the number of training points is small ( for example consider if there are a very large number of noisy features and a very small training set size then one could pick a small number of the noisy features which appear highly correlated with the target , but do not generalize to new examples ) .
applications the zero norm is directly related to some optimization problems in learning , for ex - ample in minimizing the number of training errors or nding minimal subsets , e . g . , in vector quan - tization .
in some applications we consider , however , the value of the objective function kwk123 123 is not exactly what has to be minimized .
from a machine learning perspective , as we already mentioned , we are often more interested in the generalization performance .
in feature selection problems one is often interested in parameterizing the problem to control the trade off between number of features used and training error ( as well as regularizing appropriately ) .
this has usually been solved in a two stage approach ( feature selection , followed by minimizing error + regularization ) but we show how , by using the zero norm with a constraint on the minimum number of features allowed , one can try to perform the error minimization and feature selection in one step .
the latter is not re - stricted to only two - class models : we will derive a similar formulation as problem 123 for multi - class discrimination .
feature selection for regression , for example , can also be cast into a mathematical problem similar to what we are studying here .
the duality between feature selection and classica - tion errors is known ( ( amaldi and kann , 123 ) ) , and one can use the same technique to solve both problems .
minimizing the classication error on the training set or a trade - off between this error and the number of desired features is another application we deal with in this paper .
previous work on minimizing the zero - norm problem 123 already appeared in the literature : we mentioned the work of amaldi and kann who studied its computational complexity .
in the machine learning community , the rst work we are aware of concerning this problem is that of bradley and mangasarian .
bradley et al .
( 123 ) and bradley and mangasarian ( 123 ) propose an approximation method called fsv , which is used for variable selection and , later , by fung et al .
( 123 ) , for nding sparse kernel expansions .
it is based on an approximation of the step function :
123 = cardfwijwi 123= 123g ( cid : 123 ) ( cid : 123 )
is a parameter that must be chosen .
bradley and mangasarian suggest to set the value of a to 123 , although it is also proposed ( but in practice not attempted ) to slowly increase the value of a order to improve the approximation .
the authors showed that minimizing the above expression can be achieved by solving a sequence of linear programs of the following form : 123
i ( vi v
subject to :
yi ( w ( cid : 123 ) xi + b ) ( cid : 123 ) 123 , v ( cid : 123 ) w ( cid : 123 ) v
is the solution v from the last iteration .
note that each of these iterations nds the while keeping consistency with the con - is sufciently large then an optimal solution of problem 123 is also an
steepest descent direction of the objective ( cid : 123 ) straints .
it is proved that if a 123
the algorithm also can trade off the training error with the variable selection criteria , which we have omitted in our
weston , elisseeff , sch olkopf and tipping
optimal solution of problem 123
despite some experimental evidence that shows that this approxima - tion is appropriate , we see one main drawback of the approach of bradley and mangasarian .
when the dimensionality of the input is large , the optimization problem becomes harder to solve and involves a larger number of variables .
this makes it somewhat restrictive for large problems .
con - trarily to svm optimization problems , the dual of problem 123 does not have sparse constraints and cannot be solved as easily .
to overcome this drawback , a decomposition method has been derived by bradley and mangasarian ( 123 ) .
unfortunately , we have not seen its efciency on datasets with thousands of features and we still argue that even this decomposition requires a signicant amount of computational resources compared to what is required on the same problem for a svm .
another remark about the approach of bradley and mangasarian concerns the hyperparameter .
for large a , problem 123 resembles problem 123 , has a large number of local minima and is very difcult to optimize : ultimately , if a is large enough , the convergence of the procedure is reached at the rst iteration .
on the other hand , if a is too small then the step function is not well approximated and the minimization does not provide a vector with a small number of features .
unfortunately , the sensitivity of the method to a has not been assessed and it is not known whether it depends on the dimensionality of the problem at hand , since only problems with less than 123 features are considered by bradley et al .
( 123 ) and bradley and mangasarian ( 123 ) .
note however that we applied the same technique of bradley and mangasarian and observed that the value a = 123 led to reasonable results when the starting value v
was set to zero .
mainly because of the computational drawback we have just mentioned , we argue that the method of bradley and mangasarian may not be appropriate when dealing with datasets of high dimensionality occurring for instance in domains such as bioinformatics or medical science .
the algorithm we introduce in this paper has been designed to be able to handle a large number of features and to be easily implemented .
there are a number of other works which aim to minimize the zero - norm in a variety of domains .
fung et al .
( 123 ) use a modication of the algorithm just described to try to use as few kernel functions ( training data points ) as possible to construct a decision rule .
the resulting classier is intended to provide fast online decision making for time critical applications .
perez - cruz et al .
( 123 ) describe a scheme to ( more ) directly minimize the training error of svms in the case where the data are linearly inseparable .
usually , in svms an approximation of the step function is used , such as the hinge loss , a piece - wise linear approximation .
the authors instead describe an efcient optimization scheme based on either a sigmoidal or polynomial approximation of the step function .
they showed that this can improve performance in certain situations .
in this work we will also consider the problem of nding a minimal subset of features in spaces induced by kernels , and applying the zero - norm to feature selection tasks .
hence feature selection works such as those by guyon et al .
( 123 ) and weston et al .
( 123b ) where the authors propose algorithms to choose a small number of features for either linear or nonlinear decision rules are also relevant .
related work by takimoto and warmuth ( 123 ) shows how to nd a sparse normal vector in a feature space induced by a polynomial kernel using a kernel version of winnow .
outline of the paper the paper is organized as follows : the next section presents the notation we will use throughout the paper and briey explains some concepts such as feature spaces and kernels .
section 123b presents the new procedure we propose to minimize the zero - norm of a linear model .
it gives a number of examples on how to apply this technique to two - class as well as multiclass models .
in section 123 , we present some experimental results on different variable and feature selection tasks
use of the zero - norm with linear models and kernel methods
for articial problems and for real biological problems from microarray experiments .
section 123 is related to kernel methods and shows how to minimize the zero - norm of the parameters of a non - linear model .
finally , section 123 concludes the work with a discussion .
notation and preliminaries
before presenting our work , we recall some basics of svm and kernel methods .
the svm proce - dure aims at solving the following problem :
subject to :
yi ( w ( cid : 123 ) xi + b ) ( cid : 123 ) 123 x
i ( cid : 123 ) 123
to solve it , it is convenient to consider the dual formulation ( vapnik , 123 , 123 , scholkopf and
smola , 123 , provide detailed discussion and derivation ) :
jyiy j ( xi ( cid : 123 ) x j )
subject to :
iyi = 123 , c ( cid : 123 ) a
i ( cid : 123 ) 123
i are the dual variables related to the constraints yi ( w ( cid : 123 ) xi + b ) ( cid : 123 ) 123 x this problem can then be used to compute the value of w and b .
it turns out that
the solution of
w ( cid : 123 ) x =
iyi ( xi ( cid : 123 ) x )
this means that the decision function g ( x ) dened in the introduction can be computed by using dot products only .
this remark allows us to use any kind of functions k ( x , x if k can be 123 ) corresponds to a dot - product one understood as a dot - product .
to know whether a function k ( x , x can use results from functional analysis , among them mercers theorem .
this theorem shows that under certain conditions , 123 many functions called kernels satisfy the following
123 ) instead of x ( cid : 123 ) x
i ( x123 ) = f ( x123 ) ( cid : 123 ) f ( x123 )
k ( x123 , x123 ) = ( cid : 123 ) i ( x ) , .
. ) 123 ( cid : 123 ) 123
the function f
where f ( x ) = ( f 123 ( x ) , .
, f embeds the points xi into a space such that k ( x123 , x123 ) can be interpreted as a dot product in that space .
this space is called the feature space .
many existing functions such as gaussian kernels k ( x123 , x123 ) = exp ( kx123 x123k123 / s 123 ) and polynomials k ( x123 , x123 ) = ( x123 ( cid : 123 ) x123 + 123 ) d are such kernels .
in the following , we will denote by x ( cid : 123 ) w the component - wise product between two vectors :
x ( cid : 123 ) w = ( x123w123 , .
, xnwn )
we do not go into detail on these conditions here , see work by courant and hilbert ( 123 ) , vapnik ( 123 ) , scholkopf
and smola ( 123 ) .
weston , elisseeff , sch olkopf and tipping
minimizing the zero - norm
in this section we propose a novel method of minimizing the zero norm .
we present different formulations of the minimization of the zero - norm for different machine learning tasks and an opti - mization method for each of them .
these methods are all based on the same idea that is explained in the following subsection , and introduced for two - class linear classication tasks .
123 for two - class linear models
we wish to construct an algorithm which minimizes the zero - norm of linear models .
formally , we would like to solve the following optimization problem :
subject to :
yi ( w ( cid : 123 ) xi + b ) ( cid : 123 ) 123
that is , we wish to nd the separating hyperplane with the fewest nonzero elements in the vector of coefcients w ( note this may not be unique ) .
unfortunately , as we discussed in the introduction , this problem is combinatorially hard ( ( amaldi and kann , 123 ) ) .
we thus propose to study the
ln ( e +jw jj )
yi ( w ( cid : 123 ) xi + b ) ( cid : 123 ) 123
subject to : where 123 < e ( cid : 123 ) 123 has been introduced in order to avoid problems when one of the w j is zero .
this new problem is still not easy since there are many local minima , but at least it can be dealt with by using constrained gradient descent .
the relation with the minimization of the zero - norm can be understood as follows :
let wl ( e ) , also written wl when the context is clear , be a minimizer of problem 123 , and w123 a
minimizer of problem 123 , then we have :
ln ( e +j ( wl ) jj ) ( cid : 123 ) n ( cid : 123 )
ln ( e +j ( w123 ) jj ) .
the following inequality is equivalent to ( 123 ) ,
ln ( e ) + ( cid : 123 )
ln ( e +j ( wl ) jj ) ( cid : 123 ) ( cid : 123 )
ln ( e ) + ( cid : 123 )
ln ( e +j ( w123 ) jj ) ,
123 ) ln ( e ) + ( cid : 123 )
ln ( e +j ( wl ) jj ) ( cid : 123 ) ( nkw123k123
123 ) ln ( e ) + ( cid : 123 )
ln ( e +j ( w123 ) jj ) .
use of the zero - norm with linear models and kernel methods
we thus see that
123 ( cid : 123 ) kw123k123
123 + ( cid : 123 )
123 + ( cid : 123 )
ln ( e +j ( wl ) jj )
ln ( e +j ( wl ) jj )
ln ( e +j ( w123 ) jj )
if wl were independent of the choice of e , then it could be summarized as o we make the additional assumption that all nonzero entries of wl satisfy j ( wl ) jj ( cid : 123 ) d d > 123 ( e . g . , setting d
to equal the machine precision ) , we get
just like w123
if for some xed
123 ( cid : 123 ) kw123k123
and thus the zero norm of wl is almost the same as the one of w123
the approximation to the zero norm given in problem 123 also has a connection with sparse bayesian modelling as shown by tipping ( 123 ) .
this latter framework incorporates an implicit prior over the parameters given by ln p ( w ) = ( cid : 123 ) n j=123 lnjw jj , which may be considered to be mini - mized in some sense , and is clearly closely related to the objective function of problem 123
while this prior is never used explicitly ( computation is performed instead in the space of hyperparameters ) , and the bayesian framework is not formulated in terms of the zero norm , the models obtained are nevertheless highly sparse .
in summary , due to the structure of problem 123 it is always better to set w j to zero when it is possible .
this is due to the form of the logarithm function that decreases quickly to zero compared to its increase for larger values of w j .
said differently , you can increase signicantly one w j without changing that much the objective although , by decreasing w j toward zero , the objective function will decrease a lot .
so , it is better to increase one w j while setting to zero another one rather than doing some compromise between both .
from now on , we will assume that e is equal to the machine precision .
doing so , we will identify e with zero .
to solve problem 123 , we use an iterative method that performs a gradient step at each iteration known as franke and wolfes method ( franke and wolfe , 123 ) .
it is proved to converge to a local minimum .
for the problem of interest , it takes the following form ( see appendix a of weston et al .
123a for more details about the derivation of this
set z = ( 123 ,
subject to :
yi ( w ( cid : 123 ) ( xi ( cid : 123 ) z ) + b ) ( cid : 123 ) 123
let w be the solution of the previous problem .
set z z ( cid : 123 ) w
weston , elisseeff , sch olkopf and tipping
go back to 123 until convergence .
the method we have just dened is thus an approximation of the zero - norm minimization ( arom ) .
it is simply a succession of linear programs combined with a multiplicative update .
sometimes , it can be advantageous to consider fast approximations of this algorithm .
for in - stance , one could take a suboptimal descent direction by using the ( cid : 123 ) 123 - norm instead of the ( cid : 123 ) 123 - norm in step 123
the ( cid : 123 ) 123 - norm has the advantage that it leads to a simple dual formulation which then becomes equivalent to training an svm :
set z = ( 123 , . . , 123 )
jyiy j ( z ( cid : 123 ) xi ( cid : 123 ) z ( cid : 123 ) x j )
subject to :
iyi = 123 , a
i ( cid : 123 ) 123
let w be the solution of the previous problem .
set z z ( cid : 123 ) w
go back to 123 until convergence .
this may be useful for a number of reasons : rstly , the dual form may be much easier to solve if the number of features is larger than the number of examples ( otherwise the primal form is preferable ) .
secondly , there exist several decomposition methods for the dual form of the ( cid : 123 ) 123 - norm in the svm literature ( vapnik , 123 , scholkopf and smola , 123 ) which greatly reduce computational resources required and speed up training times .
finally , in the dual form ( which only employs dot products ) kernel functions can also be used , as we shall show later .
note that the use of the ( cid : 123 ) 123 - norm in our algorithm can also be replaced with the ( cid : 123 ) 123 - norm ( i . e replacing problem 123 with problem 123 ) in later iterations when the solution is already suitably sparse .
this is possible due to the sparsity in z .
this new formulation of problem 123 allows a different interpretation of the algorithm .
intuitively , whether using the ( cid : 123 ) 123 or ( cid : 123 ) 123 - norm , one can understand what the algorithm does in the following way .
on a given iteration suppose one feature is given less weight relative to others ( according to the values in the vector w ) .
on the subsequent iteration the training data are rescaled according to the previous iterations ( stored in the vector z ) , so this feature is likely to achieve an even smaller weighting in the next round , the multiplicative nature of the weighting making its scaling factor zi rapidly decay to zero if it is not necessarily to fulll the separability constraints ( to classify the data correctly ) .
if a features scaling factor zi is forced to zero note that it cannot be increased again .
if , on the other hand , the feature is necessary to describe the data this cannot diminish to zero as its weight wi cannot be assigned a value zero .
the methods developed here apply for a linearly separable training set .
when this is not the case , we make use of the method by freund and schapire ( 123 ) , cortes and vapnik ( 123 ) which involves adding a constant to the diagonal of the kernel matrix .
this is described in the next section .
use of the zero - norm with linear models and kernel methods
123 the linearly inseparable case
it is possible to extend the proposed algorithm to also trade off the training error with the number of features selected , which is necessary in the linearly inseparable case .
for simplicity let us again consider the case of two - class linear models .
introducing slack variables x i for each training point ( bennett and mangasarian , 123 , cortes and vapnik , 123 ) we can in general solve :
jjwjjp + l jjx jjq yi ( w ( cid : 123 ) xi + b ) ( cid : 123 ) 123 x
let us consider the case p = 123 and q = 123
using our approach it is possible to solve this problem by rewriting the training data xi ( xi , 123 i ) j = 123 if i = j and 123 otherwise .
then minimizing problem 123 is equivalent to minimizing ( 123 ) with the original data .
this can be implemented in the ( cid : 123 ) 123 - arom method by adding a ridge to the kernel rather than explicitly storing the extended vectors .
i 123 f123 , 123gn and ( d
i ) where d
note that it is also possible to derive a system to minimize the training error of svms , i . e using p = 123 and q = 123
this is a kind of minimization that researchers have been interested in solving but have rarely in practice been able to solve , although fung et al .
( 123 ) and perez - cruz et al .
( 123 ) also propose solutions to this problem .
123 for multi - class linear models
when many classes are involved , one could use a classical trick that consists in decomposing the multi - class problem into many two - class ones .
often , the one - against - all approach is used : one vector wk and one bias bk are dened for each class k , and the output is computed as :
g ( x ) = arg max
wk ( cid : 123 ) x + bk
here , the vector wk is learned by discriminating the class k from all the other classes .
this gives many two - class problems .
in this framework , the minimization of the zero - norm is done for each vector wk independently of the others .
however , the actual zero - norm we are interested in is the
where q is the number of classes and jwkj stands for the component - wise absolute value of wk .
thus , applying this kind of decomposition scheme adds a suboptimal process to the overall method .
to perform a zero - norm minimization for the multi - class problems , we use the same scheme as the ( cid : 123 ) 123 approximation scheme exposed before , but the original problems we approximate are different and are stated as :
subject to :
weston , elisseeff , sch olkopf and tipping
the constraints can take different forms depending on the way the data has to be discriminated .
for instance , it could be related to the one - against - the - rest approach : for all i 123 f123 , . . , mg
yki ( wk ( cid : 123 ) xi + bk ) ( cid : 123 ) 123 , 123k 123 f123 , . . , qg
where yk 123 f ( cid : 123 ) 123gm is a target vector corresponding to the discrimination of class k from the others , i . e .
yki = 123 iff xi is in class k .
another possibility would be to consider the constraints related to the multi - class svm of weston and watkins ( 123 ) : for all i 123 f123 , . . , mg ( wc ( i ) w j ) ( cid : 123 ) xi + bc ( i ) b j ( cid : 123 ) 123 , 123 j 123 f123 , . . , qg ,
j 123= c ( i )
where c ( i ) is the target class for point xi .
considering the logarithmic approximation as we did previously , we can derive the following
problem ( see appendix b of weston et al .
123a for calculations ) ,
set z = ( 123 ,
subject to :
for k = 123 , . . , q , k 123= c ( i )
( cid : 123 ) ( cid : 123 ) ( xi ( cid : 123 ) z ) + bc ( i ) bk ( cid : 123 ) 123
let ( wk , bk ) be the solution , set z z ( cid : 123 ) ( ( cid : 123 ) q 123
go back to step 123 until convergence .
output z
the features that are selected are those for which the components of the output z are non - zero .
as done before in the binary case , for implementation purposes , the objective function of problem 123 can be replaced by ,
the solution given by problem 123 with this new objective function is no more the steepest direction desired for the gradient descent but only an approximation of it .
this approximation can bring many advantages such as being able to handle input spaces with very high dimensionality .
in the above calculations we have considered only the constraints relative to the multi - class svm of weston and watkins ( 123 ) .
other constraints such as those for the one - against - the - rest approach could be used .
note however that the objective function minimized here is specic to the way we deal with multi - class systems and that it corresponds to an overall goal although the naive one - against - the - rest approach developed for feature selection would be as suggested in the beginning of this section .
the nal algorithm with the one - against - the - rest constraints takes the
use of the zero - norm with linear models and kernel methods
set z = ( 123 ,
subject to :
yki ( wk ( cid : 123 ) xi ( cid : 123 ) z + bk ) ( cid : 123 ) 123 for k = 123 , . . , q
let ( wk , bk ) be the solution , set z z ( cid : 123 ) ( ( cid : 123 ) q 123
go back to step 123 until convergence .
output z
note that the latter procedure is the preferred one for very large data sets with large number of classes .
in this case indeed , problem 123 is equivalent to running a one - against - the - rest linear svm on the data sets where the latter has been scaled by the vector z .
123 using nonlinear functions via kernels
it is also possible to perform a minimization of the zero - norm with nonlinear classiers .
in this situation one would like to nd a minimal set of features in the high dimensional space induced by a kernel .
in contrast to input feature selection the goal of kernel space feature selection is usually of improving generalization performance rather than improving running time or attempting to interpret the decision rule , although it is also possible for these factors to play a role .
note that this is not the same as the objective of fung et al .
( 123 ) in which the authors try to use as few kernel functions as possible to construct a decision rule .
another different but related tech - nique is explored by weston et al .
( 123b ) where the authors implement a nonlinear decision rule via kernels but use a feature selection algorithm to select useful input features rather than selecting features directly in feature space .
finally , in other related work by takimoto and warmuth ( 123 ) , the authors show how to nd a sparse normal vector in a feature space induced by a polynomial kernel using a kernel version of winnow .
minimization of the zero - norm with nonlinear classiers is possible using the ( cid : 123 ) 123 - arom method .
to do this one must compute k ( x , y ) = ( z ( cid : 123 ) f ( x ) ) ( cid : 123 ) ( z ( cid : 123 ) f ( y ) ) .
note that if this is computed explicitly to form a kernel matrix it does not slow down the optimization process ( there are still only m vari - ables ) and can be fast in later iterations when w is sparse .
essentially , one only needs to compute the explicit vectors in feature space for the multiplicative update between optimization steps , the optimization steps still have the same complexity after the kernel matrix is calculated .
this could already be much more efcient than using an algorithm which requires an explicit representation in the optimization step such as required when using the ( cid : 123 ) 123 norm .
nevertheless , to avoid explicit computation in feature space we wish to perform all the steps with kernels .
to do this it will be necessary to compute element - wise multiplication in feature space , i . e . , multiplications in feature space of the form
f ( x ) ( cid : 123 ) f ( y )
weston , elisseeff , sch olkopf and tipping
this is possible with polynomial type kernels . 123 we now show that in this case , the multiplication reduces to a multiplication in the input space .
consider the polynomial kernel of degree d , dened as k ( x , y ) = ( x ( cid : 123 ) y ) d .
we denote its feature map by f d , hence k ( x , y ) = f d ( x ) ( cid : 123 ) f d ( y ) .
letting n denote the input dimension , we have
123 ( cid : 123 ) x ) ( cid : 123 ) f d ( y
123 ( cid : 123 ) y ) ) d
123 ( cid : 123 ) y ) = ( ( x 123 ( cid : 123 ) x ) ( cid : 123 ) ( y
x id xid y
x 123 ) ( cid : 123 ) f d ( x )
xid 123 ) ( cid : 123 ) f d ( y )
using the same reasoning this can be extended to feature spaces with monomials of degree d or less ( polynomials ) by dening f 123 : d ( x ) = hf p ( x ) : 123 ( cid : 123 ) p ( cid : 123 ) di ( which is similar to the conventional polynomial kernel map , but with slightly different scaling factors123 ) and then noticing that f 123 : d ( x ( cid : 123 ) y ) = f 123 : d ( x ) ( cid : 123 ) f 123 : d ( y ) .
this can be calculated with the kernel :
now , to apply it to our algorithm , we need to be able to compute not only z ( cid : 123 ) xi but also the dot product in feature space ( z ( cid : 123 ) xi ) ( cid : 123 ) x j which will be used in the next iteration .
in appendix c of weston et al .
( 123a ) , we show how to perform such a calculation .
it is then possible to perform an approximation of the minimization of the zero - norm in the feature space for polynomial kernels .
contrary to the linear case , it is not easy to explicitly look at the coordinates of the resulting vector w .
it is dened in feature space and only a dot product can be performed easily .
thus , once the algorithm has nished , one can use the resulting classier for prediction but less easily for interpretation .
in the linear case , on the other hand , one can also be interested in interpretation of the sparse solution found : one may be interested in which features are used as well as the quality of the predictor .
in the following sections we will discuss applications of the algorithms we have exposed in the
problems of feature selection and feature selection in spaces induced by kernels .
feature selection
our method of approximately minimizing the zero - norm chooses a small number of features and can therefore be used as a feature selection algorithm .
we describe this method below .
note that it is not surprising that this is impossible with some other types of kernels , for example with gaussian
kernels which are innite dimensional
it turns out we need these particular scaling factors to make the zero - norm minimization feasible .
use of the zero - norm with linear models and kernel methods
123 using the zero - norm for feature selection
feature selection can be implemented using our method in the following way :
yi ( w ( cid : 123 ) xi + b ) ( cid : 123 ) 123 and jjwjj123 ( cid : 123 ) r
for p = f123 , 123g and the desired number of features r .
this method can be approximated by minimiz - ing the zero - norm using the ( cid : 123 ) 123 - arom or ( cid : 123 ) 123 - arom methods , stopping the step - wise minimization when the constraint jjwjj123 ( cid : 123 ) r is met .
one can then re - train a p - norm classier on the features which are the nonzero elements of w123
in this way one is free to choose the parameter r which dictates how many features the classier will use .
this should thus be differentiated from a zero - norm classier which would try to use the minimum number of features possible , which is not always the optimum choice in terms of generalization .
in order to assess the quality of our method , we compare it with using correlation coefcients ( a standard approach ) and also with three recently introduced methods , recursive feature elimination and using the generalization bound r123w 123 , which we briey review in the following section , and the zero - norm minimization algorithm of bradley et al .
( 123 ) .
123 correlation coefcients ( corr )
correlation coefcients score the importance of each feature independently of the other features by comparing that features correlation to the output labels .
the score f j of feature j is given by :
( j ( + ) j ( ) ) 123 j ( + ) ) 123 + ( s
where ( + ) and ( ) are the mean of the feature values for the positive and negative examples respectively , and s
( ) are their respective standard deviations .
( + ) and s
123 recursive feature elimination
recursive feature elimination ( rfe ) is a recently proposed feature selection algorithm described by guyon et al .
( 123 ) .
the method , given that one wishes to employ only r < n input dimensions in the nal decision rule , attempts to nd the best subset r .
the method operates by trying to choose the r features which lead to the largest margin of class separation , using an svm classier .
this combinatorial problem is solved in a greedy fashion at each iteration of training by removing the input dimension that decreases the margin the least until only r input dimensions remain ( this is known as backward selection ) .
jyiy jk ( xi , x j ) is a measure of predictive ability ( and is inversely pro - portionate to the margin ) .
the algorithm is thus to remove features which keep this quantity small .
this can be done with the following iterative procedure :
for svms w 123 ( a ) = ( cid : 123 )
this is necessary because the algorithm , in nding the features , scales them differently to their original values ( be - cause the update process scales the data on each iteration ) .
hence , having found the features , we then use the original scaling factors again in re - training , which we observed gives a slight performance gain .
we thus used this two - stage process in our experiments .
note one cannot do this in the kernel version ( section 123 ) as the original features are no
weston , elisseeff , sch olkopf and tipping
( cid : 123 ) given solution a
, calculate for each feature p :
( p ) ( a ) = ( cid : 123 )
i means training point i with feature p removed ) .
( cid : 123 ) remove the feature with smallest value of jw 123 ( a ) w 123
if the classier is a linear one ( of type g ( x ) = w ( cid : 123 ) x + b ) , this algorithm corresponds to removing the smallest corresponding value of jwij in each iteration .
to speed up computations when the number of features is large the authors suggest to remove half of the features each iteration .
note that rfe has been designed for two - class problems although a multi - class version can be derived easily for a one - against - the - rest approach .
the idea is then to remove the features that lead to the smallest value ( a ) is the corresponding margin based value w 123 ( a k ) for of ( cid : 123 ) q the machine discriminating class k from all the others .
we will consider this implementation of multi - class rfe in the multi - class experiments .
k , ( p ) ( a k ) j where w 123
( a k ) w 123
123 feature selection via r123w 123
an alternative method of using svms for feature selection is described by weston et al .
( 123b ) .
the idea is the following .
feature selection is performed by scaling the input parameters by a real valued vector s i indicate more useful features .
thus the problem is now one of choosing the best kernel of the form :
larger values of s
123 ) = k ( x ( cid : 123 ) s
123 ( cid : 123 ) s )
which means nding the parameters s .
this can be achieved by choosing these hyperparameters via a generalization bound ( or a validation set ) .
for svms , expectation of the error probability has
eperr ( cid : 123 ) 123
r123w 123 ( a 123 )
if the training data of size m belong to a sphere of size r and are separable with margin m ( both in the feature space ) .
here , the expectation is taken over sets of training data of size m .
the values of s can thus be found by minimizing such a bound by using gradient descent .
this method is related to the automatic relevance determination ( ard ) feature - selection methods by mackay and neal
we compared our arom approach to a standard svm with no feature selection , and to svms using the feature selection methods described above .
finally , we also compared to the previous approach of minimizing the zero - norm called fsv by bradley et al .
( 123 ) , which is described in section 123 ( problem 123 ) .
in order to make our method ( and fsv ) choose exactly r features we stop at the last iteration before the constraint jjwjj123 ( cid : 123 ) r is satised and choose the r largest elements in all methods we then train a linear svm on the r chosen features .
we note that one could try to improve on all these results by optimizing over the so - called soft margin parameter c , which we left xed to c = .
one can nd the datasets used in these experiments at http :
use of the zero - norm with linear models and kernel methods
linear problem we start with an articial problem , where six dimensions out of 123 were rele - vant .
the probability of y = 123 or 123 was equal .
the rst three features fx123 , x123 , x123g were drawn123 as xi = yn ( i , 123 ) , and the second three features fx123 , x123 , x123g were drawn as xi = n ( 123 , 123 ) with a probability of 123 , otherwise the rst three were drawn as xi = n ( 123 , 123 ) and the second three as xi = yn ( i 123 , 123 ) .
the remaining features are noise xi = n ( 123 , 123 ) , i = 123 , .
the inputs are then scaled to have mean zero and standard deviation one .
in this problem the rst six features have redundancy and the rest of the features are irrelevant .
we used linear decision rules and for feature selection we selected the 123 best features .
we trained on 123 , 123 and 123 randomly drawn training points , testing on a further 123 points , and averaging test error over 123 trials .
the results are given in table 123
for each technique the test error and standard error is given .
for 123 or more training points , arom svms outperform rfe , r123w123 and corr svms whereas conventional svms overt .
note that for very small training set sizes ( e . g .
size 123 ) the number of times that two relevant and non - redundant features are chosen ( given in brackets in the gure ) is not completely correlated with the test error .
this can happen when it is easier for an algorithm to choose two relevant and redundant features ( as in corr ) than to try to nd the
( cid : 123 ) 123 - arom svm 123% ( cid : 123 ) 123% ( 123 ) ( cid : 123 ) 123 - arom svm 123% ( cid : 123 ) 123% ( 123 )
table 123 : feature selection on a linear problem .
arom svms ( using both the ( cid : 123 ) 123 and ( cid : 123 ) 123 multi - plicative updates ) outperform rfe , r123w123 and corr svms , other techniques of feature selection in the case where conventional svms overt .
for each technique the percentage test error and standard error is given .
in brackets is the number of times that two relevant and non - redundant features are chosen .
see the text for more details .
we measured the signicance of these results using the wilcoxon signed rank test with a signif - icance level a = 123 .
the results show that the ( cid : 123 ) 123 - arom svm , ( cid : 123 ) 123 - arom svm and fsv svm are signicantly better than all the other methods for training set sizes 123 with a p - value less than 123 but are not signicantly different from each other .
for training set size 123 , r123w123 svm is also not signicantly different to these best performing algorithms ( but signicantly outperforms we also compared these methods with some naive wrapper feature selection methods using the ( cid : 123 ) 123 - or ( cid : 123 ) 123 - norm : choose the two largest values of jwij as the features of choice .
this is in effect using only the rst iteration of the arom ( or rfe ) algorithm , and as such represents a sanity check that the iterative procedure does improve the quality of the chosen features .
in fact these naive
we denote n ( , s ) to be a normal distribution with mean and standard deviation s
weston , elisseeff , sch olkopf and tipping
methods perform similarly to the corr svm : the ( cid : 123 ) 123 - norm yields for 123 , 123 and 123 training points : 123% ( cid : 123 ) 123 ( 123 ) , 123% ( cid : 123 ) 123 ( 123 ) and 123% ( cid : 123 ) 123 ( 123 ) .
the ( cid : 123 ) 123 - norm yields : 123% ( cid : 123 ) 123 ( 123 ) , 123% ( cid : 123 ) 123 ( 123 ) and 123% ( cid : 123 ) 123 ( 123 ) .
two - class microarray datasets we then performed experiments on real - life problems .
for dna microarray data analysis one needs to determine the relevant genes in discrimination as well as discriminate accurately .
we look at a problem of distinguishing between cancerous and normal tissue in a colon cancer problem examined by alon et al .
( 123 ) ( see also guyon et al .
123 for a treatment of this problem ) and in a large b - cell lymphoma problem described by alizadeh ( 123 ) .
in the colon cancer problem , 123 tissue samples probed by oligonucleotide arrays contain 123 normal and 123 colon cancer tissues that must be discriminated based upon the expression of 123 genes .
splitting the data into a training set of 123 and a test set of 123 in 123 separate trials we obtained a test error of 123% for standard linear svms .
we then measured the test error of svms trained with features chosen by ve input selection methods : corr , rfe , r123w123 , fsv and our approach , ( cid : 123 ) 123arom .
we chose subsets of 123 , 123 , 123 , 123 , 123 , 123 and 123 genes .
the results are shown in table 123
123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123%
table 123 : input selection on micro - array data colon cancer vs normal .
the average percentage test error over 123 splits is given for various numbers of genes selected using ve approaches .
corr svm
fsv svm ( cid : 123 ) 123 - arom svm 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123% 123% ( cid : 123 ) 123%
table 123 : input selection on the lymphoma micro - array data .
the average test error over 123 splits is
given for various numbers of genes selected using ve approaches .
use of the zero - norm with linear models and kernel methods
in this experiment we can no longer use the wilcoxon signed rank test to assess signicance because the trials are dependent .
we therefore used the corrected resampled t - test statistic of nadeau and bengio ( 123 ) which is a test developed to attempt to take this dependence into account .
in fact , taking a condence level of a = 123 the test showed that none of the differences between algorithms were signicant apart from between corr svm and the other algorithms for feature size 123
note that the wilcoxon test , on the other hand , returns that many of the differences are signicant .
this serves to highlight the difculty of assessing signicance with such small dataset
in the lymphoma problem the gene expression of 123 samples is measured with microarrays to give 123 features , 123 of the samples are in classes dlcl , fl or cll ( malignant ) and 123 are labelled otherwise ( usually normal ) .
we followed the same approach as in the colon cancer problem , splitting the data this time into training sets of size 123 and test sets of size 123 over 123 separate trials .
the results are given in table 123
similarly to the colon cancer experiment , the corrected resampled t - test statistic returned that none of the differences between algorithms were
to give an idea of the relative training times of the methods we computed the cpu time of one training run on the lymphoma dataset .
we obtained the following times : fsv : 123 seconds , ( cid : 123 ) 123 - arom svm : 123 seconds , rfe ( choosing 123 features ) : 123 seconds , svm : 123 secs , corr : 123 secs .
clearly computational efciency is not accurately measured by computer time if the algorithms are not optimized .
yet we think this provides an idea of the computational efciency , e . g .
the difference between fsv and ( cid : 123 ) 123 - arom svm ( which both try to minimize the zero - norm ) is due to the inability of fsv to take advantage of dual optimization and so scales with respect to the number of features , rather the number of training patterns .
features arom m - svm arom 123vsr - svm rfe 123vsr - svm
table 123 : result of the feature selection preprocessing on the brown - yeast dataset : 123 classes , 123 training points of dimension 123
the percentages represent the fraction of errors using 123 fold cross validation , as well as their standard errors .
the number of features used by the methods is also given .
m - svm means multiclass svm and 123vsr stands for one - against - the - rest .
the arom is performed with the ( cid : 123 ) 123 approximation .
multi - class microarray dataset we used another microarray dataset ( brown yeast dataset ) of 123 genes that has to be discriminated into ve classes based on 123 gene expression values corre - sponding to different experimental conditions .
we performed 123 - fold cross validation for the differ - ent methods .
the rst algorithm we tested is a classical multi - class svm described by weston and watkins ( 123 ) without any feature selection method , the second is the same but with a preprocess - ing step using our multi - class ( cid : 123 ) 123 - arom procedure to select features .
the constraints are chosen to be the same as for the multi - class implementation of weston and watkins ( 123 ) .
we also applied a one - against - the - rest svm with and without feature selection , the latter being performed via the
weston , elisseeff , sch olkopf and tipping
( cid : 123 ) 123 - arom procedure .
finally , we also ran a one - against - the - rest svm combined with rfe .
table 123 presents the result .
note that the performance of the learning system combined with arom for feature selection improves generalization performance .
being able to reduce the number of features while having lower or the same generalization error allows the user to focus on a limited amount of information and to check whether this information is relevant or not .
it is worth noticing also that the performance of the one - against - the - rest approach is improved when selecting only 123 features with the rfe method .
the number of features is however larger than with the arom procedure .
thus both in terms of identifying a small number of useful features and improving generalization ability , arom seems to be preferable to rfe .
however we expect the number of trials performed here to be too small to suggest the signicance of these results .
we have shown how the zero - norm minimization can be used for feature selection , and have com - pared it to some existing feature selection methods .
it performs about as well as the best alternative method compared on some specic microarray problems .
when comparing feature selection algorithms , the generalizaton performance is not the only
reason for choosing a particular method .
some key differences between the methods include :
( cid : 123 ) computational efciency .
of the algorithms tested correlation scores are the fastest to com - pute , then methods that use dual optimization ( which is possible with the two - norm ) such as svm , rfe and ( cid : 123 ) 123 - arom , whereas the slowest are the methods which minimize the one - norm such as fsv and ( cid : 123 ) 123 - arom svm .
( cid : 123 ) applicability to nonlinear problems .
of the methods tested , only r123w123 and rfe are appli -
cable for choosing features in input space relevant for nonlinear problems .
( cid : 123 ) capacity .
by searching the space of subsets of features wrapper approaches ( e . g .
rfe ) and the zero - norm minimization can more effectively minimize the training error than lter methods such as corr .
hence in some sense they have a higher capacity .
the applicability of these algorithms thus depends upon the complexity of the problem at hand : lter methods can undert complex data ( e . g .
corr can fail if the data are multivariate or nonlinear ) whereas the other methods can overt the data .
clearly , choosing features via a criterion such as cross validation error or a generalization bound can bias the estimate of the criterion through its minimization in just then same way as training error is a biased measure of generalization ability after minimizing it .
as a nal remark this means that even though methods such as rfe and even the arom methods end up being forms of backward selection which do not search through the whole space of possible subsets this is not necessarily a bad thing in that their capacity is thus not as high as a more complete search of the space ( which as well as overtting , would be computational less tractable anyway ) .
several other issues are noteworthy : ( cid : 123 ) model selection : number of features .
we have not addressed the issue of model selection ( in terms of selecting the number of features ) in this work , however we believe this is an impor - tant problem .
of course this hyperparameter can be chosen like any other hyperparameter , e . g .
trying different values and estimating generalization error .
however , making this com - putationally efcient ( given that the feature selection algorithm itself with xed value of the
use of the zero - norm with linear models and kernel methods
parameter can be already expensive to compute ) could be difcult without somehow solving both problems at once .
furthermore , as many approaches , e . g .
wrapper approaches have al - ready used an estimate of generalization error to select features it makes using the same or related measures more biased and thus less effective for this task .
( cid : 123 ) model selection : other parameters .
for svms it would also be nice to be able to select the other parameters ( kernel parameters , soft margin parameter c ) at the same times as the number of features .
of the techniques described only r123w123 provides an obvious mechanism for doing this .
( cid : 123 ) the goal of feature selection .
in this work we have concentrated on the goal of choosing features such that generalization error is minimized .
however , this is not always of interest : for example the goal may be to know which features would be ( the most ) relevant features if the optimal decision function were known .
in microarray data , which we have focussed on , the true goal is often more application specic than what we have addressed .
one may be interested in nding genes which are potential drug targets .
filter methods such as correlation scores are thus often preferred because they return a ranked list of ( possibly redundant ) genes ( rather than subsets ) which are highly correlated with the labels .
the redundancy in this case can be benecial due to application specic constraints ( e . g .
usefulness of this gene as a drug target ) .
however , in the future choosing subsets of genes may become more important , especially as the amount of available data increases , making such ( more difcult ) problems
nonlinear feature selection via kernels
in the nonlinear feature selection problem one would like to select a subset of features in the space induced by a kernel , usually with the aim of improving the discriminative ability of a classier ( see section 123 for the details of how to apply the zero - norm to this problem ) .
an example of the use of such a method would be an application where the data require one to use a nonlinear decision rule ( e . g .
a polynomial ) but the best decision rule is sparse in the param - eters of the nonlinear rule ( e . g .
a sparse polynomial ) .
for example , one possible application could thus be image recognition , where not all polynomial terms are expected to have nonzero weight ( e . g .
terms involving pixels far away from each other ) .
however , in this case it might be better to explicitly implement this prior knowledge into the construction of the kernel .
in this section we only demonstrate the effectiveness of this method on toy examples .
123 experimental results
we compared the zero - norm minimization to the solution of svms on some general nonlinear toy problems .
note that we do not compare to conventional feature selection methods because if w is too large in dimension these methods can no longer be easily used .
we chose input spaces of dimension n = 123 and a mapping into feature space of polynomial of degree 123 : f ( x ) = f 123 : 123 ( x ) .
the following noiseless problems ( target functions ) were chosen : ( a ) f ( x ) = x123x123 + x123 , ( b ) f ( x ) = x123; and randomly chosen polynomial functions with ( c ) 123% , ( d ) 123% , ( e ) 123% and ( f ) 123% sparsity of the target .
that is , d% of the coefcients of the polynomial to be learned are zero .
the problems were attempted for training set sizes of 123 , 123 , 123 and 123 over 123 trials , and test error measured on a further 123 testing points .
weston , elisseeff , sch olkopf and tipping
f ( x ) = x123 x123+x123
f ( x ) = x123
123% sparse poly degree 123
123% sparse poly degree 123
123% sparse poly degree 123
123% sparse poly degree 123
figure 123 : a comparison of svms ( dashed lines ) and the ( cid : 123 ) 123 - arom svm ( solid lines ) for learning sparse and non - sparse target functions with polynomials of degree 123 over 123 inputs .
the ( cid : 123 ) 123 - arom svm outperforms the svm solution if the target is sparse , and gives compara - ble performance otherwise .
when the target is a sparse linear target the ( cid : 123 ) 123 - arom svm even outperforms a linear svm ( dotted lines ) .
see the text for details .
we implemented the multiplicative updates by calculating the nonlinear map and using the method of section 123 .
note that comparing to the explicit calculation of w ( which is not always
use of the zero - norm with linear models and kernel methods
possible if w is large ) we found the performance was identical .
we do not believe this will always be the case : if the required solution does not exist in the span of the training vectors then equation ( 123 ) in appendix c of weston et al .
( 123a ) could be a poor approximation .
results are shown in
in problems ( a ) - ( d ) the ( cid : 123 ) 123 - arom svm method clearly outperforms svms .
this is due to the norm that svms use , the ( cid : 123 ) 123 - norm which places a preference on using as many coefcients as possible in its decision rule .
this is costly when the number of features one should use is small .
in problem ( b ) where the decision rule to be learned is linear ( just one feature ) the difference is the largest .
on this problem the dotted lines in the plot represent the performance of a linear svm ( as the target is linear ) .
the linear svm outperforms the polynomial svm , as one would expect , but surprisingly the arom feature selection method in the space of polynomial coefcients of degree two still outperforms the linear svm .
this is again because the linear svm , using the ( cid : 123 ) 123 - norm does not perform feature selection .
finally , problems ( e ) and ( f ) show an 123% and 123% sparse target respectively .
note that although our method outperforms svms in the case of sparse targets it is not much worse ( in this example at least ) when the target is not sparse , as in problem ( f ) .
note that is not the case in the micro - array analysis of the previous section , the feature selection degrades performance when the number of features becomes too small even though the data are still linearly separable .
final discussion and conclusions
in conclusion we have introduced a simple optimization technique for minimizing the zero - norm and have studied several applications of the zero - norm in machine learning .
the main advantage of our algorithm over existing ones is its computational advantage when the number of variables exceeds the number of examples .
in particular , we are able to use existing speedup methods and heuristics that have been designed for svm and to use them to minimize the zero - norm .
it allows one to handle large data sets with relatively small computer resources as has been shown in the original papers ( e . g .
osuna et al . , 123a , b ) .
it is also simple to adapt our method to different domains , as shown in the applications .
we have shown how it can be useful in terms of feature selection ( when one is interested in discovering features ) and pattern recognition ( for obtaining good generalization performance ) .
in related work ( weston et al . , 123a ) , we also show how to use it for obtaining sparse representations and for compression ( vector quantization ) on both toy and real - life problems .
the usefulness of the minimization of the zero - norm in general depends on the type of problem .
in vector quantization and feature selection problems the methods usefulness is clear , since sparsity is an explicit goal in these applications .
in pattern recognition it depends on the ( unknown ) target .
for example : are your data such that a rule using just a small number of features yields good performance ? are there many irrelevant features ( noise ) ? in these cases the zero - norm can be very useful .
constructing toy examples of this type shows one can obtain large performance gains selecting the appropriate norm , for example in figure 123
however , the nature of the ( cid : 123 ) 123 - norm as a regularizer is not completely clear , in our experiments we trained a subsequent ( cid : 123 ) 123 - norm classier on the chosen features , giving improved performance .
part of the problem is that the ( cid : 123 ) 123 - norm does not have a unique solution .
trading off the number of variables with badness of t is perhaps not really enough to produce good classiers and a third ( regularization ) term is in fact necessary .
finally , our algorithm for minimizing the zero - norm can be used in many other contexts which we have not described , some of which are the subject of our next research activities .
these include
weston , elisseeff , sch olkopf and tipping
multi - label categorisation ( where each example may belong to one or many categories ) , in regres - sion , and in time series analysis .
we plan to further research these kinds of system which are of particular importance in a growing number of real - world applications , especially in the domain of
