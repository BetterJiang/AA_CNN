learning is posed as a problem of function estimation , for which two princi ( cid : 123 ) ples of solution are considered : empirical risk minimization and structural risk minimization .
these two principles are applied to two different state ( cid : 123 ) ments of the function estimation problem : global and local .
systematic improvements in prediction power are illustrated in application to zip - code
the structure of the theory of learning differs from that of most other theories for applied problems .
the search for a solution to an applied problem usually requires the three following steps :
state the problem in mathematical terms .
formulate a general principle to look for a solution to the problem .
develop an algorithm based on such general principle .
the first two steps of this procedure offer in general no major difficulties; the third step requires most efforts , in developing computational algorithms to solve the problem at hand .
in the case of learning theory , however , many algorithms have been developed , but we still lack a clear understanding of the mathematical statement needed to describe the learning procedure , and of the general principle on which the search for solutions
should be based .
this paper is devoted to these first two steps , the statement of the problem and the general principle of solution .
the paper is organized as follows .
first , the problem of function estimation is stated , and two principles of solution are discussed : the principle of empirical risk minimization and the principle of structural risk minimization .
a new statement is then given : that of local estimation of function , to which the same principles are applied .
an application to zip - code recognition is used to illustrate these ideas .
123 function estimation model
the learning process is described through three components :
a generator of random vectors x , drawn independently from a fixed but unknown 123
a supervisor which returns an output vector y to every input vector x , according to a conditional distribution function p ( ylx ) , also fixed but unknown .
a learning machine capable of implementing a set of functions ! ( x , w ) , we w .
the problem of learning is that of choosing from the given set of functions the one which approximates best the supervisor ' s response .
the selection is based on a training set of e independent observations :
the formulation given above implies that learning corresponds to the problem of
123 problem of risk minimization
in order to choose the best available approximation to the supervisor ' s response , we measure the loss or discrepancy l ( y , ! ( x , w between the response y of the supervisor to a given input x and the response ! ( x , w ) provided by the learning machine .
consider the expected value of the loss , given by the risk functional
r ( w ) = j l ( y , ! ( x , wdp ( x , y ) .
the goal is to minimize the risk functional r ( w ) over the class of functions ! ( x , w ) , w e w .
but the joint probability distribution p ( x , y ) = p ( ylx ) p ( x ) is unknown and the only available information is contained in the training set ( 123 ) .
123 empirical risk minimization
in order to solve this problem , the following induction principle is proposed : the risk functional r ( w ) is replaced by the empirical risk functional
e ( w ) = i ll ( yi , ! ( xi ' w
principles of risk minimization for learning theory
constructed on the basis of the training set ( 123 ) .
the induction principle of empirical risk minimization ( erm ) assumes that the function i ( x , wi ) , which minimizes e ( w ) over the set w e w , results in a risk r ( wi ) which is close to its minimum .
this induction principle is quite general; many classical methods such as least square or maximum likelihood are realizations of the erm principle .
the evaluation of the soundness of the erm principle requires answers to the fol ( cid : 123 ) lowing two questions : 123
is the principle consistent ? ( does r ( wi ) converge to its minimum value on the set we w when f - oo ? ) 123
how fast is the convergence as f increases ?
the answers to these two questions have been shown ( vapnik et al . , 123 ) to be equivalent to the answers to the following two questions : 123
does the empirical risk e ( w ) converge uniformly to the actual risk r ( w ) over the full set i ( x , w ) , we w ? uniform convergence is defined as
prob ( sup ir ( w ) - e ( w ) 123 > ) -
what is the rate of convergence ?
it is important to stress that uniform convergence ( 123 ) for the full set of functions is a necessary and sufficient condition for the consistency of the erm principle .
123 vc - dimension of the set of functions
the theory of uniform convergence of empirical risk to actual risk developed in the 123 ' s and so ' s , includes a description of necessary and sufficient conditions as well as bounds for the rate of convergence ( vapnik , 123s123 ) .
these bounds , which are independent of the distribution function p ( x , y ) , are based on a quantitative measure of the capacity of the set offunctions implemented by the learning machine : the vc - dimension of the set .
for simplicity , these bounds will be discussed here only for the case of binary pat ( cid : 123 ) tern recognition , for which y e ( o , 123 ) and i ( x , w ) , we w is the class of indicator functions .
the loss function takes only two values l ( y , i ( x , w ) ) = 123 if y = i ( x , w ) and l ( y , i ( x , w ) ) = 123 otherwise .
in this case , the risk functional ( 123 ) is the prob ( cid : 123 ) ability of error , denoted by pew ) .
the empirical risk functional ( 123 ) , denoted by v ( w ) , is the frequency of error in the training set .
the vc - dimension of a set of indicator functions is the maximum number h of vectors which can be shattered in all possible 123h ways using functions in the set .
for instance , h = n + 123 for linear decision rules in n - dimensional space , since they can shatter at most n + 123 points .
123 rates of uniform convergence
the notion of vc - dimension provides a bound to the rate of uniform convergence .
for a set of indicator functions with vc - dimension h , the following inequality holds :
prob ( sup ip ( w ) - v ( w ) 123 > c ) < ( - h ) exp ( - e fl
it then follows that with probability 123 - t ) , simultaneously for all w e w ,
pew ) < v ( w ) + co ( f / h , t ) ) ,
with confidence interval
ih ( 123n 123 / h + 123 ) - in t )
this important result provides a bound to the actual risk p ( w ) for all w e w , including the w which minimizes the empirical risk v ( w ) .
the deviation ip ( w ) - v ( w ) 123 in ( 123 ) is expected to be maximum for pew ) close to 123 / 123 , since it is this value of pew ) which maximizes the error variance u ( w ) = j p ( w ) ( 123 - p ( w ) ) .
the worst case bound for the confidence interval ( 123 ) is thus likely be controlled by the worst decision rule .
the bound ( 123 ) is achieved for the worst case pew ) = 123 / 123 , but not for small pew ) , which is the case of interest .
a uniformly good approximation to p ( w ) follows from considering
pew ) - v ( w )
the variance of the relative deviation ( p ( w ) - v ( w ) ) / ( j ( w ) is now independent of w .
a bound for the probability ( 123 ) , if available , would yield a uniformly good bound for actual risks for all p ( w ) .
such a bound has not yet been established .
but for pew ) 123 , the approximation ( j ( w ) ~ jp ( w ) is true , and the following inequality holds :
pew ) - v ( w )
> e ) < ( - ) exp ( - - ) .
it then follows that with probability 123 - t ) , simultaneously for all w e w ,
pew ) < v ( w ) + ci ( f / h , v ( w ) , t ) ) ,
with confidence interval
ci ( l / h , v ( w ) , t ) ) =123 ( h ( ln123f / h;l ) - lnt ) ) ( 123+ 123
note that the confidence interval now depends on v ( w ) , and that for v ( w ) = 123 it
+ h ( 123n 123f / h + 123 ) - in t )
ci ( f / h , 123 , t ) ) = 123c ' 123 ( f / h , t ) ) ,
which provides a more precise bound for real case learning .
123 structural risk minimization
the method of erm can be theoretically justified by considering the inequalities ( 123 ) or ( 123 ) .
when l / h is large , the confidence intervals co or c123 become small , and
principles of risk minimization for learning theory
can be neglected .
the actual risk is then bound by only the empirical risk , and the probability of error on the test set can be expected to be small when the frequency of error in the training set is small .
however , if ljh is small , the confidence interval cannot be neglected , and even v ( w ) = 123 does not guarantee a small probability of error .
in this case the minimiza ( cid : 123 ) tion of p ( w ) requires a new principle , based on the simultaneous minimization of v ( w ) and the confidence interval .
it is then necessary to control the vc - dimension of the learning machine .
to do this , we introduce a nested structure of subsets sp = ( lex , w ) , we wp ) , such
the corresponding vc - dimensions of the subsets satisfy
slcs123c . . .
csn .
hl < h123 < . . .
< hn .
the principle of structure risk minimization ( srm ) requires a two - step process : the empirical risk has to be minimized for each element of the structure .
the optimal element s* is then selected to minimize the guaranteed risk , defined as the sum of the empirical risk and the confidence interval .
this process involves a trade - off : as h increases the minimum empirical risk decreases , but the confidence interval
123 examples of structures for neural nets
the general principle of srm can be implemented in many different ways .
here we consider three different examples of structures built for the set of functions implemented by a neural network .
structure given by the architecture of the neural network .
consider an ensemble of fully connected neural networks in which the number of units in one of the hidden layers is monotonically increased .
the set of implement able functions makes a structure as the number of hidden units is increased .
structure given by the learning procedure .
consider the set of functions s = ( lex , w ) , w e w ) implementable by a neural net of fixed architecture .
the parameters ( w ) are the weights of the neural network .
a structure is introduced through sp = ( lex , w ) , ilwll < cp ) and cl < c123 < . . .
for a convex loss function , the minimization of the empirical risk within the element sp of the structure is achieved through the minimizat . ion of
e ( w " p ) = l ll ( yi , ! ( xi ' w + ' pllwi123 123
with appropriately chosen lagrange multipliers ii > 123 > . . .
> in ' the well - known " weight decay " procedure refers to the minimization of this functional .
structure given by preprocessing .
consider a neural net with fixed ar ( cid : 123 ) chitecture .
the input representation is modified by a transformation z = k ( x , 123 ) , where the parameter f123 controls the degree of the degeneracy introduced by this transformation ( for instance f123 could be the width of a smoothing kernel ) .
a structure is introduced in the set of functions s = ( ! ( ix , 123 ) , w ) , w e w ) through 123 > cp123 and cl > c123 > . .
123 problem of local function estimation
the problem of learning has been formulated as the problem of selecting from the class of functions ! ( x , w ) , w e w that which provides the best available approxi ( cid : 123 ) mation to the response of the supervisor .
such a statement of the learning problem implies that a unique function ! ( x , w ) will be used for prediction over the full input space x .
this is not necessarily a good strategy : the set ! ( x , w ) , w e w might not contain a good predictor for the full input space , but might contain functions capable of good prediction on specified regions of input space .
in order to formulate the learning problem as a problem of local function approxi ( cid : 123 ) mation , consider a kernel ix - xo , b ) ~ 123 which selects a region of input space of width b , centered at xo .
for example , consider the rectangular kernel ,
i< ( x _ x b ) = ( 123
if ix - ~ o i < b
and a more general general continuous kernel , such as the gaussian
the goal is to minimize the local risk functional
r ( w , b , xo ) =
l ( y , ! ( x , w k ( xo , b ) dp ( x , v )
k ( x - xo , b )
the normalization is defined by
k ( xo , b ) = j k ( x - xo , b ) dp ( x ) .
the local risk functional ( 123 ) is to be minimized over the class of functions ! ( x , w ) , w e wand over all possible neighborhoods b e ( 123 , 123 ) centered at xo .
as before , the joint probability distribution p ( x , y ) is unknown , and the only avail ( cid : 123 ) able information is contained in the training set ( 123 ) .
123 empirical risk minimization for local
in order to solve this problem , the following induction principle is proposed : for fixed b , the local risk functional ( 123 ) is replaced by the empirical risk functional
e ( w , b , xo ) = l l . . tl ( yj , ! ( xj , w
k ( xi - xo , b )
principles of risk minimization for learning theory
constructed on the basis of the training set .
the empirical risk functional ( 123 ) is to be minimized over w e w .
in the simplest case , the class of functions is that of constant functions , i ( x , w ) = c ( w ) .
consider the following examples : 123
k - nearest neighbors method : for the case of binary pattern recogni ( cid : 123 ) tion , the class of constant indicator functions contains only two functions : either
i ( x , w ) = for all x , or i ( x , w ) = 123 for all x .
the minimization of the empirical
risk functional ( 123 ) with the rectangular kernel kr ( x - xo , b ) leads to the k - nearest 123
watson - nadaraya method : for the case y e r , the class of constant func ( cid : 123 ) tions contains an infinite number of elements , i ( x , w ) = c ( w ) , c ( w ) e r .
the minimization of the empirical risk functional ( 123 ) for general kernel and a quadratic loss function l ( y , i ( x , w ) ) = ( y - i ( x , w ) ) 123 leads to the estimator
) - " " .
k ( xi - xo , b ) - ~ yi l .
i=123 l;=i / \ ( x; - xo , b )
which defines the watson - nadaraya algorithm .
these classical methods minimize ( 123 ) with a fixed b over the class of constant functions .
the supervisor ' s response in the vicinity of xo is thus approximated by a constant , and the characteristic size b of the neighborhood is kept fixed , independent a truly local algorithm would adjust the parameter b to the characteristics of the region in input space centered at xo .
further improvement is possible by allowing for a richer class of predictor functions i ( x , w ) within the selected neighborhood .
the srm principle for local estimation provides a tool for incorporating these two
123 structural risk minimization for local
the arguments that lead to the inequality ( 123 ) for the risk functional ( 123 ) can be extended to the local risk functional ( 123 ) , to obtain the following result : with probability 123 - t ) , and simultaneously for all w e wand all b e ( 123 , 123 )
r ( w , b , xo ) < e ( w , b , xo ) + c123 ( flh , b , t ) ) .
the confidence interval c123 ( flh , b , t ) ) reduces to co ( llh , t ) ) in the b - + 123 limit .
as before , a nested structure is introduced in the class of functions , and the empirical risk ( 123 ) is minimized with respect to both w e wand be ( 123 , 123 ) for each element of the structure .
the optimal element is then selected to minimize the guaranteed risk , defined as the sum of the empirical risk and the confidence interval .
for fixed b this process involves an already discussed trade - off : as h increases , the empirical risk decreases but the confidence interval increases .
a new trade - off appears by varying b at fixed h : as b increases the empirical risk increases , but the confidence interval decreases .
the use of b as an additional free parameter allows us to find deeper minima of the guaranteed risk .
123 application to zip - code recognition
we now discuss results for the recognition of the hand written and printed digits in the us postal database , containing 123 training examples and 123 testing exam ( cid : 123 ) ples .
human recognition of this task results in an approximately 123% prediction error ( sackinger et al . , 123 ) .
the learning machine considered here is a five - layer neural network with shared weights and limited receptive fields .
when trained with a back - propagation algo ( cid : 123 ) rithm for the minimization of the empirical risk , the network achieves 123% predic ( cid : 123 ) tion error ( le cun et al . , 123 ) .
further performance improvement with the same network architecture has required the introduction a new induction principle .
methods based on srm have achieved prediction errors of 123% ( training based on a double - back - propagation algorithm which incorporates a special form of weight decay ( drucker , 123 and 123% ( using a smoothing transformation in input space ( simard , 123
the best result achieved so far , of 123% prediction error , is based on the use of the srm for local estimation of the predictor function ( bottou , 123 ) .
it is obvious from these results that dramatic gains cannot be achieved through minor algorithmic modifications , but require the introduction of new principles .
i thank the members of the neural networks research group at bell labs , holmdel , for supportive and useful discussions .
sara solla , leon bottou , and larry jackel provided invaluable help to render my presentation more clear and accessible to the neural networks community .
