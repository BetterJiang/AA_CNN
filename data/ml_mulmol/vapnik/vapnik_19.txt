abstract traditional classication approaches generalize poorly on image classication tasks , because of dimensionality of the feature space .
this paper shows that support vector machines ( svms ) can generalize well on difcult image classication problems where the only features are high dimensional histograms .
heavy - tailed rbf kernels of jx y j with a 123 and b 123 the form k ( x; y ) = e are evaluated on the classication of images extracted from the corel stock photo collection and shown to far outperform traditional polynomial or gaussian radial basis function ( rbf ) kernels .
moreover , we observed that a simple remapping of the improves the performance of linear svms to input xi ! xa such an extend that it makes them , for this problem , a valid alternative to rbf kernels .
index terms corel ,
radial basis functions , support vector machines .
large collections of images are becoming available to
the public , from photo collections to web pages or even video databases .
to index or retrieve them is a challenge which is the focus of many research projects ( for instance ibms qbic ( 123 ) ) .
a large part of this research work is devoted to nding suitable representations for the images , and retrieval generally involves comparisons of images .
in this paper , we choose to use color histograms as an image representation because of the reasonable performance that can be obtained in spite of their extreme simplicity ( 123 ) .
using this histogram representation , our initial goal is to perform generic object classication with a winner takes all approach : nd the one category of object that is the most likely to be present in a
from classication trees to neural networks , there are many possible choices for what classier to use .
the support vector machine ( svm ) approach is considered a good candidate because of its high generalization performance without the need to add a priori knowledge , even when the dimension of the input space is very high .
intuitively , given a set of points which belongs to either one of two classes , a linear svm nds the hyperplane leaving the largest possible fraction of points of the same class on the same side , while maximizing the distance of either class from the hyperplane .
according to ( 123 ) , this hyperplane minimizes the risk of misclassifying examples of the test set .
manuscript received january 123 , 123; revised april 123 , 123
the authors are with the speech and image processing services research
laboratory , at&t labs - research , red bank , nj 123 usa .
publisher item identier s 123 - 123 ( 123 ) 123 - 123
this paper follows an experimental approach , and its or - ganization unfolds as increasingly better results are obtained through modications of the svm architecture .
section ii provides a brief introduction to svms .
section iii describes the image recognition problem on corel photo images .
section iv compares svm and knn - based recognition techniques which are inspired by previous work .
from these results , section v explores novel techniques , by either selecting the svm kernel , or remapping the input , that provide high image recognition performance with low computational requirements .
support vector machines
optimal separating hyperplanes
we give in this section a very brief introduction to svms .
be a set of training examples , each example being the dimension of the input space , belongs .
the aim is to dene a to a class labeled by hyperplane which divides the set of examples such that all the points with the same label are on the same side of the hyperplane .
this amounts to nding
if there exists a hyperplane satisfying ( 123 ) , the set is said to be linearly separable .
in this case , it is always possible to
i . e . , so that the distance between the closest point to the
then , ( 123 ) becomes
among the separating hyperplanes , the one for which the distance to the closest point is maximal is called optimal separating hyperplane ( osh ) .
since the distance to the closest under constraints ( 123 ) .
, nding the osh amounts to minimizing
is called the margin , and thus the osh is the separating hyperplane which maximizes the margin .
the margin can be seen as a measure of the generalization ability : the larger the margin , the better the generalization is expected to be ( 123 ) , ( 123 ) .
is convex , minimizing it under linear constraints ( 123 ) can be achieved with lagrange multipliers .
if we denote
ieee transactions on neural networks , vol .
123 , no .
123 , september 123
non negative lagrange multipli - ers associated with constraints ( 123 ) , our optimization problem amounts to maximizing
if we replace
by its mapping in the feature space
and under constraint
this can be achieved by the use of standard quadratic programming
once the vector
mization problem ( 123 ) has been found , the osh the following expansion :
solution of the maxi -
the support vectors are the points for which
( 123 ) with equality .
considering the expansion ( 123 ) of sion function can thus be written as
, the hyperplane deci -
linearly nonseparable case
when the data is not linearly separable , we introduce slack
( 123 ) such that
to allow the possibility of examples that violate ( 123 ) .
the is to allow misclassied points , purpose of the variables which have their corresponding upper bound on the number of training errors .
the generalized osh is then regarded as the solution of the following problem :
the rst
to constraints ( 123 ) and
minimized to control the learning capacity as in the separable case; the purpose of the second term is to control the number of is chosen by the user , a misclassied points .
the parameter corresponding to assigning a higher penalty to errors .
in ( 123 ) , the penalty term for misclassications .
when dealing with images , most of the time , the dimension of the input space is large ( compared to the size of the training set , so that the training data is generally linearly separable .
consequently , the value
has in this case little impact on performance .
svm training requires to x
if we have
needed in the training algorithm and the mapping explicitly used .
conversely , given a symmetric positive kernel , mercers theorem ( 123 ) indicates us that there exists a
, then only
once a kernel
satisfying mercers condition has been
chosen , the training algorithm consists of minimizing
and the decision function becomes
multiclass learning
svms are designed for binary classication .
when dealing with several classes , as in object recognition and image classication , one needs an appropriate multiclass method .
different possibilities include the following .
modify the design of the svm , as in ( 123 ) ,
to incorporate the multiclass learning directly in the quadratic solving algorithm .
combine several binary classiers : one against one ( 123 ) applies pairwise comparisons between classes , while one against the others ( 123 ) compares a given class with all the others put together .
according to a comparison study ( 123 ) , the accuracies of these methods are almost the same .
as a consequence , we chose the one with the lowest complexity , which is one against the
in the one against the others algorithm ,
is the number of classes .
each hyperplane separates one class from the other classes .
in this way , we get of the form ( 123 ) .
the class of , i . e . , the class with
is given by
a new point the largest decision function .
we made the assumption that every point has a single label .
nevertheless , in image classication , an image may belong to several classes as its content is not unique .
it would be possible to make multiclass learning more robust , and extend it to handle multilabel classication problems by using error correcting codes ( 123 ) .
this more complex approach has not been experimented in this paper .
nonlinear support vector machines
the input data is mapped into a high - dimensional feature space through some nonlinear mapping chosen a priori ( 123 ) .
in this feature space , the osh is constructed .
the data and its representation
among the many possible features that can be extracted from an image , we restrict ourselves to ones which are global and low - level ( the segmentation of the image into regions , objects or relations is not in the scope of the present paper ) .
chapelle et al . : svms for histogram - based image classification
for the height and
the simplest way to represent an image is to consider its bitmap representation .
assuming the sizes of the images in the database are xed to for the width ) , then the input data for the svm are vectors images .
each component of the vector is associated to a pixel in the image .
some major drawbacks of this representation are its large size and its lack of invariance with respect to translations .
for these reasons , our rst choice was the histogram representation which is described presently .
for grey - level images and 123
color histograms
in spite of the fact that the color histogram technique is a very simple and low - level method , it has shown good results in practice ( 123 ) especially for image indexing and retrieval tasks , where feature extraction has to be as simple and as fast as possible .
spatial features are lost , meaning that spatial relations between parts of an image cannot be used .
this also ensures full translation and rotation invariance .
a color is represented by a three dimensional vector corre - sponding to a position in a color space .
this leaves us to select the color space and the quantization steps in this color space .
as a color space , we chose the hue - saturation - value ( hsv ) space , which is in bijection with the redgreenblue ( rgb ) space .
the reason for the choice of hsv is that it is widely used in the literature .
hsv is attractive in theory .
it is considered more suitable since it separates the color components ( hs ) from the lu - minance component ( v ) and is less sensitive to illumination changes .
note also that distances in the hsv space correspond to perceptual differences in color in a more consistent way than in the rgb space .
however , this does not seem to matter in practice .
all the experiments reported in the paper use the hsv space .
for the sake of comparison , we have selected a few experiments and used the rgb space instead of the hsv space , while keeping the other conditions identical : the impact of the choice of the color space on performance was found to be minimal compared to the impacts of the other experimental conditions ( choice of the kernel , remapping of the input ) .
an explanation for this fact is that , after quantization into bins , no information about the color space is used by the classier .
the number of bins per color component has been xed to 123 , and the dimension of each histogram is some experiments with a smaller number of bins have been undertaken , but the best results have been reached with 123 bins .
we have not tried to increase this number , because it is computationally too intensive .
it is preferable to compute the histogram from the highest spatial resolution available .
subsampling the image too much results in signicant losses in performance .
this may be explained by the fact that by subsampling , the histogram loses its sharp peaks , as pixel colors turn into averages ( aliasing ) .
selecting classes of images in the corel stock photo collection
the corel stock photo collection consists of a set of photographs divided into about 123 categories , each one with
123 images .
for our experiments , the original 123 categories have been reduced using two different labeling approaches .
in the rst one , named corel123 , we chose to keep the cat - egories dened by corel .
for the sake of comparison , we chose the same subset of categories as ( 123 ) , which are : air shows , bears , elephants , tigers , arabian horses , polar bears , african specialty animals , cheetahs - leopards - jaguars , bald eagles , mountains , elds , deserts , sunrises - sunsets , night scenes .
it is important to note that we had no inuence on the choices made in corel123 : the classes were selected by ( 123 ) and the examples illustrating a class are the 123 images we found in a corel category .
in ( 123 ) , some images which were visually deemed inconsistent with the rest of their category were removed .
in the results reported in this paper , we use all 123 images in each category and kept many obvious outliers : see for instance , in fig .
123 , the polar bear alert sign which is considered to be an image of a polar bear .
with 123 categories , this results in a database of 123 images .
note that some corel categories come from the same batch of photographs : a system trained to classify them may only have to classify color and
in an attempt to avoid these potential problems and to move toward a more generic classication , we also dened a second labeling approach , corel123 , in which we designed our own seven categories : airplanes , birds , boats , buildings , sh , people , vehicles .
the number of images in each category varies from 123 to 123 for a total of 123 samples .
for each category images were hand - picked from several original corel categories .
for example , the airplanes category includes images of air shows , aviation photography , ghter jets and ww - ii planes .
the representation of what is an airplane is then more general .
table i shows the origin of the images for each category .
selecting the kernel
the design of the svm classier architecture is very simple and mainly requires the choice of the kernel ( the only other ) .
nevertheless , it has to be chosen carefully since an inappropriate kernel can lead to poor performance .
there are currently no techniques available to learn the form of the kernel; as a consequence , the rst kernels investigated were borrowed from the pattern recognition literature .
the kernel products between input vectors
results in a classier which has a polynomial decision gives a gaussian radial basis function ( rbf ) classier .
in the gaussian rbf case , the number of centers ( number of support vectors ) , the centers themselves ( the support vectors ) , the weights are all produced automatically by the svm training and give excellent results compared to rbfs trained with non - svms
and the threshold
ieee transactions on neural networks , vol .
123 , no .
123 , september 123
corel123 : each row includes images from the following seven categories : air shows , bears , arabian horses , night scenes , elephants , bald eagles ,
encouraged by the positive results obtained with
we looked at generalized forms of rbf kernels
can be chosen to be any distance in the input norm seems to space .
in the case of images as input , the be quite meaningful .
but as histograms are discrete densities , more suitable comparison functions exist , especially the function , which has been used extensively for histogram comparisons ( 123 ) .
we use here a symmetrized approximation
it is not known if the kernel satises mercers condition . 123
another obvious alternative is the
distance , which gives
a laplacian rbf
123 it is still possible apply the svm training procedure to kernels that do not satisfy mercers condition .
what is no longer guaranteed is that the optimal hyperplane maximizes some margin in a hidden space .
chapelle et al . : svms for histogram - based image classification
corel123 : each row includes images from the following seven categories : tigers , african specialty animals , mountains , elds , deserts , sun - rises - sunsets , polar bears .
the rst series of experiments are designed to roughly assess the performance of the aforementioned input represen - tations and svm kernels on our two corel tasks .
the 123 examples of corel123 were divided into 123 training examples and 123 test examples .
the 123 examples of corel123 were split evenly between 123 training and test examples .
the svm error penalty parameter was set to 123 , which can be considered in most cases as large .
however , in this series of experiments , this parameter setting was found to enforce full separability for all types of kernels except the linear one .
values were selected in the cases of the rbf kernels , the
heuristically .
more rigorous procedures will be described in the second series of experiments .
table ii shows very similar results for both the rbg and hsv histogram representations , and also , with hsv histograms , similar behaviors between corel123 and corel123
the leap in performance does not happen , as normally expected by using rbf kernels but with the proper choice of metric within the rbf kernel .
laplacian or reduce the gaussian rbf error rate from around 123% down
this improved performance is not only due to the choice of the appropriate metric , but also to the good generalization of
ieee transactions on neural networks , vol .
123 , no .
123 , september 123
hand - labeled categories used with the corel database
error rates using the following kernels : linear , polynomial of
degree 123 , gaussian rbf , laplacian rbf and 123 rbf
original histogram , the kernel values are
error rates with knn
to demonstrate this , we conducted some experiments of image histogram classication with a k - nearest neighbors ( knn ) algorithm with the distances the best results .
table iii presents the results .
as expected , the - based svm is still roughly
distance is better suited; the
twice as good as the
we also did some experiments using the pixel image as input 123 images .
except in the linear to svm classiers with 123 case , the convergence of the support vector search process was problematic , often nding a hyperplane where every sample is a support vector .
the error rate never dropped below 123% .
the same database has been used by ( 123 ) with a decision tree classier and the error rate was about 123% , similar to the 123% error rate obtained with the traditional combination of an hsv histogram and a knn classier .
the 123% error rate rbf represents a nearly obtained with the laplacian or
one partial explanation for the superior performance of
or laplacian rbf kernels comes from the specic nature of the histogram representation .
let us start with an example : in many images , the largest coordinate in the histogram vector corresponds to the blue of the sky .
a small shift in the color of the sky , which does not affect the nature of the object to be recognized ( for instance plane or bird ) results into a large
- pixel bin in the histogram accounts for a single uniform color region in the image ( with histogram ) .
a small pixels to a change of color in this region can move the neighboring bin , resulting in a slightly different histogram .
if we assume that this neighboring bin was empty in the
the kernel has a linear exponential decay in the laplacian cases , while it has a quadratic exponential decay in
the gaussian case .
kernel design versus input remapping
the experiments performed in the previous section show that non - gaussian rbf kernels with exponential decay rates that are less than quadratic can lead to remarkable svm classication performances on image histograms .
this section explores two ways to reduce the decay rate of rbf kernels .
it shows that one of them amounts to a simple remapping of the input , in which case the use of the kernel trick is not always
non - gaussian rbf kernels
we introduce kernels of the form
the decay rate around zero is given by
decreasing the value of
in the case of gaussian rbf kernels ,
would provide for a slower decay .
a data - generating interpretation of rbfs is that they corre - spond to a mixture of local densities ( generally gaussian ) : in amounts to using heavy - tailed this case , lowering the value of distributions .
such distributions have been observed in speech recognition and improved performances have been obtained by moving from ( sublinear ) ( 123 ) .
note that if we assume that histograms are often distributed around zero ( only a few bins have nonzero values ) , decreasing the value of roughly the same impact as lowering . 123
123 an even more general type of kernel is k ( x; y ) = ed
da;b;c ( x; y ) =
decreasing the value of c does not improve performance as much as decreasing a and b , and signicantly increases the number of support vectors .
chapelle et al . : svms for histogram - based image classification
corel123 : each row includes images from the following categories : airplanes , birds , boats , buildings , sh , people , cars .
the choice of
has no impact on mercers condition as it
amounts to a change of input variables .
satises mercers condition if and only if
( ( 123 ) page 123 ) .
nonlinear remapping of the input
the exponentiation of each component of the input vector does not have to be interpreted in terms of kernel products .
one can see it as the simplest possible nonlinear remapping of the input that does not affect the dimension .
to believe that exponentiation may improve robustness with respect changes in scale .
imagine that the histogram component is caused by the presence of color col in some object
the following gives us
increase the size of the object by some scaling factor number of pixels is multiplied by by the same factor .
the quadratic scaling effect to a more reasonable
- exponentiation could lower this , which transforms all the components which are not zero to one ( we assume that
an interesting case is
experimental setup
to avoid a combinatorial explosion of kernel / remapping combinations , it is important to restrict the number of kernels we try .
we chose three types of rbf kernels : gaussian
basis for comparison , we also kept the linear svms .
ieee transactions on neural networks , vol .
123 , no .
123 , september 123
for the reasons stated in section iii . a , the only image
representation we consider here is the 123
our second series of experiments attempts to dene a rigor - and .
because we are only testing ous procedure to choose linear and rbf kernels , we can reduce these two choices to one , a multiplicative renormalization of the input data .
in the case of rbf kernels , we observed experimentally that full separability was always desirable on both corel123 and has to be chosen large enough corel123
as a consequence , compared to the diameter of the sphere containing is equal to , which is always smaller than 123 ) .
however , rbf kernels still do not specify .
with proper renormalization of what value to choose for the input data , we can set
the input data ( the distance between
in the linear case , the diameter of the data depends on the is equivalent to the for the input data .
way it is normalized .
the choice of choice of a multiplicative factor if , in ( 123 ) , we replace
similar experimental conditions are applied to both corel123 and corel123
each category is divided into three sets , each containing one third of the images , used as training , validation and test sets .
for each value of the input renormalization , support vectors are obtained from the training set and tested renormalization for which we on the validation set .
the obtain the best result is then used to obtain a set of support vectors from both , the training and the validation sets .
each usable pixels : the 123 corel image contains and sum up to histogram vector components range from 123 to
they were renormalized with
usually , the optimal values are 123 values increase the error rate by values ranging from 123% to 123% .
this very sparse sampling rate was found to be sufcient for all kernels except gaussian rbfs .
in the latter case , we
123 , 123 , 123 , 123 , 123 , 123 .
nonoptimal
the nal performance result is measured on the test set .
to obtain more test samples , we applied this procedure three times , each time with a different test set : the number of testing samples is the total number of data ( 123 for corel123 and 123 for corel123 ) .
on corel123 , each of the three training sessions used 123 examples and required between 123 and 123 support vectors to separate one class from the others .
on corel123 , each of the three training sessions used 123 examples and required between 123 and 123 support vectors to separate one class from the others .
the algorithms and the software used to train the svms were designed by osuna ( 123 ) , ( 123 ) .
average error rates on corel123
each column corresponds to a different kernel .
the first line reports the average number of support vectors required for the full recognizer ( i . e . , 123 one against the others svm classifiers ) .
the next lines report the
error rates using nonlinear input remappings ( exponentiation by a )
average error rates on corel123
perform an operation depends on the machine , we count the three main types of operations we nd in our svm classiers .
flt basic oating point operation such as the multiply - add or the computation of the absolute value of the difference between two vector components .
this is the central opera - tion of the kernel dot product .
this operation can be avoided if both components are zero , but we assume that verifying this condition usually takes more time than the operation can be reduced to a itself .
the computation of sqrt square root except in the sublinear rbf case , the number of flt is the dominating factor .
in the linear case , the decision function ( 123 ) allows the support vectors to be linearly combined : there is only one flt per class and component .
in the rbf case , there is one flt per class , component and support vector .
123 , the number that because of the normalization by 123 appears on the table equals the number of support vectors .
fluctuations of this number are mostly caused by changes in the input normalization
can be computed in advance .
in the sublinear rbf case , the number of sqrt is dom - inating .
sqrt is in theory required for each component of the kernel product : pessimistic upper bound since computations can be avoided for components with value zero .
this is the number we report
computation requirements
we also measured the computation required to classify one image histogram .
since the number of cycles required to
the analysis of the tables ivvi shows the following characteristics that apply consistently to both corel123 and
chapelle et al . : svms for histogram - based image classification
computational requirements for corel123 , reported as the number of operations for the recognition of one example , divided by 123 123
as anticipated , decreasing
as decreasing .
( compare column on both tables iv and v ) .
has roughly the same impact
for both , corel123 and corel123 , the best performance is
for histogram classication , gaussian rbf kernels are hardly better than linear svms and require around nsv ( number of support vectors ) times more computations at
sublinear rbf kernels are no better than laplacian rbf ) and are too computationally kernels ( provided that intensive : a time - consuming square root is required for nonzero components of every support vector .
for the practical use of rbf kernels , memory require - ments may also be an issue .
a full oating point rep - resentation of 123 support vectors , each with 123 components , requires 123 megabytes of memory .
to 123 makes linear svms a very attractive solution for many applications : its error rate is only 123% higher than the best rbf - based svm , while its compu - tational and memory requirements are several orders of magnitude smaller than for the most efcient rbf - based
yield surprisingly good results , and show that what is important about a histogram bin is not its value , but whether it contains any pixel at all .
note that in this case , gaussian , laplacian , and sublinear rbfs are exactly equivalent .
the input space has 123 dimensions : this is high enough to enforce full separability in the linear case .
however , when optimizing for with the validation set , a solution with training misclassications was preferred ( around 123% error on the case of corel123 and 123% error in the case of
table vii presents the class - confusion matrix corresponding
to the use of the laplacian kernel on corel123 with ( these values yield the best results for both corel123 and corel123 ) .
the most common confusions happen between birds and airplanes , which is consistent .
in this paper , we have shown that it is possible to push the classication performance obtained on image histograms to surprisingly high levels with error rates as low as 123% for the classication of 123 corel categories and 123% for a more generic set of objects .
this is achieved without any other knowledge about the task than the fact that the input is some sort of color histogram or discrete density .
class - confusion matrix for a = 123 : 123 and b = 123 : 123
for example , row ( 123 ) indicates that on the 123 images of the airplanes category , 123
have been correctly classified , 123 have been classified in birds , seven in boats , four in buildings , and 123 in vehicles
this extremely good performance is due to the superior generalization ability of svms in high - dimensional spaces to the use of heavy - tailed rbfs as kernels and to nonlin - ear transformations applied to the histogram bin values .
we distance used in a rbf studied how the choice of the kernel affects performance on histogram classication , and found laplacian rbf kernels to be superior to the standard gaussian rbf kernels .
as a nonlinear transformation of the ranging from 123 bin values , we used - exponentiation with down to 123
in the case of rbf kernels , the lowering of have similar effects , and their combined inuence yields the
the lowering of
improves the performance of linear svms to such an extent that it makes them a valid alternative to rbf kernels , giving comparable performance for a fraction of the computational and memory requirements .
this suggests a new strategy for the use of svms when the dimension of the input space is extremely high .
rather than introducing kernels intended at making this dimension even higher , which may not be useful , it is recommended to rst try nonlinear transformations of the input components in combination with linear svms .
the computations may be orders of magnitude faster and the performances comparable .
this work can be extended in several ways .
higher - level spatial features can be added to the histogram features .
al - lowing for the detection of multiple objects in a single image would make this classication - based technique usable for image retrieval : an image would be described by the list of objects it contains .
histograms are used to characterize other types of data than images , and can be used , for instance , for fraud detection applications .
it would be interesting to investigate if the same type of kernel brings the same gains
