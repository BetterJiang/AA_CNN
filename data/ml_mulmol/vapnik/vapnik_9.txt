abstract we study the use of support vector machines ( svms ) in classifying e - mail as spam or nonspam by comparing it to three other classication algorithms : ripper , rocchio , and boosting decision trees .
these four algorithms were tested on two different data sets : one data set where the number of features were constrained to the 123 best features and another data set where the dimensionality was over 123
svms performed best when using binary features .
for both data sets , boosting trees and svms had acceptable test performance in terms of accuracy and speed .
however , svms had signicantly less training time .
index terms boosting algorithms , classication , e - mail , fea -
ture representation , ripper , rocchio , support vector machines .
we define spam as an e - mail message that is un -
wantedbasically it is the electronic version of junk mail that is delivered by the postal service .
one of the reasons for the proliferation of spam is that bulk e - mail is very cheap to send and although it is possible to build lters that reject e - mail if it is from a known spammer , it is easy to obtain alternative sending addresses .
good online sources and information about spam include http : / / spam . abuse . net , http : / / www . cauce . org , and http : / / www . junke - mail . org .
there have been various attempts to use learning machines that classify e - mail ( 123 ) , ( 123 ) .
solutions to the proliferation of spam are either technical or regulatory ( 123 ) .
technical solutions include ltering based on sender address or header content .
the problem with ltering is that sometimes a valid message may be blocked .
thus , it is not our intent to automatically reject e - mail that is classied as spam .
rather , we envision the following scenario : in the training mode , users will mark their e - mail as either spam or nonspam .
after a nite number of examples are collected , the learning machine will be trained and the performance on new examples predicted .
the user can then invoke the e - mail classier immediately or wait until the number of examples is enough such that performance is acceptable .
after the training mode is complete , new e - mail will be classied as spam or nonspam .
in one presentation mode , a set of new e - mail messages is presented in a manner consistent with the time of delivery and the spam messages color - coded .
it is then up to the user to either read the e - mail or trash the e - mail .
an alternative presentation mode is to deliver e - mail to the user in decreasing order of probability that the e - mail is nonspam .
that is , e - mail with high probability ( according to
manuscript received january 123 , 123; revised april 123 , 123
drucker is with at&t labs - research , red bank , nj 123 usa , and is also with the department of electronic engineering , monmouth university , west long branch , nj 123 - 123 usa .
wu is with rensselaer polytechnic institute , troy , ny123 usa .
vapnik is with at&t labs - research , red bank , nj , 123 usa .
publisher item identier s 123 - 123 ( 123 ) 123 - 123
the classier ) of being nonspam is at the top of the list .
in either of these modes , the lter does not reject any messages , only indicates whether the message has a high priority of being
it is highly desirable that if the user decides that e - mail messages be rank - ordered by degree of condence that the rank ordering be reliable .
by reliable , we mean that the user can either start at the top of the list of e - mail messages and be fairly condent that they represent nonspam messages or start at the bottom of the list and be condent that the messages are spam .
it is only near the middle of the list ( low condence ) that it is reasonable to the user that a few nonspam or spam messages may be misclassied .
therefore , it is important that our learning algorithm not only classify the messages correctly but that a measure of condence is associated with that classication so that the message can be
in the next section a number of design choices are outlined .
in section iii we describe the data sets and experiments .
in section iv , we show how variation of the one parameter in svms changes performance .
the conclusions are presented in section v .
design choices
feature representation
a feature is a word .
in the development below ,
is a feature vector that is composed of the to a word , various words from a dictionary formed by analyzing the documents .
there is one feature vector per message .
to a weight vector usually obtained from some combination s .
there are various alternatives and enhancements in
vectors .
we consider some of them :
tfterm frequency : the th component of the feature appears in that vector is the number of times that word document .
in our case , a word is a feature only if it occurs in three or more documents .
( this prevents misspelled words and words used rarely from appearing in the dictionary ) .
sometimes the feature vector is normalized to unit length .
tf - idf uses the above tf multiplied by the idf ( inverse document frequency ) .
the document frequency ( df occurs in all the is the number of times that word documents ( excluding words that occur in less than three documents ) .
the inverse document frequency ( idf ) is
drucker et al . : svms for spam categorization
is the number of documents .
typically , the feature vector that consists of the tf - idf entries is normalized to unit length .
binary representation which indicates whether a particular word occurs in a particular document .
a word is a candidate only if it occurs in three or more documents .
use of a stop list in addition to any of the above : words like of , and , the , etc . , are used to form a stop list .
words on the stop list are not used in forming a feature vector .
the rationale for this is that common words are not very useful in classication .
the argument against using a stop list is that it is not obvious which words , beyond the trivial , should be on the stop list .
it may be obvious that articles like a , an , should be on the stop list .
however , should a word like now be on the stop list ? the choice of words to put on a stop list is probably a function of the classication task and it would be better if learning algorithm itself determined whether a particular word is important or not .
use of word stemming : words such as build , builder , and building are shortened to the word stem build .
this lowers the size of the feature vector but it may be the case that certain forms of a word ( such as the active tense ) may be important in
number of features
the choice is between using some of the features or all of the features .
in text recognition , a feature is a word .
one possible advantage of using a nite number of features is better generalization .
by generalization we mean that good perfor - mance on the training set generalizes to good performance on a separate test set .
depending on the learning algorithm , it may be the case that there is an optimum set of features , less than the total number of available features .
for example , if the dimensionality of the classication space is greater than the number of examples , then the examples may always be separable by a nonunique hyperplane with zero training error ( assuming the patterns are independent ) .
since there are , in general , an innite number of separating hyperplanes , one does not obtain the optimal separating hyperplane ( the one that has the best test performance ) .
a few of the mechanisms designed to nd the optimum number of features ( and the best features ) are ( 123 ) document frequency thresholding , information gain , mutual information , .
in comparing two learning algorithms , term strength , and yang and petersen found that , except for mutual information gain , all these feature selection methods had similar perfor - mance and similar characteristics .
that is , as the number of features used was decreased from all the features to some smaller number , the test performance improved .
then , once the number of features decreased below a critical value , the test error rate increased .
thorsten joachims ( 123 ) did similar experiments and com - pared ve learning algorithms including nave bayes , rocchio , - nearest neighbors , c123 ( 123 ) and svms .
he found that , except for svm and bayes , the optimum number of features was less than the total number of features .
for svm , using
all the features gave better performance than any of the other
the main disadvantage of searching for the best features is that it requires additional time in the training algorithm .
in the case of information gain and mutual information gain , the features are ranked by the feature selection method from high to low , which is linear in the number of examples and linear in the number of features ( giving quadratic complexity ) .
then , to nd the optimum number of features , one must apply the learning algorithms to different set sizes to nd the minimum error rate .
thus the complexity of the algorithm to nd the optimum number of features is at least quadratic times the complexity of the learning algorithm .
it would be far better if the learning machine itself either made the feature selection automatically or used all the features .
c123 has the former characteristic while svms have the latter .
performance criteria
recall and precision : in information retrieval tasks , docu - ments could be assigned multiple categories .
for instance , a story about the high wages of baseball players could be categorized as belonging to both the term nancial and the term sports .
when there are multiple categories , performance measures such as recall and precision ( 123 )
categories found and correct
total categories correct
categories found and correct
total categories found
since our problem is a two - class classication task , recall and precision are not needed here .
error rate : error rate is the typical performance measure for two - class classication schemes .
however , two learn - ing algorithms can have the same error rate , but the one which groups the errors near the decision border is the
false alarm and miss rate : we dene the false alarm
and miss rates as
false alarm rate
nonspam samples misclassied
total nonspam examples spam samples misclassied
total spam examples
the advantage of the false alarm and miss rates is that that they are a good indicator of whether the errors are close to the decision border or not .
given two classiers with the same error rate , the one with lower false alarm and miss rates is the better one .
we x the miss rate and try to nd the algorithm that minimizes the false alarm rate for that xed miss rate .
these rates will correspond to the error rates averaged using ten - fold cross validation .
in ten - fold cross validation , the entire set of examples is divided into ten approximately equal sets .
nine of the ten parts are using for training , one of the ten parts is used for testing .
this is repeated ten times , each time using another test set .
the total misses and false alarms are counted and divided by ten times the number of test examples
ieee transactions on neural networks , vol .
123 , no .
123 , september 123
is more accurate than averaging the ten false alarms and miss
recall that that even if an input message is mistakenly misclassied as spam when it is not , it is still presented to the user for the ultimate decision .
training and classication speed
the e - mail server may be a unit on which many users have an account .
therefore , the training time must be reasonable and the classication speed must be fast .
generally , this leaves out neural networks which take extensive time for training .
we therefore tried the following four algorithms : boosting ( using decision trees ) , ripper , rocchio , and linear svms .
the rst two are nonlinear learning algorithms but can be very fast in execution and the last two are linear .
choice of learning algorithms
123 ) boosting algorithms : the boosting algorithms ( fig .
123 ) are techniques to combine a number of weak learners to form an ensemble .
the term weak leaner arrives from the pac ( probably approximately correct ) ( 123 ) , ( 123 ) learning community and indicates that the learning algorithm can learn with error rate slightly better than 123% .
c123 classication trees are candidate weak learners even though their error rates can be much better than 123% .
this version of boosting works as following : train the rst member of the ensemble with training samples .
in order to train the next member of the ensemble , the probability that a training sample will be picked to train the second member of the ensemble is adjusted upwards for hard examples and down for easy examples .
by hard examples , we mean those examples that the rst weak
each member of the ensemble is subsequently trained on examples picked from the original training set with their probabilities adjusted upwards or downwards depending on whether the previous members of the ensemble classied the training pattern incorrectly or correctly , respectively .
for our weak learner , we use classication trees built using a version of c123 .
because classication trees can build dis - connected decision regions , they are nonlinear .
classication trees can be very fast in execution .
an advantage of c123 is that features are picked as part of the training algorithm and therefore there is no need to rank order the features by some other mechanism ( like mutual information ) rst .
c123 decision trees are built by examining a measure related to information gain and this can be time consuming because it has to be done multiple times ( equal to the number of nodes in a tree ) .
boosting has been shown to drive the error rate far below that of one weak learner .
both the boosting algorithms itself and the building of decision trees make training long unless there is a small number of trees in the ensemble .
however , there is no way to predict in advance how many trees should be in
in fig .
123 weaklearn refers in our case to a c123 type algorithm .
when the distribution is recalculated , a typical method of picking a training set for this member of the ensemble is as follows .
boosting algorithm .
construct line segments of length
with total length numbers at random from that total length .
if the number is from interval used as one of the training samples for that round of boosting .
it may be the case that multiple copies of a particular vector are used in training and no samples of easy training examples
, then example
it is also important to note that in calculating
, all the training examples are used in that calculation even if they were all not used in training .
123 ) support vector machines : svms are discussed exten - sively in this issue .
also see ( 123 ) ( 123 ) .
the key concepts there are two classes , labeled training examples : is the dimension -
to use are the following :
, and there are
ality of the vector .
if the two classes are linearly separable , then one can nd an is minimum; and
optimal weight vector
drucker et al . : svms for spam categorization
training examples that satisfy the equality are termed support vectors .
the support vectors dene two hyperplanes , one that goes through the support vectors of one class and one goes through the support vectors of the other class .
the distance between the two hyperplanes denes a margin ( 123 ) and this margin is maximized when the norm of the weight is minimum .
vapnik has shown we may perform this minimization by maximizing the following function with respect to the variables
subject to the constraint :
represents the dot product
support vector .
for an unknown vector corresponds to nding
where it is assumed there is one of the training vectors , and is termed a
and the sum is over the
nonzero support vectors ( whose
the advantage of the linear representation is that
be calculated after training and classication amounts to computing the dot product of this optimum weight vector with the input vector .
for the nonseparable case , training errors are allowed and
we now must minimize
subject to the constraint
is quadratic in
but the constraint is now
is a slack variable and allows training examples to exist in the region between the two hyperplanes that go through the support points of the two classes .
we can equivalently constraints and may be solved using quadratic programming techniques , some of which are particular to svms ( 123 ) , ( 123 ) .
the advantage of linear svms is that execution speed is very fast and there are no parameters to tune except the .
we will show that the performance of the svm is remarkably independent of the choice of is large ( over 123 ) .
another advantage of svms is that they are remarkably intolerant of the relative sizes of the number of training examples of the two classes .
in most learning algorithms , if there are many more examples of one class than another , the algorithm will tend to correctly classify the
as long as
class with the larger number of examples , thereby driving down the error rate .
since svms are not directly trying to minimize the error rate , but trying to separate the patterns in high dimensional space , the result is that svms are relatively insensitive to the relative numbers of each class .
for instance , new examples that are far behind the hyperplanes do not change the support vectors .
the possible disadvantages of svms are that the training time can be very large if there are large numbers of training examples and execution can be slow for nonlinear svms , neither of these cases being present here .
123 ) ripper : ripper is a program for inducing classication rules from a set of examples .
unlike the other algorithms , it does not need a feature vector .
it forms ifthen rules which are disjunctions of conjunctions , e . g . ,
is considered to be spam if and only if
( word free appears in ) or ( word low appears in and word cost appears in ) ( word ! ! ! ! appears in ) .
ripper ( 123 ) works by adding rules to cover positive exam - ples and then prunes those rules to form a best t to a separate pruning set .
the advantages of ripper are as follows .
the ifthen clauses are easy for humans to understand .
ripper is very fast in training and testing .
ripper allows users to supply prior knowledge con -
ripper allows values to be nominal , continuous , or set -
ripper is nonlinear .
123 ) rocchio : this type of classier ( 123 ) , ( 123 ) uses normal - ized tf - idf representation of the training vectors .
a prototype
indicates the number of documents that are classied as spam or nonspam .
elements of the prototype vector that are is normalized to unit length .
negative are set to zero and then classication is performed by the dot product of the prototype vector and the candidate test vector .
those with large positive dot products are spam and those with large negative values are nonspam .
unlike any of the algorithms discussed previously , there is no natural threshold for the dot product .
that is , the algorithm does not tell us for what values above a critical value of the dot product should we classify the document as spam .
this critical value must be obtained by rank ordering the outputs of dot products of the prototype vector with all the training vectors and nding that critical value that minimizes the training error .
we emphasize that the test vectors should not be used to nd that threshold .
similarly , the optimum value should not be obtained from the test set .
it should be obtained from the training set and it is that the training error .
the advantage of rocchios algorithm is that it is fast in training and testing .
the disadvantage is that
ieee transactions on neural networks , vol .
123 , no .
123 , september 123
for data set i , false alarm rates corresponding to a 123% miss rate .
x indicates that ripper was unable to
obtain the 123% miss rate for these two data sets and thus no false alarm or error rates are reported
one has to search for the optimum threshold and the optimum on the training set which takes extra training time and does
not necessarily generalize well to the test set .
further text categorization methods may be found in ( 123 )
data sets
we used two data sets .
data set i was collected by an at&t staff member and consists of 123 messages that he considered spam and 123 messages that were nonspam .
all e - mail messages consist of a subject and body and the various algorithms were tried on either the subject alone , the body alone , or the subject and body together .
in addition , a stop list was either used or not used and all words were converted to lower case .
the 123 best features in each case were used ( ranked using mutual information ) .
the original feature entries are tf ( term frequency ) and could later be converted to tf - idf or binary features .
thus there were six data sets constructed from the original
bodnostopwords from the body only without using a
bodstopwords from the body only and the stop list was
subbodnostopwords from the subject and body without
using a stop list
subbodstopwords from the subject and body using a
subnostopwords form the subject without using a stop
substopwords from the subject using a stop list .
results are shown in table i .
it can therefore be seen that the smallest error rates are given by using the subject and body without using a stop list ( subbodnostop ) and that svm should be used with binary features .
to put the error rates in context , it should be noted that the test sample size is 123 and thus a difference of approximately 123 can be attributed to a numerical difference of one error per run of the ten - fold cross validation error rate estimate .
thus there is not much difference on subbodnostop between svm using binary features and boosting .
it should be emphasized that the spam lter will never actually reject any messages classied as spam .
the use of the false alarm and miss rates is a mechanism to
the second data set was collected from members of the at&t technical staff who forwarded their spam and nonspam
error and false alarm rates for data set ii using a dictionary consisting of upper and lowercase words
error and false alarm rates for data set ii using a dictionary consisting of lowercase words only
to the authors .
since the spam had already passed through the at&t rewall , it would be expected that the spam would be harder to classify .
there were 123 spam and 123 nonspam messages .
rather than limiting the feature vector to a nite size we used all words in the messages as long as they occurred in at least three messages .
furthermore , stemming was not used .
ripper was unable to achieve either a 123% or 123% miss rate .
this data set had two versions .
in version i ( table ii ) , words that were all capitals were retained but words that were a mixture of upper and lower case were translated to lower case .
the rationale for this is that we could conjecture that words like free would more likely to occur in spam than words like free .
thus , these two words would be two separate features in this version of the database .
furthermore the word free if it occurred in the subject at least three times and the body at least three times would be two separate features .
the net result is that the feature vector of version i was of size
once again , use of binary features is preferred for svm and boosting give approximately equal performance if performance is based on raw error rate .
with approximately 123 messages in the test set , one error corresponds to a numerical difference of 123 123
as pointed out previously , the false alarm rates give a measure of dispersion of the errors .
therefore , on this basis although boosting and svm using binary features are comparable in error rate performance , svm is much better in terms of dispersion of errors .
drucker et al . : svms for spam categorization
error , false alarm , and miss rates for data set ii using
a dictionary consisting of lowercase words only
error and false alarm rates as a function of the upper bound c for binary data using lowercase words only
in version ii of this data set , all words were converted to lower case which lowered the dimensionality to 123
the results for this data set are shown in table iii .
first , in comparing tables ii and iii , we see that there is no advantage in keeping words that are all upper case .
second , it appears that once again , it is better to use binary features with svm machines .
finally , although boosting has the better performance in terms of raw error rate , the spread of errors is better using svm with binary features .
operating points other than the false alarm rate due to a xed miss rate may be of interest .
therefore , in table iv , we present the miss rate due a xed false alarm rate and the operating point where the false alarm rates and miss rate are equal ( last column ) .
these results are consistent with the conclusions reach previously , namely that boosting is superior in terms of test error rate but that svm is better in terms of the spread of errors .
the running times for training and testing have not been optimized for either boosting machines or svms .
however , since the svm only has to execute one dot product , it would be expected to be faster than boosting c123 decision trees .
both have acceptable speeds , in the order of milliseconds for boosting decision trees and microseconds for svm .
this does not include the time to parse a message to build up its feature set .
however , there is a remarkable difference in training time for trees against that of svms .
in our research version of these algorithms , boosting trees take hours to train and svms , in the order of minutes .
the average tree size for this database is approximately 123 trees which take inordinately long to build .
thus based on a combination of training time and performance , svms are superior .
there are two reasons we believe that the training time for svms with binary features can be reduced .
the rst is that the vectors are binary and the second is that the feature vectors are sparse ( typically only 123% of a vector is nonzero ) .
this allows the dot product in the svm optimization to be replaced with faster nonmultiplicative routines .
performance as a function of upper bound the upper bound
affects performance for the cases where the training data is not separable by a linear svm .
in general there is an optimum value of this constant .
however , cannot be obtained be examining the this best value of training data and it is unfair to do so by examining the test data .
only by having a third set of data , a validation set , would it be
error and false alarm rates as a function of the upper
bound c for tf data using lowercase words only
possible to optimize for
in that case , one would maximize using the training set , and then nd the performance on the validation set .
then one would pick another until the performance on the validation set is optimum .
one could then use the test set to nd the nal performance .
even if we had the luxury of using a validation set , this iterative process would take took long .
however , it is illustrative to see what happens if we tune using the test data ( tables v and vi ) .
we see that as
is decreased , the number of support vectors increases and the number of the support vectors that have maximum increases .
however , for the binary case , the performance degrades while for the tf features case , the performance improves and then is lowered .
the results seem to indicate that one can obtain almost equivalent performance by using the on the tf formatted data .
however , in that case we must spend time searching for that optimum , while for the binary features , that is not the case .
furthermore , there is a that gives equivalent performance on the wide variation in
based on examining a number of data sets , different feature representation , and different learning algorithms , we come to the following conclusions .
123 ) for the best case ( all words converted to lower case ) , svms ( using binary features ) and boosting ( using tf features ) are the two best candidates .
boosting has a lower error rate but the dispersion of errors is better
ieee transactions on neural networks , vol .
123 , no .
123 , september 123
123 ) in a choice of between using a stop list and not using a
stop list , it is preferable that a stop list not be used .
123 ) based on a review of the literature , all the features should be used rather than a subset .
the literature seems to indicate that there is an optimum set of features that depends on both the data and the algorithm .
however , searching for the best features takes an unacceptable period of time .
boosting using c123 decision trees implicitly uses a choice of best features as part of the algorithm and svm performance does not degrade if too many features are used .
123 ) training time using boosting decision trees is inordi -
it should be remembered that these performance gures are based on data sets collected from different individuals .
when an individual marks his or her own e - mail , then the from eld of the message can be used .
this can improve the accuracy in at least two ways : the user can generate a list of acceptable senders that is always noted as nonspam no matter what the subject and body contents .
furthermore , return e - mail that is a response to a user query will always be accepted as nonspam .
