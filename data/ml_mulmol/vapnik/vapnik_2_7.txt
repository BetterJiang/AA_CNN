large vc - dimension classi ers can learn dicult tasks , but are usually
impractical because they generalize well only if they are trained with huge
quantities of data .
in this paper we show that even very high - order poly -
nomial classi ers can be trained with a small amount of training data and
yet generalize better than classi ers with a smaller vc - dimension
is achieved with a maximum margin algorithm ( the generalized portrait ) .
the technique is applicable to a wide variety of classi ers , including per -
ceptrons , polynomial classi ers ( sigma - pi unit networks ) and radial basis
functions .
the e ective number of parameters is adjusted automatically
by the training algorithm to match the complexity of the problem
shown to equal the number of those training patterns which are closest
patterns to the decision boundary ( supporting patterns ) .
bounds on the
generalization error and the speed of convergence of the algorithm are
experimental results on handwritten digit recognition demonstrate
good generalization compared to other algorithms .
both experimental evidence and theoretical studies ( 123 ) link the generalization of a
classi er to the error on the training examples and the capacity of the classi er .
part of this work was done while b .
boser was at at&t bell laboratories .
he is now
at the university of california , berkeley .
classi ers with a large number of adjustable parameters , and therefore large ca -
pacity , likely learn the training set without error , but exhibit poor generalization .
conversely , a classi er with insucient capacity might not be able to learn the task
at all .
the goal of capacity tuning methods is to nd the optimal capacity which
minimizes the expected generalization error for a given amount of training data .
one distinguishes two ways of tuning the capacity : starting with a low capacity
system and allocating more parameters as needed or starting with an large capacity
system and eliminating unnecessary adjustable parameters with regularization
rst method requires searching in the space of classi er structures which possibly
contains many local minima .
the second method is computationally inecient since
it does not avoid adjusting a large number of parameters although the e ective
number of parameters may be small .
with the method proposed in this paper , the capacity of some very large vc -
dimension classi ers ( such as polynomial classi ers of high order ) is adjusted au -
tomatically in the process of training .
the problem is formulated as a quadratic
programming problem which has a single global minimum .
only the e ective pa -
rameters get adjusted during training which ensures computational eciency .
123 maximum margin and supporting patterns
here is a familiar problem : given is a limited number of training examples from two
classes a and b; nd the linear decision boundary which yields best generalization
performance .
when the training data is scarce , there exist usually many errorless
separations ( gure 123 ) .
this is especially true when the dimension of input space
the number of tunable parameters ) is large compared to the number of training
examples .
the question arises which of these solutions to choose ? although there
is no de nite answer to this question , the one solution that achieves the largest
possible margin between the decision boundary and the training patterns on either
side appears to be a good choice ( gure 123 ) .
this solution is intuitively justi able :
a new example from class a is likely to fall within or near the convex envelope of
the examples of class a ( and similarly for class b ) .
by providing the largest possible
\safety " margin , we minimize the chances that examples from class a and b cross
the border to the wrong side .
an important property of the maximum margin solution is that it is only depen -
dent upon a restricted number of training examples , called supporting patterns ( or
informative patterns ) .
these are those examples which lie on the margin and there -
fore are closest to the decision boundary ( gure 123 ) .
the number m of linearly
independent supporting patterns satis es the inequality :
m min ( n + 123; p ) :
in this equation , ( n + 123 ) is the number of adjustable parameters and equals the
vapnik - chervonenkis dimension ( vc - dimension ) ( 123 ) , and p is the number of training
examples .
in reference ( 123 ) , we show that the generalization error is bounded by m=p
and therefore m is a measure of complexity of the learning problem .
because m is
bounded by p and is generally a lot smaller than p , the maximum margin solution
obtains good generalization even when the problem is grossly underdetermined ,
the number of training patterns p is much smaller than the number of adjustable
figure 123 : linear separations .
( 123 ) when many linear decision rules separate the training set , which one to choose ?
( 123 ) the maximum margin solution .
the distance to the decision boundary of the
closest training patterns is maximized .
the grey shading indicates the margin area
in which no pattern falls .
the supporting patterns ( in white ) lie on the margin .
parameters , n + 123
in section 123 we show that the existence of supporting patterns
is advantageous for computational reasons as well .
123 non - linear classifiers
although algorithms that maximize the margin between classes have been known
for many years ( 123 ) , they have for computational reasons so far been limited to the
special case of nding linear separations and consequently to relatively simple clas -
si cation problems .
in this paper , we present an extension to one of these maximum
margin training algorithms called the \generalized portrait method " ( gp ) ( 123 ) to
various non - linear classi ers , including including perceptrons , polynomial classi ers
( sigma - pi unit networks ) and kernel classi ers ( radial basis functions ) ( gure 123 ) .
the new algorithm trains eciently very high vc - dimension classi ers with a huge
number of tunable parameters .
despite the large number of free parameters , the
solution exhibits good generalization due to the inherent regularizationof the max -
imum margin cost function .
as an example , let us consider the case of a second order polynomial classi ers
decision surface is described by the following equation :
+ b = 123 :
and b are adjustable parameters , and x
are the coordinates of a pattern
if n is the dimension of input pattern x , the number of adjustable parameters
figure 123 : non - linear separations .
decision boundaries obtained by maximizing the margin in ' - space ( see text )
grey shading indicates the margin area pro jected back to x - space .
the supporting
patterns ( white ) lie on the margin .
( 123 ) polynomial classi er of order two ( sigma - pi
unit network ) , with kernel k ( x; x
) = ( x x
( 123 ) kernel classi er ( rbf ) with
kernel k ( x; x ) = ( exp kx x
of the second order polynomial classi er is ( n ( n + 123 ) =123 ) + 123
in general , the number
of adjustable parameters of a q
order polynomial is of the order of n n
the gp algorithm has been tested on the problem of handwritten digit recognition
( table 123 ) .
the input patterns consist of 123 123 pixel images ( n = 123 )
results achieved with polynomial classi ers of order q are summarized in table 123 .
also listed is the number of adjustable parameters , n .
this quantity increases
rapidly with q and quickly reaches a level that is computationally intractable for
algorithms that explicitly compute each parameter ( 123 ) .
moreover , as n increases ,
the learning problem becomes grossly underdetermined : the number of training
patterns ( p = 123 for db123 and p = 123 for db123 ) becomes very small compared
nevertheless , good generalization is achieved as shown by the experimental
results listed in the table .
this is a consequence of the inherent regularization of
an important concern is the sensitivity of the maximum margin solution to the
presence of outliers in the training data .
it is indeed important to remove undesired
outliers ( such as meaningless or mislabeled patterns ) to get best generalization
performance .
conversely , \good " outliers ( such as examples of rare styles ) must be
cleaning techniques have been developed based on the re - examination by a
human supervisor of those supporting patterns which result in the largest increase
of the margin when removed and thus are the most likely candidates for outliers ( 123 ) .
in our experiments on db123 with linear classi ers , the error rate on the test set
dropped from 123 : 123% to 123 : 123% after cleaning the training data ( not the test data ) .
db123 ( p=123 ) db123 ( p=123 )
table 123 : handwritten digit recognition experiments .
the rst database
( db123 ) consists of 123 clean images recorded from ten sub jects .
half of this data
is used for training , and the other half is used to evaluate the generalization per -
formance .
the other database ( db123 ) consists of 123 images for training and 123
for testing and has been recorded from actual mail pieces .
we use ten polynomial
classi cation functions of order q , separating one class against all others .
we list the
number n of adjustable parameters , the error rates on the test set and the average
number <m>of supporting patterns per separating hypersurface .
the results com -
pare favorably to neural network classi ers which minimize the mean squared error
with backpropagation .
for the one layer network ( linear classi er ) , the error on the
test set is 123 % on db123 and larger than 123 % on db123
the lowest error rate
for db123 , 123 % , obtained with a forth order polynomial is comparable to the 123 %
error obtained with a multi - layer neural network with sophisticated architecture
being trained and tested on the same data .
123 algorithm design
the properties of the gp algorithm arise from merging two separate concepts de -
scribed in this section : training in dual space , and minimizing the maximum loss .
for large vc - dimension classi ers ( n p ) , the rst idea reduces the number of
e ective parameters to be actually computed from n to p .
the second idea reduces
it from p to m .
we seek a decision function for pattern vectors x of dimension n belonging to either
of two classes a and b .
the input to the training algorithm is a set of p examples
with labels y
123 class a
= 123 if x
123 class b :
from these training examples the algorithm nds the parameters of the decision
function d ( x ) during a learning phase .
after training , the classi cation of unknown
patterns is predicted according to the following rule :
x 123 a if d ( x ) > 123
x 123 b otherwise .
we limit ourselves to classi ers linear in their parameters , but not restricted to
linear dependences in their input components , such as perceptrons and kernel - based
classi ers .
perceptrons ( 123 ) have a decision function de ned as :
d ( x ) = w ' ( x ) + b =
( x ) + b;
where the '
are prede ned functions of x , and the w
and b are the adjustable
parameters of the decision function .
this de nition encompasses that of polynomial
classi ers .
in that particular case , the '
are products of components of vector x ( see
equation 123 ) .
kernel - based classi ers , have a decision function de ned as :
; x ) + b;
and the bias b are the parameters to be adjusted and the x
are the training patterns .
the function k is a prede ned kernel , for example a
potential function ( 123 ) or any radial basis function ( see for instance ( 123 ) ) .
perceptrons and rbfs are often considered two very distinct approaches to classi -
cation .
however , for a number of training algorithms , the resulting decision function
can be cast either in the form of equation ( 123 ) or ( 123 ) .
this has been pointed out
in the literature for the perceptron and potential function algorithms ( 123 ) , for the
polynomial classi ers trained with pseudo - inverse ( 123 ) and more recently for regular -
ization algorithms and rbf ' s ( 123 ) .
in those cases , perceptrons and rbfs constitute
dual representations of the same decision function .
the duality principle can be understood simply in the case of hebb ' s learning rule .
the weight vector of a linear perceptron ( '
( x ) = x
) , trained with hebb ' s rule , is
simply the average of all training patterns x
, multiplied by their class membership
substituting this solution into equation ( 123 ) , we obtain the dual representation
d ( x ) = w x + b =
x + b :
the corresponding kernel classi er has kernel k ( x; x
) = x x
and the dual param -
are equal to ( 123=p ) y
in general , a training algorithm for perceptron classi ers admits a dual kernel rep -
resentation if its solution is a linear combination of the training patterns in ' - space :
reciprocally , a kernel classi er admits a dual perceptron representation if the kernel
function possesses a nite ( or in nite ) expansion of the form :
k ( x; x
such is the case for instance for some symmetric kernels ( 123 ) .
examples of kernels
that we have been using include
k ( x; x
) = ( x x
( polynomial expansion of order q ) ;
k ( x; x
) = tanh (
k ( x; x
) = exp (
k ( x; x
) = exp
k ( x; x
) = exp ( kx x
k ( x; x
) = ( x x
exp ( kx x
( mixed polynomial and rbf ) :
these kernels have positive parameters ( the integer q or the real number
can be determined with a structural risk minimization or cross - validation proce -
dure ( see for instance ( 123 ) ) .
more elaborate kernels incorporating known invariances
of the data could be used also .
the gp algorithm computes the maximum margin solution in the kernel representa -
this is crucial for making the computation tractable when training very large
vc - dimension classi ers .
training a classi er in the kernel representation is compu -
tationally advantageous when the dimension n of vectors w ( or the vc - dimension
n + 123 ) is large compared to the number of parameters
, which equals the number
of training patterns p .
this is always true if the kernel function possesses an in nite
expansions ( 123 ) .
the experimental results listed in table `refresults indicate that this
argument holds in practice even for low order polynomial expansions .
123 minimizing the maximum loss
the margin , de ned as the euclidean distance between the decision boundary and
the closest training patterns in ' - space can be computed as
m = min
the goal of the maximum margin training algorithm is to nd the decision function
d ( x ) which maximizes m , that is the solution of the optimization problem
the solution w os this problem depends only on those patterns which are on the
margin , i . e .
the ones that are closest to the decision boundary , called supporting
patterns .
it can be shown that w can indeed be represented as a linear combination
of the supporting patterns in ' - space .
in the classical framework of loss minimization , problem 123 is equivalent to mini -
mizing ( over w ) the maximum loss .
the loss function is de ned as
this \minimax " approach contrasts with training algorithms which minimize the
average loss .
for example , backpropagation minimizes the mean squared error
( mse ) , which is the average of
) = ( d ( x
the bene t of minimax algorithms is that the solution is a function only of a
restricted number of training patterns , namely the supporting patterns .
this results
in high computational eciency in those cases when the number m of supporting
patterns is small compared to both the total number of training patterns p and the
dimension n of ' - space .
123 the generalized portrait
the gp algorithm consists in formulating the problem 123 in the dual - space as
the quadratic programming problem of maximizing the cost function
j ( ; b ) =
under the constrains
> 123 ( 123 , 123 ) .
the p p square matrix h has elements :
where k ( x; x
) is a kernel , such as the ones proposed in ( 123 ) , which can be expanded
as in ( 123 ) .
k ( x; x
) is not restricted to the dot product k ( x; x
) = x x
as in the
original formulation of the gp algorithm ( 123 ) ) .
in order for a unique solution to exist , h must be positive de nite .
the bias b can
be either xed or optimized together with the parameters
this case introduces
another set of constraints :
= 123 ( 123 ) .
the quadratic programming problem thus de ned can be solved eciently by stan -
dard numerical methods ( 123 ) .
numerical computation can be further reduced by
processing iteratively small chunks of data ( 123 ) .
the computational time is linear the
dimension n of x - space ( not the dimension n of ' - space ) and in the number p of
training examples and polynomial in the number m < min ( n + 123; p ) of supporting
patterns .
it can be theoretically proven that it is a polynomial in m of order lower
than 123 , but experimentally an order 123 was observed .
only the supporting patterns appear in the solution with non - zero weight
; x ) + b;
' ( x ) + b
using the kernel representation , with a factorized kernel ( such as 123 ) , the classi ca -
tion time is linear in n ( not n ) and in m ( not p ) .
we presented an algorithm to train polynomial classi ers of high order and radial
basis functions which has remarquable computational and generalization perfor -
mances .
the algorithms seeks the solution with the largest possible margin on both
side of the decision boundary .
the properties of the algorithm arise from the fact
that the solution is a function only of a small number of supporting patterns , namely
those training examples that are closest to the decision boundary .
the generaliza -
tion error of the maximum margin classi er is bounded by the ratio of the number
of linearly independent supporting patterns and the number of training examples .
this bound is tighter than a bound based on the vc - dimension of the classi er
family .
for further improvement of the generalization error , outliers corresponding
to supporting patterns with large
can be eliminated automatically or with the
assistance of a supervisor .
this feature suggests other interesting applications of
the maximum margin algorithm for database cleaning .
we wish to thank our colleagues at uc berkeley and at&t bell laboratories for
many suggestions and stimulating discussions .
comments by l .
bottou , c .
cortes ,
sanders , s .
solla , a .
zakhor , are gratefully acknowledged .
we are especially in -
debted to r .
baldick and d .
hochbaum for investigating the polynomial convergence
property , s .
hein for providing the code for constrained nonlinear optimization , and
haussler and m .
warmuth for help and advice regarding performance bounds .
