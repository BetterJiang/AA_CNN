that the probability is at most
from a continuous proba our method is a modifica
of an object
in the training
a method for predicting
the pairs object / classification by an i . i . d .
process tion of vapnik ' s main novelty is tion .
we also describe by the support
that it gives not only the pre but also a practicable found in support
of the algorithms
of that predic
among xi , . . .
, xl+i )
where the points xi , . . .
, xl+l are generated dently from the underlying are defined in section we need to know the p , while the only information
123 below .
to apply this we do know is
this is not sufficient
inference; in our present
tion in ( 123 ) .
remark 123 dawid ( 123 ) distinguishes ference is some assertion about the accuracy oped , the situation
to use this terminology
to change in the future . )
, the sv method
since the sv method is being actively
but no stochastic
is the prediction
123 the problem
by n real - valued
points ( xi , yi ) ( i = 123 , 123 , .
) , where and yi e ( - 123 , 123 ) , are generated from an unknown ( but the same for all points )
xi e lrn ( our objects i = 123 , .
, l , together ( - 123 , 123 ) , and an ( l + 123 ) th unclassified should it be classified ? tion , in the sense that we are interested of a particular rule for classifying sion of transduction ,
in the classifi than in a general for further discus
with their classifications
we are given l points xi ,
( this is a problem
point xt+l how
is vapnik ' s ( 123 )
vector ( sv ) machines .
method of support method works very well in practice , of the accuracy tions are known if our only information in this context , theorem
and one unclassified
from ( 123 ) ( theorem
of its predic is l classified the most relevant ,
123 predicting with
set and one point to be classified ) , and the only
( 123 ) , our transduc off its substantiation in the space lrn : ( the l points ( l + 123 ) points
now we briefly describe , tion 123
we consider in the training sified as - 123 and in the 123 - picture can be proven that the ( l+ l ) th point will be a support let sv ( l ) ( resp .
sv ( - 123 ) ) be the set of indices
in at least one of the pictures .
we let #a stand for the
set are classified in the - ! - picture
that point is clas it is classified
is the classification
learning by transduction 123
of set a .
our algorithm
gives the following
( ( l + 123 ) e sv ( - 123 ) ) & ( ( l + 123 ) sv ( 123 ) )
( ( l + 123 ) e sv ( - 123 ) n sv ( 123 ) ) & ( #sv ( - 123 ) < #sv ( 123 ) ) ; of this prediction
we found that its performance
by the number of mistakes
made ) is not as
of the standard
in some applications confidence
this finding does not might be even
good as the performance mean that the standard it gives the same confidences ) .
but it does make it
( and in the case of our transductive
for any prediction
( ( l + 123 ) e sv ( 123 ) )
& ( ( l + 123 ) sv ( - 123 ) )
( ( l + 123 ) e sv ( - 123 ) n sv ( 123 ) ) < #sv ( - 123 ) ) ,
l + 123 '
( ( l + 123 ) e sv ( - 123 ) n sv ( 123 ) ) & ( #sv ( - 123 ) = #sv ( 123 ) )
we have two pictures ,
the 123 - picture .
let fi be the prediction by the given prediction
the - 123 - picture and for yl+l made
is defined to be
#sv ( - fi ) if ( l + 123 ) e sv ( - a ) 123
oo , otherwise .
l + 123 '
of a prediction
is the same as be
as a win of on a 123 ticket ways true that ( l + 123 ) e sv ( - fi ) .
so in this most
. ) for the 123 can realize : it is al
in a fair lottery
j - l is as likely
for us case our procedure
is just 123 - #s ' ( / : " ;vl .
in our experiments role is played by what we call the " possibility " data set; this is discussed
we found that a very
123 and 123 below .
123 measures of impossibility
is as follows : as a win of
j - l is as likely
of a prediction on a 123 ticket will be given below .
in a fair lottery .
in this section us to define the notion of incertitude;
( 123 ) and ( 123 ) .
which will enable
or , in other words , confident
of small in
is small in both
the sv method , will be satisfied; in vapnik ( 123 ) ( footnote
in the experi 123 on p .
123 ) 123% to 123% of the data
our method works well ( gives when the number of support that this assumption set .
our experiments in terms of confidences
123 ) have typically
to be 123 - i , i being incertitude .
it is often easier
signed to optimize
above was de
in our computer
space xi e 123r n with their classifications
sample space is ( x123 , . . .
, xl+ 123 ) of l + 123 points with the usual if p is a probability distribution
let n be some sample space ( a typical the set of all sequences yi e ( - 123 , 123 ) , i = 123 , . . .
, l + 123 , equipped p : n - 123 123r such that
inn , a p
is defined to be a non - negative
of the notion of lottery;
p as the randomizing
this is our explication lots and p ( w ) as the value of the prize won by a par w .
notice that we do ( 123 ) with an not exclude " fair "
when p produces
used for drawing
123 gammerman , vovk , and vapnik
ing the tickets though in real lotteries ally much less than 123
means that all proceeds side of ( 123 ) is usu
in the form of prizes ) ,
y , pis large with small prob c > 123 , for any constant
p ( w e s123 : p ( w ) c ) c
that if p is chosen in ad
put z = x x ( - 123 , 123 ) we are given a sam - zi = ( xi , yi ) e z , example xf+123 e x; p in z .
yl+ 123 e ( - 123 , 123 )
( y = ( - 123 , 123 ) is our label space ) .
ple z123 , . . .
, zz of classified i = 123 , . .
, l , and one unclassified ( xi , yi ) are assumed to be generated from some unknown probability our goal is to predict for doing so is as follows .
choose a permutation measure
z123 , . . .
' zz , xz+123 we cal
z123+123 - - t jr .
after observing
vance and we believe
that p ( w ) will turn
that p is the true probability dis the data w e 123 , then it is hardly
p , 123 = 123jp ( z123 , . . .
, zz , ( xf+123 , 123 ) ) .
then we predict p . - 123 < p , 123 , with - 123 if fi . - 123 > p , 123 , and predict arbitrarily if p , - 123 = p , 123 ) ; the incertitude
of our prediction
with argmaxp .
( i . e . ,
with 123 if
= min ( p , - 123 , f123 )
( and our confidence
is small ,
in our prediction
of this measure of our incertitude is right unless
a 123 ticket
is 123 - p . ) .
the sure that our
we can be pretty
pm in zm ' p running over in z .
our interpretation
for any constant
if pis a cm ( z ) - measure
notice that chebyshev
e & prediction
is wrong ) e ,
e > 123 and any distribution of data is given by the possibility
remark 123 the notion of a " critical the theory of testing a small probability
case of our notion of a measure a subset a n of the sample space of o = p ( a ) is identified ( 123 / 123 , ifw e a ,
p ( w ) = 123 , otherwise .
from a continuous distribution ,
m is a positive
to be a function for all p e p .
most of
for the set
( the sample size ) ,
and em ( z ) stands
in the cm ( z ) - measures
ty , where z is a measurable
if p is a family a p - measure of impossibility all we are interested of all product of this definition is as follows : and z123 , .
, zm are generated it is hardly pos sible that p ( z123 , , zm ) is large ( provided
the data z123 , . . .
, zm are generated ) .
p : zm - - t 123r is a permutation if , for any sequence
now we shall introduce z123 , .
, zm in zm , p ( z123 , . . .
, zm ) = oo if zi = zj for some i = / : .
j; , ; , , l123r p ( z123f ( 123 ) , , z123r ( m ) ) = 123 ( the sum is over all 123r of the set ( 123 , . . .
, m ) ) , if all ele ments of the set ( z123 , . . .
, zm ) are different .
it is obvious measure of impossibil
that every such p is indeed a cm ( z )
123 gen eral scheme
p ( z123 , . . .
, zz+123 ) is guaranteed
if this value is small , big no matter which yl+123 will turn up; therefore , data are hardly possible , and our experiments shown that the quality ity does not depend ( unlike 123 in the case where it exceeds
very poor .
notice that the notion
the value of possibility
used and is a property
on the predic
for such data is
of the data
of the predictions
the prediction algorithm we have already we can associate
to be used , with the al a measure of incertitude
if , however ,
on the algorithm
y for y123+123 is
first we describe l ( a positive
our task .
w fix a training space x ( an
and an attribute
of this measure to what we had before :
wins in a lottery .
a 123 ticket
learning by transduction 123 ( 123 ) with i > 123 and b123 =f .
b123 is clearly impossible
at least one of the inequalities be ( b123 , b123 ) would be strict would not be attained ) ;
and so the minimum in ( 123 )
in the separable
to be any ( xi , yi )
we define a support in ( 123 ) holds as equality .
a sample ( x123 , yl ) , . . .
, ( xi , yl ) and one unclas where yl+l = 123 , and the - 123 - picture , the most important , for our purposes ,
where yl+l = - 123
is the following .
as before ,
lemma 123 if the sample ( x123 , yl ) , . . .
, ( xi , yl ) contains in at least one of the two pictures .
zl+i is a support
follows from lemma
yi = - 123
that an inequality
we define a permutation
proof this immediately the simple observation
of type ( 123 ) cannot be strict ( with i = 123 ) for both yi = 123 and p ( zl , . . .
, zl+l ) = #sv ( zl , . . .
, , +l ) ( l+l , if zl+l e sv , set ( z123 , . . .
, zl+r ) andzi = ( xi , yi ) , i = 123 , . . .
, l+ l .
, zl+l ) are the support
that zi are all different;
( we were assuming p ( z123 , . . .
, zl+l ) = oo by definition . ) now we can apply lemma 123 , we have three possibilities :
scheme of section
zl+l is a support
only in - 123 - picture;
zl+l is a support
only in 123 - picture;
zl+l is a support
in both pictures .
let cl123 be the fraction cl123 and j123 are small ( as already
of the support and j123 in the 123 - picture;
of our prediction is
we will assume that
the case ) .
in cases 123 and 123 , the incertitude
123 sv implement at ion
a general pre
in the previous ser ' s ( 123 ) procedure
of this general
scheme ( in particular ,
this scheme covers
we shall consider
one of the possi
123 and a . 123 , or vapnik ( 123 ) ) .
this defini
to begin with , we briefly describe nik ( 123 ) , sections tion is usually tion .
in this paper , we shall always assume that this
is identical; extension
under some , often
not to the original
case is trivial .
data but to
of our results
yl+d ) , where xi e
let our data be ( ( x123 , yl ) , . . .
, ( xl+l , irn and yi e ( - 123 , 123 ) , i e ( 123 , . . .
, l +123 ) ( our notation 123 for the sample size is chosen for agreement with yi = 123 ( resp .
rest of the paper ) .
- 123 ) will be called
<i> ( w , ) = 123 ( w w ) + c t; ; - + min ( 123 ) ( we lrn , = ( 123 , . . .
, l+i ) e jr123+123 ) ,
where cis an a priori to the constraints
i123 : 123 , i=123 , . .
lemma 123 quadratic ( 123 ) and ( 123 ) has a unique solution the sample ( x123 , y123 ) , .
, ( xl+l , yl+d contains
itive and negative
it is clear that a solution
( w ( l ) , b ( l ) , ( 123 ) ) , ( w ( 123 ) , b ( 123 ) , ( 123 ) ) ( where ( j ) = ( dj ) , . . .
, 123s ) 123 ) , j = 123 , 123 ) be any two ( w ( l ) + w ( 123 ) b ( l ) + b ( 123 ) ( 123 ) + ( 123 ) )
123 ' 123 ' 123
( 123 ) and ( 123 ) and , because w ( l ) = w ( 123 )
of the functional
<i> ( w , ) , will provide
value for this functional
and ( l ) = ( 123 ) .
therefore , w and are determined and b = b123 and b123 =f .
b123 , then all i are zero ( otherwise ,
at both b = b123
if the minimum is attained
m x - b 123
in other words , case 123 , whatever sure of impossibility
we make a confident yl+l turns up , our permutation p will take a large value ( at least ) , and so this case is hardly possible .
a d 123 )
remark 123 even in case 123 our algorithm
that cl123 and j123 are
123 gammerman , vovk , and vapnik
which looks counterintuitive .
dawid sug both min ( l123 , 123 ) and max ( l123 , 123i ) as
notice that ( 123 ) is the incertitude arg maxy 123y in case 123 as well .
this justifies in the introduction .
we can see that vapnik ' s sv method provides sures of impossibility the scheme of section
that are especially 123
the reason why this is so is
in the inductive
that , given the training
of our procedure
we are interested
in this section ence ( with k = 123 ) ; recall for our problem , requires z123 , . . .
, zz , we should work out a general a future object and what we are interested of this rule .
x as - 123 or 123
it is clear that such a general
in are the explicit
rule for clas
let us solve the quadratic
of the prediction
there are usually zl+l is a support
in at least one of the pic
yi ( ( w xi ) + b ) ; : : : 123 - i , i ; : : : 123 , i = 123 , . .
which is an analogue
for the training
notion being defined as fol
vector if the value of the optimization
( xi , yi ) , j e ( 123 , .
, l+ 123 ) , is an essential
remark 123 it is clear that the above argument hold if we replace " support lows .
a vector from the sum in ( 123 ) and deleting and ( 123 ) corresponding ple shows that these two notions sider the set ( ( x123 , yt ) , . . .
, ( xwo , ywo ) ) of 123 classified
the term j in ( 123 ) to i = j .
the following
are indeed different .
does not change after deleting
in the plane defined as
yi = - 123 , i = 123 , . .
xi= ( i - 123 , 123 ) , yi = 123 , i =123 , . . .
' 123
here we have 123 support
and no essential
let the unique ( see lemma 123 ) solution be ( w* , b* , c ) , and let the number of support be n .
we shall say that x is a y - point ,
to this problem y e ( - 123 , 123 ) , if
y ( ( w* x ) + b* ) > 123
it is easy to see that our method will always y for a y - point #sv ( - y ) is the number of support
#s ' ( j - ; y ) ( recall #s ' : ' . t ( - ;y ) of support
if the fraction
the situation where
in the - y - picture
to the " bor
in the - y
i ( w* x ) + b*l 123
is more complicated : depend on the exact positions this border region
of the positive
is an interesting
we omit the derivation ( see the end of section
at the end of section
to the predictions
of the procedure
made by the sv machine
123 ) from the general
123 tran sduction and
to a set
the most well - known
remark 123 transduction of algorithms known this class is k - nearest is not based on the similarities most of the instance - based
us to introduce
in this paper , however ,
using the support vec
for the problem of pattern
is inference from particular
yi , i = 123 , .
, l , set , we are to guess the classifications
means that , given the classifications x123 , .
, x ! in the training of the l points x123+123 , . . .
, x ! +k in the test set .
in the main part of this paper we only consider to the case k > 123 ( see
the case k = 123 , though our
can be easily
of the k points
123 computer experiments
ant of the sv method ( described have been conducted .
using a database its , where each digit was a 123 x 123 vector
we have chosen a simple pat
of us postal
data of 123 dig
learning by transduction 123
for a sub
for the training
for the test
the experiments set of these data ( 123 examples set ) , and included tion of two - class a digit " 123 " .
a set of preliminary that the minimum number of errors
of degree 123
a digit " 123 " from
out of 123 examples
digit 123 was as 123 and three times digit 123 was
the transductive algorithm , made 123 errors nised as digit 123 ) .
as 123 ) with one undecided show just 123 error ( digit
123 was recog
using the support
cases where the new example in one of the pictures made no mistakes .
ample is a support case the transductive be the wrong picture actly by the fact that we can see that optimizing
and we believe
of why support
than our algorithm
is as follows .
is not a support
are easy and both algorithms
so let us suppose
that the new ex
vector in both pictures .
trying to optimize
figure 123 : measures
to the picture
that have more support
of support vector machines
data sets can usually
with a small number of support vectors .
are , at least to some degree ,
them is an interesting
how to identify
for a new patient symptoms given a set of past patients the records were collected at a hospital and our main purpose of the transductive
( such as those presented
is to compare
in gammerman and
using the support vector machine with
the results
in the end in figure 123
the data have been split into two clusters :
sures of confidence ter and 123 correct
less than 123 ( cluster
equal to 123 ( cluster 123 ) .
there are 123
and 123 incorrect in the second cluster .
for both clusters .
by o ' s ) in the first clus by x ' s ) clas table 123 gives some
on a number of parameters
the sv method depends c in ( 123 ) ; it is clear that there ( such as the constant are many possible of the sv method : say , we could replace a by ef+c123 , where j > 123 , in ( 123 ) ) .
in the transductive it is important , of the sv method ( see section 123 ) , that the number of the support
should be small .
we plan to
for determining which values
we expect that good
table 123 : some characteristics ( which can be identified in figure 123
by their average
of the two clusters
one of the results that we can assess with high accuracy to 123; and the us to classify
from these experiments of the data by using the
the new example
data which do not enable
the new example confidently with low measure
we are currently
with the measures of confidence
123 gammerman , vovk , and vapnik
will be obtained
w ) + cl . : : t+<> - + min ,
for the objective
the degree of zi ' s " supportiveness " ;
we could use the value of the lagrange
to ( 123 ) that will allow us to cope with the distortion
a possible
with c large and 123 > 123 small ( or even 123 = 123 ) .
the 123 > 123 ensures that the objective sible and the arguments
it is computationally
p ( zt , . . .
, zz+t ) =f ( )
f ( o : z+t ) ( l + 123 ) o : t + . . .
+ 123 : ! +123
f ( ) ' ( 123 )
in this section
we will very briefly
of this paper could be
in which the results
where f is some monotonic with f ( o ) = 123
sometimes as the vectors when o : > 123 ) .
zi for which o : i > 123; under this definition case of ( 123 ) corresponding
f ( o : ) =signa : ( that is , f ( o ) = 123 and f ( o : ) = 123
123 more than one unclassified
is to extend our ap
way to the prob ( see vapnik ( 123 ) ) .
in the yi are no longer
proach in a computationally lem of regression quired to be binary and can take any real values .
case the key observation ( which is an of lemma 123 above ) is the following : in at least one of these two pic tures the last object will be a support e is the constant of e or less from the if the fraction we will be able to give a prediction most e and high confidence .
of the last object in two pictures
are not punished . )
is small in all pictures ,
if our task is of k new examples
yl+l , . . .
, yl+k xt+l , . . .
, xt+k given the classifi xt , .
, xz in the
yt , . . .
, yt of the examples set , ( 123 ) can be generalized
pzl , , zl+l -
) - f ( o : l+l ) + . . .
+ f ( o : t+k ) l + k .
o : t + . . .
+ o : ! +k f ( ) f ( ) - k - ,
it is easy to check that this formula defines a valid
w ith each possible
yl+l = a123 , . .
, yl+k = ak
we can associate
123 distortion phenomenon
, ak ) p ( ( xt ' yt ) , ' ( xl+k ' yt+k ) )
and make a prediction
with the smallest
all data points
by the number of support
of a correct
our data are far from being ran random data we can expect that
number of support
will be support
dom; for completely in the " wrong picture " , that if that picture ber of support there is little for very large data sets; in this subsection
and so the will drop .
we have not
of our prediction
in our experiments
will grow sharply
is " too wrong " ,
and a natural
doubt that it will be a serious obstacle
123 non - continuous case
it is easy ( but tedious ) to the case of a probability distribution
we shall only
of a permutation
in this subsection
all our results
that is not
in z is a subset of z to each element is the sum of the of a finite se
which is assigned ber ) ; the cardinality of a hyperset
some arity ( a positive
of its elements .
the value of our permutation ( see ( 123 ) ) depends on every example being a support zi is a support
zi only through idea is to use not
but to take
measure of impossibility
is the hyperset the arity of each element
of all elements
of ( 123 ) , with
equal to the number of times
learning by transduction 123
pm in zm ( with p running of a subclass
we let pm ( z ) stand for the set of all product over all , not nec n l p ( z123 , . . .
, zm ) = 123 ,
the following to the definition 123
a non - negative p : zm - - - + 123r is an exchangeable
if , for any hyperset
b of car
( z123 , . . .
, z , . ) of signature
where n is the number of all possible ( z123 , . . .
, zm ) of signature n = ( h+ , +b / ) 123 ) .
to its elements ,
b ( if b assigns bl . .
b123 , . .
123 the exchangeability model
w iley , new york , 123 , vol
fraser .
sequentially
cally equivalent blocks .
gammerman .
gammerman
thatcher .
dence of symptoms .
ics , 123 , pp .
123 - 123
lecun ,
boser , j .
denker ,
son , r .
howard ,
hubbard ,
jackel .
morgan kaufmann ,
in neural information
vapnik .
the nature of statistical
new york , 123
on the concept
of the bernoulli
123 : 123 - 123 ,
( 123 ) v .
a logic of probability soc .
b 123 : 123 - 123 ,
to the foundations
, with application
( 123 ) v .
vovk and v .
v ' yugin .
b 123 : 123 - 123 , 123
of the bayesian
on the empirical j .
statist .
that our examples
which only assumes
what we actually
under the exchangeability
was not this i . i . d .
model but a weaker model
so far we have assumed erated by an i . i . d .
that the ex amples z123 , . . .
, zt+k are equiprobable .
model shows that the model of ex of the bernoulli weaker than the i . i . d .
( see , e . g , ( 123 ) ) .
it is clear that the scheme of the extra strength other hand , if the idea of replacing by the exchangeability it would be natural sures of impossibility ) .
model , but it one can make use of the i . i . d .
assumption .
to drop the requirement of a permutation
this would make little
model is to be taken seriously
in the definitions
an open question
and so a fortiori
we are interested in
we thank epsrc through grant gr / l123 ments by the members of program committee
we are also grateful for help with
and mark stitson
( " support vector and
