abstract .
the support - vector network is a new learning machine for two - group classification problems .
the machine conceptually implements the following idea : input vectors are non - linearly mapped to a very high - dimension feature space .
in this feature space a linear decision surface is constructed .
special properties of the decision surface ensures high generalization ability of the learning machine .
the idea behind the support - vector network was previously implemented for the restricted case where the training data can be separated without errors .
we here extend this result to non - separable training data .
high generalization ability of support - vector networks utilizing polynomial input transformations is demon - strated .
we also compare the performance of the support - vector network to various classical learning algorithms that all took part in a benchmark study of optical character recognition .
keywords : pattern recognition , efficient learning algorithms , neural networks , radial basis function classifiers ,
more than 123 years ago r . a .
fisher ( fisher , 123 ) suggested the first algorithm for pattern recognition .
he considered a model of two normal distributed populations , n ( m123 , ei ) and n ( m123 , e123 ) of n dimensional vectors x with mean vectors m123 and m123 and co - variance matrices e123 and e123 , and showed that the optimal ( bayesian ) solution is a quadratic decision
in the case where e123 = e123 = e the quadratic decision function ( 123 ) degenerates to a linear
to estimate the quadratic decision function one has to determine " ( " +123 ) free parameters .
to estimate the linear function only n free parameters have to be determined .
in the case where the number of observations is small ( say less than 123n123 ) estimating o ( n123 ) parameters is not reliable .
fisher therefore recommended , even in the case of ei ^ 123 , to use the linear discriminator function ( 123 ) with of the form :
where t is some constant123
fisher also recommended a linear decision function for the case where the two distributions are not normal .
algorithms for pattern recognition
cortes and vapnik
figure 123
a simple feed - forward perceptron with 123 input units , 123 layers of hidden units , and 123 output unit .
the gray - shading of the vector entries reflects their numeric value .
were therefore from the very beginning associated with the construction of linear deci -
in 123 rosenblatt ( rosenblatt , 123 ) explored a different kind of learning machines : perceptrons or neural networks .
the perceptron consists of connected neurons , where each neuron implements a separating hyperplane , so the perceptron as a whole implements a piecewise linear separating surface .
see fig
no algorithm that minimizes the error on a set of vectors by adjusting all the weights of the network was found in rosenblatt ' s time , and rosenblatt suggested a scheme where only the weights of the output unit were adaptive .
according to the fixed setting of the other weights the input vectors are non - linearly transformed into the feature space , z , of the last layer of units .
in this space a linear decision function is constructed :
by adjusting the weights ai from the ith hidden unit to the output unit so as to minimize some error measure over the training data .
as a result of rosenblatt ' s approach , construction of decision rules was again associated with the construction of linear hyperplanes in some
an algorithm that allows for all weights of the neural network to adapt in order locally to minimize the error on a set of vectors belonging to a pattern recognition problem was found in 123 ( rumelhart , hinton & williams , 123 , 123; parker , 123; lecun , 123 ) when the back - propagation algorithm was discovered .
the solution involves a slight modification of the mathematical model of neurons .
therefore , neural networks implement " piece - wise linear - type " decision functions .
in this article we construct a new type of learning machine , the so - called support - vector network .
the support - vector network implements the following idea : it maps the input vectors into some high dimensional feature space z through some non - linear mapping chosen a priori .
in this space a linear decision surface is constructed with special properties that ensure high generalization ability of the network .
figure 123
an example of a separable problem in a 123 dimensional space .
the support vectors , marked with grey squares , define the margin of largest separation between the two classes .
example .
to obtain a decision surface corresponding to a polynomial of degree two , one can create a feature space , z , which has n = 123&123 coordinates of the form :
where x = ( x\ , . . . , xn ) .
the hyperplane is then constructed in this space .
two problems arise in the above approach : one conceptual and one technical .
the con - ceptual problem is how to find a separating hyperplane that will generalize well : the dimen - sionality of the feature space will be huge , and not all hyperplanes that separate the training data will necessarily generalize well123
the technical problem is how computationally to treat such high - dimensional spaces : to construct polynomial of degree 123 or 123 in a 123 dimensional space it may be necessary to construct hyperplanes in a billion dimensional
the conceptual part of this problem was solved in 123 ( vapnik , 123 ) for the case of optimal hyperplanes for separable classes .
an optimal hyperplane is here defined as the linear decision function with maximal margin between the vectors of the two classes , see fig .
it was observed that to construct such optimal hyperplanes one only has to take into account a small amount of the training data , the so called support vectors , which determine this margin .
it was shown that if the training vectors are separated without errors by an optimal hyperplane the expectation value of the probability of committing an error on a test example is bounded by the ratio between the expectation value of the number of support vectors and the number of training vectors :
cortes and vapnik
note that this bound does not explicitly contain the dimensionality of the space of separation .
it follows from this bound , that if the optimal hyperplane can be constructed from a small number of support vectors relative to the training set size the generalization ability will be higheven in an infinite dimensional space .
in section 123 we will demonstrate that the ratio ( 123 ) for a real life problems can be as low as 123 and the optimal hyperplane generalizes well in a billion dimensional feature space .
be the optimal hyperplane in feature space .
we will show , that the weights w123 for the optimal hyperplane in the feature space can be written as some linear combination of
the linear decision function / ( z ) in the feature space will accordingly be of the form :
where zi - z is the dot - product between support vectors zi and vector z in feature space .
the decision function can therefore be described as a two layer network ( fig
however , even if the optimal hyperplane generalizes well the technical problem of how to treat the high dimensional feature space remains .
in 123 it was shown ( boser , guyon , & vapnik , 123 ) , that the order of operations for constructing a decision function can be interchanged : instead of making a non - linear transformation of the input vectors fol - lowed by dot - products with support vectors in feature space , one can first compare two vectors in input space ( by e . g .
taking their dot - product or some distance measure ) , and then make a non - linear transformation of the value of the result ( see fig .
this en - ables the construction of rich classes of decision surfaces , for example polynomial decision surfaces of arbitrary degree .
we will call this type of learning machine a support - vector
the technique of support - vector networks was first developed for the restricted case of separating training data without errors .
in this article we extend the approach of support - vector networks to cover when separation without error on the training vectors is impossible .
with this extension we consider the support - vector networks as a new class of learning machine , as powerful and universal as neural networks .
in section 123 we will demonstrate how well it generalizes for high degree polynomial decision surfaces ( up to order 123 ) in a high dimensional space ( dimension 123 ) .
the performance of the algorithm is compared to that of classical learning machines e . g .
linear classifiers , ^ - nearest neighbors classifiers , and neural networks .
sections 123 , 123 , and 123 are devoted to the major points of the derivation of the algorithm and a discussion of some of its properties .
details of the derivation are relegated to an appendix .
figure 123
classification by a support - vector network of an unknown pattern is conceptually done by first trans - forming the pattern into some high - dimensional feature space .
an optimal hyperplane constructed in this feature space determines the output .
the similarity to a two - layer perceptron can be seen by comparison to fig
optimal hyperplanes
in this section we review the method of optimal hyperplanes ( vapnik , 123 ) for separation of training data without errors .
in the next section we introduce a notion of soft margins , that will allow for an analytic treatment of learning with errors on the training set .
the optimal hyperplane algorithm
the set of labeled training patterns
is said to be linearly separable if there exists a vector w and a scalar b such that the inequalities
cortes and vapnik
figure 123
classification of an unknown pattern by a support - vector network .
the pattern is in input space compared to support vectors .
the resulting values are non - linearly transformed .
a linear function of these transformed values determine the output of the classifier .
are valid for all elements of the training set ( 123 ) .
below we write the inequalities ( 123 ) in the
the optimal hyperplane
is the unique one which separates the training data with a maximal margin : it determines the direction w / |w| where the distance between the projections of the training vectors of two different classes is maximal , recall fig .
this distance p ( w , b ) is given by
the optimal hyperplane ( w123 , b123 ) is the arguments that maximize the distance ( 123 ) .
it follows from ( 123 ) and ( 123 ) that
this means that the optimal hyperplane is the unique one that minimizes w w under the constraints ( 123 ) .
constructing an optimal hyperplane is therefore a quadratic programming
vectors xi for which yi ( w x , + 123 ) = 123 will be termed support vectors .
in appendix a .
123 we show that the vector wq that determines the optimal hyperplane can be written as a linear combination of training vectors :
where a > 123
since a > 123 only for support vectors ( see appendix ) , the expression ( 123 ) represents a compact form of writing w123
we also show that to find the vector of parameters
one has to solve the following quadratic programming problem :
with respect to ar = ( a 123 , . . . , at ) , subject to the constraints :
where 123t = ( 123 , . . . , 123 ) is an - dimensional unit vector , yr = ( yi sional vector of labels , and d is a symmetric i x ^ - matrix with elements
y^ ) is the - dimen -
the inequality ( 123 ) describes the nonnegative quadrant .
we therefore have to maximize the quadratic form ( 123 ) in the nonnegative quadrant , subject to the constraints ( 123 ) .
when the training data ( 123 ) can be separated without errors we also show in appendix a the following relationship between the maximum of the functional ( 123 ) , the pair ( ao , b123 ) , and the maximal margin po from ( 123 ) :
if for some a* and large constant w123 the inequality
is valid , one can accordingly assert that all hyperplanes that separate the training data ( 123 ) have a margin
cortes and vapnik
if the training set ( 123 ) cannot be separated by a hyperplane , the margin between patterns of the two classes becomes arbitrary small , resulting in the value of the functional w ( a ) turning arbitrary large .
maximizing the functional ( 123 ) under constraints ( 123 ) and ( 123 ) one therefore either reaches a maximum ( in this case one has constructed the hyperplane with the maximal margin po ) , or one finds that the maximum exceeds some given ( large ) constant wo ( in which case a separation of the training data with a margin larger then v^ / wo is impossible ) .
the problem of maximizing functional ( 123 ) under constraints ( 123 ) and ( 123 ) can be solved very efficiently using the following scheme .
divide the training data into a number of portions with a reasonable small number of training vectors in each portion .
start out by solving the quadratic programming problem determined by the first portion of training data .
for this problem there are two possible outcomes : either this portion of the data cannot be separated by a hyperplane ( in which case the full set of data as well cannot be separated ) , or the optimal hyperplane for separating the first portion of the training data is found .
let the vector that maximizes functional ( 123 ) in the case of separation of the first portion be a123
among the coordinates of vector a123 some are equal to zero .
they correspond to non - support training vectors of this portion .
make a new set of training data containing the support vectors from the first portion of training data and the vectors of the second portion that do not satisfy constraint ( 123 ) , where w is determined by a123
for this set a new functional w123 ( a ) is constructed and maximized at a123
continuing this process of incrementally constructing a solution vector a* covering all the portions of the training data one either finds that it is impossible to separate the training set without error , or one constructs the optimal separating hyperplane for the full data set , a , = a123 - .
note , that during this process the value of the functional w ( a ) is monotonically increasing , since more and more training vectors are considered in the optimization , leading to a smaller and smaller separation between the two classes .
the soft margin hyperplane
consider the case where the training data cannot be separated without error .
in this case one may want to separate the training set with a minimal number of errors .
to express this formally let us introduce some non - negative variables , - > 123 , i = i , . . . , ( , .
we can now minimize the functional
for small a > 123 , subject to the constraints
for sufficiently small a the functional ( 123 ) describes the number of the training errors123
minimizing ( 123 ) one finds some minimal subset of training errors :
if these data are excluded from the training set one can separate the remaining part of the training set without errors .
to separate the remaining part of the training data one can construct an optimal separating hyperplane .
this idea can be expressed formally as : minimize the functional
subject to constraints ( 123 ) and ( 123 ) , where f ( u ) is a monotonic convex function and c is
for sufficiently large c and sufficiently small a , the vector wo and constant b123 , that minimize the functional ( 123 ) under constraints ( 123 ) and ( 123 ) , determine the hyperplane that minimizes the number of errors on the training set and separate the rest of the elements with maximal margin .
note , however , that the problem of constructing a hyperplane which minimizes the number of errors on the training set is in general np - complete .
to avoid np - completeness of our problem we will consider the case of a 123 ( the smallest value of a for which the optimization problem ( 123 ) has a unique solution ) .
in this case the functional ( 123 ) describes ( for sufficiently large c ) the problem of constructing a separating hyperplane which minimizes the sum of deviations , , of training errors and maximizes the margin for the correctly classified vectors .
if the training data can be separated without errors the constructed hyperplane coincides with the optimal margin hyperplane .
in contrast to the case with a < i there exists an efficient method for finding the solution
of ( 123 ) in the case of a = 123
let us call this solution the soft margin hyperplane .
in appendix a we consider the problem of minimizing the functional
subject to the constraints ( 123 ) and ( 123 ) , where f ( u ) is a monotonic convex function with f ( 123 ) = 123
to simplify the formulas we only describe the case of f ( u ) = u123 in this section .
for this function the optimization problem remains a quadratic programming problem .
in appendix a we show that the vector w , as for the optimal hyperplane algorithm , can
be written as a linear combination of support vectors x , :
to find the vector at = ( i , . . . , at ) one has to solve the dual quadratic programming problem of maximizing
subject to constraints
cortes and vapnik
where 123 , a , y , and d are the same elements as used in the optimization problem for constructing an optimal hyperplane , s is a scalar , and ( 123 ) describes coordinate - wise in -
note that ( 123 ) implies that the smallest admissible value s in functional ( 123 ) is
therefore to find a soft margin classifier one has to find a vector a that maximizes
under the constraints a > 123 and ( 123 ) .
this problem differs from the problem of constructing an optimal margin classifier only by the additional term with amax in the functional ( 123 ) .
due to this term the solution to the problem of constructing the soft margin classifier is unique and exists for any data set .
the functional ( 123 ) is not quadratic because of the term with amax .
maximizing ( 123 ) subject to the constraints a > 123 and ( 123 ) belongs to the group of so - called convex pro - gramming problems .
therefore , to construct a soft margin classifier one can either solve the convex programming problem in the ^ - dimensional space of the parameters a , or one can solve the quadratic programming problem in the dual t + 123 space of the parameters a and s .
in our experiments we construct the soft margin hyperplanes by solving the dual quadratic programming problem .
the method of convolution of the dot - product in feature space
the algorithms described in the previous sections construct hyperplanes in the input space .
to construct a hyperplane in a feature space one first has to transform the n - dimensional input vector x into an / / - dimensional feature vector through a choice of an n - dimensional vector function 123 :
an n dimensional linear separator w and a bias b is then constructed for the set of
classification of an unknown vector x is done by first transforming the vector to the sepa - rating space ( x i - - 123 ( x ) ) and then taking the sign of the function
according to the properties of the soft margin classifier method the vector w can be
written as a linear combination of support vectors ( in the feature space ) .
that means
the linearity of the dot - product implies , that the classification function / in ( 123 ) for an
unknown vector x only depends on the dot - products :
the idea of constructing support - vector networks comes from considering general forms of the dot - product in a hilbert space ( anderson & bahadur , 123 ) :
according to the hilbert - schmidt theory ( courant & hilbert , 123 ) any symmetric
function k ( u , v ) , with k ( u , v ) e li , can be expanded in the form
where a , e sk and fa are eigenvalues and eigenfunctions
of the integral operator defined by the kernel k ( u , v ) .
a sufficient condition to ensure that ( 123 ) defines a dot - product in a feature space is that all the eigenvalues in the expansion ( 123 ) are positive .
to guarantee that these coefficients are positive , it is necessary and sufficient ( mercer ' s theorem ) that the condition
is satisfied for all g such that
functions that satisfy mercer ' s theorem can therefore be used as dot - products .
aizerman , braverman and rozonoer ( 123 ) consider a convolution of the dot - product in the feature space given by function of the form
which they call potential functions .
however , the convolution of the dot - product in feature space can be given by any function satisfying mercer ' s condition; in particular , to construct a polynomial classifier of degree d in n - dimensional input space one can use the following function
cortes and vapnik
using different dot - products k ( u , v ) one can construct different learning machines with arbitrary types of decision surfaces ( boser , guyon & vapnik , 123 ) .
the decision surface of these machines has a form
where xi is the image of a support vector in input space and ai is the weight of a support vector in the feature space .
to find the vectors xi and weights ai one follows the same solution scheme as for the original optimal margin classifier or soft margin classifier .
the only difference is that instead of matrix d ( determined by ( 123 ) ) one uses the matrix
general features of support - vector networks
constructing the decision rules by support - vector networks is efficient
to construct a support - vector network decision rule one has to solve a quadratic optimization
under the simple constraints :
is determined by the elements of the training set , and k ( u , v ) is the function determining the convolution of the dot - products .
the solution to the optimization problem can be found efficiently by solving intermediate optimization problems determined by the training data , that currently constitute the support vectors .
this technique is described in section 123
the obtained optimal decision function
each optimization problem can be solved using any standard techniques .
the support - vector network is a universal machine
by changing the function k ( u , v ) for the convolution of the dot - product one can implement
in the next section we will consider support - vector network machines that use polynomial decision surfaces .
to specify polynomials of different order d one can use the following functions for convolution of the dot - product
radial basis function machines with decision functions of the form
can be implemented by using convolutions of the type
in this case the support - vector network machine will construct both the centers xi of the approximating function and the weights ai .
one can also incorporate a priori knowledge of the problem at hand by constructing special convolution functions .
support - vector networks are therefore a rather general class of learning machines which changes its set of decision functions simply by changing the form of the dot - product .
support - vector networks and control of generalization ability
to control the generalization ability of a learning machine one has to control two different factors : the error - rate on the training data and the capacity of the learning machine as measured by its vc - dimension ( vapnik , 123 ) .
there exists a bound for the probability of errors on the test set of the following form : with probability 123 r; the inequality
is valid .
in the bound ( 123 ) the confidence interval depends on the vc - dimension of the learning machine , the number of elements in the training set , and the value of n .
the two factors in ( 123 ) form a trade - off : the smaller the vc - dimension of the set of functions of the learning machine , the smaller the confidence interval , but the larger the value of the error frequency .
a general way for resolving this trade - off was proposed as the principle of structural risk minimization : for the given data set one has to find a solution that minimizes their sum .
a particular case of structural risk minimization principle is the occam - razor principle : keep the first term equal to zero and minimize the second one .
it is known that the vc - dimension of the set of linear indicator functions
with fixed threshold b is equal to the dimensionality of the input space .
however , the vc - dimension of the subset
cortes and vapnik
( the set of functions with bounded norm of the weights ) can be less than the dimensionality of the input space and will depend on cw .
from this point of view the optimal margin classifier method executes an occam - razor principle .
it keeps the first term of ( 123 ) equal to zero ( by satisfying the inequality ( 123 ) ) and it minimizes the second term ( by minimizing the functional w w ) .
this minimization prevents an over - fitting problem .
however , even in the case where the training data are separable one may obtain better generalization by minimizing the confidence term in ( 123 ) even further at the expense of errors on the training set .
in the soft margin classifier method this can be done by choosing appropriate values of the parameter c .
in the support - vector network algorithm one can control the trade - off between complexity of decision rule and frequency of error by changing the parameter c , even in the more general case where there exists no solution with zero error on the training set .
therefore the support - vector network can control both factors for generalization ability of the learning machine .
experimental analysis
to demonstrate the support - vector network method we conduct two types of experiments .
we construct artificial sets of patterns in the plane and experiment with 123nd degree poly - nomial decision surfaces , and we conduct experiments with the real - life problem of digit
experiments in the plane
using dot - products of the form
with d = 123 we construct decision rules for different sets of patterns in the plane .
results of these experiments can be visualized and provide nice illustrations of the power of the algorithm .
examples are shown in fig .
the 123 classes are represented by black and white
figure 123
examples of the dot - product ( 123 ) with d = 123
support patterns are indicated with double circles , errors with a cross .
figure 123
examples of patterns with labels from the us postal service digit database .
bullets .
in the figure we indicate support patterns with a double circle , and errors with a cross .
the solutions are optimal in the sense that no 123nd degree polynomials exist that make less errors .
notice that the numbers of support patterns relative to the number of training patterns are small .
experiments with digit recognition
our experiments for constructing support - vector networks make use of two different data - bases for bit - mapped digit recognition , a small and a large database .
the small one is a us postal service database that contains 123 , 123 training patterns and 123 , 123 test patterns .
the resolution of the database is 123 x 123 pixels , and some typical examples are shown in fig .
on this database we report experimental research with polynomials of various degree .
the large database consists of 123 , 123 training and 123 , 123 test patterns , and is a 123 - 123 mixture of the nist123 training and test sets .
the resolution of these patterns is 123 x 123 yielding an input dimensionality of 123
on this database we have only constructed a 123th degree polynomial classifier .
the performance of this classifier is compared to other types of learning machines that took part in a benchmark study ( bottou , 123 ) .
in all our experiments ten separators , one for each class , are constructed .
each hyper - surface makes use of the same dot product and pre - processing of the data .
classification of an unknown patterns is done according to the maximum output of these ten classifiers .
experiments with us postal service database .
the us postal service database has been recorded from actual mail pieces and results from this database have been reported by several researchers .
in table 123 we list the performance of various classifiers collected
