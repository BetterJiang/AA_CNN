we describe an algorithm for support vector machines ( svm ) that can be parallelized efficiently and scales to very large problems with hundreds of thousands of training vectors .
instead of analyzing the whole training set in one optimization step , the data are split into subsets and optimized separately with multiple svms .
the partial results are combined and filtered again in a cascade of svms , until the global optimum is reached .
the cascade svm can be spread over multiple processors with minimal communication overhead and requires far less memory , since the kernel matrices are much smaller than for a regular svm .
convergence to the global optimum is guaranteed with multiple passes through the cascade , but already a single pass provides good generalization .
a single pass is 123x 123x faster than a regular svm for problems of 123 , 123 vectors when implemented on a single processor .
parallel implementations on a cluster of 123 processors were tested with over 123 million vectors ( 123 - class problems ) , converging in a day or two , while a regular svm never converged in over a week .
support vector machines ( 123 ) are powerful classification and regression tools , but their compute and storage requirements increase rapidly with the number of training vectors , putting many problems of practical interest out of their reach .
the core of an svm is a quadratic programming problem ( qp ) , separating support vectors from the rest of the training data .
general - purpose qp solvers tend to scale with the cube of the number of training vectors ( o ( k123 ) ) .
specialized algorithms , typically based on gradient descent methods , achieve impressive gains in efficiency , but still become impractically slow for problem sizes on the order of 123 , 123 training vectors ( 123 - class
one approach for accelerating the qp is based on chunking ( 123 ) ( 123 ) ( 123 ) , where subsets of the training data are optimized iteratively , until the global optimum is reached .
sequential minimal optimization ( smo ) ( 123 ) , which reduces the chunk size to 123 vectors , is the most popular of these algorithms .
eliminating non - support vectors
early during the optimization process is another strategy that provides substantial savings in computation .
efficient svm implementations incorporate steps known as shrinking for identifying non - support vectors early ( 123 ) ( 123 ) ( 123 ) .
in combination with caching of the kernel data , such techniques reduce the computation requirements by orders of magnitude .
another approach , named digesting optimizes subsets closer to completion before adding new data ( 123 ) , saving considerable amounts of storage .
improving compute - speed through parallelization is difficult due to dependencies between the computation steps .
parallelizations have been proposed by splitting the problem into smaller subsets and training a network to assign samples to different subsets ( 123 ) .
variations of the standard svm algorithm , such as the proximal svm have been developed that are better suited for parallelization ( 123 ) , but how widely they are applicable , in particular to high - dimensional problems , remains to be seen .
a parallelization scheme was proposed where the kernel matrix is approximated by a block - diagonal ( 123 ) .
a technique called variable projection method ( 123 ) looks promising for improving the parallelization of the optimization loop .
in order to break through the limits of todays svm implementations we developed a distributed architecture , where smaller optimizations are solved independently and can be spread over multiple processors , yet the ensemble is guaranteed to converge to the globally optimal solution .
123 the cascade svm as mentioned above , eliminating non - support vectors early from the optimization proved to be an effective strategy for accelerating svms .
using this concept we developed a filtering process that can be parallelized efficiently .
after evaluating multiple techniques , such as projections onto subspaces ( in feature space ) or clustering techniques , we opted to use svms as filters .
this makes it straightforward to drive partial solutions towards the global optimum , while alternative techniques may optimize criteria that are not directly relevant for finding the global solution .
td / 123 td / 123
td / 123 td / 123
td / 123 td / 123
td / 123 td / 123
td / 123 td / 123
td / 123 td / 123
td / 123 td / 123
td / 123 td / 123
figure 123 : schematic of a binary cascade architecture .
the data are split into subsets and each one is evaluated individually for support vectors in the first layer .
the results are combined two - by - two and entered as training sets for the next layer .
the resulting support vectors are tested for global convergence by feeding the result of the last layer into the first layer , together with the non - support vectors .
td : training data , svi : support vectors produced by
we initialize the problem with a number of independent , smaller optimizations and combine the partial results in later stages in a hierarchical fashion , as shown in figure 123
splitting the data and combining the results can be done in many different ways .
figure 123 merely represents one possible architecture , a binary cascade that proved to be efficient in many tests .
it is guaranteed to advance the optimization function in every layer , requires only modest communication from one layer to the next , and converges to a good solution quickly .
in the architecture of figure 123 sets of support vectors from two svms are combined and the optimization proceeds by finding the support vectors in each of the combined subsets .
this continues until only one set of vectors is left .
often a single pass through this cascade produces satisfactory accuracy , but if the global optimum has to be reached , the result of the last layer is fed back into the first layer .
each of the svms in the first layer receives all the support vectors of the last layer as inputs and tests its fraction of the input vectors , if any of them have to be incorporated into the optimization .
if this is not the case for all svms of the input layer , the cascade has converged to the global optimum , otherwise it proceeds with another pass through the
in this architecture a single svm never has to deal with the whole training set .
if the filters in the first few layers are efficient in extracting the support vectors then the largest optimization , the one of the last layer , has to handle only a few more vectors than the number of actual support vectors .
therefore , in problems where the support vectors are a small subset of the training vectors - which is usually the case - each of the sub - problems is much smaller than the whole problem ( compare section 123 ) .
123 n o t a t i o n ( 123 - c l a s s , ma xi mu m ma r g i n )
we discuss here the 123 - class classification problem , solved in dual formulation .
the cascade does not depend on details of the optimization algorithm and alternative formulations or regression algorithms map equally well onto this architecture .
the 123 - class problem is the most difficult one to parallelize because there is no natural split into sub - problems .
multi - class problems can always be separated into 123 - class
let us consider a set of l training examples ( xi; yi ) ; where the class label .
k ( xi , xj ) is the matrix of kernel values d - dimensional pattern and between patterns and i the lagrange coefficients to be determined by the optimization .
the svm solution for this problem consists in maximizing the following quadratic optimization function ( dual formulation ) :
of w with respect to is then : the gradient g
123 f o r ma l p r o o f o f c o n v e r g e n c e
the main issue is whether a cascade architecture will actually converge to the global optimum .
the following theorems show that this is the case for a wide range of conditions .
let s denote a subset of the training set , w ( s ) is the optimal objective function over s ( equation 123 ) , and let be the subset of s for which the optimal are non - zero ( support vectors of s ) .
it is obvious that :
let us consider a family f of sets of training examples for which we can independently that achieves the greatest w ( s ) will be compute the svm solution .
the set called the best set in family f .
we will write w ( f ) as a shorthand for w ( s* ) , that is :
we are interested in defining a sequence of families ft such that w ( ft ) converges to the optimum .
two results are relevant for proving convergence .
theorem 123 : let us consider two families f and g of subsets of .
if a set contains the support vectors of the best set , we have theorem 123 : let us consider two families f and g of subsets of .
assume that every
gt contains the support vectors of the best set
consider a vector * solution of the proof : theorem 123 implies that gt , we have svm problem restricted to the support vectors subset of t .
we also have implies that * is also a solution of the svm on set t .
therefore * satisfies all the gt .
this implies that * also satisfies the kkt conditions corresponding to all sets kkt conditions for the union of all sets in g .
definition 123
a cascade is a sequence ( ft ) of families of subsets of satisfying : i ) for all t > 123 , a set ii ) for all t , there is a k > t such that :
tft contains the support vectors of the best set in ft - 123
kft contain the support vectors of the best set in fk - 123
the union of all sets in fk is equal to .
for all
theorem 123 : a cascade ( ft ) converges to the svm solution of in finite proof : assumption i ) of definition 123 plus theorem 123 imply that the sequence w ( ft ) is monotonically increasing .
since this sequence is bounded by w ( ) , it converges to .
the sequence w ( ft ) takes its values in the finite set of the w ( s ) for all observation , assertion ii ) of definition 123 , plus theorem 123 imply that there is a k > l such for all t > k .
therefore there is a l > 123 such that
since w ( ft ) is monotonically increasing ,
= wfw k
as stated in theorem 123 , a layered cascade architecture is guaranteed to converge to the global optimum if we keep the best set of support vectors produced in one layer , and use it in at least one of the subsets in the next layer .
this is the case in the binary cascade shown in figure 123
however , not all layers meet assertion ii ) of definition 123
the union of sets in a layer is not equal to the whole training set , except in the first layer .
by introducing the feedback loop that enters the result of the last layer into the
first one , combined with all non - support vectors , we fulfill all assertions of definition 123
we can test for global convergence in layer 123 and do a fast filtering in the
i n t e r p r e t a t i o n o f t h e s v m f i l t e r i n g p r o c e s s
an intuitive picture of the filtering process is provided in figure 123
if a subset is chosen randomly from the training set , it will most likely not contain all support vectors of and its support vectors may not be support vectors of the whole problem .
however , if there is not a serious bias in a subset , support vectors of s are likely to contain some support vectors of the whole problem .
stated differently , it is plausible that interior points in a subset are going to be interior points in the whole set .
therefore , a non - support vector of a subset has a good chance of being a non - support vector of the whole set and we can eliminate it from further analysis .
figure 123 : a toy problem illustrating the filtering process .
two disjoint subsets are selected from the training data and each of them is optimized individually ( left , center; the data selected for the optimizations are the solid elements ) .
the support vectors in each of the subsets are marked with frames .
they are combined for the final optimization ( right ) , resulting in a classification boundary ( solid curve ) close to the one for the whole problem ( dashed curve ) .
123 distributed optimization
figure 123 : a cascade with two input sets d123 , d123
wi , gi and qi are objective function , gradient , and kernel matrix , respectively , of svmi ( in vector notation ) ; ei is a vector with all 123
gradients of svm123 and svm123 are merged ( extend ) as indicated in ( 123 ) and are entered into svm123
support vectors of svm123 are used to test d123 , d123 for violations of the kkt conditions .
violators are combined with the support vectors for the next iteration .
section 123 shows that a distributed architecture like the cascade indeed converges to the global solution , but no indication is provided how efficient this approach is .
for a good performance we try to advance the optimization as much as possible in each stage .
this depends on how the data are split initially , how partial results are merged and how well an optimization can start from the partial results provided by the previous stage .
we focus on gradient - ascent algorithms here , and discuss how to handle merging efficiently .
123 m e r g i n g s u b s e t s for this discussion we look at a cascade with two layers ( figure 123 ) .
when merging the two results of svm123 and svm123 , we can initialize the optimization of svm123 to different starting points .
in the general case the merged set starts with the following optimization function and gradient :
we consider two possible initializations :
since each of the subsets fulfills the kkt conditions , each of these cases represents a feasible starting point with :
intuitively one would probably assume that case 123 is the preferred one since we start from a point that is optimal in the two spaces defined by the vectors d123 and d123
if q123 is 123 ( q123 is then also 123 since the kernel matrix is symmetric ) , the two spaces are orthogonal ( in feature space ) and the sum of the two solutions is the solution of the whole problem .
therefore , case 123 is indeed the best choice for initialization , because it represents the final solution .
if , on the other hand , the two subsets are identical , then an initialization with case 123 is optimal , since this represents now the solution of the whole problem .
in general , we are probably somewhere between these two cases and therefore it is not obvious , which case is best .
while the theorems of section 123 guarantee the convergence to the global optimum , they do not provide any indication how fast this going to happen .
empirically we find that the cascade converges quickly to the global solution , as is indicated in the examples below .
all the problems we tested converge in 123 to 123 passes .
123 experimental results we implemented the cascade architecture for a single processor as well as for a cluster of processors and tested it extensively with several problems; the largest are : mnist123 , forest123 , norb123 ( all are converted to 123 - class problems ) .
one of the main advantages of the cascade architecture is that it requires far less memory than a single svm , because the size of the kernel matrix scales with the square of the active set .
this effect is illustrated in figure 123
it has to be emphasized that both cases , single svm and cascade , use shrinking , but shrinking alone does not solve the problem of exorbitant sizes of the kernel matrix .
a good indication of the cascades inherent efficiency is obtained by counting the number of kernel evaluations required for one pass .
as shown in table 123 , a 123 - layer cascade requires only about 123% as many kernel evaluations as a single svm for
123 mnist : handwritten digits , d=123 ( 123x123 pixels ) ; training : 123 , 123; testing : 123 , 123; classes : odd digits - even digits; http : / / yann . lecun . com / exdb / mnist .
123 forest : d=123; class 123 versus rest; training : 123 , 123; testing : 123 , 123 123 norb : images , d=123 , 123 ; trainingr=123 , 123; testing=123 , 123; monocular; merged class 123 and 123 versus the rest .
http : / / www . cs . nyu . edu / ~ ylclab / data / norb - v123
123 , 123 training vectors .
how many kernel evaluations actually have to be computed depends on the caching strategy and the memory size .
active set size
number of iterations
figure 123 : the size of the active set as a function of the number of iterations for a problem with 123 , 123 training vectors .
the upper curve represents a single svm , while the lower one shows the active set size for a 123 - layer cascade .
as indicated in table 123 , this parameter , and with it the compute times , are reduced even more .
therefore , even a simulation on a single processor can produce a speed - up of 123x to 123x or more , depending on the available memory size .
for practical purposes often a single pass through the cascade produces sufficient accuracy ( compare figure 123 ) .
this offers a particularly simple way for solving problems of a size that would otherwise be out of reach for svms .
number of layers k - eval request x123
123 123 123 123
table 123 : number of kernel evaluations ( requests and actual , with a cache size of 123mb ) for different numbers of layers in the cascade ( single pass ) .
the number of kernel evaluations is reduced as the number of cascade layers increases .
then , larger amounts of the problems fit in the cache , reducing the actual kernel computations even more .
problem : forest , 123k vectors .
max # training vect .
per machine
table 123 : training times for a large data set with 123 , 123 , 123 vectors ( mnist was expanded by warping the handwritten digits ) .
a cascade with 123 layers is executed on a linux cluster with 123 machines ( amd 123 , dual processors , 123gb ram per machine ) .
the solution converges in 123 iterations .
shown are also the maximum number of training vectors on one machine and the number of support vectors in the last stage .
w : optimization function; acc : accuracy on test set .
kernel : rbf ,
table 123 shows how a problem with over one million vectors is solved in about a day ( single pass ) with a generalization performance equivalent to the fully converged solution .
while the full training set contains over 123m vectors , one processor never handles more than 123k vectors in the optimization and 123k for the convergence test .
the cascade provides several advantages over a single svm because it can reduce compute - as well as storage - requirements .
the main limitation is that the last layer consists of one single optimization and its size has a lower limit given by the number of support vectors .
this is why the acceleration saturates at a relatively small number
of layers .
yet this is not a hard limit since a single optimization can be distributed over multiple processors as well , and we are working on efficient implementations of such
figure 123 : speed - up for a parallel implementation of the cascades with 123 to 123 layers ( 123 to 123 svms , each running on a separate processor ) , relative to a single svm : single pass ( left ) , fully converged ( middle ) ( mnist , norb : 123 iterations , forest : 123 iterations ) .
on the right is the generalization performance of a 123 - layer cascade , measured after each iteration .
for mnist and norb , the accuracy after one pass is the same as after full convergence ( 123 iterations ) .
for forest , the accuracy improves from 123% after a single pass to 123% after convergence ( 123 iterations ) .
training set sizes : mnist : 123k , norb : 123k , forest : 123k .
r e f e r e n c e s
vapnik , statistical learning theory , wiley , new york , 123
boser , i .
guyon , v .
vapnik , a training algorithm for optimal margin classifiers in proc .
123th annual workshop on computational learning theory , pittsburgh , acm , 123
osuna , r .
freund , f .
girosi , training support vector machines , an application to face detection , in computer vision and pattern recognition , pp . 123 - 123 , 123
joachims , making large - scale support vector machine learning practical , in advances in kernel methods , b .
schlkopf , c .
burges , a .
smola , ( eds . ) , cambridge , mit press , 123
( 123 ) j . c .
platt , fast training of support vector machines using sequential minimal optimization , in adv .
in kernel methods : schlkopf , c .
burges , a .
smola ( eds . ) , 123
chang , c .
lin , libsvm , http : / / www . csie . ntu . edu . tw / ~ cjlin / libsvm / .
collobert , s .
bengio , and j .
marithoz .
torch : a modular machine learning software library .
technical report idiap - rr 123 - 123 , idiap , 123
( 123 ) d .
decoste and b .
schlkopf , training invariant support vector machines , machine learning , 123 , 123 - 123 , 123
collobert , y .
bengio , s .
bengio , a parallel mixture of svms for very large scale problems , in neural information processing systems , vol .
123 , mit press , 123
( 123 ) a .
tveit , h .
parallelization of the incremental proximal support vector machine classifier using a heap - based tree topology .
report , idi , ntnu , trondheim , 123
( 123 ) j .
dong , a .
krzyzak , c .
suen , a fast parallel optimization for training support vector machine .
proceedings of 123rd international conference on machine learning and data mining , p .
perner and a .
rosenfeld ( eds . ) springer lecture notes in artificial intelligence ( lnai 123 ) , pp .
123 - - 123 , leipzig , germany , july 123 - 123 , 123
( 123 ) g .
zanghirati , l .
zanni , a parallel solver for large quadratic programs in training support vector machines , parallel computing , vol .
123 , pp . 123 - 123 , 123
