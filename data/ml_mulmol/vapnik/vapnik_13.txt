abstract .
dna micro - arrays now permit scientists to screen thousands of genes simultaneously and determine whether those genes are active , hyperactive or silent in normal or cancerous tissue .
because these new micro - array devices generate bewildering amounts of raw data , new analytical methods must be developed to sort out whether cancer tissues have distinctive signatures of gene expression over normal tissues or other types of cancer tissues .
in this paper , we address the problem of selection of a small subset of genes from broad patterns of gene expression data , recorded on dna micro - arrays .
using available training examples from cancer and normal patients , we build a classier suitable for genetic diagnosis , as well as drug discovery .
previous attempts to address this problem select genes with correlation techniques .
we propose a new method of gene selection utilizing support vector machine methods based on recursive feature elimination ( rfe ) .
we demonstrate experimentally that the genes selected by our techniques yield better classication performance and are biologically relevant to cancer .
in contrast with the baseline method , our method eliminates gene redundancy automatically and yields better and more compact gene subsets .
in patients with leukemia our method discovered 123 genes that yield zero leave - one - out error , while 123 genes are necessary for the baseline method to get the best result ( one leave - one - out error ) .
in the colon cancer database , using only 123 genes our method is 123% accurate , while the baseline method is only
keywords : diagnosis , diagnostic tests , drug discovery , rna expression , genomics , gene selection , dna micro - array , proteomics , cancer classication , feature selection , support vector machines , recursive feature elimination
the advent of dna micro - array technology has brought to data analysts broad patterns of gene expression simultaneously recorded in a single experiment ( fodor , 123 ) .
in the past few months , several data sets have become publicly available on the internet .
these data sets present multiple challenges , including a large number of gene expression values per experiment ( several thousands to tens of thousands ) , and a relatively small number of experiments ( a few dozen ) .
the data can be analyzed from many different viewpoints .
the literature already abounds in studies of gene clusters discovered by unsupervised learning techniques ( see e . g .
eisen ,
guyon et al .
123; perou , 123; alon , 123; alizadeh , 123 ) .
clustering is often done along the other dimension of the data .
for example , each experiment may correspond to one patient carrying or not carrying a specic disease ( see e . g .
golub , 123 ) .
in this case , clustering usually groups patients with similar clinical records .
recently , supervised learning has also been applied , to the classication of proteins ( brown , 123 ) and to cancer classication ( golub ,
this last paper on leukemia classication presents a feasibility study of diagnosis based solely on gene expression monitoring .
in the present paper , we go further in this direction and demonstrate that , by applying state - of - the - art classication algorithms ( support vector machines ( boser , 123; vapnik , 123 ) , a small subset of highly discriminant genes can be extracted to build very reliable cancer classiers .
we make connections with related approaches that were developed independently , which either combine ( furey , 123; pavlidis , 123 ) or integrate ( mukherjee , 123; chapelle , 123; weston , 123 ) feature selection with
the identication of discriminant genes is of fundamental and practical interest .
research in biology and medicine may benet from the examination of the top ranking genes to conrm recent discoveries in cancer research or suggest new avenues to be explored .
medical diagnostic tests that measure the abundance of a given protein in serum may be derived from a small subset of discriminant genes .
this application also illustrates new aspects of the applicability of support vector ma - chines ( svms ) in knowledge discovery and data mining .
svms were already known as a tool that discovers informative patterns ( guyon , 123 ) .
the present application demon - strates that svms are also very effective for discovering informative features or attributes ( such as critically important genes ) .
in a comparison with several other gene selection methods on colon cancer data ( alon , 123 ) we demonstrate that svms have both quantita - tive and qualitative advantages .
our techniques outperform other methods in classication performance for small gene subsets while selecting genes that have plausible relevance to
after formally stating the problem and reviewing prior work ( section 123 ) , we present in section 123 a new method of gene selection using svms .
before turning to the experimental section ( section 123 ) , we describe the data sets under study and provide the basis of our experimental method ( section 123 ) .
particular care is given to evaluate the statistical signi - cance of the results for small sample sizes .
in the discussion section ( section 123 ) , we review computational complexity issues , contrast qualitatively our feature selection method with others , and propose possible extensions of the algorithm .
problem description and prior work
classication problems
in this paper we address classication problems where the input is a vector that we call a pattern of n components which we call features .
we call f the n - dimensional feature space .
in the case of the problem at hand , the features are gene expression coefcients and patterns correspond to patients .
we limit ourselves to two - class classication problems
gene selection for cancer classification
identify the two classes with the symbols ( + ) and ( ) .
a training set of a number of patterns ( x123 , x123 , .
x ( cid : 123 ) ) with known class labels ( y123 , y123 , .
y ( cid : 123 ) ) , yk ( 123 , +123 ) , is given .
the training patterns are used to build a decision function ( or discriminant function ) d ( x ) , that is a scalar function of an input pattern x .
new patterns are classied according to the sign of the decision function : d ( x ) > 123 x class ( + ) d ( x ) < 123 x class ( ) d ( x ) = 123 , decision boundary .
decision functions that are simple weighted sums of the training patterns plus a bias are called linear discriminant functions ( see e . g .
duda , 123 ) .
in our notations :
d ( x ) = w x+ b ,
where w is the weight vector and b is a bias value .
a data set is said to be linearly separable if a linear discriminant function can separate
it without error .
space dimensionality reduction and feature selection
a known problem in classication specically , and machine learning in general , is to nd ways to reduce the dimensionality n of the feature space f to overcome the risk of overtting .
data overtting arises when the number n of features is large ( in our case thousands of genes ) and the number ( cid : 123 ) of training patterns is comparatively small ( in our case a few dozen patients ) .
in such a situation , one can easily nd a decision function that separates the training data ( even a linear decision function ) but will perform poorly on test data .
training techniques that use regularization ( see e . g .
vapnik , 123 ) avoid overtting of the data to some extent without requiring space dimensionality reduction .
such is the case , for instance , of support vector machines ( svms ) ( boser , 123; vapnik , 123; cristianini , 123 ) .
yet , as we shall see from experimental results ( section 123 ) , even svms benet from space dimensionality reduction .
projecting on the rst few principal directions of the data is a method commonly used to reduce feature space dimensionality ( see , e . g .
duda , 123 ) .
with such a method , new features are obtained that are linear combinations of the original features .
one disadvantage of projection methods is that none of the original input features can be discarded .
in this paper we investigate pruning techniques that eliminate some of the original input features and retain a minimum subset of features that yield best classication performance .
pruning techniques lend themselves to the applications that we are interested in .
to build diagnostic tests , it is of practical importance to be able to select a small subset of genes .
the reasons include cost effectiveness and ease of verication of the relevance of selected genes .
the problem of feature selection is well known in machine learning .
for a review of feature selection , see e . g .
( kohavi , 123 ) .
given a particular classication technique , it is conceivable to select the best subset of features satisfying a given model selection criterion
guyon et al .
by exhaustive enumeration of all subsets of features .
for a review of model selection , see e . g .
( kearns , 123 ) .
exhaustive enumeration is impractical for large numbers of features ( in our case thousands of genes ) because of the combinatorial explosion of the number of subsets .
in the discussion section ( section 123 ) , we shall go back to this method that can be used in combination with another method that rst reduces the number of features to a
performing feature selection in large dimensional input spaces therefore involves greedy algorithms .
among various possible methods feature - ranking techniques are particularly attractive .
a xed number of top ranked features may be selected for further analysis or to design a classier .
alternatively , a threshold can be set on the ranking criterion .
only the features whose criterion exceeds the threshold are retained .
in the spirit of structural risk minimization ( see e . g .
vapnik , 123; guyon , 123 ) it is possible to use the ranking to dene nested subsets of features f123 f123 f , and select an optimum subset of features with a model selection criterion by varying a single parameter : the number of features .
in the following , we compare several feature - ranking algorithms .
feature ranking with correlation coefcients
in the test problems under study , it is not possible to achieve an errorless separation with a single gene .
better results are obtained when increasing the number of genes .
classical gene selection methods select the genes that individually classify best the training data .
these methods include correlation methods and expression ratio methods .
they eliminate genes that are useless for discrimination ( noise ) , but they do not yield compact gene sets because genes are redundant .
moreover , complementary genes that individually do not separate well the data are missed .
evaluating how well an individual feature contributes to the separation ( e . g .
cancer vs .
normal ) can produce a simple feature ( gene ) ranking .
various correlation coefcients are used as ranking criteria .
the coefcient used in golub ( 123 ) is dened as :
wi = ( i ( + ) i ( ) ) / ( i ( + ) + i ( ) )
where i and i are the mean and standard deviation of the gene expression values of gene i for all the patients of class ( + ) or class ( ) , i = 123 , .
large positive wi values indicate strong correlation with class ( + ) whereas large negative wi values indicate strong correlation with class ( ) .
the original method of golub ( 123 ) is to select an equal number of genes with positive and with negative correlation coefcient .
others ( furey , 123 ) have been using the absolute value of wi as ranking criterion .
recently , in pavlidis ( 123 ) , the authors have been using a related coefcient ( i ( + ) i ( ) ) 123 / ( i ( + ) 123 + i ( ) 123 ) , which is similar to fishers discriminant criterion ( duda , 123 ) .
what characterizes feature ranking with correlation methods is the implicit orthogonality assumptions that are made .
each coefcient wi is computed with information about a single feature ( gene ) and does not take into account mutual information between features .
in the next section , we explain in more details what such orthogonality assumptions mean .
gene selection for cancer classification
ranking criterion and classication
one possible use of feature ranking is the design of a class predictor ( or classier ) based on a pre - selected subset of features .
each feature that is correlated ( or anti - correlated ) with the separation of interest is by itself such a class predictor , albeit an imperfect one .
this suggests a simple method of classication based on weighted voting : the features vote proportionally to their correlation coefcient .
such is the method being used in golub ( 123 ) .
the weighted voting scheme yields a particular linear discriminant classier :
d ( x ) = w ( x )
where w is dened in eq .
( 123 ) and = ( ( + ) + ( ) ) / 123
it is interesting to relate this classier to fishers linear discriminant .
such a classier is
also of the form of eq .
( 123 ) , with
where s is the ( n , n ) within class scatter matrix dened as
( x ( + ) ) ( x ( + ) ) t +
( x ( ) ) ( x ( ) ) t
and where is the mean vector over all training patterns .
we denote by x ( + ) and x ( ) the training sets of class ( + ) and ( ) .
this particular form of fishers linear discriminant implies that s is invertible .
this is not the case if the number of features n is larger than the number of examples ( cid : 123 ) since then the rank of s is at most ( cid : 123 ) .
the classier of golub ( 123 ) and fishers classier are particularly similar in this formulation if the scatter matrix is approximated by its diagonal elements .
this approximation is exact when the vectors formed by the values of one feature across all training patterns are orthogonal , after subtracting the class mean .
it retains some validity if the features are uncorrelated , that is if the expected value of the product of two different feature is zero , after removing the class mean .
approximating s by its diagonal elements is one way of regularizing it ( making it invertible ) .
but , in practice , features are usually correlated and therefore the diagonal approximation is not valid .
we have just established that the feature ranking coefcients can be used as classier weights .
reciprocally , the weights multiplying the inputs of a given classier can be used as feature ranking coefcients .
the inputs that are weighted by the largest value inuence most the classication decision .
therefore , if the classier performs well , those inputs with the largest weights correspond to the most informative features .
this scheme generalizes the previous one .
in particular , there exist many algorithms to train linear discriminant functions that may provide a better feature ranking than correlation coefcients .
these algorithms include fishers linear discriminant , just mentioned , and svms that are the subject of this paper .
both methods are known in statistics as multivariate classiers , which means that they are optimized during training to handle multiple variables ( or features ) simultaneously .
the method of golub ( 123 ) , in contrast , is a combination of multiple
guyon et al .
feature ranking by sensitivity analysis
in this section , we show that ranking features with the magnitude of the weights of a linear discriminant classier is a principled method .
several authors have suggested to use the change in objective function when one feature is removed as a ranking criterion ( kohavi , 123 ) .
for classication problems , the ideal objective function is the expected value of the error , that is the error rate computed on an innite number of examples .
for the purpose of training , this ideal objective is replaced by a cost function j computed on training examples only .
such a cost function is usually a bound or an approximation of the ideal objective , chosen for convenience and efciency reasons .
hence the idea to compute the change in cost function d j ( i ) caused by removing a given feature or , equivalently , by bringing its weight to zero .
the obd algorithm ( lecun , 123 ) approximates d j ( i ) by expanding j in taylor series to second order .
at the optimum of j , the rst order term can be neglected ,
d j ( i ) = ( 123 / 123 )
the change in weight dwi = wi corresponds to removing feature i .
the authors of the obd algorithm advocate using d j ( i ) instead of the magnitude of the weights as a weight pruning criterion .
for linear discriminant functions whose cost function j is a quadratic function of classier ( duda , 123 ) with cost function j = ( cid : 123 ) wi these two criteria are equivalent .
this is the case for example of the mean - squared - error ( cid : 123 ) w x y ( cid : 123 ) 123 and linear svms ( boser , 123; vapnik , 123; cristianini , 123 ) , which minimize j = ( 123 / 123 ) ||w||123 , under constrains .
this justies the use of ( wi ) 123 as a feature ranking criterion .
recursive feature elimination
a good feature ranking criterion is not necessarily a good feature subset ranking criterion .
the criteria d j ( i ) or ( wi ) 123 estimate the effect of removing one feature at a time on the objective function .
they become very sub - optimal when it comes to removing several features at a time , which is necessary to obtain a small feature subset .
this problem can be overcome by using the following iterative procedure that we call recursive feature
train the classier ( optimize the weights wi with respect to j ) .
compute the ranking criterion for all features ( d j ( i ) or ( wi ) 123 ) .
remove the feature with smallest ranking criterion .
