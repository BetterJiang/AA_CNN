the method of structural risk minimization refers to tuning the capacity of the classifier to the available amount of training data .
this capac ( cid : 123 ) ity is influenced by several factors , including : ( 123 ) properties of the input space , ( 123 ) nature and structure of the classifier , and ( 123 ) learning algorithm .
actions based on these three factors are combined here to control the ca ( cid : 123 ) pacity of linear classifiers and improve generalization on the problem of handwritten digit recognition .
123 risk minimization and capacity
123 empirical risk minimization
a common way of training a given classifier is to adjust the parameters w in the classification function f ( x , w ) to minimize the training error etrain , i . e .
the fre ( cid : 123 ) quency of errors on a set of p training examples .
etrain estimates the expected risk based on the empirical data provided by the p available examples .
the method is thus called empirical risk minimization .
but the classification function f ( x , w* ) which minimizes the empirical risk does not necessarily minimize the generalization error , i . e .
the expected value of the risk over the full distribution of possible inputs and their corresponding outputs .
such generalization error egene cannot in general be computed , but it can be estimated on a separate test set ( ete$t ) .
other ways of
guyon , vapnik , boser , bottou , and solla
estimating egene include the leave - one - out or moving control method ( vap123 ) ( for a review , see ( moo123 ) ) .
123 capacity and guaranteed risk
any family of classification functions ( f ( x , w ) ) can be characterized by its capacity .
the vapnik - chervonenkis dimension ( or vc - dimension ) ( vap123 ) is such a capacity , defined as the maximum number h of training examples which can be learnt without error , for all possible binary labelings .
the vc - dimension is in some cases simply given by the number of free parameters of the classifier , but in most practical cases it is quite difficult to determine it analytically .
the vc - theory provides bounds .
let ( f ( x , w ) ) be a set of classification functions of capacity h .
with probability ( 123 - 123 ) , for a number of training examples p > h , simultaneously for all classification functions f ( x , w ) , the generalization error egene is lower than a guaranteed risk defined by :
eguarant = etrain + ( ( p , h , etrain , 123 ) ,
where ( ( p , h , etrain , 123 ) is proportional to ( 123 = ( h ( ln123p / h+ 123 ) - 123l / p for small etrain , and to fa for etrain close to one ( vap123 , vap123 ) .
for a fixed number of training examples p , the training error decreases monoton ( cid : 123 ) ically as the capacity h increases , while both guaranteed risk and generalization error go through a minimum .
before the minimum , the problem is overdetermined : the capacity is too small for the amount of training data .
beyond the minimum the problem is underdetermined .
the key issue is therefore to match the capacity of the classifier to the amount of training data in order to get best generalization performance .
the method of structural risk minimization ( srm ) ( vap123 , vap123 ) provides a way of achieving this goal .
123 structural risk minimization
let us choose a family of classifiers ( f ( x , w ) ) , and define a structure consisting of nested subsets of elements of the family : s123 c s123 c . . .
c sr c . . . .
by defining such a structure , we ensure that the capacity hr of the subset of classifiers sr is less than hr+l of subset sr+l .
the method of srm amounts to finding the subset sopt for which the classifier f ( x , w* ) which minimizes the empirical risk within such subset yields the best overall generalization performance .
two problems arise in implementing srm : ( i ) how to select sopt ? ( ii ) how to find a good structure ? problem ( i ) arises because we have no direct access to egene .
in our experiments , we will use the minimum of either e te123t or eguarant to select sopt , and show that these two minima are very close .
a good structure reflects the a priori knowledge of the designer , and only few guidelines can be provided from the theory to solve problem ( ii ) .
the designer must find the best compromise between two competing terms : etrain and i .
reducing h causes ( to decrease , but etrain to increase .
a good structure should be such that decreasing the vc - dimension happens at the expense of the smallest possible increase in training error .
we now examine several ways in which such a structure can be built .
structural risk minimization for character recognition
123 principal component analysis , optimal
brain damage , and weight decay
consider three apparently different methods of improving generalization perfor ( cid : 123 ) mance : principal component analysis ( a preprocessing transformation of input space ) ( the123 ) , optimal brain damage ( an architectural modification through weight pruning ) ( lds123 ) , and a regularization method , weight decay ( a modifi ( cid : 123 ) cation of the learning algorithm ) ( vap123 ) .
for the case of a linear classifier , these three approaches are shown here to control the capacity of the learning system through the same underlying mechanism : a reduction of the effective dimension of weight space , based on the curvature properties of the mean squared error ( m se ) cost function used for training .
123 linear classifier and mse training
consider a binary linear classifier f ( x , w ) = ( ) o ( wt x ) , where w t is the transpose of wand the function ( ) o takes two values 123 and 123 indicating to which class x belongs .
the vc - dimension of such classifier is equal to the dimension of input space 123 ( or the number of weights ) : h = dim ( w ) = dim ( x ) = n .
the empirical risk is given by :
etrain = ! l ( yk - ( ) o ( wt xk123 ,
where xk is the kth example , and yk is the corresponding desired output .
the problem of minimizing etrain as a function of w can be approached in different ways ( dh123 ) , but it is often replaced by the problem of minimizing a mean square error ( mse ) cost function , which differs from ( 123 ) in that the nonlinear function ( ) o has been removed .
123 curvature properties of the mse cost function
the three structures that we investigate rely on curvature properties of the m s e cost function .
consider the dependence of mse on one of the parameters wi .
training leads to the optimal value wi for this parameter .
one way of reducing the capacity is to set wi to zero .
for the linear classifier , this reduces the vc ( cid : 123 ) dimension by one : h ' = dim ( w ) - 123 = n - 123
the mse increase resulting from setting wi = 123 is to lowest order proportional to the curvature of the m seat wi .
since the decrease in capacity should be achieved at the smallest possible expense in m s e increase , directions in weight space corresponding to small m s e curvature are good candidates for elimination .
the curvature of the m s e is specified by the hessian matrix h of second derivatives of the m se with respect to the weights .
for a linear classifier , the hessian matrix is given by twice the correlation matrix of the training inputs , h = ( 123 / p ) 123 : ~ =123 xkxkt .
the hessian matrix is symmetric , and can be diagonalized to get rid of cross terms ,
123 we assume , for simplicity , that the first component of vector x is constant and set to
123 , so that the corresponding weight introduces the bias value .
guyon , vapnik , boser , bottou , and saba
to facilitate decisions about the simultaneous elimination of several directions in weight space .
the elements of the hessian matrix after diagonalization are the eigenvalues ai; the corresponding eigenvectors give the principal directions wi of the mse .
in the rotated axis , the increase ilmse due to setting w : = 123 takes a
the quadratic approximation becomes an exact equality for the linear classifier .
principal directions wi corresponding to small eigenvalues ai of h are good candi ( cid : 123 ) dates for elimination .
123 principal component analysis
one common way of reducing the capacity of a classifier is to reduce the dimension of the input space and thereby reduce the number of necessary free parameters ( or weights ) .
principal component analysis ( pca ) is a feature extraction method based on eigenvalue analysis .
input vectors x of dimension n are approximated by a linear combination of m ~ n vectors forming an ortho - normal basis .
the coefficients of this linear combination form a vector x ' of dimension m .
the optimal basis in the least square sense is given by the m eigenvectors corresponding to the m largest eigenvalues of the correlation matrix of the training inputs ( this matrix is 123 / 123 of h ) .
a structure is obtained by ranking the classifiers according to m .
the vc - dimension of the classifier is reduced to : h ' = dim ( x / ) = m .
123 optimal brain damage
for a linear classifier , pruning can be implemented in two different but equivalent ways : ( i ) change input coordinates to a principal axis representation , prune the components corresponding to small eigenvalues according to pca , and then train with the m se cost function; ( ii ) change coordinates to a principal axis represen ( cid : 123 ) tation , train with m s e first , and then prune the weights , to get a weight vector w ' of dimension m < n .
procedure ( i ) can be understood as a preprocessing , whereas procedure ( ii ) involves an a posteriori modification of the structure of the classifier ( network architecture ) .
the two procedures become identical if the weight elimination in ( ii ) is based on a ' smallest eigenvalue ' criterion .
procedure ( ii ) is very reminiscent of optimal brain damage ( obd ) , a weight prun ( cid : 123 ) ing procedure applied after training .
in obd , the best candidates for pruning are those weights which minimize the increase ilm se defined in equation ( 123 ) .
the m weights that are kept do not necessarily correspond to the largest m eigenvalues , due to the extra factor of ( wi* ) 123 in equation ( 123 ) .
in either implementation , the vc - dimension is reduced to h ' = dim ( w / ) = dim ( x / ) = m .
123 weight decay
capacity can also be controlled through an additional term in the cost function , to be minimized simultaneously with al s e .
linear classifiers can be ranked according to the norm iiwll123 = l123=123 wj of the weight vector .
a structure is constructed
structural risk minimization for character recognition
by allowing within the subset sr only those classifiers which satisfy iiwll123 < cr .
the positive bounds cr form an increasing sequence : cl < c123 < ' " < cr < . . .
this sequence can be matched with a monotonically decreasing sequence of positive lagrange multipliers 123 ~ 123 ~ . . .
~ ir > . . .
, such that our training problem stated as the minimization of m s e within a specific set sr is implemented through the minimization of a new cost function : mse + ' rllwil 123 this is equivalent to the weight decay procedure ( wd ) .
in a mechanical analogy , the term , rllwll123 is like the energy of a spring of tension ir which pulls the weights to zero .
as it is easier to pull in the directions of small curvature of the mse , wd pulls the weights to zero predominantly along the principal directions of the hessian matrix h associated with small eigenvalues .
in the principal axis representation , the minimum w - y of the cost function mse + , liwil123 , is a simple function of the minimum wo of the mse in the i - + 123+ limit : wi = w ? ad ( ai + i ) ' the weight w ? is attenuated by a factor ad ( ai + i ) ' weights become negligible for i ~ ai , and remain unchanged for i : ai the effect of this attenuation can be compared to that of weight pruning .
pruning all weights such that ai < i reduces the capacity to :
h ' = l : 123 - y ( ai ) ,
where 123 - y ( u ) = 123 if u > i and 123 - y ( u ) = 123 otherwise .
by analogy , we introduce the weight decay capacity :
h ' = t ai
i=123 ai + i
this expression arises in various theoretical frameworks valid only for broad spectra of eigenvalues .
( moo123 , mck123 ) ' and is
123 smoothing , higher - order units , and
combining several different structures achieves further performance improvements .
the combination of exponential smoothing ( a preprocessing transformation of input space ) and regularization ( a modification of the learning algorithm ) is shown here to improve character recognition .
the generalization ability is dramatically improved by the further introduction of second - order units ( an architectural modification ) .
smoothing is a preprocessing which aims at reducing the effective dimension of input space by degrading the resolution : after smoothing , decimation of the inputs could be performed without further image degradation .
smoothing is achieved here through convolution with an exponential kernel :
lk ll pixel ( i + k , j + i ) exp ( - ~ jk123 + 123 )
lk ll exp ( - fj k123 + 123 )
guyon , vapnik , boser , bottou , and soil a
where ( 123 is the smoothing parameter which determines the structure .
convolution with the chosen kernel is an invertible linear operation .
such prepro ( cid : 123 ) cessing results in no capacity change for a mse - trained linear classifier .
smoothing only modifies the spectrum of eigenvalues and must be combined with an eigenvalue ( cid : 123 ) based regularization procedure such as obd or wd , to obtain performance improve ( cid : 123 ) ment through capacity decrease .
123 higher - order units
higher - order ( or sigma - pi ) units can be substituted for the linear units to get poly ( cid : 123 ) nomial classifiers : f ( x , w ) = 123o ( wte ( x ) ) , where e ( x ) is an m - dimensional vector ( m > n ) with components : x ) , x123 , . . .
, xn , ( xixt ) , ( xix123 ) , .
, ( xnx n ) , , ( x123x123 .
the structure is geared towards increasing the capacity , and is controlled by the or ( cid : 123 ) der of the polynomial : sl contains all the linear terms , s123 linear plus quadratic , etc .
computations are kept tractable with the method proposed in reference ( pog123 ) .
123 experimental results
experiments were performed on the benchmark problem of handwritten digit recog ( cid : 123 ) nition described in reference ( gpp+s123 ) .
the database consists of 123 ( 123 x 123 ) binary pixel images , divided into 123 training examples and 123 test examples .
in figure 123 , we compare the results obtained by pruning inputs or weights with pca and the results obtained with wd .
the overall appearance of the curves is very similar .
in both cases , the capacity ( computed from ( 123 ) and ( 123 ) ) decreases as a function of r , whereas the training error increases .
for the optimum value r* , the capacity is only 123 / 123 of the nominal capacity , computed solely on the basis of the network architecture .
at the price of some error on the training set , the error rate on the test set is only half the error rate obtained with r = 123+ .
the competition between capacity and training error always results in a unique minimum of the guaranteed risk ( 123 ) .
it is remarkable that our experiments show the minimum of eguarant coinciding with the minimum of e te123t any of these two quantities can therefore be used to determine r* .
in principle , another independent test set should be used to get a reliable estimate of egene ( cross - validation ) .
it seems therefore advantageous to determine r* using the minimum of eguarant and use the test set to predict the generalization performance .
using eguarant to determine r* raises the problem of determining the capacity of the system .
the capacity can be measured when analytic computation is not possible .
measurements performed with the method proposed by vapnik , levin , and le cun yield results in good agreement with those obtained using ( 123 ) .
the method yields an effective vcdimension which accounts for the global capacity of the system , including the effects of input data , architecture , and learning algorithm 123
123 schematically , measurements of the effective vc . dimension consist of splitting the
training data into two subsets .
the difference between etrain in these subsets is maxi ( cid : 123 ) mized .
the value of h is extracted from the fit to a theoretical prediction for such maximal
structural risk minimization for character recognition
figure 123 : percent error and capacity h ' as a function of log r ( linear classifier , no smoothing ) : ( a ) weight / input pruning via pea ( r is a threshold ) , ( b ) wd ( r is the decay parameter ) .
the guaranteed risk has been rescaled to fit in the figure .
guyon , vapnik , boser , bottou , and solla
table 123 : eteat for smoothing , wd , and higher - order combined .
ii 123t order i 123nd order i
in table 123 we report results obtained when several structures are combined .
weight decay with ' y = " 123* reduces e te123t by a factor of 123
input space smoothing used in conjunction with wd results in an additional reduction by a factor of 123 .
the best performance is achieved for the highest level of smoothing , ( 123 = 123 , for which the blurring is considerable .
as expected , smoothing has no effect in the absence the use of second - order units provides an additional factor of 123 reduction in ete123t for second order units , the number of weights scales like the square of the number of inputs n 123 = 123
but the capacity ( 123 ) is found to be only 123 , for the optimum values of " i and ( 123
123 conclusions and epilogue
our results indicate that the vc - dimension must measure the global capacity of the system .
it is crucial to incorporate the effects of preprocessing of the input data and modifications of the learning algorithm .
capacities defined solely on the basis of the network architecture give overly pessimistic upper bounds .
the method of srm provides a powerful tool for tuning the capacity .
we have shown that structures acting at different levels ( preprocessing , architecture , learn ( cid : 123 ) ing mechanism ) can produce similar effects .
we have then combined three different structures to improve generalization .
these structures have interesting comple ( cid : 123 ) mentary properties .
the introduction of higher - order units increases the capacity .
smoothing and weight decay act in conjunction to decrease it .
elaborate neural networks for character recognition ( lbd+123 , gal +123 ) also incor ( cid : 123 ) porate similar complementary structures .
in multilayer sigmoid - unit networks , the capacity is increased through additional hidden units .
feature extracting neurons introduce smoothing , and regularization follows from prematurely stopping training before reaching the m s e minimum .
when initial weights are chosen to be small , this stopping technique produces effects similar to those of weight decay .
structural risk minimization for character recognition
we wish to thank l .
jackel ' s group at bell labs for useful discussions , and are particularly grateful to e .
levin and y .
le cun for communicating to us the un ( cid : 123 ) published method of computing the effective vc - dimension .
