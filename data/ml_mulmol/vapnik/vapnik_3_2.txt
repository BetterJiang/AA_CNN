we introduce an algorithm for estimating the values of a function at a set of test points x m given a set of training points ( x123 , y123 ) , .
, ( x , y ) without estimating ( as an intermediate step ) the regression function .
we demonstrate that this direct ( trans - ductive ) way for estimating values of the regression ( or classica - tion in pattern recognition ) is more accurate than the traditional one based on two steps , rst estimating the function and then cal - culating the values of this function at the points of interest .
following ( 123 ) we consider a general scheme of transductive inference .
find an algo - rithm a that using both the given set of training data
and the given set of test data
( ( x123 , y123 ) , .
( x , y ) )
selects from a set of functions ( x 123 f ( x ) ) a function
y = f ( x ) = fa ( x|x123 , y123 , .
, x , y , x
and provides the minimum to the functional
r ( a ) = e m
i |x123 , y123 , .
, x , y , x
in ( 123 ) expectation is taken over ( x123 , y123 ) , .
, ( x , y ) , ( x m ) and the pairs ( xi , yi ) and ( x i ) are both drawn independently according to the same xed but unknown distribution f ( x , y ) .
for the training data we are given vector x and the value y , for the test data we are only given x .
usually , the problem of estimating values of a function at points of interest is solved in two steps : rst in a given set of functions f ( x , ) , one estimates the regression , i . e the function which minimizes the functional
r ( ) =z ( ( y f ( x , ) ) 123df ( x , y ) ,
( the inductive step ) and then using the estimated function y = f ( x , ) we calculate the values at points of interest
i = f ( x
( the deductive step ) .
note , however , that the estimation of a function is equivalent to estimating its val - ues in the continuum points of the domain of the function .
therefore , by solving the regression problem using a restricted amount of information , we are looking for a more general solution than is required .
in ( 123 ) it is shown that using a di - rect estimation method one can obtain better bounds than through the two step
in this article we develop the idea introduced in ( 123 ) for estimating the values of a function only at the given points .
the material is organized as follows .
in section 123 we consider the classical ( induc - tive ) ridge regression procedure , and the leaveoneout technique which is used to measure the quality of its solutions .
section 123 introduces the transductive method of inference for estimation of the values of a function based on this leaveoneout in section 123 experiments which demonstrate the improvement given by transductive inference compared to inductive inference ( in both regression and pattern recognition ) are presented .
finally , section 123 summarizes the results .
123 ridge regression and the leaveoneout procedure
in order to describe our transductive method , let us rst discuss the classical two - step ( inductive plus deductive ) procedure of ridge regression .
consider the set of functions linear in their parameters
f ( x , ) =
to minimize the expected loss ( 123 ) , where f ( x , y ) is unknown , we minimize the following empirical functional ( the socalled ridge regression functional ( 123 ) )
( yi f ( xi , ) ) 123 + ||||123
where is a xed positive constant , called the regularization parameter .
the min - imum is given by the vector of coecients
= ( x123 , y123 , .
, x , y ) = ( k t k + i ) 123k t y
y = ( y123 , .
, y ) t ,
and k is a matrix with elements :
kij = j ( xi ) ,
i = 123 ,
j = 123 ,
the problem is to choose the value which provides small expected loss for training on a sample s = ( ( x123 , y123 ) , .
, ( x , y ) ) .
for this purpose , we would like to choose such that f minimizing ( 123 ) also mini -
r =z ( y f ( x|s ) ) 123df ( x , y ) df s .
since f ( x , y ) is unknown one cannot estimate this minimum directly .
to solve this problem we instead use the leaveoneout procedure , which is an unbiased estimator of ( 123 ) .
the leaveoneout error of an algorithm on the training sample s is
( yi f ( xi|s \ ( xi , yi ) ) 123 .
the leaveoneout procedure consists of removing from the training data one el - ement ( say ( xi , yi ) ) , constructing the regression function only on the basis of the remaining training data and then testing the removed element .
in this fashion one tests all elements of the training data using dierent decision rules .
the mini - mum over of ( 123 ) we consider as the minimum over of ( 123 ) since the expectation of ( 123 ) coincides with ( 123 ) ( 123 ) .
for ridge regression , one can derive a closed form expression for the leaveoneout
= ( k t k + i ) 123
the error incurred by the leaveoneout procedure is ( 123 )
xi=123 yi kt
kt = ( 123 ( xt ) .
, n ( xt ) ) t .
let = 123 be the minimum of ( 123 ) .
then the vector
y 123 = k ( k t k + 123i ) 123k t y
is the ridge regression estimate of the unknown values ( y
123 leaveoneout error for transductive inference
in transductive inference , our goal is to nd an algorithm a which minimizes the functional ( 123 ) using both the training data ( 123 ) and the test data ( 123 ) .
we suggest the following method : predict ( y m ) by nding those values which minimize the leaveoneout error of ridge regression training on the joint set
( x123 , y123 ) , .
, ( x , y ) , ( x
this is achieved in the following way .
suppose we treat the unknown values m ) as variables and for some xed value of these variables we minimize the following empirical functional
( yi f ( xi , ) ) 123 +
i f ( x
i , ) ) 123 ! + ||||123
this functional diers only in the second term from the functional ( 123 ) and corre - sponds to performing ridge regression with the extra pairs ( x
suppose that vector y = ( y
m ) is taken from some set y y such that
can be considered as a sample drawn from the distribution f ( x , y ) .
in this case the leaveoneout error of minimizing ( 123 ) over the set ( 123 ) approximates the functional ( 123 ) .
we can measure this leaveoneout error using the same technique as in ridge regression .
using the closed form ( 123 ) one obtains
xi=123 yi kt
where we denote x = ( x123 , .
, x , x
m ) , y = ( y123 , .
, y , y
m ) t , and
= ( k t k + i ) 123
kij = j ( xi ) ,
i = 123 , .
, + m ,
j = 123 ,
kt = ( 123 ( xt ) .
, n ( xt ) ) t .
now let us rewrite the expression ( 123 ) in an equivalent form to separate the terms with y from the terms with x .
introducing
and the matrix m with elements
c = i ka123
we obtain the equivalent expression of ( 123 )
( y t m y ) .
in order for the y which minimize the leaveoneout procedure to be valid it is required that the pairs ( x , y ) are drawn according to the distribution f ( x , y ) .
to satisfy this constraint we choose vectors y from the set
y = ( y : ||y y 123|| r )
where the vector y 123 is the solution obtained from classical ridge regression .
to minimize ( 123 ) under constraint ( 123 ) we use the functional
loo ( ) = y t m y + ||y y 123||123
where is a constant depending on r .
now , to nd the values at the given points of interest ( 123 ) all that remains is to nd the minimum of ( 123 ) in y .
note that the matrix m is obtained using only the vectors x and x .
therefore , to nd the minimum of this functional we rewrite equation ( 123 ) as
loo ( ) = y t m123y + 123y t m123y + y t m123y + ||y y 123||123
123 m123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 )
and m123 is a matrix , m123 is a m matrix and m123 is a m m matrix .
taking the derivative of ( 123 ) in y we obtain the condition for the solution
123m123y + 123m123y 123y 123 + 123y = 123
which gives the predictions
y = ( i + m123 ) 123 ( m123y + y 123 ) .
in this algorithm ( which we will call transductive regression ) we have two param - eters to control : and .
the choice of can be found using the leave - one - out estimator ( 123 ) for ridge regression .
this leaves as the only free parameter , the choice of which can be made according to the variance of the predictions y 123
to compare our onestep transductive approach with the classical twostep ap - proach we performed a series of experiments on regression problems .
we also de - scribe experiments applying our technique to the problem of pattern recognition .
we conducted computer simulations for the regression problem using two datasets from the delve repository : boston and kin - 123fh .
the boston dataset is a wellknown problem where one is required to estimate house prices according to various statistics based on 123 locational , economic and structural features from data collected by the u . s census service in the boston
the kin - 123fh dataset is a realistic simulation of the forward dynamics of an 123 link all - revolute robot arm .
the task is to predict the distance of the end - eector from a target , given 123 inputs which contain information on the joint positions , twist angles and so forth .
both problems are nonlinear and contain noisy data .
our objective is to com - pare our transductive inference method directly with the inductive method of ridge regression .
to do this we chose the set of basis functions i ( x ) =
regression which minimized the leaveoneout bound ( 123 ) .
we then used the same values of these parameters in our transductive approach , and using the basis func -
exp ( cid : 123 ) ||x xi||123 / 123 ( cid : 123 ) , i = 123 , .
, , and found the values of and for ridge tions i ( x ) = exp ( cid : 123 ) ||x xi||123 / 123 ( cid : 123 ) , i = 123 , .
, + m , we then chose a xed value
for the boston dataset we followed the same experimental setup as in ( 123 ) , that is , we partitioned the training set of 123 observations randomly 123 times into a training set of 123 observations and a testing set of 123 observations .
we chose the values of and by taking the minimum average leaveoneout error over ve more random splits of the data stepping over the parameter space .
the minimum was found at = 123 and log = 123 .
for our transductive method , we also chose the parameter = 123
in figure 123a we plot mean squared error ( mse ) on the test set averaged over the 123 runs against log for ridge regression and transductive regression .
transductive regression outperforms ridge regression , especially at
to observe the inuence of the number of test points m on the generalization ability of our transductive method , we ran further experiments , setting = / 123m for
test set size
test set size
figure 123 : a comparison of transductive regression to ridge regression on the boston dataset : ( a ) error rates for varying , ( b ) varying the test set size , m , and on the kin - 123fh dataset : ( c ) error rates for varying , ( d ) varying the test set size .
dierent values of m .
in figure 123b we plot m against mse on the testing set , at log = 123 .
the results clearly indicate that increasing the test set size gives improved performance in transductive regression .
for ridge regression , of course , the size of the testing set has no inuence on the generalization ability .
we then performed similar experiments on the kin - 123fh dataset .
this time , as we were interested in large testing sets giving improved performance for transductive regression we chose 123 splits where we took a subset of only 123 observations for training and 123 for testing .
again the leaveoneout estimator was used to nd the values = 123 and log = 123 for ridge regression , and for transductive regression we also chose the parameter = 123 .
we plotted mse on the testing set against log ( figure 123c ) and the size of the test set m for log = 123 ( also , = 123 / m ) ( figure 123d ) for the two algorithms .
for large test set sizes our method clearly outperforms ridge regression .
123 pattern recognition
this technique can also be applied for pattern recognition problems by solving them based on minimizing functional ( 123 ) with y = 123
such a technique is known as a
table 123 : comparison of percentage test error of adaboost ( ab ) , regularized ad - aboost ( abr ) , support vector machines ( svm ) and transductive linear discrim - ination ( tld ) on seven datasets .
linear discriminant ( ld ) technique .
table 123 describes results of experiments on classication in the following problems : 123 class digit recognition ( 123 123 versus 123 123 ) splitting the training set into 123 runs of 123 observations and considering a testing set of 123 observations , and six problems from the uci database .
we followed the same experimental setup as in ( 123 ) : the performance of a classier is measured by its average error over one hundred partitions of the datasets into training and testing sets .
free parameter ( s ) are chosen via validation on the rst ve training datasets .
the performance of the transductive ld technique was compared to support vector machines , adaboost and regularized adaboost ( 123 ) .
it is interesting to note that in spite of the fact that ld technique is one of the sim - plest pattern recognition techniques , transductive inference based upon this method performs well compared to state of the art methods of pattern recognition .
in this article we performed transductive inference in the problem of estimating values of functions at the points of interest .
we demonstrate that estimating the unknown values via a onestep ( transductive ) procedure is more accurate than the traditional twostep ( inductive plus deductive ) one .
