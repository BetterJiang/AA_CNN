abstract .
model selection is an important ingredient of many machine learning algorithms , in particular when the sample size in small , in order to strike the right trade - off between overtting and undertting .
previous classical results for linear regression are based on an asymptotic analysis .
we present a new penalization method for performing model selection for regression that is appropriate even for small samples .
our penalization is based on an accurate estimator of the ratio of the expected training error and the expected generalization error , in terms of the expected eigenvalues of the input covariance matrix .
keywords : model selection , parametric regression , uniform convergence bounds
consider the problem of estimating a regression function in the set of functions
f ( x , ) =
k k ( x )
where ( k ) form a basis of l123 ( r p ) , e . g .
a fourier or wavelet basis .
given a collection of data ( x123 , y123 ) , .
, ( xn , yn ) , where yi = f ( xi , 123 ) + i and xi , i are independently generated by unknown distributions p ( x ) and p ( ) , one wants to nd the function f ( x , ) that provides the smallest value of the expected loss
l ( y , f ( x , ) ) dp ( x ) dp ( )
where l ( y , f ( x , ) ) is a given loss function , usually the quadratic loss l ( y , f ( x , ) ) = ( y f ( x , ) ) 123
to minimize the expected risk ( 123 ) , one minimizes the empirical risk
chapelle , v .
vapnik , and y .
bengio
remp ( ) = 123
l ( yi , f ( xi , ) )
however since the set ( 123 ) has an innite expansion , this idea does not work : for any nite number of ( different ) examples there are functions which have zero empirical risk and a large value of the expected loss .
to guarantee a small expected risk , one can minimize the empirical functional over only the rst d = d ( n ) functions k ( x ) .
this is reasonable if the k are ordered in such way that puts the smoother components rst , introducing a preference for smooth functions .
the problem of choosing an appropriate value d = d ( n ) is called model selection .
for the case of quadratic loss and a large number of observations , several penalty - based methods were proposed in the mid - 123s , and these are asymptotically optimal .
all of these solutions , described in more detail below , minimize functionals of the form
( fd ) = remp ( fd ) t ( d , n )
where n is the sample size , remp ( fd ) is the minimum of the empirical risk when training with a model of size d ( achieved by the function fd ) , and t ( d , n ) is a correction factor for
performing model selection .
in particular akaike ( akaike , 123 ) dened in the context of autoregressive models the
future prediction error ( fpe ) correction factor
t ( d , n ) = ( 123 + d / n ) ( 123 d / n ) 123 ,
for small ratios d / n this multiplicative factor has a linear approximation ( 123 + 123 d generalized cross - validation ( wahba , golub , & heath , 123 ) and shibatas model selector ( shibata , 123 ) have the same linear approximation .
some other criteria which provide a different asymptotic behavior have been proposed including ric ( foster & george , 123 ) bic ( schwartz , 123 ) as well as criteria derived from the minimum description length ( mdl ) principle ( rissanen , 123; barron , rissanen , & yu , 123 ) .
during the same years , a general theory of minimizing the empirical risk ( for any set of functions , any loss functions , and any number of samples ) has been constructed ( vapnik , 123 ) .
in the framework of this theory , the method of structural risk minimization for model selection was proposed .
in the case studied here , this yields the following multiplicative factor ( cherkassky , mulier , & vapnik , 123 ) , derived from uniform convergence bounds
t ( d , n ) =
d ( ln n / d + 123 ) ln
where u+ = max ( 123 , u ) and c , are some constants .
in spite of the fact that in the asymptotic case , this factor is less accurate than classical ones , simulation experiments showed that this correction factor outperforms other classical ones ( cherkassky , mulier , & vapnik , 123 ) .
model selection for small sample regression
this article is the development of an idea described in vapnik ( 123 ) .
we rst show that the expectation of the loss of the function minimizing the empirical risk depends both on the ratio d / n and the eigenvalues of a covariance matrix .
it appears that by taking into account those eigenvalues we obtain a correction factor t ( d , n ) which for small d / n coincides with akaikes factor , but which is signicantly different for larger d / n .
this analysis aims at characterizing the relation between empirical risk and bias ( residual of the approximation and noise ) on one hand , and between bias and generalization error on the other hand .
for this purpose we made an independence assumption which might not be satised in practice .
however , in our experiments the obtained estimator has a very good accuracy which suggests that this assumption is reasonable .
in the last section of the article , we compare the estimation accuracy of our method with
classical ones and show that one can use it to perform state - of - the - art model selection .
risk of the mean square error estimator
we consider a linear model of dimension d ,
i i ( x )
with i r and the family ( i ( x ) ) in is orthonormal with respect to the probability measure ( x ) , which means e p ( x ) q ( x ) = pq . 123 we assume without any loss of generality that this family is also a basis of l123 ( r p ) ( if it is not , it is always possible to extend it ) .
let fd be the function minimizing the empirical mean square error over the set of functions fd , i . e .
fd = arg min
remp ( f ) ,
the following section gives an estimator of the risk of fd .
this risk estimator will lead
directly to the choice of the correcting term in the model selection problem .
we suppose without loss of generality that the rst function 123 is the constant function
123 and then by orthonormality we have for all p > 123 ,
e p ( x ) = 123
derivation of the risk estimator in the orthonormal basis ( i ( x ) ) in , the desired regression function can be written as
f ( x ) =
i i ( x )
chapelle , v .
vapnik , and y .
bengio
and the regression function minimizing the empirical risk is
let the i . i . d .
noise have variance 123 and mean zero , then the risk of this function is
i i ( x )
fd ( x ) = d ( cid : 123 )
r ( fd ) =
= 123 +
( f ( x ) + fd ( x ) ) 123 d ( x ) dp ( ) = 123 + d ( cid : 123 )
( f ( x ) fd ( x ) ) 123 d ( x ) ( i i ) 123 +
the last equality comes from the orthonormality of the family ( i ) in .
the rst term 123 corresponds to the risk of the true regression function , r ( f ) .
the second
term is the estimation error and the third term is the approximation error that we call rd ,
to analyze eq .
( 123 ) , let us introduce the vector i = i i of estimation errors and express the empirical risk in function of ,
( p + p ) p ( xi )
yi p ( xi ) + d ( cid : 123 )
remp ( ) = 123
p ( xi ) q ( xi ) ,
yi = i +
p p ( xi ) .
if we introduce the n d matrix , with i , p = p ( xi ) , then the empirical risk is
= ( t ) 123t y ,
model selection for small sample regression
where y = ( y123 , .
, yn ) t and the minimum value of the empirical risk is
remp ( fd ) = 123
y t ( i ( t ) 123t ) y .
the svd decomposition of the matrix writes = usv t , where u and v are or - thogonal matrices of size n n and d d respectively .
s is a n d diagonal matrix .
( t ) 123t = uidu t ,
with s ( st s ) 123st = id being a diagonal n n matrix with its rst d diagonal elements equal to 123 and the others zero .
thus eq .
( 123 ) writes
remp ( fd ) = 123
y t u ( in id ) u t y
let us now make the assumption that y and are statistically independent .
this as - sumption will be discussed at the end of the section .
then y and u are independent and e yiui p = e yi eui p = 123 from ( 123 ) .
from eq .
( 123 ) , we conclude
eremp ( fd ) = 123
( rd + 123 )
the second equality is derived using the independence of i and xi , the orthonormality of the bases ( yielding e yi 123 = ( rd + 123 ) ) , and orthogonality of the matrix u ( yielding
i=123 eu 123 in eq .
( 123 ) we have to estimate ( cid : 123 ) ( cid : 123 ) 123 = y t ( t ) 123t y .
( p p ) 123 = ( cid : 123 )
to do this , let us write
and denote by ( 123 , .
, d ) the eigenvalues of the covariance matrix c = 123
c pq = 123
p ( xi ) q ( xi ) .
chapelle , v .
vapnik , and y .
bengio
then one can show using the same technique as above that
i=123 e ( 123 / i )
( rd + 123 )
finally combining this last equality with eqs .
( 123 ) and ( 123 ) , we obtain
er ( fd ) = eremp ( fd )
we have made the assumption that y and are independent .
actually , the matrix depends only on the rst d functions in the basis and y depends only on the functions beyond d and on the noise .
thus and y are orthogonal but might not be statistically independent .
however in practice this assumption seems to be reasonable ( see gure 123 ) .
also when the residual is small compared to the noise , yi i , and the independence of i and motivates this assumption .
note that the assumption that there is no residual was also made in the derivation of the akaike information criterion ( akaike , 123 ) .
finally ,
figure 123
comparison of the ratio ( in log scale ) of the median of the generalization error and training error ( over 123 trials ) with the penalty term ( 123 ) and with akaikes penalty .
the latter is only accurate when d / n is small .
the number of training examples n is 123 , the target function is the step function , the noise level is 123 , the training points are uniformly generated in ( , ) and the empirical risk minimization has been carried out in the fourier
model selection for small sample regression
the assumption would also be valid if i ( x ) is independent of j ( x ) ( e . g .
representing independent components of the vector x ) .
we computed the ratio of the expected generalization error and the expected empirical error .
however , in practice , one would like to estimate the actual generalization error in function of the actual empirical error .
to do this , in the previous derivation , one should replace equalities of the type
by statements of the following type : with high probability ,
123 = rd + 123
123 ( rd + 123 )
this kind of statement can be done if we have assumptions on the probability distribution of yi and would lead to risk bounds for the model selection strategy , as shown in bartlett , boucheron , and lugosi ( 123 ) .
this derivation is based on the assumption that the set of basis functions is orthonormal with respect to the probability measure ( x ) .
however in the learning problem this probability distribution is usually unknown and therefore it is impossible to get an explicit orthonormal basis .
nevertheless , for any given independent set of basis functions ( i ( x ) ) and any probability distribution , using gram - schmidt orthonormalization , one can theoretically get a unique orthonormal family ( i ( x ) ) that describes the same set of from the previous argument , one can still use ( 123 ) for a non orthonormal family , keeping in mind however that the eigenvalues appearing in this estimator are the ones corresponding to the covariance matrix constructed from the gram - schmidt orthonor -
in practice this orthogonalization can be made using unlabeled data ( more details are
provided in the next section ) .
application to model selection
as the goal in model selection is to choose the model with the smallest expected risk , the previous analysis ( see eq .
( 123 ) ) suggests to take the correcting term t ( d , n ) as
t ( d , n ) =
where i is the i - th eigenvalue of the covariance matrix ( 123 ) .
note that in the asymptotic case , since the covariance matrix is almost the identity ma - trix ( from the orthonormality assumption ) , e ( 123 / i ) 123 and we obtain akaikes term ( 123 ) .
chapelle , v .
vapnik , and y .
bengio
however , in the non - asymptotic case the covariance matrix is not well - conditioned and it can happen that e ( 123 / i ) ( cid : 123 ) 123 ( see gure 123 ) .
direct eigenvalue estimator method ( dee ) .
in the case when along with training data , unlabeled data are available ( x without y ) , one can compute two covariance matrices : one from unlabeled data c and another from the training data cemp .
there is a unique matrix p ( horn & johnson , 123; corollary 123 . 123 ) such that
p t cp = i and p t cemp p = ,
where is a diagonal matrix with diagonal elements 123 , .
to perform model selec - i=123 123 / i with its empirical tion , we used the correcting term ( 123 ) where we replace e
123 / i = trace
( p t ) 123 p t c p
this enables us to deal with a non orthonormal family .
as before the quantity trace ( c is an indicator of the discrepancy between the empirical covariance matrix cemp and its ex - pected value c .
smallest eigenvalue bound ( seb ) .
to estimate e use a lower bound on the smallest eigenvalue of the covariance matrix .
lemma 123
with probability at least 123
123 / i appearing in eq .
( 123 ) , one can
vd d ( n ) ,
min > 123
i i ( x )
and d ( n ) = d
vd = sup
the proof is in appendix .
in practice , we take = 123 and vd = 123 and we get the following bound ,
er ( fd ) eremp ( fd )
( cid : 123 ) + 123
model selection for small sample regression
remark : expected risk minimization and model selection .
in section 123 , we derived an unbiased estimator of the risk of the function minimizing the mean square error on a linear model of dimension d .
the model selection procedure we proposed is to choose the model minimizing this unbiased estimator .
however a more detailed analysis should be carried out .
indeed , if the variance of our estimator is large and the number of models tested is also large , then some overtting problems might occur .
to avoid this , one needs to increase the penalty in order to capture the variance of the risk estimator and the number of models .
a related explanation can also be found in remark 123
we do not consider here the case where of lot of models are available , but just the standard case of nested regression ( in which the number of models is less than the number of training points ) and choosing the model which minimizes an unbiased estimator of the test error should give good results .
as explained before , the case of non - nested regression ( choice of wavelet coefcients
for example ) needs some additional analysis and is left for future work .
experimental results
we performed toy experiments in order to compare model selection algorithms .
the input distribution is the uniform distribution on ( , ) and the set of basis functions is the
123 ( x ) = 123
123 p+123 =
123 cos ( px )
123 sin ( px )
we compared our model selection methods , seb ( smallest empirical bound ) , and dee ( direct eigenvalue estimator ) , to eight other methods .
six of them are penalty - based : fpe ( akaike , eq .
( 123 ) ) , uniform convergence bound ( ucb ) ( 123 ) , gcv ( wahba , golub , & heath , 123 ) , ric ( foster & george , 123 ) , bic ( schwartz , 123 ) , mallows c p ( cpm ) ( mallows , 123 ) .
for the ucb method , we took c = 123 and ln = 123 in eq
the two other model selection algorithms we considered are adj ( a state - of - the - art
heuristic method ( schuurmans , 123 ) ) , and cv123 ( 123 - fold cross - validation ) .
note that both adj and dee need some information about the distribution of input data ( x ) which can be provided by unlabeled data .
in the experiments we used 123 unlabeled
we rst compared the accuracy of some of these methods in the prediction of the gener -
alization error .
for this purpose , we considered the regression function
a gaussian noise with standard deviation = 123 and a training set of 123 examples .
for each d 123 , we computed the empirical risk minimizer fd and tried to predict the generalization error r ( fd ) .
the results are shown in gure 123 and are averaged over 123 trials .
f ( x ) = 123
chapelle , v .
vapnik , and y .
bengio
figure 123
prediction of the generalization error for the following methods : dee , cv123 ( left ) , fpe , adj ( right ) .
both dee and adj predict accurately the test error , adj being a little bit over pessimistic .
when the number of dimension becomes large , fpe underestimates the generalization error while cv123 overestimates ( this is explained by the fact that during cross - validation a smaller training set is used ) .
for the model selection itself we are interested in the generalization error of the function chosen by the model selection procedure .
indeed , as explained at the end of section 123 , an unbiased estimator of the generalization error with a large variance might give a poor criterion for model selection .
different experiments have been carried out by changing the variance of the gaussian noise , the number of training points or the target function .
for each model selection proce - dure , if the model d is chosen , we compute the log of the approximation ratio ,
r ( f d mind r ( fd )
the results are shown in boxplot style in gures 123 and 123 , each one corresponding to a different target function : sinc ( sin ( 123x ) / 123x ) and step ( 123x>123 ) functions .
all the experiments have been repeated 123 times .
the plots for the sinc function ( gure 123 ) show that the model selection procedures have a similar performance when the function is easy to estimate ( the fourier coefcients of this function decrease very rapidly ) .
only fpe is far from the optimal solution for 123 training
the second example is the step function ( gure 123 ) , which is difcult to approximate in the fourier basis .
in this case , traditional penalty based method ( ric , bic , cpm , gcv , fpe ) fail whereas dee , seb , ucb , adj and cv123 are able to select a good model .
for each experiment , tables 123 and 123 indicate the median and the mean ( over 123 trials )
of the approximation ratio ( 123 ) .
judging from these experiments , both proposed methods dee and seb perform as well as the state - of - the - art methods , such as the adj heuristic or cross - validation and ucb ,
model selection for small sample regression
figure 123
approximation ratios for the sinc function .
numerical results can be found in tables 123 and 123 , each letter corresponding to the same experiment .
chapelle , v .
vapnik , and y .
bengio
figure 123
approximation ratios for the step function .
numerical results can be found in tables 123 and 123 , each letter corresponding to the same experiment .
model selection for small sample regression
table 123
median of the ratio of the test error to the best model for the 123 experiments reported in gures 123 and 123
the last row is an average over the 123 experiments .
table 123
mean of the ratio of the test error to the best model for the 123 experiments reported in gures 123 and 123
the last row is an average over the 123 experiments .
while classical penalty - based methods fail .
it is worth noting that the adj heuristic seems to be the best model selection procedure among all the ones we tested .
the comparison between table 123 ( median of the approximation ratio ) and table 123 ( mean of the approximation ) gives a better insight of the behavior of some model selection algo - rithms .
for example , ucb has a median of 123 , but a mean of 123 .
this is due to the fact
chapelle , v .
vapnik , and y .
bengio
that sometimes it selects a very large model ( i . e .
it overts ) incurring a catastrophic gen - eralization error .
the same explanation applies obviously to other penalty - based methods ( which have terrible approximation ratios in mean ) and to a certain extent to cv123 ( see row c of table 123 ) and seb ( see row j ) .
intuitively , cross - validation gives an almost unbiased estimator of the generalization error , but because of its variance , it might select sometimes a model which is far from the optimal one .
this is also true for seb and dee , even though we expect these methods to have a smaller variance .
a discussion on this topic can be found at the end of section 123
in this article we showed that to select models using small sample size the formulas obtained for asymptotic classical models are insufcient .
in our analysis , we pointed out that the discrepancy between the empirical covariance matrix and its expectation is critical for small sample size regression .
taking this discrepancy into account , we obtain a model selection algorithm which behaves similarly to the state - of - the - art .
further research includes improvement of the seb method thanks to a deeper analysis of the distribution of the eigenvalues of a covariance matrix .
the dee method is very attractive since it provides an unbiased estimator of the generalization error of a given model .
unfortunately it requires unlabeled data .
if such data is not available , we believe this method will still be efcient by generating unlabeled data from a parzen window estimator of the input density .
new experiments will be carried out to assess this supposition .
from a theoretical point of view , we will focus on the remark at the end of section 123 and try to extend this method for non - nested regression .
a typical application of this in machine learning would be to determine the number of centers in a rbf network .
proof of lemma 123 : consider the quantity
q ( x , ) =
for all such that ( cid : 123 ) ( cid : 123 ) = 123 , we have eq ( x , ) = 123
on the other hand ,
where c = t / n is the covariance matrix and then
q ( xi , ) = t c
q ( xi , ) = min ,
model selection for small sample regression
where min is the smallest eigenvalue of c .
in vapnik ( 123 ) , it is shown that for any family of functions q satisfying 123 q ( x , ) b and of vc dimension d , the following
eq ( x , ) 123
q ( xi , ) >
( log ( 123n / d ) + 123 ) 123
using this last inequality , we get that with probability 123 ,
123 min <
vd d ( n )
the authors would like to thank a anonymous referee for helpful and valuable comments .
note that the choice of such a family requires knowledge about ( x ) .
see remark 123 of section 123 for more 123
vd might be much larger than 123 for some basis , but in our experiments vd = 123 seems to be a good choice .
