we present a novel clustering method using the approach of support vector machines .
data points are mapped by means of a gaussian kernel to a high dimensional feature space , where we search for the minimal enclosing sphere .
this sphere , when mapped back to data space , can separate into several components , each enclosing a separate cluster of points .
we present a simple algorithm for identifying these clusters .
the width of the gaussian kernel controls the scale at which the data is probed while the soft margin constant helps coping with outliers and overlapping clusters .
the structure of a dataset is explored by varying the two parameters , maintaining a minimal number of support vectors to assure smooth cluster boundaries .
we demonstrate the performance of our algorithm on several datasets .
keywords : clustering , support vectors machines , gaussian kernel
clustering algorithms group data points according to various criteria , as discussed by jain and dubes ( 123 ) , fukunaga ( 123 ) , duda et al .
( 123 ) .
clustering may proceed according to some parametric model , as in the k - means algorithm of macqueen ( 123 ) , or by grouping points according to some distance or similarity measure as in hierarchical clustering algorithms .
other approaches include graph theoretic methods , such as shamir and sharan ( 123 ) , physically motivated algorithms , as in blatt et al .
( 123 ) , and algorithms based on density estimation as in roberts ( 123 ) and fukunaga ( 123 ) .
in this paper we propose a non - parametric clustering algorithm based on the support vector approach of
c ( cid : 123 ) 123 ben - hur , horn , siegelmann and vapnik .
ben - hur , horn , siegelmann and vapnik
in scholkopf et al .
( 123 , 123 ) , tax and duin ( 123 ) a support vector algorithm was used to characterize the support of a high dimensional distribution .
as a by - product of the algorithm one can compute a set of contours which enclose the data points .
these contours were interpreted by us as cluster boundaries in ben - hur et al .
( 123 ) .
here we discuss in detail a method which allows for a systematic search for clustering solutions without making assumptions on their number or shape , rst introduced in ben - hur et al .
in our support vector clustering ( svc ) algorithm data points are mapped from data space to a high dimensional feature space using a gaussian kernel .
in feature space we look for the smallest sphere that encloses the image of the data .
this sphere is mapped back to data space , where it forms a set of contours which enclose the data points .
these contours are interpreted as cluster boundaries .
points enclosed by each separate contour are associated with the same cluster .
as the width parameter of the gaussian kernel is decreased , the number of disconnected contours in data space increases , leading to an increasing number of clusters .
since the contours can be interpreted as delineating the support of the underlying probability distribution , our algorithm can be viewed as one identifying valleys in this probability distribution .
svc can deal with outliers by employing a soft margin constant that allows the sphere in feature space not to enclose all points .
for large values of this parameter , we can also deal with overlapping clusters .
in this range our algorithm is similar to the scale space clustering method of roberts ( 123 ) that is based on a parzen window estimate of the probability density with a gaussian kernel function .
in the next section we dene the svc algorithm .
in section 123 it is applied to problems with and without outliers .
we rst describe a problem without outliers to illustrate the type of clustering boundaries and clustering solutions that are obtained by varying the scale of the gaussian kernel .
then we proceed to discuss problems that necessitate invoking outliers in order to obtain smooth clustering boundaries .
these problems include two standard
the svc algorithm
123 cluster boundaries
following scholkopf et al .
( 123 ) and tax and duin ( 123 ) we formulate a support vector description of a data set , that is used as the basis of our clustering algorithm .
let ( xi ) be a data set of n points , with ird , the data space .
using a nonlinear transformation from to some high dimensional feature - space , we look for the smallest enclosing sphere of radius r .
this is described by the constraints :
|| ( xj ) a||123 r123 j ,
where || || is the euclidean norm and a is the center of the sphere .
soft constraints are incorporated by adding slack variables j :
|| ( xj ) a||123 r123 + j
support vector clustering
j = c j .
with j 123
to solve this problem we introduce the lagrangian
l = r123
( r123 + j || ( xj ) a||123 ) j
jj + c
where j 123 and j 123 are lagrange multipliers , c is a constant , and c j is a penalty term .
setting to zero the derivative of l with respect to r , a and j , respectively , leads to
the kkt complementarity conditions of fletcher ( 123 ) result in
jj = 123 , ( r123 + j || ( xj ) a||123 ) j = 123
it follows from eq .
( 123 ) that the image of a point xi with i > 123 and i > 123 lies outside the feature - space sphere .
( 123 ) states that such a point has i = 123 , hence we conclude from eq .
( 123 ) that i = c .
this will be called a bounded support vector or bsv .
a point xi with i = 123 is mapped to the inside or to the surface of the feature space sphere .
if its 123 < i < c then eq .
( 123 ) implies that its image ( xi ) lies on the surface of the feature space sphere .
such a point will be referred to as a support vector or sv .
svs lie on cluster boundaries , bsvs lie outside the boundaries , and all other points lie inside them .
note that when c 123 no bsvs exist because of the constraint ( 123 ) .
using these relations we may eliminate the variables r , a and j , turning the lagrangian
into the wolfe dual form that is a function of the variables j :
since the variables j dont appear in the lagrangian they may be replaced with the con -
we follow the sv method and represent the dot products ( xi ) ( xj ) by an appropriate mercer kernel k ( xi , xj ) .
throughout this paper we use the gaussian kernel
123 j c , j = 123 ,
k ( xi , xj ) = eq||xixj||123 ,
with width parameter q .
as noted in tax and duin ( 123 ) , polynomial kernels do not yield tight contours representations of a cluster .
the lagrangian w is now written as :
ben - hur , horn , siegelmann and vapnik
at each point x we dene the distance of its image in feature space from the center of
r123 ( x ) = || ( x ) a||123 .
in view of ( 123 ) and the denition of the kernel we have :
r123 ( x ) = k ( x , x ) 123
jk ( xj , x ) +
ijk ( xi , xj ) .
the radius of the sphere is :
r = ( r ( xi ) | xi is a support vector ) .
the contours that enclose the points in data space are dened by the set
( x | r ( x ) = r ) .
they are interpreted by us as forming cluster boundaries ( see figures 123 and 123 ) .
in view of equation ( 123 ) , svs lie on cluster boundaries , bsvs are outside , and all other points lie inside the clusters .
123 cluster assignment
the cluster description algorithm does not dierentiate between points that belong to dier - ent clusters .
to do so , we use a geometric approach involving r ( x ) , based on the following observation : given a pair of data points that belong to dierent components ( clusters ) , any path that connects them must exit from the sphere in feature space .
therefore , such a path contains a segment of points y such that r ( y ) > r .
this leads to the denition of the adjacency matrix aij between pairs of points xi and xj whose images lie in or on the sphere in feature space :
123 if , for all y on the line segment connecting xi and xj , r ( y ) r
clusters are now dened as the connected components of the graph induced by a .
checking the line segment is implemented by sampling a number of points ( 123 points were used in our numerical experiments ) .
bsvs are unclassied by this procedure since their feature space images lie outside the enclosing sphere .
one may decide either to leave them unclassied , or to assign them to the cluster that they are closest to , as we will do in the examples studied below .
the shape of the enclosing contours in data space is governed by two parameters : q , the scale parameter of the gaussian kernel , and c , the soft margin constant .
in the examples studied in this section we will demonstrate the eects of these two parameters .
support vector clustering
figure 123 : clustering of a data set containing 123 points using svc with c = 123
support vectors are designated by small circles , and cluster assignments are represented by dierent grey scales of the data points .
( a ) : q = 123 ( b ) : q = 123 ( c ) : q = 123 ( d ) : q = 123
123 example without bsvs
we begin with a data set in which the separation into clusters can be achieved without invoking outliers , i . e .
figure 123 demonstrates that as the scale parameter of the gaussian kernel , q , is increased , the shape of the boundary in data - space varies : with increasing q the boundary ts more tightly the data , and at several q values the enclosing contour splits , forming an increasing number of components ( clusters ) .
figure 123a has the smoothest cluster boundary , dened by six svs .
with increasing q , the number of support vectors nsv increases .
this is demonstrated in figure 123 where we plot nsv as a function of q for the data considered in figure 123
123 example with bsvs
in real data , clusters are usually not as well separated as in figure 123
thus , in order to observe splitting of contours , we must allow for bsvs .
the number of outliers is controlled
ben - hur , horn , siegelmann and vapnik
figure 123 : number of svs as a function of q for the data of figure 123
contour splitting
points are denoted by vertical lines .
by the parameter c .
from the constraints ( 123 , 123 ) it follows that
nbsv < 123 / c ,
where nbsv is the number of bsvs .
thus 123 / ( n c ) is an upper bound on the fraction of bsvs , and it is more natural to work with the parameter
asymptotically ( for large n ) , the fraction of outliers tends to p , as noted in scholkopf et al .
when distinct clusters are present , but some outliers ( e . g .
due to noise ) prevent contour separation , it is very useful to employ bsvs .
this is demonstrated in figure 123a : without bsvs contour separation does not occur for the two outer rings for any value of q .
when some bsvs are present , the clusters are separated easily ( figure 123b ) .
the dierence between data that are contour - separable without bsvs and data that require use of bsvs is illus - trated schematically in figure 123
a small overlap between the two probability distributions that generate the data is enough to prevent separation if there are no bsvs .
in the spirit of the examples displayed in figures 123 and 123 we propose to use svc iteratively : starting with a low value of q where there is a single cluster , and increasing it , to observe the formation of an increasing number of clusters , as the gaussian kernel describes the data with increasing precision .
if , however , the number of svs is excessive ,
support vector clustering
figure 123 : clustering with and without bsvs .
the inner cluster is composed of 123 points generated from a gaussian distribution .
the two concentric rings contain 123 / 123 points , generated from a uniform angular distribution and radial gaussian dis - tribution .
( a ) the rings cannot be distinguished when c = 123
shown here is q = 123 , the lowest q value that leads to separation of the inner cluster .
outliers allow easy clustering .
the parameters are p = 123 and q = 123 .
figure 123 : clusters with overlapping density functions require the introduction of bsvs .
a large fraction of the data turns into svs ( figure 123a ) , or a number of singleton clusters form , one should increase p to allow these points to turn into outliers , thus facilitating contour separation ( figure 123b ) .
as p is increased not only does the number of bsvs increase , but their inuence on the shape of the cluster contour decreases , as shown in ben - hur et al .
( 123 ) .
the number of support vectors depends on both q and p .
for xed
ben - hur , horn , siegelmann and vapnik
q , as p is increased , the number of svs decreases since some of them turn into bsvs and the contours become smoother ( see figure 123 ) .
strongly overlapping clusters
our algorithm may also be useful in cases where clusters strongly overlap , however a dierent interpretation of the results is required .
we propose to use in such a case a high bsv regime , and reinterpret the sphere in feature space as representing cluster cores , rather than the envelope of all data .
note that equation ( 123 ) for the reection of the sphere in data space can be expressed
ik ( xi , x ) = ) ,
where is determined by the value of this sum on the support vectors .
the set of points enclosed by the contour is :
ik ( xi , x ) > ) .
in the extreme case when almost all data points are bsvs ( p 123 ) , the sum in this expres -
is approximately equal to
k ( xi , x ) .
this last expression is recognized as a parzen window estimate of the density function ( up to a normalization factor , if the kernel is not appropriately normalized ) , see duda et al .
( 123 ) .
in this high bsv regime , we expect the contour in data space to enclose a small number of points which lie near the maximum of the parzen - estimated density .
in other words , the contour species the core of the probability distribution .
this is schematically represented in figure 123
in this regime our algorithm is closely related to the scale - space algorithm proposed by roberts ( 123 ) .
he denes cluster centers as maxima of the parzen window estimator pw ( x ) .
the gaussian kernel plays an important role in his analysis : it is the only kernel for which the number of maxima ( hence the number of clusters ) is a monotonically non - decreasing function of q .
this is the counterpart of contour splitting in svc .
as an example we study the crab data set of ripley ( 123 ) in figure 123
we plot the topographic maps of pw and psvc in the high bsv regime .
the two maps are very similar .
in figure 123a we present the svc clustering assignment .
figure 123b shows the original classication superimposed on the topographic map of pw .
in the scale space clustering approach it is dicult to identify the bottom right cluster , since there is only a small region that attracts points to this local maximum .
we propose to rst identify the contours that form cluster cores , the dark contours in figure 123a , and then associate points ( including bsvs ) to clusters according to their distances from cluster cores .
support vector clustering
figure 123 : in the case of signicant overlap between clusters the algorithm identies clusters according to dense cores , or maxima of the underlying probability distribution .
figure 123 : ripleys crab data displayed on a plot of their 123nd and 123rd principal compo - nents : ( a ) topographic map of psvc ( x ) and svc cluster assignments .
cluster core boundaries are denoted by bold contours; parameters were q = 123 , p = 123 .
( b ) the parzen window topographic map pw ( x ) for the same q value , and the data represented by the original classication given by ripley ( 123 ) .
the computational advantage of svc over roberts method is that , instead of solving a problem with many local maxima , we identify core boundaries by an sv method with a global optimal solution .
the conceptual advantage of our method is that we dene a region , rather than just a peak , as the core of the cluster .
ben - hur , horn , siegelmann and vapnik
figure 123 : cluster boundaries of the iris data set analyzed in a two - dimensional space spanned by the rst two principal components .
parameters used are q = 123 p =
123 the iris data
we ran svc on the iris data set of fisher ( 123 ) , which is a standard benchmark in the pattern recognition literature , and can be obtained from blake and merz ( 123 ) .
the data set contains 123 instances each composed of four measurements of an iris ower .
there are three types of owers , represented by 123 instances each .
clustering of this data in the space of its rst two principal components is depicted in figure 123 ( data was centered prior to extraction of principal components ) .
one of the clusters is linearly separable from the other two by a clear gap in the probability distribution .
the remaining two clusters have signicant overlap , and were separated at q = 123 p = 123 .
however , at these values of the parameters , the third cluster split into two ( see figure 123 ) .
when these two clusters are considered together , the result is 123 misclassications .
adding the third principal component we obtained the three clusters at q = 123 p = 123 , with four misclassications .
with the fourth principal component the number of misclassications increased to 123 ( using q = 123 p = 123 ) .
in addition , the number of support vectors increased with increasing dimensionality ( 123 in 123 dimensions , 123 in 123 dimensions and 123 in 123 dimensions ) .
the improved performance in 123 or 123 dimensions can be attributed to the noise reduction eect of pca .
our results compare favorably with other non - parametric clustering algorithms : the information theoretic approach of tishby and slonim ( 123 ) leads to 123 misclassications and the spc algorithm of blatt et al .
( 123 ) , when applied to the dataset in the original data - space , has 123 misclassications .
for high dimensional datasets , e . g .
the isolet dataset which has 123 dimensions , the problem was obtaining a support vector description : the number of support vectors jumped from very few ( one cluster ) to all data points being support vectors ( every point in a separate cluster ) .
using pca to reduce the dimensionality produced data that clustered well .
support vector clustering
123 varying q and p
we propose to use svc as a divisive clustering algorithm , see jain and dubes ( 123 ) : starting from a small value of q and increasing it .
the initial value of q may be chosen as
maxi , j ||xi xj||123
at this scale all pairs of points produce a sizeable kernel value , resulting in a single cluster .
at this value no outliers are needed , hence we choose c = 123
as q is increased we expect to nd bifurcations of clusters .
although this may look as hierarchical clustering , we have found counterexamples when using bsvs .
thus strict hierarchy is not guaranteed , unless the algorithm is applied separately to each cluster rather than to the whole dataset .
we do not pursue this choice here , in order to show how the cluster structure is unraveled as q is increased .
starting out with p = 123 / n , or c = 123 , we do not allow for any outliers .
if , as q is being increased , clusters of single or few points break o , or cluster boundaries become very rough ( as in figure 123a ) , p should be increased in order to investigate what happens when bsvs are allowed .
in general , a good criterion seems to be the number of svs : a low number guarantees smooth boundaries .
as q increases this number increases , as in figure 123
if the number of svs is excessive , p should be increased , whereby many svs may be turned into bsvs , and smooth cluster ( or core ) boundaries emerge , as in figure 123b .
in other words , we propose to systematically increase q and p along a direction that guarantees a minimal number of svs .
a second criterion for good clustering solutions is the stability of cluster assignments over some range of the two
