abstract .
wahbas classical representer theorem states that the solu - tions of certain risk minimization problems involving an empirical risk term and a quadratic regularizer can be written as expansions in terms of the training examples .
we generalize the theorem to a larger class of regularizers and empirical risk terms , and give a self - contained proof utilizing the feature space associated with a kernel .
the result shows that a wide range of problems have optimal solutions that live in the nite dimensional span of the training examples mapped into feature space , thus enabling us to carry out kernel algorithms independent of the ( potentially innite ) dimensionality of the feature space .
following the development of support vector ( sv ) machines ( 123 ) , positive denite kernels have recently attracted considerable attention in the machine learning community .
it turns out that a number of results that have now become popular were already known in the approximation theory community , as witnessed by the work of wahba ( 123 ) .
the present work brings together tools from both areas .
this allows us to formulate a generalized version of a classical theorem from the latter eld , and to give a new and simplied proof for it , using the geometrical view of kernel function classes as corresponding to vectors in linear feature spaces .
the paper is organized as follows .
in the present rst section , we review some basic concepts .
the two subsequent sections contain our main result , some examples and a short discussion .
123 positive denite kernels
the question under which conditions kernels correspond to dot products in linear spaces has been brought to the attention of the machine learning community by vapnik and coworkers ( 123 , 123 , 123 ) .
in functional analysis , the same problem has been studied under the heading of hilbert space representations of kernels .
a good monograph on the functional analytic theory of kernels is ( 123 ) .
most of the material in the present introductory section is taken from that work .
readers familiar with the basics of kernels can skip over the remainder of it .
helmbold and b .
williamson ( eds . ) : colt / eurocolt 123 , lnai 123 , pp .
123 , 123
c ( cid : 123 ) springer - verlag berlin heidelberg 123
a generalized representer theorem
suppose we are given empirical data
( x123 , y123 ) , .
, ( xm , ym ) x r .
here , the target values yi live in r , and the patterns xi are taken from a domain x .
the only thing we assume about x is that is a nonempty set .
in order to study the problem of learning , we need additional structure .
in kernel methods , this is provided by a similarity measure k : x x r ,
( x , x ( cid : 123 ) ) ( cid : 123 ) k ( x , x ( cid : 123 ) ) .
the function k is called a kernel ( 123 ) .
the term stems from the rst use of this type of function in the study of integral operators , where a function k giving rise to an operator tk via
k ( x , x ( cid : 123 ) ) f ( x ( cid : 123 ) ) dx ( cid : 123 )
is called the kernel of tk .
note that we will state most results for the more general case of complex - valued kernels;123 they specialize to the real - valued case in a straightforward manner .
below , unless stated otherwise , indices i and j will be understood to run over the training set , i . e .
i , j = 123 , .
denition 123 ( gram matrix ) .
given a kernel k and patterns x123 , .
, xm x , the m m matrix
k : = ( k ( xi , xj ) ) ij
is called the gram matrix of k with respect to x123 , .
denition 123 ( positive denite matrix ) .
an m m matrix k over the com - plex numbers c satisfying
for all c123 , .
, cm c is called positive denite .
denition 123 ( positive denite kernel ) .
let x be a nonempty set .
a function k : x x c which for all m n , x123 , .
, xm x gives rise to a positive denite gram matrix is called a positive denite ( pd ) kernel . 123 real - valued kernels are contained in the above denition as a special case .
how - ever , it is not sucient to require that ( 123 ) hold for real coecients ci .
if we want to get away with real coecients only , we additionally have to require that the 123 we use the notation c to denote the complex conjugate of c .
123 one might argue that the term positive denite kernel is slightly misleading .
in matrix theory , the term denite is sometimes used to denote the case where equality in ( 123 ) only occurs if c123 = = cm = 123
simply using the term positive kernel , on the other hand , could be confused with a kernel whose values are positive .
scholkopf , r .
herbrich , and a . j
kernel be symmetric .
the complex case is slightly more elegant; in that case , ( 123 ) can be shown to imply symmetry , i . e .
k ( xi , xj ) = k ( xj , xi ) .
positive denite kernels can be regarded as generalized dot products .
indeed , any dot product is a pd kernel; however , linearity does not carry over from dot products to general pd kernels .
another property of dot products , the cauchy - schwarz inequality , does have a natural generalization : if k is a positive denite kernel , and x123 , x123 x , then
|k ( x123 , x123 ) |123 k ( x123 , x123 ) k ( x123 , x123 ) .
x ( cid : 123 ) k ( , x ) .
: x cx ,
123 . . .
and associated feature spaces we dene a map from x into the space of functions mapping x into c , denoted as cx , via ( 123 )
here , ( x ) = k ( , x ) denotes the function that assigns the value k ( x ( cid : 123 ) , x ) to x ( cid : 123 ) x .
applying to x amounts to representing it by its similarity to all other points in the input domain x .
this seems a very rich representation , but it turns out that the kernel allows the computation of a dot product in that representation .
we shall now construct a dot product space containing the images of the input patterns under .
to this end , we rst need to endow it with the linear structure of a vector space .
this is done by forming linear combinations of the
here , m , m ( cid : 123 ) n , 123 , .
, m , 123 , .
, m ( cid : 123 ) c and x123 , .
, xm , x ( cid : 123 ) are arbitrary .
a dot product between f and g can be constructed as
, x ( cid : 123 )
( cid : 123 ) f , g ( cid : 123 ) : =
j , xi ) = k ( xi , x ( cid : 123 )
to see that this is well - dened , although it explicitly contains the expansion coecients ( which need not be unique ) , note that ( cid : 123 ) f , g ( cid : 123 ) = j ) .
the latter , however , does not depend on the particular expansion of f .
similarly , for g , note that ( cid : 123 ) f , g ( cid : 123 ) = i ig ( xi ) .
this also shows that ( cid : 123 ) , ( cid : 123 ) is anti - linear in the rst argument and linear in the second one .
it is symmetric , since ( cid : 123 ) f , g ( cid : 123 ) = ( cid : 123 ) g , f ( cid : 123 ) .
moreover , given functions f123 , .
, fn , and coecients 123 , .
, n c , we have
ij ( cid : 123 ) fi , fj ( cid : 123 ) =
hence ( cid : 123 ) , ( cid : 123 ) is actually a pd kernel on our function space .
a generalized representer theorem
for the last step in proving that it even is a dot product , one uses the following interesting property of , which follows directly from the denition : for all functions ( 123 ) ,
i . e . , k is the representer of evaluation .
in particular , ( cid : 123 ) k ( , x ) , k ( , x ( cid : 123 ) ) ( cid : 123 ) = k ( x , x ( cid : 123 ) ) , the reproducing kernel property ( 123 , 123 , 123 ) , hence ( cf .
( 123 ) ) we indeed have k ( x , x ( cid : 123 ) ) = ( cid : 123 ) ( x ) , ( x ( cid : 123 ) ) ( cid : 123 ) .
moreover , by ( 123 ) and ( 123 ) we have
( cid : 123 ) k ( , x ) , f ( cid : 123 ) = f ( x ) ,
|f ( x ) |123 = | ( cid : 123 ) k ( , x ) , f ( cid : 123 ) |123 k ( x , x ) ( cid : 123 ) f , f ( cid : 123 ) .
therefore , ( cid : 123 ) f , f ( cid : 123 ) = 123 implies f = 123 , which is the last property that was left to prove in order to establish that ( cid : 123 ) , ( cid : 123 ) is a dot product .
one can complete the space of functions ( 123 ) in the norm corresponding to the dot product , i . e . , add the limit points of sequences that are convergent in that norm , and thus gets a hilbert space hk , usually called a reproducing kernel hilbert space ( rkhs ) .
the case of real - valued kernels is included in the above; in that case , hk can be chosen as a real hilbert space .
123 the representer theorem
as a consequence of the last section , one of the crucial properties of kernels is that even if the input domain x is only a set , we can nevertheless think of the pair ( x , k ) as a ( subset of a ) hilbert space .
from a mathematical point of view , this is attractive , since we can thus study various data structures ( e . g . , strings over discrete alphabets ( 123 , 123 , 123 ) ) in hilbert spaces , whose theory is very well developed .
from a practical point of view , however , we now face the problem that for many popular kernels , the hilbert space is known to be innite - dimensional ( 123 ) .
when training a learning machine , however , we do not normally want to solve an optimization problem in an innite - dimensional space .
this is where the main result of this paper will be useful , showing that a large class of optimization problems with rkhs regularizers have solutions that can be expressed as kernel expansions in terms of the training data .
these optimization problems are of great interest for learning theory , both since they comprise a number of useful algorithms as special cases and since their statistical performance can be analyzed with tools of learning theory ( see ( 123 , 123 ) , and , more specically dealing with regularized risk functionals , ( 123 ) ) .
theorem 123 ( nonparametric representer theorem ) .
suppose we are given a nonempty set x , a positive denite real - valued kernel k on x x , a training sample ( x123 , y123 ) , .
, ( xm , ym ) x r , a strictly monotonically increas - ing real - valued function g on ( 123 , ( , an arbitrary cost function c : ( x r123 ) m r ( ) , and a class of functions
ik ( , zi ) , i r , zi x , ( cid : 123 ) f ( cid : 123 ) <
scholkopf , r .
herbrich , and a . j
here , ( cid : 123 ) ( cid : 123 ) is the norm in the rkhs hk associated with k , i . e .
for any zi
x , i r ( i n ) , ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 )
then any f f minimizing the regularized risk functional
c ( ( x123 , y123 , f ( x123 ) ) , .
, ( xm , ym , f ( xm ) ) ) + g ( ( cid : 123 ) f ( cid : 123 ) )
admits a representation of the form
let us give a few remarks before the proof .
in its original form , with mean
c ( ( x123 , y123 , f ( x123 ) ) , .
, ( xm , ym , f ( xm ) ) ) =
or hard constraints on the outputs , and g ( ( cid : 123 ) f ( cid : 123 ) ) = ( cid : 123 ) f ( cid : 123 ) 123 ( > 123 ) , the theorem is due to ( 123 ) .
note that in our formulation , hard constraints on the solution are included by the possibility of c taking the value .
a generalization to non - quadratic cost functions was stated by ( 123 ) , cf .
the discussion in ( 123 ) ( note , however , that ( 123 ) did not yet allow for coupling of losses at dierent points ) .
the present generalization to g ( ( cid : 123 ) f ( cid : 123 ) ) is , to our knowledge , new .
for a machine learning point of view on the representer theorem and a variational proof , cf .
the signicance of the theorem is that it shows that a whole range of learning algorithms have solutions that can be expressed as expansions in terms of the training examples .
note that monotonicity of g is necessary to ensure that the theorem holds .
it does not ensure that the regularized risk functional ( 123 ) does not have multiple local minima .
for this , we would need to require convexity of g and of the cost function c .
if we discarded the strictness of the monotonicity of g , it would no longer follow that each minimizer ( there might be multiple ones ) of the regularized risk admits an expansion ( 123 ) ; however , it would still follow that there is always another solution minimizing ( 123 ) that does admit the expansion .
in the sv community , ( 123 ) is called the sv expansion .
as we have assumed that k maps into r , we will use ( cf
: x rx ,
x ( cid : 123 ) k ( , x ) .
since k is a reproducing kernel , evaluation of the function ( x ) on the point x ( cid : 123 )
( ( x ) ) ( x ( cid : 123 ) ) = k ( x ( cid : 123 ) , x ) = ( cid : 123 ) ( x ( cid : 123 ) ) , ( x ) ( cid : 123 )
a generalized representer theorem
for all x , x ( cid : 123 ) x .
here , ( cid : 123 ) , ( cid : 123 ) denotes the dot product of hk .
given x123 , .
, xm , any f f can be decomposed into a part that lives in the
span of the ( xi ) and a part which is orthogonal to it , i . e .
i ( xi ) + v
for some rm and v f satisfying , for all j , ( cid : 123 ) v , ( xj ) ( cid : 123 ) = 123
using the latter and ( 123 ) , application of f to an arbitrary training point xj
independent of v .
consequently , the rst term of ( 123 ) is independent of v .
as i=123 i ( xi ) , and g is strictly for the second term , since v is orthogonal to monotonic , we get
i ( xi ) + v , ( xj )
g ( ( cid : 123 ) f ( cid : 123 ) ) = g
i ( xi ) + v
with equality occuring if and only if v = 123
setting v = 123 thus does not aect the rst term of ( 123 ) , while strictly reducing the second term hence , any minimizer must have v = 123
consequently , any solution takes the form f = i . e . , using ( 123 ) ,
the theorem is proven .
the extension to the case where we also include a parametric part is slightly more technical but straightforward .
we state the corresponding result without theorem 123 ( semiparametric representer theorem ) .
suppose that in ad - dition to the assumptions of the previous theorem we are given a set of m real - valued functions ( p ) m p=123 on x , with the property that the m m matrix ( p ( xi ) ) ip has rank m .
then any f : = f + h , with f f and h span ( p ) , minimizing the regularized risk
( x123 , y123 , f ( x123 ) ) , .
, ( xm , ym , f ( xm ) )
+ g ( ( cid : 123 ) f ( cid : 123 ) )
scholkopf , r .
herbrich , and a . j
admits a representation of the form
with unique coecients p r for all p = 123 , .
remark 123 ( biased regularization ) .
a straightforward extension of the represen - ter theorems can be obtained by including a term ( cid : 123 ) f123 , f ( cid : 123 ) into ( 123 ) or ( 123 ) , respectively , where f123 hk .
in this case , if a solution to the minimization prob - lem exists , it admits an expansion which diers from the above ones in that it additionally contains a multiple of f123
to see this , decompose the vector v used in the proof of theorem 123 into a part orthogonal to f123 and the remainder .
123 ( cid : 123 ) f ( cid : 123 ) 123 , this can be seen to correspond to an 123 ( cid : 123 ) f f123 ( cid : 123 ) 123
thus , it is no longer the size
in the case where g ( ( cid : 123 ) f ( cid : 123 ) ) = 123
eective overall regularizer of the form 123 of ( cid : 123 ) f ( cid : 123 ) that is penalized , but the dierence to f123
some explicit applications of theorems 123 and 123 are given below .
example 123 ( sv regression ) .
for sv regression with the insensitive loss ( 123 ) we
( xi , yi , f ( xi ) ) i=123 , . . . , m
max ( 123 , |f ( xi ) yi| )
and g ( ( cid : 123 ) f ( cid : 123 ) ) = ( cid : 123 ) f ( cid : 123 ) 123 , where > 123 and 123 are xed parameters which determine the trade - o between regularization and t to the training set .
in addition , a single ( m = 123 ) constant function 123 ( x ) = b ( b r ) is used as an oset that is not regularized by the algorithm ( 123 ) .
in ( 123 ) , a semiparametric extension was proposed which shows how to deal
with the case m > 123 algorithmically .
theorem 123 applies in that case , too .
example 123 ( sv classication ) .
here , the targets satisfy yi ( 123 ) , and we use
( xi , yi , f ( xi ) ) i=123 , . . . , m
max ( 123 , 123 yif ( xi ) ) ,
the regularizer g ( ( cid : 123 ) f ( cid : 123 ) ) = ( cid : 123 ) f ( cid : 123 ) 123 , and 123 ( x ) = b .
for 123 , we recover the hard margin svm , i . e . , the minimizer must correctly classify each training point ( xi , yi ) .
note that after training , the actual classier will be sgn ( f ( ) + b ) .
example 123 ( svms minimizing actual risk bounds ) .
the reason why svm algo - rithms such as the ones discussed above use the regularizer g ( ( cid : 123 ) f ( cid : 123 ) ) = ( cid : 123 ) f ( cid : 123 ) 123 are practical ones .
it is usually argued that theoretically , we should really be min - imizing an upper bound on the expected test error , but in practice , we use a quadratic regularizer , traded o with an empirical risk term via some constant .
one can show that in combination with certain loss functions ( hard constraints , linear loss , quadratic loss , or suitable combinations thereof ) , this regularizer
a generalized representer theorem
leads to a convex quadratic programming problem ( 123 , 123 ) .
in that case , the stan - dard kuhn - tucker machinery of optimization theory ( 123 ) can be applied to derive a so - called dual optimization problem , which consists of nding the expansion coecients 123 , .
, m rather than the solution f in the rkhs .
from the point of view of learning theory , on the other hand , more general functions g might be preferable , such as ones that are borrowed from uniform convergence bounds .
for instance , we could take inspiration from the basic pat - tern recognition bounds of ( 123 ) and use , for some small > 123 , the ( strictly
r123 ( cid : 123 ) f ( cid : 123 ) 123 + 123
more sophisticated functions based on other bounds on the test error are con - ceivable , as well as variants for regression estimation ( e . g . , ( 123 , 123 ) ) .
unlike wahbas original version , the generalized representer theorem can help dealing with these cases .
it asserts that the solution still has an expansion in terms of the training examples .
it is thus sucient to minimize the risk bound over expansions of the form ( 123 ) ( or ( 123 ) ) .
substituting ( 123 ) into ( 123 ) , we get an ( m - dimensional ) problem in coecient representation ( without having to appeal to methods of optimization theory )
if g and c are convex , then so will be the dual , thus we can solve it employing methods of convex optimization ( such as interior point approaches often used even for standard svms ) .
if the dual is non - convex , we will typically only be able to nd local minima .
independent of the convexity issue , the result lends itself well to gradient - based on - line algorithms for minimizing rkhs - based risk functionals ( 123 , 123 , 123 , 123 , 123 , 123 ) : for the computation of gradients , we only need the objective function to be dierentiable; convexity is not required .
such algorithms can thus be adapted to deal with more general regularizers .
example 123 ( bayesian map estimates ) .
the well - known correspondence to bayesian methods is established by identifying ( 123 ) with the negative log poste - rior ( 123 , 123 ) .
in this case , exp ( c ( ( xi , yi , f ( xi ) ) i=123 , . . . , m ) ) is the likelihood of the data , while exp ( g ( ( cid : 123 ) f ( cid : 123 ) ) ) is the prior over the set of functions .
the well - known gaussian process prior ( e . g .
( 123 , 123 ) ) , with covariance function k , is obtained by using g ( ( cid : 123 ) f ( cid : 123 ) ) = ( cid : 123 ) f ( cid : 123 ) 123 ( here , > 123 , and , as above , ( cid : 123 ) ( cid : 123 ) is the norm of the rkhs associated with k ) .
a laplacian prior would be obtained by using
ik ( x123 , xi ) ) , .
, ( xm , ym ,
scholkopf , r .
herbrich , and a . j
g ( ( cid : 123 ) f ( cid : 123 ) ) = ( cid : 123 ) f ( cid : 123 ) .
in all cases , the minimizer of ( 123 ) corresponds to a function with maximal a posteriori probability ( map ) .
example 123 ( kernel pca ) .
pca in a kernel feature space can be shown to cor - respond to the case of
c ( ( xi , yi , f ( xi ) ) i=123 , . . . , m ) =
with g an arbitrary strictly monotonically increasing function ( 123 ) .
the con - straint ensures that we are only considering linear feature extraction functionals that produce outputs of unit empirical variance .
note that in this case of unsu - pervised learning , there are no labels yi to consider .
we have shown that for a large class of algorithms minimizing a sum of an em - pirical risk term and a regularization term in a reproducing kernel hilbert space , the optimal solutions can be written as kernel expansions in terms of training ex - amples .
this has been known for specic algorithms; e . g . , for the sv algorithm , where it is a direct consequence of the structure of the optimization problem , but not in more complex cases , such as the direct minimization of some bounds on the test error ( cf .
example 123 ) .
the representer theorem puts these individ - ual ndings into a wider perspective , and we hope that the reader will nd the present generalization useful by either gaining some insight , or by taking it as a practical guideline for designing novel kernel algorithms : as long as the objec - tive function can be cast into the form considered in the generalized representer theorem , one can recklessly carry out algorithms in innite dimensional spaces , since the solution will always live in a specic subspace whose dimensionality equals at most the number of training examples .
acknowledgements .
thanks to bob williamson , grace wahba , jonathan baxter , peter bartlett , and nello cristianini for useful comments .
this work was supported by the australian research council .
as was supported by dfg grant sm 123 / 123 - 123
