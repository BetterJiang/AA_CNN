with the increase in available data parallel machine learning has become an in - creasingly pressing problem .
in this paper we present the rst parallel stochastic gradient descent algorithm including a detailed analysis and experimental evi - dence .
unlike prior work on parallel optimization algorithms ( 123 , 123 ) our variant comes with parallel acceleration guarantees and it poses no overly tight latency constraints , which might only be available in the multicore setting .
our analy - sis introduces a novel proof technique contractive mappings to quantify the speed of convergence of parameter distributions to their asymptotic limits .
as a side effect this answers the question of how quickly stochastic gradient descent algorithms reach the asymptotically normal regime ( 123 , 123 ) .
over the past decade the amount of available data has increased steadily .
by now some industrial scale datasets are approaching petabytes .
given that the bandwidth of storage and network per computer has not been able to keep up with the increase in data , the need to design data analysis algorithms which are able to perform most steps in a distributed fashion without tight constraints on communication has become ever more pressing .
a simple example illustrates the dilemma .
at current disk bandwidth and capacity ( 123tb at 123mb / s throughput ) it takes at least 123 hours to read the content of a single harddisk .
for a decade , the move from batch to online learning algorithms was able to deal with increasing data set sizes , since it reduced the runtime behavior of inference algorithms from cubic or quadratic to linear in the sample size .
however , whenever we have more than a single disk of data , it becomes computationally infeasible to process all data by stochastic gradient descent which is an inherently sequential algorithm , at least if we want the result within a matter of hours rather than days .
three recent papers attempted to break this parallelization barrier , each of them with mixed suc - cess .
( 123 ) show that parallelization is easily possible for the multicore setting where we have a tight coupling of the processing units , thus ensuring extremely low latency between the processors .
in particular , for non - adversarial settings it is possible to obtain algorithms which scale perfectly in the number of processors , both in the case of bounded gradients and in the strongly convex case .
unfortunately , these algorithms are not applicable to a mapreduce setting since the latter is fraught with considerable latency and bandwidth constraints between the computers .
a more mapreduce friendly set of algorithms was proposed by ( 123 , 123 ) .
in a nutshell , they rely on distributed computation of gradients locally on each computer which holds parts of the data and subsequent aggregation of gradients to perform a global update step .
this algorithm scales linearly
in the amount of data and log - linearly in the number of computers .
that said , the overall cost in terms of computation and network is very high : it requires many passes through the dataset for convergence .
moreover , it requires many synchronization sweeps ( i . e .
mapreduce iterations ) .
in other words , this algorithm is computationally very wasteful when compared to online algorithms .
( 123 ) attempted to deal with this issue by a rather ingenious strategy : solve the sub - problems exactly on each processor and in the end average these solutions to obtain a joint solution .
the key advantage of this strategy is that only a single mapreduce pass is required , thus dramatically reducing the amount of communication .
unfortunately their proposed algorithm has a number of drawbacks : the theoretical guarantees they are able to obtain imply a signicant variance reduction relative to the single processor solution ( 123 , theorem 123 , equation 123 ) but no bias reduction whatsoever ( 123 , theorem 123 , equation 123 ) relative to a single processor approach .
furthermore , their approach requires a relatively expensive algorithm ( a full batch solver ) to run on each processor .
a further drawback of the analysis in ( 123 ) is that the convergence guarantees are very much dependent on the degree of strong convexity as endowed by regularization .
however , since regularization tends to decrease with increasing sample size the guarantees become increasingly loose in practice as we see more data .
we attempt to combine the benets of a single - average strategy as proposed by ( 123 ) with asymptotic analysis ( 123 ) of online learning .
our proposed algorithm is strikingly simple : denote by ci ( w ) a loss function indexed by i and with parameter w .
then each processor carries out stochastic gradient descent on the set of ci ( w ) with a xed learning rate for t steps as described in algorithm 123
algorithm 123 sgd ( ( c123 , .
, cm ) , t , , w123 )
for t = 123 to t do
draw j ( 123 .
m ) uniformly at random .
wt wt123 wcj ( wt123 ) .
return wt .
on top of the sgd routine which is carried out on each computer we have a master - routine which aggregates the solution in the same fashion as ( 123 ) .
algorithm 123 parallelsgd ( ( c123 , .
cm ) , t , , w123 , k )
for all i ( 123 , .
k ) parallel do aggregate from all computers v = 123
vi = sgd ( ( c123 , .
cm ) , t , , w123 ) on client
i=123 vi and return v
m of the data which is likely to exceed 123
the key algorithmic difference to ( 123 ) is that the batch solver of the inner loop is replaced by a stochastic gradient descent algorithm which digests not a xed fraction of data but rather a random xed subset of data .
this means that if we process t instances per machine , each processor ends up distributed subgradient ( 123 , 123 ) distributed convex solver ( 123 ) multicore stochastic gradient ( 123 ) a direct implementation of the algorithms above would place every example on every machine : however , if t is much less than m , then it is only necessary for a machine to have access to the data it actually touches .
large scale learning , as dened in ( 123 ) , is when an algorithm is bounded by the time available instead of by the amount of data available .
practically speaking , that means that one can consider the actual data in the real dataset to be a subset of a virtually innite set , and drawing with replacement ( as the theory here implies ) and drawing without replacement on the
latency tolerance mapreduce network io scalability
algorithm 123 simuparallelsgd ( examples ( c123 , .
cm ) , learning rate , machines k )
dene t = $m / k% randomly partition the examples , giving t examples to each machine .
for all i ( 123 , .
k ) parallel do
randomly shufe the data on machine i .
initialize wi , 123 = 123
for all t ( 123 , .
t ) : do
get the tth example on the ith machine ( this machine ) , ci , t wi , t wi , t123 wci ( wi , t123 )
aggregate from all computers v = 123
i=123 wi , t and return v .
innite data set can both be simulated by shufing the real data and accessing it sequentially .
the initial distribution and shufing can be a part of how the data is saved .
simuparallelsgd ts very well with the large scale learning paradigm as well as the mapreduce framework .
our paper applies an anytime algorithm via stochastic gradient descent .
the algorithm requires no communication between machines until the end .
this is perfectly suited to mapreduce settings .
asymptotically , the error approaches zero .
the amount of time required is independent of the number of examples , only depending upon the regularization parameter and the desired error at the end .
in stark contrast to the simplicity of algorithm 123 , its convergence analysis is highly technical .
hence we limit ourselves to presenting the main results in this extended abstract .
detailed proofs are given in the appendix .
before delving into details we briey outline the proof strategy :
when performing stochastic gradient descent with xed ( and sufciently small ) learning rate the distribution of the parameter vector is asymptotically normal ( 123 , 123 ) .
since all computers are drawing from the same data distribution they all converge to the same limit .
averaging between the parameter vectors of k computers reduces variance by o ( k 123 similar to the result of ( 123 ) .
however , it does not reduce bias ( this is where ( 123 ) falls short ) .
to show that the bias due to joint initialization decreases we need to show that the distri - bution of parameters per machine converges sufciently quickly to the limit distribution .
finally , we also need to show that the mean of the limit distribution for xed learning rate is sufciently close to the risk minimizer .
that is , we need to take nite - size learning rate effects into account relative to the asymptotically normal regime .
123 loss and contractions
in this paper we consider estimation with convex loss functions ci : #123 ( 123 , ) .
while our analysis extends to other hilbert spaces such as rkhss we limit ourselves to this class of functions for convenience .
for instance , in the case of regularized risk minimization we have
123 ( w ( 123 + l ( xi , yi , w xi )
where l is a convex function in wxi , such as 123 for binary classication .
the goal is to nd an approximate minimizer of the overall risk
123 ( yiwxi ) 123 for regression or log ( 123+exp ( yiwxi ) )
to deal with stochastic gradient descent we need tools for quantifying distributions over w .
lipschitz continuity : a function f : x r is lipschitz continuous with constant l with respect
to a distance d if |f ( x ) f ( y ) | ld ( x , y ) for all x , y x .
holder continuity : a function f is holder continous with constant l and exponent if |f ( x ) lipschitz seminorm : ( 123 ) introduce a seminorm .
with minor modication we use
f ( y ) | ld ( x , y ) for all x , y x .
( f ( lip : = inf ( l where |f ( x ) f ( y ) | ld ( x , y ) for all x , y x ) .
that is , ( f ( lip is the smallest constant for which lipschitz continuity holds .
holder seminorm : extending the lipschitz norm for 123 :
: = inf ( l where |f ( x ) f ( y ) | ld ( x , y ) for all x , y x ) .
contraction : for a metric space ( m , d ) , f : m m is a contraction mapping if ( f ( lip < 123
in the following we assume that ( l ( x , y , y " ) ( lip g as a function of y " for all occurring data ( x , y ) x y and for all values of w within a suitably chosen ( often compact ) domain .
theorem 123 ( banachs fixed point theorem ) if ( m , d ) is a non - empty complete metric space , then any contraction mapping f on ( m , d ) has a unique xed point x = f ( x ) .
corollary 123 the sequence xt = f ( xt123 ) converges linearly with d ( x , xt ) ( f ( t our strategy is to show that the stochastic gradient descent mapping
lip d ( x123 , x ) .
is a contraction , where i is selected uniformly at random from ( 123 , .
this would allow us to demonstrate exponentially fast convergence .
note that since the algorithm selects i at random , different runs with the same initial settings can produce different results .
a key tool is the following :
w i ( w ) : = w ci ( w )
lemma 123 let c ##yl ( xi , yi , y ) ##lip be a lipschitz bound on the loss gradient .
then if ( ##xi##123 c + ) 123 the update rule ( 123 ) is a contraction mapping in #123 with lipschitz constant 123 .
we prove this in appendix b .
if we choose low enough , gradient descent uniformly becomes a contraction .
we dene
c + %123
123 contraction for distributions
for xed learning rate stochastic gradient descent is a markov process with state vector w .
while there is considerable research regarding the asymptotic properties of this process ( 123 , 123 ) , not much is known regarding the number of iterations required until the asymptotic regime is assumed .
we now address the latter by extending the notion of contractions from mappings of points to mappings of distributions .
for this we introduce the monge - kantorovich - wasserstein earth movers distance .
denition 123 ( wasserstein metric ) for a radon space ( m , d ) let p ( m , d ) be the set of all distri - butions over the space .
the wasserstein distance between two distributions x , y p ( m , d ) is
wz ( x , y ) =&
dz ( x , y ) d ( x , y ) ( 123
where ( x , y ) is the set of probability distributions on ( m , d ) ( m , d ) with marginals x and y .
this metric has two very important properties : it is complete and a contraction in ( m , d ) induces a contraction in ( p ( m , d ) , wz ) .
given a mapping : m m , we can construct p : p ( m , d ) p ( m , d ) by applying pointwise to m .
let x p ( m , d ) and let x " : = p ( x ) .
denote for any measurable event e its pre - image by 123 ( e ) .
then we have that x " ( e ) = x ( 123 ( e ) ) .
lemma 123 given a metric space ( m , d ) and a contraction mapping on ( m , d ) with constant c , p is a contraction mapping on ( p ( m , d ) , wz ) with constant c .
this is proven in appendix c .
this shows that any single mapping is a contraction .
however , since we draw ci at random we need to show that a mixture of such mappings is a contraction , too .
here the fact that we operate on distributions comes handy since the mixture of mappings on distribution is a mapping on distributions .
lemma 123 given a radon space ( m , d ) , if p123 .
pk are contraction mappings with constants i=123 aipi is a contrac -
ck with respect to wz , and ! i ai = 123 where ai 123 , then p = ! k tion mapping with a constant of no more than ( ! i ai ( ci ) z ) corollary 123 if for all i , ci c , then p is a contraction mapping with a constant of no more than c .
i=123 pi to be the this is proven in appendix c .
we apply this to sgd as follows : dene p = 123 the initial parameter distribution from which w123 is stochastic operation in one step .
denote by d123 drawn and by dt then the following holds :
the parameter distribution after t steps , which is obtained via dt
theorem 123 for any z n , if , then p is a contraction mapping on ( m , wz ) with contrac - tion rate ( 123 ) .
moreover , there exists a unique xed point d such that p ( d ) = d .
finally , if w123 = 123 with probability 123 , then wz ( d123
, d ) = g
, and wz ( dt
, d ) g
( 123 ) t .
, d ) g
this is proven in appendix f .
the contraction rate ( 123 ) can be proven by applying lemma 123 , lemma 123 , and corollary 123
as we show later , wt g / with probability 123 , so prwd ( d ( 123 , w ) g / ) = 123 , and since w123 = 123 , this implies wz ( d123 , d ) = g / .
from this , corollary 123 establishes this means that for a suitable choice of we achieve exponentially fast convergence in t to some stationary distribution d .
note that this distribution need not be centered at the risk minimizer of c ( w ) .
what the result does , though , is establish a guarantee that each computer carrying out algorithm 123 will converge rapidly to the same distribution over w , which will allow us to obtain good bounds if we can bound the bias and variance of d .
( 123 ) t .
123 guarantees for the stationary distribution
at this point , we know there exists a stationary distribution , and our algorithms are converging to that distribution exponentially fast .
however , unlike in traditional gradient descent , the stationary distribution is not necessarily just the optimal point .
in particular , the harder parts of understanding this algorithm involve understanding the properties of the stationary distribution .
first , we show that the mean of the stationary distribution has low error .
therefore , if we ran for a really long time and averaged over many samples , the error would be low .
theorem 123 c ( ewd ( w ) ) minwrn c ( w ) 123g123
proven in appendix g using techniques from regret minimization .
secondly , we show that the squared distance from the optimal point , and therefore the variance , is low .
theorem 123 the average squared distance of d from the optimal point is bounded by :
ewd ( ( w w ) 123 )
in other words , the squared distance is bounded by o ( g123 / ) .
proven in appendix i using techniques from reinforcement learning .
in what follows , if x m , y p ( m , d ) , we dene wz ( x , y ) to be the wz distance between y and a distribution with a probability of 123 at x .
throughout the appendix , we develop tools to show that the distribution over the output vector of the algorithm is near d , the mean of the stationary distribution .
in is the distribution over the nal vector of parallelsgd after t iterations on each particular , if dt , k of k machines with a learning rate , then w123 ( d , dt , k ( ( x d ) 123 ) becomes small .
then , we need to connect the error of the mean of the stationary distribution to a distribution that is near to this mean .
theorem 123 given a cost function c such that ( c ( l and ( c ( l are bounded , a distribution d such that d and is bounded , then , for any v :
) = ) exdt , k
c ( w ) ) + ( c ( l
( w123 ( v , d ) ) 123 + ( c ( v ) min
( w123 ( v , d ) ) ) 123 ( c ( l ( c ( v ) min this is proven in appendix k .
the proof is related to the kantorovich - rubinstein theorem , and bounds on the lipschitz of c near v based on c ( v ) minw c ( w ) .
at this point , we are ready to get the main theorem : theorem 123 if and t = ln k ( ln +ln )
this is proven in appendix k .
123 discussion of the bound
the guarantee obtained in ( 123 ) appears rather unusual insofar as it does not have an explicit depen - dency on the sample size .
this is to be expected since we obtained a bound in terms of risk min - imization of the given corpus rather than a learning bound .
instead the runtime required depends only on the accuracy of the solution itself .
in comparison to ( 123 ) , we look at the number of iterations to reach for sgd in table 123
ignoring the effect of the dimensions ( such as and d ) , setting these parameters to 123 , and assuming that the , and = .
in terms of our bound , we assume g = 123 and ( c ( l = 123
conditioning number = 123 .
so , the bottou paper claims a bound of 123 in order to make our error order , we must set k = 123 machines to run 123 iterations , which we interpret as time , which is the same order of computation , but a dramatic speedup of a factor of 123 in wall clock another important aspect of the algorithm is that it can be arbitrarily precise .
by halving and roughly doubling t , you can halve the error .
also , the bound captures how much paralllelization can help .
if k > %c%l
modulo logarithmic factors , we require 123
, then the last term g123 will start to dominate .
data : we performed experiments on a proprietary dataset drawn from a major email system with labels y 123 and binary , sparse features .
the dataset contains 123 , 123 , 123 time - stamped instances out of which the last 123 , 123 instances are used to form the test set , leaving 123 , 123 , 123 training points .
we used hashing to compress the features into a 123 dimensional space .
in total , the dataset contained 123 , 123 , 123 features after hashing , which means that each instance has about 123 fea - tures on average .
thus , the average sparsity of each data point is 123 .
all instance have been normalized to unit length for the experiments .
figure 123 : relative training error with = 123e123 : huber loss ( left ) and squared error ( right )
approach : in order to evaluate the parallelization ability of the proposed algorithm , we followed the following procedure : for each conguration ( see below ) , we trained up to 123 models , each on an independent , random permutation of the full training data .
during training , the model is stored on disk after k = 123 , 123 123i updates .
we then averaged the models obtained for each i and evaluated the resulting model .
that way , we obtained the performance for the algorithm after each machine has seen k samples .
this approach is geared towards the estimation of the parallelization ability of our optimization algorithm and its application to machine learning equally .
this is in contrast to the evaluation approach taken in ( 123 ) which focussed solely on the machine learning aspect without studying the performance of the optimization approach .
evaluation measures : we report both the normalized root mean squared error ( rmse ) on the test set and the normalized value of the objective function during training .
we normalize the rmse such that 123 is the rmse obtained by training a model in one single , sequential pass over the data .
the objective function values are normalized in much the same way such that the objective function value of a single , full sequential pass over the data reaches the value 123 .
congurations : we studied both the huber and the squared error loss .
while the latter does not satisfy all the assumptions of our proofs ( its gradient is unbounded ) , it is included due to its popu - larity .
we choose to evaluate using two different regularization constants , = 123e123 and = 123e123 in order to estimate the performance characteristics both on smooth , easy problems ( 123e123 ) and on high - variance , hard problems ( 123e123 ) .
in all experiments , we xed the learning rate to = 123e123
123 results and discussion
optimization : figure 123 shows the relative objective function values for training using 123 , 123 and 123 machines with = 123e123
in terms of wall clock time , the models obtained on 123 machines clearly outperform the ones obtained on 123 machines , which in turn outperform the model trained on a single machine .
there is no signicant difference in behavior between the squared error and the huber loss in these experiments , despite the fact that the squared error is effectively unbounded .
thus , the parallelization works in the sense that many machines obtain a better objective function value after each machine has seen k instances .
additionally , the results also show that data - local parallelized training is feasible and benecial with the proposed algorithm in practice .
note that the parallel training needs slightly more machine time to obtain the same objective function value , which is to be expected .
also unsurprising , yet noteworthy , is the trade - off between the number of machines and the quality of the solution : the solution obtained by 123 machines is much more of an improvement over using one machine than using 123 machines is over 123
predictive performance : figure 123 shows the relative test rmse for 123 , 123 and 123 machines with = 123e123
as expected , the results are very similar to the objective function comparison : the parallel training decreases wall clock time at the price of slightly higher machine time .
again , the gain in performance between 123 and 123 machines is much higher than the one between 123 and 123
figure 123 : relative test - rmse with = 123e123 : huber loss ( left ) and squared error ( right )
figure 123 : relative train - error using huber loss : = 123e123 ( left ) , = 123e123 ( right )
performance using different : the last experiment is conducted to study the effect of the regu - larization constant on the parallelization ability : figure 123 shows the objective function plot using the huber loss and = 123e123 and = 123e123
the lower regularization constant leads to more variance in the problem which in turn should increase the benet of the averaging algorithm .
the plots exhibit exactly this characteristic : for = 123e123 , the loss for 123 and 123 machines not only drops faster , but the nal solution for both beats the solution found by a single pass , adding further empirical evidence for the behaviour predicted by our theory .
in this paper , we propose a novel data - parallel stochastic gradient descent algorithm that enjoys a number of key properties that make it highly suitable for parallel , large - scale machine learning : it imposes very little i / o overhead : training data is accessed locally and only the model is communi - cated at the very end .
this also means that the algorithm is indifferent to i / o latency .
these aspects make the algorithm an ideal candidate for a mapreduce implementation .
thereby , it inherits the lat - ters superb data locality and fault tolerance properties .
our analysis of the algorithms performance is based on a novel technique that uses contraction theory to quantify nite - sample convergence rate of stochastic gradient descent .
we show worst - case bounds that are comparable to stochastic gradient descent in terms of wall clock time , and vastly faster in terms of overall time .
lastly , our experiments on a large - scale real world dataset show that the parallelization reduces the wall - clock time needed to obtain a set solution quality .
unsurprisingly , we also see diminishing marginal util - ity of adding more machines .
finally , solving problems with more variance ( smaller regularization constant ) benets more from the parallelization .
