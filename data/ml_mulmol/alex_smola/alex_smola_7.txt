function estimation .
furthermore , we include a summary of currently used algorithms for training sv machines , covering both the quadratic ( or convex ) programming part and advanced methods for dealing with large datasets .
finally , we mention some modications and extensions that have been applied to the standard sv algorithm , and discuss the aspect of regularization from a sv perspective .
keywords : machine learning , support vector machines , regression estimation
the purpose of this paper is twofold .
it should serve as a self - contained introduction to support vector regression for readers new to this rapidly developing eld of research . 123 on the other hand , it attempts to give an overview of recent developments in
to this end , we decided to organize the essay as follows .
we start by giving a brief overview of the basic techniques in sections 123 , 123 and 123 , plus a short summary with a number of gures and diagrams in section 123
section 123 reviews current algorithmic techniques used for actually implementing sv machines .
this may be of most interest for practitioners .
the following section covers more advanced topics such as extensions of the basic sv algorithm , connections between sv machines and regularization and briey mentions methods for carrying out model selection .
we conclude with a discussion of open questions and problems and current directions of sv research .
most of the results presented in this review paper already have been published elsewhere , but the comprehensive presentations and some details are new .
historic background
the sv algorithm is a nonlinear generalization of the gener - alized portrait algorithm developed in russia in the sixties123
an extended version of this paper is available as neurocolt technical report 123 - 123 c ( cid : 123 ) 123 kluwer academic publishers
( vapnik and lerner 123 , vapnik and chervonenkis 123 ) .
as such , it is rmly grounded in the framework of statistical learn - ing theory , or vc theory , which has been developed over the last three decades by vapnik and chervonenkis ( 123 ) and vapnik ( 123 , 123 ) .
in a nutshell , vc theory characterizes properties of learning machines which enable them to generalize well to
in its present form , the sv machine was largely developed at at&t bell laboratories by vapnik and co - workers ( boser , guyon and vapnik 123 , guyon , boser and vapnik 123 , cortes and vapnik , 123 , scholkopf , burges and vapnik 123 , 123 , vapnik , golowich and smola 123 ) .
due to this industrial con - text , sv research has up to date had a sound orientation towards real - world applications .
initial work focused on ocr ( optical character recognition ) .
within a short period of time , sv clas - siers became competitive with the best available systems for both ocr and object recognition tasks ( scholkopf , burges and vapnik 123 , 123a , blanz et al .
123 , scholkopf 123 ) .
a comprehensive tutorial on sv classiers has been published by burges ( 123 ) .
but also in regression and time series predic - tion applications , excellent performances were soon obtained ( muller et al .
123 , drucker et al .
123 , stitson et al .
123 , mattera and haykin 123 ) .
a snapshot of the state of the art in sv learning was recently taken at the annual neural in - formation processing systems conference ( scholkopf , burges , and smola 123a ) .
sv learning has now evolved into an active area of research .
moreover , it is in the process of entering the standard methods toolbox of machine learning ( haykin 123 , cherkassky and mulier 123 , hearst et al .
scholkopf and
smola and scholkopf
smola ( 123 ) contains a more in - depth overview of svm regres - sion .
additionally , cristianini and shawe - taylor ( 123 ) and her - brich ( 123 ) provide further details on kernels in the context of
the basic idea suppose we are given training data ( ( x123 , y123 ) , .
, ( x ( cid : 123 ) , y ( cid : 123 ) ) ) x r , where x denotes the space of the input patterns ( e . g .
x = rd ) .
these might be , for instance , exchange rates for some currency measured at subsequent days together with correspond - ing econometric indicators .
in - sv regression ( vapnik 123 ) , our goal is to nd a function f ( x ) that has at most deviation from the actually obtained targets yi for all the training data , and at the same time is as at as possible .
in other words , we do not care about errors as long as they are less than , but will not accept any deviation larger than this .
this may be important if you want to be sure not to lose more than money when dealing with exchange rates , for instance .
for pedagogical reasons , we begin by describing the case of
linear functions f , taking the form
f ( x ) = ( cid : 123 ) w , x ( cid : 123 ) + b with w x , b r
where ( cid : 123 ) , ( cid : 123 ) denotes the dot product in x .
flatness in the case of ( 123 ) means that one seeks a small w .
one way to ensure this is to minimize the norm , 123 i . e .
( cid : 123 ) w ( cid : 123 ) 123 = ( cid : 123 ) w , w ( cid : 123 ) .
we can write this problem as a convex optimization problem :
yi ( cid : 123 ) w , xi ( cid : 123 ) b ( cid : 123 ) w , xi ( cid : 123 ) + b yi
the tacit assumption in ( 123 ) was that such a function f actually exists that approximates all pairs ( xi , yi ) with precision , or in other words , that the convex optimization problem is feasible .
sometimes , however , this may not be the case , or we also may want to allow for some errors .
analogously to the soft mar - gin loss function ( bennett and mangasarian 123 ) which was used in sv machines by cortes and vapnik ( 123 ) , one can in - troduce slack variables i , to cope with otherwise infeasible constraints of the optimization problem ( 123 ) .
hence we arrive at the formulation stated in vapnik ( 123 ) .
( cid : 123 ) w ( cid : 123 ) 123 + c
yi ( cid : 123 ) w , xi ( cid : 123 ) b + i
( cid : 123 ) w , xi ( cid : 123 ) + b yi +
the constant c > 123 determines the trade - off between the at - ness of f and the amount up to which deviations larger than are tolerated .
this corresponds to dealing with a so called - insensitive loss function || described by
the soft margin loss setting for a linear svm ( from scholkopf and smola , 123 )
figure 123 depicts the situation graphically .
only the points outside the shaded region contribute to the cost insofar , as the deviations are penalized in a linear fashion .
it turns out that in most cases the optimization problem ( 123 ) can be solved more easily in its dual formulation . 123 moreover , as we will see in section 123 , the dual for - mulation provides the key for extending sv machine to nonlinear functions .
hence we will use a standard dualization method uti - lizing lagrange multipliers , as described in e . g .
fletcher ( 123 ) .
dual problem and quadratic programs
the key idea is to construct a lagrange function from the ob - jective function ( it will be called the primal objective function in the rest of this article ) and the corresponding constraints , by introducing a dual set of variables .
it can be shown that this function has a saddle point with respect to the primal and dual variables at the solution .
for details see e . g .
mangasarian ( 123 ) , mccormick ( 123 ) , and vanderbei ( 123 ) and the explanations in section 123 .
we proceed as follows :
i ) ( cid : 123 ) ( cid : 123 )
( i i +
( cid : 123 ) w ( cid : 123 ) 123 + c
l : = 123
i ( + i yi + ( cid : 123 ) w , xi ( cid : 123 ) + b )
+ yi ( cid : 123 ) w , xi ( cid : 123 ) b )
here l is the lagrangian and i , i are lagrange multi - pliers .
hence the dual variables in ( 123 ) have to satisfy positivity
, we refer to i and
note that by ( ) it follows from the saddle point condition that the partial derivatives of l with respect to the primal variables ( w , b , i , have to vanish for optimality .
i ) = 123
b l = ( cid : 123 ) ( cid : 123 ) w l = w ( cid : 123 ) ( cid : 123 )
l = c ( )
i ) xi = 123
a tutorial on support vector regression
substituting ( 123 ) , ( 123 ) , and ( 123 ) into ( 123 ) yields the dual optimization
i ) + ( cid : 123 ) ( cid : 123 )
j ) ( cid : 123 ) xi , x j ( cid : 123 )
i ) ( j
i ) = 123 and i ,
in conjunction with an analogous analysis on
max ( + yi ( cid : 123 ) w , xi ( cid : 123 ) | i < c or min ( + yi ( cid : 123 ) w , xi ( cid : 123 ) | i > 123 or
( 123 , c ) the inequalities become equalities
if some ( ) also keerthi et al .
( 123 ) for further means of choosing b .
i we have > 123 ) b
another way of computing b will be discussed in the context of interior point optimization ( cf .
section 123 ) .
there b turns out to be a by - product of the optimization process .
further consid - erations shall be deferred to the corresponding section .
see also keerthi et al .
( 123 ) for further methods to compute the constant a nal note has to be made regarding the sparsity of the sv expansion .
from ( 123 ) it follows that only for | f ( xi ) yi| the lagrange multipliers may be nonzero , or in other words , for all samples inside the tube ( i . e .
the shaded region in fig .
123 ) i vanish : for | f ( xi ) yi| < the second factor in the i , ( 123 ) is nonzero , hence i , i has to be zero such that the kkt conditions are satised .
therefore we have a sparse expansion of w in terms of xi ( i . e .
we do not need all xi to describe w ) .
the examples that come with nonvanishing coefcients are called
nonlinearity by preprocessing
the next step is to make the sv algorithm nonlinear .
this , for instance , could be achieved by simply preprocessing the training patterns xi by a map : x f into some feature space f , as described in aizerman , braverman and rozonoer ( 123 ) and nilsson ( 123 ) and then applying the standard sv regression algorithm .
let us have a brief look at an example given in vapnik
example 123 ( quadratic features in r123 ) .
consider the map : r123 r123 with ( x123 , x123 ) = ( x 123 123 ) .
it is understood that the subscripts in this case refer to the components of x r123
training a linear sv machine on the preprocessed features would yield a quadratic function .
123x123x123 , x 123
while this approach seems reasonable in the particular ex - ample above , it can easily become computationally infeasible for both polynomial features of higher order and higher di - mensionality , as the number of different monomial features ) , where d = dim ( x ) .
typical values of degree p is ( d+ p123 for ocr tasks ( with good performance ) ( scholkopf , burges and vapnik 123 , scholkopf et al .
123 , vapnik 123 ) are p = 123 , d = 123 123 = 123 , corresponding to approximately 123 123 features .
implicit mapping via kernels
clearly this approach is not feasible and we have to nd a com - putationally cheaper way .
the key observation ( boser , guyon
in deriving ( 123 ) we already eliminated the dual variables i , through condition ( 123 ) which can be reformulated as ( )
w = ( cid : 123 ) ( cid : 123 )
equation ( 123 ) can be rewritten as follows
i ) xi , thus f ( x ) = ( cid : 123 ) ( cid : 123 )
i ) ( cid : 123 ) xi , x ( cid : 123 ) + b .
this is the so - called support vector expansion , i . e .
w can be completely described as a linear combination of the training patterns xi .
in a sense , the complexity of a functions represen - tation by svs is independent of the dimensionality of the input space x , and depends only on the number of svs .
moreover , note that the complete algorithm can be described in terms of dot products between the data .
even when evalu - ating f ( x ) we need not compute w explicitly .
these observa - tions will come in handy for the formulation of a nonlinear
computing b
so far we neglected the issue of computing b .
the latter can be done by exploiting the so called karushkuhntucker ( kkt ) conditions ( karush 123 , kuhn and tucker 123 ) .
these state that at the point of the solution the product between dual variables and constraints has to vanish .
i ( + i yi + ( cid : 123 ) w , xi ( cid : 123 ) + b ) = 123 i ( + + yi ( cid : 123 ) w , xi ( cid : 123 ) b ) = 123
( c i ) i = 123
this allows us to make several useful conclusions .
firstly only = c lie outside the - samples ( xi , yi ) with corresponding ( ) = 123 , i . e .
there can never be a set insensitive tube .
secondly i of dual variables i , i which are both simultaneously nonzero .
this allows us to conclude that yi + ( cid : 123 ) w , xi ( cid : 123 ) + b 123 yi + ( cid : 123 ) w , xi ( cid : 123 ) + b 123
if i < c ( 123 )
if i > 123
and vapnik 123 ) is that for the feature map of example 123 we
123x123x123 , x 123
( cid : 123 ) ( cid : 123 ) = ( cid : 123 ) x , x
) : = ( cid : 123 ) ( x ) , ( x
as noted in the previous section , the sv algorithm only depends on dot products between patterns xi .
hence it sufces to know ) ( cid : 123 ) rather than explicitly which allows us to restate the sv optimization problem :
i ) ( j
i ) + ( cid : 123 ) ( cid : 123 )
j ) k ( xi , x j )
and i ,
i ) = 123
likewise the expansion of f ( 123 ) may be written as
smola and scholkopf
condition ( scholkopf , burges and smola 123a ) .
in the follow - ing we will call such functions k admissible sv kernels .
corollary 123 ( positive linear combinations of kernels ) .
denote by k123 , k123 admissible sv kernels and c123 , c123 123 then
) : = c123k123 ( x , x
) + c123k123 ( x , x
is an admissible kernel .
this follows directly from ( 123 ) by virtue of the linearity of integrals .
more generally , one can show that the set of admissible ker - nels forms a convex cone , closed in the topology of pointwise convergence ( berg , christensen and ressel 123 ) .
corollary 123 ( integrals of kernels ) .
let s ( x , x on x x such that
( cid : 123 ) , z ) dz
) be a function
i ) ( xi )
exists .
then k is an admissible sv kernel .
w = ( cid : 123 ) ( cid : 123 ) f ( x ) = ( cid : 123 ) ( cid : 123 )
i ) k ( xi , x ) + b .
the difference to the linear case is that w is no longer given ex - plicitly .
also note that in the nonlinear setting , the optimization problem corresponds to nding the attest function in feature space , not in input space .
conditions for kernels
tk f ( ) : =
the question that arises now is , which functions k ( x , x spond to a dot product in some feature space f .
the following theorem characterizes these functions ( dened on x ) .
theorem 123 ( mercer 123 ) .
suppose k l ( x 123 ) such that the integral operator tk : l 123 ( x ) l 123 ( x ) , k ( , x ) f ( x ) d ( x )
is positive ( here denotes a measure on x with ( x ) nite and supp ( ) = x ) .
let j l 123 ( x ) be the eigenfunction of tk ( cid : 123 ) = 123 and normalized such that associated with the eigenvalue j = 123 and let j denote its complex conjugate .
then 123
( j ( t ) ) j ( cid : 123 ) 123
k ( x , x
where the series converges absolutely and uniformly for al - most all ( x , x
) holds for almost all ( x , x
j j ( x ) j ( x
less formally speaking this theorem means that if
) f ( x ) f ( x
( cid : 123 ) 123 for all f l 123 ( x )
holds we can write k ( x , x ) as a dot product in some feature space .
from this condition we can conclude some simple rules for compositions of kernels , which then also satisfy mercers
this can be shown directly from ( 123 ) and ( 123 ) by rearranging the order of integration .
we now state a necessary and sufcient con - ) : = k ( x x dition for translation invariant kernels , i . e .
k ( x , x as derived in smola , scholkopf and muller ( 123c ) .
theorem 123 ( products of kernels ) .
denote by k123 and k123 admis - sible sv kernels then
) : = k123 ( x , x
is an admissible kernel .
this can be seen by an application of the expansion part of mercers theorem to the kernels k123 and k123 and observing that each term in the double sum gives rise to a positive coefcient when checking ( 123 ) .
i ( x ) 123
j ( x ) 123
theorem 123 ( smola , scholkopf and muller 123c ) .
a transla - ) = k ( x x tion invariant kernel k ( x , x ) is an admissible sv kernels if and only if the fourier transform
f ( k ) ( ) = ( 123 )
we will give a proof and some additional explanations to this theorem in section 123
it follows from interpolation theory ( micchelli 123 ) and the theory of regularization networks ( girosi , jones and poggio 123 ) .
for kernels of the dot - product ( cid : 123 ) ( cid : 123 ) ) , there exist sufcient conditions type , i . e .
k ( x , x for being admissible .
) = k ( ( cid : 123 ) x , x
theorem 123 ( burges 123 ) .
any kernel of dot - product type
) = k ( ( cid : 123 ) x , x k ( ) 123 , k ( ) 123
( cid : 123 ) ( cid : 123 ) ) has to satisfy
k ( ) + 123
k ( ) 123 ( 123 )
for any 123 in order to be an admissible sv kernel .
a tutorial on support vector regression
note that the conditions in theorem 123 are only necessary but not sufcient .
the rules stated above can be useful tools for practitioners both for checking whether a kernel is an admissible sv kernel and for actually constructing new kernels .
the general case is given by the following theorem .
b - splines of order 123n + 123 , dened by the 123n + 123 convolution of the unit inverval , are also admissible .
we shall postpone further considerations to section 123 where the connection to regulariza - tion operators will be pointed out in more detail .
theorem 123 ( schoenberg 123 ) .
a kernel of dot - product type ( cid : 123 ) ( cid : 123 ) ) dened on an innite dimensional hilbert space , with a power series expansion
) = k ( ( cid : 123 ) x , x
is admissible if and only if all an 123
a slightly weaker condition applies for nite dimensional spaces .
for further details see berg , christensen and ressel ( 123 ) and smola , ovari and williamson ( 123 ) .
in scholkopf , smola and muller ( 123b ) it has been shown , by explicitly computing the mapping , that homogeneous polyno - mial kernels k with p n and
) = ( cid : 123 ) x , x
are suitable sv kernels ( cf .
poggio 123 ) .
from this observation one can conclude immediately ( boser , guyon and vapnik 123 , vapnik 123 ) that kernels of the type ) = ( ( cid : 123 ) x , x
inhomogeneous polynomial kernels with p n , c 123 are admissible , too : rewrite k as a sum of homogeneous kernels and apply corollary 123
another kernel , that might seem appealing due to its resemblance to neural networks is the hyperbolic
( cid : 123 ) ( cid : 123 ) + c ) p
) = tanh ( + ( cid : 123 ) x , x
by applying theorem 123 one can check that this kernel does not actually satisfy mercers condition ( ovari 123 ) .
curiously , the kernel has been successfully used in practice; cf .
scholkopf ( 123 ) for a discussion of the reasons .
translation invariant kernels k ( x , x
quite widespread .
it was shown in aizerman , braverman and rozonoer ( 123 ) , micchelli ( 123 ) and boser , guyon and vap - nik ( 123 ) that
) = k ( x x
is an admissible sv kernel .
moreover one can show ( smola 123 , vapnik , golowich and smola 123 ) that ( 123x denotes the indicator function on the set x and the convolution operation )
( cid : 123 ) ( cid : 123 ) ) with bk : = k ( cid : 123 )
) = b123n+123 ( ( cid : 123 ) x x
cost functions
so far the sv algorithm for regression may seem rather strange and hardly related to other existing methods of function esti - mation ( e . g .
huber 123 , stone 123 , hardle 123 , hastie and tibshirani 123 , wahba 123 ) .
however , once cast into a more standard mathematical notation , we will observe the connec - tions to previous work .
for the sake of simplicity we will , again , only consider the linear case , as extensions to the nonlinear one are straightforward by using the kernel method described in the
the risk functional
let us for a moment go back to the case of section 123 .
there , we had some training data x : = ( ( x123 , y123 ) , .
, ( x ( cid : 123 ) , y ( cid : 123 ) ) ) x r .
we will assume now , that this training set has been drawn iid ( independent and identically distributed ) from some probabil - ity distribution p ( x , y ) .
our goal will be to nd a function f minimizing the expected risk ( cf .
vapnik 123 )
r ( f ) =
c ( x , y , f ( x ) ) d p ( x , y )
( c ( x , y , f ( x ) ) denotes a cost function determining how we will penalize estimation errors ) based on the empirical data x .
given that we do not know the distribution p ( x , y ) we can only use x for estimating a function f that minimizes r ( f ) .
a possi - ble approximation consists in replacing the integration by the empirical estimate , to get the so called empirical risk functional
remp ( f ) : = 123
c ( xi , yi , f ( xi ) ) .
a rst attempt would be to nd the empirical risk minimizer f123 : = argmin f h remp ( f ) for some function class h .
however , if h is very rich , i . e .
its capacity is very high , as for instance when dealing with few data in very high - dimensional spaces , this may not be a good idea , as it will lead to overtting and thus bad generalization properties .
hence one should add a capacity control term , in the sv case ( cid : 123 ) w ( cid : 123 ) 123 , which leads to the regularized risk functional ( tikhonov and arsenin 123 , morozov 123 ,
rreg ( f ) : = remp ( f ) +
where > 123 is a so called regularization constant .
many algorithms like regularization networks ( girosi , jones and poggio 123 ) or neural networks with weight decay networks ( e . g .
bishop 123 ) minimize an expression similar to ( 123 ) .
maximum likelihood and density models
the standard setting in the sv case is , as already mentioned in section 123 , the - insensitive loss
c ( x , y , f ( x ) ) = |y f ( x ) | .
it is straightforward to show that minimizing ( 123 ) with the par - ticular loss function of ( 123 ) is equivalent to minimizing ( 123 ) , the only difference being that c = 123 / ( ( cid : 123 ) ) .
loss functions such like |y f ( x ) | p
with p > 123 may not be desirable , as the superlinear increase leads to a loss of the robustness properties of the estimator ( huber 123 ) : in those cases the derivative of the cost function grows without bound .
for p < 123 , on the other hand , c becomes nonconvex .
for the case of c ( x , y , f ( x ) ) = ( y f ( x ) ) 123 we recover the least mean squares t approach , which , unlike the standard sv loss function , leads to a matrix inversion instead of a quadratic
the question is which cost function should be used in ( 123 ) .
on the one hand we will want to avoid a very complicated function c as this may lead to difcult optimization problems .
on the other hand one should use that particular cost function that suits the problem best .
moreover , under the assumption that the samples were generated by an underlying functional dependency plus additive noise , i . e .
yi = ftrue ( xi ) + i with density p ( ) , then the optimal cost function in a maximum likelihood sense is
c ( x , y , f ( x ) ) = log p ( y f ( x ) ) .
this can be seen as follows .
the likelihood of an estimate
x f : = ( ( x123 , f ( x123 ) ) , .
, ( x ( cid : 123 ) , f ( x ( cid : 123 ) ) ) )
for additive noise and iid data is
p ( x f | x ) = ( cid : 123 ) ( cid : 123 )
p ( f ( xi ) | ( xi , yi ) ) = ( cid : 123 ) ( cid : 123 )
maximizing p ( x f | x ) is equivalent to minimizing log p ( x f | x ) .
by using ( 123 ) we get
p ( yi f ( xi ) ) .
log p ( x f | x ) = ( cid : 123 ) ( cid : 123 )
smola and scholkopf
however , the cost function resulting from this reasoning might be nonconvex .
in this case one would have to nd a convex proxy in order to deal with the situation efciently ( i . e .
to nd an efcient implementation of the corresponding optimization
if , on the other hand , we are given a specic cost function from a real world problem , one should try to nd as close a proxy to this cost function as possible , as it is the performance wrt .
this particular cost function that matters ultimately .
table 123 contains an overview over some common density models and the corresponding loss functions as dened by
the only requirement we will impose on c ( x , y , f ( x ) ) in the following is that for xed x and y we have convexity in f ( x ) .
this requirement is made , as we want to ensure the existence and uniqueness ( for strict convexity ) of a minimum of optimization problems ( fletcher 123 ) .
solving the equations
for the sake of simplicity we will additionally assume c to be symmetric and to have ( at most ) two ( for symmetry ) dis - continuities at , 123 in the rst derivative , and to be zero in the interval ( , ) .
all loss functions from table 123 take on the following belong to this class .
hence c will
c ( x , y , f ( x ) ) = c ( |y f ( x ) | )
note the similarity to vapniks - insensitive loss .
it is rather straightforward to extend this special choice to more general convex cost functions .
for nonzero cost functions in the inter - val ( , ) use an additional pair of slack variables .
moreover
we might choose different cost functions ci , c i and different values of i , i for each sample .
at the expense of additional lagrange multipliers in the dual formulation additional discon - tinuities also can be taken care of .
analogously to ( 123 ) we arrive at a convex minimization problem ( smola and scholkopf 123a ) .
to simplify notation we will stick to the one of ( 123 ) and use c
c ( xi , yi , f ( xi ) ) .
table 123
common loss functions and corresponding density models
hubers robust loss
c ( ) = || c ( ) = || c ( ) = 123 c ( ) = c ( ) = 123 c ( ) =
123 ( ) 123 p p123 ( ) p
p ( ) = 123 p ( ) = 123
p ( ) = 123
123 ( 123 / p ) exp ( || p ) p ( ) = p
a tutorial on support vector regression
instead of normalizing by and ( cid : 123 ) .
( cid : 123 ) w ( cid : 123 ) 123 + c
( c ( i ) + c (
yi ( cid : 123 ) w , xi ( cid : 123 ) b + i
( cid : 123 ) w , xi ( cid : 123 ) + b yi +
again , by standard lagrange multiplier techniques , exactly in the same manner as in the|| case , one can compute the dual op - timization problem ( the main difference is that the slack variable terms c ( ( ) ) now have nonvanishing derivatives ) .
we will omit the indices i and , where applicable to avoid tedious notation .
j ) ( cid : 123 ) xi , x j ( cid : 123 )
i ) ( j i ) ( i +
t ( i ) + t (
w = ( cid : 123 ) ( cid : 123 )
t ( ) : = c ( ) c ( )
c c ( ) = inf ( | c c )
i ) = 123
table 123
terms of the convex optimization problem depending on the choice of the loss function
c t ( )
in the second case ( ) we have
t ( ) =
and = inf ( | c ) = , which , in turn yields ( 123 , c ) .
combining both cases we have
( 123 , c ) and t ( ) = p 123
table 123 contains a summary of the various conditions on and formulas for t ( ) ( strictly speaking t ( ( ) ) ) for different cost functions . 123 note that the maximum slope of c determines the region of feasibility of , i . e .
s : = supr+ c ( ) < leads to compact intervals ( 123 , cs ) for .
this means that the inuence of a single pattern is bounded , leading to robust estimators ( huber 123 ) .
one can also observe experimentally that the performance of a sv machine depends signicantly on the cost function used ( muller et al .
123 , smola , scholkopf and muller a cautionary remark is necessary regarding the use of cost functions other than the - insensitive one .
unless ( cid : 123 ) = 123 we will lose the advantage of a sparse decomposition .
this may be acceptable in the case of few data , but will render the pre - diction step extremely slow otherwise .
hence one will have to trade off a potential loss in prediction accuracy with faster pre - dictions .
note , however , that also a reduced set algorithm like in burges ( 123 ) , burges and scholkopf ( 123 ) and scholkopf et al .
( 123b ) or sparse decomposition techniques ( smola and scholkopf 123 ) could be applied to address this issue .
in a bayesian setting , tipping ( 123 ) has recently shown how an l 123 cost function can be used without sacricing sparsity .
the bigger picture
before delving into algorithmic details of the implementation let us briey review the basic properties of the sv algorithm for regression as described so far .
figure 123 contains a graphical overview over the different steps in the regression stage .
the input pattern ( for which a prediction is to be made ) is mapped into feature space by a map .
then dot products are computed with the images of the training patterns under
let us consider the examples of table 123
we will show explicitly for two examples how ( 123 ) can be further simplied to bring it into a form that is practically useful .
in the - insensitive case , i . e .
c ( ) = || we get
t ( ) = 123 = 123
morover one can conclude from c ( ) = 123 that
= inf ( | c ) = 123
and ( 123 , c ) .
for the case of piecewise polynomial loss we have to distinguish two different cases : and > .
in the rst case we get
t ( ) = 123
p = p 123
and = inf ( | c 123 p p123 ) = c
t ( ) = p 123
p123 and thus
architecture of a regression machine constructed by the sv
the map .
this corresponds to evaluating kernel functions k ( xi , x ) .
finally the dot products are added up using the weights i = i i .
this , plus the constant term b yields the nal prediction output .
the process described here is very similar to regression in a neural network , with the difference , that in the sv case the weights in the input layer are a subset of the training
figure 123 demonstrates how the sv algorithm chooses the attest function among those approximating the original data with a given precision .
although requiring atness only in feature space , one can observe that the functions also are very at in input space .
this is due to the fact , that ker - nels can be associated with atness properties via regular -
smola and scholkopf
ization operators .
this will be explained in more detail in
finally fig .
123 shows the relation between approximation qual - ity and sparsity of representation in the sv case .
the lower the precision required for approximating the original data , the fewer svs are needed to encode that .
the non - svs are redundant , i . e .
even without these patterns in the training set , the sv machine would have constructed exactly the same function f .
one might think that this could be an efcient way of data compression , namely by storing only the support patterns , from which the es - timate can be reconstructed completely .
however , this simple analogy turns out to fail in the case of high - dimensional data , and even more drastically in the presence of noise .
in vapnik , golowich and smola ( 123 ) one can see that even for moderate approximation quality , the number of svs can be considerably high , yielding rates worse than the nyquist rate ( nyquist 123 ,
optimization algorithms
while there has been a large number of implementations of sv algorithms in the past years , we focus on a few algorithms which will be presented in greater detail .
this selection is somewhat biased , as it contains these algorithms the authors are most fa - miliar with .
however , we think that this overview contains some of the most effective ones and will be useful for practitioners who would like to actually code a sv machine by themselves .
but before doing so we will briey cover major optimization packages and strategies .
left to right : approximation of the function sinc x with precisions = 123 , 123 , and 123 .
the solid top and the bottom lines indicate the size of the - tube , the dotted line in between is the regression
left to right : regression ( solid line ) , datapoints ( small dots ) and svs ( big dots ) for an approximation with = 123 , 123 , and 123 .
note the decrease in the number of svs
a tutorial on support vector regression
most commercially available packages for quadratic program - ming can also be used to train sv machines .
these are usually numerically very stable general purpose codes , with special en - hancements for large sparse systems .
while the latter is a feature that is not needed at all in sv problems ( there the dot product matrix is dense and huge ) they still can be used with good suc -
osl : this package was written by ibm - corporation ( 123 ) .
it uses a two phase algorithm .
the rst step consists of solving a linear approximation of the qp problem by the simplex al - gorithm ( dantzig 123 ) .
next a related very simple qp prob - lem is dealt with .
when successive approximations are close enough together , the second subalgorithm , which permits a quadratic objective and converges very rapidly from a good starting value , is used .
recently an interior point algorithm was added to the software suite .
cplex by cplex - optimization - inc .
( 123 ) uses a primal - dual logarithmic barrier algorithm ( megiddo 123 ) instead with predictor - corrector step ( see e . g .
lustig , marsten and shanno 123 , mehrotra and sun 123 ) .
minos by the stanford optimization laboratory ( murtagh and saunders 123 ) uses a reduced gradient algorithm in con - junction with a quasi - newton algorithm .
the constraints are handled by an active set strategy .
feasibility is maintained throughout the process .
on the active constraint manifold , a quasi - newton approximation is used .
matlab : until recently the matlab qp optimizer delivered only agreeable , although below average performance on classi - cation tasks and was not all too useful for regression tasks ( for problems much larger than 123 samples ) due to the fact that one is effectively dealing with an optimization prob - lem of size 123 ( cid : 123 ) where at least half of the eigenvalues of the hessian vanish .
these problems seem to have been addressed in version 123 / r123
matlab now uses interior point codes .
loqo by vanderbei ( 123 ) is another example of an interior point code .
section 123 discusses the underlying strategies in detail and shows how they can be adapted to sv algorithms .
maximum margin perceptron by kowalczyk ( 123 ) is an algo - rithm specically tailored to svs .
unlike most other tech - niques it works directly in primal space and thus does not have to take the equality constraint on the lagrange multipli - ers into account explicitly .
iterative free set methods the algorithm by kaufman ( bunch , kaufman and parlett 123 , bunch and kaufman 123 , 123 , drucker et al .
123 , kaufman 123 ) , uses such a technique starting with all variables on the boundary and adding them as the karush kuhn tucker conditions become more violated .
this approach has the advantage of not having to compute the full dot product matrix from the beginning .
instead it is evaluated on the y , yielding a performance improvement in comparison to tackling the whole optimization problem at once .
however , also other algorithms can be modied by
subset selection techniques ( see section 123 ) to address this
basic notions
most algorithms rely on results from the duality theory in convex optimization .
although we already happened to mention some basic ideas in section 123 we will , for the sake of convenience , briey review without proof the core results .
these are needed in particular to derive an interior point algorithm .
for details and proofs ( see e . g .
fletcher 123 ) .
uniqueness : every convex constrained optimization problem has a unique minimum .
if the problem is strictly convex then the solution is unique .
this means that svs are not plagued with the problem of local minima as neural networks are . 123 lagrange function : the lagrange function is given by the pri - mal objective function minus the sum of all products between constraints and corresponding lagrange multipliers ( cf .
fletcher 123 , bertsekas 123 ) .
optimization can be seen as minimzation of the lagrangian wrt .
the primal variables and simultaneous maximization wrt .
the lagrange multipli - ers , i . e .
dual variables .
it has a saddle point at the solution .
usually the lagrange function is only a theoretical device to derive the dual objective function ( cf .
section 123 ) .
dual objective function : it
is derived by minimizing the lagrange function with respect to the primal variables and subsequent elimination of the latter .
hence it can be written solely in terms of the dual variables .
duality gap : for both feasible primal and dual variables the pri - mal objective function ( of a convex minimization problem ) is always greater or equal than the dual objective function .
since svms have only linear constraints the constraint qual - ications of the strong duality theorem ( bazaraa , sherali and shetty 123 , theorem 123 . 123 ) are satised and it follows that gap vanishes at optimality .
thus the duality gap is a measure how close ( in terms of the objective function ) the current set of variables is to the solution .
karushkuhntucker ( kkt ) conditions : a set of primal and dual variables that is both feasible and satises the kkt conditions is the solution ( i . e .
constraint dual variable = 123 ) .
the sum of the violated kkt terms determines exactly the size of the duality gap ( that is , we simply compute the constraint lagrangemultiplier part as done in ( 123 ) ) .
this allows us to compute the latter quite easily .
a simple intuition is that for violated constraints the dual variable could be increased arbitrarily , thus rendering the lagrange function arbitrarily large .
this , however , is in con - tradition to the saddlepoint property .
interior point algorithms
in a nutshell the idea of an interior point algorithm is to com - pute the dual of the optimization problem ( in our case the dual dual of rreg ( f ) ) and solve both primal and dual simultaneously .
this is done by only gradually enforcing the kkt conditions
to iteratively nd a feasible solution and to use the duality gap between primal and dual objective function to determine the quality of the current set of variables .
the special avour of algorithm we will describe is primal - dual path - following
in order to avoid tedious notation we will consider the slightly more general problem and specialize the result to the svm later .
it is understood that unless stated otherwise , variables like denote vectors and i denotes its i - th component .
q ( ) + ( cid : 123 ) c , ( cid : 123 )
subject to a = b
and l u
with c , , l , u rn , a rnm , b rm , the inequalities be - tween vectors holding componentwise and q ( ) being a convex function of .
now we will add slack variables to get rid of all inequalities but the positivity constraints .
this yields :
subject to a = b , g = l , + t = u ,
q ( ) + ( cid : 123 ) c , ( cid : 123 )
g , t 123 , free
the dual of ( 123 ) is
( q ( ) ( cid : 123 ) ( cid : 123 ) q ( ) , ) ( cid : 123 ) + ( cid : 123 ) b , y ( cid : 123 ) + ( cid : 123 ) l , z ( cid : 123 ) ( cid : 123 ) u , s ( cid : 123 ) ( cid : 123 ) q ( ) + c ( ay ) ( cid : 123 ) + s = z , s , z 123 , y free
moreover we get the kkt conditions , namely
gi zi = 123 and
si ti = 123 for all i ( 123
a necessary and sufcient condition for the optimal solution is that the primal / dual variables satisfy both the feasibility condi - tions of ( 123 ) and ( 123 ) and the kkt conditions ( 123 ) .
we pro - ceed to solve ( 123 ) ( 123 ) iteratively .
the details can be found in
useful tricks
before proceeding to further algorithms for quadratic optimiza - tion let us briey mention some useful tricks that can be applied to all algorithms described subsequently and may have signif - icant impact despite their simplicity .
they are in part derived from ideas of the interior - point approach .
training with different regularization parameters : for several reasons ( model selection , controlling the number of support vectors , etc . ) it may happen that one has to train a sv ma - chine with different regularization parameters c , but other - wise rather identical settings .
if the parameters cnew = cold is not too different it is advantageous to use the rescaled val - ues of the lagrange multipliers ( i . e .
i , i ) as a starting point for the new optimization problem .
rescaling is necessary to satisfy the modied constraints .
one gets
new = old
and likewise bnew = bold .
smola and scholkopf
assuming that the ( dominant ) convex part q ( ) of the pri - mal objective is quadratic , the q scales with 123 where as the linear part scales with .
however , since the linear term dom - inates the objective function , the rescaled values are still a better starting point than = 123
in practice a speedup of approximately 123% of the overall training time can be ob - served when using the sequential minimization algorithm , cf .
( smola 123 ) .
a similar reasoning can be applied when retraining with the same regularization parameter but differ - ent ( yet similar ) width parameters of the kernel function .
see cristianini , campbell and shawe - taylor ( 123 ) for details thereon in a different context .
monitoring convergence via the feasibility gap : in the case of both primal and dual feasible variables the following con - nection between primal and dual objective function holds :
dual obj .
= primal obj .
( gi zi + si ti )
i gi zi + si ti
this can be seen immediately by the construction of the lagrange function .
in regression estimation ( with the - insensitive loss function ) one obtains for
+ max ( 123 , f ( xi ) ( yi + i ) ) ( c min ( 123 , f ( xi ) ( yi + i ) ) + max ( 123 , ( yi i ) f ( xi ) ) ( c i ) i ) f ( xi ) ) i
thus convergence with respect to the point of the solution can be expressed in terms of the duality gap .
an effective
stopping rule is to require ( cid : 123 )
i gi zi + si ti
|primal objective| + 123
for some precision tol .
this condition is much in the spirit of primal dual interior point path following algorithms , where convergence is measured in terms of the number of signicant gures ( which would be the decimal logarithm of ( 123 ) ) , a convention that will also be adopted in the subsequent parts of this exposition .
subset selection algorithms
the convex programming algorithms described so far can be used directly on moderately sized ( up to 123 ) samples datasets without any further modications .
on large datasets , however , it is difcult , due to memory and cpu limitations , to compute the dot product matrix k ( xi , x j ) and keep it in memory .
a simple calculation shows that for instance storing the dot product matrix of the nist ocr database ( 123 samples ) at single precision would consume 123 gbytes .
a cholesky decomposition thereof , which would additionally require roughly the same amount of memory and 123 teraops ( counting multiplies and adds sepa - rately ) , seems unrealistic , at least at current processor speeds .
a rst solution , which was introduced in vapnik ( 123 ) relies on the observation that the solution can be reconstructed from the svs alone .
hence , if we knew the sv set beforehand , and
a tutorial on support vector regression
it tted into memory , then we could directly solve the reduced problem .
the catch is that we do not know the sv set before solving the problem .
the solution is to start with an arbitrary subset , a rst chunk that ts into memory , train the sv algorithm on it , keep the svs and ll the chunk up with data the current estimator would make errors on ( i . e .
data lying outside the - tube of the current regression ) .
then retrain the system and keep on iterating until after training all kkt - conditions are satised .
the basic chunking algorithm just postponed the underlying problem of dealing with large datasets whose dot - product matrix cannot be kept in memory : it will occur for larger training set sizes than originally , but it is not completely avoided .
hence the solution is osuna , freund and girosi ( 123 ) to use only a subset of the variables as a working set and optimize the problem with respect to them while freezing the other variables .
this method is described in detail in osuna , freund and girosi ( 123 ) , joachims ( 123 ) and saunders et al .
( 123 ) for the case of pattern
an adaptation of these techniques to the case of regression with convex cost functions can be found in appendix b .
the basic structure of the method is described by algorithm 123
algorithm 123 : basic structure of a working set algorithm
initialize i , choose arbitrary working set sw
compute coupling terms ( linear and constant ) for sw ( see solve reduced optimization problem choose new sw from variables i ,
i not satisfying the
until working set sw =
sequential minimal optimization
recently an algorithmsequential minimal optimization ( smo ) was proposed ( platt 123 ) that puts chunking to the extreme by iteratively selecting subsets only of size 123 and op - timizing the target function with respect to them .
it has been reported to have good convergence properties and it is easily implemented .
the key point is that for a working set of 123 the optimization subproblem can be solved analytically without ex - plicitly invoking a quadratic optimizer .
while readily derived for pattern recognition by platt ( 123 ) , one simply has to mimick the original reasoning to obtain an extension to regression estimation .
this is what will be done in appendix c ( the pseudocode can be found in smola and scholkopf ( 123b ) ) .
the modications consist of a pattern de - pendent regularization , convergence control via the number of signicant gures , and a modied system of equations to solve the optimization problem in two variables for regression analyt -
note that the reasoning only applies to sv regression with the insensitive loss functionfor most other convex cost func -
tions an explicit solution of the restricted quadratic programming problem is impossible .
yet , one could derive an analogous non - quadratic convex optimization problem for general cost func - tions but at the expense of having to solve it numerically .
the exposition proceeds as follows : rst one has to derive the ( modied ) boundary conditions for the constrained 123 indices ( i , j ) subproblem in regression , next one can proceed to solve the optimization problem analytically , and nally one has to check , which part of the selection rules have to be modied to make the approach work for regression .
since most of the content is fairly technical it has been relegated to appendix c .
the main difference in implementations of smo for regres - sion can be found in the way the constant offset b is determined ( keerthi et al .
123 ) and which criterion is used to select a new set of variables .
we present one such strategy in appendix c . 123
however , since selection strategies are the focus of current re - search we recommend that readers interested in implementing the algorithm make sure they are aware of the most recent de - velopments in this area .
finally , we note that just as we presently describe a generaliza - tion of smo to regression estimation , other learning problems can also benet from the underlying ideas .
recently , a smo algorithm for training novelty detection systems ( i . e .
one - class classication ) has been proposed ( scholkopf et al .
123 ) .
variations on a theme
there exists a large number of algorithmic modications of the sv algorithm , to make it suitable for specic settings ( inverse problems , semiparametric settings ) , different ways of measuring capacity and reductions to linear programming ( convex com - binations ) and different ways of controlling capacity .
we will mention some of the more popular ones .
convex combinations and ( cid : 123 ) 123 - norms
all the algorithms presented so far involved convex , and at best , quadratic programming .
yet one might think of reducing the problem to a case where linear programming techniques can be applied .
this can be done in a straightforward fashion ( mangasarian 123 , 123 , weston et al .
123 , smola , scholkopf and ratsch 123 ) for both sv pattern recognition and regression .
the key is to replace ( 123 ) by
rreg ( f ) : = remp ( f ) + ( cid : 123 ) ( cid : 123 ) 123
where ( cid : 123 ) ( cid : 123 ) 123 denotes the ( cid : 123 ) 123 norm in coefcient space .
hence one uses the sv kernel expansion ( 123 )
with a different way of controlling capacity by minimizing
rreg ( f ) = 123
c ( xi , yi , f ( xi ) ) +
i k ( xi , x ) + b
f ( x ) = ( cid : 123 ) ( cid : 123 )
smola and scholkopf
for the - insensitive loss function this leads to a linear program - ming problem .
in the other cases , however , the problem still stays a quadratic or general convex one , and therefore may not yield the desired computational advantage .
therefore we will limit ourselves to the derivation of the linear programming problem in the case of | | cost function .
reformulating ( 123 ) yields
i ) + c
j ) k ( x j , xi ) b + i j ) k ( x j , xi ) + b yi +
unlike in the classical sv case , the transformation into its dual does not give any improvement in the structure of the optimiza - tion problem .
hence it is best to minimize rreg ( f ) directly , which can be achieved by a linear optimizer , ( e . g .
dantzig 123 , lustig , marsten and shanno 123 , vanderbei 123 ) .
in ( weston et al .
123 ) a similar variant of the linear sv ap - proach is used to estimate densities on a line .
one can show ( smola et al .
123 ) that one may obtain bounds on the gener - alization error which exhibit even better rates ( in terms of the entropy numbers ) than the classical sv case ( williamson , smola and scholkopf 123 ) .
automatic tuning of the insensitivity tube
besides standard model selection issues , i . e .
how to specify the trade - off between empirical error and model capacity there also exists the problem of an optimal choice of a cost function .
in particular , for the - insensitive cost function we still have the problem of choosing an adequate parameter in order to achieve good performance with the sv machine .
smola et al .
( 123a ) show the existence of a linear depen - dency between the noise level and the optimal - parameter for sv regression .
however , this would require that we know some - thing about the noise model .
this knowledge is not available in general .
therefore , albeit providing theoretical insight , this nd - ing by itself is not particularly useful in practice .
moreover , if we really knew the noise model , we most likely would not choose the - insensitive cost function but the corresponding maximum likelihood loss function instead .
there exists , however , a method to construct sv machines that automatically adjust and moreover also , at least asymptot - ically , have a predetermined fraction of sampling points as svs ( scholkopf et al .
we modify ( 123 ) such that becomes a variable of the optimization problem , including an extra term in the primal objective function which attempts to minimize
minimize r ( f ) : = remp ( f ) +
( cid : 123 ) w ( cid : 123 ) 123 + c
( cid : 123 ) w , xi ( cid : 123 ) + b yi +
yi ( cid : 123 ) w , xi ( cid : 123 ) b + i
i ) = 123 i ) c ( cid : 123 )
i ) ( j
j ) k ( xi , x j )
for some > 123
hence ( 123 ) becomes ( again carrying out the usual transformation between , ( cid : 123 ) and c )
( c ( i ) + c (
i ) ) + ( cid : 123 )
we consider the standard || loss function .
computing the dual of ( 123 ) yields
note that the optimization problem is thus very similar to the - sv one : the target function is even simpler ( it is homogeneous ) , but there is an additional constraint .
for information on how this affects the implementation ( cf .
chang and lin 123 ) .
besides having the advantage of being able to automatically determine ( 123 ) also has another advantage .
it can be used to prespecify the number of svs :
theorem 123 ( scholkopf et al .
123 ) .
is an upper bound on the fraction of errors .
is a lower bound on the fraction of svs .
suppose the data has been generated iid from a distribution p ( x , y ) = p ( x ) p ( y | x ) with a continuous conditional distri - bution p ( y | x ) .
with probability 123 , asymptotically , equals the fraction of svs and the fraction of errors .
essentially , - sv regression improves upon - sv regression by allowing the tube width to adapt automatically to the data .
what is kept xed up to this point , however , is the shape of the tube .
one can , however , go one step further and use parametric tube models with non - constant width , leading to almost identical op - timization problems ( scholkopf et al .
123 ) .
combining - sv regression with results on the asymptotical optimal choice of for a given noise model ( smola et al .
123a ) leads to a guideline how to adjust provided the class of noise models ( e . g .
gaussian or laplacian ) is known .
remark 123 ( optimal choice of ) .
denote by p a probability density with unit variance , and by p a famliy of noise models generated from p by p : = ( p| p = 123 ) ) .
moreover assume
a tutorial on support vector regression
by making problems seemingly easier yet reliable via a map into some even higher dimensional space .
in this section we focus on the connections between sv methods and previous techniques like regularization networks ( girosi , jones and poggio 123 ) . 123 in particular we will show that sv machines are essentially regularization networks ( rn ) with a clever choice of cost functions and that the kernels are greens function of the corresponding regularization operators .
for a full exposition of the subject the reader is referred to smola , scholkopf and muller ( 123c ) .
regularization networks
let us briey review the basic concepts of rns .
as in ( 123 ) we minimize a regularized risk functional .
however , rather than enforcing atness in feature space we try to optimize some smoothness criterion for the function in input space .
thus we
rreg ( f ) : = remp ( f ) +
( cid : 123 ) p f ( cid : 123 ) 123
here p denotes a regularization operator in the sense of tikhonov and arsenin ( 123 ) , i . e .
p is a positive semidenite operator mapping from the hilbert space h of functions f under consideration to a dot product space d such that the expression ( cid : 123 ) p f pg ( cid : 123 ) is well dened for f , g h .
for instance by choos - ing a suitable operator that penalizes large variations of f one can reduce the wellknown overtting effect .
another possible setting also might be an operator p mapping from l 123 ( rn ) into some reproducing kernel hilbert space ( rkhs ) ( aronszajn , 123 , kimeldorf and wahba 123 , saitoh 123 , scholkopf 123 ,
using an expansion of f in terms of some symmetric function k ( xi , x j ) ( note here , that k need not fulll mercers condition and can be chosen arbitrarily since it is not used to dene a
f ( x ) = ( cid : 123 ) ( cid : 123 )
i k ( xi , x ) + b ,
and the - insensitive cost function , this leads to a quadratic pro - gramming problem similar to the one for svs
being the solution of
di j : = ( cid : 123 ) ( pk ) ( xi , . ) ( pk ) ( x j , . ) ( cid : 123 ) 123 k ( ) , with , 123 k ( ) and i ,
i ) = 123
we get = d
optimal and for various degrees of polynomial additive
that the data were drawn iid from p ( x , y ) = p ( x ) p ( y f ( x ) ) with p ( y f ( x ) ) continuous .
then under the assumption of uniform convergence , the asymptotically optimal value of is
( p ( ) + p ( ) )
for polynomial noise models , i . e .
densities of type exp ( || p ) one may compute the corresponding ( asymptotically ) optimal values of .
they are given in fig .
for further details see ( scholkopf et al .
123 , smola 123 ) ; an experimental validation has been given by chalimourda , scholkopf and smola ( 123 ) .
we conclude this section by noting that - sv regression is related to the idea of trimmed estimators .
one can show that the regression is not inuenced if we perturb points lying outside the tube .
thus , the regression is essentially computed by discarding a certain fraction of outliers , specied by , and computing the regression estimate from the remaining points ( scholkopf et al .
so far we were not concerned about the specic properties of the map into feature space and used it only as a convenient trick to construct nonlinear regression functions .
in some cases the map was just given implicitly by the kernel , hence the map itself and many of its properties have been neglected .
a deeper understanding of the kernel map would also be useful to choose appropriate kernels for a specic task ( e . g .
by incorporating prior knowledge ( scholkopf et al .
123a ) ) .
finally the feature map seems to defy the curse of dimensionality ( bellman 123 )
unfortunately , this setting of the problem does not preserve spar - sity in terms of the coefcients , as a potentially sparse decom - position in terms of i and 123 k , which is not in general diagonal .
i is spoiled by d
greens functions
comparing ( 123 ) with ( 123 ) leads to the question whether and un - der which condition the two methods might be equivalent and therefore also under which conditions regularization networks might lead to sparse decompositions , i . e .
only a few of the ex - pansion coefcients i in f would differ from zero .
a sufcient condition is d = k and thus kd 123 k = k ( if k does not have 123 k = k holds on the image of full rank we only need that kd
k ( xi , x j ) = ( cid : 123 ) ( pk ) ( xi , . ) ( pk ) ( x j , . ) ( cid : 123 )
our goal now is to solve the following two problems :
given a regularization operator p , nd a kernel k such that a sv machine using k will not only enforce atness in feature space , but also correspond to minimizing a regularized risk functional with p as regularizer .
given an sv kernel k , nd a regularization operator p such that a sv machine using this kernel can be viewed as a reg - ularization network using p .
these two problems can be solved by employing the concept of greens functions as described in girosi , jones and poggio ( 123 ) .
these functions were introduced for the purpose of solv - ing differential equations .
in our context it is sufcient to know that the greens functions g xi ( x ) of p
pg xi ) ( x ) = xi ( x ) .
here , xi ( x ) is the - distribution ( not to be confused with the kro - necker symbol i j ) which has the property that ( cid : 123 ) f xi ( cid : 123 ) = f ( xi ) .
the relationship between kernels and regularization operators is formalized in the following proposition :
proposition 123 ( smola , scholkopf and muller 123b ) .
let p be a regularization operator , and g be the greens function of p .
then g is a mercer kernel such that d = k .
sv machines using g minimize risk functional ( 123 ) with p as regularization
in the following we will exploit this relationship in both ways : to compute greens functions for a given regularization operator p and to infer the regularizer , given a kernel k .
translation invariant kernels
let us now more specically consider regularization operators p that may be written as multiplications in fourier space
( cid : 123 ) p f pg ( cid : 123 ) =
f ( ) g ( )
smola and scholkopf with f ( ) denoting the fourier transform of f ( x ) , and p ( ) = p ( ) real valued , nonnegative and converging to 123 for || and : = supp ( p ( ) ) .
small values of p ( ) correspond to a strong attenuation of the corresponding frequencies .
hence small values of p ( ) for large are desirable since high fre - f correspond to rapid changes in f .
quency components of p ( ) describes the lter properties of p p .
note that no atten - uation takes place for p ( ) = 123 as these frequencies have been excluded from the integration domain .
one can show by exploiting p ( ) = p ( ) = p ( ) that
for regularization operators dened in fourier space by ( 123 )
g ( xi , x ) =
ei ( xix ) p ( ) d
is a corresponding greens function satisfying translational in -
g ( xi , x j ) = g ( xi x j )
g ( ) = p ( ) .
this provides us with an efcient tool for analyzing sv kernels and the types of capacity control they exhibit .
in fact the above is a special case of bochners theorem ( bochner 123 ) stating that the fourier transform of a positive measure constitutes a positive hilbert schmidt kernel .
example 123 ( gaussian kernels ) .
following the exposition of yuille and grzywacz ( 123 ) as described in girosi , jones and poggio ( 123 ) , one can see that for
( cid : 123 ) p f ( cid : 123 ) 123 =
m ! 123m ( o m f ( x ) ) 123
with o 123m = m and o 123m+123 = m , being the laplacian and the gradient operator , we get gaussians kernels ( 123 ) .
moreover , we can provide an equivalent representation of p in terms of its fourier properties , i . e .
p ( ) = e up to a
training an sv machine with gaussian rbf kernels ( scholkopf et al .
123 ) corresponds to minimizing the specic cost func - tion with a regularization operator of type ( 123 ) .
recall that ( 123 ) means that all derivatives of f are penalized ( we have a pseudod - ifferential operator ) to obtain a very smooth estimate .
this also explains the good performance of sv machines in this case , as it is by no means obvious that choosing a at function in some high dimensional space will correspond to a simple function in low dimensional space , as shown in smola , scholkopf and muller ( 123c ) for dirichlet kernels .
the question that arises now is which kernel to choose
us think about two extreme situations .
suppose we already knew the shape of the power spectrum pow ( ) of the function we would like to estimate .
in this case we choose k such that k matches the power spectrum ( smola
if we happen to know very little about the given data a gen - eral smoothness assumption is a reasonable choice
a tutorial on support vector regression
we might want to choose a gaussian kernel .
if computing time is important one might moreover consider kernels with compact support , e . g .
using the bqspline kernels ( cf .
this choice will cause many matrix elements ki j = k ( xix j )
the usual scenario will be in between the two extreme cases and we will have some limited prior knowledge available .
for more information on using prior knowledge for choosing kernels ( see scholkopf et al .
123a ) .
capacity control
all the reasoning so far was based on the assumption that there exist ways to determine model parameters like the regularization constant or length scales of rbfkernels .
the model selec - tion issue itself would easily double the length of this review and moreover it is an area of active and rapidly moving research .
therefore we limit ourselves to a presentation of the basic con - cepts and refer the interested reader to the original publications .
it is important to keep in mind that there exist several fun - damentally different approaches such as minimum description length ( cf .
rissanen 123 , li and vitanyi 123 ) which is based on the idea that the simplicity of an estimate , and therefore also its plausibility is based on the information ( number of bits ) needed to encode it such that it can be reconstructed .
bayesian estimation , on the other hand , considers the pos - terior probability of an estimate , given the observations x = ( ( x123 , y123 ) , .
( x ( cid : 123 ) , y ( cid : 123 ) ) ) , an observation noise model , and a prior probability distribution p ( f ) over the space of estimates is given by bayes rule p ( f | x ) p ( x ) = p ( x | f ) p ( f ) .
since p ( x ) does not depend on f , one can maxi - mize p ( x | f ) p ( f ) to obtain the so - called map estimate . 123 as a rule of thumb , to translate regularized risk functionals into bayesian map estimation schemes , all one has to do is to con - sider exp ( rreg ( f ) ) = p ( f | x ) .
for a more detailed discussion ( see e . g .
kimeldorf and wahba 123 , mackay 123 , neal 123 , rasmussen 123 , williams 123 ) .
a simple yet powerful way of model selection is cross valida - tion .
this is based on the idea that the expectation of the error on a subset of the training sample not used during training is identical to the expected error itself .
there exist several strate - gies such as 123 - fold crossvalidation , leave - one out error ( ( cid : 123 ) - fold crossvalidation ) , bootstrap and derived algorithms to estimate the crossvalidation error itself ( see e . g .
stone 123 , wahba 123 , efron 123 , efron and tibshirani 123 , wahba 123 , jaakkola and haussler 123 ) for further details .
finally , one may also use uniform convergence bounds such as the ones introduced by vapnik and chervonenkis ( 123 ) .
the basic idea is that one may bound with probability 123 ( with > 123 ) the expected risk r ( f ) by remp ( f ) + ( f , ) , where is a condence term depending on the class of functions f .
several criteria for measuring the capacity off exist , such as the vc - dimension which , in pattern recognition problems , is given by the maximum number of points that can be separated by the
function class in all possible ways , the covering number which is the number of elements fromf that are needed to coverf with accuracy of at least , entropy numbers which are the functional inverse of covering numbers , and many more variants thereof ( see e . g .
vapnik 123 , 123 , devroye , gyor and lugosi 123 , williamson , smola and scholkopf 123 , shawe - taylor et al .
due to the already quite large body of work done in the eld of sv research it is impossible to write a tutorial on sv regression which includes all contributions to this eld .
this also would be quite out of the scope of a tutorial and rather be relegated to textbooks on the matter ( see scholkopf and smola ( 123 ) for a comprehensive overview , scholkopf , burges and smola ( 123a ) for a snapshot of the current state of the art , vapnik ( 123 ) for an overview on statistical learning theory , or cristianini and shawe - taylor ( 123 ) for an introductory textbook ) .
still the authors hope that this work provides a not overly biased view of the state of the art in sv regression research .
we deliberately omitted ( among others ) the following topics .
missing topics
mathematical programming : starting from a completely differ - ent perspective algorithms have been developed that are sim - ilar in their ideas to sv machines .
a good primer might be ( bradley , fayyad and mangasarian 123 ) .
( also see mangasarian 123 , 123 , street and mangasarian 123 ) .
a comprehensive discussion of connections between mathe - matical programming and sv machines has been given by
density estimation : with sv machines ( weston et al .
123 , vapnik 123 ) .
there one makes use of the fact that the cu - mulative distribution function is monotonically increasing , and that its values can be predicted with variable condence which is adjusted by selecting different values of in the loss
dictionaries : were originally introduced in the context of wavelets by ( chen , donoho and saunders 123 ) to allow for a large class of basis functions to be considered simulta - neously , e . g .
kernels with different widths .
in the standard sv case this is hardly possible except by dening new kernels as linear combinations of differently scaled ones : choosing the regularization operator already determines the kernel com - pletely ( kimeldorf and wahba 123 , cox and osullivan 123 , scholkopf et al .
hence one has to resort to lin - ear programming ( weston et al .
123 ) .
applications : the focus of this review was on methods and theory rather than on applications .
this was done to limit the size of the exposition .
state of the art , or even record performance was reported in muller et al .
( 123 ) , drucker et al .
( 123 ) , stitson et al .
( 123 ) and mattera and haykin
in many cases , it may be possible to achieve similar per - formance with neural network methods , however , only if many parameters are optimally tuned by hand , thus depend - ing largely on the skill of the experimenter .
certainly , sv machines are not a silver bullet .
however , as they have only few critical parameters ( e . g .
regularization and kernel width ) , state - of - the - art results can be achieved with relatively
open issues
being a very active eld there exist still a number of open is - sues that have to be addressed by future research .
after that the algorithmic development seems to have found a more sta - ble stage , one of the most important ones seems to be to nd tight error bounds derived from the specic properties of ker - nel functions .
it will be of interest in this context , whether sv machines , or similar approaches stemming from a lin - ear programming regularizer , will lead to more satisfactory
moreover some sort of luckiness framework ( shawe - taylor et al .
123 ) for multiple model selection parameters , similar to multiple hyperparameters and automatic relevance detection in bayesian statistics ( mackay 123 , bishop 123 ) , will have to be devised to make sv machines less dependent on the skill of
it is also worth while to exploit the bridge between regulariza - tion operators , gaussian processes and priors ( see e . g .
( williams 123 ) ) to state bayesian risk bounds for sv machines in order to compare the predictions with the ones from vc theory .
op - timization techniques developed in the context of sv machines also could be used to deal with large datasets in the gaussian
prior knowledge appears to be another important question in sv regression .
whilst invariances could be included in pattern recognition in a principled way via the virtual sv mechanism and restriction of the feature space ( burges and scholkopf 123 , scholkopf et al .
123a ) , it is still not clear how ( probably ) more subtle properties , as required for regression , could be dealt with
reduced set methods also should be considered for speeding up prediction ( and possibly also training ) phase for large datasets ( burges and scholkopf 123 , osuna and girosi 123 , scholkopf et al .
123b , smola and scholkopf 123 ) .
this topic is of great importance as data mining applications require algorithms that are able to deal with databases that are often at least one order of magnitude larger ( 123 million samples ) than the current practical size for sv regression .
many more aspects such as more data dependent generaliza - tion bounds , efcient training algorithms , automatic kernel se - lection procedures , and many techniques that already have made their way into the standard neural networks toolkit , will have to be considered in the future .
readers who are tempted to embark upon a more detailed exploration of these topics , and to contribute their own ideas to
smola and scholkopf
this exciting eld , may nd it useful to consult the web page
appendix a : solving the interior - point
path following
rather than trying to satisfy ( 123 ) directly we will solve a modied version thereof for some > 123 substituted on the rhs in the rst place and decrease while iterating .
gi zi = ,
si ti = for all i ( 123
still it is rather difcult to solve the nonlinear system of equa - tions ( 123 ) , ( 123 ) , and ( 123 ) exactly .
however we are not interested in obtaining the exact solution to the approximation ( 123 ) .
in - stead , we seek a somewhat more feasible solution for a given , then decrease and repeat .
this can be done by linearizing the above system and solving the resulting equations by a predictor corrector approach until the duality gap is small enough .
the advantage is that we will get approximately equal performance as by trying to solve the quadratic system directly , provided that the terms in 123 are small enough .
a ( + ) = b + g g = l + + t + t = u c + 123 + s + s = z + z ( gi + gi ) ( zi + zi ) = ( si + si ) ( ti + ti ) =
q ( ) + 123
q ( ) ( a ( y + y ) )
solving for the variables in we get
a = b a = : g = l + g = : + t = u t = :
( cid : 123 ) + z s 123
( cid : 123 ) + s z + 123
= c ( ay ) 123zg + z = g 123st + s = t 123 denotes the vector ( 123 / g123 , .
, 123 / gn ) , and t analo - 123s the vector generated gously .
moreover denote g by the componentwise product of the two vectors .
solving for
123gz = : z 123t s = : s
123 z g 123 s t
123z and t
and subsequently restricts the solution to a feasible set
x = max g = min ( l , u ) t = min ( u , u ) z = min
q ( ) + c ( ay )
s = min
q ( ) c + ( ay )
( ) denotes the heavyside function , i . e .
( x ) = 123 for x > 123 and ( x ) = 123 otherwise .
a tutorial on support vector regression
g , t , z , s we get
g = z t = s
123g ( z z ) z = g 123t ( s s ) s = t
now we can formulate the reduced kktsystem ( see ( vanderbei 123 ) for the quadratic case ) :
where h : = ( 123
q ( ) + g
iteration strategies
123z + t
for the predictor - corrector method we proceed as follows .
in the predictor step solve the system of ( 123 ) and ( 123 ) with = 123 and all - terms on the rhs set to 123 , i . e .
z = z , s = s .
the values in are substituted back into the denitions for z and s and ( 123 ) and ( 123 ) are solved again in the corrector step .
as the quadratic part in ( 123 ) is not affected by the predictorcorrector steps , we only need to invert the quadratic matrix once .
this is done best by manually pivoting for the h part , as it is positive
next the values in obtained by such an iteration step are used to update the corresponding values in , s , t , z , .
to ensure that the variables meet the positivity constraints , the steplength is chosen such that the variables move at most 123 of their initial distance to the boundaries of the positive orthant .
usually ( vanderbei 123 ) one sets = 123 .
another heuristic is used for computing , the parameter de - termining how much the kkt - conditions should be enforced .
obviously it is our aim to reduce as fast as possible , however if we happen to choose it too small , the condition of the equa - tions will worsen drastically .
a setting that has proven to work
= ( cid : 123 ) g , z ( cid : 123 ) + ( cid : 123 ) s , t ( cid : 123 )
the rationale behind ( 123 ) is to use the average of the satisfac - tion of the kkt conditions ( 123 ) as point of reference and then decrease rapidly if we are far enough away from the bound - aries of the positive orthant , to which all variables ( except y ) are
finally one has to come up with good initial values .
analo - gously to vanderbei ( 123 ) we choose a regularized version of ( 123 ) in order to determine the initial conditions .
one solves
q ( ) + 123
special considerations for sv regression
the algorithm described so far can be applied to both sv pattern recognition and regression estimation .
for the standard setting in pattern recognition we have
i j yi y j k ( xi , x j )
and consequently i q ( ) = 123 , 123 the hessian is dense and the only thing we can do is compute its cholesky factorization to compute ( 123 ) .
in the case of sv re - gression , however we have ( with : = ( 123 , .
, ( cid : 123 ) , ,
i j q ( ) = yi y j k ( xi , x j ) , i . e .
q ( ) = ( cid : 123 ) ( cid : 123 )
q ( ) = ( cid : 123 ) ( cid : 123 )
i ) ( j
j ) k ( xi , x j )
t ( i ) + t (
t ( i )
i q ( ) = d i j q ( ) = k ( xi , x j ) + i j
t ( i )
( cid : 123 ) ) where d , d
q ( ) = k ( xi , x j ) q ( ) analogously .
hence we are dealing with
a matrix of type m : = ( k+d k matrices .
by applying an orthogonal transformation m can be inverted essentially by inverting an ( cid : 123 ) ( cid : 123 ) matrix instead of a 123 ( cid : 123 ) 123 ( cid : 123 ) system .
this is exactly the additional advantage one can gain from implementing the optimization algorithm directly instead of using a general purpose optimizer .
one can show that for practical implementations ( smola , scholkopf and muller 123b ) one can solve optimization problems using nearly ar - bitrary convex cost functions as efciently as the special case of - insensitive loss functions .
finally note that due to the fact that we are solving the pri - mal and dual optimization problem simultaneously we are also
computing parameters corresponding to the initial sv optimiza - tion problem .
this observation is useful as it allows us to obtain the constant term b directly , namely by setting b = y .
( see smola ( 123 ) for details ) .
at sample xi , i . e .
i : = yi f ( xi ) = yi
smola and scholkopf
k ( xi , x j ) ( i
i ) + b
appendix b : solving the subset selection
subset optimization problem
we will adapt the exposition of joachims ( 123 ) to the case of regression with convex cost functions .
without loss of general - ity we will assume ( cid : 123 ) = 123 and ( 123 , c ) ( the other situations can be treated as a special case ) .
first we will extract a reduced optimization problem for the working set when all other vari - ables are kept xed .
denote sw ( 123 , .
, ( cid : 123 ) ) the working set and s f : = ( 123 , .
, ( cid : 123 ) ) \sw the xed set .
writing ( 123 ) as an opti - mization problem only in terms of sw yields j ) ( cid : 123 ) xi , x j ( cid : 123 ) i ) ( j
j ) ( cid : 123 ) xi , x j ( cid : 123 ) i ) + c ( t ( i ) + t ( i ) =
i ( 123 , c ) with the xed set ( cid : 123 ) the equality constraint by ( cid : 123 )
hence we only have to update the linear term by the coupling j ) ( cid : 123 ) xi , x j ( cid : 123 ) and i ) .
it is easy to see that maximizing ( 123 ) also decreases ( 123 ) by exactly the same amount .
if we choose variables for which the kktconditions are not satised the overall objective function tends to decrease whilst still keeping all variables feasible .
finally it is bounded
even though this does not prove convergence ( contrary to statement in osuna , freund and girosi ( 123 ) ) this algorithm proves very useful in practice .
it is one of the few methods ( be - sides ( kaufman 123 , platt 123 ) ) that can deal with problems whose quadratic part does not completely t into memory .
still in practice one has to take special precautions to avoid stalling of convergence ( recent results of chang , hsu and lin ( 123 ) indicate that under certain conditions a proof of convergence is possible ) .
the crucial part is the one of sw .
a note on optimality
for convenience the kkt conditions are repeated in a slightly modied form .
denote i the error made by the current estimate
rewriting the feasibility conditions ( 123 ) in terms of yields
for all i ( 123 , .
, m ) with zi , z feasible variables z , s is given by
, si , s
i ) + + i + s
123i t ( i ) + i + si zi = 123 i t ( 123
a set of dual 123i t ( i ) + i , 123 123i t ( i ) + i , 123 i ) + + i , 123 i t ( i ) + + i , 123 i t (
zi = max si = min
consequently the kkt conditions ( 123 ) can be translated into
( c i ) si = 123
i zi = 123
all variables i , i violating some of the conditions of ( 123 ) may be selected for further optimization .
in most cases , especially in the initial stage of the optimization algorithm , this set of pat - terns is much larger than any practical size of sw .
unfortunately osuna , freund and girosi ( 123 ) contains little information on how to select sw .
the heuristics presented here are an adaptation of joachims ( 123 ) to regression .
see also lin ( 123 ) for details on optimization for svr .
selection rules
similarly to a merit function approach ( el - bakry et al .
123 ) the idea is to select those variables that violate ( 123 ) and ( 123 ) most , thus contribute most to the feasibility gap .
hence one denes a score variable i by
i : = gi zi + si ti = i zi +
+ ( c i ) si + ( c
i is the size of the feasibility gap ( cf .
( 123 ) for the case of - insensitive loss ) .
by decreasing this gap , one approaches the the solution ( upper bounded by the primal objec - tive and lower bounded by the dual objective function ) .
hence , the selection rule is to choose those patterns for which i is
a tutorial on support vector regression
largest .
some algorithms use
analytic solution for regression
i : = i ( zi ) +
+ ( c i ) ( si ) + ( c
i ) ( si )
: = ( i ) zi + (
+ ( c i ) si + ( c
i ) si .
= 123 mutually imply each one can see that i = 123 , ( cid : 123 ) other .
however , only i gives a measure for the contribution of the variable i to the size of the feasibility gap .
= 123 , and ( cid : 123 ) ( cid : 123 )
finally , note that heuristics like assigning stickyags ( cf .
burges 123 ) to variables at tively solving smaller subproblems , or completely removing the corresponding patterns from the training set while ac - counting for their couplings ( joachims 123 ) can signi - cantly decrease the size of the problem one has to solve and thus result in a noticeable speedup .
also caching ( joachims 123 , kowalczyk 123 ) of already computed entries of the dot product matrix may have a signicant impact on the
appendix c : solving the smo equations
pattern dependent regularization
consider the constrained optimization problem ( 123 ) for two in - dices , say ( i , j ) .
pattern dependent regularization means that ci may be different for every pattern ( possibly even different for i ) .
since at most two variables may become nonzero at the same time and moreover we are dealing with a con - strained optimization problem we may express everything in terms of just one variable .
from the summation constraint we
( l , h ) .
for regression .
exploiting ( ) this is taking account of the fact that there may be only four different pairs of nonzero variables : ( i , j ) , ( , j ) , ( i , j ) .
for convenience dene an auxiliary variables s such that s = 123 in the rst and the last case and s = 123 other -
i ) + ( j
j ) yields ( )
j ) = ( cid : 123 )
( 123 , c ( )
max ( 123 , c j ) min ( ci , )
, + c j )
min ( ci , c
next one has to solve the optimization problem analytically .
we make use of ( 123 ) and substitute the values of i into the reduced optimization problem ( 123 ) .
in particular we use
i ) ki j = i + b +
ki j .
moreover with the auxiliary variables = i : = ( kii + k j j 123ki j ) one obtains the following constrained optimization problem in i ( after eliminating j , ignoring terms independent of j , i ) 123 ( i + i ) ( 123 s )
i j + ( l ( ) , h ( ) ) .
j and noting that this only holds for i
the unconstrained maximum of ( 123 ) with respect to i or can be found below .
i , j
+ 123 ( i j ) + 123 ( i j 123 ) old 123 ( i j + 123 ) old 123 ( i j )
the problem is that we do not know beforehand which of the four quadrants ( i ) ( iv ) contains the solution .
however , by con - sidering the sign of we can distinguish two cases : for > 123 only ( i ) ( iii ) are possible , for < 123 the coefcients satisfy one of the cases ( ii ) ( iv ) .
in case of = 123 only ( ii ) and ( iii ) have to be considered .
see also the diagram below .
for > 123 it is best to start with quadrant ( i ) , test whether the unconstrained solution hits one of the boundaries l , h and if so , probe the corresponding adjacent quadrant ( ii ) or ( iii ) .
< 123 can be dealt with analogously .
due to numerical instabilities , it may happen that < 123
in that case should be set to 123 and one has to solve ( 123 ) in a linear
smola and scholkopf
selection rule for regression
finally , one has to pick indices ( i , j ) such that the objective function is maximized .
again , the reasoning of smo ( platt 123 , section 123 . 123 ) for classication will be mimicked .
this means that a two loop approach is chosen to maximize the objective function .
the outer loop iterates over all patterns violating the kkt conditions , rst only over those with lagrange multipliers neither on the upper nor lower boundary , and once all of them are satised , over all patterns violating the kkt conditions , to ensure self consistency on the complete dataset . 123 this solves the problem of choosing i .
now for j : to make a large step towards the minimum , one looks for large steps in i .
as it is computationally expensive to compute for all possible pairs ( i , j ) one chooses the heuristic to maximize the absolute value of the numerator in the expressions i , i . e .
|i j| and |i j 123| .
the index j for i and corresponding to the maximum absolute value is chosen for this
if this heuristic happens to fail , in other words if little progress is made by this choice , all other indices j are looked at ( this is what is called second choice hierarcy in platt ( 123 ) in the
all indices j corresponding to nonbound examples are
looked at , searching for an example to make progress on .
in the case that the rst heuristic was unsuccessful , all other samples are analyzed until an example is found where progress can be made .
if both previous steps fail proceed to the next i .
for a more detailed discussion ( see platt 123 ) .
unlike interior point algorithms smo does not automatically provide a value for b .
however this can be chosen like in section 123 by having a close look at the lagrange multipliers ( )
stopping criteria
by essentially minimizing a constrained primal optimization problem one cannot ensure that the dual objective function in - creases with every iteration step . 123 nevertheless one knows that the minimum value of the objective function lies in the interval , primal objectivei ) for all steps i , hence also in the interval ( ( max ji dual objective j ) , primal objectivei ) .
one uses the latter to determine the quality of the current solution .
the calculation of the primal objective function from the pre -
diction errors is straightforward .
one uses
this work has been supported in part by a grant of the dfg ( ja 123 / 123 , sm 123 / 123 ) .
the authors thank peter bartlett , chris burges , stefan harmeling , olvi mangasarian , klaus - robert muller , vladimir vapnik , jason weston , robert williamson , and andreas ziehe for helpful discussions and comments .
our use of the term regression is somewhat lose in that it also includes cases of function estimation where one minimizes errors other than the mean square loss .
this is done mainly for historical reasons ( vapnik , golowich and smola 123 ) .
a similar approach , however using linear instead of quadratic programming , was taken at the same time in the usa , mainly by mangasarian ( 123 , 123 ,
see smola ( 123 ) for an overview over other ways of specifying atness of
this is true as long as the dimensionality of w is much higher than the number of observations .
if this is not the case , specialized methods can offer considerable computational savings ( lee and mangasarian 123 ) .
the table displays ct ( ) instead of t ( ) since the former can be plugged
directly into the corresponding optimization equations .
the high price tag usually is the major deterrent for not using them .
moreover one has to bear in mind that in sv regression , one may speed up the solution considerably by exploiting the fact that the quadratic form has a special structure or that there may exist rank degeneracies in the kernel matrix
for large and noisy problems ( e . g .
123 patterns and more with a substan - tial fraction of nonbound lagrange multipliers ) it is impossible to solve the problem exactly : due to the size one has to use subset selection algorithms , hence joint optimization over the training set is impossible .
however , unlike in neural networks , we can determine the closeness to the optimum .
note that this reasoning only holds for convex cost functions .
a similar technique was employed by bradley and mangasarian ( 123 ) in
the context of linear programming in order to deal with large datasets .
due to length constraints we will not deal with the connection between gaussian processes and svms .
see williams ( 123 ) for an excellent
strictly speaking , in bayesian estimation one is not so much concerned about f of p ( f | x ) but rather about the posterior distribution of
condition : 123 ( cid : 123 ) ( xi ) ( x j ) ( cid : 123 ) 123 = kii + k j j 123ki j = .
negative values of are theoretically impossible since k satises mercers
it is sometimes useful , especially when dealing with noisy data , to iterate over the complete kkt violating dataset already before complete self con - sistency on the subset has been achieved .
otherwise much computational resources are spent on making subsets self consistent that are not globally self consistent .
this is the reason why in the pseudo code a global loop is initiated already when only less than 123% of the non bound variables
it is still an open question how a subset selection optimization algorithm could be devised that decreases both primal and dual objective function at the same time .
the problem is that this usually involves a number of dual variables of the order of the sample size , which makes this attempt
i ) ( j
j ) ki j =
i ) ( i + yi b ) ,
