indexing systems for the world wide web , such as lycos and alta vista , play an essential role in making the web useful and usable .
these systems are based on information retrieval methods for indexing plain text documents , but also include for adjusting their document rankings based on the special html structure of web doc - uments .
in this paper , we describe a wide range of such heuristics - - including a novel one inspired by reinforcement learning techniques for propagating rewards through a graph - - which can be used to affect a search engines rankings .
we then demon - to combine these heuristics automatically , based on feedback col - lected unintrusively from users , resulting in much
a system which learns
lycos ( mauldin & leavitt 123 ) , alta vista , and sim - ilar web search engines have become essential as tools information on the ever - growing world wide web .
underlying these systems are statistical text documents .
how - markup language ( html ) documents , which exhibit two kinds of structure not present in general text doc -
the bulk of the web consists
they have an internal
typed text segments marked by meta - linguistic ( markup ) .
html defines a set of roles to which text in a document can be assigned .
some of these roles relate to formatting , such as those defining bold and text .
others have richer semantic import , such as headlines and anchors , the text segments which serve as hyperlinks to other documents .
they also have an external structure .
as a node in
a hypertext , a html page is related
huge numbers of other pages , through both the hy - perlinks it contains and the hyperlinks that point to it from other pages .
because html pages are more structured text , web search engines enhance traditional methods with heuristics such heuristics most effectively , however .
that take advantage of this it is by no means clear how to integrate
in the following section we describe our prototype web - indexing system , called laser , and outline heuristics for exploiting the internal and external struc - ture of the web .
in the section entitled automatic optimization , we describe how the parameters combining these heuristics based on system usage .
finally , we present and dis - cuss our first empirical results with the system .
laser , a learning architecture
is a system designed to investigate
for search engine plicability of machine learning methods to the index - ing of web pages .
from a users perspective , much of lasers functionality is identical to that of other pop - ular web search engines ( see figure 123 ) .
the user enters unstructured keyword queries , which laser matches against its index of pages , returning abstracts of and links to the 123 pages matching the query most closely .
from this page of search results , the user can proceed to any of the abstracted pages or enter a new query .
is based on the tfidf
vector space retrieval model ( salton 123 ) .
in this model documents and queries are represented as vec - tors of real numbers , one for each word; documents and queries with similar contents are transformed into sim - ilar vectors .
laser uses an inner product similarity metric to compare documents with a query .
typically , the value of a word depends both on its
123 ) the reinforcement
learning group at carnegie mellon " ~ a fre ~ foccement learr ~ mol
reinforcement learning and friends at cmu talks of interest ( ~ = ~ . , ~ = ~ , a ~ : s . c ~ . ~ u ) ( tha e* , 123 ~ ~ - ~ ster rl t ~ k$ ~ ix* hek : l on " #e ~ s , 123 : 123 , wean 123 unless otherwise noted .
pizza protided; byobi ) =nov 123 : anthony robins ( u .
ot ~ o , new ze ~ and , md cmu psych ) , rehear ~ and pi ~ ktorehe ~ k ~ = solutions to the c&ta ~ trophlc forgettlrki woblem " 123 : 123 , r , i nov 123 , well 123 : ~ e keams ( at&t b ~ i l ~ ) , declalon tree lelltlll ~ ~ $tlthma are boosting ~ , ~ ocitp , ma =nov 123 : 123edaatian lhrun , task cluaterkr ~ selective transfer of knowledge zeroes multiple lemdl ~ l ta . , , k$ : " lho*rhte ~ results " nov 123 : no talk - - nips " dec 123 : geoff gordon , rar=k - based tests ( siides . ps . z ) - - - end of ~ eme ~ ter - - - .
/ . . .
( 123 kt ~ s , 123 links , size=sk )
123 ) ml - 123 workshop reinforcement
learning ~ ~ e ~ forcernent
( 123 b ~ , ge , 123 inke , eize=123k )
workshop call for papers twelfth international conference on machine learning; value function in reinforcement learning ~ s , 123 ~ ~ ~ =t ~ , ~ re= ~ t , tahoe city , c ~ ifornia , u . b . a .
i ~ worksiu ~ o exl123me the i ~ aues that ariae in nlk ~ elceme ~ ~ when the v ~ ue function u / ~ ot be lea - ned exactly , but must be ~ olor o ~ t ~ . t ed .
it ~ ioncj been the state space is too large to permit t ~ ble - io ~ up ~ opro ~ hes .
need to ger ~ rl ~ ze from past experlmces e r ~ evmt here , bet in ~ actlce . . .
that appr o ~ rnatlon is eesantid o123 brge , ~ e= - wodd problems because
to futwe ones , which inevit ~ 123y involves m ~ kin123 ~ opro ~ matjor ~ s ,
h principle , ~ t methods
in addition , we for lel / idl ~ j from exl ~ ole$
figure 123 : the laser interface .
our prototype system indexes approximately 123 , 123 hypertext documents avail - able from the cmu computer science department web server .
the document under consideration and its frequency in the entire collection of documents .
if a word occurs more frequently in a particular document than in the collection as a whole , then it is considered salient for that document and is given a high score .
in its simplest form , tfidf assigns to each word a score proportional to its frequency in the document ( term frequency or tf ) and a decreasing function of the number of docu - ments it occurs in overall ( inverse document frequency
function , based on this model , of - fers a number of parameters which influence the rank - ings it produces .
the parameters affect how the re - fields ( like headlines ) , how hyperlinks are incorporated , how to adjust for partial - word matches or query - term there are 123 real - adjacency , and more : altogether ,
function responds to words in certain html
valued parameters .
123 using a particular parameter set - ting makes it possible to pick a certain retrieval func - tion from the family of functions laser offers .
function can be adjusted to the dif - way , the retrieval of various document collections and user groups .
using html formatting
in html .
html is a most web pages are written markup language which allows the designer of a page to assign certain semantics to parts of a document and to control the layout .
the designer can specify , for ex - ample , the title of a document , hierarchies of headlines and hyperlinks , and character formats such as boldfac -
~ a listing of the parameters laser uses , in the form of a function for calculating document scores , can be found in appendix a .
laser makes use of the structure html imposes on documents .
for example , one parameter governs to what extent words in the title of a document should re - ceive stronger indexing weight than words near the end of a document .
laser has parameters for weighting words in the following html fields :
h123 , h123 , h123 ( headlines ) b ( bold ) , i ( italics ) , blink a ( underlined anchor text )
the parameters for these html tags are simply mul - factors for the " term frequency " of words within their scope .
unlike most other document collections , web pages are part of a hypertext graph .
for retrieval it might be useful not only to look at a document in isolation , but also to take its neighboring documents into account .
the approach we took is motivated by an analogy to reinforcement learning as studied in artificial ligence ( barto , bradtke , & singh 123 ) .
imagine that an agent searching for information on the web can move from page to page only by following hyperlinks .
whenever the agent finds information relevant search goal , it gets a certain amount of reward .
re - inforcement learning could be used to have the agent learn how to maximize the reward it receives , i . e .
how to navigate to relevant information .
the idea , then , is to have laser rank highly pages that would serve as good starting points for a search by such an agent .
good starting points are pages from which it is easy to reach other pages with relevant in - formation .
we conjecture that these pages are relevant to a query even if they do not contain much relevant information themselves , but just link to a number of
hyperlinks are incorporated as follows .
first , given a query q the retrieval status values rsv123 ( q , d ) are cal - culated for each page d in the collection independently , based on the html - specific tfidf parameters de - scribed above .
in reinforcement - learning the " immediate reward " associated with each page .
then , laser propagates the rewards back through the hypertext graph , discounting them at each step , by value iteration ( bellman 123 ) : d ) = rsv123 ( q , d ) + 123 z ) links ( d )
rsvt ( q , d )
~ ( 123 )
123 is a discount factor that controls the influence of is the set of pages neighboring pages , and links ( d )
referenced by hyperlinks in page d .
this dynamic - programming update formula is applied repeatedly for each document in a subset of the collection .
this sub - set consists of the documents with a sigificant rsvo , and it also includes the documents that link to at least one of those .
after convergence ( in practice , 123 iterations ) , pages which are n hyperlinks away from document d make a contribution proportional to 123n times their re - trieval status value to the retrieval status value of d .
of this mechanism : one is 123 , and the other , v e ( 123 , 123 ) , controls the normalization of the denominator in for - mula 123 in a range from ilinks ( d ) l down to 123
alto - gether , our retrieval function has 123 parame - the score assigned to document d in the context of query q is computed by rsv123 ( q , d ) as detailed in ap - pendix a on page 123
to laser influence
the 123 numerical parameters of lasers retrieval function allow for a wide variety of search engine be - from plain tfidf to very complex ranking schemes .
qualitatively , different produce markedly different rankings ( see table 123 ) .
our goal is to analyze system usage patterns to ( 123 ) quan - tify these differences , and ( 123 ) automatically optimize the parameter settings .
in order to keep the system interface easy to use , we made a design decision not to require users to give feedback on which search hits were good and which were bad .
instead , we simply record which hits people follow , e . g .
" user searched for vegetar - ian restaurant and clicked on restaurant reviews and eating indian in pittsburgh . " because the user gets to see a detailed abstract of each hit ( see figure 123 ) , believe that the hits actually clicked by the user are highly likely to be relevant .
a good retrieval
function will obey the probability ranking principle ( van rijsbergen 123 ) .
this means places documents which are most likely to be relevant to the users query near the top of the hit list .
to eval - uate a retrieval function fs performance on a single query q , we simply take the mean ranking according to f of all documents the user followed .
( example scores are shown in table 123 ) we then define the overall per - formance of retrieval function f to be the average of in the database .
in its performance over all queries
vegetarian chili recipes 123
vegetarian recipes 123
eating " indian " 123
restaurant reviews 123
greek dishes 123
focus on vegetarian 123
for the professional cook simple count of query terms 123
collection : thai recipes 123
food stores online 123
a list of food and cooking sites 123
cookbook of the year 123
collection : tofu 123
eating " indian " in pittsburgh
restaurant reviews
using html structure; 123
restaurant reviews 123
eating " indian " in pittsburgh 123
a list of food and cooking sites 123
duanes home page & ~ gay lists 123
eating & shopping green in pittsburgh 123
living indian in pittsburgh 123
for the professional cook using html structure; 123
restaurant reviews 123
a list of food and cooking sites 123
vegetarian chili recipes 123
for the professional cook 123
eating ~ shopping green in pittsburgh 123
vegetarian recipes
table 123 : rankings produced by four different retrieval functions in response to the query " vegetarian restaurant . " supposing that the user had clicked on the eating " indian " in pittsburgh and restaurant reviews pages , these
functions would be scored as shown .
123 ~ 123 perf ( f ) = ~ - ~ i=123 ~
rank ( f , qi , dij ) ( 123 )
where q123 . . qiqi are the queries in our database and di is the set of documents the user followed after pos - ing query qi .
the input used by this performance method is clearly noisier and more biased than that used in methods based on precision - recall gen 123 ) , which employ exhaustive relevance feedback information assigned manually by experts .
the users choice of hits to follow is strongly biased toward documents appearing early in the hit list - - regardless of the quality of retrieval func - tion used .
users rarely have the patience through pages and pages of hits .
thus , when eval - uating performances of new retrieval our collected database , we attempt to equalize these " presentation biases . " we do this by evaluating perf on a subsample q of our query database , where q to contain an equal number of queries from each different presentation bias; or alternatively , we weight each query qi so as to give equal total weight to each presentation bias .
tions performance , we can now pose the problem of finding the best retrieval function as a problem of func - find the parameter vector f mini -
the calculation of perf is based on averages of dis - crete rankings , so we expect it to be quite discontinu - ous and probably riddled with local minima .
thus , we chose to apply a global optimization algorithm , simu - lated annealing .
in particular , we applied the " modi - fied downhill simplex " variant of simulated annealing , as described in ( press et al .
123 ) .
because we calculate perf from only a fixed sub - sample of queries , aggressive minimization introduces that is , our converged pa - the danger of overfitting; rameter vector / 123 may exploit particular of the subsample at the expense of generalization over the whole space .
to guard against overfitting , we use early stopping with a holdout set , as is frequently done in neural network optimization 123 ) , as follows :
( morgan & bourlard
we consider the sequence of parameter vectors which are the " best so far " during the simulated anneal - ing run .
these produce a monotonically decreasing learning curve ( see , for example , figure 123 ) .
given our parametrization of the space of retrieval functions and our metric for evaluating a retrieval rune -
we then evaluate
the performance of each of these
vectors on a separate holdout set of queries .
we smooth the holdout - set performance curve and pick
$ f used for presentation
table 123 : performance comparison for three retrieval functions as of march 123 , 123
lower numbers indicate better performance .
rows correspond to the indexing method used by laser at query time; columns hold values from subsequent evaluation with other methods .
figures reported are means two standard errors
its minimum; the parameter setting the final answer from . our optimization run .
thus chosen is
each evaluation of perf ( ff ) on a new set of parame - ters is quite expensive , since it involves one call to the search engine for each query in q .
these evaluations could be sped up if q were subsampled randomly on each call to perf; however , this adds noise to the eval - uation .
we are investigating the use of stochastic opti - mization techniques , which are designed for optimiza - tion of just this type of noisy and expensive objective function ( moore & schneider 123 ) .
the system currently
since february 123 , laser has been in operation of about 123 , 123 pages by the cmu computer science depart - ment web server , tem is available munity from the departments http : / / www , cs .
edu / web / scs - home , html .
( we are considering plans for larger indexes and wider re -
for use by the
of performance measure
ran an experiment to determine whether our
performance function could really measure significant differences between search engines , based only on unin - trusive user feedback .
we manually constructed
scores a document by counting
number of query terms which appear in it;
captures word relevance much better but does not take advantage of html struc -
hand - tuned includes manually - chosen values for all
123 parameters of our html - specific retrieval
from february 123 through march 123 , we operated laser in a mode where it would randomly select one of these three retrieval functions to use for each query .
during this time laser answered a total of 123 user queries ( not including queries made by its designers ) .
for about half these queries , the user followed one or more of the suggested documents .
we evaluated perf ( f )
for each engine according to shown in the bottom row equation 123
the results , that our performance metric does of table 123 , indicate the three ranking functions : hand - tuned is significantly better than tfidf , which in turn is significantly better than simple - count .
three rows of table 123 break down the per -
to which retrieval formance measurement according function generated the original ranking seen by the user .
the presentation bias is clear : the diagonal en - tries are by far the smallest .
note that the diagonal entries are not significantly different from one another with the quantity of data we had collected at this point .
however , we do see significant differences when we
average down the columns to produce our full perfor - mance measurements in row four .
moreover , ranking the three methods according to these scores produces the expected order .
we take this as evidence our performance measure captures to some extent the " goodness " of a retrieval function and can serve as a reasonable objective function for optimization .
to date , we have had time to run only one optimiza - tion experiment , so the results of this section should be considered preliminary .
our goal was to minimize
thereby producing a new and better
for efficiency in evaluating perf , we let q be a fixed subsample of 123 queries from our full query database , 123 from each presentation bias .
to make the search space more tractable , we optimized over only a 123 - dimensional projection of the full 123 - dimensional pa - rameter space .
these 123 parameters still
simulated annealing - - best - so - far " points - e - - .
holdout set +
figure 123 : optimization of search engine performance by simulated annealing .
evaluations on a separate holdout set are used to prevent overfitting .
tuning of such heuristics as title and heading bonuses , query - word adjacency bonus , partial - match document length penalty , near - top - of - page bonus , and 123 , our hypertext discount factor .
as described above , we ran simulated annealing to find a new , optimal set of retrieval ters .
simulated annealings own parameters were set as follows : temperature at evaluation #i = 123 123 stepsize = 123% of the legal range for each dimension .
this run converged after about 123 eval - uations of perf ( ff ) ( see figure 123 ) .
using the early - stopping technique , we chose the parameter setting at evaluation #123 as our final answer .
compared to our hand - tuned parameter setting ,
learned parameter setting gave more weight to title and words near the words , underlined anchor text , beginning of a document .
surprisingly , it set 123 ( our graph - propagation discount factor ) almost to 123
in - function into our search en - gine interface , we found it produced qualitatively good rankings ( e . g . , refer back to table 123 ) .
the new retrieval
from march 123 through may 123 , laser generated
function half the its rankings with the new retrieval time and with our hand - tuned retrieval the time .
the cumulative results are shown in ta - ble 123
according to our overall performance metric , the hand - tuned and learned retrieval
differ significantly from one another .
outperform count and tfidf , but do not
however , the diagonal entries , which reflect tual use of the system , provide some indication is an improvement : with 123% functions value of confidence , the learned retrieval 123 - 123 - 123 is better than our hand - tuned functions value of 123 - 123 - 123 .
if this trend continues , we will be learned a new and better ranking scheme .
that we have successfully
engines have been developed to index world wide web pages .
descriptions of some can be found in ( mauldin &123 leavitt 123 ) and ( pinker - ton 123 ) .
these retrieval engines make use of the structure of documents , but they do not in - corporate hyperlinks .
other researchers have focused on retrieval using hypertext structure without making use of the internal structure of documents ( savoy 123; croft & turtle 123 ) .
automatic parameter optimization was previously proposed by ( fuhret al .
123 ) as well as ( bartell , cottrell , & belew 123 ) .
both approaches differ lasers in that they use real relevance feedback data .
laser does not require it uses more noisy data which can ment by the user;
$ f used for presentation
table 123 : cumulative performance comparison for four retrieval in the same format as in table 123
functions as of may 123 , 123
the data is reported
be collected unintrusively by observing users actions .
and future work
from laser are promising .
we have
an index which takes advantage of html
shown that unintrusive feedback can provide sufficient information to evaluate and optimize the performance of a retrieval function .
according to our performance structure outperforms a more traditional furthermore , we have begun to collect that it is possible to automatically improve a retrieval function by learning from user actions , with - out recourse to the intrusive methods of relevance feed -
there are many directions
which we see as falling into three general areas :
function parametrization laser cur -
into a retrieval
rently offers 123 tunable parameters for combining function , but certainly many other heuristics are possible .
for example , we would like to further refine our method for in - corporating hyperlinks .
we are also planning to in - clude per - document popularity statistics , from regular laser usage , into the relevance func - users , the system should learn to punish that docu - ment in the rankings .
if a document is always skipped by laser
from the probabilistic
evaluation metrics while our performance function and agrees with our has an appealing simplicity , judgments on the three search engines of table 123 , we cannot defend it on theoretical grounds .
a metric derived directly ( van rijsbergen 123 ) , for exam - ple , would allow us to make stronger claims about our optimization procedure .
another alternative to implement a cost function over rankings , where the cost increases with the number of irrelevant links those which the user explicitly skipped over ) high in the ranking .
it is not clear whether this is a useful metric , or even how to decide among these
issue , we have documented a pro - on a related nounced tendency for users to select links that are high in the rankings , no matter how poor the index , in " presentation bias . " this complicates the problem of evaluating new retrieval fline during optimization , since our query database will strongly bias the retrieval parameters toward those used for the original presentation .
we have an ad hoc method for compensating for this effect , but would be interested in more principled approaches .
optimization as mentioned in section 123 , we plan to the use of stochastic optimization tech - niques , in place of simulated annealing , for optimiz - ing the parameter settings .
there is also an interest - for " lifetime learning . " we would like to see how the system improves over time , iteratively index with a new and improved one learned from user data .
we can only speculate about the system might take .
there is the possibility of an interesting kind of feedback between the system and its users; as the system changes its indexing behavior , perhaps the users of the system will change their model of it and use it somehow is there a globally optimal differently from at first .
for laser ? it may be that , given the presentation bias and the possibility of drifting patterns of use , its parameters would never settle into a stable state .
to thank tom mitchell
we would like moore for the computational and cognitive resources they shared with us for these experiments .
thanks , too , to darrell kindred for counseling us on indexing the local web , and to michael mauldin for authoring the scout retrieval engine which we used as a basis for our own .
finally , we acknowledge the support of nsf
appendix a parametric form of retrieval
rsvt+ 123 ( q , d )
dt elinks ( d )
rsvt ( q , p )
rsv123 ( d , q ) = multihit ( q , d ) .
~ ~ ( q , = dr ) "
dweight ( j , qi , d j )
qweight ( i , qi , dj ) = -
( 123 + isfullmatch ( qi ,
dweight ( j , dj ) = idf ( dj ) .
( 123 +in - hl_headline ( dj ) . $hl_factor
+ispartmatch ( qi , r ) spartmatch_f actor )
+in_h123_headline ( d123 ) .
$h123_f actor +in_h123headline ( dj ) .
$h123_f actor +in_bold ( dj ) $bold_f actor +in_italics ( d r ) $italics_f actor - t - in_blink ( d r ) $blink_f actor +in_anehor ( dj ) .
$anchor_f actor
+ log ( j + stoppage_add ) )
= ( q ~ - i ~ d123_ , ) + ( qi - , = dj_ , ) .
$adjacency_factor
