discriminative training for structured outputs has found in - creasing applications in areas such as natural language pro - cessing , bioinformatics , information retrieval , and computer vision .
focusing on large - margin methods , the most gen - eral ( in terms of loss function and model structure ) training algorithms known to date are based on cutting - plane ap - proaches .
while these algorithms are very ecient for lin - ear models , their training complexity becomes quadratic in the number of examples when kernels are used .
to overcome this bottleneck , we propose new training algorithms that use approximate cutting planes and random sampling to enable ecient training with kernels .
we prove that these algo - rithms have improved time complexity while providing ap - proximation guarantees .
in empirical evaluations , our algo - rithms produced solutions with training and test error rates close to those of exact solvers .
even on binary classica - tion problems where highly optimized conventional training methods exist ( e . g .
svm - light ) , our methods are about an order of magnitude faster than conventional training meth - ods on large datasets , while remaining competitive in speed on datasets of medium size .
categories and subject descriptors i . 123 ( articial intelligence ) : learning algorithms , experimentation , performance support vector machines , kernels , large - scale problems
large - margin methods for structured output prediction like maximum - margin markov networks ( 123 ) and structural svms ( 123 ) have recently received substantial interest for challenging problems in natural language processing ( 123 ) , bioinformatics ( 123 ) , and information retrieval ( 123 ) .
as train - ing algorithms for these problems , cutting - plane approaches
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for prot or commercial advantage and that copies bear this notice and the full citation on the rst page .
to copy otherwise , to republish , to post on servers or to redistribute to lists , requires prior specic permission and / or a fee .
kdd123 , august 123 , 123 , las vegas , nevada , usa .
copyright 123 acm 123 - 123 - 123 - 123 - 123 / 123 / 123 . . . $123 .
( 123 , 123 ) are among the most generally applicable methods that provide well - understood performance guarantees .
first , cutting - plane methods can be use to train any type of structured linear prediction model for which inference and subgradi - ent can be computed ( or at least approximated ) eciently .
this makes them applicable to problems ranging from hmm training and natural language parsing , to supervised clus - tering and learning ranking functions .
second , they allow optimizing directly to non - standard loss functions that do not necessarily have to decompose linearly ( e . g .
average precision , roc - area , f123 - score ) ( 123 , 123 ) .
and third , their runtime provably scales linearly with the number of train - ing examples for linear models .
this makes cutting - plane methods not only attractive for training structured predic - tion models , but they are also orders of magnitude faster than conventional methods for training binary classiers ( 123 ) .
unfortunately , the computational eciency of cutting - plane
methods becomes substantially worse for non - linear models that involve kernels .
while it is possible to train kernel mod - els , the computational complexity scales quadratically with the number of examples , not linearly as in the non - kernel case .
in particular , each iteration of the algorithm requires a quadratic number of kernel evaluations .
this makes it infea - sible to train large - scale structural models that involve ker - nels , and it makes cutting - plane methods non - competitive for training kernelized binary classiers compared to con - ventional decomposition methods like svm - light .
in this paper we present new cutting - plane training meth - ods for structural svms that can be used to train kernel - ized models eciently .
these methods are equally broadly applicable , requiring only the ability to compute subgradi - ents eciently , but exploit sparse approximations to each cut in order to limit the number of kernel computations .
in particular , we present two new cutting - plane methods that exploit random sampling in computing a cut , so that the number of kernel evaluations depends only linearly on the number of examples in one algorithm , or is independent of the number of examples in the other algorithm .
instead , the number of kernel evaluations depends only on the quality of the solution that the user desires and that is sensible for the learning task .
in addition to providing theoretical guar - antees regarding runtime and quality of the solutions , we also provide empirical results in comparison to conventional decomposition methods and a subspace method that uses a 123
structural svms structual svms are a method for learning rules h : x y from some space x of complex and structured objects
x x to some space y of complex and structured objects y y ( e . g .
sentences x to parse trees y in natural language parsing ) .
given a labeled training sample
s = ( ( x123 , y123 ) , .
, ( xn , yn ) ) ,
structural svms learn a linear discriminant rule
h ( x ) = argmaxyy ( cid : 123 ) w ( x , y )
by minimizing a regularized version of the empirical risk i=123 ( yi , h ( xi ) ) for a given non - negative loss function .
in the case of margin - rescaling ( 123 , 123 ) we con - sider in this paper , training a structural svm amounts to solving the following quadratic program .
optimization problem 123
( struct svm primal )
y123 y : ( y123 , y123 ) ( cid : 123 ) w 123 ( y123 ) 123 . . . yn y : ( yn , yn ) ( cid : 123 ) w n ( yn ) n
we use the short - hand notation i ( y ) : = ( xi , yi ) ( xi , y ) .
while this program is convex , it has an exponential or in - nite number of constraints ( i . e .
proportional to |y | ) on most interesting problems , making naive approaches to its solution intractable .
fortunately , it can be shown that the cutting - plane algorithm 123 can nevertheless solve op123 to ar - bitrary precision .
j = j ( ( c ( t ) , ( cid : 123 ) g ( t ) ) ) t = t + 123 ( ( cid : 123 ) w , ) = solve qp ( j ) ( c ( t ) , ( cid : 123 ) g ( t ) ) = find cutting plane ( ( cid : 123 ) w )
algorithm 123 123 - slack cutting plane algorithm 123 : input : s = ( ( x123 , y123 ) , .
, ( xn , yn ) ) , c , 123 : j = ( ) , t = 123 , ( cid : 123 ) w = ( cid : 123 ) 123 , = 123 123 : ( c ( t ) , ( cid : 123 ) g ( t ) ) = find cutting plane ( ( cid : 123 ) w ) 123 : while c ( t ) + ( cid : 123 ) w ( cid : 123 ) g ( t ) > + do 123 : end while 123 : return ( ( cid : 123 ) w , ) 123 : procedure find cutting plane ( ( cid : 123 ) w ) 123 : end procedure 123 : procedure solve qp ( j ) s . t .
( c , ( cid : 123 ) g ) j , c + ( cid : 123 ) w ( cid : 123 ) g
i=123 ( yi , yi ) , 123
return ( 123
( cid : 123 ) ( cid : 123 ) w ( cid : 123 ) 123 + c
for i = 123 to n do
yi = argmax yy ( ( yi , y ) + ( cid : 123 ) w ( xi , y ) )
( ( cid : 123 ) w , ) =
return ( ( cid : 123 ) w , ) 123 : end procedure
this cutting plane algorithm is currently one of the fastest solution method for large margin structural learning prob - lems .
its time complexity scales linearly with the number of examples n ( 123 , 123 ) when the learned discriminant func - tion ( cid : 123 ) w ( x , y ) is linear .
however , with the use of kernels , it becomes neccessary to work in the dual and algorithm 123 now scales quadratic in the number of examples .
to see this , lets look at this dual optimization problem .
optimization problem 123
( cutting - plane dual )
( cid : 123 ) t g ( cid : 123 ) + ( cid : 123 ) ht ( cid : 123 )
( cid : 123 ) ( cid : 123 ) 123 and ( cid : 123 ) t ( cid : 123 ) 123 c
where gij = ( cid : 123 ) g ( i ) ( cid : 123 ) g ( j ) and hi = c ( i ) for i , j = 123 to t .
the primal and dual solution are related via ( cid : 123 ) w = ( cid : 123 )
one of the major issue with the dual algorithm is the compu - tation of the inner product ( cid : 123 ) g ( i ) ( cid : 123 ) g ( j ) in the nonlinear case ,
k ) ) ( ( xl , yl ) ( xl , y
( ( xk , yk ) ( xk , y
( k ( xk , yk , xl , yl ) k ( xk , yk , xl , y
k , xl , yl ) + k ( xk , y
k , xl , y
which involves o ( n 123 ) kernel computations .
this makes al - gorithm 123 impractical even if n is only moderately large .
removing this bottleneck is central to our approach .
related works
there has been many training methods proposed in the structural learning literature .
the maximum - margin markov networks ( 123 ) use smo ( 123 ) for training with linearly decom - posable loss functions , while the more general framework of structural svm ( 123 ) introduces the cutting plane method as a training procedure .
subgradient methods ( 123 ) have also been proposed as an ecient training method for structural learning .
recently a faster 123 - slack version of the cutting plane algorithm ( 123 ) has been introduced to solve large margin structural learning problems .
a generalization of the cutting plane method called the bundle method ( 123 ) has also been recently proposed for the minimization of dierent convex loss functions in structural learning .
most of these works consider only linear discriminant functions .
our work con - tinues this line of research by extending the cutting plane method to structural learning with kernels .
our work is also related to the use of stochastic optimiza - tion in structural learning .
the work in ( 123 ) investigated the use of stochastic gradient in the training of conditional random fields , while the work in ( 123 ) employed stochastic subgradient to train linear svms .
in stochastic optimization methods , decreasing step sizes or more accurate estimates of the gradient is required as the optimization progresses .
we aim to provide methods that automatically terminate when a solution with guaranteed precision is reached .
we take a somewhat dierent approach by directly modifying the op -
besides structural learning there have also been extensive work on speeding up kernel methods based on kernel matrix approximation .
the nystrom method has been proposed in ( 123 ) to approximate the kernel matrix used for gaussian process classication .
low - rank approximation has been ex - ploited to speed up the training of kernel svms ( 123 ) .
a greedy basis - pursuit - style algorithm is also proposed in ( 123 ) to build sparse kernel svms to speed up both training and classi -
the cut subsampling algorithms
our main idea is to speed up the expensive double sum kernel computations in equation 123 with approximate cuts that involve fewer basis functions .
such approximate cuts could be constructed by various methods such as greedy ap - proaches , but we take the simpler approach of sampling since
it allows us to prove performance guarantees later .
in the following we will present two dierent sampling strategies and analyze their complexity .
123 a constant time algorithm
our rst algorithm has constant time scaling with re - spect to the training set size .
let us look at the new cut - ting plane oracle in algorithm 123 , modied from algorithm 123
there are no other changes apart from the function find cutting plane ( ) .
the vector ( cid : 123 ) s contains r indices sampled uniformly from 123 to n .
both the oset c ( t ) and the subgradient ( cid : 123 ) g ( t ) are constructed from these r examples instead of the full training set .
in general , the approximate subgradient points in a dierent direction than the exact subgradient .
if we regard the exact constraint as a state - ment of how we want the classier to behave on the whole training set , we can regard the sampled cut as a statement on a bootstrap sample .
notice that the exit condition of the while loop on line 123 of algorithm 123 is now based on an es - timate of the loss from a small sample instead of the whole
j = j ( ( c ( t ) , ( cid : 123 ) g ( t ) ) ) t = t + 123 ( ( cid : 123 ) w , ) = solve qp ( j ) ( c ( t ) , ( cid : 123 ) g ( t ) ) = find cutting plane ( ( cid : 123 ) w )
algorithm 123 constant time cut subsampling algorithm for structural svm 123 : input : s = ( ( x123 , y123 ) , .
, ( xn , yn ) ) , c , 123 : j = ( ) , t = 123 , ( cid : 123 ) w = ( cid : 123 ) 123 , = 123 123 : ( c ( t ) , ( cid : 123 ) g ( t ) ) = find cutting plane ( ( cid : 123 ) w ) 123 : while c ( t ) + ( cid : 123 ) w ( cid : 123 ) g ( t ) > + do 123 : end while 123 : return ( ( cid : 123 ) w , ) 123 : procedure find cutting plane ( ( cid : 123 ) w ) sample r examples uniformly for ( cid : 123 ) s for j = 123 to r do ( c , ( cid : 123 ) g ) = ( 123 return ( c , ( cid : 123 ) g ) 123 : end procedure
ysj = argmax yy ( ( ysj , y ) + ( cid : 123 ) w ( xsj , y ) )
j=123 ( ysj , ysj ) , 123
j=123 sj ( ysj ) )
123 . 123 complexity per iteration
since the optimization problem is solved in the dual , we focus on complexity analysis of the dual of algorithm 123
we defer the analysis on the number of cutting planes required before convergence to the next section , and analyze the time and especially the number of kernel computations required in each iteration .
the dual form of the argmax operation of line 123 in algorithm 123 is :
yj = argmax yy
( ysj , y ) +
( k ) ( xsj , y )
expanding the inner product in equation 123 , ( k ) ( xsj , y ) ) =
, xsj , y ) k ( x
, xsj , y ) )
denotes sl and yl at the kth iteration .
this involves o ( tr ) kernel computations at iteration t when
we sum up from k = 123 to t 123 , provided the argmax compu - tation over y involves only a small constant number of kernel computation overall for dierent y .
this is true for binary or multi - class classication , and also true for the case when the kernel function factorizes into components ( e . g .
mrf cliques ) .
since we need to compute this inner product for all the sampled examples yj for j = 123 to r , the overall complex - ity of sampling a cut involves o ( tr123 ) kernel computations .
for computing the gram matrix g , we can update it in - crementally from one iteration to the next .
at iteration t , it involves expanding g by computing git for 123 i t .
following from equation 123 in the case of the exact algo - rithm , we can infer that the inner product of two sampled cuts ( cid : 123 ) g ( i ) ( cid : 123 ) g ( j ) involves o ( r123 ) kernel computations .
it takes o ( tr123 ) kernel computations overall since we need to do this for 123 i t .
we can see that the subsequent iterations are more expensive since the cost scales linearly with t .
if it takes t iterations for the algorithm to terminate , then the overall complexity would be o ( t 123r123 ) kernel computations .
we omit the time spent on the quadratic program in this analysis since in practice kernel computations account for over 123% of training time .
123 a linear time algorithm
the previous sampling approach never looks at the whole training set , making the complexity independent of the train - ing set size n in each iteration .
our second sampling algo - rithm trades o additional work in each iteration for the ability to sample in a more targeted way .
let us consider algorithm 123 , especially the changes to the cutting plane or - acle .
like the exact algorithm , it computes the argmax and the loss over all examples .
however , it only samples r of the examples with non - zero loss to construct the cutting plane .
this has the eect of focusing on those examples that are more important to determining the decision surface .
two cutting planes ( c ( t ) , ( cid : 123 ) g ( t ) ) and ( c ) are returned , one for in - clusion in the optimization problem while the other is used for the stopping criterion .
in the case of a linear feature space this sampling is not needed because the cutting plane can be represented com - pactly by just adding up the n feature vectors returned by the argmax computation .
but in the nonlinear kernel case , sampling helps because it reduces the number of basis func - tions used in the kernel expansion from o ( n ) to o ( r ) .
since the argmax computation is performed on all n examples , the algorithm has more information on the whole training set compared to the constant time algorithm , such as the average loss and the primal objective value .
in particular we can use the exact cutting plane ( c ) as the stopping criterion of the algorithm .
123 . 123 complexity per iteration
since we are computing the argmax over all n examples , it is possible to save computation in return for increased memory usage .
suppose we have a structure aki to store all the information required to compute ( cid : 123 ) g ( k ) ( xi , y ) for 123 k t , 123 i n , and for all y y .
this is a single inner product ( cid : 123 ) g ( k ) ( xi ) for binary classication , and m numbers for multi - class classication if there are m classes , one for each class .
for hmm with kernelized emissions , this involves storing the kernelized emission score at each position for each possible hidden state .
in all of these cases it amounts to o ( n ) storage requirement for each cut .
) ) = find cutting plane ( ( cid : 123 ) w )
) ) = find cutting plane ( ( cid : 123 ) w ) > + do
j = j ( ( c ( t ) , ( cid : 123 ) g ( t ) ) ) t = t + 123 ( ( cid : 123 ) w , ) = solve qp ( j ) ( ( c ( t ) , ( cid : 123 ) g ( t ) ) , ( c
algorithm 123 linear time cut subsampling algorithm for 123 : input : s = ( ( x123 , y123 ) , .
, ( xn , yn ) ) , c , 123 : j = ( ) , t = 123 , ( cid : 123 ) w = ( cid : 123 ) 123 , = 123 123 : ( ( c ( t ) , ( cid : 123 ) g ( t ) ) , ( c + ( cid : 123 ) w ( cid : 123 ) g 123 : while c 123 : end while 123 : return ( ( cid : 123 ) w , ) 123 : procedure find cutting plane ( ( cid : 123 ) w ) 123 : end procedure
i = ( 123 i n | ( yi , yi ) > 123 ) i=123 ( yi , yi ) , 123 sample r examples uniformly from i for ( cid : 123 ) s i=123 ( yi , yi ) , |i| ( c , ( cid : 123 ) g ) = ( 123 ( cid : 123 ) + or c + ( cid : 123 ) w ( cid : 123 ) g > + + ( cid : 123 ) w ( cid : 123 ) g return ( ( c , ( cid : 123 ) g ) , ( c
yi = argmax yy ( ( yi , y ) + ( cid : 123 ) w ( xi , y ) )
for i = 123 to n do
) = ( 123
j=123 sj ( ysj ) )
analysis of the algorithms
in this section we analyze theoretically the termination and solution accuracies of the two algorithms .
we rst prove bounds on the number of iterations for the algorithms to terminate , and then use the results to prove error bounds on the solutions .
we prove the results for the two algorithms under a general framework to show that these results could also apply to the design of other sampling schemes .
to prove termination for the above algorithms , we con - sider the following template of the cutting plane algorithm :
) ) = find cutting plane ( ( cid : 123 ) w ( t ) )
+ ( cid : 123 ) w ( t ) ( cid : 123 ) g
algorithm 123 generic cutting plane algorithm 123 : j = ( ) , t = 123 , ( cid : 123 ) w ( 123 ) = ( cid : 123 ) 123 , = 123 123 : ( ( c ( t ) , ( cid : 123 ) g ( t ) ) , ( c 123 : while c 123 : end while 123 : return ( ( cid : 123 ) w ( t ) , )
j = j ( ( c ( t ) , ( cid : 123 ) g ( t ) ) ) t = t + 123 ( ( cid : 123 ) w ( t ) , ) = solve qp ( j ) ( ( c ( t ) , ( cid : 123 ) g ( t ) ) , ( c
> + do
) ) = find cutting plane ( ( cid : 123 ) w ( t ) )
the dual form of the argmax operation in line 123 is :
( k ) ( xi , y )
( yi , y ) +
yi = argmax yy
with the saved kernel computations in aki for 123 k < t , the argmax computation requires no extra kernel computations since the term ( cid : 123 ) g ( k ) ( xi , y ) can be retrieved from aki .
updating ati for a new iteration t involves computing
, xi , y ) k ( x
, xi , y ) ) .
this requires o ( r ) kernel computations , assuming that com - puting and storing the information required for recontruct - ing the above inner product for each y takes a constant num - ber of kernel computations and storage .
as this has to be done for all n examples , the overall complexity is o ( n r ) kernel compuations for each update in each iteration .
the gram matrix g can be updated conveniently with
the information stored in aki , since this involves no new kernel computations since both ( cid : 123 ) g ( i ) ) can be reconstructed
) and ( cid : 123 ) g ( i ) ( x
therefore if the algorithm terminates in t iterations , the
overall complexity is o ( t n r ) kernel compuations , with o ( t n ) storage required .
although storing each cut requires o ( n ) storage , it is still feasible even for large datasets if the num - ber of active cuts is small ( e . g . , less than 123 ) .
this is the basic assumption in this space - time tradeo and is conrmed by our experiments in section 123
notice that what the above algorithm returns as solution de - pends crucially on the implementation of find cutting plane ( ) .
however the specic detail of the implementation does not aect the termination property of the above cutting plane algorithm , and we have the following theorem :
) returned by the cutting plane oracle find cutting plane ( ) :
theorem 123
assume the following holds for the cuts ( c ( t ) , ( cid : 123 ) g ( t ) ) , ( i ) 123 c + ( cid : 123 ) w ( t ) ( cid : 123 ) g ( iii ) if c
> + , then c ( t ) + ( cid : 123 ) w ( t ) ( cid : 123 ) g ( t ) > +
then algorithm 123 terminates after at most 123c r123 / 123 calls to the cutting plane oracle find cutting plane ( ) .
consider the optimization problem solved by solve qp ( )
on line 123 of the generic cutting plane algorithm :
optimization problem 123
( c , ( cid : 123 ) g ) j , c + ( cid : 123 ) w ( cid : 123 ) g
consider also the following optimization problem : optimization problem 123
( c , ( cid : 123 ) g ) c , c + ( cid : 123 ) w ( cid : 123 ) g where c = ( ( c , ( cid : 123 ) g ) | c r , 123 c , ( cid : 123 ) g h , ( cid : 123 ) ( cid : 123 ) g ( cid : 123 ) r ) c contains all possible bounded cutting planes where c is bounded above by and ( cid : 123 ) g is bounded above in norm by r .
since conditions ( i ) and ( ii ) hold for the cutting plane ora - cle , op123 is always a relaxation of op123
therefore the value of the primal solution of op123 is always smaller than the value of the primal solution of op123 , and hence the value of any feasible solution of op123 upper bounds the value of any
dual solution of op123
as ( cid : 123 ) w = ( cid : 123 ) 123 , = is a feasible solu - tion to op123 , the value of the dual solution of op123 is upper bounded by c .
by proposition 123 of ( 123 ) , the inclusion of each - violated constraint increases the dual objective of op123 by at least 123 / 123r123 , where r is the upper bound on the norm of any ( cid : 123 ) g .
as the dual objective is bounded from above by c , at most 123c r123 / 123 constraints could be added be - fore the cutting plane algorithm terminates .
condition ( iii ) ensures that whenever we are not termi - nating the while loop , an - violated constraint ( c ( t ) , ( cid : 123 ) g ( t ) ) will always be added to the working set .
corollary 123
let = maxi , y ( yi , y ) and
r = maxi , y ( cid : 123 ) i ( y ) ( cid : 123 ) .
algorithm 123 terminates after at most 123c r123 / 123 calls to find cutting plane ( ) .
first of all notice that algorithm 123 ts into the generic template of algorithm 123
the cut ( c ( t ) , ( cid : 123 ) g ( t ) ) returned by find cutting plane ( ) in algorithm 123 serves both as the cut to be included into the working cut set j and also as the cut for the termination criterion ( c ) as in line 123 of algorithm 123 above .
therefore condition ( iii ) of theorem j=123 ( ysj , ysj ) 123 holds trivially .
since 123 c ( t ) = 123 and ( cid : 123 ) ( cid : 123 ) g ( t ) ( cid : 123 ) = ( cid : 123 ) 123 j=123 sj ( ysj ) ( cid : 123 ) r , both conditions ( i ) and ( ii ) hold .
invoking theorem 123 , we can conclude that at most 123c r123 / 123 calls are made to find cutting plane ( ) before algorithm 123 terminates .
corollary 123
let = maxi , y ( yi , y ) and
r = maxi , y ( cid : 123 ) i ( y ) ( cid : 123 ) .
algorithm 123 terminates after at most 123c r123 / 123 calls to find cutting plane ( ) .
j=123 sj ( ysj ) and ( cid : 123 ) g
the proof is very similar to the previous corol - lary .
algorithm 123 ts the generic template of algorithm 123
i=123 ( yi , yi ) , so condition first of all c ( t ) = c ( i ) of theorem 123 is satised .
it is also easy to see that ( cid : 123 ) g ( t ) = i=123 i ( yi ) are bounded in norm by r , so condition ( ii ) holds as well .
it is also easy to see that the exit condition of the repeat loop on line 123 of algorithm 123 makes condition ( iii ) hold .
therefore we can invoke theorem 123 and conclude that at most 123c r123 / 123 calls are made to find cutting plane ( ) before termina - tion .
moreover , the repeat loop in find cutting plane ( ) always terminate in nte expected time .
when the ex - act cutting plane is - violated , we can always sample an - violated approximate cut with probability bounded away from 123 ( for example , by sampling the worst violating exam - ple r times ) .
123 accuracy of solution
after proving termination and bounding the number of cutting planes required , we turn our attention to the accu - racy of the solutions .
specically we will characterize the dierence between the regularized risk of the exact solution and our approximate solutions .
the main idea used in the if the error introduced by each approximate cut is small with high probability , then the dierence between the exact and approximate solutions will also be small with high probability .
bounding the dierence between the ex - act cut and the sampled cut can be done with hoedings
let us start the proofs by dening some notation .
let be an exact cutting plane model
c ( t ) + ( cid : 123 ) w ( cid : 123 ) g ( t )
f ( ( cid : 123 ) w ) = max
of the empirical risk , and let f ( ( cid : 123 ) w ) = max
c ( t ) + ( cid : 123 ) w ( cid : 123 ) g ( t ) be an approximate cutting plane model , with ( c ( t ) , ( cid : 123 ) g ( t ) ) be - ing the approximate cutting planes .
we have the following
lemma 123
let a xed ( cid : 123 ) v in the rkhs h be given .
suppose for some > 123 each of the cutting plane and its approximate
+ ( cid : 123 ) v ( cid : 123 ) g
+ ( cid : 123 ) v ( cid : 123 ) g
for t = 123 .
then f ( ( cid : 123 ) v ) < f ( ( cid : 123 ) v ) + with probability at least 123 t p .
by union bound we know that ( c ( t ) + ( cid : 123 ) v ( cid : 123 ) g ( t ) ) ( c ( t ) + ( cid : 123 ) v ( cid : 123 ) g ( t ) ) < for 123 t t occurs with probability at least 123 t p .
the following chain of implications holds :
+ ( cid : 123 ) v ( cid : 123 ) g
+ ( cid : 123 ) v ( cid : 123 ) g
+ ( cid : 123 ) v ( cid : 123 ) g + ( cid : 123 ) v ( cid : 123 ) g
+ ( cid : 123 ) v ( cid : 123 ) g
+ ( cid : 123 ) v ( cid : 123 ) g
hence f ( ( cid : 123 ) v ) < f ( ( cid : 123 ) v ) + with probability at least 123 t p .
the lemma shows that the approximate cutting plane model does not overestimate the loss by more than a certain amount with high probability .
notice that t is a xed number above .
if t is a bounded random variable such as the termination iteration , then we can replace t by its upper bound t and the lemma still holds .
from the termination bound in sec - tion 123 we have t = 123c r123 / 123
now we are going to use this lemma to analyze the lin - in the linear time algo - ear time algorithm algorithm 123
rithm we denote the exact cutting plane ( c ( t ) , ( cid : 123 ) g ( t ) ) with i=123 ( yi , yi ) , 123 i=123 i ( yi ) ) , and the approximate i=123 ( yi , yi ) , 123 cut ( c ( t ) , ( cid : 123 ) g ( t ) ) with ( 123 j=123 sj ( ysj ) ) .
we can bound the dierence between the exact cutting planes and the approximate cutting planes using hoedings in - equality in the following lemma :
lemma 123
let a xed ( cid : 123 ) v h , ( cid : 123 ) ( cid : 123 ) v ( cid : 123 )
123c be given , and let the exact cutting planes ( c ( t ) , ( cid : 123 ) g ( t ) ) and approximate cut - ting planes ( c ( t ) , ( cid : 123 ) g ( t ) ) be dened as above .
we have for each t = 123
where = |i| / n , i being the index set at the t - th iteration .
dene zj = ( cid : 123 ) vsj ( ysj ) .
since sj are sampled uniformly from the index set i , zj s are independent with e ( zj ) = 123|i| ii ( cid : 123 ) v i ( yi ) .
each zj is also bounded
apply hoedings inequality and after some arithmetic we obtain the result .
now we are ready to prove our main theorem relating the regularized risk of the optimal solution to our approximate solution .
let ( cid : 123 ) v be the optimal solution to op123
we have the following theorem :
theorem 123
suppose algorithm 123 terminates in t iter - as solution .
then with probability at
ations and return ( cid : 123 ) w least 123 ,
where t = 123c r123 / 123 , and l ( ( cid : 123 ) w ) is the margin loss 123 as in op123
with the exact cutting planes ( c ( t ) , ( cid : 123 ) g ( t ) ) and ap - proximate cutting planes ( c ( t ) , ( cid : 123 ) g ( t ) ) as dened in lemma 123 , , and p = exp ( r123 / 123c r123 ) we apply lemma 123
put ( cid : 123 ) v = ( cid : 123 ) v ( we omit since it is bounded above by 123 ) , we obtain f ( ( cid : 123 ) v ) + with probability at least 123 t exp ( r123 / 123c r123 ) .
inverting the statement and we have with probability at least 123 :
) < f ( ( cid : 123 ) v
( cid : 123 ) ( cid : 123 ) w ( cid : 123 ) 123 + c f ( ( cid : 123 ) w ) at
is the optimal solution of min ( cid : 123 ) w the t th iteration , we have the following :
+c f ( ( cid : 123 ) w
+c f ( ( cid : 123 ) v
( with prob
the last line makes use of the subgradient property that f ( ( cid : 123 ) w ) l ( ( cid : 123 ) w ) for any exact cutting plane model f of a convex loss function l .
since we are using the exact cutting plane as the condition for exiting the while loop , so we must have
( yi , yi ) 123
( ( yi , yi ) ( cid : 123 ) w
( y123 , . . . , yn ) yn
i ( yi ) f ( ( cid : 123 ) w
i ( yi ) ) f ( ( cid : 123 ) w ) f ( ( cid : 123 ) w
+ c ( f ( ( cid : 123 ) w
therefore we have :
+ cl ( ( cid : 123 ) w
) + c with probability at least 123 .
the theorem shows that as far as obtaining a nite precision solution to the regularized risk minimization problem is con - cerned , it is sucient to use sampled cuts with suciently large sample size r to match the desired accuracy of the solution .
we will see in the experiment section that fairly small values of r work well in practice .
we state a similar result for algorithm 123
the proof is fairly similar with a few technical dierences .
we assign its proof to the appendix .
theorem 123
suppose algorithm 123 terminates in t iter - returned as solution .
then with probability
ations with ( cid : 123 ) w
at least 123 123 ,
where t = 123c r123 / 123
while theory gives us the worst case bounds that are re - assuring , we now study the empirical behaviour of the algo - 123 experiment setup
we implemented algorithm 123 and algorithm 123 and eval - uated them on the task of binary classication with kernels .
we choose this task for evaluation because binary classi - cation with kernels is a well - studied problem , and there are stable svm solvers that are suitable for comparisons .
moreover , scaling up svm with kernels to large datasets is an interesting research problem on its own ( 123 ) .
in binary classication the loss function is just the zero - one loss .
the feature map is dened by ( x , y ) = y ( x ) , where y ( 123 , 123 ) and is the nonlinear feature map in - duced from a mercer kernel ( such as the commonly used polynomial kernels and gaussian kernels ) .
we implemented the algorithms in c , using mosek as the quadratic program solver and the sfmt implementation ( 123 ) of mersenne twister as the random number generator .
the experiments were run on machines with opteron 123ghz cpus with 123gb of memory ( with the exception of the control experiments with incomplete cholesky factorization , which we ran on machines with 123gb of memory ) .
for all the experiments below we x the precision parame - ter at 123 .
we remove cuts that are inactive for 123 itera - tions .
we found that the constant time algorithm has better performance if we use a more stringent stopping criterion .
we terminate the algorithm only when for p consecutive it - erations , the sampled cut is not violated by more than .
in the experiments below we use p = 123
for each combination of parameters we ran the experiment for 123 runs using dier - ent random seeds , and report the average result in the plots and tables below .
in section 123 we also investigate the sta - bility of the algorithms by reporting the standard deviation of the results .
in the experiments below we test our algorithms on three dierent datasets : checkers , adult , and covertype .
check - ers is a synthetic dataset with 123 million training points , with classes alternating on a 123x123 checkerboard .
we generated the data using the simplesvm toolbox ( 123 ) , with noise level pa - rameter sigma set to 123 .
the kernel width for the gaussian kernel used for the checkers dataset was determined by cross validation on a small subsample of 123 examples .
adult is a medium - sized dataset with 123 examples , with a sam - ple of 123 examples taken as training set .
the gaussian kernel width is taken from ( 123 ) .
covertype is a dataset with 123 training points , the kernel width of the gaussian kernel we use below is obtained from the study ( 123 ) .
123 scaling with training set size
our rst set of experiments is about how the two algo - rithms scale with training set size .
we perform the exper - iments on the two large datasets checkers and covertype .
we pick c to be 123 multiplied by the training set size , since
123 123 123 123e+123
123 123 123 123e+123
training set size
training set size
figure 123 : cpu time against training set size
training set size
training set size
figure 123 : training set error against training set
that is the largest value of c we could get svmlightto train within 123 days .
for the linear time algorithm we x the sam - ple size r at 123 , and for the constant time algorithm we use a sample size r of 123 to compensate for the less e - cient sampling .
we train svm models on subsets of the full training sets of various sizes to evaluate scaling .
figure 123 shows the cpu time required to train svms on training sets of dierent sizes on the checkers and cover - type dataset .
we can observe that the linear time algorithm scales roughly linearly in the log - log plot , while the constant time algorithm has a roughly at curve in both plots .
this conrms the scaling behaviour we expect from the complex - ity of each iteration .
svmlight shows superlinear scaling on both of these datasets .
figures 123 and 123 show the training and test set errors of
training set size
training set size
figure 123 : test set error against training set size
constant time alg .
on adult
linear time alg .
on adult
123 123 123
figure 123 : cpu time against sample size
constant time alg .
on adult
linear time alg .
on adult
figure 123 : number of iteration against sample size
the algorithms .
in general svmlighthas the lowest training and test set errors , followed by the linear time algorithm and then the constant time algorithm .
both the training and test set errors lie within a very narrow band , and they are never more than 123 percentage point apart even in the 123 effect of different sample sizes
the next set of experiments is about the eect of the sample size r on training time and solution quality .
we investigate the eect of sample size using the adult dataset , since on this dataset it is easier to collect more data points for dierent sample sizes .
we use sample sizes r from ( 123 , 123 , 123 , 123 , 123 ) and c from ( 123 , 123 , 123 , 123 , 123 ) multiplied by the training set size 123
the constant time algorithm did not nish the training within 123 days for the largest sample size 123 and c ( 123 , 123 ) , hence there are two missing data points in the gures .
in figure 123 shows that the number of iterations required generally decreases with increasing sample size .
however the decrease in the number of iterations to convergence does not result in overall savings in time due to the extra cost in - volved in each iteration with larger sample sizes .
this can be observed from the cpu time plots in figure 123
in general , the linear time algorithm has better scaling behaviour with respect to sample size compared to the constant time algo - rithm .
this is predicted by our complexity analysis .
what is most interesting is the stability of training and test set errors with respect to changes to sample size , as shown in figures 123 and 123
except for very small sample sizes like 123 or small values of c like 123 the sets of curves are essentially
constant time alg .
on adult
linear time alg .
on adult
figure 123 : training set error against sample size
constant time alg .
on adult
linear time alg .
on adult
figure 123 : test set error against sample size
123 quality of solutions
table 123 shows a comparison of the two algorithms against two conventional training methods , namely svmlight and a sampling - based method that uses cholesky decomposi - tion as described below .
for each dataset we train dier - ent models using values of c ( 123 , 123 , 123 , 123 , 123 ) , mul - tipled by the size of the training set .
we used the results of svmlightas a yardstick to compare against , and report the value of c for which the test performance is optimal for svmlight .
for the larger datasets checkers and covertype , svmlightterminated early due to slow progress for c 123 , so for those two datasets we use c = 123
first of all , we notice from table 123 that all the solutions have training and test set error rates very close to the so - lutions produced by svmlight .
for the constant time algo - rithm the error rates are usually within 123 to 123 above the svmlightsolutions , while the linear time algorithm has error rates usually within 123 above the svmlightsolutions .
the error rates also have very small standard deviation , on the order of 123 , which is the same as our tolerance parameter .
we also notice when using the same sample size r , the linear time algorithm provides more accurate solutions than the constant time algorithm due to its use of more focused
we also provide control experiments with cholesky de - composition method , where we subsample a set of points from the training set , and then compute the projection of all the points in the training set onto the subspace spanned by these examples .
then we train a linear svm using svmperf ( 123 ) ( with options - t 123 - w 123 - b 123 ) on the whole training set .
our implementation involves storing all the projected train - ing vectors , and this consumes a lot of memory , especially for large datasets like checkers and covertype .
we can only do 123 and 123 basis functions on those datasets respectively
without running out of memory on a 123gb machine , and on the adult dataset we can only do up to 123 basis func - tions .
an alternative implementation with smaller storage requirement would involve recomputing the projected train - ing vector when needed , but this would become prohibitively
we observe that the cholesky decomposition is generally faster than all the other methods , but its accuracy is usu - ally substantially below that of svmlight and our sampling algorithms .
moreover , unlike our algorithms , the accuracy of the cholesky method depends crucially on the number of basis functions , which is dicult to pick in advance .
the accuracies of our sampling algorithms are more stable with respect to the choice of sample size , where decreasing the sample size ususally results in more iterations to converge without much loss in accuracy of the solutions .
we presented two methods that make cutting - plane train - ing of structural svms with kernels tractable through the use of random sampling in constructing a cut .
the meth - ods maintain the full generality of the cutting - plane ap - proach , making it possible to kernelize any structural predic - tion problem where linear models are currently used .
the theoretical analysis shows that these algorithms have lin - ear or constant - time termination guarantees while providing bounds on the solution quality .
empirically , the algorithms can handle datasets with hundred - thousands of examples , and they are competitive or faster than conventional de - composition methods even on binary classication problems , where highly optimized special - purpose algorithms exist .
the current algorithms can be improved along several di - rections .
the two sampling methods presented here are cho - sen for their simplicity and ease of analysis .
sampling e - ciency can be improved by designing alternative sampling schemes , for example , by having dierent sampling rates for bound support vectors and non - bound support vectors following the popular shrinking heuristic used in training svms .
on the other hand , one major bottleneck in the speed of the current algorithm is the large number of cuts required before convergence .
recently ( 123 ) proposes a sta - bilized cutting plane algorithm for linear svms with much improved convergence , and it will be interesting to extend their techniques to improve the speed of our sampled - cut algorithm for kernels .
we would like to thank the reviewers for their careful read - ing and helpful comments for improving this paper .
this work was supported in part by nsf award iis - 123 and by a gift from yahoo ! .
