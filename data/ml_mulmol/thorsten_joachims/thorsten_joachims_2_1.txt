this paper presents a method for learning a distance metric from rel - ative comparison such as a is closer to b than a is to c .
taking a support vector machine ( svm ) approach , we develop an algorithm that provides a exible way of describing qualitative training data as a set of constraints .
we show that such constraints lead to a convex quadratic programming problem that can be solved by adapting standard meth - ods for svm training .
we empirically evaluate the performance and the modelling exibility of the algorithm on a collection of text documents .
distance metrics are an essential component in many applications ranging from supervised learning and clustering to product recommendations and document browsing .
since de - signing such metrics by hand is difcult , we explore the problem of learning a metric from examples .
in particular , we consider relative and qualitative examples of the form a is closer to b than a is to c .
we believe that feedback of this type is more easily available in many application setting than quantitative examples ( e . g .
the distance between a and b is 123 ) as considered in metric multidimensional scaling ( mds ) ( see ( 123 ) ) , or absolute qualitative feedback ( e . g .
a and b are similar , a and c are not similar ) as considered
building on the study in ( 123 ) , search - engine query logs are one example where feedback of the form a is closer to b than a is to c is readily available for learning a ( more semantic ) similarity metric on documents .
given a ranked result list for a query , documents that are clicked on can be assumed to be semantically closer than those documents that the user observed but decided to not click on ( i . e .
aclick is closer to bclick than aclick is to cnoclick ) .
in contrast , drawing the conclusion that aclick and cnoclick are not similar is probably less justied , since a cnoclick high in the presented ranking is probably still closer to aclick than most documents in the collection .
in this paper , we present an algorithm that can learn a distance metric from such relative and qualitative examples .
given a parametrized family of distance metrics , the algorithms discriminately searches for the parameters that best fulll the training examples .
taking a maximum - margin approach ( 123 ) , we formulate the training problem as a convex quadratic
program for the case of learning a weighting of the dimensions .
we evaluate the perfor - mance and the modelling exibility of the algorithm on a collection of text documents .
the notation used throughout this paper is as follows .
vectors are denoted with an arrow ~ x where xi is the ith entry in vector ~ x .
the vector ~ 123 is the vector composed of all zeros , and ~ 123 is the vector composed of all ones .
~ xt is the transpose of vector ~ x and the dot product is denoted by ~ xt ~ y .
we denote the element - wise product of two vectors ~ x = ( x123; : : : ; xn ) t and ~ y = ( y123; : : : ; yn ) t as ~ x ~ y = ( x123y123; : : : ; xnyn ) t .
123 learning from relative qualitative feedback
we consider the following learning setting .
given is a set xtrain of objects ~ xi 123 <n .
as training data , we receive a subset ptrain of all potential relative comparisons dened over the set xtrain .
each relative comparison ( i; j; k ) 123 ptrain with ~ xi; ~ xj; ~ xk 123 xtrain has
~ xi is closer to ~ xj than ~ xi is to ~ xk .
the goal of the learner is to learn a weighted distance metric d ~ w ( ; ) from ptrain and xtrain that best approximates the desired notion of distance on a new set of test points xtest , xtrain \ xtest = ; .
we evaluate the performance of a metric d ~ w ( ; ) by how many relative comparisons ptest it fullls on the test set .
123 parameterized distance metrics
a ( pseudo ) distance metric d ( ~ x; ~ y ) is a function over pairs of objects ~ x and ~ y from some set x .
d ( ~ x; ~ y ) is a pseudo metric , iff it obeys the four following properties for all ~ x; ~ y , and
d ( ~ x; ~ x ) = 123;
d ( ~ x; ~ y ) = d ( ~ y; ~ x ) ;
d ( ~ x; ~ y ) 123;
d ( ~ x; ~ y ) + d ( ~ y; ~ z ) d ( ~ x; ~ z )
it is a metric , iff it also obeys d ( ~ x; ~ y ) = 123 ) ~ x = ~ y .
in this paper , we consider a distance metric da;w ( ~ x; ~ y ) between vectors ~ x; ~ y 123 <n param - eterized by two matrices , a and w .
da;w ( ~ x; ~ y ) = q ( ~ x ~ y ) t aw at ( ~ x ~ y )
w is a diagonal matrix with non - negative entries and a is any real matrix .
note that the matrix aw at is semi - positive denite so that da;w ( ~ x; ~ y ) is a valid distance metric .
this parametrization is very exible .
in the simplest case , a is the identity matrix , i , and di;w ( ~ x; ~ y ) = p ( ~ x ~ y ) t iw i t ( ~ x ~ y ) = p ( ~ x ~ y ) t w ( ~ x ~ y ) is a weighted , eu - clidean distance di;w ( ~ x; ~ y ) = ppi wii ( xi yi ) 123
in a general case , a can be any real matrix .
this corresponds to applying a linear transfor - mation to the input data with the matrix a .
after the transformation , the distance becomes a euclidean distance on the transformed input points at ~ x , at ~ y .
da;w ( ~ x; ~ y ) = q ( ( ~ x ~ y ) t a ) w ( at ( ~ x ~ y ) )
the use of kernels k ( ~ x; ~ y ) = ` ( ~ x ) ` ( ~ y ) suggests a particular choice of a .
let ' be the matrix where the i - th column is the ( training ) vector ~ xi projected into a feature space using
the function ` ( ~ xi )
d ' ;w ( ` ( ~ x ) ; ` ( ~ y ) ) = q ( ( ` ( ~ x ) ` ( ~ y ) ) t ' ) w ( ' t ( ` ( ~ x ) ` ( ~ y ) ) )
is a distance metric in the feature space .
wii ( k ( ~ x; ~ xi ) k ( ~ y; ~ xi ) ) 123
123 an svm algorithm for learning from relative comparisons
given a training set ptrain of n relative comparisons over a set of vectors xtrain , and the matrix a , we aim to t the parameters in the diagonal matrix w of distance metric da;w ( ~ x; ~ y ) so that the training error ( i . e .
the number of violated constraints ) is minimized .
finding a solution of zero training error is equivalent to nding a w that fullls the fol - lowing set of constraints .
123 ( i; j; k ) 123 ptrain : da;w ( ~ xi; ~ xk ) da;w ( ~ xi; ~ xj ) > 123
if the set of constraints is feasible and a w exists that fullls all constraints , the solution is typically not unique .
we aim to select a matrix aw at such that da;w ( ~ x; ~ y ) remains as close to an unweighted euclidean metric as possible .
following ( 123 ) , we minimize the f , this leads to the norm of the eigenvalues jjjj123 of aw at .
since jjjj123 = jjaw at jj123 following optimization problem .
jjaw at jj123
s : t : 123 ( i;j;k ) 123 ptrain : ( ~ xi ~ xk ) tawat ( ~ xi ~ xk ) ( ~ xi ~ xj ) tawat ( ~ xi ~ xj ) 123
unlike in ( 123 ) , this formulation ensures that da;w ( ~ x; ~ y ) is a metric , avoiding the need for semi - denite programming like in ( 123 ) .
as in classication svms , we add slack variables ( 123 ) to account for constraints that cannot be satised .
this leads to the following optimization problem .
jjaw at jj123
f + c x
s : t : 123 ( i;j;k ) 123 ptrain : ( ~ xi ~ xk ) tawat ( ~ xi ~ xk ) ( ~ xi ~ xj ) tawat ( ~ xi ~ xj ) 123 ijk
the sum of the slack variables ijk in the objective is an upper bound on the number of all distances da;w ( ~ x; ~ y ) can be written in the following linear form .
if we let ~ w be the diagonal elements of w then the distance da;w can be written as da;w ( ~ x; ~ y ) = q ( ( ~ x ~ y ) t a ) w ( at ( ~ x ~ y ) )
= q ~ wt ( at ~ x at ~ y ) ( at ~ x at ~ y )
where denotes the element - wise product .
if we let ~ xi;xj = ( at ~ xi at ~ xk ) ( at ~ xi at ~ xk ) , then the constraints in the optimization problem can be rewritten in the following
123 ( i; j; k ) 123 ptrain : ~ wt ( ~ xi;xk ~ xi;xk ) 123 ijk
figure 123 : graphical example of using different a matrices .
in example 123 , a is the iden - tity matrix and in example 123 a is composed of the training examples projected into high dimensional space using an rbf kernel .
furthermore , the objective function is quadratic , so that the optimization problem can be
~ wt l ~ w + c x
123 ( i; j; k ) 123 ptrain : ~ wt ( ~ xi;xk ~ xi;xj ) 123 ijk
for the case of a = i , jjaw at jj123 dene l = ( at a ) ( at a ) so that jjaw at jj123 denite in both cases and that , therefore , the optimization problem is convex quadratic .
f = wt lw with l = i .
for the case of a = ' , we f = wt lw .
note that l is positive semi -
in figure 123 , we display a graphical example of our method .
example 123 is an example of a weighted euclidean distance .
the input data points are shown in 123a ) and our training constraints specify that the distance between two square points should be less than the dis - tance to a circle .
similarly , circles should be closer to each other than to squares .
figure 123 ( 123b ) shows the points after an mds analysis with the learned distance metric as input .
this learned distance metric intuitively correponds to stretching the x - axis and shrinking the y - axis in the original input space .
example 123 in figure 123 is an example where we have a similar goal of grouping the squares together and separating them from the circles .
in this example though , there is no way to use a linear weighting measure to accomplish this task .
we used an rbf kernel and learned a distance metric to separate the clusters .
the result is shown in 123b .
to validate the method using a real world example , we ran several experiments on the webkb data set ( 123 ) .
in order to illustrate the versatility of relative comparisons , we gen - erated three different distance metrics from the same data set and ran three types of tests : an accuracy test , a learning curve to show how the method generalizes from differing amounts of training data , and an mds test to graphically illustrate the new distance measures .
the experimental setup for each of the experiments was the same .
we rst split x , the set of all 123 , 123 documents , into separate training and test sets , xtrain and xtest .
123% of the
all examples x added to xtrain and the remaining 123% are in xtest .
we used a binary feature vector without stemming or stop word removal ( 123 , 123 features ) to represent each document because it is the least biased distance metric to start out with .
it also performed best among several different variations of term weighting , stemming and stopword removal .
the relative comparison sets , ptrain and ptest , were generated as follows .
we present results for learning three different notions of distance .
university distance : this distance is small when the two examples , ~ x; ~ y , are from the same university and larger otherwise .
for this data set we used webpages from
topic distance : this distance metric is small when the two examples , ~ x; ~ y , are from the same topic ( e . g .
both are student webpages ) and larger when they are each from a different topic .
there are four topics : student , faculty , course and
topic+facultystudent distance : again when two examples , ~ x; ~ y , are from the same topic then they have a small distance between them and a larger distance when they come from different topics .
however , we add the additional constraint that the distance between a faculty and a student page is smaller than the distance to pages from other topics .
to build the training constraints , ptrain , we rst randomly selected three documents , xi; xj; xk , from xtrain .
for the university distance we added the triplet ( i; j; k ) to ptrain if xi and xj were from the same university and xk was from a different university .
in build - ing ptrain for the topic distance we added the ( i; j; k ) to ptrain if xi and xj were from the same topic ( e . g .
student webpages ) and xk was from a different topic ( e . g .
project webpages ) .
for the topic+facultystudent distance , the training triple ( i; j; k ) was added to ptrain if either the topic rule occurred , when xi and xj were from the same topic and xk was from a different topic , or if xi was a faculty webpage , xj was a student webpage and xk was either a project or course webpage .
thus the constraints would specify that a student webpage is closer to a faculty webpage than a faculty webpage is to a course
learned d ~ w ( ; )
table 123 : accuracy of different distance metrics on an unseen test set ptest .
the results of the learned distance measures on unseen test sets ptest are reported in table 123
in each experiment the regularization parameter c was set to 123 and we used a = i .
we report the percentage of the relative comparisons in ptest that were satised for each of the three experiments .
as a baseline for comparison , we give the results for the static ( not learned ) distance metric that performs best on the test set .
the best performing metric for all static euclidean distances ( binary and tfidf ) used stemming and stopword removal , which our learned distance did not use .
the learned university distance satised 123% of the constraints .
this veries that the learning method can effectively nd the relevant features , since pages usually mentioned which university they were from .
for the other distances , both the topic distance and topic+facultystudent distance satised more than 123% more constraints in ptest than the best unweighted distance .
using a kernel instead of a = i did not yield improved results .
for the second test , we illustrate on the topic+facultystudent data set how the prediction accuracy of the method scales with the number of training constraints .
the learning curve
size of training set in thousands of constraints
figure 123 : learning curves for the topic+facultystudent dataset where the x axis is the size of the training set ptrain plotted against the y axis which is the percent of constraints in ptest that were satised .
is shown in figure 123 where we plot the training set size ( in number of constraints ) versus the percentage of test constraints satised .
the test set ptest was held constant and sampled in the same way as the training set ( jptestj = 123 , 123 ) .
as figure 123 illustrates , after the data set contained more than 123 , 123 constraints , the performance of the algorithm remained
as a nal test of our method , we graphically display our distance metrics in table 123
we plot three distance metrics : the standard binary distance ( figure a ) for the topic dis - tance , the learned metric for topic distance ( figure b ) and , and the learned metric for the topic+facultystudent distance ( figure c ) .
to produce the plots in table 123 , all pairwise distances between the points in xtest were computed and then projected into 123d using a classical , metric mds algorithm ( 123 ) .
figure a ) in table 123 is the result of using the pairwise distances resulting from the un - weighted , binary l123 norm in mds .
there is no clear distinction between any of the clusters in 123 dimensions .
in figure b ) we see the results of the learned topic distance measure .
the classes were reasonably separated from each other .
figure c ) shows the result of using the learned topic+facultystudent distance metric .
when compared to figure b ) , the faculty and student webpages have now moved closer together as desired .
123 related work
the most relevant related work is the work of xing et al ( 123 ) which focused on the problem of learning a distance metric to increase the accuracy of nearest neighbor algorithms .
their work used absolute , qualitative feedback such as a is similar to b or a is dissimilar to b which is different from the relative constraints considered here .
secondly , their method does not use regularization .
related are also techniques for semi - supervised clustering , as it is also considered in ( 123 ) .
while ( 123 ) does not change the distance metric , ( 123 ) uses gradient descent to adapt a param - eterized distance metric according to user feedback .
other related work are dimension reduction techniques such as multidimensional scaling ( mds ) ( 123 ) and latent semantic indexing ( 123 ) .
metric mds techniques take as input a matrix d of dissimilarities ( or similarities ) between all points in some collection and then seeks to arrange the points in a d - dimensional space to minimize the stress .
the stress of the
arrangement is roughly the difference between the distances in the d - dimensional space and the distances input in matrix d .
lsi uses an eigenvalue decomposition of the original input space to nd the rst d principal eigenvectors to describe the data in d dimensions .
our work differs because the input is a set of relative comparisons , not quantitative distances and does not project the data into a lower dimensional space .
non - metric mds is more similar to our technique than metric mds .
instead of preserving the exact distances input , the non - metric mds seeks to maintain the rank order of the distances .
however , the goal of our method is not a low dimensional projection , but a new distance metric in the original
123 conclusion and future work
in this paper we presented a method for learning a weighted euclidean distance from rela - tive constraints .
this was accomplished by solving a convex optimization problem similar to svms to nd the maximum margin weight vector .
one of the main benets of the algo - rithm is that the new type of the constraint enables its use in a wider range of applications than conventional methods .
we evaluated the method on a collection of high dimensional text documents and showed that it can successfully learn different notions of distance .
future work is needed both with respect to theory and application .
in particular , we do not yet know generalization error bounds for this problem .
furthermore , the power of the method would be increased , if it was possible to learn more complex metrics that go beyond feature weighting , for example by incorporating kernels in a more adaptive way .
