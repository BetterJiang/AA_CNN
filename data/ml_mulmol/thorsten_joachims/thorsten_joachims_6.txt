we present a new method for transductive learning , which can be seen as a transductive version of the k nearest - neighbor classier .
unlike for many other transductive learning methods , the training problem has a mean - ingful relaxation that can be solved glob - ally optimally using spectral methods .
we propose an algorithm that robustly achieves good generalization performance and that can be trained eciently .
a key advantage of the algorithm is that it does not require ad - ditional heuristics to avoid unbalanced splits .
furthermore , we show a connection to trans - ductive support vector machines , and that an eective co - training algorithm arises as a special case .
for some applications , the examples for which a pre - diction is needed are already known when training the classier .
this kind of prediction is called transduc - tive learning ( vapnik , 123 ) .
an example of such a task is relevance feedback in information retrieval .
in relevance feedback , users can give positive and neg - ative examples for the kinds of documents they are interested in .
these documents are the training ex - amples , while the rest of the collection is the test set .
the goal is to generalize from the training examples and nd all remaining documents in the collection that match the users information need .
why is the transductive setting dierent from the reg - ular inductive setting ? in the transductive setting , the learner can observe the examples in the test set and potentially exploit structure in their distribution .
several methods have been designed with this goal in mind .
vapnik introduced transductive svms ( vap - nik , 123 ) which were later rened by ( bennett , 123 ) and ( joachims , 123 ) .
other methods are based on s - t mincuts ( blum & chawla , 123 ) or on multi - way cuts ( kleinberg & tardos , 123 ) .
related is also the idea of co - training ( blum & mitchell , 123 ) , which
exploits structure resulting from two redundant repre - sentations .
we will study what these approaches have in common and where they have problems .
in partic - ular , we will focus on s - t mincuts , co - training , and tsvms and show that they have undesirable biases that require additional , dicult to control heuristics .
to overcome this problem , we rst propose and moti - vate a set of design principles for transductive learn - ers .
following these principles , we introduce a new transductive learning method that can be viewed as a transductive version of the k nearest - neighbor ( knn ) rule .
one key advantage is that it does not require greedy search , but leads to an optimization problem that can be solved eciently and globally optimally via spectral methods .
we evaluate the algorithm em - pirically on 123 benchmarks , showing improved and more robust performance than for transductive svms .
fur - thermore , we show that co - training emerges as a spe - cial case and that the new algorithm performs substan - tially better than the original co - training algorithm .
transductive learning model
the setting of transductive inference was introduced by v .
vapnik ( see ( vapnik , 123 ) ) .
the learning task is dened on a xed array x of n points ( ( cid : 123 ) x123 , ( cid : 123 ) x123 , . . . , ( cid : 123 ) xn ) .
each data point has a desired classication y = ( y123 , y123 , . . . , yn ) .
for simplicity , lets assume the labels yi are binary , i . e .
yi ( +123 , 123 ) , and that the data points ( cid : 123 ) x are vectors in ( cid : 123 ) n .
for training , the learner receives the labels for a random subset |yl| = l
of l < n ( training ) data points .
the goal of the learner is to predict the labels of the remaining ( test ) points in x as accurately as possible .
vapnik gives bounds on the deviation of error rates observed in the training sample and in the test sam - ple ( vapnik , 123 ) .
the bounds depend on the ca - x , which can be measured , for example , as the number of dierent la - belings the learner l can potentially produce on x .
the smaller d x , the smaller the bound .
following
x of the hypothesis space h
proceedings of the twentieth international conference on machine learning ( icml - 123 ) , washington dc , 123
the idea of structural risk minimization ( srm ) ( vap - nik , 123 ) , one can consider a sequence of learners l123 , l123 , . . .
with nested hypothesis spaces so that their x < . . .
if the struc - capacity increases d ture is well aligned with the learning task , the bound limits the generalization error .
what information can we exploit to built a good structure ?
principles for designing a
in contrast to inductive learning , the transductive learner can analyze the location of all data points ( cid : 123 ) xi x , in particular those in the test set .
therefore , a transductive learner can structure its hypothesis space based on x .
how can the location of the test points help design a good hypothesis space ? imagine we knew the labels of all examples ( cid : 123 ) xi x , in - cluding both the training and the test examples .
lets call this the perfect classication .
if we gave all these examples to an inductive learner lind , it would be learning from a large training set and it might be rea - sonable to assume that lind learns an accurate classi - er .
for example , our experience might tell us that for text classication , svms typically achieve low predic - tion error if we have at least 123 , 123 training examples .
therefore , if x contains 123 , 123 data points , we would expect that an svm given all labels y achieves low leave - one - out cross - validation error err ( x , y ) on ( x , y ) , since err loo ( x , y ) is an ( almost ) unbiased es - timate of the prediction error that typically has low how can this help a transductive learner that has ac - cess to only a small subset yl of the labels ? assum - ing that an inductive learner lind achieves low pre - diction error when trained on the full x implies that the perfect classication of the test set is highly self - consistent in terms of leave - one - out error .
if a trans - ductive learner ltrans uses only those labelings of the test set for which the corresponding inductive learner ltrans has low leave - one - out error , the transductive learner will not exclude the perfect classication , while potentially excluding bad labelings .
this suggests the following srm structure for a transductive learner
( y123 , . . . , yn ) |err
loo ( x , y ) k
leading to a general principle for dening a transduc - tive learner from an inductive learner .
a transductive learner should label the test set so that
postulate 123 : it achieves low training error , and
postulate 123 : the corresponding inductive learner is
highly self - consistent ( e . g .
low leave - one - out ) .
while initially not phrased in these terms , a trans - ductive svm ( vapnik , 123 ) ( joachims , 123 ) follows these two postulates ( joachims , 123 ) .
a trans - ductive svm labels the test examples so that the margin is maximized .
a large - margin svm can be shown to have low leave - one - out error ( vapnik , 123 ) .
other transductive learning algorithms like transduc - tive ridge - regression ( chapelle et al . , 123 ) and min - cuts ( blum & chawla , 123 ) minimize leave - one - out error as well .
however , leave - one - out is not the only measure of self - consistency .
the co - training algorithm ( blum & mitchell , 123 ) maximizes consistency be - tween two classiers .
it will be discussed in more detail in section 123 .
however , the following shows that postulates 123 and 123 are not enough to dene an eective transductive learner .
consider the case of k - nearest neighbor ( knn ) ( k odd ) .
the knn - rule makes a leave - one - out error on example ( ( cid : 123 ) xi , yi ) , if the majority of the nearest neighbors are not from the same class .
for a similarity - weighted knn , we can dene a margin - like quantity
i = yi
jknn ( ( cid : 123 ) xi ) yj wij mknn ( ( cid : 123 ) xi ) wim
where wij reects the similarity between xi and xj .
the similarity weighted knn - rule makes a leave - one - out error whenever i 123
therefore , an upper bound on the leave - one - out error is
( x , y ) n ( cid : 123 )
while it is computationally dicult to nd a label - ing of the test examples that minimizes the leave - one - out error of knn while having a low training error , there are ecient algorithms for minimizing the upper bound ( 123 ) .
we can write this as the following opti -
mknn ( ( cid : 123 ) xi ) wim
yi = 123 , if i yl and positive yi = 123 , if i yl and negative j : yj ( +123 , 123 )
in matrix notation , the objective can be written equiv - alently as ( cid : 123 ) yt a ( cid : 123 ) y where aij = if ( cid : 123 ) xj is among the k nearest neighbors of ( cid : 123 ) xi and zero oth - erwise .
while the problem is typically not convex , there are ecient methods for its solution .
ticular , a can be thought of as the adjacency ma - trix of a graph , so that vertices represent examples and edges represent similarities ( see figure 123 )
123 and 123 do not yet specify a transductive learner su - ciently .
one other reasonable constraint to put on the transductive solution is the following postulate .
figure 123
mincut example .
, denote with g+ the set of examples the solution ( cid : 123 ) y ( i . e .
vertices ) with yi = 123 , and with g yi = 123
g+ and g dene a cut ( bi - partitioning ) of the graph .
for an undirected graph , the cut - value ig aij is the sum of the edge - weights across the cut .
since the maximum of ( 123 ) is determined by the matrix entries aij with yiyj = yiyj =123 aij = 123 cut ( g+ , g mizes ( 123 ) .
therefore , for undirected graphs , maximiz - ing ( 123 ) subject to ( 123 ) - ( 123 ) is equivalent to nding the s - t mincut where the positive examples form the source , and the negative examples form the sink .
the s - t min - cut is a cut that separates positive and negative exam - ples while minimizing the cut value .
this connects to the transduction method of blum and chawla ( blum & chawla , 123 ) .
they use mincut / maxow algo - rithms , starting from the intuition that the separation of positive and negative examples should put strongly connected examples into the same class .
blum and chawla discuss the connection to leave - one - out error in the case of 123 - nn .
while the s - t mincut algorithm is intuitively appeal - ing , it easily leads to degenerate cuts .
consider the graph in figure 123 , where line thickness indicates edge weight aij .
the graph contains two training examples which are labeled as indicated .
all other nodes repre - sent test examples .
while we would like to split the graph according to the two dominant clusters , the s - t mincut leads to a degenerate solution that just cuts o the positive example .
the behavior is due to the fact that s - t mincut minimizes the sum of the weight , and that balanced cuts have more potential edges to cut .
while using a sparser graph would help in the example , this degenerate behavior also occurs for knn graphs whenever the correct partitioning has more wrong neighbors than edges connecting the training exam - ples to the unlabeled examples .
this is practically always the case for suciently large numbers of un - labeled examples .
consider an ( undirected ) 123 - nn graph , where each example has 123 of its neighbors in the correct class , and 123 in the incorrect class .
if there is one positive training example , then this example will on average have 123 in / out edges .
so , if there are more than 123 unlabeled examples , s - t mincut will return a degenerate cut even for such a strongly clus - tered graph .
since these degenerate cuts fulll postu - late 123 ( i . e .
zero training error ) and postulate 123 ( high self consistency in terms of leave - one - out ) , postulates
postulate 123 : averages over examples ( e . g .
average margin , pos / neg ratio ) should have the same ex - pected value in the training and in the test set .
again , this postulate can be motivated using the per - fect classication .
for example , the average margin of knn should fulll
p ( yl ) =
for the perfect classication , since the distribution p ( yl ) of drawing a training set is uniform over all sub - the s - t mincut violates postulate 123 both for the pos / neg ratio , as well as for the average margin .
in particular , training examples have negative mar - gins , while test examples have large margin .
other functions are conceivable as well ( joachims , 123 ) .
blum and chawla experiment with dierent heuris - tics for pre - processing the graph to avoid degener - ate cuts .
however , none appears to work well across all problems they study .
other transductive learning algorithms have similar degeneracies .
for example , in transductive svms ( joachims , 123 ) and in co - training ( blum & mitchell , 123 ) the fraction of pos - itive examples in the test set has to be xed a priori .
such constraints are problematic , since for small train - ing sets an estimated pos / neg - ratio can be unreliable .
how can the problem of degenerate cuts be avoided in a more principled way ?
normalized graph cuts with
the problem of s - t mincut can be traced to its objec - tive function ( 123 ) , which aims to minimize the sum of the edge weights cut through .
the number of elements in the sum depends directly on the size of the two cut in particular , the number of edges a cut with |g+| vertices on one side and |g | vertices on the other side can potentially cut through is |g+||g | .
the s - t mincut objective is inappropriate , since it does not account for the dependency on the cut size .
a natural way to normalize for cut size is by dividing the objec - tive with |g+||g | .
instead of minimizing the sum of the weights , the following optimization problem mini - mizes the average weight of the cut .
| ( i : yi = 123 ) || ( i : yi = 123 ) | yi = 123 , if i yl and positive yi = 123 , if i yl and negative ( cid : 123 ) y ( +123 , 123 ) n
this problem is related to the ratiocut ( hagen & kahng , 123 ) .
however , the traditional ratiocut prob - lem is unsupervised , i . e .
there are no constraints ( 123 ) and ( 123 ) .
solving the unconstrained ratiocut is known to be np hard ( shi & malik , 123 ) .
however , e - cient methods based on the spectrum of the graph exist that give good approximations to the solution ( hagen & kahng , 123 ) .
the following will generalize these methods to the case of constrained ratiocuts for lets denote with l = ba the laplacian of the graph with adjacency matrix a and diagonal degree matrix b , bii = j aij .
we require that the graph is undi - rected , so that l is symmetric positive semi - denite .
following ( dhillon , 123 ) and ignoring the constraints , the unsupervised ratiocut optimization problem can equivalently be written as
with zi ( + , )
| ( i : zi>123 ) | and =
where + = | ( i : zi<123 ) | .
it is straightforward to verify that ( cid : 123 ) zt ( cid : 123 ) z = n and ( cid : 123 ) zt 123 = 123 for every feasible point .
while this problem is still np hard , the minimum of its real relaxation
( cid : 123 ) zt 123 = 123 and ( cid : 123 ) zt ( cid : 123 ) z = n
is equal to the second eigenvalue of l and the corre - sponding eigenvector is the solution .
using this solu - tion of the relaxed problem as an approximation to the solution of ( 123 ) is known to be eective in practice .
moving to the supervised ratiocut problem , we pro - pose to include constraints ( 123 ) and ( 123 ) by adding a quadratic penalty to the objective function .
( cid : 123 ) zt l ( cid : 123 ) z + c ( ( cid : 123 ) z ( cid : 123 ) ) t c ( ( cid : 123 ) z ( cid : 123 ) ) ( cid : 123 ) zt 123 = 123 and ( cid : 123 ) zt ( cid : 123 ) z = n
for each labeled example , the corresponding element of ( cid : 123 ) is equal to + ( ) for positive ( negative ) exam - ples , and it is zero for test examples .
+ and are estimates of + and ( e . g .
based on the number of observed positive and negative examples in the train - ing data ) .
we will see later that these estimates do not need to be very precise .
c is a parameter that trades o training error versus cut - value , and c is
a diagonal cost matrix that allows dierent misclas - sication costs for each example .
taking the eigen - decomposition l = uu t of the laplacian , one can introduce a new parameter vector ( cid : 123 ) w and substitute ( cid : 123 ) z = u ( cid : 123 ) w .
since the eigenvector of the smallest eigen - value of a laplacian is always ( cid : 123 ) 123 , the constraint ( 123 ) be - comes equivalent to setting w123 = 123
let v ( d ) be the matrix with all eigenvectors u ( eigenvalues ) except the smallest one , then we get the following equivalent
( cid : 123 ) wt d ( cid : 123 ) w + c ( v ( cid : 123 ) w ( cid : 123 ) ) t c ( v ( cid : 123 ) w ( cid : 123 ) ) ( 123 ) ( cid : 123 ) wt ( cid : 123 ) w = n
dening g = ( d + cv t cv ) and ( cid : 123 ) b = cv t c ( cid : 123 ) , the objective function can also be written as ( cid : 123 ) wt g ( cid : 123 ) w 123 ( cid : 123 ) bt ( cid : 123 ) w + c ( cid : 123 ) t c ( cid : 123 ) , where the last term can be dropped since it is constant .
following the argument in ( gan - der et al . , 123 ) , problem ( 123 ) - ( 123 ) is minimized for is the smallest eigenvalue
= v ( cid : 123 ) w
i is the identity matrix .
from this we can compute the optimal value of ( 123 ) and ( 123 ) as ( cid : 123 ) z a predicted value for each example .
we can use this value to rank the test examples , or use a threshold to make hard class assignment .
an obvious choice for the threshold is the midpoint = 123 123 ( + + ) which we will use in the following , but more rened methods are probably more appropriate .
spectral graph transducer
the basic method for computing supervised rati - ocuts suggests the following algorithm for trans - ductive learning , which we call a spectral graph transducer ( sgt ) .
an implementation is available at input to the algorithm are the training labels yl , and a weighted undirected graph on x with adjacency matrix a .
in the following , we will use the similarity - weighted k nearest - neighbor
if ( cid : 123 ) xj knn ( ( cid : 123 ) xi ) ( cid : 123 ) + a
( cid : 123 ) t .
the rst over x symmetricized by a = a step preprocesses the graph , which has to be done only
compute diagonal degree matrix b , bii = compute laplacian l = b a , or compute nor - malized laplacian l = b to the normalized - cut criterion of ( shi & malik ,
compute the smallest 123 to d + 123 eigenvalues and
eigenvectors of l and store them in d and v .
to normalize the spectrum of the graph , replace the eigenvalues in d with some monotonically in - creasing function .
we use dii = i123 ( see also ( chapelle et al . , 123 ) ) .
fixing the spectrum of the graph in this way abstracts , for example , from dierent magnitudes of edge weights , and focuses on the ranking among the smallest cuts .
l+ and =
the following steps have to be done for each new train -
estimate + =
l , where l+ ( l ) is the number of positive ( negative ) training examples .
set i = + ( i = ) for positive
to give equal weight to positive and negative ex - amples , set cost of positive ( negative ) training ex - amples to cii = l 123l ) .
cii = 123 for all
123l+ ( cii = l
compute g = ( d+cv t cv ) and ( cid : 123 ) b = cv t c ( cid : 123 ) and
nd smallest eigenvalue compute predictions as ( cid : 123 ) z
of equation ( 123 ) .
= v ( g
which can be used for ranking .
class assignments yi = sign ( zi ) .
123 ( + + ) to get hard
connection to transductive svms
the following argument shows the relationship of the sgt to a tsvm .
we consider the tsvm as described in ( joachims , 123 ) .
for hyperplanes passing through the origin , the tsvm optimizes min ( cid : 123 ) y max ( cid : 123 ) 123 123t 123
yi = 123 , if i yl and positive
123 ( cid : 123 ) t diag ( ( cid : 123 ) y ) a diag ( ( cid : 123 ) y ) ( cid : 123 ) ( 123 ) yi = 123 , if i yl and negative ( 123 ) ( cid : 123 ) y ( +123 , 123 ) n | ( i : yi = 123 ) | = p .
for our analysis , we simplify this problem by adding the constraint 123 = 123 = . . .
since the objective ( 123 ) can now be written as n 123 123 123 ( cid : 123 ) yt a ( cid : 123 ) y where is a scalar , the maximum is achieved for ( cid : 123 ) yt a ( cid : 123 ) y .
substituting the solution into the objective shows that the value of the maximum is 123 ( cid : 123 ) ya ( cid : 123 ) y .
this shows that the simplied tsvm problem is equivalent to an s - t mincut on graph a , where the balance of the cut is xed by ( 123 ) .
the sgt removes the need for xing the exact cut size a priori .
connection to co - training
co - training can be applied , if there are two redun - dant representations a and b of all training exam - ples ( blum & mitchell , 123 ) .
the goal is to train two classiers ha and hb , one for each representation , so that their predictions are maximally consistent , i . e .
ha ( ( cid : 123 ) x ) = hb ( ( cid : 123 ) x ) for most examples ( cid : 123 ) x .
with this goal , blum and mitchell propose a greedy algorithm that it - eratively labels examples for which one of the current classiers is most condent .
however , also for this al - gorithm the ratio of predicted positive and negative examples in the test set must be xed a priori to avoid co - training emerges as a special case of the sgt .
con - sider a knn classiers for each of the two representa - tions and note that i ) ( see eq .
( 123 ) ) is an upper bound on the number of inconsistent pre - dictions .
therefore , to maximize consistency , we can apply the sgt to the graph that contains k links for the knn from representation a , as well as another k links per example for the knn from representation b .
connections to other work
several other approaches to using unlabeled data for supervised learning exist .
most related is the approach to image segmentation described in ( yu et al . , 123 ) .
they aim to segment images under higher level con - straints .
one dierence is that they arrive at con - strained cut problems where all the constraints are homogeneous , leading to a dierent technique for their solution .
the spectrum of the laplacian is also consid - ered in the recent work in ( chapelle et al . , 123 ) and ( belkin & niyogi , 123 ) .
they use the leading eigen - vectors for feature extraction and the design of kernels .
in addition , chapelle et al .
use the same normaliza - tion of the spectrum .
szummer and jaakkola apply short random walks on the knn graph for labeling test examples , exploiting that a random walk will less likely cross cluster boundaries , but stay within clusters ( szummer & jaakkola , 123 ) .
there might be an in - teresting connection to the sgt , since the normalized cut minimizes the transition probability of a random walk over the cut ( meila & shi , 123 ) .
this might also lead to a connection to the generative modeling approach of nigam et al . , where the label of each test example is a latent variable ( nigam et al . , 123 ) .
to evaluate the sgt , we performed experiments on six datasets and report results for all of them .
the datasets are the ten most frequent categories from the reuters - 123 text classication collection follow - ing the setup in ( joachims , 123 ) , the uci reposi -
table 123
prbep ( macro - ) averages for ve datasets and training sets of size l .
n is the total number of examples , and n is the number of features .
l sgt knn tsvm svm
123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123
tory datasets optdigits ( digit recognition ) , iso - let ( speech recognition ) , ionosphere , as well as the adult data in a representation produced by john platt .
to evaluate the co - training connection , we use the webkb data of blum and mitchell with tfidf the goal of the empirical evaluation is threefold .
first , we will evaluate whether the sgt can make use of the transductive setting by comparing it against induc - tive learning methods , in particular knn and a linear svm .
second , we compare against existing transduc - tion methods , in particular a tsvm .
and third , we evaluate how robustly the sgt performs over dier - ent data sets and parameter settings .
for all learning tasks , both knn and the sgt ( with 123 ( b a ) ) use the cosine normalized laplacian l = b as the similarity measure .
while this is probably sub - optimal for some tasks , the following results indicate that it is a reasonable choice .
furthermore , it equally aects both knn and the sgt , so that relative com - parisons between the two remain valid .
if an example has zero cosine with all other examples , it is randomly connected to k nodes with uniform weight .
to make sure that performance dierences against the other learning methods are not due to bad choices for their parameters , we give the conventional learning methods ( i . e .
knn , svm , tsvm ) an unfair advan - tage .
for these methods we report the results for the parameter setting with the best average performance on the test set .
for the sgt , on the other hand , we chose c = 123 and d = 123 constant over all datasets .
the choice of k for building the k nearest - neighbor graph is discussed below .
all results reported in the following are averages over 123 stratied transductive samples123
so , all substan - tial dierences are also signicant with respect to this distribution .
samples are chosen so that they contain at least one positive example .
while we report er - ror rate where appropriate , we found it too unstable
123an exception is the tsvm on adult and isolet , where
123 samples would have been prohibitively expensive .
table 123
prbep / error rate on the webkb course data averaged over training sets of size 123
tsvm svm b&m
page+link 123 / 123 123 / 123 123 / 123 123 / 123
123 / 123 123 / 123 123 / 123 123 / 123 - / 123 123 / 123 123 / 123 123 / 123 123 / 123 - / 123
for a fair comparison with very unbalanced class ra - tios .
we therefore use rank - based measures for most comparisons .
the most popular such measure in infor - mation retrieval is the precision / recall curve , which we summarize by its break - even point ( prbep ) ( see e . g .
( joachims , 123 ) ) .
for tasks with multiple classes ( i . e .
reuters , optdigits , and isolet ) , we summa - rize the performance by reporting an average over all classes ( i . e .
macro - averaging ) .
does the unlabeled data help improve predic - tion performance ? the results are summarized in table 123
on all tasks except ionosphere , the sgt gives substantially improved prediction performance compared to the inductive methods .
also , the sgt performs better than knn ( as its inductive variant ) , on each individual binary task of reuters and optdig - its .
for isolet , the sgt performs better than knn on 123 of the 123 binary tasks .
the improvements of the tsvm are typically smaller .
on adult , the tsvm was too inecient to be applied to the full dataset , so that we give the results for a subsample of size 123
the tsvm failed to produce reasonable results for isolet .
while the tsvm does improve performance on reuters , the improvement is less than reported in ( joachims , 123 ) .
there , the as - sumption is made that the ratio of positive to negative examples in the test set is known accurately .
however , this is typically not the case and we use an estimate based on the training set in this work .
if the true fraction is used , the tsvm achieves a performance of 123 .
while ( joachims , 123 ) proposes measures to de - tect when the wrong fraction was used , this can only be done after running the tsvm .
repeatedly trying dierent fractions is prohibitively expensive .
how eective is the sgt for co - training ? ta - ble 123 shows the results for the co - training on webkb .
we built the graph with 123nn from the page and 123nn from the links .
the table compares the co - training setting with just using the page or the links , and a combined representation where both feature sets are concatenated .
the sgt in the co - training set - ting achieves the highest performance .
the tsvm also gives large improvements compared to the induc - tive methods , outperforming the sgt .
however , the tsvm cannot take advantage of the co - training set -
number of training examples
value of c
figure 123
amount by which the prbep of the sgt ex - ceeds the prbep of the optimal knn .
figure 123
amount by which the prbep of the sgt is lower for a particular value of c compared to the best c .
number of eigenvectors
number of nearest neighbors
figure 123
amount by which the prbep of the sgt is lower for a particular number of eigenvectors d compared to the prbep of the best d ( l as in tables 123 and 123 ) .
the results from ( blum & mitchell , 123 ) are added in the last column .
for which training set sizes is transductive learning most eective ? figure 123 shows the dif - ference in average prbep between the sgt and knn for dierent training set sizes .
for all learn - ing tasks , the performance improvement is largest for small training sets .
for larger sets , the performance of the sgt approaches that of knn .
the negative values are largely due to the bias from selecting the param - eters of knn based on the test set .
if this is also allowed for the sgt , the dierences vanish or become
how sensitive is the sgt to the choice of the number of eigenvectors ? figure 123 plots the loss of prbep compared to the prbep achieved on the test set for the optimal number of eigenvectors .
on all tasks , the sgt achieves close to optimal performance , if more than 123 eigenvectors are included .
we conclude that d 123 should be sucient for most tasks .
figure 123
average prbep and the average normalized value of the objective function ( vertically ipped and po - sitioned to t the graph ) for the sgt depending on the number of nearest neighbors ( l as in tables 123 and 123 ) .
how sensitive is the sgt to the choice of the error parameter ? analogous to figure 123 , figure 123 plots the loss of prbep compared to the best value of c .
due to the normalization of the spectrum of the laplacian , the optimum values of c are comparable between datasets .
for most tasks , the performance is less than two prbep points away from the optimum for any c between 123 and 123
an exception is isolet , which requires larger values .
we conclude that any c between 123 and 123 should give reasonable performance for most tasks .
how sensitive is the sgt to the choice of the graph ? unlike c and d , the choice of k for building the k nearest - neighbor graph has a strong inuence on the performance .
the top part of figure 123 shows aver - age prbep depending on k .
how should we select k ? for small training set sizes ( often only one positive ex - ample ) , cross - validation is not feasible .
however , the value of the objective function can be interpreted as a
measure of capacity and might be suitable for model selection .
the bottom half of figure 123 shows the aver - age value of the objective function after normalization .
in particular , the objective value oik for training set i and choice of k is normalized to onorm the average normalized objective tracks the perfor - mance curve very well , suggesting that there might be an interesting connection between this value and the capacity of the sgt .
for all experiments reported in the previous section , we used the value of k that min - imizes the average normalized objective .
for adult , this is k = 123 , for reuters k = 123 , for optdigits k = 123 , for isolet k = 123 , for ionosphere k = 123 , and for co - training k = 123 123
such a kind of model selection might be particularly useful for tasks like rel - evance feedback in information retrieval , where there are many learning tasks with few examples on the same collection of objects .
how eciently can the sgt be trained ? due to our naive implementation , most of the time is spent on computing the k - nn graph .
however , this can be sped up using appropriate data structures like inverted indices or kd - trees .
computing the 123 smallest eigen - values takes approximately 123 minutes for a task with 123 , 123 examples and 123 neighbors on a 123ghz cpu using matlab .
however , these preprocessing steps have to be performed only once .
training on a particular training set and predicting 123 , 123 test examples takes less than one second .
we studied existing transductive learning methods and abstracted their principles and problems .
based on this , we introduced a new transductive learning method , which can be seen as the a transductive ver - sion of the knn classier .
the new method can be trained eciently using spectral methods .
we evalu - ated the classier on a variety of test problems showing substantial improvements over inductive methods for small training sets .
unlike most other algorithms that use unlabeled data , it does not need additional heuris - tics to avoid unbalanced splits .
furthermore , since it does not require greedy search , it is more robust than existing methods , outperforming the tsvm on most tasks .
modeling the learning problem as a graph oers a large degree of exibility for encoding prior knowledge about the relationship between individual examples .
in particular , we showed that co - training arises as a special case and that the new algorithm outperforms the original co - training algorithm .
the algorithm opens interesting areas for research .
in par - ticular , is it possible to derive tight , sample dependent capacity bounds based on the cut value ? furthermore , it is interesting to consider other settings beyond co - training that can be modeled as a graph ( e . g .
temporal
drifts in the distribution , co - training with more than two views , etc . ) .
this research was supported in part by the nsf projects iis - 123 and iis - 123 and by a gift from google .
thanks to lillian lee , filip radlinski , bo pang , and eric breck for their insightful comments .
