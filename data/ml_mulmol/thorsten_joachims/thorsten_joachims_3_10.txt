we address the task of learning rankings of documents from search engine logs of user behavior .
previous work on this problem has relied on passively collected clickthrough data .
in contrast , we show that an active exploration strategy can provide data that leads to much faster learning .
specically , we develop a bayesian approach for selecting rankings to present users so that interactions result in more informative training data .
our results using the trec - 123 web corpus , as well as synthetic data , demonstrate that a directed ex - ploration strategy quickly leads to users being presented im - proved rankings in an online learning setting .
we nd that active exploration substantially outperforms passive obser - vation and random exploration .
categories and subject descriptors h . 123 ( information storage and retrieval ) : information search and retrieval
algorithms , measurement , performance
clickthrough data , web search , active exploration , learn - ing to rank
there has recently been an interest in training search en - gines automatically using machine learning ( e . g .
( 123 , 123 , 123 ) ) .
the ideal training data would be rankings of documents or - dered by relevance for some set of queries .
in some cases it is practical to hire experts to manually provide relevance information for particular queries ( as in ( 123 ) ) , but usually data provided by experts is too expensive and is not guar - anteed to agree with the judgments of regular users .
this is a particular problem when typical user queries are short and ambiguous , as is often the case in web search .
permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for prot or commercial advantage and that copies bear this notice and the full citation on the rst page .
to copy otherwise , to republish , to post on servers or to redistribute to lists , requires prior specic permission and / or a fee .
kdd123 , august 123 , 123 , san jose , california , usa .
copyright 123 acm 123 - 123 - 123 - 123 - 123 / 123 / 123 . . . $123 .
as an alternative source of training data , previous work has used clickthrough logs that record user interactions with search engines .
however , as far as we are aware , all previous work has only used logs collected passively , simply using the recorded interactions that take place anyway .
we instead propose techniques to guide users so as to provide more use - ful training data for a learning search engine .
to see the limitations of passively collected data , consider the typical interactions of search engine users .
usually , a user executes a query then perhaps considers the rst two or three results presented ( 123 , 123 ) .
the feedback ( clicks ) on these results is recorded and used to infer relevance judg - ments .
these judgments are then used to train a learning algorithm such as a ranking support vector machine ( 123 ) .
in particular , users very rarely evaluate results beyond the rst page , so the data obtained is strongly biased toward documents already ranked highly .
highly relevant results that are not initially ranked highly may never be observed and evaluated , usually leading to the learned ranking never converging to an optimal ranking .
to avoid this presentation eect , we propose that the ranking presented to users be optimized to obtain useful data , rather than strictly in terms of estimated document relevance .
for example , one possibility would be to inten - tionally present unevaluated results in the top few positions , aiming to collect more feedback on them .
however , such an ad - hoc approach is unlikely to be useful in the long run and would hurt user satisfaction .
we instead introduce princi - pled modications that can be made to the rankings pre - sented .
these changes , which do not substantially reduce the quality of the ranking shown to users , produce much more informative training data and quickly lead to higher quality rankings being shown to users .
the primary contribution of this paper is to present a principled approach to eciently obtain training data that leads to rankings of higher quality .
we start by presenting a summary of observations about user behavior in section 123 , as how real users behave guides our approach .
next , in sec - tion 123 we formalize the learning problem as an optimization task , present a suitable bayesian probabilistic model and discuss inference and learning .
following this we present strategies to modify the rankings shown to users so that performance of learned rankings improves rapidly over time in section 123
we describe our evaluation method in section 123 and present results on synthetic data and trec - 123 web data in section 123
in particular , we see the improvements using our exploration strategies are much faster than with passive or random data collection .
user behavior
learning rankings relies on training data collected from users .
we will now examine the specic properties of user behavior as they will guide our approach for the remainder of this work .
in particular we will see what sort of data can reasonably be collected from clickthrough logs and how we should include user behavior in our learning task .
a number of studies have shown that users tend to click on results ranked highly by search engines much more of - ten than those ranked lower .
for example , in recent work agichtein et al . ( 123 ) present a summary distribution of the rel - ative click frequency on web search results for a large search engine as a function of rank for 123 , 123 searches for 123 , 123 queries .
they show that the relative number of clicks rapidly drops with the rank compared with the top ranked result , they observe approximately 123% as many clicks on the sec - ond result , 123% as many clicks on the third , and 123% as many clicks on the fourth .
while we may hypothesize that this is simply because better results tend to be presented higher , joachims et al . ( 123 ) showed that there is an inherent bias to rank in user behavior .
they showed that users still click more often on higher ranked results even if presented with rankings reversing the top ten results .
in fact , eye tracking studies performed by granka et al . ( 123 ) show that the probability that users even look at a search result decays very quickly with rank .
on the one hand , this explains why most common per - formance measures in information retrieval place greater emphasis on highly ranked results ( such as mean average precision , normalized discriminative cumulative gain and mean reciprocal rank ) .
on the other hand , this observa - tion means that rank strongly inuences how many times a document is evaluated by users , and hence how much train - ing data can be collected from clickthrough logs .
given the recorded clicks , the question also remains how best to interpret the log entries to infer relevance infor - mation about a document collection .
two alternative ap - proaches to interpreting clickthrough logs are ( 123 ) consider a click on a document in a result set as an absolute sign of rel - evance , or ( 123 ) consider a click on a document as a pairwise judgment comparing the relevance of that document to some other document considered earlier .
specically , a common approach is to assume that a clicked on result is more rel - evant than a non - clicked higher ranked result .
joachims et al . ( 123 ) showed that interpreting clicks as relative relevance judgments is generally reliable , while absolute judgments are not .
such pairwise relevance judgments have been success - fully used to learn improved rankings ( 123 , 123 ) .
a nal observation of user behavior is that clickthrough logs are inherently very noisy .
users often click on search results without carefully considering them ( 123 ) .
in partic - ular this means that any single judgment that states that one document is more relevant than another has a signi - cant probability of being incorrect .
however , previous work has shown that if the dierence in relevance between docu - ments is larger , relative relevance judgments are less likely to be noisy ( 123 ) .
this work also showed that if adjacent pairs of documents in the ranking shown to users are randomly swapped to compensate for presentation bias , provably re - liable pairwise relevance judgments about adjacent pairs of documents can be collected .
we summarize that ( 123 ) clickthrough data is best inter - preted as relative relevance judgments; ( 123 ) the relevance
judgments are noisy; ( 123 ) the top ranked documents are the most important to estimate correctly .
guided by these prop - erties , we now turn to formalizing the learning problem .
learning problem
assume we have a document corpus c = ( d123 , .
, d|c| ) and some xed user query q .
for this query , we want to i < of each document estimate the average relevance di ( for some user population ) .
from the previous section , we know that users can provide us with noisy judgments of j .
we assume that some ranking function can provide initial estimates of i .
the goal is accurately
i with as little training data as possible .
the estimation task involves a three step iterative process : first , given relevance estimates , we must select a ranking to display to users .
second , given the ranking displayed , users provide relevance feedback .
third , using the relevance feedback , we update the relevance estimates and repeat the process for the next user .
in this paper , we focus most on the rst step , namely selecting rankings of documents to show users so that the collected judgments allow the relevance estimates to be improved quickly , while at the same time maximizing the quality of the rankings .
123 probabilistic model |c| ) m be the true relevance values of the documents in c .
modeling the problem of nding m given training data d in a bayesian framework , we want to maintain our knowledge about m in the distribution
let m = (
p ( m|d ) =
p ( d|m ) p ( m )
we assume that p ( m|d ) is a multivariate normal with zero
p ( m|d ) = n ( 123 , .
, |c|; 123
graphically , we can draw p ( m|d ) as a set of gaussians centered at i with variance 123
for example :
, 123|c| )
this model is motivated by ability estimates maintained for chess players ( 123 ) .
in the most closely related previous work , chu and ghahramani address a similar problem using gaussian processes ( 123 ) .
however , instead maintaining the distribution p ( m|d ) , they directly estimate m given d .
this is also true of other related prior work ( 123 , 123 , 123 ) .
the key dierence in our approach is that we are not simply nd - ing the optimizing ranking .
rather , maintaining p ( m|d ) is key as it allows us to optimize for collected training data .
we measure the dierence between relevance assignments using a loss function l : m m < .
to nd good rele - vance estimates , we want to nd an m = ( 123 , .
, |c| ) m such that l ( m , m ) is small .
noting that m is unknown , we want to nd the ranking that minimizes the expected loss given what we know about m , namely p ( m|d ) :
emp ( m|d ) ( l ( m , m
where m is drawn from the probability distribution p ( m|d ) .
suppose the loss function l can be decomposed over pairs of documents in c .
we can then decompose the expected loss into a form easier to work with :
ep ( m|d ) ( l ( m , m
, i , j )
= ep ( m|d )
, i , j )
where p ( m|d ) is shorthand for m p ( m|d ) .
we will now show that the mode of p ( m|d ) , namely m = ( 123 , .
, |c| ) , is often the solution to equation 123
con - sider solving equation 123 for a loss function that counts the number of misordered pairs of documents .
the assignment with minimum expected loss is the mode of p ( m|d ) .
lemma 123
let p ( m|d ) = n ( 123 , .
, |c|; 123 , .
, |c| ) be a distribution over models .
assume l ( m , m ) counts the number of dierently ordered pairs of documents , when they are sorted by i and
i respectively .
a solution of emp ( m|d ) ( l ( m , m
is m = ( 123 , .
, |c| ) .
assume m opt= ( opt
, opt|c| ) is the minimizing relevance assignment , and has lower loss than m .
there must exist two documents di and dj that are ranked adja - cently when documents are ordered by m opt yet are ordered dierently by m , i . e .
opt and i < j .
let m f lip be the ranking obtained by reversing di and dj .
let these rankings have expected loss ef lip and eopt .
i > opt
as the documents are adjacent , the loss of m opt and m f lip only diers in the contribution of the pair ( di , dj ) .
plugging in the loss function we get ef lip eopt = p (
j ) p (
123@ i j
123@ j i
123a < 123
where is the cumulative distribution function of the stan - dard normal distribution , since i < j .
hence we have a contradiction as m opt is not the minimizing ranking .
we see a similar result for the loss function that penal - izes any error in the dierence of document relevances , i . e .
lpair ( m , m , i , j ) = ( ( i j ) (
lemma 123
let p ( m|d ) = n ( 123 , .
, |c|; 123 , .
, |c| ) be a distribution over models .
assume lpair ( m , m , i , j ) = ( ( i j ) (
a solution of emp ( m|d ) ( l ( m , m
for m and m respectively .
let ij = ( 123 j ) 123 / 123
the contribution to the expected loss for the pair of documents ( di , dj ) is ep ( m|d ) ( lpair ( m , m
, i , j ) )
ij + ( ij opt ij = ij .
hence m opt = m mini - which is minimized if opt mizes all terms in the sum in equation 123 simultaneously and thus minimizes the expected loss .
we see that the mode of the distribution p ( m|d ) minimizes the expected loss for two reasonable loss functions .
as it can also be obtained very eciently given p ( m|d ) , for the remainder of this work we will assume that the mode is , or is close to , the minimizer of the expected loss .
we will refer to the ranking obtained by sorting documents by their relevance according to the mode of p ( m|d ) as the mode 123 loss function
given our analysis of real user behavior in section 123 , we see that the loss functions discussed above are too simple .
specically , two properties to expect of an appropriate loss function are ( 123 ) the loss for ranking a less relevant docu - ment above a more relevant document should be larger if the documents are presented higher in the ranking ( i . e .
where users are more likely to observe them ) ; ( 123 ) the loss should be larger if the dierence in relevance is larger .
to the best of our knowledge there is no common pairwise decomposable loss function with these properties so we propose a quadratic hinge - loss function with cost of misordering decaying expo - nentially with rank : with rij we denote the minimum rank of di or dj when all documents are ordered by m ( i . e .
the relevance assignments used to present results to users ) divided by 123 , and 123 is the indicator function .
a pair of documents is considered misordered if the relative ranking according to m does not agree with that according to m .
making use of the pairwise form of the loss function and plugging in the mode ranking m , the inner term of equation 123 can now be written as ep ( m|d ) ( lpair ( m , m
j ) 123 123misordered
, i , j ) = e
j , ij = i j and 123
, i , j ) ) j|j , j ) lpair ( m , m
i |i , i )
j , noting that the dierence of two normally distributed variables is also normally distributed .
plugging in the loss , and choos - ing to sum over the pairs such that ij is always negative , equation 123 becomes :
ij = 123
, i , j ) d
ij , ij , rij ) d
is m = ( 123 , .
, |c| ) .
let m opt be the minimizing model .
let opt dj according to m opt , and ij and
be the dierence in relevance estimates of di and ij be dened equivalently
where erf ( ) is the error function .
substituting this into equation 123 gives an easy to compute closed form expres - sion for the expected loss .
123 estimating p ( m|d )
as discussed in section 123 , clickthrough data is best inter - preted as relative relevance judgments .
we can write them in the form di ( cid : 123 ) dj , indicating that di was judged more relevant that dj .
a standard approach to modeling noise in pairwise comparisons is to assume that the probability of an outcome is determined by the bradley - terry model ( 123 ) :
i i +
j ) ( si e ( s|i , j , 123
p123 + 123q123 / 123
p ( di ( cid : 123 ) dj ) =
rel ( di ) + rel ( dj )
e ( s|i , j , 123
j ) ( ij ) / 123
123 + 123
where rel ( di ) is the relevance of di .
the bradley - terry model can be reparameterized setting rel ( di ) = 123i / where is a known , global and xed parameter .
assuming the pairwise judgments are independent ( as can be reasonably expected with clickthrough data from multiple users ) , p ( d|m = ( 123 , .
, |c| ) ) =
p ( di dj|i , j )
123 + 123 ( ij ) /
given this likelihood model and a gaussian prior , we can ap - ply an o - the - shelf algorithm to maintain p ( m|d ) , namely the glicko rating system commonly used for rating chess players ( 123 ) .
given an estimate of player ability ( document relevance ) i and error in the estimate i , this algorithm provides a set of approximate online update equations for maintaining the estimated relevance and error as data is col - lected .
the update to the estimates for di following a single comparison to dj ( where si is 123 if di wins and 123 otherwise ) is presented in table 123
while it would also be interesting to compare alternative ways of maintaining p ( m|d ) ( e . g .
( 123 ) ) , or using a batch algorithm ( see ( 123 ) for a discussion of alternatives ) , the sim - plicity and online aspects of the glicko system are appealing .
in particular , in real world settings where large amounts of data are collected for large document collections with a large number of queries , a global optimization is likely to be slow and thus infeasible .
exploration strategies
as we have seen that users are much more likely to pro - vide feedback on highly ranked documents , we turn to the question of optimizing the data collection process to most quickly minimize the loss .
in particular , by selecting which documents to present at high rank , we inuence the pairs of documents for which we obtain relevance judgments .
in this paper , we will consider only modications that change two documents in a ranking , limiting ourselves to the top two most of the time .
we will see that despite the simplicity of this approach , substantial improvements in performance can be obtained at small cost in presented ranking quality .
we consider the following algorithms for determining which
ranking to present users .
passive collection ( top123 ) .
present the mode ranking , i . e .
sorting documents by m = ( 123 , .
, |c| ) .
the algorithm top123 assumes no changes are made to the mode ranking , ignoring bias in data collection .
this is the
e ( s|i , j , 123
j ) ( 123 e ( s|i , j , 123
table 123 : glicko update equations
approach used in all previous work in learning to rank , and would be eective if users provided feedback about results throughout the ranking .
in some settings this may be the case , for example in search engines for academic articles where many users thoroughly consider all retrieved results .
however in general web search settings , as discussed above , users focus their attention on the highest ranked results .
random exploration ( random ) .
select a random pair of documents and present them rst and second .
then rank the remaining documents according to m .
this algorithm is a naive modication to the mode rank - ing .
two random documents are picked uniformly and in - serted at the top of the ranking presented to users .
given the uniform distribution , this perturbation is likely to often pick documents that have a low prior expectation of be - ing relevant , thus likely presents users with poorer results .
however , it benets from the potential for feedback on all documents regardless of rank , even in the presence of signif - icant user bias .
a similar method was proposed by pandey et al .
( 123 ) in the context of identifying new web pages that would soon become popular , suggesting to randomly insert new documents into web search results .
largest expected loss pair ( lelpair ) .
select the pair of documents di and dj that have the largest pairwise expected loss contribution , and present these rst and second .
rank the remaining documents according to m .
formally , this means we select the pair di and dj that satises :
ep ( m|d ) ( lpair ( m , m
, i , j ) )
lelpair selects the pair of documents with largest pairwise contribution to the expected loss out of all pairs of docu - ments .
by presenting these documents at a high rank , the feedback given on them will reduce the uncertainty in the relative relevance of the documents .
this will , in the long run , drive the expected loss contribution of the pair of doc - uments down .
given the glicko update rules , the pairwise contribution of all other pairs of documents will not increase .
hence this method will eventually drive the expected loss
additionally , due to the exponential decay in the loss of misordered pairs as the rank increases , lelpair tends to select pairs of documents where at least one has a high esti - mated relevance .
lower ranked documents are also eventu - ally selected , but only after high rank documents have been evaluated and their expected loss contribution is reduced .
if we ignore the eect of rank in the loss function , this ap - proach is similar to previous work in active learning where users are asked to label items where the predicted label is most uncertain ( e . g .
( 123 , 123 ) ) .
in our setting , document pairs with high pairwise contribution tend to be those with large estimated error in relevance .
one step lookahead ( osl ) .
for each pair of documents , compute the expected pairwise loss and the expected pair - wise loss after a comparison based on the bradley - terry model ( using m to estimate the probability of possible out - comes ) and glicko updates .
select the pair of documents with the largest expected reduction in the pairwise loss and present these rst and second .
rank the remaining doc - uments according to m .
formally , if m123 is the mode of p ( m|d ) after updating it given the outcome of a compari - son of di and dj , we select the pair di and dj that satises :
ep ( m|d ) ( lpair ( m , m ep ( m|d ) ( lpair ( m
, i , j ) )
, i , j ) )
intuitively , this algorithm performs approximate gradient descent on the loss function .
osl nds the pair whose con - tribution to the expected loss is likely to decrease most fol - lowing a pairwise comparison .
the expected contribution of the pair after a comparison is a weighted sum of the expected loss contribution for the two possible outcomes ( either di wins the comparison or dj wins ) .
in this computation , the eect of possible rank changes is ignored for eciency rea - sons .
this method is also related to an approach proposed by chajewska et al .
in the context of utility estimation where they found that the true utility of many dierent outcomes can be quickly discovered by maximizing the reduction in expected loss given new data ( 123 ) .
largest expected loss documents ( leldoc ) .
for each document di , compute the total contribution of all pairs including di to the expected loss of the ranking .
present the two documents with highest total contributions rst and second , and rank the remainder according to m .
formally , this method selects the pair di and dj that satises :
ep ( m|d ) ( lpair ( m , m
, i , a ) )
ep ( m|d ) ( lpair ( m , m
, j , a ) )
this method addresses a potential limitation of lelpair and osl : they only consider individual pairwise document con - tributions to the expected loss despite the contributions of pairs not being independent .
leldoc addresses this by com - puting the total contribution of each document by summing over all pairs including that document .
for example , if some document d is ranked third , its total contribution is that from d and the top ranked document , plus the contribution from d and the second document , plus that from d and the fourth document and so forth .
leldoc selects the two doc -
uments with highest total contributions and presents them rst and second .
by comparing these two documents and reducing the uncertainty in their relevances , we are likely to reduce the contributions to the risk of all pairs including the
an alternative selection algorithm proposed in previous ( 123 , 123 ) ) is to compare pairs of items such that the probability distribution over models changes most in terms of kl - divergence or entropy .
we do not pursue this alternative as it does not take into account the loss function
finally , we note that explorations strategies for rankings are related to the opponent assignment problem in sports tournaments .
however , there are two key dierences .
first , a tournament has a dierent concept of loss .
a criterion often optimized is the probability of the true best player winning the nal game ( e . g . ( 123 , 123 ) ) .
second , pairwise com - parisons in a tournament have no cost .
in most sports , a common constraint is that all teams or players must com - pete for at least n rounds .
this means that each item must be compared with some other item every round and the optimization problem is to select which pairs are com - pared such that the loss is eventually minimized rather than aiming to minimize the loss as quickly as possible .
evaluation methodology
we now have a number of strategies for eliciting useful training data from users of a search system , and have a method to estimate the relevance of the documents using our probabilistic model .
in this section , we will describe how these strategies were evaluated .
in particular , we will com - pare how eective each strategy is at improving the quality of the rankings shown to users .
we evaluate as follows : given an initial ranking of one thousand documents as returned by a search engine in re - sponse to a query , we derive a prior p ( m ) .
this prior initial - izes p ( m|d ) .
for a particular exploration strategy , we next select a ranking to present to users .
we evaluate the loss of the presented ranking and of the mode ranking derived from m .
next , we simulate user behavior on the presented ranking , using a simple behavioral model , and collect train - ing data that is used to update the model parameters .
we repeat this process 123 , 123 times for each initial ranking .
this experimental setup is formalized in algorithm 123
the behavioral model we use to simulate clickthrough data is detailed in algorithm 123
by using a simulation , it is possible to evaluate the exploration strategies in detail without needing large numbers of test subjects , and avoid eects that may be unique to specic users ( e . g .
to academic users ) .
our model simplies real behavior by assuming that users only click on top two results , and do so with probabil - ity specied by the bradley - terry model .
this is motivated by the fast decay observed in the number of clicks as rank in - creases in real search systems .
clearly , in a real setting some additional data would be collected from lower ranks , making the results we report conservative in this respect .
however , the amount of data collected about results at lower ranks would be signicantly smaller .
we repeated each experiment with either 123 or 123 ini - tial rankings , each giving a dierent initial set of relevance estimates .
we report the mean loss across all runs ( nor - malized such that the initial loss is 123 ) , or the mean average precision ( map ) .
in some results we present a single nal
algorithm 123 evaluation setup 123 : input : estimated relevances ( i ) for di c 123 : i 123 for di c 123 : for iteration 123 through 123 , 123 do 123 : update i , j , i , j per equations 123 and 123 123 : end for
pick two documents di , dj to rank 123st and 123nd randomly swap di and dj ( see section 123 ) show the selected ranking to user record training data given user feedback
algorithm 123 user behavioral model 123 : input : ranking of documents ( d123 , .
, d|c| ) 123 : input : true relevances of documents ( 123 : if u nif ormrandom ( 123 , 123 ) < 123 : winner is d123 : s123 123; s123 123 123 : winner is d123 : s123 123; s123 123 123 : end if
, 123 ) / 123 then
performance , i . e .
the loss or map after 123 , 123 pairwise com - parisons and model updates .
note that as our rankings are of 123 , 123 documents , 123 , 123 comparisons is on average just six noisy pairwise comparisons involving each document .
we start by evaluating the exploration strategies on syn - thetic data , where we evaluate their eectiveness if the as - sumed prior distribution matches the true generating model .
this will be followed by an evaluation using trec - 123 data .
123 synthetic data
we randomly generated a corpus of 123 documents with i drawn from n ( 123 , 123 ) .
we chose this scale as it is comparable to typical chess scores .
we then drew ten independent initial models , drawing i from i , 123 ) and initializing i = 123 123
we repeated this process three times , giving 123 initial rankings over three dierent random corpora .
for each initial ranking , we ran each strategy for 123 , 123 iterations .
figure 123 shows the loss of the mode ranking at each iteration .
along the horizontal axis is the number of pairwise comparisons .
after each pairwise comparison , p ( m|d ) is updated and a new pair to compare is selected .
the vertical axis is the average loss of the mode ranking relative to the initial average loss .
the error bars indicate one standard error in the mean scaled loss .
which exploration strategy learns fastest ? the rst question to answer is which strategy learns fastest .
the passive top123 approach does not lead to a meaningful overall reduction in the loss .
this is because our user model assumes that users only provide feedback on the top two documents .
after a few comparisons , the top few documents have their relative position correctly established and no new documents are ever compared again .
our other baseline algorithm , random , sees the loss decrease slowly .
we see that the other exploration strategies all perform substantially better than the baselines .
both lelpair and osl quickly reduce the total loss by selecting pairs of doc -
figure 123 : change in loss as a function of the number of pairwise comparisons for each exploration strat - egy on synthetic data
figure 123 : eect of weight of prior on the nal loss evaluated on synthetic data ( true noise is 123 = 123 )
uments with high contributions to the expected loss , and high expected reductions in it .
we see this improvement continues for a large number of comparisons .
in contrast , leldoc appears to asymptote more quickly .
this is be - cause the documents selected continue to be those at high ranks even after many comparisons .
in eect , leldoc is too biased toward highly ranked documents .
comparing with lelpair and osl , we see that while lower ranked doc - uments may have lower total contribution to expected loss , they often have higher individual pairwise contributions .
how robust is the approach to prior assumptions ? the second natural question to ask is how robust the results are to the weight given to the initial ranking , as in the case of real data the correct weight is likely to be unknown .
this weight is encoded by the initial values of i , i . e .
we now explore the eect of changing that value .
this experiment is possible because we know the level of noise used when generating synthetic data .
figure 123 shows the eect of selecting an assumed noise level that diers from the true noise level .
with our default setting , and that used to generate our synthetic datasets , the probability of a document in the top 123 according to the prior not being in the top 123 according to true relevance is
123 123 123 123 123 123 123 123 123 123 123 123 123loss as a fraction of original lossnumber of pairwise comparisonstop123randomlelpairoslleldocs 123 123 123 123 123 123 123 123 123 123 123 123 123 123loss after 123 pairwise comparisons , as a fraction of initial lossassumed initial standard deviation of prior relevance estimatestop123randomlelpairoslleldoc figure 123 : change in loss as a function of the number of pairwise comparisons for each selection algorithm on trec - 123 data
figure 123 : change in map score as a function of the number of pairwise comparisons for each selection algorithm on trec - 123 data
about 123% .
we can see that selecting a suboptimal 123 does not drastically reduce performance after a xed number of pairwise comparisons .
apart from leldoc , the best perfor - mance is achieved when the actual noise is correctly known .
interestingly , leldoc performs best when the error in the prior is underestimated as it then selects documents further down the ranking for comparison .
123 trec data
in addition to the synthetic data , we also evaluated the exploration strategies using the trec - 123 web track queries ( topics 123 through 123 ) in the wt123g document corpus .
this subset of the corpus includes 123 topics and topic de - scriptions , run as queries on documents that are part of the corpus .
as part of the 123th text retrieval confer - ence ( trec - 123 ) ( 123 ) , 123 teams submitted a ranking of doc - uments for each topic .
then , for each topic , documents ranked highly by the teams were manually judged to be ei - ther highly relevant ( relevance score of 123 ) , relevant ( score of 123 ) or non - relevant ( score of 123 ) .
all other documents in the corpus were assumed non - relevant ( score of 123 ) .
the discretized nature of these relevance judgments is unrealis - tic , as few documents are likely to have precisely the same relevance in the real world .
to compensate and make the learning problem more realistic , we added uniform random noise in the range ( 123 , 123 ) to the true relevance judgments , preserving the relative order of highly relevant , relevant and
for each of the 123 trec - 123 topics , we randomly selected two submissions and used the submitted scores to initialize our model .
we then repeated the evaluation described for synthetic data .
each submission includes a ranking of typi - cally 123 documents , with a score given to each document .
the scores are arbitrary and unnormalized .
clearly they can be interpreted as a prior in any number of ways .
as each ranking typically contains both highly relevant docu - ments and non - relevant documents , we chose to normalize the scores to a linear interval ( 123 + 123 , 123 123 ) with 123 = 123
the resulting scores were used to set the initial i .
the initial estimated error i were set to 123
this means that a document with a score near the maximum score is es - timated to have approximately a 123% percent chance of in fact being in the lower half of the ranking .
which exploration strategy learns fastest ? figure 123 shows the performance of the exploration strate - gies .
we see that the loss improves rapidly with lel - pair , osl and leldoc .
we also see that top123 performs as it did on the synthetic data .
one the other hand , ran - dom performs dierently : the loss of the mode ranking ini - tially increases , then improves slightly but remains high .
we believe that this is due to the mismatch between the prior and model .
when two documents are compared , if the outcome is not the expected one then the update to the rel - evance estimates can be large .
sometimes , the lower ranked document moves to a much higher rank , and the loss does not quickly recover due to few comparisons per document .
interestingly , performance of random depends on the loss of the initial ranking .
when the initial loss is high , after 123 , 123 pairwise comparisons the loss tends to be reduced .
the op - posite is true when we start with a very good ranking .
how do the strategies perform with respect to map ? the loss function presented earlier is not one commonly used to evaluate rankings .
a measure much more widely used by the research community is the mean average precision ( map ) .
to compute the map , we consider each document with true relevance i above some threshold as relevant and others as irrelevant .
the average precision of a ranking is the average of the precision measured at each relevant docu - ment123
the map score is the mean of the average precisions across all 123 experiments .
we used a threshold of 123 scaled in the same way the scores were scaled .
figure 123 shows how the map of the ranking changes as more pairwise comparisons are performed .
map visibly be - haves very similarly to our loss function .
we see that lel - pair and osl result in the largest map improvement , and appear likely to continue to improve further with more com - parisons .
as before , leldoc performance plateaus quickly and top123 sees almost no improvement .
random performs poorly , with an initial drop in map although after 123 , 123 pairwise comparisons the map is above baseline .
for simplicity , we only consider the 123 ranked documents when computing the map score .
while the corpus may contain relevant documents that never made it into the top 123 , these do not con - tribute to the map score .
