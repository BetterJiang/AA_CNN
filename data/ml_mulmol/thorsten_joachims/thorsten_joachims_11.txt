learning general functional dependencies is one of the main goals in machine learning .
recent progress in kernel - based methods has focused on designing ( cid : 123 ) exible and powerful in - put representations .
this paper addresses the complementary issue of problems involv - ing complex outputs such as multiple depen - dent output variables and structured output spaces .
we propose to generalize multiclass support vector machine learning in a formu - lation that involves features extracted jointly from inputs and outputs .
the resulting op - timization problem is solved eciently by a cutting plane algorithm that exploits the sparseness and structural decomposition of the problem .
we demonstrate the versatility and eectiveness of our method on problems ranging from supervised grammar learning and named - entity recognition , to taxonomic text classication and sequence alignment .
this paper deals with the general problem of learn - ing a mapping from inputs x 123 x to discrete outputs y 123 y based on a training sample of input - output pairs ( x123; y123 ) ; : : : ; ( xn; yn ) 123 x y drawn from some xed but unknown probability distribution .
unlike the case of multiclass classication where y = f123; : : : ; kg with interchangeable , arbitrarily numbered labels , we con - sider structured output spaces y .
elements y 123 y may be , for instance , sequences , strings , labeled trees ,
appearing in proceedings of the 123 st international confer - ence on machine learning , ban , canada , 123
copyright by the authors .
lattices , or graphs .
such problems arise in a variety of applications , ranging from multilabel classication and classication with class taxonomies , to label sequence learning , sequence alignment learning , and supervised grammar learning , to name just a few .
we approach these problems by generalizing large margin methods , more specically multi - class support vector machines ( svms ) ( weston & watkins , 123; crammer & singer , 123 ) , to the broader problem of learning structured responses .
the naive approach of treating each structure as a separate class is often in - tractable , since it leads to a multiclass problem with a very large number of classes .
we overcome this prob - lem by specifying discriminant functions that exploit the structure and dependencies within y .
in that re - spect , our approach follows the work of collins ( 123; 123 ) on perceptron learning with a similar class of discriminant functions .
however , the maximum mar - gin algorithm we propose has advantages in terms of accuracy and tunability to specic loss functions .
a similar philosophy of using kernel methods for learning general dependencies was pursued in kernel depen - dency estimation ( kde ) ( weston et al . , 123 ) .
yet , the use of separate kernels for inputs and outputs and the use of kernel pca with standard regression tech - niques signicantly diers from our formulation , which is a more straightforward and natural generalization of
discriminants and loss functions
we are interested in the general problem of learning functions f : x ! y based on a training sample of input - output pairs .
as an illustrating example , con - sider the case of natural language parsing , where the function f maps a given sentence x to a parse tree
should be treated dierently from a parse tree that is radically dierent .
typically , the correctness of a predicted parse tree is measured by its f123 score ( see e . g .
johnson ( 123 ) ) , the harmonic mean of precision of recall as calculated based on the overlap of nodes between the trees .
we thus assume the availability of a bounded loss function 123 : y y ! < where 123 ( y; ^y ) quanties the loss associated with a prediction ^y , if the true output value is y .
if p ( x; y ) denotes the data gen - erating distribution , then the goal is to nd a function f within a given hypothesis class such that the risk
p ( f ) =zx y 123 ( y; f ( x ) ) dp ( x; y ) :
is minimized .
we assume that p is unknown , but that a nite training set of pairs s = f ( xi; yi ) 123 x y : i = 123; : : : ; ng generated i . i . d .
according to p is given .
the performance of a function f on the training sample s is described by the empirical risk r123 s ( f ) .
for w - parameterized hypothesis classes , we will also write p ( w ) r123 p ( f ( ; w ) ) and similarly for the empirical
margins and margin maximization
first , we consider the separable case in which there exists a function f parameterized by w such that the empirical risk is zero .
if we assume that 123 ( y; y123 ) > 123 for y 123= y123 and 123 ( y; y ) = 123 , then the condition of zero training error can then be compactly written as a set of non - linear constraints
y123ynyifhw; ( xi; y ) ig < hw; ( xi; yi ) i :
each nonlinear inequality in ( 123 ) can be equivalently replaced by jyj 123 linear inequalities , resulting in a total of njyj n linear constraints ,
123i; 123y 123 y n yi : hw; i ( y ) i > 123 ;
where we have dened the shorthand i ( y ) ( xi; yi ) ( xi; y ) .
if the set of inequalities in ( 123 ) is feasible , there will typically be more than one solution w .
to specify a unique solution , we propose to select the w with kwk 123 for which the score of the correct label yi is uniformly most dierent from the closest runner - up ^yi ( w ) = argmaxy123=yihw; ( xi; y ) i .
this general - izes the maximum - margin principle employed in svms ( vapnik , 123 ) to the more general case considered in this paper .
the resulting hard - margin optimization
figure 123
illustration of natural language parsing model .
this is depicted graphically in figure 123
the ap - proach we pursue is to learn a discriminant function f : x y ! < over input / output pairs from which we can derive a prediction by maximizing f over the response variable for a specic given input x .
hence , the general form of our hypotheses f is
f ( x; w ) = argmax
f ( x; y; w ) ;
where w denotes a parameter vector .
it might be use - ful to think of f as a w - parameterized family of cost functions , which we try to design in such a way that the minimum of f ( x;; w ) is at the desired output y for inputs x of interest .
throughout this paper , we assume f to be linear in some combined feature repre - sentation of inputs and outputs ( x; y ) ,
f ( x; y; w ) = hw; ( x; y ) i :
the specic form of depends on the nature of the problem and special cases will be discussed subse -
using again natural language parsing as an illustrative example , we can chose f such that we get a model that is isomorphic to a probabilistic context free grammar ( pcfg ) .
each node in a parse tree y for a sentence x corresponds to grammar rule gj , which in turn has a score wj .
all valid parse trees y ( i . e .
trees with a designated start symbol s as the root and the words in the sentence x as the leaves ) for a sentence x are scored by the sum of the wj of their nodes .
this score can thus be written as f ( x; y; w ) = hw; ( x; y ) i , where ( x; y ) is a histogram vector counting how often each grammar rule gj occurs in the tree y .
f ( x; w ) can be eciently computed by nding the structure y 123 y that maximizes f ( x; y; w ) via the cky algorithm ( see manning and schuetze ( 123 ) ) .
learning over structured output spaces y inevitably involves loss functions other than the standard zero - one classication loss ( cf .
weston et al .
( 123 ) ) .
for in natural language parsing , a parse tree that diers from the correct parse in a few nodes only
svm123 : min
123i; 123y 123 y n yi : hw; i ( y ) i 123 :
to allow errors in the training set , we introduce slack variables and propose to optimize a soft - margin crite - rion .
while there are several ways of doing this , we follow crammer and singer ( 123 ) and introduce one slack variable for every non - linear constraint ( 123 ) , which will result in an upper bound on the empirical risk and oers some additional algorithmic advantages .
adding a penalty term that is linear in the slack variables to the objective results in the quadratic program
i; s . t .
123i; i 123 ( 123a ) svm123 : min 123i; 123y 123 y n yi : hw; i ( y ) i 123 i : ( 123b ) alternatively , we can also penalize margin violations by a quadratic term c leading to an analogue optimization problem which we refer to as svm123 .
in both cases , c > 123 is a constant that controls the trade - o between training error minimization and margin
svm123 implicitly considers the zero - one classication loss .
as argued above , this is inappropriate for prob - lems like natural language parsing , where jyj is large .
we now propose two approaches that generalize the above formulations to the case of arbitrary loss func - tions 123
our rst approach is to re - scale the slack vari - ables according to the loss incurred in each of the linear constraints .
intuitively , violating a margin constraint involving a y 123= yi with high loss 123 ( yi; y ) should be penalized more severely than a violation involving an output value with smaller loss .
this can be accom - plished by multiplying the violation by the loss , or equivalently , by scaling slack variables with the inverse loss , which yields the problem
i; s . t .
123i; i 123
123i;123y123ynyi : hw; i ( y ) i 123
a justication for this formulation is given by the sub - sequent proposition ( proof omitted ) .
proposition 123
denote by ( w; ) the optimal solu - tion to svm123s .
then 123 is an upper bound on the empirical risk r123 the optimization problem svm123s
can be derived
analogously , where 123 ( yi; y ) is replaced by p123 ( yi; y )
in order to obtain an upper bound on the empirical
a second way to include loss functions is to re - scale the margin as proposed by taskar et al .
the special case of the hamming loss .
the margin constraints in this setting take the following form :
123i; 123y 123 y n yi : hw; i ( y ) i 123 ( yi; y ) i
this set of constraints yield an optimization prob - 123 which also results in an upper bound on s ( w ) .
in our opinion , a potential disadvantage of the margin scaling approach is that it may give signif - icant weight to output values y 123 y that are not even close to being confusable with the target values yi , be - cause every increase in the loss increases the required
support vector machine learning
the key challenge in solving the qps for the gener - alized svm learning is the large number of margin constraints; more specically the total number of con - straints is njyj .
in many cases , jyj may be extremely large , in particular , if y is a product space of some sort ( e . g .
in grammar learning , label sequence learn - ing , etc . ) .
this makes standard quadratic program - ming solvers unsuitable for this type of problem .
in the following , we propose an algorithm that exploits the special structure of the maximum - margin problem , so that only a much smaller subset of constraints needs to be explicitly examined .
the algorithm is a general - ization of the svm algorithm for label sequence learn - ing ( hofmann et al . , 123; altun et al . , 123 ) and the algorithm for inverse sequence alignment ( joachims , 123 ) .
we will show how to compute arbitrarily close approximations to all of the above svm optimization problems in polynomial time for a large range of struc - tures and loss functions .
since the algorithm operates on the dual program , we will rst derive the wolfe dual for the various soft margin formulations .
dual programs
we will denote by iy the lagrange multiplier enforc - ing the margin constraint for label y 123= yi and exam - ple ( xi; yi ) .
using standard lagrangian duality tech - niques , one arrives at the following dual qp for the hard margin case svm123
123i;123y 123= y n yi :
iyj y hi ( y ) ; j ( y ) i ( 123a )
iy 123 :
a kernel k ( ( x; y ) ; ( x123; y123 ) ) can be used to replace the inner products , since inner products in can be easily expressed as inner products of the original -
for soft - margin optimization with slack re - scaling and linear penalties ( svm123s ) , additional box constraints
123 ( yi; y ) c; 123i
are added to the dual .
quadratic slack penal - ties ( svm123 ) lead to the same dual as svm123 after altering the inner product to hi ( y ) ; j ( y ) i +
ij = 123 , if i = j , else 123
finally , in the case of margin re - scaling , the loss func - tion aects the linear part of the objective function
part q is unchanged from ( 123a ) ) and introduces stan -
maxpi;y iy123 ( yi; y ) q ( ) ( where the quadratic dard box constraints npy123=yi
123 and svm123s
algorithm 123 algorithm for solving svm123 and the loss re - scaling formulations svm123s 123 : input : ( x123; y123 ) ; : : : ; ( xn; yn ) , c , 123 : si ; for all i = 123; : : : ; n
for i = 123; : : : ; n do
set up cost function
: h ( y ) ( 123 hi ( y ) ; wi ) 123 ( yi; y ) : h ( y ) ( 123hi ( y ) ; wi ) p123 ( yi; y ) : h ( y ) 123 ( yi; y ) hi ( y ) ; wi : h ( y ) p123 ( yi; y ) hi ( y ) ; wi
where w pjpy compute ^y = arg maxy123y h ( y ) compute i = maxf123; maxy123si h ( y ) g if h ( ^y ) > i + then
123 : until no si has changed during iteration
si si ( f^yg s optimize dual over s , s = ( isi .
the algorithm we propose aims at nding a small set of active constraints that ensures a suciently accu - rate solution .
more precisely , it creates a nested se - quence of successively tighter relaxations of the origi - nal problem using a cutting plane method .
the latter is implemented as a variable selection approach in the dual formulation .
we will show that this is a valid strategy , since there always exists a polynomially - sized subset of constraints so that the corresponding solu - tion fullls all constraints with a precision of at least .
this means , the remaining ( potentially exponentially many ( constraints are guaranteed to be violated by no more than , without the need for explicitly adding them to the optimization problem .
we will base the optimization on the dual program formulation which has two important advantages over the primal qp .
first , it only depends on inner prod - ucts in the joint feature space dened by , hence allowing the use of kernel functions .
second , the con - straint matrix of the dual program ( for the l123 - svms ) supports a natural problem decomposition , since it is block diagonal , where each block corresponds to a spe - cic training instance .
pseudocode of the algorithm is depicted in algo - rithm 123
the algorithm applies to all svm formula - tions discussed above .
the only dierence is in the way the cost function gets set up in step 123
the algorithm maintains a working set si for each training example ( xi; yi ) to keep track of the selected constraints which dene the current relaxation .
iterating through the training examples ( xi; yi ) , the algorithm proceeds by
nding the ( potentially ) \most violated " constraint , involving some output value ^y ( line 123 ) .
if the ( ap - propriately scaled ) margin violation of this constraint exceeds the current value of i by more than ( line 123 ) , the dual variable corresponding to ^y is added to the working set ( line 123 ) .
this variable selection process in the dual program corresponds to a successive strength - ening of the primal problem by a cutting plane that cuts o the current primal solution from the feasible set .
the chosen cutting plane corresponds to the con - straint that determines the lowest feasible value for i .
once a constraint has been added , the solution is re - computed wrt .
s ( line 123 ) .
alternatively , we have also devised a scheme where the optimization is restricted to si only , and where optimization over the full s is performed much less frequently .
this can be benecial due to the block diagonal structure of the optimization problems , which implies that variables jy with j 123= i , y 123 sj can simply be \frozen " at their current val - ues .
notice that all variables not included in their respective working set are implicitly treated as 123
the algorithm stops , if no constraint is violated by more than .
the presented algorithm is implemented and available123 as part of svmlight .
note that the svm optimization problems from iteration to iteration dif - fer only by a single constraint .
we therefore restart the svm optimizer from the current solution , which greatly reduces the runtime .
a convenient property of both algorithms is that they have a very general and well - dened interface independent of the choice of
to apply the algorithm , it is sucient to im - plement the feature mapping ( x; y ) ( either explicit or via a joint kernel function ) , the loss function 123 ( yi; y ) , as well as the maximization in step 123
all of those , in particular the constraint / cut selection method , are treated as black boxes .
while the modeling of ( x; y ) and 123 ( yi; y ) is more or less straightforward , solving the maximization problem for constraint selection typ - ically requires exploiting the structure of .
it is straightforward to show that the algorithm nds a solution that is close to optimal ( e . g .
for the svm123s adding to each i is a feasible point of the primal at most c from the maximum ) .
however , it is not im - mediately obvious how fast the algorithm converges .
we will show in the following that the algorithm con - verges in polynomial time for a large class of problems , despite a possibly exponential or innite jyj .
let us begin with an elementary lemma that will be helpful for proving subsequent results .
how the dual objective changes , if one optimizes over a single variable .
lemma 123
let j be a positive denite matrix and let us dene a concave quadratic program
w ( ) =
123j + hh; i s . t
and assume 123 is given with r = 123
then max - imizing w with respect to r while keeping all other components xed will increase the objective by
( hr ps sjrs ) 123
provided that hr ps sjrs .
denote by ( r ) the solution with the r - th coecient changed to , then
w ( ( r ) ) w ( ) = hr xs hr ps sjrs
the dierence is maximized for
notice that 123 , since hr ps sjrs and jrr >
using this lemma , we can lower bound the improve - ment of the dual objective in step 123 of algorithm 123
for brevity , let us focus on the case of svm123s lar results can be derived also for the other variants .
proposition 123
dene 123i = maxy 123 ( yi; y ) and ri = maxy ki ( y ) k .
then step 123 in algorithm 123 , improves the dual objective for svm123s 123 at least by
i + n=c ) 123
using the notation in algorithm 123 one can apply lemma 123 with r = ( i; ^y ) denoting the newly added constraint , hr = 123 , jrr = ing the fact that py123=yi
c123 ( yi;^y ) and ps sjrs = hw; i ( ^y ) i +
shows the following increase of the objective function when optimizing over r alone :
note that r = 123
= i , lemma 123
( cid : 123 ) 123 hw; i ( ^y ) i py123=yi 123ki ( ^y ) k123 ( yi; ^y ) + n
the step follows from the fact that i 123 and p123 ( yi; ^y ) ( 123hw; i ( ^y ) i ) > i + , which is the con - dition of step 123
replacing the quantities in the de - nominator by their upper limit proves the claim , since jointly optimizing over more variables than just r can only further increase the dual objective .
this leads to the following polynomial bound on the maximum size of s .
theorem 123
with r = maxi ri , 123 = maxi 123i and for a given > 123 , algorithm 123 for the terminates after incrementally adding at most 123 ( c 123 r123 + n 123 ) constraints to the working set s .
with s = ; the optimal value of the dual is 123
in each iteration a constraint ( i; y ) is added that is violated by at least , provided such a constraint exists .
after solving the s - relaxed qp in step 123 , the 123 123 ( 123 r123 + n=c ) 123 objective will increase by at least 123 according to proposition 123
hence after t constraints , the dual objective will be at least t times this amount .
the result follows from the fact that the dual objective is upper bounded by the minimum of the primal , which in turn can be bounded by 123
123 c 123
note that the number of constraints in s does not de - pend on jyj .
this is crucial , since jyj is exponential or innite for many interesting problems .
for problems where step 123 can be computed in polynomial time , the overall algorithm has a runtime polynomial in n; r; 123 , 123= , since at least one constraint will be added while
cycling through all n instances and since step 123 is
applications and experiments
to demonstrate the eectiveness and versatility of our approach , we report results on a number of dierent tasks to adapt the algorithm to a new problem , it is sucient to implement the feature mapping ( x; y ) , the loss function 123 ( yi; y ) , as well as the maximization in step 123
multiclass classication
123; : : : ; v123
our algorithm can implement the conventional winner - takes - all ( wta ) multiclass classication ( crammer & singer , 123 ) as follows .
let y = fy123; : : : ; ykg and w = ( v123 k ) 123 is a stack of vectors , vk being a weight vector associated with the k - th class yk .
fol - lowing crammer and singer ( 123 ) one can then dene f ( x; yk; w ) = hvk; ' ( x ) i , where ' ( x ) 123 <d denotes an arbitrary input representation .
these discriminant functions can be equivalently represented in the pro - posed framework by dening a joint feature map as follows ( x; y ) ' ( x ) c ( y ) .
here c refers to the orthogonal ( binary ) encoding of the label y and is the tensor product which forms all products between coecients of the two argument vectors .
classication with taxonomies
the rst generalization we propose is to make use of more interesting output features than the orthogonal representation c .
as an exemplary application of this kind , we show how to take advantage of known class taxonomies .
here a taxonomy is treated as a lattice in which the classes y 123 y are the minimal elements .
for every node z in the lattice ( corresponding to a super - class or class ) we introduce a binary attribute z ( y ) indicating whether or not z is a predecessor of y .
notice that h ( y ) ; ( y123 ) i will count the number of
we have performed experiments using a document collection released by the world intellectual prop - erty organization ( wipo ) , which uses the interna - tional patent classication ( ipc ) scheme .
we have restricted ourselves to one of the 123 sections , namely section d , consisting of 123 , 123 documents in the wipo - alpha collection .
for our experiments , we have indexed the title and claim tags .
we have furthermore sub - sampled the training data to investigate the eect of the training set size .
document parsing , tokenization and term normalization have been performed with the
table 123
results on the wipo - alpha corpus , section d with 123 groups using 123 - fold and 123 - fold cross validation , re - spectively .
( cid : 123 ) t is a standard ( ( cid : 123 ) at ) svm multiclass model , tax the hierarchical architecture .
123 / 123 denotes training based on the classication loss , 123 refers to training based on the tree loss .
( cid : 123 ) t 123 tax 123
123 training instances per class 123 training instances per class
123 +123 %
123 +123 %
mindserver retrieval engine . 123 as a suitable loss func - tion 123 , we have used a tree loss function which denes the loss between two classes y and y123 as the height of the rst common ancestor of y and y123 in the taxon - omy .
the results are summarized in table 123 and show that the proposed hierarchical svm learning architec - ture improves performance over the standard multi - class svm in terms of classication accuracy as well as in terms of the tree loss .
label sequence learning
label sequence learning deals with the problem of pre - dicting a sequence of labels y = ( y123; : : : ; ym ) , yk 123 , from a given sequence of inputs x = ( x123; : : : ; xm ) .
it subsumes problems like segmenting or annotat - ing observation sequences and has widespread appli - cations in optical character recognition , natural lan - guage processing , information extraction , and compu - tational biology .
in the following , we study our algo - rithm on a named entity recognition ( ner ) problem .
more specically , we consider a sub - corpus consisting of 123 sentences from the spanish news wire article corpus which was provided for the special session of conll123 devoted to ner .
the label set in this corpus consists of non - name and the beginning and continuation of person names , organizations , locations and miscellaneous names , resulting in a total of jj = 123 dierent labels .
in the setup followed in altun et al .
( 123 ) , the joint feature map ( x; y ) is the histogram of state transition plus a set of features describing the emissions .
an adapted version of the viterbi algorithm is used to solve the argmax in line 123
for both per - ceptron and svm a second degree polynomial kernel
the results given in table 123 for the zero - one loss , compare the generative hmm with conditional ran - dom fields ( crf ) ( laerty et al . , 123 ) , collins per -
table 123
results of various algorithms on the named en - tity recognition task ( altun et al . , 123 ) .
table 123
error rates and number of constraints jsj depend - ing on the number of training examples ( = 123 : 123 , c = 123 : 123 ) .
method hmm crf perceptron svm
table 123
results for various svm formulations on the named entity recognition task ( = 123 : 123 , c = 123 ) .
method train err test err
123 123 . 123 123 . 123 123 . 123 123 . 123 123 123 . 123 123 . 123 123 . 123 123 . 123 123 . 123 123 123 . 123 123 . 123 123 . 123 123 . 123 123 . 123 123 . 123 123 . 123 123 . 123
ceptron and the svm algorithm .
all discriminative learning methods substantially outperform the stan - dard hmm .
in addition , the svm performs slightly better than the perceptron and crfs , demonstrating the benet of a large - margin approach .
table 123 shows that all svm formulations perform comparably , prob - ably due to the fact the vast majority of the support label sequences end up having hamming distance 123 to the correct label sequence ( notice that for loss equal to 123 all svm formulations are equivalent ) .
sequence alignment
next we show how to apply the proposed algorithm to the problem of learning how to align sequences x 123 x = .
for a given pair of sequences x and z , alignment methods like the smith - waterman algo - rithm select the sequence of operations ( e . g .
insertion , substitution ) ^a ( x; z ) = argmaxa123a hw; ( x; z; a ) i that transforms x into y and that maximizes a linear ob - jective function derived from the ( negative ) operation costs w .
( x; z; a ) is the histogram of alignment op - erations .
we use the value of hw; ( x; z; ^a ( x; z ) ) i as a measure of similarity .
in order to learn the cost vector w we use training data of the following type .
for each native sequence xi there is a most similar homologue sequence zi along with what is believed to be the ( close to ) optimal alignment ai .
in addition we are given a set of decoy sequences i , t = 123; : : : ; k with unknown alignments .
the goal is to nd a cost vector w so that homologue sequences are close to the native sequence , and so that decoy sequences are further away .
with yi = fzi; z123 i ; : : : ; zk as the output space for the i - th example , we seek a w so that hw; ( xi; zi; ai ) i exceeds hw; ( xi; zt i; a ) i for all t and a .
this implies a zero - one loss and hypotheses of the form f ( xi; w ) = argmaxy123yi maxa hw; ( x; z; a ) i .
we use the smith - waterman algorithm to implement
table 123 shows the test error rates ( i . e .
fraction of times the homolog is not selected ) on the synthetic dataset
described in joachims ( 123 ) .
the results are aver - aged over 123 train / test samples .
the model contains 123 parameters in the substitution matrix and a cost for \insert / delete " .
we train this model using the svm123 and compare against a generative sequence alignment model , where the substitution matrix is
computed as ij = log p ( xi;zj ) p ( xi ) p ( zj ) using laplace esti - mates .
for the generative model , we report the results for = 123 : 123 , which performs best on the test set .
de - spite this unfair advantage , the svm performs better for low training set sizes .
for larger training sets , both methods perform similarly , with a small preference for the generative model .
however , an advantage of the svm model is that it is straightforward to train gap penalties .
as predicted by theorem 123 , the number of constraints jsj is low .
it appears to grows sub - linearly with the number of examples .
natural language parsing
we test the feasibility of our approach for learning a weighted context - free grammar ( see figure 123 ) on a subset of the penn treebank wall street journal cor - pus .
we consider the 123 sentences of length at most 123 from sections f123 - 123 as the training set , and the 123 sentences of length at most 123 from f123 as the test set .
following the setup in johnson ( 123 ) , we start based on the part - of - speech tags and learn a weighted gram - mar consisting of all rules that occur in the training data .
to solve the argmax in line 123 of the algorithm , we use a modied version of the cky parser of mark johnson123 and incorporated it into svmlight .
the results are given in table 123
they show accu - racy and micro - averaged f123 for the training and the test set .
the rst line shows the performance for gen - erative pcfg model using the maximum likelihood estimate ( mle ) as computed by johnsons implemen - tation .
the second line show the svm123 with zero - one loss , while the following lines give the results for the f123 - loss 123 ( yi; y ) = ( 123 f123 ( yi; y ) ) using svm123s
table 123
results for learning a weighted context - free gram - mar on the penn treebank .
cpu time measured in hours .
f123 const cpu ( %qp )
123 123 123 123 n / a 123 123 123 123 123 123 123 123 123 123 123 123
all results are for c = 123 and = 123 : 123
all values of c between 123 to 123 gave comparable re - sults .
while the zero - one loss achieves better accuracy ( i . e .
predicting the complete tree correctly ) , the f123 - score is only marginally better .
using the f123 - loss gives substantially better f123 - scores , outperforming the mle substantially .
the dierence is signicant according to a mcnemar test on the f123 - scores .
we conjecture that we can achieve further gains by incorporating more complex features into the grammar , which would be impossible or at best awkward to use in a generative pcfg model .
note that our approach can handle ar - bitrary models ( e . g .
with kernels and overlapping fea - tures ) for which the argmax in line 123 can be computed .
in terms of training time , table 123 shows that the to - tal number of constraints added to the working set is small .
it is roughly twice the number of training ex - amples in all cases .
while the training is faster for the zero - one loss , the time for solving the qps remains roughly comparable .
the re - scaling formulations lose time mostly on the argmax in line 123
this might be sped up , since we were using a rather naive algorithm in the experiments .
we formulated a support vector method for super - vised learning with structured and interdependent out - it is based on a joint feature map over in - put / output pairs , which covers a large class of interest - ing models including weighted context - free grammars , hidden markov models , and sequence alignment .
fur - thermore , the approach is very ( cid : 123 ) exible in its ability to handle application specic loss functions .
to solve the resulting optimization problems , we proposed a simple and general algorithm for which we prove convergence bounds .
our empirical results verify that the algo - rithm is indeed tractable .
furthermore , we show that the generalization accuracy of our method is at least comparable or often exceeds conventional approaches for a wide range of problems .
a promising property of our method is that it can be used to train com - plex models , which would be dicult to handle in a
the authors would like to thank lijuan cai for con - ducting the experiments on classication with tax - onomies .
this work was supported by the kanellakis dissertation fellowship , nsf - itr grant iis - 123 , and nsf career award 123
