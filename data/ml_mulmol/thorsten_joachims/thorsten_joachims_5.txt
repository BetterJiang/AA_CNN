those trying to make sense of the notion of textual content and semantics within the wild , wild world of information retrieval , categorization , and ltering have to deal often with an overwhelming sea of problems .
the really strange story is that most of them ( myself included ) still believe that developing a linguistically principled approach to text categorization is an interesting research problem .
this will also emerge in the discussion of the book that is the focus of this review .
learning to classify texts using support vector machines by thorsten joachims proposes a theory for automatic learning of text categorization models that has been repeatedly shown to be very successful .
at the same time , the approach proposed is based on a rather rough linguistic generalization of ( what apparently is ) a language - dependent task : topic text classication ( tc ) .
the result is twofold : on the one hand , a learning theory , based on statistical learnability principles and results , that avoids the limita - tions of the strong empiricism typical of most text classication research; and on the other hand , the application of a naive linguistic model , the bag - of - words representa - tion , to linguistic objects ( i . e . , the documents ) that still achieves impressive accuracy .
several good reasons for reading the book
when you can measure what you are speaking about , and express it in numbers , you know something about it; but when you cannot measure it , when you cannot express it in numbers , your knowledge is of a meager and unsatisfactory kind : it may be the beginning of knowledge , but you have scarcely in your thoughts advanced to the stage of science .
123 statistical learning theory applied to text classication joachimss book presents the application of a statistical learning model , support vector machines ( svms ) ( vapnik 123 ) , to the problem of text categorization .
as the author emphasizes from the beginning of the book , it affects not only theoretical aspects , and not only empirical ndings or implementation ideas , but all these aspects .
the book
volume 123 , number 123
is the results of the authors ph . d .
thesis , and this is strongly reected in its overall
the original contribution of the ph . d .
thesis is the specic application of svms and some theoretical and algorithmic novelty developed for ( but not limited to ) text categorization .
the author divides the book into four parts : notation , theory , methods , and algorithms .
notation introduces the problem and surveys existing approaches to automatic text categorization .
first , in chapter 123 , feature representation methods are discussed , from bag of words to multiwords or semantically justied features .
then , four tra - ditional methods for learning the categorization functionnaive bayes ( tzeras and hartmann 123 ) , rocchio ( rocchio 123 ) , k - nearest neighbors ( yang 123 ) , and deci - sion trees ( quinlan 123 ) are presented in the same chapter .
in this chapter ( that is , very early in the book ) a specic notion of performance is dened .
it is expressed as a function of the categorization error ( i . e . , the number of mismatches between the system outcome and the gold standard ) .
notice how this is a bias to the entire matter , as discussed below .
finally , the basic denitions for svms are given ( chapter 123 ) .
theory proposes a general model for tc that is based on the distributional properties of ( bags of ) words .
this results in an abstract notion of target categoriza - tion ( tcat ) concept the learnability of which via svms derives from their formal properties ( chapter 123 ) .
moreover , the tcat concept has an inductive nature , as it is based on the availability of a large set of training examples : under certain assump - tions on the training material , the tcats results are linearly separable by an svm with a controlled amount of error ( or loss in predictive accuracy ) .
accordingly , chapter 123 analyzes methods for estimating the predictive accuracy of the target svm .
the task knowledge embedded in the training examples provides complete information about the training error , and formal results from statistical learnability theory ( e . g . , vapnik 123 ) ( or newly introduced by using vapniks results as inspiration ) are directly em - ployed here as an upper bound on the testing error ( i . e . , the error of the generalization achieved as measured over test data ) .
methods discusses the core technique for inducing the tc functions by means of svms .
this is rst done in the tradition of inductive approaches to tc : training examples are used to induce the maximum margin hyperplane that separates positive from negative examples in a binary setting .
the empirical evidencegood perfor - mance over three well - known benchmarking data setsconrms the viability of the svm induction ( chapter 123 ) .
in chapter 123 the notion of transductive svms ( vapnik 123 ) is introduced .
it is the inductive task that exploits a consistent set of testing examples as a bias for building the maximum margin hyperplane .
such an approach has several analogies with forms of active learning ( e . g . , co - training ( blum and mitchell 123 ) ) , wherein evidence during learning is derived from pieces of more or less weak test evidence ( e . g . , independent feature spaces are used as selective information on how to sample training examples in co - training ) .
algorithms and concrete methods for the application of the previous results are then reported in the fourth part , algorithms .
in chapters 123 and 123 , efcient algorithms for training inductive and transductive svms are nally presented , with reference to the software platform svmlight , another ( nonsecondary ) side effect of the authors
123 empirically grounding a powerful theory the book has great merit , as much space is given not only to theoretical and experi - mental aspects , but to an attempt to empirically assess support vector machines as a general learning theory for tc .
grounding formal results on large - scale data is always
attempted where possible .
it is carried out over the target benchmarking corpora : a collection of reuters news ( reuters - 123 ) , a collection of medical texts ( ohsumed ) , and a set of manually classied web pages ( the webkb collection ) . 123 this evidence is also used to dene an abstract model of what a target text categorization concept is and how it is learnable by a svm .
the notion of tcat concept tries to capture exactly this .
these two aspects are very important , as the reader can better understand the large set of ( often mathematically complex ) theoretical notions against empirical data and also validate progressively the results of the theory against the evidence derived from real collections .
for example , interesting sections ( especially for a computational linguistics researcher ) are those in which mathematical notions , such as linear separa - bility , training error , and the tcat concept itself , are discussed via estimation against the benchmarking data ( e . g . , section 123 of chapter 123 ) .
although test data cannot be considered exhaustive samples , tc benchmarks are a rather precious source of infor - mation about the distributional and linguistic properties of words .
notice that such combined theoretical and empirical analyses are rare in the literature .
the result is an attempt to reconcile ir ( often too much focused on empirical performance mea - sures ) and ai ( more often targeted to theories of learning with weaker possibilities for large - scale empirical assessment ) .
finally , as large - scale data analysis is required in the study of several nlp tasks ( and not only tc ) , the book is a good example for researchers and practitioners in empirical language processing .
123 theory , application , and implementation of support vector machines covering aspects related not just to methods , but also to theory and efcient imple - mentation of svms is a positive aspect of the book .
the book helps in building a rather rich picture of the eld .
introductory matter is presented in a comprehensive and theoretically well - founded way .
then the learning theory based on earlier results from vapnik ( 123 ) is described , and this has a valuable effect on the methodological contribution .
all the aspects of the application of svms to tc are thus framed in a
examples are the discussion of benchmarking data in section 123 of chapter 123 , in which theoretical estimators used to upper - bound the categorization error are com - pared to their measures as carried out over the benchmarking collections .
almost every specic parameter of the estimators that raises questions about the quality and applicability of the theory ( e . g . , dimension of the training set , the error embedded in training material ) is analyzed comparatively against the benchmarking data .
the analytical estimates are thus compared to the effective measures .
this is very relevant for anyone interested in empirical analysis of linguistic data .
123 expressiveness versus efciency one of the contributions of joachimss thesis is the effort spent in making svm learning for text classication feasible .
in svms , the induction is based on the solution of a quadratic optimization problem in which the size of the matrix is quadratic in the number of available training examples and the different values depend on the choice of the kernel functions . 123 the problems here are related to the iterative evaluation of
123 see details at these sites , respectively : http : / / www . research . att . com / lewise / reuters123html , 123 kernel functions are used to map the problem from its source feature space to a new feature space where linear separability is ensured .
typical kernel functions k are polynomial , e . g . , k ( ( cid : 123 ) x123 , ( cid : 123 ) x123 ) = ( ( cid : 123 ) x123 ( cid : 123 ) x123 + 123 ) d .
volume 123 , number 123
the matrix , which cannot be completely computed efciently or stored in memory ( for high values of n ) .
the author capitalizes on the idea of working on decomposed matrixes of smaller size , previously suggested by osuna , freund , and girosi ( 123 ) .
first , during an iteration of the algorithm , a subset of the variables is xed so that the current re - evaluation is carried out on the remaining subset of the variables , called the working set .
the different values in the working set determine a specic bias for the algorithm at each step .
the selection of a good working set limits the complexity of each step and increases the convergence speed .
then , a second optimization criterion is discussed that is related to the use of support vectors ( svs ) .
svs are the examples closer to the separating hyperplane , such that they are at distance d ( the margin ) from it .
as the hyperplane coefcients depend only on svs , an algorithm able to work from the beginning on just the svs would be the most efcient .
the selection of svs is clearly heuristic , as their position within the training data set is unknown at the beginning .
the author suggests a selection algorithm that predicts ( and then neglects ) non - svs among the training data according to their behavior in the previous h steps .
data left out of the analysis are checked a posteriori .
the optimality conditions are checked after a solution for the subproblem has been built .
in case an invalid solution has been found , optimization is rerun , although it is reinitialized through the last partial solution found .
although more details would have been benecial , 123 empirical analysis is then discussed at length .
the impacts of the different factors on the learning times are reported as they have been measured against benchmarking data collections ( section 123 of chapter 123 ) .
stay far away ?
not everything that counts can be counted , and not everything that can be counted counts
123 where is the language ? after a few words spent discussing possible text representation formalisms that have been adopted in the tc literature , the author completely neglects choices other than bags of words .
this representational issue is secondary throughout the book .
the au - thor instead concentrates just on learnability and categorization accuracy achieved with bags of words .
according to the unquestionable fact that a text is a linguistic object , a theory related to its classication should at least include an analysis along a linguistic dimensionthat is , the study of representations different in terms of lin - guistic properties .
unfortunately , in the book no empirical analysis of different feature representations is even attempted .
as a result , whenever empirical ndings do not t well with theoretical estimates , weak explanations are given .
for example , in table 123 ( chapter 123 , section 123 ) , linear separability ( i . e . , separability of hyperplane with a linear kernel and with no contri - bution from slack variables compensating for the noise in training data ) is discussed over the reuters news and the ohsumed abstracts .
the fact that in some cases linear separability is not achieved is quickly explained in terms of dubious ( invalid ? ) doc - uments or inconsistent classications reecting human errors .
note that the reuters collection is full of linguistically dubious documents ( e . g . , tables with long lists of numbers and almost no text at all; see , for example , the earn category ) .
moreover ,
123 the accuracy of the empirical data made available by the author in this section is lower than in other
parts of the book .
these phenomena seem to favor coarse approaches like rocchios or svms : measures obtained without any feature selection ( thus including most nonlinguistic tokens ) sug - gest nontrivial increases in performances ( see , for example , experiments discussed by basili and moschitti ( 123 ) ) .
evidently , the reason for mismatches between empirical evidence and the theory lies in the fact that something is missing from the latter , which in fact is based on a rather rough approximation .
separability seems harder when the space based on bags of words is not expressive enough .
this strictly depends on the target class ( i . e . , the underlying knowledge domain ) and on the quality of the available training material .
bags of words provide strong evidence in a large set of tc tasks but do not suitably express the required information in all cases .
the variability of categorization performances of svms across different corpora ( e . g . , about 123 break - even point on reuters but 123 break - even point for ohsumed ) is further evidence that although optimization implied by svms is very powerful ( i . e . , is able to build a very good prediction function whatever the underlying feature space is ) , there is still something missing .
some other dimensions exist where further improvement should be looked for .
language processing could be employed in all cases in which the gap between the best bag - of - words performance and the expected optimum is not trivial .
123 what is a tcat ? the concept of tcat , discussed in chapter 123 , seems to be a complex artifact with no clear relationship to the target task .
a tcat is fully dened in terms of the distributional properties of ( a set of ) words that represent it .
note that these properties are exactly the ones requested by the statistical learnability criteria for svms .
a tcat is dened as a set of word classes determined by their common average frequency gures in the training data .
such a representation seems determined by the research goal ( i . e . , statistical learnability ) , and no other strong evidence is brought forward except for frequency gures in the benchmarking data .
this is in fact an empirical validation of the learnability theory for which bags of words and their distributions in texts are sufcient to induce those svms that minimize categorization errors .
this only implies , however , that successful learning via svms is a validation of the tcat notion .
the reverse is not true , as this latter does not represent any theoretical explanation of what learning for text categorization is .
123 which performances ? most of the research in information retrieval is targeted to the maximization of utility functions that are operational ( e . g . , accuracy in categorization or relevance of docu - ments to queries ) .
and this is also the perspective adopted in performance evaluation ( losee 123 ) and in building benchmarks .
the questions that make sense are still : is the minimization of categorization error the entire target of text categorization ? is it the only research focus ? is separating hyperplanes the only interesting aspect of the problem , or should more powerful explanations be learned from training data ?
this book suggests that classication functions can be successfully built from ex - tensive training data ( i . e . , manually classied documents ) .
the outcome of the learning framework , however , is just a boolean function working as a black box .
unfortunately , the author never tries to give an interpretation of the kind of information induced .
are the weights induced by the svm meaningful in some sense ? are they telling something even outside of the black box ? if we are to compare two svms , can we rely just on differences in performance ? or should we perhaps look to other aspects to measure their usefulness ? my feeling is that very little has been done for evaluating how well svms represent the critical aspects of the problems .
for example , are the induced coefcients wi of the hyperplane ( i . e . , dimensions related to the ith word )
volume 123 , number 123
linguistically meaningful elements , directly mappable toward lexical or semantic phe - nomena ? in this perspective , performance measures that are counts or probabilities of classication errors are limited , or even misleading .
research should avoid the tempta - tion of reducing to merely optimization of the precision / recall trade - off .
linguistically justied features have the inherent benet of supporting natural explanations of the system induction , although evaluation usually does not account for them .
in my view , the ( linguistic ) interpretation of different aspects of svm learning in tc is an impor - tant research direction .
the study of the relationship between training materials ( as represented under a linguistic perspective ) and the choice of kernel functions optimal for the task is a promising research line .
late reections
support vector machines are widely adopted today for several tasks ( e . g . , kudo and matsumoto 123 ) .
they seem to reproduce effectively the induction of separation func - tions from training data .
the svm inductive setting ( of which the transductive one is just a derivation ) is a straightforward approach to tc induction , and this book is proof .
although it leaves a large set of open problems , the book must be seen as a relevant contribution in the area of machine learning for natural language .
it is a powerful means of getting acquainted with theoretical and methodological knowledge for text classication .
unfortunately , obvious gaps highly affect its completeness as a handbook in courses on machine learning for text classication and nlp , but this was outside of the authors aims .
in current research and practice , the theory proposed in this book and its empirical grounding are important contributions to the area of empirical natural language pro - cessing .
first , it embodies new ideas about learning that look for applications to new nlp tasks .
classication of nl questions in question answering and pattern acquisi - tion for adaptive information extraction are just two examples in which a transductive svm approach is currently being applied .
second , the book embodies an approach to empirical research that is very elegant .
solid theoretical inspiration is here systemat - ically mirrored with real data , where motivations for the rst are tentatively found in the latter .
the impressive methodological and applicative achievements say much about the effectiveness reachable by elegant ways of doing research .
