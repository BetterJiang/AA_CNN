this paper addresses the problem of planning under uncertainty in large markov decision processes ( mdps ) .
factored mdps represent a complex state space using state variables and the transition model using a dynamic bayesian network .
this representation often allows an exponential reduction in the representation size of structured mdps , but the complexity of exact solution algorithms for such mdps can grow exponentially in the representation size .
in this paper , we present two approximate solution algorithms that exploit structure in factored mdps .
both use an approximate value function represented as a linear combination of basis functions , where each basis function involves only a small subset of the domain variables .
a key contribution of this paper is that it shows how the basic operations of both algorithms can be performed eciently in closed form , by exploiting both additive and context - specic structure in a factored mdp .
a central element of our algorithms is a novel linear program decomposition technique , analogous to variable elimination in bayesian networks , which reduces an exponentially large lp to a provably equivalent , polynomial - sized one .
one algorithm uses approximate linear programming , and the second approximate dynamic programming .
our dynamic programming algorithm is novel in that it uses an approximation based on max - norm , a technique that more directly minimizes the terms that appear in error bounds for approximate mdp algorithms .
we provide experimental results on problems with over 123 states , demonstrating a promising indication of the scalability of our approach , and compare our algorithm to an existing state - of - the - art approach , showing , in some problems , exponential gains in computation time .
over the last few years , markov decision processes ( mdps ) have been used as the basic semantics for optimal planning for decision theoretic agents in stochastic environments .
in the mdp framework , the system is modeled via a set of states which evolve stochastically .
the main problem with this representation is that , in virtually any real - life domain , the state space is quite large .
however , many large mdps have signicant internal structure , and can be modeled compactly if the structure is exploited in the representation .
factored mdps ( boutilier , dearden , & goldszmidt , 123 ) are one approach to repre - senting large , structured mdps compactly .
in this framework , a state is implicitly described by an assignment to some set of state variables .
a dynamic bayesian network ( dbn ) ( dean & kanazawa , 123 ) can then allow a compact representation of the transition model , by exploiting the fact that the transition of a variable often depends only on a small number
c ( cid : 123 ) 123 ai access foundation and morgan kaufmann publishers .
all rights reserved .
guestrin , koller , parr & venkataraman
of other variables .
furthermore , the momentary rewards can often also be decomposed as a sum of rewards related to individual variables or small clusters of variables .
there are two main types of structure that can simultaneously be exploited in factored mdps : additive and context - specic structure .
additive structure captures the fact that typical large - scale systems can often be decomposed into a combination of locally inter - acting components .
for example , consider the management of a large factory with many production cells .
of course , in the long run , if a cell positioned early in the production line generates faulty parts , then the whole factory may be aected .
however , the quality of the parts a cell generates depends directly only on the state of this cell and the quality of the parts it receives from neighboring cells .
such additive structure can also be present in the reward function .
for example , the cost of running the factory depends , among other things , on the sum of the costs of maintaining each local cell .
context - specic structure encodes a dierent type of locality of inuence : although a part of a large system may , in general , be inuenced by the state of every other part of this system , at any given point in time only a small number of parts may inuence it directly .
in our factory example , a cell responsible for anodization may receive parts directly from any other cell in the factory .
however , a work order for a cylindrical part may restrict this dependency only to cells that have a lathe .
thus , in the context of producing cylindrical parts , the quality of the anodized parts depends directly only on the state of cells with a
even when a large mdp can be represented compactly , for example , by using a factored representation , solving it exactly may still be intractable : typical exact mdp solution al - gorithms require the manipulation of a value function , whose representation is linear in the number of states , which is exponential in the number of state variables .
one approach is to approximate the solution using an approximate value function with a compact represen - tation .
a common choice is the use of linear value functions as an approximation value functions that are a linear combination of potentially non - linear basis functions ( bellman , kalaba , & kotkin , 123; sutton , 123; tsitsiklis & van roy , 123b ) .
our work builds on the ideas of koller and parr ( 123 , 123 ) , by using factored ( linear ) value functions , where each basis function is restricted to some small subset of the domain variables .
this paper presents two new algorithms for computing linear value function approxi - mations for factored mdps : one that uses approximate dynamic programming and another that uses approximate linear programming .
both algorithms are based on the use of fac - tored linear value functions , a highly expressive function approximation method .
this representation allows the algorithms to take advantage of both additive and context - specic structure , in order to produce high - quality approximate solutions very eciently .
the ca - pability to exploit both types of structure distinguishes these algorithms dier from earlier approaches ( boutilier et al . , 123 ) , which only exploit context - specic structure .
we provide a more detailed discussion of the dierences in section 123
we show that , for a factored mdp and factored value functions , various critical oper - ations for our planning algorithms can be implemented in closed form without necessarily enumerating the entire state space .
in particular , both our new algorithms build upon a novel linear programming decomposition technique .
this technique reduces structured lps with exponentially many constraints to equivalent , polynomially - sized ones .
this decompo - sition follows a procedure analogous to variable elimination that applies both to additively
efficient solution algorithms for factored mdps
structured value functions ( bertele & brioschi , 123 ) and to value functions that also ex - ploit context - specic structure ( zhang & poole , 123 ) .
using these basic operations , our planning algorithms can be implemented eciently , even though the size of the state space grows exponentially in the number of variables .
our rst method is based on the approximate linear programming algorithm ( schweitzer & seidmann , 123 ) .
this algorithm generates a linear , approximate value function by solving a single linear program .
unfortunately , the number of constraints in the lp proposed by schweitzer and seidmann grows exponentially in the number of variables .
using our lp decomposition technique , we exploit structure in factored mdps to represent exactly the same optimization problem with exponentially fewer constraints .
in terms of approximate dynamic programming , this paper makes a twofold contribution .
first , we provide a new approach for approximately solving mdps using a linear value function .
previous approaches to linear function approximation typically have utilized a least squares ( l123 - norm ) approximation to the value function .
least squares approximations are incompatible with most convergence analyses for mdps , which are based on max - norm .
we provide the rst mdp solution algorithms both value iteration and policy iteration that use a linear max - norm projection to approximate the value function , thereby directly optimizing the quantity that appears in our provided error bounds .
second , we show how to exploit the structure of the problem to apply this technique to factored mdps , by again leveraging on our lp decomposition technique .
although approximate dynamic programming currently possesses stronger theoretical guarantees , our experimental results suggest that approximate linear programming is a good alternative .
whereas the former tends to generate better policies for the same set of basis functions , due to the simplicity and computational advantages of approximate linear programming , we can add more basis functions , obtaining a better policy and still requiring less computation than the approximate dynamic programming approach .
finally , we present experimental results comparing our approach to the work of boutilier et al .
( 123 ) , illustrating some of the tradeos between the two methods .
in particular , for problems with signicant context - specic structure in the value function , their approach can be faster due to their ecient handling of their value function representation .
however , there are cases with signicant context - specic structure in the problem , rather than in the value function , in which their algorithm requires an exponentially large value function representation .
in such classes of problems , we demonstrate that by using a value func - tion that exploits both additive and context - specic structure , our algorithm can obtain a polynomial - time near - optimal approximation of the true value function .
this paper starts with a presentation of factored mdps and approximate solution al - gorithms for mdps .
in section 123 , we describe the basic operations used in our algorithms , including our lp decomposition technique .
in section 123 , we present the rst of our two algorithms : the approximate linear programming algorithm for factored mdps .
the second algorithm , approximate policy iteration with max - norm projection , is presented in section 123
section 123 describes an approach for eciently computing bounds on policy quality based on the bellman error .
section 123 shows how to extend our methods to deal with context - specic structure .
our paper concludes with an empirical evaluation in section 123 and a discussion of related work in section 123
guestrin , koller , parr & venkataraman
this paper is a greatly expanded version of work that was published before in guestrin
( 123a ) , and some of the work presented in guestrin et al .
( 123b , 123 ) .
factored markov decision processes
a markov decision process ( mdp ) is a mathematical framework for sequential decision problems in stochastic domains .
it thus provides an underlying semantics for the task of planning under uncertainty .
we begin with a concise overview of the mdp framework , and then describe the representation of factored mdps .
123 markov decision processes we briey review the mdp framework , referring the reader to the books by bertsekas and tsitsiklis ( 123 ) or puterman ( 123 ) for a more in - depth review .
a markov decision process ( mdp ) is dened as a 123 - tuple ( x , a , r , p ) where : x is a nite set of |x| = n states; a is a nite set of actions; r is a reward function r : x a ( cid : 123 ) r , such that r ( x , a ) represents the reward obtained by the agent in state x after taking action a; and p is a markovian transition model where p ( x ( cid : 123 ) | x , a ) represents the probability of going from state x to state x ( cid : 123 ) with action a .
we assume that the rewards are bounded , that is , there exists rmax such that rmax |r ( x , a ) | , x , a .
example 123 consider the problem of optimizing the behavior of a system administrator ( sysadmin ) maintaining a network of m computers .
in this network , each machine is connected to some subset of the other machines .
various possible network topologies can be dened in this manner ( see figure 123 for some examples ) .
in one simple network , we might connect the machines in a ring , with machine i connected to machines i + 123 and i 123
( in this example , we assume addition and subtraction are performed modulo m . )
each machine is associated with a binary random variable xi , representing whether it is working or has failed .
at every time step , the sysadmin receives a certain amount of money ( reward ) for each working machine .
the job of the sysadmin is to decide which machine to reboot; thus , there are m+123 possible actions at each time step : reboot one of the m machines or do nothing ( only one machine can be rebooted per time step ) .
if a machine is rebooted , it will be working with high probability at the next time step .
every machine has a small probability of failing at each time step .
however , if a neighboring machine fails , this probability increases dramatically .
these failure probabilities dene the transition model p ( x ( cid : 123 ) | x , a ) , where x is a particular assignment describing which machines are working or have failed in the current time step , a is the sysadmins choice of machine to reboot and x ( cid : 123 ) is the resulting state in the next time step .
we assume that the mdp has an innite horizon and that future rewards are discounted exponentially with a discount factor ( 123 , 123 ) .
a stationary policy for an mdp is a mapping : x ( cid : 123 ) a , where ( x ) is the action the agent takes at state x .
in the computer network problem , for each possible conguration of working and failing machines , the policy would tell the sysadmin which machine to reboot .
each policy is associated with a value function v rn , where v ( x ) is the discounted cumulative value that the agent gets if it starts at state x and follows policy .
more precisely , the value v of a state x under
efficient solution algorithms for factored mdps
figure 123 : network topologies tested; the status of a machine is inuence by the status of
its parent in the network .
policy is given by :
v ( x ) = e
( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) x ( 123 ) = x
where x ( t ) is a random variable representing the state of the system after t steps .
in our running example , the value function represents how much money the sysadmin expects to collect if she starts acting according to when the network is at state x .
the value function for a xed policy is the xed point of a set of linear equations that dene the value of a state in terms of the value of its possible successor states .
more formally , we dene : denition 123 the dp operator , t , for a stationary policy is :
tv ( x ) = r ( x , ( x ) ) +
p ( x ( cid : 123 ) | x , ( x ) ) v ( x ( cid : 123 ) ) .
the value function of policy , v , is the xed point of the t operator : v = tv .
the optimal value function v describes the optimal value the agent can achieve for each starting state .
v is also dened by a set of non - linear equations .
in this case , the value of a state must be the maximal expected value achievable by any policy starting at that state .
more precisely , we dene : denition 123 the bellman operator , t , is :
t v ( x ) = max
( r ( x , a ) +
p ( x ( cid : 123 ) | x , a ) v ( x ( cid : 123 ) ) ) .
the optimal value function v is the xed point of t : v = t v .
for any value function v , we can dene the policy obtained by acting greedily relative to v .
in other words , at each state , the agent takes the action that maximizes the one - step
serverbidirectionalringring and starserverstarserver123 legsring of rings guestrin , koller , parr & venkataraman
utility , assuming that v represents our long - term utility achieved at the next state .
more precisely , we dene :
greedy ( v ) ( x ) = arg max
( r ( x , a ) +
p ( x ( cid : 123 ) | x , a ) v ( x ( cid : 123 ) ) ) .
the greedy policy relative to the optimal value function v is the optimal policy =
123 factored mdps factored mdps are a representation language that allows us to exploit problem structure to represent exponentially large mdps very compactly .
the idea of representing a large mdp using a factored model was rst proposed by boutilier et al .
( 123 ) .
in a factored mdp , the set of states is described via a set of random variables x = ( x123 , .
, xn ) , where each xi takes on values in some nite domain dom ( xi ) .
a state x denes a value xi dom ( xi ) for each variable xi .
in general , we use upper case letters ( e . g . , x ) to denote random variables , and lower case ( e . g . , x ) to denote their values .
we use boldface to denote vectors of variables ( e . g . , x ) or their values ( x ) .
for an instantiation y dom ( y ) and a subset of these variables z y , we use y ( z ) to denote the value of the variables z in the instantiation y .
in a factored mdp , we dene a state transition model using a dynamic bayesian network ( dbn ) ( dean & kanazawa , 123 ) .
let xi denote the variable xi at the current time and x ( cid : 123 ) i , the same variable at the next step .
the transition graph of a dbn is a two - layer directed acyclic graph g whose nodes are ( x123 , .
, xn , x ( cid : 123 ) n ) .
we denote the parents of x ( cid : 123 ) i in the graph by parents ( x ( cid : 123 ) i ) .
for simplicity of exposition , we assume i ) x; thus , all arcs in the dbn are between variables in consecutive that parents ( x ( cid : 123 ) time slices .
( this assumption is used for expository purposes only; intra - time slice arcs are handled by a small modication presented in section 123 . ) each node x ( cid : 123 ) i is associated with a conditional probability distribution ( cpd ) p ( x ( cid : 123 ) i ) ) .
the transition probability p ( x ( cid : 123 ) | x ) is then dened to be : p ( x ( cid : 123 ) | x ) =
i | parents ( x ( cid : 123 )
, x ( cid : 123 )
i | ui ) ,
where ui is the value in x of the variables in parents ( x ( cid : 123 ) example 123 consider an instance of the sysadmin problem with four computers , labelled m123 , .
, m123 , in an unidirectional ring topology as shown in figure 123 ( a ) .
our rst task in modeling this problem as a factored mdp is to dene the state space x .
each machine is associated with a binary random variable xi , representing whether it is working or has failed .
thus , our state space is represented by four random variables : ( x123 , x123 , x123 , x123 ) .
the next task is to dene the transition model , represented as a dbn .
the parents of the next time step variables x ( cid : 123 ) i depend on the network topology .
specically , the probability that machine i will fail at the next time step depends on whether it is working at the current time step and on the status of its direct neighbors ( parents in the topology ) in the network at the current time step .
as shown in figure 123 ( b ) , the parents of x ( cid : 123 ) i in this example are xi and xi123
the cpd of x ( cid : 123 ) i = false with high probability;
i is such that if xi = false , then x ( cid : 123 )
efficient solution algorithms for factored mdps
i = t | xi , xi123 , a ) :
xi123 = f xi123 = f xi123 = t xi123 = t
xi = t
xi = f
xi = f
xi = t
action is reboot :
figure 123 : factored mdp example :
from a network topology ( a ) we obtain the factored
mdp representation ( b ) with the cpds described in ( c ) .
that is , failures tend to persist .
if xi = true , then x ( cid : 123 ) the unidirectional ring topology x ( cid : 123 ) of its neighbors can independently cause machine i to fail .
i is a noisy or of its other parents ( in i has only one other parent xi123 ) ; that is , a failure in any
we have described how to represent factored the markovian transition dynamics arising from an mdp as a dbn , but we have not directly addressed the representation of actions .
generally , we can dene the transition dynamics of an mdp by dening a separate dbn model a = ( cid : 123 ) ga , pa ( cid : 123 ) for each action a .
example 123 in our system administrator example , we have an action ai for rebooting each one of the machines , and a default action d for doing nothing .
the transition model described above corresponds to the do nothing action .
the transition model for ai is dierent from d only in the transition model for the variable x ( cid : 123 ) i = true with probability one , regardless of the status of the neighboring machines .
figure 123 ( c ) shows i = w orking | xi , xi123 , a ) , with one entry for each assignment to the actual cpd for p ( x ( cid : 123 ) the state variables xi and xi123 , and to the action a .
i , which is now x ( cid : 123 )
to fully specify an mdp , we also need to provide a compact representation of the reward function .
we assume that the reward function is factored additively into a set of localized reward functions , each of which only depends on a small set of variables .
in our example , we might have a reward function associated with each machine i , which depends on xi .
that is , the sysadmin is paid on a per - machine basis : at every time step , she receives money for machine i only if it is working .
we can formalize this concept of localized functions : denition 123 a function f has a scope scope ( f ) = c x if f : dom ( c ) ( cid : 123 ) r .
if f has scope y and y z , we use f ( z ) as shorthand for f ( y ) where y is the part of the instantiation z that corresponds to variables in y .
m123m123m123m123r123r123x123x123r123r123r123r123x123x123x123x123x123x123x123x123x123x123x123x123x123x123h123h123h123h123h123h123h123h123r123r123 guestrin , koller , parr & venkataraman
we can now characterize the concept of local rewards .
let ra
r be a set of i ( x123 , .
functions , where the scope of each ra i ) r .
in the reward for taking action a at state x is dened to be ra ( x ) = our example , we have a reward function ri associated with each machine i , which depends only xi , and does not depend on the action choice .
these local rewards are represented by the diamonds in figure 123 ( b ) , in the usual notation for inuence diagrams ( howard &
i is restricted to variable cluster ua
approximate solution algorithms
there are several algorithms to compute the optimal policy in an mdp .
the three most commonly used are value iteration , policy iteration , and linear programming .
a key compo - nent in all three algorithms is the computation of value functions , as dened in section 123 .
recall that a value function denes a value for each state x in the state space .
with an explicit representation of the value function as a vector of values for the dierent states , the solution algorithms all can be implemented as a series of simple algebraic steps .
thus , in this case , all three can be implemented very eciently .
unfortunately , in the case of factored mdps , the state space is exponential in the number of variables in the domain .
in the sysadmin problem , for example , the state x of the system is an assignment describing which machines are working or have failed; that is , a state x is an assignment to each random variable xi .
thus , the number of states is exponential in the number m of machines in the network ( |x| = n = 123m ) .
hence , even representing an explicit value function in problems with more than about ten machines is infeasible .
one might be tempted to believe that factored transition dynamics and rewards would result in a factored value function , which can thereby be represented compactly .
unfortunately , even in trivial factored mdps , there is no guarantee that structure in the model is preserved in the value function ( koller & parr , 123 ) .
in this section , we discuss the use of an approximate value function , that admits a compact representation .
we also describe approximate versions of these exact algorithms , that use approximate value functions .
our description in this section is somewhat abstract , and does not specify how the basic operations required by the algorithms can be performed explicitly .
in later sections , we elaborate on these issues , and describe the algorithms in detail .
for brevity , we choose to focus on policy iteration and linear programming; our techniques easily extend to value iteration .
123 linear value functions a very popular choice for approximating value functions is by using linear regression , as rst proposed by bellman et al .
( 123 ) .
here , we dene our space of allowable value functions v h rn via a set of basis functions : denition 123 a linear value function over a set of basis functions h = ( h123 , .
, hk ) is a function v that can be written as v ( x ) = j=123 wj hj ( x ) for some coecients w = ( w123 , .
, wk ) ( cid : 123 ) .
we can now dene h to be the linear subspace of rn spanned by the basis functions h .
it is useful to dene an n k matrix h whose columns are the k basis functions viewed as
efficient solution algorithms for factored mdps
vectors .
in a more compact notation , our approximate value function is then represented
the expressive power of this linear representation is equivalent , for example , to that of a single layer neural network with features corresponding to the basis functions dening h .
once the features are dened , we must optimize the coecients w in order to obtain a good approximation for the true value function .
we can view this approach as separating the problem of dening a reasonable space of features and the induced space h , from the problem of searching within the space .
the former problem is typically the purview of domain experts , while the latter is the focus of analysis and algorithmic design .
clearly , feature selection is an important issue for essentially all areas of learning and approximation .
we oer some simple methods for selecting good features for mdps in section 123 , but it is not our goal to address this large and important topic in this paper .
once we have a chosen a linear value function representation and a set of basis functions , the problem becomes one of nding values for the weights w such that hw will yield a good approximation of the true value function .
in this paper , we consider two such approaches : approximate dynamic programming using policy iteration and approximate linear programming .
in this section , we present these two approaches .
in section 123 , we show how we can exploit problem structure to transform these approaches into practical algorithms that can deal with exponentially large state spaces .
123 policy iteration
123 . 123 the exact algorithm the exact policy iteration algorithm iterates over policies , producing an improved policy at each iteration .
starting with some initial policy ( 123 ) , each iteration consists of two phases .
value determination computes , for a policy ( t ) , the value function v ( t ) , by nding the xed point of the equation t ( t ) v ( t ) = v ( t ) , that is , the unique solution to the set of linear
v ( t ) ( x ) = r ( x , ( t ) ( x ) ) +
p ( x ( cid : 123 ) | x , ( t ) ( x ) ) v ( t ) ( x ( cid : 123 ) ) , x .
the policy improvement step denes the next policy as ( t+123 ) = greedy ( v ( t ) ) .
it can be shown that this process converges to the optimal policy ( bertsekas & tsitsiklis , 123 ) .
furthermore , in practice , the convergence to the optimal policy is often very quick .
123 . 123 approximate policy iteration the steps in the policy iteration algorithm require a manipulation of both value functions and policies , both of which often cannot be represented explicitly in large mdps .
to dene a version of the policy iteration algorithm that uses approximate value functions , we use the following basic idea : we restrict the algorithm to using only value functions within the provided h; whenever the algorithm takes a step that results in a value function v that is outside this space , we project the result back into the space by nding the value function within the space which is closest to v .
more precisely :
guestrin , koller , parr & venkataraman
denition 123 a projection operator is a mapping : rn h .
is said to be a projection w . r . t .
a norm ( cid : 123 ) ( cid : 123 ) if v = hw such that w arg minw ( cid : 123 ) hw v ( cid : 123 ) .
that is , v is the linear combination of the basis functions , that is closest to v with respect to the chosen norm .
our approximate policy iteration algorithm performs the policy improvement step ex - actly .
in the value determination step , the value function the value of acting according to the current policy ( t ) is approximated through a linear combination of basis functions .
we now consider the problem of value determination for a policy ( t ) .
at this point , it is useful to introduce some notation : although the rewards are a function of the state and action choice , once the policy is xed , the rewards become a function of the state only , which we denote as r ( t ) , where r ( t ) ( x ) = r ( x , ( t ) ( x ) ) .
similarly , for the transition model : p ( t ) ( x ( cid : 123 ) | x ) = p ( x ( cid : 123 ) | x , ( t ) ( x ) ) .
we can now rewrite the value determination step in terms of matrices and vectors .
if we view v ( t ) and r ( t ) as n - vectors , and p ( t ) as an n n matrix , we have the equations :
v ( t ) = r ( t ) + p ( t ) v ( t ) .
this is a system of linear equations with one equation for each state , which can only be solved exactly for relatively small n .
our goal is to provide an approximate solution , within h .
more precisely , we want to nd :
( cid : 123 ) hw ( r ( t ) + p ( t ) hw ) ( cid : 123 ) ;
( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( h p ( t ) h ) w ( t ) r ( t )
w ( t ) = arg min = arg min
thus , our approximate policy iteration alternates between two steps : ( cid : 123 ) hw ( r ( t ) + p ( t ) hw ) ( cid : 123 ) ;
w ( t ) = arg min
( t+123 ) = greedy ( hw ( t ) ) .
123 . 123 max - norm projection an approach along the lines described above has been used in various papers , with several recent theoretical and algorithmic results ( schweitzer & seidmann , 123; tsitsiklis & van roy , 123b; van roy , 123; koller & parr , 123 , 123 ) .
however , these approaches suer from a problem that we might call norm incompatibility .
when computing the projection , they utilize the standard euclidean projection operator with respect to the l123 norm or a weighted l123 norm . 123 on the other hand , most of the convergence and error analyses for mdp algorithms utilize max - norm ( l ) .
this incompatibility has made it dicult to provide we can tie the projection operator more closely to the error bounds through the use of a projection operator in l norm .
the problem of minimizing the l norm has been studied in the optimization literature as the problem of nding the chebyshev solution123 to 123
weighted l123 norm projections are stable and have meaningful error bounds when the weights correspond to the stationary distribution of a xed policy under evaluation ( value determination ) ( van roy , 123 ) , but they are not stable when combined with t .
averagers ( gordon , 123 ) are stable and non - expansive in l , but require that the mixture weights be determined a priori .
thus , they do not , in general , minimize l error .
the chebyshev norm is also referred to as max , supremum and l norms and the minimax solution .
efficient solution algorithms for factored mdps
an overdetermined linear system of equations ( cheney , 123 ) .
the problem is dened as nding w such that :
w arg min
( cid : 123 ) cw b ( cid : 123 ) .
we use an algorithm due to stiefel ( 123 ) , that solves this problem by linear program -
variables : w123 , .
, wk , ;
subject to : ( cid : 123 ) k
j=123 cijwj bi
i = 123 . . n .
( cid : 123 ) ( cid : 123 ) ( cid : 123 ) for each i , or
the constraints in this linear program imply that equivalently , that ( cid : 123 ) cw b ( cid : 123 ) .
the objective of the lp is to minimize .
thus , at the solution ( w , ) of this linear program , w is the solution of equation ( 123 ) and is the l we can use the l projection in the context of the approximate policy iteration in the obvious way .
when implementing the projection operation of equation ( 123 ) , we can use the l projection ( as in equation ( 123 ) ) , where c = ( h p ( t ) h ) and b = r ( t ) .
this minimization can be solved using the linear program of ( 123 ) .
j=123 cijwj bi
a key point is that this lp only has k + 123 variables .
however , there are 123n constraints , which makes it impractical for large state spaces .
in the sysadmin problem , for example , the number of constraints in this lp is exponential in the number of machines in the network ( a total of 123 123m constraints for m machines ) .
in section 123 , we show that , in factored mdps with linear value functions , all the 123n constraints can be represented eciently , leading to a tractable algorithm .
123 . 123 error analysis we motivated our use of the max - norm projection within the approximate policy iteration algorithm via its compatibility with standard error analysis techniques for mdp algorithms .
we now provide a careful analysis of the impact of the l error introduced by the projec - tion step .
the analysis provides motivation for the use of a projection step that directly minimizes this quantity .
we acknowledge , however , that the main impact of this analysis is motivational .
in practice , we cannot provide a priori guarantees that an l projection will outperform other methods .
our goal is to analyze approximate policy iteration in terms of the amount of error if the error is zero , then we are introduced at each step by the projection operation .
performing exact value determination , and no error should accrue .
if the error is small , we should get an approximation that is accurate .
this result follows from the analysis below .
more precisely , we dene the projection error as the error resulting from the approximate value determination step :
r ( t ) + p ( t ) hw ( t )
note that , by using our max - norm projection , we are nding the set of weights w ( t ) that exactly minimizes the one - step projection error ( t ) .
that is , we are choosing the best
guestrin , koller , parr & venkataraman
possible weights with respect to this error measure .
furthermore , this is exactly the error measure that is going to appear in the bounds of our theorem .
thus , we can now make the bounds for each step as tight as possible .
we rst show that the projection error accrued in each step is bounded :
lemma 123 the value determination error is bounded : there exists a constant p rmax such that p ( t ) for all iterations t of the algorithm .
proof : see appendix a . 123
due to the contraction property of the bellman operator , the overall accumulated error
is a decaying average of the projection error incurred throughout all iterations :
denition 123 the discounted value determination error at iteration t is dened as :
( 123 ) = 123
lemma 123 implies that the accumulated error remains bounded in approximate policy .
we can now bound the loss incurred when acting according to the policy generated by our approximate policy iteration algorithm , as opposed to the
( t ) p ( 123t )
theorem 123 in the approximate policy iteration algorithm , let ( t ) be the policy generated at iteration t .
furthermore , let v ( t ) be the actual value of acting according to this policy .
the loss incurred by using policy ( t ) as opposed to the optimal policy with value v is
( cid : 123 ) v v ( t ) ( cid : 123 ) t ( cid : 123 ) v v ( 123 ) ( cid : 123 ) +
( 123 ) 123 .
proof : see appendix a . 123
in words , equation ( 123 ) shows that the dierence between our approximation at iteration t and the optimal value function is bounded by the sum of two terms .
the rst term is present in standard policy iteration and goes to zero exponentially fast .
the second is the discounted accumulated projection error and , as lemma 123 shows , is bounded .
this second term can be minimized by choosing w ( t ) as the one that minimizes :
r ( t ) + p ( t ) hw ( t )
which is exactly the computation performed by the max - norm projection .
therefore , this theorem motivates the use of max - norm projections to minimize the error term that appears in our bound .
the bounds we have provided so far may seem fairly trivial , as we have not provided a strong a priori bound on ( t ) .
fortunately , several factors make these bounds interest - ing despite the lack of a priori guarantees .
if approximate policy iteration converges , as
occurred in all of our experiments , we can obtain a much tighter bound : if ( cid : 123 ) is the policy
after convergence , then :
where ( cid : 123 ) is the one - step max - norm projection error associated with estimating the value of ( cid : 123 ) .
since the max - norm projection operation provides ( cid : 123 ) , we can easily obtain an a
( 123 ) ,
efficient solution algorithms for factored mdps
posteriori bound as part of the policy iteration procedure .
more details are provided in
one could rewrite the bound in theorem 123 in terms of the worst case projection er - ror p , or the worst projection error in a cycle of policies , if approximate policy iteration gets stuck in a cycle .
these formulations would be closer to the analysis of bertsekas and tsitsiklis ( 123 , proposition 123 , p . 123 ) .
however , consider the case where most policies ( or most policies in the nal cycle ) have a low projection error , but there are a few policies that cannot be approximated well using the projection operation , so that they have a large one - step projection error .
a worst - case bound would be very loose , because it would be dictated by the error of the most dicult policy to approximate .
on the other hand , using our discounted accumulated error formulation , errors introduced by policies that are hard to approximate decay very rapidly .
thus , the error bound represents an average case analysis : a decaying average of the projection errors for policies encountered at the succes - sive iterations of the algorithm .
as in the convergent case , this bound can be computed easily as part of the policy iteration procedure when max - norm projection is used .
the practical benet of a posteriori bounds is that they can give meaningful feedback on the impact of the choice of the value function approximation architecture .
while we are not explicitly addressing the dicult and general problem of feature selection in this paper , our error bounds motivate algorithms that aim to minimize the error given an approximation architecture and provide feedback that could be useful in future eorts to automatically discover or improve approximation architectures .
123 approximate linear programming
123 . 123 the exact algorithm linear programming provides an alternative method for solving mdps .
it formulates the problem of nding a value function as a linear program ( lp ) .
here the lp variables are v123 , .
, vn , where vi represents v ( xi ) : the value of starting at the ith state of the system .
the lp is given by :
, vn ;
subject to : vi ( r ( xi , a ) +
xi ( xi ) vi ;
j p ( xj | xi , a ) vj ) xi x , a a ,
where the state relevance weights are positive .
note that , in this exact case , the solution obtained is the same for any positive weight vector .
it is interesting to note that steps of the simplex algorithm correspond to policy changes at single states , while steps of policy iteration can involve policy changes at multiple states .
in practice , policy iteration tends to be faster than the linear programming approach ( puterman , 123 ) .
123 . 123 approximate linear program the approximate formulation for the lp approach , rst proposed by schweitzer and sei - dmann ( 123 ) , restricts the space of allowable value functions to the linear space spanned by our basis functions .
in this approximate formulation , the variables are w123 , .
, wk : the weights for our basis functions .
the lp is given by :
guestrin , koller , parr & venkataraman
variables : w123 , .
, wk ;
i wi hi ( x ) ( r ( x , a ) +
i wi hi ( x ) ;
i wi hi ( x ( cid : 123 ) ) ) x x , a a .
in other words , this formulation takes the lp in ( 123 ) and substitutes the explicit state value function by a linear value function representation i wi hi ( x ) , or , in our more compact notation , v is replaced by hw .
this linear program is guaranteed to be feasible if a constant function a function with the same constant value for all states is included in the set of basis functions .
x ( cid : 123 ) p ( x ( cid : 123 ) | x , a )
in this approximate linear programming formulation , the choice of state relevance weights , , becomes important .
intuitively , not all constraints in this lp are binding; that is , the constraints are tighter for some states than for others .
for each state x , the relevance weight ( x ) indicates the relative importance of a tight constraint .
therefore , unlike the exact case , the solution obtained may dier for dierent choices of the positive weight vector .
furthermore , there is , in general , no guarantee as to the quality of the greedy policy generated from the approximation hw .
however , the recent work of de farias and van roy ( 123a ) provides some analysis of the error relative to that of the best possible approx - imation in the subspace , and some guidance as to selecting so as to improve the quality of the approximation .
in particular , their analysis shows that this lp provides the best approximation hw of the optimal value function v in a weighted l123 sense subject to the constraint that hw t hw , where the weights in the l123 norm are the state relevance
the transformation from an exact to an approximate problem formulation has the ef - fect of reducing the number of free variables in the lp to k ( one for each basis function coecient ) , but the number of constraints remains n |a| .
in our sysadmin problem , for example , the number of constraints in the lp in ( 123 ) is ( m + 123 ) 123m , where m is the number of machines in the network .
thus , the process of generating the constraints and solving the lp still seems unmanageable for more than a few machines .
in the next section , we discuss how we can use the structure of a factored mdp to provide for a compact representation and an ecient solution to this lp .
factored value functions
the linear value function approach , and the algorithms described in section 123 , apply to any choice of basis functions .
in the context of factored mdps , koller and parr ( 123 ) suggest a particular type of basis function , that is particularly compatible with the structure of a factored mdp .
they suggest that , although the value function is typically not structured , there are many cases where it might be close to structured .
that is , it might be well - approximated using a linear combination of functions each of which refers only to a small number of variables .
more precisely , we dene :
denition 123 a factored ( linear ) value function is a linear function over the basis set h123 , .
, hk , where the scope of each hi is restricted to some subset of variables ci .
value functions of this type have a long history in the area of multi - attribute utility the - ory ( keeney & raia , 123 ) .
in our example , we might have a basis function hi for each
efficient solution algorithms for factored mdps
machine , indicating whether it is working or not .
each basis function has scope restricted to xi .
these are represented as diamonds in the next time step in figure 123 ( b ) .
factored value functions provide the key to performing ecient computations over the exponential - sized state spaces we have in factored mdps .
the main insight is that re - stricted scope functions ( including our basis functions ) allow for certain basic operations to be implemented very eciently .
in the remainder of this section , we show how structure in factored mdps can be exploited to perform two crucial operations very eciently : one - step lookahead ( backprojection ) , and the representation of exponentially many constraints in the lps .
then , we use these basic building blocks to formulate very ecient approxima - tion algorithms for factored mdps , each presented in its own self - contained section : the approximate linear programming for factored mdps in section 123 , and approximate policy iteration with max - norm projection in section 123
123 one - step lookahead a key step in all of our algorithms is the computation of the one - step lookahead value of some action a .
this is necessary , for example , when computing the greedy policy as in equation ( 123 ) .
lets consider the computation of a q function , qa ( x ) , which represents the expected value the agent obtains after taking action a at the current time step and receiving a long - term value v thereafter .
this q function can be computed by :
qa ( x ) = r ( x , a ) +
p ( x ( cid : 123 ) | x , a ) v ( x ) .
that is , qa ( x ) is given by the current reward plus the discounted expected future value .
using this notation , we can express the greedy policy as : greedy ( v ) ( x ) = maxa qa ( x ) .
functions : v ( x ) =
recall that we are estimating the long - term value of our policy using a set of basis
i wi hi ( x ) .
thus , we can rewrite equation ( 123 ) as :
qa ( x ) = r ( x , a ) +
p ( x ( cid : 123 ) | x , a )
x ( cid : 123 ) p ( x ( cid : 123 ) | the size of the state space is exponential , so that computing the expectation i wi hi ( x ) seems infeasible .
fortunately , as discussed by koller and parr ( 123 ) , this expectation operation , or backprojection , can be performed eciently if the transition model and the value function are both factored appropriately .
the linearity of the value function permits a linear decomposition , where each summand in the expectation can be viewed as an independent value function and updated in a manner similar to the value iteration procedure used by boutilier et al .
( 123 ) .
we now recap the construction briey , by rst dening :
p ( x ( cid : 123 ) | x , a )
p ( x ( cid : 123 ) | x , a ) hi ( x ( cid : 123 ) ) .
thus , we can compute the expectation of each basis function separately :
wi hi ( x ( cid : 123 ) ) =
i ( x ) =
p ( x ( cid : 123 ) | x , a ) hi ( x ( cid : 123 ) ) ,
guestrin , koller , parr & venkataraman
the and then weight them by wi to obtain the total expectation ga ( x ) = is called the backprojection of the basis function hi through the intermediate function ga i = pahi .
note that , in factored mdps , the transition model pa , which we denote by ga transition model pa is factored ( represented as a dbn ) and the basis functions hi have scope restricted to a small set of variables .
these two important properties allow us to compute the backprojections very eciently .
i wi ga
we now show how some restricted scope function h ( such as our basis functions ) can be backprojected through some transition model p represented as a dbn .
here h has scope restricted to y; our goal is to compute g = p h .
we dene the back - projected scope of y through as the set of parents of y ( cid : 123 ) in the transition graph g ; ( y ( cid : 123 ) ) = y ( cid : 123 ) ( x123 , .
, xn , x ( cid : 123 ) n ) , then the only change in our algorithm is in the denition of back - projected scope of y through .
the denition now includes not only direct parents of y ( cid : 123 ) , but also all variables in ( x123 , .
, xn ) that are ancestors of y ( cid : 123 ) :
if intra - time slice arcs are included , so that parents ( x ( cid : 123 )
i y ( cid : 123 ) parents ( y ( cid : 123 )
, x ( cid : 123 )
( y ( cid : 123 ) ) = ( xj | there exist a directed path from xj to any x ( cid : 123 )
thus , the backprojected scope may become larger , but the functions are still factored .
we can now show that , if h has scope restricted to y , then its backprojection g has scope restricted to the parents of y ( cid : 123 ) , i . e . , ( y ( cid : 123 ) ) .
furthermore , each backprojection can be computed by only enumerating settings of variables in ( y ( cid : 123 ) ) , rather than settings of all variables x :
g ( x ) = ( p h ) ( x ) ;
p ( x ( cid : 123 ) | x ) h ( x ( cid : 123 ) ) ; p ( x ( cid : 123 ) | x ) h ( y ( cid : 123 ) ) ; p ( y ( cid : 123 ) | x ) h ( y ( cid : 123 ) )
p ( y ( cid : 123 ) | z ) h ( y ( cid : 123 ) ) ;
p ( u ( cid : 123 ) | x ) ;
u ( cid : 123 ) ( x ( cid : 123 ) y ( cid : 123 ) ) p ( u ( cid : 123 ) | x ) = 123 as it is the where z is the value of ( y ( cid : 123 ) ) in x and the term sum of a probability distribution over a complete domain .
therefore , we see that ( p h ) is a function whose scope is restricted to ( y ( cid : 123 ) ) .
note that the cost of the computation depends linearly on |dom ( ( y ( cid : 123 ) ) ) | , which depends on y ( the scope of h ) and on the complexity of the process dynamics .
this backprojection procedure is summarized in figure 123
returning to our example , consider a basis function hi that is an indicator of variable xi : it takes value 123 if the ith machine is working and 123 otherwise .
each hi has scope restricted to i ) = ( xi123 , xi ) .
i , thus , its backprojection gi has scope restricted to parents ( x ( cid : 123 )
123 representing exponentially many constraints as seen in section 123 , both our approximation algorithms require the solution of linear pro - grams : the lp in ( 123 ) for approximate policy iteration , and the lp in ( 123 ) for the approximate
efficient solution algorithms for factored mdps
backproja ( h ) where basis function h has scope c .
dene the scope of the backprojection : a ( c ( cid : 123 ) ) = x ( cid : 123 ) for each assignment y a ( c ( cid : 123 ) ) :
i ) | y ) h ( c ( cid : 123 ) ) .
figure 123 : backprojection of basis function h .
linear programming algorithm .
these lps have some common characteristics : they have a small number of free variables ( for k basis functions there are k + 123 free variables in ap - proximate policy iteration and k in approximate linear programming ) , but the number of constraints is still exponential in the number of state variables .
however , in factored mdps , these lp constraints have another very useful property : the functionals in the constraints have restricted scope .
this key observation allows us to represent these constraints very
first , observe that the constraints in the linear programs are all of the form :
wi ci ( x ) b ( x ) , x ,
where only and w123 , .
, wk are free variables in the lp and x ranges over all states .
this general form represents both the type of constraint in the max - norm projection lp in ( 123 ) and the approximate linear programming formulation in ( 123 ) . 123
the rst insight in our construction is that we can replace the entire set of constraints
in equation ( 123 ) by one equivalent non - linear constraint :
wi ci ( x ) b ( x ) .
the second insight is that this new non - linear constraint can be implemented by a set of linear constraints using a construction that follows the structure of variable elimination in cost networks .
this insight allows us to exploit structure in factored mdps to represent this constraint compactly .
we tackle the problem of representing the constraint in equation ( 123 ) in two steps : rst , computing the maximum assignment for a xed set of weights; then , representing the non - linear constraint by small set of linear constraints , using a construction we call the
123 . 123 maximizing over the state space the key computation in our algorithms is to represent a non - linear constraint of the form in equation ( 123 ) eciently by a small set of linear constraints .
before presenting this con - struction , lets rst consider a simpler problem : given some xed weights wi , we would i wi ci ( x ) b ( x ) , that is , the state x , such like to compute the maximization : = maxx i wi ci ( x ) , can be formulated using an analogous construction to the one we present in this section by changing the sign of ci ( x ) and b ( x ) .
the approximate linear programming constraints of ( 123 ) can also be formulated in this form , as we show in section 123
the complementary constraints in ( 123 ) , b ( x ) ( cid : 123 )
guestrin , koller , parr & venkataraman
that the dierence between i wi ci ( x ) and b ( x ) is maximal .
however , we cannot explic - itly enumerate the exponential number of states and compute the dierence .
fortunately , structure in factored mdps allows us to compute this maximum eciently .
in the case of factored mdps , our state space is a set of vectors x which are assign - ments to the state variables x = ( x123 , .
we can view both cw and b as functions of these state variables , and hence also their dierence .
thus , we can dene a function i wi ci ( x ) b ( x ) .
note that we have executed a f w ( x123 , .
, xn ) such that f w ( x ) = representation shift; we are viewing f w as a function of the variables x , which is pa - rameterized by w .
recall that the size of the state space is exponential in the number of variables .
hence , our goal in this section is to compute maxx f w ( x ) without explicitly considering each of the exponentially many states .
the solution is to use the fact that f w has a factored representation .
more precisely , cw has the form i wi ci ( zi ) , where zi is a subset of x .
for example , we might have c123 ( x123 , x123 ) which takes value 123 in states where x123 = true and x123 = false and 123 otherwise .
similarly , the vector b in our case is also a sum of restricted scope functions .
thus , we can express f w as a sum or may not depend on w .
in the future , we sometimes drop the superscript w when it is clear from context .
i wi ci ( x ) b ( x ) = maxx f w ( x ) , that is , to nd the state x over which f w is maximized .
recall that f w = j=123 fj ( zj ) .
we can maximize such a function , f w , without enumerating every state using non - serial dynamic programming ( bertele & brioschi , 123 ) .
the idea is virtually identical to variable elimination in a bayesian network .
we review this construction here , as it is a central component in our solution lp .
using our more compact notation , our goal here is simply to compute maxx
j ( zj ) , where f w
our goal is to compute
the main idea is that , rather than summing all functions and then doing the maximization , we maximize over variables one at a time .
when maximizing over xl , only summands involving xl participate in the maximization .
example 123 assume
f = f123 ( x123 , x123 ) + f123 ( x123 , x123 ) + f123 ( x123 , x123 ) + f123 ( x123 , x123 ) .
we therefore wish to compute :
f123 ( x123 , x123 ) + f123 ( x123 , x123 ) + f123 ( x123 , x123 ) + f123 ( x123 , x123 ) .
we can rst compute the maximum over x123; the functions f123 and f123 are irrelevant , so we can push them out .
we get
f123 ( x123 , x123 ) + f123 ( x123 , x123 ) + max
( f123 ( x123 , x123 ) + f123 ( x123 , x123 ) ) .
the result of the internal maximization depends on the values of x123 , x123; thus , we can intro - duce a new function e123 ( x123 , x123 ) whose value at the point x123 , x123 is the value of the internal max expression .
our problem now reduces to computing
f123 ( x123 , x123 ) + f123 ( x123 , x123 ) + e123 ( x123 , x123 ) ,
efficient solution algorithms for factored mdps
variableelimination ( f , o )
/ / f = ( f123 , .
, fm ) is the set of functions to be maximized; / / o stores the elimination order .
for i = 123 to number of variables :
/ / select the next variable to be eliminated .
let l = o ( i ) ; / / select the relevant functions .
let e123 , .
, el be the functions in f whose scope contains xl .
/ / maximize over current variable xl .
dene a new function e = maxxl / / update set of functions .
update the set of functions f = f ( e ) \ ( e123 , .
, el ) .
; note that scope ( e ) =
/ / now , all functions have empty scope and their sum is the maximum value of f123 + + fm .
return the maximum value
figure 123 : variable elimination procedure for computing the maximum value f123 + + fm ,
where each fi is a restricted scope function .
having one fewer variable .
next , we eliminate another variable , say x123 , with the resulting expression reducing to :
f123 ( x123 , x123 ) + e123 ( x123 , x123 ) ,
e123 ( x123 , x123 ) = max
( f123 ( x123 , x123 ) + e123 ( x123 , x123 ) ) .
finally , we dene
e123 = max
f123 ( x123 , x123 ) + e123 ( x123 , x123 ) .
the result at this point is a number , which is the desired maximum over x123 , .
while the naive approach of enumerating all states requires 123 arithmetic operations if all variables are binary , using variable elimination we only need to perform 123 operations .
the general variable elimination algorithm is described in figure 123
the inputs to the algorithm are the functions to be maximized f = ( f123 , .
, fm ) and an elimination ordering o on the variables , where o ( i ) returns the ith variable to be eliminated .
as in the example above , for each variable xl to be eliminated , we select the relevant functions e123 , .
, el , those whose scope contains xl .
these functions are removed from the set f and j=123 ej .
at this point , the scope of the functions in we introduce a new function e = maxxl f no longer depends on xl , that is , xl has been eliminated .
this procedure is repeated until all variables have been eliminated .
the remaining functions in f thus have empty scope .
the desired maximum is therefore given by the sum of these remaining functions .
the computational cost of this algorithm is linear in the number of new function values introduced in the elimination process .
more precisely , consider the computation of a new function e whose scope is z .
to compute this function , we need to compute |dom ( z ) | dierent values .
the cost of the algorithm is linear in the overall number of these values , introduced throughout the execution .
as shown by dechter ( 123 ) , this cost is exponential
guestrin , koller , parr & venkataraman
in the induced width of the cost network , the undirected graph dened over the variables x123 , .
, xn , with an edge between xl and xm if they appear together in one of the original functions fj .
the complexity of this algorithm is , of course , dependent on the variable elimination order and the problem structure .
computing the optimal elimination order is an np - hard problem ( arnborg , corneil , & proskurowski , 123 ) and elimination orders yielding low induced tree width do not exist for some problems .
these issues have been confronted successfully for a large variety of practical problems in the bayesian network community , which has beneted from a large variety of good heuristics which have been developed for the variable elimination ordering problem ( bertele & brioschi , 123; kjaerul , 123; reed , 123; becker & geiger , 123 ) .
123 . 123 factored lp in this section , we present the centerpiece of our planning algorithms : a new , general approach for compactly representing exponentially large sets of lp constraints in problems with factored structure those where the functions in the constraints can be decomposed as the sum of restricted scope functions .
consider our original problem of representing the non - linear constraint in equation ( 123 ) compactly .
recall that we wish to represent the non - linear constraint maxx i wi ci ( x ) b ( x ) , or equivalently , maxx f w ( x ) , without generating one constraint for each state as in equation ( 123 ) .
the new , key insight is that this non - linear constraint can be implemented using a construction that follows the structure of variable elimination in cost networks .
consider any function e used within f ( including the original fis ) , and let z be its scope .
for any assignment z to z , we introduce variable ue z , whose value represents ez , into the linear program .
for the initial functions f w i ( z ) .
as is linear in w , this constraint is linear in the lp variables .
now , consider a new function e introduced into f by eliminating a variable xl .
let e123 , .
, el be the functions extracted from f , and let z be the scope of the resulting e .
we introduce a set of constraints :
i , we include the constraint that ufi
z = f w
let en be the last function generated in the elimination , and recall that its scope is empty .
hence , we have only a single variable uen .
we introduce the additional constraint uen .
the complete algorithm , presented in figure 123 , is divided into three parts : first , we generate equality constraints for functions that depend on the weights wi ( basis functions ) .
in the second part , we add the equality constraints for functions that do not depend on the weights ( target functions ) .
these equality constraints let us abstract away the dierences between these two types of functions and manage them in a unied fashion in the third part of the algorithm .
this third part follows a procedure similar to variable elimination described in figure 123
however , unlike standard variable elimination where we would in - j=123 ej , in our factored lp procedure we troduce a new function e , such that e = maxxl z .
to enforce the denition of e as the maximum over xl of introduce new lp variables ue
j=123 ej , we introduce the new lp constraints in equation ( 123 ) .
example 123 to understand this construction , consider our simple example above , and assume we want to express the fact that maxx f w ( x ) .
we rst introduce a set of
efficient solution algorithms for factored mdps
factoredlp ( c , b , o )
/ / c = ( c123 , .
, ck ) is the set of basis functions .
/ / b = ( b123 , .
, bm ) is the set of target functions .
/ / o stores the elimination order .
/ / return a ( polynomial ) set of constraints equivalent to ( cid : 123 )
j bj ( x ) , x .
i wici ( x ) +
/ / data structure for the constraints in factored lp .
let = ( ) .
/ / data structure for the intermediate functions generated in variable elimination .
let f = ( ) .
/ / generate equality constraint to abstract away basis functions .
for each ci c :
let z = scope ( ci ) .
for each assignment z z , create a new lp variable ufi constraint to :
z = wici ( z ) .
z and add a
store new function fi to use in variable elimination step : f = f ( fi ) .
/ / generate equality constraint to abstract away target functions .
for each bj b :
let z = scope ( bj ) .
for each assignment z z , create a new lp variable ufj constraint to :
z and add a
z = bj ( z ) .
store new function fj to use in variable elimination step : f = f ( fj ) .
/ / now , f contains all of the functions involved in the lp , our constraints become : eif ei ( x ) , x , which we represent compactly using a variable elimination procedure .
for i = 123 to number of variables :
/ / select the next variable to be eliminated .
let l = o ( i ) ; / / select the relevant functions .
let e123 , .
, el be the functions in f whose scope contains xl , and let zj = scope ( ej ) .
/ / introduce linear constraints for the maximum over current variable xl .
j=123zj ( xl ) to represent dene a new function e with scope z = l add constraints to to enforce maximum : for each assignment z z :
/ / update set of functions .
update the set of functions f = f ( e ) \ ( e123 , .
, el ) .
/ / now , all variables have been eliminated and all functions have empty scope .
add last constraint to :
i wici ( x ) +
figure 123 : factored lp algorithm for the compact representation of the exponential set of
guestrin , koller , parr & venkataraman
x123 , x123 for every instantiation of values x123 , x123 to the variables x123 , x123
thus , if x123 and x123 are both binary , we have four such variables .
we then introduce a constraint dening the value of uf123 t , t = 123 t , f = w123
we have similar variables and constraints for each fj and each value z in zj .
note that each of the constraints is a simple equality constraint involving numerical constants and perhaps the weight variables w .
x123 , x123 appropriately .
for example , for our f123 above , we have uf123
next , we introduce variables for each of the intermediate expressions generated by vari - able elimination .
for example , when eliminating x123 , we introduce a set of lp variables x123 , x123; for each of them , we have a set of constraints x123 , x123 + uf123
one for each value x123 of x123
we have a similar set of constraint for ue123 x123 , x123 and ue123
note that each constraint is a simple linear inequality .
x123 , x123 in terms of
we can now prove that our factored lp construction represents the same constraint as
non - linear constraint in equation ( 123 ) : theorem 123 the constraints generated by the factored lp construction are equivalent to the non - linear constraint in equation ( 123 ) .
that is , an assignment to ( , w ) satises the factored lp constraints if and only if it satises the constraint in equation ( 123 ) .
proof : see appendix a . 123
is cw b in the original set of constraints .
hence our new set of constraints is equivalent to the original set : i wi ci ( x ) b ( x ) in equation ( 123 ) , which in turn is equivalent to the exponential i wi ci ( x ) b ( x ) , x in equation ( 123 ) .
thus , we can represent this exponential set of constraints by a new set of constraints and lp variables .
the size of this new set , as in variable elimination , is exponential only in the induced width of the cost network , rather than in the total number of variables .
set of constraints ( cid : 123 )
returning to our original formulation , we have that
in this section , we presented a new , general approach for compactly representing expo - nentially - large sets of lp constraints in problems with factored structure .
in the remainder of this paper , we exploit this construction to design ecient planning algorithms for factored
123 . 123 factored max - norm projection we can now use our procedure for representing the exponential number of constraints in equation ( 123 ) compactly to compute ecient max - norm projections , as in equation ( 123 ) :
w arg min
( cid : 123 ) cw b ( cid : 123 ) .
of constraints in this lp : ( cid : 123 ) k
j=123 cijwj bi , i and bi ( cid : 123 ) k
the max - norm projection is computed by the linear program in ( 123 ) .
there are two sets j=123 cijwj , i .
each of these sets is an instance of the constraints in equation ( 123 ) , which we have just addressed in the previous section .
thus , if each of the k basis functions in c is a restricted scope function and the target function b is the sum of restricted scope functions , then we can use our factored lp technique to represent the constraints in the max - norm projection lp compactly .
the correctness of our algorithm is a corollary of theorem 123 :
efficient solution algorithms for factored mdps
corollary 123 the solution ( , w ) of a linear program that minimizes subject to the constraints in factoredlp ( c , b , o ) and factoredlp ( c , b , o ) , for any elimination order o satises :
w arg min
( cid : 123 ) cw b ( cid : 123 ) ,
( cid : 123 ) cw b ( cid : 123 ) .
the original max - norm projection lp had k + 123 variables and two constraints for each state x; thus , the number of constraints is exponential in the number of state variables .
on the other hand , our new factored max - norm projection lp has more variables , but exponentially fewer constraints .
the number of variables and constraints in the new factored lp is exponential only in the number of state variables in the largest factor in the cost network , rather than exponential in the total number of state variables .
as we show in section 123 , this exponential gain allows us to compute max - norm projections eciently when solving very large factored mdps .
approximate linear programming
we begin with the simplest of our approximate mdp solution algorithms , based on the approximate linear programming formulation in section 123 .
using the basic operations described in section 123 , we can formulate an algorithm that is both simple and ecient .
123 the algorithm as discussed in section 123 , approximate linear program formulation is based on the linear programming approach to solving mdps presented in section 123 .
however , in this ap - proximate version , we restrict the space of value functions to the linear space dened by our basis functions .
more precisely , in this approximate lp formulation , the variables are w123 , .
, wk the weights for our basis functions .
the lp is given by :
i wi hi ( x ) ( r ( x , a ) +
i wi hi ( x ) ;
variables : w123 , .
, wk ;
x ( cid : 123 ) p ( x ( cid : 123 ) | x , a )
i wi hi ( x ( cid : 123 ) ) ) x x , a a .
in other words , this formulation takes the lp in ( 123 ) and substitutes the explicit state value function with a linear value function representation i wi hi ( x ) .
this transformation from an exact to an approximate problem formulation has the eect of reducing the number of free variables in the lp to k ( one for each basis function coecient ) , but the number of constraints remains |x| |a| .
in our sysadmin problem , for example , the number of constraints in the lp in ( 123 ) is ( m + 123 ) 123m , where m is the number of machines in the network .
however , using our algorithm for representing exponentially large constraint sets compactly we are able to compute the solution to this approximate linear programming algorithm in closed form with an exponentially smaller lp , as in section 123 .
first , consider the objective function
i wi hi ( x ) of the lp ( 123 ) .
naively representing this objective function requires a summation over a exponentially large state space .
however , we can rewrite the objective and obtain a compact representation .
we rst reorder the terms :
guestrin , koller , parr & venkataraman
factoredalp ( p , r , , h , o , )
/ / p is the factored transition model .
/ / r is the set of factored reward functions .
/ / is the discount factor .
/ / h is the set of basis functions h = ( h123 , .
/ / o stores the elimination order .
/ / are the state relevance weights .
/ / return the basis function weights w computed by approximate linear programming .
/ / cache the backprojections of the basis functions .
for each basis function hi h; for each action a :
i = backproja ( hi ) .
/ / compute factored state relevance weights .
for each basis function hi , compute the factored state relevance weights
i as in equation ( 123 ) .
/ / generate approximate linear programming constraints let = ( ) .
for each action a :
let = factoredlp ( ( ga
123 h123 ,
k hk ) , ra , o ) .
x ( cid : 123 ) p ( x ( cid : 123 )
/ / so far , our constraints guarantee that r ( x , a ) +
i wi hi ( x ( cid : 123 ) ) i wi hi ( x ) ; to satisfy the approximate linear programming solution in ( 123 ) we must add
| x , a )
a nal constraint .
let = ( = 123 ) .
/ / we can now obtain the solution weights by solving an lp .
let w be the solution of the linear program : minimize
the constraints .
i iwi , subject to
figure 123 : factored approximate linear programming algorithm .
efficient solution algorithms for factored mdps
wi hi ( x ) =
now , consider the state relevance weights ( x ) as a distribution over states , so that ( x ) > 123
x ( x ) = 123
as in backprojections , we can now write :
( x ) hi ( x ) =
where ( ci ) represents the marginal of the state relevance weights over the domain dom ( ci ) of the basis function hi .
for example , if we use uniform state relevance weights as in our experiments ( x ) = 123|x| then the marginals become ( ci ) = 123|ci| .
thus , we can rewrite the objective function as i wi i , where each basis weight i is computed as shown in equation ( 123 ) .
if the state relevance weights are represented by marginals , then the cost of computing each i depends exponentially on the size of the scope of ci only , rather than exponentially on the number of state variables .
on the other hand , if the state relevance weights are represented by arbitrary distributions , we need to obtain the marginals over the cis , which may not be an ecient computation .
thus , greatest eciency is achieved by using a compact representation , such as a bayesian network , for the state relevance weights .
second , note that the right side of the constraints in the lp ( 123 ) correspond to the qa
qa ( x ) = ra ( x ) +
using the ecient backprojection operation in factored mdps described in section 123 we can rewrite the qa functions as :
p ( x ( cid : 123 ) | x , a )
qa ( x ) = ra ( x ) +
discussed , if hi has scope restricted to ci , then ga
i is the backprojection of basis function hi through the transition model pa .
as we is a restricted scope function of a ( c ( cid : 123 ) i and the basis relevance weights i
we can precompute the backprojections ga
approximate linear programming lp of ( 123 ) can be written as :
variables : w123 , .
, wk ;
i i wi ; i wi hi ( x ) ( ra ( x ) +
i wi ga
i ( x ) ) x x , a a .
finally , we can rewrite this lp to use constraints of the same form as the one in equa -
variables : w123 , .
, wk ; subject to : 123 maxx ( ra ( x ) +
i i wi ;
i wi ( ga
i ( x ) hi ( x ) ) ) a a .
we can now use our factored lp construction in section 123 to represent these non - linear constraints compactly .
basically , there is one set of factored lp constraints for each action a .
specically , we can write the non - linear constraint in the same form as those in equa - tion ( 123 ) by expressing the functions c as : ci ( x ) = hi ( x ) ga i ( x ) .
each ci ( x ) is a restricted
guestrin , koller , parr & venkataraman
i ) , which means that ci ( x ) has scope restricted to ci a ( c ( cid : 123 )
i ( x ) has scope restricted scope function; that is , if hi ( x ) has scope restricted to ci , then ga i ) .
next , the target function b becomes the reward function ra ( x ) which , by assumption , is factored .
finally , in the constraint in equation ( 123 ) , is a free variable .
on the other hand , in the lp in ( 123 ) the maximum in the right hand side must be less than zero .
this nal condition can be achieved by adding a constraint = 123
thus , our algorithm generates a set of factored lp constraints , one for each action .
the total number of constraints and variables in this new lp is linear in the number of actions |a| and only exponential in the induced width of each cost network , rather than in the total number of variables .
the complete factored approximate linear programming algorithm is outlined in figure 123
123 an example
we now present a complete example of the operations required by the approximate lp algo - rithm to solve the factored mdp shown in figure 123 ( a ) .
our presentation follows four steps : problem representation , basis function selection , backprojections and lp construction .
first , we must fully specify the factored mdp model for the problem .
the structure of the dbn is shown in figure 123 ( b ) .
this structure is maintained for all action choices .
next , we must dene the transition probabilities for each action .
there are 123 actions in this problem : do nothing , or reboot one of the 123 machines in the network .
the cpds for these actions are shown in figure 123 ( c ) .
finally , we must dene the reward function .
we decompose the global reward as the sum of 123 local reward functions , one for each machine , such that there is a reward if the machine is working .
specically , ri ( xi = true ) = 123 and ri ( xi = false ) = 123 , breaking symmetry by setting r123 ( x123 = true ) = 123
we use a discount factor of = 123 .
in this simple example , we use ve simple basis functions .
basis function selection : first , we include the constant function h123 = 123
next , we add indicators for each machine which take value 123 if the machine is working : hi ( xi = true ) = 123 and hi ( xi = false ) = 123
the rst algorithmic step is computing the backprojection of the basis functions , as dened in section 123 .
the backprojection of the constant basis is
pa ( x ( cid : 123 ) | x ) h123 ; pa ( x ( cid : 123 ) | x ) 123 ;
next , we must backproject our indicator basis functions hi :
pa ( x ( cid : 123 ) | x ) hi ( x ( cid : 123 )
j | xj123 , xj ) hi ( x ( cid : 123 )
efficient solution algorithms for factored mdps
i | xi123 , xi ) hi ( x ( cid : 123 ) i | xi123 , xi ) hi ( x ( cid : 123 )
i = true | xi123 , xi ) 123 + pa ( x ( cid : 123 ) i = true | xi123 , xi ) .
j | xj123 , xj ) ;
i = false | xi123 , xi ) 123 ;
is a restricted scope function of ( xi123 , xi ) .
we can now use the cpds in fig -
ure 123 ( c ) to specify ga
greboot = i
( xi123 , xi ) =
greboot ( cid : 123 ) = i
( xi123 , xi ) =
xi123 = true xi123 = false
xi123 = true xi123 = false
xi = true xi = false
xi = true xi = false
to illustrate the factored lps constructed by our algorithms , we dene the constraints for the approximate linear programming approach presented above .
i hi , as shown in equation ( 123 ) .
in our example , first , we dene the functions ca 123 = 123 = 123 for the constant basis , and for the indicator bases : these functions are ca
i = ga
creboot = i
( xi123 , xi ) =
xi123 = true xi123 = false
creboot ( cid : 123 ) = i
( xi123 , xi ) =
xi = true xi = false
xi = true xi = false
xi123 = true xi123 = false
using this denition of ca
i , the approximate linear programming constraints are given by :
we present the lp construction for one of the 123 actions : reboot = 123
analogous constructions can be made for the other actions .
in the rst set of constraints , we abstract away the dierence between rewards and basis functions by introducing lp variables u and equality constraints .
we begin with the reward
x123 = 123 , ur123 x123 = 123 , ur123
x123 = 123 ; x123 = 123 ;
x123 = 123 , ur123 x123 = 123 , ur123
x123 = 123 ; x123 = 123 .
we now represent the equality constraints for the ca j functions for the reboot = 123 action .
note that the appropriate basis function weight from equation ( 123 ) appears in these constraints :
guestrin , koller , parr & venkataraman
uc123 = 123 w123 ; x123 , x123 = 123 w123 , x123 , x123 = 123 w123 , x123 , x123 = 123 w123 , x123 , x123 = 123 w123 ; x123 , x123 = 123 w123 , uc123 x123 , x123 = 123 w123 , uc123 x123 , x123 = 123 w123 , uc123 x123 , x123 = 123 w123 ; x123 , x123 = 123 w123 , uc123 x123 , x123 = 123 w123 , uc123 x123 , x123 = 123 w123 , uc123 x123 , x123 = 123 w123 ; x123 , x123 = 123 w123 , uc123 x123 , x123 = 123 w123 , uc123 x123 , x123 = 123 w123 , uc123 x123 , x123 = 123 w123 .
using these new lp variables , our lp constraint from equation ( 123 ) for the reboot = 123 action
+ uc123 +
we are now ready for the variable elimination process .
we illustrate the elimination of
+ uc123 +
we can represent the term maxx123 one for each assignment of x123 and x123 , using the new lp variables ue123
by a set of linear constraints , to represent this
x123 + uc123 x123 + uc123 x123 + uc123 x123 + uc123 x123 + uc123 x123 + uc123 x123 + uc123 x123 + uc123
x123 , x123 + uc123 x123 , x123 + uc123 x123 , x123 + uc123 x123 , x123 + uc123 x123 , x123 + uc123 x123 , x123 + uc123 x123 , x123 + uc123 x123 , x123 + uc123
we have now eliminated variable x123 and our global non - linear constraint becomes :
+ uc123 +
next , we eliminate variable x123
the new lp constraints and variables have the form :
, x123 , x123 , x123 ;
thus , removing x123 from the global non - linear constraint :
+ uc123 + uc123
efficient solution algorithms for factored mdps
figure 123 : number of constraints in the lp generated by the explicit state representation versus the factored lp construction for the solution of the ring problem with basis functions over single variables and approximate linear programming as the
we can now eliminate x123 , generating the linear constraints :
, x123 , x123 .
now , our global non - linear constraint involves only x123 : + uc123 + ue123
as x123 is the last variable to be eliminated , the scope of the new lp variable is empty and the linear constraints are given by :
, x123 .
all of the state variables have now been eliminated , turning our global non - linear constraint into a simple linear constraint :
123 uc123 + ue123 ,
which completes the lp description for the approximate linear programming solution to the problem in figure 123
in this small example with only four state variables , our factored lp technique generates a total of 123 equality constraints , 123 inequality constraints and 123 lp variables , while the explicit state representation in equation ( 123 ) generates only 123 inequality constraints and 123 lp variables .
however , as the problem size increases , the number of constraints and lp variables in our factored lp approach grow as o ( n123 ) , while the explicit state approach grows exponentially , at o ( n123n ) .
this scaling eect is illustrated in figure 123
approximate policy iteration with max - norm projection
the factored approximate linear programming approach described in the previous section is both elegant and easy to implement .
however , we cannot , in general , provide strong
123number of machines in ringnumber of lp constraintsexplicit lpfactored lp# factored constraints = 123n + 123n - 123# explicit constraints = ( n+123 ) 123 n guestrin , koller , parr & venkataraman
guarantees about the error it achieves .
an alternative is to use the approximate policy iteration described in section 123 , which does oer certain bounds on the error .
however , as we shall see , this algorithm is signicantly more complicated , and requires that we place additional restrictions on the factored mdp .
in particular , approximate policy iteration requires a representation of the policy at each iteration .
in order to obtain a compact policy representation , we must make an additional assumption : each action only aects a small number of state variables .
we rst state this assumption formally .
then , we show how to obtain a compact representation of the greedy policy with respect to a factored value function , under this assumption .
finally , we describe our factored approximate policy iteration algorithm using max - norm projections .
123 default action model in section 123 , we presented the factored mdp model , where each action is associated with its own factored transition model represented as a dbn and with its own factored reward function .
however , dierent actions often have very similar transition dynamics , only dif - fering in their eect on some small set of variables .
in particular , in many cases a variable has a default evolution model , which only changes if an action aects it directly ( boutilier et al . , 123 ) .
this type of structure turns out to be useful for compactly representing policies , a prop - erty which is important in our approximate policy iteration algorithm .
thus , in this section of the paper , we restrict attention to factored mdps that are dened using a default transi - tion model d = ( cid : 123 ) gd , pd ( cid : 123 ) ( koller & parr , 123 ) .
for each action a , we dene eects ( a ) x ( cid : 123 ) to be the variables in the next state whose local probability model is dierent from d , i . e . , those variables x ( cid : 123 ) example 123 in our system administrator example , we have an action ai for rebooting each one of the machines , and a default action d for doing nothing .
the transition model described above corresponds to the do nothing action , which is also the default transition model .
the transition model for ai is dierent from d only in the transition model for the i = true with probability one , regardless of the status of the neighboring machines .
thus , in this example , eects ( ai ) = x ( cid : 123 )
i , which is now x ( cid : 123 )
i such that pa ( x ( cid : 123 )
i | parentsa ( x ( cid : 123 )
i ) ) ( cid : 123 ) = pd ( x ( cid : 123 )
i | parentsd ( x ( cid : 123 )
as in the transition dynamics , we can also dene the notion of default reward model .
in i=123 ri ( ui ) associated with the default action this case , there is a set of reward functions d .
in addition , each action a can have a reward function ra ( ua ) .
here , the extra reward of i ( x123 , .
thus , the total reward action a has scope restricted to rewards ( a ) = ua associated with action a is given by ra + i=123 ri .
note that ra can also be factored as a linear combination of smaller terms for an even more compact representation .
we can now build on this additional assumption to dene the complete algorithm .
recall that the approximate policy iteration algorithm iterates through two steps : policy improvement and approximate value determination .
we now discuss each of these steps .
123 computing greedy policies the policy improvement step computes the greedy policy relative to a value function v ( t123 ) :
( t ) = greedy ( v ( t123 ) ) .
efficient solution algorithms for factored mdps
recall that our value function estimates have the linear form hw .
as we described in section 123 , the greedy policy for this type of value function is given by :
greedy ( hw ) ( x ) = arg max
where each qa can be represented by : qa ( x ) = r ( x , a ) +
i wi ga
if we attempt to represent this policy naively , we are again faced with the problem of exponentially large state spaces .
fortunately , as shown by koller and parr ( 123 ) , the greedy policy relative to a factored value function has the form of a decision list .
more precisely , the policy can be written in the form ( cid : 123 ) t123 , a123 ( cid : 123 ) , ( cid : 123 ) t123 , a123 ( cid : 123 ) , .
, ( cid : 123 ) tl , al ( cid : 123 ) , where each ti is an assignment of values to some small subset ti of variables , and each ai is an action .
the greedy action to take in state x is the action aj corresponding to the rst event tj in the list with which x is consistent .
for completeness , we now review the construction of this decision - list policy .
the critical assumption that allows us to represent the policy as a compact decision list is the default action assumption described in section 123 .
under this assumption , the qa functions can be written as :
qa ( x ) = ra ( x ) +
where ra has scope restricted to ua .
the q function for the default action d is just :
i=123 ri ( x ) +
i wi gd
is equal to gd
i for most i .
intuitively , a component ga
we now have a set of linear q - functions which implicitly describes a policy .
not immediately obvious that these q functions result in a compactly expressible policy .
an important insight is that most of the components in the weighted combination are identical , so that ga to the backprojection of basis function hi ( ci ) is only dierent if the action a inuences one of the variables in ci .
more formally , assume that eects ( a ) ci = .
in this case , all of the variables in ci have the same transition model in a and d .
thus , we have i ( x ) ; in other words , the ith component of the qa function is irrelevant when deciding whether action a is better than the default action d .
we can dene which components are actually relevant : let ia be the set of indices i such that eects ( a ) ci ( cid : 123 ) = .
these are the indices of those basis functions whose backprojection diers in pa and pd .
in our example dbn of figure 123 , actions and basis functions involve single variables , so iai = i .
i ( x ) = gd
let us now consider the impact of taking action a over the default action d .
we can
dene the impact the dierence in value as : a ( x ) = qa ( x ) qd ( x ) ;
= ra ( x ) +
this analysis shows that a ( x ) is a function whose scope is restricted to
i ( x ) gd
ta = ua ( cid : 123 ) iiaa ( c ( cid : 123 )
guestrin , koller , parr & venkataraman
/ / qa is the set of q - functions , one for each action; / / return the decision list policy .
/ / initialize decision list .
let = ( ) .
/ / compute the bonus functions .
for each action a , other than the default action d :
compute the bonus for taking action a ,
a ( x ) = qa ( x ) qd ( x ) ;
as in equation ( 123 ) .
note that a has scope restricted to ta , as in for each assignment t ta :
/ / add states with positive bonuses to the ( unsorted ) decision list .
if a ( t ) > 123 , add branch to decision list :
= ( ( cid : 123 ) t , a , a ( t ) ( cid : 123 ) ) .
/ / add the default action to the ( unsorted ) decision list .
let = ( ( cid : 123 ) , d , 123 ( cid : 123 ) ) .
/ / sort decision list to obtain nal policy .
sort the decision list in decreasing order on the element of ( cid : 123 ) t , a , ( cid : 123 ) .
figure 123 : method for computing the decision list policy from the factored representation
of the qa functions .
in our example dbn , ta123 = ( x123 , x123 ) .
intuitively , we now have a situation where we have a baseline value function qd ( x ) which denes a value for each state x .
each action a changes that baseline by adding or subtracting an amount from each state .
the point is that this amount depends only on ta , so that it is the same for all states in which the variables in ta take the same values .
we can now dene the greedy policy relative to our q functions .
for each action a , dene a set of conditionals ( cid : 123 ) t , a , ( cid : 123 ) , where each t is some assignment of values to the variables ta , and is a ( t ) .
now , sort the conditionals for all of the actions by order of decreasing :
( cid : 123 ) t123 , a123 , 123 ( cid : 123 ) , ( cid : 123 ) t123 , a123 , 123 ( cid : 123 ) , .
, ( cid : 123 ) tl , al , l ( cid : 123 ) .
consider our optimal action in a state x .
we would like to get the largest possible bonus over the default value .
if x is consistent with t123 , we should clearly take action a123 , as it gives us bonus 123
if not , then we should try to get 123; thus , we should check if x is consistent with t123 , and if so , take a123
using this procedure , we can compute the decision - list policy associated with our linear estimate of the value function .
the complete algorithm for computing the decision list policy is summarized in figure 123
a |dom ( ta ) |; ta , in turn , depends on the set of basis function clusters that intersect with the eects of a .
thus , the size of the policy depends in a natural way on the interaction between the structure of our
note that the number of conditionals in the list is
efficient solution algorithms for factored mdps
process description and the structure of our basis functions .
in problems where the actions modify a large number of variables , the policy representation could become unwieldy .
the approximate linear programming approach in section 123 is more appropriate in such cases , as it does not require an explicit representation of the policy .
123 value determination in the approximate value determination step our algorithm computes : ( cid : 123 ) hw ( r ( t ) + p ( t ) hw ) ( cid : 123 ) .
w ( t ) = arg min
by rearranging the expression , we get :
w ( t ) = arg min
( cid : 123 ) ( h p ( t ) h ) w r ( t ) ( cid : 123 ) .
this equation is an instance of the optimization in equation ( 123 ) .
if p ( t ) is factored , we can conclude that c = ( h p ( t ) h ) is also a matrix whose columns correspond to restricted - scope functions .
more specically :
ci ( x ) = hi ( x ) g ( t )
is the backprojection of the basis function hi through the transition model p ( t ) , as described in section 123 .
the target b = r ( t ) corresponds to the reward function , which for the moment is assumed to be factored .
thus , we can again apply our factored lp in section 123 . 123 to estimate the value of the policy ( t ) .
unfortunately , the transition model p ( t ) is not factored , as a decision list representa - tion for the policy ( t ) will , in general , induce a transition model p ( t ) which cannot be represented by a compact dbn .
nonetheless , we can still generate a compact lp by ex - ploiting the decision list structure of the policy .
the basic idea is to introduce cost networks corresponding to each branch in the decision list , ensuring , additionally , that only states consistent with this branch are considered in the cost network maximization .
specically , we have a factored lp construction for each branch ( cid : 123 ) ti , ai ( cid : 123 ) .
the ith cost network only considers a subset of the states that is consistent with the ith branch of the decision list .
let si be the set of states x such that ti is the rst event in the decision list for which x is consistent .
that is , for each state x si , x is consistent with ti , but it is not consistent with any tj with j < i .
imply that ( cid : 123 ) i wi ci ( x ) b ( x ) for each state x .
recall that , as in equation ( 123 ) , our lp construction denes a set of constraints that instead , we have a separate set of constraints for the states in each subset si .
for each state in si , we know that action ai is taken .
hence , we can apply our construction above using pai a transition model which is factored by assumption in place of the non - factored p ( t ) .
similarly , the reward function becomes rai ( x ) +
i=123 ri ( x ) for this subset of states .
the only issue is to guarantee that the cost network constraints derived from this tran - sition model are applied only to states in si .
specically , we must guarantee that they are applied only to states consistent with ti , but not to states that are consistent with some tj for j < i .
to guarantee the rst condition , we simply instantiate the variables in ti to take the values specied in ti .
that is , our cost network now considers only the variables in
guestrin , koller , parr & venkataraman
factoredapi ( p , r , , h , o , , tmax )
/ / p is the factored transition model .
/ / r is the set of factored reward functions .
/ / is the discount factor .
/ / h is the set of basis functions h = ( h123 , .
/ / o stores the elimination order .
/ / bellman error precision .
/ / tmax maximum number of iterations .
/ / return the basis function weights w computed by approximate policy iteration .
let w ( 123 ) = 123
/ / cache the backprojections of the basis functions .
for each basis function hi h; for each action a :
i = backproja ( hi ) .
/ / main approximate policy iteration loop .
let t = 123
/ / policy improvement part of the loop .
/ / compute decision list policy for iteration t weights .
let ( t ) = decisionlistpolicy ( ra +
/ / value determination part of the loop .
/ / initialize constraints for max - norm projection lp .
let + = ( ) and = ( ) .
let i = ( ) .
/ / for every branch of the decision list policy , generate the relevant set of constraints , and
update the indicators to constraint the state space for future branches .
, hk gaj
for each branch ( cid : 123 ) tj , aj ( cid : 123 ) in the decision list policy ( t ) : / / instantiate the variables in tj to the assignment given in tj .
instantiate the set of functions ( h123 gaj partial state assignment tj and store in c .
k ) with the instantiate the target functions raj with the partial state assign - ment tj and store in b .
instantiate the indicator functions i with the partial state as - signment tj and store in i ( cid : 123 ) .
/ / generate the factored lp constraints for the current decision list branch .
let + = + factoredlp ( c , b + i ( cid : 123 ) , o ) .
let = factoredlp ( c , b + i ( cid : 123 ) , o ) .
/ / update the indicator functions .
let ij ( x ) = 123 ( x = tj ) and update the indicators i = i ij .
/ / we can now obtain the new set of weights by solving an lp , which corresponds to the
let w ( t+123 ) be the solution of the linear program : minimize , subject to the constraints ( + , ) .
let t = t + 123
until bellmanerr ( hw ( t ) ) or t tmax or w ( t123 ) = w ( t ) .
figure 123 : factored approximate policy iteration with max - norm projection algorithm .
efficient solution algorithms for factored mdps
, xn ) ti , and computes the maximum only over the states consistent with ti = ti .
to guarantee the second condition , we ensure that we do not impose any constraints on states associated with previous decisions .
this is achieved by adding indicators ij for each previous decision tj , with weight .
more specically , ij is a function that takes value for states consistent with tj and zero for other all assignments of tj .
the constraints for the ith branch will be of the form :
r ( x , ai ) +
wl ( gl ( x , ai ) h ( x ) ) +
123 ( x = tj ) ,
where x ( ti ) denes the assignments of x consistent with ti .
the introduction of these indicators causes the constraints associated with ti to be trivially satised by states in sj for j < i .
note that each of these indicators is a restricted - scope function of tj and can be handled in the same fashion as all other terms in the factored lp .
thus , for a decision list of size l , our factored lp contains constraints from 123l cost networks .
the complete approximate policy iteration with max - norm projection algorithm is outlined in figure 123
it is instructive to compare our max - norm policy iteration algorithm to the l123 - projection policy iteration algorithm of koller and parr ( 123 ) in terms of computational costs per iteration and implementation complexity .
computing the l123 projection requires ( among other things ) a series of dot product operations between basis functions and backprojected basis functions ( cid : 123 ) hig j ( cid : 123 ) .
these expressions are easy to compute if p refers to the transition model of a particular action a .
however , if the policy is represented as a decision list , as is the result of the policy improvement step , then this step becomes much more complicated .
in particular , for every branch of the decision list , for every pair of basis functions i and j , and for each assignment to the variables in scope ( hi ) scope ( ga j ) , it requires the solution of a counting problem which is ( cid : 123 ) p - complete in general .
although koller and parr show that this computation can be performed using a bayesian network ( bn ) inference , the algorithm still requires a bn inference for each one of those assignments at each branch of the decision list .
this makes the algorithm very dicult to implement eciently in practice .
the max - norm projection , on the other hand , relies on solving a linear program at every iteration .
the size of the linear program depends on the cost networks generated .
as we discuss , two cost networks are needed for each point in the decision list .
the complexity of each of these cost networks is approximately the same as only one of the bn inferences in the counting problem for the l123 projection .
overall , for each branch in the decision list , we have a total of two of these inferences , as opposed to one for each assignment of j ) for every pair of basis functions i and j .
thus , the max - norm policy iteration algorithm is substantially less complex computationally than the approach based on l123 - projection .
furthermore , the use of linear programming allows us to rely on existing lp packages ( such as cplex ) , which are very highly optimized .
it is also interesting to compare the approximate policy iteration algorithm to the ap - proximate linear programming algorithm we presented in section 123
in the approximate linear programming algorithm , we never need to compute the decision list policy .
the policy is always represented implicitly by the qa functions .
thus , this algorithm does not
guestrin , koller , parr & venkataraman
require explicit computation or manipulation of the greedy policy .
this dierence has two important consequences : one computational and the other in terms of generality .
first , not having to compute or consider the decision lists makes approximate linear programming faster and easier to implement .
in this algorithm , we generate a single lp with one cost network for each action and never need to compute a decision list policy .
on the other hand , in each iteration , approximate policy iteration needs to generate two lps for every branch of the decision list of size l , which is usually signicantly longer than |a| , with a total of 123l cost networks .
in terms of representation , we do not require the policies to be compact; thus , we do not need to make the default action assumption .
therefore , the approximate linear programming algorithm can deal with a more general class of problems , where each action can have its own independent dbn transition model .
on the other hand , as described in section 123 , approximate policy iteration has stronger guarantees in terms of error bounds .
these dierences will be highlighted further in our experimental results presented in section 123
computing bounds on policy quality
are the resulting basis function weights .
in practice , the agent will dene its behavior by
we have presented two algorithms for computing approximate solutions to factored mdps .
in section 123 , we showed some a priori bounds for the quality of the policy .
another possible procedure is to compute an a posteriori bound .
that is , given our resulting weights
all these algorithms generate linear value functions which can be denoted by h ( cid : 123 ) w , where ( cid : 123 ) w acting according to the greedy policy ( cid : 123 ) = greedy ( h ( cid : 123 ) w ) .
one issue that remains is how this policy ( cid : 123 ) compares to the true optimal policy ; that is , how the actual value v ( cid : 123 ) of policy ( cid : 123 ) compares to v .
( cid : 123 ) w , we compute a bound on the loss of acting according to the greedy policy ( cid : 123 ) rather than the optimal policy .
this can be achieved by using the bellman error analysis of williams and baird ( 123 ) .
policy ( cid : 123 ) = greedy ( v ) , their analysis provides the bound : the bellman error is dened as bellmanerr ( v ) = ( cid : 123 ) t v v ( cid : 123 ) .
given the greedy
thus , we can use the bellman error bellmanerr ( h ( cid : 123 ) w ) to evaluate the quality of our resulting
note that computing the bellman error involves a maximization over the state space .
thus , the complexity of this computation grows exponentially with the number of state variables .
koller and parr ( 123 ) suggested that structure in the factored mdp can be exploited to compute the bellman error eciently .
here , we show how this error bound can be computed by a set of cost networks using a similar construction to the one in our max -
norm projection algorithms .
this technique can be used for any ( cid : 123 ) that can be represented for some set of weights ( cid : 123 ) w , the bellman error is given by :
as a decision list and does not depend on the algorithm used to determine the policy .
thus , we can apply this technique to solutions determined approximate linear programming if the action descriptions permit a decision list representation of the policy .
efficient solution algorithms for factored mdps
factoredbellmanerr ( p , r , , h , o , ( cid : 123 ) w )
/ / p is the factored transition model .
/ / r is the set of factored reward functions .
/ / is the discount factor .
/ / h is the set of basis functions h = ( h123 , .
/ / o stores the elimination order .
/ / ( cid : 123 ) w are the weights for the linear value function .
/ / return the bellman error for the value function h ( cid : 123 ) w .
/ / compute decision list policy for value function h ( cid : 123 ) w .
let ( cid : 123 ) = decisionlistpolicy ( ra +
for each basis function hi h; for each action a :
/ / cache the backprojections of the basis functions .
i = backproja ( hi ) .
let i = ( ) .
/ / initialize bellman error .
let = 123
/ / for every branch of the decision list policy , generate the relevant cost networks , solve it with variable elimination , and update the indicators to constraint the state space for future branches .
for each branch ( cid : 123 ) tj , aj ( cid : 123 ) in the decision list policy ( cid : 123 ) :
instantiate the set of functions ( ( cid : 123 ) w123 ( h123gaj
partial state assignment tj and store in c .
k ) ) with the instantiate the target functions raj with the partial state assignment tj and store in b .
instantiate the indicator functions i with the partial state assignment tj and store in i ( cid : 123 ) .
, ( cid : 123 ) wk ( hkgaj
/ / instantiate the variables in tj to the assignment given in tj .
/ / use variable elimination to solve rst cost network , and update bellman error , if error for this branch is larger .
let = max ( , variableelimination ( c b + i ( cid : 123 ) , o ) ) .
/ / use variable elimination to solve second cost network , and update bellman error , if error for this branch is larger .
let = max ( , variableelimination ( c + b + i ( cid : 123 ) , o ) ) .
/ / update the indicator functions .
let ij ( x ) = 123 ( x = tj ) and update the indicators i = i ij .
figure 123 : algorithm for computing bellman error for factored value function h ( cid : 123 ) w .
guestrin , koller , parr & venkataraman
bellmanerr ( h ( cid : 123 ) w ) = ( cid : 123 ) t h ( cid : 123 ) w h ( cid : 123 ) w ( cid : 123 ) ;
x ( cid : 123 ) p ( cid : 123 ) ( x ( cid : 123 ) | x ) i wihi ( x ) r ( cid : 123 ) ( x ) j wjhj ( x ( cid : 123 ) ) ( cid : 123 ) x ( cid : 123 ) p ( cid : 123 ) ( x ( cid : 123 ) | x ) j wjhj ( x ( cid : 123 ) ) , if the rewards r ( cid : 123 ) and the transition model p ( cid : 123 ) are factored appropriately , then we can network as described in section 123 . 123
however , ( cid : 123 ) is a decision list policy and it does not
compute each one of these two maximizations ( maxx ) using variable elimination in a cost
maxx r ( cid : 123 ) ( x ) +
induce a factored transition model .
fortunately , as in the approximate policy iteration algorithm in section 123 , we can exploit the structure in the decision list to perform such maximization eciently .
in particular , as in approximate policy iteration , we will generate two cost networks for each branch in the decision list .
to guarantee that our maximization is performed only over states where this branch is relevant , we include the same type of indicator functions , which will force irrelevant states to have a value of , thus guaran - teeing that at each point of the decision list policy we obtain the corresponding state with the maximum error .
the state with the overall largest bellman error will be the maximum over the ones generated for each point the in the decision list policy .
the complete factored algorithm for computing the bellman error is outlined in figure 123
one last interesting note concerns our approximate policy iteration algorithm with max - norm projection of section 123
in all our experiments , this algorithm converged , so that w ( t ) = w ( t+123 ) after some iterations .
if such convergence occurs , then the objective function ( t+123 ) of the linear program in our last iteration is equal to the bellman error of the nal
lemma 123 if approximate policy iteration with max - norm projection converges , so that w ( t ) = w ( t+123 ) for some iteration t , then the max - norm projection error ( t+123 ) of the last
iteration is equal to the bellman error for the nal value function estimate h ( cid : 123 ) w = hw ( t ) :
bellmanerr ( h ( cid : 123 ) w ) = ( t+123 ) .
proof : see appendix a . 123
thus , we can bound the loss of acting according to the nal policy ( t+123 ) by substituting
( t+123 ) into the bellman error bound :
corollary 123 if approximate policy iteration with max - norm projection converges after
t iterations to a nal value function estimate h ( cid : 123 ) w associated with a greedy policy ( cid : 123 ) = greedy ( h ( cid : 123 ) w ) , then the loss of acting according to ( cid : 123 ) instead of the optimal policy is
where v ( cid : 123 ) is the actual value of the policy ( cid : 123 ) .
therefore , when approximate policy iteration converges we can obtain a bound on the quality of the resulting policy without needing to compute the bellman error explicitly .
efficient solution algorithms for factored mdps
exploiting context - specic structure
thus far , we have presented a suite of algorithms which exploit additive structure in the reward and basis functions and sparse connectivity in the dbn representing the transition model .
however , there exists another important type of structure that should also be exploited for ecient decision making : context - specic independence ( csi ) .
for example , consider an agent responsible for building and maintaining a house , if the painting task can only be completed after the plumbing and the electrical wiring have been installed , then the probability that the painting is done is 123 , in all contexts where plumbing or electricity are not done , independently of the agents action .
the representation we have used so far in this paper would use a table to represent this type of function .
this table is exponentially large in the number of variables in the scope of the function , and ignores the context - specic structure inherent in the problem denition .
boutilier et al .
( boutilier et al . , 123; dearden & boutilier , 123; boutilier , dean , & hanks , 123; boutilier et al . , 123 ) have developed a set of algorithms which can exploit csi in the transition and reward models to perform ecient ( approximate ) planning .
although this approach is often successful in problems where the value function contains sucient context - specic structure , the approach is not able to exploit the additive structure which is also often present in real - world problems .
in this section , we extend the factored mdp model to include context - specic structure .
we present a simple , yet eective extension of our algorithms which can exploit both csi and additive structure to obtain ecient approximations for factored mdps .
we rst extend the factored mdp representation to include context - specic structure and then show how the basic operations from section 123 required by our algorithms can be performed eciently in this new representation .
123 factored mdps with context - specic and additive structure there are several representations for context - specic functions .
the most common are decision trees ( boutilier et al . , 123 ) , algebraic decision diagrams ( adds ) ( hoey , st - aubin , hu , & boutilier , 123 ) , and rules ( zhang & poole , 123 ) .
we choose to use rules as our basic representation , for two main reasons .
first , the rule - based representation allows a fairly simple algorithm for variable elimination , which is a key operation in our framework .
second , rules are not required to be mutually exclusive and exhaustive , a requirement that can be restrictive if we want to exploit additive independence , where functions can be represented as a linear combination of a set of non - mutually exclusive functions .
we begin by describing the rule - based representation ( along the lines of zhang and pooles presentation ( 123 ) ) for the probabilistic transition model , in particular , the cpds of our dbn model .
roughly speaking , each rule corresponds to some set of cpd entries that are all associated with a particular probability value .
these entries with the same value are referred to as consistent contexts : denition 123 let c ( x , x ( cid : 123 ) ) and c dom ( c ) .
we say that c is consistent with b dom ( b ) , for b ( x , x ( cid : 123 ) ) , if c and b have the same assignment for the variables in the probability of these consistent contexts will be represented by probability rules :
guestrin , koller , parr & venkataraman
123 = ( cid : 123 ) electrical : 123 ( cid : 123 )
123 = ( cid : 123 ) electrical plumbing : 123 ( cid : 123 ) 123 = ( cid : 123 ) electrical plumbing : 123 ( cid : 123 )
123 = ( cid : 123 ) electrical : 123 ( cid : 123 )
123 = ( cid : 123 ) electrical plumbing : 123 ( cid : 123 )
123 = ( cid : 123 ) electrical plumbing painting : 123 ( cid : 123 ) 123 = ( cid : 123 ) electrical plumbing painting : 123 ( cid : 123 )
figure 123 : example cpds for variable the painting = true represented as decision trees : ( a ) when the action is paint; ( b ) when the action is not paint .
the same cpds can be represented by probability rules as shown in ( c ) and ( d ) , respectively .
denition 123 a probability rule = ( cid : 123 ) c : p ( cid : 123 ) is a function : ( x , x ( cid : 123 ) ) ( cid : 123 ) ( 123 , 123 ) , where the context c dom ( c ) for c ( x , x ( cid : 123 ) ) and p ( 123 , 123 ) , such that ( x , x ( cid : 123 ) ) = p if ( x , x ( cid : 123 ) ) is consistent with c and is equal to 123 otherwise .
in this case , it is convenient to require that the rules be mutually exclusive and exhaus -
tive , so that each cpd entry is uniquely dened by its association with a single rule .
denition 123 a rule - based conditional probability distribution ( rule cpd ) pa is a func - tion pa : ( ( x ( cid : 123 ) i ) x ) ( cid : 123 ) ( 123 , 123 ) , composed of a set of probability rules ( 123 , 123 , .
, m ) whose contexts are mutually exclusive and exhaustive .
we dene :
where j is the unique rule in pa for which cj is consistent with ( x ( cid : 123 ) for all x ,
we require that ,
i | x ) = j ( x , x ( cid : 123 ) ) ,
i | x ) = 123
we can dene parentsa ( x ( cid : 123 ) i ) to be the union of the contexts of the rules in pa ( x ( cid : 123 ) example of a cpd represented by a set of probability rules is shown in figure 123
i | x )
rules can also be used to represent additive functions , such as reward or basis functions .
we represent such context specic value dependencies using value rules :
electrical electrical plumbingplumbingp ( painting ) = 123not donedonedonenot donep ( painting ) = 123p ( painting ) = 123electrical electrical plumbingplumbingp ( painting ) = 123not donedonepaintingpaintingdonedonenot donenot donep ( painting ) = 123p ( painting ) = 123p ( painting ) = 123 efficient solution algorithms for factored mdps
denition 123 a value rule = ( cid : 123 ) c : v ( cid : 123 ) is a function : x ( cid : 123 ) r such that ( x ) = v when x is consistent with c and 123 otherwise .
note that a value rule ( cid : 123 ) c : v ( cid : 123 ) has a scope c .
it is important to note that value rules are not required to be mutually exclusive and exhaustive .
each value rule represents a ( weighted ) indicator function , which takes on a value v in states consistent with some context c , and 123 in all other states .
in any given state , the values of the zero or more rules consistent with that state are simply added together .
example 123 in our construction example , we might have a set of rules :
123 = ( cid : 123 ) plumbing = done : 123 ( cid : 123 ) ; 123 = ( cid : 123 ) electricity = done : 123 ( cid : 123 ) ; 123 = ( cid : 123 ) painting = done : 123 ( cid : 123 ) ; 123 = ( cid : 123 ) action = plumb : 123 ( cid : 123 ) ;
which , when summed together , dene the reward function r = 123 + 123 + 123 + 123 + .
in general , our reward function ra is represented as a rule - based function :
denition 123 a rule - based function f : x ( cid : 123 ) r is composed of a set of rules ( 123 , .
, n ) such that f ( x ) =
in the same manner , each one of our basis functions hj is now represented as a rule - based
this notion of a rule - based function is related to the tree - structure functions used by boutilier et al .
( 123 ) , but is substantially more general .
in the tree - structure value func - tions , the rules corresponding to the dierent leaves are mutually exclusive and exhaustive .
thus , the total number of dierent values represented in the tree is equal to the number of leaves ( or rules ) .
in the rule - based function representation , the rules are not mutually exclusive , and their values are added to form the overall function value for dierent settings of the variables .
dierent rules are added in dierent settings , and , in fact , with k rules , one can easily generate 123k dierent possible values , as is demonstrated in section 123
thus , the rule - based functions can provide a compact representation for a much richer class of
using this rule - based representation , we can exploit both csi and additive independence in the representation of our factored mdp and basis functions .
we now show how the basic operations in section 123 can be adapted to exploit our rule - based representation .
123 adding , multiplying and maximizing consistent rules in our table - based algorithms , we relied on standard sum and product operators applied to tables .
in order to exploit csi using a rule - based representation , we must redene these standard operations .
in particular , the algorithms will need to add or multiply rules that ascribe values to overlapping sets of states .
we will start by dening these operations for rules with the same context :
guestrin , koller , parr & venkataraman
denition 123 let 123 = ( cid : 123 ) c : v123 ( cid : 123 ) and 123 = ( cid : 123 ) c : v123 ( cid : 123 ) be two rules with context c .
dene the rule product as 123 123 = ( cid : 123 ) c : v123 v123 ( cid : 123 ) , and the rule sum as 123 + 123 = ( cid : 123 ) c : v123 + v123 ( cid : 123 ) .
note that this denition is restricted to rules with the same context .
we will address this issue in a moment .
first , we will introduce an additional operation which maximizes a variable from a set of rules , which otherwise share a common context : denition 123 let y be a variable with dom ( y ) = ( y123 , .
, yk ) , and let i , for each i = 123 , .
, k , be a rule of the form i = ( cid : 123 ) c y = yi : vi ( cid : 123 ) .
then for the rule - based function f = 123 + + k , dene the rule maximization over y as maxy f = ( cid : 123 ) c : maxi vi ( cid : 123 ) .
after this operation , y has been maximized out from the scope of the function f .
these three operations we have just described can only be applied to sets of rules that satisfy very stringent conditions .
to make our set of rules amenable to the application of these operations , we might need to rene some of these rules .
we therefore dene the denition 123 let = ( cid : 123 ) c : v ( cid : 123 ) be a rule , and y be a variable .
dene the rule split split ( ( cid : 123 ) y ) of on a variable y as follows : if y scope ( c ) , then split ( ( cid : 123 ) y ) = ( ) ;
split ( ( cid : 123 ) y ) = ( ( cid : 123 ) c y = yi : v ( cid : 123 ) | yi dom ( y ) ) .
thus , if we split a rule on variable y that is not in the scope of the context of , then we generate a new set of rules , with one for each assignment in the domain of y .
in general , the purpose of rule splitting is to extend the context c of one rule coincide with the context c ( cid : 123 ) of another consistent rule ( cid : 123 ) .
naively , we might take all variables in scope ( c ( cid : 123 ) ) scope ( c ) and split recursively on each one of them .
however , this process creates unnecessarily many rules : if y is a variable in scope ( c ( cid : 123 ) ) scope ( c ) and we split on y , then only one of the |dom ( y ) | new rules generated will remain consistent with ( cid : 123 ) : the one which has the same assignment for y as the one in c ( cid : 123 ) .
thus , only this consistent rule needs to be split further .
we can now dene the recursive splitting procedure that achieves this more parsimonious representation : denition 123 let = ( cid : 123 ) c : v ( cid : 123 ) be a rule , and b be a context such that b dom ( b ) .
dene the recursive rule split split ( ( cid : 123 ) b ) of on a context b as follows :
( ) , if c is not consistent with b; else , 123
( ) , if scope ( b ) scope ( c ) ; else , 123
( split ( i ( cid : 123 ) b ) | i split ( ( cid : 123 ) y ) ) , for some variable y scope ( b ) scope ( c ) .
in this denition , each variable y scope ( b ) scope ( c ) leads to the generation of k = |dom ( y ) | rules at the step in which it is split .
however , only one of these k rules is used in the next recursive step because only one is consistent with b .
therefore , the size of the y scope ( b ) scope ( c ) ( |dom ( y ) | 123 ) .
this size is independent of the split set is simply 123 + order in which the variables are split within the operation .
efficient solution algorithms for factored mdps
note that only one of the rules in split ( ( cid : 123 ) b ) is consistent with b : the one with context c b .
thus , if we want to add two consistent rules 123 = ( cid : 123 ) c123 : v123 ( cid : 123 ) and 123 = ( cid : 123 ) c123 : v123 ( cid : 123 ) , then all we need to do is replace these rules by the set :
split ( 123 ( cid : 123 ) c123 ) split ( 123 ( cid : 123 ) c123 ) ,
and then simply replace the resulting rules ( cid : 123 ) c123 c123 : v123 ( cid : 123 ) and ( cid : 123 ) c123 c123 : v123 ( cid : 123 ) by their sum ( cid : 123 ) c123 c123 : v123 + v123 ( cid : 123 ) .
multiplication is performed in an analogous manner .
example 123 consider adding the following set of consistent rules :
123 = ( cid : 123 ) a b : 123 ( cid : 123 ) , 123 = ( cid : 123 ) a c d : 123 ( cid : 123 ) .
in these rules , the context c123 of 123 is a b , and the context c123 of 123 is a c d .
rules 123 and 123 are consistent , therefore , we must split them to perform the addition
( cid : 123 ) a b c : 123 ( cid : 123 ) ,
( cid : 123 ) a b c d : 123 ( cid : 123 ) , ( cid : 123 ) a b c d : 123 ( cid : 123 ) .
( cid : 123 ) a b c d : 123 ( cid : 123 ) , ( cid : 123 ) a b c d : 123 ( cid : 123 ) .
split ( 123 ( cid : 123 ) c123 ) =
split ( 123 ( cid : 123 ) c123 ) =
the result of adding rules 123 and 123 is
( cid : 123 ) a b c : 123 ( cid : 123 ) , ( cid : 123 ) a b c d : 123 ( cid : 123 ) , ( cid : 123 ) a b c d : 123 ( cid : 123 ) , ( cid : 123 ) a b c d : 123 ( cid : 123 ) .
123 rule - based one - step lookahead using this compact rule - based representation , we are able to compute a one - step lookahead plan eciently for models with signicant context - specic or additive independence .
as in section 123 for the table - based case , the rule - based qa function can be represented as the sum of the reward function and the discounted expected value of the next state .
due to our linear approximation of the value function , the expectation term is , in turn , represented as the linear combination of the backprojections of our basis functions .
to exploit csi , we are representing the rewards and basis functions as rule - based functions .
to represent qa as a rule - based function , it is sucient for us to show how to represent the backprojection gj of the basis function hj as a rule - based function .
each hj is a rule - based function , which can be written as hj ( x ) =
each rule is a restricted scope function; thus , we can
i ( hj )
has the form
: v ( hj ) simplify the backprojection as :
guestrin , koller , parr & venkataraman
rulebackproja ( ) , where is given by ( cid : 123 ) c : v ( cid : 123 ) , with c dom ( c ) .
let g = ( ) .
select the set p of relevant probability rules : i c and c is consistent with cj ) .
p = ( j p ( x ( cid : 123 ) remove the x ( cid : 123 ) assignments from the context of all rules in p .
/ / multiply consistent rules : while there are two consistent rules 123 = ( cid : 123 ) c123 : p123 ( cid : 123 ) and 123 = ( cid : 123 ) c123 : p123 ( cid : 123 ) :
i | parents ( x ( cid : 123 )
i ) ) | x ( cid : 123 )
if c123 = c123 , replace these two rules by ( cid : 123 ) c123 : p123p123 ( cid : 123 ) ; else replace these two rules by the set : split ( 123 ( cid : 123 ) c123 ) split ( 123 ( cid : 123 ) c123 ) .
/ / generate value rules : for each rule i in p :
update the backprojection g = g ( ( cid : 123 ) ci : piv ( cid : 123 ) ) .
figure 123 : rule - based backprojection .
j ( x ) =
pa ( x ( cid : 123 ) | x ) hj ( x ( cid : 123 ) ) ; pa ( x ( cid : 123 ) | x )
pa ( x ( cid : 123 ) | x ) ( hj )
i pa ( c ( hj )
where the term v ( hj ) projection operation by rulebackproja ( ( hj )
i pa ( c ( hj )
| x ) can be written as a rule function .
we denote this back -
the backprojection procedure , described in figure 123 , follows three steps .
first , the relevant rules are selected : in the cpds for the variables that appear in the context of , we select the rules consistent with this context , as these are the only rules that play a role in the backprojection computation .
second , we multiply all consistent probability rules to form a local set of mutually - exclusive rules .
this procedure is analogous to the addition procedure described in section 123 .
now that we have represented the probabilities that can aect by a mutually - exclusive set , we can simply represent the backprojection of by the product of these probabilities with the value of .
that is , the backprojection of is a rule - based function with one rule for each one of the mutually - exclusive probability rules i .
the context of this new value rule is the same as that of i , and the value is the product of the probability of i and the value of .
example 123 for example , consider the backprojection of a simple rule ,
= ( cid : 123 ) painting = done : 123 ( cid : 123 ) ,
through the cpd in figure 123 ( c ) for the paint action :
ppaint ( x ( cid : 123 ) | x ) ( x ( cid : 123 ) ) ;
efficient solution algorithms for factored mdps
( cid : 123 ) | x ) ( painting
i ( painting = done , x ) .
note that the product of these simple rules is equivalent to the decision tree cpd shown in figure 123 ( a ) .
hence , this product is equal to 123 in most contexts , for example , when electricity is not done at time t .
the product in non - zero only in one context : in the context associated with rule 123
thus , we can express the result of the backprojection operation by a rule - based function with a single rule :
rulebackprojpaint ( ) = ( cid : 123 ) plumbing electrical : 123 ( cid : 123 ) .
similarly , the backprojection of when the action is not paint can also be represented by a
rulebackprojpaint ( ) = ( cid : 123 ) plumbing electrical painting : 123 ( cid : 123 ) .
using this algorithm , we can now write the backprojection of the rule - based basis func -
tion hj as :
j ( x ) =
simplicity of notation , we use ga jection .
using this notation , we can write qa ( x ) = ra ( x ) +
j is a sum of rule - based functions , and therefore also a rule - based function .
for j = rulebackproja ( hj ) to refer to this denition of backpro - j ( x ) , which is again a
123 rule - based maximization over the state space the second key operation required to extend our planning algorithms to exploit csi is to modify the variable elimination algorithm in section 123 . 123 to handle the rule - based rep - in section 123 . 123 , we showed that the maximization of a linear combination of table - based functions with restricted scope can be performed eciently using non - serial dynamic programming ( bertele & brioschi , 123 ) , or variable elimination .
to exploit struc - ture in rules , we use an algorithm similar to variable elimination in a bayesian network with context - specic independence ( zhang & poole , 123 ) .
intuitively , the algorithm operates by selecting the value rules relevant to the variable being maximized in the current iteration .
then , a local maximization is performed over this subset of the rules , generating a new set of rules without the current variable .
the procedure is then repeated recursively until all variables have been eliminated .
more precisely , our algorithm eliminates variables one by one , where the elimina - tion process performs a maximization step over the variables domain .
suppose that we are eliminating xi , whose collected value rules lead to a rule function f , and f involves additional variables in some set b , so that fs scope is b ( xi ) .
we need to compute the maximum value for xi for each choice of b dom ( b ) .
we use maxout ( f , xi ) to de - note a procedure that takes a rule function f ( b , xi ) and returns a rule function g ( b ) such
guestrin , koller , parr & venkataraman
maxout ( f , b ) let g = ( ) .
add completing rules to f : ( cid : 123 ) b = bi : 123 ( cid : 123 ) , i = 123 , .
/ / summing consistent rules : while there are two consistent rules 123 = ( cid : 123 ) c123 : v123 ( cid : 123 ) and 123 = ( cid : 123 ) c123 : v123 ( cid : 123 ) :
if c123 = c123 , then replace these two rules by ( cid : 123 ) c123 : v123 + v123 ( cid : 123 ) ; else replace these two rules by the set : split ( 123 ( cid : 123 ) c123 ) split ( 123 ( cid : 123 ) c123 ) .
/ / maximizing out variable b : repeat until f is empty :
then remove these rules from f and add rule ( cid : 123 ) c : maxi vi ( cid : 123 ) to g;
if there are rules ( cid : 123 ) c b = bi : vi ( cid : 123 ) , bi dom ( b ) : else select two rules : i = ( cid : 123 ) ci b = bi : vi ( cid : 123 ) and j = ( cid : 123 ) cj b = bj : vj ( cid : 123 ) such that ci is consistent with cj , but not identical , and replace them with split ( i ( cid : 123 ) cj ) split ( j ( cid : 123 ) ci ) .
figure 123 : maximizing out variable b from rule function f .
that : g ( b ) = maxxi f ( b , xi ) .
such a procedure is an extension of the variable elimination algorithm of zhang and poole ( zhang & poole , 123 ) .
the rule - based variable elimination algorithm maintains a set f of value rules , initially containing the set of rules to be maximized .
the algorithm then repeats the following steps for each variable xi until all variables have been eliminated :
collect all rules which depend on xi into fi fi = ( ( cid : 123 ) c : v ( cid : 123 ) f | xi c ) and
remove these rules from f .
perform the local maximization step over xi : gi = maxout ( fi , xi ) ; 123
add the rules in gi to f; now , xi has been eliminated .
the cost of this algorithm is polynomial in the number of new rules generated in the maximization operation maxout ( fi , xi ) .
the number of rules is never larger and in many cases exponentially smaller than the complexity bounds on the table - based maximization in section 123 . 123 , which , in turn , was exponential only in the induced width of the cost network graph ( dechter , 123 ) .
however , the computational costs involved in managing sets of rules usually imply that the computational advantage of the rule - based approach over the table - based one will only be signicant in problems that possess a fair amount of context - specic
in the remainder of this section , we present the algorithm for computing the local maximization maxout ( fi , xi ) .
in the next section , we show how these ideas can be applied to extending the algorithm in section 123 . 123 to exploit csi in the lp representation for planning in factored mdps .
the procedure , presented in figure 123 , is divided into two parts : rst , all consistent rules are added together as described in section 123; then , variable b is maximized .
this maximization is performed by generating a set of rules , one for each assignment of b , whose contexts have the same assignment for all variables except for b , as in denition 123 .
this set is then substituted by a single rule without a b assignment in its context and with value equal to the maximum of the values of the rules in the original set .
note that , to simplify
efficient solution algorithms for factored mdps
the algorithm , we initially need to add a set of value rules with 123 value , which guarantee that our rule function f is complete ( i . e . , there is at least one rule consistent with every
the correctness of this procedure follows directly from the correctness of the rule - based variable elimination procedure described by zhang and poole , merely by replacing summa - tions with product with max , and products with products with sums .
we conclude this section with a small example to illustrate the algorithm : example 123 suppose we are maximizing a for the following set of rules :
123 = ( cid : 123 ) a : 123 ( cid : 123 ) , 123 = ( cid : 123 ) a b : 123 ( cid : 123 ) , 123 = ( cid : 123 ) a b c : 123 ( cid : 123 ) , 123 = ( cid : 123 ) a b : 123 ( cid : 123 ) .
when we add completing rules , we get :
123 = ( cid : 123 ) a : 123 ( cid : 123 ) , 123 = ( cid : 123 ) a : 123 ( cid : 123 ) .
in the rst part of the algorithm , we need to add consistent rules : we add 123 to 123 ( which remains unchanged ) , combine 123 with 123 , 123 with 123 , and then the split of 123 on the context of 123 , to get the following inconsistent set of rules :
123 = ( cid : 123 ) a b : 123 ( cid : 123 ) , 123 = ( cid : 123 ) a b c : 123 ( cid : 123 ) , 123 = ( cid : 123 ) a b : 123 ( cid : 123 ) , 123 = ( cid : 123 ) a b : 123 ( cid : 123 ) , 123 = ( cid : 123 ) a b c : 123 ( cid : 123 ) ,
( from adding 123 to the consistent rule from split ( 123 ( cid : 123 ) b ) ) ( from split ( 123 ( cid : 123 ) b ) ) ( from split ( 123 ( cid : 123 ) a b c ) ) .
note that several rules with value 123 are also generated , but not shown here because they are added to other rules with consistent contexts .
we can move to the second stage ( repeat loop ) of maxout .
we remove 123 , and 123 , and maximize a out of them , to give :
123 = ( cid : 123 ) b : 123 ( cid : 123 ) .
we then select rules 123 and 123 and split 123 on c ( 123 is split on the empty set and is not
123 = ( cid : 123 ) a b c : 123 ( cid : 123 ) , 123 = ( cid : 123 ) a b c : 123 ( cid : 123 ) .
maximizing out a from rules 123 and 123 , we get :
123 = ( cid : 123 ) b c : 123 ( cid : 123 ) .
we are left with 123 , which maximized over its counterpart 123 gives
123 = ( cid : 123 ) b c : 123 ( cid : 123 ) .
notice that , throughout this maximization , we have not split on the variable c when b ci , giving us only 123 distinct rules in the nal result .
this is not possible in a table - based representation , since our functions would then be over the 123 variables a , b , c , and therefore must have 123 entries .
guestrin , koller , parr & venkataraman
constraints of the form : ( cid : 123 )
123 rule - based factored lp in section 123 . 123 , we showed that the lps used in our algorithms have exponentially many i wi ci ( x ) b ( x ) , x , which can be substituted by a single , i wi ci ( x ) b ( x ) .
we then showed that , using equivalent , non - linear constraint : maxx variable elimination , we can represent this non - linear constraint by an equivalent set of linear constraints in a construction we called the factored lp .
the number of constraints in the factored lp is linear in the size of the largest table generated in the variable elimination procedure .
this table - based algorithm can only exploit additive independence .
we now extend the algorithm in section 123 . 123 to exploit both additive and context - specic structure , by using the rule - based variable elimination described in the previous section .
suppose we wish to enforce the more general constraint 123 maxy f w ( y ) , where f w ( y ) = j ( y ) such that each fj is a rule .
as in the table - based version , the superscript w means j f w that fj might depend on w .
specically , if fj comes from basis function hi , it is multiplied by the weight wi; if fj is a rule from the reward function , it is not .
in our rule - based factored linear program , we generate lp variables associated with contexts; we call these lp rules .
an lp rule has the form ( cid : 123 ) c : u ( cid : 123 ) ; it is associated with a context c and a variable u in the linear program .
we begin by transforming all our original into lp rules as follows : if rule fj has the form ( cid : 123 ) cj : vj ( cid : 123 ) and comes from basis rules f w function hi , we introduce an lp rule ej = ( cid : 123 ) cj : uj ( cid : 123 ) and the equality constraint uj = wivj .
if fj has the same form but comes from a reward function , we introduce an lp rule of the same form , but the equality constraint becomes uj = vj .
now , we have only lp rules and need to represent the constraint : 123 maxy
to represent such a constraint , we follow an algorithm very similar to the variable elimina - tion procedure in section 123 .
the main dierence occurs in the maxout ( f , b ) operation in figure 123
instead of generating new value rules , we generate new lp rules , with associated new variables and new constraints .
the simplest case occurs when computing a split or adding two lp rules .
for example , when we add two value rules in the original algorithm , we instead perform the following operation on their associated lp rules : if the lp rules are ( cid : 123 ) c : ui ( cid : 123 ) and ( cid : 123 ) c : uj ( cid : 123 ) , we replace these by a new rule ( cid : 123 ) c : uk ( cid : 123 ) , associated with a new lp variable uk with context c , whose value should be ui + uj .
to enforce this value constraint , we simply add an additional constraint to the lp : uk = ui + uj .
a similar procedure can be followed when computing the split .
more interesting constraints are generated when we perform a maximization .
rule - based variable elimination algorithm in figure 123 , this maximization occurs when we replace a set of rules :
( cid : 123 ) c b = bi : vi ( cid : 123 ) , bi dom ( b ) ,
by a new rule
c : max
following the same process as in the lp rule summation above , if we are maximizing
ei = ( cid : 123 ) c b = bi : ui ( cid : 123 ) , bi dom ( b ) ,
we generate a new lp variable uk associated with the rule ek = ( cid : 123 ) c : uk ( cid : 123 ) .
however , we cannot add the nonlinear constraint uk = maxi ui , but we can add a set of equivalent linear
efficient solution algorithms for factored mdps
therefore , using these simple operations , we can exploit structure in the rule functions to represent the nonlinear constraint en maxy j ej ( y ) , where en is the very last lp rule we generate .
a nal constraint un = implies that we are representing exactly the constraints in equation ( 123 ) , without having to enumerate every state .
uk ui , i .
the correctness of our rule - based factored lp construction is a corollary of theorem 123 and of the correctness of the rule - based variable elimination algorithm ( zhang & poole , corollary 123 the constraints generated by the rule - based factored lp construction are equivalent to the non - linear constraint in equation ( 123 ) .
that is , an assignment to ( , w ) satises the rule - based factored lp constraints if and only if it satises the constraint in the number of variables and constraints in the rule - based factored lp is linear in the number of rules generated by the variable elimination process .
in turn , the number of rules is no larger , and often exponentially smaller , than the number of entries in the table - based
to illustrate the generation of lp constraints as just described , we now present a small
example 123 let e123 , e123 , e123 , and e123 be the set of lp rules which depend on the variable b being maximized .
here , rule ei is associated with the lp variable ui :
e123 = ( cid : 123 ) a b : u123 ( cid : 123 ) , e123 = ( cid : 123 ) a b c : u123 ( cid : 123 ) , e123 = ( cid : 123 ) a b : u123 ( cid : 123 ) , e123 = ( cid : 123 ) a b c : u123 ( cid : 123 ) .
in this set , note that rules e123 and e123 are consistent .
we combine them to generate the
e123 = ( cid : 123 ) a b c : u123 ( cid : 123 ) , e123 = ( cid : 123 ) a b c : u123 ( cid : 123 ) .
and the constraint u123 + u123 = u123
similarly , e123 and e123 may be combined , resulting in :
e123 = ( cid : 123 ) a b c : u123 ( cid : 123 ) .
with the constraint u123 = u123 + u123
now , we have the following three inconsistent rules for
e123 = ( cid : 123 ) a b : u123 ( cid : 123 ) , e123 = ( cid : 123 ) a b c : u123 ( cid : 123 ) , e123 = ( cid : 123 ) a b c : u123 ( cid : 123 ) .
following the maximization procedure , since no pair of rules can be eliminated right away , we split e123 and e123 to generate the following rules :
e123 = ( cid : 123 ) a b c : u123 ( cid : 123 ) , e123 = ( cid : 123 ) a b c : u123 ( cid : 123 ) , e123 = ( cid : 123 ) a b c : u123 ( cid : 123 ) .
guestrin , koller , parr & venkataraman
we can now maximize b out from e123 and e123 , resulting in the following rule and constraints
e123 = ( cid : 123 ) a c : u123 ( cid : 123 ) ,
likewise , maximizing b out from e123 and e123 , we get :
e123 = ( cid : 123 ) a c : u123 ( cid : 123 ) ,
which completes the elimination of variable b in our rule - based factored lp .
we have presented an algorithm for exploiting both additive and context - specic struc - ture in the lp construction steps of our planning algorithms .
this rule - based factored lp approach can now be applied directly in our approximate linear programming and approx - imate policy iteration algorithms , which were presented in sections 123 and 123
the only additional modication required concerns the manipulation of the decision list policies presented in section 123 .
although approximate linear programming does not require any explicit policy representation ( or the default action model ) , approximate pol - icy iteration require us to represent such policy .
fortunately , no major modications are in particular , the conditionals ( cid : 123 ) ti , ai , i ( cid : 123 ) in the decision required in the rule - based case .
list policies are already context - specic rules .
thus , the policy representation algorithm in section 123 can be applied directly with our new rule - based representation .
therefore , we now have a complete framework for exploiting both additive and context - specic structure for ecient planning in factored mdps .
experimental results
the factored representation of a value function is most appropriate in certain types of systems : systems that involve many variables , but where the strong interactions between the variables are fairly sparse , so that the decoupling of the inuence between variables does not induce an unacceptable loss in accuracy .
as argued by herbert simon ( 123 ) in architecture of complexity , many complex systems have a nearly decomposable , hierarchical structure , with the subsystems interacting only weakly between themselves .
to evaluate our algorithm , we selected problems that we believe exhibit this type of structure .
in this section , we perform various experiments intended to explore the performance of our algorithms .
first , we compare our factored approximate linear programming ( lp ) and approximate policy iteration ( pi ) algorithms .
we also compare to the l123 - projection algorithm of koller and parr ( 123 ) .
our second evaluation compares a table - based im - plementation to a rule - based implementation that can exploit csi .
finally , we present comparisons between our approach and the algorithms of boutilier et al .
( 123 ) .
123 approximate lp and approximate pi in order to compare our approximate lp and approximate pi algorithms , we tested both on the sysadmin problem described in detail in section 123 .
this problem relates to a system
efficient solution algorithms for factored mdps
administrator who has to maintain a network of computers; we experimented with various network architectures , shown in figure 123
machines fail randomly , and a faulty machine increases the probability that its neighboring machines will fail .
at every time step , the sysadmin can go to one machine and reboot it , causing it to be working in the next time step with high probability .
recall that the state space in this problem grows exponentially in the number of machines in the network , that is , a problem with m machines has 123m states .
each machine receives a reward of 123 when working ( except in the ring , where one machine receives a reward of 123 , to introduce some asymmetry ) , a zero reward is given to faulty machines , and the discount factor is = 123 .
the optimal strategy for rebooting machines will depend upon the topology , the discount factor , and the status of the machines in the network .
if machine i and machine j are both faulty , the benet of rebooting i must be weighed against the expected discounted impact of delaying rebooting j on js successors .
for topologies such as rings , this policy may be a function of the status of every single machine in the network .
the basis functions used included independent indicators for each machine , with value 123 if it is working and zero otherwise ( i . e . , each one is a restricted scope function of a single variable ) , and the constant basis , whose value is 123 for all states .
we selected straightforward variable elimination orders : for the star and three legs topologies , we rst eliminated the variables corresponding to computers in the legs , and the center computer ( server ) was eliminated last; for ring , we started with an arbitrary computer and followed the ring order; for ring and star , the ring machines were eliminated rst and then the center one; nally , for the ring of rings topology , we eliminated the computers in the outer rings rst and then the ones in the inner ring .
we implemented the factored policy iteration and linear programming algorithms in matlab , using cplex as the lp solver .
experiments were performed on a sun ultrasparc - ii , 123 mhz with 123mb of ram .
to evaluate the complexity of the approximate policy iteration with max - norm projection algorithm , tests were performed with increasing the number of states , that is , increasing number of machines on the network .
figure 123 shows the running time for increasing problem sizes , for various architectures .
the simplest one is the star , where the backprojection of each basis function has scope restricted to two variables and the largest factor in the cost network has scope restricted to two variables .
the most dicult one was the bidirectional ring , where factors contain ve variables .
note that the number of states is growing exponentially ( indicated by the log scale in figure 123 ) , but running times increase only logarithmically in the number of states , or polynomially in the number of variables .
we illustrate this behavior in figure 123 ( d ) , where we t a 123rd order polynomial to the running times for the unidirectional ring .
note that the size of the problem description grows quadratically with the number of variables : adding a machine to the network also adds the possible action of xing that machine .
for this problem , the computation cost of our factored algorithm empirically grows approximately , for a problem with n variables , as opposed to the exponential complexity poly ( 123n , |a| ) of the explicit algorithm .
for further evaluation , we measured the error in our approximate value function relative to the true optimal value function v .
note that it is only possible to compute v for small problems; in our case , we were only able to go up to 123 machines .
for comparison , we also evaluated the error in the approximate value function produced by the l123 - projection
guestrin , koller , parr & venkataraman
figure 123 : ( a ) ( c ) running times for policy iteration with max - norm projection on variants of the sysadmin problem; ( d ) fitting a polynomial to the running time for the
algorithm of koller and parr ( 123 ) .
as we discussed in section 123 , the l123 projections in factored mdps by koller and parr are dicult and time consuming; hence , we were only able to compare the two algorithms for smaller problems , where an equivalent l123 - projection can be implemented using an explicit state space formulation .
results for both algorithms are presented in figure 123 ( a ) , showing the relative error of the approximate solutions to the true value function for increasing problem sizes .
the results indicate that , for larger problems , the max - norm formulation generates a better approximation of the true optimal value function v than the l123 - projection .
here , we used two types of basis functions : the same single variable functions , and pairwise basis functions .
the pairwise basis functions contain indicators for neighboring pairs of machines ( i . e . , functions of two variables ) .
as expected , the use of pairwise basis functions resulted in better approximations .
123e+123e+123e+123e+123e+123e+123e+123e+123number of statestotal time ( minutes ) ring123 legsstar123e+123number of statestotal time ( minutes ) ring of ringsring and star123e+123e+123e+123e+123e+123e+123e+123e+123number of statestotal time ( minutes ) unidirectionalbidirectionalring : fitting a polynomial : time = 123|x|123 - 123|x|123 + 123|x| - 123quality of the fit : r123 = 123number of variables |x|total time ( minutes ) efficient solution algorithms for factored mdps
figure 123 : ( a ) relative error to optimal value function v and comparison to l123 projection for ring; ( b ) for large models , measuring bellman error after convergence .
for these small problems , we can also compare the actual value of the policy generated by our algorithm to the value of the optimal policy .
here , the value of the policy generated by our algorithm is much closer to the value of the optimal policy than the error implied by the dierence between our approximate value function and v .
for example , for the star architecture with one server and up to 123 clients , our approximation with single variable basis functions had relative error of 123% , but the policy we generated had the same value as the optimal policy .
in this case , the same was true for the policy generated by the l123 in a unidirectional ring with 123 machines and pairwise basis , the relative error between our approximation and v was about 123% , but the resulting policy only had a 123% loss over the optimal policy .
for the same problem , the l123 approximation has a value function error of 123% , and a true policy loss was 123% .
in other words , both methods induce policies that have lower errors than the errors in the approximate value function ( at least for small problems ) .
however , our algorithm continues to outperform the l123 algorithm , even with respect to actual policy loss .
for large models , we can no longer compute the correct value function , so we cannot evaluate our results by computing ( cid : 123 ) v hw ( cid : 123 ) .
fortunately , as discussed in section 123 , the bellman error can be used to provide a bound on the approximation error and can be computed eciently by exploiting problem - specic structure .
figure 123 ( b ) shows that the bellman error increases very slowly with the number of states .
it is also valuable to look at the actual decision - list policies generated in our experiments .
first , we noted that the lists tended to be short , the length of the nal decision list policy grew approximately linearly with the number of machines .
furthermore , the policy itself is often fairly intuitive .
in the ring and star architecture , for example , the decision list says : if the server is faulty , x the server; else , if another machine is faulty , x it .
thus far , we have presented scaling results for running times and approximation error for our approximate pi approach .
we now compare this algorithm to the simpler approximate
123 . 123number of variablesrelative error : max norm , single basisl123 , single basismax norm , pair basisl123 , pair basis123 . 123 . 123e+123e+123e+123e+123e+123e+123e+123e+123number of statesbellman error / rmaxring123 legsstar guestrin , koller , parr & venkataraman
figure 123 : approximate lp versus approximate pi on the sysadmin problem with a ring
topology : ( a ) running time; ( b ) estimated value of policy .
lp approach of section 123
as shown in figure 123 ( a ) , the approximate lp algorithm for factored mdps is signicantly faster than the approximate pi algorithm .
in fact , approxi - mate pi with single - variable basis functions variables is more costly computationally than the lp approach using basis functions over consecutive triples of variables .
as shown in figure 123 ( b ) , for singleton basis functions , the approximate pi policy obtains slightly better performance for some problem sizes .
however , as we increase the number of basis functions for the approximate lp formulation , the value of the resulting policy is much better .
thus , in this problem , our factored approximate linear programming formulation allows us to use more basis functions and to obtain a resulting policy of higher value , while still maintaining a faster running time .
these results , along with the simpler implementation , suggest that in practice one may rst try to apply the approximate linear programming algorithm before deciding to move to the more elaborate approximate policy iteration approach .
123 comparing table - based and rule - based implementations
our next evaluation compares a table - based representation , which exploits only additive independence , to the rule - based representation presented in section 123 , which can exploit both additive and context - specic independence .
for these experiments , we implemented our factored approximate linear programming algorithm with table - based and rule - based representations in c++ , using cplex as the lp solver .
experiments were performed on a sun ultrasparc - ii , 123 mhz with 123gb of ram .
to evaluate and compare the algorithms , we utilized a more complex extension of the sysadmin problem .
this problem , dubbed the process - sysadmin problem , contains three state variables for each machine i in the network : loadi , statusi and selectori .
each com - puter runs processes and receives rewards when the processes terminate .
these processes are represented by the loadi variable , which takes values in ( idle , loaded , success ) , and the computer receives a reward when the assignment of loadi is success .
the statusi variable ,
123number of machinestotal running time ( minutes ) pi single basislp single basislp pair basislp triple basis123number of machinesdiscounted reward of final policy ( averaged over 123 trials of 123 steps ) pi single basislp single basislp pair basislp triple basis efficient solution algorithms for factored mdps
figure 123 : running time for process - sysadmin problem for various topologies : ( a ) star;
( b ) ring; ( c ) reverse star ( with t function ) .
123e+123e+123e+123e+123e+123e+123e+123number of statestotal running time ( minutes ) table - based , single+ basisrule - based , single+ basistable - based , pair basisrule - based , pair basis123e+123e+123e+123e+123e+123e+123e+123e+123number of statestotal running time ( minutes ) table - based , single+ basisrule - based , single+ basistable - based , pair basisrule - based , pair basis123number of machinestotal running time ( minutes ) table - based , single+ basisrule - based , single+ basisy = 123x - 123x + 123x - 123r = 123y = 123e - 123 x * 123 + 123e - 123 x * 123 + 123r = 123 ( x - 123 ) 123 ( x - 123 ) 123 guestrin , koller , parr & venkataraman
figure 123 : fraction of total running time spent in cplex for the process - sysadmin prob -
lem with a ring topology .
representing the status of machine i , takes values in ( good , faulty , dead ) ; if its value is faulty , then processes have a smaller probability of terminating and if its value is dead , then any running process is lost and loadi becomes idle .
the status of machine i can be - come faulty and eventually dead at random; however , if machine i receives a packet from a dead machine , then the probability that statusi becomes faulty and then dead increases .
the selectori variable represents this communication by selecting one of the neighbors of i uniformly at random at every time step .
the sysadmin can select at most one computer to reboot at every time step .
if computer i is rebooted , then its status becomes good with probability 123 , but any running process is lost , i . e . , the loadi variable becomes idle .
thus , in this problem , the sysadmin must balance several conicting goals : rebooting a machine kills processes , but not rebooting a machine may cause cascading faults in network .
furthermore , the sysadmin can only choose one machine to reboot , which imposes the ad - ditional tradeo of selecting only one of the ( potentially many ) faulty or dead machines in the network to reboot .
we experimented with two types of basis functions : single+ includes indicators over all of the joint assignments of loadi , statusi and selectori , and pair which , in addition , includes a set of indicators over statusi , statusj , and selectori = j , for each neighbor j of machine i in the network .
the discount factor was = 123 .
the variable elimination order eliminated all of the loadi variables rst , and then followed the same patterns as in the simple sysadmin problem , eliminating rst statusi and then selectori when machine i
figure 123 compares the running times for the table - based implementation to the ones for the rule - based representation for three topologies : star , ring , and reverse star .
the reverse star topology reverses the direction of the inuences in the star : rather than the central machine inuencing all machines in the topology , all machines inuence the central one .
these three topologies demonstrate three dierent levels of csi : in the
123 . 123 . 123number of machinescplex time / total timetable - based , single+ basisrule - based , single+ basis efficient solution algorithms for factored mdps
star topology , the factors generated by variable elimination are small .
thus , although the running times are polynomial in the number of state variables for both methods , the table - based representation is signicantly faster than the rule - based one , due to the overhead of managing the rules .
the ring topology illustrates an intermediate behavior : single+ basis functions induce relatively small variable elimination factors , thus the table - based approach is faster .
however , with pair basis the factors are larger and the rule - based approach starts to demonstrate faster running times in larger problems .
finally , the re - verse star topology represents the worst - case scenario for the table - based approach .
here , the scope of the backprojection of a basis function for the central machine will involve all computers in the network , as all machines can potentially inuence the central one in the next time step .
thus , the size of the factors in the table - based variable elimination ap - proach are exponential in the number of machines in the network , which is illustrated by the exponential growth in figure 123 ( c ) .
the rule - based approach can exploit the csi in this problem; for example , the status of the central machine status123 only depends on machine j if the value selector is j , i . e . , if selector123 = j .
by exploiting csi , we can solve the same problem in polynomial time in the number of state variables , as seen in the second curve in
it is also instructive to compare the portion of the total running time spent in cplex for the table - based as compared to the rule - based approach .
figure 123 illustrates this comparison .
note that amount of time spent in cplex is signicantly higher for the table - based approach .
there are two reasons for this dierence : rst , due to csi , the lps generated by the rule - based approach are smaller than the table - based ones; second , rule - based variable elimination is more complex than the table - based one , due to the overhead introduced by rule management .
interestingly , the proportion of cplex time increases as the problem size increases , indicating that the asymptotic complexity of the lp solution is higher than that of variable elimination , thus suggesting that , for larger problems , additional large - scale lp optimization procedures , such as constraint generation , may be helpful .
123 comparison to apricodd
the most closely related work to ours is a line of research that began with the work of boutilier et al .
( 123 ) .
in particular , the approximate apricodd algorithm of hoey et al .
( 123 ) , which uses analytic decision diagrams ( adds ) to represent the value function is a strong alternative approach for solving factored mdps .
as discussed in detail in sec - tion 123 , the apricodd algorithm can successfully exploit context - specic structure in the value function , by representing it with the set of mutually - exclusive and exhaustive branches of the add .
on the other hand , our approach can exploit both additive and context - specic structure in the problem , by using a linear combination of non - mutually - exclusive rules .
to better understand this dierence , we evaluated both our rule - based approximate linear programming algorithm and apricodd in two problems , linear and expon , designed by boutilier et al .
( 123 ) to illustrate respectively the best - case and the worst - case behavior of their algorithm .
in these experiments , we used the web - distributed version of apri - codd ( hoey , st - aubin , hu , & boutilier , 123 ) , running it locally on a linux pentium iii 123mhz with 123gb of ram .
guestrin , koller , parr & venkataraman
figure 123 : comparing apricodd to rule - based approximate linear programming on the ( a )
linear and ( b ) expon problems .
these two problems involve n binary variables x123 , .
, xn and n deterministic actions a123 , .
the reward is 123 when all variables xk are true , and is 123 otherwise .
the problem is discounted by a factor = 123 .
the dierence between the linear and the expon problems is in the transition probabilities .
in the linear problem , the action ak sets the variable xk to true and makes all succeeding variables , xi for i > k , false .
if the state space of the linear problem is seen as a binary number , the optimal policy is to set repeatedly the largest bit ( xk variable ) which has all preceding bits set to true .
using an add , the optimal value function for this problem can be represented in linear space , with n+123 leaves ( boutilier et al . , 123 ) .
this is the best - case for apricodd , and the algorithm can compute this value function quite eciently .
figure 123 ( a ) compares the running time of apricodd to that of one of our algorithms with indicator basis functions between pairs of consecutive variables .
note that both algorithms obtain the same policy in polynomial time in the number of variables .
however , in such structured problems , the ecient implementation of the add package used in apricodd makes it faster in this problem .
on the other hand , the expon problem illustrates the worst - case for apricodd .
in this problem , the action ak sets the variable xk to true , if all preceding variables , xi for i < k , are true , and it makes all preceding variables false .
if the state space is seen as a binary number , the optimal policy goes through all binary numbers in sequence , by repeatedly setting the largest bit ( xk variable ) which has all preceding bits set to true .
due to discounting , the optimal value function assigns a value of 123nj123 to the jth binary number , so that the value function contains exponentially many dierent values .
using an add , the optimal value function for this problem requires an exponential number of leaves ( boutilier et al . , 123 ) , which is illustrated by the exponential running time in figure 123 ( b ) .
however , the same value function can be approximated very compactly as a factored linear value function using n + 123 basis functions : an indicator over each variable xk and the constant base .
as shown in figure 123 ( b ) , using this representation , our factored approximate linear programming algorithm computes the value function in polynomial time .
furthermore , the
y = 123x123 - 123x123 + 123x - 123r123 = 123y = 123x123 + 123x + 123r123 = 123number of variablestime ( in seconds ) apricoddrule - basedy = 123x123 - 123x123 + 123x - 123r123 = 123number of variables time ( in seconds ) apricoddrule - basedy = 123e - 123 * 123 - 123 * 123 + 123r123 = 123xx123 efficient solution algorithms for factored mdps
figure 123 : comparing apricodd to rule - based approximate linear programming with sin - gle+ basis functions on the process - sysadmin problem with ring topology ( a ) running time and ( b ) value of the resulting policy; and with star topology ( c ) running time and ( d ) value of the resulting policy .
policy obtained by our approach was optimal for this problem .
thus , in this problem , the ability to exploit additive independence allows an ecient polynomial time solution .
we have also compared apricodd to our rule - based approximate linear programming algorithm on the process - sysadmin problem .
this problem has signicant additive struc - ture in the reward function and factorization in the transition model .
although this type of structure is not exploited directly by apricodd , the add approximation steps performed by the algorithm can , in principle , allow apricodd to nd approximate solutions to the prob - lem .
we spent a signicant amount of time attempting to nd the best set of parameters for apricodd for these problems . 123 we settled on the sift method of variable reordering and the round approximation method with the size ( maximum add size ) criteria
we are very grateful to jesse hoey and robert st - aubin for their assistance in selecting the parameters .
123number of machinesrunning time ( minutes ) rule - based lpapricodd123number of machinesdiscounted value of policy ( avg .
123 runs of 123 steps ) rule - based lpapricodd123number of machinesrunning time ( minutes ) rule - based lpapricodd123number of machinesdiscounted value of policy ( avg .
123 runs of 123 steps ) rule - based lpapricodd guestrin , koller , parr & venkataraman
allow the value function representation to scale with the problem size , we set the maximum add size to 123 + 123n for a network with n machines .
( we experimented with a variety of dierent growth rates for the maximum add size; here , as for the other parameters , we selected the choice that gave the best results for apricodd . ) we compared apricodd with these parameters to our rule - based approximate linear programming algorithm with single+ basis functions on a pentium iii 123mhz with 123gb of ram .
these results are summarized in figure 123
on very small problems ( up to 123 machines ) , the performance of the two algorithms is fairly similar in terms of both the running time and the quality of the policies generated .
however , as the problem size grows , the running time of apricodd increases rapidly , and becomes signicantly higher than that of our algorithm .
furthermore , as the problem size increases , the quality of the policies generated by apricodd also deteriorates .
this dierence in policy quality is caused by the dierent value function representation used by the two algorithms .
the adds used in apricodd represent k dierent values with k leaves; thus , they are forced to agglomerate many dierent states and represent them using a single value .
for smaller problems , such agglomeration can still represent good policies .
unfortunately , as the problem size increases and the state space grows exponentially , apricodds policy representation becomes inadequate , and the quality of the policies decreases .
on the other hand , our linear value functions can represent exponentially many values with only k basis functions , which allows our approach to scale up to signicantly larger problems .
related work
