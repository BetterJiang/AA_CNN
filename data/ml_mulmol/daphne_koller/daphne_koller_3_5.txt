we present a novel discriminative approach to parsing inspired by the large - margin criterion underlying sup - port vector machines .
our formulation uses a factor - ization analogous to the standard dynamic programs for parsing .
in particular , it allows one to e ( cid : 123 ) ciently learn a model which discriminates among the entire space of parse trees , as opposed to reranking the top few candi - dates .
our models can condition on arbitrary features of input sentences , thus incorporating an important kind of lexical information without the added algorithmic com - plexity of modeling headedness .
we provide an e ( cid : 123 ) cient algorithm for learning such models and show experimen - tal evidence of the models improved performance over a natural baseline model and a lexicalized probabilistic
recent work has shown that discriminative techniques frequently achieve classi ( cid : 123 ) cation ac - curacy that is superior to generative techniques , over a wide range of tasks .
the empirical utility of models such as logistic regression and sup - port vector machines ( svms ) in ( cid : 123 ) at classi ( cid : 123 ) ca - tion tasks like text categorization , word - sense disambiguation , and relevance routing has been repeatedly demonstrated .
for sequence tasks like part - of - speech tagging or named - entity ex - traction , recent top - performing systems have also generally been based on discriminative se - quence models , like conditional markov mod - els ( toutanova et al . , 123 ) or conditional ran - dom ( cid : 123 ) elds ( la ( cid : 123 ) erty et al . , 123 ) .
a number of recent papers have consid - ered discriminative approaches for natural lan - guage parsing ( johnson et al . , 123; collins , 123; johnson , 123; geman and johnson ,
123; miyao and tsujii , 123; clark and cur - ran , 123; kaplan et al . , 123; collins , 123 ) .
broadly speaking , these approaches fall into two categories , reranking and dynamic programming in reranking methods ( johnson et al . , 123; collins , 123; shen et al . , 123 ) , an initial parser is used to generate a number of candidate parses .
a discriminative model is then used to choose between these candi - in dynamic programming methods , a large number of candidate parse trees are repre - sented compactly in a parse tree forest or chart .
given su ( cid : 123 ) ciently \local " features , the decod - ing and parameter estimation problems can be solved using dynamic programming algorithms .
for example , ( johnson , 123; geman and john - son , 123; miyao and tsujii , 123; clark and curran , 123; kaplan et al . , 123 ) describe ap - proaches based on conditional log - linear ( max - imum entropy ) models , where variants of the inside - outside algorithm can be used to e ( cid : 123 ) - ciently calculate gradients of the log - likelihood function , despite the exponential number of trees represented by the parse forest .
in this paper , we describe a dynamic pro - gramming approach to discriminative parsing that is an alternative to maximum entropy estimation .
our method extends the max - margin approach of taskar et al .
the case of context - free grammars .
the present method has several compelling advantages .
un - like reranking methods , which consider only a pre - pruned selection of \good " parses , our method is an end - to - end discriminative model over the full space of parses .
this distinction can be very signi ( cid : 123 ) cant , as the set of n - best parses often does not contain the true parse
example , in the work of collins ( 123 ) , 123% of the correct parses were not in the candidate pool of ( cid : 123 ) 123 - best parses .
unlike previous dynamic programming approaches , which were based on maximum entropy estimation , our method in - corporates an articulated loss function which penalizes larger tree discrepancies more severely than smaller ones . 123
moreover , like perceptron - based learning , it requires only the calculation of viterbi trees , rather than expectations over all trees ( for ex - ample using the inside - outside algorithm ) .
practice , it converges in many fewer iterations than crf - like approaches .
for example , while our approach generally converged in 123 - 123 iter - ations , clark and curran ( 123 ) report exper - iments involving 123 iterations of training for one model , and 123 iterations for another .
the primary contribution of this paper is the extension of the max - margin approach of taskar ( 123 ) to context free grammars .
we show that this framework allows high - accuracy parsing in cubic time by exploiting novel kinds of lexical information .
123 discriminative parsing
in the discriminative parsing task , we want to learn a function f : x ! y , where x is a set of sentences , and y is a set of valid parse trees according to a ( cid : 123 ) xed grammar g .
g maps an input x 123 x to a set of candidate parses g ( x ) ( cid : 123 )
we assume a loss function l : x ( cid : 123 ) y ( cid : 123 ) y ! r+ .
the function l ( x; y; ^y ) measures the penalty for proposing the parse ^y for x when y is the true parse .
this penalty may be de ( cid : 123 ) ned , for example , as the number of labeled spans on which the two trees do not agree .
in general we assume that l ( x; y; ^y ) = 123 for y = ^y .
given labeled training examples ( xi; yi ) for i = 123 : : : n , we seek a function f with small expected loss on unseen sentences .
the functions we consider take the following
linear discriminant form :
fw ( x ) = arg max
hw; ( cid : 123 ) ( x; y ) i;
where h ( cid : 123 ) ; ( cid : 123 ) i denotes the vector inner product , w 123 rd and ( cid : 123 ) is a feature - vector representation of a parse tree ( cid : 123 ) : x ( cid : 123 ) y ! rd ( see examples
note that this class of
viterbi pcfg parsers , where the feature - vector consists of the counts of the productions used in the parse , and the parameters w are the log - probabilities of those productions .
123 probabilistic estimation
the traditional method of estimating the pa - rameters of pcfgs assumes a generative gram - mar that de ( cid : 123 ) nes p ( x; y ) and maximizes the joint log - likelihood pi log p ( xi; yi ) ( with some a alternative probabilistic approach is to estimate the parameters dis - criminatively by maximizing conditional likelihood .
for example , the maximum entropy approach ( johnson , 123 ) de ( cid : 123 ) nes a conditional
pw ( y j x ) =
expfhw; ( cid : 123 ) ( x; y ) ig;
where zw ( x ) = py123g ( x ) expfhw; ( cid : 123 ) ( x; y ) ig , and maximizes the conditional log - likelihood of the sample , pi log p ( yi j xi ) , ( with some regular -
123 max - margin estimation
in this paper , we advocate a di ( cid : 123 ) erent estima - tion criterion , inspired by the max - margin prin - ciple of svms .
max - margin estimation has been used for parse reranking ( collins , 123 ) .
re - cently , it has also been extended to graphical models ( taskar et al . , 123; altun et al . , 123 ) and shown to outperform the standard max - likelihood methods .
the main idea is to forego the probabilistic interpretation , and directly en -
yi = arg max
hw; ( cid : 123 ) ( xi; y ) i;
for all i in the training data .
we de ( cid : 123 ) ne the margin of the parameters w on the example i and parse y as the di ( cid : 123 ) erence in value between the true parse yi and y :
123this articulated loss is supported by empirical suc - cess and theoretical generalization bound in taskar et al .
123for all x , we assume here that g ( x ) is ( cid : 123 ) nite .
the space of parse trees over many grammars is naturally in - ( cid : 123 ) nite , but can be made ( cid : 123 ) nite if we disallow unary chains and empty productions .
hw; ( cid : 123 ) ( xi; yi ) i ( cid : 123 ) hw; ( cid : 123 ) ( xi; y ) i = hw; ( cid : 123 ) i;yi ( cid : 123 ) ( cid : 123 ) i;yi;
123note that in the case that two members y123 and y123 have the same tied value for hw; ( cid : 123 ) ( x; y ) i , we assume that there is some ( cid : 123 ) xed , deterministic way for breaking ties .
for example , one approach would be to assume some default ordering on the members of y .
where ( cid : 123 ) i;y = ( cid : 123 ) ( xi; y ) , and ( cid : 123 ) i;yi = ( cid : 123 ) ( xi; yi ) .
in - tuitively , the size of the margin quanti ( cid : 123 ) es the con ( cid : 123 ) dence in rejecting the mistaken parse y us - ing the function fw ( x ) , modulo the scale of the parameters jjwjj .
we would like this rejection con ( cid : 123 ) dence to be larger when the mistake y is more severe , i . e .
l ( xi; yi; y ) is large .
we can ex - press this desideratum as an optimization prob -
s : t : hw; ( cid : 123 ) i;yi ( cid : 123 ) ( cid : 123 ) i;yi ( cid : 123 ) ( cid : 123 ) li;y 123i; y 123 g ( xi ) ;
jjwjj123 ( cid : 123 ) 123;
where li;y = l ( xi; yi; y ) .
this quadratic pro - gram aims to separate each y 123 g ( xi ) from the target parse yi by a margin that is propor - tional to the loss l ( xi; yi; y ) .
after a standard transformation , in which maximizing the mar - gin is reformulated as minimizing the scale of the weights ( for a ( cid : 123 ) xed margin of 123 ) , we get the
kwk123 + c x
s : t : hw; ( cid : 123 ) i;yi ( cid : 123 ) ( cid : 123 ) i;yi ( cid : 123 ) li;y ( cid : 123 ) ( cid : 123 ) i 123i; y 123 g ( xi ) :
the addition of non - negative slack variables ( cid : 123 ) i allows one to increase the global margin by pay - ing a local penalty on some outlying examples .
the constant c dictates the desired trade - o ( cid : 123 ) between margin size and outliers .
note that this formulation has an exponential number of con - straints , one for each possible parse y for each sentence i .
we address this issue in section 123
123 the max - margin dual in svms , the optimization problem is solved by working with the dual of a quadratic program analogous to eq .
for our problem , just as for svms , the dual has important computational advantages , including the \kernel trick , " which allows the e ( cid : 123 ) cient use of high - dimensional fea - tures spaces endowed with e ( cid : 123 ) cient dot products ( cristianini and shawe - taylor , 123 ) .
more - over , the dual view plays a crucial role in cir - cumventing the exponential size of the primal
123 , there is a constraint for each mistake y one might make on each example i , which rules out that mistake .
for each mistake - exclusion constraint , the dual contains a variable ( cid : 123 ) i;y .
in - tuitively , the magnitude of ( cid : 123 ) i;y is proportional to the attention we must pay to that mistake in order not to make it .
the dual of eq .
123 ( after renormalizing by c )
is given by :
max c x
( ii;y ( cid : 123 ) ( cid : 123 ) i;y ) ( cid : 123 ) i;y
( cid : 123 ) i;y = 123;
123i; ( cid : 123 ) i;y ( cid : 123 ) 123; 123i; y;
where ii;y = i ( xi; yi; y ) indicates whether y is the true parse yi .
given the dual solution ( cid : 123 ) ( cid : 123 ) , the solution to the primal problem w ( cid : 123 ) is sim - ply a weighted linear combination of the feature vectors of the correct parse and mistaken parses :
w ( cid : 123 ) = c x
( ii;y ( cid : 123 ) ( cid : 123 ) ( cid : 123 )
this is the precise sense in which mistakes with large ( cid : 123 ) contribute more strongly to the model .
123 factored models
there is a major problem with both the pri - mal and the dual formulations above : since each potential mistake must be ruled out , the num - ber of variables or constraints is proportional to jg ( x ) j , the number of possible parse trees .
even in grammars without unary chains or empty el - ements , the number of parses is generally ex - ponential in the length of the sentence , so we cannot expect to solve the above problem with - out any assumptions about the feature - vector representation ( cid : 123 ) and loss function l .
for that matter ,
for arbitrary representa - tions , to ( cid : 123 ) nd the best parse given a weight vec - tor , we would have no choice but to enumerate all trees and score them .
however , our gram - mars and representations are generally struc - tured to enable e ( cid : 123 ) cient inference .
for exam - ple , we usually assign scores to local parts of the parse such as pcfg productions .
such factored models have shared substructure prop - erties which permit dynamic programming de - compositions .
in this section , we describe how this kind of decomposition can be done over the dual ( cid : 123 ) distributions .
the idea of this decom - position has previously been used for sequences and other markov random ( cid : 123 ) elds in taskar et al .
( 123 ) , but the present extension to cfgs
for clarity of presentation , we restrict the grammar to be in chomsky normal form ( cnf ) , where all rules in the grammar are of the form ha ! b ci or ha ! ai , where a; b and c are non - terminal symbols , and a is some terminal
q = hs ! np vp; 123; 123; 123i
r = hnp; 123; 123i
123 123 123 123 123
figure 123 : two representations of a binary parse tree : ( a ) nested tree structure , and ( b ) grid of labeled spans .
symbol .
for example ( cid : 123 ) gure 123 ( a ) shows a tree in this form .
we will represent each parse as a set of two types of parts .
parts of the ( cid : 123 ) rst type are sin - gle constituent tuples ha; s; e; ii , consisting of a non - terminal a , start - point s and end - point e , and sentence i , such as r in ( cid : 123 ) gure 123 ( b ) .
in this representation , indices s and e refer to po - sitions between words , rather than to words themselves .
these parts correspond to the tra - ditional notion of an edge in a tabular parser .
parts of the second type consist of cf - rule - tuples ha ! b c; s; m; e; ii .
the tuple speci ( cid : 123 ) es a particular rule a ! b c , and its position , including split point m , within the sentence i , such as q in ( cid : 123 ) gure 123 ( b ) , and corresponds to the traditional notion of a traversal in a tabular parser .
note that parts for a basic pcfg model are not just rewrites ( which can occur multiple times ) , but rather anchored items .
formally , we assume some countable set of parts , r .
we also assume a function r which maps each object ( x; y ) 123 x ( cid : 123 ) y to a ( cid : 123 ) nite subset of r .
thus r ( x; y ) is the set of parts be - longing to a particular parse .
equivalently , the function r ( x; y ) maps a derivation y to the set of parts which it includes .
because all rules are in binary - branching form , jr ( x; y ) j is constant across di ( cid : 123 ) erent derivations y for the same input sentence x .
we assume that the feature vector for a sentence and parse tree ( x; y ) decomposes
into a sum of the feature vectors for its parts :
( cid : 123 ) ( x; y ) = x
in cfgs , the function ( cid : 123 ) ( x; r ) can be any func - tion mapping a rule production and its posi - tion in the sentence x , to some feature vector representation .
for example , ( cid : 123 ) could include features which identify the rule used in the pro - duction , or features which track the rule iden - tity together with features of the words at po - sitions s; m; e , and neighboring positions in the
in addition , we assume that the loss function l ( x; y; ^y ) also decomposes into a sum of local loss functions l ( x; y; r ) over parts , as follows :
l ( x; y; ^y ) = x
l ( x; y; r ) :
one approach would be to de ( cid : 123 ) ne l ( x; y; r ) to be 123 only if the non - terminal a spans words s : : : e in the derivation y and 123 otherwise .
this would lead to l ( x; y; ^y ) tracking the number of \constituent errors " in ^y , where a constituent is a tuple such as ha; s; e; ii .
another , more strict de ( cid : 123 ) nition would be to de ( cid : 123 ) ne l ( x; y; r ) to be 123 if r of the type ha ! b c; s; m; e; ii is in the derivation y and 123 otherwise .
this de ( cid : 123 ) nition would lead to l ( x; y; ^y ) being the number of cf -
rule - tuples in ^y which are not seen in y . 123
finally , we de ( cid : 123 ) ne indicator variables i ( x; y; r ) which are 123 if r 123 r ( x; y ) , 123 otherwise .
we also de ( cid : 123 ) ne sets r ( xi ) = ( y123g ( xi ) r ( xi; y ) for the training examples i = 123 : : : n .
thus , r ( xi ) is the set of parts that is seen in at least one of the objects f ( xi; y ) : y 123 g ( xi ) g .
123 factored dual
the dual in eq .
123 involves variables ( cid : 123 ) i;y for all i = 123 : : : n , y 123 g ( xi ) , and the objec - tive is quadratic in these ( cid : 123 ) variables .
in addi - tion , it turns out that the set of dual variables ( cid : 123 ) i = f ( cid : 123 ) i;y : y 123 g ( xi ) g for each example i is constrained to be non - negative and sum to 123
it is interesting that , while the parameters w lose their probabilistic interpretation , the dual variables ( cid : 123 ) i for each sentence actually form a kind of probability distribution .
furthermore , the objective can be expressed in terms of ex - pectations with respect to these distributions :
e ( cid : 123 ) i ( li;y ) ( cid : 123 )
( cid : 123 ) i;yi ( cid : 123 ) e ( cid : 123 ) i ( ( cid : 123 ) i;y )
we now consider how to e ( cid : 123 ) ciently solve the max - margin optimization problem for a factored model .
as shown in taskar et al .
( 123 ) , the dual in eq .
123 can be reframed using \marginal " terms .
we will also ( cid : 123 ) nd it useful to consider this alternative formulation of the dual .
given dual variables ( cid : 123 ) , we de ( cid : 123 ) ne the marginals ( cid : 123 ) i;r ( ( cid : 123 ) ) for all i; r , as follows :
( cid : 123 ) i;r ( ( cid : 123 ) i ) = x
( cid : 123 ) i;yi ( xi; y; r ) = e ( cid : 123 ) i ( i ( xi; y; r ) ) :
since the dual variables ( cid : 123 ) i form probability dis - tributions over parse trees for each sentence i , the marginals ( cid : 123 ) i;r ( ( cid : 123 ) i ) represent the proportion of parses that would contain part r if they were drawn from a distribution ( cid : 123 ) i .
note that the number of such marginal terms is the number of parts , which is polynomial in the length of
now consider the dual objective q ( ( cid : 123 ) ) in it can be shown that the original ob - jective q ( ( cid : 123 ) ) can be expressed in terms of these
123the constituent loss function does not exactly cor - respond to the standard scoring metrics , such as f123 or crossing brackets , but shares the sensitivity to the num - ber of di ( cid : 123 ) erences between trees .
we have not thoroughly investigated the exact interplay between the various loss choices and the various parsing metrics .
we used the constituent loss in our experiments .
marginals as qm ( ( cid : 123 ) ( ( cid : 123 ) ) ) , where ( cid : 123 ) ( ( cid : 123 ) ) is the vector with components ( cid : 123 ) i;r ( ( cid : 123 ) i ) , and qm ( ( cid : 123 ) ) is de ( cid : 123 ) ned
( ii;r ( cid : 123 ) ( cid : 123 ) i;r ) ( cid : 123 ) i;r
where li;r = l ( xi; yi; r ) , ( cid : 123 ) i;r = ( cid : 123 ) ( xi; r ) and ii;r = i ( xi; yi; r ) .
this follows from substituting the factored de ( cid : 123 ) nitions of the feature representation ( cid : 123 ) and loss function l together with de ( cid : 123 ) nition of
having expressed the objective in terms of a polynomial number of variables , we now turn to the constraints on these variables .
the feasible set for ( cid : 123 ) is
( cid : 123 ) = f ( cid : 123 ) : ( cid : 123 ) i;y ( cid : 123 ) 123; 123i; y x
( cid : 123 ) i;y = 123;
now let ( cid : 123 ) m be the space of marginal vectors which are feasible :
( cid : 123 ) m = f ( cid : 123 ) : 123 ( cid : 123 ) 123 ( cid : 123 ) s : t : ( cid : 123 ) = ( cid : 123 ) ( ( cid : 123 ) ) g :
then our original optimization problem can be reframed as max ( cid : 123 ) 123 ( cid : 123 ) m qm ( ( cid : 123 ) ) .
fortunately , in case of pcfgs , the domain ( cid : 123 ) m can be described compactly with a polyno - mial number of linear constraints .
essentially , we need to enforce the condition that the ex - pected proportions of parses having particular parts should be consistent with each other .
our marginals track constituent parts ha; s; e; ii and cf - rule - tuple parts ha ! b c; s; m; e; ii the consistency constraints are precisely the inside - outside probability relations :
( cid : 123 ) i;a;s;e = x
( cid : 123 ) i;a;s;e = x
( cid : 123 ) i;b ! a c;s;m;e + x
where ni is the length of the sentence .
in ad - dition , we must ensure non - negativity and nor - malization to 123 :
( cid : 123 ) i;r ( cid : 123 ) 123; x
( cid : 123 ) i;a;123;ni = 123 :
the number of variables in our factored dual for cfgs is cubic in the length of the sentence ,
figure 123 : development set results of the various models when trained and tested on penn treebank sentences of length ( cid : 123 ) 123
figure 123 : test set results of the various models when trained and tested on penn treebank sentences of length ( cid : 123 ) 123
while the number of constraints is quadratic .
this polynomial size formulation should be con - trasted with the earlier formulation in collins ( 123 ) , which has an exponential number of
123 factored smo
we have reduced the problem to a polynomial size qp , which , in principle , can be solved us - ing standard qp toolkits .
however , although the number of variables and constraints in the factored dual is polynomial in the size of the data , the number of coe ( cid : 123 ) cients in the quadratic term in the objective is very large : quadratic in the number of sentences and dependent on the sixth power of sentence length .
hence , in our experiments we use an online coordinate descent method analogous to the sequential minimal op - timization ( smo ) used for svms ( platt , 123 ) and adapted to structured max - margin estima - tion in taskar et al .
( 123 ) .
we omit the details of the structured smo procedure , but the important fact about this kind of training is that , similar to the basic per - ceptron approach , it only requires picking up sentences one at a time , checking what the best parse is according to the current primal and dual weights , and adjusting the weights .
we used the penn english treebank for all of our experiments .
we report results here for
each model and setting trained and tested on only the sentences of length ( cid : 123 ) 123 words .
aside from the length restriction , we used the stan - dard splits : sections 123 - 123 for training ( 123 sen - tences ) , 123 for development ( 123 sentences ) , and 123 for ( cid : 123 ) nal testing ( 123 sentences ) .
as a baseline , we trained a cnf transforma - tion of the unlexicalized model of klein and manning ( 123 ) on this data .
the resulting grammar had 123 non - terminal symbols and contained two kinds of productions : binary non - terminal rewrites and tag - word rewrites . 123 the scores for the binary rewrites were estimated us - ing unsmoothed relative frequency estimators .
the tagging rewrites were estimated with a smoothed model of p ( wjt ) , also using the model from klein and manning ( 123 ) .
figure 123 shows the performance of this model ( generative ) : 123 f123 on the test set .
for the basic max - margin model , we used exactly the same set of allowed rewrites ( and therefore the same set of candidate parses ) as in the generative case , but estimated their weights according to the discriminative method of sec - tion 123
tag - word production weights were ( cid : 123 ) xed to be the log of the generative p ( wjt ) model .
that is , the only change between genera - tive and basic is the use of the discriminative maximum - margin criterion in place of the gen - erative maximum likelihood one .
this change alone results in a small improvement ( 123 vs .
on top of the basic model , we ( cid : 123 ) rst added lex - ical features of each span; this gave a lexical model .
for a span hs; ei of a sentence x , the base lexical features were :
( cid : 123 ) xs , the ( cid : 123 ) rst word in the span ( cid : 123 ) xs ( cid : 123 ) 123 , the preceding adjacent word ( cid : 123 ) xe ( cid : 123 ) 123 , the last word in the span ( cid : 123 ) xe , the following adjacent word ( cid : 123 ) hxs ( cid : 123 ) 123; xsi ( cid : 123 ) hxe ( cid : 123 ) 123; xei ( cid : 123 ) xs+123 for spans of length 123
these base features were conjoined with the span length for spans of length 123 and below , since short spans have highly distinct behaviors ( see the examples below ) .
the features are lex - ical in the sense than they allow speci ( cid : 123 ) c words
123unary rewrites were compiled into a single com - pound symbol , so for example a subject - gapped sentence would have label like s+vp .
these symbols were ex - panded back into their source unary chain before parses
