supervised and unsupervised learning methods have tradi - tionally focused on data consisting of independent instances of a single type .
however , many real - world domains are best described by relational models in which instances of multiple types are related to each other in complex ways .
for exam - ple , in a scientic paper domain , papers are related to each other via citation , and are also related to their authors .
in this case , the label of one entity ( e . g . , the topic of the paper ) is of - ten correlated with the labels of related entities .
we propose a general class of models for classication and clustering in re - lational domains that capture probabilistic dependencies be - tween related instances .
we show how to learn such models efciently from data .
we present empirical results on two real world data sets .
our experiments in a transductive clas - sication setting indicate that accuracy can be signicantly improved by modeling relational dependencies .
our algo - rithm automatically induces a very natural behavior , where our knowledge about one instance helps us classify related ones , which in turn help us classify others .
in an unsuper - vised setting , our models produced coherent clusters with a very natural interpretation , even for instance types that do not have any attributes .
most supervised and unsupervised learning methods assume that data instances are independent and identically distributed ( iid ) .
numerous classication and clustering approaches have been designed to work on such at data , where each data instance is a x ed - length vector of attribute val - ues ( see ( duda et al . , 123 ) for a survey ) .
however , many real - world data sets are much richer in structure , involving in - stances of multiple types that are related to each other .
hyper - text is one example , where web pages are connected by links .
another example is a domain of scientic papers , where pa - pers are related to each other via citation , and are also related to their authors .
the iid assumption is clearly violated for two papers written by the same author or two papers linked by citation , which are likely to have the same topic .
recently , there has been a growing interest in learning techniques for more richly structured datasets .
relational links between instances provide a unique source of infor - mation that has been proved useful for both classication and clustering in the hypertext domain ( slattery and craven ,
123; kleinberg , 123 ) .
intuitively , relational learning meth - ods attempt to use our knowledge about one object to reach conclusions about other , related objects .
for example , we would like to propagate information about the topic of a pa - to papers that it cites .
these , in turn , would propagate information to papers that they cite .
we would also like to use information about s topic to help us reach conclusion about the research area of s author , and about the topics of other papers written by that author .
several authors have proposed relational classication methods along the lines of this inuence propagation idea .
neville and jensen ( 123 ) present an iterative classication algorithm which essentially implements this process exactly , by iteratively assigning labels to test instances the classier is condent about , and using these labels to classify related instances .
slattery and mitchell ( 123 ) propose an iterative algorithm called foil - hubs for the problem of classify - ing web pages , e . g . , as belonging to a university student or not .
however , none of these approaches proposes a single coherent model of the correlations between different related instances .
hence they are forced to provide a purely procedu - ral approach , where the results of different classication steps or algorithms are combined without a unifying principle .
in clustering , the emphasis so far has been on dyadic data , such as word - document co - occurrence ( hofmann and puzicha , 123 ) , document citations ( cohn and chang , 123 ) , web links ( cohn and hofmann , 123; kleinberg , 123 ) , and gene expression data .
kleinbergs hubs and authorities al - gorithm exploits the link structure to dene a mutually rein - forcing relationship between hub and authority pages , where a good hub page points to many good authorities and a good authority page is pointed to by many good hubs .
these techniques can be viewed as relational clustering methods for one or two types of instances ( e . g . , web pages , documents and words ) , with a single relation between them ( e . g . , hyperlinks , word occurrence ) .
however , we would like to model richer structures present in many real world domains with multiple types of instances and complex relationships between them .
for example , in a movie database the instance types might be movies , actors , directors , and producers .
in - stances of the same type may also be directly related .
in a scientic paper database , a paper is described by its set of words and its relations to the papers it cites ( as well as to the authors who wrote it ) .
we would like to identify , for each in - stance type , sub - populations ( or segments ) of instances that are similar in both their attributes and their relations to other
in this paper , we propose a general class of generative probabilistic models for classication and clustering in rela - tional data .
the key to our approach is the use of a single probabilistic model for the entire database that captures inter - actions between instances in the domain .
our work builds on the framework of probabilistic relational models ( prms ) of koller and pfeffer ( 123 ) that extend bayesian networks to a relational setting .
prms provide a language that allows us to capture probabilistic dependencies between related instances in a coherent way .
in particular , we use it to allow depen - dencies between the class variables of related instances , pro - viding a principled mechanism for propagating information
like all generative probabilistic models , our models ac - commodate the entire spectrum between purely supervised classication and purely unsupervised clustering .
thus , we can learn from data where some instances have a class label and other do not .
we can also deal with cases where one ( or more ) of the instance types does not have an observed class attribute by introducing a new latent class variable to repre - sent the ( unobserved ) cluster .
note that , in relational mod - els , it is often impossible to segment the data into a training and test set that are independent of each other since the train - ing and test instances may be interconnected .
using naive random sampling to select training instances is very likely to sever links between instances in the training and test set data .
we circumvent this difculty by using a transductive learning setting , where we use the test data , albeit without the labels , in the training phase .
hence , even if all the instance types have observed class attributes , the training phase involves learning with latent variables .
we provide an approximate em algorithm for learning such prms with latent variables from a relational database .
this task is quite complex : our models induce a complex web of dependencies between the latent variables of all of the entities in the data , rendering standard approaches intractable .
we provide an efcient approximate algorithm that scales lin - early with the number of instances , and thus can be applied to large data sets .
we present experimental results for our approach on two domains : a dataset of scientic papers and authors and a database of movies , actors and directors .
our classication experiments show that the relational information provides a substantial boost in accuracy .
applied to a clustering task , we show that our methods are able to exploit the relational structure and nd coherent clusters even for instance types that do not have any attributes .
123 generative models for relational data probabilistic classication and clustering are often viewed from a generative perspective as a density estimation task .
data instances are assumed to be independent and identi - cally distributed ( iid ) samples from a mixture model distri - in clustering , a latent class random variable is associated with the instance to indicate its cluster .
other attributes of an instance are then assumed to be samples
bution .
each instance belongs to exactly one of
from a distribution associated with its class .
a simple yet powerful model often used for this distribution is the naive in the naive bayes model , the attributes of each instance are assumed to be conditionally independent given the class variable .
although this independence as - sumption is often unrealistic , this model has nevertheless proven to be robust and effective for classication and clus - tering across a wide range of applications ( duda et al . , 123; cheeseman and stutz , 123 ) .
both classication and cluster - ing involve estimation of the parameters of the naive bayes model; however , clustering is signicantly more difcult due to the presence of latent variables .
the iid assumption made by these standard classication and clustering models is inappropriate in rich relational do - mains , where different instances are related to each other , and are therefore likely to be correlated .
in this section , we describe a probabilistic model for classication and cluster - ing in relational domains , where entities are related to each other .
our construction utilizes the framework of proba - bilistic relational models ( prms ) ( koller and pfeffer , 123; friedman et al . , 123 ) .
123 probabilistic relational models a prm is a template for a probability distribution over a re - lational database of a given schema .
it species probabilistic models for different classes of entities , including probabilis - tic dependencies between related objects .
given a set of in - stances and relations between them , a prm denes a joint probability distribution over the attributes of the instances .
relational schema .
a relational schema describes at -
is associated with a set of at - is also associated with a set
tributes and relations of a set of instance types
each type .
each type of typed binary relations relation with the type to use the relation as a set - valued function , whose value is the set of instances " ! related to an instance example , for an actor # , #
role is the set of movies in which
we associate each
of its rst argument , allowing us
the actor has appeared .
in certain cases , relations might have attributes of their own .
for example , the role relation might be associated with the attribute credit - order , which indicates the ranking of the actor in the credits .
we can introduce an explicit type corresponding to the relation .
in this case , a relation object is itself related to both of its arguments .
for example , if one of the role objects is meryl streep in sophies choice , this role object would be related to the actor object meryl streep and the movie object sophies choice .
by denition , these relations are many - to - one .
it will be useful to distinguish be -
% ) , and relation
species the set of objects in each type , the relations that hold between them , and the values of the to denote the
tween entity types ( such as $&% ( ' * ) , +& - or . / +123 ( 123 types ( such as 123+123 attributes for all of the objects .
a skeleton 123
the objects and the relations .
we will use 123 set of objects of type probabilistic model .
a probabilistic relational model : 123 of the relational schema .
more precisely , a prm is a tem -
species a probability distribution over a set of instantiations
( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 )
figure 123 : ( a ) model for imdb domain; ( b ) model for cora domain; ( c ) fragment of unrolled network for cora model .
plate , which is instantiated for different skeletons 123
the re - sult of this instantiation is a probabilistic model over a set of random variables corresponding to all of the attributes of all of the objects in the skeleton .
we can view a prm as a com - pact way of representing a bayesian network for any skeleton over this schema .
a prm consists of a qualitative dependency structure , and the parameters associated with it , structure is dened by associating with each attribute
set of parents pa
the dependency has the form
( prms also allow
dependencies along chains of relations , but we have chosen to omit those for simplicity of presentation . )
each parent of
for a given skeleton 123 rolled bayesian network over the random variables ents of the form the dependence of
, the prm structure induces an un - depends probabilistically on par - is not single - is actually a set of random variables , one .
we address this problem by interpreting as dependence on an ag - gregate function ( e . g . , mode or mean ) of the multiset of values of these variables ( see below ) .
note that if
when one
the quantitative part of the prm species the parameteri - zation of the model .
given a set of parents for an attribute , we can dene a local probability model by associating with it a conditional probability distribution ( cpd ) .
for each attribute
the cpd represents the dependence on the value of the aggre - in the unrolled net -
we have a cpd that species of the parents is of the form
the cpd for
work , for every
is used for .
thus , the cpd for
many times in the network .
aggregates .
there are many possible choices of aggre - gation operator to allow dependencies on a set of variables .
an obvious choice for categorical variables is the mode ag - gregate , which computes the most common value of its par -
let the domain of each
more precisely , consider some variable whose par - we wish to aggregate into a single
aggregator is as follows : we dene a distribution
for each ; given a multiset of values for for the value which is the most
has the same domain .
the effect of the mode
, we use
common in this multiset .
the mode aggregator is not very sensitive to the distribu - tion of values of its parents; for example , it cannot differ - entiate between a highly skewed and a fairly uniform set of values that have the same most frequent value .
an aggre - gate that better reects the value distribution is a stochastic mode aggregator .
in this case , we still dene a set of dis -
, but the effect of the aggregator is that
is a weighted average of these distribu - is the frequency of this value .
we accomplish this behavior by using
tions , where the weight of
an aggregate variable dened as follows .
the aggregate variable also takes on val -
that take on the value
the desired effect .
be the number of variables
then we dene
it is easy to verify that this aggregator has exactly
we note that this aggregate can also be viewed as a ran - domized selector node that chooses one of its parents uni - formly at random and takes on its value .
one appealing con - sequence is that , like min or max , the stochastic model can be decomposed to allow its representation as a cpd to scale lin - early with the number of parents .
we simply decompose the aggregate in a cascading binary tree .
the rst layer computes aggregates of disjoint pairs , with each aggregate randomly selecting the value of one of its parents; the following layer repeats the procedure for disjoint pairs of results from the rst layer , and so on .
( this construction can also be extended to
omit details for lack of space . )
cases where the number of variables is not a power of # ; we 123 classication and clustering models we use the prm framework as the basis of our models for relational classication and clustering .
as in the at prob - abilistic generative approaches , our approach is based on the use of a special variable to represent the class or the cluster .
this variable is the standard class variable in the classi - cation task .
as usual , we deal with the clustering task by introducing a new latent class variable .
thus , for each entity
as in at classication and clustering , we dene the at - to depend on the class variable .
for simplicity , we choose the naive bayes dependency model for the other
class we have a designated attribute attributes : for each attribute
, the only parent of
note that we have only dened class attributes for entity types .
we connect the attributes of relation types to the class attributes of the two associated entity types .
thus , for example , an attribute such as credit - order in the relation
% will depend on the class attributes of
note that , as the dependence in this case is single -
valued by denition , no aggregates are necessary .
most in - terestingly , we also allow direct dependence between class attributes of related entities .
thus , for example , we could al - , or vice versa .
in this case , as the relation is many - to - many , we use aggregates , as described above .
low a dependence of . / +123 ( 123
123 ( a ) shows a simple model for a movie dataset , extracted from the internet movie database ( imdb ) ( www . imdb . com ) .
we see that role is both a class on its own , as well as dening the relation between movies and actors .
we have chosen , in this case , not to have the attribute
credit - order depend on the class of movies , but only of actors .
123 ( b ) shows a model for a domain of scientic papers and authors , derived from the cora dataset ( mccal - lum et al . , 123 ) ( cora . whizbang . com ) .
in this case , we see that the cites relation connects two objects of the same type .
we have chosen to make the class attribute of the cited paper depend on the class attribute of the citing paper .
note that this dependency appears cyclic at the type level .
however , recall that this model is only a template , which is instantiated for particular skeletons to produce an unrolled network; fig .
123 ( c ) shows a fragment of such a network .
if we do not have citation cycles in the domain , then this un - rolled network is acyclic , and the prm induces a coherent probability model over the random variables of the skeleton .
( see ( friedman et al . , 123 ) for more details . )
we can also use latent variable models to represent dyadic clustering .
consider , for example , a domain where we have people and movies , and a relation between them that cor - responds to a person rating a movie .
in this case , we will have a class tribute rating representing the actual rating given .
this at -
% , corresponding to the relation , with the at - tribute will depend on the cluster attributes of both . / +123 ( 123 and $&% ( ' * ) , +& - , leading naturally to a two - sided clustering model .
however , our approach is exible enough to accommodate a much richer model , e . g . , where we also have other attributes of person , and perhaps an entire relational model for movies , such as shown in fig .
our approach will take all of this information into consideration when constructing the clus -
123 learning the models we now show how we learn our models from data .
our train - consists of a partial instantiation of the schema , one where everything except the values of some or all the class attributes is given .
we can view this data as a single large mega - instance of the model , with a large number of miss - ing values .
note that we cannot view the data as a set of inde - pendent instances corresponding to the objects in the model .
in our setting , we typically assume that the structure of our latent variable model is given , as described in section 123 .
thus , our task is parameter estimation .
123 parameter estimation in this case , we assume that we are given the probabilistic de - , and need only estimate the parameters , i . e . , the cpds of the attributes .
a standard approach is to
use maximum likelihood ( ml ) estimation , i . e . , to nd if we had a complete instantiation 123
, the likelihood func - tion has a unique global maximum .
the maximum likelihood parameters can be found very easily simply by counting oc - currences in the data .
recall that all of the objects in the same class share the same cpd .
thus , to estimate the parameter for
, we simply consider all objects of class
, and count the number of times that each combination 123
and its parents jointly take .
these counts are known as sufcient statistics .
see ( friedman et al . , 123 ) for details .
the case of incomplete data is substantially more com - in this case , the likelihood function has multiple lo - cal maxima , and no general method exists for nding the global maximum .
the expectation maximization ( em ) al - gorithm ( dempster et al . , 123 ) , provides an approach for nding a local maximum of the likelihood function .
start -
for the parameters , em iterates
the following two steps .
the e - step computes the distribution over the unobserved variables given the observed data and the be the set of
ing from an initial guess current estimate of the parameters .
letting unobserved cluster variables , we compute
from which it can compute the expected sufcient statistics :
to compute the posterior distribution over the hidden vari - ables , we must run inference over the model .
the m - step re - estimates the parameters by maximizing the likelihood with respect to the distribution computed in the e - step .
123 belief propagation for e step to perform the e step , we need to compute the posterior dis - tribution over the unobserved variables given our data .
this inference is over the unrolled network dened in section 123 .
we cannot decompose this task into separate inference tasks over the objects in the model , as they are all correlated .
( in some cases , the unrolled network may have several connected components that can be treated separately; however , it will generally contain one or more large connected components . ) in general , the unrolled network can be fairly complex , in - volving many objects that are related in various ways .
our experiments , the networks involve tens of thousands of nodes . ) exact inference over these networks is clearly im - practical , so we must resort to approximate inference .
there is a wide variety of approximation schemes for bayesian net - works .
for various reasons ( some of which are described be - low ) , we chose to use belief propagation .
belief propaga - tion ( bp ) is a local message passing algorithm introduced by pearl ( pearl , 123 ) .
it is guaranteed to converge to the cor - rect marginal probabilities for each node only for singly con - nected bayesian networks .
however , empirical results ( mur - phy and weiss , 123 ) show that it often converges in general
networks , and when it does , the marginals are a good approx - imation to the correct posteriors .
( when bp does not con - verge , the marginals for some nodes can be very inaccurate .
this happens very rarely in our experiments and does not af - fect convergence of em . )
a family graph , with a node
cpd; i . e . , if
for each variable
we provide a brief outline of one variant of bp , referring to ( murphy and weiss , 123 ) for more details .
consider a bayesian network over some set of nodes ( which in our case would be the variables ) .
we rst convert the graph into
and its parents .
two nodes are connected if they have some variable in common .
the cpd of
represent the factor dened by the
, , then is a function from the domains of these variables to that encompasses our is not observed
the belief propagation algorithm is now very simple .
at each iteration , all the family nodes simultaneously send mes - sage to all others , as follows :
contains the variables to be a factor over
, we have that
our posterior distribution is then is a normalizing constant .
is a ( different ) normalizing constant and
in the family graph .
set of families that are neighbors of at any point in the algorithm , our marginal distribution about
this process is repeated until the beliefs converge .
give us the marginal distribu - tion over each of the families in the unrolled network .
these marginals are precisely what we need for the computation of the expected sufcient statistics .
after convergence , the
we note that occasionally bp does not converge; to alle - viate this problem , we start the em algorithm from several different starting points ( initial guesses ) .
as our results in section 123 show , this approach works well in practice .
123 inuence propagation over relations among the strong motivations for using a relational model is its ability to model dependencies between related instances .
as described in the introduction , we would like to propagate information about one object to help us reach conclusions about other , related objects .
recently , several papers have proposed a process along the lines of this inuence prop - agation idea .
neville and jensen ( 123 ) propose an itera - tive classication algorithm which builds a classier based on a fully observed relational training set; the classier uses both base attributes and more relational attributes ( e . g . , the number of related entities of a given type ) .
it then uses this classier on a test set where the base attributes are observed , but the class variables are not .
those instances that are clas - sied with high condence are temporarily labeled with the predicted class; the classication algorithm is then rerun , with the additional information .
the process repeats several times .
the classication accuracy is shown to improve substantially as the process iterates .
slattery and mitchell ( 123 ) propose an application of this idea to the problem of classifying web pages , e . g . , as belong - ing to a university student or not .
they rst train a classier on a set of labeled documents , and use it to classify docu - ments in the test set .
to classify more documents in the test set , they suggest combining the classication of the test set pages and the relational structure of the test set .
as a moti - vating example , they describe a scenario where there exists a page that points to several other pages , some of which were classied as student home pages .
their approach tries to iden - tify this page as a student directory page , and conclude that other pages to which it points are also more likely to be stu - dent pages .
they show that classication accuracy improves by exploiting the relational structure .
neither of these approaches proposes a single coherent model of the dependencies between related objects and thus combine different classication steps or algorithms without a unifying principle .
our approach achieves the inuence prop - agation effect through the probabilistic inuences induced by the unrolled bayesian network over the instances in our do - main .
for example , in the cora domain , our network models correlations between the topics of papers that cite each other .
thus , our beliefs about the topic of one paper will inuence our beliefs about the topic of its related papers .
in general , probabilistic inuence ows through active paths in the un - rolled network , allowing beliefs about one cluster to inuence others to which it is related ( directly or indirectly ) .
moreover , the use of belief propagation implements this effect directly .
by propagating a local message from one family to another in the family graph network , the algorithm propagates our beliefs about one variable to other variables to which it is di - rectly connected .
we demonstrate this property in the next
this spreading inuence is particularly useful in our frame - work due to the application of the em algorithm .
the em algorithm constructs a sequence of models , using the proba - bilities derived from the belief propagation algorithm to train a new model .
hence , we not only use our probabilistic in - ference process to spread the information in the relational structure , we then use the results to construct a better clas - sier , which in turn allows us to obtain even better results , etc .
from a different perspective , we are using the structure in the test set not only to provide better classications , but also to learn a better classier .
as we show below , this pro - cess results in substantial improvements in accuracy over the iterations of em .
we note that this bootstrapping ability arises very naturally in the probabilistic framework , where it is also associated with compelling convergence guarantees .
we evaluated our method on the cora and imdb data sets .
the structure of the cora dataset , and the model we used , are shown in fig .
123 ( b , c ) .
for our experiments , we selected a subset of 123 papers from the machine learn - ing category , along with 123 of their authors .
these papers are classied into seven topics : probablistic methods , neu - ral networks , reinforcement learning , rule learning , case - based , and theory .
authors & citations ( ac ) nave bayes ( nb )
figure 123 : ( a ) comparison of classication accuracies; ( b ) inuence propagation in bp; ( c ) accuracy improvement in em .
we evaluated the ability of our algorithm to use the rela - tional structure to aid in classication .
we took our entire data set , and hid the classications for all but a fraction of the papers .
we then constructed our model based on all of this data , including the documents whose topics were unob - served .
the resulting model was used to classify the topic for the test documents .
in effect , we are performing a type of transduction , where the test set is also used to train the model ( albeit without the class labels ) .
to investigate how our method benets from exploiting the relational structure , we considered four different mod - els which vary in the amount of relational information they use .
the baseline model does not use relational information at all .
it is a standard multinomial naive bayes model ( nb ) over the set of words ( bag of words model ) in the abstract .
the full model ( ac ) was shown in fig .
123 ( b ) ; it makes use of both the authors and citations .
the other two models are fragments of ac : model a incorporates only the author infor - mation ( eliminating the citation relation from the model ) , and model c only citations .
all four models were trained using em; model nb was trained using exact em and the others using our algorithm of section 123
we initialized the cpds for the word attributes using the cpds in a naive bayes model that was trained only on the observed portion of the data set .
all models were initialized with the same cpds .
we varied the percentage of labeled papers , ranging from 123% to 123% .
for each different percentage , we tested the classication accuracy over ve random training / test splits .
the results are shown in fig .
each point is the average of the accuracy on the ve runs , and the error bars correspond to the standard error .
as can be seen , incorporating more relational dependencies signicantly improves classication accuracy .
both a and c outperform the baseline model , and the combined model ac achieves by far the highest accuracy .
the local message passing of loopy belief propagation ( bp ) resembles the process of spreading the inuence of beliefs for a particular instance to
as discussed in section 123 ,
eral labeled papers .
upon initialization , we have some initial from its words alone .
however , after the rst iteration , this belief will be updated to reect the labels of the papers it cites , and is likely to become more
its related instances .
for example , suppose paper cites sev - belief about the topic of peaked around a single value , increasing the condence in s
in the following iteration , unlabeled papers that cite ( as well as unlabeled papers that cites ) will be updated to reect the increased condence about the topic of
, and so on .
to measure this effect , we examine the belief state of the topic variable of the unlabeled papers after every iteration of loopy belief propagation .
for every iteration , we report the fraction of variables whose topic can be determined with high condence , i . e . , whose belief for a single topic is above a .
123 ( b ) shows several series of these mea - surements on a dataset with 123% labeled papers .
the series show bp iterations performed within the rst , third and sev - enth iteration of em .
each series shows a gradual increase of the fraction of papers whose topics we are condent in .
the accuracy on those high - condence papers is fairly constant over the iterations around 123 , 123 , and 123 for the rst , third and seventh iteration of em , respectively .
loopy belief propagation is an approximation to the infer - ence required for the e step of em .
although loopy bp is not guaranteed to converge , in our experiments , it generally converges to a solution which is good enough to allow em to make progress .
indeed , fig .
123 ( c ) shows that the classi - cation accuracy improves for every em iteration .
this g - ure also demonstrates the performance improvement obtained from bootstrapping the results of iterative classication , as discussed in section 123
the attributes and relations in the imdb database , and the latent variable model we used , are shown in are shown in fig .
123 ( a ) ; the genre attribute actually refers to a set of 123 binary attributes ( action , comedy , .
note that actors and directors have almost no descriptive attributes and hence can - not be clustered meaningfully without considering their rela - tions .
we selected a subset of this database that contains 123 movies , 123 actors , and 123 directors .
in fig .
123 , we show two example clusters for each class , listing several highest condence members of the clusters .
in general , clusters for movies consist of movies of pre - dominantly of a particular genre , time period and popularity .
for example , the rst movie cluster shown can be labeled as classic musicals and childrens lms .
the second cluster cor - responds roughly to action / adventure / sci - movies .
model , the clusters for actors and directors are relational in nature , since they are induced by the movie attributes .
for example , the rst cluster of actors consists primarily of action
acknowledgments .
this work was supported by onr contract n123 - 123 - c - 123 under darpas hpkb pro - gram .
eran segal was also supported by a stanford graduate
