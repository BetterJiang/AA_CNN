stanford , ca 123 - 123
the probability that a random individual in classc is also in classd .
the effectiveness of the algorithm re -
knowledge representation languages invariably reect a trade - off between expressivity and tractability .
evi - dence suggests that the compromise chosen by descrip - tion logics is a particularly successful one .
however , description logic ( as for all variants of rst - order logic ) is severely limited in its ability to express uncertainty .
in this paper , we present p - classic , a probabilistic version of the description logic classic .
tion to terminological knowledge , the language utilizes bayesian networks to express uncertainty about the ba - sic properties of an individual , the number of llers for its roles , and the properties of these llers .
we provide a semantics for p - classic and an effective inference procedure for probabilistic subsumption : computing
lies on independence assumptions and on our ability to execute lifted inference : reasoning about similar indi - viduals as a group rather than as separate ground terms .
we show that the complexity of the inference algorithm is the best that can be hoped for in a language that combines description logic with bayesian networks .
in particular , if we restrict to bayesian networks that support polynomial time inference , the complexity of our inference procedure is also polynomial time .
first - order logic has been the basis for most knowledge rep - resentation formalisms .
its basic unitsindividuals , their properties , and the relations between themnaturally cap - ture the way in which people encode their knowledge .
un - fortunately , it is severely limited in its ability to represent our uncertainty about the world : a fact can only be known to be true , known to be false , or neither .
by contrast , most of our knowledge about the real world is not absolutely true , but only true with some degree of certainty .
this limita - tion renders rst - order logic inapplicable to a large range of
the fundamental step in addressing this problem was taken by bacchus ( 123 ) and halpern ( 123 ) .
they dened and analyzed ways in which probabilities can be added to
rst - order logic , and claried the semantics of the resulting formalisms .
their work focused on probabilistic extensions of full rst - order logic .
as a consequence , these logics were shown to be highly undecidable ( much more so even than rst - order logic ) .
furthermore , they do not support a natural and compact specication of independence assumptions , which are crucial to getting nontrivial conclusions from a probabilistic knowledge base .
there has been a recent move towards integrating prob - abilities into less expressive subsets of rst - order logic .
by and large , knowledge representation formalisms based on subsets of rst - order logic fall into two categories : rule - based languages , and object - centered formalisms ( e . g . , frame - based languages , description logics ) .
so far , most work on probabilistic extensions has focused on augment - ing rule - based languages ( goldman and charniak , 123; breese , 123; poole , 123; ngo et al . , 123 ) .
in this pa - per , we take a rst step towards integrating probabilities with object - centered languages , by developing a probabilis - tic description logic .
our language provides the ability to describe classes of individuals , and to reason about the re - lationships between classes .
( binary relations ) .
for a given individuala , the individ - uals related toa by some roler are called ther - llers ofa .
description logic allows us to describe classes of r - llers of an individual , and the properties of the llers .
description logics are subsets of rst - order logic with equality that have been designed to model rich class hier - archies .
informally , in a description logic we begin with a set of primitive concepts ( i . e . , unary predicates ) and roles
individuals ( complex concepts ) based on their properties : the primitive concepts to which they belong , the number of
description logics support subsumption querieswhether one complex concept is always a subset of another , and membership querieswhether a particular individual is an instance of a given concept .
several systems have been built based on description log - ics ( e . g . , classic ( brachman et al . , 123 ) , loom ( macgre - gor , 123 ) , and back ( petalson , 123 ) ) , and they have been used in several applications ( e . g . , ( wright et al . , 123 ) ) .
in addition , several information integration systems ( e . g . , the information manifold ( levy et al . , 123 ) and sims ( arens et al . , 123 ) ) use description logics to represent the infor -
individuals ind .
by contrast , standard subsumption can only tell us whether this number is 123 ( d is subsumed byc ) , 123 ( d is disjoint fromc ) , or somewhere in between .
essentially identical ( e . g . , differentr - llers of the same
of course , the fact that our representation uniquely deter - mines this number does not necessarily imply that we can effectively compute it in practice .
we show that the par - ticular description logic and independence assumptions that we have made also enable us to develop an effective infer - ence algorithm for p - classic .
the algorithm follows the same general lines as the inference algorithm for standard classic , by representing concepts as a graph .
however , in p - classic we replace the logical inference for comparing pieces of the graph by inference with bayesian networks for computing the probability of parts of the graph .
further - more , in contrast to other algorithms for reasoning in other rst - order probabilistic formalisms , our algorithm imple - ments a form of lifted inferencereasoning at the level of variables rather than at the level of ground terms .
this is possible because our independence assumptions enable the algorithm to reuse computation for individuals that are
individual ) .
we show that , in some sense , the complex - ity of this algorithm is the best that can be hoped for in a language that combines the expressive power of clas - sic and bayesian networks .
in particular , if we restrict to polynomial time bayesian networks ( e . g . , polytrees ( pearl , 123 ) ) the complexity of our inference algorithm remains
several works ( shastri , 123; jaeger , 123; heinsohn , 123 ) have considered probabilistic extensions of descrip - tion logics .
there , the focus was on completing partial sta - tistical information using default probabilistic procedures such as entropy maximization or cross - entropy minimiza - tion , or on deriving the minimal conclusions available from a small , incomplete set of probabilistic statements .
by con - trast , our approach follows the more recent tradition , es - tablished by bayesian networks , of having the probabilistic knowledge base completely specify a probability distribu - tion .
the full specication approach has been shown to be both conceptually and computationally simpler .
as we have discussed , similar computational benets are obtained in our framework , which supports an inference algorithm which is signicantly more efcient than those for the pre - vious works on probabilistic terminological languages .
itive concepts ( unary predicates ) a and roles ( binary rela - tions ) r .
the language uses a set of constructors to build
we rst briey review the variant of the classic descrip - tion logic that underlies p - classic , and then describe the probabilisticcomponent of p - classic .
finally , we describe how these components come together to form a p - classic
123 the description logic the basic vocabulary of a description logic consists of prim -
123 the p - classic language
descriptions , dening new classes of individuals called com -
mation sources by specifying the class of individuals con - tained in the information source .
for example , an individual might be one article in a bibliographic database .
to retrieve the complete answer to a query , the system accesses each information source whose description overlaps with the de - scription of the query .
one of the main limitations of description logics is that they can express very little about the overlap between two concepts .
given two concepts , we can infer that one sub - sumes the other , that they are disjoint , or that they may have a non - empty overlap .
however , the degree of the overlap cannot be described or inferred .
the need for such knowledge is clearly demonstrated in the information inte - gration domain .
accessing every information source which potentially overlaps with our query may be prohibitively expensive .
if we want the system to nd a large fraction of the answers as soon as possible , it is very important to infer the degree of overlap between classes of individuals .
in general , of course , the domain consists of many dif - ferent types of individuals .
it is rarely the case that the same distribution will be appropriate for each of them .
for example , the distribution over the properties of an individ - ual is usually quite different from the distribution over the properties of its llers .
therefore , the probabilistic com - ponent of a p - classic knowledge base includes a number of different p - classes ( probabilistic classes ) , each of which is a bayesian network over basic properties , the number
in this paper , we describe a language , p - classic , which is a probabilistic extension of the description logic clas - sic .
p - classic allows the specication of a probability distribution over the properties of individuals .
as the ba - sic representational tool , we use bayesian networks ( pearl , 123 ) .
bayesian networks allow a compact and natural rep - resentation of complex probability distributions by using independence assumptions .
in this case , we use a bayesian network whose random variables are the basic properties of individuals ( the primitive concepts ) , the numbers of their llers , and the properties of their llers .
ofr - llers ( for the different rolesr ) , and the p - classes c;d , what is the probability thatc holds within the set of
the semantics for p - classic is a simple extension to the standard semantics of classic .
following ( halpern , 123 ) , we interpret a p - class as a probability distribution over the elements in the domain .
intuitively , this corresponds to the probability of choosing ( encountering ) this element in this p - class .
by assuming that the p - class distribution is as described in the corresponding bayesian network , and that llers are chosen independently of each other ( from the appropriate p - class ) , we can show that our p - classes uniquely determine the probability of any complex concept .
a p - classic knowledge base allows us to answer any probabilistic subsumption query : for two complex concepts
from which the role llers are chosen .
in addition to the probabilistic component , a p - classic knowledge base also contains a standard terminological component , describing complex concepts in terms of primitive ones .
in this paper we do not consider knowledge bases with ground facts ( i . e . ,
( negation on primitive concepts )
the non - probabilistic component of the p - classic lan - guage is a variant of the classic description logic
to standard roles .
attributes are binary relations which are functional : each individual has exactly one ller for that at - tribute .
in p - classic we add the restriction that the ller for an attribute be one of a nite prespecied set of individuals .
complex descriptions in our language are built using the
classic , we also allow a set of attributesq in addition following grammar , whereaa denotes a primitive con - cept , rr a role , qq denotes an attribute , andc and d represent concept descriptions : ( ( cid : 123 ) nr ) j ( ( cid : 123 ) nr ) j ( number restrictions ) nology dt ( the tbox ) and a set of ground atomic facts da tence of the formc : =d , wherec is a name of a dened concept andd is a description .
each dened concept is tion .
a canonical form of a description is ( cid : 123 ) u ( cid : 123 ) r123 where ( cid : 123 ) is a conjunction of primitive concepts and their attribute appearing more than once ) , and ( cid : 123 ) r is of the form ( ( cid : 123 ) mrr ) u ( ( cid : 123 ) nrr ) u ( r : c ) , wherec is also in description is dened as follows .
the depth of ( cid : 123 ) is 123
the depth of a concept ( cid : 123 ) u ( cid : 123 ) r123 u : : : u ( cid : 123 ) rm is 123+max ( depth ( ( cid : 123 ) ) , depth ( ( cid : 123 ) r123 ) , : : : , depth ( ( cid : 123 ) rm ) ) .
readers familiar with classic will see that our language does not contain classics same - as constructor , but does support negation on primitive concepts that is not allowed in classic .
classic also allows the lls constructor to be ap - plied to nonfunctionalroles .
it should be noted that allowing negation on primitive concepts does not change the expres - sive power of classic; it therefore follows from ( borgida and patel - schneider , 123 ) that subsumption in the language described above can be done in polynomial time .
( the abox ) .
in this paper , we do not consider aboxes .
in classic , a terminology contains two kinds of statements : concept introductions , describing the primitive concepts in the terminology , and concept denitions , specifying the de - ned concepts .
in p - classic a terminology includes only the concept denitions ( as we describe shortly ) , while con - cept introductions are given as part of the probabilisticcom - ponent of a knowledge base .
a concept denition is a sen -
dened by a single concept denition , and we assume that names of dened concepts do not appear in the descriptions . 123 in our analysis , we use the canonical form of a descrip -
canonical form .
any description in our language can be converted to canonical form in linear time .
the depth of a
negations and of ller specications ( with no concept or
a description logic knowledge base d
includes a termi -
123this is not a restriction when the terminology has no cycles ( as in classic ) because the denitions in the terminology can be unfolded .
however , as usual , unfolding a terminology may cause its size to grow exponentially .
class ( p - class ) p species a probability distribution over the that an object belongs to conceptd given that it belongs to conceptc ? .
we write such queries as pr ( djc ) .
setp of p - classes .
intuitively , a p - class represents our prob - each p - classpp is represented using a bayesian net - work ( pearl , 123 ) np .
a bayesian network is a dag in
properties of individuals , allowing us to dene the extent of the overlap between them .
from these numbers , and appro - priate probabilistic independence assumptions , we are able to deduce the answers to arbitrary probabilistic subsump - tion queries : queries of the form what is the probability
123 the probabilistic component of p - classic the main motivation for p - classic is to be able to express the degree of overlap between concepts .
a probabilistic
abilistic information relating to a certain class of individuals .
the probabilistic component of p - classic consists of a
which the nodes represent random variables .
each variable takes on a value in some predened range .
each node in the network is associated with a conditional probability ta - ble ( cpt ) , which denes the probability of each possible value of the node , given each combination of values for the nodes parents in the dag .
the network structure encodes the independence assumption that only the parent values are relevant when making this choice .
a bayesian network de - nes a joint probability distribution over all combinations of values of the variables in the network .
the probability of a particular assignment of values is the product over all the nodes in the network of the conditional probability of the value of that node given the values of its parents .
a , - m - a , m - a , - m
ple , consider the truth assignment ( animal; : vegetable;
figure 123 shows part of the bayesian network for a p - class .
this network contains a node for each of the primitive concepts animal , vegetable , mammal , carnivore and herbivore .
the value of each node is either true or false , depending on whether an object belongs to the concept or not .
the network denes a joint probability distribution over the set of truth assignments to these concepts .
for exam -
natural thing p - class .
the bayesian network for
computed by the product
of primitive concepts and their negations , the probability
: mammal; carnivore; : herbivore ) .
its probability is pr ( animal ) ( cid : 123 ) pr ( : vegetablej animal ) ( cid : 123 ) pr ( : mammalj animal ) ( cid : 123 ) pr ( carnivorej animal; : mammal ) ( cid : 123 ) pr ( : herbivorej animal; carnivore ) = 123 : 123 ( cid : 123 ) 123 ( cid : 123 ) 123 : 123 ( cid : 123 ) 123 : 123 ( cid : 123 ) 123= 123 : 123 : in general , np will contain a node for each primitive con - ceptaa .
for any descriptionc which is a conjunction np ( c ) is dened by the network .
this allows simple sub - sumption queries of the form pr ( djc ) to be answered when bothc andd are conjunctions of primitive concepts distribution over attribute values .
for each attributeq q , np contains a node fills ( q ) .
the cpt for a node lists some nite set of objectsv123; : : : ;vk , and for eachvi , the probability that ( llsqvi ) holds given each possible each rolerr , the network species the number of r - llers by including a number ( r ) node , which takes on values between 123 and some upper boundbr .
the nodes value denotes the number ofr - llers that the object has .
which the llers are chosen .
thus , for each roler , the the setp of p - classes .
this node is always deterministic , of an object .
one of the p - classes , denotedp ( cid : 123 ) networknp ( cid : 123 ) .
as we traverse the bayesian network from its
i . e . , for any combination of values of its parents , exactly one p - class is assigned probability 123 , and all other p - classes are assigned probability 123
for simplicity of presentation , we place some restrictions on the topology of the network .
we assume that a number ( r ) node may only be a parent of the corresponding pc ( r ) node .
the pc ( r ) node may not be a parent of any other node .
the probabilistic component of a p - classic knowledge base recursively describes a distribution over the properties , is the root p - class , denoting the distribution over all objects .
the properties of an object are chosen according to the bayeisan
combination of values for the nodes parents .
note that the node for an attribute also enumerates the set of values that the attribute may take , and therefore can be used instead of classics one - of constructor for attributes .
to describe the properties of the llers , we simply assign a p - class for each role , which species the distribution from
to fully describe an individual , we also need to describe the number of its various llers and their properties
network contains a pc ( r ) node , whose value ranges over
the network for a p - class also determines the probability
and their negations .
roots down , each node tells us the probability with which its value should be chosen .
the root nodes are chosen with the appropriate unconditional probability , while the distribution used for other nodes is determined by prior choices for the values of their parents .
in particular , the network dictates how the number of llers for each role is chosen given the basic properties and attribute values .
by specifying the p - class for the llers , the network species how the properties of the llers are chosen recursively using a similar process .
as stated earlier , classic allows us to state concept in - troductions in its terminology .
in p - classic , concept in - troductions can be represented directly in the probabilistic
description .
for simplicity of exposition , we assume that
component of the knowledge base ( in fact , concept intro - ductions are a special case of probabilistic assertions )
concept introduction is a sentence of the forma ( cid : 123 ) d , wherea is a name of a primitive concept andd is a concept d does not mention any of the roles .
( in section 123 we ' 123; : : : ; ' n is a list of concept introductions , then the de - scription in ' i can only mention the concepts introduced in ' 123; : : : ; ' i ( cid : 123 ) 123 , or the concept thing ( which denotes the set of all individuals ) .
a concept introductiona ( cid : 123 ) d is class that the probability of : dua is 123
since the concept by making the concepts appearing ind parents ofa , and setting the appropriate entries in the cpt fora to 123
in the natural thing p - class , pr ( animal ) = 123 : 123 , while pr ( vegetablej animal ) = 123 , and pr ( vegetablej : animal ) = 123
these assertions encode the termino - and pr ( mammalj animal ) = 123 : 123
only animals can : animal , mammal are irrelevant since that combination
figure 123 shows the probabilistic component of the knowledge - base for a domain of natural objects .
there are three p - classes , each being a network containing the nodes animal , vegetable , mammal , carnivore , herbivore , fills ( size ) , number ( eats ) and pc ( eats ) .
logical knowledge that everything is either an animal or a vegetable , and the two concepts are disjoint . 123 in the cpt for mammal , we see that only animals can be mammals ,
describe how to remove this restriction . ) as in classic , we consider concept introductions that are acyclic , i . e . , if
be carnivores , and mammals are more likely than other animals to be carnivorous; the entries in the column for
encoded in the knowledge base by specifying in each p -
introductions are acyclic , we can encode this information
is impossible .
the fills ( size ) node indicates that the value of the size attribute must be big , medium , or small , and the conditional probability of each value .
since vegetables dont eat , number ( eats ) is always 123 if vegetable is true , while for non - vegetables ( i . e . , animals ) it is a number be - tween 123 and 123 with the given distribution .
finally , pc ( eats ) depends on carnivore and herbivore : for carnivores it is carnivore food , for herbivores it is herbivore food , while for things which are neither it is natural thing .
since nothing is both a carnivore and a herbivore , that col - umn is irrelevant .
the carnivore food p - class
natural thing conditioned on animal being true .
thus vegetable is false , and the other nodes only contain columns that are consistent with these facts .
herbivore food is the same as natural thing condi - tioned on vegetable being true .
in this case the p - class is deterministic except for the value of fills ( size ) , since animal , mammal , carnivore and herbivore are all false , and number ( eats ) is 123
pc ( eats ) is irrelevant since there are no eats - llers .
123strictly speaking , the probabilistic version of this statement is slightly weaker than the terminological version , because it is possible that the set of things that are both animal and vegetable is non - empty but of measure zero .
figure 123 : p - classes for the nature domain .
a , - m - a , m - a , - m
123 semantics of p - classic
the semantics of p - classic is an extension of the semantics of classic .
the basic element is an interpretation .
it assigns
interpretationi contains a non - empty domainoi to every individuala , a unary relation to every concept nameaa , a binary relationri to every rolerr , and a total function : o ! o to every attributeqq .
the interpretations as follows ( ) fsg denotes the cardinality of a sets ) : an interpretationi is a model of a terminology dt if for every concept denitionc : =d in dt .
a conceptc is said to be subsumed by a conceptd w . r . t .
a terminology dt ifci ( cid : 123 ) di for every modeli of dt .
with a distribution over the domainoi p describes a random event : the selection of an individual
in order to extend this semantics to p - classic , we have to provide an interpretation for the p - classes .
following ( halpern , 123 ) , we interpret a p - class as an objective ( sta - tistical ) probability . 123 that is , each p - class will be associated .
intuitively , a p - class
of the descriptions are dened recursively on their structure
123it is also possible to ascribe semantics to this language using subjective probabilities , i . e . , distribution over possible interpre - tations .
for our purposes , the statistical interpretation is quite natural , and signicantly simpler to explain .
from the domain .
the probability with which an individ - ual is chosen depends on its properties ( as determined by the other components of the interpretation ) .
first , a truth assignment to the primitive concepts and an assignment of values to attributes is chosen , according to the probability
distribution dened in the networknp .
given this assign - denition 123 : leti be some interpretation over our vo - cabulary .
a probability distribution ( cid : 123 ) overoi with a p - classp if the following condition holds .
for every conjunctive descriptionc such that : ( a ) for every primitive classaa , c contains eithera or : a as a conjunct , ( b ) for every attributeqq , c contains a conjunct ( lls qv ) for somev , and ( c ) for every rolerr , c contains a conjunct ( =hr ) for some integerh , 123 123 ( cid : 123 ) ( ci ) =np ( c ) , i . e . , the probability of the set of in - dividuals in the interpretation ofc is the same as the probability assigned toc by the bayesian network forp .
for every roler , consider the experiment consisting of selecting an objectx according to ( cid : 123 ) , conditioning on the 123we use ( =hr ) as a shorthand for ( ( cid : 123 ) hr ) u ( ( cid : 123 ) hr ) .
ment , the number of llers for each role is chosen , according to the conditional probability tables of the number nodes .
finally , the properties of the llers are chosen , using another random event , as described by the p - class determined by the pc node for the appropriate role .
our semantics require that the probability of choosing an element be consistent with this type of random experiment for choosing its properties .
we have that :
for a p - classic
description logic component and a set of probability dis -
in other words , the experiment just described gen - erates the same distribution when we also condition on properties of other llers , and recursively on the proper - ties of llers of other llers , and so on .
this independence assumption applies both to other llers of the same role , and to llers of other roles .
, and picking one ofxsr - llers at random .
this experiment generates a new distribution ( cid : 123 ) must be consistent withp is the deterministic value ofnp ( pc ( r ) jc ) .
givenc , the different llers are all independent of each
we can now dene an interpretationip knowledge base to consist of an interpretationi for the , one for every p - classp , such that ( cid : 123 ) ip is consistent withp .
for any descriptiond , we say ipj= pr ( d ) = ( cid : 123 ) if ( cid : 123 ) ip ( cid : 123 ) ( d ) = ( cid : 123 ) , wherep ( cid : 123 ) distribution forr - llers must assign a non - zero probability to some element .
if that element is chosen as the rstr - r - ller , thereby violating the independence condition .
in an innite domain , on the other hand , ( cid : 123 ) ip is a prob - any descriptionc , there is a unique ( cid : 123 ) ( 123; 123 ) such that
the conditions in denition 123 are sufcient to guar - antee that the probability of any description is uniquely determined by a p - classic knowledge base .
as we will see , this result is a consequence of the fact that a bayesian network uniquely species a probability distribution , and of our independence assumptions .
the following theorem is proved by induction on the depth of the canonical form of a theorem 123 : for any p - classic knowledge base d
ability measure over the domain .
while each individual element has probability zero of being selected , the measure assigns a non - zero probability to properties of elements .
in fact , we can construct a domain of this type , by dening do - main elements to correspond to the set of description - logic properties that they satisfy .
this construction is the basis for the following theorem .
proofs of theorems are omitted due to space limitations .
this result follows because the acyclicity and locality of the cpts prevent us from specifying an inconsistent set of probabilistic constraints ( including with respect to termino - logical information ) .
note that a satisable interpretation may assign the empty set to some concepts .
one somewhat counterintuitive consequence of the inde - pendence assumption is that if any role has non - zero prob - ability of having more than one ller , then any model for the knowledge base must be innite .
in a nite domain , the
theorem 123 : a p - classic knowledge base is always sat - isable in some interpretation .
ller , then by the requirement that different role - llers be distinct , it must have zero probability of being the second
is the root
123 inference algorithm
it is possible to devise a simple algorithm for computing the
network terms , this probability is d - separated ( pearl , 123 ) from the rest of the network by number ( r ) and pc ( r ) .
thus , we can add a node to the network representing the
cost of such a procedure would be exponential in the depth of the concept and in the number of primitive concepts and attributes .
to obtain a tractable algorithm , we make two observations .
first , probabilities can be computed bottom up , beginning with subexpressions of depth 123
probabilities of deeper expressions are computed using the stored prob - abilities of the subexpressions .
the second observation is
probability of a concept of the form ( cid : 123 ) u ( cid : 123 ) r123 u : : : u ( cid : 123 ) rm , by rst computing the probability of ( cid : 123 ) , and then , recursively , computing the probabilities of ( cid : 123 ) r123 : : : ; ( cid : 123 ) rm .
however , the that the probability that ( cid : 123 ) r holds is completely determined by the number and p - class of ther - llers .
event ( cid : 123 ) r , with the parents number ( r ) and pc ( r ) .
for each pair of values ( h;p ) for the parents , we can compute the conditional probability of ( cid : 123 ) r givenh andp then assert as evidence that all the ( cid : 123 ) r hold , as well as ( cid : 123 ) , and compute the probability ofc by computing the probability and complete .
in other words , for any descriptionc and , it returns the number ( cid : 123 ) such algorithm is linear in the length ofc and polynomial in puteprobability is linear in the length ofc and quadratic in the number of p - classes inkb .
if all the bayesian net - mammalu ( ( cid : 123 ) 123 eats ) ueats . mammal using the networks contains the depth 123 subexpressionmammal .
the rst stage of the computation calculates prob ( mammal ) for
we rely on the bayesian network inference algorithm to compute this probability in the most efcient manner pos - sible .
in particular , if the original network is a polytree , 123 thus supporting linear time inference , the new network will continue to support linear time inference after the transfor - mation .
the complete algorithm is shown in figure 123
theorem 123 : algorithm computeprobability is sound p - classic knowledge base d
the number of p - classes .
it is important to note that the complexity of reasoning in p - classic is no worse than that of its components , i . e . , classic and bayes nets alone .
theorem 123 : the running time of algorithm com -
works for the p - classes are polytrees , then the running time of computeprobability is also polynomial in the size of the
, and add it to the conditional probability table for the new node
in figure 123
the depth of this expression is 123 , and it
the following theorem shows that the complexity of the
as an example , consider computing the probability of
of the evidence in the bayesian network .
each of the p - classes natural thing , carnivore food these probabilities are found to and herbivore food;
123a polytree is a bayesian network whose underlying undirected graph is acyclic , i . e . , there is only one path of inuence between any pair of nodes .
polytrees support linear time probabilistic inference ( pearl , 123 ) .
, cpt ) adds node n with parents p
/ / addnode ( bn , n , p / / conditional probability table cpt to bayesian network bn .
/ / addevidence ( bn , n = v ) asserts that n has value v in bn .
/ / evaluate ( bn ) computes the probability of the evidence in bn .
/ / c is a description in canonical form .
/ / kb is a p - classic knowledge base .
/ / returns ( cid : 123 ) such thatkbj= ( pr ( c ) = ( cid : 123 ) ) .
forx = 123 todepth ( c ) for each p - classp and subexpressionc of depthx inc do : / / whenx=depth ( c ) , only do this for the root p - classp ( cid : 123 ) forj : = 123 to m / / skip if m = 123 forh : = 123 tobj / / bj is the bound on the for each p - classp / / number ofrj - llers cpt ( h;p; true ) : = probp ( dj ) h else cpt ( h;p; true ) : = 123 cpt ( h;p; false ) = 123 - cpt ( h;p; true ) addnode ( bn , ( cid : 123 ) rj , p / / i . e . , addevidence for each conjunct in ( cid : 123 ) probp ( c ) = evaluate ( bn ) probp ( cid : 123 ) ( mammalu ( ( cid : 123 ) 123 eats ) ueats . mammal ) .
in this description , ( cid : 123 ) is mammal , and there is one ( cid : 123 ) compo - nent corresponding to the eats role . p ( cid : 123 ) natural thing .
a node is added tonp ( cid : 123 ) for ( cid : 123 ) eats , shows the conditional probability that ( cid : 123 ) eats is true for quires that ( ( cid : 123 ) 123 eats ) , the entries in the rst column are ously computed value of probnt ( mammal ) .
the remain - ing entries in the rst row are 123 : 123 , 123 : 123 and so on .
similarly , the entries for carnivore food are 123 , 123 : 123 , probhf ( mammal ) = 123
the probability of our query is mal and ( cid : 123 ) eats to be true , resulting in an answer of approx -
when there is exactly one ller of the p - class natural thing , the entry is 123 , which is the previ -
then computed in the resulting network , by asserting mam -
with the parents number ( eats ) and pc ( eats ) .
figure 123
each value of its parents .
because the description re -
the entries for herbivore food are all 123 because
figure 123 : algorithm computeprobability
be 123 , 123 and 123 respectively .
next we calculate
is the p - class
given values of number ( eats ) and pc ( eats ) .
inference algorithm .
in this section , we discuss several fea - tures and limitations of the expressive power of p - classic .
we mention several possible extensions both to the under - lying description logic and to the probabilistic component .
we examine the extent to which these extensions can be ac - commodated in our framework and how they would affect the complexity of reasoning .
figure 123 : the probability of ( ( cid : 123 ) 123 eats ) u ( eats . mammal ) by expressing the probability that the number of llers isn using a closed form functionf ( n ) .
the inference problem nite series involvingf ( n ) .
alternatively , arbitrarily close
123 the underlying description logic p - classic can be easily extended to handle disjunctive concepts , existential quantication , negation on arbitrary concepts ( not only primitive ones ) and qualied number restrictions .
our semantics provide a well - dened proba - bility for descriptions using these constructors .
however , the inference algorithm for computing probabilities for such descriptions is signicantly more complicated , and is no longer polynomial in the length of the description .
this is not surprising , since the lower bounds ( np - hardness or worse ) of subsumption in the presence of these constructs will apply to the probabilistic extension as well .
another restriction , as mentioned above , is that the num - ber of llers for each role be bounded .
this restriction exists for two reasons .
first , it allows us to completely specify the distribution over the number of llers for a role .
second , the inference algorithm considers all possible values for the number of llers , so this restriction is required to guarantee that the algorithm terminates .
we can address the rst issue
is somewhat harder .
in certain cases , the algorithm may be able to compute a closed form expression for an in -
approximations to the true probability can be obtained by summing a sufciently long nite prex of this series .
the only constructor from classic that is not included in p - classic is same - as , which enables to describe equal - ity of attribute paths ( e . g . , to state that the path wife . father reaches the same individual as the path mother ) .
such equalities are harder to incorporate into our language be - cause our semantics depends heavily on the assumption that different llers are completely independent .
equality of individuals reached via different ller chains obviously contradicts this independence assumption .
123 discussion and extensions
the motivation in designing p - classic was to develop a tractable rst - order probabilistic logic .
to that end , we have shown that p - classic has a sound , complete and efcient
123 the probabilistic component the tractability of our language rests heavily on the in - dependence assumptions made in the probabilistic compo - nent : ( 123 ) the properties of different llers are independent ,
and ( 123 ) the properties of an object and of one of its llers are independent given the llers p - class .
for example , our assumptions prevent us from stating that a non - carnivore always eats at least one vegetable .
such an assertion would imply that the eats - llers are not independent , since if all but one of the llers are not vegetables , the last one must be .
thus , although we can ( albeit at a high computational cost ) compute the probabilityof any description with disjunctions or existentials , we cannot assert terminological properties involving these constructs without modifying the semantics .
one type of correlation between llers can actually be dealt with fairly easily within our framework .
assume that our vocabulary contains the concept healthy - to - eat .
we may want to assert that the healthiness of the vari - ous foods eaten by a person tends to be correlated .
we can accomplish this by introducing a new concept health - conscious and two p - classes representing healthy food and unhealthy food .
the value of the node pc ( eats ) can now depend on the value of health - conscious , in that a value of healthy food is more likely when health - conscious is true .
this new concept is a hidden concept ( analogous to a hidden variable in bayesian networks ) .
it plays the same role as a regular primitive concept in the def - inition of the p - class , but it is not a primitive concept in the language and therefore does not appear in the terminology or in queries .
one promising alternative with greater expressive power arises from our recent work on functional stochastic pro - grams ( koller et al . , 123 ) .
the basic idea is that we view each p - class as a stochastic function , and each individual as a call to the function for that appropriate p - class .
a call to such a stochastic function chooses ( randomly ) the proper - ties of the individual for which it is called .
the properties of the llers for an individual are chosen by recursive calls to the functions for the appropriate p - classes .
once we view a ller as a function , we can pass the relevant properties of the individual ( e . g . , on - a - diet ) as a parameter to the function .
the parameters can directly inuence the distribution of the ller properties .
in our example of section 123 , the p - classes for carnivore food and herbivore food are equivalent to those that would have been obtained had we passed the value of animal to the ller as a parameter ( with a value of true and false respectively ) .
concepteats . low - fat .
the node representing cholesterol - a ( cid : 123 ) d whered is an arbitrary complex concept
what surprisingly , we can accomodate the functional view
in addition to taking arguments , a stochastic function can also return values .
in our context , this feature would allow properties of an object to depend on those of its llers .
thus , for example , we can have the probability over the cholesterol - level attribute depend on the actual fat content of the foods eaten by the person .
we could represent this type of dependency by introducing a node into the bayesian network corresponding to the complex
level can now be a child of this new node , encoding the desired dependency .
in general , we could have llers recur - sively pass back the truth value of deeply nested complex concepts .
this extension supports concept introductions
of llers fairly easily within our framework , and without a signicant increase in computational cost .
we defer details to the full version of this paper .
acknowledgements we thank manfred jaeger for useful discussions .
some of this work was done while daphne koller and avi pfeffer were visiting at&t research .
this work was also supported through the generosity of the pow - ell foundation , by onr grant n123 - 123 - 123 - 123 , and by darpa contract daca123 - 123 - c - 123 , under subcontract to information extraction and transport , inc .
