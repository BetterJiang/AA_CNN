support vector machines have met with signicant success in numerous real - world learning tasks .
however , like most machine learning algorithms , they are generally applied using a randomly selected training set classied in advance .
in many settings , we also have the option of using pool - based active learning .
instead of using a randomly selected training set , the learner has access to a pool of unlabeled instances and can request the labels for some number of them .
we introduce a new algorithm for performing active learning with support vector machines , i . e . , an algorithm for choosing which instances to request next .
we provide a theoretical motivation for the algorithm using the notion of a version space .
we present experimental results showing that employing our active learning method can signicantly reduce the need for labeled training instances in both the standard inductive and transductive settings .
keywords : active learning , selective sampling , support vector machines , classica - tion , relevance feedback
in many supervised learning tasks , labeling instances to create a training set is time - consuming and costly; thus , nding ways to minimize the number of labeled instances is benecial .
usually , the training set is chosen to be a random sampling of instances .
how - ever , in many cases active learning can be employed .
here , the learner can actively choose the training data .
it is hoped that allowing the learner this extra exibility will reduce the learners need for large quantities of labeled data .
pool - based active learning for classication was introduced by lewis and gale ( 123 ) .
the learner has access to a pool of unlabeled data and can request the true class label for a certain number of instances in the pool .
in many domains this is a reasonable approach since a large quantity of unlabeled data is readily available .
the main issue with active learning is nding a way to choose good requests or queries from the pool .
examples of situations in which pool - based active learning can be employed are : web searching .
a web - based company wishes to search the web for particular types of pages ( e . g . , pages containing lists of journal publications ) .
it employs a number of people to hand - label some web pages so as to create a training set for an automatic
c ( cid : 123 ) 123 simon tong and daphne koller .
tong and koller
classier that will eventually be used to classify the rest of the web .
since human expertise is a limited resource , the company wishes to reduce the number of pages the employees have to label .
rather than labeling pages randomly drawn from the web , the computer requests targeted pages that it believes will be most informative
email ltering .
the user wishes to create a personalized automatic junk email lter .
in the learning phase the automatic learner has access to the users past email les .
it interactively brings up past email and asks the user whether the displayed email is junk mail or not .
based on the users answer it brings up another email and queries the user .
the process is repeated some number of times and the result is an email lter tailored to that specic person .
relevance feedback .
the user wishes to sort through a database or website for items ( images , articles , etc . ) that are of personal interestan ill know it when i see it type of search .
the computer displays an item and the user tells the learner whether the item is interesting or not .
based on the users answer , the learner brings up another item from the database .
after some number of queries the learner then returns a number of items in the database that it believes will be of interest to the
the rst two examples involve induction .
the goal is to create a classier that works well on unseen future instances .
the third example is an example of transduction ( vapnik , 123 ) .
the learners performance is assessed on the remaining instances in the database rather than a totally independent test set .
we present a new algorithm that performs pool - based active learning with support vector machines ( svms ) .
we provide theoretical motivations for our approach to choosing the queries , together with experimental results showing that active learning with svms can signicantly reduce the need for labeled training instances .
we shall use text classication as a running example throughout this paper .
this is the task of determining to which pre - dened topic a given text document belongs .
text classication has an important role to play , especially with the recent explosion of readily available text data .
there have been many approaches to achieve this goal ( rocchio , 123 , dumais et al . , 123 , sebastiani , 123 ) .
furthermore , it is also a domain in which svms have shown notable success ( joachims , 123 , dumais et al . , 123 ) and it is of interest to see whether active learning can oer further improvement over this already highly eective
the remainder of the paper is structured as follows .
section 123 discusses the use of svms both in terms of induction and transduction .
section 123 then introduces the notion of a version space and section 123 provides theoretical motivation for three methods for performing active learning with svms .
in section 123 we present experimental results for two real - world text domains that indicate that active learning can signicantly reduce the need for labeled instances in practice .
we conclude in section 123 with some discussion of the potential signicance of our results and some directions for future work .
svm active learning with applications to text classification
figure 123 : ( a ) a simple linear support vector machine .
( b ) a svm ( dotted line ) and a
transductive svm ( solid line ) .
solid circles represent unlabeled instances .
support vector machines
support vector machines ( vapnik , 123 ) have strong theoretical foundations and excellent empirical successes .
they have been applied to tasks such as handwritten digit recognition , object recognition , and text classication .
123 svms for induction we shall consider svms in the binary classication setting .
we are given training data ( x123 .
xn ) that are vectors in some space x rd .
we are also given their labels ( y123 .
yn ) where yi ( 123 , 123 ) .
in their simplest form , svms are hyperplanes that separate the training data by a maximal margin ( see fig .
all vectors lying on one side of the hyperplane are labeled as 123 , and all vectors lying on the other side are labeled as 123
the training instances that lie closest to the hyperplane are called support vectors .
more generally , svms allow one to project the original training data in space x to a higher dimensional feature space f via a mercer kernel operator k .
in other words , we consider the set of classiers of the form :
when k satises mercers condition ( burges , 123 ) we can write : k ( u , v ) = ( u ) ( v ) where : x f and denotes an inner product .
we can then rewrite f as :
f ( x ) = w ( x ) , where w =
thus , by using k we are implicitly projecting the training data into a dierent ( often higher dimensional ) feature space f .
the svm then computes the is that correspond to the maximal margin hyperplane in f .
by choosing dierent kernel functions we can
tong and koller
implicitly project the training data from x into spaces f for which hyperplanes in f correspond to more complex decision boundaries in the original space x .
two commonly used kernels are the polynomial kernel given by k ( u , v ) = ( u v + 123 ) p which induces polynomial boundaries of degree p in the original space x 123 and the radial basis function kernel k ( u , v ) = ( e ( uv ) ( uv ) ) which induces boundaries by placing weighted gaussians upon key training instances .
for the majority of this paper we will assume that the modulus of the training data feature vectors are constant , i . e . , for all training instances xi , ( cid : 123 ) ( xi ) ( cid : 123 ) = for some xed .
the quantity ( cid : 123 ) ( xi ) ( cid : 123 ) is always constant for radial basis function kernels , and so the assumption has no eect for this kernel .
for ( cid : 123 ) ( xi ) ( cid : 123 ) to be constant with the polynomial kernels we require that ( cid : 123 ) xi ( cid : 123 ) be constant .
it is possible to relax this constraint on ( xi ) and we shall discuss this at the end of section 123
123 svms for transduction the previous subsection worked within the framework of induction .
there was a labeled training set of data and the task was to create a classier that would have good performance on unseen test data .
in addition to regular induction , svms can also be used for transduc - tion .
here we are rst given a set of both labeled and unlabeled data .
the learning task is to assign labels to the unlabeled data as accurately as possible .
svms can perform trans - duction by nding the hyperplane that maximizes the margin relative to both the labeled and unlabeled data .
see figure 123b for an example .
recently , transductive svms ( tsvms ) have been used for text classication ( joachims , 123b ) , attaining some improvements in precision / recall breakeven performance over regular inductive svms .
version space
given a set of labeled training data and a mercer kernel k , there is a set of hyperplanes that separate the data in the induced feature space f .
we call this set of consistent hypotheses the version space ( mitchell , 123 ) .
in other words , hypothesis f is in version space if for every training instance xi with label yi we have that f ( xi ) > 123 if yi = 123 and f ( xi ) < 123 if yi = 123
more formally : denition 123 our set of possible hypotheses is given as :
f | f ( x ) =
where w w
where our parameter space w is simply equal to f .
the version space , v is then dened
v = ( f h | i ( 123 .
n ) yif ( xi ) > 123 ) .
notice that since h is a set of hyperplanes , there is a bijection between unit vectors w and hypotheses f in h .
thus we will redene v as :
v = ( w w | ( cid : 123 ) w ( cid : 123 ) = 123 , yi ( w ( xi ) ) > 123 , i = 123
we have not introduced a bias weight in eq .
thus , the simple euclidean inner product will produce hyperplanes that pass through the origin .
however , a polynomial kernel of degree one induces hyperplanes that do not need to pass through the origin .
svm active learning with applications to text classification
figure 123 : ( a ) version space duality .
the surface of the hypersphere represents unit weight vectors .
each of the two hyperplanes corresponds to a labeled training instance .
each hyperplane restricts the area on the hypersphere in which consistent hy - potheses can lie .
here , the version space is the surface segment of the hypersphere closest to the camera .
( b ) an svm classier in a version space .
the dark em - bedded sphere is the largest radius sphere whose center lies in the version space and whose surface does not intersect with the hyperplanes .
the center of the em - bedded sphere corresponds to the svm , its radius is proportional to the margin of the svm in f , and the training points corresponding to the hyperplanes that it touches are the support vectors .
note that a version space only exists if the training data are linearly separable in the feature space .
thus , we require linear separability of the training data in the feature space .
this restriction is much less harsh than it might at rst seem .
first , the feature space often has a very high dimension and so in many cases it results in the data set being linearly separable .
second , as noted by shawe - taylor and cristianini ( 123 ) , it is possible to modify any kernel so that the data in the new induced feature space is linearly separable123
there exists a duality between the feature space f and the parameter space w ( vapnik , 123 , herbrich et al . , 123 ) which we shall take advantage of in the next section : points in f correspond to hyperplanes in w and vice versa .
by denition , points in w correspond to hyperplanes in f .
the intuition behind the converse is that observing a training instance xi in the feature space restricts the set of separating hyperplanes to ones that classify xi correctly .
in fact , we can show that the set 123
this is done by redening for all training instances xi : k ( xi , xi ) k ( xi , xi ) + where is a positive regularization constant .
this essentially achieves the same eect as the soft margin error function ( cortes and vapnik , 123 ) commonly used in svms .
it permits the training data to be linearly non - separable in the original feature space .
tong and koller
of allowable points w in w is restricted to lie on one side of a hyperplane in w .
more formally , to show that points in f correspond to hyperplanes in w , suppose we are given a new training instance xi with label yi .
then any separating hyperplane must satisfy yi ( w ( xi ) ) > 123
now , instead of viewing w as the normal vector of a hyperplane in f , think of ( xi ) as being the normal vector of a hyperplane in w .
thus yi ( w ( xi ) ) > 123 denes a half space in w .
furthermore w ( xi ) = 123 denes a hyperplane in w that acts as one of the boundaries to version space v .
notice that the version space is a connected region on the surface of a hypersphere in parameter space .
see figure 123a for an example .
svms nd the hyperplane that maximizes the margin in the feature space f .
one way
to pose this optimization task is as follows :
( cid : 123 ) w ( cid : 123 ) = 123
yi ( w ( xi ) ) > 123 i = 123
by having the conditions ( cid : 123 ) w ( cid : 123 ) = 123 and yi ( w ( xi ) ) > 123 we cause the solution to lie in the version space .
now , we can view the above problem as nding the point w in the version space that maximizes the distance : mini ( yi ( w ( xi ) ) ) .
from the duality between feature and parameter space , and since ( cid : 123 ) ( xi ) ( cid : 123 ) = , each ( xi ) / is a unit normal vector of a hyperplane in parameter space .
because of the constraints yi ( w ( xi ) ) > 123 i = 123 .
n each of these hyperplanes delimit the version space .
the expression yi ( w ( xi ) ) can be
the distance between the point w and the hyperplane with normal vector ( xi ) .
thus , we want to nd the point w in the version space that maximizes the minimum distance to any of the delineating hyperplanes .
that is , svms nd the center of the largest radius hypersphere whose center can be placed in the version space and whose surface does not intersect with the hyperplanes corresponding to the labeled instances , as in figure 123b .
the normals of the hyperplanes that are touched by the maximal radius hypersphere are the ( xi ) for which the distance yi ( w ( xi ) ) is minimal .
now , taking the original rather than the dual view , and regarding w as the unit normal vector of the svm and ( xi ) as points in feature space , we see that the hyperplanes that are touched by the maximal radius hypersphere correspond to the support vectors ( i . e . , the labeled points that are closest to the svm hyperplane boundary ) .
the radius of the sphere is the distance from the center of the sphere to one of the touching hyperplanes and is given by yi ( w ( xi ) / ) where ( xi ) is a support vector .
now , viewing w as a unit normal vector of the svm and ( xi ) as points in feature space , we have that the distance yi ( w ( xi ) / ) is :
the distance between support vector ( xi ) and the hyperplane with normal vector w ,
which is the margin of the svm divided by .
thus , the radius of the sphere is proportional to the margin of the svm .
svm active learning with applications to text classification
active learning
it is assumed that in pool - based active learning we have a pool of unlabeled instances .
the instances x are independently and identically distributed according to some underlying distribution f ( x ) and the labels are distributed according to some conditional distribution p ( y | x ) .
given an unlabeled pool u , an active learner ( cid : 123 ) has three components : ( f , q , x ) .
the rst component is a classier , f : x ( 123 , 123 ) , trained on the current set of labeled data x ( and possibly unlabeled instances in u too ) .
the second component q ( x ) is the querying function that , given a current labeled set x , decides which instance in u to query next .
the active learner can return a classier f after each query ( online learning ) or after some xed number of queries .
the main dierence between an active learner and a passive learner is the querying component q .
this brings us to the issue of how to choose the next unlabeled instance to query .
similar to seung et al .
( 123 ) , we use an approach that queries points so as to attempt to reduce the size of the version space as much as possible .
we take a myopic approach that greedily chooses the next query based on this criterion .
we also note that myopia is a standard approximation used in sequential decision making problems horvitz and rutledge ( 123 ) , latombe ( 123 ) , heckerman et al .
( 123 ) .
we need two more denitions before we
denition 123 area ( v ) is the surface area that the version space v occupies on the hyper - sphere ( cid : 123 ) w ( cid : 123 ) = 123
denition 123 given an active learner ( cid : 123 ) , let vi denote the version space of ( cid : 123 ) after i queries have been made .
now , given the ( i + 123 ) th query xi+123 , dene :
i = vi ( w w | ( w ( xi+123 ) ) > 123 ) , i = vi ( w w | + ( w ( xi+123 ) ) > 123 ) .
i and v +
123 and 123 respectively .
i denote the resulting version spaces when the next query xi+123 is labeled as
we wish to reduce the version space as fast as possible .
intuitively , one good way of doing this is to choose a query that halves the version space .
the follow lemma says that , for any given number of queries , the learner that chooses successive queries that halves the version spaces is the learner that minimizes the maximum expected size of the version space , where the maximum is taken over all conditional distributions of y given x :
tong and koller
lemma 123 suppose we have an input space x , nite dimensional feature space f ( induced via a kernel k ) , and parameter space w .
suppose active learner ( cid : 123 ) always queries instances whose corresponding hyperplanes in parameter space w halves the area of the current version space .
let ( cid : 123 ) be any other active learner .
denote the version spaces of ( cid : 123 ) and ( cid : 123 ) after i queries i and vi respectively .
let p denote the set of all conditional distributions of y given x .
i n+ sup
i ) ) sup
with strict inequality whenever there exists a query j ( 123 .
i ) by ( cid : 123 ) that does not halve version space vj123
the proof is straightforward .
the learner , ( cid : 123 ) always chooses to query instances that halve the version space .
thus area ( v i ) no matter what the labeling of the query points are .
let r denote the dimension of feature space f .
then r is also the dimension of the parameter space w .
let sr denote the surface area of the unit hypersphere of dimension r .
then , under any conditional distribution p , area ( v
i+123 ) = 123
i ) = sr / 123i .
now , suppose ( cid : 123 ) does not always query an instance that halves the area of the version space .
then after some number , k , of queries ( cid : 123 ) rst chooses to query a point xk+123 that does not halve the current version space vk .
let yk+123 ( 123 , 123 ) correspond to the labeling of xk+123 that will cause the larger half of the version space to be chosen .
k ) and so yk+123 = 123
note that
k ) = sr / 123k , so we have that area ( v
k ) > area ( v +
k ) + area ( v +
without loss of generality assume area ( v
now consider the conditional distribution p123 :
k ) > sr / 123k+123
p123 ( 123 | x ) =
if x ( cid : 123 ) = xk+123 if x = xk+123
then under this distribution , i > k ,
hence , i > k ,
i ) ) > sup
now , suppose w w is the unit parameter vector corresponding to the svm that we would have obtained had we known the actual labels of all of the data in the pool .
we know that w must lie in each of the version spaces v123 v123 v123 .
. , where vi denotes the version space after i queries .
thus , by shrinking the size of the version space as much as possible with each query , we are reducing as fast as possible the space in which w can lie .
hence , the svm that we learn from our limited number of queries will lie close to w .
if one is willing to assume that there is a hypothesis lying within h that generates the data and that the generating hypothesis is deterministic and that the data are noise free , then strong generalization performance properties of an algorithm that halves version space can also be shown ( freund et al . , 123 ) .
for example one can show that the generalization error decreases exponentially with the number of queries .
svm active learning with applications to text classification
figure 123 : ( a ) simple margin will query b .
( b ) simple margin will query a .
figure 123 : ( a ) maxmin margin will query b .
the two svms with margins m and m+ for b are shown .
( b ) ratio margin will query e .
the two svms with margins m and m+ for e are shown .
this discussion provides motivation for an approach where we query instances that split the current version space into two equal parts as much as possible .
given an unlabeled instance x from the pool , it is not practical to explicitly compute the sizes of the new version spaces v and v + ( i . e . , the version spaces obtained when x is labeled as 123 and +123 respectively ) .
we next present three ways of approximating this procedure .
simple margin .
recall from section 123 that , given some data ( x123 .
xi ) and labels ( y123 .
yi ) , the svm unit vector wi obtained from this data is the center of the largest hypersphere that can t inside the current version space vi .
the position of wi in the version space vi clearly depends on the shape of the region vi , however it is often approximately in the center of the version space .
now , we can test each of the unlabeled instances x in the pool to see how close their corresponding hyperplanes in w come to the centrally placed wi .
the closer a hyperplane in w is to the point wi , the more centrally it is placed in the version space , and the more it bisects the version space .
thus we can pick the unlabeled instance in the pool whose hyperplane
tong and koller
in w comes closest to the vector wi .
for each unlabeled instance x , the shortest distance between its hyperplane in w and the vector wi is simply the distance between the feature vector ( x ) and the hyperplane wi in fwhich is easily computed by |wi ( x ) | .
this results in the natural rule : learn an svm on the existing labeled data and choose as the next instance to query the instance that comes closest to the hyperplane in f .
figure 123a presents an illustration .
in the stylized picture we have attened out the surface of the unit weight vector hypersphere that appears in figure 123a .
the white area is version space vi which is bounded by solid lines corresponding to labeled instances .
the ve dotted lines represent unlabeled instances in the pool .
the circle represents the largest radius hypersphere that can t in the version space .
note that the edges of the circle do not touch the solid linesjust as the dark sphere in 123b does not meet the hyperplanes on the surface of the larger hypersphere ( they meet somewhere under the surface ) .
the instance b is closest to the svm wi and so we will choose to query b .
maxmin margin .
the simple margin method can be a rather rough approximation .
it relies on the assumption that the version space is fairly symmetric and that wi is centrally placed .
it has been demonstrated , both in theory and practice , that these assumptions can fail signicantly ( herbrich et al . , 123 ) .
indeed , if we are not careful we may actually query an instance whose hyperplane does not even intersect the version space .
the maxmin approximation is designed to overcome these problems to some degree .
given some data ( x123 .
xi ) and labels ( y123 .
yi ) , the svm unit vector wi is the center of the largest hypersphere that can t inside the current version space vi and the radius mi of the hypersphere is proportional123 to the size of the margin of wi .
we can use the radius mi as an indication of the size of the version space ( vapnik , 123 ) .
suppose we have a candidate unlabeled instance x in the pool .
we can estimate the relative size of the resulting version space v by labeling x as 123 , nding the svm obtained from adding x to our labeled training data and looking at the size of its margin m .
we can perform a similar calculation for v + by relabeling x as class +123 and nding the resulting svm to obtain margin m+ .
since we want an equal split of the version space , we wish area ( v ) and area ( v + ) to be similar .
now , consider min ( area ( v ) , area ( v + ) ) .
it will be small if area ( v ) and area ( v + ) are very dierent .
thus we will consider min ( m , m+ ) as an approximation and we will choose to query the x for which this quantity is largest .
hence , the maxmin query algorithm is as follows : for each unlabeled instance x compute the margins m and m+ of the svms obtained when we label x as 123 and +123 respectively; then choose to query the unlabeled instance for which the quantity min ( m , m+ ) is greatest .
figures 123b and 123a show an example comparing the simple margin and maxmin margin
ratio margin .
this method is similar in spirit to the maxmin margin method .
we use m and m+ as indications of the sizes of v and v + .
however , we shall try to
to ease notation , without loss of generality we shall assume the the constant of proportionality is 123 , i . e . ,
the radius is equal to the margin .
svm active learning with applications to text classification
take into account the fact that the current version space vi may be quite elongated and for some x in the pool both m and m+ may be small simply because of the shape of version space .
thus we will instead look at the relative sizes of m and m+ and choose to query the x for which min ( m
m ) is largest ( see figure 123b ) .
m+ , m+
the above three methods are approximations to the querying component that always halves version space .
after performing some number of queries we then return a classier by learning a svm with the labeled instances .
the margin can be used as an indication of the version space size irrespective of whether the feature vectors have constant modulus .
thus the explanation for the maxmin and ratio methods still holds even without the constraint on the modulus of the training feature vectors .
the simple method can still be used when the training feature vectors do not have constant modulus , but the motivating explanation no longer holds since the maximal margin hyperplane can no longer be viewed as the center of the largest allowable sphere .
however , for the simple method , alternative motivations have recently been proposed by campbell et al .
( 123 ) that do not require the constraint on the modulus .
for inductive learning , after performing some number of queries we then return a classi - er by learning a svm with the labeled instances .
for transductive learning , after querying some number of instances we then return a classier by learning a transductive svm with the labeled and unlabeled instances .
for our empirical evaluation of the above methods we used two real - world text classication domains : the reuters - 123 data set and the newsgroups data set .
123 reuters data collection experiments
the reuters - 123 data set123 is a commonly used collection of newswire stories categorized into hand - labeled topics .
each news story has been hand - labeled with some number of topic labels such as corn , wheat and corporate acquisitions .
note that some of the topics overlap and so some articles belong to more than one category .
we used the 123 articles from the modapte split of the data123 and , to stay comparable with previous studies , we considered the top ten most frequently occurring topics .
we learned ten dierent binary classiers , one to distinguish each topic .
each document was represented as a stemmed , tfidf - weighted word frequency vector . 123 each vector had unit modulus .
a stop list of common words was used and words occurring in fewer than three documents were also ignored .
using this representation , the document vectors had about 123 dimensions .
we rst compared the three querying methods in the inductive learning setting
test set consisted of the 123 documents present in the modapte test set .
obtained from www . research . att . com / lewis .
the reuters - 123 collection comes with a set of predened training and test set splits .
the commonly usedmodapte split lters out duplicate articles and those without a labeled topic , and then uses earlier articles as the training set and later articles as the test set .
we used rainbow ( mccallum , 123 ) for text processing .
tong and koller
labeled training set size
labeled training set size
figure 123 : ( a ) average test set accuracy over the ten most frequently occurring topics when using a pool size of 123
( b ) average test set precision / recall breakeven point over the ten most frequently occurring topics when using a pool size of 123
123 123 123 123 123 123
table 123 : average test set accuracy over the top ten most frequently occurring topics ( most frequent topic rst ) when trained with ten labeled documents .
boldface indicates
for each of the ten topics we performed the following steps .
we created a pool of unlabeled data by sampling 123 documents from the remaining data and removing their labels .
we then randomly selected two documents in the pool to give as the initial labeled training set .
one document was about the desired topic , and the other document was not about the topic .
thus we gave each learner 123 unlabeled documents and 123 labeled documents .
after a xed number of queries we asked each learner to return a classier ( an
svm active learning with applications to text classification
123 123 123 123 123 123 123 123 123 123 123 123
table 123 : average test set precision / recall breakeven point over the top ten most frequently occurring topics ( most frequent topic rst ) when trained with ten labeled docu - ments .
boldface indicates statistical signicance .
svm with a polynomial kernel of degree one123 learned on the labeled training documents ) .
we then tested the classier on the independent test set .
the above procedure was repeated thirty times for each topic and the results were averaged .
we considered the simple margin , maxmin margin and ratio margin querying methods as well as a random sample method .
the random sample method simply ran - domly chooses the next query point from the unlabeled pool .
this last method reects what happens in the regular passive learning settingthe training set is a random sampling of
to measure performance we used two metrics :
test set classication error and , to stay compatible with previous reuters corpus results , the precision / recall breakeven point ( joachims , 123 ) .
precision is the percentage of documents a classier labels as relevant that are really relevant .
recall is the percentage of relevant documents that are labeled as relevant by the classier .
by altering the decision threshold on the svm we can trade pre - cision for recall and can obtain a precision / recall curve for the test set .
the precision / recall breakeven point is a one number summary of this graph : it is the point at which precision
figures 123a and 123b present the average test set accuracy and precision / recall breakeven points over the ten topics as we vary the number of queries permitted .
the horizontal line is the performance level achieved when the svm is trained on all 123 labeled documents comprising the pool .
over the reuters corpus , the three active learning methods perform almost identically with little notable dierence to distinguish between them .
each method also appreciably outperforms random sampling .
tables 123 and 123 show the test set accuracy and breakeven performance of the active methods after they have asked for just eight labeled instances ( so , together with the initial two random instances , they have seen ten labeled instances ) .
they demonstrate that the three active methods perform similarly on this
for svm and transductive svm learning we used svmlight joachims ( 123a ) .
tong and koller
labeled training set size
labeled training set size
figure 123 : ( a ) average test set accuracy over the ten most frequently occurring topics when using a pool size of 123
( b ) average test set precision / recall breakeven point over the ten most frequently occurring topics when using a pool size of 123
reuters data set after eight queries , with the maxmin and ratio showing a very slight edge in performance .
the last columns in each table are of more interest .
they show approximately how many instances would be needed if we were to use random to achieve the same level of performance as the ratio active learning method .
in this instance , passive learning on average requires over six times as much data to achieve comparable levels of performance as the active learning methods .
the tables indicate that active learning provides more benet with the infrequent classes , particularly when measuring performance by the precision / recall breakeven point .
this last observation has also been noted before in previous empirical tests ( mccallum and nigam , 123 ) .
we noticed that approximately half of the queries that the active learning methods asked tended to turn out to be positively labeled , regardless of the true overall proportion of positive instances in the domain .
we investigated whether the gains that the active learning methods had over regular random sampling were due to this biased sampling .
we created a new querying method called balancedrandom which would randomly sample an equal number of positive and negative instances from the pool .
obviously in practice the ability to randomly sample an equal number of positive and negative instances without having to label an entire pool of instances rst may or may not be reasonable depending upon the domain in question .
figures 123a and 123b show the average accuracy and breakeven point of the balancedrandom method compared with the ratio active method and regular random method on the reuters dataset with a pool of 123 unlabled instances .
the ratio and random curves are the same as those shown in figures 123a and 123b .
the maxmin and simple curves are omitted to ease legibility .
the balancedrandom method has a much bet - ter precision / recall breakeven performance than the regular random method , although it is still matched and then outperformed by the active method .
for classication accuracy , the balancedrandom method initially has extremely poor performance ( less than 123% which is
svm active learning with applications to text classification
figure 123 : ( a ) average test set accuracy over the ten most frequently occurring topics when using a pool sizes of 123 and 123
( b ) average breakeven point over the ten most frequently occurring topics when using a pool sizes of 123 and 123
even worse than pure random guessing ) and is always consistently and signicantly out - performed by the active method .
this indicates that the performance gains of the active methods are not merely due to their ability to bias the class of the instances they queries .
the active methods are choosing special targeted instances and approximately half of these instances happen to have positive labels .
figures 123a and 123b show the average accuracy and breakeven point of the ratio method with two dierent pool sizes .
clearly the random sampling methods performance will not be aected by the pool size .
however , the graphs indicate that increasing the pool of unlabeled data will improve both the accuracy and breakeven performance of active learning .
this is quite intuitive since a good active method should be able to take advantage of a larger pool of potential queries and ask more targeted questions .
we also investigated active learning in a transductive setting .
here we queried the points as usual except now each method ( simple and random ) returned a transductive svm trained on both the labeled and remaining unlabeled data in the pool .
as described by joachims ( 123 ) the breakeven point for a tsvm was computed by gradually altering the number of unlabeled instances that we wished the tsvm to label as positive .
this invovles re - learning the tsvm multiple times and was computationally intensive .
since our setting was transduction , the performance of each classier was measured on the pool of data rather than a separate test set .
this reects the relevance feedback transductive inference example presented in the introduction .
figure 123 shows that using a tsvm provides a slight advantage over a regular svm in both querying methods ( random and simple ) when comparing breakeven points .
however , the graph also shows that active learning provides notably more benet than transduction indeed using a tsvm with a random querying method needs over 123 queries to achieve
tong and koller
labeled training set size
figure 123 : average pool set precision / recall breakeven point over the ten most frequently
occurring topics when using a pool size of 123
labeled training set size
labeled training set size
figure 123 : ( a ) average test set accuracy over the ve comp .
topics when using a pool size of 123
( b ) average test set accuracy for comp . sys . ibm . pc . hardware with a 123
the same breakeven performance as a regular svm with a simple method that has only seen 123 labeled instances .
123 newsgroups experiments
our second data collection was k .
langs newsgroups collection lang ( 123 ) .
we used the ve comp .
groups , discarding the usenet headers and subject lines .
we processed the text documents exactly as before , resulting in vectors of about 123 dimensions .
svm active learning with applications to text classification
figure 123 : ( a ) a simple example of querying unlabeled clusters .
( b ) macro - average test set accuracy for comp . os . ms - windows . misc and comp . sys . ibm . pc . hardware where hybrid uses the ratio method for the rst ten queries and simple for the rest .
we placed half of the 123 documents aside to use as an independent test set , and repeatedly , randomly chose a pool of 123 documents from the remaining instances .
we performed twenty runs for each of the ve topics and averaged the results .
we used test set accuracy to measure performance .
figure 123a contains the learning curve ( averaged over all of the results for the ve comp .
topics ) for the three active learning methods and random sampling .
again , the horizontal line indicates the performance of an svm that has been trained on the entire pool .
there is no appreciable dierence between the maxmin and ratio methods but , in two of the ve newsgroups ( comp . sys . ibm . pc . hardware and comp . os . ms - windows . misc ) the simple active learning method performs notably worse than the maxmin and ratio methods .
figure 123b shows the average learning curve for the comp . sys . ibm . pc . hardware topic .
in around ten to fteen per cent of the runs for both of the two newsgroups the simple method was misled and performed extremely poorly ( for instance , achieving only 123% accuracy even with fty training instances , which is worse than just randomly guessing a label ! ) .
this indicates that the simple querying method may be more unstable than the other two methods .
one reason for this could be that the simple method tends not to explore the feature space as aggressively as the other active methods , and can end up ignoring entire clusters of unlabeled instances .
in figure 123a , the simple method takes several queries before it even considers an instance in the unlabeled cluster while both the maxmin and ratio query a point in the unlabeled cluster immediately .
while maxmin and ratio appear more stable they are much more computationally in - tensive .
with a large pool of s instances , they require about 123s svms to be learned for each query .
most of the computational cost is incurred when the number of queries that have already been asked is large .
the reason is that the cost of training an svm grows polynomi - ally with the size of the labeled training set and so now training each svm is costly ( taking
tong and koller
query simple maxmin ratio hybrid
table 123 : typical run times in seconds for the active methods on the newsgroups dataset
over 123 seconds to generate the 123th query on a sun ultra 123 123mhz workstation with a pool of 123 documents ) .
however , when the quantity of labeled data is small , even with a large pool size , maxmin and ratio are fairly fast ( taking a few seconds per query ) since now training each svm is fairly cheap .
interestingly , it is in the rst ten queries that the simple seems to suer the most through its lack of aggressive exploration .
this motivates a hybrid method .
we can use maxmin or ratio for the rst few queries and then use the simple method for the rest .
experiments with the hybrid method show that it maintains the stability of the maxmin and ratio methods while allowing the scalability of the simple method .
figure 123b compares the hybrid method with the ratio and simple methods on the two newsgroups for which the simple method performed poorly .
the test set accuracy of the hybrid method is virtually identical to that of the ratio method while the hybrid methods run time was about the same as the simple method , as indicated by table 123
related work
there have been several studies of active learning for classication .
the query by com - mittee algorithm ( seung et al . , 123 , freund et al . , 123 ) uses a prior distribution over hypotheses .
this general algorithm has been applied in domains and with classiers for which specifying and sampling from a prior distribution is natural .
they have been used with probabilistic models ( dagan and engelson , 123 ) and specically with the naive bayes model for text classication in a bayesian learning setting ( mccallum and nigam , 123 ) .
the naive bayes classier provides an interpretable model and principled ways to incorpo - rate prior knowledge and data with missing values .
however , it typically does not perform as well as discriminative methods such as svms , particularly in the text classication do - main ( joachims , 123 , dumais et al . , 123 ) .
we re - created mccallum and nigams ( 123 ) experimental setup on the reuters - 123 corpus and compared the reported results from their algorithm ( which we shall call the mn - algorithm hereafter ) with ours .
in line with their experimental setup , queries were asked ve at a time , and this was achieved by picking the ve instances closest to the current hyperplane .
figure 123a compares mccallum and nigams reported results with ours .
the graph indicates that the active svm performance is signicantly better than that of the
an alternative committee approach to query by committee was explored by liere and tadepalli ( 123 , 123 ) .
although their algorithm ( lt - algorithm hereafter ) lacks the the -
svm active learning with applications to text classification
svm simple active
labeled training set size
svm simple active ltalgorithm winnow active ltalgorthm winnow passive
labeled training set size
figure 123 : ( a ) average breakeven point performance over the corn , trade and acq reuters - 123 categories .
( b ) average test set accuracy over the top ten reuters - 123
oretical justications of the query by committee algorithm , they successfully used their committee based active learning method with winnow classiers in the text categorization domain .
figure 123b was produced by emulating their experimental setup on the reuters - 123 data set and it compares their reported results with ours .
their algorithm does not require a positive and negative instance to seed their classier .
rather than seeding our active svm with a positive and negative instance ( which would give the active svm an unfair advantage ) the active svm randomly sampled 123 documents for its rst 123 queries .
this process virtually guaranteed that the training set contained at least one posi - tive instance .
the active svm then proceeded to query instances actively using the simple method .
despite the very naive initialization policy for the active svm , the graph shows that the active svm accuracy is signicantly better than that of the lt - algorithm .
lewis and gale ( 123 ) introduced uncertainty sampling and applied it to a text domain using logistic regression and , in a companion paper , using decision trees ( lewis and catlett , 123 ) .
the simple querying method for svm active learning is essentially the same as their uncertainty sampling method ( choose the instance that our current classier is most uncer - tain about ) , however they provided substantially less justication as to why the algorithm should be eective .
they also noted that the performance of the uncertainty sampling method can be variable , performing quite poorly on occasions .
two other studies ( campbell et al . , 123 , schohn and cohn , 123 ) independently de - veloped our simple method for active learning with support vector machines and provided dierent formal analyses .
campbell , cristianini and smola extend their analysis for the simple method to cover the use of soft margin svms ( cortes and vapnik , 123 ) with lin - early non - separable data .
schohn and cohn note interesting behaviors of the active learning curves in the presence of outliers .
tong and koller
conclusions and future work
we have introduced a new algorithm for performing active learning with svms .
by taking advantage of the duality between parameter space and feature space , we arrived at three algorithms that attempt to reduce version space as much as possible at each query .
we have shown empirically that these techniques can provide considerable gains in both the inductive and transductive settingsin some cases shrinking the need for labeled instances by over an order of magnitude , and in almost all cases reaching the performance achievable on the entire pool having seen only a fraction of the data .
furthermore , larger pools of unlabeled data improve the quality of the resulting classier .
of the three main methods presented , the simple method is computationally the fastest .
however , the simple method seems to be a rougher and more unstable approximation , as we witnessed when it performed poorly on two of the ve newsgroup topics .
if asking each query is expensive relative to computing time then using either the maxmin or ratio may be preferable .
however , if the cost of asking each query is relatively cheap and more emphasis is placed upon fast feedback then the simple method may be more suitable .
in either case , we have shown that the use of these methods for learning can substantially outperform standard passive learning .
furthermore , experiments with the hybrid method indicate that it is possible to combine the benets of the ratio and simple methods .
the work presented here leads us to many directions of interest .
several studies have noted that gains in computational speed can be obtained at the expense of generalization performance by querying multiple instances at a time ( lewis and gale , 123 , mccallum and nigam , 123 ) .
viewing svms in terms of the version space gives an insight as to where the approximations are being made , and this may provide a guide as to which multiple instances are better to query .
for instance , it is suboptimal to query two instances whose version space hyperplanes are fairly parallel to each other .
so , with the simple method , instead of blindly choosing to query the two instances that are the closest to the current svm , it may be better to query two instances that are close to the current svm and whose hyperplanes in the version space are fairly perpendicular .
similar tradeos can be made for the ratio and maxmin methods .
bayes point machines ( herbrich et al . , 123 ) approximately nd the center of mass of the version space .
using the simple method with this point rather than the svm point in the version space may produce an improvement in performance and stability .
the use of monte carlo methods to estimate version space areas may also give improvements .
one way of viewing the strategy of always choosing to halve the version space is that we have essentially placed a uniform distribution over the current space of consistent hypotheses and we wish to reduce the expected size of the version space as fast as possible .
rather than maintaining a uniform distribution over consistent hypotheses , it is plausible that the addition of prior knowledge over our hypotheses space may allow us to modify our query algorithm and provided us with an even better strategy .
furthermore , the pac - bayesian framework introduced by mcallester ( 123 ) considers the eect of prior knowledge on generalization bounds and this approach may lead to theoretical guarantees for the modied querying algorithms .
finally , the ratio and maxmin methods are computationally expensive since they have to step through each of the unlabeled data instances and learn an svm for each possible
svm active learning with applications to text classification
labeling .
however , the temporarily modied data sets will only dier by one instance from the original labeled data set and so one can envisage learning an svm on the original data set and then computing the incremental updates to obtain the new svms ( cauwenberghs and poggio , 123 ) for each of the possible labelings of each of the unlabeled instances .
thus , one would hopefully obtain a much more ecient implementation of the ratio and maxmin methods and hence allow these active learning algorithms to scale up to larger problems .
this work was supported by darpas information assurance program under subcontract to sri international , and by aro grant daah123 - 123 - 123 - 123 under the muri program integrated approach to intelligent systems .
