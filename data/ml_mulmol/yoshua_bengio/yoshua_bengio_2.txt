the success of machine learning algorithms generally depends on data representation , and we hypothesize that this is because different representations can entangle and hide more or less the different ex - planatory factors of variation behind the data .
although specic domain knowledge can be used to help design representations , learning with generic priors can also be used , and the quest for ai is motivating the design of more powerful representation - learning algorithms imple - menting such priors .
this paper reviews recent work in the area of unsupervised feature learning and deep learning , covering advances in probabilistic models , auto - encoders , manifold learning , and deep networks .
this motivates longer - term unanswered questions about the appropriate objectives for learning good representations , for computing representations ( i . e . , inference ) , and the geometrical connections be - tween representation learning , density estimation and manifold learning .
index termsdeep learning , representation learning , feature learning , unsupervised learning , boltzmann machine , autoencoder , neural nets
the performance of machine learning methods is heavily dependent on the choice of data representation ( or features ) on which they are applied .
for that reason , much of the actual effort in deploying machine learning algorithms goes into the design of preprocessing pipelines and data transformations that result in a representation of the data that can support effective machine learning .
such feature engineering is important but labor - intensive and highlights the weakness of current learning algorithms : their inability to extract and organize the discrimi - native information from the data .
feature engineering is a way to take advantage of human ingenuity and prior knowledge to compensate for that weakness .
in order to expand the scope and ease of applicability of machine learning , it would be highly desirable to make learning algorithms less dependent on feature engineering , so that novel applications could be constructed faster , and more importantly , to make progress towards articial intelligence ( ai ) .
an ai must fundamentally understand the world around us , and we argue that this can only be achieved if it can learn to identify and disentangle the underlying explanatory factors hidden in the observed milieu of low - level sensory data .
this paper is about representation learning , i . e . , learning representations of the data that make it easier to extract useful information when building classiers or other predictors .
in the case of probabilistic models , a good representation is often one that captures the posterior distribution of the underlying
explanatory factors for the observed input .
a good representa - tion is also one that is useful as input to a supervised predictor .
among the various ways of learning representations , this paper focuses on deep learning methods : those that are formed by the composition of multiple non - linear transformations , with the goal of yielding more abstract and ultimately more useful representations .
here we survey this rapidly developing area with special emphasis on recent progress .
we consider some of the fundamental questions that have been driving research in this area .
specically , what makes one representation better than another ? given an example , how should we compute its representation , i . e .
perform feature extraction ? also , what are appropriate objectives for learning good representations ? 123 why should we care about learning representation learning has become a eld in itself in the machine learning community , with regular workshops at the leading conferences such as nips and icml , and a new conference dedicated to it , iclr123 , sometimes under the header of deep learning or feature learning .
although depth is an important part of the story , many other priors are interesting and can be conveniently captured when the problem is cast as one of learning a representation , as discussed in the next sec - tion .
the rapid increase in scientic activity on representation learning has been accompanied and nourished by a remarkable string of empirical successes both in academia and in industry .
below , we briey highlight some of these high points .
speech recognition and signal processing
speech was one of the early applications of neural networks , in particular convolutional ( or time - delay ) neural networks 123
the recent revival of interest in neural networks , deep learning , and representation learning has had a strong impact in the area of speech recognition , with breakthrough results ( dahl et al . , 123; deng et al . , 123; seide et al . , 123a; mohamed et al . , 123; dahl et al . , 123; hinton et al . , 123 ) obtained by several academics as well as researchers at industrial labs bringing these algorithms to a larger scale and into products .
for example , microsoft has released in 123 a new version of their mavis ( microsoft audio video indexing service )
international conference on learning representations 123
see bengio ( 123 ) for a review of early work in this area .
speech system based on deep learning ( seide et al . , 123a ) .
these authors managed to reduce the word error rate on four major benchmarks by about 123% ( e . g .
from 123% to 123% on rt123s ) compared to state - of - the - art models based on gaussian mixtures for the acoustic modeling and trained on the same amount of data ( 123 hours of speech ) .
the relative improvement in error rate obtained by dahl et al .
( 123 ) on a smaller large - vocabulary speech recognition benchmark ( bing mobile business search dataset , with 123 hours of speech ) is between 123% and 123% .
representation - learning algorithms have also been applied to music , substantially beating the state - of - the - art in poly - phonic transcription ( boulanger - lewandowski et al . , 123 ) , with relative error improvement between 123% and 123% on a standard benchmark of 123 datasets .
deep learning also helped to win mirex ( music information retrieval ) competitions , e . g .
in 123 on audio tagging ( hamel et al . , 123 ) .
the beginnings of deep learning in 123 have focused on the mnist digit image classication problem ( hinton et al . , 123; bengio et al . , 123 ) , breaking the supremacy of svms ( 123% error ) on this dataset123
the latest records are still held by deep networks : ciresan et al .
( 123 ) currently claims the title of state - of - the - art for the unconstrained version of the task ( e . g . , using a convolutional architecture ) , with 123% error , and rifai et al .
( 123c ) is state - of - the - art for the knowledge - free version of mnist , with 123% error .
in the last few years , deep learning has moved from digits to object recognition in natural images , and the latest breakthrough has been achieved on the imagenet dataset123 bringing down the state - of - the - art error rate from 123% to 123% ( krizhevsky et al . , 123 ) .
natural language processing
besides speech recognition , there are many other natural language processing ( nlp ) applications of representation learning .
distributed representations for symbolic data were introduced by hinton ( 123 ) , and rst developed in the language modeling by bengio et al .
context of statistical ( 123 ) in so - called neural net language models ( bengio , 123 ) .
they are all based on learning a distributed repre - sentation for each word , called a word embedding .
adding a convolutional architecture , collobert et al .
( 123 ) developed the senna system123 that shares representations across the tasks of language modeling , part - of - speech tagging , chunking , named entity recognition , semantic role labeling and syntactic parsing .
senna approaches or surpasses the state - of - the - art on these tasks but is simpler and much faster than traditional predictors .
learning word embeddings can be combined with learning image representations in a way that allow to associate text and images .
this approach has been used successfully to build googles image search , exploiting huge quantities of data to map images and queries in the same space ( weston et al . ,
for the knowledge - free version of the task , where no image - specic prior
is used , such as image deformations or convolutions
the 123 - class imagenet benchmark , whose results are detailed here :
downloadable from http : / / ml . nec - labs . com / senna /
123 ) and it has recently been extended to deeper multi - modal representations ( srivastava and salakhutdinov , 123 ) .
the neural net
language model was also improved by adding recurrence to the hidden layers ( mikolov et al . , 123 ) , the state - of - the - art ( smoothed n - gram models ) not only in terms of perplexity ( exponential of the average negative log - likelihood of predicting the right next word , going down from 123 to 123 ) but also in terms of word error rate in speech recognition ( since the language model is an important component of a speech recognition system ) , decreasing it from 123% ( kn123 baseline ) or 123% ( discriminative language model ) to 123% on the wall street journal benchmark task .
similar models have been applied in statistical machine translation ( schwenk et al . , 123; le et al . , 123 ) , improving perplexity and bleu scores .
re - cursive auto - encoders ( which generalize recurrent networks ) have also been used to beat the state - of - the - art in full sentence paraphrase detection ( socher et al . , 123a ) almost doubling the f123 score for paraphrase detection .
representation learning can also be used to perform word sense disambiguation ( bordes et al . , 123 ) , bringing up the accuracy from 123% to 123% on the subset of senseval - 123 where the system could be applied ( with subject - verb - object sentences ) .
finally , it has also been successfully used to surpass the state - of - the - art in sentiment analysis ( glorot et al . , 123b; socher et al . , 123b ) .
multi - task and transfer learning , domain adaptation
transfer learning is the ability of a learning algorithm to exploit commonalities between different learning tasks in order to share statistical strength , and transfer knowledge across tasks .
as discussed below , we hypothesize that representation learning algorithms have an advantage for such tasks because they learn representations that capture underlying factors , a subset of which may be relevant for each particular task , as illustrated in figure 123
this hypothesis seems conrmed by a number of empirical results showing the strengths of repre - sentation learning algorithms in transfer learning scenarios .
illustration of representation - learning discovering ex - planatory factors ( middle hidden layer , in red ) , some explaining the input ( semi - supervised setting ) , and some explaining target for each task .
because these subsets overlap , sharing of statis - tical strength helps generalization .
most impressive are the two transfer learning challenges held in 123 and won by representation learning algorithms .
first , the transfer learning challenge , presented at an icml 123 workshop of the same name , was won using unsuper - vised layer - wise pre - training ( bengio , 123; mesnil et al . , 123 ) .
a second transfer learning challenge was held the
raw input x task 123 output y123 task 123 output y123 task 123 output y123 task%a%task%b%task%c%%output%%input%%shared%subsets%of%factors% same year and won by goodfellow et al .
( 123 ) .
results were presented at nips 123s challenges in learning hier - archical models workshop .
in the related domain adaptation setup , the target remains the same but the input distribution changes ( glorot et al . , 123b; chen et al . , 123 ) .
in the multi - task learning setup , representation learning has also been found advantageous krizhevsky et al .
( 123 ) ; collobert et al .
( 123 ) , because of shared factors across tasks .
123 what makes a representation good ? 123 priors for representation learning in ai in bengio and lecun ( 123 ) , one of us introduced the notion of ai - tasks , which are challenging for current machine learning algorithms , and involve complex but highly structured dependencies .
one reason why explicitly dealing with repre - sentations is interesting is because they can be convenient to express many general priors about the world around us , i . e . , priors that are not task - specic but would be likely to be useful for a learning machine to solve ai - tasks .
examples of such general - purpose priors are the following : smoothness : assumes the function to be learned f is s . t .
x y generally implies f ( x ) f ( y ) .
this most basic prior is present in most machine learning , but is insufcient to get around the curse of dimensionality , see section 123 .
multiple explanatory factors : the data generating distribu - tion is generated by different underlying factors , and for the most part what one learns about one factor generalizes in many congurations of the other factors .
the objective to recover or at least disentangle these underlying factors of variation is discussed in section 123 .
this assumption is behind the idea of distributed representations , discussed in section 123 below .
a hierarchical organization of explanatory factors : the concepts that are useful for describing the world around us can be dened in terms of other concepts , in a hierarchy , with more abstract concepts higher in the hierarchy , dened in terms of less abstract ones .
this assumption is exploited with deep representations , elaborated in section 123 below .
semi - supervised learning : with inputs x and target y to predict , a subset of the factors explaining xs distribution explain much of y , given x .
hence representations that are useful for p ( x ) tend to be useful when learning p ( y |x ) , allowing sharing of statistical strength between the unsuper - vised and supervised learning tasks , see section 123
shared factors across tasks : with many y s of interest or many learning tasks in general , tasks ( e . g . , the corresponding p ( y |x , task ) ) are explained by factors that are shared with other tasks , allowing sharing of statistical strengths across tasks , as discussed in the previous section ( multi - task and transfer learning , domain adaptation ) .
manifolds : probability mass concentrates near regions that have a much smaller dimensionality than the original space where the data lives .
this is explicitly exploited in some of the auto - encoder algorithms and other manifold - inspired algorithms described respectively in sections 123 and 123
natural clustering : different values of categorical variables such as object classes are associated with separate manifolds .
more precisely , the local variations on the manifold tend to preserve the value of a category , and a linear interpolation
between examples of different classes in general going through a low density region , i . e . , p ( x|y = i ) for different i tend to be well separated and not overlap much .
for example , this is exploited in the manifold tangent classier discussed in section 123 .
this hypothesis is consistent with the idea that humans have named categories and classes because of such statistical structure ( discovered by their brain and propagated by their culture ) , and machine learning tasks often involves predicting such categorical variables .
temporal and spatial coherence : consecutive ( from a se - quence ) or spatially nearby observations tend to be associated with the same value of relevant categorical concepts , or result in a small move on the surface of the high - density manifold .
more generally , different factors change at different temporal and spatial scales , and many categorical concepts of interest change slowly .
when attempting to capture such categorical variables , this prior can be enforced by making the associated representations slowly changing , i . e . , penalizing changes in values over time or space .
this prior was introduced in becker and hinton ( 123 ) and is discussed in section 123 .
sparsity : for any given observation x , only a small fraction of the possible factors are relevant .
in terms of representation , this could be represented by features that are often zero ( as initially proposed by olshausen and field ( 123 ) ) , or by the fact that most of the extracted features are insensitive to small variations of x .
this can be achieved with certain forms of priors on latent variables ( peaked at 123 ) , or by using a non - linearity whose value is often at at 123 ( i . e . , 123 and with a 123 derivative ) , or simply by penalizing the magnitude of the jacobian matrix ( of derivatives ) of the function mapping input to representation .
this is discussed in sections 123 . 123 and 123 .
simplicity of factor dependencies : in good high - level representations , the factors are related to each other through simple , typically linear dependencies .
this can be seen in many laws of physics , and is assumed when plugging a linear predictor on top of a learned representation .
we can view many of the above priors as ways to help the learner discover and disentangle some of the underlying ( and a priori unknown ) factors of variation that the data may reveal .
this idea is pursued further in sections 123 and 123 .
123 smoothness and the curse of dimensionality for ai - tasks , such as vision and nlp , it seems hopeless to rely only on simple parametric models ( such as linear models ) because they cannot capture enough of the complexity of in - terest unless provided with the appropriate feature space .
con - versely , machine learning researchers have sought exibility in local123 non - parametric learners such as kernel machines with a xed generic local - response kernel ( such as the gaussian kernel ) .
unfortunately , as argued at length by bengio and monperrus ( 123 ) ; bengio et al .
( 123a ) ; bengio and lecun ( 123 ) ; bengio ( 123 ) ; bengio et al .
( 123 ) , most of these algorithms only exploit the principle of local generalization , i . e . , the assumption that the target function ( to be learned ) is smooth enough , so they rely on examples to explicitly map out the wrinkles of the target function .
generalization
local in the sense that the value of the learned function at x depends
mostly on training examples x ( t ) s close to x
expressive , meaning that
is mostly achieved by a form of local interpolation between neighboring training examples .
although smoothness can be a useful assumption , it is insufcient to deal with the curse of dimensionality , because the number of such wrinkles ( ups and downs of the target function ) may grow exponentially with the number of relevant interacting factors , when the data are represented in raw input space .
we advocate learning algorithms that are exible and non - parametric123 but do not rely exclusively on the smoothness assumption .
instead , we propose to incorporate generic priors such as those enumerated above into representation - learning algorithms .
smoothness - based learners ( such as kernel machines ) and linear models can still be useful on top of such learned representations .
in fact , the combination of learning a representation and kernel machine is equivalent to learning the kernel , i . e . , the feature space .
kernel machines are useful , but they depend on a prior denition of a suitable similarity metric , or a feature space in which naive similarity metrics sufce .
we would like to use the data , along with very generic priors , to discover those features , or equivalently , a similarity function .
123 distributed representations reasonably - sized learned representation can capture a huge number of possible input congurations .
a simple counting argument helps us to assess the expressiveness of a model pro - ducing a representation : how many parameters does it require compared to the number of input regions ( or congurations ) it can distinguish ? learners of one - hot representations , such as traditional clustering algorithms , gaussian mixtures , nearest - neighbor algorithms , decision trees , or gaussian svms all re - quire o ( n ) parameters ( and / or o ( n ) examples ) to distinguish o ( n ) input regions .
one could naively believe that one cannot do better .
however , rbms , sparse coding , auto - encoders or multi - layer neural networks can all represent up to o ( 123k ) input regions using only o ( n ) parameters ( with k the number of non - zero elements in a sparse representation , and k = n in non - sparse rbms and other dense representations ) .
these are all distributed 123 or sparse123 representations .
the generalization of clustering to distributed representations is multi - clustering , where either several clusterings take place in parallel or the same clustering is applied on different parts of the input , such as in the very popular hierarchical feature extraction for object recognition based on a histogram of cluster categories detected in different patches of an image ( lazebnik et al . , 123; coates and ng , 123a ) .
the exponential gain from distributed or sparse representations is discussed further in section 123 ( and figure 123 ) of bengio ( 123 ) .
it comes about because each parameter ( e . g .
the parameters of one of the units in a sparse code , or one of the units in a restricted
boltzmann machine ) can be re - used in many examples that are not simply near neighbors of each other , whereas with local generalization , different regions in input space are basically associated with their own private set of parameters , e . g . , as in decision trees , nearest - neighbors , gaussian svms , etc .
in a distributed representation , an exponentially large number of possible subsets of features or hidden units can be activated in response to a given input .
in a single - layer model , each feature is typically associated with a preferred input direction , corresponding to a hyperplane in input space , and the code or representation associated with that input is precisely the pattern of activation ( which features respond to the input , and how much ) .
this is in contrast with a non - distributed representation such as the one learned by most clustering algorithms , e . g . , k - means , in which the representation of a given input vector is a one - hot code identifying which one of a small number of cluster centroids best represents the input 123
123 depth and abstraction depth is a key aspect to representation learning strategies we consider in this paper .
as we will discuss , deep architectures are often challenging to train effectively and this has been the subject of much recent research and progress .
however , despite these challenges , they carry two signicant advantages that motivate our long - term interest in discovering successful training strategies for deep architectures .
these advantages are : ( 123 ) deep architectures promote the re - use of features , and ( 123 ) deep architectures can potentially lead to progressively more abstract features at higher layers of representations ( more removed from the data ) .
feature re - use .
the notion of re - use , which explains the power of distributed representations , is also at the heart of the theoretical advantages behind deep learning , i . e . , constructing multiple levels of representation or learning a hierarchy of features .
the depth of a circuit is the length of the longest path from an input node of the circuit to an output node of the circuit .
the crucial property of a deep circuit is that its number of paths , i . e . , ways to re - use different parts , can grow exponentially with its depth .
formally , one can change the depth of a given circuit by changing the denition of what each node can compute , but only by a constant factor .
the typical computations we allow in each node include : weighted sum , product , articial neuron model ( such as a monotone non - linearity on top of an afne transformation ) , computation of a kernel , or logic gates .
theoretical results clearly show families of functions where a deep representation can be exponentially more efcient than one that is insufciently deep ( hastad , 123; hastad and goldmann , 123; bengio et al . , 123a; bengio and lecun , 123; bengio and delalleau , 123 ) .
if the same family of functions can be represented with fewer
we understand non - parametric as including all
whose capacity can be increased appropriately as the amount of data and its complexity demands it , e . g .
including mixture models and neural networks where the number of parameters is a data - selected hyper - parameter .
distributed representations : where k out of n representation elements or feature values can be independently varied , e . g . , they are not mutually exclusive .
each concept is represented by having k features being turned on or active , while each feature is involved in representing many concepts .
sparse representations : distributed representations where only a few of
the elements can be varied at a time , i . e . , k < n .
as discussed in ( bengio , 123 ) , things are only slightly better when in ordinary mixture allowing continuous - valued membership values , e . g . , models ( with separate parameters for each mixture component ) , but difference in representational power is still exponential ( montufar and morton , 123 ) .
the situation may also seem better with a decision tree , where each given input is associated with a one - hot code over the tree leaves , which deterministically selects associated ancestors ( the path from root to node ) .
number of leaves of the tree ) still only grows linearly with the number of parameters used to specify it ( bengio and delalleau , 123 ) .
the number of different regions represented ( equal
parameters ( or more precisely with a smaller vc - dimension ) , learning theory would suggest that it can be learned with fewer examples , yielding improvements in both computational efciency ( less nodes to visit ) and statistical efciency ( less parameters to learn , and re - use of these parameters over many different kinds of inputs ) .
abstraction and invariance .
deep architectures can lead to abstract representations because more abstract concepts can often be constructed in terms of less abstract ones .
in some cases , such as in the convolutional neural network ( lecun et al . , 123b ) , we build this abstraction in explicitly via a pooling mechanism ( see section 123 ) .
more abstract concepts are generally invariant to most local changes of the input .
that makes the representations that capture these concepts generally highly non - linear functions of the raw input .
this is obviously true of categorical concepts , where more abstract representa - tions detect categories that cover more varied phenomena ( e . g .
larger manifolds with more wrinkles ) and thus they potentially have greater predictive power .
abstraction can also appear in high - level continuous - valued attributes that are only sensitive to some very specic types of changes in the input .
learning these sorts of invariant features has been a long - standing goal in pattern recognition .
123 disentangling factors of variation beyond being distributed and invariant , we would like our rep - resentations to disentangle the factors of variation .
different explanatory factors of the data tend to change independently of each other in the input distribution , and only a few at a time tend to change when one considers a sequence of consecutive
complex data arise from the rich interaction of many sources .
these factors interact in a complex web that can complicate ai - related tasks such as object classication .
for example , an image is composed of the interaction between one or more light sources , the object shapes and the material prop - erties of the various surfaces present in the image .
shadows from objects in the scene can fall on each other in complex patterns , creating the illusion of object boundaries where there are none and dramatically effect the perceived object shape .
how can we cope with these complex interactions ? how can we disentangle the objects and their shadows ? ultimately , we believe the approach we adopt for overcoming these challenges must leverage the data itself , using vast quantities of unlabeled examples , to learn representations that separate the various explanatory sources .
doing so should give rise to a representation signicantly more robust to the complex and richly structured variations extant in natural data sources for
it is important to distinguish between the related but distinct goals of learning invariant features and learning to disentangle explanatory factors .
the central difference is the preservation of information .
invariant features , by denition , have reduced sensitivity in the direction of invariance .
this is the goal of building features that are insensitive to variation in the data that are uninformative to the task at hand .
unfortunately , it is often difcult to determine a priori which set of features and variations will ultimately be relevant to the task at hand .
further , as is often the case in the context of deep learning methods , the feature set being trained may be destined to be used in multiple tasks that may have distinct subsets of relevant features .
considerations such as these lead us to the conclusion that the most robust approach to feature learning is to disentangle as many factors as possible , discarding as little information about the data as is practical .
if some form of dimensionality reduction is desirable , then we hypothesize that the local directions of variation least represented in the training data should be rst to be pruned out ( as in pca , for example , which does it globally instead of around each
123 good criteria for learning representations ? one of the challenges of representation learning that distin - guishes it from other machine learning tasks such as classi - cation is the difculty in establishing a clear objective , or target for training .
in the case of classication , the objective is ( at least conceptually ) obvious , we want to minimize the number of misclassications on the training dataset .
in the case of representation learning , our objective is far - removed from the ultimate objective , which is typically learning a classier or some other predictor .
our problem is reminiscent of the credit assignment problem encountered in reinforcement learning .
we have proposed that a good representation is one that disentangles the underlying factors of variation , but how do we translate that into appropriate training criteria ? is it even necessary to do anything but maximize likelihood under a good model or can we introduce priors such as those enumerated above ( possibly data - dependent ones ) that help the representa - tion better do this disentangling ? this question remains clearly open but is discussed in more detail in sections 123 and 123 .
123 building deep representations in 123 , a breakthrough in feature learning and deep learning was initiated by geoff hinton and quickly followed up in the same year ( hinton et al . , 123; bengio et al . , 123; ranzato et al . , 123 ) , and soon after by lee et al .
( 123 ) and many more later .
it has been extensively reviewed and discussed in bengio ( 123 ) .
a central idea , referred to as greedy layerwise unsupervised pre - training , was to learn a hierarchy of features one level at a time , using unsupervised feature learning to learn a new transformation at each level to be composed with the previously learned transformations; essentially , each iteration of unsupervised feature learning adds one layer of weights to a deep neural network .
finally , the set of layers could be combined to initialize a deep supervised pre - dictor , such as a neural network classier , or a deep generative model , such as a deep boltzmann machine ( salakhutdinov and hinton , 123 ) .
this paper is mostly about feature learning algorithms that can be used to form deep architectures .
in particular , it was empirically observed that layerwise stacking of feature extraction often yielded better representations , e . g . , in terms of classication error ( larochelle et al . , 123; erhan et al . , 123b ) , quality of the samples generated by a probabilistic model ( salakhutdinov and hinton , 123 ) or in terms of the invariance properties of the learned features ( goodfellow
et al . , 123 ) .
whereas this section focuses on the idea of stacking single - layer models , section 123 follows up with a discussion on joint training of all the layers .
after greedy layerwise unsuperivsed pre - training , the re - sulting deep features can be used either as input to a standard supervised machine learning predictor ( such as an svm ) or as initialization for a deep supervised neural network ( e . g . , by ap - pending a logistic regression layer or purely supervised layers of a multi - layer neural network ) .
the layerwise procedure can also be applied in a purely supervised setting , called the greedy layerwise supervised pre - training ( bengio et al . , 123 ) .
for example , after the rst one - hidden - layer mlp is trained , its output layer is discarded and another one - hidden - layer mlp can be stacked on top of it , etc .
although results reported in bengio et al .
( 123 ) were not as good as for unsupervised pre - training , they were nonetheless better than without pre - training at all .
alternatively , the outputs of the previous layer can be fed as extra inputs for the next layer ( in addition to the raw input ) , as successfully done in yu et al .
( 123 ) .
another variant ( seide et al . , 123b ) pre - trains in a supervised way all the previously added layers at each step of the iteration , and in their experiments this discriminant variant yielded better results than unsupervised pre - training .
whereas combining single layers into a supervised model is straightforward , it is less clear how layers pre - trained by unsupervised learning should be combined to form a better unsupervised model .
we cover here some of the approaches to do so , but no clear winner emerges and much work has to be done to validate existing proposals or improve them .
the rst proposal was to stack pre - trained rbms into a deep belief network ( hinton et al . , 123 ) or dbn , where the top layer is interpreted as an rbm and the lower layers as a directed sigmoid belief network .
however , it is not clear how to approximate maximum likelihood training to further optimize this generative model .
one option is the wake - sleep algorithm ( hinton et al . , 123 ) but more work should be done to assess the efciency of this procedure in terms of improving the generative model .
the second approach that has been put forward is to combine the rbm parameters into a deep boltzmann machine ( dbm ) , by basically halving the rbm weights to obtain the dbm weights ( salakhutdinov and hinton , 123 ) .
the dbm can then be trained by approximate maximum likelihood as discussed in more details later ( section 123 ) .
this joint training has brought substantial improvements , both in terms of likelihood and in terms of classication performance of the resulting deep feature learner ( salakhutdinov and hinton ,
another early approach was to stack rbms or auto - encoders into a deep auto - encoder ( hinton and salakhutdi - nov , 123 ) .
if we have a series of encoder - decoder pairs ( f ( i ) ( ) , g ( i ) ( ) ) , then the overall encoder is the composition of the encoders , f ( n ) ( .
f ( 123 ) ( f ( 123 ) ( ) ) ) , and the overall decoder is its transpose ( often with transposed weight matrices as well ) , g ( 123 ) ( g ( 123 ) ( .
f ( n ) ( ) ) ) .
the deep auto - encoder ( or its regularized version , as discussed in section 123 ) can then be jointly trained , with all the parameters optimized with respect to a global reconstruction error criterion .
more work
on this avenue clearly needs to be done , and it was probably avoided by fear of the challenges in training deep feedfor - ward networks , discussed in the section 123 along with very encouraging recent results .
yet another recently proposed approach to training deep architectures ( ngiam et al . , 123 ) is to consider the iterative construction of a free energy function ( i . e . , with no explicit latent variables , except possibly for a top - level layer of hidden units ) for a deep architecture as the composition of transforma - tions associated with lower layers , followed by top - level hid - den units .
the question is then how to train a model dened by an arbitrary parametrized ( free ) energy function .
ngiam et al .
( 123 ) have used hybrid monte carlo ( neal , 123 ) , but other options include contrastive divergence ( hinton , 123; hinton et al . , 123 ) , score matching ( hyvarinen , 123; hyvarinen , 123 ) , denoising score matching ( kingma and lecun , 123; vincent , 123 ) , ratio - matching ( hyvarinen , 123 ) and noise - contrastive estimation ( gutmann and hyvarinen , 123 ) .
123 single - layer learning modules within the community of researchers interested in representa - tion learning , there has developed two broad parallel lines of inquiry : one rooted in probabilistic graphical models and one rooted in neural networks .
fundamentally , the difference be - tween these two paradigms is whether the layered architecture of a deep learning model is to be interpreted as describing a probabilistic graphical model or as describing a computation graph .
in short , are hidden units considered latent random variables or as computational nodes ?
this is likely a function of the fact
to date , the dichotomy between these two paradigms has remained in the background , perhaps because they appear to have more characteristics in common than separating them .
much recent progress in both of these areas has focused on single - layer greedy learning modules and the similarities between the types of single - layer models that have been explored : mainly , the restricted boltzmann machine ( rbm ) on the probabilistic side , and the auto - encoder variants on the neural network side .
indeed , as shown by one of us ( vincent , 123 ) and others ( swersky et al . , 123 ) , in the case of the restricted boltzmann machine , training the model via an inductive principle known as score matching ( hyvarinen , 123 ) ( to be discussed in sec .
123 . 123 ) is essentially identical to applying a regularized reconstruction objective to an auto - encoder .
another strong link between pairs of models on both sides of this divide is when the computational graph for computing representation in the neural network model corre - sponds exactly to the computational graph that corresponds to inference in the probabilistic model , and this happens to also correspond to the structure of graphical model itself ( e . g . , as in the rbm ) .
the connection between these two paradigms becomes more tenuous when we consider deeper models where , in the case of a probabilistic model , exact inference typically becomes intractable .
in the case of deep models , the computational graph diverges from the structure of the model .
for example , in the case of a deep boltzmann machine , unrolling variational ( approximate ) inference into a computational graph results in
a recurrent graph structure .
we have performed preliminary exploration ( savard , 123 ) of deterministic variants of deep auto - encoders whose computational graph is similar to that of a deep boltzmann machine ( in fact very close to the mean - eld variational approximations associated with the boltzmann machine ) , and that is one interesting intermediate point to ex - plore ( between the deterministic approaches and the graphical
in the next few sections we will review the major de - velopments in single - layer training modules used to support feature learning and particularly deep learning .
we divide these sections between ( section 123 ) the probabilistic models , with inference and training schemes that directly parametrize the generative or decoding pathway and ( section 123 ) the typ - ically neural network - based models that directly parametrize the encoding pathway .
interestingly , some models , like pre - dictive sparse decomposition ( psd ) ( kavukcuoglu et al . , 123 ) inherit both properties , and will also be discussed ( sec - tion 123 . 123 ) .
we then present a different view of representation learning , based on the associated geometry and the manifold assumption , in section 123
first , let us consider an unsupervised single - layer represen - tation learning algorithm spaning all three views : probabilistic , auto - encoder , and manifold learning .
principal components analysis we will use probably the oldest feature extraction algorithm , principal components analysis ( pca ) , to illustrate the proba - bilistic , auto - encoder and manifold views of representation - learning .
pca learns a linear transformation h = f ( x ) = w t x + b of input x rdx , where the columns of dx dh matrix w form an orthogonal basis for the dh orthogonal directions of greatest variance in the training data .
the result is dh features ( the components of representation h ) that are decorrelated .
the three interpretations of pca are the following : a ) it is related to probabilistic models ( section 123 ) such as probabilistic pca , factor analysis and the traditional multivariate gaussian distribution ( the leading eigenvectors of the covariance matrix are the principal components ) ; b ) the representation it learns is essentially the same as that learned by a basic linear auto - encoder ( section 123 ) ; and c ) it can be viewed as a simple linear form of linear manifold learning ( section 123 ) , i . e . , characterizing a lower - dimensional region in input space near which the data density is peaked .
thus , pca may be in the back of the readers mind as a common thread relating these various viewpoints .
unfortunately the expressive power of linear features is very limited : they cannot be stacked to form deeper , more abstract representations since the composition of linear operations yields another linear operation .
here , we focus on recent algorithms that have been developed to extract non - linear features , which can be stacked in the construction of deep networks , although some authors simply insert a non - linearity between learned single - layer linear projections ( le et al . , 123c; chen et al . , 123 ) .
another rich family of feature extraction techniques that this review does not cover in any detail due to space constraints is independent component analysis or ica ( jutten and herault , 123; bell and sejnowski , 123 ) .
instead , we refer the reader to hyvarinen et al .
( 123a ) ; hyvarinen et al .
( 123 ) .
note that ,
while in the simplest case ( complete , noise - free ) ica yields linear features , in the more general case it can be equated with a linear generative model with non - gaussian independent latent variables , similar to sparse coding ( section 123 . 123 ) , which result in non - linear features .
therefore , ica and its variants like independent and topographic ica ( hyvarinen et al . , 123b ) can and have been used to build deep networks ( le et al . , 123 , 123c ) : see section 123 .
the notion of obtaining independent components also appears similar to our stated goal of disentangling underlying explanatory factors through deep networks .
however , for complex real - world distributions , it is doubtful that the relationship between truly independent underlying factors and the observed high - dimensional data can be adequately characterized by a linear transformation .
123 probabilistic models from the probabilistic modeling perspective , the question of feature learning can be interpreted as an attempt to recover a parsimonious set of latent random variables that describe a distribution over the observed data .
we can express as p ( x , h ) a probabilistic model over the joint space of the latent variables , h , and observed data or visible variables x .
feature values are conceived as the result of an inference process to determine the probability distribution of the latent variables given the data , i . e .
p ( h | x ) , often referred to as the posterior probability .
learning is conceived in term of estimating a set of model parameters that ( locally ) maximizes the regularized likelihood of the training data .
the probabilistic graphical model formalism gives us two possible modeling paradigms in which we can consider the question of inferring latent variables , directed and undirected graphical models , which differ in their parametrization of the joint distribution p ( x , h ) , yielding major impact on the nature and computational costs of both inference and learning .
123 directed graphical models directed latent factor models separately parametrize the con - ditional likelihood p ( x | h ) and the prior p ( h ) to construct the joint distribution , p ( x , h ) = p ( x | h ) p ( h ) .
examples of this decomposition include : principal components analysis ( pca ) ( roweis , 123; tipping and bishop , 123 ) , sparse coding ( olshausen and field , 123 ) , sigmoid belief net - works ( neal , 123 ) and the newly introduced spike - and - slab sparse coding model ( goodfellow et al . , 123 ) .
123 . 123 explaining away directed models often leads to one important property : ex - plaining away , i . e . , a priori independent causes of an event can become non - independent given the observation of the event .
latent factor models can generally be interpreted as latent cause models , where the h activations cause the observed x .
this renders the a priori independent h to be non - independent .
as a consequence , recovering the posterior distribution of h , p ( h | x ) ( which we use as a basis for feature representation ) , is often computationally challenging and can be entirely intractable , especially when h is discrete .
a classic example that illustrates the phenomenon is to imagine you are on vacation away from home and you receive a phone call from the security system company , telling you that
the alarm has been activated .
you begin worrying your home has been burglarized , but then you hear on the radio that a minor earthquake has been reported in the area of your home .
if you happen to know from prior experience that earthquakes sometimes cause your home alarm system to activate , then suddenly you relax , condent that your home has very likely not been burglarized .
the example illustrates how the alarm activation rendered two otherwise entirely independent causes , burglarized and earthquake , to become dependent in this case , the depen - dency is one of mutual exclusivity .
since both burglarized and earthquake are very rare events and both can cause alarm activation , the observation of one explains away the other .
despite the computational obstacles we face when attempting to recover the posterior over h , explaining away promises to provide a parsimonious p ( h | x ) , which can be an extremely useful characteristic of a feature encoding scheme .
if one thinks of a representation as being composed of various feature detectors and estimated attributes of the observed input , it is useful to allow the different features to compete and collaborate with each other to explain the input .
this is naturally achieved with directed graphical models , but can also be achieved with undirected models ( see section 123 ) such as boltzmann machines if there are lateral connections between the corresponding units or corresponding interaction terms in the energy function that denes the probability model .
probabilistic interpretation of pca .
pca can be given a natural probabilistic interpretation ( roweis , 123; tipping and bishop , 123 ) as factor analysis : p ( h ) = n ( h; 123 , 123
p ( x | h ) = n ( x; w h + x , 123
where x rdx , h rdh , n ( v; , ) is the multivariate normal density of v with mean and covariance , and columns of w span the same space as leading dh principal components , but are not constrained to be orthonormal .
sparse coding .
like pca , sparse coding has both a proba - bilistic and non - probabilistic interpretation .
sparse coding also relates a latent representation h ( either a vector of random variables or a feature vector , depending on the interpretation ) to the data x through a linear mapping w , which we refer to as the dictionary .
the difference between sparse coding and pca is that sparse coding includes a penalty to ensure a sparse activation of h is used to encode each input x .
from a non - probabilistic perspective , sparse coding can be seen as recovering the code or feature vector associated with a new input x via :
learning the dictionary w can be accomplished by optimizing the following training criterion with respect to w :
123 + ( cid : 123 ) h ( cid : 123 ) 123 ,
( cid : 123 ) x w h ( cid : 123 ) 123
= f ( x ) = argmin
( cid : 123 ) x ( t ) w h
where x ( t ) is the t - th example and h ( t ) is the corresponding sparse code determined by eq .
w is usually constrained to have unit - norm columns ( because one can arbitrarily exchange scaling of column i with scaling of h ( t ) , such a constraint is necessary for the l123 penalty to have any effect ) .
the probabilistic interpretation of sparse coding differs from that of pca , in that instead of a gaussian prior on the latent random variable h , we use a sparsity inducing laplace prior ( corresponding to an l123 penalty ) :
p ( x | h ) = n ( x; w h + x , 123
to the norm constraint on w .
note that
data p ( x ) = ( cid : 123 )
in the case of sparse coding , because we will seek a sparse representation ( i . e . , one with many features set to exactly zero ) , we will be interested in recovering the map ( maximum a posteriori value of h : i . e .
h = argmaxh p ( h | x ) rather than its expected value e ( h|x ) .
under this interpretation , dictionary learning proceeds as maximizing the likelihood of the data t p ( x ( t ) | h ( t ) ) given these map values of h : argmaxw rameter learning scheme , subject to the map values of is not standard practice in the probabilistic the latent h , literature .
typically the likelihood of the h p ( x | h ) p ( h ) is maximized directly .
in the presence of latent variables , expectation maximization is em - ployed where the parameters are optimized with respect to the marginal likelihood , i . e . , summing or integrating the joint log - likelihood over the all values of the latent variables under their posterior p ( h | x ) , rather than considering only the single map value of h .
the theoretical properties of this form of parameter learning are not yet well understood but seem to work well in practice ( e . g .
k - means vs gaussian mixture models and viterbi training for hmms ) .
note also that the interpretation of sparse coding as a map estimation can be questioned ( gribonval , 123 ) , because even though the interpretation of the l123 penalty as a log - prior is a possible there can be other bayesian interpretations compatible with the training criterion .
sparse coding is an excellent example of the power of explaining away .
even with a very overcomplete dictionary123 , the map inference process used in sparse coding to nd h can pick out the most appropriate bases and zero the others , despite them having a high degree of correlation with the input .
this property arises naturally in directed graphical models such as sparse coding and is entirely owing to the explaining away effect .
it is not seen in commonly used undirected prob - abilistic models such as the rbm , nor is it seen in parametric feature encoding methods such as auto - encoders .
the trade - off is that , compared to methods such as rbms and auto - encoders , inference in sparse coding involves an extra inner - loop of optimization to nd h with a corresponding increase in the computational cost of feature extraction .
compared to auto - encoders and rbms , the code in sparse coding is a free variable for each example , and in that sense the implicit encoder is non - parametric .
one might expect that the parsimony of the sparse coding representation and its explaining away effect would be advan - tageous and indeed it seems to be the case .
coates and ng ( 123a ) demonstrated on the cifar - 123 object classication task ( krizhevsky and hinton , 123 ) with a patch - base feature extraction pipeline , that in the regime with few ( < 123 )
overcomplete : with more dimensions of h than dimensions of x .
labeled training examples per class , the sparse coding repre - sentation signicantly outperformed other highly competitive encoding schemes .
possibly because of these properties , and because of the very computationally efcient algorithms that have been proposed for it ( in comparison with the general case of inference in the presence of explaining away ) , sparse coding enjoys considerable popularity as a feature learning and encoding paradigm .
there are numerous examples of its suc - cessful application as a feature representation scheme , includ - ing natural image modeling ( raina et al . , 123; kavukcuoglu et al . , 123; coates and ng , 123a; yu et al . , 123 ) , audio classication ( grosse et al . , 123 ) , nlp ( bagnell and bradley , 123 ) , as well as being a very successful model of the early visual cortex ( olshausen and field , 123 ) .
sparsity criteria can also be generalized successfully to yield groups of features that prefer to all be zero , but if one or a few of them are active then the penalty for activating others in the group is small .
different group sparsity patterns can incorporate different forms of prior knowledge ( kavukcuoglu et al . , 123; jenatton et al . , 123; bach et al . , 123; gregor et al . , 123 ) .
spike - and - slab sparse coding .
spike - and - slab sparse cod - ing ( s123c ) is one example of a promising variation on sparse coding for feature learning ( goodfellow et al . , 123 ) .
the s123c model possesses a set of latent binary spike variables together with a a set of latent real - valued slab variables .
the activation of the spike variables dictates the sparsity pattern .
s123c has been applied to the cifar - 123 and cifar - 123 object classication tasks ( krizhevsky and hinton , 123 ) , and shows the same pattern as sparse coding of superior performance in the regime of relatively few ( < 123 ) labeled examples per class ( goodfellow et al . , 123 ) .
in fact , in both the cifar - 123 dataset ( with 123 examples per class ) and the cifar - 123 dataset ( when the number of examples is reduced to a similar range ) , the s123c representation actually outperforms sparse coding representations .
this advantage was revealed clearly with s123c winning the nips123 transfer learning challenge ( goodfellow et al . , 123 ) .
123 undirected graphical models undirected graphical models , also called markov random elds ( mrfs ) , parametrize the joint p ( x , h ) through a product of unnormalized non - negative clique potentials :
p ( x , h ) =
where i ( x ) , j ( h ) and k ( x , h ) are the clique potentials de - scribing the interactions between the visible elements , between the hidden variables , and those interaction between the visible and hidden variables respectively .
the partition function z ensures that the distribution is normalized .
within the context of unsupervised feature learning , we generally see a particular form of markov random eld called a boltzmann distribution with clique potentials constrained to be positive :
exp ( e ( x , h ) ) ,
p ( x , h ) =
where e ( x , h ) is the energy function and contains the inter - actions described by the mrf clique potentials and are the model parameters that characterize these interactions .
the boltzmann machine was originally dened as a network of symmetrically - coupled binary random variables or units .
( x , h ) = 123
these stochastic units can be divided into two groups : ( 123 ) the visible units x ( 123 , 123 ) dx that represent the data , and ( 123 ) the hidden or latent units h ( 123 , 123 ) dh that mediate dependencies between the visible units through their mutual interactions .
the pattern of interaction is specied through the energy function : where = ( u , v , w , b , d ) are the model parameters which respectively encode the visible - to - visible interactions , the visible - to - hidden interac - the visible self - connections , and the hidden self - connections ( called biases ) .
to avoid over - parametrization , the diagonals of u and v are set to zero .
ht v h xt w h bt x dt h ,
xt u x 123
the boltzmann machine energy function species the prob - ability distribution over ( x , h ) , via the boltzmann distribution , eq .
123 , with the partition function z given by :
( x , h; )
this joint probability distribution gives rise to the set of conditional distributions of the form :
p ( hi | x , h\i ) = sigmoid
p ( xj | h , x\j ) = sigmoid
vii ( cid : 123 ) hi ( cid : 123 ) + di
ujj ( cid : 123 ) xj ( cid : 123 ) + bj
in general , inference in the boltzmann machine is intractable .
for example , computing the conditional probability of hi given the visibles , p ( hi | x ) , requires marginalizing over the rest of the hiddens , which implies evaluating a sum with 123dh123 terms :
p ( hi | x ) =
p ( h | x )
however with some judicious choices in the pattern of inter - actions between the visible and hidden units , more tractable subsets of the model family are possible , as we discuss next .
restricted boltzmann machines ( rbms ) .
the rbm is likely the most popular subclass of boltzmann ma - chine ( smolensky , 123 ) .
it is dened by restricting the interactions in the boltzmann energy function , in eq .
123 , to only those between h and x , i . e .
e rbm with u = 123 and v = 123
as such , the rbm can be said to form a bipartite graph with the visibles and the hiddens forming two layers of vertices in the graph ( and no connection between units of the same layer ) .
with this restriction , the rbm possesses the useful property that the conditional distribution over the hidden units factorizes given the visibles :
is e bm
p ( h | x ) =
p ( hi = 123 | x ) = sigmoid
i p ( hi | x )
j wjixj + di
likewise , the conditional distribution over the visible units given the hiddens also factorizes :
p ( x | h ) =
p ( xj = 123 | h ) = sigmoid
wjihi + bj
p ( xj | h )
this makes inferences readily tractable in rbms .
for exam - ple , the rbm feature representation is taken to be the set of posterior marginals p ( hi | x ) , which , given the conditional independence described in eq .
123 , are immediately available .
note that this is in stark contrast to the situation with popular directed graphical models for unsupervised feature extraction , where computing the posterior probability is intractable .
importantly , the tractability of the rbm does not extend to its partition function , which still involves summing an exponential number of terms .
it does imply however that we can limit the number of terms to min ( 123dx , 123dh ) .
usually this is still an unmanageable number of terms and therefore we must resort to approximate methods to deal with its estimation .
it is difcult to overstate the impact the rbm has had to the elds of unsupervised feature learning and deep learning .
it has been used in a truly impressive variety of applica - tions , including fmri image classication ( schmah et al . , 123 ) , motion and spatial transformations ( taylor and hinton , 123; memisevic and hinton , 123 ) , collaborative ltering ( salakhutdinov et al . , 123 ) and natural ( ranzato and hinton , 123; courville et al . , 123b ) .
123 generalizations of the rbm to real - valued data important progress has been made in the last few years in dening generalizations of the rbm that better capture real - valued data , in particular real - valued image data , by better modeling the conditional covariance of the input pixels .
the standard rbm , as discussed above , is dened with both binary visible variables v ( 123 , 123 ) and binary latent variables h ( 123 , 123 ) .
the tractability of inference and learning in the rbm has inspired many authors to extend it , via modications of its energy function , to model other kinds of data distributions .
in particular , there has been multiple attempts to develop rbm - type models of real - valued data , where x rdx .
the most straightforward approach to modeling real - valued observations within the rbm framework is the so - called gaussian rbm ( grbm ) where the only change in the rbm energy function is to the visible units biases , by adding a bias term that is quadratic in the visible units x .
while it probably remains the most popular way to model real - valued data within the rbm framework , ranzato and hinton ( 123 ) suggest that the grbm has proved to be a somewhat unsatisfactory model of natural images .
the trained features typically do not represent sharp edges that occur at object boundaries and lead to latent representations that are not particularly useful features for classication tasks .
ranzato and hinton ( 123 ) argue that the failure of the grbm to adequately capture the statistical structure of natural images stems from the exclusive use of the model capacity to capture the conditional mean at the expense of the conditional covariance .
natural images , they argue , are chiey characterized by the covariance of the pixel values , not by their absolute values .
this point is supported by the common use of preprocessing methods that standardize the global scaling of the pixel values across images in a dataset or across the pixel values within each image .
these kinds of concerns about the ability of the grbm to model natural image data has lead to the development of alternative rbm - based models that each attempt to take on this
objective of better modeling non - diagonal conditional covari - ( ranzato and hinton , 123 ) introduced the mean and covariance rbm ( mcrbm ) .
like the grbm , the mcrbm is a 123 - layer boltzmann machine that explicitly models the visible units as gaussian distributed quantities .
however unlike the grbm , the mcrbm uses its hidden layer to independently parametrize both the mean and covariance of the data through two sets of hidden units .
the mcrbm is a combination of the covariance rbm ( crbm ) ( ranzato et al . , 123a ) , that models the conditional covariance , with the grbm that captures the conditional mean .
while the grbm has shown considerable potential as the basis of a highly successful phoneme recogni - tion system ( dahl et al . , 123 ) , it seems that due to difculties in training the mcrbm , the model has been largely superseded by the mpot model .
the mpot model ( mean - product of students t - distributions model ) ( ranzato et al . , 123b ) is a combination of the grbm and the product of students t - distributions model ( welling et al . , 123 ) .
it is an energy - based model where the conditional distribution over the visible units conditioned on the hidden variables is a multivariate gaussian ( non - diagonal covariance ) and the complementary conditional distribution over the hidden variables given the visibles are a set of independent gamma distributions .
the pot model has recently been generalized to the mpot model ( ranzato et al . , 123b ) to include nonzero gaussian means by the addition of grbm - like hidden units , similarly to how the mcrbm generalizes the crbm .
the mpot model has been used to synthesize large - scale natural images ( ranzato et al . , 123b ) that show large - scale features and shadowing structure .
it has been used to model natural textures ( kivinen and williams , 123 ) in a tiled - convolution conguration ( see
another recently introduced rbm - based model with the objective of having the hidden units encode both the mean and covariance information is the spike - and - slab restricted boltzmann machine ( ssrbm ) ( courville et al . , 123a , b ) .
the ssrbm is dened as having both a real - valued slab variable and a binary spike variable associated with each unit in the hidden layer .
the ssrbm has been demonstrated as a feature learning and extraction scheme in the context of cifar - 123 object classication ( krizhevsky and hinton , 123 ) from natural images and has performed well in the role ( courville et al . , 123a , b ) .
when trained convolutionally ( see section 123 ) on full cifar - 123 natural images , the model demonstrated the ability to generate natural image samples that seem to capture the broad statistical structure of natural images better than previous parametric generative models , as illustrated with the samples of figure 123
the mcrbm , mpot and ssrbm each set out to model real - valued data such that the hidden units encode not only the conditional mean of the data but also its conditional covariance .
other than differences in the training schemes , the most signicant difference between these models is how they encode their conditional covariance .
while the mcrbm and the mpot use the activation of the hidden units to enforce con - straints on the covariance of x , the ssrbm uses the hidden unit to pinch the precision matrix along the direction specied by the corresponding weight vector .
these two ways of modeling
boltzmann machines , is given by :
log p ( x ( t ) ) = t ( cid : 123 )
where we have the expectations with respect to p ( h ( t ) | x ( t ) ) in the clamped condition ( also called the positive phase ) , and over the full joint p ( x , h ) in the unclamped condition ( also called the negative phase ) .
intuitively , the gradient acts to locally move the model distribution ( the negative phase distribution ) toward the data distribution ( positive phase dis - tribution ) , by pushing down the energy of ( h , x ( t ) ) pairs ( for h p ( h|x ( t ) ) ) while pushing up the energy of ( h , x ) pairs ( for ( h , x ) p ( h , x ) ) until the two forces are in equilibrium , at which point the sufcient statistics ( gradient of the energy function ) have equal expectations with x sampled from the training distribution or with x sampled from the model .
the rbm conditional independence properties imply that the expectation in the positive phase of eq .
123 is tractable .
the negative phase term arising from the partition func - tions contribution to the log - likelihood gradient is more problematic because the computation of the expectation over the joint is not tractable .
the various ways of dealing with the partition functions contribution to the gradient have brought about a number of different training algorithms , many trying to approximate the log - likelihood gradient .
to approximate the expectation of the joint distribution in the negative phase contribution to the gradient , it is natural to again consider exploiting the conditional independence of the rbm in order to specify a monte carlo approximation of the expectation over the joint :
with the samples ( x ( l ) , h ( l ) ) drawn by a block gibbs mcmc ( markov chain monte carlo ) sampling procedure :
x ( l ) p ( x | h ( l123 ) ) h ( l ) p ( h | x ( l ) ) .
naively , for each gradient update step , one would start a gibbs sampling chain , wait until the chain converges to the equilibrium distribution and then draw a sufcient number of samples to approximate the expected gradient with respect to the model ( joint ) distribution in eq .
then restart the process for the next step of approximate gradient ascent on the log - likelihood .
this procedure has the obvious aw that waiting for the gibbs chain to burn - in and reach equilibrium anew for each gradient update cannot form the basis of a prac - tical training algorithm .
contrastive divergence ( hinton , 123; hinton et al . , 123 ) , stochastic maximum likelihood ( younes , 123; tieleman , 123 ) and fast - weights persistent contrastive divergence or fpcd ( tieleman and hinton , 123 ) are all ways to avoid or reduce the need for burn - in .
123 . 123 contrastive divergence contrastive divergence ( cd ) estimation ( hinton , 123; hinton et al . , 123 ) estimates the negative phase expectation ( eq .
123 ) with a very short gibbs chain ( often just one step ) initialized
( top ) samples from convolutionally trained - ssrbm from courville et al .
( 123b ) .
( bottom ) images in cifar - 123 train - ing set closest ( l123 distance with contrast normalized training images ) to corresponding model samples on top .
the model does not appear to be overtting particular training examples .
conditional covariance diverge when the dimensionality of the hidden layer is signicantly different from that of the input .
in the overcomplete setting , sparse activation with the ssrbm parametrization permits variance only in the select directions of the sparsely activated hidden units .
this is a property the ssrbm shares with sparse coding models ( olshausen and field , 123; grosse et al . , 123 ) .
on the other hand , the case of the mpot or mcrbm , an overcomplete set of constraints on the covariance implies that capturing arbitrary covariance along a particular direction of the input requires decreasing potentially all constraints with positive projection in that direction .
this perspective would suggest that the mpot and mcrbm do not appear to be well suited to provide a sparse representation in the overcomplete setting .
123 rbm parameter estimation many of the rbm training methods we discuss here are ap - plicable to more general undirected graphical models , but are particularly practical in the rbm setting .
freund and haussler ( 123 ) proposed a learning algorithm for harmoniums ( rbms ) based on projection pursuit .
contrastive divergence ( hinton , 123; hinton et al . , 123 ) has been used most often to train rbms , and many recent papers use stochastic maximum likelihood ( younes , 123; tieleman , 123 ) .
as discussed in sec .
123 , in training probabilistic models parameters are typically adapted in order to maximize the like - lihood of the training data ( or equivalently the log - likelihood , or its penalized version , which adds a regularization term ) .
with t training examples , the log likelihood is given by :
log p ( x ( t ) ; ) =
p ( x ( t ) , h; ) .
gradient - based optimization requires its gradient , which for
at the training data used in the positive phase .
this reduces the variance of the gradient estimator and still moves in a direction that pulls the negative chain samples towards the as - sociated positive chain samples .
much has been written about the properties and alternative interpretations of cd and its similarity to auto - encoder training , e . g .
carreira - perpinan and hinton ( 123 ) ; yuille ( 123 ) ; bengio and delalleau ( 123 ) ; sutskever and tieleman ( 123 ) .
123 . 123 stochastic maximum likelihood the stochastic maximum likelihood ( sml ) algorithm ( also known as persistent contrastive divergence or pcd ) ( younes , 123; tieleman , 123 ) is an alternative way to sidestep an extended burn - in of the negative phase gibbs sampler .
at each gradient update , rather than initializing the gibbs chain at the positive phase sample as in cd , sml initializes the chain at the last state of the chain used for the previous update .
in other words , sml uses a continually running gibbs chain ( or often a number of gibbs chains run in parallel ) from which samples are drawn to estimate the negative phase expectation .
despite the model parameters changing between updates , these changes should be small enough that only a few steps of gibbs ( in practice , often one step is used ) are required to maintain samples from the equilibrium distribution of the gibbs chain , i . e .
the model distribution .
a troublesome aspect of sml is that it relies on the gibbs chain to mix well ( especially between modes ) for learning to succeed .
typically , as learning progresses and the weights of the rbm grow , the ergodicity of the gibbs sample begins to break down123
if the learning rate associated with gradient ascent + g ( with e ( g ) log p ( x ) ) is not reduced to compensate , then the gibbs sampler will diverge from the model distribution and learning will fail .
desjardins et al .
( 123 ) ; cho et al .
( 123 ) ; salakhutdinov ( 123b , a ) have all considered various forms of tempered transitions to address the failure of gibbs chain mixing , and convincing solutions have not yet been clearly demonstrated .
a recently introduced promising avenue relies on depth itself , showing that mixing between modes is much easier on deeper layers ( bengio et al . ,
tieleman and hinton ( 123 ) have proposed quite a dif - ferent approach to addressing potential mixing problems of sml with their fast - weights persistent contrastive divergence ( fpcd ) , and it has also been exploited to train deep boltz - mann machines ( salakhutdinov , 123a ) and construct a pure sampling algorithm for rbms ( breuleux et al . , 123 ) .
fpcd builds on the surprising but robust tendency of gibbs chains to mix better during sml learning than when the model parameters are xed .
the phenomenon is rooted in the form of the likelihood gradient itself ( eq .
the samples drawn from the sml gibbs chain are used in the negative phase of the gradient , which implies that the learning update will slightly increase the energy ( decrease the probability ) of those samples , making the region in the neighborhood of those samples
when weights become large , the estimated distribution is more peaky , and the chain takes very long time to mix , to move from mode to mode , so that practically the gradient estimator can be very poor .
this is a serious chicken - and - egg problem because if sampling is not effective , nor is the training procedure , which may seem to stall , and yields even larger weights .
less likely to be resampled and therefore making it more likely that the samples will move somewhere else ( typically going near another mode ) .
rather than drawing samples from the distribution of the current model ( with parameters ) , fpcd exaggerates this effect by drawing samples from a local perturbation of the model with parameters and an update
t+123 = ( 123 ) t+123 +
where is the relatively large fast - weight ( > ) and 123 < < 123 ( but near 123 ) is a forgetting factor that keeps the perturbed model close to the current model .
unlike tempering , fpcd does not converge to the model distribution as and go to 123 , and further work is necessary to characterize the nature of its approximation to the model distribution .
nevertheless , fpcd is a popular and apparently effective means of drawing approximate samples from the model distribution that faithfully represent its diversity , at the price of sometimes generating spurious samples in between two modes ( because the fast weights roughly correspond to a smoothed view of the current models energy function ) .
it has been applied in a variety of applications ( tieleman and hinton , 123; ranzato et al . , 123; kivinen and williams , 123 ) and it has been transformed into a sampling algorithm ( breuleux et al . , 123 ) that also shares this fast mixing property with herding ( welling , 123 ) , for the same reason , i . e . , introducing negative correlations between consecutive samples of the chain in order to promote faster mixing .
123 . 123 pseudolikelihood , ratio - matching and more while cd , sml and fpcd are by far the most popular meth - ods for training rbms and rbm - based models , all of these methods are perhaps most naturally described as offering dif - ferent approximations to maximum likelihood training .
there exist other inductive principles that are alternatives to maxi - mum likelihood that can also be used to train rbms .
in partic - ular , these include pseudo - likelihood ( besag , 123 ) and ratio - matching ( hyvarinen , 123 ) .
both of these inductive principles attempt to avoid explicitly dealing with the partition function , and their asymptotic efciency has been analyzed ( marlin and de freitas , 123 ) .
pseudo - likelihood seeks to maximize the product of all one - dimensional conditional distributions of the form p ( xd|x\d ) , while ratio - matching can be interpreted as an extension of score matching ( hyvarinen , 123 ) to discrete data types .
both methods amount to weighted differences of the gradient of the rbm free energy123 evaluated at a data point and at neighboring points .
one potential drawback of these methods is that depending on the parametrization of the energy function , their computational requirements may scale up to o ( nd ) worse than cd , sml , fpcd , or denoising score matching ( kingma and lecun , 123; vincent , 123 ) , discussed below .
marlin et al .
( 123 ) empirically compared all of these methods ( except denoising score matching ) on a range of classication , reconstruction and density modeling tasks and found that , in general , sml provided the best combination of overall performance and computational tractability .
however , in a later study , the same authors ( swersky et al . , 123 ) 123
the free energy f ( x; ) is the energy associated with the data marginal probability , f ( x; ) = log p ( x ) log z and is tractable for the rbm .
found denoising score matching to be a competitive inductive principle both in terms of classication performance ( with respect to sml ) and in terms of computational efciency ( with respect to analytically obtained score matching ) .
denoising score matching is a special case of the denoising auto - encoder training criterion ( section 123 . 123 ) when the reconstruction error residual equals a gradient , i . e . , the score function associated with an energy function , as shown in ( vincent , 123 ) .
in the spirit of the boltzmann machine gradient ( eq .
123 ) several approaches have been proposed to train energy - based models .
one is noise - contrastive estimation ( gutmann and hy - varinen , 123 ) , in which the training criterion is transformed into a probabilistic classication problem : distinguish between ( positive ) training examples and ( negative ) noise samples generated by a broad distribution ( such as the gaussian ) .
another family of approaches , more in the spirit of contrastive divergence , relies on distinguishing positive examples ( of the training distribution ) and negative examples obtained by perturbations of the positive examples ( collobert and weston , 123; bordes et al . , 123; weston et al . , 123 ) .
123 directly learning a parametric map from input to representation within the framework of probabilistic models adopted in section 123 , the learned representation is always associated with latent variables , specically with their posterior distribution given an observed input x .
unfortunately , this posterior dis - tribution tends to become very complicated and intractable if the model has more than a couple of interconnected layers , whether in the directed or undirected graphical model frame - works .
it then becomes necessary to resort to sampling or approximate inference techniques , and to pay the associated computational and approximation error price .
if the true pos - terior has a large number of modes that matter then current inference techniques may face an unsurmountable challenge or endure a potentially serious approximation .
this is in addition to the difculties raised by the intractable partition function in undirected graphical models .
moreover a posterior distribution over latent variables is not yet a simple usable feature vector that can for example be fed to a classier .
so actual feature values are typically derived from that distribution , taking the latent variables expectation ( as is typically done with rbms ) , their marginal probability , or nding their most likely value ( as in sparse coding ) .
if we are to extract stable deterministic numerical feature values in the end anyway , an alternative ( apparently ) non - probabilistic feature learning paradigm that focuses on carrying out this part of the computation , very ef - ciently , is that of auto - encoders and other directly parametrized feature or representation functions .
the commonality between these methods is that they learn a direct encoding , i . e . , a parametric map from inputs to their representation .
regularized auto - encoders , discussed next , also involve learning a decoding function that maps back from represen - tation to input space .
sections 123 and 123 discuss direct encoding methods that do not require a decoder , such as semi - supervised embedding ( weston et al . , 123 ) and slow feature analysis ( wiskott and sejnowski , 123 ) .
in the auto - encoder framework ( lecun , 123; bourlard and kamp , 123; hinton and zemel , 123 ) , one starts by ex - plicitly dening a feature - extracting function in a specic parametrized closed form .
this function , that we will denote f , is called the encoder and will allow the straightforward and efcient computation of a feature vector h = f ( x ) from an input x .
for each example x ( t ) from a data set ( x ( 123 ) , .
, x ( t ) ) , we dene
h ( t ) = f ( x ( t ) )
where h ( t ) is the feature - vector or representation or code com - puted from x ( t ) .
another closed form parametrized function g , called the decoder , maps from feature space back into input space , producing a reconstruction r = g ( h ) .
whereas probabilistic models are dened from an explicit probability function and are trained to maximize ( often approximately ) the data likelihood ( or a proxy ) , auto - encoders are parametrized through their encoder and decoder and are trained using a different training principle .
the set of parameters of the encoder and decoder are learned simultaneously on the task of reconstructing as well as possible the original input , i . e .
attempting to incur the lowest possible reconstruction error l ( x , r ) a measure of the discrepancy between x and its reconstruction r over training examples .
good generalization means low reconstruction error at test examples , while having high reconstruction error for most other x congurations .
to capture the structure of the data - generating distribution , it is therefore important that something in the training crite - rion or the parametrization prevents the auto - encoder from learning the identity function , which has zero reconstruction error everywhere .
this is achieved through various means in the different forms of auto - encoders , as described below in more detail , and we call these regularized auto - encoders .
a particular form of regularization consists in constraining the code to have a low dimension , and this is what the classical auto - encoder or pca do .
in summary , basic auto - encoder training consists in nding a value of parameter vector minimizing reconstruction error
where x ( t ) is a training example .
this minimization is usually carried out by stochastic gradient descent as in the training of multi - layer - perceptrons ( mlps ) .
since auto - encoders were primarily developed as mlps predicting their input , the most commonly used forms for the encoder and decoder are afne mappings , optionally followed by a non - linearity :
f ( x ) = sf ( b + w x ) g ( h ) = sg ( d + w
where sf and sg are the encoder and decoder activation functions ( typically the element - wise sigmoid or hyperbolic tangent non - linearity , or the identity function if staying linear ) .
the set of parameters of such a model is = ( w , b , w ( cid : 123 ) , d ) where b and d are called encoder and decoder bias vectors , and w and w ( cid : 123 ) are the encoder and decoder weight matrices .
the choice of sg and l depends largely on the input domain range and nature , and are usually chosen so that l returns a negative log - likelihood for the observed value of x .
a natural choice for an unbounded domain is a linear decoder with a
squared reconstruction error , i . e .
sg ( a ) = a and l ( x , r ) = ( cid : 123 ) x r ( cid : 123 ) 123
if inputs are bounded between 123 and 123 however , ensuring a similarly - bounded reconstruction can be achieved by using sg = sigmoid .
in addition if the inputs are of a binary nature , a binary cross - entropy loss123 is sometimes used .
if both encoder and decoder use a sigmoid non - linearity , then f ( x ) and g ( h ) have the exact same form as the conditionals p ( h | v ) and p ( v | h ) of binary rbms ( see section 123 ) .
this similarity motivated an initial study ( bengio et al . , 123 ) of the possibility of replacing rbms with auto - encoders as the basic pre - training strategy for building deep networks , as well as the comparative analysis of auto - encoder reconstruction error gradient and contrastive divergence up - dates ( bengio and delalleau , 123 ) .
one notable difference in the parametrization is that rbms use a single weight matrix , which follows naturally from their energy function , whereas the auto - encoder framework allows for a different matrix in the encoder and decoder .
in practice however , weight - tying in which one denes w ( cid : 123 ) = w t may be ( and is most often ) used , rendering the parametrizations identical .
the usual training procedures however differ greatly between the two approaches .
a practical advantage of training auto - encoder variants is that they dene a simple tractable optimization objective that can be used to monitor progress .
in the case of a linear auto - encoder ( linear encoder and decoder ) with squared reconstruction error , minimizing eq .
123 learns the same subspace123 as pca .
this is also true when using a sigmoid nonlinearity in the encoder ( bourlard and kamp , 123 ) , but not if the weights w and w ( cid : 123 ) are tied ( w ( cid : 123 ) = w t ) , because w cannot be forced into being small and w ( cid : 123 ) large to achieve a linear encoder .
regularization term of the form ( cid : 123 )
similarly , le et al .
( 123b ) recently showed that adding a j s123 ( wjx ( t ) ) to a linear auto - encoder with tied weights , where s123 is a nonlinear convex function , yields an efcient algorithm for learning linear ica .
123 regularized auto - encoders like pca , auto - encoders were originally seen as a dimen - sionality reduction technique and thus used a bottleneck , i . e .
dh < dx .
on the other hand , successful uses of sparse coding and rbm approaches tend to favour overcomplete i . e .
dh > dx .
this can allow the auto - encoder to simply duplicate the input in the features , with perfect reconstruction without having extracted more mean - ingful features .
recent research has demonstrated very suc - cessful alternative ways , called regulrized auto - encoders , to constrain the representation , even when it is overcomplete .
the effect of a bottleneck or of this regularization is that the auto - encoder cannot reconstruct well everything , it is trained to reconstruct well the training examples and generalization means that reconstruction error is also small on test examples .
an interesting justication ( ranzato et al . , 123 ) for the sparsity penalty ( or any penalty that restricts in a soft way
i=123 xi log ( ri ) + ( 123 xi ) log ( 123 ri )
contrary to traditional pca loading factors , but similarly to the parameters learned by probabilistic pca , the weight vectors learned by a linear auto - encoder are not constrained to form an orthonormal basis , nor to have a meaningful ordering .
they will however span the same subspace .
l ( x , r ) = ( cid : 123 ) dx
the volume of hidden congurations easily accessible by the learner ) is that it acts in spirit like the partition function of rbms , by making sure that only few input congurations can have a low reconstruction error .
alternatively , one can view the objective of the regulariza - tion applied to an auto - encoder as making the representation as constant ( insensitive ) as possible with respect to changes in input .
this view immediately justies two variants of regularized auto - encoders described below : contractive auto - encoders reduce the number of effective degrees of freedom of the representation ( around each point ) by making the encoder contractive , i . e . , making the derivative of the encoder small ( thus making the hidden units saturate ) , while the denoising auto - encoder makes the whole mapping robust , i . e . , insen - sitive to small random perturbations , or contractive , making sure that the reconstruction cannot stay good when moving in most directions around a training example .
123 . 123 sparse auto - encoders the earliest uses of single - layer auto - encoders for building deep architectures by stacking them ( bengio et al . , 123 ) considered the idea of tying the encoder weights and decoder weights to restrict capacity as well as the idea of introducing a form of sparsity regularization ( ranzato et al . , 123 ) .
sparsity in the representation can be achieved by penalizing the hidden unit biases ( making these additive offset parameters more negative ) ( ranzato et al . , 123; lee et al . , 123; goodfellow et al . , 123; larochelle and bengio , 123 ) or by directly penalizing the output of the hidden unit activations ( making them closer to their saturating value at 123 ) ( ranzato et al . , 123; le et al . , 123a; zou et al . , 123 ) .
penalizing the bias runs the danger that the weights could compensate for the bias , which could hurt numerical optimization .
when directly penalizing the hidden unit outputs , several variants can be found in the literature , but a clear comparative analysis is still lacking .
although the l123 penalty ( i . e . , simply the sum of output elements hj in the case of sigmoid non - linearity ) would seem the most natural ( because of its use in sparse cod - ing ) , it is used in few papers involving sparse auto - encoders .
a close cousin of the l123 penalty is the student - t penalty j ) ) , originally proposed for sparse coding ( olshausen and field , 123 ) .
several papers penalize the average output hj ( e . g .
over a minibatch ) , and instead of pushing it to 123 , encourage it to approach a xed target , either through a mean - square error penalty , or maybe more sensibly ( because hj behaves like a probability ) , a kullback - liebler divergence with respect to the binomial distribution with probability : log hj ( 123 ) log ( 123 hj ) +constant , e . g . , with = 123 .
123 . 123 denoising auto - encoders vincent et al .
( 123 , 123 ) proposed altering the training ob - jective in eq .
123 from mere reconstruction to that of denoising an articially corrupted input , i . e .
learning to reconstruct the clean input from a corrupted version .
learning the identity is no longer enough : the learner must capture the structure of the input distribution in order to optimally undo the effect of the corruption process , with the reconstruction essentially being a nearby but higher density point than the corrupted input .
figure 123 illustrates that the denoising auto - encoder
( dae ) is learning a reconstruction function that corresponds to a vector eld pointing towards high - density regions ( the manifold where examples concentrate ) .
123 . 123 contractive auto - encoders contractive auto - encoders ( cae ) , proposed by rifai et al .
( 123a ) , follow up on denoising auto - encoders ( dae ) and share a similar motivation of learning robust representations .
caes achieve this by adding an analytic contractive penalty to eq .
123 : the frobenius norm of the encoders jacobian , and results in penalizing the sensitivity of learned features to innitesimal input variations .
let j ( x ) = f x ( x ) the jacobian matrix of the encoder at x .
the caes training objective is
l ( x ( t ) , g ( f ( x ( t ) ) ) ) +
when data concentrate near a lower - dimensional manifold , the corruption vector is typically almost orthogonal to the manifold , and the reconstruction function learns to denoise , map from low - probability congurations ( corrupted inputs ) to high - probability ones ( original inputs ) , creating a vector eld aligned with the score ( derivative of the estimated density ) .
formally , the objective optimized by a dae is :
q ( x|x ( t ) ) ( ) averages over corrupted examples x drawn from corruption process q ( x|x ( t ) ) .
in practice this is optimized by stochastic gradient descent , where the stochastic gradient is estimated by drawing one or a few corrupted versions of x ( t ) each time x ( t ) is considered .
corruptions considered in vin - cent et al .
( 123 ) include additive isotropic gaussian noise , salt and pepper noise for gray - scale images , and masking noise ( salt or pepper only ) , e . g . , setting some randomly chosen inputs to 123 ( independently per example ) .
masking noise has been used in most of the simulations .
qualitatively better features are reported with denoising , resulting in improved classication , and dae features performed similarly or better than rbm features .
chen et al .
( 123 ) show that a simpler alternative with a closed form solution can be obtained when restricting to a linear auto - encoder and have successfully applied it to domain adaptation .
vincent ( 123 ) relates daes to energy - based probabilistic models : daes basically learn in r ( x ) x a vector pointing in the direction of the estimated score log p ( x ) ( figure 123 ) .
in the special case of linear reconstruction and squared error , vincent ( 123 ) shows that training an afne - sigmoid - afne dae amounts to learning an energy - based model , whose energy function is very close to that of a grbm .
training uses a regularized variant of the score matching parameter estima - tion technique ( hyvarinen , 123; hyvarinen , 123; kingma and lecun , 123 ) termed denoising score matching ( vincent , 123 ) .
swersky ( 123 ) had shown that training grbms with score matching is equivalent to training a regular auto - encoder with an additional regularization term , while , following up on the theoretical results in vincent ( 123 ) , swersky et al .
( 123 ) showed the practical advantage of denoising to implement score matching efciently .
finally alain and bengio ( 123 ) generalize vincent ( 123 ) and prove that daes of arbitrary parametrization with small gaussian corruption noise are general estimators of the score .
where is a hyper - parameter controlling the strength of the regularization .
for an afne sigmoid encoder , the contractive penalty term is easy to compute :
jj ( x ) = f ( x ) j ( 123 f ( x ) j ) wj
there are at least three notable differences with daes , which may be partly responsible for the better performance that cae features seem to empirically demonstrate : ( a ) the sensitivity of the features is penalized123 rather than that of the reconstruc - tion; ( b ) the penalty is analytic rather than stochastic : an ef - ciently computable expression replaces what might otherwise require dx corrupted samples to size up ( i . e .
the sensitivity in dx directions ) ; ( c ) a hyper - parameter allows a ne control of the trade - off between reconstruction and robustness ( while the two are mingled in a dae ) .
note however that there is a tight connection between the dae and the cae : as shown in ( alain and bengio , 123 ) a dae with small corruption noise can be seen ( through a taylor expansion ) as a type of contractive auto - encoder where the contractive penalty is on the whole reconstruction function rather than just on the encoder123
a potential disadvantage of the caes analytic penalty is that it amounts to only encouraging robustness to innitesimal input variations .
this is remedied in rifai et al .
( 123b ) with the cae+h , that penalizes all higher order derivatives , in an efcient stochastic manner , by adding a term that encourages j ( x ) and j ( x + ) to be close :
l ( x ( t ) , g ( x ( t ) ) ) +
( cid : 123 ) ( cid : 123 ) j ( x ) j ( x + ) ( cid : 123 ) 123
where n ( 123 , 123i ) , and is the associated regularization
the dae and cae have been successfully used to win the nal phase of the unsupervised and transfer learning challenge ( mesnil et al . , 123 ) .
the representation learned by the cae tends to be saturated rather than sparse , i . e . , most hidden units are near the extremes of their range ( e . g .
123 or 123 ) , and their derivative hi ( x ) is near 123
the non - saturated units are few and sensitive to the inputs , with their associated lters ( weight vectors ) together forming a basis explaining the local changes around x , as discussed in section 123 .
another way to get saturated ( nearly binary ) units is semantic hashing ( salakhutdinov and hinton , 123 ) .
i . e . , the robustness of the representation is encouraged .
but note that in the cae , the decoder weights are tied to the encoder weights , to avoid degenerate solutions , and this should also make the decoder
corrupted input corrupted input prior : &examples&concentrate&near&a&lower&dimensional&manifold&&original input ( cid : 123 )
123 . 123 predictive sparse decomposition sparse coding ( olshausen and field , 123 ) may be viewed as a kind of auto - encoder that uses a linear decoder with a squared reconstruction error , but whose non - parametric encoder f performs the comparatively non - trivial and relatively costly iterative minimization of eq .
a practically successful variant of sparse coding and auto - encoders , named predictive sparse decomposition or psd ( kavukcuoglu et al . , 123 ) replaces that costly and highly non - linear encoding step by a fast non - iterative approximation during recognition ( computing the learned features ) .
psd has been applied to object recognition in images and video ( kavukcuoglu et al . , 123 , 123; jarrett et al . , 123 ) , but also to audio ( henaff et al . , 123 ) , mostly within the framework of multi - stage convolutional deep archi - tectures ( section 123 ) .
the main idea can be summarized by the following equation for the training criterion , which is simultaneously optimized with respect to hidden codes ( representation ) h ( t ) and with respect to parameters ( w , ) : where x ( t ) is the input vector for example t , h ( t ) is the optimized hidden code for that example , and f ( ) is the encoding function , the simplest variant being f ( x ( t ) ) = tanh ( b + w t x ( t ) )
where encoding weights are the transpose of decoding weights .
many variants have been proposed , including the use of a shrinkage operation instead of the hyperbolic tan - gent ( kavukcuoglu et al . , 123 ) .
note how the l123 penalty on h tends to make them sparse , and how this is the same criterion as sparse coding with dictionary learning ( eq .
123 ) except for the additional constraint that one should be able to approximate the sparse codes h with a parametrized encoder f ( x ) .
one can thus view psd as an approximation to sparse coding , where we obtain a fast approximate encoder .
once psd is trained , object representations f ( x ) are used to feed a classier .
they are computed quickly and can be further ne - tuned : the encoder can be viewed as one stage or one layer of a trainable multi - stage system such as a feedforward neural network .
( cid : 123 ) h ( t ) ( cid : 123 ) 123 + ( cid : 123 ) x ( t ) w h ( t ) ( cid : 123 ) 123
123 + ( cid : 123 ) h ( t ) f ( x ( t ) ) ( cid : 123 ) 123
psd can also be seen as a kind of auto - encoder where the codes h are given some freedom that can help to further improve reconstruction .
one can also view the encoding penalty added on top of sparse coding as a kind of regularizer that forces the sparse codes to be nearly computable by a smooth and efcient encoder .
this is in contrast with the codes obtained by complete optimization of the sparse coding crite - rion , which are highly non - smooth or even non - differentiable , a problem that motivated other approaches to smooth the inferred codes of sparse coding ( bagnell and bradley , 123 ) , so a sparse coding stage could be jointly optimized along with following stages of a deep architecture .
123 representation learning as mani - another important perspective on representation learning is based on the geometric notion of manifold .
its premise is the manifold hypothesis , according to which real - world data presented in high dimensional spaces are expected to con - centrate in the vicinity of a manifold m of much lower
dimensionality dm , embedded in high dimensional input space rdx .
this prior seems particularly well suited for ai tasks such as those involving images , sounds or text , for which most uniformly sampled input congurations are unlike natural stimuli .
as soon as there is a notion of representation then one can think of a manifold by considering the vari - ations in input space , which are captured by or reected ( by corresponding changes ) in the learned representation .
to rst approximation , some directions are well preserved ( the tangent directions of the manifold ) while others arent ( directions orthogonal to the manifolds ) .
with this perspec - tive , the primary unsupervised learning task is then seen as modeling the structure of the data - supporting manifold123
the associated representation being learned can be associated with an intrinsic coordinate system on the embedded manifold .
the archetypal manifold modeling algorithm is , not surprisingly , also the archetypal low dimensional representation learning algorithm : principal component analysis , which models a linear manifold .
it was initially devised with the objective of nding the closest linear manifold to a cloud of data points .
the principal components , i . e .
the representation f ( x ) that pca yields for an input point x , uniquely locates its projection on that manifold : it corresponds to intrinsic co - ordinates on the manifold .
data manifold for complex real world domains are however expected to be strongly non - linear .
their modeling is sometimes approached as patchworks of locally linear tangent spaces ( vincent and bengio , 123; brand , 123 ) .
the large majority of algorithms built on this geometric perspective adopt a non - parametric approach , based on a training set nearest neighbor graph ( scholkopf et al . , 123; roweis and saul , 123; tenenbaum et al . , 123; brand , 123; belkin and niyogi , 123; donoho and grimes , 123; weinberger and saul , 123; hinton and roweis , 123; van der maaten and hinton , 123 ) .
in these non - parametric approaches , each high - dimensional training point has its own set of free low - dimensional embedding coordinates , which are optimized so that certain properties of the neighborhood graph computed in original high dimensional input space are best preserved .
these methods however do not directly learn a parametrized feature extraction function f ( x ) applicable to new test points123 , which seriously limits their use as feature extractors , except in a transductive setting .
comparatively few non - linear manifold learning methods have been proposed , learn a parametric map that can directly compute a representation for new points; we will focus on these .
123 learning a parametric mapping based on a some of the above non - parametric manifold learning al - gorithms can be modied to learn a parametric mapping instead of having free
i . e . , applicable to new points :
actually , data points need not strictly lie on the manifold , but the probability density is expected to fall off sharply as one moves away from it , and it may actually be constituted of several possibly disconnected manifolds with different intrinsic dimensionality .
for several of these techniques , representations for new points can be computed using the nystrom approximation as has been proposed as an extension in ( bengio et al . , 123 ) , but this remains cumbersome and
low - dimensional embedding coordinate parameters for each these coordinates are obtained through an explicitly parametrized function , as with the parametric vari - ant ( van der maaten , 123 ) of t - sne ( van der maaten and
instead , semi - supervised embedding ( weston et al . , 123 ) learns a direct encoding while taking into account the manifold hypothesis through a neighborhood graph .
a parametrized neural network architecture simultaneously learns a manifold embedding and a classier .
the training criterion encourages training set neigbhors to have similar representations .
the reduced and tightly controlled number of free param - eters in such parametric methods , compared to their pure non - parametric counterparts , forces models to generalize the manifold shape non - locally ( bengio et al . , 123b ) , which can translate into better features and nal performance ( van der maaten and hinton , 123 ) .
however , basing the modeling of manifolds on training set neighborhood relationships might be risky statistically in high dimensional spaces ( sparsely populated due to the curse of dimensionality ) as e . g .
most euclidean nearest neighbors risk having too little in common semantically .
the nearest neighbor graph is simply not enough densely populated to map out satisfyingly the wrinkles of the target manifold ( bengio and monperrus , 123; bengio et al . , 123b; bengio and lecun , 123 ) .
it can also become problematic computationally to consider all pairs of data points123 , which scales quadratically with training set size .
123 learning to represent non - linear manifolds can we learn a manifold without requiring nearest neighbor searches ? yes , for example , with regularized auto - encoders or pca .
in pca , the sensitivity of the extracted components ( the code ) to input changes is the same regardless of position x .
the tangent space is the same everywhere along the linear manifold .
by contrast , for a non - linear manifold , the tangent of the manifold changes as we move on the manifold , as illustrated in figure 123
in non - linear representation - learning algorithms it is convenient to think about the local variations in the representation as the input x is varied on the manifold , i . e . , as we move among high - probability congurations .
as we discuss below , the rst derivative of the encoder therefore species the shape of the manifold ( its tangent plane ) around an example x lying on it .
if the density was really concentrated on the manifold , and the encoder had captured that , we would nd the encoder derivatives to be non - zero only in the directions spanned by the tangent plane .
let us consider sparse coding in this light : parameter matrix w may be interpreted as a dictionary of input directions from which a different subset will be picked to model the local tangent space at an x on the manifold .
that subset corresponds to the active , i . e .
non - zero , features for input x .
non - zero component hi will be sensitive to small changes of the input in the direction of the associated weight vector w : , i , whereas inactive features are more likely to be stuck at 123 until a signicant displacement has taken place in input space .
even if pairs are picked stochastically , many must be considered before
obtaining one that weighs signicantly on the optimization objective .
the local coordinate coding ( lcc ) algorithm ( yu et al . , 123 ) is very similar to sparse coding , but is explicitly derived from a manifold perspective .
using the same notation as that of sparse coding in eq .
123 , lcc replaces regularization term
( cid : 123 ) x ( t ) w h ( t ) ( cid : 123 ) 123
j | yielding objective
j | ( cid : 123 ) w : , j x ( t ) ( cid : 123 ) 123+p
this is identical to sparse coding when p = 123 , but with larger p it encourages the active anchor points for x ( t ) ( i . e .
the codebook vectors w : , j with non - negligible |h ( t ) are combined to reconstruct x ( t ) ) to be not too far from x ( t ) , hence the local aspect of the algorithm .
an important theoretical contribution of yu et al .
( 123 ) is to show that that any lipschitz - smooth function : m r dened on a smooth nonlinear manifold m embedded in rdx can be well approximated by a globally linear function with respect to the resulting coding scheme ( i . e .
linear in h ) , where the accuracy of the approximation and required number dh of anchor points depend on dm rather than dx .
this result has been further extended with the use of local tangent directions ( yu and zhang , 123 ) , as well as to multiple layers ( lin et al . , 123 ) .
the efcient non - iterative feed - forward encoders f , used by psd and the auto - encoders reviewed in section 123 , that are in the form of eq .
123 or 123the computed representation for x will be only signi - cantly sensitive to input space directions associated with non - saturated hidden units ( see e . g .
123 for the jacobian of a sigmoid layer ) .
these directions to which the representation is signicantly sensitive , like in the case of pca or sparse coding , may be viewed as spanning the tangent space of the manifold at training point x .
let us now consider
the tangent vectors to the high - density manifold as estimated by a contractive auto - encoder ( rifai et al . , 123a ) .
the original input is shown on the top left .
each tangent vector ( images on right side of rst row ) corresponds to a plausible additive deformation of the original input , as illustrated on the second row , where a bit of the 123rd singular vector is added to the original , to form a translated and deformed image .
unlike in pca , the tangent vectors are different for different inputs , because the estimated manifold is highly non - linear .
rifai et al .
( 123a ) empirically analyze in this light the singular value spectrum of the jacobian ( derivatives of rep - resentation vector with respect to input vector ) of a trained cae .
here the svd provides an ordered orthonormal basis of most sensitive directions .
the spectrum is sharply decreasing , indicating a relatively small number of signicantly sensi - tive directions .
this is taken as empirical evidence that the cae indeed modeled the tangent space of a low - dimensional manifold .
the leading singular vectors form a basis for the tangent plane of the estimated manifold , as illustrated in figure 123
the cae criterion is believed to achieve this thanks to its two opposing terms : the isotropic contractive penalty ,
123 " mnist " input " point " tangents " that encourages the representation to be equally insensitive to changes in any input directions , and the reconstruction term , that pushes different training points ( in particular neighbors ) to have a different representation ( so they may be reconstructed accurately ) , thus counteracting the isotropic contractive pres - sure only in directions tangent to the manifold .
analyzing learned representations through the lens of the spectrum of the jacobian and relating it to the notion of tangent space of a manifold is feasible , whenever the mapping is differentiable , and regardless of how it was learned , whether as direct encoding ( as in auto - encoder variants ) , or derived from latent variable inference ( as in sparse coding or rbms ) .
exact low dimensional manifold models ( like pca ) would yield non - zero singular values associated to directions along the manifold , and exact zeros for directions orthogonal to the manifold .
but in smooth models like the cae or the rbm we will instead have large versus relatively small singular values ( as opposed to non - zero versus exactly zero ) .
123 leveraging the modeled tangent spaces the local tangent space , at a point along the manifold , can be thought of capturing locally valid transformations that were prominent in the training data .
for example rifai et al .
( 123c ) examine the tangent directions extracted with an svd of the jacobian of caes trained on digits , images , or text - document data : they appear to correspond to small transla - tions or rotations for images or digits , and to substitutions of words within a same theme for documents .
such very local transformations along a data manifold are not expected to change class identity .
to build their manifold tangent classier ( mtc ) , rifai et al .
( 123c ) then apply techniques such as tangent distance ( simard et al . , 123 ) and tangent propagation ( simard et al . , 123 ) , that were initially developed to build classiers that are insensitive to input deformations provided as prior domain knowledge .
now these techniques are applied using the local leading tangent directions extracted by a cae , i . e .
not using any prior domain knowledge ( except the broad prior about the existence of a manifold ) .
this approach set a new record for mnist digit classication among prior - knowledge free approaches123
123 connections between probabilistic and direct encoding models the standard likelihood framework for probabilistic mod - els decomposes the training criterion for models with pa - rameters in two parts : the log - likelihood log p ( x| ) ( or log p ( x|h , ) with latent variables h ) , and the prior log p ( ) ( or log p ( h| ) + log p ( ) with latent variables ) .
123 psd : a probabilistic interpretation in the case of the psd algorithm , a connection can be made between the above standard probabilistic view and the direct encoding computation graph .
the probabilistic model of psd is the same directed generative model p ( x|h ) of sparse coding ( section 123 . 123 ) , which only accounts for the decoder .
the encoder is viewed as an approximate inference mechanism to
guess p ( h|x ) and initialize a map iterative inference ( where the sparse prior p ( h ) is taken into account ) .
however , in psd , the encoder is trained jointly with the decoder , rather than simply taking the end result of iterative inference as a target to approximate .
an interesting view123 to reconcile these facts is that the encoder is a parametric approximation for the map solution of a variational lower bound on the joint log - likelihood .
when map learning is viewed as a special case of variational learning ( where the approximation of the joint log - likelihood is with a dirac distribution located at the map solution ) , the variational recipe tells us to simultaneously improve the likelihood ( reduce reconstruction error ) and im - prove the variational approximation ( reduce the discrepancy between the encoder output and the latent variable value ) .
hence psd sits at the intersection of probabilistic models ( with latent variables ) and direct encoding methods ( which directly parametrize the mapping from input to representation ) .
rbms also sit at the intersection because their particular parametrization includes an explicit mapping from input to representation , thanks to the restricted connectivity between hidden units .
however , this nice property does not extend to their natural deep generalizations , i . e . , deep boltzmann machines , discussed in section 123 .
123 regularized auto - encoders capture local structure of the density can we also say something about the probabilistic interpreta - tion of regularized auto - encoders ? their training criterion does not t the standard likelihood framework because this would involve a data - dependent prior .
an interesting hypothesis emerges to answer that question , out of recent theoretical results ( vincent , 123; alain and bengio , 123 ) : the training criterion of regularized auto - encoders , instead of being a form of maximum likelihood , corresponds to a different inductive principle , such as score matching .
the score matching con - nection is discussed in section 123 . 123 and has been shown for a particular parametrization of dae and equivalent gaussian rbm ( vincent , 123 ) .
the work in alain and bengio ( 123 ) generalizes this idea to a broader class of parametrizations ( ar - bitrary encoders and decoders ) , and shows that by regularizing the auto - encoder so that it be contractive , one obtains that the reconstruction function and its derivative estimate rst and second derivatives of the underlying data - generative density .
this view can be exploited to successfully sample from auto - encoders , as shown in rifai et al .
( 123 ) ; bengio et al .
( 123 ) .
the proposed sampling algorithms are mcmcs similar to langevin mcmc , using not just the estimated rst derivative of the density but also the estimated manifold tangents so as to stay close to manifolds of high density .
this interpretation connects well with the geometric per - spective introduced in section 123
the regularization effects ( e . g . , due to a sparsity regularizer , a contractive regularizer , or the denoising criterion ) asks the learned representation to be as insensitive as possible to the input , while minimiz - ing reconstruction error on the training examples forces the representation to contain just enough information to distin -
it yielded 123% error rate using the full mnist training set , with no
prior deformations , and no convolution .
suggested by ian goodfellow , personal communication
reconstruction ) and ( b ) adds noise in the directions of the leading singular vectors of the reconstruction ( or encoder ) jacobian , corresponding to those associated with smallest second derivative of the log - density .
123 learning approximate inference let us now consider from closer how a representation is computed in probabilistic models with latent variables , when iterative inference is required .
there is a computation graph ( possibly with random number generation in some of the nodes , in the case of mcmc ) that maps inputs to repre - sentation , and in the case of deterministic inference ( e . g . , map inference or variational inference ) , that function could be optimized directly .
this is a way to generalize psd that has been explored in recent work on probabilistic models at the intersection of inference and learning ( bagnell and bradley , 123; gregor and lecun , 123b; grubb and bagnell , 123; salakhutdinov and larochelle , 123; stoyanov et al . , 123; eisner , 123 ) , where a central idea is that instead of using a generic inference mechanism , one can use one that is learned and is more efcient , taking advantage of the specics of the type of data on which it is applied .
123 sampling challenges a troubling challenge with many probabilistic models with latent variables like most boltzmann machine variants is that good mcmc sampling is required as part of the learning procedure , but that sampling becomes extremely inefcient ( or unreliable ) as training progresses because the modes of the learned distribution become sharper , making mixing between modes very slow .
whereas initially during training a learner as - signs mass almost uniformly , as training progresses , its entropy decreases , approaching the entropy of the target distribution as more examples and more computation are provided .
according to our manifold and natural clustering priors of section 123 , the target distribution has sharp modes ( manifolds ) separated by extremely low density areas .
mixing then becomes more difcult because mcmc methods , by their very nature , tend to make small steps to nearby high - probability congurations .
this is illustrated in figure 123
reconstruction function r ( x ) ( green ) learned by a high - capacity autoencoder on 123 - dimensional input , minimizing reconstruction error at training examples x ( t ) ( r ( x ( t ) ) in red ) while trying to be as constant as possible otherwise .
the dotted line is the identity reconstruction ( which might be obtained without the regularizer ) .
the blue arrows shows the vector eld of r ( x ) x pointing towards high density peaks estimated by the model , and estimating the score ( log - density derivative ) .
guish them .
the solution is that variations along the high - density manifolds are preserved while other variations are compressed : the reconstruction function should be as constant as possible while reproducing training examples , i . e . , points near a training example should be mapped to that training example ( figure 123 ) .
the reconstruction function should map an input towards the nearest point manifold , i . e . , the difference between reconstruction and input is a vector aligned with the estimated score ( the derivative of the log - density with respect to the input ) .
the score can be zero on the manifold ( where reconstruction error is also zero ) , at local maxima of the log - density , but it can also be zero at local minima .
it means that we cannot equate low reconstruction error with high estimated probability .
the second derivatives of the log - density corresponds to the rst derivatives of the reconstruction function , and on the manifold ( where the rst derivative is 123 ) , they indicate the tangent directions of the manifold ( where the rst derivative remains near 123 ) .
sampling from regularized auto - encoders ( rifai et al . , 123; bengio et al . , 123 ) : each mcmc step adds to current state x the noise mostly in the directions of the estimated man - ifold tangent plane h and projects back towards the manifold ( high - density regions ) by performing a reconstruction step .
as illustrated in figure 123 , the basic idea of the auto - encoder sampling algorithms in rifai et al .
( 123 ) ; bengio et al .
( 123 ) is to make mcmc moves where one ( a ) moves toward the manifold by following the density gradient ( i . e . , applying a
top : early during training , mcmc mixes easily between modes because the estimated distribution has high entropy and puts enough mass everywhere for small - steps movements ( mcmc ) to go from mode to mode .
bottom : later on , training relying on good mixing can stall because estimated modes are separated by wide low - density deserts .
bengio et al .
( 123 ) suggest that deep representations could help mixing between such well separated modes , based on both theoretical arguments and on empirical evidence .
the idea is that if higher - level representations disentangle better
x " r ( x ) " x123 " x123 " x123 " 123 " the underlying abstract factors , then small steps in this abstract space ( e . g . , swapping from one category to another ) can easily be done by mcmc .
the high - level representations can then be mapped back to the input space in order to obtain input - level samples , as in the deep belief networks ( dbn ) sampling algorithm ( hinton et al . , 123 ) .
this has been demonstrated both with dbns and with the newly proposed algorithm for sampling from contracting and denoising auto - encoders ( rifai et al . , 123; bengio et al . , 123 ) .
this observation alone does not sufce to solve the problem of training a dbn or a dbm , but it may provide a crucial ingredient , and it makes it possible to consider successfully sampling from deep models trained by procedures that do not require an mcmc , like the stacked regularized auto - encoders used in rifai et al .
( 123 ) .
123 evaluating and monitoring performance it is always possible to evaluate a feature learning algorithm in terms of its usefulness with respect to a particular task ( e . g .
object classication ) , with a predictor that is fed or initialized with the learned features .
in practice , we do this by saving the features learned ( e . g .
at regular intervals during training , to perform early stopping ) and training a cheap classier on top ( such as a linear classier ) .
however , training the nal classier can be a substantial computational overhead ( e . g . , supervised ne - tuning a deep neural network takes usually more training iterations than the feature learning itself ) , so we may want to avoid having to train a classier for ev - ery training iteration of the unsupervised learner and every hyper - parameter setting .
more importantly this may give an incomplete evaluation of the features ( what would happen for other tasks ? ) .
all these issues motivate the use of methods to monitor and evaluate purely unsupervised performance .
this is rather easy with all the auto - encoder variants ( with some caution outlined below ) and rather difcult with the undirected graphical models such as the rbm and boltzmann machines .
for auto - encoder and sparse coding variants , test set re - construction error can readily be computed , but by itself may be misleading because larger capacity ( e . g . , more features , more training time ) tends to systematically lead to lower reconstruction error , even on the test set .
hence it cannot be used reliably for selecting most hyper - parameters .
on the other hand , denoising reconstruction error is clearly immune to this problem , so that solves the problem for daes .
based on the connection between daes and caes uncovered in bengio et al .
( 123 ) ; alain and bengio ( 123 ) , this immunity can be extended to daes , but not to the hyper - parameter controlling the amount of noise or of contraction .
for rbms and some ( not too deep ) boltzmann machines , one option is the use of annealed importance sampling ( mur - ray and salakhutdinov , 123 ) in order to estimate the partition function ( and thus the test log - likelihood ) .
note that this esti - mator can have high variance and that it becomes less reliable ( variance becomes too large ) as the model becomes more interesting , with larger weights , more non - linearity , sharper modes and a sharper probability density function ( see our previous discussion in section 123 ) .
another interesting and recently proposed option for rbms is to track the partition function during training ( desjardins et al . , 123 ) , which could
be useful for early stopping and reducing the cost of ordinary ais .
for toy rbms ( e . g . , 123 hidden units or less , or 123 inputs or less ) , the exact log - likelihood can also be computed analytically , and this can be a good way to debug and verify some properties of interest .
123 global training of deep models one of the most interesting challenges raised by deep archi - tectures is : how should we jointly train all the levels ? in the previous section and in section 123 we have only discussed how single - layer models could be combined to form a deep model .
here we consider joint training of all the levels and the difculties that may arise .
123 the challenge of training deep architectures higher - level abstraction means more non - linearity .
it means that two nearby input congurations may be interpreted very differently because a few surface details change the underlying semantics , whereas most other changes in the surface details would not change the underlying semantics .
the representa - tions associated with input manifolds may be complex because the mapping from input to representation may have to unfold and distort input manifolds that generally have complicated shapes into spaces where distributions are much simpler , where relations between factors are simpler , maybe even linear or involving many ( conditional ) independencies .
our expectation is that modeling the joint distribution between high - level abstractions and concepts should be much easier in the sense of requiring much less data to learn .
the hard part is learning a good representation that does this unfolding and disentangling .
this may be at the price of a more difcult training problem , possibly involving ill - conditioning and local minima .
it is only since 123 that researchers have seriously inves - tigated ways to train deep architectures , to the exception of the convolutional networks ( lecun et al . , 123b ) .
the rst realization ( section 123 ) was that unsupervised or supervised layer - wise training was easier , and that this could be taken advantage of by stacking single - layer models into deeper ones .
it is interesting to ask why does the layerwise unsuper - vised pre - training procedure sometimes help a supervised learner ( erhan et al . , 123b ) .
there seems to be a more general principle at play 123 of guiding the training of inter - mediate representations , which may be easier than trying to learn it all in one go .
this is nicely related to the curriculum learning idea ( bengio et al . , 123 ) , that it may be much easier to learn simpler concepts rst and then build higher - level ones on top of simpler ones .
this is also coherent with the success of several deep learning algorithms that provide some such guidance for intermediate representations , supervised embedding ( weston et al . , 123 ) .
the question of why unsupervised pre - training could be helpful was extensively studied ( erhan et al . , 123b ) , trying the answer into a regularization effect and an optimization effect .
the regularization effect is clear from the experiments where the stacked rbms or denoising auto - encoders are used to initialize a supervised classication neural network ( erhan et al . , 123b ) .
it may simply come from the
first suggested to us by leon bottou
use of unsupervised learning to bias the learning dynamics and initialize it in the basin of attraction of a good local minimum ( of the training criterion ) , where good is in terms of generalization error .
the underlying hypothesis exploited by this procedure is that some of the features or latent factors that are good at capturing the leading variations in the input distribution are also good at capturing the variations in the target output random variables of interest ( e . g . , classes ) .
the optimization effect is more difcult to tease out because the top two layers of a deep neural net can just overt the training set whether the lower layers compute useful features or not , but there are several indications that optimizing the lower levels with respect to a supervised training criterion can be
one such indication is that changing the numerical con - ditions of the optimization procedure can have a profound impact on the joint training of a deep architecture , for ex - ample by changing the initialization range and changing the type of non - linearity used ( glorot and bengio , 123 ) , much more so than with shallow architectures .
one hypothesis to explain some of the difculty in the optimization of deep architectures is centered on the singular values of the jacobian matrix associated with the transformation from the features at one level into the features at the next level ( glorot and bengio , 123 ) .
if these singular values are all small ( less than 123 ) , then the mapping is contractive in every direction and gradients would vanish when propagated backwards through many layers .
this is a problem already discussed for recurrent neural networks ( bengio et al . , 123 ) , which can be seen as very deep networks with shared parameters at each layer , when unfolded in time .
this optimization difculty has motivated the exploration of second - order methods for deep architectures and recurrent networks , in particular hessian - free second - order methods ( martens , 123; martens and sutskever , 123 ) .
unsupervised pre - training has also been proposed to help training recurrent networks and temporal rbms ( sutskever et al . , 123 ) , i . e . , at each time step there is a local signal to guide the discovery of good features to capture in the state variables : model with the current state ( as hidden units ) the joint distribution of the previous state and the current input .
natural gradient ( amari , 123 ) methods that can be applied to networks with millions of parameters ( i . e .
with good scaling properties ) have also been proposed ( le roux et al . , 123b; pascanu and bengio , 123 ) .
cho et al .
( 123 ) proposes to use adaptive learning rates for rbm training , along with a novel and interesting idea for a gradient estimator that takes into account the invariance of the model to ipping hidden unit bits and inverting signs of corresponding weight vectors .
at least one study indicates that the choice of initialization ( to make the jacobian of each layer closer to 123 across all its singular values ) could substantially reduce the training difculty of deep networks ( glorot and bengio , 123 ) and this is coherent with the success of the initialization procedure of echo state networks ( jaeger , 123 ) , as recently studied by sutskever ( 123 ) .
there are also several experimental results ( glorot and bengio , 123; glorot et al . , 123a; nair and hinton , 123 ) showing that the choice of hidden units non - linearity could inuence both training and generalization performance , with
particularly interesting results obtained with sparse rectifying units ( jarrett et al . , 123; nair and hinton , 123; glorot et al . , 123a; krizhevsky et al . , 123 ) .
an old idea regarding the ill - conditioning issue with neural networks is that of symmetry breaking : part of the slowness of convergence may be due to many units moving together ( like sheep ) and all trying to reduce the output error for the same examples .
by initializing with sparse weights ( martens , 123 ) or by using often saturated non - linearities ( such as rectiers as max - pooling units ) , gradients only ow along a few paths , which may help hidden units to specialize more quickly .
another promising idea to improve the conditioning of neural network training is to nullify the average value and slope of each hidden unit output ( raiko et al . , 123 ) , and possibly locally normalize magnitude as well ( jarrett et al . , 123 ) .
the debate still rages between using online methods such as stochastic gradient descent and using second - order methods on large minibatches ( of several thousand examples ) ( martens , 123; le et al . , 123a ) , with a variant of stochastic gradient descent recently winning an optimization challenge 123
that with proper
finally , several recent results exploiting large quantities labeled data suggest and choice of non - linearity , very deep purely supervised networks can be trained successfully without any layerwise pre - training ( ciresan et al . , 123; glorot et al . , 123a; seide et al . , 123a; krizhevsky et al . , 123 ) .
researchers report than in such conditions , layerwise unsupervised pre - training little or no improvement over pure supervised learning from scratch when training for long enough .
this that unsupervised pre - training acts as a prior , which may be less necessary when very large quantities of labeled data are available , but begs the question of why this had not been discovered earlier .
the latest results reported in this respect ( krizhevsky et al . , 123 ) are particularly interesting because they allowed to drastically reduce the error rate of object recognition on a benchmark ( the 123 - class imagenet task ) where many more traditional computer vision approaches had been evaluated the main techniques that allowed this success include the following : efcient gpu training allowing one to train longer ( more than 123 million visits of examples ) , an aspect rst reported by lee et al .
( 123a ) ; ciresan et al .
( 123 ) , large number of labeled examples , articially transformed examples ( see section 123 ) , a large number of tasks ( 123 or 123 classes for imagenet ) , convolutional architecture with max - pooling ( see section 123 for initialization ( discussed above ) , careful parameter update and adaptive learning rate heuristics , feature normalization ( across features ) , and a new dropout trick based on injecting strong binary multiplicative noise on hidden units .
this trick is similar to the binary noise injection used at each layer of a stack of denoising auto - encoders .
future work is hopefully going to help identify which of these elements matter most , how to generalize them across
a large variety of tasks and architectures , and in particular contexts where most examples are unlabeled , i . e . , including an unsupervised component in the training criterion .
123 joint training of deep boltzmann machines we now consider the problem of joint training of all layers of a specic unsupervised model , the deep boltzmann machine ( dbm ) .
whereas much progress ( albeit with many unan - swered questions ) has been made on jointly training all the layers of deep architectures using back - propagated gradients ( i . e . , mostly in the supervised setting ) , much less work has been done on their purely unsupervised counterpart , e . g .
with dbms123
note however that one could hope that the successful techniques described in the previous section could be applied to unsupervised learning algorithms .
like the rbm , the dbm is another particular subset of the boltzmann machine family of models where the units are again arranged in layers .
however unlike the rbm , the dbm possesses multiple layers of hidden units , with units in odd - numbered layers being conditionally independent given even - numbered layers , and vice - versa .
with respect to the boltzmann energy function of eq .
123 , the dbm corresponds to setting u = 123 and a sparse connectivity structure in both v and w .
we can make the structure of the dbm more explicit by specifying its energy function .
for the model with two hidden layers it is given as :
( v , h ( 123 ) , h ( 123 ) ; ) = vt w h ( 123 ) h ( 123 ) t h ( 123 ) d ( 123 ) t
h ( 123 ) bt v ,
with = ( w , v , d ( 123 ) , d ( 123 ) , b ) .
the dbm can also be char - acterized as a bipartite graph between two sets of vertices , formed by odd and even - numbered layers ( with v : = h ( 123 ) ) .
123 . 123 mean - eld approximate inference a key point of departure from the rbm is that the pos - terior distribution over the hidden units ( given the visibles ) is no longer tractable , due to the interactions between the hidden units .
salakhutdinov and hinton ( 123 ) resort to a mean - eld approximation to the posterior .
specically , the case of a model with two hidden layers , we wish to
approximate p ( cid : 123 ) h ( 123 ) , h ( 123 ) | v ( cid : 123 ) with the factored distribution qv ( h ( 123 ) , h ( 123 ) ) = ( cid : 123 ) n123 that the kl divergence kl ( cid : 123 ) p ( cid : 123 ) h ( 123 ) , h ( 123 ) | v ( cid : 123 ) ( cid : 123 ) qv ( h123 , h123 ) ( cid : 123 ) ( cid : 123 ) p ( v , h ( 123 ) , h ( 123 ) ) log p ( v ) > l ( qv ) ( cid : 123 )
is minimized or equivalently , that a lower bound to the log likelihood is maximized :
qv ( h ( 123 ) , h ( 123 ) ) log
maximizing this lower - bound with respect to the mean - eld distribution qv ( h123 , h123 ) ( by setting derivatives to zero ) yields the following mean eld update equations :
i + d ( 123 )
k + d ( 123 )
joint training of all the layers of a deep belief net is much more
challenging because of the much harder inference problem involved .
note how the above equations ostensibly look like a xed point recurrent neural network , i . e . , with constant input .
in the same way that an rbm can be associated with a simple auto - encoder , the above mean - eld update equations for the dbm can be associated with a recurrent auto - encoder .
in that case the training criterion involves the reconstruction error at the last or at consecutive time steps .
this type of model has been explored by savard ( 123 ) and seung ( 123 ) and shown to do a better job at denoising than ordinary auto - encoders .
iterating eq .
( 123 - 123 ) until convergence yields the q param -
eters of the variational positive phase of eq
log p ( v , h ( 123 ) , h ( 123 ) ) log qv ( h ( 123 ) , h ( 123 ) )
( v , h ( 123 ) , h ( 123 ) ) log qv ( h ( 123 ) , h ( 123 ) )
( cid : 123 ) e dbm ( cid : 123 ) e dbm
( v , h ( 123 ) , h ( 123 ) )
( v , h ( 123 ) , h ( 123 ) )
learning procedure leaves the negative phase untouched , which can thus be estimated through sml or contrastive divergence ( hinton , 123 ) as in the rbm case .
123 . 123 training deep boltzmann machines the major difference between training a dbm and an rbm instead of maximizing the likelihood directly , we instead choose parameters to maximize the lower - bound on the likelihood given in eq .
the sml - based algorithm for maximizing this lower - bound is as follows :
123 ) clamp the visible units to a training example .
123 ) iterate over eq .
( 123 - 123 ) until convergence .
123 ) generate negative phase samples v , h ( 123 ) and h ( 123 ) 123 ) compute l ( qv ) / using the values obtained in steps
123 ) finally , update the model parameters with a step of
approximate stochastic gradient ascent .
while the above procedure appears to be a simple extension of the highly effective sml scheme for training rbms , as we demonstrate in desjardins et al .
( 123 ) , this procedure seems vulnerable to falling in poor local minima which leave many hidden units effectively dead ( not signicantly different from its random initialization with small norm ) .
the failure of the sml joint training strategy was noted by salakhutdinov and hinton ( 123 ) .
as an alternative , they proposed a greedy layer - wise training strategy .
this procedure consists in pre - training the layers of the dbm , in much the same way as the deep belief network : i . e .
by stacking rbms and training each layer to independently model the output of the previous layer .
a nal joint ne - tuning is done following the above sml - based procedure .
123 building - in invariance it is well understood that incorporating prior domain knowl - edge helps machine learning .
exploring good strategies for doing so is a very important research avenue .
however , if we are to advance our understanding of core machine learning principles , it is important that we keep comparisons between predictors fair and maintain a clear awareness of the prior
domain knowledge used by different especially when comparing their performance on benchmark problems .
we have so far only presented algorithms that exploited only generic inductive biases for high dimensional thus making them potentially applicable to any high dimensional problem .
the most prevalent approach to incorporating prior knowledge is to hand - design better features to feed a generic classier , and has been used extensively in computer vision ( e . g .
( lowe , 123 ) ) .
here , we rather focus on how basic domain knowledge of the input , in particular its topological structure ( e . g .
bitmap images having a 123d structure ) , may be used to learn better features .
123 generating transformed examples generalization performance is usually improved by providing a larger quantity of representative data .
this can be achieved by generating new examples by applying small random defor - mations to the original training examples , using deformations that are known not to change the target variables of interest , e . g . , an object class is invariant to small transformations of images such as translations , rotations , scaling , or shearing .
this old approach ( baird , 123 ) has been recently applied with great success in the work of ciresan et al .
( 123 ) who used an efcient gpu implementation ( 123 speedup ) to train a stan - dard but large deep multilayer perceptron on deformed mnist digits .
using both afne and elastic deformations ( simard et al . , 123 ) , with plain old stochastic gradient descent , they reach a record 123% classication error rate .
123 convolution and pooling another powerful approach is based on even more basic knowledge of merely the topological structure of the input dimensions .
by this we mean e . g . , the 123d layout of pixels in images or audio spectrograms , the 123d structure of videos , the 123d sequential structure of text or of temporal sequences in general .
based on such structure , one can dene local receptive elds ( hubel and wiesel , 123 ) , so that each low - level feature will be computed from only a subset of the input : a neighborhood in the topology ( e . g .
a sub - image at a given position ) .
this topological locality constraint corresponds to a layer having a very sparse weight matrix with non - zeros only allowed for topologically local connections .
computing the associated matrix products can of course be made much more efcient than having to handle a dense matrix , in addition to the statistical gain from a much smaller number of free parameters .
in domains with such topological structure , similar input patterns are likely to appear at different positions , and nearby values ( e . g .
consecutive frames or nearby pixels ) are likely to have stronger dependencies that are also important to model the data .
in fact these dependencies can be exploited to discover the topology ( le roux et al . , 123a ) , i . e .
recover a regular grid of pixels out of a set of vectors without any order information , e . g .
after the elements have been arbitrarily shufed in the same way for all examples .
thus a same local feature computation is likely to be relevant at all translated po - sitions of the receptive eld .
hence the idea of sweeping such a local feature extractor over the topology : this corresponds to a convolution , and transforms an input into a similarly shaped
feature map .
equivalently to sweeping , this may be seen as static but differently positioned replicated feature extractors that all share the same parameters .
this is at the heart of convolutional networks ( lecun et al . , 123 , 123b ) which have been applied both to object recognition and to image segmentation ( turaga et al . , 123 ) .
another hallmark of the convolutional architecture is that values computed by the same feature detector applied at several neighboring input locations are then summarized through a pooling operation , typically taking their max or their sum .
this confers the resulting pooled feature layer some degree of invariance to input translations , and this style of architecture ( alternating selective feature extraction and invariance - creating pooling ) has been the ba - sis of convolutional networks , the neocognitron ( fukushima , 123 ) and hmax ( riesenhuber and poggio , 123 ) models , and argued to be the architecture used by mammalian brains for object recognition ( riesenhuber and poggio , 123; serre et al . , 123; dicarlo et al . , 123 ) .
the output of a pooling unit will be the same irrespective of where a specic feature is located inside its pooling region .
empirically the use of pooling seems to contribute signicantly to improved classi - cation accuracy in object classication tasks ( lecun et al . , 123b; boureau et al . , 123 , 123 ) .
a successful variant of pooling connected to sparse coding is l123 pooling ( hyvarinen et al . , 123; kavukcuoglu et al . , 123; le et al . , 123 ) , for which the pool output is the square root of the possibly weighted sum of squares of lter outputs .
ideally , we would like to generalize feature - pooling so as to learn what features should be pooled together , e . g .
as successfully done in several papers ( hyvarinen and hoyer , 123; kavukcuoglu et al . , 123; le et al . , 123; ranzato and hinton , 123; courville et al . , 123b; coates and ng , 123b; gregor et al . , 123 ) .
in this way , the pool output learns to be invariant to the variations captured by the span of the features pooled .
the simplest approach for learning a convolutional layer in an unsupervised fashion is patch - based training : simply feeding a generic unsupervised feature learning algorithm with local patches extracted at random positions of the inputs .
the resulting feature extractor can then be swiped over the input to produce the convolutional feature maps .
that map may be used as a new input for the next layer , and the operation repeated to thus learn and stack several layers .
such an approach was recently used with independent subspace analysis ( le et al . , 123c ) on 123d video blocks , reaching the state - of - the - art on hollywood123 , ucf , kth and youtube action recognition datasets .
similarly ( coates and ng , 123a ) compared several feature learners with patch - based training and reached state - of - the - art results on several classication benchmarks .
inter - estingly , in this work performance was almost as good with very simple k - means clustering as with more sophisticated feature learners .
we however conjecture that this is the case only because patches are rather low dimensional ( compared to the dimension of a whole image ) .
a large dataset might provide sufcient coverage of the space of e . g .
edges prevalent in 123 123 patches , so that a distributed representation is not absolutely necessary .
another plausible explanation for this
success is that the clusters identied in each image patch are then pooled into a histogram of cluster counts associated with a larger sub - image .
whereas the output of a regular clustering is a one - hot non - distributed code , this histogram is itself a distributed representation , and the soft k - means ( coates and ng , 123a ) representation allows not only the nearest lter but also its neighbors to be active .
convolutional and tiled - convolutional training it is possible to directly train large convolutional layers using an unsupervised criterion .
an early approach ( jain and seung , 123 ) trained a standard but deep convolutional mlp on the task of denoising images , i . e .
as a deep , convolutional , denoising auto - encoder .
convolutional versions of the rbm or its extensions have also been developed ( desjardins and bengio , 123; lee et al . , 123a; taylor et al . , 123 ) as well as a probabilistic max - pooling operation built into convolutional deep networks ( lee et al . , 123a , b; krizhevsky , 123 ) .
other unsupervised feature learning approaches that were adapted to the convolutional setting include psd ( kavukcuoglu et al . , 123 , 123; jarrett et al . , 123; henaff et al . , 123 ) , a convolutional version of sparse coding called deconvolutional networks ( zeiler et al . , 123 ) , topographic ica ( le et al . , 123 ) , and mpot that kivinen and williams ( 123 ) applied to modeling natural textures .
gregor and lecun ( 123a ) ; le et al .
( 123 ) also demonstrated the technique of tiled - convolution , where parameters are shared only between feature extractors whose receptive elds are k steps away ( so the ones looking at immediate neighbor locations are not shared ) .
this allows pooling units to be invariant to more than just translations , and is a hybrid between convolutional networks and earlier neural networks with local connections but no weight sharing ( lecun , 123 , 123 ) .
alternatives to pooling alternatively , one can also use explicit knowledge of the expected invariants expressed mathematically to dene trans - formations that are robust to a known family of input defor - mations , using so - called scattering operators ( mallat , 123; bruna and mallat , 123 ) , which can be computed in a way interestingly analogous to deep convolutional networks and wavelets .
like convolutional networks , the scattering operators alternate two types of operations : convolution and pooling ( as a norm ) .
unlike convolutional networks , approach keeps at each level all of the information about the input ( in a way that can be inverted ) , and automatically yields a very sparse ( but very high - dimensional ) representation .
an - other difference is that the lters are not learned but instead set so as to guarantee that a priori specied invariances are robustly achieved .
just a few levels were sufcient to achieve impressive results on several benchmark datasets .
123 temporal coherence and slow features the principle of identifying slowly moving / changing factors in temporal / spatial data has been investigated by many ( becker and hinton , 123; wiskott and sejnowski , 123; hurri and hyvarinen , 123; kording et al . , 123; cadieu and olshausen , 123 ) as a principle for nding useful representations
particular this idea has been applied to image sequences and as an explanation for why v123 simple and complex cells behave the way they do .
a good overview can be found in hurri and hyvarinen ( 123 ) ; berkes and wiskott ( 123 ) .
more recently , temporal coherence has been successfully exploited in deep architectures to model video ( mobahi et al . , 123 ) .
it was also found that temporal coherence discov - ered visual features similar to those obtained by ordinary unsupervised feature learning ( bergstra and bengio , 123 ) , and a temporal coherence penalty has been combined with a training criterion for unsupervised feature learning ( zou et al . , 123 ) , sparse auto - encoders with l123 regularization , in this case , yielding improved classication performance .
the temporal coherence prior can be expressed in several the simplest being the squared difference between feature values at times t and t + 123
other plausible tempo - ral coherence priors include the following .
first , instead of penalizing the squared change , penalizing the absolute value ( or a similar sparsity penalty ) would state that most of the time the change should be exactly 123 , which would intuitively make sense for the real - life factors that surround us .
second , one would expect that instead of just being slowly changing , different factors could be associated with their own different time scale .
the specicity of their time scale could thus become a hint to disentangle explanatory factors .
third , one would expect that some factors should really be represented by a group of numbers ( such as x , y , and z position of some object in space and the pose parameters of hinton et al .
( 123 ) ) rather than by a single scalar , and that these groups tend to move together .
structured sparsity penalties ( kavukcuoglu et al . , 123; jenatton et al . , 123; bach et al . , 123; gregor et al . , 123 ) could be used for this purpose .
123 algorithms to disentangle factors of variation the goal of building invariant features is to remove sensitivity of the representation to directions of variance in the data that are uninformative to the task at hand .
however it is often the case that the goal of feature extraction is the disentangling or separation of many distinct but informative factors in the data , e . g . , in a video of people : subject identity , action performed , subject pose relative to the camera , etc .
in this situation , the methods of generating invariant features , such as feature - pooling , may be inadequate .
the process of building invariant features can be seen as consisting of two steps .
first , low - level features are recovered that account for the data .
second , subsets of these low level features are pooled together to form higher - level invariant features , exemplied by the pooling and subsampling layers of convolutional neural networks .
the invariant representation formed by the pooling features offers an incomplete window on the data as the detailed representation of the lower - level features is abstracted away in the pooling procedure .
while we would like higher - level features to be more abstract and exhibit greater invariance , we have little control over what information is lost through pooling .
what we really would like is for a particular feature set to be invariant to the irrelevant features and disentangle the relevant features .
unfortunately , it is often difcult to determine a priori which set of features will ultimately be relevant to the task at hand .
an interesting approach to taking advantage of some of the factors of variation known to exist in the data is the transforming auto - encoder ( hinton et al . , 123 ) : instead of a scalar pattern detector ( e . g , .
corresponding to the probability of presence of a particular form in the input ) one can think of the features as organized in groups that include both a pattern detector and pose parameters that specify attributes of the detected pattern .
in ( hinton et al . , 123 ) , what assumed a priori is that pairs of examples ( or consecutive ones ) are observed with an associated value for the corresponding change in the pose parameters .
for example , an animal that controls its eyes knows what changes to its ocular motor system were applied when going from one image on its retina to the next .
in that work , it is also assumed that the pose changes are the same for all the pattern detectors , and this makes sense for global changes such as image translation and camera geometry changes .
instead , we would like to discover the pose parameters and attributes that should be associated with each feature detector , without having to specify ahead of time what they should be , force them to be the same for all features , and having to necessarily observe the changes in all of the pose parameters or attributes .
the approach taken recently in the manifold tangent clas - sier , discussed in section 123 , is interesting in this respect .
without any supervision or prior knowledge , it nds prominent local factors of variation ( tangent vectors to the manifold , extracted from a cae , interpreted as locally valid input defor - mations ) .
higher - level features are subsequently encouraged to be invariant to these factors of variation , so that they must depend on other characteristics .
in a sense this approach is disentangling valid local deformations along the data manifold from other , more drastic changes , associated to other factors of variation such as those that affect class identity . 123
one solution to the problem of information loss that would t within the feature - pooling paradigm , is to consider many overlapping pools of features based on the same low - level feature set .
such a structure would have the potential learn a redundant set of invariant features that may not cause signicant loss of information .
however it is not obvious learning principle could be applied that can ensure the features are invariant while maintaining as much information as possible .
while a deep belief network or a deep boltzmann machine ( as discussed in sections 123 and 123 respectively ) with two hidden layers would , in principle , be able to preserve information into the pooling second hidden layer , there is no guarantee that the second layer features are more invariant than the low - level rst layer features .
however , there is some empirical evidence that the second layer of the dbn tends to display more invariance than the rst layer ( erhan et al . , 123a ) .
a more principled approach , from the perspective of en - suring a more robust compact feature representation , can be conceived by reconsidering the disentangling of features through the lens of its generative equivalent feature com - position .
since many unsupervised learning algorithms have a
the changes that affect class identity might , in input space , actually be of similar magnitude to local deformations , but not follow along the manifold , i . e .
cross zones of low density .
generative interpretation ( or a way to reconstruct inputs from their high - level representation ) , the generative perspective can provide insight into how to think about disentangling fac - tors .
the majority of the models currently used to construct invariant features have the interpretation that their low - level features linearly combine to construct the data . 123 this is a fairly rudimentary form of feature composition with signicant limitations .
for example , it is not possible to linearly combine a feature with a generic transformation ( such as translation ) to generate a transformed version of the feature .
nor can we even consider a generic color feature being linearly combined with a gray - scale stimulus pattern to generate a colored pattern .
it would seem that if we are to take the notion of disentangling seriously we require a richer interaction of features than that offered by simple linear combinations .
this review of representation learning and deep learning has covered three major and apparently disconnected approaches : the probabilistic models ( both the directed kind such as sparse coding and the undirected kind such as boltzmann machines ) , the reconstruction - based algorithms related to auto - encoders , and the geometrically motivated manifold - learning approaches .
drawing connections between these approaches is currently a very active area of research and is likely to continue to produce models and methods that take advantage of the relative strengths of each paradigm .
practical concerns and guidelines .
one of the criticisms addressed to articial neural networks and deep learning algo - rithms is that they have many hyper - parameters and variants and that exploring their congurations and architectures is an art .
this has motivated an earlier book on the tricks of the trade ( orr and muller , 123 ) of which lecun et al .
( 123a ) is still relevant for training deep architectures , in particular what concerns initialization , ill - conditioning and stochastic gradient descent .
a good and more modern compendium of good training practice , particularly adapted to training rbms , is provided in hinton ( 123 ) , while a similar guide oriented more towards deep neural networks can be found in bengio ( 123 ) , both of which are part of a novel version of the above book .
recent work on automating hyper - parameter search ( bergstra and bengio , 123; bergstra et al . , 123; snoek et al . , 123 ) is also making it more convenient , efcient
incorporating generic ai - level priors .
we have covered many high - level generic priors that we believe could bring machine learning closer to ai by improving representation learning .
many of these priors relate to the assumed existence of multiple underlying factors of variation , whose variations are in some sense orthogonal to each other .
they are expected to be organized at multiple levels of abstraction , hence the need for deep architectures , which also have statistical advan - tages because they allow to re - use parameters in a combi - natorially efcient way .
only a few of these factors would
as an aside , if we are given only the values of the higher - level pooling features , we cannot accurately recover the data because we do not know how to apportion credit for the pooling feature values to the lower - level features .
this is simply the generative version of the consequences of the loss of information caused by pooling .
typically be relevant for any particular example , justifying sparsity of representation .
these factors are expected to be related to simple ( e . g . , linear ) dependencies , with subsets of these explaining different random variables of interest ( inputs , tasks ) and varying in structured ways in time and space ( temporal and spatial coherence ) .
we expect future successful applications of representation learning to rene and increase that list of priors , and to incorporate most of them instead of focusing on only one .
research in training criteria that better take these priors into account are likely to move us closer to the long - term objective of discovering learning algorithms that can disentangle the underlying explanatory factors .
inference .
we anticipate that methods based on directly parametrizing a representation function will incorporate more and more of the iterative type of computation one nds in the inference procedures of probabilistic latent - variable models .
there is already movement in the other direction , with prob - abilistic latent - variable models exploiting approximate infer - ence mechanisms that are themselves learned ( i . e . , producing a parametric description of the representation function ) .
a major appeal of probabilistic models is that the semantics of the latent variables are clear and this allows a clean separation of the problems of modeling ( choose the energy function ) , inference ( estimating p ( h|x ) ) , and learning ( optimizing the parameters ) , using generic tools in each case .
on the other hand , doing approximate inference and not taking that approxi - mation into account explicitly in the approximate optimization for learning could have detrimental effects , hence the appeal of learning approximate inference .
more fundamentally , there is the question of the multimodality of the posterior p ( h|x ) .
if there are exponentially many probable congurations of values of the factors hi that can explain x , then we seem to be stuck with very poor inference , either focusing on a single mode ( map inference ) , assuming some kind of strong factorization ( as in variational inference ) or using an mcmc that cannot visit enough modes of p ( h|x ) .
what we propose as food for thought is the idea of dropping the requirement of an explicit representation of the posterior and settle for an implicit representation that exploits potential structure in p ( h|x ) in order to represent it compactly : even though p ( h|x ) may have an exponential number of modes , it may be possible to represent it with a small set of numbers .
for example , consider computing a deterministic feature representation f ( x ) that implicitly captures the information about a highly multi - modal p ( h|x ) , in the sense that all the questions ( e . g .
making some prediction about some target concept ) that can be asked from p ( h|x ) can also be answered from f ( x ) .
optimization .
much remains to be done to better under - stand the successes and failures of training deep architectures , both in the supervised case ( with many recent successes ) and the unsupervised case ( where much more work needs to be done ) .
although regularization effects can be important on small datasets , the effects that persist on very large datasets suggest some optimization issues are involved .
are they more due to local minima ( we now know there are huge numbers of them ) and the dynamics of the training procedure ? or are they due mostly to ill - conditioning and may be handled by approximate second - order methods ? these basic questions
remain unanswered and deserve much more study .
