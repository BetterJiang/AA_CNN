whereas before 123 it appears that deep multi - layer neural networks were not successfully trained , since then several algorithms have been shown to successfully train them , with experi - mental results showing the superiority of deeper vs less deep architectures .
all these experimen - tal results were obtained with new initialization or training mechanisms .
our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks , to better understand these recent relative successes and help design better algorithms in the future .
we rst observe the inuence of the non - linear activations func - tions .
we nd that the logistic sigmoid activation is unsuited for deep networks with random ini - tialization because of its mean value , which can drive especially the top hidden layer into satu - ration .
surprisingly , we nd that saturated units can move out of saturation by themselves , albeit slowly , and explaining the plateaus sometimes seen when training neural networks .
we nd that a new non - linearity that saturates less can often be benecial .
finally , we study how activations and gradients vary across layers and during train - ing , with the idea that training may be more dif - cult when the singular values of the jacobian associated with each layer are far from 123
based on these considerations , we propose a new ini - tialization scheme that brings substantially faster
123 deep neural networks
deep learning methods aim at learning feature hierarchies with features from higher levels of the hierarchy formed by the composition of lower level features .
they include
appearing in proceedings of the 123th international conference on articial intelligence and statistics ( aistats ) 123 , chia la - guna resort , sardinia , italy .
volume 123 of jmlr : w&cp 123
copy - right 123 by the authors .
learning methods for a wide array of deep architectures , including neural networks with many hidden layers ( vin - cent et al . , 123 ) and graphical models with many levels of hidden variables ( hinton et al . , 123 ) , among others ( zhu et al . , 123; weston et al . , 123 ) .
much attention has re - cently been devoted to them ( see ( bengio , 123 ) for a re - view ) , because of their theoretical appeal , inspiration from biology and human cognition , and because of empirical success in vision ( ranzato et al . , 123; larochelle et al . , 123; vincent et al . , 123 ) and natural language process - ing ( nlp ) ( collobert & weston , 123; mnih & hinton , 123 ) .
theoretical results reviewed and discussed by ben - gio ( 123 ) , suggest that in order to learn the kind of com - plicated functions that can represent high - level abstractions in vision , language , and other ai - level tasks ) , one may need deep architectures .
most of the recent experimental results with deep archi - tecture are obtained with models that can be turned into deep supervised neural networks , but with initialization or training schemes different from the classical feedforward neural networks ( rumelhart et al . , 123 ) .
why are these new algorithms working so much better than the standard random initialization and gradient - based optimization of a supervised training criterion ? part of the answer may be found in recent analyses of the effect of unsupervised pre - training ( erhan et al . , 123 ) , showing that it acts as a regu - larizer that initializes the parameters in a better basin of attraction of the optimization procedure , corresponding to an apparent local minimum associated with better general - ization .
but earlier work ( bengio et al . , 123 ) had shown that even a purely supervised but greedy layer - wise proce - dure would give better results .
so here instead of focus - ing on what unsupervised pre - training or semi - supervised criteria bring to deep architectures , we focus on analyzing what may be going wrong with good old ( but deep ) multi - layer neural networks .
our analysis is driven by investigative experiments to mon - itor activations ( watching for saturation of hidden units ) and gradients , across layers and across training iterations .
we also evaluate the effects on these of choices of acti - vation function ( with the idea that it might affect satura - tion ) and initialization procedure ( since unsupervised pre - training is a particular form of initialization and it has a
123 understanding the difculty of training deep feedforward neural networks
123 experimental setting and datasets
code to produce the new datasets introduced in this section is available from : http : / / www . iro . umontreal .
123 online learning on an innite dataset :
recent work with deep architectures ( see figure 123 in ben - gio ( 123 ) ) shows that even with very large training sets or online learning , initialization from unsupervised pre - training yields substantial improvement , which does not vanish as the number of training examples increases .
the online setting is also interesting because it focuses on the optimization issues rather than on the small - sample regu - larization effects , so we decided to include in our experi - ments a synthetic images dataset inspired from larochelle et al .
( 123 ) and larochelle et al .
( 123 ) , from which as many examples as needed could be sampled , for testing the online learning scenario .
we call this dataset the shapeset - 123 123 dataset , with ex - ample images in figure 123 ( top ) .
shapeset - 123 123 con - tains images of 123 or 123 two - dimensional objects , each taken from 123 shape categories ( triangle , parallelogram , ellipse ) , and placed with random shape parameters ( relative lengths and / or angles ) , scaling , rotation , translation and grey - scale .
we noticed that for only one shape present in the image the task of recognizing it was too easy .
we therefore decided to sample also images with two objects , with the constraint that the second object does not overlap with the rst by more than fty percent of its area , to avoid hiding it en - tirely .
the task is to predict the objects present ( e . g .
trian - gle + ellipse , parallelogram + parallelogram , triangle alone , etc . ) without having to distinguish between the foreground shape and the background shape when they overlap .
this therefore denes nine conguration classes .
the task is fairly difcult because we need to discover in - variances over rotation , translation , scaling , object color , occlusion and relative position of the shapes .
in parallel we need to extract the factors of variability that predict which object shapes are present .
the size of the images are arbitrary but we xed it to 123 in order to work with deep dense networks efciently .
123 finite datasets
the mnist digits ( lecun et al . , 123a ) , dataset has 123 , 123 training images , 123 , 123 validation images ( for hyper - parameter selection ) , and 123 , 123 test images , each showing a 123 grey - scale pixel image of one of the 123 cifar - 123 ( krizhevsky & hinton , 123 ) is a labelled sub -
figure 123 : top : shapeset - 123 images at 123 resolution .
the examples we used are at 123 resolution .
the learner tries to predict which objects ( parallelogram , triangle , or el - lipse ) are present , and 123 or 123 objects can be present , yield - ing 123 possible classications .
bottom : small - imagenet images at full resolution .
set of the tiny - images dataset that contains 123 , 123 training examples ( from which we extracted 123 , 123 as validation data ) and 123 , 123 test examples .
there are 123 classes cor - responding to the main object in each image : airplane , au - tomobile , bird , cat , deer , dog , frog , horse , ship , or truck .
the classes are balanced .
each image is in color , but is just 123 123 pixels in size , so the input is a vector of 123 123 123 = 123 real values .
small - imagenet which is a set of tiny 123 gray level images dataset computed from the higher - resolution and larger set at http : / / www . image - net . org , with la - bels from the wordnet noun hierarchy .
we have used 123 , 123 examples for training , 123 , 123 for the validation set , and 123 , 123 for testing .
there are 123 balanced classes : rep - tiles , vehicles , birds , mammals , sh , furniture , instruments , tools , owers and fruits figure 123 ( bottom ) shows randomly
123 experimental setting
we optimized feedforward neural networks with one to ve hidden layers , with one thousand hidden units per layer , and with a softmax logistic regression for the out - put layer .
the cost function is the negative log - likelihood log p ( y|x ) , where ( x , y ) is the ( input image , target class ) pair .
the neural networks were optimized with stochastic back - propagation on mini - batches of size ten , i . e . , the av - erage g of log p ( y|x ) was computed over 123 consecutive
123 xavier glorot , yoshua bengio
training pairs ( x , y ) and used to update parameters in that direction , with g .
the learning rate is a hyper - parameter that is optimized based on validation set error after a large number of updates ( 123 million ) .
we varied the type of non - linear activation function in the the sigmoid 123 / ( 123 + ex ) , the hyperbolic tangent tanh ( x ) , and a newly proposed activation func - tion ( bergstra et al . , 123 ) called the softsign , x / ( 123 + |x| ) .
the softsign is similar to the hyperbolic tangent ( its range is - 123 to 123 ) but its tails are quadratic polynomials rather than exponentials , i . e . , it approaches its asymptotes much in the comparisons , we search for the best hyper - parameters ( learning rate and depth ) separately for each model .
note that the best depth was always ve for shapeset - 123 123 , except for the sigmoid , for which it was we initialized the biases to be 123 and the weights wij at each layer with the following commonly used heuristic :
where u ( a , a ) is the uniform distribution in the interval ( a , a ) and n is the size of the previous layer ( the number of columns of w ) .
123 effect of activation functions and
saturation during training
two things we want to avoid and that can be revealed from the evolution of activations is excessive saturation of acti - vation functions on one hand ( then gradients will not prop - agate well ) , and overly linear units ( they will not compute
123 experiments with the sigmoid
the sigmoid non - linearity has been already shown to slow down learning because of its none - zero mean that induces important singular values in the hessian ( lecun et al . , 123b ) .
in this section we will see another symptomatic behavior due to this activation function in deep feedforward we want to study possible saturation , by looking at the evo - lution of activations during training , and the gures in this section show results on the shapeset - 123 123 data , but sim - ilar behavior is observed with the other datasets .
figure 123 shows the evolution of the activation values ( after the non - linearity ) at each hidden layer during training of a deep ar - chitecture with sigmoid activation functions .
layer 123 refers to the output of rst hidden layer , and there are four hidden layers .
the graph shows the means and standard deviations of these activations .
these statistics along with histograms are computed at different times during learning , by looking at activation values for a xed set of 123 test examples .
figure 123 : mean and standard deviation ( vertical bars ) of the activation values ( output of the sigmoid ) during supervised learning , for the different hidden layers of a deep archi - tecture .
the top hidden layer quickly saturates at 123 ( slow - ing down all learning ) , but then slowly desaturates around
we see that very quickly at the beginning , all the sigmoid activation values of the last hidden layer are pushed to their lower saturation value of 123
inversely , the others layers have a mean activation value that is above 123 , and decreas - ing as we go from the output layer to the input layer .
we have found that this kind of saturation can last very long in deeper networks with sigmoid activations , e . g . , the depth - ve model never escaped this regime during training .
the big surprise is that for intermediate number of hidden lay - ers ( here four ) , the saturation regime may be escaped .
at the same time that the top hidden layer moves out of satura - tion , the rst hidden layer begins to saturate and therefore we hypothesize that this behavior is due to the combina - tion of random initialization and the fact that an hidden unit output of 123 corresponds to a saturated sigmoid .
note that deep networks with sigmoids but initialized from unsuper - vised pre - training ( e . g .
from rbms ) do not suffer from this saturation behavior .
our proposed explanation rests on the hypothesis that the transformation that the lower layers of the randomly initialized network computes initially is not useful to the classication task , unlike the transforma - tion obtained from unsupervised pre - training .
the logistic layer output softmax ( b+ w h ) might initially rely more on its biases b ( which are learned very quickly ) than on the top hidden activations h derived from the input image ( because h would vary in ways that are not predictive of y , maybe correlated mostly with other and possibly more dominant variations of x ) .
thus the error gradient would tend to push w h towards 123 , which can be achieved by pushing h towards 123
in the case of symmetric activation functions like the hyperbolic tangent and the softsign , sitting around 123 is good because it allows gradients to ow backwards .
however , pushing the sigmoid outputs to 123 would bring them into a saturation regime which would prevent gradi - ents to ow backward and prevent the lower layers from learning useful features .
eventually but slowly , the lower layers move toward more useful features and the top hidden layer then moves out of the saturation regime .
note how - ever that , even after this , the network moves into a solution that is of poorer quality ( also in terms of generalization )
123 understanding the difculty of training deep feedforward neural networks
then those found with symmetric activation functions , as can be seen in gure 123
where the gradients would ow well .
123 experiments with the hyperbolic tangent
as discussed above , the hyperbolic tangent networks do not suffer from the kind of saturation behavior of the top hid - den layer observed with sigmoid networks , because of its symmetry around 123
however , with our standard weight , we observe a sequentially oc - curring saturation phenomenon starting with layer 123 and propagating up in the network , as illustrated in figure 123
why this is happening remains to be understood .
figure 123 : activation values normalized histogram at the end of learning , averaged across units of the same layer and across 123 test examples .
top : activation function is hyper - bolic tangent , we see important saturation of the lower lay - ers .
bottom : activation function is softsign , we see many activation values around ( - 123 , - 123 ) and ( 123 , 123 ) where the units do not saturate but are non - linear .
123 studying gradients and their propagation
123 effect of the cost function
we have found that the logistic regression or conditional log - likelihood cost function ( log p ( y|x ) coupled with softmax outputs ) worked much better ( for classication problems ) than the quadratic cost which was tradition - ally used to train feedforward neural networks ( rumelhart et al . , 123 ) .
this is not a new observation ( solla et al . , 123 ) but we nd it important to stress here .
we found that the plateaus in the training criterion ( as a function of the pa - rameters ) are less present with the log - likelihood cost func - tion .
we can see this on figure 123 , which plots the training criterion as a function of two weights for a two - layer net - work ( one hidden layer ) with hyperbolic tangent units , and a random input and target signal .
there are clearly more severe plateaus with the quadratic cost .
123 gradients at initialization 123 . 123 theoretical considerations and a new
we study the back - propagated gradients , or equivalently the gradient of the cost function on the inputs biases at each layer .
bradley ( 123 ) found that back - propagated gradients were smaller as one moves from the output layer towards the input layer , just after initialization .
he studied networks with linear activation at each layer , nding that the variance of the back - propagated gradients decreases as we go back - wards in the network .
we will also start by studying the
figure 123 : top : 123 percentiles ( markers alone ) and standard deviation ( solid lines with markers ) of the distribution of the activation values for the hyperbolic tangent networks in the course of learning .
we see the rst hidden layer satu - rating rst , then the second , etc .
bottom : 123 percentiles ( markers alone ) and standard deviation ( solid lines with markers ) of the distribution of activation values for the soft - sign during learning .
here the different layers saturate less and do so together .
123 experiments with the softsign the softsign x / ( 123+|x| ) is similar to the hyperbolic tangent but might behave differently in terms of saturation because of its smoother asymptotes ( polynomial instead of expo - nential ) .
we see on figure 123 that the saturation does not occur one layer after the other like for the hyperbolic tan - it is faster at the beginning and then slow , and all layers move together towards larger weights .
we can also see at the end of training that the histogram of activation values is very different from that seen with the hyperbolic tangent ( figure 123 ) .
whereas the latter yields modes of the activations distribution mostly at the extremes ( asymptotes - 123 and 123 ) or around 123 , the softsign network has modes of activations around its knees ( between the linear regime around 123 and the at regime around - 123 and 123 ) .
these are the areas where there is substantial non - linearity but
123 xavier glorot , yoshua bengio
from a forward - propagation point of view , to keep infor - mation owing we would like that
( i , i123 ) , v ar ( zi ) = v ar ( zi123
nv ar ( w )
from a back - propagation point of view we would similarly like to have
( i , i123 ) , v ar
= v ar
these two conditions transform to :
i , niv ar ( w i ) = 123 i , ni+123v ar ( w i ) = 123
as a compromise between these two constraints , we might want to have
i , v ar ( w i ) =
ni + ni+123
note how both constraints are satised when all layers have the same width .
if we also have the same initialization for the weights we could get the following interesting proper -
i , v ar
i , v ar
nv ar ( w )
v ar ( x ) v ar
we can see that the variance of the gradient on the weights is the same for all layers , but the variance of the back - propagated gradient might still vanish or explode as we consider deeper networks .
note how this is reminis - cent of issues raised when studying recurrent neural net - works ( bengio et al . , 123 ) , which can be seen as very deep networks when unfolded through time .
the standard initialization that we have used ( eq . 123 ) gives rise to variance with the following property :
nv ar ( w ) =
where n is the layer size ( assuming all layers of the same size ) .
this will cause the variance of the back - propagated gradient to be dependent on the layer ( and decreasing ) .
the normalization factor may therefore be important when initializing deep networks because of the multiplicative ef - fect through layers , and we suggest the following initializa - tion procedure to approximately satisfy our objectives of maintaining activation variances and back - propagated gra - dients variance as one moves up or down the network .
we call it the normalized initialization :
nj + nj+123
nj + nj+123
figure 123 : cross entropy ( black , surface on top ) and quadratic ( red , bottom surface ) cost as a function of two weights ( one at each layer ) of a network with two layers , w123 respectively on the rst layer and w123 on the second ,
for a dense articial neural network using symmetric acti - vation function f with unit derivative at 123 ( i . e .
f123 ( 123 ) = 123 ) , if we write zi for the activation vector of layer i , and si the argument vector of the activation function at layer i , we have si = ziw i + bi and zi+123 = f ( si ) .
from these denitions we obtain the following :
the variances will be expressed with respect to the input , outpout and weight initialization randomness .
consider the hypothesis that we are in a linear regime at the initial - ization , that the weights are initialized independently and that the inputs features variances are the same ( = v ar ( x ) ) .
then we can say that , with ni the size of layer i and x the
v ar ( zi ) = v ar ( x )
ni123v ar ( w i123
we write v ar ( w i123 ) for the shared scalar variance of all weights at layer i123
then for a network with d layers , ni123+123v ar ( w i123
= v ar
v ar ( x ) v ar
ni123v ar ( w i123
ni123+123v ar ( w i123
123 understanding the difculty of training deep feedforward neural networks
123 . 123 gradient propagation study to empirically validate the above theoretical ideas , we have plotted some normalized histograms of activation values , weight gradients and of the back - propagated gradients at initialization with the two different initialization methods .
the results displayed ( figures 123 , 123 and 123 ) are from exper - iments on shapeset - 123 123 , but qualitatively similar results were obtained with the other datasets .
we monitor the singular values of the jacobian matrix as - sociated with layer i :
j i = zi+123
when consecutive layers have the same dimension , the av - erage singular value corresponds to the average ratio of in - nitesimal volumes mapped from zi to zi+123 , as well as to the ratio of average activation variance going from zi to zi+123
with our normalized initialization , this ratio is around 123 whereas with the standard initialization , it drops down to 123 .
figure 123 : activation values normalized histograms with hyperbolic tangent activation , with standard ( top ) vs nor - malized initialization ( bottom ) .
top : 123 - peak increases for 123 back - propagated gradients during learning
the dynamic of learning in such networks is complex and we would like to develop better tools to analyze and track it .
in particular , we cannot use simple variance calculations in our theoretical analysis because the weights values are not anymore independent of the activation values and the linearity hypothesis is also violated .
as rst noted by bradley ( 123 ) , we observe ( figure 123 ) that at the beginning of training , after the standard initializa - tion ( eq .
123 ) , the variance of the back - propagated gradients gets smaller as it is propagated downwards .
however we nd that this trend is reversed very quickly during learning .
using our normalized initialization we do not see such de - creasing back - propagated gradients ( bottom of figure 123 ) .
figure 123 : back - propagated gradients normalized his - tograms with hyperbolic tangent activation , with standard ( top ) vs normalized ( bottom ) initialization .
top : 123 - peak decreases for higher layers .
what was initially really surprising is that even when the back - propagated gradients become smaller ( standard ini - tialization ) , the variance of the weights gradients is roughly constant across layers , as shown on figure 123
however , this is explained by our theoretical analysis above ( eq .
in - terestingly , as shown in figure 123 , these observations on the weight gradient of standard and normalized initialization change during training ( here for a tanh network ) .
indeed , whereas the gradients have initially roughly the same mag - nitude , they diverge from each other ( with larger gradients in the lower layers ) as training progresses , especially with the standard initialization .
note that this might be one of the advantages of the normalized initialization , since hav - ing gradients of very different magnitudes at different lay - ers may yield to ill - conditioning and slower training .
finally , we observe that the softsign networks share simi - larities with the tanh networks with normalized initializa - tion , as can be seen by comparing the evolution of activa - tions in both cases ( resp .
figure 123 - bottom and figure 123 ) .
123 error curves and conclusions the nal consideration that we care for is the success of training with different strategies , and this is best il - lustrated with error curves showing the evolution of test error as training progresses and asymptotes .
figure 123 shows such curves with online training on shapeset - 123 123 , while table 123 gives nal test error for all the datasets studied ( shapeset - 123 123 , mnist , cifar - 123 , and small - imagenet ) .
as a baseline , we optimized rbf svm mod - els on one hundred thousand shapeset examples and ob - tained 123% test error , while on the same set we obtained 123% with a depth ve hyperbolic tangent network with these results illustrate the effect of the choice of activa - tion and initialization .
as a reference we include in fig -
123 xavier glorot , yoshua bengio
figure 123 : weight gradient normalized histograms with hy - perbolic tangent activation just after initialization , with standard initialization ( top ) and normalized initialization ( bottom ) , for different layers .
even though with standard initialization the back - propagated gradients get smaller , the weight gradients do not !
figure 123 : standard deviation intervals of the weights gradi - ents with hyperbolic tangents with standard initialization ( top ) and normalized ( bottom ) during training .
we see that the normalization allows to keep the same variance of the weights gradient across layers , during training ( top : smaller variance for higher layers ) .
table 123 : test error with different activation functions and initialization schemes for deep networks with 123 hidden lay - ers .
n after the activation function name indicates the use of normalized initialization .
results in bold are statistically different from non - bold ones under the null hypothesis test with p = 123 .
shapeset mnist cifar - 123
ure 123 the error curve for the supervised ne - tuning from the initialization obtained after unsupervised pre - training with denoising auto - encoders ( vincent et al . , 123 ) .
for each network the learning rate is separately chosen to min - imize error on the validation set .
we can remark that on shapeset - 123 123 , because of the task difculty , we observe important saturations during learning , this might explain that the normalized initialization or the softsign effects are several conclusions can be drawn from these error curves : the more classical neural networks with sigmoid or hyperbolic tangent units and standard initialization fare rather poorly , converging more slowly and appar - ently towards ultimately poorer local minima .
the softsign networks seem to be more robust to the initialization procedure than the tanh networks , pre - sumably because of their gentler non - linearity .
for tanh networks , the proposed normalized initial - ization can be quite helpful , presumably because the layer - to - layer transformations maintain magnitudes of
figure 123 : 123 percentile ( markers alone ) and standard de - viation ( solid lines with markers ) of the distribution of ac - tivation values for hyperbolic tangent with normalized ini - tialization during learning .
activations ( owing upward ) and gradients ( owing
others methods can alleviate discrepancies between lay - ers during learning , e . g . , exploiting second order informa - tion to set the learning rate separately for each parame - ter .
for example , we can exploit the diagonal of the hes - sian ( lecun et al . , 123b ) or a gradient variance estimate .
both those methods have been applied for shapeset - 123 123 with hyperbolic tangent and standard initialization .
we ob - served a gain in performance but not reaching the result ob - tained from normalized initialization .
in addition , we ob - served further gains by combining normalized initialization with second order methods : the estimated hessian might then focus on discrepancies between units , not having to correct important initial discrepancies between layers .
in all reported experiments we have used the same num - ber of units per layer .
however , we veried that we obtain the same gains when the layer size increases ( or decreases ) with layer number .
the other conclusions from this study are the following :
monitoring activations and gradients across layers and
123 understanding the difculty of training deep feedforward neural networks
bengio , y . , lamblin , p . , popovici , d . , & larochelle , h .
( 123 ) .
greedy layer - wise training of deep networks .
nips 123 ( pp .
123 ) .
mit press .
bengio , y . , simard , p . , & frasconi , p .
( 123 ) .
learning long - term dependencies with gradient descent is difcult .
ieee transac - tions on neural networks , 123 , 123
bergstra , j . , desjardins , g . , lamblin , p . , & bengio , y .
( 123 ) .
quadratic polynomials learn better image features ( technical report 123 ) .
departement dinformatique et de recherche operationnelle , universite de montreal .
bradley , d .
( 123 ) .
learning in modular systems .
doctoral dis - sertation , the robotics institute , carnegie mellon university .
collobert , r . , & weston , j .
( 123 ) .
a unied architecture for nat - ural language processing : deep neural networks with multitask learning .
icml 123
erhan , d . , manzagol , p . - a . , bengio , y . , bengio , s . , & vincent , p .
( 123 ) .
the difculty of training deep architectures and the effect of unsupervised pre - training .
aistats123 ( pp
hinton , g .
e . , osindero , s . , & teh , y .
( 123 ) .
a fast learning algorithm for deep belief nets .
neural computation , 123 , 123
krizhevsky , a . , & hinton , g .
( 123 ) .
learning multiple layers of features from tiny images ( technical report ) .
university of
larochelle , h . , bengio , y . , louradour , j . , & lamblin , p .
( 123 ) .
exploring strategies for training deep neural networks .
the journal of machine learning research , 123 , 123
larochelle , h . , erhan , d . , courville , a . , bergstra , j . , & bengio , y .
( 123 ) .
an empirical evaluation of deep architectures on problems with many factors of variation .
icml 123
lecun , y . , bottou , l . , bengio , y . , & haffner , p .
( 123a ) .
gradient - based learning applied to document recognition .
pro - ceedings of the ieee , 123 , 123
lecun , y . , bottou , l . , orr , g .
b . , & muller , k . - r .
( 123b ) .
ef - cient backprop .
in neural networks , tricks of the trade , lecture notes in computer science lncs 123
springer verlag .
mnih , a . , & hinton , g .
( 123 ) .
a scalable hierarchical dis -
tributed language model .
nips 123 ( pp .
123 ) .
ranzato , m . , poultney , c . , chopra , s . , & lecun , y .
( 123 ) .
ef - cient learning of sparse representations with an energy - based model .
nips 123
rumelhart , d .
e . , hinton , g .
e . , & williams , r .
( 123 ) .
learn - ing representations by back - propagating errors .
nature , 123 ,
solla , s .
a . , levin , e . , & fleisher , m .
( 123 ) .
accelerated learn - ing in layered neural networks .
complex systems , 123 , 123
vincent , p . , larochelle , h . , bengio , y . , & manzagol , p . - a .
( 123 ) .
extracting and composing robust features with denoising au - toencoders .
icml 123
weston , j . , ratle , f . , & collobert , r .
( 123 ) .
deep learning via semi - supervised embedding .
icml 123 ( pp .
123 ) .
new york , ny , usa : acm .
zhu , l . , chen , y . , & yuille , a .
( 123 ) .
unsupervised learning of probabilistic grammar - markov models for object categories .
ieee transactions on pattern analysis and machine intelli - gence , 123 , 123
figure 123 : test error during online training on the shapeset - 123 dataset , for various activation functions and initialization schemes ( ordered from top to bottom in de - creasing nal error ) .
n after the activation function name indicates the use of normalized initialization .
figure 123 : test error curves during training on mnist and cifar123 , for various activation functions and initialization schemes ( ordered from top to bottom in decreasing nal error ) .
n after the activation function name indicates the use of normalized initialization .
training iterations is a powerful investigative tool for understanding training difculties in deep nets .
sigmoid activations ( not symmetric around 123 ) should be avoided when initializing from small random weights , because they yield poor learning dynamics , with initial saturation of the top hidden layer .
keeping the layer - to - layer transformations such that both activations and gradients ow well ( i . e .
with a ja - cobian around 123 ) appears helpful , and allows to elim - inate a good part of the discrepancy between purely supervised deep networks and ones pre - trained with
many of our observations remain unexplained , sug - gesting further investigations to better understand gra - dients and training dynamics in deep architectures .
