grid search and manual search are the most widely used strategies for hyper - parameter optimiza - tion .
this paper shows empirically and theoretically that randomly chosen trials are more efcient for hyper - parameter optimization than trials on a grid .
empirical evidence comes from a compar - ison with a large previous study that used grid search and manual search to congure neural net - works and deep belief networks .
compared with neural networks congured by a pure grid search , we nd that random search over the same domain is able to nd models that are as good or better within a small fraction of the computation time .
granting random search the same computational budget , random search nds better models by effectively searching a larger , less promising con - guration space .
compared with deep belief networks congured by a thoughtful combination of manual search and grid search , purely random search over the same 123 - dimensional conguration space found statistically equal performance on four of seven data sets , and superior performance on one of seven .
a gaussian process analysis of the function from hyper - parameters to validation set performance reveals that for most data sets only a few of the hyper - parameters really matter , but that different hyper - parameters are important on different data sets .
this phenomenon makes grid search a poor choice for conguring algorithms for new data sets .
our analysis casts some light on why recent high throughput methods achieve surprising successthey appear to search through a large number of hyper - parameters because most hyper - parameters do not matter much .
we anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper - parameter optimization; this work shows that random search is a natural base - line against which to judge progress in the development of adaptive ( sequential ) hyper - parameter
keywords : global optimization , model selection , neural networks , deep learning , response surface
the ultimate objective of a typical learning algorithm a is to nd a function f that minimizes some expected loss l ( x; f ) over i . i . d .
samples x from a natural ( grand truth ) distribution gx .
a learning algorithm a is a functional that maps a data set x ( train ) ( a nite set of samples from gx ) to a function f .
very often a learning algorithm produces f through the optimization of a training criterion with respect to a set of parameters q .
however , the learning algorithm itself often has bells and whistles called hyper - parameters l , and the actual learning algorithm is the one obtained after choosing , which can be denoted al , and f = al ( x ( train ) ) for a training set x ( train ) .
for example , with a
c ( cid : 123 ) 123 james bergstra and yoshua bengio .
bergstra and bengio
gaussian kernel svm , one has to select a regularization penalty c for the training criterion ( which controls the margin ) and the bandwidth s of the gaussian kernel , that is , l = ( c , s ) .
what we really need in practice is a way to choose l
so as to minimize generalization error exgx ( l ( x; al ( x ( train ) ) ) ) .
note that the computation performed by a itself often involves an inner optimization problem , which is usually iterative and approximate .
the problem of identifying a good value for hyper - parameters l is called the problem of hyper - parameter optimization .
this paper takes a look at algorithms for this difcult outer - loop optimization problem , which is of great practical importance in empirical machine learning work :
l ( ) = argmin
exgx ( l ( cid : 123 ) x; al ( x ( train ) ) ( cid : 123 ) ) .
in general , we do not have efcient algorithms for performing the optimization implied by equa - tion 123
furthermore , we cannot even evaluate the expectation over the unknown natural distribution gx , the value we wish to optimize .
nevertheless , we must carry out this optimization as best we can .
with regards to the expectation over gx , we will employ the widely used technique of cross - validation to estimate it .
cross - validation is the technique of replacing the expectation with a mean over a validation set x ( valid ) whose elements are drawn i . i . d x gx .
cross - validation is unbiased as long as x ( valid ) is independent of any data used by al ( see bishop , 123 , pp .
123 - 123 ) .
we see in equations 123 - 123 the hyper - parameter optimization problem as it is addressed in practice :
l ( ) argmin
l ( cid : 123 ) x; al ( x ( train ) ) ( cid : 123 ) .
l ( l ( 123 ) . . . l ( s ) )
( l ) l
( l ) over l l
hyper - parameter optimization is the minimization of y
equation 123 expresses the hyper - parameter optimization problem in terms of a hyper - parameter response function , y function is sometimes called the response surface in the experiment design literature .
different data and functions y sets , tasks , and learning algorithm families give rise to different sets l in general very little about the response surface y or the search space l , the dominant strategy for nding a good l ( l ) for each one , and return the l ( i ) that worked the best as l
is to choose some number ( s ) of trial points ( l ( 123 ) . . . l ( s ) ) , to evaluate y
this strategy is made explicit by equation 123
the critical step in hyper - parameter optimization is to choose the set of trials ( l ( 123 ) . . . l ( s ) ) .
the most widely used strategy is a combination of grid search and manual search ( e . g . , lecun et al . , 123b; larochelle et al . , 123; hinton , 123 ) , as well as machine learning software packages such as libsvm ( chang and lin , 123 ) and scikits . learn . 123 if l is a set indexed by k conguration variables ( e . g . , for neural networks it would be the learning rate , the number of hidden units , the strength of weight regularization , etc . ) , then grid search requires that we choose a set of values for each variable ( l ( 123 ) . . . l ( k ) ) .
in grid search the set of trials is formed by assembling every possible combination of values , so the number of trials in a grid search is s = ( cid : 123 ) k k=123 |l ( k ) | elements .
this product over k sets makes grid search suffer from the curse of dimensionality because the number of joint values grows exponentially with the number of hyper - parameters ( bellman , 123 ) .
manual
scikits . learn : machine learning in python can be found at http : / / scikit - learn . sourceforge . net .
random search for hyper - parameter optimization
search is used to identify regions in l that are promising and to develop the intuition necessary to choose the sets l ( k ) .
a major drawback of manual search is the difculty in reproducing results .
this is important both for the progress of scientic research in machine learning as well as for ease of application of learning algorithms by non - expert users .
on the other hand , grid search alone does very poorly in practice ( as discussed here ) .
we propose random search as a substitute and baseline that is both reasonably efcient ( roughly equivalent to or better than combinining manual search and grid search , in our experiments ) and keeping the advantages of implementation simplicity and reproducibility of pure grid search .
random search is actually more practical than grid search because it can be applied even when using a cluster of computers that can fail , and allows the experimenter to change the resolution on the y : adding new trials to the set or ignoring failed trials are both feasible because the trials are i . i . d . , which is not the case for a grid search .
of course , random search can probably be improved by automating what manual search does , i . e . , a sequential optimization , but this is left to future work .
there are several reasons why manual search and grid search prevail as the state of the art despite decades of research into global optimization ( e . g . , nelder and mead , 123; kirkpatrick et al . , 123; powell , 123; weise , 123 ) and the publishing of several hyper - parameter optimization algorithms ( e . g . , nareyek , 123; czogiel et al . , 123; hutter , 123 ) :
manual optimization gives researchers some degree of insight into y
there is no technical overhead or barrier to manual optimization;
grid search is simple to implement and parallelization is trivial; grid search ( with access to a compute cluster ) typically nds a better l
sequential optimization ( in the same amount of time ) ;
than purely manual
grid search is reliable in low dimensional spaces ( e . g . , 123 - d , 123 - d ) .
we will come back to the use of global optimization algorithms for hyper - parameter selection in our discussion of future work ( section 123 ) .
in this paper , we focus on random search , that is , inde - pendent draws from a uniform density from the same conguration space as would be spanned by a regular grid , as an alternative strategy for producing a trial set ( l ( 123 ) . . . l ( s ) ) .
we show that random search has all the practical advantages of grid search ( conceptual simplicity , ease of implementation , trivial parallelism ) and trades a small reduction in efciency in low - dimensional spaces for a large improvement in efciency in high - dimensional search spaces .
of interest have a low effective dimensionality; essentially , y
in this work we show that random search is more efcient than grid search in high - dimensional spaces because functions y are more sensitive to changes in some dimensions than others ( caisch et al . , 123 ) .
in particular , if a function f of two variables could be approximated by another function of one variable ( f ( x123 , x123 ) g ( x123 ) ) , we could say that f has a low effective dimension .
figure 123 illustrates how point grids and uniformly random point sets differ in how they cope with low effective dimensionality , as in the above example with f .
a grid of points gives even coverage in the original 123 - d space , but projections onto either the x123 or x123 subspace produces an inefcient coverage of the subspace .
in contrast , random points are slightly less evenly distributed in the original space , but far more evenly distributed in the subspaces .
if the researcher could know ahead of time which subspaces would be important , then he or she could design an appropriate grid .
however , we show the failings of this strategy in section 123
bergstra and bengio
figure 123 : grid and random search of nine trials for optimizing a function f ( x , y ) = g ( x ) + h ( y ) g ( x ) with low effective dimensionality .
above each square g ( x ) is shown in green , and left of each square h ( y ) is shown in yellow .
with grid search , nine trials only test g ( x ) in three distinct places .
with random search , all nine trials explore distinct values of g .
this failure of grid search is the rule rather than the exception in high dimensional
given learning algorithm , looking at several relatively similar data sets ( from different distributions ) reveals that on different data sets , different subspaces are important , and to different degrees .
a grid with sufcient granularity to optimizing hyper - parameters for all data sets must consequently be inefcient for each individual data set because of the curse of dimensionality : the number of wasted grid search trials is exponential in the number of search dimensions that turn out to be irrelevant for a particular data set .
in contrast , random search thrives on low effective dimensionality .
random search has the same efciency in the relevant subspace as if it had been used to search only the
this paper is organized as follows .
section 123 looks at the efciency of random search in practice vs .
grid search as a method for optimizing neural network hyper - parameters .
we take the grid search experiments of larochelle et al .
( 123 ) as a point of comparison , and repeat similar experiments using random search .
section 123 uses gaussian process regression ( gpr ) to analyze the results of the neural network trials .
the gpr lets us characterize what y looks like for various data sets , and establish an empirical link between the low effective dimensionality of y and the efciency of random search .
section 123 compares random search and grid search with more sophisticated point sets developed for quasi monte - carlo numerical integration , and argues that in the regime of interest for hyper - parameter selection grid search is inappropriate and more sophisticated methods bring little advantage over random search .
section 123 compares random search with the expert - guided manual sequential optimization employed in larochelle et al .
( 123 ) to optimize deep belief networks .
section 123 comments on the role of global optimization algorithms in future work .
we conclude in section 123 that random search is generally superior to grid search for optimizing hyper -
random search for hyper - parameter optimization
random vs .
grid for optimizing neural networks
in this section we take a second look at several of the experiments of larochelle et al .
( 123 ) us - ing random search , to compare with the grid searches done in that work .
we begin with a look at hyper - parameter optimization in neural networks , and then move on to hyper - parameter opti - mization in deep belief networks ( dbns ) .
to characterize the efciency of random search , we present two techniques in preliminary sections : section 123 explains how we estimate the general - ization performance of the best model from a set of candidates , taking into account our uncertainty in which model is actually best; section 123 explains the random experiment efciency curve that we use to characterize the performance of random search experiments .
with these preliminaries out of the way , section 123 describes the data sets from larochelle et al .
( 123 ) that we use in our work .
section 123 presents our results optimizing neural networks , and section 123 presents our results
123 estimating generalization
because of nite data sets , test error is not monotone in validation error , and depending on the set of particular hyper - parameter values l evaluated , the test error of the best - validation error congu - ration may vary .
when reporting performance of learning algorithms , it can be useful to take into account the uncertainty due to the choice of hyper - parameters values .
this section describes our procedure for estimating test set accuracy , which takes into account any uncertainty in the choice of which trial is actually the best - performing one .
to explain this procedure , we must distinguish between estimates of performance y ( test ) based on the validation and test sets
( valid ) = y
( valid ) ( l ) = meanxx ( valid ) l ( cid : 123 ) x; al ( x ( train ) ) ( cid : 123 ) , ( test ) ( l ) = meanxx ( test ) l ( cid : 123 ) x; al ( x ( train ) ) ( cid : 123 ) .
likewise , we must dene the estimated variance v about these means on the validation and test sets , for example , for the zero - one loss ( bernoulli variance ) :
v ( valid ) ( l ) =
v ( test ) ( l ) =
( valid ) ( l ) ( cid : 123 ) 123 y ( test ) ( l ) ( cid : 123 ) 123 y
|x ( valid ) | 123
|x ( test ) | 123
with other loss functions the estimator of variance will generally be different .
the standard practice for evaluating a model found by cross - validation is to report y
for the l ( s ) that minimizes y ( valid ) ( l ( s ) ) .
however , when different trials have nearly optimal val - idation means , then it is not clear which test score to report , and a slightly different choice of l could have yielded a different test error .
to resolve the difculty of choosing a winner , we report a weighted average of all the test set scores , in which each one is weighted by the probability that its particular l ( s ) is in fact the best .
in this view , the uncertainty arising from x ( valid ) being a nite sam - ple of gx makes the test - set score of the best model among l ( 123 ) , . . . , l ( s ) a random variable , z .
this score z is modeled by a gaussian mixture model whose s components have means s = y
bergstra and bengio
variances s 123
s = v ( test ) ( l ( s ) ) , and weights ws dened by
ws = p ( cid : 123 ) z ( s ) < z ( s ) , s 123= s ( cid : 123 ) , where ( valid ) ( l ( i ) ) , v ( valid ) ( l ( i ) ) ( cid : 123 ) .
z ( i ) n ( cid : 123 ) y
to summarize , the performance z of the best model in an experiment of s trials has mean z and standard error s 123
s + s 123
it is simple and practical to estimate weights ws by simulation .
the procedure for doing so is to repeatedly draw hypothetical validation scores z ( s ) from normal distributions whose means are the ( valid ) ( l ( s ) ) and whose variances are the squared standard errors v ( valid ) ( l ( s ) ) , and to count how often each trial generates a winning score .
since the test scores of the best validation scores are typically relatively close , ws need not be estimated very precisely and a few tens of hypothetical
in expectation , this technique for estimating generalization gives a higher estimate than the traditional technique of reporting the test set error of the best model in validation .
the difference is related to the variance y ( l ( i ) ) near the best value .
to the extent that y ( valid ) casts doubt on which model was best , this technique averages the performance of the best model together with the performance of models which were not the best .
the next section ( random experiment efcieny curve ) illustrates this phenomenon and discusses it in more detail .
( valid ) and the density of validation set scores y
123 random experiment efciency curve
figure 123 illustrates the results of a random experiment : an experiment of 123 trials training neural networks to classify the rectangles data set .
since the trials of a random experiment are indepen - dently identically distributed ( i . i . d . ) , a random search experiment involving s i . i . d .
trials can also be interpreted as n independent experiments of s trials , as long as sn s .
this interpretation al - lows us to estimate statistics such as the minimum , maximum , median , and quantiles of any random experiment of size s , where s is a divisor of s .
there are two general trends in random experiment efciency curves , such as the one in figure 123 : a sharp upward slope of the lower extremes as experiments grow , and a gentle downward slope of the upper extremes .
the sharp upward slope occurs because when we take the maximum over larger subsets of the s trials , trials with poor performance are rarely the best within their subset .
it is natural that larger experiments nd trials with better scores .
the shape of this curve indicates the frequency of good models under random search , and quanties the relative volumes ( in search space ) of the various levels of performance .
the gentle downward slope occurs because as we take the maximum over larger subsets of trials ( in equation 123 ) , we are less sure about which trial is actually the best .
large experiments average together good validation trials with unusually high test scores with other good validation trials with unusually low test scores to arrive at a more accurate estimate of generalization .
for example ,
random search for hyper - parameter optimization
experiment size ( # trials )
figure 123 : a random experiment efciency curve .
the trials of a random experiment are i . i . d , so an experiment of many trials ( here , 123 trials optimizing a neural network to classify the rectangles basic data set , section 123 ) can be interpreted as several independent smaller experiments .
for example , at horizontal axis position 123 , we consider our 123 trials to be 123 experiments of 123 trials each .
the vertical axis shows the test accuracy of the best trial ( s ) from experiments of a given size , as determined by equation 123
when there are sufciently many experiments of a given size ( i . e . , 123 ) , the distribution of performance is illustrated by a box plot whose boxed section spans the lower and upper quartiles and includes a line at the median .
the whiskers above and below each boxed section show the position of the most extreme data point within 123 times the inter - quartile range of the nearest quartile .
data points beyond the whiskers are plotted with + symbols .
when there are not enough experiments to support a box plot , as occurs here for experiments of 123 trials or more , the best generalization score of each experiment is shown by a scatter plot .
the two thin black lines across the top of the gure mark the upper and lower boundaries of a 123% condence interval on the generalization of the best trial overall
consider what figure 123 would look like if the experiment had included lucky trial whose validation score were around 123% as usual , but whose test score were 123% .
in the bar plot for trials of size 123 , we would see the top performer scoring 123% .
in larger experiments , we would average that 123% performance together with other test set performances because 123% is not clearly the best validation score; this averaging would make the upper envelope of the efciency curve slope downward from 123% to a point very close to the current test set estimate of 123% .
figure 123 characterizes the range of performance that is to be expected from experiments of vari - ous sizes , which is valuable information to anyone trying to reproduce these results .
for example , if we try to repeat the experiment and our rst four random trials fail to nd a score better than 123% , then the problem is likely not in hyper - parameter selection .
bergstra and bengio
figure 123 : from top to bottom , samples from the mnist rotated , mnist background random , mnist background images , mnist rotated background images data sets .
in all data sets the task is to identify the digit ( 123 - 123 ) and ignore the various distracting factors of variation .
123 data sets
following the work of larochelle et al .
( 123 ) and vincent et al .
( 123 ) , we use a variety of classi - cation data sets that include many factors of variation . 123
the mnist basic data set is a subset of the well - known mnist handwritten digit data set ( lecun et al . , 123a ) .
this data set has 123x123 pixel grey - scale images of digits , each belonging to one of ten classes .
we chose a different train / test / validation splitting in order to have faster experiments and see learning performance differences more clearly .
we shufed the original splits randomly , and used 123 123 training examples , 123 validation examples , and 123 123 testing examples .
these images are presented as white ( 123 - valued ) foreground digits against a black ( 123 - valued ) background .
the mnist background images data set is a variation on mnist basic in which the white fore - ground digit has been composited on top of a 123x123 natural image patch .
technically this was done by taking the maximum of the original mnist image and the patch .
natural image patches with very low pixel variance were rejected .
as with mnist basic there are 123 classes , 123 123 training examples , 123 validation examples , and 123 123 test examples .
the mnist background random data set is a similar variation on mnist basic in which the white foreground digit has been composited on top of random uniform ( 123 , 123 ) pixel values .
as with mnist basic there are 123 classes , 123 123 training examples , 123 validation examples , and 123 123
the mnist rotated data set is a variation on mnist basic in which the images have been rotated radians .
this data set included 123 training
by an amount chosen randomly between 123 and 123p examples , 123 validation examples , 123 123 test examples .
random search for hyper - parameter optimization
figure 123 : top : samples from the rectangles data set .
middle : samples from the rectangles images data set .
bottom : samples from the convex data set .
in rectangles data sets , the image is formed by overlaying a small rectangle on a background .
the task is to label the small rectangle as being either tall or wide .
in convex , the task is to identify whether the set of white pixels is convex ( images 123 and 123 ) or not convex ( images 123 and 123 ) .
the mnist rotated background images data set is a variation on mnist rotated in which the images have been rotated by an amount chosen randomly between 123 and 123p radians , and then sub - sequently composited onto natural image patch backgrounds .
this data set included 123 training examples , 123 validation examples , 123 123 test examples .
the rectangles data set ( figure 123 , top ) is a simple synthetic data set of outlines of rectangles .
the images are 123x123 , the outlines are white ( 123 - valued ) and the backgrounds are black ( 123 - valued ) .
the height and width of the rectangles were sampled uniformly , but when their difference was smaller than 123 pixels the samples were rejected .
the top left corner of the rectangles was also sampled uniformly , with the constraint that the whole rectangle ts in the image .
each image is labelled as one of two classes : tall or wide .
this task was easier than the mnist digit classication , so we only used 123 training examples , and 123 validation examples , but we still used 123 123
the rectangles images data set ( figure 123 , middle ) is a variation on rectangles in which the foreground rectangles were lled with one natural image patch , and composited on top of a different background natural image patch .
the process for sampling rectangle shapes was similar to the one used for rectangles , except a ) the area covered by the rectangles was constrained to be between 123% and 123% of the total image , b ) the length and width of the rectangles were forced to be of at least 123 pixels , and c ) their difference was forced to be of at least 123 pixels .
this task was harder than rectangles , so we used 123 training examples , 123 validation examples , and 123 123 testing
the convex data set ( figure 123 , bottom ) is a binary image classication task .
each 123x123 image consists entirely of 123 - valued and 123 - valued pixels .
if the 123 - valued pixels form a convex region in image space , then the image is labelled as being convex , otherwise it is labelled as non - convex .
the convex sets consist of a single convex region with pixels of value 123 .
candidate convex images were constructed by taking the intersection of a number of half - planes whose location and orienta -
bergstra and bengio
tion were chosen uniformly at random .
the number of intersecting half - planes was also sampled randomly according to a geometric distribution with parameter 123 .
a candidate convex image was rejected if there were less than 123 pixels in the convex region .
candidate non - convex images were constructed by taking the union of a random number of convex sets generated as above , but with the number of half - planes sampled from a geometric distribution with parameter 123 and with a minimum number of 123 pixels .
the number of convex sets was sampled uniformly from 123 to 123
the candidate non - convex images were then tested by checking a convexity condition for every pair of pixels in the non - convex set .
those sets that failed the convexity test were added to the data set .
the parameters for generating the convex and non - convex sets were balanced to ensure that the conditional overall pixel mean is the same for both classes .
123 case study : neural networks
in larochelle et al .
( 123 ) , the hyper - parameters of the neural network were optimized by search over a grid of trials .
we describe the hyper - parameter conguration space of our neural network learning algorithm in terms of the distribution that we will use to randomly sample from that con - guration space .
the rst hyper - parameter in our conguration is the type of data preprocessing : with equal probability , one of ( a ) none , ( b ) normalize ( center each feature dimension and divide by its standard deviation ) , or ( c ) pca ( after removing dimension - wise means , examples are projected onto principle components of the data whose norms have been divided by their eigenvalues ) .
part of pca preprocessing is choosing how many components to keep .
we choose a fraction of variance to keep with a uniform distribution between 123 and 123 .
there have been several suggestions for how the random weights of a neural network should be initialized ( we will look at unsupervised learning pretraining algorithms later in section 123 ) .
we experimented with two distributions and two scaling heuristics .
the possible distributions were ( a ) uniform on ( 123 , 123 ) , and ( b ) unit normal .
the two scaling heuristics were ( a ) a hyper - parameter multiplier between 123 and 123 divided by the square root of the number of inputs ( lecun et al . , 123b ) , and ( b ) the square root of 123 divided by the square root of the number of inputs plus hidden units ( bengio and glorot , 123 ) .
the weights themselves were chosen using one of three random seeds to the mersenne twister pseudo - random number generator .
in the case of the rst heuristic , we chose a multiplier uniformly from the range ( 123 , 123 ) .
the number of hidden units was drawn geometrically123 from 123 to 123
we selected either a sigmoidal or tanh nonlinearity with equal probability .
the output weights from hidden units to prediction units were initialized to zero .
the cost function was the mean error over minibatches of either 123 or 123 ( with equal probability ) examples at a time : in expectation these give the same gradient directions , but with more or less variance .
the optimization algorithm was stochastic gra - dient descent with ( initial ) learning rate e 123 drawn geometrically from 123 to 123 .
we offered the possibility of an annealed learning rate via a time point t123 drawn geometrically from 123 to 123
the effective learning rate e
t after t minibatch iterations was
we permitted a minimum of 123 and a maximum of 123 iterations over the training data , stopping if ever , at iteration t , the best validation performance was observed before iteration t / 123
with 123%
we will use the phrase drawn geometrically from a to b for 123 < a < b to mean drawing uniformly in the log domain between log ( a ) and log ( b ) , exponentiating to get a number between a and b , and then rounding to the nearest integer .
the phrase drawn exponentially means the same thing but without rounding .
random search for hyper - parameter optimization
probability , an 123 regularization penalty was applied , whose strength was drawn exponentially from 123 123 to 123 123
this sampling process covers roughly the same domain with the same density as the grid used in larochelle et al .
( 123 ) , except for the optional preprocessing steps .
the grid optimization of larochelle et al .
( 123 ) did not consider normalizing or keeping only leading pca dimensions of the inputs; we compare to random sampling with and without these restrictions . 123 we formed experiments for each data set by drawing s = 123 trials from this distribution .
the results of these experiments are illustrated in figures 123 and 123
random sampling of trials is surpris - ingly effective in these settings .
figure 123 shows that even among the fraction of jobs ( 123 / 123 ) that used no preprocessing , the random search with 123 trials is better than the grid search employed in larochelle et al .
( 123 ) .
typically , the extent of a grid search is determined by a computational budget .
figure 123 shows what is possible if we use random search in a larger space that requires more trials to explore .
the larger search space includes the possibility of normalizing the input or applying pca preprocessing .
in the larger space , 123 trials were necessary to consistently outperform grid search rather than 123 , indicating that there are many harmful ways to preprocess the data .
however , when we allowed larger experiments of 123 trials or more , random search found superior results to those found more quickly within the more restricted search .
this tradeoff between exploration and exploitation is central to the design of an effective random search .
the efciency curves in figures 123 and 123 reveal that different data sets give rise to functions y
with different shapes .
the mnist basic results converge very rapidly toward what appears to be a global maximum .
the fact that experiments of just 123 or 123 trials often have the same maximum as much larger experiments indicates that the region of l that gives rise to the best performance is approximately a quarter or an eighth respectively of the entire conguration space .
assuming that the random search has not missed a tiny region of signicantly better performance , we can say that random search has solved this problem in 123 or 123 guesses .
it is hard to imagine any optimization algorithm doing much better on a non - trivial 123 - dimensional function .
in contrast the mnist rotated background images and convex curves show that even with 123 or 123 random trials , there is consid - erable variation in the generalization of the reportedly best model .
this indicates that the y in these cases is more peaked , with small regions of good performance .
the low effective dimension of y
section 123 showed that random sampling is more efcient than grid sampling for optimizing func - corresponding to several neural network families and classication tasks .
in this section we show that indeed y has a low effective dimension , which explains why randomly sampled trials found better values .
one simple way to characterize the shape of a high - dimensional function is to look at how much it varies in each dimension .
gaussian process regression gives us the statis - tical machinery to look at y and measure its effective dimensionality ( neal , 123; rasmussen and
we estimated the sensitivity of y
to each hyper - parameter by tting a gaussian process ( gp ) with squared exponential kernels to predict y .
the squared exponential kernel ( or gaussian kernel ) measures similarity between two real - valued hyper - parameter values a and b by ) .
the positive - valued l governs the sensitivity of the gp to change in this hyper -
( l ) from l
source code for the simulations is available at https : / / github . com / jaberg / hyperopt .
bergstra and bengio
mnist background images
experiment size ( # trials )
experiment size ( # trials )
mnist rotated background images
mnist background random
experiment size ( # trials )
experiment size ( # trials )
experiment size ( # trials )
experiment size ( # trials )
experiment size ( # trials )
experiment size ( # trials )
figure 123 : neural network performance without preprocessing .
random experiment efciency curves of a single - layer neural network for eight of the data sets used in larochelle et al .
( 123 ) , looking only at trials with no preprocessing ( 123 hyper - parameters to optimize ) .
the vertical axis is test - set accuracy of the best model by cross - validation , the horizontal axis is the experiment size ( the number of models compared in cross - validation ) .
the dashed blue line represents grid search accuracy for neural network models based on a selection by grids averaging 123 trials ( larochelle et al . , 123 ) .
random searches of 123 trials match or outperform grid searches of ( on average ) 123 trials .
parameter .
the kernels dened for each hyper - parameter were combined by multiplication ( joint gaussian kernel ) .
we t a gp to samples of y by nding the length scale ( l ) for each hyper - parameter that maximized the marginal likelihood .
to ensure relevance could be compared between hyper - parameters , we shifted and scaled each one to the unit interval .
for hyper - parameters that were drawn geometrically or exponentially ( e . g . , learning rate , number of hidden units ) , kernel calculations were based on the logarithm of the effective value .
random search for hyper - parameter optimization
mnist background images
experiment size ( # trials )
experiment size ( # trials )
mnist rotated background images
mnist background random
experiment size ( # trials )
experiment size ( # trials )
experiment size ( # trials )
experiment size ( # trials )
experiment size ( # trials )
experiment size ( # trials )
figure 123 : neural network performance when standard preprocessing algorithms are considered ( 123 hyper - parameters ) .
dashed blue line represents grid search accuracy using ( on average ) 123 trials ( larochelle et al . , 123 ) , in which no preprocessing was done .
often the extent of a search is determined by a computational budget , and with random search 123 trials are enough to nd better models in a larger less promising space .
exploring just four pca variance levels by grid search would have required 123 times as many ( average 123 ) trials per data set .
figure 123 shows the relevance of each component of l
finding the length scales that maximize marginal likelihood is not a convex problem and many local minima exist .
to get a sense of what length scales were supported by the data , we t each set of samples from y 123 times , resampling different subsets of 123% of the observations every time , and reinitializing the length scale estimates randomly between 123 and 123
figure 123 reveals two important properties of y for neural networks that suggest why grid search performs so poorly relative to random experiments :
in modelling y
a small fraction of hyper - parameters matter for any one data set , but
bergstra and bengio
mnist background images
mnist background random
relevance ( 123 / length scale )
relevance ( 123 / length scale )
relevance ( 123 / length scale )
mnist rotated back .
images
relevance ( 123 / length scale )
relevance ( 123 / length scale )
relevance ( 123 / length scale )
hidden units
initial w algo .
initial w norm
learn rate anneal .
relevance ( 123 / length scale )
relevance ( 123 / length scale )
figure 123 : automatic relevance determination ( ard ) applied to hyper - parameters of neural net - work experiments ( with raw preprocessing ) .
for each data set , a small number of hyper - parameters dominate performance , but the relative importance of each hyper - parameter varies from each data set to the next .
section 123 describes the seven hyper - parameters in each panel .
boxplots are obtained by randomizing the subset of data used to t the length scales , and randomizing the length scale initialization .
( best viewed in color . )
random search for hyper - parameter optimization
different hyper - parameters matter on different data sets .
even in this simple 123 - d problem , y has a much lower effective dimension of between 123 and 123 , depending on the data set .
it would be impossible to cover just these few dimensions with a reli - able grid however , because different data sets call for grids on different dimensions .
the learning rate is always important , but sometimes the learning rate annealing rate was important ( rectangles images ) , sometimes the 123 - penalty was important ( convex , mnist rotated ) , sometimes the number of hidden units was important ( rectangles ) , and so on .
while random search optimized these y functions with 123 to 123 trials , a grid with , say , four values in each of these axes would already require 123 trials , and yet provide no guarantee that y
for a new data set would be well optimized .
figure 123 also allows us to establish a correlation between effective dimensionality and ease of optimization .
the data sets for which the effective dimensionality was lowest ( 123 or 123 ) were mnist basic , mnist background images , mnist background random , and rectangles images .
looking back at the corresponding efciency curves ( figure 123 ) we nd that these are also the data sets whose curves plateau most sharply , indicating that these functions are the easiest to optimize .
they are often optimized reasonably well by just 123 random trials .
looking to figure 123 at the data sets with largest effective dimensionality ( 123 or 123 ) , we identify convex , mnist rotated , rectangles .
looking at their efciency curves in figure 123 reveals that they consistently required at least 123 random trials .
this correlation offers another piece of evidence that the effective dimensionality of y is playing a strong role in determining the difculty of hyper - parameter optimization .
grid search and sets with low effective dimensionality
it is an interesting mathematical challenge to choose a set of trials for sampling functions of un - known , but low effective dimensionality .
we would like it to be true that no matter which dimen - sions turn out to be important , our trials sample the important dimensions evenly .
sets of points with this property are well studied in the literature of quasi - random methods for numerical integration , where they are known as low - discrepancy sets because they try to match ( minimize discrepancy with ) the uniform distribution .
although there are several formal denitions of low discrepancy , they all capture the intuition that the points should be roughly equidistant from one another , in order that there be no clumps or holes in the point set .
several procedures for constructing low - discrepancy point sets in multiple dimensions also try to ensure as much as possible that subspace projections remain low - discrepancy sets in the subspace .
for example , the sobol ( antonov and saleev , 123 ) , halton ( halton , 123 ) , and niederreiter ( brat - ley et al . , 123 ) sequences , as well as latin hypercube sampling ( mckay et al . , 123 ) are all more or less deterministic schemes for getting point sets that are more representative of random uniform draws than actual random uniform draws .
in quasi monte - carlo integration , such point sets are shown to asymptotically minimize the variance of nite integrals faster than true random uniform samples , but in this section , we will look at these point sets in the setting of relatively small sample sizes , to see if they can be used for more efcient search than random draws .
rather than repeat the very computationally expensive experiments conducted in section 123 , we used an articial simulation to compare the efciency of grids , random draws , and the four low - discrepancy point sets mentioned in the previous paragraph .
the articial search problem was to nd a uniformly randomly placed multi - dimensional target interval , which occupies 123% of the volume of the unit hyper - cube .
we looked at four variants of the search problem , in which the target
bergstra and bengio
a cube in a 123 - dimensional space ,
a hyper - rectangle in a 123 - dimensional space ,
a hyper - cube in a 123 - dimensional space ,
a hyper - rectangle in a 123 - dimensional space .
the shape of the target rectangle in variants ( 123 ) and ( 123 ) was determined by sampling side lengths uniformly from the unit interval , and then scaling the rectangle to have a volume of 123% .
this process gave the rectangles a shape that was often wide or tall - much longer along some axes than others .
the position of the target was drawn uniformly among the positions totally inside the unit hyper - cube .
in the case of tall or wide targets ( 123 ) and ( 123 ) , the indicator function ( of the target ) had a lower effective dimension than the dimensionality of the overall space because the dimensions in which the target is elongated can be almost ignored .
the simulation experiment began with the generation of 123 random search problems .
then for each experiment design method ( random , sobol , latin hypercube , grid ) we created experiments of 123 , 123 , 123 , and so on up to 123 trials . 123 the sobol , niederreiter , and halton sequences yielded similar results , so we used the sobol sequence to represent the performance of these low - discepancy set construction methods .
there are many possible grid experiments of any size in multiple dimensions ( at least for non - prime experiment sizes ) .
we did not test every possible grid , instead we tested every grid with a monotonic resolution .
for example , for experiments of size 123 in 123 dimensions we tried the ve grids with resolutions ( 123 , 123 , 123 , 123 , 123 ) , ( 123 , 123 , 123 , 123 , 123 ) , ( 123 , 123 , 123 , 123 , 123 ) , ( 123 , 123 , 123 , 123 , 123 ) , ( 123 , 123 , 123 , 123 , 123 ) ; for experiments of some prime size p in 123 dimensions we tried one grid with resolution ( 123 , 123 , p ) .
since the target intervals were generated in such a way that rectangles identical up to a permutation of side lengths have equal probability , grids with monotonic resolution are representative of all grids .
the score of an experiment design method for each experiment size was the fraction of the 123 targets that it found .
to characterize the performance of random search , we used the analytic form of the expectation .
the expected probability of nding the target is 123 minus the probability of missing the target with every single one of t trials in the experiment .
if the volume of the target relative to the unit hypercube is ( v / v = 123 ) and there are t trials , then this probability of nding the target is
) t = 123 123t .
figure 123 illustrates the efciency of each kind of point set at nding the multidimensional in - tervals .
there were some grids that were best at nding cubes and hyper - cubes in 123 - d and 123 - d , but most grids were the worst performers .
no grid was competitive with the other methods at nding the rectangular - shaped intervals , which had low effective dimension ( cases 123 and 123; figure 123 , right panels ) .
latin hypercubes , commonly used to initialize experiments in bayesian optimization , were no more efcient than the expected performance of random search .
interestingly , the sobol se - quence was consistently best by a few percentage points .
the low - discrepancy property that makes the sobol useful in integration helps here , where it has the effect of minimizing the size of holes where the target might pass undetected .
the advantage of the sobol sequence is most pronounced in experiments of 123 - 123 trials , where there are sufciently many trials for the structure in the sobol
samples from the sobol sequence were provided by the gnu scientic library ( m .
galassi et al . , 123 ) .
random search for hyper - parameter optimization
figure 123 : the efciency in simulation of low - discrepancy sequences relative to grid and pseudo - random experiments .
the simulation tested how reliably various experiment design meth - ods locate a multidimensional interval occupying 123% of a unit hyper - cube .
there is one grey dot in each sub - plot for every grid of every experiment size that has at least two ticks in each dimension .
the black dots indicate near - perfect grids whose nest and coarsest dimensional resolutions differ by either 123 or 123
hyper - parameter search is most typi - cally like the bottom - right scenario .
grid search experiments are inefcient for nding axis - aligned elongated regions in high dimensions ( i . e . , bottom - right ) .
pseudo - random samples are as efcient as latin hypercube samples , and slightly less efcient than the
depart signicantly from i . i . d points , but not sufciently many trials for random search to succeed with high probability .
a thought experiment gives some intuition for why grid search fails in the case of rectangles .
long thin rectangles tend to intersect with several points if they intersect with any , reducing the effective sample size of the search .
if the rectangles had been rotated away from the axes used to build the grid , then depending on the angle the efciency of grid could approach the efciency of random or low - discrepancy trials .
more generally , if the target manifold were not systematically aligned with subsets of trial points , then grid search would be as efcient as the random and quasi -
bergstra and bengio
random search vs .
sequential manual optimization
to see how random search compares with a careful combination of grid search and hand - tuning in the context of a model with many hyper - parameters , we performed experiments with the deep belief network ( dbn ) model ( hinton et al . , 123 ) .
a dbn is a multi - layer graphical model with directed and undirected components .
it is parameterized similarly to a multilayer neural network for classication , and it has been argued that pretraining a multilayer neural network by unsupervised learning as a dbn acts both to regularize the neural network toward better generalization , and to ease the optimization associated with netuning the neural network for a classication task ( erhan et al . , 123 ) .
a dbn classier has many more hyper - parameters than a neural network .
firstly , there is the number of units and the parameters of random initialization for each layer .
secondly , there are hyper - parameters governing the unsupervised pretraining algorithm for each layer .
finally , there are hyper - parameters governing the global netuning of the whole model for classication .
for the details of how dbn models are trained ( stacking restricted boltzmann machines trained by con - trastive divergence ) , the reader is referred to larochelle et al .
( 123 ) , hinton et al .
( 123 ) or bengio ( 123 ) .
we evaluated random search by training 123 - layer , 123 - layer and 123 - layer dbns , sampling from the following distribution :
we chose 123 , 123 , or 123 layers with equal probability .
for each layer , we chose :
a number of hidden units ( log - uniformly between 123 and 123 ) , a weight initialization heuristic that followed from a distribution ( uniform or normal ) , a multiplier ( uniformly between 123 and 123 ) , a decision to divide by the fan - out ( true or
a number of iterations of contrastive divergence to perform for pretraining ( log - uniformly
from 123 to 123 ) ,
whether to treat the real - valued examples used for unsupervised pretraining as bernoulli means ( from which to draw binary - valued training samples ) or as a samples themselves ( even though they are not binary ) ,
an initial learning rate for contrastive divergence ( log - uniformly between 123 and
a time point at which to start annealing the contrastive divergence learning rate as in
equation 123 ( log - uniformly from 123 to 123 123 ) .
there was also the choice of how to preprocess the data .
either we used the raw pixels or we removed some of the variance using a zca transform ( in which examples are projected onto principle components , and then multiplied by the transpose of the principle components to place them back in the inputs space ) .
if using zca preprocessing , we kept an amount of variance drawn uniformly from 123 to 123 .
we chose to seed our random number generator with one of 123 , 123 , or 123
we chose a learning rate for netuning of the nal classier log - uniformly from 123 to 123
random search for hyper - parameter optimization
we chose an anneal start time for netuning log - uniformly from 123 to 123
we chose 123 regularization of the weight matrices at each layer during netuning to be either
123 ( with probability 123 ) , or log - uniformly from 123 to 123
this hyper - parameter space includes 123 global hyper - parameters and 123 hyper - parameters for each layer , for a total of 123 hyper - parameters for 123 - layer models .
a grid search is not practical for the 123 - dimensional search problem of dbn model selection , because even just 123 possible values for each of 123 hyper - parameters would yield more trials than we could conduct ( 123 > 123 trials and each can take hours ) .
for many of the hyper - parameters , especially real valued ones , we would really like to try more than two values .
the approach taken in larochelle et al .
( 123 ) was a combination of manual search , multi - resolution grid search and coordinate descent .
the algorithm ( including manual steps ) is somewhat elaborate , but sensible , and we believe that it is representative of how model search is typically done in several research groups , if not the community at large .
larochelle et al .
( 123 ) describe it as follows :
the hyper - parameter search procedure we used alternates between xing a neural net - work architecture and searching for good optimization hyper - parameters similarly to coordinate descent .
more time would usually be spent on nding good optimization parameters , given some empirical evidence that we found indicating that the choice of the optimization hyper - parameters ( mostly the learning rates ) has much more inuence on the obtained performance than the size of the network .
we used the same procedure to nd the hyper - parameters for dbn - 123 , which are the same as those of dbn - 123 except the second hidden layer and third hidden layer sizes .
we also allowed ourselves to test for much larger rst - hidden layer sizes , in order to make the comparison between dbn - 123 and dbn - 123 fairer .
we usually started by testing a relatively small architecture ( between 123 and 123 units in the rst and second hidden layer , and between 123 and 123 hidden units in the last layer ) .
given the results obtained on the validation set ( compared to those of nnet for instance ) after selecting appropriate optimization parameters , we would then consider growing the number of units in all layers simultaneously .
the biggest networks we eventually tested had up to 123 , 123 and 123 hidden units in the rst , second and third hidden layers respectively .
as for the optimization hyper - parameters , we would proceed by rst trying a few com - binations of values for the stochastic gradient descent learning rate of the supervised and unsupervised phases ( usually between 123 and 123 ) .
we then rene the choice of tested values for these hyper - parameters .
the rst trials would simply give us a trend on the validation set error for these parameters ( is a change in the hyper - parameter making things worse of better ) and we would then consider that information in selecting ap - propriate additional trials .
one could choose to use learning rate adaptation techniques ( e . g . , slowly decreasing the learning rate or using momentum ) but we did not nd these techniques to be crucial .
there was large variation in the number of trials used in larochelle et al .
( 123 ) to optimize the dbn - 123
one data set ( mnist background images ) beneted from 123 trials , while another ( mnist background random ) only 123 because a good result was found more quickly .
the average number
bergstra and bengio
mnist background random
mnist background images
mnist rotated back .
images
experiment size ( # trials )
experiment size ( # trials )
experiment size ( # trials )
experiment size ( # trials )
figure 123 : deep belief network ( dbn ) performance according to random search .
here random search is used to explore up to 123 hyper - parameters .
results obtained by grid - assisted manual search using an average of 123 trials are marked in nely - dashed green ( 123 - layer dbn ) and coarsely - dashed red ( 123 - layer dbn ) .
random experiments of 123 random trials found an inferior best model for three data sets , a competitive model in four , and superior model in one ( convex ) .
( best viewed in color . )
of trials across data sets for the dbn - 123 model was 123
in considering the number of trials per data set , it is important to bear in mind that the experiments on different data sets were not performed independently .
rather , later experiments beneted from the experience the authors had drawn from earlier ones .
although grid search was part of the optimization loop , the manual intervention turns the overall optimization process into something with more resemblance to an adaptive sequential
random search versions of the dbn experiments from larochelle et al .
( 123 ) are shown in figure 123
in this more challenging optimization problem random search is still effective , but not
random search for hyper - parameter optimization
superior as it was as in the case of neural network optimization .
comparing to the 123 - layer dbn results in larochelle et al .
( 123 ) , random search found a better model than the manual search in one data set ( convex ) , an equally good model in four ( mnist basic , mnist rotated , rectangles , and rectangles images ) , and an inferior model in three ( mnist background images , mnist background random , mnist rotated background images ) .
comparing to the 123 - layer dbn results , random search of the 123 - layer , 123 - layer and 123 - layer conguration space found at least a good a model in all cases .
in comparing these scores , the reader should bear in mind that the scores in the original experiments were not computed using the same score - averaging technique that we described in section 123 , and our averaging technique is slightly biased toward underestimation .
in the dbn efciency curves we see that even experiments with larger numbers of trials ( 123 and larger ) feature signicant variability .
this indicates that the regions of the search space with the best performance are small , and randomly chosen i . i . d .
trials do not reliably nd them .
future work
our result on the multidimensional interval task , together with the gpr characterization of the shape , together with the computational constraint that hyper - parameter searches only draw on a few hundred trials , all suggest that pseudo - random or quasi - random trials are optimal for non - adaptive hyper - parameter search .
there is still work to be done for each model family , to establish how it should be parametrized for i . i . d .
random search to be as reliable as possible , but the most promising and interesting direction for future work is certainly in adaptive algorithms .
there is a large body of literature on global optimization , a great deal of which bears on the ap - plication of hyper - parameter optimization .
general numeric methods such as simplex optimization ( nelder and mead , 123 ) , constrained optimization by linear approximation ( powell , 123; weise , 123 ) , nite difference stochastic approximation and simultaneous prediction stochastic approxi - mation ( kleinman et al . , 123 ) could be useful , as well as methods for search in discrete spaces such as simulated annealing ( kirkpatrick et al . , 123 ) and evolutionary algorithms ( rechenberg , 123; hansen et al . , 123 ) .
drew and de mello ( 123 ) have already proposed an optimization al - gorithm that identies effective dimensions , for more efcient search .
they present an algorithm that distinguishes between important and unimportant dimensions : a low - discrepancy point set is used to choose points in the important dimensions , and unimportant dimensions are padded with thinner coverage and cheaper samples .
their algorithms success hinges on the rapid and successful identication of important dimensions .
sequential model - based optimization methods and partic - ularly bayesian optimization methods are perhaps more promising because they offer principled approaches to weighting the importance of each dimension ( hutter , 123; hutter et al . , 123; srini - vasan and ramakrishnan , 123 ) .
with so many sophisticated algorithms to draw on , it may seem strange that grid search is still widely used , and , with straight faces , we now suggest using random search instead .
we believe the reason for this state of affairs is a technical one .
manual optimization followed by grid search is easy to implement : grid search requires very little code infrastructure beyond access to a cluster of computers .
random search is just as simple to carry out , uses the same tools , and ts in the same workow .
adaptive search algorithms on the other hand require more code complexity .
they require client - server architectures in which a master process keeps track of the trials that have com - pleted , the trials that are in progress , the trials that were started but failed to complete .
some kind of shared database and inter - process communication mechanisms are required .
trials in an adaptive
bergstra and bengio
experiment cannot be queued up all at once; the master process must be involved somehow in the scheduling and timing of jobs on the cluster .
these technical hurdles are not easy to jump with the standard tools of the trade such as matlab or python; signicant software engineering is required .
until that engineering is done and adopted by a community of researchers , progress on the study of sophisticated hyper - parameter optimization algorithms will be slow .
grid search experiments are common in the literature of empirical machine learning , where they are used to optimize the hyper - parameters of learning algorithms .
it is also common to perform multi - stage , multi - resolution grid experiments that are more or less automated , because a grid experiment with a ne - enough resolution for optimization would be prohibitively expensive .
we have shown that random experiments are more efcient than grid experiments for hyper - parameter optimization in the case of several learning algorithms on several data sets .
our analysis of the hyper - parameter response surface ( y ) suggests that random experiments are more efcient because not all hyper - parameters are equally important to tune .
grid search experiments allocate too many trials to the exploration of dimensions that do not matter and suffer from poor coverage in dimensions that are important .
compared with the grid search experiments of larochelle et al .
( 123 ) , random search found better models in most cases and required less computational time .
random experiments are also easier to carry out than grid experiments for practical reasons
related to the statistical independence of every trial .
the experiment can be stopped any time and the trials form a complete experiment .
if extra computers become available , new trials can be added to an experiment without having
to adjust the grid and commit to a much larger experiment .
every trial can be carried out asynchronously .
if the computer carrying out a trial fails for any reason , its trial can be either abandoned or
restarted without jeopardizing the experiment .
random search is not incompatible with a controlled experiment .
to investigate the effect of one hyper - parameter of interest x , we recommend random search ( instead of grid search ) for optimizing over other hyper - parameters .
choose one set of random values for these remaining hyper - parameters and use that same set for each value of x .
random experiments with large numbers of trials also bring attention to the question of how to measure test error of an experiment when many trials have some claim to being best .
when using a relatively small validation set , the uncertainty involved in selecting the best model by cross - validation can be larger than the uncertainty in measuring the test set performance of any one model .
it is important to take both of these sources of uncertainty into account when reporting the uncer - tainty around the best model found by a model search algorithm .
this technique is useful to all experiments ( including both random and grid ) in which multiple models achieve approximately the best validation set performance .
low - discrepancy sequences developed for qmc integration are also good alternatives to grid - based experiments .
in low dimensions ( e . g . , 123 - 123 ) our simulated results suggest that they can hold some advantage over pseudo - random experiments in terms of search efciency .
however , the trials
random search for hyper - parameter optimization
of a low - discrepancy experiment are not i . i . d .
which makes it inappropriate to analyze performance with the random efciency curve .
it is also more difcult in practice to conduct a quasi - random experiment because like a grid experiment , the omission of a single point can be more severe .
finally , when there are many hyper - parameter dimensions relative to the computational budget for the experiment , a low - discrepancy trial set is not expected to behave very differently from a pseudo -
finally , the hyper - parameter optimization strategies considered here are non - adaptive : they do not vary the course of the experiment by considering any results that are already available .
random search was not generally as good as the sequential combination of manual and grid search from an expert ( larochelle et al . , 123 ) in the case of the 123 - dimensional search problem of dbn op - timization , because the efciency of sequential optimization overcame the inefciency of the grid search employed at each step of the procedure .
future work should consider sequential , adaptive search / optimization algorithms in settings where many hyper - parameters of an expensive function must be optimized jointly and the effective dimensionality is high .
we hope that future work in that direction will consider random search of the form studied here as a baseline for performance , rather than grid search .
this work was supported by the national science and engineering research council of canada and compute canada , and implemented with theano ( bergstra et al . , 123 ) .
