complexity theory of circuits strongly suggests that deep architectures can be much more ef ( cid : 123 ) cient ( sometimes exponentially ) than shallow architectures , in terms of computational elements required to represent some functions .
deep multi - layer neural networks have many levels of non - linearities allowing them to compactly represent highly non - linear and highly - varying functions .
however , until recently it was not clear how to train such deep networks , since gradient - based optimization starting from random initialization appears to often get stuck in poor solutions .
hin - ton et al .
recently introduced a greedy layer - wise unsupervised learning algorithm for deep belief networks ( dbn ) , a generative model with many layers of hidden causal variables .
in the context of the above optimization problem , we study this al - gorithm empirically and explore variants to better understand its success and extend it to cases where the inputs are continuous or where the structure of the input dis - tribution is not revealing enough about the variable to be predicted in a supervised task .
our experiments also con ( cid : 123 ) rm the hypothesis that the greedy layer - wise unsu - pervised training strategy mostly helps the optimization , by initializing weights in a region near a good local minimum , giving rise to internal distributed representations that are high - level abstractions of the input , bringing better generalization .
recent analyses ( bengio , delalleau , & le roux , 123; bengio & le cun , 123 ) of modern non - parametric machine learning algorithms that are kernel machines , such as support vector machines ( svms ) , graph - based manifold and semi - supervised learning algorithms suggest fundamental limita - tions of some learning algorithms .
the problem is clear in kernel - based approaches when the kernel is ( cid : 123 ) local ( cid : 123 ) ( e . g . , the gaussian kernel ) , i . e . , k ( x; y ) converges to a constant when jjx ( cid : 123 ) yjj increases .
these analyses point to the dif ( cid : 123 ) culty of learning ( cid : 123 ) highly - varying functions ( cid : 123 ) , i . e . , functions that have a large number of ( cid : 123 ) variations ( cid : 123 ) in the domain of interest , e . g . , they would require a large number of pieces to be well represented by a piecewise - linear approximation .
since the number of pieces can be made to grow exponentially with the number of factors of variations in the input , this is connected with the well - known curse of dimensionality for classical non - parametric learning algorithms ( for regres - sion , classi ( cid : 123 ) cation and density estimation ) .
if the shapes of all these pieces are unrelated , one needs enough examples for each piece in order to generalize properly .
however , if these shapes are related and can be predicted from each other , ( cid : 123 ) non - local ( cid : 123 ) learning algorithms have the potential to generalize to pieces not covered by the training set .
such ability would seem necessary for learning in complex domains such as arti ( cid : 123 ) cial intelligence tasks ( e . g . , related to vision , language , speech , robotics ) .
kernel machines ( not only those with a local kernel ) have a shallow architecture , i . e . , only two levels of data - dependent computational elements .
this is also true of feedforward neural networks with a single hidden layer ( which can become svms when the number of hidden units becomes large ( bengio , le roux , vincent , delalleau , & marcotte , 123 ) ) .
a serious problem with shallow architectures is that they can be very inef ( cid : 123 ) cient in terms of the number of computational units ( e . g . , bases , hidden units ) , and thus in terms of required examples ( bengio & le cun , 123 ) .
one way to represent a highly - varying function compactly ( with few parameters ) is through the composition of many non - linearities , i . e . , with a deep architecture .
for example , the parity function with d inputs requires o ( 123d ) examples and parameters to be represented by a gaussian svm ( bengio et al . , 123 ) , o ( d123 ) parameters for a one - hidden - layer neural network , o ( d ) parameters and units for a multi - layer network with o ( log123 d ) layers , and o ( 123 ) parameters with a recurrent neural network .
more generally ,
boolean functions ( such as the function that computes the multiplication of two numbers from their d - bit representation ) expressible by o ( log d ) layers of combinatorial logic with o ( d ) elements in each layer may require o ( 123d ) elements when expressed with only 123 layers ( utgoff & stracuzzi , 123; bengio & le cun , 123 ) .
when the representation of a concept requires an exponential number of elements , e . g . , with a shallow circuit , the number of training examples required to learn the concept may also be impractical .
formal analyses of the computational complexity of shallow circuits can be found in ( hastad , 123 ) or ( allender , 123 ) .
they point in the same direction : shallow circuits are much less expressive than deep ones .
however , until recently , it was believed too dif ( cid : 123 ) cult to train deep multi - layer neural networks .
empiri - cally , deep networks were generally found to be not better , and often worse , than neural networks with one or two hidden layers ( tesauro , 123 ) .
as this is a negative result , it has not been much reported in the machine learning literature .
a reasonable explanation is that gradient - based optimization starting from random initialization may get stuck near poor solutions .
an approach that has been explored with some success in the past is based on constructively adding layers .
this was previously done using a supervised criterion at each stage ( fahlman & lebiere , 123; lengelle & denoeux , 123 ) .
hinton , osindero , and teh ( 123 ) recently introduced a greedy layer - wise unsupervised learning algorithm for deep belief networks ( dbn ) , a generative model with many layers of hidden causal variables .
the training strategy for such networks may hold great promise as a principle to help address the problem of training deep networks .
upper layers of a dbn are supposed to represent more ( cid : 123 ) abstract ( cid : 123 ) concepts that explain the input observation x , whereas lower layers extract ( cid : 123 ) low - level features ( cid : 123 ) from x .
they learn simpler concepts ( cid : 123 ) rst , and build on them to learn more abstract concepts .
this strategy , studied in detail here , has not yet been much exploited in machine learning .
we hypothesize that three aspects of this strategy are particularly important : ( cid : 123 ) rst , pre - training one layer at a time in a greedy way; sec - ond , using unsupervised learning at each layer in order to preserve information from the input; and ( cid : 123 ) nally , ( cid : 123 ) ne - tuning the whole network with respect to the ultimate criterion of interest .
we ( cid : 123 ) rst extend dbns and their component layers , restricted boltzmann machines ( rbm ) , so that they can more naturally handle continuous values in input .
second , we perform experiments to better understand the advantage brought by the greedy layer - wise unsupervised learning .
the basic question to answer is whether or not this approach helps to solve a dif ( cid : 123 ) cult optimization problem .
in dbns , rbms are used as building blocks , but applying this same strategy using auto - encoders yielded similar results .
finally , we discuss a problem that occurs with the layer - wise greedy unsupervised procedure when the input distribution is not revealing enough of the conditional distribution of the target variable given the input variable .
we evaluate a simple and successful solution to this problem .
123 deep belief nets let x be the input , and gi the hidden variables at layer i , with joint distribution
p ( x; g123; g123; : : : ; g ) = p ( xjg123 ) p ( g123jg123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) p ( g ( cid : 123 ) 123jg ( cid : 123 ) 123 ) p ( g ( cid : 123 ) 123; g ) ;
where all the conditional layers p ( gijgi+123 ) are factorized conditional distributions for which compu - tation of probability and sampling are easy .
in hinton et al .
( 123 ) one considers the hidden layer g i a binary random vector with ni elements gi
p ( gijgi+123 ) =
jjgi+123 ) with p ( gi
j = 123jgi+123 ) = sigm ( bi
j are biases for unit j of layer i , and w i is the weight matrix for
where sigm ( t ) = 123= ( 123 + e ( cid : 123 ) t ) , the bi layer i .
if we denote g123 = x , the generative model for the ( cid : 123 ) rst layer p ( xjg123 ) also follows ( 123 ) .
123 restricted boltzmann machines the top - level prior p ( g ( cid : 123 ) 123; g ) is a restricted boltzmann machine ( rbm ) between layer ( cid : 123 ) 123 and layer .
to lighten notation , consider a generic rbm with input layer activations v ( for visi - ble units ) and hidden layer activations h ( for hidden units ) .
it has the following joint distribution : z eh123w v+b123v+c123h , where z is the normalization constant for this distribution , b is the vec - p ( v; h ) = 123 tor of biases for visible units , c is the vector of biases for the hidden units , and w is the weight matrix for the layer .
minus the argument of the exponential is called the energy function ,
we denote the rbm parameters together with ( cid : 123 ) = ( w; b; c ) .
we denote q ( hjv ) and p ( vjh ) the layer - to - layer conditional distributions associated with the above rbm joint distribution .
energy ( v; h ) = ( cid : 123 ) h123w v ( cid : 123 ) b123v ( cid : 123 ) c123h :
the layer - to - layer conditionals associated with the rbm factorize like in ( 123 ) and give rise to
p ( vk = 123jh ) = sigm ( bk +pj wjkhj ) and q ( hj = 123jv ) = sigm ( cj +pk wjkvk ) .
123 gibbs markov chain and log - likelihood gradient in an rbm to obtain an estimator of the gradient on the log - likelihood of an rbm , we consider a gibbs markov chain on the ( visible units , hidden units ) pair of variables .
gibbs sampling from an rbm proceeds by sampling h given v , then v given h , etc .
denote vt for the t - th v sample from that chain , starting at t = 123 with v123 , the ( cid : 123 ) input observation ( cid : 123 ) for the rbm .
therefore , ( vk; hk ) for k ! 123 is a sample from the joint p ( v; h ) .
the log - likelihood of a value v123 under the model of the rbm is
log p ( v123 ) = logxh
p ( v123; h ) = logxh
e ( cid : 123 ) energy ( v123;h ) ( cid : 123 ) logxv;h
and its gradient with respect to ( cid : 123 ) = ( w; b; c ) is
@ log p ( v123 )
p ( vk; hk )
+ ehk ( cid : 123 ) @energy ( vk; hk )
for k ! 123
an unbiased sample is ( cid : 123 ) where h123 is a sample from q ( h123jv123 ) and ( vk; hk ) is a sample of the markov chain , and the expecta - tion can be easily computed thanks to p ( hkjvk ) factorizing .
the idea of the contrastive divergence algorithm ( hinton , 123 ) is to take k small ( typically k = 123 ) .
a pseudo - code for contrastive di - vergence training ( with k = 123 ) of an rbm with binomial input and hidden units is presented in the appendix ( algorithm rbmupdate ( x; ( cid : 123 ) ; w; b; c ) ) .
this procedure is called repeatedly with v123 = x sampled from the training distribution for the rbm .
to decide when to stop one may use a proxy for the training criterion , such as the reconstruction error ( cid : 123 ) log p ( v123 = xjv123 = x ) .
123 greedy layer - wise training of a dbn a greedy layer - wise training algorithm was proposed ( hinton et al . , 123 ) to train a dbn one layer at a time .
one ( cid : 123 ) rst trains an rbm that takes the empirical data as input and models it .
denote q ( g123jg123 ) the posterior over g123 associated with that trained rbm ( we recall that g123 = x with x the observed
input ) .
this gives rise to an ( cid : 123 ) empirical ( cid : 123 ) distribution bp123 over the ( cid : 123 ) rst layer g123 , when g123 is sampled from the data empirical distributionbp : we havebp123 ( g123 ) =xg123 bp ( g123 ) q ( g123jg123 ) :
note that a 123 - level dbn is an rbm .
the basic idea of the greedy layer - wise strategy is that after training the top - level rbm of a - level dbn , one changes the interpretation of the rbm parameters to insert them in a ( + 123 ) - level dbn : the distribution p ( g ( cid : 123 ) 123jg ) from the rbm associated with layers ( cid : 123 ) 123 and is kept as part of the dbn generative model .
in the rbm between layers ( cid : 123 ) 123 and , p ( g ) is de ( cid : 123 ) ned in terms on the parameters of that rbm , whereas in the dbn p ( g ) is de ( cid : 123 ) ned in terms of the parameters of the upper layers .
consequently , q ( gjg ( cid : 123 ) 123 ) of the rbm does not correspond to p ( gjg ( cid : 123 ) 123 ) in the dbn , except when that rbm is the top layer of the dbn .
however , we use q ( gjg ( cid : 123 ) 123 ) of the rbm as an approximation of the posterior p ( gjg ( cid : 123 ) 123 ) for the dbn .
the samples of g ( cid : 123 ) 123 , with empirical distributionbp ( cid : 123 ) 123 , are converted stochastically into samples of g with distribution bp through bp ( g ) = pg ( cid : 123 ) 123 bp ( cid : 123 ) 123 ( g ( cid : 123 ) 123 ) q ( gjg ( cid : 123 ) 123 ) : although bp cannot be rep -
resented explicitly it is easy to sample unbiasedly from it : pick a training example and propagate it stochastically through the q ( gijgi ( cid : 123 ) 123 ) at each level .
as a nice side bene ( cid : 123 ) t , one obtains an approxi - mation of the posterior for all the hidden variables in the dbn , at all levels , given an input g123 = x .
mean - ( cid : 123 ) eld propagation ( see below ) gives a fast deterministic approximation of posteriors p ( g jx ) : note that if we consider all the layers of a dbn from level i to the top , we have a smaller dbn , which generates the marginal distribution p ( gi ) for the complete dbn .
the motivation for the greedy procedure is that a partial dbn with ( cid : 123 ) i levels starting above level i may provide a better model for p ( gi ) than does the rbm initially associated with level i itself .
the above greedy procedure is justi ( cid : 123 ) ed using a variational bound ( hinton et al . , 123 ) .
as a con - sequence of that bound , when inserting an additional layer , if it is initialized appropriately and has enough units , one can guarantee that initial improvements on the training criterion for the next layer
( ( cid : 123 ) tting bp ) will yield improvement on the training criterion for the previous layer ( likelihood with respect tobp ( cid : 123 ) 123 ) .
the greedy layer - wise training algorithm for dbns is quite simple , as illustrated by
the pseudo - code in algorithm trainunsuperviseddbn of the appendix .
123 supervised ( cid : 123 ) ne - tuning as a last training stage , it is possible to ( cid : 123 ) ne - tune the parameters of all the layers together .
for exam - ple hinton et al .
( 123 ) propose to use the wake - sleep algorithm ( hinton , dayan , frey , & neal , 123 ) to continue unsupervised training .
hinton et al .
( 123 ) also propose to optionally use a mean - ( cid : 123 ) eld ap - proximation of the posteriors p ( gijg123 ) , by replacing the samples gi ( cid : 123 ) 123 at level i ( cid : 123 ) 123 by their bit - wise , with ( cid : 123 ) i = sigm ( bi + w i ( cid : 123 ) i ( cid : 123 ) 123 ) .
according to these propagation mean - ( cid : 123 ) eld expected value ( cid : 123 ) i ( cid : 123 ) 123 rules , the whole network now deterministically computes internal representations as functions of the network input g123 = x .
after unsupervised pre - training of the layers of a dbn following algorithm trainunsuperviseddbn ( see appendix ) the whole network can be further optimized by gradient descent with respect to any deterministically computable training criterion that depends on these rep - resentations .
for example , this can be used ( hinton & salakhutdinov , 123 ) to ( cid : 123 ) ne - tune a very deep auto - encoder , minimizing a reconstruction error .
it is also possible to use this as initialization of all except the last layer of a traditional multi - layer neural network , using gradient descent to ( cid : 123 ) ne - tune the whole network with respect to a supervised training criterion .
algorithm dbnsupervisedfinetuning in the appendix contains pseudo - code for supervised ( cid : 123 ) ne - tuning , as part of the global supervised learning algorithm trainsuperviseddbn .
note that better results were obtained when using a 123 - fold larger learning rate with the supervised criterion ( here , squared error or cross - entropy ) updates than in the contrastive divergence updates .
123 extension to continuous - valued inputs with the binary units introduced for rbms and dbns in hinton et al .
( 123 ) one can ( cid : 123 ) cheat ( cid : 123 ) and handle continuous - valued inputs by scaling them to the ( 123 , 123 ) interval and considering each input con - tinuous value as the probability for a binary random variable to take the value 123
this has worked well for pixel gray levels , but it may be inappropriate for other kinds of input variables .
previous work on continuous - valued input in rbms include ( chen & murray , 123 ) , in which noise is added to sigmoidal units , and the rbm forms a special form of diffusion network ( movellan , mineiro , & williams , 123 ) .
we concentrate here on simple extensions of the rbm framework in which only the energy function and the allowed range of values are changed .
linear energy : exponential or truncated exponential consider a unit with value y of an rbm , connected to units z of the other layer .
p ( yjz ) can be obtained from the terms in the exponential that contain y , which can be grouped in ya ( z ) for linear energy functions as in ( 123 ) , where a ( z ) = b + w 123z with b the bias of unit y , and w the vector of weights connecting unit y to units z .
if we allow y to take any value in interval i , the conditional density of y becomes p ( yjz ) = : when i = ( 123; 123 ) , this is an exponential density with parameter a ( z ) , and the normalizing integral equals ( cid : 123 ) 123=a ( z ) , but only exists if 123z , a ( z ) < 123 computing the density , computing the expected value ( = ( cid : 123 ) 123=a ( z ) ) and sampling would all be easy .
alternatively , if i is a closed interval ( as in many applications of interest ) , or if we would like to use such a unit as a hidden unit with non - linear expected value , the above density is a truncated exponential .
for simplicity we consider the case i = ( 123; 123 ) here , for which the normalizing integral , which always exists , is exp ( ( cid : 123 ) a ( z ) ) ( cid : 123 ) 123 .
the conditional expectation of u given z is interesting because a ( z ) : a it has a sigmoidal - like saturating and monotone non - linearity : e ( yjz ) = sampling from the truncated exponential is easily obtained from a uniform sample u , using the inverse cumulative f ( cid : 123 ) 123 of the conditional density yjz : f ( cid : 123 ) 123 ( u ) = log ( 123 ( cid : 123 ) u ( cid : 123 ) ( 123 ( cid : 123 ) exp ( a ( z ) ) ) ) : in both truncated and not truncated cases , the contrastive divergence updates have the same form as for binomial units ( input value times output value ) , since the updates only depend on the derivative of the energy with respect to the parameters .
only sampling is changed , according to the units conditional density .
quadratic energy : gaussian units
123 ( cid : 123 ) exp ( ( cid : 123 ) a ( z ) ) ( cid : 123 ) 123
rv exp ( va ( z ) ) 123v123i dv
to obtain gaussian - distributed units , one adds quadratic terms to the energy .
addingpi d123
rise to a diagonal covariance matrix between units of the same layer , where yi is the continuous value of a gaussian unit and d123 i is a positive parameter that is equal to the inverse of the variance of yi
deep network with no pre - training 123
logistic regression 123
dbn , binomial inputs , unsupervised 123
dbn , binomial inputs , partially supervised 123
dbn , gaussian inputs , unsupervised 123
dbn , gaussian inputs , partially supervised
deep network with no pretraining dbn with partially supervised pretraining dbn with unsupervised pretraining
figure 123 : training classi ( cid : 123 ) cation error vs training iteration , on the cotton price task , for deep net - work without pre - training , for dbn with unsuper - vised pre - training , and dbn with partially super - illustrates optimization dif ( cid : 123 ) - culty of deep networks and advantage of partially
123% 123% 123% 123% 123% 123% 123% 123% 123% 123% 123% 123% 123% 123% 123% 123% 123% 123%
table 123 : meansquaredpredictionerroronabalonetaskandclassi ( cid : 123 ) cationerroroncottontask , this case the variance is unconditional , whereas the mean depends on the inputs of the unit : for a unit y with inputs z and inverse variance d123 , e ( yjz ) = a ( z ) the contrastive divergence updates are easily obtained by computing the derivative of the energy with respect to the parameters .
for the parameters in the linear terms of the energy function ( e . g . , b and w above ) , the derivatives have the same form ( input unit value times output unit value ) as for the case of binomial units .
for quadratic parameter d > 123 , the derivative is simply 123dy 123
gaussian units were previously used as hidden units of an rbm ( with binomial or multinomial inputs ) applied to an information retrieval task ( welling , rosen - zvi , & hinton , 123 ) .
our interest here is to use them for using continuous - valued hidden units although we have introduced rbm units with continuous values to better deal with the representa - tion of input variables , they could also be considered for use in the hidden layers , in replacement or complementing the binomial units which have been used in the past .
however , gaussian and expo - nential hidden units have a weakness : the mean - ( cid : 123 ) eld propagation through a gaussian unit gives rise to a purely linear transformation .
hence if we have only such linear hidden units in a multi - layered network , the mean - ( cid : 123 ) eld propagation function that maps inputs to internal representations would be completely linear .
in addition , in a dbn containing only gaussian units , one would only be able to model gaussian data .
on the other hand , combining gaussian with other types of units could be interesting .
in contrast with gaussian or exponential units , remark that the conditional expectation of truncated exponential units is non - linear , and in fact involves a sigmoidal form of non - linearity applied to the weighted sum of its inputs .
this experiment was performed on two data sets : the uci repository abalone data set ( split in 123 training examples , 123 validation examples , 123 test examples ) and a ( cid : 123 ) nancial data set .
the latter has real - valued input variables representing averages of returns and squared returns for which the bino - mial approximation would seem inappropriate .
the target variable is next months return of a cotton futures contract .
there are 123 continuous input variables , that are averages of returns over different time - windows up to 123 days .
there are 123 training examples , 123 validation examples , and 123 test examples .
the dataset is publicly available at http : / / www . iro . umontreal . ca / ( cid : 123 ) lisa / fin_data / .
in table 123 ( rows 123 and 123 ) , we show improvements brought by dbns with gaussian inputs over dbns with binomial inputs ( with binomial hidden units in both cases ) .
the networks have two hidden layers .
all hyper - parameters are selected based on validation set performance .
123 understanding why the layer - wise strategy works a reasonable explanation for the apparent success of the layer - wise training strategy for dbns is that unsupervised pre - training helps to mitigate the dif ( cid : 123 ) cult optimization problem of deep networks by better initializing the weights of all layers .
here we present experiments that support and clarify this .
training each layer as an auto - encoder we want to verify that the layer - wise greedy unsupervised pre - training principle can be applied when using an auto - encoder instead of the rbm as a layer building block .
let x be the input vector with xi 123 ( 123; 123 ) .
for a layer with weights matrix w , hidden biases column vector b and input biases column vector c , the reconstruction probability for bit i is pi ( x ) , with the vector of proba - bilities p ( x ) = sigm ( c + w sigm ( b + w 123x ) ) : the training criterion for the layer is the average of negative log - likelihoods for predicting x from p ( x ) .
for example , if x is interpreted either as a sequence of bits or a sequence of bit probabilities , we minimize the reconstruction cross - entropy :
r = ( cid : 123 ) pi xi log pi ( x ) + ( 123 ( cid : 123 ) xi ) log ( 123 ( cid : 123 ) pi ( x ) ) : we report several experimental results using this
training criterion for each layer , in comparison to the contrastive divergence algorithm for an rbm .
pseudo - code for a deep network obtained by training each layer as an auto - encoder is given in ap - pendix ( algorithm traingreedyautoencodingdeepnet ) .
one question that arises with auto - encoders in comparison with rbms is whether the auto - encoders will fail to learn a useful representation when the number of units is not strictly decreasing from one layer to the next ( since the networks could theoretically just learn to be the identity and perfectly min - imize the reconstruction error ) .
however , our experiments suggest that networks with non - decreasing layer sizes generalize well .
this might be due to weight decay and stochastic gradient descent , prevent - ing large weights : optimization falls in a local minimum which corresponds to a good transformation of the input ( that provides a good initialization for supervised training of the whole net ) .
greedy layer - wise supervised training a reasonable question to ask is whether the fact that each layer is trained in an unsupervised way is critical or not .
an alternative algorithm is supervised , greedy and layer - wise : train each new hidden layer as the hidden layer of a one - hidden layer supervised neural network nn ( taking as input the output of the last of previously trained layers ) , and then throw away the output layer of nn and use the parameters of the hidden layer of nn as pre - training initialization of the new top layer of the deep net , to map the output of the previous layers to a hopefully better representation .
pseudo - code for a deep network obtained by training each layer as the hidden layer of a supervised one - hidden - layer neural network is given in appendix ( algorithm traingreedysuperviseddeepnet ) .
we compared the performance on the mnist digit classi ( cid : 123 ) cation task obtained with ( cid : 123 ) ve algorithms : ( a ) dbn , ( b ) deep network whose layers are initialized as auto - encoders , ( c ) above described su - pervised greedy layer - wise algorithm to pre - train each layer , ( d ) deep network with no pre - training ( random initialization ) , ( e ) shallow network ( 123 hidden layer ) with no pre - training .
the ( cid : 123 ) nal ( cid : 123 ) ne - tuning is done by adding a logistic regression layer on top of the network and train - ing the whole network by stochastic gradient descent on the cross - entropy with respect to the target classi ( cid : 123 ) cation .
the networks have the following architecture : 123 inputs , 123 outputs , 123 hidden layers with variable number of hidden units , selected by validation set performance ( typically selected layer sizes are between 123 and 123 ) .
the shallow network has a single hidden layer .
an l123 weight decay hyper - parameter is also optimized .
the dbn was slower to train and less experiments were performed , so that longer training and more appropriately chosen sizes of layers and learning rates could yield better results ( hinton 123 , unpublished , reports 123% error on the mnist test set ) .
dbn , unsupervised pre - training deep net , auto - associator pre - training deep net , supervised pre - training deep net , no pre - training shallow net , no pre - training
123% 123% 123% 123% 123% 123% 123% 123% 123%
. 123% 123% 123% . 123% 123% 123% . 123% 123% 123% 123% 123% 123%
table 123 : classi ( cid : 123 ) cation error on mnist training , validation , and test sets , with the best hyper - on mnist , differences of more than . 123% are statistically signi ( cid : 123 ) cant .
the results in table 123 suggest that the auto - encoding criterion can yield performance comparable to the dbn when the layers are ( cid : 123 ) nally tuned in a supervised fashion .
they also clearly show that the greedy unsupervised layer - wise pre - training gives much better results than the standard way to train a deep network ( with no greedy
pre - training ) or a shallow network , and that , without pre - training , deep networks tend to perform worse than shallow networks .
the results also suggest that unsupervised greedy layer - wise pre - training can perform signi ( cid : 123 ) cantly better than purely supervised greedy layer - wise pre - training .
a possible expla - nation is that the greedy supervised procedure is too greedy : in the learned hidden units representation it may discard some of the information about the target , information that cannot be captured easily by a one - hidden - layer neural network but could be captured by composing more hidden layers .
however , there is something troubling in the experiment 123 results ( table 123 ) : all the networks , even those without greedy layer - wise pre - training , perform almost perfectly on the training set , which would appear to contradict the hypothesis that the main effect of the layer - wise greedy strategy is to help the optimization ( with poor optimization one would expect poor training error ) .
a possible explanation coherent with our initial hypothesis and with the above results is captured by the following hypothesis .
without pre - training , the lower layers are initialized poorly , but still allowing the top two layers to learn the training set almost perfectly , because the output layer and the last hidden layer form a standard shallow but fat neural network .
consider the top two layers of the deep network with pre - training : it presumably takes as input a better representation , one that allows for better generalization .
instead , the network without pre - training sees a ( cid : 123 ) random ( cid : 123 ) transformation of the input , one that preserves enough information about the input to ( cid : 123 ) t the training set , but that does not help to generalize .
to test that hypothesis , we performed a second series of experiments in which we constrain the top hidden layer to be small ( 123 hidden units ) .
the experiment 123 results ( table 123 ) clearly con ( cid : 123 ) rm our hypothesis .
with no pre - training , training error degrades signi ( cid : 123 ) cantly when there are only 123 hidden units in the top hidden layer .
in addition , the results obtained without pre - training were found to have extremely large variance indicating high sensitivity to initial conditions .
overall , the results in the tables and in figure 123 are consistent with the hypothesis that the greedy layer - wise procedure essentially helps to better optimize the deep networks , probably by initializing the hidden layers so that they represent more meaningful representations of the input , which also yields to better generalization .
continuous training of all layers of a dbn with the layer - wise training algorithm for dbns ( trainunsuperviseddbn in appendix ) , one element that we would like to dispense with is having to decide the number of training iterations for each layer .
it would be good if we did not have to explicitly add layers one at a time , i . e . , if we could train all layers simultaneously , but keeping the ( cid : 123 ) greedy ( cid : 123 ) idea that each layer is pre - trained to model its input , ignoring the effect of higher layers .
to achieve this it is suf ( cid : 123 ) cient to insert a line in trainunsuperviseddbn , so that rbmupdate is called on all the layers and the stochastic hidden values are propagated all the way up .
experiments with this variant demonstrated that it works at least as well as the original algorithm .
the advantage is that we can now have a single stopping criterion ( for the whole network ) .
computation time is slightly greater , since we do more computations initially ( on the upper layers ) , which might be wasted ( before the lower layers converge to a decent representation ) , but time is saved on optimizing hyper - parameters .
this variant may be more appealing for on - line training on very large data - sets , where one would never cycle back on the training data .
123 dealing with uncooperative input distributions in classi ( cid : 123 ) cation problems such as mnist where classes are well separated , the structure of the input distribution p ( x ) naturally contains much information about the target variable y .
imagine a super - vised learning task in which the input distribution is mostly unrelated with y .
in regression problems , which we are interested in studying here , this problem could be much more prevalent .
for example imagine a task in which x ( cid : 123 ) p ( x ) and the target y = f ( x ) + noise ( e . g . , p is gaussian and f = sinus ) with no particular relation between p and f .
in such settings we cannot expect the unsupervised greedy layer - wise pre - training procedure to help in training deep supervised networks .
to deal with such uncooperative input distributions , we propose to train each layer with a mixed training criterion that combines the unsupervised objective ( modeling or reconstructing the input ) and a supervised ob - jective ( helping to predict the target ) .
a simple algorithm thus adds the updates on the hidden layer weights from the unsupervised algorithm ( contrastive divergence or reconstruction error gradient ) with the updates from the gradient on a supervised prediction error , using a temporary output layer , as with the greedy layer - wise supervised training algorithm .
in our experiments it appeared suf ( cid : 123 ) cient to perform that partial supervision with the ( cid : 123 ) rst layer only , since once the predictive information about the target is ( cid : 123 ) forced ( cid : 123 ) into the representation of the ( cid : 123 ) rst layer , it tends to stay in the upper layers .
the results in figure 123 and table 123 clearly show the advantage of this partially supervised greedy training
