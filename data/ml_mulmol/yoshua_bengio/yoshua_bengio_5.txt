this paper studies the convergence properties of the well known k - means clustering algorithm .
the k - means algorithm can be de ( cid : 123 ) scribed either as a gradient descent algorithm or by slightly extend ( cid : 123 ) ing the mathematics of the em algorithm to this hard threshold case .
we show that the k - means algorithm actually minimizes the quantization error using the very fast newton algorithm .
k - means is a popular clustering algorithm used in many applications , including the initialization of more computationally expensive algorithms ( gaussian mixtures , radial basis functions , learning vector quantization and some hidden markov models ) .
the practice of this initialization procedure often gives the frustrating feeling that k - means performs most of the task in a small fraction of the overall time .
this motivated us to better understand this convergence speed .
a second reason lies in the traditional debate between hard threshold ( e . g .
k ( cid : 123 ) means , viterbi training ) and soft threshold ( e . g .
gaussian mixtures , baum welch ) algorithms ( nowlan , 123 ) .
soft threshold algorithms are often preferred because they have an elegant probabilistic framework and a general optimization algorithm named em ( expectation - maximization ) ( dempster , laird and rubin , 123 ) .
in the case of a gaussian mixture , the em algorithm has recently been shown to approxi ( cid : 123 ) mate the newton optimization algorithm ( xu and jordan , 123 ) .
we prove in this
" also , at&t bell labs , holmdel , nj 123
leon bottou , yoshua bengio
paper that the corresponding hard threshold algorithm ) k - means ) minimizes the quantization error using exactly the newton algorithm .
in the next section ) we derive the k - means algorithm as a gradient descent pro ( cid : 123 ) cedure .
section 123 extends the mathematics of the em algorithm to the case of k - means .
this second derivation of k - means provides us with proper values for the learning rates .
in section 123 we show that this choice of learning rates opti ( cid : 123 ) mally rescales the parameter space using newton ) s method .
finally ) in section 123 we present and discuss a few experimental results comparing various versions of the k - means algorithm .
the 123 clustering algorithms presented here were chosen for a good coverage of the algorithms related to k - means ) but this paper does not have the ambition of presenting a literature survey on the subject .
123 k - means as a gradient descent given a set of p examples ( xi ) the k - means algorithm computes k prototypes w = ( wk ) which minimize the quantization error , i . e . ) the average distance between each pattern and the closest prototype :
e w = l . . . . j l xi ) w = l . . . . j - mln xi - wk )
) def " " ' (
) def " " ' 123
writing si ( w ) for the subscript of the closest prototype to example xi , we have
123 gradient descent algorithm
we can then derive a gradient descent algorithm for the quantization error : ~ w = - ( t 123 ~ ; : ) .
this leads to the following batch update equation ( updating pro ( cid : 123 ) totypes after presenting all the examples ) :
we can also derive a corresponding online algorithm which updates the prototypes after the presentation of each pattern xi :
u . w - - ( t
if k = si ( w )
the proper value of the learning rate ( t remain to be specified in both batch and online algorithms .
convergence proofs for both algorithms ( bottou ) 123 ) exist for decreasing values of the learning rates satisfying the conditions e ( t = 123 and e ( ; < 123
following ( kohonen ) 123 ) , we could choose ( t = ( o / t .
we prove however in this paper that there exist a much better choice of learning rates .
convergence properties of the k - means algorithms
123 k - means as an em style algorithm 123 em style algorithm the following derivation of k - means is similar to the derivation of ( macqueen , 123 ) .
we insist however on the identity between this derivation and the mathe ( cid : 123 ) matics of em ( liporace , 123 ) ( dempster , laird and rubin , 123 ) .
although ( ( - means does not fit in a probabilistic framework , this similarity holds for a very deep reason : the semi - ring of probabilies ( ! r+ , + , x ) and the idempo ( cid : 123 ) tent semi - ring of hard - threshold scores ( ! r , min , + ) share the most significant al ( cid : 123 ) gebraic properties ( bacceli , cohen and olsder , 123 ) .
this assertion completely describes the similarities and the potential differences between soft - threshold and hard - threshold algorithms .
a complete discussion however stands outside the scope of this paper .
the principle of em is to introduce additional " hidden " variables whose knowledge would make the optimization problem easier .
since these hidden variables are un ( cid : 123 ) known , we maximize an auxiliary function which averages over the values of the hidden variables given the values of the parameters at the previous iteration .
in our case , the hidden variables are .
the assignments si ( w ) of the patterns to the pro ( cid : 123 ) totypes .
instead of considering the expected value over the distribution on these hidden variables , we just consider the values of the hidden variables that minimize the cost , given the previous values of the parameters :
w , w = l . . . j ' 123 xi - w & . ( w )
) def ' " ' 123 (
the next step consists then in finding a new set of prototypes wi which mllll ( cid : 123 ) mizes q ( wi , w ) where w is the previous set of prototypes .
we can analytically compute the explicit solution of this minimization problem .
solving the equation 123q ( w ' , w ) / 123w ' k = 123 yields :
wk = 123
where nk is the number of examples assigned to prototype wk .
the algorithm consists in repeatedly replacing w by wi using update equation ( 123 ) until convergence .
since si ( w ' ) is by definition the best assignment of patterns xi to the prototypes w ~ , we have the following inequality :
e ( w ' ) - q ( w ' , w ) = ~ l ( xi - wi & . ( w , ) ) 123 - ( xi - wi si ( w ) ) 123 : : ; 123
using this result , the identity e ( w ) = q ( w , w ) and the definition of wi , we can derive the following inequality :
e ( w ' ) - e ( w )
e ( w ' ) - q ( w ' , w ) + q ( w ' , w ) - q ( w , w )
< q ( w ' , w ) - q ( w , w )
each iteration of the algorithm thus decreases the otherwise positive quantization error e ( equation 123 ) until the error reaches a fixed point where condition w* ' = w* is verified ( unicity of the minimum of q ( . , w* ) ) .
since the assignment functions si ( w ) are discrete , there is an open neighborhood of w* on which the assignments are constant .
according to their definition , functions e ( . ) and q ( . , w* ) are equal on this neighborhood .
being the minimum of function q ( . , w* ) , the fixed point w* of this algorithm is also a local minimum of the quantization error e .
leon bottou , yoshua bengio
123 batch k - means the above algorithm ( 123 ) can be rewritten in a form similar to that of the batch gradient descent algorithm ( 123 ) .
if k = s ( xi ' w )
this algorithm is thus equivalent to a batch gradient descent with a specific , pro ( cid : 123 ) totype dependent , learning rate jk
123 online k - means the online version of our em style update equation ( 123 ) is based on the computation of the mean flt of the examples xl , . . .
, xt with the following recursive formula :
flt+l = t ~ l ( t flt + xt+l ) = flt + t ~ l ( xt+l -
let us introduce new variables nk which count the number of examples so far assigned to prototype wk .
we can then rewrite ( 123 ) as an online update applied after the presentation of each pattern xi :
if k = s ( xi ' w )
( n\ ( xi - wk )
if k = s ( xi ' w )
this algorithm is equivalent to an online gradient descent ( 123 ) with a specific , proto ( cid : 123 ) type dependent , learning rate nlk .
unlike in the batch case , the pattern assignments s ( xi , w ) are thus changing after each pattern presentation .
before applying this al ( cid : 123 ) gorithm , we must of course set nk to zero and wk to some initial value .
various methods have been proposed including initializing wk with the first k patterns .
general convergence proofs for the batch and online gradient descent ( bottou , 123; driancourt , 123 ) directly apply for all four algorithms .
although the derivatives are undefined on a few points , these theorems prove that the algorithms almost surely converge to a local minimum because the local variations of the loss function are conveniently bounded ( semi - differentiability ) .
unlike previous results , the above convergence proofs allow for non - linearity , non - differentiability ( on a few points ) ( bottou , 123 ) , and replacing learning rates by a positive definite matrix ( drian ( cid : 123 )
123 k - means as a newton optimization we prove in this section that batch k - means ( 123 ) applies the newton algorithm .
123 the hessian of k - means let us compute the hessian h of the k - means cost function ( 123 ) .
this matrix contains the second derivatives of the cost e ( w ) with respect to each pair of pa ( cid : 123 ) rameters .
since e ( w ) is a sum of terms l ( xi ' w ) , we can decompose h as the sum
convergence properties of the k - means algorithms
of matrices hi for each term of the cost function :
l ( xi ' w ) = ~ in ~ ( xi - wk ) 123
furthermore , the hi can be decomposed in blocks corresponding to each pair of prototypes .
since l ( xi , w ) depends only on the closest prototype to pattern xi , all these blocks are zero except block ( si ( w ) , si ( w ) ) which is the identity matrix .
sum ( cid : 123 ) ming the partial hessian matrices hi thus gives a diagonal matrix whose diagonal elements are the counts of examples nk assigned to each prototype .
we can thus write the newton update of the parameters as follows :
~ w = _h - 123 oe ( w )
which can be exactly rewritten as the batch em style algorithm ( 123 ) presented earlier :
~ w = " ( ~ k ( xi - wk )
if k = ~ ( xi ' w )
123 convergence speed when optimizing a quadratic function , newton ' s algorithm requires only one step .
in the case of a non quadratic function , newton ' s algorithm is superlinear if we can bound the variations of the second derivatives .
standard theorems that bound this variation using the third derivative are not useful for k - means because the gradient of the cost function is discontinuous .
we could notice that the variations of the second derivatives are however nicely bounded and derive similar proofs for for the sake of brevity however , we are just giving here an intuitive argument : we can make the cost function indefinitely differentiable by rounding up the angles around the non differentiable points .
we can even restrict this cost function change to an arbitrary small region of the space .
the iterations of k - means will avoid this region with a probability arbitrarily close to 123
in practice , we obtain thus a batch k - means thus searches for the optimal prototypes at newton speed .
once it comes close enough to the optimal prototypes ( i . e .
the pattern assignment is optimal and the cost function becomes quadratic ) , k - means jumps to the optimum online k - means benefits of these optimal learning rates because they remove the usual conditioning problems of the optimization .
however , the stochastic noise induced by the online procedure limits the final convergence of the algorithm .
final convergence speed is thus essentially determined by the schedule of the learning online k - means also benefits from the redundancies of the training set .
it converges significantly faster than batch k - means during the first training epochs ( darken
leon bottou , yoshua bengio
123 123 123 123 123 123 123 123 123 123 123 123
figure 123 : et - eoo versus t .
black circles : online k - means; black squares : batch k - means; empty circles : online gradient; empty squares : batch gradient; no mark : em +gaussian mixture
and moody , 123 ) .
after going through the first few patterns ( depending of the amount of redundancy ) , online k - means indeed improves the prototypes as much as a complete batch k - means epoch .
other researchers have compared batch and online algorithms for neural networks , with similar conclusions ( bengio , 123 ) .
experiments have been carried out with fisher ' s iris data set , which is composed of 123 points in a four dimensional space representing physical measurements on var ( cid : 123 ) ious species of iris flowers .
codebooks of six prototypes have been computed using both batch and online k - means with the proper learning rates ( 123 ) and ( 123 ) .
these results are compared with those obtained using both gradient descent algorithms ( 123 ) and ( 123 ) using learning rate ft = 123 / t that we have found optimal .
results are also compared with likelihood maximization with the em algorithm , applied to a mixture of six gaussians , with fixed and uniform mixture weights , and fixed unit variance .
inputs were scaled down empirically so that the average cluster variance was around unity .
thus only the cluster positions were learned , as for the k - means each run of an algorithm consists in ( a ) selecting a random initial set of prototypes , ( b ) running the algorithm during 123 epochs and recording the error measure e t after each epoch , ( c ) running the batch k - means algorithm ! during 123 more epochs in order to locate the local minimum eoo corresponding to the current initialization of the algorithm .
for the four k - means algorithms , e t is the quantization error ( equation 123 ) .
for the gaussian mixture trained with em , the cost et is the negative
123 except for the case of the mixture of gaussians , in which the em algorithm was applied
convergence properties of the k - means algorithms
123 123 123 123 123 u 123 123 123 123 123
figure 123 : e ~ - ; ~ oo versus t .
black circles : online k - means; black squares : batch k - means; empty circles : online gradient; empty squares : batch gradient; no mark :
logarithm of the likelihood of the data given the model .
twenty trials were run for each algorithm .
using more than twenty runs did not improve the standard deviation of the averaged measures because various initializa ( cid : 123 ) tions lead to very different local minima .
the value eoo of the quantization error on the local minima ranges between 123 and 123
this variability is caused by the different initializations and not by the different algorithms .
the average values of eoo for each algorithm indeed fall in a very small range ( 123 to 123 ) .
figure 123 shows the average value of the residual error et - eoo during the first 123 epochs .
online k - means ( black circles ) outperforms all other algorithms during the first five epochs and stabilizes on a level related to the stochastic noise of the online procedure .
batch k - means ( black squares ) initially converges more slowly but outperforms all other methods after 123 epochs .
all 123 runs converged before the 123th epoch .
both gradients algorithms display poor convergence because they do not benefit of the newton effect .
again , the online version ( white circles ) starts faster then the batch version ( white square ) but is outperformed in the long run .
the negative logarithm of the gaussian mixture is shown on the curve with no point marks , and the scale is displayed on the right of figure 123
figure 123 show the final convergence properties of all five algorithms .
the evolutions of the ratio ( et+l - eoo ) / ( et - eoo ) characterize the relative improvement of the residual error after each iteration .
all algorithms exhibit the same behavior after a few epochs except batch k - means ( black squares ) .
the fast convergence ofthis ratio to zero demonstrates the final convergence of batch k - means .
the em algorithm displays a better behavior than all the other algorithms except batch k - means .
clearly , however , its relative improvement ratio doesn ' t display the fast convergence behavior of batch k - means .
uon bottou , yoshua bengio
the online k - means curve crosses the batch k - means curve during the second epoch , suggesting that it is better to run the online algorithm ( 123 ) during one epoch and then switch to the batch algorithm ( 123 ) .
we have shown with theoretical arguments and simple experiments that a well implemented k - means algorithm minimizes the quantization error using newton ' s algorithm .
the em style derivation of k - means shows that the mathematics of em are valid well outside the framework of probabilistic models .
moreover the provable convergence properties of the hard threshold k - means algorithm are superior to those of the em algorithm for an equivalent soft threshold mixture of gaussians .
extending these results to other hard threshold algorithms ( e . g .
viterbi training ) is an interesting open question .
