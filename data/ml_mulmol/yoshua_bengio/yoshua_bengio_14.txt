while logistic sigmoid neurons are more bi - ologically plausible than hyperbolic tangent neurons , the latter work better for train - ing multi - layer neural networks .
this pa - per shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hy - perbolic tangent networks in spite of the hard non - linearity and non - dierentiability at zero , creating sparse representations with true zeros , which seem remarkably suitable for naturally sparse data .
even though they can take advantage of semi - supervised setups with extra - unlabeled data , deep rectier net - works can reach their best performance with - out requiring any unsupervised pre - training on purely supervised tasks with large labeled datasets .
hence , these results can be seen as a new milestone in the attempts at under - standing the diculty in training deep but purely supervised neural networks , and clos - ing the performance gap between neural net - works learnt with and without unsupervised
many dierences exist between the neural network models used by machine learning researchers and those used by computational neuroscientists .
this is in part
appearing in proceedings of the 123th international con - ference on articial intelligence and statistics ( aistats ) 123 , fort lauderdale , fl , usa .
volume 123 of jmlr : w&cp 123
copyright 123 by the authors .
because the objective of the former is to obtain com - putationally ecient learners , that generalize well to new examples , whereas the objective of the latter is to abstract out neuroscientic data while obtaining ex - planations of the principles involved , providing predic - tions and guidance for future biological experiments .
areas where both objectives coincide are therefore particularly worthy of investigation , pointing towards computationally motivated principles of operation in the brain that can also enhance research in articial in this paper we show that two com - mon gaps between computational neuroscience models and machine learning neural network models can be bridged by using the following linear by part activa - tion : max ( 123 , x ) , called the rectier ( or hinge ) activa - tion function .
experimental results will show engaging training behavior of this activation function , especially for deep architectures ( see bengio ( 123 ) for a review ) , i . e . , where the number of hidden layers in the neural network is 123 or more .
recent theoretical and empirical work in statistical machine learning has demonstrated the importance of learning algorithms for deep architectures .
this is in part inspired by observations of the mammalian vi - sual cortex , which consists of a chain of processing elements , each of which is associated with a dierent representation of the raw visual input .
this is partic - ularly clear in the primate visual system ( serre et al . , 123 ) , with its sequence of processing stages : detection of edges , primitive shapes , and moving up to gradu - ally more complex visual shapes .
interestingly , it was found that the features learned in deep architectures resemble those observed in the rst two of these stages ( in areas v123 and v123 of visual cortex ) ( lee et al . , 123 ) , and that they become increasingly invariant to factors of variation ( such as camera movement ) in higher lay - ers ( goodfellow et al . , 123 ) .
123 deep sparse rectier neural networks
regarding the training of deep networks , something that can be considered a breakthrough happened in 123 , with the introduction of deep belief net - works ( hinton et al . , 123 ) , and more generally the idea of initializing each layer by unsupervised learn - ing ( bengio et al . , 123; ranzato et al . , 123 ) .
some authors have tried to understand why this unsuper - vised procedure helps ( erhan et al . , 123 ) while oth - ers investigated why the original training procedure for deep neural networks failed ( bengio and glorot , 123 ) .
from the machine learning point of view , this paper brings additional results in these lines of investigation .
we propose to explore the use of rectifying non - linearities as alternatives to the hyperbolic tangent or sigmoid in deep articial neural networks , in ad - dition to using an l123 regularizer on the activation val - ues to promote sparsity and prevent potential numer - ical problems with unbounded activation .
nair and hinton ( 123 ) present promising results of the inu - ence of such units in the context of restricted boltz - mann machines compared to logistic sigmoid activa - tions on image classication tasks .
our work extends this for the case of pre - training using denoising auto - encoders ( vincent et al . , 123 ) and provides an exten - sive empirical comparison of the rectifying activation function against the hyperbolic tangent on image clas - sication benchmarks as well as an original derivation for the text application of sentiment analysis .
our experiments on image and text data indicate that training proceeds better when the articial neurons are either o or operating mostly in a linear regime .
sur - prisingly , rectifying activation allows deep networks to achieve their best performance without unsupervised pre - training .
hence , our work proposes a new contri - bution to the trend of understanding and merging the performance gap between deep networks learnt with and without unsupervised pre - training ( erhan et al . , 123; bengio and glorot , 123 ) .
still , rectier net - works can benet from unsupervised pre - training in the context of semi - supervised learning where large amounts of unlabeled data are provided .
furthermore , as rectier units naturally lead to sparse networks and are closer to biological neurons responses in their main operating regime , this work also bridges ( in part ) a machine learning / neuroscience gap in terms of acti - vation function and sparsity .
this paper is organized as follows .
section 123 presents some neuroscience and machine learning background which inspired this work .
section 123 introduces recti - er neurons and explains their potential benets and drawbacks in deep networks .
then we propose an experimental study with empirical results on image recognition in section 123 and sentiment analysis in section 123 .
section 123 presents our conclusions .
123 neuroscience observations
for models of biological neurons , the activation func - tion is the expected ring rate as a function of the total input currently arising out of incoming signals at synapses ( dayan and abott , 123 ) .
an activation function is termed , respectively antisymmetric or sym - metric when its response to the opposite of a strongly excitatory input pattern is respectively a strongly in - hibitory or excitatory one , and one - sided when this response is zero .
the main gaps that we wish to con - sider between computational neuroscience models and machine learning models are the following :
studies on brain energy expense suggest that neurons encode information in a sparse and dis - tributed way ( attwell and laughlin , 123 ) , esti - mating the percentage of neurons active at the same time to be between 123 and 123% ( lennie , 123 ) .
this corresponds to a trade - o between richness of representation and small action potential en - ergy expenditure .
without additional regulariza - tion , such as an l123 penalty , ordinary feedforward neural nets do not have this property .
for ex - ample , the sigmoid activation has a steady state 123 , therefore , after initializing with regime around 123 small weights , all neurons re at half their satura - tion regime .
this is biologically implausible and hurts gradient - based optimization ( lecun et al . , 123; bengio and glorot , 123 ) .
important divergences between biological and a common biological model of neuron , the leaky integrate - and - re ( or lif ) ( dayan and abott , 123 ) , gives the follow - ing relation between the ring rate and the input current , illustrated in figure 123 ( left ) :
if e + ri > vth if e + ri vth
where tref is the refractory period ( minimal time between two action potentials ) , i the input cur - rent , vr the resting potential and vth the thresh - old potential ( with vth > vr ) , and r , e , the membrane resistance , potential and time con - stant .
the most commonly used activation func - tions in the deep learning and neural networks lit - erature are the standard logistic sigmoid and the
123 xavier glorot , antoine bordes , yoshua bengio
figure 123 : left : common neural activation function motivated by biological data .
right : commonly used activation functions in neural networks literature : logistic sigmoid and hyperbolic tangent ( tanh ) .
hyperbolic tangent ( see figure 123 , right ) , which are equivalent up to a linear transformation .
the hy - perbolic tangent has a steady state at 123 , and is therefore preferred from the optimization stand - point ( lecun et al . , 123; bengio and glorot , 123 ) , but it forces an antisymmetry around 123 which is absent in biological neurons .
123 advantages of sparsity
sparsity has become a concept of interest , not only in computational neuroscience and machine learning but also in statistics and signal processing ( candes and tao , 123 ) .
it was rst introduced in computational neuroscience in the context of sparse coding in the vi - sual system ( olshausen and field , 123 ) .
it has been a key element of deep convolutional networks exploit - ing a variant of auto - encoders ( ranzato et al . , 123 , 123; mairal et al . , 123 ) with a sparse distributed representation , and has also become a key ingredient in deep belief networks ( lee et al . , 123 ) .
a sparsity penalty has been used in several computational neuro - science ( olshausen and field , 123; doi et al . , 123 ) and machine learning models ( lee et al . , 123; mairal et al . , 123 ) , in particular for deep architectures ( lee et al . , 123; ranzato et al . , 123 , 123 ) .
however , in the latter , the neurons end up taking small but non - zero activation or ring probability .
we show here that using a rectifying non - linearity gives rise to real zeros of activations and thus truly sparse representations .
from a computational point of view , such representa - tions are appealing for the following reasons :
of deep learning
to disentangle the factors explaining the variations in the data .
a dense representation is highly entangled because almost any change in the input modies most of
the entries in the representation vector .
instead , if a representation is both sparse and robust to small input changes , the set of non - zero features is almost always roughly conserved by small changes of the input .
ecient variable - size representation .
dif - ferent inputs may contain dierent amounts of in - formation and would be more conveniently repre - sented using a variable - size data - structure , which is common in computer representations of infor - mation .
varying the number of active neurons allows a model to control the eective dimension - ality of the representation for a given input and the required precision .
linear separability .
sparse representations are also more likely to be linearly separable , or more easily separable with less non - linear machinery , simply because the information is represented in a high - dimensional space .
besides , this can reect the original data format .
in text - related applica - tions for instance , the original raw data is already very sparse ( see section 123 ) .
distributed but sparse .
dense distributed rep - resentations are the richest representations , be - ing potentially exponentially more ecient than purely local ones ( bengio , 123 ) .
sparse repre - sentations eciency is still exponentially greater , with the power of the exponent being the number of non - zero features .
they may represent a good trade - o with respect to the above criteria .
nevertheless , forcing too much sparsity may hurt pre - dictive performance for an equal number of neurons , because it reduces the eective capacity of the model .
123 deep sparse rectier neural networks
figure 123 : left : sparse propagation of activations and gradients in a network of rectier units .
the input selects a subset of active neurons and computation is linear in this subset .
right : rectier and softplus activation functions .
the second one is a smooth version of the rst .
123 deep rectier networks
123 rectier neurons
the neuroscience literature ( bush and sejnowski , 123; douglas and al . , 123 ) indicates that corti - cal neurons are rarely in their maximum saturation regime , and suggests that their activation function can be approximated by a rectier .
most previous stud - ies of neural networks involving a rectifying activation function concern recurrent networks ( salinas and ab - bott , 123; hahnloser , 123 ) .
the rectier function rectier ( x ) = max ( 123 , x ) is one - sided and therefore does not enforce a sign symmetry123 or antisymmetry123 : instead , the response to the oppo - site of an excitatory input pattern is 123 ( no response ) .
however , we can obtain symmetry or antisymmetry by combining two rectier units sharing parameters .
advantages the rectier activation function allows a network to easily obtain sparse representations .
for example , after uniform initialization of the weights , around 123% of hidden units continuous output val - ues are real zeros , and this fraction can easily increase with sparsity - inducing regularization .
apart from be - ing more biologically plausible , sparsity also leads to mathematical advantages ( see previous section ) .
as illustrated in figure 123 ( left ) , the only non - linearity in the network comes from the path selection associ - ated with individual neurons being active or not .
for a given input only a subset of neurons are active .
com - putation is linear on this subset : once this subset of neurons is selected , the output is a linear function of
123the hyperbolic tangent absolute value non - linearity | tanh ( x ) | used by jarrett et al .
( 123 ) enforces sign symme - try .
a tanh ( x ) non - linearity enforces sign antisymmetry .
the input ( although a large enough change can trigger a discrete change of the active set of neurons ) .
the function computed by each neuron or by the network output in terms of the network input is thus linear by parts .
we can see the model as an exponential num - ber of linear models that share parameters ( nair and hinton , 123 ) .
because of this linearity , gradients ow well on the active paths of neurons ( there is no gra - dient vanishing eect due to activation non - linearities of sigmoid or tanh units ) , and mathematical investi - gation is easier .
computations are also cheaper : there is no need for computing the exponential function in activations , and sparsity can be exploited .
potential problems one may hypothesize that the hard saturation at 123 may hurt optimization by block - ing gradient back - propagation .
to evaluate the poten - tial impact of this eect we also investigate the soft - plus activation : softplus ( x ) = log ( 123+ex ) ( dugas et al . , 123 ) , a smooth version of the rectifying non - linearity .
we lose the exact sparsity , but may hope to gain eas - ier training .
however , experimental results ( see sec - tion 123 ) tend to contradict that hypothesis , suggesting that hard zeros can actually help supervised training .
we hypothesize that the hard non - linearities do not hurt so long as the gradient can propagate along some paths , i . e . , that some of the hidden units in each layer are non - zero .
with the credit and blame assigned to these on units rather than distributed more evenly , we hypothesize that optimization is easier .
another prob - lem could arise due to the unbounded behavior of the activations; one may thus want to use a regularizer to prevent potential numerical problems .
therefore , we use the l123 penalty on the activation values , which also promotes additional sparsity .
also recall that , in or - der to eciently represent symmetric / antisymmetric behavior in the data , a rectier network would need
123 xavier glorot , antoine bordes , yoshua bengio
twice as many hidden units as a network of symmet - ric / antisymmetric activation functions .
conditioning of the parametrization .
weights can be scaled in dierent ( and consistent ) ways while preserving the same overall network function .
more precisely , consider for each layer of depth i of the network a scalar i , and scaling the parameters as .
the output units values
then change as follow : s
therefore , as
j=123 j is 123 , the network function is identical .
use a linear activation function for the reconstruc - tion layer , along with a quadratic cost .
we tried to use input unit values either before or after the rectier non - linearity as reconstruction targets .
( for the rst layer , raw inputs are directly used . )
use a rectier activation function for the recon -
struction layer , along with a quadratic cost .
the rst strategy has proven to yield better gener - alization on image data and the second one on text data .
consequently , the following experimental study presents results using those two .
123 unsupervised pre - training
123 experimental study
this paper is particularly inspired by the sparse repre - sentations learned in the context of auto - encoder vari - ants , as they have been found to be very useful in training deep architectures ( bengio , 123 ) , especially for unsupervised pre - training of neural networks ( er - han et al . , 123 ) .
nonetheless , certain diculties arise when one wants to introduce rectier activations into stacked denois - ing auto - encoders ( vincent et al . , 123 ) .
first , the hard saturation below the threshold of the rectier function is not suited for the reconstruction units .
in - deed , whenever the network happens to reconstruct a zero in place of a non - zero target , the reconstruc - tion unit can not backpropagate any gradient . 123 sec - ond , the unbounded behavior of the rectier activation also needs to be taken into account .
in the follow - ing , we denote x the corrupted version of the input x , ( ) the logistic sigmoid function and the model pa - rameters ( wenc , benc , wdec , bdec ) , and dene the linear recontruction function as :
f ( x , ) = wdec max ( wencx + benc , 123 ) + bdec .
here are the several strategies we have experimented :
use a softplus activation function for the recon -
struction layer , along with a quadratic cost : l ( x , ) = ||x log ( 123 + exp ( f ( x , ) ) ) ||123 .
scale the rectier activation values coming from the previous encoding layer to bound them be - tween 123 and 123 , then use a sigmoid activation func - tion for the reconstruction layer , along with a cross - entropy reconstruction cost .
l ( x , ) = x log ( ( f ( x , ) ) )
( 123 x ) log ( 123 ( f ( x , ) ) ) .
123why is this not a problem for hidden layers too ? we hy - pothesize that it is because gradients can still ow through the active ( non - zero ) , possibly helping rather than hurting the assignment of credit .
this section discusses our empirical evaluation of recti - er units for deep networks .
we rst compare them to hyperbolic tangent and softplus activations on image benchmarks with and without pre - training , and then apply them to the text task of sentiment analysis .
123 image recognition
experimental setup we considered the image datasets detailed below .
each of them has a train - ing set ( for tuning parameters ) , a validation set ( for tuning hyper - parameters ) and a test set ( for report - ing generalization performance ) .
they are presented according to their number of training / validation / test examples , their respective image sizes , as well as their number of classes :
mnist ( lecun et al . , 123 ) : 123k / 123k / 123k , 123
123 digit images , 123 classes .
123k / 123k / 123k , 123 123 123 rgb images , 123 classes .
nistp : 123 , 123k / 123k / 123k , 123 123 character im - ages from the nist database 123 , with randomized distortions ( bengio and al , 123 ) , 123 classes .
this dataset is much larger and more dicult than the original nist ( grother , 123 ) .
jittered - cluttered norb ( lecun et al . , 123 ) .
toys on a cluttered background , 123 classes .
the data has been prepro - cessed similarly to ( nair and hinton , 123 ) : we subsampled the original 123 123 123 stereo - pair images to 123 123 123 and scaled linearly the image in the range ( 123 , 123 ) .
we followed the procedure used by nair and hinton ( 123 ) to create the validation set .
123 deep sparse rectier neural networks
table 123 : test error on networks of depth 123
bold results represent statistical equivalence between similar ex - periments , with and without pre - training , under the null hypothesis of the pairwise test with p = 123 .
neuron mnist cifar123 nistp norb
with unsupervised pre - training
123% 123% 123% 123% 123% 123%
without unsupervised pre - training
123% 123% 123%
for all experiments except on the norb data ( le - cun et al . , 123 ) , the models we used are stacked denoising auto - encoders ( vincent et al . , 123 ) with three hidden layers and 123 units per layer .
the ar - chitecture of nair and hinton ( 123 ) has been used on norb : two hidden layers with respectively 123 and 123 units .
we used a cross - entropy reconstruc - tion cost for tanh networks and a quadratic cost over a softplus reconstruction layer for the rectier and softplus networks .
we chose masking noise as the corruption process : each pixel has a probability of 123 of being articially set to 123
the unsuper - vised learning rate is constant , and the following val - ues have been explored : ( . 123 , . 123 , . 123 , . 123 ) .
we se - lect the model with the lowest reconstruction error .
for the supervised ne - tuning we chose a constant learning rate in the same range as the unsupervised learning rate with respect to the supervised valida - tion error .
the training cost is the negative log likeli - hood log p ( correct class|input ) where the probabil - ities are obtained from the output layer ( which imple - ments a softmax logistic regression ) .
we used stochas - tic gradient descent with mini - batches of size 123 for both unsupervised and supervised training phases .
to take into account the potential problem of rectier units not being symmetric around 123 , we use a vari - ant of the activation function for which half of the units output values are multiplied by - 123
this serves to cancel out the mean activation value for each layer and can be interpreted either as inhibitory neurons or simply as a way to equalize activations numerically .
additionally , an l123 penalty on the activations with a coecient of 123 was added to the cost function dur - ing pre - training and ne - tuning in order to increase the amount of sparsity in the learned representations .
main results table 123 summarizes the results on networks of 123 hidden layers of 123 hidden units each ,
figure 123 : inuence of nal sparsity on accu - 123 randomly initialized deep rectier networks were trained on mnist with various l123 penalties ( from 123 to 123 ) to obtain dierent sparsity levels .
results show that enforcing sparsity of the activation does not hurt nal performance until around 123% of true zeros .
comparing all the neuron types123 on all the datasets , with or without unsupervised pre - training .
in the lat - ter case , the supervised training phase has been carried out using the same experimental setup as the one de - scribed above for ne - tuning .
the main observations we make are the following :
despite the hard threshold at 123 , networks trained with the rectier activation function can nd lo - cal minima of greater or equal quality than those obtained with its smooth counterpart , the soft - plus .
on norb , we tested a rescaled version of the softplus dened by 123 sof tplus ( x ) , which allows to interpolate in a smooth manner be - tween the softplus ( = 123 ) and the rectier ( = ) .
we obtained the following / test error cou - ples : 123 / 123% , 123 / 123% , 123 / 123% , 123 / 123% , 123 / 123% , / 123% .
there is no trade - o be - tween those activation functions .
rectiers are not only biologically plausible , they are also com -
there is almost no improvement when using un - supervised pre - training with rectier activations , contrary to what is experienced using tanh or soft - plus .
purely supervised rectier networks remain competitive on all 123 datasets , even against the pretrained tanh or softplus models .
123we also tested a rescaled version of the lif and max ( tanh ( x ) , 123 ) as activation functions .
we obtained worse generalization performance than those of table 123 , and chose not to report them .
123 xavier glorot , antoine bordes , yoshua bengio
rectier networks are truly deep sparse networks .
there is an average exact sparsity ( fraction of ze - ros ) of the hidden layers of 123% on mnist , 123% on cifar123 , 123% on nistp and 123% on norb .
figure 123 provides a better understand - ing of the inuence of sparsity .
it displays the mnist test error of deep rectier networks ( with - out pre - training ) according to dierent average sparsity obtained by varying the l123 penalty on the activations .
networks appear to be quite ro - bust to it as models with 123% to almost 123% of true zeros can achieve similar performances .
with labeled data , deep rectier networks appear to be attractive models .
they are biologically credible , and , compared to their standard counterparts , do not seem to depend as much on unsupervised pre - training , while ultimately yielding sparse representations .
this last conclusion is slightly dierent from those re - ported in ( nair and hinton , 123 ) in which is demon - strated that unsupervised pre - training with restricted boltzmann machines and using rectier units is ben - in particular , the paper reports that pre - trained rectied deep belief networks can achieve a test error on norb below 123% .
however , we be - lieve that our results are compatible with those : we extend the experimental framework to a dierent kind of models ( stacked denoising auto - encoders ) and dif - ferent datasets ( on which conclusions seem to be dier - ent ) .
furthermore , note that our rectied model with - out pre - training on norb is very competitive ( 123% error ) and outperforms the 123% error of the non - pretrained model from nair and hinton ( 123 ) , which is basically what we nd with the non - pretrained soft - plus units ( 123% error ) .
semi - supervised setting figure 123 presents re - sults of semi - supervised experiments conducted on the norb dataset .
we vary the percentage of the orig - inal labeled training set which is used for the super - vised training phase of the rectier and hyperbolic tan - gent networks and evaluate the eect of the unsuper - vised pre - training ( using the whole training set , unla - beled ) .
conrming conclusions of erhan et al .
( 123 ) , the network with hyperbolic tangent activations im - proves with unsupervised pre - training for any labeled set size ( even when all the training set is labeled ) .
however , the picture changes with rectifying activa - in semi - supervised setups ( with few labeled data ) , the pre - training is highly benecial .
but the more the labeled set grows , the closer the models with and without pre - training .
eventually , when all avail - able data is labeled , the two models achieve identical performance .
rectier networks can maximally ex - ploit labeled and unlabeled information .
figure 123 : eect of unsupervised pre - training .
on norb , we compare hyperbolic tangent and rectier net - works , with or without unsupervised pre - training , and ne - tune only on subsets of increasing size of the training set .
123 sentiment analysis nair and hinton ( 123 ) also demonstrated that recti - er units were ecient for image - related tasks .
they mentioned the intensity equivariance property ( i . e .
without bias parameters the network function is lin - early variant to intensity changes in the input ) as ar - gument to explain this observation .
this would sug - gest that rectifying activation is mostly useful to im - age data .
in this section , we investigate on a dierent modality to cast a fresh light on rectier units .
a recent study ( zhou et al . , 123 ) shows that deep be - lief networks with binary units are competitive with the state - of - the - art methods for sentiment analysis .
this indicates that deep learning is appropriate to this text task which seems therefore ideal to observe the behavior of rectier units on a dierent modality , and provide a data point towards the hypothesis that rec - tier nets are particarly appropriate for sparse input vectors , such as found in nlp .
sentiment analysis is a text mining area which aims to determine the judg - ment of a writer with respect to a given topic ( see ( pang and lee , 123 ) for a review ) .
the basic task consists in classifying the polarity of reviews either by predicting whether the expressed opinions are positive or negative , or by assigning them star ratings on either 123 , 123 or 123 star scales .
following a task originally proposed by snyder and barzilay ( 123 ) , our data consists of restaurant reviews which have been extracted from the restaurant review site www . opentable . com .
we have access to 123 , 123 labeled and 123 , 123 unlabeled training reviews , while the test set contains 123 , 123 examples .
the goal is to predict the rating on a 123 star scale and performance is evaluated using root mean squared error ( rmse ) . 123
123even though our tasks are identical , our database is
123 deep sparse rectier neural networks
overpriced , food small portions , not well described on menu .
food quality was good , but way too many avors and textures going on in every single dish .
didnt quite all go together .
calameri was lightly fried and not oilygood jobthey need to learn how to make desserts better as ours was frozen .
the food was wonderful , the service was excellent and it was a very vibrant scene .
only complaint would be that it was a bit noisy .
we had a great time there for mothers day .
our server was great ! attentive , funny and really took care of us !
( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 )
figure 123 : examples of restaurant reviews from www . opentable . com dataset .
the learner must predict the related rating on a 123 star scale ( right column ) .
figure 123 displays some samples of the dataset .
the re - view text is treated as a bag of words and transformed into binary vectors encoding the presence / absence of terms .
for computational reasons , only the 123 most frequent terms of the vocabulary are kept in the fea - ture set . 123 the resulting preprocessed data is very sparse : 123% of non - zero features on average .
un - supervised pre - training of the networks employs both labeled and unlabeled training reviews while the su - pervised ne - tuning phase is carried out by 123 - fold cross - validation on the labeled training examples .
the model are stacked denoising auto - encoders , with 123 or 123 hidden layers of 123 hidden units and rectier or tanh activation , which are trained in a greedy layer - wise fashion .
predicted ratings are dened by the ex - pected star value computed using multiclass ( multino - mial , softmax ) logistic regression output probabilities .
for rectier networks , when a new layer is stacked , ac - tivation values of the previous layer are scaled within the interval ( 123 , 123 ) and a sigmoid reconstruction layer with a cross - entropy cost is used .
we also add an l123 penalty to the cost during pre - training and ne - tuning .
because of the binary input , we use a salt and pepper noise ( i . e .
masking some inputs by zeros and others by ones ) for unsupervised training of the rst layer .
a zero masking ( as in ( vincent et al . , 123 ) ) is used for the higher layers .
we selected the noise level based on the classication performance , other hyperparameters are selected according to the reconstruction error .
tain a rmse lower than 123 .
additionally , although we can not replicate the original very high degree of sparsity of the training data , the 123 - layers network can still attain an overall sparsity of more than 123% .
fi - nally , on data with these particular properties ( binary , high sparsity ) , the 123 - layers network with tanh activa - tion function ( which has been learnt with the exact same pre - training+ne - tuning setup ) is clearly outper - formed .
the sparse behavior of the deep rectier net - work seems particularly suitable in this case , because the raw input is very sparse and varies in its number of non - zeros .
the latter can also be achieved with sparse internal representations , not with dense ones .
since no result has ever been published on the opentable data , we applied our model on the amazon sentiment analysis benchmark ( blitzer et al . , 123 ) in order to assess the quality of our network with respect to literature methods .
this dataset proposes reviews of 123 kinds of amazon products , for which the polarity ( positive or negative ) must be predicted .
we followed the experimental setup dened by zhou et al .
( 123 ) .
in their paper , the best model achieves a test accuracy of 123% ( on average over the 123 kinds of products ) where our 123 - layers rectier network obtains 123% .
table 123 : test rmse and sparsity level obtained by 123 - fold cross - validation on opentable data .
no hidden layer
results are displayed in table 123
rmse signicantly decreases as we add hidden layers to the rectier neural net .
these experiments con - rm that rectier networks improve after an unsuper - vised pre - training phase in a semi - supervised setting : with no pre - training , the 123 - layers model can not ob -
much larger than the one of ( snyder and barzilay , 123 ) .
123preliminary experiments suggested that larger vocab -
ulary sizes did not markedly change results .
sparsity and neurons operating mostly in a linear regime can be brought together in more biologically plausible deep neural networks .
rectier units help to bridge the gap between unsupervised pre - training and no pre - training , which suggests that they may help in nding better minima during training .
this nding has been veried for four image classication datasets of dierent scales and all this in spite of their inherent problems , such as zeros in the gradient , or ill - conditioning of the parametrization .
rather sparse networks are obtained ( from 123 to 123% sparsity for the best generalizing models , whereas the brain is hy - pothesized to have 123% to 123% sparsity ) , which may explain some of the benet of using rectiers .
furthermore , rectier activation functions have shown to be remarkably adapted to sentiment analysis , a text - based task with a very large degree of data spar - sity .
this promising result tends to indicate that deep sparse rectier networks are not only benecial to im - age classication tasks and might yield powerful text mining tools in the future .
123 xavier glorot , antoine bordes , yoshua bengio
