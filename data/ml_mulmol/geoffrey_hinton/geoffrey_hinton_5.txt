given a ^network of simple computing elements and some entities to be represented ,
straightforward scheme is to use one computing element for each entity .
this is called a local representation .
it is easy to understand and easy to implement because the structure of the physical network mirrors the structure of the knowledge it contains .
this report describes a different type of representation that is less familiar and harder to think about than local representations .
each entity is represented by a pattern of activity distributed over many computing elements , and each computing element is involved in representing many different entities .
the strength of this more complicated kind of representation does not lie in its notational convenience or its ease of implementation in a conventional computer , but rather in the efficiency with which it makes use of the processing abilities of networks of simple , neuron - like computing elements .
every representational scheme has its good and bad points .
distributed representations are no exception .
some desirable properties like content - addressable memory and automatic generalization arise very naturally from the use of patterns of activity as representations .
other properties , like the ability to temporarily store a large set of arbitrary associations , are much harder to achieve .
the best psychological evidence for distributed representations is the degree to which their strengths and weaknesses match those of the human mind .
^this research was supported by a grant from the system development foundation .
i thank jim anderson , dave ackley dana ballard , francis crick , scott fahlman , jerry feldman , christopher longuet - higgins , don norman , terry sejnowski , and tim shallice for helpful discussions .
jay mcclelland and dave rumelhart helped me refine and rewrite many of the ideas presented here a substantially revised version of this report will appear as a chapter by hinton , mcclelland and rumelhart in parallel distributed processing : explorations in the micro - structure of cognition , edited by mcclelland and rumelhart )
section 123 analyses the efficiency of distributed representations .
since each hardware unit is involved in encoding many different items , the unit has a much less specific " receptive field " than it would if the representations were strictly local - activity in a single unit reveals far less about which item is currently being this lack of specificity at the single unit level , we shall sec that distributed representations arc sometimes far more efficient at encoding items accurately .
section 123 deals with the association between the form of a word and its meaning .
this is a case in which distributed representations appear to be much less suitable than local ones , because the associations are purely arbitrary .
there arc very few underlying regularities ( for mono - morphemic words ) , and so the ability of distributed representations to generalize simply causes harmful interference .
however , even in this case distributed representations can be surprisingly efficient and error free .
section 123 compares distributed and local representations from the standpoint of creating new concepts .
local representations appear to require a homunculus with a soldering iron , but there are ways of avoiding this .
local representations also require the system to make a discrete decision about when to create a new concept .
distributed representations are more flexible , allowing concepts to be formed gradually by weight modifications that progressively differentiate one old conccpt ' into several new ones .
finally , section 123 discusses a difficult
is often avoided by advocates of distributed representations .
human knowledge is not just a set of items or pairs of items; it is structured .
at the very least , any human - like memory system must be able to represent schematic structures in which particular constituents are playing particular roles within the whole structure .
there is a gulf between this type of articulated representation and the standard distributed memory scheme in which a subset of the features of an item can give rise to the remaining features .
before examining the detailed arguments in favor of distributed representations , it is important to be clear about their status within an overall theory of human information processing .
it would be wrong to view distributed representations as an alternative to representational schemes like semantic networks or production systems that have been found useful in cognitive psychology and artificial intelligence .
it is more fruitful to view them as one way of implementing these more abstract schemes in parallel networks , but with one proviso : distributed representations give rise to some powerful and unexpected emergent properties .
these properties can therefore be taken as primitives when working in a more abstract formalism .
so if one assumes that more abstract models are implemented in the brain using distributed representations , it is not unreasonable to treat abilities like content - addressable memory , automatic generalization , or the selection of the rule that best fits the current situation as primitive operations , even though there is no easy way to
given a network of simple computing elements and some entities
to be represented ,
straightforward scheme is to use one computing clement for each entity .
this is called a local representation .
it is easy to understand and easy to implement because the structure of the physical network mirrors the structure of the knowlcdsc it contains .
the naturalness and simplicity of this relationship between knowledge and the hardware that implements it have led many people to simply assume that local representations are the best way to use parallel hardware .
there are , of course , a wide variety of more complicated implementations implementations are only worth considering if they lead to increased efficiency or to interesting emergent properties that cannot be conveniently achieved using local representations .
is no one - to - one correspondence between concepts and hardware units , but
this report describes one type of representation that is less familiar and harder to think about than local representations .
each entity is represented by a pattern of activity distributed over many computing elements , and each computing element is involved in representing many different entities .
the strength of this more complicated kind of representation does not lie in its notational convenience or its ease of implementation in a conventional computer , but rather in the efficiency with which it makes use of the processing abilities of networks of simple , neuron - like computing elements .
every representational scheme has its good and bad points .
distributed representations are no exception .
some desirable properties arise very naturally from the use of patterns of activity as representations .
other properties , like the ability to temporarily store a large set of arbitrary associations , are much harder to achieve .
as we shall see , the best psychological evidence for distributed representations is the degree to which their strengths and weaknesses match those of the human mind .
section 123 introduces a way of thinking about memory that is very different from the conventional metaphor in which a memory is stored at a particular location and is retrieved by accessing the location .
the act of remembering is viewed as a constructive process in which a whole item is created afresh from a fragment , much as dinosaurs are created in museums .
what is stored is the rules of inference that allow this constructive process to occur .
section 123 shows how this type of memory automatically generalizes newly acquired knowledge so that it can be applied in other , similar circumstances .
section 123 presents a way of classifying the various types of distributed memory that have been described in the literature .
the classification is not exhaustive and it does not take into consideration the learning schemes that are used for storing new memories .
however , it does serve to emphasize the main decisions that have to be made in designing a distributed memory system .
implement these operations in conventional computers .
thus , the contribution that an analysis of distributed representations can make to these highcr - lcvcl formalisms is to legitimize certain powerful primitive operations which would otherwise appear to be an appeal to magic .
another common source of confusion is the idea that distributed representations arc somehow in conflict with the extensive evidence for localization of function in the brain ( luria , 123 ) .
a system that uses distributed representations still requires many different modules for representing completely different kinds of thing at the same time .
the distributed representations occur within these localized modules .
for example , different modules would be devoted to th ngs as different as mental images and sentence structures , but two different mental images would correspond to alternative patterns of activity in the same module .
the representations advocated in this report are local at a global scale but global at a local scale .
123 : memory as inference
people have a very flexible way of accessing their memories : they can recall items from partial descriptions of their contents ( norman & bobrow , 123 ) .
moreover , they can do this even if some parts of the partial description arc wrong .
many people , for example , can rapidly retrieve the item that satisfies the following partial description : it is an actor , it is intelligent , it is a politician .
this kind of " content - addressable " memory is very useful and it is very hard to implement on a conventional computer because computers store each item at a particular address , and to retrieve an item they must know its address .
if all the combinations of descriptors that will be used for access are free of errors and are known in advance , it is possible to use a method called " hash coding " that quickly yields the address of an item when given part of its content .
in general , however , content - addressable memory requires a massive search for the item that best fits the partial description .
the central computational problem in memory is how to make this search efficient .
when the cues can contain errors this is very difficult because the failure to fit one of the cues cannot be used as a filter for quickly eliminating inappropriate answers .
distributed representations provide an efficient way of using parallel hardware to implement best - fit searches .
the basic idea is fairly simple , though it is quite unlike a conventional computer memory .
different items correspond to different patterns of activity over the very same group of hardware units .
a partial description activates some of the units , 123and interactions between the units then complete the pattern ,
when a partial description is presented it must be turned into a partial activity pattern .
this is easy if the partial description is simply a set of features , but it is much more difficult if the partial description mentions relationships to other objects .
if , for example , the system is asked to retrieve john ' s father , it must represent john , but if john and his father are represented by mutually exclusive patterns of activity in the very same group of units , it is hard to rec how this can be done without preventing the representation of john ' s father .
a distributed solution to this problem is described in section 123
thus generating the item that best fits the description .
a new item is " stored " by modifying the interactions between the hardware units so as to create a new stable pattern of activity .
the main difference from a conventional computer memory is that patterns which are not active do not exist anywhere .
hicy can be re - created because the connection strengths between units have been changed appropriately , but each connection strength is involved in storing many patterns , so it is impossible to point to a particular place where the memory for a particular item is stored .
one way of thinking about distributed memories is in terms of a very large set of plausible inference rules .
each active unit represents a " micro - feature " of an item , and the connection stnmgths stand for plausible inferences between micro - features .
any particular pattern of activity of the units will satisfy some of the " micro - inferences " and violate others .
a stable pattern of activity is one that violates die plausible micro - inferences less than any of the neighboring patterns .
a new stable pattern can be created by changing the inference rules so that the new pattern violates them less than its neighbors .
this view of memory makes it clear that there is no sharp distinction between genuine memory and plausible reconstruction .
a genuine memory is a pattern that is stable because the inference rules were modified when it occurred before .
a " confabulation " is a pattern that is stable because of the way the inference rules have been modified to store several different previous patterns , and so far as the subject is concerned , this may be indistinguishable from the real thing .
the blurring of die distinction between veridical recall and confabulation or plausible reconstruction seems to be characteristic of human memory ( bartlett , 123; neisser , 123 ) .
the reconstructive nature of human memory is surprising only because it conflicts with the standard metaphors we use .
we tend to think that a memory system should work by storing literal copies of items and then retrieving the stored copy , as in a filing cabinet or a typical computer database .
such systems are not
if we view memory as a process which constructs a pattern of activity that represents the most plausible item which is consistent with the given cues , we need some guarantee that it will converge on the representation of the item that best fits the description , though it might be tolerable to sometimes get a good but not optimal fit .
it is easy to imagine this happening , but it is harder to make it actually work .
one recent approach to this problem is to use statistical mechanics to analyze the behavior of groups of interacting stochastic units ( hinton , sejnowski & ackley , 123 ) .
the analysis guarantees that the better an item fits the description , the more likely it is to be produced as the solution .
123 : similarity and generalization
when a new item is stored , the modifications in the connection strengths must not wipe out existing items .
this can be achieved by modifying a very large number of weights very slightly .
if the modifications arc all in the direction that helps the pattern that is being stored , there will be a conspiracy effect : the total help for the intended pattern will be the sum of all die small separate modifications .
for unrelated patterns , however , there will be very little transfer of effect because some of the modifications will help and some will hinder .
instead of all the small modifications conspiring together , they will mainly cancel out .
this kind of statistical reasoning underpins most distributed memory models , but there arc many variations of the basic idea ( see hinton and anderson , 123 ) .
it is possible to prevent interference altogether by using orthogonal patterns of activity for the various items to be stored .
however , this eliminates one of the most interesting properties of distributed representations : they automatically give rise to generalizations .
if the task is simply to remember accurately a set of unrelated items , the generalization effects are harmful and arc called interference .
but generalization is normally a it allows us to deal effectively with situations that are similar but not identical to previously experienced situations .
people are good at generalizing newly acquired knowledge .
if you learn a new fact about an object , your expectations about other similar objects tend to change .
if , for example , you learn that chimpanzees like onions you will probably raise your estimate of the probability that gorillas like onions .
in a network that uses rrhc new knowledge about chimpanzees distributed representations , this kind of generalization is automatic .
is incorporated by modifying some of the connection strengths so as to alter the causal effects of the distributed pattern of activity that represents chimpanzees .
123 the modifications automatically change the causal effects of all similar activity patterns .
so if the representation of gorillas is a similar activity pattern over the same set of units , its causal effects will be changed in a similar way .
the very simplest distributed scheme would represent the concept of onion and the concept of chimpanzee by alternative activity patterns over the very same set of units .
it would then be hard to represent chimps and onions at the same time .
this problem can be solved by using separate modules for each possible role of an item within a larger structure .
chimps , for example , are the " agent " of the liking and so a pattern representing chimps occupies the " agent " module and the pattern representing onions occupies the " patient "
the internal structure of this pattern may also change .
there is always a choice between changing the weights on the outgoing connections and changing the pattern itself so that different outgoing connections become relevant .
changes in the pattern itself alter its similarity to other patterns and thereby alter how generalization will occur in future .
it is generally much harder to figure out how to change the pattern that represents an item than it is to figure out how to change the outgoing connections so that a particular pattern will have the desired effects on another part of the network .
module ( sec figure 123 ) .
kach module can have alternative patterns for all the various items , so this scheme does not involve local representations of items .
what is localized is the role .
if you subsequently learn that gibbons and orangutans do not like onions your estimate of the probability that gorillas like onions will fall , though it may still remain higher than it was initially .
obviously , the combination of facts suggests that liking onions is a peculiar quirk of chimpanzees .
a system that uses distributed representations will automatically arrive at tliis conclusion , provided that the alternative patterns that represent the various apes arc related to one another in a particular way that is somewhat more specific than just being similar to one another : thore needs to be a part of each complete pattern that is identical for all die various apes .
in other words , the group of units used for the distributed representations must be divided into two sub - groups , and all the variotis apes must be represented by the same pattern in the first sub - group , but by different patterns in the second sub - group .
the pattern of activity over the first subgroup represents the type of the item , and the pattern over the second sub - group represents additional " micro - features " that discriminate each instance of the type from the other instances
when the system learns a new fact about chimpanzees , it usually has no way of knowing whether the fact is true of all apes or is just a property of chimpanzees .
the obvious strategy is dicrcforc to modify the strengths of the connections emanating from all the active units , so that the new knowledge will be partly a property of apes in general , and partly a property of whatever features distinguish chimps from other apes .
if it is subsequently learned that other apes do not like onions , modifications will be made in die reverse direction so d*at the information about onions is no longer associated with the subpattern that is common to all apes .
the knowledge about onions will then be restricted to the sub - pattern that distinguishes chimps from other apes .
if it had turned out that gibbons and orangutans had also liked onions , the modifications in the weights emanating from the sub - pattern representing apes would have reinforced one another , and the knowledge would have become associated with the sub - pattern shared by all apes rather than with the patterns that distinguish one ape from another .
a very simple version of this theory of generalization has been implemented in a computer simulation ( hinton , 123 ) .
it works , but as with all simulations , many detailed and arbitrary decisions had to be made to produce a working system .
there is an obvious generalization of the idea that the representation of an item is composed of two parts , one tha* represents the type and another that represents the way in which this particular instance differs from
123any subset of the micro - features can be conside - od to define a type one subset might be common to all apes , and a different ( but
overlapping ) subset might be common to all pets .
this allows an item to be an instance of many different types simultaneously .
figure 123 : in this simplified scheme there are two different modules , one of which represents the agent and the other the patient .
to incorporate the fact that chimpanzees like onions , the pattern for chimpanzees in one module must be associated with the pattern for onions in the other module .
relationships other than " liking " can be implemented by having a third group of units whose pattern of activity represents the relationship .
this pattern must then " gate " the interactions between the agent and patient groups .
hinton ( 123 ) describes one way of doing this gating by using a fourth group of units .
others of the same type .
almost all types are themselves instances of more general types , and this can be implemented by dividing the pattern that represents the type into two sub - patterns , one for the more general type of which this type is an instance , and the other for die features diat discriminate diis particular type from others instances of the same general type .
thus die relation between a type and an instance can be implemented by the relationship between a set of units and a larger set that includes it .
notice that the more general the type , the smaller the set of units used to encode it .
as the number of terms in an description gets smaller , the corresponding exlensional set gets larger .
in a network that uses local representations , it is less obvious how to do generalization .
given that chimpanzees like onions , the obvious way of incorporating the new knowledge is by changing the strengths of connections belonging to the chimpanzee unit .
but diis does not automatically change connections that belong to die gorilla unit .
so extra processes must be invoked to implement generalization in a localist scheme .
one commonly used method is to allow activation to spread from a local unit to other units that represent similar concepts ( quillian , 123; collins and loftus , 123 ) .
then when one concept unit is activated , it will partially activate its neighbors and the so any knowledge stored in the connections emanating from tiiese neighbors will be partially effective .
many variations of this basic idea have been tried ( levin , 123; mcclelland , 123; fahlman , 123 ) and have been shown to be quite effective .
it is hard to make a clean distinction between systems which use local representations plus spreading activation and systems which use distributed representations .
in both cases die result of activating a concept is uiat many different hardware units are active .
the main difference is diat in one case there is a particular individual hardware unit that acts as a " handle " which makes it easy to attach purely conventional properties like the name of the concept .
it is intuitively obvious that it is harder to attach an arbitrary name to a distributed pattern than it is to attach it to a single unit .
as we shall sec in section 123 , such intuitions are not to
123 : varieties of distributed memory
since the 123 ' s , there have been a large number of computer simulations of distributed memory schemes .
many different variations of the same basic ideas have been discovered independently .
this section attempts to introduce some order into the profusion of models by classifying them in a binary tree with the most basic distinctions nearest the root ( see figure 123 ) .
the true space of models has a much richer structure of similarities and differences than can be expressed by a tree , but it ' s a convenient simplification .
many of the distinctions deal with the nature of the input - output functions used for the individual units .
a full discussion of the various functions will appear in rumelhart , hinton and mcclelland ( in press ) .
store by modifying
between hardware units .
store by incrementing123 a vector that has onry as many components as
a single item .
non - linear decision )
willshaw et
activity levels j
. activity levels j
anderson & mozer , 123
decision function \
l activity levels j
' discrete pulses .
& ackley , 123
hopfiew , 123 ft
figure 123 : a classification of some distributed memory models .
a few examples of each type of model are given .
123 : coarse coding
this section considers the number of units diat arc required to encode features accurately .
the central result is a surprising one .
if you want to encode features accurately using as few units as possible , it pays to use units diat arc very coarsely tuned , so diat each feature activates many different units and each unit is activated by many different features .
a specific feature is tiicn encoded by a pattern of activity in many units radicr than by a single active unit , so coarse coding is a form of distributed representation .
to keep die mathematics simple , we shall assume that the units have only two values , on and off . 123 we shall also ignore the dynamics of the system because the question of interest , for the time being , is how many units it takes to encode features with a given accuracy .
we start by considering the kind of feature that can be completely specified by giving a type ( e . g .
line - segment , corner , dot ) and the values of some continuous parameters diat distinguish it from other features of the same type ( e . g .
position , orientation , size . ) for each type of feature there is a space of possible instances .
each continuous parameter defines a dimension of the feature space , and each particular feature corresponds to a point in the space .
for features like dots in a plane , the space of possible features is two - dimensional .
for features like stopped , oriented edge - segments in 123 - d space , the feature space is six - dimensional .
we shall start by considering 123 - d feature spaces and then generalize to higher dimensionalities .
suppose that we wish to represent the position of a single dot in a plane , and we wish to achieve high accuracy without using too many units .
we define the accuracy of an encoding scheme to be the number of different encodings that are generated as the dot is moved a standard distance through the space .
one good encoding scheme would be to divide the units into an x group and a y group , and dedicate each unit to encoding a particular x or y interval as shown in figure 123
a given dot would then be encoded by activity in two units , one from each group , and the accuracy would be proportional to the number of units used .
unfortunately , if two dots have to be encoded at the same time the method breaks down .
the two dots will activate two units in each group , and diere will be no way of telling , from the active units , whether the dots were at ( xl , yl ) and ( x123 , y123 ) or at ( xl , y123 ) and ( x123 , yl ) .
this is called the binding problem .
it arises because the representation does not specify what goes with what .
in a conventional computer it is easy to solve the binding problem .
we simply create two records in the computer memory .
each record contains a pair of coordinates that go together as coordinates of one dot , and
123similar arguments apply with multi - valued activity levels , but it is important not to allow activity le - cls to have arbitrary precision ,
because this makes it possible to represent an infinite amount of information in a single activity level .
o 123 o o o o o
figure 123a : a simple way of using two groups of binary units to encode the position ot a point in a 123 - d space .
the active units in the x and y groups represent the x and y coordinates .
o o 123 o o o 123 o
figure 123b : when two points must be encoded at the same time , it is impossible to tell which x coordinate goes with which y coordinate .
the binding information is encoded by the fact that the two coordinate values arc sitting in the same record ( which usually means they arc sitting in neighboring memory locations ) .
in ' parallel networks , it is much harder to solve the binding problem .
one approach is to set aside , in advance , one unit for each possible combination of x and y values .
this amounts to covering the plane with a large number of small , non - overlapping zones , and dedicating a unit to each zone .
a dot is dicn represented by activity in a single unit so diis is a local representation .
it is much less efficient than die previous scheme because die accuracy is only proportional to the square root of die number of units .
in general , for a k - dimcnsional feature space , die local encoding yields an accuracy proportional to the k th root of die number of units .
achieving high accuracy is thus very expensive .
the use of one unit for each discriminate feature may be a reasonable encoding if a very large number of features are presented on each occasion , so that a large fraction of the units are active .
however , it is a very inefficient encoding if only a very small fraction of the possible features are presented at once .
the average amount of information conveyed by the state of a binary unit is 123 bit if the unit is active half the time , and it is much less if the unit is only rarely active .
123 it would therefore be more efficient to use an encoding in which a larger fraction of the units were active at any moment .
this can be done if we abandon the idea diat each discriminate feature is represented by activity in a single unit
suppose we divide the space into larger , overlapping zones and assign a unit to each zone .
for simplicity , i shall assume diat the zones arc circular , that their centers have a uniform random distribution throughout the space , and that all the zones used by a given encoding scheme have the same radius .
the question of interest is how accurately a feature is encoded as a function of the radius of the zones .
if we have a given number of units at our disposal is it better to use large zones so that each feature point falls in many zones , or is it better to use small zones so that each feature is represented by activity in fewer but more finely tuned units ?
the accuracy is proportional to the number of different encodings that are generated as we move a feature point along a straight line from one side of the space to the otiicr .
every time the line crosses the boundary of a zone , the encoding of the feature point changes because the activity of the unit corresponding to that zone changes .
so the number of discriminate features along the line is just twice the number of zones that the line penetrates .
123 the line penetrates every zone whose center lies within one radius of the line ( see figure 123 ) .
this number is proportional to the radius of the zones , r , and it is also proportional to their number , n .
hence the
amount of information conveyed by a unit that has a probability of p of being on is
problems arise if you enter and leave a zone without crossing other zone borders in between .
you revert to the same encoding as
before .
this effect is negligible if the zones are dense enough for there to be many zones containing each point in the space .
figure 123 : the number of zone boundaries that are cut by the line is proportional to the number of zone centers within one zone radius of the line .
accuracy , a is related to the number of zones and to their radius as follows :
a a nr
in general , for a k - dimensional space , the number of zones whose centers lie within one radius of a line through the space is proportional to the volume of a k - dimcnsional hyper - cylinder of radius r .
this volume is equal to the length of the cylinder ( which is fixed ) times its k - 123 dimensional cross - sectional area which is proportional to r* " 123
hence , the accuracy is given by :
so , for example , doubling the radius of the zones increases by a factor of 123 the linear accuracy with which a six - dimensional feature like a stopped oriented 123 - d edge is represented .
the intuitive idea that larger zones lead to sloppier representations is entirely wrong , because distributed representations hold information much more efficiently than local ones .
even though each active unit is less specific in its meaning , the combination of active units is far more specific .
123 notice also that with coarse coding the accuracy is proportional to the number of units , which is much better than being proportional to the k* root of the number .
units that respond to complex features in retinotopic maps in visual cortex often have fairly large receptive fields .
this is often interpreted as the first step on the way to a translation invariant representation .
however , it may be that *he function of the large fields is not to achieve translation invariance but to pinpoint accurately where the feature is !
limitations on coarse coding
so far , only the advantages of coarse coding have been mentioned , and its problematic aspects have been ignored .
there arc a number of limitations which cause the coarse coding strategy to break down when die " receptive fields " become too large .
one obvious limitation occurs when the fields become comparable in size to the whole space .
this limitation is generally of little interest because other , more severe , problems arise before the receptive fields become this large .
coarse coding is only effective when ti^ . c features that must be represented arc relatively sparse .
if many feature - points are crowded together , each receptive field will contain many features and the activity pattern in the coarse coded units will not discriminate between many alternative combinations of feature points .
123 thus there is a resolution / accuracy trade - off .
coarse coding can give high accuracy for the parameters of features provided that features arc widely spaced so that high resolution is not also required .
as a rough rule of thumb , the diameter of the receptive fields should be of the same order as the spacing between simultaneously present feature - points
the fact that coarse coding only works if the features are sparse should be unsurprising , given that its advantage over a local encoding is that it uses die information capacity of die units more efficiently by making each unit active more often .
if the features are so dense diat the units would be active for about half die time using a local encoding , coarse coding can only make diings worse .
a second major limitation on the use of coarse coding stems from die fact that die representation of a feature must be used to affect other representations .
there is no point using coarse coding if die features have to be recoded as activity in finely tuned units before they can have the appropriate affects on other representations .
if we assume diat the effect of a distributed representation is the sum of the effects of the individual active units that constitute the representation , there is a strong limitation on the circumstances under which coarse coding can be used effectively .
nearby features will be encoded by similar sets of active units , and so they will inevitably tend to have similar effects .
broadly speaking , coarse coding is only useful if the required effect of a feature is the average of die required effects of its neighbors .
at a fine enough scale this is nearly always true for spatial tasks .
the scale at which it breaks down determines an upper limit on the size of the receptive fields .
123 lf the units are allowed to have integer activity levels that reflect the number of feature points falling within their fields , a few nearby
points can be tolerated , but not many .
123 123it is interesting that many of the geometric visual illusions illustrate interactions between features at a distance much greater than the jncertainty in the subjects knowledge of the position of a feature .
this is just what would be expected if coarse coding is being used to represent complex features accurately .
another limitation is that whenever coarse coded representations interact , there is a tendency for the coarseness to increase .
to counteract diis tendency , it is probably necessary to have lateral inhibition operating within each representation
coarse coding and fourier analysis
there is an approximate analogy between coarse coding and fourier analysis .
consider , for example , the problem of encoding the precise time at which a click occurs using fourier analysis .
a single click of noise contains power at many different frequencies .
to encode its precise temporal location it is sufficient to encode die low frequency information accurately .
the high frequencies are only needed for resolving closely spaced clicks .
a standard way to extract the low frequency information is to pass die click dirough a low - pass filter which blurs it in time and then to sample the blurred click at regular intervals .
the use of large receptive fields in coarse coding is the analog of the blurring process , and the center points of the fields arc die analog of die sample points .
the restriction that the receptive field diameter must be comparable with the spacing between features is the analog of die sampling theorem which states that the linear separation between sample points must not exceed half the wavelength of the shortest wavelength components allowed through the blurring process .
the analogy is not perfect , however , because a fourier encoding is normally used to represent a continuous distribution rather tiian a number of discrete feature points , and each coefficient in die fourier encoding is a real - valued number radicr than a set of binary activation levels .
coarse coding and conventional data - structures
in a conventional computer , any piece of the general purpose memory can be used for any data - structure .
this allows the computer to get by with much less memory than would be required if it had to set aside , in advance , a special piece of memory for each possible data - structure .
the economy is possible because , on any one occasion , only a very small fraction of all the possible data - structures need to be present , so the very same piece of memory can be used for different data - structures on different occasions .
in a network in which the knowledge is stored in the connection strengths , it is much harder to make units stand for different things on different occasions .
to be effective as a representation , activity in a unit must have the right causal effects on other representations , so unless the strengths of its outgoing connections can be changed rapidly , the " meaning " of a unit cannot easily be changed .
coarse coding allows the same unit to be used to help represent many different features at different times without changing the connection strengths .
the perimeter of the receptive field is what does the work of
this issue requires further research .
carving up the space of possible features into small zones .
123 on any one occasion , most of the perimeter of a given unit will be irrelevant because the other active units will already have located the feature fairly accurately and so it will be known not to be near most of the perimeter .
however , that part of tine perimeter which intersects the residual region of uncertainty left by the other units will act to increase the accuracy , because the state of the unit will indicate whether the feature falls inside or outside of this part of the perimeter ( sec figure 123 ) .
on different occasions , different parts of the perimeter will be doing the work .
this ability to use the same unit in different ways on different occasions is a small step towards the flexible use of representational capacity that makes conventional computers so powerful .
if a unit has a receptive field with a large perimeter , it may do different work on different occasions .
the marginal contribution of the unit in pinning down the precise parameters of a feature point depends on the range of uncertainty left by the other units .
quite different parts of the perimeter of unit a are relevant on different occasions .
one obvious weakness of coarse coding is that units whose fields have a large overlap tend to have highly correlated states .
this is inefficient .
to maximize the accuracy , the individual units should have uncorrected receptive fields , so that activity in one unit provides no information about activity in other units .
this is just what is achieved by the standard binary representation of continuous quantities .
unfortunately , binary representations are difficult to use in systems where all the knowledge is in the connections because the receptive field of the least significant digit is very disconnected , which usually makes it hard to store useful knowledge in the strengths of the connections emanating from that unit
^that is why the accuracy is proportional to the magnitude of the perimeter .
generalization to non - continuous spaces
the principle underlying coarse coding can be generalized to non - continuous spaces by thinking of a set of items as die equivalent of a receptive field .
a local representation uses one unit for each possible item .
a distributed representation uses a unit for a set of items , and it implicitly encodes a particular item as the intersection of die sets that correspond to the active units .
in die domain of spatial features dicrc is generally a very strong regularity : sets of features with similar parameter values need to have similar effects on other representations .
coarse coding is efficient because it allows this regularity to be expressed in die connection strengths .
in other domains , the regularities are different , but the efficiency arguments arc the same : it is better to devote a unit to a set of items than to a single item , provided the set is chosen in such a way that membership of die set implies something about membership of odier sets .
this implication can then be captured as a connection strength .
it is , of course , a very difficult search problem to decide which particular sets of items should correspond to ideally , a set should be chosen so diat membership of this set has strong implications for memberships of other sets diat arc also encoded by individual units .
the ability to discover such sets is crucial if distributed representations arc to be effective , but this report is not concerned with the learning problem .
the interested reader is referred to hinton , scjnowski & ackley ( 123 ) where a mcdiod of finding good sets is described in detail .
123 : implementing an arbitrary mapping between two domains
this section shows how a distributed representation in one group of units can cause an appropriate distributed representation in another group of units .
we consider die problem of implementing an arbitrary pairing between representations in the two groups , and we take as an example the association between the visual form of a word and its meaning .
the reason for considering an arbitrary mapping is that this is the case in which local representations seem most helpful .
if distributed representations are better even in this case , then they are certainly better in cases where there are underlying regularities that can be captured by regular interactions between units in one group and units in another .
if we restrict ourselves to mono - morphemic words , the mapping from strings of graphemes onto meanings appears to be arbitrary in the sense that knowing what some strings of graphemes mean docs not help one predict what a new string means .
123 this arbitrariness in the mapping from graphemes to meanings is what
123 123evcn for mono - morphemic words there may be particular fragments that have associated meaning .
for example , words starting with msn m usually mean something unpleasant to do with the lips or nose ( sneer , snarl , snigger ) , and words with long vowels are more likely to stand for large slow things than words with short vowels ( george lakoff , personal communication ) .
much of lewis carrol ' s poetry relics on such effects .
gives plausibility to models that have explicit word units .
it is obvious that arbitrary mappings can be implemented if there arc such units .
a grapheme string activates exactly one word unit , and this activates whatever meaning we wish to associate with it ( sec figure 123a ) .
the semantics of similar grapheme strings can then be completely independent because they arc mediated by separate word units .
there is none of the automatic generalization that is characteristic of distributed representations .
it is not at all obvious that arbitrary mappings can be implemented in a system where the intermediate layer of units encodes the word as a distributed pattern of activity instead of as activity in a single local unit .
the distributed alternative appears to have a serious drawback .
the effect of a pattern of activity on other representations is the combined result of the individual effects of the active units in the pattern .
so similar patterns tend to have similar effects .
it appears that we are not free to make a given pattern have whatever effect we wish on the meaning representations , without thereby altering the effects that other patterns have .
this kind of interaction appears to make it difficult to implement arbitrary mappings from distributed representations of words onto meaning representations .
i shall now show that these intuitions are wrong , and that distributed representations of words can work perfectly well and may even be more efficient than single
figure 123b shows a three - layered system in which grapheme / position units feed into " word - set " units which , in turn , feed into " semantic " or " sememe " units .
for simplicity , we shall assume that each unit is either active ( these assumptions can be relaxed without or inactive , and that there is no feedback or cross - talk .
the argument ) .
a word - set unit is activated whenever grapheme / position units codes a word in a particular set the set could be all the four - letter words starting with " he " , for example , or all the words containing at least two " t m , s .
all that is required is that it is possible to decide whether a word is in the set by applying a simple test to the activated grapheme / position units .
so , for example , the set of all words meaning " nice " is not allowed as a word - set 123 returning to figure 123b , the question is whether it is possible to implement an arbitrary set of associations between grapheme / position vectors and sememe vectors when the word - set units are each activated by more than one word .
it will be sufficient to consider just one of the many possible specific models .
let us assume that an active word - set unit provides positive input to all the sememe units that occur in the meaning of any word in die word - set let us also assume that each sememe unit has a variable threshold that is dynamically adjusted to be just
the pattern of
models of this type , and closely related variants , have been analyzed by willshaw ( 123 ) , dobson ( personal communication ) , and by
david zipser ( unpublished workshop talk , 123 )
123 123there is an implicit assumption that word meanings can be represented as sets of sememes .
this is a contentious issue .
there appears to be a gulf between the componential view in which a meaning is a set of features and the structuralist view in which the ;ts relationships to other meanings .
section 123 discusses one way of integrating these meaning of a word can only be defined in terms of two views by allowing articulated representations to be built out of a number of different sets of active features .
figure 123a : a fragment of a three layer network .
the bottom layer contains units that represent particular graphemes in particular positions within the word .
the middle layer contains units that recognize complete words , and the top layer contains units that represent semantic features of the meaning of the word .
this network uses local representations of words in the middle layer .
figure 123b : the top and bottom layers are the same as in figure 123a , but the middle layer uses a more distributed representation .
each unit in this layer can be activated by the graphemic representation of any one of a whole set of words .
the unit then provides input to every semantic feature that occurs in the meaning of any of the words that activate it .
only those word - sets containing the word " cat " are shown in this example .
notice that the only semantic features which receive input from all these word - sets are the semantic features of cat
slightly less than the number of active word - set units .
only sememe units that are receiving input from every
active word - set unit will dicn become active .
all the sememes of the correct word will be activated , because each of these sememes will occur in the meaning of one of the words in the active word - sets .
however , additional sememes may also be activated because , just by chance , dicy may receive input from every active word - set unit .
for a sememe to receive less input than its threshold , there must be at least one active word - set that does not contain any word which has die sememe as part of its meaning .
for each active word - set the probability , / , of diis happening is :
/ = ( l - p ) < * - d
where p is the proportion of words that contain the sememe and w is the number of words in die word - set of the word - set unit the reason for the term w - 123 is that the sememe is already assumed not to be part of the meaning of the correct word , so there are only w - 123 remaining words that could have it in their meaning .
assume diat when a word is coded at die graphemic level it activates u units at the word - set level .
each sememe that is not part of the word ' s meaning has a probability / of failing to receive input from each word - set unit .
the probability , / that all of diesc word - set units will provide input to it is therefore
/ = ( l - 123 "
by inspection , this probability of a " false positive " sememe reduces to zero when w is 123
table 123 shows the value of / for various combinations of values of p , w , and w .
notice that if p is very small , / c an remain negligible even if w is quite large .
this means that distributed representations in which each word - set unit participates in the representation of many words , do not lead to errors if the semantic features are relatively sparse in die sense diat each word meaning contains only a small fraction of the total set of sememes .
so the word - set units can be fairly non - specific provided the sememe units are fairly specific ( not shared by too many different word meanings ) .
some of the entries in the table make it clear that for some values of p , there can be a negligible chance of error even though the number of word - set units is considerably less than the number of words ( the ratio of words to word - set units is w / u ) .
the example described above makes many simplifying assumptions .
for example , each word - set unit is
123 x 123 ~ 123 123 x 123 " 123
123 x 123 123 ~ n 123 x 123 " 123 123 x 123 " 123
123 x 123 " ' 123 x 123 123 " 123
. 123 123 x . 123 123 x
table 123 : the probability , f , of a false positive sememe as a function of the number of active word - set units per word , u , the number of words in each word - set , w , and the probability , p , of a sememe being part of a word meaning .
assumed to be connected to every relevant sememe unit if any of these connections were missing , we could not afford to give the sememe units a threshold equal to the number of active word set units .
to allow for missing connections we could lower the threshold , but this would increase the false - positive error rate .
alternatively , we could make each word - set unit veto the sememes that do not occur in any of its words .
this scheme is far more robust against missing connections , because the absence of one veto can be tolerated if there are other vetos ( vernon dobson , personal communication ) .
there are two more simplifying assumptions both of which lead to an underestimate of the effectiveness of distributed representations for the arbitrary mapping task .
first , the calculations assume that there is no fine - tuning procedure for incementing some weights and decrementing others to improve performance in the cases where the most frequent errors occur .
second , the calculations ignore cross - talk among the sememes .
if each word - meaning is a familiar stable pattern of sememes there will be a strong " clean - up " effect which tends to suppress erroneous sememes as soon as the pattern of activation at the sememe level is
sufficiently close to the familiar pattern for a particular word - meaning .
interactions among the sememes also ' provide an explanation for the ability of a single grapheme string ( e . g .
" bank " ) to elicit two quite different meanings .
the " bottom - up " effect of the activated word - set units helps both sets of sememes , but as soon as " top - down " factors give an advantage to one meaning , the sememes in the other meaning will be suppressed by competitive interactions at the sememe level ( kawamoto and anderson , 123 ) .
as soon as there is cross - talk among the sememe units and fine tuning of individual weights to avoid frequent errors , the relatively straight - forward probabilistic analysis given above t teaks down .
to give the cross - talk time to clean up the output , it h necessary to use an iterative procedure instead of the simple " straight - through " processing in which each layer completely determines die states of all the units in the subsequent layer in a single , synchronous step .
systems containing cross - talk , feedback , and asynchronous processing elements are probably more realistic as models of the brain , but they are generally very hard to analyze .
however , there is a special subclass of these more complex systems that behaves in a tractable way and is capable of interesting kinds of search and learning behavior ( hinton , sejnowski & acklcy , 123 ) .
it uses processing elements that are inherently stochastic .
surprisingly , the use of stochastic elements makes these networks better at performing searches , better at learning , and easier to analyze .
a simple network of this kind is used to illustrate some of die claims about die ability to " clean up " the output by using interactions among sememe units , and the ability to avoid errors by fine tuning the appropriate weights .
the network contains 123 grapheme units , 123 word - set units , and 123 sememe units .
there are no direct connections between grapheme and sememe units , but each word - set unit is connected to all the grapheme and sememe units .
the grapheme units are divided into three sets of ten , and each three - letter word has one active unit in each group of ten ( units can only have activity levels of 123 or 123 ) .
the " meaning " of a word is chosen at random by selecting each sememe unit to be active with a probability of 123 .
the network shown in figure 123 has learned to associated 123 different grapheme strings with their chosen meanings .
each word - set unit is involved in the representation of many words , and each word involves many
hinton & sejnowski ( in press ) describe the details of the learning procedure which was used to create this network and the search procedure which was used to settle on a set of active sememes when given the graphemic input here we simply summarize the main results of the simulation .
after a long period of learning , the network was able to produce the correct pattern of sememes 123% of the time when given a graphemic input removal of any one of the word - set units after the learning typically caused a slight rise in the error rate for several different words rather than the complete loss of one word
figure 123 : a compact display that shows all the connection strengths in a three - layer network which can map 123 different graphemic strings into the corresponding collections of active semantic features .
the top 123 rows of the display arc the semantic units , the middle two rows are the intermediate units , and the bottom three rows arc the grapheme units .
within each unit , the black or white blobs show the strengths of its connections to other units .
while blobs are positive connections , black are negative , and the magnitude of a blob represents uie strength of the
connection .
fhe relative position of a blob within a unit indicates the relative position within the whole network of the other unit involved in the connection ( it is as if each unit contained a little map of the whole net ) .
all connection strengths are the same in both directions , so every strength is represented twice in this display .
in the position where the connection between a unit and itself would be displayed , the threshold of the unit is shown ( black means a positive threshold )
123 , 123 tests with a missing word - set unit tiicre were 123 errors .
some of dicsc consisted of one or two missing or extra sememes , but 123 of the errors were exactly die pattern of sememes of some other word .
this is a result of the cooperative interactions among die sememe units .
if die input coming from die word - set units is noisy , die clean up effect may settle on a similar but incorrect meaning .
this effect is reminiscent of a phenomenon called deep dyslexia which occurs with certain kinds of brain damage in adults .
when shown a word and asked to read it , the subject will sometimes say a different word with a very similar meaning .
the incorrect word sometimes has a very different sound and spelling .
for example , when shown the word " peach " the subject might say " apricot " ( see coltheart & patterson ( 123 ) for more information about acquired dyslexia ) .
semantic errors of this kind seem bizarre because it seems as if the subject must have accessed die lexical item peach in order to make die scmantically related error , and if he can get to die lexical item why can ' t he say it ? ( these subjects may know and be able to say the words diat they misread ) .
distributed representations allow us to dispense with the rigid distinction between accessing a word and not accessing it .
in a network that has learned the word " peach " , the graphemic representation of " peach " will cause approximately the right input to the sememe units , and interactions at the sememe level can dien cause exactly the pattern of sememes for apricot .
another psychologically interesting effect occurs when die network relearns after it has been damaged .
the network was damaged by adding noise to every connection diat involved a word - set unit .
this reduced the performance from 123% correct to 123% .
123 the network was then retrained and it exhibited very rapid relcarning , much faster dian its original rate of learning when its performance was 123% correct .
this rapid recovery ' was predicted by a geometrical argument which shows that there is something special about a set of connection strengths that is generated by adding noise to a near - perfect set the resulting set is very different from other sets of connection strengths that exhibit die same performance , ( see hinton & sejnowski , in press ,
an even more surprising effect occurs if a few of the words are ommitted from the retraining .
the error rate for these words is substantially reduced as the retraining proceeds , even though the other grapheme - sememe pairings have no intrinsic relation to them because all the pairings were selected randomly .
the " spontaneous " recovery of words that the network is not shown again is a result of the use of distributed representations .
all the weights are involved in encoding the subset of the words that are shown during retraining , and so the added noise tends to be removed from every weight .
a scheme that used a separate unit for each word would not behave in this way , so one can view spontaneous recovery of unrehearsed items as a
123 123 t he error rate was 123% rather than 123% in this example because
effects had less time to settle on the optimal output
the network was forced to respond faster , so the cooperative
qualitative signature of distributed representations .
123 : creating new concepts
any plausible scheme for representing knowledge must be capable of learning novel concepts that could not be anticipated at the time die network was initially wired up .
a scheme diat uses local reprcscntadons must first make a discrete decision about when to form a new concept , and thenm it must find a spare hardware unit that has suitable connections for implementing the concept involved .
finding such a unit may be difficult if we assume that , after a period of early development , new knowledge is incorporated by changing the strengths of die existing connections rather dian by growing new ones .
if each unit only has connections to a small fraction of the others , there will probably not be any units that are connected to just the right other ones to implement a new concept .
for example , in a collection of a million units each connected at random to ten thousand others , the chance of there being any unit that is connected to a particular set of 123 others is only one in a million .
in an attempt to rescue local representations from this problem , several clever schemes have been proposed that use two classes of units .
the units diat correspond to concepts arc not directly connected to one another .
instead , the connections are implemented by indirect pathways through several layers of intermediate units ( fahlman , 123; fcldman , 123 ) .
this scheme works because die number of potential pathways through the intermediate layers far exceeds the total number of physical connections .
if there are k layers of units each of which has a fan - out of n connections to randomly selected units in the following layer , there are n k potential padiways .
there is almost certain to be a pathway connecting any two concept - units , and so the intermediate units along this pathway can be dedicated to connecting diose two concept - units .
however , these schemes end up having to dedicate several intermediate units to each effective connection , and once the dedication has occurred , all but one of the actual connections emanating from each intermediate unit are wasted .
the use of several intermediate units to create a single effective connection may be appropriate in switching networks containing elements that have units with relatively small fan - out but it seems to be an inefficient way of using the hardware of the brain .
the problems of finding a unit to stand for a new concept and wiring it up appropriately do not arise if we use distributed representations .
all we need to do is modify the interactions between units so as to create a new stable pattern of activity .
if this is done by modifying a large number of connections very slightly , the creation of a new pattern need not disrupt die existing representations .
the difficult problem is to choose an appropriate pattern for the new concept .
the effects of the new representation on representations in other parts of the system will be determined by the units that are active , and so it is important to use a collection of active units that have roughly the correct effects .
fine - tuning of the effects of the new pattern can be
achieved by slightly altering the effects of the active units it contains , but it would be unwise to choose a random pattern for a new concept , because major changes would then be needed in die weights , and diis would disrupt other knowledge .
ideally , the distributed representation that is chosen for a new concept should be the one that requires the least modification of weights to make the new pattern stable and to make it have the required effects on other representations .
naturally , it is not necessary to create a new stable pattern all in one step .
it is possible for the pattern to emerge as a result of modifications on many separate occasions .
this alleviates an awkward problem that a / iscs with local representations : the system must make a discrete all - or - nonc decision about when to create a new concept .
if we view concepts as stable patterns they are much less discrete in character .
it is possible , for example , to differentiate one stable pattern into two closely related but different variants by modifying some of the weights slightly .
unless we are allowed to clone the hardware units ( and all their connections ) , this kind of gradual conceptual differentiation is much harder to achieve with local representations .
123 : representing constituent structure
any system which attempts to implement the kinds of conceptual structures that people use has to be capable of representing two radier different kinds of hierarchy .
the first is the " isa " hierarchy that relates types to instances of those types .
the second is die part / whole hierarchy that relates items to die constituent items that they are composed of .
the most important characteristics of the isa hierarchy are that known properties of the types must be " inherited " by the instances , and properties that arc found to apply to all instances of a type must normally be attributed to the type .
in section 123 we saw how the isa hierarchy can be implemented by making the distributed representation of an instance include , as a subpart , the distributed representation for the type .
this representational trick automatically yields the most important characteristics of the isa hierarchy , but the trick can only be used for one kind of hierarchy .
if we use the part / whole relationship between patterns of activity to represent the type / instance relationship between items , it appears that we cannot also use it to represent the part / whole relationship between items .
we cannot make the representation of the whole be the sum of the representations of its parts .
the question of how to represent the relationship between an item and the constituent items of which it is composed has been a major stumbling block for theories diat postulate distributed representations .
in the rival , localist scheme , a whole is a node that is linked by labelled arcs to the nodes for its parts .
but the central tenet of the distributed scheme is that different items correspond to alternative patterns of activity in the same set of units , so it seems as if a whole and its parts cannot both be represented at the same time .
i can only see one way out of this dilemma .
it relies on the fact that wholes are not simply the sums of their
parts , llicy arc composed of parts diat play particular roles within die whole structure .
a shape , for example , is composed of smaller shapes that have a particular size , orientation and position relative to the whole .
each constituent shape has its own spatial role , and die whole shape is composed of a set of shape / role pairs .
123 similarly , a proposition is composed of objects diat occupy particular semantic roles in the whole propositional structure .
this suggests a way of implementing die relationship between wholes and parts : the identity of each part should first be combined with its role to produce a single pattern that represents the combination of the identity and the role , and then the distributed representation for the whole should consist of the sum of the distributed representations for diesc identity / role combinations ( plus some additional " emergent " features ) .
123 this proposal differs from die simple idea diat die representation of the whole is the sum of the representations of its parts , because die subpatterns used to represent identity / role combinations arc quite different from die patterns used to represent the identities alone .
they do not , for example , contain these patterns as parts .
naturally , there must be an access path between the representation of an item as a whole in its own right and the representation of that same item playing a particular role within a larger structure .
it must be possible , for example , to generate the identity / role representation from two separate , explicit , distributed patterns one of which represents the identity and the other of which represents the role .
it must also be possible to go the other way and generate the explicit representations of the identity and role from the single combined representation of the identity / role combination ( see figure 123 ) .
the use of patterns diat represent identity / role combinations allows the part / whole hierarchy to be represented in the same way as the type / instance hierarchy .
a whole is simply a particular instance of a number of more general types , each of which can be defined as the type that has a particular kind of part playing a particular role ( e . g men with wooden legs ) .
sequential symbol processing
if constituent structure is implemented in the way described above , there is a serious issue about how many structures can be active at any one time .
the obvious way to allocate the hardware is to use a group of units for each possible role within a structure and to make die pattern of activity in diis group represent the identity of the constituent that is currently playing that role .
this implies that only one structure can be represented at a time , which is clearly a very severe restriction .
however , people do seem to suffer from strong constraints
relationships between parts are mportant as well .
one advantage of explicitly representing shape / role pairs is that it allows different pairs to support each other .
one can view the various different locations within an object as slots and the shapes of parts of an object as the fillers of these slots .
knowledge of a whole shape can then be implemented by positive interactions between the various
see hinton , 123 for a simulation that uses this representational technique .
figure 123 : a sketch of the apparatus that might be necessary for combining separate representations of an identity and a role into a single pattern .
only one identity and only one role can be explicidy represented at a time because the identity and role groups can each only have one pattern of activity at a time .
however , the various role groups allow many identity / role combinations to be encoded simultaneously .
the small triangular symbols represent the ability of the pattern of activity in the group that explictly represents a role to determine which one of the many role groups is currently interacting with the identity group .
this allows the identity occupying a particular role to be " read out " as well as allowing the reverse operation of combining an identity and a role .
on the number of structures of the same general type that they can process at once .
to a first approximation , people are sequential symbol processors ( newell , 123 ) .
the scqucntiality that they exhibit at this high level of description is initially surprising given the massively parallel architecture of the brain , but it becomes much easier to understand if we abandon our localist predelictions in favor of the distributed alternative which uses the parallelism to give each active representation a very rich internal structure that allows the right kinds of generalization and content - addressability .
one central tenet of the sequential symbol processing approach ( newell , 123 ) is the ability to focus on any part of a structure and to expand that into a whole that is just as rich in content as the original whole of which it was a part .
the recursive ability to expand parts of a structure for indefinitely many levels , and the inverse ability to package up whole structures into a reduced form that allows them to be used as constituents of larger structures is the essence of symbol processing .
it allows a system to build structures out of things that refer to other whole structures without requiring that these other structures be represented in all their
in conventional computer implementations this ability is achieved by using pointers .
these are very convenient but they depend on the use of addresses .
in a parallel network , we need something that is functionally equivalent to arbitrary pointers in order to implement symbol processing .
this is exacdy what is provided by subpatterns that stand for identity / role combinations .
they allow the full identity of the part to be accessed from a representation of the whole and a representation of the role that the system wishes to focus on , and they also allow explicit representations of an identity and a role to be combined into a less cumbersome representation , so that several identity / role combinations can be represented simultaneously in order to form the representation of a larger structure .
given a parallel network , items can be represented by activity in a single , local unit , or by a pattern of activity in a large set of units with each unit encoding a " micro - feature " of die item .
distributed representations are efficient whenever there are underlying regularities which can be captured by interactions among micro - features .
by encoding each piece of knowledge as a large set of interactions it is possible to achieve useful properties like content - addressable memory and automatic generalization , and new items can be created without having to create new connections at the hardware level .
in the domain of continuously varying spatial features it is relatively easy to provide a mathematical analysis of the advantages and drawbacks of using distributed representions .
distributed representations seem to be unsuitable for implementing purely arbitrary mappings because
there is no underlying structure and so generalization only causes unwanted interference .
however , even for diis task , distributed representations can be made fairly efficient and they exhibit some psychologically interesting effects when damaged .
there are several very hard problems that must be solved before distributed representations can be used effectively .
one is to decide on the pattern of activity diat is to be used for representing an item .
the similarities between the chosen pattern and odier existing patterns will determine die kinds of generalization and interference that occur .
the search for good patterns to use is equivalent to the search for the underlying regularites of the domain .
this learning problem is not addressed here .
the interested reader is referred to hinton , sejnowski & ackley ( 123 ) .
another hard problem is to clarify the relationship between distributed representations and techniques used in artificial intelligence like schemas , or hierarchical structural descriptions .
existing artificial intelligence programs have great difficulty in rapidly finding the schema that best fits the current situation .
parallel networks offer the potential of rapidly applying a lot of knowledge to this best - fit search , but this potential will only be realized when diere is a good way of implementing schemas in parallel networks .
