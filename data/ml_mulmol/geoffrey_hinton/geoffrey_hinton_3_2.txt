this paper describes two new methods for modelling the manifolds of digitised images
of handwritten digits .
the models allow a priori information about the structure of
the manifolds to be combined with empirical data .
accurate modelling of the man -
ifolds allows digits to be discriminated using the relative probability densities under
the alternative models .
one of the methods is grounded in principal components
analysis , the other in factor analysis .
both methods are based on locally linear ,
low - dimensional approximations to the underlying data manifold .
links with other
methods that model the manifold are discussed .
a standard discriminative approach to digit recognition is to train a classi ( cid : 123 ) er to
output one of the ten classes based on the input image .
the classi ( cid : 123 ) er could , for
instance , be a multilayer feedforward neural network ( ) .
however , it is also possible
to discriminate by ( cid : 123 ) tting a separate probability density model to each class and then
picking the class of the model that assigns the highest density to a test image
relative density approach typically requires more computation during recognition , and
it can devote a lot of parameters to modelling aspects of the image that are irrelevant
for discrimination , but it has several advantages :
( cid : 123 ) during training , each model need only consider training examples of its own
class , saving an order of magnitude in computation .
if the models are correct ,
this saving is achieved with no reduction in discriminative performance .
( cid : 123 ) after training , it is possible to add a new class without retraining the previous
( cid : 123 ) it is possible to ( cid : 123 ) t far more parameters before over ( cid : 123 ) tting occurs because the
input vectors contain much more information than the class label .
assuming
the same number of examples in each class , a class label only contains log
bits so each example only provides : bits of constraint on the function that
maps inputs to class labels .
however , it takes many more bits to specify the
input image , so each example provides far more constraint on the parameters
one way to see why , is to imagine a table look - up scheme in which each input vector is randomly
mapped to a di ( cid : 123 ) erent word of memory .
to ( cid : 123 ) t a training set perfectly , we need as many words of
memory as training examples and each word only needs enough bits to specify the correct label .
of a density model .
for the ( cid : 123 ) real - valued images we use , ; training
examples are su ( cid : 123 ) cient to ( cid : 123 ) t about ; parameters .
( cid : 123 ) aspects of the image that are irrelevant for discrimination between classes may
nevertheless be very relevant for detecting occasional bad images that do not fall
into any of the given classes .
relative density methods have a natural rejection
criterion when all the densities are low .
( cid : 123 ) the density models we describe can be ( cid : 123 ) tted by methods like singular value de -
composition ( svd ) and expectation - maximisation ( em ) that are considerably
more e ( cid : 123 ) cient than gradient descent .
we are not claiming that the relative density approach is necessarily better than the
discriminative approach , just that it is an alternative worth considering .
certain discriminative methods can be seen in terms of relative densities .
for in -
stance , kernel density estimation ( ) , ( ) is a popular non - parametric modelling tech -
for this , the probability density for a particular digit is the weighted sum of
a collection of kernel functions .
the functions all have the same shape , but each is
centred on one of the patterns in that class in the training set .
each kernel function
typically integrates to , and the weights in the sum are usually =m , where m is the
number of the patterns in the training set , so the overall kernel density estimate is
correctly normalised .
having built ten such models , one for each digit class , the class
to which a new image belongs is inferred by evaluating the density under each of the
models at the location of the new image , and reporting the one that is the highest
the kernel functions are radially symmetric , monotonically decreasing , and have un -
bounded extent ( e . g a gaussian ) , then relative density estimation becomes identical
to nearest neighbour classi ( cid : 123 ) cation as the width parameter of the kernel goes to zero .
this is because the model that contains the kernel function closest to the data will ,
in the limit , have in ( cid : 123 ) nitely higher density than any other model , even if other models
have many kernel functions that are almost as close .
kernel density estimators are convenient models in the case that there is ample data ,
ample computational time for inference ( it is e ( cid : 123 ) ectively a memory - based technique ) ,
and in which there is little a priori information about the nature of the data
latter two are not true for images of handwritten digits .
we sought to build better
models for such images on the basis of such information .
elastically deformable tem -
plates ( ) , ( ) are one example , and have been shown to model non - normalised images
of characters well ( ) .
unfortunately , they are also computationally too expensive
for normal use .
we therefore turned to gaussian blended linear models , which are
computationally much cheaper but are also appropriate for such images .
simard et al ( ) pointed out the locally low - dimensional linear structure underlying
these images .
take the - dimensional space of all ( normalised ) ( cid : 123 ) images and
consider the subset of images of any particular digit , say the digit .
since small
changes to the image of a preserve its identity , this subset will have some properties
of a surface ( it will mostly be continuous and di ( cid : 123 ) erentiable .
in particular , a ( cid : 123 ) ne
transformations ( translations , rotations , scalings , and shearing ) as well as manipu -
lations to the thickness of the strokes of a digit preserve its identity .
considering
the e ( cid : 123 ) ect on an image of small ( ie sub - pixel ) transformations like these suggests that
the surface is locally at least dimensional and probably somewhat more .
di ( cid : 123 ) erent
styles of will likely generate separated continuous patches .
simard et al ( ) used a
nearest neighbour method ( which , as pointed out , is equivalent to a limiting case of
a relative density method ) in the space of these dimensional planes , where the dis -
tance between two points in the space is the closest distance between the underlying
planes in image space .
using this as the distance rather than the simple euclidean
distance between two images substantially improved recognition performance .
unfortunately , simards technique is founded on a nearest neighbour method , and
therefore recognition is again computationally expensive .
we can build much cheaper
models by combining information from many examples about the local structure of
the manifold in image space representing a digit .
combining information in this way
should have the added advantage of averaging out some of the noise in the estimate
of its local structure that arises from the gradient operators that simard used to
extract the dimensional subspace .
we are also not limited to only the a priori
dimensions listed above , but learn the local structure from the training examples ,
which may contain other invariances .
although the manifold seems to be locally
linear and low dimensional , there is no reason for the manifold to be globally linear .
di ( cid : 123 ) erent styles of digits , and even a ( cid : 123 ) ne transformations that are not con ( cid : 123 ) ned to a
sub - pixel regime , will lead to di ( cid : 123 ) erent local linear patches .
we were therefore forced
to mix together numbers of linear models for images of each digit , ie to use a blended
linear approximation to the surface ( ( cid : 123 ) gure ) ( ) , ( ) , ( ) , ( ) , ( ) .
the mixture
is ( cid : 123 ) t using either an expectation - maximization ( em ) based algorithm ( ) or the k -
means algorithm , which is actually a limiting case of em .
the expectation ( e ) phase
involves assigning to the linear models responsibilities for the training examples; the
maximization ( m ) phase involves re - estimating the parameters of the linear models
in the light of this assignment .
this distance is not a metric since it does not satisfy the triangle inequality .
signi ( cid : 123 ) cant performance improvements can be achieved by using various speedup mechanisms ,
for example ( )
figure : illustration of how a highly non - linear one - dimensional surface ( left panel )
is captured by locally linear models ( right panel )
a convenient framework to describe our models comes from the version of neural
nets called autoencoders .
an autoencoder is a feedforward neural network with a
single hidden layer that attempts to reconstruct its input activities at its output .
hinton & zemel ( ) and zemel ( ) show how to understand the relationship between
statistical modelling and autoencoders .
the code for an example is given by the
combination of the activations of the hidden units and the output error , which is the
information necessary to reconstruct the output given these hidden unit activations
and the weights between the hidden units and the output units .
the cost of this code
is counted in bits , and is based on empirical prior distributions for the activations
of the hidden units and the reconstruction errors .
the main link between modelling
and autoencoders is that this cost function can be considered as the negative log
probability of the data under a particular generative model ( so maximum likelihood
model ( cid : 123 ) tting and minimum cost are equivalent .
we give cost functions below which
include or exclude various elements of the code cost .
two well established linear models are principal components analysis ( pca ) and fac -
tor analysis ( fa ) .
performing pca requires nothing more than singular value decom -
position of the covariance matrix of the examples .
performing fa is computationally
more challenging .
however , fa o ( cid : 123 ) ers a sounder statistical model of examples , and
one might expect it to be more pro ( cid : 123 ) cient .
section ii describes principal components
analysis; section iii describes factor analysis; section iv shows how to incorporate
some of the tangent information that simard et al ( ) use to such good e ( cid : 123 ) ect; and
section v shows how the models perform on a large - scale digit recognition task .
ii mixtures of principal component analysers
the r principal components of n examples xi = fx
ng of n - dimensional data ; : : : x
( assumed , without loss of generality , to have mean ) are the r orthogonal directions
in n - space which capture the greatest variation in the examples .
put another way , if
the examples are projected onto r - dimensional subspaces and the resulting variance of
the projected examples is measured , then the principal components de ( cid : 123 ) ne a subspace
such that this captured variance is the highest .
alternatively , assuming that the
examples come from a multivariate gaussian distribution , the information conveyed
by the magnitude of the projections onto these r directions is the greatest
properties , together with the computational simplicity of performing pca ( involving
no more than ( cid : 123 ) nding the top r eigenvectors of the covariance matrix of the examples )
make it an obvious candidate for the linear model in the mixture .
the r principal
components de ( cid : 123 ) ne the local surface assumed for the manifold .
in the context of an
autoencoder , the projections of the input along the r directions are the activities of
the hidden units hi = fh
rg for input pattern i .
this can be written as :
hi = rxi
as discussed above , the activities of the hidden units are part of the code for an
example .
the activities of the output units are generated by a linear combination of
the hidden units :
yi = ghi
the other part of the code is the di ( cid : 123 ) erence between the image itself and these output
activities .
the resulting squared reconstruction error ( e
i = kxi
) is a measure
of how well the model ( cid : 123 ) ts the image ( the smaller it is , the better the image was
as mentioned above , either an em or a k - means procedure is used to assign the
n examples among m di ( cid : 123 ) erent pca models ( we call these sub - models ) for each
during the e step , responsibility for each pattern is assigned amongst the
sub - models; during the m - step pca is performed , altering the parameters of a sub -
model appropriately to minimise the reconstruction cost of the data for which it is
responsible .
in the soft , em version , the responsibility of sub - model a for example
i is calculated as q
) where e
b is the squared reconstruction
error and ( cid : 123 )
acts like a temperature parameter .
in the hard , k - means version ( which
can be viewed as the limiting case as ( cid : 123 )
! ) , example i is allocated to the sub - model
a for which e
a is smallest .
note that the m step is more complicated and powerful
than just taking the means of those examples for which responsibility is taken .
formally , the k - means version of the algorithm is :
choose initial assignments among the sub - models for each example in the
training set ( typically at random , or using samples from the initial data ) ;
strictly speaking there is a third component , the model - cost , which encodes the cost of specifying
the weights in each model ( ) .
we assume that this cost is the same for all models .
perform pca separately for each sub - model;
reassign patterns to the sub - model that reconstructs them the best;
stop if no patterns have changed sub - model , otherwise return to step .
for the soft version , in step , the examples are weighted for the pca by the re -
sponsibilities , and convergence is assessed by examining the change in the overall
log - likelihood ( which is equivalent to negative code - cost using shannon optimal cod -
ing ) of the data at each iteration .
this log - likelihood is based on a model for the
image under which the ( incorrectly normalised ) log probability of image i is ( ) :
a log q
minus this quantity can be considered as a cost function for learning the assign -
ments of responsibilities and the principal components .
the value of ( cid : 123 )
altogether , either procedure generates a set of local linear models for each digit .
given a test pattern we evaluate the reconstruction errors against all the models for
all the digits .
we use a hard method for classi ( cid : 123 ) cation ( determining the identity of
the pattern only by the model which reconstructs it best .
the absolute quality of the
best reconstruction and the relative qualities of slightly sub - optimal reconstructions
are available to reject ambiguous cases .
somewhat similar methods have been used for modelling images of lips for lip reading
( ) , cartoon - like drawings ( ) , digit and character recognition ( ) , ( ) and data
iii mixtures of factor analysers
unfortunately , as noted above , pca does not provide a correct statistical model for
the images because it is not properly normalisable .
components of the image that
lie in the directions of the principal components generate no reconstruction error ,
and therefore can be added at will without changing the cost .
poggio & sung ( )
suggested using as a component of the cost a quantity they called the normalised
mahalanobis distance in the subspace of the principal components .
this is :
where c is a constant , h
as is the activity of hidden unit s for example i of sub -
model a and ( cid : 123 ) as is the eigenvalue associated with that component .
this amounts to
employing a zero mean hyper - elliptical gaussian prior aligned in the direction of the
principal components .
this prior is used to code the activation of the hidden units
in the autoencoder .
choosing the relative weighting in the overall cost function for
the squared reconstruction error e
a and the coding cost is somewhat arbitrary .
factor analysis ( fa ) is a di ( cid : 123 ) erent way of analysing the covariance matrix of the
inputs ( see ( ) , for an excellent introduction ) which starts from a proper probabilistic
model , and correctly blends the reconstruction cost and a term playing a similar role
to this normalised mahalanobis distance .
fa emerges for the same linear autoencoder
network if the cost of coding the hidden units is taken into account , using as a prior
a speci ( cid : 123 ) ed multivariate gaussian ( often just the identity matrix and traditionally
known as the prior for the factor loadings ) , and if the reconstruction errors are coded
according to an elliptical multivariate gaussian whose axes are aligned with the input
dimensions .
in terms of the notation introduced above , the standard factor analysis
model is written as ( ) :
yi = ghi + ( cid : 123 )
( cid : 123 ) n ( ; ( cid : 123 ) ) ; ( cid : 123 )
( cid : 123 ) n ( ; ( cid : 123 ) )
the hidden - output weights , g , are the factor loadings and the activities of the hidden
units , hi , are the factors ( the prior over which is gaussian ) .
a key assumption of the
fa model is that the observed variables are conditionally independent of each other ,
given the factors .
this is equivalent to noting that the individual components of the
j , are also independent of each other , or that ( cid : 123 ) is a diagonal matrix with
; : : : ; ( cid : 123 )
ng along the diagonal .
it is common to take ( cid : 123 ) to be the identity
matrix .
model ( ) implies that the covariance of the observed variables is given by :
c ( g; ( cid : 123 ) ) = gg
t + ( cid : 123 )
under the model , the sample covariance matrix ( s ) follows a wishart distribution
( ) about c and everitt ( ) introduces the function :
f ( s; c ( g; ( cid : 123 ) ) ) = lnjc j + trace ( sc
( cid : 123 ) ) ( cid : 123 ) lnjsj ( cid : 123 ) n
which , up to some constant factors , is the likelihood .
maximum likelihood fa ( cid : 123 ) ts
the parameters of the model g and ( cid : 123 ) by maximising equation ( ) .
unfortunately ,
there is no technique as computationally cheap as singular value decomposition for
determining the factors from a collection of images .
consider the example in ( cid : 123 ) gure in which inputs a and b are perfectly correlated
but have low variance and input c is independent but has much higher variance .
consider what happens if we allow only one hidden unit .
the principal component
will align perfectly with c and be orthogonal to a and b .
the factor , however , will
align perfectly with a and b and will be orthogonal to c .
the di ( cid : 123 ) erence between the
two methods is easy to understand in coding terms .
pca is equivalent to minimizing
the description length of the data if we make the following simplifying assumptions :
ignore the cost of communicating the model ( i . e .
the directions of the principal
ignore the cost of communicating the projections of each data point onto the
principal components ( i . e .
the cost of conveying the activities of the hidden
use a gaussian distribution with the same variance on each dimension as an
agreed prior for communicating the residual errors when each dimension of the
data point is reconstructed from the projections onto the principal components .
assumption makes it much cheaper to communicate input c by ( cid : 123 ) rst copying it
to the principal component .
but this is only because the cost of communicating
this component is ignored .
factor analysis has a somewhat more realistic coding
interpretation .
it still ignores the cost of communicating the factor loadings , but it
takes into account the cost of communicating the projections onto each factor
a data point , there is a multivariate posterior probability distribution across the factor
values and this distribution is typically not spherical .
the cost of communicating the
factor values is the kullback - leibler divergence between the posterior distribution
and the spherical prior distribution for the factor values .
nothing is gained by having
a factor that is just a copy of c because communicating this factor value is just as
expensive as communicating the full value of c as a residual error .
assumption is another important di ( cid : 123 ) erence between the two methods .
factor
analysis allows di ( cid : 123 ) erent variances to be used for coding the residual errors on di ( cid : 123 ) erent
input dimensions .
if the residuals are coded using a ( cid : 123 ) xed variance prior the average
information required to convey a residual is linear in its variance .
if , however , the
variance of the prior is adapted to match the variance of the residual the information
is proportional to the log of the variance .
this dramatically reduces the cost of
high variance residuals as compared with low variance ones and can make it better
to decrease two low variance residuals by a little rather than decreasing one high
variance residual by a lot .
figure illustrates how the use of di ( cid : 123 ) erent noise models for di ( cid : 123 ) erent input variables
can allow factor analysis to extract a more sensible model than pca .
the task in
( cid : 123 ) gure is to infer a measured length from noisy measurements .
clearly this is a
situation where we wish to extract correlations amongst the inputs .
in the example ,
three dimensional data were generated according to the rule shown , and the results of
performing pca and fa are illustrated by the projection of the generative ( hidden -
output ) weight vector ( solid line ) onto the x ( cid : 123 ) y plane .
( we have shown the -
dimensional projections , for clarity .
because of the symmetry in the way the data
was generated , the x ( cid : 123 ) z projection is similar , while the y ( cid : 123 ) z projection does not
illustrate the point being made here . ) in the low noise case , pca and fa extract
similar models , the generative weights show that equal attention is paid to both
the x and y dimensions .
however , in the high noise case , pca must have a large
weight to generate the large variance along the x dimension .
on the other hand , fa
correctly has recognized that all output dimensions have identical dependencies on
the hidden variable , s , and so sets the generational weights accordingly .
of course ,
the ( cid : 123 ) x component in ( cid : 123 ) ( equation ( ) ) is in ( cid : 123 ) ated to account for the large variance
in this input .
this example illustrates how fa can model covariance amongst input
dimensions separately from variance whereas pca cannot .
figure : ( a ) shows three inputs and a single hidden unit .
inputs a and b have low
variance but are perfectly correlated .
input c has high variance but is independent of
a and b .
the hidden unit behaves quite di ( cid : 123 ) erently in principal component analysis
and factor analysis .
( b ) scatterplot of the data with the vector of incoming weights
of the hidden unit for pca ( solid line ) and fa ( dotted line ) .
( cid : 123 ) x = :
( cid : 123 ) x = :
figure : noisy observations of a signal s , x = s + nx , y = s + ny , z = s + nz
where s , nx , ny and nz are normal random variables with zero mean .
the standard
deviation of s and ny nz are held constant at : and : respectively on all three
panels , while that of nx is as shown .
the projection of training examples onto
the x ( cid : 123 ) y plane are shown by the dots , while the solid lines are the projections of
hidden - output weight vectors onto this plane .
see text for further explanation .
for modelling digits , fa should be immune to the fact that di ( cid : 123 ) erent dimensions might
have di ( cid : 123 ) erent intrinsic amounts of noise and look for shared structure in digits
second di ( cid : 123 ) erence is that pca is rotationally symmetric whereas fa is not .
for fa ,
the particular dimensions used to describe the image are special in the sense that the
noise corrupting them is taken to be mutually independent .
this restriction seems
reasonable for images , since the axes de ( cid : 123 ) ned by the input pixels are indeed privileged .
following analysis by neal and dayan ( ) on the relationship between factor anal -
ysis , pca , autoencoders and the helmholtz machine ( ) , ( ) , we actually used an
autoencoder to implement factor analysis .
the linearity of the model implies that
the posterior distribution of the factors hi given the input xi is gaussian :
( cid : 123 ) n ( rxi
as rubin & thayer ( ) show in their discussion of the use of em for fa , the correct
values of r and ( cid : 123 ) are determined by g and ( cid : 123 ) as :
r = ( g
g + i ) ( cid : 123 )
( cid : 123 ) ( cid : 123 ) = i + g
following the helmholtz machine , we call r and ( cid : 123 ) parameters of the recognition
model , since they are responsible for producing the bottom - up receptive ( cid : 123 ) elds of the
hidden units .
studying the form of these recognition parameters can give insight into
the elements of the images that the factors code .
preliminary experiments with this model suggested that it was prone to over ( cid : 123 ) tting
in a very particular manner .
take a pixel on the outskirts of the ( cid : 123 ) grid .
it can
easily occur that for some digit the activity of this pixel is always in the training set
for some sub - model .
the factor analysis model might correctly decide that this pixel
shares nothing in common with the other pixels and furthermore that its intrinsic
noise level is .
if , in the test
set , by some quirk of the noise , there is activity in
this pixel , then the likelihood of the image under this sub - model will be .
this is
unreasonable , since the pixel is only conveying noise .
one way to regularise learning
is to impose a minimum allowable intrinsic variance ( this is a conventional way of
regularising mixtures of gaussian models ) ; another is to add diagonal terms to the
covariance matrix of the examples so it is as if they all su ( cid : 123 ) er from extra independent
these regularisation methods were roughly equally e ( cid : 123 ) cacious , and we adopt
the latter for the empirical studies below .
the full non - linear model uses a mixture of local factor analysers in the same way
that the non - linear pca model used a mixture of principal component analysers .
the same em - based method can be used to ( cid : 123 ) t the combined model , with the e -
phase assigning responsibilities for images to the factor analysers in a hard or soft
manner , and the m - phase adapting the generative model within in analyser according
to the new covariance matrix of the data for which it takes responsibility .
because
factor analysis is a genuine generative model we avoid the arbitrary choice of ( cid : 123 )
is required to apply em to a mixture of pca models .
iv tangent information
nearest neighbour methods o ( cid : 123 ) er simple , non - parametric , ways of discriminating be -
tween the digits .
however , the metric that is used to judge proximity can make a
substantial di ( cid : 123 ) erence to the quality of the resulting inference .
there are discrimi -
native and maximum likelihood ways to look at this issue .
for instance , hastie and
tibshirani ( ) choose a metric for the nearest neighbours at a point based on infor -
mation from local linear discriminant analysis ( emphasizing directions in which the
images from the di ( cid : 123 ) erent classes di ( cid : 123 ) er and downplaying directions in which they are
on the other hand , simard et al ( ) , in the method described in the introduction ,
used an approach that owes more to modelling the local structure of the classes .
their approach is based on prior information , known to be valid for the particular
task of digit modelling , that certain local manipulations of an image preserve the
identity of the digit that is described .
e ( cid : 123 ) ectively , each point is replaced by a local
low dimensional linear manifold , deformations in the direction of which incur no cost .
equivalently , each image is modelled as a local linear surface .
note that these local
surfaces are chosen to model the images of each digit as best as possible , and not to
support the best possible discrimination between them .
this local low - dimensional
and linear behaviour is what motivated our linear models .
schwenk and milgram
( ) , ( ) take a slightly di ( cid : 123 ) erent approach and compile down all knowledge about a
character into a single prototype trained to directly minimize the distance between
the prototype and the tangent planes around each of the training examples .
figure illustrates the idea .
imagine that the four points - portray in image space
di ( cid : 123 ) erent examples of the same digit , subject to some smooth transformation
in tangent distance , one could represent this curve using the points and their local
tangents ( thick lines ) .
however one might do better splitting it into three local linear
models rather than four ( model a ( just a line in this simple case ) averages the upper
part of the curve more e ( cid : 123 ) ectively than the combination of the two tangents at and
given just the points , one might also construct model b for and , which
would be unfortunate .
incorporating information about the tangents as well would
encourage the separation of these segments .
care should be taken in generalising this
picture to high dimensional spaces .
for both pca and fa , the output is linearly related to the input , ie yi = axi with
a = rg .
in order for the models to be tolerant to distortions along tangent direction
j indexes the direction , for example , horizontal and vertical translations , scalings and shears
figure : didactic example of tangent distance and local linear models .
see text for
ti;j requires that both xi + ( cid : 123 ) jti;j and xi
( cid : 123 ) ( cid : 123 ) jti;j be reconstructed well , where ( cid : 123 ) j is
a parameter that indicates how far along the tangent vector ti;j good reconstruction
should be enforced .
the overall reconstruction error is then proportional to
and so the model has to reconstruct the tangent vectors as well as the underlying
images .
the relative importance of the latter with respect to the former is controlled
a similar expression governs the code cost for the hidden units in fa .
an alternative way of viewing this prior is in terms of adding additional examples
to the training set that are generated by these manipulations .
each example would
be replaced by a gaussian cloud of ( cid : 123 ) ctitious examples .
adding these examples is
straightforward using methods such as pca and fa which are based on just the
covariance matrix of the inputs ( it amounts to adding into the covariance matrix the
a priori tangent vectors , weighted by an amount , ( cid : 123 ) j , that trades o ( cid : 123 ) the importance
of the underlying pattern and the importance of the invariances .
the one di ( cid : 123 ) erence
from just adding extra examples to the dataset is that we set the responsibilities of
each model for the tangent vectors according to how well the model reconstructed just
the parent image , since the intention is to force the local linear models to capture the
invariances themselves , using the tangents to shape the local structure within a sub -
if the database of training examples had just been expanded , this constraint
would not have been applied .
there are two di ( cid : 123 ) erences between this use of the tangent vectors and that in simard
et al ( ) .
one is that for us , the e ( cid : 123 ) ect of these tangents has a limited spatial ex -
tent , whereas for them , the linear manifold about each image extends to in ( cid : 123 ) nity
practice , in high dimensional image spaces , it is unlikely that images will have very
large projections within the tangent space and small projections o ( cid : 123 ) it .
the second
di ( cid : 123 ) erence is that simard et al consider tangent manipulations to the test image as
well as the training images ( this requires ( cid : 123 ) nding the closest approach between the
linear manifold about the test image and the linear manifolds about all the training
images .
it would be straightforward to do this during recognition for both pca and
fa; however doing it during learning is computationally more tricky ( ) .
simard et als metric would be irrelevant in the limit of very large numbers of training
images , since the database itself would contain all the transformations that actually
preserve digit identity .
in the same limit , the local linear pca and fa methods would
also not bene ( cid : 123 ) t from the tangents .
v experimental results
we have evaluated the e ( cid : 123 ) cacy of pca and fa based density models at classifying
images of handwritten digits .
for both pca and fa models , with and without tan -
gents , we proceed as follows : during the training phase , only images of one class ( say
images of s ) are presented to the mixture model .
the learning algorithm outlined
in section ii is executed and builds a mixture of linear sub - models for each class
order to save time , we initialize the search for fa models from the pca models
pca models the coding cost is just the reconstruction cost , while for the fa models ,
one form of the complete code cost for example i is obtained as the sum of coding
the reconstruction error and the code cost of the factors .
using gaussian models for
both , this can be expressed as :
+ trace ( ( cid : 123 ) ) + ( rxi ) t
j ( cid : 123 ) gj rx
j ) + g
j ( cid : 123 ) gji +
where gj is the j
th row of g .
a test image x is presented to all density models and
estimates of the class conditional probabilities , p ( xjk ) ; k = : : : are obtained
all incorrect classi ( cid : 123 ) cations are equally costly and if the prior belief is that all classes
are equally likely then the bayes decision rule stipulates that we should assign x to
the class k
( cid : 123 ) = argmaxkp ( xjk )
we used images from the cedar cdrom database of cities , states , zip codes ,
digits , and alphabetic characters ( ) .
the br training set of binary segmented dig -
its was subdivided into two sets of size , and , respectively .
the former
subset was used to train the density models and the latter subset was used as a cross -
validation set to allow us to choose various parameters such as the number of local
linear sub - models to use in the mixtures and the number of principal components
( factors ) in the pca ( fa ) models .
when building models that use tangent informa -
tion we are also free to specify the relative weightings ( ( cid : 123 ) j in equation ( ) ) .
in reality ,
we did not perform an exhaustive search for optimal values for all these parameters ,
but simply chose values that did reasonably well on the cross - validation images
the results reported here , we allowed up to principal components ( or factors ) in
each sub - model .
there were also sub - models in each mixture .
the cedar database includes two designated test sets .
the goodbs ( images )
set is a subset of the bs
( images ) set containing only well segmented digits .
after picking all the parameter values , we used all ; images to train a ( cid : 123 ) nal
version of the models which was used to evaluate performance on the test sets .
the binary images in the data set are of varying sizes , so we ( cid : 123 ) rst scaled them to
lie on an ( cid : 123 ) pixel grid and then smoothed with a gaussian ( cid : 123 ) lter with a standard
deviation of half a pixel .
figure illustrates reconstructions of two very di ( cid : 123 ) erent styles of s .
sub - models
which have specialized for one style reconstruct poorly images of the other style .
examples of the pca and fa models weights are shown in ( cid : 123 ) gures and respec -
tively .
in the pca models , the recognition and generative weight matrices are simply
transposes of each other while for the fa models they di ( cid : 123 ) er .
the performance of the di ( cid : 123 ) erent methods are presented in table .
there are no sig -
ni ( cid : 123 ) cant di ( cid : 123 ) erences between the performances of the di ( cid : 123 ) erent methods at the p < :
level on the bs
test set when compared pairwise using a two - tailed mcnemars
in comparing the columns of table , it is important to note that the
goodbs is a carefully chosen subset of the bs test data when poorly segmented dig -
this was usually su ( cid : 123 ) cient to explain at least % of the training set variance assigned to that
sub - model .
principal components that explained in excess of % were discarded to avoid over ( cid : 123 ) tting .
this follows because the weight vectors are mutually orthogonal and so the the generative model
inverse is simply its transpose .
its were manually removed ( ) .
the training data was also manually screened
is therefore reasonable to conclude by comparing the validation and goodbs perfor -
mances that the models did not over ( cid : 123 ) t .
figure : operation of the mixture of pca models on two di ( cid : 123 ) erent styles of s
( cid : 123 ) image is shown at the bottom .
the ( cid : 123 ) rst two rows show the reconstructions
from each of the sub - models in the density estimator trained on images of s .
attached to each sub - models reconstruction is its reconstruction cost relative to the
best sub - model ( which has cost of zero )
as a comparison , other state of the art methods obtain about % error rates ( ) on
the original data which had a mean size of around ( cid : 123 ) pixels .
thus the images
we used had areas about times smaller and so could well have lost information .
as a rough guide k - nearest neighbour has an error rate of : % on the bs test set .
this is signi ( cid : 123 ) cantly worse ( p < : ) than the performance obtained with the pca
with tangents method or the fa methods .
the k - nearest neighbour method requires
k = chosen on the basis of the validation set
figure : weights in the sub - models of the pca density model for the digit .
on the left are the means .
the upper row immediately to the right of the mean
shows the input - hidden weights ( recognition weights ) , while the lower row gives the
hidden - output weights ( generative weights ) .
of the order of n = , n dimensional ( in this case n = ) dot products to classify
an image in one of the last columns in table .
our local models ( pca or fa )
require of the order ( cid : 123 ) m ( cid : 123 ) r n - dimensional dot products for each density model .
since there are digits and we used m = r = , the density models require of
the ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) = or about a factor of ( cid : 123 ) ve times less computation than
a straightforward implementation of the k - nearest neighbour method .
the number
of dot products for the tangent distance method depends on the number of tangent
directions ( ) .
if tangent directions are used on these data then of the order of ( cid : 123 ) n
dot products are needed , or about a factor of times the local model version .
figure : weights in the sub - models of the fa density model for the digit
layout is the same as in ( cid : 123 ) gure .
we have constructed two di ( cid : 123 ) erent sorts of locally linear models of images of digits , one
based on principal components analysis and one on factor analysis , and have shown
that they can both perform well at digit discrimination .
these models illustrate three
major points .
first , linear models with just a few dimensions can be used to good
e ( cid : 123 ) ect for representing the local structure of the high dimensional complex manifolds
of the pixel images of digits .
second , allowing multiple components in the mixtures
can be very bene ( cid : 123 ) cial .
in general , this could be both for capturing grossly di ( cid : 123 ) erent
styles of the digits ( such as s with and without loops in their tails ) and for the e ( cid : 123 ) ects
of a ( cid : 123 ) ne transformations of the digits by more than about a single pixel , which have
pca + tangents
fa + tangents
table : percentage of images incorrectly classi ( cid : 123 ) ed by each of the methods
validation results were obtained when the density models were trained using the
; training examples .
the results in the last two columns were obtained with
models trained on all ; training images .
highly non - linear e ( cid : 123 ) ects on the observed images . third , a priori information about
the local structure of the manifolds that comes from knowledge about invariances
of digit identities over certain transformations ( ) is very easy to incorporate into
these linear models .
note that fa is just a particular way of limiting the number of
parameters that de ( cid : 123 ) ne the covariance matrix used to model data .
it is clear that if the manifold of the images is mostly di ( cid : 123 ) erentiable , then local lin -
ear models will do well at representing them .
using the em scheme for iterative
competitive clustering ( cid : 123 ) nds the sets of images that respect the local structure and
( cid : 123 ) nds the natural separation points between the distinct parts .
nearest neighbour
since we did not have enough data to ( cid : 123 ) t a very large number of local models for each digit , and
the digits were reasonably well normalised , the di ( cid : 123 ) erent local models were mostly due to di ( cid : 123 ) erent
schemes are the logical limit of such schemes , and should work best if one has so
many examples that the local structure of the manifold can be inferred by ( cid : 123 ) nding the
nearest point or points and ( cid : 123 ) tting a linear surface to them .
there are two potential
advantages to the em scheme .
first , it is using information from somewhat distant
images to determine the local directions in the manifold , allowing it to average away
the substantial noise that corrupts the images .
of course , if too much averaging is
done , then the directions could be systematically biased .
second , at recognition time ,
rather than having to search the entire training set to ( cid : 123 ) nd the patterns closest to the
new input , knowledge about these patterns has been pre - compiled into the limited
number of centres and the limited number of principle component or factor directions
associated with them .
recognition can therefore be quite fast .
in this study , we did not ( cid : 123 ) nd that much di ( cid : 123 ) erence between our two sorts of models
for the covariance structure within a linear patch .
one might have expected factor
analysis to have had better performance , since its prior model of the image genera -
tion process is more reasonable .
for fa , given the factors , any discrepancies between
the model and the image are independent from one pixel to the next .
pca uses a
spherical gaussian in the directions not covered by the retained components , and this
can amount to a complicated distribution over the pixels themselves .
also fa explic -
itly models covariance structure , whereas pca models both variance and covariance .
this advantage may have been nulli ( cid : 123 ) ed by our normalisation and regularisation pro -
cedures .
on the other hand , it is computationally much cheaper to perform pca
during the learning phase , although they are equally expensive during recognition .
both models perform soundly on these data .
finding a better way to regularise the
fa model against irrelevant pixels is important .
given its model , fa is completely
correct to assign a zero variance to outlying pixels that are silent throughout the
training set .
our prior knowledge that this might happen can be used to specify a
more complicated prior that makes it possible that pixels that are generally inactive
can occasionally be corrupted by noise .
we also found that the inclusion of tangent vectors did not substantially improve the
performance .
our use of tangent vectors is essentially an instantiation of tangent - prop
( ) , which constrains the output of the network to satisfy appropriate invariances
through its directional derivatives .
since our networks are linear , these directional
derivatives are particularly simple , allowing the tangent vectors just to be added into
the covariance matrices .
if the tangents about input xk were perfectly captured by a
linear model centred at that point , then the reconstruction error e
i for input xi would
be exactly the one - sided tangent distance from xi to the tangent space of input xk .
hastie & simard ( ) , developed a locally linear mixture model analogous to the one
described here , except using two - sided tangent distances during the whole of learning .
tangents would be expected not to help if there are enough data points that they
express directly all the actual invariances .
it is challenging to model the low dimensional manifolds of high dimensional pixel
images of digits in a computationally tractable manner .
our locally linear models
are designed to capture aspects of the short - range structure of the manifold and to
respect other knowledge about the digit modelling problem , such as the fact that
there are di ( cid : 123 ) erent styles of handwriting even for the same digit .
the models are
conceptually and computationally straightforward .
this research was funded by apple and by the ontario information technology research centre .
we thank patrice simard , zoubin ghahramani , rob tibshirani and yann le cun for helpful discussions .
geo ( cid : 123 ) rey hinton is the nesbitt - burns fellow of the canadian institute for advanced research .
