one way of modeling a complicated , high - dimensional data distribution is to use a large number of relatively simple probabilistic models and somehow combine the distributions specied by each model .
a well - known example of this approach is a mixture of gaussians in which each simple model is a gaussian , and the combination rule consists of taking a weighted arith - metic mean of the individual distributions .
this is equivalent to assuming an overall generative model in which each data vector is generated by rst choosing one of the individual generative models and then allowing that individual model to generate the data vector .
combining models by form - ing a mixture is attractive for several reasons .
it is easy to t mixtures of tractable models to data using expectation - maximization ( em ) or gradient ascent , and mixtures are usually considerably more powerful than their in - dividual components .
indeed , if sufciently many models are included in c ( cid : 123 ) 123 massachusetts institute of technology
neural computation 123 , 123 ( 123 )
geoffrey e .
hinton
the mixture , it is possible to approximate complicated smooth distributions
unfortunately , mixture models are very inefcient in high - dimensional spaces .
consider , for example , the manifold of face images .
it takes about 123 real numbers to specify the shape , pose , expression , and illumination of a face , and under good viewing conditions , our perceptual systems produce a sharp posterior distribution on this 123 - dimensional manifold .
this cannot be done using a mixture of models , each tuned in the 123 - dimensional manifold , because the posterior distribution cannot be sharper than the individual models in the mixture and the individual models must be broadly tuned to allow them to cover the 123 - dimensional manifold .
a very different way of combining distributions is to multiply them to - gether and renormalize .
high - dimensional distributions , for example , are often approximated as the product of one - dimensional distributions .
if the individual distributions are uni - or multivariate gaussians , their product will also be a multivariate gaussian so , unlike mixtures of gaussians , prod - ucts of gaussians cannot approximate arbitrary smooth distributions .
if , however , the individual models are a bit more complicated and each con - tains one or more latent ( i . e . , hidden ) variables , multiplying their distribu - tions together ( and renormalizing ) can be very powerful .
individual models of this kind will be called experts .
products of experts ( poe ) can produce much sharper distributions than the individual expert models .
for example , each expert model can con - strain a different subset of the dimensions in a high - dimensional space , and their product will then constrain all of the dimensions .
for modeling hand - written digits , one low - resolution model can generate images that have the approximate overall shape of the digit , and other more local models can ensure that small image patches contain segments of stroke with the correct ne structure .
for modeling sentences , each expert can enforce a nugget of linguistic knowledge .
for example , one expert could ensure that the tenses agree , one could ensure that there is number agreement between the subject and verb , and one could ensure that strings in which color adjectives follow size adjectives are more probable than the the reverse .
fitting a poe to data appears difcult because it appears to be necessary to compute the derivatives , with repect to the parameters , of the partition function that is used in the renormalization .
as we shall see , however , these derivatives can be nessed by optimizing a less obvious objective function than the log likelihood of the data .
123 learning products of experts by maximizing likelihood
we consider individual expert models for which it is tractable to compute the derivative of the log probability of a data vector with respect to the
training products of experts
parameters of the expert .
we combine n individual expert models as follows :
p ( d | 123 , .
, n ) = m fm ( d | m ) m fm ( c | m )
where d is a data vector in a discrete space , m is all the parameters of individual model m , fm ( d | m ) is the probability of d under model m , and c indexes all possible vectors in the data space . 123 for continuous data spaces , the sum is replaced by the appropriate integral .
for an individual expert to t the data well , it must give high probability to the observed data , and it must waste as little probability as possible on the rest of the data space .
a poe , however , can t the data well even if each expert wastes a lot of its probability on inappropriate regions of the data space , provided different experts waste probability in different regions .
the obvious way to t a poe to a set of observed independently and identically distributed ( i . i . d . ) data vectors 123 is to follow the derivative of the log likelihood of each observed vector , d , under the poe .
this is given by
log p ( d | 123 ,
= log fm ( d | m )
p ( c | 123 ,
log fm ( c | m )
the second term on the right - hand side of equation 123 is just the ex - pected derivative of the log probability of an expert on fantasy data , c , that is generated from the poe .
so assuming that each of the individual experts has a tractable derivative , the obvious difculty in estimating the derivative of the log probability of the data under the poe is generating correctly dis - tributed fantasy data .
this can be done in various ways .
for discrete data , it is possible to use rejection sampling .
each expert generates a data vec - tor independently , and this process is repeated until all the experts happen to agree .
rejection sampling is a good way of understanding how a poe species an overall probability distribution and how different it is from a causal model , but it is typically very inefcient .
gibbs sampling is typically much more efcient .
in gibbs sampling , each variable draws a sample from its posterior distribution given the current states of the other variables ( ge - man & geman , 123 ) .
given the data , the hidden states of all the experts can always be updated in parallel because they are conditionally indepen - dent .
this is an important consequence of the product formulation . 123 if the
will generally be a probability in this article .
123 so long as fm ( d | m ) is positive , it does not need to be a probability at all , though it 123 for time - series models , d is a whole sequence .
123 the conditional independence is obvious in the undirected graphical model of a poe because the only path between the hidden states of two experts is via the observed data .
geoffrey e .
hinton
figure 123 : a visualization of alternating gibbs sampling .
at time 123 , the visible variables represent a data vector , and the hidden variables of all the experts are updated in parallel with samples from their posterior distribution given the visible variables .
at time 123 , the visible variables are all updated to produce a re - construction of the original data vector from the hidden variables , and then the hidden variables are again updated in parallel .
if this process is repeated suf - ciently often , it is possible to get arbitrarily close to the equilibrium distribution .
the correlations ( cid : 123 ) sisj ( cid : 123 ) shown on the connections between visible and hidden variables are the statistics used for learning in rbms , which are described in
individual experts also have the property that the components of the data vector are conditionally independent given the hidden state of the expert , the hidden and visible variables form a bipartite graph , and it is possible to update all of the components of the data vector in parallel given the hidden states of all the experts .
so gibbs sampling can alternate between parallel updates of the hidden and visible variables ( see figure 123 ) .
to get an un - biased estimate of the gradient for the poe , it is necessary for the markov chain to converge to the equilibrium distribution .
unfortunately , even if it is computationally feasible to approach the equi - librium distribution before taking samples , there is a second , serious dif - culty .
samples from the equilibrium distribution generally have high vari - ance since they come from all over the models distribution .
this high vari - ance swamps the estimate of the derivative .
worse still , the variance in the samples depends on the parameters of the model .
this variation in the vari - ance causes the parameters to be repelled from regions of high variance even if the gradient is zero .
to understand this subtle effect , consider a horizon - tal sheet of tin that is resonating in such a way that some parts have strong vertical oscillations and other parts are motionless .
sand scattered on the tin will accumulate in the motionless areas even though the time - averaged gradient is zero everywhere .
123 learning by minimizing contrastive divergence
maximizing the log likelihood of the data ( averaged over the data distribu - tion ) is equivalent to minimizing the kullback - leibler divergence between the data distribution , p123 , and the equilibrium distribution over the visi -
training products of experts
, that is produced by prolonged gibbs sampling from the ble variables , p
p123 ( cid : 123 ) p
p123 ( d ) log p123 ( d ) = h ( p123 ) ( cid : 123 ) log p
p123 ( d ) log p
where ( cid : 123 ) denotes a kullback - leibler divergence , the angle brackets denote expectations over the distribution specied as a subscript , and h ( p123 ) is the entropy of the data distribution .
p123 does not depend on the parameters of the
model , so h ( p123 ) can be ignored during the optimization .
note that p is just another way of writing p ( d | 123 , .
equation 123 , averaged over the data distribution , can be rewritten as
where log fm is a random variable that could be written as log fm ( d | m ) with d itself being a random variable corresponding to the data .
there is a simple and effective alternative to maximum likelihood learning that elim - inates almost all of the computation required to get samples from the equi - librium distribution and also eliminates much of the variance that masks the gradient signal .
this alternative approach involves optimizing a differ - ent objective function .
instead of just minimizing p123 ( cid : 123 ) p
, we minimize the difference between p123 ( cid : 123 ) p
is the distribution over the one - step reconstructions of the data vectors generated by one full step of gibbs sampling ( see figure 123 ) .
the intuitive motivation for using this contrastive divergence is that we would like the markov chain that is implemented by gibbs sampling to leave the initial distribution p123 over the visible variables unaltered .
instead of running the chain to equilibrium and comparing the initial and nal derivatives , we can simply run the chain for one full step and then update the parameters to reduce the tendency of the chain to wander away from the initial distribution on the rst step .
because p123 is one step closer to the equilibrium distribution than p123 , we are guaranteed that p123 ( cid : 123 ) p
unless p123 equals p123 , so the contrastive divergence can never be negative .
also , for markov chains in which all transitions have nonzero implies p123 = p probability , p123 = p123
, because if the distribution does not
123 p123 is a natural way to denote the data distribution if we imagine starting a markov
chain at the data distribution at time 123
geoffrey e .
hinton
change at all on the rst step , it must already be at equilibrium , so the contrastive divergence can be zero only if the model is perfect . 123
another way of understanding contrastive divergence learning is to view it as a method of eliminating all the ways in which the poe model would like to distort the true data .
this is done by ensuring that , on average , the reconstruction is no more probable under the poe model than the original
the mathematical motivation for the contrastive divergence is that the in -
on the right - hand side of equation 123 cancels
tractable expectation over p
( p123 ( cid : 123 ) p
if each expert is chosen to be tractable , it is possible to compute the exact values of the derivative of log fm ( d | m ) for a data vector , d .
it is also straightforward to sample from p123 and p123 , so the rst two terms on the right - hand side of equation 123 are tractable .
by denition , the following procedure produces an unbiased sample from p123
pick a data vector , d , from the distribution of the data p123
compute , for each expert separately , the posterior probability distri - bution over its latent ( i . e . , hidden ) variables given the data vector ,
pick a value for each latent variable from its posterior distribution .
given the chosen values of all the latent variables , compute the con - ditional distribution over all the visible variables by multiplying to - gether the conditional distributions specied by each expert and renor -
pick a value for each visible variable from the conditional distribution .
these values constitute the reconstructed data vector , d .
the third term on the right - hand side of equation 123 represents the effect
of the change of the distribution of the one - step reconstructions caused by a change in m .
it is problematic to compute , but extensive simu - lations ( see section 123 ) show that it can safely be ignored because it is small
123 it is obviously possible to make the contrastive divergence small by using a markov chain that mixes very slowly , even if the data distribution is very far from the eventual equilibrium distribution .
it is therefore important to ensure mixing by using techniques such as weight decay that ensure that every possible visible vector has a nonzero proba - bility given the states of the hidden variables .
training products of experts
and seldom opposes the resultant of the other two terms .
the parameters of the experts can therefore be adjusted in proportion to the approximate derivative of the contrastive divergence :
this works very well in practice even when a single reconstruction of each data vector is used in place of the full probability distribution over re - constructions .
the difference in the derivatives of the data vectors and their reconstructions has some variance because the reconstruction procedure is stochastic .
but when the poe is modeling the data moderately well , the one - step reconstructions will be very similar to the data , so the variance will be very small .
the close match between a data vector and its reconstruction reduces sampling variance in much the same way as the use of matched pairs for experimental and control conditions in a clinical trial .
the low variance makes it feasible to perform on - line learning after each data vector is presented , though the simulations described in this article use mini - batch learning in which the parameter updates are based on the summed gradi - ents measured on a rotating subset of the complete training set . 123
there is an alternative justication for the learning algorithm in equa - tion 123 .
in high - dimensional data sets , the data nearly always lie on , or close to , a much lower - dimensional , smoothly curved manifold .
the poe needs to nd parameters that make a sharp ridge of log probability along the low - dimensional manifold .
by starting with a point on the manifold and ensuring that the typical reconstructions from the latent variables of all the experts do not have signicantly higher probability , the poe ensures that the probability distribution has the right local structure .
it is possible that the poe will accidentally assign high probability to other distant and unvisited parts of the data space , but this is unlikely if the log probability surface is smooth and both its height and its local curvature are constrained at the data points .
it is also possible to nd and eliminate such points by performing prolonged gibbs sampling without any data , but this is just a way of improving the learning and not , as in boltzmann machine learning , an essential part of it .
123 a simple example
poes should work very well on data distributions that can be factorized into a product of lower - dimensional distributions .
this is demonstrated in figures 123 and 123
there are 123 unigauss experts , each of which is a
123 mini - batch learning makes better use of the ability of matlab to vectorize across
geoffrey e .
hinton
figure 123 : each dot is a data point .
the data have been tted with a product of 123 experts .
the ellipses show the one standard deviation contours of the gaus - sians in each expert .
the experts are initialized with randomly located , circular gaussians that have about the same variance as the data .
the ve unneeded experts remain vague , but the mixing proportions , which determine the prior probability with which each of these unigauss experts selects its gaussian rather than its uniform , remain high .
mixture of a uniform distribution and a single axis - aligned gaussian .
in the tted model , each tight data cluster is represented by the intersection of two experts gaussians , which are elongated along different axes .
using a con - servative learning rate , the tting required 123 updates of the parameters .
for each update of the parameters , the following computation is performed on every observed data vector :
given the data , d , calculate the posterior probability of selecting the gaussian rather than the uniform in each expert and compute the rst term on the right - hand side of equation 123 .
for each expert , stochastically select the gaussian or the uniform ac - cording to the posterior .
compute the normalized product of the se - lected gaussians , which is itself a gaussian , and sample from it to get a reconstructed vector in the data space .
to avoid problems , there is one special expert that is constrained to always pick its gaussian .
compute the negative term in equation 123 using the reconstructed
123 learning a population code
a poe can also be a very effective model when each expert is quite broadly tuned on every dimension and precision is obtained by the intersection of a
training products of experts
figure 123 : three hundred data points generated by prolonged gibbs sampling from the 123 experts tted in figure 123
the gibbs sampling started from a random point in the range of the data and used 123 parallel iterations with annealing .
notice that the tted model generates data at the grid point that is missing in the real data .
large number of experts .
figure 123 shows what happens when the contrastive divergence learning algorithm is used to t 123 unigauss experts to 123 - dimensional synthetic images that each contain one edge .
the edges varied in their orientation , position , and intensities on each side of the edge .
the intensity prole across the edge was a sigmoid .
each expert also learned a variance for each pixel , and although these variances varied , individual experts did not specialize in a small subset of the dimensions .
given an image , about half of the experts have a high probability of picking their gaussian rather than their uniform .
the products of the chosen gaussians are excellent reconstructions of the image .
the experts at the top of figure 123 look like edge detectors in various orientations , positions , and polarities .
many of the experts farther down have even symmetry and are used to locate one end of an edge .
they each work for two different sets of edges that have opposite polarities and different positions .
123 initializing the experts
one way to initialize a poe is to train each expert separately , forcing the experts to differ by giving them different or differently weighted training cases or by training them on different subsets of the data dimensions , or by using different model classes for the different experts .
once each expert has been initialized separately , the individual probability distributions need to be raised to a fractional power to create the initial poe .
separate initialization of the experts seems like a sensible idea , but sim - ulations indicate that the poe is far more likely to become trapped in poor local optima if the experts are allowed to specialize separately .
better so - lutions are obtained by simply initializing the experts randomly with very vague distributions and using the learning rule in equation 123 .
geoffrey e .
hinton
figure 123 : ( a ) some 123 123 images that each contain a single intensity edge .
the location , orientation , and contrast of the edge all vary .
( b ) the means of all the 123 - dimensional gaussians in a product of 123 experts , each of which is a mixture of a gaussian and a uniform .
the poe was tted to 123 images of the type shown on the left .
the experts have been ordered by hand so that qualitatively similar experts are adjacent .
123 poes and boltzmann machines
the boltzmann machine learning algorithm ( hinton & sejnowski , 123 ) is theoretically elegant and easy to implement in hardware but very slow in networks with interconnected hidden units because of the variance prob - lems described in section 123
smolensky ( 123 ) introduced a restricted type of boltzmann machine with one visible layer , one hidden layer , and no intralayer connections .
freund and haussler ( 123 ) realized that in this re - stricted boltzmann machine ( rbm ) , the probability of generating a visible vector is proportional to the product of the probabilities that the visible vec - tor would be generated by each of the hidden units acting alone .
an rbm is therefore a poe with one expert per hidden unit . 123 when the hidden unit
123 boltzmann machines and poes are very different classes of probabilistic generative
model , and the intersection of the two classes is rbms .
training products of experts
of an expert is off , it species a separable probability distribution in which each visible unit is equally likely to be on or off .
when the hidden unit is on , it species a different factorial distribution by using the weight on its connection to each visible unit to specify the log odds that the visible unit is on .
multiplying together the distributions over the visible states speci - ed by different experts is achieved by simply adding the log odds .
exact inference of the hidden states given the visible data is tractable in an rbm because the states of the hidden units are conditionally independent given
the learning algorithm given by equation 123 is exactly equivalent to the standard boltzmann learning algorithm for an rbm .
consider the derivative of the log probability of the data with respect to the weight wij between a visible unit i and a hidden unit j .
the rst term on the right - hand side of equation 123 is
log fj ( d | wj )
= ( cid : 123 ) sisj ( cid : 123 ) d ( cid : 123 ) sisj ( cid : 123 ) p
where wj is the vector of weights connecting hidden unit j to the visible units , ( cid : 123 ) sisj ( cid : 123 ) d is the expected value of sisj when d is clamped on the visible units and sj is sampled from its posterior distribution given d , and ( cid : 123 ) sisj ( cid : 123 ) p is the expected value of sisj when alternating gibbs sampling of the hidden and visible units is iterated to get samples from the equilibrium distribution in a network whose only hidden unit is j .
the second term on the right - hand side of equation 123 is :
p ( c | w )
log fj ( c | wj )
where w is all of the weights in the rbm and ( cid : 123 ) sisj ( cid : 123 ) p is the expected value of sisj when alternating gibbs sampling of all the hidden and all the visible units is iterated to get samples from the equilibrium distribution of the rbm .
subtracting equation 123 from equation 123 and taking expectations over
the distribution of the data gives = ( p123 ( cid : 123 ) p
= ( cid : 123 ) sisj ( cid : 123 ) p123 ( cid : 123 ) sisj ( cid : 123 ) p
the time required to approach equilibrium and the high sampling vari - ance in ( cid : 123 ) sisj ( cid : 123 ) p
make learning difcult .
it is much more effective to use the approximate gradient of the contrastive divergence .
for an rbm , this approximate gradient is particularly easy to compute :
( p123 ( cid : 123 ) p
) ( cid : 123 ) sisj ( cid : 123 ) p123 ( cid : 123 ) sisj ( cid : 123 )
is the expected value of sisj when one - step reconstructions are clamped on the visible units and sj is sampled from its posterior distribution given the reconstruction ( see figure 123 ) .
geoffrey e .
hinton
123 learning the features of handwritten digits
when presented with real high - dimensional data , a restricted boltzmann machine trained to minimize the contrastive divergence using equation 123 should learn a set of probabilistic binary features that model the data well .
to test this conjecture , an rbm with 123 hidden units and 123 visible units was trained on 123 123 123 real - valued images of handwritten digits from all 123 classes .
the images , from the br training set on the usps cedar rom123 , were normalized in width and height , but they were highly variable in style .
the pixel intensities were normalized to lie between 123 and 123 so that they could be treated as probabilities , and equation 123 was modied to use probabilities in place of stochastic binary values for both the data and the
( p123 ( cid : 123 ) p
) ( cid : 123 ) pipj ( cid : 123 ) p123 ( cid : 123 ) pipj ( cid : 123 )
stochastically chosen binary states of the hidden units were still used for computing the probabilities of the reconstructed pixels , but instead of picking binary states for the pixels from those probabilities , the probabilities themselves were used as the reconstructed data vector .
it takes 123 hours in matlab 123 on a 123 mhz pentium ii workstation to perform 123 epochs of learning .
this is much faster than standard boltz - mann machine learning , comparable with the wake - sleep algorithm ( hin - ton , dayan , frey , & neal , 123 ) and considerably slower than using em to t a mixture model with the same number of parameters .
in each epoch , the weights were updated 123 times using the approximate gradient of the contrastive divergence computed on mini - batches of size 123 that contained 123 exemplars of each digit class .
the learning rate was set empirically to be about one - quarter of the rate that caused divergent oscillations in the pa - rameters .
to improve the learning speed further , a momentum method was used .
after the rst 123 epochs , the parameter updates specied by equa - tion 123 were supplemented by adding 123 times the previous update .
the poe learned localized features whose binary states yielded almost perfect reconstructions .
for each image , about one - third of the features were turned on .
some of the learned features had on - center off - surround recep - tive elds or vice versa , some looked like pieces of stroke , and some looked like gabor lters or wavelets .
the weights of 123 of the hidden units , se - lected at random , are shown in figure 123
training products of experts
figure 123 : the receptive elds of a randomly selected subset of the 123 hidden units in a poe that was trained on 123 images of digits with equal numbers from each class .
each block shows the 123 learned weights connecting a hidden unit to the pixels .
the scale goes from +123 ( white ) to 123 ( black ) .
123 using learned models of handwritten digits for discrimination
an attractive aspect of poes is that it is easy to compute the numerator in equation 123 , so it is easy to compute the log probability of a data vector up to an additive constant , log z , which is the log of the denominator in equation 123 .
unfortunately , it is very hard to compute this additive constant .
this does not matter if we want to compare only the probabilities of two different data vectors under the poe , but it makes it difcult to evaluate the model learned by a poe , because the obvious way to measure the success of learning is to sum the log probabilities that the poe assigns to test data vectors drawn from the same distribution as the training data but not used
for a novelty detection task , it would be possible to train a poe on nor - mal data and then to learn a single scalar threshold value for the unnormal -
geoffrey e .
hinton
ized log probabilities in order to optimize discrimination between normal data and a few abnormal cases .
this makes good use of the ability of a poe to perform unsupervised learning .
it is equivalent to naive bayesian classi - cation using a very primitive separate model of abnormal cases that simply returns the same learned log probability for all abnormal cases .
an alternative way to evaluate the learning procedure is to learn two different poes on different data sets such as images of the digit 123 and im - ages of the digit 123
after learning , a test image , t , is presented to poe123 and poe123 , and they compute log p ( t | 123 ) + log z123 and log p ( t | 123 ) + log z123 , re - spectively .
if the difference between log z123 and log z123 is known , it is easy to pick the most likely class of the test image , and since this difference is only a single number , it is quite easy to estimate it discriminatively using a set of validation images whose labels are known .
if discriminative performance is the only goal and all the relevant classes are known in advance , it is prob - ably sensible to train all the parameters of a system discriminatively .
in this article , however , discriminative performance on the digits is used simply as a way of demonstrating that unsupervised poes learn very good models of the individual digit classes .
figure 123 shows features learned by a poe that contains a layer of 123 hidden units and is trained on 123 images of the digit 123
figure 123 shows some previously unseen test images of 123s and their one - step reconstructions from the binary activities of the poe trained on 123s and from an identical poe trained on 123s .
figure 123 shows the unnormalized log probability scores of some training and test images under a model trained on 123 images of the digit 123 and a model trained on 123 images of the digit 123
unfortunately , the ofcial test set for the usps digits violates the standard assumption that test data should be drawn from the same distribution as the training data , so here the test images were drawn from the unused portion of the ofcial training set .
even for the previously unseen test images , the scores under the two models allow perfect discrimination .
to achieve this excellent separation , it was necessary to use models with two hidden layers and average the scores from two separately trained models of each digit class .
for each digit class , one model had 123 units in its rst hidden layer and 123 in its second hidden layer .
the other model had 123 in the rst hidden layer and 123 in the second .
the units in the rst hidden layer were trained without regard to the second hidden layer .
after training the rst hidden layer , the second hidden layer was trained using the probabilities of feature activation in the rst hidden layer as the data .
for testing , the scores provided by the two hidden layers were simply added together .
omitting the score provided by the second hidden layer leads to considerably worse separation of the digit classes on test data .
figure 123 shows the unnormalized log probability scores for previously unseen test images of 123s and 123s , which are the most difcult classes to discriminate .
discrimination is not perfect , but it is encouraging that all
training products of experts
figure 123 : the weights learned by 123 hidden units trained on 123 123 images of the digit 123
the scale goes from +123 ( white ) to 123 ( black ) .
note that the elds are mostly quite local .
a local feature like the one in column 123 , row 123 looks like an edge detector , but it is best understood as a local deformation of a template .
suppose that all the other active features create an image of a 123 that differs from the data in having a large loop whose top falls on the black part of the receptive eld .
by turning on this feature , the top of the loop can be removed and replaced by a line segment that is a little lower in the image .
of the errors are close to the decision boundary , so there are no condent
123 dealing with multiple classes .
if there are 123 different poes for the 123 digit classes , it is slightly less obvious how to use the 123 unnormalized scores of a test image for discrimination .
one possibility is to use a validation set to train a logistic discrimination network that takes the unnormalized log probabilities given by the poes and converts them into a probability distribution across the 123 labels .
figure 123 shows the weights in a logistic
geoffrey e .
hinton
figure 123 : the center row is previously unseen images of 123s .
( top ) pixel proba - bilities when the image is reconstructed from the binary activities of 123 feature detectors that have been trained on 123s .
( bottom ) pixel probabilities for recon - structions from the binary states of 123 feature detectors trained on 123s .
log scores of both models on training data
log scores under both models on test data
score under model123
score under model123
figure 123 : ( a ) the unnormalized log probability scores of the training images of the digits 123 and 123 under the learned poes for 123 and 123
( b ) the log probability scores for previously unseen test images of 123s and 123s .
note the good separation of the two classes .
discrimination network that is trained after tting 123 poe models to the 123 separate digit classes .
in order to see whether the second hidden layers were providing useful discriminative information , each poe provided two scores .
the rst score was the unnormalized log probability of the pixel intensities under a poe model that consisted of the units in the rst hidden layer .
the second score was the unnormalized log probability of the probabilities of activation of the rst layer of hidden units under a poe model that consisted of the units in the second hidden layer .
the weights in figure 123 show that the second layer of hidden units provides useful additional information .
training products of experts
figure 123 : the unnormalized log probability scores of the previously unseen test images of 123s and 123s .
although the classes are not linearly separable , all the errors are close to the best separating line , so there are no condent errors .
presumably this is because it captures the way in which features represented in the rst hidden layer are correlated .
a second - level model should be able to assign high scores to the vectors of hidden activities that are typical of the rst - level 123 model when it is given images of 123s and low scores to the hidden activities of the rst - level 123 model when it is given images that contain combinations of features that are not normally present at the same time in a 123
the error rate is 123% which compares very favorably with the 123% error rate of a simple nearest - neighbor classier on the same training and test sets and is about the same as the very best classier based on elastic models of the digits ( revow , williams , & hinton , 123 ) .
if 123% rejects are allowed ( by choosing an appropriate threshold for the probability level of the most probable class ) , there are no errors on the 123 test images .
several different network architectures were tried for the digit - specic poes , and the results reported are for the architecture that did best on the test data .
although this is typical of research on learning algorithms , the fact that test data were used for model selection means that the reported results are a biased estimate of the performance on genuinely unseen test images .
mayraz and hinton ( 123 ) report good comparative results for the larger mnist database , and they were careful to do all the model selection using subsets of the training data so that the ofcial test data were used only to measure the nal error rate .
geoffrey e .
hinton
figure 123 : the weights learned by doing multinomial logistic discrimination on the training data with the labels as outputs and the unnormalized log probability scores from the trained , digit - specic poes as inputs .
each column corresponds to a digit class , starting with digit 123
the top row is the biases for the classes .
the next 123 rows are the weights assigned to the scores that represent the log probability of the pixels under the model learned by the rst hidden layer of each poe .
the last 123 rows are the weights assigned to the scores that represent the log probabilities of the probabilities on the rst hidden layer under the model learned by the second hidden layer .
note that although the weights in the last 123 rows are smaller , they are still quite large , which shows that the scores from the second hidden layers provide useful , additional discriminative information .
note also that the scores produced by the 123 model provide useful evidence in favor of 123s .
123 how good is the approximation ?
the fact that the learning procedure in equation 123 gives good results in the simulations described in sections 123 , 123 , and 123 suggests that it is safe to ignore the nal term in the right - hand side of equation 123 that comes from the change in the distribution p123
to get an idea of the relative magnitude of the term that is being ig - nored , extensive simulations were performed using restricted boltzmann machines with small numbers of visible and hidden units .
by performing computations that are exponential in the number of hidden units and the
training products of experts
figure 123 : ( a ) a histogram of the improvements in the contrastive divergence as a result of using equation 123 to perform one update of the weights in each of 123 networks .
the expected values on the right - hand side of equation 123 were computed exactly .
the networks had eight visible and four hidden units .
the initial weights were randomly chosen from a gaussian with mean 123 and standard deviation 123
the training data were chosen at random .
( b ) the improvements in the log likelihood of the data for 123 networks chosen in exactly the same way as in figure 123a .
note that the log likelihood decreased in two cases .
the changes in the log likelihood are the same as the changes in p123 ( cid : 123 ) p
but with a
it is also possible to measure what happens to p123 ( cid : 123 ) p
number of visible units , it is possible to compute the exact values of ( cid : 123 ) sisj ( cid : 123 ) p123
when the approximation in equation 123 is used to update the weights by an amount that is large compared with the numerical precision of the machine but small compared with the curvature of the contrastive divergence .
the rbms used for these simulations had random training data and ran - dom weights .
they did not have biases on the visible or hidden units .
the main result can be summarized as follows : for an individual weight , the right - hand side of equation 123 , summed over all training cases , occasion - ally differs in sign from the left - hand side .
but for networks containing more than two units in each layer it is almost certain that a parallel update of all the weights based on the right - hand side of equation 123 will improve the contrastive divergence .
in other words , when we average over the train - ing data , the vector of parameter updates given by the right - hand side is almost certain to have a positive cosine with the true gradient dened by the left - hand side .
figure 123a is a histogram of the improvements in the contrastive divergence when equation 123 was used to perform one par - allel weight update in each of 123 , 123 networks .
the networks contained eight visible and four hidden units , and their weights were chosen from a gaussian distribution with mean zero and standard deviation 123
with smaller weights or larger networks , the approximation in equation 123 is
geoffrey e .
hinton
figure 123b shows that the learning procedure does not always improve the log likelihood of the training data , though it has a strong tendency to do so .
note that only 123 networks were used for this histogram .
figure 123 compares the contributions to the gradient of the contrastive divergence made by the right - hand side of equation 123 and by the term that is being ignored .
the vector of weight updates given by equation 123 makes the contrastive divergence worse if the dots in figure 123 are above the diagonal line , so it is clear that in these networks , the approximation in
equation 123 is quite safe .
intuitively , we expect p123 to lie between p123 and p
closer to p123 , the changes so when the parameters are changed to move p
should also move p123
the ignored changes in p123 an improvement in the contrastive divergence ( which is what figure 123
toward p123 and away from the previous position of p
should cause an increase in p123
it is tempting to interpret the learning rule in equation 123 as approximate
optimization of the contrastive log likelihood : ( cid : 123 ) p123 ( cid : 123 ) log p
unfortunately , the contrastive log likelihood can achieve its maximum value of 123 by simply making all possible vectors in the data space equally probable .
the contrastive divergence differs from the contrastive log like - lihood by including the entropies of the distributions p123 and p123 tion 123 ) , and so the high entropy of p123 rules out the solution in which all possible data vectors are equiprobable .
123 other types of expert
binary stochastic pixels are not unreasonable for modeling preprocessed images of handwritten digits in which ink and background are represented as 123 and 123
in real images , however , there is typically very high mutual infor - mation between the real - valued intensity of one pixel and the real - valued intensities of its neighbors .
this cannot be captured by models that use bi - nary stochastic pixels because a binary pixel can never have more than 123 bit of mutual information with anything .
it is possible to use multinomial pix - els that have n discrete values .
this is a clumsy solution for images because it fails to capture the continuity and one - dimensionality of pixel intensity , though it may be useful for other types of data .
a better approach is to imagine replicating each visible unit so that a pixel corresponds to a whole set of binary visible units that all have identical weights to the hidden units .
the number of active units in the set can then approximate a real - valued in - tensity .
during reconstruction , the number of active units will be binomially distributed , and because all the replicas have the same weights , the single probability that controls this binomial distribution needs to be computed only once .
the same trick can be used to allow replicated hidden units to
training products of experts
figure 123 : a scatter plot that shows the relative magnitudes of the modeled and unmodeled effects of a parallel weight update on the contrastive divergence .
the 123 networks used for this gure have 123 visible and 123 hidden units , and their weights are drawn from a zero - mean gaussian with a standard deviation of 123
the horizontal axis shows ( p123 ( cid : 123 ) p ) ( p123 ( cid : 123 ) p
( new ) ) where old and new denote the distributions before and after the weight update .
this modeled reduction in the contrastive divergence differs from the true reduction because it ignores the fact that changing the weights changes the distribution of the one - step reconstructions .
the increase in the ( new ) ( cid : 123 ) p
contrastive divergence due to the ignored term , p123 is plotted on the vertical axis .
points above the diagonal line would correspond to cases in which the unmodeled increase outweighed the modeled decrease , so that the net effect was to make the contrastive divergence worse .
note that the unmodeled effects almost always cause an additional improvement in the contrastive divergence rather than being in conict with the modeled effects .
approximate real values using binomially distributed integer states .
a set of replicated units can be viewed as a computationally cheap approximation to units whose weights actually differ , or it can be viewed as a stationary approximation to the behavior of a single unit over time , in which case the number of active replicas is a ring rate .
teh and hinton ( 123 ) have shown that this type of rate coding can be quite effective for modeling real - valued images of faces , provided the images are normalized .
an alternative to replicating hidden units is to use unifactor experts that each consist of a mixture of a uniform distribution and a factor analyzer
geoffrey e .
hinton
with just one factor .
each expert has a binary latent variable that species whether to use the uniform or the factor analyzer and a real - valued latent variable that species the value of the factor ( if it is being used ) .
the factor analyzer has three sets of parameters : a vector of factor loadings that spec - ify the direction of the factor in image space , a vector of means in image space , and a vector of variances in image space . 123 experts of this type have been explored in the context of directed acyclic graphs ( hinton , sallans , & ghahramani , 123 ) , but they should work better in a poe .
an alternative to using a large number of relatively simple experts is to make each expert as complicated as possible , while retaining the ability to compute the exact derivative of the log likelihood of the data with respect to the parameters of an expert .
in modeling static images , for example , each expert could be a mixture of many axis - aligned gaussians .
some experts might focus on one region of an image by using very high variances for pixels outside that region .
but so long as the regions modeled by different experts overlap , it should be possible to avoid block boundary artifacts .
123 products of hidden markov models .
hidden markov models ( hmms ) are of great practical value in modeling sequences of discrete symbols or sequences of real - valued vectors because there is an efcient algorithm for updating the parameters of the hmm to improve the log like - lihood of a set of observed sequences .
hmms are , however , quite limited in their generative power because the only way that the portion of a string generated up to time t can constrain the portion of the string generated after time t is by the discrete hidden state of the generator at time t .
so if the rst part of a string has , on average , n bits of mutual information with the rest of the string , the hmm must have 123n hidden states to convey this mutual in - formation by its choice of hidden state .
this exponential inefciency can be overcome by using a product of hmms as a generator .
during generation , each hmm gets to pick a hidden state at each time so the mutual informa - tion between the past and the future can be linear in the number of hmms .
it is therefore exponentially more efcient to have many small hmms than one big one .
however , to apply the standard forward - backward algorithm to a product of hmms , it is necessary to take the cross - product of their state - spaces , which throws away the exponential win .
for products of hmms to be of practical signicance , it is necessary to nd an efcient way to train
andrew brown ( brown & hinton , 123 ) has shown that the learning algorithm in equation 123 works well .
the forward - backward algorithm is used to get the gradient of the log likelihood of an observed or reconstructed
123 the last two sets of parameters are exactly equivalent to the parameters of a uni - gauss expert introduced in section 123 , so a unigauss expert can be considered to be a mixture of a uniform with a factor analyzer that has no factors .
training products of experts
figure 123 : a hidden markov model .
the rst and third nodes have output dis - tributions that are uniform across all words .
if the rst node has a high transition probability to itself , most strings of english words are given the same low prob - ability by this expert .
strings that contain the word shut followed directly or indirectly by the word up have higher probability under this expert .
sequence with respect to the parameters of an individual expert .
the one - step reconstruction of a sequence is generated as follows :
given an observed sequence , use the forward - backward algorithm in each expert separately to calculate the posterior probability distribu - tion over paths through the hidden states .
for each expert , stochastically select a hidden path from the posterior
given the observed sequence .
at each time step , select an output symbol or output vector from the product of the output distributions specied by the selected hidden state of each hmm .
if more realistic products of hmms can be trained successfully by mini - mizing the contrastive divergence , they should be better than single hmms for many different kinds of sequential data .
consider , for example , the hmm shown in figure 123
this expert concisely captures a nonlocal regularity .
a single hmm that must also model all the other regularities in strings of english words could not capture this regularity efciently because it could not afford to devote its entire memory capacity to remembering whether the word shut had already occurred in the string .
there have been previous attempts to learn representations by adjusting parameters to cancel out the effects of brief iteration in a recurrent network ( hinton & mcclelland , 123; oreilly , 123; seung , 123 ) , but these were
geoffrey e .
hinton
not formulated using a stochastic generative model and an appropriate
minimizing contrastive divergence has an unexpected similarity to the learning algorithm proposed by winston ( 123 ) .
winstons program com - pared arches made of blocks with near misses supplied by a teacher , and it used the differences in its representations of the correct and incorrect arches to decide which aspects of its representation were relevant .
by using a stochastic generative model , we can dispense with the teacher , but it is still the differences between the real data and the near misses generated by the model that drive the learning of the signicant features .
123 logarithmic opinion pools .
the idea of combining the opinions of multiple different expert models by using a weighted average in the log probability domain is far from new ( genest & zidek , 123; heskes , 123 ) , but research has focused on how to nd the best weights for combining experts that have already been learned or programmed separately ( berger , della pietra , & della pietra , 123 ) rather than training the experts cooperatively .
the geometric mean of a set of probability distributions has the attractive property that its kullback - leibler divergence from the true distribution , p , is smaller than the average of the kullback - leibler divergences of the individual distributions , q :
p ( cid : 123 ) mqwm
wmkl ( p ( cid : 123 ) qm )
where the wm are nonnegative and sum to 123 , and z = ( cid : 123 ) m ( c ) .
when all of the individual models are identical , z = 123
otherwise , z is less than one , and the difference between the two sides of equation 123 is log ( 123 / z ) .
this makes it clear that the benet of combining very different experts comes from the fact that they make z small , especially in parts of the data space that do not contain data .
it is tempting to augment poes by giving each expert , m , an additional adaptive parameter , wm , that scales its log probabilities .
however , this makes inference much more difcult ( yee - whye teh , personal communication , 123 ) .
consider , for example , an expert with wm = 123
this is equivalent to having 123 copies of an expert but with their latent states all tied together , and this tying affects the inference .
it is easier to x wm = 123 and allow the poe learning algorithm to determine the appropriate sharpness of the
123 relationship to boosting .
within the supervised learning litera - ture , methods such as bagging or boosting attempt to make experts dif - ferent from one another by using different , or differently weighted , training data .
after learning , these methods use the weighted average of the out - puts of the individual experts .
the way in which the outputs are combined
training products of experts
is appropriate for a conditional poe model in which the output of an indi - vidual expert is the mean of a gaussian , its weight is the inverse variance of the gaussian , and the combined output is the mean of the product of all the individual gaussians .
zemel and pitassi ( 123 ) have recently shown that this view allows a version of boosting to be derived as a greedy method of optimizing a sensible objective function .
the fact that boosting works well for supervised learning suggests that it should be tried for unsupervised learning .
in boosting , the experts are learned sequentially , and each new expert is tted to data that have been reweighted so that data vectors that are modeled well by the existing ex - perts receive very little weight and hence make very little contribution to the gradient for the new expert .
a poe ts all the experts at once , but it achieves an effect similar to reweighting by subtracting the gradient of the reconstructed data so that there is very little learning on data that are al - ready modeled well .
one big advantage of a poe over boosting is that a poe can focus the learning on the parts of a training vector that have high re - construction errors .
this allows poes to learn local features .
with boosting , an entire training case receives a single scalar weight rather than a vector of reconstruction errors , so when boosting is used for unsupervised learning , it does not so easily discover local features .
123 comparison with directed acyclic graphical models .
inference in a poe is trivial because the experts are individually tractable and the prod - uct formulation ensures that the hidden states of different experts are condi - tionally independent given the data . 123 this makes them relevant as models of biological perceptual systems , which must be able to do inference very rapidly .
alternative approaches based on directed acyclic graphical models ( neal , 123 ) suffer from the explaining - away phenomenon .
when such graphical models are densely connected , exact inference is intractable , so it is necessary to resort to clever but implausibly slow iterative techniques for approximate inference ( saul & jordan , 123 ) or to use crude approxima - tions that ignore explaining away during inference and rely on the learning algorithm to nd representations for which the shoddy inference technique is not too damaging ( hinton et al . , 123 ) .
unfortunately , the ease of inference in poes is balanced by the difculty of generating fantasy data from the model .
this can be done trivially in one ancestral pass in a directed acyclic graphical model but requires an iterative procedure such as gibbs sampling in a poe .
if , however , equation 123 is used for learning , the difculty of generating samples from the model is not a
in addition to the ease of inference that results from the conditional in - dependence of the experts given the data , poes have a more subtle ad -
123 this ceases to be true when there are missing data .
geoffrey e .
hinton
vantage over generative models that work by rst choosing values for the latent variables and then generating a data vector from these latent values .
if such a model has a single hidden layer and the latent variables have independent prior distributions , there will be a strong tendency for the posterior values of the latent variables in a well - tted model to reect the marginal independence that occurs when the model generates data . 123 for this reason , there has been little success with attempts to learn such gen - erative models one hidden layer at a time in a greedy , bottom - up way .
with poes , however , even though the experts have independent priors , the latent variables in different experts will be marginally dependent : they can be highly correlated across cases even for fantasy data generated by the poe itself .
so after the rst hidden layer has been learned greedily , there may still be lots of statistical structure in the latent variables for the second hidden layer to capture .
there is therefore some hope that an en - tirely unsupervised network could learn to extract a hierarchy of repre - sentations from patches of real images , but progress toward this goal re - quires an effective way of dealing with real - valued data .
the types of ex - pert that have been investigated so far have not performed as well as inde - pendent component analysis at extracting features like edges from natural
the most attractive property of a set of orthogonal basis functions is that it is possible to compute the coefcient on each basis function separately without worrying about the coefcients on other basis functions .
a poe retains this attractive property while allowing nonorthogonal experts and a nonlinear generative model .
123 lateral connections between hidden units .
the ease of inferring the states of the hidden units ( latent variables ) given the data is a major advantage of poes .
this advantage appears to be lost if the latent variables of different experts are allowed to interact directly .
if , for example , there are direct , symmetrical connections between the hidden units in a restricted boltzmann machine , it becomes intractable to infer the exact distribution over hidden states when given the visible states .
however , preliminary simulations show that the contrastive divergence idea can still be applied successfully using a damped mean - eld method to nd an approximating distribution over the hidden units for both the data vector and its recon - struction .
the learning procedure then has the following steps for each data
123 this is easy to understand from a coding perspective in which the data are commu - nicated by rst specifying the states of the latent variables under an independent prior and then specifying the data given the latent states .
if the latent states are not marginally in - dependent , this coding scheme is inefcient , so pressure toward coding efciency creates pressure toward independence .
training products of experts
with the data vector clamped on the visible units , compute the total bottom - up input , xj , to each hidden unit , j , and initialize it to have real - valued state , q123
= ( xj ) , where ( . ) is the logistic function .
perform 123 damped mean - eld iterations in which each hidden unit
computes its total lateral input yt j at time t ,
and then adjusts its state , qt
+ 123 ( xj + yt
also measure q123
for each pair of a visible unit , i , and a hidden unit , j .
for all pairs of hidden units , j , k .
pick a nal binary state for each hidden unit , j , according to q123 use these binary states to generate a reconstructed data vector .
perform one more damped mean - eld iteration on the hidden units starting from the nal state found with the data , but using the bottom - up input from the reconstruction rather than the data .
measure the same statistics as above .
update every weight in proportion to the difference in the measured
statistics in steps 123 and 123
it is necessary for the mean - eld iterations to converge for this proce - dure to work , which may require limiting or decaying the weights on the lateral connections .
the usual worrythat the mean - eld approximation is hopeless for multimodal distributionsmay not be relevant for learn - ing a perceptual system , since it makes sense to avoid learning models of the world in which there are multiple , very different interpretations of the same sensory input .
contrastive divergence learning nesses the problem of nding the highly multimodal equilibrium distribution that results if both visible and hidden units are unclamped .
it does this by assuming that the observed data cover all of the different modes with roughly the right frequency for each mode .
the fact that the initial simulations work is very promising since net - works with multiple hidden layers can always be viewed as networks that have a single laterally connected hidden layer with many of the lateral and feedforward connections missing .
however , the time taken for the net - work to settle means that networks with lateral connections are considerably slower than purely feedforward networks , and more research is required to demonstrate that the loss of speed is worth the extra representational
geoffrey e .
hinton
a much simpler way to use xed lateral interactions is to divide the hidden units into mutually exclusive pools .
within each pool , exactly one hidden unit is allowed to turn on , and this unit is chosen using the softmax
pj = exp ( xj )
curiously , this type of probabilistic winner - take - all competition among the hidden units has no effect whatsoever on the contrastive divergence learning rule .
the learning does not even need to know which hidden units compete with each other .
the competition can be implemented in a very simple way by making each hidden unit , j , be a poisson process with a rate of exp ( xj ) .
the rst unit to emit a spike is the winner .
the other units in the pool could then be shut off by an inhibitory unit that is excited by the winner .
if the pool is allowed to produce a xed number of spikes before being shut off , the learning rule remains unaltered , but quantities like sj may take on integer values larger than 123
if a hidden unit is allowed to be a member of several different overlapping pools , the analysis gets much more difcult , and more research is needed .
123 relationship to analysis - by - synthesis .
poes provide an efcient instantiation of the old psychological idea of analysis - by - synthesis .
this idea never worked properly because the generative models were not selected to make the analysis easy .
in a poe , it is difcult to generate data from the generative model , but given the model , it is easy to compute how any given data vector might have been generated , and , as we have seen , it is relatively easy to learn the parameters of the generative model .
paradoxically , the generative models that are most appropriate for analysis - by - synthesis may be the ones in which synthesis is intractable .
this research was funded by the gatsby charitable foundation .
thanks to zoubin ghahramani , david mackay , peter dayan , radford neal , david lowe , yee - whye teh , guy mayraz , andy brown , and other members of the gatsby unit for helpful discussions and to the referees for many helpful comments that improved the article .
