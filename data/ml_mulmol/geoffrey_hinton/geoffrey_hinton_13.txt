we show how to use complementary priors to eliminate the explaining away effects that make in densely - connected belief nets that have many hidden layers .
using com - plementary priors , we derive a fast , greedy algo - rithm that can learn deep , directed belief networks one layer at a time , provided the top two lay - ers form an undirected associative memory .
the fast , greedy algorithm is used to initialize a slower learning procedure that ne - tunes the weights us - ing a contrastive version of the wake - sleep algo - rithm .
after ne - tuning , a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit im - ages and their labels .
this generative model gives better digit classication than the best discrimi - native learning algorithms .
the low - dimensional manifolds on which the digits lie are modelled by long ravines in the free - energy landscape of the top - level associative memory and it is easy to ex - plore these ravines by using the directed connec - tions to display what the associative memory has
learning is difcult in densely - connected , directed belief nets that have many hidden layers because it is difcult to infer the conditional distribution of the hidden activities when given a data vector .
variational methods use simple approximations to the true conditional distribution , but the approximations may be poor , especially at the deepest hidden layer where the prior assumes independence .
also , variational learning still requires all of the parameters to be learned together and makes the learning time scale poorly as the number of param -
we describe a model in which the top two hidden layers form an undirected associative memory ( see gure 123 ) and the
( cid : 123 ) to appear in neural computation 123
remaining hidden layers form a directed acyclic graph that converts the representations in the associative memory into observable variables such as the pixels of an image .
this hy - brid model has some attractive features :
there is a fast , greedy learning algorithm that can nd a fairly good set of parameters quickly , even in deep networks with millions of parameters and many hidden
the learning algorithm is unsupervised but can be ap - plied to labeled data by learning a model that generates both the label and the data .
there is a ne - tuning algorithm that learns an excel - lent generative model which outperforms discrimina - tive methods on the mnist database of hand - written
the generative model makes it easy to interpret the dis -
tributed representations in the deep hidden layers .
the inference required for forming a percept is both fast
the learning algorithm is local :
adjustments to a synapse strength depend only on the states of the pre - synaptic and post - synaptic neuron .
the communication is simple : neurons only need to
communicate their stochastic binary states .
section 123 introduces the idea of a complementary prior which exactly cancels the explaining away phenomenon that makes inference difcult in directed models .
an exam - ple of a directed belief network with complementary priors is presented .
section 123 shows the equivalence between re - stricted boltzmann machines and innite directed networks with tied weights .
section 123 introduces a fast , greedy learning algorithm for constructing multi - layer directed networks one layer at a time .
using a variational bound it shows that as each new layer is added , the overall generative model improves .
the greedy algorithm bears some resemblance to boosting in its repeated use of the same weak learner , but instead of re - weighting each data - vector to ensure that the next step learns something new , it re - represents it .
the weak learner that
123 top - level units
123 label units
this could be the top level of
123 x 123
figure 123 : the network used to model the joint distribution of digit images and digit labels .
in this paper , each training case consists of an image and an explicit class label , but work in progress has shown that the same learning algorithm can be used if the labels are replaced by a multilayer pathway whose inputs are spectrograms from multiple different speak - ers saying isolated digits .
the network then learns to generate pairs that consist of an image and a spectrogram of the same
is used to construct deep directed nets is itself an undirected
section 123 shows how the weights produced by the fast greedy algorithm can be ne - tuned using the up - down al - gorithm .
this is a contrastive version of the wake - sleep al - gorithm hinton et al .
( 123 ) that does not suffer from the mode - averaging problems that can cause the wake - sleep al - gorithm to learn poor recognition weights .
section 123 shows the pattern recognition performance of a network with three hidden layers and about 123 million weights on the mnist set of handwritten digits .
when no knowledge of geometry is provided and there is no special preprocessing , the generalization performance of the network is 123% errors on the 123; 123 digit ofcial test set .
this beats the 123% achieved by the best back - propagation nets when they are not hand - crafted for this particular application .
it is also slightly better than the 123% errors reported by decoste and schoelkopf ( 123 ) for support vector machines on the
finally , section 123 shows what happens in the mind of the network when it is running without being constrained by vi - sual input .
the network has a full generative model , so it is easy to look into its mind we simply generate an image from its high - level representations .
throughout the paper , we will consider nets composed of
figure 123 : a simple logistic belief net containing two inde - pendent , rare causes that become highly anti - correlated when we observe the house jumping .
the bias of ( cid : 123 ) 123 on the earth - quake node means that , in the absence of any observation , this node is e123 times more likely to be off than on .
if the earth - quake node is on and the truck node is off , the jump node has a total input of 123 which means that it has an even chance of being on .
this is a much better explanation of the observation that the house jumped than the odds of e ( cid : 123 ) 123 which apply if neither of the hidden causes is active .
but it is wasteful to turn on both hidden causes to explain the observation because the probability of them both happening is e ( cid : 123 ) 123 ( cid : 123 ) e ( cid : 123 ) 123 = e ( cid : 123 ) 123
when the earthquake node is turned on it explains away the evidence for the truck node .
stochastic binary variables but the ideas can be generalized to other models in which the log probability of a variable is an additive function of the states of its directly - connected neigh - bours ( see appendix a for details ) .
123 complementary priors
the phenomenon of explaining away ( illustrated in gure 123 ) makes inference difcult in directed belief nets .
in densely connected networks , the posterior distribution over the hid - den variables is intractable except in a few special cases such as mixture models or linear models with additive gaussian noise .
markov chain monte carlo methods ( neal , 123 ) can be used to sample from the posterior , but they are typically very time consuming .
variational methods ( neal and hinton , 123 ) approximate the true posterior with a more tractable distribution and they can be used to improve a lower bound on the log probability of the training data .
it is comforting that learning is guaranteed to improve a variational bound even when the inference of the hidden states is done incorrectly , but it would be much better to nd a way of eliminating ex - plaining away altogether , even in models whose hidden vari - ables have highly correlated effects on the visible variables .
it is widely assumed that this is impossible .
a logistic belief net ( neal , 123 ) is composed of stochas - tic binary units .
when the net is used to generate data , the
probability of turning on unit i is a logistic function of the states of its immediate ancestors , j , and of the weights , wij , on the directed connections from the ancestors :
p ( si = 123 ) =
123 + exp ( ( cid : 123 ) bi ( cid : 123 ) pj sjwij )
where bi is the bias of unit i .
if a logistic belief net only has one hidden layer , the prior distribution over the hidden variables is factorial because their binary states are chosen independently when the model is used to generate data .
the non - independence in the posterior distribution is created by the likelihood term coming from the data .
perhaps we could eliminate explaining away in the rst hidden layer by using extra hidden layers to create a complementary prior that has exactly the opposite correlations to those in the likeli - hood term .
then , when the likelihood term is multiplied by the prior , we will get a posterior that is exactly factorial .
it is not at all obvious that complementary priors exist , but gure 123 shows a simple example of an innite logistic belief net with tied weights in which the priors are complementary at every hidden layer ( see appendix a for a more general treatment of the conditions under which complementary priors exist ) .
the use of tied weights to construct complementary priors may seem like a mere trick for making directed models equiva - lent to undirected ones .
as we shall see , however , it leads to a novel and very efcient learning algorithm that works by progressively untying the weights in each layer from the weights in higher layers .
123 an innite directed model with tied weights
we can generate data from the innite directed net in g - ure 123 by starting with a random conguration at an innitely deep hidden layer123 and then performing a top - down ances - tral pass in which the binary state of each variable in a layer is chosen from the bernoulli distribution determined by the top - down input coming from its active parents in the layer above .
in this respect , it is just like any other directed acyclic belief net .
unlike other directed nets , however , we can sam - ple from the true posterior distribution over all of the hidden layers by starting with a data vector on the visible units and then using the transposed weight matrices to infer the fac - torial distributions over each hidden layer in turn .
at each hidden layer we sample from the factorial posterior before computing the factorial posterior for the layer above123
ap - pendix a shows that this procedure gives unbiased samples because the complementary prior at each layer ensures that the posterior distribution really is factorial .
since we can sample from the true posterior , we can com - pute the derivatives of the log probability of the data
123the generation process converges to the stationary distribution of the markov chain , so we need to start at a layer that is deep compared with the time it takes for the chain to reach equilibrium .
123this is exactly the same as the inference procedure used in the wake - sleep algorithm ( hinton et al . , 123 ) for the models described in this paper no variational approximation is required because the inference procedure gives unbiased samples .
us start by computing the derivative for a generative weight , ij , from a unit j in layer h123 to unit i in layer v123 ( see gure 123 ) .
in a logistic belief net , the maximum likelihood learning rule for a single data - vector , v123 , is :
@ log p ( v123 )
i ( cid : 123 ) ^v123
where < ( cid : 123 ) > denotes an average over the sampled states and i is the probability that unit i would be turned on if the visi - ble vector was stochastically reconstructed from the sampled hidden states .
computing the posterior distribution over the second hidden layer , v123 , from the sampled binary states in the rst hidden layer , h123 , is exactly the same process as recon - structing the data , so v123 i is a sample from a bernoulli random variable with probability ^v123 i .
the learning rule can therefore be written as :
@ log p ( v123 )
i ( cid : 123 ) v123
i on h123
j is unproblematic in the deriva - the dependence of v123 tion of eq .
123 from eq .
123 because ^v123 i is an expectation that is conditional on h123 j .
since the weights are replicated , the full derivative for a generative weight is obtained by summing the derivatives of the generative weights between all pairs of lay -
@ log p ( v123 )
i ( cid : 123 ) v123
j ( cid : 123 ) h123
i ( cid : 123 ) v123
all of the vertically aligned terms cancel leaving the boltz -
mann machine learning rule of eq
123 restricted boltzmann machines and
contrastive divergence learning
it may not be immediately obvious that the innite directed net in gure 123 is equivalent to a restricted boltzmann ma - chine ( rbm ) .
an rbm has a single layer of hidden units which are not connected to each other and have undirected , symmetrical connections to a layer of visible units .
to gen - erate data from an rbm , we can start with a random state in one of the layers and then perform alternating gibbs sam - pling : all of the units in one layer are updated in parallel given the current states of the units in the other layer and this is repeated until the system is sampling from its equilibrium distribution .
notice that this is exactly the same process as generating data from the innite belief net with tied weights .
to perform maximum likelihood learning in an rbm , we can use the difference between two correlations .
for each weight , wij , between a visible unit i and a hidden unit , j we measure j > when a datavector is clamped on the correlation < v123
figure 123 : an innite logistic belief net with tied weights .
the downward arrows represent the generative model .
the up - ward arrows are not part of the model .
they represent the parameters that are used to infer samples from the posterior distribution at each hidden layer of the net when a datavector is clamped on v123
the visible units and the hidden states are sampled from their conditional distribution , which is factorial .
then , using al - ternating gibbs sampling , we run the markov chain shown in gure 123 until it reaches its stationary distribution and measure the correlation <v123 j > .
the gradient of the log probability of the training data is then
@ log p ( v123 )
j> ( cid : 123 ) <v123
this learning rule is the same as the maximum likelihood learning rule for the innite logistic belief net with tied weights , and each step of gibbs sampling corresponds to computing the exact posterior distribution in a layer of the innite logistic belief net .
maximizing the log probability of the data is exactly the same as minimizing the kullback - leibler divergence , ( cid : 123 ) ) , between the distribution of the data , p 123 , and kl ( p 123jjp 123 the equilibrium distribution dened by the model , p 123 contrastive divergence learning ( hinton , 123 ) , we only run the markov chain for n full steps123 before measuring the sec - ond correlation .
this is equivalent to ignoring the derivatives 123each full step consists of updating h given v then updating v
t = 123 t = 123
t = infinity
t = infinity
figure 123 : this depicts a markov chain that uses alternating gibbs sampling .
in one full step of gibbs sampling , the hid - den units in the top layer are all updated in parallel by apply - ing eq .
123 to the inputs received from the the current states of the visible units in the bottom layer , then the visible units are all updated in parallel given the current hidden states .
the chain is initialized by setting the binary states of the visible units to be the same as a data - vector .
the correlations in the activities of a visible and a hidden unit are measured after the rst update of the hidden units and again at the end of the chain .
the difference of these two correlations provides the learning signal for updating the weight on the connection .
that come from the higher layers of the innite net .
the sum of all these ignored derivatives is the derivative of the log probability of the posterior distribution in layer vn , which is also the derivative of the kullback - leibler divergence be - tween the posterior distribution in layer vn , p n ( cid : 123 ) , and the equi - librium distribution dened by the model .
so contrastive di - vergence learning minimizes the difference of two kullback -
kl ( p 123jjp 123
( cid : 123 ) ) ( cid : 123 ) kl ( p n
( cid : 123 ) jjp 123
ignoring sampling noise , this difference is never negative because gibbs sampling is used to produce p n ( cid : 123 ) from p 123 and gibbs sampling always reduces the kullback - leibler diver - gence with the equilibrium distribution .
it is important to no - ( cid : 123 ) depends on the current model parameters and tice that p n ( cid : 123 ) changes as the parameters change is the way in which p n being ignored by contrastive divergence learning .
this prob - lem does not arise with p 123 because the training data does not depend on the parameters .
an empirical investigation of the relationship between the maximum likelihood and the con - trastive divergence learning rules can be found in carreira - perpinan and hinton ( 123 ) .
contrastive divergence learning in a restricted boltzmann machine is efcient enough to be practical ( mayraz and hin - ton , 123 ) .
variations that use real - valued units and differ - ent sampling schemes are described in teh et al .
( 123 ) and have been quite successful for modeling the formation of to - pographic maps ( welling et al . , 123 ) , for denoising natural images ( roth and black , 123 ) or images of biological cells ( ning et al . , 123 ) .
marks and movellan ( 123 ) describe a way of using contrastive divergence to perform factor analy - sis and welling et al .
( 123 ) show that a network with logistic , binary visible units and linear , gaussian hidden units can be used for rapid document retrieval .
however , it appears that
the efciency has been bought at a high price : when applied in the obvious way , contrastive divergence learning fails for deep , multilayer networks with different weights at each layer because these networks take far too long even to reach condi - tional equilibrium with a clamped data - vector .
we now show that the equivalence between rbms and innite directed nets with tied weights suggests an efcient learning algorithm for multilayer networks in which the weights are not tied .
123 a greedy learning algorithm for
an efcient way to learn a complicated model is to combine a set of simpler models that are learned sequentially .
to force each model in the sequence to learn something different from the previous models , the data is modied in some way after each model has been learned .
in boosting ( freund , 123 ) , each model in the sequence is trained on re - weighted data that emphasizes the cases that the preceding models got wrong .
in one version of principal components analysis , the variance in a modeled direction is removed thus forcing the next modeled direction to lie in the orthogonal subspace ( sanger , 123 ) .
in projection pursuit ( friedman and stuetzle , 123 ) , the data is transformed by nonlinearly distorting one direction in the data - space to remove all non - gaussianity in that direction .
the idea behind our greedy algorithm is to allow each model in the sequence to receive a different representation of the data .
the model performs a non - linear transformation on its input vectors and produces as output the vectors that will be used as input for the next model in the sequence .
figure 123 shows a multilayer generative model in which the top two layers interact via undirected connections and all of the other connections are directed .
the undirected connec - tions at the top are equivalent to having innitely many higher layers with tied weights .
there are no intra - layer connections and , to simplify the analysis , all layers have the same number of units .
it is possible to learn sensible ( though not optimal ) values for the parameters w123 by assuming that the parame - ters between higher layers will be used to construct a comple - mentary prior for w123
this is equivalent to assuming that all of the weight matrices are constrained to be equal .
the task of learning w123 under this assumption reduces to the task of learning an rbm and although this is still difcult , good ap - proximate solutions can be found rapidly by minimizing con - trastive divergence .
once w123 has been learned , the data can be mapped through wt 123 to create higher - level data at the rst hidden layer .
if the rbm is a perfect model of the original data , the higher - level data will already be modeled perfectly by the higher - level weight matrices .
generally , however , the rbm will not be able to model the original data perfectly and we can make the generative model better using the following
learn w123 assuming all the weight matrices are tied .
freeze w123 and commit ourselves to using wt
123 to infer
figure 123 : a hybrid network .
the top two layers have undi - rected connections and form an associative memory .
the lay - ers below have directed , top - down , generative connections that can be used to map a state of the associative memory to an image .
there are also directed , bottom - up , recognition connections that are used to infer a factorial representation in one layer from the binary activities in the layer below .
in the greedy initial learning the recognition connections are tied to the generative connections .
factorial approximate posterior distributions over the states of the variables in the rst hidden layer , even if subsequent changes in higher level weights mean that this inference method is no longer correct .
keeping all the higher weight matrices tied to each other , but untied from w123 , learn an rbm model of the higher - level data that was produced by using wt transform the original data .
if this greedy algorithm changes the higher - level weight matrices , it is guaranteed to improve the generative model .
as shown in ( neal and hinton , 123 ) , the negative log prob - ability of a single data - vector , v123 , under the multilayer gen - erative model is bounded by a variational free energy which is the expected energy under the approximating distribution , q ( h123jv123 ) , minus the entropy of that distribution .
for a di - rected model , the energy of the conguration v123; h123 is
e ( v123; h123 ) = ( cid : 123 ) ( cid : 123 ) log p ( h123 ) + log p ( v123jh123 ) ( cid : 123 )
so the bound is : log p ( v123 ) ( cid : 123 ) x
q ( h123jv123 ) ( cid : 123 ) log p ( h123 ) + log p ( v123jh123 ) ( cid : 123 )
q ( h123jv123 ) log q ( h123jv123 )
where h123 is a binary conguration of the units in the rst hid - den layer , p ( h123 ) is the prior probability of h123 under the cur - rent model ( which is dened by the weights above h123 ) and q ( ( cid : 123 ) jv123 ) is any probability distribution over the binary con - gurations in the rst hidden layer .
the bound becomes an equality if and only if q ( ( cid : 123 ) jv123 ) is the true posterior distribu -
when all of the weight matrices are tied together , the fac - 123 to a torial distribution over h123 produced by applying wt data - vector is the true posterior distribution , so at step 123 of the greedy algorithm log p ( v123 ) is equal to the bound .
step 123 freezes both q ( ( cid : 123 ) jv123 ) and p ( v123jh123 ) and with these terms xed , the derivative of the bound is the same as the derivative
q ( h123jv123 ) log p ( h123 )
so maximizing the bound w . r . t .
the weights in the higher lay - ers is exactly equivalent to maximizing the log probability of a dataset in which h123 occurs with probability q ( h123jv123 ) .
if the bound becomes tighter , it is possible for log p ( v123 ) to fall even though the lower bound on it increases , but log p ( v123 ) can never fall below its value at step 123 of the greedy algo - rithm because the bound is tight at this point and the bound
the greedy algorithm can clearly be applied recursively , so if we use the full maximum likelihood boltzmann machine learning algorithm to learn each set of tied weights and then we untie the bottom layer of the set from the weights above , we can learn the weights one layer at a time with a guar - antee123 that we will never decrease the log probability of the data under the full generative model .
in practice , we replace maximum likelihood boltzmann machine learning algorithm by contrastive divergence learning because it works well and is much faster .
the use of contrastive divergence voids the guarantee , but it is still reassuring to know that extra layers are guaranteed to improve imperfect models if we learn each layer with sufcient patience .
to guarantee that the generative model is improved by greedily learning more layers , it is convenient to consider models in which all layers are the same size so that the higher - level weights can be initialized to the values learned before they are untied from the weights in the layer below .
the same greedy algorithm , however , can be applied even when the lay - ers are different sizes .
123 back - fitting with the up - down algorithm
learning the weight matrices one layer at a time is efcient but not optimal .
once the weights in higher layers have been learned , neither the weights nor the simple inference proce - dure are optimal for the lower layers .
the sub - optimality pro - duced by greedy learning is relatively innocuous for super - vised methods like boosting .
labels are often scarce and each
123the guarantee is on the expected change in the log probability .
label may only provide a few bits of constraint on the parame - ters , so over - tting is typically more of a problem than under - tting .
going back and retting the earlier models may , there - fore , cause more harm than good .
unsupervised methods , however , can use very large unlabeled datasets and each case may be very high - dimensional thus providing many bits of constraint on a generative model .
under - tting is then a se - rious problem which can be alleviated by a subsequent stage of back - tting in which the weights that were learned rst are revised to t in better with the weights that were learned later .
after greedily learning good initial values for the weights in every layer , we untie the recognition weights that are used for inference from the generative weights that de - ne the model , but retain the restriction that the posterior in each layer must be approximated by a factorial distribution in which the variables within a layer are conditionally indepen - dent given the values of the variables in the layer below .
a variant of the wake - sleep algorithm described in hinton et al .
( 123 ) can then be used to allow the higher - level weights to inuence the lower level ones .
in the up - pass , the recog - nition weights are used in a bottom - up pass that stochasti - cally picks a state for every hidden variable .
the generative weights on the directed connections are then adjusted using the maximum likelihood learning rule in eq .
the weights on the undirected connections at the top level are learned as before by tting the top - level rbm to the posterior distribu - tion of the penultimate layer .
the down - pass starts with a state of the top - level asso - ciative memory and uses the top - down generative connections to stochastically activate each lower layer in turn .
during the down - pass , the top - level undirected connections and the generative directed connections are not changed .
only the bottom - up recognition weights are modied .
this is equiva - lent to the sleep phase of the wake - sleep algorithm if the as - sociative memory is allowed to settle to its equilibrium distri - bution before initiating the down - pass .
but if the associative memory is initialized by an up - pass and then only allowed to run for a few iterations of alternating gibbs sampling before initiating the down - pass , this is a contrastive form of the wake - sleep algorithm which eliminates the need to sample from the equilibrium distribution of the associative memory .
the contrastive form also xes several other problems of the sleep phase .
it ensures that the recognition weights are being learned for representations that resemble those used for real data and it also helps to eliminate the problem of mode aver - aging .
if , given a particular data vector , the current recogni - tion weights always pick a particular mode at the level above and ignore other very different modes that are equally good at generating the data , the learning in the down - pass will not try to alter those recognition weights to recover any of the other modes as it would if the sleep phase used a pure ancestral pass .
a pure ancestral pass would have to start by using pro - longed gibbs sampling to get an equilibrium sample from the top - level associative memory .
by using a top - level associa -
123because weights are no longer tied to the weights above them , i must be computed using the states of the variables in the layer above i and the generative weights from these variables to i .
figure 123 : all 123 cases in which the network guessed right but had a second guess whose probability was within 123 : 123 of the probability of the best guess .
the true classes are arranged in standard scan order .
tive memory we also eliminate a problem in the wake phase : independent top - level units seem to be required to allow an ancestral pass , but they mean that the variational approxima - tion is very poor for the top layer of weights .
appendix b species the details of the up - down algorithm using matlab - style pseudo - code for the network shown in g - ure 123
for simplicity , there is no penalty on the weights , no momentum , and the same learning rate for all parameters .
also , the training data is reduced to a single case .
123 performance on the mnist database
123 training the network
the mnist database of handwritten digits contains 123 , 123 training images and 123 , 123 test images .
results for many different pattern recognition techniques are already published for this publicly available database so it is ideal for evaluating new pattern recognition methods .
for the basic version of the mnist learning task , no knowledge of geometry is pro - vided and there is no special pre - processing or enhancement of the training set , so an unknown but xed random permuta - tion of the pixels would not affect the learning algorithm .
for this permutation - invariant version of the task , the general - ization performance of our network was 123% errors on the ofcial test set .
the network123 shown in gure 123 was trained on 123 , 123 of the training images that were divided into 123 balanced mini - batches each containing 123 examples of each digit class .
the weights were updated after each mini - batch .
123preliminary experiments with 123 ( cid : 123 ) 123 images of handwritten digits from the usps database showed that a good way to model the joint distribution of digit images and their labels was to use an architecture of this type , but for 123 ( cid : 123 ) 123 images , only 123=123 as many units were used in each hidden layer .
figure 123 : the 123 test cases that the network got wrong .
each case is labeled by the networks guess .
the true classes are arranged in standard scan order .
in the initial phase of training , the greedy algorithm de - scribed in section 123 was used to train each layer of weights separately , starting at the bottom .
each layer was trained for 123 sweeps through the training set ( called epochs ) .
dur - ing training , the units in the visible layer of each rbm had real - valued activities between 123 and 123
these were the nor - malized pixel intensities when learning the bottom layer of weights .
for training higher layers of weights , the real - valued activities of the visible units in the rbm were the activation probabilities of the hidden units in the lower - level rbm .
the hidden layer of each rbm used stochastic binary values when that rbm was being trained .
the greedy training took a few hours per layer in matlab on a 123ghz xeon processor and when it was done , the error - rate on the test set was 123% ( see below for details of how the network is tested ) .
when training the top layer of weights ( the ones in the associative memory ) the labels were provided as part of the input .
the labels were represented by turning on one unit in a softmax group of 123 units .
when the activities in this group were reconstructed from the activities in the layer above , ex - actly one unit was allowed to be active and the probability of
picking unit i was given by :
where xi is the total input received by unit i .
curiously , the learning rules are unaffected by the competition between units in a softmax group , so the synapses do not need to know which unit is competing with which other unit .
the competi - tion affects the probability of a unit turning on , but it is only this probability that affects the learning .
after the greedy layer - by - layer training , the network was trained , with a different learning rate and weight - decay , for 123 epochs using the up - down algorithm described in section 123
the learning rate , momentum , and weight - decay123 were chosen by training the network several times and observing its performance on a separate validation set of 123 , 123 im - ages that were taken from the remainder of the full training set .
for the rst 123 epochs of the up - down algorithm , the up - pass was followed by three full iterations of alternating gibbs sampling in the associative memory before perform - ing the down - pass .
for the second 123 epochs , six iterations were performed , and for the last 123 epochs , ten iterations were performed .
each time the number of iterations of gibbs sampling was raised , the error on the validation set decreased
the network that performed best on the validation set was then tested and had an error rate of 123% .
this network was then trained on all 123 , 123 training images123 until its error - rate on the full training set was as low as its nal error - rate had been on the initial training set of 123 , 123 images .
this took a further 123 epochs making the total learning time about a week .
the nal network had an error - rate of 123%123
the errors made by the network are shown in gure 123
the 123 cases that the network gets correct but for which the second best probability is within 123 of the best probability are shown in gure 123
