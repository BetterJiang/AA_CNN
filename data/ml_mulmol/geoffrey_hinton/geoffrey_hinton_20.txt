learning a restricted boltzmann machine
restricted boltzmann machines were devel - oped using binary stochastic hidden units .
these can be generalized by replacing each binary unit by an innite number of copies that all have the same weights but have pro - gressively more negative biases .
the learning and inference rules for these stepped sig - moid units are unchanged .
they can be ap - proximated eciently by noisy , rectied lin - ear units .
compared with binary units , these units learn features that are better for object recognition on the norb dataset and face verication on the labeled faces in the wild dataset .
unlike binary units , rectied linear units preserve information about relative in - tensities as information travels through mul - tiple layers of feature detectors .
restricted boltzmann machines ( rbms ) have been used as generative models of many dierent types of data including labeled or unlabeled images ( hinton et al . , 123 ) , sequences of mel - cepstral coef - cients that represent speech ( mohamed & hinton , 123 ) , bags of words ( salakhutdinov & hinton , 123 ) , and user ratings of movies ( salakhutdinov et al . , 123 ) .
in their con - form they can be used to model high - dimensional temporal sequences such as video or mo - tion capture data ( taylor et al . , 123 ) .
their most im - portant use is as learning modules that are composed to form deep belief nets ( hinton et al . , 123 ) .
images composed of binary pixels can be modeled by an rbm that uses a layer of binary hidden units ( fea - ture detectors ) to model the higher - order correlations between pixels .
if there are no direct interactions be - tween the hidden units and no direct interactions be - tween the visible units that represent the pixels , there is a simple and ecient method called contrastive divergence to learn a good set of feature detectors from a set of training images ( hinton , 123 ) .
we start with small , random weights on the symmetric connec - tions between each pixel i and each feature detector j .
then we repeatedly update each weight , wij , using the dierence between two measured , pairwise correlations
wij = ( <vihj>data <vihj>recon )
where is a learning rate , <vihj>data is the frequency with which visible unit i and hidden unit j are on to - gether when the feature detectors are being driven by images from the training set and < vihj >recon is the corresponding frequency when the hidden units are be - ing driven by reconstructed images .
a similar learning rule can be used for the biases .
given a training image , we set the binary state , hj , of each feature detector to be 123 with probability
p ( hj = 123 ) =
123 + exp ( bj pivis viwij )
where bj is the bias of j and vi is the binary state of pixel i .
once binary states have been chosen for the hidden units we produce a reconstruction of the training image by setting the state of each pixel to be 123 with probability
p ( vi = 123 ) =
123 + exp ( bi pjhid hjwij )
appearing in proceedings of the 123 th international confer - ence on machine learning , haifa , israel , 123
copyright 123 by the author ( s ) / owner ( s ) .
the learned weights and biases implicitly dene a probability distribution over all possible binary images via the energy , e ( v , h ) , of a joint conguration of the
relus improve rbms
visible and hidden units :
e ( v , h ) = x
vihj wij x
p ( v ) = ph ee ( v , h )
gaussian units
rbms were originally developed using binary stochas - tic units for both the visible and hidden layers to deal with real - valued data ( hinton & salakhutdinov , 123 ) replaced the binary visible units by linear units with independent gaus - sian noise as rst suggested by ( freund & haussler , 123 ) .
the energy function then becomes :
e ( v , h ) = x
where i is the standard deviation of the gaussian noise for visible unit i .
it is possible to learn the variance of the noise for each visible unit but this is dicult using binary hidden units .
in many applications , it is much easier to rst normalise each component of the data to have zero mean and unit variance and then to use noise - free re - constructions , with the variance in equation 123 set to 123
the reconstructed value of a gaussian visible unit is then equal to its top - down input from the binary hid - den units plus its bias .
we use this type of noise - free visible unit for the models of object and face images
rectied linear units
to allow each unit to express more information , ( teh & hinton , 123 ) introduced binomial units which can be viewed as n separate copies of a binary unit that all share the same bias and weights .
a nice side - eect of using weight - sharing to synthesize a new type of unit out of binary units is that the mathematics underlying learning in binary - binary rbms remains unchanged .
since all n copies receive the same total input , they all have the same probability , p , of turn - ing on and this only has to be computed once .
the expected number that are on is n p and the variance in this number is n p ( 123 p ) .
for small p , this acts like a poisson unit , but as p approaches 123 the variance be - comes small again which may not be desireable .
also , for small values of p the growth in p is exponential in the total input .
this makes learning much less stable
figure 123
a comparison of three dierent ways to model rectied linear units .
the red curve shows the expected value of the sum of an innite number of binary units with each having a bias one less than the previous one .
the blue curve is the approximation log ( 123+exp ( x ) ) .
the green curve is the expected value of a rectied linear unit with added gaussian noise as described in section 123
the red and blue curves are virtually indistinguishable .
than for the stepped sigmoid units ( ssu ) described
a small modication to binomial units makes them far more interesting as models of real neurons and also more useful for practical applications .
we make an in - nite number of copies that all have the same learned weight vector w and the same learned bias , b , but each copy has a dierent , xed oset to the bias .
if the o - sets are 123 , 123 , 123 , . . .
the sum of the probabili - ties of the copies is extremely close to having a closed form ( gure 123 ) :
( x i + 123 ) log ( 123 + ex ) ,
where x = vwt + b .
so the total activity of all of the copies behaves like a noisy , integer - valued version of a smoothed rectied linear unit123
even though log ( 123 + ex ) is not in the exponential family , we can model it accurately using a set of binary units with shared weights and xed bias osets .
this set has no more parameters than an ordinary binary unit , but it provides a much more ex - pressive variable .
the variance in the integer activity level is ( x ) so units that are rmly o do not create noise and the noise does not become large when x is
a drawback of giving each copy a bias that diers by a xed oset is that the logistic sigmoid function needs to be used many times to get the probabilities
123if we only use n copies , we need to subtract the term
log ( 123 + exn ) from the approximation .
relus improve rbms
required for sampling an integer value correctly .
is possible , however , to use a fast approximation in which the sampled value of the rectied linear unit is not constrained to be an integer .
instead it is given by max ( 123 , x+n ( 123 , ( x ) ) where n ( 123 , v ) is gaussian noise with zero mean and variance v .
we call a unit that uses this approximation a n oisyrectied linear u nit ( nrelu ) and this paper shows that nrelus work better than binary hidden units for several dierent
( jarrett et al . , 123 ) have explored various rectied nonlinearities ( including the max ( 123 , x ) nonlinearity , which they refer to as positive part ) in the con - text of convolutional networks and have found them to improve discriminative performance .
our empirical results in sections 123 and 123 further support this ob - servation .
we also give an approximate probabilistic interpretation for the max ( 123 , x ) nonlinearity , further justifying their use .
intensity equivariance
nrelus have some interesting mathematical proper - ties ( hahnloser et al . , 123 ) , one of which is very use - ful for object recognition .
a major consideration when designing an object recognition system is how to make the output invariant to properties of the input such as location , scale , orientation , lighting etc .
convolutional neural networks are often said to achieve translation invariance but in their pure form they actually achieve something quite dierent .
if an object is translated in the input image , its representation in a pool of local lters that have shared weights is also translated .
so if it can be represented well by a pattern of feature activities when it is in one location , it can also be rep - resented equally well by a translated pattern of feature activities when it is another location .
we call this translation equivariance : the representation varies in the same way as the image .
in a deep convolutional net , translation invaraince is achieved by using sub - sampling to introduce a small amount of translation invariance after each layer of lters .
binary hidden units do not exhibit intensity equivari - ance , but rectied linear units do , provided they have zero biases and are noise - free .
scaling up all of the in - tensities in an image by > 123 cannot change whether a zero - bias unit receives a total input above or below zero .
so all of the o units remain o and the re - mainder all increase their activities by a factor of .
this stays true for many layers of rectied linear units .
when deciding whether two face images come from the same person , we make use of this nice property of rec - tied linear units by basing the decision on the cosine
of the angle between the activities of the feature detec - tors in the last hidden layer .
the feature vectors are intensity equivariant and the cosine is intensity invari - ant .
the type of intensity invariance that is important for recognition cannot be achieved by simply dividing all the pixel intensities by their sum .
this would cause a big change in the activities of feature detectors that attend to the parts of a face when there is a bright spot in the background .
empirical evaluation
we empirically compare nrelus to stochastic bi - nary hidden units123 on two vision tasks : 123 ) object recognition on the jittered - cluttered norb dataset ( lecun et al . , 123 ) , and 123 ) face verication on the labeled faces in the wild dataset ( huang et al . , 123 ) .
both datasets contain complicated image vari - ability that make them dicult tasks .
also , they both already have a number of published results for various methods , which gives a convenient basis for judging how good our results are .
we use rbms with binary hidden units or nrelus to generatively pre - train one or more layers of features and we then discriminatively ne - tune the features using backpropagation .
on both tasks nrelus give better discriminative performance than binary units .
the discriminative models use the deterministic version of nrelus that implement the function y = max ( 123 , x ) .
for backpropagation , we take the gradient of this function to be 123 when x 123 and 123 when x > 123 ( i . e .
we ignore the discontinuity at x = 123 ) .
jittered - cluttered norb
norb is a synthetic 123d object recognition dataset that contains ve classes of toys ( humans , animals , cars , planes , trucks ) imaged by a stereo - pair cam - era system from dierent viewpoints under dierent lighting conditions .
norb comes in several versions the jittered - cluttered version has grayscale stereo - pair images with cluttered background and a central object which is randomly jittered in position , size , pixel intensity etc .
there is also a distractor object placed in the periphery .
examples from the dataset are shown in gure 123
for each class , there are ten dierent instances , ve of which are in the training set and the rest in the test set .
so at test time a clas - sier needs to recognize unseen instances of the same classes .
in addition to the ve object classes , there is a sixth class whose images contain none of the objects in the centre .
for details see ( lecun et al . , 123 ) .
123we also compared with binomial units but they were
no better than binary units , so we omit those results .
relus improve rbms
figure 123
stereo - pair training cases from the jittered - cluttered norb training set .
the stereo - pair images are subsampled from their orig - inal resolution of 123 123 123 to 123 123 123 to speed up experiments .
they are normalized to be zero - mean and divided by the average standard deviation of all the pixels in all the training images .
there are 123 , 123 training cases ( 123 , 123 cases per class ) and 123 , 123 test cases ( 123 , 123 cases per class ) .
we hold out 123 , 123 cases from the training set and use them as a validation set for selecting the model architecture ( number of hid - den units and number of layers ) and for early stop - ping .
the validation set is created by taking all 123 , 123 training images of a single ( randomly selected ) object instance from each of the ve object classes , and an equal number of randomly selected images from the
to train a classier we use a similar approach to ( larochelle et al . , 123 ) .
we rst greedily pre - train two layers of features , each as an rbm using cd .
then we use multinomial regression at the top - most hidden layer to predict the label and discriminatively ne - tune the parameters in all layers of the classier ( see gure 123 ) .
we have tried 123 , 123 and 123 units for the rst hidden layer , and 123 and 123 units for the second one .
using more units always gave better classication results , so the architecture with the best results have 123 units in the rst layer and 123 in the second .
we suspect that the results will be even better with more hidden units .
cases , the pixels are represented by gaussian units ( hinton & salakhutdinov , 123 ) and the hidden units are either nrelus or stochastic binary units .
pre - training is done for 123 epochs ( in both layers ) , using mini - batches of 123 training examples with a learn - ing rate of 123 applied to the average per - case cd update , along with momentum ( hinton et al . , 123 ) .
figure 123 shows a subset of features learned by the rst - level rbm with 123 nrelus in the hidden layer .
many of these are gabor - like lters , so nrelus seem capable of learning qualitatively sensible features from images .
the classication results ( section 123 ) show that they are quantitatively sensible as well .
figure 123
figure 123
network architecture used for cluttered norb classication task .
we greedily pre - train two hidden layers of nrelus as rbms .
the class label is represented as a k - dimensional binary vector with 123 - of - k activation , where k is the number of classes .
the classier computes the probability of the k classes from the second layer hidden activities h123 using the softmax function .
fraction of training images on which a hidden unit is active
figure 123
histogram of nrelus binned according to how often they are active ( i . e .
has a value above zero ) on training images , as computed on jittered - cluttered norb for an rbm with 123 nrelus in the hidden layer .
is a histogram that shows how often the hidden units in this rbm have values above zero on the training set .
if a unit is always active , then it would end up in the rightmost bin of the histogram .
note that an always - active unit is purely linear since it never gets rectied .
as the histgroam shows , there are no such units in this model .
there is some variety in how often the units are active .
we have looked at the features that correspond to each of the three peaks in the his - togram : the peak near 123 on the x - axis are gabor - like lters , while the peak near 123 are point lters .
the smaller peak between 123 and 123 corresponds mostly to more global lters .
classication results
table 123 lists the test set error rates of classiers with a single hidden layer , using either binary units or nre - lus , with and without pre - training .
nrelus out -
relus improve rbms
figure 123
a subset of the features learned by an rbm on images from jittered - cluttered norb .
the rbm has 123 nrelus in the hidden layer , and those shown are the 123 features with the highest l123 norm .
sorting by l123 norm tends to pick features with well - dened gabor - like weight patterns .
only about 123% of the features are gabor - like .
the rest consist of lters with more global weight patterns , as well as point lters that copy pixels to the hidden units .
perform binary units , both when randomly initialized and when pre - trained .
pre - training helps improve the performance of both unit types .
but nrelus with - out pre - training are better than binary units with pre -
table 123 lists the results for classiers with two hidden layers .
just as for single hidden layer classiers , nre - lus outperform binary units regardless of whether greedy pre - training is used only in the rst layer , in both layers , or not at all .
pre - training improves the results : pre - training only the rst layer and randomly initializing the second layer is better than randomly initialized both .
pre - training both layers gives further improvement for nrelus but not for binary units .
for comparison , the error rates of some other mod - els are : multinomial regression on pixels 123% , gaus - sian kernel svm 123% , convolutional net 123% , con - volutional net with an svm at the top - most hid - den layer 123% .
the last three results are from ( bengio & lecun , 123 ) .
our results are worse than that of convolutional nets , but 123 ) our models use heav - ily subsampled images , and 123 ) convolutional nets have knowledge of image topology and approximate trans - lation invariance hard - wired into their architecture .
table 123
test error rates for classiers with two hidden lay - ers ( 123 units in the rst , 123 in the second ) , trained on 123 123 123 jittered - cluttered norb images .
labeled faces in the wild
the prediction task for the labeled faces in the wild ( lfw ) dataset is as follows : given two face images as input , predict whether the identities of the faces are the same or dierent .
the dataset contains colour faces of public gures collected from the web using a frontal - face detector .
the bounding box computed by the face detector is used to approximately normalize the faces position and scale within the image .
some examples from the dataset are shown in gure 123
for details see ( huang et al . , 123 ) .
table 123
test error rates for classiers with 123 hidden units trained on 123 123 123 jittered - cluttered norb im -
pre - trained ? nrelu binary
figure 123
face - pair examples from the labeled faces in the
relus improve rbms
network architecture
the task requires a binary classier with two sets of inputs ( the two faces ) .
if we stitch the two inputs together and treat the result as one extended input vector , the classiers output will depend on the order in which the inputs are stitched .
to make the clas - sier symmetric with respect to the inputs , we use a siamese architecture ( chopra et al . , 123 ) .
the idea is to learn a function that takes a single face as input and computes some feature vector from it .
given a pair of faces , this function is applied to both faces separately , and the two corresponding feature vectors are com - bined using a xed , symmetric function into a single representation which is invariant to input order .
the probability of the two faces being the same person is computed as output from this representation .
the en - tire system , including the feature extractor replicated over the two faces , can be learned jointly .
here we choose the feature extractor to be a fully - connected feedforward layer of nrelus , pre - trained as an rbm .
we use cosine distance as the symmetric function that combines the two feature vectors .
co - sine distance is invariant to rescaling of its inputs , which when combined with the equivariance of nre - lus makes the entire model analytically invariant to rescaling of the pixels by a positive scalar .
the in - variance holds regardless of the number of layers of nrelus , so it is possible to train deep architectures with this property .
in order to make the feature ex - tractor exactly equivariant , we do not use biases into the hidden units .
figure 123 shows the architecture of our face verication model .
lfw images are of size 123 ( 123 colour channels ) with the face in the centre and a lot of background sur - rounding it .
recently it has been found that humans are able to get 123% accuracy on the lfw task even when the centre of the image is masked ( kumar et al . , 123 ) .
to prevent background information from arti - cially inating the results , we only use a 123 123 window from the centre .
the images are then rotated and scaled such that the coordinates of the eyes are the same across all images .
we further subsample this win - dow to 123 ( 123 channels ) .
the same image normal - ization procedure used for jittered - cluttered norb is applied here as well .
lfw contains 123 , 123 images of 123 , 123 people .
for the purposes of reporting results , the designers of lfw have pre - dened 123 splits of the dataset for 123 - fold cross validation , each containing 123 , 123 training pairs and 123 test pairs .
the number of same and dier -
figure 123
siamese network used for the labeled faces in the wild task .
the feature extractor fw contains one hidden layer of nrelus pre - trained as an rbm ( on single faces ) with parameters w .
fw is applied to the face im - ages ia and ib , and the cosine distance d between the re - sulting feature vectors fw ( ia ) and fw ( ib ) is computed .
the probability of the two faces having the same identity is then computed as p r ( same ) = w and b are scalar learnable parameters .
ent cases are always equal , both for training and test sets .
the identities of the people in the training and test sets are always kept disjoint , so at test time the model must predict on unseen identities .
we rst pre - train a layer of features as an rbm using all 123 , 123 single faces in the training set of each split , then plug it into the siamese architecture in gure 123 and discrim - inatively ne - tune the parameters on pairs of faces .
as before , during pre - training pixels are gaussian units , and the hidden units are either nrelus or stochastic
figure 123 shows 123 of the 123 features learned by an rbm on 123 123 colour images with nrelus in the hidden layer .
like the norb model in section 123 , this model is also pre - trained for 123 epochs on mini - batches of size 123 with a learning rate of 123 and momentum .
the model has learned detectors for parts of faces like eyes , nose , mouth , eye brows etc .
some features detect the boundary of the face .
there is a mix of localized lters and more global ones that detect more than just a single part of the face .
the histogram in gure 123 shows how often the units in this rbm turn on for the faces in lfw .
unlike the norb model , here the histogram has only one peak .
in particular , there are almost no point lters in this
during discriminative ne - tuning , we use a subset of the pubg face dataset ( kumar et al . , 123 ) as a vali - dation set for selecting the model architecture and for
relus improve rbms
ve - fold cross validation .
table 123 lists the average accuracy of various models , along with the standard deviations .
models using nrelus seem to be more accurate , but the standard deviations are too large to draw rm conclusions .
the two current best lfw results are 123 123 ( wolf et al . , 123 ) , and 123 123 ( kumar et al . , 123 ) .
the former uses a commercial automatic face alignment system to normalize the faces , while the lat - ter uses additional labels ( collected manually ) that de - scribe the face , such as ethnicity , sex , age etc .
such enhancements can be applied to our model as well , and they are likely to increase accuracy signicantly .
these results may also be beneting from the back - ground pixels around the face , which we have ( mostly )
mixtures of exponentially many
we have shown that nrelus work well for discrimina - tion , but they are also an interesting way of modeling the density of real - valued , high - dimensional data .
a standard way to do this is to use a mixture of diag - onal gaussians .
alternatively we can use a mixture of factor analysers .
both of these models are expo - nentially inecient if the data contains componential structure .
consider , for example , images of pairs of independent digits .
if a mixture model for single digit images needs n components , a single mixture model of pairs of digits needs n 123 components .
fortunately , this exponential growth in the number of components in the mixture can be achieved with only linear growth in the number of latent variables and quadratic growth in the number of parameters if we use rectied linear
consider using rectied linear units with zero bias to model data that lies on the surface of a unit hyper - sphere .
each rectied linear unit corresponds to a plane through the centre of the hypersphere .
an activity of 123 for one half of the hypersphere and for the other half its activity increases linearly with distance from that plane .
n units can create 123n re - gions on the surface of the hypersphere123
as we move
123assuming the hypersphere is at least n - dimensional .
table 123
accuracy on the lfw task for various models trained on 123 123 colour images .
figure 123
a subset of the features learned by an rbm on 123 123 colour images from lfw .
the rbm has 123 nre - lus in the hidden layer , and shown above are the 123 fea - tures with the highest l123 norm .
fraction of training images on which a hidden unit is active
figure 123
histogram of nrelus binned according to how often they have a value above zero on single face images in lfw for an rbm with 123 nrelus in the hidden layer .
early stopping .
this subset , called the development set by the creators of pubg , do not contain any iden - tities that are in lfw .
classication results
as explained before , after pre - training a single layer of features as an rbm , we insert it into the siamese architecture in gure 123 and discriminatively ne - tune the parameters .
we have tried models with 123 , 123 , 123 and 123 units in the hidden layer .
the dierence in accuracy is small between 123 and 123 units all the results in this section are for 123 units .
the rules of lfw specify that a models accuracy must be computed using ten - fold cross validation using the ten pre - specied splits of the dataset .
to speed up ex - periments , we merge two splits into one and perform
relus improve rbms
around within each of these regions the subset of units that are non - zero does not change so we have a lin - ear model , but it is a dierent linear model in every region .
the mixing proportions of the exponentially many linear models are dened implicitly by the same parameters as are used to dene p ( v|h ) and , unlike a directed model , the mixing proportions are hard to compute explicitly ( nair & hinton , 123 ) .
this is a much better way of exponentially large mixture of linear models with shared latent variables than the method described in ( hinton et al . , 123 ) which uses directed linear models as the components of the mixture and a separate sig - moid belief net to decide which hidden units should be part of the current linear model .
in that model , it is hard to infer the values of the binary latent variables and there can be jumps in density at the boundary be - tween two linear regions .
a big advantage of switch - ing between linear models at the point where a hidden unit receives an input of exactly zero is that it avoids discontinuities in the modeled probability density .
we showed how to create a more powerful type of hid - den unit for an rbm by tying the weights and biases of an innite set of binary units .
we then approxi - mated these stepped sigmoid units with noisy rectied linear units and showed that they work better than bi - nary hidden units for recognizing objects and compar - ing faces .
we also showed that they can deal with large intensity variations much more naturally than binary units .
finally we showed that they implement mix - tures of undirected linear models ( marks & movellan , 123 ) with a huge number of components using a mod - est number of parameters .
