we describe a method for constructing a family of low rank , penalized scatterplot smooth - ers .
these pseudosplines have shrinking behaviour that is similar to that of smoothing splines .
they require two ingredients : a basis and a penalty sequence .
the smoother is then computed by a generalized ridge regression .
the family can be used to approximate existing high rank smoothers in terms of their dominant eigenvectors .
our motivating example uses linear combinations of orthogonal polynomials to approximate smoothing splines , where the linear combination and the penalty sequence depend on the particular instance of the smoother being approximated .
as a leading application , we demonstrate the use of these pseudosplines in additive model computations .
additive models are typically fitted by an iterative smoothing algorithm , and any features other than the fit itself are difficult to compute .
these include standard error curves , degrees of freedom , generalized cross - validation and influence diagnostics .
by using a low rank pseudospline approximation for each of the smoothers involved , the entire additive fit can be approximated by a corres - ponding low rank approximation .
this can be computed exactly and efficiently , and opens the door to a variety of computations that were not feasible before .
keywords : cubic smoothing splines; eigendecomposition; penalized least squares;
let x and y denote a set of n observations .
a scatterplot smoother of y against x is a function of the data : s ( xo ) = s ( xo ix , y ) , which at each xo summarizes the dependence of y on x , usually in a flexible but smooth way .
a smoother is linear if
s ( xo ix , y ) =
s ( i , xo , x ) yi
for some weights s ( i , xo , x ) which do not depend on y .
popular linear smoothers are smoothing splines , kernel smoothers and local regression .
if we concentrate on the computation of the fit only at the points in x , we can write a linear smoother as a linear map s : rni - jrn defined by y = sy .
s is commonly referred to as a smoother matrix ( buja et al . , 123; hastie and tibshirani , 123 ) .
s is the smoothing analogue of the hat or projection matrix in regression .
although s typically has full rank ( n ) , we shall see that most of its action is concentrated in a much lower dimensional subspace .
consequently we can approximate s by a lower
taddress for correspondence : department of statistics , sequoia hall , stanford university , stanford , ca 123 ,
? 123 royal statistical society
dimensional operator .
although the techniques that we discuss are quite general , they are motivated by and focus on smoothing splines .
in this paper we describe a method for constructing a family of low rank , penalized scatterplot smoothers .
these pseudosplines have shrinking behaviour that is similar to that of smoothing splines .
they require two ingredients : a basis and a penalty sequence; the smoother is then computed by a generalized ridge regression .
the family can be used to approximate existing high rank smoothers in terms of their dominant eigenvectors .
our leading example uses linear combinations of orthogonal polynomials to approximate smoothing splines , where the linear combination and the penalty sequence depend on the particular instance of the smoother being approximated , but to a negligible extent on the value of the smoothing parameter .
there are several reasons why such a representation ( a ) the family is simple and low dimensional , like polynomial regression .
how - ever , instead of selecting the degree in integral steps , the ridge parameter allows us access to a continuum of models .
besides being a compelling application of ridge regression , this simple model offers much insight into penalized smoothers ( see fig .
123 later ) .
( b ) the family provides a good low rank approximation to an existing smoother
although the matrix s is not explicitly required to compute the fit , it is needed for secondary characteristics of the smoother , such as standard errors , degrees of freedom and diagnostics .
( c ) smoothers are often used in a compound way , such as in generalized additive models ( hastie and tibshirani , 123 ) , projection pursuit regression ( friedman and stuetzle , 123; roosen and hastie , 123 ) and recently in non - linear auto - regression models ( chen and tsay , 123 ) and other time series applications ( green and silverman , 123 ) .
simple approximations allow the fit to be computed directly without iteration .
this is especially important in cases where iterative algorithms are inefficient or may fail; autoregressive and time series models with highly correlated predictors fall into this class .
again they also make available secondary characteristics which are even less accessible for these more complicated models .
smoothing splines
in this section we give a brief review of smoothing splines , which motivate our
a cubic smoothing spline minimizes the penalized least squares criterion
syi - g ( xi ) ) 123 + a
over a suitable sobolev space w123 of functions ( silverman , 123; wahba , 123 ) .
the solution g ( x ) is a natural cubic spline with knots at each distinct xi , and for the moment we assume that all the xi in the sample are unique ( in section 123 we show how to deal with ties ) .
the smoothing parameter a trades off smoothness of the curve with its closeness to the y - values .
as a - * 123 , the solution approaches an interpolating spline , whereas , as a - * oo , the solution approaches the least squares line .
one can show that the cubic smoothing spline is a linear smoother and hence write down the smoother matrix for producing the fit at the sample points .
although the is in terms of the computationally attractive b - spline basis functions , for our purposes that given in green and yandell ( 123 ) is more useful :
this representation has the n fitted values as parameters .
the criterion ( 123 ) reduces to ily - fl + aftkf , and the quadratic form in the penalty matrix k can be seen roughly to accumulate squared second differences .
further insight is gained from the eigendecomposition of s or equivalently of k
itself .
since s is symmetric ,
it has a decomposition
where the columns ui of u are orthonormal and do is diagonal with elements qi e ( 123 , 123 ) and decreasing in i .
this demmler and reinsch ( 123 ) basis has intuitive appeal .
the eigenvalue qi shows us how much damping is done to the function ui when the smoother is applied , since sui = qiui ( this also shows that the ui themselves are natural splines ) .
the columns of u are like the sequence of orthonormal polynomials defined on x , in that the number of zero crossings appears to increase with the order .
demmler and reinsch indeed showed that for k > 123 the number of sign changes in the kth eigenvector of a cubic smoothing spline is k - 123
this decomposition suggests analogies with the traditional smoothing methods for time series ( see rice and rosenblatt ( 123 ) ) .
uty expresses y in terms of the basis defined by the columns of u ( similar to a fourier transform ) .
the qi play the role of a taper .
123 ( a ) shows the results of applying a cubic smoothing spline to some air pollution data ( 123 observations ) .
two fits are given : a smoother fit corresponding to a larger penalty a and a rougher fit for a smaller penalty .
a convenient way to calibrate the amount of smoothing is via the effective degrees offreedom , defined by dfa = tr ( sa ) ( hastie and tibshirani , 123 ) .
we have used 123df and lodf respectively .
123 ( b ) gives the eigenvalues for these smoothers .
we notice several things .
the first two eigenvalues are 123 , since the first two eigenvectors span the space of linear functions ( null space of k ) which the smoother passes unchanged .
from then on the eigenvalues decline smoothly to 123 , and by number 123 those for 123df are within 123% of 123
123 ( c ) gives the third - sixth eigenvectors and shows how much shrinking is done .
of course the rate at which the eigenvalues decrease will differ depending on the value dfa; those for lodf decline far more slowly .
in fact , a more natural decom - position uses the eigenvalues oj of k , with oj = 123 / ( 123 + aoj ) and oj independent of a .
it is also important to note that the eigenvectors of s do not depend on the particular value of the smoothing parameter .
speckman ( 123 ) discussed this decomposition in more detail and showed its use in describing bias and variance for smoothing
the contributions of higher order functions to the eigendecomposition decrease rapidly with the order .
we could think of approximating the smoother by using a low rank approximation based on its eigendecomposition .
thus a rank k approximation would have the form
daggot pressure gradient
123 123 . 123 123
( a ) smoothing spline fit of ozone concentration versus daggot pressure gradient ( the two fits correspond to different value of the smoothing parameter , chosen to achieve 123 and 123 efective degrees of freedom , defined by df , = tr ( s , \ ) ) ; ( b ) first 123 eigenvalues for the two smoothing spline matrices ( the first two are exactly l , and all are greater than or equal to 123 ) ; ( c ) third - sixth eigenvectors of the spline smoother matrices ( in each case , uj is plotted against x and as such is .
viewed as a function of x; the dots on the functions indicate the occurrence of data points; the damped functions represent the smoothed versions of these functions ( using the sdf smoother ) )
where dk is a truncated version of do with diagonal elements from k + i onwards set to 123
in fact , sk is the best rank k approximation to s ( frobenius normn ) .
the smoothing spline suggests a way to parameterize a general class of smoothers .
all we need are a sequence of orthonormal basis functions and a penalty sequence .
for the analogy to be complete , the basis functions should be ordered in complexity .
let p ( x ) be a k - vector of such functions and oj j = 123 , .
. , k , the penalties .
then the coffesponding pseudosplinle with parameter a minimizes
qa ( / y ) = iiy - p_pll123 + a / 123td123 / 123
where p is the matrix of evaluations of p at the data and do = diag ( 123l , .
the solution has smoother matrix sa ( p , 123 ) = p ( ptp + ad123 ) - 123pt .
if in addition the bases are orthonormal with respect to the observed x ( sample measure ) , then ptp = i and our smoother simplifies to sa ( p , 123 ) = p ( i+ ado ) - pt .
this has the form of the truncated smoothing spline ( 123 ) with ( i+ ad ) - ' corresponding to the non - zero block of d123 and p to the corresponding columns of u .
123 illustrates the action of a pseudospline in terms of its eigenvalues .
how do we choose the bases and penalty sequences ? some obvious choices are orthogonal polynomials , cosinusoids ( rice and rosenblatt , 123 ) or legendre or chebyshev polynomials , where orthogonality is defined in terms of a continuous measure .
rice ( 123 ) studied the rates of convergence of penalized polynomials using the last two systems .
all these candidates are naturally hierarchical they have a complexity ordering
( for this reason the popular b - spline bases are not natural candidates ) .
often it is natural for some of the basis functions to remain unpenalized ( their ojs are 123 ) .
for example , we may want the first two basis functions to span the space of linear functions ( or possibly a higher order polynomial subspace ) , and 123 = 123 = 123; this is a direct analogy with the null penalty space of cubic smoothing splines .
some applications may call for specially tailored basis functions and null spaces .
whatever the choice , we end up with a parameterized family that gives us access to a spectrum of models ranging from the fit on the null space at one extreme to the unpenalized fit on the full basis set at the other .
recently donoho and johnstone ( 123 ) have applied non - linear shrinkage schemes to bases of orthonormal wavelets; these are more adaptive schemes than the framework discussed here , but similar in spirit .
illustration of three different types of smoother based on a series of orthogonal basis functions ( shown are the first 123 eigenvalues , and all smoothers have 123 effective degrees of freedom ) : ( a ) traditional series smoother , a projection using the first five basis functions; ( b ) pseudospline , here using eight basis functions and shrinking their effect down to 123 effective degrees of freedom; ( c ) smoothing spline - none of the eigenvalues are 123
for the remainder of this paper we shall focus on using these pseudosplines to
approximate existing smoothers , in particular smoothing splines .
to fix ideas suppose that we wish our pseudospline to approximate the action of
the smoothing spline used in fig .
our approach is to estimate its truncated ( 123 ) .
it does not matter which version ( rough or smooth ) we use , since the eigenvectors are the same , and the eigenvalues 123j give us 123j up to a constant; hence we simply refer to the smoother as s .
we could simply compute s itself and truncate its eigendecomposition .
this is
expensive ( o ( n123 ) ) , requires s explicitly and defeats the purpose of the approximation .
instead we supply a surrogate or pseudobasis p , which we use to define a pseudo - eigendecomposition of s :
s ( p ) = pd , , , p
where dl , , is a k x k diagonal matrix of pseudo - eigenvalues with elements 123i = p123tsp123 = pft^ , and pj is simply the result of smoothing pj ( o ( n ) computations ) .
proposition f shows that this choice of d , is optimal in a least squares sense .
natural choices for p are the orthonormal polynomials in x .
if p is uk itself , then the v ) j are the corresponding eigenvalues of s .
123 ( a ) shows the third - sixth - order normal polynomials superimposed on the corresponding eigenvectors of s for our
it turns out that we can do better than equation ( 123 ) with very little extra work .
ptsp = vd123* vt , and define p* = pv
consider the k x k eigendecomposition s ( p* ) is a better ' approximation to s than s ( p ) is .
proposition 123
let p be any n x k orthonormal basis , s a symmetric ( smoother )
matrix and p* be defined as above .
then ( a ) jis - s ( p ) iif = min is - pdpt if , ( b ) is - s ( p* ) iif = min |is - pmp if123
ii l - s ( p ) iif -
both is - pmpt iif and iptsp - miif are minimized by the same matrix m .
the results follow immediately by matching elements of m ( or d ) with the corresponding elements of ptsp .
the inequality in ( c ) is immediate since the approximations minimize the norm subject to conditions of nested generality .
el the pseudo - eigenvalues are indistinguishable from the corresponding genuine in the very small are also very close , especially for low
components in fig .
on the log - scale , we start to see differences eigenvalues ( 123 - 123 ) .
the pseudo - eigenvectors order - see fig
remark 123
if s = ( i+ ak ) - 123 , then it is not difficult to show that s ( p* ) solves
mi ii y - pa123 + a ( p ) 123 ) tk ( p ) 123 ) .
( a ) third - sixth eigenvectors of a smoothing spline ( ) , along with the orthonormal polynomials of the same order ( o ) ( they are similar in shape and in zero - crossing behaviour; the polynomials appear to be wilder in the tails , behaviour that is far worse for higher order polynomials ) ; ( b ) as in ( a ) but showing the improved basis p* = pv ( these are very close to the genuine eigenvectors )
( this was pointed out by the associate editor . ) this amounts to solving the original penalized least squares problem over the subspace spanned by p .
note that the v above that diagonalizes ptsp also diagonalizes ptkp , which is the penalty matrix for the , qs .
their respective eigenvalues are linked via the relationship i>b= l / ( i + a123o ) .
from a computing aspect , ptsp is far more attractive than ptkp , since it simply involves the action of s on the columns of p ( see the next section ) , whereas k is often implicit or buried in the code .
remark 123
the eigenvalues of ptsp and hence ptkp , along with the p* , give us a penalty sequence ok* and basis to be used in equation ( 123 ) , and the resulting pseudo - spline is an approximation to s .
the smoothing parameter used in s is not critical , especially in the case above when s is a spline - type smoother , since the eigenvectors of pt123p do not depend on it .
since the pseudospline has a built - in smoothing parameter , a single approximation to a particular version of s gives us an entire family of pseudosplines .
remark 123
although motivated by smoothing splines , and similar in structure to
smoothing splines , the pseudospline can be used to approximate any linear smoother .
this allows us to understand the action of s in terms of regularization in a particular basis .
if s is not symmetric , the calculation of v and hence p* requires some modification .
our current strategy is to symmetrize ptsp by averaging it with its
remark 123
in our examples our seed basis p are polynomials .
since each column of p* is a linear combination of the columns of p , they are also polynomials , and p*
spans the same space as p since s has full rank .
the smoother s ( p* ) = p ( ptsp ) pt operates by projecting first onto c ( p ) , smoothing using s , and then reprojecting onto
remark 123
notice that there will be equality in proposition 123 ( c ) under at least two
( a ) if p = u , a subset of the eigenvectors of s , or ( b ) if p = p* ( so iterating the improvement will not help ) .
123 illustrates the differences for the smoothing spline used in fig .
123 s ( p* )
approaches s as k increases , whereas s ( p ) does not .
computational details and refinements
an important feature of our construction is that the matrix s itself is not explicitly
required; we simply need to be able to compute the action of s on the k n - vectors pj , an operation that can typically be performed in 123 ( n ) operations .
our recipe above can of course be used for approximating any smoother .
we have had experience with smoothing splines and locally weighted running lines ( cleveland , 123 ) , and for both of these it works well .
the rank required for the approximation will depend on df = tr ( s ) for the smoother s - larger df will require higher rank .
our approach for developing a pseudospline approximation to s is therefore adaptive .
standard sequential algo - rithms exist for computing orthogonal polynomials .
we always include the first two polynomials ( constant and linear ) , since they are known eigenvectors .
the computations proceed sequentially with each new polynomial pj , and hence basis of
* mx d - d - d - d - d - d - d - d - d
order of approximation
each curve represents the accuracy of a smoother approximation using a particular basis : d , is - s ( pk ) j123 / jjs123 as a function of k , the number of orthogonal columns in pk , the orthogonal polynomial basis; m , the corresponding curve using the basis pk*; . t , using the basis uk , the ordered eigensubspace of s , corresponding to the best rank k approximation s ( uk ) ( s itself is the smoother used in fig .
123 corresponding to dfa = tr ( sa ) = 123
vectors pj .
although we could compute pj* sequentially each time , we only need to diagonalize pjtsp123 once the approximation has been found to be satisfactory .
ideally , a criterion of the form
we need to decide how many terms j are sufficient .
would be informative , but this is unavailable without s .
for the present example we used j = 123 polynomials , and f123 was 123% .
in practice we continue to add terms until
123 s ( pj* ) iif
123pj123 spji123 + p123 p123
|| p iisj123f
is below some small threshold ( 123 ) .
once the approximation is satisfactory , we diagonalize pjspj to form pj as described earlier .
to compute the fit f at x when smoothing using s ( p* ) , we use
f = s ( p* ) y = s p* ? / ( p123 , y ) ,
and , to compute the fit f ( xo ) at a value x123 not among the original xi , we use
f ( xo ) = 123 p123 ( xo ) ' ' j ( p , * , y )
pk ( xo ) vki
although s ( p* ) performs better than s ( p ) , they both are based on polynomials
( remember that p* = pj ) .
we can include a smoothing parameter by replacing i / j by i / ( i + ag123 ) , where oj = i / ? pj - 123
and might be dangerous especially when a higher rank approximation is needed .
it turns out that we can improve any basis p as an approximation to an eigensubspace of s by smoothing each of the columns using s , followed by an orthogonalization .
the matrix algebra literature this corresponds to an iteration of the q - r algorithm for finding an eigensubspace of a symmetric matrix .
the q - r algorithm is a generalization of the power method for iteratively finding a single eigenvector .
let qr= sp , where q is orthogonal and r is the upper triangular matrix that orthog - onalizes sp .
thaen , ( qtsq ) qt is a better approximation to s than is p ( ptsp ) pt , or , in terms of s , s ( q* ) is better than s ( p* ) .
we can be more precise about these improvements .
proposition 123
let p be any n x k orthonormal basis and let qr = sp define an improved orthonormal basis q for approximating the symmetric smoother s with eigenvalues in ( 123 , 123 )
where s ( p* ) p ( ptsp ) pt
||s - s ( q* ) iif < - iis - s ( p* ) iif and s ( q* ) is defined similarly .
there is strict equality if the columns of p coincide with a subset of the columans of u , the eigenvectors of s .
notice that the proposition is not stated for s ( p ) and s ( q ) ; there are counter - examples .
a proof of the proposition is given in appendix a and depends on a lemma proved by jeff lagarias .
lagarias ( 123 ) explored inequalities of this nature in a more general context .
each iteration of this q - r algorithm requires k - 123 additional applications of the smoother , and an order k eigendecomposition .
the need for this added accuracy depends on the particular application .
so far we have found that s ( p* ) is sufficiently accurate for our applications , which typically involve small df .
application : additive models
our motivation for developing pseudosplines was to facilitate some of the difficult
computations required for analysing additive models - this section describes some of these .
these are by no means the only applications .
any scenario where smoothing splines and other smoothers are used , especially in a compound , non - standard fashion , can benefit from the parsimonious representation .
the penalized least squares criterion ( 123 ) is easily generalized for fitting an additive
yi = a + e f / ( xi ) + ei
( buja et al . , 123 ) :
min e yi - a - e fj ( ) e w123 _
jy fj " t ( t ) 123dt .
it can be shown that the solutions satisfy
i s , s , . . .
s , fs
ksp sp sp . . .
i > fp
where each of the sj is the appropriate smoothing spline matrix ( each sj actually represents ejtsjej where ej orders x ) and fj the vector of evaluations of the ) th function .
this np x np system is prohibitively expensive to solve directly - o ( ( np ) 123 ) - although by taking into account the special structure of the smoothing spline matrices can be reduced to an n x n system and hence is 123 ( n123 ) .
buja et al .
( 123 ) described a backfitting or blockwise gauss - seidel algorithm for solving the system iteratively .
it is particularly well suited for the job , since the operations sjz can be
computed in o ( n ) operations ( o ( n log n ) if the data are to be sorted ) , and thus the whole solution can be obtained in 123 ( npq ) operations , where q is the number of complete cycles .
sometimes the iterations converge slowly , especially if the smooth - ing windows are small and / or the variables are near collinear or conc rvous .
and as before d , ' , .
= ( i+ do ' ) ' .
what happens if we plug them into equation ( 123 123 ) ? since fj lies in c ( ip ) we can write f= pjp , j .
it is not difficult to show that equation ( 123 )
suppose that for each smoother sj we have a rank kj approximation =
( ptp + do ) f = pty
where p = ( p123 : p123 : . .
. : pp ) , and do and 123 are similarly composite versions of the separate penalties and coefficients .
this system has dimension k = ej kj , typically between 123p and lop , and much smaller than the original np .
in fact each pj smoother includes a constant column which we do not replicate in p , so the real dimension is k - p + 123
the estimate has the form of a generalized ridge regression as in section 123 for the
single smoother , with criterion
q ( o123 ) = iiy - _pp123 + fttd123fl .
from our knowledge of the form of the contributions to do , we see that the higher order components belonging to each variable are penalized simultaneously .
we could add an additional parameter aj for each term as in section 123 , or else a global shrinking parameter a .
123 shows the pseudo - additive model fit for three variables from the air pollution data set of breiman and friedman ( 123 ) .
the functions each have approximately 123 degrees of freedom , and each are approximated by seven pseudo - eigenvectors .
the dotted functions were obtained by using the backfitting algorithm with the same smoothing splines used in the approximations .
hat matrices
the fitted functions in fig .
123 have been enhanced by plotting standard error ( gcv ) , sensitivity analysis and curves .
later we discuss generalized cross - validation diagnostics .
the main ingredient for computing all of these is the hat matrix g for the d ) - 123pt , as well as the gq that produce fitf y 123fj = gy where g = p ( pp+ the individual fitted functions f g123y .
here g .
= j ( ptp + d ) 123pt where ( ptp + do ) 123 - .
denotes the appropnate submatrix consisting of k .
of the k rows of ( ptp + d i ) 123 here we see the real strength of the additive model approximations .
hastie and tibshirani ( 123 ) used the backfitting algorithm itself to compute g and the individual gj .
they simply ran the backfitting algorithm n times , each time using for y a column of the n x n identity matrix , and ' hence built up g and the gj a column at a time .
since this is 123 ( n123 ) ( with a large constant ) it is typically used only once at the end of a series of fits in an analysis .
the approximations , computed along with each fit .
not only are they available cheaply , but when used their factored form can be exploited to reduce the particular computations .
in contrast , can be routinely
dagot pressure gradsient
inversion ease height
inversion bass temperature
pseudoadditive model fitted to some air pollution data : the fitted functions are plotted on the same scale and superimposed on the plots ( - . . . . . . .
- ) are the fitted functions obtained using the backfitting algorithm with the original smoothing splines; included also are bands of twice the standard error curves , which also give an indication of influence; the rug plot at the base of each figure indicates the occurrence of data , jittered to represent ties
if we assume that the yi are independent and identically distributed with variance
o - 123 from equation ( i123t )
standard errors
( p p + do ) - ptp ( ptp + do ) - o123
as in buja et al .
( 123 ) and cleveland and devlin ( 123 ) we use rss / ( n - dl ) to
thus co ( =ggta - 123 = pj cv ov ( o ) j denotes the appropriate kj x kj submatrix of equation ( 123 ) .
the standard error curve for f reurs the diagonal of gjgt and an estimate for o123 .
estimate a123where n - d is an appropriate estimate of residual degrees of freedom .
since e ( rss ) - =tr ( ( i - g ) t ( i - g ) o - 123 + bias term we use d , = 123 tr ( g ) - tr ( g tg ) .
alternatively we can follow the bayesian route for smoothing splines ( hastie and tibshirani ( i123 ) , section 123 ) .
again these hat matrices are essential - for example , the posterior covariance of f+ under the natural prior is go ' .
generalized cross - validati ' on
when fitting additive models , we need to specify the amount of smoothing for each of the terms in the model .
one approach is to generalize the automatic methods for univariate smoothers , such as cross - validation or gcv .
gu et al .
( 123 ) described a gcv approach for smoothing splines which uses the newton method to optimize gcv ( a ) with respect to its vector argument a consisting of a parameter aj for each of the p vagtables in the model .
in our case the gcv cebteraon is
gcv ( a ) = n123li - g ( a ) y123
where g ( a ) p ( pitp + dao ) - pt and dab is block diagonal having ajd123
as the ith block .
gu et al .
( 123 ) have a similar form for gcv and discuss several decom - positions for optimizing it efficiently .
we shall not go into further details here but point out that all their algorithms are o ( k123 ) , where k is the rank of p .
in their case , k= n and represents a significant computational cost; here we can use their same algorithms and trade off this computational cost with that incurred by using a rank k approximation to the system .
we can use approximation ( 123 ) to gain insight into the nature and stability of the solutions , exactly here and approximately for the system ( 123 ) .
buja et al .
( 123 ) introduced the concept of concurvity , which we illustrate here for the case p = 123
equation ( 123 ) reduces to ( ( p123 p
) ( : pty; )
suppose that a column of p123 , say u , has correlation 123 with a column of p123 , say v .
this means that two columns of the left matrix pt p are identical .
however , if there are corresponding non - zero entries in the penalty part do = diag ( do , , do123 ) , this will not result in a degeneracy .
the only cases where degeneracies can occur are where the two corresponding penalty columns are zero; this will happen if the linear functions ( which carries over to the general p case are collinear .
this is a simple demonstration as well ) of the result in buja et al .
( 123 ) for general smoothers : the only exact concurvity that can exist is collinearity .
exact and approximate concurvities are defined as appropriately constrained low order eigenvectors of ( ptp + do ) ; details are beyond the scope of this paper .
donnell et al .
( 123 ) defined and discussed concurvity in general , and the related concept of additive principal components .
they made use of the approximations developed here in some of their examples .
there are several situations that call for weighted smoothers or additive model fits .
( a ) tied predictors : when there are tied values for a predictor , the correct approach for smoothing splines is ( i ) to collapse the responses to their averages at the unique values of the ( ii ) to perform a weighted fit with weights proportional to the numbers of if the responses are measured with different precision , or ( c ) generalized additive models : the newton - raphson algorithm for fitting gener - alized additive models ( hastie and tibshirani , 123 ) calls for a weighted additive model fit at each iteration .
known to have different variances , a weighted fit is more efficient .
we are now faced with a possible dilemma when approximating a weighted smoother sw , since its eigenvectors will not be the same as those of s .
this would seem to imply that each time we changed the weights ( for example in the third item above ) we would have to compute a new approximation .
our approach is to use the same basis vectors and penalties derived for the unweighted case , and simply to compute a weighted ridge regression when the observation weights change .
we can justify this when approximating smoothing splines .
the eigenvectors of s = ( i+ ak ( 123 are also those of k , and the eigenvalues of k are ok .
if we view our approximation in the unweighted case as a method for approximating k , then we can use it to construct the weighted version of s as well .
this gives sw = ( w+ pdept ) i w , and since we confine f e c ( p ) this is equivalent to f = p ( pt wp + d p ) - ipt wy a weighted ridge regression , which is what we wanted .
see appendix a for computing this estimate .
a point of possible confusion is when we use the pseudobases of several variables ( each with a different number of ties ) in an additive model fit .
we simply replicate the bases vectors for the tied observations and deal with full size n - vectors for each covariate .
once again this is equivalent in the univariate case to performing the weighted generalized ridge regression .
bates and wahba ( 123 ) discussed approximations for computing the gcv statistic for smoothing splines and similar problems .
their approximation involves a pivoted q - r - decomposition of the b - spline or other cubic spline basis used to compute the smoothing spline , and thus intimate knowledge of the smoother used .
our approximation is similar but can be used for any smoother .
although we have used orthogonal polynomials to ' seed ' our approximations , other more suitable candidates can be used .
recall that our preferred pseudospline 123 ( p* ) relies only on the fact that c ( p ) - c ( uk ) , whereas the eigendecomposition of ptsp sorts out the order .
thus a system that is better behaved than polynomials , such as trigonometric series or fixed knot splines , may provide a good approximation with a gain in stability; we have not explored this area .
regression splines are an alternative low rank method for smoothing and additive modelling ( stone and koo , 123; friedman and silverman , 123 ) .
we need to select a regression basis for each variable , a popular choice being piecewise cubic polynomials .
these in turn require the choice of knots , whose number determines the dimensiori of the basis , and whose position determines their nature .
given a basis for each variable , the regression is computed by projection onto the union of the bases .
for reasonably low rank models , it becomes crucial where these one or two interior knots are placed on a variable .
smoothing splines and similar ' shrinking ' smoothers represent an alternative
philosophy .
they use a high dimensional basis for each variable , but then rather than compute the regression by projection they use penalized least squares; this dampens the influence of elements of the basis in a structured way .
this in turn reduces the effective dimension of the fit but allows access to the richer class of functions .
pseudosplines come somewhere in between; they use a ' medium ' rank basis but
also perform shrinking .
in doing so they expose the structure of the class of shrinking smoothers in a parsimonious way ( see fig .
if the basis is chosen to estimate the
important components of a smoothing spline basis , not much is lost .
they provide an analytical tool for understanding the behaviour of a number of such smoothers operating jointly as in an additive model fit .
if too small a rank k is chosen , the family of pseudosplines will be limited to fits of total rank k which may not be sufficient .
we have not pursued any systematic way of determining an ' optimal ' value for k , since typically we intend to use the pseudospline as a building block .
it seems reasonable to use a generous value for k and then to explore smoother submodels via the parameterized form ( 123 ) , especially if this permits complicated compound fits such as in the additive model .
in our applications we have found k = 123 to be reasonable .
we have only touched on some applications in this paper .
other important
application areas are as follows :
( a ) hastie and tibshirani ( 123 ) derived diagnostic measures for additive models , which generalize the univariate versions developed for smoothing splines ( eubank , 123 ) , as well as those developed for ridge regression ( e . g .
eubank and gunst ( 123 ) and walker and birch ( 123 ) ) - typically the smoothing matrices gj and g of section 123 , or at least their diagonals , are required; the approximations can be used instead; influence measures based on the joint behaviour of the covariates as well as residuals , analogous to the influence diagnostics of linear regression;
( c ) understanding the effect on the influence diagnostics when the amount of
smoothing is changed , as well as the dimension of the pseudobases;
( d ) understanding the effects of near concurvity , and the causes; ( e ) fitting additive models in complex scenarios where iterative algorithms are not easily available ( cox model ) or where they have convergence problems
( f ) providing explicit solutions for more general penalized multivariate functional models , such as functional canonical correlation analysis and ace ( breiman and friedman , 123 ) .
this paper has benefited from many discussions with andreas buja , john chambers , jeff lagarias , colin mallows , vijay nair , daryl pregibon and rob tibshirani , as well as the comments of the referees on earlier drafts .
section 123 . 123 of hastie and tibshirani ( 123 ) was based on an earlier and longer version of this paper; the present version has been shortened to avoid overlap .
this research was done while the author was a member of the statistics and data
analysis group , at&t bell laboratories , murray hill , new jersey .
iterating pseudospline approximation
proposition 123
let p be any n x k orthonormal basis and let qr = sp define an improved
orthonormal basis q for approximating the symmetric smoother s with eigenvalues in ( 123 , 123 ) .
where s ( p* ) = p ( ptsp ) pt and s ( q* ) is defined similarly .
since qtq = i , we have that rtr - pt123p .
let s = udut be the eigen - decomposition of s , and let v = utp .
note that v is also n x k orthonormal .
expanding the squared norm on the left - hand side we obtain
is - s ( q* ) 123 = tr ( ( s - qq tsqq t ) t ( s _ qq tsqq t ) )
= tr ( sts ) - tr ( ( qtsq ) 123
and similarly for the norm on the right - hand side .
we therefore need to show that tr ( qtsq ) 123 > tr ( ptsp ) 123
tr ( qtsq ) 123 = tr ( r - tpts ptr - lr tpts ptr
= tr ( vtd123 ) ( vtd123 jyl ( vtd123 v ) ( vtd123 vy - 123 = tr ( ( vtd123 j iv / 123 ( vtd123 j / ( vtd123 ey - 123 / 123 ) 123
it is sufficient to show that
( vtd123 ey - 123 / 123 ( vtd123 j ) ( vtd123 ) 123 / 123 v d v ,
since then the trace inequality ( for any positive power ) follows .
this is shown in lemma 123
lemma 123 ( lagarias , 123 ) .
define v and d as above
> ( vtd123 j ) l / 123 ( vtdv ) ( vtd123 v ) 123 / 123
since ( vtd123 v / ) 123 is positive definite , this is equivalent to what is required in the proof of
by ando ( 123 ) , corollary 123 , part ii , vta v > ( vtaa v ) l / a for a e e123 ( 123 ) and a positive definite .
taking a = d123 and a = 123 we obtain vtd123 v > ( vtd123 v ) 123 / 123
applying the ando result again , we obtain vtd123 v ' > ( vtd v ) 123
now if b > c > 123 , b and c symmetric , then b123 > , c123 for , 123 e ( 123 , 123 ) ( e . g .
chan and kwong ( 123 ) ) .
this gives ( vtd123v ) 123 > ( vtdj ) , and thus
= ( vtd123 j ) l / 123 ( vtd123 j123 / 123 ( vt d123 v ) 123
( vtd123 v ) l / 123 ( vtdjv ) ( vtd123 v ) 123 / 123
computational details
even though the computational burden has been dramatically reduced , it can still be costly to manipulate regressions with lop predictors unless we are careful .
we outline an approach for the unweighted case since it is slightly simpler and the ideas are the same as in the a well - known trick ( for example see golub and van loan ( 123 ) ) reduces the generalized ridge regression problem ( 123 ) to an ordinary least squares problem .
define the augmented regression matrix and response variable
and y* ( o ) .
then least squares regression of y* onto pa gives the correct coefficients
denote the q - r - decomposition of pa , where q is ( n + k ) x k orthogonal and r is k x k upper triangular .
then p = q123r , d " / 123 = q123r with qtq = qtqi + qtq123 = l the following are
( b ) under independent and identically distributed errors
cov ( 123 ) = or - 123 ( i_ r - tdr ) rt
( c ) g = qiqtand cov ( f+ ) = ggt123 = qi ( i - qtq123 ) q123l123 = qi ( i - r tder ' ) qta123
the gj require a little more work , since we destroy the q - r structure when we look at subsets .
nevertheless , g .
= p ( r - 123 ) qt cov ( f123 ) = pj ( y ) pjt , and we can exploit the upper triangular structure of r - 123 and e ( and their jth partitions ) in the computations .
