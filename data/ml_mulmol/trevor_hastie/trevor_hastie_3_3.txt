boosting has been a very successful technique for solving the two - class classication problem .
it was rst introduced by ( 123 ) , with their adaboost algorithm .
in going from two - class to multi - class classication , most boosting algorithms have been restricted to reducing the multi - class classica - tion problem to multiple two - class problems , e . g .
( 123 ) , ( 123 ) , and ( 123 ) .
the ways to extend adaboost from two - class to multi - class depend on the interpretation or view of the suc - cess of adaboost in binary classication , which still remains controversial .
much theoretical work on adaboost has been based on the margin analysis , for example , see ( 123 ) and ( 123 ) .
another view on boosting , which is popular in the statistical community , regards adaboost as a functional gradient de - scent algorithm ( 123 , 123 , 123 ) .
in ( 123 ) , adaboost has been shown to be equivalent to a forward stagewise additive modeling al - gorithm that minimizes the exponential loss .
( 123 ) suggested that the success of adaboost can be understood by the fact
we thank the ae and a referee for their helpful comments and sug -
gestions which greatly improved our paper .
zhu was partially supported by nsf grant dms - 123
zou was partially supported by nsf grant dms - 123
hastie was partially supported by nsf grant dms - 123
that the population minimizer of exponential loss is one - half of the log - odds .
based on this statistical explanation , ( 123 ) derived a multi - class logit - boost algorithm .
the multi - class boosting algorithm by ( 123 ) looks very dif - ferent from adaboost , hence it is not clear if the statis - tical view of adaboost still works in the multi - class case .
to resolve this issue , we think it is desirable to derive an adaboost - like multi - class boosting algorithm by using the exact same statistical explanation of adaboost .
in this pa - per , we develop a new algorithm that directly extends the adaboost algorithm to the multi - class case without reduc - ing it to multiple two - class problems .
surprisingly , the new algorithm is almost identical to adaboost but with a sim - ple yet critical modication , and similar to adaboost in the two - class case , this new algorithm combines weak clas - siers and only requires the performance of each weak clas - sier be better than random guessing .
we show that the proposed multi - class adaboost algorithm is equivalent to a forward stagewise additive modeling algorithm that mini - mizes a novel exponential loss for multi - class classication .
furthermore , we show that the exponential loss is a mem - ber of a class of fisher - consistent loss functions for multi - class classication .
combined with forward stagewise addi - tive modeling , these loss functions can be used to derive various multi - class boosting algorithms .
we believe this pa - per complements ( 123 ) .
before delving into the new algorithm for multi - class boosting , we briey review the multi - class classication problem and the adaboost algorithm ( 123 ) .
suppose we are given a set of training data ( x123 , c123 ) , .
, ( xn , cn ) , where the input ( prediction variable ) xi rp , and the output ( re - sponse variable ) ci is qualitative and assumes values in a nite set , e . g .
( 123 , 123 , .
k is the number of classes .
usu - ally it is assumed that the training data are independently and identically distributed samples from an unknown prob - ability distribution prob ( x , c ) .
the goal is to nd a classi - cation rule c ( x ) from the training data , so that when given a new input x , we can assign it a class label c from ( 123 , .
under the 123 / 123 loss , the misclassication error rate of a classi - ic ( x ) =kprob ( c = k|x )
er c ( x ) is given by 123 ( cid : 123 )
it is clear that
( x ) = arg max
prob ( c = k|x = x )
will minimize this quantity with the misclassication error rate equal to 123 ex maxk prob ( c = k|x ) .
this classier is
known as the bayes classier , and its error rate is the bayes
the adaboost algorithm is an iterative procedure that ( x ) by combining tries to approximate the bayes classier c many weak classiers .
starting with the unweighted train - ing sample , the adaboost builds a classier , for example a classication tree ( 123 ) , that produces class labels .
if a training data point is misclassied , the weight of that training data point is increased ( boosted ) .
a second classier is built us - ing the new weights , which are no longer equal .
again , mis - classied training data have their weights boosted and the procedure is repeated .
typically , one may build 123 or 123 classiers this way .
a score is assigned to each classier , and the nal classier is dened as the linear combination of the classiers from each stage .
specically , let t ( x ) denote a weak multi - class classier that assigns a class label to x , then the adaboost algorithm proceeds as follows : algorithm 123
adaboost ( 123 )
initialize the observation weights wi = 123 / n , i =
123 , 123 ,
for m = 123 to m :
( a ) fit a classier t ( m ) ( x ) to the training data using
ci ( cid : 123 ) = t ( m ) ( xi )
( m ) = log
wi wi exp
for i = 123 , 123 , .
( e ) re - normalize wi .
ci ( cid : 123 ) = t ( m ) ( xi )
c ( x ) = arg max
( m ) i ( t ( m ) ( x ) = k ) .
when applied to two - class classication problems , ad - aboost has been proved to be extremely successful in pro - ducing accurate classiers .
in fact , ( 123 ) called adaboost with trees the best o - the - shelf classier in the world .
how - ever , it is not the case for multi - class problems , although adaboost was also proposed to be used in the multi - class case ( 123 ) .
note that the theory of ( 123 ) assumes that the error of each weak classier err ( m ) is less than 123 / 123 ( or equiva - lently ( m ) > 123 ) , with respect to the distribution on which
zhu et al .
it was trained .
this assumption is easily satised for two - class classication problems , because the error rate of ran - dom guessing is 123 / 123
however , it is much harder to achieve in the multi - class case , where the random guessing error rate is ( k 123 ) / k .
as pointed out by the inventors of adaboost , the main disadvantage of adaboost is that it is unable to handle weak learners with an error rate greater than 123 / 123
as a result , adaboost may easily fail in the multi - class case .
to illustrate this point , we consider a simple three - class simu - lation example .
each input x r123 , and the ten input vari - ables for all training examples are randomly drawn from a ten - dimensional standard normal distribution .
the three classes are dened as :
if 123 ( cid : 123 )
j < 123 j < 123
123 , k / 123 is the ( k / 123 ) 123% quantile of the 123
tion , so as to put approximately equal numbers of observa - tions in each class .
in short , the decision boundaries separat - ing successive classes are nested concentric ten - dimensional spheres .
the training sample size is 123 with approximately 123 training observations in each class .
an independently drawn test set of 123 observations is used to estimate the
figure 123 ( upper row ) shows how adaboost breaks using ten - terminal node trees as weak classiers .
as we can see ( upper left panel ) , the test error of adaboost actually starts to increase after a few iterations , then levels o around 123 .
what has happened can be understood from the upper mid - dle and upper right panels : the err ( m ) starts below 123; after a few iterations , it overshoots 123 ( ( m ) < 123 ) , then quickly hinges onto 123 .
once err ( m ) is equal to 123 , the weights of the training samples do not get updated ( ( m ) = 123 ) , hence the same weak classier is tted over and over again but is not added to the existing t , and the test error rate stays
this illustrative example may help explain why ad - aboost is never used for multi - class problems .
instead , for multi - class classication problems , ( 123 ) proposed the ad - aboost . mh algorithm which combines adaboost and the one - versus - all strategy .
there are also several other multi - class extensions of the boosting idea , for example , the ecoc in ( 123 ) and the logit - boost in ( 123 ) .
123 multi - class adaboost
we introduce a new multi - class generalization of ad - aboost for multi - class classication .
we refer to our algo - rithm as samme stagewise additive modeling using a multi - class exponential loss function this choice of name will be clear in section 123
given the same setup as that of adaboost , samme proceeds as follows :
figure 123
comparison of adaboost and the new algorithm samme on a simple three - class simulation example .
the training sample size is 123 , and the testing sample size is 123
ten - terminal node trees are used as weak classiers .
the upper row
is for adaboost and the lower row is for samme .
algorithm 123
initialize the observation weights wi = 123 / n , i =
123 , 123 ,
for m = 123 to m :
( a ) fit a classier t ( m ) ( x ) to the training data using
ci ( cid : 123 ) = t ( m ) ( xi )
( m ) = log
+ log ( k 123 ) .
ci ( cid : 123 ) = t ( m ) ( xi )
wi wi exp
for i = 123 , .
( e ) re - normalize wi .
c ( x ) = arg max
( m ) i ( t ( m ) ( x ) = k ) .
note that algorithm 123 ( samme ) shares the same simple modular structure of adaboost with a simple but subtle dif - ference in ( 123 ) , specically , the extra term log ( k 123 ) .
obvi - ously , when k = 123 , samme reduces to adaboost .
however , the term log ( k 123 ) in ( 123 ) is critical in the multi - class case ( k > 123 ) .
one immediate consequence is that now in order
multi - class adaboost 123
for ( m ) to be positive , we only need ( 123 err ( m ) ) > 123 / k , or the accuracy of each weak classier to be better than ran - dom guessing rather than 123 / 123
to appreciate its eect , we apply samme to the illustrative example in section 123 .
as can be seen from fig .
123 , the test error of samme quickly decreases to a low value and keeps decreasing even after 123 iterations , which is exactly what we could expect from a successful boosting algorithm .
in section 123 , we shall show that the term log ( k 123 ) is not articial , it follows naturally from the multi - class generalization of the exponential loss in the binary case .
the rest of the paper is organized as follows : in sec - tion 123 , we give theoretical justication for our new algo - rithm samme .
in section 123 , we present numerical results on both simulation and real - world data .
summary and dis - cussion regarding the implications of the new algorithm are in section 123
statistical justification
in this section , we are going to show that the extra term log ( k 123 ) in ( 123 ) is not articial; it makes algorithm 123 equivalent to tting a forward stagewise additive model us - ing a multi - class exponential loss function .
our arguments are in line with ( 123 ) who developed a statistical perspective on the original two - class adaboost algorithm , viewing the two - class adaboost algorithm as forward stagewise additive modeling using the exponential loss function
l ( y , f ) = e
where y = ( i ( c = 123 ) i ( c = 123 ) ) ( 123 , 123 ) in a two - class classication setting .
a key argument is to show that the population minimizer of this exponential loss function is one half of the logit transform
( x ) = arg min
ey |x=xl ( y , f ( x ) )
prob ( c = 123|x ) prob ( c = 123|x ) .
therefore , the bayes optimal classication rule agrees with ( x ) .
( 123 ) recast adaboost as a functional gra - the sign of f ( x ) .
we note that dient descent algorithm to approximate f besides ( 123 ) , ( 123 ) and ( 123 ) also made connections between the original two - class adaboost algorithm and the exponential loss function .
we acknowledge that these views have been inuential in our thinking for this paper .
123 samme as forward stagewise additive
we now show that algorithm 123 is equivalent to forward stagewise additive modeling using a multi - class exponential we start with the forward stagewise additive modeling using a general loss function l ( , ) , then apply it to the
zhu et al .
multi - class exponential loss function .
in the multi - class clas - sication setting , we can recode the output c with a k - dimensional vector y , with all entries equal to 123 a 123 in position k if c = k , i . e .
y = ( y123 , .
, yk ) t , and :
if c = k , if c ( cid : 123 ) = k .
( 123 ) and ( 123 ) used the same coding for the multi - class sup - port vector machine .
given the training data , we wish to nd f ( x ) = ( f123 ( x ) , .
, fk ( x ) ) t such that
f123 ( x ) + + fk ( x ) = 123
we consider f ( x ) that has the following form :
where ( m ) r are coecients , and g ( m ) ( x ) are basis func - tions .
we require g ( x ) to satisfy the symmetric constraint :
g123 ( x ) + + gk ( x ) = 123
for example , the g ( x ) that we consider in this paper takes value in one of the k possible k - dimensional vectors in ( 123 ) ; specically , at a given x , g ( x ) maps x onto y :
g : x rp y ,
where y is the set containing k k - dimensional vectors :
, 123 k123 , 123 ,
k123 , 123
forward stagewise modeling approximates the solution to ( 123 ) ( 123 ) by sequentially adding new basis functions to the expansion without adjusting the parameters and coecients of those that have already been added .
specically , the al - gorithm starts with f ( 123 ) ( x ) = 123 , sequentially selecting new basis functions from a dictionary and adding them to the algorithm 123
forward stagewise additive modeling
initialize f ( 123 ) ( x ) = 123
for m = 123 to m :
= arg min
l ( yi , f ( m123 ) ( xi ) + g ( xi ) ) .
now , we consider using the multi - class exponential loss
l ( y , f ) = exp
f ( m ) ( x ) = f ( m123 ) ( x ) + ( m ) g ( m ) ( x ) .
( y123f123 + + ykfk )
in the above forward stagewise modeling algorithm .
the choice of the loss function will be clear in section 123 and section 123 .
then in step ( 123a ) , we need to nd g ( m ) ( x ) ( and ( m ) ) to solve :
i ( f ( m123 ) ( xi ) + g ( xi ) )
= arg min
= arg min
i f ( m123 ) ( xi )
where wi = exp
are the un - normalized
notice that every g ( x ) as in ( 123 ) has a one - to - one corre - spondence with a multi - class classier t ( x ) in the following
and vice versa :
t ( x ) = k ,
if gk ( x ) = 123 ,
if t ( x ) = k , if t ( x ) ( cid : 123 ) = k .
hence , solving for g ( m ) ( x ) in ( 123 ) is equivalent to nding the multi - class classier t ( m ) ( x ) that can generate g ( m ) ( x ) .
lemma 123
the solution to ( 123 ) is
= arg min
wii ( ci ( cid : 123 ) = t ( xi ) ) ,
+ log ( k 123 ) ci ( cid : 123 ) = t ( m ) ( xi )
where err ( m ) is dened as
this is equal to
based on lemma 123 , the model is then updated f ( m ) ( x ) = f ( m123 ) ( x ) + ( m ) g ( m ) ( x ) ,
and the weights for the next iteration will be
wi wi exp
if ci = t ( xi ) , if ci ( cid : 123 ) = t ( xi ) ,
where ( m ) is dened as in ( 123 ) with the extra term log ( k 123 ) , and the new weight ( 123 ) is equivalent to the weight up - dating scheme in algorithm 123 ( 123d ) after normalization .
arg maxk ( f ( m ) ( x ) , .
, f ( m ) m=123 ( m ) i ( t ( m ) ( x ) = k ) in output c ( x ) = arg maxk algorithm 123
hence , algorithm 123 can be considered as forward stagewise additive modeling using the multi - class exponential loss function .
123 the multi - class exponential loss
we now justify the use of the multi - class exponential loss ( 123 ) .
firstly , we note that when k = 123 , the sum - to - zero constraint indicates f = ( f123 , f123 ) and then the multi - class exponential loss reduces to the exponential loss used in binary classication .
( 123 ) justied the exponential loss by showing that its population minimizer is equivalent to the bayes rule .
we follow the same arguments to investigate what is the population minimizer of this multi - class expo - nential loss function .
specically , we are interested in
ey |x=x exp
( y123f123 ( x ) + + ykfk ( x ) )
subject to f123 ( x ) + + fk ( x ) = 123
the lagrange of this constrained optimization problem can be written as :
prob ( c = 123|x )
( f123 ( x ) + + fk ( x ) ) ,
prob ( c = k|x )
multi - class adaboost 123
where is the lagrange multiplier .
taking derivatives with respect to fk and , we reach
prob ( c = 123|x ) = 123 , prob ( c = k|x ) = 123 , f123 ( x ) + + fk ( x ) = 123
solving this set of equations , we obtain the population min -
k ( x ) = ( k 123 ) log prob ( c = k|x )
log prob ( c = k
for k = 123 ,
k ( x ) = arg max
prob ( c = k|x ) ,
which is the multi - class bayes optimal classication rule .
this result justies the use of this multi - class exponential loss function .
equation ( 123 ) also provides a way to recover the class probability prob ( c = k|x ) once f
k ( x ) s are esti -
prob ( c = k|x ) =
for k = 123 ,
123 ( x ) + + c
123 fisher - consistent multi - class loss
we have shown that the population minimizer of the new multi - class exponential loss is equivalent to the multi - class bayes rule .
this property is shared by many other multi - class loss functions .
let us use the same notation as in sec - tion 123 , and consider a general multi - class loss function
l ( y , f ) =
( y123f123 + + ykfk )
where ( ) is a non - negative valued function .
the multi - t .
we can use the gen - class exponential loss uses ( t ) = e eral multi - class loss function in algorithm 123 to minimize the
however , to derive a sensible algorithm , we need to require the ( ) function be fisher - consistent .
specically , we say
zhu et al .
( ) is fisher - consistent for k - class classication , if for x in a set of full measure , the following optimization problem
( y123f123 ( x ) + + ykfk ( x ) )
subject to f123 ( x ) + + fk ( x ) = 123 , has a unique solution f ,
fk ( x ) = arg max
prob ( c = k|x ) .
we use the sum - to - zero constraint to ensure the existence and uniqueness of the solution to ( 123 ) .
note that as n , the empirical loss in ( 123 ) becomes ( y123f123 ( x ) + + ykfk ( x ) )
therefore , the multi - class fisher - consistent condition basi - cally says that with innite samples , one can exactly recover the multi - class bayes rule by minimizing the multi - class loss using ( ) .
thus our denition of fisher - consistent losses is a multi - class generalization of the binary fisher - consistent loss function discussed in ( 123 ) .
in the following theorem , we show that there are a class of convex functions that are fisher - consistent for k - class classication , for all k 123
theorem 123
let ( t ) be a non - negative twice dierentiable ( cid : 123 ) ( cid : 123 ) ( t ) > 123 for t , then is fisher - consistent for k - class classication for k 123
moreover , let f be the solution of ( 123 ) , then we have
( cid : 123 ) ( 123 ) < 123 and
prob ( c = k|x ) =
for k = 123 ,
theorem 123 immediately concludes that the three most popular smooth loss functions , namely , exponential , logit and l123 loss functions , are fisher - consistent for all multi - class classication problems regardless the number of classes .
the inversion formula ( 123 ) allows one to easily con - struct estimates for the conditional class probabilities .
ta - ble 123 shows the explicit inversion formulae for computing the conditional class probabilities using the exponential , logit and l123 losses .
with these multi - class fisher - consistent losses on hand , we can use the forward stagewise modeling strategy to de - rive various multi - class boosting algorithms by minimizing the empirical multi - class loss .
the biggest advantage of the exponential loss is that it gives us a simple re - weighting for - mula .
other multi - class loss functions may not lead to such a simple closed - form re - weighting scheme .
one could han - dle this computation issue by employing the computational
table 123
the probability inversion formula
( t ) = e
( t ) = log ( 123 + e
( t ) = ( 123 t ) 123
prob ( c = k|x )
trick used in ( 123 ) and ( 123 ) .
for example , ( 123 ) derived a multi - class boosting algorithm using the logit loss .
a multi - class version of the l123 boosting can be derived following the lines in ( 123 ) .
we do not explore these directions in the current pa - per .
to x ideas , we shall focus on the multi - class adaboost
numerical results
in this section , we use both simulation data and real - world data to demonstrate our multi - class adaboost algo - rithm .
for comparison , a single decision tree ( cart; ( 123 ) ) and adaboost . mh ( 123 ) are also t .
we have chosen to com - pare with the adaboost . mh algorithm because it is concep - tually easy to understand and it seems to have dominated other proposals in empirical studies ( 123 ) .
indeed , ( 123 ) also argue that with large samples , adaboost . mh has the op - timal classication performance .
the adaboost . mh algo - rithm converts the k - class problem into that of estimating a two - class classier on a training set k times as large , with an additional feature dened by the set of class labels .
it is essentially the same as the one vs .
rest scheme ( 123 ) .
we would like to emphasize that the purpose of our nu - merical experiments is not to argue that samme is the ul - timate multi - class classication tool , but rather to illustrate that it is a sensible algorithm , and that it is the natural ex - tension of the adaboost algorithm to the multi - class case .
we mimic a popular simulation example found in ( 123 ) .
this is a three - class problem with twenty one variables , and it is considered to be a dicult pattern recognition problem with bayes error equal to 123 .
the predictors are dened by
u v123 ( j ) + ( 123 u ) v123 ( j ) + j , class 123 , u v123 ( j ) + ( 123 u ) v123 ( j ) + j , class 123 , u v123 ( j ) + ( 123 u ) v123 ( j ) + j , class 123 ,
where j = 123 , .
, 123 , u is uniform on ( 123 , 123 ) , j are standard normal variables , and the v ( cid : 123 ) are the shifted triangular wave - forms : v123 ( j ) = max ( 123 |j 123| , 123 ) , v123 ( j ) = v123 ( j 123 ) and v123 ( j ) = v123 ( j + 123 ) .
the training sample size is 123 so that approximately 123 training observations are in each class .
we use the classi - cation tree as the weak classier for samme .
the trees are built using a greedy , top - down recursive partitioning strat - egy , and we restrict all trees within each method to have the
same number of terminal nodes .
this number is chosen via ve - fold cross - validation .
we use an independent test sam - ple of size 123 to estimate the error rate .
averaged results over ten such independently drawn training - test set combi - nations are shown in fig .
123 and table 123
as we can see , for this particular simulation example , samme performs slightly better than the adaboost . mh al - gorithm .
a paired t - test across the ten independent compar - isons indicates a signicant dierence with p - value around
123 real data in this section , we show the results of running samme on a collection of datasets from the uc - irvine machine learn - ing archive ( 123 ) .
seven datasets were used : letter , nursery , pendigits , satimage , segmentation , thyroid and vowel .
these datasets come with pre - specied training and testing sets , and are summarized in table 123
they cover a wide range of scenarios : the number of classes ranges from 123 to 123 , and the size of the training data ranges from 123 to 123 , 123 data points .
the types of input variables in - clude both numerical and categorical , for example , in the nursery dataset , all input variables are categorical vari - ables .
we used a classication tree as the weak classier in each case .
again , the trees were built using a greedy , top - down recursive partitioning strategy .
we restricted all trees within each method to have the same number of ter - minal nodes , and this number was chosen via ve - fold cross -
figure 123 compares samme and adaboost . mh .
the test error rates are summarized in table 123
the standard er - te . err ( 123 te . err ) / n . te , where rors are approximated by te . err is the test error , and n . te is the size of the testing
the most interesting result is on the vowel dataset .
this is a dicult classication problem , and the best methods achieve around 123% errors on the test data ( 123 ) .
the data was collected by ( 123 ) , who recorded examples of the eleven steady state vowels of english spoken by fteen speakers for a speaker normalization study .
the international phonetic association ( ipa ) symbols that represent the vowels and the words in which the eleven vowel sounds were recorded are given in table 123
four male and four female speakers were used to train the classier , and then another four male and three fe - male speakers were used for testing the performance
multi - class adaboost 123
figure 123
test errors for samme and adaboost . mh on the waveform simulation example .
the training sample size is 123 , and the testing sample size is 123
the results are averages of over ten independently drawn training - test set combinations .
table 123
test error rates % of dierent methods on the
waveform data .
the results are averaged over ten
independently drawn datasets .
for comparison , a single
decision tree is also t
cart error = 123 ( 123 )
table 123
summary of seven benchmark datasets
speaker yielded six frames of speech from eleven vowels .
this gave 123 frames from the eight speakers used as the train - ing data and 123 frames from the seven speakers used as the testing data .
ten predictors are derived from the digi - tized speech in a rather complicated way , but standard in the speech recognition world .
as we can see from fig .
123 and table 123 , for this particular dataset , the samme algorithm
zhu et al .
table 123
the international phonetic association ( ipa )
symbols that represent the eleven vowels
performs almost 123% better than the adaboost . mh algo -
for other datasets , the samme algorithm performs than the adaboost . mh algorithm on letter , pendigits , and thyroid , while slightly worse on segmentation .
in the segmentation data , there are only 123 training data points , so the dierence might be just due to randomness .
it is also worth noting that for the nursery data , both the samme algorithm and the ad - aboost . mh algorithm are able to reduce the test error to zero , while a single decision tree has about 123% test error rate .
overall , we are comfortable to say that the performance of samme is comparable with that of the
for the purpose of further investigation , we also merged the training and the test sets , and randomly split them into new training and testing sets .
the procedure was repeated ten times .
again , the performance of samme is comparable with that of the adaboost . mh .
for the sake of space , we do not present these results .
figure 123
test errors for samme and adaboost . mh on six benchmark datasets .
these datasets come with pre - specied
training and testing splits , and they are summarized in table 123
the results for the nursery data are not shown for the test
error rates are reduced to zero for both methods .
the statistical view of boosting , as illustrated in ( 123 ) , shows that the two - class adaboost builds an additive model to approximate the two - class bayes rule .
following the same statistical principle , we have derived samme , the natural and clean multi - class extension of the two - class adaboost algorithm , and we have shown that
samme adaptively implements the multi - class bayes rule by tting a forward stagewise additive model for samme follows closely to the philosophy of boosting , i . e .
adaptively combining weak classiers ( rather than regressors as in logit - boost ( 123 ) and mart ( 123 ) ) into a at each stage , samme returns only one weighted clas -
multi - class adaboost 123
table 123
test error rates % on seven benchmark real
testing splits .
the standard errors ( in parentheses ) are
datasets .
the datasets come with pre - specied training and te . err ( 123 te . err ) / n . te , where te . err is the test error , and n . te is the size of the testing data .
for comparison , a single decision tree was also t , and the tree
size was determined by ve - fold cross - validation
cart error = 123 ( 123 )
cart error = 123 ( 123 )
cart error = 123 ( 123 )
cart error = 123 ( 123 )
cart error = 123 ( 123 )
cart error = 123 ( 123 )
cart error = 123 ( 123 )
sier ( rather than k ) , and the weak classier only needs to be better than k - class random guessing; samme shares the same simple modular structure of
our numerical experiments have indicated that ad - aboost . mh in general performs very well and sammes performance is comparable with that of the adaboost . mh , and sometimes slightly better .
however , we would like to emphasize that our goal is not to argue that samme is the ultimate multi - class classication tool , but rather to il - lustrate that it is the natural extension of the adaboost algorithm to the multi - class case .
the success of samme is used here to demonstrate the usefulness of the forward stagewise modeling view of boosting .
( 123 ) called the adaboost algorithm discrete adaboost and proposed real adaboost and gentle adaboost algo - rithms which combine regressors to estimate the conditional class probability .
using their language , samme is also a dis - crete multi - class adaboost .
we have also derived the corre - sponding real multi - class adaboost and gentle multi - class
zhu et al .
adaboost ( 123 , 123 ) .
these results further demonstrate the usefulness of the forward stagewise modeling view of boost -
it should also be emphasized here that although our sta - tistical view of boosting leads to interesting and useful re - sults , we do not argue it is the ultimate explanation of boost - ing .
why boosting works is still an open question .
interested readers are referred to the discussions on ( 123 ) .
( 123 ) mentioned that the forward stagewise modeling view of adaboost does not oer a bound on the generalization error as in the orig - inal adaboost paper ( 123 ) .
( 123 ) also pointed out that the sta - tistical view of boosting does not explain why adaboost is robust against overtting .
later , his understandings of ad - aboost lead to the invention of random forests ( 123 ) .
finally , we discuss the computational cost of samme .
suppose one uses a classication tree as the weak learner , and the depth of each tree is xed as d , then the computa - tional cost for building each tree is o ( dpn log ( n ) ) , where p is the dimension of the input x .
the computational cost for our samme algorithm is then o ( dpn log ( n ) m ) since there are m iterations .
the samme algorithm has been implemented in the r computing environment , and will be publicly available from the authors websites .
lemma 123
first , for any xed value of > 123 , using the denition ( 123 ) , one can express the criterion in ( 123 ) as :
wii ( ci ( cid : 123 ) = t ( xi ) ) .
since only the last sum depends on the classier t ( x ) , we get that ( 123 ) holds .
now plugging ( 123 ) into ( 123 ) and solving for , we obtain ( 123 ) ( note that ( 123 ) is a convex function
theorem 123
firstly , we note that under the sum - to - zero
( y123f123 ( x ) + + ykfk ( x ) ) prob ( c = 123|x ) +
prob ( c = k|x ) .
therefore , we wish to solve
k 123 f123 ( x ) ) prob ( c = 123|x ) + k 123 fk ( x ) ) prob ( c = 123|x )
fk ( x ) = 123
for convenience , let pk = prob ( c = k|x ) , k = 123 , 123 , .
, k and we omit x in fk ( x ) .
using the lagrangian multiplier ,
q ( f ) = (
k 123 f123 ) p123+ k 123 fk ) pk +
k 123 ( f123 + .
+ fk ) .
then we have
for k = 123 , .
since verse function , denoted by .
equation ( 123 ) gives
by the sum - to - zero constraint on f , we have
k 123 = 123 ,
( cid : 123 ) has an in - k123 fk =
k 123 fk ) pk + ( cid : 123 ) ( cid : 123 ) ( t ) > 123 for t ,
( cid : 123 ) ( 123 ) > 123 , we have
( cid : 123 ) is a strictly monotone increasing function , so is .
thus the left hand size ( lhs ) of ( 123 ) is a decreasing function of .
it suces to show that equation ( 123 ) has a , which is the unique root .
then it is easy to see that fk = ( ) is the unique minimizer of ( 123 ) , for the hessian matrix of q ( f ) is a diagonal matrix and the k - th diagonal element is 123q ( f ) k123 fk ) > 123
note that when = ( cid : 123 ) ( 123 ) ) = 123
so the lhs of ( 123 ) is negative ) < ( ( cid : 123 ) ( 123 ) > 123
on the other hand , let us dene a = ( a : as t ( since is convex ) .
if a is not empty , denote = inf a .
by the fact ( cid : 123 ) ( 123 ) < 123 , we conclude a .
in both cases , we see that ( cid : 123 ) ( t ) 123 as t a a small enough 123 > 123 such that ( 123 ) > 123 for all k .
so the lhs of ( 123 ) is positive when = 123 > 123
therefore there ( cid : 123 ) ( 123 ) ) such that equation ( 123 ) must be a positive holds .
now we show the minimizer f agrees with the bayes rule .
without loss of generality , let p123 > pk for k ( cid : 123 ) = 123
for k ( cid : 123 ) = 123 , we have f123 > fk for k ( cid : 123 ) = 123
for the inversion formula , we note pk =
( cid : 123 ) ( a ) = 123 ) .
if a is an empty set , then
hence it
k=123 pj = 123 requires
then ( 123 ) is
we would like to dedicate this work to the memory of leo breiman , who passed away while we were nalizing this manuscript .
leo breiman has made tremendous contribu - tions to the study of statistics and machine learning .
his work has greatly inuenced us .
received 123 may 123
