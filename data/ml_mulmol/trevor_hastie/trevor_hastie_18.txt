we use convex relaxation techniques to provide a sequence of regularized low - rank solutions for large - scale matrix completion problems .
using the nuclear norm as a regularizer , we provide a sim - ple and very efcient convex algorithm for minimizing the reconstruction error subject to a bound on the nuclear norm .
our algorithm soft - impute iteratively replaces the missing elements with those obtained from a soft - thresholded svd .
with warm starts this allows us to efciently compute an entire regularization path of solutions on a grid of values of the regularization parameter .
the computationally intensive part of our algorithm is in computing a low - rank svd of a dense matrix .
exploiting the problem structure , we show that the task can be performed with a complexity of or - der linear in the matrix dimensions .
our semidenite - programming algorithm is readily scalable to large matrices; for example soft - impute takes a few hours to compute low - rank approximations of a 123 123 incomplete matrix with 123 observed entries , and ts a rank - 123 approximation to the full netix training set in 123 hours .
our methods achieve good training and test errors and exhibit superior timings when compared to other competitive state - of - the - art techniques .
keywords : collaborative ltering , nuclear norm , spectral regularization , netix prize , large scale
in many applications measured data can be represented in a matrix xmn , for which only a rela - tively small number of entries are observed .
the problem is to complete the matrix based on the observed entries , and has been dubbed the matrix completion problem ( cand`es and recht , 123; cand`es and tao , 123; rennie and srebro , 123 ) .
the netix competition ( for example , sigkdd and netix , 123 ) is a popular example , where the data is the basis for a recommender system .
the rows correspond to viewers and the columns to movies , with the entry xi j being the rating ( 123 , .
, 123 ) by viewer i for movie j .
there are about 123k viewers and 123k movies , and hence 123 billion ( 123 123 ) potential entries .
however , on average each viewer rates about 123
also in the department of health , research and policy .
also in the department of statistics .
c ( cid : 123 ) 123 rahul mazumder , trevor hastie and rob tibshirani .
mazumder , hastie and tibshirani
movies , so only 123% or 123 entries are observed .
the task is to predict the ratings that viewers would give to movies they have not yet rated .
these problems can be phrased as learning an unknown parameter ( a matrix zmn ) with very high dimensionality , based on very few observations .
in order for such inference to be meaningful , we assume that the parameter z lies in a much lower dimensional manifold .
in this paper , as is relevant in many real life applications , we assume that z can be well represented by a matrix of low rank , that is , z vmkgkn , where k min ( n , m ) .
in this recommender - system example , low rank structure suggests that movies can be grouped into a small number of genres , with g j the relative score for movie j in genre .
viewer i on the other hand has an afnity vi for genre , and hence the modeled score for viewer i on movie j is the sum ( cid : 123 ) =123vig j of genre afnities times genre scores .
typically we view the observed entries in x as the corresponding entries from z contaminated with
srebro et al .
( 123a ) studied generalization error bounds for learning low - rank matrices .
re - cently cand`es and recht ( 123 ) , cand`es and tao ( 123 ) , and keshavan et al .
( 123 ) showed the - oretically that under certain assumptions on the entries of the matrix , locations , and proportion of unobserved entries , the true underlying matrix can be recovered within very high accuracy .
for a matrix xmn let w ( 123 , .
, m ) ( 123 , .
, n ) denote the indices of observed entries
consider the following optimization problem :
( xi j zi j ) 123 d ,
where d 123 is a regularization parameter controlling the tolerance in training error .
the rank con - straint in ( 123 ) makes the problem for general w combinatorially hard ( srebro and jaakkola , 123 ) .
for a fully - observed x on the other hand , the solution is given by a truncated singular value decom - position ( svd ) of x .
the following seemingly small modication to ( 123 ) ,
( xi j zi j ) 123 d ,
makes the problem convex ( fazel , 123 ) .
here kzk is the nuclear norm , or the sum of the singular values of z .
under many situations the nuclear norm is an effective convex relaxation to the rank constraint ( fazel , 123; cand`es and recht , 123; cand`es and tao , 123; recht et al . , 123 ) .
op - timization of ( 123 ) is a semi - denite programming problem ( boyd and vandenberghe , 123 ) and can be solved efciently for small problems , using modern convex optimization software like sedumi and sdpt123 ( grant and boyd . , 123 ) .
however , since these algorithms are based on second order methods ( liu and vandenberghe , 123 ) , they can become prohibitively expensive if the dimensions of the matrix get large ( cai et al . , 123 ) .
equivalently we can reformulate ( 123 ) in lagrange form
( xi j zi j ) 123 + l kzk .
here l 123 is a regularization parameter controlling the nuclear norm of the minimizer zl of ( 123 ) ; there is a 123 - 123 mapping between d 123 and l 123 over their active domains .
matrix completion by spectral regularization
in this paper we propose an algorithm soft - impute for the nuclear norm regularized least - squares problem ( 123 ) that scales to large problems with m , n 123 with around 123 or more observed entries .
at every iteration soft - impute decreases the value of the objective function towards its minimum , and at the same time gets closer to the set of optimal solutions of the prob - lem ( 123 ) .
we study the convergence properties of this algorithm and discuss how it can be extended to other more sophisticated forms of spectral regularization .
to summarize some performance results123 we obtain a rank - 123 solution to ( 123 ) for a problem of size 123 123 and |w
| = 123 observed
entries in less than 123 minutes .
for the same sized matrix with |w for a 123 123 sized matrix with |w
| = 123 we obtain a rank - 123 solution in less than 123 minutes .
| = 123 a rank - 123 solution is obtained in approximately
we t a rank - 123 solution for the netix data in 123 hours .
here there are 123 observed entries in a matrix with 123 123 rows and 123 123 columns .
a rank 123 solution takes 123 hours .
the paper is organized as follows .
in section 123 , we discuss related work and provide some context for this paper .
in section 123 we introduce the soft - impute algorithm and study its convergence properties in section 123
the computational aspects of the algorithm are described in section 123 , and section 123 discusses how nuclear norm regularization can be generalized to more aggressive and general types of spectral regularization .
section 123 describes post - processing of selectors and initialization .
we discuss comparisons with related work , simulations and experimental studies in section 123 and application to the netix data in section 123
context and related work
cand`es and tao ( 123 ) , cai et al .
( 123 ) , and cand`es and recht ( 123 ) consider the criterion
zi j = xi j , ( i , j ) w
with d = 123 , the criterion ( 123 ) is equivalent to ( 123 ) , in that it requires the training error to be zero .
cai et al .
( 123 ) propose a rst - order singular - value - thresholding algorithm svt scalable to large matrices for the problem ( 123 ) .
they comment on the problem ( 123 ) with d > 123 , but dismiss it as being computationally prohibitive for large problems .
we believe that ( 123 ) will almost always be too rigid and will result in over - tting .
if minimization of prediction error is an important goal , then the optimal solution z will typically lie somewhere in the interior of the path indexed by d
( figures 123 , 123 and 123 ) .
in this paper we provide an algorithm soft - impute for computing solutions of ( 123 ) on a grid of l values , based on warm restarts .
the algorithm is inspired by svd - impute ( troyanskaya et al . ,
for large problems data transfer , access and reading take quite a lot of time and is dependent upon the platform and machine .
over here we report the times taken for the computational bottle - neck , that is , the svd computations over all iterations .
all times are reported based on computations done in a intel xeon linux 123ghz processor using matlab , with no c or fortran interlacing .
mazumder , hastie and tibshirani
123 ) an em - type ( dempster et al . , 123 ) iterative algorithm that alternates between imputing the missing values from a current svd , and updating the svd using the complete data matrix .
in its very motivation , soft - impute is different from generic rst order algorithms ( cai et al . , 123; ma et al . ; ji and ye , 123 ) .
the latter require the specication of a step size , and can be quite sensitive to the chosen value .
our algorithm does not require a step - size , or any such parameter .
the iterative algorithms proposed in ma et al .
and ji and ye ( 123 ) require the computation of a svd of a dense matrix ( with dimensions equal to the size of the matrix x ) at every iteration , as the bottleneck .
this makes the algorithms prohibitive for large scale computations .
ma et al .
use randomized algorithms for the svd computation .
our algorithm soft - impute also requires an svd computation at every iteration , but by exploiting the problem structure , can easily handle matrices of very large dimensions .
at each iteration the non - sparse matrix has the structure :
y = ysp ( sparse ) + ylr ( low rank ) .
in ( 123 ) ysp has the same sparsity structure as the observed x , and ylr has rank r m , n , where r is very close to r m , n the rank of the estimated matrix z ( upon convergence of the algorithm ) .
for large scale problems , we use iterative methods based on lanczos bidiagonalization with partial re - orthogonalization ( as in the propack algorithm , larsen , 123 ) , for computing the rst r sin - gular vectors / values of y .
due to the specic structure of ( 123 ) , multiplication by y and y can both be achieved in a cost - efcient way .
in decomposition ( 123 ) , the computationally burdensome work in computing a low - rank svd is of an order that depends linearly on the matrix dimensions .
more precisely , evaluating each singular vector requires computation of the order of o ( ( m+n ) r ) +o ( |w ops and evaluating r of them requires o ( ( m + n ) rr ) + o ( |w |r ) ops .
exploiting warm - starts , we observe that r rhence every svd step of our algorithm computes r singular vectors , with com - plexity of the order o ( ( m + n ) r123 ) + o ( |w |r ) ops .
this computation is performed for the number of iterations soft - impute requires to run till convergence or a certain tolerance .
in this paper we show asymptotic convergence of soft - impute and further derive its non - asymptotic rate of convergence which scales as o ( 123 / k ) ( k denotes the iteration number ) .
however , in our experimental studies on low - rank matrix completion , we have observed that our algorithm is faster ( based on timing comparisons ) than the accelerated version of nesterov ( ji and ye , 123; nesterov , 123 ) , having a provable ( worst case ) convergence rate of o ( 123 k123 ) .
with warm - starts soft - impute computes the entire regularization path very efciently along a dense series of values for
although the nuclear norm is motivated here as a convex relaxation to a rank constraint , we believe in many situations it will outperform the rank - restricted estimator ( 123 ) .
this is supported by our experimental studies .
we draw the natural analogy with model selection in linear regression , and compare best - subset regression ( 123 regularization ) with the lasso ( 123 regularization , tibshirani , 123; hastie et al . , 123 ) .
there too the 123 penalty can be viewed as a convex relaxation of the 123 penalty .
but in many situations with moderate sparsity , the lasso will outperform best subset in terms of prediction accuracy ( friedman , 123; hastie et al . , 123; mazumder et al . , 123 ) .
by shrinking the parameters in the model ( and hence reducing their variance ) , the lasso permits more parameters to be included .
the nuclear norm is the 123 penalty in matrix completion , as compared to the 123 rank .
by shrinking the singular values , we allow more dimensions to be included without incurring undue estimation variance .
another class of techniques used in collaborative ltering problems are close in spirit to ( 123 ) .
these are known as maximum margin matrix factorization methodsin short mmmfand use
matrix completion by spectral regularization
a factor model for the matrix z ( srebro et al . , 123b ) .
let z = uv where umr and vnr , and consider the following problem
( xi j ( uv ) i j ) 123 +
f + kv k123
it turns out that ( 123 ) is intimately related to ( 123 ) , since ( see lemma 123 )
||z|| = min
u , v : z=uv
f + kv k123
for example , if r = min ( m , n ) , the solution to ( 123 ) coincides with the solution to ( 123 ) . 123 however , ( 123 ) is not convex in its arguments , while ( 123 ) is .
we compare these two criteria in detail in section 123 , and the relative performance of their respective algorithms in section 123 .
soft - imputean algorithm for nuclear norm regularization
we rst introduce some notation that will be used for the rest of this article .
we adopt the notation of cai et al .
( 123 ) .
dene a matrix pw ( y ) ( with dimension m n )
pw ( y ) ( i , j ) = ( cid : 123 ) yi j
if ( i , j ) w if ( i , j ) / w
which is a projection of the matrix ymn onto the observed entries .
in the same spirit , dene the complementary projection pw ( y ) via pw ( y ) + pw ( y ) = y .
using ( 123 ) we can rewrite ( cid : 123 ) ( i , j ) w ( xi j zi j ) 123 as kpw ( x ) pw ( z ) k123
123 nuclear norm regularization
we present the following lemma , which forms a basic ingredient in our algorithm .
lemma 123 suppose the matrix wmn has rank r .
the solution to the optimization problem
f + l kzk
is given by z = sl ( w ) where
sl ( w ) udl v with dl = diag ( ( d123 l ) + , .
, ( dr l ) + ) ,
udv is the svd of w , d = diag ( d123 , .
, dr ) , and t+ = max ( t , 123 ) .
the notation sl ( w ) refers to soft - thresholding ( donoho et al . , 123 ) .
lemma 123 appears in cai et al .
( 123 ) and ma et al .
where the proof uses the sub - gradient characterization of the nuclear norm .
in appendix a . 123 we present an entirely different proof , which can be extended in a relatively straightforward way to other complicated forms of spectral regularization discussed in section 123
our proof is followed by a remark that covers these more general cases .
we note here that the original mmmf formulation uses r = min ( m , n ) .
in this paper we will consider it for a family
of r values .
mazumder , hastie and tibshirani
using the notation in 123 , we rewrite ( 123 ) as :
fl ( z ) : =
kpw ( x ) pw ( z ) k123
f + l kzk .
we now present algorithm 123soft - imputefor computing a series of solutions to ( 123 ) for
different values of l using warm starts .
algorithm 123 soft - impute
initialize zold = 123
do for l 123 > l 123 > .
> l k :
compute znew sl k ( pw ( x ) + pw ( zold ) ) .
if kznewzoldk123 iii .
assign zold znew .
< e exit .
( b ) assign zl k znew .
output the sequence of solutions zl 123 , .
, zl k .
the algorithm repeatedly replaces the missing entries with the current guess , and then updates the guess by solving ( 123 ) .
figures 123 , 123 and 123 show some examples of solutions using soft - impute ( blue continuous curves ) .
we see test and training error in the top rows as a function of the nuclear norm , obtained from a grid of values l .
these error curves show a smooth and very competitive
convergence analysis
in this section we study the convergence properties of algorithm 123
unlike generic rst - order methods ( nesterov , 123 ) including competitive rst - order methods for nuclear norm regularized problems ( cai et al . , 123; ma et al . ) , soft - impute does not involve the choice of any additional step - size .
most importantly our algorithm is readily scalable for solving large scale semidenite programming problems ( 123 ) and ( 123 ) as will be explained later in section 123
for an arbitrary matrix z , dene
ql ( z| z ) =
kpw ( x ) + pw ( z ) zk123
f + l kzk
as a surrogate of the objective function fl ( z ) .
note that fl ( z ) = ql ( z| z ) for any z .
in section 123 , we show that the sequence zk
l generated via soft - impute converges asymptot - ically , that is , as k to a minimizer of the objective function fl ( z ) .
soft - impute produces a sequence of solutions for which the criterion decreases to the optimal solution with every iteration and the successive iterates get closer to the optimal set of solutions of the problem 123
section 123
matrix completion by spectral regularization
derives the non - asymptotic convergence rate of the algorithm .
the latter analysis concentrates on the objective values fl ( zk l ) .
due to computational resources if one wishes to stop the algorithm after k iterations , then theorem 123 provides a certicate of how far zk is from the solution .
though section 123 alone establishes the convergence of fl ( zk l ) to the minimum of fl ( z ) , this does not , in general , settle the convergence of zk l unless further conditions ( like strong convexity ) are imposed on fl ( ) .
123 asymptotic convergence lemma 123 for every xed l 123 , dene a sequence zk
with any starting point z123
the sequence zk
) ql ( zk+123
l ) fl ( zk
proof note that
= sl ( pw ( x ) + pw ( zk
by lemma 123 and the denition ( 123 ) of ql ( z|zk
l ) , we have :
l ) = ql ( zk
kpw ( x ) + pw ( zk
l ) zk
f + l kzk
l ) zk123
fo + l kzk
= ql ( zk+123
123nkpw ( x ) + pw ( zk knpw ( x ) pw ( zk+123 123 nkpw ( x ) pw ( zk+123
kpw ( x ) pw ( zk+123
= ql ( zk+123 = f ( zk+123
) o +npw ( zk
f + kpw ( zk f + l kzk+123
l ) pw ( zk+123
l ) pw ( zk+123
f + l kzk+123
fo + l kzk+123
lemma 123 the nuclear norm shrinkage operator sl ( ) satises the following for any w123 , w123 ( with
ksl ( w123 ) sl ( w123 ) k123
f kw123 w123k123
in particular this implies that sl ( w ) is a continuous map in w .
mazumder , hastie and tibshirani
lemma 123 is proved in ma et al . ; their proof is complex and based on trace inequalities .
we give a concise proof based on elementary convex analysis in appendix a . 123
lemma 123 the successive differences kzk
l are monotone decreasing :
f of the sequence zk
moreover the difference sequence converges to zero .
that is l 123 as k
the proof of lemma 123 is given in appendix a . 123
lemma 123 every limit point of the sequence zk
l dened in lemma 123 is a stationary point of
kpw ( x ) pw ( z ) k123
f + l kzk .
hence it is a solution to the xed point equation
z = sl ( pw ( x ) + pw ( z ) ) .
the proof of lemma 123 is given in appendix a . 123
theorem 123 the sequence zk
l dened in lemma 123 converges to a limit z
kpw ( x ) pw ( z ) k123
f + l kzk .
proof it sufces to prove that zk
l converges; the theorem then follows from lemma 123
let zl be a limit point of the sequence zk
there exists a subsequence mk such that zmk
l zl .
by lemma 123 , zl solves the problem ( 123 ) and satises the xed point equation ( 123 ) .
k zl zk
f = ksl ( pw ( x ) + pw ( zl ) ) sl ( pw ( x ) + pw ( zk123
k ( pw ( x ) + pw ( zl ) ) ( pw ( x ) + pw ( zk123 = kpw ( zl zk123 k zl zk123
in ( 123 ) two substitutions were made; the left one using ( 123 ) in lemma 123 , the right one using ( 123 ) .
inequality ( 123 ) implies that the sequence k zl zk123 .
to show the conver - gence of the sequence zk l converges to zero .
we prove this by contradiction .
it sufces to prove that the sequence zl zk
f converges as k
suppose the sequence zk
l has another limit point z+l
123= zl .
then zl zk
l converges to zl
this contradicts the convergence of the sequence k zl zk123
points 123 and z+l zl the sequence zk the inequality in ( 123 ) implies that at every iteration zk l gets closer to an optimal solution for the problem ( 123 ) . 123 this property holds in addition to the decrease of the objective function ( lemma 123 ) at every iteration .
l has two distinct limit f
in fact this statement can be strengthened furtherat every iteration the distance of the estimate decreases from the
set of optimal solutions .
matrix completion by spectral regularization
123 convergence rate
in this section we derive the worst case convergence rate of soft - impute .
theorem 123 for every xed l 123 , the sequence zk non - asymptotic ( worst ) rate of convergence :
l ; k 123 dened in lemma 123 has the following
l ) fl ( z
the proof of this theorem is in appendix a . 123
in light of theorem 123 , a d > 123 accurate solution of fl ( z ) is obtained after a maximum of f iterations .
using warm - starts , soft - impute traces out the path of solutions on a grid
of l values l 123 > l 123 > .
> l k with a total of 123
d k zl
iterations .
here zl 123 = 123 and zl ( i ( 123 , .
, k 123 ) ) .
the solutions z dense grid of l be signicantly smaller than that obtained via arbitrary cold - starts .
i denotes the output of soft - impute ( upon convergence ) for l = l are likely to be close to each other , especially on a is .
hence every summand of ( 123 ) and the total number of iterations is expected to
computational complexity
the computationally demanding part of algorithm 123 is in sl ( pw ( x ) + pw ( zk l ) ) .
this requires cal - culating a low - rank svd of a matrix , since the underlying model assumption is that rank ( z ) min ( m , n ) .
in algorithm 123 , for xed l , the entire sequence of matrices zk l have explicit123 low - rank k corresponding to sl ( pw ( x ) + pw ( zk123 representations of the form ukdkv
in addition , observe that pw ( x ) + pw ( zk
l ) can be rewritten as
pw ( x ) + pw ( zk
l ) = ( cid : 123 ) pw ( x ) pw ( zk
l ) ( cid : 123 ) +
+ low rank .
in the numerical linear algebra literature , there are very efcient direct matrix factorization methods for calculating the svd of matrices of moderate size ( at most a few thousand ) .
when the matrix is sparse , larger problems can be solved but the computational cost depends heavily upon the sparsity structure of the matrix .
in general however , for large matrices one has to resort to indirect iterative methods for calculating the leading singular vectors / values of a matrix .
there is a lot research in numerical linear algebra for developing sophisticated algorithms for this purpose .
in this paper we will use the propack algorithm ( larsen , 123 , 123 ) because of its low storage requirements , effective op count and its well documented matlab version .
the algorithm for calculating the truncated svd for a matrix w ( say ) , becomes efcient if multiplication operations w b123 and w b123 123
we assume the solution zl at every l ( l 123 , .
, l k ) is computed to an accuracy of d > 123
though we cannot prove theoretically that every iterate of the sequence zkl will be of low - rank; this observation is rather practical based on the manner in which we trace out the entire path of solutions based on warm - starts .
our simulation results support this observation as well .
n , b123 m ) can be done with minimal cost .
mazumder , hastie and tibshirani
algorithm soft - impute requires repeated computation of a truncated svd for a matrix w l has rank r .
note that in |r ) ops using only the required outer products ( i . e . ,
with structure as in ( 123 ) .
assume that at the current iterate , the matrix zk ( 123 ) the term pw ( zk our algorithm does not compute the matrix explicitly ) .
l ) can be computed in o ( |w
|r ) to create the matrix pw ( zk
the cost of computing the truncated svd will depend upon the cost in the operations w b123 and | ) .
although it costs l ) , this is used for each of the r such multiplications ( which also |r ) ) , so we need not include that cost here .
the low rank part costs o ( ( m + n ) r ) for the | ) + o ( ( m + n ) r ) per vector multiplication .
supposing |r ) + o ( ( m + n ) ( r ) 123 ) is r , then in
w b123 ( which are equal ) .
for the sparse part these multiplications cost o ( |w multiplication by b123
hence the cost is o ( |w we want a r rank svd of the matrix ( 123 ) , the cost will be of the order of o ( |w ( for that iteration , that is , to obtain zk+123 light of our above observations r r min ( m , n ) and the order is o ( |w
suppose the rank of the solution zk
|r ) + o ( ( m + n ) r123 ) .
| nr poly ( log n ) .
in practice often |w
for the reconstruction problem to be theoretically meaningful in the sense of cand`es and tao ( 123 ) we require that |w | is very small .
hence introducing the low rank part does not add any further complexity in the multiplication by w and w .
so the dominant cost in calculating the truncated svd in our algorithm is o ( |w | ) .
the svt algorithm ( cai et al . , 123 ) for exact matrix completion ( 123 ) involves calculating the svd of a sparse matrix with cost o ( |w | ) .
this implies that the computational order of soft - impute and that of svt is the same .
this order computation does not include the number of iterations required for convergence .
in our experimental studies we use warm - starts for efciently computing the entire regularization path .
on small scale examples , based on comparisons with the accelerated gradient method of nesterov ( see section 123; ji and ye , 123; nesterov , 123 ) we nd that our algorithm converges faster than the latter in terms of run - time and number of svd computations / iterations .
this supports the computational effectiveness of soft - impute .
in addition , since the true rank of the matrix r min ( m , n ) , the computational cost of evaluating the truncated svd ( with rank r ) is linear in matrix dimensions .
this justies the large - scale computational feasibility of our algorithm .
the above discussions focus on the computational complexity for obtaining a low - rank svd , which is to be performed at every iteration of soft - impute .
similar to the total iteration complexity bound of soft - impute ( 123 ) , the total cost to compute the regularization path on a grid of l values is given by :
i + ( m + n ) r123
d k zl
here rl denotes the rank123 ( on an average ) of the iterates zk
l generated by soft - impute for xed l the propack package does not allow one to request ( and hence compute ) only the singular values larger than a threshold l one has to specify the number in advance .
so once all the com - puted singular values fall above the current threshold l , our algorithm increases the number to be computed until the smallest is smaller than l .
in large scale problems , we put an absolute limit on the maximum number .
we assume , above that the grid of values l 123 > .
. l k is such that all the solutions zl , l ( l 123 , .
, l k ) are of small
rank , as they appear in section 123
matrix completion by spectral regularization
generalized spectral regularization : from soft to hard thresholding
in section 123 we discussed the role of the nuclear norm as a convex surrogate for the rank of a matrix , and drew the analogy with lasso regression versus best - subset selection .
we argued that in many problems 123 regularization gives better prediction accuracy .
however , if the underlying model is very sparse , then the lasso with its uniform shrinkage can both overestimate the number of non - zero coefcients ( friedman , 123 ) in the model , and overly shrink ( bias ) those included toward zero .
in this section we propose a natural generalization of soft - impute to overcome these problems .
consider again the problem
kpw ( x ) pw ( z ) k123
a rephrasing of ( 123 ) .
this best rank - k solution also solves f + l
kpw ( x ) pw ( z ) k123
j ( z ) > 123 ) ,
j ( z ) is the jth singular value of z , and for a suitable choice of l
with rank k .
that produces a solution
the fully observed matrix version of the above problem is given by the 123 version of ( 123 ) as
f + l kzk123 ,
where kzk123 = rank ( z ) .
the solution of ( 123 ) is given by a reduced - rank svd of w ; for every l there is a corresponding q = q ( l ) number of singular - values to be retained in the svd decompo - sition .
problem ( 123 ) is non - convex in w but its global minimizer can be evaluated .
as in ( 123 ) the thresholding operator resulting from ( 123 ) is
shl ( w ) = udqv where dq = diag ( d123 , .
, dq , 123 , .
, 123 ) .
similar to soft - impute ( algorithm 123 ) , we present below hard - impute ( algorithm 123 ) for the 123 penalty .
the continuous parameterization via l does not appear to offer obvious advantages over rank - truncation methods .
we note that it does allow for a continuum of warm starts , and is a natural post - processor for the output of soft - impute ( next section ) .
but it also allows for further generalizations that bridge the gap between hard and soft regularization methods .
in penalized regression there have been recent developments directed towards bridging the gap between the 123 and 123 penalties ( friedman , 123; zhang , 123; mazumder et al . , 123 ) .
this is done via using non - convex penalties that are a better surrogate ( in the sense of approximating the penalty ) to 123 over the 123
they also produce less biased estimates than those produced by the 123 penalized solutions .
when the underlying model is very sparse they often perform very well , and enjoy superior prediction accuracy when compared to softer penalties like 123
these methods still shrink , but are less aggressive than the best - subset selection .
by analogy , we propose using a more sophisticated version of spectral regularization .
this goes beyond nuclear norm regularization by using slightly more aggressive penalties that bridge the gap between 123 ( nuclear norm ) and 123 ( rank constraint ) .
we propose minimizing
fp , l ( z ) =
kpw ( x ) pw ( z ) k123
mazumder , hastie and tibshirani
algorithm 123 hard - impute
initialize zl k k = 123 , .
, k ( for example , using soft - impute; see section 123 ) .
do for l 123 > l 123 > .
> l k :
( pw ( x ) + pw ( zold ) ) .
compute znew shl k < e exit .
if kznewzoldk123 iii .
assign zold znew .
( b ) assign zh , l k znew .
output the sequence of solutions zh , l 123 , .
, zh , l k .
where p ( |t|; ) is concave in |t| .
the parameter ( inf , sup ) controls the degree of concavity .
we may think of p ( |t|; inf ) = |t| ( 123 penalty ) on one end and p ( |t|; sup ) = ktk123 ( 123 penalty ) on the other .
in particular for the 123 penalty denote fp , l ( z ) by fh , l ( z ) for hard thresholding .
see friedman ( 123 ) , mazumder et al .
( 123 ) and zhang ( 123 ) for examples of such penalties .
in remark 123 in appendix a . 123 we argue how the proof can be modied for general types of spectral regularization .
hence for minimizing the objective ( 123 ) we will look at the analogous version of ( 123 , 123 ) which is
the solution is given by a thresholded svd of w ,
l ( w ) = udp , l v ,
is a entry - wise thresholding of the diagonal entries of the matrix d consisting of singular values of the matrix w .
the exact form of the thresholding depends upon the form of the penalty function p ( ; ) , as discussed in remark 123
algorithm 123 and algorithm 123 can be modied for the penalty p ( ; ) by using a more general thresholding function sp l ( ) in step 123 ( a ) i .
the corresponding
l ( pw ( x ) + pw ( zold ) ) .
however these types of spectral regularization make the criterion ( 123 ) non - convex and hence it becomes difcult to optimize globally .
recht et al .
( 123 ) and bach ( 123 ) also consider the rank estimation problem from a theoretical standpoint .
post - processing of selectors and initialization
because the 123 norm regularizes by shrinking the singular values , the number of singular values retained ( through cross - validation , say ) may exceed the actual rank of the matrix .
in such cases it is reasonable to undo the shrinkage of the chosen models , which might permit a lower - rank solution .
matrix completion by spectral regularization
is the solution to ( 123 ) , then its post - processed version zu
eigen - values of the matrix zl
is obtained by
l obtained by unshrinking the
l = uda v , where da = diag ( a 123 , .
, a is the rank of zl and zl = udl v is its svd .
the estimation in ( 123 ) can be done via ordinary least squares , which is feasible because of the sparsity of pw ( uiv is small . 123 if the least squares solutions a do not meet the positivity constraints , then the and that rl negative sign can be absorbed into the corresponding singular vector .
here rl
rather than estimating a diagonal matrix da as above , one can insert a matrix mrl rl between u and v above to obtain better training error for the same rank .
hence given u , v ( each of rank rl ) from the soft - impute algorithm , we solve
m = argmin
zl = u mv .
kpw ( x ) pw ( umv ) k123 ,
the objective function in ( 123 ) is the frobenius norm of an afne function of m and hence can be optimized very efciently .
scalability issues pertaining to the optimization problem ( 123 ) can be handled fairly efciently via conjugate gradients .
criterion ( 123 ) will denitely lead to a decrease in training error as that attained by z = udl v for the same rank and is potentially an attractive proposal for the original problem ( 123 ) .
however this heuristic cannot be caste as a ( jointly ) convex problem in ( u , m , v ) .
in addition , this requires the estimation of up to r123 l parameters , and has the potential for over - tting .
in this paper we report experiments based on ( 123 ) .
in many simulated examples we have observed that this post - processing step gives a good es - timate of the underlying true rank of the matrix ( based on prediction error ) .
since xed points of algorithm 123 correspond to local minima of the function ( 123 ) , well - chosen warm starts zl are help - ful .
a reasonable prescription for warms - starts is the nuclear norm solution via ( soft - impute ) , or the post processed version ( 123 ) .
the latter appears to signicantly speed up convergence for hard - impute .
this observation is based on our simulation studies .
soft - impute and maximum - margin matrix factorization
in this section we compare in detail the mmmf criterion ( 123 ) with the soft - impute criterion ( 123 ) .
for ease of comparison here , we put down these criteria again using our pw notation .
||pw ( x uv ) ||123
f + kv k123
where umr and vnr are arbitrary ( non - orthogonal ) matrices .
this problem formulation and re - lated optimization methods have been explored by srebro et al .
( 123b ) and rennie and srebro
observe that the pw ( uiv
i ) , i = 123 , .
, rl are not orthogonal , though the uiv
mazumder , hastie and tibshirani
||pw ( x z ) ||123
f + l kzk .
for each given maximum rank , mmmf produces an estimate by doing further shrinkage with its quadratic regularization .
soft - impute performs rank reduction and shrinkage at the same time , in one smooth convex operation .
the following theorem shows that this one - dimensional soft - impute family lies exactly in the two - dimensional mmmf family .
theorem 123 let x be m n with observed entries indexed by w
let r = min ( m , n ) .
then the solutions to ( 123 ) and ( 123 ) coincide for all l 123
suppose z is a solution to ( 123 ) for l > 123 , and let r be its rank .
then for any solution u , v to ( 123 ) with r = r and l = l , u v t is a solution to ( 123 ) .
the svd factorization of z provides one such solution to ( 123 ) .
this implies that the solution space of ( 123 ) is contained in that of ( 123 ) .
part 123 of this theorem appears in a slightly different form in srebro et al .
( 123b ) .
in part 123 , we could use r > min ( m , n ) and get the same equivalence .
while this might seem unnecessary , there may be computational advantages; searching over a bigger space might protect against local minima .
likewise in part 123 , we could use r > r and achieve the same equivalence .
in either case , no matter what r we use , the solution matrices u and v have the same rank as z .
let z ( l ) be a solution to ( 123 ) at l
we conjecture that rank ( z ( l ) ) is monotone non - increasing .
if this is the case , then theorem 123 , part 123 can be further strengthened to say that for all
l l and r = r the solutions of ( 123 ) coincide with that of ( 123 ) .
the mmmf criterion ( 123 ) denes a two - dimensional family of models indexed by ( r , l ) , while the soft - impute criterion ( 123 ) denes a one - dimensional family .
in light of theorem 123 , this family is a special path in the two - dimensional grid of solutions ( u ( r , l ) , v ( r , l ) ) .
figure 123 depicts the situation .
any mmmf model at parameter combinations above the red squares are redundant , since their t is the same at the red square .
however , in practice the red squares are not known to mmmf , nor is the actual rank of the solution .
further orthogonalization of u and v would be required to reveal the rank , which would only be approximate ( depending on the convergence criterion of the mmmf
despite the equivalence of ( 123 ) and ( 123 ) when r = min ( m , n ) , the criteria are quite different .
while ( 123 ) is a convex optimization problem in z , ( 123 ) is a non - convex problem in the variables u , v and has possibly several local minima; see also abernethy et al .
( 123 ) .
it has been observed empirically and theoretically ( burer and monteiro , 123; rennie and srebro , 123 ) that bi - convex methods used in the optimization of ( 123 ) can get stuck in sub - optimal local minima for a small value of r or a poorly chosen starting point .
for a large number of factors r and large dimensions m , n the computational cost may be quite high ( see also experimental studies in section 123 ) .
criterion ( 123 ) is convex in z for every value of l
, and it outputs the solution z in the form of its soft - thresholded svd , implying that the factors u , v are already orthogonal and the rank is
matrix completion by spectral regularization
figure 123 : comparison of the parameter space for mmmf ( grey and black points ) , and soft - impute ( red squares ) for a simple example .
since all mmmf solutions with parameters above the red squares are identical to the soft - impute solutions at the red squares , all the grey points are redundant .
mmmf has two different tuning parameters r and l
or spectral properties of the matrices u , v .
soft - impute has only one tuning parameter l presence of two tuning parameters is problematic :
, both of which are related to the rank
it results in a signicant increase in computational burden , since for every given value of r , ( see section 123 for illustra -
one needs to compute an entire system of solutions by varying l
in practice when neither the optimal values of r and l are known , a two - dimensional search
( for example , by cross validation ) is required to select suitable values .
further discussions and connections between the tuning parameters and spectral properties of the matrices can be found in burer and monteiro ( 123 ) and abernethy et al .
( 123 ) .
the proof of theorem 123 requires a lemma .
lemma 123 for any matrix z , the following holds :
f + kv k123
||z|| = min
u , v : z=uv t
if rank ( z ) = k min ( m , n ) , then the minimum above is attained at a factor decomposition z = note that in the decomposition z = uv t in ( 123 ) there is no constraint on the number of columns r of the factor matrices umr and vnr .
lemma 123 is stronger than similar results appearing in rennie and srebro ( 123 ) and abernethy et al .
( 123 ) which establish ( 123 ) for r = min ( m , n ) we give a tighter estimate of the rank k of the underlying matrices .
the proof is given in appendix a . 123
mazumder , hastie and tibshirani
123 proof of theorem 123 part 123
for r = min ( m , n ) , any matrix zmn can be written in the form of z = uv t .
the criterion ( 123 ) can be written as
123 ||pw ( x uv t ) ||123
123 ||pw ( x uv t ) ||123 123 ||pw ( x z ) ||123
f + kv k123
f + l kuv t k f + l kzk .
( by lemma 123 )
the equivalence of the criteria in ( 123 ) and ( 123 ) completes the proof of part 123
part 123
note that if we know that the solution z to ( 123 ) with l = l has rank r , then z also solves
123 ||pw ( x z ) ||123
f +l kzk .
we now repeat the steps ( 123 ) ( 123 ) , restricting the rank r of u and v to be r = r , and the result
numerical experiments and comparisons
in this section we study the performance of soft - impute , its post - processed variants , and hard - impute for noisy matrix completion problems .
the examples assert our claim that the matrix reconstruction criterion ( 123 ) ( cai et al . , 123 ) is too rigid if one seeks good predictive models .
we include the related procedures of rennie and srebro ( 123 ) and keshavan et al .
( 123 ) in our com -
the reconstruction algorithm optspace , described in keshavan et al .
( 123 ) considers crite - rion ( 123 ) ( in the presence of noise ) .
it uses the representation z = usv ( which need not correspond to the svd ) .
optspace alternates between estimating s and u , v ( in a grassmann manifold ) for computing a rank - r decomposition z = u s v .
it starts with a sparse svd on a clean version of the observed matrix pw ( x ) .
this is similar to the formulation of mmmf ( 123 ) as detailed in section 123 , without the squared frobenius norm regularization on the components u , v .
to summarize , we study the following methods :
soft - imputealgorithm 123;
soft - impute+post - processing on the output of soft - impute , as in section 123;
hard - imputealgorithm 123 , starting with the output of soft - impute+;
svtalgorithm by cai et al .
( 123 ) ;
optspacereconstruction algorithm by keshavan et al .
( 123 ) ;
mmmfalgorithm for ( 123 ) as in rennie and srebro ( 123 ) .
matrix completion by spectral regularization
rn +e , where u and v are in all our simulation studies we use the underlying model zmn = umrv random matrices with standard normal gaussian entries , and e random over the indices of the matrix with p% percent of missing entries .
these are the models under which the coherence conditions hold true for the matrix completion problem to be meaningful ( cand`es and tao , 123; keshavan et al . , 123 ) .
the signal to noise ratio for the model and the test - error ( standardized ) are dened as
is i . i . d .
gaussian .
snr =svar ( uv )
; test error =
kpw ( uv z ) k123
kpw ( uv ) k123
training error ( standardized ) is dened as
training error =
kpw ( z z ) k123
the fraction of the error explained on the observed entries by the estimate relative to a zero estimate .
figures 123 , 123 and 123 show training and test error for all of the algorithms mentioned aboveboth as a function of nuclear norm and rankfor the three problem instances .
the results displayed in the gures are averaged over 123 simulations , and also show one - standard - error bands ( hardly visible ) .
in all examples ( m , n ) = ( 123 , 123 ) .
for mmmf we use r = min ( m , n ) = 123 , the number of columns in u and v .
the performance of mmmf is displayed only in the plots with the nuclear norm along the horizontal axis , since the algorithm does not deliver a precise rank .
snr , true rank and percentage of missing entries are indicated in the gures .
there is a unique correspondence between l and nuclear norm .
the plots versus rank indicate how effective the nuclear norm is as a rank approximationthat is whether it recovers the true rank while minimizing prediction error .
for routines not our own we use the matlab code as supplied on webpages by the authors .
for svt second author of cai et al .
( 123 ) , for optspace third author of keshavan et al .
( 123 ) , and for mmmf rst author of rennie and srebro ( 123 ) .
the captions of each of figures 123 detail the results , which we summarize here .
for the rst two gures , the noise is quite high with snr= 123 , and 123% of the entries are missing .
in figure 123 the true rank is 123 , while in figure 123 it is 123
soft - impute , mmmf and soft - impute+ have the best prediction performance , while soft - impute+ is better at estimating the correct rank .
the other procedures perform poorly here , although optspace improves somewhat in figure 123
svt has very poor prediction error , suggesting once again that exactly tting the training data is far too rigid .
soft - impute+ has the best performance in figure 123 ( smaller rankmore aggressive tting ) , and hard - impute starts recovering here .
in both gures the training error for soft - impute ( and hence mmmf ) wins as a function of nuclear norm ( as it must , by construction ) , but the more aggressive tters soft - impute+ and hard - impute have better training error as a function of
though the nuclear norm is often viewed as a surrogate for the rank of a matrix , we see in these examples that it can provide a superior mechanism for regularization .
this is similar to the performance of lasso in the context of regression .
although the lasso penalty can be viewed as a convex surrogate for the 123 penalty in model selection , its 123 penalty provides a smoother and often better basis for regularization .
mazumder , hastie and tibshirani
123% missing entries with snr=123 , true rank =123
figure 123 : softimp+ refers to the post - processing after soft - impute; hard - impute uses soft - imp+ as starting values .
both soft - impute and soft - impute+ perform well ( predic - tion error ) in the presence of noise; the latter estimates the actual rank of the matrix .
mmmf ( with full rank 123 factor matrices ) has performance similar to soft - impute .
hard - impute and optspace show poor prediction error .
svt also has poor predic - tion error , conrming our claim in this example that criterion ( 123 ) can result in overtting; it recovers a matrix with high nuclear norm and rank > 123 where the true rank is only 123
values of test error larger than one are not shown in the gure .
optspace is evaluated for a series of ranks 123
matrix completion by spectral regularization
123% missing entries with snr=123 , true rank =123
figure 123 : soft - impute+ has the best prediction error , closely followed by soft - impute and mmmf .
both hard - impute and optspace have poor prediction error apart from near the true rank 123 of the matrix , where they show reasonable performance .
svt has very poor prediction error; it recovers a matrix with high nuclear norm and rank > 123 , where the true rank is only 123
optspace is evaluated for a series of ranks 123
mazumder , hastie and tibshirani
123% missing entries with snr=123 , true rank =123
figure 123 : with low noise the performance of hard - impute improves .
it gets the correct rank whereas optspace slightly overestimates the rank .
hard - impute has the best pre - diction error , followed by optspace .
here mmmf has slightly better prediction error than soft - impute .
although the noise is low here , svt recovers a matrix with high rank ( approximately 123 ) and has poor prediction error as well .
the test error of svt is found to be different from the limiting solution of soft - impute; although in theory the limiting solution of ( 123 ) should coincide with that of svt , in practice we never go to the
matrix completion by spectral regularization
in figure 123 with snr= 123 the noise is relatively small compared to the other two cases .
the true underlying rank is 123 , but the proportion of missing entries is much higher at eighty percent .
test errors of both soft - impute+ and soft - impute are found to decrease till a large nuclear norm after which they become roughly the same , suggesting no further impact of regularization .
mmmf has slightly better test error than soft - impute around a nuclear norm of 123 , while in theory they should be identical .
notice , however , that the training error is slightly worse ( everywhere ) , suggesting that mmmf is sometimes trapped in local minima .
the fact that this slightly undert solution does better in test error is a quirk of this particular example .
optspace performs well in this high - snr example , achieving a sharp minima at the true rank of the matrix .
hard - impute performs the best in this example .
the better performance of both optspace and hard - impute over soft - impute can be attributed both to the low - rank truth and the high snr .
this is reminis - cent of the better predictive performance of best - subset or concave penalized regression often seen over lasso in setups where the underlying model is very sparse ( friedman , 123 ) .
123 comparison with fast mmmf ( rennie and srebro , 123 )
in this section we compare soft - impute with mmmf in terms of computational efciency .
we also examine the consequences of two regularization parameters ( r , l ) for mmmf over one for
rennie and srebro ( 123 ) describes a fast algorithm based on conjugate - gradient descent for minimization of the mmmf criterion ( 123 ) .
with ( 123 ) being non - convex , it is hard to provide theo - retical optimality guarantees for the algorithm for arbitrary r , l that is , what type of solution it converges to or how far it is from the global minimizer .
in table 123 we summarize the performance results of the two algorithms .
for both soft - impute and mmmf we consider a equi - spaced grid of 123 l ( l min , l max ) , with l min corresponding to a full - rank solution of soft - impute and l max the zero solution .
for mmmf , three different values of r were used , and for each ( u , v ) were solved for over the grid of l values .
a separate held - out validation set with twenty percent of the missing entries sampled from w were used to train the tuning parameter l ( for each value of r ) for mmmf and soft - impute .
finally we evaluate the standardized prediction errors on a test set consisting of the remaining eighty percent of the missing entries in w .
in all cases we report the training errors and test errors on the optimally tuned .
soft - impute was run till a tolerance of 123 was achieved ( fraction of decrease of objective value ) .
likewise for mmmf we set the tolerance of the conjugate gradient method to 123
in table 123 , for every algorithm total time indicates the time required for evaluating solutions over the entire grid of l values .
in these examples , we used direct svd factorization based methods for the svd computation , since the size of the problems were quite small .
in all these examples we observe that soft - impute performs very favorably in terms of total times .
for mmmf the time to train the models increase with increasing rank r; and in case the underlying matrix has rank which is larger than r , the computational cost will be large in order to get competitive predictive accuracy .
this point is supported in the examples of table 123
it is important to note that , the prediction error of soft - impute as obtained on the validation set is actually within standard error of the best prediction error produced by all the mmmf models .
in addition we also performed some medium - scale examples increasing the dimensions of the matrices .
to make comparisons fair , soft - impute made use of direct svd computations ( in matlab ) instead of iterative algorithms
mazumder , hastie and tibshirani
| = 123 123 ( 123% ) mmmf ( 123 )
| = 123 123 ( 123% ) mmmf ( 123 )
( m , n ) = ( 123 , 123 ) rank ( r ) = 123 ( m , n ) = ( 123 , 123 ) ( m , n ) = ( 123 , 123 )
| = 123 123 ( 123% ) mmmf ( 123 )
training error time ( secs )
table 123 : performances of soft - impute and mmmf for different problem instances , in terms of test error ( with standard errors in parentheses ) , training error and times for learning the models .
soft - impute , rank denotes the rank of the recovered matrix , at the optimally chosen value of l .
for the mmmf , rank indicates the value of r in umr , vnr .
results are averaged over 123 simulations .
exploiting the specialized sparse+low - rank structure ( 123 ) .
we report our ndings on one such
for ( m , n ) = ( 123 , 123 ) , |w
| / ( m n ) = 123 , rank = 123 and snr=123; soft - impute takes 123 hours to compute solutions on a grid of 123 l values .
the test error on the validation set and training error are 123 and 123 with the recovered solution having a rank of 123
for the same problem , mmmf with r = 123 takes 123 hours returning a solution with test - error 123 and training error 123 .
with r = 123 it takes 123 hrs with test and training errors 123 and 123 respectively .
we will like to note that decoste ( 123 ) proposed an efcient implementation of mmmf via an ensemble based approach , which is quite different in spirit from the batch optimization algorithms we are studying in this paper .
hence we do not compare it with soft - impute .
123 comparison with nesterovs accelerated gradient method
ji and ye ( 123 ) proposed a rst - order algorithm based on nesterovs acceleration scheme ( nes - terov , 123 ) , for nuclear norm minimization for a generic multi - task learning problem ( argyriou et al . , 123 , 123 ) .
their algorithm ( liu et al . , 123; ji and ye , 123 ) can be adapted to the soft - impute problem ( 123 ) ; hereafter we refer to it as nesterov .
it requires one to compute the svd of a dense matrix having the dimensions of x , which makes it prohibitive for large - scale problems .
we instead would make use of the structure ( 123 ) for the svd computation , a special characteristic of matrix completion which is not present in a generic multi - task learning problem .
here we compare the performances of soft - impute and nesterov on small scale examples , where direct svds can be computed easily .
matrix completion by spectral regularization
since both algorithms solve the same criterion , the quality of the solutionsobjective values , training and test errorswill be the same ( within tolerance ) .
we hence compare their performances based on the times taken by the algorithms to converge to the optimal solution of ( 123 ) on a grid of values of l .
both algorithms compute a path of solutions using warm starts .
results are shown in figure 123 , for four different scenarios described in table 123
| / ( m n ) rank test error
table 123 : four different examples used for timing comparisons of soft - impute and nesterov
( accelerated nesterov algorithm of ji and ye 123 ) .
in all cases the snr= 123
figure 123 shows the time to convergence for the two algorithms .
their respective number of iterations are not comparable .
this is because nesterov uses a line - search to compute an adaptive step - size ( approximate the lipschitz constant ) at every iteration , whereas soft - impute does not .
soft - impute has a rate of convergence given by theorem 123 , which for large k is worse than the accelerated version nesterov with rate o ( 123 / k123 ) .
however , timing comparisons in figure 123 show that soft - impute performs very favorably .
we do not know the exact reason behind this , but mention some possibilities .
firstly the rates are worst case convergence rates .
on particular problem instances of the form ( 123 ) , the rates of convergence in practice of soft - impute and nesterov may be quite similar .
since ji and ye ( 123 ) uses an adaptive step - size strategy , the choice of a step - size may be time consuming .
soft - impute on the other hand , uses a constant step size .
additionally , it appears that the use of the momentum term in nesterov affects the sparse+low - rank decomposition ( 123 ) .
this may prevent the algorithm to be adapted for solving large problems , due to costly svd computations .
123 large scale simulations for soft - impute
table 123 reports the performance of soft - impute on some large - scale problems .
all computations are performed in matlab and the matlab implementation of propack is used .
data input , access and transfer in matlab take a sizable portion of the total computational time , given the size of these problems .
however , the main computational bottle neck in our algorithm is the struc - tured svd computation .
in order to focus more on the essential computational task , table 123 displays the total time required to perform the svd computations over all iterations of the algorithm .
note that for all the examples considered in table 123 , the implementations of algorithms nesterov ( liu et al . , 123; ji and ye , 123 ) and mmmf ( rennie and srebro , 123 ) are prohibitively expensive both in terms of computational time and memory requirements , and hence could not be run .
we used the value l = ||pw ( x ) ||123 / k with soft - impute , with k = 123 for all examples but the last , where k = 123
l 123 = ||pw ( x ) ||123 is the largest singular value of the input matrix x ( padded with ze - ros ) ; this is the smallest value of l for which sl 123 ( pw ( x ) ) = 123 in the rst iteration of soft - impute
mazumder , hastie and tibshirani
log ( k zl k / c + 123 )
log ( k zl k / c + 123 )
figure 123 : timing comparisons of soft - impute and nesterov ( accelerated nesterov algorithm of ji and ye 123 ) .
the horizontal axis corresponds to the standardized nuclear norm , with c = maxl k zl k .
shown are the times till convergence for the two algorithms over an entire grid of l values for examples iiv ( in the last the matrix dimensions are much larger ) .
the overall time differences between examples iiii and example iv is due to the increased cost of the svd computations .
results are averaged over 123 simulations .
the times for nesterov change far more erratically with l than they do for soft - impute .
the prediction performance is awful for all but one of the models , because in most cases the fraction of observed data is very small .
these simulations were mainly to show the computational capabilities of soft - impute on very large problems .
application to the netix data set
the netix training data consists of the ratings of 123 , 123 movies by 123 , 123 netix customers .
the resulting data matrix is extremely sparse , with 123 , 123 , 123 or 123% of the entries observed .
the task was to predict the unseen ratings for a qualifying set and a test set of about 123 million ratings each , with the true ratings in these data sets held in secret by netix .
a probe set of about 123 million
matrix completion by spectral regularization
| / ( m n ) recovered time ( mins ) test error training error
table 123 : performance of soft - impute on different problem instances .
all models are generated with snr=123 and underlying rank=123
recovered rank is the rank of the solution matrix z at the value of l used in ( 123 ) .
those with stars reached the maximum rank threshold , and option in our algorithm .
convergence criterion is taken as fraction of improvement of objective value less than 123 or a maximum of 123 iterations for the last four examples .
all implementations are done in matlab including the matlab implementation of propack on a intel xeon linux 123ghz processor .
ratings was distributed to participants , for calibration purposes .
the movies and customers in the qualifying , test and probe sets are all subsets of those in the training set .
the ratings are integers from 123 ( poor ) to 123 ( best ) .
netixs own algorithm has an rmse of 123 , and the contest goal was to improve this by 123% , or an rmse of 123 or better .
the contest ran for about 123 years , and the winning team was bellkors pragmatic chaos , a merger of three earlier teams ( see http : / / www . netflixprize . com / for details ) .
they claimed the grand prize of $123m on september 123 , 123
many of the competitive algorithms build on a regularized low - rank factor model similar to ( 123 ) using randomization schemes like mini - batch , stochastic gradient descent or sub - sampling to reduce the computational cost over making several passes over the entire data - set ( see salakhutdinov et al . , 123; bell and koren . , 123; takacs et al . , 123 , for example ) .
in this paper , our focus is not on using randomized or sub - sampling schemes .
here we demonstrate that our nuclear - norm regular - ization algorithm can be applied in batch mode on the entire netix training set with a reasonable computation time .
we note however that the conditions under which the nuclear - norm regulariza - tion is theoretically meaningful ( cand`es and tao , 123; srebro et al . , 123a ) are not met on the netix data set .
we applied soft - impute to the training data matrix and then performed a least - squares un - shrinking on the singular values with the singular vectors and the training data row and column means as the bases .
the latter was performed on a data - set of size 123 randomly drawn from the probe set .
the prediction error ( rmse ) is obtained on a left out portion of the probe set .
table 123 reports the performance of the procedure for different choices of the tuning parameter l corresponding rank ) ; times indicate the total time taken for the svd computations over all itera - tions .
a maximum of 123 iterations were performed for each of the examples .
again , these results are not competitive with those of the competition leaders , but rather demonstrate the feasibility of applying soft - impute to such a large data set .
mazumder , hastie and tibshirani
rank time ( hrs ) rmse
table 123 : results of applying soft - impute to the netix data .
l 123 = ||pw ( x ) ||123; see section 123 .
the computations were done on a intel xeon linux 123ghz processor; timings are reported based on matlab implementations of propack and our algorithm .
rmse is root - mean squared error , as dened in the text .
we thank the reviewers for their suggestions that lead to improvements in this paper .
we thank stephen boyd , emmanuel candes , andrea montanari , and nathan srebro for helpful discussions .
trevor hastie was partially supported by grant dms - 123 from the national science founda - tion , and grant 123r123 ca 123 - 123 from the national institutes of health .
robert tibshirani was partially supported from national science foundation grant dms - 123 and national institutes of health contract n123 - hv - 123
appendix a .
proofs
we begin with the proof of lemma 123
a . 123 proof of lemma 123 proof let z = umn dnn v explicitly evaluate the closed form solution of the problem ( 123 ) .
note that
nn be the svd of z .
assume without loss of generality , m n .
we will
kz w k123
f + l kzk =
iw vi +
i ) + l
minimizing ( 123 ) is equivalent to minimizing
d = diag ( cid : 123 ) d123 , .
, dn ( cid : 123 ) ,
iw vi +
u = ( u123 , .
, un ) ,
v = ( v123 , .
, vn ) .
di; w . r . t .
( ui , vi , di ) , i = 123 ,
under the constraints u u = in , v v = in and di 123 i .
observe the above is equivalent to minimizing ( w . r . t .
u , v ) the function q ( u , v ) :
q ( u , v ) = min
iw vi +
i ) + l
matrix completion by spectral regularization
since the objective ( 123 ) to be minimized w . r . t .
minimize it w . r . t .
each di separately .
d , is separable in di , i = 123 , .
, n; it sufces to
iw vi + d123
123 ( cid : 123 ) 123 di u
i ( cid : 123 ) + l
can be solved looking at the stationary conditions of the function using its sub - gradient ( nes - iw vi l ) + , the soft - iw vi ) = ( u terov , 123 ) .
the solution of the above problem is given by sl ( u iw vi ( without loss of generality , we can take u thresholding of u iw vi to be non - negative ) .
more generally the soft - thresholding operator ( friedman et al . , 123; hastie et al . , 123 ) is given by sl ( x ) = sgn ( x ) ( |x| l ) + .
see friedman et al .
( 123 ) for more elaborate discussions on how the soft - thresholding operator arises in univariate penalized least - squares problems with the 123 penal -
plugging the values of optimal di , i = 123 , .
, n; obtained from ( 123 ) in ( 123 ) we get
q ( u , v ) =
iw vi l ) + ( u
iw vi l ) + ( u
ix vi l ) 123
minimizing q ( u , v ) w . r . t .
( u , v ) is equivalent to maximizing
iw vi l ) + ( u
iw vi l ) ( u
iw vi l ) 123
iw vi l ) 123
+ ( cid : 123 ) = ( cid : 123 )
it is a standard fact that for every i the problem
uw v; such that u ( u123 , .
, ui123 ) , v ( v123 , .
, vi123 )
is solved by ui , vi , the left and right singular vectors of the matrix w corresponding to its ith largest singular value .
the maximum value equals the singular value .
it is easy to see that maximizing the expression to the right of ( 123 ) wrt ( ui , vi ) , i = 123 , .
, n is equivalent to maximizing the individual then the ( ui , vi ) , i =
if r ( l ) denotes the number of singular values of w larger than l
and right singular vectors of w corresponding to the largest singular values .
from ( 123 ) the optimal
that maximize the expression ( 123 ) correspond to ( cid : 123 ) u123 , .
, ur ( l ) ( cid : 123 ) and ( cid : 123 ) v123 , .
, vr ( l ) ( cid : 123 ) ; the r ( l ) left d = diag ( cid : 123 ) d123 , .
, dn ( cid : 123 ) is given by dl = diag ( ( d123 l ) + , .
, ( dn l ) + ) .
since the rank of w is r , the minimizer z of ( 123 ) is given by udl v as in ( 123 ) .
remark 123 for a more general spectral regularization of the form l of the resultant univariate minimization problem will be given by sp thresholding operator sp
i ( z ) ) ( as compared to i ( z ) used above ) the optimization problem ( 123 ) will be modied accordingly .
the solution iw vi ) for some generalized
l ( ) , where
the optimization problem analogous to ( 123 ) will be
iw vi ) = argmin
iw vi + d123
iw vi +
i ( cid : 123 ) + l p ( di ) .
i ) + l
123 ( cid : 123 ) 123 di u
mazumder , hastie and tibshirani
where di = sp iw vi ) , i .
any spectral function for which the above ( 123 ) is monotonically increas - ing in u iw vi for every i can be solved by a similar argument as given in the above proof .
the solution will correspond to the rst few largest left and right singular vectors of the matrix w .
the optimal singular values will correspond to the relevant shrinkage / threshold operator sp l ( ) operated on the singular values of w .
in particular for the indicator function p ( t ) = l 123 ( t 123= 123 ) , the top few singular values ( un - shrunk ) and the corresponding singular vectors is the solution .
a . 123 proof of lemma 123
this proof is based on sub - gradient characterizations and is inspired by some techniques used in cai et al .
( 123 ) .
proof from lemma 123 , we know that if z solves the problem ( 123 ) , then it satises the sub - gradient
123 ( w z ) + l
sl ( w123 ) and sl ( w123 ) solve the problem ( 123 ) with w = w123 and w = w123 respectively , hence ( 123 ) holds with w = w123 , z123 = sl ( w123 ) and w = w123 , z123 = sl ( w123 ) .
the sub - gradients of the nuclear norm kzk are given by
kzk = ( uv + w
: w mn , u w = 123 , w v = 123 , kw k123 123 ) ,
where z = udv is the svd of z .
let p ( zi ) denote an element in k zik
zi wi + l p ( zi ) = 123 , i = 123 , 123
the above gives
from which we obtain
( z123 z123 ) ( w123 w123 ) + l ( p ( z123 ) p ( z123 ) ) = 123 ,
h z123 z123 , z123 z123i hw123 w123 , z123 z123i + l hp ( z123 ) p ( z123 ) , z123 z123i = 123 ,
where ha , bi = trace ( ab ) .
now observe that
hp ( z123 ) p ( z123 ) , z123 z123i = hp ( z123 ) , z123i hp ( z123 ) , z123i hp ( z123 ) , z123i + hp ( z123 ) , z123i .
by the characterization of subgradients as in ( 123 ) , we have
hp ( zi ) , zii = k zik and kp ( zi ) k123 123 , i = 123 , 123
|hp ( zi ) , z ji| kp ( zi ) k123k z jk k z jk for i 123= j ( 123 , 123 ) .
using the above inequalities in ( 123 ) we obtain :
hp ( z123 ) , z123i + hp ( z123 ) , z123i = k z123k + k z123k
hp ( z123 ) , z123i hp ( z123 ) , z123i k z123k k z123k .
matrix completion by spectral regularization
using ( 123 , 123 ) we see that the r . h . s .
of ( 123 ) is non - negative
hp ( z123 ) p ( z123 ) , z123 z123i 123
using the above in ( 123 ) , we obtain :
k z123 z123k123
f = h z123 z123 , z123 z123i hw123 w123 , z123 z123i .
using the cauchy - schwarz inequality , k z123 z123kf kw123 w123kf h z123 z123 , w123 w123i in ( 123 ) we
and in particular
k z123 z123k123
f h z123 z123 , w123 w123i k z123 z123kf kw123 w123kf
k z123 z123k123
f k z123 z123kf kw123 w123kf .
the above further simplies to
f k z123 z123k123
f = ksl ( w123 ) sl ( w123 ) k123
a . 123 proof of lemma 123
proof we will rst show ( 123 ) by observing the following inequalities :
f = ksl ( pw ( x ) + pw ( zk
l ) ) sl ( pw ( x ) + pw ( zk123
( by lemma 123 ) k ( cid : 123 ) pw ( x ) + pw ( zk
l ) ( cid : 123 ) ( cid : 123 ) pw ( x ) + pw ( zk123
= kpw ( zk
the above implies that the sequence ( kzk below ) .
we still require to show that ( kzk
f ) converges ( since it is decreasing and bounded f ) converges to zero .
the convergence of ( kzk
f ) implies that :
f 123 as k
the above observation along with the inequality in ( 123 , 123 ) gives
f 123 = pw ( zk
lemma 123 shows that the non - negative sequence fl ( zk
l ) is decreasing in k .
so as k
sequence fl ( zk
l ) converges .
furthermore from ( 123 , 123 ) we have
l ) ql ( zk+123
) 123 as k
mazumder , hastie and tibshirani
which implies that
l ) pw ( zk+123
f 123 as k
the above along with ( 123 ) gives
l 123 as k
this completes the proof .
a . 123 proof of lemma 123
proof the sub - gradients of the nuclear norm kzk are given by
kzk = ( uv +w : wmn , u w = 123 , wv = 123 , kw k123 123 ) ,
where z = udv is the svd of z .
since zk
l minimizes ql ( z|zk123
) , it satises :
123 ( pw ( x ) + pw ( zk123
l ) + kzk
l k k .
suppose z is a limit point of the sequence zk
then there exists a subsequence ( nk ) ( 123 , 123 ,
such that znkl z as k
by lemma 123 this subsequence znkl
) znkl pw ( zl ) zl = pw ( z ) .
( pw ( x ) + pw ( znk123 l ) kzk
for every k , a sub - gradient p ( zk ties of the set kzk
l k ( 123 ) .
) znkl ) ( pw ( x ) pw ( zl ) ) .
l k corresponds to a tuple ( uk , vk , wk ) satisfying the proper -
consider p ( znkl ) along the sub - sequence nk
, znkl zl .
znkl = unkdnkv
nk , z = u dv
denote the svds .
the product of the singular vectors converge u .
furthermore due .
the limit u v + w to boundedness ( passing on to a further subsequence if necessary ) wnk w clearly satises the criterion of being a sub - gradient of z .
hence this limit corresponds to p ( zl )
nkvnk u v
furthermore from ( 123 , 123 ) , passing on to the limits along the subsequence nk , we have
123 ( pw ( x ) pw ( zl ) ) + kzl k .
hence the limit point zl
is a stationary point of fl ( z ) .
matrix completion by spectral regularization
we shall now prove ( 123 ) .
we know that for every nk
znkl = sl ( pw ( x ) + pw ( znk123
from lemma 123 , we know znkl znk123
this observation along with the continuity of sl ( ) gives
sl ( pw ( x ) + pw ( znk123
) ) sl ( pw ( x ) + pw ( zl ) ) .
thus passing over to the limits on both sides of ( 123 ) we get
zl = sl ( pw ( x ) + pw ( zl ) ) ,
therefore completing the proof .
a . 123 proof of lemma 123
the proof is motivated by the principle of embedding an arbitrary matrix into a positive semidenite matrix ( fazel , 123 ) .
we require the following proposition , which we prove using techniques used in the same reference .
proposition 123 suppose matrices wmm , wnn , zmn satisfy the following :
( cid : 123 ) w z
w ( cid : 123 ) ( cid : 123 ) 123
then trace ( w ) + trace ( w ) 123kzk .
proof let zmn = lmrs nr denote the svd of z , where r is the rank of the matrix z .
observe that the trace of the product of two positive semidenite matrices is always non - negative .
hence we have the following inequality :
trace ( cid : 123 ) llt lrt
rrt ( cid : 123 ) ( cid : 123 ) w z
w ( cid : 123 ) ( cid : 123 ) 123
simplifying the above expression we get :
trace ( lltw ) trace ( lrt zt ) trace ( rlt z ) + trace ( rrt w ) 123
due to the orthogonality of the columns of l , r we have the following inequalities :
trace ( lltw ) trace ( w ) and trace ( rrt w ) trace ( w ) .
furthermore , using the svd of z :
trace ( lrt zt ) = trace ( s ) = trace ( lrt zt ) .
using the above in ( 123 ) , we have :
trace ( w ) + trace ( w ) 123trace ( s ) = 123 kzk .
mazumder , hastie and tibshirani
proof ( proof of lemma 123 ) for the matrix z , consider any decomposition of the form z = umr v t and construct the following matrix
( cid : 123 ) u u t
v v t ( cid : 123 ) = ( cid : 123 ) u
v ( cid : 123 ) ( u t v t ) ,
which is positive semidenite .
applying proposition 123 to the left hand matrix in ( 123 ) , we have :
trace ( u u t ) + trace ( v v t ) 123 kzk .
minimizing both sides above w . r . t .
the decompositions z = umr v t
nr; we have
u , v ; z= u v t ( cid : 123 ) trace ( u u t ) + trace ( v v t ) ( cid : 123 ) 123 kzk .
through the svd of z we now show that equality is attained in ( 123 ) .
suppose z is of rank k min ( m , n ) , and denote its svd by zmn = lmks kkrt kk and v =
then for u = lmks
kk the equality in ( 123 ) is attained .
hence , we have :
u , v ;z= u v t ( cid : 123 ) trace ( u u t ) + trace ( v v t ) ( cid : 123 )
nk ( cid : 123 ) trace ( u u t ) + trace ( v v t ) ( cid : 123 ) .
u , v ;z= umk v t
note that the minimum can also be attained for matrices with r k or even r min ( m , n ) ; however , it sufces to consider matrices with r = k .
also it is easily seen that the minimum cannot be attained for any r < k; hence the minimal rank r for which ( 123 ) holds true is r = k .
a . 123 proof of theorem 123
there is a close resemblance between soft - impute and nesterovs gradient method ( nesterov , 123 , section 123 ) .
however , as mentioned earlier the original motivation of our algorithm is very
the techniques used in this proof are adapted from nesterov ( 123 ) .
proof plugging zk
l = z in ( 123 ) , we have
l ) = fl ( zk
l ( q ) denote a convex combination of the optimal solution ( z
l ) and the kth iterate ( zk
l ( q ) = q z
l + ( 123 q ) zk
matrix completion by spectral regularization
using the convexity of fl ( ) we get :
l ( q ) using ( 123 ) , and simplifying pw ( zk
l ( q ) ) ( 123 q ) fl ( zk
l ) + q l ( q ) ) we have :
l ( q ) ) k123
f = q 123kpw ( zk
line 123 follows from ( 123 ) by observing that kzm the inequalities ( 123 ) and ( 123 ) , established in theorem 123
f , ma consequence of
using ( 123 ) , the value of fl ( z ) at the ( k + 123 ) th iterate satises the following chain of inequalities :
z ( cid : 123 ) fl ( z ) + q ( 123 , 123 ) ( cid : 123 ) fl ( zk q ( 123 , 123 ) ( cid : 123 ) fl ( zk
l ( q ) ) +
l ( q ) ) k123
l ) + q ( fl ( z
l ) fl ( zk
l ) ) +
line 123 follows from line 123 , by using ( 123 ) and ( 123 ) .
the r . h . s .
expression in ( 123 ) , is minimized atbq ( k + 123 ) given by bq ( k + 123 ) = min ( 123 , q k ) ( 123 , 123 ) , where ,
l ) fl ( z
f = 123 , then we take q k =
note that q k is a decreasing sequence .
this implies that if q k123 123 then q m 123 for all m k123
suppose , q 123 > 123
thenbq ( 123 ) = 123
hence using ( 123 ) we have :
l ) fl ( z
f = q 123
thus we get back to the former case .
hence q k 123 for all k 123
in addition , observe the previous deductions show that , if q 123 > 123 then ( 123 ) holds true for k = 123
in ( 123 ) and simplifying , we get :
combining the above observations , plugging in the value ofbq
) fl ( zk
( fl ( zk
l ) fl ( z
for the sake of notational convenience , we dene the sequence a k = fl ( zk seen that a k is a non - negative decreasing sequence .
l ) fl ( z
it is easily
mazumder , hastie and tibshirani
using this notation in ( 123 ) we get :
( since a k )
a ka k+123
+ a k+123
+ a k+123
dividing both sides of the inequality in ( 123 ) , by a ka k+123 we have :
summing both sides of ( 123 ) over 123 k ( k 123 ) we get :
since q 123 123 , we observe a 123 / ( 123kz123 get , the desired inequality ( 123 ) completing the proof of the theorem .
f ) 123 / 123using this in ( 123 ) and rearranging terms we
