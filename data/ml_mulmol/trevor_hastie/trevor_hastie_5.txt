nearest neighbor classification expects the class con - ditional probabilities to be locally constant , and suf - fers from bias in high dimensions we propose a lo - cally adaptive form of nearest neighbor classification to try to finesse this curse of dimensionality .
we use a local linear discriminant analysis to estimate an ef - fective metric for computing neighborhoods .
we de - termine the local decision boundaries from centroid information , and then shrink neighborhoods in direc - tions orthogonal to these local decision boundaries , and elongate them parallel to the boundaries .
there - after , any neighborhood - based classifier can be em - ployed , using the modified neighborhoods .
the poste - rior probabilities tend to be more homogeneous in the modified neighborhoods .
we also propose a method for global dimension reduction , that combines local dimension information .
in a number of examples , the methods demonstrate the potential for substantial im - provements over nearest neighbour classification .
we consider a discrimination problem with d classes and n training consist of predictor measurements x = and the known class memberships .
our goal is to predict bership of an observation with predictor vector x123
on p predictors
the class mem -
nearest neighbor classification
is a simple and ap - pealing approach to this problem .
we find the set of k nearest neighbors in the training set to x123 and then classify x123 as the most frequent class among the k neighbors .
nearest neighbors is an extremely flex - scheme , and does not involve any pre - processing ( fitting ) of the training data .
this can offer both space and speed advantages in very large problems : see cover ( 123 ) , duda & hart ( 123 ) , mclachlan ( 123 ) for background material on nearest
cover & hart ( 123 ) show that
the one nearest
neighbour rule has asymptotic error rate at most twice the bayes rate .
however in finite samples the curse of dimensionality can severely hurt the nearest neigh - bor rule .
the relative radius of the nearest - neighbor
sphere grows like r 123 / p where p is the dimension and r the radius for p = 123 , resulting in severe bias at the target point x .
figure 123 illustrates the situation for a
figure 123 : the vertical strip denotes the nn region using only the x coordinate to find the nearest neighbor for the target point ( solid dot ) .
the sphere shows the nn region using both coordinates , and we see in this case it has ex - tended into the class 123 region ( and found the wrong class in this instance ) .
our illustration here is based on a 123 - nn rule , but the same phenomenon occurs for k - nn rules as well .
near - est neighbor techniques are based on the assumption that locally the class posterior probabilities are con - stant .
while that is clearly true in the vertical strip using only coordinate x , using x and y this
the techniques outlined in the abstract are designed to overcome these problems .
figure 123 shows an exam - ple .
there are two classes in two dimensions , one of which almost completely surrounds the other .
the left panel shows a nearest neighborhood of size 123 at the target point ( shown as origin ) , which is chosen to near the class boundary .
the right panel shows the same size neighborhood using our discriminant adap -
components of distance in the null space of b* are
other components are weighted according
eigenvalues of b* when there are more than 123 classes in which the centroids are more spread out are weighted more than those in which they are
in neighborhoods sim - thus this metric would result ilar to the narrow strip in figure 123 : infinitely the null space of b , and then deformed appropriately in the centroid subspace according to how they are placed .
it is dangerous to allow neighborhoods to ex - in any direction , so we need to limit this stretching .
our proposal is
= w - x / ~ ( b* +el ) w - 123 / ~
where e is some small tuning parameter to be deter - mined .
the metric shrinks the neighborhood in direc - tions in which the local class centroids differ , with the intention of ending up with a neighborhood in which the class centroids coincide ( and hence nearest neigh - is appropriate ) .
with this goal in the procedure , and mind one can think of iterating thus successively shrinking in directions in which the class centroids do not coincide .
here is a summary of the proposal .
discriminant adaptive nearest neighbor classifier
the metric ~ = i , the identity matriz .
spread out a nearest neighborhood of km points
around the test point x123 , in the metric ~ , , .
the weighted within and between sum of
squares matrices w and b using the points neighborhood ( see formula ( 123 below ) .
define a new metric ~ , , = w - ~ / ~ ( w - 123 / 123bw - ~ / 123 + 123
iterate steps 123 , 123 , and 123
at completion , use the metric ~ for k - nearest neigh -
bor classification at the test point x123
the metric ( 123 ) can be given a more formal justifica -
suppose we are classifying at a test point x123 and find a single nearest neighbor x according to a metric d ( x , x123 ) .
let p ( j ) x ) be the true probability of class at point x .
we consider the chi - squared distance
which measures the distance ( appropriately weighted ) between the true and estimated posteriors .
figure 123 : the lejt panel shows a spherical neighborhood containing ~ 123 points .
the right panel shows the ellipsoidal neighborhood found by the dann procedure , also contain - ing ~ 123 points .
the latter is elongated along the true decision boundary , and flattened orthogonal to it .
tive nearest neighbour procedure .
notice how the mod - ified neighborhood extends further in the direction par - to the decision boundary .
as we will see in our this new neighborhood can often provide improvement in classification
while the idea of local adaptation of the nearest
neighbour metric may seem obvious , we could find few proposms along these lines in the literature .
a sum - mary of previous work in given in section .
our proposal is motivated as follows .
consider first a standard linear discriminant ( lda ) classification pro - cedure with k classes .
let b and w denote the be - tween and within sum of squares matrices .
the data are first sphered with respect to w , then the target point is classified to the class of the closest cen - troid ( with a correction for the class prior membership probabilities ) .
since only relative distances are rele - vant , any distances in the complement of the subspace spanned by the sphered centroids can be ignored .
this complement corresponds to the null space of b .
we propose to estimate b and w locally ,
them to form a local metric that approximately be - haves like the lda metric .
one such candidate is
where b* is the between sum - of - squares in the sphered space .
consider the action of ~ as a metric for com - puting distances ( x - x123 ) t ~ ( x - - x123 ) : * it first spheres the space using w;
r ( x , x123 ) implies that the misclassification error rate will be close to the asymptotic error rate for inn , which is achieved when x = x123 or more generally when p ( jlx ) = p ( jlxo ) .
we show that the first metric ( 123 ) approximates r ( x , x123 ) .
assuming that in the neighborhood xlj has a gaus - sian distribution with mean pj and covariance e , we obtain by a simple first order taylor approximation
p ( jix ) ~ p ( jlx123 ) p ( jlxo ) ( #j
- p ) t ~ ( ~ - l ( x - -
where # = ~ j p ( jlx123 ) pj .
plugging this into ( 123 ) we
, ( x , x123 ) ~ ep ( jlx123 )
- # ) te - i ( x 123 ( 123 )
thus the approximately best distance metric - p ) ( pj p ) te - i .
es timating e
e - i ~ j p ( j ( xo ) ( pj by w and ~ j p ( jlxo ) ( l ~ j
term in the metric ( 123 ) .
- # ) ( pi by b giv esthe
by allowing prior uncertainty for the class means pi ,
that is , assume pj , . ~ n ( v / , ei ) in the sphered space , obtain the second term in the metric ( 123 ) .
define a weight function at x123 by
k ( x , x123; h )
here e123 is an initial non - negative metric ( often i ) , and ch is a symmetric real - valued function depending on a parameter h .
we use a lri - cube function defined over a k - nearest neighborhood nk ( x123 ) of x123
formally , define di = iie123 / 123 ( xi
- xo ) ll , h maxiegk ( xo ) di and
k ( xi , xo; h ) = ( 123 ( ddh ) 123 ) ai ( id l <
let b ( xo; eo , h ) and w ( xo; eo , h ) be the weighted
between and within class sum of squares matrices , where the weights assigned to the ith observation are given by wi = k ( xi , xo; eo , h ) .
that is ,
u ( 123; r , 123 , h )
to start with e123 = i ( the identity matrix ) and iterate is a metric e for use in this procedure .
the result a nearest neighbor classification rule at x123
in our examples we try either a single step of this procedure , or larger number of iterations .
the dann metric
to ask whether the mapping g ( . ) has
it is natural fixed point , and if it does , whether an iteration of the form e , - - g ( e ) converges to it .
these questions seem to answer in general .
to get some insight , it is helpful to consider an equivalent form of the it - eration .
at each step we take a spherical neighbor - hood around the test point , estimate the metric i123 , and then transform the predictors via x " ~ = el / 123xta .
at completion we use a spherical nearest neighbor rule in transformed space .
it is easy to show that this procedure is equivalent to the one given above .
if the metrics estimated in j iterations are el , e123 , ej , then the effective metric for the original coordinates is
expressed in this way , the fixed points of the itera -
v , 123 ~ , v . 123 v , ll123 vdl123 tion satisfy w - 123bw - 123 + ew - l = ci .
a fixed point occurs when b is zero and w is pro - portional to the identity matrix , in the space of the
in practice we find it more effective to estimate only the diagonal elements of w , and assume that the off diagonal elements are zero .
this is especially true if the dimension of the predictor space is large , as there will be insufficient data locally to estimate the o ( p123 ) elements of w .
with the diagonal approximation , the two forms of the algorithm are not equivalent : we use the version that transforms the space at each step since a diagonal approximation makes most sense in the transformed coordinates .
if the predictors are spatially or temporally related , we might use a penalized estimate of w that down - weights components of the covariance that correspond to spatially noisy signals ( hastie , buja & tibshirani 123 ) .
a related approach is to pre - filter using a smooth basis , and then operate in the reduced
in the final neighborhood we perform k nearest
neighbor classification .
an alternative approach would be to use discriminant analysis to perform the classi - fication , using the locally determined parameters .
we are currently investigating this approach .
e;=i eyi : j wi ( xi
where 123j is the weighted mean of the nj observations in the jth group .
finally , we let b ( x123;e123 , h ) and b ( x123; e123 , h ) determine the metric e in ( 123 ) .
notice that equations ( 123 ) and ( 123 ) produce a mapping
s123 - * e , say e = g ( e123 ) .
an approach we explore
the dann procedure has a number of adjustable
km : the number of nearest neighbors in the neighborhood
nkm ( x123 ) for estimation of the metric;
k : the number of neighbors in the final nearest neighbor
e : the " softening " parameter in the metric .
test set or cross validation could be used to estimate an optimal values for these parameters .
in the exam - ples in the next section we instead use fixed choices .
the value of km must be reasonably large since initial neighborhood is used to estimate a covariance : we use km = max ( n / 123 , 123 ) .
to ensure consistency one should take km to be a vanishing fraction of n , and should also use larger values for higher dimensional problems .
a smaller number of neighbors is preferable rule to avoid bias : we used for the final classification k = 123 , and compared it to standard 123 nearest neigh - bors .
note that the metric ( 123 ) is invariant under non - singular transformations of the predictors , and hence it makes sense to consider fixed values of e .
after some experimentation , we found that the value e = 123 works well , and we use this value in the examples below .
where the b ( i ) are the local between sum of squares matrices .
this latter problem is solved by finding the largest eigenvectors of the average between sum of squares matrix ~ 123 b ( i ) / n .
lda a ~ l lo ~ l subspac ~ - - k = 123
figure 123 : ( left panel ) two dimensional gaussian data with two classes and correlation 123 .
the solid lines are the lda decision boundary and its equivalent subspace for clas - sification .
the dashed lines were produced by the local pro - cedure described in this section .
( right panel ) each line segment represents the local between information centered at that point .
figure 123 shows a simple illustrative
two classes are gaussian with substantial within class covariance between the two predictors xl and x123
in the left panel , the solid line is the gaussian decision boundary that optimally separates the classes .
the orthogonal vector labeled s is a one dimensional sub - space onto which we can project the data and perform classification .
using the knowledge that the data are gaussian , it is the leading discriminant direction .
the broken lines are the boundaries and equivalent sub - space produced by our procedure .
in the right panel , each line segment represents the local between infor - mation centered at that point .
our procedure uses a principal components analysis of these n x j line seg - ments to produce the broken line subspace in the left
in a meaningful way , notice
to allow combination of the local between informa - that we have not sphered the data locally before computing the mean deviations .
a justification for this is that any local spherical window containing two classes , say , will have approximately a linear decision boundary orthogonal to the vector joining the two means .
figure 123 shows the eigenvalues of the average be - tween matrix for an instance of a two class , 123 dimen - sional sphere model with 123 noise dimensions .
the de - cision boundary is a 123 dimensional sphere , although locally linear ( full details of this example are given in the next section ) .
for this demonstration we randomly the 123 dimensional data , so that the dimen - sions to be trimmed are not coordinate directions .
the eigenvalues show a distinct change after 123 ( the correct
so far our technique has been entirely based " , in that we locally adapt a neighborhood about a query point at the time of classification .
here we describe a method for performing a global dimension reduction , by pooling the local dimension information over all points in the training set .
in a nutshell we con - sider subspaces corresponding to eigenvectors of the average local between sum - of - squares matrices .
consider first how linear discriminant
( lda ) works .
after sphering the data , it concentrates in the space spanned by the class means ~ j or a reduced rank space that lies close to these means .
if ~ denote the overall mean , this subspace is exactly the princi - pal component hyperplane for the data points ~ j - ~ , weighted by the class proportions .
our idea to compute the deviations : ~ j i locally
a neighborhood around each of the n training points , and then do an overall principal components analysis for the n x j deviations .
here are the details .
let xi ( i ) be the mean of class j vectors in a neighborhood of the ith training point , and ~ ( i ) be the overall mean .
all means are weighted by the local class membership pro - portions zr123 ( i ) , the local centroid deviations .
we seek a subspace that gets close in average weighted squared distance to all n x j of these .
denoting by u ( p x j ) an orthonormal basis for the k < p dimensional subspace , we minimize
j = 123 , . . . , d .
let xi ( i ) = xj ( i ) -
or the total weighted residual sum of squares .
it is not hard to show that minimizing rss ( u ) amounts
123d sphere with 123 noise variables
figure 123 : the eigenvalues of the average between matrix for the 123d sphere - i - 123 noise variable problem .
using these four dimensions followed by our dann nearest neigh - bor routine , we get better performance than 123nn in the real
dimension ) , and using our dann classifier four dimensions actually beats ordinary 123nn in the known four dimensional sphere subspace .
it is desirable to automate the dimension reduction is based on in high dimen -
operation .
since our local spherical neighborhoods ( potentially sions ) , we find an iterative approach most success - ful .
we apply this procedure in the full space , and use cross - validated dann to find the best nested sub - space ( with a built in bias towards larger subspaces ) .
we then successively repeat these operations new subspaces , until no further suitable by cv .
using dann in this final subspace is what we have labelled sub - dann in the boxplots of
in the following examples we compare several classifi - lda - - linear discriminant analysis
reduced lda - - linear discriminant restricted ( known ) relevant subspace , where appropriate .
123 - nn : 123 nearest neighbor classification
dann - - discriminant adaptive nearest neighbor ,
iter - dann - - discriminant adaptive nearest neigh - bor , five iterations .
sub - dann - - discriminant adaptive nearest neigh - bor , with automatic subspace reduction .
this is de - scribed in section .
for all methods , the predictors were first standard - ized so as to have zero mean and unit variance over the training set , and the test set predictors was standard - ized by the corresponding training mean and variance .
the training and test set sizes were 123 and 123 , unless
123 dimensional gaussian with 123 ~ noise . two gaus - in two dimensions ( x123 , x123 ) separated by 123 units in x123
the predictors have variance ( 123 , 123 ) and correlation 123 .
the additional 123 predictors are independent standard gaussians .
unstructured with 123 noise .
there are 123 classes each with 123 spherical bivariate normal subclasses , hav - ing standard deviation 123 .
the means of the 123 subclasses were chosen at random ( without replace - ment ) from the integers ( 123 , 123 , . . .
123 ) x ( 123 , 123 , . . . 123 ) .
each training sample had 123 observations per subclass , for a total of 123 observations .
the additional 123 predic - tors are independent standard gaussians .
123 dimensional spheres with 123 noise .
in this exam - ple there are 123 predictors and 123 classes .
the last 123 predictors are noise variables , with standard gaus - independent of each other and the class membership .
the first four predictors in class 123 are independent standard normal , conditioned on the radius being greater than 123 , while the first four predictors in class 123 are independent standard nor - mal without the restriction .
the first class almost completely surrounds the second class in the four di - mensional subspace of the first four predictors .
this example was designed to see if dann could improve upon nearest neighbors in the presence of noise vari -
123 dimensional spheres .
as in the previous exam - ple there are 123 predictors and 123 classes .
now all 123 predictors in class 123 are independent standard nor - mal , conditioned on the radius being greater 123 and less than 123 , while the predictors in class 123 are independent standard normal without the re - in this example there are no pure noise the kind that a nearest neighbor subset selection rule might be able weed out .
at any given point in the feature space , the class discrimination occurs along only one direction .
however this direc - tion changes as we move across the feature space and all variables are important somewhere in the space .
the first class almost completely surrounds the sec - ond class in the full ten - dimensional space .
the results for the simulated examples are summarized in figures 123
dann seems to do as well as 123 - nn across the board , and offers significant improvements in problems with noise variables .
dann does not do as well as reduced
two gausslane with noise
unstructured with noise
123 - d sphere in 123 - d
123 - d sphere in 123 - d
. + - , , , + <> / /
figure 123 : boxplots of error rates over ~ 123 simulations .
nearest neighbors in problems 123 and 123 : this is not sur - prising since in effect we are giving the nearest neigh - bor rule the information that dann is trying to infer from the training data .
a nearest neighbor method with variable selection might do well in these problems : however this procedure can be foiled by by rotating the relevant subspace away from the coordinate directions .
on the average there seems to be no advantage in carrying out more than one iteration of the dann pro - cedure .
the subspace dann procedure is the over - all winner , producing big gains in problems admitting global dimension reduction .
the top panel of figure 123 shows error rates relative
to 123 - nn , accumulated across 123 x 123 simulated problems ( these 123 and another 123 described in hastie & tibshirani ( 123 ) .
the bottom panel shows the rates relative
we see that dann is 123 - 123% better than 123 - nn on the average , and is at most 123% worse .
dann is also better than lda on the average but can be three times worse ( in problem 123 ) .
image scene classification is an important and difficult problem .
the example we consider here is classifying images of an area of the earths surface into land and vegetation type .
other examples include clas - sification of x - rays , such as mammograms , or calcified into normal or cancerous regions .
figure 123 shows four spectral bands of a section of spectrum ( red and green )
land , two in the visible
enor rell fcaljve
e / tot r ~ r ~ ll to lda
+* ~ i
figure 123 : relative error rates ol the methods across the 123 simulated problems .
in the top panel the error rate has been divided by the error rate 123 - nn , on a simulation by simulation basis .
in the bottom panel we have divided by the error rate ol lda .
spectral band 123
speclml band 123
spectral band 123
spectral band 123
land use ( actual )
land use ( predicted )
figure 123 : the first four images are the satellite in the four spectral bands .
the fifth image is the known classification , and the final image is the classification map produced by linear discrlmlnant analysis .
a pixel and its 123 neighbors
figure 123 : the pixel intensities of the 123 - neighbors of a pixel ( and itself ) are used as features for classification
and two in the infra red spectrum .
these data are taken from the statlog projects archive spigelhalter & taylor 123 ) 123
the goal is to clas - sify each pixel into one of 123 land types : red soil , cotton , vegetalion stubble , mixture , grey soil , damp grey soil , very damp grey soil .
we extract for each pixel its 123 - neighbors , as depicted in figure 123 , giving us ( 123+ 123 ) x 123 = 123 features ( the pixel intensities ) per pixel to be classified .
the data come scrambled , with 123 training pixels and 123 test pixels , each with their 123 features and the known classification .
figure 123 is the true classification , as well as that pro - duced by linear discriminant analysis .
figure 123 shows
satellite image classification
figure 123 : miscla . ssification results as a function of sub - space size , for the satellite image data
figure 123 : misclassification results of a variety of classifi - cation procedures on the satellite image test data ( taken from michie et al .
( 123 ) ) .
the results reported in michie et al .
( 123 ) for a va - riety of classifiers; they reported the best result for included in the figure is the result for dann , which has outperformed 123 - nn .
we also ran the subspace version of dann , and figure 123 shows the sequence of of test - error results as a function of sub - space size .
again , a low - dimensional subspace actually improves the misclassification error .
that can offer substantial
we have developed an adaptive form of nearest neigh - ments over standard nearest neighbors method in some problems .
we have also proposed a method that uses local discrimination information to estimate a subspace for global dimension reduction .
short & fukanaga ( 123 ) proposed a technique close to ours for the two class problem .
in our terminology they used our metric with w = i and e = 123 , with b
123the authors thank c .
taylor and d .
spiegelhalter for
making these images and data available
this extends the neighborhood infinitely
in a neighborhood of size km .
in null space of the local between class directions , but this neighborhood to the original km ob - servations .
this amounts to projecting the local data onto the line joining the two local centroids .
experiments this approach tended to perform on aver - age 123% worse than our metric , and we did not pursue it further .
short & fukanaga ( 123 ) extended this j > 123 classes , but here their approach differs even more from ours .
they computed a weighted average of the j local centroids from the overall average , and project the data onto it , a one dimensional projection .
even with e = 123 we project the data onto the sub - space containing the local centroids , and deform the in that subspace .
myles & hand ( 123 ) recognized a shortfall of the short and fukanaga approach , since the averaging can cause cancellation , and proposed other metrics to avoid this .
although their metrics differ from ours , the chi - squared moti - vation for our metric ( 123 ) was inspired by the metrics developed in their paper .
we have not tested out their proposals , but they report results of experiments with far more modest improvements over standard nearest neighbors than we achieved .
friedman ( 123 ) proposes a number of techniques for flexible metric nearest neighbor classification .
these techniques use a recursive partitioning style strategy to adaptively shrink and shape rectangular neighbor - hoods around the test point .
friedman also uses de - rived variables in the process , including discriminant variates .
with the latter variables , his procedures have to the discriminant adaptive nearest
other recent work that is somewhat related to this is that of lowe ( 123 ) .
he estimates matrix in a variable kernel classifier using a neural net -
there are a number of ways in which this work might
in some discrimination problems , it to use specialized distance measures that in the feature space .
for exam -
ple simard , lecun & denker ( 123 ) , iiastie , & sackinger ( 123 ) , use a transformation - invariant metric to measure distance between digitized of handwritten numerals in a nearest neighbor rule .
ages such as rotation , invariant distance measure might be used in a linear discriminant analysis and hence in the dann proce -
include local transformations of im - shear and stroke - thickness
another interesting possibility would be to apply the techniques of this paper to regression problems .
in this case the response variable is quantitative a class label .
natural analogues of the local between and within matrices exist , and can be used to shape the neighborhoods for near - neighbor and local poly - nomial regression techniques .
likewise , the dimension reduction ideas of section can also be applied .
there is a strong connection between the latter and the sliced inverse regression technique of duan & li ( 123 ) for - subspace identification .
we are currently exploring acknowledgments we thank jerome friedman for sharing his recent work , which stimulated us to embark on this project , and for many enjoyable conversations .
the second author was supported by a grant from the natural sciences and engineering research coun - cil of canada .
