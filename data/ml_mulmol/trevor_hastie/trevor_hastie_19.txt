summary .
we propose the elastic net , a new regularization and variable selection method .
real world data and a simulation study show that the elastic net often outperforms the lasso , while enjoying a similar sparsity of representation .
in addition , the elastic net encourages a grouping effect , where strongly correlated predictors tend to be in or out of the model together . the elastic net is particularly useful when the number of predictors ( p ) is much bigger than the number of observations ( n ) .
by contrast , the lasso is not a very satisfactory variable selection method in the p ( cid : 123 ) n case .
an algorithm called lars - en is proposed for computing elastic net regularization paths efciently , much like algorithm lars does for the lasso .
keywords : grouping effect; lars algorithm; lasso; penalization; p ( cid : 123 ) n problem; variable
introduction and motivation
we consider the usual linear regression model : given p predictors x123 , .
, xp , the response y is
y= 123 + x123 123 + .
+ xp p :
a model tting procedure produces the vector of coefcients = .
for example , the ordinary least squares ( ols ) estimates are obtained by minimizing the residual sum of squares .
the criteria for evaluating the quality of a model will differ according to the circumstances .
typically the following two aspects are important :
( a ) accuracy of prediction on future datait is difcult to defend a model that predicts
( b ) interpretation of the modelscientists prefer a simpler model because it puts more light on the relationship between the response and covariates .
parsimony is especially an impor - tant issue when the number of predictors is large .
it is well known that ols often does poorly in both prediction and interpretation .
penaliza - tion techniques have been proposed to improve ols .
for example , ridge regression ( hoerl and kennard , 123 ) minimizes the residual sum of squares subject to a bound on the l123 - norm of the coefcients .
as a continuous shrinkage method , ridge regression achieves its better prediction performance through a biasvariance trade - off .
however , ridge regression cannot produce a parsimonious model , for it always keeps all the predictors in the model .
best subset selection in
address for correspondence : trevor hastie , department of statistics , stanford university , stanford , ca 123 ,
123 royal statistical society
zou and t .
hastie
contrast produces a sparse model , but it is extremely variable because of its inherent discreteness , as addressed by breiman ( 123 ) .
a promising technique called the lasso was proposed by tibshirani ( 123 ) .
the lasso is a penalized least squares method imposing an l123 - penalty on the regression coefcients .
owing to the nature of the l123 - penalty , the lasso does both continuous shrinkage and automatic vari - able selection simultaneously .
tibshirani ( 123 ) and fu ( 123 ) compared the prediction per - formance of the lasso , ridge and bridge regression ( frank and friedman , 123 ) and found that none of them uniformly dominates the other two .
however , as variable selection becomes increasingly important in modern data analysis , the lasso is much more appealing owing to its
although the lasso has shown success in many situations , it has some limitations .
consider
the following three scenarios .
( a ) in the p > n case , the lasso selects at most n variables before it saturates , because of the nature of the convex optimization problem .
this seems to be a limiting feature for a variable selection method .
moreover , the lasso is not well dened unless the bound on the l123 - norm of the coefcients is smaller than a certain value .
( b ) if there is a group of variables among which the pairwise correlations are very high , then the lasso tends to select only one variable from the group and does not care which one is selected .
see section 123 .
( c ) for usual n > p situations , if there are high correlations between predictors , it has been empirically observed that the prediction performance of the lasso is dominated by ridge regression ( tibshirani , 123 ) .
scenarios ( a ) and ( b ) make the lasso an inappropriate variable selection method in some sit - uations .
we illustrate our points by considering the gene selection problem in microarray data analysis .
a typical microarray data set has many thousands of predictors ( genes ) and often fewer than 123 samples .
for those genes sharing the same biological pathway , the correlations between them can be high ( segal and conklin , 123 ) .
we think of those genes as forming a group .
the ideal gene selection method should be able to do two things : eliminate the trivial genes and automatically include whole groups into the model once one gene among them is selected ( grouped selection ) .
for this kind of p ( cid : 123 ) n and grouped variables situation , the lasso is not the ideal method , because it can only select at most n variables out of p candidates ( efron et al . , 123 ) , and it lacks the ability to reveal the grouping information .
as for prediction per - formance , scenario ( c ) is not rare in regression problems .
so it is possible to strengthen further the prediction power of the lasso .
our goal is to nd a new method that works as well as the lasso whenever the lasso does the best , and can x the problems that were highlighted above , i . e .
it should mimic the ideal variable selection method in scenarios ( a ) and ( b ) , especially with microarray data , and it should deliver better prediction performance than the lasso in scenario ( c ) .
in this paper we propose a new regularization technique which we call the elastic net .
similar to the lasso , the elastic net simultaneously does automatic variable selection and continuous shrinkage , and it can select groups of correlated variables .
it is like a stretchable shing net that retains all the big sh .
simulation studies and real data examples show that the elastic net often outperforms the lasso in terms of prediction accuracy .
in section 123 we dene the nave elastic net , which is a penalized least squares method using a novel elastic net penalty .
we discuss the grouping effect that is caused by the elastic net penalty .
in section 123 , we show that this nave procedure tends to overshrink in regression problems .
we then introduce the elastic net , which corrects this problem .
an efcient algorithm lars - en is
proposed for computing the entire elastic net regularization paths with the computational effort of a single ols t .
prostate cancer data are used to illustrate our methodology in section 123 , and simulation results comparing the lasso and the elastic net are presented in section 123
section 123 shows an application of the elastic net to classication and gene selection in a leukae - mia microarray problem .
nave elastic net
suppose that the data set has n observations with p predictors .
let y = . y123 , .
, yn / t be the response and x= . x123| .
. |xp / be the model matrix , where xj = . x123j , .
, xnj / t , j = 123 , .
, p , are the predictors .
after a location and scale transformation , we can assume that the response is centred and the predictors are standardized ,
xij = 123
ij = 123 ,
for j = 123 , 123 ,
yi = 123 ,
for any xed non - negative 123 and 123 , we dene the nave elastic net criterion
l . 123 , 123 , / =|y x|123 + 123||123 + 123||123 ,
||123 = p ( cid : 123 ) ||123 = p ( cid : 123 )
the nave elastic net estimator is the minimizer of equation ( 123 ) :
this procedure can be viewed as a penalized least squares method .
let = 123= . 123 + 123 / ; then solving in equation ( 123 ) is equivalent to the optimization problem
= arg min
( l . 123 , 123 , / ) :
= arg min
subject to . 123 / ||123 + ||123 ( cid : 123 ) t for some t :
we call the function . 123 / ||123 + ||123 the elastic net penalty , which is a convex combination of the lasso and ridge penalty .
when = 123 , the nave elastic net becomes simple ridge regression .
in this paper , we consider only < 123
for all ( 123 , 123 / , the elastic net penalty function is singular ( without rst derivative ) at 123 and it is strictly convex for all > 123 , thus having the characteristics of both the lasso and ridge regression .
note that the lasso penalty ( = 123 ) is convex but not strictly convex .
these arguments can be seen clearly from fig
we now develop a method to solve the nave elastic net problem efciently .
it turns out that minimizing equation ( 123 ) is equivalent to a lasso - type optimization problem .
this fact implies that the nave elastic net also enjoys the computational advantage of the lasso .
zou and t .
hastie
two - dimensional contour plots ( level 123 ) ( ( cid : 123 ) - ( cid : 123 ) - ( cid : 123 ) - , shape of the ridge penalty; - - - - - - - , contour of the , contour of the elastic net penalty with d 123 : 123 ) : we see that singularities at the vertices and the edges are strictly convex; the strength of convexity varies with
lemma 123
given data set . y , x / and . 123 , 123 / , dene an articial data set . y , x / by
= . 123+ 123 / 123=123
. 123+ 123 / and =
. 123+ 123 / .
then the nave elastic net criterion can be written
l . , / = l . , / = ( cid : 123 ) ( cid : 123 ) y x ( cid : 123 ) ( cid : 123 ) 123 +
let = 123=
= arg min
the proof is just simple algebra , which we omit .
lemma 123 says that we can transform the nave elastic net problem into an equivalent lasso problem on augmented data .
note that the sample size in the augmented problem is n+ p and x has rank p , which means that the nave elastic net can potentially select all p predictors in all situations .
this important property overcomes the limitations of the lasso that were described in scenario ( a ) .
lemma 123 also shows that the nave elastic net can perform an automatic variable selection in a fashion similar to the lasso .
in the next section we show that the nave elastic net has the ability of selecting grouped variables , a property that is not shared by the lasso .
exact solutions for the lasso ( - - - - - - - ) , ridge regression ( ( cid : 123 ) - ( cid : 123 ) - ( cid : 123 ) - ) and the nave elastic net ( in an orthogonal design ( .
. , ols ) : the shrinkage parameters are 123 d 123 and 123 d 123
in the case of an orthogonal design , it is straightforward to show that with parameters . 123 , 123 /
the nave elastic net solution is
i . nave elastic net / = . | i . ols / | 123=123 / +
where . ols / = xty and z+ denotes the positive part , which is z if z > 123 and 123 otherwise .
the solution of ridge regression with parameter 123 is given by . ridge / = . ols / = . 123+ 123 / , and the lasso solution with parameter 123 is
i . lasso / = . | i . ols / | 123=123 / + sgn ( i . ols / ) :
123 shows the operational characteristics of the three penalization methods in an orthogonal design , where the nave elastic net can be viewed as a two - stage procedure : a ridge - type direct shrinkage followed by a lasso - type thresholding .
the grouping effect in the large p , small n problem ( west et al . , 123 ) , the grouped variables situation is a partic - ularly important concern , which has been addressed many times in the literature .
for example , principal component analysis has been used to construct methods for nding a set of highly correlated genes in hastie et al .
( 123 ) and daz - uriarte ( 123 ) .
tree harvesting ( hastie et al . , 123 ) uses supervised learning methods to select groups of predictive genes found by hierar - chical clustering .
using an algorithmic approach , dettling and buhlmann ( 123 ) performed the clustering and supervised learning together .
a careful study by segal and conklin ( 123 ) strongly motivates the use of a regularized regression procedure to nd the grouped genes .
we consider the generic penalization method
zou and t .
hastie
where j . / is positive valued for ( cid : 123 ) = 123
= arg min
|y x|123 + j . /
qualitatively speaking , a regression method exhibits the grouping effect if the regression coefcients of a group of highly correlated variables tend to be equal ( up to a change of sign if negatively correlated ) .
in particular , in the extreme situation where some variables are exactly identical , the regression method should assign identical coefcients to the identical variables .
lemma 123
assume that xi = xj , i , j ( 123 , .
( a ) if j . / is strictly convex , then i = j , > 123
( b ) if j . / =||123 , then i j ( cid : 123 ) 123 and
is another minimizer of equation ( 123 ) , where
i + j / . s / .
i + j / . 123 s /
if k ( cid : 123 ) = i and k ( cid : 123 ) = j , if k= i , if k= j ,
for any s ( 123 , 123 ) .
lemma 123 shows a clear distinction between strictly convex penalty functions and the lasso penalty .
strict convexity guarantees the grouping effect in the extreme situation with identical predictors .
in contrast the lasso does not even have a unique solution .
the elastic net penalty with 123 > 123 is strictly convex , thus enjoying the property in assertion ( 123 ) .
theorem 123
given data . y , x / and parameters . 123 , 123 / , the response y is centred and the predictors x are standardized .
let . 123 , 123 / be the nave elastic net estimate .
suppose that i . 123 , 123 / j . 123 , 123 / > 123
. i , j / = 123
| i . 123 , 123 / j . 123 , 123 / |;
. i , j / ( cid : 123 ) 123
where = xt the unitless quantity d123 , 123
i xj , the sample correlation .
. i , j / describes the difference between the coefcient paths of : =123 then consider xj ) , predictors i and j .
if xi and xj are highly correlated , i . e .
theorem 123 says that the difference between the coefcient paths of predictor i and predictor j is almost 123
the upper bound in the above inequality provides a quantitative description for the grouping effect of the nave elastic net .
: = 123 ( if
the lasso does not have the grouping effect .
scenario ( b ) in section 123 occurs frequently in practice .
a theoretical explanation is given in efron et al .
( 123 ) .
for a simpler illustration , let us consider the linear model with p= 123
tibshirani ( 123 ) gave the explicit expression for .
123 , 123 / , from which we easily obtain that | 123 123|=| cos . / | , where is the angle between y and x123 x123
it is easy to construct examples such that = corr . x123 , x123 / 123 but cos . / does not
bayesian connections and the lq - penalty |j|q in equa - bridge regression ( frank and friedman , 123; fu , 123 ) has j . / =||q tion ( 123 ) , which is a generalization of both the lasso ( q = 123 ) and ridge regression ( q = 123 )
bridge estimator can be viewed as the bayes posterior mode under the prior
ridge regression ( q= 123 ) corresponds to a gaussian prior and the lasso ( q= 123 ) a laplacian ( or double - exponential ) prior .
the elastic net penalty corresponds to a new prior given by
p , q . / = c . , q / exp . ||q
p , . / = c . , / exp ( ( ||123 + . 123 / ||123 ) ) ,
a compromise between the gaussian and laplacian priors .
although bridge regression with 123 < q < 123 will have many similarities with the elastic net , there is a fundamental difference between them .
the elastic net produces sparse solutions , whereas bridge regression does not .
fan and li ( 123 ) proved that , in the lq ( q ( cid : 123 ) 123 ) penalty family , only the lasso penalty ( q= 123 ) can produce a sparse solution .
bridge regression ( 123 < q < 123 ) always keeps all predictors in the model , as does ridge regression .
since automatic variable selection via penalization is a primary objective of this paper , lq ( 123 < q < 123 ) penalization is not a candidate .
elastic net
deciency of the nave elastic net as an automatic variable selection method , the nave elastic net overcomes the limitations of the lasso in scenarios ( a ) and ( b ) .
however , empirical evidence ( see sections 123 and 123 ) shows that the nave elastic net does not perform satisfactorily unless it is very close to either ridge regression or the lasso .
this is why we call it nave .
in the regression prediction setting , an accurate penalization method achieves good pre - diction performance through the biasvariance trade - off .
the nave elastic net estimator is a two - stage procedure : for each xed 123 we rst nd the ridge regression coefcients , and then we do the lasso - type shrinkage along the lasso coefcient solution paths .
it appears to incur a double amount of shrinkage .
double shrinkage does not help to reduce the variances much and introduces unnecessary extra bias , compared with pure lasso or ridge shrinkage .
in the next section we improve the prediction performance of the nave elastic net by correcting this double
the elastic net estimate we follow the notation in section 123 .
given data . y , x / , penalty parameter . 123 , 123 / and aug - mented data . y , x / , the nave elastic net solves a lasso - type problem
|y x|123 +
= arg min
the elastic net ( corrected ) estimates are dened by
recall that . nave elastic net / = ( 123=
. elastic net / = . 123+ 123 / . nave elastic net / :
hence the elastic net coefcient is a rescaled nave elastic net coefcient .
such a scaling transformation preserves the variable selection property of the nave elastic net and is the simplest way to undo shrinkage .
hence all the good properties of the nave elastic
zou and t .
hastie
net that were described in section 123 hold for the elastic net .
empirically we have found that the elastic net performs very well when compared with the lasso and ridge regression .
we have another justication for choosing 123+ 123 as the scaling factor .
consider the exact solution of the nave elastic net when the predictors are orthogonal .
the lasso is known to be minimax optimal ( donoho et al . , 123 ) in this case , which implies that the nave elastic net is not optimal .
after scaling by 123+ 123 , the elastic net automatically achieves minimax optimality .
a strong motivation for the . 123 + 123 / - rescaling comes from a decomposition of the ridge operator .
since the predictors x are standardized , we have
123 p123 , p
where i , j is sample correlation .
ridge estimates with parameter 123 are given by . ridge / = ry ,
r= . xtx+ 123i / 123xt :
we can rewrite r as
r is like the usual ols operator except that the correlations are shrunk by the factor 123= . 123+ 123 / , which we call decorrelation .
hence from equation ( 123 ) we can interpret the ridge operator as decorrelation followed by direct scaling shrinkage .
this decomposition suggests that the grouping effect of ridge regression is caused by the decorrelation step .
when we combine the grouping effect of ridge regression with the lasso , the direct 123= . 123+ 123 / shrinkage step is not needed and is removed by rescaling .
although ridge regression requires 123= . 123+ 123 / shrinkage to control the estimation variance effectively , in our new method , we can rely on the lasso shrinkage to control the variance and to obtain sparsity .
from now on , let stand for . elastic net / .
the next theorem gives another presentation of
the elastic net , in which the decorrelation argument is more explicit .
theorem 123
given data . y , x / and . 123 , 123 / , then the elastic net estimates are given by
= arg min
123ytx+ 123 ||123 :
it is easy to see that
. lasso / = arg min
t . xtx / 123ytx+ 123 ||123 :
hence theorem 123 interprets the elastic net as a stabilized version of the lasso .
note that = xtx is a sample version of the correlation matrix and
= . 123 / + i
with = 123= . 123+ 123 / shrinks towards the identity matrix .
together equations ( 123 ) and ( 123 ) say that rescaling after the elastic net penalization is mathematically equivalent to replacing with its shrunken version in the lasso .
in linear discriminant analysis , the prediction accuracy can often be improved by replacing by a shrunken estimate ( friedman , 123; hastie et al . , 123 ) .
likewise we improve the lasso by regularizing in equation ( 123 ) .
connections with univariate soft thresholding the lasso is a special case of the elastic net with 123 = 123
the other interesting special case of the elastic net emerges when 123 .
by theorem 123 , . / as 123 , where
t 123ytx+ 123 ||123 :
. / = arg min . / has a simple closed form
i= 123 , 123 ,
observe that ytxi is the univariate regression coefcient of the ith predictor and . / are the estimates by applying soft thresholding on univariate regression coefcients; thus equation ( 123 ) is called univariate soft thresholding ( ust ) .
ust totally ignores the dependence between predictors and treats them as independent vari - ables .
although this may be considered illegitimate , ust and its variants are used in other meth - ods such as signicance analysis of microarrays ( tusher et al . , 123 ) and the nearest shrunken centroids classier ( tibshirani et al . , 123 ) , and have shown good empirical performance .
the elastic net naturally bridges the lasso and ust .
computation : the algorithm lars - en we propose an efcient algorithm called lars - en to solve the elastic net efciently , which is based on the recently proposed algorithm lars of efron et al .
( 123 ) .
they proved that , starting from zero , the lasso solution paths grow piecewise linearly in a predictable way .
they proposed a new algorithm called lars to solve the entire lasso solution path efciently by using the same order of computations as a single ols t .
by lemma 123 , for each xed 123 the elastic net problem is equivalent to a lasso problem on the augmented data set .
so algorithm lars can be directly used to create the entire elastic net solution path efciently with the computational efforts of a single ols t .
note , however , that for p ( cid : 123 ) n the augmented data set has p+ n observations and p variables , which can slow the computation considerably .
we further facilitate the computation by taking advantage of the sparse structure of x , which is crucial in the p ( cid : 123 ) n case .
in detail , as outlined in efron et al .
( 123 ) , at the kth step we need to invert the matrix gak ak , where ak is the active variable set .
this is done efciently by updating or downdating the cholesky factorization of gak123 that is found at the previous step .
note that
ga = 123
axa + 123i /
for any index set a , so it amounts to updating or downdating the cholesky factorization of + 123i .
it turns out that we can use a simple formula to update the cholesky fac - + 123i , which is very similar to the formula that is used for updat - torization of xt xak123 ( golub and van loan , 123 ) .
the exact same ing the cholesky factorization of xt
zou and t .
hastie
downdating function can be used for downdating the cholesky factorization of xt 123i .
in addition , when calculating the equiangular vector and the inner products of the non - active predictors with the current residuals , we can save computations by using the simple fact j has p 123 zero elements .
in a word , we do not explicitly use x to compute all the quantities in algorithm lars .
it is also economical to record only the non - zero coefcients and the active variables set at each lars - en step .
algorithm lars - en sequentially updates the elastic net ts .
in the p ( cid : 123 ) n case , such as with microarray data , it is not necessary to run the algorithm to the end ( early stopping ) .
real data and simulated computational experiments show that the optimal results are achieved at an early stage of algorithm lars - en .
if we stop the algorithm after m steps , then it requires o . m123 + pm123 / operations .
choice of tuning parameters we now discuss how to choose the type and value of the tuning parameter in the elastic net .
although we dened the elastic net by using . 123 , 123 / , it is not the only choice as the tuning parameter .
in the lasso , the conventional tuning parameter is the l123 - norm of the coefcients ( t ) or the fraction of the l123 - norm ( s ) .
by the proportional relationship between , we can also use . 123 , s / or . 123 , t / to parameterize the elastic net .
the advantage of using . 123 , s / is that s is always valued within ( 123 , 123 ) .
in algorithm lars the lasso is des - cribed as a forward stagewise additive tting procedure and shown to be ( almost ) identical to " - l123 boosting ( efron et al . , 123 ) .
this new view adopts the number of steps k of algorithm lars as a tuning parameter for the lasso .
for each xed 123 , the elastic net is solved by our algorithm lars - en; hence similarly we can use the number of the lars - en steps . k / as the second tuning parameter besides 123
the above three types of tuning parameter corres - pond to three ways to interpret the piecewise elastic net or lasso solution paths as shown in
there are well - established methods for choosing such tuning parameters ( hastie et al .
( 123 ) , chapter 123 ) .
if only training data are available , tenfold cross - validation ( cv ) is a popular method for estimating the prediction error and comparing different models , and we use it here .
note that there are two tuning parameters in the elastic net , so we need to cross - validate on a two - dimensional surface .
typically we rst pick a ( relatively small ) grid of values for 123 , say . 123 , 123 : 123 , 123 : 123 , 123 , 123 , 123 / .
then , for each 123 , algorithm lars - en produces the entire solution path of the elastic net .
the other tuning parameter ( 123 , s or k ) is selected by tenfold cv .
the chosen 123 is the one giving the smallest cv error .
for each 123 , the computational cost of tenfold cv is the same as 123 ols ts .
thus two - dimensional cv is computationally thrifty in the usual n > p setting .
in the p ( cid : 123 ) n case , the cost grows linearly with p and is still manageable .
practically , early stopping is used to ease the computational burden .
for example , suppose that n= 123 and p= 123; if we do not want more than 123 variables in the nal model , we may stop algorithm lars - en after 123 steps and consider only the best k within 123
from now on we drop the subscript of 123 if s or k is the other parameter .
prostate cancer example
the data in this example come from a study of prostate cancer ( stamey et al . , 123 ) .
the predic - tors are eight clinical measures : log ( cancer volume ) ( lcavol ) , log ( prostate weight ) ( lweight ) , age , the logarithm of the amount of benign prostatic hyperplasia ( lbph ) , seminal vesicle invasion
s = beta / max beta
s = beta / max beta
( a ) lasso estimates as a function of s and ( b ) elastic net estimates ( d 123 ) as a function of s : both estimates are piecewise linear , which is a key property of our efcient algorithm; the solution paths also
show that the elastic net is identical to univariate soft thresholding in this example ( , nal model selected )
( svi ) , log ( capsular penetration ) ( lcp ) , gleason score ( gleason ) and percentage gleason score 123 or 123 ( pgg123 ) .
the response is the logarithm of prostate - specic antigen ( lpsa ) .
ols , ridge regression , the lasso , the nave elastic net and the elastic net were all applied to these data .
the prostate cancer data were divided into two parts : a training set with 123 obser - vations and a test set with 123 observations .
model tting and tuning parameter selection by tenfold cv were carried out on the training data .
we then compared the performance of those methods by computing their prediction mean - squared error on the test data .
table 123 clearly shows that the elastic net is the winner among all the competitors in terms of both prediction accuracy and sparsity .
ols is the worst method .
the nave elastic net per - forms identically to ridge regression in this example and fails to do variable selection .
the lasso includes lcavol , lweight lbph , svi and pgg123 in the nal model , whereas the elastic net selects
table 123
prostate cancer data : comparing different methods
test mean - squared error
nave elastic net
= 123 , s= 123 = 123 , s= 123 : 123
zou and t .
hastie
lcavol , lweight , svi , lcp and pgg123
the prediction error of the elastic net is about 123% lower than that of the lasso .
we also see in this case that the elastic net is actually ust , because the selected is very big ( 123 ) .
this can be considered as a piece of empirical evidence supporting ust .
123 displays the lasso and the elastic net solution paths .
if we check the correlation matrix of these eight predictors , we see that there are some medium correlations , although the highest is 123 ( between pgg123 and gleason ) .
we have seen that the elastic net dominates the lasso by a good margin .
in other words , the lasso is hurt by the high correlation .
we conjecture that , whenever ridge regression improves on ols , the elastic net will improve the lasso .
we demonstrate this point by simulations in the next section .
a simulation study
the purpose of this simulation is to show that the elastic net not only dominates the lasso in terms of prediction accuracy but also is a better variable selection procedure than the lasso .
we simulate data from the true model
y= x+ " ,
" n . 123 , 123 / :
four examples are presented here .
the rst three examples were used in the original lasso paper ( tibshirani , 123 ) , to compare the prediction performance of the lasso and ridge regression systematically .
the fourth example creates a grouped variable situation .
within each example , our simulated data consist of a training set , an independent validation set and an independent test set .
models were tted on training data only , and the validation data were used to select the tuning parameters .
we computed the test error ( the mean - squared error ) on the test data set .
we use the notation = = to describe the number of observations in the training , validation and test set respectively , e . g .
123 / 123 / 123
here are the details of the four
( a ) in example 123 , we simulated 123 data sets consisting of 123 / 123 / 123 observations and eight predictors .
we let = . 123 , 123 : 123 , 123 , 123 , 123 , 123 , 123 , 123 / and = 123
the pairwise correlation between xi and xj was set to be corr . i , j / = 123 : 123|ij| .
( b ) example 123 is the same as example 123 , except that j = 123 : 123 for all j .
( c ) in example 123 , we simulated 123 data sets consisting of 123 / 123 / 123 observations and 123
predictors .
we set
( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) and = 123; corr . i , j / = 123 : 123 for all i and j .
( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) = . 123 ,
( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 )
( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 )
( d ) in example 123 we simulated 123 data sets consisting of 123 / 123 / 123 observations and 123 pre -
dictors .
we chose
( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) = . 123 ,
( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 )
and = 123
the predictors x were generated as follows :
xi = z123 + " x xi = z123 + " x xi = z123 + " x xi independent identically distributed ,
i , z123 n . 123 , 123 / , i , z123 n . 123 , 123 / , i , z123 n . 123 , 123 / ,
i= 123 , .
, 123 , i= 123 , .
, 123 , i= 123 ,
i= 123 , : : : , 123 ,
xi n . 123 , 123 / ,
table 123
median mean - squared errors for the simulated examples and four methods based on 123 replications
results for the following examples :
nave elastic net
the numbers in parentheses are the corresponding standard errors ( of the medians ) estimated by using the bootstrap with b= 123 resamplings on the 123 mean - squared errors .
i are independent identically distributed n . 123 , 123 : 123 / , i= 123 , : : : , 123 : in this model , we have three equally important groups , and within each group there are ve members .
there are also 123 pure noise features .
an ideal method would select only the 123 true features and set the coefcients of the 123 noise features to 123
table 123 and fig .
123 ( box plots ) summarize the prediction results .
first we see that the nave elastic net either has a very poor performance ( in example 123 ) or behaves almost identically to either ridge regression ( in examples 123 and 123 ) or the lasso ( in example 123 ) .
in all the examples , the elastic net is signicantly more accurate than the lasso , even when the lasso is doing much better than ridge regression .
the reductions in the prediction error in examples 123 , 123 , 123 and 123 are 123% , 123% , 123% and 123% respectively .
the simulation results indicate that the elastic net dominates the lasso under collinearity .
table 123 shows that the elastic net produces sparse solutions .
the elastic net tends to select more variables than the lasso does , owing to the grouping effect .
in example 123 where grouped selection is required , the elastic net behaves like the oracle .
the additional grouped selection ability makes the elastic net a better variable selection method than the lasso .
here is an idealized example showing the important differences between the elastic net and the lasso .
let z123 and z123 be two independent u . 123 , 123 / variables .
the response y is generated as n . z123 + 123 : 123z123 , 123 / .
suppose that we observe only
x123 = z123 + " 123 , x123 = z123 + " 123 ,
x123 =z123 + " 123 , x123 =z123 + " 123 ,
x123 = z123 + " 123 , x123 = z123 + " 123 ,
where " i are independent identically distributed n . 123 , 123=123 / .
123 observations were generated from this model .
the variables x123 , x123 and x123 form a group whose underlying factor is z123 , and x123 , x123 and x123 form a second group whose underlying factor is z123
the within - group correla - tions are almost 123 and the between - group correlations are almost 123
an oracle would identify the z123 - group as the important variates .
123 compares the solution paths of the lasso and the
microarray classication and gene selection
a typical microarray data set has thousands of genes and fewer than 123 samples .
because of the unique structure of the microarray data , we feel that a good classication method should have the following properties .
zou and t .
hastie
lasso enet ridge nenet
lasso enet ridge nenet
lasso enet ridge nenet
comparing the accuracy of prediction of the lasso , the elasic net ( enet ) , ridge regression and the nave elastic net ( nenet ) ( the elastic net outperforms the lasso in all four examples ) : ( a ) example 123; ( b ) example 123; ( c ) example 123; ( d ) example 123
( a ) gene selection should be built into the procedure .
( b ) it should not be limited by the fact that p ( cid : 123 ) n .
( c ) for those genes sharing the same biological pathway , it should be able to include whole
groups into the model automatically once one gene among them is selected .
from published results in this domain , it appears that many classiers achieve similar low classication error rates .
but many of these methods do not select genes in a satisfactory way .
most of the popular classiers fail with respect to at least one of the above properties .
the lasso is good at ( a ) but fails both ( b ) and ( c ) .
the support vector machine ( guyon et al . , 123 ) and penalized logistic regression ( zhu and hastie , 123 ) are very successful classiers , but they cannot do gene selection automatically and both use either univariate ranking ( golub et al . ,
table 123
median number of non - zero coefcients
results for the following examples :
s = beta / max beta
s = beta / max beta
( a ) lasso and ( b ) elastic net ( 123 d 123 : 123 ) solution paths : the lasso paths are unstable and ( a ) does not reveal any correction information by itself; in contrast , the elastic net has much smoother solution paths , while clearly showing the grouped selectionx123 , x123 and x123 are in one signicant group and x123 , x123 and x123 are in the other trivial group; the decorrelation yields the grouping effect and stabilizes the lasso solution
123 ) or recursive feature elimination ( guyon et al . , 123 ) to reduce the number of genes in the as an automatic variable selection method , the elastic net naturally overcomes the difculty of p ( cid : 123 ) n and has the ability to do grouped selection .
we use the leukaemia data to illustrate the elastic net classier .
the leukaemia data consist of 123 genes and 123 samples ( golub et al . , 123 ) .
in the training data set , there are 123 samples , among which 123 are type 123 leukaemia ( acute lymphoblastic leu - kaemia ) and 123 are type 123 leukaemia ( acute myeloid leukaemia ) .
the goal is to construct a diagnostic rule based on the expression level of those 123 genes to predict the type of leukae - mia .
the remaining 123 samples are used to test the prediction accuracy of the diagnostic rule .
to apply the elastic net , we rst coded the type of leukaemia as a 123 response y .
the classication
zou and t .
hastie
function is i . tted value > 123 : 123 / , where i . / is the indicator function .
we used tenfold cv to select the tuning parameters .
we used prescreening to make the computation more manageable .
each time that a model is tted , we rst select the 123 most signicant genes as the predictors , according to their t - statistic scores ( tibshirani et al . , 123 ) .
note that this screening is done separately in each training fold in the cv .
in practice , this screening does not affect the results , because we stop
leukaemia classication and gene selection by the elastic net ( d 123 : 123 ) : ( a ) the early stopping strategy at 123 steps nds the optimal classier with much less computational cost than ( b ) the whole elastic net paths; with early stopping , the number of steps is much more convenient than s , the fraction of l123 - norm , since computing s depends on the t at the last step of algorithm lars - en; the actual values of s are not available in tenfold cv ( + ) if the algorithm is stopped early; on the training set , 123 steps are equivalent to s d 123 : 123 (
table 123
summary of the leukaemia classication results
test error number of
support vector machingrecursive
penalized logistic regressionrecursive
nearest shrunken centroids
123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123 123
leukaemia dataelastic net coefcients paths ( up to kd 123 ) : the numbers on the top indicate the number of non - zero coefcients ( selected genes ) at each step; the optimal elastic net model is given by the
t at step 123 ( ) with 123 selected genes; note that the size of the training set is 123 , so the lasso can at most select 123 genes; in contrast , the elastic net selected more than 123 genes , not limited by the sample size; d 123 : 123 is chosen by tenfold cv; if a bigger is used , the grouping effect will be stronger
the elastic net path relatively early , at a stage when the screened variables are unlikely to be in
all the prescreening , tting and tuning were done using only the training set and the classi -
cation error is evaluated on the test data .
we stopped algorithm lars - en after 123 steps .
as can be seen from fig .
123 , using the num - ber of steps k in the algorithm as the tuning parameter , the elastic net classier ( = 123 : 123 and k = 123 ) gives a tenfold cv error of 123 / 123 and a test error of 123 / 123 with 123 genes selected .
123 displays the elastic net solution paths and the gene selection results .
table 123 compares the elastic net with several competitors including golubs method , the support vector machine , penalized logistic regression and the nearest shrunken centroid ( tibshirani et al . , 123 ) .
the elastic net gives the best classication , and it has an internal gene selection facility .
we have proposed the elastic net , a novel shrinkage and selection method .
the elastic net pro - duces a sparse model with good prediction accuracy , while encouraging a grouping effect .
the empirical results and simulations demonstrate the good performance of the elastic net and its superiority over the lasso .
when used as a ( two - class ) classication method , the elastic net appears to perform well on microarray data in terms of the misclassication error , and it does automatic gene selection .
zou and t .
hastie
although our methodology is motivated by regression problems , the elastic net penalty can be used in classication problems with any consistent ( zhang , 123 ) loss functions , including the l123 - loss which we have considered here and binomial deviance .
some nice properties of the elastic net are better understood in the classication paradigm .
for example , fig .
123 is a familiar picture in boosting : the test error keeps decreasing and reaches a long at region and then slightly increases ( hastie et al . , 123 ) .
this is no coincidence .
in fact we have discovered that the elastic net penalty has a close connection with the maximum margin explanation ( rosset et al . , 123 ) to the success of the support vector machine and boosting .
thus fig .
123 has a nice margin - based explanation .
we have made some progress in using the elastic net penalty in classication , which will be reported in a future paper .
we view the elastic net as a generalization of the lasso , which has been shown to be a valuable tool for model tting and feature extraction .
recently the lasso was used to explain the success of boosting : boosting performs a high dimensional lasso without explicitly using the lasso penalty ( hastie et al . , 123; friedman et al . , 123 ) .
our results offer other insights into the lasso , and ways to improve it .
