abstract .
we propose general procedures for posterior sampling from additive and generalized additive models .
the procedure is a stochastic generalization of the well - known backtting algorithm for tting addi - tive models .
one chooses a linear operator ( smoother ) for each predic - tor , and the algorithm requires only the application of the operator and its square root .
the procedure is general and modular , and we describe its application to nonparametric , semiparametric and mixed models .
key words and phrases : additive models , backtting , bayes , gibbs sam - pling , random effects , metropolishastings procedure .
in this paper we propose general procedures for posterior sampling from additive and generalized additive models .
the main idea evolves from the close relationship between the backtting algorithm for tting additive models , and the gibbs sampler for drawing realizations from a posterior distribu -
as an example , figure 123 shows the results of an additive model t to four air pollution variables , in a dataset with 123 observations .
the response variable is log ozone concentration .
the t is repre - sented by the solid curves in each of the panels and was obtained using cubic smoothing splines within the popular backtting algorithm .
also shown are posterior realizations from a bayesian version of the additive model .
the posterior realizations were pro - duced from a stochastic version of backtting , which we call bayesian backtting .
that is the central topic of this paper .
an additive model is a popular tool for modelling regression data .
it expresses the response variable as a sum of ( typically nonlinear ) functions of the predictor variables .
the backtting procedure is a modular way of tting an additive model .
it cycles through the predictors , replacing each current func -
trevor hastie is professor , department of statis - tics and division of biostatistics , stanford uni - versity , stanford , california 123 ( e - mail : trevor @stat . stanford . edu ) .
robert tibshirani is professor , departments of health research and policy and statistics , stanford university , stanford , california 123 ( e - mail : tibs@stat . stanford . edu ) .
tion estimate by a curve derived from smoothing a partial residual on each predictor .
the bayesian backtting procedure , introduced here , smooths the same partial residual and then adds appropriate noise to obtain a new realization of the current func - tion .
this is equivalent to gibbs sampling for an appropriately dened bayesian model .
in the important special case of an additive cubic smoothing spline model with n observations , we obtain an o ( cid : 123 ) n ( cid : 123 ) algorithm for sampling from the posterior .
this is not the rst such procedure : wong and kohn ( 123 ) derive an o ( cid : 123 ) n ( cid : 123 ) algorithm using the state - space representation of splines; see also carter and kohn ( 123 ) .
denison , mallick and smith ( 123 ) employ polynomial splines and back - tting in a bayesian additive model .
our proposal has the advantage of being conceptually simple , modular and general; it can be used with a wide range of operators representing nonparametric smoothers , as well as linear xed and random
we begin with an exposition of posterior sampling for cubic smoothing splines in section 123 and then discuss our general proposal for additive models ( section 123 ) and give an example involving growth curves .
in section 123 we discuss approaches for estimation of the variance components ( including bayes , empirical bayes , reml and ml ) , and how to choose appropriate priors .
the relationship with bootstrap sampling is briey explored in section 123
generalized additive models and the metropolis hastings procedure are discussed in section 123 , and we end with a discussion , including a description of a new public domain s - plus function for bayesian
fifty posterior realizations ( cid : 123 ) grey curves ( cid : 123 ) for an additive model t to four air - pollution variables ( cid : 123 ) the additive model tted functions are shown with thick ( cid : 123 ) dark curves ( cid : 123 ) the points are partial residuals from the posterior means and give an idea of the spread of the data available for each posterior sample ( cid : 123 )
posterior sampling for a
cubic smoothing spline
consider a scatterplot smoothing problem with data ( cid : 123 ) x123 ( cid : 123 ) y123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) x123 ( cid : 123 ) y123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) , ( cid : 123 ) xn ( cid : 123 ) yn ( cid : 123 ) .
here yi are the response values and xi are the inputs ( predictors ) .
we postulate a model
yi = f ( cid : 123 ) xi ( cid : 123 ) + i ( cid : 123 )
i n ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 )
the smoothing spline is a popular model for rep - resenting f ( cid : 123 ) x ( cid : 123 ) , and is usually derived as the mini - mizer of the penalized sum of squares criterion
( cid : 123 ) yi f ( cid : 123 ) xi ( cid : 123 ) ( cid : 123 ) 123 +
over all functions f ( cid : 123 ) x ( cid : 123 ) such that the integral exists .
the constant 123 is a tuning parameter , with larger values resulting in smoother curves .
the solution function f is a natural cubic spline , with knots at each of the unique values of xi
implies a representation
f ( cid : 123 ) x ( cid : 123 ) = m ( cid : 123 )
f = s ( cid : 123 ) ( cid : 123 ) y ( cid : 123 )
where the m n basis functions bj represent the linear space of such functions .
letting y = ( cid : 123 ) y123 ( cid : 123 ) y123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) yn ( cid : 123 ) t , f = ( cid : 123 ) f ( cid : 123 ) x123 ( cid : 123 ) ( cid : 123 ) f ( cid : 123 ) x123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) f ( cid : 123 ) xn ( cid : 123 ) ( cid : 123 ) t , the tted values at the n input values xi can be written as here s ( cid : 123 ) ( cid : 123 ) is a symmetric n n operator matrix , called the smoother matrix .
it depends on the val - ues xi and the tuning parameter , but not on y .
we will sometimes write it simply as s .
it is also possible to estimate in an adaptive ( nonlinear ) manner , depending on the response y , but we do not consider that here .
most smoothers , and in par - ticular smoothing splines , give a prediction at any values of x , not just the ones in the dataset , since they produce a function f .
hastie and r .
tibshirani
there is a bayesian characterization of
it is often convenient to parametrize the smooth - ing spline in terms of this tted vector f rather than in ( 123 ) .
this is an equivalent representation ( green and silverman , 123 ) , since f = b , where b is the full - rank n m basis matrix evaluated at the n val - ues of xi .
an advantage is that one obtains expres - sions that immediately suggest generalizations to other smoothing methods .
choosing a particular partially improper gaussian prior for f , the resulting posterior distribution of f has the form with = 123 / 123
hence the smoothing spline is the mean of the posterior distribution .
often it is con - venient to parametrize s ( cid : 123 ) ( cid : 123 ) using df ( cid : 123 ) ( cid : 123 ) = tr s ( cid : 123 ) ( cid : 123 ) , the effective degrees of freedom , which is monotone in .
we have assumed that 123 , 123 and hence , are
f ( cid : 123 ) y n ( cid : 123 ) s ( cid : 123 ) ( cid : 123 ) y ( cid : 123 ) s ( cid : 123 ) ( cid : 123 ) 123 ( cid : 123 )
f n ( cid : 123 ) 123 ( cid : 123 ) k123 ( cid : 123 ) ( cid : 123 )
here and elsewhere , the notation k refers to a generalized inverse of a matrix k , with the under - standing that an eigenvalue of zero for k gives an eigenvalue of + for k .
in the case of smooth - ing splines and the parametrization f , k computes ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) 123 dx = f tkf , and the the penalty in ( 123 ) : zero eigenvectors correspond to linear functions of x .
the prior therefore gives innite variance to linear functions ( is vague ) , and hence they are unrestricted .
more details on k for splines are given in appendix a , as well as hastie and tibshi - rani ( 123 ) .
more generally , for symmetric smoother operators s ( cid : 123 ) ( cid : 123 ) we can identify a prior covariance where s indicates a generalized matrix inverse .
see buja , hastie and tibshirani ( 123 ) for more
k = ( cid : 123 ) s ( cid : 123 ) ( cid : 123 ) i ( cid : 123 ) ( cid : 123 )
in this paper our interest is not just the mean but the entire posterior distribution of f given in ( 123 ) .
throughout the paper we use the notation z = ( cid : 123 ) z123 ( cid : 123 ) z123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) zn ( cid : 123 ) t to represent a vector of indepen - dent n ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) variates .
notice that ( 123 ) can be written
f = sy + s123 / 123z ( cid : 123 )
where we have dropped the dependence on .
there - fore we can generate a posterior realization of f by adding the noise s123 / 123z to the tted smoothing spline .
the quantity s123 / 123z can be generated ef - ciently with the same order of computations as sy , typically o ( cid : 123 ) n ( cid : 123 ) .
in appendix a , we give two algorithms for this , one exclusively for smoothing
los angeles air pollution data : upland maximum ozone vs daggot pressure gradient .
shown 123 realizations from the posterior distribution f ( cid : 123 ) y ( cid : 123 ) with the smoothing parameter xed at df = 123 ( cid : 123 ) the smoothing - spline ( cid : 123 ) posterior mean ( cid : 123 ) is shown with a thick ( cid : 123 ) dark curve ( cid : 123 ) included are the pointwise 123% posterior intervals , computed exactly ( cid : 123 )
splines and the other for general smoothing oper - ators .
although ( 123 ) is derived for cubic smoothing splines , by analogy we can use it for any smooth - ing operator , even nonlinear smoothers .
once again , expression ( 123 ) can be used to generate pos - terior realizations at any arbitrary input values t123 ( cid : 123 ) t123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) tm , including ones not in the dataset .
figure 123 shows an example .
the response vari - able represents ozone measurements on 123 days in the los angeles basin , and the predictor vari - able is the pressure gradient measured at the dag - got airport .
the gure shows 123 realizations of the posterior distribution , using a cubic smoothing spline with a xed number ( df = 123 ) degrees of free - dom , and xed at the unbiased estimate 123 =
cid : 123 ) yi yi ( cid : 123 ) 123 / ( cid : 123 ) n df ( cid : 123 ) .
we used a burn - in period of
123 iterations .
convergence issues for markov chain monte carlo methods are important , but there is insufcient space to address here .
see , for example , gelman , stern and rubin ( 123 ) for a general dis - cussion and references .
the gure suggests that the variance of log ozone is not constant as a function of daggot pressure gradient .
an appropriate trans - formation of the response might help alleviate this , but we do not pursue that here .
the gure includes pointwise 123% posterior bands , which can in fact be computed exactly from the diagonal of s 123 ( also in o ( cid : 123 ) n ( cid : 123 ) operations ) .
while they show the shadow of the posterior distribution , they do not show the individual realizations .
in section 123 we make use of the individual real - izations , and display the posterior distributions of
interesting functionals of them .
here the smoothing parameter is xed at df ( cid : 123 ) ( cid : 123 ) = 123; in section 123 we show how to incorporate priors for the smoothing parameters and 123
notice that adding noise sz in ( 123 ) would give posterior covariance s123 , which is not the same as s 123 since s is not idempotent .
in fact , s123 123 is the frequentist covariance of sy , and s 123 s123 123 : the posterior covariance exceeds the frequentist covari - ance because it incorporates prior uncertainty .
there is a version of result ( 123 ) for simple linear
and multiple regression .
suppose
yi = xi + i ( cid : 123 )
i n ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 )
letting x = ( cid : 123 ) x123 ( cid : 123 ) x123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) xn ( cid : 123 ) t , the posterior distri -
bution of xt is
xt ( cid : 123 ) y n ( cid : 123 ) h ( cid : 123 ) 123 ( cid : 123 ) y ( cid : 123 ) h ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 )
h ( cid : 123 ) 123 ( cid : 123 ) = x ( cid : 123 ) xtx + i / 123 ( cid : 123 ) 123xt
with i being the nn identity matrix .
as for general smoothers we can write the posterior realizations as
xt = h ( cid : 123 ) 123 ( cid : 123 ) y + h ( cid : 123 ) 123 ( cid : 123 ) 123 / 123z ( cid : 123 )
taking to represent prior ignorance , then h ( cid : 123 ) 123 ( cid : 123 ) h ( cid : 123 ) ( cid : 123 ) = x ( cid : 123 ) xtx ( cid : 123 ) 123xt = h , the hat matrix .
the operator h is an idempotent projection matrix and h123 / 123 = h , so that the posterior realizations can be written in the simpler form ,
xt = hy + hz ( cid : 123 )
additive models and
we now consider the main topic of this paper , bayesian posterior sampling for additive models .
our data consists of n observations of an out - come variable and p inputs : we write this as ( cid : 123 ) x123 ( cid : 123 ) y123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) x123 ( cid : 123 ) y123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) xn ( cid : 123 ) yn ( cid : 123 ) with xi = ( cid : 123 ) xi123 ( cid : 123 ) xi123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) xip ( cid : 123 ) .
our model is
i n ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 )
yi = + p ( cid : 123 ) fj ( cid : 123 ) xij ( cid : 123 ) + i ( cid : 123 )
for identiability between and the fj ( cid : 123 ) j > 123 we
fj ( cid : 123 ) xij ( cid : 123 ) = 123
suppose we dene a cubic smoothing spline opera - tor sj ( cid : 123 ) j ( cid : 123 ) for each input variable j .
then the back - tting procedure for estimating the fjs uses itera - tions of the form
y y123 ( cid : 123 )
for j = 123 ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) p ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) .
at each stage , the most current values of the functions fk are used on the right - hand side , forming a partial residual that is smoothed as a function of xj .
rather than obtain estimates of the fj , which in bayesian language means to compute the map esti - mates ( here , the posterior means ) we want to gen - erate from their joint distribution .
to achieve this we simply add the appropriate noise to the estimate at each backtting step .
for ease of notation dene f123 = 123 and the associated operator s123 = 123t / n .
recall that the variance 123 is considered to be xed .
we dene the bayesian backtting algorithm as fol -
algorithm 123 .
bayesian backtting .
take initial values for f 123 do for t = 123 ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) :
j ( cid : 123 ) j 123
do for j = 123 ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) p :
* dene the partial residual k>j f t123
* generate zt
k<j f t j n ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) and update j + s123 / 123
until the joint distribution of ( cid : 123 ) f t
123 ( cid : 123 ) f t
123 ( cid : 123 ) f t
123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) f t
the most appropriate starting values in the rst step are the tted curves from a standard additive model t to the data .
at the end of the procedure , the phrase doesnt change means that the proce - dure has converged to an appropriate stationary dis - tribution .
convergence may be checked in practice in a number of ways; see , for example , the discus - sion in gelman et al .
( 123 ) .
bayesian backtting is the gibbs sampling pro - cedure applied to additive models .
gibbs sampling ( geman and geman , 123; gelfand and smith , 123 ) for general random variables a123 ( cid : 123 ) a123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ap operates by successive sampling of each aj condi - tional on the other ak .
at steady state a complete cycle delivers a sample from their joint distribu - tion .
the connection between bayesian backtting and gibbs sampling is established by examining the conditional distribution of each fj .
for cubic smoothing splines this connection is clear from the above development , but we give a more general result for a larger class of operators sj .
let sj be any symmetric matrices with eigen - values in ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) .
dene priors on the fj by fj j i ( cid : 123 ) are sym - metric with eigenvalues in ( cid : 123 ) 123 ( cid : 123 ) + ( cid : 123 ) and hence are nonnegative denite .
consider 123 to be xed ( as well
j i ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 ) the matrices ( cid : 123 ) s
hastie and r .
tibshirani
as the smoothing parameter implicit in sj ) .
then fk ( cid : 123 ) ( cid : 123 ) fk ( cid : 123 ) k ( cid : 123 ) = j ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) fj ( cid : 123 ) y ( cid : 123 ) fk ( cid : 123 ) k ( cid : 123 ) = j ( cid : 123 ) = ( cid : 123 )
( cid : 123 ) sj 123
hence bayesian backtting corresponds to sampling from the conditional distribution of each fj .
by the results in tierney ( 123 ) , the joint distribution of the iterates ( cid : 123 ) f123 ( cid : 123 ) f t p ( cid : 123 ) convergences to that of the true distribution of ( cid : 123 ) f123 ( cid : 123 ) f123 ( cid : 123 ) f123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) fp ( cid : 123 ) ( cid : 123 ) y .
further - more , sample averages of functions of these quanti - ties converge to their true values .
this holds since the conditional densities are everywhere positive and hence the markov chain is ergodic .
123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) f t
123 ( cid : 123 ) f t
figure 123 shows 123 realizations for the addi - tive model t to four air pollution variables .
we xed the degrees of freedom of the smoothers at 123 ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) 123 , which are the values obtained from generalized cross - validation using an adap - tive backtting procedure ( hastie and tibshirani , 123 , chapter 123 : the bruto procedure ) .
from this information we can form posterior bands for the functions or carry out bayesian inference for any other quantity of interest .
recall the additive model constraints ( 123 ) .
these are necessary to ensure that the posterior distri - bution of and the fj is not singular .
practically speaking , it means that in the bayesian backtting algorithm , we have to center the ts after smoothing and generation .
we discuss this and more sophisti - cated decorrelation procedures in appendix a .
the standard backtting algorithm is a general , modular method for tting a wide variety of addi - tive models .
one chooses the smoother operator sj for each input , and then backts to estimate the joint model .
the operator sj can t a exi - ble smooth , a linear regression ( including dummy variable ts ) , an adaptive regression ( e . g . , wavelet smoother ) , and , more generally , any regression operator .
convergence has only been proved for a certain class of xed , nonadaptive operators ( buja , hastie and tibshirani , 123 ) , such as smoothing splines , but the algorithm seems well behaved in
in the same way , we can choose an operator sj for each input , and then paste them together as condi - tional sampling steps of the form fj sjrj + s123 / 123
in the bayesian backtting algorithm ( see appen - dix a for a general procedure for computing s123 / 123z . ) given a single input xj = ( cid : 123 ) x123j ( cid : 123 ) x123j ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) xnj ( cid : 123 ) t , we summarize some of the possibilities for choice of sj :
smoothing splines
computes a cubic smoothing spline .
the conditional sampling step corresponds to the gaussian process prior fj n ( cid : 123 ) 123 ( cid : 123 ) 123k can be com - puted in o ( cid : 123 ) n ( cid : 123 ) operations , the latter discussed in appendix a .
j ( cid : 123 ) .
both sj and s123 / 123
fixed linear effects .
sj = xj ( cid : 123 ) xt
we simply obtain n ( cid : 123 ) ave ( cid : 123 ) y p
general nonparametric smoother .
sj denes the smoothing operation , with implicit prior j i ( cid : 123 ) 123 ( cid : 123 ) .
the operator s123 / 123 applied using algorithm a . 123 in the appendix .
where xj is a matrix consisting of one or more predictors .
this results from the model fj = xjj with j n ( cid : 123 ) 123 ( cid : 123 ) 123d ( cid : 123 ) and d diago - j = sj and is easily nal , and .
then s123 / 123 applied .
for the intercept term , for example , 123 fj ( cid : 123 ) ( cid : 123 ) 123 / n ( cid : 123 ) .
j xj + with j n ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 ) .
algorithms for implement - ing these random effects smoothers are very similar to those used in smoothing splines , which we discuss in appendix a .
we look more closely at a special case in the mixed effects
random linear effects .
sj = xj ( cid : 123 ) xt
this results from fj = xt
123 example : growth curves
the data in the top left panel of figure 123 are measurements of spinal bone mineral density for a sample of 123 girls , as a function of age .
there are between two and four measurements per girl , 123 in all .
the consecutive data for each girl are connected in the plot .
we see a great deal of between - girl vari - ation , and a clear indication of the growth spurt around age 123
there is also a strong ethnic effect that is hidden in the variation of the growth frag - ments .
a goal is to characterize the growth behavior and establish whether ethnic differences exist .
yij = f ( cid : 123 ) tij ( cid : 123 ) + xt
we consider the mixed effects model : i e + vi + ij ( cid : 123 )
yij is the bone mineral density for girl i mea - sured on occasion j , for i = 123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) 123 , and j = 123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ni with ni ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) .
f ( cid : 123 ) tij ( cid : 123 ) is the population growth curve as a func - tion of the age measurement tij made on girl i on e is an effect due to ethnic class; the data consist of white , black , asian and hispanic north american girls .
xi is any appropriate coding of con - trasts to represent the 123 - level factor .
vi is a random girl effect that allows a separate vertical shift in f .
the top left panel contains 123 measurements of bone mineral density against age for 123 girls of different ethnic origin ( cid : 123 ) repeated measurements are connected ( cid : 123 ) the remaining three panels show 123 posterior realizations from model ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 ) in the nal panel ( cid : 123 ) each random effect distribution is plotted against the mean age for that girl ( cid : 123 )
ij is measurement and other variation , which we assume to be i . i . d .
a standard frequentist approach for tting such data would be to treat f and the parameters in e as parametric xed effects , and vi as a ran - dom effect .
one could model f by polynomials or more exibly by splines with selected knots in age .
typically one assumes the vi n ( cid : 123 ) 123 ( cid : 123 ) 123 independently across girls , and ij n ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) inde - pendently across all measurements .
estimation of these mixed effects models is typically done by maximum likelihood ( laird and ware , 123 ) , and
the parameters of the xed effects and their the variance components 123 the posterior mean or blup estimates of the
v and 123 ,
here we take a bayesian approach , and treat everything as random .
we treat f as random , and
use the smoothing spline process prior .
we also treat the coefcients e as random , but with a at prior .
for the moment we assume the variance com - ponents are xed ( df 123 , v 123 ( cid : 123 ) 123 and 123 ( cid : 123 ) 123 ) , and focus on generating realizations from the joint posterior .
in section 123 we describe a variety of methods for estimating the variance components as well , including the reml method and the fully bayesian procedure that was used here .
the set - up is tailor - made for the bayesian backt - ting procedure .
the random effects have conditional vi ( cid : 123 ) y ( cid : 123 ) f ( cid : 123 ) e n
ni + v
ni + v i e and v = 123 / 123
where rij = yij f ( cid : 123 ) tij ( cid : 123 ) xt the remaining three panels in figure 123 show 123 realizations from the model .
the 123 posterior real - izations for the random effects are shown vertically in the last panel , centered at the average age for that girl .
figure 123 focuses on the posterior realiza -
hastie and r .
tibshirani
gcv ( wahba , 123 ) , or related methods aimed at minimizing prediction error on future observations .
we give more details on the rst two of these ( in
123 reml , ml and empirical bayes
model ( 123 ) can be regarded as an hierarchical mixed effects model .
the function f is random at level 123 ( a single coefcient vector ) , while the vi are random at level 123 ( a coefcient per cluster ) .
mixed effects models are typically t by maximum likelihood or reml ( laird and ware , 123 ) , and the popular packages such as sas and splus have rou - tines for tting them .
maximum likelihood provides estimates of the variance components , 123 , 123 v and 123 in this case , the parameters of the xed effects , and the blup or posterior mean estimates e ( cid : 123 ) f ( cid : 123 ) ( cid : 123 ) yij ( cid : 123 ) ( cid : 123 ) and e ( cid : 123 ) vi ( cid : 123 ) ( cid : 123 ) yij ( cid : 123 ) ( cid : 123 ) of the random effects .
restricted maximum likelihood ( reml ) is a slight modica - tion which takes into account the degrees of freedom used in estimating the xed effects , when estimat - ing the variance components ( like the n 123 versus n correction in the sample variance . )
the empirical bayes approach is to form the marginal likelihood by integrating out everything random and then estimating the remaining hyper - parameters by maximum likelihood .
it turns out , that if the xed effects are given a at prior , then empirical bayes is equivalent to reml ( laird and
treating smoothing splines as random effects and estimating 123 by reml is not a new idea ( speed , 123 ) , also known as gml in the spline literature ( wahba , 123 ) .
lin and zhang ( 123 ) in fact use reml in this way to estimate the smoothing param - eters for additive spline models .
their approach is to represent the functions as fj = pjj with j ji ( cid : 123 ) , and treat the pj as a block of regression variables with random coefcients j .
the number of columns in pj is mj 123 , where mj is the number of unique elements of xj .
in general their algorithm j mj ( cid : 123 ) n ( cid : 123 ) 123 ( cid : 123 ) computations , and so defeats our purposes here of efciency .
a promising alterna - tive is to approximate pjj by p a xed number ( 123 or 123 ) columns , for the purpose of estimating the variance components efciently .
approximations of this kind , based on the leading eigenvectors of k , are developed in hastie ( 123 ) ( but are put to different uses there ) .
j , where p
123 priors for the variance components
a more mainstream bayesian approach would be to provide priors for the variance components and integrate .
wong and kohn ( 123 ) suggest the fol -
one hundred posterior realizations of the derivative of f ( cid : 123 ) included in the plot are the distributions of two functionals ( cid : 123 ) the location of the maximum and the location of the point at which growth is less than 123 ( cid : 123 ) 123% per year ( cid : 123 )
tions for f .
since each realization is a natural cubic spline , we are easily able to produce the derivatives for each curve ( in which the natural boundary con - ditions are evident ) .
these are displayed , along with the posterior distributions of two functionals :
the location of the maximum , which is the age at which the growth velocity is fastest .
this distribution is fairly tightly concentrated at 123 years old .
the location of the point at which bone growth increase levels off .
we have used a threshold of 123 , which corresponds to 123% per annum .
this distribution is rather spread out; indeed , the derivative posterior is rather wiggly in this region .
estimating the variance components
in the preceding development , the smoothing j , j = 123 ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) p , parameters or variances 123 and 123 were considered xed .
in practice they have to be determined as well .
there are several approaches : the full bayesian approach , where we put pri - ors on the variance components and estimate their posterior along with the functions; the empirical bayes approach , that treats the variance components as parameters , which are esti - mated by maximum ( marginal ) likelihood .
the frequentist approach , that treats every - thing as a smoother or regression tting method , including the random effects operators .
all the parameters are then estimated by cross - validation ,
p ( cid : 123 ) 123 ( cid : 123 ) 123
j ( cid : 123 ) with j = 123 ( cid : 123 )
these priors are almost indistinguishable .
the prior for 123 makes the prior for log ( cid : 123 ) 123 ( cid : 123 ) at .
the prior for j is almost at and still improper , and we give some insight into the additional term involving j later in this section .
both these priors are conjugate for the gaussian distribution and lead to inverse gamma posterior distributions .
more generally , one can use proper inverse gamma priors
for which both the above are degenerate special
j enters the model only through fj , the
corresponding conditional distributions are j ( cid : 123 ) y ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 ) fj ( cid : 123 ) j = 123 ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) p ( cid : 123 ) ( cid : 123 )
j kjfj + j
123 ( cid : 123 ) ( cid : 123 ) e ( cid : 123 ) ( cid : 123 ) 123 + ( cid : 123 ) where e = y
this is an inverse gamma distribution ig ( cid : 123 ) n / 123 + j kjfj + j ( cid : 123 ) .
similarly the conditional distribution of 123 is ig ( cid : 123 ) n / 123 + r ( cid : 123 ) 123 j fj .
to generate from the full posterior distribution , we include conditional sampling steps for the 123 123 in the bayesian backtting algorithm .
theoret - ical convergence of the procedure to stationarity is unaffected .
note that calculation in ( 123 ) of j kfj f t j fj requires no new computation besides inner products , since j fj = rt j z + 123ztz , and sjrj
j i ( cid : 123 ) fj = f t
j sjrj + 123rt
j z are already available .
j fj f t
the posterior realizations of the air - pollution functions do not look any different from those in figure 123 , so we do not repeat them here .
the real - izations produced for the bone data in figures 123 and 123 were obtained in the manner just described .
figure 123 shows the posterior distributions of the degrees of freedom dfj from bayesian backtting , both for the air pollution data and the bone - growth data .
the degrees of freedom are a one - to - one func - tion of j = 123 / 123 j : df ( cid : 123 ) ( cid : 123 ) = tr s ( cid : 123 ) ( cid : 123 ) .
estimated optimal values from generalized cross - validation ( gcv ) are indicated by horizontal broken lines for the air pollution data .
compared to gcv , the fully
bayesian procedure applies slightly less smooth - ing ( more degrees of freedom ) to inversion base temperature , but the t does not change much .
for the lower two panels , the horizontal lines indi - cate the df chosen by reml , which appear to match these posterior realizations more closely .
since the between - girl variation , 123 v , is very large compared to the within - girl 123 , not much shrinking is done from the maximum of 123 df for the v effect .
for the remainder of this section , we investigate the priors ( 123 ) and ( 123 ) in terms of the prior degrees of freedom and develop a more general framework for any smoother .
one might ask what the implicit prior is for df , given particular priors on 123 and 123
holmes and mallick ( 123 ) and hodges and sargent ( 123 ) similarly investigate priors based on degrees of freedom .
figure 123 shows the implied prior dis - tributions for df for two commonly used priors on the variance components , obtained by simulation .
the x - values are taken to be 123 uniformly spaced observations on ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) .
for any given pair 123 and 123 , we compute = 123 / 123 , and df ( cid : 123 ) ( cid : 123 ) = tr s ( cid : 123 ) ( cid : 123 ) , where s ( cid : 123 ) ( cid : 123 ) is the smoothing spline operator applied to the 123 values of x .
notice that if k = udut is the eigen - decomposition of k in s ( cid : 123 ) ( cid : 123 ) = ( cid : 123 ) i+k ( cid : 123 ) 123 , then j=123 123 / ( cid : 123 ) 123 + dj ( cid : 123 ) and can be computed ef -
ciently for different values of .
using the prior ( 123 ) for both p ( cid : 123 ) 123
in the left panel , we have used at improper pri - ors ( 123 ) for both log 123 123 , log 123 123
the prior for df puts mass 123 123 on 123 and 123 , the linear and interpo - lating ts ! this is easily proved ( see appendix ) , and has some negative consequences on the gibbs sam - pler .
it implies that these two states are absorbing , and hence the real posterior would end up in one of these states as well .
j ( cid : 123 ) ( cid : 123 ) 123 / 123 j ( cid : 123 ) and likewise for 123 , one sees exactly the same behavior .
this prior is still improper , but the presence of = 123 appears to prevent the absorptions at the two extreme states .
in the right panel , we use ig ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) , con - sidered to be reasonably at proper priors in the mcmc literature ( spiegelhalter , best , gilks and inskip , 123 ) .
the histogram was obtained by sim - ulating 123 ( cid : 123 ) 123 values from these priors .
it exhibits very similar behavior to the rst , although it appears there is support everywhere .
a gibbs sam - pler , starting at some value of df in this at interior immediately concentrates the posterior away from the boundaries and appears not to run into trouble .
in all cases the strong u - shape is troublesome and does not seem very sensible as a prior for df .
after some experimentation , we found that priors 123
hastie and r .
tibshirani
top four panels ( cid : 123 ) 123 posterior realizations of the df for each predictor for the air pollution data ( cid : 123 ) estimated optimal values from generalized cross - validation are indicated by horizontal broken lines ( cid : 123 ) lower two panels ( cid : 123 ) 123 posterior realizations of df for the age curve and the random effect v for the bone growth data ( cid : 123 ) ig ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) and 123 ig ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) gave a reasonable prior for df , without the right spike ( figure 123 ) .
range of df .
since df is monotone with , a mea - sure of noise - to - signal ratio , it is quite reasonable to generate these independently of each other .
here is an alternative strategy that one might use for prior selection .
one could use the usual prior for 123 , but then pose a prior p ( cid : 123 ) df ( cid : 123 ) for df itself , rather than indirectly through 123 , and avoid the rather strange right tail behavior .
this prior might put more mass on smoother models than rough ( as in figure 123 ) , or might itself be at over the entire
the posterior ( 123 ) is expressed in terms of k , the penalty matrix for a smoothing spline or sim - ilar smoother , which is based on a prior covariance 123k for f .
since k ( cid : 123 ) ( cid : 123 ) = ( cid : 123 ) s ( cid : 123 ) ( cid : 123 ) i ( cid : 123 ) = k , and parametrizing the smoother through df rather than , we get an equivalent representation for the prior
left panel ( cid : 123 ) the prior distribution of df based on a at improper prior for log 123 and log 123 ( cid : 123 ) right panel ( cid : 123 ) the prior for df based on fairly noninformative proper priors ig ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) prior for 123 and 123 ( cid : 123 )
the joint posterior distribution is now
p ( cid : 123 ) f ( cid : 123 ) i ( cid : 123 ) vi ( cid : 123 ) e ( cid : 123 ) y ( cid : 123 )
( cid : 123 ) yij f ( cid : 123 ) tij i ( cid : 123 ) xt
i e vi ( cid : 123 ) 123
the implied prior for df based on 123 ig ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) and 123 ig ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 )
covariance : 123k = 123k ( cid : 123 ) df ( cid : 123 ) = 123 ( cid : 123 ) s ( cid : 123 ) ( cid : 123 ) i ( cid : 123 ) .
the posterior distribution for dfj is then
p ( cid : 123 ) dfj ( cid : 123 ) y ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 ) fj ( cid : 123 ) j = 123 ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) p ( cid : 123 ) ( cid : 123 )
123 example : growth curves continued
the bone growth model
in section 123 assumes that each girl has her growth spurt at the same age .
there is some evidence of the deciency of this model in the lower right panel of figure 123 , where the random effects distributions seem to have larger variance 123 v around age 123
here we consider a richer model , that attempts to correct for
yij = f ( cid : 123 ) tij i ( cid : 123 ) + xt
i e + vi + ij ( cid : 123 )
the parametrization is the same as before , except we introduced an additional random effect , the age shift i , which we assume has distribution n ( cid : 123 ) 123 ( cid : 123 ) 123
up to a constant and the components of variance ( cid : 123 ) v ( cid : 123 ) f and .
the prior of e is at .
we rst produce the map estimates for all the random and xed effects .
this is a large penalized nonlinear least squares problem .
we have introduced an additional variance com - ponent .
in practice this needs to be estimated as well , either via empirical or full bayes methods .
for expediency , we selected = 123 ( cid : 123 ) 123 based on a crude grid search and the bic statistic and base our sub - sequent analysis on this value .
we alternate between : 123
fitting all the parameters holding the i xed .
this includes the variance components , as well as the map estimates of vi and f and .
fixing all the parameters in step ( i ) , and com -
puting the map estimates of the i .
the rst step ( i ) requires exactly the same technol - ogy as in section 123 .
the alternating procedure requires initial val - ues for the i .
ignoring the vi and xed effects , we can produce an approximate collapsed version of the
yi = f ( cid : 123 ) i ( cid : 123 ) + i ( cid : 123 ) ti = i + i ( cid : 123 )
where yi represents an average of all the y val - ues for subject i , and so on .
this has the form of a nonlinear errors - in - variables model , and can be
hastie and r .
tibshirani
the map estimates of the nonlinear random effects model ( cid : 123 ) the top left panel shows a principal curve t to the reduced data ( cid : 123 ) from which initial estimates of i were obtained ( cid : 123 ) the top right panel shows the estimate of f ( cid : 123 ) along with the overall residuals ij ( cid : 123 ) the lower left panel shows the estimated random effects vi and the lower right the estimated i ( cid : 123 ) the latter are far more variable around the estimated by the principal curves algorithm ( hastie and stuetzle , 123 ) .
figures 123 and 123 show the map estimates and illustrate the effect of the inclusion of the ran - dom age shift effects i on the tted random bmd
this sample modication to the model has made it quite nonlinear , and in particular the joint pos - terior of ( cid : 123 ) i ( cid : 123 ) vi ( cid : 123 ) will depend on where the obser - vations lie .
we do not expect to learn much about i if the growth spurt is over and expect the pos - terior distributions to look much like the prior .
the area where we can learn something is at the ear - lier ages , where large deviations are attributable to both horizontal and vertical shifts .
the gibbs simpler for xed values of i proceeds exactly as before .
the only difcult part is sampling from the posterior for i given the rest .
the poste -
( cid : 123 ) rij f ( cid : 123 ) tij i ( cid : 123 ) ( cid : 123 ) 123
where rij = yij xte vi .
this is a univariate simulation problem , and we resort to a simple tay - lor approximation to f in ( 123 ) ,
f ( cid : 123 ) tij i ( cid : 123 ) f ( cid : 123 ) tij i ( cid : 123 ) f ( cid : 123 ) ( cid : 123 ) tij i ( cid : 123 ) ( cid : 123 ) i i ( cid : 123 ) ( cid : 123 )
where i is the previous realization of i .
we let aij = f ( cid : 123 ) tij i ( cid : 123 ) , bij = f ( cid : 123 ) ( cid : 123 ) tij i ( cid : 123 ) and uij = bij aij rij , and after some simple calculations we nd
ij + 123 / 123
ij + 123 / 123
the top left panel shows a single average age curve ( cid : 123 ) along with the various shifted versions obtained by adjusting for individual values of i ( cid : 123 ) the top right panel is similar ( cid : 123 ) except the adjustments are now vertical shifts caused by the estimated random effects vi ( cid : 123 ) the lower left panel shows the movement of the vi when the i are included in the model ( cid : 123 )
figure 123 ( left gure ) shows the joint distribution of ( cid : 123 ) i ( cid : 123 ) vi ( cid : 123 ) for nine particular values of i .
the right gure show the original data , with the four map curves for each ethnic class and with the data for the nine values of i indicated .
relationship to bootstrap sampling
there is a close relation between bayesian back - tting for additive models and the bootstrap applied to standard backtting procedure .
assume for simplicity that 123 is known .
in the standard backtting algorithm with smoothers sj , the tting values y and functions fj satisfy y = ay , fj = ajy where the matrices a and aj are functions of sj , j = 123 ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) p .
it can be shown that the bayes posterior functions have marginal distributions ,
fj n ( cid : 123 ) ajy ( cid : 123 ) ( cid : 123 ) i aj ( cid : 123 ) sj ( cid : 123 ) i sj ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 )
on the other hand , suppose we carry out para - metric bootstrap sampling by adding residuals r n ( cid : 123 ) 123 ( cid : 123 ) i 123 ( cid : 123 ) to the t y = ay giving responses y = ay + r .
we then apply standard backtting to the data y , giving
in the simple case of only one function ( p = 123 ) , we have aj = a = sj , and the bayesian and bootstrap distributions are n ( cid : 123 ) sjy ( cid : 123 ) sj 123 ( cid : 123 ) and n ( cid : 123 ) s123 j < sj for cubic spline smoothers and many other smoothers , but typically the two are not very
j = ajy = aj ( cid : 123 ) ay+ r ( cid : 123 ) n ( cid : 123 ) ajay ( cid : 123 ) a123
in the general case with p functions , the boot - strap mean ajay is what we obtain if we apply backtting twice : once to y to obtain ay and then again to the response ay .
hence it will tend to be smoother than ( but similar to ) the bayesian mean ay .
the bayesian covariance matrice reduces to sj 123 in the orthogonal case , that is , the inputs are
hastie and r .
tibshirani
average standard deviation for bayes posterior
f123 and bootstrap realization f
model , reducing the effect of collinearity in the pos - terior .
this interesting issue deserves further study .
generalized additive models
hastie and tibshirani ( 123 ) introduced the gen - eralized additive model for modeling non - gaussian data .
this includes members of the exponential fam - ily of distributions and other models such as the proportional hazards model for survival data .
for a bayesian analysis of this model , the conditional distributions do not have a simple form in general , as they do in the gaussian case .
hence gibbs sam - pling is no longer convenient .
however the basic gibbs step fj = sjrj + s123 / 123 j z can instead be used as a proposal distribution in a metropolishastings algorithm , as we outline below .
we rst give some background on generalized additive models .
in the exponential mean i of the response variable yi is assumed to be related to the inputs via
i g ( cid : 123 ) i ( cid : 123 ) = ( cid : 123 )
where g ( cid : 123 ) ( cid : 123 ) is a specied function , known as the link function .
the functions fj ( cid : 123 ) ( cid : 123 ) are estimated by max - imizing a penalized log - likelihood analogous to the penalized least squares criterion used for gaussian additive models .
using vector notation , this crite - rion has the form
j ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) = log l ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 )
jf tkjf ( cid : 123 )
the function l ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) is the likelihood of the data , j fj , ( cid : 123 ) = ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) p ( cid : 123 ) 123 ( cid : 123 ) , the tuning parame - ters and kj is the penalty matrix , as dened previ - ously .
the local scoring algorithm for maximization of j ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) , proposed in hastie and tibshirani ( 123 ) , is equivalent to a newtonraphson procedure .
it works by approximating the log likelihood by a quadratic , resulting in a working response variate vi .
a weighted backtting algorithm is applied to the response vi , and then vi is recomputed and the process is repeated , until convergence .
the actual
upper gure ( cid : 123 ) the joint posterior scatterplot for 123 real - izations of ( cid : 123 ) i ( cid : 123 ) vi ( cid : 123 ) for eight different values of i ( cid : 123 ) ordered from left to right ( cid : 123 ) and bottom to top ( cid : 123 ) in age ( cid : 123 ) for large values of age ( cid : 123 ) where f is at ( cid : 123 ) the posterior distribution of i has spread simi - lar to the prior ( cid : 123 ) 123 ( cid : 123 ) 123 units ( cid : 123 ) ( cid : 123 ) for values of i corresponding to age at the growth spurt ( cid : 123 ) the posterior has smaller spread and is corre - lated with vi ( cid : 123 ) lower gure ( cid : 123 ) the numbered data fragments show the data and values of i corresponding to the eight panels in the
arranged on a lattice .
in general however , it is not clear how the bayesian covariance compares to the we did a small simulation with n = 123 obser - vations and two bivariate standard gaussian pre - dictors having correlation , for = 123 ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) 123
we computed the matrices s123 and a123 , and the resulting square root of the average diagonal of the covariance matrices .
table 123 shows the results .
we see that the standard deviations are roughly equal , except when the correlation between the inputs is very high .
in that case , the bootstrap standard devia - tion is nearly twice the bayes posterior standard deviation , and hence would lead to condence bands that are nearly twice as wide .
this may be due to the assumption of proper independence in the bayes
forms for vi and wi are
vi = i + ( cid : 123 ) yi i ( cid : 123 )
where vi is the variance of yi at i .
how can we simulate from the posterior density exp ( cid : 123 ) j ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ? the metropolishastings procedure ( hastings , 123 ) is a convenient approach here .
in general this method works as follows .
given a pos - terior density ( cid : 123 ) u ( cid : 123 ) from which we wish to generate realizations u , we dene a proposal distribution q ( cid : 123 ) u ( cid : 123 ) v ( cid : 123 ) that species the probability of moving from state u to v .
if we are currently at state u , we gen - erate a random state v according to q ( cid : 123 ) u ( cid : 123 ) v ( cid : 123 ) , and then move to v with probability
if ( cid : 123 ) u ( cid : 123 ) q ( cid : 123 ) u ( cid : 123 ) v ( cid : 123 ) > 123 , if ( cid : 123 ) u ( cid : 123 ) q ( cid : 123 ) u ( cid : 123 ) v ( cid : 123 ) = 123
in our application , we consider a move for a sin - gle function fj f ( cid : 123 ) j , with all other parameters j and let the corresponding working responses and diagonal weight matrices be v ( cid : 123 ) v ( cid : 123 ) and w ( cid : 123 ) w ( cid : 123 ) .
we choose as the proposal distribution the normal approxima - tion q ( cid : 123 ) fj ( cid : 123 ) f ( cid : 123 ) j ( cid : 123 ) = n ( cid : 123 ) sjv ( cid : 123 ) sjw123 123 ( cid : 123 ) which results from expanding the log - likelihood in a second - order taylor series .
the move from f to f ( cid : 123 ) has the form
held xed .
let ( cid : 123 ) =
( cid : 123 ) u ( cid : 123 ) q ( cid : 123 ) u ( cid : 123 ) v ( cid : 123 ) ( cid : 123 ) 123
fj , ( cid : 123 ) ( cid : 123 ) =
k ( cid : 123 ) =j fk + f ( cid : 123 )
f ( cid : 123 ) = sjv + s123 / 123
which is just a weighted version of the basic oper - ation in the bayesian backtting procedure given in algorithm 123 .
the acceptance probability works out to be
( cid : 123 ) f ( cid : 123 ) q ( cid : 123 ) f ( cid : 123 ) f ( cid : 123 ) ( cid : 123 ) =
for smoothing splines the operator that uses observation weights w has the form sj = ( cid : 123 ) w + jkj ( cid : 123 ) 123w , and the acceptance probability is easily computed in o ( cid : 123 ) n ( cid : 123 ) operations .
j = 123 / j and 123 can the tuning parameters 123 be sampled in a similar way .
in expression ( 123 ) for the conditional distribution of 123 j , we again need to j i ( cid : 123 ) f is replaced j kjf .
f t ( cid : 123 ) s compute the penalty f t j f = rtswr + by f tw ( cid : 123 ) s 123rts123 / 123w123 / 123z + ztz , all quantities that are already
j i ( cid : 123 ) f and we have f tws
the determinant ratio is not as easy to compute , but is typically very close to 123 ( all that changes are the weights ) , and we can ignore it in the calcula - tions .
details of this procedure , including an splus software implementation and a comparison to the related approach of ( zeger and karim , 123 ) , will
the additive model used here is a special case of the gaussian process model for exible regression .
in this class of models , a gaussian process prior is assumed for the regression function , and inference is carried out from the posterior .
different choices for the prior covariance function lead to particular models : the additive smoothing spline model results from the prior discussed in this paper .
the general gaussian process model was proposed by ohagan ( 123 ) .
more recently neal ( 123 ) and williams and rasmussen ( 123 ) have explored the computational aspects in depth .
they use mcmc for the tuning parameters , and an o ( cid : 123 ) n123 ( cid : 123 ) procedure to obtain the the mean and covariance of the gaussian posterior .
this o ( cid : 123 ) n123 ( cid : 123 ) operation can make the analysis infea - sible for large n .
because of the banded nature of the matrices arising in the cubic smoothing spline model , we are able to reduce this computation to o ( cid : 123 ) nm ( cid : 123 ) where m is the number of gibbs sampling as mentioned in the introduction , wong and kohn ( 123 ) provide an o ( cid : 123 ) n ( cid : 123 ) algorithm for the additive spline model using the state - space representation of splines , introduced in ansley and kohn ( 123 ) .
this framework is formally equivalent to that of wahba ( 123 ) .
we make no claims that our proce - dure is more efcient that theirs in the additive spline model; rather we believe that our proposal has the advantages of conceptual simplicity and
as suggested by a referee , extensions to scale mix - ture and auto - correlated errors are possible using the methods proposed in smith , wong and kohn ( 123 ) .
we have also restricted ourselves to the use of proper priors .
in general , choosing the prior to ensure that the posterior is proper can be a tricky exercise , especially in random effects models .
see hobert and casella ( 123 ) for detailed discussion of
we have written several functions in the s - plus language for implementing the ideas in this paper .
in particular , a function gibbs . gam ( ) takes as input a tted gam object ( chambers and hastie , 123 ) and samples from the posterior distribution .
the follow -
hastie and r .
tibshirani
ing lines produced the essential ingredients for the gures in section 123 :
bonefit < - gam ( spnbmd s ( age , 123 ) + ethnic
+ random ( factor ( idnum ) ) , data=bonef )
bone . samples < - gibbs . gam ( bonefit , nwarm=123 ,
even though bonefit requested 123df in s ( age , 123 ) , this acts simply as a starting value in the call to gibbs . gam ( ) .
the gam ( ) object can specify any number of smooth terms and random effects , and they all get accommodated automatically .
although random ( ) is a rather simple random intercept smoother , it is not difcult for users to provide their own random effects methods .
the gibbs . gam collection will be made available from the public archive at carnegie - mellon univer - from a mixed effects or empirical bayes point of view , we have provided an o ( cid : 123 ) n ( cid : 123 ) algorithm for sampling from the posterior distributions , given the variance components , even when the random effects ( smoothing splines ) have dimension n .
the usual backtting procedure delivers the posterior means or blups in o ( cid : 123 ) n ( cid : 123 ) computations .
we are currently exploring approximations that allow the estimation of the variance components as well in o ( cid : 123 ) n ( cid : 123 ) com - putations .
we gave one such approximation in sec -
algorithms for generating s123 / 123z
we present two algorithms for generating an n - vector s123 / 123z , where as before z is a vector of n ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) variates .
the rst algorithm is iterative , and uses repeated applications of the smoothing operator s .
it has the same order of complexity as the smoother and hence is o ( cid : 123 ) n ( cid : 123 ) if the smoother can be applied with only o ( cid : 123 ) n ( cid : 123 ) calculations .
this is the case for many popular smoothers including cubic smooth - ing splines , kernels and wavelet smoothers .
the second procedure is specically designed for cubic smoothing splines , and uses the banded nature of the covariance kernel to generate s123 / 123z .
it is more efcient than the rst algorithm but applicable only to cubic smoothing splines .
general algorithm .
consider
s123 / 123 = i 123
123 ( cid : 123 ) s i ( cid : 123 ) 123 123
number of iterations until convergence in 123 experiments ( cid : 123 )
for general s123 / 123z algorithm
and premultiply by s , giving
s123 / 123 = s s123 / 123
= s 123
123 s ( cid : 123 ) s i ( cid : 123 ) + 123
123 s ( cid : 123 ) s i ( cid : 123 ) 123 123 s ( cid : 123 ) s i ( cid : 123 ) 123 ( cid : 123 )
hence we can apply s123 / 123 by repeated applications of s representing the right - hand side of ( 123 ) .
this leads to the following algorithm .
algorithm a . 123
general procedure for generat -
take z n ( cid : 123 ) 123 ( cid : 123 ) i ( cid : 123 ) .
set z ( cid : 123 ) = sz ( cid : 123 ) z ( cid : 123 ) ( cid : 123 ) = z .
do for b = 123 ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 )
z ( cid : 123 ) z ( cid : 123 ) + sz ( cid : 123 ) ( cid : 123 )
( cid : 123 ) b123 ( cid : 123 ) ( cid : 123 ) sz ( cid : 123 ) ( cid : 123 ) z ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 )
until ( cid : 123 ) ( cid : 123 ) sz ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) is small .
in step 123 , the strange - looking multiplier ( cid : 123 ) 123
b ( cid : 123 ) / ( cid : 123 ) b 123 ( cid : 123 ) generates the coefcients in the tay - lor series ( 123 ) .
it is easy to show that z ( cid : 123 ) s123 / 123z , as long as s ( cid : 123 ) s i ( cid : 123 ) b 123
this is true for any symmetric smoother having eigenvalues in ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) : this includes cubic smoothing splines and some symmetrized kernel smoothers .
note that for pro - jections , s ( cid : 123 ) s i ( cid : 123 ) = 123 and so convergence is immediate ( no iterations of step 123 ) .
table 123 shows the results of a simulation experi - ment to examine the convergence of this procedure .
with a sample size n = 123 , we generated a ran - dom normal vector z and applied the above algo - rithm with a cubic smoothing spline operator with degrees of freedom randomly chosen between 123 and 123
the convergence criterion was max ( cid : 123 ) sz ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) < 123 ( cid : 123 ) 123
the number of iterations until convergence for 123 simulations is shown in table 123
the convergence is quite fast , never requiring more than 123 iterations and usually no more than six or seven .
algorithm for smoothing splines .
when the smoother s represents a smoothing spline , we can implement a more precise and efcient algo - rithm for generating s123 / 123z .
our implementation of smoothing splines follows de boor ( 123 ) , where we represent the tted functions in a basis of cubic
f ( cid : 123 ) x ( cid : 123 ) = m ( cid : 123 )
the number of basis functions m depends on the number of unique values of x among the n input val - ues xi , as well as the particular representation used .
in our case nu unique values of x dene nu 123 inte - rior knots and a corresponding basis of m = nu + 123 cubic b - splines .
if all the n values of x are unique , then m = n + 123
the smoothing spline solution is
f = sy = b ( cid : 123 ) btb + ( cid : 123 ) 123bty
where the n rows of the nm basis matrix b consist of the vector of m basis functions b ( cid : 123 ) x ( cid : 123 ) evaluated at the n sample values xi .
the m m penalty matrix
likewise , the tted function at an arbitrary input value x is given by
f ( cid : 123 ) x ( cid : 123 ) = bt ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) btb + ( cid : 123 ) 123bty
the coefcient estimates ( cid : 123 ) are the posterior mean
for ( cid : 123 ) based on a model y = bt ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) + , where :
( cid : 123 ) has a ( degenerate ) prior normal distribution n ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) with = 123 / 123
here has a two - dimensional null space corresponding to parameters leading to linear functions of x .
it also gives effec - tively innite penalty to nonzero second derivatives at the boundary knots , and hence enforces the nat - ural boundary conditions .
the prior covariance matrix k for f in ( 123 ) is the above expressions generalize easily to the case where each observation has a weight .
this hap - pens naturally in nonlinear likelihood settings as in the next section , and also when the x values are tied .
in the latter case the observations are collapsed onto the unique values of xi , the yi are replaced by the average at the tied values of xi , and the observations receive weights proportional to the counts at each unique x .
bbt evaluated at the data .
thus the posterior distribution for ( cid : 123 ) is ( cid : 123 ) ( cid : 123 ) y
n ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) 123 ( cid : 123 ) btb + ( cid : 123 ) 123 ( cid : 123 ) .
likewise ,
distribution of f is
f ( cid : 123 ) y n ( cid : 123 ) b ( cid : 123 ) ( cid : 123 ) 123b ( cid : 123 ) btb + ( cid : 123 ) 123bt ( cid : 123 )
= n ( cid : 123 ) sy ( cid : 123 ) 123s ( cid : 123 ) ( cid : 123 )
hence to simulate from this posterior , it is sufcient to simulate a parameter ( cid : 123 ) n ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) btb+ ( cid : 123 ) 123 ( cid : 123 ) , and hence we can produce a posterior realization of the entire function .
it turns out that there is no additional computa - tional burden over and above the usual smoothing spline o ( cid : 123 ) n ( cid : 123 ) computations .
the matrix b has four nonzero bands , and both btb and are 123 - banded .
this means that btb + = ltl has a 123 - banded cholesky factorization l ( silverman , 123 ) .
this l is computed as part of the smoothing - spline calcu - lations , and is available as part of the t .
hence ( cid : 123 ) = l123z n ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 ) btb + ( cid : 123 ) 123 ( cid : 123 ) if z n ( cid : 123 ) 123 ( cid : 123 ) i ( cid : 123 ) .
we obtain by solving l ( cid : 123 ) = z , which takes o ( cid : 123 ) n ( cid : 123 ) com - putations , because of the banded nature of l .
modied backtting and efciency
in section 123 we mentioned that the output of the smoothers have to be centered , to avoid identiabil - ity problems .
here we describe a more general cen - tering that speeds up convergence of the bayesian
in the standard backtting algorithm , strong correlations among the inputs can cause slow con - vergence , because the procedure slowly seesaws towards the solution .
in buja , hastie and tibshi - rani ( 123 ) a modied backtting algorithm was proposed , in which all of the ( linear ) projections for all of the inputs were t together , while the iterative one - at - a - time smoothing was applied just to the nonlinear parts of each function .
this can noticeably speed up the convergence of backtting , because it immediately captures the linear corre - lations .
we let x denote the linear part of the model ( including intercept ) with projection opera - tor h , and hj the operator that projects onto the two - dimensional linear subspace of eigenvalue 123 of sj .
then the modied backtting algorithm uses the smoothers h and sj = sj hj ( for symmetric smoothers , such as smoothing splines ) .
sj produces the nonlinear part of the t for variable xj .
an analogous strategy can be used to speed up the bayesian backtting procedure ( liu , wong and kong , 123 ) .
we can separate each function f = xjj + f , where xjj includes the constant and linear part of fj .
the prior and posterior distribu - tions factor accordingly , xjj ( cid : 123 ) y n ( cid : 123 ) hjy ( cid : 123 ) 123hj ( cid : 123 ) and f ( cid : 123 ) y n ( cid : 123 ) sjy ( cid : 123 ) 123 sj ( cid : 123 ) , and they are independent .
notice as well that sj j sj .
then we alter - nately generate realizations of the linear component
123 / 123 = s123 / 123
hastie and r .
tibshirani
x all grouped together , separately from the non - linear functions fj .
the latter is achieved by rst generating the usual realization fj , and then remov - ing the linear trend fj = fj hjfj .
exact prior for df based on flat priors for variance components
the bayesian smoothing spline model , on n unique values of x ( cid : 123 ) let the prior for 123 and 123 both be improper and at on the log - scale ( cid : 123 ) p ( cid : 123 ) 123 ( cid : 123 ) 123 / 123 and p ( cid : 123 ) 123 ( cid : 123 ) 123 / 123 ( cid : 123 ) then the implicit prior on df is discrete , and puts mass 123 123 and n ( cid : 123 )
consider the random variable vd = 123 / ( cid : 123 ) 123 + / zd ( cid : 123 ) ( cid : 123 ) where log ( cid : 123 ) zd ( cid : 123 ) u ( cid : 123 ) d ( cid : 123 ) d ( cid : 123 ) ( cid : 123 ) then v = limd vd is 123 or 123 with probability 123
proof of lemma .
p ( cid : 123 ) vd > v123 ( cid : 123 ) = p ( cid : 123 ) log ( cid : 123 ) zd ( cid : 123 ) < log
123 + log
123 + / ed
123 + / ed
now for any v123 ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) ,
d p ( cid : 123 ) vd > v123 ( cid : 123 ) = p ( cid : 123 ) v > v123 ( cid : 123 )
proof of theorem .
for a smoothing spline ,
df = n ( cid : 123 )
123 + dj
here = 123 / 123 , the di are the eigenvalues of the n n penalty matrix k , and d123 = d123 = 123 and dj > 123 for j = 123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) n .
so for any xed value of 123 , each of the contributions for j > 123 is the same and either 123 or 123 , and the rst two contributions are always 123
thus for xed 123 df is 123 or n with probability 123 .
since this does not depend on 123 , this is also unconditionally true .
finally , since p ( cid : 123 ) log ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 ) 123 , we see that limd ( cid : 123 ) ( cid : 123 ) log zd ( cid : 123 ) = ( cid : 123 ) ( cid : 123 ) log 123 ( cid : 123 ) .
we have not proved this for the nearly - at prior p ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 ) 123 / 123 ( cid : 123 ) exp ( cid : 123 ) / 123 ( cid : 123 ) ( which is in fact not integrable and so also improper ) .
empirical evi - dence suggests that this has the same distribution , obtained by simulating from an ig ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) density ,
and studying the behavior of the quantiles of v as
one puzzling aspect of this prior distribution is that it has no support except on these two extreme points .
this implies the posterior should be the same .
one explanation for this somewhat contra - dictory behavior is that the gibbs samplers are never run long enough ! df = n is an absorb - ing state , since this implies that ! e ! = 123 and hence the posterior for 123 , ig ( cid : 123 ) n / 123 ( cid : 123 ) ! e ! 123 ( cid : 123 ) will pro - duce a 123 with probability 123 , leading to = 123 and another exact t .
likewise df = 123 is an absorb - ing state if 123 / 123 is used for the prior .
this is not the case for ( cid : 123 ) 123 / 123 ( cid : 123 ) exp ( cid : 123 ) / 123 ( cid : 123 ) , whose posterior is 123f tkf + ( cid : 123 ) .
even though the penalty may be zero ( for exact linear ts ) , the presence of > 123
we thank radford neal for suggesting the use of the metropolishastings procedure in section 123 , bernard silverman for help with the smoothing spline representation , larry wassermen , the edi - tor and two referees for helpful comments .
xihong lin was especially helpful in providing ( personal communication ) an up - to - date survey of the mixed effects eld and making her preprints available .
trevor hastie was supported in part by nsf grant dms - 123 - 123 nih grant roi - ca - 123 - 123
robert tibshirani was supported by the natural sciences and engineering research council of
ansley , c .
and kohn , r .
( 123 ) .
estimation , ltering and smoothing in state space models with diffuse initial condi - tions .
statist .
123 123
buja , a . , hastie , t .
and tibshirani , r .
( 123 ) .
linear smoothers and additive models ( with discussion ) .
statist .
123 123
carter , c .
and kohn , r .
( 123 ) .
on gibbs sampling for state
space models .
biometrika 123 123
chambers , j .
and hastie , t .
( 123 ) .
statistical models in s .
wadsworth / brooks cole , pacic grove , ca .
de boor , c .
( 123 ) .
a practical guide to splines .
springer , new
denison , d . , mallick , b .
and smith , a .
bayesian curve tting .
statist .
b 123 123
gelfand , a .
and smith , a .
( 123 ) .
sampling based approaches to calculating marginal densities .
statist .
123 123
gelman , a . , carlin , j . , stern , h .
and rubin , d .
bayesian data analysis .
crc press , boca raton , fl .
geman , s .
and geman , d .
( 123 ) .
stochastic relaxation , gibbs distributions and the bayesian restoration of images .
ieee trans .
pattern anal .
machine intelligence 123 123
green , p .
and silverman , b .
( 123 ) .
nonparametric regres - sion and generalized linear models : a roughness penalty approach .
chapman and hall , london .
hastie , t .
( 123 ) .
pseudosplines .
statist
( 123 ) .
curve tting and optimal design for regression ( with discussion ) .
statist
silverman , b .
( 123 ) .
spline smoothing : the equivalent kernel
hastie , t .
and stuetzle , w .
( 123 ) .
principle curves
statist .
123 123
hastie , t .
and tibshirani , r .
( 123 ) .
generalized additive mod -
statist .
123 123
method .
statist .
123 123
smith , m . , wong , c .
and kohn , r .
( 123 ) .
additive nonpara - metric regression with autocorrelated errors .
statist .
b 123 123
speed , t .
( 123 ) .
comment on that blup is a good thing : the
hastie , t .
and tibshirani , r .
( 123 ) .
generalized additive mod -
estimation of random effects .
statist .
123 123
chapman and hall , london .
hastings , w .
( 123 ) .
monte carlo sampling methods using markov chains and their applications .
biometrika 123 123
hobert , j .
and casella , g .
( 123 ) .
the effect of improper pri - ors on gibbs sampling in hierarchical linear mixed models .
statist .
123 123
hodges , j .
and sargent , d .
( 123 ) .
counting degrees of free - dom in hierarchical and other richly parametrized models .
technical report , div . , biostatistics , univ .
minnesota .
holmes , c .
and mallick , b .
( 123 ) .
bayesian wavelet networks for nonparametric regression .
neural net - works .
to appear .
laird , n .
and ware , j .
( 123 ) .
random - effects models for
longitudinal data .
biometrics 123 123
lin , x .
and zhang , d .
( 123 ) .
inference in generalized additive mixed models .
technical report , biostatistics , dept . , univ .
liu , j .
s . , wong , w .
and kong , a .
( 123 ) .
covariance struc - ture of the gibbs sampler with applications to the compar - isons of estimators and augmentation schemes .
biometrika
neal , r .
( 123 ) .
bayesian learning for neural networks .
springer , new york .
spiegelhalter , d . , best , n . , gilks , w .
and inskip , h .
( 123 ) .
hepatitis b : a case study in mcmc methods .
in markov chain monte carlo in practice ( w .
gilks , s .
richardson and d .
spegelhalter , eds . ) chapman and hall ,
tierney , l .
( 123 ) .
markov chains for exploring posterior distri -
butions ( with discussion ) .
statist .
123 123
wahba , g .
( 123 ) .
spline bases , regularization , and generalized cross - validation for solving approximation problems with large quantities of noisy data .
in proceedings of the inter - national conference on approximation theory in honour of george lorenz .
academic press , austin , tx .
wahba , g .
( 123 ) .
spline models for observational data
williams , c .
and rasmussen , c .
( 123 ) .
gaussian processes for regression .
in neural information processing systems 123 ( d .
touretzky , m .
mozer and m .
hasselmo , eds . ) mit
wong , c .
and kohn , r .
( 123 ) .
a bayesian approach to estimat - ing and forecasting additive nonparametric autoregressive models .
time ser .
123 123
zeger , s .
and karim , m .
( 123 ) .
generalized linear models with random effects : a gibbs sampling approach .
statist .
123 123
dennis cook and iain pardoe
hastie and tibshirani propose an intriguing idea , neatly linking bayesian modeling of the functions in a generalized additive model with gibbs sam - pling to obtain posterior realizations of these func - tions .
since their procedure utilizes only smoother matrices for individual predictors , sj , partial resid - uals , rj , and normal random vectors , zj , the method would appear to be applicable to any models with additive components that can be expressed in the
dennis cook is professor and iain pardoe is graduate student at the school of statistics , uni - versity of minnesota , st .
paul , minnesota 123
a natural question to ask of any proposed method - ology is to what use can it be put ? hastie and tibshiranis examples , while interesting in them - selves , left us questioning what information could be gleaned from plots such as figures 123 and 123 for the ozone data and figure 123 for the growth curves data .
for example , do the individual realiza - tions in figure 123 add anything to the information already provided by the pointwise posterior inter - vals ? figure 123 goes some way to addressing these thoughts with a graphical display of two function - als of the posterior realizations .
we decided to pur - sue these thoughts in a different direction , that of model checking , and we outline our ndings in sec - tion 123
we discuss other potential applications in section 123 and make some more general comments in
marginal model plot for the tted values for the additive model t to four air pollution variables ( cid : 123 )
gibbs marginal model plot for inversion base height for the additive model t to ( cid : 123 ) w123 ( cid : 123 ) w123 ( cid : 123 ) w123 ( cid : 123 ) ( cid : 123 )
marginal model plots
the goal of a regression analysis can be expressed as inference about the dependence of an unknown cdf f of the conditional random variable y ( cid : 123 ) x on the value of x .
consider a generic regression model for y ( cid : 123 ) x represented by the cdf m; estimating this
model gives rise to an estimated cdf ( cid : 123 ) m .
we now ( cid : 123 ) m .
we use the fact that f ( cid : 123 ) y ( cid : 123 ) x ( cid : 123 ) = m ( cid : 123 ) y ( cid : 123 ) x ( cid : 123 ) for
consider graphics for comparing selected character - istics of f to the corresponding characteristics of
all values of x in its sample space if and only if f ( cid : 123 ) y ( cid : 123 ) h ( cid : 123 ) = m ( cid : 123 ) y ( cid : 123 ) h ( cid : 123 ) for all functions h = h ( cid : 123 ) x ( cid : 123 ) .
this is a more general version of the approach proposed by cook and weisberg ( 123 ) which sets h = atx , where a ( cid : 123 ) p .
in particular , we focus on comparing a nonparametric estimate of the mean of y ( cid : 123 ) h to the
for some xed h , plot y versus h .
add a nonpara - metric mean estimate , say a cubic smoothing spline with xed degrees of freedom , to the plot; denote
corresponding mean computed from ( cid : 123 ) m , for various this ( cid : 123 ) ef ( cid : 123 ) y ( cid : 123 ) h ( cid : 123 ) , where ef denotes expectation under a mean estimate under ( cid : 123 ) m , ( cid : 123 ) e ( cid : 123 ) m ( cid : 123 ) y ( cid : 123 ) h ( cid : 123 ) , where e ( cid : 123 ) m denotes expectation under ( cid : 123 ) m .
since e ( cid : 123 ) m ( cid : 123 ) y ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) h ( cid : 123 ) , we can obtain ( cid : 123 ) e ( cid : 123 ) m the tted values under m , e ( cid : 123 ) m
we wish to compare this mean estimate with ( cid : 123 ) y ( cid : 123 ) h ( cid : 123 ) = ( cid : 123 ) y ( cid : 123 ) h ( cid : 123 ) from a nonparametric mean estimate for the regression of ( cid : 123 ) y ( cid : 123 ) x ( cid : 123 ) , on h .
we can then add this to the plot to obtain a marginal model plot ( mmp ) for h; this can be thought of as a plot for checking the model in the ( marginal ) direction h .
using the same method ( and smoothing param - eter ) to obtain this estimate as that used to obtain the mean estimate under f allows point - wise com - parison of the two estimates , since any estimation bias should cancel .
see bowman and young ( 123 ) for further discussion of this point .
if the model is
gibbs marginal model plot for the tted values for the additive model t to four air pollution variables ( cid : 123 )
gibbs marginal model plot for the tted values for the additive model t to ( cid : 123 ) w123 ( cid : 123 ) w123 ( cid : 123 ) w123 ( cid : 123 ) ( cid : 123 )
a close representation of f , we can expect that for any quantity h the marginal mean estimates should
( cid : 123 ) y ( cid : 123 ) h ( cid : 123 ) ( cid : 123 ) ef ( cid : 123 ) y ( cid : 123 ) h ( cid : 123 ) .
ideas for selecting which mmps ( i . e . , which func - tions h ) to consider in practice are given in cook and weisberg ( 123 ) , with additional discussion pro - vided in cook ( 123 ) and cook and weisberg ( 123 ) .
some examples of useful mmps include those for tted values , individual predictors and linear com - binations of the predictors .
any indication that the estimated marginal means do not agree for one par - ticular mmp suggests that the model could perhaps be improved; if they agree for a variety of plots , we have support for the model .
the ideas above can be extended to variance estimates to provide further ways for checking models .
consider , for example , a mmp for the tted val - ues for hastie and tibshiranis ozone data example with four predictor variables .
the plot in figure 123 shows a systematic discrepancy between the ( black ) mean estimate under f and the ( gray ) mean esti -
mate under ( cid : 123 ) m; the mean estimate under ( cid : 123 ) m is too
low on the left , too high in the middle and too low again on the right .
both mean estimates were calcu - lated using the s - plus function smooth . spline with ( the default ) four degrees of freedom .
the data , the mean estimate under ( cid : 123 ) m does not
on the other hand , relative to the variation in
appear to be too far from the mean estimate under f .
so , are the discrepancies enough to indicate any potential for model improvement ? porzio and weis - berg ( 123 ) provide some frequentist methodology to address this issue : pointwise reference bands to aid visualization and statistics to calibrate discrep - ancies .
hastie and tibshiranis procedure also pro - vides methodology to address this issue .
they make the well - taken points that we can make use of the individual realizations of the posterior distributions of the functions in an additive model , and display the posterior distributions of interesting function - als of them .
they also note that we can carry out bayesian inference for any quantity of interest .
this would appear to offer a bayesian way to aid visu - alization in a mmp , with potential possibilities for
for any particular mmp , it would be useful to dis - play mean estimates for the individual realizations from the posterior distribution of the tted values , j=123 f t where the tted - value realizations are just t = 123 ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) and f t j .
so , instead
of adding the mean estimate under ( cid : 123 ) m to the plot
j + s123 / 123
j = sjrt
of the mean estimate under f , we can instead add a mean estimate for each gibbs sample , gt , and obtain what we call a gibbs marginal model plot ( gmmp ) .
if enough samples are taken , say 123 or
imate mean estimate band under ( cid : 123 ) m .
this plot may
123 , the gibbs mean estimates will form an approx -
provide a visual way of determining whether there is any evidence to contradict the possibility that f ( cid : 123 ) y ( cid : 123 ) h ( cid : 123 ) = m ( cid : 123 ) y ( cid : 123 ) h ( cid : 123 ) .
intuitively , if , for a particular h , the mean estimate under f lies substantially out -
side the mean estimate band under ( cid : 123 ) m ( formed from inside the mean estimate band under ( cid : 123 ) m , then per -
the mean estimates under gt ) , then perhaps the model can be improved .
if , no matter what the func - tion h is , the mean estimate under f lies broadly
haps the model provides a reasonable description of the conditional distribution of y ( cid : 123 ) x .
it would appear to be possible to supplement this purely graphical methodology with more formal bayesian inference .
consider a gmmp for the tted values for the ozone data .
hastie and tibshirani kindly provided us with the s plus functions for implementing the ideas in their paper , as well as with help in using their code .
this enabled us to construct the gmmp in figure 123
the gibbs sampling was carried out using the fully bayesian procedure described in hastie and tibshiranis section 123 , with a warm - up period of 123 iterations .
the plot shows the ( black ) mean estimate under f lying mostly outside the
mean estimate band under ( cid : 123 ) m ( formed from 123
( gray ) mean estimates under gt ) .
this appears to offer clear evidence that the tted model can be
as curious applied statisticians , we couldnt resist trying to see if we could come up with a bet - ter model for these data .
one particular technique we applied was sliced average variance estimation ( save ) , introduced by cook and weisberg ( 123 ) and developed by cook and lee ( 123 ) .
save is a model - free method for estimating the smallest sub - space ( cid : 123 ) of ( cid : 123 ) p so that y and x are independent given the projection of x onto ( cid : 123 ) , p ( cid : 123 ) x .
in words , all the information about y that is available from x is contained in p ( cid : 123 ) x .
following li ( 123 ) , ( cid : 123 ) is a dimension reduction subspace for the regression of y on x .
the smallest such ( cid : 123 ) is called the central sub - space , ( cid : 123 ) y ( cid : 123 ) x ( cook , 123; cook , 123 ) ; save yields a subspace estimate , ( cid : 123 ) save ( cid : 123 ) y ( cid : 123 ) x .
this estimate can then be used to postulate a model , as described by
since the additive model t above appears unable to account for the curvature in the mmp for the tted values , we felt that save might be able to provide us with a better model .
we used the save methodology to infer the dimension of ( cid : 123 ) y ( cid : 123 ) x to be two , and obtained two linear combinations of predictors , w123 and w123 , as an estimate of a basis for ( cid : 123 ) y ( cid : 123 ) x .
a three - dimensional plot of y versus w123 and w123 indi - cated that an interaction term , w123 , might also be
important .
so , we decided to t an additive model : e ( cid : 123 ) y ( cid : 123 ) x ( cid : 123 ) = + f123 ( cid : 123 ) w123 ( cid : 123 ) + f123 ( cid : 123 ) w123 ( cid : 123 ) + f123 ( cid : 123 ) w123 ( cid : 123 ) .
smooth - ing splines with ( the s plus default ) four degrees of freedom were used to estimate the f functions .
a gmmp for the tted values for this model is shown in figure 123
the plot shows the ( black ) mean esti - mate under f lying inside the mean estimate band
under ( cid : 123 ) m ( formed from the ( gray ) mean estimates
under gt ) .
there is little evidence in this plot to suggest that the tted model can be improved .
however , there is evidence from a mmp for one of the original predictors , inversion base height , that this model too could be improved .
again , the dis - crepancy between the marginal mean estimates in this plot ( not shown ) is difcult to assess relative to the variability in the data .
the corresponding gmmp in figure 123 allows this discrepancy to be evaluated visually , and the plot reinforces the sup - position that the model could possibly be improved ( at least for low values of inversion base height ) .
having applied hastie and tibshiranis method - ology to these data , gmmps appear to offer a quick and easy way to graphically check models .
the gibbs sampling only needs to be done once for each model; with hastie and tibshiranis s plus code this is straightforward .
the analyst can then cycle through a variety of gmmps to get some guidance on whether ( and how ) an alternative model might provide an improvement .
for example , in the above analysis , a next step might be to develop a model that deals with low values of inversion base height more satisfactorily , say by increasing the degrees of freedom for the smoothers in the additive model , or by trying different smoothers such as loess .
does a gmmp suffer the same shortcoming as hastie and tibshiranis figure 123; namely , would we be able to obtain equivalent information by plotting pointwise posterior intervals instead of individual posterior realizations ? the answer to this question would surely be yes , were it not for the fact that it is not clear how such intervals might be dened in practice .
for example , posterior intervals could be calculated for the tted values in an additive model by summing the posterior intervals for the individual functions in the model .
it would then be straightforward to plot the pointwise intervals on a mmp for the tted values .
but , for mmps for any other function h , it is unclear what pointwise pos - terior intervals should be dened to be .
one possi - bility would be to smooth the pointwise upper and lower limits for the tted values using the same method as used to obtain the mean estimates under
f and ( cid : 123 ) m , but it is not clear that this will give us pointwise posterior intervals for e ( cid : 123 ) m
( cid : 123 ) y ( cid : 123 ) h ( cid : 123 ) .
other potential applications
returning to hastie and tibshiranis figure 123 , can these plots ( of partial residuals versus individ - ual predictors ) be used for model checking ? the answer to this question would appear to be no .
the black curves are smooths of the partial residuals , fj = sjrj , while the gray curves are the gibbs pos - terior realizations , f t j .
these plots would appear to offer visualization only of the vari - ability in the tted functions .
appropriate plots for model checking in this context are gmmps for the individual predictors , as shown for example in fig -
j + s123 / 123
j = sjrt
there are other plots used in model checking and regression diagnostics that can be difcult to assess relative to the variation in the data .
some exam - ples include : residual plots; ceres plots , which are a generalization of partial residual plots and were introduced by cook ( 123 ) ; net effect plots , which aid in assessing the contribution of a selected pre - dictor to a regression and were introduced by cook ( 123 ) .
the ideas discussed above would appear to have a role to play in the analysis of such plots .
work is in progress on these issues , as well as on developing supplementary bayesian inference
hastie and tibshiranis procedure appears to live up to its claim of modularity and generality .
although the procedure derives from the backtting algorithm for tting additive models , it could proba - bly be applied fairly easily to other families of mod - els such as generalized linear models .
whether the procedure could also be described as conceptually simple is perhaps more open to debate .
for example , choosing priors for the variance components is far from trivial , and mcmc convergence should always be checked in practice .
that said , there is clearly a wealth of potential applications for the posterior samples generated with this technique .
save techniques can be applied using arc ( cook and weisberg , 123 ) , a comprehensive regression program .
information about the program is avail - able at the internet site www . stat . umn . edu / arc .
research supported in part by national science
alan e .
gelfand
this generous manuscript offers much food for thought .
i admire its generality , the ability to han - dle both gaussian and non - gaussian likelihoods , to accommodate both nonparametric and semipara - metric forms , to handle a broad range of smoothers .
i applaud its pragmatic stance .
much energy is invested on details of the model tting , worrying about efciency of algorithms and introducing use - ful approximations .
finally , i appreciate its effort to be comparative , frequently linking bayesian , empir - ical bayes and classical perspectives , attempting a bridging of ideologies within a rich regression set -
but then , what is there that is worth comment - ing upon ? i will focus on two main issues .
first , i believe the authors are a bit too casual in their bayesian formulation .
there is confusion through - out with regard to singular versus improper priors , with regard to proper versus improper posteriors .
if the contribution is viewed as primarily algorithmic , so be it .
but if the claim is that legitimate bayesian inference is being implemented , that credible poste - rior analysis is being provided , then i have reserva -
second , at least in the gaussian case , within the authors general objectives , there seems to be no need to introduce iterative simulation .
a direct sim - ulation formulation is straightforward , avoiding all concerns with mcmc model tting .
to my rst point , at the outset ( section 123 ) , we begin with a prior on ( cid : 123 ) which induces a prior on f = b ( cid : 123 ) .
if the dimension of ( cid : 123 ) is less than that of f , a proper gaussian prior on ( cid : 123 ) induces a singular but proper distribution on f .
an improper distribution on ( cid : 123 ) necessarily yields an improper distribution on f .
for smoothers , it is apparently more typical that the dimension of ( cid : 123 ) is greater than that of f and that an improper ( not degenerate ) prior is implicit for ( cid : 123 ) , hence an induced improper prior for f .
thus , these priors are not normal .
expression ( 123 ) should be written as
f ( cid : 123 ) 123 123 ( cid : 123 )
( cid : 123 ) f tkf / 123 ( cid : 123 )
alan e .
gelfand is professor , department of statis - tics , university of connecticut , storrs , connecticut 123 - 123 ( e - mail : alan@stat . uconn . edu ) .
where the power a is arbitrary since the distribu - tion is improper .
similarly ,
( cid : 123 ) ( cid : 123 ) 123 123 ( cid : 123 )
f ( cid : 123 ) 123 123 ( cid : 123 )
( cid : 123 ) f tbtbf / 123 ( cid : 123 )
if we start with ( 123 ) , adopting a specic general - ized inverse b and operate formally ( since ( 123 ) is improper ) we obtain
thus determining k .
introduction of k , ( cid : 123 ) and s ( cid : 123 ) ( cid : 123 ) ( e . g . , expres - sion ( 123 ) ) serves to cloud matters .
ultimately , in ( 123 ) , if f ( cid : 123 ) y is proper , i + k is full rank and s ( cid : 123 ) ( cid : 123 ) = ( cid : 123 ) i + k ( cid : 123 ) 123 ( which doesnt appear until halfway through section 123 ) is all we need .
if f ( cid : 123 ) y is not proper , how can we speak of its posterior ? in this case , when the dim of ( cid : 123 ) exceeds the dimension of f , an improper posterior distribution for ( cid : 123 ) may induce a unique proper posterior for f .
see , for example , gelfand and sahu ( 123 ) in this regard .
when we move to section 123 , the situation becomes even more disturbing .
now , p functions are intro - duced additively in the mean structure , along with an intercept .
a centering constraint is introduced on each function for identiability .
in fact , from a bayesian point of view , with a proper posterior , there is no identiability issue ( as in , e . g . , lind - ley , 123 ) .
with improper priors , such constraints are customarily introduced to achieve proper pos - teriors as well as to provide well - behaved posteri - ors , yielding well - behaved mcmc algorithms .
the recommended centering on - the - y , that is , after each iteration , has become standard in these situa - tions .
see , for example , besag , green , higdon and
in any event , while the bayesian backtting method is presented as an algorithm , it does not appear to correspond to a gibbs sampler for a well - dened bayes model .
the existence of proper full conditionals for all model unknowns says nothing about the propriety of the joint posterior ( see , e . g . , casella and george , 123 ) .
i cannot see how a proper posterior can be associated with the model in ( 123 ) , using the various improper priors proposed for the fj ( cid : 123 ) furthermore , what is the prior on ? why isnt it updated in the sampler ? what is the distri - butional justication for inserting y into ( 123 ) ? in
this spirit , the updating using the approximation in ( 123 ) can be implemented , perhaps as a metropolis step , but it is not a draw from the full conditional distribution for i .
as to my second point , the authors concede that xing 123 in ( 123 ) , or , more generally in ( 123 ) , clearly inappropriate , as is xing the various 123 the improper prior for 123 in ( 123 ) along with the improper choices discussed for the 123s will surely produce an improper posterior , following hobert and casella ( 123 ) .
such priors , along with the improper priors for fj , appear to run counter to the authors claim in section 123 that , we have restricted our - selves to the use of proper priors , citing hobert and casella in this regard ! the essentially improper prior in ( 123 ) in place of ( 123 ) cannot be expected to yield a convergent mcmc algorithm if the posterior is improper using ( 123 ) .
moreover , even if priors are introduced for 123 and the 123 j such that a proper posterior is ensured , the gibbs sampler in this setting will be very slow .
apart from the usual convergence concerns , at each iteration each sj must be updated since j changes with each iteration .
an alternative specication enables direct simula - tion of the entire posterior and permits sj to be con - stant for each simulation .
convergence and updat - ing problems vanish .
suppose we retain the same
peter j
rst - stage specication as in ( 123 ) but take the prior on ( cid : 123 ) fj ( cid : 123 ) to be of the form ( cid : 123 ) fj ( cid : 123 ) ( cid : 123 ) 123 exp
adding a proper n ( cid : 123 ) 123 ( cid : 123 ) 123 / 123 ( cid : 123 ) prior for
( cid : 123 ) kj = jkj with j specied but not 123
with the j xed , ( cid : 123 ) kj need only be computed once at the
outset .
we may view j as a relative precision .
again , if ( 123 ) is improper , there is no unique choice for the power of 123 in the proportionality constant , as in ( 123 ) ( 123 ) .
however , once this power is provided , if 123 fol - lows an inverse gamma prior , we may factor the joint posterior density for , ( cid : 123 ) fj ( cid : 123 ) and 123 as
( cid : 123 ) ( cid : 123 ) fj ( cid : 123 ) ( cid : 123 ) 123 ( cid : 123 ) y
( cid : 123 ) ( cid : 123 ) fj ( cid : 123 ) ( cid : 123 ) 123 ( cid : 123 ) y
( cid : 123 ) = p
123 ( cid : 123 ) y
if ( 123 ) is proper , both densities on the right - hand side are .
in fact , the rst is an updated normal , the second an updated inverse gamma .
sampling 123 from p ( cid : 123 ) 123 ( cid : 123 ) y ( cid : 123 ) and then , ( cid : 123 ) ( cid : 123 ) f j ( cid : 123 ) from p ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) fj ( cid : 123 ) ( cid : 123 ) 123 ( cid : 123 ) y ( cid : 123 ) directly provides a posterior realization .
this approach appears to t well with the authors goals of pragmatism and efciency .
in summary , while the ideas in this paper are attractive , the bayesian modeling and resultant simulation - based inference , which are at its heart , are somewhat uncomfortable .
i warmly congratulate the authors on this paper .
i am sure they will succeed in broadening acceptance of the bayesian paradigm in inference in regres - sion by providing this well - written and accessible treatment of the use of markov chain monte carlo ( mcmc ) in tting the important class of ( general - ized ) additive models .
the paper promotes several
it can be interesting and revealing to examine bayesian analogues of familiar frequentist models and pro - cedures; mcmc is important in bayesian infer - ence; gibbs sampling is a convenient general recipe for mcmc; if that isnt available , try metropolis hastings; gibbs sampling is a close analogue of
peter j .
green is professor , department of mathemat - ics , university of bristol , bristol bs123 123tw , united kingdom ( e - mail : p . j . green@bristol . ac . uk ) .
backtting .
none of these points are individually very original , of course ! but it is very appealing to see their combination applied to additive mod - els , with a number of practical details worked out to produce an efcient methodology , especially since splus software to implement the resulting method - ology is provided .
my comments focus on some of these practical details , and some other relations and connections to the proposed methods .
bayesian function estimation
the reader who comes to this work from a back - ground of backtting in ( generalized ) additive mod - els rather than experience of inference about func - tions and surfaces in the bayesian paradigm might get an impression that this paper is close to the state - of - the - art in bayesian function estimation .
in fact , the authors do not claim this; researchers have
been investigating and using bayesian models and mcmc calculations for more complicated situations than this , almost from the earliest days of mcmc in statistics .
one could even claim that a driving force in the broader acceptance of practical bayesian methodology for inference in complex data struc - tures has been that using mcmc methods there was a comparatively trivial computational penalty to be paid in moving on from simple models to more complicated ones ( in the case of inference about functions , for example , replacing gaussian priors on functions by non - gaussian ones ) .
mcmc in statistics is generally accepted to have begun in statistical image analysis , and of course nonlinear smoothers , which do not destroy bound - aries between objects in the scene , are routinely needed there .
in discrete spatial settings , such as arise in pixellized images and region - based geo - graphical and ecological problems , particular use has been made of pairwise - difference priors that are not gaussian , but have heavier tails; for example
is the true image intensity in pixel i , and i j means the summation is over pairs of neighboring pixels ( see geman and mcclure , 123; green , 123; besag , green , hig - don and mengersen , 123 , among many others ) .
instead of using ( cid : 123 ) u ( cid : 123 ) = u123 , taking it to be ( cid : 123 ) u ( cid : 123 ) , or something more complicated such as log cosh u or 123 / ( cid : 123 ) 123 + u123 ( cid : 123 ) has proved successful in many image restoration contexts .
( the apparent connection here with m - estimation and robustness is not entirely
these methods are all for discrete spatial prob - lems , whether on lattices or irregular graphs .
non - parametric bayesian surface tting methods for continuous space include that of heikkinen and arjas ( 123 ) for inference on a poisson intensity , using ideas of model averaging over appropriately dened step functions to yield smooth posterior mean surfaces , and the variogram - based methods of diggle , tawn and moyeed ( 123 ) .
turning to regres - sion on more general , nonspatial , covariates , apart from the work of denison , mallick and smith ( 123 ) and holmes and mallick ( 123 ) that is mentioned in this paper , and other methods investigated by these researchers and colleagues , there is the interesting approach of m uller , erkanli and west ( 123 ) , based on dirichlet process priors .
bayes from conviction
the conceptual connection between smoothing methods , especially those based on penalized likeli - hood , and bayesian formulations of inference about functions is often made , but there always seems to be an implicit or explicit warning : dont take this too literally .
formal use of such connections is rarely made , a notable exception being wahbas important 123 paper , exploiting the bayesian con - nection to construct condence intervals about spline estimates .
however , the frequentist proper - ties of such intervals are well known to be problem -
this is all well understood by the authors , but i do think that if they seek to use the bayesian paradigmand presenting credible intervals for functions is certainly doing thatthey should try to take it a bit more seriously ! there are several
( cid : 123 ) ( cid : 123 ) f ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) x ( cid : 123 ) ( cid : 123 ) 123 dx have
the rst concerns the use of the roughness penalty as a negative - log - prior on the regression function .
does the usual penalty particular merit in this context , over other ( say ) quadratic forms in f that are zero for constant or rst - order f , or is the choice driven , as in straight - forward smoothing , by the computational advan - tages ? specically , do autocorrelations decline with lag in a reasonable way , and what about homogene - ity of variance ? ( answers to these questions are complicated by the partially improper nature of the prior in this case . ) are gaussian prior assumptions reasonable , or should we consider heavy - tailed mod - ications ? choice of functional form of the penalty has far greater consequences for the bayesian pro - cedure than for the simple smoother , as we are going to use it to generate much more subtle inferences , invested , presumably , with real probabilistic inter - pretations .
to be fair , the authors are in good com - pany in not raising these questions; they are almost never considered by anybody else either !
the second issue is the treatment of smoothing ( tuning ) parameters ( or , equivalently , variance com - ponents ) .
this is explicitly discussed , in section 123 of the paper .
i must say that i nd the full bayesian version much more compelling than either of the
a third concern is about sensitivity to prior assumptions , always problematic in hierarchical models .
it would be good to see at least an empiri - cal study of the effect on posterior inference of vari - ations in the authors assumptions .
i would antici - pate that everything about the inference except the posterior means is actually rather sensitive .
the root - s approximation
some mcmc details
i cannot be the only reader to worry about the quality of the approximation to s123 / 123 ( cid : 123 ) pro - posed in algorithm a . 123
the approach looks sim - plistic : why should the size of the rst neglected term of the series be a reliable guide to the the sum of all neglected terms ? and indeed a small numerical experiment bears out this
although an alternative , superior , approach using cholesky decomposition is available for the cubic spline smoother , this case provides a convenient choice for a numerical check .
taking xi = i , i = 123 ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) n = 123 and drawing a single vector z n ( cid : 123 ) 123 ( cid : 123 ) i ( cid : 123 ) , i compared ! s123 / 123z s123 / 123 htz ! with the toler - ance on ! sz ( cid : 123 ) ( cid : 123 ) ! used in the authors algorithm , where ht represents their approximation .
table 123 shows that , especially if higher precision is sought , the method becomes both expensive and much less suc -
are better approximations available for an amount of work comparable , say , to algorithm a . 123 with a 123 tolerance ? if not , i wonder if there is merit in turning the approach around , and tak - ing s123 / 123 to be some convenient ( symmetric ) linear smoother , and dening s to be its square , that is , the smoother obtained by applying s123 / 123 twice ? this obviously changes the prior covariance structure so we would have to revisit that question .
however , there is a clear potential for saving computational to a rough degree of approximation , there can be surprisingly little change; for example , with xi = i , i = 123 ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) n = 123 again , s123 123 is close to s123 ( cid : 123 ) 123 where sdf is the cubic spline smoother with df degrees of freedom; the eigenvalues differ by a max - imum of about 123 , and their outputs are visually
relationship between tolerance on rst neglected term ( cid : 123 ) number of terms included and overall precision for algorithm a . 123 ( cid : 123 ) ! ! is
the sup norm ( cid : 123 )
df = 123
( cid : 123 ) = 123 ( cid : 123 ) 123
df = 123 ( cid : 123 ) = 123 ( cid : 123 ) 123
there are several ideas for improving mcmc per - formance in the literature that could be benecial in the present context .
for example , the modications to backtting introduced at the end of appendix a appear to be related to the ideas of hierarchical cen - tering in normal linear mixed models , of gelfand , sahu and carlin ( 123 ) .
organizing variables in a gibbs sampler into blocks to be simultaneously updated is a common strategy , and often pays off if the blocked variables are highly correlated and the multivariable update is not expensive to implement .
the authors back - tting strategy is precisely an example of this idea .
the variables ( cid : 123 ) fj ( cid : 123 ) xij ( cid : 123 ) ( cid : 123 ) in the jth block will indeed be strongly correlated .
many questions of mcmc strategy about blocking , updating schedules and reparameterization , precisely for the present case of multivariate gaussian models , are discussed by roberts and sahu ( 123 ) , which is strongly recom -
a posteriori , there are of course also strong corre - lations between some variables in different blocks , especially if the predictors are highly correlated themselves , and the anticipated impact of this on mcmc convergence provides another explanation for poor performance in this case .
in other contexts , the correlation between vari - ance parameters and their associated sums - of - squares has proved damaging for mcmc perfor - mance; a commonly successful work - around has been to integrate out the variance parameter ( assuming we are in the usual conjugate setting with normal random effects and inverse gamma hyperpriors ) and then update the random effects by metropolishastings targetted at a t density .
this approach may be useful if the sampler based on equation ( 123 ) , for example , should mix slowly in a
regarding the approach to generalized additive models in section 123 , i have also been a great enthu - siast ( in other contexts ) for metropolishastings based on a gaussian approximation to the full con - ditional .
people tell me this is not a free lunch , how - ever .
whether the resulting chain is even geomet - rically ergodic is , i think , not fully understood .
in similar problems , this depends on the relative size of the tails of the proposal and target densities ( see , e . g . , roberts and tweedie , 123 ) , and so geometric ergodicity may be problematical in skew cases such as gams .
can the authors reassure me ? is the sam - pler provably good ?
finally , brief mention should be made of the required length of mcmc runs .
the authors are
rather silent on this; a passing reference in the dis - cussion seems to suggest that only 123 sweeps were made , after burn - in , in one example .
this seems very few for such a large - dimensional problem , espe -
cially if full posterior distributions are being esti - mated , not just their means .
in all their emphasis on order - n computation , the authors do not allow for any increase in mcmc convergence times with n .
trevor hastie and robert tibshirani
we knew we were taking a chance writing a paper with bayesian in the title , since neither of us work in this area , and we have not paid our dues by attending a valencia meeting .
visions of the bayesianfrequentist battles that have raged over the years in the royal statistical society jour - nals left many a sleepless night .
we breathed a sigh of relief when professor greens discussion arrived; we appear to have got off lightly with a few small raps on the knuckles from a well - respected applied bayesian .
the discussion of professor cook and mr pardoe did no damage either .
just as we began to relax , the computer screen started to quiver , and after a quick degauss , there it was ! the dis - cussion of professor gelfand had arrived in all
we thank all the discussants for their contribu - tions .
all three were complimentary in their open - ing paragraphs about our pragmatic approach to the problem , and we thus feel that our main mis - sion in writing this paper was accomplished .
we will address their comments and concerns separately .
professor green gives a very useful history of mcmc and its use in bayesian function estimation and image smoothing .
he is quite right; we have not spent much time investigating other priors , and realize , especially in spatial statistics and signal processing , that there are many other considerations besides smoothness .
in ( 123 ) below we express the prior in a slightly differ - ent format , which allows perhaps for relatively easy tailoring for function approximation .
the prior assumptions on the individual effects in model ( 123 ) clearly have an important impact on the model .
without priors to pin them down , i and vi are strongly aliased .
consider an individual with bone measurements above the curve near the growth spurt ( e . g . , girl 123 in figure 123 ) .
she could either have a positive value for i ( and vi = 123 ) or a negative value for vi ( and i = 123 ) or values in between for both .
priors can save the day , since we have some idea from many different sources of the distribution of the onset of puberty in girls .
it seems our root - s approximation is not too precise; truth be told , our splus implementation , gibbs . gam ( ) , handles smoothing splines only , where this approximation is not needed .
we are reassured by professor greens endorse - ment of our blocking and efciency strategy out - lined in the appendix , and grateful for his providing more details and references .
unfortunately , we can - not vouch at this time for the geometric ergodicity of our metropolishastings sampler for gams , but expect such results to be forthcoming .
professor cook and mr .
pardoe provide some interesting graphical techniques for model assess - ment .
almost surely an additive model is an approx - imation to the truth , so we are not surprised by the small discrepancy in their figure 123 between the mmp and the additive t .
one has to decide whether the gains obtained by tting a more com - is worth the sacrice in simplicity .
one small concern : since typically smoothers are not projection operators ( ! ssy ! ! sy ! ) , we wonder whether the bands in the gmmp are articially narrow due to double
( based on projections )
professor gelfand reproaches us for our vague description of the prior distribution in section 123 , motivated by smoothing splines .
our goal in the sec - tion was to avoid details and inspire generalities; in fact , we purposely postponed details for smooth - ing splines till the appendix .
professor gelfand then attempts a more precise statement of the prior dis - tribution ( apparently to correct our treatment for smoothing splines in the appendix ) , but does not get it quite right .
for simplicity we assume the n values xi are dis - tinct .
when a smoothing spline is represented in an m - dimensional b - spline basis , with m = n + 123 , the coefcients ( cid : 123 ) have an m - dimensional prior distribu - tion which is both :
corresponding to constant and linear functions
in a two - dimensional
degenerate or singular in a two - dimensional subspace , corresponding to the two natural bound -
so our statement in the the second bulleted item below ( 123 ) in the appendix does have errors , but not the errors claimed by professor gelfand .
( cid : 123 ) has both a degenerate and improper prior distribution .
it appears that professor gelfands ( 123 ) is simply a more precise way of stating our ( 123 ) .
that does not make our ( 123 ) incorrect; we admit it is sloppy , but it appears to be the style used by many bayesian
there is a better way of expressing the prior dis - tribution for smoothing splines or similar methods .
let k = udut be an eigen - decomposition of the nn penalty matrix k ( see ( 123 ) and below in our arti - cle; also green and silverman ( 123 ) for a detailed description of k and an algorithm for computing it ) .
the null - space of k is two - dimensional , and spans the column space of ( cid : 123 ) 123 ( cid : 123 ) x ( cid : 123 ) ( linear functions of x ) .
suppose we partition u = ( cid : 123 ) u123 ( cid : 123 ) u123 ( cid : 123 ) such that u123 spans this null space , and the diagonal matrix d = diag ( cid : 123 ) d123 ( cid : 123 ) d123 ( cid : 123 ) with the 123 123 matrix d123 = 123
u123 is n ( cid : 123 ) n 123 ( cid : 123 ) and represents nonlinear functions in x .
u123 can be represented as a linear transformation of the n m b - spline matrix b , which imposes ( a ) the natural boundary conditions , ( b ) orthogonality to u123 and ( c ) orthogonal columns .
the same trans - formation applied to the m b - spline basis functions bj ( cid : 123 ) x ( cid : 123 ) yields the n 123 demmlerreinch basis func - tions he ( cid : 123 ) x ( cid : 123 ) , e = 123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) n ( and u123 is the matrix of sample realizations of these he ( cid : 123 ) x ( cid : 123 ) ) .
this leads to a representation for the smoothing
f ( cid : 123 ) x ( cid : 123 ) = 123 + 123x + n123 ( cid : 123 )
the parameters are divided into ( cid : 123 ) = ( cid : 123 ) 123 ( cid : 123 ) 123 ( cid : 123 ) and ( cid : 123 ) = ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) n123 ( cid : 123 ) .
the prior on ( cid : 123 ) is noninfor - mative , and ( cid : 123 ) n ( cid : 123 ) 123 ( cid : 123 ) 123d123 ( cid : 123 ) is proper .
this is commonly referred to as a mixed effects model , with ( cid : 123 ) regarded as a xed effect .
this repre - sentation makes explicit the proper and improper parts of the prior for smoothing splines and simi - lar models and avoids the degeneracy due to over - parametrization .
the roughness ( as computed by the second - derivative penalty ) of the he ( cid : 123 ) x ( cid : 123 ) increases with e , and the prior variances on the diagonal of 123d123 decrease toward zero accordingly .
we are not quite sure what is bothering profes - sor gelfand in our section 123
an additive model f = 123 + f123 + + fp in which each fj includes the constant term has a ( p - fold ) degeneracy which j 123 = 123
this can be removed by assuming each f t
is all we are doing .
the priors are easily modied to accommodate this centering ( see ( 123 ) below ) .
pro - fessor gelfand is concerned about the existence of a proper posterior for the additive model ( 123 ) .
this is most easily demonstrated by extending ( 123 ) to the additive case ( lin and zhang , 123 ) :
123 x + p ( cid : 123 )
f ( cid : 123 ) ( cid : 123 ) = 123 + ( cid : 123 ) t
where the hej represent the j different series of demmlerreinch basis functions dened separately for each predictor .
there is an improper prior on the p + 123 ( xed effects ) ( cid : 123 ) , and proper ( and indepen - dent ) normal priors on all the ej , ( cid : 123 ) = ( cid : 123 ) ( cid : 123 ) 123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) p ( cid : 123 ) n this has the same structure as the one - dimensional smoothing spline and has a proper gaussian poste - rior for the same reasons .
the composed functions are simple linear combinations of the parameters and have proper posteriors as well .
123d123 ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) 123
we do sample the constant in algorithm 123; the index j runs from 123 , and step 123 samples the con - stant from a n ( cid : 123 ) 123 ( cid : 123 ) 123 / n ( cid : 123 ) .
possibly professor gelfand was misled by ( 123 ) , which is simply an iterative algorithm ( backtting ) for computing the posterior
although the additive model can be sampled with - out using gibbs sampling , there is an overhead of o ( cid : 123 ) 123 ( cid : 123 ) p + 123 ( cid : 123 ) n123 ( cid : 123 ) computations up front to com - pute the relevant posterior covariances and diag - onalize them .
each realization from the gibbs sam - pler takes o ( cid : 123 ) n ( cid : 123 ) computations and can represent a dramatic savings for large problems .
the authors thank jun liu for reassuring dis - cussions while preparing this rejoinder .
they also thank the editors for arranging this forum .
