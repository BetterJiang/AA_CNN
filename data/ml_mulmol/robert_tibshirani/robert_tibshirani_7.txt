the support vector machine ( svm ) is a widely used tool for classication .
many efcient imple - mentations exist for tting a two - class svm model .
the user has to supply values for the tuning parameters : the regularization cost parameter , and the kernel parameters .
it seems a common prac - tice is to use a default value for the cost parameter , often leading to the least restrictive model .
in this paper we argue that the choice of the cost parameter can be critical .
we then derive an algorithm that can t the entire path of svm solutions for every value of the cost parameter , with essentially the same computational cost as tting one svm model .
we illustrate our algorithm on some examples , and use our representation to give further insight into the range of svm solutions .
keywords : support vector machines , regularization , coefcient path
in this paper we study the support vector machine ( svm ) ( vapnik , 123; scholkopf and smola , 123 ) for two - class classication .
we have a set of n training pairs xi , yi , where xi rp is a p - vector of real - valued predictors ( attributes ) for the ith observation , and yi ( 123 , +123 ) codes its binary response .
we start off with the simple case of a linear classier , where our goal is to estimate a linear decision function
f ( x ) = b 123 + b t x ,
c ( cid : 123 ) 123 trevor hastie , saharon rosset , robert tibshirani and ji zhu .
hastie , rosset , tibshirani and zhu
f ( x ) = +123
f ( x ) = 123
f ( x ) = 123
figure 123 : a simple example shows the elements of a svm model .
the +123 points are solid , the - 123 hollow .
c = 123 , and the width of the soft margin is 123 / ||b || = 123 123 .
two hollow points ( 123 , 123 ) are misclassied , while the two solid points ( 123 , 123 ) are correctly classied , but on the wrong side of their margin f ( x ) = +123; each of these has x i > 123
the three square shaped points ( 123 , 123 , 123 ) are exactly on the margin .
and its associated classier
class ( x ) = sign ( f ( x ) ) .
there are many ways to t such a linear classier , including linear regression , fishers linear discriminant analysis , and logistic regression ( hastie et al . , 123 , chapter 123 ) .
if the training data are linearly separable , an appealing approach is to ask for the decision boundary ( x : f ( x ) = 123 ) that maximizes the margin between the two classes ( vapnik , 123 ) .
solving such a problem is an exercise in convex optimization; the popular setup is
||b ||123 subject to , for each i : yi ( b 123 + xt
b ) 123
a bit of linear algebra shows that boundary .
when the data are not separable , this criterion is modied to
||b || ( b 123 + xt
b ) is the signed distance from xi to the decision
subject to , for each i : yi ( b 123 + xt
||b ||123 +c
b ) 123 x
svm regularization path
y f ( x )
figure 123 : the hinge loss penalizes observation margins y f ( x ) less than +123 linearly , and is indiffer - ent to margins greater than +123
the negative binomial log - likelihood ( deviance ) has the same asymptotes , but operates in a smoother fashion near the elbow at y f ( x ) = 123
here the x i are non - negative slack variables that allow points to be on the wrong side of their soft margin ( f ( x ) = 123 ) , as well as the decision boundary , and c is a cost parameter that controls the amount of overlap .
figure 123 shows a simple example .
if the data are separable , then for sufciently large c the solutions to ( 123 ) and ( 123 ) coincide .
if the data are not separable , as c gets large the solution approaches the minimum overlap solution with largest margin , which is attained for some nite value of c .
alternatively , we can formulate the problem using a loss + penalty criterion ( wahba et al . ,
123; hastie et al . , 123 ) :
( 123 yi ( b 123 + b t xi ) ) + +
the regularization parameter l in ( 123 ) corresponds to 123 / c , with c in ( 123 ) .
here the hinge loss l ( y , f ( x ) ) = ( 123 y f ( x ) ) + can be compared to the negative binomial log - likelihood l ( y , f ( x ) ) = log ( 123 + exp ( y f ( x ) ) ) for estimating the linear function f ( x ) = b 123 + b t x; see figure 123
this formulation emphasizes the role of regularization .
in many situations we have sufcient variables ( e . g .
gene expression arrays ) to guarantee separation .
we may nevertheless avoid the maximum margin separator ( l 123 ) , which is governed by observations on the boundary , in favor of a more regularized solution involving more observations .
this formulation also admits a class of more exible , nonlinear generalizations
l ( yi , f ( xi ) ) + l j ( f ) ,
where f ( x ) is an arbitrary function in some hilbert space h , and j ( f ) is a functional that measures the roughness of f in h .
the nonlinear kernel svms arise naturally in this context .
in this case f ( x ) = b 123 + g ( x ) , and j ( f ) = j ( g ) is a norm in a reproducing kernel hilbert space of functions hk generated by a
hastie , rosset , tibshirani and zhu
radial kernel : c = 123 , g = 123
radial kernel : c = 123 , 123 , g = 123
training error : 123 test error : 123 bayes error : 123
training error : 123 test error : 123 bayes error : 123
figure 123 : simulated data illustrate the need for regularization .
the 123 data points are generated from a pair of mixture densities .
the two svm models used radial kernels with the scale and cost parameters as indicated at the top of the plots .
the thick black curves are the decision boundaries , the dotted curves the margins .
the less regularized t on the right overts the training data , and suffers dramatically on test error .
the broken purple curve is the optimal bayes decision boundary .
positive - denite kernel k ( x , x123 ) .
by the well - studied properties of such spaces ( wahba , 123; ev - geniou et al . , 123 ) , the solution to ( 123 ) is nite dimensional ( even if hk is innite dimensional ) , in this case with a representation f ( x ) = b 123 + ( cid : 123 ) n ik ( x , xi ) .
consequently ( 123 ) reduces to the nite
l ( yi , b 123 +
ik ( xi , x j ) ) +
j123k ( x j , x123
with l the hinge loss , this is an alternative route to the kernel svm; see hastie et al .
( 123 ) for
it seems that the regularization parameter c ( or l ) is often regarded as a genuine nuisance in the community of svm users .
software packages , such as the widely used svmlight ( joachims , 123 ) , provide default settings for c , which are then used without much further exploration .
a recent introductory document ( hsu et al . , 123 ) supporting the libsvm package does encourage grid search for c .
figure 123 shows the results of tting two svm models to the same simulated data set .
the data are generated from a pair of mixture densities , described in detail in hastie et al .
( 123 , chapter 123 ) . 123 the radial kernel function k ( x , x123 ) = exp ( g ||x x123||123 ) was used , with g = 123
the model on the left is more regularized than that on the right ( c = 123 vs c = 123 , 123 , or l = 123 vs l = 123 ) , and
the actual training data and test distribution are available from
svm regularization path
test error curves svm with radial kernel
g = 123
g = 123
c = 123 / l
figure 123 : test error curves for the mixture example , using four different values for the radial kernel parameter g .
small values of c correspond to heavy regularization , large values of c to light regularization .
depending on the value of g , the optimal c can occur at either end of the spectrum or anywhere in between , emphasizing the need for careful selection .
performs much better on test data .
for these examples we evaluate the test error by integration over the lattice indicated in the plots .
figure 123 shows the test error as a function of c for these data , using four different values for the kernel scale parameter g .
here we see a dramatic range in the correct choice for c ( or l = 123 / c ) ; when g = 123 , the most regularized model is called for , and we will see in section 123 that the svm is really performing kernel density classication .
on the other hand , when g = 123 , we would want to choose among the least regularized models .
one of the reasons that investigators avoid extensive exploration of c is the computational in this paper we develop an algorithm which ts the entire path of svm solu - tions ( b 123 ( c ) , b ( c ) ) , for all possible values of c , with essentially the computational cost of tting a single model for a particular value of c .
our algorithm exploits the fact that the lagrange multi - pliers implicit in ( 123 ) are piecewise - linear in c .
this also means that the coefcients b ( c ) are also piecewise - linear in c .
this is true for all svm models , both linear and nonlinear kernel - based svms .
figure 123 on page 123 shows these lagrange paths for the mixture example .
this work was inspired by the related least angle regression ( lar ) algorithm for tting lasso models ( efron et al . , 123 ) , where again the coefcient paths are piecewise linear .
these speedups have a big impact on the estimation of the accuracy of the classier , using a validation dataset ( e . g .
as in k - fold cross - validation ) .
we can rapidly compute the t for each test data point for any and all values of c , and hence the generalization error for the entire validation set as a function of c .
in the next section we develop our algorithm , and then demonstrate its capabilities on a number of examples .
apart from offering dramatic computational savings when computing multiple solu -
hastie , rosset , tibshirani and zhu
tions ( section 123 ) , the nature of the path , in particular at the boundaries , sheds light on the action of the kernel svm ( section 123 ) .
problem setup
we use a criterion equivalent to ( 123 ) , implementing the formulation in ( 123 ) :
b , b 123 subject to 123 yi f ( xi ) x
i 123; f ( x ) = b 123 + b t x .
initially we consider only linear svms to get the intuitive avor of our procedure; we then general - ize to kernel svms .
we construct the lagrange primal function
b t b +
i ( 123 yi f ( xi ) x
and set the derivatives to zero .
this gives
i = 123 ,
i = 123 g
i ( 123 yi f ( xi ) x
i ) = 123 , i = 123
along with the kkt conditions
we see that 123 a i = 123 since no cost is incurred , and a
i 123 , with a
i = 123 when x
i > 123 ( which is when yi f ( xi ) < 123 ) .
also when yi f ( xi ) > 123 ,
when yi f ( xi ) = 123 , a
i can lie between 123 and 123
we wish to nd the entire solution path for all values of l 123
the basic idea of our algorithm is as follows .
we start with l large and decrease it toward zero , keeping track of all the events that occur along the way .
as l decreases , ||b || increases , and hence the width of the margin decreases ( see figure 123 ) .
as this width decreases , points move from being inside to outside the margin .
their i = 123 when they are outside the margin ( yi f ( xi ) > 123 ) .
by continuity , points must linger on the margin ( yi f ( xi ) = 123 ) while their a i ( l ) trajectories are piecewise - linear , which affords a great computational savings : as long as we can establish the break points , all
i = 123 when they are inside the margin ( yi f ( xi ) < 123 ) to a
i decrease from 123 to 123
we will see that the a
i change from a
for readers more familiar with the traditional svm formulation ( 123 ) , we note that there is a simple connection be - i ( 123 , c ) .
we prefer our i ( 123 , 123 ) , and this simplies the denition of the paths we dene .
tween the corresponding lagrange multipliers , a formulation here since our a
i , and hence in that case a
i / l = ca
svm regularization path
values in between can be found by simple linear interpolation .
note that points can return to the margin , after having passed through it .
it is easy to show that if the a
i ( c ) and b ( c ) are piecewise linear in c .
it turns out that b 123 ( c ) is also piecewise linear in c .
we will frequently switch between these two representations .
i ( l ) are piecewise linear in l
, then both a
i ( c ) = ca
we denote by i+ the set of indices corresponding to yi = +123 points , there being n+ = |i+| in total .
likewise for i and n .
our algorithm keeps track of the following sets ( with names inspired by the hinge loss function in figure 123 ) :
e = ( i : yi f ( xi ) = 123 , 123 a l = ( i : yi f ( xi ) < 123 , a r = ( i : yi f ( xi ) > 123 , a
i 123 ) , e for elbow ,
i = 123 ) , l for left of the elbow ,
i = 123 ) , r for right of the elbow .
we need to establish the initial state of the sets dened above .
when l ) , from ( 123 ) b = 123 , and the initial values of b 123 and the a i depend on whether n = n+ or not .
if the classes are balanced , one can directly nd the initial conguration by nding the most extreme points in each class .
we will see that when n 123= n+ , this is no longer the case , and in order to satisfy the constraint ( 123 ) , a quadratic programming algorithm is needed to obtain the initial conguration .
is very large (
in fact , our svmpath algorithm can be started at any intermediate solution of the svm optimiza - tion problem ( i . e .
the solution for any l ) , since the values of a i and f ( xi ) determine the sets l , e and r .
we will see in section 123 that if there is no intercept in the model , the initialization is again trivial , no matter whether the classes are balanced or not .
we have prepared some mpeg movies to illustrate the two special cases detailed below .
the movies can be downloaded at the web site
sufciently large , all the a i = n+ + n .
the initial b 123 ( 123 , 123 ) any value gives the
123 initialization : n = n+ lemma 123 for l same loss ( cid : 123 ) n proof our proof relies on the criterion and the kkt conditions in section 123
since b = 123 , f ( x ) = b 123
to minimize ( cid : 123 ) n i = 123 in ( 123 ) , and hence a i = 123 , i i+ , and hence also a we also have that for these early and large values of l
i , we should clearly restrict b 123 to ( 123 , 123 ) .
for b 123 ( 123 , 123 ) , all the x i = 123
picking one of the endpoints , say b 123 = 123 , causes a
i = 123 , i i , for ( 123 ) to hold .
i > 123 , g
b where b =
now in order that ( 123 ) remain satised , we need that one or more positive and negative examples hit the elbow simultaneously .
hence as l decreases , we require that i yi f ( xi ) 123 or
yi " b t xi
+ b 123# 123
hastie , rosset , tibshirani and zhu
c = 123 / l
c = 123 / l
figure 123 : the initial paths of the coefcients in a small simulated dataset with n = n+ .
we see the zone of allowable values for b 123 shrinking toward a xed point ( 123 ) .
the vertical lines indicate the breakpoints in the piecewise linear coefcient paths .
b 123 123
b t xi
b 123 123
b t xi
for all i i+
for all i i .
b t xi and i = argminii
b t xi ( for simplicity we assume that these are pick i+ = argmaxii+ unique ) .
then at this point of entry and beyond for a while we have a i , and f ( xi+ ) = 123 and f ( xi ) = 123
this gives us two equations to solve for the initial point of entry l 123 and b 123 , with
i+ = a
b t xi+ b t xi
b 123 = b t xi+ + b t xi b t xi+ b t xi ! .
figure 123 ( left panel ) shows a trajectory of b 123 ( c ) as a function of c , for a small simulated data set .
these solutions were computed directly using a quadratic - programming algorithm , using a
svm regularization path
figure 123 : the initial paths of the coefcients in a case where n < n+ .
all the n points are misclassied , and start off with a margin of 123
the a i remain constant until one of the points in i reaches the margin .
the vertical lines indicate the breakpoints in the piecewise linear b ( c ) paths .
note that the a i ( c ) are not piecewise linear in c , but rather in l = 123 / c .
hastie , rosset , tibshirani and zhu
predened grid of values for l nature of this path .
the breakpoints were found using our exact - path algorithm .
the arbitrariness of the initial values is indicated by the zig - zag
123 initialization : n+ > n in this case , when b = 123 , the optimal choice for b 123 is 123 , and the loss is ( cid : 123 ) n also require that ( 123 ) holds .
lemma 123 with b ( a ) = ( cid : 123 ) n
i ) = argmina
||b ( a ) ||123
i ( 123 , 123 ) for i i+ , a
however , we
i = 123 for i i , and ( cid : 123 ) i , and b = b / l
then for some l 123 we have that for all l > l 123 , a proof the lagrange dual corresponding to ( 123 ) is obtained by substituting ( 123 ) ( 123 ) into ( 123 ) ( hastie et al . , 123 , equation 123 ) :
, with b = ( cid : 123 ) n
since we start with b = 123 , b 123 = 123 , all the i points are misclassied , and hence we will have i = 123i i , and hence from ( 123 ) ( cid : 123 ) n i = 123n .
this latter sum will remain 123n for a while as b grows away from zero .
this means that during this phase , the rst term in the lagrange dual is 123l ||b ( a ) ||123 , and since we maximize the dual , this proves constant; the second term is equal to 123
we now establish the starting point l 123 and b 123 when the a
i start to change .
let b be the xed
coefcient direction corresponding to a
i ( as in ( 123 ) ) :
there are two possible scenarios :
there exist two or more elements in i+ with 123 < a
i ( 123 , 123 ) i i+ .
i < 123 , or
consider the rst scenario ( depicted in figure 123 ) , and suppose a i = argminii margin , we can nd
i+ ( 123 , 123 ) ( on the margin ) .
let b t xi .
then since the point i+ remains on the margin until an i point reaches its
b t xi+ b t xi
identical in form to to ( 123 ) , as is the corresponding b 123 to ( 123 ) .
for the second scenario , it is easy to see that we nd ourselves in the same situation as in i = 123 must reach the margin b t xi , where
section 123a point from i and one of the points in i+ with a simultaneously .
hence we get an analogous situation , except with i+ = argmaxii 123 + is the subset of i+ with a
i = 123
svm regularization path
the development so far has been in the original feature space , since it is easier to visualize .
it is easy to see that the entire development carries through with kernels as well .
in this case f ( x ) = b 123 + g ( x ) , and the only change that occurs is that ( 123 ) is changed to
jy jk ( xi , x j ) , i = 123 ,
jy j / l using the notation in ( 123 ) .
j ( l ) = a our initial conditions are dened in terms of expressions b t xi+ , for example , and again it is
easy to see that the relevant quantities are
jy jk ( xi+ , x j ) ,
where the a
i are all 123 in section 123 , and dened by lemma 123 in section 123 .
hereafter we will develop our algorithm for this more general kernel case .
the path
the algorithm hinges on the set of points e sitting at the elbow of the loss function i . e on the margin .
these points have yi f ( xi ) = 123 and a i ( 123 , 123 ) .
these are distinct from the points r to the right of the elbow , with yi f ( xi ) > 123 and a i = 123 , and those points l to the left with yi f ( xi ) < 123 and i = 123
we consider this set at the point that an event has occurred .
the event can be either :
the initial event , which means 123 or more points start at the elbow , with their initial values of
a point from l has just entered e , with its value of a 123
a point from r has reentered e , with its value of a
i initially 123
i initially 123
one or more points in e has left the set , to join either r or l .
whichever the case , for continuity reasons this set will stay stable until the next event occurs , i must change from 123 to 123 or vice versa .
since all points in e
since to pass through e , a points a have yi f ( xi ) = 123 , we can establish a path for their a
event 123 allows for the possibility that e becomes empty while l is not .
if this occurs , then the kkt condition ( 123 ) implies that l is balanced w . r . t .
+123s and - 123s , and we resort to the initial condition as in section 123 .
we use the subscript ` to index the sets above immediately after the `th event has occurred .
123 and l ` be the values of these parameters at the point of entry .
123 = l `b `
suppose |e`| = m , and let a likewise f ` is the function at this point .
for convenience we dene a 123 = l
123 , and hence a
i , b `
f ( x ) =
jk ( x , x j ) + a 123 ! ,
hastie , rosset , tibshirani and zhu
for l ` > l > l `+123 we can write
f ( x ) = ( cid : 123 ) f ( x ) l " ( cid : 123 )
f ` ( x ) ( cid : 123 ) +
j ) y jk ( x , x j ) + ( a 123 a
123 ) + l ` f ` ( x ) # .
the second line follows because all the observations in l` have their a
i = 123 , and those in r` have .
since each of the m points xi e` are to stay at the elbow , we have
i = 123 , for this range of l
l " ( cid : 123 )
j , from ( 123 ) we have
j ) yiy jk ( xi , x j ) + yi ( a 123 a
123 ) + l `# = 123 , i e` .
jyiy jk ( xi , x j ) + yid 123 = l ` l , i e` .
furthermore , since at all times ( cid : 123 ) n
i = 123 , we have that
j = 123
equations ( 123 ) and ( 123 ) constitute m + 123 linear equations in m + 123 unknowns d
j , and can be solved .
` the m m matrix with i jth entry yiy jk ( xi , x j ) for i and j in e` , we have from
denoting by k
d + d 123y` = ( l ` l ) 123 ,
where y` is the m vector with entries yi , i e` .
from ( 123 ) we have
d = 123
we can combine these two into one matrix equation as follows
then ( 123 ) and ( 123 ) can be written
a` = ( cid : 123 ) 123
d a = ( cid : 123 ) d 123
d ( cid : 123 ) , and a`d a = ( l ` l ) 123a .
if a` has full rank , then we can write
ba = a`
hence for l `+123 < l < l ` , the a
j ( l ` l ) b j , j ( 123 ) e` .
j = a j for points at the elbow proceed linearly in l
f ( x ) =
h f ` ( x ) h` ( x ) i + h` ( x ) ,
from ( 123 ) we have
svm regularization path
h` ( x ) = ( cid : 123 )
y jb jk ( x , x j ) + b123
thus the function itself changes in a piecewise - inverse manner in l
if a` does not have full rank , then the solution paths for some of the a
i are not unique , and more care has to be taken in solving the system ( 123 ) .
this occurs , for example , when two training observations are identical ( tied in x and y ) .
other degeneracies can occur , but rarely in practice , such as three different points on the same margin in r123
these issues and some of the related updating and downdating schemes are an area we are currently researching , and will be reported elsewhere .
123 finding l `+123 the paths ( 123 ) ( 123 ) continue until one of the following events occur :
one of the a
i for i e` reaches a boundary ( 123 or 123 ) .
for each i the value of l
for which this
occurs is easily established from ( 123 ) .
one of the points in l ` or r ` attains yi f ( xi ) = 123
from ( 123 ) this occurs for point i at
l = l ` ( cid : 123 ) f ` ( xi ) h` ( xi ) yi h` ( xi ) ( cid : 123 ) .
by examining these conditions , we can establish the largest l < l ` for which an event occurs , and hence establish l `+123 and update the sets .
one special case not addressed above is when the set e becomes empty during the course of the algorithm .
in this case , we revert to an initialization setup using the points in l .
it must be the case that these points have an equal number of +123s as - 123s , and so we are in the balanced situation as in
by examining in detail the linear boundary in examples where p = 123 , we observed several
different types of behavior :
if |e | = 123 , than as l decreases , the orientation of the decision boundary stays xed , but the
margin width narrows as l decreases .
if |e | = 123 or |e | = 123 , but with the pair of points of opposite classes , then the orientation
typically rotates as the margin width gets narrower .
if |e | = 123 , with both points having the same class , then the orientation remains xed , with
the one margin stuck on the two points as the decision boundary gets shrunk toward it .
if |e | 123 , then the margins and hence f ( x ) remains xed , as the a
i ( l ) change .
this implies
that h` = f ` in ( 123 ) .
in the separable case , we terminate when l becomes empty .
at this point , all the x and further movement increases the norm of b unnecessarily .
in the non - separable case , l
runs all the way down to zero .
for this to happen without f blowing up in ( 123 ) , we must have f ` h` = 123 , and hence the boundary and margins remain
i in ( 123 ) are zero ,
hastie , rosset , tibshirani and zhu
xed at a point where ( cid : 123 )
i is as small as possible , and the margin is as wide as possible subject to
123 computational complexity
at any update event ` along the path of our algorithm , the main computational burden is solving the system of equations of size m` = |e`| .
while this normally involves o ( m123 ` ) computations , since e`+123 differs from e` by typically one observation , inverse updating / downdating can reduce the computations to o ( m123 ` ) .
the computation of h` ( xi ) in ( 123 ) requires o ( nm` ) computations .
beyond that , several checks of cost o ( n ) are needed to evaluate the next move .
g = 123
g = 123
figure 123 : the elbow sizes |e`| as a function of l
, for different values of the radial - kernel parameter
the vertical lines show the positions used to compare the times with libsvm .
we have explored using partitioned inverses for updating / downdating the solutions to the elbow equations ( for the nonsingular case ) , and our experiences are mixed .
in our r implementations , the computational savings appear negligible for the problems we have tackled , and after repeated updating , rounding errors can cause drift .
at the time of this publication , we in fact do not use updating at all , and simply solve the system each time .
we are currently exploring numerically stable ways for managing these updates .
although we have no hard results , our experience so far suggests that the total number l
moves is o ( k min ( n+ , n ) ) , for k around 123 123; hence typically some small multiple c of n .
if the average size of e` is m , this suggests the total computational burden is o ( cn123m + nm123 ) , which is similar to that of a single svm t .
our r function svmpath computes all 123 steps in the mixture example ( n+ = n = 123 , radial kernel , g = 123 ) in 123 ( 123 ) secs on a pentium 123 , 123ghz linux machine; the svm function ( using the optimized code libsvm , from the r library e123 ) takes 123 ( 123 ) seconds to compute the solution at 123 points along the path .
hence it takes our procedure about 123% more time to compute the entire path , than it costs libsvm to compute a typical single solution .
svm regularization path
we often wish to make predictions at new inputs .
we can also do this efciently for all values , because from ( 123 ) we see that ( modulo 123 / l ) , these also change in a piecewise - linear fashion .
hence we can compute the entire t path for a single input x in o ( n ) calculations , plus an additional o ( nq ) operations to compute the kernel evaluations ( assuming it costs o ( q ) operations to compute k ( x , xi ) ) .
in this section we look at three examples , two synthetic and one real .
we examine our running mix - ture example in some more detail , and expose the nature of quadratic regularization in the kernel feature space .
we then simulate and examine a scaled - down version of the p ( cid : 123 ) n problemmany more inputs than samples .
despite the fact that perfect separation is possible with large margins , a heavily regularized model is optimal in this case .
finally we t svm path models to some microar - ray cancer data .
123 mixture simulation in figure 123 we show the test - error curves for a large number of values of l , and four different values for the radial kernel .
these l ` are in fact the entire collection of change points as described in section 123
for example , for the second panel , with g = 123 , there are 123 change points .
figure 123 ( upper plot ) shows the paths of all the a i ( l ) , as well as ( lower plot ) a few individual examples .
an mpeg movie of the sequence of models can be downloaded from the rst authors website .
we were at rst surprised to discover that not all these sequences achieved zero training errors on the 123 training data points , at their least regularized t .
in fact the minimal training errors , and the corresponding values for g are summarized in table 123
it is sometimes argued that the implicit
table 123 : the number of minimal training errors for different values of the radial kernel scale pa - rameter g , for the mixture simulation example .
also shown is the effective rank of the 123 123 gram matrix kg .
feature space is innite dimensional for this kernel , which suggests that perfect separation is always possible .
the last row of the table shows the effective rank of the kernel gram matrix k ( which we dened to be the number of singular values greater than 123 ) .
this 123 123 matrix has elements ki , j = k ( xi , x j ) , i , j = 123 , .
in general a full rank k is required to achieve perfect separation .
similar observations have appeared in the literature ( bach and jordan , 123; williams and seeger , 123 ) .
this emphasizes the fact that not all features in the feature map implied by k are of equal stature; many of them are shrunk way down to zero .
alternatively , the regularization in ( 123 ) and ( 123 ) penalizes unit - norm features by the inverse of their eigenvalues , which effectively annihilates some , depending on g .
small g implies wide , at kernels , and a suppression of wiggly , rough functions .
hastie , rosset , tibshirani and zhu
figure 123 : ( upper plot ) the entire collection of piece - wise linear paths a
i ( l ) , i = 123 , .
, n , for the is plotted on the log - scale .
( lower plot ) paths for 123 selected
mixture example .
note : l
is not on the log scale .
svm regularization path
writing ( 123 ) in matrix form ,
l ( y , kq ) +
q t kq ,
we reparametrize using the eigen - decomposition of k = udut .
let kq = uq where q = dut q then ( 123 ) becomes
l ( y , uq ) +
q t d123q .
now the columns of u are unit - norm basis functions ( in r123 ) spanning the column space of k;
g = 123
g = 123
figure 123 : the eigenvalues ( on the log scale ) for the kernel matrices kg corresponding to the four values of g as in figure 123
the larger eigenvalues correspond in this case to smoother eigenfunctions , the small ones to rougher .
the rougher eigenfunctions get penalized ex - ponentially more than the smoother ones .
for smaller values of g , the effective dimension of the space is truncated .
from ( 123 ) we see that those members corresponding to near - zero eigenvalues ( the elements of the diagonal matrix d ) get heavily penalized and hence ignored .
figure 123 shows the elements of d for the four values of g .
see hastie et al .
( 123 , chapter 123 ) for more details .
123 p ( cid : 123 ) n simulation
the svm is popular in situations where the number of features exceeds the number of observations .
gene expression arrays are a leading example , where a typical dataset has p > 123 , 123 while n < 123
here one typically ts a linear classier , and since it is easy to separate the data , the optimal marginal classier is the de facto choice .
we argue here that regularization can play an important role for these kinds of data .
hastie , rosset , tibshirani and zhu
we mimic a simulation found in marron ( 123 ) .
we have p = 123 and n = 123 , with a 123 - 123 split of + and - class members .
the xi j are all iid realizations from a n ( 123 , 123 ) distribution , except for the rst coordinate , which has mean +123 and - 123 in the respective classes . 123 the bayes classier in this case uses only the rst coordinate of x , with a threshold at 123
the bayes risk is 123 .
figure 123 summarizes the experiment .
we see that the most regularized models do the best here , not the maximal margin classier .
although the most regularized linear svm is the best in this example , we notice a disturbing aspect of its endpoint behavior in the top - right plot .
although b is determined by all the points , the threshold b 123 is determined by the two most extreme points in the two classes ( see section 123 ) .
this can lead to irregular behavior , and indeed in some realizations from this model this was the case .
for values of l larger than the initial value l 123 , we saw in section 123 that the endpoint behavior depends on whether the classes are balanced or not .
in either case , as l increases , the error converges to the estimated null error rate nmin / n .
this same objection is often made at the other extreme of the optimal margin; however , it typi - cally involves more support points ( 123 points on the margin here ) , and tends to be more stable ( but still no good in this case ) .
for solutions in the interior of the regularization path , these objections no longer hold .
here the regularization forces more points to overlap the margin ( support points ) , and hence determine its orientation .
included in the gures are regularized linear discriminant analysis and logistic regression mod - els ( using the same l ` sequence as the svm ) .
both show similar behavior to the regularized svm , having the most regularized solutions perform the best .
logistic regression can be seen to assign weights pi ( 123 pi ) to observations in the tting of its coefcients b and b 123 , where
123 + eb 123b t xi
is the estimated probability of +123 occurring at xi ( hastie and tibshirani , 123 , e . g . ) .
since the decision boundary corresponds to p ( x ) = 123 , these weights can be seen to die down
in a quadratic fashion from 123 / 123 , as we move away from the boundary .
the rate at which the weights die down with distance from the boundary depends on ||b ||; the
smaller this norm , the slower the rate .
it can be shown , for separated classes , that the limiting solution ( l 123 ) for the regularized logistic regression model is identical to the svm solution : the maximal margin separator ( rosset et al . , 123 ) .
not surprisingly , given the similarities in their loss functions ( figure 123 ) , both regularized svms and logistic regression involve more or less observations in determining their solutions , depending on the amount of regularization .
this involvement is achieved in a smoother fashion by logistic
123 microarray classication
we illustrate our algorithm on a large cancer expression data set ( ramaswamy et al . , 123 ) .
there are 123 training tumor samples and 123 test tumor samples , spanning 123 common tumor classes that
here we have one important feature; the remaining 123 are noise .
with expression arrays , the important features
typically occur in groups , but the total number p is much larger .
svm regularization path
l = 123 / c
l = 123 / c
figure 123 : p ( cid : 123 ) n simulation .
( top left ) the training data projected onto the space spanned by the ( known ) optimal coordinate 123 , and the optimal margin coefcient vector found by a non - regularized svm .
we see the large gap in the margin , while the bayes - optimal classier ( vertical red line ) is actually expected to make a small number of errors .
( top right ) the same as the left panel , except we now project onto the most regularized svm coefcient vector .
this solution is closer to the bayes - optimal solution .
( lower left ) the angles between the bayes - optimal direction , and the directions found by the svm ( s ) along the regularized path .
included in the gure are the corresponding coefcients for regularized lda ( r ) ( hastie et al . , 123 , chapter 123 ) and regularized logistic regression ( l ) ( zhu and hastie , 123 ) , using the same quadratic penalties .
( lower right ) the test errors corresponding to the three paths .
the horizontal line is the estimated bayes rule using only the rst coordinate .
hastie , rosset , tibshirani and zhu
account for 123% of new cancer diagnoses in the u . s . a .
there are 123 , 123 genes for each sample .
hence p = 123 , 123 and n = 123
we denote the number of classes by k = 123
a goal is to build a classier for predicting the cancer class of a new sample , given its expression values .
we used a common approach for extending the svm from two - class to multi - class classica -
fit k different svm models , each one classifying a single cancer class ( +123 ) versus the rest
let ( f l
a test observation x .
123 ( x ) ,
k ( x ) ) be the vector of evaluations of the tted functions ( with parameter l ) at
classify cl
( x ) = argmaxk f l
other , more direct , multi - class generalizations exist ( rosset et al . , 123; weston and watkins , 123 ) ; although exact path algorithms are possible here too , we were able to implement our ap - proach most easily with the one vs all strategy above .
figure 123 shows the results of tting this family of svm models .
shown are the training error , test error , as well as 123 - fold balanced cross - validation . 123 the training error is zero everywhere , but both the test and cv error increase sharply when the model is too regularized .
the right plot shows similar results using quadratically regularized multinomial regression ( zhu and hastie , 123 ) .
although the least regularized svm and multinomial models do the best , this is still not very
with fourteen classes , this is a tough classication problem .
it is worth noting that :
the 123 different classication problems are very lop - sided; in many cases 123 observations in one class vs the 123 others .
this tends to produce solutions with all members of the small class on the boundary , a somewhat unnatural situation .
for both the svm and the quadratically regularized multinomial regression , one can reduce the logistics by pre - transforming the data .
if x is the n p data matrix , with p ( cid : 123 ) n , let its singular - value decomposition be udvt .
we can replace x by the n n matrix xv = ud = r and obtain identical results ( hastie and tibshirani , 123 ) .
the same transformation v is applied to the test data .
this transformation is applied once upfront , and the transformed data is used in all subsequent analyses ( i . e .
k - fold cross - validation as well ) .
no intercept and kernel density classication
here we consider a simplication of the models ( 123 ) and ( 123 ) where we leave out the intercept term b 123
it is easy to show that the solution for g ( x ) has the identical form as in ( 123 ) :
jy jk ( x , x j ) .
however , f ( x ) = g ( x ) ( or f ( x ) = b t x in the linear case ) , and we lose the constraint ( 123 ) due to the
this also adds considerable simplication to our algorithm , in particular the initial conditions .
by balanced we mean the 123 cancer classes were represented equally in each of the folds; 123 folds were used to
accommodate this balance , since the class sizes in the training set were multiples of 123
svm regularization path
figure 123 : misclassication rates for cancer classication by gene expression measurements .
the left panel shows the the training ( lower green ) , cross - validation ( middle black , with standard errors ) and test error ( upper blue ) curves for the entire svm path .
although the cv and test error curves appear to have quite different levels , the region of interesting behavior is the same ( with a curious dip at about l = 123 ) .
seeing the entire path leaves no guesswork as to where the region of interest might be .
the right panel shows the same for the regularized multiple logistic regression model .
here we do not have an exact path algorithm , so a grid of 123 values of l
is used ( on a log scale ) .
it is easy to see that initially a
, and hence all points are in l .
this is true whether or not n = n+ , unlike the situation when an intercept is present ( section 123 ) .
i = 123i , since f ( x ) is close to zero for large l
with f ( x ) = ( cid : 123 ) n
j=123 y jk ( x , x j ) , the rst element of e is i = argmaxi | f ( xi ) | , with l 123 =
| f ( xi ) | .
for l ( l 123 , ) , f ( x ) = f ( x ) / l
the linear equations that govern the points in e are similar to ( 123 ) :
d = ( l ` l ) 123 ,
we now show that in the most regularized case , these no - intercept kernel models are actually performing kernel density classication .
initially , for l > l 123 , we classify to class +123 if f ( x ) / l > 123 ,
hastie , rosset , tibshirani and zhu
else to class - 123
f ( x ) = ( cid : 123 )
k ( x , x j ) ( cid : 123 )
k ( x , x j )
= n n+
k ( x , x j )
+h+ ( x ) p h ( x ) .
k ( x , x j ) !
is relaxed , the a
in other words , this is the estimated bayes decision rule , with h+ the kernel density ( parzen window ) estimate for the + class , p + the sample prior , and likewise for h ( x ) and p .
a similar observation is made in scholkopf and smola ( 123 ) , for the model with intercept .
so at this end of the regular - ization scale , the kernel parameter g plays a crucial role , as it does in kernel density classication .
increases , the behavior of the classier approaches that of the 123 - nearest neighbor classier .
for very small g , or in fact a linear kernel , this amounts to closest centroid classication .
i ( l ) will change , giving ultimately zero weight to points well within their own class , and sharing the weights among points near the decision boundary .
in the context of nearest neighbor classication , this has the avor of editing , a way of thinning out the training set retaining only those prototypes essential for classication ( ripley , 123 ) .
all these interpretations get blurred when the intercept b 123 is present in the model .
for the radial kernel , a constant term is included in span ( k ( x , xi ) ) n
123 , so it is not strictly necessary to include one in the model .
however , it will get regularized ( shrunk toward zero ) along with all the other coefcients , which is usually why these intercept terms are separated out and freed from regularization .
adding a constant b123 to k ( , ) will reduce the amount of shrinking on the intercept ( since the amount of shrinking of an eigenfunction of k is inversely proportional to its eigenvalue; see section 123 ) .
for the linear svm , we can augment the xi vectors with a constant element b , and then t the no - intercept model .
the larger b , the closer the solution will be to that of the linear svm
our work on the svm path algorithm was inspired by earlier work on exact path algorithms in other settings .
least angle regression ( efron et al . , 123 ) shows that the coefcient path for the sequence of lasso coefcients ( tibshirani , 123 ) is piecewise linear .
the lasso solves the following regularized linear regression problem ,
( yi b 123 xt
b ) 123 + l |b | ,
l ; the larger l
where |b | = ( cid : 123 ) j| is the l123 norm of the coefcient vector .
this l123 constraint delivers a sparse solution vector b l are zero , the remainder shrunk toward zero .
in fact , any model with an l123 constraint and a quadratic , piecewise quadratic , piecewise linear , or mixed quadratic and linear loss function , will have piecewise linear coefcient paths , which can be calculated exactly and efciently for all values of l ( rosset and zhu , 123 ) .
these models include ,
, the more elements of b
a robust version of the lasso , using a huberized loss function .
svm regularization path
the l123 constrained support vector machine ( zhu et al . , 123 ) .
the svm model has a quadratic constraint and a piecewise linear ( hinge ) loss function .
this i are piecewise
leads to a piecewise linear path in the dual space , hence the lagrange coefcients a
other models that would share this property include the e - insensitive svm regression model
quadratically regularized l123 regression , including exible models based on kernels or smooth -
of course , quadratic criterion + quadratic constraints also lead to exact path solutions , as in the classic case of ridge regression , since a closed form solution is obtained via the svd .
however , these paths are nonlinear in the regularization parameter .
for general non - quadratic loss functions and l123 constraints , the solution paths are typically piecewise non - linear .
logistic regression is a leading example .
in this case , approximate path - following algorithms are possible ( rosset , 123 ) .
the general techniques employed in this paper are known as parametric programming via active sets in the convex optimization literature ( allgower and georg , 123 ) .
the closest we have seen to our work in the literature employ similar techniques in incremental learning for svms ( fine and scheinberg , 123; cauwenberghs and poggio , 123; decoste and wagstaff , 123 ) .
these authors do not , however , construct exact paths as we do , but rather focus on updating and downdating the solutions as more ( or less ) data arises .
diehl and cauwenberghs ( 123 ) allow for updating the parameters as well , but again do not construct entire solution paths .
the work of pontil and verri ( 123 ) recently came to our notice , who also observed that the lagrange multipliers for the margin vectors change in a piece - wise linear fashion , while the others remain constant .
the svmpath has been implemented in the r computing environment ( contributed library svmpath
at cran ) , and is available from the rst authors website .
the authors thank jerome friedman for helpful discussions , and mee - young park for assisting with some of the computations .
they also thank two referees and the associate editor for helpful comments .
trevor hastie was partially supported by grant dms - 123 from the national science foundation , and grant ro123 - eb123 - 123 from the national institutes of health .
tibshirani was partially supported by grant dms - 123 from the national science foundation and grant ro123 - eb123 - 123 from the national institutes of health .
