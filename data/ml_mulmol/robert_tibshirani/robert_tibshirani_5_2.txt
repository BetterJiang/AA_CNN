we propose a method for the classication of more than two classes , from high - dimensional fea - tures .
our approach is to build a binary decision tree in a top - down manner , using the optimal margin classier at each split .
we implement an exact greedy algorithm for this task , and compare its performance to less greedy procedures based on clustering of the matrix of pairwise margins .
we compare the performance of the margin tree to the closely related all - pairs ( one versus one ) support vector machine , and nearest centroids on a number of cancer microarray data sets .
we also develop a simple method for feature selection .
we nd that the margin tree has accuracy that is competitive with other methods and offers additional interpretability in its putative grouping of the
keywords : maximum margin classier , support vector machine , decision tree , cart
we consider the problem of classifying objects into two or more classes , from a set of features .
our main application area is the classication of cancer patient samples from gene expression measure -
when the number of classes k is greater than two , maximum margin classiers do not gen - eralize easily .
various approaches have been suggested , some based on the two - class classier ( one - versus - all and one - versus one or all pairs ) , and others modifying the support vector loss function to deal directly with more than two classes ( weston and watkins , 123; lee et al . , 123; rosset et al . , 123 ) .
these latter proposals have a nice generalization of the maximum margin property of the two class support vector classier .
statnikov et al .
( 123 ) contains a comparison of different support vector approaches to classication from microarray gene expression cancer data sets .
while these methods can produce accurate predictions , they lack interpretability .
in particular , with a large number of classes , the investigator may want not only a classier but also a meaningful organization of the classes .
in this paper we propose a tree - based maximum margin classier .
figure 123 illustrates our idea .
there are three classes and two features , as shown in the top left panel .
we seek the line that partitions the classes into two groups , that has the maximum margin .
( the margin is the minimum distance to the decision line among all of the data points . )
c ( cid : 123 ) 123 robert tibshirani and trevor hastie .
tibshirani and hastie
the best line is shown in the top right panel , splitting class 123 from classes 123 and 123
we then focus just on classes 123 and 123 , and their maximum margin classier is shown in the bottom left .
the overall top - down classier is summarized by the binary tree shown in the bottom right panel .
we employ strategies like this for larger numbers of classes , producing a binary decision tree
with a maximum margin classier at each junction in the tree .
in section 123 we give details of the margin tree classier .
section 123 shows the application of the margin tree to a number of cancer microarray data sets .
for construction of the tree , all of the classiers in the margin tree use all of the features ( genes ) .
in section 123 we discuss approaches to feature selection .
finally in section 123 we have some further comments and a discussion of related work in the literature .
the margin tree classier
denote the gene expression proles by x j = ( x123 j; x123 j; : : : xp j ) for j = 123;123; : : : n samples falling into one of k classes .
the features ( genes ) are indexed by i = 123;123; : : : p .
consider rst the case of k = 123 classes , c123 and c123
the class outcome is denoted by y j = ( cid : 123 ) 123
the maximum margin classier is dened by the constant b 123 and the weight vector b with
i = 123 that maximizes the gap between the classes , or the margin .
formally ,
( b 123;b ) = argmax ( c )
where y j ( b 123 + ( cid : 123 )
ixi j ) ( cid : 123 ) c 123 j :
the achieved margin m = 123 ( cid : 123 ) c .
in the examples of this paper , p > n so that all classes are separable and m > 123
we have some discussion of the non - separable case in section 123
now suppose we have k > 123 classes .
we consider three different strategies for constructing the tree .
these use different criteria for deciding on the best partition of the classes into two groups at each juncture .
having settled on the partition , we use the maximum margin classier between the two groups of classes , for future predictions .
let m ( j; k ) be the maximum margin between classes j and k .
also , let g123; g123 be groups of classes , and let m ( g123; g123 ) denote the maximum margin between the groups .
that is , m ( g123; g123 ) is the maximum margin between two hyper - classes : all classes in g123 and all classes in g123
finally , denote a partition by p = fg123; g123g .
then we consider three approaches for splitting a node in the decision tree :
( a ) greedy : maximize m ( g123; g123 ) over all partitions p .
( b ) single linkage : find the partition p yielding the largest margin m123 so that min m ( j123; j123 ) ( cid : 123 ) m123
for j123; j123 123 gk; k = 123;123 and min m ( j123; j123 ) ( cid : 123 ) m for j123 123 g123; j123 123 g123
( c ) complete linkage : find the partition p yielding the largest margin m123 so that max m ( j123; j123 ) ( cid : 123 )
m123 for j123; j123 123 gk; k = 123;123 and max m ( j123; j123 ) ( cid : 123 ) m123 for j123 123 g123; j123 123 g123
the greedy method nds the partition that maximizes the resulting margin over all possible partitions .
although this may seem prohibitive to compute for a large number of classes , we derive an exact , reasonably fast algorithm for this approach ( details below ) .
the second and third methods require some explanation .
they are derived from the bottom up 123 ( cid : 123 ) margin classiers for
( as opposed to top - down ) clustering methods .
each one requires just the ( cid : 123 ) k
figure 123 : simple illustration of a margin tree .
there are three classes shown in the top left panel .
the largest margin is between class 123 and ( 123 , 123 ) , with the optimal classier shown on the top right .
then we separate class 123 from 123 , in the bottom left .
these top - down splits are summarized in the margin tree in the bottom right .
tibshirani and hastie
each pair of classes .
single linkage clustering successively merges groups based on the minimum distance between any pair of items in each of the group .
complete linkage clustering does the same , but using the maximum distance .
now having built a clustering tree bottom - up , we can interpret each split in the tree in a top - down manner , and that is how criteria ( b ) and ( c ) above were derived .
in particular it is easy to see that the single and complete linkage problems are solved by single and complete linkage agglomerative clustering , respectively , applied to the margin matrix m ( j123; j123 ) .
note that we are applying single or complete linkage clustering to the classes of objects c j , while one usually applies clustering to individual objects .
the greedy method focuses on the form of the nal classier , and tries to optimize that classi - cation at each stage .
note that the greedy method cares only about the distance between classes in the different partitions , and not about the distance between classes within the same partition .
both the single linkage and complete linkage methods take into account both the between and within partition distances .
we will also see in the next section that the complete linkage method can be viewed as an approximation to the greedy search .
figure 123 shows a toy example that illustrates the difference between the greedy and complete linkage algorithms .
there are six classes with circular distributions .
the greedy algorithm splits off group 123 , 123 , and 123 in succession , and then splits off 123 , 123 , 123 as a group .
this is summarized in the bottom left panel .
the complete linkage algorithm in the bottom right panel instead groups 123 , 123 and 123 together and 123 , 123 , and 123 together .
the complete linkage tree is more balanced and hence may be more useful biologically .
in the experiments in this paper we nd that :
( cid : 123 ) all three methods produce about the same test set accuracy , and about the same as the all - pairs
maximum margin classier .
( cid : 123 ) the complete linkage approach gives more balanced trees , that may be more interpretable that those from the other two methods; the single linkage and greedy methods tend to produce long stringy trees that usually split off one class at a time at each branch .
the complete linkage method is also considerably faster to compute than the greedy method .
thus the complete linkage margin tree emerges as our method of choice .
it requires computation of 123 ( cid : 123 ) support vector classiers for each pair of classes for the complete linkage clustering and then for the nal tree , one computation of a support vector classier for each node in the tree ( at most k and typically ( cid : 123 ) log123 ( k ) classiers . )
123 an exact algorithm for the greedy criterion
a key fact is
m ( g123; g123 ) ( cid : 123 ) minfm ( j123; j123 ) ; j123 123 g123; j123 123 g123g :
that is , the margin between two groups of classes is less than or equal to the smallest margin between any pair of classes , one chosen from each group .
now suppose we seek a partition p with margin m .
rather than enumerate all possible partitions ( and their associated maximum margin classiers ) , we can speed up the computation by constructing the complete linkage clustering tree , and collapsing all nodes at height m .
we know that all classes in any collapsed node must be on the same side of the decision plane , since each class has margin
figure 123 : a toy example illustrating the difference between the greedy and complete linkage algo - rithms .
there are six classes with circular distributions ( top panel ) .
the greedy algorithm splits off groups 123 , 123 , and 123 in succession , and then splits off 123 , 123 , 123 as a group .
this is sum - marized in the bottom left panel .
the complete linkage algorithm ( bottom right panel ) instead groups 123 , 123 and 123 together , and 123 , 123 , and 123 together .
for example , the margin be - tween classes 123 and 123 is 123 , while that between 123 and 123 is less : 123 .
the height in each plot is the margin corresponding to each join .
at least m with every other class in that node .
hence we need only consider partitions that keep the collapsed nodes intact .
we summarize the algorithm below :
exact computation of the best greedy split
construct the complete linkage clustering tree based on the margin matrix m ( j123; j123 ) .
starting with all classes at the top of the tree , nd the partition of each individual class versus the rest , and also the partition that produces two classes in the complete linkage tree ( that is , make a horizontal cut in the tree to produce two classes ) .
let m123 be the largest margin achieved amongst all of these competitors .
tibshirani and hastie
cut the complete linkage tree at height m123 , and collapse all nodes at that height .
consider all partitions of all classes that keep the collapsed nodes intact , and choose the one
that gives maximal margin m .
this procedure nds the partition of the classes that yields the maximum margin .
we then apply this procedure in a top - down recursive manner , until the entire margin tree is grown .
this algorithm is exact in that it nds the best split at each node in the top - down tree building process .
this is because the best greedy split must be among the candidates considered in step 123 , since as mentioned above , all classes in a collapsed node must be on the same side of the decision plane .
but it is not exact in a global sense , that is , it does not nd the best tree among all possible
note that if approximation ( 123 ) is an equality , then the complete linkage tree is itself the greedy
margin classier solution .
this follows because m = m123 in the above algorithm .
as an example , consider the problem in figure 123
we cut the complete linkage tree to produce two nodes ( 123 , 123 , 123 ) and ( 123 , 123 , 123 ) .
we compute the achieved margin for this split and also the margin for partitions ( 123 ) vs .
( 123 , 123 , 123 , 123 , 123 ) , ( 123 ) vs .
( 123 , 123 , 123 , 123 , 123 ) etc .
we nd that the largest margin corresponds to ( 123 ) vs .
( 123 , 123 , 123 , 123 , 123 ) , and so this becomes the rst split in the greedy tree .
we then repeat this process on the daughter subtrees : in this case , just ( 123 , 123 , 123 , 123 , 123 ) .
thus we consider ( 123 ) vs .
( 123 , 123 , 123 , 123 ) , ( 123 ) vs ( 123 , 123 , 123 , 123 ) etc , as well as the complete linkage split ( 123 , 123 ) vs ( 123 , 123 , 123 ) .
the largest margin is achieved by the latter , so me make that split and continue the process .
123 example : 123 cancer microarray data
as an example , we consider the microarray cancer data of ramaswamy et al .
( 123 ) : there are 123 , 123 genes and 123 samples in 123 classes .
the authors provide training and test sets of size 123 and 123 respectively .
the margin trees are shown in figure 123
the length of each ( non - terminal ) arm corresponds to the margin that is achieved by the classier at that split .
the nal classiers yielded 123 , 123 and 123 errors , respectively on the test set .
by comparison , the all - pairs support - vector classier yielded 123 errors and the nearest centroid classier had 123 errors .
nearest centroid classication ( e . g . , tibshirani et al . , 123 ) computes the standardized mean feature vector in each class , and then assigns a test sample to the class with the closest centroid .
later we do a more comprehensive comparison of all of these methods .
we note that the greedy and single linkage margin tree are stringy , with each partition separating off just one class in most cases .
the complete linkage tree is more balanced , producing some potentially useful subgroupings of the cancer classes .
in this example , full enumeration of the partitions at each node would have required computation of 123 , 123 two class maximum margin classiers .
the exact greedy algorithm required only 123 such classiers .
in general the cost savings can vary , depending on the height m123 of the initial cut in the complete linkage tree .
figure 123 displays the margins that were achieved by each method at their collection of splits .
we see that the complete method gives larger margins than the other methods .
the largest margin achieved is about 123 , 123 , corresponding to the split between class cns and collerectal and so on . .
this is larger than the margin between leukemia and the rest at the top of the greedy tree .
this shows that the greediness of the exact algorithm can hurt its overall performance in nding large
figure 123 : margin trees for the 123 - tumor cancer data of ramaswamy et al .
( 123 )
tibshirani and hastie
figure 123 : 123 tumor cancer data : margins achieved by each method over the collection of splits .
the number of points represented in each boxplot is the number of splits in the corresponding
svm ( all pairs ) mt ( greedy ) mt ( single ) mt ( complete )
table 123 : number of disagreements on the test set , for different margin tree - building methods .
table 123 shows the number of times each classier disagreed on the test set .
the number of disagreements is quite large .
however the methods got almost all of the same test cases correct ( over 123% overlap ) , and the disagreements occur almost entirely for test cases in which all methods got the prediction wrong .
figure 123 shows the test errors at each node of the complete linkage tree , for the 123 tumor data
application to other cancer microarray data sets
we applied the methods described earlier to the seven microarray cancer data sets shown in table 123
in each case we randomly sampled 123=123rds of the data to form a training set , and the balance of the data became the test set .
the sampling was done in a stratied way to retain balance of the class sizes .
this entire process was repeated 123 times , and the mean and standard errors of the test set misclassication rates are shown in table 123
the nearest centroid method is as described in
figure 123 : test errors for the 123 tumor data set using the complete linkage approach .
error rates at each decision junction is shown : notice that the errors tend to increase farther down the
tibshirani et al .
( 123 ) and uses no shrinkage for feature selection : we discuss feature selection in section 123
we see that for problems involving more than 123 or 123 classes , the one - versus - one support vector classier and the margin tree methods sometimes offer an advantage over nearest centroids .
the margin tree methods are all very similar to each other and the one - versus - one support vector
tibshirani and hastie
small round blue cell tumors
pomeroy et al .
( 123 ) 123 alizadeh et al .
( 123 ) 123 khan et al .
( 123 ) 123 munagala et al .
( 123 ) staunton et al .
( 123 ) su et al .
( 123 ) 123 ramaswamy et al .
( 123 )
table 123 : summary of data sets for comparative study
svm ( ovo ) mt ( single ) mt ( complete ) mt ( greedy )
table 123 : mean test error rates ( standard errors ) over 123 simulations , from various cancer microarray data sets .
svm ( ovo ) is the support vector machine , using the one - versus - one approach; each pairwise classier uses a large value for the cost parameter , to yield the maximal margin classier; mt are the margin tree methods , with different tree - building strategies .
feature selection
the classiers at each junction of the margin tree each use all of the features ( genes ) .
for in - terpretability it would be clearly benecial to reduce the set of genes to a smaller set , if one can improve , or at least not signicantly worsen , its accuracy .
how one does this depends on the goal .
the investigator probably wants to know which genes have the largest contribution in each classier .
for this purpose , we rank each gene by the absolute value of its coefcient b j .
then to form a reduced classier , we simply set to zero the rst nk coefcients at split k in the margin tree .
we call this hard - thresholding .
how do we choose nk ? it is not all clear that nk should be the same for each tree split .
for example we might be able to use fewer genes near the top of the tree , where the margins between the classes is largest .
our strategy is as follows .
we compute reduced classiers at each tree split , for a range of values of nk , and for each , the proportion of the full margin achieved by the classier .
then we use a common value a for the margin proportion throughout the tree .
this strategy allows the classiers at different parts of the tree to use different number of genes .
in real applications , we use tenfold cross - validation to estimate the best value for a
figure 123 shows the result of applying hard thresholding to the 123 - class cancer data .
the plot is varied .
the average number of genes at each of
shows the test error as the margin proportion a
mean number of genes
figure 123 : 123 tumor data set : test errors for reduced numbers of genes .
the 123 tree junctions is shown along the horizontal axis .
we see that average number of genes can be reduced from about 123;123 to about 123;123 without too much loss of accuracy .
but beyond that , the test error increases .
figure 123 shows a more successful application of the feature selection procedure .
the gure shows the result for one training / test split of the 123 class data ( 123 , 123 genes ) described earlier .
with no feature selection the margin tree ( left panel ) achieves 123=123 errors , the same as the one - versus one support vector machine .
hard - thresholding ( middle panel ) also yields 123 errors , with an average of just 123 genes per split .
the margin proportion is shown at the top of the plot .
the right panel shows the number of genes used as a function of the height of the split in the tree , for margin proportion
the feature selection procedure described above is simple and computationally fast .
note that having identied a set of features to be removed , we simply set their coefcients b i to zero .
for reasons of speed and interpretability , we do not recompute the maximum margin classier in the subspace of the remaining features ( we do however recompute the classier cutpoint , equal to mid - point between the classes ) .
how much do we lose in this approximation ? for the top and bottom splits in the tree of figure 123 , figure 123 shows the margins achieved by the maximum margin classier ( black points ) and the approximation ( blue points ) as the numbers of genes is reduced .
the approx - imation gives margins remarkably close to the optimal margin until the number of genes drop below 123
also shown in the gure are the margins achieved by recursive feature elimination ( rfe ) ( guyon et al . , 123 ) .
this is a full backward stepwise procedure , in which successive coefcients are dropped and the optimal margin classier for the remaining features is recomputed .
we see that rfe offers only a small advantage , when the number of genes becomes quite small .
tibshirani and hastie
mean number of genes
height in tree
figure 123 : results for 123 tumor data set .
the left panel shows the margin tree using complete link - age; the test errors from hard - thresholding are shown in the middle , with the margin indicated along the top of the plot; for the tree using a = 123 : 123 , the right panel shows the resulting number of genes at each split in the tree , as a function of the height of that split .
123 results on real data sets
table 123 shows the results of applying the margin tree classier ( complete linkage ) with feature selection , on the data sets described earlier .
tenfold cross - validation was used to choose the margin fraction parameter a , and both cv error and test set error are reported in the table .
also shown are results for nearest shrunken centroids ( tibshirani et al . , 123 ) , using cross - validation to choose the shrinkage parameter .
this method starts with centroids for each class , and then shrinks them towards the overall centroid by soft - thresholding .
we see that ( a ) hard thresholding generally improves upon the error rate of the full margin tree; ( b ) margin trees outperform nearest shrunken centroids on the whole , but not in every case .
in some cases , the number of genes used has dropped substantially; to get smaller number of genes one could look more closely at the cross - validation curve , to check how quickly it was rising .
if two genes are correlated and both contribute too the classier , they might both remain in the model , under the above scheme .
one the other hand , if there is a set of many highly correlated genes that contribute , their coefcients will be diluted and they might all be removed .
hence it might be desirable to to select among the genes in a more aggressive fashion .
there are a number of approaches one might try here , for example the recursive feature elimination of guyon et al .
( 123 ) , mentioned above .
one could also try the l123 - norm support - vector machine ( see , for example , zhu et al . , 123 ) , but this is also quite slow to compute .
another approach would be to apply the lasso ( tibshirani , 123 ) .
all of these methods would be worth trying; however they also
top split in tree
number of genes
bottom split in tree
number of genes
figure 123 : results for the 123 tumor data set : margins achieved by the maximum margin classier using simple hard thresholding without recomputing the weights ( black points ) , with re - computation ( blue points ) and recursive feature elimination ( red points ) .
top panel refers to the top split in the margin tree; bottom panel refers to the bottom split .
suffer from interpretability issues .
in particular , the best classier with say 123 genes might have only a few genes in common with the best classier with 123 genes .
the hard thresholding method described above does not suffer from this drawback .
it gives a single ranked list of weights for all genes , for the classier at each node of the tree .
tibshirani and hastie
nearest shrunken centroids
margin tree with selection
# genes used
table 123 : cv and test error rates for nearest shrunken centroids and margin trees with feature se - lection by simple hard thresholding .
the rightmost column reports the average number of genes used at each split in the tree .
the margin - tree method proposed here seems well suited to high - dimensional problems with more than two classes .
it has prediction accuracy competitive with multiclass support vector machines and nearest centroid methods , and provides a hierarchical grouping of the classes .
all of the classiers considered here use a linear kernel , that is , they use the original input features .
the construction of margin tree could also be done using other kernels , using the support vector machine framework .
the greedy algorithm and linkage algorithms will work without change .
however in the p > n case considered in this paper , a linear svm can separate the data , so the utility of a non - linear kernel is not clear .
and importantly , the ability to select features would be lost with a non - linear svm .
we have restricted attention to the case p > n in which the classes are separable by a hyperplane .
when p < n and the classes may not be separable , our approach can be modied to work in principle but may not perform well in practice .
the nodes of the clustering tree will be impure , that is contain observations from more than one class .
hence a larger treeone with more leaves than there are classesmight be needed to effectively classify the observations .
in addition to the papers on the multiclass support vector classier mentioned earlier , there is other work related to our paper .
the decision tree methods of breiman et al .
( 123 ) ( cart ) and quinlan ( 123 ) use top - down splitting to form a binary tree , but use other criteria ( different from the margin ) for splitting .
with p ( cid : 123 ) n , splits on individual predictors can get unwieldy and exhibit high variance .
the use of linear combination splits is closer to our approach , but again it is not designed for large numbers of predictors .
it does not produce a partition of the classes but rather operates on
closely related to carts linear combination splits is the fact approach of loh and vanichse - takul ( 123 ) and the followup work of kim and loh ( 123 ) .
these use fishers linear discriminant function to make multi - way splits of each node of the tree .
while linear discriminants might per - form similarly to the support vector classier , the latter has the maximum margin property which we have exploited in this paper .
probably the closest paper to our work is that of vural and dy ( 123 ) , who use a top - down binary tree approach .
they use k - means clustering of the class means to divide the points in two groups at each node , before applying a support vector classier .
bennett and blue ( 123 ) investigate
decision trees with support vector classiers at the node , but do not discuss adaptive construction of the tree topology .
park and hastie ( 123 ) propose hierarchical classication methods using nearest centroid classiers at each node .
they use clustering methods to nd the topology of the tree , and their paper has some ideas in common with this one .
in fact , the mixture model used for each merged node gives a decision boundary that is similar to the support vector classier .
however the maximum margin classier used here seems more natural and the overall performance of the margin tree is better .
we would like the thank the referees for helpful comments that led to improvements in this manuscript .
tibshirani was partially supported by national science foundation grant dms - 123 and national institutes of health contract n123 - hv - 123
trevor hastie was partially supported by grant dms - 123 from the national science foundation , and grant 123r123 ca 123 - 123 from the national institutes of health .
