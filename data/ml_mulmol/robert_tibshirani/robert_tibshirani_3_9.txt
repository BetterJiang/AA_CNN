abstract .
for high dimensional supervised learning problems , often using problem specic assumptions can lead to greater ac - curacy .
for problems with grouped covariates , which are believed to have sparse eects both on a group and within group level , we introduce a regularized model for linear regression with ( cid : 123 ) 123 and ( cid : 123 ) 123 penalties .
we discuss the sparsity and other regularization prop - erties of the optimal t for this model , and show that it has the desired eect of group - wise and within group sparsity .
we propose an algorithm to t the model via accelerated generalized gradi - ent descent , and extend this model and algorithm to convex loss functions .
we also demonstrate the ecacy of our model and the eciency of our algorithm on simulated data .
keywords : penalize , regularize , regression , model , nesterov
consider the usual linear regression framework .
our data consists of an n response vector y , and an n by p matrix of features , x .
in many recent applications we have p >> n : a case where standard linear regression fails .
to combat this , tibshirani ( 123 ) regularized the problem by bounding the ( cid : 123 ) 123 norm of the solution .
this approach , known as the lasso , minimizes
123 + ||||123
it nds a solution with few nonzero entries in .
suppose , further , that our predictor variables were divided into m dierent groups for example in gene expression data these groups may be gene pathways , or factor level indicators in categorical data .
we are given these group memberships and rather than just sparsity in we would like a solution which uses only a few of the groups .
yuan and lin ( 123 ) proposed
+ ( 123 )
pl|| ( l ) ||123 + ||||123
123noah simon , jerome friedman , trevor hastie , and rob tibshirani
the group lasso criterion for this problem; the problem is
where x ( l ) is the submatrix of x with columns corresponding to the predictors in group l , ( l ) the coecient vector of that group and pl is the length of ( l ) .
this criterion exploits the non - dierentiability of || ( l ) ||123 at ( l ) = 123; setting groups of coecients to exactly 123
the sparsity of the solution is determined by the magnitude of the tuning parameter .
if the size of each group is 123 , this gives us exactly the regular lasso solution .
while the group lasso gives a sparse set of groups , if it includes a group in the model then all coecients in the group will be nonzero .
some - times we would like both sparsity of groups and within each group for example if the predictors are genes we would like to identify partic - ularly important genes in pathways of interest .
toward this end we focus on the sparse - group lasso
where ( 123 , 123 ) a convex combination of the lasso and group lasso penalties ( = 123 gives the group lasso t , = 123 gives the lasso t ) .
be - fore we move on , we would like to dene consistent terminology for our 123 types of sparsity : we use groupwise sparsity to refer to the number of groups with at least one nonzero coecient , and within group spar - sity to refer to the number of nonzero coecients within each nonzero group .
occasionally , we will also use the term overall sparsity to refer to the total number of nonzero coecients irregardless of grouping .
in this paper we discuss properties of this criterion , rst proposed in our unpublished note , friedman et al . .
we discuss using this idea for logistic and cox regression , and develop an algorithm to solve the original problem and extensions to other loss functions .
our algorithm is based on nesterovs method for generalized gradient descent .
by employing warm starts we solve the problem along a path of constraint values .
we demonstrate the ecacy of our objective function and algo - rithm on real and simulated data , and we provide a publically available r implementation of our algorithm in the package sgl .
this paper is the continuation of friedman et al . , a brief note on the criterion .
a sparse - group lasso
this criterion was also discussed in zhou et al .
( 123 ) .
they applied it to snp data for linear and logistic regression with an emphasis on variable selection and found that it performed well .
in section 123 we develop the criterion and discuss some of its proper - ties .
we present the details of the algorithm we use to t this model in section 123
in section 123 we extend this model to any log - concave likelihood in particular to logistic regression and the cox proportional in section 123 we discuss when we might expect our model to outperform the lasso and group lasso , and give some real data examples .
in section 123 we show the ecacy of our model and the eciency of our algorithm on simulated data .
+ ( 123 )
returning to the usual regression framework we have an n response vector y , and an n by p covariate matrix x broken down into m sub - matrices , x ( 123 ) , x ( 123 ) , .
, x ( m ) , with each x ( l ) an n by pl matrix , where pl is the number of covariates in group l .
we choose to minimize pl|| ( l ) ||123 + ||||123
for the rest of the paper we will supress the
penalty term for ease of notation .
to add it back in , simply replace all future ( 123 ) by pk ( 123 ) .
one might note that this looks very similar to the elastic net penalty proposed by zou and hastie ( 123 ) .
it diers because the || ||123 penalty is not dierentiable at 123 , so some groups are completely zeroed out .
however , as we show shortly , within each nonzero group it gives an elastic net t ( though with the || ||123 penalty parameter a function of the optimal || ( k ) ||123 ) .
pl in the ( cid : 123 ) m
the objective in ( 123 ) is convex , so the optimal solution is characterized by the subgradient equations .
we consider these conditions to better understand the properties of .
for group k , ( k ) must satisfy
x ( l ) ( l )
= ( 123 ) u + v
where u and v are subgradients of || ( k ) ||123 and || ( k ) ||123 respectively , evaluated at ( k )
if ( k ) ( cid : 123 ) = 123 ( u : ||u||123 123 ) if ( k ) = 123
123noah simon , jerome friedman , trevor hastie , and rob tibshirani
( vj : |vj| 123 ) if ( k )
j = 123
( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) s ( cid : 123 ) x ( k ) ( cid : 123 ) r ( k ) / n , ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) 123 ( 123 )
with a little bit of algebra we see that the subgradient equations are satised with ( k ) = 123 if where r ( k ) the partial residual of y , subtracting all group ts other than group k
r ( k ) = y ( cid : 123 )
x ( l ) ( l )
and with s ( ) the coordinate - wise soft thresholding operator :
( s ( z , ) ) j = sign ( zj ) ( |zj| ) + .
in comparison , the usual group lasso has ( k ) = 123 if
on a group sparsity level the two act similarly , though the sparse - group lasso adds univariate shrinkage before checking if a group is nonzero .
the subgradient equations can also give insight into the sparsity within a group which is at least partially nonzero .
if ( k ) ( cid : 123 ) = 123 then the subgradient conditions for a particular ( k ) = ( 123 )
x ( l ) ( l )
this is satised for ( k )
with r ( k , i ) = r ( k ) ( cid : 123 )
i = 123 if j ( cid : 123 ) =i x ( k )
( k ) the partial residual of y subtracting .
this is the same
all other covariate ts , excluding only the t of x ( k ) condition for a covariate to be inactive as in the regular lasso .
nonzero , more algebra gives us that ( k )
i / n + ( 123 ) / || ( k ) ||123
these are elastic net type conditions as in friedman et al .
( 123 ) .
un - like the usual elastic net , the proportional shrinkage here is a function of the optimal solution , net , 123 = ( 123 ) / || ( k ) ||123
formula ( 123 ) suggests
a sparse - group lasso
a cyclical coordinate - wise algorithm to t the model within group .
we tried this algorithm in a number of incarnations and found it inferior in both timing and accuracy to the algorithm discussed in section 123
puig et al .
( 123 ) and foygel and drton ( 123 ) t the group lasso and sparse - group lasso respectively by explicitly solving for || ( k ) ||123 and ap - plying ( 123 ) in a cyclic fashion for each group with all other groups xed .
this requires doing matrix calculations , which may be slow for larger group sizes , so we take a dierent approach .
from the subgradient conditions we see that this model promotes the desired sparsity pattern .
furthermore , it regularizes nicely within each group giving an elastic net - like solution .
in this section we describe how to t the sparse - group lasso using blockwise descent to solve within each group we employ an accel - erated generalized gradient algorithm with backtracking .
because our penalty is separable between groups , blockwise descent is guaranteed to converge to the global optimum .
within group solution .
we choose a group k to minimize over , and consider the other group coecients as xed we can ignore penalties corresponding to coecients in these groups .
our minimiza - tion problem becomes , nd ( k ) to minimize
we denote our unpenalized loss function by
123 + ( 123 ) || ( k ) ||123 + || ( k ) ||123
( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) r ( k ) x ( k ) ( k ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) 123
( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) r ( k ) x ( k ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) 123
( cid : 123 ) ( r ( k ) , ) =
note , we are using here to denote the coecients in only group k .
the modern approach to gradient descent is to consider it as a majorization minimization scheme .
we majorize our loss function , centered at a point 123 by ( 123 ) ( cid : 123 ) ( r ( k ) , ) ( cid : 123 ) ( r ( k ) , 123 ) + ( 123 ) ( cid : 123 ) ( cid : 123 ) ( r ( k ) , 123 ) + where t is suciently small that the quadratic term dominates the hessian of our loss; note , the gradient in ( cid : 123 ) ( r ( k ) , 123 ) is only taken over group k .
minimizing this function would give us our usual gradient step ( with stepsize t ) in the unpenalized case .
adding our penalty to ( 123 ) majorizes the objective ( 123 ) .
m ( ) = ( cid : 123 ) ( r ( k ) , 123 ) + ( 123 ) ( cid : 123 ) ( cid : 123 ) ( r ( i ) , 123 ) +
123noah simon , jerome friedman , trevor hastie , and rob tibshirani our goal now is to nd to minimize m ( ) .
minimizing m ( ) is equiv - alent to minimizing
( 123 ) m ( ) =
123 + ( 123 ) ||||123 +||||123
combining the subgradient conditions with basic algebra , we get that = 123 if
and otherwise satises
||s ( 123 t ( cid : 123 ) ( r ( k ) , 123 ) , t ) ||123 t ( 123 )
|| ||123 = ( cid : 123 ) ||s ( 123 t ( cid : 123 ) ( r ( k ) , 123 ) , t ) ||123 t ( 123 ) ( cid : 123 )
= s ( 123 t ( cid : 123 ) ( r ( k ) , 123 ) , t ) .
taking the norm of both sides we see that
if we plug this into ( 123 ) , we see that our generalized gradient step ( ie .
the solution to ( 123 ) ) is
s ( 123t ( cid : 123 ) ( r ( k ) , 123 ) , t ) .
||s ( 123 t ( cid : 123 ) ( r ( k ) , 123 ) , t ) ||123
if we iterate ( 123 ) , and recenter each pass at ( 123 ) new = ( ) old , then we will converge on the optimal solution for ( k ) given xed values of the other coecient vectors .
if we apply this per block , and cyclically iterate through the blocks we will converge on the overall optimum .
for ease of notation in the future we let u ( 123 , t ) denote our update
u ( 123 , t ) =
||s ( 123 t ( cid : 123 ) ( r ( k ) , 123 ) , t ) ||123
s ( 123t ( cid : 123 ) ( r ( k ) , 123 ) , t ) .
note that in our case ( linear regression )
( cid : 123 ) ( r ( k ) , 123 ) = x ( k ) ( cid : 123 ) r ( k ) / n .
algorithm overview .
this algorithm is a sequence of nested
( 123 ) ( outer loop ) cyclically iterate through the groups; at each
group ( k ) execute step 123
( 123 ) check if the groups coecients are identically 123 , by seeing if
they obey ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) s ( cid : 123 ) x ( k ) ( cid : 123 ) r ( k ) , ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) 123 ( 123 ) .
a sparse - group lasso
( 123 ) ( inner loop ) until convergence iterate :
if not , within the group apply step 123 ( a ) update the center by ( k ) ( b ) update ( k ) from eq ( 123 ) , by ( k ) u ( , t )
this is the basic idea behind our algorithm .
meier et al .
( 123 ) have a similar approach to t the group lasso for generalized linear mod - els .
for a convergence threshold of , in the worst - case scenario within each group this algorithm requires o ( 123 / ) steps to converge .
however , recent work in rst order methods have shown vast improvements to
gradient descent by a simple modication .
as seen in nesterov ( 123 ) we can improve this class of algorithm to o ( 123 / ) , by including a mo - mentum term ( known as accelerated generalized gradient descent ) .
in practice as well , we have seen signicant empirical improvement by including momentum in our gradient updates .
we have also included step size optimization , which we have found important as often the lipschitz constant for a problem of interest is unknown .
the actual algorithm that we employ changes the inner loop to the following :
123 , step size t = 123 , and
( inner loop ) start with ( k , l ) = ( k , l ) = ( k ) counter l = 123
repeat the following until convergence ( 123 ) optimize step size by iterating t = 123 t until
( 123 ) update gradient g by g = ( cid : 123 ) ( cid : 123 ) r ( k ) , ( k , l ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) u ( cid : 123 ) ( k , l ) , t ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( k , l ) ( cid : 123 ) + g ( cid : 123 ) ( l , t ) + ( k , l+123 ) u ( cid : 123 ) ( k , l ) , t ( cid : 123 )
( 123 ) update ( k , l ) by
( 123 ) update the center via a nesterov step by
( k , l+123 ) ( k , l ) +
( 123 ) set l = l + 123
where ( l , t ) is the change between our old solution and new solution
( l , t ) = u ( cid : 123 ) ( k , l ) , t ( cid : 123 ) ( k , l )
our choice of 123 in step 123 was somewhat arbitrary; any value in ( 123 , 123 ) will work .
this is very similar to the basic generalized gradient al - gorithm the major dierences are steps 123 and 123
in 123 , we search for a t such that in our direction of descent , the majorization scheme
123noah simon , jerome friedman , trevor hastie , and rob tibshirani
still holds .
in 123 we apply nesterov - style momentum updates this allows us to leverage some higher order information while only calcu - lating gradients .
while these momentum updates are unintuitive they have shown great theoretical and practical speedup in a large class of
pathwise solution .
usually , we will be interested in models for more than one amount of regularization .
one could solve over a 123 dimensional grid of and values , however we found this to be com - putationally impractical , and to do a poor job of model selection .
in - stead , we x the mixing parameter and compute solutions for a path of values ( as regulates the degree of sparsity ) .
we begin the path with suciently large to set = 123 , and decrease until we are near the unregularized solution .
by using the previous solution as the start position for our algorithm at the next - value along the path , we make this procedure ecient for nding a pathwise solution .
notice that in eq 123 if
||s ( cid : 123 ) x ( l ) ( cid : 123 ) y / n , ( cid : 123 ) ||123 < fact that for a xed ||s ( cid : 123 ) x ( l ) ( cid : 123 ) y / n , ( cid : 123 ) ||123
for all l , then = 123 minimizes the objective .
we can leverage the 123 pl ( 123 ) 123 is piecewise quadratic in to nd the smallest l for each group that sets that groups coecients to 123
thus , we begin our path with
max = maxi i
this is the exact value at which the rst coecient enters the model .
we choose min to be some small fraction of max ( default value is 123 in our implementation ) and log - linearly interpolate between these two for other values of on this path .
we do not have a theoretically optimal value for the optimal value would need to be a function of the number of covariates and group sizes among other things .
in practice for problems where we expect strong overall sparsity and would like to encourage grouping we have used = 123 with reasonable success ( this was used in our simulated data in section 123 ) .
in contrast , if we expect strong group - wise sparsity , but only mild sparsity within group we have used = 123 ( an example of this is given in section 123 ) .
that said , dierent problems will possibly be better served by dierent values of and in practice some exploration may be needed .
simple extensions .
we can also use this algorithm to t either the lasso or group lasso penalty : setting = 123 or = 123
for the group lasso the only change is to remove the soft thresholding in update ( 123 )
u ( 123 , t ) =
a sparse - group lasso
||123 + t ( cid : 123 ) ( r ( i ) , 123 ) ||123
( cid : 123 ) 123 t ( cid : 123 ) ( r ( i ) , 123 ) ( cid : 123 ) .
for the lasso penalty , the algorithm changes a bit more .
there is no longer any grouping , so there is no need for an outer group loop
u ( 123 , t ) = s ( 123 t ( cid : 123 ) ( y , 123 ) , t )
which we iterate , updating 123 at each step .
without backtracking , this is just the nesta algorithm in lagrange form as described in becker et al .
( 123 ) .
extensions to other models
with little eort we can extend the sparse - group penalty to other models .
if the likelihood function , l ( ) , for the model of interest is log - concave then for the sparse - group lasso we minimize
( cid : 123 ) ( ) + ( 123 )
( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( l ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) 123 + ||||123
where ( cid : 123 ) ( ) = 123 / n log ( l ( ) ) .
two commonly used cases , which we in - clude in our implementation , are logistic regression and the cox model for survival data .
for logistic regression we have y , an n - vector of binary responses , and x , an n by p covariate matrix divided into m groups , x ( 123 ) , .
, x ( m ) .
in this case the sparse - group lasso takes the form
log ( cid : 123 ) 123 + exp ( cid : 123 ) x ( cid : 123 )
i ( cid : 123 ) ( cid : 123 ) + yix ( cid : 123 )
for cox regression our data is a covariate matrix , x ( again with sub - matrices by group ) , an n - vector y corresponding to failure / censoring times and an n - vector indicating failure or censoring for each obser - vation ( i = 123 if observation i failed , while i = 123 if censored ) .
here the sparse - group lasso corresponds to
j ( cid : 123 ) x ( cid : 123 )
where d is the set of failure indices , ri is the set of indices , j , with yj yi ( those still at risk at failure time i ) .
123noah simon , jerome friedman , trevor hastie , and rob tibshirani
fitting extensions .
fitting the model in these cases is straight - forward .
as before we use blockwise descent .
within each block our algorithm is nearly identical to the squared error case .
while before
( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) r ( k ) x ( k ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) ( cid : 123 ) 123
( cid : 123 ) ( r ( k ) , ) =
that form is only applicable with squared error loss .
we dene ( cid : 123 ) k ( ( k ) , ( k ) ) to be our unpenalized loss function , ( cid : 123 ) ( ) , considered as a function of only ( k ) , with the rest of the coecients , ( k ) , xed .
in the case of square error loss , this is exactly ( cid : 123 ) ( r ( k ) , ( k ) ) .
from here , we can use the algorithm in section 123 only replacing every instance of ( cid : 123 ) ( r ( k ) , ) by ( cid : 123 ) k ( ( k ) , ( k ) ) .
we would like to note that although the algorithm employed is straightforward , due to the curvature of these losses , in some cases our algorithm scales poorly ( eg .
cox regression ) .
overlap group lasso .
the sparse - group lasso can also be con - sidered as a special case of a group lasso model which allows overlap in groups ( in this case many groups would be size 123 ) .
in the more general overlap case one may see strange behavior if a variable in the overlap of many groups is included in a model then all of those groups will need to be active .
jacob et al .
( 123 ) give a very nice x for this issue by slightly reformulating the problem , but this more general framework is beyond the scope of our paper .
in this section we discuss when one might expect good performance from the sparse - group lasso , and when another tool might be preferable .
one common statistical scenario is regression with categorical predic - tors .
for predictors with few levels it is reasonable to use the group lasso sparsity within group is unnecessary as groups are small .
as the number of levels per predictor rises , it becomes more likely that even for predictors which we include , many of the levels may not be informative .
the sparse - group lasso will take this into account , set - ting the coecients for many levels equal to 123 even in nonzero groups .
at the other extreme , few predictors each with many levels , groupwise sparsity often proves unhelpful and one may see the best performance with the lasso ( for example , if there are only 123 groups and one is active in the true model , this is still 123% of groups active ) .
along similar lines , often we run regression in a setting where the predictors have a natural grouping .
we mentioned gene pathways be - fore and will expand on it here .
in many ( if not all ) genetic conditions ,
a sparse - group lasso
genes do not function ( or fail to function ) independently .
if a number of genes in a given pathway all seem moderately successful at predict - ing outcome , we would like to up - weight this evidence over similarly predictive genes in dierent pathways .
however , we also do not believe that every gene in an active pathway is necessarily indicated in the genetic condition .
the sparse - group lasso is potentially useful for this scenario it nds pathways of interest and , from them , selects driving genes .
furthermore , it shrinks the estimated eects of driving genes within a group toward one another .
to further investigate this , we have compared the sparse - group lasso to the lasso and group lasso on two real data examples with gene ex - pression data .
our rst dataset was the colitis data of burczynski et al .
( 123 ) .
there were 123 total patients , 123 with colitis ( 123 crohns pa - tients + 123 ulcerative colitis patients ) and 123 healthy controls .
each pa - tient had expression data for 123 genes run on an aymetrix u123a microarray .
these genes were grouped into genesets using cytoge - netic position data ( the c123 set from subramanian et al .
( 123 ) ) .
of the original 123 genes only 123 of these genes were found in the c123 geneset the others were removed from the analysis .
the c123 set contains 123 cytogenetic bands , each averaging about 123 genes ( from out dataset ) .
we chose 123 observations at random and used these to t our models .
we used the remaining 123 observations as a test set .
because there were a large number of small pathways we chose = 123 for the sparse - group lasso model .
each of the 123 models was t for a path of 123 - values with min = 123max ( this value was chosen because the peak in validation accuracy occured at the end of the path for min = 123max ) .
in figure 123 , we see that the lasso performed slightly better than the group lasso and sparse - group lasso with a 123% correct classication rate at its peak to the 123% of the sparse - group lasso and 123% for the group lasso .
if we look into the solution slightly more we see that , at its peak , the lasso chose to include 123 genes from totally dierent cyto - genetic bands , whereas the sparse - group lasso included 123 genes from only 123 cytogenetic bands , and the group lasso included all 123 genes from 123 bands .
in this example the group lasso and sparse - group lasso chose nearly the same predictors ( the sparse - group lasso included an additional group ) .
all of these bands were very small ( the largest had 123 genes , 123 was the median ) , and the sparse - group lasso actually did not employ any within group sparsity .
from our results , we can see that the sparse - group lasso ( with our choice of genesets ) was not ideal
123noah simon , jerome friedman , trevor hastie , and rob tibshirani
figure 123
classication accuracy on 123 test samples for colitis data .
all models were t for a path of 123 - values with min = 123max .
for the sgl , = 123 was used .
for this problem .
one might question our choice of the positional bands rather than another collection of gene sets .
we chose the c123 collection because it seemed reasonable and had no overlapping groups .
in general , scien - tists with domain specic knowledge would likely do better choosing a domain specic collection ( eg .
using a colitis associated collection for the colitis data ) .
the second dataset we used for comparison was the breast cancer data of ma et al .
( 123 ) .
this dataset contains gene expression values from 123 patients with estrogen positive breast cancer .
the patients were treated with tamoxifen for 123 years and classied according to whether cancer recurred ( there were 123 recurrences ) .
gene expression values were run on a gpl123 : arcturus 123k human oligonucleotide microarray .
unfortunately , there was signicant missing data .
as a rst pass , all genes with more than 123% missingness were removed .
other missing values were imputed by simple mean imputation .
this left us with 123 of our 123 original genes .
we again grouped genes together by cytogenetic position data , removing genes which were not recorded in the gsea c123 dataset .
our nal design matrix had 123 genes in 123 pathways ( an average of 123 genes per pathway )
correct classification rate for colitis datalambda indexcorrect classification rate123 . 123 . 123methodgllassosgl a sparse - group lasso
figure 123
classication accuracy on 123 test samples for cancer data .
both models were t for a path of 123 - values with min = 123max .
for the sgl , = 123 was
patients were chosen at random and used to build the 123 models .
the remaining 123 were used to test their accuracies .
we again used = 123 for the sparse - group lasso .
each of the 123
models was t for a path of 123 - values with min = 123max .
referring to figure 123 , we see that in this example the sparse - group lasso outperforms the lasso and group lasso .
the sparse - group lasso reaches 123% classication accuracy ( though this is a narrow peak , so may be slightly biased high ) , while the group lasso peaks at 123% and the lasso comes in last at 123% accuracy .
at its optimum the sparse - group lasso includes 123 genes from 123 bands , while the group lasso selects all 123 genes from 123 bands ( again , largely smaller bands for the group lasso ) , and the lasso selects 123 genes all from separate bands .
this example really highlights the advantage of the sparse - group lasso it allows us to use group information , but does not force us to use entire
these two examples highlight two dierent possibilities for the sparse - group lasso .
in the cancer data , the addition of group information is critical for classication , and the grouping may help give insight into the biological mechanisms .
in the colitis data , the group informa - tion largely just increases model variance .
the sparse - group lasso is
correct classification rate for cancer datalambda indexcorrect classification rate123 . 123 . 123 . 123methodgllassosgl 123noah simon , jerome friedman , trevor hastie , and rob tibshirani
certainly not perfect for every scenario with grouped data , but as evi - denced in the cancer data , it can sometimes be helpful .
simulated data
in the previous section we compared the predictive accuracy of the lasso and sparse - group lasso on real data .
one might also be interested in its accuracy as a variable selection tool in this section we compare the regular lasso to the sparse - group lasso for variable selection on simulated data .
we simulated our covariate matrix x with dierent numbers of covariates , observations , and groups .
the columns of x were iid .
gaussian , and the response , y was constructed as
x ( l ) ( l ) +
where n ( 123 , i ) , ( l ) = ( 123 , 123 , .
, 123 , 123 , .
, 123 ) for l = 123 , .
, g , and set so that the signal to noise ratio was 123
the number of generative groups , g varied from 123 to 123 changing the amount of the sparsity .
we chose penalty parameters for both the lasso and sparse - group lasso ( with = 123 ) so that the number of nonzero coecients chosen in the ts matched the true number of nonzero coecients in the gen - erative model ( 123 ) ( 123 , 123 , or 123 corresponding to g = 123 , 123 , 123 ) .
we then compared the proportion of correctly identied covariates averaged over 123 trials .
referring to table 123 , we can see that the sparse - group lasso improves performance in almost all scenarios .
the two scenarios where the sparse - group lasso is slightly outperformed is unsurprising as there are few groups ( m = 123 ) and each group has more covariates than ob - servations ( n = 123 , p = 123 ) , so we gain little by modeling sparsity of
we would like to note that in some trials we were unable to make the sparse - group lasso select exactly the true number of nonzero coecients ( due to the grouping eects ) .
in these cases we allowed the sparse - group lasso to select extra variables ( as few as it could manage ) , however when calculating the proportion of correct nonzero coecient identications we used the total number of variables selected in our denominator , eg .
if the sparse - group lasso selected 123 variables in the 123 true variable case , it would be unable to get a proportion better than 123 / 123 = 123 .
while not ideal , we nd no reason to believe that this would bias our results in favor of the sparse - group lasso .
a sparse - group lasso
number of groups in
123 group 123 groups
n = 123 , p = 123 , m = 123
n = 123 , p = 123 , m = 123
n = 123 , p = 123 , m = 123
n = 123 , p = 123 , m = 123
table 123
proportions of correct nonzero coecient standardized and unstandardized group lasso out of 123 simulated data sets .
timings .
we also timed our algorithm on simulated data for lin - ear , logistic , and cox regression .
our linear data was simulated as in section 123
to simulate binary responses , we applied a logit transfor - mation to a scaling of our linear responses
123 + exp ( 123yi )
and simulated bernoulli random variables with these probabilities .
for cox regression , we set survival / censoring time for observation i to be exp ( yi ) , and simulated our indicators death / censoring independently with equal probability of censoring and death ( ber ( 123 ) ) .
we used the same covariate matrix for each 123 regression types .
123noah simon , jerome friedman , trevor hastie , and rob tibshirani
we found that the unregularized end of the regularization path re - quired by far the most time to solve .
to illustrate this we ran 123 sets of simulations .
for the rst set we use min = 123max , running relatively far along the path .
for the second set we ran a much shorter path with min = 123max .
for some problems it may be necessary to solve for min small .
however , these solutions have many nonzero variables .
as such in large p , small n problems , these unregularized solutions gener - ally have very poor prediction accuracy as they tend to include many noise variables .
in this situations , solving far into the regularization path may often be unnecessary .
our implementation of the sparse - group lasso is called from r , but much of the optimization code is written in c++ and compiled as a shared r / c++ library .
all timings were carried out on an intel xeon 123 ghz processor .
referring to table 123 , we see that while , in some cases , our algorithm scales somewhat poorly , it can still solve fairly large problems with times on the order of minutes .
one noteworthy point is that smaller group sizes allow our algorithm to make better use of active sets , and this is reected in the runtime dierences between the 123 and 123 group cases .
also , as we run further into the regularization path , more groups become active .
this is the main reason our solutions for min = 123 are much slower than for min = 123 even though the 123 paths solve for the same number of - values .
we have proposed and given insight into a method for modeling groupwise and within group sparsity in regression .
we have extended this model to other likelihoods .
we have shown the ecacy of this method on real and simulated data , and given an algorithm to t this model .
an r implementation of this algorithm is available on request , and will soon be uploaded to cran .
supplemental materials
r - package for sgl : r - package sgl containing the code used to t all the spare - group lasso models .
( sgl_123 . tar . gz , gnu zipped tar le )
a sparse - group lasso
number of groups in
123 group 123 groups 123 groups 123 group 123 groups 123 groups
n = 123 , p = 123 , m = 123
n = 123 , p = 123 , m = 123
n = 123 , p = 123 , m = 123
n = 123 , p = 123 , m = 123
table 123
time in seconds to solve for the short and long paths of 123 - values averaged over 123 simulated data sets .
where min = 123max for the short path and min = 123max for the long path .
test code : all the r script les used for simulations and real data calculations run in the manuscript .
( testcode . tar . gz , gnu zipped tar le )
123noah simon , jerome friedman , trevor hastie , and rob tibshirani
